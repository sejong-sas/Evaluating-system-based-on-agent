{
  "1-1 (Weights)": "The only direct statement about weight availability says: \"Our model checkpoints are stored in block-fp8 format, you can find it on [Huggingface](https://huggingface.co/moonshotai/Kimi-K2-Instruct).\"  From this we learn that the Kimi-K2 parameters are (a) already uploaded, (b) hosted on Hugging Face under the repository name \"moonshotai/Kimi-K2-Instruct\", and (c) saved in the specialised \"block-fp8\" numeric format.  The sentence is phrased in the present tense (\"are stored\", \"you can find\"), so the checkpoints are publicly reachable and can be downloaded directly from the indicated URL without any further access-control mechanism being described in the quotes.  No other quotes mention additional mirrors, shard layouts, size, or authentication requirements, so the Hugging Face repository and the block-fp8 format constitute the entirety of the weight-distribution details presently disclosed.",
  "1-1 (Weights)__evidence": [
    {
      "source": "[readme]",
      "quote": "Our model checkpoints are stored in block-fp8 format, you can find it on [Huggingface](https://huggingface.co/moonshotai/Kimi-K2-Instruct)."
    }
  ],
  "1-2 (Code)": "The quote set contains no sentences that explicitly describe a public code repository, training scripts, data-processing pipelines, or configuration files for Kimi-K2.  The only indirect hint comes from a licensing line under item 1-3 that says \"Both the code and the model weights are released under the [Modified MIT License](LICENSE)\", which confirms that some form of code exists and is intended to be distributed under that license.  However, there is no URL, no Git commit, no folder path, and no description distinguishing training code from inference-only code.  Consequently, based solely on the provided material, we cannot point to any concrete location or content of the training pipeline or inference stack; we can only note that the license statement implies the authors intend the code to be available.",
  "1-2 (Code)__evidence": [],
  "1-3 (License)": "Several consecutive license excerpts establish that the project is governed by a \"Modified MIT License\".  The headline line reads: \"Both the code and the model weights are released under the [Modified MIT License](LICENSE).\"  The familiar MIT grant is reproduced verbatim: \"Permission is hereby granted, free of charge, to any person obtaining a copy of this software and associated documentation files ‚Ä¶ to deal in the Software without restriction, including without limitation the rights to use, copy, modify, merge, publish, distribute, sublicense, and/or sell copies ‚Ä¶\".  After this standard text, the authors insert one additional commercial-use provision: \"Our only modification part is that, if the Software (or any derivative works thereof) is used for any of your commercial products or services that have more than 100 million monthly active users, or more than 20 million US dollars ‚Ä¶ in monthly revenue, you shall prominently display \\\"Kimi K2\\\" on the user interface of such product or service.\"  In summary, all four classic MIT rights‚Äîuse, modification, redistribution, and commercial exploitation‚Äîremain permitted, but large-scale commercial deployments (‚â•100 M MAU or ‚â•$20 M monthly revenue) must visibly attribute the model by showing the name \"Kimi K2\" in the product UI.",
  "1-3 (License)__evidence": [
    {
      "source": "[readme]",
      "quote": "Both the code and the model weights are released under the [Modified MIT License](LICENSE)."
    },
    {
      "source": "[license_files]",
      "quote": "Modified MIT License"
    },
    {
      "source": "[license_files]",
      "quote": "Permission is hereby granted, free of charge, to any person obtaining a copy of this software and associated documentation files (the ‚ÄúSoftware‚Äù), to deal in the Software without restriction, including without limitation the rights to use, copy, modify, merge, publish, distribute, sublicense, and/or sell copies of the Software, and to permit persons to whom the Software is furnished to do so, subject to the following conditions:"
    },
    {
      "source": "[license_files]",
      "quote": "Our only modification part is that, if the Software (or any derivative works thereof) is used for any of your commercial products or services that have more than 100 million monthly active users, or more than 20 million US dollars (or equivalent in other currencies) in monthly revenue, you shall prominently display \"Kimi K2\" on the user interface of such product or service."
    }
  ],
  "1-4 (Paper)": "A bundled reference section advertises both a technical blog and a full formal report.  The HTML snippet \"<b>üì∞  <a href=\\\"https://moonshotai.github.io/Kimi-K2/\\\">Tech Blog</a></b>\" points to an official write-up hosted on the MoonshotAI website, while \"<b>üìÑ  <a href=\\\"https://www.arxiv.org/abs/2507.20534\\\">Full Report</a></b>\" links to the arXiv preprint.  The BibTeX stub supplied in the quotes begins with \"@misc{kimiteam2025kimik2openagentic,\" followed by the title line \"title={Kimi K2: Open Agentic Intelligence},\" and repeats the arXiv URL.  Together these quotes confirm that (1) there is a dedicated tech-blog article for readers wanting a narrative overview, and (2) there is a full, citable research report on arXiv, formally titled \"Kimi K2: Open Agentic Intelligence\" and attributed to the \"Kimi Team\" with an intended citation year of 2025.",
  "1-4 (Paper)__evidence": [
    {
      "source": "[readme]",
      "quote": "<b>üì∞&nbsp;&nbsp;<a href=\"https://moonshotai.github.io/Kimi-K2/\">Tech Blog</a></b> &nbsp;&nbsp;&nbsp; | &nbsp;&nbsp;&nbsp; <b>üìÑ&nbsp;&nbsp;<a href=\"https://www.arxiv.org/abs/2507.20534\">Full Report</a></b>"
    },
    {
      "source": "[readme]",
      "quote": "@misc{kimiteam2025kimik2openagentic,"
    },
    {
      "source": "[readme]",
      "quote": "title={Kimi K2: Open Agentic Intelligence},"
    },
    {
      "source": "[readme]",
      "quote": "url={https://arxiv.org/abs/2507.20534},"
    }
  ],
  "1-5 (Architecture)": "The statements that explicitly mention Kimi-K2 give four concrete architectural facts. First, the quote ‚Äú| **Activated Parameters** | 32B |‚Äù establishes that the model keeps 32 billion parameters active during inference or training. Second, the line ‚Äú| **MoE Hidden Dimension** (per Expert) | 2048 |‚Äù indicates that Kimi-K2 uses a Mixture-of-Experts design in which each expert has a hidden size of 2 048. Third, the row ‚Äú| **Selected Experts per Token** | 8 |‚Äù reveals that eight experts are routed to every individual token, confirming a multi-expert routing strategy. Finally, ‚Äú| **Context Length** | 128K |‚Äù specifies that the maximum sequence length processed in one pass is 128 k tokens. Taken together, these four figures describe Kimi-K2 as a 32-billion-parameter MoE architecture with 2 048-dimension experts, eight experts chosen per token, and an extended 128 k-token window.",
  "1-5 (Architecture)__evidence": [
    {
      "source": "[readme]",
      "quote": "| **Activated Parameters** | 32B |"
    },
    {
      "source": "[readme]",
      "quote": "| **MoE Hidden Dimension** (per Expert) | 2048 |"
    },
    {
      "source": "[readme]",
      "quote": "| **Selected Experts per Token** | 8 |"
    },
    {
      "source": "[readme]",
      "quote": "| **Context Length** | 128K |"
    }
  ],
  "1-6 (Tokenizer)": "The tokenizer information is limited to a single quantitative line: ‚Äú| **Vocabulary Size** | 160K |.‚Äù This shows that Kimi-K2‚Äôs tokenizer is built with a 160 000-token vocabulary, but no further structural or download details are given in the supplied text.",
  "1-6 (Tokenizer)__evidence": [
    {
      "source": "[readme]",
      "quote": "| **Vocabulary Size** | 160K |"
    }
  ],
  "2-1 (Hardware)": "",
  "2-1 (Hardware)__evidence": [],
  "2-2 (Software)": "The only training-software detail provided is the sentence: ‚ÄúTrained with the Muon optimizer, Kimi K2 achieves exceptional performance across frontier knowledge, reasoning, and coding tasks while being meticulously optimized for agentic capabilities.‚Äù From this we can conclude that the Muon optimizer was employed during Kimi-K2‚Äôs training run; no additional frameworks, libraries, or version specifics are disclosed in the available excerpts.",
  "2-2 (Software)__evidence": [
    {
      "source": "[readme]",
      "quote": "Trained with the Muon optimizer, Kimi K2 achieves exceptional performance across frontier knowledge, reasoning, and coding tasks while being meticulously optimized for agentic capabilities."
    }
  ],
  "2-3 (API)": "The quotes specify that ‚ÄúYou can access Kimi K2's API on https://platform.moonshot.ai,‚Äù underlining that the developer-facing endpoint is hosted at Moonshot‚Äôs platform domain. The same sentence clarifies compatibility, stating that Kimi-K2 exposes ‚ÄúOpenAI/Anthropic-compatible API‚Äù semantics, implying users can interact with it through request/response patterns and tooling originally designed for those well-known ecosystems. Thus, the project publicly advertises a self-service, remotely hosted, production API for the Kimi-K2 model family, reachable via a dedicated URL and designed to integrate with existing GPT-style or Anthropic-style client libraries.",
  "2-3 (API)__evidence": [
    {
      "source": "[readme]",
      "quote": "> You can access Kimi K2's API on https://platform.moonshot.ai , we provide OpenAI/Anthropic-compatible API for you."
    }
  ],
  "3-1 (Pre-training)": "According to the provided material, the Kimi-K2 effort relies on ‚ÄúLarge-Scale Training‚Äù of ‚Äúa 1T parameter MoE model.‚Äù The model was pre-trained on an immense corpus amounting to ‚Äú15.5T tokens,‚Äù and the training run achieved ‚Äúzero training instability,‚Äù highlighting robustness during scale-up. Optimization relied on the ‚ÄúMuonClip Optimizer,‚Äù described as applying ‚Äúthe Muon optimizer to an unprecedented scale.‚Äù Novel optimization techniques were introduced specifically to ‚Äúresolve instabilities while scaling up,‚Äù indicating that the team focused on stabilizing gradient updates and convergence when training a trillion-parameter mixture-of-experts architecture on tens of trillions of tokens.",
  "3-1 (Pre-training)__evidence": [
    {
      "source": "[readme]",
      "quote": "- Large-Scale Training: Pre-trained a 1T parameter MoE model on 15.5T tokens with zero training instability."
    },
    {
      "source": "[readme]",
      "quote": "- MuonClip Optimizer: We apply the Muon optimizer to an unprecedented scale, and develop novel optimization techniques to resolve instabilities while scaling up."
    }
  ],
  "3-2 (Fine-tuning)": "The fine-tuning layer of the release is split into at least two named checkpoints. First, ‚ÄúKimi-K2-Base‚Äù is positioned as ‚Äúthe foundation model,‚Äù intended for ‚Äúresearchers and builders who want full control for fine-tuning and custom solutions.‚Äù This signals that the base checkpoint is delivered without instruction or preference specialization, giving downstream users a blank slate on which to apply their own supervised or alignment procedures. Second, ‚ÄúKimi-K2-Instruct‚Äù is described as ‚Äúthe post-trained model best for drop-in, general-purpose chat and agentic experiences.‚Äù It is explicitly characterized as ‚Äúa reflex-grade model without long thinking,‚Äù suggesting that the post-training stage produces fast, instruction-following outputs suitable for conversational applications.",
  "3-2 (Fine-tuning)__evidence": [
    {
      "source": "[readme]",
      "quote": "- **Kimi-K2-Base**: The foundation model, a strong start for researchers and builders who want full control for fine-tuning and custom solutions."
    },
    {
      "source": "[readme]",
      "quote": "- **Kimi-K2-Instruct**: The post-trained model best for drop-in, general-purpose chat and agentic experiences. It is a reflex-grade model without long thinking."
    }
  ],
  "3-3 (Reinforcement Learning)": "",
  "3-3 (Reinforcement Learning)__evidence": [],
  "4-1 (Pre-training Data)": "The quoted statement specifies that pre-training was carried out at very large scale: a Mixture-of-Experts architecture with 1 trillion parameters was exposed to a corpus containing 15.5 trillion tokens. The source stresses that the entire run completed \"with zero training instability,\" implying that the volume and composition of tokens were sufficient to sustain stable optimization throughout the pre-training phase.",
  "4-1 (Pre-training Data)__evidence": [
    {
      "source": "[readme]",
      "quote": "Large-Scale Training: Pre-trained a 1T parameter MoE model on 15.5T tokens with zero training instability."
    }
  ],
  "4-2 (Fine-tuning Data)": "",
  "4-2 (Fine-tuning Data)__evidence": [],
  "4-3 (Reinforcement Learning Data)": "",
  "4-3 (Reinforcement Learning Data)__evidence": [],
  "4-4 (Data Filtering)": "",
  "4-4 (Data Filtering)__evidence": [],
  "__usage": {
    "fine_tuning": "used",
    "rl": "not_used"
  }
}