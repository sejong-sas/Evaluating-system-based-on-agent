{
  "4-1 (Pre-training Data)": "The only disclosed pre-training detail for moonshotai/kimi-k2-instruct is that “Kimi K2 is a state-of-the-art mixture-of-experts (MoE) language model with 32 billion activated parameters and 1 trillion total parameters … Pre-trained a 1 T parameter MoE model on 15.5 T tokens with zero training instability,” and that the training run used the Muon optimizer. From these sentences we can infer that the developers fed the model an extremely large corpus—15.5 trillion tokens—during pre-training, achieving stable convergence throughout. No additional information about the nature of those tokens (data domains, languages, sources, licenses, or geographic/temporal coverage) is revealed in the provided material.",
  "4-1 (Pre-training Data)__evidence": [
    {
      "source": "[readme]",
      "quote": "Kimi K2 is a state-of-the-art mixture-of-experts (MoE) language model with 32 billion activated parameters and 1 trillion total parameters. Trained with the Muon optimizer, Kimi K2 achieves exceptional performance across frontier knowledge, reasoning, and coding tasks while being meticulously optimized for agentic capabilities.\n- Large-Scale Training: Pre-trained a 1T parameter MoE model on 15.5T tokens with zero training instability."
    }
  ],
  "4-2 (Fine-tuning Data)": "",
  "4-2 (Fine-tuning Data)__evidence": [],
  "4-3 (Reinforcement Learning Data)": "",
  "4-3 (Reinforcement Learning Data)__evidence": [],
  "4-4 (Data Filtering)": "",
  "4-4 (Data Filtering)__evidence": []
}