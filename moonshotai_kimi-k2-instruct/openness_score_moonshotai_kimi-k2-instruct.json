{
  "model": "moonshotai/Kimi-K2-Instruct",
  "scores": {
    "1-1 Weights": {
      "score": 1.0,
      "reason": "Hugging Face presence detected; forced Open per FORCE_HF_CORE_OPEN."
    },
    "1-2 Code": {
      "score": 0.0,
      "reason": "No training pipeline files; README mentions are ignored."
    },
    "1-3 License": {
      "score": 1.0,
      "reason": "Modified-MIT allows use, modification, redistribution and commercial use; the only extra clause is an attribution requirement for very-large-scale commercial deployments, which the rubric treats as still Open."
    },
    "1-4 Paper": {
      "score": 1.0,
      "reason": "An official, provider-authored technical report (tech_report.pdf) is linked from the project’s GitHub/website."
    },
    "1-5 Architecture": {
      "score": 1.0,
      "reason": "Hugging Face presence detected; forced Open per FORCE_HF_CORE_OPEN."
    },
    "1-6 Tokenizer": {
      "score": 1.0,
      "reason": "Hugging Face presence detected; forced Open per FORCE_HF_CORE_OPEN."
    },
    "2-1 Hardware": {
      "score": 0.5,
      "reason": "Quotes specify training ran on nodes each with 8 × NVIDIA H800 GPUs and 2 TB RAM; the *total* GPU count is not given, so only partial quantity disclosure."
    },
    "2-2 Software": {
      "score": 0.5,
      "reason": "Training stack details include Muon / MuonClip optimizer, WSD LR schedule, QK-Clip, ZeRO-1 DP, 16-way PP & EP, but no full version list or complete stack; thus partial disclosure."
    },
    "2-3 API": {
      "score": 1.0,
      "reason": "Only generic or non-official mentions; no qualifying official API docs.  apifind: API_OPEN via https://kimi-k2.ai/api-docs, https://docs.api.nvidia.com/nim/reference/moonshotai-kimi-k2-instruct, https://docs.skypilot.co/en/latest/examples/models/kimi-k2.html. Owner inferred as Moonshot AI. Major version matched."
    },
    "3-1 Pre-training": {
      "score": 0.5,
      "reason": "Methodology (MuonClip, τ = 100, 4 k ctx, WSD schedule, QK-Clip behaviour) is described, but not to the level of full reproducibility (missing full configs, exact hyper-param tables)."
    },
    "3-2 Fine-tuning": {
      "score": 0.5,
      "reason": "The post-training pipeline (instruction SFT with Muon, synthetic data generation) is outlined but not fully specified."
    },
    "3-3 Reinforcement Learning": {
      "score": 0.5,
      "reason": "RL stage, reward sources and the policy-optimisation algorithm inherited from K1.5 are described, yet complete reproducible details are absent."
    },
    "4-1 Pre-training Data": {
      "score": 0.5,
      "reason": "Domain mix (Web, Code, Math, Knowledge) and total 15.5 T tokens are given, but the actual corpus or manifest is not released."
    },
    "4-2 Fine-tuning Data": {
      "score": 0.5,
      "reason": "Authors describe a large synthetic+curated instruction dataset and filtering process, but do not release the data itself."
    },
    "4-3 Reinforcement Learning Data": {
      "score": 0.5,
      "reason": "High-level description of RL prompts, Gym-like framework, and reward sources is provided; the concrete data are not released."
    },
    "4-4 Data Filtering": {
      "score": 0.5,
      "reason": "Mentions of ‘rigorous correctness and quality validation’, fidelity checks, decontamination procedures, and human/LLM filtering, but without reproducible thresholds or algorithmic parameters."
    }
  },
  "included_scores": {
    "1-1 Weights": {
      "score": 1.0,
      "reason": "Hugging Face presence detected; forced Open per FORCE_HF_CORE_OPEN."
    },
    "1-2 Code": {
      "score": 0.0,
      "reason": "No training pipeline files; README mentions are ignored."
    },
    "1-3 License": {
      "score": 1.0,
      "reason": "Modified-MIT allows use, modification, redistribution and commercial use; the only extra clause is an attribution requirement for very-large-scale commercial deployments, which the rubric treats as still Open."
    },
    "1-4 Paper": {
      "score": 1.0,
      "reason": "An official, provider-authored technical report (tech_report.pdf) is linked from the project’s GitHub/website."
    },
    "1-5 Architecture": {
      "score": 1.0,
      "reason": "Hugging Face presence detected; forced Open per FORCE_HF_CORE_OPEN."
    },
    "1-6 Tokenizer": {
      "score": 1.0,
      "reason": "Hugging Face presence detected; forced Open per FORCE_HF_CORE_OPEN."
    },
    "2-1 Hardware": {
      "score": 0.5,
      "reason": "Quotes specify training ran on nodes each with 8 × NVIDIA H800 GPUs and 2 TB RAM; the *total* GPU count is not given, so only partial quantity disclosure."
    },
    "2-2 Software": {
      "score": 0.5,
      "reason": "Training stack details include Muon / MuonClip optimizer, WSD LR schedule, QK-Clip, ZeRO-1 DP, 16-way PP & EP, but no full version list or complete stack; thus partial disclosure."
    },
    "2-3 API": {
      "score": 1.0,
      "reason": "Only generic or non-official mentions; no qualifying official API docs.  apifind: API_OPEN via https://kimi-k2.ai/api-docs, https://docs.api.nvidia.com/nim/reference/moonshotai-kimi-k2-instruct, https://docs.skypilot.co/en/latest/examples/models/kimi-k2.html. Owner inferred as Moonshot AI. Major version matched."
    },
    "3-1 Pre-training": {
      "score": 0.5,
      "reason": "Methodology (MuonClip, τ = 100, 4 k ctx, WSD schedule, QK-Clip behaviour) is described, but not to the level of full reproducibility (missing full configs, exact hyper-param tables)."
    },
    "3-2 Fine-tuning": {
      "score": 0.5,
      "reason": "The post-training pipeline (instruction SFT with Muon, synthetic data generation) is outlined but not fully specified."
    },
    "3-3 Reinforcement Learning": {
      "score": 0.5,
      "reason": "RL stage, reward sources and the policy-optimisation algorithm inherited from K1.5 are described, yet complete reproducible details are absent."
    },
    "4-1 Pre-training Data": {
      "score": 0.5,
      "reason": "Domain mix (Web, Code, Math, Knowledge) and total 15.5 T tokens are given, but the actual corpus or manifest is not released."
    },
    "4-2 Fine-tuning Data": {
      "score": 0.5,
      "reason": "Authors describe a large synthetic+curated instruction dataset and filtering process, but do not release the data itself."
    },
    "4-3 Reinforcement Learning Data": {
      "score": 0.5,
      "reason": "High-level description of RL prompts, Gym-like framework, and reward sources is provided; the concrete data are not released."
    },
    "4-4 Data Filtering": {
      "score": 0.5,
      "reason": "Mentions of ‘rigorous correctness and quality validation’, fidelity checks, decontamination procedures, and human/LLM filtering, but without reproducible thresholds or algorithmic parameters."
    }
  },
  "final_score_10pt": 6.562,
  "meta": {
    "usage_from_dispatch": {
      "fine_tuning": "used",
      "rl": "used"
    },
    "excluded": [],
    "denominator": 16,
    "raw_sum": 10.5,
    "scale": "10/16",
    "code_detection_reason": "No training pipeline files; README mentions are ignored.",
    "pretrain_sources_used": false
  }
}