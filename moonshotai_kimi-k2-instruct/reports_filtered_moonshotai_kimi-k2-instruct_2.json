{
  "1-5 (Architecture)": "The available quotes portray Kimi K2 as a very large Mixture-of-Experts (MoE) transformer. The model’s overall size is repeatedly stated as roughly one trillion parameters: “1 T-parameter” or more precisely “1.04 trillion-parameter,” but in any single forward pass only “32 billion activated parameters” are used. The design “follows a similar design to DeepSeek-V3,” and specifically adopts the Multi-head Latent Attention (MLA) mechanism. Core numerical hyper-parameters are given: the model-wide hidden dimension is 7 168, while each MoE expert has a hidden dimension of 2 048. To reduce inference cost, the number of attention heads has been halved relative to DeepSeek-V3, dropping from 128 to 64. Sparsity is explicitly described as “48,” operationalized as “activating 8 out of 384 experts per forward pass.” Because the attention computation is lighter (only 64 heads), the team also “ensures full computation-communication overlap during the 1F1B stage” by shrinking Expert Parallelism (EP) size so the EP operations do not become the new bottleneck. In summary, Kimi K2 is a 1.04-T parameter, sparsity-48 MoE transformer that activates 32 B parameters per token, employs MLA attention with a 7 168 hidden size and 64 heads, and uses 2 048-wide experts organized so that 8 of 384 experts participate in each pass.",
  "1-6 (Tokenizer)": "",
  "2-1 (Hardware)": "Training was performed on a cluster of NVIDIA H800 GPUs. Each node in this H800 cluster is described as having 2 TB of RAM and 8 GPUs that are internally connected via both NVLink and NVSwitch interconnects.",
  "2-2 (Software)": "The training stack for Kimi K2 is documented in several aspects. Optimization is carried out with the “token-efficient Muon optimizer,” which is used both during the main training run and recommended for later fine-tuning. Weight decay and “consistent update RMS scaling” are mentioned as parts of the Muon configuration, and the run further employs “MuonClip with τ = 100.” The broader learning-algorithm foundation is the “policy optimization algorithm introduced in K1.5,” now reused for K2. Parallelism is deliberately flexible: the system can train on any node count that is a multiple of 32 by combining “16-way Pipeline Parallelism (PP) with virtual stages,” “16-way Expert Parallelism (EP),” and “ZeRO-1 Data Parallelism.” One stability/regularization mechanism—QK-Clip—is tracked in detail: in the first 70 000 steps 12.7 % of attention heads invoked QK-Clip (clamping Smax = 100), after which all heads gradually reduced Smax below 100 so the clipping became inactive. Together, the quotes indicate a PyTorch-style large-scale training stack centered on Muon optimization, specialized parallelism (PP + EP + ZeRO-1), and selective use of QK-Clip for attention stability.",
  "1-5 (Architecture)__evidence": [
    {
      "source": "[pdf_text]",
      "quote": "We introduce Kimi K2, a Mixture-of-Experts (MoE) large language model with 32 billion activated parameters and 1 trillion total parameters."
    },
    {
      "source": "[pdf_text]",
      "quote": "Kimi K2 is a 1.04 trillion-parameter Mixture-of-Experts (MoE) transformer model with 32 billion activated parameters. The architecture follows a similar design to DeepSeek-V3 [11] , employing Multi-head Latent Attention (MLA) [45] as the attention mechanism, with a model hidden dimension of 7168 and an MoE expert hidden dimension of 2048."
    },
    {
      "source": "[pdf_text]",
      "quote": "We introduced Kimi K2, a 1T-parameter open-weight MoE model built for agentic intelligence."
    },
    {
      "source": "[pdf_text]",
      "quote": "To reduce computational overhead during inference, we cut the number of attention heads to 64, as opposed to 128 in DeepSeek-V3. Table 2 presents a detailed comparison of architectural parameters between Kimi K2 and DeepSeek-V3."
    },
    {
      "source": "[pdf_text]",
      "quote": "To balance model performance with cost, we adopt a sparsity of 48 for Kimi K2, activating 8 out of 384 experts per forward pass."
    },
    {
      "source": "[pdf_text]",
      "quote": "Smaller EP size  To ensure full computation-communication overlap during the 1F1B stage, the reduced attention computation time in K2 (which has 64 attention heads compared to 128 heads in DeepSeek-V3) necessitates minimizing the time of EP operations."
    }
  ],
  "1-6 (Tokenizer)__evidence": [],
  "2-1 (Hardware)__evidence": [
    {
      "source": "[pdf_text]",
      "quote": "Kimi K2 was trained on a cluster equipped with NVIDIA H800 GPUs. Each node in the H800 cluster contains 2 TB RAM and 8 GPUs connected by NVLink and NVSwitch within nodes."
    }
  ],
  "2-2 (Software)__evidence": [
    {
      "source": "[pdf_text]",
      "quote": "We train Kimi K2 using the token-efficient Muon optimizer [34], incorporating weight decay and consistent update RMS scaling [47]."
    },
    {
      "source": "[pdf_text]",
      "quote": "We employ the Muon optimizer [34] in our post-training and recommend its use for fine-tuning with K2."
    },
    {
      "source": "[pdf_text]",
      "quote": "Instead of optimizing one parallelism strategy that's only applicable under specific amount of resources, we pursue a flexible strategy that allows Kimi K2 to be trained on any number of nodes that is a multiple of 32."
    },
    {
      "source": "[pdf_text]",
      "quote": "We adopt the policy optimization algorithm introduced in K1.5 [36] as the foundation for K2."
    },
    {
      "source": "[pdf_text]",
      "quote": "Finally, we train Kimi K2, a large-scale MoE model, using MuonClip with τ = 100 and monitor the maximum attention logits throughout the training run."
    },
    {
      "source": "[pdf_text]",
      "quote": "Instead of optimizing one parallelism strategy that’s only applicable under specific amount of resources, we pursue a flexible strategy that allows Kimi K2 to be trained on any number of nodes that is a multiple of 32. Our strategy leverages a combination of 16-way Pipeline Parallelism (PP) with virtual stages, 16-way Expert Parallelism (EP), and ZeRO-1 Data Parallelism."
    },
    {
      "source": "[pdf_text]",
      "quote": "In Kimi K2, QK-Clip was only transiently active: • Initial 70000 steps: 12.7% of attention heads triggered QK-Clip for at least once, clamping Smax to 100. • Post-70000 steps: All heads at some point reduced their Smax below 100, rendering QK-Clip inactive."
    }
  ]
}