{
  "1-5 (Architecture)": "Meta indicates that the Llama 3 release provides both model weights and starter code for pre-trained as well as instruction-tuned variants spanning 8 B, 70 B and intermediate parameter counts. The architecture supports a context window of 8,192 tokens; consequently, any configuration must keep the max_seq_len parameter at or below 8,192 tokens to respect the design limits of llama3.",
  "1-5 (Architecture)__evidence": [
    {
      "source": "[readme]",
      "quote": "This release includes model weights and starting code for pre-trained and instruction-tuned Llama 3 language models â€” including sizes of 8B to 70B parameters."
    },
    {
      "source": "[py_files/example_chat_completion.py]",
      "quote": "The context window of llama3 models is 8192 tokens, so `max_seq_len` needs to be <= 8192."
    }
  ],
  "1-6 (Tokenizer)": "The tokenizer that accompanies Llama 3 is distributed together with the model weights, and users must download it from the official Meta Llama website after affirmatively accepting the license terms presented there.",
  "1-6 (Tokenizer)__evidence": [
    {
      "source": "[readme]",
      "quote": "To download the model weights and tokenizer, please visit the Meta Llama website and accept our License."
    }
  ],
  "2-1 (Hardware)": "",
  "2-1 (Hardware)__evidence": [],
  "2-2 (Software)": "",
  "2-2 (Software)__evidence": []
}