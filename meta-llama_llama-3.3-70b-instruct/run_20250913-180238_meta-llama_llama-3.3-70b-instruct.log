
======== 1/3 ▶ meta-llama/Llama-3.3-70B-Instruct ========
📁 Directory to create/use: meta-llama_llama-3.3-70b-instruct
📁 Output path: meta-llama_llama-3.3-70b-instruct
1️⃣ HF: True, GH: False
✅ Resolved GH repo: meta-llama/llama3 (score=17, detail={'org_affinity': 9, 'name_hits': 2, 'readme_hits': 4, 'path_hits': 2, 'bad_keywords': 10, 'from_hf_link': 0, 'version_conflict': 0})
✅ HF model: meta-llama/llama-3.3-70b-instruct (found at priority: 1)
📄 Reports saved/merged (HF): meta-llama_llama-3.3-70b-instruct\reports_fulltext_huggingface_meta-llama_llama-3.3-70b-instruct.json
✅ JSON file saved: meta-llama_llama-3.3-70b-instruct\huggingface_meta-llama_llama-3.3-70b-instruct.json
📄 Reports merged to: meta-llama_llama-3.3-70b-instruct\reports_fulltext_meta-llama_llama-3.3-70b-instruct.json (HF sources)
✅ Saved group 1 result: meta-llama_llama-3.3-70b-instruct\huggingface_filtered_meta-llama_llama-3.3-70b-instruct_1.json
✅ Saved group 2 result: meta-llama_llama-3.3-70b-instruct\huggingface_filtered_meta-llama_llama-3.3-70b-instruct_2.json
✅ Saved group 3 result: meta-llama_llama-3.3-70b-instruct\huggingface_filtered_meta-llama_llama-3.3-70b-instruct_3.json
✅ Saved group 4 result: meta-llama_llama-3.3-70b-instruct\huggingface_filtered_meta-llama_llama-3.3-70b-instruct_4.json
✅ Saved final merged result: meta-llama_llama-3.3-70b-instruct\huggingface_filtered_final_meta-llama_llama-3.3-70b-instruct.json
✅ GH repo: meta-llama/llama3
📄 Reports saved/merged (GH): meta-llama_llama-3.3-70b-instruct\reports_fulltext_github_meta-llama_llama3.json
ℹ️ Merge skipped: could not determine a single target HF model id.
✅ GitHub JSON file saved: meta-llama_llama-3.3-70b-instruct\github_meta-llama_llama3.json
evidence counts before/after model-guard: {'raw': {'1-1 (Weights)': 3, '1-2 (Code)': 1, '1-3 (License)': 7, '1-4 (Paper)': 1}, 'kept': {'1-1 (Weights)': 3, '1-2 (Code)': 1, '1-3 (License)': 7, '1-4 (Paper)': 1}}
✅ Saved group 1 result: meta-llama_llama-3.3-70b-instruct\github_filtered_meta-llama_llama3_1.json
evidence counts before/after model-guard: {'raw': {'1-5 (Architecture)': 3, '1-6 (Tokenizer)': 2, '2-1 (Hardware)': 0, '2-2 (Software)': 0}, 'kept': {'1-5 (Architecture)': 2, '1-6 (Tokenizer)': 1, '2-1 (Hardware)': 0, '2-2 (Software)': 0}}
✅ Saved group 2 result: meta-llama_llama-3.3-70b-instruct\github_filtered_meta-llama_llama3_2.json
evidence counts before/after model-guard: {'raw': {'2-3 (API)': 0, '3-1 (Pre-training)': 2, '3-2 (Fine-tuning)': 2, '3-3 (Reinforcement Learning)': 0}, 'kept': {'2-3 (API)': 0, '3-1 (Pre-training)': 1, '3-2 (Fine-tuning)': 0, '3-3 (Reinforcement Learning)': 0}}
✅ Saved group 3 result: meta-llama_llama-3.3-70b-instruct\github_filtered_meta-llama_llama3_3.json
evidence counts before/after model-guard: {'raw': {'4-1 (Pre-training Data)': 0, '4-2 (Fine-tuning Data)': 0, '4-3 (Reinforcement Learning Data)': 0, '4-4 (Data Filtering)': 2}, 'kept': {'4-1 (Pre-training Data)': 0, '4-2 (Fine-tuning Data)': 0, '4-3 (Reinforcement Learning Data)': 0, '4-4 (Data Filtering)': 2}}
✅ Saved group 4 result: meta-llama_llama-3.3-70b-instruct\github_filtered_meta-llama_llama3_4.json
✅ Saved final merged result: meta-llama_llama-3.3-70b-instruct\github_filtered_final_meta-llama_llama3.json
🔎 HF tags found arXiv IDs: ['2204.05149']
🔄 Simplified query: 'llama 3.3'
🔎 Tavily search: llama 3.3 paper
  → arXiv link found: https://arxiv.org/abs/2407.21783
🔎 Tavily search: llama 3.3 technical report
  → arXiv link found: https://arxiv.org/abs/2407.21783
🛰️ Tavily candidates: ['2407.21783']
🔬 Verifying 1 Tavily candidate(s) with GPT…
  • Candidate: 2407.21783
    - GPT verdict: ✅ match (The paper is the official technical report for Llama 3 models. Although it reports on Llama 3.1, its MAJOR version (3) is the same as that of the target (3.3) and the minor version difference (0.2) is)
✅ GPT-verified IDs: ['2407.21783']
📦 Final merged arXiv IDs: ['2204.05149', '2407.21783']
📄 PDF saved: meta-llama_llama-3.3-70b-instruct\arxiv_2204.05149.pdf
📄 PDF saved: meta-llama_llama-3.3-70b-instruct\arxiv_2407.21783.pdf
✅ Full paper text saved: meta-llama_llama-3.3-70b-instruct\arxiv_fulltext_meta-llama_llama-3.3-70b-instruct.json
📄 Reports merged to: meta-llama_llama-3.3-70b-instruct\reports_fulltext_meta-llama_llama-3.3-70b-instruct.json
🔎 recall keys: ['1-1 (Weights)', '1-2 (Code)', '1-3 (License)', '1-4 (Paper)']
🔎 recall keys: ['1-1 (Weights)', '1-2 (Code)', '1-3 (License)', '1-4 (Paper)']
🔎 recall keys: ['1-1 (Weights)', '1-2 (Code)', '1-3 (License)', '1-4 (Paper)']
⚠️ Error in group 1: Error code: 400 - {'error': {'message': "Invalid prompt: we've limited access to this content for safety reasons. This type of information may be used to benefit or to harm people. We are continuously refining our work in this area, and you can read more about our approach in our blog post (https://openai.com/index/preparing-for-future-ai-capabilities-in-biology) and Model Spec (https://openai.com/index/introducing-the-model-spec).", 'type': 'invalid_request_error', 'param': None, 'code': 'invalid_prompt'}}
✅ Saved group 1 : meta-llama_llama-3.3-70b-instruct\arxiv_filtered_meta-llama_llama-3.3-70b-instruct_1.json
🔎 recall keys: ['1-5 (Architecture)', '1-6 (Tokenizer)', '2-1 (Hardware)', '2-2 (Software)']
🔎 recall keys: ['1-5 (Architecture)', '1-6 (Tokenizer)', '2-1 (Hardware)', '2-2 (Software)']
🔎 recall keys: ['1-5 (Architecture)', '1-6 (Tokenizer)', '2-1 (Hardware)', '2-2 (Software)']
🔎 recall keys: ['1-5 (Architecture)', '1-6 (Tokenizer)', '2-1 (Hardware)', '2-2 (Software)']
🔎 recall keys: ['1-5 (Architecture)', '1-6 (Tokenizer)', '2-1 (Hardware)', '2-2 (Software)']
🔎 recall keys: ['1-5 (Architecture)', '1-6 (Tokenizer)', '2-1 (Hardware)', '2-2 (Software)']
🔎 recall keys: ['1-5 (Architecture)', '1-6 (Tokenizer)', '2-1 (Hardware)', '2-2 (Software)']
🔎 recall keys: ['1-5 (Architecture)', '1-6 (Tokenizer)', '2-1 (Hardware)', '2-2 (Software)']
🔎 recall keys: ['1-5 (Architecture)', '1-6 (Tokenizer)', '2-1 (Hardware)', '2-2 (Software)']
🔎 recall keys: ['1-5 (Architecture)', '1-6 (Tokenizer)', '2-1 (Hardware)', '2-2 (Software)']
🔎 recall keys: ['1-5 (Architecture)', '1-6 (Tokenizer)', '2-1 (Hardware)', '2-2 (Software)']
🔎 recall keys: ['1-5 (Architecture)', '1-6 (Tokenizer)', '2-1 (Hardware)', '2-2 (Software)']
🔎 recall keys: ['1-5 (Architecture)', '1-6 (Tokenizer)', '2-1 (Hardware)', '2-2 (Software)']
🔎 recall keys: ['1-5 (Architecture)', '1-6 (Tokenizer)', '2-1 (Hardware)', '2-2 (Software)']
evidence counts before/after model-guard: {'raw': {'1-5 (Architecture)': 17, '1-6 (Tokenizer)': 2, '2-1 (Hardware)': 6, '2-2 (Software)': 6}, 'kept': {'1-5 (Architecture)': 15, '1-6 (Tokenizer)': 2, '2-1 (Hardware)': 6, '2-2 (Software)': 5}}
✅ Saved group 2 : meta-llama_llama-3.3-70b-instruct\arxiv_filtered_meta-llama_llama-3.3-70b-instruct_2.json
🔎 recall keys: ['2-3 (API)', '3-1 (Pre-training)', '3-2 (Fine-tuning)', '3-3 (Reinforcement Learning)']
🔎 recall keys: ['2-3 (API)', '3-1 (Pre-training)', '3-2 (Fine-tuning)', '3-3 (Reinforcement Learning)']
🔎 recall keys: ['2-3 (API)', '3-1 (Pre-training)', '3-2 (Fine-tuning)', '3-3 (Reinforcement Learning)']
⚠️ Error in group 3: Error code: 400 - {'error': {'message': "Invalid prompt: we've limited access to this content for safety reasons. This type of information may be used to benefit or to harm people. We are continuously refining our work in this area, and you can read more about our approach in our blog post (https://openai.com/index/preparing-for-future-ai-capabilities-in-biology) and Model Spec (https://openai.com/index/introducing-the-model-spec).", 'type': 'invalid_request_error', 'param': None, 'code': 'invalid_prompt'}}
✅ Saved group 3 : meta-llama_llama-3.3-70b-instruct\arxiv_filtered_meta-llama_llama-3.3-70b-instruct_3.json
🔎 recall keys: ['4-1 (Pre-training Data)', '4-2 (Fine-tuning Data)', '4-3 (Reinforcement Learning Data)', '4-4 (Data Filtering)']
🔎 recall keys: ['4-1 (Pre-training Data)', '4-2 (Fine-tuning Data)', '4-3 (Reinforcement Learning Data)', '4-4 (Data Filtering)']
🔎 recall keys: ['4-1 (Pre-training Data)', '4-2 (Fine-tuning Data)', '4-3 (Reinforcement Learning Data)', '4-4 (Data Filtering)']
🔎 recall keys: ['4-1 (Pre-training Data)', '4-2 (Fine-tuning Data)', '4-3 (Reinforcement Learning Data)', '4-4 (Data Filtering)']
🔎 recall keys: ['4-1 (Pre-training Data)', '4-2 (Fine-tuning Data)', '4-3 (Reinforcement Learning Data)', '4-4 (Data Filtering)']
🔎 recall keys: ['4-1 (Pre-training Data)', '4-2 (Fine-tuning Data)', '4-3 (Reinforcement Learning Data)', '4-4 (Data Filtering)']
🔎 recall keys: ['4-1 (Pre-training Data)', '4-2 (Fine-tuning Data)', '4-3 (Reinforcement Learning Data)', '4-4 (Data Filtering)']
🔎 recall keys: ['4-1 (Pre-training Data)', '4-2 (Fine-tuning Data)', '4-3 (Reinforcement Learning Data)', '4-4 (Data Filtering)']
🔎 recall keys: ['4-1 (Pre-training Data)', '4-2 (Fine-tuning Data)', '4-3 (Reinforcement Learning Data)', '4-4 (Data Filtering)']
🔎 recall keys: ['4-1 (Pre-training Data)', '4-2 (Fine-tuning Data)', '4-3 (Reinforcement Learning Data)', '4-4 (Data Filtering)']
🔎 recall keys: ['4-1 (Pre-training Data)', '4-2 (Fine-tuning Data)', '4-3 (Reinforcement Learning Data)', '4-4 (Data Filtering)']
🔎 recall keys: ['4-1 (Pre-training Data)', '4-2 (Fine-tuning Data)', '4-3 (Reinforcement Learning Data)', '4-4 (Data Filtering)']
🔎 recall keys: ['4-1 (Pre-training Data)', '4-2 (Fine-tuning Data)', '4-3 (Reinforcement Learning Data)', '4-4 (Data Filtering)']
🔎 recall keys: ['4-1 (Pre-training Data)', '4-2 (Fine-tuning Data)', '4-3 (Reinforcement Learning Data)', '4-4 (Data Filtering)']
evidence counts before/after model-guard: {'raw': {'4-1 (Pre-training Data)': 12, '4-2 (Fine-tuning Data)': 9, '4-3 (Reinforcement Learning Data)': 4, '4-4 (Data Filtering)': 15}, 'kept': {'4-1 (Pre-training Data)': 12, '4-2 (Fine-tuning Data)': 9, '4-3 (Reinforcement Learning Data)': 4, '4-4 (Data Filtering)': 15}}
✅ Saved group 4 : meta-llama_llama-3.3-70b-instruct\arxiv_filtered_meta-llama_llama-3.3-70b-instruct_4.json
✅ Saved final merged: meta-llama_llama-3.3-70b-instruct\arxiv_filtered_final_meta-llama_llama-3.3-70b-instruct.json
evidence counts before/after model-guard: {'raw': {'1-1 (Weights)': 7, '1-2 (Code)': 5, '1-3 (License)': 0, '1-4 (Paper)': 6}, 'kept': {'1-1 (Weights)': 7, '1-2 (Code)': 5, '1-3 (License)': 0, '1-4 (Paper)': 6}}
✅ Saved group 1 : meta-llama_llama-3.3-70b-instruct\reports_filtered_meta-llama_llama-3.3-70b-instruct_1.json
evidence counts before/after model-guard: {'raw': {'1-5 (Architecture)': 0, '1-6 (Tokenizer)': 1, '2-1 (Hardware)': 1, '2-2 (Software)': 0}, 'kept': {'1-5 (Architecture)': 0, '1-6 (Tokenizer)': 1, '2-1 (Hardware)': 1, '2-2 (Software)': 0}}
✅ Saved group 2 : meta-llama_llama-3.3-70b-instruct\reports_filtered_meta-llama_llama-3.3-70b-instruct_2.json
evidence counts before/after model-guard: {'raw': {'2-3 (API)': 4, '3-1 (Pre-training)': 0, '3-2 (Fine-tuning)': 5, '3-3 (Reinforcement Learning)': 0}, 'kept': {'2-3 (API)': 4, '3-1 (Pre-training)': 0, '3-2 (Fine-tuning)': 5, '3-3 (Reinforcement Learning)': 0}}
✅ Saved group 3 : meta-llama_llama-3.3-70b-instruct\reports_filtered_meta-llama_llama-3.3-70b-instruct_3.json
evidence counts before/after model-guard: {'raw': {'4-1 (Pre-training Data)': 0, '4-2 (Fine-tuning Data)': 0, '4-3 (Reinforcement Learning Data)': 0, '4-4 (Data Filtering)': 0}, 'kept': {'4-1 (Pre-training Data)': 0, '4-2 (Fine-tuning Data)': 0, '4-3 (Reinforcement Learning Data)': 0, '4-4 (Data Filtering)': 0}}
✅ Saved group 4 : meta-llama_llama-3.3-70b-instruct\reports_filtered_meta-llama_llama-3.3-70b-instruct_4.json
✅ Saved final merged: meta-llama_llama-3.3-70b-instruct\reports_filtered_final_meta-llama_llama-3.3-70b-instruct.json
🔑 Hugging Face API says 401/403 — model may be private. Set HF_TOKEN in .env if you have access.
📝 Starting openness evaluation...
📝 Saved evaluation result: meta-llama_llama-3.3-70b-instruct\openness_score_meta-llama_llama-3.3-70b-instruct.json
✅ Openness evaluation complete. Result file: meta-llama_llama-3.3-70b-instruct\openness_score_meta-llama_llama-3.3-70b-instruct.json
✅ Saved model ID: meta-llama_llama-3.3-70b-instruct\identified_model.txt
⏳ **Time taken for this model: 3726.91 seconds**
🧾 Log saved to: meta-llama_llama-3.3-70b-instruct\run_20250913-180238_meta-llama_llama-3.3-70b-instruct.log
