{
  "model": "meta-llama/Llama-3.3-70B-Instruct",
  "scores": {
    "1-1 Weights": {
      "score": 1.0,
      "reason": "Hugging Face presence detected; forced Open per FORCE_HF_CORE_OPEN."
    },
    "1-2 Code": {
      "score": 0.0,
      "reason": "No training pipeline files; README mentions are ignored."
    },
    "1-3 License": {
      "score": 1.0,
      "reason": "The Llama 3.3 Community License allows use, modification, redistribution and commercial use; the only extra restriction targets entities with >700 M MAU, which is treated as ‘open’ per rubric."
    },
    "1-4 Paper": {
      "score": 1.0,
      "reason": "An official provider-authored Llama 3 paper is referenced for further technical and safety details."
    },
    "1-5 Architecture": {
      "score": 1.0,
      "reason": "Hugging Face presence detected; forced Open per FORCE_HF_CORE_OPEN."
    },
    "1-6 Tokenizer": {
      "score": 1.0,
      "reason": "Hugging Face presence detected; forced Open per FORCE_HF_CORE_OPEN."
    },
    "2-1 Hardware": {
      "score": 1.0,
      "reason": "Quotes reveal both type (H100-80 GB) and quantity/compute (39.3 M GPU-hours; table indicates 2 040 GPUs)."
    },
    "2-2 Software": {
      "score": 0.0,
      "reason": "Only the generic phrase “custom training libraries … production infrastructure” is quoted—no concrete stack components or versions, so counts as closed."
    },
    "2-3 API": {
      "score": 1.0,
      "reason": "Official API docs in quotes: https://llama.developer.meta.com/join_waitlist, https://llama.developer.meta.com/join_waitlist  No qualifying official API docs via web search."
    },
    "3-1 Pre-training": {
      "score": 0.5,
      "reason": "Some method details (tokens, cut-off date, GPU hours, GQA) are given, but not a fully reproducible recipe."
    },
    "3-2 Fine-tuning": {
      "score": 0.5,
      "reason": "Sources (public instruction sets + 25 M synthetic) and objectives are described, but the full fine-tuning pipeline is not released."
    },
    "3-3 Reinforcement Learning": {
      "score": 0.5,
      "reason": "RLHF is confirmed, yet no full algorithmic or hyper-parameter description is supplied."
    },
    "4-1 Pre-training Data": {
      "score": 0.5,
      "reason": "High-level source categories, size (15 T tokens) and language mix adjustments are disclosed, but not the complete dataset list or access links."
    },
    "4-2 Fine-tuning Data": {
      "score": 0.5,
      "reason": "Mentions public instruction sets, human-annotated prompts and >25 M synthetic examples, but no full dataset release."
    },
    "4-3 Reinforcement Learning Data": {
      "score": 0.5,
      "reason": "Processes and scale (multi-turn comparisons, millions of judgements) are outlined, yet the data itself is not shared."
    },
    "4-4 Data Filtering": {
      "score": 1.0,
      "reason": "Concrete multi-stage filtering pipeline is described: Llama/DistilRoBERTa classifiers, top-quartile RM threshold, model-as-judge 0/1 scoring, Instag difficulty, semantic dedup, etc.—meets ‘Open’ criteria."
    }
  },
  "included_scores": {
    "1-1 Weights": {
      "score": 1.0,
      "reason": "Hugging Face presence detected; forced Open per FORCE_HF_CORE_OPEN."
    },
    "1-2 Code": {
      "score": 0.0,
      "reason": "No training pipeline files; README mentions are ignored."
    },
    "1-3 License": {
      "score": 1.0,
      "reason": "The Llama 3.3 Community License allows use, modification, redistribution and commercial use; the only extra restriction targets entities with >700 M MAU, which is treated as ‘open’ per rubric."
    },
    "1-4 Paper": {
      "score": 1.0,
      "reason": "An official provider-authored Llama 3 paper is referenced for further technical and safety details."
    },
    "1-5 Architecture": {
      "score": 1.0,
      "reason": "Hugging Face presence detected; forced Open per FORCE_HF_CORE_OPEN."
    },
    "1-6 Tokenizer": {
      "score": 1.0,
      "reason": "Hugging Face presence detected; forced Open per FORCE_HF_CORE_OPEN."
    },
    "2-1 Hardware": {
      "score": 1.0,
      "reason": "Quotes reveal both type (H100-80 GB) and quantity/compute (39.3 M GPU-hours; table indicates 2 040 GPUs)."
    },
    "2-2 Software": {
      "score": 0.0,
      "reason": "Only the generic phrase “custom training libraries … production infrastructure” is quoted—no concrete stack components or versions, so counts as closed."
    },
    "2-3 API": {
      "score": 1.0,
      "reason": "Official API docs in quotes: https://llama.developer.meta.com/join_waitlist, https://llama.developer.meta.com/join_waitlist  No qualifying official API docs via web search."
    },
    "3-1 Pre-training": {
      "score": 0.5,
      "reason": "Some method details (tokens, cut-off date, GPU hours, GQA) are given, but not a fully reproducible recipe."
    },
    "3-2 Fine-tuning": {
      "score": 0.5,
      "reason": "Sources (public instruction sets + 25 M synthetic) and objectives are described, but the full fine-tuning pipeline is not released."
    },
    "3-3 Reinforcement Learning": {
      "score": 0.5,
      "reason": "RLHF is confirmed, yet no full algorithmic or hyper-parameter description is supplied."
    },
    "4-1 Pre-training Data": {
      "score": 0.5,
      "reason": "High-level source categories, size (15 T tokens) and language mix adjustments are disclosed, but not the complete dataset list or access links."
    },
    "4-2 Fine-tuning Data": {
      "score": 0.5,
      "reason": "Mentions public instruction sets, human-annotated prompts and >25 M synthetic examples, but no full dataset release."
    },
    "4-3 Reinforcement Learning Data": {
      "score": 0.5,
      "reason": "Processes and scale (multi-turn comparisons, millions of judgements) are outlined, yet the data itself is not shared."
    },
    "4-4 Data Filtering": {
      "score": 1.0,
      "reason": "Concrete multi-stage filtering pipeline is described: Llama/DistilRoBERTa classifiers, top-quartile RM threshold, model-as-judge 0/1 scoring, Instag difficulty, semantic dedup, etc.—meets ‘Open’ criteria."
    }
  },
  "final_score_10pt": 6.875,
  "meta": {
    "usage_from_dispatch": {
      "fine_tuning": "used",
      "rl": "used"
    },
    "excluded": [],
    "denominator": 16,
    "raw_sum": 11.0,
    "scale": "10/16",
    "code_detection_reason": "No training pipeline files; README mentions are ignored.",
    "pretrain_sources_used": false
  }
}