{
    "repo": "mistralai/mistral-inference",
    "branch": "main",
    "files": [
        ".github/ISSUE_TEMPLATE/bug_report.yml",
        ".github/ISSUE_TEMPLATE/config.yml",
        ".gitignore",
        "LICENSE",
        "README.md",
        "assets/smoe.png",
        "deploy/.dockerignore",
        "deploy/Dockerfile",
        "deploy/entrypoint.sh",
        "poetry.lock",
        "pyproject.toml",
        "src/mistral_inference/__init__.py",
        "src/mistral_inference/args.py",
        "src/mistral_inference/cache.py",
        "src/mistral_inference/generate.py",
        "src/mistral_inference/lora.py",
        "src/mistral_inference/main.py",
        "src/mistral_inference/mamba.py",
        "src/mistral_inference/model.py",
        "src/mistral_inference/moe.py",
        "src/mistral_inference/rope.py",
        "src/mistral_inference/transformer.py",
        "src/mistral_inference/transformer_layers.py",
        "src/mistral_inference/vision_encoder.py",
        "tests/test_generate.py",
        "tutorials/classifier.ipynb",
        "tutorials/getting_started.ipynb"
    ],
    "license_files": {
        "LICENSE": "                                 Apache License\n                           Version 2.0, January 2004\n                        http://www.apache.org/licenses/\n\n   TERMS AND CONDITIONS FOR USE, REPRODUCTION, AND DISTRIBUTION\n\n   1. Definitions.\n\n      \"License\" shall mean the terms and conditions for use, reproduction,\n      and distribution as defined by Sections 1 through 9 of this document.\n\n      \"Licensor\" shall mean the copyright owner or entity authorized by\n      the copyright owner that is granting the License.\n\n      \"Legal Entity\" shall mean the union of the acting entity and all\n      other entities that control, are controlled by, or are under common\n      control with that entity. For the purposes of this definition,\n      \"control\" means (i) the power, direct or indirect, to cause the\n      direction or management of such entity, whether by contract or\n      otherwise, or (ii) ownership of fifty percent (50%) or more of the\n      outstanding shares, or (iii) beneficial ownership of such entity.\n\n      \"You\" (or \"Your\") shall mean an individual or Legal Entity\n      exercising permissions granted by this License.\n\n      \"Source\" form shall mean the preferred form for making modifications,\n      including but not limited to software source code, documentation\n      source, and configuration files.\n\n      \"Object\" form shall mean any form resulting from mechanical\n      transformation or translation of a Source form, including but\n      not limited to compiled object code, generated documentation,\n      and conversions to other media types.\n\n      \"Work\" shall mean the work of authorship, whether in Source or\n      Object form, made available under the License, as indicated by a\n      copyright notice that is included in or attached to the work\n      (an example is provided in the Appendix below).\n\n      \"Derivative Works\" shall mean any work, whether in Source or Object\n      form, that is based on (or derived from) the Work and for which the\n      editorial revisions, annotations, elaborations, or other modifications\n      represent, as a whole, an original work of authorship. For the purposes\n      of this License, Derivative Works shall not include works that remain\n      separable from, or merely link (or bind by name) to the interfaces of,\n      the Work and Derivative Works thereof.\n\n      \"Contribution\" shall mean any work of authorship, including\n      the original version of the Work and any modifications or additions\n      to that Work or Derivative Works thereof, that is intentionally\n      submitted to Licensor for inclusion in the Work by the copyright owner\n      or by an individual or Legal Entity authorized to submit on behalf of\n      the copyright owner. For the purposes of this definition, \"submitted\"\n      means any form of electronic, verbal, or written communication sent\n      to the Licensor or its representatives, including but not limited to\n      communication on electronic mailing lists, source code control systems,\n      and issue tracking systems that are managed by, or on behalf of, the\n      Licensor for the purpose of discussing and improving the Work, but\n      excluding communication that is conspicuously marked or otherwise\n      designated in writing by the copyright owner as \"Not a Contribution.\"\n\n      \"Contributor\" shall mean Licensor and any individual or Legal Entity\n      on behalf of whom a Contribution has been received by Licensor and\n      subsequently incorporated within the Work.\n\n   2. Grant of Copyright License. Subject to the terms and conditions of\n      this License, each Contributor hereby grants to You a perpetual,\n      worldwide, non-exclusive, no-charge, royalty-free, irrevocable\n      copyright license to reproduce, prepare Derivative Works of,\n      publicly display, publicly perform, sublicense, and distribute the\n      Work and such Derivative Works in Source or Object form.\n\n   3. Grant of Patent License. Subject to the terms and conditions of\n      this License, each Contributor hereby grants to You a perpetual,\n      worldwide, non-exclusive, no-charge, royalty-free, irrevocable\n      (except as stated in this section) patent license to make, have made,\n      use, offer to sell, sell, import, and otherwise transfer the Work,\n      where such license applies only to those patent claims licensable\n      by such Contributor that are necessarily infringed by their\n      Contribution(s) alone or by combination of their Contribution(s)\n      with the Work to which such Contribution(s) was submitted. If You\n      institute patent litigation against any entity (including a\n      cross-claim or counterclaim in a lawsuit) alleging that the Work\n      or a Contribution incorporated within the Work constitutes direct\n      or contributory patent infringement, then any patent licenses\n      granted to You under this License for that Work shall terminate\n      as of the date such litigation is filed.\n\n   4. Redistribution. You may reproduce and distribute copies of the\n      Work or Derivative Works thereof in any medium, with or without\n      modifications, and in Source or Object form, provided that You\n      meet the following conditions:\n\n      (a) You must give any other recipients of the Work or\n          Derivative Works a copy of this License; and\n\n      (b) You must cause any modified files to carry prominent notices\n          stating that You changed the files; and\n\n      (c) You must retain, in the Source form of any Derivative Works\n          that You distribute, all copyright, patent, trademark, and\n          attribution notices from the Source form of the Work,\n          excluding those notices that do not pertain to any part of\n          the Derivative Works; and\n\n      (d) If the Work includes a \"NOTICE\" text file as part of its\n          distribution, then any Derivative Works that You distribute must\n          include a readable copy of the attribution notices contained\n          within such NOTICE file, excluding those notices that do not\n          pertain to any part of the Derivative Works, in at least one\n          of the following places: within a NOTICE text file distributed\n          as part of the Derivative Works; within the Source form or\n          documentation, if provided along with the Derivative Works; or,\n          within a display generated by the Derivative Works, if and\n          wherever such third-party notices normally appear. The contents\n          of the NOTICE file are for informational purposes only and\n          do not modify the License. You may add Your own attribution\n          notices within Derivative Works that You distribute, alongside\n          or as an addendum to the NOTICE text from the Work, provided\n          that such additional attribution notices cannot be construed\n          as modifying the License.\n\n      You may add Your own copyright statement to Your modifications and\n      may provide additional or different license terms and conditions\n      for use, reproduction, or distribution of Your modifications, or\n      for any such Derivative Works as a whole, provided Your use,\n      reproduction, and distribution of the Work otherwise complies with\n      the conditions stated in this License.\n\n   5. Submission of Contributions. Unless You explicitly state otherwise,\n      any Contribution intentionally submitted for inclusion in the Work\n      by You to the Licensor shall be under the terms and conditions of\n      this License, without any additional terms or conditions.\n      Notwithstanding the above, nothing herein shall supersede or modify\n      the terms of any separate license agreement you may have executed\n      with Licensor regarding such Contributions.\n\n   6. Trademarks. This License does not grant permission to use the trade\n      names, trademarks, service marks, or product names of the Licensor,\n      except as required for reasonable and customary use in describing the\n      origin of the Work and reproducing the content of the NOTICE file.\n\n   7. Disclaimer of Warranty. Unless required by applicable law or\n      agreed to in writing, Licensor provides the Work (and each\n      Contributor provides its Contributions) on an \"AS IS\" BASIS,\n      WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or\n      implied, including, without limitation, any warranties or conditions\n      of TITLE, NON-INFRINGEMENT, MERCHANTABILITY, or FITNESS FOR A\n      PARTICULAR PURPOSE. You are solely responsible for determining the\n      appropriateness of using or redistributing the Work and assume any\n      risks associated with Your exercise of permissions under this License.\n\n   8. Limitation of Liability. In no event and under no legal theory,\n      whether in tort (including negligence), contract, or otherwise,\n      unless required by applicable law (such as deliberate and grossly\n      negligent acts) or agreed to in writing, shall any Contributor be\n      liable to You for damages, including any direct, indirect, special,\n      incidental, or consequential damages of any character arising as a\n      result of this License or out of the use or inability to use the\n      Work (including but not limited to damages for loss of goodwill,\n      work stoppage, computer failure or malfunction, or any and all\n      other commercial damages or losses), even if such Contributor\n      has been advised of the possibility of such damages.\n\n   9. Accepting Warranty or Additional Liability. While redistributing\n      the Work or Derivative Works thereof, You may choose to offer,\n      and charge a fee for, acceptance of support, warranty, indemnity,\n      or other liability obligations and/or rights consistent with this\n      License. However, in accepting such obligations, You may act only\n      on Your own behalf and on Your sole responsibility, not on behalf\n      of any other Contributor, and only if You agree to indemnify,\n      defend, and hold each Contributor harmless for any liability\n      incurred by, or claims asserted against, such Contributor by reason\n      of your accepting any such warranty or additional liability.\n\n   END OF TERMS AND CONDITIONS\n\n   APPENDIX: How to apply the Apache License to your work.\n\n      To apply the Apache License to your work, attach the following\n      boilerplate notice, with the fields enclosed by brackets \"[]\"\n      replaced with your own identifying information. (Don't include\n      the brackets!)  The text should be enclosed in the appropriate\n      comment syntax for the file format. We also recommend that a\n      file or class name and description of purpose be included on the\n      same \"printed page\" as the copyright notice for easier\n      identification within third-party archives.\n\n   Copyright [yyyy] [name of copyright owner]\n\n   Licensed under the Apache License, Version 2.0 (the \"License\");\n   you may not use this file except in compliance with the License.\n   You may obtain a copy of the License at\n\n       http://www.apache.org/licenses/LICENSE-2.0\n\n   Unless required by applicable law or agreed to in writing, software\n   distributed under the License is distributed on an \"AS IS\" BASIS,\n   WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n   See the License for the specific language governing permissions and\n   limitations under the License.\n"
    },
    "readme": "# Mistral Inference\n<a target=\"_blank\" href=\"https://colab.research.google.com/github/mistralai/mistral-inference/blob/main/tutorials/getting_started.ipynb\">\n  <img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/>\n</a>\n\n\nThis repository contains minimal code to run Mistral models.\n\nBlog 7B: [https://mistral.ai/news/announcing-mistral-7b/](https://mistral.ai/news/announcing-mistral-7b/)\\\nBlog 8x7B: [https://mistral.ai/news/mixtral-of-experts/](https://mistral.ai/news/mixtral-of-experts/)\\\nBlog 8x22B: [https://mistral.ai/news/mixtral-8x22b/](https://mistral.ai/news/mixtral-8x22b/)\\\nBlog Codestral 22B: [https://mistral.ai/news/codestral](https://mistral.ai/news/codestral/) \\\nBlog Codestral Mamba 7B: [https://mistral.ai/news/codestral-mamba/](https://mistral.ai/news/codestral-mamba/) \\\nBlog Mathstral 7B: [https://mistral.ai/news/mathstral/](https://mistral.ai/news/mathstral/) \\\nBlog Nemo: [https://mistral.ai/news/mistral-nemo/](https://mistral.ai/news/mistral-nemo/) \\\nBlog Mistral Large 2: [https://mistral.ai/news/mistral-large-2407/](https://mistral.ai/news/mistral-large-2407/) \\\nBlog Pixtral 12B: [https://mistral.ai/news/pixtral-12b/](https://mistral.ai/news/pixtral-12b/)\nBlog Mistral Small 3.1: [https://mistral.ai/news/mistral-small-3-1/](https://mistral.ai/news/mistral-small-3-1/)\n\nDiscord: [https://discord.com/invite/mistralai](https://discord.com/invite/mistralai)\\\nDocumentation: [https://docs.mistral.ai/](https://docs.mistral.ai/)\\\nGuardrailing: [https://docs.mistral.ai/usage/guardrailing](https://docs.mistral.ai/usage/guardrailing)\n\n## Installation\n\nNote: You will use a GPU to install `mistral-inference`, as it currently requires `xformers` to be installed and `xformers` itself needs a GPU for installation.\n\n### PyPI\n\n```\npip install mistral-inference\n```\n\n### Local\n\n```\ncd $HOME && git clone https://github.com/mistralai/mistral-inference\ncd $HOME/mistral-inference && poetry install .\n```\n\n## Model download\n\n### Direct links\n\n| Name        | Download | md5sum |\n|-------------|-------|-------|\n| 7B Instruct | https://models.mistralcdn.com/mistral-7b-v0-3/mistral-7B-Instruct-v0.3.tar | `80b71fcb6416085bcb4efad86dfb4d52` |\n| 8x7B Instruct | https://models.mistralcdn.com/mixtral-8x7b-v0-1/Mixtral-8x7B-v0.1-Instruct.tar (**Updated model coming soon!**) | `8e2d3930145dc43d3084396f49d38a3f` |\n| 8x22 Instruct | https://models.mistralcdn.com/mixtral-8x22b-v0-3/mixtral-8x22B-Instruct-v0.3.tar | `471a02a6902706a2f1e44a693813855b` |\n| 7B Base | https://models.mistralcdn.com/mistral-7b-v0-3/mistral-7B-v0.3.tar | `0663b293810d7571dad25dae2f2a5806` |\n| 8x7B |     **Updated model coming soon!**       | - |\n| 8x22B | https://models.mistralcdn.com/mixtral-8x22b-v0-3/mixtral-8x22B-v0.3.tar | `a2fa75117174f87d1197e3a4eb50371a` |\n| Codestral 22B | https://models.mistralcdn.com/codestral-22b-v0-1/codestral-22B-v0.1.tar | `1ea95d474a1d374b1d1b20a8e0159de3` |\n| Mathstral 7B | https://models.mistralcdn.com/mathstral-7b-v0-1/mathstral-7B-v0.1.tar | `5f05443e94489c261462794b1016f10b` |\n| Codestral-Mamba 7B | https://models.mistralcdn.com/codestral-mamba-7b-v0-1/codestral-mamba-7B-v0.1.tar | `d3993e4024d1395910c55db0d11db163` |\n| Nemo Base | https://models.mistralcdn.com/mistral-nemo-2407/mistral-nemo-base-2407.tar | `c5d079ac4b55fc1ae35f51f0a3c0eb83` |\n| Nemo Instruct | https://models.mistralcdn.com/mistral-nemo-2407/mistral-nemo-instruct-2407.tar | `296fbdf911cb88e6f0be74cd04827fe7` |\n| Mistral Large 2 | https://models.mistralcdn.com/mistral-large-2407/mistral-large-instruct-2407.tar | `fc602155f9e39151fba81fcaab2fa7c4` |\n\nNote:\n- **Important**:\n  - `mixtral-8x22B-Instruct-v0.3.tar` is exactly the same as [Mixtral-8x22B-Instruct-v0.1](https://huggingface.co/mistralai/Mixtral-8x22B-Instruct-v0.1), only stored in `.safetensors` format\n  - `mixtral-8x22B-v0.3.tar` is the same as [Mixtral-8x22B-v0.1](https://huggingface.co/mistralai/Mixtral-8x22B-v0.1), but has an extended vocabulary of 32768 tokens.\n  - `codestral-22B-v0.1.tar` has a custom non-commercial license, called [Mistral AI Non-Production (MNPL) License](https://mistral.ai/licenses/MNPL-0.1.md)\n  - `mistral-large-instruct-2407.tar` has a custom non-commercial license, called [Mistral AI Research (MRL) License](https://mistral.ai/licenses/MRL-0.1.md)\n- All of the listed models above support function calling. For example, Mistral 7B Base/Instruct v3 is a minor update to Mistral 7B Base/Instruct v2,  with the addition of function calling capabilities.\n- The \"coming soon\" models will include function calling as well.\n- You can download the previous versions of our models from our [docs](https://docs.mistral.ai/getting-started/open_weight_models/#downloading).\n\n### From Hugging Face Hub\n\n| Name        | ID | URL |\n|-------------|-------|-------|\n| Pixtral Large Instruct | mistralai/Pixtral-Large-Instruct-2411 | https://huggingface.co/mistralai/Pixtral-Large-Instruct-2411 |\n| Pixtral 12B Base | mistralai/Pixtral-12B-Base-2409 | https://huggingface.co/mistralai/Pixtral-12B-Base-2409 |\n| Pixtral 12B | mistralai/Pixtral-12B-2409 | https://huggingface.co/mistralai/Pixtral-12B-2409 |\n| Mistral Small 3.1 24B Base | mistralai/Mistral-Small-3.1-24B-Base-2503 | https://huggingface.co/mistralai/Mistral-Small-3.1-24B-Base-2503\n| Mistral Small 3.1 24B Instruct | mistralai/Mistral-Small-3.1-24B-Instruct-2503 | https://huggingface.co/mistralai/Mistral-Small-3.1-24B-Instruct-2503 |\n\n\n### Usage\n\n**News!!!**: Mistral Large 2 is out. Read more about its capabilities [here](https://mistral.ai/news/mistral-large-2407/).\n\nCreate a local folder to store models\n```sh\nexport MISTRAL_MODEL=$HOME/mistral_models\nmkdir -p $MISTRAL_MODEL\n```\n\nDownload any of the above links and extract the content, *e.g.*:\n\n```sh\nexport 12B_DIR=$MISTRAL_MODEL/12B_Nemo\nwget https://models.mistralcdn.com/mistral-nemo-2407/mistral-nemo-instruct-2407.tar\nmkdir -p $12B_DIR\ntar -xf mistral-nemo-instruct-2407.tar -C $12B_DIR\n```\n\nor\n\n```sh\nexport M8x7B_DIR=$MISTRAL_MODEL/8x7b_instruct\nwget https://models.mistralcdn.com/mixtral-8x7b-v0-1/Mixtral-8x7B-v0.1-Instruct.tar\nmkdir -p $M8x7B_DIR\ntar -xf Mixtral-8x7B-v0.1-Instruct.tar -C $M8x7B_DIR\n```\n\nFor Hugging Face models' weights, here is an example to download [Mistral Small 3.1 24B Instruct](https://huggingface.co/mistralai/Mistral-Small-3.1-24B-Instruct-2503):\n\n```python\nfrom pathlib import Path\nfrom huggingface_hub import snapshot_download\n\n\nmistral_models_path = Path.home().joinpath(\"mistral_models\")\n\nmodel_path = mistral_models_path / \"mistral-small-3.1-instruct\"\nmodel_path.mkdir(parents=True, exist_ok=True)\n\nrepo_id = \"mistralai/Mistral-Small-3.1-24B-Instruct-2503\"\n\nsnapshot_download(\n    repo_id=repo_id,\n    allow_patterns=[\"params.json\", \"consolidated.safetensors\", \"tekken.json\"],\n    local_dir=model_path,\n)\n```\n\n## Usage\n\nThe following sections give an overview of how to run the model from the Command-line interface (CLI) or directly within Python.\n\n### CLI\n\n- **Demo**\n\nTo test that a model works in your setup, you can run the `mistral-demo` command.\n*E.g.* the 12B Mistral-Nemo model can be tested on a single GPU as follows:\n\n```sh\nmistral-demo $12B_DIR\n```\n\nLarge models, such **8x7B** and **8x22B** have to be run in a multi-GPU setup.\nFor these models, you can use the following command:\n\n```sh\ntorchrun --nproc-per-node 2 --no-python mistral-demo $M8x7B_DIR\n```\n\n*Note*: Change `--nproc-per-node` to more GPUs if available.\n\n- **Chat**\n\nTo interactively chat with the models, you can make use of the `mistral-chat` command.\n\n```sh\nmistral-chat $12B_DIR --instruct --max_tokens 1024 --temperature 0.35\n```\n\nFor large models, you can make use of `torchrun`.\n\n```sh\ntorchrun --nproc-per-node 2 --no-python mistral-chat $M8x7B_DIR --instruct\n```\n\n*Note*: Change `--nproc-per-node` to more GPUs if necessary (*e.g.* for 8x22B).\n\n- **Chat with Codestral**\n\nTo use [Codestral](https://mistral.ai/news/codestral/) as a coding assistant you can run the following command using `mistral-chat`.\nMake sure `$M22B_CODESTRAL` is set to a valid path to the downloaded codestral folder, e.g. `$HOME/mistral_models/Codestral-22B-v0.1`\n\n```sh\nmistral-chat $M22B_CODESTRAL --instruct --max_tokens 256\n```\n\nIf you prompt it with *\"Write me a function that computes fibonacci in Rust\"*, the model should generate something along the following lines:\n\n```sh\nSure, here's a simple implementation of a function that computes the Fibonacci sequence in Rust. This function takes an integer `n` as an argument and returns the `n`th Fibonacci number.\n\nfn fibonacci(n: u32) -> u32 {\n    match n {\n        0 => 0,\n        1 => 1,\n        _ => fibonacci(n - 1) + fibonacci(n - 2),\n    }\n}\n\nfn main() {\n    let n = 10;\n    println!(\"The {}th Fibonacci number is: {}\", n, fibonacci(n));\n}\n\nThis function uses recursion to calculate the Fibonacci number. However, it's not the most efficient solution because it performs a lot of redundant calculations. A more efficient solution would use a loop to iteratively calculate the Fibonacci numbers.\n```\n\nYou can continue chatting afterwards, *e.g.* with *\"Translate it to Python\"*.\n\n- **Chat with Codestral-Mamba**\n\nTo use [Codestral-Mamba](https://mistral.ai/news/codestral-mamba/) as a coding assistant you can run the following command using `mistral-chat`.\nMake sure `$7B_CODESTRAL_MAMBA` is set to a valid path to the downloaded codestral-mamba folder, e.g. `$HOME/mistral_models/mamba-codestral-7B-v0.1`.\n\nYou then need to additionally install the following packages:\n\n```\npip install packaging mamba-ssm causal-conv1d transformers\n```\n\nbefore you can start chatting:\n\n```sh\nmistral-chat $7B_CODESTRAL_MAMBA --instruct --max_tokens 256\n```\n\n- **Chat with Mathstral**\n\nTo use [Mathstral](https://mistral.ai/news/mathstral/) as an assistant you can run the following command using `mistral-chat`.\nMake sure `$7B_MATHSTRAL` is set to a valid path to the downloaded codestral folder, e.g. `$HOME/mistral_models/mathstral-7B-v0.1`\n\n```sh\nmistral-chat $7B_MATHSTRAL --instruct --max_tokens 256\n```\n\nIf you prompt it with *\"Albert likes to surf every week. Each surfing session lasts for 4 hours and costs $20 per hour. How much would Albert spend in 5 weeks?\"*, the model should answer with the correct calculation.\n\nYou can then continue chatting afterwards, *e.g.* with *\"How much would he spend in a year?\"*.\n\n- **Chat with Mistral Small 3.1 24B Instruct**\n\nTo use [Mistral Small 3.1 24B Instruct](https://mistral.ai/news/mistral-small-3-1/) as an assistant you can run the following command using `mistral-chat`.\nMake sure `$MISTRAL_SMALL_3_1_INSTRUCT` is set to a valid path to the downloaded mistral small folder, e.g. `$HOME/mistral_models/mistral-small-3.1-instruct`\n\n```sh\n    mistral-chat $MISTRAL_SMALL_3_1_INSTRUCT --instruct --max_tokens 256\n```\n\nIf you prompt it with *\"The above image presents an image of which park ? Please give the hints to identify the park.\"* with the following image URL *https://huggingface.co/datasets/patrickvonplaten/random_img/resolve/main/yosemite.png*, the model should answer with the Yosemite park and give hints to identify it.\n\nYou can then continue chatting afterwards, *e.g.* with *\"What is the name of the lake in the image?\"*. The model should respond that it is not a lake but a river.\n\n### Python\n\n- *Instruction Following*:\n\n```py\nfrom mistral_inference.transformer import Transformer\nfrom mistral_inference.generate import generate\n\nfrom mistral_common.tokens.tokenizers.mistral import MistralTokenizer\nfrom mistral_common.protocol.instruct.messages import UserMessage\nfrom mistral_common.protocol.instruct.request import ChatCompletionRequest\n\n\ntokenizer = MistralTokenizer.from_file(\"./mistral-nemo-instruct-v0.1/tekken.json\")  # change to extracted tokenizer file\nmodel = Transformer.from_folder(\"./mistral-nemo-instruct-v0.1\")  # change to extracted model dir\n\nprompt = \"How expensive would it be to ask a window cleaner to clean all windows in Paris. Make a reasonable guess in US Dollar.\"\n\ncompletion_request = ChatCompletionRequest(messages=[UserMessage(content=prompt)])\n\ntokens = tokenizer.encode_chat_completion(completion_request).tokens\n\nout_tokens, _ = generate([tokens], model, max_tokens=1024, temperature=0.35, eos_id=tokenizer.instruct_tokenizer.tokenizer.eos_id)\nresult = tokenizer.instruct_tokenizer.tokenizer.decode(out_tokens[0])\n\nprint(result)\n```\n\n- *Multimodal Instruction Following*:\n\n\n```python\nfrom pathlib import Path\n\nfrom huggingface_hub import snapshot_download\nfrom mistral_common.protocol.instruct.messages import ImageURLChunk, TextChunk\nfrom mistral_common.tokens.tokenizers.mistral import MistralTokenizer\nfrom mistral_inference.generate import generate\nfrom mistral_inference.transformer import Transformer\n\nmodel_path = Path.home().joinpath(\"mistral_models\") / \"mistral-small-3.1-instruct\" # change to extracted model\n\ntokenizer = MistralTokenizer.from_file(model_path / \"tekken.json\")\nmodel = Transformer.from_folder(model_path)\n\nurl = \"https://huggingface.co/datasets/patrickvonplaten/random_img/resolve/main/yosemite.png\"\nprompt = \"The above image presents an image of which park ? Please give the hints to identify the park.\"\n\nuser_content = [ImageURLChunk(image_url=url), TextChunk(text=prompt)]\n\ntokens, images = tokenizer.instruct_tokenizer.encode_user_content(user_content, False)\n\nout_tokens, _ = generate(\n    [tokens],\n    model,\n    images=[images],\n    max_tokens=256,\n    temperature=0.15,\n    eos_id=tokenizer.instruct_tokenizer.tokenizer.eos_id,\n)\nresult = tokenizer.decode(out_tokens[0])\n\nprint(\"Prompt:\", prompt)\nprint(\"Completion:\", result)\n```\n\n- *Function Calling*:\n\n```py\nfrom mistral_common.protocol.instruct.tool_calls import Function, Tool\n\ncompletion_request = ChatCompletionRequest(\n    tools=[\n        Tool(\n            function=Function(\n                name=\"get_current_weather\",\n                description=\"Get the current weather\",\n                parameters={\n                    \"type\": \"object\",\n                    \"properties\": {\n                        \"location\": {\n                            \"type\": \"string\",\n                            \"description\": \"The city and state, e.g. San Francisco, CA\",\n                        },\n                        \"format\": {\n                            \"type\": \"string\",\n                            \"enum\": [\"celsius\", \"fahrenheit\"],\n                            \"description\": \"The temperature unit to use. Infer this from the users location.\",\n                        },\n                    },\n                    \"required\": [\"location\", \"format\"],\n                },\n            )\n        )\n    ],\n    messages=[\n        UserMessage(content=\"What's the weather like today in Paris?\"),\n        ],\n)\n\ntokens = tokenizer.encode_chat_completion(completion_request).tokens\n\nout_tokens, _ = generate([tokens], model, max_tokens=64, temperature=0.0, eos_id=tokenizer.instruct_tokenizer.tokenizer.eos_id)\nresult = tokenizer.instruct_tokenizer.tokenizer.decode(out_tokens[0])\n\nprint(result)\n```\n\n- *Fill-in-the-middle (FIM)*:\n\nMake sure to have `mistral-common >= 1.2.0` installed:\n```\npip install --upgrade mistral-common\n```\n\nYou can simulate a code completion in-filling as follows.\n\n```py\nfrom mistral_inference.transformer import Transformer\nfrom mistral_inference.generate import generate\nfrom mistral_common.tokens.tokenizers.mistral import MistralTokenizer\nfrom mistral_common.tokens.instruct.request import FIMRequest\n\ntokenizer = MistralTokenizer.from_model(\"codestral-22b\")\nmodel = Transformer.from_folder(\"./mistral_22b_codestral\")\n\nprefix = \"\"\"def add(\"\"\"\nsuffix = \"\"\"    return sum\"\"\"\n\nrequest = FIMRequest(prompt=prefix, suffix=suffix)\n\ntokens = tokenizer.encode_fim(request).tokens\n\nout_tokens, _ = generate([tokens], model, max_tokens=256, temperature=0.0, eos_id=tokenizer.instruct_tokenizer.tokenizer.eos_id)\nresult = tokenizer.decode(out_tokens[0])\n\nmiddle = result.split(suffix)[0].strip()\nprint(middle)\n```\n\n### Test\n\nTo run logits equivalence:\n```\npython -m pytest tests\n```\n\n## Deployment\n\nThe `deploy` folder contains code to build a [vLLM](https://M7B_DIR.com/vllm-project/vllm) image with the required dependencies to serve the Mistral AI model. In the image, the [transformers](https://github.com/huggingface/transformers/) library is used instead of the reference implementation. To build it:\n\n```bash\ndocker build deploy --build-arg MAX_JOBS=8\n```\n\nInstructions to run the image can be found in the [official documentation](https://docs.mistral.ai/quickstart).\n\n\n## Model platforms\n\n- Use Mistral models on [Mistral AI official API](https://console.mistral.ai/) (La Plateforme)\n- Use Mistral models via [cloud providers](https://docs.mistral.ai/deployment/cloud/overview/)\n\n## References\n\n[1]: [LoRA](https://arxiv.org/abs/2106.09685): Low-Rank Adaptation of Large Language Models, Hu et al. 2021\n",
    "py_files": {
        "src/mistral_inference/__init__.py": "__version__ = \"1.6.0\"\n",
        "src/mistral_inference/args.py": "from dataclasses import dataclass\nfrom typing import List, Optional\n\nfrom simple_parsing.helpers import Serializable\n\nfrom mistral_inference.lora import LoraArgs\nfrom mistral_inference.moe import MoeArgs\n\nPATCH_MERGE = \"patch_merge\"\n\n\n@dataclass\nclass VisionEncoderArgs:\n    hidden_size: int\n    num_channels: int\n    image_size: int\n    patch_size: int\n    intermediate_size: int\n    num_hidden_layers: int\n    num_attention_heads: int\n    rope_theta: float = 1e4  # for rope-2D\n    image_token_id: int = 10\n    adapter_bias: bool = True\n    spatial_merge_size: int = 1\n    add_pre_mm_projector_layer_norm: bool = False\n    mm_projector_id: str = \"\"\n\n\n@dataclass\nclass TransformerArgs(Serializable):\n    dim: int\n    n_layers: int\n    head_dim: int\n    hidden_dim: int\n    n_heads: int\n    n_kv_heads: int\n    norm_eps: float\n    vocab_size: int\n\n    max_batch_size: int = 0\n\n    # For rotary embeddings. If not set, will be inferred\n    rope_theta: Optional[float] = None\n    # If this is set, we will use MoE layers instead of dense layers.\n    moe: Optional[MoeArgs] = None\n    # If this is set, we will load LoRA linear layers instead of linear layers.\n    lora: Optional[LoraArgs] = None\n    sliding_window: Optional[int] | Optional[List[int]] = None\n    _sliding_window: Optional[int] | Optional[List[int]] = None\n    model_type: str = \"transformer\"\n\n    vision_encoder: Optional[VisionEncoderArgs] = None\n\n    def __post_init__(self) -> None:\n        assert self.model_type == \"transformer\", self.model_type\n        assert self.sliding_window is None or self._sliding_window is None\n\n        # hack for now so that vLLM is supported correctly\n        self.sliding_window = self.sliding_window if self.sliding_window is not None else self._sliding_window\n\n\n@dataclass\nclass MambaArgs(Serializable):\n    dim: int\n    n_layers: int\n    vocab_size: int\n    n_groups: int\n    rms_norm: bool\n    residual_in_fp32: bool\n    fused_add_norm: bool\n    pad_vocab_size_multiple: int\n    tie_embeddings: bool\n    model_type: str = \"mamba\"\n\n    def __post_init__(self) -> None:\n        assert self.model_type == \"mamba\", self.model_type\n",
        "src/mistral_inference/cache.py": "from dataclasses import dataclass\nfrom typing import List, Optional, Tuple\n\nimport torch\nfrom xformers.ops.fmha.attn_bias import (  # type: ignore\n    AttentionBias,\n    BlockDiagonalCausalMask,\n    BlockDiagonalCausalWithOffsetPaddedKeysMask,\n    BlockDiagonalMask,\n)\n\n\ndef get_cache_sizes(n_layers: int, max_seq_len: int, sliding_window: Optional[int] | Optional[List[int]]) -> List[int]:\n    if sliding_window is None:\n        return n_layers * [max_seq_len]\n    elif isinstance(sliding_window, int):\n        return n_layers * [sliding_window]\n    else:\n        assert isinstance(sliding_window, list), f\"Expected list, got {type(sliding_window)}\"\n        assert (\n            n_layers % len(sliding_window) == 0\n        ), f\"Expected n_layers % len(sliding_window) == 0, got {n_layers} % {len(sliding_window)}\"\n        num_repeats = n_layers // len(sliding_window)\n        return num_repeats * [w if w is not None else max_seq_len for w in sliding_window]\n\n\n@dataclass\nclass CacheInputMetadata:\n    # # rope absolute positions\n    # positions: torch.Tensor\n    # # where tokens should go in the cache\n    # cache_positions: torch.Tensor\n\n    # # if prefill, use block diagonal causal mask\n    # # else use causal with padded key mask\n    # prefill: bool\n    # mask: AttentionBias\n    # seqlens: List[int]\n    # rope absolute positions\n    positions: torch.Tensor\n    # which elements in the sequences need to be cached\n    to_cache_mask: torch.Tensor\n    # how many elements are cached per sequence\n    cached_elements: torch.Tensor\n    # where tokens should go in the cache\n    cache_positions: torch.Tensor\n    # if prefill, use block diagonal causal mask\n    # else use causal with padded key mask\n    prefill: bool\n    mask: AttentionBias\n    seqlens: List[int]\n\n\ndef interleave_list(l1: List[torch.Tensor], l2: List[torch.Tensor]) -> List[torch.Tensor]:\n    assert len(l1) == len(l2)\n    return [v for pair in zip(l1, l2) for v in pair]\n\n\ndef unrotate(cache: torch.Tensor, seqlen: int) -> torch.Tensor:\n    assert cache.ndim == 3  # (W, H, D)\n    position = seqlen % cache.shape[0]\n    if seqlen < cache.shape[0]:\n        return cache[:seqlen]\n    elif position == 0:\n        return cache\n    else:\n        return torch.cat([cache[position:], cache[:position]], dim=0)\n\n\nclass CacheView:\n    def __init__(\n        self,\n        cache_k: torch.Tensor,\n        cache_v: torch.Tensor,\n        metadata: CacheInputMetadata,\n        kv_seqlens: torch.Tensor,\n    ):\n        self.cache_k = cache_k\n        self.cache_v = cache_v\n        self.kv_seqlens = kv_seqlens\n        self.metadata = metadata\n\n    def update(self, xk: torch.Tensor, xv: torch.Tensor) -> None:\n        \"\"\"\n        to_cache_mask masks the last [max_seq_len] tokens in each sequence\n        \"\"\"\n        n_kv_heads, head_dim = self.cache_k.shape[-2:]\n        flat_cache_k = self.cache_k.view(-1, n_kv_heads, head_dim)\n        flat_cache_v = self.cache_v.view(-1, n_kv_heads, head_dim)\n\n        flat_cache_k.index_copy_(0, self.metadata.cache_positions, xk[self.metadata.to_cache_mask])\n        flat_cache_v.index_copy_(0, self.metadata.cache_positions, xv[self.metadata.to_cache_mask])\n\n    def interleave_kv(self, xk: torch.Tensor, xv: torch.Tensor) -> Tuple[torch.Tensor, torch.Tensor]:\n        \"\"\"\n        This is a naive implementation and not optimized for speed.\n        \"\"\"\n        assert xk.ndim == xv.ndim == 3  # (B * T, H, D)\n        assert xk.shape == xv.shape\n\n        if all([s == 0 for s in self.metadata.seqlens]):\n            # No cache to interleave\n            return xk, xv\n\n        # Make it a list of [(T, H, D)]\n        xk: Tuple[torch.Tensor] = torch.split(xk, self.metadata.seqlens)  # type: ignore\n        xv: Tuple[torch.Tensor] = torch.split(xv, self.metadata.seqlens)  # type: ignore\n        assert len(xk) == len(self.kv_seqlens), f\"Batch size is {len(self.kv_seqlens)}, got {len(xk)}\"\n\n        # Order elements in cache by position by unrotating\n        cache_k = [unrotate(t, s) for t, s in zip(self.cache_k, self.kv_seqlens)]\n        cache_v = [unrotate(t, s) for t, s in zip(self.cache_v, self.kv_seqlens)]\n\n        interleaved_k = interleave_list(cache_k, list(xk))\n        interleaved_v = interleave_list(cache_v, list(xv))\n\n        return torch.cat(interleaved_k, dim=0), torch.cat(interleaved_v, dim=0)\n\n    @property\n    def max_seq_len(self) -> int:\n        return self.cache_k.shape[1]\n\n    @property\n    def key(self) -> torch.Tensor:\n        return self.cache_k[: len(self.kv_seqlens)]\n\n    @property\n    def value(self) -> torch.Tensor:\n        return self.cache_v[: len(self.kv_seqlens)]\n\n    @property\n    def prefill(self) -> bool:\n        return self.metadata.prefill\n\n    @property\n    def mask(self) -> AttentionBias:\n        return self.metadata.mask\n\n\nclass BufferCache:\n    \"\"\"\n    This is an example that implements a buffer cache, allowing for variable length sequences.\n    Allocated cache is rectangular which is wasteful (see PagedAttention for better mechanisms)\n    \"\"\"\n\n    def __init__(\n        self,\n        n_layers: int,\n        max_batch_size: int,\n        max_seq_len: int,\n        n_kv_heads: int,\n        head_dim: int,\n        sliding_window: Optional[int] | Optional[List[int]] = None,\n    ):\n        self.max_seq_len = max_seq_len\n        self.n_kv_heads = n_kv_heads\n        self.head_dim = head_dim\n        self.n_layers = n_layers\n\n        self.cache_sizes: List[int] = get_cache_sizes(n_layers, max_seq_len, sliding_window)\n        assert len(self.cache_sizes) == n_layers, f\"Expected {n_layers} cache sizes, got {len(self.cache_sizes)}\"\n\n        self.cache_k = {}\n        self.cache_v = {}\n        for i, cache_size in enumerate(self.cache_sizes):\n            self.cache_k[i] = torch.empty((max_batch_size, cache_size, n_kv_heads, head_dim))\n            self.cache_v[i] = torch.empty((max_batch_size, cache_size, n_kv_heads, head_dim))\n\n        # holds the valid length for each batch element in the cache\n        self.kv_seqlens: Optional[torch.Tensor] = None\n\n    def get_view(self, layer_id: int, metadata: CacheInputMetadata) -> CacheView:\n        assert self.kv_seqlens is not None\n        return CacheView(self.cache_k[layer_id], self.cache_v[layer_id], metadata, self.kv_seqlens)\n\n    def reset(self) -> None:\n        self.kv_seqlens = None\n\n    def init_kvseqlens(self, batch_size: int) -> None:\n        self.kv_seqlens = torch.zeros((batch_size,), device=self.device, dtype=torch.long)\n\n    @property\n    def device(self) -> torch.device:\n        return self.cache_k[0].device\n\n    def to(self, device: torch.device, dtype: torch.dtype) -> \"BufferCache\":\n        for i in range(self.n_layers):\n            self.cache_k[i] = self.cache_k[i].to(device=device, dtype=dtype)\n            self.cache_v[i] = self.cache_v[i].to(device=device, dtype=dtype)\n\n        return self\n\n    def update_seqlens(self, seqlens: List[int]) -> None:\n        assert self.kv_seqlens is not None\n        self.kv_seqlens += torch.tensor(seqlens, device=self.device, dtype=torch.long)\n\n    def get_input_metadata(self, seqlens: List[int]) -> List[CacheInputMetadata]:\n        \"\"\"\n        input = seqlens [5,7,2] // seqpos [0, 1, 3] // sliding_window 3\n        --> only cache last 3 tokens in each sequence\n        - to_cache_mask = [0 0 1 1 1 | 0 0 0 0 1 1 1 | 1 1]\n        - cached_elements = [3 | 3 | 2]\n        --> absolute positions are used for rope\n        - positions = [0 1 2 3 4 | 1 2 3 4 5 6 7 | 3 4]\n        --> cache positions are positions cache_masked, modulo sliding_window + batch_idx * sliding_window\n        - cache_positions = [2 0 1 | 5 3 4 | 6 7]\n        \"\"\"\n        metadata: List[CacheInputMetadata] = []\n\n        if self.kv_seqlens is None:\n            self.init_kvseqlens(len(seqlens))\n\n        assert self.kv_seqlens is not None\n        assert len(seqlens) == len(\n            self.kv_seqlens\n        ), f\"Batch size is {len(self.kv_seqlens)}, got {len(seqlens)}, did you forget to reset cache?\"\n        seqpos = self.kv_seqlens.tolist()\n        assert len(seqlens) > 0, seqlens\n\n        for cache_size in self.cache_sizes:\n            metadata.append(self._get_input_metadata_layer(cache_size, seqlens, seqpos))\n\n        return metadata\n\n    def _get_input_metadata_layer(self, cache_size: int, seqlens: List[int], seqpos: List[int]) -> CacheInputMetadata:\n        masks = [[x >= seqlen - cache_size for x in range(seqlen)] for seqlen in seqlens]\n        to_cache_mask = torch.tensor(sum(masks, []), device=self.device, dtype=torch.bool)\n        cached_elements = torch.tensor([sum(mask) for mask in masks], device=self.device, dtype=torch.long)\n        positions = torch.cat([torch.arange(pos, pos + seqlen) for pos, seqlen in zip(seqpos, seqlens)]).to(\n            device=self.device, dtype=torch.long\n        )\n        batch_idx = torch.tensor(\n            sum([[i] * seqlen for i, seqlen in enumerate(seqlens)], []), device=self.device, dtype=torch.long\n        )\n        cache_positions = positions % cache_size + batch_idx * cache_size\n        first_prefill = seqpos[0] == 0\n        subsequent_prefill = any(seqlen > 1 for seqlen in seqlens)\n        if first_prefill:\n            assert all([pos == 0 for pos in seqpos]), seqpos\n            mask = BlockDiagonalCausalMask.from_seqlens(seqlens).make_local_attention(cache_size)\n        elif subsequent_prefill:\n            assert self.kv_seqlens is not None\n            mask = BlockDiagonalMask.from_seqlens(\n                q_seqlen=seqlens,\n                kv_seqlen=[\n                    s + cached_s.clamp(max=cache_size).item() for (s, cached_s) in zip(seqlens, self.kv_seqlens)\n                ],\n            ).make_local_attention_from_bottomright(cache_size)\n        else:\n            mask = BlockDiagonalCausalWithOffsetPaddedKeysMask.from_seqlens(\n                q_seqlen=seqlens,\n                kv_padding=cache_size,\n                kv_seqlen=(self.kv_seqlens + cached_elements).clamp(max=cache_size).tolist(),\n            )\n        return CacheInputMetadata(\n            positions=positions,\n            to_cache_mask=to_cache_mask,\n            cached_elements=cached_elements,\n            cache_positions=cache_positions[to_cache_mask],\n            prefill=first_prefill or subsequent_prefill,\n            mask=mask,\n            seqlens=seqlens,\n        )\n",
        "src/mistral_inference/generate.py": "from typing import List, Optional, Tuple\n\nimport numpy as np\nimport torch\n\nfrom mistral_inference.cache import BufferCache\nfrom mistral_inference.mamba import Mamba\nfrom mistral_inference.transformer import Transformer\n\n\n@torch.inference_mode()\ndef generate_mamba(\n    encoded_prompts: List[List[int]],\n    model: Mamba,\n    *,\n    max_tokens: int,\n    temperature: float,\n    chunk_size: Optional[int] = None,\n    eos_id: Optional[int] = None,\n) -> Tuple[List[List[int]], List[List[float]]]:\n    input_ids = torch.tensor(encoded_prompts, device=model.device)\n    output = model.model.generate(\n        input_ids=input_ids,\n        max_length=input_ids.shape[-1] + max_tokens,\n        cg=True,\n        return_dict_in_generate=True,\n        output_scores=True,\n        enable_timing=False,\n        eos_token_id=eos_id,\n        temperature=temperature,\n        top_p=0.8,\n    )\n    generated_tokens = output.sequences[:, input_ids.shape[-1] :].tolist()\n\n    _logprobs: List[List[float]] = [[] for _ in range(len(generated_tokens))]\n    for seq_idx, batch_score in enumerate(output.scores):\n        for batch_idx, score in enumerate(batch_score.tolist()):\n            _logprobs[batch_idx].append(score[generated_tokens[batch_idx][seq_idx]])\n\n    return generated_tokens, _logprobs\n\n\n@torch.inference_mode()\ndef generate(\n    encoded_prompts: List[List[int]],\n    model: Transformer,\n    images: List[List[np.ndarray]] = [],\n    *,\n    max_tokens: int,\n    temperature: float,\n    chunk_size: Optional[int] = None,\n    eos_id: Optional[int] = None,\n) -> Tuple[List[List[int]], List[List[float]]]:\n    images_torch: List[List[torch.Tensor]] = []\n    if images:\n        assert chunk_size is None\n        images_torch = [\n            [torch.tensor(im, device=model.device, dtype=model.dtype) for im in images_for_sample]\n            for images_for_sample in images\n        ]\n\n    model = model.eval()\n    B, V = len(encoded_prompts), model.args.vocab_size\n\n    seqlens = [len(x) for x in encoded_prompts]\n\n    # Cache\n    cache_window = max(seqlens) + max_tokens\n    cache = BufferCache(\n        model.n_local_layers,\n        model.args.max_batch_size,\n        cache_window,\n        model.args.n_kv_heads,\n        model.args.head_dim,\n        model.args.sliding_window,\n    )\n    cache.to(device=model.device, dtype=model.dtype)\n    cache.reset()\n\n    # Bookkeeping\n    logprobs: List[List[float]] = [[] for _ in range(B)]\n    last_token_prelogits = None\n\n    # One chunk if size not specified\n    max_prompt_len = max(seqlens)\n    if chunk_size is None:\n        chunk_size = max_prompt_len\n\n    flattened_images: List[torch.Tensor] = sum(images_torch, [])\n\n    # Encode prompt by chunks\n    for s in range(0, max_prompt_len, chunk_size):\n        prompt_chunks = [p[s : s + chunk_size] for p in encoded_prompts]\n        assert all(len(p) > 0 for p in prompt_chunks)\n        prelogits = model.forward(\n            torch.tensor(sum(prompt_chunks, []), device=model.device, dtype=torch.long),\n            images=flattened_images,\n            seqlens=[len(p) for p in prompt_chunks],\n            cache=cache,\n        )\n        logits = torch.log_softmax(prelogits, dim=-1)\n\n        if last_token_prelogits is not None:\n            # Pass > 1\n            last_token_logits = torch.log_softmax(last_token_prelogits, dim=-1)\n            for i_seq in range(B):\n                logprobs[i_seq].append(last_token_logits[i_seq, prompt_chunks[i_seq][0]].item())\n\n        offset = 0\n        for i_seq, sequence in enumerate(prompt_chunks):\n            logprobs[i_seq].extend([logits[offset + i, sequence[i + 1]].item() for i in range(len(sequence) - 1)])\n            offset += len(sequence)\n\n        last_token_prelogits = prelogits.index_select(\n            0,\n            torch.tensor([len(p) for p in prompt_chunks], device=prelogits.device).cumsum(dim=0) - 1,\n        )\n        assert last_token_prelogits.shape == (B, V)\n\n    # decode\n    generated_tensors = []\n    is_finished = torch.tensor([False for _ in range(B)])\n\n    assert last_token_prelogits is not None\n    for _ in range(max_tokens):\n        next_token = sample(last_token_prelogits, temperature=temperature, top_p=0.8)\n\n        if eos_id is not None:\n            is_finished = is_finished | (next_token == eos_id).cpu()\n\n        if is_finished.all():\n            break\n\n        last_token_logits = torch.log_softmax(last_token_prelogits, dim=-1)\n        for i in range(B):\n            logprobs[i].append(last_token_logits[i, next_token[i]].item())\n\n        generated_tensors.append(next_token[:, None])\n        last_token_prelogits = model.forward(next_token, seqlens=[1] * B, cache=cache)\n        assert last_token_prelogits.shape == (B, V)\n\n    generated_tokens: List[List[int]]\n    if generated_tensors:\n        generated_tokens = torch.cat(generated_tensors, 1).tolist()\n    else:\n        generated_tokens = []\n\n    return generated_tokens, logprobs\n\n\ndef sample(logits: torch.Tensor, temperature: float, top_p: float) -> torch.Tensor:\n    if temperature > 0:\n        probs = torch.softmax(logits / temperature, dim=-1)\n        next_token = sample_top_p(probs, top_p)\n    else:\n        next_token = torch.argmax(logits, dim=-1).unsqueeze(0)\n\n    return next_token.reshape(-1)\n\n\ndef sample_top_p(probs: torch.Tensor, p: float) -> torch.Tensor:\n    assert 0 <= p <= 1\n\n    probs_sort, probs_idx = torch.sort(probs, dim=-1, descending=True)\n    probs_sum = torch.cumsum(probs_sort, dim=-1)\n    mask = probs_sum - probs_sort > p\n    probs_sort[mask] = 0.0\n    probs_sort.div_(probs_sort.sum(dim=-1, keepdim=True))\n    next_token = torch.multinomial(probs_sort, num_samples=1)\n    return torch.gather(probs_idx, -1, next_token)\n",
        "src/mistral_inference/lora.py": "import logging\nfrom dataclasses import dataclass\nfrom pathlib import Path\nfrom typing import Any, Dict, NamedTuple, Union\n\nimport safetensors.torch\nimport torch\nimport torch.nn as nn\nfrom simple_parsing.helpers import Serializable\n\n\n@dataclass\nclass LoraArgs(Serializable):\n    rank: int\n    scaling: float\n\n    def __post_init__(self) -> None:\n        assert self.rank > 0\n        assert self.scaling > 0.0\n\n\nclass LoRALinear(nn.Module):\n    \"\"\"\n    Implementation of:\n        - LoRA: https://arxiv.org/abs/2106.09685\n\n    Notes:\n        - Freezing is handled at network level, not layer level.\n        - Scaling factor controls relative importance of LoRA skip\n          connection versus original frozen weight. General guidance is\n          to keep it to 2.0 and sweep over learning rate when changing\n          the rank.\n    \"\"\"\n\n    def __init__(\n        self,\n        in_features: int,\n        out_features: int,\n        rank: int,\n        scaling: float,\n        bias: bool = False,\n    ):\n        super().__init__()\n\n        self.in_features = in_features\n        self.out_features = out_features\n        assert not bias\n        self.bias = bias\n        self.rank = rank\n        self.scaling = scaling\n\n        self.lora_A = nn.Linear(\n            self.in_features,\n            self.rank,\n            bias=self.bias,\n        )\n        self.lora_B = nn.Linear(\n            self.rank,\n            self.out_features,\n            bias=self.bias,\n        )\n\n        self.linear = nn.Linear(self.in_features, self.out_features, bias=self.bias)\n\n        # make sure no LoRA weights are marked as \"missing\" in load_state_dict\n        def ignore_missing_keys(m: nn.Module, incompatible_keys: NamedTuple) -> None:\n            incompatible_keys.missing_keys[:] = []  # type: ignore\n\n        self.register_load_state_dict_post_hook(ignore_missing_keys)\n\n    def forward(self, x: torch.Tensor) -> torch.Tensor:\n        lora = self.lora_B(self.lora_A(x))\n        result: torch.Tensor = self.linear(x) + lora * self.scaling\n        return result\n\n    def _load_from_state_dict(self, state_dict: Dict[str, Any], prefix: str, *args, **kwargs) -> None:  # type: ignore[no-untyped-def]\n        key_name = prefix + \"weight\"\n\n        # full checkpoint\n        if key_name in state_dict:\n            w_ref = state_dict[key_name]\n\n            # load frozen weights\n            state_dict = {\n                \"linear.weight\": w_ref,\n                \"lora_A.weight\": torch.zeros_like(self.lora_A.weight, device=w_ref.device, dtype=w_ref.dtype),\n                \"lora_B.weight\": torch.zeros_like(self.lora_B.weight, device=w_ref.device, dtype=w_ref.dtype),\n            }\n            self.load_state_dict(state_dict, assign=True, strict=True)\n\n\nclass LoRALoaderMixin:\n    def load_lora(self, lora_path: Union[Path, str], scaling: float = 2.0) -> None:\n        \"\"\"Loads LoRA checkpoint\"\"\"\n\n        lora_path = Path(lora_path)\n        assert lora_path.is_file(), f\"{lora_path} does not exist or is not a file\"\n\n        state_dict = safetensors.torch.load_file(lora_path)\n\n        self._load_lora_state_dict(state_dict, scaling=scaling)\n\n    def _load_lora_state_dict(self, lora_state_dict: Dict[str, torch.Tensor], scaling: float = 2.0) -> None:\n        \"\"\"Loads LoRA state_dict\"\"\"\n        lora_dtypes = set([p.dtype for p in lora_state_dict.values()])\n        assert (\n            len(lora_dtypes) == 1\n        ), f\"LoRA weights have multiple different dtypes {lora_dtypes}. All weights need to have the same dtype\"\n        lora_dtype = lora_dtypes.pop()\n        assert lora_dtype == self.dtype, f\"LoRA weights dtype differs from model's dtype {lora_dtype} != {self.dtype}\"  # type: ignore[attr-defined]\n        assert all(\"lora\" in key for key in lora_state_dict.keys())\n\n        # move tensors to device\n        lora_state_dict = {k: v.to(self.device) for k, v in lora_state_dict.items()}  # type: ignore[attr-defined]\n\n        state_dict = self.state_dict()  # type: ignore[attr-defined]\n\n        if self.args.lora is None:  # type: ignore[attr-defined]\n            logging.info(\"Loading and merging LoRA weights...\")\n\n            # replace every nn.Linear with a LoRALinear with 'meta' device except the output layer\n            named_modules = dict(self.named_modules())  # type: ignore[attr-defined]\n            for name, module in named_modules.items():\n                if isinstance(module, nn.Linear) and name != \"output\":\n                    layer_id = name.split(\".\")[1]\n                    if layer_id not in self.layers:  # type: ignore[attr-defined]\n                        logging.debug(\n                            \"Skipping parameter %s at pipeline rank %d\",\n                            name,\n                            self.pipeline_rank,  # type: ignore[attr-defined]\n                        )\n                    elif (name + \".lora_B.weight\") in lora_state_dict:\n                        weight = (\n                            module.weight\n                            + (lora_state_dict[name + \".lora_B.weight\"] @ lora_state_dict[name + \".lora_A.weight\"])\n                            * scaling\n                        )\n\n                        state_dict[name + \".weight\"] = weight\n        else:\n            logging.info(\"Loading LoRA weights...\")\n            for k, v in lora_state_dict.items():\n                state_dict.update(lora_state_dict)\n\n                layer_id = k.split(\".\")[1]\n                if layer_id in self.layers:  # type: ignore[attr-defined]\n                    state_dict[k] = v\n                else:\n                    logging.debug(\n                        \"Skipping parameter %s at pipeline rank %d\",\n                        k,\n                        self.pipeline_rank,  # type: ignore[attr-defined]\n                    )\n\n        self.load_state_dict(state_dict, strict=True)  # type: ignore[attr-defined]\n",
        "src/mistral_inference/main.py": "import json\nimport logging\nimport os\nimport warnings\nfrom pathlib import Path\nfrom typing import List, Optional, Tuple, Type, Union\n\nimport fire  # type: ignore\nimport torch\nimport torch.distributed as dist\nfrom mistral_common.protocol.instruct.messages import (\n    AssistantMessage,\n    ContentChunk,\n    ImageChunk,\n    ImageURLChunk,\n    TextChunk,\n    UserMessage,\n)\nfrom mistral_common.protocol.instruct.request import ChatCompletionRequest\nfrom mistral_common.tokens.tokenizers.base import Tokenizer\nfrom mistral_common.tokens.tokenizers.mistral import MistralTokenizer\nfrom mistral_common.tokens.tokenizers.sentencepiece import is_sentencepiece\nfrom mistral_common.tokens.tokenizers.tekken import (\n    SpecialTokenPolicy,\n    Tekkenizer,\n    is_tekken,\n)\nfrom PIL import Image\n\nfrom mistral_inference.args import TransformerArgs\nfrom mistral_inference.generate import generate, generate_mamba\nfrom mistral_inference.mamba import Mamba\nfrom mistral_inference.transformer import Transformer\n\n\ndef is_torchrun() -> bool:\n    required_vars = [\"MASTER_ADDR\", \"MASTER_PORT\", \"RANK\", \"WORLD_SIZE\"]\n    return all(var in os.environ for var in required_vars)\n\n\ndef load_tokenizer(model_path: Path) -> MistralTokenizer:\n    tokenizer = [f for f in os.listdir(model_path) if is_tekken(model_path / f) or is_sentencepiece(model_path / f)]\n    assert (\n        len(tokenizer) > 0\n    ), f\"No tokenizer in {model_path}, place a `tokenizer.model.[v1,v2,v3]` or `tekken.json` file in {model_path}.\"\n    assert (\n        len(tokenizer) == 1\n    ), f\"Multiple tokenizers {', '.join(tokenizer)} found in `model_path`, make sure to only have one tokenizer\"\n\n    mistral_tokenizer = MistralTokenizer.from_file(str(model_path / tokenizer[0]))\n\n    if isinstance(mistral_tokenizer.instruct_tokenizer.tokenizer, Tekkenizer):\n        mistral_tokenizer.instruct_tokenizer.tokenizer.special_token_policy = SpecialTokenPolicy.KEEP\n\n    logging.info(f\"Loaded tokenizer of type {mistral_tokenizer.instruct_tokenizer.__class__}\")\n\n    return mistral_tokenizer\n\n\ndef get_model_cls(model_path: str) -> Union[Type[Mamba], Type[Transformer]]:\n    with open(Path(model_path) / \"params.json\", \"r\") as f:\n        args_dict = json.load(f)\n\n    return {\"mamba\": Mamba, \"transformer\": Transformer}[args_dict.get(\"model_type\", \"transformer\")]  # type: ignore[return-value]\n\n\ndef pad_and_convert_to_tensor(list_of_lists: List[List[int]], pad_id: int) -> List[List[int]]:\n    # Determine the length of the longest list\n    max_len = max(len(lst) for lst in list_of_lists)\n\n    # Left pad each list to the maximum length\n    padded_lists = [[pad_id] * (max_len - len(lst)) + lst for lst in list_of_lists]\n\n    return padded_lists\n\n\ndef _get_multimodal_input() -> Tuple[UserMessage, bool]:\n    chunks: List[ContentChunk] = []\n\n    response = input(\"Text prompt: \")\n    if response:\n        chunks.append(TextChunk(text=response))\n\n    print(\"[You can input zero, one or more images now.]\")\n    while True:\n        did_something = False\n        response = input(\"Image path or url [Leave empty and press enter to finish image input]: \")\n        if response:\n            if Path(response).is_file():\n                chunks.append(ImageChunk(image=Image.open(response)))\n            else:\n                assert response.startswith(\"http\"), f\"{response} does not seem to be a valid url.\"\n                chunks.append(ImageURLChunk(image_url=response))\n            did_something = True\n\n        if not did_something:\n            break\n\n    return UserMessage(content=chunks), not chunks\n\n\ndef interactive(\n    model_path: str,\n    max_tokens: int = 35,\n    temperature: float = 0.7,\n    num_pipeline_ranks: int = 1,\n    instruct: bool = False,\n    lora_path: Optional[str] = None,\n) -> None:\n    if is_torchrun():\n        torch.distributed.init_process_group()\n        torch.cuda.set_device(torch.distributed.get_rank())\n        should_print = torch.distributed.get_rank() == 0\n\n        num_pipeline_ranks = torch.distributed.get_world_size()\n    else:\n        should_print = True\n        num_pipeline_ranks = 1\n\n    mistral_tokenizer: MistralTokenizer = load_tokenizer(Path(model_path))\n    tokenizer: Tokenizer = mistral_tokenizer.instruct_tokenizer.tokenizer\n\n    model_cls = get_model_cls(model_path)\n    model = model_cls.from_folder(Path(model_path), max_batch_size=3, num_pipeline_ranks=num_pipeline_ranks)\n    is_multimodal = isinstance(model.args, TransformerArgs) and model.args.vision_encoder is not None\n\n    if is_multimodal:\n        assert instruct, \"Multimodal models should only be used in instruct mode\"\n\n    # load LoRA\n    if lora_path is not None:\n        model.load_lora(Path(lora_path))\n\n    prompt: str = \"\"\n    messages: List[UserMessage | AssistantMessage] = []\n\n    while True:\n        if should_print:\n            if not is_multimodal:\n                user_input = input(\"Prompt: \")\n\n            if instruct:\n                if is_multimodal:\n                    mm_input, finished = _get_multimodal_input()\n                    if finished:\n                        break\n                    messages += [mm_input]\n                else:\n                    messages += [UserMessage(content=user_input)]\n                chat_completion_request = ChatCompletionRequest(messages=messages)\n\n                tokenized = mistral_tokenizer.encode_chat_completion(chat_completion_request)\n                tokens = tokenized.tokens\n                images = tokenized.images\n            else:\n                prompt += user_input\n\n                tokens = tokenizer.encode(prompt, bos=True, eos=False)\n                images = []\n\n            length_tensor = torch.tensor([len(tokens)], dtype=torch.int)\n        else:\n            length_tensor = torch.tensor([0], dtype=torch.int)\n            images = []\n\n        if is_torchrun():\n            dist.broadcast(length_tensor, src=0)\n\n        if not should_print:\n            tokens = int(length_tensor.item()) * [0]\n\n        generate_fn = generate if isinstance(model, Transformer) else generate_mamba\n        generated_tokens, _ = generate_fn(  # type: ignore[operator]\n            [tokens],\n            model,\n            [images],\n            max_tokens=max_tokens,\n            temperature=temperature,\n            eos_id=tokenizer.eos_id,\n        )\n\n        answer = tokenizer.decode(generated_tokens[0])\n\n        if should_print:\n            print(answer)\n            print(\"=====================\")\n\n        if instruct:\n            messages += [AssistantMessage(content=answer)]\n        else:\n            prompt += answer\n\n\ndef demo(\n    model_path: str,\n    max_tokens: int = 35,\n    temperature: float = 0,\n    lora_path: Optional[str] = None,\n) -> None:\n    if is_torchrun():\n        torch.distributed.init_process_group()\n        torch.cuda.set_device(torch.distributed.get_rank())\n        should_print = torch.distributed.get_rank() == 0\n\n        num_pipeline_ranks = torch.distributed.get_world_size()\n    else:\n        should_print = True\n        num_pipeline_ranks = 1\n\n    model_cls = get_model_cls(model_path)\n    model = model_cls.from_folder(Path(model_path), max_batch_size=3, num_pipeline_ranks=num_pipeline_ranks)\n    # load LoRA\n    if lora_path is not None:\n        model.load_lora(Path(lora_path))\n\n    mistral_tokenizer: MistralTokenizer = load_tokenizer(Path(model_path))\n    tokenizer: Tokenizer = mistral_tokenizer.instruct_tokenizer.tokenizer\n\n    prompts = [\n        \"This is a test\",\n        \"This is another great test\",\n        \"This is a third test, mistral AI is very good at testing. \",\n    ]\n\n    encoded_prompts = [tokenizer.encode(prompt, bos=True, eos=False) for prompt in prompts]\n\n    if isinstance(model, Transformer):\n        generate_fn = generate\n    else:\n        generate_fn = generate_mamba  # type: ignore[assignment]\n        warnings.warn(\n            \"Batched generation is not correctly supported at the moment and therefore might lead to worse results \"\n            \"as compared to non-batched generation. \"\n            \"See https://github.com/state-spaces/mamba/issues/66#issuecomment-1862349718 for more information.\"\n        )\n        encoded_prompts = pad_and_convert_to_tensor(encoded_prompts, mistral_tokenizer.instruct_tokenizer.BOS)  # type: ignore[attr-defined]\n\n    generated_tokens, _logprobs = generate_fn(\n        encoded_prompts,\n        model,  # type: ignore[arg-type]\n        max_tokens=max_tokens,\n        temperature=temperature,\n        eos_id=tokenizer.eos_id,\n    )\n\n    generated_words = []\n    for i, x in enumerate(generated_tokens):\n        generated_words.append(tokenizer.decode(encoded_prompts[i] + x))\n\n    res = generated_words\n\n    if should_print:\n        for w, logprob in zip(res, _logprobs):\n            print(w)\n            logging.debug(\"Logprobs: %s\", logprob)\n            print(\"=====================\")\n\n\ndef mistral_chat() -> None:\n    fire.Fire(interactive)\n\n\ndef mistral_demo() -> None:\n    fire.Fire(demo)\n\n\nif __name__ == \"__main__\":\n    logging.basicConfig(level=logging.INFO)\n    fire.Fire(\n        {\n            \"interactive\": interactive,\n            \"demo\": demo,\n        }\n    )\n",
        "src/mistral_inference/mamba.py": "import json\nfrom pathlib import Path\nfrom typing import List, Optional, Union\n\nimport safetensors\nimport torch\nimport torch.nn as nn\n\nfrom mistral_inference.args import MambaArgs\nfrom mistral_inference.cache import BufferCache\nfrom mistral_inference.model import ModelBase\n\n_is_mamba_installed = False\ntry:\n    from mamba_ssm.models.config_mamba import MambaConfig\n    from mamba_ssm.models.mixer_seq_simple import MambaLMHeadModel\n\n    _is_mamba_installed = True\nexcept ImportError:\n    _is_mamba_installed = False\n\n\nclass Mamba(ModelBase, nn.Module):\n    def __init__(self, args: MambaArgs):\n        super().__init__()\n        self.args = args\n        assert _is_mamba_installed, \"Mamba is not installed. Please install it using `pip install mamba-ssm`.\"\n\n        # make sure naming is consistent with `mamba_ssm`\n        config = MambaConfig(\n            d_model=args.dim,\n            n_layer=args.n_layers,\n            vocab_size=args.vocab_size,\n            ssm_cfg={\"ngroups\": args.n_groups, \"layer\": \"Mamba2\"},\n            attn_layer_idx=[],\n            attn_cfg={},\n            rms_norm=args.rms_norm,\n            residual_in_fp32=args.residual_in_fp32,\n            fused_add_norm=args.fused_add_norm,\n            pad_vocab_size_multiple=args.pad_vocab_size_multiple,\n            tie_embeddings=args.tie_embeddings,\n        )\n        self.model = MambaLMHeadModel(config)\n\n    @property\n    def dtype(self) -> torch.dtype:\n        return next(self.parameters()).dtype\n\n    @property\n    def device(self) -> torch.device:\n        return next(self.parameters()).device\n\n    def forward(\n        self,\n        input_ids: torch.Tensor,\n        seqlens: List[int],  # not supported for now\n        cache: Optional[BufferCache] = None,  # not supported for now\n    ) -> torch.Tensor:\n        lm_output = self.model(input_ids)\n        result: torch.Tensor = lm_output.logits\n        return result\n\n    @staticmethod\n    def from_folder(\n        folder: Union[Path, str],\n        max_batch_size: int = 1,\n        num_pipeline_ranks: int = 1,\n        device: Union[torch.device, str] = \"cuda\",\n        dtype: Optional[torch.dtype] = None,\n    ) -> \"Mamba\":\n        with open(Path(folder) / \"params.json\", \"r\") as f:\n            model_args = MambaArgs.from_dict(json.load(f))\n\n        with torch.device(\"meta\"):\n            model = Mamba(model_args)\n\n        model_file = Path(folder) / \"consolidated.safetensors\"\n\n        assert model_file.exists(), f\"Make sure {model_file} exists.\"\n        loaded = safetensors.torch.load_file(str(model_file))\n\n        model.load_state_dict(loaded, assign=True, strict=True)\n        return model.to(device=device, dtype=dtype)\n",
        "src/mistral_inference/model.py": "from abc import ABC, abstractmethod\nfrom pathlib import Path\nfrom typing import List, Optional, Union\n\nimport torch\nimport torch.nn as nn\n\nfrom mistral_inference.cache import BufferCache\n\n\nclass ModelBase(nn.Module, ABC):\n    def __init__(self) -> None:\n        super().__init__()\n\n    @property\n    @abstractmethod\n    def dtype(self) -> torch.dtype:\n        pass\n\n    @property\n    @abstractmethod\n    def device(self) -> torch.device:\n        pass\n\n    @abstractmethod\n    def forward(\n        self,\n        input_ids: torch.Tensor,\n        seqlens: List[int],  # not supported for now\n        cache: Optional[BufferCache] = None,  # not supported for now\n    ) -> torch.Tensor:\n        pass\n\n    @staticmethod\n    @abstractmethod\n    def from_folder(\n        folder: Union[Path, str],\n        max_batch_size: int = 1,\n        num_pipeline_ranks: int = 1,\n        device: Union[torch.device, str] = \"cuda\",\n        dtype: Optional[torch.dtype] = None,\n    ) -> \"ModelBase\":\n        pass\n",
        "src/mistral_inference/moe.py": "import dataclasses\nfrom typing import List\n\nimport torch\nimport torch.nn.functional as F\nfrom simple_parsing.helpers import Serializable\nfrom torch import nn\n\n\n@dataclasses.dataclass\nclass MoeArgs(Serializable):\n    num_experts: int\n    num_experts_per_tok: int\n\n\nclass MoeLayer(nn.Module):\n    def __init__(self, experts: List[nn.Module], gate: nn.Module, moe_args: MoeArgs):\n        super().__init__()\n        assert len(experts) > 0\n        self.experts = nn.ModuleList(experts)\n        self.gate = gate\n        self.args = moe_args\n\n    def forward(self, inputs: torch.Tensor) -> torch.Tensor:\n        gate_logits = self.gate(inputs)\n        weights, selected_experts = torch.topk(gate_logits, self.args.num_experts_per_tok)\n        weights = F.softmax(weights, dim=1, dtype=torch.float).to(inputs.dtype)\n        results = torch.zeros_like(inputs)\n        for i, expert in enumerate(self.experts):\n            batch_idx, nth_expert = torch.where(selected_experts == i)\n            results[batch_idx] += weights[batch_idx, nth_expert, None] * expert(inputs[batch_idx])\n        return results\n",
        "src/mistral_inference/rope.py": "from typing import Tuple\n\nimport torch\n\n\ndef precompute_freqs_cis(dim: int, end: int, theta: float) -> torch.Tensor:\n    freqs = 1.0 / (theta ** (torch.arange(0, dim, 2)[: (dim // 2)].float() / dim))\n    t = torch.arange(end, device=freqs.device)\n    freqs = torch.outer(t, freqs).float()\n    return torch.polar(torch.ones_like(freqs), freqs)  # complex64\n\n\ndef apply_rotary_emb(\n    xq: torch.Tensor,\n    xk: torch.Tensor,\n    freqs_cis: torch.Tensor,\n) -> Tuple[torch.Tensor, torch.Tensor]:\n    xq_ = torch.view_as_complex(xq.float().reshape(*xq.shape[:-1], -1, 2))\n    xk_ = torch.view_as_complex(xk.float().reshape(*xk.shape[:-1], -1, 2))\n    freqs_cis = freqs_cis[:, None, :]\n    xq_out = torch.view_as_real(xq_ * freqs_cis).flatten(-2)\n    xk_out = torch.view_as_real(xk_ * freqs_cis).flatten(-2)\n    return xq_out.type_as(xq), xk_out.type_as(xk)\n\n\ndef precompute_freqs_cis_2d(\n    dim: int,\n    height: int,\n    width: int,\n    theta: float,\n) -> torch.Tensor:\n    \"\"\"\n    freqs_cis: 2D complex tensor of shape (height, width, dim // 2) to be indexed by\n        (height, width) position tuples\n    \"\"\"\n    # (dim / 2) frequency bases\n    freqs = 1.0 / (theta ** (torch.arange(0, dim, 2).float() / dim))\n\n    h = torch.arange(height, device=freqs.device)\n    w = torch.arange(width, device=freqs.device)\n\n    freqs_h = torch.outer(h, freqs[::2]).float()\n    freqs_w = torch.outer(w, freqs[1::2]).float()\n    freqs_2d = torch.cat(\n        [\n            freqs_h[:, None, :].repeat(1, width, 1),\n            freqs_w[None, :, :].repeat(height, 1, 1),\n        ],\n        dim=-1,\n    )\n    return torch.polar(torch.ones_like(freqs_2d), freqs_2d)\n",
        "src/mistral_inference/transformer.py": "import json\nimport logging\nimport math\nfrom dataclasses import dataclass\nfrom pathlib import Path\nfrom typing import Any, List, Mapping, Optional, Union\n\nimport safetensors.torch\nimport torch\nfrom torch import nn\n\nfrom mistral_inference.args import PATCH_MERGE, TransformerArgs\nfrom mistral_inference.cache import BufferCache, CacheInputMetadata\nfrom mistral_inference.lora import LoRALoaderMixin\nfrom mistral_inference.model import ModelBase\nfrom mistral_inference.rope import precompute_freqs_cis\nfrom mistral_inference.transformer_layers import RMSNorm, TransformerBlock\nfrom mistral_inference.vision_encoder import PatchMerger, VisionLanguageAdapter, VisionTransformer\n\n\n@dataclass\nclass SimpleInputMetadata:\n    # rope absolute positions\n    positions: torch.Tensor\n\n    @staticmethod\n    def from_seqlens(seqlens: List[int], device: torch.device) -> \"SimpleInputMetadata\":\n        return SimpleInputMetadata(\n            positions=torch.cat([torch.arange(0, seqlen) for seqlen in seqlens]).to(device=device, dtype=torch.long)\n        )\n\n\nclass Transformer(ModelBase, LoRALoaderMixin):\n    def __init__(\n        self,\n        args: TransformerArgs,\n        pipeline_rank: int = 0,\n        num_pipeline_ranks: int = 1,\n        softmax_fp32: bool = True,\n    ):\n        super().__init__()\n        self.args = args\n        self.vocab_size = args.vocab_size\n        self.n_layers = args.n_layers\n        self._precomputed_freqs_cis: Optional[torch.Tensor] = None\n        assert self.vocab_size > 0\n        assert pipeline_rank < num_pipeline_ranks, (pipeline_rank, num_pipeline_ranks)\n        self.pipeline_rank = pipeline_rank\n        self.num_pipeline_ranks = num_pipeline_ranks\n        self.softmax_fp32 = softmax_fp32\n\n        # Modules specific to some ranks:\n        self.tok_embeddings: Optional[nn.Embedding] = None\n        self.norm: Optional[RMSNorm] = None\n        self.output: Optional[nn.Linear] = None\n        if pipeline_rank == 0:\n            self.tok_embeddings = nn.Embedding(args.vocab_size, args.dim)\n\n            self.vision_encoder: Optional[VisionTransformer] = None\n            self.vision_language_adapter: Optional[VisionLanguageAdapter] = None\n\n            if args.vision_encoder is not None:\n                self.vision_encoder = VisionTransformer(args.vision_encoder)\n                self.vision_language_adapter = VisionLanguageAdapter(\n                    args.vision_encoder.hidden_size, args.dim, args.vision_encoder.adapter_bias\n                )\n\n                if args.vision_encoder.add_pre_mm_projector_layer_norm:\n                    self.pre_mm_projector_norm = RMSNorm(args.vision_encoder.hidden_size, eps=1e-5)\n\n                if args.vision_encoder.mm_projector_id == PATCH_MERGE:\n                    self.patch_merger = PatchMerger(\n                        vision_encoder_dim=args.vision_encoder.hidden_size,\n                        spatial_merge_size=args.vision_encoder.spatial_merge_size,\n                    )\n\n        if pipeline_rank == num_pipeline_ranks - 1:\n            self.norm = RMSNorm(args.dim, eps=args.norm_eps)\n            self.output = nn.Linear(args.dim, args.vocab_size, bias=False)\n        # Initialize all layers but slice off those not of this rank.\n        layers = [\n            TransformerBlock(\n                dim=args.dim,\n                hidden_dim=args.hidden_dim,\n                n_heads=args.n_heads,\n                n_kv_heads=args.n_kv_heads,\n                head_dim=args.head_dim,\n                norm_eps=args.norm_eps,\n                lora=args.lora,\n                moe=args.moe,\n            )\n            for _ in range(args.n_layers)\n        ]\n        num_layers_per_rank = math.ceil(self.n_layers / self.num_pipeline_ranks)\n        offset = self.pipeline_rank * num_layers_per_rank\n        end = min(self.n_layers, offset + num_layers_per_rank)\n        self.layers = nn.ModuleDict({str(i): layers[i] for i in range(offset, end)})\n        self.n_local_layers = len(self.layers)\n\n    @property\n    def dtype(self) -> torch.dtype:\n        return next(self.parameters()).dtype\n\n    @property\n    def device(self) -> torch.device:\n        return next(self.parameters()).device\n\n    @property\n    def freqs_cis(self) -> torch.Tensor:\n        # We cache freqs_cis but need to take care that it is on the right device\n        # and has the right dtype (complex64). The fact that the dtype is different\n        # from the module's  dtype means we cannot register it as a buffer\n        if self._precomputed_freqs_cis is None:\n            # default to 10**6\n            theta = self.args.rope_theta or 1000000.0\n            self._precomputed_freqs_cis = precompute_freqs_cis(self.args.head_dim, 128_000, theta)\n\n        if self._precomputed_freqs_cis.device != self.device:\n            self._precomputed_freqs_cis = self._precomputed_freqs_cis.to(device=self.device)\n        return self._precomputed_freqs_cis\n\n    def embed_vision_language_features(self, input_ids: torch.Tensor, images: List[torch.Tensor]) -> torch.Tensor:\n        assert self.tok_embeddings is not None\n        assert self.vision_encoder is not None\n        assert self.vision_language_adapter is not None\n        assert self.args.vision_encoder is not None\n\n        text_locations = input_ids != self.args.vision_encoder.image_token_id\n        image_locations = input_ids == self.args.vision_encoder.image_token_id\n        text_features = self.tok_embeddings(input_ids[text_locations])\n\n        image_features = self.vision_encoder(images)\n\n        if self.args.vision_encoder.add_pre_mm_projector_layer_norm:\n            image_features = self.pre_mm_projector_norm(image_features)\n\n        if self.args.vision_encoder.mm_projector_id == PATCH_MERGE:\n            patch_size = self.args.vision_encoder.patch_size\n            img_patch_dims = [(img.shape[1] // patch_size, img.shape[2] // patch_size) for img in images]\n            image_features = self.patch_merger(image_features, image_sizes=img_patch_dims)\n\n        image_features = self.vision_language_adapter(image_features)\n\n        N_txt, D_txt = text_features.shape\n        N_img, D_img = image_features.shape\n\n        seq_len = input_ids.shape[0]\n\n        assert D_txt == D_img, f\"Text features dim {D_txt} should be equal to image features dim {D_img}\"\n        assert seq_len == N_txt + N_img, (\n            f\"seq_len {seq_len} should be equal to N_txt + N_img {(N_txt, N_img, image_locations.sum().item())}\"\n        )\n\n        combined_features = torch.empty(\n            (seq_len, D_txt),\n            dtype=text_features.dtype,\n            device=text_features.device,\n        )\n        combined_features[text_locations, :] = text_features\n        combined_features[image_locations, :] = image_features\n        return combined_features\n\n    def forward_partial(\n        self,\n        input_ids: torch.Tensor,\n        seqlens: List[int],\n        cache: Optional[BufferCache] = None,\n        images: Optional[List[torch.Tensor]] = None,\n    ) -> torch.Tensor:\n        \"\"\"Local forward pass.\n\n        If doing pipeline parallelism, this will return the activations of the last layer of this stage.\n        For the last stage, this will return the normalized final embeddings.\n        \"\"\"\n        assert len(seqlens) <= self.args.max_batch_size, (\n            f\"Max batch size is {self.args.max_batch_size}, got batch size of {len(seqlens)}\"\n        )\n        (num_toks,) = input_ids.shape\n        assert sum(seqlens) == num_toks, (sum(seqlens), num_toks)\n\n        input_metadata: List[CacheInputMetadata] | List[SimpleInputMetadata]\n\n        if cache is not None:\n            input_metadata = cache.get_input_metadata(seqlens)\n        else:\n            input_metadata = [SimpleInputMetadata.from_seqlens(seqlens, self.device) for _ in range(len(self.layers))]\n\n        if self.pipeline_rank == 0:\n            assert self.tok_embeddings is not None\n            if self.vision_encoder is not None and images:\n                h = self.embed_vision_language_features(input_ids, images)\n            else:\n                h = self.tok_embeddings(input_ids)\n        else:\n            h = torch.empty(num_toks, self.args.dim, device=self.device, dtype=self.dtype)\n            torch.distributed.recv(h, src=self.pipeline_rank - 1)\n\n        # freqs_cis is always the same for every layer\n        freqs_cis = self.freqs_cis[input_metadata[0].positions]\n\n        for local_layer_id, layer in enumerate(self.layers.values()):\n            if cache is not None:\n                assert input_metadata is not None\n                cache_metadata = input_metadata[local_layer_id]\n                assert isinstance(cache_metadata, CacheInputMetadata)\n                cache_view = cache.get_view(local_layer_id, cache_metadata)\n            else:\n                cache_view = None\n            h = layer(h, freqs_cis, cache_view)\n\n        if cache is not None:\n            cache.update_seqlens(seqlens)\n        if self.pipeline_rank < self.num_pipeline_ranks - 1:\n            torch.distributed.send(h, dst=self.pipeline_rank + 1)\n            return h\n        else:\n            # Last rank has a final normalization step.\n            assert self.norm is not None\n            return self.norm(h)  # type: ignore\n\n    def forward(\n        self,\n        input_ids: torch.Tensor,\n        seqlens: List[int],\n        cache: Optional[BufferCache] = None,\n        images: Optional[List[torch.Tensor]] = None,\n    ) -> torch.Tensor:\n        h = self.forward_partial(input_ids, seqlens, cache=cache, images=images)\n        if self.pipeline_rank < self.num_pipeline_ranks - 1:\n            # ignore the intermediate activations as we'll get the final output from\n            # the last stage\n            outs = torch.empty(h.shape[0], self.vocab_size, device=h.device, dtype=h.dtype)\n        else:\n            assert self.output is not None\n            outs = self.output(h)\n        if self.num_pipeline_ranks > 1:\n            torch.distributed.broadcast(outs, src=self.num_pipeline_ranks - 1)\n\n        if self.softmax_fp32:\n            return outs.float()\n        else:\n            return outs\n\n    def load_state_dict(self, state_dict: Mapping[str, Any], strict: bool = True, assign: bool = False) -> None:\n        state_to_load = {}\n        skipped = set([])\n        for k, v in state_dict.items():\n            if k.startswith(\"tok_embeddings\"):\n                if self.pipeline_rank == 0:\n                    state_to_load[k] = v\n                else:\n                    logging.debug(\n                        \"Skipping parameter %s at pipeline rank %d\",\n                        k,\n                        self.pipeline_rank,\n                    )\n                    skipped.add(k)\n            elif k.startswith(\"norm\") or k.startswith(\"output\"):\n                if self.pipeline_rank == self.num_pipeline_ranks - 1:\n                    state_to_load[k] = v\n                else:\n                    logging.debug(\n                        \"Skipping parameter %s at pipeline rank %d\",\n                        k,\n                        self.pipeline_rank,\n                    )\n                    skipped.add(k)\n            elif k.startswith(\"layers\"):\n                layer_id = k.split(\".\")[1]\n                if layer_id in self.layers:\n                    state_to_load[k] = v\n                else:\n                    logging.debug(\n                        \"Skipping parameter %s at pipeline rank %d\",\n                        k,\n                        self.pipeline_rank,\n                    )\n                    skipped.add(k)\n            elif any(\n                k.startswith(key)\n                for key in [\"vision_encoder\", \"vision_language_adapter\", \"patch_merger\", \"pre_mm_projector_norm\"]\n            ):\n                if self.pipeline_rank == 0:\n                    state_to_load[k] = v\n                else:\n                    logging.debug(\n                        \"Skipping parameter %s at pipeline rank %d\",\n                        k,\n                        self.pipeline_rank,\n                    )\n                    skipped.add(k)\n            else:\n                raise ValueError(f\"Unexpected key {k}\")\n        assert set(state_dict.keys()) == skipped.union(set(state_to_load.keys()))\n        super().load_state_dict(state_to_load, strict=strict, assign=assign)\n\n    @staticmethod\n    def from_folder(\n        folder: Union[Path, str],\n        max_batch_size: int = 1,\n        num_pipeline_ranks: int = 1,\n        device: Union[torch.device, str] = \"cuda\",\n        dtype: Optional[torch.dtype] = None,\n        softmax_fp32: bool = True,\n    ) -> \"Transformer\":\n        with open(Path(folder) / \"params.json\", \"r\") as f:\n            model_args = TransformerArgs.from_dict(json.load(f))\n        model_args.max_batch_size = max_batch_size\n        if num_pipeline_ranks > 1:\n            pipeline_rank = torch.distributed.get_rank()\n        else:\n            pipeline_rank = 0\n        with torch.device(\"meta\"):\n            model = Transformer(\n                model_args,\n                pipeline_rank=pipeline_rank,\n                num_pipeline_ranks=num_pipeline_ranks,\n                softmax_fp32=softmax_fp32,\n            )\n\n        pt_model_file = Path(folder) / \"consolidated.00.pth\"\n        safetensors_model_file = Path(folder) / \"consolidated.safetensors\"\n\n        assert pt_model_file.exists() or safetensors_model_file.exists(), (\n            f\"Make sure either {pt_model_file} or {safetensors_model_file} exists\"\n        )\n        assert not (pt_model_file.exists() and safetensors_model_file.exists()), (\n            f\"Both {pt_model_file} and {safetensors_model_file} cannot exist\"\n        )\n\n        if pt_model_file.exists():\n            loaded = torch.load(str(pt_model_file), mmap=True)\n        else:\n            loaded = safetensors.torch.load_file(str(safetensors_model_file))\n\n        model.load_state_dict(loaded, assign=True, strict=True)\n\n        return model.to(device=device, dtype=dtype)\n",
        "src/mistral_inference/transformer_layers.py": "from functools import partial\nfrom typing import Optional, Tuple, Type, Union\n\nimport torch\nfrom torch import nn\nfrom xformers.ops.fmha import memory_efficient_attention  # type: ignore\nfrom xformers.ops.fmha.attn_bias import BlockDiagonalMask\n\nfrom mistral_inference.args import LoraArgs\nfrom mistral_inference.cache import CacheView\nfrom mistral_inference.lora import LoRALinear\nfrom mistral_inference.moe import MoeArgs, MoeLayer\nfrom mistral_inference.rope import apply_rotary_emb\n\n\ndef repeat_kv(keys: torch.Tensor, values: torch.Tensor, repeats: int, dim: int) -> Tuple[torch.Tensor, torch.Tensor]:\n    keys = torch.repeat_interleave(keys, repeats=repeats, dim=dim)\n    values = torch.repeat_interleave(values, repeats=repeats, dim=dim)\n    return keys, values\n\n\ndef maybe_lora(\n    lora_args: Optional[LoraArgs],\n) -> Union[Type[nn.Linear], partial[LoRALinear]]:\n    if lora_args is None:\n        return nn.Linear\n    else:\n        return partial(LoRALinear, rank=lora_args.rank, scaling=lora_args.scaling)\n\n\nclass Attention(nn.Module):\n    def __init__(\n        self,\n        dim: int,\n        n_heads: int,\n        head_dim: int,\n        n_kv_heads: int,\n        lora: Optional[LoraArgs] = None,\n    ):\n        super().__init__()\n\n        self.n_heads: int = n_heads\n        self.head_dim: int = head_dim\n        self.n_kv_heads: int = n_kv_heads\n\n        self.repeats = self.n_heads // self.n_kv_heads\n\n        self.scale = self.head_dim**-0.5\n\n        MaybeLora = maybe_lora(lora)\n        self.wq = MaybeLora(dim, n_heads * head_dim, bias=False)\n        self.wk = MaybeLora(dim, n_kv_heads * head_dim, bias=False)\n        self.wv = MaybeLora(dim, n_kv_heads * head_dim, bias=False)\n        self.wo = MaybeLora(n_heads * head_dim, dim, bias=False)\n\n    def forward(\n        self,\n        x: torch.Tensor,\n        freqs_cis: torch.Tensor,\n        cache: Optional[CacheView] = None,\n        mask: Optional[BlockDiagonalMask] = None,\n    ) -> torch.Tensor:\n        assert mask is None or cache is None\n        seqlen_sum, _ = x.shape\n\n        xq, xk, xv = self.wq(x), self.wk(x), self.wv(x)\n        xq = xq.view(seqlen_sum, self.n_heads, self.head_dim)\n        xk = xk.view(seqlen_sum, self.n_kv_heads, self.head_dim)\n        xv = xv.view(seqlen_sum, self.n_kv_heads, self.head_dim)\n        xq, xk = apply_rotary_emb(xq, xk, freqs_cis=freqs_cis)\n\n        if cache is None:\n            key, val = xk, xv\n        elif cache.prefill:\n            key, val = cache.interleave_kv(xk, xv)\n            cache.update(xk, xv)\n        else:\n            cache.update(xk, xv)\n            key, val = cache.key, cache.value\n            key = key.view(seqlen_sum * cache.max_seq_len, self.n_kv_heads, self.head_dim)\n            val = val.view(seqlen_sum * cache.max_seq_len, self.n_kv_heads, self.head_dim)\n\n        # Repeat keys and values to match number of query heads\n        key, val = repeat_kv(key, val, self.repeats, dim=1)\n\n        # xformers requires (B=1, S, H, D)\n        xq, key, val = xq[None, ...], key[None, ...], val[None, ...]\n        output = memory_efficient_attention(xq, key, val, mask if cache is None else cache.mask)\n        output = output.view(seqlen_sum, self.n_heads * self.head_dim)\n\n        assert isinstance(output, torch.Tensor)\n\n        return self.wo(output)  # type: ignore\n\n\nclass FeedForward(nn.Module):\n    def __init__(self, dim: int, hidden_dim: int, lora: Optional[LoraArgs] = None):\n        super().__init__()\n\n        MaybeLora = maybe_lora(lora)\n        self.w1 = MaybeLora(dim, hidden_dim, bias=False)\n        self.w2 = MaybeLora(hidden_dim, dim, bias=False)\n        self.w3 = MaybeLora(dim, hidden_dim, bias=False)\n\n    def forward(self, x: torch.Tensor) -> torch.Tensor:\n        return self.w2(nn.functional.silu(self.w1(x)) * self.w3(x))  # type: ignore\n\n\nclass RMSNorm(torch.nn.Module):\n    def __init__(self, dim: int, eps: float = 1e-6):\n        super().__init__()\n        self.eps = eps\n        self.weight = nn.Parameter(torch.ones(dim))\n\n    def _norm(self, x: torch.Tensor) -> torch.Tensor:\n        return x * torch.rsqrt(x.pow(2).mean(-1, keepdim=True) + self.eps)\n\n    def forward(self, x: torch.Tensor) -> torch.Tensor:\n        output = self._norm(x.float()).type_as(x)\n        return output * self.weight\n\n\nclass TransformerBlock(nn.Module):\n    def __init__(\n        self,\n        dim: int,\n        hidden_dim: int,\n        n_heads: int,\n        n_kv_heads: int,\n        head_dim: int,\n        norm_eps: float,\n        lora: Optional[LoraArgs] = None,\n        moe: Optional[MoeArgs] = None,\n    ):\n        super().__init__()\n        self.n_heads = n_heads\n        self.dim = dim\n        self.attention = Attention(\n            dim=dim,\n            n_heads=n_heads,\n            head_dim=head_dim,\n            n_kv_heads=n_kv_heads,\n            lora=lora,\n        )\n        self.attention_norm = RMSNorm(dim, eps=norm_eps)\n        self.ffn_norm = RMSNorm(dim, eps=norm_eps)\n\n        self.feed_forward: nn.Module\n        if moe is not None:\n            self.feed_forward = MoeLayer(\n                experts=[FeedForward(dim=dim, hidden_dim=hidden_dim, lora=lora) for _ in range(moe.num_experts)],\n                gate=nn.Linear(dim, moe.num_experts, bias=False),\n                moe_args=moe,\n            )\n        else:\n            self.feed_forward = FeedForward(dim=dim, hidden_dim=hidden_dim, lora=lora)\n\n    def forward(\n        self,\n        x: torch.Tensor,\n        freqs_cis: torch.Tensor,\n        cache: Optional[CacheView] = None,\n        mask: Optional[BlockDiagonalMask] = None,\n    ) -> torch.Tensor:\n        r = self.attention.forward(self.attention_norm(x), freqs_cis, cache)\n        h = x + r\n        r = self.feed_forward.forward(self.ffn_norm(h))\n        out = h + r\n        return out\n",
        "src/mistral_inference/vision_encoder.py": "from typing import List, Optional\n\nimport torch\nimport torch.nn as nn\nfrom xformers.ops.fmha.attn_bias import BlockDiagonalMask\n\nfrom mistral_inference.args import VisionEncoderArgs\nfrom mistral_inference.rope import precompute_freqs_cis_2d\nfrom mistral_inference.transformer_layers import RMSNorm, TransformerBlock\n\n\ndef position_meshgrid(\n    patch_embeds_list: list[torch.Tensor],\n) -> torch.Tensor:\n    positions = torch.cat(\n        [\n            torch.stack(\n                torch.meshgrid(\n                    torch.arange(p.shape[-2]),\n                    torch.arange(p.shape[-1]),\n                    indexing=\"ij\",\n                ),\n                dim=-1,\n            ).reshape(-1, 2)\n            for p in patch_embeds_list\n        ]\n    )\n    return positions\n\n\nclass VisionTransformer(nn.Module):\n    def __init__(self, args: VisionEncoderArgs):\n        super().__init__()\n        self.args = args\n        self.patch_conv = nn.Conv2d(\n            in_channels=args.num_channels,\n            out_channels=args.hidden_size,\n            kernel_size=args.patch_size,\n            stride=args.patch_size,\n            bias=False,\n        )\n        self.ln_pre = RMSNorm(args.hidden_size, eps=1e-5)\n        self.transformer = VisionTransformerBlocks(args)\n\n        head_dim = self.args.hidden_size // self.args.num_attention_heads\n        assert head_dim % 2 == 0, \"ROPE requires even head_dim\"\n        self._freqs_cis: Optional[torch.Tensor] = None\n\n    @property\n    def max_patches_per_side(self) -> int:\n        return self.args.image_size // self.args.patch_size\n\n    @property\n    def device(self) -> torch.device:\n        return next(self.parameters()).device\n\n    @property\n    def freqs_cis(self) -> torch.Tensor:\n        if self._freqs_cis is None:\n            self._freqs_cis = precompute_freqs_cis_2d(\n                dim=self.args.hidden_size // self.args.num_attention_heads,\n                height=self.max_patches_per_side,\n                width=self.max_patches_per_side,\n                theta=self.args.rope_theta,\n            )\n\n        if self._freqs_cis.device != self.device:\n            self._freqs_cis = self._freqs_cis.to(device=self.device)\n\n        return self._freqs_cis\n\n    def forward(\n        self,\n        images: List[torch.Tensor],\n    ) -> torch.Tensor:\n        \"\"\"\n        Args:\n            images: list of N_img images of variable sizes, each of shape (C, H, W)\n\n        Returns:\n            image_features: tensor of token features for all tokens of all images of\n                shape (N_toks, D)\n        \"\"\"\n        # pass images through initial convolution independently\n        patch_embeds_list = [self.patch_conv(img.unsqueeze(0)).squeeze(0) for img in images]\n\n        # flatten to a single sequence\n        patch_embeds = torch.cat([p.flatten(1).permute(1, 0) for p in patch_embeds_list], dim=0)\n        patch_embeds = self.ln_pre(patch_embeds)\n\n        # positional embeddings\n        positions = position_meshgrid(patch_embeds_list).to(self.device)\n        freqs_cis = self.freqs_cis[positions[:, 0], positions[:, 1]]\n\n        # pass through Transformer with a block diagonal mask delimiting images\n        mask = BlockDiagonalMask.from_seqlens(\n            [p.shape[-2] * p.shape[-1] for p in patch_embeds_list],\n        )\n        out = self.transformer(patch_embeds, mask=mask, freqs_cis=freqs_cis)\n\n        # remove batch dimension of the single sequence\n        return out  # type: ignore[no-any-return]\n\n\nclass VisionLanguageAdapter(nn.Module):\n    def __init__(self, in_dim: int, out_dim: int, bias: bool = True):\n        super().__init__()\n        self.w_in = nn.Linear(\n            in_dim,\n            out_dim,\n            bias=bias,\n        )\n        self.gelu = nn.GELU()\n        self.w_out = nn.Linear(out_dim, out_dim, bias=bias)\n\n    def forward(self, x: torch.Tensor) -> torch.Tensor:\n        return self.w_out(self.gelu(self.w_in(x)))  # type: ignore[no-any-return]\n\n\nclass VisionTransformerBlocks(nn.Module):\n    def __init__(self, args: VisionEncoderArgs):\n        super().__init__()\n        self.layers = torch.nn.ModuleList()\n        for _ in range(args.num_hidden_layers):\n            self.layers.append(\n                TransformerBlock(\n                    dim=args.hidden_size,\n                    hidden_dim=args.intermediate_size,\n                    n_heads=args.num_attention_heads,\n                    n_kv_heads=args.num_attention_heads,\n                    head_dim=args.hidden_size // args.num_attention_heads,\n                    norm_eps=1e-5,\n                )\n            )\n\n    def forward(\n        self,\n        x: torch.Tensor,\n        mask: BlockDiagonalMask,\n        freqs_cis: Optional[torch.Tensor],\n    ) -> torch.Tensor:\n        for layer in self.layers:\n            x = layer(x, mask=mask, freqs_cis=freqs_cis)\n        return x\n\n\nclass PatchMerger(nn.Module):\n    \"\"\"\n    Learned merging of spatial_merge_size ** 2 patches\n    \"\"\"\n\n    def __init__(\n        self,\n        vision_encoder_dim: int,\n        spatial_merge_size: int,\n    ) -> None:\n        super().__init__()\n\n        mlp_input_dim = vision_encoder_dim * (spatial_merge_size**2)\n\n        self.spatial_merge_size = spatial_merge_size\n        self.mlp_input_dim = mlp_input_dim\n\n        self.merging_layer = nn.Linear(mlp_input_dim, vision_encoder_dim, bias=False)\n\n    def forward(self, x: torch.Tensor, image_sizes: list[tuple[int, int]]) -> torch.Tensor:\n        # image_sizes specified in tokens\n        assert sum([h * w for h, w in image_sizes]) == len(x), f\"{sum([h * w for h, w in image_sizes])} != {len(x)}\"\n\n        # x is (N, vision_encoder_dim)\n        x = self.permute(x, image_sizes)\n\n        # x is (N / spatial_merge_size ** 2,\n        #       vision_encoder_dim * spatial_merge_size ** 2)\n        x = self.merging_layer(x)\n\n        # x is (N / spatial_merge_size ** 2, vision_encoder_dim)\n        return x\n\n    def permute(\n        self,\n        x: torch.Tensor,\n        image_sizes: list[tuple[int, int]],\n    ) -> torch.Tensor:\n        \"\"\"\n        Args:\n            x: (N, D) where N is flattened and concatenated patch tokens\n                for all images\n            image_sizes: list of tuple of (height, width) in tokens for\n                each image\n        Returns:\n            image_features: reorders patch tokens so each grid of\n                (spatial_merge_size, spatial_merge_size) is contiguous.\n                now (N / spatial_merge_size ** 2, D * spatial_merge_size ** 2)\n        \"\"\"\n\n        sub_grids = get_sub_grids(\n            x=x, image_sizes=image_sizes, spatial_merge_size=self.spatial_merge_size\n        )  # list of [d x sub_grid_size x sub_grid_size x n_patches]\n        permuted_tensor = [\n            grid.view(-1, grid.shape[-1]).t() for grid in sub_grids\n        ]  # n_patches x d * sub_grid_size * sub_grid_size\n        return torch.cat(permuted_tensor, dim=0)  # (N / spatial_merge_size ** 2, d * spatial_merge_size ** 2)\n\n\ndef get_sub_grids(\n    x: torch.Tensor,\n    image_sizes: list[tuple[int, int]],\n    spatial_merge_size: int,\n) -> list[torch.Tensor]:\n    # image_sizes specified in tokens\n    tokens_per_image = [h * w for h, w in image_sizes]\n    d = x.shape[-1]\n    all_img_sub_grids: list[torch.Tensor] = []\n    sub_grid_size = spatial_merge_size\n\n    for image_index, image_tokens in enumerate(x.split(tokens_per_image)):\n        # Reshape image_tokens into a 2D grid\n        h, w = image_sizes[image_index]\n        image_grid = image_tokens.view(h, w, d).permute(2, 0, 1)[None, :, :, :]  # 1 x d x h x w\n        sub_grids = torch.nn.functional.unfold(image_grid, kernel_size=sub_grid_size, stride=sub_grid_size)\n        sub_grids = sub_grids.view(\n            1, d, sub_grid_size, sub_grid_size, -1\n        )  # 1 x d x sub_grid_size x sub_grid_size x n_patches\n\n        all_img_sub_grids.append(sub_grids[0])\n\n    return all_img_sub_grids\n",
        "tests/test_generate.py": "from typing import List\n\nimport numpy as np\nimport torch\nfrom mistral_inference.args import VisionEncoderArgs\nfrom mistral_inference.generate import generate_mamba\nfrom mistral_inference.main import generate\nfrom mistral_inference.mamba import Mamba, MambaArgs\nfrom mistral_inference.transformer import Transformer, TransformerArgs\n\n\nclass DebugTokenizer:\n    @property\n    def bos_id(self) -> int:\n        return 0\n\n    @property\n    def eos_id(self) -> int:\n        return 1\n\n    @property\n    def pad_id(self) -> int:\n        return -1\n\n    def encode(self, s: str, bos: bool = True) -> List[int]:\n        assert isinstance(s, str)\n        t = [int(x) for x in s.split()]\n        if bos:\n            t = [self.bos_id, *t]\n        return t\n\n    def decode(self, t: List[int]) -> str:\n        return \" \".join([str(x) for x in t])\n\n\ndef test_generation_transformer() -> None:\n    torch.manual_seed(42)\n\n    sequences = [\"1 2 3 4 5 6 7\", \"0 1 2\", \"12 13 14\", \"2 4 34\"]\n    args = TransformerArgs(\n        dim=512,\n        n_layers=1,\n        head_dim=128,\n        hidden_dim=2048,\n        n_heads=4,\n        n_kv_heads=2,\n        norm_eps=1e-5,\n        vocab_size=32_000,\n        max_batch_size=len(sequences),\n    )\n    model = Transformer(args).to(\"cuda\", dtype=torch.float32)\n    tokenizer = DebugTokenizer()\n\n    encoded = [tokenizer.encode(s, bos=True) for s in sequences]\n    toks, all_logprobs_old = generate(encoded, model, temperature=0.0, max_tokens=7)\n\n    # concat generated and prompt\n    encoded = [e + t for e, t in zip(encoded, toks)]\n\n    generated, all_logprobs_new = generate(encoded, model, temperature=0.0, max_tokens=0)\n\n    assert generated == []\n\n    # Verify that logprobs are the same\n    assert len(sequences) == len(all_logprobs_old) == len(all_logprobs_new)\n    for lp_old, lp_new in zip(all_logprobs_old, all_logprobs_new):\n        assert all([abs(x - y) < 5e-4 for x, y in zip(lp_old, lp_new)]), f\"\\n{lp_old}\\n{lp_new}\"\n\n    print(\"All tests passed.\")\n\n\ndef test_generation_pixtral() -> None:\n    torch.manual_seed(42)\n    gen = np.random.default_rng(seed=42)\n\n    sequences = [\"1 2 2 2 2 4 5 6 7\", \"12 13 14\", \"2 2 2 2 7 8 9\"]\n    images = [[gen.normal(size=(3, 4, 4))], [], [gen.normal(size=(3, 4, 4))]]\n    args = TransformerArgs(\n        dim=512,\n        n_layers=1,\n        head_dim=128,\n        hidden_dim=2048,\n        n_heads=4,\n        n_kv_heads=2,\n        norm_eps=1e-5,\n        vocab_size=32_000,\n        max_batch_size=len(sequences),\n        vision_encoder=VisionEncoderArgs(\n            hidden_size=128,\n            num_channels=3,\n            image_size=4,\n            patch_size=2,\n            intermediate_size=256,\n            num_hidden_layers=1,\n            num_attention_heads=2,\n            rope_theta=10000,\n            image_token_id=2,\n        ),\n    )\n    model = Transformer(args).to(\"cuda\", dtype=torch.float32)\n    tokenizer = DebugTokenizer()\n\n    encoded = [tokenizer.encode(s, bos=True) for s in sequences]\n    toks, all_logprobs_old = generate(encoded, model, images=images, temperature=0.0, max_tokens=7)\n\n    # concat generated and prompt\n    encoded = [e + t for e, t in zip(encoded, toks)]\n\n    generated, all_logprobs_new = generate(encoded, model, images=images, temperature=0.0, max_tokens=0)\n\n    assert generated == []\n\n    # Verify that logprobs are the same\n    assert len(sequences) == len(all_logprobs_old) == len(all_logprobs_new)\n    for lp_old, lp_new in zip(all_logprobs_old, all_logprobs_new):\n        assert all([abs(x - y) < 5e-4 for x, y in zip(lp_old, lp_new)]), f\"\\n{lp_old}\\n{lp_new}\"\n\n    print(\"All tests passed.\")\n\n\ndef test_generation_pixtral_patch_merger() -> None:\n    torch.manual_seed(42)\n    gen = np.random.default_rng(seed=42)\n\n    sequences = [\"1 2 2 2 2 4 5 6 7\", \"12 13 14\", \"2 2 2 2 7 8 9\"]\n    images = [[gen.normal(size=(3, 8, 8))], [], [gen.normal(size=(3, 8, 8))]]\n    args = TransformerArgs(\n        dim=512,\n        n_layers=1,\n        head_dim=128,\n        hidden_dim=2048,\n        n_heads=4,\n        n_kv_heads=2,\n        norm_eps=1e-5,\n        vocab_size=32_000,\n        max_batch_size=len(sequences),\n        vision_encoder=VisionEncoderArgs(\n            hidden_size=128,\n            num_channels=3,\n            image_size=8,\n            patch_size=2,\n            intermediate_size=256,\n            num_hidden_layers=1,\n            num_attention_heads=2,\n            rope_theta=10000,\n            image_token_id=2,\n            adapter_bias=False,\n            spatial_merge_size=2,\n            add_pre_mm_projector_layer_norm=True,\n            mm_projector_id=\"patch_merge\",\n        ),\n    )\n    model = Transformer(args).to(\"cuda\", dtype=torch.float32)\n    tokenizer = DebugTokenizer()\n\n    encoded = [tokenizer.encode(s, bos=True) for s in sequences]\n    toks, all_logprobs_old = generate(encoded, model, images=images, temperature=0.0, max_tokens=7)\n\n    # concat generated and prompt\n    encoded = [e + t for e, t in zip(encoded, toks)]\n\n    generated, all_logprobs_new = generate(encoded, model, images=images, temperature=0.0, max_tokens=0)\n\n    assert generated == []\n\n    # Verify that logprobs are the same\n    assert len(sequences) == len(all_logprobs_old) == len(all_logprobs_new)\n    for lp_old, lp_new in zip(all_logprobs_old, all_logprobs_new):\n        assert all([abs(x - y) < 5e-4 for x, y in zip(lp_old, lp_new)]), f\"\\n{lp_old}\\n{lp_new}\"\n\n    print(\"All tests passed.\")\n\n\ndef test_generation_mamba() -> None:\n    torch.manual_seed(42)\n\n    sequences = [\"1 2 3 4 5 6 7\"]\n    args = MambaArgs(\n        dim=512,\n        n_layers=1,\n        n_groups=1,\n        rms_norm=True,\n        residual_in_fp32=True,\n        fused_add_norm=True,\n        pad_vocab_size_multiple=1,\n        tie_embeddings=False,\n        vocab_size=32768,\n    )\n    model = Mamba(args).to(\"cuda\", dtype=torch.float32)\n    tokenizer = DebugTokenizer()\n\n    encoded = [tokenizer.encode(s, bos=True) for s in sequences]\n    toks, all_logprobs_old = generate_mamba(encoded, model, temperature=0.0, max_tokens=7)\n\n    assert len(toks[0]) == 7\n    assert toks == [[25574, 14821, 11843, 23698, 12735, 23522, 27542]]\n\n\ndef test_chunks_transformer() -> None:\n    torch.manual_seed(42)\n\n    sequences = [\n        \" \".join([str(i) for i in range(7)]),\n        \" \".join([str(i) for i in range(9, 0, -1)]),\n    ]\n    args = TransformerArgs(\n        dim=512,\n        n_layers=1,\n        head_dim=128,\n        hidden_dim=2048,\n        n_heads=4,\n        n_kv_heads=2,\n        norm_eps=1e-5,\n        vocab_size=32_000,\n        max_batch_size=3,\n    )\n    model = Transformer(args).to(\"cuda\", dtype=torch.float32)\n    tokenizer = DebugTokenizer()\n\n    encoded = [tokenizer.encode(s, bos=True) for s in sequences]\n    toks, all_logprobs_old = generate(encoded, model, temperature=0.0, max_tokens=8)\n\n    # concat generated and prompt\n    encoded = [e + t for e, t in zip(encoded, toks)]\n\n    generated, all_logprobs_new = generate(encoded, model, temperature=0.0, max_tokens=0, chunk_size=5)\n    assert len(generated) == 0\n\n    for lp_old, lp_new in zip(all_logprobs_old, all_logprobs_new):\n        assert all([abs(x - y) < 5e-4 for x, y in zip(lp_old, lp_new)]), f\"\\n{lp_old}\\n{lp_new}\"\n"
    }
}