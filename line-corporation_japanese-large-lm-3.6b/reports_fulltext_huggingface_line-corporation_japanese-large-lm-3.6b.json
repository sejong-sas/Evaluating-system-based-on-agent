{
  "model_id": "line-corporation/japanese-large-lm-3.6b",
  "full_texts": [
    {
      "arxiv_id": "https://engineering.linecorp.com/ja/blog/3.6-billion-parameter-japanese-language-model",
      "full_text": " 36åãã©ã¡ã¼ã¿ã®æ¥æ¬èªè¨èªã¢ãã«ãå ¬éãã¾ãã Open/Close Blog Interview Culture Open Source Careers æ¥æ¬èª æ¥æ¬èª Search Search Search Close LINEæ ªå¼ä¼ç¤¾ã¯ã2023å¹´10æ1æ¥ã«LINEã¤ãã¼æ ªå¼ä¼ç¤¾ã«ãªãã¾ãããLINEã¤ãã¼æ ªå¼ä¼ç¤¾ã®æ°ãããã­ã°ã¯ãã¡ãã§ãã LINEã¤ãã¼ Tech Blog Blog 36åãã©ã¡ã¼ã¿ã®æ¥æ¬èªè¨èªã¢ãã«ãå ¬éãã¾\u0000\u0000ãã Shun Kiyono, Sho Takase, Toshinori Sato(overlast) 2023-08-14 NLP Foundation Dev Team ããã«ã¡ã¯ã LINEã®NLP Foundation Devãã¼ã ã®æ¸ éèã¨é«ç¬ç¿ã¨overlastã§ãã LINEã§ã¯2020å¹´11æãã æ¥æ¬èªã«ç¹åããå¤§è¦æ¨¡è¨èªã¢ãã«ãHyperCLOVAãã®æ§ç¯ã¨å¿ç¨ã«é¢ããç ç©¶éçº ã«åãçµãã§ãã¾ãããããã®ãHyperCLOVAãã¨ä¸¦è¡ããããã¡ã§è¤æ°ã®å¤§è¦æ¨¡è¨èªã¢ãã«ã®ç ç©¶éçºãã­ã¸ã§ã¯ããé²è¡ãã¦ãã¾ãã ä»åã¯ãããã®ç ç©¶éçºãã­ã¸ã§ã¯ãã®ãã¡ãæã ãå«ãMassive LMéçºã¦ããããããæ¥æ¬èªè¨èªã¢ãã«ãjapanese-large-lmï¼ã¸ã£ããã¼ãº ã©ã¼ã¸ ã¨ã«ã¨ã ï¼ããOSSã¨ãã¦å ¬éã§ããç¶æ³ã«ãªãã¾ããã®ã§ãæ¬ãã­ã°ãéãã¦ãä¼ããããã¨ã«ãã¾ããã ãã®è¨äºã§ã¯ãæã ãè¨ç·´ã»å ¬éãã36åï¼3.6 Billionï¼ããã³ 17åï¼1.7 Billionï¼ãã©ã¡ã¼ã¿ ã®æ¥æ¬èªè¨èªã¢ãã«ï¼ä»¥ä¸ã ãããã3.6Bã¢ãã«ã1.7Bã¢ãã«ã¨å¼ã³ã¾ãï¼ ãç´¹ä»ãã¤ã¤ãéä¸­ã§å¾ãããè¨èªã¢ãã«æ§ç¯ã®ãã¦ãã¦ãå ±æãã¾ãã ä½¿ãæ¹ 1.7Bã¢ãã«ã3.6Bã¢ãã«å ±ã«HuggingFace Hubã®ä»¥ä¸ã®URLã«ããã¦å ¬éãã¦ããã transformersã©ã¤ãã©ãª ããå©ç¨å¯è½ã§ãã ã©ã¤ã»ã³ã¹ã¯åç¨å©ç¨ãå¯è½ãªApache License 2.0ã¨ãã¾ããã®ã§ãç ç©¶è ã®æ¹ã ãã§ã¯ãªããä¼æ¥­ã®æ¹ã«ãæ§ã ãªç¨éã§ãå©ç¨é ãã¾ãã https://huggingface.co/line-corporation/japanese-large-lm-1.7b https://huggingface.co/line-corporation/japanese-large-lm-3.6b ä»¥ä¸ã®ãããªã³ã¼ãã§çæãè©¦ããã¨ãã§ãã¾ãã import torch from transformers import AutoModelForCausalLM, AutoTokenizer, pipeline, set_seed model = AutoModelForCausalLM.from_pretrained(\"line-corporation/japanese-large-lm-3.6b\", torch_dtype=torch.float16) # float16ã¯æå®ããªãã¦ãåé¡ããã¾ãã tokenizer = AutoTokenizer.from_pretrained(\"line-corporation/japanese-large-lm-3.6b\", use_fast=False) # use_fast=False ã¯å¿ ãä»ä¸ãã¦ãã ããããªãã¦ãåãã¾ãããæã ã®å­¦ç¿ç¶æ³ã¨ã¯ç°ãªãã®ã§æ§è½ãä¸ããã¾ãã generator = pipeline(\"text-generation\", model=model, tokenizer=tokenizer, device=0) set_seed(101) text = generator( \"ãã¯ãããããã¾ããä»æ¥ã®å¤©æ°ã¯\", max_length=30, do_sample=True, pad_token_id=tokenizer.pad_token_id, num_return_sequences=5, ) for t in text: print(t) # ä¸è¨ã¯çæãããåºåã®ä¾ # [{'generated_text': 'ãã¯ãããããã¾ããä»æ¥ã®å¤©æ°ã¯é¨æ¨¡æ§ã§ãã­ãæ¢ é¨ã®ãã®ææã® æã¯æ´æ¿¯ç©ãä¹¾ãã«ãããªã©ãä¸»å©¦ã«ã¨ã£ã¦ã¯æ©ã¿ã©ããã§ãã­ã ã§ã¯ã'}, # {'generated_text': 'ãã¯ãããããã¾ããä»æ¥ã®å¤©æ°ã¯æ´ãã æ°æ¸©ã¯8&deg;Cä½ã§ãã ææ©ã¯çµæ§å·ãè¾¼ãããã«ãªãã¾ããã å¯ããªã£ã¦ããã¨ã...'}, # {'generated_text': 'ãã¯ãããããã¾ããä»æ¥ã®å¤©æ°ã¯æãã§ãã æèµ·ãããéªãè»½ãç©ãã£ã¦ããã å¯ããããã»ã©ã§ãããã¾ããã æ¥ä¸­ã¯æ´ããã¿ããã§ãã­ã'}, # {'generated_text': 'ãã¯ãããããã¾ããä»æ¥ã®å¤©æ°ã¯âã®ã¡âã§ãã æã®æ°æ¸©5&deg;Cãæ¥ä¸­ã21&deg;Cã¨ æããäºå ±ã§ã'}, # {'generated_text': 'ãã¯ãããããã¾ããä»æ¥ã®å¤©æ°ã¯æ´å¤©ã§ããæ¶¼ãã1æ¥ã§ããæ°æ¸©ã¯åå¾ã«ãªãä½ããªã25&deg;Cããããé¢¨ãå¼·ãããã§ãã®ã§ã'}] ãã®æ¥æ¬èªè¨èªã¢ãã«ã®ç¹å¾´ è¶ å¤§è¦æ¨¡ï¼é«åè³ªãªè¨ç·´ãã¼ã¿ã®æ´»ç¨ å¤§è¦æ¨¡ãã¤é«åè³ªãªãã¼ã¿ã¯æ§è½ã®è¯ãè¨èªã¢ãã«ã®å­¦ç¿ã«ã¯å¿ è¦ä¸å¯æ¬ ã§ãã ä»åãã¢ãã«ã®è¨ç·´ã«ã¯LINEç¬èªã®å¤§è¦æ¨¡æ¥æ¬èªWebã³ã¼ãã¹ãå©ç¨ãã¦ãã¾ãã Webç±æ¥ã®ãã­ã¹ãã«ã¯ ã½ã¼ã¹ã³ã¼ããéæ¥æ¬èªæã®ãããªãã¤ãº ãå¤§éã«å«ã¾ãã¦ããããããã£ã«ã¿ãªã³ã°å¦çãé©ç¨ããå¤§è¦æ¨¡ãã¤é«åè³ªãªãã¼ã¿ã®æ§ç¯ããããªã£ã¦ãã¾ãï¼ãã£ã«ã¿ãªã³ã°ã«ããåè³ªãåä¸ãããå¹æã«ã¤ãã¦ã¯å¾è¿°ãã¾ãï¼ã ãªãããã£ã«ã¿ãªã³ã°å¦çã«ã¯NLPãã¼ã ã®ã¡ã³ãã¼ãéçºãã OSSã©ã¤ãã©ãªã® &nbsp; HojiChar &nbsp; ãå©ç¨ãã¦ãã¾ãã æçµçãªå­¦ç¿ã«ã¯ç´650 GBã®ã³ã¼ãã¹ ãå©ç¨ãã¦ãã¾ãããè±èªã®å¤§è¦æ¨¡ã³ã¼ãã¹ã¨ãã¦ä¸è¬çã«ç¨ãããã¦ãããã®ï¼ Pileã³ã¼ãã¹ ï¼ãç´800GBã§ãããã¨ãè¸ã¾ããã¨ãæã ã®ãã¼ã¿ãéè²ãªãå¤§ããã§ããã¨è¨ãã¾ãã å¹ççãªå®è£ ã®æ´»ç¨ å¤§éã®ãã¼ã¿ãè¨èªã¢ãã«ã®å­¦ç¿ã«åãå ¥ããããã«ã¯ãç¸å¿ã®è¨ç®è³æºã ãã§ã¯ãªããè¨ç®æ©ãå¹ççã«å©ç¨ããããã®ãã¯ããã¯ã¨ãã®å®è£ ãå¿ è¦ä¸å¯æ¬ ã§ãã æã ã¯ã 3D Parallelism ã Activation Checkpointing ã¨ãã£ããã¯ããã¯ãé§ä½¿ãããã¨ã«ãã£ã¦ãããããããµã¤ãºãå·¨å¤§åããã¢ãã«ã®å­¦ç¿ãé«éåãã¦ãã¾ãããããã¯ä¾ãã° Megatron-DeepSpeed ã«å®è£ ããã¦ãã¾ãã&nbsp; æ¬ã¢ãã«ã®æ§ç¯ã«è¦ããæéã«ã¤ãã¦ãä¾ãã°1.7Bã¢ãã«ã«ã¤ãã¦ã¯A100 80GBã§æç®ããç´4000GPUæéãè²»ããã¦ãã¾ããå­¦ç¿æéã¯ç¹ã«æ¥æ¬èªã®å¤§è¦æ¨¡è¨èªã¢ãã«ã®å­¦ç¿ã§ã¯å ¬éããã¦ããªããã¨ãå¤ããé©åãªæ¯è¼ã¯ã§ãã¾ããããä¾ãã° rinna 0.3Bã¢ãã«ã®å­¦ç¿ã¯ V100 32GBã§ç´8600GPUæéãè²»ããã¦ãã ããã§ãè²»ãããæéã«æ¯ãã¦å¹çã®è¯ãå­¦ç¿ãè¡ãã¦ããã¨èãããã¾ãã HyperCLOVAã¨ã¯ç°ãªãã¢ãã«æ§ç¯ãã­ã»ã¹ ä»åã®æ¥æ¬èªè¨èªã¢ãã«ã¯ HyperCLOVAã¨ã¯å¥ã®éçºã©ã¤ã³ã§ä¸¦è¡ãã¦æ§ç¯ãã¦ãããã®ã§ã LINEã®Massive LMéçºã¦ããããä¸è²«ãã¦æ§ç¯ãæ å½ãã¦ãã¾ãã LINEã®å¤§è¦æ¨¡è¨èªã¢ãã«ã®æ§ç¯ã»éç¨ã»å¿ç¨ã«é¢ããå ¨ã¦ã®çµé¨ãç¥è¦ãåæ ãã¦ãè¯è³ªãªæ¥æ¬èªåãã®äºåå­¦ç¿æ¸ã¿ã¢ãã«ãç¶ç¶çã«æ§ç¯ãã¾ãã å­¦ç¿ããã¢ãã«ã®æ§è½ å­¦ç¿ããã¢ãã«ã®æ§è½è©ä¾¡ã¨ãã¦ãéçºãã¼ã¿ã§ã®Perplexityã¹ã³ã¢ï¼PPLï¼ããã³ãè³ªåå¿ç­ã»èª­è§£ã¿ã¹ã¯ã®æ­£è§£çãè¨æ¸¬ãã¾ãããPPLã¯ã³ã¼ãã¹ä¸­ã«åºç¾ããåèªãã¢ãã«ãã©ã®ç¨åº¦æ­£ç¢ºã«äºæ¸¬ã§ããããç¤ºãå¤ã§ãå°ããã»ã©æ§è½ãè¯ããããªãã¡ãæ­£ç¢ºã«äºæ¸¬ã§ãã¦ãããã¨ãè¡¨ãã¾ãããªããã¢ãã«ã®PPLã¨ã¢ãã«ãfinetuningããã¨ãã®æ§è½ã«ã¯ç¸é¢ããããã¨ãçµé¨çã«ç¤ºããã¦ããï¼ä¾ï¼ Switch Transformersè«æã®Appendix E ï¼PPLãè¯ãã¢ãã«ã¯finetuningæã®æ§è½ãè¯ããã¨ãæå¾ ããã¾ãã PPLã®è¨æ¸¬ã«ã¯ C4ãã¼ã¿ã»ãã ããéçºãã¼ã¿ã¨ãã¦ä½æãããã®ãä½¿ç¨ããè³ªåå¿ç­ã»èª­è§£ã¿ã¹ã¯ã«ã¤ãã¦ã¯ AIç ãã¯ã¤ãºAIæ¥æ¬ä¸æ±ºå®æ¦ã ã§é å¸ããã¦ãã éçºãã¼ã¿ ããã³ JSQuADã®éçºãã¼ã¿ ãç¨ãã¾ãããè³ªåå¿ç­ã»èª­è§£ã«ããã¦ã¯ãAIçã§ã¯Zero-shotã§ãJSQuADã«ã¤ãã¦ã¯ Stability AIã«ããevaluation-harnessã®å®è£ ãå©ç¨ã3-shotã®è¨­å®ã§è©ä¾¡ãã¾ããã PPLã¨æ­£è§£çãä»¥ä¸ã®è¡¨ã«è¨ãã¾ãã åèã¨ãã¦2023å¹´7æç¾å¨ãæ¥æ¬èªè¨èªã¢ãã«ã¨ãã¦åºãä½¿ããã¦ããã§ããããRinna-3.6Bï¼ rinna/japanese-gpt-neox-3.6b ï¼ã¨OpenCALM-7Bï¼ cyberagent/open-calm-7b ï¼ã®æ§è½ãè¨ãã¦ãã¾ããã¾ãã æ¥æ¬èªã¨è±èªã®ãã¤ãªã³ã¬ã«è¨èªã¢ãã« ï¼ rinna/bilingual-gpt-neox-4b ï¼ã®æ§è½ãè¨ãã¾ããã ã¢ãã«å ãã©ã¡ã¼ã¿æ° C4 (PPL) AIçï¼æ­£è§£çï¼ JSQuADï¼æ­£è§£çï¼ line-corporation/japanese-large-lm-1.7b 1.7B 8.57 32.2 57.50 line-corporation/japanese-large-lm-3.6b 3.6B 7.50 48.5 63.85 rinna/japanese-gpt-neox-3.6b 3.6B 8.18 48.6 50.09 rinna/bilingual-gpt-neox-4b 3.8B 8.83 43.0 52.72 cyberagent/open-calm-7b 6.9B 11.87 35.4 48.13 ä¸è¨ãããæã ã®1.7Bã¢ãã«ã¯OpenCALM-7Bã¨åç­ãã¿ã¹ã¯ã«ãã£ã¦ã¯è¯ãæ§è½ãéæãã3.6Bã¢ãã«ã¯Rinna-3.6Bã¨åç­ãã¿ã¹ã¯ã«ãã£ã¦ã¯è¯ãæ§è½ãéæå¯è½ãªãã¨ãåããã¾ãã è¨ç·´ãã¼ã¿ã®åè³ªãåä¸ãããå¹æã«ã¤ãã¦ãWebã³ã¼ãã¹ã«å¤ãå­å¨ããéæãéè¤ãã¦ããæãæ¥æ¬èªä»¥å¤ã®è¨èªã®ææ¸ã®é¤å»ãªã©ãè¡ããã¨ã§ãä¸è¨ã®ãããªå®éææ¨ã§ã®æ§è½ã¨çæããæã®åè³ªãåä¸ãã¾ãã è¨ç·´ãã¼ã¿ã«ãã®ãããªãã£ã«ã¿ãªã³ã°ãè¡ããã«å­¦ç¿ããå ´åãä¾ãã°ãåèçè¹æ©å¸ã®ä¸æ¸å»ºã¦ãå£²ã åèçåèå¸ä¸­å¤®åºã®ä¸æ¸å»ºã¦ãå£²ã åèå¸è±è¦å·åºã®ä¸æ¸å»ºã¦ãå£²ã åèå¸ç¨²æ¯åºã®ä¸æ¸å»ºã¦ãå£²ã&hellip;ãã¨ãããããªç¹°ãè¿ããå¤ãå«ãæãéæãçæãããããªã£ã¦ãã¾ãã¾ãã å¤§è¦æ¨¡è¨èªã¢ãã«å­¦ç¿Tips: å®å®ããå­¦ç¿ã§è¯ãã¢ãã«ãå¾ãããã« ãã©ã¡ã¼ã¿ã®åæå¤ã«ã¤ãã¦ ã¢ãã«ãµã¤ãºãå¤§ããããå ´åãå­¦ç¿ãå®å®ãããããã«ã¯ã¢ãã«ãã©ã¡ã¼ã¿ã® åæå¤ ãå°ãããã¦ããå¿ è¦ãããã¾ãã ä»åã¯ BigScienceã®ã¬ãã¼ã ãåèã«ã(2.0 / (é ãå±¤ã®æ¬¡å æ° * 5)) ** 0.5 ã¨ããå¤ãæ¡ç¨ãã¦ãã¾ãã ã¾ããä¸é¨ã®ãã©ã¡ã¼ã¿ã«ã¤ãã¦ã¯ã¢ãã«ã®å±¤ã®æ°ã«å¿ãã¦åæåããæ¹ãè¯ãã¨ããè­°è«ã GPT-2ã®è«æ ã Megatron-LMã®è«æ ã§ãªããã¦ãããä»åã¯Megatron-LMã§è¡ããã¦ããå±¤ã®æ°ã«å¿ããåæå¤ã®ãªã¹ã±ã¼ãªã³ã°ãæ¡ç¨ãã¾ãããçè«çãªæ­£å½æ§ã¯ä¸æã§ãããMegatron-LMã®æ¡ç¨ãã¦ãããªã¹ã±ã¼ãªã³ã°ã¯çµé¨çã«ä»ã®åæåãããæ§è½ãé«ãå¾åãç¢ºèªãã¦ãã¾ãã Adam ã® beta2 ã«ã¤ãã¦ ä»åã®ã¢ãã«æ§ç¯ã§ã¯åºãå©ç¨ããã¦ãã Adam ãä½¿ã£ã¦ãã¾ãã Adamã«ã¯å­¦ç¿çã®ä»ã«beta1, beta2ã¨ãããã¤ãã¼ãã©ã¡ã¼ã¿ããããbeta2ã¯å¤§ããªå¤ã«è¨­å®ããã¨æ§è½ã¯ä¸ãããå­¦ç¿ã¯ä¸å®å®ã«ãªãããããå°ããå¤ã«è¨­å®ããã¨å­¦ç¿ã¯å®å®ãããæ§è½ã¯ä¸ããã¥ããï¼ï¼å­¦ç¿ã®é²ã¿ãé ããªãï¼ã¨ããæ§è³ªãããã¾ããAdamã®è«æã§æ¡ç¨ããã¦ããbeta2ã®å¤ã¯0.999ã§ããããããä½¿ã£ã¦ããã¨ä¸è¨ã®ããã«æå¤±ãè·³ã­ä¸ãããã¨ãã¾ã¾ããã¾ãã ä¸è¨ã®ä¾ã§ã¯æ´æ°åæ°ãå¢ãããéã«ãéè¯ãæå¤±ãä½ãå¤ã«æ»ã£ã¦ãã¦ãã¾ããããã®ã¾ã¾çºæ£ãã¦å­¦ç¿ã«å¤±æããå ´åãããã¾ãã ãããé²ãããã«ããã©ã«ãå¤ãããå°ããªå¤ãä½¿ã£ã¦ãã¾ãã Transformerã®è«æ ã§ã¯0.98ãä½¿ããã¦ãããå¤§è¦æ¨¡è¨èªã¢ãã«ã®å­¦ç¿ã§ã¯ BigScienceã®è©¦è¡é¯èª¤ã§ã®è­°è« ã«ãããããã«0.95ãä½¿ããããã¨ãå¤ãããã§ãã å­¦ç¿çã«ã¤ãã¦ å­¦ç¿çã¯é«ãã»ã©å­¦ç¿ãæ©ããªãï¼ï¼æ§è½ãé«ããªãï¼ã®ã§ãããæå¤±ãçºæ£ããå¯è½æ§ãé«ã¾ãã¾ãã ããã¤ãæ¢å­ã®è¨­å®ãè¦ã¦ã¿ãææã¨ãã¦ã¯ã Megatron-LM ã®1.5e-4ã¨ããè¨­å®ããã®è¦æ¨¡ã®ã¢ãã«ã®å­¦ç¿ã«ã¯ã¡ããã©è¯ãããã§ãã Metaã®å ¬éãã¦ããå¤§è¦æ¨¡è¨èªã¢ãã«ã LLaMA ã®è«æã«ããã¦ã¯13Bãã©ã¡ã¼ã¿ã®ã¢ãã«ã¾ã§ã¯3.0e-4ã®å­¦ç¿çãç¨ãã¦ãã¾ãããæã ã®æå ã§ã¯ããã¾ã§é«ãå­¦ç¿çãç¨ããå ´åã«ã¯çºæ£ãã¦ãã¾ãã¾ãããã¢ãã«ã®ä»æ§ãè¨ç·´ãã¼ã¿ãã©ã®ç¨åº¦ã¯ãªã¼ãã³ã°ãã¦ãããã«ãã£ã¦ãå¾åãå¤ããã¨èãããã¾ãããä¸»ã«è±èªã³ã¼ãã¹ãç¨ããéã®å­¦ç¿çãããä½ãå¤ã«è¨­å®ããæ¹ãçµé¨çã«ã¯å®å®ãã¾ãã å­¦ç¿çã®ã¹ã±ã¸ã¥ã¼ã©ã«ã¤ãã¦ Transformerã®è«æ ã§ã¯æ´æ°åæ°ã®éå¹³æ¹æ ¹ã§å­¦ç¿çãå¤åãããææ³ãæ¡ç¨ããã¦ãã¾ããããå¤§è¦æ¨¡è¨èªã¢ãã«ã®å­¦ç¿ã§ã¯Cosineã¹ã±ã¸ã¥ã¼ã©ãåºãä½¿ããã¦ãã¾ãï¼ä¾ï¼ Chinchillaã¢ãã«ã®è«æ ï¼ çµé¨çã«ã¯ä»¥ä¸ã®ãããªå¾åããããåä¸ã®å­¦ç¿çããã¤å­¦ç¿ã«å¤±æããªããã°Cosineã¹ã±ã¸ã¥ã¼ã©ã®æ¹ãåªç§ã§ãã æ´æ°åæ°ã®éå¹³æ¹æ ¹ï¼å­¦ç¿çãæ¥æ¿ã«ä¸ãããå­¦ç¿ã®å®å®æ§ãé«ããæçµçãªæ§è½ã¯Cosineãããä½ã Cosineã¹ã±ã¸ã¥ã¼ã©ï¼éå¹³æ¹æ ¹ã®å ´åã¨æ¯è¼ãã¦ç¸å¯¾çã«å­¦ç¿çãé«ãæéãé·ããããæå¤±ãçºæ£ããå¯è½æ§ã®é«ãæéãé·ãããã®ä¸æ¹ã§ãå­¦ç¿ã«æåããå ´åã«ã¯æ§è½ãé«ã ãããã« ä»åå ¬éãã¾ãã1.7Bã3.6Bã®æ¥æ¬èªè¨èªã¢ãã«ãçæ§ã«åºãä½¿ã£ã¦é ããã°å¹¸ãã§ãã ã¾ãããããã®ã¢ãã«ã«ã¤ãã¦ãæç¤ºæã«å¯¾ãã¦é©åãªåºåãè¡ããããã«ãã¥ã¼ãã³ã°ï¼Instruction tuningï¼ããã¢ãã«ãè¿æ¥ä¸­ã«å ¬éäºå®ã§ããç¶å ±ã¯ @LINE_DEV ããã©ã­ã¼ãã¦ãå¾ ã¡ä¸ããã LINE ã¯ä»å¾ãæ§ç¯ããã¢ãã«ã®ä¸é¨ãç¶ç¶çã«å ¬éãã¾ãã®ã§ãã©ããæ¥½ãã¿ã«ãå¾ ã¡ãã ããã NLP language models Tags Â© LY Corporation Privacy Policy Family sites LINE LINE Developers LINE CAREERS [KR]Careers [KR]Careers [KR]Careers [KR]Careers [KR]Careers [KR]Careers [KR]Careers [KR]Careers [KR]Careers [KR]Careers [KR]Careers [KR]Careers "
    }
  ]
}