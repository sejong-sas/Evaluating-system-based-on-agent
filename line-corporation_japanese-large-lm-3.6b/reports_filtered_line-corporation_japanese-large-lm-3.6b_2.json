{
  "1-5 (Architecture)": "The only architectural detail explicitly given is the parameter count: the author states that they have trained and publicly released two Japanese language models—a 3.6 billion-parameter model (referred to as the “3.6Bモデル”) and a 1.7 billion-parameter model (“1.7Bモデル”). No other layers, dimensions, or hyper-parameter values are disclosed in the provided material.",
  "1-6 (Tokenizer)": "A concrete code snippet specifies that users should load the tokenizer with\n   tokenizer = AutoTokenizer.from_pretrained(\"line-corporation/japanese-large-lm-3.6b\", use_fast=False)\nThe example makes three points: (1) the tokenizer can be obtained directly from the Hugging Face Hub under the exact repository name “line-corporation/japanese-large-lm-3.6b”; (2) it is compatible with the AutoTokenizer API; and (3) the flag use_fast=False is strongly recommended—even though the tokenizer will still function without it, omitting the flag is said to yield degraded performance because it no longer matches the author’s original training environment.",
  "2-1 (Hardware)": "",
  "2-2 (Software)": "",
  "1-5 (Architecture)__evidence": [
    {
      "source": "[pdf_text]",
      "quote": "この記 事では、私 が訓練・公開した36億（3.6 Billion）および 17億（1.7 Billion）パラメータ の日本語言語モデル（以下、それぞれ3.6Bモデル、1.7Bモデルと呼びます）を紹介しつつ、途中で得られた言語モデル構築のノウハウを報有します。"
    }
  ],
  "1-6 (Tokenizer)__evidence": [
    {
      "source": "[pdf_text]",
      "quote": "tokenizer = AutoTokenizer.from_pretrained(\"line-corporation/japanese-large-lm-3.6b\", use_fast=False) # use_fast=False は必ず付与してください。なくても動きますが、私の学習状況とは異なるので性能が下がります。"
    }
  ],
  "2-1 (Hardware)__evidence": [],
  "2-2 (Software)__evidence": []
}