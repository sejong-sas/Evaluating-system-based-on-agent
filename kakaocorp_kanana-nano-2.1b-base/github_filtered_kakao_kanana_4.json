{
  "4-1 (Pre-training Data)": "The materials reviewed contain no sentences that mention Kanana’s pre-training corpus, the size or makeup of that corpus, the geographic or linguistic origin of the data, licensing constraints, or any other empirical details. Consequently, there is no publicly disclosed information in the supplied quotations about the types of data fed into the base Kanana model, their proportions, sources (e.g., web crawl, books, code, or proprietary corpora), or any stated rationale for selecting or excluding particular subsets. In short, the quotations provide no visibility into Kanana’s pre-training data pipeline, so no further summary can be given beyond noting the complete absence of disclosures.",
  "4-1 (Pre-training Data)__evidence": [],
  "4-2 (Fine-tuning Data)": "The only disclosure regarding fine-tuning comes from the sentence: “Furthermore, the report outlines the methodologies utilized during the post-training of the Kanana models, encompassing supervised fine-tuning and preference optimization, aimed at enhancing their capability for seamless interaction with users.” From this we can infer that (i) Kanana undergoes a supervised fine-tuning phase after base pre-training, (ii) an additional preference-optimization stage is applied—most likely learning from human-labeled examples to improve response quality—and (iii) the overall goal of these stages is to improve user interaction quality. No concrete dataset names, sizes, domain composition, licensing status, or public-availability claims are made. The quote does not specify how many examples were used, which languages or content types were represented, or whether any proprietary or open data sources were employed. Thus, the fine-tuning data remain entirely opaque apart from confirmation that supervised and preference-based post-training exist in the Kanana development lifecycle.",
  "4-2 (Fine-tuning Data)__evidence": [
    {
      "source": "[readme]",
      "quote": "Furthermore, the report outlines the methodologies utilized during the post-training of the Kanana models, encompassing supervised fine-tuning and preference optimization, aimed at enhancing their capability for seamless interaction with users."
    }
  ],
  "4-3 (Reinforcement Learning Data)": "Reinforcement-learning information is limited to the sentence: “Furthermore, `Kanana-1.5-15.7B-A3B` is powered by our newly enhanced post-training strategy, which includes on-policy distillation followed by reinforcement learning.” This statement indicates that at least the Kanana 1.5 series employs a two-stage post-training routine: first an on-policy distillation step that aligns the model to a target policy, and second a reinforcement-learning (RL) phase. The wording suggests that RL data consist of trajectories or preferences produced on-policy—i.e., gathered while the model interacts under its current policy. However, no explicit description of the RL dataset—such as the number of dialog turns, human raters, reward models, or content safety screens—is provided. The quote does not clarify whether synthetic self-play, human preference labeling, or automated reward shaping was used, nor does it disclose any licensing restrictions or public release plans. All that can be definitively said is that reinforcement learning is an integral part of Kanana’s advanced post-training workflow, specifically for the 1.5-15.7B-A3B variant.",
  "4-3 (Reinforcement Learning Data)__evidence": [
    {
      "source": "[readme]",
      "quote": "Furthermore, `Kanana-1.5-15.7B-A3B` is powered by our newly enhanced post-training strategy, which includes on-policy distillation followed by reinforcement learning."
    }
  ],
  "4-4 (Data Filtering)": "None of the supplied quotations describe data-filtering or data-cleaning criteria—there are no mentions of profanity removal, deduplication, toxicity filtering, Jaccard similarity thresholds, perplexity ranges, classifier names (e.g., language-ID filters, NSFW detectors, or copyright screeners), or pipeline stages that purge low-quality or disallowed content. As a result, no information can be summarized regarding Kanana’s data-filtering techniques or their quantitative impact on the dataset. The absence of such details precludes discussion of tool usage (e.g., `cc-net`, `Llama Guard`, or `ppl < 100` heuristics), removed-percentage statistics, or downstream effects on model quality. Therefore, the available evidence yields only a confirmation that no filtering methodology has been publicly disclosed in the provided text.",
  "4-4 (Data Filtering)__evidence": []
}