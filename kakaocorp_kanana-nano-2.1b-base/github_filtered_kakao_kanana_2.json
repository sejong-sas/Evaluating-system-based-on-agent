{
  "1-5 (Architecture)": "The architecture information focuses on the Kanana-1.5-15.7B-A3B model. It is explicitly described as the first Mixture-of-Experts (MoE) member of the Kanana family, highlighting that its design is intentionally sparse. This sparse MoE configuration allows the model to achieve capabilities on par with the dense Kanana-1.5-8B model while requiring only 37 % of the FLOPs per tokenâ€”underscoring both inference efficiency and cost-effectiveness. The quotes also note that the broader Kanana series ranges from 2.1 B to 32.5 B parameters, with several 2.1 B-parameter variants (base, instruct, embedding, function-call, and RAG) publicly released to encourage research on Korean language models.",
  "1-5 (Architecture)__evidence": [
    {
      "source": "[readme]",
      "quote": "Introducing `Kanana-1.5-15.7B-A3B`, the first Mixture-of-Experts (MoE) model in our Kanana family, engineered for exceptional efficiency and powerful performance."
    },
    {
      "source": "[readme]",
      "quote": "`Kanana-1.5-15.7B-A3B`, which has sparse architecture, delivers capabilities comparable to the `Kanana-1.5-8B` dense model while utilizing only 37% of the FLOPS per token, making it a highly inference-efficient and cost-effective solution for real-world applications."
    },
    {
      "source": "[readme]",
      "quote": "The Kanana model series spans from 2.1B to 32.5B parameters with 2.1B models (base, instruct, embedding, function call, and RAG) publicly released to promote research on Korean language models."
    }
  ],
  "1-6 (Tokenizer)": "",
  "1-6 (Tokenizer)__evidence": [],
  "2-1 (Hardware)": "",
  "2-1 (Hardware)__evidence": [],
  "2-2 (Software)": "",
  "2-2 (Software)__evidence": []
}