{
  "2-3 (API)": "",
  "3-1 (Pre-training)": "The report states that the Kanana Nano 2.1B model is not trained from scratch; instead, it is \"derived … through pruning and distillation from the 8B model,\" which \"reduces training costs while achieving superior performance compared to training a model from scratch.\"  Across the Kanana family the authors employ a compute-efficient pre-training recipe that explicitly lists \"high quality data filtering, staged pre-training, depth up-scaling, and pruning and distillation.\"  Although the Llama 3 tokenizer is adopted, they \"do not utilize either the weights or the outputs of Llama 3 during the training of Kanana.\"  Optimization details are given: the team follows the independent weight-decay formulation of Loshchilov & Hutter, setting the decay to 1 × 10⁻⁴, and adds a z-loss with coefficient 5 × 10⁻⁶.  These hyper-parameters are kept constant regardless of model size and, according to the report, provide effective and stable training for Kanana Nano 2.1B and the wider series.",
  "3-2 (Fine-tuning)": "Once the Kanana base (including Nano 2.1B) models are trained, the developers \"further develop instruction and domain-specific adaptation models\" via a dedicated post-training stage.  This stage \"includes supervised fine-tuning and preference optimization\" whose purpose is \"enhancing their capability for seamless interaction with users.\"  Thus, supervised fine-tuning is the primary mechanism used to turn the pre-trained Kanana Nano 2.1B into instruction-tuned and specialized variants.",
  "3-3 (Reinforcement Learning)": "In the same post-training pipeline for Kanana Nano 2.1B instruction models, \"preference optimization\" is applied after supervised fine-tuning.  The report repeatedly pairs preference optimization with the goal of alignment—\"enhancing their capability for seamless interaction with users\"—indicating that this step serves as the reinforcement-style component used to optimize the model with respect to human preferences.",
  "2-3 (API)__evidence": [],
  "3-1 (Pre-training)__evidence": [
    {
      "source": "[pdf_text]",
      "quote": "In Section 2.3.3, we derive Kanana Nano 2.1B model through pruning and distillation from the 8B model, reducing training costs while achieving superior performance compared to training a model from scratch."
    },
    {
      "source": "[sections/2502.18934]",
      "quote": "We introduce Kanana, a series of bilingual language models that demonstrate exceeding performance in Korean and competitive performance in English. The report details the techniques employed during pre-training to achieve compute-efficient yet competitive models, including high quality data filtering, staged pre-training, depth up-scaling, and pruning and distillation."
    },
    {
      "source": "[sections/Training Process]",
      "quote": "In Section 2.3.3, we derive Kanana Nano 2.1B model through pruning and distillation from the 8B model, reducing training costs while achieving superior performance compared to training a model from scratch."
    },
    {
      "source": "[sections/A.3 Details of pre-training from scratch]",
      "quote": "Note that while we use the Llama 3 tokenizer, we do not utilize either the weights or the outputs of Llama 3 during the training of Kanana. Based on the observations of Wortsman et al. (2024), we adopt independent weight decay, which follows the original proposal of Loshchilov & Hutter (2019) and differs from the PyTorch implementation, and a z-loss (Chowdhery et al., 2023) to obtain effective and stable training across various model scales. We set an independent weight decay of 1 × 10−4 and a z-loss coefficient of 5 × 10−6, regardless of model size."
    }
  ],
  "3-2 (Fine-tuning)__evidence": [
    {
      "source": "[pdf_text]",
      "quote": "Leveraging the strong performances of Kanana base models, we further develop instruction and domain-specific adaptation models. To develop instruction models, we apply a post-training process that includes supervised fine-tuning and preference optimization, aimed at enhancing their capability for seamless interaction with users."
    },
    {
      "source": "[sections/2502.18934]",
      "quote": "Furthermore, the report outlines the methodologies utilized during the post-training of the Kanana models, encompassing supervised fine-tuning and preference optimization, aimed at enhancing their capability for seamless interaction with users."
    },
    {
      "source": "[sections/Introduction]",
      "quote": "Leveraging the strong performances of Kanana base models, we further develop instruction and domain-specific adaptation models. To develop instruction models, we apply a post-training process that includes supervised fine-tuning and preference optimization."
    }
  ],
  "3-3 (Reinforcement Learning)__evidence": [
    {
      "source": "[pdf_text]",
      "quote": "Leveraging the strong performances of Kanana base models, we further develop instruction and domain-specific adaptation models. To develop instruction models, we apply a post-training process that includes supervised fine-tuning and preference optimization, aimed at enhancing their capability for seamless interaction with users."
    },
    {
      "source": "[sections/2502.18934]",
      "quote": "Furthermore, the report outlines the methodologies utilized during the post-training of the Kanana models, encompassing supervised fine-tuning and preference optimization, aimed at enhancing their capability for seamless interaction with users."
    },
    {
      "source": "[sections/Introduction]",
      "quote": "Leveraging the strong performances of Kanana base models, we further develop instruction and domain-specific adaptation models. To develop instruction models, we apply a post-training process that includes supervised fine-tuning and preference optimization."
    }
  ]
}