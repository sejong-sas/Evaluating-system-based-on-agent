{
  "1-1 (Weights)": "The only statement provided says: “The Kanana model series spans from 2.1B to 32.5B parameters with 2.1B models (base, instruct, embedding) publicly released to promote research on Korean language models.”  From this we can tell that (a) the 2.1-billion-parameter variants of the Kanana family—including the base, instruct, and embedding versions—have been placed in the public domain in the sense of being openly released; (b) larger checkpoints (anything above 2.1 B) are not described as publicly available in the quote, so their status is unclear; and (c) the explicit motivation cited for the release is to “promote research on Korean language models.”  No further specifics—such as hosting platform, download URL, authentication or request process, or any gating criteria—are mentioned in the material supplied.",
  "1-2 (Code)": "No quoted material mentions any training-pipeline, data-processing, or fine-tuning code for the Kanana-Nano-2.1B-Base model. Consequently, there is no evidence in the provided excerpts that training code (or even inference/serving code) has been made public.",
  "1-3 (License)": "The excerpts contain no sentences that refer to a license name, license text, or any usage restrictions. Therefore, based solely on the supplied quotes, no licensing information can be summarized.",
  "1-4 (Paper)": "The single sentence offered reads: “We introduce Kanana, a series of bilingual language models that demonstrate exceeding performance in Korean and competitive performance in English.”  From this we learn that (a) there exists an official technical write-up or announcement introducing the Kanana series; (b) the series is explicitly characterized as bilingual, with emphasis on both Korean and English capabilities; and (c) it claims superior results in Korean while remaining competitive in English.  No citation, publication venue, author list, or link is provided in the quote, so further bibliographic details are unavailable.",
  "1-1 (Weights)__evidence": [
    {
      "source": "[sections/https://arxiv.org/abs/2502.18934]",
      "quote": "The Kanana model series spans from 2.1B to 32.5B parameters with 2.1B models (base, instruct, embedding) publicly released to promote research on Korean language models."
    }
  ],
  "1-2 (Code)__evidence": [],
  "1-3 (License)__evidence": [],
  "1-4 (Paper)__evidence": [
    {
      "source": "[sections/https://arxiv.org/abs/2502.18934]",
      "quote": "We introduce Kanana, a series of bilingual language models that demonstrate exceeding performance in Korean and competitive performance in English."
    }
  ]
}