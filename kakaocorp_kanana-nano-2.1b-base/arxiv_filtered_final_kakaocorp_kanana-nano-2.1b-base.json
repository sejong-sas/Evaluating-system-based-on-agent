{
  "1-1 (Weights)": "The available quotes repeatedly emphasize that the 2.1 B-parameter Kanana models – including the specific “base” checkpoint that corresponds to kakaocorp/kanana-nano-2.1b-base – are PUBLICLY RELEASED. Concretely, the statements “The Kanana model series spans from 2.1B to 32.5B parameters with 2.1B models (base, instruct, embedding) publicly released to promote research on Korean language models” appear twice, underlining both the range of the series and the fact that every 2.1 B variant (base, instruct, embedding) can be downloaded openly. The disclosure that the models are “publicly released” implies that the weight files themselves are hosted for direct access. The hosting location is specified through the repeated contact block:\n“Kanana LLM Team … https://huggingface.co/kakaocorp”.  This Hugging Face organization page is therefore the official distribution point where users can retrieve the kakaocorp/kanana-nano-2.1b-base weights. No access gate, wait-list, or license pre-approval is mentioned in the quotes, so the summary evidence supports that anyone with a Hugging Face account can download the 2.1 B “base” checkpoint. The same sentences confirm that the public release is meant to “promote research on Korean language models,” suggesting a research-oriented release vision. Because all quoted sentences explicitly reference “2.1B” and “Kanana,” they meet the strict model filter and safely apply to kakaocorp/kanana-nano-2.1b-base.",
  "1-2 (Code)": "Two identical quotes list a public GitHub repository for the project: “Kanana LLM Team … https://github.com/kakao/kanana”.  The presence of the repository link signals that some form of source code is openly available. However, the quotes do not spell out exactly which components live in that repository (e.g., data-preparation scripts, pre-training schedules, fine-tuning notebooks, or inference utilities). They do not separate training versus serving code either. Consequently, the only verifiable fact is that the Kanana team has made code for the Kanana models, including the 2.1 B base variant, accessible on GitHub. Users seeking to reproduce or extend kakaocorp/kanana-nano-2.1b-base should visit that URL to inspect the repository contents and determine whether it contains full training pipelines or only partial resources. The repeated appearance of both “Kanana” and the GitHub address in the quotes satisfies the target-model filter, so the statement unambiguously applies to the 2.1 B base model.",
  "1-3 (License)": "No provided quote contains licensing terms, license identifiers, or any wording about usage, modification, redistribution, or commercial rights for kakaocorp/kanana-nano-2.1b-base. As a result, there is no extractable information on the license from the supplied material.",
  "1-4 (Paper)": "Three distinct quotes give bibliographic-style information about the canonical write-up of Kanana. The phrases “We introduce Kanana, a series of bilingual language models that demonstrate exceeding performance in Korean and competitive performance in English” and “Kanana: Compute-efficient Bilingual Language Models” together function as the abstract and the title of the technical report. A third quote elaborates, stating, “In this report, we present Kanana, a family of large language models available in sizes of {2.1B, 9.8B, 32.5B}, with a focus on the cost-effective training procedure compared to other prominent open models.”  These sentences confirm that an official report or paper exists, titled roughly “Kanana: Compute-efficient Bilingual Language Models,” and that the 2.1 B model is one of three sizes analyzed. The document highlights strong bilingual performance (Korean-first, English-second) and stresses cost-efficiency during training. All sentences explicitly contain the token “Kanana” and, in one sentence, the explicit size “2.1B,” satisfying the strict model filter. Therefore, kakaocorp/kanana-nano-2.1b-base is directly covered by this paper, and interested readers should reference that report for architecture, training setup, and evaluation details.",
  "1-1 (Weights)__evidence": [
    {
      "source": "[abstract]",
      "quote": "The Kanana model series spans from 2.1B to 32.5B parameters with 2.1B models (base, instruct, embedding) publicly released to promote research on Korean language models."
    },
    {
      "source": "[pdf_text]",
      "quote": "Kanana LLM Team ∗\nkanana-llm@kakaocorp.com\nhttps://huggingface.co/kakaocorp"
    },
    {
      "source": "[sections/2502.18934]",
      "quote": "The Kanana model series spans from 2.1B to 32.5B parameters with 2.1B models (base, instruct, embedding) publicly released to promote research on Korean language models."
    },
    {
      "source": "[sections/2502.18934]",
      "quote": "Kanana LLM Team ∗\nkanana-llm@kakaocorp.com\nhttps://huggingface.co/kakaocorp"
    }
  ],
  "1-2 (Code)__evidence": [
    {
      "source": "[pdf_text]",
      "quote": "Kanana LLM Team ∗\nkanana-llm@kakaocorp.com\nhttps://github.com/kakao/kanana"
    },
    {
      "source": "[sections/2502.18934]",
      "quote": "Kanana LLM Team ∗\nkanana-llm@kakaocorp.com\nhttps://github.com/kakao/kanana"
    }
  ],
  "1-3 (License)__evidence": [],
  "1-4 (Paper)__evidence": [
    {
      "source": "[abstract]",
      "quote": "We introduce Kanana, a series of bilingual language models that demonstrate exceeding performance in Korean and competitive performance in English."
    },
    {
      "source": "[sections/2502.18934]",
      "quote": "Kanana: Compute-efficient Bilingual Language Models"
    },
    {
      "source": "[sections/Conclusion]",
      "quote": "In this report, we present Kanana, a family of large language models available in sizes of {2.1B, 9.8B, 32.5B}, with a focus on the cost-effective training procedure compared to other prominent open models."
    }
  ],
  "1-5 (Architecture)": "The available statements indicate that kanana-nano-2.1b-base belongs to the Kanana model family whose publicly released sizes range from 2.1 B to 32.5 B parameters. For the 2.1 B tier, the same architectural blueprint is reused across the base, instruct and embedding variants, and the authors explicitly say “We use the same 2.1B architecture.” To isolate data-scaling effects from structural factors, the team “adopt[s] the architecture … of Llama 3,” while making it clear that no Llama 3 weights or activations are employed during training. Hence, kanana-nano-2.1b-base is a 2.1 B-parameter transformer that mirrors the Llama 3 architectural specification but is trained entirely from scratch for Korean-centric use cases.",
  "1-6 (Tokenizer)": "For tokenization, kanana-nano-2.1b-base also reuses the Llama 3 tokenizer. The authors note twice that they adopt this tokenizer but do not import any of Llama 3’s weights or generated outputs. No further vocabulary size or BPE/WordPiece details are given in the cited material.",
  "2-1 (Hardware)": "",
  "2-2 (Software)": "During training of kanana-nano-2.1b-base the authors employ a modified optimization stack: (1) they switch to “independent weight decay,” following Loshchilov & Hutter’s original AdamW proposal rather than PyTorch’s built-in variant; and (2) they integrate a z-loss term, as introduced by Chowdhery et al., to improve stability and effectiveness across multiple model scales. No other libraries, frameworks or specific runtime flags appear in the provided excerpts.",
  "1-5 (Architecture)__evidence": [
    {
      "source": "[abstract]",
      "quote": "The Kanana model series spans from 2.1B to 32.5B parameters with 2.1B models (base, instruct, embedding) publicly released to promote research on Korean language models."
    },
    {
      "source": "[pdf_text]",
      "quote": "The family of models includes pre-trained base model and post-trained instruction models in sizes of {2.1B, 9.8B, 32.5B}."
    },
    {
      "source": "[pdf_text]",
      "quote": "We use the same 2.1B architecture."
    },
    {
      "source": "[sections/2502.18934]",
      "quote": "To control the effects of architecture and tokenization, and to focus on improving the data scaling curve, we adopt the architecture and tokenizer of Llama 3 (Grattafiori et al., 2024). Note that while we use the Llama 3 tokenizer, we do not utilize either the weights or the outputs of Llama 3 during the training of Kanana."
    },
    {
      "source": "[sections/Introduction]",
      "quote": "The Kanana model series spans from 2.1B to 32.5B parameters with 2.1B models (base, instruct, embedding) publicly released to promote research on Korean language models."
    },
    {
      "source": "[sections/Introduction]",
      "quote": "The family of models includes pre-trained base model and post-trained instruction models in sizes of {2.1B, 9.8B, 32.5B}."
    },
    {
      "source": "[sections/A.3 Details of pre-training from scratch]",
      "quote": "To control the effects of architecture and tokenization, and to focus on improving the data scaling curve, we adopt the architecture and tokenizer of Llama 3 (Grattafiori et al., 2024). Note that while we use the Llama 3 tokenizer, we do not utilize either the weights or the outputs of Llama 3 during the training of Kanana."
    }
  ],
  "1-6 (Tokenizer)__evidence": [
    {
      "source": "[sections/2502.18934]",
      "quote": "To control the effects of architecture and tokenization, and to focus on improving the data scaling curve, we adopt the architecture and tokenizer of Llama 3 (Grattafiori et al., 2024). Note that while we use the Llama 3 tokenizer, we do not utilize either the weights or the outputs of Llama 3 during the training of Kanana."
    },
    {
      "source": "[sections/A.3 Details of pre-training from scratch]",
      "quote": "To control the effects of architecture and tokenization, and to focus on improving the data scaling curve, we adopt the architecture and tokenizer of Llama 3 (Grattafiori et al., 2024). Note that while we use the Llama 3 tokenizer, we do not utilize either the weights or the outputs of Llama 3 during the training of Kanana."
    }
  ],
  "2-1 (Hardware)__evidence": [],
  "2-2 (Software)__evidence": [
    {
      "source": "[sections/2502.18934]",
      "quote": "To control the effects of architecture and tokenization, and to focus on improving the data scaling curve, we adopt the architecture and tokenizer of Llama 3 (Grattafiori et al., 2024). Note that while we use the Llama 3 tokenizer, we do not utilize either the weights or the outputs of Llama 3 during the training of Kanana. Based on the observations of Wortsman et al. (2024), we adopt independent weight decay, which follows the original proposal of Loshchilov & Hutter (2019) and differs from the PyTorch implementation, and a z-loss (Chowdhery et al., 2023) to obtain effective and stable training across various model scales."
    },
    {
      "source": "[sections/A.3 Details of pre-training from scratch]",
      "quote": "Note that while we use the Llama 3 tokenizer, we do not utilize either the weights or the outputs of Llama 3 during the training of Kanana. Based on the observations of Wortsman et al. (2024), we adopt independent weight decay, which follows the original proposal of Loshchilov & Hutter (2019) and differs from the PyTorch implementation, and a z-loss (Chowdhery et al., 2023) to obtain effective and stable training across various model scales."
    }
  ],
  "2-3 (API)": "",
  "3-1 (Pre-training)": "The report states that the Kanana Nano 2.1B model is not trained from scratch; instead, it is \"derived … through pruning and distillation from the 8B model,\" which \"reduces training costs while achieving superior performance compared to training a model from scratch.\"  Across the Kanana family the authors employ a compute-efficient pre-training recipe that explicitly lists \"high quality data filtering, staged pre-training, depth up-scaling, and pruning and distillation.\"  Although the Llama 3 tokenizer is adopted, they \"do not utilize either the weights or the outputs of Llama 3 during the training of Kanana.\"  Optimization details are given: the team follows the independent weight-decay formulation of Loshchilov & Hutter, setting the decay to 1 × 10⁻⁴, and adds a z-loss with coefficient 5 × 10⁻⁶.  These hyper-parameters are kept constant regardless of model size and, according to the report, provide effective and stable training for Kanana Nano 2.1B and the wider series.",
  "3-2 (Fine-tuning)": "Once the Kanana base (including Nano 2.1B) models are trained, the developers \"further develop instruction and domain-specific adaptation models\" via a dedicated post-training stage.  This stage \"includes supervised fine-tuning and preference optimization\" whose purpose is \"enhancing their capability for seamless interaction with users.\"  Thus, supervised fine-tuning is the primary mechanism used to turn the pre-trained Kanana Nano 2.1B into instruction-tuned and specialized variants.",
  "3-3 (Reinforcement Learning)": "In the same post-training pipeline for Kanana Nano 2.1B instruction models, \"preference optimization\" is applied after supervised fine-tuning.  The report repeatedly pairs preference optimization with the goal of alignment—\"enhancing their capability for seamless interaction with users\"—indicating that this step serves as the reinforcement-style component used to optimize the model with respect to human preferences.",
  "2-3 (API)__evidence": [],
  "3-1 (Pre-training)__evidence": [
    {
      "source": "[pdf_text]",
      "quote": "In Section 2.3.3, we derive Kanana Nano 2.1B model through pruning and distillation from the 8B model, reducing training costs while achieving superior performance compared to training a model from scratch."
    },
    {
      "source": "[sections/2502.18934]",
      "quote": "We introduce Kanana, a series of bilingual language models that demonstrate exceeding performance in Korean and competitive performance in English. The report details the techniques employed during pre-training to achieve compute-efficient yet competitive models, including high quality data filtering, staged pre-training, depth up-scaling, and pruning and distillation."
    },
    {
      "source": "[sections/Training Process]",
      "quote": "In Section 2.3.3, we derive Kanana Nano 2.1B model through pruning and distillation from the 8B model, reducing training costs while achieving superior performance compared to training a model from scratch."
    },
    {
      "source": "[sections/A.3 Details of pre-training from scratch]",
      "quote": "Note that while we use the Llama 3 tokenizer, we do not utilize either the weights or the outputs of Llama 3 during the training of Kanana. Based on the observations of Wortsman et al. (2024), we adopt independent weight decay, which follows the original proposal of Loshchilov & Hutter (2019) and differs from the PyTorch implementation, and a z-loss (Chowdhery et al., 2023) to obtain effective and stable training across various model scales. We set an independent weight decay of 1 × 10−4 and a z-loss coefficient of 5 × 10−6, regardless of model size."
    }
  ],
  "3-2 (Fine-tuning)__evidence": [
    {
      "source": "[pdf_text]",
      "quote": "Leveraging the strong performances of Kanana base models, we further develop instruction and domain-specific adaptation models. To develop instruction models, we apply a post-training process that includes supervised fine-tuning and preference optimization, aimed at enhancing their capability for seamless interaction with users."
    },
    {
      "source": "[sections/2502.18934]",
      "quote": "Furthermore, the report outlines the methodologies utilized during the post-training of the Kanana models, encompassing supervised fine-tuning and preference optimization, aimed at enhancing their capability for seamless interaction with users."
    },
    {
      "source": "[sections/Introduction]",
      "quote": "Leveraging the strong performances of Kanana base models, we further develop instruction and domain-specific adaptation models. To develop instruction models, we apply a post-training process that includes supervised fine-tuning and preference optimization."
    }
  ],
  "3-3 (Reinforcement Learning)__evidence": [
    {
      "source": "[pdf_text]",
      "quote": "Leveraging the strong performances of Kanana base models, we further develop instruction and domain-specific adaptation models. To develop instruction models, we apply a post-training process that includes supervised fine-tuning and preference optimization, aimed at enhancing their capability for seamless interaction with users."
    },
    {
      "source": "[sections/2502.18934]",
      "quote": "Furthermore, the report outlines the methodologies utilized during the post-training of the Kanana models, encompassing supervised fine-tuning and preference optimization, aimed at enhancing their capability for seamless interaction with users."
    },
    {
      "source": "[sections/Introduction]",
      "quote": "Leveraging the strong performances of Kanana base models, we further develop instruction and domain-specific adaptation models. To develop instruction models, we apply a post-training process that includes supervised fine-tuning and preference optimization."
    }
  ],
  "4-1 (Pre-training Data)": "The available passages state that the Kanana family – explicitly including the 2.1 B-parameter variant – is trained on a very large-scale corpus of “3 trillion tokens.”  The overall design goal is bilingual strength, so the data mix is deliberately concentrated on English and Korean text.  The authors list the concrete source categories that make up this corpus: “English web, Korean web, academic, code, encyclopedic documents, and instruction data.”  In addition, the paper’s Table 5 compares “token consumption and performance” results for two alternative training strategies – pruning-and-distillation from earlier checkpoints versus training from scratch – and notes that both options employ “the same 2.1 B architecture,” confirming that the pre-training numbers and experiments all apply directly to the kanana-nano-2.1 B base model.",
  "4-2 (Fine-tuning Data)": "After the base Kanana model has been pre-trained, the authors describe an additional stage in which they build “instruction-tuned models for direct interaction by natural language.”  They say this layer is “building on Kanana pre-trained models,” and that Section 3.2 of their paper “presents the details of the specifics regarding the Supervised Fine-Tuning (SFT) and preference datasets.”  Thus, the fine-tuning phase involves both an SFT corpus and a preference dataset, each designed to adapt the 2.1 B Kanana foundation to instruction-following use-cases, although the quotes do not enumerate the exact size or sources of those datasets.",
  "4-3 (Reinforcement Learning Data)": "",
  "4-4 (Data Filtering)": "The authors repeatedly emphasize that the kanana 2.1 B training pipeline includes “high quality data filtering” as a core ingredient of their compute-efficient recipe.  In the same sentence they list it alongside “staged pre-training, depth up-scaling, and pruning and distillation,” identifying it as one of the principal engineering steps applied before or during training.  Although the excerpts do not expose numerical thresholds or specific classifiers, they explicitly attribute the resulting bilingual performance of Kanana to this rigorous filtering stage, underscoring that careful cleaning was essential for achieving competitive quality with limited computational budget.",
  "4-1 (Pre-training Data)__evidence": [
    {
      "source": "[pdf_text]",
      "quote": "We train Kanana models on 3 trillion tokens, primarily focusing on English and Korean bilingual capabilities. We collect our corpora from various sources and categorize them as English web, Korean web, academic, code, encyclopedic documents, and instruction data."
    },
    {
      "source": "[pdf_text]",
      "quote": "Table 5: Token consumption and performance of pruning & distillation (PD) from preceding models and training from scratch. We use the same 2.1B architecture."
    },
    {
      "source": "[sections/Data]",
      "quote": "We train Kanana models on 3 trillion tokens, primarily focusing on English and Korean bilingual capabilities. We collect our corpora from various sources and categorize them as English web, Korean web, academic, code, encyclopedic documents, and instruction data."
    }
  ],
  "4-2 (Fine-tuning Data)__evidence": [
    {
      "source": "[pdf_text]",
      "quote": "Building on Kanana pre-trained models, we further develop instruction-tuned models for direct interaction by natural language. Section 3.2 presents the details of the specifics regarding the Supervised Fine-Tuning (SFT) and preference datasets."
    }
  ],
  "4-3 (Reinforcement Learning Data)__evidence": [],
  "4-4 (Data Filtering)__evidence": [
    {
      "source": "[abstract]",
      "quote": "We introduce Kanana, a series of bilingual language models ... The report details the techniques employed during pre-training to achieve compute-efficient yet competitive models, including high quality data filtering, staged pre-training, depth up-scaling, and pruning and distillation."
    },
    {
      "source": "[abstract]",
      "quote": "We introduce Kanana, a series of bilingual language models that demonstrate exceeding performance in Korean and competitive performance in English. The report details the techniques employed during pre-training to achieve compute-efficient yet competitive models, including high quality data filtering, staged pre-training, depth up-scaling, and pruning and distillation."
    }
  ],
  "__usage": {
    "fine_tuning": "used",
    "rl": "used"
  }
}