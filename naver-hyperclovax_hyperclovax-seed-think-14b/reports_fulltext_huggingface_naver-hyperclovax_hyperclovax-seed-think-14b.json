{
  "model_id": "naver-hyperclovax/hyperclovax-seed-think-14b",
  "full_texts": [
    {
      "arxiv_id": "https://tinyurl.com/y3hrfz67",
      "full_text": " Planting the seeds for an AI ecosystem: Introducing HyperCLOVA X SEED, a commercial open-source AI | CLOVA HyperCLOVA X Tech Blog AI Research AI Products CLOVA X Partners Contact sales Apr 23, 2025 Planting the seeds for an AI ecosystem: Introducing HyperCLOVA X SEED, a commercial open-source AI Copy You’ve copied the link. Planting the seeds in the AI ecosystem: Introducing HyperCLOVA X SEED, a commercial open-source AI We proudly introduce HyperCLOVA X SEED, an open-source AI model available for commercial purposes. This landmark release represents more than just a model deployment—it&#8217;s a crucial step toward building Korea&#8217;s sovereign AI ecosystem. Building sovereign AI is an enormous challenge that cannot be achieved by any single company or nation. NAVER is releasing HyperCLOVA X SEED with the vision of fostering an ecosystem that empowers businesses across the board. HyperCLOVA X SEED is small yet powerful, designed to be fine-tuned according to specific business needs, enabling customized applications across diverse industry sectors. This release features three models with 3B, 1.5B, and 0.5B parameters respectively—each engineered for specialized applications. Notably, our 3B model offers vision understanding capabilities, enabling developers to build sophisticated applications that process both text and visual information. By making HyperCLOVA X SEED available as open source, we aim to accelerate the innovation and growth of Korea&#8217;s AI ecosystem. HyperCLOVA X SEED 3B HyperCLOVA X SEED 3B is a multimodal model specifically designed to understand images within Korean linguistic and cultural contexts. This model accurately interprets Korea&#8217;s unique cultural nuances and visual data—a core element in creating a sovereign AI ecosystem. While global models have inherent limitations in reflecting Korea&#8217;s distinctive characteristics, an AI built upon our own data and cultural contexts holds significant value. The practical value of HyperCLOVA X SEED 3B becomes evident in real-world scenarios. For example, when asked &#8220;Where&#8217;s the place in this video? If I were to travel somewhere nearby, what would you recommend?&#8221; the model analyzes video content, accurately comprehends Korean cultural nuances, and offers specific travel recommendations. After recognizing the location in the video, it suggests major tourist attractions, unique experiences, local restaurants, and relevant cultural information. These capabilities extend across domains—from tour guidance and visual content analysis to image-based question-answering systems. Despite its compact architecture, HyperCLOVA X SEED 3B delivers remarkable performance in image understanding compared to competing models, particularly when processing visual data in Korean-language contexts. ▲ Benchmark average: Average of nine vision metrics, including VideoMME-Ko ▲ Comparative analysis of HyperCLOVA X SEED 3B against competitor models in text performance In text processing, HyperCLOVA X SEED 3B demonstrates competitive results across multiple Korean benchmark tests. The model&#8217;s proficiency in understanding and processing Korean language and cultural elements is particularly noteworthy. Let&#8217;s examine the specific performance metrics for image and video understanding: ▲ Comparative analysis of HyperCLOVA X SEED 3B against competitor models in image and video comprehension HyperCLOVA X SEED 3B outperforms competing models in image and video understanding tasks, especially in visual content comprehension within Korean contexts. This exceptional performance stems from years of accumulated expertise, high-quality training data, and continuously refined methodologies. HyperCLOVA X SEED 3B demonstrates unique strengths in processing both visual and textual data specifically tailored to Korean language and cultural contexts. HyperCLOVA X SEED 1.5B HyperCLOVA X SEED 1.5B is an instruction-following language model designed to understand and generate text. This versatile model demonstrates exceptional promise across a wide range of industry applications. This example demonstrates how HyperCLOVA X SEED 1.5B handles translation tasks between Korean and major languages like English and Japanese. The model skillfully translates messages written in Korean into English, enabling users to express themselves more clearly. This capability serves as a valuable assistive tool for educational materials and business communications. The model processes complex instructions, including requests for specific output formats (such as JSON) or conversational styles (like casual language). These sophisticated capabilities make HyperCLOVA X SEED 1.5B well-suited for various business applications and specialized chatbot services. ▲ Comparison of HyperCLOVA X SEED 1.5B against competitor models in Korean language understanding evaluation When evaluated across major Korean-language benchmarks (KMMLU, HAE-RAE, CLiCK, and KoBEST), HyperCLOVA X SEED 1.5B consistently outperforms similarly-sized competitor models, including Qwen2.5-1.5B-instruct and gemma-3-1b-it. HyperCLOVA X SEED 0.5B HyperCLOVA X SEED 0.5B is a compact language model that excels at understanding and generating text while maintaining fluent multi-turn conversations with users in Korean. This efficiency makes it ideal for deploying natural-sounding conversational interfaces in resource-constrained environments, including mobile applications, smart home devices, and wearables. As demonstrated in the example above, HyperCLOVA X SEED 0.5B delivers well-structured responses to everyday user inquiries. When consulted about living room decoration, it provides systematic and practical interior advice—from color selection and lighting installation to furniture placement and decorative accessories. This specific capability makes it invaluable for environments requiring lightweight AI solutions, such as smart home devices, mobile applications, and customer support chatbots. ▲ Comparison of HyperCLOVA X SEED 0.5B performance across Korean language benchmarks When compared to Qwen2.5-0.5B-instruct, which has the same parameter size, HyperCLOVA X SEED 0.5B consistently outperforms across major Korean benchmarks including KMMLU, HAE-RAE, and KoBEST. Notably, HyperCLOVA X SEED 0.5B leverages quality training data and the high-performance HyperCLOVA X flagship model, enabling it to be trained at just 1/39th the cost of similarly-sized competitor model. T raining costs (pre-training) HyperCLOVA X SEED 0.5B QWEN2.5-0.5B A100 GPU hours 4,358 169,257 Cost (USD) 6,537 253,886 How can I use HyperCLOVA X SEED? Begin your journey with HyperCLOVA X Seed today, available for straightforward download from Hugging Face. With limited exceptions, it is freely available for commercial purposes. Please refer to the HyperCLOVA X license documentation [ 3B / 1.5B / 0.5B ]. NAVER is committed to serving as a trustworthy guide for all partners embarking on their sovereign AI journey. Regardless of geographical location or organizational scale, we will support your efforts to create meaningful achievements. Our mission extends beyond providing tools—we aim to empower every organization with AI innovation capabilities that ensure proprietary competitiveness. With HyperCLOVA X SEED, you can now implement the applications you&#8217;ve envisioned. Tags # CLOVA X # HyperCLOVA X # HyperCLOVA X SEED # OpenSource PREV NEXT HyperCLOVA X HyperCLOVA X HyperCLOVA X SEED Use cases Tech Blog AI Research AI Products CLOVA Studio CLOVA X NAVER AI products Related links Terms Partners Contact us ⓒ NAVER Cloud Corp. "
    },
    {
      "arxiv_id": "https://arxiv.org/pdf/2506.22403",
      "full_text": "arXiv:2506.22403v2  [cs.CL]  1 Jul 2025\nHyperCLOVA X THINK\nNAVER Cloud\nHyperCLOVA X Team\nAbstract\nWe introduce HyperCLOVA X THINK, the first reasoning-focused large language\nmodel in the HyperCLOVA X family, pre-trained on roughly 6 trillion high-quality\nKorean, and English tokens, augmented with targeted synthetic Korean data. It\nwas implemented as a compute-memory-balanced Peri-LN Transformer scaled\nwith µP, pre-trained through a three-stage curriculum that expands the context\nwindow to 128K tokens, and post-trained via supervised fine-tuning with Rein-\nforcement Learning from Verifiable Rewards supports both detailed rationale and\nconcise-answer modes. It delivers competitive performance against similarly sized\nmodels on Korea-focused benchmarks such as KMMLU, CSAT, KoBALT-700,\nHAERAE-1.0, and KoBigBench, while preserving robust bilingual consistency\nand translation quality. In addition, a vision-augmented variant matches or ex-\nceeds GPT-4.1 on the KCSAT STEM benchmark, all of which are achieved with\nsubstantially lower training compute than existing models of similar sizes. These\ncapabilities position HyperCLOVA X THINK as a robust foundation for Korean\nAI innovation and a valuable resource for the global research community. Lastly,\nwe present a pruning and distillation technique that will soon be applied to Hyper-\nCLOVA X THINK for an open-source and business-friendly foundation model.\n1\nIntroduction\nRecent advancements of large language models (LLMs) have drawn increased attention to their rea-\nsoning abilities, going beyond simple memorization of factual knowledge to deriving logical conclu-\nsions. Models like GPT-o1 (OpenAI et al., 2024b), R1 (DeepSeek-AI et al., 2025), and QwQ (Qwen\nTeam, 2025) exemplify such effort, demonstrating that the ability to perform logical inferences and\nmulti-step problem solving can significantly broaden the scope of AI applications.\nAt the same time, the notion of sovereign AI is being established as an important goal. As LLMs\ncontinue to be deployed in various regions around the globe, there is a growing need for linguistic\nfluency and cultural sensitivity tailored toward a given region, as well as data governance that aligns\nwith regional values and regulations. In this regard, our immediate focus is Korea.\nTo meet the imperatives of both advanced reasoning and sovereign AI—for Korea, in particular—we\npresent HyperCLOVA X THINK (henceforth THINK). It is the first reasoning-focused LLM in the\nHyperCLOVA X family(Yoo et al., 2024b), trained via a strategic preparation of training data and\nuse of the latest pre- and post-training techniques.\nIn particular, we curated a corpus of roughly six trillion tokens that balances high-quality Korean\nand English text with targeted synthetic Korean data. This mixture improves linguistic breadth while\nsafeguarding cultural and domain relevance. The model architecture follows a compute-memory-\nbalanced Peri-LN Transformer scaled with the µP framework, allowing consistent hyperparameter\ntransfer from small to large scales without extensive grid search.\nDuring pre-training, A three-stage curriculum gradually increases the context window, culminating\nin 128k tokens, which enables THINK to process long documents and perform multi-step reason-\ning within a single pass. Then, for post-training, we combine supervised fine-tuning on carefully\n\ndesigned reasoning tasks with Reinforcement Learning from Verifiable Rewards. This alignment\nstrategy encourages the model to generate explicit chains of thought when requested and concise\nanswers when brevity is preferred. Safety alignment follows NAVER AI Ethics guidelines through\nfiltered data, red-teaming, refusal sampling, and policy tuning.\nWe evaluate THINK on Korea-focused benchmarks such as KMMLU, CSAT, KoBALT-700,\nHAERAE-1.0, and KoBigBench. The model achieves competitive accuracy among similarly sized\nmodels while requiring substantially lower training compute. A vision-augmented variant that in-\ntegrates vision encoders to extend the same reasoning framework to image-text tasks, matches or\nsurpasses GPT-4.1 on the KCSAT STEM benchmark.\nTo ensure that academic and industry partners can benefit from the model, we introduce a pruning-\nand-distillation recipe that reduces parameter count while preserving accuracy. This technique will\nsoon be applied to THINK itself to produce a model suitable for limited resource settings. We plan\nto open-source release this model under a business-friendly license.\nOur contributions are threefold. First, we demonstrate that a regionally tailored corpus combined\nwith modern scaling laws yields a bilingual model with strong reasoning capability. Second, we\nprovide an efficient training and alignment recipe that lowers the barrier to entry for sovereign AI\ndevelopment. Third, we share a practical pruning-distillation pipeline and commit to apply it for an\nopen-source version of THINK—fostering further research and commercial deployment, even under\nmore resource-constrained settings.\n2\nPre-Training\nThis section outlines the pre-training methodology behind THINK: a scalable, Korean-centric data\npipeline enriched with targeted synthetic corpora (Section 2.1); a compute–memory-efficient yet\nstability-oriented Transformer, instantiated with scale-invariant parameterization principles (Sec-\ntion 2.2); and a three-stage curriculum that sequentially builds foundational linguistic knowledge,\nrefines competence with higher-fidelity data, and expands contextual capacity to support long-form\nreasoning (Section 2.3). See Figure 1 for an overview of the pre-training process.\n2.1\nData Preparation\nWe begin with the end-to-end data pipeline—collection, cleaning, and quality filtering—paying spe-\ncial attention to techniques tailored for our large-scale Korean corpus. We then describe a synthetic-\ndata generation strategy that enriches under-represented domains while preserving linguistic fidelity.\nData Pipeline. The data pipeline for THINK is designed around three guiding principles: scalability,\nreusability, and quick refresh, so that new corpora can be incorporated with minimal latency while\nmaintaining strict quality guarantees. Following Weber et al. (2024a), the pipeline separates schema\nstandardization from quality assessment and filtering. During standardization, raw documents in\nheterogeneous formats undergo lightweight cleansing, canonicalization of field names, and storage\nin a unified schema. The subsequent annotation stage attaches quantitative quality signals, includ-\ning structural and linguistic metrics, and applies masking to all personally identifiable information\n(PII). The filtering stage then materializes stage-specific corpora by applying threshold rules to the\nannotated data and serializes the result into shard files optimized for streaming.\nData Filtering. Korean-specific data filtering schemes have been largely underexplored from the\nliterature. To obtain a corpus that is simultaneously broad and reliably high-quality, we devise a\ntwo-tier filtering framework tailored to the linguistic and typographic characteristics of Korean.\nThe first tier extends the rule sets of Weber et al. (2024b) and Lozhkov et al. (2024) by redesign-\ning every heuristic for Korean morphology. Among various quantitative signals, five representative\nexamples—symbol-to-word ratio, mean word length, sentence count, masked-PII ratio, and the pro-\nportion of normalized to raw length—are computed for each document. Target ranges for these sig-\nnals are established through manual inspection with an internal reviewer, and thresholds are further\nadapted to each source domain (e.g., blogs, wikis) to suppress noise while preserving recall.\nThe second tier employs model-based scoring. FastText (Joulin et al., 2017, 2016) and transformer\nencoders are trained under two supervision regimes. In the binary regime, wiki-like passages con-\nstitute positive examples whereas noisy web pages form the negative class; the posterior probability\n2\n\nFigure 1: Pre-training pipeline of HyperCLOVA X THINK. (1) Data-Preparation Phase: A scal-\nable pipeline collects raw corpora, carries out cleansing, language identification, deduplication, and\nmasking; attaches quantitative quality signals, applies filtering, synthesizes targeted data, and seri-\nalizes the resulting shards (2) Training Phase: A dedicated three-stage curriculum, with each stage\noptimized for its specific objective, progressively builds and refines the model’s capabilities.\nfurnishes a continuous quality score. In the ordinal regime, a language model assigns 0–5 ratings for\neducational utility, informativeness, and narrative coherence, producing “wiki-like”, “educational”,\nand “explanatory” quality predictors analogous to GPT-3, FineWeb-edu, and DCLM filters (Brown\net al., 2020; Penedo et al., 2024; Li et al., 2024). A document is retained only if it satisfies a stage-\nspecific conjunction of heuristic thresholds and model scores. Near-duplicates are removed with a\nMinHash index that is rebuilt at every refresh.\nTable 1 summarizes the document-level yield rates of sub-sampled data achieved by the two-tier\npipeline. Even within this modest slice, the first stage discards roughly 90 % of raw pages overall,\nwhile the more selective second stage retains just 1 – 20 %. These figures reveal aggressive corpus\ncompression, with the pipeline condensing the raw crawl by roughly one to two orders of magnitude\neven on the sub-sampled slice.\nSynthetic Data Generation. In contrast to the extensive curated resources available for ma-\njor languages (e.g., English and Chinese), high-quality Korean corpus remains markedly under-\nrepresented. To redress this asymmetry, we initiate a systematic program of high-fidelity synthetic\ndata generation, focusing on domains—such as education, law, historical facts, and cultural sen-\ntiment—where native Korean content is especially sparse (Yuan et al., 2023; Lee et al., 2024).\nLeveraging our in-house model family, the pipeline follows two complementary tracks, rewriting\nexisting documents and generating new text from curated seed prompts, while placing filtering and\nverification at the core of the process to ensure that only high-fidelity Korean data is retained.\nThe synthetic-data workflow comprises four coupled phases (Cheng et al., 2024; Li et al., 2023;\nBen Allal et al., 2024; Su et al., 2024a). (1) Data-design phase: We draft a specification that fixes the\ntarget domain, desired volume, file format, and downstream use case. This document governs every\nsubsequent decision in the pipeline. (2) Seed-acquisition and generation phase: License-compliant\nseed material is collected from open-source and internal repositories. These seeds are either para-\nphrased to remove copyright artifacts or expanded into new passages through prompt-based genera-\n3\n\nData\nStage 1 Yield (Filtered / Raw)\nStage 2 Yield (Filtered / Raw)\nTotal\n9.59%\n1.36%\nBlog\n57.74%\n19.84%\nCafe\n31.53%\n2.35%\nWeb\n4.49%\n0.27%\nTable 1: Stage-wise document yield rates after two-tier filtering.\ntion with our in-house language-model family. (3) Filtering and refinement phase: The resulting text\nis processed by the same two-tier filtering stack used for web data, augmented by routines that detect\nrepetitive templates, logical inconsistencies, and machine-like phrasing. (4) Integration phase: Only\ndata that satisfy all quality checks are versioned and merged into the pre-training corpus, ensuring\nthat synthetic examples extend coverage without degrading overall corpus fidelity. We provide illus-\ntrative synthetic data examples in Appendix E. These synthetic corpora are injected into both Stage\n1 and Stage 2 of the pre-training curriculum.\n2.2\nModel Architecture\nOn the architectural front, our design integrates three key components—(i) a compute–memory-\nbalanced Transformer layout (Hoffmann et al., 2022; Rivière et al., 2024), (ii) Peri-Layer Normal-\nization (Peri-LN, Kim et al. (2025)) for training stability and performance, and (iii) Maximal Up-\ndate Parametrization (µP, Yang and Hu (2021); Yang et al. (2024)) for scale-robust hyper-parameter\ntransfer—together enabling stable scaling and cost-efficient training.\nCompute–Memory Balanced Architecture. To minimize compute-bound training cost and\nmemory-bound inference latency under a fixed parameter budget, we employ a shallower-but-wider\nTransformer configuration Hoffmann et al. (2022); Rivière et al. (2024). The model reduces the\nnumber of blocks and reallocates the freed parameters to larger hidden and feed-forward dimen-\nsions. Because each self-attention layer incurs O(L2) FLOPs and O(L) activation memory with\nrespect to sequence length L, lowering depth proportionally decreases attention overhead, while\nwidening the FFN, whose cost grows linearly in L, maintains representational capacity.\nTo empirically substantiate this design, we start from a 3B-parameter baseline comprising 26 Trans-\nformer blocks with an FFN hidden size of 7, 168 and generate a shallow-but-wide variant by re-\nducing the depth to 18 layers (30 % shallower) while proportionally increasing the FFN hidden\ndimension to 11, 264 (57 % wider), thereby conserving the total parameter budget. Owing to the\nquadratic attention cost, this reallocation lowers the theoretical compute for an 8K-token sequence\nby 13.7 % TFLOPs. Consistently with this analysis, the modified model ingested 15 % more train-\ning tokens within an identical wall-clock budget and matched the validation perplexity of the deeper\ncontrol, confirming that width-centric capacity reallocation preserves modeling quality while con-\nferring tangible hardware efficiency.\nStability-Oriented Transformer. We stabilize scale-up by coupling Maximal Update Parametriza-\ntion (µP) with a Peri-Layer-Normalized Transformer. Following the µTransfer procedure, we sweep\nlearning-rate and regularization only on small proxy models, then zero-shot port the optimal settings\nto each production scale. Because µP preserves update magnitudes across configurations, the large\nmodels inherit well-conditioned gradient norms without further tuning, greatly reducing exploration\ncost while keeping feature learning intact (Yang and Hu, 2021; Yang et al., 2024).\nPeri-Layer Normalization (Peri-LN) normalizes both the input and output of every Transformer sub-\nlayer, bounding hidden-state variance to grow at most linearly with depth and that layer-wise gra-\ndient norms remain stable throughout training.By tightly bounding hidden state statistics, Peri-LN\nsuppresses the massive activations typically observed in Pre-LN models (Sun et al., 2024). Peri-\nLN also removes the need for FLOP-intensive ablation studies to stabilize architectural or training\nhyper-parameters. Empirically, Peri-LN yields lower pre-training loss and smaller run-to-run vari-\nance (Kim et al., 2025). Maximal Update Parametrization (µP) complements Peri-LN by preserving\noptimization statistics across width and depth, so hyper-parameters tuned on sub-billion-parameter\n4\n\n(a) Training loss\n(b) Gradient-norm\nFigure 2: Performance comparison between 8 B-parameter Pre-LN and Peri-LN Transformers dur-\ning pre-training. Each model size excludes the embedding parameters.\nproxies transfer reliably to multi-billion-parameter instances. Together, Peri-LN and µP provide a\nprincipled, cost-effective pathway to stable scaling.\nTo evaluate normalization choices at production scale, we trained two Llama-style models (Dubey\net al., 2024) with 8 B parameters on the same open-corpus dataset (Su et al., 2024a), along with our\nin-house version of the TikToken tokenizer1: a standard Pre-LN (Xiong et al., 2020) baseline and\nan otherwise identical Peri-LN variant. As illustrated in Figure 2, the Peri-LN model exhibits fewer\ngradient and loss spikes than its Pre-LN counterpart, reproducing the large-scale stability benefits\nreported by Kim et al. (2025). Furthermore, the Peri-LN configuration attains, on average, a 15 %\nlower training loss within the same wall-clock budget. These findings confirm that Peri-LN delivers\nsuperior stability and performance without incurring additional computational cost, and thus we\nadopt it as the default normalization scheme in the THINK architecture.\n2.3\nPre-Training Curriculum\nWe adopt a three-staged pre-training curriculum, with each phase focused on a distinct capability tar-\nget (OLMo et al., 2025; Hu et al., 2024). Stage 1 establishes a general-purpose foundational knowl-\nedge base. Stage 2 refines domain-specialized competence by continuing training on high-quality\ncorpora. Stage 3 extends the context window to 128K tokens and internalizes long chain-of-thought\nreasoning by fine-tuning on rejection-sampled traces generated from an in-house model family. The\nstaged curriculum strategically allocates computational FLOPs to phases with the highest marginal\nutility, optimizing cost-efficiency while maximizing incremental performance gains.\nStage 1: Foundational Knowledge Construction. The first training stage establishes a broad\nknowledge base spanning multiple domains. We curate a multilingual corpus, principally Korean\nand English. Training proceeds on sequences up to 8K tokens, consuming 6 trillion tokens in total.\nThe learning rate is linearly increased during the initial 5, 000 steps to a peak of 1.59e-3 determined\nby µP scaling, after which it is annealed according to a cosine schedule to 1.59e-4 (10 % of the\nmaximum), thereby promoting stable convergence.\nStage 2: Domain-Specialized Capability Boosting. The mid-training stage introduces an additional\n1 trillion tokens to sharpen the model’s domain expertise and reasoning ability while maintaining\nthe 8K-token context length established in Stage 1. We gradually down-weight generic web text\nand increase high-quality, domain-focused corpora including the synthetic datasets constructed in\nSection 2.1. A brief 2, 000-step warm-up ensures a smooth transition to these revised distribution.\nGuided by Bi et al. (2024), for learning rate schedule, we adopt a two-step decay profile: the rate is\nheld at 1.59e-4 for 80 % of training, reduced to 31.6 % of this peak (≈4.76e-5) for the next 10 %,\nand finally to 10 % (≈1.59e-5) for the last 10 %. For the data mix, following Blakeney et al. (2024),\nwe rebalancing the dataset during the final 10 % of training steps. Sampling of lower-quality general\ntext is gradually reduced. Conversely, the sampling weight of under-represented domains, crucial\n1https://github.com/openai/tiktoken\n5\n\nFigure 3: Post-training pipeline of HyperCLOVA X THINK. (1) Data-Preparation Phase: Data is col-\nlected and then rigorously processed through steps such as format validation, automatic verification,\nlanguage-based filtering, and evaluation by LLM-based judges. The data is refined through qual-\nity filtering, difficulty filtering, and ranking to prepare data suitable for subsequent training stages\nwith different objectives. (2) Training Phase: A sequence of fine-tuning procedures—including Su-\npervised Fine-Tuning (SFT), Reward Modeling (RM), Reinforcement Learning with Verifiable Re-\nwards (RLVR), training for reasoning Length Controllability (LC), and Reinforcement Learning\nfrom Human Feedback (RLHF) —is executed across a large-scale GPU cluster.\nfor sovereign-AI applications, is increased, with emphasis on Korean medical literature, national\neconomic reports, and culturally contextualized historical archives.\nStage 3: Extended Context Alignment. Standard corpora are biased toward short documents;\nnaively over-sampling longer texts therefore disrupts training stability (Zhuang et al., 2025). We mit-\nigate this issue with length-based, proportion-preserving resampling, which increases the number of\nlong documents while maintaining each length bucket’s share of total tokens. After pre-training with\nan 8 K context window and a rotary-position-embedding base θ of 500 K, we expand the window in\nthree successive stages—32 K, 64 K, and 128 K. At each expansion, θ is raised from 500 K to 5 M,\nthen to 20 M, and finally to 100 M. A brief warm-up followed by cosine decay restores perplexity\nbefore the next enlargement (Su et al., 2024b; Xu et al., 2024). To supply explicit supervision for\nextended reasoning, we additionally train on a long chain-of-thought corpus generated in-house and\nfiltered via rejection sampling (Yuan et al., 2023; Lee et al., 2024) (see §2.1). This synthetic dataset\nspans up to 128 K tokens, enabling the model to master long-context conditioning without degrading\nthe general or domain-specific competencies obtained in Stages 1 and 2.\n3\nPost-Training\nThis section outlines the post-training methodology of THINK: a supervised fine-tuning (SFT) phase\nthat injects core reasoning patterns and task-specific capabilities (Section 3.1); and a multi-stage re-\ninforcement learning pipeline that incorporates verifiable rewards, length-controllability, and human\nfeedback to achieve aligned, efficient, and scalable reasoning (Sections 3.2–3.4). See Figure 3 for\nan overview of the training process.\n6\n\nReasoning Mode\nNon-Reasoning Mode\n<|im_start|>user\n{query}<|im_end|>\n<|im_start|>assistant/think\n{reasoning}<|im_end|>\n<|im_start|>assistant\n{response}<|im_end|><|endofturn|>\n<|im_start|>user\n{query}<|im_end|>\n<|im_start|>assistant\n{response}<|im_end|><|endofturn|>\nTable 2: Unified chat template used for training models to support both reasoning and non-reasoning\ninteraction modes.\nReasoning\n39.8%\nNon-reasoning\n60.2%\nReasoning vs Non-reasoning\nCode\n29.9%\nGeneral\n1.5%\nMath\n39.4%\nSTEM\n29.2%\nReasoning Category Distribution\nFigure 4: Data distribution utilized for Supervised Fine-Tuning (SFT), reflecting a balanced compo-\nsition tailored to support effective downstream reinforcement learning and reasoning capabilities.\nTHINK is trained to operate in an integrated manner, allowing for dynamic switching between a de-\ntailed ‘reasoning mode’ for complex, multi-step reasoning and a more direct ‘non-reasoning mode’\nfor rapid, context-driven responses. This unified framework eliminates the need for users to switch\nbetween separate models (e.g., a dedicated reasoning model and a chatbot), as illustrated in Table 2.\n3.1\nSupervised Fine-Tuning (SFT)\nSupervised Fine-Tuning (SFT) serves as a foundational step in our post-training pipeline, aiming to\ninject desired behaviors and reasoning patterns into the model. This stage establishes a strong base\nfor subsequent reinforcement learning phases.\nThe dataset used for SFT is constructed by aggregating various sources across mathematics, coding,\nSTEM, and general abilities. We carefully curate data from a series of ablation studies and utilize\nhigh-quality open-source and in-house data. For reasoning data, each sample contains prompt, assis-\ntant think, and assistant response. The assistant think contains a rather free-form chain-of-reasoning,\nwhile the assistant response is a concise, finalized output that directly answers the user’s query based\non that reasoning. The general statistics for the SFT dataset is illustrated in Figure 4.\nTo ensure data quality and consistency, we apply a multi-stage filtering pipeline across all datasets.\nEach item in data goes through a basic format check to ensure that the output contains proper format\n(e.g., boxed answers for math problems and compilability for code problems). Language filtering is\napplied to select only samples written in the target language, and language matching further ensures\nthat input and output languages are the same for each sample. For reasoning data specifically, we\nalso check whether the final answer is automatically verifiable. For non-reasoning data, we employ\na LLM-as-a-Judge method to score each example by their helpfulness and safety and filter out those\nwith low scores.\nTraining is performed with dynamic batching to fill each batch dynamically to its maximum capa-\nbility, in order to optimize GPU utilization and memory usage. The model is trained over 4 epochs\n7\n\nwith early stopping based on validation accuracy. Similarly to other reports (Yang et al., 2025), we\nobserve that selecting a checkpoint from later epochs results in reduced exploration of the model\nduring the subsequent phase. More comprehensive details on the training setup of SFT are provided\nin our previous technical report (Yoo et al., 2024b).\n3.2\nReinforcement Learning with Verifiable Rewards (RLVR)\nThe Reinforcement Learning with Verifiable Rewards (RLVR) is crucial for improving reasoning\ncapabilities through verifiable feedback mechanisms. The main objective is to optimize model per-\nformance by accurately guiding behavior through precise rewards and penalties.\nReinforcement Learning Algorithm. In our implementation of RLVR, we adopt Group Relative\nPolicy Optimization (GRPO) (Shao et al., 2024).\nUnlike more traditional RL algorithms, it calculates a baseline advantage based on multiple gener-\nations per prompt, optimizing computational efficiency and maintaining training effectiveness. To\nfurther enhance the robustness and accuracy of our RLVR framework, we introduce several targeted\nmodifications:\n• KL Divergence Penalty Removal: Our initial experiments indicated that this penalty re-\nstricts models from exploring diverse behaviors and incurs significant computational over-\nhead due to the necessity of inference from a reference model. Removing the penalty im-\nproved computational efficiency and model flexibility.\n• Constant Normalization: We observed that prompt difficulty often correlates with re-\nsponse length–more difficult prompts tend to produce longer responses–thereby introduc-\ning biases related to response length. To mitigate these biases from varying response lengths\nand prompt difficulties, we adopt constant normalization strategy from Liu et al. (2025).\n• Relaxed Upper Bound for Exploration: To encourage exploration and prevent determin-\nistic policy collapse, we adopt the clip-higher approach (Yu et al., 2025), which raises the\nupper threshold of the importance sampling ratio in GRPO. By including low-probability\ntokens into policy updates, this approach increases policy entropy and fosters diverse rea-\nsoning paths.\nCollectively, these methodological enhancements enable our RL training to achieve an optimal bal-\nance of exploration, computational efficiency, and stable training performance.\nData Efficiency. To optimize training efficiency, enhance model performance, and effectively utilize\ncomputational resources, we employ targeted difficulty filtering techniques, including both offline\nand online methods.\nWe implement offline difficulty filtering to our dataset by excluding prompts that are either too easy\nor too challenging. Specifically, we leverage predictions generated by the SFT checkpoint—our\ninitial model for RLVR—to evaluate the difficulty of each prompt. By sampling multiple responses\nfrom this checkpoint, we calculate the average accuracy of predictions and remove prompts with\naccuracy of exactly 0.0 or 1.0. This strategy ensures the inclusion of prompts only with appropriate\ndifficulty levels at the outset of training.\nHowever, offline difficulty filtering has limitations. Because this filtering method occurs only once\nbefore the training begins, it is inherently static. As the model’s performance improves as the training\nprogresses, the dataset’s difficulty level cannot be adjusted accordingly–a problem that was once\nchallenging can become solvable. Consequently, this static nature can lead to discrepancies between\nevolving model capabilities and fixed difficulty of prompts.\nTo address the shortcomings of offline filtering, we additionally incorporate an online difficulty fil-\ntering strategy. Utilizing GRPO allows us to generate multiple responses per prompt within each\nbatch. For each group, we calculate accuracy and remove prompts where all generated responses are\neither entirely correct or entirely incorrect from the batch. This dynamic filtering approach continu-\nously adapts the training set’s difficulty to the model’s evolving capabilities, ensuring that learning\nremains focused on informative examples and thereby maintaining optimal training efficiency.\nOur analysis aligns with recent findings suggesting that online difficulty filtering effectively opti-\nmizes the lower bound learnability of reinforcement learning algorithms by dynamically balancing\n8\n\nprompt difficulty (Bae et al., 2025). Importantly, we observe that even with initial offline filtering,\nonline filtering still provides substantial additional benefits. Thus, combining both offline and online\ndifficulty filtering significantly enhances our training efficiency and model performance.\nReward Shaping. To effectively guide model training and enhance its performance in our RLVR\nframework, we carefully design a reward shaping strategy consisting of several distinct components:\n• Format Reward: We establish a set of format rules that responses must follow. To calculate\nthis reward, we count the number of rules adhered to by the model’s response and divide it\nby the total number of format rules. We adopt this soft penalty approach as it demonstrates\nminimal negative impact on reasoning performance, allowing models to progressively align\nwith the desired response structure without detrimental effects.\n• Language Reward: This reward is computed based on the ratio of characters generated in\nthe same language as the prompt. By directly correlating the language of responses with the\nlanguage of prompts, this reward encourages the model to reason in the intended language,\nsignificantly enhancing multilingual reasoning capabilities.\n• Verifiable Reward: We incorporate verifiable rewards across multiple problem categories,\nincluding mathematics, code generation, code input-output (Code IO), and multiple-choice\nquestions. The verification outcomes directly determine reward allocation, with a binary\nvalue: a fully correct response receives a reward of 1.0, while any incorrect response results\nin a reward of 0.0.\n• Overlong Reward: We adopt both Soft Overlong Penalty and Overlong Loss Masking\n(Yu et al., 2025), because penalizing truncated samples harshly can introduce undesirable\nreward noise, potentially destabilizing training by penalizing valid reasoning solely due\nto length. The former gradually increases as the response length exceeds the predefined\nmaximum value, and the latter masks the loss of truncated samples, effectively stabilizing\nthe training process.\nOptimized Rollout Sampling Process. Efficiency in the rollout sampling process is crucial for op-\ntimizing the RLVR training pipeline, as this stage typically dominates the overall training duration.\nTo address this, we implement a highly efficient asynchronous sampling procedure. In this setup,\ninference nodes are utilized continuously and concurrently until the number of completed rollout\nsamples meets or exceeds the training batch size. Samples generated from these inference nodes\nare collected and stacked asynchronously, significantly reducing idle times and improving resource\nutilization.\nMoreover, due to our implementation of online difficulty filtering, certain samples may be dynam-\nically filtered out during the rollout process, potentially causing delays or inefficiencies. To coun-\nteract this, we maintain a buffered approach to concurrent sampling, ensuring multiple samples\nare processed simultaneously. This strategy effectively compensates for any filtered-out examples\nby ensuring continuous generation of alternative samples, thereby minimizing or entirely masking\nthe time loss associated with discarded examples. This optimized asynchronous sampling approach\ngreatly enhances the efficiency and stability of the RLVR training process (Bae et al., 2025).\n3.3\nReasoning Length Controllability (LC)\nReinforcement learning with Large Reasoning Models (LRMs) enables drastic improvements in\ncomplex reasoning capabilities, but often accompanies undesired tendencies to overthink (Chen\net al., 2024; Sui et al., 2025) or even underthink (Wang et al., 2025) compared to the optimal reason-\ning length. For practical and flexible deployment of computationally expensive LRMs, we identify\nlength controllability (LC) as a key desideratum. To induce LC in HyperCLOVA X THINK, we ad-\nditionally incorporate the length-penalized reward functions introduced by Aggarwal and Welleck,\n2025.\nOn top of the training configurations from the previous RLVR stage, we train our model on the\nlength-penalized reward functions (L1-Exact and L1-Max) from Aggarwal and Welleck, 2025. We\nappend ‘Think for maximum N tokens’ on the input instructions, where we sample N from\na discrete token budget set of B = {1024, 2048, 4096, 8192, 16384} to accelerate LC capability2.\n2The original L1 paper randomly samples N from U[100,4000]\n9\n\nWe first train the model on the L1-Exact penalty for about 300 steps to acquire LC and subsequently\nabout 100 steps on the L1-Max penalty to greedily reduce the reasoning length when possible.\n3.4\nReinforcement Learning from Human Feedback (RLHF)\nReinforcement Learning from Human Feedback (RLHF) aligns model outputs with human prefer-\nences and practical usability. By combining reasoning/non-reasoning RLHF and RLVR, we concur-\nrently refine model behavior to improve alignment with human preferences while preserving and\nenhancing reasoning abilities.\nTo better align the model’s outputs with human preferences, we first train a reward model using\na combined set of human preference data, as detailed in our previous technical report (Yoo et al.,\n2024b). This data consists of pairwise comparisons either annotated by expert raters or inferred via\nscoring from in-house judge models. The reward model learns to predict scores for each sequence\nin non-reasoning data. Following this, we use GRPO explained in Section 3.2 as the core RLHF\nalgorithm. The policy is optimized to maximize the expected reward predicted by the reward model.\nUnlike RLVR, we apply a KL penalty of 0.1 to maintain proximity to the SFT checkpoint. This\nrelatively strong KL penalty prioritizes training stability over exploration in RLHF.\nThe prompts used during RLHF training consist of a mixture of reasoning and non-reasoning tasks.\nFor non-reasoning, the model is expected to generate assistant response directly, while for reasoning,\nthe model first generates intermediate think step followed by assistant response. The reward model\nevaluates only the response portion of the output and the think portion is not directly scored, allowing\nthe model to freely develop internal reasoning patterns.\nLastly, when training with RLHF subsequently after RLVR, we observe a slight degradation in the\nmodel’s reasoning ability that was optimized during the RLVR phase. A similar pattern was also\nobserved in other reasoning models (Yang et al., 2025). To address this issue, we adopt a joint\ntraining strategy where RLVR and RLHF are trained concurrently. Specifically, we interleave the\ntraining batches such that each batch contains a mixture of samples from RLVR and RLHF datasets.\nThis approach preserves the performance gains of both RLHF and RLVR while unifying the training\nphases, resulting in a simpler and more effective training pipeline.\n4\nEvaluation\n4.1\nBaselines\nWe compare our model against publicly available models of comparable size that are recognized\nfor their reasoning capabilities, including Qwen3-14B, Qwen3-32B (Yang et al., 2025), QwQ-\n32B (Qwen Team, 2025), and EXAONE-Deep-32B (LG AI Research, 2025). We utilize evaluation\nscores directly from each model’s original paper when available. Otherwise, we conduct our own\nevaluations and report the corresponding results.\n4.2\nEvaluation Protocol\nWhen published metrics are unavailable, we perform in-house evaluations using primarily public\nbenchmark sets, with the exception of KoBigBench (Yoo et al., 2024b). The primary goal of our\nevaluation strategy is to interpret and extract the predicted answers from language models for both\nopen-ended and multiple-choice benchmarks as accurately as possible. Models often fail to produce\na final answer when asked to generate the reasoning chain and the answer consecutively in a single\npass. To address this, we adopt a two-pass generation scheme: the model first produces the reasoning\nchain with our chat template (<|im_start|>assistant/think\\n...<|im_end|>), and\nwe then generate the answer by appending an answer prefix. Our evaluation framework combines\nLM Eval Harness Gao et al. (2023) with an in-house toolkit that we plan to release soon for public\nreference.\nFor our model, we configure the generation temperature at 0.5 and top-p at 0.95. In the case of\nother models, their authors’ recommended optimal hyperparameters are utilized. All evaluations are\nperformed using zero-shot Chain-of-Thought (CoT) reasoning, and the maximum CoT generation\nlength is uniformly set to 4096.\n10\n\nGeneral Aptitude\nCulture & Language\nInstruction\nFollowing\n0\n20\n40\n60\n80\n100\nAverage Score\n69.4\n84.6\n92.8\n65.4\n76.7\n88.4\n62.9\n75.6\n89.8\n60.2\n78.0\n87.6\n56.3\n70.7\n80.7\nHyperCLOVA X THINK\nQwen3 32B\nQwen3 14B\nQwQ 32B\nEXAONE Deep 32B\nFigure 5: Summary of model performance on (1) General Aptitude, (2) Culture and Language,\nand (3) Instruction-following benchmarks specifically focused on Korea. The instruction-following\nbenchmark scores are normalized by multiplying their original values by 10.\n4.3\nKorea-Centric Benchmarks\nSetup. As introduced in Section 1, our model’s general performance is evaluated against various\nbaselines using a set of Korea-centric benchmarks. These evaluations are designed to assess the\nmodel’s understanding of Korean culture and knowledge. To achieve this, we curated datasets specif-\nically pertaining to Korea:\n• General Aptitude: KMMLU (Son et al., 2025) and CSAT gauge general Korean knowl-\nedge. KorMedMCQA (Kweon et al., 2024) focuses on medical problem-solving and\nKoBALT-700 (Shin et al., 2025) assesses linguistic depth and typological grounding in\nKorean.\n• Culture and Language: HAERAE-1.0 (Son et al., 2024), CLIcK (Kim et al., 2024a), and\nKoBigBench3 evaluate Korean-specific cultural, geographical, historical knowledge, etc.\n• Instruction-Following: LogicKor (Park, 2024) and KoMTBench (LG AI Research, 2024)\nmeasure the model’s ability to follow Korean instructions.\nResult. Our model’s strong performance on the comprehensive aptittude tests, Korean-specific cul-\nture and linguistic benchmarks, and a suite of benchmarks for probing instruction-following capa-\nbilities is summarized in Figure 5 and detailed in Table 3. By employing zero-shot CoT prompting\nto elicit robust reasoning and evaluating answers based on accuracy, we demonstrate that THINK\nsurpasses other baselines. Additional evaluation results can be found in the Appendix B and Ap-\npendix C. Furthermore, this superior performance is achieved with a relatively small computational\ncost, which will be discussed further in the subsequent section.\n5\nAnalysis\n5.1\nTraining Efficiency\nThere has been research showing that model performance consistently improves with increases in\ndata volume, parameter count, and computational resources in accordance with Scaling Laws (Ka-\nplan et al., 2020). This has also been further supported by more recent work on Expanded Neural\n3The dataset will be publicly released.\n11\n\nCategory\nBenchmarks\nHCX\nTHINK\nQwen3\nQwen3\nQwQ\nEXAONE\nDeep\n( - )\n(32B)\n(14B)\n(32B)\n(32B)\nGeneral\nAptitude\nKMMLU\n69.7\n63.5\n49.3\n54.1\n53.6\nCSAT\n83.2\n81.9\n77.1\n84.7\n69.7\nKorMedMCQA\n76.0\n74.7\n68.5\n69.4\n68.8\nKoBALT\n48.9\n41.4\n38.4\n32.4\n33.0\nCulture &\nLanguage\nHAERAE\n87.8\n75.1\n74.1\n76.2\n74.7\nCLIcK\n80.1\n71.1\n68.8\n73.6\n62.2\nKoBigBench\n85.9\n83.9\n83.8\n84.1\n75.3\nInstruction-\nFollowing\nLogicKor\n9.65\n8.93\n9.15\n9.02\n8.54\nKoMTBench\n8.90\n8.75\n8.82\n8.50\n7.59\nTable 3: Performance comparison of language models on Korea-centric benchmarks. Models are\nevaluated across comprehensive understanding, cultural sovereignty, and chat-based instruction-\nfollowing tasks, highlighting their capabilities and adaptability within a Korean context.\nHyperCLOVA X\nTHINK (base)\nQWEN2.5-0.5B\nQWEN2.5-1.5B\nLLaMA3-8B\nQWEN2.5-14B\nQWEN2.5-32B\nLLaMA3-70B\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\n1.2\n1.4\nGPU Hours (A100, MFU 50%)\n1e7\nFigure 6: Training Efficiency (GPU Hours / A100 / MFU 50%)\nScaling(Chang et al., 2024). However, recent studies have increasingly emphasized the importance\nof data quality. For example, Chang et al. (2024) quantifies data diversity and quality through the\nconcept of effective training tokens and proposes a corresponding scaling law. This study experi-\nmentally demonstrates that efficient training and performance improvements are achievable even for\nsmaller models.\nIt was reported that Qwen2.5 significantly improved its reasoning and long-context generation ca-\npabilities using 18 trillion tokens of high-quality training data and advanced post-training strate-\ngies(Qwen et al., 2025). Similarly, LLaMA 3, trained on 15 trillion tokens, showed continued per-\nformance gains even after surpassing the Chinchilla-optimal range. This suggests that while data\nscaling remains important, an approach centered on data quality is also necessary. Furthermore, as\nthe volume of natural language data available for collection from the internet approaches its limits,\na paradigm shift from data quantity to data quality is accelerating.\nTHINK is developed with a focus on creating a high-efficiency architecture and a training strategy\ngrounded in high-quality data. As a result, it required significantly fewer GPU hours than similar\nsized models to be trained, as shown in Figure 6. At the same time, it achieves competitive perfor-\n12\n\nCross-lingual Consistency (En, Ko)\nMT\n(✓, ✓) ↑\n(✓, ✗) ↓\n(✗, ✓) ↓\n(✗, ✗) ↑\nKo→En\nEn→Ko\nQwen3 32B\n81.0\n8.0\n4.5\n6.5\n92.8\n85.3\nEXAONE Deep 32B\n62.5\n22\n3.5\n12.0\n85.6\n77.5\nHyperCLOVA X THINK\n74.5\n12.0\n4.5\n9.0\n90.3\n85.8\nTable 4: Cross-lingual transferability between English and Korean. Each consistency column shows\nthe case of MCQA items for which the model is correct (✓) or incorrect (✗) in English (first symbol)\nand Korean (second symbol). Higher symmetric ((✓, ✓) and (✗, ✗)) and lower asymmetric ( (✗, ✓),\n(✓, ✗)) ratios imply stronger consistency. We also report xCOMET translation quality of Flores on\nboth directions.\nmance. This demonstrates that strategic data curation and training efficiency are critical factors in\ndeveloping high-performance LLMs, moving beyond reliance on sheer resource input.\n5.2\nCross-Lingual Transferability\nIn this subsection, we investigate cross-lingual consistency and bi-directional translation quality be-\ntween Korean and English to evaluate whether the model properly transfers the acquired English\nknowledge into Korean. Our cross-lingual evaluation hypothesis is twofold. First, a model that has\nefficiently encoded both languages should yield semantically equivalent answers when parallel ques-\ntions are posed in English and Korean. Second, if the same underlying representations truly capture\nlanguage-agnostic meaning, the model should also display strong translation ability in both direc-\ntions.\nCross-lingual Consistency. In order to compute the score of cross-lingual consistency, we adopt\nthe pipeline proposed by Qi et al. (2023); Xing et al. (2024); Yoo et al. (2024a) and evaluate with\nGlobal-MMLU-Lite (Singh et al., 2024). We only computed the scores in culturally agnostic samples\nto exclude examples whose gold answers depend on the source language. Our experiment utilizes\nCaliper framework, described in Section. 4.2. We categorize the model’s predictions on parallel\nEnglish-Korean MCQA prediction results into four cases: (1) (✓,✓) represents question answered\ncorrectly in both languages indicating the desired cross-lingual aligned, (2) (✓, ✗) is the number of\nsamples that are correct in English but incorrect in Korean, isolating failures of knowledge trans-\nfer, (3) (✗, ✓) represents the opposite scenario, and (4) (✗, ✗) records items answered incorrectly\nin both languages, reflecting residual knowledge gaps. This decomposition enables us to attribute\nimprovements in overall accuracy to genuine bilingual robustness.\nAs shown in Table 4, THINK achieves a comparable (✓,✓) ratio (74.5%) ,only a few points behind\nthe extensively trained Qwen3 32B, while limiting asymmetric errors to 16.5%. Given that THINK\nwas tuned almost exclusively on the Korean–English pair and required a fraction of the compute\nbudget demonstrated in Section 5.1, this result indicates that carefully targeted bilingual training\ncan offset much of the scale advantage by large multilingual models. Although the remaining asym-\nmetric cases highlight room for improvement, THINK already delivers a robust and cost-efficient\nbilingual representation. Its overall consistency is also higher than that of EXAONE Deep 32B,\nsuggesting that strategic data curation can sometimes outweigh pure parameter count.\nMachine translation. To complement the cross-lingual transferability analysis in cross-lingual con-\nsistency with MCQA task, we next assess bidirectional machine translation (MT) performance of\neach model between Korean and English. We adopt the Flores benchmark Team et al. (2022) and\ntranslate the official sub-samples of test dataset in both directions ( En→Ko, Ko→En). A prompt\nof each model includes the same 1-shot example. We only extract the translation part from the re-\nsponse. Each translation quality is measured with xCOMET-XL Guerreiro et al. (2023), a model\nbased metric that has a stronger correlation with professional human judgment compared to BLEU\nand ChrF. We report xCOMET-XL score for each direction. This performance indicates the model\ncan faithfully re-express the same underlying knowledge as fluent Korean or English.\nMT columns of Table 4 provide additional evidence of THINK’s robust cross-lingual transferability\nin a generation task. In Ko→En direction, THINK achieves a competitive xCOMET score as 90.3,\n13\n\nclosely approaching the performance of Qwen3 32B model (92.8). Furthermore, in the opposite di-\nrection (En→Ko), THINK surpasses all other models with a score of 85.8. This result indicates that\nour training pipeline not only preserves English knowledge but also enhances the model’s ability to\nrender it into high-quality Korean. These findings, combined with the consistency results, validate\nthat our data curation can deliver bidirectional translation capability without the extensive computa-\ntional overhead of full-scale multilingual pre-training.\n6\nExtensions\n6.1\nInstilling Vision-Language Reasoning in Korean\nThe pursuit of sovereign multimodal AI requires not only proficiency in native languages but also\nrobust capabilities for reasoning across modalities. Given that THINK was originally developed\nand optimized for advanced reasoning in text, can it be effectively extended into vision-grounded\nreasoning through a dedicated multimodal post-training pipeline?\nIn this subsection, we present a separate experimental branch: Starting from the text SFT pipeline\n(Section 3.1), we incorporate visual modules and multimodal tuning to construct a vision-language\nmodel. This enables direct evaluation of complex vision-language reasoning beyond simple transfer\nfrom text-only capabilities. For real-world assessment, we use challenging STEM items from the\nKorean College Scholastic Ability Test (KCSAT). As the KCSAT is administered in Korean and\nreflects rigorous local standards, it is suitable as a test of vision-language reasoning ability in Korean.\nEach item is presented to the model as an image containing mathematical expressions, tables, di-\nagrams, and scientific text (See Appendix D). The model must first accurately recognize visual\ncontent (e.g., text, layout, object and diagram recognition), then perform multi-step logical reason-\ning. Here, vision-language reasoning refers to this integrated process of visual understanding and\nabstract problem-solving, beyond perceptual recognition alone.\nArchitecture and Training. For vision-language reasoning, we augment the LLM backbone with a\nvisual encoder module, similar to the architecture in our previous work, HyperCLOVA X SEED (Hy-\nperCLOVA X Team, 2024). The architecture is composed of:\n• Vision Encoder: SigLIP-2 (Tschannen et al., 2025), operating at 512×512 pixels per grid.\n• Vision-Language Model Architecture: LLaVA-1.5-HD-based framework (Liu et al.,\n2024) with C-Abstractor (Cha et al., 2024) connector mechanism, supporting up to 1.57M\ntotal pixels distributed over 6 grids.\nThe training pipeline extends our previous protocol (Kim et al., 2024b) by inserting a dedicated\nvision SFT stage between text SFT and multimodal RLHF. More concretely, after pre-training on\nlarge-scale text corpora, we first apply SFT on instruction-oriented text data, then conduct multi-\nmodal SFT with paired image-text data, and finally perform multimodal RLHF on both text-only\nand multimodal instructions. This change to RLHF—incorporating vision-language samples in ad-\ndition to text—distinguishes our ablation pipeline from standard text-only approaches. Reasoning\nMode (Section 3) is toggled via explicit prompting throughout both model training and inference,\nwith ablations performed using both reasoning-enabled and baseline prompt formats.\nExperiments. We evaluate vision-language reasoning performance on the multimodal Korean Edu-\ncational Test benchmark (Park and Kim, 2025), with primary focus on its most difficult subset: the\nKCSAT STEM subjects. This evaluation set comprises 206 items spanning mathematics, physics,\nchemistry, earth science, and biology, each requiring advanced logical and visual inference at both\nbasic and advanced levels. The KCSAT is internationally recognized for its depth and rigor, making\nit an exemplary proxy for high-stakes, real-world STEM reasoning.\nOur experiments compare THINK with Vision against leading contemporary closed APIs in the mul-\ntimodal LLM space—specifically GPT-4 Turbo with Vision (OpenAI et al., 2024c), GPT-4o (Ope-\nnAI et al., 2024a), GPT-4.1 (OpenAI et al., 2024c) and OpenAI-o1 (OpenAI et al., 2024b). All mod-\nels are assessed under strictly identical protocols using the same visual and textual input, ensuring a\nfully standardized evaluation environment.\nResults and Analysis. As summarized in Table 5, THINK attains an overall accuracy of 46.4% on\nthe KCSAT STEM benchmark, outperforming GPT-4.1 (40.3%) and approaching the performance of\n14\n\nModel\nMath\nPhysics\nChemistry\nEarth Science\nBiology\nOverall\nBasic\nAdv.\nBasic\nAdv.\nBasic\nAdv.\nBasic\nAdv.\nBasic\nAdv.\nGPT-4 Turbo with Vision\n54.5\n20.8\n5.0\n15.0\n15.0\n20.0\n30.0\n25.0\n10.0\n40.0\n23.8\nGPT-4o\n68.2\n50.0\n15.0\n20.0\n25.0\n25.0\n40.0\n30.0\n15.0\n25.0\n32.0\nGPT-4.1\n68.2\n66.7\n20.0\n30.0\n40.0\n30.0\n30.0\n40.0\n35.0\n35.0\n40.3\nOpenAI-o1\n93.2\n83.3\n42.5\n40.0\n38.8\n56.3\n32.5\n42.5\n37.5\n31.3\n50.9\nHyperCLOVA X THINK with Vision\n68.2\n68.1\n33.3\n28.3\n41.7\n58.3\n28.3\n38.3\n43.3\n50.0\n46.4\nw/o Reasoning Mode\n22.7\n20.8\n11.7\n26.7\n11.7\n28.3\n20.0\n23.3\n30.0\n21.7\n21.7\nTable 5: Evaluation of native vision-language reasoning ability on the KCSAT STEM multimodal\nbenchmark (Park and Kim, 2025) by subject and level. The benchmark consists of 206 questions\ncovering five scientific subjects (basic/advanced), with 20 to 24 questions per subject and level.\nKCSAT is widely regarded for its difficulty, emphasis on scientific reasoning, and its reflection of\nKorea’s high-achieving STEM education system.\nGPT-o1 (50.9%). Disabling Reasoning Mode notably causes performance to drop to 21.7%, support-\ning the conclusion that advanced reasoning skills acquired during language pretraining are crucial\nand can be effectively extended to vision-centric STEM challenges when combined with specialized\nmultimodal tuning. Further qualitative analyses and representative sample outputs are provided in\nAppendix D.\nOn the other hand, we note a modest trade-off: adding multimodal SFT introduces a slight decrease\nin text-only reasoning performance, underscoring the inherent difficulty of jointly optimizing for\nboth modalities. Achieving a balanced sovereign AI—excelling in both unimodal and multimodal\nreasoning—remains open for further tuning and methodological advances. These observations high-\nlight that effective vision-language reasoning, especially in STEM, demands robust integration of\nvisual parsing and native multi-step logic. As a next step, our future work will extend the model’s\ncapabilities towards unified, native reasoning across text, vision, and audio.\n6.2\nLightening through Pruning and Distillation\nAs competition in high-performance model development intensifies, training models with tens or\nhundreds of billions of parameters using trillions of tokens has become the de facto industry stan-\ndard. This large-scale training approach entails high costs and long development cycles, while lim-\niting the ability to respond to rapidly changing service environments. Consequently, learning strate-\ngies that can efficiently build LLMs with fewer resources have recently gained attention. These\napproaches not only reduce costs, but also offer practical advantages in terms of timely model de-\nvelopment and operation.\nOne of the leading approaches combines pruning and knowledge distillation. Pruning reduces model\nsize by removing less important parameters, while distillation is a technique that transfers knowl-\nedge learned by large models to lightweight models to maintain performance. Combining these two\ntechniques can achieve both model compression and performance preservation simultaneously.\nAs a real-world example, HyperCLOVA X SEED 0.5B, recently released on HuggingFace, is the\nfirst open-source model in the HyperCLOVA X series trained using pruning and knowledge distil-\nlation. Despite being similar in size to Qwen2.5-0.5BQwen et al. (2025), it was trained at approx-\nimately 39 times lower cost and outperformed competing models in most benchmarks. Notably, it\ndemonstrated significant performance improvements in Korean language benchmarks. This model\noffers high practical value as it enables high-performance conversational interfaces even in resource-\nconstrained environments such as mobile applications or smart home devices.\nFurthermore, the combination of pruning and distillation can be utilized for efficient production\nof both lightweight and large models. Depending on the structure of the teacher model used for\ntraining, the type of data to be transferred, and the learning strategy, models of various sizes and\npurposes can be produced. This flexibility is expected to improve usability across future generative\nAI applications. Currently, a pruned and distilled version of THINK is under preparation to be open-\nsourced.\n15\n\n7\nConclusion\nIn this report, we introduced HyperCLOVA X THINK, the first reasoning-focused LLM within Hy-\nperCLOVA X family. It is efficiently trained to achieve two primary objectives: advanced reasoning\ncapabilities and the promotion of sovereign AI for Korea.\nIts pre-training dataset comprises approximately 6 trillion high-quality tokens spanning Korean, En-\nglish, and further enhanced by targeted synthetic Korean data. We employ a Peri-LN Transformer\nscaled with µP, ensuring stable scalability and cost-efficient training. A three-stage curriculum en-\nables the model to expand its context window to 128k tokens and demonstrate robust long-form\nchain-of-thought reasoning. Post-training involves supervised fine-tuning and reinforcement learn-\ning with verifiable rewards, utilizing a curated data filtering process to address both detailed reason-\ning and simple answering tasks.\nExperiments demonstrate HyperCLOVA X THINK’s competitive performance against other reason-\ning models on Korea-centric benchmarks such as KMMLU, CSAT, KoBALT-700, HAERAE-1.0,\nand KoBigBench. Analysis highlights its highly efficient training cost and its ability to preserve\nrobust bilingual consistency. Furthermore, a vision-augmented variant achieved performance com-\nparable to GPT-4.1 on the KCSAT STEM benchmark.\nOur report shows that using additional test-time compute to refine model responses is an effective\nway to push the limits of model capability and improve compute-cost efficiency. As foundational AI\ntechnology gains greater potential to enrich people’s lives and shape the future of digital business,\nwe remain committed to improving reasoning scalability and delivering foundation models that are\nboth powerful and affordable, thereby accelerating both domestic and global AI transformation in\nbusinesses.\nNote that, while we took necessary measures to improve the safety of HyperCLOVA X THINK as\nper NAVER AI Ethics guidelines, the harmlessness of the generated text cannot be fully guaran-\nteed. Thus, the responses may contain toxic remarks, exhibit biases or otherwise harmful content.\nHowever, we remain dedicated to responsible AI development and deployment.\nLastly, we plan to open-source a pruned and distilled version of HyperCLOVA X THINK. This\ninitiative aims to benefit academic and industry partners with limited resources, fostering the future\ndevelopment and utilization of sovereign LLMs.\nReferences\nPranjal Aggarwal and Sean Welleck. 2025. L1: Controlling how long a reasoning model thinks with\nreinforcement learning. arXiv preprint arXiv:2503.04697.\nSanghwan Bae, Jiwoo Hong, Min Young Lee, Hanbyul Kim, JeongYeon Nam, and Donghyun Kwak.\n2025. Online difficulty filtering for reasoning oriented reinforcement learning. arXiv preprint\narXiv:2504.03380.\nLoubna Ben Allal, Anton Lozhkov, Guilherme Penedo, Thomas Wolf, and Leandro von Werra. 2024.\nCosmopedia.\nXiao Bi, Deli Chen, Guanting Chen, Shanhuang Chen, Damai Dai, Chengqi Deng, Honghui Ding,\nKai Dong, Qiushi Du, Zhe Fu, Huazuo Gao, Kaige Gao, Wenjun Gao, Ruiqi Ge, Kang Guan, Daya\nGuo, Jianzhong Guo, Guangbo Hao, Zhewen Hao, Ying He, Wenjie Hu, Panpan Huang, Erhang\nLi, Guowei Li, Jiashi Li, Yao Li, Y. K. Li, Wenfeng Liang, Fangyun Lin, Alex X. Liu, Bo Liu,\nWen Liu, Xiaodong Liu, Xin Liu, Yiyuan Liu, Haoyu Lu, Shanghao Lu, Fuli Luo, Shirong Ma,\nXiaotao Nie, Tian Pei, Yishi Piao, Junjie Qiu, Hui Qu, Tongzheng Ren, Zehui Ren, Chong Ruan,\nZhangli Sha, Zhihong Shao, Junxiao Song, Xuecheng Su, Jingxiang Sun, Yaofeng Sun, Minghui\nTang, Bingxuan Wang, Peiyi Wang, Shiyu Wang, Yaohui Wang, Yongji Wang, Tong Wu, Y. Wu,\nXin Xie, Zhenda Xie, Ziwei Xie, Yiliang Xiong, Hanwei Xu, R. X. Xu, Yanhong Xu, Dejian Yang,\nYuxiang You, Shuiping Yu, Xingkai Yu, B. Zhang, Haowei Zhang, Lecong Zhang, Liyue Zhang,\nMingchuan Zhang, Minghua Zhang, Wentao Zhang, Yichao Zhang, Chenggang Zhao, Yao Zhao,\nShangyan Zhou, Shunfeng Zhou, Qihao Zhu, and Yuheng Zou. 2024. Deepseek LLM: scaling\nopen-source language models with longtermism. CoRR, abs/2401.02954.\n16\n\nCody Blakeney, Mansheej Paul, Brett W. Larsen, Sean Owen, and Jonathan Frankle. 2024. Does\nyour data spark joy? performance gains from domain upsampling at the end of training. CoRR,\nabs/2406.03476.\nTom B. Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared Kaplan, Prafulla Dhari-\nwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, Sandhini Agarwal,\nAriel Herbert-Voss, Gretchen Krueger, Tom Henighan, Rewon Child, Aditya Ramesh, Daniel M.\nZiegler, Jeffrey Wu, Clemens Winter, Christopher Hesse, Mark Chen, Eric Sigler, Mateusz Litwin,\nScott Gray, Benjamin Chess, Jack Clark, Christopher Berner, Sam McCandlish, Alec Radford,\nIlya Sutskever, and Dario Amodei. 2020. Language models are few-shot learners. In Advances in\nNeural Information Processing Systems 33: Annual Conference on Neural Information Process-\ning Systems 2020, NeurIPS 2020, December 6-12, 2020, virtual.\nJunbum Cha, Wooyoung Kang, Jonghwan Mun, and Byungseok Roh. 2024. Honeybee: Locality-\nenhanced Projector for Multimodal LLM. In Proceedings of the IEEE/CVF Conference on Com-\nputer Vision and Pattern Recognition (CVPR).\nErnie Chang, Matteo Paltenghi, Yang Li, Pin-Jie Lin, Changsheng Zhao, Patrick Huber, Zechun\nLiu, Rastislav Rabatin, Yangyang Shi, and Vikas Chandra. 2024. Scaling parameter-constrained\nlanguage models with quality data. In Proceedings of the 2024 Conference on Empirical Methods\nin Natural Language Processing: Industry Track, pages 80–97, Miami, Florida, US. Association\nfor Computational Linguistics.\nXingyu Chen, Jiahao Xu, Tian Liang, Zhiwei He, Jianhui Pang, Dian Yu, Linfeng Song, Qiuzhi\nLiu, Mengfei Zhou, Zhuosheng Zhang, et al. 2024. Do not think that much for 2+ 3=? on the\noverthinking of o1-like llms. arXiv preprint arXiv:2412.21187.\nDaixuan Cheng, Yuxian Gu, Shaohan Huang, Junyu Bi, Minlie Huang, and Furu Wei. 2024. Instruc-\ntion pre-training: Language models are supervised multitask learners. In Proceedings of the 2024\nConference on Empirical Methods in Natural Language Processing, EMNLP 2024, Miami, FL,\nUSA, November 12-16, 2024, pages 2529–2550. Association for Computational Linguistics.\nDeepSeek-AI, Daya Guo, Dejian Yang, Haowei Zhang, Junxiao Song, Ruoyu Zhang, Runxin Xu,\nQihao Zhu, Shirong Ma, Peiyi Wang, Xiao Bi, Xiaokang Zhang, Xingkai Yu, Yu Wu, Z. F. Wu,\nZhibin Gou, Zhihong Shao, Zhuoshu Li, Ziyi Gao, Aixin Liu, Bing Xue, Bingxuan Wang, Bochao\nWu, Bei Feng, Chengda Lu, Chenggang Zhao, Chengqi Deng, Chenyu Zhang, Chong Ruan,\nDamai Dai, Deli Chen, Dongjie Ji, Erhang Li, Fangyun Lin, Fucong Dai, Fuli Luo, Guangbo Hao,\nGuanting Chen, Guowei Li, H. Zhang, Han Bao, Hanwei Xu, Haocheng Wang, Honghui Ding,\nHuajian Xin, Huazuo Gao, Hui Qu, Hui Li, Jianzhong Guo, Jiashi Li, Jiawei Wang, Jingchang\nChen, Jingyang Yuan, Junjie Qiu, Junlong Li, J. L. Cai, Jiaqi Ni, Jian Liang, Jin Chen, Kai\nDong, Kai Hu, Kaige Gao, Kang Guan, Kexin Huang, Kuai Yu, Lean Wang, Lecong Zhang,\nLiang Zhao, Litong Wang, Liyue Zhang, Lei Xu, Leyi Xia, Mingchuan Zhang, Minghua Zhang,\nMinghui Tang, Meng Li, Miaojun Wang, Mingming Li, Ning Tian, Panpan Huang, Peng Zhang,\nQiancheng Wang, Qinyu Chen, Qiushi Du, Ruiqi Ge, Ruisong Zhang, Ruizhe Pan, Runji Wang,\nR. J. Chen, R. L. Jin, Ruyi Chen, Shanghao Lu, Shangyan Zhou, Shanhuang Chen, Shengfeng\nYe, Shiyu Wang, Shuiping Yu, Shunfeng Zhou, Shuting Pan, S. S. Li, Shuang Zhou, Shaoqing\nWu, Shengfeng Ye, Tao Yun, Tian Pei, Tianyu Sun, T. Wang, Wangding Zeng, Wanjia Zhao, Wen\nLiu, Wenfeng Liang, Wenjun Gao, Wenqin Yu, Wentao Zhang, W. L. Xiao, Wei An, Xiaodong\nLiu, Xiaohan Wang, Xiaokang Chen, Xiaotao Nie, Xin Cheng, Xin Liu, Xin Xie, Xingchao Liu,\nXinyu Yang, Xinyuan Li, Xuecheng Su, Xuheng Lin, X. Q. Li, Xiangyue Jin, Xiaojin Shen, Xi-\naosha Chen, Xiaowen Sun, Xiaoxiang Wang, Xinnan Song, Xinyi Zhou, Xianzu Wang, Xinxia\nShan, Y. K. Li, Y. Q. Wang, Y. X. Wei, Yang Zhang, Yanhong Xu, Yao Li, Yao Zhao, Yaofeng\nSun, Yaohui Wang, Yi Yu, Yichao Zhang, Yifan Shi, Yiliang Xiong, Ying He, Yishi Piao, Yisong\nWang, Yixuan Tan, Yiyang Ma, Yiyuan Liu, Yongqiang Guo, Yuan Ou, Yuduan Wang, Yue Gong,\nYuheng Zou, Yujia He, Yunfan Xiong, Yuxiang Luo, Yuxiang You, Yuxuan Liu, Yuyang Zhou,\nY. X. Zhu, Yanhong Xu, Yanping Huang, Yaohui Li, Yi Zheng, Yuchen Zhu, Yunxian Ma, Ying\nTang, Yukun Zha, Yuting Yan, Z. Z. Ren, Zehui Ren, Zhangli Sha, Zhe Fu, Zhean Xu, Zhenda\nXie, Zhengyan Zhang, Zhewen Hao, Zhicheng Ma, Zhigang Yan, Zhiyu Wu, Zihui Gu, Zijia Zhu,\nZijun Liu, Zilin Li, Ziwei Xie, Ziyang Song, Zizheng Pan, Zhen Huang, Zhipeng Xu, Zhongyu\nZhang, and Zhen Zhang. 2025. Deepseek-r1: Incentivizing reasoning capability in llms via rein-\nforcement learning.\n17\n\nAbhimanyu Dubey, Abhinav Jauhri, Abhinav Pandey, Abhishek Kadian, Ahmad Al-Dahle, Aiesha\nLetman, Akhil Mathur, Alan Schelten, Amy Yang, Angela Fan, Anirudh Goyal, Anthony\nHartshorn, Aobo Yang, Archi Mitra, Archie Sravankumar, Artem Korenev, Arthur Hinsvark,\nArun Rao, Aston Zhang, Aurélien Rodriguez, Austen Gregerson, Ava Spataru, Baptiste Rozière,\nBethany Biron, Binh Tang, Bobbie Chern, Charlotte Caucheteux, Chaya Nayak, Chloe Bi, Chris\nMarra, Chris McConnell, Christian Keller, Christophe Touret, Chunyang Wu, Corinne Wong,\nCristian Canton Ferrer, Cyrus Nikolaidis, Damien Allonsius, Daniel Song, Danielle Pintz, Danny\nLivshits, David Esiobu, Dhruv Choudhary, Dhruv Mahajan, Diego Garcia-Olano, Diego Perino,\nDieuwke Hupkes, Egor Lakomkin, Ehab AlBadawy, Elina Lobanova, Emily Dinan, Eric Michael\nSmith, Filip Radenovic, Frank Zhang, Gabriel Synnaeve, Gabrielle Lee, Georgia Lewis Anderson,\nGraeme Nail, Grégoire Mialon, Guan Pang, Guillem Cucurell, Hailey Nguyen, Hannah Korevaar,\nHu Xu, Hugo Touvron, Iliyan Zarov, Imanol Arrieta Ibarra, Isabel M. Kloumann, Ishan Misra,\nIvan Evtimov, Jade Copet, Jaewon Lee, Jan Geffert, Jana Vranes, Jason Park, Jay Mahadeokar,\nJeet Shah, Jelmer van der Linde, Jennifer Billock, Jenny Hong, Jenya Lee, Jeremy Fu, Jianfeng\nChi, Jianyu Huang, Jiawen Liu, Jie Wang, Jiecao Yu, Joanna Bitton, Joe Spisak, Jongsoo Park,\nJoseph Rocca, Joshua Johnstun, Joshua Saxe, Junteng Jia, Kalyan Vasuden Alwala, Kartikeya\nUpasani, Kate Plawiak, Ke Li, Kenneth Heafield, Kevin Stone, and et al. 2024. The llama 3 herd\nof models. CoRR, abs/2407.21783.\nLeo Gao, Jonathan Tow, Baber Abbasi, Stella Biderman, Sid Black, Anthony DiPofi, Charles Fos-\nter, Laurence Golding, Jeffrey Hsu, Alain Le Noac’h, Haonan Li, Kyle McDonell, Niklas Muen-\nnighoff, Chris Ociepa, Jason Phang, Laria Reynolds, Hailey Schoelkopf, Aviya Skowron, Lintang\nSutawika, Eric Tang, Anish Thite, Ben Wang, Kevin Wang, and Andy Zou. 2023. A framework\nfor few-shot language model evaluation.\nNuno M Guerreiro, Ricardo Rei, Daan van Stigt, Luisa Coheur, Pierre Colombo, and André FT\nMartins. 2023. xcomet: Transparent machine translation evaluation through fine-grained error\ndetection. arXiv preprint arXiv:2310.10482.\nJordan Hoffmann, Sebastian Borgeaud, Arthur Mensch, Elena Buchatskaya, Trevor Cai, Eliza\nRutherford, Diego de Las Casas, Lisa Anne Hendricks, Johannes Welbl, Aidan Clark, Tom Hen-\nnigan, Eric Noland, Katie Millican, George van den Driessche, Bogdan Damoc, Aurelia Guy,\nSimon Osindero, Karen Simonyan, Erich Elsen, Jack W. Rae, Oriol Vinyals, and Laurent Sifre.\n2022. Training compute-optimal large language models. CoRR, abs/2203.15556.\nShengding Hu, Yuge Tu, Xu Han, Chaoqun He, Ganqu Cui, Xiang Long, Zhi Zheng, Yewei Fang,\nYuxiang Huang, Weilin Zhao, Xinrong Zhang, Zhen Leng Thai, Kai Zhang, Chongyi Wang, Yuan\nYao, Chenyang Zhao, Jie Zhou, Jie Cai, Zhongwu Zhai, Ning Ding, Chao Jia, Guoyang Zeng,\nDahai Li, Zhiyuan Liu, and Maosong Sun. 2024. Minicpm: Unveiling the potential of small\nlanguage models with scalable training strategies. CoRR, abs/2404.06395.\nNAVER\nCloud\nHyperCLOVA\nX\nTeam.\n2024.\nHyperCLOVA\nX\nSEED\nVi-\nsion\nInstruct\n3B.\nhttps://huggingface.co/naver-hyperclovax/\nHyperCLOVAX-SEED-Vision-Instruct-3B.\nAvailable on Hugging Face Hub.\nAccessed: 2025-06-23.\nArmand Joulin, Edouard Grave, Piotr Bojanowski, Matthijs Douze, Hervé Jégou, and Tomás\nMikolov. 2016. Fasttext.zip: Compressing text classification models. CoRR, abs/1612.03651.\nArmand Joulin, Edouard Grave, Piotr Bojanowski, and Tomás Mikolov. 2017. Bag of tricks for\nefficient text classification. In Proceedings of the 15th Conference of the European Chapter of the\nAssociation for Computational Linguistics, EACL 2017, Valencia, Spain, April 3-7, 2017, Volume\n2: Short Papers, pages 427–431. Association for Computational Linguistics.\nJared Kaplan, Sam McCandlish, Tom Henighan, Tom B. Brown, Benjamin Chess, Rewon Child,\nScott Gray, Alec Radford, Jeffrey Wu, and Dario Amodei. 2020. Scaling laws for neural language\nmodels.\nEunsu Kim, Juyoung Suk, Philhoon Oh, Haneul Yoo, James Thorne, and Alice Oh. 2024a. CLIcK:\nA benchmark dataset of cultural and linguistic intelligence in Korean. In Proceedings of the 2024\nJoint International Conference on Computational Linguistics, Language Resources and Evalua-\ntion (LREC-COLING 2024), pages 3335–3346, Torino, Italia. ELRA and ICCL.\n18\n\nGeewook Kim, Taeho Kil, and Jinbae Im. 2024b.\n“HyperCLOVA X Vision: Open Your Eyes,\nCLOVA X!”. https://tv.naver.com/v/67447111. Conference Talk, Dan24, Naver\nAI NOW, December 21, 2023. Accessed: 2025-06-23.\nJeonghoon Kim, Byeongchan Lee, Cheonbok Park, Yeontaek Oh, Beomjun Kim, Taehwan Yoo,\nSeongjin Shin, Dongyoon Han, Jinwoo Shin, and Kang Min Yoo. 2025. Peri-ln: Revisiting nor-\nmalization layer in the transformer architecture. In Forty-second International Conference on\nMachine Learning.\nSunjun Kweon, Byungjin Choi, Gyouk Chu, Junyeong Song, Daeun Hyeon, Sujin Gan, Jueon Kim,\nMinkyu Kim, Rae Woong Park, and Edward Choi. 2024. Kormedmcqa: Multi-choice question\nanswering benchmark for korean healthcare professional licensing examinations.\nBruce W. Lee, Hyunsoo Cho, and Kang Min Yoo. 2024. Instruction tuning with human curriculum.\nIn Findings of the Association for Computational Linguistics: NAACL 2024, Mexico City, Mexico,\nJune 16-21, 2024, pages 1281–1309. Association for Computational Linguistics.\nLG\nAI\nResearch.\n2024.\nKomt-bench.\nhttps://huggingface.co/datasets/\nLGAI-EXAONE/KoMT-Bench.\nLG AI Research. 2025. EXAONE Deep: Reasoning Enhanced Language Models. arXiv preprint\narXiv:2503.12524.\nJeffrey Li, Alex Fang, Georgios Smyrnis, Maor Ivgi, Matt Jordan, Samir Yitzhak Gadre, Hritik\nBansal, Etash Guha, Sedrick Scott Keh, Kushal Arora, Saurabh Garg, Rui Xin, Niklas Muen-\nnighoff, Reinhard Heckel, Jean Mercat, Mayee F. Chen, Suchin Gururangan, Mitchell Worts-\nman, Alon Albalak, Yonatan Bitton, Marianna Nezhurina, Amro Abbas, Cheng-Yu Hsieh, Dhruba\nGhosh, Josh Gardner, Maciej Kilian, Hanlin Zhang, Rulin Shao, Sarah M. Pratt, Sunny Sanyal,\nGabriel Ilharco, Giannis Daras, Kalyani Marathe, Aaron Gokaslan, Jieyu Zhang, Khyathi Raghavi\nChandu, Thao Nguyen, Igor Vasiljevic, Sham M. Kakade, Shuran Song, Sujay Sanghavi, Far-\ntash Faghri, Sewoong Oh, Luke Zettlemoyer, Kyle Lo, Alaaeldin El-Nouby, Hadi Pouransari,\nAlexander Toshev, Stephanie Wang, Dirk Groeneveld, Luca Soldaini, Pang Wei Koh, Jenia Jitsev,\nThomas Kollar, Alex Dimakis, Yair Carmon, Achal Dave, Ludwig Schmidt, and Vaishaal Shankar.\n2024. Datacomp-lm: In search of the next generation of training sets for language models. In Ad-\nvances in Neural Information Processing Systems 38: Annual Conference on Neural Information\nProcessing Systems 2024, NeurIPS 2024, Vancouver, BC, Canada, December 10 - 15, 2024.\nYuanzhi Li, Sébastien Bubeck, Ronen Eldan, Allie Del Giorno, Suriya Gunasekar, and Yin Tat Lee.\n2023. Textbooks are all you need ii: phi-1.5 technical report. arXiv preprint arXiv:2309.05463.\nHaotian Liu, Chunyuan Li, Yuheng Li, and Yong Jae Lee. 2024. Improved Baselines with Visual\nInstruction Tuning. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern\nRecognition (CVPR), pages 26296–26306.\nZichen Liu, Changyu Chen, Wenjun Li, Penghui Qi, Tianyu Pang, Chao Du, Wee Sun Lee, and\nMin Lin. 2025.\nUnderstanding r1-zero-like training: A critical perspective.\narXiv preprint\narXiv:2503.20783.\nAnton Lozhkov, Loubna Ben Allal, Leandro von Werra, and Thomas Wolf. 2024. Fineweb-edu: the\nfinest collection of educational content.\nTeam OLMo, Pete Walsh, Luca Soldaini, Dirk Groeneveld, Kyle Lo, Shane Arora, Akshita Bhagia,\nYuling Gu, Shengyi Huang, Matt Jordan, Nathan Lambert, Dustin Schwenk, Oyvind Tafjord,\nTaira Anderson, David Atkinson, Faeze Brahman, Christopher Clark, Pradeep Dasigi, Nouha\nDziri, Michal Guerquin, Hamish Ivison, Pang Wei Koh, Jiacheng Liu, Saumya Malik, William\nMerrill, Lester James V. Miranda, Jacob Morrison, Tyler Murray, Crystal Nam, Valentina Py-\natkin, Aman Rangapur, Michael Schmitz, Sam Skjonsberg, David Wadden, Christopher Wilhelm,\nMichael Wilson, Luke Zettlemoyer, Ali Farhadi, Noah A. Smith, and Hannaneh Hajishirzi. 2025.\n2 olmo 2 furious. CoRR, abs/2501.00656.\nOpenAI, :, Aaron Hurst, Adam Lerer, Adam P. Goucher, Adam Perelman, Aditya Ramesh, Aidan\nClark, AJ Ostrow, Akila Welihinda, Alan Hayes, Alec Radford, Aleksander M ˛adry, Alex Baker-\nWhitcomb, Alex Beutel, Alex Borzunov, Alex Carney, Alex Chow, Alex Kirillov, Alex Nichol,\n19\n\nAlex Paino, Alex Renzin, Alex Tachard Passos, Alexander Kirillov, Alexi Christakis, Alexis Con-\nneau, Ali Kamali, Allan Jabri, Allison Moyer, Allison Tam, Amadou Crookes, Amin Tootoochian,\nAmin Tootoonchian, Ananya Kumar, Andrea Vallone, Andrej Karpathy, Andrew Braunstein,\nAndrew Cann, Andrew Codispoti, Andrew Galu, Andrew Kondrich, Andrew Tulloch, Andrey\nMishchenko, Angela Baek, Angela Jiang, Antoine Pelisse, Antonia Woodford, Anuj Gosalia,\nArka Dhar, Ashley Pantuliano, Avi Nayak, Avital Oliver, Barret Zoph, Behrooz Ghorbani, Ben Le-\nimberger, Ben Rossen, Ben Sokolowsky, Ben Wang, Benjamin Zweig, Beth Hoover, Blake Samic,\nBob McGrew, Bobby Spero, Bogo Giertler, Bowen Cheng, Brad Lightcap, Brandon Walkin, Bren-\ndan Quinn, Brian Guarraci, Brian Hsu, Bright Kellogg, Brydon Eastman, Camillo Lugaresi, Car-\nroll Wainwright, Cary Bassin, Cary Hudson, Casey Chu, Chad Nelson, Chak Li, Chan Jun Shern,\nChanning Conger, Charlotte Barette, Chelsea Voss, Chen Ding, Cheng Lu, Chong Zhang, Chris\nBeaumont, Chris Hallacy, Chris Koch, Christian Gibson, Christina Kim, Christine Choi, Chris-\ntine McLeavey, Christopher Hesse, Claudia Fischer, Clemens Winter, Coley Czarnecki, Colin\nJarvis, Colin Wei, Constantin Koumouzelis, Dane Sherburn, Daniel Kappler, Daniel Levin, Daniel\nLevy, David Carr, David Farhi, David Mely, David Robinson, David Sasaki, Denny Jin, Dev\nValladares, Dimitris Tsipras, Doug Li, Duc Phong Nguyen, Duncan Findlay, Edede Oiwoh, Ed-\nmund Wong, Ehsan Asdar, Elizabeth Proehl, Elizabeth Yang, Eric Antonow, Eric Kramer, Eric\nPeterson, Eric Sigler, Eric Wallace, Eugene Brevdo, Evan Mays, Farzad Khorasani, Felipe Pet-\nroski Such, Filippo Raso, Francis Zhang, Fred von Lohmann, Freddie Sulit, Gabriel Goh, Gene\nOden, Geoff Salmon, Giulio Starace, Greg Brockman, Hadi Salman, Haiming Bao, Haitang Hu,\nHannah Wong, Haoyu Wang, Heather Schmidt, Heather Whitney, Heewoo Jun, Hendrik Kirch-\nner, Henrique Ponde de Oliveira Pinto, Hongyu Ren, Huiwen Chang, Hyung Won Chung, Ian\nKivlichan, Ian O’Connell, Ian O’Connell, Ian Osband, Ian Silber, Ian Sohl, Ibrahim Okuyucu,\nIkai Lan, Ilya Kostrikov, Ilya Sutskever, Ingmar Kanitscheider, Ishaan Gulrajani, Jacob Coxon,\nJacob Menick, Jakub Pachocki, James Aung, James Betker, James Crooks, James Lennon, Jamie\nKiros, Jan Leike, Jane Park, Jason Kwon, Jason Phang, Jason Teplitz, Jason Wei, Jason Wolfe,\nJay Chen, Jeff Harris, Jenia Varavva, Jessica Gan Lee, Jessica Shieh, Ji Lin, Jiahui Yu, Jiayi\nWeng, Jie Tang, Jieqi Yu, Joanne Jang, Joaquin Quinonero Candela, Joe Beutler, Joe Landers,\nJoel Parish, Johannes Heidecke, John Schulman, Jonathan Lachman, Jonathan McKay, Jonathan\nUesato, Jonathan Ward, Jong Wook Kim, Joost Huizinga, Jordan Sitkin, Jos Kraaijeveld, Josh\nGross, Josh Kaplan, Josh Snyder, Joshua Achiam, Joy Jiao, Joyce Lee, Juntang Zhuang, Justyn\nHarriman, Kai Fricke, Kai Hayashi, Karan Singhal, Katy Shi, Kavin Karthik, Kayla Wood, Kendra\nRimbach, Kenny Hsu, Kenny Nguyen, Keren Gu-Lemberg, Kevin Button, Kevin Liu, Kiel Howe,\nKrithika Muthukumar, Kyle Luther, Lama Ahmad, Larry Kai, Lauren Itow, Lauren Workman,\nLeher Pathak, Leo Chen, Li Jing, Lia Guy, Liam Fedus, Liang Zhou, Lien Mamitsuka, Lilian\nWeng, Lindsay McCallum, Lindsey Held, Long Ouyang, Louis Feuvrier, Lu Zhang, Lukas Kon-\ndraciuk, Lukasz Kaiser, Luke Hewitt, Luke Metz, Lyric Doshi, Mada Aflak, Maddie Simens,\nMadelaine Boyd, Madeleine Thompson, Marat Dukhan, Mark Chen, Mark Gray, Mark Hudnall,\nMarvin Zhang, Marwan Aljubeh, Mateusz Litwin, Matthew Zeng, Max Johnson, Maya Shetty,\nMayank Gupta, Meghan Shah, Mehmet Yatbaz, Meng Jia Yang, Mengchao Zhong, Mia Glaese,\nMianna Chen, Michael Janner, Michael Lampe, Michael Petrov, Michael Wu, Michele Wang,\nMichelle Fradin, Michelle Pokrass, Miguel Castro, Miguel Oom Temudo de Castro, Mikhail\nPavlov, Miles Brundage, Miles Wang, Minal Khan, Mira Murati, Mo Bavarian, Molly Lin, Murat\nYesildal, Nacho Soto, Natalia Gimelshein, Natalie Cone, Natalie Staudacher, Natalie Summers,\nNatan LaFontaine, Neil Chowdhury, Nick Ryder, Nick Stathas, Nick Turley, Nik Tezak, Niko Fe-\nlix, Nithanth Kudige, Nitish Keskar, Noah Deutsch, Noel Bundick, Nora Puckett, Ofir Nachum,\nOla Okelola, Oleg Boiko, Oleg Murk, Oliver Jaffe, Olivia Watkins, Olivier Godement, Owen\nCampbell-Moore, Patrick Chao, Paul McMillan, Pavel Belov, Peng Su, Peter Bak, Peter Bakkum,\nPeter Deng, Peter Dolan, Peter Hoeschele, Peter Welinder, Phil Tillet, Philip Pronin, Philippe\nTillet, Prafulla Dhariwal, Qiming Yuan, Rachel Dias, Rachel Lim, Rahul Arora, Rajan Troll, Ran-\ndall Lin, Rapha Gontijo Lopes, Raul Puri, Reah Miyara, Reimar Leike, Renaud Gaubert, Reza\nZamani, Ricky Wang, Rob Donnelly, Rob Honsby, Rocky Smith, Rohan Sahai, Rohit Ramchan-\ndani, Romain Huet, Rory Carmichael, Rowan Zellers, Roy Chen, Ruby Chen, Ruslan Nigmat-\nullin, Ryan Cheu, Saachi Jain, Sam Altman, Sam Schoenholz, Sam Toizer, Samuel Miserendino,\nSandhini Agarwal, Sara Culver, Scott Ethersmith, Scott Gray, Sean Grove, Sean Metzger, Shamez\nHermani, Shantanu Jain, Shengjia Zhao, Sherwin Wu, Shino Jomoto, Shirong Wu, Shuaiqi, Xia,\nSonia Phene, Spencer Papay, Srinivas Narayanan, Steve Coffey, Steve Lee, Stewart Hall, Suchir\nBalaji, Tal Broda, Tal Stramer, Tao Xu, Tarun Gogineni, Taya Christianson, Ted Sanders, Tejal\nPatwardhan, Thomas Cunninghman, Thomas Degry, Thomas Dimson, Thomas Raoux, Thomas\n20\n\nShadwell, Tianhao Zheng, Todd Underwood, Todor Markov, Toki Sherbakov, Tom Rubin, Tom\nStasi, Tomer Kaftan, Tristan Heywood, Troy Peterson, Tyce Walters, Tyna Eloundou, Valerie Qi,\nVeit Moeller, Vinnie Monaco, Vishal Kuo, Vlad Fomenko, Wayne Chang, Weiyi Zheng, Wenda\nZhou, Wesam Manassra, Will Sheu, Wojciech Zaremba, Yash Patil, Yilei Qian, Yongjik Kim,\nYoulong Cheng, Yu Zhang, Yuchen He, Yuchen Zhang, Yujia Jin, Yunxing Dai, and Yury Malkov.\n2024a. GPT-4o System Card.\nOpenAI, :, Aaron Jaech, Adam Kalai, Adam Lerer, Adam Richardson, Ahmed El-Kishky, Aiden\nLow, Alec Helyar, Aleksander Madry, Alex Beutel, Alex Carney, Alex Iftimie, Alex Karpenko,\nAlex Tachard Passos, Alexander Neitz, Alexander Prokofiev, Alexander Wei, Allison Tam, Ally\nBennett, Ananya Kumar, Andre Saraiva, Andrea Vallone, Andrew Duberstein, Andrew Kondrich,\nAndrey Mishchenko, Andy Applebaum, Angela Jiang, Ashvin Nair, Barret Zoph, Behrooz Ghor-\nbani, Ben Rossen, Benjamin Sokolowsky, Boaz Barak, Bob McGrew, Borys Minaiev, Botao Hao,\nBowen Baker, Brandon Houghton, Brandon McKinzie, Brydon Eastman, Camillo Lugaresi, Cary\nBassin, Cary Hudson, Chak Ming Li, Charles de Bourcy, Chelsea Voss, Chen Shen, Chong Zhang,\nChris Koch, Chris Orsinger, Christopher Hesse, Claudia Fischer, Clive Chan, Dan Roberts, Daniel\nKappler, Daniel Levy, Daniel Selsam, David Dohan, David Farhi, David Mely, David Robinson,\nDimitris Tsipras, Doug Li, Dragos Oprica, Eben Freeman, Eddie Zhang, Edmund Wong, Eliz-\nabeth Proehl, Enoch Cheung, Eric Mitchell, Eric Wallace, Erik Ritter, Evan Mays, Fan Wang,\nFelipe Petroski Such, Filippo Raso, Florencia Leoni, Foivos Tsimpourlas, Francis Song, Fred\nvon Lohmann, Freddie Sulit, Geoff Salmon, Giambattista Parascandolo, Gildas Chabot, Grace\nZhao, Greg Brockman, Guillaume Leclerc, Hadi Salman, Haiming Bao, Hao Sheng, Hart An-\ndrin, Hessam Bagherinezhad, Hongyu Ren, Hunter Lightman, Hyung Won Chung, Ian Kivlichan,\nIan O’Connell, Ian Osband, Ignasi Clavera Gilaberte, Ilge Akkaya, Ilya Kostrikov, Ilya Sutskever,\nIrina Kofman, Jakub Pachocki, James Lennon, Jason Wei, Jean Harb, Jerry Twore, Jiacheng Feng,\nJiahui Yu, Jiayi Weng, Jie Tang, Jieqi Yu, Joaquin Quiñonero Candela, Joe Palermo, Joel Parish,\nJohannes Heidecke, John Hallman, John Rizzo, Jonathan Gordon, Jonathan Uesato, Jonathan\nWard, Joost Huizinga, Julie Wang, Kai Chen, Kai Xiao, Karan Singhal, Karina Nguyen, Karl\nCobbe, Katy Shi, Kayla Wood, Kendra Rimbach, Keren Gu-Lemberg, Kevin Liu, Kevin Lu,\nKevin Stone, Kevin Yu, Lama Ahmad, Lauren Yang, Leo Liu, Leon Maksin, Leyton Ho, Liam\nFedus, Lilian Weng, Linden Li, Lindsay McCallum, Lindsey Held, Lorenz Kuhn, Lukas Kon-\ndraciuk, Lukasz Kaiser, Luke Metz, Madelaine Boyd, Maja Trebacz, Manas Joglekar, Mark Chen,\nMarko Tintor, Mason Meyer, Matt Jones, Matt Kaufer, Max Schwarzer, Meghan Shah, Mehmet\nYatbaz, Melody Y. Guan, Mengyuan Xu, Mengyuan Yan, Mia Glaese, Mianna Chen, Michael\nLampe, Michael Malek, Michele Wang, Michelle Fradin, Mike McClay, Mikhail Pavlov, Miles\nWang, Mingxuan Wang, Mira Murati, Mo Bavarian, Mostafa Rohaninejad, Nat McAleese, Neil\nChowdhury, Neil Chowdhury, Nick Ryder, Nikolas Tezak, Noam Brown, Ofir Nachum, Oleg\nBoiko, Oleg Murk, Olivia Watkins, Patrick Chao, Paul Ashbourne, Pavel Izmailov, Peter Zhokhov,\nRachel Dias, Rahul Arora, Randall Lin, Rapha Gontijo Lopes, Raz Gaon, Reah Miyara, Reimar\nLeike, Renny Hwang, Rhythm Garg, Robin Brown, Roshan James, Rui Shu, Ryan Cheu, Ryan\nGreene, Saachi Jain, Sam Altman, Sam Toizer, Sam Toyer, Samuel Miserendino, Sandhini Agar-\nwal, Santiago Hernandez, Sasha Baker, Scott McKinney, Scottie Yan, Shengjia Zhao, Shengli Hu,\nShibani Santurkar, Shraman Ray Chaudhuri, Shuyuan Zhang, Siyuan Fu, Spencer Papay, Steph\nLin, Suchir Balaji, Suvansh Sanjeev, Szymon Sidor, Tal Broda, Aidan Clark, Tao Wang, Tay-\nlor Gordon, Ted Sanders, Tejal Patwardhan, Thibault Sottiaux, Thomas Degry, Thomas Dimson,\nTianhao Zheng, Timur Garipov, Tom Stasi, Trapit Bansal, Trevor Creech, Troy Peterson, Tyna\nEloundou, Valerie Qi, Vineet Kosaraju, Vinnie Monaco, Vitchyr Pong, Vlad Fomenko, Weiyi\nZheng, Wenda Zhou, Wes McCabe, Wojciech Zaremba, Yann Dubois, Yinghai Lu, Yining Chen,\nYoung Cha, Yu Bai, Yuchen He, Yuchen Zhang, Yunyun Wang, Zheng Shao, and Zhuohan Li.\n2024b. Openai o1 system card.\nOpenAI, Josh Achiam, Steven Adler, Sandhini Agarwal, Lama Ahmad, Ilge Akkaya, Floren-\ncia Leoni Aleman, Diogo Almeida, Janko Altenschmidt, Sam Altman, Shyamal Anadkat, Red\nAvila, Igor Babuschkin, Suchir Balaji, Valerie Balcom, Paul Baltescu, Haiming Bao, Moham-\nmad Bavarian, Jeff Belgum, Irwan Bello, Jake Berdine, Gabriel Bernadett-Shapiro, Christopher\nBerner, Lenny Bogdonoff, Oleg Boiko, Madelaine Boyd, Anna-Luisa Brakman, Greg Brockman,\nTim Brooks, Miles Brundage, Kevin Button, Trevor Cai, Rosie Campbell, Andrew Cann, Brit-\ntany Carey, Chelsea Carlson, Rory Carmichael, Brooke Chan, Che Chang, Fotis Chantzis, Derek\nChen, Sully Chen, Ruby Chen, Jason Chen, Mark Chen, Ben Chess, Chester Cho, Casey Chu,\nHyung Won Chung, Dave Cummings, Jeremiah Currier, Yunxing Dai, Cory Decareaux, Thomas\n21\n\nDegry, Noah Deutsch, Damien Deville, Arka Dhar, David Dohan, Steve Dowling, Sheila Dunning,\nAdrien Ecoffet, Atty Eleti, Tyna Eloundou, David Farhi, Liam Fedus, Niko Felix, Simón Posada\nFishman, Juston Forte, Isabella Fulford, Leo Gao, Elie Georges, Christian Gibson, Vik Goel,\nTarun Gogineni, Gabriel Goh, Rapha Gontijo-Lopes, Jonathan Gordon, Morgan Grafstein, Scott\nGray, Ryan Greene, Joshua Gross, Shixiang Shane Gu, Yufei Guo, Chris Hallacy, Jesse Han, Jeff\nHarris, Yuchen He, Mike Heaton, Johannes Heidecke, Chris Hesse, Alan Hickey, Wade Hickey,\nPeter Hoeschele, Brandon Houghton, Kenny Hsu, Shengli Hu, Xin Hu, Joost Huizinga, Shantanu\nJain, Shawn Jain, Joanne Jang, Angela Jiang, Roger Jiang, Haozhun Jin, Denny Jin, Shino Jo-\nmoto, Billie Jonn, Heewoo Jun, Tomer Kaftan, Łukasz Kaiser, Ali Kamali, Ingmar Kanitscheider,\nNitish Shirish Keskar, Tabarak Khan, Logan Kilpatrick, Jong Wook Kim, Christina Kim, Yongjik\nKim, Jan Hendrik Kirchner, Jamie Kiros, Matt Knight, Daniel Kokotajlo, Łukasz Kondraciuk, An-\ndrew Kondrich, Aris Konstantinidis, Kyle Kosic, Gretchen Krueger, Vishal Kuo, Michael Lampe,\nIkai Lan, Teddy Lee, Jan Leike, Jade Leung, Daniel Levy, Chak Ming Li, Rachel Lim, Molly\nLin, Stephanie Lin, Mateusz Litwin, Theresa Lopez, Ryan Lowe, Patricia Lue, Anna Makanju,\nKim Malfacini, Sam Manning, Todor Markov, Yaniv Markovski, Bianca Martin, Katie Mayer,\nAndrew Mayne, Bob McGrew, Scott Mayer McKinney, Christine McLeavey, Paul McMillan,\nJake McNeil, David Medina, Aalok Mehta, Jacob Menick, Luke Metz, Andrey Mishchenko,\nPamela Mishkin, Vinnie Monaco, Evan Morikawa, Daniel Mossing, Tong Mu, Mira Murati,\nOleg Murk, David Mély, Ashvin Nair, Reiichiro Nakano, Rajeev Nayak, Arvind Neelakantan,\nRichard Ngo, Hyeonwoo Noh, Long Ouyang, Cullen O’Keefe, Jakub Pachocki, Alex Paino, Joe\nPalermo, Ashley Pantuliano, Giambattista Parascandolo, Joel Parish, Emy Parparita, Alex Passos,\nMikhail Pavlov, Andrew Peng, Adam Perelman, Filipe de Avila Belbute Peres, Michael Petrov,\nHenrique Ponde de Oliveira Pinto, Michael, Pokorny, Michelle Pokrass, Vitchyr H. Pong, Tolly\nPowell, Alethea Power, Boris Power, Elizabeth Proehl, Raul Puri, Alec Radford, Jack Rae, Aditya\nRamesh, Cameron Raymond, Francis Real, Kendra Rimbach, Carl Ross, Bob Rotsted, Henri\nRoussez, Nick Ryder, Mario Saltarelli, Ted Sanders, Shibani Santurkar, Girish Sastry, Heather\nSchmidt, David Schnurr, John Schulman, Daniel Selsam, Kyla Sheppard, Toki Sherbakov, Jessica\nShieh, Sarah Shoker, Pranav Shyam, Szymon Sidor, Eric Sigler, Maddie Simens, Jordan Sitkin,\nKatarina Slama, Ian Sohl, Benjamin Sokolowsky, Yang Song, Natalie Staudacher, Felipe Pet-\nroski Such, Natalie Summers, Ilya Sutskever, Jie Tang, Nikolas Tezak, Madeleine B. Thompson,\nPhil Tillet, Amin Tootoonchian, Elizabeth Tseng, Preston Tuggle, Nick Turley, Jerry Tworek,\nJuan Felipe Cerón Uribe, Andrea Vallone, Arun Vijayvergiya, Chelsea Voss, Carroll Wainwright,\nJustin Jay Wang, Alvin Wang, Ben Wang, Jonathan Ward, Jason Wei, CJ Weinmann, Akila Weli-\nhinda, Peter Welinder, Jiayi Weng, Lilian Weng, Matt Wiethoff, Dave Willner, Clemens Winter,\nSamuel Wolrich, Hannah Wong, Lauren Workman, Sherwin Wu, Jeff Wu, Michael Wu, Kai Xiao,\nTao Xu, Sarah Yoo, Kevin Yu, Qiming Yuan, Wojciech Zaremba, Rowan Zellers, Chong Zhang,\nMarvin Zhang, Shengjia Zhao, Tianhao Zheng, Juntang Zhuang, William Zhuk, and Barret Zoph.\n2024c. GPT-4 Technical Report.\nJeonghwan Park. 2024. Logickor.\nSanghee Park and Geewook Kim. 2025. Evaluating multimodal generative AI with Korean educa-\ntional standards. In Proceedings of the 2025 Conference of the Nations of the Americas Chapter of\nthe Association for Computational Linguistics: Human Language Technologies (Volume 2: Short\nPapers), pages 671–688, Albuquerque, New Mexico. Association for Computational Linguistics.\nGuilherme Penedo, Hynek Kydlícek, Loubna Ben Allal, Anton Lozhkov, Margaret Mitchell,\nColin A. Raffel, Leandro von Werra, and Thomas Wolf. 2024. The fineweb datasets: Decanting\nthe web for the finest text data at scale. In Advances in Neural Information Processing Systems 38:\nAnnual Conference on Neural Information Processing Systems 2024, NeurIPS 2024, Vancouver,\nBC, Canada, December 10 - 15, 2024.\nJirui Qi, Raquel Fernández, and Arianna Bisazza. 2023. Cross-lingual consistency of factual knowl-\nedge in multilingual language models.\nIn Proceedings of the 2023 Conference on Empirical\nMethods in Natural Language Processing, pages 10650–10666, Singapore. Association for Com-\nputational Linguistics.\nQwen, :, An Yang, Baosong Yang, Beichen Zhang, Binyuan Hui, Bo Zheng, Bowen Yu, Chengyuan\nLi, Dayiheng Liu, Fei Huang, Haoran Wei, Huan Lin, Jian Yang, Jianhong Tu, Jianwei Zhang,\nJianxin Yang, Jiaxi Yang, Jingren Zhou, Junyang Lin, Kai Dang, Keming Lu, Keqin Bao, Kexin\nYang, Le Yu, Mei Li, Mingfeng Xue, Pei Zhang, Qin Zhu, Rui Men, Runji Lin, Tianhao Li, Tianyi\n22\n\nTang, Tingyu Xia, Xingzhang Ren, Xuancheng Ren, Yang Fan, Yang Su, Yichang Zhang, Yu Wan,\nYuqiong Liu, Zeyu Cui, Zhenru Zhang, and Zihan Qiu. 2025. Qwen2.5 technical report.\nQwen Team. 2025. Qwq-32b: Embracing the power of reinforcement learning.\nMorgane Rivière, Shreya Pathak, Pier Giuseppe Sessa, Cassidy Hardin, Surya Bhupatiraju, Léonard\nHussenot, Thomas Mesnard, Bobak Shahriari, Alexandre Ramé, Johan Ferret, Peter Liu, Pouya\nTafti, Abe Friesen, Michelle Casbon, Sabela Ramos, Ravin Kumar, Charline Le Lan, Sammy\nJerome, Anton Tsitsulin, Nino Vieillard, Piotr Stanczyk, Sertan Girgin, Nikola Momchev, Matt\nHoffman, Shantanu Thakoor, Jean-Bastien Grill, Behnam Neyshabur, Olivier Bachem, Alanna\nWalton, Aliaksei Severyn, Alicia Parrish, Aliya Ahmad, Allen Hutchison, Alvin Abdagic,\nAmanda Carl, Amy Shen, Andy Brock, Andy Coenen, Anthony Laforge, Antonia Paterson, Ben\nBastian, Bilal Piot, Bo Wu, Brandon Royal, Charlie Chen, Chintu Kumar, Chris Perry, Chris\nWelty, Christopher A. Choquette-Choo, Danila Sinopalnikov, David Weinberger, Dimple Vijayku-\nmar, Dominika Rogozinska, Dustin Herbison, Elisa Bandy, Emma Wang, Eric Noland, Erica Mor-\neira, Evan Senter, Evgenii Eltyshev, Francesco Visin, Gabriel Rasskin, Gary Wei, Glenn Cameron,\nGus Martins, Hadi Hashemi, Hanna Klimczak-Plucinska, Harleen Batra, Harsh Dhand, Ivan Nar-\ndini, Jacinda Mein, Jack Zhou, James Svensson, Jeff Stanway, Jetha Chan, Jin Peng Zhou, Joana\nCarrasqueira, Joana Iljazi, Jocelyn Becker, Joe Fernandez, Joost van Amersfoort, Josh Gordon,\nJosh Lipschultz, Josh Newlan, Ju-yeong Ji, Kareem Mohamed, Kartikeya Badola, Kat Black,\nKatie Millican, Keelin McDonell, Kelvin Nguyen, Kiranbir Sodhia, Kish Greene, Lars Lowe Sjö-\nsund, Lauren Usui, Laurent Sifre, Lena Heuermann, Leticia Lago, and Lilly McNealus. 2024.\nGemma 2: Improving open language models at a practical size. CoRR, abs/2408.00118.\nZhihong Shao, Peiyi Wang, Qihao Zhu, Runxin Xu, Junxiao Song, Xiao Bi, Haowei Zhang,\nMingchuan Zhang, YK Li, Y Wu, et al. 2024. Deepseekmath: Pushing the limits of mathematical\nreasoning in open language models. arXiv preprint arXiv:2402.03300.\nHyopil Shin, Sangah Lee, Dongjun Jang, Wooseok Song, Jaeyoon Kim, Chaeyoung Oh, Hyemi Jo,\nYoungchae Ahn, Sihyun Oh, Hyohyeong Chang, Sunkyoung Kim, and Jinsik Lee. 2025. Kobalt:\nKorean benchmark for advanced linguistic tasks.\nShivalika Singh, Angelika Romanou, Clémentine Fourrier, David I. Adelani, Jian Gang Ngui, Daniel\nVila-Suero, Peerat Limkonchotiwat, Kelly Marchisio, Wei Qi Leong, Yosephine Susanto, Ray-\nmond Ng, Shayne Longpre, Wei-Yin Ko, Madeline Smith, Antoine Bosselut, Alice Oh, Andre\nF. T. Martins, Leshem Choshen, Daphne Ippolito, Enzo Ferrante, Marzieh Fadaee, Beyza Ermis,\nand Sara Hooker. 2024. Global mmlu: Understanding and addressing cultural and linguistic biases\nin multilingual evaluation.\nGuijin Son, Hanwool Lee, Sungdong Kim, Seungone Kim, Niklas Muennighoff, Taekyoon Choi,\nCheonbok Park, Kang Min Yoo, and Stella Biderman. 2025. KMMLU: Measuring massive mul-\ntitask language understanding in Korean. In Proceedings of the 2025 Conference of the Nations of\nthe Americas Chapter of the Association for Computational Linguistics: Human Language Tech-\nnologies (Volume 1: Long Papers), pages 4076–4104, Albuquerque, New Mexico. Association for\nComputational Linguistics.\nGuijin Son, Hanwool Lee, Suwan Kim, Huiseo Kim, Jae cheol Lee, Je Won Yeom, Jihyu Jung,\nJung woo Kim, and Songseong Kim. 2024.\nHAE-RAE bench: Evaluation of Korean knowl-\nedge in language models. In Proceedings of the 2024 Joint International Conference on Compu-\ntational Linguistics, Language Resources and Evaluation (LREC-COLING 2024), pages 7993–\n8007, Torino, Italia. ELRA and ICCL.\nDan Su, Kezhi Kong, Ying Lin, Joseph Jennings, Brandon Norick, Markus Kliegl, Mostofa Patwary,\nMohammad Shoeybi, and Bryan Catanzaro. 2024a. Nemotron-cc: Transforming common crawl\ninto a refined long-horizon pretraining dataset. CoRR, abs/2412.02595.\nJianlin Su, Murtadha H. M. Ahmed, Yu Lu, Shengfeng Pan, Wen Bo, and Yunfeng Liu. 2024b.\nRoformer: Enhanced transformer with rotary position embedding. Neurocomputing, 568:127063.\nYang Sui, Yu-Neng Chuang, Guanchu Wang, Jiamu Zhang, Tianyi Zhang, Jiayi Yuan, Hongyi Liu,\nAndrew Wen, Shaochen Zhong, Hanjie Chen, et al. 2025. Stop overthinking: A survey on efficient\nreasoning for large language models. arXiv preprint arXiv:2503.16419.\n23\n\nMingjie Sun, Xinlei Chen, J. Zico Kolter, and Zhuang Liu. 2024.\nMassive activations in large\nlanguage models. CoRR, abs/2402.17762.\nNLLB Team, Marta R. Costa-jussà, James Cross, Onur Çelebi, Maha Elbayad, Kenneth Heafield,\nKevin Heffernan, Elahe Kalbassi, Janice Lam, Daniel Licht, Jean Maillard, Anna Sun, Skyler\nWang, Guillaume Wenzek, Al Youngblood, Bapi Akula, Loic Barrault, Gabriel Mejia Gonzalez,\nPrangthip Hansanti, John Hoffman, Semarley Jarrett, Kaushik Ram Sadagopan, Dirk Rowe, Shan-\nnon Spruit, Chau Tran, Pierre Andrews, Necip Fazil Ayan, Shruti Bhosale, Sergey Edunov, Angela\nFan, Cynthia Gao, Vedanuj Goswami, Francisco Guzmán, Philipp Koehn, Alexandre Mourachko,\nChristophe Ropers, Safiyyah Saleem, Holger Schwenk, and Jeff Wang. 2022. No language left\nbehind: Scaling human-centered machine translation.\nMichael Tschannen, Alexey Gritsenko, Xiao Wang, Muhammad Ferjad Naeem, Ibrahim Alabdul-\nmohsin, Nikhil Parthasarathy, Talfan Evans, Lucas Beyer, Ye Xia, Basil Mustafa, Olivier Hénaff,\nJeremiah Harmsen, Andreas Steiner, and Xiaohua Zhai. 2025. SigLIP 2: Multilingual Vision-\nLanguage Encoders with Improved Semantic Understanding, Localization, and Dense Features.\narXiv preprint arXiv:2502.14786.\nYue Wang, Qiuzhi Liu, Jiahao Xu, Tian Liang, Xingyu Chen, Zhiwei He, Linfeng Song, Dian Yu,\nJuntao Li, Zhuosheng Zhang, et al. 2025. Thoughts are all over the place: On the underthinking\nof o1-like llms. arXiv preprint arXiv:2501.18585.\nMaurice Weber, Daniel Y. Fu, Quentin Anthony, Yonatan Oren, Shane Adams, Anton Alexandrov,\nXiaozhong Lyu, Huu Nguyen, Xiaozhe Yao, Virginia Adams, Ben Athiwaratkun, Rahul Cha-\nlamala, Kezhen Chen, Max Ryabinin, Tri Dao, Percy Liang, Christopher Ré, Irina Rish, and\nCe Zhang. 2024a. Redpajama: an open dataset for training large language models. In Advances in\nNeural Information Processing Systems 38: Annual Conference on Neural Information Process-\ning Systems 2024, NeurIPS 2024, Vancouver, BC, Canada, December 10 - 15, 2024.\nMaurice Weber, Daniel Y. Fu, Quentin Anthony, Yonatan Oren, Shane Adams, Anton Alexandrov,\nXiaozhong Lyu, Huu Nguyen, Xiaozhe Yao, Virginia Adams, Ben Athiwaratkun, Rahul Cha-\nlamala, Kezhen Chen, Max Ryabinin, Tri Dao, Percy Liang, Christopher Ré, Irina Rish, and\nCe Zhang. 2024b. Redpajama: an open dataset for training large language models. NeurIPS\nDatasets and Benchmarks Track.\nXiaolin Xing, Zhiwei He, Haoyu Xu, Xing Wang, Rui Wang, and Yu Hong. 2024.\nEvaluat-\ning knowledge-based cross-lingual inconsistency in large language models.\narXiv preprint\narXiv:2407.01358.\nRuibin Xiong, Yunchang Yang, Di He, Kai Zheng, Shuxin Zheng, Chen Xing, Huishuai Zhang,\nYanyan Lan, Liwei Wang, and Tie-Yan Liu. 2020. On layer normalization in the transformer\narchitecture. In Proceedings of the 37th International Conference on Machine Learning, ICML\n2020, 13-18 July 2020, Virtual Event, volume 119 of Proceedings of Machine Learning Research,\npages 10524–10533. PMLR.\nMingyu Xu, Xin Men, Bingning Wang, Qingyu Zhang, Hongyu Lin, Xianpei Han, and Weipeng\nChen. 2024. Base of rope bounds context length. In Advances in Neural Information Processing\nSystems 38: Annual Conference on Neural Information Processing Systems 2024, NeurIPS 2024,\nVancouver, BC, Canada, December 10 - 15, 2024.\nAn Yang, Anfeng Li, Baosong Yang, Beichen Zhang, Binyuan Hui, Bo Zheng, Bowen Yu, Chang\nGao, Chengen Huang, Chenxu Lv, Chujie Zheng, Dayiheng Liu, Fan Zhou, Fei Huang, Feng Hu,\nHao Ge, Haoran Wei, Huan Lin, Jialong Tang, Jian Yang, Jianhong Tu, Jianwei Zhang, Jianxin\nYang, Jiaxi Yang, Jing Zhou, Jingren Zhou, Junyang Lin, Kai Dang, Keqin Bao, Kexin Yang,\nLe Yu, Lianghao Deng, Mei Li, Mingfeng Xue, Mingze Li, Pei Zhang, Peng Wang, Qin Zhu, Rui\nMen, Ruize Gao, Shixuan Liu, Shuang Luo, Tianhao Li, Tianyi Tang, Wenbiao Yin, Xingzhang\nRen, Xinyu Wang, Xinyu Zhang, Xuancheng Ren, Yang Fan, Yang Su, Yichang Zhang, Yinger\nZhang, Yu Wan, Yuqiong Liu, Zekun Wang, Zeyu Cui, Zhenru Zhang, Zhipeng Zhou, and Zihan\nQiu. 2025. Qwen3 technical report.\nGreg Yang and Edward J. Hu. 2021. Tensor programs IV: feature learning in infinite-width neural\nnetworks. In Proceedings of the 38th International Conference on Machine Learning, ICML\n24\n\n2021, 18-24 July 2021, Virtual Event, volume 139 of Proceedings of Machine Learning Research,\npages 11727–11737. PMLR.\nGreg Yang, Dingli Yu, Chen Zhu, and Soufiane Hayou. 2024. Tensor programs VI: feature learning\nin infinite depth neural networks. In The Twelfth International Conference on Learning Repre-\nsentations, ICLR 2024, Vienna, Austria, May 7-11, 2024. OpenReview.net.\nHaneul Yoo, Cheonbok Park, Sangdoo Yun, Alice Oh, and Hwaran Lee. 2024a. Code-switching\ncurriculum learning for multilingual transfer in llms. arXiv preprint arXiv:2411.02460.\nKang Min Yoo, Jaegeun Han, Sookyo In, Heewon Jeon, Jisu Jeong, Jaewook Kang, Hyunwook Kim,\nKyung-Min Kim, Munhyong Kim, Sungju Kim, Donghyun Kwak, Hanock Kwak, Se Jung Kwon,\nBado Lee, Dongsoo Lee, Gichang Lee, Jooho Lee, Baeseong Park, Seongjin Shin, Joonsang Yu,\nSeolki Baek, Sumin Byeon, Eungsup Cho, Dooseok Choe, Jeesung Han, Youngkyun Jin, Hyein\nJun, Jaeseung Jung, Chanwoong Kim, Jinhong Kim, Jinuk Kim, Dokyeong Lee, Dongwook Park,\nJeong Min Sohn, Sujung Han, Jiae Heo, Sungju Hong, Mina Jeon, Hyunhoon Jung, Jungeun\nJung, Wangkyo Jung, Chungjoon Kim, Hyeri Kim, Jonghyun Kim, Min Young Kim, Soeun Lee,\nJoonhee Park, Jieun Shin, Sojin Yang, Jungsoon Yoon, Hwaran Lee, Sanghwan Bae, Jeehwan\nCha, Karl Gylleus, Donghoon Ham, Mihak Hong, Youngki Hong, Yunki Hong, Dahyun Jang,\nHyojun Jeon, Yujin Jeon, Yeji Jeong, Myunggeun Ji, Yeguk Jin, Chansong Jo, Shinyoung Joo,\nSeunghwan Jung, Adrian Jungmyung Kim, Byoung Hoon Kim, Hyomin Kim, Jungwhan Kim,\nMinkyoung Kim, Minseung Kim, Sungdong Kim, Yonghee Kim, Youngjun Kim, Youngkwan\nKim, Donghyeon Ko, Dughyun Lee, Ha Young Lee, Jaehong Lee, Jieun Lee, Jonghyun Lee,\nJongjin Lee, Min Young Lee, Yehbin Lee, Taehong Min, Yuri Min, Kiyoon Moon, Hyangnam\nOh, Jaesun Park, Kyuyon Park, Younghun Park, Hanbae Seo, Seunghyun Seo, Mihyun Sim, Gyu-\nbin Son, Matt Yeo, Kyung Hoon Yeom, Wonjoon Yoo, Myungin You, Doheon Ahn, Homin Ahn,\nJoohee Ahn, Seongmin Ahn, Chanwoo An, Hyeryun An, Junho An, Sang-Min An, Boram Byun,\nEunbin Byun, Jongho Cha, Minji Chang, Seunggyu Chang, Haesong Cho, Youngdo Cho, Dal-\nnim Choi, Daseul Choi, Hyoseok Choi, Minseong Choi, Sangho Choi, Seongjae Choi, Wooy-\nong Choi, Sewhan Chun, Dong Young Go, Chiheon Ham, Danbi Han, Jaemin Han, Moonyoung\nHong, Sung Bum Hong, Dong-Hyun Hwang, Seongchan Hwang, Jinbae Im, Hyuk Jin Jang, Jae-\nhyung Jang, Jaeni Jang, Sihyeon Jang, Sungwon Jang, Joonha Jeon, Daun Jeong, Joonhyun Jeong,\nKyeongseok Jeong, Mini Jeong, Sol Jin, Hanbyeol Jo, Hanju Jo, Minjung Jo, Chaeyoon Jung,\nHyungsik Jung, Jaeuk Jung, Ju Hwan Jung, Kwangsun Jung, Seungjae Jung, Soonwon Ka, Dong-\nhan Kang, Soyoung Kang, Taeho Kil, Areum Kim, Beomyoung Kim, Byeongwook Kim, Dae-\nhee Kim, Dong-Gyun Kim, Donggook Kim, Donghyun Kim, Euna Kim, Eunchul Kim, Geewook\nKim, Gyu Ri Kim, Hanbyul Kim, Heesu Kim, Isaac Kim, Jeonghoon Kim, Jihye Kim, Joonghoon\nKim, Minjae Kim, Minsub Kim, Pil Hwan Kim, Sammy Kim, Seokhun Kim, Seonghyeon Kim,\nSoojin Kim, Soong Kim, Soyoon Kim, Sunyoung Kim, Taeho Kim, Wonho Kim, Yoonsik Kim,\nYou Jin Kim, Yuri Kim, Beomseok Kwon, Ohsung Kwon, Yoo-Hwan Kwon, Anna Lee, Byung-\nwook Lee, Changho Lee, Daun Lee, Dongjae Lee, Ha-Ram Lee, Hodong Lee, Hwiyeong Lee,\nHyunmi Lee, Injae Lee, Jaeung Lee, Jeongsang Lee, Jisoo Lee, Jongsoo Lee, Joongjae Lee,\nJuhan Lee, Jung Hyun Lee, Junghoon Lee, Junwoo Lee, Se Yun Lee, Sujin Lee, Sungjae Lee,\nSungwoo Lee, Wonjae Lee, Zoo Hyun Lee, Jong Kun Lim, Kun Lim, Taemin Lim, Nuri Na,\nJeongyeon Nam, Kyeong-Min Nam, Yeonseog Noh, Biro Oh, Jung-Sik Oh, Solgil Oh, Yeontaek\nOh, Boyoun Park, Cheonbok Park, Dongju Park, Hyeonjin Park, Hyun Tae Park, Hyunjung Park,\nJihye Park, Jooseok Park, Junghwan Park, Jungsoo Park, Miru Park, Sang Hee Park, Seunghyun\nPark, Soyoung Park, Taerim Park, Wonkyeong Park, Hyunjoon Ryu, Jeonghun Ryu, Nahyeon\nRyu, Soonshin Seo, Suk Min Seo, Yoonjeong Shim, Kyuyong Shin, Wonkwang Shin, Hyun Sim,\nWoongseob Sim, Hyejin Soh, Bokyong Son, Hyunjun Son, Seulah Son, Chi-Yun Song, Chiyoung\nSong, Ka Yeon Song, Minchul Song, Seungmin Song, Jisung Wang, Yonggoo Yeo, Myeong Yeon\nYi, Moon Bin Yim, Taehwan Yoo, Youngjoon Yoo, Sungmin Yoon, Young Jin Yoon, Hangyeol Yu,\nUi Seon Yu, Xingdong Zuo, Jeongin Bae, Joungeun Bae, Hyunsoo Cho, Seonghyun Cho, Yongjin\nCho, Taekyoon Choi, Yera Choi, Jiwan Chung, Zhenghui Han, Byeongho Heo, Euisuk Hong, Tae-\nbaek Hwang, Seonyeol Im, Sumin Jegal, Sumin Jeon, Yelim Jeong, Yonghyun Jeong, Can Jiang,\nJuyong Jiang, Jiho Jin, Ara Jo, Younghyun Jo, Hoyoun Jung, Juyoung Jung, Seunghyeong Kang,\nDae Hee Kim, Ginam Kim, Hangyeol Kim, Heeseung Kim, Hyojin Kim, Hyojun Kim, Hyun-Ah\nKim, Jeehye Kim, Jin-Hwa Kim, Jiseon Kim, Jonghak Kim, Jung Yoon Kim, Rak Yeong Kim,\nSeongjin Kim, Seoyoon Kim, Sewon Kim, Sooyoung Kim, Sukyoung Kim, Taeyong Kim, Naeun\nKo, Bonseung Koo, Heeyoung Kwak, Haena Kwon, Youngjin Kwon, Boram Lee, Bruce W. Lee,\n25\n\nDagyeong Lee, Erin Lee, Euijin Lee, Ha Gyeong Lee, Hyojin Lee, Hyunjeong Lee, Jeeyoon Lee,\nJeonghyun Lee, Jongheok Lee, Joonhyung Lee, Junhyuk Lee, Mingu Lee, Nayeon Lee, Sangkyu\nLee, Se Young Lee, Seulgi Lee, Seung Jin Lee, Suhyeon Lee, Yeonjae Lee, Yesol Lee, Young-\nbeom Lee, Yujin Lee, Shaodong Li, Tianyu Liu, Seong-Eun Moon, Taehong Moon, Max-Lasse\nNihlenramstroem, Wonseok Oh, Yuri Oh, Hongbeen Park, Hyekyung Park, Jaeho Park, Nohil\nPark, Sangjin Park, Jiwon Ryu, Miru Ryu, Simo Ryu, Ahreum Seo, Hee Seo, Kangdeok Seo,\nJamin Shin, Seungyoun Shin, Heetae Sin, Jiangping Wang, Lei Wang, Ning Xiang, Longxiang\nXiao, Jing Xu, Seonyeong Yi, Haanju Yoo, Haneul Yoo, Hwanhee Yoo, Liang Yu, Youngjae Yu,\nWeijie Yuan, Bo Zeng, Qian Zhou, Kyunghyun Cho, Jung-Woo Ha, Joonsuk Park, Jihyun Hwang,\nHyoung Jo Kwon, Soonyong Kwon, Jungyeon Lee, Seungho Lee, Seonghyeon Lim, Hyunkyung\nNoh, Seungho Choi, Sang-Woo Lee, Jung Hwa Lim, and Nako Sung. 2024b. Hyperclova x tech-\nnical report.\nQiying Yu, Zheng Zhang, Ruofei Zhu, Yufeng Yuan, Xiaochen Zuo, Yu Yue, Weinan Dai, Tiantian\nFan, Gaohong Liu, Lingjun Liu, et al. 2025. Dapo: An open-source llm reinforcement learning\nsystem at scale. arXiv preprint arXiv:2503.14476.\nZheng Yuan, Hongyi Yuan, Chengpeng Li, Guanting Dong, Chuanqi Tan, and Chang Zhou. 2023.\nScaling relationship on learning mathematical reasoning with large language models.\nCoRR,\nabs/2308.01825.\nYonghao Zhuang, Lanxiang Hu, Longfei Yun, Souvik Kundu, Zhengzhong Liu, Eric P. Xing, and\nHao Zhang. 2025. Scaling long context training data by long-distance referrals. In The Thirteenth\nInternational Conference on Learning Representations, ICLR 2025, Singapore, April 24-28, 2025.\nOpenReview.net.\n26\n\nA\nContributors\nWithin each role, names are listed in alphabetical order by last name, followed by the first name.\nCore Contributors\nSanghwan Bae\nMinseong Choi\nHyunsoo Ha\nChiheon Ham\nDonghoon Ham\nJaemin Han\nJiwoo Hong†\nYoungki Hong\nJinbae Im\nSookyo In\nYeguk Jin\nChansong Jo\nHwiyeol Jo\nShinyoung Joo\nJingu Kang\nDonghyeon Ko\nTaeho Kil\nByeongwook Kim\nDaehee Kim\nDonghyun Kim\nGeewook Kim\nHanbyul Kim\nHyunwoo Kim\nJeonghoon Kim\nJungwhan Kim\nMinkyoung Kim\nMunhyong Kim\nSeonghyun Kim\nSungdong Kim†\nSungju Kim\nYoonsik Kim\nYou Jin Kim\nDonghyun Kwak\nBeomseok Kwon\nBado Lee\nByungwook Lee\nGichang Lee\nHodong Lee\nInjae Lee\nJaehong Lee\nJeong Hyun Lee†\nJieun Lee\nJoosung Lee\nMin Young Lee\nNoah Lee\nSang-Woo Lee†\nYehbin Lee\nYujeong Lee\nTaehong Min\nKiyoon Moon\nJeongYeon Nam\nYeontaek Oh\nCheonbok Park\nJoonsuk Park\nKyuyon Park\nSanghee Park\nAhreum Seo\nSeunghyun Seo\nSuk Min Seo\nSeongjin Shin\nKa Yeon Song\nNako Sung\nMoonbin Yim\nKang Min Yoo\nTaehwan Yoo\nMyungIn You\nHangyeol Yu\nContributors\nSang Min An\nJeongin Bae\nChongho Cha\nEungsup Cho\nHaesong Cho\nSaerim Cho\nHyungwook Choi\nJaepil Choi †\nSanghyuk Choi\nJaehyeok Doo †\nSungbum Hong\nSeongchan Hwang\nDonghoon Jang\nGenie Jang\nJunseo Jang\nHeewon Jeon\nMina Jeon\nKyeongseok Jeong\nYelim Jeong\nMyunggeun Ji\nYoungkyun Jin\nAra Jo\nHyunhoon Jung\nKwangsun Jung\nSeunghwan Jung\nDain Kim †\nDong Gyun Kim\nEunchul Kim\nGinam Kim\nHyomin Kim\nHyunwook Kim\nJihye Kim\nJiseob Kim\nJonghak Kim\nJoonghoon Kim †\nMinseung Kim\nMinyoung Kim\nSingon Kim\n27\n\nSoyoon Kim\nTaeyong Kim\nYonghee Kim\nYoungjun Kim\nOhsung Kwon\nYoo Hwan Kwon\nYoungjin Kwon\nDagyeong Lee\nDughyun Lee\nGayoung Lee\nHa Ram Lee\nHagyeong Lee\nJeonghyun Lee\nJonghyun Lee\nJongjin Lee\nJoonhyung Lee\nJunghoon Lee\nSeulgi Lee\nSoeun Lee\nSujin Lee\nSungwoo Lee\nYesol Lee\nYoungbeom Lee\nTaemin Lim\nKyeong Min Nam\nBiro Oh\nSolgil Oh\nGunho Park\nWonkyeong Park\nJieun Shin\nWonkwang Shin\nChiyun Song\nHae Jin Song\nMinchul Song\nJisung Wang\nSukwon Yeo\nHwanhee Yoo\nWonjoon You\nUiseon Yu\n† Work done while at NAVER Cloud.\n28\n\nB\nPerformance on Math&Coding Benchmarks\nCategory\nBenchmarks\nHCX\nTHINK\nQwen3\nQwen3\nQwQ\nEXAONE\nDeep\n( - )\n(32B)\n(14B)\n(32B)\n(32B)\nMath\nGSM8K\n95.5\n95.9\n95.9\n96.2\n95.5\nGSM8K-ko\n92.1\n93.4\n92.6\n92.7\n91.8\nMATH500\n95.2\n97.2\n96.8\n98.0\n96.0\nMATH100-ko\n90.5\n92.9\n92.9\n92.9\n93.9\nCoding\nHumanEval\n95.7\n96.9\n95.7\n90.8\n95.1\nMBPP\n89.9\n78.6\n90.8\n90.1\n87.5\nTable 6: Performance comparison of language models on math and coding benchmarks. Differences\namong the baseline reasoning models are marginal, as these benchmarks are approaching saturation.\nC\nKoBigBench Qualitative Examples\nHyperCLOVA X THINK (Translated Model Input&Output – English)\n<|im_start|>tool_list\n<|im_end|>\n<|im_start|>system\nThe following are multiple choice questions with explanations and/or answers about\nsocial-ecofin201.<|im_end|>\n<|im_start|>user\nQuestion: What is the most reasonable measure the government can take to prevent the\n\"transaction cliff\" shock in the real estate market?\nRead the question again: What is the most reasonable measure the government can take to\nprevent the \"transaction cliff\" shock in the real estate market?\nOptions:\n(A). Increase real estate transaction tax rate\n(B). Increase property tax rate\n(C). Strengthen safety inspections for reconstruction\n(D). Lift designation of regulated areas<|im_end|>\n<|im_start|>assistant/think\nThis question asks for the most reasonable measure the government can take to address\nthe \"transaction cliff\" problem in the real estate market today. We have four options from\nA to D. I need to examine each option to see which one would help prevent a transaction cliff.\nFirst, I need to understand exactly what a \"transaction cliff\" is. It seems to refer to a\nphenomenon where real estate transaction volume rapidly decreases. This term is typically\nused when the real estate market is in a slump or when regulations are tightened, leading to\na contraction in transactions. Therefore, to prevent this, the government would likely need\nto implement policies that stimulate transactions.\nLet’s analyze each option one by one.\n(A) Increase real estate transaction tax rate: Transaction taxes refer to taxes like acquisition\ntax or registration tax paid when buying or selling real estate. Increasing the transaction\ntax rate would increase transaction costs, which would likely further reduce transactions.\nThis would worsen the transaction cliff rather than prevent it. So, A does not seem like an\n29\n\nappropriate measure.\n(B) Increase property tax rate: Property tax refers to taxes paid while owning a home,\nsuch as property tax or comprehensive real estate tax. Increasing property taxes might lead\nsome homeowners to put their properties on the market due to the tax burden. However,\nconversely, existing owners might also choose to hold onto their properties. It’s also\nsomewhat ambiguous whether an increase in property taxes would directly stimulate\ntransactions. There’s also an argument that increasing property taxes could lead to a de-\ncrease in transactions itself. So, B also seems inappropriate for preventing a transaction cliff.\n(C) Strengthen safety inspections for reconstruction: Strengthening safety inspections for\nreconstruction would reduce the supply through redevelopment. However, whether this\nis relevant depends on whether the transaction cliff is due to a supply shortage or other\nfactors. If the lack of transactions is due to insufficient new supply through reconstruction,\nthen relaxation, not strengthening, might be needed. But the question asks for a measure\nto prevent a transaction cliff, and strengthening safety inspections for reconstruction could\nfurther reduce supply, potentially negatively impacting the market. Therefore, C also seems\ninappropriate.\n(D) Lift designation of regulated areas: Regulated areas refer to regions where subscription,\nloan, and resale restrictions apply. Lifting the designation of regulated areas would ease\ntransaction regulations in those areas, which could stimulate transactions, right? For\nexample, if loan regulations are eased or resale restrictions are removed, it would become\neasier to conduct transactions, potentially increasing transaction volume. Therefore, lifting\nthe designation of regulated areas could help prevent a transaction cliff.\nHowever, even with this thought, I need to consider more specifically how lifting the\ndesignation of regulated areas actually impacts transaction activation. If regulated areas\nare lifted, they would be removed from speculative overheated zones or adjustment target\nareas, leading to relaxed subscription conditions, increased loan limits, and even allowing\nmultiple homeowners to get loans, thereby easing transaction-related regulations. This\ncould increase buying demand and lead to more active transactions. On the other hand,\nthere might be concerns that lifting regulated areas could induce speculation and cause\nmarket overheating. However, the question specifically asks for a measure to prevent a\ntransaction cliff, i.e., a rapid decrease in transactions, so regulatory easing seems appropriate.\nComparing each option, A is inappropriate because it increases transaction costs and\nreduces transactions. B might lead to property sales due to increased holding tax but is\nless directly related to transaction activation. C could lead to reduced supply and further\ndecrease transactions. D could promote transactions by easing regulations, so D seems to be\nthe most reasonable measure.\nHowever, one lingering question is whether lifting the designation of regulated areas can\nbe a direct solution to the transaction cliff. For example, if the current real estate market\nis in a slump and transactions have decreased, easing regulations could stimulate demand\nand increase transactions. Conversely, if the market was overheated and regulations were\ntightened, easing them could lead to re-overheating, but the question is about preventing a\ntransaction cliff, a decrease in transactions, so regulatory easing seems correct.\nAlso, another possibility is that if the transaction cliff is due to external factors like rising\ninterest rates or an economic downturn, the government’s regulatory easing alone might\nhave limited effect. However, the question asks to choose from government measures, so\nit’s about selecting the most appropriate policy tool.\nIn summary, to increase transactions, lowering transaction costs or easing regulations\nwould be helpful. Increasing the transaction tax rate (A) increases costs and is therefore\nopposite. Increasing the property tax rate (B) increases the burden of ownership, which\n30\n\nmight incentivize selling, but it’s less directly linked to transaction activation. Strengthening\nsafety inspections for reconstruction (C) could lead to a decrease in supply and thus\nnegatively impact transaction activation. Lifting the designation of regulated areas (D) eases\nregulations and can promote transactions, so it seems to be the most appropriate measure.\nHowever, I need to re-evaluate whether the assumption that lifting the designation of\nregulated areas helps activate transactions is correct. Regulated areas are designated as\nspeculative overheated or adjustment target areas. Lifting these designations can ease loan\nregulations, resale restrictions, etc., making transactions easier. For example, if the resale\nrestriction period is shortened, short-term trading becomes possible, which can increase\ntransaction volume. If loan regulations are eased, real demanders can more easily access\nhousing purchases, which can stimulate transactions. Therefore, lifting the designation of\nregulated areas can help alleviate the transaction cliff.\nOn the other hand, if the current transaction cliff is due to reduced liquidity from interest\nrate hikes or decreased demand due to an economic downturn, even regulatory easing\nmight not be enough to increase transactions. However, the question asks for a government\nmeasure to prevent the transaction cliff in the real estate market, so among the given options,\nD seems the most appropriate.\nLet’s double-check the other options.\n(A) Increase real estate transaction tax rate: This would definitely increase transaction costs\nand reduce transactions, thus deepening the transaction cliff rather than preventing it. So, A\nis incorrect.\n(B) Increase property tax rate: While some argue that increasing property taxes might\nmake homeowners feel the tax burden and put properties on the market, conversely, they\nmight also hold onto their properties. Also, increasing property taxes primarily affects\nmulti-homeowners or owners of high-value properties. While their putting properties on\nthe market could increase supply and activate transactions, this can vary depending on\nthe situation. However, generally, an increase in property taxes is more focused on price\nstabilization through increased supply rather than transaction activation. Therefore, B might\nnot be an appropriate direct measure to solve the transaction cliff itself.\n(C) Strengthen safety inspections for reconstruction: Expanding supply through reconstruc-\ntion can contribute to market stability by increasing housing supply. However, strengthening\nsafety inspections makes reconstruction more difficult, thus reducing supply. Reduced\nsupply can lead to upward price pressure but is a separate issue from transaction activation.\nIf the lack of transactions is due to a supply shortage, then supply needs to be increased,\nbut if the current transaction cliff is due to other factors, this measure would be ineffective.\nTherefore, C seems inappropriate for activating transactions.\n(D) Lift designation of regulated areas: As I considered earlier, easing regulations can\nmake transactions easier, potentially increasing transaction volume, so D seems the most\nappropriate.\nFurthermore, recalling the actual impact of lifting regulated area designations on trans-\naction activation in Korean real estate policy, for example, the government lifted some\nregulated areas in late 2022 or early 2023. At that time, the real estate market was\ncooling, and the purpose was to stimulate transactions by easing regulations. Therefore,\nin actual cases, lifting regulated area designations has been used as a policy to alleviate\ntransaction cliffs, which increases the likelihood that D is the correct answer in this problem.\nIn conclusion, analyzing each option, (D) Lifting designation of regulated areas seems to be\nthe most reasonable measure. <|im_end|> <|im_start|>assistant\nAnswer: D<|im_end|>\n31\n\nHyperCLOVA X THINK (Model Input&Output – Korean)\n<|im_start|>tool_list\n<|im_end|> <|im_start|>system\nThe following are multiple choice questions with explanations and/or answers about\nsocial-ecofin201.<|im_end|>\n<|im_start|>user\nQuestion: 부동산시장의’거래절벽’ 충격을막기위해정부가취할수있는조치로가장\n합리적인것은?\nRead the question again: 부동산시장의’거래절벽’ 충격을막기위해정부가취할수\n있는조치로가장합리적인것은?\nOptions:\n(A). 거래세율인상\n(B). 보유세율인상\n(C). 재건축안전진단강화\n(D). 규제지역해제<|im_end|>\n<|im_start|>assistant/think\n오늘부동산시장의’거래절벽’ 문제를해결하기위해정부가취할수있는조치중에서\n가장합리적인것을고르는문제네요. 문제는선택지A부터D까지네가지옵션이\n주어졌어요. 각옵션을하나씩살펴보면서어떤것이거래절벽을막는데도움이될지\n생각해봐야겠어요.\n먼저, ’거래절벽’이무엇인지부터정확히이해해야할것같아요. 거래절벽이란부동산\n거래량이급격히감소하는현상을말하는것같은데, 보통부동산시장이침체기에\n빠지거나규제가강화되어거래가위축될때사용되는용어인것같아요. 그렇다면\n정부가이를막기위해서는거래를활성화시킬수있는정책을펴야하지않을까싶네요.\n각선택지를하나씩분석해볼게요.\n(A) 거래세율인상: 거래세란부동산을사고팔때내는취득세나등록세같은것을\n말하는것같아요. 거래세율을인상하면거래비용이늘어나게되므로, 거래를더줄이는\n효과가있을것같은데. 그러면거래절벽을막기보다는오히려악화시키지않을까?\n따라서A는적절한조치가아닐것같아요.\n(B) 보유세율인상: 보유세는주택을소유하고있는동안내는세금, 예를들어재산세나\n종합부동산세같은걸말하죠. 보유세를올리면집을가진사람들이세금부담을느껴\n매물을내놓을수도있겠지만, 반대로생각하면기존소유자들이매물을거둬들이게될\n수도있을것같아요. 하지만보유세인상이거래활성화에직접적인영향을미칠지는좀\n애매한것같아요. 특히보유세를올리면거래자체가줄어들수있다는의견도있을것\n같고. 그래서B도거래절벽을막는데는부적절할것같네요.\n(C) 재건축안전진단강화: 재건축안전진단을강화하면재건축을통한공급이줄어들게\n되겠죠. 그런데거래절벽이공급부족으로인한것인지, 아니면다른요인때문인지에\n따라다를것같아요. 만약재건축을통한신규공급이부족해서거래가안된다면강화가\n아니라완화가필요할수도있지않을까요? 하지만문제에서는거래절벽을막기위한\n조치라고했으니, 재건축안전진단강화는공급을더줄여서오히려시장에부정적인\n영향을줄수도있을것같아요. 따라서C도적절하지않을것같네요.\n(D) 규제지역해제: 규제지역이란청약, 대출, 전매제한등이적용되는지역을말하는것\n같아요. 규제지역을해제하면해당지역에서의거래규제가완화되어거래가활성화될\n수있지않을까? 예를들어, 대출규제가완화되거나전매제한이없어지면거래를하기\n쉬워져서거래량이늘어날수있을것같아요. 따라서규제지역해제는거래절벽을막는\n데도움이될수있을것같네요.\n하지만이렇게생각해보지만, 실제로규제지역해제가거래활성화에어떤영향을미치는\n지좀더구체적으로알아볼필요가있을것같아요. 규제지역이해제되면투기과열지구,\n조정대상지역등에서벗어나게되어서청약조건이완화되고, 대출한도가늘어나거나,\n다주택자도대출을받을수있게되는등거래관련규제가완화되겠죠. 이렇게되면\n32\n\n매수수요가늘어나거래가활발해질수있을것같아요. 반면에규제지역해제가오히려\n투기를유발하여시장과열을초래할수도있다는우려도있을수있지만, 문제에서는\n거래절벽, 즉거래가급격히줄어드는현상을막기위한조치이므로규제완화가도움이\n될것같아요.\n그래서각선택지를비교해보면, A는거래비용을늘려거래를줄이므로부적절하고, B\n는보유세인상으로매물출회가있을수도있지만직접적인거래활성화보다는다른\n효과를가져올수있으며, C는공급감소로거래를더줄일수있고, D는규제완화로\n거래를촉진할수있으므로가장합리적인것은D가아닐까생각해요.\n하지만여기서한가지의문이드는점이있어요. 규제지역해제가거래절벽을해결하는\n직접적인방법이될수있을까요? 예를들어, 현재부동산시장이침체기라서거래가\n줄어든상황이라면, 규제를풀어주는것이수요를자극하여거래를늘릴수있을거예요.\n반면에시장이과열되어서규제를강화한상황이라면, 규제를완화하면다시과열될수\n있지만, 문제에서는거래절벽, 즉거래감소상황을막기위한것이므로규제완화가\n맞을것같아요.\n또한다른가능성으로는, 거래절벽이금리상승이나경기침체등외부요인에의한\n것이라면정부의규제완화만으로는효과가제한적일수있지만, 문제에서는정부의\n정책중에서선택하라고했으니정책적수단중에서가장적절한것을고르는문제일\n거예요.\n정리하자면, 거래를늘리기위해서는거래비용을낮추거나규제를완화하는것이도움이\n될텐데, 거래세율인상(A)은비용을높이므로반대이고, 보유세인상(B)은보유부담을\n높여매도유인은있지만거래활성화와는직접연관이적을것같아요. 재건축안전진단\n강화(C)는공급감소로작용할수있으니역시거래활성화에부정적일것같고, 규제지역\n해제(D)는규제를완화하여거래를촉진할수있으므로가장적절한조치로보여요.\n하지만여기서규제지역해제가거래활성화에도움이된다는가정이맞는지다시한\n번생각해봐야할것같아요. 규제지역으로지정된지역은투기과열이나조정대상지역\n등인데, 이지역들을해제하면대출규제, 전매제한등이완화되어거래가용이해질\n수있습니다. 예를들어, 전매제한기간이줄어들면단기매매가가능해져서거래량이\n늘어날수있고, 대출규제가완화되면실수요자들이주택구입에더쉽게접근할수\n있게되어거래가활성화될수있겠죠. 따라서규제지역해제는거래절벽을완화하는데\n도움이될수있을것같아요.\n반면에, 만약현재거래절벽이금리인상으로인한유동성축소나경기침체에따른수요\n감소때문이라면, 규제완화로도거래를늘리기어려울수있지만, 문제에서는부동산\n시장의거래절벽을막기위한정부의조치중에서선택하라고했으므로, 주어진선택지\n중에서는D가가장적절하다고봐야할것같아요.\n그럼다른선택지들을다시한번체크해보죠.\n(A) 거래세율인상: 이건확실히거래비용을증가시켜거래를줄이므로, 거래절벽을\n막기보다는심화시킬것같아요. 따라서A는틀린답.\n(B) 보유세율인상: 보유세를올리면집주인들이세금부담을느껴매물을내놓을수\n있다는의견도있지만, 반대로매물을내놓지않고버티기를할수도있어요. 또한, 보유세\n인상은주로다주택자나고가주택소유자에게타격을줄텐데, 이들이매물을내놓으면\n공급이늘어나거래가활성화될수도있겠지만, 이는상황에따라다를수있어요. 하지만\n일반적으로보유세인상은거래활성화보다는공급증가를통한가격안정화에더초점이\n맞춰져있을것같아요. 따라서거래절벽자체를해결하기위한직접적인조치로는B가\n적절하지않을수있어요.\n(C) 재건축안전진단강화: 재건축을통한공급확대는주택공급을늘려서시장안정에\n기여할수있지만, 안전진단을강화하면재건축이어려워져서공급이줄어들게되죠.\n공급감소는가격상승압력으로작용할수있지만, 거래활성화와는별개문제일수\n33\n\n있어요. 만약공급이부족해서거래가안된다면공급을늘려야하지만, 현재거래절벽은\n공급부족보다는다른요인때문이라면이조치는효과가없을거예요. 따라서C는거래\n활성화를위한조치로는부적절해보여요.\n(D) 규제지역해제: 앞서생각한것처럼규제를완화하면거래가용이해져서거래량이\n늘어날수있으므로D가가장적절하다고생각됩니다.\n하지만실제로한국의부동산정책에서규제지역해제가거래활성화에미친영향은\n어땠는지기억을더듬어보면, 예를들어2022년말이나2023년초에정부가일부규제지\n역을해제한적이있었어요. 그당시에는부동산시장이냉각되어있었기때문에규제를\n완화해서거래를촉진하려는목적이었을거예요. 따라서실제사례에서도규제지역\n해제가거래절벽완화에도움이되는정책으로사용된바있으니, 이문제에서도D가\n정답일가능성이높아보여요.\n결론적으로, 각옵션을분석해보면(D) 규제지역해제가가장합리적인조치로보입니\n다.<|im_end|> <|im_start|>assistant Answer: D<|im_end|>\nHyperCLOVA X THINK (Translated Model Input&Output – English)\n<|im_start|>tool_list\n<|im_end|> <|im_start|>system\nThe following are multiple choice questions with explanations and/or answers about\ncommonsense-context30.<|im_end|>\n<|im_start|>user\nQuestion: Context: Everyone eventually came back to me and made me feel loved. It\nwas truly wonderful. It’s amazing and touching that I have such wonderful friends by my\nside. I think it felt even more special because I hadn’t seen them in such a long time. It’s\nstrangely surprising to have returned like this, but meeting people again is really a great\nthing. Especially Donghwan.\nQuestion: If I hadn’t left, how would I have felt?\nRead the question again: Context: Everyone eventually came back to me and made me feel\nloved. It was truly wonderful. It’s amazing and touching that I have such wonderful friends\nby my side. I think it felt even more special because I hadn’t seen them in such a long time.\nIt’s strangely surprising to have returned like this, but meeting people again is really a great\nthing. Especially Donghwan.\nQuestion: If I hadn’t left, how would I have felt?\nOptions:\n(A). Donghwan would have hated me, and nothing would have happened.\n(B). I wouldn’t have missed these people this much.\n(C). It’s still joyful and touching to have friends who hurt me.\n(D). It would have felt strange not to have left home at all.<|im_end|>\n<|im_start|>assistant/think\nToday, the user provided context and a question, requiring me to select the most appropriate\nanswer. I’ll first carefully examine the context provided. The speaker mentions feeling loved\nand touched because friends returned to their side, emphasizing how special it felt due to\nthe long absence, especially highlighting Donghwan. The speaker mentions that returning\nfelt strangely surprising but meeting people again was truly great.\nThe question asks, \"If I hadn’t left, how would I have felt?\" Currently, the speaker is moved\nbecause of the reunion after leaving and returning. If they had never left, the intense feelings\nrelated to the reunion might not have existed.\nAnalyzing each option individually:\n(A). Donghwan would have hated me, and nothing would have happened.\nThis doesn’t align with the context. There is no indication in the context that Donghwan or\n34\n\nothers would have felt negatively if the speaker hadn’t left.\n(B). I wouldn’t have missed these people this much.\nThis is logical. The speaker emphasizes that the long absence made the reunion more special\nand touching. Without leaving, the feeling of longing would naturally be less intense.\n(C). It’s still joyful and touching to have friends who hurt me.\nThis is incorrect. The context never indicates that the friends were hurtful. Rather, it focuses\non positive and warm feelings upon reunion.\n(D). It would have felt strange not to have left home at all.\nThe context describes the return as \"strangely surprising,\" suggesting that leaving and\nreturning was unusual. If the speaker had never left, it might have felt normal rather than\nstrange to remain home. Therefore, option D is somewhat ambiguous but doesn’t strongly\nalign with the context.\nGiven the context and the reasoning above, option (B) stands out as the most appropriate\nchoice. The emotional impact of missing friends after a long absence is clearly highlighted,\nand thus, had the speaker never left, these strong feelings of longing would not have existed.\nHence, the correct answer is option (B).<|im_end|>\n<|im_start|>assistant\nAnswer: B<|im_end|>\nHyperCLOVA X THINK (Model Input&Output – Korean)\n<|im_start|>tool_list\n<|im_end|> <|im_start|>system\nThe following are multiple choice questions with explanations and/or answers about\ncommonsense-context30.<|im_end|>\n<|im_start|>user\nQuestion: 맥락: 모두가결국저한테돌아와줬고제가사랑받고있다고느끼게했어요.\n진짜최고였네요. 진짜이런멋진친구들이제곁에있다는게너무좋고감동적이에요.\n특히오랫동안못봐서더그랬던거같아요. 이렇게돌아오게된게이상할정도지만\n사람들이랑다시만난다는것은진짜좋은일이에요. 특히동환이요.\n질문: 만약내가떠나지않았더라면내가어떻게느꼈을까?\nRead the question again: 맥락: 모두가결국저한테돌아와줬고제가사랑받고있다고\n느끼게했어요. 진짜최고였네요. 진짜이런멋진친구들이제곁에있다는게너무좋고\n감동적이에요. 특히오랫동안못봐서더그랬던거같아요. 이렇게돌아오게된게이상할\n정도지만사람들이랑다시만난다는것은진짜좋은일이에요. 특히동환이요.\n질문: 만약내가떠나지않았더라면내가어떻게느꼈을까?\nOptions:\n(A). 동환이가나를미워했을거라아무일도없었을것이다.\n(B). 이만큼이사람들을그리워하진않았을것이다.\n(C). 상처주는친구들이있다는것이여전히기쁘고감동적이다.\n(D). 집을전혀떠나지않았다는것이이상하게느껴졌을것이다.<|im_end|>\n<|im_start|>assistant/think\n오늘사용자가맥락과질문을주고, 그에맞는답을선택하는문제를내주었어. 먼저\n맥락을자세히읽어봐야겠지. 맥락을보면, 주인공은친구들이돌아와주고사랑받는다고\n느껴서감동적이고좋았다고말하고있어. 특히오랫동안못봤던친구들, 특히동환이를\n다시만나게되어서더그랬다고해. 그리고\"이렇게돌아오게된게이상할정도지만\"\n이라고했어. 이게중요한부분일것같아.\n질문은\"만약내가떠나지않았더라면내가어떻게느꼈을까?\"야. 현재상황은주인공이\n떠났다가다시돌아왔고, 그로인해친구들이돌아와줘서감동받은상황이야. 그런데\n만약떠나지않았다면어떻게느꼈을까를묻는거지.\n35\n\n선택지를하나씩분석해봐야겠어.\n(A) 동환이가나를미워했을거라아무일도없었을것이다.\n이건맥락과좀안맞는것같아. 현재상황은친구들이돌아와줘서좋은상황인데, 떠나지\n않았다면동환이가미워했을거라니? 맥락에서는떠나서다시돌아왔을때친구들이\n돌아와줬으니, 떠나지않았다면친구들이떠나지않았을까? 그런데이건좀헷갈리네.\n아니면주인공이떠났기때문에친구들이돌아온것일까? 맥락에서\"모두가결국저한테\n돌아와줬고\"라고했으니까, 주인공이떠난후친구들이돌아온걸수도있겠지. 그런데\n만약떠나지않았다면친구들이돌아올필요가없었을거고, 그래서그감동을느끼지\n못했을수도있겠다. 그런데(A)는동환이가미워했을거라고하니맥락과연결이잘안\n돼보여.\n(B) 이만큼이사람들을그리워하진않았을것이다. 이건가능할것같아. 주인공이\n떠났기때문에친구들을오랫동안못봤고, 그래서다시만났을때더감동적이었던\n거잖아. 만약떠나지않았다면오랫동안못보지않았을테니그렇게까지그리워하지\n않았을거라는말이되겠지. 맥락에서\"특히오랫동안못봐서더그랬던거같아요\"라고\n했으니, 떠나지않았다면그부분이없었을테니까. 이선택지는타당해보여.\n(C) 상처주는친구들이있다는것이여전히기쁘고감동적이다.\n이건맥락과맞지않아. 현재상황은친구들이돌아와서감동적이지만, 상처주는\n친구들이라면? 맥락에서는친구들이돌아와서좋은상황이니까, 상처주는친구들이\n있다는게아니라오히려돌아와서좋은상황이잖아. 따라서(C)는부적절한것같아.\n(D) 집을전혀떠나지않았다는것이이상하게느껴졌을것이다.\n이건좀애매해. 현재상황에서\"이렇게돌아오게된게이상할정도지만사람들이랑\n다시만난다는것은진짜좋은일이에요\"라고했으니까, 돌아오는게이상했지만좋은\n일이라고했어. 만약집을떠나지않았다면, 돌아오는상황자체가없으니까, \"집을\n전혀떠나지않았다는것이이상하게느껴졌을것이다\"라고하면, 현재상황에서집을\n떠난것이이상했지만좋은거라면, 떠나지않았을때는그이상함이없으니이상하게\n느껴지지않을까? 그런데이건좀추측이필요해. 맥락에서\"이렇게돌아오게된게\n이상할정도\"라고했으니까, 떠나지않았다면돌아올필요가없으니그이상함은없었을\n거고, 따라서집을떠나지않았다는것이이상하게느껴지지않을것같은데. 아니면원래\n집을떠나는게일반적인상황이라면, 떠나지않았을때오히려이상하게느껴질수도\n있을까? 이건좀애매한것같아.\n다시질문으로돌아가서, \"만약내가떠나지않았더라면내가어떻게느꼈을까?\"라고\n했을때, 현재상황은떠났다가돌아와서친구들을다시만나감동받았어. 따라서떠나지\n않았다면친구들과떨어져있지않았을테니, 다시만나는감동은없었을거야. 특히\n오랫동안못봤기때문에더감동적이었던부분이있으므로, 떠나지않았다면그만큼\n그리워하지않았을것(B)이라는말이맞을것같아.\n(A)는동환이가미워했을거라니맥락과안맞고, (C)는상처주는친구들이라는게말이\n안되고, (D)는집을떠나지않은것이이상하게느껴진다는건데, 현재상황에서집을\n떠난것이이상했지만좋은거라면, 떠나지않았을때는그이상함이없으니이상하지\n않을것같은데. 따라서(D)는애매하고, (B)가가장자연스러워보여.\n하지만맥락에서\"이렇게돌아오게된게이상할정도지만사람들이랑다시만난다는것\n은진짜좋은일이에요\"라고했으니, 돌아오는게예상밖의일이었지만좋은일이었다는\n거지. 만약떠나지않았다면돌아올필요가없으니, 돌아올일이없어서그좋은일을\n경험하지못했을거야. 따라서\"이만큼이사람들을그리워하진않았을것이다\"라는(B)\n가맞을것같아. 왜냐하면떠나서오랫동안못봤기때문에그리워했고, 다시만나서\n감동적이었으니까, 떠나지않았다면그렇게까지그리워하지않았을테니까.\n그래서정답은(B)인것같아.<|im_end|>\n<|im_start|>assistant\nAnswer: B<|im_end|>\n36\n\nD\nQualitative Examples in Vision Adaptation Ablation\nWe present below a representative example from the KCSAT STEM vision benchmark. All prob-\nlems are rendered and presented to the model strictly as images, without any accompanying text\ninput. This setup requires the model to make use of its full vision capability–not only to recognize\nand interpret textual content within the images, but also to read and reason about mathematical ex-\npressions, tables, and diagrams where present. For all examples, the user query is standardized as:\n“이미지를보고문제를풀어주세요.” (“Please solve the problem by looking at the image.”)\nD.1\nKCSAT Biology Basic Example\nWe present our model’s output on a sample KCSAT Biology Basic problem. Figure 7 shows the\noriginal problem image, which is directly provided to the model for analysis and reasoning. The\nmodel is required to interpret, reason through, and solve the problem solely based on the image. For\nreference, we include the model’s original answer, as well as an English translation of the problem\nfor readers. A literal English translation of the model’s output is also provided for comparison.\nFigure 7: Original KCSAT Sample (in Korean).\nSample Question 8 (Translated)\nThe diagrams below represent parts of processes A and B in succession, and a graph\ndepicting the changes in total productivity and respiration amounts of plant community K\nover time. A and B refer to primary and secondary successions in no particular order, and\nlabels ㄱand ㄴrefer to hydrosere and xerosere in no particular order.\n<Process Diagram> <Graph>\nWhich of the following statements are correct based on the above?\n(Options)\n(a) B is a secondary succession.\n(b) ㄱis a hydrosere.\n(c) For K, the total productivity\nrespiration\nis greater at t2 than at t1.\nSelect the correct combination(s):\n(1) a\n(2) b\n(3) a, c\n(4) b, c\n(5) a, b, c\n37\n\nHyperCLOVA X THINK (Translated Model Output – English)\n<|im_start|>assistant/think\nOkay, let’s solve this problem. First, we need to look carefully at the figure given in the\nproblem.\nFirst, we need to examine figures (a) and (b) of question 8. According to the problem, A\nand B are one of primary succession and secondary succession, and ㉠and ㉡represent\ndeciduous forest and lichen respectively, in no particular order. Also, it shows a graph of the\nchanges in total productivity and respiration amount of plant community K over time.\nLet’s look at (a) first. A is said to go from grassland to shrub forest to ㉠, and B is said to\nstart from lava field, go through ㉡, and reach grassland. Here, we need to figure out which\nsuccession process A and B each represent. Also, we need to identify what ㉠and ㉡are.\nIf we think about the succession process, primary succession starts in lifeless areas, and\nsecondary succession is the process of recovery after disturbance in an area where plants\nalready existed. For example, starting on bare land created by disturbances like forest fires\nor logging is secondary succession. On the other hand, primary succession starts in places\nlike rocks or lava fields, where there isn’t even soil.\nIn the problem, since A starts from grassland (a place where plants already exist), this would\nbe the secondary succession process. That’s because grassland is an environment with soil\nand some living organisms. In contrast, B starts from a lava field, which is formed by vol-\ncanic activity and likely has no soil. Thus, B would correspond to primary succession. There-\nfore, the conclusion is that A is secondary succession and B is primary succession... but wait,\nin the problem it states that A and B represent “primary and secondary in no order,” so I\nmight have thought in reverse, so confirmation is necessary.\nLooking again, A: grassland →shrub forest →㉠\nB: lava field →㉡→grassland\nA already begins with grassland, so it’s a secondary succession process, and B starts from\nlava field, so it is a primary succession process. So A is secondary, B is primary. Then in the\nchoices, statement ㄱsays “B is secondary succession,” so that’s a wrong statement. So ㄱ\nis incorrect.\nNext, let’s look at what ㉠and ㉡are. A process is secondary succession, and generally, the\nfinal stage of secondary succession could be a climax, such as a shade-tolerant forest, but\ndepending on the case, it could be something else. However, usually, secondary succession\ndevelops into a more complex forest structure over time. But here, A passes through shrub\nforest and goes to ㉠, so if ㉠is a deciduous forest, the next stage would be a shade-tolerant\nforest, but does the problem refer to the final stage or an intermediate one?\nOn the other hand, B is primary succession, starting from a lava field, going through ㉡, and\nthen to grassland. In the early stage of primary succession, pioneer species such as lichen\nor moss appear, and then herbaceous plants grow, followed by shrub forest, and eventually\ndeciduous, mixed, and shade-tolerant forests in order. However, since B goes from lava field\nthrough ㉡to grassland, it’s likely that ㉡is lichen. That’s because in harsh environments\nlike lava fields, what settles first tends to be things like lichen. Then ㉡is lichen, and ㉠\nwould be deciduous forest. That’s because, as A undergoes secondary succession, after go-\ning through shrub forest it would further develop into deciduous forest. Therefore, ㉠is\ndeciduous forest and ㉡is lichen. Accordingly, statement ㄴ, “㉠is deciduous forest,” is a\ncorrect statement.\nNext, let’s look at figure (b). It shows changes in total productivity and respiration of plant\ncommunity K depending on time (t). At time t0, it appears, and up to t1, total productivity\nincreases, then afterwards decreases, while respiration continues to increase, and from t2\nonward, it is marked as the appearance of a shade-tolerant forest. That is, before t1, total\nproductivity is greater than respiration, so net productivity has a positive value, after t1,\ntotal productivity decreases and becomes similar to or even less than respiration, so net\nproductivity reduces.\nNet productivity is total productivity minus respiration, so at t1, total productivity is at its\nmaximum and respiration is relatively low, so net productivity will be very high. But at t2,\ntotal productivity is lower and respiration continues to increase, so net productivity will be\nfar lower than at t1. Therefore, the ratio of K’s net productivity to respiration is greater at\n38\n\nt1 than at t2. But the choice ㄷsays, “K’s net productivity/respiration is greater at t2 than at\nt1,” so this is a wrong explanation. So ㄷis wrong too.\nIn summary:\n- ㄱ: “B is secondary succession.” →B is primary succession, so wrong.\n- ㄴ: “㉠is deciduous forest.” →Correct.\n- ㄷ: net productivity/respiration is t2 > t1 →Wrong. t1 is greater.\nTherefore, the answer is only ㄴis correct, so we need to check if there is an option with\nonly ㄴ. The options provided are as follows:\n①ㄱ\n②ㄴ\n③ㄱ, ㄷ\n④ㄴ, ㄷ\n⑤ㄱ, ㄴ, ㄷ\nIn this, the case where only ㄴis correct is option ②, but let’s check again whether there\nwas any calculation mistake.\nAt t1, total productivity is high and respiration is still low, so net productivity is abundant,\nand at t2, total productivity has decreased and respiration has increased, so net productivity\nis lower. Therefore, the ratio of net productivity to respiration, that is, net productivity/respi-\nration, is much greater at t1. For example, if at t1, total productivity is 10 and respiration is 3,\nnet productivity is 7 and the ratio is 7/3 2.33. If at t2, total productivity is 5 and respiration\nis 4, net productivity is 1 and the ratio is 1/4 = 0.25. So t1 is definitely greater. Thus, ㄷis\nwrong, and the answer is option ②.\n**Final Answer**: (2) <|im_end|>\n<|im_start|>assistant\nAnalyze figures (a) and (b) to solve the problem.\n1. **Analysis of succession process**:\n- **A**: Starts from grassland, passes through shrub forest, and proceeds to ㉠. Since it\nstarts from grassland where plants already exist, it is **secondary succession**.\n- **B**: Starts from lava field, passes through ㉡, and proceeds to grassland. Since it starts\nfrom a lava field with no soil, it is **primary succession**.\n- Therefore, ㄱ(\"B is secondary succession.\") is an **incorrect** explanation.\n2. **Identification of ㉠and ㉡**:\n- **A** (secondary succession): Passes through shrub forest and proceeds to ㉠, and gener-\nally, the late stage of secondary succession is a **deciduous forest**.\n- **B** (primary succession): Starts from lava field, passes through ㉡, and proceeds to\ngrassland, with the initial stage being **lichen**.\n- Therefore, ㄴ(\"㉠is a deciduous forest.\") is a **correct** explanation.\n3. **Graph analysis**:\n- **Total productivity** increases until t1 and then decreases, while **respiration** contin-\nues to increase.\n- **Net productivity** is at its highest at t1 when total productivity is maximal and respira-\ntion is relatively low, so the ratio is high. At t2, total productivity decreases and respiration\nincreases, so the net productivity ratio becomes lower.\n- Therefore, ㄷ(\"K’s net productivity\nrespiration\nis greater at t2 than at t1\") is an **incorrect** explanation.\nIn conclusion, the only correct explanation is **ㄴ**.\n(2) <|im_end|>\nHyperCLOVA X THINK (Original Model Output – Korean)\n<|im_start|>assistant/think\n좋아, 이문제를풀어보자. 일단문제에서주어진그림을잘봐야겠지.\n먼저8번문제의그림(가)와(나)를살펴봐야해. 문제에따르면A와B는1차천이와2\n차천이중하나이고, ㉠과㉡은양수림과지의류를순서없이나타낸거야. 그리고식물\n군집K의시간에따른총생산량과호흡량을그래프로보여주고있어.\n39\n\n일단(가)부터보자. A는초원에서관목림을거쳐㉠으로가고, B는용암대지에서시작\n해서㉡을거쳐초원으로간다고되어있네. 여기서A와B가각각어떤천이과정인지\n알아내야해. 또㉠과㉡이무엇인지도파악해야하고.\n천이과정을생각해보면, 1차천이는생명이없는지역에서시작하는거고, 2차천이는\n기존에식물이있던곳에서교란이일어난후에다시회복되는과정이야. 예를들어산\n불이나벌목같은것들로인해생긴빈땅에서시작하는게2차천이지. 반면1차천이는\n바위나용암대지처럼토양조차없는곳에서시작돼.\n문제에서A는초원(이미식물이있는곳)에서시작하니까이건2차천이과정이겠지. 왜냐\n하면초원은이미토양이있고일부생물이존재하는환경이니까. 반면B는용암대지에서\n시작하는데, 용암대지는화산활동으로인해형성된곳으로토양이없을가능성이높아.\n그래서B는1차천이과정에해당할거야. 따라서A는2차천이, B는1차천이라는결론이\n나오지만... 잠깐, 문제에서는A와B가1차와2차를순서없이나타냈다고했으니까내가\n반대로생각했을수도있으니확인필요.\n다시보면, A: 초원→관목림→㉠\nB: 용암대지→㉡→초원\nA는이미초원이있으므로2차천이과정이고, B는용암대지에서시작하므로1차천이\n과정이다. 그러면A는2차, B는1차가맞아. 그럼보기에서ㄱ은’B는2차천이이다.’라고\n했으니틀린설명이되겠지. 그러니까ㄱ은틀렸어.\n다음으로㉠과㉡이뭔지보자. A 과정은2차천이이므로, 일반적으로2차천이의종착\n단계는극상인음수림이될수있지만, 경우에따라서는다른형태일수도있겠지. 하지만\n보통2차천이는시간이지나면더복잡한구조의숲으로발달해. 그런데여기서는A가\n관목림을거쳐㉠으로가는데, 만약㉠이양수림이라면그다음단계는음수림일텐데,\n문제에서는최종단계를나타내는걸까? 아니면중간단계일까?\n반면B는1차천이로, 용암대지에서시작하여㉡을거쳐초원으로간다. 1차천이의초기\n에는지의류나선태식물같은개척자들이나타나고, 그다음에초본식물들이자라면서\n관목림이되고, 결국에는양수림, 혼합림, 음수림순으로진행되지. 그런데B는용암대\n지에서㉡을거쳐초원으로가므로, ㉡은아마도지의류일가능성이높아. 왜냐하면용\n암대지와같은척박한환경에서가장먼저정착하는것은지의류같은것이기때문이야.\n그렇다면㉡은지의류이고, ㉠은양수림이되는거겠지. 왜냐하면A는2차천이과정에서\n관목림을거쳐더발전하면양수림이될테니까. 따라서㉠은양수림, ㉡은지의류가돼.\n그러므로보기의ㄴ은’㉠은양수림이다’라고했으니맞는설명이야.\n그다음으로(나) 그래프를보자. 식물군집K의시간(t)에따른총생산량과호흡량의변\n화인데, t0 시점에서출현하고, t1까지는총생산량이증가하다가이후감소하며, 호흡량은\n계속증가하고, t2 시점부터는음수림출현이라고표시되어있어. 즉, t1 이전에는총생산\n량이호흡량보다많아서순생산량이양의값을가지다가, t1 이후에는총생산량이감소하\n면서호흡량과비슷해지거나오히려작아져서순생산량이줄어들게되지.\n순생산량은총생산량- 호흡량이니까, t1 때는총생산량이최대이고호흡량은상대적으로\n낮아서순생산량이매우높을거야. 그러나t2 때가되면총생산량은더낮아지고호흡량\n은계속증가해서순생산량이t1 때보다훨씬낮아질거야. 따라서K의순생산물/ 호흡량\n비율은t1 때가t2 때보다크다는결론이나와. 그런데문제의보기ㄷ은’K의순생산량/\n호흡량은t2일때가t1일때보다크다’라고했으므로이는틀린설명이야. 따라서ㄷ도\n틀렸다.\n정리하면:\n- ㄱ: B는2차천이이다. →B는1차천이이므로틀림.\n- ㄴ: ㉠은양수림이다. →맞음.\n- ㄷ: 순생산량/호흡량은t2 > t1 →틀림. t1이더큼.\n따라서정답은ㄴ만맞는경우인데, 보기중에ㄴ만있는선택지가있는지확인해봐야해.\n문제에서제시된보기옵션은다음과같아:\n①ㄱ\n②ㄴ\n③ㄱ, ㄷ\n④ㄴ, ㄷ\n⑤ㄱ, ㄴ, ㄷ\n여기서ㄴ만맞는경우는②번이지만, 혹시계산실수가있었는지다시확인해보자.\nt1 시기에는총생산량이높고호흡량은아직낮은상태라순생산량이많을거고, t2 시기에\n는총생산량이감소했고호흡량은증가했으므로순생산량은더적어져. 따라서순생산량\n대비호흡량의비율, 즉순생산량/호흡량은t1 때가훨씬높다. 예를들어t1에서총생산량\n40\n\n이10, 호흡량3이면순생산량은7이고비율은7/3 2.33. t2에서총생산량5, 호흡량4라면\n순생산량1, 비율은1/4 = 0.25. 따라서확실히t1이더크다. 그러니ㄷ은틀렸고, 따라서\n정답은②번.\n**Final Answer**: (2) <|im_end|>\n<|im_start|>assistant\n그림(가)와(나)를분석하여문제를해결한다.\n1. **천이과정분석**:\n- **A**: 초원에서시작하여관목림을거쳐㉠으로진행된다. 이미식물이있는초원에서\n시작되므로**2차천이**이다.\n- **B**: 용암대지에서시작하여㉡을거쳐초원으로진행된다. 토양이없는용암대지\n에서시작하므로**1차천이**이다.\n- 따라서ㄱ(\"B는2차천이이다.\")은**틀린** 설명이다.\n2. **㉠과㉡식별**:\n- **A**(2차천이): 관목림을거쳐㉠으로진행되며, 일반적으로2차천이의후기단계는\n**양수림**이다.\n- **B**(1차천이): 용암대지에서시작하여㉡을거쳐초원으로진행되며, 초기단계는\n**지의류**이다.\n- 따라서ㄴ(\"㉠은양수림이다.\")은**맞는** 설명이다.\n3. **그래프분석**:\n- **총생산량**은t1까지증가한후감소하며, **호흡량**은지속적으로증가한다.\n- **순생산량**은t1 시기에총생산량이최대이고호흡량이상대적으로낮아비율이높\n다. t2 시기에는총생산량이감소하고호흡량이증가하여순생산량비율이낮아진다.\n- 따라서ㄷ(\"K의순생산량\n호흡량은t2일때가t1일때보다크다\")은**틀린** 설명이다.\n최종적으로옳은설명은**ㄴ**뿐이다.\n(2) <|im_end|>\nD.2\nKCSAT Biology Advanced Example\nWe present our model’s output on a sample KCSAT Biology Advanced problem. Figure 8 shows\nthe original problem image, which is directly provided to the model for analysis and reasoning. The\nmodel is required to interpret, reason through, and solve the problem solely based on the image. For\nreference, we include the model’s original answer, as well as an English translation of the problem\nfor readers. A literal English translation of the model’s output is also provided for comparison.\nFigure 8: Original KCSAT Sample (in Korean).\n41\n\nSample Question 2 (Translated)\nThe table below summarizes the characteristics of plant tissues. A and B refer to vascular\nand dermal tissues, listed in no particular order.\nTissue\nCharacteristic\nA\nCovers the surface and protects the plant body.\nB\n?\nFundamental tissue\nSynthesizes and stores nutrients.\nWhich statements below, from the options labeled (a) to (c), are correct regarding the\ntissues described above?\n(Options)\n(a) A is a dermal tissue.\n(b) Substances are transported via tissue B.\n(c) The petal of a rose contains fundamental tissue.\nSelect the correct combination(s):\n(1) a\n(2) b\n(3) a, c\n(4) b, c\n(5) a, b, c\nHyperCLOVA X THINK (Translated Model Output – English)\n<|im_start|>assistant/think\nOkay, let’s solve this problem. First, I need to carefully read the question. The question gives\na table showing the characteristics of plant tissue systems, and A and B are listed as vascular\ntissue system and epidermal tissue system in random order. Also, it is stated that the ground\ntissue system synthesizes and stores nutrients.\nFirst, looking at the table, the characteristic of A is “covers the surface and protects the\nplant body,” and B is not yet explained. However, since the ground tissue system is already\ndescribed as synthesizing and storing nutrients, the remaining two must be the vascular and\nepidermal tissue systems. The problem states that one of A or B is epidermal, and the other\nis vascular.\nLet’s summarize the characteristics of each tissue system.\n1. Ground tissue system: This is also known as the fundamental tissue and mainly consists\nof parenchyma and sclerenchyma, which do not perform photosynthesis but serve to store\nor transport water and nutrients. So, in the problem, the characteristic of the ground tissue\nsystem is given as “synthesizes and stores nutrients”... Wait, does the ground tissue system\nphotosynthesize? No, usually it’s tissues like palisade cells in the epidermal tissue or vas-\ncular tissue that do that, isn’t it? Wait, this could be a confusing point. I was taught that the\nground tissue system generally does not photosynthesize, but in the problem, its character-\nistic is “synthesizes and stores nutrients.” I need to check if this is correct.\nIs the problem wrong? Or am I misremembering?\nThe ground tissue system mainly stores, and photosynthesis occurs in the epidermal tissue\n(especially palisade cells below the leaf epidermis). So is the description of the ground tissue\nsystem in the problem incorrect? Or is there another interpretation?\nSince the problem states that the ground tissue system “synthesizes and stores nutrients,” I\nhave no choice but to accept it as it is for now. Probably, that’s how the problem defines it.\nGoing back to the question: is A the epidermal tissue system or vascular? The characteristic\nof A is “covers the surface and protects the plant body,” which is a major function of the\nepidermal tissue system. For example, epidermal cells cover the surface of leaves, preventing\nwater evaporation and protecting from pathogens. Therefore, A is epidermal tissue system,\nand B is likely vascular tissue system.\nThen, what would be the feature of B? The vascular tissue system is composed of xylem\nand phloem and is involved in the transport of water and nutrients, so the characteristic of\nB would be something like “transports water and nutrients.” But the characteristic of B is\nleft blank in the problem, so we have to infer it and select the correct explanation from the\nchoices.\n42\n\nNow let’s look at the choices.\nㄱ. A is the epidermal tissue system. →Correct. Based on the reasoning above, this is true.\nㄴ. Substances move through B. →If B is the vascular tissue system, then it is the pathway\nfor water and nutrient transport, so this is also true.\nㄷ. Rose leaves have ground tissue system. →The ground tissue system should exist in all\nplant leaves. As the internal tissue of leaves, it stores or transports water and nutrients, so\nthis is also true.\nHowever, the problem describes the ground tissue system as “synthesizes and stores nutri-\nents,” which makes me doubtful as to whether it really does photosynthesis, but since that is\nwhat the problem states, I have to accept it. So ㄷis also a correct explanation.\nBut based on prior knowledge, the ground tissue system does not photosynthesize, so I\nwonder if the problem is in error. If, for example, the problem actually refers to the epidermal\ntissue system instead? No, the problem specifically describes the characteristic of the ground\ntissue system, so I should just follow that.\nSo are ㄱ, ㄴ, ㄷall correct? The answer options include 5 as “ㄱ, ㄴ, ㄷ,” so that seems\npossible, but is there any chance I made a mistake?\nLet me check again.\nA: Epidermal tissue system →covers surface. O\nB: Vascular tissue system →transports water and nutrients. O\nGround tissue system: In problem, synthesizes and stores nutrients. Might be different from\nreality but will follow the problem.\nDo rose leaves have ground tissue system? The inside of leaves is made of parenchyma,\nwhich is ground tissue, so O.\nTherefore, it seems all three are correct. Is the answer 5?\nBut sometimes in tests there are traps—could ㄷsomehow be wrong?\nFor example, if rose leaves had only vascular tissues and not ground tissue? But in a leaf,\nthere are epidermis, guard cells, palisade tissue (ground tissue), vascular bundles, etc., so\nground tissue must be present. Thus, ㄷis valid.\nSo the answer is 5?\nEven though at first I thought the description of the ground tissue system was wrong, since\nthat’s how the problem gives it, I will follow it. Therefore, all ㄱ, ㄴ, and ㄷare correct, and\nthe answer is 5.\n**Final answer**: (5) <|im_end|>\n<|im_start|>assistant\nIn the table for question 2, A and B represent the epidermal tissue system and vascular tissue\nsystem, respectively. The characteristic of the ground tissue system is given as ’synthesizes\nand stores nutrients.’\n- **Characteristic of A**: ’Covers the surface and protects the plant body.’ This is a main\nfunction of the epidermal tissue system, which covers the surface of the plant to prevent\nwater evaporation and protect from external environments. Therefore, A is the epidermal\ntissue system.\n- **Characteristic of B**: It is a characteristic of the vascular tissue system. The vascular\ntissue system is composed of xylem and phloem and is involved in the transport of water and\nnutrients. Therefore, B is the vascular tissue system, and it is correct to say that substances\nare transported through it.\n- **Ground tissue system**: The characteristic given in the problem is ’synthesizes and\nstores nutrients.’ The ground tissue system comprises most of the plant and serves to store\nand transport water and nutrients. The leaves of a rose also contain ground tissue system,\ncarrying out these functions.\nAnalysis of choices:\n- **ㄱ**: A is the epidermal tissue system. (Correct)\n- **ㄴ**: Substances are transported through B. (Correct)\n- **ㄷ**: The leaves of a rose have ground tissue system. (Correct)\nTherefore, since all choices are correct, the answer is number 5.\n(5) <|im_end|>\n43\n\nHyperCLOVA X THINK (Original Model Output – Korean)\n<|im_start|>assistant/think\n좋아, 이문제를풀어보자. 우선문제를잘읽어봐야겠어. 문제는식물을구성하는조직계\n의특징을나타낸표인데, A와B는관다발조직계와표피조직계를순서없이나열했다고\n해. 그리고기본조직계는양분을합성하고저장한다고되어있네.\n먼저, 표를보면A의특징은’표면을덮어식물체를보호한다’고했고, B는아직설명이안\n되어있어. 그런데기본조직계는이미양분합성과저장이라고했으니까, 나머지두개가\n관다발과표피일거야. 문제에서A와B 중하나는표피이고다른하나는관다발이야.\n그럼먼저각조직계의특징을정리해보자.\n1. **기본조직계**: 이건울타리라고도불리는조직으로, 주로유조직과기계조직으로\n이루어져있고, 광합성을하지않지만물과영양분을저장하거나운반하는역할을한다고\n알고있어. 그래서문제에서도기본조직계의특징이’양분을합성하고저장한다’고했는\n데... 잠깐, 기본조직계가광합성을하나? 아니, 보통표피조직이나관다발조직중에\n책상세포같은게있지않나?\n잠깐, 여기서헷갈리기쉬운부분이있을것같아. 기본조직계는일반적으로광합성을\n하지않는다고배웠던것같은데, 문제에서는기본조직계의특징이’양분을합성하고\n저장한다’고했어. 이게맞는지확인해야할것같아.\n아니면문제가잘못된건가? 아니면내가잘못기억한건가?\n기본조직계는주로저장기능을하며, 광합성은표피조직(특히잎의표피아래에있는\n책상세포)에서일어나는걸로알고있어. 그럼문제의기본조직계설명이틀린건가?\n아니면다른해석이있을까?\n문제에서기본조직계의특징이’양분을합성하고저장한다’고했으니일단그대로받아\n들이고넘어가야겠지. 아마문제에서는그렇게정의했으니까.\n그럼다시문제로돌아가서, A는표피조직계일까관다발일까? A의특징은’표면을덮어\n식물체를보호한다’인데, 표피조직계의주요기능중하나가바로식물체의표면덮개\n역할이지. 예를들어잎의표피세포들이표면을덮어서수분증발을막고병원체로부터\n보호해주니까. 따라서A는표피조직계이고, B는관다발조직계일가능성이높아.\n그러면B의특징은뭘까? 관다발조직계는물관부와체관부로나뉘어져있어서물과양\n분의이동에관여하니까, B의특징은아마도’물과양분을운반한다’ 정도가될것같아.\n하지만문제에서는B의특징이비어있고, 우리가그걸추론해서보기중에서맞는설명을\n골라야하니까.\n이제보기를살펴보자.\nㄱ. A는표피조직계이다. →맞아. 위에서생각한대로라면맞는말이야.\nㄴ. B를통해물질이이동한다. →B가관다발조직계라면, 물과양분이이동하는통로이\n므로맞는말이지.\nㄷ. 장미의잎에는기본조직계가있다. →기본조직계는모든식물의잎에존재할거야.\n잎의내부조직으로서물과양분을저장하거나운반하는역할을하니까. 따라서이것도\n맞는말이야.\n그런데문제에서기본조직계의특징이’양분을합성하고저장한다’고했는데, 실제로기\n본조직계가광합성을하는지의문이들지만, 문제에서는그렇게주어졌으므로일단은\n받아들여야해. 그러면ㄷ도맞는설명이되겠지.\n하지만기존지식으로는기본조직계가광합성을하지않는다고알고있는데, 이부분이\n문제의오류인지아닌지가궁금해. 만약문제에서의기본조직계가실제로는표피조직\n계를가리키는것이아니라면? 아니, 문제에서는기본조직계의특징을따로주었으니\n그대로따라가야할것같아.\n따라서ㄱ, ㄴ, ㄷ모두옳은설명인가? 그런데보기에서5번이’ㄱ, ㄴ, ㄷ’이라서그럴\n수도있지만, 혹시내가뭔가잘못생각했을까?\n다시한번점검해보자.\n- A: 표피조직계→표면덮개역할. O\n- B: 관다발조직계→물과양분운반. O\n- 기본조직계: 문제에서는양분합성및저장. 실제와다를수있으나문제에따름.\n장미의잎에는기본조직계가있는가? 잎의내부는유조직으로구성되어있는데, 이는\n기본조직계에해당하므로O.\n그래서세가지모두맞는것같다. 그렇다면정답은5번?\n하지만보통시험문제에서는이런함정이있을수있으니, 혹시ㄷ가틀린경우는없을까?\n44\n\n예를들어, 장미의잎에는기본조직계가없고관다발만있다는식으로? 아니, 잎의경우\n표피, 공변세포, 울타리조직(기본조직계), 관다발등이있으니까기본조직계가있어야\n해. 따라서ㄷ는맞는설명.\n그럼답은5번?\n하지만처음에기본조직계의특징이틀렸다는생각이들었지만, 문제에서는그렇게주\n어졌으니따라가야한다. 따라서ㄱ, ㄴ, ㄷ모두맞는설명이므로5번이정답이다.\n**최종답변** (5) <|im_end|>\n<|im_start|>assistant\n2번문제의표에서A와B는각각표피조직계와관다발조직계를나타낸다. 기본조직계\n의특징은’양분을합성하고저장한다’로주어져있다.\n- **A의특징**: ’표면을덮어식물체를보호한다.’ 이는표피조직계의주요기능으로,\n표피조직계는식물체의표면을덮어수분증발을막고외부환경으로부터보호한다. 따\n라서A는표피조직계이다.\n- **B의특징**: 관다발조직계의특징이다. 관다발조직계는물관과체관으로구성되어\n물과양분의이동에관여한다. 따라서B는관다발조직계이며, 이를통해물질이이동한\n다는것은올바른설명이다.\n- **기본조직계**: 문제에서주어진특징은’양분을합성하고저장한다.’ 기본조직계는\n식물의대부분을차지하며, 물과양분의저장및운반역할을한다. 장미의잎에도기본\n조직계가존재하여이러한기능을수행한다.\n보기분석: - **ㄱ**: A는표피조직계이다. (정답) - **ㄴ**: B를통해물질이이동한다.\n(정답) - **ㄷ**: 장미의잎에는기본조직계가있다. (정답)\n따라서모든선택지가올바르므로정답은5번이다.\n(5) <|im_end|>\nD.3\nKCSAT Math Advanced Example\nWe present our model’s output on a sample KCSAT Math Advanced problem. Figure 9 shows the\noriginal problem image, which is directly provided to the model for analysis and reasoning. The\nmodel is required to interpret, reason through, and solve the problem solely based on the image. For\nreference, we include the model’s original answer, as well as an English translation of the problem\nfor readers. A literal English translation of the model’s output is also provided for comparison.\nFigure 9: Original KCSAT Sample (in Korean).\nSample Question 23 (Translated)\nDetermine the value of\nlim\nx→0\nln(1 + 3x)\nln(1 + 5x)\nChoose the correct answer: (1) 1\n5\n(2) 2\n5\n(3) 3\n5\n(4) 4\n5\n(5) 1\nHyperCLOVA X THINK (Translated Model Output – English)\n<|im_start|>assistant/think\nAlright, this seems to be a problem about finding a limit. It’s question 23, and it presents\nthe ratio of logarithmic functions in fractional form. The expression is the limit as x →0 of\nln(1+3x)\nln(1+5x). The provided choices range from 1\n5 to 1, but which one is correct?\n45\n\nFirst, for limits of this type, you can usually use L’Hôpital’s Rule to solve them, since as\nx →0, both the numerator and denominator become ln(1), so the expression is in the inde-\nterminate form 0/0. By applying L’Hôpital’s Rule, we can find the answer by differentiating\nthe numerator and denominator.\nBefore applying L’Hôpital’s Rule, let’s check: when x = 0, the numerator is ln(1 + 3 ·\n0) = ln(1) = 0, and the denominator is ln(1 + 5 · 0) = ln(1) = 0, so that checks out.\nTherefore, L’Hôpital’s Rule can be used. So, we need to differentiate both the numerator\nand denominator with respect to x. The derivative of the numerator ln(1 + 3x) is\n3\n1+3x, and\nthe derivative of the denominator ln(1 + 5x) is\n5\n1+5x.\nBy L’Hôpital’s Rule, the limit then becomes\n3\n1+3x\n5\n1+5x\n=\n3\n1 + 3x · 1 + 5x\n5\nand, simplifying this,\n3(1 + 5x)\n5(1 + 3x)\nis what you get.\nNow, if you calculate the value as x →0, 1 + 5x →1, and 1 + 3x →1, so overall it should\nbecome 3\n5, right? Looking at the choices, 3\n5 is choice ③. But why did I think of something\nelse at first? Oh, did I calculate something incorrectly?\nLet me double-check. After applying L’Hôpital’s Rule:\nlim\nx→0\n3\n1 + 3x · 1 + 5x\n5\nPlugging in x →0,\n3\n1 · 1\n5 = 3\n5\nSo, the answer is 3\n5? Among the answer choices, ③is 3\n5, ②is 2\n5, and ④is 4\n5, so I need to\nmake sure not to get confused.\nCan this be solved another way, for example, by approximating using a Taylor expansion?\nln(1 + a) ≈a −a2\n2 + · · ·\nUsing this expansion, if a is small, we can approximate ln(1 + a) ≈a.\nSo, in the numerator, ln(1 + 3x) ≈3x, and in the denominator, ln(1 + 5x) ≈5x, which\nmakes the ratio\n3x\n5x = 3\n5\nand we reach the same result.\nSo, since both methods yield 3\n5, it seems choice ③is the answer. Is there any chance I made\na calculation mistake in the process? Let’s check again. When applying L’Hôpital’s Rule,\ndid I correctly differentiate the numerator and denominator:\nd\ndx ln(1 + 3x) =\n3\n1 + 3x,\nd\ndx ln(1 + 5x) =\n5\n1 + 5x\nAnd when converting the division to multiplication, there was no sign or coefficient error,\nand as x →0, the x terms in the denominator and numerator disappear, so the result is\nindeed 3\n5. In the case of the Taylor expansion as well, considering only the first term, it’s\nexactly 3x\n5x = 3\n5. So, certainly, 3\n5 is correct. Therefore, among the choices, since ③is 3\n5, the\nfinal answer is ③.\n**Final answer**: (3) <|im_end|>\n<|im_start|>assistant\nTo find the limit limx→0\nln(1+3x)\nln(1+5x), we use the **L’Hôpital’s Rule**.\n46\n\nSince both the numerator and denominator become the undetermined form 0/0 as x →0,\nwe can apply L’Hôpital’s Rule:\nlim\nx→0\nln(1 + 3x)\nln(1 + 5x) = lim\nx→0\nd\ndx ln(1 + 3x)\nd\ndx ln(1 + 5x)\nDifferentiating each:\nd\ndx ln(1 + 3x) =\n3\n1 + 3x,\nd\ndx ln(1 + 5x) =\n5\n1 + 5x\nTherefore,\nlim\nx→0\n3\n1+3x\n5\n1+5x\n= lim\nx→0\n3\n1 + 3x · 1 + 5x\n5\nWhen x →0, 1 + 3x ≈1 and 1 + 5x ≈1, so\n3\n1 · 1\n5 = 3\n5\nAlso, using a Taylor expansion, ln(1+ax) ≈ax (where a is a constant) as an approximation:\nln(1 + 3x)\nln(1 + 5x) ≈3x\n5x = 3\n5\nTherefore, the answer is (3) .\n<|im_end|>\nHyperCLOVA X THINK (Original Model Output – Korean)\n<|im_start|>assistant/think\n좋아, 이문제는극한을구하는문제인것같아. 23번문제고, 분수꼴의로그함수의비율이\n주어졌어. 식은x →0일때ln(1+3x)\nln(1+5x)의극한이야. 보기로는1\n5부터1까지있는데, 정답이\n뭘까?\n먼저, 이런형태의극한문제는보통로피탈법칙을써서풀수있지않을까? 왜냐하면\n분자와분모모두x →0일때ln(1)이되니까0/0 형태의불정형이기때문이야. 로피탈\n법칙을적용하면미분해서계산할수있을거야.\n로피탈법칙을적용하기전에먼저확인해보자. x = 0일때분자는ln(1+3·0) = ln(1) =\n0, 분모도ln(1 + 5 · 0) = ln(1) = 0이니까맞아. 따라서로피탈법칙을사용할수있어.\n그래서분자와분모를각각x로미분해야해. 분자인ln(1 + 3x)의도함수는\n3\n1+3x이고,\n분모인ln(1 + 5x)의도함수는\n5\n1+5x이지.\n그러면로피탈법칙에의해극한은\n3\n1+3x\n5\n1+5x\n=\n3\n1 + 3x · 1 + 5x\n5\n가되고, 이걸단순화하면\n3(1 + 5x)\n5(1 + 3x)\n가되겠지.\n이제x →0일때이값을계산하면, 1 + 5x →1, 1 + 3x →1이니까전체적으로3\n5가되\n어야하지않나? 근데보기에는3\n5가③번에있네. 그런데왜내가처음생각했을때다른\n결과가나오는걸까? 아, 아니, 계산을잘못했나?\n다시확인해보자. 로피탈법칙적용후의식:\nlim\nx→0\n3\n1 + 3x · 1 + 5x\n5\n47\n\n여기서x →0을대입하면,\n3\n1 · 1\n5 = 3\n5\n맞아, 그럼답은3\n5인가? 그런데문제에주어진보기중에서③번이3\n5이고, ②번이2\n5, ④\n번이4\n5라서헷갈리지않게해야겠어.\n혹시다른방법으로풀어보면, 예를들어테일러전개를사용해서근사할수있을까?\nln(1 + a) ≈a −a2\n2 + · · ·\n이런식으로전개하면, a가작을때ln(1 + a) ≈a 정도로근사할수있지.\n그럼분자는ln(1 + 3x) ≈3x, 분모는ln(1 + 5x) ≈5x가되니까, 그비율은\n3x\n5x = 3\n5\n가되어동일한결과를얻을수있겠네.\n그래서두방법모두3\n5가나오니까정답은③번인것같다. 하지만혹시나계산과정에\n서실수한부분이있을까? 다시한번체크해보자. 로피탈법칙적용시, 분자와분모의\n도함수를제대로했는지:\nd\ndx ln(1 + 3x) =\n3\n1 + 3x,\nd\ndx ln(1 + 5x) =\n5\n1 + 5x\n그리고나눗셈을곱셈으로바꿀때부호나계수가틀린건없고, x →0일때의값은분\n모와분자의x 항이없어지므로3\n5가되는게맞지. 테일러전개의경우에도첫번째항만\n보면정확히3x\n5x = 3\n5. 그래서확실히3\n5가맞는것같아. 그럼문제의보기중에서③번이\n3\n5이므로정답은③번이다.\n**최종답변** (3) <|im_end|>\n<|im_start|>assistant\n극한limx→0\nln(1+3x)\nln(1+5x) 를구하기위해**로피탈법칙**을사용합니다.\n분자와분모모두x →0 일때0/0 형태의부정형이므로로피탈법칙을적용합니다:\nlim\nx→0\nln(1 + 3x)\nln(1 + 5x) = lim\nx→0\nd\ndx ln(1 + 3x)\nd\ndx ln(1 + 5x)\n각각미분하면:\nd\ndx ln(1 + 3x) =\n3\n1 + 3x,\nd\ndx ln(1 + 5x) =\n5\n1 + 5x\n따라서,\nlim\nx→0\n3\n1+3x\n5\n1+5x\n= lim\nx→0\n3\n1 + 3x · 1 + 5x\n5\nx →0 일때1 + 3x ≈1 및1 + 5x ≈1 이므로,\n3\n1 · 1\n5 = 3\n5\n또한테일러전개를사용하면ln(1 + ax) ≈ax (단, a 는상수)로근사할수있습니다:\nln(1 + 3x)\nln(1 + 5x) ≈3x\n5x = 3\n5\n따라서, 답은(3) 입니다.\n<|im_end|>\n48\n\nE\nSynthetic Data Examples\nFigure 10: Synthetic QA data generated from Wikipedia\nFigure 11: Translated sample example of synthetic QA data generated from Wikipedia\n49\n\nFigure 12: Synthetic educational data generated from course syllabus\nFigure 13: Translated sample example of synthetic educational data generated from course syllabus\n50\n"
    },
    {
      "arxiv_id": "https://arxiv.org/abs/2506.22403",
      "full_text": " [2506.22403] HyperCLOVA X THINK Technical Report Skip to main content We gratefully acknowledge support from the Simons Foundation, member institutions , and all contributors. Donate &gt; cs &gt; arXiv:2506.22403 Help | Advanced Search All fields Title Author Abstract Comments Journal reference ACM classification MSC classification Report number arXiv identifier DOI ORCID arXiv author ID Help pages Full text Search open search GO open navigation menu quick links Login Help Pages About --> Computer Science > Computation and Language arXiv:2506.22403 (cs) [Submitted on 27 Jun 2025 ( v1 ), last revised 1 Jul 2025 (this version, v2)] Title: HyperCLOVA X THINK Technical Report Authors: NAVER Cloud HyperCLOVA X Team View a PDF of the paper titled HyperCLOVA X THINK Technical Report, by NAVER Cloud HyperCLOVA X Team View PDF Abstract: We introduce HyperCLOVA X THINK, the first reasoning-focused large language model in the HyperCLOVA X family, pre-trained on roughly $6$ trillion high-quality Korean, and English tokens, augmented with targeted synthetic Korean data. It was implemented as a compute-memory-balanced Peri-LN Transformer scaled with $\\mu$P, pre-trained through a three-stage curriculum that expands the context window to $128$K tokens, and post-trained via supervised fine-tuning with Reinforcement Learning from Verifiable Rewards supports both detailed rationale and concise-answer modes. It delivers competitive performance against similarly sized models on Korea-focused benchmarks such as KMMLU, CSAT, KoBALT-700, HAERAE-1.0, and KoBigBench, while preserving robust bilingual consistency and translation quality. In addition, a vision-augmented variant matches or exceeds GPT-4.1 on the KCSAT STEM benchmark, all of which are achieved with substantially lower training compute than existing models of similar sizes. We also present a pruning and distillation technique that will soon be applied to HyperCLOVA X THINK for an open-source and business-friendly foundation model. Altogether, these capabilities position HyperCLOVA X THINK as a robust foundation for Korean AI innovation and a valuable resource for the global research community. Comments: 50 pages, 13 figures; fixed figures in the appendix Subjects: Computation and Language (cs.CL) ; Artificial Intelligence (cs.AI) Cite as: arXiv:2506.22403 [cs.CL] &nbsp; (or arXiv:2506.22403v2 [cs.CL] for this version) &nbsp; https://doi.org/10.48550/arXiv.2506.22403 Focus to learn more arXiv-issued DOI via DataCite Submission history From: Kang Min Yoo [ view email ] [v1] Fri, 27 Jun 2025 17:23:12 UTC (3,781 KB) [v2] Tue, 1 Jul 2025 13:39:25 UTC (3,781 KB) Full-text links: Access Paper: View a PDF of the paper titled HyperCLOVA X THINK Technical Report, by NAVER Cloud HyperCLOVA X Team View PDF TeX Source Other Formats view license Current browse context: cs.CL &lt;&nbsp;prev &nbsp; | &nbsp; next&nbsp;&gt; new | recent | 2025-06 Change to browse by: cs cs.AI References &amp; Citations NASA ADS Google Scholar Semantic Scholar a export BibTeX citation Loading... BibTeX formatted citation &times; loading... Data provided by: Bookmark Bibliographic Tools Bibliographic and Citation Tools Bibliographic Explorer Toggle Bibliographic Explorer ( What is the Explorer? ) Connected Papers Toggle Connected Papers ( What is Connected Papers? ) Litmaps Toggle Litmaps ( What is Litmaps? ) scite.ai Toggle scite Smart Citations ( What are Smart Citations? ) Code, Data, Media Code, Data and Media Associated with this Article alphaXiv Toggle alphaXiv ( What is alphaXiv? ) Links to Code Toggle CatalyzeX Code Finder for Papers ( What is CatalyzeX? ) DagsHub Toggle DagsHub ( What is DagsHub? ) GotitPub Toggle Gotit.pub ( What is GotitPub? ) Huggingface Toggle Hugging Face ( What is Huggingface? ) Links to Code Toggle Papers with Code ( What is Papers with Code? ) ScienceCast Toggle ScienceCast ( What is ScienceCast? ) Demos Demos Replicate Toggle Replicate ( What is Replicate? ) Spaces Toggle Hugging Face Spaces ( What is Spaces? ) Spaces Toggle TXYZ.AI ( What is TXYZ.AI? ) Related Papers Recommenders and Search Tools Link to Influence Flower Influence Flower ( What are Influence Flowers? ) Core recommender toggle CORE Recommender ( What is CORE? ) Author Venue Institution Topic About arXivLabs arXivLabs: experimental projects with community collaborators arXivLabs is a framework that allows collaborators to develop and share new arXiv features directly on our website. Both individuals and organizations that work with arXivLabs have embraced and accepted our values of openness, community, excellence, and user data privacy. arXiv is committed to these values and only works with partners that adhere to them. Have an idea for a project that will add value for arXiv's community? Learn more about arXivLabs . Which authors of this paper are endorsers? | Disable MathJax ( What is MathJax? ) About Help contact arXiv Click here to contact arXiv Contact subscribe to arXiv mailings Click here to subscribe Subscribe Copyright Privacy Policy Web Accessibility Assistance arXiv Operational Status Get status notifications via email or slack "
    }
  ]
}