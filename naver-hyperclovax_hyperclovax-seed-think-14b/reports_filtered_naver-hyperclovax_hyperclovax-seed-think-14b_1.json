{
  "1-1 (Weights)": "The available statements repeatedly emphasize that the full HyperCLOVA X THINK weights are not yet public, but that an open-source release is actively being prepared. Authors say that they \"plan to open-source a pruned and distilled version of HyperCLOVA X THINK\" and that such a model is \"under preparation to be open-sourced.\" They explain that the enabling technology is a \"pruning and distillation technique\" which they \"will soon [apply] to HyperCLOVA X THINK for an open-source and business-friendly foundation model.\"  As proof-of-concept, other family members have already been published on Hugging Face: \"HyperCLOVA X SEED 0.5B, recently released on HuggingFace, is the first open-source model in the HyperCLOVA X series,\" and the larger \"HyperCLOVA X SEED Vision Instruct 3B\" is likewise \"Available on Hugging Face Hub\" at https://huggingface.co/naver-hyperclovax/HyperCLOVAX-SEED-Vision-Instruct-3B.  Together, the quotes show: (1) no public checkpoint yet for THINK, (2) a concrete plan to publish a smaller pruned/distilled THINK variant, (3) precedent of earlier SEED checkpoints already downloadable from Hugging Face, demonstrating that the project uses that platform to host weights.",
  "1-2 (Code)": "Only one explicit statement addresses training code. The authors say they \"share a practical pruning-distillation pipeline and commit to apply it for an open-source version of THINK—fostering further research and commercial deployment, even under more resource-constrained settings.\"  This indicates that the code for the pruning/distillation stage of training (not merely inference) is intended to be published, but no files, repositories, or configuration specifics are yet named in the material provided.",
  "1-3 (License)": "Licensing details are still high-level and prospective. The same sentence that outlines release plans for the pruned THINK model also clarifies intent: \"We plan to open-source release this model under a business-friendly license.\"  The phrase \"business-friendly\" implies that commercial use will be permitted, but no formal license name, version, or clause text has been disclosed in the supplied excerpts.",
  "1-4 (Paper)": "The official technical report is already on arXiv: “arXiv:2506.22403v2 [cs.CL]  1 Jul 2025  HyperCLOVA X THINK.”  Throughout the text the authors describe the work: “We introduce HyperCLOVA X THINK, the first reasoning-focused large language model in the HyperCLOVA X family,\" highlighting that it is \"pre-trained on roughly 6 trillion high-quality Korean and English tokens, augmented with targeted synthetic Korean data.\"  They summarize that in this report they \"introduced HyperCLOVA X THINK, the first reasoning-focused LLM within [the] HyperCLOVA X family.\"  These quotes confirm both the existence of a dedicated paper and outline its scope—emphasizing reasoning objectives, massive bilingual token counts, and the model’s position as the flagship reasoning model in the HyperCLOVA X line.",
  "1-1 (Weights)__evidence": [
    {
      "source": "[pdf_text]",
      "quote": "Lastly, we present a pruning and distillation technique that will soon be applied to HyperCLOVA X THINK for an open-source and business-friendly foundation model."
    },
    {
      "source": "[pdf_text]",
      "quote": "This technique will soon be applied to THINK itself to produce a model suitable for limited resource settings. We plan to open-source release this model under a business-friendly license."
    },
    {
      "source": "[pdf_text]",
      "quote": "HyperCLOVA X SEED 0.5B, recently released on HuggingFace, is the first open-source model in the HyperCLOVA X series trained using pruning and knowledge distillation."
    },
    {
      "source": "[pdf_text]",
      "quote": "Lastly, we plan to open-source a pruned and distilled version of HyperCLOVA X THINK."
    },
    {
      "source": "[pdf_text]",
      "quote": "NAVER Cloud HyperCLOVA X Team. 2024. HyperCLOVA X SEED Vision Instruct 3B. https://huggingface.co/naver-hyperclovax/HyperCLOVAX-SEED-Vision-Instruct-3B. Available on Hugging Face Hub."
    },
    {
      "source": "[sections/https://arxiv.org/pdf/2506.22403]",
      "quote": "Lastly, we present a pruning and distillation technique that will soon be applied to HyperCLOVA X THINK for an open-source and business-friendly foundation model."
    },
    {
      "source": "[sections/https://arxiv.org/pdf/2506.22403]",
      "quote": "To ensure that academic and industry partners can benefit from the model, we introduce a pruning- and-distillation recipe that reduces parameter count while preserving accuracy. This technique will soon be applied to THINK itself to produce a model suitable for limited resource settings. We plan to open-source release this model under a business-friendly license."
    },
    {
      "source": "[pdf_text]",
      "quote": "As a real-world example, HyperCLOVA X SEED 0.5B, recently released on HuggingFace, is the first open-source model in the HyperCLOVA X series trained using pruning and knowledge distillation."
    },
    {
      "source": "[pdf_text]",
      "quote": "Currently, a pruned and distilled version of THINK is under preparation to be open-sourced."
    }
  ],
  "1-2 (Code)__evidence": [
    {
      "source": "[sections/https://arxiv.org/pdf/2506.22403]",
      "quote": "Third, we share a practical pruning-distillation pipeline and commit to apply it for an open-source version of THINK—fostering further research and commercial deployment, even under more resource-constrained settings."
    }
  ],
  "1-3 (License)__evidence": [
    {
      "source": "[pdf_text]",
      "quote": "This technique will soon be applied to THINK itself to produce a model suitable for limited resource settings. We plan to open-source release this model under a business-friendly license."
    },
    {
      "source": "[web:https://www.bigcode-project.org/docs/pages/model-license/]",
      "quote": "https://www.bigcode-project.org/docs/pages/model-license/"
    },
    {
      "source": "[web:https://huggingface.co/tiiuae/falcon-40b/blob/main/LICENSE]",
      "quote": "https://huggingface.co/tiiuae/falcon-40b/blob/main/LICENSE"
    }
  ],
  "1-4 (Paper)__evidence": [
    {
      "source": "[pdf_text]",
      "quote": "arXiv:2506.22403v2  [cs.CL]  1 Jul 2025\nHyperCLOVA X THINK"
    },
    {
      "source": "[pdf_text]",
      "quote": "We introduce HyperCLOVA X THINK, the first reasoning-focused large language model in the HyperCLOVA X family, pre-trained on roughly 6 trillion high-quality Korean, and English tokens, augmented with targeted synthetic Korean data."
    },
    {
      "source": "[sections/https://arxiv.org/pdf/2506.22403]",
      "quote": "We introduce HyperCLOVA X THINK, the first reasoning-focused large language model in the HyperCLOVA X family, pre-trained on roughly 6 trillion high-quality Korean, and English tokens, augmented with targeted synthetic Korean data."
    },
    {
      "source": "[pdf_text]",
      "quote": "In this report, we introduced HyperCLOVA X THINK, the first reasoning-focused LLM within HyperCLOVA X family."
    }
  ]
}