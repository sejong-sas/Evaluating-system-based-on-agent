{
  "1-5 (Architecture)": "HyperCLOVA X THINK is a 14-billion-parameter, reasoning-focused large language model. It is pre-trained on roughly 6 trillion high-quality Korean and English tokens, plus targeted synthetic Korean data. The core network is a compute-and-memory-balanced Peri-LN Transformer that has been scaled with μP. Training follows a three-stage curriculum that progressively enlarges the context window until the model can process up to 128 K tokens. After pre-training, the model undergoes supervised fine-tuning and reinforcement learning from verifiable rewards, producing two operational modes inside a single model: a detailed \"reasoning\" mode for multi-step problems and a concise \"non-reasoning\" mode for rapid answers. The adoption of Peri-LN as the default normalization scheme is highlighted as delivering greater stability and performance without added cost. Overall, the architecture is designed for high efficiency while supporting dynamic switching between the two answer styles.",
  "1-6 (Tokenizer)": "",
  "2-1 (Hardware)": "All quoted material points to training being carried out on a \"large-scale GPU cluster.\" The authors note that, because of the model’s high-efficiency architecture and data strategy, HyperCLOVA X THINK required \"significantly fewer GPU hours than similar sized models\"; however, no exact GPU model counts or compute totals are disclosed.",
  "2-2 (Software)": "The training stack for HyperCLOVA X THINK includes a multi-step post-training pipeline executed on the GPU cluster. The cited stages are: Supervised Fine-Tuning (SFT), Reward Modeling (RM), Reinforcement Learning with Verifiable Rewards (RLVR), specialized training for reasoning Length Controllability (LC), and Reinforcement Learning from Human Feedback (RLHF). LC is implemented by incorporating the length-penalized reward functions proposed by Aggarwal and Welleck (2025). No further details about underlying frameworks or distributed-training libraries are provided.",
  "1-5 (Architecture)__evidence": [
    {
      "source": "[abstract]",
      "quote": "We introduce HyperCLOVA X THINK, the first reasoning-focused large language model in the HyperCLOVA X family, pre-trained on roughly 6 trillion high-quality Korean, and English tokens, augmented with targeted synthetic Korean data. It was implemented as a compute-memory-balanced Peri-LN Transformer scaled with µP, pre-trained through a three-stage curriculum that expands the context window to 128K tokens, and post-trained via supervised fine-tuning with Reinforcement Learning from Verifiable Rewards supports both detailed rationale and concise-answer modes."
    },
    {
      "source": "[sections/Model Architecture]",
      "quote": "These findings confirm that Peri-LN delivers superior stability and performance without incurring additional computational cost, and thus we adopt it as the default normalization scheme in the THINK architecture."
    },
    {
      "source": "[pdf_text]",
      "quote": "THINK is trained to operate in an integrated manner, allowing for dynamic switching between a detailed ‘reasoning mode’ for complex, multi-step reasoning and a more direct ‘non-reasoning mode’ for rapid, context-driven responses. This unified framework eliminates the need for users to switch between separate models (e.g., a dedicated reasoning model and a chatbot), as illustrated in Table 2."
    },
    {
      "source": "[pdf_text]",
      "quote": "THINK is developed with a focus on creating a high-efficiency architecture and a training strategy grounded in high-quality data."
    },
    {
      "source": "[sections/Performance on Math & Coding Benchmarks]",
      "quote": "THINK (14B)"
    }
  ],
  "1-6 (Tokenizer)__evidence": [],
  "2-1 (Hardware)__evidence": [
    {
      "source": "[sections/Post-Training Figure 3 caption]",
      "quote": "Figure 3: Post-training pipeline of HyperCLOVA X THINK. … (2) Training Phase: A sequence of fine-tuning procedures—including Supervised Fine-Tuning (SFT), Reward Modeling (RM), Reinforcement Learning with Verifiable Rewards (RLVR), training for reasoning Length Controllability (LC), and Reinforcement Learning from Human Feedback (RLHF) —is executed across a large-scale GPU cluster."
    },
    {
      "source": "[figure3_caption]",
      "quote": "Figure 3: Post-training pipeline of HyperCLOVA X THINK. (1) Data-Preparation Phase: ... (2) Training Phase: A sequence of fine-tuning procedures—including Supervised Fine-Tuning (SFT), Reward Modeling (RM), Reinforcement Learning with Verifiable Rewards (RLVR), training for reasoning Length Controllability (LC), and Reinforcement Learning from Human Feedback (RLHF) —is executed across a large-scale GPU cluster."
    },
    {
      "source": "[pdf_text]",
      "quote": "THINK is developed with a focus on creating a high-efficiency architecture and a training strategy grounded in high-quality data. As a result, it required significantly fewer GPU hours than similar sized models to be trained, as shown in Figure 6."
    }
  ],
  "2-2 (Software)__evidence": [
    {
      "source": "[figure3_caption]",
      "quote": "Figure 3: Post-training pipeline of HyperCLOVA X THINK. (1) Data-Preparation Phase: ... (2) Training Phase: A sequence of fine-tuning procedures—including Supervised Fine-Tuning (SFT), Reward Modeling (RM), Reinforcement Learning with Verifiable Rewards (RLVR), training for reasoning Length Controllability (LC), and Reinforcement Learning from Human Feedback (RLHF) —is executed across a large-scale GPU cluster."
    },
    {
      "source": "[pdf_text]",
      "quote": "To induce LC in HyperCLOVA X THINK, we additionally incorporate the length-penalized reward functions introduced by Aggarwal and Welleck, 2025."
    }
  ]
}