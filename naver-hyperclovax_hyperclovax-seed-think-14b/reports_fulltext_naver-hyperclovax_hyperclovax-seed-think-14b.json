{
  "model_id": "naver-hyperclovax/hyperclovax-seed-think-14b",
  "full_texts": [
    {
      "arxiv_id": "https://arxiv.org/pdf/2506.22403",
      "full_text": "arXiv:2506.22403v2  [cs.CL]  1 Jul 2025\nHyperCLOVA X THINK\nNAVER Cloud\nHyperCLOVA X Team\nAbstract\nWe introduce HyperCLOVA X THINK, the first reasoning-focused large language\nmodel in the HyperCLOVA X family, pre-trained on roughly 6 trillion high-quality\nKorean, and English tokens, augmented with targeted synthetic Korean data. It\nwas implemented as a compute-memory-balanced Peri-LN Transformer scaled\nwith µP, pre-trained through a three-stage curriculum that expands the context\nwindow to 128K tokens, and post-trained via supervised fine-tuning with Rein-\nforcement Learning from Verifiable Rewards supports both detailed rationale and\nconcise-answer modes. It delivers competitive performance against similarly sized\nmodels on Korea-focused benchmarks such as KMMLU, CSAT, KoBALT-700,\nHAERAE-1.0, and KoBigBench, while preserving robust bilingual consistency\nand translation quality. In addition, a vision-augmented variant matches or ex-\nceeds GPT-4.1 on the KCSAT STEM benchmark, all of which are achieved with\nsubstantially lower training compute than existing models of similar sizes. These\ncapabilities position HyperCLOVA X THINK as a robust foundation for Korean\nAI innovation and a valuable resource for the global research community. Lastly,\nwe present a pruning and distillation technique that will soon be applied to Hyper-\nCLOVA X THINK for an open-source and business-friendly foundation model.\n1\nIntroduction\nRecent advancements of large language models (LLMs) have drawn increased attention to their rea-\nsoning abilities, going beyond simple memorization of factual knowledge to deriving logical conclu-\nsions. Models like GPT-o1 (OpenAI et al., 2024b), R1 (DeepSeek-AI et al., 2025), and QwQ (Qwen\nTeam, 2025) exemplify such effort, demonstrating that the ability to perform logical inferences and\nmulti-step problem solving can significantly broaden the scope of AI applications.\nAt the same time, the notion of sovereign AI is being established as an important goal. As LLMs\ncontinue to be deployed in various regions around the globe, there is a growing need for linguistic\nfluency and cultural sensitivity tailored toward a given region, as well as data governance that aligns\nwith regional values and regulations. In this regard, our immediate focus is Korea.\nTo meet the imperatives of both advanced reasoning and sovereign AI—for Korea, in particular—we\npresent HyperCLOVA X THINK (henceforth THINK). It is the first reasoning-focused LLM in the\nHyperCLOVA X family(Yoo et al., 2024b), trained via a strategic preparation of training data and\nuse of the latest pre- and post-training techniques.\nIn particular, we curated a corpus of roughly six trillion tokens that balances high-quality Korean\nand English text with targeted synthetic Korean data. This mixture improves linguistic breadth while\nsafeguarding cultural and domain relevance. The model architecture follows a compute-memory-\nbalanced Peri-LN Transformer scaled with the µP framework, allowing consistent hyperparameter\ntransfer from small to large scales without extensive grid search.\nDuring pre-training, A three-stage curriculum gradually increases the context window, culminating\nin 128k tokens, which enables THINK to process long documents and perform multi-step reason-\ning within a single pass. Then, for post-training, we combine supervised fine-tuning on carefully\n\ndesigned reasoning tasks with Reinforcement Learning from Verifiable Rewards. This alignment\nstrategy encourages the model to generate explicit chains of thought when requested and concise\nanswers when brevity is preferred. Safety alignment follows NAVER AI Ethics guidelines through\nfiltered data, red-teaming, refusal sampling, and policy tuning.\nWe evaluate THINK on Korea-focused benchmarks such as KMMLU, CSAT, KoBALT-700,\nHAERAE-1.0, and KoBigBench. The model achieves competitive accuracy among similarly sized\nmodels while requiring substantially lower training compute. A vision-augmented variant that in-\ntegrates vision encoders to extend the same reasoning framework to image-text tasks, matches or\nsurpasses GPT-4.1 on the KCSAT STEM benchmark.\nTo ensure that academic and industry partners can benefit from the model, we introduce a pruning-\nand-distillation recipe that reduces parameter count while preserving accuracy. This technique will\nsoon be applied to THINK itself to produce a model suitable for limited resource settings. We plan\nto open-source release this model under a business-friendly license.\nOur contributions are threefold. First, we demonstrate that a regionally tailored corpus combined\nwith modern scaling laws yields a bilingual model with strong reasoning capability. Second, we\nprovide an efficient training and alignment recipe that lowers the barrier to entry for sovereign AI\ndevelopment. Third, we share a practical pruning-distillation pipeline and commit to apply it for an\nopen-source version of THINK—fostering further research and commercial deployment, even under\nmore resource-constrained settings.\n2\nPre-Training\nThis section outlines the pre-training methodology behind THINK: a scalable, Korean-centric data\npipeline enriched with targeted synthetic corpora (Section 2.1); a compute–memory-efficient yet\nstability-oriented Transformer, instantiated with scale-invariant parameterization principles (Sec-\ntion 2.2); and a three-stage curriculum that sequentially builds foundational linguistic knowledge,\nrefines competence with higher-fidelity data, and expands contextual capacity to support long-form\nreasoning (Section 2.3). See Figure 1 for an overview of the pre-training process.\n2.1\nData Preparation\nWe begin with the end-to-end data pipeline—collection, cleaning, and quality filtering—paying spe-\ncial attention to techniques tailored for our large-scale Korean corpus. We then describe a synthetic-\ndata generation strategy that enriches under-represented domains while preserving linguistic fidelity.\nData Pipeline. The data pipeline for THINK is designed around three guiding principles: scalability,\nreusability, and quick refresh, so that new corpora can be incorporated with minimal latency while\nmaintaining strict quality guarantees. Following Weber et al. (2024a), the pipeline separates schema\nstandardization from quality assessment and filtering. During standardization, raw documents in\nheterogeneous formats undergo lightweight cleansing, canonicalization of field names, and storage\nin a unified schema. The subsequent annotation stage attaches quantitative quality signals, includ-\ning structural and linguistic metrics, and applies masking to all personally identifiable information\n(PII). The filtering stage then materializes stage-specific corpora by applying threshold rules to the\nannotated data and serializes the result into shard files optimized for streaming.\nData Filtering. Korean-specific data filtering schemes have been largely underexplored from the\nliterature. To obtain a corpus that is simultaneously broad and reliably high-quality, we devise a\ntwo-tier filtering framework tailored to the linguistic and typographic characteristics of Korean.\nThe first tier extends the rule sets of Weber et al. (2024b) and Lozhkov et al. (2024) by redesign-\ning every heuristic for Korean morphology. Among various quantitative signals, five representative\nexamples—symbol-to-word ratio, mean word length, sentence count, masked-PII ratio, and the pro-\nportion of normalized to raw length—are computed for each document. Target ranges for these sig-\nnals are established through manual inspection with an internal reviewer, and thresholds are further\nadapted to each source domain (e.g., blogs, wikis) to suppress noise while preserving recall.\nThe second tier employs model-based scoring. FastText (Joulin et al., 2017, 2016) and transformer\nencoders are trained under two supervision regimes. In the binary regime, wiki-like passages con-\nstitute positive examples whereas noisy web pages form the negative class; the posterior probability\n2\n\nFigure 1: Pre-training pipeline of HyperCLOVA X THINK. (1) Data-Preparation Phase: A scal-\nable pipeline collects raw corpora, carries out cleansing, language identification, deduplication, and\nmasking; attaches quantitative quality signals, applies filtering, synthesizes targeted data, and seri-\nalizes the resulting shards (2) Training Phase: A dedicated three-stage curriculum, with each stage\noptimized for its specific objective, progressively builds and refines the model’s capabilities.\nfurnishes a continuous quality score. In the ordinal regime, a language model assigns 0–5 ratings for\neducational utility, informativeness, and narrative coherence, producing “wiki-like”, “educational”,\nand “explanatory” quality predictors analogous to GPT-3, FineWeb-edu, and DCLM filters (Brown\net al., 2020; Penedo et al., 2024; Li et al., 2024). A document is retained only if it satisfies a stage-\nspecific conjunction of heuristic thresholds and model scores. Near-duplicates are removed with a\nMinHash index that is rebuilt at every refresh.\nTable 1 summarizes the document-level yield rates of sub-sampled data achieved by the two-tier\npipeline. Even within this modest slice, the first stage discards roughly 90 % of raw pages overall,\nwhile the more selective second stage retains just 1 – 20 %. These figures reveal aggressive corpus\ncompression, with the pipeline condensing the raw crawl by roughly one to two orders of magnitude\neven on the sub-sampled slice.\nSynthetic Data Generation. In contrast to the extensive curated resources available for ma-\njor languages (e.g., English and Chinese), high-quality Korean corpus remains markedly under-\nrepresented. To redress this asymmetry, we initiate a systematic program of high-fidelity synthetic\ndata generation, focusing on domains—such as education, law, historical facts, and cultural sen-\ntiment—where native Korean content is especially sparse (Yuan et al., 2023; Lee et al., 2024).\nLeveraging our in-house model family, the pipeline follows two complementary tracks, rewriting\nexisting documents and generating new text from curated seed prompts, while placing filtering and\nverification at the core of the process to ensure that only high-fidelity Korean data is retained.\nThe synthetic-data workflow comprises four coupled phases (Cheng et al., 2024; Li et al., 2023;\nBen Allal et al., 2024; Su et al., 2024a). (1) Data-design phase: We draft a specification that fixes the\ntarget domain, desired volume, file format, and downstream use case. This document governs every\nsubsequent decision in the pipeline. (2) Seed-acquisition and generation phase: License-compliant\nseed material is collected from open-source and internal repositories. These seeds are either para-\nphrased to remove copyright artifacts or expanded into new passages through prompt-based genera-\n3\n\nData\nStage 1 Yield (Filtered / Raw)\nStage 2 Yield (Filtered / Raw)\nTotal\n9.59%\n1.36%\nBlog\n57.74%\n19.84%\nCafe\n31.53%\n2.35%\nWeb\n4.49%\n0.27%\nTable 1: Stage-wise document yield rates after two-tier filtering.\ntion with our in-house language-model family. (3) Filtering and refinement phase: The resulting text\nis processed by the same two-tier filtering stack used for web data, augmented by routines that detect\nrepetitive templates, logical inconsistencies, and machine-like phrasing. (4) Integration phase: Only\ndata that satisfy all quality checks are versioned and merged into the pre-training corpus, ensuring\nthat synthetic examples extend coverage without degrading overall corpus fidelity. We provide illus-\ntrative synthetic data examples in Appendix E. These synthetic corpora are injected into both Stage\n1 and Stage 2 of the pre-training curriculum.\n2.2\nModel Architecture\nOn the architectural front, our design integrates three key components—(i) a compute–memory-\nbalanced Transformer layout (Hoffmann et al., 2022; Rivière et al., 2024), (ii) Peri-Layer Normal-\nization (Peri-LN, Kim et al. (2025)) for training stability and performance, and (iii) Maximal Up-\ndate Parametrization (µP, Yang and Hu (2021); Yang et al. (2024)) for scale-robust hyper-parameter\ntransfer—together enabling stable scaling and cost-efficient training.\nCompute–Memory Balanced Architecture. To minimize compute-bound training cost and\nmemory-bound inference latency under a fixed parameter budget, we employ a shallower-but-wider\nTransformer configuration Hoffmann et al. (2022); Rivière et al. (2024). The model reduces the\nnumber of blocks and reallocates the freed parameters to larger hidden and feed-forward dimen-\nsions. Because each self-attention layer incurs O(L2) FLOPs and O(L) activation memory with\nrespect to sequence length L, lowering depth proportionally decreases attention overhead, while\nwidening the FFN, whose cost grows linearly in L, maintains representational capacity.\nTo empirically substantiate this design, we start from a 3B-parameter baseline comprising 26 Trans-\nformer blocks with an FFN hidden size of 7, 168 and generate a shallow-but-wide variant by re-\nducing the depth to 18 layers (30 % shallower) while proportionally increasing the FFN hidden\ndimension to 11, 264 (57 % wider), thereby conserving the total parameter budget. Owing to the\nquadratic attention cost, this reallocation lowers the theoretical compute for an 8K-token sequence\nby 13.7 % TFLOPs. Consistently with this analysis, the modified model ingested 15 % more train-\ning tokens within an identical wall-clock budget and matched the validation perplexity of the deeper\ncontrol, confirming that width-centric capacity reallocation preserves modeling quality while con-\nferring tangible hardware efficiency.\nStability-Oriented Transformer. We stabilize scale-up by coupling Maximal Update Parametriza-\ntion (µP) with a Peri-Layer-Normalized Transformer. Following the µTransfer procedure, we sweep\nlearning-rate and regularization only on small proxy models, then zero-shot port the optimal settings\nto each production scale. Because µP preserves update magnitudes across configurations, the large\nmodels inherit well-conditioned gradient norms without further tuning, greatly reducing exploration\ncost while keeping feature learning intact (Yang and Hu, 2021; Yang et al., 2024).\nPeri-Layer Normalization (Peri-LN) normalizes both the input and output of every Transformer sub-\nlayer, bounding hidden-state variance to grow at most linearly with depth and that layer-wise gra-\ndient norms remain stable throughout training.By tightly bounding hidden state statistics, Peri-LN\nsuppresses the massive activations typically observed in Pre-LN models (Sun et al., 2024). Peri-\nLN also removes the need for FLOP-intensive ablation studies to stabilize architectural or training\nhyper-parameters. Empirically, Peri-LN yields lower pre-training loss and smaller run-to-run vari-\nance (Kim et al., 2025). Maximal Update Parametrization (µP) complements Peri-LN by preserving\noptimization statistics across width and depth, so hyper-parameters tuned on sub-billion-parameter\n4\n\n(a) Training loss\n(b) Gradient-norm\nFigure 2: Performance comparison between 8 B-parameter Pre-LN and Peri-LN Transformers dur-\ning pre-training. Each model size excludes the embedding parameters.\nproxies transfer reliably to multi-billion-parameter instances. Together, Peri-LN and µP provide a\nprincipled, cost-effective pathway to stable scaling.\nTo evaluate normalization choices at production scale, we trained two Llama-style models (Dubey\net al., 2024) with 8 B parameters on the same open-corpus dataset (Su et al., 2024a), along with our\nin-house version of the TikToken tokenizer1: a standard Pre-LN (Xiong et al., 2020) baseline and\nan otherwise identical Peri-LN variant. As illustrated in Figure 2, the Peri-LN model exhibits fewer\ngradient and loss spikes than its Pre-LN counterpart, reproducing the large-scale stability benefits\nreported by Kim et al. (2025). Furthermore, the Peri-LN configuration attains, on average, a 15 %\nlower training loss within the same wall-clock budget. These findings confirm that Peri-LN delivers\nsuperior stability and performance without incurring additional computational cost, and thus we\nadopt it as the default normalization scheme in the THINK architecture.\n2.3\nPre-Training Curriculum\nWe adopt a three-staged pre-training curriculum, with each phase focused on a distinct capability tar-\nget (OLMo et al., 2025; Hu et al., 2024). Stage 1 establishes a general-purpose foundational knowl-\nedge base. Stage 2 refines domain-specialized competence by continuing training on high-quality\ncorpora. Stage 3 extends the context window to 128K tokens and internalizes long chain-of-thought\nreasoning by fine-tuning on rejection-sampled traces generated from an in-house model family. The\nstaged curriculum strategically allocates computational FLOPs to phases with the highest marginal\nutility, optimizing cost-efficiency while maximizing incremental performance gains.\nStage 1: Foundational Knowledge Construction. The first training stage establishes a broad\nknowledge base spanning multiple domains. We curate a multilingual corpus, principally Korean\nand English. Training proceeds on sequences up to 8K tokens, consuming 6 trillion tokens in total.\nThe learning rate is linearly increased during the initial 5, 000 steps to a peak of 1.59e-3 determined\nby µP scaling, after which it is annealed according to a cosine schedule to 1.59e-4 (10 % of the\nmaximum), thereby promoting stable convergence.\nStage 2: Domain-Specialized Capability Boosting. The mid-training stage introduces an additional\n1 trillion tokens to sharpen the model’s domain expertise and reasoning ability while maintaining\nthe 8K-token context length established in Stage 1. We gradually down-weight generic web text\nand increase high-quality, domain-focused corpora including the synthetic datasets constructed in\nSection 2.1. A brief 2, 000-step warm-up ensures a smooth transition to these revised distribution.\nGuided by Bi et al. (2024), for learning rate schedule, we adopt a two-step decay profile: the rate is\nheld at 1.59e-4 for 80 % of training, reduced to 31.6 % of this peak (≈4.76e-5) for the next 10 %,\nand finally to 10 % (≈1.59e-5) for the last 10 %. For the data mix, following Blakeney et al. (2024),\nwe rebalancing the dataset during the final 10 % of training steps. Sampling of lower-quality general\ntext is gradually reduced. Conversely, the sampling weight of under-represented domains, crucial\n1https://github.com/openai/tiktoken\n5\n\nFigure 3: Post-training pipeline of HyperCLOVA X THINK. (1) Data-Preparation Phase: Data is col-\nlected and then rigorously processed through steps such as format validation, automatic verification,\nlanguage-based filtering, and evaluation by LLM-based judges. The data is refined through qual-\nity filtering, difficulty filtering, and ranking to prepare data suitable for subsequent training stages\nwith different objectives. (2) Training Phase: A sequence of fine-tuning procedures—including Su-\npervised Fine-Tuning (SFT), Reward Modeling (RM), Reinforcement Learning with Verifiable Re-\nwards (RLVR), training for reasoning Length Controllability (LC), and Reinforcement Learning\nfrom Human Feedback (RLHF) —is executed across a large-scale GPU cluster.\nfor sovereign-AI applications, is increased, with emphasis on Korean medical literature, national\neconomic reports, and culturally contextualized historical archives.\nStage 3: Extended Context Alignment. Standard corpora are biased toward short documents;\nnaively over-sampling longer texts therefore disrupts training stability (Zhuang et al., 2025). We mit-\nigate this issue with length-based, proportion-preserving resampling, which increases the number of\nlong documents while maintaining each length bucket’s share of total tokens. After pre-training with\nan 8 K context window and a rotary-position-embedding base θ of 500 K, we expand the window in\nthree successive stages—32 K, 64 K, and 128 K. At each expansion, θ is raised from 500 K to 5 M,\nthen to 20 M, and finally to 100 M. A brief warm-up followed by cosine decay restores perplexity\nbefore the next enlargement (Su et al., 2024b; Xu et al., 2024). To supply explicit supervision for\nextended reasoning, we additionally train on a long chain-of-thought corpus generated in-house and\nfiltered via rejection sampling (Yuan et al., 2023; Lee et al., 2024) (see §2.1). This synthetic dataset\nspans up to 128 K tokens, enabling the model to master long-context conditioning without degrading\nthe general or domain-specific competencies obtained in Stages 1 and 2.\n3\nPost-Training\nThis section outlines the post-training methodology of THINK: a supervised fine-tuning (SFT) phase\nthat injects core reasoning patterns and task-specific capabilities (Section 3.1); and a multi-stage re-\ninforcement learning pipeline that incorporates verifiable rewards, length-controllability, and human\nfeedback to achieve aligned, efficient, and scalable reasoning (Sections 3.2–3.4). See Figure 3 for\nan overview of the training process.\n6\n\nReasoning Mode\nNon-Reasoning Mode\n<|im_start|>user\n{query}<|im_end|>\n<|im_start|>assistant/think\n{reasoning}<|im_end|>\n<|im_start|>assistant\n{response}<|im_end|><|endofturn|>\n<|im_start|>user\n{query}<|im_end|>\n<|im_start|>assistant\n{response}<|im_end|><|endofturn|>\nTable 2: Unified chat template used for training models to support both reasoning and non-reasoning\ninteraction modes.\nReasoning\n39.8%\nNon-reasoning\n60.2%\nReasoning vs Non-reasoning\nCode\n29.9%\nGeneral\n1.5%\nMath\n39.4%\nSTEM\n29.2%\nReasoning Category Distribution\nFigure 4: Data distribution utilized for Supervised Fine-Tuning (SFT), reflecting a balanced compo-\nsition tailored to support effective downstream reinforcement learning and reasoning capabilities.\nTHINK is trained to operate in an integrated manner, allowing for dynamic switching between a de-\ntailed ‘reasoning mode’ for complex, multi-step reasoning and a more direct ‘non-reasoning mode’\nfor rapid, context-driven responses. This unified framework eliminates the need for users to switch\nbetween separate models (e.g., a dedicated reasoning model and a chatbot), as illustrated in Table 2.\n3.1\nSupervised Fine-Tuning (SFT)\nSupervised Fine-Tuning (SFT) serves as a foundational step in our post-training pipeline, aiming to\ninject desired behaviors and reasoning patterns into the model. This stage establishes a strong base\nfor subsequent reinforcement learning phases.\nThe dataset used for SFT is constructed by aggregating various sources across mathematics, coding,\nSTEM, and general abilities. We carefully curate data from a series of ablation studies and utilize\nhigh-quality open-source and in-house data. For reasoning data, each sample contains prompt, assis-\ntant think, and assistant response. The assistant think contains a rather free-form chain-of-reasoning,\nwhile the assistant response is a concise, finalized output that directly answers the user’s query based\non that reasoning. The general statistics for the SFT dataset is illustrated in Figure 4.\nTo ensure data quality and consistency, we apply a multi-stage filtering pipeline across all datasets.\nEach item in data goes through a basic format check to ensure that the output contains proper format\n(e.g., boxed answers for math problems and compilability for code problems). Language filtering is\napplied to select only samples written in the target language, and language matching further ensures\nthat input and output languages are the same for each sample. For reasoning data specifically, we\nalso check whether the final answer is automatically verifiable. For non-reasoning data, we employ\na LLM-as-a-Judge method to score each example by their helpfulness and safety and filter out those\nwith low scores.\nTraining is performed with dynamic batching to fill each batch dynamically to its maximum capa-\nbility, in order to optimize GPU utilization and memory usage. The model is trained over 4 epochs\n7\n\nwith early stopping based on validation accuracy. Similarly to other reports (Yang et al., 2025), we\nobserve that selecting a checkpoint from later epochs results in reduced exploration of the model\nduring the subsequent phase. More comprehensive details on the training setup of SFT are provided\nin our previous technical report (Yoo et al., 2024b).\n3.2\nReinforcement Learning with Verifiable Rewards (RLVR)\nThe Reinforcement Learning with Verifiable Rewards (RLVR) is crucial for improving reasoning\ncapabilities through verifiable feedback mechanisms. The main objective is to optimize model per-\nformance by accurately guiding behavior through precise rewards and penalties.\nReinforcement Learning Algorithm. In our implementation of RLVR, we adopt Group Relative\nPolicy Optimization (GRPO) (Shao et al., 2024).\nUnlike more traditional RL algorithms, it calculates a baseline advantage based on multiple gener-\nations per prompt, optimizing computational efficiency and maintaining training effectiveness. To\nfurther enhance the robustness and accuracy of our RLVR framework, we introduce several targeted\nmodifications:\n• KL Divergence Penalty Removal: Our initial experiments indicated that this penalty re-\nstricts models from exploring diverse behaviors and incurs significant computational over-\nhead due to the necessity of inference from a reference model. Removing the penalty im-\nproved computational efficiency and model flexibility.\n• Constant Normalization: We observed that prompt difficulty often correlates with re-\nsponse length–more difficult prompts tend to produce longer responses–thereby introduc-\ning biases related to response length. To mitigate these biases from varying response lengths\nand prompt difficulties, we adopt constant normalization strategy from Liu et al. (2025).\n• Relaxed Upper Bound for Exploration: To encourage exploration and prevent determin-\nistic policy collapse, we adopt the clip-higher approach (Yu et al., 2025), which raises the\nupper threshold of the importance sampling ratio in GRPO. By including low-probability\ntokens into policy updates, this approach increases policy entropy and fosters diverse rea-\nsoning paths.\nCollectively, these methodological enhancements enable our RL training to achieve an optimal bal-\nance of exploration, computational efficiency, and stable training performance.\nData Efficiency. To optimize training efficiency, enhance model performance, and effectively utilize\ncomputational resources, we employ targeted difficulty filtering techniques, including both offline\nand online methods.\nWe implement offline difficulty filtering to our dataset by excluding prompts that are either too easy\nor too challenging. Specifically, we leverage predictions generated by the SFT checkpoint—our\ninitial model for RLVR—to evaluate the difficulty of each prompt. By sampling multiple responses\nfrom this checkpoint, we calculate the average accuracy of predictions and remove prompts with\naccuracy of exactly 0.0 or 1.0. This strategy ensures the inclusion of prompts only with appropriate\ndifficulty levels at the outset of training.\nHowever, offline difficulty filtering has limitations. Because this filtering method occurs only once\nbefore the training begins, it is inherently static. As the model’s performance improves as the training\nprogresses, the dataset’s difficulty level cannot be adjusted accordingly–a problem that was once\nchallenging can become solvable. Consequently, this static nature can lead to discrepancies between\nevolving model capabilities and fixed difficulty of prompts.\nTo address the shortcomings of offline filtering, we additionally incorporate an online difficulty fil-\ntering strategy. Utilizing GRPO allows us to generate multiple responses per prompt within each\nbatch. For each group, we calculate accuracy and remove prompts where all generated responses are\neither entirely correct or entirely incorrect from the batch. This dynamic filtering approach continu-\nously adapts the training set’s difficulty to the model’s evolving capabilities, ensuring that learning\nremains focused on informative examples and thereby maintaining optimal training efficiency.\nOur analysis aligns with recent findings suggesting that online difficulty filtering effectively opti-\nmizes the lower bound learnability of reinforcement learning algorithms by dynamically balancing\n8\n\nprompt difficulty (Bae et al., 2025). Importantly, we observe that even with initial offline filtering,\nonline filtering still provides substantial additional benefits. Thus, combining both offline and online\ndifficulty filtering significantly enhances our training efficiency and model performance.\nReward Shaping. To effectively guide model training and enhance its performance in our RLVR\nframework, we carefully design a reward shaping strategy consisting of several distinct components:\n• Format Reward: We establish a set of format rules that responses must follow. To calculate\nthis reward, we count the number of rules adhered to by the model’s response and divide it\nby the total number of format rules. We adopt this soft penalty approach as it demonstrates\nminimal negative impact on reasoning performance, allowing models to progressively align\nwith the desired response structure without detrimental effects.\n• Language Reward: This reward is computed based on the ratio of characters generated in\nthe same language as the prompt. By directly correlating the language of responses with the\nlanguage of prompts, this reward encourages the model to reason in the intended language,\nsignificantly enhancing multilingual reasoning capabilities.\n• Verifiable Reward: We incorporate verifiable rewards across multiple problem categories,\nincluding mathematics, code generation, code input-output (Code IO), and multiple-choice\nquestions. The verification outcomes directly determine reward allocation, with a binary\nvalue: a fully correct response receives a reward of 1.0, while any incorrect response results\nin a reward of 0.0.\n• Overlong Reward: We adopt both Soft Overlong Penalty and Overlong Loss Masking\n(Yu et al., 2025), because penalizing truncated samples harshly can introduce undesirable\nreward noise, potentially destabilizing training by penalizing valid reasoning solely due\nto length. The former gradually increases as the response length exceeds the predefined\nmaximum value, and the latter masks the loss of truncated samples, effectively stabilizing\nthe training process.\nOptimized Rollout Sampling Process. Efficiency in the rollout sampling process is crucial for op-\ntimizing the RLVR training pipeline, as this stage typically dominates the overall training duration.\nTo address this, we implement a highly efficient asynchronous sampling procedure. In this setup,\ninference nodes are utilized continuously and concurrently until the number of completed rollout\nsamples meets or exceeds the training batch size. Samples generated from these inference nodes\nare collected and stacked asynchronously, significantly reducing idle times and improving resource\nutilization.\nMoreover, due to our implementation of online difficulty filtering, certain samples may be dynam-\nically filtered out during the rollout process, potentially causing delays or inefficiencies. To coun-\nteract this, we maintain a buffered approach to concurrent sampling, ensuring multiple samples\nare processed simultaneously. This strategy effectively compensates for any filtered-out examples\nby ensuring continuous generation of alternative samples, thereby minimizing or entirely masking\nthe time loss associated with discarded examples. This optimized asynchronous sampling approach\ngreatly enhances the efficiency and stability of the RLVR training process (Bae et al., 2025).\n3.3\nReasoning Length Controllability (LC)\nReinforcement learning with Large Reasoning Models (LRMs) enables drastic improvements in\ncomplex reasoning capabilities, but often accompanies undesired tendencies to overthink (Chen\net al., 2024; Sui et al., 2025) or even underthink (Wang et al., 2025) compared to the optimal reason-\ning length. For practical and flexible deployment of computationally expensive LRMs, we identify\nlength controllability (LC) as a key desideratum. To induce LC in HyperCLOVA X THINK, we ad-\nditionally incorporate the length-penalized reward functions introduced by Aggarwal and Welleck,\n2025.\nOn top of the training configurations from the previous RLVR stage, we train our model on the\nlength-penalized reward functions (L1-Exact and L1-Max) from Aggarwal and Welleck, 2025. We\nappend ‘Think for maximum N tokens’ on the input instructions, where we sample N from\na discrete token budget set of B = {1024, 2048, 4096, 8192, 16384} to accelerate LC capability2.\n2The original L1 paper randomly samples N from U[100,4000]\n9\n\nWe first train the model on the L1-Exact penalty for about 300 steps to acquire LC and subsequently\nabout 100 steps on the L1-Max penalty to greedily reduce the reasoning length when possible.\n3.4\nReinforcement Learning from Human Feedback (RLHF)\nReinforcement Learning from Human Feedback (RLHF) aligns model outputs with human prefer-\nences and practical usability. By combining reasoning/non-reasoning RLHF and RLVR, we concur-\nrently refine model behavior to improve alignment with human preferences while preserving and\nenhancing reasoning abilities.\nTo better align the model’s outputs with human preferences, we first train a reward model using\na combined set of human preference data, as detailed in our previous technical report (Yoo et al.,\n2024b). This data consists of pairwise comparisons either annotated by expert raters or inferred via\nscoring from in-house judge models. The reward model learns to predict scores for each sequence\nin non-reasoning data. Following this, we use GRPO explained in Section 3.2 as the core RLHF\nalgorithm. The policy is optimized to maximize the expected reward predicted by the reward model.\nUnlike RLVR, we apply a KL penalty of 0.1 to maintain proximity to the SFT checkpoint. This\nrelatively strong KL penalty prioritizes training stability over exploration in RLHF.\nThe prompts used during RLHF training consist of a mixture of reasoning and non-reasoning tasks.\nFor non-reasoning, the model is expected to generate assistant response directly, while for reasoning,\nthe model first generates intermediate think step followed by assistant response. The reward model\nevaluates only the response portion of the output and the think portion is not directly scored, allowing\nthe model to freely develop internal reasoning patterns.\nLastly, when training with RLHF subsequently after RLVR, we observe a slight degradation in the\nmodel’s reasoning ability that was optimized during the RLVR phase. A similar pattern was also\nobserved in other reasoning models (Yang et al., 2025). To address this issue, we adopt a joint\ntraining strategy where RLVR and RLHF are trained concurrently. Specifically, we interleave the\ntraining batches such that each batch contains a mixture of samples from RLVR and RLHF datasets.\nThis approach preserves the performance gains of both RLHF and RLVR while unifying the training\nphases, resulting in a simpler and more effective training pipeline.\n4\nEvaluation\n4.1\nBaselines\nWe compare our model against publicly available models of comparable size that are recognized\nfor their reasoning capabilities, including Qwen3-14B, Qwen3-32B (Yang et al., 2025), QwQ-\n32B (Qwen Team, 2025), and EXAONE-Deep-32B (LG AI Research, 2025). We utilize evaluation\nscores directly from each model’s original paper when available. Otherwise, we conduct our own\nevaluations and report the corresponding results.\n4.2\nEvaluation Protocol\nWhen published metrics are unavailable, we perform in-house evaluations using primarily public\nbenchmark sets, with the exception of KoBigBench (Yoo et al., 2024b). The primary goal of our\nevaluation strategy is to interpret and extract the predicted answers from language models for both\nopen-ended and multiple-choice benchmarks as accurately as possible. Models often fail to produce\na final answer when asked to generate the reasoning chain and the answer consecutively in a single\npass. To address this, we adopt a two-pass generation scheme: the model first produces the reasoning\nchain with our chat template (<|im_start|>assistant/think\\n...<|im_end|>), and\nwe then generate the answer by appending an answer prefix. Our evaluation framework combines\nLM Eval Harness Gao et al. (2023) with an in-house toolkit that we plan to release soon for public\nreference.\nFor our model, we configure the generation temperature at 0.5 and top-p at 0.95. In the case of\nother models, their authors’ recommended optimal hyperparameters are utilized. All evaluations are\nperformed using zero-shot Chain-of-Thought (CoT) reasoning, and the maximum CoT generation\nlength is uniformly set to 4096.\n10\n\nGeneral Aptitude\nCulture & Language\nInstruction\nFollowing\n0\n20\n40\n60\n80\n100\nAverage Score\n69.4\n84.6\n92.8\n65.4\n76.7\n88.4\n62.9\n75.6\n89.8\n60.2\n78.0\n87.6\n56.3\n70.7\n80.7\nHyperCLOVA X THINK\nQwen3 32B\nQwen3 14B\nQwQ 32B\nEXAONE Deep 32B\nFigure 5: Summary of model performance on (1) General Aptitude, (2) Culture and Language,\nand (3) Instruction-following benchmarks specifically focused on Korea. The instruction-following\nbenchmark scores are normalized by multiplying their original values by 10.\n4.3\nKorea-Centric Benchmarks\nSetup. As introduced in Section 1, our model’s general performance is evaluated against various\nbaselines using a set of Korea-centric benchmarks. These evaluations are designed to assess the\nmodel’s understanding of Korean culture and knowledge. To achieve this, we curated datasets specif-\nically pertaining to Korea:\n• General Aptitude: KMMLU (Son et al., 2025) and CSAT gauge general Korean knowl-\nedge. KorMedMCQA (Kweon et al., 2024) focuses on medical problem-solving and\nKoBALT-700 (Shin et al., 2025) assesses linguistic depth and typological grounding in\nKorean.\n• Culture and Language: HAERAE-1.0 (Son et al., 2024), CLIcK (Kim et al., 2024a), and\nKoBigBench3 evaluate Korean-specific cultural, geographical, historical knowledge, etc.\n• Instruction-Following: LogicKor (Park, 2024) and KoMTBench (LG AI Research, 2024)\nmeasure the model’s ability to follow Korean instructions.\nResult. Our model’s strong performance on the comprehensive aptittude tests, Korean-specific cul-\nture and linguistic benchmarks, and a suite of benchmarks for probing instruction-following capa-\nbilities is summarized in Figure 5 and detailed in Table 3. By employing zero-shot CoT prompting\nto elicit robust reasoning and evaluating answers based on accuracy, we demonstrate that THINK\nsurpasses other baselines. Additional evaluation results can be found in the Appendix B and Ap-\npendix C. Furthermore, this superior performance is achieved with a relatively small computational\ncost, which will be discussed further in the subsequent section.\n5\nAnalysis\n5.1\nTraining Efficiency\nThere has been research showing that model performance consistently improves with increases in\ndata volume, parameter count, and computational resources in accordance with Scaling Laws (Ka-\nplan et al., 2020). This has also been further supported by more recent work on Expanded Neural\n3The dataset will be publicly released.\n11\n\nCategory\nBenchmarks\nHCX\nTHINK\nQwen3\nQwen3\nQwQ\nEXAONE\nDeep\n( - )\n(32B)\n(14B)\n(32B)\n(32B)\nGeneral\nAptitude\nKMMLU\n69.7\n63.5\n49.3\n54.1\n53.6\nCSAT\n83.2\n81.9\n77.1\n84.7\n69.7\nKorMedMCQA\n76.0\n74.7\n68.5\n69.4\n68.8\nKoBALT\n48.9\n41.4\n38.4\n32.4\n33.0\nCulture &\nLanguage\nHAERAE\n87.8\n75.1\n74.1\n76.2\n74.7\nCLIcK\n80.1\n71.1\n68.8\n73.6\n62.2\nKoBigBench\n85.9\n83.9\n83.8\n84.1\n75.3\nInstruction-\nFollowing\nLogicKor\n9.65\n8.93\n9.15\n9.02\n8.54\nKoMTBench\n8.90\n8.75\n8.82\n8.50\n7.59\nTable 3: Performance comparison of language models on Korea-centric benchmarks. Models are\nevaluated across comprehensive understanding, cultural sovereignty, and chat-based instruction-\nfollowing tasks, highlighting their capabilities and adaptability within a Korean context.\nHyperCLOVA X\nTHINK (base)\nQWEN2.5-0.5B\nQWEN2.5-1.5B\nLLaMA3-8B\nQWEN2.5-14B\nQWEN2.5-32B\nLLaMA3-70B\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\n1.2\n1.4\nGPU Hours (A100, MFU 50%)\n1e7\nFigure 6: Training Efficiency (GPU Hours / A100 / MFU 50%)\nScaling(Chang et al., 2024). However, recent studies have increasingly emphasized the importance\nof data quality. For example, Chang et al. (2024) quantifies data diversity and quality through the\nconcept of effective training tokens and proposes a corresponding scaling law. This study experi-\nmentally demonstrates that efficient training and performance improvements are achievable even for\nsmaller models.\nIt was reported that Qwen2.5 significantly improved its reasoning and long-context generation ca-\npabilities using 18 trillion tokens of high-quality training data and advanced post-training strate-\ngies(Qwen et al., 2025). Similarly, LLaMA 3, trained on 15 trillion tokens, showed continued per-\nformance gains even after surpassing the Chinchilla-optimal range. This suggests that while data\nscaling remains important, an approach centered on data quality is also necessary. Furthermore, as\nthe volume of natural language data available for collection from the internet approaches its limits,\na paradigm shift from data quantity to data quality is accelerating.\nTHINK is developed with a focus on creating a high-efficiency architecture and a training strategy\ngrounded in high-quality data. As a result, it required significantly fewer GPU hours than similar\nsized models to be trained, as shown in Figure 6. At the same time, it achieves competitive perfor-\n12\n\nCross-lingual Consistency (En, Ko)\nMT\n(✓, ✓) ↑\n(✓, ✗) ↓\n(✗, ✓) ↓\n(✗, ✗) ↑\nKo→En\nEn→Ko\nQwen3 32B\n81.0\n8.0\n4.5\n6.5\n92.8\n85.3\nEXAONE Deep 32B\n62.5\n22\n3.5\n12.0\n85.6\n77.5\nHyperCLOVA X THINK\n74.5\n12.0\n4.5\n9.0\n90.3\n85.8\nTable 4: Cross-lingual transferability between English and Korean. Each consistency column shows\nthe case of MCQA items for which the model is correct (✓) or incorrect (✗) in English (first symbol)\nand Korean (second symbol). Higher symmetric ((✓, ✓) and (✗, ✗)) and lower asymmetric ( (✗, ✓),\n(✓, ✗)) ratios imply stronger consistency. We also report xCOMET translation quality of Flores on\nboth directions.\nmance. This demonstrates that strategic data curation and training efficiency are critical factors in\ndeveloping high-performance LLMs, moving beyond reliance on sheer resource input.\n5.2\nCross-Lingual Transferability\nIn this subsection, we investigate cross-lingual consistency and bi-directional translation quality be-\ntween Korean and English to evaluate whether the model properly transfers the acquired English\nknowledge into Korean. Our cross-lingual evaluation hypothesis is twofold. First, a model that has\nefficiently encoded both languages should yield semantically equivalent answers when parallel ques-\ntions are posed in English and Korean. Second, if the same underlying representations truly capture\nlanguage-agnostic meaning, the model should also display strong translation ability in both direc-\ntions.\nCross-lingual Consistency. In order to compute the score of cross-lingual consistency, we adopt\nthe pipeline proposed by Qi et al. (2023); Xing et al. (2024); Yoo et al. (2024a) and evaluate with\nGlobal-MMLU-Lite (Singh et al., 2024). We only computed the scores in culturally agnostic samples\nto exclude examples whose gold answers depend on the source language. Our experiment utilizes\nCaliper framework, described in Section. 4.2. We categorize the model’s predictions on parallel\nEnglish-Korean MCQA prediction results into four cases: (1) (✓,✓) represents question answered\ncorrectly in both languages indicating the desired cross-lingual aligned, (2) (✓, ✗) is the number of\nsamples that are correct in English but incorrect in Korean, isolating failures of knowledge trans-\nfer, (3) (✗, ✓) represents the opposite scenario, and (4) (✗, ✗) records items answered incorrectly\nin both languages, reflecting residual knowledge gaps. This decomposition enables us to attribute\nimprovements in overall accuracy to genuine bilingual robustness.\nAs shown in Table 4, THINK achieves a comparable (✓,✓) ratio (74.5%) ,only a few points behind\nthe extensively trained Qwen3 32B, while limiting asymmetric errors to 16.5%. Given that THINK\nwas tuned almost exclusively on the Korean–English pair and required a fraction of the compute\nbudget demonstrated in Section 5.1, this result indicates that carefully targeted bilingual training\ncan offset much of the scale advantage by large multilingual models. Although the remaining asym-\nmetric cases highlight room for improvement, THINK already delivers a robust and cost-efficient\nbilingual representation. Its overall consistency is also higher than that of EXAONE Deep 32B,\nsuggesting that strategic data curation can sometimes outweigh pure parameter count.\nMachine translation. To complement the cross-lingual transferability analysis in cross-lingual con-\nsistency with MCQA task, we next assess bidirectional machine translation (MT) performance of\neach model between Korean and English. We adopt the Flores benchmark Team et al. (2022) and\ntranslate the official sub-samples of test dataset in both directions ( En→Ko, Ko→En). A prompt\nof each model includes the same 1-shot example. We only extract the translation part from the re-\nsponse. Each translation quality is measured with xCOMET-XL Guerreiro et al. (2023), a model\nbased metric that has a stronger correlation with professional human judgment compared to BLEU\nand ChrF. We report xCOMET-XL score for each direction. This performance indicates the model\ncan faithfully re-express the same underlying knowledge as fluent Korean or English.\nMT columns of Table 4 provide additional evidence of THINK’s robust cross-lingual transferability\nin a generation task. In Ko→En direction, THINK achieves a competitive xCOMET score as 90.3,\n13\n\nclosely approaching the performance of Qwen3 32B model (92.8). Furthermore, in the opposite di-\nrection (En→Ko), THINK surpasses all other models with a score of 85.8. This result indicates that\nour training pipeline not only preserves English knowledge but also enhances the model’s ability to\nrender it into high-quality Korean. These findings, combined with the consistency results, validate\nthat our data curation can deliver bidirectional translation capability without the extensive computa-\ntional overhead of full-scale multilingual pre-training.\n6\nExtensions\n6.1\nInstilling Vision-Language Reasoning in Korean\nThe pursuit of sovereign multimodal AI requires not only proficiency in native languages but also\nrobust capabilities for reasoning across modalities. Given that THINK was originally developed\nand optimized for advanced reasoning in text, can it be effectively extended into vision-grounded\nreasoning through a dedicated multimodal post-training pipeline?\nIn this subsection, we present a separate experimental branch: Starting from the text SFT pipeline\n(Section 3.1), we incorporate visual modules and multimodal tuning to construct a vision-language\nmodel. This enables direct evaluation of complex vision-language reasoning beyond simple transfer\nfrom text-only capabilities. For real-world assessment, we use challenging STEM items from the\nKorean College Scholastic Ability Test (KCSAT). As the KCSAT is administered in Korean and\nreflects rigorous local standards, it is suitable as a test of vision-language reasoning ability in Korean.\nEach item is presented to the model as an image containing mathematical expressions, tables, di-\nagrams, and scientific text (See Appendix D). The model must first accurately recognize visual\ncontent (e.g., text, layout, object and diagram recognition), then perform multi-step logical reason-\ning. Here, vision-language reasoning refers to this integrated process of visual understanding and\nabstract problem-solving, beyond perceptual recognition alone.\nArchitecture and Training. For vision-language reasoning, we augment the LLM backbone with a\nvisual encoder module, similar to the architecture in our previous work, HyperCLOVA X SEED (Hy-\nperCLOVA X Team, 2024). The architecture is composed of:\n• Vision Encoder: SigLIP-2 (Tschannen et al., 2025), operating at 512×512 pixels per grid.\n• Vision-Language Model Architecture: LLaVA-1.5-HD-based framework (Liu et al.,\n2024) with C-Abstractor (Cha et al., 2024) connector mechanism, supporting up to 1.57M\ntotal pixels distributed over 6 grids.\nThe training pipeline extends our previous protocol (Kim et al., 2024b) by inserting a dedicated\nvision SFT stage between text SFT and multimodal RLHF. More concretely, after pre-training on\nlarge-scale text corpora, we first apply SFT on instruction-oriented text data, then conduct multi-\nmodal SFT with paired image-text data, and finally perform multimodal RLHF on both text-only\nand multimodal instructions. This change to RLHF—incorporating vision-language samples in ad-\ndition to text—distinguishes our ablation pipeline from standard text-only approaches. Reasoning\nMode (Section 3) is toggled via explicit prompting throughout both model training and inference,\nwith ablations performed using both reasoning-enabled and baseline prompt formats.\nExperiments. We evaluate vision-language reasoning performance on the multimodal Korean Edu-\ncational Test benchmark (Park and Kim, 2025), with primary focus on its most difficult subset: the\nKCSAT STEM subjects. This evaluation set comprises 206 items spanning mathematics, physics,\nchemistry, earth science, and biology, each requiring advanced logical and visual inference at both\nbasic and advanced levels. The KCSAT is internationally recognized for its depth and rigor, making\nit an exemplary proxy for high-stakes, real-world STEM reasoning.\nOur experiments compare THINK with Vision against leading contemporary closed APIs in the mul-\ntimodal LLM space—specifically GPT-4 Turbo with Vision (OpenAI et al., 2024c), GPT-4o (Ope-\nnAI et al., 2024a), GPT-4.1 (OpenAI et al., 2024c) and OpenAI-o1 (OpenAI et al., 2024b). All mod-\nels are assessed under strictly identical protocols using the same visual and textual input, ensuring a\nfully standardized evaluation environment.\nResults and Analysis. As summarized in Table 5, THINK attains an overall accuracy of 46.4% on\nthe KCSAT STEM benchmark, outperforming GPT-4.1 (40.3%) and approaching the performance of\n14\n\nModel\nMath\nPhysics\nChemistry\nEarth Science\nBiology\nOverall\nBasic\nAdv.\nBasic\nAdv.\nBasic\nAdv.\nBasic\nAdv.\nBasic\nAdv.\nGPT-4 Turbo with Vision\n54.5\n20.8\n5.0\n15.0\n15.0\n20.0\n30.0\n25.0\n10.0\n40.0\n23.8\nGPT-4o\n68.2\n50.0\n15.0\n20.0\n25.0\n25.0\n40.0\n30.0\n15.0\n25.0\n32.0\nGPT-4.1\n68.2\n66.7\n20.0\n30.0\n40.0\n30.0\n30.0\n40.0\n35.0\n35.0\n40.3\nOpenAI-o1\n93.2\n83.3\n42.5\n40.0\n38.8\n56.3\n32.5\n42.5\n37.5\n31.3\n50.9\nHyperCLOVA X THINK with Vision\n68.2\n68.1\n33.3\n28.3\n41.7\n58.3\n28.3\n38.3\n43.3\n50.0\n46.4\nw/o Reasoning Mode\n22.7\n20.8\n11.7\n26.7\n11.7\n28.3\n20.0\n23.3\n30.0\n21.7\n21.7\nTable 5: Evaluation of native vision-language reasoning ability on the KCSAT STEM multimodal\nbenchmark (Park and Kim, 2025) by subject and level. The benchmark consists of 206 questions\ncovering five scientific subjects (basic/advanced), with 20 to 24 questions per subject and level.\nKCSAT is widely regarded for its difficulty, emphasis on scientific reasoning, and its reflection of\nKorea’s high-achieving STEM education system.\nGPT-o1 (50.9%). Disabling Reasoning Mode notably causes performance to drop to 21.7%, support-\ning the conclusion that advanced reasoning skills acquired during language pretraining are crucial\nand can be effectively extended to vision-centric STEM challenges when combined with specialized\nmultimodal tuning. Further qualitative analyses and representative sample outputs are provided in\nAppendix D.\nOn the other hand, we note a modest trade-off: adding multimodal SFT introduces a slight decrease\nin text-only reasoning performance, underscoring the inherent difficulty of jointly optimizing for\nboth modalities. Achieving a balanced sovereign AI—excelling in both unimodal and multimodal\nreasoning—remains open for further tuning and methodological advances. These observations high-\nlight that effective vision-language reasoning, especially in STEM, demands robust integration of\nvisual parsing and native multi-step logic. As a next step, our future work will extend the model’s\ncapabilities towards unified, native reasoning across text, vision, and audio.\n6.2\nLightening through Pruning and Distillation\nAs competition in high-performance model development intensifies, training models with tens or\nhundreds of billions of parameters using trillions of tokens has become the de facto industry stan-\ndard. This large-scale training approach entails high costs and long development cycles, while lim-\niting the ability to respond to rapidly changing service environments. Consequently, learning strate-\ngies that can efficiently build LLMs with fewer resources have recently gained attention. These\napproaches not only reduce costs, but also offer practical advantages in terms of timely model de-\nvelopment and operation.\nOne of the leading approaches combines pruning and knowledge distillation. Pruning reduces model\nsize by removing less important parameters, while distillation is a technique that transfers knowl-\nedge learned by large models to lightweight models to maintain performance. Combining these two\ntechniques can achieve both model compression and performance preservation simultaneously.\nAs a real-world example, HyperCLOVA X SEED 0.5B, recently released on HuggingFace, is the\nfirst open-source model in the HyperCLOVA X series trained using pruning and knowledge distil-\nlation. Despite being similar in size to Qwen2.5-0.5BQwen et al. (2025), it was trained at approx-\nimately 39 times lower cost and outperformed competing models in most benchmarks. Notably, it\ndemonstrated significant performance improvements in Korean language benchmarks. This model\noffers high practical value as it enables high-performance conversational interfaces even in resource-\nconstrained environments such as mobile applications or smart home devices.\nFurthermore, the combination of pruning and distillation can be utilized for efficient production\nof both lightweight and large models. Depending on the structure of the teacher model used for\ntraining, the type of data to be transferred, and the learning strategy, models of various sizes and\npurposes can be produced. This flexibility is expected to improve usability across future generative\nAI applications. Currently, a pruned and distilled version of THINK is under preparation to be open-\nsourced.\n15\n\n7\nConclusion\nIn this report, we introduced HyperCLOVA X THINK, the first reasoning-focused LLM within Hy-\nperCLOVA X family. It is efficiently trained to achieve two primary objectives: advanced reasoning\ncapabilities and the promotion of sovereign AI for Korea.\nIts pre-training dataset comprises approximately 6 trillion high-quality tokens spanning Korean, En-\nglish, and further enhanced by targeted synthetic Korean data. We employ a Peri-LN Transformer\nscaled with µP, ensuring stable scalability and cost-efficient training. A three-stage curriculum en-\nables the model to expand its context window to 128k tokens and demonstrate robust long-form\nchain-of-thought reasoning. Post-training involves supervised fine-tuning and reinforcement learn-\ning with verifiable rewards, utilizing a curated data filtering process to address both detailed reason-\ning and simple answering tasks.\nExperiments demonstrate HyperCLOVA X THINK’s competitive performance against other reason-\ning models on Korea-centric benchmarks such as KMMLU, CSAT, KoBALT-700, HAERAE-1.0,\nand KoBigBench. Analysis highlights its highly efficient training cost and its ability to preserve\nrobust bilingual consistency. Furthermore, a vision-augmented variant achieved performance com-\nparable to GPT-4.1 on the KCSAT STEM benchmark.\nOur report shows that using additional test-time compute to refine model responses is an effective\nway to push the limits of model capability and improve compute-cost efficiency. As foundational AI\ntechnology gains greater potential to enrich people’s lives and shape the future of digital business,\nwe remain committed to improving reasoning scalability and delivering foundation models that are\nboth powerful and affordable, thereby accelerating both domestic and global AI transformation in\nbusinesses.\nNote that, while we took necessary measures to improve the safety of HyperCLOVA X THINK as\nper NAVER AI Ethics guidelines, the harmlessness of the generated text cannot be fully guaran-\nteed. Thus, the responses may contain toxic remarks, exhibit biases or otherwise harmful content.\nHowever, we remain dedicated to responsible AI development and deployment.\nLastly, we plan to open-source a pruned and distilled version of HyperCLOVA X THINK. This\ninitiative aims to benefit academic and industry partners with limited resources, fostering the future\ndevelopment and utilization of sovereign LLMs.\nReferences\nPranjal Aggarwal and Sean Welleck. 2025. L1: Controlling how long a reasoning model thinks with\nreinforcement learning. arXiv preprint arXiv:2503.04697.\nSanghwan Bae, Jiwoo Hong, Min Young Lee, Hanbyul Kim, JeongYeon Nam, and Donghyun Kwak.\n2025. Online difficulty filtering for reasoning oriented reinforcement learning. arXiv preprint\narXiv:2504.03380.\nLoubna Ben Allal, Anton Lozhkov, Guilherme Penedo, Thomas Wolf, and Leandro von Werra. 2024.\nCosmopedia.\nXiao Bi, Deli Chen, Guanting Chen, Shanhuang Chen, Damai Dai, Chengqi Deng, Honghui Ding,\nKai Dong, Qiushi Du, Zhe Fu, Huazuo Gao, Kaige Gao, Wenjun Gao, Ruiqi Ge, Kang Guan, Daya\nGuo, Jianzhong Guo, Guangbo Hao, Zhewen Hao, Ying He, Wenjie Hu, Panpan Huang, Erhang\nLi, Guowei Li, Jiashi Li, Yao Li, Y. K. Li, Wenfeng Liang, Fangyun Lin, Alex X. Liu, Bo Liu,\nWen Liu, Xiaodong Liu, Xin Liu, Yiyuan Liu, Haoyu Lu, Shanghao Lu, Fuli Luo, Shirong Ma,\nXiaotao Nie, Tian Pei, Yishi Piao, Junjie Qiu, Hui Qu, Tongzheng Ren, Zehui Ren, Chong Ruan,\nZhangli Sha, Zhihong Shao, Junxiao Song, Xuecheng Su, Jingxiang Sun, Yaofeng Sun, Minghui\nTang, Bingxuan Wang, Peiyi Wang, Shiyu Wang, Yaohui Wang, Yongji Wang, Tong Wu, Y. Wu,\nXin Xie, Zhenda Xie, Ziwei Xie, Yiliang Xiong, Hanwei Xu, R. X. Xu, Yanhong Xu, Dejian Yang,\nYuxiang You, Shuiping Yu, Xingkai Yu, B. Zhang, Haowei Zhang, Lecong Zhang, Liyue Zhang,\nMingchuan Zhang, Minghua Zhang, Wentao Zhang, Yichao Zhang, Chenggang Zhao, Yao Zhao,\nShangyan Zhou, Shunfeng Zhou, Qihao Zhu, and Yuheng Zou. 2024. Deepseek LLM: scaling\nopen-source language models with longtermism. CoRR, abs/2401.02954.\n16\n\nCody Blakeney, Mansheej Paul, Brett W. Larsen, Sean Owen, and Jonathan Frankle. 2024. Does\nyour data spark joy? performance gains from domain upsampling at the end of training. CoRR,\nabs/2406.03476.\nTom B. Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared Kaplan, Prafulla Dhari-\nwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, Sandhini Agarwal,\nAriel Herbert-Voss, Gretchen Krueger, Tom Henighan, Rewon Child, Aditya Ramesh, Daniel M.\nZiegler, Jeffrey Wu, Clemens Winter, Christopher Hesse, Mark Chen, Eric Sigler, Mateusz Litwin,\nScott Gray, Benjamin Chess, Jack Clark, Christopher Berner, Sam McCandlish, Alec Radford,\nIlya Sutskever, and Dario Amodei. 2020. Language models are few-shot learners. In Advances in\nNeural Information Processing Systems 33: Annual Conference on Neural Information Process-\ning Systems 2020, NeurIPS 2020, December 6-12, 2020, virtual.\nJunbum Cha, Wooyoung Kang, Jonghwan Mun, and Byungseok Roh. 2024. Honeybee: Locality-\nenhanced Projector for Multimodal LLM. In Proceedings of the IEEE/CVF Conference on Com-\nputer Vision and Pattern Recognition (CVPR).\nErnie Chang, Matteo Paltenghi, Yang Li, Pin-Jie Lin, Changsheng Zhao, Patrick Huber, Zechun\nLiu, Rastislav Rabatin, Yangyang Shi, and Vikas Chandra. 2024. Scaling parameter-constrained\nlanguage models with quality data. In Proceedings of the 2024 Conference on Empirical Methods\nin Natural Language Processing: Industry Track, pages 80–97, Miami, Florida, US. Association\nfor Computational Linguistics.\nXingyu Chen, Jiahao Xu, Tian Liang, Zhiwei He, Jianhui Pang, Dian Yu, Linfeng Song, Qiuzhi\nLiu, Mengfei Zhou, Zhuosheng Zhang, et al. 2024. Do not think that much for 2+ 3=? on the\noverthinking of o1-like llms. arXiv preprint arXiv:2412.21187.\nDaixuan Cheng, Yuxian Gu, Shaohan Huang, Junyu Bi, Minlie Huang, and Furu Wei. 2024. Instruc-\ntion pre-training: Language models are supervised multitask learners. In Proceedings of the 2024\nConference on Empirical Methods in Natural Language Processing, EMNLP 2024, Miami, FL,\nUSA, November 12-16, 2024, pages 2529–2550. Association for Computational Linguistics.\nDeepSeek-AI, Daya Guo, Dejian Yang, Haowei Zhang, Junxiao Song, Ruoyu Zhang, Runxin Xu,\nQihao Zhu, Shirong Ma, Peiyi Wang, Xiao Bi, Xiaokang Zhang, Xingkai Yu, Yu Wu, Z. F. Wu,\nZhibin Gou, Zhihong Shao, Zhuoshu Li, Ziyi Gao, Aixin Liu, Bing Xue, Bingxuan Wang, Bochao\nWu, Bei Feng, Chengda Lu, Chenggang Zhao, Chengqi Deng, Chenyu Zhang, Chong Ruan,\nDamai Dai, Deli Chen, Dongjie Ji, Erhang Li, Fangyun Lin, Fucong Dai, Fuli Luo, Guangbo Hao,\nGuanting Chen, Guowei Li, H. Zhang, Han Bao, Hanwei Xu, Haocheng Wang, Honghui Ding,\nHuajian Xin, Huazuo Gao, Hui Qu, Hui Li, Jianzhong Guo, Jiashi Li, Jiawei Wang, Jingchang\nChen, Jingyang Yuan, Junjie Qiu, Junlong Li, J. L. Cai, Jiaqi Ni, Jian Liang, Jin Chen, Kai\nDong, Kai Hu, Kaige Gao, Kang Guan, Kexin Huang, Kuai Yu, Lean Wang, Lecong Zhang,\nLiang Zhao, Litong Wang, Liyue Zhang, Lei Xu, Leyi Xia, Mingchuan Zhang, Minghua Zhang,\nMinghui Tang, Meng Li, Miaojun Wang, Mingming Li, Ning Tian, Panpan Huang, Peng Zhang,\nQiancheng Wang, Qinyu Chen, Qiushi Du, Ruiqi Ge, Ruisong Zhang, Ruizhe Pan, Runji Wang,\nR. J. Chen, R. L. Jin, Ruyi Chen, Shanghao Lu, Shangyan Zhou, Shanhuang Chen, Shengfeng\nYe, Shiyu Wang, Shuiping Yu, Shunfeng Zhou, Shuting Pan, S. S. Li, Shuang Zhou, Shaoqing\nWu, Shengfeng Ye, Tao Yun, Tian Pei, Tianyu Sun, T. Wang, Wangding Zeng, Wanjia Zhao, Wen\nLiu, Wenfeng Liang, Wenjun Gao, Wenqin Yu, Wentao Zhang, W. L. Xiao, Wei An, Xiaodong\nLiu, Xiaohan Wang, Xiaokang Chen, Xiaotao Nie, Xin Cheng, Xin Liu, Xin Xie, Xingchao Liu,\nXinyu Yang, Xinyuan Li, Xuecheng Su, Xuheng Lin, X. Q. Li, Xiangyue Jin, Xiaojin Shen, Xi-\naosha Chen, Xiaowen Sun, Xiaoxiang Wang, Xinnan Song, Xinyi Zhou, Xianzu Wang, Xinxia\nShan, Y. K. Li, Y. Q. Wang, Y. X. Wei, Yang Zhang, Yanhong Xu, Yao Li, Yao Zhao, Yaofeng\nSun, Yaohui Wang, Yi Yu, Yichao Zhang, Yifan Shi, Yiliang Xiong, Ying He, Yishi Piao, Yisong\nWang, Yixuan Tan, Yiyang Ma, Yiyuan Liu, Yongqiang Guo, Yuan Ou, Yuduan Wang, Yue Gong,\nYuheng Zou, Yujia He, Yunfan Xiong, Yuxiang Luo, Yuxiang You, Yuxuan Liu, Yuyang Zhou,\nY. X. Zhu, Yanhong Xu, Yanping Huang, Yaohui Li, Yi Zheng, Yuchen Zhu, Yunxian Ma, Ying\nTang, Yukun Zha, Yuting Yan, Z. Z. Ren, Zehui Ren, Zhangli Sha, Zhe Fu, Zhean Xu, Zhenda\nXie, Zhengyan Zhang, Zhewen Hao, Zhicheng Ma, Zhigang Yan, Zhiyu Wu, Zihui Gu, Zijia Zhu,\nZijun Liu, Zilin Li, Ziwei Xie, Ziyang Song, Zizheng Pan, Zhen Huang, Zhipeng Xu, Zhongyu\nZhang, and Zhen Zhang. 2025. Deepseek-r1: Incentivizing reasoning capability in llms via rein-\nforcement learning.\n17\n\nAbhimanyu Dubey, Abhinav Jauhri, Abhinav Pandey, Abhishek Kadian, Ahmad Al-Dahle, Aiesha\nLetman, Akhil Mathur, Alan Schelten, Amy Yang, Angela Fan, Anirudh Goyal, Anthony\nHartshorn, Aobo Yang, Archi Mitra, Archie Sravankumar, Artem Korenev, Arthur Hinsvark,\nArun Rao, Aston Zhang, Aurélien Rodriguez, Austen Gregerson, Ava Spataru, Baptiste Rozière,\nBethany Biron, Binh Tang, Bobbie Chern, Charlotte Caucheteux, Chaya Nayak, Chloe Bi, Chris\nMarra, Chris McConnell, Christian Keller, Christophe Touret, Chunyang Wu, Corinne Wong,\nCristian Canton Ferrer, Cyrus Nikolaidis, Damien Allonsius, Daniel Song, Danielle Pintz, Danny\nLivshits, David Esiobu, Dhruv Choudhary, Dhruv Mahajan, Diego Garcia-Olano, Diego Perino,\nDieuwke Hupkes, Egor Lakomkin, Ehab AlBadawy, Elina Lobanova, Emily Dinan, Eric Michael\nSmith, Filip Radenovic, Frank Zhang, Gabriel Synnaeve, Gabrielle Lee, Georgia Lewis Anderson,\nGraeme Nail, Grégoire Mialon, Guan Pang, Guillem Cucurell, Hailey Nguyen, Hannah Korevaar,\nHu Xu, Hugo Touvron, Iliyan Zarov, Imanol Arrieta Ibarra, Isabel M. Kloumann, Ishan Misra,\nIvan Evtimov, Jade Copet, Jaewon Lee, Jan Geffert, Jana Vranes, Jason Park, Jay Mahadeokar,\nJeet Shah, Jelmer van der Linde, Jennifer Billock, Jenny Hong, Jenya Lee, Jeremy Fu, Jianfeng\nChi, Jianyu Huang, Jiawen Liu, Jie Wang, Jiecao Yu, Joanna Bitton, Joe Spisak, Jongsoo Park,\nJoseph Rocca, Joshua Johnstun, Joshua Saxe, Junteng Jia, Kalyan Vasuden Alwala, Kartikeya\nUpasani, Kate Plawiak, Ke Li, Kenneth Heafield, Kevin Stone, and et al. 2024. The llama 3 herd\nof models. CoRR, abs/2407.21783.\nLeo Gao, Jonathan Tow, Baber Abbasi, Stella Biderman, Sid Black, Anthony DiPofi, Charles Fos-\nter, Laurence Golding, Jeffrey Hsu, Alain Le Noac’h, Haonan Li, Kyle McDonell, Niklas Muen-\nnighoff, Chris Ociepa, Jason Phang, Laria Reynolds, Hailey Schoelkopf, Aviya Skowron, Lintang\nSutawika, Eric Tang, Anish Thite, Ben Wang, Kevin Wang, and Andy Zou. 2023. A framework\nfor few-shot language model evaluation.\nNuno M Guerreiro, Ricardo Rei, Daan van Stigt, Luisa Coheur, Pierre Colombo, and André FT\nMartins. 2023. xcomet: Transparent machine translation evaluation through fine-grained error\ndetection. arXiv preprint arXiv:2310.10482.\nJordan Hoffmann, Sebastian Borgeaud, Arthur Mensch, Elena Buchatskaya, Trevor Cai, Eliza\nRutherford, Diego de Las Casas, Lisa Anne Hendricks, Johannes Welbl, Aidan Clark, Tom Hen-\nnigan, Eric Noland, Katie Millican, George van den Driessche, Bogdan Damoc, Aurelia Guy,\nSimon Osindero, Karen Simonyan, Erich Elsen, Jack W. Rae, Oriol Vinyals, and Laurent Sifre.\n2022. Training compute-optimal large language models. CoRR, abs/2203.15556.\nShengding Hu, Yuge Tu, Xu Han, Chaoqun He, Ganqu Cui, Xiang Long, Zhi Zheng, Yewei Fang,\nYuxiang Huang, Weilin Zhao, Xinrong Zhang, Zhen Leng Thai, Kai Zhang, Chongyi Wang, Yuan\nYao, Chenyang Zhao, Jie Zhou, Jie Cai, Zhongwu Zhai, Ning Ding, Chao Jia, Guoyang Zeng,\nDahai Li, Zhiyuan Liu, and Maosong Sun. 2024. Minicpm: Unveiling the potential of small\nlanguage models with scalable training strategies. CoRR, abs/2404.06395.\nNAVER\nCloud\nHyperCLOVA\nX\nTeam.\n2024.\nHyperCLOVA\nX\nSEED\nVi-\nsion\nInstruct\n3B.\nhttps://huggingface.co/naver-hyperclovax/\nHyperCLOVAX-SEED-Vision-Instruct-3B.\nAvailable on Hugging Face Hub.\nAccessed: 2025-06-23.\nArmand Joulin, Edouard Grave, Piotr Bojanowski, Matthijs Douze, Hervé Jégou, and Tomás\nMikolov. 2016. Fasttext.zip: Compressing text classification models. CoRR, abs/1612.03651.\nArmand Joulin, Edouard Grave, Piotr Bojanowski, and Tomás Mikolov. 2017. Bag of tricks for\nefficient text classification. In Proceedings of the 15th Conference of the European Chapter of the\nAssociation for Computational Linguistics, EACL 2017, Valencia, Spain, April 3-7, 2017, Volume\n2: Short Papers, pages 427–431. Association for Computational Linguistics.\nJared Kaplan, Sam McCandlish, Tom Henighan, Tom B. Brown, Benjamin Chess, Rewon Child,\nScott Gray, Alec Radford, Jeffrey Wu, and Dario Amodei. 2020. Scaling laws for neural language\nmodels.\nEunsu Kim, Juyoung Suk, Philhoon Oh, Haneul Yoo, James Thorne, and Alice Oh. 2024a. CLIcK:\nA benchmark dataset of cultural and linguistic intelligence in Korean. In Proceedings of the 2024\nJoint International Conference on Computational Linguistics, Language Resources and Evalua-\ntion (LREC-COLING 2024), pages 3335–3346, Torino, Italia. ELRA and ICCL.\n18\n\nGeewook Kim, Taeho Kil, and Jinbae Im. 2024b.\n“HyperCLOVA X Vision: Open Your Eyes,\nCLOVA X!”. https://tv.naver.com/v/67447111. Conference Talk, Dan24, Naver\nAI NOW, December 21, 2023. Accessed: 2025-06-23.\nJeonghoon Kim, Byeongchan Lee, Cheonbok Park, Yeontaek Oh, Beomjun Kim, Taehwan Yoo,\nSeongjin Shin, Dongyoon Han, Jinwoo Shin, and Kang Min Yoo. 2025. Peri-ln: Revisiting nor-\nmalization layer in the transformer architecture. In Forty-second International Conference on\nMachine Learning.\nSunjun Kweon, Byungjin Choi, Gyouk Chu, Junyeong Song, Daeun Hyeon, Sujin Gan, Jueon Kim,\nMinkyu Kim, Rae Woong Park, and Edward Choi. 2024. Kormedmcqa: Multi-choice question\nanswering benchmark for korean healthcare professional licensing examinations.\nBruce W. Lee, Hyunsoo Cho, and Kang Min Yoo. 2024. Instruction tuning with human curriculum.\nIn Findings of the Association for Computational Linguistics: NAACL 2024, Mexico City, Mexico,\nJune 16-21, 2024, pages 1281–1309. Association for Computational Linguistics.\nLG\nAI\nResearch.\n2024.\nKomt-bench.\nhttps://huggingface.co/datasets/\nLGAI-EXAONE/KoMT-Bench.\nLG AI Research. 2025. EXAONE Deep: Reasoning Enhanced Language Models. arXiv preprint\narXiv:2503.12524.\nJeffrey Li, Alex Fang, Georgios Smyrnis, Maor Ivgi, Matt Jordan, Samir Yitzhak Gadre, Hritik\nBansal, Etash Guha, Sedrick Scott Keh, Kushal Arora, Saurabh Garg, Rui Xin, Niklas Muen-\nnighoff, Reinhard Heckel, Jean Mercat, Mayee F. Chen, Suchin Gururangan, Mitchell Worts-\nman, Alon Albalak, Yonatan Bitton, Marianna Nezhurina, Amro Abbas, Cheng-Yu Hsieh, Dhruba\nGhosh, Josh Gardner, Maciej Kilian, Hanlin Zhang, Rulin Shao, Sarah M. Pratt, Sunny Sanyal,\nGabriel Ilharco, Giannis Daras, Kalyani Marathe, Aaron Gokaslan, Jieyu Zhang, Khyathi Raghavi\nChandu, Thao Nguyen, Igor Vasiljevic, Sham M. Kakade, Shuran Song, Sujay Sanghavi, Far-\ntash Faghri, Sewoong Oh, Luke Zettlemoyer, Kyle Lo, Alaaeldin El-Nouby, Hadi Pouransari,\nAlexander Toshev, Stephanie Wang, Dirk Groeneveld, Luca Soldaini, Pang Wei Koh, Jenia Jitsev,\nThomas Kollar, Alex Dimakis, Yair Carmon, Achal Dave, Ludwig Schmidt, and Vaishaal Shankar.\n2024. Datacomp-lm: In search of the next generation of training sets for language models. In Ad-\nvances in Neural Information Processing Systems 38: Annual Conference on Neural Information\nProcessing Systems 2024, NeurIPS 2024, Vancouver, BC, Canada, December 10 - 15, 2024.\nYuanzhi Li, Sébastien Bubeck, Ronen Eldan, Allie Del Giorno, Suriya Gunasekar, and Yin Tat Lee.\n2023. Textbooks are all you need ii: phi-1.5 technical report. arXiv preprint arXiv:2309.05463.\nHaotian Liu, Chunyuan Li, Yuheng Li, and Yong Jae Lee. 2024. Improved Baselines with Visual\nInstruction Tuning. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern\nRecognition (CVPR), pages 26296–26306.\nZichen Liu, Changyu Chen, Wenjun Li, Penghui Qi, Tianyu Pang, Chao Du, Wee Sun Lee, and\nMin Lin. 2025.\nUnderstanding r1-zero-like training: A critical perspective.\narXiv preprint\narXiv:2503.20783.\nAnton Lozhkov, Loubna Ben Allal, Leandro von Werra, and Thomas Wolf. 2024. Fineweb-edu: the\nfinest collection of educational content.\nTeam OLMo, Pete Walsh, Luca Soldaini, Dirk Groeneveld, Kyle Lo, Shane Arora, Akshita Bhagia,\nYuling Gu, Shengyi Huang, Matt Jordan, Nathan Lambert, Dustin Schwenk, Oyvind Tafjord,\nTaira Anderson, David Atkinson, Faeze Brahman, Christopher Clark, Pradeep Dasigi, Nouha\nDziri, Michal Guerquin, Hamish Ivison, Pang Wei Koh, Jiacheng Liu, Saumya Malik, William\nMerrill, Lester James V. Miranda, Jacob Morrison, Tyler Murray, Crystal Nam, Valentina Py-\natkin, Aman Rangapur, Michael Schmitz, Sam Skjonsberg, David Wadden, Christopher Wilhelm,\nMichael Wilson, Luke Zettlemoyer, Ali Farhadi, Noah A. Smith, and Hannaneh Hajishirzi. 2025.\n2 olmo 2 furious. CoRR, abs/2501.00656.\nOpenAI, :, Aaron Hurst, Adam Lerer, Adam P. Goucher, Adam Perelman, Aditya Ramesh, Aidan\nClark, AJ Ostrow, Akila Welihinda, Alan Hayes, Alec Radford, Aleksander M ˛adry, Alex Baker-\nWhitcomb, Alex Beutel, Alex Borzunov, Alex Carney, Alex Chow, Alex Kirillov, Alex Nichol,\n19\n\nAlex Paino, Alex Renzin, Alex Tachard Passos, Alexander Kirillov, Alexi Christakis, Alexis Con-\nneau, Ali Kamali, Allan Jabri, Allison Moyer, Allison Tam, Amadou Crookes, Amin Tootoochian,\nAmin Tootoonchian, Ananya Kumar, Andrea Vallone, Andrej Karpathy, Andrew Braunstein,\nAndrew Cann, Andrew Codispoti, Andrew Galu, Andrew Kondrich, Andrew Tulloch, Andrey\nMishchenko, Angela Baek, Angela Jiang, Antoine Pelisse, Antonia Woodford, Anuj Gosalia,\nArka Dhar, Ashley Pantuliano, Avi Nayak, Avital Oliver, Barret Zoph, Behrooz Ghorbani, Ben Le-\nimberger, Ben Rossen, Ben Sokolowsky, Ben Wang, Benjamin Zweig, Beth Hoover, Blake Samic,\nBob McGrew, Bobby Spero, Bogo Giertler, Bowen Cheng, Brad Lightcap, Brandon Walkin, Bren-\ndan Quinn, Brian Guarraci, Brian Hsu, Bright Kellogg, Brydon Eastman, Camillo Lugaresi, Car-\nroll Wainwright, Cary Bassin, Cary Hudson, Casey Chu, Chad Nelson, Chak Li, Chan Jun Shern,\nChanning Conger, Charlotte Barette, Chelsea Voss, Chen Ding, Cheng Lu, Chong Zhang, Chris\nBeaumont, Chris Hallacy, Chris Koch, Christian Gibson, Christina Kim, Christine Choi, Chris-\ntine McLeavey, Christopher Hesse, Claudia Fischer, Clemens Winter, Coley Czarnecki, Colin\nJarvis, Colin Wei, Constantin Koumouzelis, Dane Sherburn, Daniel Kappler, Daniel Levin, Daniel\nLevy, David Carr, David Farhi, David Mely, David Robinson, David Sasaki, Denny Jin, Dev\nValladares, Dimitris Tsipras, Doug Li, Duc Phong Nguyen, Duncan Findlay, Edede Oiwoh, Ed-\nmund Wong, Ehsan Asdar, Elizabeth Proehl, Elizabeth Yang, Eric Antonow, Eric Kramer, Eric\nPeterson, Eric Sigler, Eric Wallace, Eugene Brevdo, Evan Mays, Farzad Khorasani, Felipe Pet-\nroski Such, Filippo Raso, Francis Zhang, Fred von Lohmann, Freddie Sulit, Gabriel Goh, Gene\nOden, Geoff Salmon, Giulio Starace, Greg Brockman, Hadi Salman, Haiming Bao, Haitang Hu,\nHannah Wong, Haoyu Wang, Heather Schmidt, Heather Whitney, Heewoo Jun, Hendrik Kirch-\nner, Henrique Ponde de Oliveira Pinto, Hongyu Ren, Huiwen Chang, Hyung Won Chung, Ian\nKivlichan, Ian O’Connell, Ian O’Connell, Ian Osband, Ian Silber, Ian Sohl, Ibrahim Okuyucu,\nIkai Lan, Ilya Kostrikov, Ilya Sutskever, Ingmar Kanitscheider, Ishaan Gulrajani, Jacob Coxon,\nJacob Menick, Jakub Pachocki, James Aung, James Betker, James Crooks, James Lennon, Jamie\nKiros, Jan Leike, Jane Park, Jason Kwon, Jason Phang, Jason Teplitz, Jason Wei, Jason Wolfe,\nJay Chen, Jeff Harris, Jenia Varavva, Jessica Gan Lee, Jessica Shieh, Ji Lin, Jiahui Yu, Jiayi\nWeng, Jie Tang, Jieqi Yu, Joanne Jang, Joaquin Quinonero Candela, Joe Beutler, Joe Landers,\nJoel Parish, Johannes Heidecke, John Schulman, Jonathan Lachman, Jonathan McKay, Jonathan\nUesato, Jonathan Ward, Jong Wook Kim, Joost Huizinga, Jordan Sitkin, Jos Kraaijeveld, Josh\nGross, Josh Kaplan, Josh Snyder, Joshua Achiam, Joy Jiao, Joyce Lee, Juntang Zhuang, Justyn\nHarriman, Kai Fricke, Kai Hayashi, Karan Singhal, Katy Shi, Kavin Karthik, Kayla Wood, Kendra\nRimbach, Kenny Hsu, Kenny Nguyen, Keren Gu-Lemberg, Kevin Button, Kevin Liu, Kiel Howe,\nKrithika Muthukumar, Kyle Luther, Lama Ahmad, Larry Kai, Lauren Itow, Lauren Workman,\nLeher Pathak, Leo Chen, Li Jing, Lia Guy, Liam Fedus, Liang Zhou, Lien Mamitsuka, Lilian\nWeng, Lindsay McCallum, Lindsey Held, Long Ouyang, Louis Feuvrier, Lu Zhang, Lukas Kon-\ndraciuk, Lukasz Kaiser, Luke Hewitt, Luke Metz, Lyric Doshi, Mada Aflak, Maddie Simens,\nMadelaine Boyd, Madeleine Thompson, Marat Dukhan, Mark Chen, Mark Gray, Mark Hudnall,\nMarvin Zhang, Marwan Aljubeh, Mateusz Litwin, Matthew Zeng, Max Johnson, Maya Shetty,\nMayank Gupta, Meghan Shah, Mehmet Yatbaz, Meng Jia Yang, Mengchao Zhong, Mia Glaese,\nMianna Chen, Michael Janner, Michael Lampe, Michael Petrov, Michael Wu, Michele Wang,\nMichelle Fradin, Michelle Pokrass, Miguel Castro, Miguel Oom Temudo de Castro, Mikhail\nPavlov, Miles Brundage, Miles Wang, Minal Khan, Mira Murati, Mo Bavarian, Molly Lin, Murat\nYesildal, Nacho Soto, Natalia Gimelshein, Natalie Cone, Natalie Staudacher, Natalie Summers,\nNatan LaFontaine, Neil Chowdhury, Nick Ryder, Nick Stathas, Nick Turley, Nik Tezak, Niko Fe-\nlix, Nithanth Kudige, Nitish Keskar, Noah Deutsch, Noel Bundick, Nora Puckett, Ofir Nachum,\nOla Okelola, Oleg Boiko, Oleg Murk, Oliver Jaffe, Olivia Watkins, Olivier Godement, Owen\nCampbell-Moore, Patrick Chao, Paul McMillan, Pavel Belov, Peng Su, Peter Bak, Peter Bakkum,\nPeter Deng, Peter Dolan, Peter Hoeschele, Peter Welinder, Phil Tillet, Philip Pronin, Philippe\nTillet, Prafulla Dhariwal, Qiming Yuan, Rachel Dias, Rachel Lim, Rahul Arora, Rajan Troll, Ran-\ndall Lin, Rapha Gontijo Lopes, Raul Puri, Reah Miyara, Reimar Leike, Renaud Gaubert, Reza\nZamani, Ricky Wang, Rob Donnelly, Rob Honsby, Rocky Smith, Rohan Sahai, Rohit Ramchan-\ndani, Romain Huet, Rory Carmichael, Rowan Zellers, Roy Chen, Ruby Chen, Ruslan Nigmat-\nullin, Ryan Cheu, Saachi Jain, Sam Altman, Sam Schoenholz, Sam Toizer, Samuel Miserendino,\nSandhini Agarwal, Sara Culver, Scott Ethersmith, Scott Gray, Sean Grove, Sean Metzger, Shamez\nHermani, Shantanu Jain, Shengjia Zhao, Sherwin Wu, Shino Jomoto, Shirong Wu, Shuaiqi, Xia,\nSonia Phene, Spencer Papay, Srinivas Narayanan, Steve Coffey, Steve Lee, Stewart Hall, Suchir\nBalaji, Tal Broda, Tal Stramer, Tao Xu, Tarun Gogineni, Taya Christianson, Ted Sanders, Tejal\nPatwardhan, Thomas Cunninghman, Thomas Degry, Thomas Dimson, Thomas Raoux, Thomas\n20\n\nShadwell, Tianhao Zheng, Todd Underwood, Todor Markov, Toki Sherbakov, Tom Rubin, Tom\nStasi, Tomer Kaftan, Tristan Heywood, Troy Peterson, Tyce Walters, Tyna Eloundou, Valerie Qi,\nVeit Moeller, Vinnie Monaco, Vishal Kuo, Vlad Fomenko, Wayne Chang, Weiyi Zheng, Wenda\nZhou, Wesam Manassra, Will Sheu, Wojciech Zaremba, Yash Patil, Yilei Qian, Yongjik Kim,\nYoulong Cheng, Yu Zhang, Yuchen He, Yuchen Zhang, Yujia Jin, Yunxing Dai, and Yury Malkov.\n2024a. GPT-4o System Card.\nOpenAI, :, Aaron Jaech, Adam Kalai, Adam Lerer, Adam Richardson, Ahmed El-Kishky, Aiden\nLow, Alec Helyar, Aleksander Madry, Alex Beutel, Alex Carney, Alex Iftimie, Alex Karpenko,\nAlex Tachard Passos, Alexander Neitz, Alexander Prokofiev, Alexander Wei, Allison Tam, Ally\nBennett, Ananya Kumar, Andre Saraiva, Andrea Vallone, Andrew Duberstein, Andrew Kondrich,\nAndrey Mishchenko, Andy Applebaum, Angela Jiang, Ashvin Nair, Barret Zoph, Behrooz Ghor-\nbani, Ben Rossen, Benjamin Sokolowsky, Boaz Barak, Bob McGrew, Borys Minaiev, Botao Hao,\nBowen Baker, Brandon Houghton, Brandon McKinzie, Brydon Eastman, Camillo Lugaresi, Cary\nBassin, Cary Hudson, Chak Ming Li, Charles de Bourcy, Chelsea Voss, Chen Shen, Chong Zhang,\nChris Koch, Chris Orsinger, Christopher Hesse, Claudia Fischer, Clive Chan, Dan Roberts, Daniel\nKappler, Daniel Levy, Daniel Selsam, David Dohan, David Farhi, David Mely, David Robinson,\nDimitris Tsipras, Doug Li, Dragos Oprica, Eben Freeman, Eddie Zhang, Edmund Wong, Eliz-\nabeth Proehl, Enoch Cheung, Eric Mitchell, Eric Wallace, Erik Ritter, Evan Mays, Fan Wang,\nFelipe Petroski Such, Filippo Raso, Florencia Leoni, Foivos Tsimpourlas, Francis Song, Fred\nvon Lohmann, Freddie Sulit, Geoff Salmon, Giambattista Parascandolo, Gildas Chabot, Grace\nZhao, Greg Brockman, Guillaume Leclerc, Hadi Salman, Haiming Bao, Hao Sheng, Hart An-\ndrin, Hessam Bagherinezhad, Hongyu Ren, Hunter Lightman, Hyung Won Chung, Ian Kivlichan,\nIan O’Connell, Ian Osband, Ignasi Clavera Gilaberte, Ilge Akkaya, Ilya Kostrikov, Ilya Sutskever,\nIrina Kofman, Jakub Pachocki, James Lennon, Jason Wei, Jean Harb, Jerry Twore, Jiacheng Feng,\nJiahui Yu, Jiayi Weng, Jie Tang, Jieqi Yu, Joaquin Quiñonero Candela, Joe Palermo, Joel Parish,\nJohannes Heidecke, John Hallman, John Rizzo, Jonathan Gordon, Jonathan Uesato, Jonathan\nWard, Joost Huizinga, Julie Wang, Kai Chen, Kai Xiao, Karan Singhal, Karina Nguyen, Karl\nCobbe, Katy Shi, Kayla Wood, Kendra Rimbach, Keren Gu-Lemberg, Kevin Liu, Kevin Lu,\nKevin Stone, Kevin Yu, Lama Ahmad, Lauren Yang, Leo Liu, Leon Maksin, Leyton Ho, Liam\nFedus, Lilian Weng, Linden Li, Lindsay McCallum, Lindsey Held, Lorenz Kuhn, Lukas Kon-\ndraciuk, Lukasz Kaiser, Luke Metz, Madelaine Boyd, Maja Trebacz, Manas Joglekar, Mark Chen,\nMarko Tintor, Mason Meyer, Matt Jones, Matt Kaufer, Max Schwarzer, Meghan Shah, Mehmet\nYatbaz, Melody Y. Guan, Mengyuan Xu, Mengyuan Yan, Mia Glaese, Mianna Chen, Michael\nLampe, Michael Malek, Michele Wang, Michelle Fradin, Mike McClay, Mikhail Pavlov, Miles\nWang, Mingxuan Wang, Mira Murati, Mo Bavarian, Mostafa Rohaninejad, Nat McAleese, Neil\nChowdhury, Neil Chowdhury, Nick Ryder, Nikolas Tezak, Noam Brown, Ofir Nachum, Oleg\nBoiko, Oleg Murk, Olivia Watkins, Patrick Chao, Paul Ashbourne, Pavel Izmailov, Peter Zhokhov,\nRachel Dias, Rahul Arora, Randall Lin, Rapha Gontijo Lopes, Raz Gaon, Reah Miyara, Reimar\nLeike, Renny Hwang, Rhythm Garg, Robin Brown, Roshan James, Rui Shu, Ryan Cheu, Ryan\nGreene, Saachi Jain, Sam Altman, Sam Toizer, Sam Toyer, Samuel Miserendino, Sandhini Agar-\nwal, Santiago Hernandez, Sasha Baker, Scott McKinney, Scottie Yan, Shengjia Zhao, Shengli Hu,\nShibani Santurkar, Shraman Ray Chaudhuri, Shuyuan Zhang, Siyuan Fu, Spencer Papay, Steph\nLin, Suchir Balaji, Suvansh Sanjeev, Szymon Sidor, Tal Broda, Aidan Clark, Tao Wang, Tay-\nlor Gordon, Ted Sanders, Tejal Patwardhan, Thibault Sottiaux, Thomas Degry, Thomas Dimson,\nTianhao Zheng, Timur Garipov, Tom Stasi, Trapit Bansal, Trevor Creech, Troy Peterson, Tyna\nEloundou, Valerie Qi, Vineet Kosaraju, Vinnie Monaco, Vitchyr Pong, Vlad Fomenko, Weiyi\nZheng, Wenda Zhou, Wes McCabe, Wojciech Zaremba, Yann Dubois, Yinghai Lu, Yining Chen,\nYoung Cha, Yu Bai, Yuchen He, Yuchen Zhang, Yunyun Wang, Zheng Shao, and Zhuohan Li.\n2024b. Openai o1 system card.\nOpenAI, Josh Achiam, Steven Adler, Sandhini Agarwal, Lama Ahmad, Ilge Akkaya, Floren-\ncia Leoni Aleman, Diogo Almeida, Janko Altenschmidt, Sam Altman, Shyamal Anadkat, Red\nAvila, Igor Babuschkin, Suchir Balaji, Valerie Balcom, Paul Baltescu, Haiming Bao, Moham-\nmad Bavarian, Jeff Belgum, Irwan Bello, Jake Berdine, Gabriel Bernadett-Shapiro, Christopher\nBerner, Lenny Bogdonoff, Oleg Boiko, Madelaine Boyd, Anna-Luisa Brakman, Greg Brockman,\nTim Brooks, Miles Brundage, Kevin Button, Trevor Cai, Rosie Campbell, Andrew Cann, Brit-\ntany Carey, Chelsea Carlson, Rory Carmichael, Brooke Chan, Che Chang, Fotis Chantzis, Derek\nChen, Sully Chen, Ruby Chen, Jason Chen, Mark Chen, Ben Chess, Chester Cho, Casey Chu,\nHyung Won Chung, Dave Cummings, Jeremiah Currier, Yunxing Dai, Cory Decareaux, Thomas\n21\n\nDegry, Noah Deutsch, Damien Deville, Arka Dhar, David Dohan, Steve Dowling, Sheila Dunning,\nAdrien Ecoffet, Atty Eleti, Tyna Eloundou, David Farhi, Liam Fedus, Niko Felix, Simón Posada\nFishman, Juston Forte, Isabella Fulford, Leo Gao, Elie Georges, Christian Gibson, Vik Goel,\nTarun Gogineni, Gabriel Goh, Rapha Gontijo-Lopes, Jonathan Gordon, Morgan Grafstein, Scott\nGray, Ryan Greene, Joshua Gross, Shixiang Shane Gu, Yufei Guo, Chris Hallacy, Jesse Han, Jeff\nHarris, Yuchen He, Mike Heaton, Johannes Heidecke, Chris Hesse, Alan Hickey, Wade Hickey,\nPeter Hoeschele, Brandon Houghton, Kenny Hsu, Shengli Hu, Xin Hu, Joost Huizinga, Shantanu\nJain, Shawn Jain, Joanne Jang, Angela Jiang, Roger Jiang, Haozhun Jin, Denny Jin, Shino Jo-\nmoto, Billie Jonn, Heewoo Jun, Tomer Kaftan, Łukasz Kaiser, Ali Kamali, Ingmar Kanitscheider,\nNitish Shirish Keskar, Tabarak Khan, Logan Kilpatrick, Jong Wook Kim, Christina Kim, Yongjik\nKim, Jan Hendrik Kirchner, Jamie Kiros, Matt Knight, Daniel Kokotajlo, Łukasz Kondraciuk, An-\ndrew Kondrich, Aris Konstantinidis, Kyle Kosic, Gretchen Krueger, Vishal Kuo, Michael Lampe,\nIkai Lan, Teddy Lee, Jan Leike, Jade Leung, Daniel Levy, Chak Ming Li, Rachel Lim, Molly\nLin, Stephanie Lin, Mateusz Litwin, Theresa Lopez, Ryan Lowe, Patricia Lue, Anna Makanju,\nKim Malfacini, Sam Manning, Todor Markov, Yaniv Markovski, Bianca Martin, Katie Mayer,\nAndrew Mayne, Bob McGrew, Scott Mayer McKinney, Christine McLeavey, Paul McMillan,\nJake McNeil, David Medina, Aalok Mehta, Jacob Menick, Luke Metz, Andrey Mishchenko,\nPamela Mishkin, Vinnie Monaco, Evan Morikawa, Daniel Mossing, Tong Mu, Mira Murati,\nOleg Murk, David Mély, Ashvin Nair, Reiichiro Nakano, Rajeev Nayak, Arvind Neelakantan,\nRichard Ngo, Hyeonwoo Noh, Long Ouyang, Cullen O’Keefe, Jakub Pachocki, Alex Paino, Joe\nPalermo, Ashley Pantuliano, Giambattista Parascandolo, Joel Parish, Emy Parparita, Alex Passos,\nMikhail Pavlov, Andrew Peng, Adam Perelman, Filipe de Avila Belbute Peres, Michael Petrov,\nHenrique Ponde de Oliveira Pinto, Michael, Pokorny, Michelle Pokrass, Vitchyr H. Pong, Tolly\nPowell, Alethea Power, Boris Power, Elizabeth Proehl, Raul Puri, Alec Radford, Jack Rae, Aditya\nRamesh, Cameron Raymond, Francis Real, Kendra Rimbach, Carl Ross, Bob Rotsted, Henri\nRoussez, Nick Ryder, Mario Saltarelli, Ted Sanders, Shibani Santurkar, Girish Sastry, Heather\nSchmidt, David Schnurr, John Schulman, Daniel Selsam, Kyla Sheppard, Toki Sherbakov, Jessica\nShieh, Sarah Shoker, Pranav Shyam, Szymon Sidor, Eric Sigler, Maddie Simens, Jordan Sitkin,\nKatarina Slama, Ian Sohl, Benjamin Sokolowsky, Yang Song, Natalie Staudacher, Felipe Pet-\nroski Such, Natalie Summers, Ilya Sutskever, Jie Tang, Nikolas Tezak, Madeleine B. Thompson,\nPhil Tillet, Amin Tootoonchian, Elizabeth Tseng, Preston Tuggle, Nick Turley, Jerry Tworek,\nJuan Felipe Cerón Uribe, Andrea Vallone, Arun Vijayvergiya, Chelsea Voss, Carroll Wainwright,\nJustin Jay Wang, Alvin Wang, Ben Wang, Jonathan Ward, Jason Wei, CJ Weinmann, Akila Weli-\nhinda, Peter Welinder, Jiayi Weng, Lilian Weng, Matt Wiethoff, Dave Willner, Clemens Winter,\nSamuel Wolrich, Hannah Wong, Lauren Workman, Sherwin Wu, Jeff Wu, Michael Wu, Kai Xiao,\nTao Xu, Sarah Yoo, Kevin Yu, Qiming Yuan, Wojciech Zaremba, Rowan Zellers, Chong Zhang,\nMarvin Zhang, Shengjia Zhao, Tianhao Zheng, Juntang Zhuang, William Zhuk, and Barret Zoph.\n2024c. GPT-4 Technical Report.\nJeonghwan Park. 2024. Logickor.\nSanghee Park and Geewook Kim. 2025. Evaluating multimodal generative AI with Korean educa-\ntional standards. In Proceedings of the 2025 Conference of the Nations of the Americas Chapter of\nthe Association for Computational Linguistics: Human Language Technologies (Volume 2: Short\nPapers), pages 671–688, Albuquerque, New Mexico. Association for Computational Linguistics.\nGuilherme Penedo, Hynek Kydlícek, Loubna Ben Allal, Anton Lozhkov, Margaret Mitchell,\nColin A. Raffel, Leandro von Werra, and Thomas Wolf. 2024. The fineweb datasets: Decanting\nthe web for the finest text data at scale. In Advances in Neural Information Processing Systems 38:\nAnnual Conference on Neural Information Processing Systems 2024, NeurIPS 2024, Vancouver,\nBC, Canada, December 10 - 15, 2024.\nJirui Qi, Raquel Fernández, and Arianna Bisazza. 2023. Cross-lingual consistency of factual knowl-\nedge in multilingual language models.\nIn Proceedings of the 2023 Conference on Empirical\nMethods in Natural Language Processing, pages 10650–10666, Singapore. Association for Com-\nputational Linguistics.\nQwen, :, An Yang, Baosong Yang, Beichen Zhang, Binyuan Hui, Bo Zheng, Bowen Yu, Chengyuan\nLi, Dayiheng Liu, Fei Huang, Haoran Wei, Huan Lin, Jian Yang, Jianhong Tu, Jianwei Zhang,\nJianxin Yang, Jiaxi Yang, Jingren Zhou, Junyang Lin, Kai Dang, Keming Lu, Keqin Bao, Kexin\nYang, Le Yu, Mei Li, Mingfeng Xue, Pei Zhang, Qin Zhu, Rui Men, Runji Lin, Tianhao Li, Tianyi\n22\n\nTang, Tingyu Xia, Xingzhang Ren, Xuancheng Ren, Yang Fan, Yang Su, Yichang Zhang, Yu Wan,\nYuqiong Liu, Zeyu Cui, Zhenru Zhang, and Zihan Qiu. 2025. Qwen2.5 technical report.\nQwen Team. 2025. Qwq-32b: Embracing the power of reinforcement learning.\nMorgane Rivière, Shreya Pathak, Pier Giuseppe Sessa, Cassidy Hardin, Surya Bhupatiraju, Léonard\nHussenot, Thomas Mesnard, Bobak Shahriari, Alexandre Ramé, Johan Ferret, Peter Liu, Pouya\nTafti, Abe Friesen, Michelle Casbon, Sabela Ramos, Ravin Kumar, Charline Le Lan, Sammy\nJerome, Anton Tsitsulin, Nino Vieillard, Piotr Stanczyk, Sertan Girgin, Nikola Momchev, Matt\nHoffman, Shantanu Thakoor, Jean-Bastien Grill, Behnam Neyshabur, Olivier Bachem, Alanna\nWalton, Aliaksei Severyn, Alicia Parrish, Aliya Ahmad, Allen Hutchison, Alvin Abdagic,\nAmanda Carl, Amy Shen, Andy Brock, Andy Coenen, Anthony Laforge, Antonia Paterson, Ben\nBastian, Bilal Piot, Bo Wu, Brandon Royal, Charlie Chen, Chintu Kumar, Chris Perry, Chris\nWelty, Christopher A. Choquette-Choo, Danila Sinopalnikov, David Weinberger, Dimple Vijayku-\nmar, Dominika Rogozinska, Dustin Herbison, Elisa Bandy, Emma Wang, Eric Noland, Erica Mor-\neira, Evan Senter, Evgenii Eltyshev, Francesco Visin, Gabriel Rasskin, Gary Wei, Glenn Cameron,\nGus Martins, Hadi Hashemi, Hanna Klimczak-Plucinska, Harleen Batra, Harsh Dhand, Ivan Nar-\ndini, Jacinda Mein, Jack Zhou, James Svensson, Jeff Stanway, Jetha Chan, Jin Peng Zhou, Joana\nCarrasqueira, Joana Iljazi, Jocelyn Becker, Joe Fernandez, Joost van Amersfoort, Josh Gordon,\nJosh Lipschultz, Josh Newlan, Ju-yeong Ji, Kareem Mohamed, Kartikeya Badola, Kat Black,\nKatie Millican, Keelin McDonell, Kelvin Nguyen, Kiranbir Sodhia, Kish Greene, Lars Lowe Sjö-\nsund, Lauren Usui, Laurent Sifre, Lena Heuermann, Leticia Lago, and Lilly McNealus. 2024.\nGemma 2: Improving open language models at a practical size. CoRR, abs/2408.00118.\nZhihong Shao, Peiyi Wang, Qihao Zhu, Runxin Xu, Junxiao Song, Xiao Bi, Haowei Zhang,\nMingchuan Zhang, YK Li, Y Wu, et al. 2024. Deepseekmath: Pushing the limits of mathematical\nreasoning in open language models. arXiv preprint arXiv:2402.03300.\nHyopil Shin, Sangah Lee, Dongjun Jang, Wooseok Song, Jaeyoon Kim, Chaeyoung Oh, Hyemi Jo,\nYoungchae Ahn, Sihyun Oh, Hyohyeong Chang, Sunkyoung Kim, and Jinsik Lee. 2025. Kobalt:\nKorean benchmark for advanced linguistic tasks.\nShivalika Singh, Angelika Romanou, Clémentine Fourrier, David I. Adelani, Jian Gang Ngui, Daniel\nVila-Suero, Peerat Limkonchotiwat, Kelly Marchisio, Wei Qi Leong, Yosephine Susanto, Ray-\nmond Ng, Shayne Longpre, Wei-Yin Ko, Madeline Smith, Antoine Bosselut, Alice Oh, Andre\nF. T. Martins, Leshem Choshen, Daphne Ippolito, Enzo Ferrante, Marzieh Fadaee, Beyza Ermis,\nand Sara Hooker. 2024. Global mmlu: Understanding and addressing cultural and linguistic biases\nin multilingual evaluation.\nGuijin Son, Hanwool Lee, Sungdong Kim, Seungone Kim, Niklas Muennighoff, Taekyoon Choi,\nCheonbok Park, Kang Min Yoo, and Stella Biderman. 2025. KMMLU: Measuring massive mul-\ntitask language understanding in Korean. In Proceedings of the 2025 Conference of the Nations of\nthe Americas Chapter of the Association for Computational Linguistics: Human Language Tech-\nnologies (Volume 1: Long Papers), pages 4076–4104, Albuquerque, New Mexico. Association for\nComputational Linguistics.\nGuijin Son, Hanwool Lee, Suwan Kim, Huiseo Kim, Jae cheol Lee, Je Won Yeom, Jihyu Jung,\nJung woo Kim, and Songseong Kim. 2024.\nHAE-RAE bench: Evaluation of Korean knowl-\nedge in language models. In Proceedings of the 2024 Joint International Conference on Compu-\ntational Linguistics, Language Resources and Evaluation (LREC-COLING 2024), pages 7993–\n8007, Torino, Italia. ELRA and ICCL.\nDan Su, Kezhi Kong, Ying Lin, Joseph Jennings, Brandon Norick, Markus Kliegl, Mostofa Patwary,\nMohammad Shoeybi, and Bryan Catanzaro. 2024a. Nemotron-cc: Transforming common crawl\ninto a refined long-horizon pretraining dataset. CoRR, abs/2412.02595.\nJianlin Su, Murtadha H. M. Ahmed, Yu Lu, Shengfeng Pan, Wen Bo, and Yunfeng Liu. 2024b.\nRoformer: Enhanced transformer with rotary position embedding. Neurocomputing, 568:127063.\nYang Sui, Yu-Neng Chuang, Guanchu Wang, Jiamu Zhang, Tianyi Zhang, Jiayi Yuan, Hongyi Liu,\nAndrew Wen, Shaochen Zhong, Hanjie Chen, et al. 2025. Stop overthinking: A survey on efficient\nreasoning for large language models. arXiv preprint arXiv:2503.16419.\n23\n\nMingjie Sun, Xinlei Chen, J. Zico Kolter, and Zhuang Liu. 2024.\nMassive activations in large\nlanguage models. CoRR, abs/2402.17762.\nNLLB Team, Marta R. Costa-jussà, James Cross, Onur Çelebi, Maha Elbayad, Kenneth Heafield,\nKevin Heffernan, Elahe Kalbassi, Janice Lam, Daniel Licht, Jean Maillard, Anna Sun, Skyler\nWang, Guillaume Wenzek, Al Youngblood, Bapi Akula, Loic Barrault, Gabriel Mejia Gonzalez,\nPrangthip Hansanti, John Hoffman, Semarley Jarrett, Kaushik Ram Sadagopan, Dirk Rowe, Shan-\nnon Spruit, Chau Tran, Pierre Andrews, Necip Fazil Ayan, Shruti Bhosale, Sergey Edunov, Angela\nFan, Cynthia Gao, Vedanuj Goswami, Francisco Guzmán, Philipp Koehn, Alexandre Mourachko,\nChristophe Ropers, Safiyyah Saleem, Holger Schwenk, and Jeff Wang. 2022. No language left\nbehind: Scaling human-centered machine translation.\nMichael Tschannen, Alexey Gritsenko, Xiao Wang, Muhammad Ferjad Naeem, Ibrahim Alabdul-\nmohsin, Nikhil Parthasarathy, Talfan Evans, Lucas Beyer, Ye Xia, Basil Mustafa, Olivier Hénaff,\nJeremiah Harmsen, Andreas Steiner, and Xiaohua Zhai. 2025. SigLIP 2: Multilingual Vision-\nLanguage Encoders with Improved Semantic Understanding, Localization, and Dense Features.\narXiv preprint arXiv:2502.14786.\nYue Wang, Qiuzhi Liu, Jiahao Xu, Tian Liang, Xingyu Chen, Zhiwei He, Linfeng Song, Dian Yu,\nJuntao Li, Zhuosheng Zhang, et al. 2025. Thoughts are all over the place: On the underthinking\nof o1-like llms. arXiv preprint arXiv:2501.18585.\nMaurice Weber, Daniel Y. Fu, Quentin Anthony, Yonatan Oren, Shane Adams, Anton Alexandrov,\nXiaozhong Lyu, Huu Nguyen, Xiaozhe Yao, Virginia Adams, Ben Athiwaratkun, Rahul Cha-\nlamala, Kezhen Chen, Max Ryabinin, Tri Dao, Percy Liang, Christopher Ré, Irina Rish, and\nCe Zhang. 2024a. Redpajama: an open dataset for training large language models. In Advances in\nNeural Information Processing Systems 38: Annual Conference on Neural Information Process-\ning Systems 2024, NeurIPS 2024, Vancouver, BC, Canada, December 10 - 15, 2024.\nMaurice Weber, Daniel Y. Fu, Quentin Anthony, Yonatan Oren, Shane Adams, Anton Alexandrov,\nXiaozhong Lyu, Huu Nguyen, Xiaozhe Yao, Virginia Adams, Ben Athiwaratkun, Rahul Cha-\nlamala, Kezhen Chen, Max Ryabinin, Tri Dao, Percy Liang, Christopher Ré, Irina Rish, and\nCe Zhang. 2024b. Redpajama: an open dataset for training large language models. NeurIPS\nDatasets and Benchmarks Track.\nXiaolin Xing, Zhiwei He, Haoyu Xu, Xing Wang, Rui Wang, and Yu Hong. 2024.\nEvaluat-\ning knowledge-based cross-lingual inconsistency in large language models.\narXiv preprint\narXiv:2407.01358.\nRuibin Xiong, Yunchang Yang, Di He, Kai Zheng, Shuxin Zheng, Chen Xing, Huishuai Zhang,\nYanyan Lan, Liwei Wang, and Tie-Yan Liu. 2020. On layer normalization in the transformer\narchitecture. In Proceedings of the 37th International Conference on Machine Learning, ICML\n2020, 13-18 July 2020, Virtual Event, volume 119 of Proceedings of Machine Learning Research,\npages 10524–10533. PMLR.\nMingyu Xu, Xin Men, Bingning Wang, Qingyu Zhang, Hongyu Lin, Xianpei Han, and Weipeng\nChen. 2024. Base of rope bounds context length. In Advances in Neural Information Processing\nSystems 38: Annual Conference on Neural Information Processing Systems 2024, NeurIPS 2024,\nVancouver, BC, Canada, December 10 - 15, 2024.\nAn Yang, Anfeng Li, Baosong Yang, Beichen Zhang, Binyuan Hui, Bo Zheng, Bowen Yu, Chang\nGao, Chengen Huang, Chenxu Lv, Chujie Zheng, Dayiheng Liu, Fan Zhou, Fei Huang, Feng Hu,\nHao Ge, Haoran Wei, Huan Lin, Jialong Tang, Jian Yang, Jianhong Tu, Jianwei Zhang, Jianxin\nYang, Jiaxi Yang, Jing Zhou, Jingren Zhou, Junyang Lin, Kai Dang, Keqin Bao, Kexin Yang,\nLe Yu, Lianghao Deng, Mei Li, Mingfeng Xue, Mingze Li, Pei Zhang, Peng Wang, Qin Zhu, Rui\nMen, Ruize Gao, Shixuan Liu, Shuang Luo, Tianhao Li, Tianyi Tang, Wenbiao Yin, Xingzhang\nRen, Xinyu Wang, Xinyu Zhang, Xuancheng Ren, Yang Fan, Yang Su, Yichang Zhang, Yinger\nZhang, Yu Wan, Yuqiong Liu, Zekun Wang, Zeyu Cui, Zhenru Zhang, Zhipeng Zhou, and Zihan\nQiu. 2025. Qwen3 technical report.\nGreg Yang and Edward J. Hu. 2021. Tensor programs IV: feature learning in infinite-width neural\nnetworks. In Proceedings of the 38th International Conference on Machine Learning, ICML\n24\n\n2021, 18-24 July 2021, Virtual Event, volume 139 of Proceedings of Machine Learning Research,\npages 11727–11737. PMLR.\nGreg Yang, Dingli Yu, Chen Zhu, and Soufiane Hayou. 2024. Tensor programs VI: feature learning\nin infinite depth neural networks. In The Twelfth International Conference on Learning Repre-\nsentations, ICLR 2024, Vienna, Austria, May 7-11, 2024. OpenReview.net.\nHaneul Yoo, Cheonbok Park, Sangdoo Yun, Alice Oh, and Hwaran Lee. 2024a. Code-switching\ncurriculum learning for multilingual transfer in llms. arXiv preprint arXiv:2411.02460.\nKang Min Yoo, Jaegeun Han, Sookyo In, Heewon Jeon, Jisu Jeong, Jaewook Kang, Hyunwook Kim,\nKyung-Min Kim, Munhyong Kim, Sungju Kim, Donghyun Kwak, Hanock Kwak, Se Jung Kwon,\nBado Lee, Dongsoo Lee, Gichang Lee, Jooho Lee, Baeseong Park, Seongjin Shin, Joonsang Yu,\nSeolki Baek, Sumin Byeon, Eungsup Cho, Dooseok Choe, Jeesung Han, Youngkyun Jin, Hyein\nJun, Jaeseung Jung, Chanwoong Kim, Jinhong Kim, Jinuk Kim, Dokyeong Lee, Dongwook Park,\nJeong Min Sohn, Sujung Han, Jiae Heo, Sungju Hong, Mina Jeon, Hyunhoon Jung, Jungeun\nJung, Wangkyo Jung, Chungjoon Kim, Hyeri Kim, Jonghyun Kim, Min Young Kim, Soeun Lee,\nJoonhee Park, Jieun Shin, Sojin Yang, Jungsoon Yoon, Hwaran Lee, Sanghwan Bae, Jeehwan\nCha, Karl Gylleus, Donghoon Ham, Mihak Hong, Youngki Hong, Yunki Hong, Dahyun Jang,\nHyojun Jeon, Yujin Jeon, Yeji Jeong, Myunggeun Ji, Yeguk Jin, Chansong Jo, Shinyoung Joo,\nSeunghwan Jung, Adrian Jungmyung Kim, Byoung Hoon Kim, Hyomin Kim, Jungwhan Kim,\nMinkyoung Kim, Minseung Kim, Sungdong Kim, Yonghee Kim, Youngjun Kim, Youngkwan\nKim, Donghyeon Ko, Dughyun Lee, Ha Young Lee, Jaehong Lee, Jieun Lee, Jonghyun Lee,\nJongjin Lee, Min Young Lee, Yehbin Lee, Taehong Min, Yuri Min, Kiyoon Moon, Hyangnam\nOh, Jaesun Park, Kyuyon Park, Younghun Park, Hanbae Seo, Seunghyun Seo, Mihyun Sim, Gyu-\nbin Son, Matt Yeo, Kyung Hoon Yeom, Wonjoon Yoo, Myungin You, Doheon Ahn, Homin Ahn,\nJoohee Ahn, Seongmin Ahn, Chanwoo An, Hyeryun An, Junho An, Sang-Min An, Boram Byun,\nEunbin Byun, Jongho Cha, Minji Chang, Seunggyu Chang, Haesong Cho, Youngdo Cho, Dal-\nnim Choi, Daseul Choi, Hyoseok Choi, Minseong Choi, Sangho Choi, Seongjae Choi, Wooy-\nong Choi, Sewhan Chun, Dong Young Go, Chiheon Ham, Danbi Han, Jaemin Han, Moonyoung\nHong, Sung Bum Hong, Dong-Hyun Hwang, Seongchan Hwang, Jinbae Im, Hyuk Jin Jang, Jae-\nhyung Jang, Jaeni Jang, Sihyeon Jang, Sungwon Jang, Joonha Jeon, Daun Jeong, Joonhyun Jeong,\nKyeongseok Jeong, Mini Jeong, Sol Jin, Hanbyeol Jo, Hanju Jo, Minjung Jo, Chaeyoon Jung,\nHyungsik Jung, Jaeuk Jung, Ju Hwan Jung, Kwangsun Jung, Seungjae Jung, Soonwon Ka, Dong-\nhan Kang, Soyoung Kang, Taeho Kil, Areum Kim, Beomyoung Kim, Byeongwook Kim, Dae-\nhee Kim, Dong-Gyun Kim, Donggook Kim, Donghyun Kim, Euna Kim, Eunchul Kim, Geewook\nKim, Gyu Ri Kim, Hanbyul Kim, Heesu Kim, Isaac Kim, Jeonghoon Kim, Jihye Kim, Joonghoon\nKim, Minjae Kim, Minsub Kim, Pil Hwan Kim, Sammy Kim, Seokhun Kim, Seonghyeon Kim,\nSoojin Kim, Soong Kim, Soyoon Kim, Sunyoung Kim, Taeho Kim, Wonho Kim, Yoonsik Kim,\nYou Jin Kim, Yuri Kim, Beomseok Kwon, Ohsung Kwon, Yoo-Hwan Kwon, Anna Lee, Byung-\nwook Lee, Changho Lee, Daun Lee, Dongjae Lee, Ha-Ram Lee, Hodong Lee, Hwiyeong Lee,\nHyunmi Lee, Injae Lee, Jaeung Lee, Jeongsang Lee, Jisoo Lee, Jongsoo Lee, Joongjae Lee,\nJuhan Lee, Jung Hyun Lee, Junghoon Lee, Junwoo Lee, Se Yun Lee, Sujin Lee, Sungjae Lee,\nSungwoo Lee, Wonjae Lee, Zoo Hyun Lee, Jong Kun Lim, Kun Lim, Taemin Lim, Nuri Na,\nJeongyeon Nam, Kyeong-Min Nam, Yeonseog Noh, Biro Oh, Jung-Sik Oh, Solgil Oh, Yeontaek\nOh, Boyoun Park, Cheonbok Park, Dongju Park, Hyeonjin Park, Hyun Tae Park, Hyunjung Park,\nJihye Park, Jooseok Park, Junghwan Park, Jungsoo Park, Miru Park, Sang Hee Park, Seunghyun\nPark, Soyoung Park, Taerim Park, Wonkyeong Park, Hyunjoon Ryu, Jeonghun Ryu, Nahyeon\nRyu, Soonshin Seo, Suk Min Seo, Yoonjeong Shim, Kyuyong Shin, Wonkwang Shin, Hyun Sim,\nWoongseob Sim, Hyejin Soh, Bokyong Son, Hyunjun Son, Seulah Son, Chi-Yun Song, Chiyoung\nSong, Ka Yeon Song, Minchul Song, Seungmin Song, Jisung Wang, Yonggoo Yeo, Myeong Yeon\nYi, Moon Bin Yim, Taehwan Yoo, Youngjoon Yoo, Sungmin Yoon, Young Jin Yoon, Hangyeol Yu,\nUi Seon Yu, Xingdong Zuo, Jeongin Bae, Joungeun Bae, Hyunsoo Cho, Seonghyun Cho, Yongjin\nCho, Taekyoon Choi, Yera Choi, Jiwan Chung, Zhenghui Han, Byeongho Heo, Euisuk Hong, Tae-\nbaek Hwang, Seonyeol Im, Sumin Jegal, Sumin Jeon, Yelim Jeong, Yonghyun Jeong, Can Jiang,\nJuyong Jiang, Jiho Jin, Ara Jo, Younghyun Jo, Hoyoun Jung, Juyoung Jung, Seunghyeong Kang,\nDae Hee Kim, Ginam Kim, Hangyeol Kim, Heeseung Kim, Hyojin Kim, Hyojun Kim, Hyun-Ah\nKim, Jeehye Kim, Jin-Hwa Kim, Jiseon Kim, Jonghak Kim, Jung Yoon Kim, Rak Yeong Kim,\nSeongjin Kim, Seoyoon Kim, Sewon Kim, Sooyoung Kim, Sukyoung Kim, Taeyong Kim, Naeun\nKo, Bonseung Koo, Heeyoung Kwak, Haena Kwon, Youngjin Kwon, Boram Lee, Bruce W. Lee,\n25\n\nDagyeong Lee, Erin Lee, Euijin Lee, Ha Gyeong Lee, Hyojin Lee, Hyunjeong Lee, Jeeyoon Lee,\nJeonghyun Lee, Jongheok Lee, Joonhyung Lee, Junhyuk Lee, Mingu Lee, Nayeon Lee, Sangkyu\nLee, Se Young Lee, Seulgi Lee, Seung Jin Lee, Suhyeon Lee, Yeonjae Lee, Yesol Lee, Young-\nbeom Lee, Yujin Lee, Shaodong Li, Tianyu Liu, Seong-Eun Moon, Taehong Moon, Max-Lasse\nNihlenramstroem, Wonseok Oh, Yuri Oh, Hongbeen Park, Hyekyung Park, Jaeho Park, Nohil\nPark, Sangjin Park, Jiwon Ryu, Miru Ryu, Simo Ryu, Ahreum Seo, Hee Seo, Kangdeok Seo,\nJamin Shin, Seungyoun Shin, Heetae Sin, Jiangping Wang, Lei Wang, Ning Xiang, Longxiang\nXiao, Jing Xu, Seonyeong Yi, Haanju Yoo, Haneul Yoo, Hwanhee Yoo, Liang Yu, Youngjae Yu,\nWeijie Yuan, Bo Zeng, Qian Zhou, Kyunghyun Cho, Jung-Woo Ha, Joonsuk Park, Jihyun Hwang,\nHyoung Jo Kwon, Soonyong Kwon, Jungyeon Lee, Seungho Lee, Seonghyeon Lim, Hyunkyung\nNoh, Seungho Choi, Sang-Woo Lee, Jung Hwa Lim, and Nako Sung. 2024b. Hyperclova x tech-\nnical report.\nQiying Yu, Zheng Zhang, Ruofei Zhu, Yufeng Yuan, Xiaochen Zuo, Yu Yue, Weinan Dai, Tiantian\nFan, Gaohong Liu, Lingjun Liu, et al. 2025. Dapo: An open-source llm reinforcement learning\nsystem at scale. arXiv preprint arXiv:2503.14476.\nZheng Yuan, Hongyi Yuan, Chengpeng Li, Guanting Dong, Chuanqi Tan, and Chang Zhou. 2023.\nScaling relationship on learning mathematical reasoning with large language models.\nCoRR,\nabs/2308.01825.\nYonghao Zhuang, Lanxiang Hu, Longfei Yun, Souvik Kundu, Zhengzhong Liu, Eric P. Xing, and\nHao Zhang. 2025. Scaling long context training data by long-distance referrals. In The Thirteenth\nInternational Conference on Learning Representations, ICLR 2025, Singapore, April 24-28, 2025.\nOpenReview.net.\n26\n\nA\nContributors\nWithin each role, names are listed in alphabetical order by last name, followed by the first name.\nCore Contributors\nSanghwan Bae\nMinseong Choi\nHyunsoo Ha\nChiheon Ham\nDonghoon Ham\nJaemin Han\nJiwoo Hong†\nYoungki Hong\nJinbae Im\nSookyo In\nYeguk Jin\nChansong Jo\nHwiyeol Jo\nShinyoung Joo\nJingu Kang\nDonghyeon Ko\nTaeho Kil\nByeongwook Kim\nDaehee Kim\nDonghyun Kim\nGeewook Kim\nHanbyul Kim\nHyunwoo Kim\nJeonghoon Kim\nJungwhan Kim\nMinkyoung Kim\nMunhyong Kim\nSeonghyun Kim\nSungdong Kim†\nSungju Kim\nYoonsik Kim\nYou Jin Kim\nDonghyun Kwak\nBeomseok Kwon\nBado Lee\nByungwook Lee\nGichang Lee\nHodong Lee\nInjae Lee\nJaehong Lee\nJeong Hyun Lee†\nJieun Lee\nJoosung Lee\nMin Young Lee\nNoah Lee\nSang-Woo Lee†\nYehbin Lee\nYujeong Lee\nTaehong Min\nKiyoon Moon\nJeongYeon Nam\nYeontaek Oh\nCheonbok Park\nJoonsuk Park\nKyuyon Park\nSanghee Park\nAhreum Seo\nSeunghyun Seo\nSuk Min Seo\nSeongjin Shin\nKa Yeon Song\nNako Sung\nMoonbin Yim\nKang Min Yoo\nTaehwan Yoo\nMyungIn You\nHangyeol Yu\nContributors\nSang Min An\nJeongin Bae\nChongho Cha\nEungsup Cho\nHaesong Cho\nSaerim Cho\nHyungwook Choi\nJaepil Choi †\nSanghyuk Choi\nJaehyeok Doo †\nSungbum Hong\nSeongchan Hwang\nDonghoon Jang\nGenie Jang\nJunseo Jang\nHeewon Jeon\nMina Jeon\nKyeongseok Jeong\nYelim Jeong\nMyunggeun Ji\nYoungkyun Jin\nAra Jo\nHyunhoon Jung\nKwangsun Jung\nSeunghwan Jung\nDain Kim †\nDong Gyun Kim\nEunchul Kim\nGinam Kim\nHyomin Kim\nHyunwook Kim\nJihye Kim\nJiseob Kim\nJonghak Kim\nJoonghoon Kim †\nMinseung Kim\nMinyoung Kim\nSingon Kim\n27\n\nSoyoon Kim\nTaeyong Kim\nYonghee Kim\nYoungjun Kim\nOhsung Kwon\nYoo Hwan Kwon\nYoungjin Kwon\nDagyeong Lee\nDughyun Lee\nGayoung Lee\nHa Ram Lee\nHagyeong Lee\nJeonghyun Lee\nJonghyun Lee\nJongjin Lee\nJoonhyung Lee\nJunghoon Lee\nSeulgi Lee\nSoeun Lee\nSujin Lee\nSungwoo Lee\nYesol Lee\nYoungbeom Lee\nTaemin Lim\nKyeong Min Nam\nBiro Oh\nSolgil Oh\nGunho Park\nWonkyeong Park\nJieun Shin\nWonkwang Shin\nChiyun Song\nHae Jin Song\nMinchul Song\nJisung Wang\nSukwon Yeo\nHwanhee Yoo\nWonjoon You\nUiseon Yu\n† Work done while at NAVER Cloud.\n28\n\nB\nPerformance on Math&Coding Benchmarks\nCategory\nBenchmarks\nHCX\nTHINK\nQwen3\nQwen3\nQwQ\nEXAONE\nDeep\n( - )\n(32B)\n(14B)\n(32B)\n(32B)\nMath\nGSM8K\n95.5\n95.9\n95.9\n96.2\n95.5\nGSM8K-ko\n92.1\n93.4\n92.6\n92.7\n91.8\nMATH500\n95.2\n97.2\n96.8\n98.0\n96.0\nMATH100-ko\n90.5\n92.9\n92.9\n92.9\n93.9\nCoding\nHumanEval\n95.7\n96.9\n95.7\n90.8\n95.1\nMBPP\n89.9\n78.6\n90.8\n90.1\n87.5\nTable 6: Performance comparison of language models on math and coding benchmarks. Differences\namong the baseline reasoning models are marginal, as these benchmarks are approaching saturation.\nC\nKoBigBench Qualitative Examples\nHyperCLOVA X THINK (Translated Model Input&Output – English)\n<|im_start|>tool_list\n<|im_end|>\n<|im_start|>system\nThe following are multiple choice questions with explanations and/or answers about\nsocial-ecofin201.<|im_end|>\n<|im_start|>user\nQuestion: What is the most reasonable measure the government can take to prevent the\n\"transaction cliff\" shock in the real estate market?\nRead the question again: What is the most reasonable measure the government can take to\nprevent the \"transaction cliff\" shock in the real estate market?\nOptions:\n(A). Increase real estate transaction tax rate\n(B). Increase property tax rate\n(C). Strengthen safety inspections for reconstruction\n(D). Lift designation of regulated areas<|im_end|>\n<|im_start|>assistant/think\nThis question asks for the most reasonable measure the government can take to address\nthe \"transaction cliff\" problem in the real estate market today. We have four options from\nA to D. I need to examine each option to see which one would help prevent a transaction cliff.\nFirst, I need to understand exactly what a \"transaction cliff\" is. It seems to refer to a\nphenomenon where real estate transaction volume rapidly decreases. This term is typically\nused when the real estate market is in a slump or when regulations are tightened, leading to\na contraction in transactions. Therefore, to prevent this, the government would likely need\nto implement policies that stimulate transactions.\nLet’s analyze each option one by one.\n(A) Increase real estate transaction tax rate: Transaction taxes refer to taxes like acquisition\ntax or registration tax paid when buying or selling real estate. Increasing the transaction\ntax rate would increase transaction costs, which would likely further reduce transactions.\nThis would worsen the transaction cliff rather than prevent it. So, A does not seem like an\n29\n\nappropriate measure.\n(B) Increase property tax rate: Property tax refers to taxes paid while owning a home,\nsuch as property tax or comprehensive real estate tax. Increasing property taxes might lead\nsome homeowners to put their properties on the market due to the tax burden. However,\nconversely, existing owners might also choose to hold onto their properties. It’s also\nsomewhat ambiguous whether an increase in property taxes would directly stimulate\ntransactions. There’s also an argument that increasing property taxes could lead to a de-\ncrease in transactions itself. So, B also seems inappropriate for preventing a transaction cliff.\n(C) Strengthen safety inspections for reconstruction: Strengthening safety inspections for\nreconstruction would reduce the supply through redevelopment. However, whether this\nis relevant depends on whether the transaction cliff is due to a supply shortage or other\nfactors. If the lack of transactions is due to insufficient new supply through reconstruction,\nthen relaxation, not strengthening, might be needed. But the question asks for a measure\nto prevent a transaction cliff, and strengthening safety inspections for reconstruction could\nfurther reduce supply, potentially negatively impacting the market. Therefore, C also seems\ninappropriate.\n(D) Lift designation of regulated areas: Regulated areas refer to regions where subscription,\nloan, and resale restrictions apply. Lifting the designation of regulated areas would ease\ntransaction regulations in those areas, which could stimulate transactions, right? For\nexample, if loan regulations are eased or resale restrictions are removed, it would become\neasier to conduct transactions, potentially increasing transaction volume. Therefore, lifting\nthe designation of regulated areas could help prevent a transaction cliff.\nHowever, even with this thought, I need to consider more specifically how lifting the\ndesignation of regulated areas actually impacts transaction activation. If regulated areas\nare lifted, they would be removed from speculative overheated zones or adjustment target\nareas, leading to relaxed subscription conditions, increased loan limits, and even allowing\nmultiple homeowners to get loans, thereby easing transaction-related regulations. This\ncould increase buying demand and lead to more active transactions. On the other hand,\nthere might be concerns that lifting regulated areas could induce speculation and cause\nmarket overheating. However, the question specifically asks for a measure to prevent a\ntransaction cliff, i.e., a rapid decrease in transactions, so regulatory easing seems appropriate.\nComparing each option, A is inappropriate because it increases transaction costs and\nreduces transactions. B might lead to property sales due to increased holding tax but is\nless directly related to transaction activation. C could lead to reduced supply and further\ndecrease transactions. D could promote transactions by easing regulations, so D seems to be\nthe most reasonable measure.\nHowever, one lingering question is whether lifting the designation of regulated areas can\nbe a direct solution to the transaction cliff. For example, if the current real estate market\nis in a slump and transactions have decreased, easing regulations could stimulate demand\nand increase transactions. Conversely, if the market was overheated and regulations were\ntightened, easing them could lead to re-overheating, but the question is about preventing a\ntransaction cliff, a decrease in transactions, so regulatory easing seems correct.\nAlso, another possibility is that if the transaction cliff is due to external factors like rising\ninterest rates or an economic downturn, the government’s regulatory easing alone might\nhave limited effect. However, the question asks to choose from government measures, so\nit’s about selecting the most appropriate policy tool.\nIn summary, to increase transactions, lowering transaction costs or easing regulations\nwould be helpful. Increasing the transaction tax rate (A) increases costs and is therefore\nopposite. Increasing the property tax rate (B) increases the burden of ownership, which\n30\n\nmight incentivize selling, but it’s less directly linked to transaction activation. Strengthening\nsafety inspections for reconstruction (C) could lead to a decrease in supply and thus\nnegatively impact transaction activation. Lifting the designation of regulated areas (D) eases\nregulations and can promote transactions, so it seems to be the most appropriate measure.\nHowever, I need to re-evaluate whether the assumption that lifting the designation of\nregulated areas helps activate transactions is correct. Regulated areas are designated as\nspeculative overheated or adjustment target areas. Lifting these designations can ease loan\nregulations, resale restrictions, etc., making transactions easier. For example, if the resale\nrestriction period is shortened, short-term trading becomes possible, which can increase\ntransaction volume. If loan regulations are eased, real demanders can more easily access\nhousing purchases, which can stimulate transactions. Therefore, lifting the designation of\nregulated areas can help alleviate the transaction cliff.\nOn the other hand, if the current transaction cliff is due to reduced liquidity from interest\nrate hikes or decreased demand due to an economic downturn, even regulatory easing\nmight not be enough to increase transactions. However, the question asks for a government\nmeasure to prevent the transaction cliff in the real estate market, so among the given options,\nD seems the most appropriate.\nLet’s double-check the other options.\n(A) Increase real estate transaction tax rate: This would definitely increase transaction costs\nand reduce transactions, thus deepening the transaction cliff rather than preventing it. So, A\nis incorrect.\n(B) Increase property tax rate: While some argue that increasing property taxes might\nmake homeowners feel the tax burden and put properties on the market, conversely, they\nmight also hold onto their properties. Also, increasing property taxes primarily affects\nmulti-homeowners or owners of high-value properties. While their putting properties on\nthe market could increase supply and activate transactions, this can vary depending on\nthe situation. However, generally, an increase in property taxes is more focused on price\nstabilization through increased supply rather than transaction activation. Therefore, B might\nnot be an appropriate direct measure to solve the transaction cliff itself.\n(C) Strengthen safety inspections for reconstruction: Expanding supply through reconstruc-\ntion can contribute to market stability by increasing housing supply. However, strengthening\nsafety inspections makes reconstruction more difficult, thus reducing supply. Reduced\nsupply can lead to upward price pressure but is a separate issue from transaction activation.\nIf the lack of transactions is due to a supply shortage, then supply needs to be increased,\nbut if the current transaction cliff is due to other factors, this measure would be ineffective.\nTherefore, C seems inappropriate for activating transactions.\n(D) Lift designation of regulated areas: As I considered earlier, easing regulations can\nmake transactions easier, potentially increasing transaction volume, so D seems the most\nappropriate.\nFurthermore, recalling the actual impact of lifting regulated area designations on trans-\naction activation in Korean real estate policy, for example, the government lifted some\nregulated areas in late 2022 or early 2023. At that time, the real estate market was\ncooling, and the purpose was to stimulate transactions by easing regulations. Therefore,\nin actual cases, lifting regulated area designations has been used as a policy to alleviate\ntransaction cliffs, which increases the likelihood that D is the correct answer in this problem.\nIn conclusion, analyzing each option, (D) Lifting designation of regulated areas seems to be\nthe most reasonable measure. <|im_end|> <|im_start|>assistant\nAnswer: D<|im_end|>\n31\n\nHyperCLOVA X THINK (Model Input&Output – Korean)\n<|im_start|>tool_list\n<|im_end|> <|im_start|>system\nThe following are multiple choice questions with explanations and/or answers about\nsocial-ecofin201.<|im_end|>\n<|im_start|>user\nQuestion: 부동산시장의’거래절벽’ 충격을막기위해정부가취할수있는조치로가장\n합리적인것은?\nRead the question again: 부동산시장의’거래절벽’ 충격을막기위해정부가취할수\n있는조치로가장합리적인것은?\nOptions:\n(A). 거래세율인상\n(B). 보유세율인상\n(C). 재건축안전진단강화\n(D). 규제지역해제<|im_end|>\n<|im_start|>assistant/think\n오늘부동산시장의’거래절벽’ 문제를해결하기위해정부가취할수있는조치중에서\n가장합리적인것을고르는문제네요. 문제는선택지A부터D까지네가지옵션이\n주어졌어요. 각옵션을하나씩살펴보면서어떤것이거래절벽을막는데도움이될지\n생각해봐야겠어요.\n먼저, ’거래절벽’이무엇인지부터정확히이해해야할것같아요. 거래절벽이란부동산\n거래량이급격히감소하는현상을말하는것같은데, 보통부동산시장이침체기에\n빠지거나규제가강화되어거래가위축될때사용되는용어인것같아요. 그렇다면\n정부가이를막기위해서는거래를활성화시킬수있는정책을펴야하지않을까싶네요.\n각선택지를하나씩분석해볼게요.\n(A) 거래세율인상: 거래세란부동산을사고팔때내는취득세나등록세같은것을\n말하는것같아요. 거래세율을인상하면거래비용이늘어나게되므로, 거래를더줄이는\n효과가있을것같은데. 그러면거래절벽을막기보다는오히려악화시키지않을까?\n따라서A는적절한조치가아닐것같아요.\n(B) 보유세율인상: 보유세는주택을소유하고있는동안내는세금, 예를들어재산세나\n종합부동산세같은걸말하죠. 보유세를올리면집을가진사람들이세금부담을느껴\n매물을내놓을수도있겠지만, 반대로생각하면기존소유자들이매물을거둬들이게될\n수도있을것같아요. 하지만보유세인상이거래활성화에직접적인영향을미칠지는좀\n애매한것같아요. 특히보유세를올리면거래자체가줄어들수있다는의견도있을것\n같고. 그래서B도거래절벽을막는데는부적절할것같네요.\n(C) 재건축안전진단강화: 재건축안전진단을강화하면재건축을통한공급이줄어들게\n되겠죠. 그런데거래절벽이공급부족으로인한것인지, 아니면다른요인때문인지에\n따라다를것같아요. 만약재건축을통한신규공급이부족해서거래가안된다면강화가\n아니라완화가필요할수도있지않을까요? 하지만문제에서는거래절벽을막기위한\n조치라고했으니, 재건축안전진단강화는공급을더줄여서오히려시장에부정적인\n영향을줄수도있을것같아요. 따라서C도적절하지않을것같네요.\n(D) 규제지역해제: 규제지역이란청약, 대출, 전매제한등이적용되는지역을말하는것\n같아요. 규제지역을해제하면해당지역에서의거래규제가완화되어거래가활성화될\n수있지않을까? 예를들어, 대출규제가완화되거나전매제한이없어지면거래를하기\n쉬워져서거래량이늘어날수있을것같아요. 따라서규제지역해제는거래절벽을막는\n데도움이될수있을것같네요.\n하지만이렇게생각해보지만, 실제로규제지역해제가거래활성화에어떤영향을미치는\n지좀더구체적으로알아볼필요가있을것같아요. 규제지역이해제되면투기과열지구,\n조정대상지역등에서벗어나게되어서청약조건이완화되고, 대출한도가늘어나거나,\n다주택자도대출을받을수있게되는등거래관련규제가완화되겠죠. 이렇게되면\n32\n\n매수수요가늘어나거래가활발해질수있을것같아요. 반면에규제지역해제가오히려\n투기를유발하여시장과열을초래할수도있다는우려도있을수있지만, 문제에서는\n거래절벽, 즉거래가급격히줄어드는현상을막기위한조치이므로규제완화가도움이\n될것같아요.\n그래서각선택지를비교해보면, A는거래비용을늘려거래를줄이므로부적절하고, B\n는보유세인상으로매물출회가있을수도있지만직접적인거래활성화보다는다른\n효과를가져올수있으며, C는공급감소로거래를더줄일수있고, D는규제완화로\n거래를촉진할수있으므로가장합리적인것은D가아닐까생각해요.\n하지만여기서한가지의문이드는점이있어요. 규제지역해제가거래절벽을해결하는\n직접적인방법이될수있을까요? 예를들어, 현재부동산시장이침체기라서거래가\n줄어든상황이라면, 규제를풀어주는것이수요를자극하여거래를늘릴수있을거예요.\n반면에시장이과열되어서규제를강화한상황이라면, 규제를완화하면다시과열될수\n있지만, 문제에서는거래절벽, 즉거래감소상황을막기위한것이므로규제완화가\n맞을것같아요.\n또한다른가능성으로는, 거래절벽이금리상승이나경기침체등외부요인에의한\n것이라면정부의규제완화만으로는효과가제한적일수있지만, 문제에서는정부의\n정책중에서선택하라고했으니정책적수단중에서가장적절한것을고르는문제일\n거예요.\n정리하자면, 거래를늘리기위해서는거래비용을낮추거나규제를완화하는것이도움이\n될텐데, 거래세율인상(A)은비용을높이므로반대이고, 보유세인상(B)은보유부담을\n높여매도유인은있지만거래활성화와는직접연관이적을것같아요. 재건축안전진단\n강화(C)는공급감소로작용할수있으니역시거래활성화에부정적일것같고, 규제지역\n해제(D)는규제를완화하여거래를촉진할수있으므로가장적절한조치로보여요.\n하지만여기서규제지역해제가거래활성화에도움이된다는가정이맞는지다시한\n번생각해봐야할것같아요. 규제지역으로지정된지역은투기과열이나조정대상지역\n등인데, 이지역들을해제하면대출규제, 전매제한등이완화되어거래가용이해질\n수있습니다. 예를들어, 전매제한기간이줄어들면단기매매가가능해져서거래량이\n늘어날수있고, 대출규제가완화되면실수요자들이주택구입에더쉽게접근할수\n있게되어거래가활성화될수있겠죠. 따라서규제지역해제는거래절벽을완화하는데\n도움이될수있을것같아요.\n반면에, 만약현재거래절벽이금리인상으로인한유동성축소나경기침체에따른수요\n감소때문이라면, 규제완화로도거래를늘리기어려울수있지만, 문제에서는부동산\n시장의거래절벽을막기위한정부의조치중에서선택하라고했으므로, 주어진선택지\n중에서는D가가장적절하다고봐야할것같아요.\n그럼다른선택지들을다시한번체크해보죠.\n(A) 거래세율인상: 이건확실히거래비용을증가시켜거래를줄이므로, 거래절벽을\n막기보다는심화시킬것같아요. 따라서A는틀린답.\n(B) 보유세율인상: 보유세를올리면집주인들이세금부담을느껴매물을내놓을수\n있다는의견도있지만, 반대로매물을내놓지않고버티기를할수도있어요. 또한, 보유세\n인상은주로다주택자나고가주택소유자에게타격을줄텐데, 이들이매물을내놓으면\n공급이늘어나거래가활성화될수도있겠지만, 이는상황에따라다를수있어요. 하지만\n일반적으로보유세인상은거래활성화보다는공급증가를통한가격안정화에더초점이\n맞춰져있을것같아요. 따라서거래절벽자체를해결하기위한직접적인조치로는B가\n적절하지않을수있어요.\n(C) 재건축안전진단강화: 재건축을통한공급확대는주택공급을늘려서시장안정에\n기여할수있지만, 안전진단을강화하면재건축이어려워져서공급이줄어들게되죠.\n공급감소는가격상승압력으로작용할수있지만, 거래활성화와는별개문제일수\n33\n\n있어요. 만약공급이부족해서거래가안된다면공급을늘려야하지만, 현재거래절벽은\n공급부족보다는다른요인때문이라면이조치는효과가없을거예요. 따라서C는거래\n활성화를위한조치로는부적절해보여요.\n(D) 규제지역해제: 앞서생각한것처럼규제를완화하면거래가용이해져서거래량이\n늘어날수있으므로D가가장적절하다고생각됩니다.\n하지만실제로한국의부동산정책에서규제지역해제가거래활성화에미친영향은\n어땠는지기억을더듬어보면, 예를들어2022년말이나2023년초에정부가일부규제지\n역을해제한적이있었어요. 그당시에는부동산시장이냉각되어있었기때문에규제를\n완화해서거래를촉진하려는목적이었을거예요. 따라서실제사례에서도규제지역\n해제가거래절벽완화에도움이되는정책으로사용된바있으니, 이문제에서도D가\n정답일가능성이높아보여요.\n결론적으로, 각옵션을분석해보면(D) 규제지역해제가가장합리적인조치로보입니\n다.<|im_end|> <|im_start|>assistant Answer: D<|im_end|>\nHyperCLOVA X THINK (Translated Model Input&Output – English)\n<|im_start|>tool_list\n<|im_end|> <|im_start|>system\nThe following are multiple choice questions with explanations and/or answers about\ncommonsense-context30.<|im_end|>\n<|im_start|>user\nQuestion: Context: Everyone eventually came back to me and made me feel loved. It\nwas truly wonderful. It’s amazing and touching that I have such wonderful friends by my\nside. I think it felt even more special because I hadn’t seen them in such a long time. It’s\nstrangely surprising to have returned like this, but meeting people again is really a great\nthing. Especially Donghwan.\nQuestion: If I hadn’t left, how would I have felt?\nRead the question again: Context: Everyone eventually came back to me and made me feel\nloved. It was truly wonderful. It’s amazing and touching that I have such wonderful friends\nby my side. I think it felt even more special because I hadn’t seen them in such a long time.\nIt’s strangely surprising to have returned like this, but meeting people again is really a great\nthing. Especially Donghwan.\nQuestion: If I hadn’t left, how would I have felt?\nOptions:\n(A). Donghwan would have hated me, and nothing would have happened.\n(B). I wouldn’t have missed these people this much.\n(C). It’s still joyful and touching to have friends who hurt me.\n(D). It would have felt strange not to have left home at all.<|im_end|>\n<|im_start|>assistant/think\nToday, the user provided context and a question, requiring me to select the most appropriate\nanswer. I’ll first carefully examine the context provided. The speaker mentions feeling loved\nand touched because friends returned to their side, emphasizing how special it felt due to\nthe long absence, especially highlighting Donghwan. The speaker mentions that returning\nfelt strangely surprising but meeting people again was truly great.\nThe question asks, \"If I hadn’t left, how would I have felt?\" Currently, the speaker is moved\nbecause of the reunion after leaving and returning. If they had never left, the intense feelings\nrelated to the reunion might not have existed.\nAnalyzing each option individually:\n(A). Donghwan would have hated me, and nothing would have happened.\nThis doesn’t align with the context. There is no indication in the context that Donghwan or\n34\n\nothers would have felt negatively if the speaker hadn’t left.\n(B). I wouldn’t have missed these people this much.\nThis is logical. The speaker emphasizes that the long absence made the reunion more special\nand touching. Without leaving, the feeling of longing would naturally be less intense.\n(C). It’s still joyful and touching to have friends who hurt me.\nThis is incorrect. The context never indicates that the friends were hurtful. Rather, it focuses\non positive and warm feelings upon reunion.\n(D). It would have felt strange not to have left home at all.\nThe context describes the return as \"strangely surprising,\" suggesting that leaving and\nreturning was unusual. If the speaker had never left, it might have felt normal rather than\nstrange to remain home. Therefore, option D is somewhat ambiguous but doesn’t strongly\nalign with the context.\nGiven the context and the reasoning above, option (B) stands out as the most appropriate\nchoice. The emotional impact of missing friends after a long absence is clearly highlighted,\nand thus, had the speaker never left, these strong feelings of longing would not have existed.\nHence, the correct answer is option (B).<|im_end|>\n<|im_start|>assistant\nAnswer: B<|im_end|>\nHyperCLOVA X THINK (Model Input&Output – Korean)\n<|im_start|>tool_list\n<|im_end|> <|im_start|>system\nThe following are multiple choice questions with explanations and/or answers about\ncommonsense-context30.<|im_end|>\n<|im_start|>user\nQuestion: 맥락: 모두가결국저한테돌아와줬고제가사랑받고있다고느끼게했어요.\n진짜최고였네요. 진짜이런멋진친구들이제곁에있다는게너무좋고감동적이에요.\n특히오랫동안못봐서더그랬던거같아요. 이렇게돌아오게된게이상할정도지만\n사람들이랑다시만난다는것은진짜좋은일이에요. 특히동환이요.\n질문: 만약내가떠나지않았더라면내가어떻게느꼈을까?\nRead the question again: 맥락: 모두가결국저한테돌아와줬고제가사랑받고있다고\n느끼게했어요. 진짜최고였네요. 진짜이런멋진친구들이제곁에있다는게너무좋고\n감동적이에요. 특히오랫동안못봐서더그랬던거같아요. 이렇게돌아오게된게이상할\n정도지만사람들이랑다시만난다는것은진짜좋은일이에요. 특히동환이요.\n질문: 만약내가떠나지않았더라면내가어떻게느꼈을까?\nOptions:\n(A). 동환이가나를미워했을거라아무일도없었을것이다.\n(B). 이만큼이사람들을그리워하진않았을것이다.\n(C). 상처주는친구들이있다는것이여전히기쁘고감동적이다.\n(D). 집을전혀떠나지않았다는것이이상하게느껴졌을것이다.<|im_end|>\n<|im_start|>assistant/think\n오늘사용자가맥락과질문을주고, 그에맞는답을선택하는문제를내주었어. 먼저\n맥락을자세히읽어봐야겠지. 맥락을보면, 주인공은친구들이돌아와주고사랑받는다고\n느껴서감동적이고좋았다고말하고있어. 특히오랫동안못봤던친구들, 특히동환이를\n다시만나게되어서더그랬다고해. 그리고\"이렇게돌아오게된게이상할정도지만\"\n이라고했어. 이게중요한부분일것같아.\n질문은\"만약내가떠나지않았더라면내가어떻게느꼈을까?\"야. 현재상황은주인공이\n떠났다가다시돌아왔고, 그로인해친구들이돌아와줘서감동받은상황이야. 그런데\n만약떠나지않았다면어떻게느꼈을까를묻는거지.\n35\n\n선택지를하나씩분석해봐야겠어.\n(A) 동환이가나를미워했을거라아무일도없었을것이다.\n이건맥락과좀안맞는것같아. 현재상황은친구들이돌아와줘서좋은상황인데, 떠나지\n않았다면동환이가미워했을거라니? 맥락에서는떠나서다시돌아왔을때친구들이\n돌아와줬으니, 떠나지않았다면친구들이떠나지않았을까? 그런데이건좀헷갈리네.\n아니면주인공이떠났기때문에친구들이돌아온것일까? 맥락에서\"모두가결국저한테\n돌아와줬고\"라고했으니까, 주인공이떠난후친구들이돌아온걸수도있겠지. 그런데\n만약떠나지않았다면친구들이돌아올필요가없었을거고, 그래서그감동을느끼지\n못했을수도있겠다. 그런데(A)는동환이가미워했을거라고하니맥락과연결이잘안\n돼보여.\n(B) 이만큼이사람들을그리워하진않았을것이다. 이건가능할것같아. 주인공이\n떠났기때문에친구들을오랫동안못봤고, 그래서다시만났을때더감동적이었던\n거잖아. 만약떠나지않았다면오랫동안못보지않았을테니그렇게까지그리워하지\n않았을거라는말이되겠지. 맥락에서\"특히오랫동안못봐서더그랬던거같아요\"라고\n했으니, 떠나지않았다면그부분이없었을테니까. 이선택지는타당해보여.\n(C) 상처주는친구들이있다는것이여전히기쁘고감동적이다.\n이건맥락과맞지않아. 현재상황은친구들이돌아와서감동적이지만, 상처주는\n친구들이라면? 맥락에서는친구들이돌아와서좋은상황이니까, 상처주는친구들이\n있다는게아니라오히려돌아와서좋은상황이잖아. 따라서(C)는부적절한것같아.\n(D) 집을전혀떠나지않았다는것이이상하게느껴졌을것이다.\n이건좀애매해. 현재상황에서\"이렇게돌아오게된게이상할정도지만사람들이랑\n다시만난다는것은진짜좋은일이에요\"라고했으니까, 돌아오는게이상했지만좋은\n일이라고했어. 만약집을떠나지않았다면, 돌아오는상황자체가없으니까, \"집을\n전혀떠나지않았다는것이이상하게느껴졌을것이다\"라고하면, 현재상황에서집을\n떠난것이이상했지만좋은거라면, 떠나지않았을때는그이상함이없으니이상하게\n느껴지지않을까? 그런데이건좀추측이필요해. 맥락에서\"이렇게돌아오게된게\n이상할정도\"라고했으니까, 떠나지않았다면돌아올필요가없으니그이상함은없었을\n거고, 따라서집을떠나지않았다는것이이상하게느껴지지않을것같은데. 아니면원래\n집을떠나는게일반적인상황이라면, 떠나지않았을때오히려이상하게느껴질수도\n있을까? 이건좀애매한것같아.\n다시질문으로돌아가서, \"만약내가떠나지않았더라면내가어떻게느꼈을까?\"라고\n했을때, 현재상황은떠났다가돌아와서친구들을다시만나감동받았어. 따라서떠나지\n않았다면친구들과떨어져있지않았을테니, 다시만나는감동은없었을거야. 특히\n오랫동안못봤기때문에더감동적이었던부분이있으므로, 떠나지않았다면그만큼\n그리워하지않았을것(B)이라는말이맞을것같아.\n(A)는동환이가미워했을거라니맥락과안맞고, (C)는상처주는친구들이라는게말이\n안되고, (D)는집을떠나지않은것이이상하게느껴진다는건데, 현재상황에서집을\n떠난것이이상했지만좋은거라면, 떠나지않았을때는그이상함이없으니이상하지\n않을것같은데. 따라서(D)는애매하고, (B)가가장자연스러워보여.\n하지만맥락에서\"이렇게돌아오게된게이상할정도지만사람들이랑다시만난다는것\n은진짜좋은일이에요\"라고했으니, 돌아오는게예상밖의일이었지만좋은일이었다는\n거지. 만약떠나지않았다면돌아올필요가없으니, 돌아올일이없어서그좋은일을\n경험하지못했을거야. 따라서\"이만큼이사람들을그리워하진않았을것이다\"라는(B)\n가맞을것같아. 왜냐하면떠나서오랫동안못봤기때문에그리워했고, 다시만나서\n감동적이었으니까, 떠나지않았다면그렇게까지그리워하지않았을테니까.\n그래서정답은(B)인것같아.<|im_end|>\n<|im_start|>assistant\nAnswer: B<|im_end|>\n36\n\nD\nQualitative Examples in Vision Adaptation Ablation\nWe present below a representative example from the KCSAT STEM vision benchmark. All prob-\nlems are rendered and presented to the model strictly as images, without any accompanying text\ninput. This setup requires the model to make use of its full vision capability–not only to recognize\nand interpret textual content within the images, but also to read and reason about mathematical ex-\npressions, tables, and diagrams where present. For all examples, the user query is standardized as:\n“이미지를보고문제를풀어주세요.” (“Please solve the problem by looking at the image.”)\nD.1\nKCSAT Biology Basic Example\nWe present our model’s output on a sample KCSAT Biology Basic problem. Figure 7 shows the\noriginal problem image, which is directly provided to the model for analysis and reasoning. The\nmodel is required to interpret, reason through, and solve the problem solely based on the image. For\nreference, we include the model’s original answer, as well as an English translation of the problem\nfor readers. A literal English translation of the model’s output is also provided for comparison.\nFigure 7: Original KCSAT Sample (in Korean).\nSample Question 8 (Translated)\nThe diagrams below represent parts of processes A and B in succession, and a graph\ndepicting the changes in total productivity and respiration amounts of plant community K\nover time. A and B refer to primary and secondary successions in no particular order, and\nlabels ㄱand ㄴrefer to hydrosere and xerosere in no particular order.\n<Process Diagram> <Graph>\nWhich of the following statements are correct based on the above?\n(Options)\n(a) B is a secondary succession.\n(b) ㄱis a hydrosere.\n(c) For K, the total productivity\nrespiration\nis greater at t2 than at t1.\nSelect the correct combination(s):\n(1) a\n(2) b\n(3) a, c\n(4) b, c\n(5) a, b, c\n37\n\nHyperCLOVA X THINK (Translated Model Output – English)\n<|im_start|>assistant/think\nOkay, let’s solve this problem. First, we need to look carefully at the figure given in the\nproblem.\nFirst, we need to examine figures (a) and (b) of question 8. According to the problem, A\nand B are one of primary succession and secondary succession, and ㉠and ㉡represent\ndeciduous forest and lichen respectively, in no particular order. Also, it shows a graph of the\nchanges in total productivity and respiration amount of plant community K over time.\nLet’s look at (a) first. A is said to go from grassland to shrub forest to ㉠, and B is said to\nstart from lava field, go through ㉡, and reach grassland. Here, we need to figure out which\nsuccession process A and B each represent. Also, we need to identify what ㉠and ㉡are.\nIf we think about the succession process, primary succession starts in lifeless areas, and\nsecondary succession is the process of recovery after disturbance in an area where plants\nalready existed. For example, starting on bare land created by disturbances like forest fires\nor logging is secondary succession. On the other hand, primary succession starts in places\nlike rocks or lava fields, where there isn’t even soil.\nIn the problem, since A starts from grassland (a place where plants already exist), this would\nbe the secondary succession process. That’s because grassland is an environment with soil\nand some living organisms. In contrast, B starts from a lava field, which is formed by vol-\ncanic activity and likely has no soil. Thus, B would correspond to primary succession. There-\nfore, the conclusion is that A is secondary succession and B is primary succession... but wait,\nin the problem it states that A and B represent “primary and secondary in no order,” so I\nmight have thought in reverse, so confirmation is necessary.\nLooking again, A: grassland →shrub forest →㉠\nB: lava field →㉡→grassland\nA already begins with grassland, so it’s a secondary succession process, and B starts from\nlava field, so it is a primary succession process. So A is secondary, B is primary. Then in the\nchoices, statement ㄱsays “B is secondary succession,” so that’s a wrong statement. So ㄱ\nis incorrect.\nNext, let’s look at what ㉠and ㉡are. A process is secondary succession, and generally, the\nfinal stage of secondary succession could be a climax, such as a shade-tolerant forest, but\ndepending on the case, it could be something else. However, usually, secondary succession\ndevelops into a more complex forest structure over time. But here, A passes through shrub\nforest and goes to ㉠, so if ㉠is a deciduous forest, the next stage would be a shade-tolerant\nforest, but does the problem refer to the final stage or an intermediate one?\nOn the other hand, B is primary succession, starting from a lava field, going through ㉡, and\nthen to grassland. In the early stage of primary succession, pioneer species such as lichen\nor moss appear, and then herbaceous plants grow, followed by shrub forest, and eventually\ndeciduous, mixed, and shade-tolerant forests in order. However, since B goes from lava field\nthrough ㉡to grassland, it’s likely that ㉡is lichen. That’s because in harsh environments\nlike lava fields, what settles first tends to be things like lichen. Then ㉡is lichen, and ㉠\nwould be deciduous forest. That’s because, as A undergoes secondary succession, after go-\ning through shrub forest it would further develop into deciduous forest. Therefore, ㉠is\ndeciduous forest and ㉡is lichen. Accordingly, statement ㄴ, “㉠is deciduous forest,” is a\ncorrect statement.\nNext, let’s look at figure (b). It shows changes in total productivity and respiration of plant\ncommunity K depending on time (t). At time t0, it appears, and up to t1, total productivity\nincreases, then afterwards decreases, while respiration continues to increase, and from t2\nonward, it is marked as the appearance of a shade-tolerant forest. That is, before t1, total\nproductivity is greater than respiration, so net productivity has a positive value, after t1,\ntotal productivity decreases and becomes similar to or even less than respiration, so net\nproductivity reduces.\nNet productivity is total productivity minus respiration, so at t1, total productivity is at its\nmaximum and respiration is relatively low, so net productivity will be very high. But at t2,\ntotal productivity is lower and respiration continues to increase, so net productivity will be\nfar lower than at t1. Therefore, the ratio of K’s net productivity to respiration is greater at\n38\n\nt1 than at t2. But the choice ㄷsays, “K’s net productivity/respiration is greater at t2 than at\nt1,” so this is a wrong explanation. So ㄷis wrong too.\nIn summary:\n- ㄱ: “B is secondary succession.” →B is primary succession, so wrong.\n- ㄴ: “㉠is deciduous forest.” →Correct.\n- ㄷ: net productivity/respiration is t2 > t1 →Wrong. t1 is greater.\nTherefore, the answer is only ㄴis correct, so we need to check if there is an option with\nonly ㄴ. The options provided are as follows:\n①ㄱ\n②ㄴ\n③ㄱ, ㄷ\n④ㄴ, ㄷ\n⑤ㄱ, ㄴ, ㄷ\nIn this, the case where only ㄴis correct is option ②, but let’s check again whether there\nwas any calculation mistake.\nAt t1, total productivity is high and respiration is still low, so net productivity is abundant,\nand at t2, total productivity has decreased and respiration has increased, so net productivity\nis lower. Therefore, the ratio of net productivity to respiration, that is, net productivity/respi-\nration, is much greater at t1. For example, if at t1, total productivity is 10 and respiration is 3,\nnet productivity is 7 and the ratio is 7/3 2.33. If at t2, total productivity is 5 and respiration\nis 4, net productivity is 1 and the ratio is 1/4 = 0.25. So t1 is definitely greater. Thus, ㄷis\nwrong, and the answer is option ②.\n**Final Answer**: (2) <|im_end|>\n<|im_start|>assistant\nAnalyze figures (a) and (b) to solve the problem.\n1. **Analysis of succession process**:\n- **A**: Starts from grassland, passes through shrub forest, and proceeds to ㉠. Since it\nstarts from grassland where plants already exist, it is **secondary succession**.\n- **B**: Starts from lava field, passes through ㉡, and proceeds to grassland. Since it starts\nfrom a lava field with no soil, it is **primary succession**.\n- Therefore, ㄱ(\"B is secondary succession.\") is an **incorrect** explanation.\n2. **Identification of ㉠and ㉡**:\n- **A** (secondary succession): Passes through shrub forest and proceeds to ㉠, and gener-\nally, the late stage of secondary succession is a **deciduous forest**.\n- **B** (primary succession): Starts from lava field, passes through ㉡, and proceeds to\ngrassland, with the initial stage being **lichen**.\n- Therefore, ㄴ(\"㉠is a deciduous forest.\") is a **correct** explanation.\n3. **Graph analysis**:\n- **Total productivity** increases until t1 and then decreases, while **respiration** contin-\nues to increase.\n- **Net productivity** is at its highest at t1 when total productivity is maximal and respira-\ntion is relatively low, so the ratio is high. At t2, total productivity decreases and respiration\nincreases, so the net productivity ratio becomes lower.\n- Therefore, ㄷ(\"K’s net productivity\nrespiration\nis greater at t2 than at t1\") is an **incorrect** explanation.\nIn conclusion, the only correct explanation is **ㄴ**.\n(2) <|im_end|>\nHyperCLOVA X THINK (Original Model Output – Korean)\n<|im_start|>assistant/think\n좋아, 이문제를풀어보자. 일단문제에서주어진그림을잘봐야겠지.\n먼저8번문제의그림(가)와(나)를살펴봐야해. 문제에따르면A와B는1차천이와2\n차천이중하나이고, ㉠과㉡은양수림과지의류를순서없이나타낸거야. 그리고식물\n군집K의시간에따른총생산량과호흡량을그래프로보여주고있어.\n39\n\n일단(가)부터보자. A는초원에서관목림을거쳐㉠으로가고, B는용암대지에서시작\n해서㉡을거쳐초원으로간다고되어있네. 여기서A와B가각각어떤천이과정인지\n알아내야해. 또㉠과㉡이무엇인지도파악해야하고.\n천이과정을생각해보면, 1차천이는생명이없는지역에서시작하는거고, 2차천이는\n기존에식물이있던곳에서교란이일어난후에다시회복되는과정이야. 예를들어산\n불이나벌목같은것들로인해생긴빈땅에서시작하는게2차천이지. 반면1차천이는\n바위나용암대지처럼토양조차없는곳에서시작돼.\n문제에서A는초원(이미식물이있는곳)에서시작하니까이건2차천이과정이겠지. 왜냐\n하면초원은이미토양이있고일부생물이존재하는환경이니까. 반면B는용암대지에서\n시작하는데, 용암대지는화산활동으로인해형성된곳으로토양이없을가능성이높아.\n그래서B는1차천이과정에해당할거야. 따라서A는2차천이, B는1차천이라는결론이\n나오지만... 잠깐, 문제에서는A와B가1차와2차를순서없이나타냈다고했으니까내가\n반대로생각했을수도있으니확인필요.\n다시보면, A: 초원→관목림→㉠\nB: 용암대지→㉡→초원\nA는이미초원이있으므로2차천이과정이고, B는용암대지에서시작하므로1차천이\n과정이다. 그러면A는2차, B는1차가맞아. 그럼보기에서ㄱ은’B는2차천이이다.’라고\n했으니틀린설명이되겠지. 그러니까ㄱ은틀렸어.\n다음으로㉠과㉡이뭔지보자. A 과정은2차천이이므로, 일반적으로2차천이의종착\n단계는극상인음수림이될수있지만, 경우에따라서는다른형태일수도있겠지. 하지만\n보통2차천이는시간이지나면더복잡한구조의숲으로발달해. 그런데여기서는A가\n관목림을거쳐㉠으로가는데, 만약㉠이양수림이라면그다음단계는음수림일텐데,\n문제에서는최종단계를나타내는걸까? 아니면중간단계일까?\n반면B는1차천이로, 용암대지에서시작하여㉡을거쳐초원으로간다. 1차천이의초기\n에는지의류나선태식물같은개척자들이나타나고, 그다음에초본식물들이자라면서\n관목림이되고, 결국에는양수림, 혼합림, 음수림순으로진행되지. 그런데B는용암대\n지에서㉡을거쳐초원으로가므로, ㉡은아마도지의류일가능성이높아. 왜냐하면용\n암대지와같은척박한환경에서가장먼저정착하는것은지의류같은것이기때문이야.\n그렇다면㉡은지의류이고, ㉠은양수림이되는거겠지. 왜냐하면A는2차천이과정에서\n관목림을거쳐더발전하면양수림이될테니까. 따라서㉠은양수림, ㉡은지의류가돼.\n그러므로보기의ㄴ은’㉠은양수림이다’라고했으니맞는설명이야.\n그다음으로(나) 그래프를보자. 식물군집K의시간(t)에따른총생산량과호흡량의변\n화인데, t0 시점에서출현하고, t1까지는총생산량이증가하다가이후감소하며, 호흡량은\n계속증가하고, t2 시점부터는음수림출현이라고표시되어있어. 즉, t1 이전에는총생산\n량이호흡량보다많아서순생산량이양의값을가지다가, t1 이후에는총생산량이감소하\n면서호흡량과비슷해지거나오히려작아져서순생산량이줄어들게되지.\n순생산량은총생산량- 호흡량이니까, t1 때는총생산량이최대이고호흡량은상대적으로\n낮아서순생산량이매우높을거야. 그러나t2 때가되면총생산량은더낮아지고호흡량\n은계속증가해서순생산량이t1 때보다훨씬낮아질거야. 따라서K의순생산물/ 호흡량\n비율은t1 때가t2 때보다크다는결론이나와. 그런데문제의보기ㄷ은’K의순생산량/\n호흡량은t2일때가t1일때보다크다’라고했으므로이는틀린설명이야. 따라서ㄷ도\n틀렸다.\n정리하면:\n- ㄱ: B는2차천이이다. →B는1차천이이므로틀림.\n- ㄴ: ㉠은양수림이다. →맞음.\n- ㄷ: 순생산량/호흡량은t2 > t1 →틀림. t1이더큼.\n따라서정답은ㄴ만맞는경우인데, 보기중에ㄴ만있는선택지가있는지확인해봐야해.\n문제에서제시된보기옵션은다음과같아:\n①ㄱ\n②ㄴ\n③ㄱ, ㄷ\n④ㄴ, ㄷ\n⑤ㄱ, ㄴ, ㄷ\n여기서ㄴ만맞는경우는②번이지만, 혹시계산실수가있었는지다시확인해보자.\nt1 시기에는총생산량이높고호흡량은아직낮은상태라순생산량이많을거고, t2 시기에\n는총생산량이감소했고호흡량은증가했으므로순생산량은더적어져. 따라서순생산량\n대비호흡량의비율, 즉순생산량/호흡량은t1 때가훨씬높다. 예를들어t1에서총생산량\n40\n\n이10, 호흡량3이면순생산량은7이고비율은7/3 2.33. t2에서총생산량5, 호흡량4라면\n순생산량1, 비율은1/4 = 0.25. 따라서확실히t1이더크다. 그러니ㄷ은틀렸고, 따라서\n정답은②번.\n**Final Answer**: (2) <|im_end|>\n<|im_start|>assistant\n그림(가)와(나)를분석하여문제를해결한다.\n1. **천이과정분석**:\n- **A**: 초원에서시작하여관목림을거쳐㉠으로진행된다. 이미식물이있는초원에서\n시작되므로**2차천이**이다.\n- **B**: 용암대지에서시작하여㉡을거쳐초원으로진행된다. 토양이없는용암대지\n에서시작하므로**1차천이**이다.\n- 따라서ㄱ(\"B는2차천이이다.\")은**틀린** 설명이다.\n2. **㉠과㉡식별**:\n- **A**(2차천이): 관목림을거쳐㉠으로진행되며, 일반적으로2차천이의후기단계는\n**양수림**이다.\n- **B**(1차천이): 용암대지에서시작하여㉡을거쳐초원으로진행되며, 초기단계는\n**지의류**이다.\n- 따라서ㄴ(\"㉠은양수림이다.\")은**맞는** 설명이다.\n3. **그래프분석**:\n- **총생산량**은t1까지증가한후감소하며, **호흡량**은지속적으로증가한다.\n- **순생산량**은t1 시기에총생산량이최대이고호흡량이상대적으로낮아비율이높\n다. t2 시기에는총생산량이감소하고호흡량이증가하여순생산량비율이낮아진다.\n- 따라서ㄷ(\"K의순생산량\n호흡량은t2일때가t1일때보다크다\")은**틀린** 설명이다.\n최종적으로옳은설명은**ㄴ**뿐이다.\n(2) <|im_end|>\nD.2\nKCSAT Biology Advanced Example\nWe present our model’s output on a sample KCSAT Biology Advanced problem. Figure 8 shows\nthe original problem image, which is directly provided to the model for analysis and reasoning. The\nmodel is required to interpret, reason through, and solve the problem solely based on the image. For\nreference, we include the model’s original answer, as well as an English translation of the problem\nfor readers. A literal English translation of the model’s output is also provided for comparison.\nFigure 8: Original KCSAT Sample (in Korean).\n41\n\nSample Question 2 (Translated)\nThe table below summarizes the characteristics of plant tissues. A and B refer to vascular\nand dermal tissues, listed in no particular order.\nTissue\nCharacteristic\nA\nCovers the surface and protects the plant body.\nB\n?\nFundamental tissue\nSynthesizes and stores nutrients.\nWhich statements below, from the options labeled (a) to (c), are correct regarding the\ntissues described above?\n(Options)\n(a) A is a dermal tissue.\n(b) Substances are transported via tissue B.\n(c) The petal of a rose contains fundamental tissue.\nSelect the correct combination(s):\n(1) a\n(2) b\n(3) a, c\n(4) b, c\n(5) a, b, c\nHyperCLOVA X THINK (Translated Model Output – English)\n<|im_start|>assistant/think\nOkay, let’s solve this problem. First, I need to carefully read the question. The question gives\na table showing the characteristics of plant tissue systems, and A and B are listed as vascular\ntissue system and epidermal tissue system in random order. Also, it is stated that the ground\ntissue system synthesizes and stores nutrients.\nFirst, looking at the table, the characteristic of A is “covers the surface and protects the\nplant body,” and B is not yet explained. However, since the ground tissue system is already\ndescribed as synthesizing and storing nutrients, the remaining two must be the vascular and\nepidermal tissue systems. The problem states that one of A or B is epidermal, and the other\nis vascular.\nLet’s summarize the characteristics of each tissue system.\n1. Ground tissue system: This is also known as the fundamental tissue and mainly consists\nof parenchyma and sclerenchyma, which do not perform photosynthesis but serve to store\nor transport water and nutrients. So, in the problem, the characteristic of the ground tissue\nsystem is given as “synthesizes and stores nutrients”... Wait, does the ground tissue system\nphotosynthesize? No, usually it’s tissues like palisade cells in the epidermal tissue or vas-\ncular tissue that do that, isn’t it? Wait, this could be a confusing point. I was taught that the\nground tissue system generally does not photosynthesize, but in the problem, its character-\nistic is “synthesizes and stores nutrients.” I need to check if this is correct.\nIs the problem wrong? Or am I misremembering?\nThe ground tissue system mainly stores, and photosynthesis occurs in the epidermal tissue\n(especially palisade cells below the leaf epidermis). So is the description of the ground tissue\nsystem in the problem incorrect? Or is there another interpretation?\nSince the problem states that the ground tissue system “synthesizes and stores nutrients,” I\nhave no choice but to accept it as it is for now. Probably, that’s how the problem defines it.\nGoing back to the question: is A the epidermal tissue system or vascular? The characteristic\nof A is “covers the surface and protects the plant body,” which is a major function of the\nepidermal tissue system. For example, epidermal cells cover the surface of leaves, preventing\nwater evaporation and protecting from pathogens. Therefore, A is epidermal tissue system,\nand B is likely vascular tissue system.\nThen, what would be the feature of B? The vascular tissue system is composed of xylem\nand phloem and is involved in the transport of water and nutrients, so the characteristic of\nB would be something like “transports water and nutrients.” But the characteristic of B is\nleft blank in the problem, so we have to infer it and select the correct explanation from the\nchoices.\n42\n\nNow let’s look at the choices.\nㄱ. A is the epidermal tissue system. →Correct. Based on the reasoning above, this is true.\nㄴ. Substances move through B. →If B is the vascular tissue system, then it is the pathway\nfor water and nutrient transport, so this is also true.\nㄷ. Rose leaves have ground tissue system. →The ground tissue system should exist in all\nplant leaves. As the internal tissue of leaves, it stores or transports water and nutrients, so\nthis is also true.\nHowever, the problem describes the ground tissue system as “synthesizes and stores nutri-\nents,” which makes me doubtful as to whether it really does photosynthesis, but since that is\nwhat the problem states, I have to accept it. So ㄷis also a correct explanation.\nBut based on prior knowledge, the ground tissue system does not photosynthesize, so I\nwonder if the problem is in error. If, for example, the problem actually refers to the epidermal\ntissue system instead? No, the problem specifically describes the characteristic of the ground\ntissue system, so I should just follow that.\nSo are ㄱ, ㄴ, ㄷall correct? The answer options include 5 as “ㄱ, ㄴ, ㄷ,” so that seems\npossible, but is there any chance I made a mistake?\nLet me check again.\nA: Epidermal tissue system →covers surface. O\nB: Vascular tissue system →transports water and nutrients. O\nGround tissue system: In problem, synthesizes and stores nutrients. Might be different from\nreality but will follow the problem.\nDo rose leaves have ground tissue system? The inside of leaves is made of parenchyma,\nwhich is ground tissue, so O.\nTherefore, it seems all three are correct. Is the answer 5?\nBut sometimes in tests there are traps—could ㄷsomehow be wrong?\nFor example, if rose leaves had only vascular tissues and not ground tissue? But in a leaf,\nthere are epidermis, guard cells, palisade tissue (ground tissue), vascular bundles, etc., so\nground tissue must be present. Thus, ㄷis valid.\nSo the answer is 5?\nEven though at first I thought the description of the ground tissue system was wrong, since\nthat’s how the problem gives it, I will follow it. Therefore, all ㄱ, ㄴ, and ㄷare correct, and\nthe answer is 5.\n**Final answer**: (5) <|im_end|>\n<|im_start|>assistant\nIn the table for question 2, A and B represent the epidermal tissue system and vascular tissue\nsystem, respectively. The characteristic of the ground tissue system is given as ’synthesizes\nand stores nutrients.’\n- **Characteristic of A**: ’Covers the surface and protects the plant body.’ This is a main\nfunction of the epidermal tissue system, which covers the surface of the plant to prevent\nwater evaporation and protect from external environments. Therefore, A is the epidermal\ntissue system.\n- **Characteristic of B**: It is a characteristic of the vascular tissue system. The vascular\ntissue system is composed of xylem and phloem and is involved in the transport of water and\nnutrients. Therefore, B is the vascular tissue system, and it is correct to say that substances\nare transported through it.\n- **Ground tissue system**: The characteristic given in the problem is ’synthesizes and\nstores nutrients.’ The ground tissue system comprises most of the plant and serves to store\nand transport water and nutrients. The leaves of a rose also contain ground tissue system,\ncarrying out these functions.\nAnalysis of choices:\n- **ㄱ**: A is the epidermal tissue system. (Correct)\n- **ㄴ**: Substances are transported through B. (Correct)\n- **ㄷ**: The leaves of a rose have ground tissue system. (Correct)\nTherefore, since all choices are correct, the answer is number 5.\n(5) <|im_end|>\n43\n\nHyperCLOVA X THINK (Original Model Output – Korean)\n<|im_start|>assistant/think\n좋아, 이문제를풀어보자. 우선문제를잘읽어봐야겠어. 문제는식물을구성하는조직계\n의특징을나타낸표인데, A와B는관다발조직계와표피조직계를순서없이나열했다고\n해. 그리고기본조직계는양분을합성하고저장한다고되어있네.\n먼저, 표를보면A의특징은’표면을덮어식물체를보호한다’고했고, B는아직설명이안\n되어있어. 그런데기본조직계는이미양분합성과저장이라고했으니까, 나머지두개가\n관다발과표피일거야. 문제에서A와B 중하나는표피이고다른하나는관다발이야.\n그럼먼저각조직계의특징을정리해보자.\n1. **기본조직계**: 이건울타리라고도불리는조직으로, 주로유조직과기계조직으로\n이루어져있고, 광합성을하지않지만물과영양분을저장하거나운반하는역할을한다고\n알고있어. 그래서문제에서도기본조직계의특징이’양분을합성하고저장한다’고했는\n데... 잠깐, 기본조직계가광합성을하나? 아니, 보통표피조직이나관다발조직중에\n책상세포같은게있지않나?\n잠깐, 여기서헷갈리기쉬운부분이있을것같아. 기본조직계는일반적으로광합성을\n하지않는다고배웠던것같은데, 문제에서는기본조직계의특징이’양분을합성하고\n저장한다’고했어. 이게맞는지확인해야할것같아.\n아니면문제가잘못된건가? 아니면내가잘못기억한건가?\n기본조직계는주로저장기능을하며, 광합성은표피조직(특히잎의표피아래에있는\n책상세포)에서일어나는걸로알고있어. 그럼문제의기본조직계설명이틀린건가?\n아니면다른해석이있을까?\n문제에서기본조직계의특징이’양분을합성하고저장한다’고했으니일단그대로받아\n들이고넘어가야겠지. 아마문제에서는그렇게정의했으니까.\n그럼다시문제로돌아가서, A는표피조직계일까관다발일까? A의특징은’표면을덮어\n식물체를보호한다’인데, 표피조직계의주요기능중하나가바로식물체의표면덮개\n역할이지. 예를들어잎의표피세포들이표면을덮어서수분증발을막고병원체로부터\n보호해주니까. 따라서A는표피조직계이고, B는관다발조직계일가능성이높아.\n그러면B의특징은뭘까? 관다발조직계는물관부와체관부로나뉘어져있어서물과양\n분의이동에관여하니까, B의특징은아마도’물과양분을운반한다’ 정도가될것같아.\n하지만문제에서는B의특징이비어있고, 우리가그걸추론해서보기중에서맞는설명을\n골라야하니까.\n이제보기를살펴보자.\nㄱ. A는표피조직계이다. →맞아. 위에서생각한대로라면맞는말이야.\nㄴ. B를통해물질이이동한다. →B가관다발조직계라면, 물과양분이이동하는통로이\n므로맞는말이지.\nㄷ. 장미의잎에는기본조직계가있다. →기본조직계는모든식물의잎에존재할거야.\n잎의내부조직으로서물과양분을저장하거나운반하는역할을하니까. 따라서이것도\n맞는말이야.\n그런데문제에서기본조직계의특징이’양분을합성하고저장한다’고했는데, 실제로기\n본조직계가광합성을하는지의문이들지만, 문제에서는그렇게주어졌으므로일단은\n받아들여야해. 그러면ㄷ도맞는설명이되겠지.\n하지만기존지식으로는기본조직계가광합성을하지않는다고알고있는데, 이부분이\n문제의오류인지아닌지가궁금해. 만약문제에서의기본조직계가실제로는표피조직\n계를가리키는것이아니라면? 아니, 문제에서는기본조직계의특징을따로주었으니\n그대로따라가야할것같아.\n따라서ㄱ, ㄴ, ㄷ모두옳은설명인가? 그런데보기에서5번이’ㄱ, ㄴ, ㄷ’이라서그럴\n수도있지만, 혹시내가뭔가잘못생각했을까?\n다시한번점검해보자.\n- A: 표피조직계→표면덮개역할. O\n- B: 관다발조직계→물과양분운반. O\n- 기본조직계: 문제에서는양분합성및저장. 실제와다를수있으나문제에따름.\n장미의잎에는기본조직계가있는가? 잎의내부는유조직으로구성되어있는데, 이는\n기본조직계에해당하므로O.\n그래서세가지모두맞는것같다. 그렇다면정답은5번?\n하지만보통시험문제에서는이런함정이있을수있으니, 혹시ㄷ가틀린경우는없을까?\n44\n\n예를들어, 장미의잎에는기본조직계가없고관다발만있다는식으로? 아니, 잎의경우\n표피, 공변세포, 울타리조직(기본조직계), 관다발등이있으니까기본조직계가있어야\n해. 따라서ㄷ는맞는설명.\n그럼답은5번?\n하지만처음에기본조직계의특징이틀렸다는생각이들었지만, 문제에서는그렇게주\n어졌으니따라가야한다. 따라서ㄱ, ㄴ, ㄷ모두맞는설명이므로5번이정답이다.\n**최종답변** (5) <|im_end|>\n<|im_start|>assistant\n2번문제의표에서A와B는각각표피조직계와관다발조직계를나타낸다. 기본조직계\n의특징은’양분을합성하고저장한다’로주어져있다.\n- **A의특징**: ’표면을덮어식물체를보호한다.’ 이는표피조직계의주요기능으로,\n표피조직계는식물체의표면을덮어수분증발을막고외부환경으로부터보호한다. 따\n라서A는표피조직계이다.\n- **B의특징**: 관다발조직계의특징이다. 관다발조직계는물관과체관으로구성되어\n물과양분의이동에관여한다. 따라서B는관다발조직계이며, 이를통해물질이이동한\n다는것은올바른설명이다.\n- **기본조직계**: 문제에서주어진특징은’양분을합성하고저장한다.’ 기본조직계는\n식물의대부분을차지하며, 물과양분의저장및운반역할을한다. 장미의잎에도기본\n조직계가존재하여이러한기능을수행한다.\n보기분석: - **ㄱ**: A는표피조직계이다. (정답) - **ㄴ**: B를통해물질이이동한다.\n(정답) - **ㄷ**: 장미의잎에는기본조직계가있다. (정답)\n따라서모든선택지가올바르므로정답은5번이다.\n(5) <|im_end|>\nD.3\nKCSAT Math Advanced Example\nWe present our model’s output on a sample KCSAT Math Advanced problem. Figure 9 shows the\noriginal problem image, which is directly provided to the model for analysis and reasoning. The\nmodel is required to interpret, reason through, and solve the problem solely based on the image. For\nreference, we include the model’s original answer, as well as an English translation of the problem\nfor readers. A literal English translation of the model’s output is also provided for comparison.\nFigure 9: Original KCSAT Sample (in Korean).\nSample Question 23 (Translated)\nDetermine the value of\nlim\nx→0\nln(1 + 3x)\nln(1 + 5x)\nChoose the correct answer: (1) 1\n5\n(2) 2\n5\n(3) 3\n5\n(4) 4\n5\n(5) 1\nHyperCLOVA X THINK (Translated Model Output – English)\n<|im_start|>assistant/think\nAlright, this seems to be a problem about finding a limit. It’s question 23, and it presents\nthe ratio of logarithmic functions in fractional form. The expression is the limit as x →0 of\nln(1+3x)\nln(1+5x). The provided choices range from 1\n5 to 1, but which one is correct?\n45\n\nFirst, for limits of this type, you can usually use L’Hôpital’s Rule to solve them, since as\nx →0, both the numerator and denominator become ln(1), so the expression is in the inde-\nterminate form 0/0. By applying L’Hôpital’s Rule, we can find the answer by differentiating\nthe numerator and denominator.\nBefore applying L’Hôpital’s Rule, let’s check: when x = 0, the numerator is ln(1 + 3 ·\n0) = ln(1) = 0, and the denominator is ln(1 + 5 · 0) = ln(1) = 0, so that checks out.\nTherefore, L’Hôpital’s Rule can be used. So, we need to differentiate both the numerator\nand denominator with respect to x. The derivative of the numerator ln(1 + 3x) is\n3\n1+3x, and\nthe derivative of the denominator ln(1 + 5x) is\n5\n1+5x.\nBy L’Hôpital’s Rule, the limit then becomes\n3\n1+3x\n5\n1+5x\n=\n3\n1 + 3x · 1 + 5x\n5\nand, simplifying this,\n3(1 + 5x)\n5(1 + 3x)\nis what you get.\nNow, if you calculate the value as x →0, 1 + 5x →1, and 1 + 3x →1, so overall it should\nbecome 3\n5, right? Looking at the choices, 3\n5 is choice ③. But why did I think of something\nelse at first? Oh, did I calculate something incorrectly?\nLet me double-check. After applying L’Hôpital’s Rule:\nlim\nx→0\n3\n1 + 3x · 1 + 5x\n5\nPlugging in x →0,\n3\n1 · 1\n5 = 3\n5\nSo, the answer is 3\n5? Among the answer choices, ③is 3\n5, ②is 2\n5, and ④is 4\n5, so I need to\nmake sure not to get confused.\nCan this be solved another way, for example, by approximating using a Taylor expansion?\nln(1 + a) ≈a −a2\n2 + · · ·\nUsing this expansion, if a is small, we can approximate ln(1 + a) ≈a.\nSo, in the numerator, ln(1 + 3x) ≈3x, and in the denominator, ln(1 + 5x) ≈5x, which\nmakes the ratio\n3x\n5x = 3\n5\nand we reach the same result.\nSo, since both methods yield 3\n5, it seems choice ③is the answer. Is there any chance I made\na calculation mistake in the process? Let’s check again. When applying L’Hôpital’s Rule,\ndid I correctly differentiate the numerator and denominator:\nd\ndx ln(1 + 3x) =\n3\n1 + 3x,\nd\ndx ln(1 + 5x) =\n5\n1 + 5x\nAnd when converting the division to multiplication, there was no sign or coefficient error,\nand as x →0, the x terms in the denominator and numerator disappear, so the result is\nindeed 3\n5. In the case of the Taylor expansion as well, considering only the first term, it’s\nexactly 3x\n5x = 3\n5. So, certainly, 3\n5 is correct. Therefore, among the choices, since ③is 3\n5, the\nfinal answer is ③.\n**Final answer**: (3) <|im_end|>\n<|im_start|>assistant\nTo find the limit limx→0\nln(1+3x)\nln(1+5x), we use the **L’Hôpital’s Rule**.\n46\n\nSince both the numerator and denominator become the undetermined form 0/0 as x →0,\nwe can apply L’Hôpital’s Rule:\nlim\nx→0\nln(1 + 3x)\nln(1 + 5x) = lim\nx→0\nd\ndx ln(1 + 3x)\nd\ndx ln(1 + 5x)\nDifferentiating each:\nd\ndx ln(1 + 3x) =\n3\n1 + 3x,\nd\ndx ln(1 + 5x) =\n5\n1 + 5x\nTherefore,\nlim\nx→0\n3\n1+3x\n5\n1+5x\n= lim\nx→0\n3\n1 + 3x · 1 + 5x\n5\nWhen x →0, 1 + 3x ≈1 and 1 + 5x ≈1, so\n3\n1 · 1\n5 = 3\n5\nAlso, using a Taylor expansion, ln(1+ax) ≈ax (where a is a constant) as an approximation:\nln(1 + 3x)\nln(1 + 5x) ≈3x\n5x = 3\n5\nTherefore, the answer is (3) .\n<|im_end|>\nHyperCLOVA X THINK (Original Model Output – Korean)\n<|im_start|>assistant/think\n좋아, 이문제는극한을구하는문제인것같아. 23번문제고, 분수꼴의로그함수의비율이\n주어졌어. 식은x →0일때ln(1+3x)\nln(1+5x)의극한이야. 보기로는1\n5부터1까지있는데, 정답이\n뭘까?\n먼저, 이런형태의극한문제는보통로피탈법칙을써서풀수있지않을까? 왜냐하면\n분자와분모모두x →0일때ln(1)이되니까0/0 형태의불정형이기때문이야. 로피탈\n법칙을적용하면미분해서계산할수있을거야.\n로피탈법칙을적용하기전에먼저확인해보자. x = 0일때분자는ln(1+3·0) = ln(1) =\n0, 분모도ln(1 + 5 · 0) = ln(1) = 0이니까맞아. 따라서로피탈법칙을사용할수있어.\n그래서분자와분모를각각x로미분해야해. 분자인ln(1 + 3x)의도함수는\n3\n1+3x이고,\n분모인ln(1 + 5x)의도함수는\n5\n1+5x이지.\n그러면로피탈법칙에의해극한은\n3\n1+3x\n5\n1+5x\n=\n3\n1 + 3x · 1 + 5x\n5\n가되고, 이걸단순화하면\n3(1 + 5x)\n5(1 + 3x)\n가되겠지.\n이제x →0일때이값을계산하면, 1 + 5x →1, 1 + 3x →1이니까전체적으로3\n5가되\n어야하지않나? 근데보기에는3\n5가③번에있네. 그런데왜내가처음생각했을때다른\n결과가나오는걸까? 아, 아니, 계산을잘못했나?\n다시확인해보자. 로피탈법칙적용후의식:\nlim\nx→0\n3\n1 + 3x · 1 + 5x\n5\n47\n\n여기서x →0을대입하면,\n3\n1 · 1\n5 = 3\n5\n맞아, 그럼답은3\n5인가? 그런데문제에주어진보기중에서③번이3\n5이고, ②번이2\n5, ④\n번이4\n5라서헷갈리지않게해야겠어.\n혹시다른방법으로풀어보면, 예를들어테일러전개를사용해서근사할수있을까?\nln(1 + a) ≈a −a2\n2 + · · ·\n이런식으로전개하면, a가작을때ln(1 + a) ≈a 정도로근사할수있지.\n그럼분자는ln(1 + 3x) ≈3x, 분모는ln(1 + 5x) ≈5x가되니까, 그비율은\n3x\n5x = 3\n5\n가되어동일한결과를얻을수있겠네.\n그래서두방법모두3\n5가나오니까정답은③번인것같다. 하지만혹시나계산과정에\n서실수한부분이있을까? 다시한번체크해보자. 로피탈법칙적용시, 분자와분모의\n도함수를제대로했는지:\nd\ndx ln(1 + 3x) =\n3\n1 + 3x,\nd\ndx ln(1 + 5x) =\n5\n1 + 5x\n그리고나눗셈을곱셈으로바꿀때부호나계수가틀린건없고, x →0일때의값은분\n모와분자의x 항이없어지므로3\n5가되는게맞지. 테일러전개의경우에도첫번째항만\n보면정확히3x\n5x = 3\n5. 그래서확실히3\n5가맞는것같아. 그럼문제의보기중에서③번이\n3\n5이므로정답은③번이다.\n**최종답변** (3) <|im_end|>\n<|im_start|>assistant\n극한limx→0\nln(1+3x)\nln(1+5x) 를구하기위해**로피탈법칙**을사용합니다.\n분자와분모모두x →0 일때0/0 형태의부정형이므로로피탈법칙을적용합니다:\nlim\nx→0\nln(1 + 3x)\nln(1 + 5x) = lim\nx→0\nd\ndx ln(1 + 3x)\nd\ndx ln(1 + 5x)\n각각미분하면:\nd\ndx ln(1 + 3x) =\n3\n1 + 3x,\nd\ndx ln(1 + 5x) =\n5\n1 + 5x\n따라서,\nlim\nx→0\n3\n1+3x\n5\n1+5x\n= lim\nx→0\n3\n1 + 3x · 1 + 5x\n5\nx →0 일때1 + 3x ≈1 및1 + 5x ≈1 이므로,\n3\n1 · 1\n5 = 3\n5\n또한테일러전개를사용하면ln(1 + ax) ≈ax (단, a 는상수)로근사할수있습니다:\nln(1 + 3x)\nln(1 + 5x) ≈3x\n5x = 3\n5\n따라서, 답은(3) 입니다.\n<|im_end|>\n48\n\nE\nSynthetic Data Examples\nFigure 10: Synthetic QA data generated from Wikipedia\nFigure 11: Translated sample example of synthetic QA data generated from Wikipedia\n49\n\nFigure 12: Synthetic educational data generated from course syllabus\nFigure 13: Translated sample example of synthetic educational data generated from course syllabus\n50\n",
      "fetch_method": "direct-pdf"
    },
    {
      "arxiv_id": "https://huggingface.co/docs/transformers/installation",
      "full_text": " Installation Hugging Face Models Datasets Spaces Community Docs Enterprise Pricing Log In Sign Up Transformers documentation Installation Transformers 🏡 View all docs AWS Trainium &amp; Inferentia Accelerate Argilla AutoTrain Bitsandbytes Chat UI Dataset viewer Datasets Deploying on AWS Diffusers Distilabel Evaluate Gradio Hub Hub Python Library Huggingface.js Inference Endpoints (dedicated) Inference Providers LeRobot Leaderboards Lighteval Microsoft Azure Optimum PEFT Safetensors Sentence Transformers TRL Tasks Text Embeddings Inference Text Generation Inference Tokenizers Trackio Transformers Transformers.js smolagents timm Search documentation main v4.56.1 v4.55.4 v4.53.3 v4.52.3 v4.51.3 v4.50.0 v4.49.0 v4.48.2 v4.47.1 v4.46.3 v4.45.2 v4.44.2 v4.43.4 v4.42.4 v4.41.2 v4.40.2 v4.39.3 v4.38.2 v4.37.2 v4.36.1 v4.35.2 v4.34.1 v4.33.3 v4.32.1 v4.31.0 v4.30.0 v4.29.1 v4.28.1 v4.27.2 v4.26.1 v4.25.1 v4.24.0 v4.23.1 v4.22.2 v4.21.3 v4.20.1 v4.19.4 v4.18.0 v4.17.0 v4.16.2 v4.15.0 v4.14.1 v4.13.0 v4.12.5 v4.11.3 v4.10.1 v4.9.2 v4.8.2 v4.7.0 v4.6.0 v4.5.1 v4.4.2 v4.3.3 v4.2.2 v4.1.1 v4.0.1 v3.5.1 v3.4.0 v3.3.1 v3.2.0 v3.1.0 v3.0.2 v2.11.0 v2.10.0 v2.9.1 v2.8.0 v2.7.0 v2.6.0 v2.5.1 v2.4.1 v2.3.0 v2.2.2 v2.1.1 v2.0.0 v1.2.0 v1.1.0 v1.0.0 doc-builder-html AR DE EN ES FR HI IT JA KO PT TE TR ZH Get started Transformers Installation Quickstart Base classes Inference Training Quantization Export to production Resources Contribute API Join the Hugging Face community and get access to the augmented documentation experience Collaborate on models, datasets and Spaces Faster examples with accelerated inference Switch between documentation themes Sign Up to get started Installation Transformers works with PyTorch . It has been tested on Python 3.9+ and PyTorch 2.2+. Virtual environment A virtual environment helps manage different projects and avoids compatibility issues between dependencies. Take a look at the Install packages in a virtual environment using pip and venv guide if you’re unfamiliar with Python virtual environments. venv uv Create and activate a virtual environment in your project directory with venv . Copied python -m venv . env source . env /bin/activate Python You can install Transformers with pip or uv. pip uv pip is a package installer for Python. Install Transformers with pip in your newly created virtual environment. Copied pip install transformers For GPU acceleration, install the appropriate CUDA drivers for PyTorch . Run the command below to check if your system detects an NVIDIA GPU. Copied nvidia-smi To install a CPU-only version of Transformers and a machine learning framework, run the following command. Copied pip install &#x27;transformers[torch]&#x27; uv pip install &#x27;transformers[torch]&#x27; Test whether the install was successful with the following command. It should return a label and score for the provided text. Copied python -c &quot;from transformers import pipeline; print(pipeline(&#x27;sentiment-analysis&#x27;)(&#x27;hugging face is the best&#x27;))&quot; [{ &#x27;label&#x27; : &#x27;POSITIVE&#x27; , &#x27;score&#x27; : 0.9998704791069031}] Source install Installing from source installs the latest version rather than the stable version of the library. It ensures you have the most up-to-date changes in Transformers and it’s useful for experimenting with the latest features or fixing a bug that hasn’t been officially released in the stable version yet. The downside is that the latest version may not always be stable. If you encounter any problems, please open a GitHub Issue so we can fix it as soon as possible. Install from source with the following command. Copied pip install git+https://github.com/huggingface/transformers Check if the install was successful with the command below. It should return a label and score for the provided text. Copied python -c &quot;from transformers import pipeline; print(pipeline(&#x27;sentiment-analysis&#x27;)(&#x27;hugging face is the best&#x27;))&quot; [{ &#x27;label&#x27; : &#x27;POSITIVE&#x27; , &#x27;score&#x27; : 0.9998704791069031}] Editable install An editable install is useful if you’re developing locally with Transformers. It links your local copy of Transformers to the Transformers repository instead of copying the files. The files are added to Python’s import path. Copied git clone https://github.com/huggingface/transformers.git cd transformers pip install -e . You must keep the local Transformers folder to keep using it. Update your local version of Transformers with the latest changes in the main repository with the following command. Copied cd ~/transformers/ git pull conda conda is a language-agnostic package manager. Install Transformers from the conda-forge channel in your newly created virtual environment. Copied conda install conda-forge::transformers Set up After installation, you can configure the Transformers cache location or set up the library for offline usage. Cache directory When you load a pretrained model with from_pretrained() , the model is downloaded from the Hub and locally cached. Every time you load a model, it checks whether the cached model is up-to-date. If it’s the same, then the local model is loaded. If it’s not the same, the newer model is downloaded and cached. The default directory given by the shell environment variable TRANSFORMERS_CACHE is ~/.cache/huggingface/hub . On Windows, the default directory is C:\\Users\\username\\.cache\\huggingface\\hub . Cache a model in a different directory by changing the path in the following shell environment variables (listed by priority). HF_HUB_CACHE or TRANSFORMERS_CACHE (default) HF_HOME XDG_CACHE_HOME + /huggingface (only if HF_HOME is not set) Older versions of Transformers uses the shell environment variables PYTORCH_TRANSFORMERS_CACHE or PYTORCH_PRETRAINED_BERT_CACHE . You should keep these unless you specify the newer shell environment variable TRANSFORMERS_CACHE . Offline mode To use Transformers in an offline or firewalled environment requires the downloaded and cached files ahead of time. Download a model repository from the Hub with the snapshot_download method. Refer to the Download files from the Hub guide for more options for downloading files from the Hub. You can download files from specific revisions, download from the CLI, and even filter which files to download from a repository. Copied from huggingface_hub import snapshot_download snapshot_download(repo_id= &quot;meta-llama/Llama-2-7b-hf&quot; , repo_type= &quot;model&quot; ) Set the environment variable HF_HUB_OFFLINE=1 to prevent HTTP calls to the Hub when loading a model. Copied HF_HUB_OFFLINE=1 \\ python examples/pytorch/language-modeling/run_clm.py --model_name_or_path meta-llama/Llama-2-7b-hf --dataset_name wikitext ... Another option for only loading cached files is to set local_files_only=True in from_pretrained() . Copied from transformers import LlamaForCausalLM model = LlamaForCausalLM.from_pretrained( &quot;./path/to/local/directory&quot; , local_files_only= True ) &lt; &gt; Update on GitHub ← Transformers Quickstart → Installation Virtual environment Python Source install Editable install conda Set up Cache directory Offline mode ",
      "fetch_method": "direct-html"
    },
    {
      "arxiv_id": "https://r.jina.ai/https://platform.openai.com/docs/guides/function-calling?api-mode=responses.",
      "full_text": "Title: OpenAI Platform\n\nURL Source: https://platform.openai.com/docs/guides/function-calling?api-mode=responses.\n\nMarkdown Content:\nOops!\n-----\n\nWe ran into an issue while authenticating you. If this issue persists, please contact us through our help center at https://help.openai.com.\n\nReturn to homepage\n",
      "fetch_method": "jina-reader"
    },
    {
      "arxiv_id": "https://r.jina.ai/https://arxiv.org/abs/2506.22403},",
      "full_text": "Title: [2506.22403},] Article identifier not recognized\n\nURL Source: https://arxiv.org/abs/2506.22403%7D,\n\nWarning: Target URL returned error 404: Not Found\n\nMarkdown Content:\n[2506.22403},] Article identifier not recognized\n\n===============\n\n[Skip to main content](https://arxiv.org/abs/2506.22403%7D,#content)\n\n[![Image 1: Cornell University Logo](https://arxiv.org/static/browse/0.3.4/images/icons/cu/cornell-reduced-white-SMALL.svg)](https://www.cornell.edu/)\n\nWe gratefully acknowledge support from the Simons Foundation, [member institutions](https://info.arxiv.org/about/ourmembers.html), and all contributors.[Donate](https://info.arxiv.org/about/donate.html)\n\n[](https://arxiv.org/IgnoreMe)\n![Image 2: arxiv logo](https://arxiv.org/static/browse/0.3.4/images/arxiv-logo-one-color-white.svg)\n===================================================================================================\n\n[Help](https://info.arxiv.org/help) | [Advanced Search](https://arxiv.org/search/advanced)\n\nSearch\n\n[![Image 3: arXiv logo](https://arxiv.org/static/browse/0.3.4/images/arxiv-logomark-small-white.svg)](https://arxiv.org/)\n\n[![Image 4: Cornell University Logo](https://arxiv.org/static/browse/0.3.4/images/icons/cu/cornell-reduced-white-SMALL.svg)](https://www.cornell.edu/)\n\nGO\n\nquick links\n-----------\n\n*   [Login](https://arxiv.org/login)\n*   [Help Pages](https://info.arxiv.org/help)\n*   [About](https://info.arxiv.org/about)\n\nArticle identifier '2506.22403},' not recognized\n================================================\n\nYou might instead try to [search for articles](https://arxiv.org/search) using title or author information.\n\nFor additional help on arXiv identifiers, see [understanding the arXiv identifier](https://info.arxiv.org/help/arxiv_identifier.html).\n\n*   [About](https://info.arxiv.org/about)\n*   [Help](https://info.arxiv.org/help)\n\n*   [Contact](https://info.arxiv.org/help/contact.html)\n*   [Subscribe](https://info.arxiv.org/help/subscribe)\n\n*   [Copyright](https://info.arxiv.org/help/license/index.html)\n*   [Privacy Policy](https://info.arxiv.org/help/policies/privacy_policy.html)\n\n*   [Web Accessibility Assistance](https://info.arxiv.org/help/web_accessibility.html)\n*   [arXiv Operational Status](https://status.arxiv.org/)\n\n Get status notifications via [email](https://subscribe.sorryapp.com/24846f03/email/new) or [slack](https://subscribe.sorryapp.com/24846f03/slack/new)\n",
      "fetch_method": "jina-reader"
    },
    {
      "arxiv_id": "https://arxiv.org/pdf/2203.03466",
      "full_text": "Tensor Programs V:\nTuning Large Neural Networks via\nZero-Shot Hyperparameter Transfer\nGreg Yang∗×\nEdward J. Hu∗×†\nIgor Babuschkin◦\nSzymon Sidor◦\nXiaodong Liu×\nDavid Farhi◦\nNick Ryder◦\nJakub Pachocki◦\nWeizhu Chen×\nJianfeng Gao×\n×Microsoft Corporation\n◦OpenAI\nAbstract\nHyperparameter (HP) tuning in deep learning is an expensive process, prohibitively\nso for neural networks (NNs) with billions of parameters. We show that, in the\nrecently discovered Maximal Update Parametrization (µP), many optimal HPs\nremain stable even as model size changes. This leads to a new HP tuning paradigm\nwe call µTransfer: parametrize the target model in µP, tune the HP indirectly on a\nsmaller model, and zero-shot transfer them to the full-sized model, i.e., without\ndirectly tuning the latter at all. We verify µTransfer on Transformer and ResNet.\nFor example, 1) by transferring pretraining HPs from a model of 13M parameters,\nwe outperform published numbers of BERT-large (350M parameters), with a total\ntuning cost equivalent to pretraining BERT-large once; 2) by transferring from\n40M parameters, we outperform published numbers of the 6.7B GPT-3 model, with\ntuning cost only 7% of total pretraining cost. A Pytorch implementation of our\ntechnique can be found at github.com/microsoft/mup and installable via pip\ninstall mup.\n1\nIntroduction\n20\n18\n16\n14\n12\n10\nlog2LearningRate\n3.5\n4.0\n4.5\n5.0\n5.5\n6.0\n6.5\n7.0\nTraining Loss\noptimum shifts\nStandard Practice\nWidth\n128\n256\n512\n1024\n2048\n4096\n8192\n20\n18\n16\n14\n12\n10\nlog2LearningRate\noptimum stable\nOur Work\nFigure 1: Training loss against learning rate on\nTransformers of varying dmodel trained with Adam.\nConventionally and in contrast with our technique,\ndifferent widths do not share the same optimal hy-\nperparameter; wider networks do not always per-\nform better than narrower ones; in fact they under-\nperform the same-width networks in our technique\neven after tuning learning rate (see dashed line).\nSee Sections 3 and 4 for experimental setup.\nHyperparameter (HP) tuning is critical to deep\nlearning. Poorly chosen HPs result in subpar\nperformance and training instability. Many pub-\nlished baselines are hard to compare to one\nanother due to varying degrees of HP tuning.\nThese issues are exacerbated when training ex-\ntremely large deep learning models, since state-\nof-the-art networks with billions of parameters\nbecome prohibitively expensive to tune.\nRecently, [57] showed that different neural net-\nwork parametrizations induce different inﬁnite-\nwidth limits and proposed the Maximal Update\nParametrization (abbreviated µP) (summarized\nin Table 3) that enables “maximal” feature learn-\ning in the limit. Intuitively, it ensures that each\nlayer is updated on the same order during train-\ning regardless of width.2 In contrast, while the\nstandard parametrization (SP) ensures activations are of unit order at initialization, it actually causes\nthem to blow up in wide models during training [57] essentially due to an imbalance of per-layer\n†Work done partly during Microsoft AI Residency Program.\n∗Equal contribution. Order is random. Correspondence to {gregyang, edwardhu}@microsoft.com\n2i.e., the updates’ effect on activations becomes roughly independent of width in the large width limit.\narXiv:2203.03466v2  [cs.LG]  28 Mar 2022\n\nAlgorithm 1 Tuning a Large Target Model via µTransfer\n1: Parametrize target model in Maximal Update Parametrization (µP)\n2: Tune a smaller version (in width and/or depth) of target model\n3: Copy tuned hyperparameters to target model\nTable 1: Hyperparameters That Can Be µTransferred, Not µTransferred, or µTransferred\nAcross, with a few caveats discussed in Section 6.1. * means empirically validated only on Trans-\nformers, while all others additionally have theoretical justiﬁcation.\nµTransferable\nNot µTransferable\nµTransferred Across\noptimization related, init,\nregularization\nwidth, depth*, batch size*,\nparameter multipliers, etc\n(dropout, weight decay, etc)\ntraining time*, seq length*\nlearning rate (also see Fig. 5). We leverage µP to zero-shot transfer HPs from small models to large\nmodels in this work – that is, we obtain near optimal HPs on a large model without directly tuning\nit at all! While practitioners have always guessed HPs of large models from those of small models,\nthe results are hit-or-miss at best because of incorrect parametrization. For example, as shown in\nFig. 1, in a Transformer, the optimal learning rate is stable with width in µP (right) but far from\nso in standard parametrization (left). In addition to width, we empirically verify that, with a few\ncaveats, HPs can also be transferred across depth (in Section 6.1) as well as batch size, language\nmodel sequence length, and training time (in Appendix G.2.1). This reduces the tuning problem of\nan (arbitrarily) large model to that of a (ﬁxed-sized) small model. Our overall procedure, which we\ncall µTransfer, is summarized in Algorithm 1 and Fig. 2, and the HPs we cover are summarized in\nTables 1 and 2.\n$$$$$$\n$$\nStandard\nPractice\nOur Method\nDirectly tune large model\nShrink\nTune\nTransfer\n𝜇\n𝜇\nFigure 2: Illustration of µTransfer\nThere are several beneﬁts to our approach: 1. Better Per-\nformance: µTransfer is not just about predicting how the\noptimal learning rate scales in SP. In general, we expect the\nµTransferred model to outperform its SP counterpart with\nlearning rate optimally tuned. For example, this is the case\nin Fig. 1 with the width-8192 Transformer. We discuss the\nreason for this in Section 5 and Appendix C. 2. Speedup:\nIt provides massive speedup to the tuning of large mod-\nels. For example, we are able to outperform published\nnumbers of (350M) BERT-large [11] purely by zero-shot\nHP transfer, with tuning cost approximately equal to 1\nBERT-large pretraining. Likewise, we outperform the pub-\nlished numbers of the 6.7B GPT-3 model [7] with tuning\ncost being only 7% of total pretraining cost. For models\non this scale, HP tuning is not feasible at all without our\napproach. 3. Tune Once for Whole Family: For any ﬁxed family of models with varying width and\ndepth (such as the BERT family or the GPT-3 family), we only need to tune a single small model\nand can reuse its HPs for all models in the family.3 For example, we will use this technique to\ntune BERT-base (110M parameters) and BERT-large (350M parameters) simultaneously by trans-\nferring from a 13M model. 4. Better Compute Utilization: While large model training needs to\nbe distributed across many GPUs, the small model tuning can happen on individual GPUs, greatly\nincreasing the level of parallelism for tuning (and in the context of organizational compute clusters,\nbetter scheduling and utilization ratio). 5. Painless Transition from Exploration to Scaling Up:\nOften, researchers explore new ideas on small models but, when scaling up, ﬁnd their HPs optimized\nduring exploration work poorly on large models. µTransfer would solve this problem.\nIn addition to the HP stability property, we ﬁnd that wider is better throughout training in µP, in\ncontrast to SP (Section 8). This increases the reliability of model scaling in deep learning.\nIn this work, we primarily focus on hyperparameter transfer with respect to training loss. In settings\nwhere regularization is not the bottleneck to test performance, as in all of our experiments here, this\nalso translates to efﬁcacy in terms of test loss. In other settings, such as ﬁnetuning of models on small\ndatasets, µTransfer may not be sufﬁcient, as we discuss in Section 6.1.\n3but possibly not for different data and/or tasks.\n2\n\nTable 2: Examples of µTransferable Hyperparameters. All of the below can also be specialized\nto per-layer hyperparameters.\nOptimizer Related\nInitialization\nParameter Multipliers\nlearning rate (LR), momentum,\nper-layer\nmultiplicative constants after\nAdam beta, LR schedule, etc\ninit. variance\nweight/biases, etc\nOur Contributions\n• We demonstrate it is possible to zero-shot transfer near optimal HPs to a large model from a\nsmall version via the Maximal Update Parametrization (µP) from [57].\n• While [57] only covered SGD, here we derive µP for Adam as well (Table 3).\n• We propose a new HP tuning technique, µTransfer, for large neural networks based on this\nobservation that provides massive speedup over conventional methods and covers both SGD\nand Adam training;\n• We thoroughly verify our method on machine translation and large language model pretrain-\ning (in Section 7.3) as well as image classiﬁcation (in Appendix G.1);\n• We release a PyTorch [35] package for implementing µTransfer painlessly. A sketch of this\npackage is given in Appendix H.\nTerminologies\nSometimes, to be less ambiguous, we often refer to the “large model” as the target\nmodel, as it is the model we wish to ultimately tune, while we refer to the “small model” as the\nproxy model, as it proxies the HP tuning process. We follow standard notation dmodel, dhead =\ndk, dv, nhead, dffn regarding dimensions in a Transformer; one can see Fig. 11 for a refresher.\nTensor Programs Series\nThis paper is the 5th installment of the Tensor Programs series. While it\nis self-contained with the target audience being practitioners and empirical researchers, this paper\npresents the ﬁrst major practical payoff of the theoretical foundation built in previous works [53–58].\n2\nParametrization Matters: A Primer\nIn this section, we give a very basic primer on why the correct parametrization can allow HP transfer\nacross width, but see Appendices J.1 to J.3 for more (mathematical) details.\nThe Central Limit Theorem (CLT) says that, if x1, . . . , xn are iid samples from a zero-mean, unit-\nvariance distribution, then\n1\n√n(x1 + · · · + xn) converges to a standard Gaussian N(0, 1) as n →∞.\nTherefore, we can say that\n1\n√n is the right order of scaling factor cn such that cn(x1 + · · · + xn)\nconverges to something nontrivial. In contrast, if we set cn = 1/n, then cn(x1 + · · · + xn) →0; or\nif cn = 1, then cn(x1 + · · · + xn) blows up in variance as n →∞.\nNow suppose we would like to minimize the function\nFn(c)\ndef\n=\nE\nx1,...,xn f(c(x1 + · · · + xn))\n(1)\nover c ∈R, for some bounded continuous function f : R →R. If we reparametrize c = α/√n for\nα ∈R, then by CLT, Gn(α)\ndef\n= Fn(c) →E f(N(0, α2)) stabilizes into a function of α as n →∞.\nThen for sufﬁciently large n, the optimal α∗\nn\ndef\n= arg minα Gn(α) should be close to α∗\nN for any\nN > n, and indeed, for N = ∞— this precisely means we can transfer the optimal c∗\nn or α∗\nn for a\nsmaller problem (say Fn) to a larger problem (say FN): GN is approximately minimized by α∗\nn and\nFN is approximately minimized by c∗\nn\np\nn/N. Because the transfer algorithm is simply copying α,\nwe say the parametrization c = α/√n is the correct parametrization for this problem.\nIn the scenario studied in this paper, x1, . . . , xn are akin to randomly initialized parameters of a\nwidth-n neural network, c is akin to a HP such as learning rate, and f is the test-set performance\nof the network after training, so that Fn gives its expectation over random initializations. Just as\nin this example, if we parametrize the learning rate and other HPs correctly, then we can directly\ncopy the optimal HPs for a narrower network into a wide network and expect approximately optimal\n3\n\nperformance — this is the (zero-shot) hyperparameter transfer we propose here. It turns out the\nMaximal Update Parametrization (µP) introduced in [57] is correct (akin to the parametrization in α\nabove), while the standard parametrization (SP) is incorrect (akin to the parametrization in c). We will\nreview both parametrizations shortly. Theoretically, a µP network has a well-deﬁned inﬁnite-width\nlimit — akin to (x1 + · · · + xn)/√n having a N(0, 1) limit by CLT — while a SP network does not\n(the limit will blow up) [57].4 In fact, based on the theoretical foundation laid in [57], we argue in\nAppendix J.3 that µP should also be the unique parametrization that allows HP transfer across width.\nFor a more formal discussion of the terminologies parametrization and transfer, see Appendix A\nWe emphasize that, to ensure transferability of any hyperparameter (such as learning rate), it’s not\nsufﬁcient to reparametrize only that hyperparameter, but rather, we need to identify and correctly\nreparametrize all hyperparameters in Table 2. For example, in Fig. 1, the wide models in SP still\nunderperform their counterparts in µP, even with learning rate tuned optimally. This is precisely\nbecause SP does not scale parameter multipliers and input/output layer learning rates correctly in\ncontrast to µP (see Table 3). See Appendix C for more intuition via a continuation of our example\nhere. We shall also explain this more concretely in the context of neural networks in Section 5.\n3\nHyperparameters Don’t Transfer Conventionally\nIn the community there seem to be conﬂicting assumptions about HP stability. A priori, models\nof different sizes don’t have any reason to share the optimal HPs. Indeed, papers aiming for state-\nof-the-art results often tune them separately. On the other hand, a nontrivial fraction of papers in\ndeep learning ﬁxes all HPs when comparing against baselines, which reﬂects an assumption that\nthe optimal HPs should be stable — not only among the same model of different sizes but also\namong models of different designs — therefore, such comparisons are fair. Here, we demonstrate HP\ninstability across width explicitly in MLP and Transformers in the standard parametrization. We will\nonly look at training loss to exclude the effect of regularization.\nMLP with Standard Parametrization\nWe start with a 2-hidden-layer MLP with activation func-\ntion φ, using the standard parametrization5 with LeCun initialization6 akin to the default in PyTorch:\nf(ξ) = W 3⊤φ(W 2⊤φ(W 1⊤ξ + b1) + b2)\nwith init.\nW 1 ∼N(0, 1/din), W {2,3} ∼N(0, 1/n), b{1,2} = 0,\n(2)\n14\n12\n10\n8\n6\n4\n2\nlog2LearningRate\n0.0\n0.5\n1.0\n1.5\n2.0\nTraining Loss\nSP / xent\nwidth\n256\n512\n1024\n2048\n4096\n8192\n14\n12\n10\n8\n6\n4\n2\nlog2LearningRate\n0.0\n0.5\n1.0\n1.5\n2.0\nP / xent\nFigure 3:\nMLP width different hidden sizes trained\nfor 20 epoch on CIFAR-10 using SGD. Left uses stan-\ndard parametrization (SP); right uses maximal update\nparametrization (µP). µP networks exhibit better learning\nrate stability than their SP counterparts.\nwhere W 1\n∈\nRdin×n, b1\n∈\nRn,\nW 2\n∈\nRn×n, b2\n∈\nRn, W 3\n∈\nRn×dout and din, n, and dout are\nthe input, hidden, and output dimen-\nsions. The particular MLP we use has\nφ = ReLU and a cross-entropy (xent)\nloss function. We deﬁne the width of\nMLP as the hidden size n, which is\nvaried from 256 to 8192. The mod-\nels are trained on CIFAR-10 for 20\nepochs, which is more than enough to\nensure convergence.\nAs shown on the left in Fig. 3, the\noptimal learning rate shifts by roughly\nan order of magnitude as the width\nincreases from 256 to 8192; using the\noptimal learning of the smallest model\non the largest model gives very bad performance, if not divergence.\nTransformer with Standard Parametrization\nThis perhaps unsurprising observation holds for\nmore complex architectures such as Transformer as well, as shown in Fig. 1 (left). We deﬁne width\n4The more theoretically astute reader may observe that SP with a Θ(1/width) learning rate induces a\nwell-deﬁned inﬁnite-width limit exists as well. Nevertheless, this does not allow HP transfer because this limit is\nin kernel regime as shown in [57]. See Appendix J.3 for more discussions.\n5i.e. the default parametrization offered by common deep learning frameworks. See Table 3 for a review.\n6The key here is that the init. variance ∝1/fan_in, so the same insights here apply with e.g. He initialization.\n4\n\nTable 3: µP[57] and SP for General Neural Networks. Here, we emphasize the scaling with width\n(fan_in or fan_out); in practice, we may insert tunable multipliers in front of fan_in and fan_out as\nin Eq. (4). The fan_out of a bias vector is its dimension (whereas fan_in is 1). Purple text highlights\nkey differences from standard parametrization (SP); Gray text recalls the corresponding SP. SGD\n(resp. Adam) here can be replaced by variants such as SGD with momentum (resp. Adagrad, etc);\nsee Appendix B.3 for other optimizers. In general, the three columns here can be interpreted as\nlinear layers that have {ﬁnite, inﬁnite, inﬁnite} input dimension and {inﬁnite, ﬁnite, inﬁnite} output\ndimension in an inﬁnite-width network; this description generalizes more readily to other parameters\nsuch as those of layernorm. Transformer µP requires one more modiﬁcation (1/d attention instead\nof 1/\n√\nd); see Deﬁnition 4.1. This version of µP gets rid of parameter multipliers; for the version\nsimilar to that in [57], see Table 9. Also see Table 8 for a µP formulation that is easier to implement\n(and compatible with input/output weight sharing). Further explanation of this table can be found in\nAppendix B. Its derivation can be found in Appendix J.\nInput weights & all biases\nOutput weights\nHidden weights\nInit. Var.\n1/fan_in\n1/fan_in2\n(1/fan_in)\n1/fan_in\nSGD LR\nfan_out\n(1)\n1/fan_in\n(1)\n1\nAdam LR\n1\n1/fan_in\n(1)\n1/fan_in\n(1)\nas dmodel, with dk = dq = dv = dmodel/nhead and dffn = 4dmodel. The models are trained on\nwikitext-2 for 5 epochs. In Fig. 18 in the appendix we also show the instability of initialization scale\nand other HPs.\n4\nUnlocking Zero-Shot Hyperparameter Transfer with µP\nWe show that µP solves the problems we see in Section 3.\nMLP with µP\nFor the MLP in Section 3, to switch to µP, we just need to modify Eq. (2)’s\ninitialization of the last layer and its learning rates of the ﬁrst and last layer as well as of the biases.\nThe basic form is7\ninitialize\nW 1 ∼N(0, 1/din), W 2 ∼N(0, 1/n), W 3 ∼N(0, 1/n2), b{1,2} = 0\nwith SGD learning rates\nηW 1 = ηb1 = ηb2 = ηn, ηW 2 = η, ηW 3 = ηn−1.\n(3)\nHere, η speciﬁes the “master” learning rate, and we highlighted in purple the differences in the two\nparametrizations. This basic form makes clear the scaling with width n of the parametrization, but in\npractice we will often insert (possibly tune-able) multiplicative constants in front of each appearance\nof n. For example, this is useful when we would like to be consistent with a SP MLP at a base width\nn0. Then we may insert constants as follows: For ˜n\ndef\n= n/n0,\ninitialize\nW 1 ∼N(0, 1/din), W 2 ∼N(0, 1/n), W 3 ∼N(0, 1/n·˜n), b{1,2} = 0\nwith SGD learning rates\nηW 1 = ηb1 = ηb2 = η˜n, ηW 2 = η, ηW 3 = η˜n−1.\n(4)\nThen at width n = n0, all purple factors above are 1, and the parametrization is identical to SP\n(Eq. (2)) at width n0. Of course, as n increases from n0, then Eq. (4) quickly deviates from Eq. (2).\nIn other words, for a particular n, µP and SP can be identical up to the choice of some constants (in\nthis case n0), but µP determines a different “set\" of networks and optimization trajectory than SP as\none varies n. As we will see empirically in the next section, this deviation is crucial for HP transfer.\nIndeed, in Fig. 3(right), we plot the CIFAR10 performances, over various learning rates and widths,\nof µP MLPs with n0 = 128. In contrast to SP, the optimal learning rate under µP is stable. This\nmeans that, the best learning rate for a width-128 network is also best for a width-8192 network in µP\n— i.e. HP transfer works — but not for SP. In addition, we observe performance for a ﬁxed learning\nrate always weakly improves with width in µP , but not in SP.\nThis MLP µP example can be generalized easily to general neural networks trained under SGD or\nAdam, as summarized in Table 3, which is derived in Appendix J.\n7While superﬁcially different, this parametrization is equivalent to the µP deﬁned in [57].\n5\n\n14\n12\n10\n8\n1\n2\n3\n4\n5\nWidth\nTraining Loss\nWidth\n128\n256\n512\n1024\n2048\n4096\n5\n0\n5\n10\n15\n1\n2\n3\n4\n5\n5.0\n2.5\n0.0\n2.5\n1\n2\n3\n4\n5\n(a)\n(b)\n(c)\n(d)\n(e)\n(f)\n1.0\n1.5\n2.0\n2.5\n3.0\n3.5\n4.0\n4.5\n14\n12\n10\n8\nlog2LearningRate\n3.75\n4.00\n4.25\n4.50\n4.75\n5.00\n5.25\n5.50\n5.75\nDepth\nTraining Loss\nDepth\n2\n4\n8\n16\n32\n5\n0\n5\n10\n15\nlog2\noutput\n3.0\n3.5\n4.0\n4.5\n5.0\n5.0\n2.5\n0.0\n2.5\nlog2InitStd\n3.5\n4.0\n4.5\n5.0\n5.5\n6.0\n6.5\n7.0\n(a)\n(b)\n(c)\n(d)\n(e)\n(f)\nLR Schedule\n3.8\n3.9\n4.0\n4.1\n4.2\n4.3\n4.4\n4.5\nFigure 4: Empirical validation of the stability of four representative hyperparameters on pre-\nLN Transformers in µP: learning rate, last layer weight multiplier αoutput, weight initialization\nstandard deviation, and learning rate schedule. We use the following learning rate schedules: (a)\nlinear decay; (b) StepLR @ [5k, 8k] with a decay factor of 0.1; (c) StepLR @ [4k, 7k] with a decay\nfactor of 0.3; (d) cosine annealing; (e) constant; (f) inverse square-root decay. All models are trained\non wikitext-2 for 10k steps. When not speciﬁed in the legend, the width used is 256, depth 2, batch\nsize 20, sequence length 256, and LR schedule constant. We sweep a particular HP, corresponding to\neach column, while ﬁxing all others constant. See Section 6.1 for discussion of these results.\nTransformers with µP\nWe repeat the experiments with base width n0 = 128 for Transformers:\nDeﬁnition 4.1. The Maximal Update Parametrization (µP) for a Transformer is given by Table 3\nand 1/d attention instead of 1/\n√\nd, i.e. the attention logit is calculated as q⊤k/d instead of q⊤k/\n√\nd\nwhere query q and key k have dimension d.8\nThe results are shown on the right in Fig. 1, where the optimal learning rate is stable, and the\nperformance improves monotonically with width. See Appendix B for further explanation of µP.\n5\nThe Defects of SP and How µP Fixes Them\nThe question of SP vs µP has already been studied at length in [57]. Here we aim to recapitulate the\nkey insights, with more explanations given in Appendix J.3.\nAn Instructive Example\nAs shown in [57] and Appendix J.3, in SP, the network output will blow\nup with width after 1 step of SGD. It’s instructive to consider a 1-hidden-layer linear perceptron\nf(x) = V ⊤Ux with scalar inputs and outputs, as well as weights V, U ∈Rn×1. In SP, Vα ∼\nN(0, 1/n) ad Uα ∼N(0, 1) for each α ∈[n]. This sampling ensures that f(x) = Θ(|x|) at\ninitialization. After 1 step of SGD with learning rate 1, the new weights are V ′ ←V + θU, U ′ ←\nU + θV , where θ is some scalar of size Θ(1) depending on the inputs, labels, and loss function. But\nnow\nf(x) = V ′⊤U ′x = (V ⊤U + θU ⊤U + θV ⊤V + θ2U ⊤V )x\n(5)\nblows up with width n because U ⊤U = Θ(n) by Law of Large Numbers.\nNow consider the same network in µP. According to Table 3, we now have Vα ∼N(0, 1/n2) in\ncontrast to SP, but Uα ∼N(0, 1) as before, with learning rates ηV = 1/n, ηU = n. After 1 step of\nSGD, we now have\nf(x) = (V ⊤U + θn−1U ⊤U + θnV ⊤V + θ2U ⊤V )x,\n8This is roughly because during training, q and k will be correlated so q⊤k actually scales like d due to Law\nof Large Numbers, in contrast to the original motivation that q, k are uncorrelated at initialization so Central\nLimit applies instead. See Appendix J.2.1 for a more in-depth discussion.\n6\n\n0.0\n0.5\n1.0\n1.5\nSP\nstd(xt\nx0)\nlogits\nt\n0\n1\n2\n3\n4\n0\n20\n40\n60\nattn logits\n0.0000\n0.0005\n0.0010\n0.0015\n0.0020\nword embedding\n0\n2000\n4000\nwidth\n0.00\n0.05\n0.10\n0.15\nP\nstd(xt\nx0)\n0\n2000\n4000\nwidth\n0.000\n0.025\n0.050\n0.075\n0.100\n0.125\n0\n2000\n4000\nwidth\n0.0000\n0.0005\n0.0010\n0.0015\nFigure 5: Logits and attention logits, but not word embeddings, of a Transformer blow up with\nwidth in SP after 1 step of training. In contrast, all three are well-behaved with width in µP. Here\nwe measure how much different values change coordinatewise from initialization over 4 steps of\nAdam updates, as a function of width. Speciﬁcally, we plot the standard deviation of the coordinates\nof xt −x0, for t = 0, . . . , 4, and x ∈{logits, attention logits, word embeddings}, where t = 0\nindicates initialization.\nand one can verify this is Θ(1) and thus does not blow up with width.9\nSome Layers Update Too Fast, Others Too Slow\nOne can observe the same behavior in more\nadvanced architectures like Transformers and optimizers like Adam; in fact, in SP, other hidden\nquantities like attention logits will also blow up with width after 1 step, but in µP still remain bounded,\nas shown in Fig. 5(middle).\nOne might think scaling down the learning rate with width can solve this problem in SP. However,\nother hidden activations like the word embedding (Fig. 5(right)) in a Transformer update by a width-\nindependent amount for each step of training, so scaling down the learning rate will effectively mean\nthe word embeddings are not learned in large width models. Similar conclusions apply to other\nmodels like ResNet (in fact, one can observe in the SP linear MLP example above, the input layer\nis updated much more slowly than the output layer). On the other hand, µP is designed so that all\nhidden activations update with the same speed in terms of width (see Appendix J.2 for why).\nPerformance Advantage of µP\nThis is why a wide model tuned with µTransfer should in general\noutperform its SP counterpart with (global) learning rate tuned. For example, this is the case for\nthe width-8192 Transformer in Fig. 1, where, in SP, the optimal learning rate needs to mollify the\nblow-up in quantities like logits and attention logits, but this implies others like word embeddings do\nnot learn appreciably. This performance advantage means µTransfer does more than just predicting\nthe optimal learning rate of wide SP models. Relatedly, we observe, for any ﬁxed HP combination,\ntraining performance never decreases with width in µP, in contrast to SP (e.g., the µP curves in\nFigs. 1, 3 and 16 do not cross, but the SP curves do; see also Section 8).\n6\nWhich Hyperparameters Can Be µTransferred?\nIn this section, we explore how common HPs ﬁt into our framework. In general, they can be divided\ninto three kinds, summarized in Table 1:\n1. those that can transfer from the small to the large model, such as learning rate (Table 2);\n2. those that primarily control regularization and don’t work well with our technique; and\n3. those that deﬁne training scale, such as width as discussed above as well as others like depth\nand batch size, across which we transfer other HPs.\nThose in the ﬁrst category transfer across width, as theoretically justiﬁed above in Section 2. To\npush the practicality and generality of our technique, we empirically explore the transfer across\n9Note in this example, Glorot initialization [13] (i.e. with variance 1/(fan_in + fan_out)) would scale\nasymptotically the same as µP and thus is similarly well-behaved. However, if one adds layernorm or batchnorm,\nthen Glorot will cause logit blowup like SP, but µP still will not.\n7\n\nthe other dimensions in the third category. Note that µTransfer across width is quite general, e.g.\nit allows varying width ratio of different layers or number of attention heads in a Transformer;\nsee Appendix E.2. This will be very useful in practice. For the second category, the amount of\nregularization (for the purpose of controlling overﬁtting) naturally depends on both the model size\nand data size, so we should not expect transfer to work if the parametrization only depends on model\nsize. We discuss these HPs in more detail in Appendix E.1.\n6.1\nEmpirical Validation and Limitations\nOur empirical investigations focus on Transformers (here) and ResNet (in Appendix G.1.1), the\nmost popular backbones of deep learning models today. We train a 2-layer pre-layernorm µP10\nTransformer with 4 attention heads on Wikitext-2. We sweep one of four HPs (learning rate, output\nweight multiplier, initialization standard deviation, and learning rate schedule) while ﬁxing the others\nand sweeping along width and depth (with additional results in Fig. 19 on transfer across batch size,\nsequence length, and training time). Fig. 4 shows the results averaged over 5 random seeds.\nEmpirically, we ﬁnd that for language modeling on Transformers, HPs generally transfer across scale\ndimensions if some minimum width (e.g. 256), depth (e.g., 4), batch size (e.g., 32), sequence length\n(e.g., 128), and training steps (e.g., 5000) are met, and the target scale is within the “reasonable range”\nas in our experiments. Now, there are some caveats. While the exact optimum can shift slightly\nwith increasing scale, this shift usually has very small impact on the loss, compared to SP (Figs. 1\nand 3(left)). However, there are some caveats. For example, the best initialization standard deviation\ndoes not seem to transfer well across depth (2nd row, 3rd column), despite having a stabler optimum\nacross width. In addition, while our results on width, batch size, sequence length, and training time\nstill hold for post-layernorm (Fig. 17),11 the transfer across depth only works for pre-layernorm\nTransformer. Nevertheless, in practice (e.g. our results in Section 7.3) we ﬁnd that ﬁxing initialization\nstandard deviation while tuning other HPs works well when transferring across depth.\n7\nEfﬁciency and Performance of µTransfer\nNow that the plausibility of µTransfer has been established in toy settings, we turn to more realistic\nscenarios to see if one can achieve tangible gains. Speciﬁcally, we perform HP tuning only on a\nsmaller proxy model, test the obtained HPs on the large target model directly, and compare against\nbaselines tuned using the target model. We seek to answer the question: Can µTransfer make HP\ntuning more efﬁcient while achieving performance on par with traditional tuning? As we shall see by\nthe end of the section, the answer is positive. We focus on Transformers here, while experiments on\nResNets on CIFAR10 and Imagenet can be found as well in Appendix G.1. All of our experiments\nare run on V100 GPUs.\n7.1\nTransformer on IWSLT14 De-En\nSetup\nIWSLT14 De-En is a well-known machine translation benchmark. We use the default IWSLT\n(post-layernorm) Transformer implemented in fairseq [33] with 40M parameters, which we denote\nas the 1x model.12 For µTransfer, we tune on a 0.25x model with 1/4 of the width, amounting to\n4M parameters. For this experiment, we tune via random search the learning rate η, the output layer\nparameter multiplier αoutput, and the attention key-projection weight multiplier αattn. See the grid\nand other experimental details in Appendix F.1.\nWe compare transferring from the 0.25x model with tuning the 1x model while controlling the total\ntuning budget in FLOPs.13 To improve the reproducibility of our result: 1) we repeat the entire HP\nsearch process (a trial) 25 times for each setup, with number of samples as indicated in Table 4, and\nreport the 25th, 50th, 75th, and 100th percentiles in BLEU score; 2) we evaluate each selected HP\ncombination using 5 random initializations and report the mean performance.14\n10“2 layers” means the model has 2 self-attention blocks. To compare with SP Transformer, see Fig. 18.\n11in fact, post-layernorm Transformers are much more sensitive to HPs than pre-layernorm, so our technique\nis more crucial for them, especially for transfer across width. Fig. 1 uses post-layernorm.\n12https://github.com/pytorch/fairseq/blob/master/examples/translation/README.md.\n13Ideally we would like to measure the wall clock time used for tuning. However, smaller models such as the\nproxy Transformer used for IWSLT are not efﬁcient on GPUs, so wall clock time would not reﬂect the speedup\nfor larger models like GPT-3. Thus, we measure in FLOPs, which is less dependent on hardware optimization.\n14We do not report the standard deviation over random initializations to avoid confusion.\n8\n\nTable 4: Transformer on IWSLT14 De-En. 1x and 0.25x refers to scaling of width only. Compared\nto traditional tuning (“Tuning on 1x”), µTransfer from 0.25x provides better and more reliable\noutcome given ﬁxed amount of compute. On the other hand, naive transfer (i.e. with SP instead of\nµP) fails completely. The percentiles are over independent trials, with each trial involving the entire\ntuning process with a new HP random search.\nVal. BLEU Percentiles\nSetup\nTotal Compute\n#Samples\n25\n50\n75\n100\nfairseq[33] default\n-\n-\n-\n-\n-\n35.40\nTuning on 1x\n1x\n5\n33.62\n35.00\n35.35\n35.45\nNaive transfer from 0.25x\n1x\n64\ntraining diverged\nµTransfer from 0.25x (Ours)\n1x\n64\n35.27\n35.33\n35.45\n35.53\nWe pick the HP combination that achieves the lowest validation loss15 for each trial. The reported\nbest outcome is chosen according to the validation loss during tuning. We compare against the default\nin fairseq, which is presumably heavily tuned. The result is shown in Table 4.\n4\n3\n2\n1\n0\n1\n2\n3\nlog2Compute\n34.4\n34.6\n34.8\n35.0\n35.2\n35.4\nBLEU Score\nMethod\nOurs\nConventional\n10\n20\n30\n40\n50\n60\n# Samples\nMethod\nOurs\nConventional\nFigure 6: Efﬁciency-performance Pareto fron-\ntier of µTransfer compared to conventional tuning,\non IWSLT Transformer, using random HP search\nas the base method. We plot the median BLEU\nscore over 25 trials (Left) against relative compute\nbudget in log scale and (Right) against number\nof HP samples taken. While with the same num-\nber of samples, µTransfer slightly underperforms\nconventional tuning, this gap vanishes with more\nsamples, and in terms of compute, our Pareto fron-\ntier strongly and consistently dominates that of\nconventional tuning. Note that, in larger models\n(e.g. BERT or GPT-3, not shown here), we believe\nour efﬁciency advantage will only widen as our\nsmall proxy model can stay the same size while\nthe target model grows.\nPerformance Pareto Frontier\nThe result\nabove only describes a particular compute bud-\nget. Is µTransfer still preferable when we have\na lot more (or less) compute? To answer this\nquestion, we produce the compute-performance\nPareto frontier in Fig. 6(left), where we repeat\nthe above experiment with different compute\nbudgets. Evidently, our approach completely\ndominates conventional tuning.\nSample Quality of Proxy Model vs Target\nModel\nThe Pareto frontier in Fig. 6(right) sug-\ngests that, given a ﬁxed number of random sam-\nples from the HP space, 1) tuning the target\nmodel directly yields slightly better results than\ntuning the proxy model (while taking much\nmore compute of course), but 2) this perfor-\nmance gap seems to vanish as more samples\nare taken. This can be explained by the intu-\nition that the narrower proxy model is a “noisy\nestimator” of the wide target model [57].With\nfew samples, this noise can distort the random\nHP search, but with more samples, this noise is\nsuppressed.\n7.2\nTransformer on WMT14 En-De\nWe scale up to WMT14 En-De using the large (post-layernorm) Transformer from [50] with 211M\nparameters. We tune on a proxy model with 15M parameters by shrinking dmodel, dffn, and nhead.\nFor this experiment, we tune via random search the learning rate η, the output layer parameter\nmultiplier αoutput, and the attention key-projection weight multiplier αattn following the grid\nin Appendix F.2. The result is shown in Table 5: While random search with 3 HP samples far\nunderperforms the fairseq default, we are able to match it via transfer using the same tuning budget.\n7.3\nBERT\nFinally, we consider large-scale language model pretraining where HP tuning is known to be challeng-\ning. Using Megatron (pre-layernorm) BERT [43] as a baseline, we hope to recover the performance\nof the published HPs by only tuning a proxy model that has roughly 13M parameters, which we call\nBERT-prototype. While previous experiments scaled only width, here we will also scale depth, as\ndiscussed in Section 6 and validated in Fig. 4. We use a batch size of 256 for all runs and follow the\n15We ﬁnd this provides more reliable result than selecting for the best BLEU score.\n9\n\nTable 5: Transformers on WMT14 En-De. 1x and 0.25x refers to scaling of width only. We report\nBLEU ﬂuctuation over 3 independent trials, i.e., 3 independent random HP searches.\nVal. BLEU Percentiles\nSetup\nTotal Compute\n#Samples\nWorst\nMedian\nBest\nfairseq[33] default\n-\n-\n-\n-\n26.40\nTuning on 1x\n1x\n3\ntraining diverged\n25.69\nNaive transfer from 0.25x\n1x\n64\ntraining diverged\nµTransfer from 0.25x (Ours)\n1x\n64\n25.94\n26.34\n26.42\nstandard ﬁnetuning procedures. For more details on BERT-prototype, what HPs we tune, and how we\nﬁnetune the trained models, see Appendix F.3.\nDuring HP tuning, we sample 256 combinations from the search space and train each combination\non BERT-prototype for 105 steps. The total tuning cost measured in FLOPs is roughly the same as\ntraining 1 BERT-large for the full 106 steps; the exact calculation is shown in Appendix F.3. The\nresults are shown in Table 6. Notice that on BERT-large, we obtain sizeable improvement over the\nwell-tuned Megatron BERT-large baseline.\nTable 6: BERT pretraining. HP transfer outperforms published baselines without tuning the full\nmodel directly at all. We tune BERT-base and BERT-large simultaneously via a single proxy model,\nBERT-prototype. The total tuning cost = the cost of pretraining a single BERT-large. Model speedup\nrefers to the training speedup of BERT-prototype over BERT-base or BERT-large. Total speedup in\naddition includes time saving from transferring across training steps. Both speedups can be interpreted\neither as real-time speedup on V100s or as FLOPs speedup (which turn out to be empirically very\nsimilar in this case).\nModel\nMethod\nModel Speedup\nTotal Speedup\nTest loss\nMNLI (m/mm)\nQQP\nBERTbase\nMegatron Default\n1x\n1x\n1.995\n84.2/84.2\n90.6\nBERTbase\nNaive Transfer\n4x\n40x\ntraining diverged\nBERTbase\nµTransfer (Ours)\n4x\n40x\n1.970\n84.3/84.8\n90.8\nBERTlarge\nMegatron Default\n1x\n1x\n1.731\n86.3/86.2\n90.9\nBERTlarge\nNaive Transfer\n22x\n220x\ntraining diverged\nBERTlarge\nµTransfer (Ours)\n22x\n220x\n1.683\n87.0/86.5\n91.4\n7.4\nGPT-3\nIn order to further verify µTransfer at scale, we applied it to GPT-3 6.7B [7] with relative attention.\nThis target model consists of 32 residual blocks with width 4096. We form the small proxy model by\nshrinking width to 256, resulting in roughly 40 million trainable parameters, 168 times smaller than\nthe target model. HPs were then determined by a random search on the proxy model. The total tuning\ncost was only 7% of total pretraining cost. Details of the HP sweep can be found in Appendix F.4.\nIn order to exclude code difference as a possible confounder, we also re-trained GPT-3 6.7B from\nscratch using the original HPs from [7]. Unfortunately, after we have ﬁnished all experiments, we\nfound this baseline mistakenly used absolute attention (like models in [7]) when it was supposed to\nuse relative attention like the target model. In addition, during training of the µTransfer model we\nencountered numerical issues that lead to frequent divergences. In order to avoid them, the model was\ntrained using FP32 precision, even though the original 6.7B model and our re-run were trained using\nFP16.16 17 The resulting µTransfer model outperforms the 6.7B from [7], and is in fact comparable\nto the twice-as-large 13B model across our evaluation suite (see Table 11). Selected evaluation results\ncan be found in Table 7 and further details are given in Table 10 and Appendix F.4.\n16While we are mainly focused on the efﬁcacy of µTransfer regardless of precision, it would be interesting to\nablate the effect of precision in our results, but we did not have enough resources to rerun the baseline in FP32\n17It is quite interesting that µTransfer identiﬁed a useful region of hyperparameters leading to much improved\nperformance, which probably would be difﬁcult to discover normally because 1) researchers usually change\nhyperparameters to accomodate precision and 2) there was no precise enough justiﬁcation to go against this\njudgment until µTransfer.\n10\n\nTable 7: GPT-3 6.7B Pretraining. Selected evaluation results for the GPT-3 6.7B model tuned\nwith µTransfer (transfered from a small proxy model of 40M parameters), compared to the results\npublished in [7] and a re-run with original HPs, as well as the 13B model in [7] for reference. Note\nthat the perplexities in this table are based on a custom tokenization and are not comparable to the\nliterature. The validation loss refers to the loss achieved on a random held-out part of our dataset.\nZero-shot, One-Shot and Few-Shot refer to the number of additional query and answer pairs passed in\nthe context when performing the sampling-based evaluations. See Appendix F.4 for full evaluation.\nTask\nMetric\n6.7B+µP\n6.7B re-run\n6.7B [7]\n13B [7]\nValidation loss\ncross-entropy\n1.98\n2.03\n-\n-\nPTB\nperplexity\n11.4\n13.0\n-\n-\nWikiText-103\nperplexity\n8.56\n9.13\n-\n-\nOne Billion Words\nperplexity\n20.5\n21.7\n-\n-\nLAMBADA Zero-Shot\naccuracy\n73.5\n70.8\n70.3\n72.5\nLAMBADA One-Shot\naccuracy\n69.9\n64.8\n65.4\n69.0\nLAMBADA Few-Shot\naccuracy\n74.7\n77.1\n79.1\n81.3\nHellaSwag Zero-Shot\naccuracy\n72.0\n66.7\n67.4\n70.9\nHellaSwag One-Shot\naccuracy\n71.1\n65.9\n66.5\n70.0\nHellaSwag Few-Shot\naccuracy\n72.4\n66.4\n67.3\n71.3\n0\n2000\n4000\n6000\n8000\n10000\nTraining Step\n1\n2\n3\n4\n5\n6\n7\n8\n9\nTraining Loss\nP LR=0.001\nWidth\n128\n256\n512\n1024\n2048\n4096\n0\n2000\n4000\n6000\n8000\n10000\nTraining Step\nSP LR=0.001\n0\n2000\n4000\n6000\n8000\n10000\nTraining Step\nSP LR=0.00025\nFigure 7: Wider is always better in training loss under µP, but not in SP, given the same HP.\nLearning curves for µP and SP with different learning rates, aggregated over 5 seeds. (Left) Wider\nµP models always achieve better training loss at any time in training. (Middle) If using a small\nlearning rate, SP models can appear to do so up to some large width, at which point the pattern fails\n(at width 2048 in our plot). (Right) If using a large learning rate, SP model can strictly do worse with\nwidth; here the SP model is identical to the µP model in (Left) at width 128.\n8\nWider is Better in µP Throughout Training\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\n1.2\n1.4\nTraining tokens\n1e9\n3.0\n3.5\n4.0\n4.5\n5.0\n5.5\n6.0\nValidation loss\nmodel width\n256\n512\n1024\n2048\n4096\n8192\n16384\n32768\nFigure 8: Stress-testing “wider-is-better” in µP.\nHere we trained a GPT-3 transformer with 4 layers\nand widths from 256 to 32,768. Modulo a brief\nperiod around 1e8 training tokens, wider is better\nthroughout training.\nIn earlier plots like Figs. 1 and 3, we saw that\nat the end of training, wider is always better\nin µP but not in SP. In fact, we ﬁnd this to\nbe true throughout training, as seen in Fig. 7,\nmodulo noise from random initialization and/or\ndata ordering, and assuming the output layer is\nzero-initialized (which has no impact on perfor-\nmance as discussed in Appendix D.2). We then\nstress-tested this on a µP GPT-3 Transformer\n(on the GPT-3 training data) by scaling width\nfrom 256 to 32,768 using a ﬁxed set of HPs\n(Fig. 8). Wider models consistently match or\noutperform narrower models at each point in\ntraining (except a brief period around 1e8 train-\ning tokens, likely due to noise because we ran\nonly 1 seed due to computational cost). Our ob-\nservation suggests that wider models are strictly\nmore data-efﬁcient if scaled appropriately. By\nchecking “wider-is-better” early in training, one can also cheaply debug a µP implementation.\n11\n\n9\nUseful Hyperparameter Transfer: A Theoretical Puzzle\nWe want to tune HPs on a small model with width N such that its HP landscape looks like that of\na large model with width ≫N. Our intuition in Section 2 and Appendices C and J leads us to µP.\nHowever, for this to be useful, we do not want the small model (as a function) after training to be\nclose to that of the large model — otherwise there is no point in training the large model to begin\nwith. So N 1) must be large enough so that the HP optimum converges, but 2) cannot be so large\nthat the functional dynamics (and the loss) converges. The fact that such N exists, as demonstrated\nby our experiments, shows that: In some sense, the HP optimum is a “macroscopic” or “coarse”\nvariable which converges quickly with width, while the neural network function (and its loss) is a very\n“microscopic” or “ﬁne” detail that converges much more slowly with width. However, theoretically,\nit is unclear why this should happen, and where else we should expect such useful HP transfer. We\nleave an explanation to future work.\n10\nRelated Works\n10.1\nHyperparameter Tuning\nMany have sought to speedup HP tuning beyond the simple grid or random search. Snoek et al.\n[45] treated HP tuning as an optimization process and used Bayesian optimization by treating the\nperformance of each HP combination as a sample from a Gaussian process (GP). Snoek et al. [46]\nfurther improved the runtime by swapping the GP with a neural network. Another thread of work\ninvestigated how massively parallel infrasture can be used for efﬁcient tuning under the multi-arm\nbandit problem [18, 22]. There are also dedicated tools such as Optuna [4] and Talos [3] which\nintegrate with existing deep learning frameworks and provide an easy way to apply more advanced\ntuning techniques.\nOur approach is distinct from all of the above in that it does not work on the HP optimization process\nitself. Instead, it decouples the size of the target model from the tuning cost, which was not feasible\nprior to this work. This means that no matter how large the target model is, we can always use a\nﬁxed-sized proxy model to probe its HP landscape. Nevertheless, our method is complementary,\nas the above approaches can naturally be applied to the tuning of the proxy model; it is only for\nscientiﬁc reasons that we use either grid search or random search throughout this work.\n10.2\nHyperparameter Transfer\nMany previous works explored transfer learning of HP tuning (e.g. [15, 36, 47, 62]). However, to the\nbest of our knowledge, our work is the ﬁrst to explore zero-shot HP transfer. In addition, we focus on\ntransferring across model scale rather than between different tasks or datasets. Some algorithms like\nHyperband [23] can leverage cheap estimates of HP evaluations (like using a small model to proxy a\nlarge model) but they are not zero-shot algorithms, so would still be very expensive to apply to large\nmodel training. Nevertheless, all of the above methods are complementary to ours as they can be\napplied to the tuning of our proxy model.\n10.3\nPreviously Proposed Scaling Rules of Hyperparameters\n(Learning Rate, Batch Size) Scaling\n[44] proposed to scale learning rate with batch size while\nﬁxing the total epochs of training; [14] proposed to scale learning rate as\n√\nbatchsize while ﬁxing\nthe total number of steps of training. However, [41] showed that there’s no consistent (learning\nrate, batch size) scaling law across a range of dataset and models. Later, [30] studied the trade-off\nof training steps vs computation as a result of changing batch size. They proposed an equation of\na/(1 + b/batchsize), where a and b are task- and model-speciﬁc constants, for the optimal learning\nrate (see their ﬁg 3 and ﬁg 5). This law suggests that for sufﬁciently large batch size, the optimal\nlearning rate is roughly constant.18 This supports our results here as well as the empirical results in\n[41, ﬁg 8].\nLearning Rate Scaling with Width\nAssuming that the optimal learning rate should scale with\nbatch size following [44], [34] empirically investigated how the optimal “noise ratio” LR/batchsize\nscales with width for MLP and CNNs in NTK parametrization (NTP) or standard parametrization\n18while the optimal learning is roughly linear in batch size when the latter is small\n12\n\n(SP) trained with SGD. They in particular focus on test loss in the regime of small batch size and\ntraining to convergence. In this regime, they claimed that in networks without batch normalization,\nthe optimal noise ratio is constant in SP but scales like 1/width for NTP. However, they found this\nlaw breaks down for networks with normalization.\nIn contrast, here we focus on training loss, without training to convergence and with a range of batch\nsizes from small to very large (as is typical in large scale pretraining). Additionally, our work applies\nuniversally to 1) networks with normalization, along with 2) Adam and other adaptive optimizers;\nfurthermore 3) we empirically validate transfer across depth and sequence length, and 4) explicitly\nvalidate tuning via µTransfer on large models like BERT-large and GPT-3.\nFinally, as argued in [57] and Appendix J.3, SP and NTP lead to bad inﬁnite-width limits in contrast\nto µP and hence are suboptimal for wide neural networks. For example, sufﬁciently wide neural\nnetworks in SP and NTP would lose the ability to learn features, as concretely demonstrated on\nword2vec in [57].\nInput Layer Parametrization\nThe original formulation of µP in [57] (see Table 9, which is\nequivalent to Table 3) uses a fan-out initialization for the input layer. This is atypical in vision\nmodels, but in language models where the input and output layers are shared (corresponding to word\nembeddings), it can actually be more natural to use a fan-out initialization (corresponding to fan-in\ninitialization of the output layer). In fact, we found that fairseq [33] by default actually implements\nboth the fan-out initialization and the √fan_out multiplier.\nOther Scaling Rules\nMany previous works proposed different initialization or parametrizations\nwith favorable properties, such as better stability for training deep neural networks [5, 13, 16, 26, 40,\n59, 60, 66]. Our work differs from these in that we focus on the transferability of optimal HPs from\nsmall models to large models in the same parametrization.\n10.4\nInﬁnite-Width Neural Networks: From Theory to Practice and Back\n[57] introduced µP as the unique parametrization that enables all layers of a neural network to learn\nfeatures in the inﬁnite-width limit, especially in contrast to the NTK parametrization [17] (which\ngives rise to the NTK limit) that does not learn features in the limit. Based on this theoretical insight,\nin Appendix J.3, we argue that µP should also be the unique parametrization (in the sense of [57]) that\nallows HP transfer across width; in short this is because it both 1) preserves feature learning, so that\nperformance on feature learning tasks (such as language model pretraining) does not become trivial\nin the limit, and 2) ensures each parameter tensor is not stuck at initialization in the large width limit,\nso that its learning rate does not become meaningless. At the same time, our results here suggest\nthat µP is indeed the correct parametrization for wide neural networks and thus provide empirical\nmotivation for the theoretical study of the inﬁnite-width µP limit. Note, parametrization here refers\nto a rule to scale hyperparameters with width (“how should my initialization and learning rate change\nwhen my width doubles?”), which is coarser than a prescription for setting hyperparameters at any\nparticular width (“how should I set my initialization and learning rate at width 1024?”).\n11\nConclusion\nLeveraging the discovery of a feature learning neural network inﬁnite-width limit, we hypothesized\nand veriﬁed that the HP landscape across NNs of different width is reasonably stable if parametrized\naccording to Maximal Update Parametrization (µP). We further empirically showed that it’s possible\nto transfer across depth, batch size, sequence length, and training time, with a few caveats. This\nallowed us to indirectly tune a very large network by tuning its smaller counterparts and transferring\nthe HPs to the full model. Our results raise an interesting new theoretical question of how useful HP\ntransfer is possible in neural networks in the ﬁrst place.\nVenues of Improvement\nNevertheless, our method has plenty of room to improve. For example,\ninitialization does not transfer well across depth, and depth transfer generally still does not work for\npost-layernorm Transformers. This begs the question whether a more principled parametrization in\ndepth could solve these problems. Additionally, Fig. 4 shows that the optimal HP still shifts slightly\nfor smaller models. Perhaps by considering ﬁnite-width corrections to µP one can ﬁx this shift.\nFinally, it will be interesting to study if there’s a way to transfer regularization HPs as a function of\nboth the model size and data size, especially in the context of ﬁnetuning of pretrained models.\n13\n\nAcknowledgements\nIn alphabetical order, we thank Arthur Jacot, Arturs Backurs, Colin Raffel,\nDenny Wu, Di He, Huishuai Zhang, Ilya Sutskever, James Martens, Janardhan Kulkarni, Jascha\nSohl-Dickstein, Jeremy Bernstein, Lenaic Chizat, Luke Metz, Mark Chen, Michael Santacroce,\nMuhammad ElNokrashy, Pengchuan Zhang, Sam Schoenholz, Sanjeev Arora, Taco Cohen, Yiping\nLu, Yisong Yue, and Yoshua Bengio for discussion and help during our research.\nReferences\n[1] NVIDIA/DeepLearningExamples, apache v2 license. URL https://github.com/NVIDIA/\nDeepLearningExamples.\n[2] Davidnet, mit license, 2019. URL https://github.com/davidcpage/cifar10-fast.\n[3] Autonomio talos, mit license, 2019. URL http://github.com/autonomio/talos.\n[4] Takuya Akiba, Shotaro Sano, Toshihiko Yanase, Takeru Ohta, and Masanori Koyama. Optuna:\nA next-generation hyperparameter optimization framework, 2019.\n[5] Thomas Bachlechner, Bodhisattwa Prasad Majumder, Huanru Henry Mao, Garrison W. Cot-\ntrell, and Julian McAuley.\nReZero is All You Need: Fast Convergence at Large Depth.\narXiv:2003.04887 [cs, stat], June 2020. URL http://arxiv.org/abs/2003.04887.\n[6] Jeremy Bernstein, Arash Vahdat, Yisong Yue, and Ming-Yu Liu. On the distance between two\nneural networks and the stability of learning. arXiv:2002.03432 [cs, math, stat], January 2021.\nURL http://arxiv.org/abs/2002.03432.\n[7] Tom B. Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared Kaplan, Prafulla Dhariwal,\nArvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, Sandhini Agarwal, Ariel\nHerbert-Voss, Gretchen Krueger, Tom Henighan, Rewon Child, Aditya Ramesh, Daniel M.\nZiegler, Jeffrey Wu, Clemens Winter, Christopher Hesse, Mark Chen, Eric Sigler, Mateusz\nLitwin, Scott Gray, Benjamin Chess, Jack Clark, Christopher Berner, Sam McCandlish, Alec\nRadford, Ilya Sutskever, and Dario Amodei. Language models are few-shot learners, 2020.\n[8] Simon Carbonnelle and Christophe De Vleeschouwer. Layer rotation: a surprisingly powerful\nindicator of generalization in deep networks? arXiv:1806.01603 [cs, stat], July 2019. URL\nhttp://arxiv.org/abs/1806.01603.\n[9] Ciprian Chelba, Tomas Mikolov, Mike Schuster, Qi Ge, Thorsten Brants, Phillipp Koehn, and\nTony Robinson. One billion word benchmark for measuring progress in statistical language\nmodeling, 2014.\n[10] Zihang Dai, Zhilin Yang, Yiming Yang, Jaime Carbonell, Quoc V. Le, and Ruslan Salakhutdinov.\nTransformer-xl: Attentive language models beyond a ﬁxed-length context, 2019.\n[11] Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. BERT: Pre-training of\nDeep Bidirectional Transformers for Language Understanding. arXiv:1810.04805 [cs], May\n2019. URL http://arxiv.org/abs/1810.04805.\n[12] Xiaohan Ding, Chunlong Xia, Xiangyu Zhang, Xiaojie Chu, Jungong Han, and Guiguang Ding.\nRepMLP: Re-parameterizing Convolutions into Fully-connected Layers for Image Recognition.\narXiv:2105.01883 [cs], August 2021. URL http://arxiv.org/abs/2105.01883.\n[13] Xavier Glorot and Yoshua Bengio. Understanding the difﬁculty of training deep feedforward\nneural networks. In Yee Whye Teh and Mike Titterington, editors, Proceedings of the Thirteenth\nInternational Conference on Artiﬁcial Intelligence and Statistics, volume 9 of Proceedings of\nMachine Learning Research, pages 249–256, Chia Laguna Resort, Sardinia, Italy, May 2010.\nPMLR. URL http://proceedings.mlr.press/v9/glorot10a.html.\n[14] Elad Hoffer, Itay Hubara, and Daniel Soudry. Train longer, generalize better: closing the\ngeneralization gap in large batch training of neural networks. arXiv:1705.08741 [cs, stat], May\n2017. URL http://arxiv.org/abs/1705.08741.\n[15] Samuel Horváth, Aaron Klein, Peter Richtárik, and Cédric Archambeau. Hyperparameter\ntransfer learning with adaptive complexity.\nCoRR, abs/2102.12810, 2021.\nURL https:\n//arxiv.org/abs/2102.12810.\n14\n\n[16] Xiao Shi Huang and Felipe Pérez. Improving Transformer Optimization Through Better\nInitialization. page 9.\n[17] Arthur Jacot, Franck Gabriel, and Clément Hongler. Neural Tangent Kernel: Convergence\nand Generalization in Neural Networks. arXiv:1806.07572 [cs, math, stat], June 2018. URL\nhttp://arxiv.org/abs/1806.07572.\n[18] Kevin Jamieson and Ameet Talwalkar. Non-stochastic best arm identiﬁcation and hyperparame-\nter optimization, 2015.\n[19] Jared Kaplan, Sam McCandlish, Tom Henighan, Tom B. Brown, Benjamin Chess, Rewon Child,\nScott Gray, Alec Radford, Jeffrey Wu, and Dario Amodei. Scaling Laws for Neural Language\nModels. arXiv:2001.08361 [cs, stat], January 2020. URL http://arxiv.org/abs/2001.\n08361.\n[20] Diederik Kingma and Jimmy Ba. Adam: A method for stochastic optimization. arXiv preprint\narXiv:1412.6980, 2014.\n[21] Jaehoon Lee, Yasaman Bahri, Roman Novak, Sam Schoenholz, Jeffrey Pennington, and Jascha\nSohl-dickstein. Deep Neural Networks as Gaussian Processes. In International Conference on\nLearning Representations, 2018. URL https://openreview.net/forum?id=B1EA-M-0Z.\n[22] Liam Li, Kevin Jamieson, Afshin Rostamizadeh, Ekaterina Gonina, Moritz Hardt, Benjamin\nRecht, and Ameet Talwalkar. A system for massively parallel hyperparameter tuning, 2020.\n[23] Lisha Li, Kevin Jamieson, Giulia DeSalvo, Afshin Rostamizadeh, and Ameet Talwalkar. Hyper-\nband: A Novel Bandit-Based Approach to Hyperparameter Optimization. JMLR 18, page 52.\n[24] Hanxiao Liu, Zihang Dai, David R. So, and Quoc V. Le.\nPay Attention to MLPs.\narXiv:2105.08050 [cs], June 2021. URL http://arxiv.org/abs/2105.08050.\n[25] Liyuan Liu, Xiaodong Liu, Jianfeng Gao, Weizhu Chen, and Jiawei Han. Understanding\nthe difﬁculty of training transformers. In Proceedings of the 2020 Conference on Empirical\nMethods in Natural Language Processing (EMNLP), pages 5747–5763, Online, November\n2020. Association for Computational Linguistics. doi: 10.18653/v1/2020.emnlp-main.463.\nURL https://www.aclweb.org/anthology/2020.emnlp-main.463.\n[26] Liyuan Liu, Xiaodong Liu, Jianfeng Gao, Weizhu Chen, and Jiawei Han. Understanding the\nDifﬁculty of Training Transformers. arXiv:2004.08249 [cs, stat], September 2020. URL\nhttp://arxiv.org/abs/2004.08249.\n[27] Xiaodong Liu, Pengcheng He, Weizhu Chen, and Jianfeng Gao.\nMulti-task deep neural\nnetworks for natural language understanding. In Proceedings of the 57th Annual Meeting of\nthe Association for Computational Linguistics, pages 4487–4496, Florence, Italy, July 2019.\nAssociation for Computational Linguistics. URL https://www.aclweb.org/anthology/\nP19-1441.\n[28] Yang Liu, Jeremy Bernstein, Markus Meister, and Yisong Yue. Learning by Turning: Neural\nArchitecture Aware Optimisation. arXiv:2102.07227 [cs], September 2021. URL http:\n//arxiv.org/abs/2102.07227.\n[29] Alexander G. de G. Matthews, Mark Rowland, Jiri Hron, Richard E. Turner, and Zoubin\nGhahramani. Gaussian Process Behaviour in Wide Deep Neural Networks. arXiv:1804.11271\n[cs, stat], April 2018. URL http://arxiv.org/abs/1804.11271. arXiv: 1804.11271.\n[30] Sam McCandlish, Jared Kaplan, Dario Amodei, and OpenAI Dota Team. An Empirical\nModel of Large-Batch Training. arXiv:1812.06162 [cs, stat], December 2018. URL http:\n//arxiv.org/abs/1812.06162.\n[31] Luke Melas-Kyriazi. Do You Even Need Attention? A Stack of Feed-Forward Layers Does\nSurprisingly Well on ImageNet. arXiv:2105.02723 [cs], May 2021. URL http://arxiv.\norg/abs/2105.02723.\n[32] Stephen Merity, Caiming Xiong, James Bradbury, and Richard Socher. Pointer sentinel mixture\nmodels, 2016.\n15\n\n[33] Myle Ott, Sergey Edunov, Alexei Baevski, Angela Fan, Sam Gross, Nathan Ng, David Grangier,\nand Michael Auli. fairseq: A fast, extensible toolkit for sequence modeling, mit license. In\nProceedings of NAACL-HLT 2019: Demonstrations, 2019.\n[34] Daniel S. Park, Jascha Sohl-Dickstein, Quoc V. Le, and Samuel L. Smith. The Effect of Network\nWidth on Stochastic Gradient Descent and Generalization: an Empirical Study. May 2019.\nURL https://arxiv.org/abs/1905.03776v1.\n[35] Adam Paszke, Sam Gross, Francisco Massa, Adam Lerer, James Bradbury, Gregory\nChanan, Trevor Killeen, Zeming Lin, Natalia Gimelshein, Luca Antiga, Alban Desmai-\nson, Andreas Kopf, Edward Yang, Zachary DeVito, Martin Raison, Alykhan Tejani,\nSasank Chilamkurthy, Benoit Steiner, Lu Fang, Junjie Bai, and Soumith Chintala.\nPytorch:\nAn imperative style, high-performance deep learning library, bsd-style li-\ncense.\nIn H. Wallach, H. Larochelle, A. Beygelzimer, F. dÁlché Buc, E. Fox, and\nR. Garnett, editors, Advances in Neural Information Processing Systems 32, pages\n8024–8035. Curran Associates, Inc., 2019.\nURL http://papers.neurips.cc/paper/\n9015-pytorch-an-imperative-style-high-performance-deep-learning-library.\npdf.\n[36] Valerio Perrone, Rodolphe Jenatton, Matthias W Seeger, and Cedric Archambeau. Scalable\nHyperparameter Transfer Learning. NeurIPS 2018, page 11.\n[37] Martin Popel and Ondˇrej Bojar. Training Tips for the Transformer Model. The Prague Bulletin\nof Mathematical Linguistics, 110(1):43–70, April 2018. ISSN 1804-0462. doi: 10.2478/\npralin-2018-0002. URL http://content.sciendo.com/view/journals/pralin/110/\n1/article-p43.xml.\n[38] Colin Raffel, Noam Shazeer, Adam Roberts, Katherine Lee, Sharan Narang, Michael Matena,\nYanqi Zhou, Wei Li, and Peter J. Liu. Exploring the Limits of Transfer Learning with a Uniﬁed\nText-to-Text Transformer. arXiv:1910.10683 [cs, stat], July 2020. URL http://arxiv.org/\nabs/1910.10683.\n[39] Jonathan S. Rosenfeld, Amir Rosenfeld, Yonatan Belinkov, and Nir Shavit. A Constructive\nPrediction of the Generalization Error Across Scales. arXiv:1909.12673 [cs, stat], December\n2019. URL http://arxiv.org/abs/1909.12673.\n[40] Samuel S. Schoenholz, Justin Gilmer, Surya Ganguli, and Jascha Sohl-Dickstein. Deep Infor-\nmation Propagation. arXiv:1611.01232 [cs, stat], November 2016. URL http://arxiv.org/\nabs/1611.01232.\n[41] Christopher J. Shallue, Jaehoon Lee, Joseph Antognini, Jascha Sohl-Dickstein, Roy Frostig,\nand George E. Dahl. Measuring the Effects of Data Parallelism on Neural Network Training.\narXiv:1811.03600 [cs, stat], November 2018. URL http://arxiv.org/abs/1811.03600.\n[42] Noam Shazeer and Mitchell Stern. Adafactor: Adaptive Learning Rates with Sublinear Memory\nCost. April 2018. URL https://arxiv.org/abs/1804.04235v1.\n[43] Mohammad Shoeybi, Mostofa Patwary, Raul Puri, Patrick LeGresley, Jared Casper, and Bryan\nCatanzaro. Megatron-lm: Training multi-billion parameter language models using model\nparallelism. CoRR, abs/1909.08053, 2019. URL http://arxiv.org/abs/1909.08053.\n[44] Samuel L. Smith, Pieter-Jan Kindermans, and Quoc V. Le. Don’t Decay the Learning Rate,\nIncrease the Batch Size. arXiv:1711.00489 [cs, stat], November 2017. URL http://arxiv.\norg/abs/1711.00489.\n[45] Jasper Snoek, Hugo Larochelle, and Ryan P. Adams. Practical bayesian optimization of machine\nlearning algorithms, 2012.\n[46] Jasper Snoek, Oren Rippel, Kevin Swersky, Ryan Kiros, Nadathur Satish, Narayanan Sundaram,\nMd. Mostofa Ali Patwary, Prabhat, and Ryan P. Adams. Scalable bayesian optimization using\ndeep neural networks, 2015.\n[47] Danny Stoll, Jörg K.H. Franke, Diane Wagner, Simon Selg, and Frank Hutter. Hyperparameter\ntransfer across developer adjustments, 2021. URL https://openreview.net/forum?id=\nWPO0vDYLXem.\n16\n\n[48] Ilya Tolstikhin, Neil Houlsby, Alexander Kolesnikov, Lucas Beyer, Xiaohua Zhai, Thomas\nUnterthiner, Jessica Yung, Andreas Steiner, Daniel Keysers, Jakob Uszkoreit, Mario Lucic, and\nAlexey Dosovitskiy. MLP-Mixer: An all-MLP Architecture for Vision. arXiv:2105.01601 [cs],\nJune 2021. URL http://arxiv.org/abs/2105.01601.\n[49] Hugo Touvron, Piotr Bojanowski, Mathilde Caron, Matthieu Cord, Alaaeldin El-Nouby,\nEdouard Grave, Gautier Izacard, Armand Joulin, Gabriel Synnaeve, Jakob Verbeek, and Hervé\nJégou. ResMLP: Feedforward networks for image classiﬁcation with data-efﬁcient training.\narXiv:2105.03404 [cs], June 2021. URL http://arxiv.org/abs/2105.03404.\n[50] Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N. Gomez,\nLukasz Kaiser, and Illia Polosukhin. Attention is all you need. CoRR, abs/1706.03762, 2017.\nURL http://arxiv.org/abs/1706.03762.\n[51] Alex Wang, Amanpreet Singh, Julian Michael, Felix Hill, Omer Levy, and Samuel R Bowman.\nGlue: A multi-task benchmark and analysis platform for natural language understanding.\nEMNLP 2018, page 353, 2018.\n[52] Adina Williams, Nikita Nangia, and Samuel Bowman. A broad-coverage challenge corpus\nfor sentence understanding through inference. In Proceedings of the 2018 Conference of the\nNorth American Chapter of the Association for Computational Linguistics: Human Language\nTechnologies, Volume 1 (Long Papers), pages 1112–1122. Association for Computational\nLinguistics, 2018. URL http://aclweb.org/anthology/N18-1101.\n[53] Greg Yang. Tensor Programs I: Wide Feedforward or Recurrent Neural Networks of Any Archi-\ntecture are Gaussian Processes. arXiv:1910.12478 [cond-mat, physics:math-ph], December\n2019. URL http://arxiv.org/abs/1910.12478.\n[54] Greg Yang. Scaling Limits of Wide Neural Networks with Weight Sharing: Gaussian Process\nBehavior, Gradient Independence, and Neural Tangent Kernel Derivation. arXiv:1902.04760\n[cond-mat, physics:math-ph, stat], February 2019. URL http://arxiv.org/abs/1902.\n04760.\n[55] Greg Yang. Tensor Programs II: Neural Tangent Kernel for Any Architecture. arXiv:2006.14548\n[cond-mat, stat], August 2020. URL http://arxiv.org/abs/2006.14548.\n[56] Greg Yang. Tensor Programs III: Neural Matrix Laws. arXiv:2009.10685 [cs, math], September\n2020. URL http://arxiv.org/abs/2009.10685.\n[57] Greg Yang and Edward J. Hu. Feature learning in inﬁnite-width neural networks. arXiv, 2020.\n[58] Greg Yang and Etai Littwin.\nTensor Programs IIb: Architectural Universality of Neural\nTangent Kernel Training Dynamics. arXiv:2105.03703 [cs, math], May 2021. URL http:\n//arxiv.org/abs/2105.03703.\n[59] Greg Yang and Sam S. Schoenholz. Deep Mean Field Theory: Layerwise Variance and\nWidth Variation as Methods to Control Gradient Explosion. February 2018. URL https:\n//openreview.net/forum?id=rJGY8GbR-.\n[60] Greg Yang and Samuel S. Schoenholz. Mean Field Residual Networks: On the Edge of Chaos.\narXiv:1712.08969 [cond-mat, physics:nlin], December 2017. URL http://arxiv.org/abs/\n1712.08969.\n[61] Greg Yang, Michael Santacroce, and Edward J Hu. Efﬁcient computation of deep nonlinear\ninﬁnite-width neural networks that learn features. In International Conference on Learning\nRepresentations, 2022. URL https://openreview.net/forum?id=tUMr0Iox8XW.\n[62] Dani Yogatama and Gideon Mann. Efﬁcient Transfer Learning Method for Automatic Hyperpa-\nrameter Tuning. In Artiﬁcial Intelligence and Statistics, pages 1077–1085. PMLR, April 2014.\nURL http://proceedings.mlr.press/v33/yogatama14.html.\n[63] Yang You, Igor Gitman, and Boris Ginsburg. Large Batch Training of Convolutional Networks.\narXiv:1708.03888 [cs], September 2017. URL http://arxiv.org/abs/1708.03888.\n17\n\n[64] Yang You, Jing Li, Sashank Reddi, Jonathan Hseu, Sanjiv Kumar, Srinadh Bhojanapalli,\nXiaodan Song, James Demmel, Kurt Keutzer, and Cho-Jui Hsieh. Large Batch Optimization\nfor Deep Learning: Training BERT in 76 minutes. arXiv:1904.00962 [cs, stat], January 2020.\nURL http://arxiv.org/abs/1904.00962.\n[65] Sergey Zagoruyko and Nikos Komodakis. Wide residual networks, 2017.\n[66] Hongyi Zhang, Yann N. Dauphin, and Tengyu Ma. Residual Learning Without Normalization\nvia Better Initialization. In International Conference on Learning Representations, 2019. URL\nhttps://openreview.net/forum?id=H1gsz30cKX.\n18\n\nContents\n1\nIntroduction\n1\n2\nParametrization Matters: A Primer\n3\n3\nHyperparameters Don’t Transfer Conventionally\n4\n4\nUnlocking Zero-Shot Hyperparameter Transfer with µP\n5\n5\nThe Defects of SP and How µP Fixes Them\n6\n6\nWhich Hyperparameters Can Be µTransferred?\n7\n6.1\nEmpirical Validation and Limitations . . . . . . . . . . . . . . . . . . . . . . . . .\n8\n7\nEfﬁciency and Performance of µTransfer\n8\n7.1\nTransformer on IWSLT14 De-En . . . . . . . . . . . . . . . . . . . . . . . . . . .\n8\n7.2\nTransformer on WMT14 En-De\n. . . . . . . . . . . . . . . . . . . . . . . . . . .\n9\n7.3\nBERT . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n9\n7.4\nGPT-3 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n10\n8\nWider is Better in µP Throughout Training\n11\n9\nUseful Hyperparameter Transfer: A Theoretical Puzzle\n12\n10 Related Works\n12\n10.1 Hyperparameter Tuning . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n12\n10.2 Hyperparameter Transfer . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n12\n10.3 Previously Proposed Scaling Rules of Hyperparameters . . . . . . . . . . . . . . .\n12\n10.4 Inﬁnite-Width Neural Networks: From Theory to Practice and Back . . . . . . . .\n13\n11 Conclusion\n13\nA Parametrization Terminologies\n22\nB\nFurther Explanations of the µP Tables\n22\nB.1\nWalkthrough of µP Implementation in a Transformer . . . . . . . . . . . . . . . .\n24\nB.2\nOther Parameters . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n25\nB.3\nOptimizer Variants and Hyperparameters . . . . . . . . . . . . . . . . . . . . . . .\n25\nC Parametrization Matters: A Primer for Multiple Hyperparameters\n26\nD Practical Considerations\n26\nD.1\nVerifying µP Implementation via Coordinate Checking . . . . . . . . . . . . . . .\n27\nD.2\nZero Initialization for Output Layers and Query Layers in Attention\n. . . . . . . .\n27\nD.3\nActivation Functions\n. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n27\n19\n\nD.4\nEnlarge dk . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n28\nD.5\nNon-Gaussian vs Gaussian Initialization . . . . . . . . . . . . . . . . . . . . . . .\n28\nD.6\nUsing a Larger Sequence Length . . . . . . . . . . . . . . . . . . . . . . . . . . .\n28\nD.7\nTuning Per-Layer Hyperparameters\n. . . . . . . . . . . . . . . . . . . . . . . . .\n28\nE\nWhich Hyperparameters Can Be Transferred? (Continued)\n29\nE.1\nFurther Discussions on Hyperparameter Categories . . . . . . . . . . . . . . . . .\n29\nE.2\nOn the Deﬁnitions of Width . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n29\nF\nExperimental Details\n31\nF.1\nIWSLT\n. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n31\nF.2\nWMT . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n31\nF.3\nBERT . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n31\nF.4\nGPT-3 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n32\nG Additional Experiments\n34\nG.1\nExperiments on ResNets\n. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n34\nG.1.1\nResNet on CIFAR-10 . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n34\nG.1.2\nWide ResNet on ImageNet . . . . . . . . . . . . . . . . . . . . . . . . . .\n37\nG.2\nExperiments on Transformers . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n37\nG.2.1\nVerifying Transfer across Batch Size, Sequence Length, and Training Time\non Wikitext-2 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n37\nG.2.2\nPost-Layernorm Transformers . . . . . . . . . . . . . . . . . . . . . . . .\n37\nG.2.3\nHyperparameter Instability of SP Transformers . . . . . . . . . . . . . . .\n38\nH Implementing µTransfer in a Jiffy\n38\nI\nReverse-µTransfer for Diagnosing Training Instability in Large Models\n41\nJ\nAn Intuitive Introduction to the Theory of Maximal Update Parametrization\n42\nJ.1\nBehaviors of Gaussian Matrices vs Tensor Product Matrices\n. . . . . . . . . . . .\n43\nJ.1.1\nPreparation for the Derivations . . . . . . . . . . . . . . . . . . . . . . . .\n43\nJ.1.2\nLinear Tensor Product Matrix (e.g. SGD Updates)\n. . . . . . . . . . . . .\n44\nJ.1.3\nNonlinear Tensor Product Matrix (e.g. Adam Updates) . . . . . . . . . . .\n44\nJ.1.4\nVector Case (e.g. Readout Layer)\n. . . . . . . . . . . . . . . . . . . . . .\n45\nJ.1.5\nGaussian Matrix (e.g. Hidden Weights Initialization) . . . . . . . . . . . .\n45\nJ.2\nDeriving µP for Any Architecture\n. . . . . . . . . . . . . . . . . . . . . . . . . .\n45\nJ.2.1\nµP Derivation From the Desiderata\n. . . . . . . . . . . . . . . . . . . . .\n46\nJ.3\nWhy Other Parametrizations Cannot Admit Hyperparameter Transfer\n. . . . . . .\n47\nList of Figures\n1\nTraining loss against learning rate on Transformers of varying dmodel trained with\nAdam . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n1\n20\n\n2\nIllustration of µTransfer . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n2\n3\nSP vs µP for MLPs on CIFAR10 . . . . . . . . . . . . . . . . . . . . . . . . . . .\n4\n4\nEmpirical validation of the stability of four representative hyperparameters on pre-LN\nTransformers in µP . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n6\n5\nActivations blow up in SP but maintain a consistent scale in µP . . . . . . . . . . .\n7\n6\nEfﬁciency-performance Pareto frontier of µTransfer\n. . . . . . . . . . . . . . . .\n9\n7\nWider is always better in training loss under µP, but not in SP, given the same HP .\n11\n8\nStress-testing “wider-is-better” in µP . . . . . . . . . . . . . . . . . . . . . . . . .\n11\n9\nSquashing activation functions reduce transfer quality.\n. . . . . . . . . . . . . . .\n27\n10\nEnlarging dk makes µTransfer more precise in Transformers . . . . . . . . . . . .\n28\n11\nSchematics of each Transformer layer . . . . . . . . . . . . . . . . . . . . . . . .\n30\n12\nWidth ratio can be varied arbitrarily in µTransfer\n. . . . . . . . . . . . . . . . . .\n30\n13\nµTransfer can handle increasing nhead while ﬁxing dhead as well as increasing dhead\nwhile ﬁxing nhead, or a mix of both\n. . . . . . . . . . . . . . . . . . . . . . . . .\n31\n14\nResults of the random search over reduced-width GPT-3 proxy models . . . . . . .\n33\n15\nThe training curves of the GPT-3 6.7B model with µTransfer and a re-run with the\noriginal settings from [7] . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n34\n16\nVerifying µP hyperparameter stability on ResNet\n. . . . . . . . . . . . . . . . . .\n36\n17\nVerifying hyperparameter stability under µP for Post-LN Transformers . . . . . . .\n38\n18\nµTransfer vs naive transfer for post-layernorm Transformers on Wikitext-2 . . . . .\n39\n19\nEmpirical validation of µTransfer across Batch Size, Sequence Length, and Training\nTime on pre-LN Transformers\n. . . . . . . . . . . . . . . . . . . . . . . . . . . .\n39\n20\nLearning rate landscape is highly unstable under standard parametrization in IWSLT\n40\n21\nReplicating training instability issue on a small Transformer by reverse-µtransferring\nhyperparameters . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n42\nList of Tables\n1\nHyperparameters That Can Be µTransferred, Not µTransferred, or µTransferred Across\n2\n2\nExamples of µTransferable Hyperparameters\n. . . . . . . . . . . . . . . . . . . .\n3\n3\nµP[57] and SP for General Neural Networks . . . . . . . . . . . . . . . . . . . . .\n5\n4\nµTransfer results for Transformer on IWSLT14 De-En\n. . . . . . . . . . . . . . .\n9\n5\nµTransfer results for Transformer on WMT14 En-De . . . . . . . . . . . . . . . .\n10\n6\nµTransfer results for BERT pretraining . . . . . . . . . . . . . . . . . . . . . . . .\n10\n7\nµTransfer results for GPT-3 pretraining\n. . . . . . . . . . . . . . . . . . . . . . .\n11\n8\nAlternative (Equivalent) µP Formulation for Easier Implementation\n. . . . . . . .\n23\n9\nµP Formulation in the Style of [57] . . . . . . . . . . . . . . . . . . . . . . . . . .\n23\n10\nFull evaluation results of our GPT-3 6.7B models . . . . . . . . . . . . . . . . . .\n35\n11\nOur µTransferred GPT-3 6.7B model performs comparably to the twice-as-large\nGPT-3 13B model from [7] . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n36\n12\nµTransfer results for ResNet on CIFAR10 . . . . . . . . . . . . . . . . . . . . . .\n37\n13\nµTransfer results for Wide ResNet on ImageNet . . . . . . . . . . . . . . . . . . .\n37\n14\nExpected output size of matrix multiplication between different types of random\nmatrices and a random vector, as preparation for deriving µP . . . . . . . . . . . .\n43\n21\n\nA\nParametrization Terminologies\nThis section seeks to make formal and clarify some of the notions regarding parametrization discussed\ninformally in the main text.\nDeﬁnition A.1 (Multiplier and Parameter Multiplier). In a neural network, one may insert a “multiply\nby c” operation anywhere, where c is a non-learnable scalar hyperparameter. If c = 1, then this\noperation is a no-op. This c is called a multiplier.\nRelatedly, for any parameter tensor W in a neural network, we may replace W with cW for some\nnon-learnable scalar hyperparameter c. When c = 1, we recover the original formulation. This c is\nreferred to as a parameter multiplier.\nFor example, in the attention logit calculation ⟨k, q⟩/√dhead where q = Wx, the 1/√dhead factor is\na multiplier. It may also be thought of as the parameter multiplier of W if we rewrite the attention\nlogit as ⟨k, (W/√dhead)x⟩.\nNote parameter multipliers cannot be absorbed into the initialization in general, since they affect\nbackpropagation. Nevertheless, after training is done, parameter multipliers can always be absorbed\ninto the weight.\nDeﬁnition A.2 (Parametrization). In this work, a parametrization is a rule for how to change\nhyperparameters when the widths of a neural network change, but note that it does not necessarily\nprescribes how to set the hyperparameters for any speciﬁc width. In particular, for any neural network,\nan abc-parametrization is a rule for how to scale a) the parameter multiplier, b) the initialization, and\nc) the learning rate individually for each parameter tensor as the widths of the network change, as\nwell as any other multiplier in the network; all other hyperparameters are kept ﬁxed with width.\nFor example, SP and µP are both abc-parametrizations. Again, we note that, in this sense, a\nparametrization does not prescribe, for example, that the initialization variance be 1/fan_in, but\nrather that it be halved when fan_in doubles.\nDeﬁnition A.3 (Zero-Shot Hyperparameter Transfer). In this work, we say a parametrization admits\nzero-shot transfer of a set of hyperparameters H w.r.t. a metric L if the optimal combination of values\nof H w.r.t. L converges as width goes to inﬁnity, i.e. it stays approximately optimal w.r.t. L under\nthis parametrization as width increases.\nThroughout this paper, we take L to be the training loss, but because regularization is not the bottleneck\nin our experiments (especially large scale pretraining with BERT and GPT-3), we nevertheless see\nhigh quality test performance in all of our results. We also remark that empirically, using training loss\nas the metric can be more robust to random seed compared to validation loss and especially BLEU\nscore. See Table 1(left) for H. By our arguments in Appendix J.3 and our empirical results, µP is the\nunique abc-parametrization admitting zero-shot transfer for such H and L in this sense.\nMore generally, one may deﬁne a K-shot transfer algorithm of a set of hyperparameters H w.r.t. a\nmetric L as one that 1) takes width values n and n′ and an approximately optimal combination of\nvalues of H w.r.t. L at a width n and 2) returns an approximately optimal combination of values of H\nw.r.t. L at width n′, given 3) a budget of K evaluations of candidate hyperparameter combinations on\nmodels of width n′. However, we will have no use for this deﬁnition in this paper.\nB\nFurther Explanations of the µP Tables\nIn addition to Table 3, we provide Table 8 as an equivalent µP formulation that is easier to implement,\nas well as Table 9 for those more familiar with the original µP formulation in [57]. Below, we provide\nsome commentary on corner cases not well speciﬁed by the tables. Ultimately, by understanding\nAppendix J, one can derive µP for any architecture, new or old.\nMatrix-Like, Vector-Like, Scalar-Like Parameters\nWe can classify any dimension in a neural\nnetwork as “inﬁnite” if it scales with width, or “ﬁnite” otherwise. For example, in a Transformer,\ndmodel, dffn, dhead, nhead are all inﬁnite, but vocab size and context size are ﬁnite. Then we can\ncategorize parameter tensors by how many inﬁnite dimensions they have. If there are two such\ndimensions, then we say the parameter is matrix-like; if there is only one, then we say it is vector-like;\nif there is none, we say it is scalar-like. Then in Tables 3, 8 and 9, “input weights & all biases” and\n“output weights” are all vector-like parameters, while hidden weights are matrix-like parameters. An\n22\n\nTable 8: Alternative (Equivalent) µP Formulation for Easier Implementation. Same format as\nin Table 3. In contrast to the formulation in Table 3, here all “vector-like” parameters (i.e. those\nthat have only one dimension tending to inﬁnity), including input and output weights and biases,\nhave the same width scaling for initialization variance and SGD/Adam LR (note the 1/fan_in for\ninput weight/bias init. var. is Θ(1) in width). This has two beneﬁts in practice: 1) implementation\nis uniﬁed and simpliﬁed for all “vector-like” parameters; 2) input and output weights can now be\ntied, in contrast to Table 3, which is a common design feature of Transformer models. Note that in\nthis table, for biases, the fan_in is 1 (compare to PyTorch nn.Linear default initialization of biases,\nwhere fan_in refers to fan_in of the layer.) This table can be derived from Table 3 via Lemma J.1.\nSee Appendix B for further explanations.\nInput weights & all biases\nOutput weights\nHidden weights\nInit. Var.\n1/fan_in\n1\n(1/fan_in)\n1/fan_in\nMultiplier\n1\n1/fan_in\n(1)\n1\nSGD LR\nfan_out\n(1)\nfan_in\n(1)\n1\nAdam LR\n1\n1\n1/fan_in\n(1)\nTable 9: µP Formulation in the Style of [57]. This table can be derived from Table 3 via Lemma J.1.\nInput weights & all biases\nOutput weights\nHidden weights\nInit. Var.\n1/fan_out\n(1/fan_in)\n1/fan_in\n1/fan_in\nMultiplier\n√fan_out\n(1)\n1/√fan_in\n(1)\n1\nSGD LR\n1\n1\n1\nAdam LR\n1/√fan_out\n(1)\n1/√fan_in\n(1)\n1/fan_in\n(1)\nadvantage of Table 8 is that it gives a uniform scaling rule of initialization and learning rate for all\nvector-like parameters. The multiplier rule in Table 8 can be more interpreted more generally as\nthe following: a multiplier of order 1/fan_in should accompany any weight that maps an inﬁnite\ndimension to a ﬁnite one. This interpretation then nicely covers both the output logits and the attention\nlogits (i.e. 1/d attention).\nScalar-like parameters are not as common as matrix-like and vector-like ones, but we will mention a\nfew examples in Appendix B.2. The scaling rule for their initialization, learning rate (for both SGD\nand Adam), and multiplier is very simple: hold them constant with width.\nInitialization Mean\nWe did not specify the initialization mean in the tables, since most commonly\nthe mean is just set to 0, but it can be nonzero for vector-like parameters (e.g., layernorm weights)\nand scalar-like parameters but must be 0 for matrix-like parameters.\nZero Initialization Variance\nThe initialization scaling rules in our tables can all be trivially satis-\nﬁed if the initialization variance is set to 0. This can be useful in some settings (e.g., Appendix D.2)\nbut detrimental in other settings (e.g., hidden weights).\nWhat Are Considered Input Weights? Output Weights?\nHere, input weights very speciﬁcally\nrefer to weights that map from an inﬁnite dimension to a ﬁnite dimension. As a counterexample, in\nsome architectures, the ﬁrst layer can actually map from a ﬁnite dimension to another ﬁnite dimension,\ne.g., a PCA layer. Then this is not an “input weight”; if the next layer maps into an inﬁnite dimension,\nthen that’s the input weight. A similar, symmetric discussion applies to output weights.\nWhat Counts As a “Model”? Does the MLP in a Transformer Count As a “Model”?\nFor our\ntables, a model is speciﬁcally a function that maps a ﬁnite dimension to another ﬁnite dimension,\nconsistent with the discussion above. For example, for an image model on CIFAR10, it maps from\n3×32×32 = 3072 dimensions to 10 dimensions, and these numbers are ﬁxed regardless of the width\nof the model. Likewise, for an autoregressive Transformer model, the input and output dimension are\nboth the vocab size, which is independent of the width. In contrast, an MLP inside a Transformer is\nnot a “model” in this sense because its input and output dimension are both equal to the width of the\nTransformer.\n23\n\nB.1\nWalkthrough of µP Implementation in a Transformer\nTo ground the abstract description in Tables 3, 8 and 9, we walk through the parameters of a typical\nTransformer and discuss concretely how to parametrize each.\nWe assume that the user wants to replicate SP when the model widths are equal to some base widths,\nfor example, when dmodel = dmodel,0 = 128, dffn = dffn,0 = 512, etc, as in the MLP example in\nSection 4. For this purpose, it’s useful to deﬁne ˜dmodel = dmodel/dmodel,0, ˜dffn = dffn/dffn,0,\nand so on. One can always take dmodel,0 = dffn,0 = · · · = 1 for a “pure” µP.\nBelow, we introduce hyperparameters σ•, η• for each parameter tensor, as well as a few multipliers\nα•. One may always tie σ• (resp. η•) across all parameter tensors, but in our experiments, we found\nit beneﬁcial to at least distinguish the input and output layer initialization and learning rates.\nInput Word Embeddings\nThe input word embedding matrix W wordemb has size dmodel ×\nvocabsize, where vocabsize is the fan-in and dmodel is the fan-out. Follow the “input weight\n& all biases” column in Tables 3, 8 and 9. For example, for Tables 3 and 8,\nW wordemb ∼N(0, σ2\nwordemb),\nwith Adam LR ηwordemb\nNote here, because fan-in (vocabsize) here is independent of width (dmodel), the “1/fan_in” for\nthe initialization variance in these tables is equivalent to “1”, i.e. the initialization variance can be\nanything ﬁxed with width. In this case of the word embedding, setting the variance to 1, for example,\nis more natural than setting the variance to 1/fan_in, because the embedding is one-hot (1/fan_in\nwould be more natural for image inputs).\nPositional Embeddings\nThe (absolute or relative) positional embedding matrix W posemb has size\ndmodel × contextsize, where contextsize is the fan-in and dmodel is the fan-out. With the same\ndiscussion as above for input word embeddings, follow the “input weight & all biases” column in\nTables 3, 8 and 9. For example, for Tables 3 and 8,\nW posemb ∼N(0, σ2\nposemb),\nwith Adam LR ηposemb\nLayernorm Weights and Biases\nLayernorm weights wLN and biases bLN both have shape dmodel\nand can be thought of “input weights” to the scalar input of 1. Hence one should follow the “input\nweight & all biases” column in Tables 3, 8 and 9. In particular, the usual initialization of layernorm\nweights as all 1s and biases as all 0s sufﬁce (where the initialization variance is 0). For example, for\nTables 3 and 8,\nwLN ←1,\nwith Adam LR ηLNw,\nand\nbLN ←0,\nwith Adam LR ηLNb\nSelf-Attention\nThere are 4 matrices, W q, W k ∈R(dknhead)×dmodel, W v ∈R(dvnhead)×dmodel,\nand W o ∈Rdmodel×(dvnhead) (where the shapes are Rfan_out×fan_in). Since dmodel, (dknhead), and\n(dvnhead) all scale with width (where the latter two are commonly just set to dmodel), all 4 matrices\nshould be parametrized according to the “hidden weights” column in Tables 3, 8 and 9. For example,\nfor Tables 3 and 8,\nW q ∼N(0, σ2\nq/dmodel),\nwith Adam LR ηq/ ˜dmodel\nW k ∼N(0, σ2\nk/dmodel),\nwith Adam LR ηk/ ˜dmodel\nW v ∼N(0, σ2\nv/dmodel),\nwith Adam LR ηv/ ˜dmodel\nW o ∼N(0, σ2\no/(dvnhead)),\nwith Adam LR ηo/( ˜dv˜nhead).\nAttention Logit Scaling\nWe use 1/d instead of 1/\n√\nd attention. To be compatible with 1/\n√\nd\nattention when at a particular base dhead = dhead,0, we set\nAttnLogit = αattn\np\ndhead,0\ndhead\nq⊤k,\nwhere αattn is a tunable multiplier.\n24\n\nMLP\nThere are 2 matrices, W 1 ∈Rdffn×dmodel, W 2 ∈Rdmodel×dffn (where the shapes are\nRfan_out×fan_in), where dffn is commonly set to 4dmodel. Since both dmodel, dffn scale with width,\nboth matrices are considered “hidden weights.” For example, for Tables 3 and 8,\nW 1 ∼N(0, σ2\nq/dmodel),\nwith Adam LR ηq/ ˜dmodel\nW 2 ∼N(0, σ2\nk/dffn),\nwith Adam LR ηk/ ˜dffn\nWord Unembeddings\nSymmetric to the discussion on input word embeddings, the output word\nunembeddings should be parametrized according to the “output weights” column of Tables 3, 8 and 9.\nOften, the unembeddings are tied with the embeddings, and Tables 8 and 9 allow for this as their\ninitialization schemes are symmetric between input and output weights.\nFor example, for Table 3, we’d set\nW unemb ∼N(0, σ2\nunemb/(dmodel ˜dmodel)),\nwith Adam LR ηunemb/ ˜dmodel.\nFor Table 8, we would instead have\nW unemb ∼N(0, σ2\nunemb/dmodel,0),\nwith Adam LR ηunemb,\n(note dmodel,0 here is the base width and therefore is a constant) and the output is computed as\nlogits = αoutput\n˜dmodel\nW unembz\nwhere z is the ﬁnal layer embedding of a token, and αoutput is a tunable multiplier.\nB.2\nOther Parameters\nLearnable scalar multipliers\nFor learnable scalar multipliers (e.g., softmax inverse temperature),\none can initialize them to 1 and use a constant (in width) learning rate for both SGD and Adam. This\nis compatible with Tables 3, 8 and 9.\nPositional Bias\nSome Transformers use positional bias (of size contextsize×contextsize, which\nare added to the attention logits). They are considered “scalar-like” in that it has no width dimension.\nOne can initialize them to 0 and use a constant (in width) learning rate for both SGD and Adam. This\nis compatible with Tables 3, 8 and 9.\nSpatial MLPs\nRecent works [12, 24, 31, 48, 49] on MLP-only architectures in NLP and CV replace\nthe self-attention layer in Transformers with MLPs across tokens or spatial locations. In our language\nhere, such MLPs have ﬁnite input and output dimensions (the context size) and inﬁnite hidden\ndimensions, so their input, output, and hidden weights should be parametrized via the corresponding\ncolumns in Tables 3, 8 and 9.\nB.3\nOptimizer Variants and Hyperparameters\nAdamW\nExactly the same as Adam in all of our tables, with the added beneﬁt that weight decay is\nautomatically scaled correctly in AdamW (but is incompatible with µP Adam). For this reason, we\nrecommend using AdamW when weight decay is desired (which is consistent with current standard\npractice).\nFrobenius Normalization\nLARS [63], Adafactor [42], Lamb [64], Layca [8], Fromage [6], Nero\n[28] all involve a normalization step in which the update g (which may be obtained from SGD,\nAdam, or other optimzers) is normalized to have Frobenius norm equal to that of the parameter\nw: g ←∥w∥F\n∥g∥F g. They can be made compatible with µP in Table 8 by scaling their learning rate\nfor hidden weights like 1/√fan_in (for Table 3, the output weight learning rate should be likewise\nscaled). The intuitive reasoning (which can be formalized straightforwardly using Tensor Programs)\nis as follows.\nThis normalization implicitly encodes a width scaling: If one initializes a weight matrix with variance\n1/fan_in, then an n×n matrix (e.g., a hidden weight matrix) has Frobenius norm √n at initialization.\nThus, in the ﬁrst step and, by induction, in any step t, the normalized update to this n × n weight also\n25\n\nhas Frobenius norm Θ(√n) (for any ﬁxed t, as n →∞). Heuristically, this means each entry of g is\napproximately of size Θ(1/√n). But, by the derivation of Appendix J, we want Θ(1/n) and this is\nΘ(√n) too large! Thus, in wide enough networks, one should see a network blowup after one update,\nlike demonstrated in Fig. 5.\nHowever, note that the Θ(1/√n) coordinate size induced by the normalization here is closer to the\nright size Θ(1/n) than Adam, whose update have coordinate size Θ(1). This may partially explain\nthe apparent beneﬁt of these optimizers. In particular, this may explain the observation that T5 [38],\nusing Adafactor, was able to train its entire range of models from 220 million to 11 billion parameters\nwith a ﬁxed set of hyperparameters, while GPT-3 [7], using Adam, needed to decrease its learning\nrate with model size.\nRAdam\nRAdam [25] is a variant of Adam that uses SGD with momentum in an initial stage with\nlearning rate warmup, followed by a second stage of Adam with a particular setting of learning rate\nwith time. Thus, one can adapt RAdam to µP by individually scaling the learning rates of the initial\nSGD stage and the ﬁnal Adam stage according to Table 3, Table 8, or Table 9.\nAdagrad and RMSProp\nExactly the same as Adam in all of our tables.\nϵ in Adam and Its Variants\nAll of our derivations here assume ϵ is negligible in Adam. If it is set\nto a non-negligible number, then it needs to be scaled, for all parameters, like 1/fan_in2 if it is added\nbefore the square root, or like 1/fan_in if it is added after the square root.\nGradient Clipping\nGradient (ℓ2-norm-wise) clipping is compatible with Table 3 (as well as\nTables 8 and 9), for either SGD or Adam, if the clip value is held constant with respect to width.\nWeight Decay\nWeight decay should be scaled independently of width in SGD and AdamW, for all\nof our tables. However, note it’s not compatible with µP Adam.\nMomentum\nMomentum should be scaled independently of width for all of our tables.\nC\nParametrization Matters: A Primer for Multiple Hyperparameters\nHere we give more intuition why we need to reparametrize all hyperparameters. In practice, neural\nnetworks have multitudes of hyperparameters all interacting together. In our example of Section 2,\nhyperparameter optimization would be akin to minimizing the function19\nFn(c1, . . . , ck)\ndef\n=\nE\nx1,...,xn f((c1 + · · · + ck)(x1 + · · · + xn)).\nwhere x1, . . . , xn are as in Eq. (1) and c1, . . . , ck are analogous to k hyperparameters. For the same\nreasoning in Section 2, the correct parametrization is in (α1, . . . , αk) where αi = ci√n.\nWhile this is straightforward, in practice, researchers often ﬁx some hyperparameters (e.g., they tune\nonly learning rate but neglects to scale parameter multipliers or initialization correctly). For example,\nif we only partially reparametrize and optimize in α1 while ﬁxing c2, . . . , ck, then the optimal α1 is\n(α1)∗= α∗−(c1 + . . . + ck)√n where α∗is the optimal α for Eq. (1). Thus, as n →∞, (α1)∗still\nblows up even though we parametrized α1 correctly. More generally, the incorrect parametrization\nof some hyperparameters forces other hyperparameters to increasingly compensate for it as width\ngrows, distorting their optima, even if the latter are correctly parametrized.\nD\nPractical Considerations\nIn this section, we outline several useful tips and tricks that can improve the quality of hyperparameter\ntransfer in practice.\n19Here, for simplicity of the example, we model the interaction between “hyperparameters” c1, . . . , ck as\nadditive, but in real neural networks such interactions are usually much more complicated.\n26\n\nD.1\nVerifying µP Implementation via Coordinate Checking\nEven though µP is neatly encapsulated by Table 3, implementing it correctly can in practice be\nerror-prone, just like how implementing autograd by hand can be error-prone even though the math\nbehind is just chain-rule. In the case of autograd, gradient checking is a simple way of verifying\nimplementation correctness; similarly, we propose coordinate checking to verify the correctness\nof µP implementation: Exempliﬁed by Fig. 5, one calculates the average coordinate size of every\n(pre)activation vector in the network over a few steps of training, as width is varied over a large\nrange. An incorrect implementation will see some activation vector blow up or shrink to zero with\nwidth (like in the top row of Fig. 5). In the mup package we release with this paper, we include an\neasy-to-use method for coordinate checking.\nD.2\nZero Initialization for Output Layers and Query Layers in Attention\nWe ﬁnd that the optimal hyperparameters of small and large width models match more closely when\nwe initialize output layers at 0 (i.e. with variance σ2/fan_in where σ = 0 instead of positive σ).\nThis is because the neural network in µP is approximately a Gaussian process (GP) at initialization\nwith variance on the order Θ(σ2/width) (contrast this with SP networks, which approximates a\nGP with Θ(σ2) variance) [21, 29, 53, 57]. Of course, when width is large, this variance vanishes,\nbut this can be far from so in the small proxy model. This discrepancy in the initial GP can cause\nthe training trajectory of the proxy model to be very different from the trajectory of the large target\nmodel, causing a mismatch in the optimal hyperparameters. By initializing the output layer at 0, we\nremove this mismatch in the initial GP. Empirically we do not ﬁnd this modiﬁcation to be detrimental\nto performance.\nA similar consideration applies to the query layer in self-attention: At initialization, the attention logit\nq⊤k/dhead looks like a Gaussian with variance Θ(1/dhead) because q and k are almost independent\nand zero-mean. In the limit dhead →∞, the logit is exactly 0, which can be a large discrepancy\ncompared to when dhead is small in the small proxy model we want to tune. By initializing the\nquery projection matrix W q to 0, q will also be 0, and hence the attention logit is always 0 at\ninitialization regardless of width (but will generally become nonzero after a gradient step), resolving\nthis discrepancy.\nMore generally, any layer or computation that goes from an “inﬁnite” dimension (i.e. width) to a\n“ﬁnite” dimension (e.g. output dimension or sequence length) can exhibit this kind of discrepancy due\nto the initial GP. When dhead →∞and nhead is ﬁxed, attention logit calculation can be viewed in\nthe same vein as a function Rseqlen×dmodel →Rnhead×seqlen×seqlen, which “reduces to” R∞→R1.\nD.3\nActivation Functions\n14\n12\n10\n8\n6\nlog2LearningRate\n0.50\n0.75\n1.00\n1.25\n1.50\n1.75\nTraining Loss\nSP / tanh / xent\n256\n512\n1024\n2048\n4096\n8192\n14\n12\n10\n8\n6\nlog2LearningRate\n0.50\n0.75\n1.00\n1.25\n1.50\n1.75\nP / tanh / xent\n15\n10\n5\n0\nlog2LearningRate\n0.02\n0.04\n0.06\n0.08\n0.10\nSP / tanh / mse\n15\n10\n5\n0\nlog2LearningRate\n0.02\n0.04\n0.06\n0.08\n0.10\nP / tanh / mse\nFigure 9: Squashing activation functions reduce transfer quality. MLP of different hidden sizes\nwith tanh activation trained for 20 epoch on CIFAR-10 using SGD. Left uses cross-entropy as loss\nfunction; right uses mean squared error; columns alternate between standard parametrization (SP)\nand maximal update parametrization (µP). Compared to ReLU, tanh exhibits slower convergence for\nµP, yet it still outperforms SP when width is increased\nWhen the network is narrow, its approximation to the inﬁnite-width behavior becomes crude, which\nis manifested as large ﬂuctuations in preactivation coordinates. When using a squashing activation\nfunctions like softmax or tanh, this causes narrower networks to saturate the activation more than\nwider ones, which results in a systematic bias toward small gradients and therefore distorting the\nhyperparameter landscape. This can be seen in Fig. 9, where we use tanh as the network activation\nfunction.\n27\n\n1\n2\n3\n4\nlog2(cattn)\n0.02\n0.00\n0.02\n0.04\n0.06\n0.08\n0.10\n0.12\n Valid. Loss from Best\ndk not enlarged\nWidth Mult.\n0.0625\n1.0\n1\n2\n3\n4\nlog2(cattn)\ndk enlarged\nWidth Mult.\n0.0625\n1.0\nFigure 10: Enlarging dk makes µTransfer more precise. Here we plot all curves after subtracting\ntheir minima for easier visual comparison. Transformer on IWSLT 14 similar to the setup in Ap-\npendix F.1 where the dmodel = 512 for a width multiplier of 1, nhead = 4, and dq = dk. (Left) We\nleave dq = dk = dmodel/nhead, so dk = 8 for width-multiplier 0.0625. The optimum for the attention\nlogit multiplier cattn is noisy and does not accurately transfer across width. (Right) We enlarge\ndq = dk to a minimum of 128. The HP landscape is much smoother than in (Left), and the optima\nalign between narrow and wide models.\nTherefore, we recommend replacing non-essential squashing activation functions with ReLU, whose\nderivative depends only on the sign of the pre-activation. A similar reasoning can be applied to\nsuperlinear activation functions, where the distribution of activation values can have heavy tails,\nleading to slow convergence to the inﬁnite-width limit. However, such activations are rarely used in\npractice.\nD.4\nEnlarge dk\nWe ﬁnd that small dhead = dk can lead to a highly noisy HP landscape, as shown in Fig. 10. This\ncan signiﬁciantly decrease the quality of random HP search on the small proxy model. To solve this,\nwe ﬁnd it useful to decouple dk from dmodel (so that dmodel ̸= dk · nhead) and maintain a relatively\nlarge dk even as dmodel is shrunk in the proxy model. For example, pegging dk = 32 is generally\neffective. Training or inference speed are not usually affected much by the larger dk because of\nCUDA optimizations. By Appendix E.2, this decoupling of dk from dmodel is theoretically justiﬁed,\nand as shown in Fig. 10, it signiﬁcantly denoises the HP landscape.\nD.5\nNon-Gaussian vs Gaussian Initialization\nWe ﬁnd non-Gaussian (e.g. uniform) initialization can sometimes cause wider models to perform\nworse than narrower models, whereas we do not ﬁnd this behavior for Gaussian initialization. This is\nconsistent with theory, since in the large width limit, one should expect non-Gaussian initialization\nto behave like Gaussian initializations anyway (essentially due to Central Limit Theorem, or more\nprecisely, universality), but the non-Gaussianity slows down the convergence to this limit.\nD.6\nUsing a Larger Sequence Length\nFor Transformers, we empirically ﬁnd that we can better transfer initialization standard deviation\nfrom a narrower model (to a wide model) if we use a larger sequence length. It is not clear why this\nis the case. We leave an explanation to future work.\nD.7\nTuning Per-Layer Hyperparameters\nThe techniques in this paper allow the transfer across width of (learning rate, initialization, multipliers)\nsimultaneously for all parameter tensors. Thus, to get the best results, one should ideally tune all\nsuch hyperparameters. In practice, we ﬁnd that just tuning the global learning rate and initialization,\nalong with input, output, and attention multipliers, yield good results.\n28\n\nE\nWhich Hyperparameters Can Be Transferred? (Continued)\nE.1\nFurther Discussions on Hyperparameter Categories\nBelow, we discuss the reasoning behind each kind, which are supported by our empirical evidence\ncollected in Fig. 4 on Transformers as well as those in Appendix G.1 on ResNet.\nTransferable Hyperparameters\nIn Table 2, we summarize which HPs can be transferred across\ntraining scale. The transfer across width, as explained in Section 2, is theoretically justiﬁed, while we\npresent the transfer across the other dimensions as empirical results.\nThese cover most of the well-known and important HPs when the need for regularization is not\nparamount, e.g., during large scale language model pretraining. Parameter Multipliers are not well-\nknown HPs, yet we include them here as they serve a bridge between SP and µP and can impact\nmodel performance in practice. Concretely, any SP and µP neural networks of the same width can\nhave their Parameter Multipliers tuned so that their training dynamics become identical.\nHyperparameters That Don’t Transfer Well\nNot all HPs transfer well even if we use µP. In\nparticular, those whose primary function is to regularize training to mitigate “overﬁtting\" tend not to\ntransfer well. Intuitively, regularization needs to be applied more heavily in larger models and when\ndata is scarce, but µP does not know the data size so cannot adjust the regularization accordingly.\nTo the best of our knowledge, there is no strict separation between HPs that regularize and those that\ndon’t. However, conventional wisdom tells us that there exists a spectrum of how much regularizing\neffect a HP has. For example, dropout probability and weight decay are among those whose primary\nfunction is to regularize, whereas batch size and learning rate might regularize training in some cases\nbut affect the dynamics more so in other ways. Our empirical exploration tells us that the former do\nnot transfer well, while the latter do. Our subsequent discussion will focus on the latter; we leave to\nfuture works the expansion to the former.\nHyperparameters Transfered Across\nWe have left out a category of HPs that deﬁnes the training\nscale, or in practical terms, training cost. This includes 1) those that deﬁne how many operations a\nmodel’s forward/backward pass takes, such as the model’s width, depth, and in the case of language\nmodeling, sequence length; and 2) those that deﬁne how many such passes are performed, such as\nbatch size and number of training steps.\nAs recent works have shown [7, 19, 39], improvements along any of these scale dimensions lead to\napparently sustainable gain in performance; as a result, we are primarily interested in transferring\nother HPs across these dimensions that deﬁne scale, rather than ﬁnding the optimal scale.20 This\ncategory of HPs is particularly crucial as one can speedup training by downsizing in one or multiple\nsuch dimensions. Indeed, it’s very common for practitioners to implicitly transfer HPs across the\nnumber of training samples by tuning on only a subset of the full training data.\nOur insights from the inﬁnite-width limit inspired us to explore HP tranfer across width, which\ndoes not work under SP as we have shown earlier. Building upon our success with width, which\nis well explained theoretically, we hope to push the limit of compute-saving by investigating the\nother dimensions empirically. To the best of our knowledge, the transferability of optimal HPs across\ndepth, batch size, sequence length, and training time has not been rigorously investigated previously,\nwith the main exception of the literature on (learning rate, batch size) scaling [41, 44] where our\ntransferability result of learning rate across batch size recapitulates [30].21 See Section 10.3 on how\nour results relate to prior works. We will primarily focus on the Transformer architecture in the main\ntext with evidence for ResNet in Appendix G.1.\nE.2\nOn the Deﬁnitions of Width\nOur theory allows more general notions of width. This is especially relevant in Transformers,\nwhere dmodel, dhead = dk, dv, nhead, dffn (see Fig. 11) can all be construed as measures of width.\n20In particular, we are not ﬁxing the total training FLOPs when we scale, which requires understanding the\ntradeoff of different scale HPs. For example, when we transfer across batch size, we ﬁx the number of steps of\ntraining (not the number of epochs), so that the total FLOPs scales linearly.\n21There’s also a literature on the proper initialization for training deep networks effectively (e.g. [5, 16, 26,\n40, 59, 60, 66]), but they do not study the transferability per se. See Section 10.3\n29\n\n𝑊𝐾\n𝑊𝑄\n𝑊𝑉\n𝑑𝑘\n𝑑𝑘\n𝑑𝑣\nSelf-attn (paramless)\n𝑊𝑂\n𝑑𝑚𝑜𝑑𝑒𝑙\n𝑑𝑓𝑓𝑛\n𝑑𝑚𝑜𝑑𝑒𝑙\n𝑑𝑚𝑜𝑑𝑒𝑙\nSkip connection\nSkip connection\n𝑑𝑚𝑜𝑑𝑒𝑙\n𝑑𝑓𝑓𝑛\n𝑑𝑚𝑜𝑑𝑒𝑙\n𝑑𝑚𝑜𝑑𝑒𝑙\n𝑑𝑣⋅𝑛ℎ𝑒𝑎𝑑\n𝑛𝑜. ℎ𝑒𝑎𝑑𝑠= 𝑛ℎ𝑒𝑎𝑑\nSkip connection\nSkip connection\n(a) Single-head attention\n(b) Multi-head attention\nFigure 11: Schematics of each Transformer layer. Commonly, the key and value dimensions dk\nand dv are both set to dmodel/nhead, and this is referred to as dhead.\n4\n3\n2\n1\n0\n1\nlog2(LearningRate)\n4.5\n5.0\n5.5\n6.0\nValidation Loss\nTransformer on IWSLT14 De-En\n(Varying dffn)\ndffn/dmodel\n0.5\n1.0\n2.0\n4.0\n8.0\n16.0\nFigure 12: Learning rate landscape in µP is stable even if we vary dffn by a factor of 32, ﬁxing\ndmodel.\nWe brieﬂy discuss these here, with more theoretical justiﬁcation in Appendix J.2.1 and empirical\nvalidation below.\nVarying Width Ratio\nSo far we have assumed that every hidden layer is widened by the same\nfactor. But in fact we can widen different hidden layers differently. This is useful, for example, in a\nTransformer where we may want to use a smaller dffn during tuning. If we are using Adam, as long\nas the width of every layer still tends to inﬁnity, we still obtain approximately the same limit22, so the\nµTransfer remains theoretically justiﬁed.\nSee Fig. 12 for an empirical validation on IWSLT-14 using a Transformer.\nNumber of Attention Heads\nIn attention-based models, one typically splits hidden size into\nmultiple attention heads following dmodel = dhead × nhead. So far we have assumed dhead and\ndmodel to be width, but it’s possible and potentially advantageous to ﬁx dhead and treat nhead as\nthe width, or increasing both simultaneously. This allows our technique to handle many popular\nmodels, including GPT-3 [7], which scale up by ﬁxing dhead and increasing nhead. See Fig. 13 for\nan empirical validation on Wikitext-2.\nVarying Just the Width of Attention Heads\nA speciﬁc useful instance of varying width ratio is\ndecoupling the key and value dimensions dk and dv and scaling dk differently from (typically larger\n22This also applies for SGD, but we need more involved scaling to keep the limit approximately the same.\n30\n\n14\n12\n10\nlog2\n3.5\n4.0\n4.5\n5.0\n5.5\n6.0\nTraining Loss\nWidth\n256\n512\n1024\n2048\n4096\n8192\n5\n0\n5\n10\n15\nlog2\noutput\n3.0\n3.5\n4.0\n4.5\n5.0\n10\n0\n10\nlog2\nattn\n3.5\n4.0\n4.5\n5.0\n5.0\n2.5\n0.0\n2.5\nlog2\n4\n5\n6\nP - Fixing dhead while varying dmodel and nhead\nFigure 13: µTransfer across width when we ﬁx dhead and vary dmodel and nhead. αoutput, αattn are\nmultipliers for output and key weights, and σ is initialization standard deviation.\nthan) dmodel/nhead. This works as long as we use 1/d scaled-attention as in Deﬁnition 4.1 (instead\nof 1/\n√\nd as is done commonly). When tuning on the small proxy model, if dk is too small, the HP\nlandscape can be quite noisy. Keeping dk relatively large while shrinking all other dimensions solves\nthis problem, while still obtaining signiﬁcant speedup.\nF\nExperimental Details\nF.1\nIWSLT\nIWSLT14 De-En is a well-known machine translation benchmark. We use a Transformer implemented\nin fairseq [33] with a default dmodel = 1/4dffn = 512 and dk = dq = dv = dmodel/nhead = 128\n(amounting to 40M parameters), which we denote as the 1x model. For transfer, we tune on a proxy\nmodel with the same nhead but with dmodel and other dimensions 4 times smaller; we will call this\nthe 0.25x model (but it has 4M parameters). All models are trained with Adam for 100 epochs and\nvalidated at the end of every epoch. We tune via random search the learning rate η, the output layer\nparameter multiplier αoutput, and the attention key-projection weight multiplier αattn following the\ngrid\n• η: 5 × 10−4 × 2z, where z ∈{−1.5, −1.25, −1, ..., 1.25}\n• αoutput: 2z, where z ∈{−8, −7, −6, ..., 7}\n• αattn: 2z, where z ∈{−3, −2, −1, ..., 8}\nF.2\nWMT\nWe scale up to WMT14 En-De using the large Transformer from [50], with a dmodel = 1/4dffn =\n1024 and dq = dk = dv = dmodel/nhead = 64. We use the exact same setup and reproduce their\nresult as our baseline. Then, we build the proxy model by shrinking the target model’s dmodel from\nthe original 1024 to 256, dffn from 4096 to 256 and nhead from 16 to 4. This reduces the total\nparameter count from 211M to 15M. We then perform the HP search on the proxy model and take the\nbest according to validation loss, before testing on the target model. We tune via random search the\nlearning rate η, the output layer parameter multiplier αoutput, and the attention key-projection weight\nmultiplier αattn following the grid\n• η: 6 × 10−4 × 2z, where z ∈{−1.5, −1.25, −1, ..., 1.25}\n• αoutput: 2z, where z ∈{−8, −7, −6, ..., 7}\n• αattn: 2z, where z ∈{−3, −2, −1, ..., 8}\nF.3\nBERT\nDetails of BERT Prototype\nOur proxy model has 10 Transformer layers with dmodel = dffn =\n256. We also reduce the number of attention heads to 8 with a dhead of 32. We call it BERT Prototype\nsince we can increase its width and depth according to our deﬁnitions to recover both BERT Base and\nBERT Large, which enables us to sweep HPs once and use for both models. Overall, BERT Prototype\nhas 13M trainable parameters, a fraction of the 110M in BERT Base and the 350M in BERT Large.\n31\n\nHyperparameters Tuned for Pretraining\nWe tune the following HPs for pretraining: Adam\nlearning rate η, embedding learning rate ηemb, output weight multiplier αoutput, attention logits\nmultiplier αattn, layernorm gain multiplier αLNgain, and bias multiplier αbias.\nWe sample 256 combinations from the follow grid:\n• η: 1 × 10−4 × 2z, where z ∈{1.5, 2, 2.5, 3, 3.5}\n• ηemb: 1 × 10−4 × 2z, where z ∈{−1, −0.5, 0, 0.5, 1}\n• αoutput: 2z, where z ∈{2, 4, 6}\n• αattn: 2z, where z ∈{3, 3.5, 4, ..., 7}\n• αLNgain: 2z, where z ∈{8.5, 9, 9.5, 10, 10.5}\n• αbias: 2z, where z ∈{8.5, 9, 9.5, 10, 10.5}\nThe ranges are chosen to include the implicit choices of these HPs in SP BERT Large.\nFinetuning Procedure and Hyperparameters\nWe hand-pick the ﬁnetuning HPs after training the\nfull-sized model. As regularization is an essential ingredient in successful ﬁnetuning, we do not\ntransfer such HPs (at least via the suite of techniques presented in this work) (see Table 1). We focus\non MNLI [52] and QQP, which are two representative tasks from GLUE [51]. Following [27], we\nused Adam [20] with a learning rate of 5 × 10−5 and a batch size of 64. The maximum number of\nepochs was set to 5. A linear learning rate decay schedule with warm-up of 0.1 was used. All the\ntexts were tokenized using wordpieces and were chopped to spans no longer than 128 tokens.\nF.4\nGPT-3\nBaseline 6.7B GPT-3 Transformer\nAs the GPT-3 codebase has evolved since the publication of\n[7], we re-trained the 6.7B model from scratch to remove changes in our codebase as a possible\nconfounder. The main differences to [7] are 1) a modiﬁed learning rate decay schedule, where\nthe learning rate is decayed to zero at the end of training rather than being decayed to 0.1 of the\ninitial value, and 2) use of relative attention in place of absolute attention. Unfortunately, after all\nexperiments were ﬁnished, we found this re-run baseline used absolute attention instead of relative\nattention, while the µTransfer model still used relative attention.\nRandom Search using Reduced-Width Proxy Model\nIn order to ﬁnd a good set of hyperparam-\neters for the µTransfer version of the 6.7B model, we performed a hyperparameter search over a\nreduced version of the model (i.e., the proxy model), where the width is set to 256 hidden units. This\nproxy model inherits changes from the evolved GPT-3 codebase: it uses relative [10] (instead of\nabsolute) position encoding. Early on, we noted that on the proxy model, linear learning rate decay\noutperformed the default cosine schedule, so all subsequent experiments for the proxy models use a\nlinear decay schedule. By Fig. 4, µTransferring this linear decay schedule to the full model should\nmaintain such a performance advantage over the cosine schedule.\nThe hyperparameter search space consists of the following hyperparameters:\n• learning rate: Sampled from 10Uniform(−4,−1)\n• initialization scale: All the parameters are multiplied - sampled from 10Uniform(−1,1)\n• attention temperature: Reciprocal of the multiplier applied to the input to attention soft-\nmax. Sampled from 4Uniform(−1,1).\n• output temperature: Reciprocal of the multiplier applied to the input to softmax that\nproduces the distribution over output tokens. Sampled from 4Uniform(−1,1).\n• embedding multiplier: Scalar by which we multiply the output of the embedding layer.\nSampled from 10Uniform(−1,1).\n• relative position embedding multiplier: Scalar by which we multiply vectors representing\nrelative position. Sampled from 10Uniform(−1,1).\nIn order to make the search more efﬁcient we reduced the total number of training tokens. We\nhypothesized that tuning hyperparameters on a reduced total number of tokens does not signiﬁcantly\naffect optimal hyperparameters. To verify, we trained two different horizons and compared the results.\n32\n\n3.2\n3.22\n3.24\n3.26\n3.28\n3.3\nloss\n3.3\n3.19674\n0.0001\n0.000562\n0.00316\n0.0178\n0.1\nlearning rate\n0.1\n0.316\n1\n3.16\n10\ninitialization\n0.1\n0.316\n1\n3.16\n10\nembedding\nmultiplier\n0.25\n0.5\n1\n2\n4\nattention\ntemperature\n0.25\n0.5\n1\n2\n4\noutput\ntemperature\n0.1\n0.316\n1\n3.16\n10\nrelative\nmultiplier\n3.1\n3.15\n3.2\n3.25\nloss\n3.3\n3.09418\n0.0001\n0.000562\n0.00316\n0.0178\n0.1\nlearning rate\n0.1\n0.316\n1\n3.16\n10\ninitialization\n0.1\n0.316\n1\n3.16\n10\nembedding\nmultiplier\n0.25\n0.5\n1\n2\n4\nattention\ntemperature\n0.25\n0.5\n1\n2\n4\noutput\ntemperature\n0.1\n0.316\n1\n3.16\n10\nrelative\nmultiplier\nFigure 14: Results of the random search over reduced-width GPT-3 proxy models trained on 4\n(left) and 16 (right) billion tokens. Only the best performing runs are highlighted.\nWhile the target model was to be trained on 300 billion tokens, we tuned the proxy model on only\nsubsets consisting of 4 billion and 16 billion tokens. This impacts both the total training time and the\nlength of the linear learning rate decay schedule. Other than hyperparameters explicitly listed above\nand the training horizon, the rest was the same as what we intended to use for the full width 6.7B\ntraining run.\nAnalyzing the Results of the Random Search\nWe performed 467 training runs of the proxy\nmodel, out of which 350 were for 4 billion tokens (286 completed without diverging) and 117 for\n16b tokens (80 completed without diverging). See Fig. 14 for summary of the results.\nAs suspected, we observed that the results are well-aligned for both 4 and 16 billion tokens versions.\nWe observe learning rate and initialization scale impact the results the most. Based on the results we\nchose 0.006 for the former and 2.5 for the latter. Since most other hyperparameters appear to have\nnegligible effect on performance, they were kept at their default values of 1, the only exception being\nthe embedding scale, where higher values seem to perform better and it was therefore set to 10.\nTraining the µTransfer Model\nWe encountered frequent divergences in our initial attempt to train\nthe µTransfer model. We traced the issue back to underﬂow of FP16 tensors in the backwards pass\nand therefore switched to training the model in FP32. This allowed us to ﬁnish the training run\nwithout divergences. We hypothesize that the divergence issue is related to µTransfer picking more\naggressive hyperparameters, for example a higher learning rate on linear weight tensors compared\nto the original model. In order to exclude code differences as a possible confounder, we re-trained\nGPT-3 6.7B from scratch using the original hyperparameters. The only difference compared to the\nversion published in [7] is that the learning rate was decayed fully, whereas the learning rate of the\nmodel from [7] was only decayed to 10% of its starting value. The retrained model performs slightly\nworse than the original published in [7]. We suspect that this is because it made less progress during\nthe last phase of training where the learning rate is close to zero. The training curves of the µTransfer\nmodel and the re-run of the original 6.7B can be seen in Fig. 15. Detailed evaluation results can be\nfound in Table 10 and Table 11.\nRatio of Tuning Cost to Pretraining Cost\nin FLOPs can be approximated as\ns(t1N1 + t2N2)\nST\n≈0.07\nwhere\n• s = 40 Million is number of parameters of the proxy model\n• S = 6.7 Billion is number of parameters of the target model\n• t1 = 4 Billion is the number of training tokens for the short horizon HP search, and\nN1 = 350 is the corresponding number of random HP search trials.\n• t2 = 16 Billion is the number of training tokens for the longer horizon HP search, and\nN1 = 117 is the corresponding number of random HP search trials.\n33\n\n0.5\n1.0\n1.5\n2.0\n2.5\n3.0\nTraining tokens\n1e11\n2.0\n2.1\n2.2\n2.3\n2.4\n2.5\nValidation loss\nGPT-3 6.7B\nGPT-3 6.7B + Transfer\nFigure 15: The training curves of the GPT-3 6.7B model with µTransfer (orange) and a re-run\nwith the original settings from [7] (blue). The µTransfer model uses relative attention while the\nre-run uses absolute attention. In addition, the former was trained using FP32 activations and weights\nafter initially encountering stability issues with the hyperparameters computed using µP, while the\nre-run used the original FP16 training. The µTransfer model seems to underperform in the middle\nof training, but achieves a much better ﬁnal validation loss once the learning rate is fully decayed.\nWhile the original model uses a cosine schedule, the µTransfer model uses a linear learning rate\ndecay schedule transferred from the proxy model.\n• T = 300 Billion is the number of training tokens for the 6.7B target model.\nHere we are using the fact that the training FLOPs of a Transformer per token is roughly proportional\nto its number of parameters.\nG\nAdditional Experiments\nG.1\nExperiments on ResNets\nG.1.1\nResNet on CIFAR-10\nSetup\nFor this case we use Davidnet [2], a ResNet variant that trains quickly on CIFAR-10, so as\nto efﬁciently investigate its HP landscape. We train with SGD on CIFAR-10 for 10 epochs; all results\nare averaged over 15 random seeds. We use a width multiplier to identify models of different width,\nand a multiplier of 1 corresponds to the original model in [2]. We look at validation accuracy here as\nthe model barely overﬁts, and our observations will hold for the training accuracy as well. We ﬁrst\nconduct a learning rate sweep for models of different widths using SP; the result is shown in Fig. 16,\non the left.\nHyperparameter Stability\nNote that the best model with a width multiplier of 8 under-performs\nthat with a multiplier of 4. We run the same sweep with µP, along with a sweep of the output\nmultiplier (αoutput); the result is shown in Fig. 16, on the right. We notice that wider models always\nperform better under µP and that the optimal learning rate η and αoutput are stable across width.\n34\n\nTable 10: Full evaluation results of our GPT-3 6.7B models: The new model tuned with µTransfer\n(marked µP), the original model from [7], and a re-training of this model from scratch with the\noriginal hyperparameter settings (marked re-run). The sampling-based evaluations shown here are\na subset of the ones from [7]. Since the sampling-based evaluations are subject to high variance,\nWikitext 103 and the LM1B benchmark have been added to help distinguish the relative performance\nof the µP and non-µP model. Note that Wikitext-103 [32] and the LM1B [9] benchmarks overlap\nwith the training dataset. Accuracies and F1 scores have been multiplied by 100. The perplexities\nreported in this table are based on a custom BPE encoding and are not comparable to other results in\nthe literature. The number k of examples in the context for each task is identical to [7].\nNote: Zero-shot, One-Shot and Few-Shot refer to the number of additional query and answer pairs\npassed in the context when performing the sampling-based evaluations, not the ”shots” involved in\nhyperparameter transfer.\nZero-shot\nOne-shot\nFew-shot\nTask\nSplit\nMetric\nµP\n[7]\nre-run\nµP\n[7]\nre-run\nµP\n[7]\nre-run\nValidation dataset\nvalid\nce\n1.98\n2.03\nPTB\ntest\nppl\n11.4\n13.0\nWikitext 103\ntest\nppl\n8.56\n9.13\nLM1B\ntest\nppl\n20.5\n21.7\nHellaSwag\ndev\nacc\n72.0\n67.4\n66.7\n71.1\n66.5\n65.9\n72.4\n67.3\n66.4\nLAMBADA\ntest\nacc\n73.5\n70.3\n70.8\n69.9\n65.4\n64.8\n74.7\n79.1\n77.1\nStoryCloze\ntest\nacc\n79.4\n77.7\n77.3\n80.6\n78.7\n78.3\n84.2\n81.2\n81.1\nNaturalQS\ntest\nacc\n9.86\n5.79\n7.20\n14.7\n9.78\n10.6\n20.2\n17.0\n15.7\nTriviaQA\ndev\nacc\n47.0\n38.7\n37.5\n50.4\n44.4\n42.5\n55.5\n51.6\n49.9\nWebQS\ntest\nacc\n11.3\n7.73\n9.79\n20.2\n15.1\n16.2\n33.0\n27.7\n28.2\nRo→En 16\ntest\nBLEU-sb\n26.9\n8.75\n13.7\n36.5\n34.2\n33.5\n38.2\n36.2\n35.6\nEn→Ro 16\ntest\nBLEU-sb\n18.1\n5.31\n4.40\n21.0\n18.2\n17.3\n22.0\n19.6\n18.8\nFr→En 14\ntest\nBLEU-sb\n29.8\n15.5\n19.6\n31.7\n31.6\n30.1\n38.0\n36.4\n36.5\nEn→Fr 14\ntest\nBLEU-sb\n29.6\n11.4\n11.6\n28.8\n28.3\n26.0\n33.3\n33.3\n31.2\nDe→En 16\ntest\nBLEU-sb\n31.7\n18.2\n21.7\n33.3\n31.9\n31.1\n38.9\n36.5\n36.2\nEn→De 16\ntest\nBLEU-sb\n23.1\n9.36\n9.00\n24.6\n21.7\n21.1\n27.6\n24.1\n24.5\nWinograd\ntest\nacc\n85.3\n85.7\n86.8\n84.6\n84.6\n84.2\n86.4\n85.4\n83.9\nWinogrande\ndev\nacc\n66.8\n64.5\n62.5\n67.6\n65.8\n64.5\n71.0\n67.4\n67.2\nPIQA\ndev\nacc\n79.1\n78.0\n78.0\n77.3\n76.3\n76.9\n79.2\n77.8\n77.7\nARC (Challenge)\ntest\nacc\n42.1\n41.4\n42.5\n44.0\n41.5\n42.4\n43.8\n43.7\n42.7\nARC (Easy)\ntest\nacc\n64.3\n60.2\n61.9\n65.3\n62.6\n63.4\n67.3\n65.8\n65.3\nOpenBookQA\ntest\nacc\n54.4\n50.4\n52.6\n56.4\n53.0\n52.8\n58.4\n55.2\n54.4\nQuac\ndev\nf1\n41.8\n36.1\n38.2\n43.1\n39.0\n39.5\n44.0\n39.9\n39.9\nRACE-h\ntest\nacc\n45.0\n44.1\n43.2\n44.9\n44.3\n42.9\n45.2\n44.7\n43.4\nRACE-m\ntest\nacc\n58.4\n54.4\n54.0\n57.9\n54.7\n53.8\n58.6\n55.4\n55.4\nSQuADv2\ndev\nf1\n59.9\n52.7\n50.9\n64.9\n57.1\n54.7\n68.9\n62.1\n58.4\nCoQA\ndev\nf1\n78.5\n72.8\n72.9\n80.9\n75.1\n74.4\n81.3\n77.3\n75.4\nDROP\ndev\nf1\n17.1\n17.0\n17.4\n23.3\n27.3\n25.7\n33.9\n29.7\n28.7\nBoolQ\ndev\nacc\n69.4\n65.4\n60.9\n74.1\n68.7\n65.0\n73.9\n70.0\n69.7\nCB\ndev\nacc\n21.4\n28.6\n37.5\n60.7\n33.9\n32.1\n62.5\n60.7\n66.1\nCopa\ndev\nacc\n82.0\n80.0\n77.0\n81.0\n82.0\n81.0\n88.0\n83.0\n82.0\nRTE\ndev\nacc\n55.2\n55.2\n46.2\n61.0\n54.9\n58.8\n52.7\n49.5\n59.9\nWiC\ndev\nacc\n0.\n0.\n0.\n50.0\n50.3\n50.3\n50.5\n53.1\n51.3\nANLI R1\ntest\nacc\n33.7\n32.3\n33.4\n32.4\n31.6\n31.7\n30.9\n33.1\n30.7\nANLI R2\ntest\nacc\n33.8\n33.5\n33.0\n34.8\n33.9\n33.7\n35.0\n33.3\n32.2\nANLI R3\ntest\nacc\n32.7\n34.8\n33.4\n34.8\n33.1\n33.3\n36.9\n33.9\n32.3\n35\n\nTable 11: Evaluation results comparing the GPT-3 6.7B model tuned with µTransfer against\nthe twice-as-large GPT-3 13B model from [7]. The two models have similar performance on most\nof the evaluation tasks.\nZero-shot\nOne-shot\nFew-shot\nTask\nSplit\nMetric\n6.7B+µP\n13B[7]\n6.7B+µP\n13B[7]\n6.7B+µP\n13B[7]\nHellaSwag\ndev\nacc\n72.0\n70.9\n71.1\n70.0\n72.4\n71.3\nLAMBADA\ntest\nacc\n73.5\n72.5\n69.9\n69.0\n74.7\n81.3\nStoryCloze\ntest\nacc\n79.4\n79.5\n80.6\n79.7\n84.2\n83.0\nNaturalQS\ntest\nacc\n9.86\n7.84\n14.7\n13.7\n20.2\n21.0\nTriviaQA\ndev\nacc\n47.0\n41.8\n50.4\n51.3\n55.5\n57.5\nWebQS\ntest\nacc\n11.3\n8.22\n20.2\n19.0\n33.0\n33.5\nRo→En 16\ntest\nBLEU-sb\n26.9\n20.8\n36.5\n36.7\n38.2\n38.4\nEn→Ro 16\ntest\nBLEU-sb\n18.1\n6.43\n21.0\n20.8\n22.0\n21.8\nFr→En 14\ntest\nBLEU-sb\n29.8\n22.4\n31.7\n31.4\n38.0\n38.3\nEn→Fr 14\ntest\nBLEU-sb\n29.6\n15.3\n28.8\n30.1\n33.3\n35.5\nDe→En 16\ntest\nBLEU-sb\n31.7\n24.4\n33.3\n34.5\n38.9\n39.1\nEn→De 16\ntest\nBLEU-sb\n23.1\n11.0\n24.6\n23.3\n27.6\n27.7\nWinograd\ntest\nacc\n85.3\n87.9\n84.6\n86.1\n86.4\n82.4\nWinogrande\ndev\nacc\n66.8\n67.9\n67.6\n66.9\n71.0\n70.0\nPIQA\ndev\nacc\n79.1\n78.5\n77.3\n77.8\n79.2\n79.9\nARC (Challenge)\ntest\nacc\n42.1\n43.7\n44.0\n43.1\n43.8\n44.8\nARC (Easy)\ntest\nacc\n64.3\n63.8\n65.3\n66.8\n67.3\n69.1\nOpenBookQA\ntest\nacc\n54.4\n55.6\n56.4\n55.8\n58.4\n60.8\nQuac\ndev\nf1\n41.8\n38.4\n43.1\n40.6\n44.0\n40.9\nRACE-h\ntest\nacc\n45.0\n44.6\n44.9\n44.6\n45.2\n45.1\nRACE-m\ntest\nacc\n58.4\n56.7\n57.9\n56.9\n58.6\n58.1\nSQuADv2\ndev\nf1\n59.9\n56.3\n64.9\n61.8\n68.9\n67.7\nCoQA\ndev\nf1\n78.5\n76.3\n80.9\n77.9\n81.3\n79.9\nDROP\ndev\nf1\n17.1\n24.0\n23.3\n29.2\n33.9\n32.3\nBoolQ\ndev\nacc\n69.4\n66.2\n74.1\n69.0\n73.9\n70.2\nCB\ndev\nacc\n21.4\n19.6\n60.7\n55.4\n62.5\n66.1\nCopa\ndev\nacc\n82.0\n84.0\n81.0\n86.0\n88.0\n86.0\nRTE\ndev\nacc\n55.2\n62.8\n61.0\n56.3\n52.7\n60.6\nWiC\ndev\nacc\n0.\n0.\n50.0\n50.0\n50.5\n51.1\nANLI R1\ntest\nacc\n33.7\n33.2\n32.4\n32.7\n30.9\n33.3\nANLI R2\ntest\nacc\n33.8\n33.5\n34.8\n33.9\n35.0\n32.6\nANLI R3\ntest\nacc\n32.7\n34.4\n34.8\n32.5\n36.9\n34.5\n2\n0\nlog2\n0.90\n0.91\n0.92\n0.93\n0.94\n0.95\nValidation Accuracy\nStandard Parametrization\nWidth mult.\n0.5\n1.0\n2.0\n4.0\n8.0\n3\n2\n1\n0\nlog2\n0.90\n0.91\n0.92\n0.93\n0.94\n0.95\n    \n5\n0\n5\nlog2\noutput\n0.91\n0.92\n0.93\n0.94\n0.95\nMax Update Parametrization ( P)\nFigure 16: ResNet on CIFAR-10 for different widths (compared to a base network). On the left, the\nwidest network SP underperforms; on the right, the µP network has a more consistent HP landscape\nand performs better. Both networks are tuned at the smallest width for the HP (η or αoutput) not in\nthe x-axis.\n36\n\nHyperparameter Transfer\nNext, we perform a grid search for learning rate (η) and αoutput on\nthe 0.5x model for both SP and µP.23 Then, we take the best combination and test on the 8x model,\nsimulating how a practitioner might use µTransfer. The result is shown in Table 12, where µP\noutperforms SP by 0.43% ± .001%.\nTable 12: ResNet on CIFAR10: Transferring the best learning rate (η) and αoutput from widening\nfactor 0.5 to 8; µP signiﬁcantly outperforms SP given the same search grid. The best HPs are different\nas the models are parametrized to be identical at 1x width.23\nTransfer Setup\nBest η\nBest αoutput\nValid. Acc. (0.5x)\nValid. Acc. (8x)\nSP\n0.707\n4\n92.82%\n94.86%\nµP\n0.5\n4\n92.78%\n95.29%\nG.1.2\nWide ResNet on ImageNet\nSetup\nFor this case we use Wide-Resnet, or WRN [65], a ResNet variant with more channels per\nlayer, to further showcase µTransfer across width, i.e., number of channels. We train with SGD\non ImageNet for 50 epochs following standard data augmentation procedures. We use a width\nmultiplier to identify models of different width, and a multiplier of 1 corresponds to the original\nWRN-50-2-bottleneck in [65].\nHyperparameter Transfer\nWe start with a proxy model with a width multiplier of 0.125 and tune\nseveral HPs using the following grid:\n• η: 1 × 2.048 × 2z, where z ∈{−5, −4, −3, ..., 4}\n• αoutput: 10 × 2z, where z ∈{−5, −4, −3, ..., 4}\n• weight decay co-efﬁcient γ: 3.05 × 10−5 × 2z, where z ∈{−2, −1.5, −1, ..., 1.5}\n• SGD momentum β: 0.875 × 2z, where z ∈{−2, −1.5, −1, ..., 1.5}\nThe grid is centered around the default HPs used by [1] for ResNet-50; while not expected to be\ncompetitive for WRN, they represent a reasonable starting point for our experiment.\nWe randomly sample 64 HP combinations from the grid and train for 50 epochs, before selecting\nthe one with the highest top-1 validation accuracy. Then, we scale up the model following both µP\nand SP and run with the same HPs we just selected. The result is shown in Table 13, where µP\noutperforms SP by 0.41% in terms of top-1 validation accuracy.\nTable 13: ResNet on ImageNet: Transferring the best learning rate (η), αoutput, γ, and β from\nwidening factor 0.125 to 1; µP signiﬁcantly outperforms SP given the same search grid.\nTransfer Setup\nBest η\nBest αoutput\nBest γ\nBest β\nValid. Acc. (0.125x)\nValid. Acc. (1x)\nSP\n32.768\n.625\n.000015\n.4375\n58.12%\n76.75%\nµP\n32.768\n.625\n.000015\n.4375\n58.12%\n77.16%\nG.2\nExperiments on Transformers\nG.2.1\nVerifying Transfer across Batch Size, Sequence Length, and Training Time on\nWikitext-2\nSee Fig. 19.\nG.2.2\nPost-Layernorm Transformers\nFig. 17 shows the transferability of learning rate, αoutput, initialization standard deviation, and Adam\nβ2 across width, batch size, sequence length, and training steps for post-layernorm Transformers.\nHowever, in general, we ﬁnd transfer across depth to be fragile.\n23Here we tune the 0.5x model instead of the 1x model to simulate the situation that one does “exploratory\nwork” on the 1x model but, when scaling up, would like to tune faster by using a smaller proxy model.\n37\n\n14\n12\n10\n8\n1\n2\n3\n4\n5\nTraining Loss\nWidth\n128\n256\n512\n1024\n2048\n4096\n5\n0\n5\n10\n15\n1\n2\n3\n4\n5\n5.0\n2.5\n0.0\n2.5\n1\n2\n3\n4\n5\n6\n7\n(a)\n(b)\n(c)\n(d)\n(e)\n(f)\n1.0\n1.5\n2.0\n2.5\n3.0\n3.5\n4.0\n4.5\n14\n12\n10\n2.5\n3.0\n3.5\n4.0\n4.5\n5.0\n5.5\n6.0\nTraining Loss\nBatchSize\n20\n32\n64\n128\n5\n0\n5\n10\n15\n2.5\n3.0\n3.5\n4.0\n4.5\n5.0\n2.5\n0.0\n2.5\n3\n4\n5\n6\n(a)\n(b)\n(c)\n(d)\n(e)\n(f)\n2.4\n2.6\n2.8\n3.0\n3.2\n3.4\n3.6\n3.8\n4.0\n14\n12\n10\nlog2LearningRate\n3.0\n3.5\n4.0\n4.5\n5.0\n5.5\n6.0\n6.5\nTraining Loss\nSeqLen\n32\n64\n128\n256\n512\n1024\n5\n0\n5\n10\n15\nlog2\noutput\n2.5\n3.0\n3.5\n4.0\n4.5\n5.0\n5.5\n5.0\n2.5\n0.0\n2.5\nlog2InitStd\n2.5\n3.0\n3.5\n4.0\n4.5\n5.0\n5.5\n6.0\n6.5\n(a)\n(b)\n(c)\n(d)\n(e)\n(f)\nLR Schedule\n3.0\n3.5\n4.0\n4.5\n5.0\n14\n12\n10\n8\nlog2LearningRate\n3.5\n4.0\n4.5\n5.0\n5.5\n6.0\n6.5\n7.0\nTraining Loss\nStep\n2032\n4072\n5912\n7952\n9992\n5\n0\n5\n10\n15\nlog2\noutput\n3.5\n4.0\n4.5\n5.0\n5.5\n6.0\n5.0\n2.5\n0.0\n2.5\nlog2InitStd\n3.5\n4.0\n4.5\n5.0\n5.5\n6.0\n6.5\n(a)\n(b)\n(c)\n(d)\n(e)\n(f)\nLR Schedule\n3.5\n4.0\n4.5\n5.0\n5.5\n6.0\n6.5\nFigure 17: Empirical validation of µTransfer for Post-LN Transformers. Same setting as Fig. 4.\nG.2.3\nHyperparameter Instability of SP Transformers\nFig. 18 and Fig. 20 show the HP instability inherent in SP Transformers.\nH\nImplementing µTransfer in a Jiffy\nAs we have shown, one can enable µTransfer by just reparametrizing the desired model in Maximal\nUpdate Parametrization (µP). While conceptually simple, switching from Standard Parametrization\n(SP) to µP can be error-prone, as popular deep learning frameworks are built around SP. We strive to\nbuild a tool that fulﬁlls two goals:\n1. Minimize code changes when switching to µP;\n2. Keep model behavior invariant, under this switch, at a given base model shape.\nBy model shape, we mean the collection of dimensions of all parameters of the model. The latter goal,\nwhich we call parametrization backward compatibility, ensures that any code base works exactly as\nbefore at the base model shape, similar to Eq. (4), e.g. the loss at any time step remains exactly the\nsame before and after the switch to µP. Of course, when widths start to differ from the base model\nshape, the model behavior necessarily changes so that HPs can be transferred.\n38\n\n20\n15\n10\nlog2LearningRate\n4\n5\n6\n7\nTraining Loss\n128\n256\n512\n1024\n2048\n4096\n8192\n0\n10\nlog2\noutput\n4.5\n5.0\n5.5\n6.0\n6.5\n7.0\n10\n0\nlog2\nattn\n4.0\n4.5\n5.0\n5.5\n6.0\n6.5\n7.0\n7.5\n5\n0\nlog2\n4.5\n5.0\n5.5\n6.0\n6.5\n7.0\n12\n10\nlog2LearningRate\n3.5\n4.0\n4.5\n5.0\nTraining Loss\n0\n10\nlog2\noutput\n3.50\n3.75\n4.00\n4.25\n4.50\n4.75\n5.00\n5\n10\n15\nlog2\nattn\n3.5\n4.0\n4.5\n5.0\n5\n0\nlog2\n3\n4\n5\n6\nStandard Parametrization (SP)\nMaximal Update Parametrization ( P)\nFigure 18: Post-layernorm Transformer with SP and µP on Wikitext-2. We sweep one HP across\nwidth (dmodel) at a time while keeping the rest ﬁxed; we also scale dhead linearly with dmodel and\nﬁxing nhead. αoutput, αattn are multipliers for output and key weights, and σ is initialization standard\ndeviation. This yields unstable result for SP, as expected, where missing points/curves represent\ndivergence; in µP, the optimal HP choices stabilize as width increases.\n14\n12\n10\n8\n2.0\n2.5\n3.0\n3.5\n4.0\n4.5\n5.0\n5.5\nBatchSize\nTraining Loss\nBatchSize\n20\n32\n64\n128\n256\n512\n5\n0\n5\n10\n15\n2.0\n2.5\n3.0\n3.5\n4.0\n4.5\n5.0\n5.5\n5.0\n2.5\n0.0\n2.5\n2.0\n2.5\n3.0\n3.5\n4.0\n4.5\n(a)\n(b)\n(c)\n(d)\n(e)\n(f)\n2.0\n2.5\n3.0\n3.5\n4.0\n14\n12\n10\n8\n3.0\n3.5\n4.0\n4.5\n5.0\n5.5\n6.0\n6.5\nSeqLen\nTraining Loss\nSeqLen\n32\n64\n128\n256\n512\n5\n0\n5\n10\n15\n3.0\n3.5\n4.0\n4.5\n5.0\n5.5\n6.0\n6.5\n5.0\n2.5\n0.0\n2.5\n3.0\n3.5\n4.0\n4.5\n5.0\n5.5\n(a)\n(b)\n(c)\n(d)\n(e)\n(f)\n3.0\n3.5\n4.0\n4.5\n5.0\n14\n12\n10\n8\nlog2LearningRate\n3.5\n4.0\n4.5\n5.0\n5.5\n6.0\n6.5\nStep\nTraining Loss\nStep\n2032\n4072\n5912\n7952\n9992\n5\n0\n5\n10\n15\nlog2\noutput\n3.5\n4.0\n4.5\n5.0\n5.5\n5.0\n2.5\n0.0\n2.5\nlog2InitStd\n3.5\n4.0\n4.5\n5.0\n5.5\n6.0\n6.5\n7.0\n(a)\n(b)\n(c)\n(d)\n(e)\n(f)\nLR Schedule\n3.5\n4.0\n4.5\n5.0\n5.5\n6.0\nFigure 19: Empirical validation of µTransfer across Batch Size, Sequence Length, and Training\nTime on pre-LN Transformers. Same setting as Fig. 4. Despite some shift, the optimal HPs are\nroughly stable when transferring from batch size 32, sequence length 128, and 5000 training steps.\n39\n\n17.5\n15.0\n12.5\n10.0\n7.5\nlog2LearningRate\n2\n4\n6\n8\n10\n12\nTraining Loss\nTransformer on IWSLT14 De-En\n(Standard Parametrization)\n64\n128\n256\n512\n1024\n2048\nFigure 20: Learning rate landscape is highly unstable under standard parametrization in IWSLT.\nThere are two common approaches to setting the base model shape: 1) If one intends to tune a large\ntarget model, then the user can set the base model shape to be the shape of the target model (e.g.\nBERT-large or T5-large), so that the target model itself is in standard parametrization. Then one\ncan tune a proxy model with e.g. width = 124 to obtain the optimal HPs for the target model. In\naddition, if one wishes to scale up further e.g. width = 1024, then these HPs remain optimal. 2)\nIf one has done exploration on a new idea with a small model and now wishes to scale up, reusing\nthe HP found during this exploration, then one can set the base model shape to be the shape of the\nexploratory small model. Of course, in both scenarios, depth, batch size, and sequence lengths can\nbe scaled up and down as well according to Fig. 19 (though note that currently we require users to\nrecreate the base model shape at new depths, since the number of parameters now change with depth).\nThe mup Package\nWe provide our tool as a Python package called mup designed to work with\nPyTorch. The following example illustrates the usage of our package.\n40\n\nWhat Happens in the mup Package\nUnder the hood, mup implements the µP formulation in\nTable 8. By invoking set_base_shape(model, base_model), each parameter tensor p of model\ngets a p.infshape attribute that stores, for each of its dimensions, the corresponding base dimension\nand whether that dimension should be considered “inﬁnite” (i.e. will be scaled up/down, e.g.,\ndmodel of a Transformer) or “ﬁnite” (i.e. will be ﬁxed, e.g., vocabulary size). This information\nis used in the initializers and optimizers to automatically scale the parameters or learning rates\nto be compliant with µP. For example, by Table 8, the Adam learning rate of hidden weights p\nis calculated as η/p.infshape.width_mult(), where p.infshape.width_mult() essentially\ncalculates\nfan_in\nbase_fan_in.\nI\nReverse-µTransfer for Diagnosing Training Instability in Large Models\nLarge Transformers are famously ﬁckle to train [25, 37]. We note that a possible source of this\ninstability for larger transformers is the failure of naive hyperparameter transfer via the standard\nparametrization. This is certainly consistent with Fig. 1, which shows that the optimal learning\nrate for small Transformers can lead to trivial performance in large Transformers. We support this\nhypothesis further by reverse-µTransferring the instability-inducing HPs from a large Transformer to\na small one and replicating the training instability. This is shown in Fig. 21.\nPractically, this reverse-µTransfer technique can be used to diagnose or debug training instability\nproblems of large models. We offer two case studies toward this claim.\n1) When training transformers of width 8192 on Wikitext-2, we found certain HP combinations\ncaused divergence in the middle of training. We reverse-µTransferred one such HP combination to\na model of width 256 and replicated this divergence. By analyzing this small model’s activations\nright before this divergence, we found that the cause is due to attention logits blowing up. Note this\n41\n\ndebugging session proceeded much more quickly than if we directly worked with the large model.\nLater we conﬁrmed this is indeed the same cause of the width-8192 model’s divergence.\n2) A 6B-parameter language model (in standard parametrization) in a separate project experienced\nrepeated blow-up in the middle of training. We reverse-µTransferred its hyperparameters to a smaller,\n100M-parameter model and replicated the training instability. This was solved by a retuning of the\nsmall model via random search.\n20\n18\n16\n14\n12\n10\n8\nlog2LearningRate\n4.0\n4.5\n5.0\n5.5\n6.0\n6.5\n7.0\n7.5\nTraining Loss\ntraining instability\nFix Hparam., Change Width\nActual Width\n256\n512\n1024\n2048\n4096\n8192\n20\n18\n16\n14\n12\n10\n8\nlog2LearningRate\n4.5\n5.0\n5.5\n6.0\n6.5\n7.0\n7.5\nTraining Loss\ntraining instability\nFix Width, Change Hparam.\nSimulated\n    Width\n256\n512\n1024\n2048\n4096\n8192\nFigure 21: Replicating training instability on a small Transformer by reverse-µTransferring\nhyperparameters. These experiments concern 2-layer Transformers in Standard Parametrization\n(SP) on Wikitext-2, trained with Adam, where width is deﬁned as dmodel = dffn. (Left) LR-vs-\nloss for wider and wider Transformers. (Right) Likewise for simulated width: Here each point\n(log2 η, loss) for simulated width n indicates the loss from training a width-256 µP Transformer\nwith base width n and LR η (i.e. loosely speaking, it’s using LR transferred from η in a width-n SP\nTransformer). Takeaway: The overall shapes of the curves are identical between the left and right\nplots24; in particular, a learning rate leads to instability in a wide model iff it does so when transferred\nback to a narrow model.\nJ\nAn Intuitive Introduction to the Theory of Maximal Update\nParametrization\nIn what follows, we seek to describe useful intuitions and rules of thumb that would be helpful\nto practitioners and empirical researchers alike in ﬁguring out what is the right neural network\nparametrization. The intuitions we shall describe regarding SGD can be made rigorous as in [56, 57];\nthose regarding Adam are new, and their formalization will be done in an upcoming paper.\nFirst, we write down the most basic intuition regarding sums of many random elements, which will\nunderlie all of the calculations that follow.\nLaw of Large Numbers (LLN)\nIf x1, . . . , xn, . . . “look like” random independent samples of a\nrandom variable X, then\n1\nn\nn\nX\ni=1\nxi →E[X],\nas n →∞.\nCentral Limit Theorem (CLT)\nIn the same scenario as above,\n1\n√n\nn\nX\ni=1\n(xi −E[X]) →N(0, σ(X)),\nas n →∞,\nwhere σ(X) is the standard deviation of the random variable X.\nOf course, there are many subtleties one must resolve to make the statements above truly rigorous\n(e.g., what is the meaning of “look like”?), but as rules of thumb, they typically give the correct\nprediction.\n24 Note that the curves on the left are “lower” than curves on the right. This just reﬂects the increasing capacity\nof wider models able to ﬁt the training data better, so is orthogonal to our point.\n42\n\nTable 14: Expected entry size of Av for different matrices A and vector v correlated with each other,\nboth having entries of size Θ(1).\nStandard Gaussian\n(Nonlinear) Tensor Product\nVector\nA ∈Rn×n\nA ∈Rn×n\nA ∈R1×n\nEntry size of Av\nΘ(√n)\nΘ(n)\nΘ(n)\nIn particular, here we want to note the following basic intuition regarding the size of a sum of xi:\nwhen n is large,\nn\nX\ni=1\nxi has typical size\n\u001aΘ(n)\nif E[X] ̸= 0\nΘ(√n)\notherwise\nHere, “typical size” can be taken to mean the size 99% of time. Again, we stress that this is a good\nrule of thumb that yields the correct prediction in the cases we are concerned with here; the rigorous\nversions of this will come from the Tensor Programs framework (e.g., [56]).\nJ.1\nBehaviors of Gaussian Matrices vs Tensor Product Matrices\nCentral to the derivation of µP for any architecture are key insights on the behaviors of two kinds of\nrandom matrices: 1) iid Gaussian random matrix and 2) tensor product matrix (by which we mean a\nsum of outer products) and more generally what we call nonlinear tensor product matrix (see Eq. (7)).\nFor example, a neural network, randomly initialized in the typical way, will have each weight matrix\nlook like the former. However, every step of training by gradient descent adds a sum of outer products\nto this initial matrix, so that the change in weights constitute a tensor product matrix. For Adam,\nthe change in weights is not a tensor product but a more general nonlinear tensor product matrix\n(see Eq. (7)). In this section, we will particularly focus on the right scaling for the entries of such\nmatrices, leading to a discussion of the right neural network parametrization in the next section. We\nconcentrate on the key heuristics but eschew burdensome rigor.\nKey Insights\nConsider a random vector v ∈Rn with approximately iid entries and a random\nmatrix A of either size n × n or 1 × n, both having entries of size Θ(1).25 In the context of deep\nlearning, v for example can be an activation vector in an MLP, a Gaussian A the hidden weights at\ninitialization, a (nonlinear) tensor product A the change in hidden weights due to training, and a\nvector A the readout layer weights. Then Av corresponds to a part of the next layer preactivation\nor the network output. To make sure the preactivations and the output don’t blow up, we thus need\nto understand the scale of Av, especially in the general case where A is correlated with v.26 This\nis summarized in Table 14, with the derivations below. Intuitively, a (nonlinear) tensor product or\nvector A will interact with a correlated v via Law of Large Numbers, hence the n-scaling, while a\nGaussian A interacts with v via Central Limit Theorem, hence the √n-scaling.\nIn the derivations below, we answer a slightly different but equivalent question of “how to scale A\nsuch that Av has entry size Θ(1)?”\nJ.1.1\nPreparation for the Derivations\nBy the results of [57], each (pre-)activation vector and its gradient vector in a multi-layer perceptron,\nat any time during training, have approximately iid coordinates in the large width limit,27 and\nsomething similar can be said for more advanced networks such as ResNet and Transformers 28.\nDeﬁnition J.1. We say any such vector v ∈Rn has Θ(na)-sized coordinates, or just Θ(na)-\ncoordinates for short, if ∥v∥2/n = Θ(n2a) as n →∞. Because, by the above discussion, the\ncoordinates are roughly iid when n is large, this intuitively means that each entry of v has “typical\nsize” Θ(na). We make similar deﬁnitions with Θ replaced by O and Ω.\n25in the sense that the the variance of the entries are Θ(1)\n26Here “correlated” formally means v depends on W ⊤in a Tensor Program. This essentially captures all\nscenarios of “v correlated with W” that occurs in deep learning.\n27Our intuition here is derived from the assumption that width is much larger than training time; of course, as\nillustrated by our myriad experiments, these intuition are very useful even when this is not the case, such as\nwhen training to convergence.\n28E.g. in a convnet, the (pre-)activations are iid across channels, but correlated across pixels\n43\n\nFurthermore, to each such vector v with Θ(1)-sized coordinates, we can associate a random variable\nZv, independent of n, that represents the coordinate distribution of v, in such a way that: If vector u\nis correlated with v, then Zu will also be correlated with Zv, and limn→∞v⊤u/n = E ZuZv.\nJ.1.2\nLinear Tensor Product Matrix (e.g. SGD Updates)\nThe case of (linear) tensor product matrix can be reduced to the outer product case by linearity. Given\nu, v, x ∈Rn having approximately iid coordinates (of size Θ(1)) like discussed above, we can form\nthe outer product\nA\ndef\n= u ⊗v/n = uv⊤/n,\n(6)\nwhich is the form of a single (batch size 1) gradient update to a weight matrix. Then, by Law of\nLarge Numbers,\nAx = uv⊤x\nn\n≈cu,\nwhere\nc = E ZvZx.\nSo Ax also has approximately iid coordinates, distributed like ZAx def\n= Zu E ZvZx. Likewise, if A is\na sum of outer products A = Pk\ni=1 ui ⊗vi/n, then\nAx =\nk\nX\ni=1\nui vi⊤x\nn\n,\nwith coordinates distributed as\nZAx =\nk\nX\ni=1\nZui E ZviZx.\nNotice that each coordinate of A has size Θ(1/n). The above reasoning shows that, in order for Ax\nto have coordinate size Θ(1) (assuming x does), then Θ(1/n) is the right coordinate size for A, in\nthe general case that vi and x are correlated (as is generically the case during gradient descent, with\nA = ∆W for some weights W and x being the previous activations).29\nJ.1.3\nNonlinear Tensor Product Matrix (e.g. Adam Updates)\nWhen using Adam or another adaptive optimizer that normalizes the gradient coordinatewise before\napplying them, we need to modify our argument slightly to obtain the right coordinate size scaling of\nthe matrix. The gradient update A, after such normalization, will take the form of\nAαβ = ψ(u1\nα, . . . , uk\nα, v1\nβ, . . . , vk\nβ),\nfor some ψ : R2k →R and vectors ui, vj∈Rn.\n(7)\nWe say a matrix of this form is a nonlinear tensor product matrix.\nFirst, note the tensor product matrices (e.g. the form of SGD update) discussed previously (Eq. (6))\nalready takes this form, with ψ(u1\nα, . . . , uk\nα, v1\nβ, . . . , vk\nβ) = n−1(u1\nαv1\nβ + · · · + uk\nαvk\nβ), so Eq. (7)\nis a strict generalization of linear tensor products. Next, for the example of Adam, each gradient\nupdate is µ/σ where µ (resp. σ2) is the moving average of previous (unnormalized) gradients (resp.\nthe coordinatewise square of the same).30 If these unnormalized gradients are the outer products\nu1 ⊗v1, . . . , uk ⊗vk, then the update has coordinates\n(µ/σ)αβ = ψ(u1\nα, . . . , uk\nα, v1\nβ, . . . , vk\nβ)\ndef\n=\nX\ni\nγiui\nαvi\nβ/\nsX\ni\nωi(uiαvi\nβ)2,\n(8)\nwhere γi and ωi are the weights involved in the moving averages.\nNow suppose we have some A ∈Rn×n of the form Eq. (7), where ui, vi ∈Rn have approximately\niid coordinates (of size Θ(1)), and ψ = n−1 ¯ψ where ¯ψ doesn’t depend on n (in terms of Adam where\n¯ψ corresponds to the ψ of Eq. (8), this corresponds to using a learning rate of 1/n). Then for x ∈Rn\nhaving approximately iid coordinates of size Θ(1), by Law of Large Numbers,\n(Ax)α = 1\nn\nn\nX\nβ=1\n¯ψ(u1\nα, . . . , uk\nα, v1\nβ, . . . , vk\nβ)xβ ≈E ¯ψ(u1\nα, . . . , uk\nα, Zv1, . . . , Zvk)Zxdef\n= Ψ(u1\nα, . . . , uk\nα).\n29In some corner cases when x is uncorrelated with v, then v⊤x = Θ(√n) by Central Limit, so actually\nAx has Θ(1/√n)-coordinates. However, this case does not come up much in the context of training neural\nnetworks.\n30Adam also has bias correction for the moving averages which can be accomodated easily, but for simplicity\nwe omit them here.\n44\n\nHere we made the obvious deﬁnition\nΨ : Rk →R,\nΨ(r1, . . . , rk)\ndef\n= E ¯ψ(r1, . . . , rk, Zv1, . . . , Zvk)Zx.\nThus Ax also has approximately iid coordinates (of size Θ(1)),\nZAx def\n= Ψ(Zu1, . . . , Zuk).\nFor example, in the SGD example with A = u ⊗v/n and ¯ψ(uα, vβ) = uαvβ, this formula gives\nZAx = Ψ(Zu) where Ψ(z) = z E ZvZx, recovering the earlier derivation.\nIn any case, the point here is that A has coordinate size Θ(1/n), and this is the unique scaling that\nleads to Ax having coordinate size Θ(1).\nJ.1.4\nVector Case (e.g. Readout Layer)\nThe vector A case is similar to the tensor product cases above.\nJ.1.5\nGaussian Matrix (e.g. Hidden Weights Initialization)\nNow consider the case where A ∈Rn×n is random Gaussian matrix with Aαβ ∼N(0, 1/n) and\nx ∈Rn has approximately iid coordinates distributed like Zx. In the context of neural network\ntraining, A should be thought of as a randomly initialized weight matrix, and x for example can be\ntaken to be an activation vector in the ﬁrst forward pass.\nA Quick Intuition\nBy standard random matrix theory, A has Θ(1) operator norm with high\nprobability. Thus, with high probability, for any “typical” vector x, we expect ∥Ax∥= Θ(∥x∥), even\nif x is correlated with A. If Ax’s coordinates are “evenly distributed”, then this would imply Ax has\nΘ(1)-coordinates if x does. However, this is not so clear. Below we provide intuitions for why this\nwould be the case.\nIntuition for Evenness of Coordinate Distribution\nIf x is independent from A (or sufﬁciently\nuncorrelated), then each coordinate (Ax)α has variance E(Zx)2 = Θ(1) (so by deﬁnition has size\nΘ(1)). Thus, here A having Θ(1/√n)-coordinates leads to Ax having Θ(1)-coordinates, in contrast\nto the tensor product case above.\nWhen x is correlated with A, it turns out the same scaling applies (Θ(1/√n) is the unique scaling for\nA’s entries such so that Ax has Θ(1) entries), but the reasoning is much more subtle: In the context\nof neural network training, it turns out all scenario where x is correlated with A can be reduced\nto the case where x = φ(A⊤y, . . .) for some coordinatewise nonlinearity φ and some other vector\nRn.31 Let’s consider a very simple example with x = A⊤1 for the all 1s vector 1 ∈Rn (which has\ncoordinate size Θ(1) as can be checked easily). Then, for each index α ∈[n], we can calculate\n(AA⊤1)α =\nX\nβ,γ\nAαβAγβ =\nX\nβ\nA2\nαβ +\nX\nβ\nX\nγ̸=α\nAαβAγβ.\nSince E A2\nαβ = 1/n, by the Law of Large Number, the ﬁrst sum P\nβ A2\nαβ ≈1. On the other hand,\nthere are n summands of the form P\nγ̸=α AαβAγβ, all iid with variance n−1\nn2 = Θ(1/n). Thus by\nthe Central Limit Theorem, we expect P\nβ\nP\nγ̸=α AαβAγβ ≈N(0, 1). Therefore, each coordinate\nof (AA⊤1)α looks like 1 + N(0, 1) = N(1, 1) and thus has size Θ(1); again this is caused by A\nhaving Θ(1/√n)-coordinates.\nThis example can be generalized to more general x that is correlated with A, but the mathematics is\nquite involved. See [56] for more details.\nJ.2\nDeriving µP for Any Architecture\nArmed with the insight from the last section, we now outline the key steps to derive µP in Table 3 for\nany architecture. In practice, µP implies the following desiderata\nDesiderata J.1. At any time during training\n1. Every (pre)activation vector in a network should have Θ(1)-sized coordinates32\n31This is because every “reasonable” deep learning computation can be expressed in a Tensor Program.\n32In a convnet, a (pre-)activation vector corresponds to a single pixel across all channels; in general , we\nexpect (pre-)activations are iid across channels, but correlated across pixels\n45\n\n2. Neural network output should be O(1).\n3. All parameters should be updated as much as possible (in terms of scaling in width) without\nleading to divergence.\nLet’s brieﬂy justify these desiderata. For the desideratum 1, if the coordinates are ω(1) or o(1),\nthen for sufﬁciently wide networks their values will go out of ﬂoating point range. This problem is\nparticularly acute for low-precision formats that are essential for training large models such as BERT\nor GPT. Moreover, a general nonlinearity is only well-behaved if its input is in a ﬁxed range (although\nthis is not a problem for homogeneous nonlinearities like relu). For example, for tanh nonlinearity, if\nthe preactivation is vanishing o(1), then tanh is essentially linear; if the preactivation is exploding\nω(1), then the tanh gradient vanishes.\nFor the desideratum 2, a similar justiﬁcation applies to the numerical ﬁdelity of the loss function and\nloss derivative. Note that, with desideratum 3, this means the network output should be Θ(1) after\ntraining (but it can go to zero at initialization).\nFinally, desideratum 3 means that 1) we are doing “maximal feature learning” [57] and 2) every\nparameter contribute meaningfully in the inﬁnite-width limit. This ensures that learning rate “plays\nthe same role” in the ﬁnite-width case as in the inﬁnite-width limit. For example, it prevents the\nscenario where a weight matrix gets stuck at initialization in the limit for any learning rate (so\nlearning rate does not matter) but evolves nontrivially in any ﬁnite-width network (so learning rate\ndoes matter).\nThese desiderata will essentially uniquely single out µP. More formally, µP is the unique parametriza-\ntion that admits feature learning in all parameters of the neural network [57], and this property\ntheoretically guarantees HP transfer across width (for sufﬁciently large width). However, for the sake\nof reaching a broader audience, we will focus more on the intuitive derivations from the desiderata\nrather than on this formal aspect.\nBelow, we ﬁrst assume for simplicity that the width of every layer is n, and we focus only on dense\nweights. Later, we will discuss convolutions and varying the widths between layers.\nJ.2.1\nµP Derivation From the Desiderata\nBelow, we will derive the µP formulation in Table 3. Tables 8 and 9 can be derived from Table 3 via\nthe following equivalences, which can be easily derived via some simple calculations.\nLemma J.1. Let ft(ξ) denote the neural network function after t steps of training (using any ﬁxed\nsequence of batches), evaluated on input ξ. Consider a parameter tensor W with learning rate C,\ninitialized as W ∼N(0, B2), and with a multiplier A. Then for any θ > 0, ft(ξ) stays ﬁxed for all t\nand ξ if we set\n• when the optimizer is SGD\nA ←Aθ, B ←B/θ, C ←C/θ2\n• when the optimizer is Adam,\nA ←Aθ, B ←B/θ, C ←C/θ;\nFor example, for output weights, Table 3 has A = 1, B = 1/fan_in, C = η/fan_in for SGD and\nAdam. Then taking θ = 1/fan_in, we get the entries in Table 8, with A = 1/fan_in, B = 1,\nC = η · fan_in for SGD and C = η for Adam. Taking θ = 1/√fan_in instead, we get the entries in\nTable 9, with A = 1/√fan_in, B = 1/fan_in, C = η for SGD and η/√fan_in for Adam. Similar\ncalculations hold for the input weights scaling in those tables, after taking into consideration that\nfan_in is considered a constant in terms of width for the input layer.\nWe proceed with the derivation of Table 3 below. Recall the deﬁnitions of Θ(na)-sized coordinates\nor Θ(na)-coordinates from Deﬁnition J.1.\nOutput Weights\nSuppose W ∈R1×n is an output weight. By desideratum 1, the input x to W\nhas Θ(1)-sized coordinates. Thus W should have Θ(1/n)-coordinates so that |Wx| = O(1). We\ncan initialize W with Θ(1/n)-coordinates and scale its (per-layer) LR so that ∆W has Θ(1/n)-\ncoordinates as well. This means initializing Wαβ ∼N(0, Θ(1/n2)) and use Θ(1/n) learning rate\nfor both SGD and Adam.\n46\n\nHidden Weights\nConsider a square weight matrix W ∈Rn×n. Desiderata 1 guarantees that the\ninput x to W has Θ(1)-sized coordinates. Generally, x will be correlated with W. By Table 14, we\ncan immediately derive\nInitialization W should be randomly initialized with coordinate size Θ(1/√n)\nLR The learning rate should be scaled so that ∆W has coordinate size Θ(1/n)\nso that (W0 + ∆W)x is Θ(1) if x is, inductively satisfying desideratum 1. With Adam, this just\nmeans the per-layer LR is Θ(1/n). With SGD and the scaling of output layers above, we can calculate\nthat the gradient of W has Θ(1/n)-coordinates, so the Θ(1) SGD LR derived above sufﬁces as well.\nInput Weights\nSuppose W ∈Rn×d is an input weight. To satisfy desideratum 1 (i.e. for any\ninput ξ, Wξ should have Θ(1)-coordinates), we want W to have Θ(1)-coordinates. We can initialize\nW with Θ(1)-coordinates and scale its (per-layer) LR so that ∆W has Θ(1)-coordinates as well.\nThis implies initialization variance of Θ(1) (or Θ(1/fan_in) since fan_in = Θ(1) here) and Adam\nlearning rate Θ(n). As above, we can calculate that the gradient of W has Θ(1/n)-coordinates, so\nwe want SGD learning rate Θ(n).\nBiases\nBiases follow the same reasoning as input weights (just think of it as an input weight with\ninput 1).\nAttention\nSuppose the key dimension dk is tending to inﬁnity with width with number of heads\nnhead ﬁxed. Then the key-query contraction q⊤k ∈R scales like Θ(dk) by Law of Large Numbers\n(instead of Central Limit Theorem because q and k are generally correlated) and desideratum 1, hence\nthe 1/dk we propose rather than 1/√dk.\nNow suppose instead that nhead tends to inﬁnity with width with dk ﬁxed.\nLet K, Q ∈\nRN×dk×nhead, V ∈RN×dv×nhead be keys, queries, and values across all heads and tokens. Think-\ning of N × dk as constants, we may view attention as a nonlinearity coordinatewise in the nhead\ndimension. Then it’s clear that our parametrization described above already works.\nFinally, we may freely let dk and nhead both tend to inﬁnity, and the above reasoning shows that our\nparametrization still works.\nChanging Width Ratios\nAs noted above, at any time in training, every (pre-)activation vector will\nhave approximately iid coordinates (of order Θ(1) by desideratum 1). Another desideratum for µP is\nto ensure that this coordinate distribution (at any particular time) stays roughly invariant as widths\nincreases. When all layer widths are tied, this is automatic if the other desiderata are satisﬁed, hence\nwhy we did not list this above.\nWhen width ratios vary, this is not automatic. In this case, we need to choose whether to replace each\nn with fan-in or fan-out (or some function of them). Making the wrong choices will let the coordinate\ndistributions vary with width ratios.\nObviously, we should replace n with fan-in for the output layers and with fan-out for the input layers\nsince they are the only dimension scaling with n. For the hidden weights, we replace n with fan-in\nso that the forward pass is preserved. When using Adam (and assuming the initialization of W is\nquickly dominated by the change in W), this ensures that the (pre-)activation coordinate distributions\nare preserved at any time during training even if we vary widths in different layers differently. (For\nSGD this doesn’t quite work in general because the varying width ratios change the gradient sizes of\ndifferent layers differently, whereas Adam always normalizes the gradient coordinatewise).\nConvolution\nA convolution weight tensor W ∈Rfan_out×fan_in×s1×s2 with kernel size s1 × s2\ncan be thought of just as a s1s2 = Θ(1)-sized collection of fan_out × fan_in dense weights. Then\nall of our discussions above apply accordingly.\nJ.3\nWhy Other Parametrizations Cannot Admit Hyperparameter Transfer\nStandard Parametrization (SP)\nSP doesn’t work essentially because it leads to blow-up in the\ninﬁnite-width limit.\n1. For Adam with LR Θ(1), ∆W would have Θ(1)-coordinates, causing preactivations to blow\nup like Θ(n) by Desideratum 1 and Table 14. We can avoid this blowup with LR Θ(1/n),\n47\n\nbut this induces a non-maximal feature learning limit, which, as we argue below, cannot\ntransfer hyperparameters in all situations.\n2. For SGD, the gradient of Rn×n weight has Θ(1/√n)-coordinates, so Θ(1) learning rate\nwould make preactivation scale like Θ(√n) and hence blow up. If we use Θ(1/width)\nlearning rate, then blow-up does not occur. However, this inﬁnite-width limit is in the kernel\nregime [57] and thus does not allow HP transfer for the same reason that NTP below does\nnot.\nNeural Tangent Parametrization (NTP)\nWe have concrete examples, e.g. Word2Vec in [57],\nwhere the NTK limit has trivial performance — so HPs have no effect at all — vastly outperformed\nby ﬁnite-width networks — where HPs matter. More importantly, wider does not always do better in\nNTP, especially in tasks where feature learning is crucial [57, 61]. So in the context of modern deep\nlearning e.g. large language model pretraining, NTP (or SP with Θ(1/width) LR) does not make\nsense for wide neural networks.\nOther Parametrizations\nRecall the Dynamical Dichotomy Theorem proven in [57], which says\nthat any nontrivial stable “natural parametrization” (formally, “abc-parametrization,” [57]) either\nadmits a feature learning limit or a kernel limit, but not both.\nOur argument above against SP and NTP will also work against any parametrization inducing a kernel\nlimit. Therefore, it remains to ask, can other feature learning parametrizations transfer HPs?\nWe argue no. As shown in [57], any other feature learning parametrization differs from µP essentially\nonly in that some parameters are not updated maximally. By [57, Sec 6.4], in the inﬁnite-width limit,\nsuch parameters can be thought of as being ﬁxed at initialization. Therefore, in such inﬁnite-width\nlimits, the learning rate of such parameters becomes useless. As such, we cannot hope for the HP\nlandscape of the limit to reﬂect the HP landscape of ﬁnite-width neural networks.\nµP is the unique feature learning parametrization that updates all parameters maximally, so that the\nlearning rate of each parameter plays approximately the same role in ﬁnite-width neural networks\nas in the inﬁnite-width limit. Consequently, the HP landscape of the µP limit should reﬂect the HP\nlandscape of ﬁnite-width neural networks.\n48\n",
      "fetch_method": "direct-pdf"
    },
    {
      "arxiv_id": "https://arxiv.org/pdf/2502.02732",
      "full_text": "arXiv:2502.02732v3  [cs.LG]  6 Jun 2025\nPeri-LN: Revisiting Normalization Layer in the Transformer Architecture\nJeonghoon Kim 1 2 Byeongchan Lee 2 Cheonbok Park 1 2 Yeontaek Oh 1 Beomjun Kim 2 Taehwan Yoo 1\nSeongjin Shin 1 Dongyoon Han 3 Jinwoo Shin\n† 2 Kang Min Yoo\n† 1\nAbstract\nSelecting a layer normalization (LN) strategy\nthat stabilizes training and speeds convergence in\nTransformers remains difficult, even for today’s\nlarge language models (LLM). We present a com-\nprehensive analytical foundation for understand-\ning how different LN strategies influence train-\ning dynamics in large-scale Transformers. Until\nrecently, Pre-LN and Post-LN have long domi-\nnated practices despite their limitations in large-\nscale training. However, several open-source mod-\nels have recently begun silently adopting a third\nstrategy without much explanation. This strategy\nplaces normalization layer peripherally around\nsublayers, a design we term Peri-LN. While Peri-\nLN has demonstrated promising performance, its\nprecise mechanisms and benefits remain almost\nunexplored. Our in-depth analysis delineates the\ndistinct behaviors of LN strategies, showing how\neach placement shapes activation variance and\ngradient propagation. To validate our theoret-\nical insight, we conduct extensive experiments\non Transformers up to 3.2B parameters, showing\nthat Peri-LN consistently achieves more balanced\nvariance growth, steadier gradient flow, and con-\nvergence stability. Our results suggest that Peri-\nLN warrants broader consideration for large-scale\nTransformer architectures, providing renewed in-\nsights into the optimal placement of LN.\n1. Introduction\nBuilding on a rapidly expanding lineage of Transformer-\nbased large language models, open-source models have\n†Equal correspondence. 1NAVER Cloud 2Korea Advanced Insti-\ntute of Science and Technology (KAIST) 3NAVER AI Lab. Corre-\nspondence to: Jeonghoon Kim <jeonghoon.samuel@gmail.com>,\nJinwoo Shin <jinwoos@kaist.ac.kr>, Kang Min Yoo <kang-\nmin.yoo@navercorp.com>.\nProceedings of the 42 nd International Conference on Machine\nLearning, Vancouver, Canada. PMLR 267, 2025. Copyright 2025\nby the author(s).\nshown remarkable impact (Hoffmann et al., 2022; Guo et al.,\n2025; Yoo et al., 2024). As the demand for larger and more\npowerful models grows, various training stabilization tech-\nniques have been introduced (Yang et al., 2022; Zhai et al.,\n2023; Loshchilov et al., 2024). Among these, the choice\nof where and how to apply layer normalization (LN: Lay-\nerNorm or RMSNorm; Ba et al., 2016; Zhang & Sennrich,\n2019) critically influences model convergence (Xiong et al.,\n2020; Kedia et al., 2024; Wortsman et al., 2024). However,\ntheir immense computational requirements have restricted\ndeeper exploration of the underlying Transformer structure.\nAre we truly employing the optimal LN placement? In\npractice, fully revealing the results of massive resource in-\nvestments can be challenging (Rivi`ere et al., 2024). Despite\nits importance, there is still no consensus on a single best\nLN placement strategy.\nTwo prominent LN placements have been widely explored.\nPost-LN (Vaswani et al., 2017) normalizes the hidden state\nafter adding the sub-layer output to the residual stream (that\nis, Norm(x + Module(x)) where x is input hidden state.\nNorm is LN). This helps constrain the variance of hidden\nstates but may inadvertently weaken gradient signals, partic-\nularly in deeper models (Kedia et al., 2024). Pre-LN (Dubey\net al., 2024), by contrast, normalizes before passing the hid-\nden state to the sub-layer (that is, x + Module(Norm(x))).\nWhile this can enhance gradient propagation, it also admits\nso-called “massive activations,” where hidden states grow\nexponentially across layers (Sun et al., 2024).\nPrevious studies on deep convolutional neural networks\n(CNNs) have analyzed the impact of batch normalization on\nvariance changes during the initialization stage of ResNet\narchitectures, demonstrating its relationship to model per-\nformance (De & Smith, 2020). They noted that, in models\nwithout normalization, hidden activation growth at initial-\nization can be exponential, leading to poor performance\nand stability. In contrast, in pre-normalized CNNs, the vari-\nance of hidden activations was shown to increase linearly\nas model depth grows. In the same vein, Kedia et al. (2024)\nreported that, for Transformer architectures as well, the\nvariance in the forward propagation of Transformer-based\nlanguage models at initialization increases linearly with\ndepth. However, in the context of Transformer architectures,\nwe observed that this variance growth at initialization does\n1\n\nPeri-LN: Revisiting Normalization Layer in the Transformer Architecture\nFigure 1. Illustration of hidden-state variance across different\nmodel depths and training iterations. From initialization through\ntraining on 6.3 billion tokens, we observe the growth in hidden-\nstate variance for both Pre-LN and Post-LN architectures. The\nanalysis is based on a 1.5B-parameter model. Detailed settings\nand additional results are provided in Section 5.1.\nnot persist as training progresses as shown in Figure 1. Sec-\ntions 3, 4, 5, and 6 provide a more detailed discussion of\nthese hidden-state growth patterns.\nBeyond these two common strategies, Post-LN and Pre-LN,\na third LN placement has quietly emerged in large-scale\nopen-source models: applying LN around the sub-layer, i.e.,\non both its input and output. Although recent open-source\nmodels (Team et al., 2025; Rivi`ere et al., 2024; OLMo et al.,\n2024) have quietly adopted such designs and demonstrated\npromising performance on a large scale, these efforts often\nappeared isolated, lacking a conceptual unifying framework\nor a thorough investigation into their benefits. In this pa-\nper, we coin the term Peri-LN1 to unify these scattered\napproaches and highlight an underexplored avenue for sta-\nbilizing large-scale Transformer training. By dissecting the\nforward- and backward-pass dynamics of each LN strategy,\nwe clarify how, when, and why they differ, interpreting these\ndistinctions through the lens of training stability.\nAccordingly, this paper revisits LN placement in Trans-\nformers from both analytical and empirical perspectives. In\nparticular, we:\n1. Present an in-depth analysis of Post-LN and Pre-LN in\nlarge-scale Transformers, examining how variance and\ngradient properties evolve beyond initialization.\n2. Investigate Peri-LN to understand how normalizing\nboth the inputs and outputs of each module moderates\nhidden-state behavior during forward and backward\npropagation, providing a systematic perspective on this\nunderexplored alternative.\n3. Provide quantitative evidence on how large activation\ninfluences training stability, benchmark performance,\nand model behaviors.\n1Peri- (Prefix) means “around,” reflecting that LN encapsulates\nthe entire sub-layer. (e.g. peripherally)\n2. Background and Motivation\nThe analysis of activation variance at model initialization\nhas long been central to understanding normalization layers\nand enhancing stability in convolutional neural networks\n(CNNs) (De & Smith, 2020; He et al., 2016; Brock et al.,\n2021a). Specifically, De & Smith (2020) showed that batch\nnormalization in residual blocks can bias networks toward\nthe identity function, thereby stabilizing gradients and im-\nproving overall training dynamics.\nSimilar investigations have emerged for Transformer ar-\nchitectures, examining how variance propagates and how\ngradients behave in both post-layer normalization (Post-LN)\nand pre-layer normalization (Pre-LN) configurations (Xiong\net al., 2020; Kedia et al., 2024; Wortsman et al., 2024). Early\nwork comparing Post- and Pre-LN primarily focused only\non the initialization stage. Xiong et al. (2020) observed that\nPre-LN architectures tend to exhibit more stable gradients,\nbut can still encounter issues such as gradient spikes and\ndivergence, especially in deeper models or large-scale pre-\ntraining scenarios (Zhai et al., 2023; Wortsman et al., 2024;\nFishman et al., 2024; Chung et al., 2024).\nAmong these challenges, the phenomenon of “massive acti-\nvations” has attracted attention (Dettmers et al., 2022; Yu\net al., 2024; Fishman et al., 2024). Notably, Sun et al. (2024)\nidentified that in Pre-LN architectures, large spikes in acti-\nvation magnitude can persist across layers due to residual\nconnections. These massive activations act as fixed biases,\npotentially narrowing the model’s focus to certain tokens\nand may influence generalization. However, the underlying\nmechanisms behind these large values, and their exact im-\npact on the training process, remain not yet well understood.\nAnalytical work has provided theoretical frameworks to ex-\nplain phenomena like gradient explosion and vanishing in\nTransformers. For instance, Kedia et al. (2024) introduced\na signal propagation theory that details how activation vari-\nance and gradient instability can evolve with depth, identi-\nfying critical factors that impair stability and performance.\nRecent studies have discussed how Pre-LN architectures\ncan allow large values from Attention or MLP modules\nto flow unimpeded through residual connections (Csord´as\net al., 2024; Fishman et al., 2024; Zhai et al., 2023; Worts-\nman et al., 2024), but the precise impact of this behavior on\nlarge-scale training remains insufficiently explored.\nThese observations underscore the ongoing need to clar-\nify how activation dynamics and normalization strategies\ninteract, especially in large-scale training.\nIn response, this work aims to deepen our understanding of\nhow normalization strategies influence Transformer training,\nwith particular attention to the emergence of large activa-\ntions and their implications for stability and performance.\n2\n\nPeri-LN: Revisiting Normalization Layer in the Transformer Architecture\n3. Normalization Strategies\nIn this section, we discuss how different placements of layer\nnormalization (LN 2) in Transformer architecture affect both\ntraining stability and the statistics of hidden states (activa-\ntions 3).\n3.1. Post- & Pre-Normalization in Transformers\nPost-LN.\nThe\nPost-Layer\nNormalization\n(Post-LN)\n(Vaswani et al., 2017) scheme, normalization is applied\nafter summing the module’s output and residual input:\nyl = Norm\n\u0000xl + Module(xl)\n\u0001\n,\n(1)\nwhere xl is the input hidden state of l-th layer, yl is the\noutput hidden state of l-th layer, and Module denotes At-\ntention or Multi-Layer Perceptron (MLP) module in the\nTransformer sub-layer. Norm denotes normalization lay-\ners such as RMSNorm or LayerNorm. It is known that by\nstabilizing the activation variance at a constant scale, Post-\nLN prevents activations from growing. However, several\nevidence (Xiong et al., 2020; Kedia et al., 2024) suggest\nthat Post-LN can degrade gradient flow in deeper networks,\nleading to vanishing gradients and slower convergence.\nPre-LN.\nThe Pre-Layer Normalization (Pre-LN) (Dubey\net al., 2024) scheme, normalization is applied to the mod-\nule’s input before processing:\nyl = xl + Module\n\u0000Norm(xl)\n\u0001\n.\n(2)\nAs for Llama 3 architecture, a final LN is applied to the\nnetwork output. Pre-LN improves gradient flow during\nbackpropagation, stabilizing early training (Xiong et al.,\n2020). Nonetheless, in large-scale Transformers, even Pre-\nLN architectures are not immune to instability during train-\ning (Wortsman et al., 2024; Zhai et al., 2023). As shown\nin Figure 2, unlike Post-LN—which places LN at position\nC—Pre-LN, which places LN only at position A, can lead\nto a “highway” structure that is continuously maintained\nthroughout the entire model if the module produces an out-\nput with a large magnitude. This phenomenon might be\nrelated to the “massive activations” observed in trained mod-\nels (Sun et al., 2024; Fishman et al., 2024).\n3.2. Variance Behavior from Initialization to Training\nAs discussed by Xiong et al. (2020) and Kedia et al. (2024),\nTransformer models at initialization exhibit near-constant\nhidden-state variance under Post-LN and linearly increasing\nvariance under Pre-LN. Most of the previous studies have\nconcentrated on this early-stage behavior. However, Recent\n2Unless stated otherwise, LN refers to both LayerNorm (Ba\net al., 2016) and RMSNorm (Zhang & Sennrich, 2019).\n3We use “hidden state” and “activation” interchangeably.\nA\nB\nC\nPost-LN\n×\n×\n✓\nPre-LN\n✓\n×\n×\nPeri-LN\n✓\n✓\n×\nFigure 2. Placement of normalization in Transformer sub-layer.\nstudies have also reported large output magnitudes in both\nthe pre-trained Attention and MLP modules (Dehghani et al.,\n2023; Wortsman et al., 2024; Fishman et al., 2024). To\nbridge the gap from initialization to the fully trained stage,\nwe extend our empirical observations in Figure 1 beyond\ninitial conditions by tracking how these variance trends\nevolve at intermediate points in training.\nDuring training, we find that Post-LN maintains a roughly\nconstant variance, which helps avert exploding activations.\nHowever, as models grow deeper and training progresses,\nconsistently normalizing xl + Module(xl) can weaken gra-\ndient flow, occasionally causing partial vanishing gradients\nand slower convergence. In contrast, Pre-LN normalizes xl\nbefore the module but leaves the module output unnormal-\nized, allowing hidden-state variance to accumulate exponen-\ntially once parameter updates amplify the input. Although\nPre-LN preserves gradients more effectively in earlier stages,\nthis exponential growth in variance can lead to “massive\nactivations” (Sun et al., 2024), risking numeric overflow and\ndestabilizing large-scale training.\nTakeaways from Pre-LN & Post-LN.\n(1) Keeping the\nHighway Clean: Post-LN’s Potential for Gradient Vanishing\nand Slow Convergence. When layer normalization is placed\ndirectly on the main path (Placement C in Figure 2), it can\ncause gradient vanishing and introduce fluctuations in the\ngradient scale, potentially leading to instability (Xiong et al.,\n2020). (2) Maintaining a Stable Highway: Pre-LN May Not\nSuffice for Training Stability. Pre-LN does not normalize\nthe main path of the hidden states, thereby avoiding the\nissues that Post-LN encounters. Nevertheless, a structural\ncharacteristic of Pre-LN is that any large values arising in\nthe Attention or MLP modules persist through the residual\nidentity path. In particular, as shown in Figure 1, the ex-\nponentially growing magnitude and variance of the hidden\nstates in the forward path may lead to numerical instability.\n3.3. Peri-Normalization in Transformers\nRecent open-source Transformer architectures have placed\nnormalization layers in unconventional placements (Rivi`ere\net al., 2024; Team et al., 2025; OLMo et al., 2024). In\nparticular, these models apply an additional normalization\n3\n\nPeri-LN: Revisiting Normalization Layer in the Transformer Architecture\nlayer at the module output (Output-LN), yet the benefits of\nthis design choice remain unclear. To assess the impact of\nOutput-LN, we analyze the Peri-LN architecture.\nPeri-LN.\nThe Peri-Layer Normalization (Peri-LN) applies\nLN twice within each layer—before and after the module—\nand further normalizes the input and final output embed-\ndings. Formally, for the hidden state xl at layer l:\n1. (Optional) Initial Embedding Normalization:\nyo = Norm(xo),\n2. Input- & Output-Normalization per Layer:\nyl = xl + Norm\n\u0010\nModule\n\u0000Norm(xl)\n\u0001\u0011\n,\n(3)\n3. Final Embedding Normalization:\nyL = Norm(xL),\nwhere xo denotes the output of the embedding layer, the hid-\nden input state. y0 represents the normalized input hidden\nstate. xL denotes the hidden state output by the final layer\nL of the Transformer sub-layer. This design unifies pre- and\noutput-normalization to regulate variance from both ends.\nFor clarity, the locations of normalization layers in the Post-,\nPre-, and Peri-LN architectures are illustrated in Figure 2.\nBoth the latest Gemma (Team et al., 2025; Rivi`ere et al.,\n2024) and OLMo (OLMo et al., 2024) model families,\nwhich apply output layer normalization, adopt the same\nperi-normalization strategy. However, neither line of work\nrigorously examines how this placement constrains variance\nor mitigates large residual activations. Our study extends\nthese open-sourced large-scale models by providing both\ntheoretical and empirical insights into the Peri-LN scheme.\nControlling Variance & Preserving Gradients.\nBy nor-\nmalizing both the input and output of each sub-layer, Peri-\nLN constrains the residual spikes commonly observed in\nPre-LN, while maintaining a stronger gradient pathway\nthan Post-LN. Concretely, if Norm(Module(Norm(xl)))\nexhibits near-constant variance β0, then\nVar(xl+1) ≈Var(xl) + β0,\n(4)\nresulting in linear or sub-exponential growth of activations,\nin contrast to the exponential growth patterns of Pre-LN.\nAlthough Pre-LN and Peri-LN exhibit comparable, roughly\nlinear variance growth at initialization (De & Smith, 2020;\nXie et al., 2023), their trajectories diverge once training\nbegins. The additional normalization layer (Output-LN)\nin Peri-LN preserves the conditions of Eq. 4, enabling the\nmodel’s hidden states to remain better conditioned. By\ncontrast, the rapid surge in variance observed in Pre-LN\ncan trigger instability during the early stages of training, an\neffect we quantify empirically in Sections 5 and 6.\n3.4. Stability Analysis in Normalization Strategies\nXiong et al. (2020) showed that, at initialization, Pre-LN\nexhibits smaller gradient scales at the final layer compared\nto Post-LN, with respect to model depth. In this study, we\nbroaden our analysis beyond initialization to monitor hidden\nstate variance over the full course of training. Building on\nthe earlier observation that the deepest layer exhibits the\nlargest activations, we focus on this surge in the final layer\nunder the Peri-LN strategy to clarify its impact on training\nstability. To this end, we analyze stability by examining\nthe gradient norm with respect to the final layer weights in\nthe presence of massive activation. Formal statements and\ndetailed proofs are presented in Appendix C.\nProposition 3.1 (Informal). Let L(·) be the loss function,\nand let W (2) denote the weight of the last layer of MLP(·).\nLet γ be the scaling parameter in Norm(·), and let D be the\ndimension. Then, the gradient norm for each normalization\nstrategy behaves as follows.\n(1) Pre-LN (exploding gradient). Consider the following\nsequence of operations:\n˜x = Norm(x), a = MLP(˜x), o = x + a,\n(5)\nthen\n\r\r\r\r\r\n∂L(o)\n∂W (2)\ni,j\n\r\r\r\r\r ∝∥hi∥,\n(6)\nwhere h := ReLU\n\u0000˜xW (1) + b(1)\u0001\n. In this case, when\na massive activation ∥h∥occurs, an exploding gradient\n∥∂L/∂W (2)∥can arise, leading to training instability.\n(2) Peri-LN (self-regularizing gradient). Consider the fol-\nlowing sequence of operations:\n˜x = Norm(x), a = MLP(˜x), ˜a = Norm(a), o = x + ˜a,\n(7)\nthen\n\r\r\r\r\r\n∂L(o)\n∂W (2)\ni,j\n\r\r\r\r\r ≤4 γ\n√\nD ∥h∥\n∥a∥\n,\n(8)\nwhere h := ReLU\n\u0000˜xW (1) + b(1)\u0001\n. In this case, even when\na massive activation ∥h∥occurs, Norm(·) introduces a\ndamping factor ∥a∥, which ensures that the gradient norm\n∥∂L/∂W (2)∥remains bounded.\nThe layer-wise amplification documented in §3.2, combined\nwith the bounds in Proposition 3.1, naturally explains the\n4\n\nPeri-LN: Revisiting Normalization Layer in the Transformer Architecture\n(a) Learning rate exploration\n(b) Training loss\n(c) Gradient-norm\nFigure 3. Performance comparison of Post-, Pre-, and Peri-LN Transformers during pre-training. Figure 3(a) llustrates the pre-training\nloss across learning rates. Pre-training loss and gradient norm of best performing 400M size Transformers are in Figure 3(b) & 3(c).\n(a) Divergence at seed 2\n(b) Loss spike at seed 3\n(c) Gradient spikes at seed 5\n(d) Loss spikes at seed 5\nFigure 4. Common case of early stage instability in pre-training. In most of our experiments across different random seeds, the Pre-LN\narchitecture exhibited early-stage instability. Although we initially suspected that a high learning rate might be the root cause, lowering it\ndid not substantially mitigate these issues. By contrast, under the same settings, Peri-LN displayed stable training curves.\ngradient spikes, and occasional divergences that arise in\nPre-LN during large-scale pre-training. We revisit this phe-\nnomenon in §4.3. By contrast, the additional normalization\nin Peri-LN acts as a self-regularizing mechanism that damps\nvariance growth, making the architecture less sensitive to\nlarge activations and therefore more stable in practice. The\nformal analysis for Post-LN is deferred to Appendix B.\n4. Experiments\nIn this section, we provide a comprehensive comparison\nof Post-, Pre-, and Peri-Layer Normalization (LN) across\nlarge-scale Transformer pre-training, instruction-tuning, and\nsubsequent evaluations on the language domain.\n4.1. Experimental Setting\nExcluding the embedding parameters, the model size is set\nto the parameters 400M, 1.5B and 3.2B, respectively. Each\nmodel is trained on 30 billion tokens. To ensure reliable\nvalidation, we pre-train each model with five different train-\ning seeds in all experiments. We perform a exploration of\nthe learning rates, ranging from 1 × 10−4 to 5 × 10−3 to\nidentify the U-shaped pattern for each LN strategy. The\nsequence length is set to 8192, and the weight decay co-\nefficient is fixed at 0.033. We employ Megatron-LM4 to\n4https://github.com/NVIDIA/Megatron-LM\npre-train the Transformers under each LN strategy. We use\nthe DCLM-baseline dataset (Li et al., 2024a), along with the\n“cl100k base” version of the TikToken tokenizer5. Unless\notherwise noted, most training and model configurations\nfollow those of the DCLM experiments(Li et al., 2024a).\nFor normalization layer, we primarily employ RMSNorm.\nFurther details are in Appendix D.\n4.2. Pre-Training Large Language Models\nFigure 3(a) illustrates the pre-training loss across learning\nrates for models ranging in size from 400M to 3.2B parame-\nters. Notably, the Peri-LN architecture consistently achieves\nsuperior loss curves over this entire model size. Since Pre-\nLN shows best performance at learning rate 2×10−3 across\nall model size, we set this to the default learning rate for\nPre-LN and Peri-LN. Unlike Pre-LN, Post-LN’s appropriate\nlearning rate lies in a lower range, so we provide a separate\nsummary in Appendix E.1. In Figures 3(b) and 3(c), we\ncompare the pre-training loss and the gradient norm curve at\neach LN strategy’s best-performing learning rate of 400M\nsize models. The same trend is observed across different\nmodel sizes (§E). In particular, when we sweep over train-\ning seeds and learning rates, Pre-LN frequently exhibits\nspikes in the gradient-norm curve, whereas Peri-LN shows\ncomparatively few, thereby supporting Proposition 3.1.\n5https://github.com/openai/tiktoken\n5\n\nPeri-LN: Revisiting Normalization Layer in the Transformer Architecture\nTable 1. Average benchmark scores (with standard deviations) across 5 different training seeds for Post-, Pre-, and Peri-Layer Normaliza-\ntion language models. Each model size excludes the embedding parameters. Loss denotes the evaluation loss on random samples of the\nC4 dataset. Arch. denotes architecture, and Avg. denotes the averaged benchmark score across tasks. SFT avg. denotes the averaged\nbenchmark score across tasks of instruction fine-tuned models. Diverged checkpoints are excluded from the evaluation score computation.\nSize\nArch.\nARC-Easy\nHellaSwag\nPIQA\nSIQA\nWinogrande\nAvg. ↑\nLoss ↓\nSFT Avg. ↑\nPost-LN\n35.70 ±1.09\n28.91 ±0.16\n62.26 ±0.73\n34.48 ±1.04\n50.88 ±0.75\n42.45\n7.46\n46.44\n400M\nPre-LN\n54.87 ±1.63\n34.17 ±1.66\n68.79 ±1.34\n39.73 ±0.59\n50.88 ±2.35\n49.69\n3.43\n49.96\nPeri-LN\n57.51 ±0.81\n37.46 ±0.34\n69.48 ±0.39\n40.64 ±0.51\n52.74 ±0.67\n51.57\n3.34\n51.96\nPost-LN\n42.92 ±0.93\n31.69 ±0.41\n66.72 ±0.40\n35.84 ±0.61\n50.30 ±1.87\n45.49\n5.38\n48.95\n1.5B\nPre-LN\n61.51 ±1.22\n39.88 ±1.53\n71.41 ±0.88\n41.23 ±0.97\n54.51 ±2.07\n53.71\n3.29\n53.89\nPeri-LN\n66.17 ±0.21\n43.94 ±0.34\n73.63 ±0.24\n42.34 ±0.83\n56.64 ±0.44\n56.55\n3.18\n56.94\nPost-LN\n45.30 ±3.23\n33.59 ±0.44\n66.45 ±2.86\n35.82 ±1.09\n51.10 ±1.60\n46.45\n4.43\n49.33\n3.2B\nPre-LN\n65.24 ±2.32\n44.23 ±2.32\n73.86 ±1.19\n42.68 ±0.07\n57.42 ±2.51\n56.69\n3.20\n57.08\nPeri-LN\n68.73 ±0.57\n46.99 ±0.21\n74.31 ±0.41\n43.00 ±0.73\n59.76 ±0.78\n58.56\n3.11\n59.02\n(a) Gradient-norm at seed 5\n(b) Gradient-norm at seed 4\nFigure 5. Final-layer gradient norms for seeds 4 and 5.\n4.3. Early Stage Instability in Pre-Training\nEarly in pre-training, Pre-LN models consistently show gra-\ndient spikes, loss surges, and occasional divergence across\nseeds and scales (Fig. 4). These issues are far less pro-\nnounced in Peri-LN. We posit that the instability of Pre-LN\narises from three factors: (1) the hidden state variance ex-\nhibits a sudden surge from initialization through the early\nstages of optimization, deviating from the linear trend pre-\ndicted by Eq. 4 (see §3.3); (2) the exponential growth of\nhidden state variance across both depth and training steps;\nand (3) the instability caused by the massive activations\n(Proposition 3.1). Among these, we highlight the vari-\nance growth along the main path as the principal driver\nof the observed divergence. To corroborate this, Section 6\npresents targeted experiments that manipulate weight decay\nand weight initialization schemes, demonstrating how curb-\ning extreme variance mitigates the instability of each LN\nstrategy. The curves in 4(a), 4(b), and 4(c) are from a 400M\nmodel, whereas 4(d) corresponds to a 1.5B model.\n4.4. Gradient Norm of the Final-layer\nMotivated by Proposition 3.1, we track the final-layer\ngradient-norm in two representative runs selected from five\ntraining seeds. Figure 5(a), now including the newly added\nPeri-LN results, confirms the hierarchy reported by Xiong\net al. (2020): whenever training remains stable, the gradient\nnorms satisfy Post-LN > Pre-LN > Peri-LN. However, Fig-\nure 5(b) shows a run in which the Pre-LN model diverges\neven though every hyperparameter matches the stable run\nexcept for the random seed. In Section 5.1, we examine this\nfailure in greater depth and relate it to Proposition 3.1. The\ncurves in Figure 5 are obtained from 400M models.\n4.5. Benchmark Evaluations & Instruction Tuning\nTo evaluate how well the pre-training loss aligns with its\nbenchmark performance, we conduct five separate bench-\nmark evaluations. Furthermore, to investigate instruction-\nfollowing capabilities under different layer normalization\nstrategies, we conduct additional training using the LIMA\ndataset (Ouyang et al., 2022; Zhou et al., 2023). Diverged\ncheckpoints are excluded from the evaluation score compu-\ntation (mostly occurs in Pre-LN). Additional training hyper-\nparameters for SFT are given in Appendix D.2. As shown\nin Table 1, Peri-LN consistently demonstrates superior per-\nformance across all model sizes. Additionally, we note that,\nbeyond the improved scores, the standard deviation of the\nbenchmark results across different training seeds is reduced\nby more than half with Peri-LN. From this, we observe that\nPeri-LN helps maintain consistency not only in gradient\nstability and final loss but also in benchmark performance.\nFor the evaluation loss, we used 10K random samples from\nthe C4 dataset (Raffel et al., 2020). Detailed settings and\nindividual benchmark scores are provided in Appendix J.\n5. Analysis\nDespite emerging evidence that Peri-LN outperforms Post-\nand Pre-LN, key uncertainties remain: How do different\nLN placements shape hidden-state statistics and gradient\nflow (§5.1, §5.2)? What role does the Output-LN scale\nparameter γ play (§5.3)? And why does Peri-LN produce\nmore distinctive representations than its counterparts (§5.4)?\nThe subsections below tackle these questions in turn.\n5.1. Growth of Hidden State\nTo examine in greater depth how Peri-LN affects forward\npropagation, we analyze the absolute magnitude and vari-\nance of the hidden states using 1, 000 samples from the\n6\n\nPeri-LN: Revisiting Normalization Layer in the Transformer Architecture\n(a) Absolute magnitude growth\n(b) Variance growth\nFigure 6. Forward hidden state growth patterns for each LN strategy in a 1.5B-parameter Transformer.\n(a) Grad-norm at init.\n(b) Grad-norm at final\n(c) Grad-variance at init.\n(d) Grad-variance at final\nFigure 7. Backward gradient norm and variance of 1.5B Post-, Pre-, and Peri-LN Transformers at initialization (init.) and final training.\nWikitext dataset (Merity et al., 2016). Figure 6 shows how\ndifferent normalization strategies influence forward-path\nhidden states over the course of training and across model\ndepth. We observe the same pattern across all models trained\nwith five different random seeds (§F).\nAcross layers, Post-LN maintains stable hidden state magni-\ntudes and variances because the main path includes a nor-\nmalization layer. In contrast, Pre-LN omits normalization\nafter each attention and MLP sub-layer, so the magnitude\nand variance of the hidden states grow exponentially after\nthe residual addition. For Peri-LN, which adds an Output-\nLN, these statistics remain comparatively well controlled.\nAcross training iterations, Post-LN’s block-level normaliza-\ntion continues to suppress large shifts, preventing substantial\ndrift in magnitude or variance. Pre-LN starts with an approx-\nimately linear variance profile at initialization but escalates\nexponentially to extremely large values as optimization pro-\nceeds. Peri-LN again exhibits only moderate fluctuations,\nowing to Output-LN’s consistent regulation of hidden-state\nstatistics. Further discussion appears in Section 6.\n5.2. Layer-wise Gradient Norm & Variance\nEnsuring a uniform gradient flow in large-scale model train-\ning is crucial for balanced learning across the entire network\n(Yang & Hu, 2021; Yang et al., 2024). As shown in Figure 7,\nin Post-LN, gradients decrease as they propagate backward\nthrough the layers in the final stage of training, which can\nlead to vanishing gradients in lower-index layers. In Pre-LN,\ngradients increase as they propagate backward through the\nlayers at initialization, potentially causing explosive gradi-\nents in the early phase of training. Both strategies display\nnon-uniform gradient distributions—either vanishing or ex-\nploding—at different stages of training. On the other hand,\nPeri-LN demonstrates a consistent, layer-wise gradient dis-\ntribution at both initialization and the end of training. By\nmaintaining comparatively uniform gradients with lower\nvariance across layers, Peri-LN avoids the extremes of van-\nishing or exploding behaviors. This stability is particularly\nbeneficial in deeper architectures, where balanced gradient\nflow is essential for effective backpropagation.\n5.3. Learnable Parameter γ of RMSNorm\nTo investigate the impact of module output normalization on\ntraining stability, as proposed in the Proposition 3.1, we fix\nthe learnable parameter γ of RMSNorm to 1, isolating the\neffect of normalization. As illustrated in Figure 8, adding\noutput normalization to each sub-layer suppresses gradi-\nent spikes and lowers the loss relative to Transformers that\nemploy only pre-normalization. Nonetheless, we also con-\nfirm that allowing γ to be learnable yields slightly better\nperformance. The trend persists consistently across model\nscales and random seeds. In this experiment, we omit Peri-\nLN’s embedding layer normalization in order to isolate and\nevaluate the precise role and benefits of output-LN.\n5.4. Hidden State Representation\nTo assess hidden state redundancy after training, we employ\nangular distance (Li et al., 2024b), which quantifies how sim-\n7\n\nPeri-LN: Revisiting Normalization Layer in the Transformer Architecture\n(a) Training loss\n(b) Loss in the final 5B token interval\n(c) Gradient-norm\nFigure 8. Freezing learnable parameter γ of output normalization layer in Peri-LN. we set γ to its initial value of 1 and keep it fixed.\n(a) After 30B tokens training\n(b) Learnable scale γ in Output-LN\nFigure 9. Angular distance between hidden states after training. Fig. 9(b) monitor γ of every Output-LN in Peri-LN during training.\nilar or distinct the layer representations are. As Figure 9(a)\nillustrates, Pre-LN produces markedly more redundant hid-\nden states than the other variants by the end of training.\nWe attribute this effect to the exponential growth of the\nmain residual path in Pre-LN, which diminishes the relative\ncontribution of individual sub-layers. In contrast, Peri-LN\nretains an identity path whose learnable scale begins near 1\nand gradually adjusts with depth (Figure 9(b)), thereby mod-\nerating redundancy. These observations highlight the role\nof module-output normalization in controlling hidden state\nsimilarity. All statistics are computed on 256 random sam-\nples from RedPajama-Data-1T (Computer, 2023). Appendix\nL includes additional figures and initialization comparisons.\n6. Ablation Study\nTo probe massive activations across conditions, we sweep\nweight decay coefficient and initialization variance for both\nPre- and Peri-LN models, holding other settings fixed. Per-\nrun results and detailed settings are in Appendix G.8.\n6.1. Weight Decay\nIn Figure 10(a), stronger L2 regularization markedly lowers\nthe variance curve, confirming that heavier weight decay\ndirectly curbs forward-path explosions in Pre-LN. In con-\ntrast, the same increase in weight decay reduces Peri-LN’s\nvariance growth only marginally. We take the stable run\ninitialized with seed 3 (Table 11) and sweep the weight de-\ncay coefficient. Table 7 in the Appendix further shows that,\nirrespective of the presence of massive activations, Peri-LN\nachieves better performance than Pre-LN.\nTo further probe stability under varying degrees of massive\nactivation, we replicate the previously divergent run (seed\n4) and repeat the same weight decay sweep. As Figure\n10(b) demonstrates, raising the weight decay coefficient\nfrom the baseline 0.033 to 0.33 (a tenfold rise) prevents\ndivergence, providing empirical support for Proposition 3.1.\nNevertheless, strong weight decay can stabilize Pre-LN, it\nstill fails to close the performance gap relative to Peri-LN.\n6.2. Weight Initialization\nAs the initialization variance increases, the severity of mas-\nsive activations rises correspondingly for Pre-LN (Figure\n10(c)); at the largest variance, the model diverges outright\n(Appendix G.8.3). Pre-LN therefore displays marked sensi-\ntivity to its initial conditions. In contrast, across the same\nrange of ablations, Peri-LN’s loss curves and activation vari-\nances shift only marginally. This hyperparameter-insensitive\nrobustness is corroborated by the low downstream standard\ndeviations reported in Table 1.\n6.3. Additional Results\nFor brevity, we defer an extensive set of supplementary ex-\nperiments to the appendix. Appendix G reports the core\nrobustness checks: replacing RMSNorm with LayerNorm,\nvarying sequence lengths, reducing pre-training budgets,\nand ablating embedding-level normalization. OLMo2-style\n8\n\nPeri-LN: Revisiting Normalization Layer in the Transformer Architecture\n(a) Variance growth by weight decay\n(b) Training loss by weight decay\n(c) Variance growth by weight init.\nFigure 10. Effects of weight decay and initialization (init.) on massive activations. 10(a) Strong weight decay relieves the variance\nexplosion in Pre-LN. 10(b) Strong weight decay (0.33, which is 10× the baseline.) suppresses Pre-LN divergence. 10(c) Smaller-scale\ninitialization slightly curbs Pre-LN variance, while Peri-LN remains bounded regardless. d denotes the model’s hidden dimensionality.\nPeri-LN pre-training runs appear in Appendix H. Stochastic\ngradient descent (SGD) baselines are summarized in Ap-\npendix I. Alternative LN placements, extending Figure 2,\nare provided in Appendix G.6. Across all settings, the re-\nsults are consistent with the trends presented in the main\nSection 4 and 5, further substantiating our conclusions.\n7. Implications\nThis section integrates our findings into practical guidance\non variance-driven stability and precision constraints in\nlarge-scale Transformers.\n7.1. Mitigating Variance-Driven Instability via Peri-LN\nPre-LN, the prevailing normalization strategy, is inherently\nprone to unchecked growth in activation variance (§5),\nwhich in turn induces numerical instability during train-\ning. Our extensive empirical analysis shows that Peri-LN—\nwhich normalizes the outputs of the Attention and MLP\nsub-layers—markedly curbs this variance and often pre-\nvents divergence (§4 & §6). Proposition 3.1 formalizes how\nexcessive variance amplifies gradient norms, clarifying its\ncausal role in destabilizing large-scale pre-training. In Pre-\nLN, instability is further exacerbated when the statistical\nconditions assumed at initialization depart markedly from\nthose observed later in training (§3.3). By contrast, Peri-\nLN alleviates this discrepancy, thereby improving training\nstability, and delivering additional performance gains.\n7.2. Precision Constraints Imposed by Pre-LN\nBoth Pre-LN and Peri-LN architectures leave the main hid-\nden state path unnormalized, so once large values arise in\nearlier layers, they persist through to later layers. Conse-\nquently, Pre-LN’s additive residual path might generates ac-\ntivations near or beyond the FP16 limit. To gauge how often\nthese values exceed FP16 yet remain within BF16, we track\nthe top-100 absolute hidden state values for 3.2B-parameter\nPre-LN and Peri-LN models. In Figure 11, the blue band\nFigure 11. Evolution of extreme hidden state absolute magnitudes\nduring training. Colored bands trace the range of the global top-\n100 absolute activations. Pre-LN (blue) quickly surpasses the FP16\nrepresentable maximum, while Peri-LN (red) stays below it.\nfor Pre-LN surpasses the FP16 maximum bound as early\nas 0.5B training tokens whereas Peri-LN (red band) consis-\ntently remains below this threshold. This pattern, echoing\nSun et al. (2024), highlights that choosing FP16 or BF16 is\nnot just a hardware preference but is closely linked to how\nhidden state magnitudes evolve within the model. Earlier\nwork on OPT (Zhang et al., 2022), which was pre-trained\nusing FP16 precision, suggests that the training instabili-\nties they observed were likely exacerbated by numerical\noverflows and gradient pathologies (Proposition 3.1) arising\nwhen activations exceeded the representable range of FP16.\n8. Conclusion\nWe explore the placement of layer normalization within\nthe Transformer architecture to better understand its role\nduring training. By systematically comparing Post-LN, Pre-\nLN, and newly termed Peri-LN, we highlight their distinct\nimpacts on stability, final performance, and optimization\ndynamics. Our findings suggest that placing LN on module\noutputs in addition to the Pre-LN can help manage large ac-\ntivations while preserving beneficial gradient flow, thereby\noffering a promising balance for stable optimization. By\nunifying these approaches under the term Peri-LN, we seek\nto consolidate existing variants and encourage deeper inves-\ntigation into this underexplored alternative.\n9\n\nPeri-LN: Revisiting Normalization Layer in the Transformer Architecture\nAcknowledgements\nWe thank our colleague Jeongin Bae for inspiring the un-\nderlying motivation for this research. We are also grateful\nto Jung Hyun Lee, Seonghyeon Kim, and Seunghyun Seo\nfor their valuable assistance during the early stages discus-\nsions. Finally, we extend our gratitude to Gichang Lee,\nLead of the Backbone Mission at NAVER Cloud, for his\nunwavering support. This work was partly supported by\nInstitute for Information & communications Technology\nTechnology Planning & Evaluation(IITP) grant funded by\nthe Korea government(MSIT)(RS-2019-II190075, Artificial\nIntelligence Graduate School Support Program(KAIST),\nNo.RS-2021-II212068, Artificial Intelligence Innovation\nHub, No. RS-2024-00509279, Global AI Frontier Lab)\nImpact Statement\nThe rapid advancement of Transformer-based large lan-\nguage models (LLMs) has enabled remarkable break-\nthroughs in natural language understanding and generation.\nHowever, these models also pose significant challenges, in-\ncluding concerns around safety, bias, and the computational\ncost associated with large-scale training. As LLMs become\nincreasingly integral to various AI applications, ensuring\ntheir stability, efficiency, and accessibility remains a critical\nresearch focus.\nOur work addresses these challenges by proposing a more\nstable and cost-effective large-scale training methodology.\nBy improving training efficiency and reducing the associ-\nated computational overhead, we lower the barrier to entry\nfor organizations seeking to develop or fine-tune foundation\nmodels. This democratization of LLM technology fosters\nbroader participation in AI research and development, accel-\nerating innovation while mitigating concerns over resource\nconcentration in a few major players. Given the growing\nindustry focus on optimizing LLM deployment costs, our\ncontributions are particularly relevant in the current AI re-\nsearch landscape.\nImproving the cost-effectiveness of large-scale training si-\nmultaneously lowers AI’s environmental footprint by reduc-\ning the vast energy consumption and carbon emissions inher-\nent in state-of-the-art LLM development. This efficiency not\nonly aligns with global sustainability goals but also enables\nsmaller research labs and academic groups to pursue cutting-\nedge AI without prohibitive resource demands, fostering a\nmore inclusive and responsible ecosystem.\nReferences\nBa, L. J., Kiros, J. R., and Hinton, G. E. Layer normalization.\nCoRR, abs/1607.06450, 2016. URL http://arxiv.\norg/abs/1607.06450.\nBisk, Y., Zellers, R., Gao, J., Choi, Y., et al. Piqa: Reasoning\nabout physical commonsense in natural language. In Pro-\nceedings of the AAAI conference on artificial intelligence,\nvolume 34, pp. 7432–7439, 2020.\nBrock, A., De, S., Smith, S. L., and Simonyan, K. High-\nperformance large-scale image recognition without nor-\nmalization.\nIn Meila, M. and Zhang, T. (eds.), Pro-\nceedings of the 38th International Conference on Ma-\nchine Learning, ICML 2021, 18-24 July 2021, Virtual\nEvent, volume 139 of Proceedings of Machine Learn-\ning Research, pp. 1059–1071. PMLR, 2021a.\nURL\nhttp://proceedings.mlr.press/v139/b\nrock21a.html.\nBrock, A., De, S., Smith, S. L., and Simonyan, K. High-\nperformance large-scale image recognition without nor-\nmalization.\nIn Meila, M. and Zhang, T. (eds.), Pro-\nceedings of the 38th International Conference on Ma-\nchine Learning, ICML 2021, 18-24 July 2021, Virtual\nEvent, volume 139 of Proceedings of Machine Learn-\ning Research, pp. 1059–1071. PMLR, 2021b.\nURL\nhttp://proceedings.mlr.press/v139/b\nrock21a.html.\nChung, W., Hong, J., An, N. M., Thorne, J., and Yun, S. Sta-\nble language model pre-training by reducing embedding\nvariability. In Al-Onaizan, Y., Bansal, M., and Chen, Y.\n(eds.), Proceedings of the 2024 Conference on Empirical\nMethods in Natural Language Processing, EMNLP 2024,\nMiami, FL, USA, November 12-16, 2024, pp. 10852–\n10863. Association for Computational Linguistics, 2024.\nURL https://aclanthology.org/2024.em\nnlp-main.606.\nClark, P., Cowhey, I., Etzioni, O., Khot, T., Sabharwal, A.,\nSchoenick, C., and Tafjord, O. Think you have solved\nquestion answering? try arc, the ai2 reasoning challenge,\n2018. URL https://arxiv.org/abs/1803.0\n5457.\nComputer, T. Redpajama: An open source recipe to re-\nproduce llama training dataset, 2023. URL https:\n//github.com/togethercomputer/RedPaj\nama-Data.\nCsord´as, R., Irie, K., Schmidhuber, J., Potts, C., and\nManning, C. D.\nMoeut: Mixture-of-experts univer-\nsal transformers. CoRR, abs/2405.16039, 2024. doi:\n10.48550/ARXIV.2405.16039. URL https://doi.\norg/10.48550/arXiv.2405.16039.\n10\n\nPeri-LN: Revisiting Normalization Layer in the Transformer Architecture\nDe, S. and Smith, S. L. Batch normalization biases resid-\nual blocks towards the identity function in deep net-\nworks.\nIn Larochelle, H., Ranzato, M., Hadsell, R.,\nBalcan, M., and Lin, H. (eds.), Advances in Neural\nInformation Processing Systems 33: Annual Confer-\nence on Neural Information Processing Systems 2020,\nNeurIPS 2020, December 6-12, 2020, virtual, 2020. URL\nhttps://proceedings.neurips.cc/paper\n/2020/hash/e6b738eca0e6792ba8a9cbcba\n6c1881d-Abstract.html.\nDehghani, M., Djolonga, J., Mustafa, B., Padlewski, P.,\nHeek, J., Gilmer, J., Steiner, A. P., Caron, M., Geirhos,\nR., Alabdulmohsin, I., Jenatton, R., Beyer, L., Tschan-\nnen, M., Arnab, A., Wang, X., Ruiz, C. R., Minderer, M.,\nPuigcerver, J., Evci, U., Kumar, M., van Steenkiste, S.,\nElsayed, G. F., Mahendran, A., Yu, F., Oliver, A., Huot,\nF., Bastings, J., Collier, M., Gritsenko, A. A., Birodkar,\nV., Vasconcelos, C. N., Tay, Y., Mensink, T., Kolesnikov,\nA., Pavetic, F., Tran, D., Kipf, T., Lucic, M., Zhai, X.,\nKeysers, D., Harmsen, J. J., and Houlsby, N. Scaling\nvision transformers to 22 billion parameters. In Krause,\nA., Brunskill, E., Cho, K., Engelhardt, B., Sabato, S.,\nand Scarlett, J. (eds.), International Conference on Ma-\nchine Learning, ICML 2023, 23-29 July 2023, Honolulu,\nHawaii, USA, volume 202 of Proceedings of Machine\nLearning Research, pp. 7480–7512. PMLR, 2023. URL\nhttps://proceedings.mlr.press/v202/d\nehghani23a.html.\nDettmers, T., Lewis, M., Belkada, Y., and Zettlemoyer,\nL.\nGpt3.int8(): 8-bit matrix multiplication for trans-\nformers at scale. In Koyejo, S., Mohamed, S., Agar-\nwal, A., Belgrave, D., Cho, K., and Oh, A. (eds.), Ad-\nvances in Neural Information Processing Systems 35:\nAnnual Conference on Neural Information Processing\nSystems 2022, NeurIPS 2022, New Orleans, LA, USA,\nNovember 28 - December 9, 2022, 2022. URL http:\n//papers.nips.cc/paper_files/paper/2\n022/hash/c3ba4962c05c49636d4c6206a97\ne9c8a-Abstract-Conference.html.\nDing, M., Yang, Z., Hong, W., Zheng, W., Zhou, C., Yin,\nD., Lin, J., Zou, X., Shao, Z., Yang, H., et al. Cogview:\nMastering text-to-image generation via transformers. Ad-\nvances in neural information processing systems, 34:\n19822–19835, 2021.\nDubey, A., Jauhri, A., Pandey, A., Kadian, A., Al-Dahle,\nA., Letman, A., Mathur, A., Schelten, A., Yang, A., Fan,\nA., et al. The llama 3 herd of models. arXiv preprint\narXiv:2407.21783, 2024.\nFishman, M., Chmiel, B., Banner, R., and Soudry, D.\nScaling FP8 training to trillion-token llms.\nCoRR,\nabs/2409.12517, 2024. doi: 10.48550/ARXIV.2409.\n12517. URL https://doi.org/10.48550/arX\niv.2409.12517.\nGao, L., Tow, J., Abbasi, B., Biderman, S., Black, S., DiPofi,\nA., Foster, C., Golding, L., Hsu, J., Le Noac’h, A., Li, H.,\nMcDonell, K., Muennighoff, N., Ociepa, C., Phang, J.,\nReynolds, L., Schoelkopf, H., Skowron, A., Sutawika, L.,\nTang, E., Thite, A., Wang, B., Wang, K., and Zou, A. A\nframework for few-shot language model evaluation, 07\n2024. URL https://zenodo.org/records/1\n2608602.\nGlorot, X. and Bengio, Y. Understanding the difficulty of\ntraining deep feedforward neural networks. In Teh, Y. W.\nand Titterington, D. M. (eds.), Proceedings of the Thir-\nteenth International Conference on Artificial Intelligence\nand Statistics, AISTATS 2010, Chia Laguna Resort, Sar-\ndinia, Italy, May 13-15, 2010, volume 9 of JMLR Proceed-\nings, pp. 249–256. JMLR.org, 2010. URL http://pr\noceedings.mlr.press/v9/glorot10a.html.\nGuo, D., Yang, D., Zhang, H., Song, J., Zhang, R., Xu, R.,\nZhu, Q., Ma, S., Wang, P., Bi, X., et al. Deepseek-r1: In-\ncentivizing reasoning capability in llms via reinforcement\nlearning. arXiv preprint arXiv:2501.12948, 2025.\nHe, K., Zhang, X., Ren, S., and Sun, J. Delving deep\ninto rectifiers: Surpassing human-level performance on\nimagenet classification. In 2015 IEEE International Con-\nference on Computer Vision, ICCV 2015, Santiago, Chile,\nDecember 7-13, 2015, pp. 1026–1034. IEEE Computer\nSociety, 2015. doi: 10.1109/ICCV.2015.123. URL\nhttps://doi.org/10.1109/ICCV.2015.123.\nHe, K., Zhang, X., Ren, S., and Sun, J. Identity mappings in\ndeep residual networks. In Leibe, B., Matas, J., Sebe, N.,\nand Welling, M. (eds.), Computer Vision - ECCV 2016 -\n14th European Conference, Amsterdam, The Netherlands,\nOctober 11-14, 2016, Proceedings, Part IV, volume 9908\nof Lecture Notes in Computer Science, pp. 630–645.\nSpringer, 2016. doi: 10.1007/978-3-319-46493-0\\ 38.\nURL https://doi.org/10.1007/978-3-319\n-46493-0_38.\nHeo, J. H., Kim, J., Kwon, B., Kim, B., Kwon, S. J., and\nLee, D. Rethinking channel dimensions to isolate outliers\nfor low-bit weight quantization of large language models.\nIn The Twelfth International Conference on Learning\nRepresentations, ICLR 2024, Vienna, Austria, May 7-11,\n2024. OpenReview.net, 2024. URL https://openre\nview.net/forum?id=JzG7kSpjJk.\nHoffmann, J., Borgeaud, S., Mensch, A., Buchatskaya, E.,\nCai, T., Rutherford, E., de Las Casas, D., Hendricks,\nL. A., Welbl, J., Clark, A., Hennigan, T., Noland, E.,\nMillican, K., van den Driessche, G., Damoc, B., Guy,\nA., Osindero, S., Simonyan, K., Elsen, E., Rae, J. W.,\n11\n\nPeri-LN: Revisiting Normalization Layer in the Transformer Architecture\nVinyals, O., and Sifre, L.\nTraining compute-optimal\nlarge language models. CoRR, abs/2203.15556, 2022.\ndoi: 10.48550/ARXIV.2203.15556. URL https:\n//doi.org/10.48550/arXiv.2203.15556.\nKedia, A., Zaidi, M. A., Khyalia, S., Jung, J., Goka, H., and\nLee, H. Transformers get stable: An end-to-end signal\npropagation theory for language models. In Forty-first\nInternational Conference on Machine Learning, ICML\n2024, Vienna, Austria, July 21-27, 2024. OpenReview.net,\n2024. URL https://openreview.net/forum\n?id=30waYPIZUA.\nKim, J., Lee, J. H., Kim, S., Park, J., Yoo, K. M., Kwon,\nS. J., and Lee, D. Memory-efficient fine-tuning of com-\npressed large language models via sub-4-bit integer quan-\ntization. In Oh, A., Naumann, T., Globerson, A., Saenko,\nK., Hardt, M., and Levine, S. (eds.), Advances in Neu-\nral Information Processing Systems 36: Annual Confer-\nence on Neural Information Processing Systems 2023,\nNeurIPS 2023, New Orleans, LA, USA, December 10 -\n16, 2023, 2023. URL http://papers.nips.cc/p\naper_files/paper/2023/hash/7183f4fc8\n7598f6c6e947b96714acbd6-Abstract-Con\nference.html.\nLee, J. H., Kim, J., Kwon, S. J., and Lee, D. Flexround:\nLearnable rounding based on element-wise division for\npost-training quantization. In Krause, A., Brunskill, E.,\nCho, K., Engelhardt, B., Sabato, S., and Scarlett, J. (eds.),\nInternational Conference on Machine Learning, ICML\n2023, 23-29 July 2023, Honolulu, Hawaii, USA, volume\n202 of Proceedings of Machine Learning Research, pp.\n18913–18939. PMLR, 2023. URL https://procee\ndings.mlr.press/v202/lee23h.html.\nLee, J. H., Kim, J., Yang, J. Y., Kwon, S. J., Yang, E., Yoo,\nK. M., and Lee, D. LRQ: optimizing post-training quan-\ntization for large language models by learning low-rank\nweight-scaling matrices. In Chiruzzo, L., Ritter, A., and\nWang, L. (eds.), Proceedings of the 2025 Conference\nof the Nations of the Americas Chapter of the Associa-\ntion for Computational Linguistics: Human Language\nTechnologies, NAACL 2025 - Volume 1: Long Papers, Al-\nbuquerque, New Mexico, USA, April 29 - May 4, 2025, pp.\n7708–7743. Association for Computational Linguistics,\n2025. URL https://aclanthology.org/2025.\nnaacl-long.393/.\nLi, J., Fang, A., Smyrnis, G., Ivgi, M., Jordan, M., Gadre,\nS. Y., Bansal, H., Guha, E. K., Keh, S., Arora, K., Garg,\nS., Xin, R., Muennighoff, N., Heckel, R., Mercat, J.,\nChen, M., Gururangan, S., Wortsman, M., Albalak, A.,\nBitton, Y., Nezhurina, M., Abbas, A., Hsieh, C., Ghosh,\nD., Gardner, J., Kilian, M., Zhang, H., Shao, R., Pratt,\nS. M., Sanyal, S., Ilharco, G., Daras, G., Marathe, K.,\nGokaslan, A., Zhang, J., Chandu, K. R., Nguyen, T.,\nVasiljevic, I., Kakade, S. M., Song, S., Sanghavi, S.,\nFaghri, F., Oh, S., Zettlemoyer, L., Lo, K., El-Nouby, A.,\nPouransari, H., Toshev, A., Wang, S., Groeneveld, D.,\nSoldaini, L., Koh, P. W., Jitsev, J., Kollar, T., Dimakis,\nA. G., Carmon, Y., Dave, A., Schmidt, L., and Shankar, V.\nDatacomp-lm: In search of the next generation of training\nsets for language models. CoRR, abs/2406.11794, 2024a.\ndoi: 10.48550/ARXIV.2406.11794. URL https:\n//doi.org/10.48550/arXiv.2406.11794.\nLi, P., Yin, L., and Liu, S. Mix-ln: Unleashing the power\nof deeper layers by combining pre-ln and post-ln. arXiv\npreprint arXiv:2412.13795, 2024b.\nLoshchilov, I., Hsieh, C., Sun, S., and Ginsburg, B. ngpt:\nNormalized transformer with representation learning on\nthe hypersphere.\nCoRR, abs/2410.01131, 2024.\ndoi:\n10.48550/ARXIV.2410.01131. URL https://doi.\norg/10.48550/arXiv.2410.01131.\nMerity, S., Xiong, C., Bradbury, J., and Socher, R. Pointer\nsentinel mixture models, 2016.\nMihaylov, T., Clark, P., Khot, T., and Sabharwal, A. Can a\nsuit of armor conduct electricity? a new dataset for open\nbook question answering. In EMNLP, 2018.\nOLMo, T., Walsh, P., Soldaini, L., Groeneveld, D., Lo, K.,\nArora, S., Bhagia, A., Gu, Y., Huang, S., Jordan, M.,\net al. 2 olmo 2 furious. arXiv preprint arXiv:2501.00656,\n2024.\nOuyang, L., Wu, J., Jiang, X., Almeida, D., Wainwright,\nC. L., Mishkin, P., Zhang, C., Agarwal, S., Slama, K.,\nRay, A., Schulman, J., Hilton, J., Kelton, F., Miller, L.,\nSimens, M., Askell, A., Welinder, P., Christiano, P. F.,\nLeike, J., and Lowe, R. Training language models to\nfollow instructions with human feedback. In Koyejo, S.,\nMohamed, S., Agarwal, A., Belgrave, D., Cho, K., and\nOh, A. (eds.), Advances in Neural Information Processing\nSystems 35: Annual Conference on Neural Information\nProcessing Systems 2022, NeurIPS 2022, New Orleans,\nLA, USA, November 28 - December 9, 2022, 2022. URL\nhttp://papers.nips.cc/paper_files/pap\ner/2022/hash/b1efde53be364a73914f588\n05a001731-Abstract-Conference.html.\nRaffel, C., Shazeer, N., Roberts, A., Lee, K., Narang, S.,\nMatena, M., Zhou, Y., Li, W., and Liu, P. J. Exploring\nthe limits of transfer learning with a unified text-to-text\ntransformer. The Journal of Machine Learning Research,\n21(1):5485–5551, 2020.\nRivi`ere, M., Pathak, S., Sessa, P. G., Hardin, C., Bhupati-\nraju, S., Hussenot, L., Mesnard, T., Shahriari, B., Ram´e,\nA., Ferret, J., Liu, P., Tafti, P., Friesen, A., Casbon, M.,\n12\n\nPeri-LN: Revisiting Normalization Layer in the Transformer Architecture\nRamos, S., Kumar, R., Lan, C. L., Jerome, S., Tsitsulin,\nA., Vieillard, N., Stanczyk, P., Girgin, S., Momchev,\nN., Hoffman, M., Thakoor, S., Grill, J., Neyshabur, B.,\nBachem, O., Walton, A., Severyn, A., Parrish, A., Ahmad,\nA., Hutchison, A., Abdagic, A., Carl, A., Shen, A., Brock,\nA., Coenen, A., Laforge, A., Paterson, A., Bastian, B.,\nPiot, B., Wu, B., Royal, B., Chen, C., Kumar, C., Perry,\nC., Welty, C., Choquette-Choo, C. A., Sinopalnikov, D.,\nWeinberger, D., Vijaykumar, D., Rogozinska, D., Her-\nbison, D., Bandy, E., Wang, E., Noland, E., Moreira,\nE., Senter, E., Eltyshev, E., Visin, F., Rasskin, G., Wei,\nG., Cameron, G., Martins, G., Hashemi, H., Klimczak-\nPlucinska, H., Batra, H., Dhand, H., Nardini, I., Mein,\nJ., Zhou, J., Svensson, J., Stanway, J., Chan, J., Zhou,\nJ. P., Carrasqueira, J., Iljazi, J., Becker, J., Fernandez, J.,\nvan Amersfoort, J., Gordon, J., Lipschultz, J., Newlan,\nJ., Ji, J., Mohamed, K., Badola, K., Black, K., Millican,\nK., McDonell, K., Nguyen, K., Sodhia, K., Greene, K.,\nSj¨osund, L. L., Usui, L., Sifre, L., Heuermann, L., Lago,\nL., and McNealus, L. Gemma 2: Improving open lan-\nguage models at a practical size. CoRR, abs/2408.00118,\n2024. doi: 10.48550/ARXIV.2408.00118. URL https:\n//doi.org/10.48550/arXiv.2408.00118.\nSakaguchi, K., Bras, R. L., Bhagavatula, C., and Choi, Y.\nWinogrande: an adversarial winograd schema challenge\nat scale. Commun. ACM, 64(9):99–106, August 2021.\nISSN 0001-0782. doi: 10.1145/3474381. URL https:\n//doi.org/10.1145/3474381.\nSap, M., Rashkin, H., Chen, D., Bras, R. L., and Choi, Y.\nSocial iqa: Commonsense reasoning about social interac-\ntions. In Inui, K., Jiang, J., Ng, V., and Wan, X. (eds.),\nProceedings of the 2019 Conference on Empirical Meth-\nods in Natural Language Processing and the 9th Interna-\ntional Joint Conference on Natural Language Processing,\nEMNLP-IJCNLP 2019, Hong Kong, China, November\n3-7, 2019, pp. 4462–4472. Association for Computational\nLinguistics, 2019. doi: 10.18653/V1/D19-1454. URL\nhttps://doi.org/10.18653/v1/D19-1454.\nSun, M., Chen, X., Kolter, J. Z., and Liu, Z. Massive activa-\ntions in large language models. CoRR, abs/2402.17762,\n2024. doi: 10.48550/ARXIV.2402.17762. URL https:\n//doi.org/10.48550/arXiv.2402.17762.\nTakase, S., Kiyono, S., Kobayashi, S., and Suzuki, J. Spike\nno more: Stabilizing the pre-training of large language\nmodels. CoRR, abs/2312.16903, 2023. doi: 10.48550/A\nRXIV.2312.16903. URL https://doi.org/10.4\n8550/arXiv.2312.16903.\nTalmor, A., Herzig, J., Lourie, N., and Berant, J. Common-\nsenseQA: A question answering challenge targeting com-\nmonsense knowledge. In Proceedings of the 2019 Confer-\nence of the North American Chapter of the Association for\nComputational Linguistics: Human Language Technolo-\ngies, Volume 1 (Long and Short Papers), pp. 4149–4158,\nMinneapolis, Minnesota, June 2019. Association for\nComputational Linguistics. doi: 10.18653/v1/N19-1421.\nURL https://aclanthology.org/N19-1421.\nTeam, G., Kamath, A., Ferret, J., Pathak, S., Vieillard,\nN., Merhej, R., Perrin, S., Matejovicova, T., Ram´e, A.,\nRivi`ere, M., et al. Gemma 3 technical report. arXiv\npreprint arXiv:2503.19786, 2025.\nVaswani, A., Shazeer, N., Parmar, N., Uszkoreit, J., Jones,\nL., Gomez, A. N., Kaiser, L., and Polosukhin, I. Attention\nis all you need. In Guyon, I., von Luxburg, U., Bengio,\nS., Wallach, H. M., Fergus, R., Vishwanathan, S. V. N.,\nand Garnett, R. (eds.), Advances in Neural Information\nProcessing Systems 30: Annual Conference on Neural\nInformation Processing Systems 2017, December 4-9,\n2017, Long Beach, CA, USA, pp. 5998–6008, 2017. URL\nhttps://proceedings.neurips.cc/paper\n/2017/hash/3f5ee243547dee91fbd053c1c\n4a845aa-Abstract.html.\nWang, A., Singh, A., Michael, J., Hill, F., Levy, O., and Bow-\nman, S. GLUE: A multi-task benchmark and analysis plat-\nform for natural language understanding. In Proceedings\nof the 2018 EMNLP Workshop BlackboxNLP: Analyz-\ning and Interpreting Neural Networks for NLP, pp. 353–\n355, Brussels, Belgium, November 2018. Association for\nComputational Linguistics. doi: 10.18653/v1/W18-5446.\nURL https://aclanthology.org/W18-5446.\nWolf, T., Debut, L., Sanh, V., Chaumond, J., Delangue, C.,\nMoi, A., Cistac, P., Rault, T., Louf, R., Funtowicz, M.,\nDavison, J., Shleifer, S., von Platen, P., Ma, C., Jernite,\nY., Plu, J., Xu, C., Scao, T. L., Gugger, S., Drame, M.,\nLhoest, Q., and Rush, A. M. Transformers: State-of-\nthe-art natural language processing. In Proceedings of\nthe 2020 Conference on Empirical Methods in Natural\nLanguage Processing: System Demonstrations, pp. 38–\n45, Online, October 2020. Association for Computational\nLinguistics. URL https://www.aclweb.org/a\nnthology/2020.emnlp-demos.6.\nWortsman, M., Liu, P. J., Xiao, L., Everett, K. E., Alemi,\nA. A., Adlam, B., Co-Reyes, J. D., Gur, I., Kumar, A.,\nNovak, R., Pennington, J., Sohl-Dickstein, J., Xu, K.,\nLee, J., Gilmer, J., and Kornblith, S. Small-scale proxies\nfor large-scale transformer training instabilities. In The\nTwelfth International Conference on Learning Represen-\ntations, ICLR 2024, Vienna, Austria, May 7-11, 2024.\nOpenReview.net, 2024. URL https://openreview\n.net/forum?id=d8w0pmvXbZ.\nXie, S., Zhang, H., Guo, J., Tan, X., Bian, J., Awadalla,\nH. H., Menezes, A., Qin, T., and Yan, R.\nResidual:\n13\n\nPeri-LN: Revisiting Normalization Layer in the Transformer Architecture\nTransformer with dual residual connections.\nCoRR,\nabs/2304.14802, 2023. doi: 10.48550/ARXIV.2304.\n14802. URL https://doi.org/10.48550/arX\niv.2304.14802.\nXiong, R., Yang, Y., He, D., Zheng, K., Zheng, S., Xing, C.,\nZhang, H., Lan, Y., Wang, L., and Liu, T. On layer nor-\nmalization in the transformer architecture. In Proceedings\nof the 37th International Conference on Machine Learn-\ning, ICML 2020, 13-18 July 2020, Virtual Event, volume\n119 of Proceedings of Machine Learning Research, pp.\n10524–10533. PMLR, 2020. URL http://procee\ndings.mlr.press/v119/xiong20b.html.\nYang, G. and Hu, E. J. Tensor programs IV: feature learn-\ning in infinite-width neural networks. In Meila, M. and\nZhang, T. (eds.), Proceedings of the 38th International\nConference on Machine Learning, ICML 2021, 18-24\nJuly 2021, Virtual Event, volume 139 of Proceedings of\nMachine Learning Research, pp. 11727–11737. PMLR,\n2021. URL http://proceedings.mlr.press/\nv139/yang21c.html.\nYang, G., Hu, E. J., Babuschkin, I., Sidor, S., Liu, X., Farhi,\nD., Ryder, N., Pachocki, J., Chen, W., and Gao, J. Tensor\nprograms V: tuning large neural networks via zero-shot\nhyperparameter transfer. CoRR, abs/2203.03466, 2022.\ndoi: 10.48550/ARXIV.2203.03466. URL https:\n//doi.org/10.48550/arXiv.2203.03466.\nYang, G., Yu, D., Zhu, C., and Hayou, S. Tensor programs\nVI: feature learning in infinite depth neural networks.\nIn The Twelfth International Conference on Learning\nRepresentations, ICLR 2024, Vienna, Austria, May 7-11,\n2024. OpenReview.net, 2024. URL https://openre\nview.net/forum?id=17pVDnpwwl.\nYoo, K. M., Han, J., In, S., Jeon, H., Jeong, J., Kang, J.,\nKim, H., Kim, K., Kim, M., Kim, S., Kwak, D., Kwak,\nH., Kwon, S. J., Lee, B., Lee, D., Lee, G., Lee, J., Park,\nB., Shin, S., Yu, J., Baek, S., Byeon, S., Cho, E., Choe,\nD., Han, J., Jin, Y., Jun, H., Jung, J., Kim, C., Kim,\nJ., Kim, J., Lee, D., Park, D. W., Sohn, J. M., Han, S.,\nHeo, J., Hong, S., Jeon, M., Jung, H., Jung, J., Jung, W.,\nKim, C., Kim, H., Kim, J., Kim, M. Y., Lee, S., Park,\nJ., Shin, J., Yang, S., Yoon, J., Lee, H., Bae, S., Cha, J.,\nGylleus, K., Ham, D., Hong, M., Hong, Y., Hong, Y.,\nJang, D., Jeon, H., Jeon, Y., Jeong, Y., Ji, M., Jin, Y., Jo,\nC., Joo, S., Jung, S., Kim, A. J., Kim, B. H., Kim, H.,\nKim, J., Kim, M., Kim, M., Kim, S., Kim, Y., Kim, Y.,\nKim, Y., Ko, D., Lee, D., Lee, H., Lee, J., Lee, J., Lee,\nJ., Lee, J., Lee, M. Y., Lee, Y., Min, T., Min, Y., Moon,\nK., Oh, H., Park, J., Park, K., Park, Y., Seo, H., Seo, S.,\nSim, M., Son, G., Yeo, M., Yeom, K. H., and Yoo, W.\nHyperclova X technical report. CoRR, abs/2404.01954,\n2024. doi: 10.48550/ARXIV.2404.01954. URL https:\n//doi.org/10.48550/arXiv.2404.01954.\nYu, M., Wang, D., Shan, Q., and Wan, A.\nThe su-\nper weight in large language models. arXiv preprint\narXiv:2411.07191, 2024.\nZellers, R., Holtzman, A., Bisk, Y., Farhadi, A., and Choi,\nY. HellaSwag: Can a machine really finish your sen-\ntence?\nIn Korhonen, A., Traum, D., and M`arquez,\nL. (eds.), Proceedings of the 57th Annual Meeting of\nthe Association for Computational Linguistics, pp. 4791–\n4800, Florence, Italy, July 2019. Association for Compu-\ntational Linguistics. doi: 10.18653/v1/P19-1472. URL\nhttps://aclanthology.org/P19-1472/.\nZhai, S., Likhomanenko, T., Littwin, E., Busbridge, D.,\nRamapuram, J., Zhang, Y., Gu, J., and Susskind, J. M.\nStabilizing transformer training by preventing attention\nentropy collapse. In Krause, A., Brunskill, E., Cho, K.,\nEngelhardt, B., Sabato, S., and Scarlett, J. (eds.), Inter-\nnational Conference on Machine Learning, ICML 2023,\n23-29 July 2023, Honolulu, Hawaii, USA, volume 202 of\nProceedings of Machine Learning Research, pp. 40770–\n40803. PMLR, 2023. URL https://proceedings.\nmlr.press/v202/zhai23a.html.\nZhang, B. and Sennrich, R. Root mean square layer nor-\nmalization. In Wallach, H. M., Larochelle, H., Beygelz-\nimer, A., d’Alch´e-Buc, F., Fox, E. B., and Garnett, R.\n(eds.), Advances in Neural Information Processing Sys-\ntems 32: Annual Conference on Neural Information Pro-\ncessing Systems 2019, NeurIPS 2019, December 8-14,\n2019, Vancouver, BC, Canada, pp. 12360–12371, 2019.\nURL https://proceedings.neurips.cc/p\naper/2019/hash/1e8a19426224ca89e83ce\nf47f1e7f53b-Abstract.html.\nZhang, S., Roller, S., Goyal, N., Artetxe, M., Chen, M.,\nChen, S., Dewan, C., Diab, M. T., Li, X., Lin, X. V.,\nMihaylov, T., Ott, M., Shleifer, S., Shuster, K., Simig,\nD., Koura, P. S., Sridhar, A., Wang, T., and Zettlemoyer,\nL. OPT: open pre-trained transformer language models.\nCoRR, abs/2205.01068, 2022. doi: 10.48550/ARXIV.2\n205.01068. URL https://doi.org/10.48550\n/arXiv.2205.01068.\nZhang, Y., Chen, C., Ding, T., Li, Z., Sun, R., and Luo,\nZ. Why transformers need adam: A hessian perspec-\ntive.\nIn Globersons, A., Mackey, L., Belgrave, D.,\nFan, A., Paquet, U., Tomczak, J. M., and Zhang, C.\n(eds.), Advances in Neural Information Processing Sys-\ntems 38: Annual Conference on Neural Information\nProcessing Systems 2024, NeurIPS 2024, Vancouver,\nBC, Canada, December 10 - 15, 2024, 2024.\nURL\nhttp://papers.nips.cc/paper_files/p\n14\n\nPeri-LN: Revisiting Normalization Layer in the Transformer Architecture\naper/2024/hash/ee0e45ff4de76cbfdf070\n15a7839f339-Abstract-Conference.html.\nZhou, C., Liu, P., Xu, P., Iyer, S., Sun, J., Mao, Y., Ma, X.,\nEfrat, A., Yu, P., Yu, L., Zhang, S., Ghosh, G., Lewis,\nM., Zettlemoyer, L., and Levy, O. LIMA: less is more\nfor alignment. In Oh, A., Naumann, T., Globerson, A.,\nSaenko, K., Hardt, M., and Levine, S. (eds.), Advances\nin Neural Information Processing Systems 36: Annual\nConference on Neural Information Processing Systems\n2023, NeurIPS 2023, New Orleans, LA, USA, December\n10 - 16, 2023, 2023. URL http://papers.nip\ns.cc/paper_files/paper/2023/hash/ac6\n62d74829e4407ce1d126477f4a03a-Abstrac\nt-Conference.html.\n15\n\nPeri-LN: Revisiting Normalization Layer in the Transformer Architecture\nA. Related Work\nActivation Dynamics in Large Language Models.\nStudies on the distribution and magnitude of activations in deep\nneural networks have revealed that certain outlier features can significantly affect model behavior and efficiency. Dettmers\net al. (2022) examined Transformer architectures, highlighting how specific feature dimensions may exhibit unusually large\nvalues (outliers) that disrupt quantization and overall system performance. Extending this line of work, Sun et al. (2024)\nidentified the occurrence of “massive activations”—extremely large activation values that persist across multiple layers.\nUnlike standard outliers, these massive activations remain relatively invariant to different inputs, effectively functioning as\nimplicit bias terms in large language models (LLMs). Notably, such extreme values can skew the self-attention mechanism,\ncausing the model to attend disproportionately to certain tokens. These observations demonstrate that even with standard\nnormalization layers in place, hidden biases may linger in internal representations, underscoring the importance of deeper\nanalyses of activation behavior in LLMs.\nVariance Control and Normalization in Convolutional Networks.\nThe interplay between activation variance and\ntraining stability has also been extensively explored in convolutional neural networks (CNNs). De & Smith (2020) showed\nthat Batch Normalization (BN) stabilizes the training of residual networks by effectively downscaling activation variance in\nthe residual branches, thereby improving gradient behavior. However, BN imposes certain constraints, such as dependence on\nbatch size and additional computational overhead for estimating batch statistics. Consequently, several normalization-free or\nalternative normalization approaches have been investigated. For instance, Brock et al. (2021b) introduced “Normalizer-Free\nResNets,” which manage activation variance through learnable scaling parameters. This approach achieved competitive\nperformance without relying on BN, highlighting the critical role of effective variance control in fostering stable optimization\nand strong generalization in CNNs.\nLayer Normalization in Transformers.\nTraining stability in Transformer architectures is closely tied to the choice and\nplacement of layer normalization (LN). Xiong et al. (2020) reported that Transformers employing a Post-LN structure\noften suffer from gradient instabilities at initialization, requiring a careful learning-rate warm-up phase to mitigate these\nissues. In contrast, Pre-LN helps maintain more stable gradients during the early stages of training. However, Kedia\net al. (2024) showed that while Post-LN can lead to vanishing or exploding gradients in deep Transformers, Pre-LN may\ninduce hyperbolic gradient growth. These findings illustrate the nuanced trade-offs of normalization placement and draw\nparallels to earlier CNN studies, where careful management of activation variance proved essential for stable deep learning.\nDing et al. (2021) introduced Sandwich-LN in the vision domain for the first time, yet they paid little attention to the\nstructural characteristics and differences that distinguish one LN placement from another. In language domain, several\nmajor open-source language models (e.g., Olmo2 (OLMo et al., 2024), Gemma2 (Rivi`ere et al., 2024), and Gemma3\n(Team et al., 2025)) already employ a Peri-LN-like structure (see Section 3). Nevertheless, previous technical reports have\nnot explained why this design might be advantageous compared with the more widely studied Pre-LN and Post-LN. By\ninvestigating Peri-LN in detail, we aim to highlight the structural benefits that appear to underlie its empirical success in\nthese implementations.\nGradient Propagation and Depth Scaling\nEnsuring consistent gradient propagation across many layers is pivotal for\nstable training in very deep models. Yang & Hu (2021) (Tensor Programs IV) introduced the concept of Maximal Update\nParametrization (µP) in the infinite-width regime to preserve feature learning, preventing gradients from collapsing into\nkernel-like dynamics. Building on this, Yang et al. (2024) (Tensor Programs VI) proposed Depth-µP, which scales residual\nbranches and learning rates according to network depth. Their theoretical analysis indicates that improper depth-dependent\nscaling leads to vanishing or exploding gradients, ultimately diminishing the diversity of learned representations. These\ninsights highlight the necessity for principled scaling strategies and careful initialization to maintain robust gradient flow in\ndeep architectures.\nSummary.\nTaken together, these studies underscore the importance of managing activation variance and hidden biases to\nachieve stable training and expressive internal representations in modern deep networks. In Transformer-based models,\nnormalization choice and placement—such as Post-LN, Pre-LN, or other variants—play a significant role in controlling\ngradient dynamics and overall performance. While Post-LN and Pre-LN have received significant attention, we focus on a\ncomparative analysis that includes Peri-LN, an alternative normalization placement that has thus far been underexplored but\nholds potential for enhancing training stability and model performance.\n16\n\nPeri-LN: Revisiting Normalization Layer in the Transformer Architecture\nB. Proposition 3.1 of Post-LN\nProposition B.1. Post-LN (vanishing gradient). Consider the following sequence of operations:\na = MLP(x), o = x + a, ˜o = Norm(o),\n(9)\nthen\n\r\r\r\r\r\n∂L(˜o)\n∂W (2)\ni,j\n\r\r\r\r\r ≤4 γ\n√\nD ∥h∥\n∥x + a∥\n,\n(10)\nwhere h := ReLU\n\u0000xW (1) + b(1)\u0001\n. Since Post-LN normalizes the hidden state after each residual addition along the main\npath, the activation norm |h| tends to remain relatively moderate. As a result, in Post-LN, the gradient scale is less sensitive\nto the magnitude of activations and more significantly influenced by model depth, as previously analyzed by Xiong et al.\n(2020) and Kedia et al. (2024), leading to vanishing gradients as depth increases.\nC. Proof of Theoretical Insight\nTo support the claim that Peri-LN enhances the stability of training in such cases, we analyze the gradient norm in the final\nlayer. For simplicity, we use RMSNorm and ReLU here. Here, we assume that γ, the scaling parameter used in RMSNorm,\nis positive, and we empirically verified that it remains strictly positive across models of all sizes during training.\nProposition C.1. Consider the following sequence of operations:\n˜x = RMSNorm(x),\na = ReLU(˜xW (1) + b(1))W (2) + b(2),\no = x + a.\nThen,\n∂L(o)\n∂W (2)\ni,j\n= hi(ˆpj −yj),\n(11)\nwhere h := ReLU\n\u0000xW (1) + b(1)\u0001\n, ˆp := softmax(o), and y is the label (one-hot vector).\nProof. By the chain rule,\n∂L(o)\n∂W (2)\ni,j\n= ∂L(o)\n∂o\n| {z }\n(a):1×D\n×\n∂o\n∂a\n|{z}\n(b):D×D\n×\n∂a\n∂W (2)\ni,j\n| {z }\n(c):D×1\n.\n(12)\n(a) It is known that\n∂L(o)\n∂ok\n= ˆpk −yk.\n(13)\nSo,\n∂L(o)\n∂o\n=\n\u0002ˆp1 −y1\nˆp2 −y2\n· · ·\nˆpD −yD\n\u0003\n.\n(14)\n(b) Since o = x + a,\n∂o\n∂a = I.\n(15)\n(c) Recall that\na := ReLU(˜xW (1) + b(1))W (2) + b(2).\n(16)\nFor convenience, let\nh := ReLU(˜xW (1) + b(1)).\n(17)\n17\n\nPeri-LN: Revisiting Normalization Layer in the Transformer Architecture\nThen, we have\n∂ak\n∂W (2)\ni,j\n=\n∂\n∂W (2)\ni,j\n H\nX\np=1\nhpW (2)\np,k + b(2)\nk\n!\n= hi δk,j.\n(18)\nIn vector representation,\n∂a\n∂W (2)\ni,j\n=\n\u00020\n· · ·\nhi\n· · ·\n0\u0003⊤,\n(19)\nwhere the only nonzero entry is in the j-th component.\nThus, by putting these all together,\n∂L(o)\n∂W (2)\ni,j\n= hi(ˆpj −yj).\n(20)\nProposition C.2. Consider the following sequence of operations:\n˜x = RMSNorm(x),\na = ReLU(˜xW (1) + b(1))W (2) + b(2),\n˜a = RMSNorm(a),\no = x + ˜a.\nThen,\n\r\r\r\r\r\n∂L(o)\n∂W (2)\ni,j\n\r\r\r\r\r ≤4γ\n√\nD∥h∥\n∥a∥\n,\n(21)\nwhere γ is the scaling parameter used in RMSNorm(·), D is the dimensionality, and h := ReLU\n\u0000xW (1) + b(1)\u0001\n.\nProof. By the chain rule,\n∂L(o)\n∂W (2)\ni,j\n= ∂L(o)\n∂o\n| {z }\n(a):1×D\n×\n∂o\n∂˜a\n|{z}\n(b):D×D\n×\n∂˜a\n∂a\n|{z}\n(c):D×D\n×\n∂a\n∂W (2)\ni,j\n| {z }\n(d):D×1\n.\n(22)\n(a) We have\n\r\r\r\r\n∂L(o)\n∂o\n\r\r\r\r = ∥ˆp −y∥≤∥ˆp∥+ ∥y∥= 2.\n(23)\n(b) We also have\n\r\r\r\r\n∂o\n∂˜a\n\r\r\r\r = ∥I∥= 1.\n(24)\n(c) Recall that\n˜a := RMSNorm(a) = γ ·\na\nq\n1\nD\nPD\nk=1 a2\nk + ϵ\n.\n(25)\nThen, ∂˜a\n∂a is the Jacobian matrix J of RMSNorm(·). For brevity, let\nα := 1\nD\nD\nX\nk=1\n(ak)2.v\n(26)\n18\n\nPeri-LN: Revisiting Normalization Layer in the Transformer Architecture\nThen,\nJp,q = ∂˜ap\n∂aq\n= γ ·\n∂\n∂aq\n\u0012\nap\n√α + ϵ\n\u0013\n(27)\n= γ ·\n1\n√α + ϵ\n∂ap\n∂aq\n+ γ · ap\n∂\n∂aq\n\u0012\n1\n√α + ϵ\n\u0013\n(28)\n= γ ·\n1\n√α + ϵδp,q −γ ·\napaq\nD(α + ϵ)3/2 .\n(29)\nIn matrix representation,\nJ =\nγ\n√α + εI\n|\n{z\n}\nA\n−\nγ\nD(α + ε)3/2 (a)⊤(a)\n|\n{z\n}\nB\n.\n(30)\nThen, we have\n∥A∥=\n\r\r\r\r\nγ\n√α + εI\n\r\r\r\r =\nγ\n√α + ε∥I∥=\nγ\n√α + ε,\n(31)\nand\n∥B∥=\n\r\r\r\r\nγ\nD(α + ε)3/2 (a)⊤(a)\n\r\r\r\r =\nγ\nD(α + ε)3/2 × Dα =\nγα\n(α + ε)3/2 .\n(32)\nSo, we have\n∥J∥= ∥A −B∥≤∥A∥+ ∥B∥≤2γ\n√α = 2γ\n√\nD\n∥a∥.\n(33)\n(d) Since\n∂a\n∂W (2)\ni,j\n=\n\u00020\n· · ·\nhi\n· · ·\n0\u0003⊤,\n(34)\nwe have\n\r\r\r\r\r\n∂a\n∂W (2)\ni,j\n\r\r\r\r\r ≤∥h∥.\n(35)\nThus,\n\r\r\r\r\r\n∂L(o)\n∂W (2)\ni,j\n\r\r\r\r\r ≤2 × 1 × 2γ\n√\nD\n∥a∥\n× ∥h∥= 4γ\n√\nD∥h∥\n∥a∥\n.\n(36)\nProposition C.3. Consider the following sequence of operations:\na = ReLU(xW (1) + b(1))W (2) + b(2),\no = x + a,\n˜o = RMSNorm(o).\nThen,\n\r\r\r\r\r\n∂L(˜o)\n∂W (2)\ni,j\n\r\r\r\r\r ≤4γ\n√\nD∥h∥\n∥x + a∥,\n(37)\nwhere γ is the scaling parameter used in RMSNorm(·), D is the dimensionality, and h := ReLU\n\u0000xW (1) + b(1)\u0001\n.\nProof. The proof is analogous to the proof of the previous proposition.\n19\n\nPeri-LN: Revisiting Normalization Layer in the Transformer Architecture\nD. Detailed Experimental Setting\nIn this section, we provide detailed configurations of both the pretraining and supervised fine-tuning to reproduce our results.\nD.1. Configurations on Pre-Training\nThe common training settings are provided in Table 2. Embedding settings for the language models are listed in Table 3.\nFor the model architecture, we primarily follow the Llama 3 architecture (Dubey et al., 2024). In the MLP module, we use\nSwiGLU activations. Additional details regarding the model configurations are shown in Table 4. Note that embedding\nparameters are excluded from the model size. Unless otherwise noted, most training and model settings follow those of the\nDCLM experiments (Li et al., 2024a). During the pretraining stage, each model was trained under a controlled random seed.\nTable 2. Common configurations. LR Schedule denotes learning rate schedule.\nGlobal Batch Size\nWeight Decay\nIterations\nOptimizer\nLR Schedule\nWarmup\nWeight Initialization\n256\n0.033\n14400\nAdam\nCosine\n10%\n0.02\nTable 3. Embedding configurations.\nMax Position Embeddings\nPosition Embedding Type\nUntie-embeddings-and-output-weights\n8192\nRope\nTrue\nTable 4. Model configurations.\nSize\nnlayers\nnheads\ndmodel\ndhead\n400M\n24\n8\n1024\n128\n1.5B\n24\n16\n2048\n128\n3.2B\n32\n16\n2560\n160\nD.2. Configurations on Supervised Fine-Tuning\nTo examine downstream task performance after instruction tuning, we employed a high-quality LIMA alignment training\nset consisting of 1, 000 samples (Zhou et al., 2023). Our supervised fine-tuning configuration was slightly modified from\nthe original setup of LIMA: we fine-tuned the model for 15 epochs with a batch size of 128. The optimizer was Adam\nwith an initial learning rate of 1e-5 and a cosine learning rate schedule. We selected the best checkpoints for each model\nby evaluating on OpenBookQA (Mihaylov et al., 2018), CommonSenseQA (Talmor et al., 2019), and the NLI dataset in\nGLUE (Wang et al., 2018).\n20\n\nPeri-LN: Revisiting Normalization Layer in the Transformer Architecture\nE. Additional Results on Pre-Training Study\nE.1. Post-Layer Normalization Architecture & Learning Rate Exploration\nIn order to identify the optimal performance configuration for Post-LN within the experimental setup, we conducted a\nlearning rate exploration as shown in Figure 12. Because the appropriate learning rate for Post-LN fell into a much lower\nrange than those for Pre-LN and Peri-LN, we treated it separately. For each Post-LN setting, the best learning rate was\ndetermined as the one yielding the lowest final training loss, with the random seed held constant during this selection\nprocess.\nFigure 12. Learning rate explorations for Post-LN architectures.\nE.2. Best Performing Checkpoints Comparisons of Other Model Sizes\nAs an extension of Section 4.2, we present below the results for additional model sizes that were omitted previously due\nto space constraints. In Figures 13, we compare the pre-training loss and the gradient norm curve at each LN strategy’s\nbest-performing learning rate of 3.2B and 1.5B size models.\n(a) Training loss for 3.2B\n(b) Gradient-norm for 3.2B\n(c) Training loss for 1.5B\n(d) Gradient-norm for 1.5B\nFigure 13. Performance comparison of Post-LN, Pre-LN, and Peri-LN Transformers during pre-training for other two.\n21\n\nPeri-LN: Revisiting Normalization Layer in the Transformer Architecture\nF. Additional Results on Growth of Hidden State\nIn this section, we examine the 400M- and 3.2B-parameter models, which were omitted in Section 5.1 due to space\nconstraints. As illustrated in Figures 14 and 15, these models exhibit the same overall trend.\n(a) Absolute magnitude growth\n(b) Variance growth\nFigure 14. The forward growth patterns of hidden state for different architectures highlight the structural impact of normalization\nplacement. 3.2B size model.\n(a) Absolute magnitude growth\n(b) Variance growth\nFigure 15. The forward growth patterns of hidden state for different architectures highlight the structural impact of normalization\nplacement. 400M size model.\n22\n\nPeri-LN: Revisiting Normalization Layer in the Transformer Architecture\nG. Additional Experimental Results on Ablation Study\nG.1. Amount of Training Tokens\nIn order to investigate whether the learning behavior of each LN strategy varies with the number of training tokens, we\nconducted an additional round of learning-rate exploration for both the Pre-LN and Peri-LN architectures. As shown in\nFigure 16, even as the number of training tokens increases, there is no observable shift in the optimal learning-rate range.\nBased on these findings, we conclude that our overall results remain consistent, even when the training token count is further\nincreased. Furthermore, although a learning rate of 5 × 10−3 leads to divergence in the smaller-scale experiments with 8B\nor 16B training tokens, it does not do so in the 30B-token setting. We attribute this discrepancy to the 10% warmup rate,\nsuggesting that the warmup phase may be insufficient for the smaller-scale experiments.\nFigure 16. Learning rate explorations of Pre-& Peri-LN architecture with sequence length 2048 configuration.\nG.2. Sequence Length\nIn language models, the number of iterations per token is influenced by the sequence length, which in turn, along with the\nbatch size, affects training statistics. We conducted an experiment to determine whether the performance trend changes\nwhen the sequence length is reduced from 8192, as set in the main text, to 2048. As shown in Figure 17, Peri-LN still\nsurpasses Pre-LN in the learning rate exploration.\nFigure 17. Learning rate explorations of Pre-& Peri-LN architecture with sequence length 2048 configuration.\n23\n\nPeri-LN: Revisiting Normalization Layer in the Transformer Architecture\nG.3. Warm-up\nWarmup is widely recognized to influence training stability. To investigate whether a 10% warmup rate might unfairly\ndisadvantage Pre-LN, we conducted an additional learning-rate exploration using a 30% warmup rate. As illustrated in\nFigure 18, the overall trend remained unchanged, and Peri-LN continued to exhibit better performance than Pre-LN in\nterms of loss. Furthermore, we observed that increasing the warmup rate from 10% to 30% did not reduce the frequency of\ngradient norm spikes in Pre-LN.\nFigure 18. Learning rate explorations of Pre-& Peri-LN architecture with warmup 30% configuration.\nG.4. RMSNorm & LayerNorm\nAs illustrated in Figure 19, we conducted experiments in which RMSNorm and LayerNorm were interchanged. Consistent\nwith the findings reported in (OLMo et al., 2024), we did not observe any notable performance differences in our RMSNorm\nand LayerNorm replacement experiments. Learning rate was set to 2e-3 (best performance learning rate).\n(a) Pre-training loss curve\n(b) Gradient-norm curve\nFigure 19. LayerNorm vs. RMSNorm on Peri-LN architecture. 400M size model.\n24\n\nPeri-LN: Revisiting Normalization Layer in the Transformer Architecture\nG.5. Embedding Layer Normalization of Peri-Layer Normalization Transformers\nMotivated by Takase et al. (2023), we empirically explore the addition of embedding layer normalization to improve training\nstability and overall model performance in Transformer architectures. As illustrated in Figures 20, 21, and 22, incorporating\nEmbedding LN in the Peri-LN architecture yields a slight improvement in pre-training loss. Furthermore, our empirical\nobservations suggest that this effect becomes more pronounced in smaller models.\n(a) Pre-training loss curve\n(b) Gradient-norm curve\nFigure 20. Loss and Gradient-norm curves comparing the presence and absence of Embedding LN in the Peri-LN architecture. 400M size\nmodel.\n(a) Pre-training loss curve\n(b) Gradient-norm curve\nFigure 21. Loss and Gradient-norm curves comparing the presence and absence of Embedding LN in the Peri-LN architecture. 1.5B size\nmodel.\n(a) Pre-training loss curve\n(b) Gradient-norm curve\nFigure 22. Loss and Gradient-norm curves comparing the presence and absence of Embedding LN in the Peri-LN architecture. 3.2B size\nmodel.\n25\n\nPeri-LN: Revisiting Normalization Layer in the Transformer Architecture\nG.6. Ablation Study on Additional Normalization Layer Placement\nWe additionally conduct further experiments on LN placements to compare different combinations (referred to as A, B, and\nC positions in Figure 2). We add configurations where LN is placed at both A + C (akin to combining Pre- and Post-LN), as\nwell as only at B, to compare them with Peri-LN at final training loss under the controlled same training seed. We pre-train\nthe 400M-parameter Transformers on 30B tokens each, using the same training configurations described in Section 4. As\naligned with Xiong et al. (2020), our new results confirm that placing LN exclusively at C leads to training instability or\nsuboptimal performance. In particular, the A + C configuration inherits characteristics of Post-LN (large gradient norm\nshifts), forcing the use of smaller learning rates and still resulting in lower overall performance than Peri-LN architecture.\nTable 5. Final training loss and additional normalization layer placement.\n400M\nA + C\nPost-LN\nB\nPeri-LN\nFinal Training Loss\n3.01\n3.05\nDiverged\n2.91\nG.7. Peri-LN with QK-Norm\nWhile Peri-LN alone provides robust training dynamics, QK-Norm can still enhance performance. We conducted additional\nexperiments that confirm combining Peri-LN with QK-Norm yields slight improvements in training loss. We pre-train the\n1.5B-parameter Transformers on 30B tokens each, using the same training configurations described in Section 4. As shown\nin Table 6, adding QK-norm to Peri-LN indeed yielded better performance, consistent with Wortsman et al. (2024). In this\nexperiment, the Peri-LN variant equipped with QK-norm used LayerNorm instead of RMSNorm.\nTable 6. Peri-LN with QK-Norm.\n1.5B\nPeri-LN\n+QK-Norm (Wortsman et al., 2024)\nFinal Training Loss\n2.722\n2.711\n26\n\nPeri-LN: Revisiting Normalization Layer in the Transformer Architecture\nG.8. Weight Decay and Weight Initialization\nG.8.1. COMMON SETTINGS\nWe pre-train the 400M-parameter Transformers on 30B tokens each under the controlled same training seed. We measure\nthe training loss and averaged benchmark score for these experiments under the same evaluation settings used in Table 1 of\nthe paper. Other configurations follow those outlined in Section 4. For the variance growth experiments in Figure 10, we\nadopt the same settings as in Section 5.1, except that we use 100 samples for the forward-pass statistics.\nG.8.2. WEIGHT DECAY\nWe conduct additional studies for various weight decay condition for both Pre-LN and Peri-LN architectures. As shown in\nthe Table 7, Peri-LN continues to offer better performance than Pre-LN under the same settings. We provide per-run results\nas below:\nTable 7. Effect of weight decay on 400M-parameter Pre-LN and Peri-LN Transformers: Final training loss and averaged benchmark score.\n400M\nWeight Decay Coefficient\n0\n0.0033\n0.033\n0.33\nFinal Training Loss\nPre-LN\n3.03\n3.03\n3.03\n3\nPeri-LN\n2.94\n2.94\n2.93\n2.90\nAveraged Benchmark Score\nPre-LN\n49.26\n49.18\n49.01\n49.51\nPeri-LN\n51.41\n51.14\n50.68\n52.13\n(a) Pre-training loss curve\n(b) Gradient-norm curve\nFigure 23. Comparison of pre-training loss and gradient norms for Pre-LN and Peri-LN architectures with the weight decay coefficient\nfixed at 0, while all other hyperparameters are held constant.\n(a) Pre-training loss curve\n(b) Gradient-norm curve\nFigure 24. Comparison of pre-training loss and gradient norms for Pre-LN and Peri-LN architectures with the weight decay coefficient\nfixed at 0.0033, while all other hyperparameters are held constant.\nG.8.3. WEIGHT INITIALIZATION\nWe run an additional ablation on weight-initialization schemes. For both Pre-LN and Peri-LN models, we first adopt Xavier\ninitialization (Glorot & Bengio, 2010) and then compare it with He initialization (2/d) (He et al., 2015), LeCun initialization\n27\n\nPeri-LN: Revisiting Normalization Layer in the Transformer Architecture\n(a) Pre-training loss curve\n(b) Gradient-norm curve\nFigure 25. Comparison of pre-training loss and gradient norms for Pre-LN and Peri-LN architectures with the weight decay coefficient\nfixed at 0.033, while all other hyperparameters are held constant.\n(a) Pre-training loss curve\n(b) Gradient-norm curve\nFigure 26. Comparison of pre-training loss and gradient norms for Pre-LN and Peri-LN architectures with the weight decay coefficient\nfixed at 0.33, while all other hyperparameters are held constant.\n(1/d), and two scaled variants, 10/d and 1/(10d).\nAs Table 8 shows, Xavier initialization yields the strongest overall performance, improving on the configurations used\nin our earlier experiments. Crucially, our central finding remains intact: hidden-state variance sharply grows in Pre-LN\nTransformers but stays bounded in Peri-LN Transformers. Table 9 confirms the same pattern across all initialization scales,\nand detailed per-run results appear below.\nTable 8. Xavier initialization (Glorot & Bengio, 2010) yields better performance compared to our previous weight initialization configura-\ntions.\n400M\nArchitecture\nBaseline(0.02)\nXavier Initialization\nLoss\nPre-LN\n3.03\n2.95\nPeri-LN\n2.93\n2.91\nAvg.\nPre-LN\n49.01\n51.25\nPeri-LN\n50.68\n52.04\nTable 9. Effect of weight-initialization variance on final pre-training loss for 400M-parameter Pre-LN and Peri-LN Transformers.\n400M\nInitialization Variance\n10/d\nHe (2/d)\nLeCun (1/d)\n1/(10d)\nBaseline (0.02)\nLoss\nPre-LN\n4.526\n2.965\n3.005\n3.012\n3.035\nPeri-LN\n3.027\n2.929\n2.915\n2.902\n2.916\n28\n\nPeri-LN: Revisiting Normalization Layer in the Transformer Architecture\n(a) Pre-training loss curve\n(b) Gradient-norm curve\nFigure 27. Comparison of pre-training loss and gradient norms for Pre-LN and Peri-LN architectures with the weight initialization variance\nset to 10/d, while all other hyperparameters are held constant.\n(a) Pre-training loss curve\n(b) Gradient-norm curve\nFigure 28. Comparison of pre-training loss and gradient norms for Pre-LN and Peri-LN architectures with the weight initialization variance\nset to 2/d (He init (He et al., 2015)), while all other hyperparameter are held constant.\n(a) Pre-training loss curve\n(b) Gradient-norm curve\nFigure 29. Comparison of pre-training loss and gradient norms for Pre-LN and Peri-LN architectures with the weight initialization variance\nset to 1/d (LeCun init), while all other hyperparameter are held constant.\n(a) Pre-training loss curve\n(b) Gradient-norm curve\nFigure 30. Comparison of pre-training loss and gradient norms for Pre-LN and Peri-LN architectures with the weight initialization variance\nset to 1/(10d), while all other hyperparameter are held constant.\n29\n\nPeri-LN: Revisiting Normalization Layer in the Transformer Architecture\nH. Output-Layer Normalization with QK-Norm Architecture\nQuery and Key layer-normalization (QK-Norm) has been widely studied in modern Transformer architectures (Wortsman\net al., 2024; Zhai et al., 2023; OLMo et al., 2024). In particular, OLMo et al. (2024) reported that QK-Norm combined\nwith module output layer-normalization (output-LN, B in Figure 31 referred to as “reordered norm” in the OLMo2 paper)\nimproves both training loss and stability. As shown in Figure 31, QK-Norm is applied after the Query and Key projections,\nsimilar to output-LN. From another perspective, QK-Norm is also applied immediately before the attention calculation, akin\nto a Pre-LN approach. In our view, QK-Norm and Pre-LN (placed at A2 and A respectively in Figure 31) serve the same\nrole but differ in certain details. As shown in Figures 32, 33, and 34, the two architectures exhibit comparable performance\noverall in terms of both training loss and stability.. However, Peri-LN provides a slight performance advantage over the\nOLMo2-style Peri-LN in the 400M- and 1B-parameter models.\nFigure 31. QK-layer normalization in the Attention module.\n(a) Pre-training loss curve\n(b) Gradient-norm curve\nFigure 32. Comparison of pre-training loss and gradient norm between OLMo2-Style Peri-LN and the Peri-LN architecture. To ensure an\naccurate comparison, we present the pre-training loss over the final 5B tokens. 400M size model.\n30\n\nPeri-LN: Revisiting Normalization Layer in the Transformer Architecture\n(a) Pre-training loss curve\n(b) Gradient-norm curve\nFigure 33. Comparison of pre-training loss and gradient norm between OLMo2-Style Peri-LN and the Peri-LN architecture. To ensure an\naccurate comparison, we present the pre-training loss over the final 5B tokens. 1.5B size model.\n(a) Pre-training loss curve\n(b) Gradient-norm curve\nFigure 34. Comparison of pre-training loss and gradient norm between OLMo2-Style Peri-LN and the Peri-LN architecture. To ensure an\naccurate comparison, we present the pre-training loss over the final 5B tokens. 3.2B size model.\nI. Training the Transformer using Stochastic Gradient Descent\nUsing Stochastic Gradient Descent (SGD) for training Transformers is not a common practice. As Zhang et al. (2024) point\nout, Transformer-based models tend to perform worse with SGD than with Adam by a considerable margin. One reason is\nthat SGD struggles to handle the heterogeneity across different blocks. Although these aspects are certainly intriguing and\nwarrant further investigation, they lie beyond the scope of our current work, as Zhang et al. (2024) also note.\nNonetheless, for someone who might wonder to know, we conduct additional experiments using SGD. We are searching\nfor U-shaped patterns during the learning rate exploration for both Pre-LN & Peri-LN as shown in the Figure 35. We\nobserve that: (1) SGD performs worse than Adam, consistent with findings reported in (Zhang et al., 2024); and (2) Peri-LN\ndemonstrates better performance than Pre-LN. In these results, we use 400 M-parameter Transformers and apply the same\nconfigurations as in the main experiments (Section 4.1). We provide detailed training curves in Figures 36, 37, 38.\nFigure 35. Learning Rate Exploration of Pre-& Peri-LN architecture trained with SGD optimizer. The individual training-loss and\ngradient-norm curves appear in Figures 36, 37, and 38.\n31\n\nPeri-LN: Revisiting Normalization Layer in the Transformer Architecture\n(a) Pre-training loss curve\n(b) Gradient-norm curve\nFigure 36. Using SGD with learning rate 5e −3.\n(a) Pre-training loss curve\n(b) Gradient-norm curve\nFigure 37. Using SGD with learning rate 3e −3.\n(a) Pre-training loss curve\n(b) Gradient-norm curve\nFigure 38. Using SGD with learning rate 1e −3.\n32\n\nPeri-LN: Revisiting Normalization Layer in the Transformer Architecture\nJ. Additional Details on Evaluation\nJ.1. Detailed Configurations\nWe utilized the Language Model Evaluation Harness6with the HuggingFace Transformers library (Gao et al., 2024; Wolf\net al., 2020) to assess overall performance. We employ five different evaluation benchmarks: ARC (Clark et al., 2018),\nHellaSwag (Zellers et al., 2019), PIQA (Bisk et al., 2020), SIQA (Sap et al., 2019), Winogrande (Sakaguchi et al., 2021).\nDuring the pretraining stage, each model was trained under a controlled random seed. We used the training loss at iteration\n14, 000—corresponding to the completion of 30B tokens—as our main reference point. When calculating the evaluation\nscore, diverged checkpoints were excluded.\nJ.2. Detailed Results on Benchmark Evaluations\nIn this section, we present the evaluation results for each model trained with five different training seeds. We exclude any\ndiverged scores and average the remaining values, which are then reported in Table 1 in the main text.\nJ.2.1. PRE-TRAINING\nTable 10. Detailed results on pre-training the Peri-LN architecture. These results are averaged to produce the values reported in Table 1.\nSEED denotes pre-training seed.\nPeri-LN\nSEED\nARC-Easy\nHellaSwag\nPIQA\nSIQA\nWinogrande\n1\n0.5758\n0.3803\n0.6980\n0.4115\n0.5225\n2\n0.5728\n0.3739\n0.6915\n0.4017\n0.5367\n400M\n3\n0.5842\n0.3745\n0.6986\n0.4125\n0.5249\n4\n0.5800\n0.3722\n0.6959\n0.4038\n0.5209\n5\n0.5627\n0.3719\n0.6899\n0.4028\n0.5320\n1\n0.6599\n0.4437\n0.7339\n0.4304\n0.5714\n2\n0.6591\n0.4394\n0.7399\n0.4145\n0.5699\n1.5B\n3\n0.6625\n0.4357\n0.7372\n0.4166\n0.5627\n4\n0.6633\n0.4367\n0.7345\n0.4222\n0.5667\n5\n0.6637\n0.4416\n0.7361\n0.4335\n0.5612\n1\n0.6953\n0.4734\n0.7443\n0.4417\n0.5872\n2\n0.6839\n0.4684\n0.7427\n0.4324\n0.6054\n3.2B\n3\n0.6902\n0.4680\n0.7486\n0.4243\n0.5967\n4\n0.6864\n0.4700\n0.7427\n0.4273\n0.5935\n5\n0.6806\n0.4698\n0.7372\n0.4243\n0.6054\nTable 11. Detailed results on pre-training the Pre-LN architecture. These results are averaged to produce the values reported in Table 1.\nSEED denotes pre-training seed.\nPre-LN\nSEED\nARC-Easy\nHellaSwag\nPIQA\nSIQA\nWinogrande\n1\n0.5669\n0.3609\n0.7008\n0.4002\n0.5359\n2\nDiverged\n400M\n3\n0.5354\n0.3328\n0.6741\n0.3905\n0.4957\n4\nDiverged\n5\n0.5438\n0.3314\n0.6888\n0.4012\n0.4949\n1\n0.6326\n0.4259\n0.7242\n0.4263\n0.5691\n2\n0.6019\n0.3924\n0.7111\n0.3992\n0.5627\n1.5B\n3\n0.6077\n0.3932\n0.7008\n0.4125\n0.5272\n4\n0.6111\n0.3886\n0.7187\n0.4135\n0.5225\n5\n0.6221\n0.3941\n0.7160\n0.4099\n0.5438\n1\n0.6688\n0.4588\n0.7470\n0.4273\n0.5919\n2\nDiverged\n3.2B\n3\nDiverged\n4\n0.6359\n0.4259\n0.7301\n0.4263\n0.5564\n5\nDiverged\n6https://github.com/EleutherAI/lm-evaluation-harness\n33\n\nPeri-LN: Revisiting Normalization Layer in the Transformer Architecture\nTable 12. Detailed results on pre-training the Post-LN architecture. These results are averaged to produce the values reported in Table 1.\nSEED denotes pre-training seed.\nPost-LN\nSEED\nARC-Easy\nHellaSwag\nPIQA\nSIQA\nWinogrande\n1\n0.3413\n0.2881\n0.6311\n0.3378\n0.5067\n2\n0.3691\n0.2886\n0.6132\n0.3337\n0.5099\n400M\n3\n0.3632\n0.2889\n0.6257\n0.3603\n0.5051\n4\n0.3603\n0.2920\n0.6262\n0.3490\n0.5012\n5\n0.3510\n0.2880\n0.6170\n0.3434\n0.5209\n1\n0.4268\n0.3121\n0.6659\n0.3628\n0.5185\n2\n0.4196\n0.3150\n0.6654\n0.3639\n0.5004\n1.5B\n3\nDiverged\n4\n0.4285\n0.3212\n0.6730\n0.3511\n0.4775\n5\n0.4419\n0.3193\n0.6643\n0.3557\n0.5154\n1\n0.4731\n0.3427\n0.6774\n0.3664\n0.5343\n2\n0.4638\n0.3326\n0.6779\n0.3577\n0.4917\n3.2B\n3\n0.3956\n0.3321\n0.6143\n0.3408\n0.5067\n4\n0.4663\n0.3380\n0.6692\n0.3685\n0.5178\n5\n0.4663\n0.3340\n0.6839\n0.3577\n0.5043\nJ.2.2. SUPERVISED FINE-TUNING\nTable 13. Detailed results on SFT with Peri-LN architecture. These results are averaged to produce the values reported in Table 1. SEED\ndenotes pre-training seed.\nPeri-LN\nSEED\nARC-Easy\nHellaSwag\nPIQA\nSIQA\nWinogrande\n1\n0.5800\n0.3819\n0.6991\n0.4145\n0.5328\n2\n0.5783\n0.3783\n0.6921\n0.4038\n0.5391\n400M\n3\n0.5888\n0.3806\n0.6980\n0.4222\n0.5288\n4\n0.5892\n0.3738\n0.6948\n0.4089\n0.5099\n5\n0.5783\n0.3757\n0.6991\n0.4099\n0.5312\n1\n0.6633\n0.4502\n0.7356\n0.4304\n0.5746\n2\n0.6641\n0.4437\n0.7405\n0.4207\n0.5706\n1.5B\n3\n0.6671\n0.4454\n0.7454\n0.4207\n0.5620\n4\n0.6700\n0.4455\n0.7378\n0.4284\n0.5659\n5\n0.6688\n0.4478\n0.7421\n0.4324\n0.5620\n1\n0.7058\n0.4810\n0.7486\n0.4422\n0.5880\n2\n0.6898\n0.4774\n0.7437\n0.4391\n0.6054\n3.2B\n3\n0.6995\n0.4770\n0.7481\n0.4278\n0.5912\n4\n0.6911\n0.4777\n0.7432\n0.4350\n0.5943\n5\n0.6894\n0.4781\n0.7448\n0.4319\n0.6046\n34\n\nPeri-LN: Revisiting Normalization Layer in the Transformer Architecture\nTable 14. Detailed results on SFT with Pre-LN architecture. These results are averaged to produce the values reported in Table 1. SEED\ndenotes pre-training seed.\nPre-LN\nSEED\nARC-Easy\nHellaSwag\nPIQA\nSIQA\nWinogrande\n1\n0.5762\n0.3625\n0.7078\n0.4058\n0.5343\n2\nN/A\n400M\n3\n0.5370\n0.3339\n0.6757\n0.3905\n0.4972\n4\nN/A\n5\n0.5509\n0.3372\n0.6893\n0.4074\n0.4886\n1\n0.6385\n0.4310\n0.7247\n0.4227\n0.5620\n2\n0.6035\n0.3934\n0.7095\n0.4038\n0.5572\n1.5B\n3\n0.6098\n0.3944\n0.7035\n0.4150\n0.5257\n4\n0.6208\n0.3929\n0.7182\n0.4161\n0.5272\n5\n0.6258\n0.4017\n0.7171\n0.4181\n0.5391\n1\n0.6785\n0.4681\n0.7568\n0.4345\n0.5825\n2\nN/A\n3.2B\n3\nN/A\n4\n0.6427\n0.4293\n0.7274\n0.4299\n0.5580\n5\nN/A\nTable 15. Detailed results on SFT with Post-LN architecture. These results are averaged to produce the values reported in Table 1. SEED\ndenotes pre-training seed.\nPost-LN\nSEED\nARC-Easy\nHellaSwag\nPIQA\nSIQA\nWinogrande\n1\n0.4428\n0.3307\n0.6583\n0.3797\n0.5099\n2\n0.4280\n0.3208\n0.6404\n0.3746\n0.5178\n400M\n3\n0.4693\n0.3241\n0.6578\n0.3905\n0.5122\n4\n0.4680\n0.3247\n0.6610\n0.3726\n0.4830\n5\n0.4520\n0.3283\n0.6572\n0.3849\n0.5225\n1\n0.5316\n0.3774\n0.6980\n0.3889\n0.5359\n2\n0.4731\n0.3316\n0.6719\n0.3813\n0.5028\n1.5B\n3\nN/A\n4\n0.5387\n0.3546\n0.6779\n0.3864\n0.4909\n5\n0.5261\n0.3510\n0.6752\n0.3767\n0.5209\n1\n0.5623\n0.4029\n0.7008\n0.3920\n0.5051\n2\n0.5417\n0.3644\n0.6823\n0.3833\n0.5264\n3.2B\n3\n0.4444\n0.3604\n0.6333\n0.3618\n0.5043\n4\n0.5400\n0.3645\n0.6844\n0.3823\n0.5020\n5\n0.5341\n0.3677\n0.6942\n0.3976\n0.5012\n35\n\nPeri-LN: Revisiting Normalization Layer in the Transformer Architecture\nK. Additional Discussions of Precision Constraints Imposed by Pre-LN\nThis section provide additional discussions of Section 7.2. Given that both Pre-LN and Peri-LN exhibit a structural property\nwhereby large values do not readily disappear once they arise, it is important to monitor the occurrence of these extreme\nactivations. Pre-LN’s additive residual path often produces activations that approach, and occasionally exceed, the FP16\n(float16) representable maximum7. To quantify how often these values would overflow FP16 yet remain within the BF16\n(bfloat16) range8, we measure the 100 largest absolute hidden-state values (the global top-100 tokens) for each Pre-LN and\nPeri-LN 3.2B-parameter Transformer. The shaded region indicates the range of these global top-100 tokens. The blue curve\n(with shaded band) represents a Pre-LN model, the red curve a Peri-LN model, and the dashed orange line denotes the FP16\nrepresentable maximum.\nAs shown in Figure 11, consistent with the observations of Sun et al. (2024), activations in the Pre-LN model routinely\nexceed the FP16 bound from the very first 0.5B training tokens, with the overshoot becoming more pronounced in deeper\nlayers—an indication of growing numerical instability. By contrast, Peri-LN consistently maintains a comfortable margin\nbelow the FP16 limit throughout training. This finding underscores that the choice between FP16 and BF16 is not merely a\nhardware preference; it is tightly coupled to how hidden-state magnitudes evolve within the network.\nSince NVIDIA V100 GPUs do not support BF16, these results imply that training and inference with Pre-LN models on\nsuch hardware are inherently disadvantaged. Moreover, from the standpoint of large-language-model quantization (Dettmers\net al., 2022; Lee et al., 2023; Kim et al., 2023; Heo et al., 2024; Lee et al., 2025), the prevalence of massive activations in\nPre-LN can severely disrupt outlier-aware compression algorithms. When aggressive low-precision compression is the goal,\nthe Pre-LN architecture’s tendency to generate extreme hidden state values therefore constitutes a particularly challenging\nobstacle.\nMeta AI’s publicly released OPT training logbooks and chronicles document repeated episodes of gradient divergence\nand loss spikes encountered while pre-training entirely in FP16 precision 9. Since FP16 saturates at an absolute value of\n65, 504, any hidden state activation that exceeds this threshold silently overflows, corrupting the forward pass and, through\nback-propagation, inducing erratic gradients. Earlier analyses of OPT (Zhang et al., 2022) therefore ascribe much of the\nobserved instability to numerical overflow, a view formalized in our Proposition 3.1, which shows how such out-of-range\nactivations propagate into severe gradient pathologies. These external reports provide further corroboration that architectures\nprone to generating large-magnitude activations—as Pre-LN does—require either a wider numerical format (e.g., BF16) or\nexplicit magnitude-regularization to ensure stable large-scale training.\n7https://en.wikipedia.org/wiki/Bfloat16_floating-point_format\n8https://en.wikipedia.org/wiki/Half-precision_floating-point_format\n9https://github.com/facebookresearch/metaseq/blob/main/projects/OPT/chronicles/10_perce\nnt_update.md\n36\n\nPeri-LN: Revisiting Normalization Layer in the Transformer Architecture\nL. Additional Discussions of Hidden State Representations\nAs shown in Figure 39(a), Post-LN exhibits smaller angular distances due to the LN being located on the main path, whereas\nPre-LN and Peri-LN begin with very similar states. As shown in Figure 39(c), at the end of training, Pre-LN tends to\nproduce more redundant hidden state representations compared to the others. This phenomenon may stem from Pre-LN’s\nrepeated residual additions, which amplify certain representations over others. We use 30B tokens trained 400M size model\nin this experiments. For dataset, we utilize 256 random samples of RedPajama-Data-1T (Computer, 2023) for this results.\nTo investigate further, we focus on module-output normalization, the primary factor distinguishing Pre-LN from Peri-LN.\nAs shown in Figure 39(b), the learnable scale starts around 1 in the early stages of training and gradually changes with\nincreasing depth. Because Peri-LN preserves the identity path, it appears to adjust the scale of the module output accordingly.\nThis suggests that the exponential growth of the main path’s magnitude in Pre-LN diminishes the relative contribution of\nindividual modules, resulting in more redundant hidden representations. Figure 39(d) shows that fixing the learnable scale\nof Peri-LN’s module output LN at 1 causes the main path contribution to decrease in deeper layers. This finding confirms\nthe role of module output normalization in controlling hidden state redundancy.\n(a) At initialization\n(b) Learnable scale γ in Output-LN\n(c) After 30B tokens training\n(d) Frozen γ in Output-LN\nFigure 39. Angular distance of hidden state is presented in Figure 39(a), 39(c), and 39(d). In Figure 39(b), we monitor γ of every\nOutput-LN in Peri-LN during training. We use 30B tokens trained 400M size model in this experiments.\n37\n",
      "fetch_method": "direct-pdf"
    },
    {
      "arxiv_id": "https://docs.vllm.ai/en/latest/design/plugin_system.html",
      "full_text": " Plugin System - vLLM Skip to content You are viewing the latest developer preview docs. Click here to view docs for the latest stable release. vLLM Plugin System Initializing search GitHub Home User Guide Developer Guide API Reference CLI Reference Community vLLM GitHub Home User Guide User Guide Getting Started Getting Started Quickstart Installation Installation GPU CPU Google TPU AWS Neuron Examples Examples Offline Inference Offline Inference Async LLM Streaming Audio Language Automatic Prefix Caching Basic Batch LLM Inference Chat With Tools Context Extension Convert Model To Seq Cls Data Parallel Disaggregated Prefill V1 Disaggregated Prefill Dolphin Embed Jina Embeddings V3 Embed Matryoshka Fy Encoder Decoder Encoder Decoder Multimodal LLM Engine Example Load Sharded State LoRA With Quantization Inference Metrics Mistral-Small MLPSpeculator MultiLoRA Inference Offline Inference with the OpenAI Batch file format Prefix Caching Prithvi Geospatial MAE Prithvi Geospatial MAE Io Processor Profiling vLLM TPU Profiling Prompt Embed Inference Qwen2.5-Omni Offline Inference Examples Qwen3 Reranker Qwen 1M Reproducibility RLHF RLHF Colocate RLHF Utils Save Sharded State Simple Profiling Skip Loading Weights In Engine Init Spec Decode Structured Outputs Torchrun Example TPU Vision Language Vision Language Multi Image Vision Language Pooling Online Serving Online Serving API Client Helm Charts Cohere Rerank Client Disaggregated Prefill Disaggregated Serving Gradio OpenAI Chatbot Webserver Gradio Webserver Jinaai Rerank Client Kv Events Subscriber Multi-Node-Serving Multi Instance Data Parallel OpenAI Chat Completion Client OpenAI Chat Completion Client For Multimodal OpenAI Chat Completion Client With Tools OpenAI Chat Completion Client With Tools Required OpenAI Chat Completion Client With Tools Xlam OpenAI Chat Completion Client With Tools Xlam Streaming OpenAI Chat Completion Tool Calls With Reasoning OpenAI Chat Completion With Reasoning OpenAI Chat Completion With Reasoning Streaming OpenAI Chat Embedding Client For Multimodal OpenAI Classification Client OpenAI Completion Client OpenAI Cross Encoder Score OpenAI Cross Encoder Score For Multimodal OpenAI Embedding Client Long Text Embedding with Chunked Processing OpenAI Embedding Matryoshka Fy OpenAI Pooling Client OpenAI Transcription Client OpenAI Translation Client Setup OpenTelemetry POC Prithvi Geospatial MAE Prometheus and Grafana Prompt Embed Inference With OpenAI Client Ray Serve Deepseek Retrieval Augmented Generation With Langchain Retrieval Augmented Generation With Llamaindex Run Cluster Sagemaker-Entrypoint Streamlit OpenAI Chatbot Webserver Structured Outputs Utils Others Others LMCache Examples Logging Configuration Tensorize vLLM Model General General vLLM V1 Frequently Asked Questions Production Metrics Reproducibility Security Troubleshooting Usage Stats Collection Inference and Serving Inference and Serving Offline Inference OpenAI-Compatible Server Data Parallel Deployment Troubleshooting distributed deployments Expert Parallel Deployment Parallelism and Scaling Integrations Integrations LangChain LlamaIndex Deployment Deployment Using Docker Using Kubernetes Using Nginx Frameworks Frameworks Anyscale AnythingLLM AutoGen BentoML Cerebrium Chatbox Dify dstack Haystack Helm LiteLLM Lobe Chat LWS Modal Open WebUI Retrieval-Augmented Generation SkyPilot Streamlit NVIDIA Triton Integrations Integrations KServe KubeAI KubeRay Llama Stack llmaz Production stack Training Training Reinforcement Learning from Human Feedback Transformers Reinforcement Learning Configuration Configuration Conserving Memory Engine Arguments Environment Variables Model Resolution Optimization and Tuning Server Arguments TPU Optimization Tips Models Models Supported Models Generative Models Pooling Models Extensions Extensions Loading Model weights with fastsafetensors Loading models with Run:ai Model Streamer Loading models with CoreWeave's Tensorizer Hardware Supported Models Hardware Supported Models TPU Features Features Automatic Prefix Caching Disaggregated Prefilling (experimental) LoRA Adapters Multimodal Inputs Prompt Embedding Inputs Reasoning Outputs Sleep Mode Speculative Decoding Structured Outputs Tool Calling Quantization Quantization AutoAWQ AutoRound BitBLAS BitsAndBytes FP8 W8A8 GGUF GPTQModel FP8 INC INT4 W4A16 INT8 W8A8 NVIDIA TensorRT Model Optimizer Quantized KV Cache AMD Quark TorchAO Developer Guide Developer Guide General General Benchmark Suites Deprecation Policy Dockerfile Incremental Compilation Workflow Profiling vLLM Vulnerability Management Model Implementation Model Implementation Basic Model Registering a Model Unit Testing Multi-Modal Support Speech-to-Text (Transcription/Translation) Support CI CI CI Failures Update PyTorch version on vLLM OSS CI/CD Design Documents Design Documents Architecture Overview Fused MoE Modular Kernel Integration with Hugging Face Hybrid KV Cache Manager IO Processor Plugins Metrics Multi-Modal Data Processing Python Multiprocessing P2P NCCL Connector Paged Attention Plugin System Plugin System Table of contents How Plugins Work in vLLM How vLLM Discovers Plugins Types of supported plugins Guidelines for Writing Plugins Compatibility Guarantee Automatic Prefix Caching torch.compile integration API Reference API Reference vllm vllm beam_search collect_env connections env_override envs forward_context logger logits_process logprobs outputs pooling_params sampling_params scalar_type scripts sequence tasks test_utils tracing version adapter_commons adapter_commons layers models request utils worker_manager assets assets audio base image video attention attention layer selector backends backends abstract differential_flash_attn dual_chunk_flash_attn flash_attn flashmla placeholder_attn rocm_aiter_mla rocm_flash_attn triton_mla utils xformers mla mla common layers layers chunked_local_attention cross_attention encoder_only_attention ops ops chunked_prefill_paged_decode common flashmla merge_attn_states paged_attn pallas_kv_cache_update prefix_prefill rocm_aiter_mla rocm_aiter_paged_attn triton_decode_attention triton_flash_attention triton_merge_attn_states triton_unified_attention utils utils fa_utils kv_sharing_utils benchmarks benchmarks datasets latency serve throughput lib lib endpoint_request_func ready_checker utils compilation compilation activation_quant_fusion backends base_static_graph collective_fusion compiler_interface counter cuda_graph cuda_piecewise_backend decorators fix_functionalization fusion fusion_attn fx_utils inductor_pass monitor multi_output_match noop_elimination pass_manager sequence_parallelism torch25_custom_graph_pass vllm_inductor_pass wrapper config config cache compilation kv_events kv_transfer load lora parallel scheduler utils core core block_manager evictor interfaces placeholder_block_space_manager scheduler block block block_table common cpu_gpu_block_allocator interfaces naive_block prefix_caching_block utils device_allocator device_allocator cumem distributed distributed communication_op kv_events parallel_state tpu_distributed_utils utils device_communicators device_communicators all2all all_reduce_utils base_device_communicator cpu_communicator cuda_communicator cuda_wrapper custom_all_reduce pynccl pynccl_wrapper quick_all_reduce ray_communicator shm_broadcast shm_object_storage symm_mem tpu_communicator xpu_communicator eplb eplb eplb_state rebalance_algo rebalance_execute kv_transfer kv_transfer kv_transfer_state kv_connector kv_connector base factory utils v1 v1 base lmcache_connector multi_connector nixl_connector shared_storage_connector p2p p2p p2p_nccl_connector p2p_nccl_engine tensor_memory_pool kv_lookup_buffer kv_lookup_buffer base mooncake_store simple_buffer kv_pipe kv_pipe base mooncake_pipe pynccl_pipe engine engine arg_utils async_llm_engine async_timeout llm_engine metrics metrics_types protocol multiprocessing multiprocessing client engine output_processor output_processor interfaces single_step stop_checker util entrypoints entrypoints api_server chat_utils constants context harmony_utils launcher llm logger renderer score_utils ssl tool tool_server utils cli cli collect_env main openai run_batch serve types benchmark benchmark base latency main serve throughput openai openai api_server cli_args logits_processors protocol run_batch serving_chat serving_classification serving_completion serving_embedding serving_engine serving_models serving_pooling serving_responses serving_score serving_tokenization serving_transcription speech_to_text tool_parsers tool_parsers abstract_tool_parser deepseekv3_tool_parser deepseekv31_tool_parser glm4_moe_tool_parser granite_20b_fc_tool_parser granite_tool_parser hermes_tool_parser hunyuan_a13b_tool_parser internlm2_tool_parser jamba_tool_parser kimi_k2_tool_parser llama4_pythonic_tool_parser llama_tool_parser minimax_tool_parser mistral_tool_parser openai_tool_parser phi4mini_tool_parser pythonic_tool_parser qwen3coder_tool_parser seed_oss_tool_parser step3_tool_parser utils xlam_tool_parser executor executor executor_base mp_distributed_executor msgspec_utils multiproc_worker_utils ray_distributed_executor ray_utils uniproc_executor inputs inputs data parse preprocess registry logging_utils logging_utils dump_input formatter lora lora lora models peft_helper request resolver utils worker_manager layers layers base base_linear column_parallel_linear logits_processor qkv_x_parallel_linear replicated_linear row_parallel_linear utils vocal_parallel_embedding ops ops ipex_ops ipex_ops lora_ops torch_ops torch_ops lora_ops triton_ops triton_ops kernel_utils lora_expand_op lora_kernel_metadata lora_shrink_op utils xla_ops xla_ops lora_ops punica_wrapper punica_wrapper punica_base punica_cpu punica_gpu punica_selector punica_tpu punica_xpu utils model_executor model_executor custom_op parameter sampling_metadata utils layers layers activation attention_layer_base layernorm lightning_attn linear logits_processor mla pooler resampler sampler utils vocab_parallel_embedding fla fla ops ops chunk chunk_delta_h chunk_o chunk_scaled_dot_kkt cumsum fused_recurrent index l2norm layernorm_guard op solve_tril utils wy_fast fused_moe fused_moe batched_deep_gemm_moe batched_triton_or_deep_gemm_moe config cpu_fused_moe cutlass_moe deep_gemm_moe deep_gemm_utils deepep_ht_prepare_finalize deepep_ll_prepare_finalize flashinfer_cutlass_moe flashinfer_cutlass_prepare_finalize fused_batched_moe fused_marlin_moe fused_moe gpt_oss_triton_kernels_moe layer modular_kernel moe_align_block_size moe_pallas moe_permute_unpermute moe_torch_iterative pplx_prepare_finalize prepare_finalize rocm_aiter_fused_moe routing_simulator topk_weight_and_reduce triton_deep_gemm_moe trtllm_moe utils mamba mamba abstract linear_attn mamba2_metadata mamba_mixer mamba_mixer2 mamba_utils short_conv ops ops causal_conv1d layernorm_gated mamba_ssm ssd_bmm ssd_chunk_scan ssd_chunk_state ssd_combined ssd_state_passing quantization quantization auto_round awq awq_marlin awq_triton base_config bitblas bitsandbytes deepgemm deepspeedfp experts_int8 fbgemm_fp8 fp8 gguf gptq gptq_bitblas gptq_marlin gptq_marlin_24 hqq_marlin inc input_quant_fp8 ipex_quant kv_cache modelopt moe_wna16 mxfp4 petit ptpc_fp8 rtn schema torchao tpu_int8 compressed_tensors compressed_tensors compressed_tensors compressed_tensors_moe triton_scaled_mm utils schemes schemes compressed_tensors_24 compressed_tensors_scheme compressed_tensors_w4a4_nvfp4 compressed_tensors_w4a8_fp8 compressed_tensors_w4a8_int compressed_tensors_w4a16_24 compressed_tensors_w4a16_nvfp4 compressed_tensors_w8a8_fp8 compressed_tensors_w8a8_int8 compressed_tensors_w8a16_fp8 compressed_tensors_wNa16 kernels kernels mixed_precision mixed_precision allspark bitblas conch cutlass dynamic_4bit exllama MPLinearKernel machete marlin scaled_mm scaled_mm aiter cpu cutlass ScaledMMLinearKernel triton xla quark quark quark quark_moe utils schemes schemes quark_scheme quark_w4a4_mxfp4 quark_w8a8_fp8 quark_w8a8_int8 utils utils allspark_utils bitblas_utils flashinfer_fp4_moe flashinfer_utils fp8_utils gptq_utils int8_utils layer_utils machete_utils marlin_utils marlin_utils_fp4 marlin_utils_fp8 marlin_utils_test marlin_utils_test_24 mxfp4_utils mxfp8_utils nvfp4_emulation_utils nvfp4_moe_support petit_utils quant_utils w8a8_utils rotary_embedding rotary_embedding base common deepseek_scaling_rope dual_chunk_rope dynamic_ntk_alpha_rope dynamic_ntk_scaling_rope ernie45_vl_rope linear_scaling_rope llama3_rope llama4_vision_rope mrope ntk_scaling_rope phi3_long_rope_scaled_rope yarn_scaling_rope shared_fused_moe shared_fused_moe shared_fused_moe model_loader model_loader base_loader bitsandbytes_loader default_loader dummy_loader gguf_loader runai_streamer_loader sharded_state_loader tensorizer tensorizer_loader tpu utils weight_utils models models adapters aimv2 apertus arcee arctic aria aya_vision baichuan bailing_moe bamba bart bert bert_with_rope blip blip2 bloom chameleon chatglm clip cohere2_vision commandr config constant_size_cache dbrx deepseek deepseek_eagle deepseek_mtp deepseek_v2 deepseek_vl2 donut dots1 ernie45 ernie45_moe ernie45_vl ernie45_vl_moe ernie_mtp exaone exaone4 fairseq2_llama falcon falcon_h1 florence2 fuyu gemma gemma2 gemma3 gemma3_mm gemma3n gemma3n_mm glm glm4 glm4_1v glm4_moe glm4_moe_mtp glm4v gpt2 gpt_bigcode gpt_j gpt_neox gpt_oss granite granite_speech granitemoe granitemoehybrid granitemoeshared gritlm grok1 h2ovl hunyuan_v1 hyperclovax_vision idefics2_vision_model idefics3 interfaces interfaces_base intern_vit internlm2 internlm2_ve interns1 interns1_vit internvl jais jamba jina_vl keye keye_vl1_5 kimi_vl lfm2 llama llama4 llama4_eagle llama_eagle llama_eagle3 llava llava_next llava_next_video llava_onevision mamba mamba2 mamba_cache medusa midashenglm mimo mimo_mtp minicpm minicpm3 minicpm_eagle minicpmo minicpmv minimax_cache minimax_text_01 minimax_vl_01 mistral3 mixtral mllama mllama4 mlp_speculator modernbert module_mapping molmo moonvit motif mpt nano_nemotron_vl nemotron nemotron_h nemotron_nas nemotron_vl nvlm_d olmo olmo2 olmoe opt orion ovis ovis2_5 paligemma persimmon phi phi3 phi3v phi4_multimodal phi4flash phi4mm phi4mm_audio phi4mm_utils phimoe pixtral plamo2 qwen qwen2 qwen2_5_omni_thinker qwen2_5_vl qwen2_audio qwen2_moe qwen2_rm qwen2_vl qwen3 qwen3_moe qwen3_next qwen3_next_mtp qwen_vl registry roberta rvl seed_oss siglip siglip2navit skyworkr1v smolvlm solar stablelm starcoder2 step3_text step3_vl swin tarsier telechat2 teleflm terratorch transformers ultravox utils vision voxtral whisper zamba2 warmup warmup deep_gemm_warmup kernel_warmup multimodal multimodal audio base cache hasher image inputs parse processing profiling registry utils video platforms platforms cpu cuda interface rocm tpu xpu plugins plugins io_processors io_processors interface lora_resolvers lora_resolvers filesystem_resolver profiler profiler layerwise_profile utils ray ray lazy_utils ray_env reasoning reasoning abs_reasoning_parsers deepseek_r1_reasoning_parser glm4_moe_reasoning_parser gptoss_reasoning_parser granite_reasoning_parser hunyuan_a13b_reasoning_parser mistral_reasoning_parser qwen3_reasoning_parser step3_reasoning_parser transformers_utils transformers_utils config config_parser_base detokenizer detokenizer_utils dynamic_module processor runai_utils s3_utils tokenizer tokenizer_base tokenizer_group utils chat_templates chat_templates registry configs configs arctic chatglm deepseek_vl2 eagle falcon jais kimi_vl medusa midashenglm mistral mlp_speculator moonvit nemotron nemotron_h nemotron_vl ovis qwen3_next step3_vl ultravox speculators speculators algos base processors processors deepseek_vl2 ovis ovis2_5 tokenizers tokenizers mistral triton_utils triton_utils importing usage usage usage_lib utils utils deep_gemm flashinfer jsontree tensor_schema v1 v1 cudagraph_dispatcher kv_cache_interface outputs request serial_utils utils attention attention backends backends cpu_attn flash_attn flashinfer flex_attention gdn_attn linear_attn mamba1_attn mamba2_attn mamba_attn pallas rocm_aiter_fa short_conv_attn tree_attn triton_attn utils xformers mla mla common cutlass_mla flashattn_mla flashinfer_mla flashmla rocm_aiter_mla triton_mla core core block_pool encoder_cache_manager kv_cache_coordinator kv_cache_manager kv_cache_utils single_type_kv_cache_manager sched sched async_scheduler interface output request_queue scheduler utils engine engine async_llm coordinator core core_client detokenizer exceptions llm_engine logprobs output_processor parallel_sampling processor utils executor executor abstract multiproc_executor ray_distributed_executor utils metrics metrics loggers prometheus ray_wrappers reader stats pool pool metadata sample sample metadata rejection_sampler sampler logits_processor logits_processor builtin interface state ops ops bad_words logprobs penalties topk_topp_sampler tpu tpu metadata sampler spec_decode spec_decode eagle medusa metadata metrics ngram_proposer utils structured_output structured_output backend_guidance backend_lm_format_enforcer backend_outlines backend_types backend_xgrammar request utils worker worker block_table cpu_model_runner cpu_worker gpu_input_batch gpu_model_runner gpu_worker kv_connector_model_runner_mixin lora_model_runner_mixin tpu_input_batch tpu_model_runner tpu_worker utils worker_base xpu_model_runner xpu_worker worker worker cache_engine enc_dec_model_runner model_runner model_runner_base utils worker worker_base CLI Reference CLI Reference vllm serve vllm chat vllm complete vllm run-batch vllm bench vllm bench vllm bench latency vllm bench serve vllm bench throughput Community Community Contact Us Meetups Sponsors Blog Forum Slack Table of contents How Plugins Work in vLLM How vLLM Discovers Plugins Types of supported plugins Guidelines for Writing Plugins Compatibility Guarantee Plugin System ¶ The community frequently requests the ability to extend vLLM with custom features. To facilitate this, vLLM includes a plugin system that allows users to add custom features without modifying the vLLM codebase. This document explains how plugins work in vLLM and how to create a plugin for vLLM. How Plugins Work in vLLM ¶ Plugins are user-registered code that vLLM executes. Given vLLM's architecture (see Arch Overview ), multiple processes may be involved, especially when using distributed inference with various parallelism techniques. To enable plugins successfully, every process created by vLLM needs to load the plugin. This is done by the load_general_plugins function in the vllm.plugins module. This function is called for every process created by vLLM before it starts any work. How vLLM Discovers Plugins ¶ vLLM's plugin system uses the standard Python entry_points mechanism. This mechanism allows developers to register functions in their Python packages for use by other packages. An example of a plugin: Code # inside `setup.py` file from setuptools import setup setup ( name = 'vllm_add_dummy_model' , version = '0.1' , packages = [ 'vllm_add_dummy_model' ], entry_points = { 'vllm.general_plugins' : [ \"register_dummy_model = vllm_add_dummy_model:register\" ] }) # inside `vllm_add_dummy_model.py` file def register (): from vllm import ModelRegistry if \"MyLlava\" not in ModelRegistry . get_supported_archs (): ModelRegistry . register_model ( \"MyLlava\" , \"vllm_add_dummy_model.my_llava:MyLlava\" , ) For more information on adding entry points to your package, please check the official documentation . Every plugin has three parts: Plugin group : The name of the entry point group. vLLM uses the entry point group vllm.general_plugins to register general plugins. This is the key of entry_points in the setup.py file. Always use vllm.general_plugins for vLLM's general plugins. Plugin name : The name of the plugin. This is the value in the dictionary of the entry_points dictionary. In the example above, the plugin name is register_dummy_model . Plugins can be filtered by their names using the VLLM_PLUGINS environment variable. To load only a specific plugin, set VLLM_PLUGINS to the plugin name. Plugin value : The fully qualified name of the function to register in the plugin system. In the example above, the plugin value is vllm_add_dummy_model:register , which refers to a function named register in the vllm_add_dummy_model module. Types of supported plugins ¶ General plugins (with group name vllm.general_plugins ): The primary use case for these plugins is to register custom, out-of-the-tree models into vLLM. This is done by calling ModelRegistry.register_model to register the model inside the plugin function. Platform plugins (with group name vllm.platform_plugins ): The primary use case for these plugins is to register custom, out-of-the-tree platforms into vLLM. The plugin function should return None when the platform is not supported in the current environment, or the platform class's fully qualified name when the platform is supported. IO Processor plugins (with group name vllm.io_processor_plugins ): The primary use case for these plugins is to register custom pre/post processing of the model prompt and model output for poling models. The plugin function returns the IOProcessor's class fully qualified name. Guidelines for Writing Plugins ¶ Being re-entrant : The function specified in the entry point should be re-entrant, meaning it can be called multiple times without causing issues. This is necessary because the function might be called multiple times in some processes. Compatibility Guarantee ¶ vLLM guarantees the interface of documented plugins, such as ModelRegistry.register_model , will always be available for plugins to register models. However, it is the responsibility of plugin developers to ensure their plugins are compatible with the version of vLLM they are targeting. For example, \"vllm_add_dummy_model.my_llava:MyLlava\" should be compatible with the version of vLLM that the plugin targets. The interface for the model may change during vLLM's development. September 1, 2025 Back to top Made with Material for MkDocs ",
      "fetch_method": "direct-html"
    },
    {
      "arxiv_id": "https://docs.vllm.ai/en/latest/serving/deploying_with_docker.html",
      "full_text": " Using Docker - vLLM Skip to content You are viewing the latest developer preview docs. Click here to view docs for the latest stable release. vLLM Using Docker Initializing search GitHub Home User Guide Developer Guide API Reference CLI Reference Community vLLM GitHub Home User Guide User Guide Getting Started Getting Started Quickstart Installation Installation GPU CPU Google TPU AWS Neuron Examples Examples Offline Inference Offline Inference Async LLM Streaming Audio Language Automatic Prefix Caching Basic Batch LLM Inference Chat With Tools Context Extension Convert Model To Seq Cls Data Parallel Disaggregated Prefill V1 Disaggregated Prefill Dolphin Embed Jina Embeddings V3 Embed Matryoshka Fy Encoder Decoder Encoder Decoder Multimodal LLM Engine Example Load Sharded State LoRA With Quantization Inference Metrics Mistral-Small MLPSpeculator MultiLoRA Inference Offline Inference with the OpenAI Batch file format Prefix Caching Prithvi Geospatial MAE Prithvi Geospatial MAE Io Processor Profiling vLLM TPU Profiling Prompt Embed Inference Qwen2.5-Omni Offline Inference Examples Qwen3 Reranker Qwen 1M Reproducibility RLHF RLHF Colocate RLHF Utils Save Sharded State Simple Profiling Skip Loading Weights In Engine Init Spec Decode Structured Outputs Torchrun Example TPU Vision Language Vision Language Multi Image Vision Language Pooling Online Serving Online Serving API Client Helm Charts Cohere Rerank Client Disaggregated Prefill Disaggregated Serving Gradio OpenAI Chatbot Webserver Gradio Webserver Jinaai Rerank Client Kv Events Subscriber Multi-Node-Serving Multi Instance Data Parallel OpenAI Chat Completion Client OpenAI Chat Completion Client For Multimodal OpenAI Chat Completion Client With Tools OpenAI Chat Completion Client With Tools Required OpenAI Chat Completion Client With Tools Xlam OpenAI Chat Completion Client With Tools Xlam Streaming OpenAI Chat Completion Tool Calls With Reasoning OpenAI Chat Completion With Reasoning OpenAI Chat Completion With Reasoning Streaming OpenAI Chat Embedding Client For Multimodal OpenAI Classification Client OpenAI Completion Client OpenAI Cross Encoder Score OpenAI Cross Encoder Score For Multimodal OpenAI Embedding Client Long Text Embedding with Chunked Processing OpenAI Embedding Matryoshka Fy OpenAI Pooling Client OpenAI Transcription Client OpenAI Translation Client Setup OpenTelemetry POC Prithvi Geospatial MAE Prometheus and Grafana Prompt Embed Inference With OpenAI Client Ray Serve Deepseek Retrieval Augmented Generation With Langchain Retrieval Augmented Generation With Llamaindex Run Cluster Sagemaker-Entrypoint Streamlit OpenAI Chatbot Webserver Structured Outputs Utils Others Others LMCache Examples Logging Configuration Tensorize vLLM Model General General vLLM V1 Frequently Asked Questions Production Metrics Reproducibility Security Troubleshooting Usage Stats Collection Inference and Serving Inference and Serving Offline Inference OpenAI-Compatible Server Data Parallel Deployment Troubleshooting distributed deployments Expert Parallel Deployment Parallelism and Scaling Integrations Integrations LangChain LlamaIndex Deployment Deployment Using Docker Using Docker Table of contents Use vLLM's Official Docker Image Building vLLM's Docker Image from Source Building for Arm64/aarch64 Use the custom-built vLLM Docker image Using Kubernetes Using Nginx Frameworks Frameworks Anyscale AnythingLLM AutoGen BentoML Cerebrium Chatbox Dify dstack Haystack Helm LiteLLM Lobe Chat LWS Modal Open WebUI Retrieval-Augmented Generation SkyPilot Streamlit NVIDIA Triton Integrations Integrations KServe KubeAI KubeRay Llama Stack llmaz Production stack Training Training Reinforcement Learning from Human Feedback Transformers Reinforcement Learning Configuration Configuration Conserving Memory Engine Arguments Environment Variables Model Resolution Optimization and Tuning Server Arguments TPU Optimization Tips Models Models Supported Models Generative Models Pooling Models Extensions Extensions Loading Model weights with fastsafetensors Loading models with Run:ai Model Streamer Loading models with CoreWeave's Tensorizer Hardware Supported Models Hardware Supported Models TPU Features Features Automatic Prefix Caching Disaggregated Prefilling (experimental) LoRA Adapters Multimodal Inputs Prompt Embedding Inputs Reasoning Outputs Sleep Mode Speculative Decoding Structured Outputs Tool Calling Quantization Quantization AutoAWQ AutoRound BitBLAS BitsAndBytes FP8 W8A8 GGUF GPTQModel FP8 INC INT4 W4A16 INT8 W8A8 NVIDIA TensorRT Model Optimizer Quantized KV Cache AMD Quark TorchAO Developer Guide Developer Guide General General Benchmark Suites Deprecation Policy Dockerfile Incremental Compilation Workflow Profiling vLLM Vulnerability Management Model Implementation Model Implementation Basic Model Registering a Model Unit Testing Multi-Modal Support Speech-to-Text (Transcription/Translation) Support CI CI CI Failures Update PyTorch version on vLLM OSS CI/CD Design Documents Design Documents Architecture Overview Fused MoE Modular Kernel Integration with Hugging Face Hybrid KV Cache Manager IO Processor Plugins Metrics Multi-Modal Data Processing Python Multiprocessing P2P NCCL Connector Paged Attention Plugin System Automatic Prefix Caching torch.compile integration API Reference API Reference vllm vllm beam_search collect_env connections env_override envs forward_context logger logits_process logprobs outputs pooling_params sampling_params scalar_type scripts sequence tasks test_utils tracing version adapter_commons adapter_commons layers models request utils worker_manager assets assets audio base image video attention attention layer selector backends backends abstract differential_flash_attn dual_chunk_flash_attn flash_attn flashmla placeholder_attn rocm_aiter_mla rocm_flash_attn triton_mla utils xformers mla mla common layers layers chunked_local_attention cross_attention encoder_only_attention ops ops chunked_prefill_paged_decode common flashmla merge_attn_states paged_attn pallas_kv_cache_update prefix_prefill rocm_aiter_mla rocm_aiter_paged_attn triton_decode_attention triton_flash_attention triton_merge_attn_states triton_unified_attention utils utils fa_utils kv_sharing_utils benchmarks benchmarks datasets latency serve throughput lib lib endpoint_request_func ready_checker utils compilation compilation activation_quant_fusion backends base_static_graph collective_fusion compiler_interface counter cuda_graph cuda_piecewise_backend decorators fix_functionalization fusion fusion_attn fx_utils inductor_pass monitor multi_output_match noop_elimination pass_manager sequence_parallelism torch25_custom_graph_pass vllm_inductor_pass wrapper config config cache compilation kv_events kv_transfer load lora parallel scheduler utils core core block_manager evictor interfaces placeholder_block_space_manager scheduler block block block_table common cpu_gpu_block_allocator interfaces naive_block prefix_caching_block utils device_allocator device_allocator cumem distributed distributed communication_op kv_events parallel_state tpu_distributed_utils utils device_communicators device_communicators all2all all_reduce_utils base_device_communicator cpu_communicator cuda_communicator cuda_wrapper custom_all_reduce pynccl pynccl_wrapper quick_all_reduce ray_communicator shm_broadcast shm_object_storage symm_mem tpu_communicator xpu_communicator eplb eplb eplb_state rebalance_algo rebalance_execute kv_transfer kv_transfer kv_transfer_state kv_connector kv_connector base factory utils v1 v1 base lmcache_connector multi_connector nixl_connector shared_storage_connector p2p p2p p2p_nccl_connector p2p_nccl_engine tensor_memory_pool kv_lookup_buffer kv_lookup_buffer base mooncake_store simple_buffer kv_pipe kv_pipe base mooncake_pipe pynccl_pipe engine engine arg_utils async_llm_engine async_timeout llm_engine metrics metrics_types protocol multiprocessing multiprocessing client engine output_processor output_processor interfaces single_step stop_checker util entrypoints entrypoints api_server chat_utils constants context harmony_utils launcher llm logger renderer score_utils ssl tool tool_server utils cli cli collect_env main openai run_batch serve types benchmark benchmark base latency main serve throughput openai openai api_server cli_args logits_processors protocol run_batch serving_chat serving_classification serving_completion serving_embedding serving_engine serving_models serving_pooling serving_responses serving_score serving_tokenization serving_transcription speech_to_text tool_parsers tool_parsers abstract_tool_parser deepseekv3_tool_parser deepseekv31_tool_parser glm4_moe_tool_parser granite_20b_fc_tool_parser granite_tool_parser hermes_tool_parser hunyuan_a13b_tool_parser internlm2_tool_parser jamba_tool_parser kimi_k2_tool_parser llama4_pythonic_tool_parser llama_tool_parser minimax_tool_parser mistral_tool_parser openai_tool_parser phi4mini_tool_parser pythonic_tool_parser qwen3coder_tool_parser seed_oss_tool_parser step3_tool_parser utils xlam_tool_parser executor executor executor_base mp_distributed_executor msgspec_utils multiproc_worker_utils ray_distributed_executor ray_utils uniproc_executor inputs inputs data parse preprocess registry logging_utils logging_utils dump_input formatter lora lora lora models peft_helper request resolver utils worker_manager layers layers base base_linear column_parallel_linear logits_processor qkv_x_parallel_linear replicated_linear row_parallel_linear utils vocal_parallel_embedding ops ops ipex_ops ipex_ops lora_ops torch_ops torch_ops lora_ops triton_ops triton_ops kernel_utils lora_expand_op lora_kernel_metadata lora_shrink_op utils xla_ops xla_ops lora_ops punica_wrapper punica_wrapper punica_base punica_cpu punica_gpu punica_selector punica_tpu punica_xpu utils model_executor model_executor custom_op parameter sampling_metadata utils layers layers activation attention_layer_base layernorm lightning_attn linear logits_processor mla pooler resampler sampler utils vocab_parallel_embedding fla fla ops ops chunk chunk_delta_h chunk_o chunk_scaled_dot_kkt cumsum fused_recurrent index l2norm layernorm_guard op solve_tril utils wy_fast fused_moe fused_moe batched_deep_gemm_moe batched_triton_or_deep_gemm_moe config cpu_fused_moe cutlass_moe deep_gemm_moe deep_gemm_utils deepep_ht_prepare_finalize deepep_ll_prepare_finalize flashinfer_cutlass_moe flashinfer_cutlass_prepare_finalize fused_batched_moe fused_marlin_moe fused_moe gpt_oss_triton_kernels_moe layer modular_kernel moe_align_block_size moe_pallas moe_permute_unpermute moe_torch_iterative pplx_prepare_finalize prepare_finalize rocm_aiter_fused_moe routing_simulator topk_weight_and_reduce triton_deep_gemm_moe trtllm_moe utils mamba mamba abstract linear_attn mamba2_metadata mamba_mixer mamba_mixer2 mamba_utils short_conv ops ops causal_conv1d layernorm_gated mamba_ssm ssd_bmm ssd_chunk_scan ssd_chunk_state ssd_combined ssd_state_passing quantization quantization auto_round awq awq_marlin awq_triton base_config bitblas bitsandbytes deepgemm deepspeedfp experts_int8 fbgemm_fp8 fp8 gguf gptq gptq_bitblas gptq_marlin gptq_marlin_24 hqq_marlin inc input_quant_fp8 ipex_quant kv_cache modelopt moe_wna16 mxfp4 petit ptpc_fp8 rtn schema torchao tpu_int8 compressed_tensors compressed_tensors compressed_tensors compressed_tensors_moe triton_scaled_mm utils schemes schemes compressed_tensors_24 compressed_tensors_scheme compressed_tensors_w4a4_nvfp4 compressed_tensors_w4a8_fp8 compressed_tensors_w4a8_int compressed_tensors_w4a16_24 compressed_tensors_w4a16_nvfp4 compressed_tensors_w8a8_fp8 compressed_tensors_w8a8_int8 compressed_tensors_w8a16_fp8 compressed_tensors_wNa16 kernels kernels mixed_precision mixed_precision allspark bitblas conch cutlass dynamic_4bit exllama MPLinearKernel machete marlin scaled_mm scaled_mm aiter cpu cutlass ScaledMMLinearKernel triton xla quark quark quark quark_moe utils schemes schemes quark_scheme quark_w4a4_mxfp4 quark_w8a8_fp8 quark_w8a8_int8 utils utils allspark_utils bitblas_utils flashinfer_fp4_moe flashinfer_utils fp8_utils gptq_utils int8_utils layer_utils machete_utils marlin_utils marlin_utils_fp4 marlin_utils_fp8 marlin_utils_test marlin_utils_test_24 mxfp4_utils mxfp8_utils nvfp4_emulation_utils nvfp4_moe_support petit_utils quant_utils w8a8_utils rotary_embedding rotary_embedding base common deepseek_scaling_rope dual_chunk_rope dynamic_ntk_alpha_rope dynamic_ntk_scaling_rope ernie45_vl_rope linear_scaling_rope llama3_rope llama4_vision_rope mrope ntk_scaling_rope phi3_long_rope_scaled_rope yarn_scaling_rope shared_fused_moe shared_fused_moe shared_fused_moe model_loader model_loader base_loader bitsandbytes_loader default_loader dummy_loader gguf_loader runai_streamer_loader sharded_state_loader tensorizer tensorizer_loader tpu utils weight_utils models models adapters aimv2 apertus arcee arctic aria aya_vision baichuan bailing_moe bamba bart bert bert_with_rope blip blip2 bloom chameleon chatglm clip cohere2_vision commandr config constant_size_cache dbrx deepseek deepseek_eagle deepseek_mtp deepseek_v2 deepseek_vl2 donut dots1 ernie45 ernie45_moe ernie45_vl ernie45_vl_moe ernie_mtp exaone exaone4 fairseq2_llama falcon falcon_h1 florence2 fuyu gemma gemma2 gemma3 gemma3_mm gemma3n gemma3n_mm glm glm4 glm4_1v glm4_moe glm4_moe_mtp glm4v gpt2 gpt_bigcode gpt_j gpt_neox gpt_oss granite granite_speech granitemoe granitemoehybrid granitemoeshared gritlm grok1 h2ovl hunyuan_v1 hyperclovax_vision idefics2_vision_model idefics3 interfaces interfaces_base intern_vit internlm2 internlm2_ve interns1 interns1_vit internvl jais jamba jina_vl keye keye_vl1_5 kimi_vl lfm2 llama llama4 llama4_eagle llama_eagle llama_eagle3 llava llava_next llava_next_video llava_onevision mamba mamba2 mamba_cache medusa midashenglm mimo mimo_mtp minicpm minicpm3 minicpm_eagle minicpmo minicpmv minimax_cache minimax_text_01 minimax_vl_01 mistral3 mixtral mllama mllama4 mlp_speculator modernbert module_mapping molmo moonvit motif mpt nano_nemotron_vl nemotron nemotron_h nemotron_nas nemotron_vl nvlm_d olmo olmo2 olmoe opt orion ovis ovis2_5 paligemma persimmon phi phi3 phi3v phi4_multimodal phi4flash phi4mm phi4mm_audio phi4mm_utils phimoe pixtral plamo2 qwen qwen2 qwen2_5_omni_thinker qwen2_5_vl qwen2_audio qwen2_moe qwen2_rm qwen2_vl qwen3 qwen3_moe qwen3_next qwen3_next_mtp qwen_vl registry roberta rvl seed_oss siglip siglip2navit skyworkr1v smolvlm solar stablelm starcoder2 step3_text step3_vl swin tarsier telechat2 teleflm terratorch transformers ultravox utils vision voxtral whisper zamba2 warmup warmup deep_gemm_warmup kernel_warmup multimodal multimodal audio base cache hasher image inputs parse processing profiling registry utils video platforms platforms cpu cuda interface rocm tpu xpu plugins plugins io_processors io_processors interface lora_resolvers lora_resolvers filesystem_resolver profiler profiler layerwise_profile utils ray ray lazy_utils ray_env reasoning reasoning abs_reasoning_parsers deepseek_r1_reasoning_parser glm4_moe_reasoning_parser gptoss_reasoning_parser granite_reasoning_parser hunyuan_a13b_reasoning_parser mistral_reasoning_parser qwen3_reasoning_parser step3_reasoning_parser transformers_utils transformers_utils config config_parser_base detokenizer detokenizer_utils dynamic_module processor runai_utils s3_utils tokenizer tokenizer_base tokenizer_group utils chat_templates chat_templates registry configs configs arctic chatglm deepseek_vl2 eagle falcon jais kimi_vl medusa midashenglm mistral mlp_speculator moonvit nemotron nemotron_h nemotron_vl ovis qwen3_next step3_vl ultravox speculators speculators algos base processors processors deepseek_vl2 ovis ovis2_5 tokenizers tokenizers mistral triton_utils triton_utils importing usage usage usage_lib utils utils deep_gemm flashinfer jsontree tensor_schema v1 v1 cudagraph_dispatcher kv_cache_interface outputs request serial_utils utils attention attention backends backends cpu_attn flash_attn flashinfer flex_attention gdn_attn linear_attn mamba1_attn mamba2_attn mamba_attn pallas rocm_aiter_fa short_conv_attn tree_attn triton_attn utils xformers mla mla common cutlass_mla flashattn_mla flashinfer_mla flashmla rocm_aiter_mla triton_mla core core block_pool encoder_cache_manager kv_cache_coordinator kv_cache_manager kv_cache_utils single_type_kv_cache_manager sched sched async_scheduler interface output request_queue scheduler utils engine engine async_llm coordinator core core_client detokenizer exceptions llm_engine logprobs output_processor parallel_sampling processor utils executor executor abstract multiproc_executor ray_distributed_executor utils metrics metrics loggers prometheus ray_wrappers reader stats pool pool metadata sample sample metadata rejection_sampler sampler logits_processor logits_processor builtin interface state ops ops bad_words logprobs penalties topk_topp_sampler tpu tpu metadata sampler spec_decode spec_decode eagle medusa metadata metrics ngram_proposer utils structured_output structured_output backend_guidance backend_lm_format_enforcer backend_outlines backend_types backend_xgrammar request utils worker worker block_table cpu_model_runner cpu_worker gpu_input_batch gpu_model_runner gpu_worker kv_connector_model_runner_mixin lora_model_runner_mixin tpu_input_batch tpu_model_runner tpu_worker utils worker_base xpu_model_runner xpu_worker worker worker cache_engine enc_dec_model_runner model_runner model_runner_base utils worker worker_base CLI Reference CLI Reference vllm serve vllm chat vllm complete vllm run-batch vllm bench vllm bench vllm bench latency vllm bench serve vllm bench throughput Community Community Contact Us Meetups Sponsors Blog Forum Slack Table of contents Use vLLM's Official Docker Image Building vLLM's Docker Image from Source Building for Arm64/aarch64 Use the custom-built vLLM Docker image Using Docker ¶ Use vLLM's Official Docker Image ¶ vLLM offers an official Docker image for deployment. The image can be used to run OpenAI compatible server and is available on Docker Hub as vllm/vllm-openai . docker run --runtime nvidia --gpus all \\ -v ~/.cache/huggingface:/root/.cache/huggingface \\ --env \"HUGGING_FACE_HUB_TOKEN= $HF_TOKEN \" \\ -p 8000 :8000 \\ --ipc = host \\ vllm/vllm-openai:latest \\ --model Qwen/Qwen3-0.6B This image can also be used with other container engines such as Podman . podman run --device nvidia.com/gpu = all \\ -v ~/.cache/huggingface:/root/.cache/huggingface \\ --env \"HUGGING_FACE_HUB_TOKEN= $HF_TOKEN \" \\ -p 8000 :8000 \\ --ipc = host \\ docker.io/vllm/vllm-openai:latest \\ --model Qwen/Qwen3-0.6B You can add any other engine-args you need after the image tag ( vllm/vllm-openai:latest ). Note You can either use the ipc=host flag or --shm-size flag to allow the container to access the host's shared memory. vLLM uses PyTorch, which uses shared memory to share data between processes under the hood, particularly for tensor parallel inference. Note Optional dependencies are not included in order to avoid licensing issues (e.g. Issue #8030 ). If you need to use those dependencies (having accepted the license terms), create a custom Dockerfile on top of the base image with an extra layer that installs them: FROM vllm/vllm-openai:v0.9.0 # e.g. install the `audio` optional dependencies # NOTE: Make sure the version of vLLM matches the base image! RUN uv pip install --system vllm [ audio ]== 0 .9.0 Tip Some new models may only be available on the main branch of HF Transformers . To use the development version of transformers , create a custom Dockerfile on top of the base image with an extra layer that installs their code from source: FROM vllm/vllm-openai:latest RUN uv pip install --system git+https://github.com/huggingface/transformers.git Building vLLM's Docker Image from Source ¶ You can build and run vLLM from source via the provided docker/Dockerfile . To build vLLM: # optionally specifies: --build-arg max_jobs=8 --build-arg nvcc_threads=2 DOCKER_BUILDKIT = 1 docker build . \\ --target vllm-openai \\ --tag vllm/vllm-openai \\ --file docker/Dockerfile Note By default vLLM will build for all GPU types for widest distribution. If you are just building for the current GPU type the machine is running on, you can add the argument --build-arg torch_cuda_arch_list=\"\" for vLLM to find the current GPU type and build for that. If you are using Podman instead of Docker, you might need to disable SELinux labeling by adding --security-opt label=disable when running podman build command to avoid certain existing issues . Building for Arm64/aarch64 ¶ A docker container can be built for aarch64 systems such as the Nvidia Grace-Hopper. At time of this writing, this requires the use of PyTorch Nightly and should be considered experimental . Using the flag --platform \"linux/arm64\" will attempt to build for arm64. Note Multiple modules must be compiled, so this process can take a while. Recommend using --build-arg max_jobs= &amp; --build-arg nvcc_threads= flags to speed up build process. However, ensure your max_jobs is substantially larger than nvcc_threads to get the most benefits. Keep an eye on memory usage with parallel jobs as it can be substantial (see example below). Command # Example of building on Nvidia GH200 server. (Memory usage: ~15GB, Build time: ~1475s / ~25 min, Image size: 6.93GB) python3 use_existing_torch.py DOCKER_BUILDKIT = 1 docker build . \\ --file docker/Dockerfile \\ --target vllm-openai \\ --platform \"linux/arm64\" \\ -t vllm/vllm-gh200-openai:latest \\ --build-arg max_jobs = 66 \\ --build-arg nvcc_threads = 2 \\ --build-arg torch_cuda_arch_list = \"9.0 10.0+PTX\" Note If you are building the linux/arm64 image on a non-ARM host (e.g., an x86_64 machine), you need to ensure your system is set up for cross-compilation using QEMU. This allows your host machine to emulate ARM64 execution. Run the following command on your host machine to register QEMU user static handlers: docker run --rm --privileged multiarch/qemu-user-static --reset -p yes After setting up QEMU, you can use the --platform \"linux/arm64\" flag in your docker build command. Use the custom-built vLLM Docker image ¶ To run vLLM with the custom-built Docker image: docker run --runtime nvidia --gpus all \\ -v ~/.cache/huggingface:/root/.cache/huggingface \\ -p 8000 :8000 \\ --env \"HUGGING_FACE_HUB_TOKEN=&lt;secret&gt;\" \\ vllm/vllm-openai &lt;args...&gt; The argument vllm/vllm-openai specifies the image to run, and should be replaced with the name of the custom-built image (the -t tag from the build command). Note For version 0.4.1 and 0.4.2 only - the vLLM docker images under these versions are supposed to be run under the root user since a library under the root user's home directory, i.e. /root/.config/vllm/nccl/cu12/libnccl.so.2.18.1 is required to be loaded during runtime. If you are running the container under a different user, you may need to first change the permissions of the library (and all the parent directories) to allow the user to access it, then run vLLM with environment variable VLLM_NCCL_SO_PATH=/root/.config/vllm/nccl/cu12/libnccl.so.2.18.1 . July 31, 2025 Back to top Made with Material for MkDocs ",
      "fetch_method": "direct-html"
    }
  ]
}