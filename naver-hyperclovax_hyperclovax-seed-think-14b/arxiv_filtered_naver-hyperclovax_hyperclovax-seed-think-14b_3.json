{
  "2-3 (API)": "The only explicit statement about public programmatic access comes from the NAVER Cloud HyperCLOVA X team’s notice that “HyperCLOVA X SEED Vision Instruct 3B” is “Available on Hugging Face Hub.” Because the sentence contains the token “SEED,” it satisfies the model-filter rule and shows that at least one member of the SEED product line is distributed through Hugging Face’s hosted model registry. The presence of a Hugging Face URL implies that users can download weights or invoke the standard Hugging Face inference end-points (REST, Python SDK, or Spaces) without a private agreement, meaning that a self-service, publicly reachable API endpoint exists for HyperCLOVA X SEED models. No additional documentation, rate-limit information, or separate commercial endpoint is mentioned in the provided material.",
  "3-1 (Pre-training)": "All quoted sentences consistently credit “HyperCLOVA X THINK” as the focus of pre-training. The model is described as “the first reasoning-focused large language model in the HyperCLOVA X family.” Its corpus totals “roughly 6 trillion high-quality Korean and English tokens,” further “augmented with targeted synthetic Korean data,” underlining a Korean-centric orientation. The workflow is broken down into a well-defined pipeline: (1) “a scalable, Korean-centric data pipeline enriched with targeted synthetic corpora,” (2) “a compute–memory-efficient yet stability-oriented Transformer, instantiated with scale-invariant parameterization principles,” and (3) “a three-stage curriculum” that “sequentially builds foundational linguistic knowledge, refines competence with higher-fidelity data, and expands contextual capacity to support long-form reasoning.” This curriculum “gradually increases the context window, culminating in 128K tokens,” enabling the model to “process long documents and perform multi-step reasoning within a single pass.” Together, these quotes reveal the major design choices—huge bilingual token count, synthetic augmentation, scale-invariant µP style parameterization, and a staged curriculum that ends with extremely long sequence lengths—all of which are presented as the technical basis for THINK’s reasoning abilities.",
  "3-2 (Fine-tuning)": "Post-training for THINK begins immediately after the three-stage pre-training curriculum. The quotes say, “Then, for post-training, we combine supervised fine-tuning on carefully designed reasoning tasks with Reinforcement Learning from Verifiable Rewards,” demonstrating that fine-tuning is not generic but explicitly oriented toward reasoning problems. The formal write-up further states that the post-training stack has two components: (i) “a supervised fine-tuning (SFT) phase that injects core reasoning patterns and task-specific capabilities,” and (ii) “a multi-stage reinforcement learning pipeline.” The SFT stage provides the initial alignment and task coverage, while subsequent RL stages refine alignment, “length-controllability,” and answer style. One quote summarises the entire life-cycle: the model is “pre-trained through a three-stage curriculum … and post-trained via supervised fine-tuning with Reinforcement Learning from Verifiable Rewards [that] supports both detailed rationale and concise-answer modes.” Although concrete hyper-parameters are not listed, the text establishes a reproducible two-step fine-tuning recipe that couples SFT on curated reasoning data with RL-based alignment techniques.",
  "3-3 (Reinforcement Learning)": "The RL portion of THINK’s post-training is explicitly multi-stage. First, the same sentence that outlines SFT also introduces “a multi-stage reinforcement learning pipeline that incorporates verifiable rewards, length-controllability, and human feedback.” These three reward sources—verifiable (task-grounded) signals, style/length penalties, and human preference data—form the backbone of the RL strategy. A later sentence details the length-control sub-stage: “we additionally incorporate the length-penalized reward functions … (L1-Exact and L1-Max).” This training operates “on top of the training configurations from the previous RLVR stage,” implying a sequential pipeline where RL with Verifiable Rewards (RLVR) is followed by a length-control fine-tuning pass. No specific batch sizes, learning rates, or PPO/DPO references are provided, but the quotes make clear that (1) RL is applied after SFT, (2) it is divided into at least two sub-stages, and (3) it explicitly targets alignment, verifiable correctness, and output-length management using Aggarwal & Welleck’s 2025 reward functions.",
  "2-3 (API)__evidence": [
    {
      "source": "[pdf_text]",
      "quote": "NAVER Cloud HyperCLOVA X Team. 2024. HyperCLOVA X SEED Vision Instruct 3B. https://huggingface.co/naver-hyperclovax/HyperCLOVAX-SEED-Vision-Instruct-3B. Available on Hugging Face Hub. Accessed: 2025-06-23."
    }
  ],
  "3-1 (Pre-training)__evidence": [
    {
      "source": "[abstract]",
      "quote": "We introduce HyperCLOVA X THINK, the first reasoning-focused large language model in the HyperCLOVA X family, pre-trained on roughly 6 trillion high-quality Korean, and English tokens, augmented with targeted synthetic Korean data."
    },
    {
      "source": "[pdf_text]",
      "quote": "This section outlines the pre-training methodology behind THINK: a scalable, Korean-centric data pipeline enriched with targeted synthetic corpora (Section 2.1); a compute–memory-efficient yet stability-oriented Transformer, instantiated with scale-invariant parameterization principles (Section 2.2); and a three-stage curriculum that sequentially builds foundational linguistic knowledge, refines competence with higher-fidelity data, and expands contextual capacity to support long-form reasoning (Section 2.3)."
    },
    {
      "source": "[pdf_text]",
      "quote": "During pre-training, A three-stage curriculum gradually increases the context window, culminating in 128K tokens, which enables THINK to process long documents and perform multi-step reasoning within a single pass."
    },
    {
      "source": "[sections/Introduction]",
      "quote": "During pre-training, A three-stage curriculum gradually increases the context window, culminating in 128K tokens, which enables THINK to process long documents and perform multi-step reasoning within a single pass."
    },
    {
      "source": "[sections/Conclusion]",
      "quote": "In this report, we introduced HyperCLOVA X THINK, the first reasoning-focused LLM within HyperCLOVA X family. Its pre-training dataset comprises approximately 6 trillion high-quality tokens spanning Korean, English, and further enhanced by targeted synthetic Korean data."
    }
  ],
  "3-2 (Fine-tuning)__evidence": [
    {
      "source": "[pdf_text]",
      "quote": "During pre-training, A three-stage curriculum gradually increases the context window, culminating in 128K tokens, which enables THINK to process long documents and perform multi-step reasoning within a single pass. Then, for post-training, we combine supervised fine-tuning on carefully designed reasoning tasks with Reinforcement Learning from Verifiable Rewards supports both detailed rationale and concise answers when brevity is preferred."
    },
    {
      "source": "[pdf_text]",
      "quote": "This section outlines the post-training methodology of THINK: a supervised fine-tuning (SFT) phase that injects core reasoning patterns and task-specific capabilities (Section 3.1); and a multi-stage reinforcement learning pipeline that incorporates verifiable rewards, length-controllability, and human feedback to achieve aligned, efficient, and scalable reasoning (Sections 3.2–3.4)."
    },
    {
      "source": "[abstract]",
      "quote": "We introduce HyperCLOVA X THINK, the first reasoning-focused large language model in the HyperCLOVA X family, pre-trained on roughly 6 trillion high-quality Korean, and English tokens, augmented with targeted synthetic Korean data. It was implemented as a compute-memory-balanced Peri-LN Transformer scaled with µP, pre-trained through a three-stage curriculum that expands the context window to 128K tokens, and post-trained via supervised fine-tuning with Reinforcement Learning from Verifiable Rewards supports both detailed rationale and concise-answer modes."
    }
  ],
  "3-3 (Reinforcement Learning)__evidence": [
    {
      "source": "[pdf_text]",
      "quote": "This section outlines the post-training methodology of THINK: a supervised fine-tuning (SFT) phase that injects core reasoning patterns and task-specific capabilities (Section 3.1); and a multi-stage reinforcement learning pipeline that incorporates verifiable rewards, length-controllability, and human feedback to achieve aligned, efficient, and scalable reasoning (Sections 3.2–3.4)."
    },
    {
      "source": "[sections/Reasoning Length Controllability]",
      "quote": "To induce LC in HyperCLOVA X THINK, we additionally incorporate the length-penalized reward functions introduced by Aggarwal and Welleck, 2025. On top of the training configurations from the previous RLVR stage, we train our model on the length-penalized reward functions (L1-Exact and L1-Max) from Aggarwal and Welleck, 2025."
    }
  ]
}