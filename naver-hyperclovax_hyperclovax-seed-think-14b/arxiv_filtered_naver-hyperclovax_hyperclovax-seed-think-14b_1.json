{
  "1-1 (Weights)": "Across several statements, the developers repeatedly say they have not yet released the full 14-billion-parameter HyperCLOVA X THINK checkpoint, but that they are actively preparing a smaller, pruned-and-distilled edition that WILL be released. Direct wording includes: “Currently, a pruned and distilled version of THINK is under preparation to be open-sourced.”  Similar language is echoed in two other places: “Lastly, we plan to open-source a pruned and distilled version of HyperCLOVA X THINK,” and, with more detail, “Lastly, we present a pruning and distillation technique that will soon be applied to HyperCLOVA X THINK for an open-source and business-friendly foundation model. We plan to open-source release this model under a business-friendly license.”  One sentence places the effort in a broader series context by noting that “HyperCLOVA X SEED 0.5B, recently released on HuggingFace, is the first open-source model in the HyperCLOVA X series trained using pruning and knowledge distillation.”  Collectively, these quotes convey that (a) weights for the original 14 B THINK are not yet public, (b) a lighter distilled/pruned variant is in active preparation, (c) the release will be under a business-friendly license and open to anyone, and (d) the precedent of releasing HyperCLOVA X SEED 0.5B on HuggingFace indicates the likely distribution platform and download-by-anyone model once the distilled THINK weights are ready.",
  "1-2 (Code)": "The provided material contains no sentence that explicitly describes the availability (or absence) of the full TRAINING code, data-prep scripts, configuration files, or scheduling scripts for HyperCLOVA X THINK. Therefore, based strictly on the supplied quotes, no information is available regarding public release or licensing of the training pipeline or fine-tuning code.",
  "1-3 (License)": "Every mention of licensing ties back to the future open-sourced, pruned-and-distilled THINK model. The authors repeatedly emphasize a “…business-friendly license.”  For example: “This technique will soon be applied to THINK itself … We plan to open-source release this model under a business-friendly license.”  They reinforce the same point in the pipeline description: “Third, we share a practical pruning-distillation pipeline and commit to apply it for an open-source version of THINK—fostering further research and commercial deployment, even under more resource-constrained settings.”  Finally they restate: “We plan to open-source release this model under a business-friendly license.”  The language explicitly highlights that commercial use will be allowed (“business-friendly” and “commercial deployment”), but does not set out any restrictions or conditions on modification or redistribution beyond implying the license will permit those actions. No specific license name (e.g., Apache-2.0, MIT) nor explicit prohibitions such as “non-commercial,” “research-only,” or “no derivatives” is provided in the quoted text.",
  "1-4 (Paper)": "Multiple sentences reference an official technical report on HyperCLOVA X THINK. It is characterized as “the first reasoning-focused large language model in the HyperCLOVA X family,” trained on “roughly 6 trillion high-quality Korean, and English tokens, augmented with targeted synthetic Korean data.”  The report itself is cited with an arXiv identifier: “arXiv:2506.22403 v2 [cs.CL] 1 Jul 2025 HyperCLOVA X THINK.”  Two additional lines specify separate demonstration or appendix files: “HyperCLOVA X THINK (Translated Model Input&Output – English)” and “HyperCLOVA X THINK (Model Input&Output – Korean).”  A second sentence restates the scope: “In this report, we introduced HyperCLOVA X THINK, the first reasoning-focused LLM within HyperCLOVA X family.”  Collectively, these quotes confirm the existence of an official arXiv paper (version 2, July 2025), provide a high-level overview of the training data scale, emphasize the model’s focus on reasoning, and list at least two supplementary documents covering I/O in English and Korean.",
  "1-1 (Weights)__evidence": [
    {
      "source": "[sections/Introduction]",
      "quote": "This technique will soon be applied to THINK itself to produce a model suitable for limited resource settings. We plan to open-source release this model under a business-friendly license."
    },
    {
      "source": "[sections/Extensions/Lightening through Pruning and Distillation]",
      "quote": "Currently, a pruned and distilled version of THINK is under preparation to be open-sourced."
    },
    {
      "source": "[sections/Conclusion]",
      "quote": "Lastly, we plan to open-source a pruned and distilled version of HyperCLOVA X THINK."
    },
    {
      "source": "[sections/Introduction]",
      "quote": "Lastly, we present a pruning and distillation technique that will soon be applied to HyperCLOVA X THINK for an open-source and business-friendly foundation model. We plan to open-source release this model under a business-friendly license."
    },
    {
      "source": "[sections/6.2 Lightening through Pruning and Distillation]",
      "quote": "As a real-world example, HyperCLOVA X SEED 0.5B, recently released on HuggingFace, is the first open-source model in the HyperCLOVA X series trained using pruning and knowledge distillation."
    }
  ],
  "1-2 (Code)__evidence": [],
  "1-3 (License)__evidence": [
    {
      "source": "[sections/Introduction]",
      "quote": "This technique will soon be applied to THINK itself to produce a model suitable for limited resource settings. We plan to open-source release this model under a business-friendly license."
    },
    {
      "source": "[sections/Introduction]",
      "quote": "Third, we share a practical pruning-distillation pipeline and commit to apply it for an open-source version of THINK—fostering further research and commercial deployment, even under more resource-constrained settings."
    },
    {
      "source": "[sections/Introduction]",
      "quote": "Lastly, we present a pruning and distillation technique that will soon be applied to HyperCLOVA X THINK for an open-source and business-friendly foundation model. We plan to open-source release this model under a business-friendly license."
    }
  ],
  "1-4 (Paper)__evidence": [
    {
      "source": "[abstract]",
      "quote": "We introduce HyperCLOVA X THINK, the first reasoning-focused large language model in the HyperCLOVA X family, pre-trained on roughly 6 trillion high-quality Korean, and English tokens, augmented with targeted synthetic Korean data."
    },
    {
      "source": "[sections/Conclusion]",
      "quote": "In this report, we introduced HyperCLOVA X THINK, the first reasoning-focused LLM within HyperCLOVA X family."
    },
    {
      "source": "[sections/2506.22403]",
      "quote": "arXiv:2506.22403v2  [cs.CL]  1 Jul 2025\nHyperCLOVA X THINK"
    },
    {
      "source": "[pdf_text]",
      "quote": "HyperCLOVA X THINK (Translated Model Input&Output – English)"
    },
    {
      "source": "[pdf_text]",
      "quote": "HyperCLOVA X THINK (Model Input&Output – Korean)"
    }
  ]
}