{
  "repo": "NAVER-Cloud-HyperCLOVA-X/hcx-vllm-plugin",
  "full_texts": [
    {
      "arxiv_id": "https://arxiv.org/pdf/2203.03466",
      "full_text": "Tensor Programs V:\nTuning Large Neural Networks via\nZero-Shot Hyperparameter Transfer\nGreg Yang‚àó√ó\nEdward J. Hu‚àó√ó‚Ä†\nIgor Babuschkin‚ó¶\nSzymon Sidor‚ó¶\nXiaodong Liu√ó\nDavid Farhi‚ó¶\nNick Ryder‚ó¶\nJakub Pachocki‚ó¶\nWeizhu Chen√ó\nJianfeng Gao√ó\n√óMicrosoft Corporation\n‚ó¶OpenAI\nAbstract\nHyperparameter (HP) tuning in deep learning is an expensive process, prohibitively\nso for neural networks (NNs) with billions of parameters. We show that, in the\nrecently discovered Maximal Update Parametrization (¬µP), many optimal HPs\nremain stable even as model size changes. This leads to a new HP tuning paradigm\nwe call ¬µTransfer: parametrize the target model in ¬µP, tune the HP indirectly on a\nsmaller model, and zero-shot transfer them to the full-sized model, i.e., without\ndirectly tuning the latter at all. We verify ¬µTransfer on Transformer and ResNet.\nFor example, 1) by transferring pretraining HPs from a model of 13M parameters,\nwe outperform published numbers of BERT-large (350M parameters), with a total\ntuning cost equivalent to pretraining BERT-large once; 2) by transferring from\n40M parameters, we outperform published numbers of the 6.7B GPT-3 model, with\ntuning cost only 7% of total pretraining cost. A Pytorch implementation of our\ntechnique can be found at github.com/microsoft/mup and installable via pip\ninstall mup.\n1\nIntroduction\n20\n18\n16\n14\n12\n10\nlog2LearningRate\n3.5\n4.0\n4.5\n5.0\n5.5\n6.0\n6.5\n7.0\nTraining Loss\noptimum shifts\nStandard Practice\nWidth\n128\n256\n512\n1024\n2048\n4096\n8192\n20\n18\n16\n14\n12\n10\nlog2LearningRate\noptimum stable\nOur Work\nFigure 1: Training loss against learning rate on\nTransformers of varying dmodel trained with Adam.\nConventionally and in contrast with our technique,\ndifferent widths do not share the same optimal hy-\nperparameter; wider networks do not always per-\nform better than narrower ones; in fact they under-\nperform the same-width networks in our technique\neven after tuning learning rate (see dashed line).\nSee Sections 3 and 4 for experimental setup.\nHyperparameter (HP) tuning is critical to deep\nlearning. Poorly chosen HPs result in subpar\nperformance and training instability. Many pub-\nlished baselines are hard to compare to one\nanother due to varying degrees of HP tuning.\nThese issues are exacerbated when training ex-\ntremely large deep learning models, since state-\nof-the-art networks with billions of parameters\nbecome prohibitively expensive to tune.\nRecently, [57] showed that different neural net-\nwork parametrizations induce different inÔ¨Ånite-\nwidth limits and proposed the Maximal Update\nParametrization (abbreviated ¬µP) (summarized\nin Table 3) that enables ‚Äúmaximal‚Äù feature learn-\ning in the limit. Intuitively, it ensures that each\nlayer is updated on the same order during train-\ning regardless of width.2 In contrast, while the\nstandard parametrization (SP) ensures activations are of unit order at initialization, it actually causes\nthem to blow up in wide models during training [57] essentially due to an imbalance of per-layer\n‚Ä†Work done partly during Microsoft AI Residency Program.\n‚àóEqual contribution. Order is random. Correspondence to {gregyang, edwardhu}@microsoft.com\n2i.e., the updates‚Äô effect on activations becomes roughly independent of width in the large width limit.\narXiv:2203.03466v2  [cs.LG]  28 Mar 2022\n\nAlgorithm 1 Tuning a Large Target Model via ¬µTransfer\n1: Parametrize target model in Maximal Update Parametrization (¬µP)\n2: Tune a smaller version (in width and/or depth) of target model\n3: Copy tuned hyperparameters to target model\nTable 1: Hyperparameters That Can Be ¬µTransferred, Not ¬µTransferred, or ¬µTransferred\nAcross, with a few caveats discussed in Section 6.1. * means empirically validated only on Trans-\nformers, while all others additionally have theoretical justiÔ¨Åcation.\n¬µTransferable\nNot ¬µTransferable\n¬µTransferred Across\noptimization related, init,\nregularization\nwidth, depth*, batch size*,\nparameter multipliers, etc\n(dropout, weight decay, etc)\ntraining time*, seq length*\nlearning rate (also see Fig. 5). We leverage ¬µP to zero-shot transfer HPs from small models to large\nmodels in this work ‚Äì that is, we obtain near optimal HPs on a large model without directly tuning\nit at all! While practitioners have always guessed HPs of large models from those of small models,\nthe results are hit-or-miss at best because of incorrect parametrization. For example, as shown in\nFig. 1, in a Transformer, the optimal learning rate is stable with width in ¬µP (right) but far from\nso in standard parametrization (left). In addition to width, we empirically verify that, with a few\ncaveats, HPs can also be transferred across depth (in Section 6.1) as well as batch size, language\nmodel sequence length, and training time (in Appendix G.2.1). This reduces the tuning problem of\nan (arbitrarily) large model to that of a (Ô¨Åxed-sized) small model. Our overall procedure, which we\ncall ¬µTransfer, is summarized in Algorithm 1 and Fig. 2, and the HPs we cover are summarized in\nTables 1 and 2.\n$$$$$$\n$$\nStandard\nPractice\nOur Method\nDirectly tune large model\nShrink\nTune\nTransfer\nùúá\nùúá\nFigure 2: Illustration of ¬µTransfer\nThere are several beneÔ¨Åts to our approach: 1. Better Per-\nformance: ¬µTransfer is not just about predicting how the\noptimal learning rate scales in SP. In general, we expect the\n¬µTransferred model to outperform its SP counterpart with\nlearning rate optimally tuned. For example, this is the case\nin Fig. 1 with the width-8192 Transformer. We discuss the\nreason for this in Section 5 and Appendix C. 2. Speedup:\nIt provides massive speedup to the tuning of large mod-\nels. For example, we are able to outperform published\nnumbers of (350M) BERT-large [11] purely by zero-shot\nHP transfer, with tuning cost approximately equal to 1\nBERT-large pretraining. Likewise, we outperform the pub-\nlished numbers of the 6.7B GPT-3 model [7] with tuning\ncost being only 7% of total pretraining cost. For models\non this scale, HP tuning is not feasible at all without our\napproach. 3. Tune Once for Whole Family: For any Ô¨Åxed family of models with varying width and\ndepth (such as the BERT family or the GPT-3 family), we only need to tune a single small model\nand can reuse its HPs for all models in the family.3 For example, we will use this technique to\ntune BERT-base (110M parameters) and BERT-large (350M parameters) simultaneously by trans-\nferring from a 13M model. 4. Better Compute Utilization: While large model training needs to\nbe distributed across many GPUs, the small model tuning can happen on individual GPUs, greatly\nincreasing the level of parallelism for tuning (and in the context of organizational compute clusters,\nbetter scheduling and utilization ratio). 5. Painless Transition from Exploration to Scaling Up:\nOften, researchers explore new ideas on small models but, when scaling up, Ô¨Ånd their HPs optimized\nduring exploration work poorly on large models. ¬µTransfer would solve this problem.\nIn addition to the HP stability property, we Ô¨Ånd that wider is better throughout training in ¬µP, in\ncontrast to SP (Section 8). This increases the reliability of model scaling in deep learning.\nIn this work, we primarily focus on hyperparameter transfer with respect to training loss. In settings\nwhere regularization is not the bottleneck to test performance, as in all of our experiments here, this\nalso translates to efÔ¨Åcacy in terms of test loss. In other settings, such as Ô¨Ånetuning of models on small\ndatasets, ¬µTransfer may not be sufÔ¨Åcient, as we discuss in Section 6.1.\n3but possibly not for different data and/or tasks.\n2\n\nTable 2: Examples of ¬µTransferable Hyperparameters. All of the below can also be specialized\nto per-layer hyperparameters.\nOptimizer Related\nInitialization\nParameter Multipliers\nlearning rate (LR), momentum,\nper-layer\nmultiplicative constants after\nAdam beta, LR schedule, etc\ninit. variance\nweight/biases, etc\nOur Contributions\n‚Ä¢ We demonstrate it is possible to zero-shot transfer near optimal HPs to a large model from a\nsmall version via the Maximal Update Parametrization (¬µP) from [57].\n‚Ä¢ While [57] only covered SGD, here we derive ¬µP for Adam as well (Table 3).\n‚Ä¢ We propose a new HP tuning technique, ¬µTransfer, for large neural networks based on this\nobservation that provides massive speedup over conventional methods and covers both SGD\nand Adam training;\n‚Ä¢ We thoroughly verify our method on machine translation and large language model pretrain-\ning (in Section 7.3) as well as image classiÔ¨Åcation (in Appendix G.1);\n‚Ä¢ We release a PyTorch [35] package for implementing ¬µTransfer painlessly. A sketch of this\npackage is given in Appendix H.\nTerminologies\nSometimes, to be less ambiguous, we often refer to the ‚Äúlarge model‚Äù as the target\nmodel, as it is the model we wish to ultimately tune, while we refer to the ‚Äúsmall model‚Äù as the\nproxy model, as it proxies the HP tuning process. We follow standard notation dmodel, dhead =\ndk, dv, nhead, dffn regarding dimensions in a Transformer; one can see Fig. 11 for a refresher.\nTensor Programs Series\nThis paper is the 5th installment of the Tensor Programs series. While it\nis self-contained with the target audience being practitioners and empirical researchers, this paper\npresents the Ô¨Årst major practical payoff of the theoretical foundation built in previous works [53‚Äì58].\n2\nParametrization Matters: A Primer\nIn this section, we give a very basic primer on why the correct parametrization can allow HP transfer\nacross width, but see Appendices J.1 to J.3 for more (mathematical) details.\nThe Central Limit Theorem (CLT) says that, if x1, . . . , xn are iid samples from a zero-mean, unit-\nvariance distribution, then\n1\n‚àön(x1 + ¬∑ ¬∑ ¬∑ + xn) converges to a standard Gaussian N(0, 1) as n ‚Üí‚àû.\nTherefore, we can say that\n1\n‚àön is the right order of scaling factor cn such that cn(x1 + ¬∑ ¬∑ ¬∑ + xn)\nconverges to something nontrivial. In contrast, if we set cn = 1/n, then cn(x1 + ¬∑ ¬∑ ¬∑ + xn) ‚Üí0; or\nif cn = 1, then cn(x1 + ¬∑ ¬∑ ¬∑ + xn) blows up in variance as n ‚Üí‚àû.\nNow suppose we would like to minimize the function\nFn(c)\ndef\n=\nE\nx1,...,xn f(c(x1 + ¬∑ ¬∑ ¬∑ + xn))\n(1)\nover c ‚ààR, for some bounded continuous function f : R ‚ÜíR. If we reparametrize c = Œ±/‚àön for\nŒ± ‚ààR, then by CLT, Gn(Œ±)\ndef\n= Fn(c) ‚ÜíE f(N(0, Œ±2)) stabilizes into a function of Œ± as n ‚Üí‚àû.\nThen for sufÔ¨Åciently large n, the optimal Œ±‚àó\nn\ndef\n= arg minŒ± Gn(Œ±) should be close to Œ±‚àó\nN for any\nN > n, and indeed, for N = ‚àû‚Äî this precisely means we can transfer the optimal c‚àó\nn or Œ±‚àó\nn for a\nsmaller problem (say Fn) to a larger problem (say FN): GN is approximately minimized by Œ±‚àó\nn and\nFN is approximately minimized by c‚àó\nn\np\nn/N. Because the transfer algorithm is simply copying Œ±,\nwe say the parametrization c = Œ±/‚àön is the correct parametrization for this problem.\nIn the scenario studied in this paper, x1, . . . , xn are akin to randomly initialized parameters of a\nwidth-n neural network, c is akin to a HP such as learning rate, and f is the test-set performance\nof the network after training, so that Fn gives its expectation over random initializations. Just as\nin this example, if we parametrize the learning rate and other HPs correctly, then we can directly\ncopy the optimal HPs for a narrower network into a wide network and expect approximately optimal\n3\n\nperformance ‚Äî this is the (zero-shot) hyperparameter transfer we propose here. It turns out the\nMaximal Update Parametrization (¬µP) introduced in [57] is correct (akin to the parametrization in Œ±\nabove), while the standard parametrization (SP) is incorrect (akin to the parametrization in c). We will\nreview both parametrizations shortly. Theoretically, a ¬µP network has a well-deÔ¨Åned inÔ¨Ånite-width\nlimit ‚Äî akin to (x1 + ¬∑ ¬∑ ¬∑ + xn)/‚àön having a N(0, 1) limit by CLT ‚Äî while a SP network does not\n(the limit will blow up) [57].4 In fact, based on the theoretical foundation laid in [57], we argue in\nAppendix J.3 that ¬µP should also be the unique parametrization that allows HP transfer across width.\nFor a more formal discussion of the terminologies parametrization and transfer, see Appendix A\nWe emphasize that, to ensure transferability of any hyperparameter (such as learning rate), it‚Äôs not\nsufÔ¨Åcient to reparametrize only that hyperparameter, but rather, we need to identify and correctly\nreparametrize all hyperparameters in Table 2. For example, in Fig. 1, the wide models in SP still\nunderperform their counterparts in ¬µP, even with learning rate tuned optimally. This is precisely\nbecause SP does not scale parameter multipliers and input/output layer learning rates correctly in\ncontrast to ¬µP (see Table 3). See Appendix C for more intuition via a continuation of our example\nhere. We shall also explain this more concretely in the context of neural networks in Section 5.\n3\nHyperparameters Don‚Äôt Transfer Conventionally\nIn the community there seem to be conÔ¨Çicting assumptions about HP stability. A priori, models\nof different sizes don‚Äôt have any reason to share the optimal HPs. Indeed, papers aiming for state-\nof-the-art results often tune them separately. On the other hand, a nontrivial fraction of papers in\ndeep learning Ô¨Åxes all HPs when comparing against baselines, which reÔ¨Çects an assumption that\nthe optimal HPs should be stable ‚Äî not only among the same model of different sizes but also\namong models of different designs ‚Äî therefore, such comparisons are fair. Here, we demonstrate HP\ninstability across width explicitly in MLP and Transformers in the standard parametrization. We will\nonly look at training loss to exclude the effect of regularization.\nMLP with Standard Parametrization\nWe start with a 2-hidden-layer MLP with activation func-\ntion œÜ, using the standard parametrization5 with LeCun initialization6 akin to the default in PyTorch:\nf(Œæ) = W 3‚ä§œÜ(W 2‚ä§œÜ(W 1‚ä§Œæ + b1) + b2)\nwith init.\nW 1 ‚àºN(0, 1/din), W {2,3} ‚àºN(0, 1/n), b{1,2} = 0,\n(2)\n14\n12\n10\n8\n6\n4\n2\nlog2LearningRate\n0.0\n0.5\n1.0\n1.5\n2.0\nTraining Loss\nSP / xent\nwidth\n256\n512\n1024\n2048\n4096\n8192\n14\n12\n10\n8\n6\n4\n2\nlog2LearningRate\n0.0\n0.5\n1.0\n1.5\n2.0\nP / xent\nFigure 3:\nMLP width different hidden sizes trained\nfor 20 epoch on CIFAR-10 using SGD. Left uses stan-\ndard parametrization (SP); right uses maximal update\nparametrization (¬µP). ¬µP networks exhibit better learning\nrate stability than their SP counterparts.\nwhere W 1\n‚àà\nRdin√ón, b1\n‚àà\nRn,\nW 2\n‚àà\nRn√ón, b2\n‚àà\nRn, W 3\n‚àà\nRn√ódout and din, n, and dout are\nthe input, hidden, and output dimen-\nsions. The particular MLP we use has\nœÜ = ReLU and a cross-entropy (xent)\nloss function. We deÔ¨Åne the width of\nMLP as the hidden size n, which is\nvaried from 256 to 8192. The mod-\nels are trained on CIFAR-10 for 20\nepochs, which is more than enough to\nensure convergence.\nAs shown on the left in Fig. 3, the\noptimal learning rate shifts by roughly\nan order of magnitude as the width\nincreases from 256 to 8192; using the\noptimal learning of the smallest model\non the largest model gives very bad performance, if not divergence.\nTransformer with Standard Parametrization\nThis perhaps unsurprising observation holds for\nmore complex architectures such as Transformer as well, as shown in Fig. 1 (left). We deÔ¨Åne width\n4The more theoretically astute reader may observe that SP with a Œò(1/width) learning rate induces a\nwell-deÔ¨Åned inÔ¨Ånite-width limit exists as well. Nevertheless, this does not allow HP transfer because this limit is\nin kernel regime as shown in [57]. See Appendix J.3 for more discussions.\n5i.e. the default parametrization offered by common deep learning frameworks. See Table 3 for a review.\n6The key here is that the init. variance ‚àù1/fan_in, so the same insights here apply with e.g. He initialization.\n4\n\nTable 3: ¬µP[57] and SP for General Neural Networks. Here, we emphasize the scaling with width\n(fan_in or fan_out); in practice, we may insert tunable multipliers in front of fan_in and fan_out as\nin Eq. (4). The fan_out of a bias vector is its dimension (whereas fan_in is 1). Purple text highlights\nkey differences from standard parametrization (SP); Gray text recalls the corresponding SP. SGD\n(resp. Adam) here can be replaced by variants such as SGD with momentum (resp. Adagrad, etc);\nsee Appendix B.3 for other optimizers. In general, the three columns here can be interpreted as\nlinear layers that have {Ô¨Ånite, inÔ¨Ånite, inÔ¨Ånite} input dimension and {inÔ¨Ånite, Ô¨Ånite, inÔ¨Ånite} output\ndimension in an inÔ¨Ånite-width network; this description generalizes more readily to other parameters\nsuch as those of layernorm. Transformer ¬µP requires one more modiÔ¨Åcation (1/d attention instead\nof 1/\n‚àö\nd); see DeÔ¨Ånition 4.1. This version of ¬µP gets rid of parameter multipliers; for the version\nsimilar to that in [57], see Table 9. Also see Table 8 for a ¬µP formulation that is easier to implement\n(and compatible with input/output weight sharing). Further explanation of this table can be found in\nAppendix B. Its derivation can be found in Appendix J.\nInput weights & all biases\nOutput weights\nHidden weights\nInit. Var.\n1/fan_in\n1/fan_in2\n(1/fan_in)\n1/fan_in\nSGD LR\nfan_out\n(1)\n1/fan_in\n(1)\n1\nAdam LR\n1\n1/fan_in\n(1)\n1/fan_in\n(1)\nas dmodel, with dk = dq = dv = dmodel/nhead and dffn = 4dmodel. The models are trained on\nwikitext-2 for 5 epochs. In Fig. 18 in the appendix we also show the instability of initialization scale\nand other HPs.\n4\nUnlocking Zero-Shot Hyperparameter Transfer with ¬µP\nWe show that ¬µP solves the problems we see in Section 3.\nMLP with ¬µP\nFor the MLP in Section 3, to switch to ¬µP, we just need to modify Eq. (2)‚Äôs\ninitialization of the last layer and its learning rates of the Ô¨Årst and last layer as well as of the biases.\nThe basic form is7\ninitialize\nW 1 ‚àºN(0, 1/din), W 2 ‚àºN(0, 1/n), W 3 ‚àºN(0, 1/n2), b{1,2} = 0\nwith SGD learning rates\nŒ∑W 1 = Œ∑b1 = Œ∑b2 = Œ∑n, Œ∑W 2 = Œ∑, Œ∑W 3 = Œ∑n‚àí1.\n(3)\nHere, Œ∑ speciÔ¨Åes the ‚Äúmaster‚Äù learning rate, and we highlighted in purple the differences in the two\nparametrizations. This basic form makes clear the scaling with width n of the parametrization, but in\npractice we will often insert (possibly tune-able) multiplicative constants in front of each appearance\nof n. For example, this is useful when we would like to be consistent with a SP MLP at a base width\nn0. Then we may insert constants as follows: For Àún\ndef\n= n/n0,\ninitialize\nW 1 ‚àºN(0, 1/din), W 2 ‚àºN(0, 1/n), W 3 ‚àºN(0, 1/n¬∑Àún), b{1,2} = 0\nwith SGD learning rates\nŒ∑W 1 = Œ∑b1 = Œ∑b2 = Œ∑Àún, Œ∑W 2 = Œ∑, Œ∑W 3 = Œ∑Àún‚àí1.\n(4)\nThen at width n = n0, all purple factors above are 1, and the parametrization is identical to SP\n(Eq. (2)) at width n0. Of course, as n increases from n0, then Eq. (4) quickly deviates from Eq. (2).\nIn other words, for a particular n, ¬µP and SP can be identical up to the choice of some constants (in\nthis case n0), but ¬µP determines a different ‚Äúset\" of networks and optimization trajectory than SP as\none varies n. As we will see empirically in the next section, this deviation is crucial for HP transfer.\nIndeed, in Fig. 3(right), we plot the CIFAR10 performances, over various learning rates and widths,\nof ¬µP MLPs with n0 = 128. In contrast to SP, the optimal learning rate under ¬µP is stable. This\nmeans that, the best learning rate for a width-128 network is also best for a width-8192 network in ¬µP\n‚Äî i.e. HP transfer works ‚Äî but not for SP. In addition, we observe performance for a Ô¨Åxed learning\nrate always weakly improves with width in ¬µP , but not in SP.\nThis MLP ¬µP example can be generalized easily to general neural networks trained under SGD or\nAdam, as summarized in Table 3, which is derived in Appendix J.\n7While superÔ¨Åcially different, this parametrization is equivalent to the ¬µP deÔ¨Åned in [57].\n5\n\n14\n12\n10\n8\n1\n2\n3\n4\n5\nWidth\nTraining Loss\nWidth\n128\n256\n512\n1024\n2048\n4096\n5\n0\n5\n10\n15\n1\n2\n3\n4\n5\n5.0\n2.5\n0.0\n2.5\n1\n2\n3\n4\n5\n(a)\n(b)\n(c)\n(d)\n(e)\n(f)\n1.0\n1.5\n2.0\n2.5\n3.0\n3.5\n4.0\n4.5\n14\n12\n10\n8\nlog2LearningRate\n3.75\n4.00\n4.25\n4.50\n4.75\n5.00\n5.25\n5.50\n5.75\nDepth\nTraining Loss\nDepth\n2\n4\n8\n16\n32\n5\n0\n5\n10\n15\nlog2\noutput\n3.0\n3.5\n4.0\n4.5\n5.0\n5.0\n2.5\n0.0\n2.5\nlog2InitStd\n3.5\n4.0\n4.5\n5.0\n5.5\n6.0\n6.5\n7.0\n(a)\n(b)\n(c)\n(d)\n(e)\n(f)\nLR Schedule\n3.8\n3.9\n4.0\n4.1\n4.2\n4.3\n4.4\n4.5\nFigure 4: Empirical validation of the stability of four representative hyperparameters on pre-\nLN Transformers in ¬µP: learning rate, last layer weight multiplier Œ±output, weight initialization\nstandard deviation, and learning rate schedule. We use the following learning rate schedules: (a)\nlinear decay; (b) StepLR @ [5k, 8k] with a decay factor of 0.1; (c) StepLR @ [4k, 7k] with a decay\nfactor of 0.3; (d) cosine annealing; (e) constant; (f) inverse square-root decay. All models are trained\non wikitext-2 for 10k steps. When not speciÔ¨Åed in the legend, the width used is 256, depth 2, batch\nsize 20, sequence length 256, and LR schedule constant. We sweep a particular HP, corresponding to\neach column, while Ô¨Åxing all others constant. See Section 6.1 for discussion of these results.\nTransformers with ¬µP\nWe repeat the experiments with base width n0 = 128 for Transformers:\nDeÔ¨Ånition 4.1. The Maximal Update Parametrization (¬µP) for a Transformer is given by Table 3\nand 1/d attention instead of 1/\n‚àö\nd, i.e. the attention logit is calculated as q‚ä§k/d instead of q‚ä§k/\n‚àö\nd\nwhere query q and key k have dimension d.8\nThe results are shown on the right in Fig. 1, where the optimal learning rate is stable, and the\nperformance improves monotonically with width. See Appendix B for further explanation of ¬µP.\n5\nThe Defects of SP and How ¬µP Fixes Them\nThe question of SP vs ¬µP has already been studied at length in [57]. Here we aim to recapitulate the\nkey insights, with more explanations given in Appendix J.3.\nAn Instructive Example\nAs shown in [57] and Appendix J.3, in SP, the network output will blow\nup with width after 1 step of SGD. It‚Äôs instructive to consider a 1-hidden-layer linear perceptron\nf(x) = V ‚ä§Ux with scalar inputs and outputs, as well as weights V, U ‚ààRn√ó1. In SP, VŒ± ‚àº\nN(0, 1/n) ad UŒ± ‚àºN(0, 1) for each Œ± ‚àà[n]. This sampling ensures that f(x) = Œò(|x|) at\ninitialization. After 1 step of SGD with learning rate 1, the new weights are V ‚Ä≤ ‚ÜêV + Œ∏U, U ‚Ä≤ ‚Üê\nU + Œ∏V , where Œ∏ is some scalar of size Œò(1) depending on the inputs, labels, and loss function. But\nnow\nf(x) = V ‚Ä≤‚ä§U ‚Ä≤x = (V ‚ä§U + Œ∏U ‚ä§U + Œ∏V ‚ä§V + Œ∏2U ‚ä§V )x\n(5)\nblows up with width n because U ‚ä§U = Œò(n) by Law of Large Numbers.\nNow consider the same network in ¬µP. According to Table 3, we now have VŒ± ‚àºN(0, 1/n2) in\ncontrast to SP, but UŒ± ‚àºN(0, 1) as before, with learning rates Œ∑V = 1/n, Œ∑U = n. After 1 step of\nSGD, we now have\nf(x) = (V ‚ä§U + Œ∏n‚àí1U ‚ä§U + Œ∏nV ‚ä§V + Œ∏2U ‚ä§V )x,\n8This is roughly because during training, q and k will be correlated so q‚ä§k actually scales like d due to Law\nof Large Numbers, in contrast to the original motivation that q, k are uncorrelated at initialization so Central\nLimit applies instead. See Appendix J.2.1 for a more in-depth discussion.\n6\n\n0.0\n0.5\n1.0\n1.5\nSP\nstd(xt\nx0)\nlogits\nt\n0\n1\n2\n3\n4\n0\n20\n40\n60\nattn logits\n0.0000\n0.0005\n0.0010\n0.0015\n0.0020\nword embedding\n0\n2000\n4000\nwidth\n0.00\n0.05\n0.10\n0.15\nP\nstd(xt\nx0)\n0\n2000\n4000\nwidth\n0.000\n0.025\n0.050\n0.075\n0.100\n0.125\n0\n2000\n4000\nwidth\n0.0000\n0.0005\n0.0010\n0.0015\nFigure 5: Logits and attention logits, but not word embeddings, of a Transformer blow up with\nwidth in SP after 1 step of training. In contrast, all three are well-behaved with width in ¬µP. Here\nwe measure how much different values change coordinatewise from initialization over 4 steps of\nAdam updates, as a function of width. SpeciÔ¨Åcally, we plot the standard deviation of the coordinates\nof xt ‚àíx0, for t = 0, . . . , 4, and x ‚àà{logits, attention logits, word embeddings}, where t = 0\nindicates initialization.\nand one can verify this is Œò(1) and thus does not blow up with width.9\nSome Layers Update Too Fast, Others Too Slow\nOne can observe the same behavior in more\nadvanced architectures like Transformers and optimizers like Adam; in fact, in SP, other hidden\nquantities like attention logits will also blow up with width after 1 step, but in ¬µP still remain bounded,\nas shown in Fig. 5(middle).\nOne might think scaling down the learning rate with width can solve this problem in SP. However,\nother hidden activations like the word embedding (Fig. 5(right)) in a Transformer update by a width-\nindependent amount for each step of training, so scaling down the learning rate will effectively mean\nthe word embeddings are not learned in large width models. Similar conclusions apply to other\nmodels like ResNet (in fact, one can observe in the SP linear MLP example above, the input layer\nis updated much more slowly than the output layer). On the other hand, ¬µP is designed so that all\nhidden activations update with the same speed in terms of width (see Appendix J.2 for why).\nPerformance Advantage of ¬µP\nThis is why a wide model tuned with ¬µTransfer should in general\noutperform its SP counterpart with (global) learning rate tuned. For example, this is the case for\nthe width-8192 Transformer in Fig. 1, where, in SP, the optimal learning rate needs to mollify the\nblow-up in quantities like logits and attention logits, but this implies others like word embeddings do\nnot learn appreciably. This performance advantage means ¬µTransfer does more than just predicting\nthe optimal learning rate of wide SP models. Relatedly, we observe, for any Ô¨Åxed HP combination,\ntraining performance never decreases with width in ¬µP, in contrast to SP (e.g., the ¬µP curves in\nFigs. 1, 3 and 16 do not cross, but the SP curves do; see also Section 8).\n6\nWhich Hyperparameters Can Be ¬µTransferred?\nIn this section, we explore how common HPs Ô¨Åt into our framework. In general, they can be divided\ninto three kinds, summarized in Table 1:\n1. those that can transfer from the small to the large model, such as learning rate (Table 2);\n2. those that primarily control regularization and don‚Äôt work well with our technique; and\n3. those that deÔ¨Åne training scale, such as width as discussed above as well as others like depth\nand batch size, across which we transfer other HPs.\nThose in the Ô¨Årst category transfer across width, as theoretically justiÔ¨Åed above in Section 2. To\npush the practicality and generality of our technique, we empirically explore the transfer across\n9Note in this example, Glorot initialization [13] (i.e. with variance 1/(fan_in + fan_out)) would scale\nasymptotically the same as ¬µP and thus is similarly well-behaved. However, if one adds layernorm or batchnorm,\nthen Glorot will cause logit blowup like SP, but ¬µP still will not.\n7\n\nthe other dimensions in the third category. Note that ¬µTransfer across width is quite general, e.g.\nit allows varying width ratio of different layers or number of attention heads in a Transformer;\nsee Appendix E.2. This will be very useful in practice. For the second category, the amount of\nregularization (for the purpose of controlling overÔ¨Åtting) naturally depends on both the model size\nand data size, so we should not expect transfer to work if the parametrization only depends on model\nsize. We discuss these HPs in more detail in Appendix E.1.\n6.1\nEmpirical Validation and Limitations\nOur empirical investigations focus on Transformers (here) and ResNet (in Appendix G.1.1), the\nmost popular backbones of deep learning models today. We train a 2-layer pre-layernorm ¬µP10\nTransformer with 4 attention heads on Wikitext-2. We sweep one of four HPs (learning rate, output\nweight multiplier, initialization standard deviation, and learning rate schedule) while Ô¨Åxing the others\nand sweeping along width and depth (with additional results in Fig. 19 on transfer across batch size,\nsequence length, and training time). Fig. 4 shows the results averaged over 5 random seeds.\nEmpirically, we Ô¨Ånd that for language modeling on Transformers, HPs generally transfer across scale\ndimensions if some minimum width (e.g. 256), depth (e.g., 4), batch size (e.g., 32), sequence length\n(e.g., 128), and training steps (e.g., 5000) are met, and the target scale is within the ‚Äúreasonable range‚Äù\nas in our experiments. Now, there are some caveats. While the exact optimum can shift slightly\nwith increasing scale, this shift usually has very small impact on the loss, compared to SP (Figs. 1\nand 3(left)). However, there are some caveats. For example, the best initialization standard deviation\ndoes not seem to transfer well across depth (2nd row, 3rd column), despite having a stabler optimum\nacross width. In addition, while our results on width, batch size, sequence length, and training time\nstill hold for post-layernorm (Fig. 17),11 the transfer across depth only works for pre-layernorm\nTransformer. Nevertheless, in practice (e.g. our results in Section 7.3) we Ô¨Ånd that Ô¨Åxing initialization\nstandard deviation while tuning other HPs works well when transferring across depth.\n7\nEfÔ¨Åciency and Performance of ¬µTransfer\nNow that the plausibility of ¬µTransfer has been established in toy settings, we turn to more realistic\nscenarios to see if one can achieve tangible gains. SpeciÔ¨Åcally, we perform HP tuning only on a\nsmaller proxy model, test the obtained HPs on the large target model directly, and compare against\nbaselines tuned using the target model. We seek to answer the question: Can ¬µTransfer make HP\ntuning more efÔ¨Åcient while achieving performance on par with traditional tuning? As we shall see by\nthe end of the section, the answer is positive. We focus on Transformers here, while experiments on\nResNets on CIFAR10 and Imagenet can be found as well in Appendix G.1. All of our experiments\nare run on V100 GPUs.\n7.1\nTransformer on IWSLT14 De-En\nSetup\nIWSLT14 De-En is a well-known machine translation benchmark. We use the default IWSLT\n(post-layernorm) Transformer implemented in fairseq [33] with 40M parameters, which we denote\nas the 1x model.12 For ¬µTransfer, we tune on a 0.25x model with 1/4 of the width, amounting to\n4M parameters. For this experiment, we tune via random search the learning rate Œ∑, the output layer\nparameter multiplier Œ±output, and the attention key-projection weight multiplier Œ±attn. See the grid\nand other experimental details in Appendix F.1.\nWe compare transferring from the 0.25x model with tuning the 1x model while controlling the total\ntuning budget in FLOPs.13 To improve the reproducibility of our result: 1) we repeat the entire HP\nsearch process (a trial) 25 times for each setup, with number of samples as indicated in Table 4, and\nreport the 25th, 50th, 75th, and 100th percentiles in BLEU score; 2) we evaluate each selected HP\ncombination using 5 random initializations and report the mean performance.14\n10‚Äú2 layers‚Äù means the model has 2 self-attention blocks. To compare with SP Transformer, see Fig. 18.\n11in fact, post-layernorm Transformers are much more sensitive to HPs than pre-layernorm, so our technique\nis more crucial for them, especially for transfer across width. Fig. 1 uses post-layernorm.\n12https://github.com/pytorch/fairseq/blob/master/examples/translation/README.md.\n13Ideally we would like to measure the wall clock time used for tuning. However, smaller models such as the\nproxy Transformer used for IWSLT are not efÔ¨Åcient on GPUs, so wall clock time would not reÔ¨Çect the speedup\nfor larger models like GPT-3. Thus, we measure in FLOPs, which is less dependent on hardware optimization.\n14We do not report the standard deviation over random initializations to avoid confusion.\n8\n\nTable 4: Transformer on IWSLT14 De-En. 1x and 0.25x refers to scaling of width only. Compared\nto traditional tuning (‚ÄúTuning on 1x‚Äù), ¬µTransfer from 0.25x provides better and more reliable\noutcome given Ô¨Åxed amount of compute. On the other hand, naive transfer (i.e. with SP instead of\n¬µP) fails completely. The percentiles are over independent trials, with each trial involving the entire\ntuning process with a new HP random search.\nVal. BLEU Percentiles\nSetup\nTotal Compute\n#Samples\n25\n50\n75\n100\nfairseq[33] default\n-\n-\n-\n-\n-\n35.40\nTuning on 1x\n1x\n5\n33.62\n35.00\n35.35\n35.45\nNaive transfer from 0.25x\n1x\n64\ntraining diverged\n¬µTransfer from 0.25x (Ours)\n1x\n64\n35.27\n35.33\n35.45\n35.53\nWe pick the HP combination that achieves the lowest validation loss15 for each trial. The reported\nbest outcome is chosen according to the validation loss during tuning. We compare against the default\nin fairseq, which is presumably heavily tuned. The result is shown in Table 4.\n4\n3\n2\n1\n0\n1\n2\n3\nlog2Compute\n34.4\n34.6\n34.8\n35.0\n35.2\n35.4\nBLEU Score\nMethod\nOurs\nConventional\n10\n20\n30\n40\n50\n60\n# Samples\nMethod\nOurs\nConventional\nFigure 6: EfÔ¨Åciency-performance Pareto fron-\ntier of ¬µTransfer compared to conventional tuning,\non IWSLT Transformer, using random HP search\nas the base method. We plot the median BLEU\nscore over 25 trials (Left) against relative compute\nbudget in log scale and (Right) against number\nof HP samples taken. While with the same num-\nber of samples, ¬µTransfer slightly underperforms\nconventional tuning, this gap vanishes with more\nsamples, and in terms of compute, our Pareto fron-\ntier strongly and consistently dominates that of\nconventional tuning. Note that, in larger models\n(e.g. BERT or GPT-3, not shown here), we believe\nour efÔ¨Åciency advantage will only widen as our\nsmall proxy model can stay the same size while\nthe target model grows.\nPerformance Pareto Frontier\nThe result\nabove only describes a particular compute bud-\nget. Is ¬µTransfer still preferable when we have\na lot more (or less) compute? To answer this\nquestion, we produce the compute-performance\nPareto frontier in Fig. 6(left), where we repeat\nthe above experiment with different compute\nbudgets. Evidently, our approach completely\ndominates conventional tuning.\nSample Quality of Proxy Model vs Target\nModel\nThe Pareto frontier in Fig. 6(right) sug-\ngests that, given a Ô¨Åxed number of random sam-\nples from the HP space, 1) tuning the target\nmodel directly yields slightly better results than\ntuning the proxy model (while taking much\nmore compute of course), but 2) this perfor-\nmance gap seems to vanish as more samples\nare taken. This can be explained by the intu-\nition that the narrower proxy model is a ‚Äúnoisy\nestimator‚Äù of the wide target model [57].With\nfew samples, this noise can distort the random\nHP search, but with more samples, this noise is\nsuppressed.\n7.2\nTransformer on WMT14 En-De\nWe scale up to WMT14 En-De using the large (post-layernorm) Transformer from [50] with 211M\nparameters. We tune on a proxy model with 15M parameters by shrinking dmodel, dffn, and nhead.\nFor this experiment, we tune via random search the learning rate Œ∑, the output layer parameter\nmultiplier Œ±output, and the attention key-projection weight multiplier Œ±attn following the grid\nin Appendix F.2. The result is shown in Table 5: While random search with 3 HP samples far\nunderperforms the fairseq default, we are able to match it via transfer using the same tuning budget.\n7.3\nBERT\nFinally, we consider large-scale language model pretraining where HP tuning is known to be challeng-\ning. Using Megatron (pre-layernorm) BERT [43] as a baseline, we hope to recover the performance\nof the published HPs by only tuning a proxy model that has roughly 13M parameters, which we call\nBERT-prototype. While previous experiments scaled only width, here we will also scale depth, as\ndiscussed in Section 6 and validated in Fig. 4. We use a batch size of 256 for all runs and follow the\n15We Ô¨Ånd this provides more reliable result than selecting for the best BLEU score.\n9\n\nTable 5: Transformers on WMT14 En-De. 1x and 0.25x refers to scaling of width only. We report\nBLEU Ô¨Çuctuation over 3 independent trials, i.e., 3 independent random HP searches.\nVal. BLEU Percentiles\nSetup\nTotal Compute\n#Samples\nWorst\nMedian\nBest\nfairseq[33] default\n-\n-\n-\n-\n26.40\nTuning on 1x\n1x\n3\ntraining diverged\n25.69\nNaive transfer from 0.25x\n1x\n64\ntraining diverged\n¬µTransfer from 0.25x (Ours)\n1x\n64\n25.94\n26.34\n26.42\nstandard Ô¨Ånetuning procedures. For more details on BERT-prototype, what HPs we tune, and how we\nÔ¨Ånetune the trained models, see Appendix F.3.\nDuring HP tuning, we sample 256 combinations from the search space and train each combination\non BERT-prototype for 105 steps. The total tuning cost measured in FLOPs is roughly the same as\ntraining 1 BERT-large for the full 106 steps; the exact calculation is shown in Appendix F.3. The\nresults are shown in Table 6. Notice that on BERT-large, we obtain sizeable improvement over the\nwell-tuned Megatron BERT-large baseline.\nTable 6: BERT pretraining. HP transfer outperforms published baselines without tuning the full\nmodel directly at all. We tune BERT-base and BERT-large simultaneously via a single proxy model,\nBERT-prototype. The total tuning cost = the cost of pretraining a single BERT-large. Model speedup\nrefers to the training speedup of BERT-prototype over BERT-base or BERT-large. Total speedup in\naddition includes time saving from transferring across training steps. Both speedups can be interpreted\neither as real-time speedup on V100s or as FLOPs speedup (which turn out to be empirically very\nsimilar in this case).\nModel\nMethod\nModel Speedup\nTotal Speedup\nTest loss\nMNLI (m/mm)\nQQP\nBERTbase\nMegatron Default\n1x\n1x\n1.995\n84.2/84.2\n90.6\nBERTbase\nNaive Transfer\n4x\n40x\ntraining diverged\nBERTbase\n¬µTransfer (Ours)\n4x\n40x\n1.970\n84.3/84.8\n90.8\nBERTlarge\nMegatron Default\n1x\n1x\n1.731\n86.3/86.2\n90.9\nBERTlarge\nNaive Transfer\n22x\n220x\ntraining diverged\nBERTlarge\n¬µTransfer (Ours)\n22x\n220x\n1.683\n87.0/86.5\n91.4\n7.4\nGPT-3\nIn order to further verify ¬µTransfer at scale, we applied it to GPT-3 6.7B [7] with relative attention.\nThis target model consists of 32 residual blocks with width 4096. We form the small proxy model by\nshrinking width to 256, resulting in roughly 40 million trainable parameters, 168 times smaller than\nthe target model. HPs were then determined by a random search on the proxy model. The total tuning\ncost was only 7% of total pretraining cost. Details of the HP sweep can be found in Appendix F.4.\nIn order to exclude code difference as a possible confounder, we also re-trained GPT-3 6.7B from\nscratch using the original HPs from [7]. Unfortunately, after we have Ô¨Ånished all experiments, we\nfound this baseline mistakenly used absolute attention (like models in [7]) when it was supposed to\nuse relative attention like the target model. In addition, during training of the ¬µTransfer model we\nencountered numerical issues that lead to frequent divergences. In order to avoid them, the model was\ntrained using FP32 precision, even though the original 6.7B model and our re-run were trained using\nFP16.16 17 The resulting ¬µTransfer model outperforms the 6.7B from [7], and is in fact comparable\nto the twice-as-large 13B model across our evaluation suite (see Table 11). Selected evaluation results\ncan be found in Table 7 and further details are given in Table 10 and Appendix F.4.\n16While we are mainly focused on the efÔ¨Åcacy of ¬µTransfer regardless of precision, it would be interesting to\nablate the effect of precision in our results, but we did not have enough resources to rerun the baseline in FP32\n17It is quite interesting that ¬µTransfer identiÔ¨Åed a useful region of hyperparameters leading to much improved\nperformance, which probably would be difÔ¨Åcult to discover normally because 1) researchers usually change\nhyperparameters to accomodate precision and 2) there was no precise enough justiÔ¨Åcation to go against this\njudgment until ¬µTransfer.\n10\n\nTable 7: GPT-3 6.7B Pretraining. Selected evaluation results for the GPT-3 6.7B model tuned\nwith ¬µTransfer (transfered from a small proxy model of 40M parameters), compared to the results\npublished in [7] and a re-run with original HPs, as well as the 13B model in [7] for reference. Note\nthat the perplexities in this table are based on a custom tokenization and are not comparable to the\nliterature. The validation loss refers to the loss achieved on a random held-out part of our dataset.\nZero-shot, One-Shot and Few-Shot refer to the number of additional query and answer pairs passed in\nthe context when performing the sampling-based evaluations. See Appendix F.4 for full evaluation.\nTask\nMetric\n6.7B+¬µP\n6.7B re-run\n6.7B [7]\n13B [7]\nValidation loss\ncross-entropy\n1.98\n2.03\n-\n-\nPTB\nperplexity\n11.4\n13.0\n-\n-\nWikiText-103\nperplexity\n8.56\n9.13\n-\n-\nOne Billion Words\nperplexity\n20.5\n21.7\n-\n-\nLAMBADA Zero-Shot\naccuracy\n73.5\n70.8\n70.3\n72.5\nLAMBADA One-Shot\naccuracy\n69.9\n64.8\n65.4\n69.0\nLAMBADA Few-Shot\naccuracy\n74.7\n77.1\n79.1\n81.3\nHellaSwag Zero-Shot\naccuracy\n72.0\n66.7\n67.4\n70.9\nHellaSwag One-Shot\naccuracy\n71.1\n65.9\n66.5\n70.0\nHellaSwag Few-Shot\naccuracy\n72.4\n66.4\n67.3\n71.3\n0\n2000\n4000\n6000\n8000\n10000\nTraining Step\n1\n2\n3\n4\n5\n6\n7\n8\n9\nTraining Loss\nP LR=0.001\nWidth\n128\n256\n512\n1024\n2048\n4096\n0\n2000\n4000\n6000\n8000\n10000\nTraining Step\nSP LR=0.001\n0\n2000\n4000\n6000\n8000\n10000\nTraining Step\nSP LR=0.00025\nFigure 7: Wider is always better in training loss under ¬µP, but not in SP, given the same HP.\nLearning curves for ¬µP and SP with different learning rates, aggregated over 5 seeds. (Left) Wider\n¬µP models always achieve better training loss at any time in training. (Middle) If using a small\nlearning rate, SP models can appear to do so up to some large width, at which point the pattern fails\n(at width 2048 in our plot). (Right) If using a large learning rate, SP model can strictly do worse with\nwidth; here the SP model is identical to the ¬µP model in (Left) at width 128.\n8\nWider is Better in ¬µP Throughout Training\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\n1.2\n1.4\nTraining tokens\n1e9\n3.0\n3.5\n4.0\n4.5\n5.0\n5.5\n6.0\nValidation loss\nmodel width\n256\n512\n1024\n2048\n4096\n8192\n16384\n32768\nFigure 8: Stress-testing ‚Äúwider-is-better‚Äù in ¬µP.\nHere we trained a GPT-3 transformer with 4 layers\nand widths from 256 to 32,768. Modulo a brief\nperiod around 1e8 training tokens, wider is better\nthroughout training.\nIn earlier plots like Figs. 1 and 3, we saw that\nat the end of training, wider is always better\nin ¬µP but not in SP. In fact, we Ô¨Ånd this to\nbe true throughout training, as seen in Fig. 7,\nmodulo noise from random initialization and/or\ndata ordering, and assuming the output layer is\nzero-initialized (which has no impact on perfor-\nmance as discussed in Appendix D.2). We then\nstress-tested this on a ¬µP GPT-3 Transformer\n(on the GPT-3 training data) by scaling width\nfrom 256 to 32,768 using a Ô¨Åxed set of HPs\n(Fig. 8). Wider models consistently match or\noutperform narrower models at each point in\ntraining (except a brief period around 1e8 train-\ning tokens, likely due to noise because we ran\nonly 1 seed due to computational cost). Our ob-\nservation suggests that wider models are strictly\nmore data-efÔ¨Åcient if scaled appropriately. By\nchecking ‚Äúwider-is-better‚Äù early in training, one can also cheaply debug a ¬µP implementation.\n11\n\n9\nUseful Hyperparameter Transfer: A Theoretical Puzzle\nWe want to tune HPs on a small model with width N such that its HP landscape looks like that of\na large model with width ‚â´N. Our intuition in Section 2 and Appendices C and J leads us to ¬µP.\nHowever, for this to be useful, we do not want the small model (as a function) after training to be\nclose to that of the large model ‚Äî otherwise there is no point in training the large model to begin\nwith. So N 1) must be large enough so that the HP optimum converges, but 2) cannot be so large\nthat the functional dynamics (and the loss) converges. The fact that such N exists, as demonstrated\nby our experiments, shows that: In some sense, the HP optimum is a ‚Äúmacroscopic‚Äù or ‚Äúcoarse‚Äù\nvariable which converges quickly with width, while the neural network function (and its loss) is a very\n‚Äúmicroscopic‚Äù or ‚ÄúÔ¨Åne‚Äù detail that converges much more slowly with width. However, theoretically,\nit is unclear why this should happen, and where else we should expect such useful HP transfer. We\nleave an explanation to future work.\n10\nRelated Works\n10.1\nHyperparameter Tuning\nMany have sought to speedup HP tuning beyond the simple grid or random search. Snoek et al.\n[45] treated HP tuning as an optimization process and used Bayesian optimization by treating the\nperformance of each HP combination as a sample from a Gaussian process (GP). Snoek et al. [46]\nfurther improved the runtime by swapping the GP with a neural network. Another thread of work\ninvestigated how massively parallel infrasture can be used for efÔ¨Åcient tuning under the multi-arm\nbandit problem [18, 22]. There are also dedicated tools such as Optuna [4] and Talos [3] which\nintegrate with existing deep learning frameworks and provide an easy way to apply more advanced\ntuning techniques.\nOur approach is distinct from all of the above in that it does not work on the HP optimization process\nitself. Instead, it decouples the size of the target model from the tuning cost, which was not feasible\nprior to this work. This means that no matter how large the target model is, we can always use a\nÔ¨Åxed-sized proxy model to probe its HP landscape. Nevertheless, our method is complementary,\nas the above approaches can naturally be applied to the tuning of the proxy model; it is only for\nscientiÔ¨Åc reasons that we use either grid search or random search throughout this work.\n10.2\nHyperparameter Transfer\nMany previous works explored transfer learning of HP tuning (e.g. [15, 36, 47, 62]). However, to the\nbest of our knowledge, our work is the Ô¨Årst to explore zero-shot HP transfer. In addition, we focus on\ntransferring across model scale rather than between different tasks or datasets. Some algorithms like\nHyperband [23] can leverage cheap estimates of HP evaluations (like using a small model to proxy a\nlarge model) but they are not zero-shot algorithms, so would still be very expensive to apply to large\nmodel training. Nevertheless, all of the above methods are complementary to ours as they can be\napplied to the tuning of our proxy model.\n10.3\nPreviously Proposed Scaling Rules of Hyperparameters\n(Learning Rate, Batch Size) Scaling\n[44] proposed to scale learning rate with batch size while\nÔ¨Åxing the total epochs of training; [14] proposed to scale learning rate as\n‚àö\nbatchsize while Ô¨Åxing\nthe total number of steps of training. However, [41] showed that there‚Äôs no consistent (learning\nrate, batch size) scaling law across a range of dataset and models. Later, [30] studied the trade-off\nof training steps vs computation as a result of changing batch size. They proposed an equation of\na/(1 + b/batchsize), where a and b are task- and model-speciÔ¨Åc constants, for the optimal learning\nrate (see their Ô¨Åg 3 and Ô¨Åg 5). This law suggests that for sufÔ¨Åciently large batch size, the optimal\nlearning rate is roughly constant.18 This supports our results here as well as the empirical results in\n[41, Ô¨Åg 8].\nLearning Rate Scaling with Width\nAssuming that the optimal learning rate should scale with\nbatch size following [44], [34] empirically investigated how the optimal ‚Äúnoise ratio‚Äù LR/batchsize\nscales with width for MLP and CNNs in NTK parametrization (NTP) or standard parametrization\n18while the optimal learning is roughly linear in batch size when the latter is small\n12\n\n(SP) trained with SGD. They in particular focus on test loss in the regime of small batch size and\ntraining to convergence. In this regime, they claimed that in networks without batch normalization,\nthe optimal noise ratio is constant in SP but scales like 1/width for NTP. However, they found this\nlaw breaks down for networks with normalization.\nIn contrast, here we focus on training loss, without training to convergence and with a range of batch\nsizes from small to very large (as is typical in large scale pretraining). Additionally, our work applies\nuniversally to 1) networks with normalization, along with 2) Adam and other adaptive optimizers;\nfurthermore 3) we empirically validate transfer across depth and sequence length, and 4) explicitly\nvalidate tuning via ¬µTransfer on large models like BERT-large and GPT-3.\nFinally, as argued in [57] and Appendix J.3, SP and NTP lead to bad inÔ¨Ånite-width limits in contrast\nto ¬µP and hence are suboptimal for wide neural networks. For example, sufÔ¨Åciently wide neural\nnetworks in SP and NTP would lose the ability to learn features, as concretely demonstrated on\nword2vec in [57].\nInput Layer Parametrization\nThe original formulation of ¬µP in [57] (see Table 9, which is\nequivalent to Table 3) uses a fan-out initialization for the input layer. This is atypical in vision\nmodels, but in language models where the input and output layers are shared (corresponding to word\nembeddings), it can actually be more natural to use a fan-out initialization (corresponding to fan-in\ninitialization of the output layer). In fact, we found that fairseq [33] by default actually implements\nboth the fan-out initialization and the ‚àöfan_out multiplier.\nOther Scaling Rules\nMany previous works proposed different initialization or parametrizations\nwith favorable properties, such as better stability for training deep neural networks [5, 13, 16, 26, 40,\n59, 60, 66]. Our work differs from these in that we focus on the transferability of optimal HPs from\nsmall models to large models in the same parametrization.\n10.4\nInÔ¨Ånite-Width Neural Networks: From Theory to Practice and Back\n[57] introduced ¬µP as the unique parametrization that enables all layers of a neural network to learn\nfeatures in the inÔ¨Ånite-width limit, especially in contrast to the NTK parametrization [17] (which\ngives rise to the NTK limit) that does not learn features in the limit. Based on this theoretical insight,\nin Appendix J.3, we argue that ¬µP should also be the unique parametrization (in the sense of [57]) that\nallows HP transfer across width; in short this is because it both 1) preserves feature learning, so that\nperformance on feature learning tasks (such as language model pretraining) does not become trivial\nin the limit, and 2) ensures each parameter tensor is not stuck at initialization in the large width limit,\nso that its learning rate does not become meaningless. At the same time, our results here suggest\nthat ¬µP is indeed the correct parametrization for wide neural networks and thus provide empirical\nmotivation for the theoretical study of the inÔ¨Ånite-width ¬µP limit. Note, parametrization here refers\nto a rule to scale hyperparameters with width (‚Äúhow should my initialization and learning rate change\nwhen my width doubles?‚Äù), which is coarser than a prescription for setting hyperparameters at any\nparticular width (‚Äúhow should I set my initialization and learning rate at width 1024?‚Äù).\n11\nConclusion\nLeveraging the discovery of a feature learning neural network inÔ¨Ånite-width limit, we hypothesized\nand veriÔ¨Åed that the HP landscape across NNs of different width is reasonably stable if parametrized\naccording to Maximal Update Parametrization (¬µP). We further empirically showed that it‚Äôs possible\nto transfer across depth, batch size, sequence length, and training time, with a few caveats. This\nallowed us to indirectly tune a very large network by tuning its smaller counterparts and transferring\nthe HPs to the full model. Our results raise an interesting new theoretical question of how useful HP\ntransfer is possible in neural networks in the Ô¨Årst place.\nVenues of Improvement\nNevertheless, our method has plenty of room to improve. For example,\ninitialization does not transfer well across depth, and depth transfer generally still does not work for\npost-layernorm Transformers. This begs the question whether a more principled parametrization in\ndepth could solve these problems. Additionally, Fig. 4 shows that the optimal HP still shifts slightly\nfor smaller models. Perhaps by considering Ô¨Ånite-width corrections to ¬µP one can Ô¨Åx this shift.\nFinally, it will be interesting to study if there‚Äôs a way to transfer regularization HPs as a function of\nboth the model size and data size, especially in the context of Ô¨Ånetuning of pretrained models.\n13\n\nAcknowledgements\nIn alphabetical order, we thank Arthur Jacot, Arturs Backurs, Colin Raffel,\nDenny Wu, Di He, Huishuai Zhang, Ilya Sutskever, James Martens, Janardhan Kulkarni, Jascha\nSohl-Dickstein, Jeremy Bernstein, Lenaic Chizat, Luke Metz, Mark Chen, Michael Santacroce,\nMuhammad ElNokrashy, Pengchuan Zhang, Sam Schoenholz, Sanjeev Arora, Taco Cohen, Yiping\nLu, Yisong Yue, and Yoshua Bengio for discussion and help during our research.\nReferences\n[1] NVIDIA/DeepLearningExamples, apache v2 license. URL https://github.com/NVIDIA/\nDeepLearningExamples.\n[2] Davidnet, mit license, 2019. URL https://github.com/davidcpage/cifar10-fast.\n[3] Autonomio talos, mit license, 2019. URL http://github.com/autonomio/talos.\n[4] Takuya Akiba, Shotaro Sano, Toshihiko Yanase, Takeru Ohta, and Masanori Koyama. Optuna:\nA next-generation hyperparameter optimization framework, 2019.\n[5] Thomas Bachlechner, Bodhisattwa Prasad Majumder, Huanru Henry Mao, Garrison W. Cot-\ntrell, and Julian McAuley.\nReZero is All You Need: Fast Convergence at Large Depth.\narXiv:2003.04887 [cs, stat], June 2020. URL http://arxiv.org/abs/2003.04887.\n[6] Jeremy Bernstein, Arash Vahdat, Yisong Yue, and Ming-Yu Liu. On the distance between two\nneural networks and the stability of learning. arXiv:2002.03432 [cs, math, stat], January 2021.\nURL http://arxiv.org/abs/2002.03432.\n[7] Tom B. Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared Kaplan, Prafulla Dhariwal,\nArvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, Sandhini Agarwal, Ariel\nHerbert-Voss, Gretchen Krueger, Tom Henighan, Rewon Child, Aditya Ramesh, Daniel M.\nZiegler, Jeffrey Wu, Clemens Winter, Christopher Hesse, Mark Chen, Eric Sigler, Mateusz\nLitwin, Scott Gray, Benjamin Chess, Jack Clark, Christopher Berner, Sam McCandlish, Alec\nRadford, Ilya Sutskever, and Dario Amodei. Language models are few-shot learners, 2020.\n[8] Simon Carbonnelle and Christophe De Vleeschouwer. Layer rotation: a surprisingly powerful\nindicator of generalization in deep networks? arXiv:1806.01603 [cs, stat], July 2019. URL\nhttp://arxiv.org/abs/1806.01603.\n[9] Ciprian Chelba, Tomas Mikolov, Mike Schuster, Qi Ge, Thorsten Brants, Phillipp Koehn, and\nTony Robinson. One billion word benchmark for measuring progress in statistical language\nmodeling, 2014.\n[10] Zihang Dai, Zhilin Yang, Yiming Yang, Jaime Carbonell, Quoc V. Le, and Ruslan Salakhutdinov.\nTransformer-xl: Attentive language models beyond a Ô¨Åxed-length context, 2019.\n[11] Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. BERT: Pre-training of\nDeep Bidirectional Transformers for Language Understanding. arXiv:1810.04805 [cs], May\n2019. URL http://arxiv.org/abs/1810.04805.\n[12] Xiaohan Ding, Chunlong Xia, Xiangyu Zhang, Xiaojie Chu, Jungong Han, and Guiguang Ding.\nRepMLP: Re-parameterizing Convolutions into Fully-connected Layers for Image Recognition.\narXiv:2105.01883 [cs], August 2021. URL http://arxiv.org/abs/2105.01883.\n[13] Xavier Glorot and Yoshua Bengio. Understanding the difÔ¨Åculty of training deep feedforward\nneural networks. In Yee Whye Teh and Mike Titterington, editors, Proceedings of the Thirteenth\nInternational Conference on ArtiÔ¨Åcial Intelligence and Statistics, volume 9 of Proceedings of\nMachine Learning Research, pages 249‚Äì256, Chia Laguna Resort, Sardinia, Italy, May 2010.\nPMLR. URL http://proceedings.mlr.press/v9/glorot10a.html.\n[14] Elad Hoffer, Itay Hubara, and Daniel Soudry. Train longer, generalize better: closing the\ngeneralization gap in large batch training of neural networks. arXiv:1705.08741 [cs, stat], May\n2017. URL http://arxiv.org/abs/1705.08741.\n[15] Samuel Horv√°th, Aaron Klein, Peter Richt√°rik, and C√©dric Archambeau. Hyperparameter\ntransfer learning with adaptive complexity.\nCoRR, abs/2102.12810, 2021.\nURL https:\n//arxiv.org/abs/2102.12810.\n14\n\n[16] Xiao Shi Huang and Felipe P√©rez. Improving Transformer Optimization Through Better\nInitialization. page 9.\n[17] Arthur Jacot, Franck Gabriel, and Cl√©ment Hongler. Neural Tangent Kernel: Convergence\nand Generalization in Neural Networks. arXiv:1806.07572 [cs, math, stat], June 2018. URL\nhttp://arxiv.org/abs/1806.07572.\n[18] Kevin Jamieson and Ameet Talwalkar. Non-stochastic best arm identiÔ¨Åcation and hyperparame-\nter optimization, 2015.\n[19] Jared Kaplan, Sam McCandlish, Tom Henighan, Tom B. Brown, Benjamin Chess, Rewon Child,\nScott Gray, Alec Radford, Jeffrey Wu, and Dario Amodei. Scaling Laws for Neural Language\nModels. arXiv:2001.08361 [cs, stat], January 2020. URL http://arxiv.org/abs/2001.\n08361.\n[20] Diederik Kingma and Jimmy Ba. Adam: A method for stochastic optimization. arXiv preprint\narXiv:1412.6980, 2014.\n[21] Jaehoon Lee, Yasaman Bahri, Roman Novak, Sam Schoenholz, Jeffrey Pennington, and Jascha\nSohl-dickstein. Deep Neural Networks as Gaussian Processes. In International Conference on\nLearning Representations, 2018. URL https://openreview.net/forum?id=B1EA-M-0Z.\n[22] Liam Li, Kevin Jamieson, Afshin Rostamizadeh, Ekaterina Gonina, Moritz Hardt, Benjamin\nRecht, and Ameet Talwalkar. A system for massively parallel hyperparameter tuning, 2020.\n[23] Lisha Li, Kevin Jamieson, Giulia DeSalvo, Afshin Rostamizadeh, and Ameet Talwalkar. Hyper-\nband: A Novel Bandit-Based Approach to Hyperparameter Optimization. JMLR 18, page 52.\n[24] Hanxiao Liu, Zihang Dai, David R. So, and Quoc V. Le.\nPay Attention to MLPs.\narXiv:2105.08050 [cs], June 2021. URL http://arxiv.org/abs/2105.08050.\n[25] Liyuan Liu, Xiaodong Liu, Jianfeng Gao, Weizhu Chen, and Jiawei Han. Understanding\nthe difÔ¨Åculty of training transformers. In Proceedings of the 2020 Conference on Empirical\nMethods in Natural Language Processing (EMNLP), pages 5747‚Äì5763, Online, November\n2020. Association for Computational Linguistics. doi: 10.18653/v1/2020.emnlp-main.463.\nURL https://www.aclweb.org/anthology/2020.emnlp-main.463.\n[26] Liyuan Liu, Xiaodong Liu, Jianfeng Gao, Weizhu Chen, and Jiawei Han. Understanding the\nDifÔ¨Åculty of Training Transformers. arXiv:2004.08249 [cs, stat], September 2020. URL\nhttp://arxiv.org/abs/2004.08249.\n[27] Xiaodong Liu, Pengcheng He, Weizhu Chen, and Jianfeng Gao.\nMulti-task deep neural\nnetworks for natural language understanding. In Proceedings of the 57th Annual Meeting of\nthe Association for Computational Linguistics, pages 4487‚Äì4496, Florence, Italy, July 2019.\nAssociation for Computational Linguistics. URL https://www.aclweb.org/anthology/\nP19-1441.\n[28] Yang Liu, Jeremy Bernstein, Markus Meister, and Yisong Yue. Learning by Turning: Neural\nArchitecture Aware Optimisation. arXiv:2102.07227 [cs], September 2021. URL http:\n//arxiv.org/abs/2102.07227.\n[29] Alexander G. de G. Matthews, Mark Rowland, Jiri Hron, Richard E. Turner, and Zoubin\nGhahramani. Gaussian Process Behaviour in Wide Deep Neural Networks. arXiv:1804.11271\n[cs, stat], April 2018. URL http://arxiv.org/abs/1804.11271. arXiv: 1804.11271.\n[30] Sam McCandlish, Jared Kaplan, Dario Amodei, and OpenAI Dota Team. An Empirical\nModel of Large-Batch Training. arXiv:1812.06162 [cs, stat], December 2018. URL http:\n//arxiv.org/abs/1812.06162.\n[31] Luke Melas-Kyriazi. Do You Even Need Attention? A Stack of Feed-Forward Layers Does\nSurprisingly Well on ImageNet. arXiv:2105.02723 [cs], May 2021. URL http://arxiv.\norg/abs/2105.02723.\n[32] Stephen Merity, Caiming Xiong, James Bradbury, and Richard Socher. Pointer sentinel mixture\nmodels, 2016.\n15\n\n[33] Myle Ott, Sergey Edunov, Alexei Baevski, Angela Fan, Sam Gross, Nathan Ng, David Grangier,\nand Michael Auli. fairseq: A fast, extensible toolkit for sequence modeling, mit license. In\nProceedings of NAACL-HLT 2019: Demonstrations, 2019.\n[34] Daniel S. Park, Jascha Sohl-Dickstein, Quoc V. Le, and Samuel L. Smith. The Effect of Network\nWidth on Stochastic Gradient Descent and Generalization: an Empirical Study. May 2019.\nURL https://arxiv.org/abs/1905.03776v1.\n[35] Adam Paszke, Sam Gross, Francisco Massa, Adam Lerer, James Bradbury, Gregory\nChanan, Trevor Killeen, Zeming Lin, Natalia Gimelshein, Luca Antiga, Alban Desmai-\nson, Andreas Kopf, Edward Yang, Zachary DeVito, Martin Raison, Alykhan Tejani,\nSasank Chilamkurthy, Benoit Steiner, Lu Fang, Junjie Bai, and Soumith Chintala.\nPytorch:\nAn imperative style, high-performance deep learning library, bsd-style li-\ncense.\nIn H. Wallach, H. Larochelle, A. Beygelzimer, F. d√Ålch√© Buc, E. Fox, and\nR. Garnett, editors, Advances in Neural Information Processing Systems 32, pages\n8024‚Äì8035. Curran Associates, Inc., 2019.\nURL http://papers.neurips.cc/paper/\n9015-pytorch-an-imperative-style-high-performance-deep-learning-library.\npdf.\n[36] Valerio Perrone, Rodolphe Jenatton, Matthias W Seeger, and Cedric Archambeau. Scalable\nHyperparameter Transfer Learning. NeurIPS 2018, page 11.\n[37] Martin Popel and OndÀárej Bojar. Training Tips for the Transformer Model. The Prague Bulletin\nof Mathematical Linguistics, 110(1):43‚Äì70, April 2018. ISSN 1804-0462. doi: 10.2478/\npralin-2018-0002. URL http://content.sciendo.com/view/journals/pralin/110/\n1/article-p43.xml.\n[38] Colin Raffel, Noam Shazeer, Adam Roberts, Katherine Lee, Sharan Narang, Michael Matena,\nYanqi Zhou, Wei Li, and Peter J. Liu. Exploring the Limits of Transfer Learning with a UniÔ¨Åed\nText-to-Text Transformer. arXiv:1910.10683 [cs, stat], July 2020. URL http://arxiv.org/\nabs/1910.10683.\n[39] Jonathan S. Rosenfeld, Amir Rosenfeld, Yonatan Belinkov, and Nir Shavit. A Constructive\nPrediction of the Generalization Error Across Scales. arXiv:1909.12673 [cs, stat], December\n2019. URL http://arxiv.org/abs/1909.12673.\n[40] Samuel S. Schoenholz, Justin Gilmer, Surya Ganguli, and Jascha Sohl-Dickstein. Deep Infor-\nmation Propagation. arXiv:1611.01232 [cs, stat], November 2016. URL http://arxiv.org/\nabs/1611.01232.\n[41] Christopher J. Shallue, Jaehoon Lee, Joseph Antognini, Jascha Sohl-Dickstein, Roy Frostig,\nand George E. Dahl. Measuring the Effects of Data Parallelism on Neural Network Training.\narXiv:1811.03600 [cs, stat], November 2018. URL http://arxiv.org/abs/1811.03600.\n[42] Noam Shazeer and Mitchell Stern. Adafactor: Adaptive Learning Rates with Sublinear Memory\nCost. April 2018. URL https://arxiv.org/abs/1804.04235v1.\n[43] Mohammad Shoeybi, Mostofa Patwary, Raul Puri, Patrick LeGresley, Jared Casper, and Bryan\nCatanzaro. Megatron-lm: Training multi-billion parameter language models using model\nparallelism. CoRR, abs/1909.08053, 2019. URL http://arxiv.org/abs/1909.08053.\n[44] Samuel L. Smith, Pieter-Jan Kindermans, and Quoc V. Le. Don‚Äôt Decay the Learning Rate,\nIncrease the Batch Size. arXiv:1711.00489 [cs, stat], November 2017. URL http://arxiv.\norg/abs/1711.00489.\n[45] Jasper Snoek, Hugo Larochelle, and Ryan P. Adams. Practical bayesian optimization of machine\nlearning algorithms, 2012.\n[46] Jasper Snoek, Oren Rippel, Kevin Swersky, Ryan Kiros, Nadathur Satish, Narayanan Sundaram,\nMd. Mostofa Ali Patwary, Prabhat, and Ryan P. Adams. Scalable bayesian optimization using\ndeep neural networks, 2015.\n[47] Danny Stoll, J√∂rg K.H. Franke, Diane Wagner, Simon Selg, and Frank Hutter. Hyperparameter\ntransfer across developer adjustments, 2021. URL https://openreview.net/forum?id=\nWPO0vDYLXem.\n16\n\n[48] Ilya Tolstikhin, Neil Houlsby, Alexander Kolesnikov, Lucas Beyer, Xiaohua Zhai, Thomas\nUnterthiner, Jessica Yung, Andreas Steiner, Daniel Keysers, Jakob Uszkoreit, Mario Lucic, and\nAlexey Dosovitskiy. MLP-Mixer: An all-MLP Architecture for Vision. arXiv:2105.01601 [cs],\nJune 2021. URL http://arxiv.org/abs/2105.01601.\n[49] Hugo Touvron, Piotr Bojanowski, Mathilde Caron, Matthieu Cord, Alaaeldin El-Nouby,\nEdouard Grave, Gautier Izacard, Armand Joulin, Gabriel Synnaeve, Jakob Verbeek, and Herv√©\nJ√©gou. ResMLP: Feedforward networks for image classiÔ¨Åcation with data-efÔ¨Åcient training.\narXiv:2105.03404 [cs], June 2021. URL http://arxiv.org/abs/2105.03404.\n[50] Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N. Gomez,\nLukasz Kaiser, and Illia Polosukhin. Attention is all you need. CoRR, abs/1706.03762, 2017.\nURL http://arxiv.org/abs/1706.03762.\n[51] Alex Wang, Amanpreet Singh, Julian Michael, Felix Hill, Omer Levy, and Samuel R Bowman.\nGlue: A multi-task benchmark and analysis platform for natural language understanding.\nEMNLP 2018, page 353, 2018.\n[52] Adina Williams, Nikita Nangia, and Samuel Bowman. A broad-coverage challenge corpus\nfor sentence understanding through inference. In Proceedings of the 2018 Conference of the\nNorth American Chapter of the Association for Computational Linguistics: Human Language\nTechnologies, Volume 1 (Long Papers), pages 1112‚Äì1122. Association for Computational\nLinguistics, 2018. URL http://aclweb.org/anthology/N18-1101.\n[53] Greg Yang. Tensor Programs I: Wide Feedforward or Recurrent Neural Networks of Any Archi-\ntecture are Gaussian Processes. arXiv:1910.12478 [cond-mat, physics:math-ph], December\n2019. URL http://arxiv.org/abs/1910.12478.\n[54] Greg Yang. Scaling Limits of Wide Neural Networks with Weight Sharing: Gaussian Process\nBehavior, Gradient Independence, and Neural Tangent Kernel Derivation. arXiv:1902.04760\n[cond-mat, physics:math-ph, stat], February 2019. URL http://arxiv.org/abs/1902.\n04760.\n[55] Greg Yang. Tensor Programs II: Neural Tangent Kernel for Any Architecture. arXiv:2006.14548\n[cond-mat, stat], August 2020. URL http://arxiv.org/abs/2006.14548.\n[56] Greg Yang. Tensor Programs III: Neural Matrix Laws. arXiv:2009.10685 [cs, math], September\n2020. URL http://arxiv.org/abs/2009.10685.\n[57] Greg Yang and Edward J. Hu. Feature learning in inÔ¨Ånite-width neural networks. arXiv, 2020.\n[58] Greg Yang and Etai Littwin.\nTensor Programs IIb: Architectural Universality of Neural\nTangent Kernel Training Dynamics. arXiv:2105.03703 [cs, math], May 2021. URL http:\n//arxiv.org/abs/2105.03703.\n[59] Greg Yang and Sam S. Schoenholz. Deep Mean Field Theory: Layerwise Variance and\nWidth Variation as Methods to Control Gradient Explosion. February 2018. URL https:\n//openreview.net/forum?id=rJGY8GbR-.\n[60] Greg Yang and Samuel S. Schoenholz. Mean Field Residual Networks: On the Edge of Chaos.\narXiv:1712.08969 [cond-mat, physics:nlin], December 2017. URL http://arxiv.org/abs/\n1712.08969.\n[61] Greg Yang, Michael Santacroce, and Edward J Hu. EfÔ¨Åcient computation of deep nonlinear\ninÔ¨Ånite-width neural networks that learn features. In International Conference on Learning\nRepresentations, 2022. URL https://openreview.net/forum?id=tUMr0Iox8XW.\n[62] Dani Yogatama and Gideon Mann. EfÔ¨Åcient Transfer Learning Method for Automatic Hyperpa-\nrameter Tuning. In ArtiÔ¨Åcial Intelligence and Statistics, pages 1077‚Äì1085. PMLR, April 2014.\nURL http://proceedings.mlr.press/v33/yogatama14.html.\n[63] Yang You, Igor Gitman, and Boris Ginsburg. Large Batch Training of Convolutional Networks.\narXiv:1708.03888 [cs], September 2017. URL http://arxiv.org/abs/1708.03888.\n17\n\n[64] Yang You, Jing Li, Sashank Reddi, Jonathan Hseu, Sanjiv Kumar, Srinadh Bhojanapalli,\nXiaodan Song, James Demmel, Kurt Keutzer, and Cho-Jui Hsieh. Large Batch Optimization\nfor Deep Learning: Training BERT in 76 minutes. arXiv:1904.00962 [cs, stat], January 2020.\nURL http://arxiv.org/abs/1904.00962.\n[65] Sergey Zagoruyko and Nikos Komodakis. Wide residual networks, 2017.\n[66] Hongyi Zhang, Yann N. Dauphin, and Tengyu Ma. Residual Learning Without Normalization\nvia Better Initialization. In International Conference on Learning Representations, 2019. URL\nhttps://openreview.net/forum?id=H1gsz30cKX.\n18\n\nContents\n1\nIntroduction\n1\n2\nParametrization Matters: A Primer\n3\n3\nHyperparameters Don‚Äôt Transfer Conventionally\n4\n4\nUnlocking Zero-Shot Hyperparameter Transfer with ¬µP\n5\n5\nThe Defects of SP and How ¬µP Fixes Them\n6\n6\nWhich Hyperparameters Can Be ¬µTransferred?\n7\n6.1\nEmpirical Validation and Limitations . . . . . . . . . . . . . . . . . . . . . . . . .\n8\n7\nEfÔ¨Åciency and Performance of ¬µTransfer\n8\n7.1\nTransformer on IWSLT14 De-En . . . . . . . . . . . . . . . . . . . . . . . . . . .\n8\n7.2\nTransformer on WMT14 En-De\n. . . . . . . . . . . . . . . . . . . . . . . . . . .\n9\n7.3\nBERT . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n9\n7.4\nGPT-3 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n10\n8\nWider is Better in ¬µP Throughout Training\n11\n9\nUseful Hyperparameter Transfer: A Theoretical Puzzle\n12\n10 Related Works\n12\n10.1 Hyperparameter Tuning . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n12\n10.2 Hyperparameter Transfer . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n12\n10.3 Previously Proposed Scaling Rules of Hyperparameters . . . . . . . . . . . . . . .\n12\n10.4 InÔ¨Ånite-Width Neural Networks: From Theory to Practice and Back . . . . . . . .\n13\n11 Conclusion\n13\nA Parametrization Terminologies\n22\nB\nFurther Explanations of the ¬µP Tables\n22\nB.1\nWalkthrough of ¬µP Implementation in a Transformer . . . . . . . . . . . . . . . .\n24\nB.2\nOther Parameters . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n25\nB.3\nOptimizer Variants and Hyperparameters . . . . . . . . . . . . . . . . . . . . . . .\n25\nC Parametrization Matters: A Primer for Multiple Hyperparameters\n26\nD Practical Considerations\n26\nD.1\nVerifying ¬µP Implementation via Coordinate Checking . . . . . . . . . . . . . . .\n27\nD.2\nZero Initialization for Output Layers and Query Layers in Attention\n. . . . . . . .\n27\nD.3\nActivation Functions\n. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n27\n19\n\nD.4\nEnlarge dk . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n28\nD.5\nNon-Gaussian vs Gaussian Initialization . . . . . . . . . . . . . . . . . . . . . . .\n28\nD.6\nUsing a Larger Sequence Length . . . . . . . . . . . . . . . . . . . . . . . . . . .\n28\nD.7\nTuning Per-Layer Hyperparameters\n. . . . . . . . . . . . . . . . . . . . . . . . .\n28\nE\nWhich Hyperparameters Can Be Transferred? (Continued)\n29\nE.1\nFurther Discussions on Hyperparameter Categories . . . . . . . . . . . . . . . . .\n29\nE.2\nOn the DeÔ¨Ånitions of Width . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n29\nF\nExperimental Details\n31\nF.1\nIWSLT\n. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n31\nF.2\nWMT . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n31\nF.3\nBERT . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n31\nF.4\nGPT-3 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n32\nG Additional Experiments\n34\nG.1\nExperiments on ResNets\n. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n34\nG.1.1\nResNet on CIFAR-10 . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n34\nG.1.2\nWide ResNet on ImageNet . . . . . . . . . . . . . . . . . . . . . . . . . .\n37\nG.2\nExperiments on Transformers . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n37\nG.2.1\nVerifying Transfer across Batch Size, Sequence Length, and Training Time\non Wikitext-2 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n37\nG.2.2\nPost-Layernorm Transformers . . . . . . . . . . . . . . . . . . . . . . . .\n37\nG.2.3\nHyperparameter Instability of SP Transformers . . . . . . . . . . . . . . .\n38\nH Implementing ¬µTransfer in a Jiffy\n38\nI\nReverse-¬µTransfer for Diagnosing Training Instability in Large Models\n41\nJ\nAn Intuitive Introduction to the Theory of Maximal Update Parametrization\n42\nJ.1\nBehaviors of Gaussian Matrices vs Tensor Product Matrices\n. . . . . . . . . . . .\n43\nJ.1.1\nPreparation for the Derivations . . . . . . . . . . . . . . . . . . . . . . . .\n43\nJ.1.2\nLinear Tensor Product Matrix (e.g. SGD Updates)\n. . . . . . . . . . . . .\n44\nJ.1.3\nNonlinear Tensor Product Matrix (e.g. Adam Updates) . . . . . . . . . . .\n44\nJ.1.4\nVector Case (e.g. Readout Layer)\n. . . . . . . . . . . . . . . . . . . . . .\n45\nJ.1.5\nGaussian Matrix (e.g. Hidden Weights Initialization) . . . . . . . . . . . .\n45\nJ.2\nDeriving ¬µP for Any Architecture\n. . . . . . . . . . . . . . . . . . . . . . . . . .\n45\nJ.2.1\n¬µP Derivation From the Desiderata\n. . . . . . . . . . . . . . . . . . . . .\n46\nJ.3\nWhy Other Parametrizations Cannot Admit Hyperparameter Transfer\n. . . . . . .\n47\nList of Figures\n1\nTraining loss against learning rate on Transformers of varying dmodel trained with\nAdam . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n1\n20\n\n2\nIllustration of ¬µTransfer . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n2\n3\nSP vs ¬µP for MLPs on CIFAR10 . . . . . . . . . . . . . . . . . . . . . . . . . . .\n4\n4\nEmpirical validation of the stability of four representative hyperparameters on pre-LN\nTransformers in ¬µP . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n6\n5\nActivations blow up in SP but maintain a consistent scale in ¬µP . . . . . . . . . . .\n7\n6\nEfÔ¨Åciency-performance Pareto frontier of ¬µTransfer\n. . . . . . . . . . . . . . . .\n9\n7\nWider is always better in training loss under ¬µP, but not in SP, given the same HP .\n11\n8\nStress-testing ‚Äúwider-is-better‚Äù in ¬µP . . . . . . . . . . . . . . . . . . . . . . . . .\n11\n9\nSquashing activation functions reduce transfer quality.\n. . . . . . . . . . . . . . .\n27\n10\nEnlarging dk makes ¬µTransfer more precise in Transformers . . . . . . . . . . . .\n28\n11\nSchematics of each Transformer layer . . . . . . . . . . . . . . . . . . . . . . . .\n30\n12\nWidth ratio can be varied arbitrarily in ¬µTransfer\n. . . . . . . . . . . . . . . . . .\n30\n13\n¬µTransfer can handle increasing nhead while Ô¨Åxing dhead as well as increasing dhead\nwhile Ô¨Åxing nhead, or a mix of both\n. . . . . . . . . . . . . . . . . . . . . . . . .\n31\n14\nResults of the random search over reduced-width GPT-3 proxy models . . . . . . .\n33\n15\nThe training curves of the GPT-3 6.7B model with ¬µTransfer and a re-run with the\noriginal settings from [7] . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n34\n16\nVerifying ¬µP hyperparameter stability on ResNet\n. . . . . . . . . . . . . . . . . .\n36\n17\nVerifying hyperparameter stability under ¬µP for Post-LN Transformers . . . . . . .\n38\n18\n¬µTransfer vs naive transfer for post-layernorm Transformers on Wikitext-2 . . . . .\n39\n19\nEmpirical validation of ¬µTransfer across Batch Size, Sequence Length, and Training\nTime on pre-LN Transformers\n. . . . . . . . . . . . . . . . . . . . . . . . . . . .\n39\n20\nLearning rate landscape is highly unstable under standard parametrization in IWSLT\n40\n21\nReplicating training instability issue on a small Transformer by reverse-¬µtransferring\nhyperparameters . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n42\nList of Tables\n1\nHyperparameters That Can Be ¬µTransferred, Not ¬µTransferred, or ¬µTransferred Across\n2\n2\nExamples of ¬µTransferable Hyperparameters\n. . . . . . . . . . . . . . . . . . . .\n3\n3\n¬µP[57] and SP for General Neural Networks . . . . . . . . . . . . . . . . . . . . .\n5\n4\n¬µTransfer results for Transformer on IWSLT14 De-En\n. . . . . . . . . . . . . . .\n9\n5\n¬µTransfer results for Transformer on WMT14 En-De . . . . . . . . . . . . . . . .\n10\n6\n¬µTransfer results for BERT pretraining . . . . . . . . . . . . . . . . . . . . . . . .\n10\n7\n¬µTransfer results for GPT-3 pretraining\n. . . . . . . . . . . . . . . . . . . . . . .\n11\n8\nAlternative (Equivalent) ¬µP Formulation for Easier Implementation\n. . . . . . . .\n23\n9\n¬µP Formulation in the Style of [57] . . . . . . . . . . . . . . . . . . . . . . . . . .\n23\n10\nFull evaluation results of our GPT-3 6.7B models . . . . . . . . . . . . . . . . . .\n35\n11\nOur ¬µTransferred GPT-3 6.7B model performs comparably to the twice-as-large\nGPT-3 13B model from [7] . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n36\n12\n¬µTransfer results for ResNet on CIFAR10 . . . . . . . . . . . . . . . . . . . . . .\n37\n13\n¬µTransfer results for Wide ResNet on ImageNet . . . . . . . . . . . . . . . . . . .\n37\n14\nExpected output size of matrix multiplication between different types of random\nmatrices and a random vector, as preparation for deriving ¬µP . . . . . . . . . . . .\n43\n21\n\nA\nParametrization Terminologies\nThis section seeks to make formal and clarify some of the notions regarding parametrization discussed\ninformally in the main text.\nDeÔ¨Ånition A.1 (Multiplier and Parameter Multiplier). In a neural network, one may insert a ‚Äúmultiply\nby c‚Äù operation anywhere, where c is a non-learnable scalar hyperparameter. If c = 1, then this\noperation is a no-op. This c is called a multiplier.\nRelatedly, for any parameter tensor W in a neural network, we may replace W with cW for some\nnon-learnable scalar hyperparameter c. When c = 1, we recover the original formulation. This c is\nreferred to as a parameter multiplier.\nFor example, in the attention logit calculation ‚ü®k, q‚ü©/‚àödhead where q = Wx, the 1/‚àödhead factor is\na multiplier. It may also be thought of as the parameter multiplier of W if we rewrite the attention\nlogit as ‚ü®k, (W/‚àödhead)x‚ü©.\nNote parameter multipliers cannot be absorbed into the initialization in general, since they affect\nbackpropagation. Nevertheless, after training is done, parameter multipliers can always be absorbed\ninto the weight.\nDeÔ¨Ånition A.2 (Parametrization). In this work, a parametrization is a rule for how to change\nhyperparameters when the widths of a neural network change, but note that it does not necessarily\nprescribes how to set the hyperparameters for any speciÔ¨Åc width. In particular, for any neural network,\nan abc-parametrization is a rule for how to scale a) the parameter multiplier, b) the initialization, and\nc) the learning rate individually for each parameter tensor as the widths of the network change, as\nwell as any other multiplier in the network; all other hyperparameters are kept Ô¨Åxed with width.\nFor example, SP and ¬µP are both abc-parametrizations. Again, we note that, in this sense, a\nparametrization does not prescribe, for example, that the initialization variance be 1/fan_in, but\nrather that it be halved when fan_in doubles.\nDeÔ¨Ånition A.3 (Zero-Shot Hyperparameter Transfer). In this work, we say a parametrization admits\nzero-shot transfer of a set of hyperparameters H w.r.t. a metric L if the optimal combination of values\nof H w.r.t. L converges as width goes to inÔ¨Ånity, i.e. it stays approximately optimal w.r.t. L under\nthis parametrization as width increases.\nThroughout this paper, we take L to be the training loss, but because regularization is not the bottleneck\nin our experiments (especially large scale pretraining with BERT and GPT-3), we nevertheless see\nhigh quality test performance in all of our results. We also remark that empirically, using training loss\nas the metric can be more robust to random seed compared to validation loss and especially BLEU\nscore. See Table 1(left) for H. By our arguments in Appendix J.3 and our empirical results, ¬µP is the\nunique abc-parametrization admitting zero-shot transfer for such H and L in this sense.\nMore generally, one may deÔ¨Åne a K-shot transfer algorithm of a set of hyperparameters H w.r.t. a\nmetric L as one that 1) takes width values n and n‚Ä≤ and an approximately optimal combination of\nvalues of H w.r.t. L at a width n and 2) returns an approximately optimal combination of values of H\nw.r.t. L at width n‚Ä≤, given 3) a budget of K evaluations of candidate hyperparameter combinations on\nmodels of width n‚Ä≤. However, we will have no use for this deÔ¨Ånition in this paper.\nB\nFurther Explanations of the ¬µP Tables\nIn addition to Table 3, we provide Table 8 as an equivalent ¬µP formulation that is easier to implement,\nas well as Table 9 for those more familiar with the original ¬µP formulation in [57]. Below, we provide\nsome commentary on corner cases not well speciÔ¨Åed by the tables. Ultimately, by understanding\nAppendix J, one can derive ¬µP for any architecture, new or old.\nMatrix-Like, Vector-Like, Scalar-Like Parameters\nWe can classify any dimension in a neural\nnetwork as ‚ÄúinÔ¨Ånite‚Äù if it scales with width, or ‚ÄúÔ¨Ånite‚Äù otherwise. For example, in a Transformer,\ndmodel, dffn, dhead, nhead are all inÔ¨Ånite, but vocab size and context size are Ô¨Ånite. Then we can\ncategorize parameter tensors by how many inÔ¨Ånite dimensions they have. If there are two such\ndimensions, then we say the parameter is matrix-like; if there is only one, then we say it is vector-like;\nif there is none, we say it is scalar-like. Then in Tables 3, 8 and 9, ‚Äúinput weights & all biases‚Äù and\n‚Äúoutput weights‚Äù are all vector-like parameters, while hidden weights are matrix-like parameters. An\n22\n\nTable 8: Alternative (Equivalent) ¬µP Formulation for Easier Implementation. Same format as\nin Table 3. In contrast to the formulation in Table 3, here all ‚Äúvector-like‚Äù parameters (i.e. those\nthat have only one dimension tending to inÔ¨Ånity), including input and output weights and biases,\nhave the same width scaling for initialization variance and SGD/Adam LR (note the 1/fan_in for\ninput weight/bias init. var. is Œò(1) in width). This has two beneÔ¨Åts in practice: 1) implementation\nis uniÔ¨Åed and simpliÔ¨Åed for all ‚Äúvector-like‚Äù parameters; 2) input and output weights can now be\ntied, in contrast to Table 3, which is a common design feature of Transformer models. Note that in\nthis table, for biases, the fan_in is 1 (compare to PyTorch nn.Linear default initialization of biases,\nwhere fan_in refers to fan_in of the layer.) This table can be derived from Table 3 via Lemma J.1.\nSee Appendix B for further explanations.\nInput weights & all biases\nOutput weights\nHidden weights\nInit. Var.\n1/fan_in\n1\n(1/fan_in)\n1/fan_in\nMultiplier\n1\n1/fan_in\n(1)\n1\nSGD LR\nfan_out\n(1)\nfan_in\n(1)\n1\nAdam LR\n1\n1\n1/fan_in\n(1)\nTable 9: ¬µP Formulation in the Style of [57]. This table can be derived from Table 3 via Lemma J.1.\nInput weights & all biases\nOutput weights\nHidden weights\nInit. Var.\n1/fan_out\n(1/fan_in)\n1/fan_in\n1/fan_in\nMultiplier\n‚àöfan_out\n(1)\n1/‚àöfan_in\n(1)\n1\nSGD LR\n1\n1\n1\nAdam LR\n1/‚àöfan_out\n(1)\n1/‚àöfan_in\n(1)\n1/fan_in\n(1)\nadvantage of Table 8 is that it gives a uniform scaling rule of initialization and learning rate for all\nvector-like parameters. The multiplier rule in Table 8 can be more interpreted more generally as\nthe following: a multiplier of order 1/fan_in should accompany any weight that maps an inÔ¨Ånite\ndimension to a Ô¨Ånite one. This interpretation then nicely covers both the output logits and the attention\nlogits (i.e. 1/d attention).\nScalar-like parameters are not as common as matrix-like and vector-like ones, but we will mention a\nfew examples in Appendix B.2. The scaling rule for their initialization, learning rate (for both SGD\nand Adam), and multiplier is very simple: hold them constant with width.\nInitialization Mean\nWe did not specify the initialization mean in the tables, since most commonly\nthe mean is just set to 0, but it can be nonzero for vector-like parameters (e.g., layernorm weights)\nand scalar-like parameters but must be 0 for matrix-like parameters.\nZero Initialization Variance\nThe initialization scaling rules in our tables can all be trivially satis-\nÔ¨Åed if the initialization variance is set to 0. This can be useful in some settings (e.g., Appendix D.2)\nbut detrimental in other settings (e.g., hidden weights).\nWhat Are Considered Input Weights? Output Weights?\nHere, input weights very speciÔ¨Åcally\nrefer to weights that map from an inÔ¨Ånite dimension to a Ô¨Ånite dimension. As a counterexample, in\nsome architectures, the Ô¨Årst layer can actually map from a Ô¨Ånite dimension to another Ô¨Ånite dimension,\ne.g., a PCA layer. Then this is not an ‚Äúinput weight‚Äù; if the next layer maps into an inÔ¨Ånite dimension,\nthen that‚Äôs the input weight. A similar, symmetric discussion applies to output weights.\nWhat Counts As a ‚ÄúModel‚Äù? Does the MLP in a Transformer Count As a ‚ÄúModel‚Äù?\nFor our\ntables, a model is speciÔ¨Åcally a function that maps a Ô¨Ånite dimension to another Ô¨Ånite dimension,\nconsistent with the discussion above. For example, for an image model on CIFAR10, it maps from\n3√ó32√ó32 = 3072 dimensions to 10 dimensions, and these numbers are Ô¨Åxed regardless of the width\nof the model. Likewise, for an autoregressive Transformer model, the input and output dimension are\nboth the vocab size, which is independent of the width. In contrast, an MLP inside a Transformer is\nnot a ‚Äúmodel‚Äù in this sense because its input and output dimension are both equal to the width of the\nTransformer.\n23\n\nB.1\nWalkthrough of ¬µP Implementation in a Transformer\nTo ground the abstract description in Tables 3, 8 and 9, we walk through the parameters of a typical\nTransformer and discuss concretely how to parametrize each.\nWe assume that the user wants to replicate SP when the model widths are equal to some base widths,\nfor example, when dmodel = dmodel,0 = 128, dffn = dffn,0 = 512, etc, as in the MLP example in\nSection 4. For this purpose, it‚Äôs useful to deÔ¨Åne Àúdmodel = dmodel/dmodel,0, Àúdffn = dffn/dffn,0,\nand so on. One can always take dmodel,0 = dffn,0 = ¬∑ ¬∑ ¬∑ = 1 for a ‚Äúpure‚Äù ¬µP.\nBelow, we introduce hyperparameters œÉ‚Ä¢, Œ∑‚Ä¢ for each parameter tensor, as well as a few multipliers\nŒ±‚Ä¢. One may always tie œÉ‚Ä¢ (resp. Œ∑‚Ä¢) across all parameter tensors, but in our experiments, we found\nit beneÔ¨Åcial to at least distinguish the input and output layer initialization and learning rates.\nInput Word Embeddings\nThe input word embedding matrix W wordemb has size dmodel √ó\nvocabsize, where vocabsize is the fan-in and dmodel is the fan-out. Follow the ‚Äúinput weight\n& all biases‚Äù column in Tables 3, 8 and 9. For example, for Tables 3 and 8,\nW wordemb ‚àºN(0, œÉ2\nwordemb),\nwith Adam LR Œ∑wordemb\nNote here, because fan-in (vocabsize) here is independent of width (dmodel), the ‚Äú1/fan_in‚Äù for\nthe initialization variance in these tables is equivalent to ‚Äú1‚Äù, i.e. the initialization variance can be\nanything Ô¨Åxed with width. In this case of the word embedding, setting the variance to 1, for example,\nis more natural than setting the variance to 1/fan_in, because the embedding is one-hot (1/fan_in\nwould be more natural for image inputs).\nPositional Embeddings\nThe (absolute or relative) positional embedding matrix W posemb has size\ndmodel √ó contextsize, where contextsize is the fan-in and dmodel is the fan-out. With the same\ndiscussion as above for input word embeddings, follow the ‚Äúinput weight & all biases‚Äù column in\nTables 3, 8 and 9. For example, for Tables 3 and 8,\nW posemb ‚àºN(0, œÉ2\nposemb),\nwith Adam LR Œ∑posemb\nLayernorm Weights and Biases\nLayernorm weights wLN and biases bLN both have shape dmodel\nand can be thought of ‚Äúinput weights‚Äù to the scalar input of 1. Hence one should follow the ‚Äúinput\nweight & all biases‚Äù column in Tables 3, 8 and 9. In particular, the usual initialization of layernorm\nweights as all 1s and biases as all 0s sufÔ¨Åce (where the initialization variance is 0). For example, for\nTables 3 and 8,\nwLN ‚Üê1,\nwith Adam LR Œ∑LNw,\nand\nbLN ‚Üê0,\nwith Adam LR Œ∑LNb\nSelf-Attention\nThere are 4 matrices, W q, W k ‚ààR(dknhead)√ódmodel, W v ‚ààR(dvnhead)√ódmodel,\nand W o ‚ààRdmodel√ó(dvnhead) (where the shapes are Rfan_out√ófan_in). Since dmodel, (dknhead), and\n(dvnhead) all scale with width (where the latter two are commonly just set to dmodel), all 4 matrices\nshould be parametrized according to the ‚Äúhidden weights‚Äù column in Tables 3, 8 and 9. For example,\nfor Tables 3 and 8,\nW q ‚àºN(0, œÉ2\nq/dmodel),\nwith Adam LR Œ∑q/ Àúdmodel\nW k ‚àºN(0, œÉ2\nk/dmodel),\nwith Adam LR Œ∑k/ Àúdmodel\nW v ‚àºN(0, œÉ2\nv/dmodel),\nwith Adam LR Œ∑v/ Àúdmodel\nW o ‚àºN(0, œÉ2\no/(dvnhead)),\nwith Adam LR Œ∑o/( ÀúdvÀúnhead).\nAttention Logit Scaling\nWe use 1/d instead of 1/\n‚àö\nd attention. To be compatible with 1/\n‚àö\nd\nattention when at a particular base dhead = dhead,0, we set\nAttnLogit = Œ±attn\np\ndhead,0\ndhead\nq‚ä§k,\nwhere Œ±attn is a tunable multiplier.\n24\n\nMLP\nThere are 2 matrices, W 1 ‚ààRdffn√ódmodel, W 2 ‚ààRdmodel√ódffn (where the shapes are\nRfan_out√ófan_in), where dffn is commonly set to 4dmodel. Since both dmodel, dffn scale with width,\nboth matrices are considered ‚Äúhidden weights.‚Äù For example, for Tables 3 and 8,\nW 1 ‚àºN(0, œÉ2\nq/dmodel),\nwith Adam LR Œ∑q/ Àúdmodel\nW 2 ‚àºN(0, œÉ2\nk/dffn),\nwith Adam LR Œ∑k/ Àúdffn\nWord Unembeddings\nSymmetric to the discussion on input word embeddings, the output word\nunembeddings should be parametrized according to the ‚Äúoutput weights‚Äù column of Tables 3, 8 and 9.\nOften, the unembeddings are tied with the embeddings, and Tables 8 and 9 allow for this as their\ninitialization schemes are symmetric between input and output weights.\nFor example, for Table 3, we‚Äôd set\nW unemb ‚àºN(0, œÉ2\nunemb/(dmodel Àúdmodel)),\nwith Adam LR Œ∑unemb/ Àúdmodel.\nFor Table 8, we would instead have\nW unemb ‚àºN(0, œÉ2\nunemb/dmodel,0),\nwith Adam LR Œ∑unemb,\n(note dmodel,0 here is the base width and therefore is a constant) and the output is computed as\nlogits = Œ±output\nÀúdmodel\nW unembz\nwhere z is the Ô¨Ånal layer embedding of a token, and Œ±output is a tunable multiplier.\nB.2\nOther Parameters\nLearnable scalar multipliers\nFor learnable scalar multipliers (e.g., softmax inverse temperature),\none can initialize them to 1 and use a constant (in width) learning rate for both SGD and Adam. This\nis compatible with Tables 3, 8 and 9.\nPositional Bias\nSome Transformers use positional bias (of size contextsize√ócontextsize, which\nare added to the attention logits). They are considered ‚Äúscalar-like‚Äù in that it has no width dimension.\nOne can initialize them to 0 and use a constant (in width) learning rate for both SGD and Adam. This\nis compatible with Tables 3, 8 and 9.\nSpatial MLPs\nRecent works [12, 24, 31, 48, 49] on MLP-only architectures in NLP and CV replace\nthe self-attention layer in Transformers with MLPs across tokens or spatial locations. In our language\nhere, such MLPs have Ô¨Ånite input and output dimensions (the context size) and inÔ¨Ånite hidden\ndimensions, so their input, output, and hidden weights should be parametrized via the corresponding\ncolumns in Tables 3, 8 and 9.\nB.3\nOptimizer Variants and Hyperparameters\nAdamW\nExactly the same as Adam in all of our tables, with the added beneÔ¨Åt that weight decay is\nautomatically scaled correctly in AdamW (but is incompatible with ¬µP Adam). For this reason, we\nrecommend using AdamW when weight decay is desired (which is consistent with current standard\npractice).\nFrobenius Normalization\nLARS [63], Adafactor [42], Lamb [64], Layca [8], Fromage [6], Nero\n[28] all involve a normalization step in which the update g (which may be obtained from SGD,\nAdam, or other optimzers) is normalized to have Frobenius norm equal to that of the parameter\nw: g ‚Üê‚à•w‚à•F\n‚à•g‚à•F g. They can be made compatible with ¬µP in Table 8 by scaling their learning rate\nfor hidden weights like 1/‚àöfan_in (for Table 3, the output weight learning rate should be likewise\nscaled). The intuitive reasoning (which can be formalized straightforwardly using Tensor Programs)\nis as follows.\nThis normalization implicitly encodes a width scaling: If one initializes a weight matrix with variance\n1/fan_in, then an n√ón matrix (e.g., a hidden weight matrix) has Frobenius norm ‚àön at initialization.\nThus, in the Ô¨Årst step and, by induction, in any step t, the normalized update to this n √ó n weight also\n25\n\nhas Frobenius norm Œò(‚àön) (for any Ô¨Åxed t, as n ‚Üí‚àû). Heuristically, this means each entry of g is\napproximately of size Œò(1/‚àön). But, by the derivation of Appendix J, we want Œò(1/n) and this is\nŒò(‚àön) too large! Thus, in wide enough networks, one should see a network blowup after one update,\nlike demonstrated in Fig. 5.\nHowever, note that the Œò(1/‚àön) coordinate size induced by the normalization here is closer to the\nright size Œò(1/n) than Adam, whose update have coordinate size Œò(1). This may partially explain\nthe apparent beneÔ¨Åt of these optimizers. In particular, this may explain the observation that T5 [38],\nusing Adafactor, was able to train its entire range of models from 220 million to 11 billion parameters\nwith a Ô¨Åxed set of hyperparameters, while GPT-3 [7], using Adam, needed to decrease its learning\nrate with model size.\nRAdam\nRAdam [25] is a variant of Adam that uses SGD with momentum in an initial stage with\nlearning rate warmup, followed by a second stage of Adam with a particular setting of learning rate\nwith time. Thus, one can adapt RAdam to ¬µP by individually scaling the learning rates of the initial\nSGD stage and the Ô¨Ånal Adam stage according to Table 3, Table 8, or Table 9.\nAdagrad and RMSProp\nExactly the same as Adam in all of our tables.\nœµ in Adam and Its Variants\nAll of our derivations here assume œµ is negligible in Adam. If it is set\nto a non-negligible number, then it needs to be scaled, for all parameters, like 1/fan_in2 if it is added\nbefore the square root, or like 1/fan_in if it is added after the square root.\nGradient Clipping\nGradient (‚Ñì2-norm-wise) clipping is compatible with Table 3 (as well as\nTables 8 and 9), for either SGD or Adam, if the clip value is held constant with respect to width.\nWeight Decay\nWeight decay should be scaled independently of width in SGD and AdamW, for all\nof our tables. However, note it‚Äôs not compatible with ¬µP Adam.\nMomentum\nMomentum should be scaled independently of width for all of our tables.\nC\nParametrization Matters: A Primer for Multiple Hyperparameters\nHere we give more intuition why we need to reparametrize all hyperparameters. In practice, neural\nnetworks have multitudes of hyperparameters all interacting together. In our example of Section 2,\nhyperparameter optimization would be akin to minimizing the function19\nFn(c1, . . . , ck)\ndef\n=\nE\nx1,...,xn f((c1 + ¬∑ ¬∑ ¬∑ + ck)(x1 + ¬∑ ¬∑ ¬∑ + xn)).\nwhere x1, . . . , xn are as in Eq. (1) and c1, . . . , ck are analogous to k hyperparameters. For the same\nreasoning in Section 2, the correct parametrization is in (Œ±1, . . . , Œ±k) where Œ±i = ci‚àön.\nWhile this is straightforward, in practice, researchers often Ô¨Åx some hyperparameters (e.g., they tune\nonly learning rate but neglects to scale parameter multipliers or initialization correctly). For example,\nif we only partially reparametrize and optimize in Œ±1 while Ô¨Åxing c2, . . . , ck, then the optimal Œ±1 is\n(Œ±1)‚àó= Œ±‚àó‚àí(c1 + . . . + ck)‚àön where Œ±‚àóis the optimal Œ± for Eq. (1). Thus, as n ‚Üí‚àû, (Œ±1)‚àóstill\nblows up even though we parametrized Œ±1 correctly. More generally, the incorrect parametrization\nof some hyperparameters forces other hyperparameters to increasingly compensate for it as width\ngrows, distorting their optima, even if the latter are correctly parametrized.\nD\nPractical Considerations\nIn this section, we outline several useful tips and tricks that can improve the quality of hyperparameter\ntransfer in practice.\n19Here, for simplicity of the example, we model the interaction between ‚Äúhyperparameters‚Äù c1, . . . , ck as\nadditive, but in real neural networks such interactions are usually much more complicated.\n26\n\nD.1\nVerifying ¬µP Implementation via Coordinate Checking\nEven though ¬µP is neatly encapsulated by Table 3, implementing it correctly can in practice be\nerror-prone, just like how implementing autograd by hand can be error-prone even though the math\nbehind is just chain-rule. In the case of autograd, gradient checking is a simple way of verifying\nimplementation correctness; similarly, we propose coordinate checking to verify the correctness\nof ¬µP implementation: ExempliÔ¨Åed by Fig. 5, one calculates the average coordinate size of every\n(pre)activation vector in the network over a few steps of training, as width is varied over a large\nrange. An incorrect implementation will see some activation vector blow up or shrink to zero with\nwidth (like in the top row of Fig. 5). In the mup package we release with this paper, we include an\neasy-to-use method for coordinate checking.\nD.2\nZero Initialization for Output Layers and Query Layers in Attention\nWe Ô¨Ånd that the optimal hyperparameters of small and large width models match more closely when\nwe initialize output layers at 0 (i.e. with variance œÉ2/fan_in where œÉ = 0 instead of positive œÉ).\nThis is because the neural network in ¬µP is approximately a Gaussian process (GP) at initialization\nwith variance on the order Œò(œÉ2/width) (contrast this with SP networks, which approximates a\nGP with Œò(œÉ2) variance) [21, 29, 53, 57]. Of course, when width is large, this variance vanishes,\nbut this can be far from so in the small proxy model. This discrepancy in the initial GP can cause\nthe training trajectory of the proxy model to be very different from the trajectory of the large target\nmodel, causing a mismatch in the optimal hyperparameters. By initializing the output layer at 0, we\nremove this mismatch in the initial GP. Empirically we do not Ô¨Ånd this modiÔ¨Åcation to be detrimental\nto performance.\nA similar consideration applies to the query layer in self-attention: At initialization, the attention logit\nq‚ä§k/dhead looks like a Gaussian with variance Œò(1/dhead) because q and k are almost independent\nand zero-mean. In the limit dhead ‚Üí‚àû, the logit is exactly 0, which can be a large discrepancy\ncompared to when dhead is small in the small proxy model we want to tune. By initializing the\nquery projection matrix W q to 0, q will also be 0, and hence the attention logit is always 0 at\ninitialization regardless of width (but will generally become nonzero after a gradient step), resolving\nthis discrepancy.\nMore generally, any layer or computation that goes from an ‚ÄúinÔ¨Ånite‚Äù dimension (i.e. width) to a\n‚ÄúÔ¨Ånite‚Äù dimension (e.g. output dimension or sequence length) can exhibit this kind of discrepancy due\nto the initial GP. When dhead ‚Üí‚àûand nhead is Ô¨Åxed, attention logit calculation can be viewed in\nthe same vein as a function Rseqlen√ódmodel ‚ÜíRnhead√óseqlen√óseqlen, which ‚Äúreduces to‚Äù R‚àû‚ÜíR1.\nD.3\nActivation Functions\n14\n12\n10\n8\n6\nlog2LearningRate\n0.50\n0.75\n1.00\n1.25\n1.50\n1.75\nTraining Loss\nSP / tanh / xent\n256\n512\n1024\n2048\n4096\n8192\n14\n12\n10\n8\n6\nlog2LearningRate\n0.50\n0.75\n1.00\n1.25\n1.50\n1.75\nP / tanh / xent\n15\n10\n5\n0\nlog2LearningRate\n0.02\n0.04\n0.06\n0.08\n0.10\nSP / tanh / mse\n15\n10\n5\n0\nlog2LearningRate\n0.02\n0.04\n0.06\n0.08\n0.10\nP / tanh / mse\nFigure 9: Squashing activation functions reduce transfer quality. MLP of different hidden sizes\nwith tanh activation trained for 20 epoch on CIFAR-10 using SGD. Left uses cross-entropy as loss\nfunction; right uses mean squared error; columns alternate between standard parametrization (SP)\nand maximal update parametrization (¬µP). Compared to ReLU, tanh exhibits slower convergence for\n¬µP, yet it still outperforms SP when width is increased\nWhen the network is narrow, its approximation to the inÔ¨Ånite-width behavior becomes crude, which\nis manifested as large Ô¨Çuctuations in preactivation coordinates. When using a squashing activation\nfunctions like softmax or tanh, this causes narrower networks to saturate the activation more than\nwider ones, which results in a systematic bias toward small gradients and therefore distorting the\nhyperparameter landscape. This can be seen in Fig. 9, where we use tanh as the network activation\nfunction.\n27\n\n1\n2\n3\n4\nlog2(cattn)\n0.02\n0.00\n0.02\n0.04\n0.06\n0.08\n0.10\n0.12\n Valid. Loss from Best\ndk not enlarged\nWidth Mult.\n0.0625\n1.0\n1\n2\n3\n4\nlog2(cattn)\ndk enlarged\nWidth Mult.\n0.0625\n1.0\nFigure 10: Enlarging dk makes ¬µTransfer more precise. Here we plot all curves after subtracting\ntheir minima for easier visual comparison. Transformer on IWSLT 14 similar to the setup in Ap-\npendix F.1 where the dmodel = 512 for a width multiplier of 1, nhead = 4, and dq = dk. (Left) We\nleave dq = dk = dmodel/nhead, so dk = 8 for width-multiplier 0.0625. The optimum for the attention\nlogit multiplier cattn is noisy and does not accurately transfer across width. (Right) We enlarge\ndq = dk to a minimum of 128. The HP landscape is much smoother than in (Left), and the optima\nalign between narrow and wide models.\nTherefore, we recommend replacing non-essential squashing activation functions with ReLU, whose\nderivative depends only on the sign of the pre-activation. A similar reasoning can be applied to\nsuperlinear activation functions, where the distribution of activation values can have heavy tails,\nleading to slow convergence to the inÔ¨Ånite-width limit. However, such activations are rarely used in\npractice.\nD.4\nEnlarge dk\nWe Ô¨Ånd that small dhead = dk can lead to a highly noisy HP landscape, as shown in Fig. 10. This\ncan signiÔ¨Åciantly decrease the quality of random HP search on the small proxy model. To solve this,\nwe Ô¨Ånd it useful to decouple dk from dmodel (so that dmodel Ã∏= dk ¬∑ nhead) and maintain a relatively\nlarge dk even as dmodel is shrunk in the proxy model. For example, pegging dk = 32 is generally\neffective. Training or inference speed are not usually affected much by the larger dk because of\nCUDA optimizations. By Appendix E.2, this decoupling of dk from dmodel is theoretically justiÔ¨Åed,\nand as shown in Fig. 10, it signiÔ¨Åcantly denoises the HP landscape.\nD.5\nNon-Gaussian vs Gaussian Initialization\nWe Ô¨Ånd non-Gaussian (e.g. uniform) initialization can sometimes cause wider models to perform\nworse than narrower models, whereas we do not Ô¨Ånd this behavior for Gaussian initialization. This is\nconsistent with theory, since in the large width limit, one should expect non-Gaussian initialization\nto behave like Gaussian initializations anyway (essentially due to Central Limit Theorem, or more\nprecisely, universality), but the non-Gaussianity slows down the convergence to this limit.\nD.6\nUsing a Larger Sequence Length\nFor Transformers, we empirically Ô¨Ånd that we can better transfer initialization standard deviation\nfrom a narrower model (to a wide model) if we use a larger sequence length. It is not clear why this\nis the case. We leave an explanation to future work.\nD.7\nTuning Per-Layer Hyperparameters\nThe techniques in this paper allow the transfer across width of (learning rate, initialization, multipliers)\nsimultaneously for all parameter tensors. Thus, to get the best results, one should ideally tune all\nsuch hyperparameters. In practice, we Ô¨Ånd that just tuning the global learning rate and initialization,\nalong with input, output, and attention multipliers, yield good results.\n28\n\nE\nWhich Hyperparameters Can Be Transferred? (Continued)\nE.1\nFurther Discussions on Hyperparameter Categories\nBelow, we discuss the reasoning behind each kind, which are supported by our empirical evidence\ncollected in Fig. 4 on Transformers as well as those in Appendix G.1 on ResNet.\nTransferable Hyperparameters\nIn Table 2, we summarize which HPs can be transferred across\ntraining scale. The transfer across width, as explained in Section 2, is theoretically justiÔ¨Åed, while we\npresent the transfer across the other dimensions as empirical results.\nThese cover most of the well-known and important HPs when the need for regularization is not\nparamount, e.g., during large scale language model pretraining. Parameter Multipliers are not well-\nknown HPs, yet we include them here as they serve a bridge between SP and ¬µP and can impact\nmodel performance in practice. Concretely, any SP and ¬µP neural networks of the same width can\nhave their Parameter Multipliers tuned so that their training dynamics become identical.\nHyperparameters That Don‚Äôt Transfer Well\nNot all HPs transfer well even if we use ¬µP. In\nparticular, those whose primary function is to regularize training to mitigate ‚ÄúoverÔ¨Åtting\" tend not to\ntransfer well. Intuitively, regularization needs to be applied more heavily in larger models and when\ndata is scarce, but ¬µP does not know the data size so cannot adjust the regularization accordingly.\nTo the best of our knowledge, there is no strict separation between HPs that regularize and those that\ndon‚Äôt. However, conventional wisdom tells us that there exists a spectrum of how much regularizing\neffect a HP has. For example, dropout probability and weight decay are among those whose primary\nfunction is to regularize, whereas batch size and learning rate might regularize training in some cases\nbut affect the dynamics more so in other ways. Our empirical exploration tells us that the former do\nnot transfer well, while the latter do. Our subsequent discussion will focus on the latter; we leave to\nfuture works the expansion to the former.\nHyperparameters Transfered Across\nWe have left out a category of HPs that deÔ¨Ånes the training\nscale, or in practical terms, training cost. This includes 1) those that deÔ¨Åne how many operations a\nmodel‚Äôs forward/backward pass takes, such as the model‚Äôs width, depth, and in the case of language\nmodeling, sequence length; and 2) those that deÔ¨Åne how many such passes are performed, such as\nbatch size and number of training steps.\nAs recent works have shown [7, 19, 39], improvements along any of these scale dimensions lead to\napparently sustainable gain in performance; as a result, we are primarily interested in transferring\nother HPs across these dimensions that deÔ¨Åne scale, rather than Ô¨Ånding the optimal scale.20 This\ncategory of HPs is particularly crucial as one can speedup training by downsizing in one or multiple\nsuch dimensions. Indeed, it‚Äôs very common for practitioners to implicitly transfer HPs across the\nnumber of training samples by tuning on only a subset of the full training data.\nOur insights from the inÔ¨Ånite-width limit inspired us to explore HP tranfer across width, which\ndoes not work under SP as we have shown earlier. Building upon our success with width, which\nis well explained theoretically, we hope to push the limit of compute-saving by investigating the\nother dimensions empirically. To the best of our knowledge, the transferability of optimal HPs across\ndepth, batch size, sequence length, and training time has not been rigorously investigated previously,\nwith the main exception of the literature on (learning rate, batch size) scaling [41, 44] where our\ntransferability result of learning rate across batch size recapitulates [30].21 See Section 10.3 on how\nour results relate to prior works. We will primarily focus on the Transformer architecture in the main\ntext with evidence for ResNet in Appendix G.1.\nE.2\nOn the DeÔ¨Ånitions of Width\nOur theory allows more general notions of width. This is especially relevant in Transformers,\nwhere dmodel, dhead = dk, dv, nhead, dffn (see Fig. 11) can all be construed as measures of width.\n20In particular, we are not Ô¨Åxing the total training FLOPs when we scale, which requires understanding the\ntradeoff of different scale HPs. For example, when we transfer across batch size, we Ô¨Åx the number of steps of\ntraining (not the number of epochs), so that the total FLOPs scales linearly.\n21There‚Äôs also a literature on the proper initialization for training deep networks effectively (e.g. [5, 16, 26,\n40, 59, 60, 66]), but they do not study the transferability per se. See Section 10.3\n29\n\nùëäùêæ\nùëäùëÑ\nùëäùëâ\nùëëùëò\nùëëùëò\nùëëùë£\nSelf-attn (paramless)\nùëäùëÇ\nùëëùëöùëúùëëùëíùëô\nùëëùëìùëìùëõ\nùëëùëöùëúùëëùëíùëô\nùëëùëöùëúùëëùëíùëô\nSkip connection\nSkip connection\nùëëùëöùëúùëëùëíùëô\nùëëùëìùëìùëõ\nùëëùëöùëúùëëùëíùëô\nùëëùëöùëúùëëùëíùëô\nùëëùë£‚ãÖùëõ‚Ñéùëíùëéùëë\nùëõùëú. ‚Ñéùëíùëéùëëùë†= ùëõ‚Ñéùëíùëéùëë\nSkip connection\nSkip connection\n(a) Single-head attention\n(b) Multi-head attention\nFigure 11: Schematics of each Transformer layer. Commonly, the key and value dimensions dk\nand dv are both set to dmodel/nhead, and this is referred to as dhead.\n4\n3\n2\n1\n0\n1\nlog2(LearningRate)\n4.5\n5.0\n5.5\n6.0\nValidation Loss\nTransformer on IWSLT14 De-En\n(Varying dffn)\ndffn/dmodel\n0.5\n1.0\n2.0\n4.0\n8.0\n16.0\nFigure 12: Learning rate landscape in ¬µP is stable even if we vary dffn by a factor of 32, Ô¨Åxing\ndmodel.\nWe brieÔ¨Çy discuss these here, with more theoretical justiÔ¨Åcation in Appendix J.2.1 and empirical\nvalidation below.\nVarying Width Ratio\nSo far we have assumed that every hidden layer is widened by the same\nfactor. But in fact we can widen different hidden layers differently. This is useful, for example, in a\nTransformer where we may want to use a smaller dffn during tuning. If we are using Adam, as long\nas the width of every layer still tends to inÔ¨Ånity, we still obtain approximately the same limit22, so the\n¬µTransfer remains theoretically justiÔ¨Åed.\nSee Fig. 12 for an empirical validation on IWSLT-14 using a Transformer.\nNumber of Attention Heads\nIn attention-based models, one typically splits hidden size into\nmultiple attention heads following dmodel = dhead √ó nhead. So far we have assumed dhead and\ndmodel to be width, but it‚Äôs possible and potentially advantageous to Ô¨Åx dhead and treat nhead as\nthe width, or increasing both simultaneously. This allows our technique to handle many popular\nmodels, including GPT-3 [7], which scale up by Ô¨Åxing dhead and increasing nhead. See Fig. 13 for\nan empirical validation on Wikitext-2.\nVarying Just the Width of Attention Heads\nA speciÔ¨Åc useful instance of varying width ratio is\ndecoupling the key and value dimensions dk and dv and scaling dk differently from (typically larger\n22This also applies for SGD, but we need more involved scaling to keep the limit approximately the same.\n30\n\n14\n12\n10\nlog2\n3.5\n4.0\n4.5\n5.0\n5.5\n6.0\nTraining Loss\nWidth\n256\n512\n1024\n2048\n4096\n8192\n5\n0\n5\n10\n15\nlog2\noutput\n3.0\n3.5\n4.0\n4.5\n5.0\n10\n0\n10\nlog2\nattn\n3.5\n4.0\n4.5\n5.0\n5.0\n2.5\n0.0\n2.5\nlog2\n4\n5\n6\nP - Fixing dhead while varying dmodel and nhead\nFigure 13: ¬µTransfer across width when we Ô¨Åx dhead and vary dmodel and nhead. Œ±output, Œ±attn are\nmultipliers for output and key weights, and œÉ is initialization standard deviation.\nthan) dmodel/nhead. This works as long as we use 1/d scaled-attention as in DeÔ¨Ånition 4.1 (instead\nof 1/\n‚àö\nd as is done commonly). When tuning on the small proxy model, if dk is too small, the HP\nlandscape can be quite noisy. Keeping dk relatively large while shrinking all other dimensions solves\nthis problem, while still obtaining signiÔ¨Åcant speedup.\nF\nExperimental Details\nF.1\nIWSLT\nIWSLT14 De-En is a well-known machine translation benchmark. We use a Transformer implemented\nin fairseq [33] with a default dmodel = 1/4dffn = 512 and dk = dq = dv = dmodel/nhead = 128\n(amounting to 40M parameters), which we denote as the 1x model. For transfer, we tune on a proxy\nmodel with the same nhead but with dmodel and other dimensions 4 times smaller; we will call this\nthe 0.25x model (but it has 4M parameters). All models are trained with Adam for 100 epochs and\nvalidated at the end of every epoch. We tune via random search the learning rate Œ∑, the output layer\nparameter multiplier Œ±output, and the attention key-projection weight multiplier Œ±attn following the\ngrid\n‚Ä¢ Œ∑: 5 √ó 10‚àí4 √ó 2z, where z ‚àà{‚àí1.5, ‚àí1.25, ‚àí1, ..., 1.25}\n‚Ä¢ Œ±output: 2z, where z ‚àà{‚àí8, ‚àí7, ‚àí6, ..., 7}\n‚Ä¢ Œ±attn: 2z, where z ‚àà{‚àí3, ‚àí2, ‚àí1, ..., 8}\nF.2\nWMT\nWe scale up to WMT14 En-De using the large Transformer from [50], with a dmodel = 1/4dffn =\n1024 and dq = dk = dv = dmodel/nhead = 64. We use the exact same setup and reproduce their\nresult as our baseline. Then, we build the proxy model by shrinking the target model‚Äôs dmodel from\nthe original 1024 to 256, dffn from 4096 to 256 and nhead from 16 to 4. This reduces the total\nparameter count from 211M to 15M. We then perform the HP search on the proxy model and take the\nbest according to validation loss, before testing on the target model. We tune via random search the\nlearning rate Œ∑, the output layer parameter multiplier Œ±output, and the attention key-projection weight\nmultiplier Œ±attn following the grid\n‚Ä¢ Œ∑: 6 √ó 10‚àí4 √ó 2z, where z ‚àà{‚àí1.5, ‚àí1.25, ‚àí1, ..., 1.25}\n‚Ä¢ Œ±output: 2z, where z ‚àà{‚àí8, ‚àí7, ‚àí6, ..., 7}\n‚Ä¢ Œ±attn: 2z, where z ‚àà{‚àí3, ‚àí2, ‚àí1, ..., 8}\nF.3\nBERT\nDetails of BERT Prototype\nOur proxy model has 10 Transformer layers with dmodel = dffn =\n256. We also reduce the number of attention heads to 8 with a dhead of 32. We call it BERT Prototype\nsince we can increase its width and depth according to our deÔ¨Ånitions to recover both BERT Base and\nBERT Large, which enables us to sweep HPs once and use for both models. Overall, BERT Prototype\nhas 13M trainable parameters, a fraction of the 110M in BERT Base and the 350M in BERT Large.\n31\n\nHyperparameters Tuned for Pretraining\nWe tune the following HPs for pretraining: Adam\nlearning rate Œ∑, embedding learning rate Œ∑emb, output weight multiplier Œ±output, attention logits\nmultiplier Œ±attn, layernorm gain multiplier Œ±LNgain, and bias multiplier Œ±bias.\nWe sample 256 combinations from the follow grid:\n‚Ä¢ Œ∑: 1 √ó 10‚àí4 √ó 2z, where z ‚àà{1.5, 2, 2.5, 3, 3.5}\n‚Ä¢ Œ∑emb: 1 √ó 10‚àí4 √ó 2z, where z ‚àà{‚àí1, ‚àí0.5, 0, 0.5, 1}\n‚Ä¢ Œ±output: 2z, where z ‚àà{2, 4, 6}\n‚Ä¢ Œ±attn: 2z, where z ‚àà{3, 3.5, 4, ..., 7}\n‚Ä¢ Œ±LNgain: 2z, where z ‚àà{8.5, 9, 9.5, 10, 10.5}\n‚Ä¢ Œ±bias: 2z, where z ‚àà{8.5, 9, 9.5, 10, 10.5}\nThe ranges are chosen to include the implicit choices of these HPs in SP BERT Large.\nFinetuning Procedure and Hyperparameters\nWe hand-pick the Ô¨Ånetuning HPs after training the\nfull-sized model. As regularization is an essential ingredient in successful Ô¨Ånetuning, we do not\ntransfer such HPs (at least via the suite of techniques presented in this work) (see Table 1). We focus\non MNLI [52] and QQP, which are two representative tasks from GLUE [51]. Following [27], we\nused Adam [20] with a learning rate of 5 √ó 10‚àí5 and a batch size of 64. The maximum number of\nepochs was set to 5. A linear learning rate decay schedule with warm-up of 0.1 was used. All the\ntexts were tokenized using wordpieces and were chopped to spans no longer than 128 tokens.\nF.4\nGPT-3\nBaseline 6.7B GPT-3 Transformer\nAs the GPT-3 codebase has evolved since the publication of\n[7], we re-trained the 6.7B model from scratch to remove changes in our codebase as a possible\nconfounder. The main differences to [7] are 1) a modiÔ¨Åed learning rate decay schedule, where\nthe learning rate is decayed to zero at the end of training rather than being decayed to 0.1 of the\ninitial value, and 2) use of relative attention in place of absolute attention. Unfortunately, after all\nexperiments were Ô¨Ånished, we found this re-run baseline used absolute attention instead of relative\nattention, while the ¬µTransfer model still used relative attention.\nRandom Search using Reduced-Width Proxy Model\nIn order to Ô¨Ånd a good set of hyperparam-\neters for the ¬µTransfer version of the 6.7B model, we performed a hyperparameter search over a\nreduced version of the model (i.e., the proxy model), where the width is set to 256 hidden units. This\nproxy model inherits changes from the evolved GPT-3 codebase: it uses relative [10] (instead of\nabsolute) position encoding. Early on, we noted that on the proxy model, linear learning rate decay\noutperformed the default cosine schedule, so all subsequent experiments for the proxy models use a\nlinear decay schedule. By Fig. 4, ¬µTransferring this linear decay schedule to the full model should\nmaintain such a performance advantage over the cosine schedule.\nThe hyperparameter search space consists of the following hyperparameters:\n‚Ä¢ learning rate: Sampled from 10Uniform(‚àí4,‚àí1)\n‚Ä¢ initialization scale: All the parameters are multiplied - sampled from 10Uniform(‚àí1,1)\n‚Ä¢ attention temperature: Reciprocal of the multiplier applied to the input to attention soft-\nmax. Sampled from 4Uniform(‚àí1,1).\n‚Ä¢ output temperature: Reciprocal of the multiplier applied to the input to softmax that\nproduces the distribution over output tokens. Sampled from 4Uniform(‚àí1,1).\n‚Ä¢ embedding multiplier: Scalar by which we multiply the output of the embedding layer.\nSampled from 10Uniform(‚àí1,1).\n‚Ä¢ relative position embedding multiplier: Scalar by which we multiply vectors representing\nrelative position. Sampled from 10Uniform(‚àí1,1).\nIn order to make the search more efÔ¨Åcient we reduced the total number of training tokens. We\nhypothesized that tuning hyperparameters on a reduced total number of tokens does not signiÔ¨Åcantly\naffect optimal hyperparameters. To verify, we trained two different horizons and compared the results.\n32\n\n3.2\n3.22\n3.24\n3.26\n3.28\n3.3\nloss\n3.3\n3.19674\n0.0001\n0.000562\n0.00316\n0.0178\n0.1\nlearning rate\n0.1\n0.316\n1\n3.16\n10\ninitialization\n0.1\n0.316\n1\n3.16\n10\nembedding\nmultiplier\n0.25\n0.5\n1\n2\n4\nattention\ntemperature\n0.25\n0.5\n1\n2\n4\noutput\ntemperature\n0.1\n0.316\n1\n3.16\n10\nrelative\nmultiplier\n3.1\n3.15\n3.2\n3.25\nloss\n3.3\n3.09418\n0.0001\n0.000562\n0.00316\n0.0178\n0.1\nlearning rate\n0.1\n0.316\n1\n3.16\n10\ninitialization\n0.1\n0.316\n1\n3.16\n10\nembedding\nmultiplier\n0.25\n0.5\n1\n2\n4\nattention\ntemperature\n0.25\n0.5\n1\n2\n4\noutput\ntemperature\n0.1\n0.316\n1\n3.16\n10\nrelative\nmultiplier\nFigure 14: Results of the random search over reduced-width GPT-3 proxy models trained on 4\n(left) and 16 (right) billion tokens. Only the best performing runs are highlighted.\nWhile the target model was to be trained on 300 billion tokens, we tuned the proxy model on only\nsubsets consisting of 4 billion and 16 billion tokens. This impacts both the total training time and the\nlength of the linear learning rate decay schedule. Other than hyperparameters explicitly listed above\nand the training horizon, the rest was the same as what we intended to use for the full width 6.7B\ntraining run.\nAnalyzing the Results of the Random Search\nWe performed 467 training runs of the proxy\nmodel, out of which 350 were for 4 billion tokens (286 completed without diverging) and 117 for\n16b tokens (80 completed without diverging). See Fig. 14 for summary of the results.\nAs suspected, we observed that the results are well-aligned for both 4 and 16 billion tokens versions.\nWe observe learning rate and initialization scale impact the results the most. Based on the results we\nchose 0.006 for the former and 2.5 for the latter. Since most other hyperparameters appear to have\nnegligible effect on performance, they were kept at their default values of 1, the only exception being\nthe embedding scale, where higher values seem to perform better and it was therefore set to 10.\nTraining the ¬µTransfer Model\nWe encountered frequent divergences in our initial attempt to train\nthe ¬µTransfer model. We traced the issue back to underÔ¨Çow of FP16 tensors in the backwards pass\nand therefore switched to training the model in FP32. This allowed us to Ô¨Ånish the training run\nwithout divergences. We hypothesize that the divergence issue is related to ¬µTransfer picking more\naggressive hyperparameters, for example a higher learning rate on linear weight tensors compared\nto the original model. In order to exclude code differences as a possible confounder, we re-trained\nGPT-3 6.7B from scratch using the original hyperparameters. The only difference compared to the\nversion published in [7] is that the learning rate was decayed fully, whereas the learning rate of the\nmodel from [7] was only decayed to 10% of its starting value. The retrained model performs slightly\nworse than the original published in [7]. We suspect that this is because it made less progress during\nthe last phase of training where the learning rate is close to zero. The training curves of the ¬µTransfer\nmodel and the re-run of the original 6.7B can be seen in Fig. 15. Detailed evaluation results can be\nfound in Table 10 and Table 11.\nRatio of Tuning Cost to Pretraining Cost\nin FLOPs can be approximated as\ns(t1N1 + t2N2)\nST\n‚âà0.07\nwhere\n‚Ä¢ s = 40 Million is number of parameters of the proxy model\n‚Ä¢ S = 6.7 Billion is number of parameters of the target model\n‚Ä¢ t1 = 4 Billion is the number of training tokens for the short horizon HP search, and\nN1 = 350 is the corresponding number of random HP search trials.\n‚Ä¢ t2 = 16 Billion is the number of training tokens for the longer horizon HP search, and\nN1 = 117 is the corresponding number of random HP search trials.\n33\n\n0.5\n1.0\n1.5\n2.0\n2.5\n3.0\nTraining tokens\n1e11\n2.0\n2.1\n2.2\n2.3\n2.4\n2.5\nValidation loss\nGPT-3 6.7B\nGPT-3 6.7B + Transfer\nFigure 15: The training curves of the GPT-3 6.7B model with ¬µTransfer (orange) and a re-run\nwith the original settings from [7] (blue). The ¬µTransfer model uses relative attention while the\nre-run uses absolute attention. In addition, the former was trained using FP32 activations and weights\nafter initially encountering stability issues with the hyperparameters computed using ¬µP, while the\nre-run used the original FP16 training. The ¬µTransfer model seems to underperform in the middle\nof training, but achieves a much better Ô¨Ånal validation loss once the learning rate is fully decayed.\nWhile the original model uses a cosine schedule, the ¬µTransfer model uses a linear learning rate\ndecay schedule transferred from the proxy model.\n‚Ä¢ T = 300 Billion is the number of training tokens for the 6.7B target model.\nHere we are using the fact that the training FLOPs of a Transformer per token is roughly proportional\nto its number of parameters.\nG\nAdditional Experiments\nG.1\nExperiments on ResNets\nG.1.1\nResNet on CIFAR-10\nSetup\nFor this case we use Davidnet [2], a ResNet variant that trains quickly on CIFAR-10, so as\nto efÔ¨Åciently investigate its HP landscape. We train with SGD on CIFAR-10 for 10 epochs; all results\nare averaged over 15 random seeds. We use a width multiplier to identify models of different width,\nand a multiplier of 1 corresponds to the original model in [2]. We look at validation accuracy here as\nthe model barely overÔ¨Åts, and our observations will hold for the training accuracy as well. We Ô¨Årst\nconduct a learning rate sweep for models of different widths using SP; the result is shown in Fig. 16,\non the left.\nHyperparameter Stability\nNote that the best model with a width multiplier of 8 under-performs\nthat with a multiplier of 4. We run the same sweep with ¬µP, along with a sweep of the output\nmultiplier (Œ±output); the result is shown in Fig. 16, on the right. We notice that wider models always\nperform better under ¬µP and that the optimal learning rate Œ∑ and Œ±output are stable across width.\n34\n\nTable 10: Full evaluation results of our GPT-3 6.7B models: The new model tuned with ¬µTransfer\n(marked ¬µP), the original model from [7], and a re-training of this model from scratch with the\noriginal hyperparameter settings (marked re-run). The sampling-based evaluations shown here are\na subset of the ones from [7]. Since the sampling-based evaluations are subject to high variance,\nWikitext 103 and the LM1B benchmark have been added to help distinguish the relative performance\nof the ¬µP and non-¬µP model. Note that Wikitext-103 [32] and the LM1B [9] benchmarks overlap\nwith the training dataset. Accuracies and F1 scores have been multiplied by 100. The perplexities\nreported in this table are based on a custom BPE encoding and are not comparable to other results in\nthe literature. The number k of examples in the context for each task is identical to [7].\nNote: Zero-shot, One-Shot and Few-Shot refer to the number of additional query and answer pairs\npassed in the context when performing the sampling-based evaluations, not the ‚Äùshots‚Äù involved in\nhyperparameter transfer.\nZero-shot\nOne-shot\nFew-shot\nTask\nSplit\nMetric\n¬µP\n[7]\nre-run\n¬µP\n[7]\nre-run\n¬µP\n[7]\nre-run\nValidation dataset\nvalid\nce\n1.98\n2.03\nPTB\ntest\nppl\n11.4\n13.0\nWikitext 103\ntest\nppl\n8.56\n9.13\nLM1B\ntest\nppl\n20.5\n21.7\nHellaSwag\ndev\nacc\n72.0\n67.4\n66.7\n71.1\n66.5\n65.9\n72.4\n67.3\n66.4\nLAMBADA\ntest\nacc\n73.5\n70.3\n70.8\n69.9\n65.4\n64.8\n74.7\n79.1\n77.1\nStoryCloze\ntest\nacc\n79.4\n77.7\n77.3\n80.6\n78.7\n78.3\n84.2\n81.2\n81.1\nNaturalQS\ntest\nacc\n9.86\n5.79\n7.20\n14.7\n9.78\n10.6\n20.2\n17.0\n15.7\nTriviaQA\ndev\nacc\n47.0\n38.7\n37.5\n50.4\n44.4\n42.5\n55.5\n51.6\n49.9\nWebQS\ntest\nacc\n11.3\n7.73\n9.79\n20.2\n15.1\n16.2\n33.0\n27.7\n28.2\nRo‚ÜíEn 16\ntest\nBLEU-sb\n26.9\n8.75\n13.7\n36.5\n34.2\n33.5\n38.2\n36.2\n35.6\nEn‚ÜíRo 16\ntest\nBLEU-sb\n18.1\n5.31\n4.40\n21.0\n18.2\n17.3\n22.0\n19.6\n18.8\nFr‚ÜíEn 14\ntest\nBLEU-sb\n29.8\n15.5\n19.6\n31.7\n31.6\n30.1\n38.0\n36.4\n36.5\nEn‚ÜíFr 14\ntest\nBLEU-sb\n29.6\n11.4\n11.6\n28.8\n28.3\n26.0\n33.3\n33.3\n31.2\nDe‚ÜíEn 16\ntest\nBLEU-sb\n31.7\n18.2\n21.7\n33.3\n31.9\n31.1\n38.9\n36.5\n36.2\nEn‚ÜíDe 16\ntest\nBLEU-sb\n23.1\n9.36\n9.00\n24.6\n21.7\n21.1\n27.6\n24.1\n24.5\nWinograd\ntest\nacc\n85.3\n85.7\n86.8\n84.6\n84.6\n84.2\n86.4\n85.4\n83.9\nWinogrande\ndev\nacc\n66.8\n64.5\n62.5\n67.6\n65.8\n64.5\n71.0\n67.4\n67.2\nPIQA\ndev\nacc\n79.1\n78.0\n78.0\n77.3\n76.3\n76.9\n79.2\n77.8\n77.7\nARC (Challenge)\ntest\nacc\n42.1\n41.4\n42.5\n44.0\n41.5\n42.4\n43.8\n43.7\n42.7\nARC (Easy)\ntest\nacc\n64.3\n60.2\n61.9\n65.3\n62.6\n63.4\n67.3\n65.8\n65.3\nOpenBookQA\ntest\nacc\n54.4\n50.4\n52.6\n56.4\n53.0\n52.8\n58.4\n55.2\n54.4\nQuac\ndev\nf1\n41.8\n36.1\n38.2\n43.1\n39.0\n39.5\n44.0\n39.9\n39.9\nRACE-h\ntest\nacc\n45.0\n44.1\n43.2\n44.9\n44.3\n42.9\n45.2\n44.7\n43.4\nRACE-m\ntest\nacc\n58.4\n54.4\n54.0\n57.9\n54.7\n53.8\n58.6\n55.4\n55.4\nSQuADv2\ndev\nf1\n59.9\n52.7\n50.9\n64.9\n57.1\n54.7\n68.9\n62.1\n58.4\nCoQA\ndev\nf1\n78.5\n72.8\n72.9\n80.9\n75.1\n74.4\n81.3\n77.3\n75.4\nDROP\ndev\nf1\n17.1\n17.0\n17.4\n23.3\n27.3\n25.7\n33.9\n29.7\n28.7\nBoolQ\ndev\nacc\n69.4\n65.4\n60.9\n74.1\n68.7\n65.0\n73.9\n70.0\n69.7\nCB\ndev\nacc\n21.4\n28.6\n37.5\n60.7\n33.9\n32.1\n62.5\n60.7\n66.1\nCopa\ndev\nacc\n82.0\n80.0\n77.0\n81.0\n82.0\n81.0\n88.0\n83.0\n82.0\nRTE\ndev\nacc\n55.2\n55.2\n46.2\n61.0\n54.9\n58.8\n52.7\n49.5\n59.9\nWiC\ndev\nacc\n0.\n0.\n0.\n50.0\n50.3\n50.3\n50.5\n53.1\n51.3\nANLI R1\ntest\nacc\n33.7\n32.3\n33.4\n32.4\n31.6\n31.7\n30.9\n33.1\n30.7\nANLI R2\ntest\nacc\n33.8\n33.5\n33.0\n34.8\n33.9\n33.7\n35.0\n33.3\n32.2\nANLI R3\ntest\nacc\n32.7\n34.8\n33.4\n34.8\n33.1\n33.3\n36.9\n33.9\n32.3\n35\n\nTable 11: Evaluation results comparing the GPT-3 6.7B model tuned with ¬µTransfer against\nthe twice-as-large GPT-3 13B model from [7]. The two models have similar performance on most\nof the evaluation tasks.\nZero-shot\nOne-shot\nFew-shot\nTask\nSplit\nMetric\n6.7B+¬µP\n13B[7]\n6.7B+¬µP\n13B[7]\n6.7B+¬µP\n13B[7]\nHellaSwag\ndev\nacc\n72.0\n70.9\n71.1\n70.0\n72.4\n71.3\nLAMBADA\ntest\nacc\n73.5\n72.5\n69.9\n69.0\n74.7\n81.3\nStoryCloze\ntest\nacc\n79.4\n79.5\n80.6\n79.7\n84.2\n83.0\nNaturalQS\ntest\nacc\n9.86\n7.84\n14.7\n13.7\n20.2\n21.0\nTriviaQA\ndev\nacc\n47.0\n41.8\n50.4\n51.3\n55.5\n57.5\nWebQS\ntest\nacc\n11.3\n8.22\n20.2\n19.0\n33.0\n33.5\nRo‚ÜíEn 16\ntest\nBLEU-sb\n26.9\n20.8\n36.5\n36.7\n38.2\n38.4\nEn‚ÜíRo 16\ntest\nBLEU-sb\n18.1\n6.43\n21.0\n20.8\n22.0\n21.8\nFr‚ÜíEn 14\ntest\nBLEU-sb\n29.8\n22.4\n31.7\n31.4\n38.0\n38.3\nEn‚ÜíFr 14\ntest\nBLEU-sb\n29.6\n15.3\n28.8\n30.1\n33.3\n35.5\nDe‚ÜíEn 16\ntest\nBLEU-sb\n31.7\n24.4\n33.3\n34.5\n38.9\n39.1\nEn‚ÜíDe 16\ntest\nBLEU-sb\n23.1\n11.0\n24.6\n23.3\n27.6\n27.7\nWinograd\ntest\nacc\n85.3\n87.9\n84.6\n86.1\n86.4\n82.4\nWinogrande\ndev\nacc\n66.8\n67.9\n67.6\n66.9\n71.0\n70.0\nPIQA\ndev\nacc\n79.1\n78.5\n77.3\n77.8\n79.2\n79.9\nARC (Challenge)\ntest\nacc\n42.1\n43.7\n44.0\n43.1\n43.8\n44.8\nARC (Easy)\ntest\nacc\n64.3\n63.8\n65.3\n66.8\n67.3\n69.1\nOpenBookQA\ntest\nacc\n54.4\n55.6\n56.4\n55.8\n58.4\n60.8\nQuac\ndev\nf1\n41.8\n38.4\n43.1\n40.6\n44.0\n40.9\nRACE-h\ntest\nacc\n45.0\n44.6\n44.9\n44.6\n45.2\n45.1\nRACE-m\ntest\nacc\n58.4\n56.7\n57.9\n56.9\n58.6\n58.1\nSQuADv2\ndev\nf1\n59.9\n56.3\n64.9\n61.8\n68.9\n67.7\nCoQA\ndev\nf1\n78.5\n76.3\n80.9\n77.9\n81.3\n79.9\nDROP\ndev\nf1\n17.1\n24.0\n23.3\n29.2\n33.9\n32.3\nBoolQ\ndev\nacc\n69.4\n66.2\n74.1\n69.0\n73.9\n70.2\nCB\ndev\nacc\n21.4\n19.6\n60.7\n55.4\n62.5\n66.1\nCopa\ndev\nacc\n82.0\n84.0\n81.0\n86.0\n88.0\n86.0\nRTE\ndev\nacc\n55.2\n62.8\n61.0\n56.3\n52.7\n60.6\nWiC\ndev\nacc\n0.\n0.\n50.0\n50.0\n50.5\n51.1\nANLI R1\ntest\nacc\n33.7\n33.2\n32.4\n32.7\n30.9\n33.3\nANLI R2\ntest\nacc\n33.8\n33.5\n34.8\n33.9\n35.0\n32.6\nANLI R3\ntest\nacc\n32.7\n34.4\n34.8\n32.5\n36.9\n34.5\n2\n0\nlog2\n0.90\n0.91\n0.92\n0.93\n0.94\n0.95\nValidation Accuracy\nStandard Parametrization\nWidth mult.\n0.5\n1.0\n2.0\n4.0\n8.0\n3\n2\n1\n0\nlog2\n0.90\n0.91\n0.92\n0.93\n0.94\n0.95\n    \n5\n0\n5\nlog2\noutput\n0.91\n0.92\n0.93\n0.94\n0.95\nMax Update Parametrization ( P)\nFigure 16: ResNet on CIFAR-10 for different widths (compared to a base network). On the left, the\nwidest network SP underperforms; on the right, the ¬µP network has a more consistent HP landscape\nand performs better. Both networks are tuned at the smallest width for the HP (Œ∑ or Œ±output) not in\nthe x-axis.\n36\n\nHyperparameter Transfer\nNext, we perform a grid search for learning rate (Œ∑) and Œ±output on\nthe 0.5x model for both SP and ¬µP.23 Then, we take the best combination and test on the 8x model,\nsimulating how a practitioner might use ¬µTransfer. The result is shown in Table 12, where ¬µP\noutperforms SP by 0.43% ¬± .001%.\nTable 12: ResNet on CIFAR10: Transferring the best learning rate (Œ∑) and Œ±output from widening\nfactor 0.5 to 8; ¬µP signiÔ¨Åcantly outperforms SP given the same search grid. The best HPs are different\nas the models are parametrized to be identical at 1x width.23\nTransfer Setup\nBest Œ∑\nBest Œ±output\nValid. Acc. (0.5x)\nValid. Acc. (8x)\nSP\n0.707\n4\n92.82%\n94.86%\n¬µP\n0.5\n4\n92.78%\n95.29%\nG.1.2\nWide ResNet on ImageNet\nSetup\nFor this case we use Wide-Resnet, or WRN [65], a ResNet variant with more channels per\nlayer, to further showcase ¬µTransfer across width, i.e., number of channels. We train with SGD\non ImageNet for 50 epochs following standard data augmentation procedures. We use a width\nmultiplier to identify models of different width, and a multiplier of 1 corresponds to the original\nWRN-50-2-bottleneck in [65].\nHyperparameter Transfer\nWe start with a proxy model with a width multiplier of 0.125 and tune\nseveral HPs using the following grid:\n‚Ä¢ Œ∑: 1 √ó 2.048 √ó 2z, where z ‚àà{‚àí5, ‚àí4, ‚àí3, ..., 4}\n‚Ä¢ Œ±output: 10 √ó 2z, where z ‚àà{‚àí5, ‚àí4, ‚àí3, ..., 4}\n‚Ä¢ weight decay co-efÔ¨Åcient Œ≥: 3.05 √ó 10‚àí5 √ó 2z, where z ‚àà{‚àí2, ‚àí1.5, ‚àí1, ..., 1.5}\n‚Ä¢ SGD momentum Œ≤: 0.875 √ó 2z, where z ‚àà{‚àí2, ‚àí1.5, ‚àí1, ..., 1.5}\nThe grid is centered around the default HPs used by [1] for ResNet-50; while not expected to be\ncompetitive for WRN, they represent a reasonable starting point for our experiment.\nWe randomly sample 64 HP combinations from the grid and train for 50 epochs, before selecting\nthe one with the highest top-1 validation accuracy. Then, we scale up the model following both ¬µP\nand SP and run with the same HPs we just selected. The result is shown in Table 13, where ¬µP\noutperforms SP by 0.41% in terms of top-1 validation accuracy.\nTable 13: ResNet on ImageNet: Transferring the best learning rate (Œ∑), Œ±output, Œ≥, and Œ≤ from\nwidening factor 0.125 to 1; ¬µP signiÔ¨Åcantly outperforms SP given the same search grid.\nTransfer Setup\nBest Œ∑\nBest Œ±output\nBest Œ≥\nBest Œ≤\nValid. Acc. (0.125x)\nValid. Acc. (1x)\nSP\n32.768\n.625\n.000015\n.4375\n58.12%\n76.75%\n¬µP\n32.768\n.625\n.000015\n.4375\n58.12%\n77.16%\nG.2\nExperiments on Transformers\nG.2.1\nVerifying Transfer across Batch Size, Sequence Length, and Training Time on\nWikitext-2\nSee Fig. 19.\nG.2.2\nPost-Layernorm Transformers\nFig. 17 shows the transferability of learning rate, Œ±output, initialization standard deviation, and Adam\nŒ≤2 across width, batch size, sequence length, and training steps for post-layernorm Transformers.\nHowever, in general, we Ô¨Ånd transfer across depth to be fragile.\n23Here we tune the 0.5x model instead of the 1x model to simulate the situation that one does ‚Äúexploratory\nwork‚Äù on the 1x model but, when scaling up, would like to tune faster by using a smaller proxy model.\n37\n\n14\n12\n10\n8\n1\n2\n3\n4\n5\nTraining Loss\nWidth\n128\n256\n512\n1024\n2048\n4096\n5\n0\n5\n10\n15\n1\n2\n3\n4\n5\n5.0\n2.5\n0.0\n2.5\n1\n2\n3\n4\n5\n6\n7\n(a)\n(b)\n(c)\n(d)\n(e)\n(f)\n1.0\n1.5\n2.0\n2.5\n3.0\n3.5\n4.0\n4.5\n14\n12\n10\n2.5\n3.0\n3.5\n4.0\n4.5\n5.0\n5.5\n6.0\nTraining Loss\nBatchSize\n20\n32\n64\n128\n5\n0\n5\n10\n15\n2.5\n3.0\n3.5\n4.0\n4.5\n5.0\n2.5\n0.0\n2.5\n3\n4\n5\n6\n(a)\n(b)\n(c)\n(d)\n(e)\n(f)\n2.4\n2.6\n2.8\n3.0\n3.2\n3.4\n3.6\n3.8\n4.0\n14\n12\n10\nlog2LearningRate\n3.0\n3.5\n4.0\n4.5\n5.0\n5.5\n6.0\n6.5\nTraining Loss\nSeqLen\n32\n64\n128\n256\n512\n1024\n5\n0\n5\n10\n15\nlog2\noutput\n2.5\n3.0\n3.5\n4.0\n4.5\n5.0\n5.5\n5.0\n2.5\n0.0\n2.5\nlog2InitStd\n2.5\n3.0\n3.5\n4.0\n4.5\n5.0\n5.5\n6.0\n6.5\n(a)\n(b)\n(c)\n(d)\n(e)\n(f)\nLR Schedule\n3.0\n3.5\n4.0\n4.5\n5.0\n14\n12\n10\n8\nlog2LearningRate\n3.5\n4.0\n4.5\n5.0\n5.5\n6.0\n6.5\n7.0\nTraining Loss\nStep\n2032\n4072\n5912\n7952\n9992\n5\n0\n5\n10\n15\nlog2\noutput\n3.5\n4.0\n4.5\n5.0\n5.5\n6.0\n5.0\n2.5\n0.0\n2.5\nlog2InitStd\n3.5\n4.0\n4.5\n5.0\n5.5\n6.0\n6.5\n(a)\n(b)\n(c)\n(d)\n(e)\n(f)\nLR Schedule\n3.5\n4.0\n4.5\n5.0\n5.5\n6.0\n6.5\nFigure 17: Empirical validation of ¬µTransfer for Post-LN Transformers. Same setting as Fig. 4.\nG.2.3\nHyperparameter Instability of SP Transformers\nFig. 18 and Fig. 20 show the HP instability inherent in SP Transformers.\nH\nImplementing ¬µTransfer in a Jiffy\nAs we have shown, one can enable ¬µTransfer by just reparametrizing the desired model in Maximal\nUpdate Parametrization (¬µP). While conceptually simple, switching from Standard Parametrization\n(SP) to ¬µP can be error-prone, as popular deep learning frameworks are built around SP. We strive to\nbuild a tool that fulÔ¨Ålls two goals:\n1. Minimize code changes when switching to ¬µP;\n2. Keep model behavior invariant, under this switch, at a given base model shape.\nBy model shape, we mean the collection of dimensions of all parameters of the model. The latter goal,\nwhich we call parametrization backward compatibility, ensures that any code base works exactly as\nbefore at the base model shape, similar to Eq. (4), e.g. the loss at any time step remains exactly the\nsame before and after the switch to ¬µP. Of course, when widths start to differ from the base model\nshape, the model behavior necessarily changes so that HPs can be transferred.\n38\n\n20\n15\n10\nlog2LearningRate\n4\n5\n6\n7\nTraining Loss\n128\n256\n512\n1024\n2048\n4096\n8192\n0\n10\nlog2\noutput\n4.5\n5.0\n5.5\n6.0\n6.5\n7.0\n10\n0\nlog2\nattn\n4.0\n4.5\n5.0\n5.5\n6.0\n6.5\n7.0\n7.5\n5\n0\nlog2\n4.5\n5.0\n5.5\n6.0\n6.5\n7.0\n12\n10\nlog2LearningRate\n3.5\n4.0\n4.5\n5.0\nTraining Loss\n0\n10\nlog2\noutput\n3.50\n3.75\n4.00\n4.25\n4.50\n4.75\n5.00\n5\n10\n15\nlog2\nattn\n3.5\n4.0\n4.5\n5.0\n5\n0\nlog2\n3\n4\n5\n6\nStandard Parametrization (SP)\nMaximal Update Parametrization ( P)\nFigure 18: Post-layernorm Transformer with SP and ¬µP on Wikitext-2. We sweep one HP across\nwidth (dmodel) at a time while keeping the rest Ô¨Åxed; we also scale dhead linearly with dmodel and\nÔ¨Åxing nhead. Œ±output, Œ±attn are multipliers for output and key weights, and œÉ is initialization standard\ndeviation. This yields unstable result for SP, as expected, where missing points/curves represent\ndivergence; in ¬µP, the optimal HP choices stabilize as width increases.\n14\n12\n10\n8\n2.0\n2.5\n3.0\n3.5\n4.0\n4.5\n5.0\n5.5\nBatchSize\nTraining Loss\nBatchSize\n20\n32\n64\n128\n256\n512\n5\n0\n5\n10\n15\n2.0\n2.5\n3.0\n3.5\n4.0\n4.5\n5.0\n5.5\n5.0\n2.5\n0.0\n2.5\n2.0\n2.5\n3.0\n3.5\n4.0\n4.5\n(a)\n(b)\n(c)\n(d)\n(e)\n(f)\n2.0\n2.5\n3.0\n3.5\n4.0\n14\n12\n10\n8\n3.0\n3.5\n4.0\n4.5\n5.0\n5.5\n6.0\n6.5\nSeqLen\nTraining Loss\nSeqLen\n32\n64\n128\n256\n512\n5\n0\n5\n10\n15\n3.0\n3.5\n4.0\n4.5\n5.0\n5.5\n6.0\n6.5\n5.0\n2.5\n0.0\n2.5\n3.0\n3.5\n4.0\n4.5\n5.0\n5.5\n(a)\n(b)\n(c)\n(d)\n(e)\n(f)\n3.0\n3.5\n4.0\n4.5\n5.0\n14\n12\n10\n8\nlog2LearningRate\n3.5\n4.0\n4.5\n5.0\n5.5\n6.0\n6.5\nStep\nTraining Loss\nStep\n2032\n4072\n5912\n7952\n9992\n5\n0\n5\n10\n15\nlog2\noutput\n3.5\n4.0\n4.5\n5.0\n5.5\n5.0\n2.5\n0.0\n2.5\nlog2InitStd\n3.5\n4.0\n4.5\n5.0\n5.5\n6.0\n6.5\n7.0\n(a)\n(b)\n(c)\n(d)\n(e)\n(f)\nLR Schedule\n3.5\n4.0\n4.5\n5.0\n5.5\n6.0\nFigure 19: Empirical validation of ¬µTransfer across Batch Size, Sequence Length, and Training\nTime on pre-LN Transformers. Same setting as Fig. 4. Despite some shift, the optimal HPs are\nroughly stable when transferring from batch size 32, sequence length 128, and 5000 training steps.\n39\n\n17.5\n15.0\n12.5\n10.0\n7.5\nlog2LearningRate\n2\n4\n6\n8\n10\n12\nTraining Loss\nTransformer on IWSLT14 De-En\n(Standard Parametrization)\n64\n128\n256\n512\n1024\n2048\nFigure 20: Learning rate landscape is highly unstable under standard parametrization in IWSLT.\nThere are two common approaches to setting the base model shape: 1) If one intends to tune a large\ntarget model, then the user can set the base model shape to be the shape of the target model (e.g.\nBERT-large or T5-large), so that the target model itself is in standard parametrization. Then one\ncan tune a proxy model with e.g. width = 124 to obtain the optimal HPs for the target model. In\naddition, if one wishes to scale up further e.g. width = 1024, then these HPs remain optimal. 2)\nIf one has done exploration on a new idea with a small model and now wishes to scale up, reusing\nthe HP found during this exploration, then one can set the base model shape to be the shape of the\nexploratory small model. Of course, in both scenarios, depth, batch size, and sequence lengths can\nbe scaled up and down as well according to Fig. 19 (though note that currently we require users to\nrecreate the base model shape at new depths, since the number of parameters now change with depth).\nThe mup Package\nWe provide our tool as a Python package called mup designed to work with\nPyTorch. The following example illustrates the usage of our package.\n40\n\nWhat Happens in the mup Package\nUnder the hood, mup implements the ¬µP formulation in\nTable 8. By invoking set_base_shape(model, base_model), each parameter tensor p of model\ngets a p.infshape attribute that stores, for each of its dimensions, the corresponding base dimension\nand whether that dimension should be considered ‚ÄúinÔ¨Ånite‚Äù (i.e. will be scaled up/down, e.g.,\ndmodel of a Transformer) or ‚ÄúÔ¨Ånite‚Äù (i.e. will be Ô¨Åxed, e.g., vocabulary size). This information\nis used in the initializers and optimizers to automatically scale the parameters or learning rates\nto be compliant with ¬µP. For example, by Table 8, the Adam learning rate of hidden weights p\nis calculated as Œ∑/p.infshape.width_mult(), where p.infshape.width_mult() essentially\ncalculates\nfan_in\nbase_fan_in.\nI\nReverse-¬µTransfer for Diagnosing Training Instability in Large Models\nLarge Transformers are famously Ô¨Åckle to train [25, 37]. We note that a possible source of this\ninstability for larger transformers is the failure of naive hyperparameter transfer via the standard\nparametrization. This is certainly consistent with Fig. 1, which shows that the optimal learning\nrate for small Transformers can lead to trivial performance in large Transformers. We support this\nhypothesis further by reverse-¬µTransferring the instability-inducing HPs from a large Transformer to\na small one and replicating the training instability. This is shown in Fig. 21.\nPractically, this reverse-¬µTransfer technique can be used to diagnose or debug training instability\nproblems of large models. We offer two case studies toward this claim.\n1) When training transformers of width 8192 on Wikitext-2, we found certain HP combinations\ncaused divergence in the middle of training. We reverse-¬µTransferred one such HP combination to\na model of width 256 and replicated this divergence. By analyzing this small model‚Äôs activations\nright before this divergence, we found that the cause is due to attention logits blowing up. Note this\n41\n\ndebugging session proceeded much more quickly than if we directly worked with the large model.\nLater we conÔ¨Årmed this is indeed the same cause of the width-8192 model‚Äôs divergence.\n2) A 6B-parameter language model (in standard parametrization) in a separate project experienced\nrepeated blow-up in the middle of training. We reverse-¬µTransferred its hyperparameters to a smaller,\n100M-parameter model and replicated the training instability. This was solved by a retuning of the\nsmall model via random search.\n20\n18\n16\n14\n12\n10\n8\nlog2LearningRate\n4.0\n4.5\n5.0\n5.5\n6.0\n6.5\n7.0\n7.5\nTraining Loss\ntraining instability\nFix Hparam., Change Width\nActual Width\n256\n512\n1024\n2048\n4096\n8192\n20\n18\n16\n14\n12\n10\n8\nlog2LearningRate\n4.5\n5.0\n5.5\n6.0\n6.5\n7.0\n7.5\nTraining Loss\ntraining instability\nFix Width, Change Hparam.\nSimulated\n    Width\n256\n512\n1024\n2048\n4096\n8192\nFigure 21: Replicating training instability on a small Transformer by reverse-¬µTransferring\nhyperparameters. These experiments concern 2-layer Transformers in Standard Parametrization\n(SP) on Wikitext-2, trained with Adam, where width is deÔ¨Åned as dmodel = dffn. (Left) LR-vs-\nloss for wider and wider Transformers. (Right) Likewise for simulated width: Here each point\n(log2 Œ∑, loss) for simulated width n indicates the loss from training a width-256 ¬µP Transformer\nwith base width n and LR Œ∑ (i.e. loosely speaking, it‚Äôs using LR transferred from Œ∑ in a width-n SP\nTransformer). Takeaway: The overall shapes of the curves are identical between the left and right\nplots24; in particular, a learning rate leads to instability in a wide model iff it does so when transferred\nback to a narrow model.\nJ\nAn Intuitive Introduction to the Theory of Maximal Update\nParametrization\nIn what follows, we seek to describe useful intuitions and rules of thumb that would be helpful\nto practitioners and empirical researchers alike in Ô¨Åguring out what is the right neural network\nparametrization. The intuitions we shall describe regarding SGD can be made rigorous as in [56, 57];\nthose regarding Adam are new, and their formalization will be done in an upcoming paper.\nFirst, we write down the most basic intuition regarding sums of many random elements, which will\nunderlie all of the calculations that follow.\nLaw of Large Numbers (LLN)\nIf x1, . . . , xn, . . . ‚Äúlook like‚Äù random independent samples of a\nrandom variable X, then\n1\nn\nn\nX\ni=1\nxi ‚ÜíE[X],\nas n ‚Üí‚àû.\nCentral Limit Theorem (CLT)\nIn the same scenario as above,\n1\n‚àön\nn\nX\ni=1\n(xi ‚àíE[X]) ‚ÜíN(0, œÉ(X)),\nas n ‚Üí‚àû,\nwhere œÉ(X) is the standard deviation of the random variable X.\nOf course, there are many subtleties one must resolve to make the statements above truly rigorous\n(e.g., what is the meaning of ‚Äúlook like‚Äù?), but as rules of thumb, they typically give the correct\nprediction.\n24 Note that the curves on the left are ‚Äúlower‚Äù than curves on the right. This just reÔ¨Çects the increasing capacity\nof wider models able to Ô¨Åt the training data better, so is orthogonal to our point.\n42\n\nTable 14: Expected entry size of Av for different matrices A and vector v correlated with each other,\nboth having entries of size Œò(1).\nStandard Gaussian\n(Nonlinear) Tensor Product\nVector\nA ‚ààRn√ón\nA ‚ààRn√ón\nA ‚ààR1√ón\nEntry size of Av\nŒò(‚àön)\nŒò(n)\nŒò(n)\nIn particular, here we want to note the following basic intuition regarding the size of a sum of xi:\nwhen n is large,\nn\nX\ni=1\nxi has typical size\n\u001aŒò(n)\nif E[X] Ã∏= 0\nŒò(‚àön)\notherwise\nHere, ‚Äútypical size‚Äù can be taken to mean the size 99% of time. Again, we stress that this is a good\nrule of thumb that yields the correct prediction in the cases we are concerned with here; the rigorous\nversions of this will come from the Tensor Programs framework (e.g., [56]).\nJ.1\nBehaviors of Gaussian Matrices vs Tensor Product Matrices\nCentral to the derivation of ¬µP for any architecture are key insights on the behaviors of two kinds of\nrandom matrices: 1) iid Gaussian random matrix and 2) tensor product matrix (by which we mean a\nsum of outer products) and more generally what we call nonlinear tensor product matrix (see Eq. (7)).\nFor example, a neural network, randomly initialized in the typical way, will have each weight matrix\nlook like the former. However, every step of training by gradient descent adds a sum of outer products\nto this initial matrix, so that the change in weights constitute a tensor product matrix. For Adam,\nthe change in weights is not a tensor product but a more general nonlinear tensor product matrix\n(see Eq. (7)). In this section, we will particularly focus on the right scaling for the entries of such\nmatrices, leading to a discussion of the right neural network parametrization in the next section. We\nconcentrate on the key heuristics but eschew burdensome rigor.\nKey Insights\nConsider a random vector v ‚ààRn with approximately iid entries and a random\nmatrix A of either size n √ó n or 1 √ó n, both having entries of size Œò(1).25 In the context of deep\nlearning, v for example can be an activation vector in an MLP, a Gaussian A the hidden weights at\ninitialization, a (nonlinear) tensor product A the change in hidden weights due to training, and a\nvector A the readout layer weights. Then Av corresponds to a part of the next layer preactivation\nor the network output. To make sure the preactivations and the output don‚Äôt blow up, we thus need\nto understand the scale of Av, especially in the general case where A is correlated with v.26 This\nis summarized in Table 14, with the derivations below. Intuitively, a (nonlinear) tensor product or\nvector A will interact with a correlated v via Law of Large Numbers, hence the n-scaling, while a\nGaussian A interacts with v via Central Limit Theorem, hence the ‚àön-scaling.\nIn the derivations below, we answer a slightly different but equivalent question of ‚Äúhow to scale A\nsuch that Av has entry size Œò(1)?‚Äù\nJ.1.1\nPreparation for the Derivations\nBy the results of [57], each (pre-)activation vector and its gradient vector in a multi-layer perceptron,\nat any time during training, have approximately iid coordinates in the large width limit,27 and\nsomething similar can be said for more advanced networks such as ResNet and Transformers 28.\nDeÔ¨Ånition J.1. We say any such vector v ‚ààRn has Œò(na)-sized coordinates, or just Œò(na)-\ncoordinates for short, if ‚à•v‚à•2/n = Œò(n2a) as n ‚Üí‚àû. Because, by the above discussion, the\ncoordinates are roughly iid when n is large, this intuitively means that each entry of v has ‚Äútypical\nsize‚Äù Œò(na). We make similar deÔ¨Ånitions with Œò replaced by O and ‚Ñ¶.\n25in the sense that the the variance of the entries are Œò(1)\n26Here ‚Äúcorrelated‚Äù formally means v depends on W ‚ä§in a Tensor Program. This essentially captures all\nscenarios of ‚Äúv correlated with W‚Äù that occurs in deep learning.\n27Our intuition here is derived from the assumption that width is much larger than training time; of course, as\nillustrated by our myriad experiments, these intuition are very useful even when this is not the case, such as\nwhen training to convergence.\n28E.g. in a convnet, the (pre-)activations are iid across channels, but correlated across pixels\n43\n\nFurthermore, to each such vector v with Œò(1)-sized coordinates, we can associate a random variable\nZv, independent of n, that represents the coordinate distribution of v, in such a way that: If vector u\nis correlated with v, then Zu will also be correlated with Zv, and limn‚Üí‚àûv‚ä§u/n = E ZuZv.\nJ.1.2\nLinear Tensor Product Matrix (e.g. SGD Updates)\nThe case of (linear) tensor product matrix can be reduced to the outer product case by linearity. Given\nu, v, x ‚ààRn having approximately iid coordinates (of size Œò(1)) like discussed above, we can form\nthe outer product\nA\ndef\n= u ‚äóv/n = uv‚ä§/n,\n(6)\nwhich is the form of a single (batch size 1) gradient update to a weight matrix. Then, by Law of\nLarge Numbers,\nAx = uv‚ä§x\nn\n‚âàcu,\nwhere\nc = E ZvZx.\nSo Ax also has approximately iid coordinates, distributed like ZAx def\n= Zu E ZvZx. Likewise, if A is\na sum of outer products A = Pk\ni=1 ui ‚äóvi/n, then\nAx =\nk\nX\ni=1\nui vi‚ä§x\nn\n,\nwith coordinates distributed as\nZAx =\nk\nX\ni=1\nZui E ZviZx.\nNotice that each coordinate of A has size Œò(1/n). The above reasoning shows that, in order for Ax\nto have coordinate size Œò(1) (assuming x does), then Œò(1/n) is the right coordinate size for A, in\nthe general case that vi and x are correlated (as is generically the case during gradient descent, with\nA = ‚àÜW for some weights W and x being the previous activations).29\nJ.1.3\nNonlinear Tensor Product Matrix (e.g. Adam Updates)\nWhen using Adam or another adaptive optimizer that normalizes the gradient coordinatewise before\napplying them, we need to modify our argument slightly to obtain the right coordinate size scaling of\nthe matrix. The gradient update A, after such normalization, will take the form of\nAŒ±Œ≤ = œà(u1\nŒ±, . . . , uk\nŒ±, v1\nŒ≤, . . . , vk\nŒ≤),\nfor some œà : R2k ‚ÜíR and vectors ui, vj‚ààRn.\n(7)\nWe say a matrix of this form is a nonlinear tensor product matrix.\nFirst, note the tensor product matrices (e.g. the form of SGD update) discussed previously (Eq. (6))\nalready takes this form, with œà(u1\nŒ±, . . . , uk\nŒ±, v1\nŒ≤, . . . , vk\nŒ≤) = n‚àí1(u1\nŒ±v1\nŒ≤ + ¬∑ ¬∑ ¬∑ + uk\nŒ±vk\nŒ≤), so Eq. (7)\nis a strict generalization of linear tensor products. Next, for the example of Adam, each gradient\nupdate is ¬µ/œÉ where ¬µ (resp. œÉ2) is the moving average of previous (unnormalized) gradients (resp.\nthe coordinatewise square of the same).30 If these unnormalized gradients are the outer products\nu1 ‚äóv1, . . . , uk ‚äóvk, then the update has coordinates\n(¬µ/œÉ)Œ±Œ≤ = œà(u1\nŒ±, . . . , uk\nŒ±, v1\nŒ≤, . . . , vk\nŒ≤)\ndef\n=\nX\ni\nŒ≥iui\nŒ±vi\nŒ≤/\nsX\ni\nœâi(uiŒ±vi\nŒ≤)2,\n(8)\nwhere Œ≥i and œâi are the weights involved in the moving averages.\nNow suppose we have some A ‚ààRn√ón of the form Eq. (7), where ui, vi ‚ààRn have approximately\niid coordinates (of size Œò(1)), and œà = n‚àí1 ¬Øœà where ¬Øœà doesn‚Äôt depend on n (in terms of Adam where\n¬Øœà corresponds to the œà of Eq. (8), this corresponds to using a learning rate of 1/n). Then for x ‚ààRn\nhaving approximately iid coordinates of size Œò(1), by Law of Large Numbers,\n(Ax)Œ± = 1\nn\nn\nX\nŒ≤=1\n¬Øœà(u1\nŒ±, . . . , uk\nŒ±, v1\nŒ≤, . . . , vk\nŒ≤)xŒ≤ ‚âàE ¬Øœà(u1\nŒ±, . . . , uk\nŒ±, Zv1, . . . , Zvk)Zxdef\n= Œ®(u1\nŒ±, . . . , uk\nŒ±).\n29In some corner cases when x is uncorrelated with v, then v‚ä§x = Œò(‚àön) by Central Limit, so actually\nAx has Œò(1/‚àön)-coordinates. However, this case does not come up much in the context of training neural\nnetworks.\n30Adam also has bias correction for the moving averages which can be accomodated easily, but for simplicity\nwe omit them here.\n44\n\nHere we made the obvious deÔ¨Ånition\nŒ® : Rk ‚ÜíR,\nŒ®(r1, . . . , rk)\ndef\n= E ¬Øœà(r1, . . . , rk, Zv1, . . . , Zvk)Zx.\nThus Ax also has approximately iid coordinates (of size Œò(1)),\nZAx def\n= Œ®(Zu1, . . . , Zuk).\nFor example, in the SGD example with A = u ‚äóv/n and ¬Øœà(uŒ±, vŒ≤) = uŒ±vŒ≤, this formula gives\nZAx = Œ®(Zu) where Œ®(z) = z E ZvZx, recovering the earlier derivation.\nIn any case, the point here is that A has coordinate size Œò(1/n), and this is the unique scaling that\nleads to Ax having coordinate size Œò(1).\nJ.1.4\nVector Case (e.g. Readout Layer)\nThe vector A case is similar to the tensor product cases above.\nJ.1.5\nGaussian Matrix (e.g. Hidden Weights Initialization)\nNow consider the case where A ‚ààRn√ón is random Gaussian matrix with AŒ±Œ≤ ‚àºN(0, 1/n) and\nx ‚ààRn has approximately iid coordinates distributed like Zx. In the context of neural network\ntraining, A should be thought of as a randomly initialized weight matrix, and x for example can be\ntaken to be an activation vector in the Ô¨Årst forward pass.\nA Quick Intuition\nBy standard random matrix theory, A has Œò(1) operator norm with high\nprobability. Thus, with high probability, for any ‚Äútypical‚Äù vector x, we expect ‚à•Ax‚à•= Œò(‚à•x‚à•), even\nif x is correlated with A. If Ax‚Äôs coordinates are ‚Äúevenly distributed‚Äù, then this would imply Ax has\nŒò(1)-coordinates if x does. However, this is not so clear. Below we provide intuitions for why this\nwould be the case.\nIntuition for Evenness of Coordinate Distribution\nIf x is independent from A (or sufÔ¨Åciently\nuncorrelated), then each coordinate (Ax)Œ± has variance E(Zx)2 = Œò(1) (so by deÔ¨Ånition has size\nŒò(1)). Thus, here A having Œò(1/‚àön)-coordinates leads to Ax having Œò(1)-coordinates, in contrast\nto the tensor product case above.\nWhen x is correlated with A, it turns out the same scaling applies (Œò(1/‚àön) is the unique scaling for\nA‚Äôs entries such so that Ax has Œò(1) entries), but the reasoning is much more subtle: In the context\nof neural network training, it turns out all scenario where x is correlated with A can be reduced\nto the case where x = œÜ(A‚ä§y, . . .) for some coordinatewise nonlinearity œÜ and some other vector\nRn.31 Let‚Äôs consider a very simple example with x = A‚ä§1 for the all 1s vector 1 ‚ààRn (which has\ncoordinate size Œò(1) as can be checked easily). Then, for each index Œ± ‚àà[n], we can calculate\n(AA‚ä§1)Œ± =\nX\nŒ≤,Œ≥\nAŒ±Œ≤AŒ≥Œ≤ =\nX\nŒ≤\nA2\nŒ±Œ≤ +\nX\nŒ≤\nX\nŒ≥Ã∏=Œ±\nAŒ±Œ≤AŒ≥Œ≤.\nSince E A2\nŒ±Œ≤ = 1/n, by the Law of Large Number, the Ô¨Årst sum P\nŒ≤ A2\nŒ±Œ≤ ‚âà1. On the other hand,\nthere are n summands of the form P\nŒ≥Ã∏=Œ± AŒ±Œ≤AŒ≥Œ≤, all iid with variance n‚àí1\nn2 = Œò(1/n). Thus by\nthe Central Limit Theorem, we expect P\nŒ≤\nP\nŒ≥Ã∏=Œ± AŒ±Œ≤AŒ≥Œ≤ ‚âàN(0, 1). Therefore, each coordinate\nof (AA‚ä§1)Œ± looks like 1 + N(0, 1) = N(1, 1) and thus has size Œò(1); again this is caused by A\nhaving Œò(1/‚àön)-coordinates.\nThis example can be generalized to more general x that is correlated with A, but the mathematics is\nquite involved. See [56] for more details.\nJ.2\nDeriving ¬µP for Any Architecture\nArmed with the insight from the last section, we now outline the key steps to derive ¬µP in Table 3 for\nany architecture. In practice, ¬µP implies the following desiderata\nDesiderata J.1. At any time during training\n1. Every (pre)activation vector in a network should have Œò(1)-sized coordinates32\n31This is because every ‚Äúreasonable‚Äù deep learning computation can be expressed in a Tensor Program.\n32In a convnet, a (pre-)activation vector corresponds to a single pixel across all channels; in general , we\nexpect (pre-)activations are iid across channels, but correlated across pixels\n45\n\n2. Neural network output should be O(1).\n3. All parameters should be updated as much as possible (in terms of scaling in width) without\nleading to divergence.\nLet‚Äôs brieÔ¨Çy justify these desiderata. For the desideratum 1, if the coordinates are œâ(1) or o(1),\nthen for sufÔ¨Åciently wide networks their values will go out of Ô¨Çoating point range. This problem is\nparticularly acute for low-precision formats that are essential for training large models such as BERT\nor GPT. Moreover, a general nonlinearity is only well-behaved if its input is in a Ô¨Åxed range (although\nthis is not a problem for homogeneous nonlinearities like relu). For example, for tanh nonlinearity, if\nthe preactivation is vanishing o(1), then tanh is essentially linear; if the preactivation is exploding\nœâ(1), then the tanh gradient vanishes.\nFor the desideratum 2, a similar justiÔ¨Åcation applies to the numerical Ô¨Ådelity of the loss function and\nloss derivative. Note that, with desideratum 3, this means the network output should be Œò(1) after\ntraining (but it can go to zero at initialization).\nFinally, desideratum 3 means that 1) we are doing ‚Äúmaximal feature learning‚Äù [57] and 2) every\nparameter contribute meaningfully in the inÔ¨Ånite-width limit. This ensures that learning rate ‚Äúplays\nthe same role‚Äù in the Ô¨Ånite-width case as in the inÔ¨Ånite-width limit. For example, it prevents the\nscenario where a weight matrix gets stuck at initialization in the limit for any learning rate (so\nlearning rate does not matter) but evolves nontrivially in any Ô¨Ånite-width network (so learning rate\ndoes matter).\nThese desiderata will essentially uniquely single out ¬µP. More formally, ¬µP is the unique parametriza-\ntion that admits feature learning in all parameters of the neural network [57], and this property\ntheoretically guarantees HP transfer across width (for sufÔ¨Åciently large width). However, for the sake\nof reaching a broader audience, we will focus more on the intuitive derivations from the desiderata\nrather than on this formal aspect.\nBelow, we Ô¨Årst assume for simplicity that the width of every layer is n, and we focus only on dense\nweights. Later, we will discuss convolutions and varying the widths between layers.\nJ.2.1\n¬µP Derivation From the Desiderata\nBelow, we will derive the ¬µP formulation in Table 3. Tables 8 and 9 can be derived from Table 3 via\nthe following equivalences, which can be easily derived via some simple calculations.\nLemma J.1. Let ft(Œæ) denote the neural network function after t steps of training (using any Ô¨Åxed\nsequence of batches), evaluated on input Œæ. Consider a parameter tensor W with learning rate C,\ninitialized as W ‚àºN(0, B2), and with a multiplier A. Then for any Œ∏ > 0, ft(Œæ) stays Ô¨Åxed for all t\nand Œæ if we set\n‚Ä¢ when the optimizer is SGD\nA ‚ÜêAŒ∏, B ‚ÜêB/Œ∏, C ‚ÜêC/Œ∏2\n‚Ä¢ when the optimizer is Adam,\nA ‚ÜêAŒ∏, B ‚ÜêB/Œ∏, C ‚ÜêC/Œ∏;\nFor example, for output weights, Table 3 has A = 1, B = 1/fan_in, C = Œ∑/fan_in for SGD and\nAdam. Then taking Œ∏ = 1/fan_in, we get the entries in Table 8, with A = 1/fan_in, B = 1,\nC = Œ∑ ¬∑ fan_in for SGD and C = Œ∑ for Adam. Taking Œ∏ = 1/‚àöfan_in instead, we get the entries in\nTable 9, with A = 1/‚àöfan_in, B = 1/fan_in, C = Œ∑ for SGD and Œ∑/‚àöfan_in for Adam. Similar\ncalculations hold for the input weights scaling in those tables, after taking into consideration that\nfan_in is considered a constant in terms of width for the input layer.\nWe proceed with the derivation of Table 3 below. Recall the deÔ¨Ånitions of Œò(na)-sized coordinates\nor Œò(na)-coordinates from DeÔ¨Ånition J.1.\nOutput Weights\nSuppose W ‚ààR1√ón is an output weight. By desideratum 1, the input x to W\nhas Œò(1)-sized coordinates. Thus W should have Œò(1/n)-coordinates so that |Wx| = O(1). We\ncan initialize W with Œò(1/n)-coordinates and scale its (per-layer) LR so that ‚àÜW has Œò(1/n)-\ncoordinates as well. This means initializing WŒ±Œ≤ ‚àºN(0, Œò(1/n2)) and use Œò(1/n) learning rate\nfor both SGD and Adam.\n46\n\nHidden Weights\nConsider a square weight matrix W ‚ààRn√ón. Desiderata 1 guarantees that the\ninput x to W has Œò(1)-sized coordinates. Generally, x will be correlated with W. By Table 14, we\ncan immediately derive\nInitialization W should be randomly initialized with coordinate size Œò(1/‚àön)\nLR The learning rate should be scaled so that ‚àÜW has coordinate size Œò(1/n)\nso that (W0 + ‚àÜW)x is Œò(1) if x is, inductively satisfying desideratum 1. With Adam, this just\nmeans the per-layer LR is Œò(1/n). With SGD and the scaling of output layers above, we can calculate\nthat the gradient of W has Œò(1/n)-coordinates, so the Œò(1) SGD LR derived above sufÔ¨Åces as well.\nInput Weights\nSuppose W ‚ààRn√ód is an input weight. To satisfy desideratum 1 (i.e. for any\ninput Œæ, WŒæ should have Œò(1)-coordinates), we want W to have Œò(1)-coordinates. We can initialize\nW with Œò(1)-coordinates and scale its (per-layer) LR so that ‚àÜW has Œò(1)-coordinates as well.\nThis implies initialization variance of Œò(1) (or Œò(1/fan_in) since fan_in = Œò(1) here) and Adam\nlearning rate Œò(n). As above, we can calculate that the gradient of W has Œò(1/n)-coordinates, so\nwe want SGD learning rate Œò(n).\nBiases\nBiases follow the same reasoning as input weights (just think of it as an input weight with\ninput 1).\nAttention\nSuppose the key dimension dk is tending to inÔ¨Ånity with width with number of heads\nnhead Ô¨Åxed. Then the key-query contraction q‚ä§k ‚ààR scales like Œò(dk) by Law of Large Numbers\n(instead of Central Limit Theorem because q and k are generally correlated) and desideratum 1, hence\nthe 1/dk we propose rather than 1/‚àödk.\nNow suppose instead that nhead tends to inÔ¨Ånity with width with dk Ô¨Åxed.\nLet K, Q ‚àà\nRN√ódk√ónhead, V ‚ààRN√ódv√ónhead be keys, queries, and values across all heads and tokens. Think-\ning of N √ó dk as constants, we may view attention as a nonlinearity coordinatewise in the nhead\ndimension. Then it‚Äôs clear that our parametrization described above already works.\nFinally, we may freely let dk and nhead both tend to inÔ¨Ånity, and the above reasoning shows that our\nparametrization still works.\nChanging Width Ratios\nAs noted above, at any time in training, every (pre-)activation vector will\nhave approximately iid coordinates (of order Œò(1) by desideratum 1). Another desideratum for ¬µP is\nto ensure that this coordinate distribution (at any particular time) stays roughly invariant as widths\nincreases. When all layer widths are tied, this is automatic if the other desiderata are satisÔ¨Åed, hence\nwhy we did not list this above.\nWhen width ratios vary, this is not automatic. In this case, we need to choose whether to replace each\nn with fan-in or fan-out (or some function of them). Making the wrong choices will let the coordinate\ndistributions vary with width ratios.\nObviously, we should replace n with fan-in for the output layers and with fan-out for the input layers\nsince they are the only dimension scaling with n. For the hidden weights, we replace n with fan-in\nso that the forward pass is preserved. When using Adam (and assuming the initialization of W is\nquickly dominated by the change in W), this ensures that the (pre-)activation coordinate distributions\nare preserved at any time during training even if we vary widths in different layers differently. (For\nSGD this doesn‚Äôt quite work in general because the varying width ratios change the gradient sizes of\ndifferent layers differently, whereas Adam always normalizes the gradient coordinatewise).\nConvolution\nA convolution weight tensor W ‚ààRfan_out√ófan_in√ós1√ós2 with kernel size s1 √ó s2\ncan be thought of just as a s1s2 = Œò(1)-sized collection of fan_out √ó fan_in dense weights. Then\nall of our discussions above apply accordingly.\nJ.3\nWhy Other Parametrizations Cannot Admit Hyperparameter Transfer\nStandard Parametrization (SP)\nSP doesn‚Äôt work essentially because it leads to blow-up in the\ninÔ¨Ånite-width limit.\n1. For Adam with LR Œò(1), ‚àÜW would have Œò(1)-coordinates, causing preactivations to blow\nup like Œò(n) by Desideratum 1 and Table 14. We can avoid this blowup with LR Œò(1/n),\n47\n\nbut this induces a non-maximal feature learning limit, which, as we argue below, cannot\ntransfer hyperparameters in all situations.\n2. For SGD, the gradient of Rn√ón weight has Œò(1/‚àön)-coordinates, so Œò(1) learning rate\nwould make preactivation scale like Œò(‚àön) and hence blow up. If we use Œò(1/width)\nlearning rate, then blow-up does not occur. However, this inÔ¨Ånite-width limit is in the kernel\nregime [57] and thus does not allow HP transfer for the same reason that NTP below does\nnot.\nNeural Tangent Parametrization (NTP)\nWe have concrete examples, e.g. Word2Vec in [57],\nwhere the NTK limit has trivial performance ‚Äî so HPs have no effect at all ‚Äî vastly outperformed\nby Ô¨Ånite-width networks ‚Äî where HPs matter. More importantly, wider does not always do better in\nNTP, especially in tasks where feature learning is crucial [57, 61]. So in the context of modern deep\nlearning e.g. large language model pretraining, NTP (or SP with Œò(1/width) LR) does not make\nsense for wide neural networks.\nOther Parametrizations\nRecall the Dynamical Dichotomy Theorem proven in [57], which says\nthat any nontrivial stable ‚Äúnatural parametrization‚Äù (formally, ‚Äúabc-parametrization,‚Äù [57]) either\nadmits a feature learning limit or a kernel limit, but not both.\nOur argument above against SP and NTP will also work against any parametrization inducing a kernel\nlimit. Therefore, it remains to ask, can other feature learning parametrizations transfer HPs?\nWe argue no. As shown in [57], any other feature learning parametrization differs from ¬µP essentially\nonly in that some parameters are not updated maximally. By [57, Sec 6.4], in the inÔ¨Ånite-width limit,\nsuch parameters can be thought of as being Ô¨Åxed at initialization. Therefore, in such inÔ¨Ånite-width\nlimits, the learning rate of such parameters becomes useless. As such, we cannot hope for the HP\nlandscape of the limit to reÔ¨Çect the HP landscape of Ô¨Ånite-width neural networks.\n¬µP is the unique feature learning parametrization that updates all parameters maximally, so that the\nlearning rate of each parameter plays approximately the same role in Ô¨Ånite-width neural networks\nas in the inÔ¨Ånite-width limit. Consequently, the HP landscape of the ¬µP limit should reÔ¨Çect the HP\nlandscape of Ô¨Ånite-width neural networks.\n48\n"
    },
    {
      "arxiv_id": "https://arxiv.org/pdf/2502.02732",
      "full_text": "arXiv:2502.02732v3  [cs.LG]  6 Jun 2025\nPeri-LN: Revisiting Normalization Layer in the Transformer Architecture\nJeonghoon Kim 1 2 Byeongchan Lee 2 Cheonbok Park 1 2 Yeontaek Oh 1 Beomjun Kim 2 Taehwan Yoo 1\nSeongjin Shin 1 Dongyoon Han 3 Jinwoo Shin\n‚Ä† 2 Kang Min Yoo\n‚Ä† 1\nAbstract\nSelecting a layer normalization (LN) strategy\nthat stabilizes training and speeds convergence in\nTransformers remains difficult, even for today‚Äôs\nlarge language models (LLM). We present a com-\nprehensive analytical foundation for understand-\ning how different LN strategies influence train-\ning dynamics in large-scale Transformers. Until\nrecently, Pre-LN and Post-LN have long domi-\nnated practices despite their limitations in large-\nscale training. However, several open-source mod-\nels have recently begun silently adopting a third\nstrategy without much explanation. This strategy\nplaces normalization layer peripherally around\nsublayers, a design we term Peri-LN. While Peri-\nLN has demonstrated promising performance, its\nprecise mechanisms and benefits remain almost\nunexplored. Our in-depth analysis delineates the\ndistinct behaviors of LN strategies, showing how\neach placement shapes activation variance and\ngradient propagation. To validate our theoret-\nical insight, we conduct extensive experiments\non Transformers up to 3.2B parameters, showing\nthat Peri-LN consistently achieves more balanced\nvariance growth, steadier gradient flow, and con-\nvergence stability. Our results suggest that Peri-\nLN warrants broader consideration for large-scale\nTransformer architectures, providing renewed in-\nsights into the optimal placement of LN.\n1. Introduction\nBuilding on a rapidly expanding lineage of Transformer-\nbased large language models, open-source models have\n‚Ä†Equal correspondence. 1NAVER Cloud 2Korea Advanced Insti-\ntute of Science and Technology (KAIST) 3NAVER AI Lab. Corre-\nspondence to: Jeonghoon Kim <jeonghoon.samuel@gmail.com>,\nJinwoo Shin <jinwoos@kaist.ac.kr>, Kang Min Yoo <kang-\nmin.yoo@navercorp.com>.\nProceedings of the 42 nd International Conference on Machine\nLearning, Vancouver, Canada. PMLR 267, 2025. Copyright 2025\nby the author(s).\nshown remarkable impact (Hoffmann et al., 2022; Guo et al.,\n2025; Yoo et al., 2024). As the demand for larger and more\npowerful models grows, various training stabilization tech-\nniques have been introduced (Yang et al., 2022; Zhai et al.,\n2023; Loshchilov et al., 2024). Among these, the choice\nof where and how to apply layer normalization (LN: Lay-\nerNorm or RMSNorm; Ba et al., 2016; Zhang & Sennrich,\n2019) critically influences model convergence (Xiong et al.,\n2020; Kedia et al., 2024; Wortsman et al., 2024). However,\ntheir immense computational requirements have restricted\ndeeper exploration of the underlying Transformer structure.\nAre we truly employing the optimal LN placement? In\npractice, fully revealing the results of massive resource in-\nvestments can be challenging (Rivi`ere et al., 2024). Despite\nits importance, there is still no consensus on a single best\nLN placement strategy.\nTwo prominent LN placements have been widely explored.\nPost-LN (Vaswani et al., 2017) normalizes the hidden state\nafter adding the sub-layer output to the residual stream (that\nis, Norm(x + Module(x)) where x is input hidden state.\nNorm is LN). This helps constrain the variance of hidden\nstates but may inadvertently weaken gradient signals, partic-\nularly in deeper models (Kedia et al., 2024). Pre-LN (Dubey\net al., 2024), by contrast, normalizes before passing the hid-\nden state to the sub-layer (that is, x + Module(Norm(x))).\nWhile this can enhance gradient propagation, it also admits\nso-called ‚Äúmassive activations,‚Äù where hidden states grow\nexponentially across layers (Sun et al., 2024).\nPrevious studies on deep convolutional neural networks\n(CNNs) have analyzed the impact of batch normalization on\nvariance changes during the initialization stage of ResNet\narchitectures, demonstrating its relationship to model per-\nformance (De & Smith, 2020). They noted that, in models\nwithout normalization, hidden activation growth at initial-\nization can be exponential, leading to poor performance\nand stability. In contrast, in pre-normalized CNNs, the vari-\nance of hidden activations was shown to increase linearly\nas model depth grows. In the same vein, Kedia et al. (2024)\nreported that, for Transformer architectures as well, the\nvariance in the forward propagation of Transformer-based\nlanguage models at initialization increases linearly with\ndepth. However, in the context of Transformer architectures,\nwe observed that this variance growth at initialization does\n1\n\nPeri-LN: Revisiting Normalization Layer in the Transformer Architecture\nFigure 1. Illustration of hidden-state variance across different\nmodel depths and training iterations. From initialization through\ntraining on 6.3 billion tokens, we observe the growth in hidden-\nstate variance for both Pre-LN and Post-LN architectures. The\nanalysis is based on a 1.5B-parameter model. Detailed settings\nand additional results are provided in Section 5.1.\nnot persist as training progresses as shown in Figure 1. Sec-\ntions 3, 4, 5, and 6 provide a more detailed discussion of\nthese hidden-state growth patterns.\nBeyond these two common strategies, Post-LN and Pre-LN,\na third LN placement has quietly emerged in large-scale\nopen-source models: applying LN around the sub-layer, i.e.,\non both its input and output. Although recent open-source\nmodels (Team et al., 2025; Rivi`ere et al., 2024; OLMo et al.,\n2024) have quietly adopted such designs and demonstrated\npromising performance on a large scale, these efforts often\nappeared isolated, lacking a conceptual unifying framework\nor a thorough investigation into their benefits. In this pa-\nper, we coin the term Peri-LN1 to unify these scattered\napproaches and highlight an underexplored avenue for sta-\nbilizing large-scale Transformer training. By dissecting the\nforward- and backward-pass dynamics of each LN strategy,\nwe clarify how, when, and why they differ, interpreting these\ndistinctions through the lens of training stability.\nAccordingly, this paper revisits LN placement in Trans-\nformers from both analytical and empirical perspectives. In\nparticular, we:\n1. Present an in-depth analysis of Post-LN and Pre-LN in\nlarge-scale Transformers, examining how variance and\ngradient properties evolve beyond initialization.\n2. Investigate Peri-LN to understand how normalizing\nboth the inputs and outputs of each module moderates\nhidden-state behavior during forward and backward\npropagation, providing a systematic perspective on this\nunderexplored alternative.\n3. Provide quantitative evidence on how large activation\ninfluences training stability, benchmark performance,\nand model behaviors.\n1Peri- (Prefix) means ‚Äúaround,‚Äù reflecting that LN encapsulates\nthe entire sub-layer. (e.g. peripherally)\n2. Background and Motivation\nThe analysis of activation variance at model initialization\nhas long been central to understanding normalization layers\nand enhancing stability in convolutional neural networks\n(CNNs) (De & Smith, 2020; He et al., 2016; Brock et al.,\n2021a). Specifically, De & Smith (2020) showed that batch\nnormalization in residual blocks can bias networks toward\nthe identity function, thereby stabilizing gradients and im-\nproving overall training dynamics.\nSimilar investigations have emerged for Transformer ar-\nchitectures, examining how variance propagates and how\ngradients behave in both post-layer normalization (Post-LN)\nand pre-layer normalization (Pre-LN) configurations (Xiong\net al., 2020; Kedia et al., 2024; Wortsman et al., 2024). Early\nwork comparing Post- and Pre-LN primarily focused only\non the initialization stage. Xiong et al. (2020) observed that\nPre-LN architectures tend to exhibit more stable gradients,\nbut can still encounter issues such as gradient spikes and\ndivergence, especially in deeper models or large-scale pre-\ntraining scenarios (Zhai et al., 2023; Wortsman et al., 2024;\nFishman et al., 2024; Chung et al., 2024).\nAmong these challenges, the phenomenon of ‚Äúmassive acti-\nvations‚Äù has attracted attention (Dettmers et al., 2022; Yu\net al., 2024; Fishman et al., 2024). Notably, Sun et al. (2024)\nidentified that in Pre-LN architectures, large spikes in acti-\nvation magnitude can persist across layers due to residual\nconnections. These massive activations act as fixed biases,\npotentially narrowing the model‚Äôs focus to certain tokens\nand may influence generalization. However, the underlying\nmechanisms behind these large values, and their exact im-\npact on the training process, remain not yet well understood.\nAnalytical work has provided theoretical frameworks to ex-\nplain phenomena like gradient explosion and vanishing in\nTransformers. For instance, Kedia et al. (2024) introduced\na signal propagation theory that details how activation vari-\nance and gradient instability can evolve with depth, identi-\nfying critical factors that impair stability and performance.\nRecent studies have discussed how Pre-LN architectures\ncan allow large values from Attention or MLP modules\nto flow unimpeded through residual connections (Csord¬¥as\net al., 2024; Fishman et al., 2024; Zhai et al., 2023; Worts-\nman et al., 2024), but the precise impact of this behavior on\nlarge-scale training remains insufficiently explored.\nThese observations underscore the ongoing need to clar-\nify how activation dynamics and normalization strategies\ninteract, especially in large-scale training.\nIn response, this work aims to deepen our understanding of\nhow normalization strategies influence Transformer training,\nwith particular attention to the emergence of large activa-\ntions and their implications for stability and performance.\n2\n\nPeri-LN: Revisiting Normalization Layer in the Transformer Architecture\n3. Normalization Strategies\nIn this section, we discuss how different placements of layer\nnormalization (LN 2) in Transformer architecture affect both\ntraining stability and the statistics of hidden states (activa-\ntions 3).\n3.1. Post- & Pre-Normalization in Transformers\nPost-LN.\nThe\nPost-Layer\nNormalization\n(Post-LN)\n(Vaswani et al., 2017) scheme, normalization is applied\nafter summing the module‚Äôs output and residual input:\nyl = Norm\n\u0000xl + Module(xl)\n\u0001\n,\n(1)\nwhere xl is the input hidden state of l-th layer, yl is the\noutput hidden state of l-th layer, and Module denotes At-\ntention or Multi-Layer Perceptron (MLP) module in the\nTransformer sub-layer. Norm denotes normalization lay-\ners such as RMSNorm or LayerNorm. It is known that by\nstabilizing the activation variance at a constant scale, Post-\nLN prevents activations from growing. However, several\nevidence (Xiong et al., 2020; Kedia et al., 2024) suggest\nthat Post-LN can degrade gradient flow in deeper networks,\nleading to vanishing gradients and slower convergence.\nPre-LN.\nThe Pre-Layer Normalization (Pre-LN) (Dubey\net al., 2024) scheme, normalization is applied to the mod-\nule‚Äôs input before processing:\nyl = xl + Module\n\u0000Norm(xl)\n\u0001\n.\n(2)\nAs for Llama 3 architecture, a final LN is applied to the\nnetwork output. Pre-LN improves gradient flow during\nbackpropagation, stabilizing early training (Xiong et al.,\n2020). Nonetheless, in large-scale Transformers, even Pre-\nLN architectures are not immune to instability during train-\ning (Wortsman et al., 2024; Zhai et al., 2023). As shown\nin Figure 2, unlike Post-LN‚Äîwhich places LN at position\nC‚ÄîPre-LN, which places LN only at position A, can lead\nto a ‚Äúhighway‚Äù structure that is continuously maintained\nthroughout the entire model if the module produces an out-\nput with a large magnitude. This phenomenon might be\nrelated to the ‚Äúmassive activations‚Äù observed in trained mod-\nels (Sun et al., 2024; Fishman et al., 2024).\n3.2. Variance Behavior from Initialization to Training\nAs discussed by Xiong et al. (2020) and Kedia et al. (2024),\nTransformer models at initialization exhibit near-constant\nhidden-state variance under Post-LN and linearly increasing\nvariance under Pre-LN. Most of the previous studies have\nconcentrated on this early-stage behavior. However, Recent\n2Unless stated otherwise, LN refers to both LayerNorm (Ba\net al., 2016) and RMSNorm (Zhang & Sennrich, 2019).\n3We use ‚Äúhidden state‚Äù and ‚Äúactivation‚Äù interchangeably.\nA\nB\nC\nPost-LN\n√ó\n√ó\n‚úì\nPre-LN\n‚úì\n√ó\n√ó\nPeri-LN\n‚úì\n‚úì\n√ó\nFigure 2. Placement of normalization in Transformer sub-layer.\nstudies have also reported large output magnitudes in both\nthe pre-trained Attention and MLP modules (Dehghani et al.,\n2023; Wortsman et al., 2024; Fishman et al., 2024). To\nbridge the gap from initialization to the fully trained stage,\nwe extend our empirical observations in Figure 1 beyond\ninitial conditions by tracking how these variance trends\nevolve at intermediate points in training.\nDuring training, we find that Post-LN maintains a roughly\nconstant variance, which helps avert exploding activations.\nHowever, as models grow deeper and training progresses,\nconsistently normalizing xl + Module(xl) can weaken gra-\ndient flow, occasionally causing partial vanishing gradients\nand slower convergence. In contrast, Pre-LN normalizes xl\nbefore the module but leaves the module output unnormal-\nized, allowing hidden-state variance to accumulate exponen-\ntially once parameter updates amplify the input. Although\nPre-LN preserves gradients more effectively in earlier stages,\nthis exponential growth in variance can lead to ‚Äúmassive\nactivations‚Äù (Sun et al., 2024), risking numeric overflow and\ndestabilizing large-scale training.\nTakeaways from Pre-LN & Post-LN.\n(1) Keeping the\nHighway Clean: Post-LN‚Äôs Potential for Gradient Vanishing\nand Slow Convergence. When layer normalization is placed\ndirectly on the main path (Placement C in Figure 2), it can\ncause gradient vanishing and introduce fluctuations in the\ngradient scale, potentially leading to instability (Xiong et al.,\n2020). (2) Maintaining a Stable Highway: Pre-LN May Not\nSuffice for Training Stability. Pre-LN does not normalize\nthe main path of the hidden states, thereby avoiding the\nissues that Post-LN encounters. Nevertheless, a structural\ncharacteristic of Pre-LN is that any large values arising in\nthe Attention or MLP modules persist through the residual\nidentity path. In particular, as shown in Figure 1, the ex-\nponentially growing magnitude and variance of the hidden\nstates in the forward path may lead to numerical instability.\n3.3. Peri-Normalization in Transformers\nRecent open-source Transformer architectures have placed\nnormalization layers in unconventional placements (Rivi`ere\net al., 2024; Team et al., 2025; OLMo et al., 2024). In\nparticular, these models apply an additional normalization\n3\n\nPeri-LN: Revisiting Normalization Layer in the Transformer Architecture\nlayer at the module output (Output-LN), yet the benefits of\nthis design choice remain unclear. To assess the impact of\nOutput-LN, we analyze the Peri-LN architecture.\nPeri-LN.\nThe Peri-Layer Normalization (Peri-LN) applies\nLN twice within each layer‚Äîbefore and after the module‚Äî\nand further normalizes the input and final output embed-\ndings. Formally, for the hidden state xl at layer l:\n1. (Optional) Initial Embedding Normalization:\nyo = Norm(xo),\n2. Input- & Output-Normalization per Layer:\nyl = xl + Norm\n\u0010\nModule\n\u0000Norm(xl)\n\u0001\u0011\n,\n(3)\n3. Final Embedding Normalization:\nyL = Norm(xL),\nwhere xo denotes the output of the embedding layer, the hid-\nden input state. y0 represents the normalized input hidden\nstate. xL denotes the hidden state output by the final layer\nL of the Transformer sub-layer. This design unifies pre- and\noutput-normalization to regulate variance from both ends.\nFor clarity, the locations of normalization layers in the Post-,\nPre-, and Peri-LN architectures are illustrated in Figure 2.\nBoth the latest Gemma (Team et al., 2025; Rivi`ere et al.,\n2024) and OLMo (OLMo et al., 2024) model families,\nwhich apply output layer normalization, adopt the same\nperi-normalization strategy. However, neither line of work\nrigorously examines how this placement constrains variance\nor mitigates large residual activations. Our study extends\nthese open-sourced large-scale models by providing both\ntheoretical and empirical insights into the Peri-LN scheme.\nControlling Variance & Preserving Gradients.\nBy nor-\nmalizing both the input and output of each sub-layer, Peri-\nLN constrains the residual spikes commonly observed in\nPre-LN, while maintaining a stronger gradient pathway\nthan Post-LN. Concretely, if Norm(Module(Norm(xl)))\nexhibits near-constant variance Œ≤0, then\nVar(xl+1) ‚âàVar(xl) + Œ≤0,\n(4)\nresulting in linear or sub-exponential growth of activations,\nin contrast to the exponential growth patterns of Pre-LN.\nAlthough Pre-LN and Peri-LN exhibit comparable, roughly\nlinear variance growth at initialization (De & Smith, 2020;\nXie et al., 2023), their trajectories diverge once training\nbegins. The additional normalization layer (Output-LN)\nin Peri-LN preserves the conditions of Eq. 4, enabling the\nmodel‚Äôs hidden states to remain better conditioned. By\ncontrast, the rapid surge in variance observed in Pre-LN\ncan trigger instability during the early stages of training, an\neffect we quantify empirically in Sections 5 and 6.\n3.4. Stability Analysis in Normalization Strategies\nXiong et al. (2020) showed that, at initialization, Pre-LN\nexhibits smaller gradient scales at the final layer compared\nto Post-LN, with respect to model depth. In this study, we\nbroaden our analysis beyond initialization to monitor hidden\nstate variance over the full course of training. Building on\nthe earlier observation that the deepest layer exhibits the\nlargest activations, we focus on this surge in the final layer\nunder the Peri-LN strategy to clarify its impact on training\nstability. To this end, we analyze stability by examining\nthe gradient norm with respect to the final layer weights in\nthe presence of massive activation. Formal statements and\ndetailed proofs are presented in Appendix C.\nProposition 3.1 (Informal). Let L(¬∑) be the loss function,\nand let W (2) denote the weight of the last layer of MLP(¬∑).\nLet Œ≥ be the scaling parameter in Norm(¬∑), and let D be the\ndimension. Then, the gradient norm for each normalization\nstrategy behaves as follows.\n(1) Pre-LN (exploding gradient). Consider the following\nsequence of operations:\nÀúx = Norm(x), a = MLP(Àúx), o = x + a,\n(5)\nthen\n\r\r\r\r\r\n‚àÇL(o)\n‚àÇW (2)\ni,j\n\r\r\r\r\r ‚àù‚à•hi‚à•,\n(6)\nwhere h := ReLU\n\u0000ÀúxW (1) + b(1)\u0001\n. In this case, when\na massive activation ‚à•h‚à•occurs, an exploding gradient\n‚à•‚àÇL/‚àÇW (2)‚à•can arise, leading to training instability.\n(2) Peri-LN (self-regularizing gradient). Consider the fol-\nlowing sequence of operations:\nÀúx = Norm(x), a = MLP(Àúx), Àúa = Norm(a), o = x + Àúa,\n(7)\nthen\n\r\r\r\r\r\n‚àÇL(o)\n‚àÇW (2)\ni,j\n\r\r\r\r\r ‚â§4 Œ≥\n‚àö\nD ‚à•h‚à•\n‚à•a‚à•\n,\n(8)\nwhere h := ReLU\n\u0000ÀúxW (1) + b(1)\u0001\n. In this case, even when\na massive activation ‚à•h‚à•occurs, Norm(¬∑) introduces a\ndamping factor ‚à•a‚à•, which ensures that the gradient norm\n‚à•‚àÇL/‚àÇW (2)‚à•remains bounded.\nThe layer-wise amplification documented in ¬ß3.2, combined\nwith the bounds in Proposition 3.1, naturally explains the\n4\n\nPeri-LN: Revisiting Normalization Layer in the Transformer Architecture\n(a) Learning rate exploration\n(b) Training loss\n(c) Gradient-norm\nFigure 3. Performance comparison of Post-, Pre-, and Peri-LN Transformers during pre-training. Figure 3(a) llustrates the pre-training\nloss across learning rates. Pre-training loss and gradient norm of best performing 400M size Transformers are in Figure 3(b) & 3(c).\n(a) Divergence at seed 2\n(b) Loss spike at seed 3\n(c) Gradient spikes at seed 5\n(d) Loss spikes at seed 5\nFigure 4. Common case of early stage instability in pre-training. In most of our experiments across different random seeds, the Pre-LN\narchitecture exhibited early-stage instability. Although we initially suspected that a high learning rate might be the root cause, lowering it\ndid not substantially mitigate these issues. By contrast, under the same settings, Peri-LN displayed stable training curves.\ngradient spikes, and occasional divergences that arise in\nPre-LN during large-scale pre-training. We revisit this phe-\nnomenon in ¬ß4.3. By contrast, the additional normalization\nin Peri-LN acts as a self-regularizing mechanism that damps\nvariance growth, making the architecture less sensitive to\nlarge activations and therefore more stable in practice. The\nformal analysis for Post-LN is deferred to Appendix B.\n4. Experiments\nIn this section, we provide a comprehensive comparison\nof Post-, Pre-, and Peri-Layer Normalization (LN) across\nlarge-scale Transformer pre-training, instruction-tuning, and\nsubsequent evaluations on the language domain.\n4.1. Experimental Setting\nExcluding the embedding parameters, the model size is set\nto the parameters 400M, 1.5B and 3.2B, respectively. Each\nmodel is trained on 30 billion tokens. To ensure reliable\nvalidation, we pre-train each model with five different train-\ning seeds in all experiments. We perform a exploration of\nthe learning rates, ranging from 1 √ó 10‚àí4 to 5 √ó 10‚àí3 to\nidentify the U-shaped pattern for each LN strategy. The\nsequence length is set to 8192, and the weight decay co-\nefficient is fixed at 0.033. We employ Megatron-LM4 to\n4https://github.com/NVIDIA/Megatron-LM\npre-train the Transformers under each LN strategy. We use\nthe DCLM-baseline dataset (Li et al., 2024a), along with the\n‚Äúcl100k base‚Äù version of the TikToken tokenizer5. Unless\notherwise noted, most training and model configurations\nfollow those of the DCLM experiments(Li et al., 2024a).\nFor normalization layer, we primarily employ RMSNorm.\nFurther details are in Appendix D.\n4.2. Pre-Training Large Language Models\nFigure 3(a) illustrates the pre-training loss across learning\nrates for models ranging in size from 400M to 3.2B parame-\nters. Notably, the Peri-LN architecture consistently achieves\nsuperior loss curves over this entire model size. Since Pre-\nLN shows best performance at learning rate 2√ó10‚àí3 across\nall model size, we set this to the default learning rate for\nPre-LN and Peri-LN. Unlike Pre-LN, Post-LN‚Äôs appropriate\nlearning rate lies in a lower range, so we provide a separate\nsummary in Appendix E.1. In Figures 3(b) and 3(c), we\ncompare the pre-training loss and the gradient norm curve at\neach LN strategy‚Äôs best-performing learning rate of 400M\nsize models. The same trend is observed across different\nmodel sizes (¬ßE). In particular, when we sweep over train-\ning seeds and learning rates, Pre-LN frequently exhibits\nspikes in the gradient-norm curve, whereas Peri-LN shows\ncomparatively few, thereby supporting Proposition 3.1.\n5https://github.com/openai/tiktoken\n5\n\nPeri-LN: Revisiting Normalization Layer in the Transformer Architecture\nTable 1. Average benchmark scores (with standard deviations) across 5 different training seeds for Post-, Pre-, and Peri-Layer Normaliza-\ntion language models. Each model size excludes the embedding parameters. Loss denotes the evaluation loss on random samples of the\nC4 dataset. Arch. denotes architecture, and Avg. denotes the averaged benchmark score across tasks. SFT avg. denotes the averaged\nbenchmark score across tasks of instruction fine-tuned models. Diverged checkpoints are excluded from the evaluation score computation.\nSize\nArch.\nARC-Easy\nHellaSwag\nPIQA\nSIQA\nWinogrande\nAvg. ‚Üë\nLoss ‚Üì\nSFT Avg. ‚Üë\nPost-LN\n35.70 ¬±1.09\n28.91 ¬±0.16\n62.26 ¬±0.73\n34.48 ¬±1.04\n50.88 ¬±0.75\n42.45\n7.46\n46.44\n400M\nPre-LN\n54.87 ¬±1.63\n34.17 ¬±1.66\n68.79 ¬±1.34\n39.73 ¬±0.59\n50.88 ¬±2.35\n49.69\n3.43\n49.96\nPeri-LN\n57.51 ¬±0.81\n37.46 ¬±0.34\n69.48 ¬±0.39\n40.64 ¬±0.51\n52.74 ¬±0.67\n51.57\n3.34\n51.96\nPost-LN\n42.92 ¬±0.93\n31.69 ¬±0.41\n66.72 ¬±0.40\n35.84 ¬±0.61\n50.30 ¬±1.87\n45.49\n5.38\n48.95\n1.5B\nPre-LN\n61.51 ¬±1.22\n39.88 ¬±1.53\n71.41 ¬±0.88\n41.23 ¬±0.97\n54.51 ¬±2.07\n53.71\n3.29\n53.89\nPeri-LN\n66.17 ¬±0.21\n43.94 ¬±0.34\n73.63 ¬±0.24\n42.34 ¬±0.83\n56.64 ¬±0.44\n56.55\n3.18\n56.94\nPost-LN\n45.30 ¬±3.23\n33.59 ¬±0.44\n66.45 ¬±2.86\n35.82 ¬±1.09\n51.10 ¬±1.60\n46.45\n4.43\n49.33\n3.2B\nPre-LN\n65.24 ¬±2.32\n44.23 ¬±2.32\n73.86 ¬±1.19\n42.68 ¬±0.07\n57.42 ¬±2.51\n56.69\n3.20\n57.08\nPeri-LN\n68.73 ¬±0.57\n46.99 ¬±0.21\n74.31 ¬±0.41\n43.00 ¬±0.73\n59.76 ¬±0.78\n58.56\n3.11\n59.02\n(a) Gradient-norm at seed 5\n(b) Gradient-norm at seed 4\nFigure 5. Final-layer gradient norms for seeds 4 and 5.\n4.3. Early Stage Instability in Pre-Training\nEarly in pre-training, Pre-LN models consistently show gra-\ndient spikes, loss surges, and occasional divergence across\nseeds and scales (Fig. 4). These issues are far less pro-\nnounced in Peri-LN. We posit that the instability of Pre-LN\narises from three factors: (1) the hidden state variance ex-\nhibits a sudden surge from initialization through the early\nstages of optimization, deviating from the linear trend pre-\ndicted by Eq. 4 (see ¬ß3.3); (2) the exponential growth of\nhidden state variance across both depth and training steps;\nand (3) the instability caused by the massive activations\n(Proposition 3.1). Among these, we highlight the vari-\nance growth along the main path as the principal driver\nof the observed divergence. To corroborate this, Section 6\npresents targeted experiments that manipulate weight decay\nand weight initialization schemes, demonstrating how curb-\ning extreme variance mitigates the instability of each LN\nstrategy. The curves in 4(a), 4(b), and 4(c) are from a 400M\nmodel, whereas 4(d) corresponds to a 1.5B model.\n4.4. Gradient Norm of the Final-layer\nMotivated by Proposition 3.1, we track the final-layer\ngradient-norm in two representative runs selected from five\ntraining seeds. Figure 5(a), now including the newly added\nPeri-LN results, confirms the hierarchy reported by Xiong\net al. (2020): whenever training remains stable, the gradient\nnorms satisfy Post-LN > Pre-LN > Peri-LN. However, Fig-\nure 5(b) shows a run in which the Pre-LN model diverges\neven though every hyperparameter matches the stable run\nexcept for the random seed. In Section 5.1, we examine this\nfailure in greater depth and relate it to Proposition 3.1. The\ncurves in Figure 5 are obtained from 400M models.\n4.5. Benchmark Evaluations & Instruction Tuning\nTo evaluate how well the pre-training loss aligns with its\nbenchmark performance, we conduct five separate bench-\nmark evaluations. Furthermore, to investigate instruction-\nfollowing capabilities under different layer normalization\nstrategies, we conduct additional training using the LIMA\ndataset (Ouyang et al., 2022; Zhou et al., 2023). Diverged\ncheckpoints are excluded from the evaluation score compu-\ntation (mostly occurs in Pre-LN). Additional training hyper-\nparameters for SFT are given in Appendix D.2. As shown\nin Table 1, Peri-LN consistently demonstrates superior per-\nformance across all model sizes. Additionally, we note that,\nbeyond the improved scores, the standard deviation of the\nbenchmark results across different training seeds is reduced\nby more than half with Peri-LN. From this, we observe that\nPeri-LN helps maintain consistency not only in gradient\nstability and final loss but also in benchmark performance.\nFor the evaluation loss, we used 10K random samples from\nthe C4 dataset (Raffel et al., 2020). Detailed settings and\nindividual benchmark scores are provided in Appendix J.\n5. Analysis\nDespite emerging evidence that Peri-LN outperforms Post-\nand Pre-LN, key uncertainties remain: How do different\nLN placements shape hidden-state statistics and gradient\nflow (¬ß5.1, ¬ß5.2)? What role does the Output-LN scale\nparameter Œ≥ play (¬ß5.3)? And why does Peri-LN produce\nmore distinctive representations than its counterparts (¬ß5.4)?\nThe subsections below tackle these questions in turn.\n5.1. Growth of Hidden State\nTo examine in greater depth how Peri-LN affects forward\npropagation, we analyze the absolute magnitude and vari-\nance of the hidden states using 1, 000 samples from the\n6\n\nPeri-LN: Revisiting Normalization Layer in the Transformer Architecture\n(a) Absolute magnitude growth\n(b) Variance growth\nFigure 6. Forward hidden state growth patterns for each LN strategy in a 1.5B-parameter Transformer.\n(a) Grad-norm at init.\n(b) Grad-norm at final\n(c) Grad-variance at init.\n(d) Grad-variance at final\nFigure 7. Backward gradient norm and variance of 1.5B Post-, Pre-, and Peri-LN Transformers at initialization (init.) and final training.\nWikitext dataset (Merity et al., 2016). Figure 6 shows how\ndifferent normalization strategies influence forward-path\nhidden states over the course of training and across model\ndepth. We observe the same pattern across all models trained\nwith five different random seeds (¬ßF).\nAcross layers, Post-LN maintains stable hidden state magni-\ntudes and variances because the main path includes a nor-\nmalization layer. In contrast, Pre-LN omits normalization\nafter each attention and MLP sub-layer, so the magnitude\nand variance of the hidden states grow exponentially after\nthe residual addition. For Peri-LN, which adds an Output-\nLN, these statistics remain comparatively well controlled.\nAcross training iterations, Post-LN‚Äôs block-level normaliza-\ntion continues to suppress large shifts, preventing substantial\ndrift in magnitude or variance. Pre-LN starts with an approx-\nimately linear variance profile at initialization but escalates\nexponentially to extremely large values as optimization pro-\nceeds. Peri-LN again exhibits only moderate fluctuations,\nowing to Output-LN‚Äôs consistent regulation of hidden-state\nstatistics. Further discussion appears in Section 6.\n5.2. Layer-wise Gradient Norm & Variance\nEnsuring a uniform gradient flow in large-scale model train-\ning is crucial for balanced learning across the entire network\n(Yang & Hu, 2021; Yang et al., 2024). As shown in Figure 7,\nin Post-LN, gradients decrease as they propagate backward\nthrough the layers in the final stage of training, which can\nlead to vanishing gradients in lower-index layers. In Pre-LN,\ngradients increase as they propagate backward through the\nlayers at initialization, potentially causing explosive gradi-\nents in the early phase of training. Both strategies display\nnon-uniform gradient distributions‚Äîeither vanishing or ex-\nploding‚Äîat different stages of training. On the other hand,\nPeri-LN demonstrates a consistent, layer-wise gradient dis-\ntribution at both initialization and the end of training. By\nmaintaining comparatively uniform gradients with lower\nvariance across layers, Peri-LN avoids the extremes of van-\nishing or exploding behaviors. This stability is particularly\nbeneficial in deeper architectures, where balanced gradient\nflow is essential for effective backpropagation.\n5.3. Learnable Parameter Œ≥ of RMSNorm\nTo investigate the impact of module output normalization on\ntraining stability, as proposed in the Proposition 3.1, we fix\nthe learnable parameter Œ≥ of RMSNorm to 1, isolating the\neffect of normalization. As illustrated in Figure 8, adding\noutput normalization to each sub-layer suppresses gradi-\nent spikes and lowers the loss relative to Transformers that\nemploy only pre-normalization. Nonetheless, we also con-\nfirm that allowing Œ≥ to be learnable yields slightly better\nperformance. The trend persists consistently across model\nscales and random seeds. In this experiment, we omit Peri-\nLN‚Äôs embedding layer normalization in order to isolate and\nevaluate the precise role and benefits of output-LN.\n5.4. Hidden State Representation\nTo assess hidden state redundancy after training, we employ\nangular distance (Li et al., 2024b), which quantifies how sim-\n7\n\nPeri-LN: Revisiting Normalization Layer in the Transformer Architecture\n(a) Training loss\n(b) Loss in the final 5B token interval\n(c) Gradient-norm\nFigure 8. Freezing learnable parameter Œ≥ of output normalization layer in Peri-LN. we set Œ≥ to its initial value of 1 and keep it fixed.\n(a) After 30B tokens training\n(b) Learnable scale Œ≥ in Output-LN\nFigure 9. Angular distance between hidden states after training. Fig. 9(b) monitor Œ≥ of every Output-LN in Peri-LN during training.\nilar or distinct the layer representations are. As Figure 9(a)\nillustrates, Pre-LN produces markedly more redundant hid-\nden states than the other variants by the end of training.\nWe attribute this effect to the exponential growth of the\nmain residual path in Pre-LN, which diminishes the relative\ncontribution of individual sub-layers. In contrast, Peri-LN\nretains an identity path whose learnable scale begins near 1\nand gradually adjusts with depth (Figure 9(b)), thereby mod-\nerating redundancy. These observations highlight the role\nof module-output normalization in controlling hidden state\nsimilarity. All statistics are computed on 256 random sam-\nples from RedPajama-Data-1T (Computer, 2023). Appendix\nL includes additional figures and initialization comparisons.\n6. Ablation Study\nTo probe massive activations across conditions, we sweep\nweight decay coefficient and initialization variance for both\nPre- and Peri-LN models, holding other settings fixed. Per-\nrun results and detailed settings are in Appendix G.8.\n6.1. Weight Decay\nIn Figure 10(a), stronger L2 regularization markedly lowers\nthe variance curve, confirming that heavier weight decay\ndirectly curbs forward-path explosions in Pre-LN. In con-\ntrast, the same increase in weight decay reduces Peri-LN‚Äôs\nvariance growth only marginally. We take the stable run\ninitialized with seed 3 (Table 11) and sweep the weight de-\ncay coefficient. Table 7 in the Appendix further shows that,\nirrespective of the presence of massive activations, Peri-LN\nachieves better performance than Pre-LN.\nTo further probe stability under varying degrees of massive\nactivation, we replicate the previously divergent run (seed\n4) and repeat the same weight decay sweep. As Figure\n10(b) demonstrates, raising the weight decay coefficient\nfrom the baseline 0.033 to 0.33 (a tenfold rise) prevents\ndivergence, providing empirical support for Proposition 3.1.\nNevertheless, strong weight decay can stabilize Pre-LN, it\nstill fails to close the performance gap relative to Peri-LN.\n6.2. Weight Initialization\nAs the initialization variance increases, the severity of mas-\nsive activations rises correspondingly for Pre-LN (Figure\n10(c)); at the largest variance, the model diverges outright\n(Appendix G.8.3). Pre-LN therefore displays marked sensi-\ntivity to its initial conditions. In contrast, across the same\nrange of ablations, Peri-LN‚Äôs loss curves and activation vari-\nances shift only marginally. This hyperparameter-insensitive\nrobustness is corroborated by the low downstream standard\ndeviations reported in Table 1.\n6.3. Additional Results\nFor brevity, we defer an extensive set of supplementary ex-\nperiments to the appendix. Appendix G reports the core\nrobustness checks: replacing RMSNorm with LayerNorm,\nvarying sequence lengths, reducing pre-training budgets,\nand ablating embedding-level normalization. OLMo2-style\n8\n\nPeri-LN: Revisiting Normalization Layer in the Transformer Architecture\n(a) Variance growth by weight decay\n(b) Training loss by weight decay\n(c) Variance growth by weight init.\nFigure 10. Effects of weight decay and initialization (init.) on massive activations. 10(a) Strong weight decay relieves the variance\nexplosion in Pre-LN. 10(b) Strong weight decay (0.33, which is 10√ó the baseline.) suppresses Pre-LN divergence. 10(c) Smaller-scale\ninitialization slightly curbs Pre-LN variance, while Peri-LN remains bounded regardless. d denotes the model‚Äôs hidden dimensionality.\nPeri-LN pre-training runs appear in Appendix H. Stochastic\ngradient descent (SGD) baselines are summarized in Ap-\npendix I. Alternative LN placements, extending Figure 2,\nare provided in Appendix G.6. Across all settings, the re-\nsults are consistent with the trends presented in the main\nSection 4 and 5, further substantiating our conclusions.\n7. Implications\nThis section integrates our findings into practical guidance\non variance-driven stability and precision constraints in\nlarge-scale Transformers.\n7.1. Mitigating Variance-Driven Instability via Peri-LN\nPre-LN, the prevailing normalization strategy, is inherently\nprone to unchecked growth in activation variance (¬ß5),\nwhich in turn induces numerical instability during train-\ning. Our extensive empirical analysis shows that Peri-LN‚Äî\nwhich normalizes the outputs of the Attention and MLP\nsub-layers‚Äîmarkedly curbs this variance and often pre-\nvents divergence (¬ß4 & ¬ß6). Proposition 3.1 formalizes how\nexcessive variance amplifies gradient norms, clarifying its\ncausal role in destabilizing large-scale pre-training. In Pre-\nLN, instability is further exacerbated when the statistical\nconditions assumed at initialization depart markedly from\nthose observed later in training (¬ß3.3). By contrast, Peri-\nLN alleviates this discrepancy, thereby improving training\nstability, and delivering additional performance gains.\n7.2. Precision Constraints Imposed by Pre-LN\nBoth Pre-LN and Peri-LN architectures leave the main hid-\nden state path unnormalized, so once large values arise in\nearlier layers, they persist through to later layers. Conse-\nquently, Pre-LN‚Äôs additive residual path might generates ac-\ntivations near or beyond the FP16 limit. To gauge how often\nthese values exceed FP16 yet remain within BF16, we track\nthe top-100 absolute hidden state values for 3.2B-parameter\nPre-LN and Peri-LN models. In Figure 11, the blue band\nFigure 11. Evolution of extreme hidden state absolute magnitudes\nduring training. Colored bands trace the range of the global top-\n100 absolute activations. Pre-LN (blue) quickly surpasses the FP16\nrepresentable maximum, while Peri-LN (red) stays below it.\nfor Pre-LN surpasses the FP16 maximum bound as early\nas 0.5B training tokens whereas Peri-LN (red band) consis-\ntently remains below this threshold. This pattern, echoing\nSun et al. (2024), highlights that choosing FP16 or BF16 is\nnot just a hardware preference but is closely linked to how\nhidden state magnitudes evolve within the model. Earlier\nwork on OPT (Zhang et al., 2022), which was pre-trained\nusing FP16 precision, suggests that the training instabili-\nties they observed were likely exacerbated by numerical\noverflows and gradient pathologies (Proposition 3.1) arising\nwhen activations exceeded the representable range of FP16.\n8. Conclusion\nWe explore the placement of layer normalization within\nthe Transformer architecture to better understand its role\nduring training. By systematically comparing Post-LN, Pre-\nLN, and newly termed Peri-LN, we highlight their distinct\nimpacts on stability, final performance, and optimization\ndynamics. Our findings suggest that placing LN on module\noutputs in addition to the Pre-LN can help manage large ac-\ntivations while preserving beneficial gradient flow, thereby\noffering a promising balance for stable optimization. By\nunifying these approaches under the term Peri-LN, we seek\nto consolidate existing variants and encourage deeper inves-\ntigation into this underexplored alternative.\n9\n\nPeri-LN: Revisiting Normalization Layer in the Transformer Architecture\nAcknowledgements\nWe thank our colleague Jeongin Bae for inspiring the un-\nderlying motivation for this research. We are also grateful\nto Jung Hyun Lee, Seonghyeon Kim, and Seunghyun Seo\nfor their valuable assistance during the early stages discus-\nsions. Finally, we extend our gratitude to Gichang Lee,\nLead of the Backbone Mission at NAVER Cloud, for his\nunwavering support. This work was partly supported by\nInstitute for Information & communications Technology\nTechnology Planning & Evaluation(IITP) grant funded by\nthe Korea government(MSIT)(RS-2019-II190075, Artificial\nIntelligence Graduate School Support Program(KAIST),\nNo.RS-2021-II212068, Artificial Intelligence Innovation\nHub, No. RS-2024-00509279, Global AI Frontier Lab)\nImpact Statement\nThe rapid advancement of Transformer-based large lan-\nguage models (LLMs) has enabled remarkable break-\nthroughs in natural language understanding and generation.\nHowever, these models also pose significant challenges, in-\ncluding concerns around safety, bias, and the computational\ncost associated with large-scale training. As LLMs become\nincreasingly integral to various AI applications, ensuring\ntheir stability, efficiency, and accessibility remains a critical\nresearch focus.\nOur work addresses these challenges by proposing a more\nstable and cost-effective large-scale training methodology.\nBy improving training efficiency and reducing the associ-\nated computational overhead, we lower the barrier to entry\nfor organizations seeking to develop or fine-tune foundation\nmodels. This democratization of LLM technology fosters\nbroader participation in AI research and development, accel-\nerating innovation while mitigating concerns over resource\nconcentration in a few major players. Given the growing\nindustry focus on optimizing LLM deployment costs, our\ncontributions are particularly relevant in the current AI re-\nsearch landscape.\nImproving the cost-effectiveness of large-scale training si-\nmultaneously lowers AI‚Äôs environmental footprint by reduc-\ning the vast energy consumption and carbon emissions inher-\nent in state-of-the-art LLM development. This efficiency not\nonly aligns with global sustainability goals but also enables\nsmaller research labs and academic groups to pursue cutting-\nedge AI without prohibitive resource demands, fostering a\nmore inclusive and responsible ecosystem.\nReferences\nBa, L. J., Kiros, J. R., and Hinton, G. E. Layer normalization.\nCoRR, abs/1607.06450, 2016. URL http://arxiv.\norg/abs/1607.06450.\nBisk, Y., Zellers, R., Gao, J., Choi, Y., et al. Piqa: Reasoning\nabout physical commonsense in natural language. In Pro-\nceedings of the AAAI conference on artificial intelligence,\nvolume 34, pp. 7432‚Äì7439, 2020.\nBrock, A., De, S., Smith, S. L., and Simonyan, K. High-\nperformance large-scale image recognition without nor-\nmalization.\nIn Meila, M. and Zhang, T. (eds.), Pro-\nceedings of the 38th International Conference on Ma-\nchine Learning, ICML 2021, 18-24 July 2021, Virtual\nEvent, volume 139 of Proceedings of Machine Learn-\ning Research, pp. 1059‚Äì1071. PMLR, 2021a.\nURL\nhttp://proceedings.mlr.press/v139/b\nrock21a.html.\nBrock, A., De, S., Smith, S. L., and Simonyan, K. High-\nperformance large-scale image recognition without nor-\nmalization.\nIn Meila, M. and Zhang, T. (eds.), Pro-\nceedings of the 38th International Conference on Ma-\nchine Learning, ICML 2021, 18-24 July 2021, Virtual\nEvent, volume 139 of Proceedings of Machine Learn-\ning Research, pp. 1059‚Äì1071. PMLR, 2021b.\nURL\nhttp://proceedings.mlr.press/v139/b\nrock21a.html.\nChung, W., Hong, J., An, N. M., Thorne, J., and Yun, S. Sta-\nble language model pre-training by reducing embedding\nvariability. In Al-Onaizan, Y., Bansal, M., and Chen, Y.\n(eds.), Proceedings of the 2024 Conference on Empirical\nMethods in Natural Language Processing, EMNLP 2024,\nMiami, FL, USA, November 12-16, 2024, pp. 10852‚Äì\n10863. Association for Computational Linguistics, 2024.\nURL https://aclanthology.org/2024.em\nnlp-main.606.\nClark, P., Cowhey, I., Etzioni, O., Khot, T., Sabharwal, A.,\nSchoenick, C., and Tafjord, O. Think you have solved\nquestion answering? try arc, the ai2 reasoning challenge,\n2018. URL https://arxiv.org/abs/1803.0\n5457.\nComputer, T. Redpajama: An open source recipe to re-\nproduce llama training dataset, 2023. URL https:\n//github.com/togethercomputer/RedPaj\nama-Data.\nCsord¬¥as, R., Irie, K., Schmidhuber, J., Potts, C., and\nManning, C. D.\nMoeut: Mixture-of-experts univer-\nsal transformers. CoRR, abs/2405.16039, 2024. doi:\n10.48550/ARXIV.2405.16039. URL https://doi.\norg/10.48550/arXiv.2405.16039.\n10\n\nPeri-LN: Revisiting Normalization Layer in the Transformer Architecture\nDe, S. and Smith, S. L. Batch normalization biases resid-\nual blocks towards the identity function in deep net-\nworks.\nIn Larochelle, H., Ranzato, M., Hadsell, R.,\nBalcan, M., and Lin, H. (eds.), Advances in Neural\nInformation Processing Systems 33: Annual Confer-\nence on Neural Information Processing Systems 2020,\nNeurIPS 2020, December 6-12, 2020, virtual, 2020. URL\nhttps://proceedings.neurips.cc/paper\n/2020/hash/e6b738eca0e6792ba8a9cbcba\n6c1881d-Abstract.html.\nDehghani, M., Djolonga, J., Mustafa, B., Padlewski, P.,\nHeek, J., Gilmer, J., Steiner, A. P., Caron, M., Geirhos,\nR., Alabdulmohsin, I., Jenatton, R., Beyer, L., Tschan-\nnen, M., Arnab, A., Wang, X., Ruiz, C. R., Minderer, M.,\nPuigcerver, J., Evci, U., Kumar, M., van Steenkiste, S.,\nElsayed, G. F., Mahendran, A., Yu, F., Oliver, A., Huot,\nF., Bastings, J., Collier, M., Gritsenko, A. A., Birodkar,\nV., Vasconcelos, C. N., Tay, Y., Mensink, T., Kolesnikov,\nA., Pavetic, F., Tran, D., Kipf, T., Lucic, M., Zhai, X.,\nKeysers, D., Harmsen, J. J., and Houlsby, N. Scaling\nvision transformers to 22 billion parameters. In Krause,\nA., Brunskill, E., Cho, K., Engelhardt, B., Sabato, S.,\nand Scarlett, J. (eds.), International Conference on Ma-\nchine Learning, ICML 2023, 23-29 July 2023, Honolulu,\nHawaii, USA, volume 202 of Proceedings of Machine\nLearning Research, pp. 7480‚Äì7512. PMLR, 2023. URL\nhttps://proceedings.mlr.press/v202/d\nehghani23a.html.\nDettmers, T., Lewis, M., Belkada, Y., and Zettlemoyer,\nL.\nGpt3.int8(): 8-bit matrix multiplication for trans-\nformers at scale. In Koyejo, S., Mohamed, S., Agar-\nwal, A., Belgrave, D., Cho, K., and Oh, A. (eds.), Ad-\nvances in Neural Information Processing Systems 35:\nAnnual Conference on Neural Information Processing\nSystems 2022, NeurIPS 2022, New Orleans, LA, USA,\nNovember 28 - December 9, 2022, 2022. URL http:\n//papers.nips.cc/paper_files/paper/2\n022/hash/c3ba4962c05c49636d4c6206a97\ne9c8a-Abstract-Conference.html.\nDing, M., Yang, Z., Hong, W., Zheng, W., Zhou, C., Yin,\nD., Lin, J., Zou, X., Shao, Z., Yang, H., et al. Cogview:\nMastering text-to-image generation via transformers. Ad-\nvances in neural information processing systems, 34:\n19822‚Äì19835, 2021.\nDubey, A., Jauhri, A., Pandey, A., Kadian, A., Al-Dahle,\nA., Letman, A., Mathur, A., Schelten, A., Yang, A., Fan,\nA., et al. The llama 3 herd of models. arXiv preprint\narXiv:2407.21783, 2024.\nFishman, M., Chmiel, B., Banner, R., and Soudry, D.\nScaling FP8 training to trillion-token llms.\nCoRR,\nabs/2409.12517, 2024. doi: 10.48550/ARXIV.2409.\n12517. URL https://doi.org/10.48550/arX\niv.2409.12517.\nGao, L., Tow, J., Abbasi, B., Biderman, S., Black, S., DiPofi,\nA., Foster, C., Golding, L., Hsu, J., Le Noac‚Äôh, A., Li, H.,\nMcDonell, K., Muennighoff, N., Ociepa, C., Phang, J.,\nReynolds, L., Schoelkopf, H., Skowron, A., Sutawika, L.,\nTang, E., Thite, A., Wang, B., Wang, K., and Zou, A. A\nframework for few-shot language model evaluation, 07\n2024. URL https://zenodo.org/records/1\n2608602.\nGlorot, X. and Bengio, Y. Understanding the difficulty of\ntraining deep feedforward neural networks. In Teh, Y. W.\nand Titterington, D. M. (eds.), Proceedings of the Thir-\nteenth International Conference on Artificial Intelligence\nand Statistics, AISTATS 2010, Chia Laguna Resort, Sar-\ndinia, Italy, May 13-15, 2010, volume 9 of JMLR Proceed-\nings, pp. 249‚Äì256. JMLR.org, 2010. URL http://pr\noceedings.mlr.press/v9/glorot10a.html.\nGuo, D., Yang, D., Zhang, H., Song, J., Zhang, R., Xu, R.,\nZhu, Q., Ma, S., Wang, P., Bi, X., et al. Deepseek-r1: In-\ncentivizing reasoning capability in llms via reinforcement\nlearning. arXiv preprint arXiv:2501.12948, 2025.\nHe, K., Zhang, X., Ren, S., and Sun, J. Delving deep\ninto rectifiers: Surpassing human-level performance on\nimagenet classification. In 2015 IEEE International Con-\nference on Computer Vision, ICCV 2015, Santiago, Chile,\nDecember 7-13, 2015, pp. 1026‚Äì1034. IEEE Computer\nSociety, 2015. doi: 10.1109/ICCV.2015.123. URL\nhttps://doi.org/10.1109/ICCV.2015.123.\nHe, K., Zhang, X., Ren, S., and Sun, J. Identity mappings in\ndeep residual networks. In Leibe, B., Matas, J., Sebe, N.,\nand Welling, M. (eds.), Computer Vision - ECCV 2016 -\n14th European Conference, Amsterdam, The Netherlands,\nOctober 11-14, 2016, Proceedings, Part IV, volume 9908\nof Lecture Notes in Computer Science, pp. 630‚Äì645.\nSpringer, 2016. doi: 10.1007/978-3-319-46493-0\\ 38.\nURL https://doi.org/10.1007/978-3-319\n-46493-0_38.\nHeo, J. H., Kim, J., Kwon, B., Kim, B., Kwon, S. J., and\nLee, D. Rethinking channel dimensions to isolate outliers\nfor low-bit weight quantization of large language models.\nIn The Twelfth International Conference on Learning\nRepresentations, ICLR 2024, Vienna, Austria, May 7-11,\n2024. OpenReview.net, 2024. URL https://openre\nview.net/forum?id=JzG7kSpjJk.\nHoffmann, J., Borgeaud, S., Mensch, A., Buchatskaya, E.,\nCai, T., Rutherford, E., de Las Casas, D., Hendricks,\nL. A., Welbl, J., Clark, A., Hennigan, T., Noland, E.,\nMillican, K., van den Driessche, G., Damoc, B., Guy,\nA., Osindero, S., Simonyan, K., Elsen, E., Rae, J. W.,\n11\n\nPeri-LN: Revisiting Normalization Layer in the Transformer Architecture\nVinyals, O., and Sifre, L.\nTraining compute-optimal\nlarge language models. CoRR, abs/2203.15556, 2022.\ndoi: 10.48550/ARXIV.2203.15556. URL https:\n//doi.org/10.48550/arXiv.2203.15556.\nKedia, A., Zaidi, M. A., Khyalia, S., Jung, J., Goka, H., and\nLee, H. Transformers get stable: An end-to-end signal\npropagation theory for language models. In Forty-first\nInternational Conference on Machine Learning, ICML\n2024, Vienna, Austria, July 21-27, 2024. OpenReview.net,\n2024. URL https://openreview.net/forum\n?id=30waYPIZUA.\nKim, J., Lee, J. H., Kim, S., Park, J., Yoo, K. M., Kwon,\nS. J., and Lee, D. Memory-efficient fine-tuning of com-\npressed large language models via sub-4-bit integer quan-\ntization. In Oh, A., Naumann, T., Globerson, A., Saenko,\nK., Hardt, M., and Levine, S. (eds.), Advances in Neu-\nral Information Processing Systems 36: Annual Confer-\nence on Neural Information Processing Systems 2023,\nNeurIPS 2023, New Orleans, LA, USA, December 10 -\n16, 2023, 2023. URL http://papers.nips.cc/p\naper_files/paper/2023/hash/7183f4fc8\n7598f6c6e947b96714acbd6-Abstract-Con\nference.html.\nLee, J. H., Kim, J., Kwon, S. J., and Lee, D. Flexround:\nLearnable rounding based on element-wise division for\npost-training quantization. In Krause, A., Brunskill, E.,\nCho, K., Engelhardt, B., Sabato, S., and Scarlett, J. (eds.),\nInternational Conference on Machine Learning, ICML\n2023, 23-29 July 2023, Honolulu, Hawaii, USA, volume\n202 of Proceedings of Machine Learning Research, pp.\n18913‚Äì18939. PMLR, 2023. URL https://procee\ndings.mlr.press/v202/lee23h.html.\nLee, J. H., Kim, J., Yang, J. Y., Kwon, S. J., Yang, E., Yoo,\nK. M., and Lee, D. LRQ: optimizing post-training quan-\ntization for large language models by learning low-rank\nweight-scaling matrices. In Chiruzzo, L., Ritter, A., and\nWang, L. (eds.), Proceedings of the 2025 Conference\nof the Nations of the Americas Chapter of the Associa-\ntion for Computational Linguistics: Human Language\nTechnologies, NAACL 2025 - Volume 1: Long Papers, Al-\nbuquerque, New Mexico, USA, April 29 - May 4, 2025, pp.\n7708‚Äì7743. Association for Computational Linguistics,\n2025. URL https://aclanthology.org/2025.\nnaacl-long.393/.\nLi, J., Fang, A., Smyrnis, G., Ivgi, M., Jordan, M., Gadre,\nS. Y., Bansal, H., Guha, E. K., Keh, S., Arora, K., Garg,\nS., Xin, R., Muennighoff, N., Heckel, R., Mercat, J.,\nChen, M., Gururangan, S., Wortsman, M., Albalak, A.,\nBitton, Y., Nezhurina, M., Abbas, A., Hsieh, C., Ghosh,\nD., Gardner, J., Kilian, M., Zhang, H., Shao, R., Pratt,\nS. M., Sanyal, S., Ilharco, G., Daras, G., Marathe, K.,\nGokaslan, A., Zhang, J., Chandu, K. R., Nguyen, T.,\nVasiljevic, I., Kakade, S. M., Song, S., Sanghavi, S.,\nFaghri, F., Oh, S., Zettlemoyer, L., Lo, K., El-Nouby, A.,\nPouransari, H., Toshev, A., Wang, S., Groeneveld, D.,\nSoldaini, L., Koh, P. W., Jitsev, J., Kollar, T., Dimakis,\nA. G., Carmon, Y., Dave, A., Schmidt, L., and Shankar, V.\nDatacomp-lm: In search of the next generation of training\nsets for language models. CoRR, abs/2406.11794, 2024a.\ndoi: 10.48550/ARXIV.2406.11794. URL https:\n//doi.org/10.48550/arXiv.2406.11794.\nLi, P., Yin, L., and Liu, S. Mix-ln: Unleashing the power\nof deeper layers by combining pre-ln and post-ln. arXiv\npreprint arXiv:2412.13795, 2024b.\nLoshchilov, I., Hsieh, C., Sun, S., and Ginsburg, B. ngpt:\nNormalized transformer with representation learning on\nthe hypersphere.\nCoRR, abs/2410.01131, 2024.\ndoi:\n10.48550/ARXIV.2410.01131. URL https://doi.\norg/10.48550/arXiv.2410.01131.\nMerity, S., Xiong, C., Bradbury, J., and Socher, R. Pointer\nsentinel mixture models, 2016.\nMihaylov, T., Clark, P., Khot, T., and Sabharwal, A. Can a\nsuit of armor conduct electricity? a new dataset for open\nbook question answering. In EMNLP, 2018.\nOLMo, T., Walsh, P., Soldaini, L., Groeneveld, D., Lo, K.,\nArora, S., Bhagia, A., Gu, Y., Huang, S., Jordan, M.,\net al. 2 olmo 2 furious. arXiv preprint arXiv:2501.00656,\n2024.\nOuyang, L., Wu, J., Jiang, X., Almeida, D., Wainwright,\nC. L., Mishkin, P., Zhang, C., Agarwal, S., Slama, K.,\nRay, A., Schulman, J., Hilton, J., Kelton, F., Miller, L.,\nSimens, M., Askell, A., Welinder, P., Christiano, P. F.,\nLeike, J., and Lowe, R. Training language models to\nfollow instructions with human feedback. In Koyejo, S.,\nMohamed, S., Agarwal, A., Belgrave, D., Cho, K., and\nOh, A. (eds.), Advances in Neural Information Processing\nSystems 35: Annual Conference on Neural Information\nProcessing Systems 2022, NeurIPS 2022, New Orleans,\nLA, USA, November 28 - December 9, 2022, 2022. URL\nhttp://papers.nips.cc/paper_files/pap\ner/2022/hash/b1efde53be364a73914f588\n05a001731-Abstract-Conference.html.\nRaffel, C., Shazeer, N., Roberts, A., Lee, K., Narang, S.,\nMatena, M., Zhou, Y., Li, W., and Liu, P. J. Exploring\nthe limits of transfer learning with a unified text-to-text\ntransformer. The Journal of Machine Learning Research,\n21(1):5485‚Äì5551, 2020.\nRivi`ere, M., Pathak, S., Sessa, P. G., Hardin, C., Bhupati-\nraju, S., Hussenot, L., Mesnard, T., Shahriari, B., Ram¬¥e,\nA., Ferret, J., Liu, P., Tafti, P., Friesen, A., Casbon, M.,\n12\n\nPeri-LN: Revisiting Normalization Layer in the Transformer Architecture\nRamos, S., Kumar, R., Lan, C. L., Jerome, S., Tsitsulin,\nA., Vieillard, N., Stanczyk, P., Girgin, S., Momchev,\nN., Hoffman, M., Thakoor, S., Grill, J., Neyshabur, B.,\nBachem, O., Walton, A., Severyn, A., Parrish, A., Ahmad,\nA., Hutchison, A., Abdagic, A., Carl, A., Shen, A., Brock,\nA., Coenen, A., Laforge, A., Paterson, A., Bastian, B.,\nPiot, B., Wu, B., Royal, B., Chen, C., Kumar, C., Perry,\nC., Welty, C., Choquette-Choo, C. A., Sinopalnikov, D.,\nWeinberger, D., Vijaykumar, D., Rogozinska, D., Her-\nbison, D., Bandy, E., Wang, E., Noland, E., Moreira,\nE., Senter, E., Eltyshev, E., Visin, F., Rasskin, G., Wei,\nG., Cameron, G., Martins, G., Hashemi, H., Klimczak-\nPlucinska, H., Batra, H., Dhand, H., Nardini, I., Mein,\nJ., Zhou, J., Svensson, J., Stanway, J., Chan, J., Zhou,\nJ. P., Carrasqueira, J., Iljazi, J., Becker, J., Fernandez, J.,\nvan Amersfoort, J., Gordon, J., Lipschultz, J., Newlan,\nJ., Ji, J., Mohamed, K., Badola, K., Black, K., Millican,\nK., McDonell, K., Nguyen, K., Sodhia, K., Greene, K.,\nSj¬®osund, L. L., Usui, L., Sifre, L., Heuermann, L., Lago,\nL., and McNealus, L. Gemma 2: Improving open lan-\nguage models at a practical size. CoRR, abs/2408.00118,\n2024. doi: 10.48550/ARXIV.2408.00118. URL https:\n//doi.org/10.48550/arXiv.2408.00118.\nSakaguchi, K., Bras, R. L., Bhagavatula, C., and Choi, Y.\nWinogrande: an adversarial winograd schema challenge\nat scale. Commun. ACM, 64(9):99‚Äì106, August 2021.\nISSN 0001-0782. doi: 10.1145/3474381. URL https:\n//doi.org/10.1145/3474381.\nSap, M., Rashkin, H., Chen, D., Bras, R. L., and Choi, Y.\nSocial iqa: Commonsense reasoning about social interac-\ntions. In Inui, K., Jiang, J., Ng, V., and Wan, X. (eds.),\nProceedings of the 2019 Conference on Empirical Meth-\nods in Natural Language Processing and the 9th Interna-\ntional Joint Conference on Natural Language Processing,\nEMNLP-IJCNLP 2019, Hong Kong, China, November\n3-7, 2019, pp. 4462‚Äì4472. Association for Computational\nLinguistics, 2019. doi: 10.18653/V1/D19-1454. URL\nhttps://doi.org/10.18653/v1/D19-1454.\nSun, M., Chen, X., Kolter, J. Z., and Liu, Z. Massive activa-\ntions in large language models. CoRR, abs/2402.17762,\n2024. doi: 10.48550/ARXIV.2402.17762. URL https:\n//doi.org/10.48550/arXiv.2402.17762.\nTakase, S., Kiyono, S., Kobayashi, S., and Suzuki, J. Spike\nno more: Stabilizing the pre-training of large language\nmodels. CoRR, abs/2312.16903, 2023. doi: 10.48550/A\nRXIV.2312.16903. URL https://doi.org/10.4\n8550/arXiv.2312.16903.\nTalmor, A., Herzig, J., Lourie, N., and Berant, J. Common-\nsenseQA: A question answering challenge targeting com-\nmonsense knowledge. In Proceedings of the 2019 Confer-\nence of the North American Chapter of the Association for\nComputational Linguistics: Human Language Technolo-\ngies, Volume 1 (Long and Short Papers), pp. 4149‚Äì4158,\nMinneapolis, Minnesota, June 2019. Association for\nComputational Linguistics. doi: 10.18653/v1/N19-1421.\nURL https://aclanthology.org/N19-1421.\nTeam, G., Kamath, A., Ferret, J., Pathak, S., Vieillard,\nN., Merhej, R., Perrin, S., Matejovicova, T., Ram¬¥e, A.,\nRivi`ere, M., et al. Gemma 3 technical report. arXiv\npreprint arXiv:2503.19786, 2025.\nVaswani, A., Shazeer, N., Parmar, N., Uszkoreit, J., Jones,\nL., Gomez, A. N., Kaiser, L., and Polosukhin, I. Attention\nis all you need. In Guyon, I., von Luxburg, U., Bengio,\nS., Wallach, H. M., Fergus, R., Vishwanathan, S. V. N.,\nand Garnett, R. (eds.), Advances in Neural Information\nProcessing Systems 30: Annual Conference on Neural\nInformation Processing Systems 2017, December 4-9,\n2017, Long Beach, CA, USA, pp. 5998‚Äì6008, 2017. URL\nhttps://proceedings.neurips.cc/paper\n/2017/hash/3f5ee243547dee91fbd053c1c\n4a845aa-Abstract.html.\nWang, A., Singh, A., Michael, J., Hill, F., Levy, O., and Bow-\nman, S. GLUE: A multi-task benchmark and analysis plat-\nform for natural language understanding. In Proceedings\nof the 2018 EMNLP Workshop BlackboxNLP: Analyz-\ning and Interpreting Neural Networks for NLP, pp. 353‚Äì\n355, Brussels, Belgium, November 2018. Association for\nComputational Linguistics. doi: 10.18653/v1/W18-5446.\nURL https://aclanthology.org/W18-5446.\nWolf, T., Debut, L., Sanh, V., Chaumond, J., Delangue, C.,\nMoi, A., Cistac, P., Rault, T., Louf, R., Funtowicz, M.,\nDavison, J., Shleifer, S., von Platen, P., Ma, C., Jernite,\nY., Plu, J., Xu, C., Scao, T. L., Gugger, S., Drame, M.,\nLhoest, Q., and Rush, A. M. Transformers: State-of-\nthe-art natural language processing. In Proceedings of\nthe 2020 Conference on Empirical Methods in Natural\nLanguage Processing: System Demonstrations, pp. 38‚Äì\n45, Online, October 2020. Association for Computational\nLinguistics. URL https://www.aclweb.org/a\nnthology/2020.emnlp-demos.6.\nWortsman, M., Liu, P. J., Xiao, L., Everett, K. E., Alemi,\nA. A., Adlam, B., Co-Reyes, J. D., Gur, I., Kumar, A.,\nNovak, R., Pennington, J., Sohl-Dickstein, J., Xu, K.,\nLee, J., Gilmer, J., and Kornblith, S. Small-scale proxies\nfor large-scale transformer training instabilities. In The\nTwelfth International Conference on Learning Represen-\ntations, ICLR 2024, Vienna, Austria, May 7-11, 2024.\nOpenReview.net, 2024. URL https://openreview\n.net/forum?id=d8w0pmvXbZ.\nXie, S., Zhang, H., Guo, J., Tan, X., Bian, J., Awadalla,\nH. H., Menezes, A., Qin, T., and Yan, R.\nResidual:\n13\n\nPeri-LN: Revisiting Normalization Layer in the Transformer Architecture\nTransformer with dual residual connections.\nCoRR,\nabs/2304.14802, 2023. doi: 10.48550/ARXIV.2304.\n14802. URL https://doi.org/10.48550/arX\niv.2304.14802.\nXiong, R., Yang, Y., He, D., Zheng, K., Zheng, S., Xing, C.,\nZhang, H., Lan, Y., Wang, L., and Liu, T. On layer nor-\nmalization in the transformer architecture. In Proceedings\nof the 37th International Conference on Machine Learn-\ning, ICML 2020, 13-18 July 2020, Virtual Event, volume\n119 of Proceedings of Machine Learning Research, pp.\n10524‚Äì10533. PMLR, 2020. URL http://procee\ndings.mlr.press/v119/xiong20b.html.\nYang, G. and Hu, E. J. Tensor programs IV: feature learn-\ning in infinite-width neural networks. In Meila, M. and\nZhang, T. (eds.), Proceedings of the 38th International\nConference on Machine Learning, ICML 2021, 18-24\nJuly 2021, Virtual Event, volume 139 of Proceedings of\nMachine Learning Research, pp. 11727‚Äì11737. PMLR,\n2021. URL http://proceedings.mlr.press/\nv139/yang21c.html.\nYang, G., Hu, E. J., Babuschkin, I., Sidor, S., Liu, X., Farhi,\nD., Ryder, N., Pachocki, J., Chen, W., and Gao, J. Tensor\nprograms V: tuning large neural networks via zero-shot\nhyperparameter transfer. CoRR, abs/2203.03466, 2022.\ndoi: 10.48550/ARXIV.2203.03466. URL https:\n//doi.org/10.48550/arXiv.2203.03466.\nYang, G., Yu, D., Zhu, C., and Hayou, S. Tensor programs\nVI: feature learning in infinite depth neural networks.\nIn The Twelfth International Conference on Learning\nRepresentations, ICLR 2024, Vienna, Austria, May 7-11,\n2024. OpenReview.net, 2024. URL https://openre\nview.net/forum?id=17pVDnpwwl.\nYoo, K. M., Han, J., In, S., Jeon, H., Jeong, J., Kang, J.,\nKim, H., Kim, K., Kim, M., Kim, S., Kwak, D., Kwak,\nH., Kwon, S. J., Lee, B., Lee, D., Lee, G., Lee, J., Park,\nB., Shin, S., Yu, J., Baek, S., Byeon, S., Cho, E., Choe,\nD., Han, J., Jin, Y., Jun, H., Jung, J., Kim, C., Kim,\nJ., Kim, J., Lee, D., Park, D. W., Sohn, J. M., Han, S.,\nHeo, J., Hong, S., Jeon, M., Jung, H., Jung, J., Jung, W.,\nKim, C., Kim, H., Kim, J., Kim, M. Y., Lee, S., Park,\nJ., Shin, J., Yang, S., Yoon, J., Lee, H., Bae, S., Cha, J.,\nGylleus, K., Ham, D., Hong, M., Hong, Y., Hong, Y.,\nJang, D., Jeon, H., Jeon, Y., Jeong, Y., Ji, M., Jin, Y., Jo,\nC., Joo, S., Jung, S., Kim, A. J., Kim, B. H., Kim, H.,\nKim, J., Kim, M., Kim, M., Kim, S., Kim, Y., Kim, Y.,\nKim, Y., Ko, D., Lee, D., Lee, H., Lee, J., Lee, J., Lee,\nJ., Lee, J., Lee, M. Y., Lee, Y., Min, T., Min, Y., Moon,\nK., Oh, H., Park, J., Park, K., Park, Y., Seo, H., Seo, S.,\nSim, M., Son, G., Yeo, M., Yeom, K. H., and Yoo, W.\nHyperclova X technical report. CoRR, abs/2404.01954,\n2024. doi: 10.48550/ARXIV.2404.01954. URL https:\n//doi.org/10.48550/arXiv.2404.01954.\nYu, M., Wang, D., Shan, Q., and Wan, A.\nThe su-\nper weight in large language models. arXiv preprint\narXiv:2411.07191, 2024.\nZellers, R., Holtzman, A., Bisk, Y., Farhadi, A., and Choi,\nY. HellaSwag: Can a machine really finish your sen-\ntence?\nIn Korhonen, A., Traum, D., and M`arquez,\nL. (eds.), Proceedings of the 57th Annual Meeting of\nthe Association for Computational Linguistics, pp. 4791‚Äì\n4800, Florence, Italy, July 2019. Association for Compu-\ntational Linguistics. doi: 10.18653/v1/P19-1472. URL\nhttps://aclanthology.org/P19-1472/.\nZhai, S., Likhomanenko, T., Littwin, E., Busbridge, D.,\nRamapuram, J., Zhang, Y., Gu, J., and Susskind, J. M.\nStabilizing transformer training by preventing attention\nentropy collapse. In Krause, A., Brunskill, E., Cho, K.,\nEngelhardt, B., Sabato, S., and Scarlett, J. (eds.), Inter-\nnational Conference on Machine Learning, ICML 2023,\n23-29 July 2023, Honolulu, Hawaii, USA, volume 202 of\nProceedings of Machine Learning Research, pp. 40770‚Äì\n40803. PMLR, 2023. URL https://proceedings.\nmlr.press/v202/zhai23a.html.\nZhang, B. and Sennrich, R. Root mean square layer nor-\nmalization. In Wallach, H. M., Larochelle, H., Beygelz-\nimer, A., d‚ÄôAlch¬¥e-Buc, F., Fox, E. B., and Garnett, R.\n(eds.), Advances in Neural Information Processing Sys-\ntems 32: Annual Conference on Neural Information Pro-\ncessing Systems 2019, NeurIPS 2019, December 8-14,\n2019, Vancouver, BC, Canada, pp. 12360‚Äì12371, 2019.\nURL https://proceedings.neurips.cc/p\naper/2019/hash/1e8a19426224ca89e83ce\nf47f1e7f53b-Abstract.html.\nZhang, S., Roller, S., Goyal, N., Artetxe, M., Chen, M.,\nChen, S., Dewan, C., Diab, M. T., Li, X., Lin, X. V.,\nMihaylov, T., Ott, M., Shleifer, S., Shuster, K., Simig,\nD., Koura, P. S., Sridhar, A., Wang, T., and Zettlemoyer,\nL. OPT: open pre-trained transformer language models.\nCoRR, abs/2205.01068, 2022. doi: 10.48550/ARXIV.2\n205.01068. URL https://doi.org/10.48550\n/arXiv.2205.01068.\nZhang, Y., Chen, C., Ding, T., Li, Z., Sun, R., and Luo,\nZ. Why transformers need adam: A hessian perspec-\ntive.\nIn Globersons, A., Mackey, L., Belgrave, D.,\nFan, A., Paquet, U., Tomczak, J. M., and Zhang, C.\n(eds.), Advances in Neural Information Processing Sys-\ntems 38: Annual Conference on Neural Information\nProcessing Systems 2024, NeurIPS 2024, Vancouver,\nBC, Canada, December 10 - 15, 2024, 2024.\nURL\nhttp://papers.nips.cc/paper_files/p\n14\n\nPeri-LN: Revisiting Normalization Layer in the Transformer Architecture\naper/2024/hash/ee0e45ff4de76cbfdf070\n15a7839f339-Abstract-Conference.html.\nZhou, C., Liu, P., Xu, P., Iyer, S., Sun, J., Mao, Y., Ma, X.,\nEfrat, A., Yu, P., Yu, L., Zhang, S., Ghosh, G., Lewis,\nM., Zettlemoyer, L., and Levy, O. LIMA: less is more\nfor alignment. In Oh, A., Naumann, T., Globerson, A.,\nSaenko, K., Hardt, M., and Levine, S. (eds.), Advances\nin Neural Information Processing Systems 36: Annual\nConference on Neural Information Processing Systems\n2023, NeurIPS 2023, New Orleans, LA, USA, December\n10 - 16, 2023, 2023. URL http://papers.nip\ns.cc/paper_files/paper/2023/hash/ac6\n62d74829e4407ce1d126477f4a03a-Abstrac\nt-Conference.html.\n15\n\nPeri-LN: Revisiting Normalization Layer in the Transformer Architecture\nA. Related Work\nActivation Dynamics in Large Language Models.\nStudies on the distribution and magnitude of activations in deep\nneural networks have revealed that certain outlier features can significantly affect model behavior and efficiency. Dettmers\net al. (2022) examined Transformer architectures, highlighting how specific feature dimensions may exhibit unusually large\nvalues (outliers) that disrupt quantization and overall system performance. Extending this line of work, Sun et al. (2024)\nidentified the occurrence of ‚Äúmassive activations‚Äù‚Äîextremely large activation values that persist across multiple layers.\nUnlike standard outliers, these massive activations remain relatively invariant to different inputs, effectively functioning as\nimplicit bias terms in large language models (LLMs). Notably, such extreme values can skew the self-attention mechanism,\ncausing the model to attend disproportionately to certain tokens. These observations demonstrate that even with standard\nnormalization layers in place, hidden biases may linger in internal representations, underscoring the importance of deeper\nanalyses of activation behavior in LLMs.\nVariance Control and Normalization in Convolutional Networks.\nThe interplay between activation variance and\ntraining stability has also been extensively explored in convolutional neural networks (CNNs). De & Smith (2020) showed\nthat Batch Normalization (BN) stabilizes the training of residual networks by effectively downscaling activation variance in\nthe residual branches, thereby improving gradient behavior. However, BN imposes certain constraints, such as dependence on\nbatch size and additional computational overhead for estimating batch statistics. Consequently, several normalization-free or\nalternative normalization approaches have been investigated. For instance, Brock et al. (2021b) introduced ‚ÄúNormalizer-Free\nResNets,‚Äù which manage activation variance through learnable scaling parameters. This approach achieved competitive\nperformance without relying on BN, highlighting the critical role of effective variance control in fostering stable optimization\nand strong generalization in CNNs.\nLayer Normalization in Transformers.\nTraining stability in Transformer architectures is closely tied to the choice and\nplacement of layer normalization (LN). Xiong et al. (2020) reported that Transformers employing a Post-LN structure\noften suffer from gradient instabilities at initialization, requiring a careful learning-rate warm-up phase to mitigate these\nissues. In contrast, Pre-LN helps maintain more stable gradients during the early stages of training. However, Kedia\net al. (2024) showed that while Post-LN can lead to vanishing or exploding gradients in deep Transformers, Pre-LN may\ninduce hyperbolic gradient growth. These findings illustrate the nuanced trade-offs of normalization placement and draw\nparallels to earlier CNN studies, where careful management of activation variance proved essential for stable deep learning.\nDing et al. (2021) introduced Sandwich-LN in the vision domain for the first time, yet they paid little attention to the\nstructural characteristics and differences that distinguish one LN placement from another. In language domain, several\nmajor open-source language models (e.g., Olmo2 (OLMo et al., 2024), Gemma2 (Rivi`ere et al., 2024), and Gemma3\n(Team et al., 2025)) already employ a Peri-LN-like structure (see Section 3). Nevertheless, previous technical reports have\nnot explained why this design might be advantageous compared with the more widely studied Pre-LN and Post-LN. By\ninvestigating Peri-LN in detail, we aim to highlight the structural benefits that appear to underlie its empirical success in\nthese implementations.\nGradient Propagation and Depth Scaling\nEnsuring consistent gradient propagation across many layers is pivotal for\nstable training in very deep models. Yang & Hu (2021) (Tensor Programs IV) introduced the concept of Maximal Update\nParametrization (¬µP) in the infinite-width regime to preserve feature learning, preventing gradients from collapsing into\nkernel-like dynamics. Building on this, Yang et al. (2024) (Tensor Programs VI) proposed Depth-¬µP, which scales residual\nbranches and learning rates according to network depth. Their theoretical analysis indicates that improper depth-dependent\nscaling leads to vanishing or exploding gradients, ultimately diminishing the diversity of learned representations. These\ninsights highlight the necessity for principled scaling strategies and careful initialization to maintain robust gradient flow in\ndeep architectures.\nSummary.\nTaken together, these studies underscore the importance of managing activation variance and hidden biases to\nachieve stable training and expressive internal representations in modern deep networks. In Transformer-based models,\nnormalization choice and placement‚Äîsuch as Post-LN, Pre-LN, or other variants‚Äîplay a significant role in controlling\ngradient dynamics and overall performance. While Post-LN and Pre-LN have received significant attention, we focus on a\ncomparative analysis that includes Peri-LN, an alternative normalization placement that has thus far been underexplored but\nholds potential for enhancing training stability and model performance.\n16\n\nPeri-LN: Revisiting Normalization Layer in the Transformer Architecture\nB. Proposition 3.1 of Post-LN\nProposition B.1. Post-LN (vanishing gradient). Consider the following sequence of operations:\na = MLP(x), o = x + a, Àúo = Norm(o),\n(9)\nthen\n\r\r\r\r\r\n‚àÇL(Àúo)\n‚àÇW (2)\ni,j\n\r\r\r\r\r ‚â§4 Œ≥\n‚àö\nD ‚à•h‚à•\n‚à•x + a‚à•\n,\n(10)\nwhere h := ReLU\n\u0000xW (1) + b(1)\u0001\n. Since Post-LN normalizes the hidden state after each residual addition along the main\npath, the activation norm |h| tends to remain relatively moderate. As a result, in Post-LN, the gradient scale is less sensitive\nto the magnitude of activations and more significantly influenced by model depth, as previously analyzed by Xiong et al.\n(2020) and Kedia et al. (2024), leading to vanishing gradients as depth increases.\nC. Proof of Theoretical Insight\nTo support the claim that Peri-LN enhances the stability of training in such cases, we analyze the gradient norm in the final\nlayer. For simplicity, we use RMSNorm and ReLU here. Here, we assume that Œ≥, the scaling parameter used in RMSNorm,\nis positive, and we empirically verified that it remains strictly positive across models of all sizes during training.\nProposition C.1. Consider the following sequence of operations:\nÀúx = RMSNorm(x),\na = ReLU(ÀúxW (1) + b(1))W (2) + b(2),\no = x + a.\nThen,\n‚àÇL(o)\n‚àÇW (2)\ni,j\n= hi(ÀÜpj ‚àíyj),\n(11)\nwhere h := ReLU\n\u0000xW (1) + b(1)\u0001\n, ÀÜp := softmax(o), and y is the label (one-hot vector).\nProof. By the chain rule,\n‚àÇL(o)\n‚àÇW (2)\ni,j\n= ‚àÇL(o)\n‚àÇo\n| {z }\n(a):1√óD\n√ó\n‚àÇo\n‚àÇa\n|{z}\n(b):D√óD\n√ó\n‚àÇa\n‚àÇW (2)\ni,j\n| {z }\n(c):D√ó1\n.\n(12)\n(a) It is known that\n‚àÇL(o)\n‚àÇok\n= ÀÜpk ‚àíyk.\n(13)\nSo,\n‚àÇL(o)\n‚àÇo\n=\n\u0002ÀÜp1 ‚àíy1\nÀÜp2 ‚àíy2\n¬∑ ¬∑ ¬∑\nÀÜpD ‚àíyD\n\u0003\n.\n(14)\n(b) Since o = x + a,\n‚àÇo\n‚àÇa = I.\n(15)\n(c) Recall that\na := ReLU(ÀúxW (1) + b(1))W (2) + b(2).\n(16)\nFor convenience, let\nh := ReLU(ÀúxW (1) + b(1)).\n(17)\n17\n\nPeri-LN: Revisiting Normalization Layer in the Transformer Architecture\nThen, we have\n‚àÇak\n‚àÇW (2)\ni,j\n=\n‚àÇ\n‚àÇW (2)\ni,j\n H\nX\np=1\nhpW (2)\np,k + b(2)\nk\n!\n= hi Œ¥k,j.\n(18)\nIn vector representation,\n‚àÇa\n‚àÇW (2)\ni,j\n=\n\u00020\n¬∑ ¬∑ ¬∑\nhi\n¬∑ ¬∑ ¬∑\n0\u0003‚ä§,\n(19)\nwhere the only nonzero entry is in the j-th component.\nThus, by putting these all together,\n‚àÇL(o)\n‚àÇW (2)\ni,j\n= hi(ÀÜpj ‚àíyj).\n(20)\nProposition C.2. Consider the following sequence of operations:\nÀúx = RMSNorm(x),\na = ReLU(ÀúxW (1) + b(1))W (2) + b(2),\nÀúa = RMSNorm(a),\no = x + Àúa.\nThen,\n\r\r\r\r\r\n‚àÇL(o)\n‚àÇW (2)\ni,j\n\r\r\r\r\r ‚â§4Œ≥\n‚àö\nD‚à•h‚à•\n‚à•a‚à•\n,\n(21)\nwhere Œ≥ is the scaling parameter used in RMSNorm(¬∑), D is the dimensionality, and h := ReLU\n\u0000xW (1) + b(1)\u0001\n.\nProof. By the chain rule,\n‚àÇL(o)\n‚àÇW (2)\ni,j\n= ‚àÇL(o)\n‚àÇo\n| {z }\n(a):1√óD\n√ó\n‚àÇo\n‚àÇÀúa\n|{z}\n(b):D√óD\n√ó\n‚àÇÀúa\n‚àÇa\n|{z}\n(c):D√óD\n√ó\n‚àÇa\n‚àÇW (2)\ni,j\n| {z }\n(d):D√ó1\n.\n(22)\n(a) We have\n\r\r\r\r\n‚àÇL(o)\n‚àÇo\n\r\r\r\r = ‚à•ÀÜp ‚àíy‚à•‚â§‚à•ÀÜp‚à•+ ‚à•y‚à•= 2.\n(23)\n(b) We also have\n\r\r\r\r\n‚àÇo\n‚àÇÀúa\n\r\r\r\r = ‚à•I‚à•= 1.\n(24)\n(c) Recall that\nÀúa := RMSNorm(a) = Œ≥ ¬∑\na\nq\n1\nD\nPD\nk=1 a2\nk + œµ\n.\n(25)\nThen, ‚àÇÀúa\n‚àÇa is the Jacobian matrix J of RMSNorm(¬∑). For brevity, let\nŒ± := 1\nD\nD\nX\nk=1\n(ak)2.v\n(26)\n18\n\nPeri-LN: Revisiting Normalization Layer in the Transformer Architecture\nThen,\nJp,q = ‚àÇÀúap\n‚àÇaq\n= Œ≥ ¬∑\n‚àÇ\n‚àÇaq\n\u0012\nap\n‚àöŒ± + œµ\n\u0013\n(27)\n= Œ≥ ¬∑\n1\n‚àöŒ± + œµ\n‚àÇap\n‚àÇaq\n+ Œ≥ ¬∑ ap\n‚àÇ\n‚àÇaq\n\u0012\n1\n‚àöŒ± + œµ\n\u0013\n(28)\n= Œ≥ ¬∑\n1\n‚àöŒ± + œµŒ¥p,q ‚àíŒ≥ ¬∑\napaq\nD(Œ± + œµ)3/2 .\n(29)\nIn matrix representation,\nJ =\nŒ≥\n‚àöŒ± + ŒµI\n|\n{z\n}\nA\n‚àí\nŒ≥\nD(Œ± + Œµ)3/2 (a)‚ä§(a)\n|\n{z\n}\nB\n.\n(30)\nThen, we have\n‚à•A‚à•=\n\r\r\r\r\nŒ≥\n‚àöŒ± + ŒµI\n\r\r\r\r =\nŒ≥\n‚àöŒ± + Œµ‚à•I‚à•=\nŒ≥\n‚àöŒ± + Œµ,\n(31)\nand\n‚à•B‚à•=\n\r\r\r\r\nŒ≥\nD(Œ± + Œµ)3/2 (a)‚ä§(a)\n\r\r\r\r =\nŒ≥\nD(Œ± + Œµ)3/2 √ó DŒ± =\nŒ≥Œ±\n(Œ± + Œµ)3/2 .\n(32)\nSo, we have\n‚à•J‚à•= ‚à•A ‚àíB‚à•‚â§‚à•A‚à•+ ‚à•B‚à•‚â§2Œ≥\n‚àöŒ± = 2Œ≥\n‚àö\nD\n‚à•a‚à•.\n(33)\n(d) Since\n‚àÇa\n‚àÇW (2)\ni,j\n=\n\u00020\n¬∑ ¬∑ ¬∑\nhi\n¬∑ ¬∑ ¬∑\n0\u0003‚ä§,\n(34)\nwe have\n\r\r\r\r\r\n‚àÇa\n‚àÇW (2)\ni,j\n\r\r\r\r\r ‚â§‚à•h‚à•.\n(35)\nThus,\n\r\r\r\r\r\n‚àÇL(o)\n‚àÇW (2)\ni,j\n\r\r\r\r\r ‚â§2 √ó 1 √ó 2Œ≥\n‚àö\nD\n‚à•a‚à•\n√ó ‚à•h‚à•= 4Œ≥\n‚àö\nD‚à•h‚à•\n‚à•a‚à•\n.\n(36)\nProposition C.3. Consider the following sequence of operations:\na = ReLU(xW (1) + b(1))W (2) + b(2),\no = x + a,\nÀúo = RMSNorm(o).\nThen,\n\r\r\r\r\r\n‚àÇL(Àúo)\n‚àÇW (2)\ni,j\n\r\r\r\r\r ‚â§4Œ≥\n‚àö\nD‚à•h‚à•\n‚à•x + a‚à•,\n(37)\nwhere Œ≥ is the scaling parameter used in RMSNorm(¬∑), D is the dimensionality, and h := ReLU\n\u0000xW (1) + b(1)\u0001\n.\nProof. The proof is analogous to the proof of the previous proposition.\n19\n\nPeri-LN: Revisiting Normalization Layer in the Transformer Architecture\nD. Detailed Experimental Setting\nIn this section, we provide detailed configurations of both the pretraining and supervised fine-tuning to reproduce our results.\nD.1. Configurations on Pre-Training\nThe common training settings are provided in Table 2. Embedding settings for the language models are listed in Table 3.\nFor the model architecture, we primarily follow the Llama 3 architecture (Dubey et al., 2024). In the MLP module, we use\nSwiGLU activations. Additional details regarding the model configurations are shown in Table 4. Note that embedding\nparameters are excluded from the model size. Unless otherwise noted, most training and model settings follow those of the\nDCLM experiments (Li et al., 2024a). During the pretraining stage, each model was trained under a controlled random seed.\nTable 2. Common configurations. LR Schedule denotes learning rate schedule.\nGlobal Batch Size\nWeight Decay\nIterations\nOptimizer\nLR Schedule\nWarmup\nWeight Initialization\n256\n0.033\n14400\nAdam\nCosine\n10%\n0.02\nTable 3. Embedding configurations.\nMax Position Embeddings\nPosition Embedding Type\nUntie-embeddings-and-output-weights\n8192\nRope\nTrue\nTable 4. Model configurations.\nSize\nnlayers\nnheads\ndmodel\ndhead\n400M\n24\n8\n1024\n128\n1.5B\n24\n16\n2048\n128\n3.2B\n32\n16\n2560\n160\nD.2. Configurations on Supervised Fine-Tuning\nTo examine downstream task performance after instruction tuning, we employed a high-quality LIMA alignment training\nset consisting of 1, 000 samples (Zhou et al., 2023). Our supervised fine-tuning configuration was slightly modified from\nthe original setup of LIMA: we fine-tuned the model for 15 epochs with a batch size of 128. The optimizer was Adam\nwith an initial learning rate of 1e-5 and a cosine learning rate schedule. We selected the best checkpoints for each model\nby evaluating on OpenBookQA (Mihaylov et al., 2018), CommonSenseQA (Talmor et al., 2019), and the NLI dataset in\nGLUE (Wang et al., 2018).\n20\n\nPeri-LN: Revisiting Normalization Layer in the Transformer Architecture\nE. Additional Results on Pre-Training Study\nE.1. Post-Layer Normalization Architecture & Learning Rate Exploration\nIn order to identify the optimal performance configuration for Post-LN within the experimental setup, we conducted a\nlearning rate exploration as shown in Figure 12. Because the appropriate learning rate for Post-LN fell into a much lower\nrange than those for Pre-LN and Peri-LN, we treated it separately. For each Post-LN setting, the best learning rate was\ndetermined as the one yielding the lowest final training loss, with the random seed held constant during this selection\nprocess.\nFigure 12. Learning rate explorations for Post-LN architectures.\nE.2. Best Performing Checkpoints Comparisons of Other Model Sizes\nAs an extension of Section 4.2, we present below the results for additional model sizes that were omitted previously due\nto space constraints. In Figures 13, we compare the pre-training loss and the gradient norm curve at each LN strategy‚Äôs\nbest-performing learning rate of 3.2B and 1.5B size models.\n(a) Training loss for 3.2B\n(b) Gradient-norm for 3.2B\n(c) Training loss for 1.5B\n(d) Gradient-norm for 1.5B\nFigure 13. Performance comparison of Post-LN, Pre-LN, and Peri-LN Transformers during pre-training for other two.\n21\n\nPeri-LN: Revisiting Normalization Layer in the Transformer Architecture\nF. Additional Results on Growth of Hidden State\nIn this section, we examine the 400M- and 3.2B-parameter models, which were omitted in Section 5.1 due to space\nconstraints. As illustrated in Figures 14 and 15, these models exhibit the same overall trend.\n(a) Absolute magnitude growth\n(b) Variance growth\nFigure 14. The forward growth patterns of hidden state for different architectures highlight the structural impact of normalization\nplacement. 3.2B size model.\n(a) Absolute magnitude growth\n(b) Variance growth\nFigure 15. The forward growth patterns of hidden state for different architectures highlight the structural impact of normalization\nplacement. 400M size model.\n22\n\nPeri-LN: Revisiting Normalization Layer in the Transformer Architecture\nG. Additional Experimental Results on Ablation Study\nG.1. Amount of Training Tokens\nIn order to investigate whether the learning behavior of each LN strategy varies with the number of training tokens, we\nconducted an additional round of learning-rate exploration for both the Pre-LN and Peri-LN architectures. As shown in\nFigure 16, even as the number of training tokens increases, there is no observable shift in the optimal learning-rate range.\nBased on these findings, we conclude that our overall results remain consistent, even when the training token count is further\nincreased. Furthermore, although a learning rate of 5 √ó 10‚àí3 leads to divergence in the smaller-scale experiments with 8B\nor 16B training tokens, it does not do so in the 30B-token setting. We attribute this discrepancy to the 10% warmup rate,\nsuggesting that the warmup phase may be insufficient for the smaller-scale experiments.\nFigure 16. Learning rate explorations of Pre-& Peri-LN architecture with sequence length 2048 configuration.\nG.2. Sequence Length\nIn language models, the number of iterations per token is influenced by the sequence length, which in turn, along with the\nbatch size, affects training statistics. We conducted an experiment to determine whether the performance trend changes\nwhen the sequence length is reduced from 8192, as set in the main text, to 2048. As shown in Figure 17, Peri-LN still\nsurpasses Pre-LN in the learning rate exploration.\nFigure 17. Learning rate explorations of Pre-& Peri-LN architecture with sequence length 2048 configuration.\n23\n\nPeri-LN: Revisiting Normalization Layer in the Transformer Architecture\nG.3. Warm-up\nWarmup is widely recognized to influence training stability. To investigate whether a 10% warmup rate might unfairly\ndisadvantage Pre-LN, we conducted an additional learning-rate exploration using a 30% warmup rate. As illustrated in\nFigure 18, the overall trend remained unchanged, and Peri-LN continued to exhibit better performance than Pre-LN in\nterms of loss. Furthermore, we observed that increasing the warmup rate from 10% to 30% did not reduce the frequency of\ngradient norm spikes in Pre-LN.\nFigure 18. Learning rate explorations of Pre-& Peri-LN architecture with warmup 30% configuration.\nG.4. RMSNorm & LayerNorm\nAs illustrated in Figure 19, we conducted experiments in which RMSNorm and LayerNorm were interchanged. Consistent\nwith the findings reported in (OLMo et al., 2024), we did not observe any notable performance differences in our RMSNorm\nand LayerNorm replacement experiments. Learning rate was set to 2e-3 (best performance learning rate).\n(a) Pre-training loss curve\n(b) Gradient-norm curve\nFigure 19. LayerNorm vs. RMSNorm on Peri-LN architecture. 400M size model.\n24\n\nPeri-LN: Revisiting Normalization Layer in the Transformer Architecture\nG.5. Embedding Layer Normalization of Peri-Layer Normalization Transformers\nMotivated by Takase et al. (2023), we empirically explore the addition of embedding layer normalization to improve training\nstability and overall model performance in Transformer architectures. As illustrated in Figures 20, 21, and 22, incorporating\nEmbedding LN in the Peri-LN architecture yields a slight improvement in pre-training loss. Furthermore, our empirical\nobservations suggest that this effect becomes more pronounced in smaller models.\n(a) Pre-training loss curve\n(b) Gradient-norm curve\nFigure 20. Loss and Gradient-norm curves comparing the presence and absence of Embedding LN in the Peri-LN architecture. 400M size\nmodel.\n(a) Pre-training loss curve\n(b) Gradient-norm curve\nFigure 21. Loss and Gradient-norm curves comparing the presence and absence of Embedding LN in the Peri-LN architecture. 1.5B size\nmodel.\n(a) Pre-training loss curve\n(b) Gradient-norm curve\nFigure 22. Loss and Gradient-norm curves comparing the presence and absence of Embedding LN in the Peri-LN architecture. 3.2B size\nmodel.\n25\n\nPeri-LN: Revisiting Normalization Layer in the Transformer Architecture\nG.6. Ablation Study on Additional Normalization Layer Placement\nWe additionally conduct further experiments on LN placements to compare different combinations (referred to as A, B, and\nC positions in Figure 2). We add configurations where LN is placed at both A + C (akin to combining Pre- and Post-LN), as\nwell as only at B, to compare them with Peri-LN at final training loss under the controlled same training seed. We pre-train\nthe 400M-parameter Transformers on 30B tokens each, using the same training configurations described in Section 4. As\naligned with Xiong et al. (2020), our new results confirm that placing LN exclusively at C leads to training instability or\nsuboptimal performance. In particular, the A + C configuration inherits characteristics of Post-LN (large gradient norm\nshifts), forcing the use of smaller learning rates and still resulting in lower overall performance than Peri-LN architecture.\nTable 5. Final training loss and additional normalization layer placement.\n400M\nA + C\nPost-LN\nB\nPeri-LN\nFinal Training Loss\n3.01\n3.05\nDiverged\n2.91\nG.7. Peri-LN with QK-Norm\nWhile Peri-LN alone provides robust training dynamics, QK-Norm can still enhance performance. We conducted additional\nexperiments that confirm combining Peri-LN with QK-Norm yields slight improvements in training loss. We pre-train the\n1.5B-parameter Transformers on 30B tokens each, using the same training configurations described in Section 4. As shown\nin Table 6, adding QK-norm to Peri-LN indeed yielded better performance, consistent with Wortsman et al. (2024). In this\nexperiment, the Peri-LN variant equipped with QK-norm used LayerNorm instead of RMSNorm.\nTable 6. Peri-LN with QK-Norm.\n1.5B\nPeri-LN\n+QK-Norm (Wortsman et al., 2024)\nFinal Training Loss\n2.722\n2.711\n26\n\nPeri-LN: Revisiting Normalization Layer in the Transformer Architecture\nG.8. Weight Decay and Weight Initialization\nG.8.1. COMMON SETTINGS\nWe pre-train the 400M-parameter Transformers on 30B tokens each under the controlled same training seed. We measure\nthe training loss and averaged benchmark score for these experiments under the same evaluation settings used in Table 1 of\nthe paper. Other configurations follow those outlined in Section 4. For the variance growth experiments in Figure 10, we\nadopt the same settings as in Section 5.1, except that we use 100 samples for the forward-pass statistics.\nG.8.2. WEIGHT DECAY\nWe conduct additional studies for various weight decay condition for both Pre-LN and Peri-LN architectures. As shown in\nthe Table 7, Peri-LN continues to offer better performance than Pre-LN under the same settings. We provide per-run results\nas below:\nTable 7. Effect of weight decay on 400M-parameter Pre-LN and Peri-LN Transformers: Final training loss and averaged benchmark score.\n400M\nWeight Decay Coefficient\n0\n0.0033\n0.033\n0.33\nFinal Training Loss\nPre-LN\n3.03\n3.03\n3.03\n3\nPeri-LN\n2.94\n2.94\n2.93\n2.90\nAveraged Benchmark Score\nPre-LN\n49.26\n49.18\n49.01\n49.51\nPeri-LN\n51.41\n51.14\n50.68\n52.13\n(a) Pre-training loss curve\n(b) Gradient-norm curve\nFigure 23. Comparison of pre-training loss and gradient norms for Pre-LN and Peri-LN architectures with the weight decay coefficient\nfixed at 0, while all other hyperparameters are held constant.\n(a) Pre-training loss curve\n(b) Gradient-norm curve\nFigure 24. Comparison of pre-training loss and gradient norms for Pre-LN and Peri-LN architectures with the weight decay coefficient\nfixed at 0.0033, while all other hyperparameters are held constant.\nG.8.3. WEIGHT INITIALIZATION\nWe run an additional ablation on weight-initialization schemes. For both Pre-LN and Peri-LN models, we first adopt Xavier\ninitialization (Glorot & Bengio, 2010) and then compare it with He initialization (2/d) (He et al., 2015), LeCun initialization\n27\n\nPeri-LN: Revisiting Normalization Layer in the Transformer Architecture\n(a) Pre-training loss curve\n(b) Gradient-norm curve\nFigure 25. Comparison of pre-training loss and gradient norms for Pre-LN and Peri-LN architectures with the weight decay coefficient\nfixed at 0.033, while all other hyperparameters are held constant.\n(a) Pre-training loss curve\n(b) Gradient-norm curve\nFigure 26. Comparison of pre-training loss and gradient norms for Pre-LN and Peri-LN architectures with the weight decay coefficient\nfixed at 0.33, while all other hyperparameters are held constant.\n(1/d), and two scaled variants, 10/d and 1/(10d).\nAs Table 8 shows, Xavier initialization yields the strongest overall performance, improving on the configurations used\nin our earlier experiments. Crucially, our central finding remains intact: hidden-state variance sharply grows in Pre-LN\nTransformers but stays bounded in Peri-LN Transformers. Table 9 confirms the same pattern across all initialization scales,\nand detailed per-run results appear below.\nTable 8. Xavier initialization (Glorot & Bengio, 2010) yields better performance compared to our previous weight initialization configura-\ntions.\n400M\nArchitecture\nBaseline(0.02)\nXavier Initialization\nLoss\nPre-LN\n3.03\n2.95\nPeri-LN\n2.93\n2.91\nAvg.\nPre-LN\n49.01\n51.25\nPeri-LN\n50.68\n52.04\nTable 9. Effect of weight-initialization variance on final pre-training loss for 400M-parameter Pre-LN and Peri-LN Transformers.\n400M\nInitialization Variance\n10/d\nHe (2/d)\nLeCun (1/d)\n1/(10d)\nBaseline (0.02)\nLoss\nPre-LN\n4.526\n2.965\n3.005\n3.012\n3.035\nPeri-LN\n3.027\n2.929\n2.915\n2.902\n2.916\n28\n\nPeri-LN: Revisiting Normalization Layer in the Transformer Architecture\n(a) Pre-training loss curve\n(b) Gradient-norm curve\nFigure 27. Comparison of pre-training loss and gradient norms for Pre-LN and Peri-LN architectures with the weight initialization variance\nset to 10/d, while all other hyperparameters are held constant.\n(a) Pre-training loss curve\n(b) Gradient-norm curve\nFigure 28. Comparison of pre-training loss and gradient norms for Pre-LN and Peri-LN architectures with the weight initialization variance\nset to 2/d (He init (He et al., 2015)), while all other hyperparameter are held constant.\n(a) Pre-training loss curve\n(b) Gradient-norm curve\nFigure 29. Comparison of pre-training loss and gradient norms for Pre-LN and Peri-LN architectures with the weight initialization variance\nset to 1/d (LeCun init), while all other hyperparameter are held constant.\n(a) Pre-training loss curve\n(b) Gradient-norm curve\nFigure 30. Comparison of pre-training loss and gradient norms for Pre-LN and Peri-LN architectures with the weight initialization variance\nset to 1/(10d), while all other hyperparameter are held constant.\n29\n\nPeri-LN: Revisiting Normalization Layer in the Transformer Architecture\nH. Output-Layer Normalization with QK-Norm Architecture\nQuery and Key layer-normalization (QK-Norm) has been widely studied in modern Transformer architectures (Wortsman\net al., 2024; Zhai et al., 2023; OLMo et al., 2024). In particular, OLMo et al. (2024) reported that QK-Norm combined\nwith module output layer-normalization (output-LN, B in Figure 31 referred to as ‚Äúreordered norm‚Äù in the OLMo2 paper)\nimproves both training loss and stability. As shown in Figure 31, QK-Norm is applied after the Query and Key projections,\nsimilar to output-LN. From another perspective, QK-Norm is also applied immediately before the attention calculation, akin\nto a Pre-LN approach. In our view, QK-Norm and Pre-LN (placed at A2 and A respectively in Figure 31) serve the same\nrole but differ in certain details. As shown in Figures 32, 33, and 34, the two architectures exhibit comparable performance\noverall in terms of both training loss and stability.. However, Peri-LN provides a slight performance advantage over the\nOLMo2-style Peri-LN in the 400M- and 1B-parameter models.\nFigure 31. QK-layer normalization in the Attention module.\n(a) Pre-training loss curve\n(b) Gradient-norm curve\nFigure 32. Comparison of pre-training loss and gradient norm between OLMo2-Style Peri-LN and the Peri-LN architecture. To ensure an\naccurate comparison, we present the pre-training loss over the final 5B tokens. 400M size model.\n30\n\nPeri-LN: Revisiting Normalization Layer in the Transformer Architecture\n(a) Pre-training loss curve\n(b) Gradient-norm curve\nFigure 33. Comparison of pre-training loss and gradient norm between OLMo2-Style Peri-LN and the Peri-LN architecture. To ensure an\naccurate comparison, we present the pre-training loss over the final 5B tokens. 1.5B size model.\n(a) Pre-training loss curve\n(b) Gradient-norm curve\nFigure 34. Comparison of pre-training loss and gradient norm between OLMo2-Style Peri-LN and the Peri-LN architecture. To ensure an\naccurate comparison, we present the pre-training loss over the final 5B tokens. 3.2B size model.\nI. Training the Transformer using Stochastic Gradient Descent\nUsing Stochastic Gradient Descent (SGD) for training Transformers is not a common practice. As Zhang et al. (2024) point\nout, Transformer-based models tend to perform worse with SGD than with Adam by a considerable margin. One reason is\nthat SGD struggles to handle the heterogeneity across different blocks. Although these aspects are certainly intriguing and\nwarrant further investigation, they lie beyond the scope of our current work, as Zhang et al. (2024) also note.\nNonetheless, for someone who might wonder to know, we conduct additional experiments using SGD. We are searching\nfor U-shaped patterns during the learning rate exploration for both Pre-LN & Peri-LN as shown in the Figure 35. We\nobserve that: (1) SGD performs worse than Adam, consistent with findings reported in (Zhang et al., 2024); and (2) Peri-LN\ndemonstrates better performance than Pre-LN. In these results, we use 400 M-parameter Transformers and apply the same\nconfigurations as in the main experiments (Section 4.1). We provide detailed training curves in Figures 36, 37, 38.\nFigure 35. Learning Rate Exploration of Pre-& Peri-LN architecture trained with SGD optimizer. The individual training-loss and\ngradient-norm curves appear in Figures 36, 37, and 38.\n31\n\nPeri-LN: Revisiting Normalization Layer in the Transformer Architecture\n(a) Pre-training loss curve\n(b) Gradient-norm curve\nFigure 36. Using SGD with learning rate 5e ‚àí3.\n(a) Pre-training loss curve\n(b) Gradient-norm curve\nFigure 37. Using SGD with learning rate 3e ‚àí3.\n(a) Pre-training loss curve\n(b) Gradient-norm curve\nFigure 38. Using SGD with learning rate 1e ‚àí3.\n32\n\nPeri-LN: Revisiting Normalization Layer in the Transformer Architecture\nJ. Additional Details on Evaluation\nJ.1. Detailed Configurations\nWe utilized the Language Model Evaluation Harness6with the HuggingFace Transformers library (Gao et al., 2024; Wolf\net al., 2020) to assess overall performance. We employ five different evaluation benchmarks: ARC (Clark et al., 2018),\nHellaSwag (Zellers et al., 2019), PIQA (Bisk et al., 2020), SIQA (Sap et al., 2019), Winogrande (Sakaguchi et al., 2021).\nDuring the pretraining stage, each model was trained under a controlled random seed. We used the training loss at iteration\n14, 000‚Äîcorresponding to the completion of 30B tokens‚Äîas our main reference point. When calculating the evaluation\nscore, diverged checkpoints were excluded.\nJ.2. Detailed Results on Benchmark Evaluations\nIn this section, we present the evaluation results for each model trained with five different training seeds. We exclude any\ndiverged scores and average the remaining values, which are then reported in Table 1 in the main text.\nJ.2.1. PRE-TRAINING\nTable 10. Detailed results on pre-training the Peri-LN architecture. These results are averaged to produce the values reported in Table 1.\nSEED denotes pre-training seed.\nPeri-LN\nSEED\nARC-Easy\nHellaSwag\nPIQA\nSIQA\nWinogrande\n1\n0.5758\n0.3803\n0.6980\n0.4115\n0.5225\n2\n0.5728\n0.3739\n0.6915\n0.4017\n0.5367\n400M\n3\n0.5842\n0.3745\n0.6986\n0.4125\n0.5249\n4\n0.5800\n0.3722\n0.6959\n0.4038\n0.5209\n5\n0.5627\n0.3719\n0.6899\n0.4028\n0.5320\n1\n0.6599\n0.4437\n0.7339\n0.4304\n0.5714\n2\n0.6591\n0.4394\n0.7399\n0.4145\n0.5699\n1.5B\n3\n0.6625\n0.4357\n0.7372\n0.4166\n0.5627\n4\n0.6633\n0.4367\n0.7345\n0.4222\n0.5667\n5\n0.6637\n0.4416\n0.7361\n0.4335\n0.5612\n1\n0.6953\n0.4734\n0.7443\n0.4417\n0.5872\n2\n0.6839\n0.4684\n0.7427\n0.4324\n0.6054\n3.2B\n3\n0.6902\n0.4680\n0.7486\n0.4243\n0.5967\n4\n0.6864\n0.4700\n0.7427\n0.4273\n0.5935\n5\n0.6806\n0.4698\n0.7372\n0.4243\n0.6054\nTable 11. Detailed results on pre-training the Pre-LN architecture. These results are averaged to produce the values reported in Table 1.\nSEED denotes pre-training seed.\nPre-LN\nSEED\nARC-Easy\nHellaSwag\nPIQA\nSIQA\nWinogrande\n1\n0.5669\n0.3609\n0.7008\n0.4002\n0.5359\n2\nDiverged\n400M\n3\n0.5354\n0.3328\n0.6741\n0.3905\n0.4957\n4\nDiverged\n5\n0.5438\n0.3314\n0.6888\n0.4012\n0.4949\n1\n0.6326\n0.4259\n0.7242\n0.4263\n0.5691\n2\n0.6019\n0.3924\n0.7111\n0.3992\n0.5627\n1.5B\n3\n0.6077\n0.3932\n0.7008\n0.4125\n0.5272\n4\n0.6111\n0.3886\n0.7187\n0.4135\n0.5225\n5\n0.6221\n0.3941\n0.7160\n0.4099\n0.5438\n1\n0.6688\n0.4588\n0.7470\n0.4273\n0.5919\n2\nDiverged\n3.2B\n3\nDiverged\n4\n0.6359\n0.4259\n0.7301\n0.4263\n0.5564\n5\nDiverged\n6https://github.com/EleutherAI/lm-evaluation-harness\n33\n\nPeri-LN: Revisiting Normalization Layer in the Transformer Architecture\nTable 12. Detailed results on pre-training the Post-LN architecture. These results are averaged to produce the values reported in Table 1.\nSEED denotes pre-training seed.\nPost-LN\nSEED\nARC-Easy\nHellaSwag\nPIQA\nSIQA\nWinogrande\n1\n0.3413\n0.2881\n0.6311\n0.3378\n0.5067\n2\n0.3691\n0.2886\n0.6132\n0.3337\n0.5099\n400M\n3\n0.3632\n0.2889\n0.6257\n0.3603\n0.5051\n4\n0.3603\n0.2920\n0.6262\n0.3490\n0.5012\n5\n0.3510\n0.2880\n0.6170\n0.3434\n0.5209\n1\n0.4268\n0.3121\n0.6659\n0.3628\n0.5185\n2\n0.4196\n0.3150\n0.6654\n0.3639\n0.5004\n1.5B\n3\nDiverged\n4\n0.4285\n0.3212\n0.6730\n0.3511\n0.4775\n5\n0.4419\n0.3193\n0.6643\n0.3557\n0.5154\n1\n0.4731\n0.3427\n0.6774\n0.3664\n0.5343\n2\n0.4638\n0.3326\n0.6779\n0.3577\n0.4917\n3.2B\n3\n0.3956\n0.3321\n0.6143\n0.3408\n0.5067\n4\n0.4663\n0.3380\n0.6692\n0.3685\n0.5178\n5\n0.4663\n0.3340\n0.6839\n0.3577\n0.5043\nJ.2.2. SUPERVISED FINE-TUNING\nTable 13. Detailed results on SFT with Peri-LN architecture. These results are averaged to produce the values reported in Table 1. SEED\ndenotes pre-training seed.\nPeri-LN\nSEED\nARC-Easy\nHellaSwag\nPIQA\nSIQA\nWinogrande\n1\n0.5800\n0.3819\n0.6991\n0.4145\n0.5328\n2\n0.5783\n0.3783\n0.6921\n0.4038\n0.5391\n400M\n3\n0.5888\n0.3806\n0.6980\n0.4222\n0.5288\n4\n0.5892\n0.3738\n0.6948\n0.4089\n0.5099\n5\n0.5783\n0.3757\n0.6991\n0.4099\n0.5312\n1\n0.6633\n0.4502\n0.7356\n0.4304\n0.5746\n2\n0.6641\n0.4437\n0.7405\n0.4207\n0.5706\n1.5B\n3\n0.6671\n0.4454\n0.7454\n0.4207\n0.5620\n4\n0.6700\n0.4455\n0.7378\n0.4284\n0.5659\n5\n0.6688\n0.4478\n0.7421\n0.4324\n0.5620\n1\n0.7058\n0.4810\n0.7486\n0.4422\n0.5880\n2\n0.6898\n0.4774\n0.7437\n0.4391\n0.6054\n3.2B\n3\n0.6995\n0.4770\n0.7481\n0.4278\n0.5912\n4\n0.6911\n0.4777\n0.7432\n0.4350\n0.5943\n5\n0.6894\n0.4781\n0.7448\n0.4319\n0.6046\n34\n\nPeri-LN: Revisiting Normalization Layer in the Transformer Architecture\nTable 14. Detailed results on SFT with Pre-LN architecture. These results are averaged to produce the values reported in Table 1. SEED\ndenotes pre-training seed.\nPre-LN\nSEED\nARC-Easy\nHellaSwag\nPIQA\nSIQA\nWinogrande\n1\n0.5762\n0.3625\n0.7078\n0.4058\n0.5343\n2\nN/A\n400M\n3\n0.5370\n0.3339\n0.6757\n0.3905\n0.4972\n4\nN/A\n5\n0.5509\n0.3372\n0.6893\n0.4074\n0.4886\n1\n0.6385\n0.4310\n0.7247\n0.4227\n0.5620\n2\n0.6035\n0.3934\n0.7095\n0.4038\n0.5572\n1.5B\n3\n0.6098\n0.3944\n0.7035\n0.4150\n0.5257\n4\n0.6208\n0.3929\n0.7182\n0.4161\n0.5272\n5\n0.6258\n0.4017\n0.7171\n0.4181\n0.5391\n1\n0.6785\n0.4681\n0.7568\n0.4345\n0.5825\n2\nN/A\n3.2B\n3\nN/A\n4\n0.6427\n0.4293\n0.7274\n0.4299\n0.5580\n5\nN/A\nTable 15. Detailed results on SFT with Post-LN architecture. These results are averaged to produce the values reported in Table 1. SEED\ndenotes pre-training seed.\nPost-LN\nSEED\nARC-Easy\nHellaSwag\nPIQA\nSIQA\nWinogrande\n1\n0.4428\n0.3307\n0.6583\n0.3797\n0.5099\n2\n0.4280\n0.3208\n0.6404\n0.3746\n0.5178\n400M\n3\n0.4693\n0.3241\n0.6578\n0.3905\n0.5122\n4\n0.4680\n0.3247\n0.6610\n0.3726\n0.4830\n5\n0.4520\n0.3283\n0.6572\n0.3849\n0.5225\n1\n0.5316\n0.3774\n0.6980\n0.3889\n0.5359\n2\n0.4731\n0.3316\n0.6719\n0.3813\n0.5028\n1.5B\n3\nN/A\n4\n0.5387\n0.3546\n0.6779\n0.3864\n0.4909\n5\n0.5261\n0.3510\n0.6752\n0.3767\n0.5209\n1\n0.5623\n0.4029\n0.7008\n0.3920\n0.5051\n2\n0.5417\n0.3644\n0.6823\n0.3833\n0.5264\n3.2B\n3\n0.4444\n0.3604\n0.6333\n0.3618\n0.5043\n4\n0.5400\n0.3645\n0.6844\n0.3823\n0.5020\n5\n0.5341\n0.3677\n0.6942\n0.3976\n0.5012\n35\n\nPeri-LN: Revisiting Normalization Layer in the Transformer Architecture\nK. Additional Discussions of Precision Constraints Imposed by Pre-LN\nThis section provide additional discussions of Section 7.2. Given that both Pre-LN and Peri-LN exhibit a structural property\nwhereby large values do not readily disappear once they arise, it is important to monitor the occurrence of these extreme\nactivations. Pre-LN‚Äôs additive residual path often produces activations that approach, and occasionally exceed, the FP16\n(float16) representable maximum7. To quantify how often these values would overflow FP16 yet remain within the BF16\n(bfloat16) range8, we measure the 100 largest absolute hidden-state values (the global top-100 tokens) for each Pre-LN and\nPeri-LN 3.2B-parameter Transformer. The shaded region indicates the range of these global top-100 tokens. The blue curve\n(with shaded band) represents a Pre-LN model, the red curve a Peri-LN model, and the dashed orange line denotes the FP16\nrepresentable maximum.\nAs shown in Figure 11, consistent with the observations of Sun et al. (2024), activations in the Pre-LN model routinely\nexceed the FP16 bound from the very first 0.5B training tokens, with the overshoot becoming more pronounced in deeper\nlayers‚Äîan indication of growing numerical instability. By contrast, Peri-LN consistently maintains a comfortable margin\nbelow the FP16 limit throughout training. This finding underscores that the choice between FP16 and BF16 is not merely a\nhardware preference; it is tightly coupled to how hidden-state magnitudes evolve within the network.\nSince NVIDIA V100 GPUs do not support BF16, these results imply that training and inference with Pre-LN models on\nsuch hardware are inherently disadvantaged. Moreover, from the standpoint of large-language-model quantization (Dettmers\net al., 2022; Lee et al., 2023; Kim et al., 2023; Heo et al., 2024; Lee et al., 2025), the prevalence of massive activations in\nPre-LN can severely disrupt outlier-aware compression algorithms. When aggressive low-precision compression is the goal,\nthe Pre-LN architecture‚Äôs tendency to generate extreme hidden state values therefore constitutes a particularly challenging\nobstacle.\nMeta AI‚Äôs publicly released OPT training logbooks and chronicles document repeated episodes of gradient divergence\nand loss spikes encountered while pre-training entirely in FP16 precision 9. Since FP16 saturates at an absolute value of\n65, 504, any hidden state activation that exceeds this threshold silently overflows, corrupting the forward pass and, through\nback-propagation, inducing erratic gradients. Earlier analyses of OPT (Zhang et al., 2022) therefore ascribe much of the\nobserved instability to numerical overflow, a view formalized in our Proposition 3.1, which shows how such out-of-range\nactivations propagate into severe gradient pathologies. These external reports provide further corroboration that architectures\nprone to generating large-magnitude activations‚Äîas Pre-LN does‚Äîrequire either a wider numerical format (e.g., BF16) or\nexplicit magnitude-regularization to ensure stable large-scale training.\n7https://en.wikipedia.org/wiki/Bfloat16_floating-point_format\n8https://en.wikipedia.org/wiki/Half-precision_floating-point_format\n9https://github.com/facebookresearch/metaseq/blob/main/projects/OPT/chronicles/10_perce\nnt_update.md\n36\n\nPeri-LN: Revisiting Normalization Layer in the Transformer Architecture\nL. Additional Discussions of Hidden State Representations\nAs shown in Figure 39(a), Post-LN exhibits smaller angular distances due to the LN being located on the main path, whereas\nPre-LN and Peri-LN begin with very similar states. As shown in Figure 39(c), at the end of training, Pre-LN tends to\nproduce more redundant hidden state representations compared to the others. This phenomenon may stem from Pre-LN‚Äôs\nrepeated residual additions, which amplify certain representations over others. We use 30B tokens trained 400M size model\nin this experiments. For dataset, we utilize 256 random samples of RedPajama-Data-1T (Computer, 2023) for this results.\nTo investigate further, we focus on module-output normalization, the primary factor distinguishing Pre-LN from Peri-LN.\nAs shown in Figure 39(b), the learnable scale starts around 1 in the early stages of training and gradually changes with\nincreasing depth. Because Peri-LN preserves the identity path, it appears to adjust the scale of the module output accordingly.\nThis suggests that the exponential growth of the main path‚Äôs magnitude in Pre-LN diminishes the relative contribution of\nindividual modules, resulting in more redundant hidden representations. Figure 39(d) shows that fixing the learnable scale\nof Peri-LN‚Äôs module output LN at 1 causes the main path contribution to decrease in deeper layers. This finding confirms\nthe role of module output normalization in controlling hidden state redundancy.\n(a) At initialization\n(b) Learnable scale Œ≥ in Output-LN\n(c) After 30B tokens training\n(d) Frozen Œ≥ in Output-LN\nFigure 39. Angular distance of hidden state is presented in Figure 39(a), 39(c), and 39(d). In Figure 39(b), we monitor Œ≥ of every\nOutput-LN in Peri-LN during training. We use 30B tokens trained 400M size model in this experiments.\n37\n"
    }
  ]
}