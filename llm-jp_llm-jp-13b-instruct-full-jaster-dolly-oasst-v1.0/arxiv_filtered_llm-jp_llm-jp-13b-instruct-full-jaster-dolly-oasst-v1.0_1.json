{
  "1-1 (Weights)": "The team states that the “LLM-jp model suite v1.0” was put online on 20 October 2023 and that a follow-up “LLM-jp model suite v2.0” appeared on 30 April 2024.  For every suite they distribute a 13 B-parameter base model together with the corresponding instruction-tuned derivatives.  They emphasise that the weights themselves, the full pre-training corpora and the fine-tuning data have all been placed in the public domain.  In total three fine-tuned checkpoints are listed—v1.0, v1.1 and v2.0—with the v1.0 fine-tuned model being released simultaneously with the v1.0 pre-trained checkpoint.  The language used (“We have made them public …”) makes it explicit that anyone can download the weights and associated data without special access agreements or licences.",
  "1-2 (Code)": "For training-time reproducibility the project supplied dedicated, open-source tooling.  When the v1.0 fine-tuned model was launched they released “llm-jp-sft”, a publicly available package that automates supervised fine-tuning (SFT).  In addition, the developers open-sourced a separate repository, “llm-jp-dpo”, which contains all scripts and configuration needed for Direct Preference Optimisation (DPO) training.  These statements make clear that code covering at least two major tuning stages—SFT and DPO—is freely accessible; the quotes do not mention inference-only tooling, so the disclosure specifically concerns training pipelines rather than serving code.",
  "1-3 (License)": "The project declares an overarching policy of “complete transparency,” explicitly committing to release “our models, corpora, and fine-tuning data … for both non-commercial and commercial use.”  This wording grants the four core rights—use, modification, redistribution and commercial exploitation—without restriction.  They further note that the English instruction corpora they rely on, “databricks-dolly-15k” and “oasst1,” were selected because their licences are compatible with the permissive stance taken by LLM-jp.  No additional caveats such as \"research-only\" or \"no derivatives\" appear in the quoted licence statements, reinforcing that the artefacts are available under a licence permitting full commercial reuse.",
  "1-4 (Paper)": "An official publication entitled simply “LLM-jp” is referenced as the canonical write-up of the work.  The authors describe it as a cross-organisational effort to advance Japanese large-language-model research and development.  They provide an explicit citation format—“LLM-jp (2024)”—and close the manuscript with a proverb underscoring their collaborative philosophy: “If you want to go fast, go alone. If you want to go far, go together.”  Although URLs are not quoted, the text confirms the existence of a formal paper that documents the models, the project structure and its goals.",
  "1-1 (Weights)__evidence": [
    {
      "source": "[pdf_text]",
      "quote": "Our first model suite, which we call the LLM-jp model suite v1.0, was released on October 20th, 2023. Subsequently, we released the next model suite, called the LLM-jp model suite v2.0, on April 30th, 2024. Each model suite provides an LLM with 13B parameters along with its fine-tuned variants. We have made them public with their pre-training corpora and fine-tuning datasets."
    },
    {
      "source": "[sections/2407.03963]",
      "quote": "Our first model suite, which we call the LLM-jp model suite v1.0, was released on October 20th, 2023. ... Each model suite provides an LLM with 13B parameters along with its fine-tuned variants. We have made them public with their pre-training corpora and fine-tuning datasets."
    },
    {
      "source": "[sections/Fine-tuning]",
      "quote": "To date, we have released three versions of our fine-tuned models: v1.0, v1.1, and v2.0. The fine-tuned model v1.0 was released alongside the pre-trained model v1.0."
    }
  ],
  "1-2 (Code)__evidence": [
    {
      "source": "[pdf_text]",
      "quote": "Upon the release of the fine-tuned model v1.0, we developed and released llm-jp-sft, an open-source tuning tool designed for supervised fine-tuning."
    },
    {
      "source": "[pdf_text]",
      "quote": "The training code specific to DPO, llm-jp-dpo, has also been made open-source."
    },
    {
      "source": "[sections/Fine-tuning]",
      "quote": "Upon the release of the fine-tuned model v1.0, we developed and released llm-jp-sft, an open-source tuning tool designed for supervised fine-tuning."
    },
    {
      "source": "[sections/Fine-tuning]",
      "quote": "The training code specific to DPO, llm-jp-dpo, has also been made open-source."
    }
  ],
  "1-3 (License)__evidence": [
    {
      "source": "[pdf_text]",
      "quote": "Against this background, LLM-jp started in May 2023 with the objective of developing Japanese LLMs on our own. Recognizing the need for widespread collaboration, we opted for complete transparency and decided to make everything openly available, from our models, corpora, and fine-tuning data to our discussions and failures, for both non-commercial and commercial use."
    },
    {
      "source": "[pdf_text]",
      "quote": "While many instruction datasets are available in English, we selected databricks-dolly-15k and oasst1, as they are widely used and provide suitable licenses for LLM-jp."
    }
  ],
  "1-4 (Paper)__evidence": [
    {
      "source": "[pdf_text]",
      "quote": "This paper introduces LLM-jp, a cross-organizational project for the research and development of Japanese large language models (LLMs)."
    },
    {
      "source": "[pdf_text]",
      "quote": "∗Please cite this paper as “LLM-jp (2024)”."
    },
    {
      "source": "[sections/2407.03963]",
      "quote": "This paper introduces LLM-jp, a cross-organizational project for the research and development of Japanese large language models (LLMs)."
    },
    {
      "source": "[sections/Conclusion]",
      "quote": "We would like to conclude this paper with a proverb that perfectly captures the spirit of LLM-jp’s activities: “If you want to go fast, go alone. If you want to go far, go together.”"
    }
  ]
}