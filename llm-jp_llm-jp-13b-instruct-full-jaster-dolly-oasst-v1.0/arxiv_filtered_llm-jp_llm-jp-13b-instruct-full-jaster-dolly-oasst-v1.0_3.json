{
  "2-3 (API)": "",
  "3-1 (Pre-training)": "The initiative’s first goal was to deliver model suite v1.0. Starting in May 2023, the Model Building WG set out to train and release a Japanese-centric 13 B-parameter LLM by autumn/winter 2023. To supply data, the Corpus Building WG assembled “corpus v1”, a mixture of readily available Japanese, English, and code corpora, specifically for pre-training the model v1.0; Shuhei Kurita subsequently binarized this corpus so that it could be streamed efficiently to the training servers. Conglong Li and Masahiro Tanaka prepared the Megatron-DeepSpeed framework that executed the large-scale run, and Shota Sasaki together with Jun Suzuki performed the actual pre-training. The resulting pre-trained model v1.0 is an LLM with 13 B parameters whose architecture is based on GPT-2. The team released the entire model suite, making both the checkpoints and their associated pre-training corpora publicly available. The same section notes that the build process also produced a later v2.0 series, but the concrete details described apply to v1.0.",
  "3-2 (Fine-tuning)": "For supervised fine-tuning, the project created three Japanese instruction datasets—jaster, Databricks Dolly-15k, and the OpenAssistant Conversations Dataset (oasst1)—and used them to obtain the fine-tuned model v1.0 from the 13 B-parameter base. To facilitate reproducibility, they simultaneously released an open-source tuning tool called “llm-jp-sft” (and later “llm-jp-sft25”) that implements the SFT pipeline for this model line. Hirokazu Kiyomaru, Takashi Kodama, and Hiroshi Matsuda carried out the training of the v1.0 fine-tuned models, and Fei Cheng together with Zhen Wan analyzed the generated outputs. All fine-tuning datasets and final checkpoints were published alongside the pre-training artifacts.",
  "3-3 (Reinforcement Learning)": "Building on the supervised fine-tuned release, the team generated a fine-tuned model v1.1 that is still rooted in the same pre-trained v1.0 weights. To enhance instruction-following performance they refined the instruction-tuning data and added Direct Preference Optimization (DPO), a preference-based reinforcement-learning technique. The DPO-enhanced v1.1 model was released in February 2024.",
  "2-3 (API)__evidence": [],
  "3-1 (Pre-training)__evidence": [
    {
      "source": "[pdf_text]",
      "quote": "Our initial milestone was to develop the model suite v1.0, and the Corpus Building WG worked on preparing a pre-training corpus to train the pre-trained model v1.0, the LLM with 13B parameters within this suite. To this end, we decided to use a mixture of readily available Japanese, English, and code corpora as our pre-training corpus."
    },
    {
      "source": "[pdf_text]",
      "quote": "The pre-trained model v1.0 uses a model architecture based on GPT-2 [42]."
    },
    {
      "source": "[sections/2407.03963]",
      "quote": "Our initial milestone was to develop the model suite v1.0, and the Corpus Building WG worked on preparing a pre-training corpus to train the pre-trained model v1.0, the LLM with 13B parameters within this suite. To this end, we decided to use a mixture of readily available Japanese, English, and code corpora as our pre-training corpus."
    },
    {
      "source": "[sections/2407.03963]",
      "quote": "Each model suite provides an LLM with 13B parameters along with its fine-tuned variants. We have made them public with their pre-training corpora and fine-tuning datasets."
    },
    {
      "source": "[sections/Model Building WG]",
      "quote": "The following subsections describe how we built the pre-trained models v1.0 and v2.0."
    },
    {
      "source": "[sections/Model Building WG]",
      "quote": "The pre-trained model v1.0 uses a model architecture based on GPT-2 [42]."
    },
    {
      "source": "[sections/Model Building WG]",
      "quote": "In May 2023, when this project started, the Model Building WG began its activities with the aim of building and releasing a 13B-parameter model specifically focusing on Japanese by autumn or winter 2023."
    },
    {
      "source": "[sections/Contributions]",
      "quote": "Shuhei Kurita binarized the corpus v1 for pre-training the model v1."
    },
    {
      "source": "[sections/Contributions]",
      "quote": "Conglong Li and Masahiro Tanaka prepared the Megatron-DeepSpeed framework for building the pre-trained model v1."
    },
    {
      "source": "[sections/Contributions]",
      "quote": "Shota Sasaki and Jun Suzuki trained the pre-trained model v1."
    }
  ],
  "3-2 (Fine-tuning)__evidence": [
    {
      "source": "[pdf_text]",
      "quote": "For the fine-tuned model v1.0, we constructed three types of Japanese instruction data: jaster, databricks-dolly-15k [10], and OpenAssistant Conversations Dataset (oasst1) [32]."
    },
    {
      "source": "[pdf_text]",
      "quote": "Upon the release of the fine-tuned model v1.0, we developed and released llm-jp-sft, an open-source tuning tool designed for supervised fine-tuning."
    },
    {
      "source": "[sections/2407.03963]",
      "quote": "Each model suite provides an LLM with 13B parameters along with its fine-tuned variants. We have made them public with their pre-training corpora and fine-tuning datasets."
    },
    {
      "source": "[sections/Fine-tuning]",
      "quote": "For the fine-tuned model v1.0, we constructed three types of Japanese instruction data: jaster, databricks-dolly-15k [10], and OpenAssistant Conversations Dataset (oasst1) [32]."
    },
    {
      "source": "[sections/Fine-tuning]",
      "quote": "Upon the release of the fine-tuned model v1.0, we developed and released llm-jp-sft25, an open-source tuning tool designed for supervised fine-tuning."
    },
    {
      "source": "[sections/Contributions]",
      "quote": "Hirokazu Kiyomaru, Takashi Kodama, and Hiroshi Matsuda trained the fine-tuned models v1.0."
    },
    {
      "source": "[sections/Contributions]",
      "quote": "Fei Cheng, Zhen Wan analyzed the output of the fine-tuned models v1.0."
    }
  ],
  "3-3 (Reinforcement Learning)__evidence": [
    {
      "source": "[pdf_text]",
      "quote": "In the fine-tuned model v1.1, which is based on the same pre-trained model v1.0, we improved the instruction-following ability by refining the instruction-tuning data and adding Direct Preference Optimization (DPO), and released it in February 2024."
    },
    {
      "source": "[sections/Fine-tuning]",
      "quote": "In the fine-tuned model v1.1, which is based on the same pre-trained model v1.0, we improved the instruction-following ability by refining the instruction-tuning data and adding Direct Preference Optimization (DPO), and released it in February 2024."
    }
  ]
}