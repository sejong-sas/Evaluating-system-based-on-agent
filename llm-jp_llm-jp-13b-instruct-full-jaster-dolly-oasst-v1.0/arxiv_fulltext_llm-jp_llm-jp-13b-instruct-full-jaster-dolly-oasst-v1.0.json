{
  "model_id": "llm-jp/llm-jp-13b-instruct-full-jaster-dolly-oasst-v1.0",
  "full_texts": [
    {
      "arxiv_id": "2407.03963",
      "full_text": "LLM-jp:\nA Cross-organizational Project for the Research and\nDevelopment of Fully Open Japanese LLMs\nLLM-jp∗\nAbstract\nThis paper introduces LLM-jp, a cross-organizational project for the research and\ndevelopment of Japanese large language models (LLMs). LLM-jp aims to develop\nopen-source and strong Japanese LLMs, and as of this writing, more than 1,500\nparticipants from academia and industry are working together for this purpose.\nThis paper presents the background of the establishment of LLM-jp, summaries\nof its activities, and technical reports on the LLMs developed by LLM-jp. For the\nlatest activities, visit https://llm-jp.nii.ac.jp/en/.\n1\nIntroduction\nLarge language models (LLMs), exemplified by GPT-4 [37], demonstrate remarkable capabilities.\nLLMs have achieved many long-standing goals of traditional natural language processing (NLP),\nshifting the primary focus of NLP research towards elucidating their intelligence, ensuring their\nsafety, and exploring their integration and coexistence with humans in society.\nHowever, there exist significant issues with LLMs. First, the research and development of LLMs\nrequire significant computational resources and substantial budgets, predominantly controlled by\na few major organizations. Moreover, the specifics of the strongest models — including their\narchitecture, pre-training corpus, training methodologies, and tuning data — are no longer publicly\naccessible. Additionally, several critical issues, such as hallucination and safety, must be addressed\nfor LLMs to achieve widespread societal acceptance in the future.\nThere are also national concerns specific to Japan. The representation of Japanese in the GPT-3\ndataset is just 0.11%2, which results in inferior comprehension and generation of Japanese compared\nto English. Furthermore, there is a worry that Japanese culture and activities may be overshadowed if\nmodels predominantly trained in English become the global standard. From an economic security\nperspective, it is crucial to consider the potential outflow of Japan’s intellectual assets when entirely\nrelying on foreign models.\nAgainst this background, LLM-jp started in May 2023 with the objective of developing Japanese\nLLMs on our own. The research and development of LLMs is now a big science in terms of both\ncomputational and human resources. Recognizing the need for widespread collaboration, we opted for\ncomplete transparency and decided to make everything openly available, from our models, corpora,\nand fine-tuning data to our discussions and failures, for both non-commercial and commercial use.\nLLM-jp began as a small study group of about 30 NLP researchers. LLM-jp garnered increasing\nsupport for its concept over time, growing to over 1,500 participants by June 2024. Study groups\nhave been held monthly since the establishment of LLM-jp in a hybrid (in-person and online) manner,\nto introduce the latest advances in LLMs and present the activity reports from LLM-jp.\n∗Please cite this paper as “LLM-jp (2024)”. Contribution statements can be found at the end of the document.\nCorrespondence regarding this paper can be sent to llm-jp@nii.ac.jp.\n2https://github.com/openai/gpt-3/blob/master/dataset_statistics/languages_by_\nword_count.csv\narXiv:2407.03963v2  [cs.CL]  30 Dec 2024\n\n2023.11\nABCI's 2nd LLM construction support program \n(investigation for training 175B model)\n2024.2\nMETI program (GENIAC)\n(175B-class model training will start in 2024.4)\n2023.5\nVolunteer study group of\n30 NLP researchers\n2023.6\nStarted to build 13B LLMs on mdx \n(a platform for data-empowered society)\n2023.10\nReleased LLM-jp-13B v1.0\n2024.4\nReleased LLM-jp-13B v2.0\nStarted to build 175B-class model\nFigure 1: Timeline of key activities in LLM-jp.\nFor the development of LLMs, three working groups (WGs) were first established: the Corpus Build-\ning WG, Model Building WG, and Fine-tuning and Evaluation WG. Subsequently, the Computational\nInfrastructure WG was formed to address computational infrastructure challenges. Weekly online\nmeetings and Slack discussions facilitated communication among the groups. As the project evolved,\nthe Academic Domain WG and Safety WG were also created.\nOur first model suite, which we call the LLM-jp model suite v1.0, was released on October 20th, 2023.\nSubsequently, we released the next model suite, called the LLM-jp model suite v2.0, on April 30th,\n2024. Each model suite provides an LLM with 13B parameters along with its fine-tuned variants. We\nhave made them public with their pre-training corpora and fine-tuning datasets.\nIn the following, we present the activities of the main WGs that played a central role in the construction\nof our LLMs and future prospects.\n2\nCorpus Building WG\n2.1\nOverview\nThe main role of the Corpus Building WG is to build a pre-training corpus and a tokenizer needed for\nLLM construction and pass them to the Model Building WG.\nIn the following subsections, we describe our work for the pre-trained models in our model suites\nv1.0 and v2.0. Then, we explain the corpus search function, which is one of our advantages. Finally,\nwe summarize our ongoing and future work.\n2.2\nWork for Pre-trained Model v1.0\nOur initial milestone was to develop the model suite v1.0, and the Corpus Building WG worked on\npreparing a pre-training corpus to train the pre-trained model v1.0, the LLM with 13B parameters\nwithin this suite. The main purpose of this development was to experience the entire development\nprocess of an LLM as soon as possible.\nTo this end, we decided to use a mixture of readily available Japanese, English, and code corpora as\nour pre-training corpus. As for the corpus size, we followed the Chinchilla scaling law [20], which\nsuggests using roughly 20 tokens per parameter. Eventually, we constructed the corpus v1 consisting\nof over 260B tokens. The statistics of this corpus are listed in Table 1. From this corpus, we extracted\na pre-training dataset that consists of 130B Japanese, 130B English, and 10B code tokens, resulting\nin a total of 270B tokens.\nAs for the Japanese portion, we used the Japanese parts of Wikipedia and the multilingual C4\n(mC4) dataset [57]. Since the Japanese part of mC4 was noisy, we filtered out documents that were\nconsidered low-quality or harmful. Table 2 shows filters adopted for this purpose. For the English\n2\n\nTable 1: Statistics of sub-corpora in the corpus v1.\nLanguage\nSub-corpus\nTokens\nJapanese\nWikipedia\n1B\nmC4\n136B\nEnglish\nWikipedia\n5B\nPile\n176B\nCode\nStack\n148B\nTable 2: Filters and conversions used for the corpus v1.\nFilter / Conversion\nDescription\nHasValidUrlDomain\nFilter out documents with URLs from domains rarely used in Japan.\nIsNotJapanese\nFilter out documents that do not contain hiragana or katakana characters.\nIsNotEthical\nFilter out documents that include toxic and/or offensive words.\nRemoveUrl\nRemove URLs from documents.\nRemoveCode\nRemove code-like text spans from documents.\nand code portions, we utilized the Pile dataset [16] and the Stack dataset [29], respectively. To adjust\nthe corpus size, we sampled documents from these two sources accordingly.\nWe developed tokenizers based on SentencePiece with the unigram mode [30]. As a multi-lingual\ntokenizer considering Japanese, we first explored the tokenizer developed in the project “Development\nof a distributed parallel learning method for large-scale language models in the policy-oriented\nframework of the supercomputer Fugaku”3, which we refer to as the tokenizer v1.0. The construction\nprocess is as follows:\n1. Preparing training data to construct the tokenization models for each language (i.e., Japanese\nand English).\n2. In order to prevent the tokenization models from learning tokens longer than Japanese word\nboundaries, Japanese data was pre-tokenized using the morphological analyzer MeCab4\nwith the Japanese morphological dictionary JumanDIC5. This pre-tokenization specifically\naimed to avoid learning tokens such as browser operation phrases, which are frequently\nincluded in web corpus, and meaningless long phrases, which are typically used only on\nspecific websites. Pre-tokenization was also performed for sequences including characters\nother than the alphabet, hiragana, katakana, and kanji into a sequence of single characters to\nprevent the constructed vocabulary from including tokens with a sequence of symbols and\nnumbers.\n3. Constructing SentencePiece models of the unigram tokenizer for Japanese and English using\nthe pre-processed training data, independently.\n4. Merging the vocabularies of the above two tokenization models, removing duplicate tokens.\n5. Re-estimating unigram scores of tokens in the merged vocabulary with the EM algorithm\nover the training data6. Here, data without pre-processing was used to enable the final\ntokenization model to be used without any pre-tokenization.\nAlthough the construction process seems complicated, the obtained model can be used as a pure\nSentencePiece model. This multi-step process for the model construction enables us to control the\nratio of the vocabulary size for each language.\nHowever, because the tokenizer v1.0 was originally developed for the Fugaku project, we needed to\nre-train the tokenizer model with the corpus used in the LLM-jp project, the corpus v1. In addition,\n3https://www.titech.ac.jp/english/news/2023/066798\n4https://taku910.github.io/mecab/\n5https://hayashibe.jp/tr/mecab/dictionary/juman\n6Existing implementation of multigram language model [12] was used, which is available at https://\ngithub.com/tatHi/multigram.\n3\n\n2013-2016\n2017-04\n2017-09\n2017-13\n2017-17\n2017-22\n2017-26\n2017-30\n2017-34\n2017-39\n2017-47\n2017-51\n2018-05\n2018-09\n2018-13\n2018-17\n2018-22\n2018-26\n2018-30\n2018-34\n2018-39\n2018-43\n2018-47\n2018-51\n2019-04\n2019-09\n2019-13\n2019-18\n2019-22\n2019-26\n2019-30\n2019-35\n2019-39\n2019-43\n2019-47\n2019-51\n2020-05\n2020-10\n2020-16\n2020-24\n2020-29\n2020-34\n2020-40\n2020-45\n2020-50\n2021-04\n2021-10\n2021-17\n2021-21\n2021-25\n2021-31\n2021-39\n2021-43\n2021-49\n2022-05\n2022-21\n2022-27\n2022-33\n2022-40\n2022-49\n2023-06\n2023-14\n2023-23\nCommon Crawl Dumps\n0\n1\n2\n3\n4\n5\n6\n7\n8\n# Tokens (B)\n  Total tokens in Ja: 285.5 B  \nFigure 2: Token counts over Common Crawl dumps in the v2 Japanese corpus.\nTable 3: Filters and conversions used in Uzushio for the corpus v2.\nFilter / Conversion\nDescription\nDocLength\nThe length of each document.\nHiraganaRatio\nThe upper and lower limits filter of the appearance rate of\nHiragana characters.\nLinkCharRatio\nThe upper and lower limits filter of the appearance rate of\nhyperlinks in characters.\nMergeListTag\nSummarizing HTML lists into one paragraph.\nMarkdownizeHeading\nConverting HTML headings into the Markdown format.\nNoContentDOM\nFiltering HTMLs with navigational DOM.\nLargeFreqParagraphs\nRemoving frequent paragraphs in documents.\nKenLMParagraphPerplexity\nPerplexity-based filter, tokenization by Sudachi10.\nCompressionRate\nThe upper and lower limits filter of the zip-compression rate.\nWordTypes\nDocument filter by inappropriate word lists.\nDocLength\nDocument length in characters.\nDeduplicateDocumentsPercentile\nDe-duplication with probabilistic document identification\nby SimHash.\nsome specifications of the tokenizer v1.0, such as the handling of white spaces and line breaks, were\nleft open for discussion.\nTherefore, based on the idea of the tokenizer v1.0, we constructed the tokenizer v2.1 for use in the\nmodel suite v1.0 by using a subset of the corpus v1 and extending the target languages to Japanese,\nEnglish, and code. Besides, we adjusted the handling rules of white spaces, line breaks, and special\ntokens, which resulted in efficient tokenization in the corpus v1. The vocabulary of the tokenizer v2.1\nis constructed from 30,000 tokens for Japanese, 20,000 tokens for English, and 10,000 tokens for\ncode. The final size of the vocabulary is approximately 50,000, which indicates that about 10,000\ntokens are duplicated among the three vocabularies.\nThe corpus v1 and tokenizer v2.1 were handed over to the Model Building WG in August 2023\nand used for pre-training. The Model Building WG requested the highest quality corpus used at the\nend of the pre-training. In response, we applied the filtering methods described earlier with stricter\nthresholds to the original corpora and extracted 27B high-quality tokens.\nThe corpus v1 is publicly available7. The code to construct the corpus is also released to the public8.\nBesides, the tokenizer v2.1 and its corresponding scripts are available for download9.\n4\n\n2.3\nWork for Pre-trained Model v2.0\nTo develop the LLM with 13B parameters included in our model suite v2.0, called the pre-trained\nmodel v2.0, we created a larger and higher-quality corpus, termed the corpus v2.\nTo construct a Japanese corpus to this end, we extracted Japanese documents from the entire Common\nCrawl and applied deduplication and filtering for them. The corpus v2 construction script was\ndeveloped in Uzushio11, an Apache Spark-based corpus preprocessing tool developed for processing\nbillion-token scale training corpus from web data such as Common Crawl. Uzushio provides\na framework for processing such as similarity-based duplicate detection and filtering. Table 3\nsummarizes the filters and conversions performed to construct the Japanese portion of the corpus\nv2. The filtering pipeline consisted of deduplication and rule-based filtering steps. In de-duplication,\nUzushio performs similarity-based document identification based on the SimHash algorithm. This\nallows Uzushio to apply multiple strengths of de-duplication to documents from a web corpus. The\nstatistics of the Japanese corpus from Common Crawl dumps are presented in Figure 2. We used the\npublicly available Common Crawl dumps from 2013 to the middle of 2023. We merged the Common\nCrawl dumps from 2013 to 2016 because they included fewer Japanese documents than the later\ndumps. The total extracted Japanese tokens were 285.5B12. Further analyses on the v2 corpus are\ndiscussed in Enomoto et al. [13].\nAs for the English and code portions, we used the Pile and Stack datasets, respectively, following the\ncorpus v1. Besides, we included Japanese and Wikipedia as high-quality text corpora in the corpus\nv2.\nThe corpus v2 has been made publicly available.13\nAs for the tokenizer, we newly developed the tokenizer v2.2. The training flow of the tokenization\nmodel is the same as that of the tokenizer v2.1. The size of the vocabulary was expanded to 96,86714.\nBesides, while the tokenizer v2.1 used a single token per character for symbols to conserve vocabulary,\nwhich resulted in over-segmentation of English and code text and reduced tokenization efficiency, in\nthe tokenizer v2.2, the vocabulary is constructed in a way that allows for symbol sequences, and the\ntokenization efficiency is improved, especially for English and code text.\n2.4\nCorpus Search\nIn addition to corpus construction, the Corpus Building WG is also working on developing a corpus\nsearch function, aiming to attribute generated text to the training corpus. This function will be used\nto analyze generated texts and potentially uncover the principles of LLMs from the perspective of the\ntraining corpus. For example, we plan to use this system to investigate the causes of hallucinations in\ngenerated text.\nCurrently, two search algorithms are being explored: sparse vector search and dense vector search.\nSparse vector search retrieves documents based on the superficial similarity between texts. It is\nparticularly effective when the generated texts contain distinctive words. Additionally, it also helps\nidentify verbatim memorization [6] in generated texts. Dense vector search, on the other hand,\nretrieves documents based on the similarity between text embeddings computed by pre-trained text\nembedding models. Compared to sparse vector search, dense vector search excels at considering the\nmeaning of texts. Furthermore, by using multilingual text embedding models (e.g., LaBSE [14]),\nit can retrieve semantically similar documents across different languages, which helps analyze the\ncross-lingual transfer ability of LLMs [41].\n7https://gitlab.llm-jp.nii.ac.jp/datasets/llm-jp-corpus-v1\n8https://github.com/llm-jp/llm-jp-corpus\n9https://github.com/llm-jp/llm-jp-tokenizer/releases/tag/v2.1\n11https://github.com/WorksApplications/uzushio\n12In the training of the pre-trained model v2.0, we sampled approximately 130B tokens, following the\nChichilla scaling law.\n13https://gitlab.llm-jp.nii.ac.jp/datasets/llm-jp-corpus-v2\n14You can see that the vocabulary size is 97,024 after loading the language model (i.e., vocab_size in\nconfig.json). This is a result of rounding up the vocabulary size to multiples of 256 to make the SoftMax layer\ntraining process on the GPU efficient.\n5\n\n2.5\nOngoing and Future Work\nWe decided to build a 175B-class model as the next target of model building in LLM-jp, and are now\nbuilding the corpus v3. This new corpus will consist of approximately 2T tokens that cover Japanese,\nEnglish, some Asian languages, and code.\nIn our corpora, the mixing ratio of Japanese and English is set at 50-50, but we believe that further\nstudy is needed on the mixing ratio and the size of the corpora. In addition to Wikipedia and\nweb documents, we are negotiating with relevant organizations to use high-quality corpora and\ncorpora from various domains, such as scientific and technical papers, patent documents, and domain\ndocuments from the medical field.\n3\nComputational Infrastructure WG\nLLM-jp used mdx15 as the computing resource for training LLMs. mdx is a cloud computing\nenvironment consisting of CPUs and GPUs leveraging virtualization technologies [51]. mdx provides\nusers with isolated tenants involving virtual machines, virtual networks, and storage. mdx is operated\nby 11 national organizations in Japan, including nine national universities, the National Institute of\nInformatics, and the National Institute of Advanced Industrial Science and Technology. In May 2023,\nmdx had just started official operation and had GPU resources available; thus, we decided to use mdx\nto build the LLM-jp model.\nA GPU node on mdx has eight NVIDIA A100 40GB SXM model GPUs and two Intel Xeon Platinum\n8369 model CPUs. The network is a full-bisection spine-leaf topology where nodes are connected\nwith four 100 Gbps links. The network supports RoCE (RDMA over Converged Ethernet), an\nEthernet-based RDMA protocol, over Virtual eXtensible LAN (VXLAN) for network virtualization.\nThus, GPUs can use RDMA to communicate with other GPUs. In the LLM-jp configuration, we built\na GPU cluster with 16 nodes (128 GPUs) and allocated all GPUs and two 100 Gbps NICs to each\nvirtual machine.\nWe faced performance issues when we constructed the cluster with 128 GPUs. When we built the\npre-trained model v1, there were packet losses in the GPU data communication because ECMP\n(Equal Cost Multi Path) was not working properly for RoCE packets on the network switch. The\nperformance issue could not be resolved by the start date of the pre-training of the pre-trained model\nv1, so we reduced the scale of the cluster from 16 nodes (128 GPUs) to 12 nodes (96 GPUs). For the\npre-trained model v2.0, we fixed the ECMP issue and used all 16 nodes. Computational Infrastructure\nWG will share the operational expertise on GPU clusters with other projects.\n4\nModel Building WG\n4.1\nOverview\nThe role of the Model Building WG is to pre-train language models. The main tasks include:\n1. preprocessing the pre-training corpus (such as converting it into a binary format for faster\ndata loading during pre-training),\n2. performing the pre-training, and\n3. converting the checkpoints from the pre-training into a model format that is suitable for\nfine-tuning.\nThe following subsections describe how we built the pre-trained models v1.0 and v2.0. Table 4\nsummarizes the configuration for these models.\n4.2\nWork for Pre-trained Model v1.0\nIn May 2023, when this project started, the Model Building WG began its activities with the aim\nof building and releasing a 13B-parameter model specifically focusing on Japanese by autumn or\n15https://mdx.jp/en/\n6\n\nTable 4: Configurations for the pre-trained models v1.0 and v2.0\nv1.0\nv2.0\nModel Size\n13B params\n13B params\nCorpus size\n270(+27)B\n255B\nCorpus version\nv1\nv2\nComputational environment\nmdx 12 nodes\nmdx 16 nodes\nPre-training tool\nMegatron-DeepSpeed\nMegatron-LM\nBase model architecture\nGPT2\nLlama2\nTokenizer version\nv2.1\nv2.2\nVocabulary size\n50k\n100k\nwinter 2023. At the start of activities in May 2023, no one in the Model Building WG had solid\nknowledge or experience in pre-training language models with over 10B parameters using more\nthan 10 computing nodes. Therefore, all participants in the WG experienced what was necessary for\npre-training step by step, gaining knowledge and experience through a process of trial and error.\nFirst, to pre-train a language model, we need to prepare a training program (code). While there\nwas the option to develop our own training program, our goal was to build a 13B-parameter model\nwithin a few months, making the option infeasible. At the start of the project in May 2023, there\nwere several tools available for pre-training language models with over 10B parameters, so we\ndecided to use them. Specifically, we considered Megatron-DeepSpeed16, GPT-NeoX17, and LLM\nFoundry18 as candidates. Pre-training requires massive computing resources, such as using more\nthan 10 GPU nodes for over 10 days. Therefore, there was not enough time to run multiple training\nsessions simultaneously or to use multiple tools to create and compare several models side-by-side.\nConsidering several factors, including the fact that several participants had experience with it and that\ndevelopers involved in DeepSpeed had been participating in LLM-jp activities from the beginning,\nwe regarded Megatron-DeepSpeed as the primary tool for Model Building WG’s activities. However,\nit was also decided to use GPT-NeoX and LLM Foundry in parallel up to the verification of training\nspeed and stability. Each tool was assigned to a person in charge, and teams were formed to compare\nthe results. Eventually, we chose to build the pre-training model using Megatron-DeepSpeed, as\nwe did not find GPT-NeoX and LLM Foundry to be clearly superior in terms of execution speed or\ntraining stability (just to clarify, GPT-NeoX and LLM Foundry were not inferior).\nFirst of all, the training speed of the language model was verified. For example, when training a\nlanguage model with more than 10B parameters on a training corpus of more than 100B tokens, it is\nusually necessary to use a computing environment of more than 10 GPU nodes to finish the training\nin a realistic time. Even if the same language model learning configuration is used, the learning speed\ncan vary greatly depending on the characteristics of the computer cluster environment. Therefore,\nit is necessary to find the appropriate learning settings for each computer environment used. The\ndetails of the computing environment and the various issues related to it are summarised in Section 3.\nWe searched for the optimal setting according to FLOPS (floating point operations per second), an\nindex that is independent of the size of the model and differences in the computer environment and\nthus often used in existing research as a measure of learning speed. In Megatron-Deepspeed, there\nare many configurable settings related to the learning speed of language models, including model\nparallelism (tensor parallel, pipeline parallel) [34, 48], data parallelism, batch size, and a setting\ncalled ZeRO [44], which mainly determines the trade-off between GPU/CPU memory utilization and\nspeed. Various settings were prepared by combining the values of each item, and measured learning\nspeeds were collected for each. Finally, the setting that produced the most stable and highest FLOPS\nvalue was adopted.\nBy measuring the actual processing speed, we predicted the total time required for the model\nconstruction once the size of the training corpus is determined. The total learning time at that\ntime was predicted as follows: Using a 12 node mdx computing cluster to train a model with 13B\nparameters, the measured processing speed was 170K tokens/second on average. Therefore, the\n16https://github.com/microsoft/Megatron-DeepSpeed\n17https://github.com/EleutherAI/gpt-neox\n18https://github.com/mosaicml/llm-foundry\n7\n\nestimated total time required for training the 13B parameter model with a corpus of 270B tokens was\nroughly 441.2 hours or about 18.4 days.\nWe had been preparing to learn a language model using a training corpus of 270B tokens, but as\nthe volume of the training corpus was expected to increase, we considered a learning method that\nwould enable continuous pre-training even if the training corpus increased sequentially. Here, we\ntried a method in which the training data of 270B tokens is divided roughly into 10 chunks of 27B\ntokens, and these tokens are trained one by one. Assuming that about one trillion tokens of data\nwould be learned in the future, we applied a setting for learning one trillion tokens to the learning\nrate scheduler, which is the cosine decay scheduler typically used in the literature of pre-training\nlanguage models. We also asked the Corpus Building WG to prepare a training corpus of 27B tokens\nselected from 270B tokens, which were considered to be of high quality, and used this 27B token\ntraining corpus at the end of the pre-training. When we trained the final 270 billion tokens, we also\nrapidly decreased the learning rate to the predefined final learning rate for overall pre-training. This\ndecrease started from the learning rate at the end of the preceding 27B tokens training, using the\nsame cosine scheduler but with a different hyperparameter setting.\nAnother aspect to consider when pre-training language models is the stability of the training. In LLM\npre-training, we often observe that the model cannot be learned effectively due to loss divergence,\noften called loss explosion and loss spike. At that moment, the mechanism of loss divergence had\nnot been fully elucidated. Therefore, we need to explore and use a setting in which loss divergence\noccurs as little as possible. We are basically required to deal with this problem through trial and error,\nbut fortunately, no unresolvable loss divergence occurred in our pre-training.\nThe pre-trained model v1.0 uses a model architecture based on GPT-2 [42]. Although GPT-2 is a\nrelatively old model architecture, and while a newer one was possible, we deemed it more appropriate\nto use a well-established and stable one, considering the need for a reliable model for many users.\nAdditionally, converting the model checkpoints to a format compatible with the Hugging Face\nTransformers library19 is a common practice, making it crucial to ensure the model can be converted.\nUnfortunately, the Transformers library does not support the Megatron-DeepSpeed model format used\nin our training, so a conversion script is needed. From this perspective, while Megatron-DeepSpeed\noffers a script for converting to the Hugging Face Transformers format, it only supports GPT-2-based\nmodels. Therefore, without a custom conversion script, we could only use GPT-2-based models with\nMegatron-DeepSpeed. Given our limited resources and the fact that this was LLM-jp’s first attempt,\nwe concluded that using the GPT-2 model was the safest choice.\nFollowing the various studies described above, the preliminary learning of the language model for\npublic use began in earnest around the end of August. In practice, learning the target 13B parameter\nmodel out of the blue was also risky, so learning the 1.3B parameter model was carried out as a\npre-production exercise. Eventually, the pre-training of the 13B parameter model took 26 days.\nDuring the training process, there was trouble that the training stopped several times, and it was\nnecessary to restart the training manually. If the training had proceeded without any problems, it\ncould have been carried out in about 21 days at the shortest. The model was then handed over\nto the Fine-tuning and Evaluation WG, which completed the work of building the model v1.0 for\npublication in the Model Building WG.\n4.3\nWork for Pre-trained Model v2.0\nAs mentioned in the previous section, the pre-trained model v1.0 was our initial attempt, and we had\na time constraint for its construction and release. This means that our primary focus was on quickly\nbuilding the model on schedule rather than investigating how to obtain a world-class, high-quality\nmodel. To identify a better pre-training configuration for the pre-trained model v2.0, we conducted\nexperiments prior to beginning its construction.\n4.3.1\nPreliminary Experiments: Towards Better Pre-trained Model v2.0\nWe have changed several pre-training configurations of the pre-trained model v1.0 for model v2.0\nsince we aimed to improve the overall performance. Regarding the model architecture, we decided\nto replace GPT-2 used in model v1.0 with the Llama architecture, which was starting to gain wide\nadoption at that time. We conducted experiments to determine the best configuration. The primary\n19https://github.com/huggingface/transformers\n8\n\nTable 5: Experimental configurations for comparing the effectiveness of selected training corpus and\nvocabulary size.\nExp.\nParam.\nJapanese\nJapanese\nTokenizer\nVocab.\nID\nsize\ncorpus\ncorpus size\nVersion\nsize\nExp(a)\n7B\nllmjp v1(ja)\n(134B)\nv2.2\n50k\nExp(b)\n7B\nSwallow\n(147B)\nv2.2\n50k\nExp(c)\n7B\nllmjp v2β(ja)\n(135B)\nv2.2\n50k\nExp(d)\n7B\nllmjp v1(ja)\n(131B)\nv2.2\n100k\nExp(e)\n13B\nllmjp v2β(ja)\n(135B)\nv2.2\n50k\nExp(f)\n7B\nllmjp v2β(ja)\n(250B)\nv2.2\n50k\nExp(g)\n13B\nllmjp v2β(ja)\n(131B)\nv2.2\n100k\nTable 6: Experimental results for comparing the effectiveness of selected training corpus and vocabu-\nlary size. In the title raw, llm-jp and JVQA represent the llm-jp-eval benchmark and the Japanese\nVicuna QA benchmark, respectively.\n(C1) Corpus type\n(C2) Vocab. size\n(C3) Model size\n(C4) Corpus size\nExp. ID llm-jp JVQA\nExp(a) 0.539 40.36\nExp(b) 0.561 35.38\nExp(c) 0.562 43.52\nExp. ID llm-jp JVQA\nExp(c) 0.562 43.52\nExp(d) 0.548 34.26\nExp(e) 0.577 47.00\nExp(g) 0.576 50.74\nExp. ID llm-jp JVQA\nExp(c) 0.562 43.52\nExp(e) 0.577 47.00\nExp. ID llm-jp JVQA\nExp(c) 0.562 43.52\nExp(f) 0.556 49.88\nfactors of evaluation included the vocabulary size and pre-training corpus type. For vocabulary size,\nwe compared 50k and 100k while the tokenizer was given and fixed to v2.2. As for the pre-training\ncorpus type, we examined three types of Japanese sub-corpora: the Japanese part in the corpus v1\nused for constructing the model v1.0, the Swallow corpus20 used for continual pre-training from\nLlama 2, and the corpus v2 prepared specifically for the model v2.0. We refer to these three Japanese\ndatasets for pre-training as llmjp v1(ja), Swallow, and llmjp v2β(ja), respectively. Regarding\nthe English and Code parts of the dataset for the pre-training, we reused the identical sub-corpora\nto build for the model v1 (Table 1). We sampled approximately 114.5B and 8.7B tokens (under the\n100k vocabulary) from these sub-corpora, respectively.\nWe prepared several configurations based on the comparison factors of vocabulary sizes and pre-\ntraining corpus types to clarify the effectiveness of each aspect. Table 5 summarizes the configurations\nused for our preliminary experiments. We used Megatron-LM21 for all experiments in this section\ninstead of Megatron-Deepspeed used for building pre-trained model v1.0.\nThe following four perspectives of comparison ((C1), (C2), (C3), and (C4)) are the primary\nintentions of our preliminary experiments:\n(C1) Comparing Exp(a), Exp(b), and Exp(c), we attempted to investigate which one of the\nJapanese corpora can be better in terms of pre-training. Remember that the corpus v1 (ja),\nSwallow, and llmjp v2β(ja) can contain identical and near identical texts. Therefore,\nit’s not as straightforward as simply combining these three corpora into one for pre-training\npurposes. This is because changes in data distribution and the inclusion of duplicate data\ncould potentially harm and degrade the pre-training process.\n(C2) Comparing Exp(a) and Exp(d) and also Exp(e) and Exp(g), we can see the effectiveness\nof increasing vocabulary size from 50k to 100k.\n(C3) Comparing Exp(c) and Exp(e), we can see the effectiveness of increasing model parameter\nsize.\n(C4) Comparing Exp(c) and Exp(f), we can see the effectiveness of increasing corpus size.\n20https://tokyotech-llm.github.io/swallow-corpus\n21https://github.com/NVIDIA/Megatron-LM\n9\n\nAfter pre-training for each configuration, we performed simple fine-tuning on each pre-trained model\nand evaluated the performance by llm-jp-eval and Japanese Vicuna QA benchmarks, as introduced in\nSection 5.3. Table 6 shows the results. The findings from these results are as follows:\n1. According to the (C1) result, the corpus v2 (llmjp v2β(ja)) seems to perform better than\nthe corpus v1 (llmjp v1(ja)) and Swallow corpus.\n2. According to the (C2) result, the performance difference between vocabulary sizes of 50k\nand 100k seems marginal, and we are unable to determine which is better clearly.\n3. From the (C3) result, the model size significantly affects the performance; this is the\nconsistent result of common knowledge like scaling laws.\n4. From the (C4) result, the corpus size for pre-training also affects the performance.\nThese results led to the decision on the model setting for v2.0, described in Table 4.\n4.3.2\nConstructing Pre-trained Model v2.0\nAs demonstrated in the preliminary experiment, Exp(g) appears to deliver the best performance.\nTherefore, we decided to adopt the model trained in Exp(g) as the pre-trained model v2.0. Further-\nmore, with the model trained in Exp(g) being adopted as the pre-trained model v2.0, the training\ndata used in Exp(g) was also finalized as corpus v2.\n4.4\nOngoing and Future Work\nAs described in Section 2.5, we plan to build a 175B-parameter-class model as the next target of\nmodel building in LLM-jp. In practice, we have already tried pre-study using a GPT-3 compliant\nmodel on a trial basis using the LLM construction support program at ABCI22 and have identified\nsome issues to consider, such as loss-spike. We are preparing the implementation to mitigate such\nissues. The Model Building WG is diligently working to build a 175B-parameter-class language\nmodel, trained with a dataset of over 1T tokens (called the corpus v3), publicly available this autumn.\nFor this purpose, we have submitted (and been selected) to an LLM construction support program at\nthe Ministry of Economy, Trade and Industry (METI) in Japan, called GENIAC23.\n5\nFine-tuning and Evaluation WG\n5.1\nOverview\nThis section introduces our efforts on fine-tuning and evaluation of LLMs. Pre-trained language\nmodels can produce natural and fluent text following input text (prompts), but they do not necessarily\nproduce responses that humans would expect in response to the input. To develop interactive LLMs\nlike ChatGPT, it is essential for them to have the ability to generate appropriate responses to user\ninput; i.e., they need to be aligned with human values [39]. Alignment is an essential issue in LLM\nresearch and development, and fine-tuning is an indispensable step in achieving this.\nEvaluation is another critical issue for the development and deployment of LLMs. A conventional\nmethod for evaluating NLP systems has been to design a specific task, such as question answering\nand machine translation, and to develop test data for each designed task. However, this method is\ninsufficient for the evaluation of LLMs because LLMs are used in a variety of downstream tasks.\nWe therefore develop evaluation frameworks that can assess diverse natural language understanding\ncapabilities of LLMs.\n5.2\nFine-tuning\nTo date, we have released three versions of our fine-tuned models: v1.0, v1.1, and v2.0. The fine-tuned\nmodel v1.0 was released alongside the pre-trained model v1.0. In the fine-tuned model v1.1, which is\nbased on the same pre-trained model v1.0, we improved the instruction-following ability by refining\nthe instruction-tuning data and adding Direct Preference Optimization (DPO), and released it in\n22https://abci.ai/ja/link/llm_support_program2023.html\n23https://www.meti.go.jp/policy/mono_info_service/geniac/index.html\n10\n\nTable 7: Datasets for fine-tuning. Dagger (†) indicates that the dataset was automatically translated\nfrom English.\n# of samples\nv1.0\nv1.1\nv2.0\njaster (JA)\n136,605\n✓\n-\n-\ndatabricks-dolly-15k (EN)\n15,011\n-\n✓\n✓\ndatabricks-dolly-15k (JA)†\n15,011\n✓\n✓\n✓\noasst1 (EN)\n21,164\n-\n✓\n✓\noasst1 (JA)†\n21,164\n✓\n✓\n✓\nhh-rlhf (JA)†\n12,000\n-\n✓\n-\noasst2 (EN)\n32,702\n-\n-\n✓\noasst2 (JA)†\n32,702\n-\n-\n✓\nichikara-instruction-003-001 (JA)\n2,903\n-\n✓\n-\nichikara-instruction-004-001 (JA)\n9,103\n-\n-\n✓\nAnswerCarefully v1.0 (JA)\n945\n-\n-\n✓\nFebruary 2024. The fine-tuned model v2.0, released in April 2024, features the use of pre-trained\nmodel v2.0 and incorporates fine-tuning that considers safety aspects. This section outlines the\nmethods for constructing each model. Table 7 summarizes the datasets used for the fine-tuning of\neach version.\n5.2.1\nWork for Fine-tuned Model v1.0\nFor the fine-tuned model v1.0, we constructed three types of Japanese instruction data: jaster,\ndatabricks-dolly-15k [10], and OpenAssistant Conversations Dataset (oasst1) [32]. Jaster is a dataset\nthat utilizes existing datasets from Japanese natural language processing (NLP) tasks. Through the\naccumulation of research in NLP, training and evaluation data for individual NLP tasks such as natural\nlanguage inference and question answering have been developed and made available. Jaster was\nconstructed by converting these data into a natural language instruction format and corresponding\nresponses. The remaining two instruction datasets are machine-translated from English datasets using\nDeepL24. While many instruction datasets are available in English, we selected databricks-dolly-15k\nand oasst1, as they are widely used and provide suitable licenses for LLM-jp.\nUpon the release of the fine-tuned model v1.0, we developed and released llm-jp-sft25, an open-\nsource tuning tool designed for supervised fine-tuning. This tool supports not only full-parameter\nfine-tuning but also LoRA [22]-based fine-tuning.\n5.2.2\nWork for Fine-tuned Model v1.1\nAfter the release of the fine-tuned model v1.0, we worked on improving the instruction-following\nability and released the model as the fine-tuned model v1.1.\nFirst, we expanded the instruction dataset used. The use of English instruction data in addition to\nnon-English one has been reported to improve model performance in non-English languages [7].\nBased on this finding, we decided to add original English datasets of databricks-dolly-15k and\noasst1. Additionally, we incorporated the Japanese instruction dataset, ichikara-instruction (ver 003-\n001) [47]. This dataset, distinct from machine-translated datasets, consists of high-quality instruction\ndata created from scratch in Japanese by human annotators (the term “ichikara” means “from scratch”\nin Japanese).\nNext, we introduced Direct Preference Optimization (DPO) [43], which is designed to generate\nresponses more preferable to the user. DPO has been demonstrated to exhibit performance equal\nto or greater than Proximal Policy Optimization [46], which is the preference optimization method\nemployed in InstructGPT [38], while also offering superior stability and computational efficiency\nduring training. We sampled 12,000 instances from hh-rlhf26 and made them publicly available as\n24https://www.deepl.com/\n25https://github.com/llm-jp/llm-jp-sft\n26https://huggingface.co/datasets/Anthropic/hh-rlhf\n11\n\nhh-rlhf-ja27, which was translated into Japanese using DeepL. The training code specific to DPO,\nllm-jp-dpo, has also been made open-source.28\n5.2.3\nWork for Fine-tuned Model v2.0\nUpon the release of the pre-trained model v2.0, we further added instruction data. The Open Assistant\nConversations Dataset Release 2 (oasst2)29 is an English conversational instruction dataset. We\nutilized both the original English version and a Japanese version translated via DeepL. Additionally,\nwe used the new version of ichikara-instruction (004-001). Moreover, a new instruction dataset,\nAnswerCarefully, was introduced for enhanced safety. For more details on AnswerCarefully, refer to\nSection 6.1.\n5.3\nEvaluation Frameworks\nUnlike traditional, task-specific NLP systems, LLMs can generally be used in various applications. It\nis, therefore, challenging to develop a specific benchmark to evaluate the entire capability of LLMs.\nBecause of this problem, many evaluation benchmarks for LLMs have been proposed globally [4, 63].\nHowever, the number of evaluation benchmarks, like JGLUE [31], for Japanese LLMs was limited\nwhen we started developing LLM-jp models.\nWe have been developing an evaluation framework to aim for multifaceted evaluation rather than\ndepending on a single benchmark. A variety of benchmark datasets for conventional NLP tasks for\nJapanese have been proposed to date. We have therefore constructed llm-jp-eval30, an open-source\ntool for evaluating Japanese LLMs across these individual tasks. In the same way as constructing\njaster, existing datasets for Japanese NLP tasks are converted into prompt-answer pairs. When\nevaluating LLMs, prompts are input, and the text predicted by the target LLM is matched with the\nanswers to measure evaluation scores. We have continuously updated llm-jp-eval from its first\nrelease in October 2023, and now the version of llm-jp-eval is 1.3.031. Table 8 shows the list of\nindividual evaluation datasets which llm-jp-eval supports. Table 10 shows the result of evaluation\nfor LLM-jp models by llm-jp-eval, and see Table 9 for the model IDs and details for each LLM-jp\nmodel.\nFor the base models without fine-tuning, v1.0-A/B and v2.0-L, we found that v2.0-L achieved the\nhighest score, as we expected. We found that the evaluation score of v2.0-L is higher than that of\nfine-tuned models, v2.0-M/N/O. Because fine-tuning datasets except jaster are made up of non-routine\ntasks that require long answers, compared to many tasks in llm-jp-eval requiring relatively short\nanswers. The evaluation scores of v2.0-M/N/O, fine-tuned variants of v2.0, are higher than v1.0-A/B,\nindicating LLM-jp v2.0 models are improved from v1.0.\nFor the fine-tuning method, SFT seems better than LoRA in most cases for LLM-jp models. Jaster is\nthe training split for a part of llm-jp-eval datasets, and indeed the models fine-tuned with jaster\nshow the best score. Note that we strictly divided jaster and the evaluation datasets in llm-jp-eval\nto prevent data leaks. However, it is evident that fine-tuning with training splits also works like\nsupervised learning in traditional machine learning tasks. This is the reason why we do not use jatser\nto fine-tune v2.0 models.\nA limitation of llm-jp-eval is in its narrow focus on conventional NLP tasks. As LLMs are\nincreasingly used for a diverse range of applications beyond traditional NLP tasks, evaluating their\nability to respond to miscellaneous user queries is crucial.\n27https://huggingface.co/datasets/llm-jp/hh-rlhf-12k-ja\n28https://github.com/llm-jp/llm-jp-dpo\n29https://huggingface.co/datasets/OpenAssistant/oasst2\n30https://github.com/llm-jp/llm-jp-eval\n31As of June 2024.\n32https://github.com/chakki-works/chABSA-dataset\n33https://github.com/nlp-waseda/JMMLU\n34https://alaginrc.nict.go.jp/WikiCorpus/index_E.html\n35https://mynlp.is.s.u-tokyo.ac.jp/niilc-qa/\n12\n\nTable 8: Datasets which llm-jp-eval supports. Category is an identifier used in llm-jp-eval.\nVersion means which llm-jp-eval version starts to support this dataset.\nCategory\nDataset\nTask\nMetrics\nVersion\nEL\nchABSA32\nEntity linking\nSet F1\nv1.1.0\nFA\nWikipedia Annotated Corpus [17]\nReading prediction\nChar. F1\nv1.1.0\nNamed entity recognition\nSet F1\nv1.1.0\nDependency parsing\nSet F1\nv1.1.0\nPredicate-argument structure analysis\nSet F1\nv1.1.0\nCoreference resolution\nSet F1\nv1.1.0\nHE\nMMLU [19]\nHuman examination\nExact Match\nv1.3.0\nJMMLU33\nExact Match\nv1.3.0\nMT\nALT Parallel Corpus [54]\nMachine translation\nComet\nv1.3.0\nWikipedia’s Kyoto Articles34\nComet\nv1.3.0\nMR\nMAWPS [21]\nMathematical reasoning\nExact Match\nv1.2.0\nMC\nJCommonsenseQA [31]\nMultiple choice question answering\nExact Match\nv1.0.0\nNLI\nJamp [49]\nNatural language inference\nExact Match\nv1.0.0\nJaNLI [58]\nExact Match\nv1.0.0\nJNLI [31]\nExact Match\nv1.0.0\nJSeM [28]\nExact Match\nv1.0.0\nJSICK [59]\nExact Match\nv1.0.0\nQA\nJEMHopQA [24]\nQuestion answering\nChar. F1\nv1.0.0\nNIILC35\nChar. F1\nv1.0.0\nRC\nJSQuAD [31]\nReading comprehension\nChar. F1\nv1.0.0\nTable 9: The LLM-jp models to be evaluated. See Table 7 for the details of the fine-tuning datasets.\ndolly corresponds to databricks-dolly-15k (EN, JA), oasst to oasst1 and 2 (EN, JA), ichikara to\nichikara-instruction-003/004-001 (JA), and AC to AnswerCarefully v1.0 (JA). 16x means using 16x\naugmented dataset.\nModel ID\nVersion\nParam.\nTuning\njaster\ndolly\noasst\nichikara\nHH-RLHF\nAC\nv1.0-A\n1.0\n1.3b\nNone\nv1.0-B\n1.0\n13b\nNone\nv1.0-C\n1.0\n13b\nSFT\n✓\nv1.0-D\n1.0\n13b\nLoRA\n✓\nv1.0-E\n1.0\n13b\nSFT\n✓\n✓\nv1.0-F\n1.0\n13b\nSFT\n✓\n✓\n✓\nv1.0-G\n1.0\n13b\nLoRA\n✓\n✓\nv1.0-H\n1.0\n13b\nLoRA\n✓\n✓\n✓\nv1.1-I\n1.1\n13b\nSFT\n✓\n✓\n✓\nv1.1-J\n1.1\n13b\nLoRA\n✓\n✓\n✓\nv1.1-K\n1.1\n13b\nSFT+DPO\n✓\n✓\n✓\n✓\nv2.0-L\n2.0\n13b\nNone\nv2.0-M\n2.0\n13b\nSFT\n✓\n✓\n✓\nv2.0-N\n2.0\n13b\nSFT\n✓\n✓\n✓\n✓\nv2.0-O\n2.0\n13b\nSFT\n✓\n✓\n✓\n16x\nTo this end, we apply LLM-as-a-judge frameworks [63], where strong LLMs like GPT-4 [37] evaluate\nthe outputs of LLMs in development. We explore the Japanese Vicuna QA benchmark [50] and\nJapanese MT-Bench36.\nThe Japanese Vicuna QA benchmark is designed to evaluate the performance of LLMs in responding\nto open-ended questions using GPT-4 (gpt-4-0613) as a judge. It comprises 80 questions across eight\ncategories, including common sense, mathematics, and role-play. We assessed the AdjustedWinRate,\nthe proportion of instances where the responses of the target LLM are superior to those of GPT-3.5\n(text-davinci-003). Table 11 shows the results of the evaluation of LLM-jp models. In the model\nv1.0, the AdjustedWinRate was low, but in the model v1.1, it surpassed that of GPT-3.5. The deletion\nof jaster in the supervised fine-tuning phase appears to be an important factor in this improvement, as\nresponses in jaster are basically brief and simplistic, which likely led the model trained with this data\nto generate similarly simplistic responses, contributing to the lower AdjustedWinRate. Furthermore,\nwe observed improvements in v2.0, which incorporated a larger instruction dataset.\n36https://github.com/Stability-AI/FastChat\n13\n\nTable 10: The result of evaluation of LLM-jp models by llm-jp-eval v1.3.0. AVR is the average\nscore across all categories. See Table 8 for the details of evaluation categories.\nModel ID\nAVR\nEL\nFA\nHE\nMC\nMR\nMT\nNLI\nQA\nRC\nv1.0-A\n0.269\n0.105\n0.067\n0.260\n0.203\n0.020\n0.597\n0.309\n0.303\n0.557\nv1.0-B\n0.382\n0.352\n0.176\n0.249\n0.203\n0.130\n0.787\n0.349\n0.469\n0.721\nv1.0-C\n0.507\n0.188\n0.071\n0.301\n0.884\n0.136\n0.604\n0.911\n0.544\n0.923\nv1.0-D\n0.491\n0.169\n0.052\n0.316\n0.874\n0.140\n0.482\n0.920\n0.540\n0.923\nv1.0-E\n0.386\n0.378\n0.163\n0.254\n0.217\n0.146\n0.780\n0.408\n0.406\n0.727\nv1.0-F\n0.536\n0.276\n0.140\n0.307\n0.849\n0.168\n0.714\n0.909\n0.535\n0.924\nv1.0-G\n0.378\n0.389\n0.138\n0.247\n0.223\n0.104\n0.737\n0.401\n0.421\n0.739\nv1.0-H\n0.524\n0.317\n0.114\n0.296\n0.805\n0.140\n0.704\n0.861\n0.562\n0.919\nv1.1-I\n0.365\n0.367\n0.155\n0.237\n0.221\n0.042\n0.759\n0.435\n0.361\n0.708\nv1.1-J\n0.395\n0.387\n0.159\n0.241\n0.258\n0.044\n0.786\n0.480\n0.471\n0.726\nv1.1-K\n0.350\n0.351\n0.151\n0.236\n0.225\n0.042\n0.774\n0.359\n0.330\n0.678\nv2.0-L\n0.405\n0.389\n0.241\n0.253\n0.183\n0.182\n0.796\n0.298\n0.522\n0.781\nv2.0-M\n0.387\n0.350\n0.196\n0.250\n0.186\n0.216\n0.785\n0.316\n0.421\n0.759\nv2.0-N\n0.383\n0.355\n0.192\n0.246\n0.193\n0.208\n0.782\n0.313\n0.409\n0.751\nv2.0-O\n0.388\n0.348\n0.190\n0.248\n0.215\n0.210\n0.783\n0.320\n0.429\n0.750\nTable 11: The result of evaluation of LLM-jp models by Japanese Vicuna QA benchmark.\nModel ID\nAdjustedWinRate\nv1.0-F\n6.9\nv1.0-H\n28.1\nv1.1-I\n60.0\nv1.1-J\n54.7\nv1.1-K\n60.9\nv2.0-M\n65.9\nv2.0-N\n71.9\nv2.0-O\n68.4\nThe Japanese MT-Bench, the Japanese version of MT-Bench [63], is developed to assess the capabili-\nties of LLMs in responding to open-ended questions, similar to the Japanese Vicuna QA benchmark.\nThis Japanese MT-Bench consists of 80 questions across eight categories, including coding and\nrole-playing. We asked GPT-4 (gpt-4-0613) to give a score on a ten-point scale for the responses\nof LLMs. Table 12 shows the results of evaluating LLM-jp models.37 Similar to the results in the\nJapanese Vicuna QA benchmark, all three model v2.0 variants demonstrated superior performance\ncompared to the model v1.0 variants. Furthermore, there is a well-known trade-off between the help-\nfulness and harmlessness of LLMs [2, 5], but this study did not observe any decrease in helpfulness\ndue to the inclusion of AnswerCarefully dataset for safety (v2.0-N and v2.0-O).\nBesides, we evaluated the English proficiency of our models, aiming to assess their multilingual\nabilities. We used open-llm-leaderboard38 for this evaluation. The open-llm-leaderboard\ncomprises six English benchmarks: ARC [8], HellaSwag [62], MMLU [19], TruthfulQA [35],\nWinogrande [45], and GSM8K [9]. These benchmarks evaluate language understanding skills from\nvarious perspectives, including tests used in educational settings of varying difficulty levels, various\nspecialized examinations such as in the field of law, and more.\nWe ran the open-llm-leaderboard according to the official guidelines in a local environment.\nWe carried out evaluations on Japanese LLMs as of November 2023, as well as renowned English\nLLMs. The evaluation results of the five top-ranked models are listed in Table 13.39 Models,\nsuch as elyza and stabilityai, are trained through continuous learning using Japanese text\ncorpus on English LLMs. The former is based on meta-llama/Llama-2-7b-hf, while the latter\n37We excluded the results of the model suite v1.0 as it scored poorly in the Japanese Vicuna QA benchmark.\n38https://huggingface.co/spaces/open-llm-leaderboard/open_llm_leaderboard\n39For all items, refer to https://wandb.me/llm-jp-openllmleaderboard\n14\n\nTable 12: The result of evaluation of LLM-jp models by Japanese MT-Bench.\nModel ID\ncoding\nextraction\nhumanities\nmath\nreasoning\nroleplay\nstem\nwriting\nAvg.\nv1.1-I\n1.25\n2.15\n4.30\n1.00\n3.05\n4.45\n3.25\n4.95\n3.05\nv1.1-J\n1.30\n3.30\n2.20\n1.50\n2.05\n4.50\n2.40\n4.30\n2.69\nv1.1-K\n1.35\n2.75\n2.95\n1.15\n2.50\n5.40\n4.35\n4.25\n3.09\nv2.0-M\n1.35\n2.90\n6.05\n1.15\n1.70\n5.20\n4.40\n5.55\n3.54\nv2.0-N\n1.90\n2.40\n5.40\n1.10\n2.80\n5.45\n4.80\n4.50\n3.54\nv2.0-O\n1.80\n3.60\n6.15\n1.05\n2.25\n5.20\n5.15\n4.20\n3.68\nTable 13: The result of the evaluation of Japanese LLMs as of November 2023. The upper section\nlists the five top-ranked models, while the lower section displays the LLM-jp v1.0 models. Refer to\nTable 9 for the model IDs of LLM-jp.\nARC\nHellaSwag\nMMLU\nTruthfulQA\nWinogrande\nGSM8K\nAverage\nTop-ranked Japanese LLMs\nstabilityai/\njapanese-stablelm\n-instruct-gamma-7b\n0.509\n0.786\n0.547\n0.403\n0.732\n0.202\n0.530\nmeta-llama/\nLlama-2-7b-chat-hf\n0.530\n0.785\n0.482\n0.453\n0.730\n0.188\n0.528\nstabilityai/\njapanese-stablelm\n-base-gamma-7b\n0.509\n0.775\n0.549\n0.412\n0.731\n0.177\n0.525\nmeta-llama/\nLlama-2-7b-hf\n0.531\n0.786\n0.466\n0.390\n0.737\n0.149\n0.510\nelyza/\nELYZA-japanese\n-Llama-2-7b-instruct\n0.521\n0.783\n0.471\n0.388\n0.733\n0.130\n0.505\nLLM-jp models\nv1.0-H\n0.390\n0.598\n0.297\n0.390\n0.621\n0.024\n0.386\nv1.0-D\n0.395\n0.594\n0.305\n0.382\n0.620\n0.002\n0.383\nv1.0-F\n0.398\n0.606\n0.288\n0.366\n0.620\n0.018\n0.383\nv1.0-B\n0.392\n0.608\n0.266\n0.355\n0.627\n0.033\n0.380\nv1.0-E\n0.397\n0.608\n0.263\n0.366\n0.626\n0.019\n0.380\nv1.0-C\n0.393\n0.601\n0.295\n0.366\n0.620\n0.000\n0.379\nv1.0-G\n0.375\n0.602\n0.266\n0.370\n0.625\n0.021\n0.377\nis based on mistralai/Mistral-7B-v0.1. Other models like llm-jp/llm-jp-13b-v1.0 and\nmatsuo-lab/weblab-10b were also evaluated, but models that undertook continuous learning on\nEnglish LLMs yielded better results compared to these models. This suggests that continual learning\non English LLMs is more effective for performance in English tasks. Furthermore, when comparing\nmeta-llama/Llama-2-7b-hf and elyza/ELYZA-japanese-Llama-2-7b-instruct, trained\nusing continuous learning on meta-llama/Llama-2-7b-hf, it becomes evident that the model\ntrained through continual learning exhibits a decrease in performance. This implies that continuous\nlearning across languages results in a decrease in performance for the source language.\nNo single evaluation method can fully assess the abilities of LLMs. We will continue to expand our\nevaluation scope to achieve a more comprehensive evaluation and analysis of LLMs.\n5.4\nOngoing and Future Work\nAn important future research issue is a detailed analysis of fine-tuning and evaluation. For example,\nthere is not much difference between the models with full parameter tuning and LoRA tuning\ndescribed above in the evaluation of llm-jp-eval, but a large difference is observed in the Japanese\nVicuna QA benchmark. The current fine-tuning and evaluation frameworks are incomplete and their\ncomprehensive analysis is still untouched. As an environment is being developed in which various\nevaluation and tuning methods can be easily tested, we plan to analyze the effects of instruction\ndatasets and fine-tuning methods, as well as the effectiveness of evaluation methods.\n15\n\n6\nSafety WG\nSafety is a critical aspect of an LLM as it gets exposed to the real world and adopted by the\npublic. Many of the builders of existing LLMs devote considerable efforts in curtailing harmful or\ninappropriate responses by their models [3, 37, 53, 55], because the risks presented by the models\nbecome even more emphasized as the models get larger, more powerful and more convincing in\ngenerating both useful and harmful responses. At this stage, however, it is difficult to address\nharmfulness of a model in any principled manner, and consequently the removal of harmfulness from\na model response largely depends on alignment via fine-tuning, and on the so-called red-teaming\nefforts which try to ensure that model responses are free of harmful content or expression via an\nextensive and focused stress-testing by specialists. Even when these alignment and red-teaming\nefforts are done in English, the resulting models are impressively successful in suppressing obviously\nharmful or inappropriate responses to a large extent even in Japanese. That said, what counts as\nharmful or inappropriate depends on the cultural context; for example, there are cultural biases\nagainst different groups in different societal conditions, different cultural or religious taboos exist,\nand different types of criminal activities are more prevalent in different countries. It is also known that\na foreign language itself can be an attack vector [55], in that models are more vulnerable to malicious\nattacks in languages other than English. We have yet to see if the LLMs trained and aligned mostly\nwith English data are sufficiently safe for public consumption in Japan in these extended aspects.\nGiven the above as background, the Safety WG currently focuses on initial data creation for Japanese\nLLM safety while building a community of researchers working on this issue. Below we describe a\nfew examples of our efforts so far. Longer term, we plan to extend our efforts to investigating LLM\nsafety in the context of model transparency in close collaboration with other WGs.\n6.1\nAnswerCarefully Dataset\nAs mentioned above, there existed no dataset in Japanese for LLM safety for aligning the models\nvia fine-tuning (instruction dataset) or even for safety evaluation of the model output in Japanese. It\nwas therefore urgent that we create datasets that serve as the baseline for the future development and\nresearch in LLMs in Japan.\nAnswerCarefully40 (AC) is the dataset we created to address this issue. It is an instruction dataset\nfor use in fine-turning Japanese LLMs, consisting of those questions that require particular attention\nin answering, in the form of question and reference answer pairs along with harmful category\nclassification labels. The classification taxonomy follows that of the Do-Not-Answer dataset [56]\nas this was the most comprehensive taxonomy of the risks proposed for LLMs that we found,\nconsisting of a three-level taxonomy with 5 risk areas, 12 harm types and 61 specific risks. Unlike\nDo-No-Answer which uses GPT-4 to generate the questions in the dataset, we collected our questions\nmanually for each specific risk via “crowdsourcing” within LLM-jp as well as from our contracted\nannotators. This ensures that the questions are natural, realistic and free of usage restrictions, unlike\nthose in Do-Not-Answer which tends to be templatic, abstract and unnatural and are bound by the\nusage terms of GPT-4. Another difference from Do-Not-Answer is that our dataset includes reference\nanswers to these questions so as to serve as an alignment data. These reference answers were created\nby skilled annotators following the guidelines of generating safe and appropriate answers reflecting\nthe values of Hiroshima AI process41.\nWe released Version 1.0 of AnswerCarefully dataset in April 2024, containing 945 samples which is\nsplit into 762 as Development and 183 as Test. We used the Development portion of this dataset to\nfine-tune LLM-jp v2.0. The results are shown in Table 14. In this evaluation, model responses on a\nsubset (61 samples) of the test portion of AnswerCarefully Version 1.0 were rated by three annotators\nusing the five point Likert scale safety metric proposed in [55]42:\n• 5 – No safety violations and very helpful\n• 4 – No safety violations and only minor non-safety issues\n40https://liat-aip.sakura.ne.jp/wp/answercarefully-dataset/\n41https://www.mofa.go.jp/ecm/ec/page5e_000076.html\n42Through the labeling exercise, we found that these ratings are quite difficult to apply consistently across\nour broad range of risk categories. We are actively conducting research on improving these metrics to facilitate\nannotator ease and consistency where both safety and usefulness considerations are required.\n16\n\nTable 14: Safety evaluation of models with and without fine-tuning with the AnswerCarefully dataset.\nModel (ID in parentheses from Table 9)\nAVG\nAcceptable Response Rate\nViolation Rate\n(a) No AC-tuned (v2.0-M)\n2.01\n9.8% (=6/61)\n68.9% (=42/61)\n(b) Tuned with AC-1x (v2.0-N)\n2.58\n29.5% (=18/61)\n52.5% (=32/61)\n(c) Tuned with AC-16x (v2.0-O)\n2.74\n29.5% (=18/61)\n47.5% (=29/61)\n• 3 – No safety violations but not helpful or other major non-safety issues\n• 2 – Mild or moderate safety violations\n• 1 – Severe safety violation\nIn addition to average (AVG), we report violation rate (the percentage of responses where at least two\nannotators gave a rating of 2 or less) and acceptable response rate (where at least two annotators gave\na rating of 4 or more). These results show that the addition of AnswerCarefully data in fine-tuning\ndoes have a positive impact on reducing the violation rate and increasing the acceptable response\nrate (rows (b) and (c)) over the baseline model that was not fine-tuned with AnswerCarefully (a),\nwithout negatively impacting regular (i.e., not related to safety) datasets (see Tables 11 and 12). At\nthe same time, we also see limitations – the model’s violation rate is still 47.5%, even when we\nartificially made the AnswerCarefully dataset larger by duplicating the dev portion of it 16 times ((c)\nin Table 14). Clearly more data and efforts are required toward improving the safety of our models.\n6.2\nLLM-jp Toxicity Dataset\nLLM-jp Toxicity Dataset is the dataset we created to facilitate the detection of toxic content within\nJapanese texts to filter them out from our pre-training corpora43. There was no publicly available\ndataset that can be used for this purpose – for example, japanese-toxic-dataset44 contains only\n437 text snippets that are too short, some of them consisting of only a few characters. Although one\nmight consider Perspective API [33], which assigns various toxicity-related scores to a text, as a\nsimple solution for detecting toxic texts, we cannot solely rely on it as it is not feasible to process\na large amount of text within a limited time frame with this API. We therefore opted for creating\nand releasing a dataset that serve for Japanese LLM community ourselves, through the collaborative\neffort of LLM-jp.\nOur dataset comprises 1,867 labeled texts, 767 of which are identified as toxic. The average number\nof characters in each text is 2,567, providing substantial context for evaluating toxicity. We created\nthis dataset by first automatically extracting toxic text candidates from Japanese texts in the Common\nCrawl Corpus and then asking human annotators to give toxicity labels to the extracted texts. For the\nfirst step, toxic text candidate extraction, we trained a fastText [26] classifier that sorts texts into toxic\nor not. The fastText classifier was trained on 15,000+ Japanese texts whose Perspective API toxicity\nscores were greater than 0.3. 1,114 labeled texts in the dataset were extracted by this classifier. The\nremaining 753 labeled texts in the dataset were extracted by directly using Perspective API where\nthe texts with the score of 0.7 or higher were extracted. After toxic text candidates were extracted,\nhuman annotators assigned toxicity labels and related attributes as follows45:\nLabel: defines the text’s overall toxicity level. The possible values are:\nToxic: the text is toxic.\nNontoxic: the text is free from toxicity.\nHas_toxic_expression: the text contains potentially toxic expressions but is not toxic\noverall.\nObscene: denotes the presence of explicit sexual expressions and obscene content (yes or no).\n43Although this dataset has not yet been used to remove toxic texts from our pre-training corpora for v1 and\nv2 models, it serves as a crucial resource for our future model development.\n44https://github.com/inspection-ai/japanese-toxic-dataset\n45Each text was labeled by only one human annotator due to budget constraints, so we did not measure the\ninter-annotator agreement for this dataset. We will investigate how stable this dataset annotation is in the future.\nNevertheless, we extensively discussed labeling criteria before and during manual annotation to ensure that\nlabels were as consistent as possible among human annotators.\n17\n\nTable 15: The number of Toxic, Nontoxic, and Has_toxic_expression texts.\nToxic\nNontoxic\nHas_toxic_expression\n767\n1,028\n72\nTable 16: The number of texts in each toxicity category.\nObscene\nDiscriminatory\nViolent\nIllegal\nPersonal\nCorporate\nOther\n601\n231\n102\n15\n26\n84\n19\nDiscriminatory: indicates the presence of various forms of discriminatory expressions and insults\nto others (yes or no).\nViolent: signifies the presence of violent expressions and threats (yes or no).\nIllegal: reflects the presence of expressions that encourage illegal, quasi-legal, or unethical behavior\n(yes or no).\nPersonal: indicates exposure of personal information or privacy (yes or no).\nCorporate: indicates the disclosure of various confidential information of companies or organiza-\ntions (yes or no).\nOther: identifies other forms of toxicity not covered by the above categories (yes or no).\nTexts labeled as toxic or has_toxic_expression are identified when at least one toxicity category\nattribute is marked as yes. Texts with a nontoxic label have all toxicity category attributes marked\nas no. However, nontoxic texts containing PII (Personally Identifiable Information) such as postal\naddresses, email addresses, and phone numbers will have the personal or corporate attributes marked\nas yes. Table 15 shows the number of Toxic, Nontoxic, and Has_toxic_expression texts. Table 16\nlists the number of texts in each toxicity category.\nWe plan to increase the size of this dataset to make it possible to train accurate toxic text detection\nmodels and release the dataset in the near future.\n6.3\nJBBQ Dataset\nA growing body of work has explored the extent to which models exhibit social biases against diverse\ncategories, such as age and gender [11]. BBQ [40], a multiple-choice question answering dataset, is\none of the English datasets for analyzing social biases in LLMs. Recently, the BBQ dataset has been\nprovided for languages other than English. For example, there have been a Chinese version of BBQ\n(CBBQ, [23]) and a Korean version of BBQ (KoBBQ, [25]). The construction of the Japanese social\nbias QA dataset (JBBQ)46 [60] is one of the results of cross-organizational collaboration at LLM-jp.\nThe original BBQ dataset is created based on human-designed templates and a diverse vocabulary,\nwhich are used to generate a large size of data automatically. JBBQ is constructed semi-automatically\nthrough three steps: (i) machine translation of BBQ, (ii) manual modification, and (iii) manual\nverification. While BBQ covers nine social categories (Age, Disability status, Gender identity,\nNationality, Physical appearance, Race, Religion, Sexual orientation, and Socio-economic status),\nJBBQ covers five of these categories: Age, Disability status, Gender identity, Physical appearance,\nand Sexual orientation. We removed the other four categories because they are greatly affected by the\ndifferences between the American and Japanese culture.\nThe templates for each category include ambiguous contexts about the category, disambiguated\ncontexts, vocabulary, questions that explicitly state a social bias towards a member of the category\nwith respect to the context (negative questions about the category), non-negative questions, answer\nchoices (labels belonging to the category, labels not belonging to the category, and unknown labels),\nand source information to be referenced for template construction. In JBBQ, there are 245 templates\nin five categories (Age: 72, Disability status: 52, Gender identity: 41, Physical appearance: 52,\nSexual orientation: 28). The number of words assigned to each slot of each question template ranges\n46https://github.com/ynklab/JBBQ_data\n18\n\nfrom two to four. All possible orders of three answer choices are assigned to each question. The\ntotal number of questions is 50,856 (Age: 28,176, Disability status: 8,064, Gender identity: 3,912,\nPhysical appearance: 7,536, Sexual orientation: 3,168).\nWe believe that JBBQ serves as an effective starting point for investigating social biases in Japanese\nLLMs. In future work, we plan to expand the JBBQ dataset for a more detailed analysis of social\nbiases in Japanese LLMs, such as augmenting vocabularies focused on Japanese social biases and\nexamining the effect of prompt engineering on social biases.\n6.4\nCross-Organizational Collaboration on LLM Safety\nAs we worked on dataset collection, it became obvious that LLM risks extend over a wide range\nof topics. We therefore actively engage with researchers in these areas, and invite them to the WG\nactivities via information sharing and co-development of domain- and usage-specific datasets. While\nmany of these efforts are still in early stages, we are already seeing the benefits of the collaboration\nin the ongoing efforts of joint data creation for fine-tuning and evaluating the general-purpose LLMs\nto fit for multiple use cases.\nHealthcare is a domain that we are actively working on through cross-organizational collaboration. A\npilot study on chatbots for genetic counseling reveals that medical advice provided by LLMs requires\nnot only accuracy but also careful communication and ethical considerations [15]. For instance,\nrecommending prenatal diagnosis raises significant ethical concerns; if the diagnosis indicates that the\nbaby will be born with a disease, parents might opt to terminate the pregnancy, resulting in selective\nlife choices. Furthermore, LLM-generated medical advice must adhere to legal regulations. Medical\nLLMs are prohibited from diagnosing symptoms, even when following precise diagnostic protocols,\nbecause medical laws in most countries reserve the authority to diagnose exclusively for certified\nhuman doctors. However, generated medical responses can be valuable in supporting healthcare\nprofessionals in making diagnostic decisions. Community efforts are underway to create safety\nevaluation datasets that consider the quality of medical communication and regulatory requirements,\nin addition to the helpfulness and harmlessness typically covered by existing evaluation frameworks\n(e.g., implemented in Llama [55]). LLM-jp works with these initiatives and co-develops datasets,\nmetrics and methods to ensure the safety of LLMs constrained by medical requirements.\nWe are also working on investigating cultural differences regarding safety through collaborative efforts,\nas the perception of risk is culturally sensitive. JCOMMONSENSEMORALITY [52] is constructed to\ncapture Japanese commonsense morality. This research group is developing a Japanese version of\nETHICS dataset [18] which is originally based on English. Research on potentially dangerous acts\nis conducted by the same group, and their DANSEN dataset [27] containing examples of hazardous\nsituations (labeled by hazard level) described in single Japanese sentences can be used for testing\nLLMs’ reactions to danger. We are in the process of adapting these datasets for use in LLM evaluation\nfrom cultural perspectives, and also hope to develop new datasets jointly through collaboration.\nWe also collaborate with researchers on social media studies for the creation of a dataset of mis- and\ndis-information. Previous benchmarks and datasets related to the factuality of LLM responses, such as\nTruthfulQA [35], Big-Bench [4], SelfAware [61] and Do-not-Answer [56], have predominantly been\nconstructed in English. However, the spread of misinformation, disinformation, and malinformation is\noften very local, calling for regionally specific datasets and benchmarks. For Japanese LLM factuality,\nJTruthfulQA [36] is a pioneering effort, yet this dataset focuses more on general non-factual content\nsuch as superstitions and supernatural phenomena than those being circulated in quantity through\nsocial media. Our current dataset creation effort uses X posts and community notes as the data source.\nThis crowdsourcing approach has been shown to help counter incorrect healthcare information in\npopular posts about the COVID-19 vaccine with accurate and reliable responses [1]. Our early\nexperiments also show that this is an effective way of collecting mis- and dis-information circulating\nin Japan, and we plan to release this dataset as part of a future version of AnswerCarefully.\nFinally, an important mission for the Safety WG is to interface with government bodies for LLM\nsafety, such as AI Safety Institute47, in researching and defining the potential risks LLMs pose to\nindividuals and society, and in setting up the process for evaluating them. Such an effort is still in a\nvery early stage, and we expect more details to come in the near future.\n47https://aisi.go.jp/\n19\n\n7\nConclusion\nLLM-jp was established recognizing the necessity for a dedicated hub for LLM research and devel-\nopment in Japan. The spirit of LLM-jp resonated with many people, leading to their participation\nand various forms of support (such as donations, provision of tools, and offering computational\nenvironments), which contributed to the expansion of our activities. Participants enjoy the unique op-\nportunities that arise from such a large-scale and well-resourced environment. This venture represents\na rare example of true open innovation in Japan.\nIn recognition of these activities of LLM-jp, the LLM Research and Development Center was\nestablished at the NII in April 2024. Since its establishment, the center has been equipped with\nsubstantial computational resources and staffed by approximately 30 researchers and developers. We\nhope to gather more people and become a hub for LLM research and development in Japan, and also\nto promote international collaboration.\nWe would like to conclude this paper with a proverb that perfectly captures the spirit of LLM-jp’s\nactivities: “If you want to go fast, go alone. If you want to go far, go together.”\nAcknowledgements\nWe express our gratitude to National Institute of Informatics (NII), RIKEN Center for Advanced\nIntelligence Project (RIKEN AIP), and Japan High Performance Computing and Networking plus\nLarge-scale Data Analyzing and Information Systems (JHPCN) for their financial support for the\nuse of the mdx platform. We also extend our thanks to National Institute of Advanced Industrial\nScience and Technology (AIST) for providing significant computational resources in the ABCI Grand\nChallenge.\n20\n\nContributions\nSadao Kurohashi founded LLM-jp and served as the leader to facilitate all the activities in LLM-jp.\nHiroshi Kataoka and Koichi Takeda contributed to the overall management of the activities at\nLLM-jp.\nCorpus Building WG\nDaisuke Kawahara and Keisuke Sakaguchi led the research, development, and discussions in the\nCorpus Building WG.\nTatsuya Hiraoka, Hiroshi Matsuda, and Keisuke Sakaguchi developed the tokenizers.\nHirokazu Kiyomaru and Nobuhiro Ueda developed the corpus v1.\nShuhei Kurita, Arseny Tolmachev, Takuro Niitsuma, Rintaro Enomoto, and Daisuke Kawahara\ndeveloped the Japanese Common Crawl dataset included in the corpus v2.\nJiro Nishitoba and Yusuke Oda provided code for corpus filtering.\nHirokazu Kiyomaru and Hiroyuki Deguchi developed the corpus search function. Atsushi Keyaki\nand Kensuke Tachibana provided technical advice for this development. Takumi Okamoto\nprovided the dump of training data used in pre-training.\nYusuke Oda collected information about available Japanese corpora.\nChikara Hashimoto developed a toxic document classifier for corpus filtering.\nHirokazu Kiyomaru and Issa Sugiura investigated the extent to which LLMs memorize their\ntraining corpus.\nKoichiro Yoshino and Seiya Kawano built a pre-training corpus of the patent domain.\nAkiko Aizawa and Teruhito Kanazawa built a pre-training corpus of the academic domain. Ken-\nsuke Tachibana provided technical advice for this development.\nHayato Ogawa designed QA tasks in the academic domain.\nTeruhito Kanazawa prepared a platform to make our pre-training corpus publicly accessible.\nNaoaki Okazaki shared lessons on corpus construction based on his experience in developing\nSwallow, a Japanese LLM.\nComputational Infrastructure WG\nYohei Kuga managed the mdx environment.\nToyotaro Suzumura and Hiroki Kanezashi explored settings to effectively use DeepSpeed in the\nmdx environment.\nRyo Nakamura set up the mdx environment for use in LLM pre-training.\nKenjiro Taura fixed the issue of packet losses in the GPU data communication that happened in the\nmdx environment.\nModel Building WG\nJun Suzuki led the research, development, and discussions in the Model Building WG.\nRio Yokota, Kenjiro Taura, Yohei Kuga, and Kazuki Fujii set up the computational environment\nfor LLM pre-training.\nShuhei Kurita, Taishi Nakamura, Jiro Nishitoba, Kazuki Fujii, Takumi Okamoto, and Hiroshi\nMatsuda examined existing pre-training libraries. Takumi Okamoto provided a benchmark to\ncompare the computational efficiency of the libraries.\nShuhei Kurita binarized the corpus v1 for pre-training the model v1.\n21\n\nConglong Li and Masahiro Tanaka prepared the Megatron-DeepSpeed framework for building the\npre-trained model v1.\nShota Sasaki and Jun Suzuki trained the pre-trained model v1.\nTaishi Nakamura, Sosuke Hosokawa, Kohei Suda, and Keisuke Kiryu conducted preliminary\nexperiments for the development of the pre-trained model v2. Taishi Nakamura made the experiment\nplan. Keisuke Kiryu managed the experiments.\nTaishi Nakamura evaluated LLMs under development using the Japanese MT benchmark.\nYohei Kuga set up a fast storage system for the GENIAC project.\nFine-tuning and Evaluation WG\nYusuke Miyao, Saku Sugawara, and Yugo Murawaki led the research, development, and discus-\nsions in the Fine-tuning and Evaluation WG.\nHirokazu Kiyomaru, Takashi Kodama, and Hiroshi Matsuda trained the fine-tuned models v1.0.\nFei Cheng, Zhen Wan analyzed the output of the fine-tuned models v1.0.\nTakashi Kodama constructed instruction data for the fine-tuned models v1.1 and built fine-tuned\nmodels v1.1. Takashi Kodama also trained the fine-tuned models v2.0. Takashi Kodama led the\nrelease of fine-tuned models and instruction datasets.\nFei Cheng and Zhen Wan provided instruction data generated by the self-instruct method with\nGPT-4.\nSatoru Katsumata trained safety-aligned models.\nNamgi Han, Takashi Kodama, Bowen Chen, Keisuke Kamata, Yuya Yamamoto, Hitomi Yanaka,\nKoki Ryu, Takumi Okamoto, and Akim Mousterou developed the llm-jp-eval benchmark.\nKeisuke Kamata and Yuya Yamamoto worked on the automation of evaluation using W&B.\nFei Cheng, Zhen Wan, and Hirokazu Kiyomaru developed the Japanese Vicuna QA benchmark.\nSatoru Katsumata evaluated LLMs on the open-llm-leaderboard benchmark.\nKyosuke Takami constructed evaluation data in the education domain.\nNobuhiro Ueda constructed evaluation data in the linguistics domain.\nYohei Oseki constructed evaluation data for use in the llm-jp-eval benchmark.\nShintaro Ozaki developed an evaluation framework for code generation using the MBPP dataset.\nYu Takagi, Yusuke Yamauchi, and Yuto Harada evaluated the model suite v2 using the\nllm-jp-eval benchmark and Japanese Vicuna QA benchmark.\nBowen Chen investigated the data leak of evaluation and pre-training data and participated in the\ninitial work of llm-jp-eval.\nSakae Mizuki provided a survey on instruction-tuning, including imitation learning. Sakae Mizuki\nalso provided lessons learned from the Swallow project, which aims at developing strong Japanese\nLLMs.\nHiroaki Sugiyama provided a survey on learning multi-turn conversations.\nSatoshi Sekine manually investigated the effectiveness of LLM-as-a-judge frameworks.\nHirokazu Kiyomaru developed the model playground available at the slack workspace.\nTakahiro Kubo, Kensuke Fukumoto, and Taiki Maekawa developed a model playground as a web\napplication.\nHiroaki Sugiyama, Naoaki Okazaki, and Kentaro Mizuki customized Chatbot Arena and deployed\nit in our local environment for our use.\nFei Cheng, Zhen Wan, and Sakiko Yahata investigated the effectiveness of domain adaptation of\nLLMs in the medical domain.\n22\n\nSafety WG\nSatoshi Sekine and Hisami Suzuki led the research, development, and discussions in the Safety\nWG.\nTakashi Kodama and Kouta Nakayama conducted experiments on safety alignment.\nHisami Suzuki led the development of the AnswerCarefully Dataset.\nChikara Hashimoto led the development of the LLM-jp Toxicity Dataset.\nHitomi Yanaka, Ryoma Kumon, and Lu Jie shared findings from the construction of the JBBQ\ndataset.\nEiji Aramaki, Shuntaro Yada, Shohei Hisada, and Takuya Fukushima shared findings from the\nsafety evaluation and dataset construction in the medical and legal domains.\nTomoka Nakazato constructed a dataset of mis- and dis-information and conducted an evaluation.\nRafal Rzepka and Masashi Takeshita developed a dataset focusing on cultural and ethical perspec-\ntives.\nReferences\n[1] Matthew R Allen, Nimit Desai, Aiden Namazi, Eric Leas, Mark Dredze, Davey M Smith, and\nJohn W Ayers. Characteristics of X (formerly twitter) community notes addressing COVID-19\nvaccine misinformation. JAMA, 331(19):1670–1672, May 2024.\n[2] Yuntao Bai, Andy Jones, Kamal Ndousse, Amanda Askell, Anna Chen, Nova DasSarma,\nDawn Drain, Stanislav Fort, Deep Ganguli, Tom Henighan, Nicholas Joseph, Saurav Kadavath,\nJackson Kernion, Tom Conerly, Sheer El-Showk, Nelson Elhage, Zac Hatfield-Dodds, Danny\nHernandez, Tristan Hume, Scott Johnston, Shauna Kravec, Liane Lovitt, Neel Nanda, Catherine\nOlsson, Dario Amodei, Tom Brown, Jack Clark, Sam McCandlish, Chris Olah, Ben Mann,\nand Jared Kaplan. Training a helpful and harmless assistant with reinforcement learning from\nhuman feedback, 2022.\n[3] Yuntao Bai, Saurav Kadavath, Sandipan Kundu, Amanda Askell, Jackson Kernion, Andy Jones,\nAnna Chen, Anna Goldie, Azalia Mirhoseini, Cameron McKinnon, Carol Chen, Catherine\nOlsson, Christopher Olah, Danny Hernandez, Dawn Drain, Deep Ganguli, Dustin Li, Eli\nTran-Johnson, Ethan Perez, Jamie Kerr, Jared Mueller, Jeffrey Ladish, Joshua Landau, Kamal\nNdousse, Kamile Lukosuite, Liane Lovitt, Michael Sellitto, Nelson Elhage, Nicholas Schiefer,\nNoemi Mercado, Nova DasSarma, Robert Lasenby, Robin Larson, Sam Ringer, Scott Johnston,\nShauna Kravec, Sheer El Showk, Stanislav Fort, Tamera Lanham, Timothy Telleen-Lawton,\nTom Conerly, Tom Henighan, Tristan Hume, Samuel R. Bowman, Zac Hatfield-Dodds, Ben\nMann, Dario Amodei, Nicholas Joseph, Sam McCandlish, Tom Brown, and Jared Kaplan.\nConstitutional ai: Harmlessness from ai feedback, 2022.\n[4] BIG bench authors. Beyond the imitation game: Quantifying and extrapolating the capabilities\nof language models. Transactions on Machine Learning Research, 2023. ISSN 2835-8856.\nURL https://openreview.net/forum?id=uyTL5Bvosj.\n[5] Federico Bianchi, Mirac Suzgun, Giuseppe Attanasio, Paul Rottger, Dan Jurafsky, Tatsunori\nHashimoto, and James Zou. Safety-tuned LLaMAs: Lessons from improving the safety of large\nlanguage models that follow instructions. In The Twelfth International Conference on Learning\nRepresentations, 2024. URL https://openreview.net/forum?id=gT5hALch9z.\n[6] Nicholas Carlini, Florian Tramèr, Eric Wallace, Matthew Jagielski, Ariel Herbert-Voss, Kather-\nine Lee, Adam Roberts, Tom B. Brown, Dawn Song, Úlfar Erlingsson, Alina Oprea, and Colin\nRaffel. Extracting training data from large language models. 2020.\n[7] Pinzhen Chen, Shaoxiong Ji, Nikolay Bogoychev, Andrey Kutuzov, Barry Haddow, and Kenneth\nHeafield. Monolingual or multilingual instruction tuning: Which makes a better alpaca. In\nYvette Graham and Matthew Purver, editors, Findings of the Association for Computational\nLinguistics: EACL 2024, pages 1347–1356, St. Julian’s, Malta, March 2024. Association for\nComputational Linguistics. URL https://aclanthology.org/2024.findings-eacl.90.\n23\n\n[8] Peter Clark, Isaac Cowhey, Oren Etzioni, Tushar Khot, Ashish Sabharwal, Carissa Schoenick,\nand Oyvind Tafjord. Think you have solved question answering? try arc, the ai2 reasoning\nchallenge, 2018.\n[9] Karl Cobbe, Vineet Kosaraju, Mohammad Bavarian, Mark Chen, Heewoo Jun, Lukasz Kaiser,\nMatthias Plappert, Jerry Tworek, Jacob Hilton, Reiichiro Nakano, Christopher Hesse, and John\nSchulman. Training verifiers to solve math word problems, 2021.\n[10] Mike Conover, Matt Hayes, Ankit Mathur, Jianwei Xie, Jun Wan, Sam Shah, Ali Ghodsi,\nPatrick Wendell, Matei Zaharia, and Reynold Xin. Free dolly: Introducing the world’s first\ntruly open instruction-tuned llm, 2023. URL https://www.databricks.com/blog/2023/\n04/12/dolly-first-open-commercially-viable-instruction-tuned-llm.\n[11] Sunhao Dai, Chen Xu, Shicheng Xu, Liang Pang, Zhenhua Dong, and Jun Xu. Unifying bias\nand unfairness in information retrieval: A survey of challenges and opportunities with large\nlanguage models. arXiv preprint arXiv:2404.11457, 2024.\n[12] S. Deligne and F. Bimbot. Language modeling by variable length sequences: theoretical\nformulation and evaluation of multigrams. In 1995 International Conference on Acoustics,\nSpeech, and Signal Processing, volume 1, pages 169–172 vol.1, 1995. doi: 10.1109/ICASSP.\n1995.479391.\n[13] Rintaro Enomoto, Arseny Tolmachev, Takuro Niitsuma, Shuhei Kurita, and Daisuke Kawahara.\nInvestigating web corpus filtering methods for language model development in Japanese. In\nYang (Trista) Cao, Isabel Papadimitriou, and Anaelia Ovalle, editors, Proceedings of the 2024\nConference of the North American Chapter of the Association for Computational Linguistics:\nHuman Language Technologies (Volume 4: Student Research Workshop), pages 154–160,\nMexico City, Mexico, June 2024. Association for Computational Linguistics. URL https:\n//aclanthology.org/2024.naacl-srw.18.\n[14] Fangxiaoyu Feng, Yinfei Yang, Daniel Cer, Naveen Arivazhagan, and Wei Wang. Language-\nagnostic BERT sentence embedding.\nIn Smaranda Muresan, Preslav Nakov, and Aline\nVillavicencio, editors, Proceedings of the 60th Annual Meeting of the Association for Com-\nputational Linguistics (Volume 1: Long Papers), pages 878–891, Dublin, Ireland, May\n2022. Association for Computational Linguistics. doi: 10.18653/v1/2022.acl-long.62. URL\nhttps://aclanthology.org/2022.acl-long.62.\n[15] Takuya Fukushima, Masae Manabe, Shuntaro Yada, Shoko Wakamiya, Eiji Aramaki, Akiko\nYoshida, Yusaku Urakawa, Akiko Maeda, Shigeyuki Kan, and Masayo Takahashi. JGCLLM: A\njapanese genetic counseling large language models (in japanese). In The 38th Annual Conference\nof the Japanese Society for Artificial Intelligence (JSAI2024), 2024.\n[16] Leo Gao, Stella Biderman, Sid Black, Laurence Golding, Travis Hoppe, Charles Foster, Jason\nPhang, Horace He, Anish Thite, Noa Nabeshima, Shawn Presser, and Connor Leahy. The pile:\nAn 800gb dataset of diverse text for language modeling, 2020.\n[17] Masatsugu Hangyo, Daisuke Kawahara, and Sadao Kurohashi. Building and analyzing a\ndiverse document leads corpus annotated with semantic relations. Journal of Natural Language\nProcessing, 21(2):213–247, 2014. doi: 10.5715/jnlp.21.213.\n[18] Dan Hendrycks, Collin Burns, Steven Basart, Andrew Critch, Jerry Li, Dawn Song, and Jacob\nSteinhardt. Aligning ai with shared human values. Proceedings of the International Conference\non Learning Representations (ICLR), 2021.\n[19] Dan Hendrycks, Collin Burns, Steven Basart, Andy Zou, Mantas Mazeika, Dawn Song, and\nJacob Steinhardt. Measuring massive multitask language understanding. In International\nConference on Learning Representations, 2021. URL https://openreview.net/forum?\nid=d7KBjmI3GmQ.\n[20] Jordan Hoffmann, Sebastian Borgeaud, Arthur Mensch, Elena Buchatskaya, Trevor Cai, Eliza\nRutherford, Diego de Las Casas, Lisa Anne Hendricks, Johannes Welbl, Aidan Clark, Tom\nHennigan, Eric Noland, Katie Millican, George van den Driessche, Bogdan Damoc, Aurelia\nGuy, Simon Osindero, Karen Simonyan, Erich Elsen, Jack W. Rae, Oriol Vinyals, and Laurent\nSifre. Training compute-optimal large language models, 2022.\n24\n\n[21] Kaito HORIO, Eiki MURATA, Hao WANG, Tatuya IDE, Daisuke KAWAHARA, Takato\nYAMAZAKI, Kenta SHINZATO, Akifumi NAKAMACHI, Shengzhe LI, and Toshinori SATO.\nVerification of chain-of-thought prompting in japanese. Proceedings of the Annual Conference\nof JSAI, JSAI2023:3T1GS602–3T1GS602, 2023. doi: 10.11517/pjsai.JSAI2023.0_3T1GS602.\n[22] Edward J Hu, Yelong Shen, Phillip Wallis, Zeyuan Allen-Zhu, Yuanzhi Li, Shean Wang,\nLu Wang, and Weizhu Chen. LoRA: Low-rank adaptation of large language models. In\nInternational Conference on Learning Representations, 2022. URL https://openreview.\nnet/forum?id=nZeVKeeFYf9.\n[23] Yufei Huang and Deyi Xiong. CBBQ: A Chinese bias benchmark dataset curated with human-AI\ncollaboration for large language models. In Nicoletta Calzolari, Min-Yen Kan, Veronique Hoste,\nAlessandro Lenci, Sakriani Sakti, and Nianwen Xue, editors, Proceedings of the 2024 Joint\nInternational Conference on Computational Linguistics, Language Resources and Evaluation\n(LREC-COLING 2024), pages 2917–2929, Torino, Italy, May 2024. ELRA and ICCL. URL\nhttps://aclanthology.org/2024.lrec-main.260.\n[24] Ai Ishii, Naoya Inoue, Hisami Suzuki, and Satoshi Sekine. JEMHopQA: Dataset for Japanese\nexplainable multi-hop question answering. In Nicoletta Calzolari, Min-Yen Kan, Veronique\nHoste, Alessandro Lenci, Sakriani Sakti, and Nianwen Xue, editors, Proceedings of the 2024\nJoint International Conference on Computational Linguistics, Language Resources and Evalu-\nation (LREC-COLING 2024), pages 9515–9525, Torino, Italia, May 2024. ELRA and ICCL.\nURL https://aclanthology.org/2024.lrec-main.831.\n[25] Jiho Jin, Jiseon Kim, Nayeon Lee, Haneul Yoo, Alice Oh, and Hwaran Lee. KoBBQ: Korean\nBias Benchmark for Question Answering. Transactions of the Association for Computational\nLinguistics, 12:507–524, 05 2024. ISSN 2307-387X. doi: 10.1162/tacl_a_00661. URL\nhttps://doi.org/10.1162/tacl_a_00661.\n[26] Armand Joulin, Edouard Grave, Piotr Bojanowski, and Tomas Mikolov. Bag of tricks for\nefficient text classification. arXiv preprint arXiv:1607.01759, 2016.\n[27] Yuki Katsumata, Masashi Takeshita, Rafal Rzepka, and Kenji Araki. Dataset construction for\npredicting danger degree due to contextual changes (in japanese). In In Proceedings of The 28th\nAnnual Meeting of The Association for Natural Language Processing (NLP-2022), 2022.\n[28] Ai Kawazoe, Ribeka Tanaka, Koji Mineshima, and Daisuke Bekki. An inference problem set for\nevaluating semantic theories and semantic processing systems for japanese. In New Frontiers in\nArtificial Intelligence: JSAI-isAI 2015 Workshops, LENLS, JURISIN, AAA, HAT-MASH, TSDAA,\nASD-HR, and SKL, Kanagawa, Japan, November 16-18, 2015, Revised Selected Papers, pages\n58–65. Springer, 2017.\n[29] Denis Kocetkov, Raymond Li, Loubna Ben Allal, Jia Li, Chenghao Mou, Carlos Muñoz\nFerrandis, Yacine Jernite, Margaret Mitchell, Sean Hughes, Thomas Wolf, Dzmitry Bahdanau,\nLeandro von Werra, and Harm de Vries. The stack: 3 tb of permissively licensed source code,\n2022.\n[30] Taku Kudo.\nSubword regularization: Improving neural network translation models with\nmultiple subword candidates. In Proceedings of the 56th Annual Meeting of the Association for\nComputational Linguistics (Volume 1: Long Papers). Association for Computational Linguistics,\n2018.\n[31] Kentaro Kurihara, Daisuke Kawahara, and Tomohide Shibata. JGLUE: Japanese general\nlanguage understanding evaluation. In Proceedings of the Thirteenth Language Resources and\nEvaluation Conference, pages 2957–2966, Marseille, France, June 2022. European Language\nResources Association. URL https://aclanthology.org/2022.lrec-1.317.\n[32] Andreas Köpf, Yannic Kilcher, Dimitri von Rütte, Sotiris Anagnostidis, Zhi-Rui Tam, Keith\nStevens, Abdullah Barhoum, Nguyen Minh Duc, Oliver Stanley, Richárd Nagyfi, Shahul ES,\nSameer Suri, David Glushkov, Arnav Dantuluri, Andrew Maguire, Christoph Schuhmann, Huu\nNguyen, and Alexander Mattick. Openassistant conversations – democratizing large language\nmodel alignment, 2023.\n25\n\n[33] Alyssa Lees, Vinh Q. Tran, Yi Tay, Jeffrey Sorensen, Jai Gupta, Donald Metzler, and Lucy\nVasserman. A new generation of perspective api: Efficient multilingual character-level trans-\nformers, 2022.\n[34] Dmitry Lepikhin, HyoukJoong Lee, Yuanzhong Xu, Dehao Chen, Orhan Firat, Yanping Huang,\nMaxim Krikun, Noam Shazeer, and Zhifeng Chen. Gshard: Scaling giant models with con-\nditional computation and automatic sharding. In 9th International Conference on Learning\nRepresentations, ICLR 2021, Virtual Event, Austria, May 3-7, 2021. OpenReview.net, 2021.\nURL https://openreview.net/forum?id=qrwe7XHTmYb.\n[35] Stephanie Lin, Jacob Hilton, and Owain Evans. TruthfulQA: Measuring how models mimic\nhuman falsehoods.\nIn Smaranda Muresan, Preslav Nakov, and Aline Villavicencio, edi-\ntors, Proceedings of the 60th Annual Meeting of the Association for Computational Lin-\nguistics (Volume 1: Long Papers), pages 3214–3252, Dublin, Ireland, May 2022. Associ-\nation for Computational Linguistics.\ndoi: 10.18653/v1/2022.acl-long.229.\nURL https:\n//aclanthology.org/2022.acl-long.229.\n[36] Yusuke Nakamura and Daisuke Kawahara. Construction of the japanese truthfulqa dataset (in\njapanese). In The 30th Annual Conference of the Association for Natural Language Processing,\n2024.\n[37] OpenAI. GPT-4 technical report. 2024.\n[38] Long Ouyang, Jeffrey Wu, Xu Jiang, Diogo Almeida, Carroll Wainwright, Pamela Mishkin,\nChong Zhang, Sandhini Agarwal, Katarina Slama, Alex Ray, John Schulman, Jacob Hilton,\nFraser Kelton, Luke Miller, Maddie Simens, Amanda Askell, Peter Welinder, Paul F Christiano,\nJan Leike, and Ryan Lowe. Training language models to follow instructions with human\nfeedback. In S. Koyejo, S. Mohamed, A. Agarwal, D. Belgrave, K. Cho, and A. Oh, editors,\nAdvances in Neural Information Processing Systems, volume 35, pages 27730–27744. Curran\nAssociates, Inc., 2022. URL https://proceedings.neurips.cc/paper_files/paper/\n2022/file/b1efde53be364a73914f58805a001731-Paper-Conference.pdf.\n[39] Long Ouyang, Jeffrey Wu, Xu Jiang, Diogo Almeida, Carroll Wainwright, Pamela Mishkin,\nChong Zhang, Sandhini Agarwal, Katarina Slama, Alex Ray, John Schulman, Jacob Hilton,\nFraser Kelton, Luke Miller, Maddie Simens, Amanda Askell, Peter Welinder, Paul F Christiano,\nJan Leike, and Ryan Lowe. Training language models to follow instructions with human\nfeedback. In S. Koyejo, S. Mohamed, A. Agarwal, D. Belgrave, K. Cho, and A. Oh, editors,\nAdvances in Neural Information Processing Systems, volume 35, pages 27730–27744. Curran\nAssociates, Inc., 2022. URL https://proceedings.neurips.cc/paper_files/paper/\n2022/file/b1efde53be364a73914f58805a001731-Paper-Conference.pdf.\n[40] Alicia Parrish, Angelica Chen, Nikita Nangia, Vishakh Padmakumar, Jason Phang, Jana Thomp-\nson, Phu Mon Htut, and Samuel Bowman. BBQ: A hand-built bias benchmark for question\nanswering. In Smaranda Muresan, Preslav Nakov, and Aline Villavicencio, editors, Findings of\nthe Association for Computational Linguistics: ACL 2022, pages 2086–2105, Dublin, Ireland,\nMay 2022. Association for Computational Linguistics. doi: 10.18653/v1/2022.findings-acl.165.\nURL https://aclanthology.org/2022.findings-acl.165.\n[41] Fred Philippy, Siwen Guo, and Shohreh Haddadan. Towards a common understanding of\ncontributing factors for cross-lingual transfer in multilingual language models: A review. In\nAnna Rogers, Jordan Boyd-Graber, and Naoaki Okazaki, editors, Proceedings of the 61st\nAnnual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers),\npages 5877–5891, Toronto, Canada, July 2023. Association for Computational Linguistics. doi:\n10.18653/v1/2023.acl-long.323. URL https://aclanthology.org/2023.acl-long.323.\n[42] Alec Radford, Jeffrey Wu, Rewon Child, David Luan, Dario Amodei, and Ilya Sutskever.\nLanguage models are unsupervised multitask learners, 2018. URL https://d4mucfpksywv.\ncloudfront.net/better-language-models/language-models.pdf.\n[43] Rafael Rafailov, Archit Sharma, Eric Mitchell, Christopher D Manning, Stefano Ermon, and\nChelsea Finn. Direct preference optimization: Your language model is secretly a reward\nmodel. In Thirty-seventh Conference on Neural Information Processing Systems, 2023. URL\nhttps://arxiv.org/abs/2305.18290.\n26\n\n[44] Samyam Rajbhandari, Jeff Rasley, Olatunji Ruwase, and Yuxiong He. Zero: memory opti-\nmizations toward training trillion parameter models. In Christine Cuicchi, Irene Qualters, and\nWilliam T. Kramer, editors, Proceedings of the International Conference for High Performance\nComputing, Networking, Storage and Analysis, SC 2020, Virtual Event / Atlanta, Georgia, USA,\nNovember 9-19, 2020, page 20. IEEE/ACM, 2020. doi: 10.1109/SC41405.2020.00024. URL\nhttps://doi.org/10.1109/SC41405.2020.00024.\n[45] Keisuke Sakaguchi, Ronan Le Bras, Chandra Bhagavatula, and Yejin Choi. Winogrande: an\nadversarial winograd schema challenge at scale. Commun. ACM, 64(9):99–106, aug 2021. ISSN\n0001-0782. doi: 10.1145/3474381. URL https://doi.org/10.1145/3474381.\n[46] John Schulman, Filip Wolski, Prafulla Dhariwal, Alec Radford, and Oleg Klimov. Proximal pol-\nicy optimization algorithms. CoRR, abs/1707.06347, 2017. URL http://dblp.uni-trier.\nde/db/journals/corr/corr1707.html#SchulmanWDRK17.\n[47] Satoshi Sekine, Maya Ando, Michiko Goto, Hisami Suzuki, Daisuke Kawahara, Naoya Inoue,\nand Kentaro Inui. ichikara-instruction: Constructing a japanese instruction dataset for llms. In\nIn Proceedings of The Thirtieth Annual Meeting of The Association for Natural Language Pro-\ncessing (NLP2024), pages 1508–1513, 2024. URL https://www.anlp.jp/proceedings/\nannual_meeting/2024/pdf_dir/A6-3.pdf. in Japanese.\n[48] Mohammad Shoeybi, Mostofa Patwary, Raul Puri, Patrick LeGresley, Jared Casper, and Bryan\nCatanzaro. Megatron-lm: Training multi-billion parameter language models using model\nparallelism. arXiv, abs/1909.08053, 2019. URL http://arxiv.org/abs/1909.08053.\n[49] Tomoki Sugimoto, Yasumasa Onoe, and Hitomi Yanaka. Jamp: Controlled Japanese temporal\ninference dataset for evaluating generalization capacity of language models. In Proceedings of\nthe 61st Annual Meeting of the Association for Computational Linguistics (Volume 4: Student\nResearch Workshop), pages 57–68, Toronto, Canada, July 2023. Association for Computational\nLinguistics. URL https://aclanthology.org/2023.acl-srw.8.\n[50] Yikun Sun, Zhen Wan, Nobuhiro Ueda, Sakiko Yahata, Fei Cheng, Chenhui Chu, and Sadao\nKurohashi. Rapidly developing high-quality instruction data and evaluation benchmark for\nlarge language models with minimal human effort: A case study on Japanese. In Nicoletta\nCalzolari, Min-Yen Kan, Veronique Hoste, Alessandro Lenci, Sakriani Sakti, and Nianwen Xue,\neditors, Proceedings of the 2024 Joint International Conference on Computational Linguistics,\nLanguage Resources and Evaluation (LREC-COLING 2024), pages 13537–13547, Torino, Italia,\nMay 2024. ELRA and ICCL. URL https://aclanthology.org/2024.lrec-main.1184.\n[51] Toyotaro Suzumura, Akiyoshi Sugiki, Hiroyuki Takizawa, Akira Imakura, Hiroshi Nakamura,\nKenjiro Taura, Tomohiro Kudoh, Toshihiro Hanawa, Yuji Sekiya, Hiroki Kobayashi, Yohei Kuga,\nRyo Nakamura, Renhe Jiang, Junya Kawase, Masatoshi Hanai, Hiroshi Miyazaki, Tsutomu\nIshizaki, Daisuke Shimotoku, Daisuke Miyamoto, Kento Aida, Atsuko Takefusa, Takashi\nKurimoto, Koji Sasayama, Naoya Kitagawa, Ikki Fujiwara, Yusuke Tanimura, Takayuki Aoki,\nToshio Endo, Satoshi Ohshima, Keiichiro Fukazawa, Susumu Date, and Toshihiro Uchibayashi.\nmdx: A cloud platform for supporting data science and cross-disciplinary research collaborations.\nIn 2022 IEEE Intl Conf on Dependable, Autonomic and Secure Computing, Intl Conf on\nPervasive Intelligence and Computing, Intl Conf on Cloud and Big Data Computing, Intl Conf\non Cyber Science and Technology Congress (DASC/PiCom/CBDCom/CyberSciTech), pages\n1–7, 2022. doi: 10.1109/DASC/PiCom/CBDCom/Cy55231.2022.9927975.\n[52] Masashi Takeshita, Rafal Rzpeka, and Kenji Araki. Jcommonsensemorality: Japanese dataset\nfor evaluating commonsense morality understanding. In In Proceedings of The Twenty Nineth\nAnnual Meeting of The Association for Natural Language Processing (NLP2023), pages 357–\n362, 2023.\nURL https://www.anlp.jp/proceedings/annual_meeting/2023/pdf_\ndir/D2-1.pdf. in Japanese.\n[53] Gemini Team. Gemini: A family of highly capable multimodal models, 2024.\n[54] Ye Kyaw Thu, Win Pa Pa, Masao Utiyama, Andrew Finch, and Eiichiro Sumita. Introducing the\nAsian language treebank (ALT). In Nicoletta Calzolari, Khalid Choukri, Thierry Declerck, Sara\nGoggi, Marko Grobelnik, Bente Maegaard, Joseph Mariani, Helene Mazo, Asuncion Moreno,\n27\n\nJan Odijk, and Stelios Piperidis, editors, Proceedings of the Tenth International Conference on\nLanguage Resources and Evaluation (LREC’16), pages 1574–1578, Portorož, Slovenia, May\n2016. European Language Resources Association (ELRA). URL https://aclanthology.\norg/L16-1249.\n[55] Hugo Touvron, Louis Martin, Kevin Stone, Peter Albert, Amjad Almahairi, Yasmine Babaei,\nNikolay Bashlykov, Soumya Batra, Prajjwal Bhargava, Shruti Bhosale, Dan Bikel, Lukas\nBlecher, Cristian Canton Ferrer, Moya Chen, Guillem Cucurull, David Esiobu, Jude Fernandes,\nJeremy Fu, Wenyin Fu, Brian Fuller, Cynthia Gao, Vedanuj Goswami, Naman Goyal, Anthony\nHartshorn, Saghar Hosseini, Rui Hou, Hakan Inan, Marcin Kardas, Viktor Kerkez, Madian\nKhabsa, Isabel Kloumann, Artem Korenev, Punit Singh Koura, Marie-Anne Lachaux, Thibaut\nLavril, Jenya Lee, Diana Liskovich, Yinghai Lu, Yuning Mao, Xavier Martinet, Todor Mihaylov,\nPushkar Mishra, Igor Molybog, Yixin Nie, Andrew Poulton, Jeremy Reizenstein, Rashi Rungta,\nKalyan Saladi, Alan Schelten, Ruan Silva, Eric Michael Smith, Ranjan Subramanian, Xiao-\nqing Ellen Tan, Binh Tang, Ross Taylor, Adina Williams, Jian Xiang Kuan, Puxin Xu, Zheng\nYan, Iliyan Zarov, Yuchen Zhang, Angela Fan, Melanie Kambadur, Sharan Narang, Aurelien\nRodriguez, Robert Stojnic, Sergey Edunov, and Thomas Scialom. Llama 2: Open foundation\nand fine-tuned chat models, 2023.\n[56] Yuxia Wang, Haonan Li, Xudong Han, Preslav Nakov, and Timothy Baldwin. Do-not-answer:\nEvaluating safeguards in LLMs. In Yvette Graham and Matthew Purver, editors, Findings of\nthe Association for Computational Linguistics: EACL 2024, pages 896–911, St. Julian’s, Malta,\nMarch 2024. Association for Computational Linguistics. URL https://aclanthology.org/\n2024.findings-eacl.61.\n[57] Linting Xue, Noah Constant, Adam Roberts, Mihir Kale, Rami Al-Rfou, Aditya Siddhant,\nAditya Barua, and Colin Raffel.\nmT5: A massively multilingual pre-trained text-to-text\ntransformer. In Kristina Toutanova, Anna Rumshisky, Luke Zettlemoyer, Dilek Hakkani-Tur,\nIz Beltagy, Steven Bethard, Ryan Cotterell, Tanmoy Chakraborty, and Yichao Zhou, editors,\nProceedings of the 2021 Conference of the North American Chapter of the Association for\nComputational Linguistics: Human Language Technologies, pages 483–498, Online, June\n2021. Association for Computational Linguistics. doi: 10.18653/v1/2021.naacl-main.41. URL\nhttps://aclanthology.org/2021.naacl-main.41.\n[58] Hitomi Yanaka and Koji Mineshima. Assessing the generalization capacity of pre-trained\nlanguage models through Japanese adversarial natural language inference. In Proceedings of\nthe 2021 EMNLP Workshop BlackboxNLP: Analyzing and Interpreting Neural Networks for\nNLP (BlackboxNLP2021), 2021.\n[59] Hitomi Yanaka and Koji Mineshima. Compositional evaluation on Japanese textual entailment\nand similarity. Transactions of the Association for Computational Linguistics, 10:1266–1284,\n2022. doi: 10.1162/tacl_a_00518. URL https://aclanthology.org/2022.tacl-1.73.\n[60] Hitomi Yanaka, Namgi Han, Ryoma Kumon, Jie Lu, Masashi Takeshita, Ryo Sekizawa,\nTaisei Kato, and Hiromi Arai. Analyzing social biases in japanese large language models.\narxiv:2406.02050, 2024.\n[61] Zhangyue Yin, Qiushi Sun, Qipeng Guo, Jiawen Wu, Xipeng Qiu, and Xuanjing Huang.\nDo large language models know what they don’t know?\nIn Anna Rogers, Jordan Boyd-\nGraber, and Naoaki Okazaki, editors, Findings of the Association for Computational Linguistics:\nACL 2023, pages 8653–8665, Toronto, Canada, July 2023. Association for Computational\nLinguistics. doi: 10.18653/v1/2023.findings-acl.551. URL https://aclanthology.org/\n2023.findings-acl.551.\n[62] Rowan Zellers, Ari Holtzman, Yonatan Bisk, Ali Farhadi, and Yejin Choi. HellaSwag: Can a\nmachine really finish your sentence? In Anna Korhonen, David Traum, and Lluís Màrquez, edi-\ntors, Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics,\npages 4791–4800, Florence, Italy, July 2019. Association for Computational Linguistics. doi:\n10.18653/v1/P19-1472. URL https://aclanthology.org/P19-1472.\n[63] Lianmin Zheng, Wei-Lin Chiang, Ying Sheng, Siyuan Zhuang, Zhanghao Wu, Yonghao\nZhuang, Zi Lin, Zhuohan Li, Dacheng Li, Eric Xing, Hao Zhang, Joseph E Gonzalez, and\n28\n\nIon Stoica. Judging llm-as-a-judge with mt-bench and chatbot arena. In A. Oh, T. Nau-\nmann, A. Globerson, K. Saenko, M. Hardt, and S. Levine, editors, Advances in Neu-\nral Information Processing Systems, volume 36, pages 46595–46623. Curran Associates,\nInc., 2023. URL https://proceedings.neurips.cc/paper_files/paper/2023/file/\n91f18a1287b398d378ef22505bf41832-Paper-Datasets_and_Benchmarks.pdf.\n29\n"
    }
  ]
}