{
  "1-5 (Architecture)": "For the llm-jp/llm-jp-13b-instruct-full-jaster-dolly-oasst-v1.0 series the quotes explicitly state “|13b model|13b|40|5120|40|2048|”, making clear that this release is the 13 billion-parameter variant.  The same block and the accompanying JSON fragments give concrete hyper-parameters: \"n_layer\": 40 means the network is built from forty Transformer decoder layers; \"n_embd\": 5120 shows each token is projected to a 5 120-wide hidden dimension; \"n_head\": 40 confirms forty parallel attention heads per layer; and \"n_positions\": 2048 defines a 2 048-token maximum context window.  A bullet line reinforces that it is a “Transformer-based Language Model.”  Taken together these quotes describe a conventional GPT-style decoder-only Transformer, scaled to the 13 b model size, with 40×(self-attention + MLP) blocks, 5 120-dim embeddings, 40-way multi-head attention, and a 2 048-token sequence length—parameters that match the architecture configuration used throughout the jp/full/jaster/dolly/oasst v1.0 family.",
  "1-5 (Architecture)__evidence": [
    {
      "source": "[readme]",
      "quote": "|13b model|13b|40|5120|40|2048|"
    },
    {
      "source": "[readme]",
      "quote": "- **Model type:** Transformer-based Language Model"
    },
    {
      "source": "[config]",
      "quote": "\"n_layer\": 40,"
    },
    {
      "source": "[config]",
      "quote": "\"n_embd\": 5120,"
    },
    {
      "source": "[config]",
      "quote": "\"n_head\": 40,"
    },
    {
      "source": "[config]",
      "quote": "\"n_positions\": 2048,"
    }
  ],
  "1-6 (Tokenizer)": "The tokenizer shipped with llm-jp-13b-instruct-full-jaster-dolly-oasst-v1.0 is described in multiple placeholders:  it is a Hugging-Face Fast Tokenizer implementation built on the `tokenizers` Rust core (requires tokenizers ≥ 0.14.0).  Internally it uses the SentencePiece Unigram algorithm with the byte-fallback extension, enabling lossless treatment of out-of-vocabulary bytes.  Training data for the tokenizer is said to be “a subset of the datasets for model pre-training,” ensuring full coverage of the same Japanese, English and code domains the model sees.  The resulting vocabulary contains 50 570 mergeable pieces that jointly encode Japanese, English and source-code tokens.  All these points—Unigram byte-fallback, HF Fast runtime, 50 570-entry mixed-language vocabulary—are taken verbatim from the supplied quotes and together give a complete picture of the text-processing front-end used by the v1.0 13b model.",
  "1-6 (Tokenizer)__evidence": [
    {
      "source": "[readme]",
      "quote": "The tokenizer of this model is based on [huggingface/tokenizers](https://github.com/huggingface/tokenizers) Unigram byte-fallback model."
    },
    {
      "source": "[readme]",
      "quote": "- **Model:** Hugging Face Fast Tokenizer using Unigram byte-fallback model which requires `tokenizers>=0.14.0`"
    },
    {
      "source": "[readme]",
      "quote": "- **Training algorithm:** SentencePiece Unigram byte-fallback"
    },
    {
      "source": "[readme]",
      "quote": "- **Training data:** A subset of the datasets for model pre-training"
    },
    {
      "source": "[readme]",
      "quote": "- **Vocabulary size:** 50,570 (mixed vocabulary of Japanese, English, and source code)"
    }
  ],
  "2-1 (Hardware)": "The hardware quotes mention two concrete GPU configurations on the mdx.jp cluster: (1) “96 × A100 40 GB” and (2) “8 × A100 40 GB.”  This indicates that the large-scale pre-training phase for llm-jp/llm-jp-13b-instruct-full-jaster-dolly-oasst-v1.0 was performed on up to ninety-six NVIDIA A100 40 GB cards, providing the multi-node, multi-GPU compute necessary for a 13 b parameter run.  A smaller eight-GPU slice of the same hardware is also listed, implying that later fine-tuning, alignment, or experimentation stages were executed on a single node equipped with 8×A100 40 GB.  No other hardware types appear in the quotes, so all reported compute for this v1.0 release is A100-based.",
  "2-1 (Hardware)__evidence": [
    {
      "source": "[readme]",
      "quote": "- **Hardware:** 96 A100 40GB GPUs ([mdx cluster](https://mdx.jp/en/))"
    },
    {
      "source": "[readme]",
      "quote": "- **Hardware:** 8 A100 40GB GPUs ([mdx cluster](https://mdx.jp/en/))"
    }
  ],
  "2-2 (Software)": "Training for llm-jp-13b-instruct-full-jaster-dolly-oasst-v1.0 used a DeepSpeed/Megatron stack according to the bullet “Software: Megatron-DeepSpeed.”  For instruction-fine-tuning or RLHF style alignment the authors additionally list the Hugging-Face tooling trio “TRL, PEFT, and DeepSpeed,” again pointing to a DeepSpeed backbone but with higher-level helper libraries for PPO/LoRA workflows.  The environment requirements are captured verbatim: PyTorch ≥ 2.0.0, Transformers ≥ 4.34.0, Tokenizers ≥ 0.14.0, and Accelerate == 0.23.0.  These version pins define the software stack that both pre-training and downstream tuning must reproduce to run or continue training the v1.0 13b model.",
  "2-2 (Software)__evidence": [
    {
      "source": "[readme]",
      "quote": "- **Software:** Megatron-DeepSpeed"
    },
    {
      "source": "[readme]",
      "quote": "- **Software:** [TRL](https://github.com/huggingface/trl), [PEFT](https://github.com/huggingface/peft), and [DeepSpeed](https://github.com/microsoft/DeepSpeed)"
    },
    {
      "source": "[readme]",
      "quote": "- torch>=2.0.0"
    },
    {
      "source": "[readme]",
      "quote": "- transformers>=4.34.0"
    },
    {
      "source": "[readme]",
      "quote": "- tokenizers>=0.14.0"
    },
    {
      "source": "[readme]",
      "quote": "- accelerate==0.23.0"
    }
  ]
}