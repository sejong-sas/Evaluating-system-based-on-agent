{
  "pretrain_method": "Hardware: 96 A100 40GB GPUs ([mdx cluster](https://mdx.jp/en/)); Software: Megatron-DeepSpeed",
  "pretrain_data": "The pre-training was continuously conducted using a total of 10 folds of non-overlapping data, each consisting of approximately 27-28B tokens.",
  "__evidence": [
    {
      "source": "readme",
      "quote": "Hardware: 96 A100 40GB GPUs ([mdx cluster](https://mdx.jp/en/))"
    },
    {
      "source": "readme",
      "quote": "Software: Megatron-DeepSpeed"
    },
    {
      "source": "readme",
      "quote": "The pre-training was continuously conducted using a total of 10 folds of non-overlapping data, each consisting of approximately 27-28B tokens."
    }
  ]
}