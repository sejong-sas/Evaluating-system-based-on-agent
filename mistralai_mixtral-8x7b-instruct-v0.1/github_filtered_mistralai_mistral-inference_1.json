{
  "1-1 (Weights)": "The project explicitly provides public download links and checksums for both the Base and Instruct variants of the 7-billion-parameter model. Quoted lines:\n• \"| 7B Instruct | https://models.mistralcdn.com/mistral-7b-v0-3/mistral-7B-Instruct-v0.3.tar | `80b71fcb6416085bcb4efad86dfb4d52` |\" \n• \"| 7B Base | https://models.mistralcdn.com/mistral-7b-v0-3/mistral-7B-v0.3.tar | `0663b293810d7571dad25dae2f2a5806` |\"  \nThese lines show that anyone can retrieve the weight archives directly from the listed URLs and verify integrity by matching the MD5/SHA-style strings. The documentation also highlights that earlier revisions remain available: \"You can download the previous versions of our models from our [docs](https://docs.mistral.ai/getting-started/open_weight_models/#downloading).\"  Together, these quotes indicate that weights for multiple versions are openly hosted, downloadable via HTTPS, and accompanied by hash values for reproducibility.",
  "1-1 (Weights)__evidence": [
    {
      "source": "[readme]",
      "quote": "| 7B Instruct | https://models.mistralcdn.com/mistral-7b-v0-3/mistral-7B-Instruct-v0.3.tar | `80b71fcb6416085bcb4efad86dfb4d52` |"
    },
    {
      "source": "[readme]",
      "quote": "| 7B Base | https://models.mistralcdn.com/mistral-7b-v0-3/mistral-7B-v0.3.tar | `0663b293810d7571dad25dae2f2a5806` |"
    },
    {
      "source": "[readme]",
      "quote": "You can download the previous versions of our models from our [docs](https://docs.mistral.ai/getting-started/open_weight_models/#downloading)."
    }
  ],
  "1-2 (Code)": "The repository accompanying the model publishes only the portions needed for running/inference, not full training pipelines. Its own description states: \"This repository contains minimal code to run Mistral models.\"  A dedicated deployment directory is supplied: \"The `deploy` folder contains code to build a [vLLM](https://M7B_DIR.com/vllm-project/vllm) image with the required dependencies to serve the Mistral AI model.\"  Users can quickly validate installation by executing an included utility: \"To test that a model works in your setup, you can run the `mistral-demo` command.\"  No material about pre-training, fine-tuning, or RLHF scripts is mentioned, so the public code scope is limited to inference/serving.",
  "1-2 (Code)__evidence": [
    {
      "source": "[readme]",
      "quote": "This repository contains minimal code to run Mistral models."
    },
    {
      "source": "[readme]",
      "quote": "The `deploy` folder contains code to build a [vLLM](https://M7B_DIR.com/vllm-project/vllm) image with the required dependencies to serve the Mistral AI model."
    },
    {
      "source": "[readme]",
      "quote": "To test that a model works in your setup, you can run the `mistral-demo` command."
    }
  ],
  "1-3 (License)": "All quoted excerpts reproduce the text of the Apache License 2.0, January 2004, which governs the repository: \n• \"Apache License\\n                           Version 2.0, January 2004\" \n• \"Licensed under the Apache License, Version 2.0 (the \\\"License\\\");   you may not use this file except in compliance with the License.\" \n• \"Subject to the terms and conditions of       this License, each Contributor hereby grants to You a perpetual,       worldwide, non-exclusive, no-charge, royalty-free, irrevocable       copyright license to reproduce, prepare Derivative Works of,       publicly display, publicly perform, sublicense, and distribute the       Work and such Derivative Works in Source or Object form.\"  These statements confirm that the model’s accompanying artifacts and code are covered by an OSI-approved permissive license, granting broad rights for use, modification, redistribution, and commercial exploitation, subject only to the standard Apache-2.0 conditions (notice preservation, patent clause, etc.).",
  "1-3 (License)__evidence": [
    {
      "source": "[license_files]",
      "quote": "Apache License\n                           Version 2.0, January 2004"
    },
    {
      "source": "[license_files]",
      "quote": "Licensed under the Apache License, Version 2.0 (the \"License\");   you may not use this file except in compliance with the License."
    },
    {
      "source": "[license_files]",
      "quote": "Subject to the terms and conditions of       this License, each Contributor hereby grants to You a perpetual,       worldwide, non-exclusive, no-charge, royalty-free, irrevocable       copyright license to reproduce, prepare Derivative Works of,       publicly display, publicly perform, sublicense, and distribute the       Work and such Derivative Works in Source or Object form."
    }
  ],
  "1-4 (Paper)": "No formal academic paper is quoted, but two official Mistral AI blog posts serve as technical write-ups and announcements: \n• \"Blog 7B: https://mistral.ai/news/announcing-mistral-7b/\" \n• \"Blog 8x7B: https://mistral.ai/news/mixtral-of-experts/\"  These links provide architecture details, training choices, evaluation results, and usage guidance for the 7 B model family (and its 8×7 B mixture-of-experts sibling).",
  "1-4 (Paper)__evidence": [
    {
      "source": "[readme]",
      "quote": "Blog 7B: [https://mistral.ai/news/announcing-mistral-7b/](https://mistral.ai/news/announcing-mistral-7b/)\\"
    },
    {
      "source": "[readme]",
      "quote": "Blog 8x7B: [https://mistral.ai/news/mixtral-of-experts/](https://mistral.ai/news/mixtral-of-experts/)\\"
    }
  ]
}