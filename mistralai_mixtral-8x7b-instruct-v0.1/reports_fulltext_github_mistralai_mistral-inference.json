{
  "repo": "mistralai/mistral-inference",
  "full_texts": [
    {
      "arxiv_id": "https://mistral.ai/news/announcing-mistral-7b/",
      "full_text": " Mistral 7B | Mistral AI Products Solutions Research Resources Pricing Company Try the API Talk to sales Mistral 7B The best 7B model to date, Apache 2.0 Product Sep 27, 2023 Mistral AI team Mistral AI team is proud to release Mistral 7B, the most powerful language model for its size to date. Mistral 7B in short Mistral 7B is a 7.3B parameter model that: Outperforms Llama 2 13B on all benchmarks Outperforms Llama 1 34B on many benchmarks Approaches CodeLlama 7B performance on code, while remaining good at English tasks Uses Grouped-query attention (GQA) for faster inference Uses Sliding Window Attention (SWA) to handle longer sequences at smaller cost We&#x27;re releasing Mistral 7B under the Apache 2.0 license, it can be used without restrictions. Download it and use it anywhere (including locally) with our reference implementation , Deploy it on any cloud (AWS/GCP/Azure), using vLLM inference server and skypilot , Use it on HuggingFace . Mistral 7B is easy to fine-tune on any task. As a demonstration, we&#x27;re providing a model fine-tuned for chat, which outperforms Llama 2 13B chat. Performance in details We compared Mistral 7B to the Llama 2 family, and re-run all model evaluations ourselves for fair comparison. Performance of Mistral 7B and different Llama models on a wide range of benchmarks. For all metrics, all models were re-evaluated with our evaluation pipeline for accurate comparison. Mistral 7B significantly outperforms Llama 2 13B on all metrics, and is on par with Llama 34B (since Llama 2 34B was not released, we report results on Llama 34B). It is also vastly superior in code and reasoning benchmarks. The benchmarks are categorized by their themes: Commonsense Reasoning: 0-shot average of Hellaswag, Winogrande, PIQA, SIQA, OpenbookQA, ARC-Easy, ARC-Challenge, and CommonsenseQA. World Knowledge: 5-shot average of NaturalQuestions and TriviaQA. Reading Comprehension: 0-shot average of BoolQ and QuAC. Math: Average of 8-shot GSM8K with maj@8 and 4-shot MATH with maj@4 Code: Average of 0-shot Humaneval and 3-shot MBPP Popular aggregated results: 5-shot MMLU, 3-shot BBH, and 3-5-shot AGI Eval (English multiple-choice questions only) An interesting metric to compare how models fare in the cost/performance plane is to compute “equivalent model sizes”. On reasoning, comprehension and STEM reasoning (MMLU), Mistral 7B performs equivalently to a Llama 2 that would be more than 3x its size. This is as much saved in memory and gained in throughput. Results on MMLU, Commonsense Reasoning, World Knowledge and Reading comprehension for Mistral 7B and Llama 2 (7B/13/70B). Mistral 7B largely outperforms Llama 2 13B on all evaluations, except on knowledge benchmarks, where it is on par (this is likely due to its limited parameter count, which restricts the amount of knowledge it can compress). Note : Important differences between our evaluation and the LLaMA2 paper&#x27;s: For MBPP, we use the hand-verified subset For TriviaQA, we do not provide Wikipedia contexts Flash and Furious: Attention drift Mistral 7B uses a sliding window attention (SWA) mechanism ( Child et al. , Beltagy et al. ), in which each layer attends to the previous 4,096 hidden states. The main improvement, and reason for which this was initially investigated, is a linear compute cost of O(sliding_window.seq_len). In practice, changes made to FlashAttention and xFormers yield a 2x speed improvement for sequence length of 16k with a window of 4k. A huge thanks to Tri Dao and Daniel Haziza for helping include these changes on a tight schedule. Sliding window attention exploits the stacked layers of a transformer to attend in the past beyond the window size: A token i at layer k attends to tokens [i-sliding_window, i] at layer k-1 . These tokens attended to tokens [i-2*sliding_window, i] . Higher layers have access to information further in the past than what the attention patterns seems to entail. Finally, a fixed attention span means we can limit our cache to a size of sliding_window tokens, using rotating buffers (read more in our reference implementation repo ). This saves half of the cache memory for inference on sequence length of 8192 , without impacting model quality. Fine-tuning Mistral 7B for chat To show the generalization capabilities of Mistral 7B, we fine-tuned it on instruction datasets publicly available on HuggingFace. No tricks, no proprietary data. The resulting model, Mistral 7B Instruct , outperforms all 7B models on MT-Bench , and is comparable to 13B chat models. Note The Mistral 7B Instruct model is a quick demonstration that the base model can be easily fine-tuned to achieve compelling performance. It does not have any moderation mechanism. We&#x27;re looking forward to engaging with the community on ways to make the model finely respect guardrails, allowing for deployment in environments requiring moderated outputs. Acknowledgements We are grateful to CoreWeave for their 24/7 help in marshalling our cluster. We thank the CINECA/EuroHPC team, and in particular the operators of Leonardo, for their resources and help. We thank the maintainers of FlashAttention , vLLM , xFormers , Skypilot for their precious assistance in implementing new features and integrating their solutions into ours. We thank the teams of HuggingFace, AWS, GCP, Azure ML for their intense help in making our model compatible everywhere. Share More resources News Models AI Services The next chapter of AI is yours. Try le Chat Build on la Plateforme Talk to an expert Mistral AI © 2025 Why Mistral About us Our customers Careers Contact us Explore AI solutions Partners Research Documentation Build La Plateforme Le Chat Mistral Code Mistral Compute Try the API Legal Terms of service Privacy policy Privacy choices Data processing agreement Legal notice Brand en Mistral AI © 2025 "
    },
    {
      "arxiv_id": "https://mistral.ai/news/mixtral-of-experts/",
      "full_text": " Mixtral of experts | Mistral AI Products Solutions Research Resources Pricing Company Try the API Talk to sales Mixtral of experts A high quality Sparse Mixture-of-Experts. Research Dec 11, 2023 Mistral AI team Mistral AI continues its mission to deliver the best open models to the developer community. Moving forward in AI requires taking new technological turns beyond reusing well-known architectures and training paradigms. Most importantly, it requires making the community benefit from original models to foster new inventions and usages. Today, the team is proud to release Mixtral 8x7B, a high-quality sparse mixture of experts model (SMoE) with open weights. Licensed under Apache 2.0. Mixtral outperforms Llama 2 70B on most benchmarks with 6x faster inference. It is the strongest open-weight model with a permissive license and the best model overall regarding cost/performance trade-offs. In particular, it matches or outperforms GPT3.5 on most standard benchmarks. Mixtral has the following capabilities. It gracefully handles a context of 32k tokens. It handles English, French, Italian, German and Spanish. It shows strong performance in code generation. It can be finetuned into an instruction-following model that achieves a score of 8.3 on MT-Bench. Pushing the frontier of open models with sparse architectures Mixtral is a sparse mixture-of-experts network. It is a decoder-only model where the feedforward block picks from a set of 8 distinct groups of parameters. At every layer, for every token, a router network chooses two of these groups (the “experts”) to process the token and combine their output additively. This technique increases the number of parameters of a model while controlling cost and latency, as the model only uses a fraction of the total set of parameters per token. Concretely, Mixtral has 46.7B total parameters but only uses 12.9B parameters per token. It, therefore, processes input and generates output at the same speed and for the same cost as a 12.9B model. Mixtral is pre-trained on data extracted from the open Web – we train experts and routers simultaneously. Performance We compare Mixtral to the Llama 2 family and the GPT3.5 base model. Mixtral matches or outperforms Llama 2 70B, as well as GPT3.5, on most benchmarks. On the following figure, we measure the quality versus inference budget tradeoff. Mistral 7B and Mixtral 8x7B belong to a family of highly efficient models compared to Llama 2 models. The following table give detailed results on the figure above. Hallucination and biases. To identify possible flaws to be corrected by fine-tuning / preference modelling, we measure the base model performance on BBQ/BOLD. Compared to Llama 2, Mixtral presents less bias on the BBQ benchmark. Overall, Mixtral displays more positive sentiments than Llama 2 on BOLD, with similar variances within each dimension. Language. Mixtral 8x7B masters French, German, Spanish, Italian, and English. Instructed models We release Mixtral 8x7B Instruct alongside Mixtral 8x7B. This model has been optimised through supervised fine-tuning and direct preference optimisation (DPO) for careful instruction following. On MT-Bench, it reaches a score of 8.30, making it the best open-source model, with a performance comparable to GPT3.5. Note: Mixtral can be gracefully prompted to ban some outputs from constructing applications that require a strong level of moderation, as exemplified here . A proper preference tuning can also serve this purpose. Bear in mind that without such a prompt, the model will just follow whatever instructions are given. Deploy Mixtral with an open-source deployment stack To enable the community to run Mixtral with a fully open-source stack, we have submitted changes to the vLLM project, which integrates Megablocks CUDA kernels for efficient inference. Skypilot allows the deployment of vLLM endpoints on any instance in the cloud. Use Mixtral on our platform. We&#x27;re currently using Mixtral 8x7B behind our endpoint mistral-small , which is available in beta . Register to get early access to all generative and embedding endpoints. Acknowledgement We thank CoreWeave and Scaleway teams for technical support as we trained our models. Share More resources News Models AI Services The next chapter of AI is yours. Try le Chat Build on la Plateforme Talk to an expert Mistral AI © 2025 Why Mistral About us Our customers Careers Contact us Explore AI solutions Partners Research Documentation Build La Plateforme Le Chat Mistral Code Mistral Compute Try the API Legal Terms of service Privacy policy Privacy choices Data processing agreement Legal notice Brand en Mistral AI © 2025 "
    },
    {
      "arxiv_id": "https://mistral.ai/news/mixtral-8x22b/",
      "full_text": " Cheaper, Better, Faster, Stronger | Mistral AI Products Solutions Research Resources Pricing Company Try the API Talk to sales Cheaper, Better, Faster, Stronger Continuing to push the frontier of AI and making it accessible to all. Research Apr 17, 2024 Mistral AI team Mixtral 8x22B is our latest open model. It sets a new standard for performance and efficiency within the AI community. It is a sparse Mixture-of-Experts (SMoE) model that uses only 39B active parameters out of 141B, offering unparalleled cost efficiency for its size. Mixtral 8x22B comes with the following strengths: It is fluent in English, French, Italian, German, and Spanish It has strong mathematics and coding capabilities It is natively capable of function calling ; along with the constrained output mode implemented on la Plateforme, this enables application development and tech stack modernisation at scale Its 64K tokens context window allows precise information recall from large documents Truly open We believe in the power of openness and broad distribution to promote innovation and collaboration in AI. We are, therefore, releasing Mixtral 8x22B under Apache 2.0, the most permissive open-source licence, allowing anyone to use the model anywhere without restrictions. Efficiency at its finest We build models that offer unmatched cost efficiency for their respective sizes , delivering the best performance-to-cost ratio within models provided by the community. Mixtral 8x22B is a natural continuation of our open model family. Its sparse activation patterns make it faster than any dense 70B model, while being more capable than any other open-weight model (distributed under permissive or restrictive licenses). The base model&#x27;s availability makes it an excellent basis for fine-tuning use cases. Figure 1: Measure of the performance (MMLU) versus inference budget tradeoff (number of active parameters). Mistral 7B, Mixtral 8x7B and Mixtral 8x22B all belong to a family of highly efficient models compared to the other open models. Unmatched open performance The following is a comparison of open models on standard industry benchmarks. Reasoning and knowledge Mixtral 8x22B is optimized for reasoning. Figure 2: Performance on widespread common sense, reasoning and knowledge benchmarks of the top-leading LLM open models: MMLU (Measuring massive multitask language in understanding), HellaSwag (10-shot), Wino Grande (5-shot), Arc Challenge (5-shot), Arc Challenge (25-shot), TriviaQA (5-shot) and NaturalQS (5-shot). Multilingual capabilities Mixtral 8x22B has native multilingual capabilities. It strongly outperforms LLaMA 2 70B on HellaSwag, Arc Challenge and MMLU benchmarks in French, German, Spanish and Italian. Figure 3: Comparison of Mistral open source models and LLaMA 2 70B on HellaSwag, Arc Challenge and MMLU in French, German, Spanish and Italian. Maths &amp; Coding Mixtral 8x22B performs best in coding and maths tasks compared to the other open models. Figure 4: Performance on popular coding and maths benchmarks of the leading open models: HumanEval pass@1, MBPP pass@1, GSM8K maj@1 (5 shot), GSM8K maj@8 (8-shot) and Math maj@4. The instructed version of the Mixtral 8x22B released today shows even better math performance, with a score of 90.8% on GSM8K maj@8 and a Math maj@4 score of 44.6%. Explore Mixtral 8x22B now on la Plateforme and join the Mistral community of developers as we define the AI frontier together. Share More resources News Models AI Services The next chapter of AI is yours. Try le Chat Build on la Plateforme Talk to an expert Mistral AI © 2025 Why Mistral About us Our customers Careers Contact us Explore AI solutions Partners Research Documentation Build La Plateforme Le Chat Mistral Code Mistral Compute Try the API Legal Terms of service Privacy policy Privacy choices Data processing agreement Legal notice Brand en Mistral AI © 2025 "
    },
    {
      "arxiv_id": "https://mistral.ai/news/codestral",
      "full_text": " Codestral | Mistral AI Products Solutions Research Resources Pricing Company Try the API Talk to sales Codestral Empowering developers and democratising coding with Mistral AI. Research May 29, 2024 Mistral AI team We introduce Codestral, our first-ever code model. Codestral is an open-weight generative AI model explicitly designed for code generation tasks. It helps developers write and interact with code through a shared instruction and completion API endpoint. As it masters code and English, it can be used to design advanced AI applications for software developers. A model fluent in 80+ programming languages Codestral is trained on a diverse dataset of 80+ programming languages, including the most popular ones, such as Python, Java, C, C++, JavaScript, and Bash. It also performs well on more specific ones like Swift and Fortran. This broad language base ensures Codestral can assist developers in various coding environments and projects. Codestral saves developers time and effort: it can complete coding functions, write tests, and complete any partial code using a fill-in-the-middle mechanism. Interacting with Codestral will help level up the developer&#x27;s coding game and reduce the risk of errors and bugs. Setting the Bar for Code Generation Performance Performance. As a 22B model, Codestral sets a new standard on the performance/latency space for code generation compared to previous models used for coding. Figure 1: With its larger context window of 32k (compared to 4k, 8k or 16k for competitors), Codestral outperforms all other models in RepoBench, a long-range eval for code generation.. We compare Codestral to existing code-specific models with higher hardware requirements. Python. We use four benchmarks: HumanEval pass@1, MBPP sanitised pass@1 to evaluate Codestral&#x27;s Python code generation ability, CruxEval to evaluate Python output prediction, and RepoBench EM to evaluate Codestral&#x27;s Long-Range Repository-Level Code Completion. SQL. To evaluate Codestral&#x27;s performance in SQL, we used the Spider benchmark. Additional languages. Additionally, we evaluated Codestral&#x27;s performance in multiple HumanEval pass@1 across six different languages in addition to Python: C++, bash, Java, PHP, Typescript, and C#, and calculated the average of these evaluations. FIM benchmarks. Codestral&#x27;s Fill-in-the-middle performance was assessed using HumanEval pass@1 in Python, JavaScript, and Java and compared to DeepSeek Coder 33B, whose fill-in-the-middle capacity is immediately usable. Get started with Codestral Download and test Codestral. Codestral is a 22B open-weight model licensed under the new Mistral AI Non-Production License , which means that you can use it for research and testing purposes. Codestral can be downloaded on HuggingFace . If you want to use the model in the course of commercial activity, Commercial licenses are also available on demand by reaching out to the team . Use Codestral via its dedicated endpoint With this release, comes the addition of a new endpoint: codestral.mistral.ai . This endpoint should be preferred by users who use our Instruct or Fill-In-the-Middle routes inside their IDE. The API Key for this endpoint is managed at the personal level and isn&#x27;t bound by the usual organization rate limits. We&#x27;re allowing use of this endpoint for free during a beta period of 8 weeks and are gating it behind a waitlist to ensure a good quality of service. This endpoint should be preferred by developers implementing IDE plugins or applications where customers are expected to bring their own API keys. Build with Codestral on la Plateforme Codestral is also immediately available on the usual API endpoint: api.mistral.ai where queries are billed per tokens. This endpoint and integrations are better suited for research, batch queries or third-party application development that exposes results directly to users without them bringing their own API keys. You can create your account on la Plateforme and start building your applications with Codestral by following this guide . Like all our other models, Codestral is available in our self-deployment offering starting today: contact sales . Talk to Codestral on le Chat We&#x27;re exposing an instructed version of Codestral, which is accessible today through Le Chat , our free conversational interface. Developers can interact with Codestral naturally and intuitively to leverage the model&#x27;s capabilities. We see Codestral as a new stepping stone towards empowering everyone with code generation and understanding. Use Codestral in your favourite coding and building environment. We worked with community partners to expose Codestral to popular tools for developer productivity and AI application-making. Application frameworks. Codestral is integrated into LlamaIndex and LangChain starting today, which allows users to build agentic applications with Codestral easily VSCode/JetBrains integration. Continue.dev and Tabnine are empowering developers to use Codestral within the VSCode and JetBrains environments and now enable them to generate and chat with the code using Codestral. Here is how you can use the Continue.dev VSCode plugin for code generation, interactive conversation, and inline editing with Codestral, and here is how users can use the Tabnine VSCode plugin to chat with Codestral. For detailed information on how various integrations work with Codestral, please check our documentation for set-up instructions and examples. Developer community feedbacks &quot;A public autocomplete model with this combination of speed and quality hadn&#x27;t existed before, and it&#x27;s going to be a phase shift for developers everywhere.&quot; -- Nate Sesti, CTO and co-founder of Continue.dev &quot;We are excited about the capabilities that Mistral unveils and delighted to see a strong focus on code and development assistance, an area that JetBrains cares deeply about.&quot; -- Vladislav Tankov, Head of JetBrains AI &quot;We used Codestral to run a test on our Kotlin-HumanEval benchmark and were impressed with the results. For instance, in the case of the pass rate for T=0.2, Codestral achieved a score of 73.75, surpassing GPT-4-Turbo&#x27;s score of 72.05 and GPT-3.5-Turbo&#x27;s score of 54.66.&quot; -- Mikhail Evtikhiev, Researcher at JetBrains &quot;As a researcher at the company that created the first developer focused GenAI tool, I&#x27;ve had the pleasure of integrating Mistal&#x27;s new code model into our chat product. I am thoroughly impressed by its performance. Despite its relatively compact size, it delivers results on par with much larger models we offer to customers. We tested several key features, including code generation, test generation, documentation, onboarding processes, and more. In each case, the model exceeded our expectations. The speed and accuracy of the model will significantly impact our product&#x27;s efficiency vs the previous Mistral model, allowing us to provide quick and precise assistance to our users. This model stands out as a powerful tool among the models we support, and I highly recommend it to others seeking high-quality performance.&quot; -- Meital Zilberstein, R&amp;D Lead @ Tabnine &quot;Cody speeds up the inner loop of software development, and developers use features like autocomplete to alleviate some of the day-to-day toil that comes with writing code. Our internal evaluations show that Mistral&#x27;s new Codestral model significantly reduces the latency of Cody autocomplete while maintaining the quality of the suggested code. This makes it an excellent model choice for autocomplete where milliseconds of latency translate to real value for developers.&quot; -- Quinn Slack, CEO and co-founder of Sourcegraph &quot;I&#x27;ve been incredibly impressed with Mistral&#x27;s new Codestral model for AI code generation. In my testing so far, it has consistently produced highly accurate and functional code, even for complex tasks. For example, when I asked it to complete a nontrivial function to create a new LlamaIndex query engine, it generated code that worked seamlessly, despite being based on an older codebase.&quot; -- Jerry Liu, CEO and co-founder of LlamaIndex &quot;Code generation is one of the most popular LLM use-cases, so we are really excited about the Codestral release. From our initial testing, it&#x27;s a great option for code generation workflows because it&#x27;s fast, has favorable context window, and the instruct version supports tool use. We tested with LangGraph for self-corrective code generation using the instruct Codestral tool use for output, and it worked really well out-of-the-box (see our video detailing this ).&quot; -- Harrison Chase, CEO and co-founder of LangChain Share More resources News Models AI Services The next chapter of AI is yours. Try le Chat Build on la Plateforme Talk to an expert Mistral AI © 2025 Why Mistral About us Our customers Careers Contact us Explore AI solutions Partners Research Documentation Build La Plateforme Le Chat Mistral Code Mistral Compute Try the API Legal Terms of service Privacy policy Privacy choices Data processing agreement Legal notice Brand en Mistral AI © 2025 "
    },
    {
      "arxiv_id": "https://mistral.ai/news/codestral/",
      "full_text": " Codestral | Mistral AI Products Solutions Research Resources Pricing Company Try the API Talk to sales Codestral Empowering developers and democratising coding with Mistral AI. Research May 29, 2024 Mistral AI team We introduce Codestral, our first-ever code model. Codestral is an open-weight generative AI model explicitly designed for code generation tasks. It helps developers write and interact with code through a shared instruction and completion API endpoint. As it masters code and English, it can be used to design advanced AI applications for software developers. A model fluent in 80+ programming languages Codestral is trained on a diverse dataset of 80+ programming languages, including the most popular ones, such as Python, Java, C, C++, JavaScript, and Bash. It also performs well on more specific ones like Swift and Fortran. This broad language base ensures Codestral can assist developers in various coding environments and projects. Codestral saves developers time and effort: it can complete coding functions, write tests, and complete any partial code using a fill-in-the-middle mechanism. Interacting with Codestral will help level up the developer&#x27;s coding game and reduce the risk of errors and bugs. Setting the Bar for Code Generation Performance Performance. As a 22B model, Codestral sets a new standard on the performance/latency space for code generation compared to previous models used for coding. Figure 1: With its larger context window of 32k (compared to 4k, 8k or 16k for competitors), Codestral outperforms all other models in RepoBench, a long-range eval for code generation.. We compare Codestral to existing code-specific models with higher hardware requirements. Python. We use four benchmarks: HumanEval pass@1, MBPP sanitised pass@1 to evaluate Codestral&#x27;s Python code generation ability, CruxEval to evaluate Python output prediction, and RepoBench EM to evaluate Codestral&#x27;s Long-Range Repository-Level Code Completion. SQL. To evaluate Codestral&#x27;s performance in SQL, we used the Spider benchmark. Additional languages. Additionally, we evaluated Codestral&#x27;s performance in multiple HumanEval pass@1 across six different languages in addition to Python: C++, bash, Java, PHP, Typescript, and C#, and calculated the average of these evaluations. FIM benchmarks. Codestral&#x27;s Fill-in-the-middle performance was assessed using HumanEval pass@1 in Python, JavaScript, and Java and compared to DeepSeek Coder 33B, whose fill-in-the-middle capacity is immediately usable. Get started with Codestral Download and test Codestral. Codestral is a 22B open-weight model licensed under the new Mistral AI Non-Production License , which means that you can use it for research and testing purposes. Codestral can be downloaded on HuggingFace . If you want to use the model in the course of commercial activity, Commercial licenses are also available on demand by reaching out to the team . Use Codestral via its dedicated endpoint With this release, comes the addition of a new endpoint: codestral.mistral.ai . This endpoint should be preferred by users who use our Instruct or Fill-In-the-Middle routes inside their IDE. The API Key for this endpoint is managed at the personal level and isn&#x27;t bound by the usual organization rate limits. We&#x27;re allowing use of this endpoint for free during a beta period of 8 weeks and are gating it behind a waitlist to ensure a good quality of service. This endpoint should be preferred by developers implementing IDE plugins or applications where customers are expected to bring their own API keys. Build with Codestral on la Plateforme Codestral is also immediately available on the usual API endpoint: api.mistral.ai where queries are billed per tokens. This endpoint and integrations are better suited for research, batch queries or third-party application development that exposes results directly to users without them bringing their own API keys. You can create your account on la Plateforme and start building your applications with Codestral by following this guide . Like all our other models, Codestral is available in our self-deployment offering starting today: contact sales . Talk to Codestral on le Chat We&#x27;re exposing an instructed version of Codestral, which is accessible today through Le Chat , our free conversational interface. Developers can interact with Codestral naturally and intuitively to leverage the model&#x27;s capabilities. We see Codestral as a new stepping stone towards empowering everyone with code generation and understanding. Use Codestral in your favourite coding and building environment. We worked with community partners to expose Codestral to popular tools for developer productivity and AI application-making. Application frameworks. Codestral is integrated into LlamaIndex and LangChain starting today, which allows users to build agentic applications with Codestral easily VSCode/JetBrains integration. Continue.dev and Tabnine are empowering developers to use Codestral within the VSCode and JetBrains environments and now enable them to generate and chat with the code using Codestral. Here is how you can use the Continue.dev VSCode plugin for code generation, interactive conversation, and inline editing with Codestral, and here is how users can use the Tabnine VSCode plugin to chat with Codestral. For detailed information on how various integrations work with Codestral, please check our documentation for set-up instructions and examples. Developer community feedbacks &quot;A public autocomplete model with this combination of speed and quality hadn&#x27;t existed before, and it&#x27;s going to be a phase shift for developers everywhere.&quot; -- Nate Sesti, CTO and co-founder of Continue.dev &quot;We are excited about the capabilities that Mistral unveils and delighted to see a strong focus on code and development assistance, an area that JetBrains cares deeply about.&quot; -- Vladislav Tankov, Head of JetBrains AI &quot;We used Codestral to run a test on our Kotlin-HumanEval benchmark and were impressed with the results. For instance, in the case of the pass rate for T=0.2, Codestral achieved a score of 73.75, surpassing GPT-4-Turbo&#x27;s score of 72.05 and GPT-3.5-Turbo&#x27;s score of 54.66.&quot; -- Mikhail Evtikhiev, Researcher at JetBrains &quot;As a researcher at the company that created the first developer focused GenAI tool, I&#x27;ve had the pleasure of integrating Mistal&#x27;s new code model into our chat product. I am thoroughly impressed by its performance. Despite its relatively compact size, it delivers results on par with much larger models we offer to customers. We tested several key features, including code generation, test generation, documentation, onboarding processes, and more. In each case, the model exceeded our expectations. The speed and accuracy of the model will significantly impact our product&#x27;s efficiency vs the previous Mistral model, allowing us to provide quick and precise assistance to our users. This model stands out as a powerful tool among the models we support, and I highly recommend it to others seeking high-quality performance.&quot; -- Meital Zilberstein, R&amp;D Lead @ Tabnine &quot;Cody speeds up the inner loop of software development, and developers use features like autocomplete to alleviate some of the day-to-day toil that comes with writing code. Our internal evaluations show that Mistral&#x27;s new Codestral model significantly reduces the latency of Cody autocomplete while maintaining the quality of the suggested code. This makes it an excellent model choice for autocomplete where milliseconds of latency translate to real value for developers.&quot; -- Quinn Slack, CEO and co-founder of Sourcegraph &quot;I&#x27;ve been incredibly impressed with Mistral&#x27;s new Codestral model for AI code generation. In my testing so far, it has consistently produced highly accurate and functional code, even for complex tasks. For example, when I asked it to complete a nontrivial function to create a new LlamaIndex query engine, it generated code that worked seamlessly, despite being based on an older codebase.&quot; -- Jerry Liu, CEO and co-founder of LlamaIndex &quot;Code generation is one of the most popular LLM use-cases, so we are really excited about the Codestral release. From our initial testing, it&#x27;s a great option for code generation workflows because it&#x27;s fast, has favorable context window, and the instruct version supports tool use. We tested with LangGraph for self-corrective code generation using the instruct Codestral tool use for output, and it worked really well out-of-the-box (see our video detailing this ).&quot; -- Harrison Chase, CEO and co-founder of LangChain Share More resources News Models AI Services The next chapter of AI is yours. Try le Chat Build on la Plateforme Talk to an expert Mistral AI © 2025 Why Mistral About us Our customers Careers Contact us Explore AI solutions Partners Research Documentation Build La Plateforme Le Chat Mistral Code Mistral Compute Try the API Legal Terms of service Privacy policy Privacy choices Data processing agreement Legal notice Brand en Mistral AI © 2025 "
    },
    {
      "arxiv_id": "https://mistral.ai/news/codestral-mamba/",
      "full_text": " Codestral Mamba | Mistral AI Products Solutions Research Resources Pricing Company Try the API Talk to sales Codestral Mamba As a tribute to Cleopatra, whose glorious destiny ended in tragic snake circumstances, we are proud to release Codestral Mamba, a Mamba2 language model specialised in code generation, available under an Apache 2.0 license. Research Jul 16, 2024 Mistral AI team Following the publishing of the Mixtral family, Codestral Mamba is another step in our effort to study and provide new architectures. It is available for free use, modification, and distribution, and we hope it will open new perspectives in architecture research. Codestral Mamba was designed with help from Albert Gu and Tri Dao. Unlike Transformer models, Mamba models offer the advantage of linear time inference and the theoretical ability to model sequences of infinite length. It allows users to engage with the model extensively with quick responses, irrespective of the input length. This efficiency is especially relevant for code productivity use cases—this is why we trained this model with advanced code and reasoning capabilities, enabling it to perform on par with SOTA transformer-based models. We have tested Codestral Mamba on in-context retrieval capabilities up to 256k tokens. We expect it to be a great local code assistant! You can deploy Codestral Mamba using the mistral-inference SDK, which relies on the reference implementations from Mamba&#x27;s GitHub repository. The model can also be deployed through TensorRT-LLM . For local inference, keep an eye out for support in llama.cpp. You may download the raw weights from HuggingFace . This is an instructed model, with 7,285,403,648 parameters. For easy testing, we made Codestral Mamba available on la Plateforme ( codestral-mamba-2407 ), alongside its big sister, Codestral 22B. While Codestral Mamba is available under the Apache 2.0 license, Codestral 22B is available under a commercial license for self-deployment or a community license for testing purposes. Share More resources News Models AI Services The next chapter of AI is yours. Try le Chat Build on la Plateforme Talk to an expert Mistral AI © 2025 Why Mistral About us Our customers Careers Contact us Explore AI solutions Partners Research Documentation Build La Plateforme Le Chat Mistral Code Mistral Compute Try the API Legal Terms of service Privacy policy Privacy choices Data processing agreement Legal notice Brand en Mistral AI © 2025 "
    },
    {
      "arxiv_id": "https://mistral.ai/news/mathstral/",
      "full_text": " MathΣtral | Mistral AI Products Solutions Research Resources Pricing Company Try the API Talk to sales MathΣtral As a tribute to Archimedes, whose 2311th anniversary we&#x27;re celebrating this year, we are proud to release our first Mathstral model, a specific 7B model designed for math reasoning and scientific discovery. The model has a 32k context window published under the Apache 2.0 license. Research Jul 16, 2024 Mistral AI team We&#x27;re contributing Mathstral to the science community to bolster efforts in advanced mathematical problems requiring complex, multi-step logical reasoning. The Mathstral release is part of our broader effort to support academic projects—it was produced in the context of our collaboration with Project Numina . Akin to Isaac Newton in his time, Mathstral stands on the shoulders of Mistral 7B and specializes in STEM subjects. It achieves state-of-the-art reasoning capacities in its size category across various industry-standard benchmarks. In particular, it achieves 56.6% on MATH and 63.47% on MMLU, with the following MMLU performance difference by subject between Mathstral 7B and Mistral 7B. Mathstral is another example of the excellent performance/speed tradeoffs achieved when building models for specific purposes – a development philosophy we actively promote in la Plateforme, particularly with its new fine-tuning capabilities . Mathstral can achieve significantly better results with more inference-time computation: Mathstral 7B scores 68.37% on MATH with majority voting and 74.59% with a strong reward model among 64 candidates. Mathstral is an instructed model – use it or fine-tune it as such, referring to our documentation. Weights are hosted on HuggingFace . You can try Mathstral now with mistral-inference and adapt it with mistral-finetune . We thank Professor Paul Bourdon for curating the GRE Math Subject Test problems used in our evaluation. Share More resources News Models AI Services The next chapter of AI is yours. Try le Chat Build on la Plateforme Talk to an expert Mistral AI © 2025 Why Mistral About us Our customers Careers Contact us Explore AI solutions Partners Research Documentation Build La Plateforme Le Chat Mistral Code Mistral Compute Try the API Legal Terms of service Privacy policy Privacy choices Data processing agreement Legal notice Brand en Mistral AI © 2025 "
    },
    {
      "arxiv_id": "https://mistral.ai/news/mistral-nemo/",
      "full_text": " Mistral NeMo | Mistral AI Products Solutions Research Resources Pricing Company Try the API Talk to sales Mistral NeMo Mistral NeMo: our new best small model. A state-of-the-art 12B model with 128k context length, built in collaboration with NVIDIA, and released under the Apache 2.0 license. Research Jul 18, 2024 Mistral AI team Today, we are excited to release Mistral NeMo, a 12B model built in collaboration with NVIDIA. Mistral NeMo offers a large context window of up to 128k tokens. Its reasoning, world knowledge, and coding accuracy are state-of-the-art in its size category. As it relies on standard architecture, Mistral NeMo is easy to use and a drop-in replacement in any system using Mistral 7B. We have released pre-trained base and instruction-tuned checkpoints checkpoints under the Apache 2.0 license to promote adoption for researchers and enterprises. Mistral NeMo was trained with quantisation awareness, enabling FP8 inference without any performance loss. The following table compares the accuracy of the Mistral NeMo base model with two recent open-source pre-trained models, Gemma 2 9B, and Llama 3 8B. Table 1: Mistral NeMo base model performance compared to Gemma 2 9B and Llama 3 8B. Multilingual Model for the Masses The model is designed for global, multilingual applications. It is trained on function calling, has a large context window, and is particularly strong in English, French, German, Spanish, Italian, Portuguese, Chinese, Japanese, Korean, Arabic, and Hindi. This is a new step toward bringing frontier AI models to everyone’s hands in all languages that form human culture. Figure 1: Mistral NeMo performance on multilingual benchmarks. Tekken, a more efficient tokenizer Mistral NeMo uses a new tokenizer, Tekken, based on Tiktoken, that was trained on over more than 100 languages, and compresses natural language text and source code more efficiently than the SentencePiece tokenizer used in previous Mistral models. In particular, it is ~30% more efficient at compressing source code, Chinese, Italian, French, German, Spanish, and Russian. It is also 2x and 3x more efficient at compressing Korean and Arabic, respectively. Compared to the Llama 3 tokenizer, Tekken proved to be more proficient in compressing text for approximately 85% of all languages. Figure 2: Tekken compression rate. Instruction fine-tuning Mistral NeMO underwent an advanced fine-tuning and alignment phase. Compared to Mistral 7B, it is much better at following precise instructions, reasoning, handling multi-turn conversations, and generating code. Table 2: Mistral NeMo instruction-tuned model accuracy. Evals done with GPT4o as judge on official references. Links Weights are hosted on HuggingFace both for the base and for the instruct models. You can try Mistral NeMo now with mistral-inference and adapt it with mistral-finetune. Mistral NeMo is exposed on la Plateforme under the name open-mistral-nemo-2407 . This model is also packaged in a container as NVIDIA NIM inference microservice and available from ai.nvidia.com . Share More resources News Models AI Services The next chapter of AI is yours. Try le Chat Build on la Plateforme Talk to an expert Mistral AI © 2025 Why Mistral About us Our customers Careers Contact us Explore AI solutions Partners Research Documentation Build La Plateforme Le Chat Mistral Code Mistral Compute Try the API Legal Terms of service Privacy policy Privacy choices Data processing agreement Legal notice Brand en Mistral AI © 2025 "
    },
    {
      "arxiv_id": "https://mistral.ai/news/mistral-large-2407/",
      "full_text": " Large Enough | Mistral AI Products Solutions Research Resources Pricing Company Try the API Talk to sales Large Enough Today, we are announcing Mistral Large 2, the new generation of our flagship model. Compared to its predecessor, Mistral Large 2 is significantly more capable in code generation, mathematics, and reasoning. It also provides a much stronger multilingual support, and advanced function calling capabilities. Research Jul 24, 2024 Mistral AI team This latest generation continues to push the boundaries of cost efficiency, speed, and performance. Mistral Large 2 is exposed on la Plateforme and enriched with new features to facilitate building innovative AI applications. Mistral Large 2 Mistral Large 2 has a 128k context window and supports dozens of languages including French, German, Spanish, Italian, Portuguese, Arabic, Hindi, Russian, Chinese, Japanese, and Korean, along with 80+ coding languages including Python, Java, C, C++, JavaScript, and Bash. Mistral Large 2 is designed for single-node inference with long-context applications in mind – its size of 123 billion parameters allows it to run at large throughput on a single node. We are releasing Mistral Large 2 under the Mistral Research License , that allows usage and modification for research and non-commercial usages. For commercial usage of Mistral Large 2 requiring self-deployment, a Mistral Commercial License must be acquired by contacting us . General performance Mistral Large 2 sets a new frontier in terms of performance / cost of serving on evaluation metrics. In particular, on MMLU, the pretrained version achieves an accuracy of 84.0%, and sets a new point on the performance/cost Pareto front of open models. Code &amp; Reasoning Following our experience with Codestral 22B and Codestral Mamba , we trained Mistral Large 2 on a very large proportion of code. Mistral Large 2 vastly outperforms the previous Mistral Large, and performs on par with leading models such as GPT-4o, Claude 3 Opus, and Llama 3 405B. A significant effort was also devoted to enhancing the model&#x27;s reasoning capabilities. One of the key focus areas during training was to minimize the model&#x27;s tendency to &quot;hallucinate&quot; or generate plausible-sounding but factually incorrect or irrelevant information. This was achieved by fine-tuning the model to be more cautious and discerning in its responses, ensuring that it provides reliable and accurate outputs. Additionally, the new Mistral Large 2 is trained to acknowledge when it cannot find solutions or does not have sufficient information to provide a confident answer. This commitment to accuracy is reflected in the improved model performance on popular mathematical benchmarks, demonstrating its enhanced reasoning and problem-solving skills: Performance accuracy on code generation benchmarks (all models were benchmarked through the same evaluation pipeline) Performance accuracy on MultiPL-E (all models were benchmarked through the same evaluation pipeline, except for the &quot;paper&quot; row) Performance accuracy on GSM8K (8-shot) and MATH (0-shot, no CoT) generation benchmarks (all models were benchmarked through the same evaluation pipeline) Instruction following &amp; Alignment We drastically improved the instruction-following and conversational capabilities of Mistral Large 2. The new Mistral Large 2 is particularly better at following precise instructions and handling long multi-turn conversations. Below we report the performance on MT-Bench, Wild Bench, and Arena Hard benchmarks: Performance on general alignment benchmarks (all models were benchmarked through the same evalutation pipeline) On some benchmarks, generating lengthy responses tends to improve the scores. However, in many business applications, conciseness is paramount – short model generations facilitate quicker interactions and are more cost-effective for inference. This is why we spent a lot of effort to ensure that generations remain succinct and to the point whenever possible. The graph below reports the average length of generations of different models on questions from the MT Bench benchmark: Language diversity A large fraction of business use cases today involve working with multilingual documents. While the majority of models are English-centric, the new Mistral Large 2 was trained on a large proportion of multilingual data. In particular, it excels in English, French, German, Spanish, Italian, Portuguese, Dutch, Russian, Chinese, Japanese, Korean, Arabic, and Hindi. Below are the performance results of Mistral Large 2 on the multilingual MMLU benchmark, compared to the previous Mistral Large, Llama 3.1 models, and to Cohere’s Command R+. Performance on Multilingual MMLU (measured on the base pretrained model) Tool Use &amp; Function Calling Mistral Large 2 is equipped with enhanced function calling and retrieval skills and has undergone training to proficiently execute both parallel and sequential function calls, enabling it to serve as the power engine of complex business applications. Try Mistral Large 2 on la Plateforme You can use Mistral Large 2 today via la Plateforme under the name mistral-large-2407 , and test it on le Chat. It is available under the version 24.07 (a YY.MM versioning system that we are applying to all our models), and the API name mistral-large-2407 . Weights for the instruct model are available and are also hosted on HuggingFace . We are consolidating the offering on la Plateforme around two general purpose models, Mistral Nemo and Mistral Large, and two specialist models, Codestral and Embed. As we progressively deprecate older models on la Plateforme, all Apache models (Mistral 7B, Mixtral 8x7B and 8x22B, Codestral Mamba, Mathstral) remain available for deployment and fine-tuning using our SDK mistral-inference and mistral-finetune. Starting today, we are extending fine-tuning capabilities on la Plateforme: those are now available for Mistral Large, Mistral Nemo and Codestral. Access Mistral models through cloud service providers We are proud to partner with leading cloud service providers to bring the new Mistral Large 2 to a global audience. In particular, today we are expanding our partnership with Google Cloud Platform to bring Mistral AI&#x27;s models on Vertex AI via a Managed API. Mistral AI&#x27;s best models are now available on Vertex AI, in addition to Azure AI Studio, Amazon Bedrock and IBM watsonx.ai . Availability timeline of Mistral AI models Share More resources News Models AI Services The next chapter of AI is yours. Try le Chat Build on la Plateforme Talk to an expert Mistral AI © 2025 Why Mistral About us Our customers Careers Contact us Explore AI solutions Partners Research Documentation Build La Plateforme Le Chat Mistral Code Mistral Compute Try the API Legal Terms of service Privacy policy Privacy choices Data processing agreement Legal notice Brand en Mistral AI © 2025 "
    },
    {
      "arxiv_id": "https://mistral.ai/news/pixtral-12b/",
      "full_text": " Announcing Pixtral 12B | Mistral AI Products Solutions Research Resources Pricing Company Try the API Talk to sales Announcing Pixtral 12B Pixtral 12B - the first-ever multimodal Mistral model. Apache 2.0. Research Sep 17, 2024 Mistral AI team Pixtral 12B in short: Natively multimodal, trained with interleaved image and text data Strong performance on multimodal tasks, excels in instruction following Maintains state-of-the-art performance on text-only benchmarks Architecture: New 400M parameter vision encoder trained from scratch 12B parameter multimodal decoder based on Mistral Nemo Supports variable image sizes and aspect ratios Supports multiple images in the long context window of 128k tokens Use: License: Apache 2.0 Try it on La Plateforme or on Le Chat Pixtral is trained to understand both natural images and documents, achieving 52.5% on the MMMU reasoning benchmark, surpassing a number of larger models. The model shows strong abilities in tasks such as chart and figure understanding, document question answering, multimodal reasoning and instruction following. Pixtral is able to ingest images at their natural resolution and aspect ratio, giving the user flexibility on the number of tokens used to process an image. Pixtral is also able to process any number of images in its long context window of 128K tokens. Unlike previous open-source models, Pixtral does not compromise on text benchmark performance to excel in multimodal tasks. Performance Pixtral was trained to be a drop-in replacement for Mistral Nemo 12B . Its key distinguishing factor from existing open-source models is the delivery of best-in-class multimodal reasoning without compromising on key text capabilities such as instruction following, coding, and math. Evaluation protocol We re-evaluate a range of open and closed models through the same evaluation harness . For each dataset, the prompt was chosen such that we could reproduce the results of leading multimodal models (GPT-4o and Claude-3.5-Sonnet). All models were then evaluated with this same prompt. Overall, Pixtral substantially outperforms all open models around its scale and, in many cases, outperforms closed models such as Claude 3 Haiku. Pixtral even outperforms or matches the performance of much larger models like LLaVa OneVision 72B on multimodal benchmarks.All prompts will be open-sourced. Performance of Pixtral compared to closed and larger multimodal models. [All models were benchmarked through the same evaluation harness and with the same prompt. We verify that prompts reproduce the performance reported for GPT-4o and Claude 3.5 Sonnet (prompts will be provided in technical report)]. Instruction following Pixtral particularly excels at both multimodal and text-only instruction following as compared to other open multimodal models. It substantially outperforms Qwen2-VL 7B, LLaVa-OneVision 7B and Phi-3.5 Vision in instruction following, with a 20% relative improvement in text IF-Eval and MT-Bench over the nearest OSS model. To further evaluate this ability for multimodal use cases, we create multimodal versions of these benchmarks: MM-IF-Eval and MM-MT-Bench . Pixtral outperforms open-source alternatives on multimodal instruction following benchmarks as well. We will open-source MM-MT-Bench to the community. Performance of Pixtral compared to open multimodal models. All models were benchmarked through the same evaluation harness and with the same prompt. Architecture Variable image size: Pixtral is designed to optimize for both speed and performance. We trained a new vision encoder that natively supports variable image sizes: We simply pass images through the vision encoder at their native resolution and aspect ratio, converting them into image tokens for each 16x16 patch in the image These tokens are then flattened to create a sequence, with [IMG BREAK] and [IMG END] tokens added between rows and at the end of the image. [IMG BREAK] tokens let the model distinguish between images of different aspect ratios with the same number of tokens. In this way, Pixtral can be used to accurately understand complex diagrams, charts and documents in high resolution, while providing fast inference speeds on small images like icons, clipart, and equations. Pixtral uses a new vision encoder trained from scratch that natively supports variable image sizes. Final architecture: Pixtral has two components: the Vision Encoder, which tokenizes images, and a Multimodal Transformer Decoder, which predicts the next text token given a sequence of text and images. The model is trained to predict the next text token on interleaved image and text data. This architecture allows Pixtral to process any number of images with arbitrary sizes in its large context window of 128K tokens. Qualitative Examples Reasoning over complex figures This table combines the data from both tables, ensuring that all models and their respective scores from each benchmark are included in a single, cohesive format. Prompt: List the top 5 countries in Europe with the highest GDP Response: The top 5 countries in Europe with the highest GDP, based on the image, are: Germany GDP: $3.99 trillion Percentage: 4.65% United Kingdom GDP: $2.82 trillion Percentage: 3.29% France GDP: $2.78 trillion Percentage: 3.24% Italy GDP: $2.07 trillion Percentage: 2.42% Spain GDP: $1.43 trillion Percentage: 1.66% These countries are highlighted in green on the diagram. Chart Understanding and Analysis Image: Prompt: When did things start to go wrong for dark dragon? Response: The image is a line graph titled &#x27;train/loss&#x27; that shows the training loss over steps for four different models: dark-dragon-50, dainty-sunset-49, different-haze-34, and morning-field-33. The x-axis represents the steps, ranging from 0 to 60k, and the y-axis represents the loss on a logarithmic scale from 1e-5 to 1e+5. To determine when things started to go wrong for the dark-dragon-50 model, we need to look at the red line representing this model. Initially, the loss decreases, indicating that the model is learning effectively. However, around the 10k step mark, the loss starts to spike significantly. This spike indicates that the model&#x27;s performance is deteriorating. The loss continues to spike and fluctuate, reaching very high values, which suggests that the model is not learning effectively and is likely overfitting or encountering other issues. Therefore, the critical point where things started to go wrong for the dark-dragon-50 model is around the 10k step mark. Multi-image instruction following Images: Prompt: Combine information in both the tables into a single markdown table Response: To combine the information from both tables into a single table, we need to align the models and their corresponding scores from each benchmark. Here is the combined table: | Model | MMMU (CoT) | Mathvista (CoT) | ChartQA (CoT) | DocVQA (ANLS) | VQAv2 (VQA Match) | |------------------------|------------|-----------------|---------------|---------------|-------------------| | Pixtral 12B | 52.5 | 58.0 | 81.8 | 90.7 | 78.6 | | Claude-3 Haiku | 50.4 | 44.8 | 69.6 | 74.6 | 68.4 | | Gemini-1.5 Flash 8B (0827) | 50.7 | 56.9 | 78.0 | 79.5 | 65.5 | | LLaVA-OV 72B | 54.4 | 57.2 | 66.9 | 91.6 | 83.8 | | GPT-4o | 68.6 | 64.6 | 85.1 | 88.9 | 77.8 | | Claude-3.5 Sonnet | 68.0 | 64.4 | 87.6 | 90.3 | 70.7 | | Qwen2-VL 7B | 47.6 | 54.4 | 38.6 | 94.5 | 75.9 | | LLaVA-OV 7B | 45.1 | 36.1 | 67.1 | 90.5 | 78.3 | | Phi-3 Vision | 40.3 | 36.4 | 72.0 | 84.9 | 42.4 | | Phi-3.5 Vision | 38.3 | 39.3 | 67.7 | 74.4 | 56.1 | This table combines the data from both tables, ensuring that all models and their respective scores from each benchmark are included in a single, cohesive format. Image to Code Image: Prompt: Write HTML code to create a website like this Response: Certainly! Below is an example of HTML code to create a simple webpage that allows users to pick an ice cream flavor. This example includes a dropdown menu for selecting the flavor and a &quot;Next&quot; button. ```html &lt;!DOCTYPE html&gt; &lt;html lang=&quot;en&quot;&gt; &lt;head&gt; &lt;meta charset=&quot;UTF-8&quot;&gt; &lt;meta name=&quot;viewport&quot; content=&quot;width=device-width, initial-scale=1.0&quot;&gt; &lt;title&gt;Pick an Ice Cream Flavor&lt;/title&gt; &lt;style&gt; body { font-family: Arial, sans-serif; display: flex; justify-content: center; align-items: center; height: 100vh; background-color: #f0f0f0; } .container { background: white; padding: 20px; border-radius: 8px; box-shadow: 0 0 10px rgba(0, 0, 0, 0.1); } h1 { text-align: center; } select { width: 100%; padding: 10px; margin-top: 10px; margin-bottom: 20px; } button { width: 100%; padding: 10px; background-color: #4CAF50; color: white; border: none; border-radius: 4px; cursor: pointer; } button:hover { background-color: #45a049; } &lt;/style&gt; &lt;/head&gt; &lt;body&gt; &lt;div class=&quot;container&quot;&gt; &lt;h1&gt;Pick An Ice Cream Flavor&lt;/h1&gt; &lt;select id=&quot;flavorSelect&quot;&gt; &lt;option value=&quot;vanilla&quot;&gt;Vanilla&lt;/option&gt; &lt;option value=&quot;chocolate&quot;&gt;Chocolate&lt;/option&gt; &lt;option value=&quot;strawberry&quot;&gt;Strawberry&lt;/option&gt; &lt;option value=&quot;cookie_dough&quot;&gt;Cookie Dough&lt;/option&gt; &lt;option value=&quot;mint_chocolate&quot;&gt;Mint Chocolate&lt;/option&gt; &lt;/select&gt; &lt;button onclick=&quot;next()&quot;&gt;Next&lt;/button&gt; &lt;/div&gt; &lt;footer style=&quot;position: fixed; bottom: 0; width: 100%; text-align: center; font-size: 12px;&quot;&gt; &amp;copy; copyright MistralAI &lt;/footer&gt; &lt;script&gt; function next() { const selectedFlavor = document.getElementById(&#x27;flavorSelect&#x27;).value; alert(&#x27;You selected: &#x27; + selectedFlavor); } &lt;/script&gt; &lt;/body&gt; &lt;/html&gt; Explanation: HTML Structure: The webpage consists of a container div that holds the heading, a dropdown menu, and a button. CSS Styling: Basic styling is applied to center the content and style the elements. JavaScript Function: The `next` function is called when the &quot;Next&quot; button is clicked. It retrieves the selected flavor from the dropdown menu and displays an alert with the chosen flavor. You can copy and paste this code into an HTML file and open it in a web browser to see the result. Website Generated: Natural scene understanding Prompt: Is this person really big, or is this building just super small? Response: The image shows a person holding their thumb in front of the Leaning Tower of Pisa, making it appear as if the thumb is much larger than the tower. This creates an optical illusion, making the tower seem smaller than it actually is. In reality, the Leaning Tower of Pisa is a large and iconic structure, and the person&#x27;s thumb is not nearly as large as it appears in the image. The purpose of this photo is to demonstrate the impressive height and size of the tower by comparing it to a familiar object, such as a thumb. How to run Pixtral? Le Chat You can try Pixtral easily and freely via Le Chat , our user-friendly conversational chat interface. You can choose Pixtral in the model list, upload an image, and start asking questions about the image. La Plateforme Pixtral is also available on La Plateforme. You can leverage Pixtral&#x27;s capabilities through API calls, enabling seamless integration with various applications and workflows. Below is a simple example. Please find more details in our docs . curl https://api.mistral.ai/v1/chat/completions \\ -H &quot;Content-Type: application/json&quot; \\ -H &quot;Authorization: Bearer $MISTRAL_API_KEY&quot; \\ -d &#x27;{ &quot;model&quot;: &quot;pixtral-12b-2409&quot;, &quot;messages&quot;: [ { &quot;role&quot;: &quot;user&quot;, &quot;content&quot;: [ { &quot;type&quot;: &quot;text&quot;, &quot;text&quot;: &quot;What’s in this image?&quot; }, { &quot;type&quot;: &quot;image_url&quot;, &quot;image_url&quot;: &quot;https://tripfixers.com/wp-content/uploads/2019/11/eiffel-tower-with-snow.jpeg&quot; } ] } ], &quot;max_tokens&quot;: 300 }&#x27; mistral-inference The easiest way to run Pixtral locally is to use mistral-inference . After installing mistral_inference , you can download the model, load the model, and run the model using the code below. For detailed information, please see here . # download the model from huggingface_hub import snapshot_download from pathlib import Path mistral_models_path = Path.home().joinpath(&#x27;mistral_models&#x27;, &#x27;Pixtral&#x27;) mistral_models_path.mkdir(parents=True, exist_ok=True) snapshot_download(repo_id=&quot;mistralai/Pixtral-12B-2409&quot;, allow_patterns=[&quot;params.json&quot;, &quot;consolidated.safetensors&quot;, &quot;tekken.json&quot;], local_dir=mistral_models_path) # load the model from mistral_inference.transformer import Transformer from mistral_inference.generate import generate from mistral_common.tokens.tokenizers.mistral import MistralTokenizer from mistral_common.protocol.instruct.messages import UserMessage, TextChunk, ImageURLChunk from mistral_common.protocol.instruct.request import ChatCompletionRequest tokenizer = MistralTokenizer.from_file(f&quot;{mistral_models_path}/tekken.json&quot;) model = Transformer.from_folder(mistral_models_path) # Run the model url = &quot;https://huggingface.co/datasets/patrickvonplaten/random_img/resolve/main/yosemite.png&quot; prompt = &quot;Describe the image.&quot; completion_request = ChatCompletionRequest(messages=[UserMessage(content=[ImageURLChunk(image_url=url), TextChunk(text=prompt)])]) encoded = tokenizer.encode_chat_completion(completion_request) images = encoded.images tokens = encoded.tokens out_tokens, _ = generate([tokens], model, images=[images], max_tokens=256, temperature=0.35, eos_id=tokenizer.instruct_tokenizer.tokenizer.eos_id) result = tokenizer.decode(out_tokens[0]) print(result) vLLM If you choose to serve Pixtral locally, we also recommend using Pixtral with the vLLM library as a fantastic option to reach higher serving throughput. We thank the vLLM team for their support to integrate Pixtral quickly. Below is a simple usage example. Please find more information here . from vllm import LLM from vllm.sampling_params import SamplingParams model_name = &quot;mistralai/Pixtral-12B-2409&quot; sampling_params = SamplingParams(max_tokens=8192) llm = LLM(model=model_name, tokenizer_mode=&quot;mistral&quot;) prompt = &quot;Describe this image in one sentence.&quot; image_url = &quot;https://picsum.photos/id/237/200/300&quot; messages = [ { &quot;role&quot;: &quot;user&quot;, &quot;content&quot;: [{&quot;type&quot;: &quot;text&quot;, &quot;text&quot;: prompt}, {&quot;type&quot;: &quot;image_url&quot;, &quot;image_url&quot;: {&quot;url&quot;: image_url}}] }, ] outputs = vllm_model.model.chat(messages, sampling_params=sampling_params) print(outputs[0].outputs[0].text) Share More resources News Models AI Services The next chapter of AI is yours. Try le Chat Build on la Plateforme Talk to an expert Mistral AI © 2025 Why Mistral About us Our customers Careers Contact us Explore AI solutions Partners Research Documentation Build La Plateforme Le Chat Mistral Code Mistral Compute Try the API Legal Terms of service Privacy policy Privacy choices Data processing agreement Legal notice Brand en Mistral AI © 2025 "
    },
    {
      "arxiv_id": "https://mistral.ai/news/mistral-small-3-1/",
      "full_text": " Mistral Small 3.1 | Mistral AI Products Solutions Research Resources Pricing Company Try the API Talk to sales Mistral Small 3.1 SOTA. Multimodal. Multilingual. Apache 2.0 Research Mar 17, 2025 Mistral AI Today we announce Mistral Small 3.1: the best model in its weight class. Building on Mistral Small 3 , this new model comes with improved text performance, multimodal understanding, and an expanded context window of up to 128k tokens. The model outperforms comparable models like Gemma 3 and GPT-4o Mini, while delivering inference speeds of 150 tokens per second. Mistral Small 3.1 is released under an Apache 2.0 license. Modern AI applications demand a blend of capabilities—handling text, understanding multimodal inputs, supporting multiple languages, and managing long contexts—with low latency and cost efficiency. As shown below, Mistral Small 3.1 is the first open source model that not only meets, but in fact surpasses, the performance of leading small proprietary models across all these dimensions. Below you will find more details on model performance. Whenever possible, we show numbers reported previously by other providers, otherwise we evaluate models through our common evaluation harness. Instruct Performance Text instruct benchmarks Multimodal Instruct Benchmarks MM-MT-Bench scaled to between 0 and 100. Multilingual Long Context Pretrained Performance We also release the pretrained base model for Mistral Small 3.1. All pretrain Use cases Mistral Small 3.1 is a versatile model designed to handle a wide range of generative AI tasks, including instruction following, conversational assistance, image understanding, and function calling. It provides a solid foundation for both enterprise and consumer-grade AI applications. Key Features and Capabilities Lightweight: Mistral Small 3.1 can run on a single RTX 4090 or a Mac with 32GB RAM. This makes it a great fit for on-device use cases. Fast-response conversational assistance: Ideal for virtual assistants and other applications where quick, accurate responses are essential. Low-latency function calling: Capable of rapid function execution within automated or agentic workflows Fine-tuning for specialized domains: Mistral Small 3.1 can be fine-tuned to specialize in specific domains, creating accurate subject matter experts. This is particularly useful in fields like legal advice, medical diagnostics, and technical support. Foundation for advanced reasoning: We continue to be impressed by how the community builds on top of open Mistral models. Just in the last few weeks, we have seen several excellent reasoning models built on Mistral Small 3, such as the DeepHermes 24B by Nous Research. To that end, we are releasing both base and instruct checkpoints for Mistral Small 3.1 to enable further downstream customization of the model. Mistral Small 3.1 can be used across various enterprise and consumer applications that require multimodal understanding, such as document verification, diagnostics, on-device image processing, visual inspection for quality checks, object detection in security systems, image-based customer support, and general purpose assistance. Availability Mistral Small 3.1 is available to download on the huggingface website Mistral Small 3.1 Base and Mistral Small 3.1 Instruct . For enterprise deployments with private and optimized inference infrastructure, please contact us . You can also try the model via API on Mistral AI’s developer playground La Plateforme starting today. The model is also available on Google Cloud Vertex AI . Mistral Small 3.1 will be available on NVIDIA NIM and Microsoft Azure AI Foundry in the coming weeks. Happy building! Share More resources News Models AI Services The next chapter of AI is yours. Try le Chat Build on la Plateforme Talk to an expert Mistral AI © 2025 Why Mistral About us Our customers Careers Contact us Explore AI solutions Partners Research Documentation Build La Plateforme Le Chat Mistral Code Mistral Compute Try the API Legal Terms of service Privacy policy Privacy choices Data processing agreement Legal notice Brand en Mistral AI © 2025 "
    },
    {
      "arxiv_id": "https://arxiv.org/abs/2106.09685",
      "full_text": " [2106.09685] LoRA: Low-Rank Adaptation of Large Language Models Skip to main content We gratefully acknowledge support from the Simons Foundation, member institutions , and all contributors. Donate &gt; cs &gt; arXiv:2106.09685 Help | Advanced Search All fields Title Author Abstract Comments Journal reference ACM classification MSC classification Report number arXiv identifier DOI ORCID arXiv author ID Help pages Full text Search open search GO open navigation menu quick links Login Help Pages About --> Computer Science > Computation and Language arXiv:2106.09685 (cs) [Submitted on 17 Jun 2021 ( v1 ), last revised 16 Oct 2021 (this version, v2)] Title: LoRA: Low-Rank Adaptation of Large Language Models Authors: Edward J. Hu , Yelong Shen , Phillip Wallis , Zeyuan Allen-Zhu , Yuanzhi Li , Shean Wang , Lu Wang , Weizhu Chen View a PDF of the paper titled LoRA: Low-Rank Adaptation of Large Language Models, by Edward J. Hu and 7 other authors View PDF Abstract: An important paradigm of natural language processing consists of large-scale pre-training on general domain data and adaptation to particular tasks or domains. As we pre-train larger models, full fine-tuning, which retrains all model parameters, becomes less feasible. Using GPT-3 175B as an example -- deploying independent instances of fine-tuned models, each with 175B parameters, is prohibitively expensive. We propose Low-Rank Adaptation, or LoRA, which freezes the pre-trained model weights and injects trainable rank decomposition matrices into each layer of the Transformer architecture, greatly reducing the number of trainable parameters for downstream tasks. Compared to GPT-3 175B fine-tuned with Adam, LoRA can reduce the number of trainable parameters by 10,000 times and the GPU memory requirement by 3 times. LoRA performs on-par or better than fine-tuning in model quality on RoBERTa, DeBERTa, GPT-2, and GPT-3, despite having fewer trainable parameters, a higher training throughput, and, unlike adapters, no additional inference latency. We also provide an empirical investigation into rank-deficiency in language model adaptation, which sheds light on the efficacy of LoRA. We release a package that facilitates the integration of LoRA with PyTorch models and provide our implementations and model checkpoints for RoBERTa, DeBERTa, and GPT-2 at this https URL . Comments: Draft V2 includes better baselines, experiments on GLUE, and more on adapter latency Subjects: Computation and Language (cs.CL) ; Artificial Intelligence (cs.AI); Machine Learning (cs.LG) Cite as: arXiv:2106.09685 [cs.CL] &nbsp; (or arXiv:2106.09685v2 [cs.CL] for this version) &nbsp; https://doi.org/10.48550/arXiv.2106.09685 Focus to learn more arXiv-issued DOI via DataCite Submission history From: Edward J. Hu [ view email ] [v1] Thu, 17 Jun 2021 17:37:18 UTC (1,791 KB) [v2] Sat, 16 Oct 2021 18:40:34 UTC (896 KB) Full-text links: Access Paper: View a PDF of the paper titled LoRA: Low-Rank Adaptation of Large Language Models, by Edward J. Hu and 7 other authors View PDF TeX Source Other Formats view license Current browse context: cs.CL &lt;&nbsp;prev &nbsp; | &nbsp; next&nbsp;&gt; new | recent | 2021-06 Change to browse by: cs cs.AI cs.LG References &amp; Citations NASA ADS Google Scholar Semantic Scholar 12 blog links ( what is this? ) DBLP - CS Bibliography listing | bibtex Yelong Shen Phillip Wallis Zeyuan Allen-Zhu Yuanzhi Li Weizhu Chen a export BibTeX citation Loading... BibTeX formatted citation &times; loading... Data provided by: Bookmark Bibliographic Tools Bibliographic and Citation Tools Bibliographic Explorer Toggle Bibliographic Explorer ( What is the Explorer? ) Connected Papers Toggle Connected Papers ( What is Connected Papers? ) Litmaps Toggle Litmaps ( What is Litmaps? ) scite.ai Toggle scite Smart Citations ( What are Smart Citations? ) Code, Data, Media Code, Data and Media Associated with this Article alphaXiv Toggle alphaXiv ( What is alphaXiv? ) Links to Code Toggle CatalyzeX Code Finder for Papers ( What is CatalyzeX? ) DagsHub Toggle DagsHub ( What is DagsHub? ) GotitPub Toggle Gotit.pub ( What is GotitPub? ) Huggingface Toggle Hugging Face ( What is Huggingface? ) Links to Code Toggle Papers with Code ( What is Papers with Code? ) ScienceCast Toggle ScienceCast ( What is ScienceCast? ) Demos Demos Replicate Toggle Replicate ( What is Replicate? ) Spaces Toggle Hugging Face Spaces ( What is Spaces? ) Spaces Toggle TXYZ.AI ( What is TXYZ.AI? ) Related Papers Recommenders and Search Tools Link to Influence Flower Influence Flower ( What are Influence Flowers? ) Core recommender toggle CORE Recommender ( What is CORE? ) Author Venue Institution Topic About arXivLabs arXivLabs: experimental projects with community collaborators arXivLabs is a framework that allows collaborators to develop and share new arXiv features directly on our website. Both individuals and organizations that work with arXivLabs have embraced and accepted our values of openness, community, excellence, and user data privacy. arXiv is committed to these values and only works with partners that adhere to them. Have an idea for a project that will add value for arXiv's community? Learn more about arXivLabs . Which authors of this paper are endorsers? | Disable MathJax ( What is MathJax? ) About Help contact arXiv Click here to contact arXiv Contact subscribe to arXiv mailings Click here to subscribe Subscribe Copyright Privacy Policy Web Accessibility Assistance arXiv Operational Status Get status notifications via email or slack "
    }
  ]
}