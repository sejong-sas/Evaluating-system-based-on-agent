{
  "model_id": "lgai-exaone/exaone-deep-32b",
  "full_texts": [
    {
      "arxiv_id": "https://arxiv.org/abs/2503.12524",
      "full_text": " [2503.12524] EXAONE Deep: Reasoning Enhanced Language Models Skip to main content We gratefully acknowledge support from the Simons Foundation, member institutions , and all contributors. Donate &gt; cs &gt; arXiv:2503.12524 Help | Advanced Search All fields Title Author Abstract Comments Journal reference ACM classification MSC classification Report number arXiv identifier DOI ORCID arXiv author ID Help pages Full text Search open search GO open navigation menu quick links Login Help Pages About --> Computer Science > Computation and Language arXiv:2503.12524 (cs) [Submitted on 16 Mar 2025 ( v1 ), last revised 19 Mar 2025 (this version, v2)] Title: EXAONE Deep: Reasoning Enhanced Language Models Authors: LG AI Research , Kyunghoon Bae , Eunbi Choi , Kibong Choi , Stanley Jungkyu Choi , Yemuk Choi , Seokhee Hong , Junwon Hwang , Hyojin Jeon , Kijeong Jeon , Gerrard Jeongwon Jo , Hyunjik Jo , Jiyeon Jung , Hyosang Kim , Joonkee Kim , Seonghwan Kim , Soyeon Kim , Sunkyoung Kim , Yireun Kim , Yongil Kim , Youchul Kim , Edward Hwayoung Lee , Haeju Lee , Honglak Lee , Jinsik Lee , Kyungmin Lee , Sangha Park , Yongmin Park , Sihoon Yang , Heuiyeen Yeen , Sihyuk Yi , Hyeongu Yun View a PDF of the paper titled EXAONE Deep: Reasoning Enhanced Language Models, by LG AI Research and 31 other authors View PDF HTML (experimental) Abstract: We present EXAONE Deep series, which exhibits superior capabilities in various reasoning tasks, including math and coding benchmarks. We train our models mainly on the reasoning-specialized dataset that incorporates long streams of thought processes. Evaluation results show that our smaller models, EXAONE Deep 2.4B and 7.8B, outperform other models of comparable size, while the largest model, EXAONE Deep 32B, demonstrates competitive performance against leading open-weight models. All EXAONE Deep models are openly available for research purposes and can be downloaded from this https URL Comments: arXiv admin note: substantial text overlap with arXiv:2412.04862 , arXiv:2408.03541 Subjects: Computation and Language (cs.CL) ; Artificial Intelligence (cs.AI) Cite as: arXiv:2503.12524 [cs.CL] &nbsp; (or arXiv:2503.12524v2 [cs.CL] for this version) &nbsp; https://doi.org/10.48550/arXiv.2503.12524 Focus to learn more arXiv-issued DOI via DataCite Submission history From: Jinsik Lee [ view email ] [v1] Sun, 16 Mar 2025 14:39:33 UTC (72 KB) [v2] Wed, 19 Mar 2025 07:09:24 UTC (72 KB) Full-text links: Access Paper: View a PDF of the paper titled EXAONE Deep: Reasoning Enhanced Language Models, by LG AI Research and 31 other authors View PDF HTML (experimental) TeX Source Other Formats view license Current browse context: cs.CL &lt;&nbsp;prev &nbsp; | &nbsp; next&nbsp;&gt; new | recent | 2025-03 Change to browse by: cs cs.AI References &amp; Citations NASA ADS Google Scholar Semantic Scholar a export BibTeX citation Loading... BibTeX formatted citation &times; loading... Data provided by: Bookmark Bibliographic Tools Bibliographic and Citation Tools Bibliographic Explorer Toggle Bibliographic Explorer ( What is the Explorer? ) Connected Papers Toggle Connected Papers ( What is Connected Papers? ) Litmaps Toggle Litmaps ( What is Litmaps? ) scite.ai Toggle scite Smart Citations ( What are Smart Citations? ) Code, Data, Media Code, Data and Media Associated with this Article alphaXiv Toggle alphaXiv ( What is alphaXiv? ) Links to Code Toggle CatalyzeX Code Finder for Papers ( What is CatalyzeX? ) DagsHub Toggle DagsHub ( What is DagsHub? ) GotitPub Toggle Gotit.pub ( What is GotitPub? ) Huggingface Toggle Hugging Face ( What is Huggingface? ) Links to Code Toggle Papers with Code ( What is Papers with Code? ) ScienceCast Toggle ScienceCast ( What is ScienceCast? ) Demos Demos Replicate Toggle Replicate ( What is Replicate? ) Spaces Toggle Hugging Face Spaces ( What is Spaces? ) Spaces Toggle TXYZ.AI ( What is TXYZ.AI? ) Related Papers Recommenders and Search Tools Link to Influence Flower Influence Flower ( What are Influence Flowers? ) Core recommender toggle CORE Recommender ( What is CORE? ) Author Venue Institution Topic About arXivLabs arXivLabs: experimental projects with community collaborators arXivLabs is a framework that allows collaborators to develop and share new arXiv features directly on our website. Both individuals and organizations that work with arXivLabs have embraced and accepted our values of openness, community, excellence, and user data privacy. arXiv is committed to these values and only works with partners that adhere to them. Have an idea for a project that will add value for arXiv's community? Learn more about arXivLabs . Which authors of this paper are endorsers? | Disable MathJax ( What is MathJax? ) About Help contact arXiv Click here to contact arXiv Contact subscribe to arXiv mailings Click here to subscribe Subscribe Copyright Privacy Policy Web Accessibility Assistance arXiv Operational Status Get status notifications via email or slack ",
      "fetch_method": "direct-html"
    },
    {
      "arxiv_id": "https://www.lgresearch.ai/news/view?seq=543",
      "full_text": " EXAONE Deep Released ━ Setting a New Standard for Reasoning AI - LG AI Research News LG AI Research ABOUT MISSION LEADERSHIP ETHICS PRINCIPLES LOCATION SOLUTION EXAONE EXAONE Showroom RESEARCH SUPERINTELLIGENCE EXAONE LANGUAGE VISION BIO INTELLIGENCE DATA INTELLIGENCE MATERIALS INTELLIGENCE PUBLICATION CAREERS RECRUIT RECRUITMENT PROCESS CULTURE &amp; BENEFIT ACTIVITY BLOG&amp;NEWS RESEARCH BLOG NEWS KR KR EN 검색 검색닫기 메뉴열기 메뉴닫기 LG AI Research ABOUT MISSION LEADERSHIP ETHICS PRINCIPLES LOCATION SOLUTION EXAONE EXAONE Showroom RESEARCH SUPERINTELLIGENCE EXAONE LANGUAGE VISION BIO INTELLIGENCE DATA INTELLIGENCE MATERIALS INTELLIGENCE PUBLICATION CAREERS RECRUIT RECRUITMENT PROCESS CULTURE &amp; BENEFIT ACTIVITY BLOG&amp;NEWS RESEARCH BLOG NEWS RECRUIT LOCATION NEWS 목록보기 Youtube Facebook Linked in CAREERS 개인정보처리방침 정도경영 사이버 신문고 ⓒ 2020. LG AI Research. All Rights Reserved. 스크롤 위로 ",
      "fetch_method": "direct-html"
    },
    {
      "arxiv_id": "https://www.lgresearch.ai/news/view?seq=543",
      "full_text": " EXAONE Deep Released ━ Setting a New Standard for Reasoning AI - LG AI Research News LG AI Research ABOUT MISSION LEADERSHIP ETHICS PRINCIPLES LOCATION SOLUTION EXAONE EXAONE Showroom RESEARCH SUPERINTELLIGENCE EXAONE LANGUAGE VISION BIO INTELLIGENCE DATA INTELLIGENCE MATERIALS INTELLIGENCE PUBLICATION CAREERS RECRUIT RECRUITMENT PROCESS CULTURE &amp; BENEFIT ACTIVITY BLOG&amp;NEWS RESEARCH BLOG NEWS KR KR EN 검색 검색닫기 메뉴열기 메뉴닫기 LG AI Research ABOUT MISSION LEADERSHIP ETHICS PRINCIPLES LOCATION SOLUTION EXAONE EXAONE Showroom RESEARCH SUPERINTELLIGENCE EXAONE LANGUAGE VISION BIO INTELLIGENCE DATA INTELLIGENCE MATERIALS INTELLIGENCE PUBLICATION CAREERS RECRUIT RECRUITMENT PROCESS CULTURE &amp; BENEFIT ACTIVITY BLOG&amp;NEWS RESEARCH BLOG NEWS RECRUIT LOCATION NEWS 목록보기 Youtube Facebook Linked in CAREERS 개인정보처리방침 정도경영 사이버 신문고 ⓒ 2020. LG AI Research. All Rights Reserved. 스크롤 위로 ",
      "fetch_method": "direct-html"
    },
    {
      "arxiv_id": "https://arxiv.org/abs/2503.12524",
      "full_text": " [2503.12524] EXAONE Deep: Reasoning Enhanced Language Models Skip to main content We gratefully acknowledge support from the Simons Foundation, member institutions , and all contributors. Donate &gt; cs &gt; arXiv:2503.12524 Help | Advanced Search All fields Title Author Abstract Comments Journal reference ACM classification MSC classification Report number arXiv identifier DOI ORCID arXiv author ID Help pages Full text Search open search GO open navigation menu quick links Login Help Pages About --> Computer Science > Computation and Language arXiv:2503.12524 (cs) [Submitted on 16 Mar 2025 ( v1 ), last revised 19 Mar 2025 (this version, v2)] Title: EXAONE Deep: Reasoning Enhanced Language Models Authors: LG AI Research , Kyunghoon Bae , Eunbi Choi , Kibong Choi , Stanley Jungkyu Choi , Yemuk Choi , Seokhee Hong , Junwon Hwang , Hyojin Jeon , Kijeong Jeon , Gerrard Jeongwon Jo , Hyunjik Jo , Jiyeon Jung , Hyosang Kim , Joonkee Kim , Seonghwan Kim , Soyeon Kim , Sunkyoung Kim , Yireun Kim , Yongil Kim , Youchul Kim , Edward Hwayoung Lee , Haeju Lee , Honglak Lee , Jinsik Lee , Kyungmin Lee , Sangha Park , Yongmin Park , Sihoon Yang , Heuiyeen Yeen , Sihyuk Yi , Hyeongu Yun View a PDF of the paper titled EXAONE Deep: Reasoning Enhanced Language Models, by LG AI Research and 31 other authors View PDF HTML (experimental) Abstract: We present EXAONE Deep series, which exhibits superior capabilities in various reasoning tasks, including math and coding benchmarks. We train our models mainly on the reasoning-specialized dataset that incorporates long streams of thought processes. Evaluation results show that our smaller models, EXAONE Deep 2.4B and 7.8B, outperform other models of comparable size, while the largest model, EXAONE Deep 32B, demonstrates competitive performance against leading open-weight models. All EXAONE Deep models are openly available for research purposes and can be downloaded from this https URL Comments: arXiv admin note: substantial text overlap with arXiv:2412.04862 , arXiv:2408.03541 Subjects: Computation and Language (cs.CL) ; Artificial Intelligence (cs.AI) Cite as: arXiv:2503.12524 [cs.CL] &nbsp; (or arXiv:2503.12524v2 [cs.CL] for this version) &nbsp; https://doi.org/10.48550/arXiv.2503.12524 Focus to learn more arXiv-issued DOI via DataCite Submission history From: Jinsik Lee [ view email ] [v1] Sun, 16 Mar 2025 14:39:33 UTC (72 KB) [v2] Wed, 19 Mar 2025 07:09:24 UTC (72 KB) Full-text links: Access Paper: View a PDF of the paper titled EXAONE Deep: Reasoning Enhanced Language Models, by LG AI Research and 31 other authors View PDF HTML (experimental) TeX Source Other Formats view license Current browse context: cs.CL &lt;&nbsp;prev &nbsp; | &nbsp; next&nbsp;&gt; new | recent | 2025-03 Change to browse by: cs cs.AI References &amp; Citations NASA ADS Google Scholar Semantic Scholar a export BibTeX citation Loading... BibTeX formatted citation &times; loading... Data provided by: Bookmark Bibliographic Tools Bibliographic and Citation Tools Bibliographic Explorer Toggle Bibliographic Explorer ( What is the Explorer? ) Connected Papers Toggle Connected Papers ( What is Connected Papers? ) Litmaps Toggle Litmaps ( What is Litmaps? ) scite.ai Toggle scite Smart Citations ( What are Smart Citations? ) Code, Data, Media Code, Data and Media Associated with this Article alphaXiv Toggle alphaXiv ( What is alphaXiv? ) Links to Code Toggle CatalyzeX Code Finder for Papers ( What is CatalyzeX? ) DagsHub Toggle DagsHub ( What is DagsHub? ) GotitPub Toggle Gotit.pub ( What is GotitPub? ) Huggingface Toggle Hugging Face ( What is Huggingface? ) Links to Code Toggle Papers with Code ( What is Papers with Code? ) ScienceCast Toggle ScienceCast ( What is ScienceCast? ) Demos Demos Replicate Toggle Replicate ( What is Replicate? ) Spaces Toggle Hugging Face Spaces ( What is Spaces? ) Spaces Toggle TXYZ.AI ( What is TXYZ.AI? ) Related Papers Recommenders and Search Tools Link to Influence Flower Influence Flower ( What are Influence Flowers? ) Core recommender toggle CORE Recommender ( What is CORE? ) Author Venue Institution Topic About arXivLabs arXivLabs: experimental projects with community collaborators arXivLabs is a framework that allows collaborators to develop and share new arXiv features directly on our website. Both individuals and organizations that work with arXivLabs have embraced and accepted our values of openness, community, excellence, and user data privacy. arXiv is committed to these values and only works with partners that adhere to them. Have an idea for a project that will add value for arXiv's community? Learn more about arXivLabs . Which authors of this paper are endorsers? | Disable MathJax ( What is MathJax? ) About Help contact arXiv Click here to contact arXiv Contact subscribe to arXiv mailings Click here to subscribe Subscribe Copyright Privacy Policy Web Accessibility Assistance arXiv Operational Status Get status notifications via email or slack ",
      "fetch_method": "direct-html"
    },
    {
      "arxiv_id": "https://docs.vllm.ai/en/latest/getting_started/quickstart.html",
      "full_text": " Quickstart - vLLM Skip to content You are viewing the latest developer preview docs. Click here to view docs for the latest stable release. vLLM Quickstart Initializing search GitHub Home User Guide Developer Guide API Reference CLI Reference Community vLLM GitHub Home User Guide User Guide Getting Started Getting Started Quickstart Quickstart Table of contents Prerequisites Installation Offline Batched Inference OpenAI-Compatible Server OpenAI Completions API with vLLM OpenAI Chat Completions API with vLLM On Attention Backends Installation Installation GPU CPU Google TPU AWS Neuron Examples Examples Offline Inference Offline Inference Async LLM Streaming Audio Language Automatic Prefix Caching Basic Batch LLM Inference Chat With Tools Context Extension Convert Model To Seq Cls Data Parallel Disaggregated Prefill V1 Disaggregated Prefill Dolphin Embed Jina Embeddings V3 Embed Matryoshka Fy Encoder Decoder Encoder Decoder Multimodal LLM Engine Example Load Sharded State LoRA With Quantization Inference Metrics Mistral-Small MLPSpeculator MultiLoRA Inference Offline Inference with the OpenAI Batch file format Prefix Caching Prithvi Geospatial MAE Prithvi Geospatial MAE Io Processor Profiling vLLM TPU Profiling Prompt Embed Inference Qwen2.5-Omni Offline Inference Examples Qwen3 Reranker Qwen 1M Reproducibility RLHF RLHF Colocate RLHF Utils Save Sharded State Simple Profiling Skip Loading Weights In Engine Init Spec Decode Structured Outputs Torchrun Example TPU Vision Language Vision Language Multi Image Vision Language Pooling Online Serving Online Serving API Client Helm Charts Cohere Rerank Client Disaggregated Prefill Disaggregated Serving Gradio OpenAI Chatbot Webserver Gradio Webserver Jinaai Rerank Client Kv Events Subscriber Multi-Node-Serving Multi Instance Data Parallel OpenAI Chat Completion Client OpenAI Chat Completion Client For Multimodal OpenAI Chat Completion Client With Tools OpenAI Chat Completion Client With Tools Required OpenAI Chat Completion Client With Tools Xlam OpenAI Chat Completion Client With Tools Xlam Streaming OpenAI Chat Completion Tool Calls With Reasoning OpenAI Chat Completion With Reasoning OpenAI Chat Completion With Reasoning Streaming OpenAI Chat Embedding Client For Multimodal OpenAI Classification Client OpenAI Completion Client OpenAI Cross Encoder Score OpenAI Cross Encoder Score For Multimodal OpenAI Embedding Client Long Text Embedding with Chunked Processing OpenAI Embedding Matryoshka Fy OpenAI Pooling Client OpenAI Transcription Client OpenAI Translation Client Setup OpenTelemetry POC Prithvi Geospatial MAE Prometheus and Grafana Prompt Embed Inference With OpenAI Client Ray Serve Deepseek Retrieval Augmented Generation With Langchain Retrieval Augmented Generation With Llamaindex Run Cluster Sagemaker-Entrypoint Streamlit OpenAI Chatbot Webserver Structured Outputs Utils Others Others LMCache Examples Logging Configuration Tensorize vLLM Model General General vLLM V1 Frequently Asked Questions Production Metrics Reproducibility Security Troubleshooting Usage Stats Collection Inference and Serving Inference and Serving Offline Inference OpenAI-Compatible Server Data Parallel Deployment Troubleshooting distributed deployments Expert Parallel Deployment Parallelism and Scaling Integrations Integrations LangChain LlamaIndex Deployment Deployment Using Docker Using Kubernetes Using Nginx Frameworks Frameworks Anyscale AnythingLLM AutoGen BentoML Cerebrium Chatbox Dify dstack Haystack Helm LiteLLM Lobe Chat LWS Modal Open WebUI Retrieval-Augmented Generation SkyPilot Streamlit NVIDIA Triton Integrations Integrations KServe KubeAI KubeRay Llama Stack llmaz Production stack Training Training Reinforcement Learning from Human Feedback Transformers Reinforcement Learning Configuration Configuration Conserving Memory Engine Arguments Environment Variables Model Resolution Optimization and Tuning Server Arguments TPU Optimization Tips Models Models Supported Models Generative Models Pooling Models Extensions Extensions Loading Model weights with fastsafetensors Loading models with Run:ai Model Streamer Loading models with CoreWeave's Tensorizer Hardware Supported Models Hardware Supported Models TPU Features Features Automatic Prefix Caching Disaggregated Prefilling (experimental) LoRA Adapters Multimodal Inputs Prompt Embedding Inputs Reasoning Outputs Sleep Mode Speculative Decoding Structured Outputs Tool Calling Quantization Quantization AutoAWQ AutoRound BitBLAS BitsAndBytes FP8 W8A8 GGUF GPTQModel FP8 INC INT4 W4A16 INT8 W8A8 NVIDIA TensorRT Model Optimizer Quantized KV Cache AMD Quark TorchAO Developer Guide Developer Guide General General Benchmark Suites Deprecation Policy Dockerfile Incremental Compilation Workflow Profiling vLLM Vulnerability Management Model Implementation Model Implementation Basic Model Registering a Model Unit Testing Multi-Modal Support Speech-to-Text (Transcription/Translation) Support CI CI CI Failures Update PyTorch version on vLLM OSS CI/CD Design Documents Design Documents Architecture Overview Fused MoE Modular Kernel Integration with Hugging Face Hybrid KV Cache Manager IO Processor Plugins Metrics Multi-Modal Data Processing Python Multiprocessing P2P NCCL Connector Paged Attention Plugin System Automatic Prefix Caching torch.compile integration API Reference API Reference vllm vllm beam_search collect_env connections env_override envs forward_context logger logits_process logprobs outputs pooling_params sampling_params scalar_type scripts sequence tasks test_utils tracing version adapter_commons adapter_commons layers models request utils worker_manager assets assets audio base image video attention attention layer selector backends backends abstract differential_flash_attn dual_chunk_flash_attn flash_attn flashmla placeholder_attn rocm_aiter_mla rocm_flash_attn triton_mla utils xformers mla mla common layers layers chunked_local_attention cross_attention encoder_only_attention ops ops chunked_prefill_paged_decode common flashmla merge_attn_states paged_attn pallas_kv_cache_update prefix_prefill rocm_aiter_mla rocm_aiter_paged_attn triton_decode_attention triton_flash_attention triton_merge_attn_states triton_unified_attention utils utils fa_utils kv_sharing_utils benchmarks benchmarks datasets latency serve throughput lib lib endpoint_request_func ready_checker utils compilation compilation activation_quant_fusion backends base_static_graph collective_fusion compiler_interface counter cuda_graph cuda_piecewise_backend decorators fix_functionalization fusion fusion_attn fx_utils inductor_pass monitor multi_output_match noop_elimination pass_manager sequence_parallelism torch25_custom_graph_pass vllm_inductor_pass wrapper config config cache compilation kv_events kv_transfer load lora parallel scheduler utils core core block_manager evictor interfaces placeholder_block_space_manager scheduler block block block_table common cpu_gpu_block_allocator interfaces naive_block prefix_caching_block utils device_allocator device_allocator cumem distributed distributed communication_op kv_events parallel_state tpu_distributed_utils utils device_communicators device_communicators all2all all_reduce_utils base_device_communicator cpu_communicator cuda_communicator cuda_wrapper custom_all_reduce pynccl pynccl_wrapper quick_all_reduce ray_communicator shm_broadcast shm_object_storage symm_mem tpu_communicator xpu_communicator eplb eplb eplb_state rebalance_algo rebalance_execute kv_transfer kv_transfer kv_transfer_state kv_connector kv_connector base factory utils v1 v1 base lmcache_connector multi_connector nixl_connector shared_storage_connector p2p p2p p2p_nccl_connector p2p_nccl_engine tensor_memory_pool kv_lookup_buffer kv_lookup_buffer base mooncake_store simple_buffer kv_pipe kv_pipe base mooncake_pipe pynccl_pipe engine engine arg_utils async_llm_engine async_timeout llm_engine metrics metrics_types protocol multiprocessing multiprocessing client engine output_processor output_processor interfaces single_step stop_checker util entrypoints entrypoints api_server chat_utils constants context harmony_utils launcher llm logger renderer score_utils ssl tool tool_server utils cli cli collect_env main openai run_batch serve types benchmark benchmark base latency main serve throughput openai openai api_server cli_args logits_processors protocol run_batch serving_chat serving_classification serving_completion serving_embedding serving_engine serving_models serving_pooling serving_responses serving_score serving_tokenization serving_transcription speech_to_text tool_parsers tool_parsers abstract_tool_parser deepseekv3_tool_parser deepseekv31_tool_parser glm4_moe_tool_parser granite_20b_fc_tool_parser granite_tool_parser hermes_tool_parser hunyuan_a13b_tool_parser internlm2_tool_parser jamba_tool_parser kimi_k2_tool_parser llama4_pythonic_tool_parser llama_tool_parser minimax_tool_parser mistral_tool_parser openai_tool_parser phi4mini_tool_parser pythonic_tool_parser qwen3coder_tool_parser seed_oss_tool_parser step3_tool_parser utils xlam_tool_parser executor executor executor_base mp_distributed_executor msgspec_utils multiproc_worker_utils ray_distributed_executor ray_utils uniproc_executor inputs inputs data parse preprocess registry logging_utils logging_utils dump_input formatter lora lora lora models peft_helper request resolver utils worker_manager layers layers base base_linear column_parallel_linear logits_processor qkv_x_parallel_linear replicated_linear row_parallel_linear utils vocal_parallel_embedding ops ops ipex_ops ipex_ops lora_ops torch_ops torch_ops lora_ops triton_ops triton_ops kernel_utils lora_expand_op lora_kernel_metadata lora_shrink_op utils xla_ops xla_ops lora_ops punica_wrapper punica_wrapper punica_base punica_cpu punica_gpu punica_selector punica_tpu punica_xpu utils model_executor model_executor custom_op parameter sampling_metadata utils layers layers activation attention_layer_base layernorm lightning_attn linear logits_processor mla pooler resampler sampler utils vocab_parallel_embedding fla fla ops ops chunk chunk_delta_h chunk_o chunk_scaled_dot_kkt cumsum fused_recurrent index l2norm layernorm_guard op solve_tril utils wy_fast fused_moe fused_moe batched_deep_gemm_moe batched_triton_or_deep_gemm_moe config cpu_fused_moe cutlass_moe deep_gemm_moe deep_gemm_utils deepep_ht_prepare_finalize deepep_ll_prepare_finalize flashinfer_cutlass_moe flashinfer_cutlass_prepare_finalize fused_batched_moe fused_marlin_moe fused_moe gpt_oss_triton_kernels_moe layer modular_kernel moe_align_block_size moe_pallas moe_permute_unpermute moe_torch_iterative pplx_prepare_finalize prepare_finalize rocm_aiter_fused_moe routing_simulator topk_weight_and_reduce triton_deep_gemm_moe trtllm_moe utils mamba mamba abstract linear_attn mamba2_metadata mamba_mixer mamba_mixer2 mamba_utils short_conv ops ops causal_conv1d layernorm_gated mamba_ssm ssd_bmm ssd_chunk_scan ssd_chunk_state ssd_combined ssd_state_passing quantization quantization auto_round awq awq_marlin awq_triton base_config bitblas bitsandbytes deepgemm deepspeedfp experts_int8 fbgemm_fp8 fp8 gguf gptq gptq_bitblas gptq_marlin gptq_marlin_24 hqq_marlin inc input_quant_fp8 ipex_quant kv_cache modelopt moe_wna16 mxfp4 petit ptpc_fp8 rtn schema torchao tpu_int8 compressed_tensors compressed_tensors compressed_tensors compressed_tensors_moe triton_scaled_mm utils schemes schemes compressed_tensors_24 compressed_tensors_scheme compressed_tensors_w4a4_nvfp4 compressed_tensors_w4a8_fp8 compressed_tensors_w4a8_int compressed_tensors_w4a16_24 compressed_tensors_w4a16_nvfp4 compressed_tensors_w8a8_fp8 compressed_tensors_w8a8_int8 compressed_tensors_w8a16_fp8 compressed_tensors_wNa16 kernels kernels mixed_precision mixed_precision allspark bitblas conch cutlass dynamic_4bit exllama MPLinearKernel machete marlin scaled_mm scaled_mm aiter cpu cutlass ScaledMMLinearKernel triton xla quark quark quark quark_moe utils schemes schemes quark_scheme quark_w4a4_mxfp4 quark_w8a8_fp8 quark_w8a8_int8 utils utils allspark_utils bitblas_utils flashinfer_fp4_moe flashinfer_utils fp8_utils gptq_utils int8_utils layer_utils machete_utils marlin_utils marlin_utils_fp4 marlin_utils_fp8 marlin_utils_test marlin_utils_test_24 mxfp4_utils mxfp8_utils nvfp4_emulation_utils nvfp4_moe_support petit_utils quant_utils w8a8_utils rotary_embedding rotary_embedding base common deepseek_scaling_rope dual_chunk_rope dynamic_ntk_alpha_rope dynamic_ntk_scaling_rope ernie45_vl_rope linear_scaling_rope llama3_rope llama4_vision_rope mrope ntk_scaling_rope phi3_long_rope_scaled_rope yarn_scaling_rope shared_fused_moe shared_fused_moe shared_fused_moe model_loader model_loader base_loader bitsandbytes_loader default_loader dummy_loader gguf_loader runai_streamer_loader sharded_state_loader tensorizer tensorizer_loader tpu utils weight_utils models models adapters aimv2 apertus arcee arctic aria aya_vision baichuan bailing_moe bamba bart bert bert_with_rope blip blip2 bloom chameleon chatglm clip cohere2_vision commandr config constant_size_cache dbrx deepseek deepseek_eagle deepseek_mtp deepseek_v2 deepseek_vl2 donut dots1 ernie45 ernie45_moe ernie45_vl ernie45_vl_moe ernie_mtp exaone exaone4 fairseq2_llama falcon falcon_h1 florence2 fuyu gemma gemma2 gemma3 gemma3_mm gemma3n gemma3n_mm glm glm4 glm4_1v glm4_moe glm4_moe_mtp glm4v gpt2 gpt_bigcode gpt_j gpt_neox gpt_oss granite granite_speech granitemoe granitemoehybrid granitemoeshared gritlm grok1 h2ovl hunyuan_v1 hyperclovax_vision idefics2_vision_model idefics3 interfaces interfaces_base intern_vit internlm2 internlm2_ve interns1 interns1_vit internvl jais jamba jina_vl keye keye_vl1_5 kimi_vl lfm2 llama llama4 llama4_eagle llama_eagle llama_eagle3 llava llava_next llava_next_video llava_onevision mamba mamba2 mamba_cache medusa midashenglm mimo mimo_mtp minicpm minicpm3 minicpm_eagle minicpmo minicpmv minimax_cache minimax_text_01 minimax_vl_01 mistral3 mixtral mllama mllama4 mlp_speculator modernbert module_mapping molmo moonvit motif mpt nano_nemotron_vl nemotron nemotron_h nemotron_nas nemotron_vl nvlm_d olmo olmo2 olmoe opt orion ovis ovis2_5 paligemma persimmon phi phi3 phi3v phi4_multimodal phi4flash phi4mm phi4mm_audio phi4mm_utils phimoe pixtral plamo2 qwen qwen2 qwen2_5_omni_thinker qwen2_5_vl qwen2_audio qwen2_moe qwen2_rm qwen2_vl qwen3 qwen3_moe qwen3_next qwen3_next_mtp qwen_vl registry roberta rvl seed_oss siglip siglip2navit skyworkr1v smolvlm solar stablelm starcoder2 step3_text step3_vl swin tarsier telechat2 teleflm terratorch transformers ultravox utils vision voxtral whisper zamba2 warmup warmup deep_gemm_warmup kernel_warmup multimodal multimodal audio base cache hasher image inputs parse processing profiling registry utils video platforms platforms cpu cuda interface rocm tpu xpu plugins plugins io_processors io_processors interface lora_resolvers lora_resolvers filesystem_resolver profiler profiler layerwise_profile utils ray ray lazy_utils ray_env reasoning reasoning abs_reasoning_parsers deepseek_r1_reasoning_parser glm4_moe_reasoning_parser gptoss_reasoning_parser granite_reasoning_parser hunyuan_a13b_reasoning_parser mistral_reasoning_parser qwen3_reasoning_parser step3_reasoning_parser transformers_utils transformers_utils config config_parser_base detokenizer detokenizer_utils dynamic_module processor runai_utils s3_utils tokenizer tokenizer_base tokenizer_group utils chat_templates chat_templates registry configs configs arctic chatglm deepseek_vl2 eagle falcon jais kimi_vl medusa midashenglm mistral mlp_speculator moonvit nemotron nemotron_h nemotron_vl ovis qwen3_next step3_vl ultravox speculators speculators algos base processors processors deepseek_vl2 ovis ovis2_5 tokenizers tokenizers mistral triton_utils triton_utils importing usage usage usage_lib utils utils deep_gemm flashinfer jsontree tensor_schema v1 v1 cudagraph_dispatcher kv_cache_interface outputs request serial_utils utils attention attention backends backends cpu_attn flash_attn flashinfer flex_attention gdn_attn linear_attn mamba1_attn mamba2_attn mamba_attn pallas rocm_aiter_fa short_conv_attn tree_attn triton_attn utils xformers mla mla common cutlass_mla flashattn_mla flashinfer_mla flashmla rocm_aiter_mla triton_mla core core block_pool encoder_cache_manager kv_cache_coordinator kv_cache_manager kv_cache_utils single_type_kv_cache_manager sched sched async_scheduler interface output request_queue scheduler utils engine engine async_llm coordinator core core_client detokenizer exceptions llm_engine logprobs output_processor parallel_sampling processor utils executor executor abstract multiproc_executor ray_distributed_executor utils metrics metrics loggers prometheus ray_wrappers reader stats pool pool metadata sample sample metadata rejection_sampler sampler logits_processor logits_processor builtin interface state ops ops bad_words logprobs penalties topk_topp_sampler tpu tpu metadata sampler spec_decode spec_decode eagle medusa metadata metrics ngram_proposer utils structured_output structured_output backend_guidance backend_lm_format_enforcer backend_outlines backend_types backend_xgrammar request utils worker worker block_table cpu_model_runner cpu_worker gpu_input_batch gpu_model_runner gpu_worker kv_connector_model_runner_mixin lora_model_runner_mixin tpu_input_batch tpu_model_runner tpu_worker utils worker_base xpu_model_runner xpu_worker worker worker cache_engine enc_dec_model_runner model_runner model_runner_base utils worker worker_base CLI Reference CLI Reference vllm serve vllm chat vllm complete vllm run-batch vllm bench vllm bench vllm bench latency vllm bench serve vllm bench throughput Community Community Contact Us Meetups Sponsors Blog Forum Slack Table of contents Prerequisites Installation Offline Batched Inference OpenAI-Compatible Server OpenAI Completions API with vLLM OpenAI Chat Completions API with vLLM On Attention Backends Quickstart ¶ This guide will help you quickly get started with vLLM to perform: Offline batched inference Online serving using OpenAI-compatible server Prerequisites ¶ OS: Linux Python: 3.9 -- 3.13 Installation ¶ If you are using NVIDIA GPUs, you can install vLLM using pip directly. It's recommended to use uv , a very fast Python environment manager, to create and manage Python environments. Please follow the documentation to install uv . After installing uv , you can create a new Python environment and install vLLM using the following commands: uv venv --python 3 .12 --seed source .venv/bin/activate uv pip install vllm --torch-backend = auto uv can automatically select the appropriate PyTorch index at runtime by inspecting the installed CUDA driver version via --torch-backend=auto (or UV_TORCH_BACKEND=auto ). To select a specific backend (e.g., cu126 ), set --torch-backend=cu126 (or UV_TORCH_BACKEND=cu126 ). Another delightful way is to use uv run with --with [dependency] option, which allows you to run commands such as vllm serve without creating any permanent environment: uv run --with vllm vllm --help You can also use conda to create and manage Python environments. You can install uv to the conda environment through pip if you want to manage it within the environment. conda create -n myenv python = 3 .12 -y conda activate myenv pip install --upgrade uv uv pip install vllm --torch-backend = auto Note For more detail and non-CUDA platforms, please refer here for specific instructions on how to install vLLM. Offline Batched Inference ¶ With vLLM installed, you can start generating texts for list of input prompts (i.e. offline batch inferencing). See the example script: examples/offline_inference/basic/basic.py The first line of this example imports the classes LLM and SamplingParams : LLM is the main class for running offline inference with vLLM engine. SamplingParams specifies the parameters for the sampling process. from vllm import LLM , SamplingParams The next section defines a list of input prompts and sampling parameters for text generation. The sampling temperature is set to 0.8 and the nucleus sampling probability is set to 0.95 . You can find more information about the sampling parameters here . Important By default, vLLM will use sampling parameters recommended by model creator by applying the generation_config.json from the Hugging Face model repository if it exists. In most cases, this will provide you with the best results by default if SamplingParams is not specified. However, if vLLM's default sampling parameters are preferred, please set generation_config=\"vllm\" when creating the LLM instance. prompts = [ \"Hello, my name is\" , \"The president of the United States is\" , \"The capital of France is\" , \"The future of AI is\" , ] sampling_params = SamplingParams ( temperature = 0.8 , top_p = 0.95 ) The LLM class initializes vLLM's engine and the OPT-125M model for offline inference. The list of supported models can be found here . llm = LLM ( model = \"facebook/opt-125m\" ) Note By default, vLLM downloads models from Hugging Face . If you would like to use models from ModelScope , set the environment variable VLLM_USE_MODELSCOPE before initializing the engine. export VLLM_USE_MODELSCOPE = True Now, the fun part! The outputs are generated using llm.generate . It adds the input prompts to the vLLM engine's waiting queue and executes the vLLM engine to generate the outputs with high throughput. The outputs are returned as a list of RequestOutput objects, which include all of the output tokens. outputs = llm . generate ( prompts , sampling_params ) for output in outputs : prompt = output . prompt generated_text = output . outputs [ 0 ] . text print ( f \"Prompt: { prompt !r} , Generated text: { generated_text !r} \" ) Note The llm.generate method does not automatically apply the model's chat template to the input prompt. Therefore, if you are using an Instruct model or Chat model, you should manually apply the corresponding chat template to ensure the expected behavior. Alternatively, you can use the llm.chat method and pass a list of messages which have the same format as those passed to OpenAI's client.chat.completions : Code # Using tokenizer to apply chat template from transformers import AutoTokenizer tokenizer = AutoTokenizer . from_pretrained ( \"/path/to/chat_model\" ) messages_list = [ [{ \"role\" : \"user\" , \"content\" : prompt }] for prompt in prompts ] texts = tokenizer . apply_chat_template ( messages_list , tokenize = False , add_generation_prompt = True , ) # Generate outputs outputs = llm . generate ( texts , sampling_params ) # Print the outputs. for output in outputs : prompt = output . prompt generated_text = output . outputs [ 0 ] . text print ( f \"Prompt: { prompt !r} , Generated text: { generated_text !r} \" ) # Using chat interface. outputs = llm . chat ( messages_list , sampling_params ) for idx , output in enumerate ( outputs ): prompt = prompts [ idx ] generated_text = output . outputs [ 0 ] . text print ( f \"Prompt: { prompt !r} , Generated text: { generated_text !r} \" ) OpenAI-Compatible Server ¶ vLLM can be deployed as a server that implements the OpenAI API protocol. This allows vLLM to be used as a drop-in replacement for applications using OpenAI API. By default, it starts the server at http://localhost:8000 . You can specify the address with --host and --port arguments. The server currently hosts one model at a time and implements endpoints such as list models , create chat completion , and create completion endpoints. Run the following command to start the vLLM server with the Qwen2.5-1.5B-Instruct model: vllm serve Qwen/Qwen2.5-1.5B-Instruct Note By default, the server uses a predefined chat template stored in the tokenizer. You can learn about overriding it here . Important By default, the server applies generation_config.json from the huggingface model repository if it exists. This means the default values of certain sampling parameters can be overridden by those recommended by the model creator. To disable this behavior, please pass --generation-config vllm when launching the server. This server can be queried in the same format as OpenAI API. For example, to list the models: curl http://localhost:8000/v1/models You can pass in the argument --api-key or environment variable VLLM_API_KEY to enable the server to check for API key in the header. You can pass multiple keys after --api-key , and the server will accept any of the keys passed, this can be useful for key rotation. OpenAI Completions API with vLLM ¶ Once your server is started, you can query the model with input prompts: curl http://localhost:8000/v1/completions \\ -H \"Content-Type: application/json\" \\ -d '{ \"model\": \"Qwen/Qwen2.5-1.5B-Instruct\", \"prompt\": \"San Francisco is a\", \"max_tokens\": 7, \"temperature\": 0 }' Since this server is compatible with OpenAI API, you can use it as a drop-in replacement for any applications using OpenAI API. For example, another way to query the server is via the openai Python package: Code from openai import OpenAI # Modify OpenAI's API key and API base to use vLLM's API server. openai_api_key = \"EMPTY\" openai_api_base = \"http://localhost:8000/v1\" client = OpenAI ( api_key = openai_api_key , base_url = openai_api_base , ) completion = client . completions . create ( model = \"Qwen/Qwen2.5-1.5B-Instruct\" , prompt = \"San Francisco is a\" ) print ( \"Completion result:\" , completion ) A more detailed client example can be found here: examples/online_serving/openai_completion_client.py OpenAI Chat Completions API with vLLM ¶ vLLM is designed to also support the OpenAI Chat Completions API. The chat interface is a more dynamic, interactive way to communicate with the model, allowing back-and-forth exchanges that can be stored in the chat history. This is useful for tasks that require context or more detailed explanations. You can use the create chat completion endpoint to interact with the model: curl http://localhost:8000/v1/chat/completions \\ -H \"Content-Type: application/json\" \\ -d '{ \"model\": \"Qwen/Qwen2.5-1.5B-Instruct\", \"messages\": [ {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"}, {\"role\": \"user\", \"content\": \"Who won the world series in 2020?\"} ] }' Alternatively, you can use the openai Python package: Code from openai import OpenAI # Set OpenAI's API key and API base to use vLLM's API server. openai_api_key = \"EMPTY\" openai_api_base = \"http://localhost:8000/v1\" client = OpenAI ( api_key = openai_api_key , base_url = openai_api_base , ) chat_response = client . chat . completions . create ( model = \"Qwen/Qwen2.5-1.5B-Instruct\" , messages = [ { \"role\" : \"system\" , \"content\" : \"You are a helpful assistant.\" }, { \"role\" : \"user\" , \"content\" : \"Tell me a joke.\" }, ] ) print ( \"Chat response:\" , chat_response ) On Attention Backends ¶ Currently, vLLM supports multiple backends for efficient Attention computation across different platforms and accelerator architectures. It automatically selects the most performant backend compatible with your system and model specifications. If desired, you can also manually set the backend of your choice by configuring the environment variable VLLM_ATTENTION_BACKEND to one of the following options: FLASH_ATTN , FLASHINFER or XFORMERS . Warning There are no pre-built vllm wheels containing Flash Infer, so you must install it in your environment first. Refer to the Flash Infer official docs or see docker/Dockerfile for instructions on how to install it. August 19, 2025 Back to top Made with Material for MkDocs ",
      "fetch_method": "direct-html"
    },
    {
      "arxiv_id": "https://docs.vllm.ai/en/latest/quantization/gguf.html",
      "full_text": " GGUF - vLLM Skip to content You are viewing the latest developer preview docs. Click here to view docs for the latest stable release. vLLM GGUF Initializing search GitHub Home User Guide Developer Guide API Reference CLI Reference Community vLLM GitHub Home User Guide User Guide Getting Started Getting Started Quickstart Installation Installation GPU CPU Google TPU AWS Neuron Examples Examples Offline Inference Offline Inference Async LLM Streaming Audio Language Automatic Prefix Caching Basic Batch LLM Inference Chat With Tools Context Extension Convert Model To Seq Cls Data Parallel Disaggregated Prefill V1 Disaggregated Prefill Dolphin Embed Jina Embeddings V3 Embed Matryoshka Fy Encoder Decoder Encoder Decoder Multimodal LLM Engine Example Load Sharded State LoRA With Quantization Inference Metrics Mistral-Small MLPSpeculator MultiLoRA Inference Offline Inference with the OpenAI Batch file format Prefix Caching Prithvi Geospatial MAE Prithvi Geospatial MAE Io Processor Profiling vLLM TPU Profiling Prompt Embed Inference Qwen2.5-Omni Offline Inference Examples Qwen3 Reranker Qwen 1M Reproducibility RLHF RLHF Colocate RLHF Utils Save Sharded State Simple Profiling Skip Loading Weights In Engine Init Spec Decode Structured Outputs Torchrun Example TPU Vision Language Vision Language Multi Image Vision Language Pooling Online Serving Online Serving API Client Helm Charts Cohere Rerank Client Disaggregated Prefill Disaggregated Serving Gradio OpenAI Chatbot Webserver Gradio Webserver Jinaai Rerank Client Kv Events Subscriber Multi-Node-Serving Multi Instance Data Parallel OpenAI Chat Completion Client OpenAI Chat Completion Client For Multimodal OpenAI Chat Completion Client With Tools OpenAI Chat Completion Client With Tools Required OpenAI Chat Completion Client With Tools Xlam OpenAI Chat Completion Client With Tools Xlam Streaming OpenAI Chat Completion Tool Calls With Reasoning OpenAI Chat Completion With Reasoning OpenAI Chat Completion With Reasoning Streaming OpenAI Chat Embedding Client For Multimodal OpenAI Classification Client OpenAI Completion Client OpenAI Cross Encoder Score OpenAI Cross Encoder Score For Multimodal OpenAI Embedding Client Long Text Embedding with Chunked Processing OpenAI Embedding Matryoshka Fy OpenAI Pooling Client OpenAI Transcription Client OpenAI Translation Client Setup OpenTelemetry POC Prithvi Geospatial MAE Prometheus and Grafana Prompt Embed Inference With OpenAI Client Ray Serve Deepseek Retrieval Augmented Generation With Langchain Retrieval Augmented Generation With Llamaindex Run Cluster Sagemaker-Entrypoint Streamlit OpenAI Chatbot Webserver Structured Outputs Utils Others Others LMCache Examples Logging Configuration Tensorize vLLM Model General General vLLM V1 Frequently Asked Questions Production Metrics Reproducibility Security Troubleshooting Usage Stats Collection Inference and Serving Inference and Serving Offline Inference OpenAI-Compatible Server Data Parallel Deployment Troubleshooting distributed deployments Expert Parallel Deployment Parallelism and Scaling Integrations Integrations LangChain LlamaIndex Deployment Deployment Using Docker Using Kubernetes Using Nginx Frameworks Frameworks Anyscale AnythingLLM AutoGen BentoML Cerebrium Chatbox Dify dstack Haystack Helm LiteLLM Lobe Chat LWS Modal Open WebUI Retrieval-Augmented Generation SkyPilot Streamlit NVIDIA Triton Integrations Integrations KServe KubeAI KubeRay Llama Stack llmaz Production stack Training Training Reinforcement Learning from Human Feedback Transformers Reinforcement Learning Configuration Configuration Conserving Memory Engine Arguments Environment Variables Model Resolution Optimization and Tuning Server Arguments TPU Optimization Tips Models Models Supported Models Generative Models Pooling Models Extensions Extensions Loading Model weights with fastsafetensors Loading models with Run:ai Model Streamer Loading models with CoreWeave's Tensorizer Hardware Supported Models Hardware Supported Models TPU Features Features Automatic Prefix Caching Disaggregated Prefilling (experimental) LoRA Adapters Multimodal Inputs Prompt Embedding Inputs Reasoning Outputs Sleep Mode Speculative Decoding Structured Outputs Tool Calling Quantization Quantization AutoAWQ AutoRound BitBLAS BitsAndBytes FP8 W8A8 GGUF GPTQModel FP8 INC INT4 W4A16 INT8 W8A8 NVIDIA TensorRT Model Optimizer Quantized KV Cache AMD Quark TorchAO Developer Guide Developer Guide General General Benchmark Suites Deprecation Policy Dockerfile Incremental Compilation Workflow Profiling vLLM Vulnerability Management Model Implementation Model Implementation Basic Model Registering a Model Unit Testing Multi-Modal Support Speech-to-Text (Transcription/Translation) Support CI CI CI Failures Update PyTorch version on vLLM OSS CI/CD Design Documents Design Documents Architecture Overview Fused MoE Modular Kernel Integration with Hugging Face Hybrid KV Cache Manager IO Processor Plugins Metrics Multi-Modal Data Processing Python Multiprocessing P2P NCCL Connector Paged Attention Plugin System Automatic Prefix Caching torch.compile integration API Reference API Reference vllm vllm beam_search collect_env connections env_override envs forward_context logger logits_process logprobs outputs pooling_params sampling_params scalar_type scripts sequence tasks test_utils tracing version adapter_commons adapter_commons layers models request utils worker_manager assets assets audio base image video attention attention layer selector backends backends abstract differential_flash_attn dual_chunk_flash_attn flash_attn flashmla placeholder_attn rocm_aiter_mla rocm_flash_attn triton_mla utils xformers mla mla common layers layers chunked_local_attention cross_attention encoder_only_attention ops ops chunked_prefill_paged_decode common flashmla merge_attn_states paged_attn pallas_kv_cache_update prefix_prefill rocm_aiter_mla rocm_aiter_paged_attn triton_decode_attention triton_flash_attention triton_merge_attn_states triton_unified_attention utils utils fa_utils kv_sharing_utils benchmarks benchmarks datasets latency serve throughput lib lib endpoint_request_func ready_checker utils compilation compilation activation_quant_fusion backends base_static_graph collective_fusion compiler_interface counter cuda_graph cuda_piecewise_backend decorators fix_functionalization fusion fusion_attn fx_utils inductor_pass monitor multi_output_match noop_elimination pass_manager sequence_parallelism torch25_custom_graph_pass vllm_inductor_pass wrapper config config cache compilation kv_events kv_transfer load lora parallel scheduler utils core core block_manager evictor interfaces placeholder_block_space_manager scheduler block block block_table common cpu_gpu_block_allocator interfaces naive_block prefix_caching_block utils device_allocator device_allocator cumem distributed distributed communication_op kv_events parallel_state tpu_distributed_utils utils device_communicators device_communicators all2all all_reduce_utils base_device_communicator cpu_communicator cuda_communicator cuda_wrapper custom_all_reduce pynccl pynccl_wrapper quick_all_reduce ray_communicator shm_broadcast shm_object_storage symm_mem tpu_communicator xpu_communicator eplb eplb eplb_state rebalance_algo rebalance_execute kv_transfer kv_transfer kv_transfer_state kv_connector kv_connector base factory utils v1 v1 base lmcache_connector multi_connector nixl_connector shared_storage_connector p2p p2p p2p_nccl_connector p2p_nccl_engine tensor_memory_pool kv_lookup_buffer kv_lookup_buffer base mooncake_store simple_buffer kv_pipe kv_pipe base mooncake_pipe pynccl_pipe engine engine arg_utils async_llm_engine async_timeout llm_engine metrics metrics_types protocol multiprocessing multiprocessing client engine output_processor output_processor interfaces single_step stop_checker util entrypoints entrypoints api_server chat_utils constants context harmony_utils launcher llm logger renderer score_utils ssl tool tool_server utils cli cli collect_env main openai run_batch serve types benchmark benchmark base latency main serve throughput openai openai api_server cli_args logits_processors protocol run_batch serving_chat serving_classification serving_completion serving_embedding serving_engine serving_models serving_pooling serving_responses serving_score serving_tokenization serving_transcription speech_to_text tool_parsers tool_parsers abstract_tool_parser deepseekv3_tool_parser deepseekv31_tool_parser glm4_moe_tool_parser granite_20b_fc_tool_parser granite_tool_parser hermes_tool_parser hunyuan_a13b_tool_parser internlm2_tool_parser jamba_tool_parser kimi_k2_tool_parser llama4_pythonic_tool_parser llama_tool_parser minimax_tool_parser mistral_tool_parser openai_tool_parser phi4mini_tool_parser pythonic_tool_parser qwen3coder_tool_parser seed_oss_tool_parser step3_tool_parser utils xlam_tool_parser executor executor executor_base mp_distributed_executor msgspec_utils multiproc_worker_utils ray_distributed_executor ray_utils uniproc_executor inputs inputs data parse preprocess registry logging_utils logging_utils dump_input formatter lora lora lora models peft_helper request resolver utils worker_manager layers layers base base_linear column_parallel_linear logits_processor qkv_x_parallel_linear replicated_linear row_parallel_linear utils vocal_parallel_embedding ops ops ipex_ops ipex_ops lora_ops torch_ops torch_ops lora_ops triton_ops triton_ops kernel_utils lora_expand_op lora_kernel_metadata lora_shrink_op utils xla_ops xla_ops lora_ops punica_wrapper punica_wrapper punica_base punica_cpu punica_gpu punica_selector punica_tpu punica_xpu utils model_executor model_executor custom_op parameter sampling_metadata utils layers layers activation attention_layer_base layernorm lightning_attn linear logits_processor mla pooler resampler sampler utils vocab_parallel_embedding fla fla ops ops chunk chunk_delta_h chunk_o chunk_scaled_dot_kkt cumsum fused_recurrent index l2norm layernorm_guard op solve_tril utils wy_fast fused_moe fused_moe batched_deep_gemm_moe batched_triton_or_deep_gemm_moe config cpu_fused_moe cutlass_moe deep_gemm_moe deep_gemm_utils deepep_ht_prepare_finalize deepep_ll_prepare_finalize flashinfer_cutlass_moe flashinfer_cutlass_prepare_finalize fused_batched_moe fused_marlin_moe fused_moe gpt_oss_triton_kernels_moe layer modular_kernel moe_align_block_size moe_pallas moe_permute_unpermute moe_torch_iterative pplx_prepare_finalize prepare_finalize rocm_aiter_fused_moe routing_simulator topk_weight_and_reduce triton_deep_gemm_moe trtllm_moe utils mamba mamba abstract linear_attn mamba2_metadata mamba_mixer mamba_mixer2 mamba_utils short_conv ops ops causal_conv1d layernorm_gated mamba_ssm ssd_bmm ssd_chunk_scan ssd_chunk_state ssd_combined ssd_state_passing quantization quantization auto_round awq awq_marlin awq_triton base_config bitblas bitsandbytes deepgemm deepspeedfp experts_int8 fbgemm_fp8 fp8 gguf gptq gptq_bitblas gptq_marlin gptq_marlin_24 hqq_marlin inc input_quant_fp8 ipex_quant kv_cache modelopt moe_wna16 mxfp4 petit ptpc_fp8 rtn schema torchao tpu_int8 compressed_tensors compressed_tensors compressed_tensors compressed_tensors_moe triton_scaled_mm utils schemes schemes compressed_tensors_24 compressed_tensors_scheme compressed_tensors_w4a4_nvfp4 compressed_tensors_w4a8_fp8 compressed_tensors_w4a8_int compressed_tensors_w4a16_24 compressed_tensors_w4a16_nvfp4 compressed_tensors_w8a8_fp8 compressed_tensors_w8a8_int8 compressed_tensors_w8a16_fp8 compressed_tensors_wNa16 kernels kernels mixed_precision mixed_precision allspark bitblas conch cutlass dynamic_4bit exllama MPLinearKernel machete marlin scaled_mm scaled_mm aiter cpu cutlass ScaledMMLinearKernel triton xla quark quark quark quark_moe utils schemes schemes quark_scheme quark_w4a4_mxfp4 quark_w8a8_fp8 quark_w8a8_int8 utils utils allspark_utils bitblas_utils flashinfer_fp4_moe flashinfer_utils fp8_utils gptq_utils int8_utils layer_utils machete_utils marlin_utils marlin_utils_fp4 marlin_utils_fp8 marlin_utils_test marlin_utils_test_24 mxfp4_utils mxfp8_utils nvfp4_emulation_utils nvfp4_moe_support petit_utils quant_utils w8a8_utils rotary_embedding rotary_embedding base common deepseek_scaling_rope dual_chunk_rope dynamic_ntk_alpha_rope dynamic_ntk_scaling_rope ernie45_vl_rope linear_scaling_rope llama3_rope llama4_vision_rope mrope ntk_scaling_rope phi3_long_rope_scaled_rope yarn_scaling_rope shared_fused_moe shared_fused_moe shared_fused_moe model_loader model_loader base_loader bitsandbytes_loader default_loader dummy_loader gguf_loader runai_streamer_loader sharded_state_loader tensorizer tensorizer_loader tpu utils weight_utils models models adapters aimv2 apertus arcee arctic aria aya_vision baichuan bailing_moe bamba bart bert bert_with_rope blip blip2 bloom chameleon chatglm clip cohere2_vision commandr config constant_size_cache dbrx deepseek deepseek_eagle deepseek_mtp deepseek_v2 deepseek_vl2 donut dots1 ernie45 ernie45_moe ernie45_vl ernie45_vl_moe ernie_mtp exaone exaone4 fairseq2_llama falcon falcon_h1 florence2 fuyu gemma gemma2 gemma3 gemma3_mm gemma3n gemma3n_mm glm glm4 glm4_1v glm4_moe glm4_moe_mtp glm4v gpt2 gpt_bigcode gpt_j gpt_neox gpt_oss granite granite_speech granitemoe granitemoehybrid granitemoeshared gritlm grok1 h2ovl hunyuan_v1 hyperclovax_vision idefics2_vision_model idefics3 interfaces interfaces_base intern_vit internlm2 internlm2_ve interns1 interns1_vit internvl jais jamba jina_vl keye keye_vl1_5 kimi_vl lfm2 llama llama4 llama4_eagle llama_eagle llama_eagle3 llava llava_next llava_next_video llava_onevision mamba mamba2 mamba_cache medusa midashenglm mimo mimo_mtp minicpm minicpm3 minicpm_eagle minicpmo minicpmv minimax_cache minimax_text_01 minimax_vl_01 mistral3 mixtral mllama mllama4 mlp_speculator modernbert module_mapping molmo moonvit motif mpt nano_nemotron_vl nemotron nemotron_h nemotron_nas nemotron_vl nvlm_d olmo olmo2 olmoe opt orion ovis ovis2_5 paligemma persimmon phi phi3 phi3v phi4_multimodal phi4flash phi4mm phi4mm_audio phi4mm_utils phimoe pixtral plamo2 qwen qwen2 qwen2_5_omni_thinker qwen2_5_vl qwen2_audio qwen2_moe qwen2_rm qwen2_vl qwen3 qwen3_moe qwen3_next qwen3_next_mtp qwen_vl registry roberta rvl seed_oss siglip siglip2navit skyworkr1v smolvlm solar stablelm starcoder2 step3_text step3_vl swin tarsier telechat2 teleflm terratorch transformers ultravox utils vision voxtral whisper zamba2 warmup warmup deep_gemm_warmup kernel_warmup multimodal multimodal audio base cache hasher image inputs parse processing profiling registry utils video platforms platforms cpu cuda interface rocm tpu xpu plugins plugins io_processors io_processors interface lora_resolvers lora_resolvers filesystem_resolver profiler profiler layerwise_profile utils ray ray lazy_utils ray_env reasoning reasoning abs_reasoning_parsers deepseek_r1_reasoning_parser glm4_moe_reasoning_parser gptoss_reasoning_parser granite_reasoning_parser hunyuan_a13b_reasoning_parser mistral_reasoning_parser qwen3_reasoning_parser step3_reasoning_parser transformers_utils transformers_utils config config_parser_base detokenizer detokenizer_utils dynamic_module processor runai_utils s3_utils tokenizer tokenizer_base tokenizer_group utils chat_templates chat_templates registry configs configs arctic chatglm deepseek_vl2 eagle falcon jais kimi_vl medusa midashenglm mistral mlp_speculator moonvit nemotron nemotron_h nemotron_vl ovis qwen3_next step3_vl ultravox speculators speculators algos base processors processors deepseek_vl2 ovis ovis2_5 tokenizers tokenizers mistral triton_utils triton_utils importing usage usage usage_lib utils utils deep_gemm flashinfer jsontree tensor_schema v1 v1 cudagraph_dispatcher kv_cache_interface outputs request serial_utils utils attention attention backends backends cpu_attn flash_attn flashinfer flex_attention gdn_attn linear_attn mamba1_attn mamba2_attn mamba_attn pallas rocm_aiter_fa short_conv_attn tree_attn triton_attn utils xformers mla mla common cutlass_mla flashattn_mla flashinfer_mla flashmla rocm_aiter_mla triton_mla core core block_pool encoder_cache_manager kv_cache_coordinator kv_cache_manager kv_cache_utils single_type_kv_cache_manager sched sched async_scheduler interface output request_queue scheduler utils engine engine async_llm coordinator core core_client detokenizer exceptions llm_engine logprobs output_processor parallel_sampling processor utils executor executor abstract multiproc_executor ray_distributed_executor utils metrics metrics loggers prometheus ray_wrappers reader stats pool pool metadata sample sample metadata rejection_sampler sampler logits_processor logits_processor builtin interface state ops ops bad_words logprobs penalties topk_topp_sampler tpu tpu metadata sampler spec_decode spec_decode eagle medusa metadata metrics ngram_proposer utils structured_output structured_output backend_guidance backend_lm_format_enforcer backend_outlines backend_types backend_xgrammar request utils worker worker block_table cpu_model_runner cpu_worker gpu_input_batch gpu_model_runner gpu_worker kv_connector_model_runner_mixin lora_model_runner_mixin tpu_input_batch tpu_model_runner tpu_worker utils worker_base xpu_model_runner xpu_worker worker worker cache_engine enc_dec_model_runner model_runner model_runner_base utils worker worker_base CLI Reference CLI Reference vllm serve vllm chat vllm complete vllm run-batch vllm bench vllm bench vllm bench latency vllm bench serve vllm bench throughput Community Community Contact Us Meetups Sponsors Blog Forum Slack GGUF ¶ Warning Please note that GGUF support in vLLM is highly experimental and under-optimized at the moment, it might be incompatible with other features. Currently, you can use GGUF as a way to reduce memory footprint. If you encounter any issues, please report them to the vLLM team. Warning Currently, vllm only supports loading single-file GGUF models. If you have a multi-files GGUF model, you can use gguf-split tool to merge them to a single-file model. To run a GGUF model with vLLM, you can download and use the local GGUF model from TheBloke/TinyLlama-1.1B-Chat-v1.0-GGUF with the following command: wget https://huggingface.co/TheBloke/TinyLlama-1.1B-Chat-v1.0-GGUF/resolve/main/tinyllama-1.1b-chat-v1.0.Q4_K_M.gguf # We recommend using the tokenizer from base model to avoid long-time and buggy tokenizer conversion. vllm serve ./tinyllama-1.1b-chat-v1.0.Q4_K_M.gguf \\ --tokenizer TinyLlama/TinyLlama-1.1B-Chat-v1.0 You can also add --tensor-parallel-size 2 to enable tensor parallelism inference with 2 GPUs: # We recommend using the tokenizer from base model to avoid long-time and buggy tokenizer conversion. vllm serve ./tinyllama-1.1b-chat-v1.0.Q4_K_M.gguf \\ --tokenizer TinyLlama/TinyLlama-1.1B-Chat-v1.0 \\ --tensor-parallel-size 2 Warning We recommend using the tokenizer from base model instead of GGUF model. Because the tokenizer conversion from GGUF is time-consuming and unstable, especially for some models with large vocab size. GGUF assumes that huggingface can convert the metadata to a config file. In case huggingface doesn't support your model you can manually create a config and pass it as hf-config-path # If you model is not supported by huggingface you can manually provide a huggingface compatible config path vllm serve ./tinyllama-1.1b-chat-v1.0.Q4_K_M.gguf \\ --tokenizer TinyLlama/TinyLlama-1.1B-Chat-v1.0 \\ --hf-config-path Tinyllama/TInyLlama-1.1B-Chat-v1.0 You can also use the GGUF model directly through the LLM entrypoint: Code from vllm import LLM , SamplingParams # In this script, we demonstrate how to pass input to the chat method: conversation = [ { \"role\" : \"system\" , \"content\" : \"You are a helpful assistant\" }, { \"role\" : \"user\" , \"content\" : \"Hello\" }, { \"role\" : \"assistant\" , \"content\" : \"Hello! How can I assist you today?\" }, { \"role\" : \"user\" , \"content\" : \"Write an essay about the importance of higher education.\" , }, ] # Create a sampling params object. sampling_params = SamplingParams ( temperature = 0.8 , top_p = 0.95 ) # Create an LLM. llm = LLM ( model = \"./tinyllama-1.1b-chat-v1.0.Q4_K_M.gguf\" , tokenizer = \"TinyLlama/TinyLlama-1.1B-Chat-v1.0\" ) # Generate texts from the prompts. The output is a list of RequestOutput objects # that contain the prompt, generated text, and other information. outputs = llm . chat ( conversation , sampling_params ) # Print the outputs. for output in outputs : prompt = output . prompt generated_text = output . outputs [ 0 ] . text print ( f \"Prompt: { prompt !r} , Generated text: { generated_text !r} \" ) July 8, 2025 Back to top Made with Material for MkDocs ",
      "fetch_method": "direct-html"
    }
  ]
}