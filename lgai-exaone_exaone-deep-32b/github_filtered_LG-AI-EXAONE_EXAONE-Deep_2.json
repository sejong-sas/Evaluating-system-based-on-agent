{
  "1-5 (Architecture)": "The available quotes describe LG AI Research’s EXAONE Deep as a family of models that is released in three discrete parameter scales—2.4 billion, 7.8 billion, and 32 billion parameters. According to the first quote, “We introduce EXAONE Deep, which exhibits superior capabilities in various reasoning tasks including math and coding benchmarks, ranging from 2.4B to 32B parameters developed and released by LG AI Research.” This explicitly tells us that the architecture is deployed in multiple sizes that span an order of magnitude. The second quote elaborates on how each of these model sizes performs relative to other systems: “Evaluation results show that 1) EXAONE Deep 2.4B outperforms other models of comparable size, 2) EXAONE Deep 7.8B outperforms not only open-weight models of comparable scale but also a proprietary reasoning model OpenAI o1-mini, and 3) EXAONE Deep 32B demonstrates competitive performance against leading open-weight models.” Taken together, the quotes indicate a tiered architectural design philosophy where each successive parameter budget aims to deliver incrementally higher competency in reasoning, math, and coding tasks. While no layer counts, hidden sizes, or attention-head configurations are disclosed, the textual evidence makes clear that the architecture is intentionally scalable (2.4 B → 7.8 B → 32 B) and optimized for competitive reasoning benchmarks across all three sizes.",
  "1-5 (Architecture)__evidence": [
    {
      "source": "[readme]",
      "quote": "We introduce EXAONE Deep, which exhibits superior capabilities in various reasoning tasks including math and coding benchmarks, ranging from 2.4B to 32B parameters developed and released by LG AI Research."
    },
    {
      "source": "[readme]",
      "quote": "Evaluation results show that 1) EXAONE Deep 2.4B outperforms other models of comparable size, 2) EXAONE Deep 7.8B outperforms not only open-weight models of comparable scale but also a proprietary reasoning model OpenAI o1-mini, and 3) EXAONE Deep 32B demonstrates competitive performance against leading open-weight models."
    }
  ],
  "1-6 (Tokenizer)": "",
  "1-6 (Tokenizer)__evidence": [],
  "2-1 (Hardware)": "",
  "2-1 (Hardware)__evidence": [],
  "2-2 (Software)": "",
  "2-2 (Software)__evidence": []
}