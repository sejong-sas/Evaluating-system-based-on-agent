{
  "1-5 (Architecture)": "For the lgai-exaone/exaone-deep-32b model the only architecture‐level details available in the supplied material come from a single description that explicitly mentions the target model. According to that passage, the model is described as a “reasoning 32B language model.” Concretely, it is built with 30.95 billion trainable parameters when embeddings are excluded. The transformer stack is 64 layers deep. Attention uses a grouped-query arrangement in which each layer has 40 query heads paired with 8 key/value heads, indicating a GQA (Grouped-Query Attention) configuration. The vocabulary is fixed at 102 400 tokens, and the maximum context window that the architecture can attend over is 32 768 tokens. No further information about hidden-state dimensionality, MLP expansion factor, rotary or positional embeddings, activation functions, or other architectural hyperparameters is provided in the quotations.",
  "1-5 (Architecture)__evidence": [
    {
      "source": "[readme]",
      "quote": "This repository contains the reasoning 32B language model with the following features:\n\n- Number of Parameters (without embeddings): 30.95B\n- Number of Layers: 64\n- Number of Attention Heads: GQA with 40 Q-heads and 8 KV-heads\n- Vocab Size: 102,400\n- Context Length: 32,768 tokens"
    }
  ],
  "1-6 (Tokenizer)": "Only two tokenizer-relevant facts are given for lgai-exaone/exaone-deep-32b: (1) the tokenizer supports a vocabulary of exactly 102 400 discrete tokens, and (2) sequence lengths of up to 32 768 tokens are accommodated. The excerpt does not state the tokenization algorithm (e.g., BPE, Unigram, WordPiece), whether it is case-sensitive, nor whether the tokenizer files are released for download. No additional information about special tokens, byte-fallback behavior, or pre-tokenization rules is present in the provided quotes.",
  "1-6 (Tokenizer)__evidence": [
    {
      "source": "[readme]",
      "quote": "This repository contains the reasoning 32B language model with the following features:\n\n- Number of Parameters (without embeddings): 30.95B\n- Number of Layers: 64\n- Number of Attention Heads: GQA with 40 Q-heads and 8 KV-heads\n- Vocab Size: 102,400\n- Context Length: 32,768 tokens"
    }
  ],
  "2-1 (Hardware)": "The provided quotations contain no references to the type, quantity, or configuration of compute hardware (GPUs, TPUs, CPU clusters, number of accelerator hours, or total FLOP budget) used to train lgai-exaone/exaone-deep-32b.",
  "2-1 (Hardware)__evidence": [],
  "2-2 (Software)": "The cited material gives no information about the software environment used to train lgai-exaone/exaone-deep-32b—there are no mentions of deep-learning frameworks, library versions, optimization settings, or training-time flags in the supplied quotes.",
  "2-2 (Software)__evidence": []
}