{
  "model_id": "LGAI-EXAONE/EXAONE-3.5-32B-Instruct",
  "full_texts": [
    {
      "arxiv_id": "2412.04862",
      "full_text": "EXAONE 3.5:\nSeries of Large Language Models for Real-world Use Cases\nLG AI Research∗\nAbstract\nThis technical report introduces the EXAONE 3.5 instruction-tuned language models, developed and\nreleased by LG AI Research. The EXAONE 3.5 language models are offered in three configurations:\n32B, 7.8B, and 2.4B. These models feature several standout capabilities: 1) exceptional instruction\nfollowing capabilities in real-world scenarios, achieving the highest scores across seven benchmarks,\n2) outstanding long-context comprehension, attaining the top performance in four benchmarks, and\n3) competitive results compared to state-of-the-art open models of similar sizes across nine general\nbenchmarks. The EXAONE 3.5 language models are open to anyone for research purposes and can\nbe downloaded from https://huggingface.co/LGAI-EXAONE. For commercial use, please reach\nout to the official contact point of LG AI Research: contact_us@lgresearch.ai.\n1\nIntroduction\nEXAONE 3.0 instruction-tuned large language model with 7.8B parameters [41] demonstrated strong bilingual\ncapabilities in Korean and English with exceptional real-world performance and instruction-following proficiency.\nSince its release, we have received diverse feedback from both academic and industrial communities. For instance,\nacademic researchers have emphasized the need for smaller models that can be trained and deployed on low-specification\nGPUs due to limited access to advanced computational infrastructure. The industry has expressed strong demand for\nlarger models with enhanced performance that remain cost-effective, as well as smaller models suitable for on-device\ndeployment. Additionally, with the increasing adoption of retrieval-augmented generation (RAG) techniques, which\ngenerate answers based on reference documents or web search results, there has been substantial demand for models\ncapable of effectively handling longer contexts.\nIn this report, we present EXAONE 3.5 language models, a collection of instruction-tuned language models ranging\nfrom 2.4B to 32B parameters, developed to meet the diverse needs of users. EXAONE 3.5 language models include:\n1) 2.4B model optimized for deployment on small or resource-constrained devices, 2) 7.8B model matching the size\nof its predecessor but offering improved performance, and 3) 32B model delivering exceptional performance. All\nmodels support long-context processing of up to 32K tokens. Each model demonstrates state-of-the-art performance in\nreal-world use cases and long-context handling, while remaining competitive in general domains compared to recently\nreleased models of similar sizes.\nWith the release of the EXAONE 3.5 language models, we hope to support researchers to push the boundaries of\ngenerative AI and inspire the development of innovative applications that enhance human life. This is in line with the\nmission of LG AI Research: ADVANCING AI FOR A BETTER LIFE.\n2\nModel Training\nThis section describes the detailed information on model configurations and the methods used for pre-training and\npost-training phases, along with the dataset construction process for each training phase.\n∗The complete list of authors who contributed to this work can be found in Appendix A.\narXiv:2412.04862v2  [cs.CL]  9 Dec 2024\n\nModel size\n32B\n7.8B\n2.4B\nd_model\n5,120\n4,096\n2,560\nNumber of layers\n64\n32\n30\nPre-normalization\nTrue\nTrue\nTrue\nNon-linearity\nSwiGLU [44]\nSwiGLU\nSwiGLU\nFeedforward dimension\n27,392\n14,336\n7,168\nHead type\nGQA [3]\nGQA\nGQA\nNumber of heads\n40\n32\n32\nNumber of KV heads\n8\n8\n8\nHead size\n128\n128\n80\nMax sequence length\n32,768\n32,768\n32,768\nRoPE theta [46]\n1,000,000\n1,000,000\n1,000,000\nTokenizer\nBBPE [51]\nBBPE\nBBPE\nVocab size\n102,400\n102,400\n102,400\nTied word embedding\nFalse\nFalse\nTrue\nTable 1: Configurations of EXAONE 3.5 language models.\n2.1\nModel Configurations\nThe EXAONE 3.5 language models are based on the latest decoder-only Transformer architecture, and detailed\nconfigurations are described in Table 1. These models are identical in structure to the EXAONE 3.0 7.8B model but\nmainly differ in their configurations related to sizes. Notably, the EXAONE 3.5 language models extend the maximum\ncontext length from 4,096 tokens in EXAONE 3.0 to 32,768 tokens by adopting the long-context fine-tuning [7]. All\nthree models share the same vocabulary, which consists roughly of 50% Korean and 50% English.\n2.2\nPre-training\nThe amount of pre-training corpus data and computational resources are shown in Table 2. The approach to data\nconstruction and model training consists of two stages: 1) we perform first-stage pre-training based on the large training\ncorpus, which is collected and processed from as diverse sources as possible aimed to increase the performance on\ngeneral domains. After that, 2) we collect more data for the domains that need to be strengthened through evaluations\nand conduct second-stage of pre-training. For instance, we focus on enhancing long-context understanding capabilities\nin the second-stage.\nModel size\n32B\n7.8B\n2.4B\nTraining tokens\n6.5T\n9T\n6.5T\nAmount of computation (FLOPs)\n1.25 × 1024\n4.21 × 1023\n9.36 × 1022\nTable 2: The sizes of the training data corpus along with the amounts of computation to build EXAONE 3.5 language\nmodels.\n2.2.1\nContext Length Extension\nTo extend the context length, we utilize the long-context fine-tuning technique [7]. To mitigate the catastrophic forgetting\nproblem [32], where the model forgets what it learned during the first pre-training stage, a replay-based method [2] is\napplied. Specifically, during the second-stage pre-training, we reuse a portion of the data used in the first-stage. While\ndocuments exceeding the maximum context length is split into smaller chunks in the first-stage, the original corpus are\ntrained without being divided into chunks in the second-stage to extend the models’ context length.\n2.2.2\nDecontamination\nBy the nature of massively web-crawled corpus, test-set examples often appear in the training corpus [43, 55]. These\ncontaminated examples are likely to harm generalization performance and confuse test metrics, thus presenting unfair\nevaluations to users. To prevent the contaminated examples undermine the generalization performance of EXAONE\n2\n\n3.5 language models, we rigorously apply a decontamination process for all targeted benchmark test data and remove\ncontaminated examples from the training pipeline.\nWe borrow a simple yet powerful substring-level matching method [36] with stricter criteria. The entire decontamination\nprocess is described in Figure 4 in Appendix C. We first normalize all test-set examples by removing all other characters\nexcept alphabets and numbers, then we extract all unique substrings with sliding window size S = 50 and a stride\nof 1. To determine whether a training example is contaminated, we randomly sample N = 10 substrings from the\nnormalized training example and check if they exist in the substring pools. Table 10 in Appendix C provides examples\nof documents found in web corpora considered as contaminated.\n2.2.3\nTraining Cost\nConsidering the computational cost of pre-training a large language model (LLM), it is necessary to make the training\nefficient by achieving as much high performance as possible with limited resources. Table 3 compares the total amounts\nof computations required for pre-training between the EXAONE 3.5 32B language model and others of similar size.\nWhen we simply approximate the total amounts of computations as the product of the model size and the number of\ntraining tokens [19, 24], Qwen 2.5 32B, for example, requires 2.77 times more computations than EXAONE 3.5 32B.\nOne of the noticeable characteristics of the EXAONE 3.5 language models is that they demonstrate high performance\ndespite being trained at lower costs than the other baseline models (see Section 3).\nModels\nModel size\nTraining tokens\nAmount of computation (ratio)\nEXAONE 3.5\n32B\n6.5T\n1.00\nQwen 2.5\n32B\n18T\n2.77\nGemma 2\n27B\n13T\n1.69\nYi 1.5\n34B\n3.6T\n0.59\nTable 3: Comparison of the total amounts of computations to build models. We approximate the amount of computations\nas the product of the model size and the number of training tokens. Although the EXAONE 3.5 32B model is behind in\nthe computations compared to Qwen 2.5 and Gemma 2, it has shown competitive performances.\n2.3\nPost-training\nAfter pre-training, models go through further processes for strengthening their instruction-following capabilities\nand aligning with human preferences, which are well known as supervised fine-tuning (SFT) [53] and preference\noptimization.\n2.3.1\nSupervised Fine-tuning\nTo perform well on new or unseen instructions, a model needs to be trained on pairs of instruction-response datasets\nwith varying difficulty from different domains. Hence, in order to build training data covering a wide range of fields,\nwe extract core knowledge from 8M web corpora using a taxonomic system, as shown in Figure 1. We then generate\nan instruction-tuning dataset based on the extracted knowledge taxonomy. Finally, leveraging an instruction evolution\nmethod, which stems from the method proposed in [58], we diversify the complexity levels so that instructions with\nvarious complexities and difficulties can be produced.\n2.3.2\nPreference Optimization\nDirect alignment algorithms (DAAs) [38], such as DPO [39] and SimPO [33], are used to train models after supervised\nfine-tuning to align models with human preferences. We create preference data for training using synthetic data and\npre-collected data. For response generation, we sample N responses from multiple models for the prompt x drawn\nfrom the preference data and select the best response as yw and the worst response as yl based on the scores of a\nreward model to create a preference data, {x, yw, yl}. To validate preference data, we use an additional reward model\nto calculate agreement based on the rankings of the two reward models and filter out data with agreement below the\nthreshold. Our preference optimization comprises multiple stages to sequentially train models M1 and M2 through\nDAAs, where M0 is initialized from the SFT model. The staged pipeline enables us to mitigate over-optimization [38]\nthat may occur during the DAAs’ training process. Figure 2 shows a schematic diagram for constructing our preference\ndataset and training process.\n3\n\nCreate a General \nDifficulty Instruction\nIs the following statement true? \nThe next term in the arithmetic sequence\n -8, -14, -20, -26 is -32.\nHow did the advent of compilation albums\nalter the perception and dissemination of\njazz music throughout its history?\nSummarize the key structural components\nthat differentiate bacteria from eukaryotic\ncells, and explain how these differences\ncontribute to the functional capabilities of\nbacteria in various environments.\nWeb Corpus\nExtracting \nKnowledge Taxonomies\nDomain: Math\nAlgebra\nExponential functions\n Arithmetic sequence\nDomain: Arts\nMusic\nJazz history\n Compilation albums, recording \nDomain: Natural Sciences\nBiology\nBacteria and archaea\nProkaryote structure \nEvolving Instruction\nHow did the sociopolitical dynamics of 1950s\nAmerica influence the arrangement and\nreception of songs in compilation albums,\nspecifically in jazz, and how did these albums\nsubsequently impact the cultural identity\nformation of their primary audiences?\nA 10-digit number is formed using each of the\ndigits from 0 to 9 exactly once. The first digit\nis not 0, and the sum of every pair of\nconsecutive digits is a prime number. What is\nthis number?\nExplain the unique biochemical pathways in\nextremophilic archaea that facilitate their\nsurvival \nand \nmetabolic \nfunctions \nin\nhyperthermal environments, including any\nnovel enzymes that are not found in bacteria\nor eukaryotes.\nFigure 1: A procedure of instruction-tuning data construction. First, we extract the core knowledge from large-volume\nweb corpora and classify it within the taxonomy we defined in advance. Next, instruction-tuning data is generated based\non the knowledge. To construct additional training data that is more complex, we leverage an instruction-evolving\nmethod [58] that lets the final dataset cover various fields with varying levels of difficulty.\nModel\nyy\nReward\nModel\nyy\nCollected \nPrompts\nGenerate\nResponses\nSelect\nPreference\nData\nPreference Data Creation\nPreference Optimization\nPreference \nData\nSFT Model\nDAA\nTraining\nDAA\nTraining\nFigure 2: Overview of the preference optimization pipeline. (Top) Preference Data Creation: It shows the process of\nconstructing preference data {x, yw, yl} by scoring the responses y generated from a model for the prompt x using\na reward model. (Bottom) Preference Optimization: Sequential training process where M0 initialized from the SFT\nmodel is trained through DAA to obtain M1 and M2.\n2.4\nData Compliance\nDeveloping AI models requires a large amount of data, and the acquisition and utilization of this data can lead to various\nlegal issues, such as copyright infringement, intellectual property infringement, and personal information protection\nviolations. To minimize these risks, LG AI Research conducts AI Compliance reviews throughout the entire process\nof data collection, AI model training, and information provision. For more detailed information, please refer to the\nEXAONE 3.0 Technical Report [41] and the LG AI Ethics Principles [28].\n3\nEvaluation\nThis section presents the evaluation settings and results of EXAONE 3.5 language models on various benchmark\ndatasets. We select recently released open language models for baselines of our models to compare our performances\non the benchmarks. All baselines and their detailed information are described in Appendix D.1.\n3.1\nBenchmarks\nConsidering the diverse nature of user intents, it is crucial for an instruction-tuned model to generate a response aligned\nto the user’s query, whatever it is. To evaluate our models in comprehensive and various scenarios, we select over a\n4\n\ndozen evaluating benchmarks along with a few in-house benchmarks. Table 4 summarizes all benchmarks, which can\nbe grouped into three categories:\n• Real-world Use Cases (Section 3.3): the benchmarks requiring the ability to understand and perform diverse\nuser instructions.\n• Long Context (Section 3.4): the benchmarks evaluating the ability to understand the long context.\n• General Domain (Section 3.5): the benchmarks embracing general domain abilities that LLMs are expected to\nhave. Specifically, this category includes benchmarks for measuring the ability to solve mathematical problems,\nthe ability to write source codes, and the parametric knowledge embedded in an LLM.\nCategory\nBenchmark\nLang\nEvaluation Settings\nMetric\nReal-world Use Cases\nMT-Bench [59]\nEN\nLLM-as-a-judge (judge: gpt-4o-2024-08-06)1\nLLM score\nLiveBench [54] (v2024-08-31)\nEN\nGround-truth match\nAccuracy\nArena-Hard-v0.1 [29]\nEN\nLLM-as-a-judge (judge: gpt-4-1106-preview)\nWin rate\nAlpacaEval 2.0 LC [12]\nEN\nLLM-as-a-judge (judge: gpt-4-1106-preview)\nWin rate\nIFEval[61]\nEN\nPrompt-level / strict accuracy\nAccuracy\nKoMT-Bench [42]\nKO\nLLM-as-a-judge (judge: gpt-4o-2024-08-06)\nLLM score\nLogicKor [37]\nKO\nLLM-as-a-judge (judge: gpt-4-1106-preview)\nLLM score\nLong Context\nNeedle-In-A-Haystack [23]\nEN/KO\nGround-truth match\nAccuracy\nLongBench [5]\nEN\nGround-truth match\nF1, Rouge\nLongRAG [21] (extended)\nEN\nLLM-as-a-judge (judge: gpt-4o-2024-08-06)\nLLM score\nKo-LongRAG (In-house)\nKO\nLLM-as-a-judge (judge: gpt-4o-2024-08-06)\nLLM score\nKo-WebRAG (In-house)\nKO\nLLM-as-a-judge (judge: gpt-4o-2024-08-06)\nLLM score\nGeneral Domain\nGSM8K [9]\nEN\n0-shot / CoT\nAccuracy\nMATH [17, 27]\nEN\n0-shot / CoT\nAccuracy\nHumanEval [6]\nEN\n0-shot\npass@1\nMBPP [4]\nEN\n0-shot (Evalplus base)2\npass@1\nGPQA [40]\nEN\n0-shot / CoT\nAccuracy\nARC-C [8]\nEN\n0-shot\nAccuracy\nBBH [47]\nEN\n0-shot / CoT\nAccuracy\nMMLU [16]\nEN\n0-shot / CoT\nAccuracy\nKMMLU [45]\nKO\n0-shot / CoT\nAccuracy\nTable 4: The benchmarks used to evaluate the performance of EXAONE 3.5 language models along with their target\nlanguages, evaluation settings, and the metrics. LONGRAG is extended from the original, and KO-LONGRAG and\nKO-WEBRAG are in-house benchmarks (see Section 3.4).\n3.2\nOverall Performance\nThe results of overall performance against three categories are presented in Table 5. Our EXAONE 3.5 language models,\nwith sizes 32B and 7.8B, perform best in Real-world Use Cases and Long Context categories compared to baseline\nmodels while showing competitive results in the General Domain category. Our smallest model, EXAONE 3.5 2.4B,\noutperforms baselines with similar sizes in all three categories, demonstrating strong performance. Surprisingly, our\n2.4B model, despite its small size, has shown better performance compared to baselines even with a larger size (< 9B)\nexcept for Qwen 2.5 7B in General Domain. Considering the recent surge in demand for smaller large language models\n(sLLM) [52], we believe that our EXAONE 3.5 2.4B model is well-positioned to be highly competitive in both academic\nand industrial use.\nIn the following sections, we elaborate on detailed evaluation settings and the results for each category.\n3.3\nReal-world Use Cases\nFor the Real-world Use Cases category, we have compiled seven benchmarks that represent real-world queries users\nmight submit to a chatbot model. In MT-BENCH, KOMT-BENCH, and LOGICKOR, models’ responses consisting of\nmulti-turns are evaluated by a judge model. For ARENA-HARD and ALPACAEVAL, responses of a target language\n1The separability of the original GPT-4 judge results is notably low, prompting the adoption of gpt-4o-2024-08-06 as judge.\n2We choose the MBPP base from EvalPlus [31], which is a subset of the original and consists of refined, high-quality problems.\n5\n\nModels\nReal-world Use Cases\nLong Context\nGeneral Domain\nEXAONE 3.5 32B\n74.3\n71.1\n74.8\nQwen 2.5 32B [49]\n69.8\n66.9\n78.7\nC4AI Command R 32B [10]\n46.0\n63.4\n56.8\nGemma 2 27B [48]\n64.2\n-\n68.7\nYi 1.5 34B [2]\n46.9\n-\n53.9\nEXAONE 3.5 7.8B\n70.7\n66.6\n70.2\nQwen 2.5 7B [49]\n52.7\n56.1\n71.0\nLlama 3.1 8B [15]\n48.6\n58.8\n62.4\nGemma 2 9B [48]\n57.9\n-\n62.9\nPhi 3 small (7B) [1]\n41.7\n33.4\n63.2\nEXAONE 3.5 2.4B\n61.1\n63.4\n63.3\nQwen 2.5 3B [49]\n44.5\n40.7\n62.1\nQwen 2.5 1.5B [49]\n30.1\n34.5\n47.9\nLlama 3.2 3B [34]\n36.7\n44.2\n54.9\nGemma 2 2B [48]\n41.7\n-\n42.2\nTable 5: Overall comparison results of EXAONE 3.5 language models with similar-sized baseline language models.\nHere, a dash (-) indicates the model does not support context lengths longer than 16K. Bold scores indicate the best\nperformance, and underlined scores mean the second best. The detailed information for each baseline is described in\nAppendix D.1.\nmodel are compared with those of a reference model (gpt-4-0314 and gpt-4-1106-preview, respectively) by a judge\nmodel, recording the win rate. LIVEBENCH (ver. 2024-08-31) and IFEVAL (prompt-strict) assess how well the models’\nresponses align with user instructions by matching them to the ground-truth responses.\nModels\nMT-Bench LiveBench Arena-Hard AlpacaEval\nIFEval\nKoMT-Bench LogicKor\nAverage\nEXAONE 3.5 32B\n8.51\n43.0\n78.6\n60.6\n81.7\n8.05\n9.06\n74.3\nQwen 2.5 32B\n8.49\n50.6\n67.0\n41.0\n78.7\n7.75\n8.89\n69.8\nC4AI Command R 32B\n7.38\n29.7\n17.0\n25.9\n26.1\n6.72\n8.24\n46.0\nGemma 2 27B\n8.28\n40.0\n57.5\n52.2\n59.7\n7.19\n8.56\n64.2\nYi 1.5 34B\n7.64\n26.2\n23.1\n34.8\n55.5\n4.88\n6.33\n46.9\nEXAONE 3.5 7.8B\n8.29\n39.8\n68.7\n54.2\n78.9\n7.96\n9.08\n70.7\nQwen 2.5 7B\n6.48\n35.6\n48.9\n31.7\n72.5\n5.19\n6.38\n52.7\nLlama 3.1 8B\n7.59\n28.3\n27.7\n25.7\n74.5\n4.85\n5.99\n48.6\nGemma 2 9B\n7.64\n32.1\n43.6\n47.3\n54.7\n7.10\n8.05\n57.9\nPhi 3 small (7B)\n7.63\n27.9\n26.8\n29.2\n59.5\n3.22\n3.99\n41.7\nEXAONE 3.5 2.4B\n7.81\n33.0\n48.2\n37.1\n73.6\n7.24\n8.51\n61.1\nQwen 2.5 3B\n7.21\n25.7\n26.4\n17.4\n60.8\n5.68\n5.21\n44.5\nQwen 2.5 1.5B\n5.72\n19.2\n10.6\n8.4\n40.7\n3.87\n3.60\n30.1\nLlama 3.2 3B\n6.94\n24.0\n14.2\n18.7\n70.1\n3.16\n2.86\n36.7\nGemma 2 2B\n7.20\n20.0\n19.1\n29.1\n50.5\n4.83\n5.29\n41.7\nTable 6: Performance comparison results of EXAONE 3.5 language models with similar-sized recently-released\nlanguage models on seven benchmarks representing real-world use case scenarios. When calculating the macro average,\nthe scores of MT-Bench, KoMT-Bench, and LogicKor are multiplied by 10 because they are scored out of 10 and the\nrest are scored out of 100. Bold scores indicate the best performance, and underlined scores mean the second best.\nAs presented in Table 6, our three models have shown the best performance against baselines of similar size in all\nbenchmarks, except for the 32B model in LIVEBENCH. Furthermore, by outperforming others in both English and\nKorean benchmarks, EXAONE 3.5 language models demonstrate their superior bilingual abilities.\n3.4\nLong Context\nThe ability to process and understand long contexts is increasingly important for modern LLMs, as it enables their\napplication in more complex scenarios. To demonstrate EXAONE 3.5 language models’ long context performance,\n6\n\n(a) EXAONE 3.5 32B - ENGLISH\n(b) EXAONE 3.5 32B - KOREAN\n(c) EXAONE 3.5 7.8B - ENGLISH\n(d) EXAONE 3.5 7.8B - KOREAN\n(e) EXAONE 3.5 2.4B - ENGLISH\n(f) EXAONE 3.5 2.4B - KOREAN\n1.0\n0.8\n0.6\n0.4\n0.2\n0.0\nColor-Code based on Score \nDepth Percent\n1k\n8k\n16k\n32k\n4k\n1k\n8k\n16k\n32k\n4k\n1k\n8k\n16k\n32k\n4k\n1k\n8k\n16k\n32k\n4k\n1k\n8k\n16k\n32k\n4k\n1k\n8k\n16k\n32k\n4k\nDepth Percent\n100\n89\n78\n67\n56\n44\n33\n22\n11\n0\n100\n89\n78\n67\n56\n44\n33\n22\n11\n0\nDepth Percent\n100\n89\n78\n67\n56\n44\n33\n22\n11\n0\nDepth Percent\n100\n89\n78\n67\n56\n44\n33\n22\n11\n0\nDepth Percent\n100\n89\n78\n67\n56\n44\n33\n22\n11\n0\nDepth Percent\n100\n89\n78\n67\n56\n44\n33\n22\n11\n0\nToken Limit\nToken Limit\nToken Limit\nToken Limit\nToken Limit\nToken Limit\nFigure 3: NIAH results of EXAONE 3.5 language models. The x-axis represents the token length of the input text, while\nthe y-axis shows the relative position within the text, expressed as a percentage (0% corresponds to the beginning, and\n100% to the end). The results are represented using a color-coded scheme: green indicates successful retrievals, and red\nrepresents unsuccessful ones. EXAONE 3.5 language models achieve near-perfect accuracy in retrieving information\nacross various document depths and context lengths in English and Korean.\nwe evaluate our models using benchmarks designed for a synthetic task with long context inputs, along with various\nretrieval-augmented generation (RAG) benchmarks.\n3.4.1\nNeedle-in-a-Haystack\nNeedle-in-a-Haystack (NIAH) [23] serves as a benchmark to assess how effectively models can locate and retrieve\ninformation hidden at random locations within long documents. We comprehensively evaluate our models’ ability to\nprocess and retrieve information from long contexts, up to 32K tokens. Furthermore, we extend NIAH to Korean and\nemploy it to evaluate our models’ long context processing ability across both English and Korean contexts.\nFigure 3 demonstrates that our models achieve near-perfect accuracy in retrieving targeted information across all tested\ndocument depths and context lengths in both English and Korean. These results highlight their robust long context\nprocessing capabilities, particularly in tasks demanding precise information retrieval and complex reasoning.\n3.4.2\nLong Context Understanding\nTo assess long context understanding capabilities, we evaluate our models using benchmarks including LONGBENCH [5]\nand LONGRAG [21]. We expand unanswerable cases in LongRAG to make it more challenging. We also build KO-\nLONGRAG, the Korean counterpart to LONGRAG, to evaluate long context understanding in Korean. For more realistic\nRAG scenario, requiring answers to difficult questions using actual web-searched results, we constructed KO-WEBRAG\nbenchmark. We refer readers to the Appendix D.2 for more details.\n7\n\nModels\nLongBench\nLongRAG\nKo-LongRAG\nKo-WebRAG\nAverage\nEXAONE 3.5 32B\n49.2\n67.6\n85.3\n82.3\n71.1\nQwen 2.5 32B\n49.1\n63.6\n73.5\n81.3\n66.9\nC4AI Command R 32B\n50.9\n55.3\n72.3\n75.0\n63.4\nGemma 2 27B\n-\n-\n-\n-\n-\nYi 1.5 34B\n-\n-\n-\n-\n-\nEXAONE 3.5 7.8B\n46.0\n68.3\n71.7\n80.3\n66.6\nQwen 2.5 7B\n47.2\n60.1\n55.3\n61.7\n56.1\nLlama 3.1 8B\n44.6\n55.1\n64.8\n70.7\n58.8\nGemma 2 9B\n-\n-\n-\n-\n-\nPhi 3 small (7B)\n40.6\n52.7\n7.7\n32.7\n33.4\nEXAONE 3.5 2.4B\n42.7\n63.3\n74.7\n73.0\n63.4\nQwen 2.5 3B\n42.0\n45.8\n40.5\n34.7\n40.7\nQwen 2.5 1.5B\n37.1\n39.0\n33.8\n28.0\n34.5\nLlama 3.2 3B\n41.7\n45.9\n39.3\n50.0\n44.2\nGemma 2 2B\n-\n-\n-\n-\n-\nTable 7: Performance comparison results of EXAONE 3.5 language models with similar-sized recently released language\nmodels across four benchmarks representing long context scenarios. A dash (-) indicates that the model does not support\ncontext lengths longer than 16K. Context lengths for each model are detailed in Table 11. The average score in the\nrightmost is calculated as a macro average across the benchmarks. Bold scores indicate the best performance, and\nunderlined scores mean the second best.\nAs shown in Table 7, EXAONE 3.5 language models have shown superior performance compared to other models3,\nexcept for the 32B and 7.8B models in LongBench. When averaged across the benchmarks, our three models outperform\nall baselines, confirming their capabilities to process complex, extended contexts effectively.\n3.5\nGeneral Domain\nLanguage models are now expected to achieve human-level capabilities in various general domains, such as solving\nmathematical problems or writing source code programs. To evaluate overall performance in the general domains, we\nselect nine benchmarks in three main domains: 1) GSM8K (CoT) and MATH (CoT) for mathematics, 2) HUMANEVAL\n(Evalplus base) and MBPP (Evalplus base) for coding, and 3) MMLU (CoT), KMMLU (CoT), GPQA (CoT), ARC-C,\nand BBH (CoT) for assessing the amount of knowledge embedded in an LLM.\nTo better simulate the real-world scenarios where a chatbot model usually receives a single query from users, we\nevaluate all benchmarks in the General Domain category using the 0-shot setting. To achieve this, we prompt language\nmodels with instructions that require specific answer formats and parse the final answer from the responses. For a fair\ncomparison, we use the same prompts across all models. We make public all the prompts we used in Appendix D.3 for\ntransparent reproducibility.\nTable 8 shows the results of EXAONE 3.5 language models and their baseline models on the benchmarks in the General\nDomain category. When averaged across the benchmarks, our EXAONE 3.5 language models with sizes 32B and 7.8B\ndemonstrate competitive performance compared to baselines of similar size. The EXAONE 3.5 2.4B model, on the\nother hand, outperforms all baselines in the average score.\n4\nResponsible AI\nEXAONE 3.5 language models were developed in accordance with the Responsible AI Development Framework\nencompassing data governance, ethical considerations, and risk management as it would be made available to a wide\nrange of users. Given the nature of open models – eventually leading to wide use in various domains – we aim to\nmaximize social benefits while ensuring humanity, fairness, safety, accountability, and transparency as mandated by the\nLG AI Ethics Principles [28].\n3Gemma models and Yi 1.5 34B model are excluded from evaluations due to their context limits (≤16k tokens), ensuring fair\ncomparison.\n8\n\nModels\nGSM8K\nMATH HumanEval MBPP\nMMLU KMMLU\nGPQA\nARC-C\nBBH\nAverage\nEXAONE 3.5 32B\n91.9\n70.5\n87.2\n81.8\n78.3\n57.0\n39.7\n91.7\n75.3\n74.8\nQwen 2.5 32B\n92.0\n76.5\n89.0\n88.9\n81.4\n62.1\n40.9\n95.1\n82.7\n78.7\nC4AI Command R 32B\n56.5\n24.3\n68.3\n78.8\n71.1\n41.5\n27.4\n88.0\n55.7\n56.8\nGemma 2 27B\n84.2\n49.4\n79.3\n80.7\n74.8\n53.8\n33.6\n92.9\n69.7\n68.7\nYi 1.5 34B\n83.7\n52.0\n5.5\n35.7\n75.3\n41.7\n30.0\n93.9\n67.6\n53.9\nEXAONE 3.5 7.8B\n87.6\n69.8\n84.2\n79.4\n69.0\n52.4\n32.5\n87.6\n69.7\n70.2\nQwen 2.5 7B\n90.4\n70.4\n82.3\n78.8\n73.1\n49.9\n33.1\n90.6\n70.1\n71.0\nLlama 3.1 8B\n82.1\n48.8\n67.7\n70.6\n72.4\n45.9\n27.4\n83.7\n63.3\n62.4\nGemma 2 9B\n82.0\n44.6\n68.3\n75.1\n73.7\n34.6\n27.9\n90.5\n69.7\n62.9\nPhi 3 small (7B)\n86.3\n47.8\n72.6\n72.0\n68.8\n33.4\n25.3\n90.4\n72.5\n63.2\nEXAONE 3.5 2.4B\n82.5\n60.2\n76.2\n74.3\n60.4\n45.8\n28.4\n79.2\n62.9\n63.3\nQwen 2.5 3B\n84.3\n61.4\n72.6\n72.5\n61.0\n41.7\n25.8\n82.1\n57.3\n62.1\nQwen 2.5 1.5B\n69.8\n48.5\n55.5\n65.6\n48.8\n5.0\n23.1\n72.4\n42.2\n47.9\nLlama 3.2 3B\n77.4\n46.6\n54.9\n60.6\n64.9\n35.0\n23.2\n78.0\n53.8\n54.9\nGemma 2 2B\n29.8\n18.7\n45.7\n55.0\n56.1\n37.4\n22.6\n76.3\n38.2\n42.2\nTable 8: Performance comparison results of EXAONE 3.5 models with similar-sized recently-released language models\non nine benchmarks representing general scenarios. The macro average is used to evaluate the overall performance.\nBold scores indicate the best performance, and underlined scores mean the second best.\n4.1\nBenefits\nEXAONE 3.5 language models are open for research purposes, aiming to advance AI research. Based on the feedback\nwe have received since the release of the EXAONE 3.0 7.8B model, we now offer models of more diverse sizes: 2.4B,\n7.8B, and 32B. This will allow researchers to select an optimal model for their research objectives and computing\nenvironment. We hope that this flexibility will support a wide spectrum, ranging from foundational research to domain-\nspecific applications. It is also expected to contribute positively to the advancement of generative AI, building upon the\nsignificant performance improvements over previous version.\nTo ensure the reliability of the release, we have implemented a standardized data compliance protocol, guaranteeing\nhigh-quality data. This standardized approach provides a trustworthy foundation for researchers to use the model across\nvarious research areas in the future.\nWhile external users can employ EXAONE 3.5 language models in diverse domains, precisely identifying specific user\nneeds has been challenging. To address this, we have conducted extensive reviews of its applicability across a wide\nrange of domains. Additionally, we have collaborated closely with LG affiliates, including business and research teams,\nto better align the model with specific user requirements.\n4.2\nRisks and Mitigations\nOpen models can positively contribute to the AI community, but there are challenges in ensuring responsible use. We\nconducted an AI ethical impact assessment to identify potential risks such as unintended inequality and discrimination\nagainst socially disadvantaged groups, the generation of harmful content, and malicious misuse by users. We have\nadopted various policies and research initiatives to mitigate the potential risks identified through this assessment.\nFirst, on the data side, we conducted a legal risk assessment on all candidate datasets to enhance privacy and security.\nBased on the outcomes, we determined the suitability of each dataset for training and performed a de-identification\nprocess to remove sensitive data from qualified dataset. To minimize bias in the training data and ensure data quality,\nwe documented all pre-processing steps and adopted a standardized data processing protocol. Considering practical\ndifficulties of verifying the representativeness of all data, we conducted a qualitative evaluation of a small sample of\ndata. For a quantitative evaluation, we endeavored to minimize data-related risks by verifying the data subsets through\nperformance evaluation after the model training was completed. Also, we carefully reviewed the open-source libraries\nused in our model development.\nThe levels of AI ethical considerations and regulatory requirements may vary across different user needs and character-\nistics (e.g., country of residence, age, etc.). To address this, we will continue to monitor global AI regulations and take\nimmediate action as needed to avoid potential regulatory violations. A lack of transparency in an AI model’s decision-\nmaking process can reduce trust among users and stakeholders. To address this limitation, we continuously analyze and\n9\n\nevaluate our model’s performance to identify weaknesses and areas for improvement. While fully explaining AI model’s\ndecision-making process remains challenging, we are committed to advancing explainability through ongoing research.\n4.3\nSafety\nWe conducted comprehensive evaluations of EXAONE 3.5 language models’ ethics and security using a third-party\ndataset: Korean Large Language Model Trustworthiness Benchmark Data [35], provided by the Ministry of Science\nand ICT of the Republic of Korea and the National Information Society Agency (NIA). This dataset is specifically\ndesigned to assess the harmlessness of language models. The evaluation results are presented in Table 9. To measure\nthe performance, we asked a model to choose one of five options. If the selected option is included in the set of\ncorrect answers, then it is scored as correct. In the provided dataset, the first two options were labeled “False” and the\nremaining three were labeled “True”. To mitigate potential bias from the order of options, we shuffled the order of\noptions randomly for each evaluation. While the experimental results demonstrated effectiveness in filtering harmful\nreactions, there is still room for improvement.\nCategory\nSubcategory\nTest Cases\nAccuracy\n32B\n7.8B\n2.4B\nBias\nGender & Sexual Orientation\n295\n91.2%\n87.5%\n76.6%\nRace & Ethnicity & Nationality\n432\n86.8%\n85.0%\n72.2%\nPolitical Affiliation\n720\n82.8%\n79.9%\n56.7%\nRegion\n415\n87.7%\n84.6%\n69.2%\nJob\n442\n86.2%\n81.9%\n67.0%\nMiscellaneous\n406\n85.2%\n86.5%\n73.2%\nSubtotal\n2,710\n86.0%\n83.5%\n67.4%\nHate\nGender & Sexual Orientation\n399\n95.2%\n92.2%\n83.5%\nRace & Ethnicity & Nationality\n749\n91.6%\n88.4%\n73.8%\nPolitical Affiliation\n1,164\n85.7%\n83.4%\n66.2%\nRegion\n499\n92.0%\n87.2%\n74.1%\nJob\n852\n91.0%\n87.8%\n72.3%\nSubtotal\n3,663\n90.0%\n86.9%\n72.2%\nIllegal\nIllegal\n1,126\n92.9%\n89.6%\n80.3%\nSensitiveness\nContentious\n710\n83.1%\n86.1%\n79.0%\nEthical\n966\n81.2%\n83.7%\n72.8%\nPredictive\n825\n79.8%\n82.3%\n71.0%\nSubtotal\n2,501\n81.2%\n83.9%\n74.0%\nOverall\n10,000\n87.1%\n85.6%\n72.2%\nTable 9: Evaluation results of EXAONE 3.5 language models on the Korean Large Language Model Trustworthiness\nBenchmark Data [35] to assess the model’s harmlessness. The accuracy is determined by the number of times the model\nselects appropriate options when presented with questions involving various harmful and dangerous categories, such as\nillegal content.\n5\nLimitations\nEXAONE 3.5 language models, like all existing language models, have certain limitations and may occasionally\ngenerate inappropriate responses. The language model generates responses based on the output probability of tokens,\nand it is determined during learning from training data. While we have made every effort to exclude personal, harmful,\nand biased information from the training data, some problematic content may still be included, potentially leading to\nundesirable responses. Please note that the text generated by EXAONE 3.5 language models does not reflect the views\nof LG AI Research.\n• Inappropriate answers may be generated, which contain personal, harmful or other inappropriate information.\n• Biased responses may be generated, which are associated with age, gender, race, and so on.\n• The generated responses rely heavily on statistics from the training data, which can result in the generation of\nsemantically or syntactically incorrect sentences.\n10\n\n• Since the models do not reflect the latest information, the responses may be false or contradictory.\nLG AI Research strives to reduce potential risks that may arise from EXAONE 3.5 language models. Users are not\nallowed to engage in any malicious activities (e.g., keying in illegal information) that may induce the creation of\ninappropriate outputs violating LG AI’s ethical principles when using EXAONE 3.5 language models.\n6\nDeployment\nSection B in Appendix provides license information for using the EXAONE 3.5 language models. Understanding the\nlicense information is essential for the legal utilization of the language model.\n7\nConclusion\nIn response to the growing interest from academia and industry, we are excited to release EXAONE 3.5 language\nmodels that excel in real-world use cases and long-context understanding. These models are available in three sizes\n(32B, 7.8B, and 2.4B).\nTo validate performance of our models in the real-world use case scenarios, we evaluated our models on seven\nbenchmarks requiring diverse instructions understanding. To assess long-context understanding, we evaluated our\nmodels on four benchmarks. Our models consistently outperformed in both categories. Additionally, our models\nexhibited competitive performance in general domains including solving mathematical problems and writing code. In\nparticular, our 2.4B model ranked first in average scores across general domains.\nOur models are available to everyone for research purposes, and we welcome your feedback to help us improve the\nmodels. If you have any feedback or are interested in exploring commercial opportunities with our models, please reach\nout to contact_us@lgresearch.ai.\n11\n\nA\nContributors\nAll authors are listed in alphabetical order by last name.\nCore Contributors\nEunbi Choi, Kibong Choi, Seokhee Hong, Junwon Hwang, Hyojin Jeon, Hyunjik Jo, Joonkee Kim,\nSeonghwan Kim, Soyeon Kim, Sunkyoung Kim, Yireun Kim, Yongil Kim, Haeju Lee, Jinsik Lee, Kyungmin Lee,\nSangha Park, Heuiyeen Yeen, Hyeongu Yun\nContributors\nSoyoung An, Kyunghoon Bae, Stanley Jungkyu Choi, Gerrard Jeongwon Jo, Jiyeon Jung, Yountae Jung,\nHyosang Kim, Youchul Kim, Edward Hwayoung Lee, Honglak Lee, Woohyung Lim, Sooyoun Park, Yongmin Park,\nSihoon Yang\n12\n\nB\nModel License\nEXAONE AI Model License Agreement 1.1 - NC\nThis License Agreement (“Agreement”) is entered into between you (“Licensee”) and LG Management De-\nvelopment Institute Co., Ltd. (“Licensor”), governing the use of the EXAONE AI Model (“Model”). By downloading,\ninstalling, copying, or using the Model, you agree to comply with and be bound by the terms of this Agreement. If you\ndo not agree to all the terms, you must not download, install, copy, or use the Model. This Agreement constitutes a\nbinding legal agreement between the Licensee and Licensor.\n1. Definitions\n1.1 Model: The artificial intelligence model provided by Licensor, which includes any software, algorithms,\nmachine learning models, or related components supplied by Licensor. This definition extends to encompass all updates,\nenhancements, improvements, bug fixes, patches, or other modifications that may be provided by Licensor from time to\ntime, whether automatically or manually implemented.\n1.2 Derivatives: Any modifications, alterations, enhancements, improvements, adaptations, or derivative works of\nthe Model created by Licensee or any third party. This includes changes made to the Model’s architecture, parame-\nters, data processing methods, or any other aspect of the Model that results in a modification of its functionality or output.\n1.3 Output: Any data, results, content, predictions, analyses, insights, or other materials generated by the\nModel or Derivatives, regardless of whether they are in their original form or have been further processed or modified\nby the Licensee. This includes, but is not limited to, textual or numerical produced directly or indirectly through the use\nof the Model.\n1.4 Licensor: LG Management Development Institute Co., Ltd., the owner, developer, and provider of the\nEXAONE AI Model. The Licensor holds all rights, title, and interest in the Model and is responsible for granting\nlicenses to use the Model under the terms specified in this Agreement.\n1.5 Licensee: The individual, organization, corporation, academic institution, government agency, or other\nentity using or intending to use the Model under the terms and conditions of this Agreement. The Licensee is responsible\nfor ensuring compliance with the Agreement by all authorized users who access or utilize the Model on behalf of the\nLicensee.\n2. License Grant\n2.1 Grant of License: Subject to the terms and conditions outlined in this Agreement, the Licensor hereby\ngrants the Licensee a limited, non-exclusive, non-transferable, worldwide, and revocable license to:\na. Access, download, install, and use the Model solely for research purposes. This includes evaluation, test-\ning, academic research, experimentation, and participation in competitions, provided that such participation is in a\nnon-commercial context. Notwithstanding Section 3.1, the Licensee may only provide the Model or Derivatives for a\ncompetition if no commercial license is granted to the competition organizer or any third party.\nb. Publicly disclose research results and findings derived from the use of the Model or Derivatives, including\npublishing papers or presentations.\nc. Modify the Model and create Derivatives based on the Model, provided that such modifications and Derivatives are\nused exclusively for research purposes. The Licensee may conduct experiments, perform analyses, and apply custom\nmodifications to the Model to explore its capabilities and performance under various scenarios. If the Model is modified,\nthe modified Model must include “EXAONE” at the beginning of its name.\nd. Distribute the Model and Derivatives in each case with a copy of this Agreement.\n2.2 Scope of License: The license granted herein does not authorize the Licensee to use the Model for any\npurpose not explicitly permitted under this Agreement. Any use beyond the scope of this license, including any com-\nmercial application or external distribution, is strictly prohibited unless explicitly agreed upon in writing by the Licensor.\n13\n\n3. Restrictions\n3.1 Commercial Use: The Licensee is expressly prohibited from using the Model, Derivatives, or Output for\nany commercial purposes, including but not limited to, developing or deploying products, services, or applications\nthat generate revenue, whether directly or indirectly. Any commercial exploitation of the Model or its derivatives\nrequires a separate commercial license agreement with the Licensor. Furthermore, the Licensee shall not use the Model,\nDerivatives or Output to develop or improve other models.\n3.2 Reverse Engineering: The Licensee shall not decompile, disassemble, reverse engineer, or attempt to\nderive the source code, underlying ideas, algorithms, or structure of the Model, except to the extent that such activities\nare expressly permitted by applicable law. Any attempt to bypass or circumvent technological protection measures\napplied to the Model is strictly prohibited.\n3.3 Unlawful Use: The Licensee shall not use the Model and Derivatives for any illegal, fraudulent, or unau-\nthorized activities, nor for any purpose that violates applicable laws or regulations. This includes but is not limited to\nthe creation, distribution, or dissemination of malicious, deceptive, or unlawful content.\n3.4 Ethical Use: The Licensee shall ensure that the Model or Derivatives is used in an ethical and responsi-\nble manner, adhering to the following guidelines:\na. The Model and Derivatives shall not be used to generate, propagate, or amplify false, misleading, or harm-\nful information, including fake news, misinformation, or disinformation.\nb. The Model and Derivatives shall not be employed to create, distribute, or promote content that is discrimi-\nnatory, harassing, defamatory, abusive, or otherwise offensive to individuals or groups based on race, gender, sexual\norientation, religion, nationality, or other protected characteristics.\nc. The Model and Derivatives shall not infringe on the rights of others, including intellectual property rights,\nprivacy rights, or any other rights recognized by law. The Licensee shall obtain all necessary permissions and consents\nbefore using the Model and Derivatives in a manner that may impact the rights of third parties.\nd. The Model and Derivatives shall not be used in a way that causes harm, whether physical, mental, emo-\ntional, or financial, to individuals, organizations, or communities. The Licensee shall take all reasonable measures to\nprevent misuse or abuse of the Model and Derivatives that could result in harm or injury.\n4. Ownership\n4.1 Intellectual Property: All rights, title, and interest in and to the Model, including any modifications,\nDerivatives, and associated documentation, are and shall remain the exclusive property of the Licensor. The Licensee\nacknowledges that this Agreement does not transfer any ownership rights to the Licensee. All trademarks, service\nmarks, and logos associated with the Model are the property of the Licensor.\n4.2 Output: All rights, title, and interest in and to the Output generated by the Model and Derivatives whether in its\noriginal form or modified, are and shall remain the exclusive property of the Licensor. Licensee may use, modify, and\ndistribute the Output and its derivatives for research purpose. The Licensee shall not claim ownership of the Output\nexcept as expressly provided in this Agreement. The Licensee may use the Output solely for the purposes permitted\nunder this Agreement and shall not exploit the Output for unauthorized or commercial purposes.\n4.3 Attribution: In any publication or presentation of results obtained using the Model, the Licensee shall\nprovide appropriate attribution to the Licensor, citing the Model’s name and version, along with any relevant\ndocumentation or references specified by the Licensor.\n14\n\n5. No Warranty\n5.1 “As-Is” Basis: The Model, Derivatives, and Output are provided on an “as-is” and “as-available” basis,\nwithout any warranties or representations of any kind, whether express, implied, or statutory. The Licensor\ndisclaims all warranties, including but not limited to, implied warranties of merchantability, fitness for a partic-\nular purpose, accuracy, reliability, non-infringement, or any warranty arising from the course of dealing or usage of trade.\n5.2 Performance and Reliability: The Licensor does not warrant or guarantee that the Model, Derivatives\nor Output will meet the Licensee’s requirements, that the operation of the Model, Derivatives or Output will be\nuninterrupted or error-free, or that defects in the Model will be corrected. The Licensee acknowledges that the use of\nthe Model, Derivatives or Output is at its own risk and that the Model, Derivatives or Output may contain bugs, errors,\nor other limitations.\n5.3 No Endorsement: The Licensor does not endorse, approve, or certify any results, conclusions, or recom-\nmendations derived from the use of the Model. The Licensee is solely responsible for evaluating the accuracy, reliability,\nand suitability of the Model for its intended purposes.\n6. Limitation of Liability\n6.1 No Liability for Damages: To the fullest extent permitted by applicable law, in no event shall the Licen-\nsor be liable for any special, incidental, indirect, consequential, exemplary, or punitive damages, including but not\nlimited to, damages for loss of business profits, business interruption, loss of business information, loss of data, or\nany other pecuniary or non-pecuniary loss arising out of or in connection with the use or inability to use the Model,\nDerivatives or any Output, even if the Licensor has been advised of the possibility of such damages.\n6.2 Indemnification: The Licensee agrees to indemnify, defend, and hold harmless the Licensor, its affili-\nates, officers, directors, employees, and agents from and against any claims, liabilities, damages, losses, costs, or\nexpenses (including reasonable attorneys’ fees) arising out of or related to the Licensee’s use of the Model, any\nDerivatives, or any Output, including any violation of this Agreement or applicable laws.\n7. Termination\n7.1 Termination by Licensor: The Licensor reserves the right to terminate this Agreement and revoke the\nLicensee’s rights to use the Model at any time, with or without cause, and without prior notice if the Licensee breaches\nany of the terms or conditions of this Agreement. Termination shall be effective immediately upon notice.\n7.2 Effect of Termination: Upon termination of this Agreement, the Licensee must immediately cease all\nuse of the Model, Derivatives, and Output and destroy all copies of the Model, Derivatives, and Output in its possession\nor control, including any backup or archival copies. The Licensee shall certify in writing to the Licensor that such\ndestruction has been completed.\n7.3 Survival: The provisions of this Agreement that by their nature should survive termination, including\nbut not limited to, Sections 4 (Ownership), 5 (No Warranty), 6 (Limitation of Liability), and this Section 7 (Termination),\nshall continue to apply after termination.\n8. Governing Law\n8.1 Governing Law: This Agreement shall be governed by and construed in accordance with the laws of\nthe Republic of Korea, without regard to its conflict of laws principles.\n8.2 Arbitration: Any disputes, controversies, or claims arising out of or relating to this Agreement, includ-\ning its existence, validity, interpretation, performance, breach, or termination, shall be referred to and finally resolved\nby arbitration administered by the Korean Commercial Arbitration Board (KCAB) in accordance with the International\nArbitration Rules of the Korean Commercial Arbitration Board in force at the time of the commencement of the\narbitration. The seat of arbitration shall be Seoul, Republic of Korea. The tribunal shall consist of one arbitrator. The\nlanguage of the arbitration shall be English.\n15\n\n9. Alterations\n9.1 Modifications: The Licensor reserves the right to modify or amend this Agreement at any time, in its\nsole discretion. Any modifications will be effective upon posting the updated Agreement on the Licensor’s website or\nthrough other means of communication. The Licensee is responsible for reviewing the Agreement periodically for\nchanges. Continued use of the Model after any modifications have been made constitutes acceptance of the revised\nAgreement.\n9.2 Entire Agreement: This Agreement constitutes the entire agreement between the Licensee and Licensor\nconcerning the subject matter hereof and supersedes all prior or contemporaneous oral or written agreements,\nrepresentations, or understandings. Any terms or conditions of any purchase order or other document submitted by\nthe Licensee in connection with the Model that are in addition to, different from, or inconsistent with the terms and\nconditions of this Agreement are not binding on the Licensor and are void.\nBy downloading, installing, or using the EXAONE AI Model, the Licensee acknowledges that it has read,\nunderstood, and agrees to be bound by the terms and conditions of this Agreement.\n16\n\nC\nDecontamination Details\nAs described in Section 2.2.2, we apply the decontamination process over our training data to remove any data instances\nthat overlap with test sets, thus harming the generalization performance of our models. Figure 4 presents an overview of\nour decontamination process, and Table 10 shows examples of contaminated, therefore removed data.\nFigure 4: A summary of the decontamination method employed to train EXAONE 3.5 language models. Adopting\nan approach borrowed from the GPT-4 method, we increase the number of random sample to N = 10 for stricter\ndecontamination.\nBenchmark\nBenchmark example\nContaminated web corpus\nMMLU [16]\nA teacher has three packages of stickers. One package contains 56 stickers,\nanother package contains 48 stickers, and the third package contains 58 stick-\ners. If the teacher divides all the stickers equally among 27 students, how\nmany stickers will each student receive?\nA. 6 stickers\nB. 9 stickers\nC. 54 stickers\nD. 81 stickers\nAnswer:\n(...truncated...) A teacher has three packages of stickers. One package con-\ntains 56 stickers, another package contains 48 stickers, and the third package\ncontains 58 stickers. If the teacher divides all the stickers equally among 27\nstudents, how many stickers will each student receive?\n6 stickers is correct\n#4 Last week Mario walked 7 3/4 miles. This week he walked 15 5/6 miles.\nWhat is the difference between the distance he walked this week and the\ndistance he walked last week? (...truncated...)\nKMMLU [45]\n국가가국민의생활안정과복지증진을위하여보험의원리를도입하여\n만든사회보험의일종으로가입자, 사용자및국가로부터일정한보험료\n를받고이를재원으로여러가지정형화된보험금을지급하는사회보장\n제도는?\nA. 국민건강보험\nB. 국민연금\nC. 고용보험\nD. 산업재해보상보험\n정답:\n[Translation] What is the social security system, which is\na type of\nsocial insurance created by the nation by introducing the principles of\ninsurance to promote stability and welfare of citizens’ lives, and which\nreceives certain premiums from subscribers, employers, and the nation and\nuse these funds to provide various standardized insurance benefits.\nA. National Health Insurance\nB. National Pension\nC. Employment Insurance\nD. Industrial Accident Compensation Insurance\nAnswer:\n(...중략...) 더군다나개인주의의확산, 핵가족화의진전에따라전통적인\n가족의역할인노인부양의기능이약화됨으로써국가개입의중요성은\n더욱증가하게되었다. 따라서국민연금제도는국가가국민의생활안정\n과복지증진을위하여보험의원리를도입하여만든사회보험의일종으\n로가입자, 사용자및국가로부터일정한보험료를받고이를재원으로여\n러가지정형화된보험금을지급하는사회보장제도이다. (...중략...)\n[Translation] (...truncated...) Moreover, with the spread of individual-\nism and the rise of nuclear families, the traditional family role of supporting\nthe elderly has weakened, thereby increasing the importance of nation\nintervention. Accordingly, the National Pension System is a type of social\ninsurance created by the nation by introducing the principles of insurance to\npromote stability and welfare of citizens’ lives, and which receives certain\npremiums from subscribers, employers, and the nation and use these funds to\nprovide various standardized insurance benefits. (...truncated...)\nTable 10: Examples of contaminated web corpus. The text highlighted in grey is a part of the text that exists in both a\nbenchmark test set and a web corpus. The text underlined is a corresponding golden answer.\n17\n\nD\nEvaluation Details\nD.1\nBaseline Models\nWe choose various open models as the baselines for our EXAONE 3.5 language models. We mainly utilize Huggingface\nlibrary4 to access each checkpoint of baselines. The overall information of each model are presented in Table 11.\nModel Name\nContext Len.\nLink\nRelease\nQwen2.5 32B\n128k\nhttps://huggingface.co/Qwen/Qwen2.5-32B-Instruct\nSep., 2024\nC4AI Command R 32B\n128k\nhttps://huggingface.co/CohereForAI/c4ai-command-r-08-2024\nAug., 2024\nGemma 2 27B\n8k\nhttps://huggingface.co/google/gemma-2-27b-it\nJun., 2024\nYi 1.5 34B\n16k\nhttps://huggingface.co/01-ai/Yi-1.5-34B-Chat-16K\nMay, 2024\nQwen2.5 7B\n128k\nhttps://huggingface.co/Qwen/Qwen2.5-7B-Instruct\nSep., 2024\nLlama 3.1 8B\n128k\nhttps://huggingface.co/meta-llama/Llama-3.1-8B-Instruct\nJul., 2024\nGemma 2 9B\n8k\nhttps://huggingface.co/google/gemma-2-9b-it\nJun., 2024\nPhi 3 small (7B)\n128k\nhttps://huggingface.co/microsoft/Phi-3-small-128k-instruct\nMay, 2024\nQwen2.5 3B\n32k\nhttps://huggingface.co/Qwen/Qwen2.5-3B-Instruct\nSep., 2024\nQwen2.5 1.5B\n32k\nhttps://huggingface.co/Qwen/Qwen2.5-1.5B-Instruct\nSep., 2024\nLlama 3.2 3B\n128k\nhttps://huggingface.co/meta-llama/Llama-3.2-3B-Instruct\nSep., 2024\nGemma 2 2B\n8k\nhttps://huggingface.co/google/gemma-2-2b-it\nJul., 2024\nTable 11: The list of baseline models used for the evaluation along with their supported context length and released date.\nD.2\nLong Context\nD.2.1\nNeedle-In-A-Haystack\nThe specific configurations used in the Needle-In-A-Haystack (NIAH) experiment are detailed in Table 12.\nLanguage\nConfiguration\nDetails\nEnglish\nHaystack\nPaul Graham essays [23]\nNeedle\n“The best thing to do in San Francisco is eat a sandwich and sit in Dolores Park on a sunny\nday.”\nQuery\n“What is the best thing to do in San Francisco?”\nInstruction\n“Analyze the content of the given document to locate the answer to the specified question. If\nfound, provide the exact wording from the document without altering or summarizing it.”\nKorean\nHaystack\nAI-Hub5 대규모구매도서기반한국어말뭉치데이터\n(Large-scale Purchased Book-based Korean Language Corpus from AI-Hub)\nNeedle\n“광화문에서가장재미있는일은햇살좋은날에샌드위치를먹으며청와대안에있는\n공원에앉아있는것입니다.”\n(“The best thing to do at Gwanghwamun is eat a sandwich and sit in the park in the Blue\nHouse on a sunny day.”)\nQuery\n“광화문에서가장재미있는일이무엇인가요?”\n(“What is the best thing to do at Gwanghwamun?”)\nInstruction\n“주어진문서를읽고질문에대한답을확인하세요. 답을찾으면, 문서의원문을그대로\n유지하여수정이나해석없이반환하세요.”\n(Identical to the English instruction)\nTable 12: Detailed configuration of the Needle-In-A-Haystack experiment. The “Needle” refers to a specific text\nfragment embedded within the “Haystack,” which consists of long distractor texts. The task involves using a “Query” as\na cue to identify the needle within the haystack and retrieve the associated values.\n4https://huggingface.co/models\n5https://www.aihub.or.kr\n18\n\nD.2.2\nLongBench\nLONGBENCH has been suggested as a bilingual benchmark to assess long context comprehension in English and\nChinese. In this report, we focus on the English subsets, specifically Single-doc QA, Multi-doc QA, Summarization,\nand Few-shot Learning.\nThe Single-doc QA task includes datasets such as NarrativeQA [25], Qasper [11], and MultiFieldQA-EN [5]. For\nthe Multi-doc QA task, datasets like HotpotQA [56], 2WikiMultihopQA [18], and MuSiQue [50] are utilized. The\nSummarization task involves datasets such as GovReport [20], QMSum [60], and MultiNews [13], while the Few-shot\nLearning task relies on datasets from TREC [30] and TriviaQA [22]. All evaluation methods and metrics for these\ndatasets adhere to the official LONGBENCH settings.\nDetailed task scores are presented in Table 13.\nModels\nSingle-doc QA\nMulti-doc QA\nSummarization\nFew-shot Learning\nAverage\nEXAONE 3.5 32B\n40.1\n52.9\n23.1\n80.1\n49.2\nQwen 2.5 32B\n43.2\n54.9\n26.1\n72.4\n49.1\nC4AI Command R 32B\n44.6\n48.9\n26.4\n83.6\n50.9\nGemma 2 27B\n-\n-\n-\n-\n-\nYi 1.5 34B\n-\n-\n-\n-\n-\nEXAONE 3.5 7.8B\n38.4\n47.7\n22.6\n75.1\n46.0\nQwen 2.5 7B\n40.8\n44.0\n26.5\n77.4\n47.2\nLlama 3.1 8B\n39.8\n41.2\n27.6\n69.9\n44.6\nGemma 2 9B\n-\n-\n-\n-\n-\nPhi 3 small (7B)\n33.2\n26.5\n26.3\n76.2\n40.6\nEXAONE 3.5 2.4B\n35.0\n43.1\n20.1\n72.8\n42.7\nQwen 2.5 3B\n35.5\n34.7\n24.7\n72.9\n42.0\nQwen 2.5 1.5B\n29.9\n32.1\n22.3\n64.0\n37.1\nLlama 3.2 3B\n33.9\n34.9\n25.8\n72.3\n41.7\nGemma 2 2B\n-\n-\n-\n-\n-\nTable 13: Performance comparison results of EXAONE 3.5 language models with similar-sized recently released\nlanguage models across four benchmarks representing long context scenarios. Context lengths for each benchmark, as\nwell as model limitations, are detailed in Table 11, where a dash (-) indicates that the model does not support context\nlengths longer than 16k. The final overall score for each model is calculated as a macro average across the benchmarks.\nBold scores indicate the best performance, and underlined scores mean the second best.\n19\n\nD.2.3\nLongRAG\nLONGRAG is a RAG benchmark that focuses on long context retrieval and generation using large text chunks. We use\nNatural Questions [26] and HotpotQA [57] subsets from the original LONGRAG.\nTo further evaluate the model’s capability to handle cases where the retrieved passage does not support a valid answer,\nwe extend the LONGRAG benchmark by incorporating unanswerable cases. The LONGRAG benchmark uses the\nopen-sourced dense retrieval toolkit as its retriever. However, the retrieved context does not always include evidence\nthat supports the correct answer. To address this limitation, we define unanswerable cases using the is_retrieval\nfunction in LONGRAG. The is_retrieval function takes the context and golden answer as inputs and determines\nwhether the context contains sufficient evidence to extract the correct answer. It returns True if such evidence exists\nand False otherwise. When the return value of is_retrieval is False, indicating that the context does not contain\nthe correct answer, we modify the ground-truth answer to [“Unanswerable”, “No relevant information found.”, “This\nquestion cannot be answered with the provided data.”]. This modification allows the model to learn and appropriately\nhandle unanswerable cases.\nAdditionally, to ensure the model responds effectively to unanswerable cases, the following sentence is added to the\nexisting prompt: “If the answer cannot be found in the context, respond with ‘Unanswerable’.” This prompt guides the\nmodel to respond explicitly with Unanswerable when it determines that no answer exists within the context. Through\nthis extension, the LONGRAG benchmark gains an enhanced evaluation framework capable of handling unanswerable\nscenarios, enabling more comprehensive and nuanced performance assessments.\nDetailed task scores are presented in Table 14.\nModels\nNQ\nHotpot QA\nAverage\nAnswerable Unanswerable\nTotal\nAnswerable Unanswerable\nTotal\nEXAONE 3.5 32B\n73.6\n35.3\n68.3\n81.8\n26.4\n66.9\n67.6\nQwen 2.5 32B\n62.3\n61.2\n62.1\n62.9\n70.6\n65.0\n63.6\nC4AI Command R 32B\n64.0\n32.4\n59.6\n63.1\n18.2\n51.0\n55.3\nGemma 2 27B\n-\n-\n-\n-\n-\n-\n-\nYi 1.5 34B\n-\n-\n-\n-\n-\n-\n-\nEXAONE 3.5 7.8B\n72.0\n41.0\n67.7\n74.3\n53.9\n68.8\n68.3\nQwen 2.5 7B\n64.5\n51.1\n62.6\n61.8\n46.1\n57.6\n60.1\nLlama 3.1 8B\n63.2\n15.1\n56.5\n67.4\n16.4\n53.7\n55.1\nGemma 2 9B\n-\n-\n-\n-\n-\n-\n-\nPhi 3 small (7B)\n66.8\n13.7\n59.4\n60.2\n7.1\n45.9\n52.7\nEXAONE 3.5 2.4B\n67.8\n25.9\n62.0\n73.1\n41.6\n64.6\n63.3\nQwen 2.5 3B\n49.5\n34.5\n47.4\n52.5\n21.6\n44.2\n45.8\nQwen 2.5 1.5B\n49.9\n18.0\n45.5\n43.6\n2.2\n32.5\n39.0\nLlama 3.2 3B\n49.4\n41.7\n48.3\n53.6\n16.0\n43.5\n45.9\nGemma 2 2B\n-\n-\n-\n-\n-\n-\n-\nTable 14: Performance comparison results of EXAONE 3.5 language models with similar-sized recently released\nlanguage models with LongRAG benchmarks. The benchmark is extended with the “Unanswerable” case, which\nrequires models to respond as “Unanswerable” when the information cannot be found within the context. Bold scores\nindicate the best performance, and underlined scores mean the second best.\nFig 5 shows the LLM-as-a-judge prompt used for LongRAG evaluation. In the LLM-as-a-judge evaluation setup,\nwe incorporate short answer evaluation to align with the methodology used in LONGRAG, where short answers are\nextracted to calculate Exact Match (EM). We extend this approach to LLM-as-a-judge to ensure consistency across\nevaluation metrics. However, as the observed trends for short answers consistently align with those for long answers,\nwe prioritize long answer evaluation in our final analysis to streamline the assessment process without compromising\nthe robustness of the results.\n20\n\nLongRAG LLM-as-a-judge Prompt\nSystem:\nYou are an expert evaluator of text answers.\nYour task is to compare the content of two answers, a long answer (long_ans) and a short answer (short_ans),\nwith the provided correct answers (Answer), which may contain multiple correct options.\nBoth the long answer and the short answer need to be checked for correctness.\nThe long and short answers do not need to match any of the answers in the Answer list word-for-word but must\nconvey the same key meaning or idea.\nIf either the long or short answer matches any one of the correct answers in the Answer list, it should be\nconsidered correct.\nFocus only on the accuracy of the content and ignore style, tone, or extra information unless it introduces\ninaccuracies.\nFor both the long and short answers, return only the evaluation result as a Python dictionary object, and ensure\nthe output is formatted as valid Python code.\nHere are two examples of how to evaluate answers:\nExample 1:\nQuestion: what does hp mean in war and order\nAnswer: ['hit points', 'health points']\nlong_ans: HP stands for Health Points in video games and war, it is a measure of an entity’s ability to function\nand survive in a combat situation. In video games, HP is often displayed as a numeric value, and can be depleted\nby taking damage from enemies or other hazards. When an entity’s HP reaches zero, it is often considered\ndefeated or eliminated. In war, HP can refer to the physical and mental resilience of soldiers, and can be\naffected by factors such as injury, fatigue.\nshort_ans: HP stands fer Health Points.\nEvaluation: {'long_ans': 'correct', 'short_ans': 'correct'}\nExample 2:\nQuestion: what is the capital of France\nAnswer: ['Paris']\nlong_ans: The capital of France is Paris, a major European city and a global center for art, fashion, and culture.\nParis is known for its cafe culture and landmarks like the Eiffel Tower, Notre-Dame Cathedral, and the Louvre\nMuseum.\nshort_ans: The capital of France is Lyon.\nEvaluation: {'long_ans': 'correct', 'short_ans': 'incorrect'}\nNow, proceed with your evaluation of the following question, answer, and responses, and return only the\nevaluation as a valid Python dictionary.\nEnsure the response is a valid Python dictionary object without any additional text.\nUser:\nEvaluate the following long and short answers based on the provided correct answer.\nYour goal is to determine if the long and short answers are correct.\nReturn the evaluation result in the form of a Python dictionary: {'long_ans': 'correct 'or 'incorrect\n', 'short_ans': 'correct'or 'incorrect'}.\nQuestion: {{question}}\nAnswer: {{answer}}\nLong_ans: {{long_ans}}\nShort_ans: {{short_ans}}\nReturn only the evaluation in the form of a Python dictionary.\nDo not include any explanation or additional comments.\nFigure 5: LLM-as-a-judge prompt for evaluating LongRAG\n21\n\nD.2.4\nKo-LongRAG\nWe construct a Korean counterpart of LONGRAG, named KO-LONGRAG, to evaluate long-context reasoning and\nretrieval capabilities in Korean. KO-LONGRAG focuses on retrieval-augmented generation (RAG) tasks with an average\ncontext length of approximately 14,000 tokens, challenging models to process extensive Korean texts, extract relevant\ninformation, and reason effectively. Similar to LONGRAG, it includes 50 unanswerable cases among a total of 300\nqueries.\nModels\nSingle-doc QA\nMulti-doc QA\nAverage\nAnswerable Unanswerable\nTotal\nAnswerable Unanswerable\nTotal\nEXAONE 3.5 32B\n92.4\n100.0\n93.7\n72.8\n98.0\n77.0\n85.3\nQwen 2.5 32B\n90.0\n98.0\n91.3\n48.4\n92.0\n55.7\n73.5\nC4AI Command R 32B\n85.6\n66.0\n82.3\n62.4\n62.0\n62.3\n72.3\nGemma 2 27B\n-\n-\n-\n-\n-\n-\n-\nYi 1.5 34B\n-\n-\n-\n-\n-\n-\n-\nEXAONE 3.5 7.8B\n68.4\n100.0\n73.7\n64.0\n98.0\n69.7\n71.7\nQwen 2.5 7B\n61.2\n98.0\n67.3\n33.2\n94.0\n43.3\n55.3\nLlama 3.1 8B\n78.0\n76.0\n77.7\n56.8\n28.0\n52.0\n64.8\nGemma 2 9B\n-\n-\n-\n-\n-\n-\n-\nPhi 3 small (7B)\n8.0\n14.0\n9.0\n4.8\n14.0\n6.3\n7.7\nEXAONE 3.5 2.4B\n80.8\n100.0\n84.0\n61.6\n84.0\n65.3\n74.7\nQwen 2.5 3B\n56.4\n98.0\n63.3\n2.4\n94.0\n17.7\n40.5\nQwen 2.5 1.5B\n22.0\n96.0\n34.3\n21.6\n92.0\n33.3\n33.8\nLlama 3.2 3B\n48.8\n12.0\n42.7\n40.0\n16.0\n36.0\n39.3\nGemma 2 2B\n-\n-\n-\n-\n-\n-\n-\nTable 15: Performance comparison results of EXAONE 3.5 language models with similar-sized recently released\nlanguage models with Ko-LongRAG benchmarks. The benchmark is extended with the “Unanswerable” case, which\nrequires models to respond as “Unanswerable” when the information cannot be found within the context. Bold scores\nindicate the best performance, and underlined scores mean the second best.\nThe detailed task scores are presented in Table 15. Similar to LONGRAG, the evaluation setup for KO-LONGRAG\nincorporates short answer evaluation to align with the methodology used in LONGRAG. However, as the trends for\nshort answers are consistent with those observed for long answers, the final evaluation focuses solely on long answer\ncorrectness to streamline the analysis without compromising robustness.\nThe detailed prompt and examples used for KO-LONGRAG evaluation are provided in Figure 6 and 7, respectively.\nThe prompt used for KO-LONGRAG evaluation is illustrated in Figure 8.\n22\n\nKo-LongRAG Prompt\n[ Single-doc QA Prompt ]\nSystem: 당신은도움이되는어시스턴트입니다.\nUser:\n다음문서를살펴보고, 질문에대한답을추출하세요.\n질문에대한답만생성하세요. 답변은매우간결해야합니다.\n답변을문서에서찾을수없는경우,\n’주어진정보로답할수없다’로응답하세요.\n문서는“Title”에따라정렬된Wikipedia 문단목록이며제목은다음과같습니다: {{titles}}.\n각위키피디아문단은’Title’ 필드와’Text’ 필드를포함합니다.\n문서는다음과같습니다: {{context}}. 질문은다음과같습니다: {{question}}.\n[ Multi-doc QA Prompt ]\nSystem: 당신은도움이되는어시스턴트입니다.\nUser:\n다음문서를검토하고질문에답하세요.\n문서는Wikipedia 문단목록이며제목은다음과같습니다: {{titles}}.\n질문에는두가지유형이있습니다:\n예또는아니오로답하거나두후보중에서선택해야하는비교질문과, 단답형형태의일반질문입니다.\n문서는다음과같습니다: {{context}}.\n문서에서필요한문단을찾아질문에답하세요: {{question}}.\n일반질문의경우문단에서정확한단어를찾아서답변해야합니다.\n질문에대한답만생성하고다른어떤것도생성하지마세요.\n답변을문서에서찾을수없는경우, ’주어진정보로답할수없다’로응답하세요.\nFigure 6: Prompt for evaluating Ko-LongRAG.\nD.2.5\nKo-WebRAG\nKO-WEBRAG is a real-world benchmark tailored to assess the performance of language models as generators within\nthe Retrieval-Augmented Generation (RAG) framework, using a web-search engine as a fixed retriever. The benchmark\ncomprises 300 RAG tasks, each featuring a user query alongside documents retrieved by the simulated web-search\nengine. The retrieved documents in KO-WEBRAG are meticulously curated to ensure they provide sufficient supporting\ninformation for generating a gold-standard answer. Context lengths vary from 4K to 32K tokens, with an average length\nof approximately 14,000 tokens.\nEach dataset instance includes a user query, a gold-standard answer, and the corresponding retrieved documents. The\nperformance of the target LLM is evaluated based on its ability to generate answers that match the gold-standard answer,\nmeasuring its effectiveness as a generator in RAG tasks.\nThe evaluation involves assessing the responses of the LLM to each of the 300 tasks using GPT-4o. The percentage of\ntasks for which the LLM’s responses pass this evaluation is reported as the final score. To align with the purpose of a\nKorean-language benchmark, the GPT-4o LLM-as-a-judge prompt incorporates additional criteria beyond semantic\nalignment with the gold-standard answer; it also checks whether questions asked in Korean have been answered in\nKorean. Note that Qwen models smaller than 32B often fail to meet this language compliance criterion, leading to lower\nscores.\n23\n\nKo-LongRAG Examples\n[ Single-doc QA Answerable Case ]\nContext:\n...\nTitle: 박진우(야구인)\nText: 박진우(朴晋佑, 1990년2월12일∼)는전KBO 리그NC 다이노스의투수이자, 현KBO 리그SSG\n랜더스의스카우트이다.\n...\n2019년시즌: 선발과불펜을가리지않고활약했다. 시즌140.2이닝3점대평균자책점, 92탈삼진, 9승7패,\n5홀드를기록했다. 이동욱감독은’가장MVP로꼽고싶은선수’라며칭찬했다.\n...\nQuestion: 박진우가NC 다이노스에서9승을기록한시즌은언제인가요?\nAnswer: 2019년\n[ Single-doc QA Unanswerable Case ]\nQuestion: 인천남동소방서의설립연도는무엇인가요?\nAnswer: 주어진문서내에서답할수있는정보가충분하지않습니다.\n[ Multi-doc QA Answerable Case ]\nContext:\n...\nTitle: 아스투리아스공상\nText: 아스투리아스공상은스페인의프린시페데아스투리아스재단(Fundación Príncipe de Asturias)\n이주관하는상이다. 1980년9월24일스페인의왕세자에해당하는호칭인아스투리아스공이었던\n펠리페(Felipe, 펠리페6세)에의해제정되었으며1981년에첫시상식이열렸다. 총9개부문(예술\n부문, 커뮤니케이션· 인문주의부문, 국제협력부문, 문학부문, 사회과학부문, 체육부문, 기술· 과학\n연구부문, 화합부문, 아스투리아스모범상부문)으로나누어시상한다. 시상식은아스투리아스지방의\n오비에도에서열린다. 수상자는주안미로가제작한조각, 상금50,000 유로를받게된다.\n...\nTitle: 미겔데세르반테스상\nText: 미겔데세르반테스상(-賞, ) 또는세르반테스상은스페인작가미겔데세르반테스의이름이붙은\n스페인어작가에게수여되는문학상으로, 영연방의맨부커상과유사한스페인어권의상이다. 그러나\n맨부커상과는다르게일생동안의문학적성취를평가해서단한번만수여하므로스페인어권에서\n그권위는노벨문학상에버금간다. 1976년제정되었다. 스페인문화부가수여하며상금은12만5천\n유로이다.\n...\nQuestion: 아스투리아스공상과미겔데세르반테스상중상금이더많은것은무엇인가요?\nAnswer: 미겔데세르반테스상\n[ Multi-doc QA Unanswerable Case ]\nQuestion: 넬슨록펠러와노아사이러스는둘다정치경력을가지고있었나요?\nAnswer: 주어진문서내에서답할수있는정보가충분하지않습니다.\nFigure 7: Examples of Ko-LongRAG.\n24\n\nKo-LongRAG LLM-as-a-Judge Prompt\nSystem:\nYou are an expert evaluator of text answers in Korean.\nYour task is to compare the content of two Korean answers, a long answer (long_ans) and a short answer\n(short_ans), with the provided correct answers (Answer), which may contain multiple correct options.\nBoth the long answer and the short answer need to be checked for correctness. The long and short answers do\nnot need to match any of the answers in the Answer list word-for-word but must convey the same key meaning\nor idea.\nIf either the long or short answer matches any one of the correct answers in the Answer list, it should be\nconsidered correct.\nFocus only on the accuracy of the content and ignore style, tone, or extra information unless it introduces\ninaccuracies.\nFor both the long and short answers, return only the evaluation result as a Python dictionary object, and ensure\nthe output is formatted as valid Python code.\nHere are two examples of how to evaluate answers:\nExample 1:\nQuestion: HP는게임에서무엇을의미하나요?\nAnswer: ['체력', '생명력']\nlong_ans: HP는’생명력’ 또는’체력’을의미하며, 게임에서캐릭터의생존력을나타내는지표입니다. HP\n가줄어들면캐릭터는점점약해지며, 0이되면게임에서탈락하거나패배할수있습니다.\nshort_ans: HP는캐릭터의체력입니다.\nEvaluation: {'long_ans': 'correct', 'short_ans': 'correct'}\nExample 2:\nQuestion: 프랑스의수도는어디인가요?\nAnswer: ['파리']\nlong_ans: 프랑스의수도는파리로, 리옹의오른쪽아래에위치하고, 문화와예술의중심지로알려져\n있습니다. 에펠탑, 루브르박물관, 노트르담대성당등유명한관광지가위치해있습니다.\nshort_ans: 프랑스의수도는리옹입니다.\nEvaluation: {'long_ans': 'correct', 'short_ans': 'incorrect'}\nNow, proceed with your evaluation of the following question, answer, and responses, and return only the\nevaluation as a valid Python dictionary.\nEnsure the response is a valid Python dictionary object without any additional text.\nUser:\nEvaluate the following long and short answers based on the provided correct answer.\nYour goal is to determine if the long and short answers are correct.\nReturn the evaluation result in the form of a Python dictionary: {'long_ans': 'correct 'or 'incorrect\n', 'short_ans': 'correct'or 'incorrect'}.\nQuestion: {{question}}\nAnswer: {{answer}}\nlong_ans: {{long_ans}}\nshort_ans: {{short_ans}}\nReturn only the evaluation in the form of a Python dictionary.\nDo not include any explanation or additional comments.\nFigure 8: LLM-as-a-judge prompt for evaluating Ko-LongRAG.\n25\n\nD.3\nGeneral Domain\nFor all benchmarks in the General Domain category, we use 0-shot prompts and parse a final answer from the generated\nmodel response. Greedy decoding is used and maximum length of generation is set to 2,048 for all tasks. From\nFigure 9 to 13, we present all prompts we use for the evaluation for each benchmarks. For BBH, we utilize 0-shot CoT\nprompts6 from Language Model Evaluation Harness [14].\nGSM8K/MATH prompt (CoT)\nGiven the following math problem, reason step-by-step and give a final answer to the problem. Put your final\nanswer within \\boxed{}.\nProblem: {{question}}\nAnswer: Let’s think step by step.\nFigure 9: Prompt for evaluating GSM8K (CoT) and MATH (CoT) benchmarks.\nHumanEval/MBPP prompt\nUser:\nPlease provide a self-contained Python script that solves the following problem in a markdown code block:\n“‘\n{{input}}\n“‘\nAssistant:\nBelow is a Python script with a self-contained function that solves the problem and passes corresponding tests:\n“‘python\nFigure 10: Prompt for evaluating HUMANEVAL and MBPP benchmarks. We use the default prompt setting from the\nofficial Github repository7 of EvalPlus [31].\nMMLU/GPQA prompt (CoT)\nGiven the following question and candidate answers (A, B, C and D), reason step-by-step and choose the best\nanswer to the question.\nQuestion: {{question}}\nA. {{option A}}\nB. {{option B}}\nC. {{option C}}\nD. {{option D}}\nYour response should end with \"The best answer is [the_answer_letter]\" where the [the_answer_letter] is one of\nA, B, C or D.\nAnswer: Let’s think step by step.\nFigure 11: Prompt for evaluating MMLU (CoT) and GPQA (CoT) benchmarks.\n6https://github.com/EleutherAI/lm-evaluation-harness/tree/main/lm_eval/tasks/bbh/cot_zeroshot\n26\n\nKMMLU prompt (CoT)\n다음시험문제에대해서, 충분히생각하고추론하여, 4개의보기(A, B, C, D) 중정답을고르세요.\n문제: {{question}}\nA. {{option A}}\nB. {{option B}}\nC. {{option C}}\nD. {{option D}}\n당신의대답은\"정답은[정답보기]입니다.\"로끝나야하고, [정답보기]는A, B, C, D 중하나여야합니다.\n정답: 문제를풀기위해, 한번천천히생각해봅시다.\nFigure 12: Prompt for evaluating KMMLU (CoT) benchmark.\nARC-C prompt\nGiven the following question and candidate answers (A, B, C and D), choose the best answer to the question.\nQuestion: {{question}}\nA. {{option A}}\nB. {{option B}}\nC. {{option C}}\nD. {{option D}}\nYour response should end with \"The best answer is [the_answer_letter]\" where the [the_answer_letter] is one of\nA, B, C or D.\nAnswer:\nFigure 13: Prompt for evaluating ARC-C benchmark.\n27\n\nReferences\n[1] Marah Abdin, Sam Ade Jacobs, Ammar Ahmad Awan, Jyoti Aneja, Ahmed Awadallah, Hany Awadalla, Nguyen\nBach, Amit Bahree, Arash Bakhtiari, Jianmin Bao, Harkirat Behl, Alon Benhaim, Misha Bilenko, Johan Bjorck,\nSébastien Bubeck, Qin Cai, Martin Cai, Caio César Teodoro Mendes, Weizhu Chen, Vishrav Chaudhary, Dong\nChen, Dongdong Chen, Yen-Chun Chen, Yi-Ling Chen, Parul Chopra, Xiyang Dai, Allie Del Giorno, Gustavo\nde Rosa, Matthew Dixon, Ronen Eldan, Victor Fragoso, Dan Iter, Mei Gao, Min Gao, Jianfeng Gao, Amit Garg,\nAbhishek Goswami, Suriya Gunasekar, Emman Haider, Junheng Hao, Russell J. Hewett, Jamie Huynh, Mojan\nJavaheripi, Xin Jin, Piero Kauffmann, Nikos Karampatziakis, Dongwoo Kim, Mahoud Khademi, Lev Kurilenko,\nJames R. Lee, Yin Tat Lee, Yuanzhi Li, Yunsheng Li, Chen Liang, Lars Liden, Ce Liu, Mengchen Liu, Weishung\nLiu, Eric Lin, Zeqi Lin, Chong Luo, Piyush Madan, Matt Mazzola, Arindam Mitra, Hardik Modi, Anh Nguyen,\nBrandon Norick, Barun Patra, Daniel Perez-Becker, Thomas Portet, Reid Pryzant, Heyang Qin, Marko Radmilac,\nCorby Rosset, Sambudha Roy, Olatunji Ruwase, Olli Saarikivi, Amin Saied, Adil Salim, Michael Santacroce,\nShital Shah, Ning Shang, Hiteshi Sharma, Swadheen Shukla, Xia Song, Masahiro Tanaka, Andrea Tupini, Xin\nWang, Lijuan Wang, Chunyu Wang, Yu Wang, Rachel Ward, Guanhua Wang, Philipp Witte, Haiping Wu, Michael\nWyatt, Bin Xiao, Can Xu, Jiahang Xu, Weijian Xu, Sonali Yadav, Fan Yang, Jianwei Yang, Ziyi Yang, Yifan Yang,\nDonghan Yu, Lu Yuan, Chengruidong Zhang, Cyril Zhang, Jianwen Zhang, Li Lyna Zhang, Yi Zhang, Yue Zhang,\nYunan Zhang, and Xiren Zhou. Phi-3 technical report: A highly capable language model locally on your phone.\narXiv preprint arXiv:2404.14219, 2024.\n[2] 01. AI, :, Alex Young, Bei Chen, Chao Li, Chengen Huang, Ge Zhang, Guanwei Zhang, Heng Li, Jiangcheng Zhu,\nJianqun Chen, Jing Chang, Kaidong Yu, Peng Liu, Qiang Liu, Shawn Yue, Senbin Yang, Shiming Yang, Tao Yu,\nWen Xie, Wenhao Huang, Xiaohui Hu, Xiaoyi Ren, Xinyao Niu, Pengcheng Nie, Yuchi Xu, Yudong Liu, Yue\nWang, Yuxuan Cai, Zhenyu Gu, Zhiyuan Liu, and Zonghong Dai. Yi: Open Foundation Models by 01.AI, 2024.\n[3] Joshua Ainslie, James Lee-Thorp, Michiel de Jong, Yury Zemlyanskiy, Federico Lebron, and Sumit Sanghai.\nGQA: Training Generalized Multi-Query Transformer Models from Multi-Head Checkpoints. In Proceedings of\nthe 2023 Conference on Empirical Methods in Natural Language Processing, pages 4895–4901, 2023.\n[4] Jacob Austin, Augustus Odena, Maxwell Nye, Maarten Bosma, Henryk Michalewski, David Dohan, Ellen Jiang,\nCarrie Cai, Michael Terry, Quoc Le, and Charles Sutton. Program synthesis with large language models. arXiv\npreprint arXiv:2108.07732, 2021.\n[5] Yushi Bai, Xin Lv, Jiajie Zhang, Hong Lyu, Jiankai Tang, Zhidian Huang, Zhengxiao Du, Xiao Liu, Aohan Zeng,\nLei Hou, Yuxiao Dong, Jie Tang, and Juanzi Li. LongBench: A Bilingual, Multitask Benchmark for Long Context\nUnderstanding. In Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics\n(Volume 1: Long Papers), pages 3119–3137, 2024.\n[6] Mark Chen, Jerry Tworek, Heewoo Jun, Qiming Yuan, Henrique Ponde de Oliveira Pinto, Jared Kaplan, Harri\nEdwards, Yuri Burda, Nicholas Joseph, Greg Brockman, Alex Ray, Raul Puri, Gretchen Krueger, Michael\nPetrov, Heidy Khlaaf, Girish Sastry, Pamela Mishkin, Brooke Chan, Scott Gray, Nick Ryder, Mikhail Pavlov,\nAlethea Power, Lukasz Kaiser, Mohammad Bavarian, Clemens Winter, Philippe Tillet, Felipe Petroski Such, Dave\nCummings, Matthias Plappert, Fotios Chantzis, Elizabeth Barnes, Ariel Herbert-Voss, William Hebgen Guss, Alex\nNichol, Alex Paino, Nikolas Tezak, Jie Tang, Igor Babuschkin, Suchir Balaji, Shantanu Jain, William Saunders,\nChristopher Hesse, Andrew N. Carr, Jan Leike, Josh Achiam, Vedant Misra, Evan Morikawa, Alec Radford,\nMatthew Knight, Miles Brundage, Mira Murati, Katie Mayer, Peter Welinder, Bob McGrew, Dario Amodei, Sam\nMcCandlish, Ilya Sutskever, and Wojciech Zaremba. Evaluating large language models trained on code. arXiv\npreprint arXiv:2107.03374, 2021.\n[7] Shouyuan Chen, Sherman Wong, Liangjian Chen, and Yuandong Tian. Extending context window of large\nlanguage models via positional interpolation. arXiv preprint arXiv:2306.15595, 2023.\n[8] Peter Clark, Isaac Cowhey, Oren Etzioni, Tushar Khot, Ashish Sabharwal, Carissa Schoenick, and Oyvind\nTafjord. Think you have Solved Question Answering? Try ARC, the AI2 Reasoning Challenge. arXiv preprint\narXiv:1803.05457, 2018.\n[9] Karl Cobbe, Vineet Kosaraju, Mohammad Bavarian, Mark Chen, Heewoo Jun, Lukasz Kaiser, Matthias Plappert,\nJerry Tworek, Jacob Hilton, Reiichiro Nakano, Christopher Hesse, and John Schulman. Training verifiers to solve\nmath word problems. arXiv preprint arXiv:2110.14168, 2021.\n[10] Cohere For AI. c4ai-command-r-08-2024 (Revision 280b5c1), 2024.\n28\n\n[11] Pradeep Dasigi, Kyle Lo, Iz Beltagy, Arman Cohan, Noah A. Smith, and Matt Gardner. A dataset of information-\nseeking questions and answers anchored in research papers. In Kristina Toutanova, Anna Rumshisky, Luke Zettle-\nmoyer, Dilek Hakkani-Tur, Iz Beltagy, Steven Bethard, Ryan Cotterell, Tanmoy Chakraborty, and Yichao Zhou,\neditors, Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational\nLinguistics: Human Language Technologies, pages 4599–4610, Online, June 2021. Association for Computational\nLinguistics.\n[12] Yann Dubois, Balázs Galambosi, Percy Liang, and Tatsunori B. Hashimoto. Length-controlled alpacaeval: A\nsimple way to debias automatic evaluators. arXiv preprint arXiv:2404.04475, 2024.\n[13] Alexander Fabbri, Irene Li, Tianwei She, Suyi Li, and Dragomir Radev. Multi-news: A large-scale multi-document\nsummarization dataset and abstractive hierarchical model. In Anna Korhonen, David Traum, and Lluís Màrquez,\neditors, Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics, pages 1074–\n1084, Florence, Italy, July 2019. Association for Computational Linguistics.\n[14] Leo Gao, Jonathan Tow, Baber Abbasi, Stella Biderman, Sid Black, Anthony DiPofi, Charles Foster, Laurence\nGolding, Jeffrey Hsu, Alain Le Noac’h, Haonan Li, Kyle McDonell, Niklas Muennighoff, Chris Ociepa, Jason\nPhang, Laria Reynolds, Hailey Schoelkopf, Aviya Skowron, Lintang Sutawika, Eric Tang, Anish Thite, Ben Wang,\nKevin Wang, and Andy Zou. A framework for few-shot language model evaluation, 07 2024.\n[15] Aaron Grattafiori, Abhimanyu Dubey, Abhinav Jauhri, Abhinav Pandey, Abhishek Kadian, Ahmad Al-Dahle,\nAiesha Letman, Akhil Mathur, Alan Schelten, Alex Vaughan, Amy Yang, Angela Fan, Anirudh Goyal, Anthony\nHartshorn, Aobo Yang, Archi Mitra, Archie Sravankumar, Artem Korenev, Arthur Hinsvark, Arun Rao, Aston\nZhang, Aurelien Rodriguez, Austen Gregerson, Ava Spataru, Baptiste Roziere, Bethany Biron, Binh Tang,\nBobbie Chern, Charlotte Caucheteux, Chaya Nayak, Chloe Bi, Chris Marra, Chris McConnell, Christian Keller,\nChristophe Touret, Chunyang Wu, Corinne Wong, Cristian Canton Ferrer, Cyrus Nikolaidis, Damien Allonsius,\nDaniel Song, Danielle Pintz, Danny Livshits, Danny Wyatt, David Esiobu, Dhruv Choudhary, Dhruv Mahajan,\nDiego Garcia-Olano, Diego Perino, Dieuwke Hupkes, Egor Lakomkin, Ehab AlBadawy, Elina Lobanova, Emily\nDinan, Eric Michael Smith, Filip Radenovic, Francisco Guzmán, Frank Zhang, Gabriel Synnaeve, Gabrielle\nLee, Georgia Lewis Anderson, Govind Thattai, Graeme Nail, Gregoire Mialon, Guan Pang, Guillem Cucurell,\nHailey Nguyen, Hannah Korevaar, Hu Xu, Hugo Touvron, Iliyan Zarov, Imanol Arrieta Ibarra, Isabel Kloumann,\nIshan Misra, Ivan Evtimov, Jack Zhang, Jade Copet, Jaewon Lee, Jan Geffert, Jana Vranes, Jason Park, Jay\nMahadeokar, Jeet Shah, Jelmer van der Linde, Jennifer Billock, Jenny Hong, Jenya Lee, Jeremy Fu, Jianfeng Chi,\nJianyu Huang, Jiawen Liu, Jie Wang, Jiecao Yu, Joanna Bitton, Joe Spisak, Jongsoo Park, Joseph Rocca, Joshua\nJohnstun, Joshua Saxe, Junteng Jia, Kalyan Vasuden Alwala, Karthik Prasad, Kartikeya Upasani, Kate Plawiak,\nKe Li, Kenneth Heafield, Kevin Stone, Khalid El-Arini, Krithika Iyer, Kshitiz Malik, Kuenley Chiu, Kunal Bhalla,\nKushal Lakhotia, Lauren Rantala-Yeary, Laurens van der Maaten, Lawrence Chen, Liang Tan, Liz Jenkins, Louis\nMartin, Lovish Madaan, Lubo Malo, Lukas Blecher, Lukas Landzaat, Luke de Oliveira, Madeline Muzzi, Mahesh\nPasupuleti, Mannat Singh, Manohar Paluri, Marcin Kardas, Maria Tsimpoukelli, Mathew Oldham, Mathieu Rita,\nMaya Pavlova, Melanie Kambadur, Mike Lewis, Min Si, Mitesh Kumar Singh, Mona Hassan, Naman Goyal,\nNarjes Torabi, Nikolay Bashlykov, Nikolay Bogoychev, Niladri Chatterji, Ning Zhang, Olivier Duchenne, Onur\nÇelebi, Patrick Alrassy, Pengchuan Zhang, Pengwei Li, Petar Vasic, Peter Weng, Prajjwal Bhargava, Pratik\nDubal, Praveen Krishnan, Punit Singh Koura, Puxin Xu, Qing He, Qingxiao Dong, Ragavan Srinivasan, Raj\nGanapathy, Ramon Calderer, Ricardo Silveira Cabral, Robert Stojnic, Roberta Raileanu, Rohan Maheswari, Rohit\nGirdhar, Rohit Patel, Romain Sauvestre, Ronnie Polidoro, Roshan Sumbaly, Ross Taylor, Ruan Silva, Rui Hou,\nRui Wang, Saghar Hosseini, Sahana Chennabasappa, Sanjay Singh, Sean Bell, Seohyun Sonia Kim, Sergey\nEdunov, Shaoliang Nie, Sharan Narang, Sharath Raparthy, Sheng Shen, Shengye Wan, Shruti Bhosale, Shun\nZhang, Simon Vandenhende, Soumya Batra, Spencer Whitman, Sten Sootla, Stephane Collot, Suchin Gururangan,\nSydney Borodinsky, Tamar Herman, Tara Fowler, Tarek Sheasha, Thomas Georgiou, Thomas Scialom, Tobias\nSpeckbacher, Todor Mihaylov, Tong Xiao, Ujjwal Karn, Vedanuj Goswami, Vibhor Gupta, Vignesh Ramanathan,\nViktor Kerkez, Vincent Gonguet, Virginie Do, Vish Vogeti, Vítor Albiero, Vladan Petrovic, Weiwei Chu, Wenhan\nXiong, Wenyin Fu, Whitney Meers, Xavier Martinet, Xiaodong Wang, Xiaofang Wang, Xiaoqing Ellen Tan,\nXide Xia, Xinfeng Xie, Xuchao Jia, Xuewei Wang, Yaelle Goldschlag, Yashesh Gaur, Yasmine Babaei, Yi Wen,\nYiwen Song, Yuchen Zhang, Yue Li, Yuning Mao, Zacharie Delpierre Coudert, Zheng Yan, Zhengxing Chen,\nZoe Papakipos, Aaditya Singh, Aayushi Srivastava, Abha Jain, Adam Kelsey, Adam Shajnfeld, Adithya Gangidi,\nAdolfo Victoria, Ahuva Goldstand, Ajay Menon, Ajay Sharma, Alex Boesenberg, Alexei Baevski, Allie Feinstein,\nAmanda Kallet, Amit Sangani, Amos Teo, Anam Yunus, Andrei Lupu, Andres Alvarado, Andrew Caples, Andrew\nGu, Andrew Ho, Andrew Poulton, Andrew Ryan, Ankit Ramchandani, Annie Dong, Annie Franco, Anuj Goyal,\nAparajita Saraf, Arkabandhu Chowdhury, Ashley Gabriel, Ashwin Bharambe, Assaf Eisenman, Azadeh Yazdan,\nBeau James, Ben Maurer, Benjamin Leonhardi, Bernie Huang, Beth Loyd, Beto De Paola, Bhargavi Paranjape,\n29\n\nBing Liu, Bo Wu, Boyu Ni, Braden Hancock, Bram Wasti, Brandon Spence, Brani Stojkovic, Brian Gamido,\nBritt Montalvo, Carl Parker, Carly Burton, Catalina Mejia, Ce Liu, Changhan Wang, Changkyu Kim, Chao\nZhou, Chester Hu, Ching-Hsiang Chu, Chris Cai, Chris Tindal, Christoph Feichtenhofer, Cynthia Gao, Damon\nCivin, Dana Beaty, Daniel Kreymer, Daniel Li, David Adkins, David Xu, Davide Testuggine, Delia David,\nDevi Parikh, Diana Liskovich, Didem Foss, Dingkang Wang, Duc Le, Dustin Holland, Edward Dowling, Eissa\nJamil, Elaine Montgomery, Eleonora Presani, Emily Hahn, Emily Wood, Eric-Tuan Le, Erik Brinkman, Esteban\nArcaute, Evan Dunbar, Evan Smothers, Fei Sun, Felix Kreuk, Feng Tian, Filippos Kokkinos, Firat Ozgenel,\nFrancesco Caggioni, Frank Kanayet, Frank Seide, Gabriela Medina Florez, Gabriella Schwarz, Gada Badeer,\nGeorgia Swee, Gil Halpern, Grant Herman, Grigory Sizov, Guangyi, Zhang, Guna Lakshminarayanan, Hakan\nInan, Hamid Shojanazeri, Han Zou, Hannah Wang, Hanwen Zha, Haroun Habeeb, Harrison Rudolph, Helen\nSuk, Henry Aspegren, Hunter Goldman, Hongyuan Zhan, Ibrahim Damlaj, Igor Molybog, Igor Tufanov, Ilias\nLeontiadis, Irina-Elena Veliche, Itai Gat, Jake Weissman, James Geboski, James Kohli, Janice Lam, Japhet Asher,\nJean-Baptiste Gaya, Jeff Marcus, Jeff Tang, Jennifer Chan, Jenny Zhen, Jeremy Reizenstein, Jeremy Teboul,\nJessica Zhong, Jian Jin, Jingyi Yang, Joe Cummings, Jon Carvill, Jon Shepard, Jonathan McPhie, Jonathan Torres,\nJosh Ginsburg, Junjie Wang, Kai Wu, Kam Hou U, Karan Saxena, Kartikay Khandelwal, Katayoun Zand, Kathy\nMatosich, Kaushik Veeraraghavan, Kelly Michelena, Keqian Li, Kiran Jagadeesh, Kun Huang, Kunal Chawla,\nKyle Huang, Lailin Chen, Lakshya Garg, Lavender A, Leandro Silva, Lee Bell, Lei Zhang, Liangpeng Guo,\nLicheng Yu, Liron Moshkovich, Luca Wehrstedt, Madian Khabsa, Manav Avalani, Manish Bhatt, Martynas\nMankus, Matan Hasson, Matthew Lennie, Matthias Reso, Maxim Groshev, Maxim Naumov, Maya Lathi, Meghan\nKeneally, Miao Liu, Michael L. Seltzer, Michal Valko, Michelle Restrepo, Mihir Patel, Mik Vyatskov, Mikayel\nSamvelyan, Mike Clark, Mike Macey, Mike Wang, Miquel Jubert Hermoso, Mo Metanat, Mohammad Rastegari,\nMunish Bansal, Nandhini Santhanam, Natascha Parks, Natasha White, Navyata Bawa, Nayan Singhal, Nick\nEgebo, Nicolas Usunier, Nikhil Mehta, Nikolay Pavlovich Laptev, Ning Dong, Norman Cheng, Oleg Chernoguz,\nOlivia Hart, Omkar Salpekar, Ozlem Kalinli, Parkin Kent, Parth Parekh, Paul Saab, Pavan Balaji, Pedro Rittner,\nPhilip Bontrager, Pierre Roux, Piotr Dollar, Polina Zvyagina, Prashant Ratanchandani, Pritish Yuvraj, Qian Liang,\nRachad Alao, Rachel Rodriguez, Rafi Ayub, Raghotham Murthy, Raghu Nayani, Rahul Mitra, Rangaprabhu\nParthasarathy, Raymond Li, Rebekkah Hogan, Robin Battey, Rocky Wang, Russ Howes, Ruty Rinott, Sachin\nMehta, Sachin Siby, Sai Jayesh Bondu, Samyak Datta, Sara Chugh, Sara Hunt, Sargun Dhillon, Sasha Sidorov,\nSatadru Pan, Saurabh Mahajan, Saurabh Verma, Seiji Yamamoto, Sharadh Ramaswamy, Shaun Lindsay, Shaun\nLindsay, Sheng Feng, Shenghao Lin, Shengxin Cindy Zha, Shishir Patil, Shiva Shankar, Shuqiang Zhang, Shuqiang\nZhang, Sinong Wang, Sneha Agarwal, Soji Sajuyigbe, Soumith Chintala, Stephanie Max, Stephen Chen, Steve\nKehoe, Steve Satterfield, Sudarshan Govindaprasad, Sumit Gupta, Summer Deng, Sungmin Cho, Sunny Virk,\nSuraj Subramanian, Sy Choudhury, Sydney Goldman, Tal Remez, Tamar Glaser, Tamara Best, Thilo Koehler,\nThomas Robinson, Tianhe Li, Tianjun Zhang, Tim Matthews, Timothy Chou, Tzook Shaked, Varun Vontimitta,\nVictoria Ajayi, Victoria Montanez, Vijai Mohan, Vinay Satish Kumar, Vishal Mangla, Vlad Ionescu, Vlad Poenaru,\nVlad Tiberiu Mihailescu, Vladimir Ivanov, Wei Li, Wenchen Wang, Wenwen Jiang, Wes Bouaziz, Will Constable,\nXiaocheng Tang, Xiaojian Wu, Xiaolan Wang, Xilun Wu, Xinbo Gao, Yaniv Kleinman, Yanjun Chen, Ye Hu,\nYe Jia, Ye Qi, Yenda Li, Yilin Zhang, Ying Zhang, Yossi Adi, Youngjin Nam, Yu, Wang, Yu Zhao, Yuchen Hao,\nYundi Qian, Yunlu Li, Yuzi He, Zach Rait, Zachary DeVito, Zef Rosnbrick, Zhaoduo Wen, Zhenyu Yang, Zhiwei\nZhao, and Zhiyu Ma. The llama 3 herd of models. arXiv preprint arXiv:2407.21783, 2024.\n[16] Dan Hendrycks, Collin Burns, Steven Basart, Andy Zou, Mantas Mazeika, Dawn Song, and Jacob Steinhardt.\nMeasuring Massive Multitask Language Understanding. arXiv preprint arXiv:2009.03300, 2020.\n[17] Dan Hendrycks, Collin Burns, Saurav Kadavath, Akul Arora, Steven Basart, Eric Tang, Dawn Song, and Jacob\nSteinhardt. Measuring Mathematical Problem Solving With the MATH Dataset. Advances in Neural Information\nProcessing Systems, 2021.\n[18] Xanh Ho, Anh-Khoa Duong Nguyen, Saku Sugawara, and Akiko Aizawa. Constructing a multi-hop QA dataset\nfor comprehensive evaluation of reasoning steps. In Donia Scott, Nuria Bel, and Chengqing Zong, editors,\nProceedings of the 28th International Conference on Computational Linguistics, pages 6609–6625, Barcelona,\nSpain (Online), December 2020. International Committee on Computational Linguistics.\n[19] Jordan Hoffmann, Sebastian Borgeaud, Arthur Mensch, Elena Buchatskaya, Trevor Cai, Eliza Rutherford, Diego\nde las Casas, Lisa Anne Hendricks, Johannes Welbl, Aidan Clark, Tom Hennigan, Eric Noland, Katherine Millican,\nGeorge van den Driessche, Bogdan Damoc, Aurelia Guy, Simon Osindero, Karen Simonyan, Erich Elsen, Oriol\nVinyals, Jack William Rae, and Laurent Sifre. An empirical analysis of compute-optimal large language model\ntraining. In Alice H. Oh, Alekh Agarwal, Danielle Belgrave, and Kyunghyun Cho, editors, Advances in Neural\nInformation Processing Systems, 2022.\n30\n\n[20] Luyang Huang, Shuyang Cao, Nikolaus Parulian, Heng Ji, and Lu Wang. Efficient attentions for long document\nsummarization. In Kristina Toutanova, Anna Rumshisky, Luke Zettlemoyer, Dilek Hakkani-Tur, Iz Beltagy, Steven\nBethard, Ryan Cotterell, Tanmoy Chakraborty, and Yichao Zhou, editors, Proceedings of the 2021 Conference of\nthe North American Chapter of the Association for Computational Linguistics: Human Language Technologies,\npages 1419–1436, Online, June 2021. Association for Computational Linguistics.\n[21] Ziyan Jiang, Xueguang Ma, and Wenhu Chen. LongRAG: Enhancing Retrieval-Augmented Generation with\nLong-context LLMs. arXiv preprint arXiv:2406.15319, 2024.\n[22] Mandar Joshi, Eunsol Choi, Daniel Weld, and Luke Zettlemoyer. TriviaQA: A large scale distantly supervised\nchallenge dataset for reading comprehension. In Regina Barzilay and Min-Yen Kan, editors, Proceedings of\nthe 55th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages\n1601–1611, Vancouver, Canada, July 2017. Association for Computational Linguistics.\n[23] Gregory Kamradt. LLMTest Needle In A Haystack - Pressure Testing LLMs. https://github.com/gkamradt/\nLLMTest_NeedleInAHaystack, 2023.\n[24] Jared Kaplan, Sam McCandlish, Tom Henighan, Tom B. Brown, Benjamin Chess, Rewon Child, Scott Gray, Alec\nRadford, Jeffrey Wu, and Dario Amodei. Scaling laws for neural language models, 2020.\n[25] Tomáš Koˇcisk`y, Jonathan Schwarz, Phil Blunsom, Chris Dyer, Karl Moritz Hermann, Gábor Melis, and Ed-\nward Grefenstette. The narrativeqa reading comprehension challenge. Transactions of the Association for\nComputational Linguistics, 6:317–328, 2018.\n[26] Tom Kwiatkowski, Jennimaria Palomaki, Olivia Redfield, Michael Collins, Ankur Parikh, Chris Alberti, Danielle\nEpstein, Illia Polosukhin, Jacob Devlin, Kenton Lee, et al. Natural questions: a benchmark for question answering\nresearch. Transactions of the Association for Computational Linguistics, 7:453–466, 2019.\n[27] Aitor Lewkowycz, Anders Andreassen, David Dohan, Ethan Dyer, Henryk Michalewski, Vinay Ramasesh,\nAmbrose Slone, Cem Anil, Imanol Schlag, Theo Gutman-Solo, Yuhuai Wu, Behnam Neyshabur, Guy Gur-Ari, and\nVedant Misra. Solving Quantitative Reasoning Problems with Language Models. Advances in Neural Information\nProcessing Systems, 2022.\n[28] LG AI Ethics Principles. https://www.lgresearch.ai/about/vision#ethics.\n[29] Tianle Li, Wei-Lin Chiang, Evan Frick, Lisa Dunlap, Tianhao Wu, Banghua Zhu, Joseph E. Gonzalez, and Ion\nStoica. From Crowdsourced Data to High-Quality Benchmarks: Arena-Hard and BenchBuilder Pipeline. arXiv\npreprint arXiv:2406.11939, 2024.\n[30] Xin Li and Dan Roth. Learning question classifiers. In COLING 2002: The 19th International Conference on\nComputational Linguistics, 2002.\n[31] Jiawei Liu, Chunqiu Steven Xia, Yuyao Wang, and LINGMING ZHANG. Is your code generated by chatGPT\nreally correct? rigorous evaluation of large language models for code generation. In Thirty-seventh Conference\non Neural Information Processing Systems, 2023.\n[32] Michael McCloskey and Neal J. Cohen. Catastrophic Interference in Connectionist Networks: The Sequential\nLearning Problem. Psychology of Learning and Motivation, 24:109–165, 1989.\n[33] Yu Meng, Mengzhou Xia, and Danqi Chen. SimPO: Simple Preference Optimization with a Reference-Free\nReward. arXiv preprint arXiv:2405.14734, 2024.\n[34] Meta. Llama 3.2: Revolutionizing edge AI and vision with open, customizable models, 2024.\n[35] Korean Large Language Model Trustworthiness Benchmark Data. https://aihub.or.kr/aihubdata/data/\nview.do?currMenu=115&topMenu=100&aihubDataSe=data&dataSetSn=71760.\n[36] OpenAI. GPT-4 Technical Report, 2023.\n[37] Jeonghwan Park. LogicKor. 2024.\n[38] Rafael Rafailov, Yaswanth Chittepu, Ryan Park, Harshit Sikchi, Joey Hejna, W. Bradley Knox, Chelsea Finn,\nand Scott Niekum. Scaling laws for reward model overoptimization in direct alignment algorithms. In The\nThirty-eighth Annual Conference on Neural Information Processing Systems, 2024.\n31\n\n[39] Rafael Rafailov, Archit Sharma, Eric Mitchell, Stefano Ermon, Christopher D. Manning, and Chelsea Finn. Direct\nPreference Optimization: Your Language Model is Secretly a Reward Model. Advances in Neural Information\nProcessing Systems, 2024.\n[40] David Rein, Betty Li Hou, Asa Cooper Stickland, Jackson Petty, Richard Yuanzhe Pang, Julien Dirani, Julian\nMichael, and Samuel R. Bowman. GPQA: A Graduate-Level Google-Proof Q&A Benchmark. arXiv preprint\narXiv:2311.12022, 2023.\n[41] LG AI Research. EXAONE 3.0 7.8B Instruction Tuned Language Model. arXiv preprint arXiv:2408.03541, 2024.\n[42] LG AI Research. KoMT-Bench. https://huggingface.co/datasets/LGAI-EXAONE/KoMT-Bench, 2024.\n[43] Martin Riddell, Ansong Ni, and Arman Cohan.\nQuantifying contamination in evaluating code generation\ncapabilities of language models. arXiv preprint arXiv:2403.04811, 2024.\n[44] Noam Shazeer. GLU Variants Improve Transformer. arXiv preprint arXiv:2002.05202, 2020.\n[45] Guijin Son, Hanwool Albert Lee, Sungdong Kim, Seungone Kim, Niklas Muennighoff, Taekyoon Choi, Cheonbok\nPark, Kang Min Yoo, and Stella Biderman. KMMLU: Measuring Massive Multitask Language Understanding in\nKorean. arXiv preprint arXiv:2402.11548, 2024.\n[46] Jianlin Su, Yu Lu, Shengfeng Pan, Ahmed Murtadha, Bo Wen, and Yunfeng Liu. RoFormer: Enhanced Transformer\nwith Rotary Position Embedding. Neurocomputing, 568:127063, 2024.\n[47] Mirac Suzgun, Nathan Scales, Nathanael Schärli, Sebastian Gehrmann, Yi Tay, Hyung Won Chung, Aakanksha\nChowdhery, Quoc V Le, Ed H Chi, Denny Zhou, , and Jason Wei. Challenging BIG-Bench Tasks and Whether\nChain-of-Thought Can Solve Them. arXiv preprint arXiv:2210.09261, 2022.\n[48] Gemma Team, Morgane Riviere, Shreya Pathak, Pier Giuseppe Sessa, Cassidy Hardin, Surya Bhupatiraju, Léonard\nHussenot, Thomas Mesnard, Bobak Shahriari, Alexandre Ramé, Johan Ferret, Peter Liu, Pouya Tafti, Abe Friesen,\nMichelle Casbon, Sabela Ramos, Ravin Kumar, Charline Le Lan, Sammy Jerome, Anton Tsitsulin, Nino Vieillard,\nPiotr Stanczyk, Sertan Girgin, Nikola Momchev, Matt Hoffman, Shantanu Thakoor, Jean-Bastien Grill, Behnam\nNeyshabur, Olivier Bachem, Alanna Walton, Aliaksei Severyn, Alicia Parrish, Aliya Ahmad, Allen Hutchison,\nAlvin Abdagic, Amanda Carl, Amy Shen, Andy Brock, Andy Coenen, Anthony Laforge, Antonia Paterson, Ben\nBastian, Bilal Piot, Bo Wu, Brandon Royal, Charlie Chen, Chintu Kumar, Chris Perry, Chris Welty, Christopher A.\nChoquette-Choo, Danila Sinopalnikov, David Weinberger, Dimple Vijaykumar, Dominika Rogozi´nska, Dustin\nHerbison, Elisa Bandy, Emma Wang, Eric Noland, Erica Moreira, Evan Senter, Evgenii Eltyshev, Francesco\nVisin, Gabriel Rasskin, Gary Wei, Glenn Cameron, Gus Martins, Hadi Hashemi, Hanna Klimczak-Pluci´nska,\nHarleen Batra, Harsh Dhand, Ivan Nardini, Jacinda Mein, Jack Zhou, James Svensson, Jeff Stanway, Jetha Chan,\nJin Peng Zhou, Joana Carrasqueira, Joana Iljazi, Jocelyn Becker, Joe Fernandez, Joost van Amersfoort, Josh\nGordon, Josh Lipschultz, Josh Newlan, Ju yeong Ji, Kareem Mohamed, Kartikeya Badola, Kat Black, Katie\nMillican, Keelin McDonell, Kelvin Nguyen, Kiranbir Sodhia, Kish Greene, Lars Lowe Sjoesund, Lauren Usui,\nLaurent Sifre, Lena Heuermann, Leticia Lago, Lilly McNealus, Livio Baldini Soares, Logan Kilpatrick, Lucas\nDixon, Luciano Martins, Machel Reid, Manvinder Singh, Mark Iverson, Martin Görner, Mat Velloso, Mateo\nWirth, Matt Davidow, Matt Miller, Matthew Rahtz, Matthew Watson, Meg Risdal, Mehran Kazemi, Michael\nMoynihan, Ming Zhang, Minsuk Kahng, Minwoo Park, Mofi Rahman, Mohit Khatwani, Natalie Dao, Nenshad\nBardoliwalla, Nesh Devanathan, Neta Dumai, Nilay Chauhan, Oscar Wahltinez, Pankil Botarda, Parker Barnes,\nPaul Barham, Paul Michel, Pengchong Jin, Petko Georgiev, Phil Culliton, Pradeep Kuppala, Ramona Comanescu,\nRamona Merhej, Reena Jana, Reza Ardeshir Rokni, Rishabh Agarwal, Ryan Mullins, Samaneh Saadat, Sara Mc\nCarthy, Sarah Cogan, Sarah Perrin, Sébastien M. R. Arnold, Sebastian Krause, Shengyang Dai, Shruti Garg, Shruti\nSheth, Sue Ronstrom, Susan Chan, Timothy Jordan, Ting Yu, Tom Eccles, Tom Hennigan, Tomas Kocisky, Tulsee\nDoshi, Vihan Jain, Vikas Yadav, Vilobh Meshram, Vishal Dharmadhikari, Warren Barkley, Wei Wei, Wenming Ye,\nWoohyun Han, Woosuk Kwon, Xiang Xu, Zhe Shen, Zhitao Gong, Zichuan Wei, Victor Cotruta, Phoebe Kirk,\nAnand Rao, Minh Giang, Ludovic Peran, Tris Warkentin, Eli Collins, Joelle Barral, Zoubin Ghahramani, Raia\nHadsell, D. Sculley, Jeanine Banks, Anca Dragan, Slav Petrov, Oriol Vinyals, Jeff Dean, Demis Hassabis, Koray\nKavukcuoglu, Clement Farabet, Elena Buchatskaya, Sebastian Borgeaud, Noah Fiedel, Armand Joulin, Kathleen\nKenealy, Robert Dadashi, and Alek Andreev. Gemma 2: Improving Open Language Models at a Practical Size.\narXiv preprint arXiv:2408.00118, 2024.\n[49] Qwen Team. Qwen2.5: A Party of Foundation Models, September 2024.\n32\n\n[50] Harsh Trivedi, Niranjan Balasubramanian, Tushar Khot, and Ashish Sabharwal. MuSiQue: Multihop questions\nvia single-hop question composition. Transactions of the Association for Computational Linguistics, 10:539–554,\n2022.\n[51] Changhan Wang, Kyunghyun Cho, and Jiatao Gu. Neural Machine Translation with Byte-Level Subwords. In\nProceedings of the AAAI conference on artificial intelligence, volume 34, pages 9154–9160, 2020.\n[52] Fali Wang, Zhiwei Zhang, Xianren Zhang, Zongyu Wu, Tzuhao Mo, Qiuhao Lu, Wanjing Wang, Rui Li, Junjie Xu,\nXianfeng Tang, Qi He, Yao Ma, Ming Huang, and Suhang Wang. A Comprehensive Survey of Small Language\nModels in the Era of Large Language Models: Techniques, Enhancements, Applications, Collaboration with\nLLMs, and Trustworthiness. arXiv preprint arXiv:arXiv:2411.03350, 2024.\n[53] Jason Wei, Maarten Bosma, Vincent Y. Zhao, Kelvin Guu, Adams Wei Yu, Brian Lester, Nan Du, Andrew M. Dai,\nand Quoc V. Le. Finetuned Language Models Are Zero-Shot Learners. arXiv preprint arXiv:2109.01652, 2021.\n[54] Colin White, Samuel Dooley, Manley Roberts, Arka Pal, Ben Feuer, Siddhartha Jain, Ravid Shwartz-Ziv, Neel\nJain, Khalid Saifullah, Siddartha Naidu, Chinmay Hegde, Yann LeCun, Tom Goldstein, Willie Neiswanger, and\nMicah Goldblum. LiveBench: A Challenging, Contamination-Free LLM Benchmark, 2024.\n[55] Ruijie Xu, Zengzhi Wang, Run-Ze Fan, and Pengfei Liu. Benchmarking benchmark leakage in large language\nmodels. arXiv preprint arXiv:2404.18824, 2024.\n[56] Zhilin Yang, Peng Qi, Saizheng Zhang, Yoshua Bengio, William Cohen, Ruslan Salakhutdinov, and Christopher D.\nManning. HotpotQA: A dataset for diverse, explainable multi-hop question answering. In Ellen Riloff, David Chi-\nang, Julia Hockenmaier, and Jun’ichi Tsujii, editors, Proceedings of the 2018 Conference on Empirical Methods\nin Natural Language Processing, pages 2369–2380, Brussels, Belgium, October-November 2018. Association for\nComputational Linguistics.\n[57] Zhilin Yang, Peng Qi, Saizheng Zhang, Yoshua Bengio, William Cohen, Ruslan Salakhutdinov, and Christopher D\nManning. Hotpotqa: A dataset for diverse, explainable multi-hop question answering. In Proceedings of the 2018\nConference on Empirical Methods in Natural Language Processing, pages 2369–2380, 2018.\n[58] Weihao Zeng, Can Xu, Yingxiu Zhao, Jianguang Lou, and Weizhu Chen. Automatic Instruction Evolving for\nLarge Language Models. In Proceedings of the 2024 Conference on Empirical Methods in Natural Language\nProcessing, pages 6998–7018, 2024.\n[59] Lianmin Zheng, Wei-Lin Chiang, Ying Sheng, Siyuan Zhuang, Zhanghao Wu, Yonghao Zhuang, Zi Lin, Zhuohan\nLi, Dacheng Li, Eric P. Xing, Haotong Zhang, Joseph E. Gonzalez, and Ion Stoica. Judging LLM-as-a-judge with\nMT-Bench and Chatbot Arena. Advances in Neural Information Processing Systems, 36:46595–46623, 2023.\n[60] Ming Zhong, Da Yin, Tao Yu, Ahmad Zaidi, Mutethia Mutuma, Rahul Jha, Ahmed Hassan Awadallah, Asli\nCelikyilmaz, Yang Liu, Xipeng Qiu, and Dragomir Radev. QMSum: A new benchmark for query-based multi-\ndomain meeting summarization. In Kristina Toutanova, Anna Rumshisky, Luke Zettlemoyer, Dilek Hakkani-Tur,\nIz Beltagy, Steven Bethard, Ryan Cotterell, Tanmoy Chakraborty, and Yichao Zhou, editors, Proceedings of\nthe 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human\nLanguage Technologies, pages 5905–5921, Online, June 2021. Association for Computational Linguistics.\n[61] Jeffrey Zhou, Tianjian Lu, Swaroop Mishra, Siddhartha Brahma, Sujoy Basu, Yi Luan, Denny Zhou, and Le Hou.\nInstruction-Following Evaluation for Large Language Models. arXiv preprint arXiv:2311.07911, 2023.\n33\n"
    }
  ]
}