{
  "1-5 (Architecture)": "The quotes describe DeepSeek-V3 as “a strong Mixture-of-Experts (MoE) language model with 671B total parameters with 37B activated for each token.”  For efficient inference and cost-effective training the model “adopts Multi-head Latent Attention (MLA) and DeepSeekMoE architectures, which were thoroughly validated in DeepSeek-V2.”  Beyond simply inheriting the efficient backbone from the previous version, DeepSeek-V3 “pioneers an auxiliary-loss-free strategy for load balancing,” thereby aiming to avoid the usual performance degradation that comes from encouraging expert-load balance in MoE systems.  The same set of quotes notes that the model also “sets a multi-token prediction training objective for stronger performance.”  One quote reiterates that, “on top of the efficient architecture of DeepSeek-V2, we pioneer an auxiliary-loss-free strategy for load balancing,” emphasizing that the auxiliary-loss-free mechanism is a central architectural innovation rather than merely a training tweak.",
  "1-5 (Architecture)__evidence": [
    {
      "source": "[readme]",
      "quote": "We present DeepSeek-V3, a strong Mixture-of-Experts (MoE) language model with 671B total parameters with 37B activated for each token."
    },
    {
      "source": "[readme]",
      "quote": "To achieve efficient inference and cost-effective training, DeepSeek-V3 adopts Multi-head Latent Attention (MLA) and DeepSeekMoE architectures, which were thoroughly validated in DeepSeek-V2."
    },
    {
      "source": "[readme]",
      "quote": "Furthermore, DeepSeek-V3 pioneers an auxiliary-loss-free strategy for load balancing and sets a multi-token prediction training objective for stronger performance."
    },
    {
      "source": "[readme]",
      "quote": "- On top of the efficient architecture of DeepSeek-V2, we pioneer an auxiliary-loss-free strategy for load balancing, which minimizes the performance degradation that arises from encouraging load balancing."
    }
  ],
  "1-6 (Tokenizer)": "",
  "1-6 (Tokenizer)__evidence": [],
  "2-1 (Hardware)": "Two separate statements quantify the compute used to train DeepSeek-V3 exclusively in terms of H800 GPU hours.  One quote says the model “requires only 2.788M H800 GPU hours for its full training,” while another specifies that “at an economical cost of only 2.664M H800 GPU hours, we complete the pre-training of DeepSeek-V3 on 14.8T tokens.”  No other hardware details (e.g., GPU count, node configuration, or memory) are disclosed in the provided material; the emphasis is solely on the aggregate GPU-hour totals, which place the full training budget in the 2.7–2.8 million H800-hour range.",
  "2-1 (Hardware)__evidence": [
    {
      "source": "[readme]",
      "quote": "Despite its excellent performance, DeepSeek-V3 requires only 2.788M H800 GPU hours for its full training."
    },
    {
      "source": "[readme]",
      "quote": "- At an economical cost of only 2.664M H800 GPU hours, we complete the pre-training of DeepSeek-V3 on 14.8T tokens, producing the currently strongest open-source base model."
    }
  ],
  "2-2 (Software)": "",
  "2-2 (Software)__evidence": []
}