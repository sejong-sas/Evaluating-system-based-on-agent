{
  "1-1 (Weights)": "The DeepSeek-V3 weights are openly hosted on HuggingFace under the repository ‚Äúdeepseek-ai/DeepSeek-V3‚Äù ( ‚Äú| DeepSeek-V3   | ‚Ä¶ | [ü§ó HuggingFace](https://huggingface.co/deepseek-ai/DeepSeek-V3) |‚Äù ).  The public snapshot contains 685 billion parameters in total: 671 billion for the main model plus 14 billion for the Multi-Token-Prediction (MTP) module ( ‚ÄúNOTE: The total size of DeepSeek-V3 models on HuggingFace is 685B, which includes 671B of the Main Model weights and 14B of the Multi-Token Prediction (MTP) Module weights.‚Äù ).  Users are instructed to ‚ÄúDownload the model weights from HuggingFace, and put them into `/path/to/DeepSeek-V3` folder.‚Äù  Only FP8 format checkpoints are provided because ‚ÄúFP8 training is natively adopted in our framework, we only provide FP8 weights.‚Äù  The repository ships the usual weight-management files such as ‚ÄúREADME_WEIGHTS.md‚Äù and sharded tensors like ‚Äúmodel-00001-of-000163.safetensors,‚Äù showing that anyone who can fetch from HuggingFace can obtain the complete set of shards locally.",
  "1-1 (Weights)__evidence": [
    {
      "source": "[readme]",
      "quote": "| DeepSeek-V3   | 671B | 37B |  128K   | [ü§ó HuggingFace](https://huggingface.co/deepseek-ai/DeepSeek-V3)   |"
    },
    {
      "source": "[readme]",
      "quote": "NOTE: The total size of DeepSeek-V3 models on HuggingFace is 685B, which includes 671B of the Main Model weights and 14B of the Multi-Token Prediction (MTP) Module weights."
    },
    {
      "source": "[readme]",
      "quote": "Download the model weights from HuggingFace, and put them into `/path/to/DeepSeek-V3` folder."
    },
    {
      "source": "[readme]",
      "quote": "Since FP8 training is natively adopted in our framework, we only provide FP8 weights."
    },
    {
      "source": "[files]",
      "quote": "README_WEIGHTS.md"
    },
    {
      "source": "[files]",
      "quote": "model-00001-of-000163.safetensors"
    }
  ],
  "1-2 (Code)": "The public code that accompanies DeepSeek-V3 is, at present, inference-oriented.  The maintainers supply a ‚Äú**DeepSeek-Infer Demo**‚Äù described as ‚Äúa simple and lightweight demo for FP8 and BF16 inference.‚Äù  Prospective users start by cloning ‚Äúour DeepSeek-V3 GitHub repository,‚Äù after which they can run the demo.  The repository‚Äôs copyright notice‚Äî‚ÄúCopyright 2023 DeepSeek-AI and The HuggingFace Inc. team. All rights reserved.‚Äù‚Äîclarifies stewardship, and the README notes that the implementation is ‚Äúbased on EleutherAI‚Äôs GPT-NeoX library and the GPT-NeoX and OPT implementations in this library.‚Äù  No quote references public release of pre-training, supervised fine-tuning, or RLHF training scripts; therefore only inference/serving code is confirmed to be available.",
  "1-2 (Code)__evidence": [
    {
      "source": "[readme]",
      "quote": "**DeepSeek-Infer Demo**: We provide a simple and lightweight demo for FP8 and BF16 inference."
    },
    {
      "source": "[readme]",
      "quote": "First, clone our DeepSeek-V3 GitHub repository:"
    },
    {
      "source": "[py_files/modeling_deepseek.py]",
      "quote": "Copyright 2023 DeepSeek-AI and The HuggingFace Inc. team. All rights reserved."
    },
    {
      "source": "[py_files/modeling_deepseek.py]",
      "quote": "This code is based on EleutherAI's GPT-NeoX library and the GPT-NeoX and OPT implementations in this library."
    }
  ],
  "1-3 (License)": "Licensing is split between code and model weights.  According to the repo: ‚ÄúThis code repository is licensed under [the MIT License](LICENSE-CODE). The use of DeepSeek-V3 Base/Chat models is subject to [the Model License](LICENSE-MODEL). DeepSeek-V3 series (including Base and Chat) supports commercial use.‚Äù  Because the statement immediately precedes the MIT text, the license grant itself is reproduced: ‚ÄúPermission is hereby granted, free of charge, to any person obtaining a copy of this software‚Ä¶ to deal in the Software without restriction, including ‚Ä¶ the rights to use, copy, modify, merge, publish, distribute, sublicense, and/or sell copies of the Software‚Ä¶.‚Äù  A second section in the docs reiterates: ‚ÄúThis code repository is licensed under [the MIT License](LICENSE-CODE). The use of DeepSeek-V3 Base/Chat models is subject to [the Model License](LICENSE-MODEL).‚Äù  Thus, (1) all code is MIT, giving broad rights including commercial redistribution; (2) the weights/models have a distinct Model License but are explicitly stated to ‚Äúsupport commercial use.‚Äù  No additional restrictive clauses are quoted for the models.",
  "1-3 (License)__evidence": [
    {
      "source": "[readme]",
      "quote": "This code repository is licensed under [the MIT License](LICENSE-CODE). The use of DeepSeek-V3 Base/Chat models is subject to [the Model License](LICENSE-MODEL). DeepSeek-V3 series (including Base and Chat) supports commercial use."
    },
    {
      "source": "[license_file]",
      "quote": "Permission is hereby granted, free of charge, to any person obtaining a copy of this software and associated documentation files (the \"Software\"), to deal in the Software without restriction, including without limitation the rights to use, copy, modify, merge, publish, distribute, sublicense, and/or sell copies of the Software, and to permit persons to whom the Software is furnished to do so, subject to the following conditions:"
    },
    {
      "source": "[py_files/modeling_deepseek.py]",
      "quote": "Licensed under the Apache License, Version 2.0 (the \"License\"); you may not use this file except in compliance with the License."
    },
    {
      "source": "[license_file]",
      "quote": "p-by-step guidance on Ascend NPUs, please follow the [instructions here](https://modelers.cn/models/MindIE/deepseekv3).\n\n\n## 7. License\nThis code repository is licensed under [the MIT License](LICENSE-CODE). The use of DeepSeek-V3 Base/Chat models is subject to [the Model License](LICENSE-MODEL). DeepSeek-V3 series (including Base and Chat)"
    }
  ],
  "1-4 (Paper)": "DeepSeek-AI provides a dedicated technical report.  The README links directly to the PDF‚Äî‚Äú<a href=\"https://github.com/deepseek-ai/DeepSeek-V3/blob/main/DeepSeek_V3.pdf\"><b>Paper Link</b>üëÅÔ∏è</a>.‚Äù  A full citation is also embedded: ‚Äú@misc{deepseekai2024deepseekv3technicalreport, title={DeepSeek-V3 Technical Report}, author={DeepSeek-AI}, year={2024}, eprint={2412.19437}, archivePrefix={arXiv}, primaryClass={cs.CL}, url={https://arxiv.org/abs/2412.19437} }.‚Äù  Therefore an official, citable document is openly released on arXiv (e-print 2412.19437, 2024) and mirrored in the GitHub repository.",
  "1-4 (Paper)__evidence": [
    {
      "source": "[readme]",
      "quote": "<p align=\"center\">  <a href=\"https://github.com/deepseek-ai/DeepSeek-V3/blob/main/DeepSeek_V3.pdf\"><b>Paper Link</b>üëÅÔ∏è</a> </p>"
    },
    {
      "source": "[readme]",
      "quote": "@misc{deepseekai2024deepseekv3technicalreport,\n      title={DeepSeek-V3 Technical Report}, \n      author={DeepSeek-AI},\n      year={2024},\n      eprint={2412.19437},\n      archivePrefix={arXiv},\n      primaryClass={cs.CL},\n      url={https://arxiv.org/abs/2412.19437}, \n}"
    }
  ]
}