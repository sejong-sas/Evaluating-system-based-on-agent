{
  "4-1 (Pre-training Data)": "All available statements consistently describe a single, very large corpus built specifically for DeepSeek-V3. Across the quotes it is repeatedly emphasized that “DeepSeek-V3” (or “the training corpus for DeepSeek-V3”) is trained on “14.8 T / 14.8 trillion high-quality and diverse tokens.” No other quantity is ever mentioned, and the 14.8 T figure is contrasted with both the earlier DeepSeek-V2 corpus and with external baselines (e.g., Qwen2.5’s 18 T token corpus). The authors explain that, relative to DeepSeek-V2, the V3 corpus was “optimized by enhancing the ratio of mathematical and programming samples” and “expanding multilingual coverage beyond English and Chinese,” suggesting deliberate class-balance adjustments and an intentional broadening of language coverage. The material is always described as “high-quality,” “diverse,” and “tokenized with our tokenizer,” but no specific public data source list, license information, or precise language breakdown is provided in the quoted text. The same 14.8 T figure is referenced when introducing the model (“a large MoE language model with 671 B total parameters and 37 B activated parameters, trained on 14.8 T tokens”) and when outlining the overall training pipeline (“pre-train … on 14.8 trillion diverse and high-quality tokens, followed by Supervised Fine-Tuning and Reinforcement Learning”). Thus, the pre-training data for DeepSeek-V3 can be summarized as a single 14.8-trillion-token, internally processed corpus that deliberately boosts math/code coverage and multilingual diversity compared with the predecessor release while maintaining high quality and low redundancy.",
  "4-2 (Fine-tuning Data)": "Fine-tuning is split into a Supervised Fine-Tuning (SFT) stage lasting two epochs and a subsequent RL stage. The SFT data are explicitly divided into two broad categories:\n• Reasoning data – These cover “mathematics, code competition problems, and logic puzzles.” Every answer in this subset is machine-generated by an internal model called “DeepSeek-R1.”\n• Non-reasoning data – These cover open-ended or creative tasks such as “creative writing, role-play, and simple question answering.” Responses for this partition are produced by “DeepSeek-V2.5” and then “human annotators verify the accuracy and correctness.”\nTraining hyper-parameters are partially disclosed: DeepSeek-V3-Base is “fine-tuned for two epochs … using the cosine decay learning-rate schedule that starts at 5×10⁻⁶ and gradually decreases to 1×10⁻⁶.” The authors reiterate that these SFT and RL steps are applied “on the base model of DeepSeek-V3” to “align it with human preferences and further unlock its potential.” No explicit token counts or public release information for the SFT data are given in the selected quotes, but the creation process is clearly a mixture of synthetic data generation from earlier DeepSeek models followed by human validation for quality control.",
  "4-3 (Reinforcement Learning Data)": "Reinforcement Learning (RL) data for DeepSeek-V3 are constructed around a preference-learning pipeline. The quotes specify that the reward model itself is “trained from the DeepSeek-V3 SFT checkpoints,” meaning it inherits the SFT data distribution as its base. To improve reliability, the team “construct[s] preference data that not only provides the final reward but also includes the chain-of-thought leading to the reward,” indicating that each preference sample stores both the chosen answer and its step-by-step rationale. During policy optimization the authors “adopt Group Relative Policy Optimization (GRPO)”—the same strategy used in DeepSeek-V2—so there is no separate value-function network; instead, the baseline is estimated “from group scores.” Prompts span “coding, math, writing, role-playing, and question answering,” explicitly stressing domain diversity. All these details show that RL data stem from a mixture of SFT-derived preference pairs augmented with chain-of-thought traces, and that training covers a wide set of task types while using the GRPO algorithm to avoid a heavyweight critic.",
  "4-4 (Data Filtering)": "The statements reveal three main filtering / cleaning mechanisms in the DeepSeek-V3 pipeline. (1) Corpus-level de-duplication and balancing: compared with DeepSeek-V2, the team “optimize[s] the pre-training corpus by enhancing the ratio of mathematical and programming samples … while expanding multilingual coverage,” and they further note that “our data processing pipeline is refined to minimize redundancy while maintaining corpus diversity.” Although no numeric thresholds are provided, this indicates an explicit redundancy-reduction pass plus deliberate topical re-weighting. (2) Tokenizer-aware preprocessing: a “new pretokenizer introduces tokens that combine punctuations and line breaks.” The authors caution that this choice “may introduce the token boundary bias … when the model processes multi-line prompts,” implying an awareness of potential evaluation artefacts caused by the new segmentation scheme. (3) Post-RL rejection sampling: “Upon completing the RL training phase, we implement rejection sampling to curate high-quality SFT data for the final model, where the expert models are used as data generation sources.” The goal is to “ensure that the final training data retains the strengths of DeepSeek-R1 while producing responses that are concise and effective.” Together, these points describe a multi-stage filtering strategy: deduplication and topical balancing at corpus build time, tokenizer-level cleaning that merges certain character sequences, and a final rejection-sampling pass that filters newly generated answers before they become part of the model’s last-stage training data.",
  "4-1 (Pre-training Data)__evidence": [
    {
      "source": "[pdf_text]",
      "quote": "During pre-training, we train DeepSeek-V3 on 14.8T high-quality and diverse tokens."
    },
    {
      "source": "[sections/4.1 Data Construction]",
      "quote": "Compared with DeepSeek-V2, we optimize the pre-training corpus by enhancing the ratio of mathematical and programming samples, while expanding multilingual coverage beyond English and Chinese."
    },
    {
      "source": "[sections/4.1 Data Construction]",
      "quote": "Finally, the training corpus for DeepSeek-V3 consists of 14.8T high-quality and diverse tokens in our tokenizer."
    },
    {
      "source": "[abstract]",
      "quote": "We pre-train DeepSeek-V3 on 14.8 trillion diverse and high-quality tokens, followed by Supervised Fine-Tuning and Reinforcement Learning stages to fully harness its capabilities."
    },
    {
      "source": "[sections/Pre-Training]",
      "quote": "During pre-training, we train DeepSeek-V3 on 14.8T high-quality and diverse tokens."
    },
    {
      "source": "[sections/Conclusion]",
      "quote": "In this paper, we introduce DeepSeek-V3, a large MoE language model with 671B total parameters and 37B activated parameters, trained on 14.8T tokens."
    },
    {
      "source": "[sections/Chinese Benchmarks]",
      "quote": "DeepSeek-V3 surpasses Qwen2.5-72B by 16.4 points, despite Qwen2.5 being trained on a larger corpus compromising 18T tokens, which are 20% more than the 14.8T tokens that DeepSeek-V3 is pre-trained on."
    }
  ],
  "4-2 (Fine-tuning Data)__evidence": [
    {
      "source": "[sections/5.1 Supervised Fine-Tuning]",
      "quote": "Reasoning Data. For reasoning-related datasets, including those focused on mathematics, code competition problems, and logic puzzles, we generate the data by leveraging an internal DeepSeek-R1 model."
    },
    {
      "source": "[sections/5.1 Supervised Fine-Tuning]",
      "quote": "Non-Reasoning Data. For non-reasoning data, such as creative writing, role-play, and simple question answering, we utilize DeepSeek-V2.5 to generate responses and enlist human annotators to verify the accuracy and correctness of the data."
    },
    {
      "source": "[sections/Introduction]",
      "quote": "Following this, we conduct post-training, including Supervised Fine-Tuning (SFT) and Reinforcement Learning (RL) on the base model of DeepSeek-V3, to align it with human preferences and further unlock its potential."
    },
    {
      "source": "[sections/Supervised Fine-Tuning]",
      "quote": "We fine-tune DeepSeek-V3-Base for two epochs using the SFT dataset, using the cosine decay learning rate scheduling that starts at 5 × 10−6 and gradually decreases to 1 × 10−6."
    },
    {
      "source": "[sections/Supervised Fine-Tuning]",
      "quote": "Reasoning Data. For reasoning-related datasets, including those focused on mathematics, code competition problems, and logic puzzles, we generate the data by leveraging an internal DeepSeek-R1 model."
    },
    {
      "source": "[sections/Supervised Fine-Tuning]",
      "quote": "For non-reasoning data, such as creative writing, role-play, and simple question answering, we utilize DeepSeek-V2.5 to generate responses and enlist human annotators to verify the accuracy and correctness of the data."
    }
  ],
  "4-3 (Reinforcement Learning Data)__evidence": [
    {
      "source": "[sections/5.2 Reinforcement Learning]",
      "quote": "Similar to DeepSeek-V2 (DeepSeek-AI, 2024c), we adopt Group Relative Policy Optimization (GRPO) (Shao et al., 2024), which foregoes the critic model that is typically with the same size as the policy model, and estimates the baseline from group scores instead. We incorporate prompts from diverse domains, such as coding, math, writing, role-playing, and question answering, during the RL process."
    },
    {
      "source": "[sections/5.2 Reinforcement Learning]",
      "quote": "The reward model is trained from the DeepSeek-V3 SFT checkpoints. To enhance its reliability, we construct preference data that not only provides the final reward but also includes the chain-of-thought leading to the reward."
    },
    {
      "source": "[abstract]",
      "quote": "We pre-train DeepSeek-V3 on 14.8 trillion diverse and high-quality tokens, followed by Supervised Fine-Tuning and Reinforcement Learning stages to fully harness its capabilities."
    },
    {
      "source": "[sections/Introduction]",
      "quote": "Following this, we conduct post-training, including Supervised Fine-Tuning (SFT) and Reinforcement Learning (RL) on the base model of DeepSeek-V3, to align it with human preferences and further unlock its potential."
    },
    {
      "source": "[sections/Reinforcement Learning/Reward Model]",
      "quote": "The reward model is trained from the DeepSeek-V3 SFT checkpoints."
    },
    {
      "source": "[sections/Reinforcement Learning/GRPO]",
      "quote": "Similar to DeepSeek-V2 (DeepSeek-AI, 2024c), we adopt Group Relative Policy Optimization (GRPO) (Shao et al., 2024), which foregoes the critic model that is typically with the same size as the policy model, and estimates the baseline from group scores instead. We incorporate prompts from diverse domains, such as coding, math, writing, role-playing, and question answering, during the RL process."
    }
  ],
  "4-4 (Data Filtering)__evidence": [
    {
      "source": "[sections/4.1 Data Construction]",
      "quote": "Compared with DeepSeek-V2, we optimize the pre-training corpus by enhancing the ratio of mathematical and programming samples, while expanding multilingual coverage beyond English and Chinese. Also, our data processing pipeline is refined to minimize redundancy while maintaining corpus diversity."
    },
    {
      "source": "[sections/4.1 Data Construction]",
      "quote": "In addition, compared with DeepSeek-V2, the new pretokenizer introduces tokens that combine punctuations and line breaks. However, this trick may introduce the token boundary bias (Lundberg, 2023) when the model processes multi-line prompts without terminal line breaks, particularly for few-shot evaluation prompts."
    },
    {
      "source": "[sections/Supervised Fine-Tuning]",
      "quote": "Upon completing the RL training phase, we implement rejection sampling to curate high-quality SFT data for the final model, where the expert models are used as data generation sources. This method ensures that the final training data retains the strengths of DeepSeek-R1 while producing responses that are concise and effective."
    }
  ]
}