{
  "1-1 (Weights)": "The provided material explicitly states that the DeepSeek-V3 weight files are publicly hosted on Hugging Face. A precise breakdown is given: “The total size of DeepSeek-V3 models on Hugging Face is 685B, which includes 671B of the Main Model weights and 14B of the Multi-Token Prediction (MTP) Module weights.”  A concrete download instruction is also supplied: “Download the model weights from Hugging Face, and put them into `/path/to/DeepSeek-V3` folder.”  From these two quotes we can conclude that (a) the weights are available for direct download without mention of any gating or approval process, (b) the storage location is Hugging Face, and (c) users are expected to place the retrieved files in a local directory named “DeepSeek-V3” for subsequent use.  No quote mentions alternative mirrors, checkpoints, or private-access requirements, so the only officially documented distribution channel is Hugging Face.  The quoted size figures (671 B main + 14 B MTP = 685 B) give an exact total footprint, indicating that both the core model and its auxiliary MTP module are shipped together as separate but related weight sets.",
  "1-1 (Weights)__evidence": [
    {
      "source": "[readme]",
      "quote": "The total size of DeepSeek-V3 models on Hugging Face is 685B, which includes 671B of the Main Model weights and 14B of the Multi-Token Prediction (MTP) Module weights."
    },
    {
      "source": "[readme]",
      "quote": "Download the model weights from Hugging Face, and put them into `/path/to/DeepSeek-V3` folder."
    }
  ],
  "1-2 (Code)": "Two sentences reference public code resources, both focused on inference rather than training.  First: “DeepSeek-V3 can be deployed locally using the following hardware and open-source community software:” (the hardware/software list is implied but not quoted).  Second: “1. **DeepSeek-Infer Demo**: We provide a simple and lightweight demo for FP8 and BF16 inference.”  From these, we can infer that (i) deployment code is released, (ii) that code includes an official ‘DeepSeek-Infer Demo’ supporting FP8 and BF16 precision, and (iii) it is intended for local execution.  No quote describes training scripts, data-preparation utilities, or hyper-parameter schedules, so there is no evidence that full training or fine-tuning code is public.  Consequently, only inference/serving code is confirmed open-sourced, while the end-to-end training pipeline remains undisclosed in the quoted material.",
  "1-2 (Code)__evidence": [
    {
      "source": "[readme]",
      "quote": "DeepSeek-V3 can be deployed locally using the following hardware and open-source community software:"
    },
    {
      "source": "[readme]",
      "quote": "1. **DeepSeek-Infer Demo**: We provide a simple and lightweight demo for FP8 and BF16 inference."
    }
  ],
  "1-3 (License)": "Licensing is split between code and model artifacts.  For code, the repository states: “This code repository is licensed under [the MIT License](LICENSE-CODE).”  The MIT excerpt is directly quoted: “Permission is hereby granted, free of charge, to any person obtaining a copy of this software and associated documentation files ... to deal in the Software without restriction, including without limitation the rights to use, copy, modify, merge, publish, distribute, sublicense, and/or sell copies of the Software,” indicating full rights for use (a), modification (b), redistribution (c), and commercial exploitation (d) under MIT.  For the weights and complementary material, a distinct model license applies: “The use of DeepSeek-V3 Base/Chat models is subject to [the Model License](LICENSE-MODEL). DeepSeek-V3 series (including Base and Chat) supports commercial use.”  The model license further clarifies: “Subject to the terms and conditions of this License, DeepSeek hereby grants to You a perpetual, worldwide, non-exclusive, no-charge, royalty-free, irrevocable copyright license to reproduce, prepare, publicly display, publicly perform, sublicense, and distribute the Complementary Material, the Model, and Derivatives of the Model.”  Taken together, these clauses confer broad rights for both code and model, explicitly permitting commercial activities and redistribution, with no ‘research-only’ or ‘non-commercial’ restrictions present in the quoted text.",
  "1-3 (License)__evidence": [
    {
      "source": "[readme]",
      "quote": "This code repository is licensed under [the MIT License](LICENSE-CODE). The use of DeepSeek-V3 Base/Chat models is subject to [the Model License](LICENSE-MODEL). DeepSeek-V3 series (including Base and Chat) supports commercial use."
    },
    {
      "source": "[license_files]",
      "quote": "Permission is hereby granted, free of charge, to any person obtaining a copy of this software and associated documentation files (the \"Software\"), to deal in the Software without restriction, including without limitation the rights to use, copy, modify, merge, publish, distribute, sublicense, and/or sell copies of the Software,"
    },
    {
      "source": "[license_files]",
      "quote": "Subject to the terms and conditions of this License, DeepSeek hereby grants to You a perpetual, worldwide, non-exclusive, no-charge, royalty-free, irrevocable copyright license to reproduce, prepare, publicly display, publicly perform, sublicense, and distribute the Complementary Material, the Model, and Derivatives of the Model."
    }
  ],
  "1-4 (Paper)": "The only direct bibliographic reference reads: “title={DeepSeek-V3 Technical Report},”.  This confirms the existence of an official technical report dedicated to DeepSeek-V3.  It signals that at least one formal document describing the architecture, training, or evaluation details has been authored, titled exactly “DeepSeek-V3 Technical Report.”  No DOI, arXiv link, or publication venue is supplied in the quote, so the precise outlet remains unspecified, but the presence of a technical report implies publicly accessible documentation beyond marketing materials.",
  "1-4 (Paper)__evidence": [
    {
      "source": "[readme]",
      "quote": "title={DeepSeek-V3 Technical Report},"
    }
  ],
  "1-5 (Architecture)": "The quotes describe DeepSeek-V3 as “a strong Mixture-of-Experts (MoE) language model with 671B total parameters with 37B activated for each token.”  For efficient inference and cost-effective training the model “adopts Multi-head Latent Attention (MLA) and DeepSeekMoE architectures, which were thoroughly validated in DeepSeek-V2.”  Beyond simply inheriting the efficient backbone from the previous version, DeepSeek-V3 “pioneers an auxiliary-loss-free strategy for load balancing,” thereby aiming to avoid the usual performance degradation that comes from encouraging expert-load balance in MoE systems.  The same set of quotes notes that the model also “sets a multi-token prediction training objective for stronger performance.”  One quote reiterates that, “on top of the efficient architecture of DeepSeek-V2, we pioneer an auxiliary-loss-free strategy for load balancing,” emphasizing that the auxiliary-loss-free mechanism is a central architectural innovation rather than merely a training tweak.",
  "1-5 (Architecture)__evidence": [
    {
      "source": "[readme]",
      "quote": "We present DeepSeek-V3, a strong Mixture-of-Experts (MoE) language model with 671B total parameters with 37B activated for each token."
    },
    {
      "source": "[readme]",
      "quote": "To achieve efficient inference and cost-effective training, DeepSeek-V3 adopts Multi-head Latent Attention (MLA) and DeepSeekMoE architectures, which were thoroughly validated in DeepSeek-V2."
    },
    {
      "source": "[readme]",
      "quote": "Furthermore, DeepSeek-V3 pioneers an auxiliary-loss-free strategy for load balancing and sets a multi-token prediction training objective for stronger performance."
    },
    {
      "source": "[readme]",
      "quote": "- On top of the efficient architecture of DeepSeek-V2, we pioneer an auxiliary-loss-free strategy for load balancing, which minimizes the performance degradation that arises from encouraging load balancing."
    }
  ],
  "1-6 (Tokenizer)": "",
  "1-6 (Tokenizer)__evidence": [],
  "2-1 (Hardware)": "Two separate statements quantify the compute used to train DeepSeek-V3 exclusively in terms of H800 GPU hours.  One quote says the model “requires only 2.788M H800 GPU hours for its full training,” while another specifies that “at an economical cost of only 2.664M H800 GPU hours, we complete the pre-training of DeepSeek-V3 on 14.8T tokens.”  No other hardware details (e.g., GPU count, node configuration, or memory) are disclosed in the provided material; the emphasis is solely on the aggregate GPU-hour totals, which place the full training budget in the 2.7–2.8 million H800-hour range.",
  "2-1 (Hardware)__evidence": [
    {
      "source": "[readme]",
      "quote": "Despite its excellent performance, DeepSeek-V3 requires only 2.788M H800 GPU hours for its full training."
    },
    {
      "source": "[readme]",
      "quote": "- At an economical cost of only 2.664M H800 GPU hours, we complete the pre-training of DeepSeek-V3 on 14.8T tokens, producing the currently strongest open-source base model."
    }
  ],
  "2-2 (Software)": "",
  "2-2 (Software)__evidence": [],
  "2-3 (API)": "The official material makes it clear that DeepSeek-V3 is exposed to end-users in two complementary, publicly reachable ways. First, there is a web-based, chat-style interface hosted at “chat.deepseek.com,” explicitly advertised as a place where anyone can log in and “chat with DeepSeek-V3.” Second, the developers state that they “provide [an] OpenAI-Compatible API” through the DeepSeek Platform at “platform.deepseek.com.” The wording emphasises that this second option is a true HTTP API (rather than a client library) and that its interface intentionally mirrors the OpenAI schema, implying immediate drop-in usability for tools that already integrate with GPT-style endpoints. Together, these two statements confirm the existence of both an interactive web application and a formal, production-grade, programmatic API for DeepSeek-V3, each documented and made available to the public through DeepSeek’s own hosted services.",
  "2-3 (API)__evidence": [
    {
      "source": "[readme]",
      "quote": "You can chat with DeepSeek-V3 on DeepSeek's official website: [chat.deepseek.com](https://chat.deepseek.com/sign_in)"
    },
    {
      "source": "[readme]",
      "quote": "We also provide OpenAI-Compatible API at DeepSeek Platform: [platform.deepseek.com](https://platform.deepseek.com/)"
    }
  ],
  "3-1 (Pre-training)": "The project description spells out two core quantitative facts about DeepSeek-V3’s pre-training effort. First, the model is trained on “14.8 trillion diverse and high-quality tokens,” signalling an extremely large-scale data regime. Second, the training consumed “2.664 M H800 GPU hours,” providing a concrete measure of compute expenditure and situating the work in an economical-yet-massive bracket for open-source releases. The authors further note that this pre-training phase precedes both supervised fine-tuning and reinforcement learning steps, implying a classic three-stage pipeline where the pre-training stage focuses purely on next-token prediction across that 14.8 T-token corpus. They claim the result of this stage is “the currently strongest open-source base model,” underscoring their view that the pre-training alone already delivers top-tier capability before any downstream adaptation. No other hyper-parameters are revealed, but the explicit token count, GPU-hour cost, and the fact that it is cited as an open-source base checkpoint form the substantive backbone of the disclosed pre-training methodology.",
  "3-1 (Pre-training)__evidence": [
    {
      "source": "[readme]",
      "quote": "We pre-train DeepSeek-V3 on 14.8 trillion diverse and high-quality tokens, followed by Supervised Fine-Tuning and Reinforcement Learning stages to fully harness its capabilities."
    },
    {
      "source": "[readme]",
      "quote": "At an economical cost of only 2.664M H800 GPU hours, we complete the pre-training of DeepSeek-V3 on 14.8T tokens, producing the currently strongest open-source base model."
    }
  ],
  "3-2 (Fine-tuning)": "The documentation states that after completing the 14.8-trillion-token pre-training run, the team subjects DeepSeek-V3 to a “Supervised Fine-Tuning” phase. One key technique highlighted is an “innovative methodology to distill reasoning capabilities from the long-Chain-of-Thought (CoT) model,” specifically drawing that signal from a DeepSeek R1-series model and injecting it into “standard LLMs, particularly DeepSeek-V3.” This indicates a supervised-distillation approach where high-quality, long-form rationales produced by a stronger or more specialised sibling model serve as teaching signals. The authors frame this step as crucial for elevating reasoning performance. Although exact hyper-parameters or dataset sizes for SFT are not supplied, the narrative firmly slots SFT as an intermediate stage between raw pre-training and a subsequent reinforcement-learning phase, giving DeepSeek-V3 both broadly learned linguistic knowledge and distilled, explicit reasoning chains.",
  "3-2 (Fine-tuning)__evidence": [
    {
      "source": "[readme]",
      "quote": "We pre-train DeepSeek-V3 on 14.8 trillion diverse and high-quality tokens, followed by Supervised Fine-Tuning and Reinforcement Learning stages to fully harness its capabilities."
    },
    {
      "source": "[readme]",
      "quote": "We introduce an innovative methodology to distill reasoning capabilities from the long-Chain-of-Thought (CoT) model, specifically from one of the DeepSeek R1 series models, into standard LLMs, particularly DeepSeek-V3."
    }
  ],
  "3-3 (Reinforcement Learning)": "The workflow description explicitly lists “Reinforcement Learning” as the final stage following pre-training and supervised fine-tuning for DeepSeek-V3. While the text does not enumerate algorithms (e.g., PPO, DPO) or reward models, its placement signals a classic RLHF-style pipeline in which the model is further optimised with feedback beyond supervised targets. The simple statement that pre-training is “followed by … Reinforcement Learning stages to fully harness its capabilities” confirms the presence of an RL phase whose goal is to push performance to a higher plateau after SFT. This establishes a three-phase training strategy—pre-training, SFT, and RL—underscoring that DeepSeek-V3’s final released weights incorporate reinforcement-based optimisation on top of the preceding stages.",
  "3-3 (Reinforcement Learning)__evidence": [
    {
      "source": "[readme]",
      "quote": "We pre-train DeepSeek-V3 on 14.8 trillion diverse and high-quality tokens, followed by Supervised Fine-Tuning and Reinforcement Learning stages to fully harness its capabilities."
    }
  ],
  "4-1 (Pre-training Data)": "The publicly released information for DeepSeek-V3’s pre-training stage is extremely concise but still conveys several concrete numerical facts. The authors repeatedly emphasize that the model is trained on “14.8 trillion diverse and high-quality tokens,” explicitly attributing this token count to DeepSeek-V3 itself. They also disclose the computational budget: “only 2.664 million H800 GPU hours” were consumed to complete the pre-training run. These quotations make no mention of the exact data sources, domain breakdowns, licensing status, or geographic/linguistic origins of the corpus; likewise, they supply no list of dataset names or URLs. However, they do frame the corpus qualitatively as “diverse” and “high-quality,” suggesting a deliberate curation strategy even though the details of that strategy remain unstated. Finally, the authors claim that this large-scale training run results in “the currently strongest open-source base model,” implying that the 14.8 T token mixture is central to attaining the model’s reported performance. No additional specifics on data composition, filtering, or provenance are provided in the quoted material.",
  "4-1 (Pre-training Data)__evidence": [
    {
      "source": "[readme]",
      "quote": "We pre-train DeepSeek-V3 on 14.8 trillion diverse and high-quality tokens, followed by Supervised Fine-Tuning and Reinforcement Learning stages to fully harness its capabilities."
    },
    {
      "source": "[readme]",
      "quote": "At an economical cost of only 2.664M H800 GPU hours, we complete the pre-training of DeepSeek-V3 on 14.8T tokens, producing the currently strongest open-source base model."
    }
  ],
  "4-2 (Fine-tuning Data)": "Two separate statements outline the fine-tuning work that follows DeepSeek-V3’s pre-training. First, the team confirms that after the 14.8 T-token pre-training phase the model undergoes “Supervised Fine-Tuning” (SFT) as part of a larger pipeline that also includes a reinforcement-learning stage. Second, they describe “an innovative methodology to distill reasoning capabilities from the long-Chain-of-Thought (CoT) model, specifically from one of the DeepSeek R1 series models, into standard LLMs, particularly DeepSeek-V3.” This indicates that part of the fine-tuning data consists of distilled outputs—or at least supervisory signals—generated by an earlier DeepSeek model capable of long CoT reasoning. Although the quotes do not enumerate dataset sizes, domains, or release status, they do make clear that the SFT corpus is explicitly engineered to transfer stronger reasoning traces from DeepSeek R1 into DeepSeek-V3. No further details—such as exact prompt/response pairs, human-authored instructions, or licensing terms—are provided in the excerpts.",
  "4-2 (Fine-tuning Data)__evidence": [
    {
      "source": "[readme]",
      "quote": "We pre-train DeepSeek-V3 on 14.8 trillion diverse and high-quality tokens, followed by Supervised Fine-Tuning and Reinforcement Learning stages to fully harness its capabilities."
    },
    {
      "source": "[readme]",
      "quote": "We introduce an innovative methodology to distill reasoning capabilities from the long-Chain-of-Thought (CoT) model, specifically from one of the DeepSeek R1 series models, into standard LLMs, particularly DeepSeek-V3."
    }
  ],
  "4-3 (Reinforcement Learning Data)": "The only direct information about DeepSeek-V3’s reinforcement-learning data is its placement in the training pipeline: the model is \"followed by ... Reinforcement Learning stages\" after pre-training and SFT. The quotes do not describe the reward model’s construction, the nature of preference or comparison data, sampling strategies, or the size and provenance of any RLHF/RLAIF datasets. They simply confirm that an RL stage exists and that it is used \"to fully harness [the model’s] capabilities.\" Consequently, all specific attributes—such as the number of comparisons, whether human or synthetic feedback was used, or any public release plans—remain undisclosed in the provided material.",
  "4-3 (Reinforcement Learning Data)__evidence": [
    {
      "source": "[readme]",
      "quote": "We pre-train DeepSeek-V3 on 14.8 trillion diverse and high-quality tokens, followed by Supervised Fine-Tuning and Reinforcement Learning stages to fully harness its capabilities."
    }
  ],
  "4-4 (Data Filtering)": "The supplied quotations contain no explicit statements about data-filtering or cleaning for DeepSeek-V3. There are no references to automated classifiers, heuristic rules, deduplication thresholds, language or domain filters, toxicity or copyright screens, or any numeric acceptance ratios. Accordingly, no summary of the project’s filtering methodology can be drawn from the given text.",
  "4-4 (Data Filtering)__evidence": [],
  "__usage": {
    "fine_tuning": "used",
    "rl": "used"
  }
}