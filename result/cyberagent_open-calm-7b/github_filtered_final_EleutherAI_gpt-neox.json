{
  "1-1 (Weights)": "The project explicitly provides two publicly-downloadable checkpoints for EleutherAI/gpt-neox-20B hosted at the-eye.eu. Users can fetch a “Slim weights” package (≈39 GB) that contains only the tensors needed for inference or downstream fine-tuning and purposefully omits optimizer states. For full reproducibility of pre-training, a much larger “Full weights” archive (≈268 GB) is offered that additionally bundles all optimizer states. No access restrictions, credentials, or gated portals are mentioned in the quotes, implying anyone can retrieve the files directly from the listed HTTP URLs.",
  "1-1 (Weights)__evidence": [
    {
      "source": "[readme]",
      "quote": "[Slim weights](https://the-eye.eu/public/AI/models/GPT-NeoX-20B/slim_weights/) - (No optimizer states, for inference or finetuning, 39GB)"
    },
    {
      "source": "[readme]",
      "quote": "[Full weights](https://the-eye.eu/public/AI/models/GPT-NeoX-20B/full_weights/) - (Including optimizer states, 268GB)"
    }
  ],
  "1-2 (Code)": "Training source code for GPT-NeoX is public. A comment states that “GPT-NeoX is optimized heavily for training only,” signalling the repository is designed around large-scale pre-training rather than mere inference. Checkpoints emitted by this codebase are “not compatible out of the box with other deep learning libraries,” so users are expected to rely on the native implementation. Configuration is managed through YAML: “All of the following can be specified in your .yml config file(s).” The codebase integrates with evaluation tooling via “An adapter to run NeoX models on LM Evaluation Harness.” Core utilities include a function signature “def save_checkpoint(neox_args, iteration, model, optimizer, lr_scheduler)” and logic that raises an error unless the run is using DeepSpeed (“raise ValueError(‘Must be using deepspeed to use neox’)”), showing tight coupling to the DeepSpeed training stack. Internally, the models re-implement GPT-2 style blocks—e.g., a docstring “\"\"\"GPT-2 model.\"\"\"” and a class that is “GPT2Model adapted for pipeline parallelism.” Initialization options such as “single_residual_scaled_normal” replicate GPT-2’s scaled init. Altogether, the repository contains the full pre-training pipeline (configs, parallelism, checkpointing, evaluation adapters) rather than just inference scripts.",
  "1-2 (Code)__evidence": [
    {
      "source": "[readme]",
      "quote": "GPT-NeoX is optimized heavily for training only, and GPT-NeoX model checkpoints are not compatible out of the box with other deep learning libraries."
    },
    {
      "source": "[py_files/configs/gen_docs.py]",
      "quote": "Arguments for gpt-neox. All of the following can be specified in your .yml config file(s):"
    },
    {
      "source": "[py_files/eval_tasks/eval_adapter.py]",
      "quote": "An adapter to run NeoX models on LM Evaluation Harness (https://github.com/EleutherAI/lm-evaluation-harness) tasks."
    },
    {
      "source": "[py_files/megatron/data/data_utils.py]",
      "quote": "def save_checkpoint(neox_args, iteration, model, optimizer, lr_scheduler):"
    },
    {
      "source": "[py_files/megatron/data/data_utils.py]",
      "quote": "else:        raise ValueError(\"Must be using deepspeed to use neox\")"
    },
    {
      "source": "[py_files/megatron/model/gpt2_model.py]",
      "quote": "\"\"\"GPT-2 model.\"\"\""
    },
    {
      "source": "[py_files/megatron/model/gpt2_model.py]",
      "quote": "GPT2Model adapted for pipeline parallelism."
    },
    {
      "source": "[py_files/megatron/model/mamba/mamba.py]",
      "quote": "# use \"single_residual_scaled_normal\" for output_layer_init_method to perform gpt-2 style scaled init as done in Mamba paper."
    }
  ],
  "1-3 (License)": "The repository’s source files repeatedly declare they are “Licensed under the Apache License, Version 2.0,” and the LICENSE header reproduces the standard notice (January 2024). The Apache-2.0 terms grant “a perpetual, worldwide, non-exclusive, no-charge, royalty-free, irrevocable copyright license to reproduce, prepare Derivative Works of, publicly display, publicly perform, sublicense, and distribute the Work and such Derivative Works in Source or Object form.” Each file reminds users they “may not use this file except in compliance with the License” and points to “http://www.apache.org/licenses/LICENSE-2.0.” The text also contains the customary warranty disclaimer (“distributed on an ‘AS IS’ BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND”). One snippet notes that a particular source file is “licensed under the MIT license found in the LICENSE file in the root directory,” indicating isolated components under a more permissive MIT license while the bulk of the project remains Apache-2.0. No phrases such as “non-commercial,” “research-only,” or redistribution restrictions appear; Apache-2.0 allows commercial use, modification, and redistribution provided attribution and license retention are satisfied.",
  "1-3 (License)__evidence": [
    {
      "source": "[readme]",
      "quote": "This repository hosts code that is part of EleutherAI's GPT-NeoX project... Licensed under the Apache License:"
    },
    {
      "source": "[license_files]",
      "quote": "Licensed under the Apache License, Version 2.0 (the \"License\"); you may not use this file except in compliance with the License."
    },
    {
      "source": "[license_files]",
      "quote": "Apache License\n                           Version 2.0, January 2024"
    },
    {
      "source": "[license_files]",
      "quote": "Subject to the terms and conditions of this License, each Contributor hereby grants to You a perpetual, worldwide, non-exclusive, no-charge, royalty-free, irrevocable copyright license to reproduce, prepare Derivative Works of, publicly display, publicly perform, sublicense, and distribute the Work and such Derivative Works in Source or Object form."
    },
    {
      "source": "[py_files/deepy.py]",
      "quote": "Licensed under the Apache License, Version 2.0 (the \"License\"); you may not use this file except in compliance with the License."
    },
    {
      "source": "[py_files/megatron/data/blendable_dataset.py]",
      "quote": "# Licensed under the Apache License, Version 2.0 (the \"License\");"
    },
    {
      "source": "[py_files/megatron/data/blendable_dataset.py]",
      "quote": "# you may not use this file except in compliance with the License."
    },
    {
      "source": "[py_files/megatron/data/indexed_dataset.py]",
      "quote": "# This source code is licensed under the MIT license found in the"
    },
    {
      "source": "[py_files/megatron/data/indexed_dataset.py]",
      "quote": "# LICENSE file in the root directory of this source tree."
    },
    {
      "source": "py_files/megatron/data/online_dataset.py",
      "quote": "Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License."
    },
    {
      "source": "py_files/megatron/data/online_dataset.py",
      "quote": "#     http://www.apache.org/licenses/LICENSE-2.0"
    },
    {
      "source": "[py_files/megatron/gradient_noise_scale/gradient_noise_scale.py]",
      "quote": "# Licensed under the Apache License, Version 2.0 (the \"License\");"
    },
    {
      "source": "[py_files/megatron/gradient_noise_scale/gradient_noise_scale.py]",
      "quote": "# you may not use this file except in compliance with the License."
    },
    {
      "source": "[py_files/megatron/gradient_noise_scale/gradient_noise_scale.py]",
      "quote": "# You may obtain a copy of the License at"
    },
    {
      "source": "[py_files/megatron/gradient_noise_scale/gradient_noise_scale.py]",
      "quote": "#     http://www.apache.org/licenses/LICENSE-2.0"
    },
    {
      "source": "[py_files/megatron/initialize.py]",
      "quote": "# Licensed under the Apache License, Version 2.0 (the \"License\");"
    },
    {
      "source": "[py_files/megatron/model/fused_rope.py]",
      "quote": "# Licensed under the Apache License, Version 2.0 (the \"License\");"
    },
    {
      "source": "[py_files/megatron/model/fused_rope.py]",
      "quote": "# You may not use this file except in compliance with the License."
    },
    {
      "source": "[py_files/megatron/model/fused_rope.py]",
      "quote": "#     http://www.apache.org/licenses/LICENSE-2.0"
    },
    {
      "source": "[py_files/megatron/model/moe_mlp.py]",
      "quote": "# Licensed under the Apache License, Version 2.0 (the \"License\");"
    },
    {
      "source": "[py_files/megatron/model/moe_mlp.py]",
      "quote": "# you may not use this file except in compliance with the License."
    },
    {
      "source": "[py_files/megatron/model/moe_mlp.py]",
      "quote": "# You may obtain a copy of the License at"
    },
    {
      "source": "[py_files/megatron/model/moe_mlp.py]",
      "quote": "#     http://www.apache.org/licenses/LICENSE-2.0"
    },
    {
      "source": "[py_files/megatron/model/moe_mlp.py]",
      "quote": "# Unless required by applicable law or agreed to in writing, software distributed under the License is distributed on an \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied."
    }
  ],
  "1-4 (Paper)": "An official technical report accompanies the model: “GPT-NeoX-20B: An Open-Source Autoregressive Language Model” (arXiv:2204.06745). The citation lists the full EleutherAI author team (Sid Black, Stella Biderman, Eric Hallahan, Quentin Anthony, Leo Gao, Laurence Golding, Horace He, Connor Leahy, Kyle McDonell, Jason Phang, Michael Pieler, Shivanshu Purohit, Laria Reynolds, Jon Tow, Ben Wang, and Samuel Weinbach). The README states: “Technical details about GPT-NeoX-20B can be found in the associated paper,” directing readers to that arXiv link for architecture, training procedure, and benchmark results.",
  "1-4 (Paper)__evidence": [
    {
      "source": "[readme]",
      "quote": "Technical details about GPT-NeoX-20B can be found in [the associated paper](https://arxiv.org/abs/2204.06745)."
    },
    {
      "source": "[readme]",
      "quote": "Sid Black, Stella Biderman, Eric Hallahan, Quentin Anthony, Leo Gao, Laurence Golding, Horace He, Connor Leahy, Kyle McDonell, Jason Phang, Michael Pieler, Shivanshu Purohit, Laria Reynolds, Jon Tow, Ben Wang, and Samuel Weinbach. \"[GPT-NeoX-20B: An Open-Source Autoregressive Language Model](https://arxiv.org/abs/2204.06745).\""
    }
  ],
  "1-5 (Architecture)": "The EleutherAI/gpt-neox codebase exposes its core architectural hyper-parameters directly through the NeoXArgs configuration object. Multiple snippets show that the model expects explicit numeric values for \"num_layers\", \"hidden_size\" and \"num_attention_heads\", and also supports a configurable \"max_position_embeddings\" field. Internal calculations such as \"hidden_size_per_attention_head = mpu.divide(self.neox_args.hidden_size, self.neox_args.num_attention_heads)\" confirm that the transformer follows the usual GPT family layout in which hidden-size is evenly split across attention heads. Several class doc-strings clarify that the implementation is a \"GPT2Model adapted for pipeline parallelism\", indicating that the forward pass is partitioned across devices. GPT-NeoX additionally supports a Mixture-of-Experts variant: \"GPT-NeoX includes support for Dropless Mixture of Experts (DMoE) through the megablocks library\" and the presence of \"self.num_experts = neox_args.moe_num_experts\" shows that expert counts are configurable. Overall, the architecture is a GPT-2-style transformer with user-defined depth, width and head count, optional pipeline parallel execution, and optional DMoE expert layers provided by Megablocks.",
  "1-5 (Architecture)__evidence": [
    {
      "source": "[readme]",
      "quote": "GPT-NeoX includes support for Dropless Mixture of Experts (DMoE) through the `megablocks` library."
    },
    {
      "source": "[py_files/megatron/checkpointing.py]",
      "quote": "\"num_layers\": neox_args.num_layers,"
    },
    {
      "source": "[py_files/megatron/checkpointing.py]",
      "quote": "\"hidden_size\": neox_args.hidden_size,"
    },
    {
      "source": "[py_files/megatron/checkpointing.py]",
      "quote": "\"num_attention_heads\": neox_args.num_attention_heads,"
    },
    {
      "source": "[py_files/configs/gen_docs.py]",
      "quote": "intro_str = \"\"\"Arguments for gpt-neox. All of the following can be specified in your .yml config file(s):\\n\"\"\""
    },
    {
      "source": "[files]",
      "quote": "\"num_layers\": neox_args.num_layers,"
    },
    {
      "source": "[files]",
      "quote": "\"hidden_size\": neox_args.hidden_size,"
    },
    {
      "source": "[files]",
      "quote": "\"num_attention_heads\": neox_args.num_attention_heads,"
    },
    {
      "source": "[files]",
      "quote": "\"max_position_embeddings\": neox_args.max_position_embeddings,"
    },
    {
      "source": "[py_files/megatron/logging.py]",
      "quote": "hidden_size = neox_args.hidden_size"
    },
    {
      "source": "[py_files/megatron/logging.py]",
      "quote": "num_layers = neox_args.num_layers"
    },
    {
      "source": "[py_files/megatron/model/gpt2_model.py]",
      "quote": "\"\"\"GPT-2 model.\"\"\""
    },
    {
      "source": "[py_files/megatron/model/gpt2_model.py]",
      "quote": "\"\"\"GPT2Model adapted for pipeline parallelism."
    },
    {
      "source": "[py_files/megatron/model/gpt2_model.py]",
      "quote": "self.hidden_size = self.neox_args.hidden_size"
    },
    {
      "source": "[py_files/megatron/model/gpt2_model.py]",
      "quote": "hidden_size_per_attention_head = mpu.divide(self.neox_args.hidden_size, self.neox_args.num_attention_heads)"
    },
    {
      "source": "[py_files/megatron/model/moe_mlp.py]",
      "quote": "self.hidden_size = neox_args.hidden_size"
    },
    {
      "source": "[py_files/megatron/model/moe_mlp.py]",
      "quote": "self.num_experts = neox_args.moe_num_experts"
    }
  ],
  "1-6 (Tokenizer)": "The training scripts reference a dedicated \"GPT-NeoX-20B tokenizer\" stored at \"./20B_checkpoints/20B_tokenizer.json\", implying that a custom JSON tokenizer file (compatible with HuggingFace tokenizers) is distributed. At configuration time the field \"tokenizer_type\" must be set via NeoXArgs, and the code also accesses \"vocab_size = neox_args.padded_vocab_size\", meaning the vocabulary can be padded to a round number for tensor parallelism. Documentation advises users to \"download the GPT2 tokenizer vocab and merge files\" before training, so the tokenizer is GPT-2 compatible in format even if the vocab differs.",
  "1-6 (Tokenizer)__evidence": [
    {
      "source": "[readme]",
      "quote": "or a single shard of the pile (`pile_subset`) with the GPT-NeoX-20B tokenizer (assuming you have it saved at `./20B_checkpoints/20B_tokenizer.json`):"
    },
    {
      "source": "[readme]",
      "quote": "Next make sure to download the GPT2 tokenizer vocab, and merge files from the following links:"
    },
    {
      "source": "[py_files/megatron/checkpointing.py]",
      "quote": "\"tokenizer_type\": neox_args.tokenizer_type,"
    },
    {
      "source": "[files]",
      "quote": "\"tokenizer_type\": neox_args.tokenizer_type,"
    },
    {
      "source": "[py_files/megatron/logging.py]",
      "quote": "vocab_size = neox_args.padded_vocab_size"
    }
  ],
  "2-1 (Hardware)": "The library is hardware-agnostic and has successfully launched on a diverse fleet: Slurm, MPI and IBM Job Step Manager clusters, as well as major supercomputers and cloud providers—including AWS, CoreWeave, ORNL Summit, ORNL Frontier, and the LUMI system. This demonstrates that EleutherAI/gpt-neox has been validated from commodity GPU clusters up to world-class exascale machines.",
  "2-1 (Hardware)__evidence": [
    {
      "source": "[readme]",
      "quote": "Uniquely among similar libraries GPT-NeoX supports a wide variety of systems and hardwares, including launching via Slurm, MPI, and the IBM Job Step Manager, and has been run at scale on AWS, CoreWeave, ORNL Summit, ORNL Frontier, LUMI, and others."
    }
  ],
  "2-2 (Software)": "EleutherAI/gpt-neox is tightly coupled with the Megatron-DeepSpeed ecosystem. The maintainers note that it \"leverages many of the same features and technologies as the popular Megatron-DeepSpeed library\". A major upgrade—\"GPT-NeoX 2.0.0\"—was rebuilt \"on the latest DeepSpeed\" and will remain in sync going forward. Historically, versions prior to 3/9/2023 depended on \"DeeperSpeed\" derived from DeepSpeed 0.3.15. Runtime code shows deep integration: \"from megatron.neox_arguments import neox_args, deepspeed_args\"; the call \"deepspeed_main_args = neox_args.get_deepspeed_main_args()\" forwards CLI flags; and safety checks such as \"else: raise ValueError('Must be using deepspeed to use neox')\" enforce the dependency. Training features include advanced activation-checkpointing with \"deepspeed.checkpointing.configure(... partition_activations, contiguous_checkpointing, checkpoint_in_cpu, synchronize_each_layer, profile_backward)\" and model-parallel initialisation through \"mpu.initialize_model_parallel(neox_args.model_parallel_size, topology=topo, fp32_allreduce=neox_args.fp32_allreduce)\". Together these quotes establish that GPT-NeoX trains under PyTorch + DeepSpeed/Megatron libraries with built-in model, data and pipeline parallel capabilities, sophisticated checkpointing, and explicit argument forwarding for reproducibility.",
  "2-2 (Software)__evidence": [
    {
      "source": "[readme]",
      "quote": "GPT-NeoX leverages many of the same features and technologies as the popular Megatron-DeepSpeed library but with substantially increased usability and novel optimizations."
    },
    {
      "source": "[readme]",
      "quote": "We have released GPT-NeoX 2.0.0, an upgraded version built on the latest DeepSpeed which will be regularly synced with going forward."
    },
    {
      "source": "[readme]",
      "quote": "Prior to 3/9/2023, GPT-NeoX relied on DeeperSpeed, which was based on an old version of DeepSpeed (0.3.15)."
    },
    {
      "source": "[py_files/configs/gen_docs.py]",
      "quote": "from megatron.neox_arguments import neox_args, deepspeed_args"
    },
    {
      "source": "[py_files/deepy.py]",
      "quote": "deepspeed_main_args = neox_args.get_deepspeed_main_args()"
    },
    {
      "source": "[files]",
      "quote": "else: raise ValueError(\"Must be using deepspeed to use neox\")"
    },
    {
      "source": "[py_files/megatron/initialize.py]",
      "quote": "deepspeed.checkpointing.configure( mpu, partition_activations=neox_args.partition_activations, contiguous_checkpointing=neox_args.contiguous_checkpointing, num_checkpoints=num_layers, checkpoint_in_cpu=neox_args.checkpoint_in_cpu, synchronize=neox_args.synchronize_each_layer, profile=neox_args.profile_backward, )"
    },
    {
      "source": "[py_files/megatron/initialize.py]",
      "quote": "m mpu.initialize_model_parallel( neox_args.model_parallel_size, topology=topo, fp32_allreduce=neox_args.fp32_allreduce, )"
    },
    {
      "source": "[py_files/megatron/model/moe_mlp.py]",
      "quote": "from megatron.neox_arguments.arguments import NeoXArgs"
    },
    {
      "source": "[py_files/megatron/model/router.py]",
      "quote": "from megatron.neox_arguments.arguments import NeoXArgs"
    }
  ],
  "2-3 (API)": "No sentences in the provided evidence mention any publicly accessible API, endpoint, documentation site, or usage example for GPT-NeoX or GPT-NeoX-20B. Consequently, the quotes supply no information about whether an inference or fine-tuning API exists, how it might be authenticated or invoked, or whether any public or private gateway offers model access. Therefore, nothing can be summarized about an API for EleutherAI/gpt-neox on the basis of the supplied text.",
  "2-3 (API)__evidence": [],
  "3-1 (Pre-training)": "The evidence gives a fragmented but concrete picture of the GPT-NeoX pre-training setup.\n• Model scale and corpus: “GPT-NeoX-20B is a 20 billion parameter autoregressive language model trained on the Pile.” This establishes that the target checkpoint is a 20 B-parameter decoder-only transformer whose raw text corpus is The Pile (≈825 GB of diverse English-centric data).\n• Mixture-of-Experts option: “GPT-NeoX includes support for Dropless Mixture of Experts (DMoE) through the megablocks library.” Although not explicitly stating that the released 20 B model used DMoE, the quote shows the training codebase can enable a MoE variant by integrating the Megablocks kernel library.\n• Configuration interface: “GPT-NeoX parameters are defined in a YAML configuration file which is passed to the deepy.py launcher,” and the CLI help banner starts with “Arguments for gpt-neox. All of the following can be specified in your .yml config file(s).” Together, these lines clarify that every hyper-parameter—model dimension, learning schedule, data paths, optimizer, etc.—is supplied by a YAML spec and consumed by DeepSpeed’s deepy.py wrapper.\n• Dataset pipeline helpers: There is a built-in notion of a “GPT2 style dataset,” yet another line warns that “non gpt2 datasets are not currently unsupported with train_epochs,” implying that when users drive training via the `train_epochs` flag, only the GPT-2 binary-indexed dataset format is accepted; other dataset formats will fail.\n• Initialization utilities: Calls such as `initialize_megatron(neox_args, allow_no_cuda=False)` and a diagnostic print statement “> setting random seeds to {} ...” reveal that the codebase wires into the Megatron-LM framework, sets global RNG seeds for deterministic runs, and can enforce CUDA availability.\n• Training loop lineage: A comment, “From pretrain_gpt2:forward_step()”, indicates that the forward-and-loss computation in GPT-NeoX is directly borrowed or adapted from Megatron’s original `pretrain_gpt2` script.\nCollectively, the quotes depict a Megatron/DeepSpeed-based YAML-driven pipeline that trains a 20 B parameter autoregressive transformer on The Pile, optionally with DMoE, using GPT-2 style indexed datasets, deterministic seeding, and code that inherits from the `pretrain_gpt2` reference implementation.",
  "3-1 (Pre-training)__evidence": [
    {
      "source": "[readme]",
      "quote": "GPT-NeoX-20B is a 20 billion parameter autoregressive language model trained on [the Pile](https://arxiv.org/abs/2101.00027)."
    },
    {
      "source": "[readme]",
      "quote": "GPT-NeoX includes support for Dropless Mixture of Experts (DMoE) through the `megablocks` library."
    },
    {
      "source": "[readme]",
      "quote": "GPT-NeoX parameters are defined in a YAML configuration file which is passed to the deepy.py launcher."
    },
    {
      "source": "[py_files/configs/gen_docs.py]",
      "quote": "intro_str = \"\"\"Arguments for gpt-neox. All of the following can be specified in your .yml config file(s):\n\"\"\""
    },
    {
      "source": "[py_files/megatron/data/gpt2_dataset.py]",
      "quote": "\"\"\"GPT2 style dataset.\"\"\""
    },
    {
      "source": "[py_files/megatron/data/data_utils.py]",
      "quote": "\"non gpt2 datasets are not currently unsupported with train_epochs\""
    },
    {
      "source": "[py_files/megatron/initialize.py]",
      "quote": "def initialize_megatron(neox_args, allow_no_cuda=False):"
    },
    {
      "source": "[py_files/megatron/initialize.py]",
      "quote": "print(\"> setting random seeds to {} ...\".format(neox_args.seed))"
    },
    {
      "source": "[py_files/megatron/model/gpt2_model.py]",
      "quote": "\"\"\"From pretrain_gpt2:forward_step()\"\"\""
    }
  ],
  "3-2 (Fine-tuning)": "The compilation contains no sentences that describe any fine-tuning regimen, objective, data, or reproducible pipeline for GPT-NeoX. There is no mention of supervised instruction tuning, task-specific datasets, hyper-parameters, or scripts dedicated to continued training. Consequently, no fine-tuning-related information can be summarized from the supplied material.",
  "3-2 (Fine-tuning)__evidence": [],
  "3-3 (Reinforcement Learning)": "None of the provided quotes refer to reinforcement-learning-based alignment techniques such as RLHF, RLAIF, DPO, or similar. There are no statements about reward models, preference datasets, PPO or related algorithms, so the evidence set offers no RL information to summarize.",
  "3-3 (Reinforcement Learning)__evidence": [],
  "4-1 (Pre-training Data)": "The single available quote explicitly states that “GPT-NeoX-20B is a 20 billion parameter autoregressive language model trained on [the Pile].” From this, we can conclude that the entirety of the pre-training corpus for the EleutherAI/gpt-neox family model referenced here (GPT-NeoX-20B) is the Pile, a large, heterogeneous, publicly described text dataset. The quote further identifies the model as having 20 billion parameters and characterizes it as an autoregressive language model, but it offers no additional quantitative breakdown (e.g., token counts or source-domain splits) beyond that high-level statement. No other quotes provide further detail on types, quantities, permitted use, or composition of the pre-training data, so the information about pre-training data is limited to the fact that the Pile serves as the sole data source.",
  "4-1 (Pre-training Data)__evidence": [
    {
      "source": "[readme]",
      "quote": "GPT-NeoX-20B is a 20 billion parameter autoregressive language model trained on [the Pile](https://arxiv.org/abs/2101.00027)."
    }
  ],
  "4-2 (Fine-tuning Data)": "No sentences in the supplied quotes mention any fine-tuning datasets, their sources, composition, number of examples, or public availability for EleutherAI/gpt-neox. Consequently, there is no information available about fine-tuning data for this model.",
  "4-2 (Fine-tuning Data)__evidence": [],
  "4-3 (Reinforcement Learning Data)": "The provided quote set contains no references to reinforcement-learning datasets, their composition, generation method, or accessibility with respect to EleutherAI/gpt-neox. Therefore, no information can be summarized for this item.",
  "4-3 (Reinforcement Learning Data)__evidence": [],
  "4-4 (Data Filtering)": "None of the supplied quotes discuss filtering or cleaning procedures, threshold criteria, classifier tools, ratios, or pipeline stages applied to either the pre-training or any downstream data for EleutherAI/gpt-neox. As a result, there is no information available on data-filtering practices.",
  "4-4 (Data Filtering)__evidence": [],
  "__usage": {
    "fine_tuning": "unknown",
    "rl": "unknown"
  }
}