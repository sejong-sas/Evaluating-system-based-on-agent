{
  "model_id": "cyberagent/open-calm-7b",
  "full_texts": [
    {
      "arxiv_id": "https://r.jina.ai/https://docs.alcf.anl.gov/polaris/data-science-workflows/applications/gpt-neox/",
      "full_text": "Title: ALCF User Guides\n\nURL Source: https://docs.alcf.anl.gov/polaris/data-science-workflows/applications/gpt-neox/\n\nWarning: Target URL returned error 404: Not Found\n\nMarkdown Content:\nALCF User Guides\n\n===============\n- [x] - [x] \n\n[Argonne Leadership Computing Facility](https://www.alcf.anl.gov/)\n==================================================================\n\n*   [ALCF Resources](https://www.alcf.anl.gov/alcf-resources)\n\nLeadership Computing Resources\n------------------------------ The ALCF provides users with access to supercomputing resources that are significantly more powerful than systems typically used for open scientific research.  \n\n### Computing Resources\n\n    *   [Aurora](https://www.alcf.anl.gov/aurora)\n    *   [Polaris](https://www.alcf.anl.gov/polaris)\n    *   [Sophia](https://www.alcf.anl.gov/sophia)\n    *   [Crux](https://www.alcf.anl.gov/crux)\n    *   [ALCF AI Testbed](https://www.alcf.anl.gov/alcf-ai-testbed)\n    *   [Evaluation Testbeds](https://www.alcf.anl.gov/alcf-resources/evaluation-testbeds)\n    *   [Storage and Networking](https://www.alcf.anl.gov/alcf-resources/storage-and-networking)\n\n### Facility Expertise\n\n    *   [Facility Expertise](https://www.alcf.anl.gov/alcf-resources/facility-expertise)\n\n[### Featured: Aurora ![Image 1](https://docs.alcf.anl.gov//images/aurora.png)](https://alcf.anl.gov/aurora)\n\n*   [Science and Engineering](https://www.alcf.anl.gov/science)\n\nComputational Science\n--------------------- The ALCF is accelerating scientific discoveries in many disciplines, ranging from chemistry and engineering to physics and materials science.  \n\n### Output\n\n    *   [Projects](https://www.alcf.anl.gov/science/projects)\n    *   [Publications](https://www.alcf.anl.gov/science/publications)\n    *   [Case Studies](https://www.alcf.anl.gov/case-studies)\n\n### Allocation Programs\n\n    *   [INCITE Program](https://www.alcf.anl.gov/science/incite-allocation-program)\n    *   [ALCC Program](https://www.alcf.anl.gov/science/alcc-allocation-program)\n    *   [Director’s Discretionary](https://www.alcf.anl.gov/science/directors-discretionary-allocation-program)\n    *   [Early Science Program](https://www.alcf.anl.gov/science/early-science-program)\n    *   [NAIRR Program](https://www.alcf.anl.gov/nairr-program)\n\n[### Featured: Engineering ![Image 2](https://docs.alcf.anl.gov//images/engineering.png)](https://www.alcf.anl.gov/science/projects/Engineering)\n\n*   [Community and Outreach](https://docs.alcf.anl.gov/polaris/data-science-workflows/applications/gpt-neox/#https://www.alcf.anl.gov/community-and-partnerships)\n\nGrowing the HPC Community\n------------------------- The ALCF is committed to providing training and outreach opportunities that prepare researchers to efficiently use its leadership computing systems, while also cultivating a diverse and skilled HPC workforce for the future.  \n\n### Partnerships\n\n    *   [Industry](https://www.alcf.anl.gov/community-and-partnerships/industry)\n    *   [Collaborations](https://www.alcf.anl.gov/community-and-partnerships/partners)\n\n### Community\n\n    *   [NAIRR Pilot](https://www.alcf.anl.gov/nairr-pilot)\n    *   [Lighthouse Initiative](https://www.alcf.anl.gov/lighthouse)\n\n### Educational Outreach\n\n    *   [Women in STEM](https://www.alcf.anl.gov/community-and-partnerships/women-in-stem)\n    *   [Student Programs](https://www.alcf.anl.gov/community-and-partnerships/student-outreach)\n\n*   [About](https://www.alcf.anl.gov/about)\n\nAccelerating Science\n-------------------- The Argonne Leadership Computing Facility enables breakthroughs in science and engineering by providing supercomputing resources and expertise to the research community.  \n\n### Get to Know More\n\n    *   [Leadership](https://www.alcf.anl.gov/about/alcf-leadership-team)\n    *   [People](https://www.alcf.anl.gov/about/people)\n    *   [Organizational Chart](https://www.alcf.anl.gov/alcf-organizational-chart)\n    *   [Code of Conduct](https://www.alcf.anl.gov/about/code-of-conduct)\n    *   [User Advisory Council](https://www.alcf.anl.gov/about/user-advisory-council)\n    *   [History](https://www.alcf.anl.gov/about/history)\n\n### Visit\n\n    *   [Visiting ALCF](https://www.alcf.anl.gov/about/visiting-alcf)\n    *   [Tours](https://www.alcf.anl.gov/about/tours)\n\n### Latest\n\n    *   [News](https://www.alcf.anl.gov/news)\n    *   [Careers](https://www.alcf.anl.gov/about/careers)\n\n### Press Kits\n\n    *   [ALCF Media Kit](https://www.alcf.anl.gov/alcf-media-kit)\n    *   [Aurora Media Kit](https://www.alcf.anl.gov/aurora-media-kit)\n    *   [Reports Archive](https://www.alcf.anl.gov/report-archive)\n\n*   [Support](https://www.alcf.anl.gov/support-center)\n\nSupport Center\n-------------- The ALCF Support Center assists users with support requests related to their ALCF projects.\n\n**Help Desk**\n\n Hours: 9:00am-5:00pm CT M-F\n\n Email: [mailto:support@alcf.anl.gov](mailto:support@alcf.anl.gov)  \n\n### Current\n\n    *   [Machine Status](https://www.alcf.anl.gov/support-center/machine-status)\n    *   [Facility Updates](https://www.alcf.anl.gov/support-center/facility-updates)\n    *   [MyALCF](https://my.alcf.anl.gov/)\n\n### Training\n\n    *   [Training Videos & Slides](https://www.alcf.anl.gov/support-center/training-assets)\n    *   [Training Overview](https://www.alcf.anl.gov/support-center/training-overview)\n    *   [Training and Events](https://www.alcf.anl.gov/events)\n\n### [User Guides](https://docs.alcf.anl.gov/)\n\n    *   [Aurora](https://docs.alcf.anl.gov/aurora/)\n    *   [AI Testbed](https://docs.alcf.anl.gov/ai-testbed/)\n    *   [Crux](https://docs.alcf.anl.gov/crux/)\n    *   [Polaris](https://docs.alcf.anl.gov/polaris/)\n    *   [Sophia](https://docs.alcf.anl.gov/sophia/)\n    *   [Data Management](https://docs.alcf.anl.gov/data-management/)\n    *   [Services](https://docs.alcf.anl.gov/services/)\n    *   [Account and Project Management](https://docs.alcf.anl.gov/account-project-management/)\n    *   [Facility Policies](https://docs.alcf.anl.gov/policies/)\n\n[### Featured: Get Started ![Image 3](https://docs.alcf.anl.gov//images/getstarted.png)](https://www.alcf.anl.gov/support-center/get-started)[### Featured: MyALCF ![Image 4](https://docs.alcf.anl.gov//images/myalcf.png)](https://my.alcf.anl.gov/)\n\n*   [News](https://www.alcf.anl.gov/news)\n*   [Events](https://www.alcf.anl.gov/events)\n*   [People](https://www.alcf.anl.gov/about/people)\n*   [Careers](https://www.alcf.anl.gov/about/careers)\n*   [MyALCF](https://my.alcf.anl.gov/)\n\n Initializing search \n\n[![Image 5: logo](https://docs.alcf.anl.gov/images/logo.svg)](https://docs.alcf.anl.gov/ \"ALCF User Guides\")\n\n ALCF User Guides \n\n[argonne-lcf/user-guides](https://github.com/argonne-lcf/user-guides \"Go to repository\")\n\n[![Image 6: logo](https://docs.alcf.anl.gov/images/logo.svg)](https://docs.alcf.anl.gov/ \"ALCF User Guides\")[ALCF User Guides](https://docs.alcf.anl.gov/ \"ALCF User Guides\") \n\n[argonne-lcf/user-guides](https://github.com/argonne-lcf/user-guides \"Go to repository\")\n\n*   [Getting Started](https://www.alcf.anl.gov/support-center/get-started)\n*   [Contribute to User Guides](https://docs.alcf.anl.gov/support/docs-issues/)\n*   - [x] [User Support](https://docs.alcf.anl.gov/support/)  User Support  \n    *   [Submit a Ticket](https://docs.alcf.anl.gov/support/ticket/)\n    *   - [x] [Get Help & Connect](https://docs.alcf.anl.gov/support/get-help/)  Get Help & Connect  \n        *   [Mailing Lists](https://docs.alcf.anl.gov/support/get-help/mailing-lists/)\n        *   [Request Software](https://docs.alcf.anl.gov/support/get-help/software-requests/)\n        *   [ALCF Users Slack](https://docs.alcf.anl.gov/support/get-help/alcf-users-slack/)\n        *   [Aurora Office Hours](https://docs.alcf.anl.gov/support/get-help/office-hours/)\n        *   [AskALCF Chatbot](https://docs.alcf.anl.gov/support/get-help/askalcf/)\n\n*   - [x] [Machines](https://docs.alcf.anl.gov/machines/)  Machines  \n    *   - [x] [Aurora](https://docs.alcf.anl.gov/aurora/)  Aurora  \n        *   [Node Performance Overview](https://docs.alcf.anl.gov/aurora/node-performance-overview/node-performance-overview/)\n        *   [Getting Started](https://docs.alcf.anl.gov/aurora/getting-started-on-aurora/)\n        *   [Known Issues](https://docs.alcf.anl.gov/aurora/known-issues/)\n        *   [System Updates](https://docs.alcf.anl.gov/aurora/system-updates/)\n        *   [Aurora Programming Environment](https://docs.alcf.anl.gov/aurora/aurora-pe/)\n        *   - [x] [Compiling and Linking](https://docs.alcf.anl.gov/aurora/compiling-and-linking/)  Compiling and Linking  \n            *   [Programming Models](https://docs.alcf.anl.gov/aurora/compiling-and-linking/aurora-programming-models/)\n            *   [Example Program and Makefile](https://docs.alcf.anl.gov/aurora/compiling-and-linking/aurora-example-program-makefile/)\n\n        *   - [x]  Build Tools   Build Tools  \n            *   [CMake](https://docs.alcf.anl.gov/aurora/build-tools/cmake-aurora/)\n\n        *   [Running Jobs](https://docs.alcf.anl.gov/aurora/running-jobs-aurora/)\n        *   - [x]  Data Management   Data Management  \n            *   [Copper](https://docs.alcf.anl.gov/aurora/data-management/copper/copper/)\n            *   [DAOS](https://docs.alcf.anl.gov/aurora/data-management/daos/daos-overview/)\n            *   [Lustre (Flare)](https://docs.alcf.anl.gov/aurora/data-management/lustre/flare/)\n            *   - [x]  Moving data to Aurora   Moving data to Aurora  \n                *   [DAOS Data Mover](https://docs.alcf.anl.gov/aurora/data-management/moving_data_to_aurora/daos_datamover/)\n                *   [Globus](https://docs.alcf.anl.gov/aurora/data-management/moving_data_to_aurora/globus/)\n\n        *   - [x]  Applications and Libraries   Applications and Libraries  \n            *   - [x]  Libraries   Libraries  \n                *   [Cabana](https://docs.alcf.anl.gov/aurora/applications-and-libraries/libraries/cabana-aurora/)\n                *   [oneDAL](https://docs.alcf.anl.gov/aurora/applications-and-libraries/libraries/onedal/)\n                *   [Spack PE](https://docs.alcf.anl.gov/aurora/applications-and-libraries/libraries/spack-pe/)\n\n        *   [Containers](https://docs.alcf.anl.gov/aurora/containers/containers/)\n        *   - [x]  Data Science   Data Science  \n            *   [Python](https://docs.alcf.anl.gov/aurora/data-science/python/)\n            *   [Jupyter](https://docs.alcf.anl.gov/aurora/data-science/jupyter/)\n            *   [Profiling](https://docs.alcf.anl.gov/aurora/data-science/profiling_dl/)\n            *   - [x]  Frameworks   Frameworks  \n                *   [DeepSpeed](https://docs.alcf.anl.gov/aurora/data-science/frameworks/deepspeed/)\n                *   [Megatron-DeepSpeed](https://docs.alcf.anl.gov/aurora/data-science/frameworks/megatron-deepspeed/)\n                *   [PyTorch](https://docs.alcf.anl.gov/aurora/data-science/frameworks/pytorch/)\n                *   [TensorFlow](https://docs.alcf.anl.gov/aurora/data-science/frameworks/tensorflow/)\n                *   [Dask](https://docs.alcf.anl.gov/aurora/data-science/frameworks/dask/)\n                *   [oneCCL](https://docs.alcf.anl.gov/aurora/data-science/frameworks/oneCCL/)\n                *   [scikit-learn](https://docs.alcf.anl.gov/aurora/data-science/frameworks/scikit-learn/)\n                *   [oneDAL](https://docs.alcf.anl.gov/aurora/applications-and-libraries/libraries/onedal/)\n                *   [GPyTorch](https://docs.alcf.anl.gov/aurora/data-science/frameworks/gpytorch/)\n                *   [PyTorch Geometric (PyG)](https://docs.alcf.anl.gov/aurora/data-science/frameworks/pyg/)\n\n            *   - [x]  AI Inference   AI Inference  \n                *   [LibTorch](https://docs.alcf.anl.gov/aurora/data-science/inference/libtorch/)\n                *   [OpenVINO](https://docs.alcf.anl.gov/aurora/data-science/inference/openvino/)\n                *   [vLLM](https://docs.alcf.anl.gov/aurora/data-science/inference/vllm/)\n\n        *   - [x]  Programming Models   Programming Models  \n            *   [Kokkos](https://docs.alcf.anl.gov/aurora/programming-models/kokkos-aurora/)\n            *   [Level Zero](https://docs.alcf.anl.gov/aurora/programming-models/level-0/)\n            *   [OpenCL](https://docs.alcf.anl.gov/aurora/programming-models/opencl-aurora/)\n            *   [OpenMP](https://docs.alcf.anl.gov/aurora/programming-models/openmp-aurora/)\n            *   [SYCL](https://docs.alcf.anl.gov/aurora/programming-models/sycl-aurora/)\n            *   [HIP](https://docs.alcf.anl.gov/aurora/programming-models/hip-aurora/)\n\n        *   - [x] [Debugging Tools](https://docs.alcf.anl.gov/aurora/debugging/)  Debugging Tools  \n            *   [gdb-oneapi](https://docs.alcf.anl.gov/aurora/debugging/gdb-oneapi/)\n            *   [DDT](https://docs.alcf.anl.gov/aurora/debugging/ddt-aurora/)\n            *   [gdb4hpc](https://docs.alcf.anl.gov/aurora/debugging/gdb4hpc/)\n            *   [Codee](https://docs.alcf.anl.gov/aurora/debugging/codee/)\n\n        *   - [x] [Performance Tools](https://docs.alcf.anl.gov/aurora/performance-tools/)  Performance Tools  \n            *   [iprof](https://docs.alcf.anl.gov/aurora/performance-tools/iprof/)\n            *   [unitrace](https://docs.alcf.anl.gov/aurora/performance-tools/unitrace/)\n            *   [xpu-smi](https://docs.alcf.anl.gov/aurora/performance-tools/xpu-smi/)\n            *   [APS](https://docs.alcf.anl.gov/aurora/performance-tools/aps/)\n            *   [Advisor](https://docs.alcf.anl.gov/aurora/performance-tools/advisor/)\n            *   [VTune](https://docs.alcf.anl.gov/aurora/performance-tools/vtune/)\n\n        *   - [x]  Services   Services  \n            *   [GitLab CI](https://docs.alcf.anl.gov/aurora/services/gitlab-ci/)\n\n        *   - [x] [Visualization](https://docs.alcf.anl.gov/aurora/visualization/)  Visualization  \n            *   [ParaView (Launch from Client)](https://docs.alcf.anl.gov/aurora/visualization/paraview/)\n            *   [ParaView (Manual Launch)](https://docs.alcf.anl.gov/aurora/visualization/paraview-manual-launch/)\n            *   [VisIt](https://docs.alcf.anl.gov/aurora/visualization/visit/)\n\n        *   - [x]  Workflows   Workflows  \n            *   [ADIOS2](https://docs.alcf.anl.gov/aurora/workflows/adios/)\n            *   [Balsam](https://docs.alcf.anl.gov/aurora/workflows/balsam/)\n            *   [libEnsemble](https://docs.alcf.anl.gov/aurora/workflows/libensemble/)\n            *   [Parsl](https://docs.alcf.anl.gov/aurora/workflows/parsl/)\n            *   [SmartSim](https://docs.alcf.anl.gov/aurora/workflows/smartsim/)\n\n    *   - [x] [AI Testbed](https://docs.alcf.anl.gov/ai-testbed/)  AI Testbed  \n        *   - [x] [Cerebras](https://docs.alcf.anl.gov/ai-testbed/cerebras/)  Cerebras  \n            *   [Getting Started](https://docs.alcf.anl.gov/ai-testbed/cerebras/getting-started/)\n            *   [Running a Model/Program](https://docs.alcf.anl.gov/ai-testbed/cerebras/running-a-model-or-program/)\n            *   [Customizing Environments](https://docs.alcf.anl.gov/ai-testbed/cerebras/customizing-environment/)\n            *   [Job Queuing and Submission](https://docs.alcf.anl.gov/ai-testbed/cerebras/job-queuing-and-submission/)\n            *   [Example Programs](https://docs.alcf.anl.gov/ai-testbed/cerebras/example-programs/)\n            *   [Tunneling and Forwarding Ports](https://docs.alcf.anl.gov/ai-testbed/cerebras/tunneling-and-forwarding-ports/)\n            *   [Miscellaneous](https://docs.alcf.anl.gov/ai-testbed/cerebras/miscellaneous/)\n            *   [CSL](https://docs.alcf.anl.gov/ai-testbed/cerebras/csl/)\n\n        *   - [x] [Graphcore](https://docs.alcf.anl.gov/ai-testbed/graphcore/)  Graphcore  \n            *   [Getting Started](https://docs.alcf.anl.gov/ai-testbed/graphcore/getting-started/)\n            *   [Virtual Environment](https://docs.alcf.anl.gov/ai-testbed/graphcore/virtual-environments/)\n            *   [Running a Model/Program](https://docs.alcf.anl.gov/ai-testbed/graphcore/running-a-model-or-program/)\n            *   [Job Queuing and Submission](https://docs.alcf.anl.gov/ai-testbed/graphcore/job-queuing-and-submission/)\n            *   [Example Programs](https://docs.alcf.anl.gov/ai-testbed/graphcore/example-programs/)\n            *   [Miscellaneous](https://docs.alcf.anl.gov/ai-testbed/graphcore/miscellaneous/)\n            *   [Documentation](https://docs.alcf.anl.gov/ai-testbed/graphcore/documentation/)\n\n        *   - [x] [Groq](https://docs.alcf.anl.gov/ai-testbed/groq/)  Groq  \n            *   [Getting Started](https://docs.alcf.anl.gov/ai-testbed/groq/getting-started/)\n            *   [Running a Model/Program](https://docs.alcf.anl.gov/ai-testbed/groq/running-a-model-or-program/)\n            *   [Virtual Environments](https://docs.alcf.anl.gov/ai-testbed/groq/virtual-environments/)\n            *   [Job Queueing and Submission](https://docs.alcf.anl.gov/ai-testbed/groq/job-queuing-and-submission/)\n            *   [Profiling with GroqView](https://docs.alcf.anl.gov/ai-testbed/groq/groqview/)\n            *   [Examples](https://docs.alcf.anl.gov/ai-testbed/groq/examples/)\n\n        *   - [x]  SambaNova   SambaNova  \n            *   - [x] [SN30](https://docs.alcf.anl.gov/ai-testbed/sambanova/)  SN30  \n                *   [Getting Started](https://docs.alcf.anl.gov/ai-testbed/sambanova/getting-started/)\n                *   [Virtual Environment](https://docs.alcf.anl.gov/ai-testbed/sambanova/virtual-environment/)\n                *   [Running a Model/Program](https://docs.alcf.anl.gov/ai-testbed/sambanova/running-a-model-or-program/)\n                *   [Job Queuing and Submission](https://docs.alcf.anl.gov/ai-testbed/sambanova/job-queuing-and-submission/)\n                *   [Example Programs](https://docs.alcf.anl.gov/ai-testbed/sambanova/example-programs/)\n                *   [Example Multi-Node Programs](https://docs.alcf.anl.gov/ai-testbed/sambanova/example-multi-node-programs/)\n                *   [ModelZoo Samples](https://docs.alcf.anl.gov/ai-testbed/sambanova/example-modelzoo-programs/)\n                *   [Tunneling and Forwarding Ports](https://docs.alcf.anl.gov/ai-testbed/sambanova/tunneling-and-forwarding-ports/)\n                *   [SambaTune for profiling and performance tuning](https://docs.alcf.anl.gov/ai-testbed/sambanova/sambatune/)\n                *   [Miscellaneous](https://docs.alcf.anl.gov/ai-testbed/sambanova/miscellaneous/)\n                *   [SambaNova Documentation](https://docs.alcf.anl.gov/ai-testbed/sambanova/documentation/)\n\n            *   - [x] [Metis Inference](https://docs.alcf.anl.gov/ai-testbed/sn40l_inference/)  Metis Inference  \n                *   [Using an Inference Endpoint](https://docs.alcf.anl.gov/ai-testbed/sn40l_inference/using_an_inference_endpoint/)\n\n        *   [Data Management](https://docs.alcf.anl.gov/ai-testbed/data-management/data-management-overview/)\n\n    *   - [x] [Crux](https://docs.alcf.anl.gov/crux/)  Crux  \n        *   [Getting Started](https://docs.alcf.anl.gov/crux/getting-started/)\n        *   [Running Jobs](https://docs.alcf.anl.gov/crux/queueing-and-running-jobs/running-jobs/)\n        *   [Compiling and Linking](https://docs.alcf.anl.gov/crux/compiling-and-linking/compiling-and-linking-overview/)\n        *   [Containers](https://docs.alcf.anl.gov/crux/containers/containers/)\n        *   - [x]  Data Science   Data Science  \n            *   [Python](https://docs.alcf.anl.gov/crux/data-science/python/)\n\n    *   - [x] [Polaris](https://docs.alcf.anl.gov/polaris/)  Polaris  \n        *   [Getting Started](https://docs.alcf.anl.gov/polaris/getting-started/)\n        *   [Known Issues](https://docs.alcf.anl.gov/polaris/known-issues/)\n        *   [System Updates](https://docs.alcf.anl.gov/polaris/system-updates/)\n        *   - [x] [Compiling and Linking](https://docs.alcf.anl.gov/polaris/compiling-and-linking/)  Compiling and Linking  \n            *   [Programming Models](https://docs.alcf.anl.gov/polaris/compiling-and-linking/polaris-programming-models/)\n            *   [Example Program and Makefile](https://docs.alcf.anl.gov/polaris/compiling-and-linking/polaris-example-program-makefile/)\n            *   [CCE Compilers](https://docs.alcf.anl.gov/polaris/compiling-and-linking/cce-compilers-polaris/)\n            *   [GNU Compilers](https://docs.alcf.anl.gov/polaris/compiling-and-linking/gnu-compilers-polaris/)\n            *   [NVIDIA Compilers](https://docs.alcf.anl.gov/polaris/compiling-and-linking/nvidia-compiler-polaris/)\n            *   [LLVM Compilers](https://docs.alcf.anl.gov/polaris/compiling-and-linking/llvm-compilers-polaris/)\n            *   [oneAPI Toolkit](https://docs.alcf.anl.gov/polaris/compiling-and-linking/oneapi-compiler/)\n\n        *   - [x]  Build Tools   Build Tools  \n            *   [CMake](https://docs.alcf.anl.gov/polaris/build-tools/cmake-polaris/)\n\n        *   - [x] [Running Jobs](https://docs.alcf.anl.gov/polaris/running-jobs/)  Running Jobs  \n            *   [Using GPUs](https://docs.alcf.anl.gov/polaris/running-jobs/using-gpus/)\n\n        *   - [x]  Applications and Libraries   Applications and Libraries  \n            *   - [x]  Applications   Applications  \n                *   [Amber](https://docs.alcf.anl.gov/polaris/applications-and-libraries/applications/amber/)\n                *   [GROMACS](https://docs.alcf.anl.gov/polaris/applications-and-libraries/applications/gromacs/)\n                *   [LAMMPS](https://docs.alcf.anl.gov/polaris/applications-and-libraries/applications/lammps/)\n                *   [NAMD](https://docs.alcf.anl.gov/polaris/applications-and-libraries/applications/namd/)\n                *   [NekRS](https://docs.alcf.anl.gov/polaris/applications-and-libraries/applications/nekrs/)\n                *   [OpenMM](https://docs.alcf.anl.gov/polaris/applications-and-libraries/applications/openmm/)\n                *   [QMCPACK](https://docs.alcf.anl.gov/polaris/applications-and-libraries/applications/QMCPACK/)\n                *   [Quantum ESPRESSO](https://docs.alcf.anl.gov/polaris/applications-and-libraries/applications/QuantumESPRESSO/)\n                *   [VASP](https://docs.alcf.anl.gov/polaris/applications-and-libraries/applications/vasp/)\n\n            *   - [x]  Libraries   Libraries  \n                *   [Math Libraries](https://docs.alcf.anl.gov/polaris/applications-and-libraries/libraries/math-libraries/)\n                *   [Cabana](https://docs.alcf.anl.gov/polaris/applications-and-libraries/libraries/cabana-polaris/)\n                *   [Spack PE](https://docs.alcf.anl.gov/polaris/applications-and-libraries/libraries/spack-pe/)\n                *   [NCCL](https://docs.alcf.anl.gov/polaris/applications-and-libraries/libraries/nccl/)\n                *   [XALT](https://docs.alcf.anl.gov/polaris/applications-and-libraries/libraries/xalt/)\n\n        *   [Containers](https://docs.alcf.anl.gov/polaris/containers/containers/)\n        *   - [x]  Data Science   Data Science  \n            *   [Python](https://docs.alcf.anl.gov/polaris/data-science/python/)\n            *   [Profiling](https://docs.alcf.anl.gov/polaris/data-science/profiling_dl/)\n            *   - [x]  Frameworks   Frameworks  \n                *   [TensorFlow](https://docs.alcf.anl.gov/polaris/data-science/frameworks/tensorflow/)\n                *   [PyTorch](https://docs.alcf.anl.gov/polaris/data-science/frameworks/pytorch/)\n                *   [JAX](https://docs.alcf.anl.gov/polaris/data-science/frameworks/jax/)\n                *   [DeepSpeed](https://docs.alcf.anl.gov/polaris/data-science/frameworks/deepspeed/)\n                *   [LibTorch](https://docs.alcf.anl.gov/polaris/data-science/frameworks/libtorch/)\n                *   [GPyTorch](https://docs.alcf.anl.gov/polaris/data-science/frameworks/gpytorch/)\n\n            *   - [x]  Applications   Applications  \n                *   [Megatron-DeepSpeed](https://docs.alcf.anl.gov/polaris/data-science/applications/megatron-deepspeed/)\n                *   [gpt-neox](https://docs.alcf.anl.gov/polaris/data-science/applications/gpt-neox/)\n\n        *   - [x]  Programming Models   Programming Models  \n            *   [OpenMP](https://docs.alcf.anl.gov/polaris/programming-models/openmp-polaris/)\n            *   [SYCL](https://docs.alcf.anl.gov/polaris/programming-models/sycl-polaris/)\n            *   [Kokkos](https://docs.alcf.anl.gov/polaris/programming-models/kokkos-polaris/)\n            *   [Julia](https://docs.alcf.anl.gov/polaris/programming-models/julia/)\n\n        *   - [x]  Debugging Tools   Debugging Tools  \n            *   [CUDA-GDB](https://docs.alcf.anl.gov/polaris/debugging-tools/CUDA-GDB/)\n\n        *   - [x]  Performance Tools   Performance Tools  \n            *   [NVIDIA-Nsight](https://docs.alcf.anl.gov/polaris/performance-tools/NVIDIA-Nsight/)\n\n        *   - [x] [Visualization](https://docs.alcf.anl.gov/polaris/visualization/)  Visualization  \n            *   [ParaView (Launch from Client)](https://docs.alcf.anl.gov/polaris/visualization/paraview/)\n            *   [ParaView (Manual Launch)](https://docs.alcf.anl.gov/polaris/visualization/paraview-manual-launch/)\n            *   [VisIt](https://docs.alcf.anl.gov/polaris/visualization/visit/)\n            *   [FFmpeg](https://docs.alcf.anl.gov/polaris/visualization/ffmpeg/)\n            *   [ImageMagick](https://docs.alcf.anl.gov/polaris/visualization/imagemagick/)\n            *   [ParaView Tutorial](https://docs.alcf.anl.gov/polaris/visualization/paraview-tutorial/)\n            *   [VNC](https://docs.alcf.anl.gov/polaris/visualization/vnc/)\n\n        *   - [x]  Workflows   Workflows  \n            *   [Balsam](https://docs.alcf.anl.gov/polaris/workflows/balsam/)\n            *   [Parsl](https://docs.alcf.anl.gov/polaris/workflows/parsl/)\n            *   [libEnsemble](https://docs.alcf.anl.gov/polaris/workflows/libensemble/)\n            *   [SmartSim](https://docs.alcf.anl.gov/polaris/workflows/smartsim/)\n\n    *   - [x] [Sophia](https://docs.alcf.anl.gov/sophia/)  Sophia  \n        *   [Getting Started](https://docs.alcf.anl.gov/sophia/getting-started/)\n        *   [Running Jobs](https://docs.alcf.anl.gov/sophia/queueing-and-running-jobs/running-jobs/)\n        *   [Compiling and Linking](https://docs.alcf.anl.gov/sophia/compiling-and-linking/compiling-and-linking-overview/)\n        *   [Containers](https://docs.alcf.anl.gov/sophia/containers/containers/)\n        *   - [x]  Data Science   Data Science  \n            *   [Python](https://docs.alcf.anl.gov/sophia/data-science/python/)\n            *   [Fine-tuning with Autotrain](https://docs.alcf.anl.gov/sophia/data-science/fine-tune-LLM-with-Autotrain/)\n            *   [Dask](https://docs.alcf.anl.gov/sophia/data-science/dask/)\n\n        *   - [x] [Visualization](https://docs.alcf.anl.gov/sophia/visualization/)  Visualization  \n            *   [ParaView (Launch from Client)](https://docs.alcf.anl.gov/sophia/visualization/paraview/)\n\n*   - [x] [Running Jobs with PBS](https://docs.alcf.anl.gov/running-jobs/)  Running Jobs with PBS  \n    *   [Example Job Scripts](https://docs.alcf.anl.gov/running-jobs/example-job-scripts/)\n    *   [Machine Reservations](https://docs.alcf.anl.gov/running-jobs/machine-reservations/)\n\n*   - [x] [Data Management](https://docs.alcf.anl.gov/data-management/)  Data Management  \n    *   - [x] [File System & Storage](https://docs.alcf.anl.gov/data-management/filesystem-and-storage/)  File System & Storage  \n        *   [HPSS](https://docs.alcf.anl.gov/data-management/filesystem-and-storage/hpss/)\n        *   [Disk Quota](https://docs.alcf.anl.gov/data-management/filesystem-and-storage/disk-quota/)\n\n    *   - [x] [Transfer & Sharing](https://docs.alcf.anl.gov/data-management/acdc/)  Transfer & Sharing  \n        *   [SFTP and SCP](https://docs.alcf.anl.gov/data-management/data-transfer/sftp-scp/)\n        *   [Using Globus](https://docs.alcf.anl.gov/data-management/data-transfer/using-globus/)\n        *   [Sharing on Eagle Using Globus](https://docs.alcf.anl.gov/data-management/acdc/eagle-data-sharing/)\n\n*   - [x] [Services](https://docs.alcf.anl.gov/services/)  Services  \n    *   [Inference Endpoints](https://docs.alcf.anl.gov/services/inference-endpoints/)\n    *   [JupyterHub](https://docs.alcf.anl.gov/services/jupyter-hub/)\n    *   - [x]  Continuous Integration   Continuous Integration  \n        *   [General](https://docs.alcf.anl.gov/services/continuous-integration/)\n        *   [GitLab CI](https://docs.alcf.anl.gov/services/gitlab-ci/)\n\n*   - [x] [Account & Project Management](https://docs.alcf.anl.gov/account-project-management/)  Account & Project Management  \n    *   [MyALCF](https://docs.alcf.anl.gov/account-project-management/MyALCF/)\n    *   - [x] [Accounts and Access](https://docs.alcf.anl.gov/account-project-management/accounts-and-access/)  Accounts and Access  \n        *   [Obtaining a Token](https://docs.alcf.anl.gov/account-project-management/accounts-and-access/obtaining-a-token/)\n        *   [Logging in with a Token](https://docs.alcf.anl.gov/account-project-management/accounts-and-access/logging-in-with-tokens/)\n        *   [Troubleshooting Tokens](https://docs.alcf.anl.gov/account-project-management/accounts-and-access/troubleshooting-tokens/)\n        *   [Accounts and Access FAQs](https://docs.alcf.anl.gov/account-project-management/accounts-and-access/accounts-and-access-faqs/)\n\n    *   - [x]  Project Management   Project Management  \n        *   [Starting Your ALCF Award](https://docs.alcf.anl.gov/account-project-management/project-management/starting-alcf-award/)\n        *   [Managing Your Team Members](https://docs.alcf.anl.gov/account-project-management/project-management/team-management/)\n        *   [Reporting](https://docs.alcf.anl.gov/account-project-management/project-management/project-reports/)\n\n    *   - [x] [Allocations](https://docs.alcf.anl.gov/account-project-management/allocation-management/)  Allocations  \n        *   [Managing Your Allocations](https://docs.alcf.anl.gov/account-project-management/allocation-management/allocation-management/)\n        *   [sbank Allocation Accounting System](https://docs.alcf.anl.gov/account-project-management/allocation-management/sbank-allocation-accounting-system/)\n\n*   - [x] [Facility Policies](https://docs.alcf.anl.gov/policies/)  Facility Policies  \n    *   [ALCF Acknowledgement Policy](https://docs.alcf.anl.gov/policies/alcf-acknowledgement-policy/)\n    *   - [x] [Account Policies](https://docs.alcf.anl.gov/policies/accounts/)  Account Policies  \n        *   [Account Sponsorship & Retention Policy](https://docs.alcf.anl.gov/policies/accounts/account-sponsorship-retention-policy/)\n        *   [User Authentication Policy](https://docs.alcf.anl.gov/policies/accounts/user-authentication-policy/)\n\n    *   - [x] [Scheduling Policies](https://docs.alcf.anl.gov/policies/queue-scheduling/)  Scheduling Policies  \n        *   [Refund Policy](https://docs.alcf.anl.gov/policies/queue-scheduling/refund-policy/)\n        *   [Pullback Policy](https://docs.alcf.anl.gov/policies/queue-scheduling/pullback-policy/)\n\n    *   - [x]  Data and Software Policies   Data and Software Policies  \n        *   [Data Policy](https://docs.alcf.anl.gov/policies/data-and-software-policies/data-policy/)\n        *   [Software Policy](https://docs.alcf.anl.gov/policies/data-and-software-policies/software-policy/)\n\n*   [Acronyms](https://docs.alcf.anl.gov/acronyms/)\n\n Table of contents  \n\n404 - Not found\n===============\n\n Back to top \n\nALCF A DOE Office of Science User Facility\n------------------------------------------\n\nArgonne National Laboratory\n\n 9700 South Cass Avenue\n\n Building 240\n\n Argonne, IL 60439\n\n[Contact](mailto:media@alcf.anl.gov)\n\n[](https://github.com/argonne-lcf/user-guides \"github.com\")[](https://www.youtube.com/c/argonneleadershipcomputingfacility \"www.youtube.com\")[](https://x.com/argonne_lcf \"x.com\")[](https://www.facebook.com/argonne.lcf/ \"www.facebook.com\")\n\n[Internal Admin](https://login.anl.gov/idp/profile/SAML2/Redirect/SSO?execution=e1s1)\n\n©  Argonne Leadership Computing Facility\n\n![Image 7](https://docs.alcf.anl.gov//images/doe.png)![Image 8](https://docs.alcf.anl.gov//images/ANL_ko_300.png)![Image 9](https://docs.alcf.anl.gov//images/UCHI.png)\n",
      "fetch_method": "jina-reader"
    },
    {
      "arxiv_id": "https://arxiv.org/abs/2305.18290",
      "full_text": " [2305.18290] Direct Preference Optimization: Your Language Model is Secretly a Reward Model Skip to main content We gratefully acknowledge support from the Simons Foundation, member institutions , and all contributors. Donate &gt; cs &gt; arXiv:2305.18290 Help | Advanced Search All fields Title Author Abstract Comments Journal reference ACM classification MSC classification Report number arXiv identifier DOI ORCID arXiv author ID Help pages Full text Search open search GO open navigation menu quick links Login Help Pages About --> Computer Science > Machine Learning arXiv:2305.18290 (cs) [Submitted on 29 May 2023 ( v1 ), last revised 29 Jul 2024 (this version, v3)] Title: Direct Preference Optimization: Your Language Model is Secretly a Reward Model Authors: Rafael Rafailov , Archit Sharma , Eric Mitchell , Stefano Ermon , Christopher D. Manning , Chelsea Finn View a PDF of the paper titled Direct Preference Optimization: Your Language Model is Secretly a Reward Model, by Rafael Rafailov and 5 other authors View PDF HTML (experimental) Abstract: While large-scale unsupervised language models (LMs) learn broad world knowledge and some reasoning skills, achieving precise control of their behavior is difficult due to the completely unsupervised nature of their training. Existing methods for gaining such steerability collect human labels of the relative quality of model generations and fine-tune the unsupervised LM to align with these preferences, often with reinforcement learning from human feedback (RLHF). However, RLHF is a complex and often unstable procedure, first fitting a reward model that reflects the human preferences, and then fine-tuning the large unsupervised LM using reinforcement learning to maximize this estimated reward without drifting too far from the original model. In this paper we introduce a new parameterization of the reward model in RLHF that enables extraction of the corresponding optimal policy in closed form, allowing us to solve the standard RLHF problem with only a simple classification loss. The resulting algorithm, which we call Direct Preference Optimization (DPO), is stable, performant, and computationally lightweight, eliminating the need for sampling from the LM during fine-tuning or performing significant hyperparameter tuning. Our experiments show that DPO can fine-tune LMs to align with human preferences as well as or better than existing methods. Notably, fine-tuning with DPO exceeds PPO-based RLHF in ability to control sentiment of generations, and matches or improves response quality in summarization and single-turn dialogue while being substantially simpler to implement and train. Subjects: Machine Learning (cs.LG) ; Artificial Intelligence (cs.AI); Computation and Language (cs.CL) Cite as: arXiv:2305.18290 [cs.LG] &nbsp; (or arXiv:2305.18290v3 [cs.LG] for this version) &nbsp; https://doi.org/10.48550/arXiv.2305.18290 Focus to learn more arXiv-issued DOI via DataCite Submission history From: Archit Sharma [ view email ] [v1] Mon, 29 May 2023 17:57:46 UTC (982 KB) [v2] Wed, 13 Dec 2023 18:48:48 UTC (983 KB) [v3] Mon, 29 Jul 2024 22:26:36 UTC (999 KB) Full-text links: Access Paper: View a PDF of the paper titled Direct Preference Optimization: Your Language Model is Secretly a Reward Model, by Rafael Rafailov and 5 other authors View PDF HTML (experimental) TeX Source Other Formats view license Current browse context: cs.LG &lt;&nbsp;prev &nbsp; | &nbsp; next&nbsp;&gt; new | recent | 2023-05 Change to browse by: cs cs.AI cs.CL References &amp; Citations NASA ADS Google Scholar Semantic Scholar 2 blog links ( what is this? ) a export BibTeX citation Loading... BibTeX formatted citation &times; loading... Data provided by: Bookmark Bibliographic Tools Bibliographic and Citation Tools Bibliographic Explorer Toggle Bibliographic Explorer ( What is the Explorer? ) Connected Papers Toggle Connected Papers ( What is Connected Papers? ) Litmaps Toggle Litmaps ( What is Litmaps? ) scite.ai Toggle scite Smart Citations ( What are Smart Citations? ) Code, Data, Media Code, Data and Media Associated with this Article alphaXiv Toggle alphaXiv ( What is alphaXiv? ) Links to Code Toggle CatalyzeX Code Finder for Papers ( What is CatalyzeX? ) DagsHub Toggle DagsHub ( What is DagsHub? ) GotitPub Toggle Gotit.pub ( What is GotitPub? ) Huggingface Toggle Hugging Face ( What is Huggingface? ) Links to Code Toggle Papers with Code ( What is Papers with Code? ) ScienceCast Toggle ScienceCast ( What is ScienceCast? ) Demos Demos Replicate Toggle Replicate ( What is Replicate? ) Spaces Toggle Hugging Face Spaces ( What is Spaces? ) Spaces Toggle TXYZ.AI ( What is TXYZ.AI? ) Related Papers Recommenders and Search Tools Link to Influence Flower Influence Flower ( What are Influence Flowers? ) Core recommender toggle CORE Recommender ( What is CORE? ) IArxiv recommender toggle IArxiv Recommender ( What is IArxiv? ) Author Venue Institution Topic About arXivLabs arXivLabs: experimental projects with community collaborators arXivLabs is a framework that allows collaborators to develop and share new arXiv features directly on our website. Both individuals and organizations that work with arXivLabs have embraced and accepted our values of openness, community, excellence, and user data privacy. arXiv is committed to these values and only works with partners that adhere to them. Have an idea for a project that will add value for arXiv's community? Learn more about arXivLabs . Which authors of this paper are endorsers? | Disable MathJax ( What is MathJax? ) About Help contact arXiv Click here to contact arXiv Contact subscribe to arXiv mailings Click here to subscribe Subscribe Copyright Privacy Policy Web Accessibility Assistance arXiv Operational Status Get status notifications via email or slack ",
      "fetch_method": "direct-html"
    },
    {
      "arxiv_id": "https://arxiv.org/abs/2402.01306",
      "full_text": " [2402.01306] KTO: Model Alignment as Prospect Theoretic Optimization Skip to main content We gratefully acknowledge support from the Simons Foundation, member institutions , and all contributors. Donate &gt; cs &gt; arXiv:2402.01306 Help | Advanced Search All fields Title Author Abstract Comments Journal reference ACM classification MSC classification Report number arXiv identifier DOI ORCID arXiv author ID Help pages Full text Search open search GO open navigation menu quick links Login Help Pages About --> Computer Science > Machine Learning arXiv:2402.01306 (cs) [Submitted on 2 Feb 2024 ( v1 ), last revised 19 Nov 2024 (this version, v4)] Title: KTO: Model Alignment as Prospect Theoretic Optimization Authors: Kawin Ethayarajh , Winnie Xu , Niklas Muennighoff , Dan Jurafsky , Douwe Kiela View a PDF of the paper titled KTO: Model Alignment as Prospect Theoretic Optimization, by Kawin Ethayarajh and 4 other authors View PDF HTML (experimental) Abstract: Kahneman &amp; Tversky&#39;s $\\textit{prospect theory}$ tells us that humans perceive random variables in a biased but well-defined manner (1992); for example, humans are famously loss-averse. We show that objectives for aligning LLMs with human feedback implicitly incorporate many of these biases -- the success of these objectives (e.g., DPO) over cross-entropy minimization can partly be ascribed to them belonging to a family of loss functions that we call $\\textit{human-aware losses}$ (HALOs). However, the utility functions these methods attribute to humans still differ from those in the prospect theory literature. Using a Kahneman-Tversky model of human utility, we propose a HALO that directly maximizes the utility of generations instead of maximizing the log-likelihood of preferences, as current methods do. We call this approach KTO, and it matches or exceeds the performance of preference-based methods at scales from 1B to 30B, despite only learning from a binary signal of whether an output is desirable. More broadly, our work suggests that there is no one HALO that is universally superior; the best loss depends on the inductive biases most appropriate for a given setting, an oft-overlooked consideration. Comments: ICML 2024 Subjects: Machine Learning (cs.LG) ; Artificial Intelligence (cs.AI) Cite as: arXiv:2402.01306 [cs.LG] &nbsp; (or arXiv:2402.01306v4 [cs.LG] for this version) &nbsp; https://doi.org/10.48550/arXiv.2402.01306 Focus to learn more arXiv-issued DOI via DataCite Submission history From: Kawin Ethayarajh [ view email ] [v1] Fri, 2 Feb 2024 10:53:36 UTC (857 KB) [v2] Mon, 3 Jun 2024 02:36:09 UTC (975 KB) [v3] Tue, 3 Sep 2024 07:41:51 UTC (975 KB) [v4] Tue, 19 Nov 2024 18:12:45 UTC (977 KB) Full-text links: Access Paper: View a PDF of the paper titled KTO: Model Alignment as Prospect Theoretic Optimization, by Kawin Ethayarajh and 4 other authors View PDF HTML (experimental) TeX Source Other Formats view license Current browse context: cs.LG &lt;&nbsp;prev &nbsp; | &nbsp; next&nbsp;&gt; new | recent | 2024-02 Change to browse by: cs cs.AI References &amp; Citations NASA ADS Google Scholar Semantic Scholar a export BibTeX citation Loading... BibTeX formatted citation &times; loading... Data provided by: Bookmark Bibliographic Tools Bibliographic and Citation Tools Bibliographic Explorer Toggle Bibliographic Explorer ( What is the Explorer? ) Connected Papers Toggle Connected Papers ( What is Connected Papers? ) Litmaps Toggle Litmaps ( What is Litmaps? ) scite.ai Toggle scite Smart Citations ( What are Smart Citations? ) Code, Data, Media Code, Data and Media Associated with this Article alphaXiv Toggle alphaXiv ( What is alphaXiv? ) Links to Code Toggle CatalyzeX Code Finder for Papers ( What is CatalyzeX? ) DagsHub Toggle DagsHub ( What is DagsHub? ) GotitPub Toggle Gotit.pub ( What is GotitPub? ) Huggingface Toggle Hugging Face ( What is Huggingface? ) Links to Code Toggle Papers with Code ( What is Papers with Code? ) ScienceCast Toggle ScienceCast ( What is ScienceCast? ) Demos Demos Replicate Toggle Replicate ( What is Replicate? ) Spaces Toggle Hugging Face Spaces ( What is Spaces? ) Spaces Toggle TXYZ.AI ( What is TXYZ.AI? ) Related Papers Recommenders and Search Tools Link to Influence Flower Influence Flower ( What are Influence Flowers? ) Core recommender toggle CORE Recommender ( What is CORE? ) IArxiv recommender toggle IArxiv Recommender ( What is IArxiv? ) Author Venue Institution Topic About arXivLabs arXivLabs: experimental projects with community collaborators arXivLabs is a framework that allows collaborators to develop and share new arXiv features directly on our website. Both individuals and organizations that work with arXivLabs have embraced and accepted our values of openness, community, excellence, and user data privacy. arXiv is committed to these values and only works with partners that adhere to them. Have an idea for a project that will add value for arXiv's community? Learn more about arXivLabs . Which authors of this paper are endorsers? | Disable MathJax ( What is MathJax? ) About Help contact arXiv Click here to contact arXiv Contact subscribe to arXiv mailings Click here to subscribe Subscribe Copyright Privacy Policy Web Accessibility Assistance arXiv Operational Status Get status notifications via email or slack ",
      "fetch_method": "direct-html"
    },
    {
      "arxiv_id": "https://arxiv.org/abs/2204.06745",
      "full_text": " [2204.06745] GPT-NeoX-20B: An Open-Source Autoregressive Language Model Skip to main content We gratefully acknowledge support from the Simons Foundation, member institutions , and all contributors. Donate &gt; cs &gt; arXiv:2204.06745 Help | Advanced Search All fields Title Author Abstract Comments Journal reference ACM classification MSC classification Report number arXiv identifier DOI ORCID arXiv author ID Help pages Full text Search open search GO open navigation menu quick links Login Help Pages About --> Computer Science > Computation and Language arXiv:2204.06745 (cs) [Submitted on 14 Apr 2022] Title: GPT-NeoX-20B: An Open-Source Autoregressive Language Model Authors: Sid Black , Stella Biderman , Eric Hallahan , Quentin Anthony , Leo Gao , Laurence Golding , Horace He , Connor Leahy , Kyle McDonell , Jason Phang , Michael Pieler , USVSN Sai Prashanth , Shivanshu Purohit , Laria Reynolds , Jonathan Tow , Ben Wang , Samuel Weinbach View a PDF of the paper titled GPT-NeoX-20B: An Open-Source Autoregressive Language Model, by Sid Black and Stella Biderman and Eric Hallahan and Quentin Anthony and Leo Gao and Laurence Golding and Horace He and Connor Leahy and Kyle McDonell and Jason Phang and Michael Pieler and USVSN Sai Prashanth and Shivanshu Purohit and Laria Reynolds and Jonathan Tow and Ben Wang and Samuel Weinbach View PDF Abstract: We introduce GPT-NeoX-20B, a 20 billion parameter autoregressive language model trained on the Pile, whose weights will be made freely and openly available to the public through a permissive license. It is, to the best of our knowledge, the largest dense autoregressive model that has publicly available weights at the time of submission. In this work, we describe \\model{}&#39;s architecture and training and evaluate its performance on a range of language-understanding, mathematics, and knowledge-based tasks. We find that GPT-NeoX-20B is a particularly powerful few-shot reasoner and gains far more in performance when evaluated five-shot than similarly sized GPT-3 and FairSeq models. We open-source the training and evaluation code, as well as the model weights, at this https URL . Comments: To appear in the Proceedings of the ACL Workshop on Challenges &amp; Perspectives in Creating Large Language Models Subjects: Computation and Language (cs.CL) Cite as: arXiv:2204.06745 [cs.CL] &nbsp; (or arXiv:2204.06745v1 [cs.CL] for this version) &nbsp; https://doi.org/10.48550/arXiv.2204.06745 Focus to learn more arXiv-issued DOI via DataCite Submission history From: Stella Biderman [ view email ] [v1] Thu, 14 Apr 2022 04:00:27 UTC (6,828 KB) Full-text links: Access Paper: View a PDF of the paper titled GPT-NeoX-20B: An Open-Source Autoregressive Language Model, by Sid Black and Stella Biderman and Eric Hallahan and Quentin Anthony and Leo Gao and Laurence Golding and Horace He and Connor Leahy and Kyle McDonell and Jason Phang and Michael Pieler and USVSN Sai Prashanth and Shivanshu Purohit and Laria Reynolds and Jonathan Tow and Ben Wang and Samuel Weinbach View PDF TeX Source Other Formats view license Current browse context: cs.CL &lt;&nbsp;prev &nbsp; | &nbsp; next&nbsp;&gt; new | recent | 2022-04 Change to browse by: cs References &amp; Citations NASA ADS Google Scholar Semantic Scholar 1 blog link ( what is this? ) a export BibTeX citation Loading... BibTeX formatted citation &times; loading... Data provided by: Bookmark Bibliographic Tools Bibliographic and Citation Tools Bibliographic Explorer Toggle Bibliographic Explorer ( What is the Explorer? ) Connected Papers Toggle Connected Papers ( What is Connected Papers? ) Litmaps Toggle Litmaps ( What is Litmaps? ) scite.ai Toggle scite Smart Citations ( What are Smart Citations? ) Code, Data, Media Code, Data and Media Associated with this Article alphaXiv Toggle alphaXiv ( What is alphaXiv? ) Links to Code Toggle CatalyzeX Code Finder for Papers ( What is CatalyzeX? ) DagsHub Toggle DagsHub ( What is DagsHub? ) GotitPub Toggle Gotit.pub ( What is GotitPub? ) Huggingface Toggle Hugging Face ( What is Huggingface? ) Links to Code Toggle Papers with Code ( What is Papers with Code? ) ScienceCast Toggle ScienceCast ( What is ScienceCast? ) Demos Demos Replicate Toggle Replicate ( What is Replicate? ) Spaces Toggle Hugging Face Spaces ( What is Spaces? ) Spaces Toggle TXYZ.AI ( What is TXYZ.AI? ) Related Papers Recommenders and Search Tools Link to Influence Flower Influence Flower ( What are Influence Flowers? ) Core recommender toggle CORE Recommender ( What is CORE? ) Author Venue Institution Topic About arXivLabs arXivLabs: experimental projects with community collaborators arXivLabs is a framework that allows collaborators to develop and share new arXiv features directly on our website. Both individuals and organizations that work with arXivLabs have embraced and accepted our values of openness, community, excellence, and user data privacy. arXiv is committed to these values and only works with partners that adhere to them. Have an idea for a project that will add value for arXiv's community? Learn more about arXivLabs . Which authors of this paper are endorsers? | Disable MathJax ( What is MathJax? ) About Help contact arXiv Click here to contact arXiv Contact subscribe to arXiv mailings Click here to subscribe Subscribe Copyright Privacy Policy Web Accessibility Assistance arXiv Operational Status Get status notifications via email or slack ",
      "fetch_method": "direct-html"
    },
    {
      "arxiv_id": "https://r.jina.ai/https://docs.olcf.ornl.gov/systems/summit_user_guide.html#job-launcher-jsrun",
      "full_text": "Title: 404 Not Found\n\nURL Source: https://docs.olcf.ornl.gov/systems/summit_user_guide.html\n\nWarning: Target URL returned error 404: Not Found\n\nMarkdown Content:\n404 Not Found\n\n===============\n\n404 Not Found\n=============\n\n* * *\n\nnginx/1.20.1\n",
      "fetch_method": "jina-reader"
    },
    {
      "arxiv_id": "https://docs.nvidia.com/datacenter/cloud-native/container-toolkit/latest/install-guide.html",
      "full_text": " Installing the NVIDIA Container Toolkit &#8212; NVIDIA Container Toolkit Skip to main content Back to top Ctrl + K NVIDIA Container Toolkit Choose version Search Ctrl + K Search Ctrl + K NVIDIA Container Toolkit Choose version Table of Contents NVIDIA Container Toolkit Overview Installing the Toolkit Running a Sample Workload Platform support Troubleshooting Release Notes Advanced Configuration Architecture Overview Container Device Interface Specialized Configurations with Docker NVIDIA Docs Hub Cloud Native Technologies NVIDIA Container Toolkit Installing the NVIDIA Container Toolkit Installing the NVIDIA Container Toolkit # Installation # Prerequisites # Read this section about platform support. Install the NVIDIA GPU driver for your Linux distribution. NVIDIA recommends installing the driver by using the package manager for your distribution. For information about installing the driver with a package manager, refer to the NVIDIA Driver Installation Quickstart Guide . Alternatively, you can install the driver by downloading a .run installer. Note There is a known issue on systems where systemd cgroup drivers are used that cause containers to lose access to requested GPUs when systemctl daemon reload is run. Please see the troubleshooting documentation for more information. With apt : Ubuntu, Debian # Note These instructions should work for any Debian-derived distribution. Configure the production repository: $ curl -fsSL https://nvidia.github.io/libnvidia-container/gpgkey | sudo gpg --dearmor -o /usr/share/keyrings/nvidia-container-toolkit-keyring.gpg \\ &amp;&amp; curl -s -L https://nvidia.github.io/libnvidia-container/stable/deb/nvidia-container-toolkit.list | \\ sed &#39;s#deb https://#deb [signed-by=/usr/share/keyrings/nvidia-container-toolkit-keyring.gpg] https://#g&#39; | \\ sudo tee /etc/apt/sources.list.d/nvidia-container-toolkit.list Optionally, configure the repository to use experimental packages: $ sed -i -e &#39;/experimental/ s/^#//g&#39; /etc/apt/sources.list.d/nvidia-container-toolkit.list Update the packages list from the repository: $ sudo apt-get update `. --> Install the NVIDIA Container Toolkit packages: $ export NVIDIA_CONTAINER_TOOLKIT_VERSION = 1 .17.8-1 sudo apt-get install -y \\ nvidia-container-toolkit=${NVIDIA_CONTAINER_TOOLKIT_VERSION} \\ nvidia-container-toolkit-base=${NVIDIA_CONTAINER_TOOLKIT_VERSION} \\ libnvidia-container-tools=${NVIDIA_CONTAINER_TOOLKIT_VERSION} \\ libnvidia-container1=${NVIDIA_CONTAINER_TOOLKIT_VERSION} With dnf : RHEL/CentOS, Fedora, Amazon Linux # Note These instructions should work for many RPM-based distributions. Configure the production repository: $ curl -s -L https://nvidia.github.io/libnvidia-container/stable/rpm/nvidia-container-toolkit.repo | \\ sudo tee /etc/yum.repos.d/nvidia-container-toolkit.repo Optionally, configure the repository to use experimental packages: $ sudo dnf-config-manager --enable nvidia-container-toolkit-experimental Install the NVIDIA Container Toolkit packages: $ export NVIDIA_CONTAINER_TOOLKIT_VERSION = 1 .17.8-1 sudo dnf install -y \\ nvidia-container-toolkit-${NVIDIA_CONTAINER_TOOLKIT_VERSION} \\ nvidia-container-toolkit-base-${NVIDIA_CONTAINER_TOOLKIT_VERSION} \\ libnvidia-container-tools-${NVIDIA_CONTAINER_TOOLKIT_VERSION} \\ libnvidia-container1-${NVIDIA_CONTAINER_TOOLKIT_VERSION} With zypper : OpenSUSE, SLE # Configure the production repository: $ sudo zypper ar https://nvidia.github.io/libnvidia-container/stable/rpm/nvidia-container-toolkit.repo Optionally, configure the repository to use experimental packages: $ sudo zypper modifyrepo --enable nvidia-container-toolkit-experimental Install the NVIDIA Container Toolkit packages: $ export NVIDIA_CONTAINER_TOOLKIT_VERSION = 1 .17.8-1 sudo zypper --gpg-auto-import-keys install -y \\ nvidia-container-toolkit-${NVIDIA_CONTAINER_TOOLKIT_VERSION} \\ nvidia-container-toolkit-base-${NVIDIA_CONTAINER_TOOLKIT_VERSION} \\ libnvidia-container-tools-${NVIDIA_CONTAINER_TOOLKIT_VERSION} \\ libnvidia-container1-${NVIDIA_CONTAINER_TOOLKIT_VERSION} Configuration # Prerequisites # You installed a supported container engine (Docker, Containerd, CRI-O, Podman). You installed the NVIDIA Container Toolkit. Configuring Docker # Configure the container runtime by using the nvidia-ctk command: $ sudo nvidia-ctk runtime configure --runtime = docker The nvidia-ctk command modifies the /etc/docker/daemon.json file on the host. The file is updated so that Docker can use the NVIDIA Container Runtime. Restart the Docker daemon: $ sudo systemctl restart docker Rootless mode # To configure the container runtime for Docker running in Rootless mode , follow these steps: Configure the container runtime by using the nvidia-ctk command: $ nvidia-ctk runtime configure --runtime = docker --config = $HOME /.config/docker/daemon.json Restart the Rootless Docker daemon: $ systemctl --user restart docker Configure /etc/nvidia-container-runtime/config.toml by using the sudo nvidia-ctk command: $ sudo nvidia-ctk config --set nvidia-container-cli.no-cgroups --in-place Configuring containerd (for Kubernetes) # Configure the container runtime by using the nvidia-ctk command: $ sudo nvidia-ctk runtime configure --runtime = containerd The nvidia-ctk command modifies the /etc/containerd/config.toml file on the host. The file is updated so that containerd can use the NVIDIA Container Runtime. Restart containerd: $ sudo systemctl restart containerd Configuring containerd (for nerdctl) # No additional configuration is needed. You can just run nerdctl run --gpus=all , with root or without root. You do not need to run the nvidia-ctk command mentioned above for Kubernetes. See also the nerdctl documentation . Configuring CRI-O # Configure the container runtime by using the nvidia-ctk command: $ sudo nvidia-ctk runtime configure --runtime = crio The nvidia-ctk command modifies the /etc/crio/crio.conf file on the host. The file is updated so that CRI-O can use the NVIDIA Container Runtime. Restart the CRI-O daemon: $ sudo systemctl restart crio Configuring Podman # For Podman, NVIDIA recommends using CDI for accessing NVIDIA devices in containers. Next Steps # Running a Sample Workload previous Overview next Running a Sample Workload On this page Installation Prerequisites With apt : Ubuntu, Debian With dnf : RHEL/CentOS, Fedora, Amazon Linux With zypper : OpenSUSE, SLE Configuration Prerequisites Configuring Docker Rootless mode Configuring containerd (for Kubernetes) Configuring containerd (for nerdctl) Configuring CRI-O Configuring Podman Next Steps so the DOM is not blocked --> Privacy Policy | Manage My Privacy | Do Not Sell or Share My Data | Terms of Service | Accessibility | Corporate Policies | Product Security | Contact Copyright Â© 2020-2025, NVIDIA Corporation. ",
      "fetch_method": "direct-html"
    },
    {
      "arxiv_id": "https://apptainer.org/docs/user/1.0/index.html",
      "full_text": " Apptainer User Guide &mdash; Apptainer User Guide 1.0 documentation Apptainer User Guide 1.0 Introduction to Apptainer Quick Start Security in Apptainer Build a container The Definition File Build Environment Fakeroot feature Sign and Verify Key management commands Encrypted Containers Remote Endpoints Library API Registries Bind Paths and Mounts Persistent Overlays Running Services Environment and Metadata Plugins Security Options Network Options Cgroups Support Application Checkpointing Singularity Compatibility Support for Docker / OCI Containers OCI Runtime Support Apptainer and MPI applications GPU Support Contributing Appendix Command Line Reference License Apptainer User Guide Warning You're browsing the documentation for an old version of Apptainer. Consider upgrading to the latest version of Apptainer. Documentation for the latest version can always be found at: https://apptainer.org/docs/user/latest/ Apptainer User Guide Edit on GitHub Apptainer User Guide  Welcome to the Apptainer User Guide! This guide aims to give an introduction to Apptainer, brief installation instructions, and cover topics relevant to users building and running containers. For a detailed guide to installation and configuration, please see the separate Admin Guide for this version of Apptainer. Getting Started &amp; Background Information  Introduction to Apptainer Why use Apptainer? Why use containers? Use Cases Quick Start Quick Installation Steps Overview of the Apptainer Interface Downloading images Interact with images Working with Files Build images from scratch Security in Apptainer Security Policy Background Setuid &amp; User Namespaces Runtime &amp; User Privilege Model Singularity Image Format (SIF) Configuration &amp; Runtime Options Building Containers  Learn how to write a definition file that can be used to build a container. Understand the environment within a build, how to perform remote builds, and how to use the --fakeroot feature to build as a non-root user. Build a container The Definition File Build Environment Fakeroot feature Container Signing &amp; Encryption  Apptainer allows containers to be signed using a PGP key. The signature travels with the container image, allowing you to verify that the image is unmodified at any time. Encryption of containers using LUKS2 is also supported. Encrypted containers can be run without decrypting them to disk first. Sign and Verify Key management commands Encrypted Containers Sharing &amp; Online Services  Remote Endpoints Library API Registries Advanced Usage  Once you’ve understood the basics, explore all the options which Apptainer provides for accessing data, running persistent services in containers, manipulating the container environment, and applying networking and security configuration. Bind Paths and Mounts Persistent Overlays Running Services Environment and Metadata Plugins Security Options Network Options Cgroups Support Application Checkpointing Compatibility  Apptainer has unique benefits and supports easy access to GPUs and other hardware. It also strives for compatibility with Docker/OCI container formats. Understand the differences between Apptainer and Docker, as well as how to use containerized MPI and GPU applications. Singularity Compatibility Support for Docker / OCI Containers OCI Runtime Support Apptainer and MPI applications GPU Support Get Involved  We’d love you to get involved in the Apptainer community! Whether through contributing feature and fixes, helping to answer questions from other users, or simply testing new releases. Contributing Reference  Appendix Apptainer’s environment variables Build Modules Command Line Reference License Next Built with Sphinx using a theme provided by Read the Docs . &copy; Contributors to the Apptainer project, established as Apptainer a Series of LF Projects LLC &copy; 2017-2022, Sylabs Inc ",
      "fetch_method": "direct-html"
    },
    {
      "arxiv_id": "https://arxiv.org/abs/2101.00027",
      "full_text": " [2101.00027] The Pile: An 800GB Dataset of Diverse Text for Language Modeling Skip to main content We gratefully acknowledge support from the Simons Foundation, member institutions , and all contributors. Donate &gt; cs &gt; arXiv:2101.00027 Help | Advanced Search All fields Title Author Abstract Comments Journal reference ACM classification MSC classification Report number arXiv identifier DOI ORCID arXiv author ID Help pages Full text Search open search GO open navigation menu quick links Login Help Pages About --> Computer Science > Computation and Language arXiv:2101.00027 (cs) [Submitted on 31 Dec 2020] Title: The Pile: An 800GB Dataset of Diverse Text for Language Modeling Authors: Leo Gao , Stella Biderman , Sid Black , Laurence Golding , Travis Hoppe , Charles Foster , Jason Phang , Horace He , Anish Thite , Noa Nabeshima , Shawn Presser , Connor Leahy View a PDF of the paper titled The Pile: An 800GB Dataset of Diverse Text for Language Modeling, by Leo Gao and 11 other authors View PDF Abstract: Recent work has demonstrated that increased training dataset diversity improves general cross-domain knowledge and downstream generalization capability for large-scale language models. With this in mind, we present \\textit{the Pile}: an 825 GiB English text corpus targeted at training large-scale language models. The Pile is constructed from 22 diverse high-quality subsets -- both existing and newly constructed -- many of which derive from academic or professional sources. Our evaluation of the untuned performance of GPT-2 and GPT-3 on the Pile shows that these models struggle on many of its components, such as academic writing. Conversely, models trained on the Pile improve significantly over both Raw CC and CC-100 on all components of the Pile, while improving performance on downstream evaluations. Through an in-depth exploratory analysis, we document potentially concerning aspects of the data for prospective users. We make publicly available the code used in its construction. Subjects: Computation and Language (cs.CL) Cite as: arXiv:2101.00027 [cs.CL] &nbsp; (or arXiv:2101.00027v1 [cs.CL] for this version) &nbsp; https://doi.org/10.48550/arXiv.2101.00027 Focus to learn more arXiv-issued DOI via DataCite Submission history From: Leo Gao [ view email ] [v1] Thu, 31 Dec 2020 19:00:10 UTC (2,152 KB) Full-text links: Access Paper: View a PDF of the paper titled The Pile: An 800GB Dataset of Diverse Text for Language Modeling, by Leo Gao and 11 other authors View PDF TeX Source Other Formats view license Current browse context: cs.CL &lt;&nbsp;prev &nbsp; | &nbsp; next&nbsp;&gt; new | recent | 2021-01 Change to browse by: cs References &amp; Citations NASA ADS Google Scholar Semantic Scholar 4 blog links ( what is this? ) DBLP - CS Bibliography listing | bibtex Stella Biderman Jason Phang Horace He a export BibTeX citation Loading... BibTeX formatted citation &times; loading... Data provided by: Bookmark Bibliographic Tools Bibliographic and Citation Tools Bibliographic Explorer Toggle Bibliographic Explorer ( What is the Explorer? ) Connected Papers Toggle Connected Papers ( What is Connected Papers? ) Litmaps Toggle Litmaps ( What is Litmaps? ) scite.ai Toggle scite Smart Citations ( What are Smart Citations? ) Code, Data, Media Code, Data and Media Associated with this Article alphaXiv Toggle alphaXiv ( What is alphaXiv? ) Links to Code Toggle CatalyzeX Code Finder for Papers ( What is CatalyzeX? ) DagsHub Toggle DagsHub ( What is DagsHub? ) GotitPub Toggle Gotit.pub ( What is GotitPub? ) Huggingface Toggle Hugging Face ( What is Huggingface? ) Links to Code Toggle Papers with Code ( What is Papers with Code? ) ScienceCast Toggle ScienceCast ( What is ScienceCast? ) Demos Demos Replicate Toggle Replicate ( What is Replicate? ) Spaces Toggle Hugging Face Spaces ( What is Spaces? ) Spaces Toggle TXYZ.AI ( What is TXYZ.AI? ) Related Papers Recommenders and Search Tools Link to Influence Flower Influence Flower ( What are Influence Flowers? ) Core recommender toggle CORE Recommender ( What is CORE? ) Author Venue Institution Topic About arXivLabs arXivLabs: experimental projects with community collaborators arXivLabs is a framework that allows collaborators to develop and share new arXiv features directly on our website. Both individuals and organizations that work with arXivLabs have embraced and accepted our values of openness, community, excellence, and user data privacy. arXiv is committed to these values and only works with partners that adhere to them. Have an idea for a project that will add value for arXiv's community? Learn more about arXivLabs . Which authors of this paper are endorsers? | Disable MathJax ( What is MathJax? ) About Help contact arXiv Click here to contact arXiv Contact subscribe to arXiv mailings Click here to subscribe Subscribe Copyright Privacy Policy Web Accessibility Assistance arXiv Operational Status Get status notifications via email or slack ",
      "fetch_method": "direct-html"
    },
    {
      "arxiv_id": "https://arxiv.org/abs/2304.01373",
      "full_text": " [2304.01373] Pythia: A Suite for Analyzing Large Language Models Across Training and Scaling Skip to main content We gratefully acknowledge support from the Simons Foundation, member institutions , and all contributors. Donate &gt; cs &gt; arXiv:2304.01373 Help | Advanced Search All fields Title Author Abstract Comments Journal reference ACM classification MSC classification Report number arXiv identifier DOI ORCID arXiv author ID Help pages Full text Search open search GO open navigation menu quick links Login Help Pages About --> Computer Science > Computation and Language arXiv:2304.01373 (cs) [Submitted on 3 Apr 2023 ( v1 ), last revised 31 May 2023 (this version, v2)] Title: Pythia: A Suite for Analyzing Large Language Models Across Training and Scaling Authors: Stella Biderman , Hailey Schoelkopf , Quentin Anthony , Herbie Bradley , Kyle O&#39;Brien , Eric Hallahan , Mohammad Aflah Khan , Shivanshu Purohit , USVSN Sai Prashanth , Edward Raff , Aviya Skowron , Lintang Sutawika , Oskar van der Wal View a PDF of the paper titled Pythia: A Suite for Analyzing Large Language Models Across Training and Scaling, by Stella Biderman and 12 other authors View PDF Abstract: How do large language models (LLMs) develop and evolve over the course of training? How do these patterns change as models scale? To answer these questions, we introduce \\textit{Pythia}, a suite of 16 LLMs all trained on public data seen in the exact same order and ranging in size from 70M to 12B parameters. We provide public access to 154 checkpoints for each one of the 16 models, alongside tools to download and reconstruct their exact training dataloaders for further study. We intend \\textit{Pythia} to facilitate research in many areas, and we present several case studies including novel results in memorization, term frequency effects on few-shot performance, and reducing gender bias. We demonstrate that this highly controlled setup can be used to yield novel insights toward LLMs and their training dynamics. Trained models, analysis code, training code, and training data can be found at \\url{ this https URL }. Comments: Code at this https URL Subjects: Computation and Language (cs.CL) Cite as: arXiv:2304.01373 [cs.CL] &nbsp; (or arXiv:2304.01373v2 [cs.CL] for this version) &nbsp; https://doi.org/10.48550/arXiv.2304.01373 Focus to learn more arXiv-issued DOI via DataCite Submission history From: Hailey Schoelkopf [ view email ] [v1] Mon, 3 Apr 2023 20:58:15 UTC (2,683 KB) [v2] Wed, 31 May 2023 17:54:07 UTC (2,371 KB) Full-text links: Access Paper: View a PDF of the paper titled Pythia: A Suite for Analyzing Large Language Models Across Training and Scaling, by Stella Biderman and 12 other authors View PDF TeX Source Other Formats view license Current browse context: cs.CL &lt;&nbsp;prev &nbsp; | &nbsp; next&nbsp;&gt; new | recent | 2023-04 Change to browse by: cs References &amp; Citations NASA ADS Google Scholar Semantic Scholar a export BibTeX citation Loading... BibTeX formatted citation &times; loading... Data provided by: Bookmark Bibliographic Tools Bibliographic and Citation Tools Bibliographic Explorer Toggle Bibliographic Explorer ( What is the Explorer? ) Connected Papers Toggle Connected Papers ( What is Connected Papers? ) Litmaps Toggle Litmaps ( What is Litmaps? ) scite.ai Toggle scite Smart Citations ( What are Smart Citations? ) Code, Data, Media Code, Data and Media Associated with this Article alphaXiv Toggle alphaXiv ( What is alphaXiv? ) Links to Code Toggle CatalyzeX Code Finder for Papers ( What is CatalyzeX? ) DagsHub Toggle DagsHub ( What is DagsHub? ) GotitPub Toggle Gotit.pub ( What is GotitPub? ) Huggingface Toggle Hugging Face ( What is Huggingface? ) Links to Code Toggle Papers with Code ( What is Papers with Code? ) ScienceCast Toggle ScienceCast ( What is ScienceCast? ) Demos Demos Replicate Toggle Replicate ( What is Replicate? ) Spaces Toggle Hugging Face Spaces ( What is Spaces? ) Spaces Toggle TXYZ.AI ( What is TXYZ.AI? ) Related Papers Recommenders and Search Tools Link to Influence Flower Influence Flower ( What are Influence Flowers? ) Core recommender toggle CORE Recommender ( What is CORE? ) Author Venue Institution Topic About arXivLabs arXivLabs: experimental projects with community collaborators arXivLabs is a framework that allows collaborators to develop and share new arXiv features directly on our website. Both individuals and organizations that work with arXivLabs have embraced and accepted our values of openness, community, excellence, and user data privacy. arXiv is committed to these values and only works with partners that adhere to them. Have an idea for a project that will add value for arXiv's community? Learn more about arXivLabs . Which authors of this paper are endorsers? | Disable MathJax ( What is MathJax? ) About Help contact arXiv Click here to contact arXiv Contact subscribe to arXiv mailings Click here to subscribe Subscribe Copyright Privacy Policy Web Accessibility Assistance arXiv Operational Status Get status notifications via email or slack ",
      "fetch_method": "direct-html"
    },
    {
      "arxiv_id": "https://arxiv.org/abs/1910.03771",
      "full_text": " [1910.03771] HuggingFace&#39;s Transformers: State-of-the-art Natural Language Processing Skip to main content We gratefully acknowledge support from the Simons Foundation, member institutions , and all contributors. Donate &gt; cs &gt; arXiv:1910.03771 Help | Advanced Search All fields Title Author Abstract Comments Journal reference ACM classification MSC classification Report number arXiv identifier DOI ORCID arXiv author ID Help pages Full text Search open search GO open navigation menu quick links Login Help Pages About --> Computer Science > Computation and Language arXiv:1910.03771 (cs) [Submitted on 9 Oct 2019 ( v1 ), last revised 14 Jul 2020 (this version, v5)] Title: HuggingFace&#39;s Transformers: State-of-the-art Natural Language Processing Authors: Thomas Wolf , Lysandre Debut , Victor Sanh , Julien Chaumond , Clement Delangue , Anthony Moi , Pierric Cistac , Tim Rault , Rémi Louf , Morgan Funtowicz , Joe Davison , Sam Shleifer , Patrick von Platen , Clara Ma , Yacine Jernite , Julien Plu , Canwen Xu , Teven Le Scao , Sylvain Gugger , Mariama Drame , Quentin Lhoest , Alexander M. Rush View a PDF of the paper titled HuggingFace&#39;s Transformers: State-of-the-art Natural Language Processing, by Thomas Wolf and Lysandre Debut and Victor Sanh and Julien Chaumond and Clement Delangue and Anthony Moi and Pierric Cistac and Tim Rault and R\\&#39;emi Louf and Morgan Funtowicz and Joe Davison and Sam Shleifer and Patrick von Platen and Clara Ma and Yacine Jernite and Julien Plu and Canwen Xu and Teven Le Scao and Sylvain Gugger and Mariama Drame and Quentin Lhoest and Alexander M. Rush View PDF Abstract: Recent progress in natural language processing has been driven by advances in both model architecture and model pretraining. Transformer architectures have facilitated building higher-capacity models and pretraining has made it possible to effectively utilize this capacity for a wide variety of tasks. \\textit{Transformers} is an open-source library with the goal of opening up these advances to the wider machine learning community. The library consists of carefully engineered state-of-the art Transformer architectures under a unified API. Backing this library is a curated collection of pretrained models made by and available for the community. \\textit{Transformers} is designed to be extensible by researchers, simple for practitioners, and fast and robust in industrial deployments. The library is available at \\url{ this https URL }. Comments: 8 pages, 4 figures, more details at this https URL Subjects: Computation and Language (cs.CL) Cite as: arXiv:1910.03771 [cs.CL] &nbsp; (or arXiv:1910.03771v5 [cs.CL] for this version) &nbsp; https://doi.org/10.48550/arXiv.1910.03771 Focus to learn more arXiv-issued DOI via DataCite Submission history From: Victor Sanh [ view email ] [v1] Wed, 9 Oct 2019 03:23:22 UTC (530 KB) [v2] Mon, 14 Oct 2019 15:33:45 UTC (530 KB) [v3] Wed, 16 Oct 2019 15:36:45 UTC (530 KB) [v4] Tue, 11 Feb 2020 14:42:10 UTC (530 KB) [v5] Tue, 14 Jul 2020 03:42:34 UTC (642 KB) Full-text links: Access Paper: View a PDF of the paper titled HuggingFace&#39;s Transformers: State-of-the-art Natural Language Processing, by Thomas Wolf and Lysandre Debut and Victor Sanh and Julien Chaumond and Clement Delangue and Anthony Moi and Pierric Cistac and Tim Rault and R\\&#39;emi Louf and Morgan Funtowicz and Joe Davison and Sam Shleifer and Patrick von Platen and Clara Ma and Yacine Jernite and Julien Plu and Canwen Xu and Teven Le Scao and Sylvain Gugger and Mariama Drame and Quentin Lhoest and Alexander M. Rush View PDF TeX Source Other Formats view license Current browse context: cs.CL &lt;&nbsp;prev &nbsp; | &nbsp; next&nbsp;&gt; new | recent | 2019-10 Change to browse by: cs References &amp; Citations NASA ADS Google Scholar Semantic Scholar DBLP - CS Bibliography listing | bibtex Thomas Wolf Victor Sanh Julien Chaumond Clement Delangue Rémi Louf export BibTeX citation Loading... BibTeX formatted citation &times; loading... Data provided by: Bookmark Bibliographic Tools Bibliographic and Citation Tools Bibliographic Explorer Toggle Bibliographic Explorer ( What is the Explorer? ) Connected Papers Toggle Connected Papers ( What is Connected Papers? ) Litmaps Toggle Litmaps ( What is Litmaps? ) scite.ai Toggle scite Smart Citations ( What are Smart Citations? ) Code, Data, Media Code, Data and Media Associated with this Article alphaXiv Toggle alphaXiv ( What is alphaXiv? ) Links to Code Toggle CatalyzeX Code Finder for Papers ( What is CatalyzeX? ) DagsHub Toggle DagsHub ( What is DagsHub? ) GotitPub Toggle Gotit.pub ( What is GotitPub? ) Huggingface Toggle Hugging Face ( What is Huggingface? ) Links to Code Toggle Papers with Code ( What is Papers with Code? ) ScienceCast Toggle ScienceCast ( What is ScienceCast? ) Demos Demos Replicate Toggle Replicate ( What is Replicate? ) Spaces Toggle Hugging Face Spaces ( What is Spaces? ) Spaces Toggle TXYZ.AI ( What is TXYZ.AI? ) Related Papers Recommenders and Search Tools Link to Influence Flower Influence Flower ( What are Influence Flowers? ) Core recommender toggle CORE Recommender ( What is CORE? ) Author Venue Institution Topic About arXivLabs arXivLabs: experimental projects with community collaborators arXivLabs is a framework that allows collaborators to develop and share new arXiv features directly on our website. Both individuals and organizations that work with arXivLabs have embraced and accepted our values of openness, community, excellence, and user data privacy. arXiv is committed to these values and only works with partners that adhere to them. Have an idea for a project that will add value for arXiv's community? Learn more about arXivLabs . Which authors of this paper are endorsers? | Disable MathJax ( What is MathJax? ) About Help contact arXiv Click here to contact arXiv Contact subscribe to arXiv mailings Click here to subscribe Subscribe Copyright Privacy Policy Web Accessibility Assistance arXiv Operational Status Get status notifications via email or slack ",
      "fetch_method": "direct-html"
    },
    {
      "arxiv_id": "https://arxiv.org/abs/2302.12433",
      "full_text": " [2302.12433] ProofNet: Autoformalizing and Formally Proving Undergraduate-Level Mathematics Skip to main content We gratefully acknowledge support from the Simons Foundation, member institutions , and all contributors. Donate &gt; cs &gt; arXiv:2302.12433 Help | Advanced Search All fields Title Author Abstract Comments Journal reference ACM classification MSC classification Report number arXiv identifier DOI ORCID arXiv author ID Help pages Full text Search open search GO open navigation menu quick links Login Help Pages About --> Computer Science > Computation and Language arXiv:2302.12433 (cs) [Submitted on 24 Feb 2023] Title: ProofNet: Autoformalizing and Formally Proving Undergraduate-Level Mathematics Authors: Zhangir Azerbayev , Bartosz Piotrowski , Hailey Schoelkopf , Edward W. Ayers , Dragomir Radev , Jeremy Avigad View a PDF of the paper titled ProofNet: Autoformalizing and Formally Proving Undergraduate-Level Mathematics, by Zhangir Azerbayev and 5 other authors View PDF Abstract: We introduce ProofNet, a benchmark for autoformalization and formal proving of undergraduate-level mathematics. The ProofNet benchmarks consists of 371 examples, each consisting of a formal theorem statement in Lean 3, a natural language theorem statement, and a natural language proof. The problems are primarily drawn from popular undergraduate pure mathematics textbooks and cover topics such as real and complex analysis, linear algebra, abstract algebra, and topology. We intend for ProofNet to be a challenging benchmark that will drive progress in autoformalization and automatic theorem proving. We report baseline results on statement autoformalization via in-context learning. Moreover, we introduce two novel statement autoformalization methods: prompt retrieval and distilled backtranslation. Subjects: Computation and Language (cs.CL) ; Artificial Intelligence (cs.AI); Logic in Computer Science (cs.LO) Cite as: arXiv:2302.12433 [cs.CL] &nbsp; (or arXiv:2302.12433v1 [cs.CL] for this version) &nbsp; https://doi.org/10.48550/arXiv.2302.12433 Focus to learn more arXiv-issued DOI via DataCite Submission history From: Zhangir Azerbayev Mr [ view email ] [v1] Fri, 24 Feb 2023 03:28:46 UTC (119 KB) Full-text links: Access Paper: View a PDF of the paper titled ProofNet: Autoformalizing and Formally Proving Undergraduate-Level Mathematics, by Zhangir Azerbayev and 5 other authors View PDF TeX Source Other Formats view license Current browse context: cs.CL &lt;&nbsp;prev &nbsp; | &nbsp; next&nbsp;&gt; new | recent | 2023-02 Change to browse by: cs cs.AI cs.LO References &amp; Citations NASA ADS Google Scholar Semantic Scholar export BibTeX citation Loading... BibTeX formatted citation &times; loading... Data provided by: Bookmark Bibliographic Tools Bibliographic and Citation Tools Bibliographic Explorer Toggle Bibliographic Explorer ( What is the Explorer? ) Connected Papers Toggle Connected Papers ( What is Connected Papers? ) Litmaps Toggle Litmaps ( What is Litmaps? ) scite.ai Toggle scite Smart Citations ( What are Smart Citations? ) Code, Data, Media Code, Data and Media Associated with this Article alphaXiv Toggle alphaXiv ( What is alphaXiv? ) Links to Code Toggle CatalyzeX Code Finder for Papers ( What is CatalyzeX? ) DagsHub Toggle DagsHub ( What is DagsHub? ) GotitPub Toggle Gotit.pub ( What is GotitPub? ) Huggingface Toggle Hugging Face ( What is Huggingface? ) Links to Code Toggle Papers with Code ( What is Papers with Code? ) ScienceCast Toggle ScienceCast ( What is ScienceCast? ) Demos Demos Replicate Toggle Replicate ( What is Replicate? ) Spaces Toggle Hugging Face Spaces ( What is Spaces? ) Spaces Toggle TXYZ.AI ( What is TXYZ.AI? ) Related Papers Recommenders and Search Tools Link to Influence Flower Influence Flower ( What are Influence Flowers? ) Core recommender toggle CORE Recommender ( What is CORE? ) Author Venue Institution Topic About arXivLabs arXivLabs: experimental projects with community collaborators arXivLabs is a framework that allows collaborators to develop and share new arXiv features directly on our website. Both individuals and organizations that work with arXivLabs have embraced and accepted our values of openness, community, excellence, and user data privacy. arXiv is committed to these values and only works with partners that adhere to them. Have an idea for a project that will add value for arXiv's community? Learn more about arXivLabs . Which authors of this paper are endorsers? | Disable MathJax ( What is MathJax? ) About Help contact arXiv Click here to contact arXiv Contact subscribe to arXiv mailings Click here to subscribe Subscribe Copyright Privacy Policy Web Accessibility Assistance arXiv Operational Status Get status notifications via email or slack ",
      "fetch_method": "direct-html"
    },
    {
      "arxiv_id": "https://arxiv.org/abs/2304.11158",
      "full_text": " [2304.11158] Emergent and Predictable Memorization in Large Language Models Skip to main content We gratefully acknowledge support from the Simons Foundation, member institutions , and all contributors. Donate &gt; cs &gt; arXiv:2304.11158 Help | Advanced Search All fields Title Author Abstract Comments Journal reference ACM classification MSC classification Report number arXiv identifier DOI ORCID arXiv author ID Help pages Full text Search open search GO open navigation menu quick links Login Help Pages About --> Computer Science > Computation and Language arXiv:2304.11158 (cs) [Submitted on 21 Apr 2023 ( v1 ), last revised 31 May 2023 (this version, v2)] Title: Emergent and Predictable Memorization in Large Language Models Authors: Stella Biderman , USVSN Sai Prashanth , Lintang Sutawika , Hailey Schoelkopf , Quentin Anthony , Shivanshu Purohit , Edward Raff View a PDF of the paper titled Emergent and Predictable Memorization in Large Language Models, by Stella Biderman and USVSN Sai Prashanth and Lintang Sutawika and Hailey Schoelkopf and Quentin Anthony and Shivanshu Purohit and Edward Raff View PDF Abstract: Memorization, or the tendency of large language models (LLMs) to output entire sequences from their training data verbatim, is a key concern for safely deploying language models. In particular, it is vital to minimize a model&#39;s memorization of sensitive datapoints such as those containing personal identifiable information (PII). The prevalence of such undesirable memorization can pose issues for model trainers, and may even require discarding an otherwise functional model. We therefore seek to predict which sequences will be memorized before a large model&#39;s full train-time by extrapolating the memorization behavior of lower-compute trial runs. We measure memorization of the Pythia model suite and plot scaling laws for forecasting memorization, allowing us to provide equi-compute recommendations to maximize the reliability (recall) of such predictions. We additionally provide further novel discoveries on the distribution of memorization scores across models and data. We release all code and data necessary to reproduce the results in this paper at this https URL Subjects: Computation and Language (cs.CL) Cite as: arXiv:2304.11158 [cs.CL] &nbsp; (or arXiv:2304.11158v2 [cs.CL] for this version) &nbsp; https://doi.org/10.48550/arXiv.2304.11158 Focus to learn more arXiv-issued DOI via DataCite Submission history From: Hailey Schoelkopf [ view email ] [v1] Fri, 21 Apr 2023 17:58:31 UTC (6,352 KB) [v2] Wed, 31 May 2023 19:09:45 UTC (13,432 KB) Full-text links: Access Paper: View a PDF of the paper titled Emergent and Predictable Memorization in Large Language Models, by Stella Biderman and USVSN Sai Prashanth and Lintang Sutawika and Hailey Schoelkopf and Quentin Anthony and Shivanshu Purohit and Edward Raff View PDF TeX Source Other Formats view license Current browse context: cs.CL &lt;&nbsp;prev &nbsp; | &nbsp; next&nbsp;&gt; new | recent | 2023-04 Change to browse by: cs References &amp; Citations NASA ADS Google Scholar Semantic Scholar export BibTeX citation Loading... BibTeX formatted citation &times; loading... Data provided by: Bookmark Bibliographic Tools Bibliographic and Citation Tools Bibliographic Explorer Toggle Bibliographic Explorer ( What is the Explorer? ) Connected Papers Toggle Connected Papers ( What is Connected Papers? ) Litmaps Toggle Litmaps ( What is Litmaps? ) scite.ai Toggle scite Smart Citations ( What are Smart Citations? ) Code, Data, Media Code, Data and Media Associated with this Article alphaXiv Toggle alphaXiv ( What is alphaXiv? ) Links to Code Toggle CatalyzeX Code Finder for Papers ( What is CatalyzeX? ) DagsHub Toggle DagsHub ( What is DagsHub? ) GotitPub Toggle Gotit.pub ( What is GotitPub? ) Huggingface Toggle Hugging Face ( What is Huggingface? ) Links to Code Toggle Papers with Code ( What is Papers with Code? ) ScienceCast Toggle ScienceCast ( What is ScienceCast? ) Demos Demos Replicate Toggle Replicate ( What is Replicate? ) Spaces Toggle Hugging Face Spaces ( What is Spaces? ) Spaces Toggle TXYZ.AI ( What is TXYZ.AI? ) Related Papers Recommenders and Search Tools Link to Influence Flower Influence Flower ( What are Influence Flowers? ) Core recommender toggle CORE Recommender ( What is CORE? ) Author Venue Institution Topic About arXivLabs arXivLabs: experimental projects with community collaborators arXivLabs is a framework that allows collaborators to develop and share new arXiv features directly on our website. Both individuals and organizations that work with arXivLabs have embraced and accepted our values of openness, community, excellence, and user data privacy. arXiv is committed to these values and only works with partners that adhere to them. Have an idea for a project that will add value for arXiv's community? Learn more about arXivLabs . Which authors of this paper are endorsers? | Disable MathJax ( What is MathJax? ) About Help contact arXiv Click here to contact arXiv Contact subscribe to arXiv mailings Click here to subscribe Subscribe Copyright Privacy Policy Web Accessibility Assistance arXiv Operational Status Get status notifications via email or slack ",
      "fetch_method": "direct-html"
    },
    {
      "arxiv_id": "https://arxiv.org/abs/2306.02254",
      "full_text": " [2306.02254] A Technical Report for Polyglot-Ko: Open-Source Large-Scale Korean Language Models Skip to main content We gratefully acknowledge support from the Simons Foundation, member institutions , and all contributors. Donate &gt; cs &gt; arXiv:2306.02254 Help | Advanced Search All fields Title Author Abstract Comments Journal reference ACM classification MSC classification Report number arXiv identifier DOI ORCID arXiv author ID Help pages Full text Search open search GO open navigation menu quick links Login Help Pages About --> Computer Science > Computation and Language arXiv:2306.02254 (cs) [Submitted on 4 Jun 2023 ( v1 ), last revised 6 Jun 2023 (this version, v2)] Title: A Technical Report for Polyglot-Ko: Open-Source Large-Scale Korean Language Models Authors: Hyunwoong Ko , Kichang Yang , Minho Ryu , Taekyoon Choi , Seungmu Yang , Jiwung Hyun , Sungho Park , Kyubyong Park View a PDF of the paper titled A Technical Report for Polyglot-Ko: Open-Source Large-Scale Korean Language Models, by Hyunwoong Ko and 7 other authors View PDF Abstract: Polyglot is a pioneering project aimed at enhancing the non-English language performance of multilingual language models. Despite the availability of various multilingual models such as mBERT (Devlin et al., 2019), XGLM (Lin et al., 2022), and BLOOM (Scao et al., 2022), researchers and developers often resort to building monolingual models in their respective languages due to the dissatisfaction with the current multilingual models non-English language capabilities. Addressing this gap, we seek to develop advanced multilingual language models that offer improved performance in non-English languages. In this paper, we introduce the Polyglot Korean models, which represent a specific focus rather than being multilingual in nature. In collaboration with TUNiB, our team collected 1.2TB of Korean data meticulously curated for our research journey. We made a deliberate decision to prioritize the development of Korean models before venturing into multilingual models. This choice was motivated by multiple factors: firstly, the Korean models facilitated performance comparisons with existing multilingual models; and finally, they catered to the specific needs of Korean companies and researchers. This paper presents our work in developing the Polyglot Korean models, which propose some steps towards addressing the non-English language performance gap in multilingual language models. Subjects: Computation and Language (cs.CL) Cite as: arXiv:2306.02254 [cs.CL] &nbsp; (or arXiv:2306.02254v2 [cs.CL] for this version) &nbsp; https://doi.org/10.48550/arXiv.2306.02254 Focus to learn more arXiv-issued DOI via DataCite Submission history From: Kichang Yang [ view email ] [v1] Sun, 4 Jun 2023 04:04:04 UTC (8,210 KB) [v2] Tue, 6 Jun 2023 03:27:33 UTC (8,210 KB) Full-text links: Access Paper: View a PDF of the paper titled A Technical Report for Polyglot-Ko: Open-Source Large-Scale Korean Language Models, by Hyunwoong Ko and 7 other authors View PDF TeX Source Other Formats view license Current browse context: cs.CL &lt;&nbsp;prev &nbsp; | &nbsp; next&nbsp;&gt; new | recent | 2023-06 Change to browse by: cs References &amp; Citations NASA ADS Google Scholar Semantic Scholar a export BibTeX citation Loading... BibTeX formatted citation &times; loading... Data provided by: Bookmark Bibliographic Tools Bibliographic and Citation Tools Bibliographic Explorer Toggle Bibliographic Explorer ( What is the Explorer? ) Connected Papers Toggle Connected Papers ( What is Connected Papers? ) Litmaps Toggle Litmaps ( What is Litmaps? ) scite.ai Toggle scite Smart Citations ( What are Smart Citations? ) Code, Data, Media Code, Data and Media Associated with this Article alphaXiv Toggle alphaXiv ( What is alphaXiv? ) Links to Code Toggle CatalyzeX Code Finder for Papers ( What is CatalyzeX? ) DagsHub Toggle DagsHub ( What is DagsHub? ) GotitPub Toggle Gotit.pub ( What is GotitPub? ) Huggingface Toggle Hugging Face ( What is Huggingface? ) Links to Code Toggle Papers with Code ( What is Papers with Code? ) ScienceCast Toggle ScienceCast ( What is ScienceCast? ) Demos Demos Replicate Toggle Replicate ( What is Replicate? ) Spaces Toggle Hugging Face Spaces ( What is Spaces? ) Spaces Toggle TXYZ.AI ( What is TXYZ.AI? ) Related Papers Recommenders and Search Tools Link to Influence Flower Influence Flower ( What are Influence Flowers? ) Core recommender toggle CORE Recommender ( What is CORE? ) Author Venue Institution Topic About arXivLabs arXivLabs: experimental projects with community collaborators arXivLabs is a framework that allows collaborators to develop and share new arXiv features directly on our website. Both individuals and organizations that work with arXivLabs have embraced and accepted our values of openness, community, excellence, and user data privacy. arXiv is committed to these values and only works with partners that adhere to them. Have an idea for a project that will add value for arXiv's community? Learn more about arXivLabs . Which authors of this paper are endorsers? | Disable MathJax ( What is MathJax? ) About Help contact arXiv Click here to contact arXiv Contact subscribe to arXiv mailings Click here to subscribe Subscribe Copyright Privacy Policy Web Accessibility Assistance arXiv Operational Status Get status notifications via email or slack ",
      "fetch_method": "direct-html"
    },
    {
      "arxiv_id": "https://arxiv.org/abs/2308.04014",
      "full_text": " [2308.04014] Continual Pre-Training of Large Language Models: How to (re)warm your model? Skip to main content We gratefully acknowledge support from the Simons Foundation, member institutions , and all contributors. Donate &gt; cs &gt; arXiv:2308.04014 Help | Advanced Search All fields Title Author Abstract Comments Journal reference ACM classification MSC classification Report number arXiv identifier DOI ORCID arXiv author ID Help pages Full text Search open search GO open navigation menu quick links Login Help Pages About --> Computer Science > Computation and Language arXiv:2308.04014 (cs) [Submitted on 8 Aug 2023 ( v1 ), last revised 6 Sep 2023 (this version, v2)] Title: Continual Pre-Training of Large Language Models: How to (re)warm your model? Authors: Kshitij Gupta , Benjamin Thérien , Adam Ibrahim , Mats L. Richter , Quentin Anthony , Eugene Belilovsky , Irina Rish , Timothée Lesort View a PDF of the paper titled Continual Pre-Training of Large Language Models: How to (re)warm your model?, by Kshitij Gupta and 7 other authors View PDF Abstract: Large language models (LLMs) are routinely pre-trained on billions of tokens, only to restart the process over again once new data becomes available. A much cheaper and more efficient solution would be to enable the continual pre-training of these models, i.e. updating pre-trained models with new data instead of re-training them from scratch. However, the distribution shift induced by novel data typically results in degraded performance on past data. Taking a step towards efficient continual pre-training, in this work, we examine the effect of different warm-up strategies. Our hypothesis is that the learning rate must be re-increased to improve compute efficiency when training on a new dataset. We study the warmup phase of models pre-trained on the Pile (upstream data, 300B tokens) as we continue to pre-train on SlimPajama (downstream data, 297B tokens), following a linear warmup and cosine decay schedule. We conduct all experiments on the Pythia 410M language model architecture and evaluate performance through validation perplexity. We experiment with different pre-training checkpoints, various maximum learning rates, and various warmup lengths. Our results show that while rewarming models first increases the loss on upstream and downstream data, in the longer run it improves the downstream performance, outperforming models trained from scratch$\\unicode{x2013}$even for a large downstream dataset. Subjects: Computation and Language (cs.CL) ; Machine Learning (cs.LG) Cite as: arXiv:2308.04014 [cs.CL] &nbsp; (or arXiv:2308.04014v2 [cs.CL] for this version) &nbsp; https://doi.org/10.48550/arXiv.2308.04014 Focus to learn more arXiv-issued DOI via DataCite Submission history From: Benjamin Thérien [ view email ] [v1] Tue, 8 Aug 2023 03:18:18 UTC (7,070 KB) [v2] Wed, 6 Sep 2023 23:13:07 UTC (7,070 KB) Full-text links: Access Paper: View a PDF of the paper titled Continual Pre-Training of Large Language Models: How to (re)warm your model?, by Kshitij Gupta and 7 other authors View PDF TeX Source Other Formats view license Current browse context: cs.CL &lt;&nbsp;prev &nbsp; | &nbsp; next&nbsp;&gt; new | recent | 2023-08 Change to browse by: cs cs.LG References &amp; Citations NASA ADS Google Scholar Semantic Scholar a export BibTeX citation Loading... BibTeX formatted citation &times; loading... Data provided by: Bookmark Bibliographic Tools Bibliographic and Citation Tools Bibliographic Explorer Toggle Bibliographic Explorer ( What is the Explorer? ) Connected Papers Toggle Connected Papers ( What is Connected Papers? ) Litmaps Toggle Litmaps ( What is Litmaps? ) scite.ai Toggle scite Smart Citations ( What are Smart Citations? ) Code, Data, Media Code, Data and Media Associated with this Article alphaXiv Toggle alphaXiv ( What is alphaXiv? ) Links to Code Toggle CatalyzeX Code Finder for Papers ( What is CatalyzeX? ) DagsHub Toggle DagsHub ( What is DagsHub? ) GotitPub Toggle Gotit.pub ( What is GotitPub? ) Huggingface Toggle Hugging Face ( What is Huggingface? ) Links to Code Toggle Papers with Code ( What is Papers with Code? ) ScienceCast Toggle ScienceCast ( What is ScienceCast? ) Demos Demos Replicate Toggle Replicate ( What is Replicate? ) Spaces Toggle Hugging Face Spaces ( What is Spaces? ) Spaces Toggle TXYZ.AI ( What is TXYZ.AI? ) Related Papers Recommenders and Search Tools Link to Influence Flower Influence Flower ( What are Influence Flowers? ) Core recommender toggle CORE Recommender ( What is CORE? ) Author Venue Institution Topic About arXivLabs arXivLabs: experimental projects with community collaborators arXivLabs is a framework that allows collaborators to develop and share new arXiv features directly on our website. Both individuals and organizations that work with arXivLabs have embraced and accepted our values of openness, community, excellence, and user data privacy. arXiv is committed to these values and only works with partners that adhere to them. Have an idea for a project that will add value for arXiv's community? Learn more about arXivLabs . Which authors of this paper are endorsers? | Disable MathJax ( What is MathJax? ) About Help contact arXiv Click here to contact arXiv Contact subscribe to arXiv mailings Click here to subscribe Subscribe Copyright Privacy Policy Web Accessibility Assistance arXiv Operational Status Get status notifications via email or slack ",
      "fetch_method": "direct-html"
    },
    {
      "arxiv_id": "https://r.jina.ai/https://arxiv.org/abs/2308.04014](https://arxiv.org/abs/2310.10631",
      "full_text": "Title: [2308.04014](https:/arxiv.org/abs/2310.10631] Article identifier not recognized\n\nURL Source: https://arxiv.org/abs/2308.04014](https:/arxiv.org/abs/2310.10631\n\nWarning: Target URL returned error 404: Not Found\n\nMarkdown Content:\n[2308.04014](https:/arxiv.org/abs/2310.10631] Article identifier not recognized\n\n===============\n\n[Skip to main content](https://arxiv.org/abs/2308.04014](https:/arxiv.org/abs/2310.10631#content)\n\n[![Image 1: Cornell University Logo](https://arxiv.org/static/browse/0.3.4/images/icons/cu/cornell-reduced-white-SMALL.svg)](https://www.cornell.edu/)\n\nWe gratefully acknowledge support from the Simons Foundation, [member institutions](https://info.arxiv.org/about/ourmembers.html), and all contributors.[Donate](https://info.arxiv.org/about/donate.html)\n\n[](https://arxiv.org/IgnoreMe)\n![Image 2: arxiv logo](https://arxiv.org/static/browse/0.3.4/images/arxiv-logo-one-color-white.svg)\n===================================================================================================\n\n[Help](https://info.arxiv.org/help) | [Advanced Search](https://arxiv.org/search/advanced)\n\nSearch\n\n[![Image 3: arXiv logo](https://arxiv.org/static/browse/0.3.4/images/arxiv-logomark-small-white.svg)](https://arxiv.org/)\n\n[![Image 4: Cornell University Logo](https://arxiv.org/static/browse/0.3.4/images/icons/cu/cornell-reduced-white-SMALL.svg)](https://www.cornell.edu/)\n\nGO\n\nquick links\n-----------\n\n*   [Login](https://arxiv.org/login)\n*   [Help Pages](https://info.arxiv.org/help)\n*   [About](https://info.arxiv.org/about)\n\nArticle identifier '2308.04014](https:/arxiv.org/abs/2310.10631' not recognized\n===============================================================================\n\nYou might instead try to [search for articles](https://arxiv.org/search) using title or author information.\n\nFor additional help on arXiv identifiers, see [understanding the arXiv identifier](https://info.arxiv.org/help/arxiv_identifier.html).\n\n*   [About](https://info.arxiv.org/about)\n*   [Help](https://info.arxiv.org/help)\n\n*   [Contact](https://info.arxiv.org/help/contact.html)\n*   [Subscribe](https://info.arxiv.org/help/subscribe)\n\n*   [Copyright](https://info.arxiv.org/help/license/index.html)\n*   [Privacy Policy](https://info.arxiv.org/help/policies/privacy_policy.html)\n\n*   [Web Accessibility Assistance](https://info.arxiv.org/help/web_accessibility.html)\n*   [arXiv Operational Status](https://status.arxiv.org/)\n\n Get status notifications via [email](https://subscribe.sorryapp.com/24846f03/email/new) or [slack](https://subscribe.sorryapp.com/24846f03/slack/new)\n",
      "fetch_method": "jina-reader"
    },
    {
      "arxiv_id": "https://arxiv.org/abs/2401.14489",
      "full_text": " [2401.14489] The Case for Co-Designing Model Architectures with Hardware Skip to main content We gratefully acknowledge support from the Simons Foundation, member institutions , and all contributors. Donate &gt; cs &gt; arXiv:2401.14489 Help | Advanced Search All fields Title Author Abstract Comments Journal reference ACM classification MSC classification Report number arXiv identifier DOI ORCID arXiv author ID Help pages Full text Search open search GO open navigation menu quick links Login Help Pages About --> Computer Science > Distributed, Parallel, and Cluster Computing arXiv:2401.14489 (cs) [Submitted on 25 Jan 2024 ( v1 ), last revised 30 Jan 2024 (this version, v2)] Title: The Case for Co-Designing Model Architectures with Hardware Authors: Quentin Anthony , Jacob Hatef , Deepak Narayanan , Stella Biderman , Stas Bekman , Junqi Yin , Aamir Shafi , Hari Subramoni , Dhabaleswar Panda View a PDF of the paper titled The Case for Co-Designing Model Architectures with Hardware, by Quentin Anthony and 8 other authors View PDF HTML (experimental) Abstract: While GPUs are responsible for training the vast majority of state-of-the-art deep learning models, the implications of their architecture are often overlooked when designing new deep learning (DL) models. As a consequence, modifying a DL model to be more amenable to the target hardware can significantly improve the runtime performance of DL training and inference. In this paper, we provide a set of guidelines for users to maximize the runtime performance of their transformer models. These guidelines have been created by carefully considering the impact of various model hyperparameters controlling model shape on the efficiency of the underlying computation kernels executed on the GPU. We find the throughput of models with efficient model shapes is up to 39\\% higher while preserving accuracy compared to models with a similar number of parameters but with unoptimized shapes. Subjects: Distributed, Parallel, and Cluster Computing (cs.DC) ; Artificial Intelligence (cs.AI) Cite as: arXiv:2401.14489 [cs.DC] &nbsp; (or arXiv:2401.14489v2 [cs.DC] for this version) &nbsp; https://doi.org/10.48550/arXiv.2401.14489 Focus to learn more arXiv-issued DOI via DataCite Submission history From: Quentin Anthony [ view email ] [v1] Thu, 25 Jan 2024 19:50:31 UTC (26,868 KB) [v2] Tue, 30 Jan 2024 21:26:09 UTC (27,008 KB) Full-text links: Access Paper: View a PDF of the paper titled The Case for Co-Designing Model Architectures with Hardware, by Quentin Anthony and 8 other authors View PDF HTML (experimental) TeX Source Other Formats view license Current browse context: cs.DC &lt;&nbsp;prev &nbsp; | &nbsp; next&nbsp;&gt; new | recent | 2024-01 Change to browse by: cs cs.AI References &amp; Citations NASA ADS Google Scholar Semantic Scholar export BibTeX citation Loading... BibTeX formatted citation &times; loading... Data provided by: Bookmark Bibliographic Tools Bibliographic and Citation Tools Bibliographic Explorer Toggle Bibliographic Explorer ( What is the Explorer? ) Connected Papers Toggle Connected Papers ( What is Connected Papers? ) Litmaps Toggle Litmaps ( What is Litmaps? ) scite.ai Toggle scite Smart Citations ( What are Smart Citations? ) Code, Data, Media Code, Data and Media Associated with this Article alphaXiv Toggle alphaXiv ( What is alphaXiv? ) Links to Code Toggle CatalyzeX Code Finder for Papers ( What is CatalyzeX? ) DagsHub Toggle DagsHub ( What is DagsHub? ) GotitPub Toggle Gotit.pub ( What is GotitPub? ) Huggingface Toggle Hugging Face ( What is Huggingface? ) Links to Code Toggle Papers with Code ( What is Papers with Code? ) ScienceCast Toggle ScienceCast ( What is ScienceCast? ) Demos Demos Replicate Toggle Replicate ( What is Replicate? ) Spaces Toggle Hugging Face Spaces ( What is Spaces? ) Spaces Toggle TXYZ.AI ( What is TXYZ.AI? ) Related Papers Recommenders and Search Tools Link to Influence Flower Influence Flower ( What are Influence Flowers? ) Core recommender toggle CORE Recommender ( What is CORE? ) Author Venue Institution Topic About arXivLabs arXivLabs: experimental projects with community collaborators arXivLabs is a framework that allows collaborators to develop and share new arXiv features directly on our website. Both individuals and organizations that work with arXivLabs have embraced and accepted our values of openness, community, excellence, and user data privacy. arXiv is committed to these values and only works with partners that adhere to them. Have an idea for a project that will add value for arXiv's community? Learn more about arXivLabs . Which authors of this paper are endorsers? | Disable MathJax ( What is MathJax? ) About Help contact arXiv Click here to contact arXiv Contact subscribe to arXiv mailings Click here to subscribe Subscribe Copyright Privacy Policy Web Accessibility Assistance arXiv Operational Status Get status notifications via email or slack ",
      "fetch_method": "direct-html"
    },
    {
      "arxiv_id": "https://arxiv.org/abs/2403.08763",
      "full_text": " [2403.08763] Simple and Scalable Strategies to Continually Pre-train Large Language Models Skip to main content We gratefully acknowledge support from the Simons Foundation, member institutions , and all contributors. Donate &gt; cs &gt; arXiv:2403.08763 Help | Advanced Search All fields Title Author Abstract Comments Journal reference ACM classification MSC classification Report number arXiv identifier DOI ORCID arXiv author ID Help pages Full text Search open search GO open navigation menu quick links Login Help Pages About --> Computer Science > Machine Learning arXiv:2403.08763 (cs) [Submitted on 13 Mar 2024 ( v1 ), last revised 4 Sep 2024 (this version, v4)] Title: Simple and Scalable Strategies to Continually Pre-train Large Language Models Authors: Adam Ibrahim , Benjamin Thérien , Kshitij Gupta , Mats L. Richter , Quentin Anthony , Timothée Lesort , Eugene Belilovsky , Irina Rish View a PDF of the paper titled Simple and Scalable Strategies to Continually Pre-train Large Language Models, by Adam Ibrahim and 7 other authors View PDF Abstract: Large language models (LLMs) are routinely pre-trained on billions of tokens, only to start the process over again once new data becomes available. A much more efficient solution is to continually pre-train these models, saving significant compute compared to re-training. However, the distribution shift induced by new data typically results in degraded performance on previous data or poor adaptation to the new data. In this work, we show that a simple and scalable combination of learning rate (LR) re-warming, LR re-decaying, and replay of previous data is sufficient to match the performance of fully re-training from scratch on all available data, as measured by the final loss and the average score on several language model (LM) evaluation benchmarks. Specifically, we show this for a weak but realistic distribution shift between two commonly used LLM pre-training datasets (English$\\rightarrow$English) and a stronger distribution shift (English$\\rightarrow$German) at the $405$M parameter model scale with large dataset sizes (hundreds of billions of tokens). Selecting the weak but realistic shift for larger-scale experiments, we also find that our continual learning strategies match the re-training baseline for a 10B parameter LLM. Our results demonstrate that LLMs can be successfully updated via simple and scalable continual learning strategies, matching the re-training baseline using only a fraction of the compute. Finally, inspired by previous work, we propose alternatives to the cosine learning rate schedule that help circumvent forgetting induced by LR re-warming and that are not bound to a fixed token budget. Subjects: Machine Learning (cs.LG) ; Artificial Intelligence (cs.AI); Computation and Language (cs.CL) Cite as: arXiv:2403.08763 [cs.LG] &nbsp; (or arXiv:2403.08763v4 [cs.LG] for this version) &nbsp; https://doi.org/10.48550/arXiv.2403.08763 Focus to learn more arXiv-issued DOI via DataCite Submission history From: Benjamin Thérien [ view email ] [v1] Wed, 13 Mar 2024 17:58:57 UTC (10,694 KB) [v2] Fri, 22 Mar 2024 17:56:38 UTC (10,732 KB) [v3] Tue, 26 Mar 2024 17:58:48 UTC (10,732 KB) [v4] Wed, 4 Sep 2024 16:13:18 UTC (10,770 KB) Full-text links: Access Paper: View a PDF of the paper titled Simple and Scalable Strategies to Continually Pre-train Large Language Models, by Adam Ibrahim and 7 other authors View PDF TeX Source Other Formats view license Current browse context: cs.LG &lt;&nbsp;prev &nbsp; | &nbsp; next&nbsp;&gt; new | recent | 2024-03 Change to browse by: cs cs.AI cs.CL References &amp; Citations NASA ADS Google Scholar Semantic Scholar a export BibTeX citation Loading... BibTeX formatted citation &times; loading... Data provided by: Bookmark Bibliographic Tools Bibliographic and Citation Tools Bibliographic Explorer Toggle Bibliographic Explorer ( What is the Explorer? ) Connected Papers Toggle Connected Papers ( What is Connected Papers? ) Litmaps Toggle Litmaps ( What is Litmaps? ) scite.ai Toggle scite Smart Citations ( What are Smart Citations? ) Code, Data, Media Code, Data and Media Associated with this Article alphaXiv Toggle alphaXiv ( What is alphaXiv? ) Links to Code Toggle CatalyzeX Code Finder for Papers ( What is CatalyzeX? ) DagsHub Toggle DagsHub ( What is DagsHub? ) GotitPub Toggle Gotit.pub ( What is GotitPub? ) Huggingface Toggle Hugging Face ( What is Huggingface? ) Links to Code Toggle Papers with Code ( What is Papers with Code? ) ScienceCast Toggle ScienceCast ( What is ScienceCast? ) Demos Demos Replicate Toggle Replicate ( What is Replicate? ) Spaces Toggle Hugging Face Spaces ( What is Spaces? ) Spaces Toggle TXYZ.AI ( What is TXYZ.AI? ) Related Papers Recommenders and Search Tools Link to Influence Flower Influence Flower ( What are Influence Flowers? ) Core recommender toggle CORE Recommender ( What is CORE? ) IArxiv recommender toggle IArxiv Recommender ( What is IArxiv? ) Author Venue Institution Topic About arXivLabs arXivLabs: experimental projects with community collaborators arXivLabs is a framework that allows collaborators to develop and share new arXiv features directly on our website. Both individuals and organizations that work with arXivLabs have embraced and accepted our values of openness, community, excellence, and user data privacy. arXiv is committed to these values and only works with partners that adhere to them. Have an idea for a project that will add value for arXiv's community? Learn more about arXivLabs . Which authors of this paper are endorsers? | Disable MathJax ( What is MathJax? ) About Help contact arXiv Click here to contact arXiv Contact subscribe to arXiv mailings Click here to subscribe Subscribe Copyright Privacy Policy Web Accessibility Assistance arXiv Operational Status Get status notifications via email or slack ",
      "fetch_method": "direct-html"
    },
    {
      "arxiv_id": "https://arxiv.org/abs/2402.00691",
      "full_text": " [2402.00691] Comparative Study of Large Language Model Architectures on Frontier Skip to main content We gratefully acknowledge support from the Simons Foundation, member institutions , and all contributors. Donate &gt; cs &gt; arXiv:2402.00691 Help | Advanced Search All fields Title Author Abstract Comments Journal reference ACM classification MSC classification Report number arXiv identifier DOI ORCID arXiv author ID Help pages Full text Search open search GO open navigation menu quick links Login Help Pages About --> Computer Science > Distributed, Parallel, and Cluster Computing arXiv:2402.00691 (cs) [Submitted on 1 Feb 2024] Title: Comparative Study of Large Language Model Architectures on Frontier Authors: Junqi Yin , Avishek Bose , Guojing Cong , Isaac Lyngaas , Quentin Anthony View a PDF of the paper titled Comparative Study of Large Language Model Architectures on Frontier, by Junqi Yin and 4 other authors View PDF Abstract: Large language models (LLMs) have garnered significant attention in both the AI community and beyond. Among these, the Generative Pre-trained Transformer (GPT) has emerged as the dominant architecture, spawning numerous variants. However, these variants have undergone pre-training under diverse conditions, including variations in input data, data preprocessing, and training methodologies, resulting in a lack of controlled comparative studies. Here we meticulously examine two prominent open-sourced GPT architectures, GPT-NeoX and LLaMA, leveraging the computational power of Frontier, the world&#39;s first Exascale supercomputer. Employing the same materials science text corpus and a comprehensive end-to-end pipeline, we conduct a comparative analysis of their training and downstream performance. Our efforts culminate in achieving state-of-the-art performance on a challenging materials science benchmark. Furthermore, we investigate the computation and energy efficiency, and propose a computationally efficient method for architecture design. To our knowledge, these pre-trained models represent the largest available for materials science. Our findings provide practical guidance for building LLMs on HPC platforms. Subjects: Distributed, Parallel, and Cluster Computing (cs.DC) Cite as: arXiv:2402.00691 [cs.DC] &nbsp; (or arXiv:2402.00691v1 [cs.DC] for this version) &nbsp; https://doi.org/10.48550/arXiv.2402.00691 Focus to learn more arXiv-issued DOI via DataCite Submission history From: Junqi Yin [ view email ] [v1] Thu, 1 Feb 2024 15:50:37 UTC (13,693 KB) Full-text links: Access Paper: View a PDF of the paper titled Comparative Study of Large Language Model Architectures on Frontier, by Junqi Yin and 4 other authors View PDF TeX Source Other Formats view license Current browse context: cs.DC &lt;&nbsp;prev &nbsp; | &nbsp; next&nbsp;&gt; new | recent | 2024-02 Change to browse by: cs References &amp; Citations NASA ADS Google Scholar Semantic Scholar a export BibTeX citation Loading... BibTeX formatted citation &times; loading... Data provided by: Bookmark Bibliographic Tools Bibliographic and Citation Tools Bibliographic Explorer Toggle Bibliographic Explorer ( What is the Explorer? ) Connected Papers Toggle Connected Papers ( What is Connected Papers? ) Litmaps Toggle Litmaps ( What is Litmaps? ) scite.ai Toggle scite Smart Citations ( What are Smart Citations? ) Code, Data, Media Code, Data and Media Associated with this Article alphaXiv Toggle alphaXiv ( What is alphaXiv? ) Links to Code Toggle CatalyzeX Code Finder for Papers ( What is CatalyzeX? ) DagsHub Toggle DagsHub ( What is DagsHub? ) GotitPub Toggle Gotit.pub ( What is GotitPub? ) Huggingface Toggle Hugging Face ( What is Huggingface? ) Links to Code Toggle Papers with Code ( What is Papers with Code? ) ScienceCast Toggle ScienceCast ( What is ScienceCast? ) Demos Demos Replicate Toggle Replicate ( What is Replicate? ) Spaces Toggle Hugging Face Spaces ( What is Spaces? ) Spaces Toggle TXYZ.AI ( What is TXYZ.AI? ) Related Papers Recommenders and Search Tools Link to Influence Flower Influence Flower ( What are Influence Flowers? ) Core recommender toggle CORE Recommender ( What is CORE? ) Author Venue Institution Topic About arXivLabs arXivLabs: experimental projects with community collaborators arXivLabs is a framework that allows collaborators to develop and share new arXiv features directly on our website. Both individuals and organizations that work with arXivLabs have embraced and accepted our values of openness, community, excellence, and user data privacy. arXiv is committed to these values and only works with partners that adhere to them. Have an idea for a project that will add value for arXiv's community? Learn more about arXivLabs . Which authors of this paper are endorsers? | Disable MathJax ( What is MathJax? ) About Help contact arXiv Click here to contact arXiv Contact subscribe to arXiv mailings Click here to subscribe Subscribe Copyright Privacy Policy Web Accessibility Assistance arXiv Operational Status Get status notifications via email or slack ",
      "fetch_method": "direct-html"
    },
    {
      "arxiv_id": "https://arxiv.org/abs/2205.09921",
      "full_text": " [2205.09921] KERPLE: Kernelized Relative Positional Embedding for Length Extrapolation Skip to main content We gratefully acknowledge support from the Simons Foundation, member institutions , and all contributors. Donate &gt; cs &gt; arXiv:2205.09921 Help | Advanced Search All fields Title Author Abstract Comments Journal reference ACM classification MSC classification Report number arXiv identifier DOI ORCID arXiv author ID Help pages Full text Search open search GO open navigation menu quick links Login Help Pages About --> Computer Science > Computation and Language arXiv:2205.09921 (cs) [Submitted on 20 May 2022 ( v1 ), last revised 13 Oct 2022 (this version, v2)] Title: KERPLE: Kernelized Relative Positional Embedding for Length Extrapolation Authors: Ta-Chung Chi , Ting-Han Fan , Peter J. Ramadge , Alexander I. Rudnicky View a PDF of the paper titled KERPLE: Kernelized Relative Positional Embedding for Length Extrapolation, by Ta-Chung Chi and 3 other authors View PDF Abstract: Relative positional embeddings (RPE) have received considerable attention since RPEs effectively model the relative distance among tokens and enable length extrapolation. We propose KERPLE, a framework that generalizes relative position embedding for extrapolation by kernelizing positional differences. We achieve this goal using conditionally positive definite (CPD) kernels, a class of functions known for generalizing distance metrics. To maintain the inner product interpretation of self-attention, we show that a CPD kernel can be transformed into a PD kernel by adding a constant offset. This offset is implicitly absorbed in the Softmax normalization during self-attention. The diversity of CPD kernels allows us to derive various RPEs that enable length extrapolation in a principled way. Experiments demonstrate that the logarithmic variant achieves excellent extrapolation performance on three large language modeling datasets. Our implementation and pretrained checkpoints are released at this https URL . Comments: Accepted at the 36th Conference on Neural Information Processing Systems (NeurIPS 2022). The first two authors contributed equally to this work Subjects: Computation and Language (cs.CL) ; Machine Learning (cs.LG) Cite as: arXiv:2205.09921 [cs.CL] &nbsp; (or arXiv:2205.09921v2 [cs.CL] for this version) &nbsp; https://doi.org/10.48550/arXiv.2205.09921 Focus to learn more arXiv-issued DOI via DataCite Submission history From: Ting-Han Fan [ view email ] [v1] Fri, 20 May 2022 01:25:57 UTC (92 KB) [v2] Thu, 13 Oct 2022 15:16:58 UTC (288 KB) Full-text links: Access Paper: View a PDF of the paper titled KERPLE: Kernelized Relative Positional Embedding for Length Extrapolation, by Ta-Chung Chi and 3 other authors View PDF TeX Source Other Formats view license Current browse context: cs.CL &lt;&nbsp;prev &nbsp; | &nbsp; next&nbsp;&gt; new | recent | 2022-05 Change to browse by: cs cs.LG References &amp; Citations NASA ADS Google Scholar Semantic Scholar export BibTeX citation Loading... BibTeX formatted citation &times; loading... Data provided by: Bookmark Bibliographic Tools Bibliographic and Citation Tools Bibliographic Explorer Toggle Bibliographic Explorer ( What is the Explorer? ) Connected Papers Toggle Connected Papers ( What is Connected Papers? ) Litmaps Toggle Litmaps ( What is Litmaps? ) scite.ai Toggle scite Smart Citations ( What are Smart Citations? ) Code, Data, Media Code, Data and Media Associated with this Article alphaXiv Toggle alphaXiv ( What is alphaXiv? ) Links to Code Toggle CatalyzeX Code Finder for Papers ( What is CatalyzeX? ) DagsHub Toggle DagsHub ( What is DagsHub? ) GotitPub Toggle Gotit.pub ( What is GotitPub? ) Huggingface Toggle Hugging Face ( What is Huggingface? ) Links to Code Toggle Papers with Code ( What is Papers with Code? ) ScienceCast Toggle ScienceCast ( What is ScienceCast? ) Demos Demos Replicate Toggle Replicate ( What is Replicate? ) Spaces Toggle Hugging Face Spaces ( What is Spaces? ) Spaces Toggle TXYZ.AI ( What is TXYZ.AI? ) Related Papers Recommenders and Search Tools Link to Influence Flower Influence Flower ( What are Influence Flowers? ) Core recommender toggle CORE Recommender ( What is CORE? ) Author Venue Institution Topic About arXivLabs arXivLabs: experimental projects with community collaborators arXivLabs is a framework that allows collaborators to develop and share new arXiv features directly on our website. Both individuals and organizations that work with arXivLabs have embraced and accepted our values of openness, community, excellence, and user data privacy. arXiv is committed to these values and only works with partners that adhere to them. Have an idea for a project that will add value for arXiv's community? Learn more about arXivLabs . Which authors of this paper are endorsers? | Disable MathJax ( What is MathJax? ) About Help contact arXiv Click here to contact arXiv Contact subscribe to arXiv mailings Click here to subscribe Subscribe Copyright Privacy Policy Web Accessibility Assistance arXiv Operational Status Get status notifications via email or slack ",
      "fetch_method": "direct-html"
    },
    {
      "arxiv_id": "https://arxiv.org/abs/2202.13169",
      "full_text": " [2202.13169] A Systematic Evaluation of Large Language Models of Code Skip to main content We gratefully acknowledge support from the Simons Foundation, member institutions , and all contributors. Donate &gt; cs &gt; arXiv:2202.13169 Help | Advanced Search All fields Title Author Abstract Comments Journal reference ACM classification MSC classification Report number arXiv identifier DOI ORCID arXiv author ID Help pages Full text Search open search GO open navigation menu quick links Login Help Pages About --> Computer Science > Programming Languages arXiv:2202.13169 (cs) [Submitted on 26 Feb 2022 ( v1 ), last revised 4 May 2022 (this version, v3)] Title: A Systematic Evaluation of Large Language Models of Code Authors: Frank F. Xu , Uri Alon , Graham Neubig , Vincent J. Hellendoorn View a PDF of the paper titled A Systematic Evaluation of Large Language Models of Code, by Frank F. Xu and 3 other authors View PDF Abstract: Large language models (LMs) of code have recently shown tremendous promise in completing code and synthesizing code from natural language descriptions. However, the current state-of-the-art code LMs (e.g., Codex (Chen et al., 2021)) are not publicly available, leaving many questions about their model and data design decisions. We aim to fill in some of these blanks through a systematic evaluation of the largest existing models: Codex, GPT-J, GPT-Neo, GPT-NeoX-20B, and CodeParrot, across various programming languages. Although Codex itself is not open-source, we find that existing open-source models do achieve close results in some programming languages, although targeted mainly for natural language modeling. We further identify an important missing piece in the form of a large open-source model trained exclusively on a multi-lingual corpus of code. We release a new model, PolyCoder, with 2.7B parameters based on the GPT-2 architecture, which was trained on 249GB of code across 12 programming languages on a single machine. In the C programming language, PolyCoder outperforms all models including Codex. Our trained models are open-source and publicly available at this https URL , which enables future research and application in this area. Comments: DL4C@ICLR 2022, and MAPS@PLDI 2022 Subjects: Programming Languages (cs.PL) ; Computation and Language (cs.CL) Cite as: arXiv:2202.13169 [cs.PL] &nbsp; (or arXiv:2202.13169v3 [cs.PL] for this version) &nbsp; https://doi.org/10.48550/arXiv.2202.13169 Focus to learn more arXiv-issued DOI via DataCite Submission history From: Frank F. Xu [ view email ] [v1] Sat, 26 Feb 2022 15:53:55 UTC (275 KB) [v2] Tue, 1 Mar 2022 19:13:06 UTC (275 KB) [v3] Wed, 4 May 2022 16:08:31 UTC (275 KB) Full-text links: Access Paper: View a PDF of the paper titled A Systematic Evaluation of Large Language Models of Code, by Frank F. Xu and 3 other authors View PDF TeX Source Other Formats view license Current browse context: cs.PL &lt;&nbsp;prev &nbsp; | &nbsp; next&nbsp;&gt; new | recent | 2022-02 Change to browse by: cs cs.CL References &amp; Citations NASA ADS Google Scholar Semantic Scholar a export BibTeX citation Loading... BibTeX formatted citation &times; loading... Data provided by: Bookmark Bibliographic Tools Bibliographic and Citation Tools Bibliographic Explorer Toggle Bibliographic Explorer ( What is the Explorer? ) Connected Papers Toggle Connected Papers ( What is Connected Papers? ) Litmaps Toggle Litmaps ( What is Litmaps? ) scite.ai Toggle scite Smart Citations ( What are Smart Citations? ) Code, Data, Media Code, Data and Media Associated with this Article alphaXiv Toggle alphaXiv ( What is alphaXiv? ) Links to Code Toggle CatalyzeX Code Finder for Papers ( What is CatalyzeX? ) DagsHub Toggle DagsHub ( What is DagsHub? ) GotitPub Toggle Gotit.pub ( What is GotitPub? ) Huggingface Toggle Hugging Face ( What is Huggingface? ) Links to Code Toggle Papers with Code ( What is Papers with Code? ) ScienceCast Toggle ScienceCast ( What is ScienceCast? ) Demos Demos Replicate Toggle Replicate ( What is Replicate? ) Spaces Toggle Hugging Face Spaces ( What is Spaces? ) Spaces Toggle TXYZ.AI ( What is TXYZ.AI? ) Related Papers Recommenders and Search Tools Link to Influence Flower Influence Flower ( What are Influence Flowers? ) Core recommender toggle CORE Recommender ( What is CORE? ) Author Venue Institution Topic About arXivLabs arXivLabs: experimental projects with community collaborators arXivLabs is a framework that allows collaborators to develop and share new arXiv features directly on our website. Both individuals and organizations that work with arXivLabs have embraced and accepted our values of openness, community, excellence, and user data privacy. arXiv is committed to these values and only works with partners that adhere to them. Have an idea for a project that will add value for arXiv's community? Learn more about arXivLabs . Which authors of this paper are endorsers? | Disable MathJax ( What is MathJax? ) About Help contact arXiv Click here to contact arXiv Contact subscribe to arXiv mailings Click here to subscribe Subscribe Copyright Privacy Policy Web Accessibility Assistance arXiv Operational Status Get status notifications via email or slack ",
      "fetch_method": "direct-html"
    },
    {
      "arxiv_id": "https://arxiv.org/abs/2304.11389",
      "full_text": " [2304.11389] Transformer-Based Language Model Surprisal Predicts Human Reading Times Best with About Two Billion Training Tokens Skip to main content We gratefully acknowledge support from the Simons Foundation, member institutions , and all contributors. Donate &gt; cs &gt; arXiv:2304.11389 Help | Advanced Search All fields Title Author Abstract Comments Journal reference ACM classification MSC classification Report number arXiv identifier DOI ORCID arXiv author ID Help pages Full text Search open search GO open navigation menu quick links Login Help Pages About --> Computer Science > Computation and Language arXiv:2304.11389 (cs) [Submitted on 22 Apr 2023 ( v1 ), last revised 22 Oct 2023 (this version, v2)] Title: Transformer-Based Language Model Surprisal Predicts Human Reading Times Best with About Two Billion Training Tokens Authors: Byung-Doh Oh , William Schuler View a PDF of the paper titled Transformer-Based Language Model Surprisal Predicts Human Reading Times Best with About Two Billion Training Tokens, by Byung-Doh Oh and 1 other authors View PDF Abstract: Recent psycholinguistic studies have drawn conflicting conclusions about the relationship between the quality of a language model and the ability of its surprisal estimates to predict human reading times, which has been speculated to be due to the large gap in both the amount of training data and model capacity across studies. The current work aims to consolidate these findings by evaluating surprisal estimates from Transformer-based language model variants that vary systematically in the amount of training data and model capacity on their ability to predict human reading times. The results show that surprisal estimates from most variants with contemporary model capacities provide the best fit after seeing about two billion training tokens, after which they begin to diverge from humanlike expectations. Additionally, newly-trained smaller model variants reveal a &#39;tipping point&#39; at convergence, after which the decrease in language model perplexity begins to result in poorer fits to human reading times. These results suggest that the massive amount of training data is mainly responsible for the poorer fit achieved by surprisal from larger pre-trained language models, and that a certain degree of model capacity is necessary for Transformer-based language models to capture humanlike expectations. Comments: Findings of the Association for Computational Linguistics: EMNLP 2023 Subjects: Computation and Language (cs.CL) Cite as: arXiv:2304.11389 [cs.CL] &nbsp; (or arXiv:2304.11389v2 [cs.CL] for this version) &nbsp; https://doi.org/10.48550/arXiv.2304.11389 Focus to learn more arXiv-issued DOI via DataCite Submission history From: Byung-Doh Oh [ view email ] [v1] Sat, 22 Apr 2023 12:50:49 UTC (1,585 KB) [v2] Sun, 22 Oct 2023 20:03:54 UTC (2,199 KB) Full-text links: Access Paper: View a PDF of the paper titled Transformer-Based Language Model Surprisal Predicts Human Reading Times Best with About Two Billion Training Tokens, by Byung-Doh Oh and 1 other authors View PDF TeX Source Other Formats view license Current browse context: cs.CL &lt;&nbsp;prev &nbsp; | &nbsp; next&nbsp;&gt; new | recent | 2023-04 Change to browse by: cs References &amp; Citations NASA ADS Google Scholar Semantic Scholar export BibTeX citation Loading... BibTeX formatted citation &times; loading... Data provided by: Bookmark Bibliographic Tools Bibliographic and Citation Tools Bibliographic Explorer Toggle Bibliographic Explorer ( What is the Explorer? ) Connected Papers Toggle Connected Papers ( What is Connected Papers? ) Litmaps Toggle Litmaps ( What is Litmaps? ) scite.ai Toggle scite Smart Citations ( What are Smart Citations? ) Code, Data, Media Code, Data and Media Associated with this Article alphaXiv Toggle alphaXiv ( What is alphaXiv? ) Links to Code Toggle CatalyzeX Code Finder for Papers ( What is CatalyzeX? ) DagsHub Toggle DagsHub ( What is DagsHub? ) GotitPub Toggle Gotit.pub ( What is GotitPub? ) Huggingface Toggle Hugging Face ( What is Huggingface? ) Links to Code Toggle Papers with Code ( What is Papers with Code? ) ScienceCast Toggle ScienceCast ( What is ScienceCast? ) Demos Demos Replicate Toggle Replicate ( What is Replicate? ) Spaces Toggle Hugging Face Spaces ( What is Spaces? ) Spaces Toggle TXYZ.AI ( What is TXYZ.AI? ) Related Papers Recommenders and Search Tools Link to Influence Flower Influence Flower ( What are Influence Flowers? ) Core recommender toggle CORE Recommender ( What is CORE? ) Author Venue Institution Topic About arXivLabs arXivLabs: experimental projects with community collaborators arXivLabs is a framework that allows collaborators to develop and share new arXiv features directly on our website. Both individuals and organizations that work with arXivLabs have embraced and accepted our values of openness, community, excellence, and user data privacy. arXiv is committed to these values and only works with partners that adhere to them. Have an idea for a project that will add value for arXiv's community? Learn more about arXivLabs . Which authors of this paper are endorsers? | Disable MathJax ( What is MathJax? ) About Help contact arXiv Click here to contact arXiv Contact subscribe to arXiv mailings Click here to subscribe Subscribe Copyright Privacy Policy Web Accessibility Assistance arXiv Operational Status Get status notifications via email or slack ",
      "fetch_method": "direct-html"
    },
    {
      "arxiv_id": "https://arxiv.org/abs/2306.09200",
      "full_text": " [2306.09200] ChessGPT: Bridging Policy Learning and Language Modeling Skip to main content We gratefully acknowledge support from the Simons Foundation, member institutions , and all contributors. Donate &gt; cs &gt; arXiv:2306.09200 Help | Advanced Search All fields Title Author Abstract Comments Journal reference ACM classification MSC classification Report number arXiv identifier DOI ORCID arXiv author ID Help pages Full text Search open search GO open navigation menu quick links Login Help Pages About --> Computer Science > Machine Learning arXiv:2306.09200 (cs) [Submitted on 15 Jun 2023 ( v1 ), last revised 21 Dec 2023 (this version, v2)] Title: ChessGPT: Bridging Policy Learning and Language Modeling Authors: Xidong Feng , Yicheng Luo , Ziyan Wang , Hongrui Tang , Mengyue Yang , Kun Shao , David Mguni , Yali Du , Jun Wang View a PDF of the paper titled ChessGPT: Bridging Policy Learning and Language Modeling, by Xidong Feng and 8 other authors View PDF HTML (experimental) Abstract: When solving decision-making tasks, humans typically depend on information from two key sources: (1) Historical policy data, which provides interaction replay from the environment, and (2) Analytical insights in natural language form, exposing the invaluable thought process or strategic considerations. Despite this, the majority of preceding research focuses on only one source: they either use historical replay exclusively to directly learn policy or value functions, or engaged in language model training utilizing mere language corpus. In this paper, we argue that a powerful autonomous agent should cover both sources. Thus, we propose ChessGPT, a GPT model bridging policy learning and language modeling by integrating data from these two sources in Chess games. Specifically, we build a large-scale game and language dataset related to chess. Leveraging the dataset, we showcase two model examples ChessCLIP and ChessGPT, integrating policy learning and language modeling. Finally, we propose a full evaluation framework for evaluating language model&#39;s chess ability. Experimental results validate our model and dataset&#39;s effectiveness. We open source our code, model, and dataset at this https URL . Comments: Published as a conference article in NeurIPS 2023 Subjects: Machine Learning (cs.LG) ; Artificial Intelligence (cs.AI) Cite as: arXiv:2306.09200 [cs.LG] &nbsp; (or arXiv:2306.09200v2 [cs.LG] for this version) &nbsp; https://doi.org/10.48550/arXiv.2306.09200 Focus to learn more arXiv-issued DOI via DataCite Submission history From: Yicheng Luo [ view email ] [v1] Thu, 15 Jun 2023 15:35:31 UTC (189 KB) [v2] Thu, 21 Dec 2023 16:59:44 UTC (1,746 KB) Full-text links: Access Paper: View a PDF of the paper titled ChessGPT: Bridging Policy Learning and Language Modeling, by Xidong Feng and 8 other authors View PDF HTML (experimental) TeX Source Other Formats view license Current browse context: cs.LG &lt;&nbsp;prev &nbsp; | &nbsp; next&nbsp;&gt; new | recent | 2023-06 Change to browse by: cs cs.AI References &amp; Citations NASA ADS Google Scholar Semantic Scholar a export BibTeX citation Loading... BibTeX formatted citation &times; loading... Data provided by: Bookmark Bibliographic Tools Bibliographic and Citation Tools Bibliographic Explorer Toggle Bibliographic Explorer ( What is the Explorer? ) Connected Papers Toggle Connected Papers ( What is Connected Papers? ) Litmaps Toggle Litmaps ( What is Litmaps? ) scite.ai Toggle scite Smart Citations ( What are Smart Citations? ) Code, Data, Media Code, Data and Media Associated with this Article alphaXiv Toggle alphaXiv ( What is alphaXiv? ) Links to Code Toggle CatalyzeX Code Finder for Papers ( What is CatalyzeX? ) DagsHub Toggle DagsHub ( What is DagsHub? ) GotitPub Toggle Gotit.pub ( What is GotitPub? ) Huggingface Toggle Hugging Face ( What is Huggingface? ) Links to Code Toggle Papers with Code ( What is Papers with Code? ) ScienceCast Toggle ScienceCast ( What is ScienceCast? ) Demos Demos Replicate Toggle Replicate ( What is Replicate? ) Spaces Toggle Hugging Face Spaces ( What is Spaces? ) Spaces Toggle TXYZ.AI ( What is TXYZ.AI? ) Related Papers Recommenders and Search Tools Link to Influence Flower Influence Flower ( What are Influence Flowers? ) Core recommender toggle CORE Recommender ( What is CORE? ) IArxiv recommender toggle IArxiv Recommender ( What is IArxiv? ) Author Venue Institution Topic About arXivLabs arXivLabs: experimental projects with community collaborators arXivLabs is a framework that allows collaborators to develop and share new arXiv features directly on our website. Both individuals and organizations that work with arXivLabs have embraced and accepted our values of openness, community, excellence, and user data privacy. arXiv is committed to these values and only works with partners that adhere to them. Have an idea for a project that will add value for arXiv's community? Learn more about arXivLabs . Which authors of this paper are endorsers? | Disable MathJax ( What is MathJax? ) About Help contact arXiv Click here to contact arXiv Contact subscribe to arXiv mailings Click here to subscribe Subscribe Copyright Privacy Policy Web Accessibility Assistance arXiv Operational Status Get status notifications via email or slack ",
      "fetch_method": "direct-html"
    },
    {
      "arxiv_id": "https://arxiv.org/abs/2310.01119",
      "full_text": " [2310.01119] Synthetic Data Generation in Low-Resource Settings via Fine-Tuning of Large Language Models Skip to main content We gratefully acknowledge support from the Simons Foundation, member institutions , and all contributors. Donate &gt; cs &gt; arXiv:2310.01119 Help | Advanced Search All fields Title Author Abstract Comments Journal reference ACM classification MSC classification Report number arXiv identifier DOI ORCID arXiv author ID Help pages Full text Search open search GO open navigation menu quick links Login Help Pages About --> Computer Science > Computation and Language arXiv:2310.01119 (cs) [Submitted on 2 Oct 2023 ( v1 ), last revised 8 Jan 2024 (this version, v2)] Title: Synthetic Data Generation in Low-Resource Settings via Fine-Tuning of Large Language Models Authors: Jean Kaddour , Qi Liu View a PDF of the paper titled Synthetic Data Generation in Low-Resource Settings via Fine-Tuning of Large Language Models, by Jean Kaddour and 1 other authors View PDF Abstract: The in-context learning ability of large language models (LLMs) enables them to generalize to novel downstream tasks with relatively few labeled examples. However, they require enormous computational resources to be deployed. Alternatively, smaller models can solve specific tasks if fine-tuned with enough labeled examples. These examples, however, are expensive to obtain. In pursuit of the best of both worlds, we study synthetic data generation of fine-tuning training data via fine-tuned teacher LLMs to improve the downstream performance of much smaller models. In four text classification and two text generation tasks, we find that both data generation and annotation dramatically improve the respective downstream model&#39;s performance, occasionally necessitating only a minor fraction of the original training dataset. Subjects: Computation and Language (cs.CL) ; Machine Learning (cs.LG) Cite as: arXiv:2310.01119 [cs.CL] &nbsp; (or arXiv:2310.01119v2 [cs.CL] for this version) &nbsp; https://doi.org/10.48550/arXiv.2310.01119 Focus to learn more arXiv-issued DOI via DataCite Submission history From: Jean Kaddour [ view email ] [v1] Mon, 2 Oct 2023 11:49:05 UTC (57 KB) [v2] Mon, 8 Jan 2024 13:09:24 UTC (98 KB) Full-text links: Access Paper: View a PDF of the paper titled Synthetic Data Generation in Low-Resource Settings via Fine-Tuning of Large Language Models, by Jean Kaddour and 1 other authors View PDF TeX Source Other Formats view license Current browse context: cs.CL &lt;&nbsp;prev &nbsp; | &nbsp; next&nbsp;&gt; new | recent | 2023-10 Change to browse by: cs cs.LG References &amp; Citations NASA ADS Google Scholar Semantic Scholar export BibTeX citation Loading... BibTeX formatted citation &times; loading... Data provided by: Bookmark Bibliographic Tools Bibliographic and Citation Tools Bibliographic Explorer Toggle Bibliographic Explorer ( What is the Explorer? ) Connected Papers Toggle Connected Papers ( What is Connected Papers? ) Litmaps Toggle Litmaps ( What is Litmaps? ) scite.ai Toggle scite Smart Citations ( What are Smart Citations? ) Code, Data, Media Code, Data and Media Associated with this Article alphaXiv Toggle alphaXiv ( What is alphaXiv? ) Links to Code Toggle CatalyzeX Code Finder for Papers ( What is CatalyzeX? ) DagsHub Toggle DagsHub ( What is DagsHub? ) GotitPub Toggle Gotit.pub ( What is GotitPub? ) Huggingface Toggle Hugging Face ( What is Huggingface? ) Links to Code Toggle Papers with Code ( What is Papers with Code? ) ScienceCast Toggle ScienceCast ( What is ScienceCast? ) Demos Demos Replicate Toggle Replicate ( What is Replicate? ) Spaces Toggle Hugging Face Spaces ( What is Spaces? ) Spaces Toggle TXYZ.AI ( What is TXYZ.AI? ) Related Papers Recommenders and Search Tools Link to Influence Flower Influence Flower ( What are Influence Flowers? ) Core recommender toggle CORE Recommender ( What is CORE? ) Author Venue Institution Topic About arXivLabs arXivLabs: experimental projects with community collaborators arXivLabs is a framework that allows collaborators to develop and share new arXiv features directly on our website. Both individuals and organizations that work with arXivLabs have embraced and accepted our values of openness, community, excellence, and user data privacy. arXiv is committed to these values and only works with partners that adhere to them. Have an idea for a project that will add value for arXiv's community? Learn more about arXivLabs . Which authors of this paper are endorsers? | Disable MathJax ( What is MathJax? ) About Help contact arXiv Click here to contact arXiv Contact subscribe to arXiv mailings Click here to subscribe Subscribe Copyright Privacy Policy Web Accessibility Assistance arXiv Operational Status Get status notifications via email or slack ",
      "fetch_method": "direct-html"
    },
    {
      "arxiv_id": "https://arxiv.org/abs/2312.02406",
      "full_text": " [2312.02406] Efficient Online Data Mixing For Language Model Pre-Training arXiv Is Hiring a DevOps Engineer Work on one of the world's most important websites and make an impact on open science. View Jobs Skip to main content arXiv Is Hiring a DevOps Engineer View Jobs We gratefully acknowledge support from the Simons Foundation, member institutions , and all contributors. Donate &gt; cs &gt; arXiv:2312.02406 Help | Advanced Search All fields Title Author Abstract Comments Journal reference ACM classification MSC classification Report number arXiv identifier DOI ORCID arXiv author ID Help pages Full text Search open search GO open navigation menu quick links Login Help Pages About --> Computer Science > Computation and Language arXiv:2312.02406 (cs) [Submitted on 5 Dec 2023 ( v1 ), last revised 9 Dec 2023 (this version, v2)] Title: Efficient Online Data Mixing For Language Model Pre-Training Authors: Alon Albalak , Liangming Pan , Colin Raffel , William Yang Wang View a PDF of the paper titled Efficient Online Data Mixing For Language Model Pre-Training, by Alon Albalak and Liangming Pan and Colin Raffel and William Yang Wang View PDF HTML (experimental) Abstract: The data used to pretrain large language models has a decisive impact on a model&#39;s downstream performance, which has led to a large body of work on data selection methods that aim to automatically determine the most suitable data to use for pretraining. Existing data selection methods suffer from slow and computationally expensive processes, a problem amplified by the increasing size of models and of pretraining datasets. Data mixing, on the other hand, reduces the complexity of data selection by grouping data points together and determining sampling probabilities across entire groups. However, data mixing proportions are typically fixed before training and therefore cannot adapt to changing training dynamics. To address these limitations, we develop an efficient algorithm for Online Data Mixing (ODM) that combines elements from both data selection and data mixing. Based on multi-armed bandit algorithms, our online approach optimizes the data mixing proportions during training. Remarkably, our method trains a model that reaches the final perplexity of the next best method with 19\\% fewer training iterations, and improves performance on the 5-shot MMLU benchmark by 1.9% relative accuracy, while adding negligible wall-clock time during pretraining. Subjects: Computation and Language (cs.CL) ; Machine Learning (cs.LG) Cite as: arXiv:2312.02406 [cs.CL] &nbsp; (or arXiv:2312.02406v2 [cs.CL] for this version) &nbsp; https://doi.org/10.48550/arXiv.2312.02406 Focus to learn more arXiv-issued DOI via DataCite Submission history From: Alon Albalak [ view email ] [v1] Tue, 5 Dec 2023 00:42:35 UTC (175 KB) [v2] Sat, 9 Dec 2023 00:27:18 UTC (175 KB) Full-text links: Access Paper: View a PDF of the paper titled Efficient Online Data Mixing For Language Model Pre-Training, by Alon Albalak and Liangming Pan and Colin Raffel and William Yang Wang View PDF HTML (experimental) TeX Source Other Formats view license Current browse context: cs.CL &lt;&nbsp;prev &nbsp; | &nbsp; next&nbsp;&gt; new | recent | 2023-12 Change to browse by: cs cs.LG References &amp; Citations NASA ADS Google Scholar Semantic Scholar a export BibTeX citation Loading... BibTeX formatted citation &times; loading... Data provided by: Bookmark Bibliographic Tools Bibliographic and Citation Tools Bibliographic Explorer Toggle Bibliographic Explorer ( What is the Explorer? ) Connected Papers Toggle Connected Papers ( What is Connected Papers? ) Litmaps Toggle Litmaps ( What is Litmaps? ) scite.ai Toggle scite Smart Citations ( What are Smart Citations? ) Code, Data, Media Code, Data and Media Associated with this Article alphaXiv Toggle alphaXiv ( What is alphaXiv? ) Links to Code Toggle CatalyzeX Code Finder for Papers ( What is CatalyzeX? ) DagsHub Toggle DagsHub ( What is DagsHub? ) GotitPub Toggle Gotit.pub ( What is GotitPub? ) Huggingface Toggle Hugging Face ( What is Huggingface? ) Links to Code Toggle Papers with Code ( What is Papers with Code? ) ScienceCast Toggle ScienceCast ( What is ScienceCast? ) Demos Demos Replicate Toggle Replicate ( What is Replicate? ) Spaces Toggle Hugging Face Spaces ( What is Spaces? ) Spaces Toggle TXYZ.AI ( What is TXYZ.AI? ) Related Papers Recommenders and Search Tools Link to Influence Flower Influence Flower ( What are Influence Flowers? ) Core recommender toggle CORE Recommender ( What is CORE? ) Author Venue Institution Topic About arXivLabs arXivLabs: experimental projects with community collaborators arXivLabs is a framework that allows collaborators to develop and share new arXiv features directly on our website. Both individuals and organizations that work with arXivLabs have embraced and accepted our values of openness, community, excellence, and user data privacy. arXiv is committed to these values and only works with partners that adhere to them. Have an idea for a project that will add value for arXiv's community? Learn more about arXivLabs . Which authors of this paper are endorsers? | Disable MathJax ( What is MathJax? ) About Help contact arXiv Click here to contact arXiv Contact subscribe to arXiv mailings Click here to subscribe Subscribe Copyright Privacy Policy Web Accessibility Assistance arXiv Operational Status Get status notifications via email or slack ",
      "fetch_method": "direct-html"
    },
    {
      "arxiv_id": "https://arxiv.org/abs/2310.06266",
      "full_text": " [2310.06266] CodeFuse-13B: A Pretrained Multi-lingual Code Large Language Model arXiv Is Hiring a DevOps Engineer Work on one of the world's most important websites and make an impact on open science. View Jobs Skip to main content arXiv Is Hiring a DevOps Engineer View Jobs We gratefully acknowledge support from the Simons Foundation, member institutions , and all contributors. Donate &gt; cs &gt; arXiv:2310.06266 Help | Advanced Search All fields Title Author Abstract Comments Journal reference ACM classification MSC classification Report number arXiv identifier DOI ORCID arXiv author ID Help pages Full text Search open search GO open navigation menu quick links Login Help Pages About --> Computer Science > Software Engineering arXiv:2310.06266 (cs) [Submitted on 10 Oct 2023 ( v1 ), last revised 10 Jan 2024 (this version, v2)] Title: CodeFuse-13B: A Pretrained Multi-lingual Code Large Language Model Authors: Peng Di , Jianguo Li , Hang Yu , Wei Jiang , Wenting Cai , Yang Cao , Chaoyu Chen , Dajun Chen , Hongwei Chen , Liang Chen , Gang Fan , Jie Gong , Zi Gong , Wen Hu , Tingting Guo , Zhichao Lei , Ting Li , Zheng Li , Ming Liang , Cong Liao , Bingchang Liu , Jiachen Liu , Zhiwei Liu , Shaojun Lu , Min Shen , Guangpei Wang , Huan Wang , Zhi Wang , Zhaogui Xu , Jiawei Yang , Qing Ye , Gehao Zhang , Yu Zhang , Zelin Zhao , Xunjin Zheng , Hailian Zhou , Lifu Zhu , Xianying Zhu View a PDF of the paper titled CodeFuse-13B: A Pretrained Multi-lingual Code Large Language Model, by Peng Di and 37 other authors View PDF HTML (experimental) Abstract: Code Large Language Models (Code LLMs) have gained significant attention in the industry due to their wide applications in the full lifecycle of software engineering. However, the effectiveness of existing models in understanding non-English inputs for multi-lingual code-related tasks is still far from well studied. This paper introduces CodeFuse-13B, an open-sourced pre-trained code LLM. It is specifically designed for code-related tasks with both English and Chinese prompts and supports over 40 programming languages. CodeFuse achieves its effectiveness by utilizing a high quality pre-training dataset that is carefully filtered by program analyzers and optimized during the training process. Extensive experiments are conducted using real-world usage scenarios, the industry-standard benchmark HumanEval-x, and the specially designed CodeFuseEval for Chinese prompts. To assess the effectiveness of CodeFuse, we actively collected valuable human feedback from the AntGroup&#39;s software development process where CodeFuse has been successfully deployed. The results demonstrate that CodeFuse-13B achieves a HumanEval pass@1 score of 37.10%, positioning it as one of the top multi-lingual code LLMs with similar parameter sizes. In practical scenarios, such as code generation, code translation, code comments, and testcase generation, CodeFuse performs better than other models when confronted with Chinese prompts. Comments: Accepted by ICSE-SEIP 2024 Subjects: Software Engineering (cs.SE) ; Artificial Intelligence (cs.AI); Machine Learning (cs.LG) Cite as: arXiv:2310.06266 [cs.SE] &nbsp; (or arXiv:2310.06266v2 [cs.SE] for this version) &nbsp; https://doi.org/10.48550/arXiv.2310.06266 Focus to learn more arXiv-issued DOI via DataCite Related DOI : https://doi.org/10.1145/3639477.3639719 Focus to learn more DOI(s) linking to related resources Submission history From: Peng Di [ view email ] [v1] Tue, 10 Oct 2023 02:38:44 UTC (4,623 KB) [v2] Wed, 10 Jan 2024 19:59:03 UTC (4,623 KB) Full-text links: Access Paper: View a PDF of the paper titled CodeFuse-13B: A Pretrained Multi-lingual Code Large Language Model, by Peng Di and 37 other authors View PDF HTML (experimental) TeX Source Other Formats view license Current browse context: cs.SE &lt;&nbsp;prev &nbsp; | &nbsp; next&nbsp;&gt; new | recent | 2023-10 Change to browse by: cs cs.AI cs.LG References &amp; Citations NASA ADS Google Scholar Semantic Scholar a export BibTeX citation Loading... BibTeX formatted citation &times; loading... Data provided by: Bookmark Bibliographic Tools Bibliographic and Citation Tools Bibliographic Explorer Toggle Bibliographic Explorer ( What is the Explorer? ) Connected Papers Toggle Connected Papers ( What is Connected Papers? ) Litmaps Toggle Litmaps ( What is Litmaps? ) scite.ai Toggle scite Smart Citations ( What are Smart Citations? ) Code, Data, Media Code, Data and Media Associated with this Article alphaXiv Toggle alphaXiv ( What is alphaXiv? ) Links to Code Toggle CatalyzeX Code Finder for Papers ( What is CatalyzeX? ) DagsHub Toggle DagsHub ( What is DagsHub? ) GotitPub Toggle Gotit.pub ( What is GotitPub? ) Huggingface Toggle Hugging Face ( What is Huggingface? ) Links to Code Toggle Papers with Code ( What is Papers with Code? ) ScienceCast Toggle ScienceCast ( What is ScienceCast? ) Demos Demos Replicate Toggle Replicate ( What is Replicate? ) Spaces Toggle Hugging Face Spaces ( What is Spaces? ) Spaces Toggle TXYZ.AI ( What is TXYZ.AI? ) Related Papers Recommenders and Search Tools Link to Influence Flower Influence Flower ( What are Influence Flowers? ) Core recommender toggle CORE Recommender ( What is CORE? ) Author Venue Institution Topic About arXivLabs arXivLabs: experimental projects with community collaborators arXivLabs is a framework that allows collaborators to develop and share new arXiv features directly on our website. Both individuals and organizations that work with arXivLabs have embraced and accepted our values of openness, community, excellence, and user data privacy. arXiv is committed to these values and only works with partners that adhere to them. Have an idea for a project that will add value for arXiv's community? Learn more about arXivLabs . Which authors of this paper are endorsers? | Disable MathJax ( What is MathJax? ) About Help contact arXiv Click here to contact arXiv Contact subscribe to arXiv mailings Click here to subscribe Subscribe Copyright Privacy Policy Web Accessibility Assistance arXiv Operational Status Get status notifications via email or slack ",
      "fetch_method": "direct-html"
    },
    {
      "arxiv_id": "https://arxiv.org/abs/2310.01602",
      "full_text": " [2310.01602] CAT-LM: Training Language Models on Aligned Code And Tests Skip to main content We gratefully acknowledge support from the Simons Foundation, member institutions , and all contributors. Donate &gt; cs &gt; arXiv:2310.01602 Help | Advanced Search All fields Title Author Abstract Comments Journal reference ACM classification MSC classification Report number arXiv identifier DOI ORCID arXiv author ID Help pages Full text Search open search GO open navigation menu quick links Login Help Pages About --> Computer Science > Software Engineering arXiv:2310.01602 (cs) [Submitted on 2 Oct 2023] Title: CAT-LM: Training Language Models on Aligned Code And Tests Authors: Nikitha Rao , Kush Jain , Uri Alon , Claire Le Goues , Vincent J. Hellendoorn View a PDF of the paper titled CAT-LM: Training Language Models on Aligned Code And Tests, by Nikitha Rao and 4 other authors View PDF Abstract: Testing is an integral part of the software development process. Yet, writing tests is time-consuming and therefore often neglected. Classical test generation tools such as EvoSuite generate behavioral test suites by optimizing for coverage, but tend to produce tests that are hard to understand. Language models trained on code can generate code that is highly similar to that written by humans, but current models are trained to generate each file separately, as is standard practice in natural language processing, and thus fail to consider the code-under-test context when producing a test file. In this work, we propose the Aligned Code And Tests Language Model (CAT-LM), a GPT-style language model with 2.7 Billion parameters, trained on a corpus of Python and Java projects. We utilize a novel pretraining signal that explicitly considers the mapping between code and test files when available. We also drastically increase the maximum sequence length of inputs to 8,192 tokens, 4x more than typical code generation models, to ensure that the code context is available to the model when generating test code. We analyze its usefulness for realistic applications, showing that sampling with filtering (e.g., by compilability, coverage) allows it to efficiently produce tests that achieve coverage similar to ones written by developers while resembling their writing style. By utilizing the code context, CAT-LM generates more valid tests than even much larger language models trained with more data (CodeGen 16B and StarCoder) and substantially outperforms a recent test-specific model (TeCo) at test completion. Overall, our work highlights the importance of incorporating software-specific insights when training language models for code and paves the way to more powerful automated test generation. Subjects: Software Engineering (cs.SE) ; Artificial Intelligence (cs.AI) Cite as: arXiv:2310.01602 [cs.SE] &nbsp; (or arXiv:2310.01602v1 [cs.SE] for this version) &nbsp; https://doi.org/10.48550/arXiv.2310.01602 Focus to learn more arXiv-issued DOI via DataCite Submission history From: Nikitha Rao [ view email ] [v1] Mon, 2 Oct 2023 19:52:22 UTC (2,232 KB) Full-text links: Access Paper: View a PDF of the paper titled CAT-LM: Training Language Models on Aligned Code And Tests, by Nikitha Rao and 4 other authors View PDF TeX Source Other Formats view license Current browse context: cs.SE &lt;&nbsp;prev &nbsp; | &nbsp; next&nbsp;&gt; new | recent | 2023-10 Change to browse by: cs cs.AI References &amp; Citations NASA ADS Google Scholar Semantic Scholar export BibTeX citation Loading... BibTeX formatted citation &times; loading... Data provided by: Bookmark Bibliographic Tools Bibliographic and Citation Tools Bibliographic Explorer Toggle Bibliographic Explorer ( What is the Explorer? ) Connected Papers Toggle Connected Papers ( What is Connected Papers? ) Litmaps Toggle Litmaps ( What is Litmaps? ) scite.ai Toggle scite Smart Citations ( What are Smart Citations? ) Code, Data, Media Code, Data and Media Associated with this Article alphaXiv Toggle alphaXiv ( What is alphaXiv? ) Links to Code Toggle CatalyzeX Code Finder for Papers ( What is CatalyzeX? ) DagsHub Toggle DagsHub ( What is DagsHub? ) GotitPub Toggle Gotit.pub ( What is GotitPub? ) Huggingface Toggle Hugging Face ( What is Huggingface? ) Links to Code Toggle Papers with Code ( What is Papers with Code? ) ScienceCast Toggle ScienceCast ( What is ScienceCast? ) Demos Demos Replicate Toggle Replicate ( What is Replicate? ) Spaces Toggle Hugging Face Spaces ( What is Spaces? ) Spaces Toggle TXYZ.AI ( What is TXYZ.AI? ) Related Papers Recommenders and Search Tools Link to Influence Flower Influence Flower ( What are Influence Flowers? ) Core recommender toggle CORE Recommender ( What is CORE? ) Author Venue Institution Topic About arXivLabs arXivLabs: experimental projects with community collaborators arXivLabs is a framework that allows collaborators to develop and share new arXiv features directly on our website. Both individuals and organizations that work with arXivLabs have embraced and accepted our values of openness, community, excellence, and user data privacy. arXiv is committed to these values and only works with partners that adhere to them. Have an idea for a project that will add value for arXiv's community? Learn more about arXivLabs . Which authors of this paper are endorsers? | Disable MathJax ( What is MathJax? ) About Help contact arXiv Click here to contact arXiv Contact subscribe to arXiv mailings Click here to subscribe Subscribe Copyright Privacy Policy Web Accessibility Assistance arXiv Operational Status Get status notifications via email or slack ",
      "fetch_method": "direct-html"
    },
    {
      "arxiv_id": "https://arxiv.org/abs/2308.12908",
      "full_text": " [2308.12908] POLCA: Power Oversubscription in LLM Cloud Providers Skip to main content We gratefully acknowledge support from the Simons Foundation, member institutions , and all contributors. Donate &gt; cs &gt; arXiv:2308.12908 Help | Advanced Search All fields Title Author Abstract Comments Journal reference ACM classification MSC classification Report number arXiv identifier DOI ORCID arXiv author ID Help pages Full text Search open search GO open navigation menu quick links Login Help Pages About --> Computer Science > Distributed, Parallel, and Cluster Computing arXiv:2308.12908 (cs) [Submitted on 24 Aug 2023] Title: POLCA: Power Oversubscription in LLM Cloud Providers Authors: Pratyush Patel , Esha Choukse , Chaojie Zhang , Íñigo Goiri , Brijesh Warrier , Nithish Mahalingam , Ricardo Bianchini View a PDF of the paper titled POLCA: Power Oversubscription in LLM Cloud Providers, by Pratyush Patel and 6 other authors View PDF Abstract: Recent innovation in large language models (LLMs), and their myriad use-cases have rapidly driven up the compute capacity demand for datacenter GPUs. Several cloud providers and other enterprises have made substantial plans of growth in their datacenters to support these new workloads. One of the key bottleneck resources in datacenters is power, and given the increasing model sizes of LLMs, they are becoming increasingly power intensive. In this paper, we show that there is a significant opportunity to oversubscribe power in LLM clusters. Power oversubscription improves the power efficiency of these datacenters, allowing more deployable servers per datacenter, and reduces the deployment time, since building new datacenters is slow. We extensively characterize the power consumption patterns of a variety of LLMs and their configurations. We identify the differences between the inference and training power consumption patterns. Based on our analysis of these LLMs, we claim that the average and peak power utilization in LLM clusters for inference should not be very high. Our deductions align with the data from production LLM clusters, revealing that inference workloads offer substantial headroom for power oversubscription. However, the stringent set of telemetry and controls that GPUs offer in a virtualized environment, makes it challenging to have a reliable and robust power oversubscription mechanism. We propose POLCA, our framework for power oversubscription that is robust, reliable, and readily deployable for GPU clusters. Using open-source models to replicate the power patterns observed in production, we simulate POLCA and demonstrate that we can deploy 30% more servers in the same GPU cluster for inference, with minimal performance loss Subjects: Distributed, Parallel, and Cluster Computing (cs.DC) ; Hardware Architecture (cs.AR); Machine Learning (cs.LG) Cite as: arXiv:2308.12908 [cs.DC] &nbsp; (or arXiv:2308.12908v1 [cs.DC] for this version) &nbsp; https://doi.org/10.48550/arXiv.2308.12908 Focus to learn more arXiv-issued DOI via DataCite Submission history From: Pratyush Patel [ view email ] [v1] Thu, 24 Aug 2023 16:32:34 UTC (847 KB) Full-text links: Access Paper: View a PDF of the paper titled POLCA: Power Oversubscription in LLM Cloud Providers, by Pratyush Patel and 6 other authors View PDF TeX Source Other Formats view license Current browse context: cs.DC &lt;&nbsp;prev &nbsp; | &nbsp; next&nbsp;&gt; new | recent | 2023-08 Change to browse by: cs cs.AR cs.LG References &amp; Citations NASA ADS Google Scholar Semantic Scholar export BibTeX citation Loading... BibTeX formatted citation &times; loading... Data provided by: Bookmark Bibliographic Tools Bibliographic and Citation Tools Bibliographic Explorer Toggle Bibliographic Explorer ( What is the Explorer? ) Connected Papers Toggle Connected Papers ( What is Connected Papers? ) Litmaps Toggle Litmaps ( What is Litmaps? ) scite.ai Toggle scite Smart Citations ( What are Smart Citations? ) Code, Data, Media Code, Data and Media Associated with this Article alphaXiv Toggle alphaXiv ( What is alphaXiv? ) Links to Code Toggle CatalyzeX Code Finder for Papers ( What is CatalyzeX? ) DagsHub Toggle DagsHub ( What is DagsHub? ) GotitPub Toggle Gotit.pub ( What is GotitPub? ) Huggingface Toggle Hugging Face ( What is Huggingface? ) Links to Code Toggle Papers with Code ( What is Papers with Code? ) ScienceCast Toggle ScienceCast ( What is ScienceCast? ) Demos Demos Replicate Toggle Replicate ( What is Replicate? ) Spaces Toggle Hugging Face Spaces ( What is Spaces? ) Spaces Toggle TXYZ.AI ( What is TXYZ.AI? ) Related Papers Recommenders and Search Tools Link to Influence Flower Influence Flower ( What are Influence Flowers? ) Core recommender toggle CORE Recommender ( What is CORE? ) Author Venue Institution Topic About arXivLabs arXivLabs: experimental projects with community collaborators arXivLabs is a framework that allows collaborators to develop and share new arXiv features directly on our website. Both individuals and organizations that work with arXivLabs have embraced and accepted our values of openness, community, excellence, and user data privacy. arXiv is committed to these values and only works with partners that adhere to them. Have an idea for a project that will add value for arXiv's community? Learn more about arXivLabs . Which authors of this paper are endorsers? | Disable MathJax ( What is MathJax? ) About Help contact arXiv Click here to contact arXiv Contact subscribe to arXiv mailings Click here to subscribe Subscribe Copyright Privacy Policy Web Accessibility Assistance arXiv Operational Status Get status notifications via email or slack ",
      "fetch_method": "direct-html"
    },
    {
      "arxiv_id": "https://arxiv.org/abs/2312.13322",
      "full_text": " [2312.13322] MonoCoder: Domain-Specific Code Language Model for HPC Codes and Tasks Skip to main content We gratefully acknowledge support from the Simons Foundation, member institutions , and all contributors. Donate &gt; cs &gt; arXiv:2312.13322 Help | Advanced Search All fields Title Author Abstract Comments Journal reference ACM classification MSC classification Report number arXiv identifier DOI ORCID arXiv author ID Help pages Full text Search open search GO open navigation menu quick links Login Help Pages About --> Computer Science > Programming Languages arXiv:2312.13322 (cs) [Submitted on 20 Dec 2023 ( v1 ), last revised 19 Sep 2024 (this version, v3)] Title: MonoCoder: Domain-Specific Code Language Model for HPC Codes and Tasks Authors: Tal Kadosh , Niranjan Hasabnis , Vy A. Vo , Nadav Schneider , Neva Krien , Mihai Capota , Abdul Wasay , Nesreen Ahmed , Ted Willke , Guy Tamir , Yuval Pinter , Timothy Mattson , Gal Oren View a PDF of the paper titled MonoCoder: Domain-Specific Code Language Model for HPC Codes and Tasks, by Tal Kadosh and 12 other authors View PDF HTML (experimental) Abstract: With easier access to powerful compute resources, there is a growing trend in AI for software development to develop large language models (LLMs) to address a variety of programming tasks. Even LLMs applied to tasks from the high-performance computing (HPC) domain are huge in size and demand expensive compute resources for training. This is partly because LLMs for HPC tasks are obtained by finetuning existing LLMs that support several natural and/or programming languages. We found this design choice confusing - why do we need LLMs trained on natural languages and programming languages unrelated to HPC for HPC-specific tasks? In this line of work, we aim to question choices made by existing LLMs by developing smaller language models (LMs) for specific domains - we call them domain-specific LMs. Specifically, we start with HPC as a domain and build an HPC-specific LM, named MonoCoder, which is orders of magnitude smaller than existing LMs but delivers better performance on non-HPC and HPC codes. Specifically, we pre-trained MonoCoder on an HPC-specific dataset (named HPCorpus) of C and C++ programs mined from GitHub. We evaluated the performance of MonoCoder against state-of-the-art multi-lingual LLMs. Results demonstrate that MonoCoder, although much smaller than existing LMs, outperforms other LLMs on normalized-perplexity tests (in relation to model size) while also delivering competing CodeBLEU scores for high-performance and parallel code generations. In other words, results suggest that MonoCoder understands HPC code better than state-of-the-art LLMs. Subjects: Programming Languages (cs.PL) ; Artificial Intelligence (cs.AI); Machine Learning (cs.LG); Software Engineering (cs.SE) Cite as: arXiv:2312.13322 [cs.PL] &nbsp; (or arXiv:2312.13322v3 [cs.PL] for this version) &nbsp; https://doi.org/10.48550/arXiv.2312.13322 Focus to learn more arXiv-issued DOI via DataCite Submission history From: Tal Kadosh [ view email ] [v1] Wed, 20 Dec 2023 15:11:06 UTC (3,506 KB) [v2] Tue, 17 Sep 2024 16:29:03 UTC (3,506 KB) [v3] Thu, 19 Sep 2024 19:28:09 UTC (3,506 KB) Full-text links: Access Paper: View a PDF of the paper titled MonoCoder: Domain-Specific Code Language Model for HPC Codes and Tasks, by Tal Kadosh and 12 other authors View PDF HTML (experimental) TeX Source Other Formats view license Current browse context: cs.PL &lt;&nbsp;prev &nbsp; | &nbsp; next&nbsp;&gt; new | recent | 2023-12 Change to browse by: cs cs.AI cs.LG cs.SE References &amp; Citations NASA ADS Google Scholar Semantic Scholar export BibTeX citation Loading... BibTeX formatted citation &times; loading... Data provided by: Bookmark Bibliographic Tools Bibliographic and Citation Tools Bibliographic Explorer Toggle Bibliographic Explorer ( What is the Explorer? ) Connected Papers Toggle Connected Papers ( What is Connected Papers? ) Litmaps Toggle Litmaps ( What is Litmaps? ) scite.ai Toggle scite Smart Citations ( What are Smart Citations? ) Code, Data, Media Code, Data and Media Associated with this Article alphaXiv Toggle alphaXiv ( What is alphaXiv? ) Links to Code Toggle CatalyzeX Code Finder for Papers ( What is CatalyzeX? ) DagsHub Toggle DagsHub ( What is DagsHub? ) GotitPub Toggle Gotit.pub ( What is GotitPub? ) Huggingface Toggle Hugging Face ( What is Huggingface? ) Links to Code Toggle Papers with Code ( What is Papers with Code? ) ScienceCast Toggle ScienceCast ( What is ScienceCast? ) Demos Demos Replicate Toggle Replicate ( What is Replicate? ) Spaces Toggle Hugging Face Spaces ( What is Spaces? ) Spaces Toggle TXYZ.AI ( What is TXYZ.AI? ) Related Papers Recommenders and Search Tools Link to Influence Flower Influence Flower ( What are Influence Flowers? ) Core recommender toggle CORE Recommender ( What is CORE? ) Author Venue Institution Topic About arXivLabs arXivLabs: experimental projects with community collaborators arXivLabs is a framework that allows collaborators to develop and share new arXiv features directly on our website. Both individuals and organizations that work with arXivLabs have embraced and accepted our values of openness, community, excellence, and user data privacy. arXiv is committed to these values and only works with partners that adhere to them. Have an idea for a project that will add value for arXiv's community? Learn more about arXivLabs . Which authors of this paper are endorsers? | Disable MathJax ( What is MathJax? ) About Help contact arXiv Click here to contact arXiv Contact subscribe to arXiv mailings Click here to subscribe Subscribe Copyright Privacy Policy Web Accessibility Assistance arXiv Operational Status Get status notifications via email or slack ",
      "fetch_method": "direct-html"
    },
    {
      "arxiv_id": "https://arxiv.org/abs/2312.07625",
      "full_text": " [2312.07625] Astrocyte-Enabled Advancements in Spiking Neural Networks for Large Language Modeling Skip to main content We gratefully acknowledge support from the Simons Foundation, member institutions , and all contributors. Donate &gt; cs &gt; arXiv:2312.07625 Help | Advanced Search All fields Title Author Abstract Comments Journal reference ACM classification MSC classification Report number arXiv identifier DOI ORCID arXiv author ID Help pages Full text Search open search GO open navigation menu quick links Login Help Pages About --> Computer Science > Neural and Evolutionary Computing arXiv:2312.07625 (cs) [Submitted on 12 Dec 2023 ( v1 ), last revised 26 Dec 2023 (this version, v2)] Title: Astrocyte-Enabled Advancements in Spiking Neural Networks for Large Language Modeling Authors: Guobin Shen , Dongcheng Zhao , Yiting Dong , Yang Li , Jindong Li , Kang Sun , Yi Zeng View a PDF of the paper titled Astrocyte-Enabled Advancements in Spiking Neural Networks for Large Language Modeling, by Guobin Shen and 6 other authors View PDF HTML (experimental) Abstract: Within the complex neuroarchitecture of the brain, astrocytes play crucial roles in development, structure, and metabolism. These cells regulate neural activity through tripartite synapses, directly impacting cognitive processes such as learning and memory. Despite the growing recognition of astrocytes&#39; significance, traditional Spiking Neural Network (SNN) models remain predominantly neuron-centric, overlooking the profound influence of astrocytes on neural dynamics. Inspired by these biological insights, we have developed an Astrocyte-Modulated Spiking Unit (AM-SU), an innovative framework that integrates neuron-astrocyte interactions into the computational paradigm, demonstrating wide applicability across various hardware platforms. Our Astrocyte-Modulated Spiking Neural Network (AstroSNN) exhibits exceptional performance in tasks involving memory retention and natural language generation, particularly in handling long-term dependencies and complex linguistic structures. The design of AstroSNN not only enhances its biological authenticity but also introduces novel computational dynamics, enabling more effective processing of complex temporal dependencies. Furthermore, AstroSNN shows low latency, high throughput, and reduced memory usage in practical applications, making it highly suitable for resource-constrained environments. By successfully integrating astrocytic dynamics into intelligent neural networks, our work narrows the gap between biological plausibility and neural modeling, laying the groundwork for future biologically-inspired neural computing research that includes both neurons and astrocytes. Subjects: Neural and Evolutionary Computing (cs.NE) ; Artificial Intelligence (cs.AI) Cite as: arXiv:2312.07625 [cs.NE] &nbsp; (or arXiv:2312.07625v2 [cs.NE] for this version) &nbsp; https://doi.org/10.48550/arXiv.2312.07625 Focus to learn more arXiv-issued DOI via DataCite Submission history From: Guobin Shen [ view email ] [v1] Tue, 12 Dec 2023 06:56:31 UTC (10,697 KB) [v2] Tue, 26 Dec 2023 02:54:29 UTC (17,322 KB) Full-text links: Access Paper: View a PDF of the paper titled Astrocyte-Enabled Advancements in Spiking Neural Networks for Large Language Modeling, by Guobin Shen and 6 other authors View PDF HTML (experimental) TeX Source Other Formats view license Current browse context: cs.NE &lt;&nbsp;prev &nbsp; | &nbsp; next&nbsp;&gt; new | recent | 2023-12 Change to browse by: cs cs.AI References &amp; Citations NASA ADS Google Scholar Semantic Scholar export BibTeX citation Loading... BibTeX formatted citation &times; loading... Data provided by: Bookmark Bibliographic Tools Bibliographic and Citation Tools Bibliographic Explorer Toggle Bibliographic Explorer ( What is the Explorer? ) Connected Papers Toggle Connected Papers ( What is Connected Papers? ) Litmaps Toggle Litmaps ( What is Litmaps? ) scite.ai Toggle scite Smart Citations ( What are Smart Citations? ) Code, Data, Media Code, Data and Media Associated with this Article alphaXiv Toggle alphaXiv ( What is alphaXiv? ) Links to Code Toggle CatalyzeX Code Finder for Papers ( What is CatalyzeX? ) DagsHub Toggle DagsHub ( What is DagsHub? ) GotitPub Toggle Gotit.pub ( What is GotitPub? ) Huggingface Toggle Hugging Face ( What is Huggingface? ) Links to Code Toggle Papers with Code ( What is Papers with Code? ) ScienceCast Toggle ScienceCast ( What is ScienceCast? ) Demos Demos Replicate Toggle Replicate ( What is Replicate? ) Spaces Toggle Hugging Face Spaces ( What is Spaces? ) Spaces Toggle TXYZ.AI ( What is TXYZ.AI? ) Related Papers Recommenders and Search Tools Link to Influence Flower Influence Flower ( What are Influence Flowers? ) Core recommender toggle CORE Recommender ( What is CORE? ) Author Venue Institution Topic About arXivLabs arXivLabs: experimental projects with community collaborators arXivLabs is a framework that allows collaborators to develop and share new arXiv features directly on our website. Both individuals and organizations that work with arXivLabs have embraced and accepted our values of openness, community, excellence, and user data privacy. arXiv is committed to these values and only works with partners that adhere to them. Have an idea for a project that will add value for arXiv's community? Learn more about arXivLabs . Which authors of this paper are endorsers? | Disable MathJax ( What is MathJax? ) About Help contact arXiv Click here to contact arXiv Contact subscribe to arXiv mailings Click here to subscribe Subscribe Copyright Privacy Policy Web Accessibility Assistance arXiv Operational Status Get status notifications via email or slack ",
      "fetch_method": "direct-html"
    },
    {
      "arxiv_id": "https://arxiv.org/abs/2402.09099",
      "full_text": " [2402.09099] Neuron-based Multifractal Analysis of Neuron Interaction Dynamics in Large Models Skip to main content We gratefully acknowledge support from the Simons Foundation, member institutions , and all contributors. Donate &gt; cs &gt; arXiv:2402.09099 Help | Advanced Search All fields Title Author Abstract Comments Journal reference ACM classification MSC classification Report number arXiv identifier DOI ORCID arXiv author ID Help pages Full text Search open search GO open navigation menu quick links Login Help Pages About --> Computer Science > Artificial Intelligence arXiv:2402.09099 (cs) [Submitted on 14 Feb 2024 ( v1 ), last revised 6 Aug 2025 (this version, v7)] Title: Neuron-based Multifractal Analysis of Neuron Interaction Dynamics in Large Models Authors: Xiongye Xiao , Heng Ping , Chenyu Zhou , Defu Cao , Yaxing Li , Yi-Zhuo Zhou , Shixuan Li , Nikos Kanakaris , Paul Bogdan View a PDF of the paper titled Neuron-based Multifractal Analysis of Neuron Interaction Dynamics in Large Models, by Xiongye Xiao and 8 other authors View PDF HTML (experimental) Abstract: In recent years, there has been increasing attention on the capabilities of large models, particularly in handling complex tasks that small-scale models are unable to perform. Notably, large language models (LLMs) have demonstrated ``intelligent&#39;&#39; abilities such as complex reasoning and abstract language comprehension, reflecting cognitive-like behaviors. However, current research on emergent abilities in large models predominantly focuses on the relationship between model performance and size, leaving a significant gap in the systematic quantitative analysis of the internal structures and mechanisms driving these emergent abilities. Drawing inspiration from neuroscience research on brain network structure and self-organization, we propose (i) a general network representation of large models, (ii) a new analytical framework, called Neuron-based Multifractal Analysis (NeuroMFA), for structural analysis, and (iii) a novel structure-based metric as a proxy for emergent abilities of large models. By linking structural features to the capabilities of large models, NeuroMFA provides a quantitative framework for analyzing emergent phenomena in large models. Our experiments show that the proposed method yields a comprehensive measure of network&#39;s evolving heterogeneity and organization, offering theoretical foundations and a new perspective for investigating emergent abilities in large models. Comments: Accepted at ICLR 2025. OpenReview: this https URL Subjects: Artificial Intelligence (cs.AI) Cite as: arXiv:2402.09099 [cs.AI] &nbsp; (or arXiv:2402.09099v7 [cs.AI] for this version) &nbsp; https://doi.org/10.48550/arXiv.2402.09099 Focus to learn more arXiv-issued DOI via DataCite Submission history From: Xiongye Xiao [ view email ] [v1] Wed, 14 Feb 2024 11:20:09 UTC (24,926 KB) [v2] Mon, 4 Mar 2024 11:22:38 UTC (25,126 KB) [v3] Tue, 5 Mar 2024 10:44:36 UTC (25,126 KB) [v4] Thu, 21 Mar 2024 05:33:23 UTC (26,400 KB) [v5] Fri, 24 Jan 2025 08:22:25 UTC (29,196 KB) [v6] Tue, 4 Feb 2025 07:46:33 UTC (29,196 KB) [v7] Wed, 6 Aug 2025 03:34:21 UTC (24,999 KB) Full-text links: Access Paper: View a PDF of the paper titled Neuron-based Multifractal Analysis of Neuron Interaction Dynamics in Large Models, by Xiongye Xiao and 8 other authors View PDF HTML (experimental) TeX Source Other Formats view license Current browse context: cs.AI &lt;&nbsp;prev &nbsp; | &nbsp; next&nbsp;&gt; new | recent | 2024-02 Change to browse by: cs References &amp; Citations NASA ADS Google Scholar Semantic Scholar a export BibTeX citation Loading... BibTeX formatted citation &times; loading... Data provided by: Bookmark Bibliographic Tools Bibliographic and Citation Tools Bibliographic Explorer Toggle Bibliographic Explorer ( What is the Explorer? ) Connected Papers Toggle Connected Papers ( What is Connected Papers? ) Litmaps Toggle Litmaps ( What is Litmaps? ) scite.ai Toggle scite Smart Citations ( What are Smart Citations? ) Code, Data, Media Code, Data and Media Associated with this Article alphaXiv Toggle alphaXiv ( What is alphaXiv? ) Links to Code Toggle CatalyzeX Code Finder for Papers ( What is CatalyzeX? ) DagsHub Toggle DagsHub ( What is DagsHub? ) GotitPub Toggle Gotit.pub ( What is GotitPub? ) Huggingface Toggle Hugging Face ( What is Huggingface? ) Links to Code Toggle Papers with Code ( What is Papers with Code? ) ScienceCast Toggle ScienceCast ( What is ScienceCast? ) Demos Demos Replicate Toggle Replicate ( What is Replicate? ) Spaces Toggle Hugging Face Spaces ( What is Spaces? ) Spaces Toggle TXYZ.AI ( What is TXYZ.AI? ) Related Papers Recommenders and Search Tools Link to Influence Flower Influence Flower ( What are Influence Flowers? ) Core recommender toggle CORE Recommender ( What is CORE? ) Author Venue Institution Topic About arXivLabs arXivLabs: experimental projects with community collaborators arXivLabs is a framework that allows collaborators to develop and share new arXiv features directly on our website. Both individuals and organizations that work with arXivLabs have embraced and accepted our values of openness, community, excellence, and user data privacy. arXiv is committed to these values and only works with partners that adhere to them. Have an idea for a project that will add value for arXiv's community? Learn more about arXivLabs . Which authors of this paper are endorsers? | Disable MathJax ( What is MathJax? ) About Help contact arXiv Click here to contact arXiv Contact subscribe to arXiv mailings Click here to subscribe Subscribe Copyright Privacy Policy Web Accessibility Assistance arXiv Operational Status Get status notifications via email or slack ",
      "fetch_method": "direct-html"
    },
    {
      "arxiv_id": "https://arxiv.org/abs/2402.12399",
      "full_text": " [2402.12399] Turn Waste into Worth: Rectifying Top-$k$ Router of MoE Skip to main content We gratefully acknowledge support from the Simons Foundation, member institutions , and all contributors. Donate &gt; cs &gt; arXiv:2402.12399 Help | Advanced Search All fields Title Author Abstract Comments Journal reference ACM classification MSC classification Report number arXiv identifier DOI ORCID arXiv author ID Help pages Full text Search open search GO open navigation menu quick links Login Help Pages About --> Computer Science > Machine Learning arXiv:2402.12399 (cs) [Submitted on 17 Feb 2024 ( v1 ), last revised 21 Feb 2024 (this version, v2)] Title: Turn Waste into Worth: Rectifying Top-$k$ Router of MoE Authors: Zhiyuan Zeng , Qipeng Guo , Zhaoye Fei , Zhangyue Yin , Yunhua Zhou , Linyang Li , Tianxiang Sun , Hang Yan , Dahua Lin , Xipeng Qiu View a PDF of the paper titled Turn Waste into Worth: Rectifying Top-$k$ Router of MoE, by Zhiyuan Zeng and 9 other authors View PDF HTML (experimental) Abstract: Sparse Mixture of Experts (MoE) models are popular for training large language models due to their computational efficiency. However, the commonly used top-$k$ routing mechanism suffers from redundancy computation and memory costs due to the unbalanced routing. Some experts are overflow, where the exceeding tokens are dropped. While some experts are vacant, which are padded with zeros, negatively impacting model performance. To address the dropped tokens and padding, we propose the Rectify-Router, comprising the Intra-GPU Rectification and the Fill-in Rectification. The Intra-GPU Rectification handles dropped tokens, efficiently routing them to experts within the GPU where they are located to avoid inter-GPU communication. The Fill-in Rectification addresses padding by replacing padding tokens with the tokens that have high routing scores. Our experimental results demonstrate that the Intra-GPU Rectification and the Fill-in Rectification effectively handle dropped tokens and padding, respectively. Furthermore, the combination of them achieves superior performance, surpassing the accuracy of the vanilla top-1 router by 4.7%. Subjects: Machine Learning (cs.LG) ; Artificial Intelligence (cs.AI); Computation and Language (cs.CL) Cite as: arXiv:2402.12399 [cs.LG] &nbsp; (or arXiv:2402.12399v2 [cs.LG] for this version) &nbsp; https://doi.org/10.48550/arXiv.2402.12399 Focus to learn more arXiv-issued DOI via DataCite Submission history From: Zhiyuan Zeng [ view email ] [v1] Sat, 17 Feb 2024 06:23:27 UTC (211 KB) [v2] Wed, 21 Feb 2024 13:33:12 UTC (211 KB) Full-text links: Access Paper: View a PDF of the paper titled Turn Waste into Worth: Rectifying Top-$k$ Router of MoE, by Zhiyuan Zeng and 9 other authors View PDF HTML (experimental) TeX Source Other Formats view license Current browse context: cs.LG &lt;&nbsp;prev &nbsp; | &nbsp; next&nbsp;&gt; new | recent | 2024-02 Change to browse by: cs cs.AI cs.CL References &amp; Citations NASA ADS Google Scholar Semantic Scholar export BibTeX citation Loading... BibTeX formatted citation &times; loading... Data provided by: Bookmark Bibliographic Tools Bibliographic and Citation Tools Bibliographic Explorer Toggle Bibliographic Explorer ( What is the Explorer? ) Connected Papers Toggle Connected Papers ( What is Connected Papers? ) Litmaps Toggle Litmaps ( What is Litmaps? ) scite.ai Toggle scite Smart Citations ( What are Smart Citations? ) Code, Data, Media Code, Data and Media Associated with this Article alphaXiv Toggle alphaXiv ( What is alphaXiv? ) Links to Code Toggle CatalyzeX Code Finder for Papers ( What is CatalyzeX? ) DagsHub Toggle DagsHub ( What is DagsHub? ) GotitPub Toggle Gotit.pub ( What is GotitPub? ) Huggingface Toggle Hugging Face ( What is Huggingface? ) Links to Code Toggle Papers with Code ( What is Papers with Code? ) ScienceCast Toggle ScienceCast ( What is ScienceCast? ) Demos Demos Replicate Toggle Replicate ( What is Replicate? ) Spaces Toggle Hugging Face Spaces ( What is Spaces? ) Spaces Toggle TXYZ.AI ( What is TXYZ.AI? ) Related Papers Recommenders and Search Tools Link to Influence Flower Influence Flower ( What are Influence Flowers? ) Core recommender toggle CORE Recommender ( What is CORE? ) IArxiv recommender toggle IArxiv Recommender ( What is IArxiv? ) Author Venue Institution Topic About arXivLabs arXivLabs: experimental projects with community collaborators arXivLabs is a framework that allows collaborators to develop and share new arXiv features directly on our website. Both individuals and organizations that work with arXivLabs have embraced and accepted our values of openness, community, excellence, and user data privacy. arXiv is committed to these values and only works with partners that adhere to them. Have an idea for a project that will add value for arXiv's community? Learn more about arXivLabs . Which authors of this paper are endorsers? | Disable MathJax ( What is MathJax? ) About Help contact arXiv Click here to contact arXiv Contact subscribe to arXiv mailings Click here to subscribe Subscribe Copyright Privacy Policy Web Accessibility Assistance arXiv Operational Status Get status notifications via email or slack ",
      "fetch_method": "direct-html"
    },
    {
      "arxiv_id": "https://together.ai/blog/redpajama-models-v1",
      "full_text": " Releasing 3B and 7B RedPajama-INCITE family of models including base, instruction-tuned &amp; chat models This website uses cookies to anonymously analyze website traffic using Google Analytics. Accept Decline ðº Deep Dive on NVIDIA Blackwell with Dylan Patel (Semianalysis) and Ian Buck (NVIDIA) on 10/1 | RSVP â â¡ Together Instant Clusters: self-service NVIDIA GPUs, now generally available â ð DeepSeek-V3.1 is now available on Together AI â ð¥ Announcing DeepSWE: our SOTA software engineering agent â ð Together AI achieves SOC 2 Type 2 compliance â Model Platform Model Platform Products Serverless Inference API for inference on open-source models Dedicated Endpoints Deploy models on custom hardware Fine-Tuning Train &amp; improve high-quality, fast models Evaluations Measure model quality Together Chat Chat app for open-source AI Code Execution Code Sandbox Build AI development environments Code Interpreter Execute LLM-generated code Tools Which LLM to Use Find the ârightâ model for your use case Models See all modelsÂ â OpenAIâ¨ gpt-oss Â â OpenAIâ¨gpt-oss This is some text inside of a div block. Â â try it â DeepSeek Â â DeepSeek This is some text inside of a div block. Â â try it â Qwen Â â Qwen3 235B This is some text inside of a div block. Â â try it â Llama Â â Llama This is some text inside of a div block. Â â try it â Kimi K2 Â â Kimi K2 This is some text inside of a div block. Â â try it â Cogito Â â Cogito This is some text inside of a div block. Â â try it â GPU Cloud GPU Cloud Clusters of Any Size Instant Clusters Ready to use, self-service GPUs Reserved Clusters Dedicated capacity, with expert support Frontier AI Factory 1K â 10K â 100K+ NVIDIA GPUs Cloud Services Data Center Locations Global GPU power in 25+ cities Slurm Cluster management system GPUs NVIDIA GB200 NVL72 Â â NVIDIA GB00 NVL72 try it â NVIDIA HGX B200 Â â NVIDIA HGX B200 try it â NVIDIA H200 Â â NVIDIA H200 try it â NVIDIA H100 Â â NVIDIA H100 try it â Solutions Solutions Solutions Enterprise Secure, reliable AI infrastructure Customer Stories Testimonials from AI pioneers Why Open Source How to own your AI Industries &amp; Use-Cases Scale your business with Together AI Customer Stories How Hedra Scales Viral AI Video Generation with 60% Cost Savings When Standard Inference Frameworks Failed, Together AI Enabled 5x Performance Breakthrough Developers Developers Developers Documentation Technical docs for using Together AI Research Advancing the open-source AI frontier Model Library All our open-source models Cookbooks Practical implementation guides Example Apps Our open-source demo apps Videos DeepSeek-R1: How It Works, Simplified! Together Code Sandbox: How To Build AI Coding Agents Pricing Pricing Pricing Pricing Overview Our platform &amp; GPU pricing. Inference Per-token &amp; per-minute pricing. Fine-Tuning LoRA and full fine-tuning pricing. GPU Clusters Hourly rates &amp; custom pricing. Questions?â¨ Weâre here to help! Talk to us â Company Company Company About us Get to know us Values Our approach to open-source AI Team Meet our leadership Careers Join our mission Resources Blog Our latest news &amp; blog posts Research Advancing the open-source AI frontier Events Explore our events calendar Knowledge Base Find answers to your questions Featured Blog Posts Together AI Delivers Top Speeds for DeepSeek-R1-0528 Inference on NVIDIA Blackwell Powering Secure AI: Together AI Achieves SOC 2 Type 2 Compliance Sign In Contact Sales Chat Docs Blog Support Contact Sales Company Releasing 3B and 7B RedPajama-INCITE family of models including base, instruction-tuned &amp; chat models May 5, 2023 ã» ByÂ Together The RedPajama project aims to create a set of leading open-source models and to rigorously understand the ingredients that yield good performance. Â A few weeks ago we released the RedPajama base dataset based on the LLaMA paper, which has galvanized the open-source community. The 5 terabyte dataset has been downloaded hundreds of times and used to train models like MPT, OpenLLaMA, OpenAlpaca. Today we are excited to release RedPajama-INCITE models, including instruct-tuned and chat versions. Todayâs release includes our first models trained on the RedPajama base dataset: a 3 billion and a 7B parameter base model that aims to replicate the LLaMA recipe as closely as possible. In addition we are releasing fully open-source instruction-tuned and chat models. Our key takeaways: The 3B model is the strongest in its class, and the small size makes it extremely fast and accessible (it even runs on a RTX 2070 released over 5 years ago). The instruction-tuned versions of the models achieve strong performance on HELM benchmarks. As expected, on HELM the 7B model performance is higher than the base LLaMA model by 3 points. We recommend using these models for downstream applications with few-shot, entity extraction, classification, or summarization tasks. The 7B model (which is 80% done training) is already outperforming the Pythia 7B model, which is showing the importance of a bigger dataset and the value of the RedPajama base dataset. Based on our observations, we see a clear path for creating a better version of the RedPajama dataset, which we will release in the coming weeks, that will go beyond the quality of LLaMA 7B. We plan to build models at larger scale with this new dataset. We expect differences between the LLaMA 7B and our replication, which we have investigated below. The biggest takeaway is the demonstration that performant LLMs can be built quickly by the open-source community. This work builds on top of our 1.2 trillion token RedPajama dataset, EleutherAIâs Pythia training code, FlashAttention from Stanford and Together, the HELM benchmarks from Stanford CRFM and generous support from EleutherAI &amp; LAION for compute time on the Summit supercomputer within the INCITE program award &quot;Scalable Foundation Models for Transferable Generalist AIâ. We believe these kind of open collaborations, at larger scales, will be behind the best AI systems of the future. âRedPajama 3B model is the strongest model in itâs class and brings a performant large language model to a wide variety of hardware.â Todayâs release includes the following models, all released under the permissive Apache 2.0 license allowing for use both in research and commercial applications. RedPajama-INCITE-Base-3B-v1 Base model that outperforms other open models of similar sizes in benchmarks. RedPajama-INCITE-Chat-3B-v1 Chat model fine-tuned using data from Dolly 2.0 and Open Assistant over the RedPajama-INCITE-Base-3B-v1 base model. RedPajama-INCITE-Instruct-3B-v1 Model designed for few-shot prompts, fine-tuned using the same formula as GPT-JT (but eliminate all datasets that overlap with the HELM benchmark) over the RedPajama-INCITE-Base-3B-v1 base model. RedPajama-INCITE-Base-7B-v0.1 Early preview of our RedPajama 7B part way through training having completed 800 billion out of the planned 1 trillion tokens, as quality continues to improve in training. RedPajama-INCITE-Chat-7B-v0.1 Early preview of chat model trained on RedPajama-INCITE-Base-7B-preview. RedPajama-INCITE-Instruct-7B-v0.1 Early preview of model designed for few-shot prompts trained on RedPajama-INCITE-Base-7B-preview. In only a few weeks the support, suggestions, and feedback for RedPajama from the open-source community has been incredible. Based on our learnings, we are also already starting the next version of the RedPajama base dataset which will be nearly twice the size of the original v1 dataset. Thank you for your support, feedback and suggestions! View fullsize3B model has stabilized at 800 billion tokens and the 7B model continues to improve as it completes training to 1 trillion tokens During RedPajama model training we have shared regular updates, and both the 3B and 7B models have now been trained on 800 billion tokens. We are excited to see that the 3B model has stabilized at 800 billion tokens and the 7B model continues to improve as it completes training to 1 trillion tokens. Â 3B RedPajama Models RedPajama-INCITE-Base-3B-v1 is trained over the RedPajama v1 dataset, with the same architecture as the popular Pythia model suite. We chose to start with the Pythia architecture to understand the value of training with the much larger RedPajama dataset with respect to the current leading open-source dataset, the Pile . Training on Summit leveraged the DeeperSpeed codebase developed by EleutherAI . We are excited to see that at 800B tokens, RedPajama-Base-INCITE-3B has better few-shot performance (measured in HELM , as the average score over 16 core scenarios) and better zero-shot performance (measured in Eleutherâs LM evaluation harness ) compared with open models of similar size, including the well-regarded GPT-Neo and Pythia-2.8B (trained with 420B and 300B tokens, respectively, with the Pile). On HELM, it outperforms these models by 3-5 points. On a subset of tasks from lm-evaluation-harness, outperforms these open models by 2-7 points. Additionally, we are excited to release an instruction-tuned version of this 3B model, RedPajama-INCITE-Instruct-3B-v1, trained following Togetherâs GPT-JT recipe and removing any data in HELM benchmarks to ensure that there is no contamination with respect to HELM. This model shows excellent performance on few-shot tasks, even approaching the quality of LLaMA 7B in a much smaller model, as shown in the results below: Few Shot Results on HELM Core Scenarios The base model also performs well on zero-shot tasks, as measured using EleutherAIâs language model evaluation harness : Models Type HELM (Average score over 16 core scenarios) GPT-Neo Base model 0.357 Pythia-2.8B Base model 0.377 RedPajama-INCITE-Base-3B-v1 Base model 0.406 RedPajama-INCITE-Instruct-3B-v1 Instruction-tuned 0.453 Llama-7B Base model 0.465 (Zero Shot) Results on a subset of lm-evaluation-harness , following LLM Worksheet âs selection of tasks &amp; metrics. We didnât run coqa because of an error as in this issue . Lambada_openai (acc) Hellaswag (acc_norm) Winogrande (acc) Piqa(acc) average GPT-Neo 0.6223 0.5579 0.5769 0.7219 0.6197 Pythia-2.8B 0.6466 0.5933 0.6006 0.7399 0.6451 Pythia-2.8B-dedup 0.6524 0.5941 0.5848 0.7404 0.6429 RedPajama-INCITE-Base-3B-v1 0.6541 0.6317 0.6322 0.7470 0.6662 Results on a subset of lm-evaluation-harness, tasks selected from what used to evaluate Pythia and GPT-J. View fullsizeRedPajama 3B results on a subset of lm-evaluation-harness RedPajama-INCITE-Chat-3B-v1 is an open-source chat model constructed with RedPajama-INCITE-Base-3B-v1 and fine-tuned over the OASST1 dataset by Open Assistant and Dolly v2.0 dataset by DataBricks . We equally mix the datasets, and fine-tune for 3 epochs. Evaluating chat models is a challenging task, and we are in the process of conducting more quantitative evaluation based on human and community feedback, and are excited to share these results soon! Nevertheless, here are some examples comparing the behavior of different chat models. We see that in many examples, RedPajama-INCITE-Chat-3B-v1 has similar quality as Open Assistant as reported in the their paper . RedPajama 3B chat model responses on example queries from the Open Assistant paper. Â RedPajama-INCITE-Chat-3B-v1 (oasst1 + Dolly 2.0) Pythia-2.8B + oasst1 + Dolly 2.0 Open Assistant (âpythia-12b-deduped fine-tuned on the OpenAssistant and Alpaca dataâ [Appendix E of https://arxiv.org/abs/2304.07327]) Imagine a world in which time travel has become possible. It would open up a whole new world of possibilities, including the ability to change the past, present, and future. This could have significant implications for our understanding of history, science, and the universe itself. Time travel has always been a fascinating concept, and the idea of being able to change the past, present, and future would be an incredible opportunity. However, it would also present many ethical and philosophical questions that would need to be addressed. One of the biggest concerns would be the impact that time travel would have on the laws of physics and causality. If we were able to travel back in time and change events, it could potentially create paradoxes and open up new possibilities for time travel. However, it would also mean that the future we have now is no longer guaranteed, and our actions and decisions could have long-lasting consequences. Another concern would be the potential impact on human history. If we were able to change the past, present, and future, it could potentially lead to new discoveries, inventions, and breakthroughs that would have significant impacts on our society and culture. However, it could also mean that we are no longer living in the same world that we know today, and that our understanding of history and the universe could be fundamentally altered. Another ethical concern would be the impact that time travel would have on the concept of free will. If we were able to change the past, present, and future, it could potentially lead to situations where we are no longer responsible for our actions and decisions. This could have significant implications for our sense of agency and autonomy, and could lead to a loss of personal responsibility and accountability. Finally, there would also be the question of what to do with the knowledge that we have access to time travel. If we were able to change the past, present, and future, it would mean that we have access to a vast amount of information and knowledge that we have never had before. This could lead to new discoveries and innovations, but it also could lead to a loss of privacy and a potential breakdown of the current social order. In conclusion, time travel would be a complex and multifaceted issue that would require careful consideration of its ethical, philosophical, and societal implications. It would also require a deep understanding of the laws of physics and causality to ensure that it does not create paradoxes or open up new possibilities for time travel. Imagine a world in which time travel has become possible. It would open up a whole new world of possibilities, including the ability to change the past, present, and future. This could have significant implications for our understanding of history, science, and the universe itself. In a world where time travel is possible, it opens up a whole new world of possibilities. One of the most significant implications would be the ability to change the past, present, and future. This could have significant implications for our understanding of history, science, and the universe itself. For example, if someone could travel back in time and prevent a certain event from occurring, then they could change the course of history and create a new reality. This could potentially lead to a world where different outcomes occur, and this could have significant implications for the way we understand history and the nature of reality. Imagine a world in which time travel has become possible. It would open up a whole new world of possibilities, including the ability to change the past, present, and future. This could have significant implications for our understanding of history, science, and the universe itself. The potential consequences of having access to time travel are vast and far-reaching. One concern is that it could lead to unintended consequences for future generations if we alter events or change history without careful consideration. Additionally, there may be legal issues regarding ownership and rights over changes made through time travel. The ethical considerations would involve weighing the risks and benefits, as well as making sure that any changes made do not cause harm to anyone involved. Finally, it will also be important to ensure that time travelers adhere to all relevant laws and regulations when using their powers. Why is Shakespeare such a renowned writer that is still studied so heavily today? Shakespeare is still studied so heavily today because of his mastery of the English language. His use of language is so beautiful and intricate that it has inspired many modern writers. His plays are still performed today, and many of his poems and sonnets are still read and enjoyed. His plays have been adapted for film and television, and his characters continue to be studied and analyzed by scholars and students. Why is Shakespeare such a renowned writer that is still studied so heavily today? Shakespeare's plays are so well known because they are thought to be timeless and universal. Shakespeare's characters are relatable and his stories are thought to be universal in nature. Shakespeare is also known for his complex and nuanced writing style that is thought to be hard to imitate. Shakespeare's plays are still studied and performed to this day because of their great depth and complexity. Shakespeare's plays are often thought to be about the human condition and how we deal with the pressures of life. Why is Shakespeare such a renowned writer that is still studied so heavily today? Shakespeare is one of the most influential writers in history, and his work continues to be studied and performed around the world. His plays are known for their powerful language, memorable characters, and timeless themes, which have made him a popular choice among playwrights, actors, and audiences alike. In addition to his literary merit, Shakespeareâs works also helped shape the English language and influenced many other authors who followed in his footsteps. Many of his lines continue to be quoted and used as examples of excellent writing. He remains an important figure in literature and culture, and his legacy will likely endure for generations to come. What is the process of creating a successful YouTube channel, from developing a niche and creating content to building a community and monetizing the channel? Creating a successful YouTube channel requires a lot of hard work, dedication, and creativity. Here are some steps to help you develop a successful channel: 1. Develop a niche: Choose a topic or subject that you are passionate about and that you can consistently provide valuable and unique content on. This will help you build an engaged audience and establish yourself as an expert in your field. 2. Create high-quality content: Invest in high-quality equipment, editing software, and other resources to create high-quality videos that are engaging and informative. Make sure to use professional lighting and sound equipment to create a polished and professional look. 3. Build a community: Create a community around your channel by engaging with your audience and responding to their comments and feedback. This will help you build a loyal following and create a sense of community around your channel. 4. Monetize your channel: Consider monetizing your channel through ads, affiliate marketing, or other revenue-generating strategies. This will help you sustain your channel and create a sustainable income. 5. Promote your channel: Promote your channel through social media, email marketing, and other platforms to build your audience and drive traffic to your videos. 6. Grow your channel: Continuously produce high-quality content and engage with your audience to grow your channel and build a loyal following. 7. Monitor analytics: Use analytics tools to monitor your channel's performance and identify areas for improvement. This will help you make data-driven decisions and continuously improve your channel. 8. Continuously learn and grow: Continuously learn and grow in your field of expertise to stay relevant and provide valuable content to your audience. By following these steps, you can develop a successful YouTube channel that can generate income and build a community around your content. What is the process of creating a successful YouTube channel, from developing a niche and creating content to building a community and monetizing the channel? To create a successful YouTube channel, it's important to have a niche and a clear focus. This will help you develop content that is specific to your audience and helps you build a community. Once you have a strong niche and a following, you can start monetizing your channel by creating ads, merchandise, and sponsorships. This will help you earn money from your videos and support your channel. What is the process of creating a successful YouTube channel, from developing a niche and creating content to building a community and monetizing the channel? Creating a successful YouTube channel involves several steps. First, you need to select a niche that aligns with your target audienceâs interests and passions. Once you have chosen a niche, it is important to create engaging content that resonates with your viewers. This can include videos on topics such as tutorials, reviews, vlogs, and more. To build a strong community around your channel, it is essential to engage with your subscribers by responding to comments, posting regular updates, and sharing relevant information. You should also use other social media platforms like Twitter and Facebook to promote your channel and interact with potential audiences. Finally, to make money off your channel, you will need to optimize your video titles, descriptions, tags, and thumbnails for search engine visibility, set up ads, and generate revenue through sponsorships or product integrations. Additionally, you may want to consider using YouTube analytics to track your channelâs performance and adjust strategies accordingly. And, following are some additional examples comparing RedPajama 3B to the Pythia 2.8B model tuned on OASST1 and Dolly v2.0 datasets. RedPajama-INCITE-Chat-3B-v1 (oasst1 + Dolly 2.0) Pythia-2.8B + oasst1 + Dolly 2.0 Write an email to my friends inviting them to come to my home on Friday for a dinner party, bring their own food to share. Hi everyone, I'm excited to host a dinner party on Friday, and I hope you can join me! We'll be having a casual dinner with a twist, where we'll each bring a dish to share. I've made some appetizers, salads, and desserts, but I'm also excited to try some new dishes. I've got a big backyard, so we can all sit outside and enjoy the fresh air. I've got plenty of seating, and we can even set up a few tables in the living room if we have too many people. I hope you can join us, and I look forward to cooking and eating with you! Write an email to my friends inviting them to come to my home on Friday for a dinner party, bring their own food to share. Dear {Friend}, I hope you had a great summer and are looking forward to a fun fall. I'm so excited to share some of my favorite things with you on Friday. I hope you can join us for a delicious dinner and a great conversation. I'm looking forward to seeing you there. Best, {Your Name} Create a list of things to do in San Francisco San Francisco is a beautiful city with plenty of things to do. Here are some of the top things to do in San Francisco: 1. Visit Golden Gate Park: Golden Gate Park is the largest park in San Francisco and is home to the iconic Golden Gate Bridge, Japanese Tea Garden, and the San Francisco Botanical Garden. 2. Take a ferry ride to Alcatraz: Take a ferry ride to Alcatraz, a former federal prison that is now a popular tourist destination. 3. Visit the Golden Gate Bridge: The iconic Golden Gate Bridge is one of the most famous landmarks in San Francisco. Take a walk across the bridge for incredible views of the city and the bay. 4. Visit Chinatown: San Francisco's Chinatown is a vibrant neighborhood with traditional Chinese markets, restaurants, and temples. 5. Visit the Castro District: The Castro District is the heart of the LGBTQ+ community in San Francisco and is home to many bars, restaurants, and shops. 6. Take a cable car ride: Take a ride on the iconic cable cars, which have been operating in San Francisco since the late 19th century. 7. Visit the Palace of Fine Arts: The Palace of Fine Arts is a historic building in San Francisco's Marina District and is a popular spot for wedding ceremonies and photo shoots. 8. Visit the San Francisco Museum of Modern Art: The San Francisco Museum of Modern Art is one of the largest museums of modern and contemporary art in the United States. 9. Visit the San Francisco Zoo: The San Francisco Zoo is a popular destination for families and is home to a variety of animals, including lions, tigers, and gorillas. 10. Take a stroll along the waterfront: The waterfront in San Francisco is a great place to take a stroll Create a list of things to do in San Francisco 1. Visit the Golden Gate Bridge 2. Visit the Alcatraz Island 3. Visit the Marina District 4. Visit the City Hall 5. Visit the Yerba Buena Center for the Arts 6. Visit the Stow Lake 7. Visit the Ferry Building 8. Visit the Presidio 9. Visit the Golden Gate Park 10. Visit the Chinatown Preview of RedPajama 7B The 7B model is still training (at 800B tokens) and we see the training loss still decrease consistently. As a result, we will continue to train it to 1T tokens. Nevertheless, this checkpoint is quite useful, and interesting to build on, and can help the community better understand our training process. Therefore, we are releasing three intermediate checkpoints as a âpreviewâ of the final models. RedPajama-INCITE-Base-7B-v0.1 is a base model trained over 800B tokens RedPajama-INCITE-Chat-7B-v0.1 is its chat counterpart trained over Dolly 2.0 and Open Assistant RedPajama-INCITE-Instruct-7B-v0.1 is instruction tuned for few-shot applications. We follow the recipe for GPT-JT but eliminate all datasets that overlap with the HELM benchmark. Each of these checkpoints are released under the Apache 2.0 license. Even at 800B tokens, we already see promising results. On HELM, the base model outperforms open models such as GPT-J and Pythia-6.9B by 0.5-2.2 points, and on EleutherAIâs lm-evaluation-harness, it outperforms these models by 1-3 points on average. We also see that, compared with LLaMA 7B, there is still a quality gap â 4.3 points on HELM at this moment. For few-shot applications (like those in HELM), the instruction-tuned model (RedPajama-INCITE-Instruct-7B-v0.1) improved over the base model significantly. We hope that some of this gap can be closed after we train for more iterations. (Few Shot) Results on HELM Core Scenarios Model Type HELM (Average score over 16 core scenarios) GPT-J Base model 0.417 Pythia-6.9B Base model 0.400 Llama-7B Base model 0.465 RedPajama-INCITE-Base-7B-v0.1 Base model 0.422 RedPajama-INCITE-Instruct-7B-v0.1 Instruction-tuned 0.499 The base model also performs well on zero-shot tasks, as measured using EleutherAIâs language model evaluation harness : (Zero Shot) Results on a subset of lm-evaluation-harness , following LLM Worksheet âs selection of tasks &amp; metrics. We didnât run coqa because of an error as in this issue . Llama numbers marked with * are taken directly from LLM Worksheet because we run into the following issue . Lambada_openai (acc) Hellaswag (acc_norm) Winogrande (acc) Piqa (acc) average GPT-J 0.6699 0.6663 0.6503 0.7565 0.6857 Pythia-6.9B 0.6712 0.6389 0.6069 0.7519 0.6672 Pythia-6.9B-dedup 0.6893 0.6588 0.6266 0.7578 0.6831 Llama-7B 0.7360* 0.7620* 0.7040 0.7810 0.7457 RedPajama-INCITE-Base-7B-v0.1 0.7061 0.6951 0.6519 0.7611 0.7035 Results on a subset of llm-evaluation-harness , tasks selected from what used to evaluate Pythia and GPT-J. Results on a subset of llm-evaluation-harness Moving Forward: RedPajama v2 with 2T Tokens We learned a lot from the community and are working on building RedPajama v2 with 2 trillion tokens, by taking a systematic approach: We measured the validation loss of different models on different slices of the Pile (for each slice, we selected the first 5K passages). We see that RedPajama lags behind on many slices of the Pile, especially for those slices that are not directly included in the RedPajama dataset. Inspired by this, we plan to mix the Pile dataset into RedPajama and form a more diverse dataset with even more tokens. And we need more code! Another immediate to-do on our plate is to mix in data from the Stack and enrich the Github slice of RedPajama, which contains only 59 billion tokens. With all these improvements together, we are shooting for a 2T token RedPajama v2 dataset. Next week we will start doing a series of runs to understand the right data mixture and start training new models over RedPajama v2. Llama-7B GPT-J RedPajama-Base-INCITE-6.9B-v0.1 ArXiv 1.727 1.511 1.990 BookCorpus2 1.904 2.226 2.213 Books3 1.664 1.979 1.909 DM Mathematics 1.411 1.158 1.910 Enron Emails 2.494 1.844 2.962 EuroParl 1.964 1.216 2.066 FreeLaw 1.425 1.121 1.889 Github 1.126 0.756 1.273 Gutenberg (PG-19) 1.837 1.718 2.079 HackerNews 2.423 2.311 2.821 NIH ExPorter 1.864 2.135 2.413 OpenSubtitles 2.184 2.136 2.510 OpenWebText2 2.027 2.264 2.321 PhilPapers 1.947 2.225 2.280 Pile-CC 2.095 2.441 2.430 PubMed Abstracts 1.694 1.937 2.220 PubMed Central 1.697 1.494 2.122 StackExchange 1.776 1.588 2.078 USPTO Backgrounds 1.740 1.841 2.142 Ubuntu IRC 2.094 1.704 2.518 Wikipedia (en) 1.597 1.629 1.758 YoutubeSubtitles 1.943 1.955 2.226 Acknowledgements The training of the first collection of RedPajama-INCITE models is performed on 3,072 V100 GPUs provided as part of the INCITE compute grant on Summit supercomputer at the Oak Ridge Leadership Computing Facility (OLCF) . This grant was awarded to AAI CERC lab at UniversitÃ© de MontrÃ©al , Â LAION and EleutherAI in fall 2022 for their collaborative project on Scalable Foundation Models for Transferrable Generalist AI . We are thankful to all the project team members helping to build the RedPajama dataset and supporting training, including Ontocord.ai , ETH DS3Lab , AAI CERC Lab at the UniversitÃ© de MontrÃ©al , Stanford Center for Research on Foundation Models (CRFM) , Stanford Hazy Research research group , LAION and EleutherAI . We are grateful to Quentin Anthony ( EleutherAI and INCITE project team ) for sharing the GPT-NeoX model architecture and training code. We are also appreciative to the work done by the growing open-source AI community that made this project possible. That includes: Meta AI â Their inspiring work on LLaMA shows a concrete path towards building strong language models, and it is the original source for our dataset replication. EleutherAI â This project is built on the backs of the great team at EleutherAI â including the source code they provided for training GPT-NeoX. INCITE project team â Their work on GPT-NeoX adaptation to Summit during early 2023 enabled distributed training that scaled efficiently to thousands of Summit GPUs, and ensured smooth training of the models. This research used resources of the Oak Ridge Leadership Computing Facility (OLCF) , which is a DOE Office of Science User Facility supported under Contract DE-AC05-00OR22725. We are Â grateful for the invaluable support provided to us by the OLCF leadership and by the OLCF liaison for the INCITE project. Lower Cost 20% faster training 4x network compression 117x Q: Should I use the RedPajama-V2 Dataset out of the box? RedPajama-V2 is conceptualized as a pool of data that serves as a foundation for creating high quality datasets. The dataset is thus not intended to be used out of the box and, depending on the application, data should be filtered out using the quality signals that accompany the data. With this dataset, we take the view that the optimal filtering of data is dependent on the intended use. Our goal is to provide all the signals and tooling that enables this. No items found. Links in this article RedPajama 3B RedPajama Chat 3B RedPajama Instruct 3B RedPajama 7B RedPajama Chat 7B RedPajama Instruct 7B PJs ð Twitter thread Join our Discord Start building yours here â Subscribe to newsletter Thank you! Your submission has been received! Oops! Something went wrong while submitting the form. Products Solutions Research Blog About Pricing Contact Support Status Trust Center Â© 2025 San Francisco, CA 94114 Privacy policy Terms of service ",
      "fetch_method": "direct-html"
    },
    {
      "arxiv_id": "https://blog.nolano.ai/Hi-NOLIN/",
      "full_text": " Introducing NOLIN | Nolano Research --> Contact Hindi &nbsp; LLMs Introducing NOLIN 30 Oct 2023 &nbsp;&middot; 13 mins read We at Nolano.AI are extending most capable open-source LLMs to new domains, languages and use cases. Help Guide Us What Open Source Model Should We Train Next! Download Hi-NOLIN! Abstract While GPT-4 and similar models achieved impressive performance, primarily in the English language, state-of-the-art (SoTA) language models for many other languages still need to be built. We, introduce Hi-NOLIN, with the goal of building the first state-of-the-art open-source English-Hindi bilingual model. We build upon the Pythia model suite, starting by expanding 7B Pythia architecture to a 9B model. This allows us to scale more efficiently on our hardware (6 GPUs per node instead of 8). We trained this on the 300B token Pile text corpus, which contains both English and code data, and continued pretraining the model on a dataset combining both Hindi and English, in order to extend the model’s ability to understand and generate Hindi language. We observe that, when using the several best practices for continual training, such as, model learning rate re-warming and data replay, we not only transfer and expand LLM’s capabilities to a new language (despite relatively small training data available in that language) but also improve on the original training language and domains (such as English and Code). For example, our continually trained 9B parameter model already outperforms a larger 12B Pythia model, while expanding its capabilities to Hindi. This contrasts with recent efforts of extending LLMs such as LLaMa to new domains such as code (Code LLaMa) and German language (LeoLM), which typically comes at the cost of reduced performance in original domains (the phenomenon known as “catastrophic forgetting” in continual learning). Below, we showcase the current progress of our ongoing training for the Hi-NOLIN-9B model - a checkpoint at the 600B tokens on the new training dataset that contains Hindi. We also observe the Bilingual model to generalize to Code-Mixed English-Hindi informal language of Hinglish - a popular mixed language currently spoken by over 350 million people. Introduction: Hindi is one of the most widely used languages, spoken by more than 600 Million people. It has a rich ancient history, deeply ingrained in culture and reflected in a large amount of literature. However, no open-source high-quality Hindi LLM was built so far, perhaps due to the relative scarcity of data available for training such models from scratch. To overcome this issue, our we continually train our Hindi model on top of the existing open-source LLMs that were pre trained primarily on English language, aiming to extend the capabilities of those models to the new language without forgetting the one learned before, and without having to incur the excessive cost of training such model from scratch on a large iid dataset containing mix of Hindi with the other pre-training data those models were already exposed to. However, previous works leading to models such as CodeLLaMa (which extended LLaMa for code generation) and LeoLM (which extended LLaMa for German Language) demonstrated that extending model to new domain comes at a cost of lower performance on original domain - the issue called “catastrophic forgetting” in continual learning literature - even with optimizer re-warming up. Current Achievements: We built the best open-source Hindi-English LLM of its size , which strongly outperforms all models of comparable sizes, such as Pythia 12B, LLaMa-2 and multilingual BLOOM models across various standard LLM Evaluation benchmark, including HellaSwag, ARC Reasoning Challenge and HellaSwag in the Hindi language. Our model extends ability to a new language while further improving performance on English and Code domains . Continual pre training also improved the performance of the model on English domain, where it already starts to significantly outperform a larger 12B parameter Pythia model and closes the gap with the 7B LLaMa-2 across the TruthfulQA, ARC Easy, ARC Challenge and HellaSwag benchmarks for English languages. Similar trend - i.e., improved coding abilities - was observed when using the HumanEval benchmark in Python programming language. Improved Code and English performance while extending to a new language: Continual pre training over pythia also improved performance in English domains of the model, where it starts to significantly outperform larger Pythia 12 Billion parameter model and closes the gap to LLaMa 2 model across the TruthfulQA, ARC Easy, ARC Challenge and HellaSwag for English languages. Similar trend of improved coding abilities is observed over the HumanEval benchmark in Python programming language. Model and Training Details We are currently still in the process of training Hi-NOLIN, and present below preliminary results on only 600B tokens with model no-where close to converging. The Hi-NOLIN’s network architecture was somewhat modified from the Pythia to ensure maximum training resource utilization over the unusual configuration of 6 GPUs per node (Summit supercomputer). We leverage several excellent pytorch open source projects of GPT-NeoX , Megatron-LM and DeepSpeed to train with 3D parallelism and ZeRO redundancy optimizer, utilizing 3072 GPUs across 512 nodes. We assume that the optimizer state after pre-training is not available for continual training, similar to prominent open-source models like LLaMa and Mistral. So we reinitialize it and consider the re-warming schedule of ( Gupta et al, 2023 . Our 9B model shows practically steady reduction in training loss, as well as in Hindi validation loss, with relatively low variance (loss spikes), and is clearly still far from convergence, promising further significant improvements - in contrast with the 410M model. Note that, while the validation loss for English initially increases, it eventually achieves far lower validation loss. Benchmarking We are using several standard LLM benchmarks, such as HellaSwag, TruthfulQA, Arc and Human Eval, for measuring the performance of Hi-NOLIN across Hindi, English and Code. Note that Hi-NOLIN is already showing better performance than Pythia 12B and multilingual Bloom at 600B across almost all evaluation benchmarks and closes the gap with LLaMa 2 models. Few-shot examples. ---- English: Given the multifaceted socio-cultural implications of the decision, it's imperative that we approach the issue with sensitivity and comprehensive understanding. Hindi: निर्णय के बहुमुखी सामाजिक-सांस्कृतिक प्रभावों को देखते हुए, यह अत्यंत आवश्यक है कि हम समस्या का समझदारी और समग्र समझ के साथ समाधान करें। English: In light of the recent paradigm shifts in the global economic landscape, businesses need to adapt proactively to remain competitive and relevant. --> --> Ramesh aur Sunita ek chhoti si gaon mein rehte the. Dono bachpan se hi dost the aur har chhuttiyon mein, woh sheher ke park mein ghoomne jaate the. Ek din, Ramesh ne ek purani diary mili jo uske dada ji ki thi. Diary mein, ek raaz chhupa hua tha. Dada ji ne likha tha ki gaon ke purane mandir mein ek khazana chhupa hua hai. Ramesh aur Sunita ne socha ki woh is raaz ko suljhaayenge. Dono ne map padha aur mandir ki or badhe. Bahut saare mushkilein aayi, lekin dono ne saath mein kaam kiya aur ant mein, khazana mil gaya. Lekin, woh khazana sona ya chandi nahi tha. Yeh ek purani kitaab thi jismein gaon ke itihaas aur sanskriti ke baare mein likha hua tha. Dono samjhe ki asli khazana yeh gyaan tha. Question based on the story: Q: Kya asli khazana tha jo Ramesh aur Sunita ne mandir mein paaya? A: Asli khazana woh purani kitaab thi jismein gaon ke itihaas aur sanskriti ke baare mein likha hua tha. ---- For the given Hinglish sentences, please generate English sentences Hinglish: Arrey, aaj raat ko movie pe chalna hai kya? English: Hey, do you want to go to the movies tonight? Hinglish: Mujhe aaj ka lesson samajh nahi aaya. ---- Find the location names (LOCATION) in the Hinglish text below: Sentence: Mumbai mein monsoon ka mausam bahut suhana hota hai. Location: Mumbai Sentence: Goa ki beaches par tourists kaafi aksar aate hain. Location: Goa --> Contact us? If you have questions or requirements related to the training, continual pretraining, or finetuning of large language models, or if you have any suggestions, we would be eager to hear from you. Please email us at hello@nolano.ai Join Our effort Nolano is actively seeking talented full-stack developers, both full-time and interns to join our growing team; if you’re interested, please reach out to us at hello@nolano.ai . Additionally, consider joining the INCITE AGI project Discord for collaboration and insights. Release: The model will be released under Apache 2.0 license at https://huggingface.co/nolanoAI/Hi-NOLIN-9B The Team Behind Hi-NOLIN Ayush Kaushal (Remote consultant at UdeM) Tejas Vaidhya (Student at UdeM and MILA ) Irina Rish (Prof at the University of Montreal) Acknowledgements The model’s training is conducted using 3,072 GPUs provided as a part of the INCITE compute grant on Summit supercomputer at the Oak Ridge Leadership Computing Facility (OLCF) , the DOE Office of Science User Facility supported under Contract DE-AC05-00OR22725. This grant was awarded to AAI CERC lab at Université de Montréal , LAION and EleutherAI for their Scalable Foundation Models for Transferrable Generalist AI project. We are grateful for the invaluable support provided to us by the OLCF leadership and by the OLCF liaison for the INCITE project. We are also grateful to the INCITE project team , and especially Kshitij Gupta and other contributors to the Continual Pretraining of Foundation Models project. Our work builds on top of this team’s work on continual pretraining of Pythia models. Many thanks to Quentin Anthony ( EleutherAI and INCITE project team ) for porting DeeperSpeed and the GPT-NeoX training framework to Summit, and providing insights on the design Pythia 9B model for maximizing training resource utilization Help Guide Us What Open Source Model Should We Train Next! References Biderman, S., et al. (2023). Pythia: A Suite for Analyzing Large Language Models Across Training and Scaling arXiv:2304.01373 . arXiv preprint arXiv:2304.01373. Gupta, K., et al. Continual Pre-Training of Large Language Models: How to (re)warm your model? arXiv:2308.04014 . arXiv preprint arXiv:2308.04014. Rozière, B., et al. Code Llama: Open Foundation Models for Code arXiv:2308.12950 . arXiv preprint arXiv:2308.12950. Rajbhandari, S., et al. (2020). ZeRO: Memory Optimizations Toward Training Trillion Parameter Models arXiv:1910.02054 . arXiv preprint arXiv:1910.02054. Andonian, A., et al. (2023). GPT-NeoX: Large Scale Autoregressive Language Modeling in PyTorch (Version 2.0.0) [Software]. https://doi.org/10.5281/zenodo.5879544 Gao, L., Tow, et al. A framework for few-shot language model evaluation (Version v0.0.1) [Software]. Zenodo. https://doi.org/10.5281/zenodo.5371628 BigScience Workshop, et al. (2023). BLOOM: A 176B-Parameter Open-Access Multilingual Language Model arXiv:2211.05100 . arXiv preprint arXiv:2211.05100. Touvron, H., et al. (2023). Llama 2: Open Foundation and Fine-Tuned Chat Models arXiv:2307.09288 . arXiv preprint arXiv:2307.09288. Plüster, B. (n.d.). LEOLM: Igniting German-Language LLM Research . LAION. Retrieved from https://laion.ai/blog/leo-lm/ Previous The Lord (Low Rank Decompos... Next Introducing Spectra Suite Please enable JavaScript to view the comments. &copy; 2025 Nolano Research. All rights reserved. ",
      "fetch_method": "direct-html"
    },
    {
      "arxiv_id": "https://stability.ai/blog/stablecode-llm-generative-ai-coding",
      "full_text": " Announcing Stable Code Alpha &mdash; Stability AI 0 Skip to Content Models Image Video Audio 3D Solutions Applications Dream Studio Stable Audio Deployment Self-Hosted License Platform API Cloud Platforms Resources Learning Hub Customer Stories Company Board of Directors News Partners Research Safety Careers Contact us Open Menu Close Menu Models Image Video Audio 3D Solutions Applications Dream Studio Stable Audio Deployment Self-Hosted License Platform API Cloud Platforms Resources Learning Hub Customer Stories Company Board of Directors News Partners Research Safety Careers Contact us Open Menu Close Menu Folder: Models Back Image Video Audio 3D Solutions Folder: Applications Back Dream Studio Stable Audio Folder: Deployment Back Self-Hosted License Platform API Cloud Platforms Folder: Resources Back Learning Hub Customer Stories Folder: Company Back Board of Directors News Partners Research Safety Careers Contact us Announcing Stable Code Alpha Product Aug 8 Written By Joshua Lopez Stability AI has just announced the release of Stable Code Alpha 3B, its very first LLM generative AI product for coding. This product is designed to assist programmers with their daily work while providing a great learning tool for new developers ready to take their skills to the next level. &nbsp;“a colorful parrot with glasses typing away at a computer, flat icon, vector” – SDXL 0.9 &nbsp; Stable Code Alpha offers a unique way for developers to become more efficient by using three different models to help in their coding. The base model was first trained on a diverse set of programming languages from the stack-dataset (v1.2) from BigCode and then trained further with popular languages like Python, Go, Java, Javascript, C, markdown, and C++.&nbsp; In total, we trained our models on 560B tokens of code on our HPC cluster.&nbsp; After the base model had been established, the instruction model was tuned for specific use cases to help solve complex programming tasks. ~120,000 code instruction/response pairs in Alpaca format were trained on the base model to achieve this result.&nbsp; Code for using StableCode Instruct to generate a response to a given instruction. Stable Code Alpha is the ideal building block for those wanting to learn more about coding, and the long-context window model is the perfect assistant to ensure single and multiple-line autocomplete suggestions are available for the user. This model is built to handle a lot more code at once (2-4X more than previously released open models with a context window of 16,000 tokens), allowing the user to review or edit the equivalent of up to five average-sized Python files at the same time, making it the ideal learning tool for a beginner who wants to rise to bigger challenges. Stable Code Alpha completing a relatively complex python file utilizing the Pytorch deep learning library (gray text shows StableCode’s prediction). And here is how we compare to other models of a similar number of parameters and number of tokens trained on. We use the standard pass@1 and pass@10 metrics using the popular HumanEval benchmark. benchmark scores of stablecode. HumanEval Benchmark Comparison with models of similar size(3B). Stability AI aims to make technology more accessible, and Stable Code Alpha is a significant step toward this goal. People of every background will soon be able to create code to solve their everyday problems and improve their lives using AI, and we’d like to help make this happen. We hope that Stable Code Alpha will help the next billion software developers learn to code while providing fairer access to technology all over the world. Updated 16/01/24 Joshua Lopez Previous Previous Japanese StableLM, Marking Entry into International Language Model&nbsp; Market Next Next Announcing SDXL 1.0 Stability AI is unlocking the power of open-source generative AI to expand human creativity. We build world-class models that are accessible, adaptable, and designed to empower creators, developers, and enterprises everywhere. Company Board of Directors Partners Safety Research Careers News Brand Resources Models Image Video Audio 3D Deployment Stability AI License Community License Agreement Platform API Cloud Platforms ResourceS Learning Hub Customer Stories Contact Us press@stability.ai partners@stability.ai Submit a Support Request Legal Acceptable Use Policy Privacy Policy Terms of Service Security Your Privacy Choices Applications Dream Studio Stable Audio Join the Mailing List © Stability AI Ltd, 2025 end Plugin Lightbox --> ",
      "fetch_method": "direct-html"
    },
    {
      "arxiv_id": "https://arxiv.org/abs/2310.10631",
      "full_text": " [2310.10631] Llemma: An Open Language Model For Mathematics Skip to main content We gratefully acknowledge support from the Simons Foundation, member institutions , and all contributors. Donate &gt; cs &gt; arXiv:2310.10631 Help | Advanced Search All fields Title Author Abstract Comments Journal reference ACM classification MSC classification Report number arXiv identifier DOI ORCID arXiv author ID Help pages Full text Search open search GO open navigation menu quick links Login Help Pages About --> Computer Science > Computation and Language arXiv:2310.10631 (cs) [Submitted on 16 Oct 2023 ( v1 ), last revised 15 Mar 2024 (this version, v3)] Title: Llemma: An Open Language Model For Mathematics Authors: Zhangir Azerbayev , Hailey Schoelkopf , Keiran Paster , Marco Dos Santos , Stephen McAleer , Albert Q. Jiang , Jia Deng , Stella Biderman , Sean Welleck View a PDF of the paper titled Llemma: An Open Language Model For Mathematics, by Zhangir Azerbayev and Hailey Schoelkopf and Keiran Paster and Marco Dos Santos and Stephen McAleer and Albert Q. Jiang and Jia Deng and Stella Biderman and Sean Welleck View PDF HTML (experimental) Abstract: We present Llemma, a large language model for mathematics. We continue pretraining Code Llama on the Proof-Pile-2, a mixture of scientific papers, web data containing mathematics, and mathematical code, yielding Llemma. On the MATH benchmark Llemma outperforms all known open base models, as well as the unreleased Minerva model suite on an equi-parameter basis. Moreover, Llemma is capable of tool use and formal theorem proving without any further finetuning. We openly release all artifacts, including 7 billion and 34 billion parameter models, the Proof-Pile-2, and code to replicate our experiments. Comments: Updated references; corrected description of COPRA search budget Subjects: Computation and Language (cs.CL) ; Artificial Intelligence (cs.AI); Logic in Computer Science (cs.LO) Cite as: arXiv:2310.10631 [cs.CL] &nbsp; (or arXiv:2310.10631v3 [cs.CL] for this version) &nbsp; https://doi.org/10.48550/arXiv.2310.10631 Focus to learn more arXiv-issued DOI via DataCite Submission history From: Zhangir Azerbayev Mr [ view email ] [v1] Mon, 16 Oct 2023 17:54:07 UTC (386 KB) [v2] Fri, 1 Dec 2023 03:51:33 UTC (116 KB) [v3] Fri, 15 Mar 2024 19:14:39 UTC (116 KB) Full-text links: Access Paper: View a PDF of the paper titled Llemma: An Open Language Model For Mathematics, by Zhangir Azerbayev and Hailey Schoelkopf and Keiran Paster and Marco Dos Santos and Stephen McAleer and Albert Q. Jiang and Jia Deng and Stella Biderman and Sean Welleck View PDF HTML (experimental) TeX Source Other Formats view license Current browse context: cs.CL &lt;&nbsp;prev &nbsp; | &nbsp; next&nbsp;&gt; new | recent | 2023-10 Change to browse by: cs cs.AI cs.LO References &amp; Citations NASA ADS Google Scholar Semantic Scholar a export BibTeX citation Loading... BibTeX formatted citation &times; loading... Data provided by: Bookmark Bibliographic Tools Bibliographic and Citation Tools Bibliographic Explorer Toggle Bibliographic Explorer ( What is the Explorer? ) Connected Papers Toggle Connected Papers ( What is Connected Papers? ) Litmaps Toggle Litmaps ( What is Litmaps? ) scite.ai Toggle scite Smart Citations ( What are Smart Citations? ) Code, Data, Media Code, Data and Media Associated with this Article alphaXiv Toggle alphaXiv ( What is alphaXiv? ) Links to Code Toggle CatalyzeX Code Finder for Papers ( What is CatalyzeX? ) DagsHub Toggle DagsHub ( What is DagsHub? ) GotitPub Toggle Gotit.pub ( What is GotitPub? ) Huggingface Toggle Hugging Face ( What is Huggingface? ) Links to Code Toggle Papers with Code ( What is Papers with Code? ) ScienceCast Toggle ScienceCast ( What is ScienceCast? ) Demos Demos Replicate Toggle Replicate ( What is Replicate? ) Spaces Toggle Hugging Face Spaces ( What is Spaces? ) Spaces Toggle TXYZ.AI ( What is TXYZ.AI? ) Related Papers Recommenders and Search Tools Link to Influence Flower Influence Flower ( What are Influence Flowers? ) Core recommender toggle CORE Recommender ( What is CORE? ) Author Venue Institution Topic About arXivLabs arXivLabs: experimental projects with community collaborators arXivLabs is a framework that allows collaborators to develop and share new arXiv features directly on our website. Both individuals and organizations that work with arXivLabs have embraced and accepted our values of openness, community, excellence, and user data privacy. arXiv is committed to these values and only works with partners that adhere to them. Have an idea for a project that will add value for arXiv's community? Learn more about arXivLabs . Which authors of this paper are endorsers? | Disable MathJax ( What is MathJax? ) About Help contact arXiv Click here to contact arXiv Contact subscribe to arXiv mailings Click here to subscribe Subscribe Copyright Privacy Policy Web Accessibility Assistance arXiv Operational Status Get status notifications via email or slack ",
      "fetch_method": "direct-html"
    },
    {
      "arxiv_id": "https://arxiv.org/abs/2406.12428",
      "full_text": " [2406.12428] PSLM: Parallel Generation of Text and Speech with LLMs for Low-Latency Spoken Dialogue Systems arXiv Is Hiring a DevOps Engineer Work on one of the world's most important websites and make an impact on open science. View Jobs Skip to main content arXiv Is Hiring a DevOps Engineer View Jobs We gratefully acknowledge support from the Simons Foundation, member institutions , and all contributors. Donate &gt; cs &gt; arXiv:2406.12428 Help | Advanced Search All fields Title Author Abstract Comments Journal reference ACM classification MSC classification Report number arXiv identifier DOI ORCID arXiv author ID Help pages Full text Search open search GO open navigation menu quick links Login Help Pages About --> Computer Science > Computation and Language arXiv:2406.12428 (cs) [Submitted on 18 Jun 2024 ( v1 ), last revised 3 Oct 2024 (this version, v2)] Title: PSLM: Parallel Generation of Text and Speech with LLMs for Low-Latency Spoken Dialogue Systems Authors: Kentaro Mitsui , Koh Mitsuda , Toshiaki Wakatsuki , Yukiya Hono , Kei Sawada View a PDF of the paper titled PSLM: Parallel Generation of Text and Speech with LLMs for Low-Latency Spoken Dialogue Systems, by Kentaro Mitsui and 4 other authors View PDF HTML (experimental) Abstract: Multimodal language models that process both text and speech have a potential for applications in spoken dialogue systems. However, current models face two major challenges in response generation latency: (1) generating a spoken response requires the prior generation of a written response, and (2) speech sequences are significantly longer than text sequences. This study addresses these issues by extending the input and output sequences of the language model to support the parallel generation of text and speech. Our experiments on spoken question answering tasks demonstrate that our approach improves latency while maintaining the quality of response content. Additionally, we show that latency can be further reduced by generating speech in multiple sequences. Demo samples are available at this https URL . Comments: 9 pages, 6 figures, 4 tables, accepted for Findings of EMNLP 2024. Demo samples: this https URL Subjects: Computation and Language (cs.CL) ; Artificial Intelligence (cs.AI); Machine Learning (cs.LG); Sound (cs.SD); Audio and Speech Processing (eess.AS) Cite as: arXiv:2406.12428 [cs.CL] &nbsp; (or arXiv:2406.12428v2 [cs.CL] for this version) &nbsp; https://doi.org/10.48550/arXiv.2406.12428 Focus to learn more arXiv-issued DOI via DataCite Submission history From: Kentaro Mitsui [ view email ] [v1] Tue, 18 Jun 2024 09:23:54 UTC (219 KB) [v2] Thu, 3 Oct 2024 05:17:25 UTC (253 KB) Full-text links: Access Paper: View a PDF of the paper titled PSLM: Parallel Generation of Text and Speech with LLMs for Low-Latency Spoken Dialogue Systems, by Kentaro Mitsui and 4 other authors View PDF HTML (experimental) TeX Source Other Formats view license Current browse context: cs.CL &lt;&nbsp;prev &nbsp; | &nbsp; next&nbsp;&gt; new | recent | 2024-06 Change to browse by: cs cs.AI cs.LG cs.SD eess eess.AS References &amp; Citations NASA ADS Google Scholar Semantic Scholar a export BibTeX citation Loading... BibTeX formatted citation &times; loading... Data provided by: Bookmark Bibliographic Tools Bibliographic and Citation Tools Bibliographic Explorer Toggle Bibliographic Explorer ( What is the Explorer? ) Connected Papers Toggle Connected Papers ( What is Connected Papers? ) Litmaps Toggle Litmaps ( What is Litmaps? ) scite.ai Toggle scite Smart Citations ( What are Smart Citations? ) Code, Data, Media Code, Data and Media Associated with this Article alphaXiv Toggle alphaXiv ( What is alphaXiv? ) Links to Code Toggle CatalyzeX Code Finder for Papers ( What is CatalyzeX? ) DagsHub Toggle DagsHub ( What is DagsHub? ) GotitPub Toggle Gotit.pub ( What is GotitPub? ) Huggingface Toggle Hugging Face ( What is Huggingface? ) Links to Code Toggle Papers with Code ( What is Papers with Code? ) ScienceCast Toggle ScienceCast ( What is ScienceCast? ) Demos Demos Replicate Toggle Replicate ( What is Replicate? ) Spaces Toggle Hugging Face Spaces ( What is Spaces? ) Spaces Toggle TXYZ.AI ( What is TXYZ.AI? ) Related Papers Recommenders and Search Tools Link to Influence Flower Influence Flower ( What are Influence Flowers? ) Core recommender toggle CORE Recommender ( What is CORE? ) Author Venue Institution Topic About arXivLabs arXivLabs: experimental projects with community collaborators arXivLabs is a framework that allows collaborators to develop and share new arXiv features directly on our website. Both individuals and organizations that work with arXivLabs have embraced and accepted our values of openness, community, excellence, and user data privacy. arXiv is committed to these values and only works with partners that adhere to them. Have an idea for a project that will add value for arXiv's community? Learn more about arXivLabs . Which authors of this paper are endorsers? | Disable MathJax ( What is MathJax? ) About Help contact arXiv Click here to contact arXiv Contact subscribe to arXiv mailings Click here to subscribe Subscribe Copyright Privacy Policy Web Accessibility Assistance arXiv Operational Status Get status notifications via email or slack ",
      "fetch_method": "direct-html"
    },
    {
      "arxiv_id": "https://r.jina.ai/https://arxiv.org/abs/2204.06745},",
      "full_text": "Title: [2204.06745},] Article identifier not recognized\n\nURL Source: https://arxiv.org/abs/2204.06745%7D,\n\nWarning: Target URL returned error 404: Not Found\n\nMarkdown Content:\n[2204.06745},] Article identifier not recognized\n\n===============\n\n[Skip to main content](https://arxiv.org/abs/2204.06745%7D,#content)\n\n[![Image 1: Cornell University Logo](https://arxiv.org/static/browse/0.3.4/images/icons/cu/cornell-reduced-white-SMALL.svg)](https://www.cornell.edu/)\n\nWe gratefully acknowledge support from the Simons Foundation, [member institutions](https://info.arxiv.org/about/ourmembers.html), and all contributors.[Donate](https://info.arxiv.org/about/donate.html)\n\n[](https://arxiv.org/IgnoreMe)\n![Image 2: arxiv logo](https://arxiv.org/static/browse/0.3.4/images/arxiv-logo-one-color-white.svg)\n===================================================================================================\n\n[Help](https://info.arxiv.org/help) | [Advanced Search](https://arxiv.org/search/advanced)\n\nSearch\n\n[![Image 3: arXiv logo](https://arxiv.org/static/browse/0.3.4/images/arxiv-logomark-small-white.svg)](https://arxiv.org/)\n\n[![Image 4: Cornell University Logo](https://arxiv.org/static/browse/0.3.4/images/icons/cu/cornell-reduced-white-SMALL.svg)](https://www.cornell.edu/)\n\nGO\n\nquick links\n-----------\n\n*   [Login](https://arxiv.org/login)\n*   [Help Pages](https://info.arxiv.org/help)\n*   [About](https://info.arxiv.org/about)\n\nArticle identifier '2204.06745},' not recognized\n================================================\n\nYou might instead try to [search for articles](https://arxiv.org/search) using title or author information.\n\nFor additional help on arXiv identifiers, see [understanding the arXiv identifier](https://info.arxiv.org/help/arxiv_identifier.html).\n\n*   [About](https://info.arxiv.org/about)\n*   [Help](https://info.arxiv.org/help)\n\n*   [Contact](https://info.arxiv.org/help/contact.html)\n*   [Subscribe](https://info.arxiv.org/help/subscribe)\n\n*   [Copyright](https://info.arxiv.org/help/license/index.html)\n*   [Privacy Policy](https://info.arxiv.org/help/policies/privacy_policy.html)\n\n*   [Web Accessibility Assistance](https://info.arxiv.org/help/web_accessibility.html)\n*   [arXiv Operational Status](https://status.arxiv.org/)\n\n Get status notifications via [email](https://subscribe.sorryapp.com/24846f03/email/new) or [slack](https://subscribe.sorryapp.com/24846f03/slack/new)\n",
      "fetch_method": "jina-reader"
    }
  ]
}