{
  "1-1 (Weights)": "The quotes show that the weights for the model identified exactly as \"cyberagent/open-calm-7b\" are openly downloadable from the Hugging Face Hub.  A code snippet demonstrates the standard loading call:  \"model = AutoModelForCausalLM.from_pretrained(\\\"cyberagent/open-calm-7b\\\", device_map=\\\"auto\\\", torch_dtype=torch.float16)\", indicating that anyone with the transformers library can retrieve them directly.  The repository exposes two sharded binary checkpoints, explicitly listed as \"pytorch_model-00001-of-00002.bin\" and \"pytorch_model-00002-of-00002.bin\".  No authentication token, EULA gate, or other access restriction is mentioned in any quote, so the evidence points to freely accessible, public weights hosted in standard Hugging-Face format.",
  "1-1 (Weights)__evidence": [
    {
      "source": "[readme]",
      "quote": "model = AutoModelForCausalLM.from_pretrained(\"cyberagent/open-calm-7b\", device_map=\"auto\", torch_dtype=torch.float16)"
    },
    {
      "source": "[files]",
      "quote": "pytorch_model-00001-of-00002.bin"
    },
    {
      "source": "[files]",
      "quote": "pytorch_model-00002-of-00002.bin"
    }
  ],
  "1-2 (Code)": "None of the supplied quotes contain any sentence that mentions publicly released *training* code, pre-training scripts, fine-tuning pipelines, or reinforcement-learning stages for cyberagent/open-calm-7b.  Because no quote addresses code availability, no statement can be made—based on the provided material—about whether such code is public or private.",
  "1-2 (Code)__evidence": [],
  "1-3 (License)": "Multiple sentences refer consistently to the same license.  The exact quoted phrases are:\n* \"* **License**: OpenCALM is licensed under the Creative Commons Attribution-ShareAlike 4.0 International License ([CC BY-SA 4.0](https://creativecommons.org/licenses/by-sa/4.0/)). When using this model, please provide appropriate credit to CyberAgent, Inc.\"\n* \"license: cc-by-sa-4.0\"\n* \"OpenCALM is licensed under the Creative Commons Attribution-ShareAlike 4.0 International License ([CC BY-SA 4.0](https://creativecommons.org/licenses/by-sa/4.0/)). When using this model, please provide appro…\"\n* \"This model is a fine-tuned version of OpenCALM-XX developed by CyberAgent, Inc. The original model is released under the CC BY-SA 4.0 license, and this model is also released under the same CC BY-SA 4.0 license.\"\nTogether these excerpts confirm that cyberagent/open-calm-7b is governed by the Creative Commons Attribution-ShareAlike 4.0 International (CC-BY-SA 4.0) license.  That license grants broad rights to use, modify, and redistribute—even commercially—provided users (1) give \"appropriate credit to CyberAgent, Inc.\" and (2) distribute any derivative works under the same CC-BY-SA 4.0 terms (the share-alike clause).  No other specific restriction or additional EULA language appears in the quotes.",
  "1-3 (License)__evidence": [
    {
      "source": "[readme]",
      "quote": "* **License**: OpenCALM is licensed under the Creative Commons Attribution-ShareAlike 4.0 International License ([CC BY-SA 4.0](https://creativecommons.org/licenses/by-sa/4.0/)). When using this model, please provide appropriate credit to CyberAgent, Inc."
    },
    {
      "source": "[readme]",
      "quote": "license: cc-by-sa-4.0"
    },
    {
      "source": "[readme]",
      "quote": "[readme]\n---\nlicense: cc-by-sa-4.0\ndatasets:\n- wikipedia\n- cc100\n- mc4\nlanguage:\n- ja\ntags:\n- japanese\n- causal-lm\ninference: false\n---\n# OpenCALM-7B\n\n## Model Description\n\nOpenCALM is a sui"
    },
    {
      "source": "[readme]",
      "quote": "agent.co.jp/)\n* **Model type**: Transformer-based Language Model\n* **Language**: Japanese\n* **Library**: [GPT-NeoX](https://github.com/EleutherAI/gpt-neox)\n* **License**: OpenCALM is licensed under the Creative Commons Attribution-ShareAlike 4.0 International License ([CC BY-SA 4.0](https://creativecommons.org/licenses/by-sa/4.0/)). When using this model, please provide appro"
    },
    {
      "source": "[readme]",
      "quote": "y**: [GPT-NeoX](https://github.com/EleutherAI/gpt-neox)\n* **License**: OpenCALM is licensed under the Creative Commons Attribution-ShareAlike 4.0 International License ([CC BY-SA 4.0](https://creativecommons.org/licenses/by-sa/4.0/)). When using this model, please provide appropriate credit to CyberAgent, Inc.\n * Example (en): This mod"
    },
    {
      "source": "[readme]",
      "quote": "t, Inc.\n * Example (en): This model is a fine-tuned version of OpenCALM-XX developed by CyberAgent, Inc. The original model is released under the CC BY-SA 4.0 license, and this model is also released under the same CC BY-SA 4.0 license. For more information, please visit: https://creativecommons.org/licenses/by-sa/4.0/\n * Example (ja): 本モデルは、株式会社サイバーエージェントによるOpenCALM-XXをファインチューニン"
    }
  ],
  "1-4 (Paper)": "The quote set contains no sentence that cites a formal paper, technical report, blog post, or other publication explicitly tied to cyberagent/open-calm-7b, so no bibliographic or URL information can be summarized.",
  "1-4 (Paper)__evidence": [],
  "1-5 (Architecture)": "The publicly-shared configuration snippets for cyberagent/open-calm-7b give a fairly complete picture of its core transformer layout. A markdown table row that explicitly names the checkpoint — “|[cyberagent/open-calm-7b](https://huggingface.co/cyberagent/open-calm-7b)|6.8B|32|4096|32|8.2|” — states that the model contains roughly 6.8 billion trainable parameters spread across 32 transformer blocks. The same row also lists a model dimension of 4096 and shows that each block employs 32 self-attention heads. Immediately below, the exported Hugging Face configuration affirms these figures in machine-readable form: \"architectures\": [\"GPTNeoXForCausalLM\"] declares that the checkpoint instantiates the GPT-NeoX implementation, while \"hidden_size\": 4096, \"num_hidden_layers\": 32, and \"num_attention_heads\": 32 repeat the width, depth and head count. The feed-forward network width is recorded as \"intermediate_size\": 16384, implying a 4× expansion over the hidden size (4096 → 16384) inside the MLP. Taken together, the quotes show that open-calm-7b is a 6.8 B-parameter GPT-NeoX-style decoder-only model with 32 layers, 4096-wide representations, 32 attention heads per layer, and 16 384-wide intermediate MLP projections, and it is served through the GPTNeoXForCausalLM wrapper on the Hugging Face Hub.",
  "1-5 (Architecture)__evidence": [
    {
      "source": "[readme]",
      "quote": "|[cyberagent/open-calm-7b](https://huggingface.co/cyberagent/open-calm-7b)|6.8B|32|4096|32|8.2|"
    },
    {
      "source": "[config]",
      "quote": "\"architectures\": [\n    \"GPTNeoXForCausalLM\"\n  ]"
    },
    {
      "source": "[config]",
      "quote": "\"hidden_size\": 4096,"
    },
    {
      "source": "[config]",
      "quote": "\"num_hidden_layers\": 32,"
    },
    {
      "source": "[config]",
      "quote": "\"num_attention_heads\": 32,"
    },
    {
      "source": "[config]",
      "quote": "\"intermediate_size\": 16384,"
    }
  ],
  "1-6 (Tokenizer)": "All tokenizer information that explicitly mentions cyberagent/open-calm-7b is concentrated in the provided code and config fragments. The example call, `tokenizer = AutoTokenizer.from_pretrained(\"cyberagent/open-calm-7b\")`, confirms that the model ships with a ready-to-download tokenizer hosted on the same Hugging Face repository. The tokenizer metadata lists \"bos_token_id\": 0 and \"eos_token_id\": 0, meaning that both the beginning-of-sequence and end-of-sequence markers share the same integer ID. The vocabulary size is given as \"vocab_size\": 52224, so the tokenizer can emit up to 52 224 unique token IDs. Two asset filenames are also enumerated — “tokenizer.json” and “tokenizer_config.json” — indicating that the full tokenization schema and configuration can be obtained in standard Hugging Face formats alongside the model weights.",
  "1-6 (Tokenizer)__evidence": [
    {
      "source": "[readme]",
      "quote": "tokenizer = AutoTokenizer.from_pretrained(\"cyberagent/open-calm-7b\")"
    },
    {
      "source": "[config]",
      "quote": "\"bos_token_id\": 0,"
    },
    {
      "source": "[config]",
      "quote": "\"eos_token_id\": 0,"
    },
    {
      "source": "[config]",
      "quote": "\"vocab_size\": 52224"
    },
    {
      "source": "[files]",
      "quote": "tokenizer.json"
    },
    {
      "source": "[files]",
      "quote": "tokenizer_config.json"
    }
  ],
  "2-1 (Hardware)": "No sentences in the supplied quote set mention GPUs, TPUs, node counts, total PF-days, or any other details about the physical compute used to train cyberagent/open-calm-7b. Consequently, the provided material does not contain hardware information for this model.",
  "2-1 (Hardware)__evidence": [],
  "2-2 (Software)": "The quotes provide a concise but explicit snapshot of the software stack associated with cyberagent/open-calm-7b. A bullet line identifies the primary training framework as “* **Library**: [GPT-NeoX](https://github.com/EleutherAI/gpt-neox)”, showing that the model was built with EleutherAI’s GPT-NeoX codebase. The exported configuration also pins a Transformers library version: \"transformers_version\": \"4.27.0.dev0\", indicating that the checkpoint expects or was produced with a development build around the 4.27 release cycle. Finally, an import snippet — `from transformers import AutoModelForCausalLM, AutoTokenizer` — illustrates the standard Hugging Face runtime API used to load both the weights and tokenizer. Together these lines confirm that GPT-NeoX was employed during training and that inference is intended to occur through Hugging Face Transformers ≥ 4.27.0.dev0.",
  "2-2 (Software)__evidence": [
    {
      "source": "[readme]",
      "quote": "* **Library**: [GPT-NeoX](https://github.com/EleutherAI/gpt-neox)"
    },
    {
      "source": "[config]",
      "quote": "\"transformers_version\": \"4.27.0.dev0\""
    },
    {
      "source": "[readme]",
      "quote": "from transformers import AutoModelForCausalLM, AutoTokenizer"
    }
  ],
  "2-3 (API)": "The available material shows that CyberAgent provides a straightforward Hugging Face–style interface for interacting with the cyberagent/open-calm-7b model. In the quoted example, users invoke AutoModelForCausalLM.from_pretrained(\"cyberagent/open-calm-7b\", device_map=\"auto\", torch_dtype=torch.float16), demonstrating that the model weights can be pulled directly from the Hugging Face Hub, instantiated as a causal-language-model object, and automatically placed across any available hardware through the device_map=\"auto\" flag while running in half-precision (FP16). A complementary call, tokenizer = AutoTokenizer.from_pretrained(\"cyberagent/open-calm-7b\"), confirms that the matching tokenizer is likewise hosted and can be fetched in a single line. Together, these two code snippets constitute the entirety of publicly documented API information: the model is accessible via the standard Hugging Face Transformers API, supports automatic device placement, and can be used out-of-the-box with an accompanying tokenizer.",
  "2-3 (API)__evidence": [
    {
      "source": "[readme]",
      "quote": "model = AutoModelForCausalLM.from_pretrained(\"cyberagent/open-calm-7b\", device_map=\"auto\", torch_dtype=torch.float16)"
    },
    {
      "source": "[readme]",
      "quote": "tokenizer = AutoTokenizer.from_pretrained(\"cyberagent/open-calm-7b\")"
    }
  ],
  "3-1 (Pre-training)": "The single explicit statement on pre-training explains that OpenCALM is a suite of decoder-only language models that have been pre-trained on Japanese datasets and that the effort was carried out by CyberAgent, Inc. From this, we learn that the foundational architecture for the 7-billion-parameter variant (open-calm-7b) is decoder-only, implying an autoregressive transformer design tailored for next-token prediction. The quote further clarifies that large-scale Japanese corpora form the core of the pre-training data, positioning the model for strong performance in Japanese language tasks. No additional hyper-parameters, dataset sizes, or optimization details are provided in the cited material.",
  "3-1 (Pre-training)__evidence": [
    {
      "source": "[readme]",
      "quote": "OpenCALM is a suite of decoder-only language models pre-trained on Japanese datasets, developed by CyberAgent, Inc."
    }
  ],
  "3-2 (Fine-tuning)": "Fine-tuning information is limited to one illustrative sentence: \"This model is a fine-tuned version of OpenCALM-XX developed by CyberAgent, Inc.\" This wording indicates that CyberAgent supports derivative models that start from a base OpenCALM checkpoint (e.g., open-calm-7b) and then undergo task- or domain-specific adaptation. The use of the generic placeholder \"XX\" hints at multiple possible fine-tuned variants within the OpenCALM family. Apart from confirming the availability of fine-tuned offspring, no further methodological details—such as training objectives, datasets, or hyper-parameters—are supplied in the provided source.",
  "3-2 (Fine-tuning)__evidence": [
    {
      "source": "[readme]",
      "quote": "Example (en): This model is a fine-tuned version of OpenCALM-XX developed by CyberAgent, Inc."
    }
  ],
  "3-3 (Reinforcement Learning)": "The supplied corpus contains no sentences that reference reinforcement learning, RLHF, PPO, DPO, or any experimental details of that nature for the cyberagent/open-calm-7b model. Consequently, no information is available on whether CyberAgent applies reinforcement-learning-based alignment techniques to this model.",
  "3-3 (Reinforcement Learning)__evidence": [],
  "4-1 (Pre-training Data)": "cyberagent/open-calm-7b, part of the OpenCALM suite, is a decoder-only language model that was pre-trained on Japanese datasets and developed by CyberAgent, Inc.",
  "4-1 (Pre-training Data)__evidence": [
    {
      "source": "[readme]",
      "quote": "OpenCALM is a suite of decoder-only language models pre-trained on Japanese datasets, developed by CyberAgent, Inc."
    }
  ],
  "4-2 (Fine-tuning Data)": "",
  "4-2 (Fine-tuning Data)__evidence": [],
  "4-3 (Reinforcement Learning Data)": "",
  "4-3 (Reinforcement Learning Data)__evidence": [],
  "4-4 (Data Filtering)": "",
  "4-4 (Data Filtering)__evidence": [],
  "__usage": {
    "fine_tuning": "used",
    "rl": "unknown"
  }
}