{
  "4-1 (Pre-training Data)": "The only details given about pre-training come from two statements that both cite the 7 B-parameter variant. They say the model is part of the first release of systems trained \"on the RedPajama base dataset\" and that the goal is to \"replicate the LLaMA recipe as closely as possible.\" One sentence further quantifies the scale, noting that \"RedPajama-INCITE-Base-7B-v0.1 is a base model trained over 800 B tokens.\" Thus, the available picture is: (1) data source = the RedPajama base corpus; (2) motivation = mimic the original LLaMA data makeup; (3) volume = roughly 800 billion tokens used for the 7 B model. No finer-grained breakdown of domains, licenses, or geographic distribution is supplied in the provided material.",
  "4-2 (Fine-tuning Data)": "Two sentences describe the 7 B fine-tuned descendants of the base checkpoint. First, \"RedPajama-INCITE-Chat-7B-v0.1\" is said to be \"trained over Dolly 2.0 and Open Assistant,\" identifying both the instruction-following Dolly corpus and the community-curated Open Assistant dialogs as the data sources for its chat specialization. Second, an \"Instruct\" variant—\"RedPajama-INCITE-Instruct-7B-v0.1\"—is noted as being \"instruction tuned for few-shot applications,\" implying an objective of general-purpose task instruction with an emphasis on maintaining few-shot capability. Other aspects—such as exact example counts, licenses, or public availability—are not disclosed within the supplied quotations.",
  "4-3 (Reinforcement Learning Data)": "The provided quotations contain no references to any reinforcement-learning-style reward modeling, preference data, or RLHF/RLAIF pipelines. Consequently, there is no information available about sources, composition, or accessibility of any RL datasets for this model family.",
  "4-4 (Data Filtering)": "Only one passage touches on filtering. In the context of \"RedPajama-INCITE-Instruct-7B-v0.1,\" the authors state: \"We follow the recipe for GPT-JT but eliminate all datasets that overlap with the HELM benchmark.\" This indicates a two-step policy. First, they reuse the general GPT-JT curation procedure (details not repeated here). Second—and more concretely—they perform a hard exclusion of every dataset that duplicates content found in the HELM evaluation benchmark, thereby preventing evaluation leakage. No numerical thresholds, classifier names, or additional cleaning heuristics are mentioned beyond this overlap-based removal rule.",
  "4-1 (Pre-training Data)__evidence": [
    {
      "source": "[sections/https://together.ai/blog/redpajama-models-v1]",
      "quote": "Todayâs release includes our first models trained on the RedPajama base dataset: a 3 billion and a 7B parameter base model that aims to replicate the LLaMA recipe as closely as possible."
    },
    {
      "source": "[sections/https://together.ai/blog/redpajama-models-v1]",
      "quote": "RedPajama-INCITE-Base-7B-v0.1 is a base model trained over 800B tokens"
    }
  ],
  "4-2 (Fine-tuning Data)__evidence": [
    {
      "source": "[sections/https://together.ai/blog/redpajama-models-v1]",
      "quote": "RedPajama-INCITE-Chat-7B-v0.1 is its chat counterpart trained over Dolly 2.0 and Open Assistant"
    },
    {
      "source": "[sections/https://together.ai/blog/redpajama-models-v1]",
      "quote": "RedPajama-INCITE-Instruct-7B-v0.1 is instruction tuned for few-shot applications."
    }
  ],
  "4-3 (Reinforcement Learning Data)__evidence": [],
  "4-4 (Data Filtering)__evidence": [
    {
      "source": "[sections/https://together.ai/blog/redpajama-models-v1]",
      "quote": "RedPajama-INCITE-Instruct-7B-v0.1 is instruction tuned for few-shot applications. We follow the recipe for GPT-JT but eliminate all datasets that overlap with the HELM benchmark."
    }
  ]
}