{
  "4-1 (Pre-training Data)": "The available passages state that the Kanana family – explicitly including the 2.1 B-parameter variant – is trained on a very large-scale corpus of “3 trillion tokens.”  The overall design goal is bilingual strength, so the data mix is deliberately concentrated on English and Korean text.  The authors list the concrete source categories that make up this corpus: “English web, Korean web, academic, code, encyclopedic documents, and instruction data.”  In addition, the paper’s Table 5 compares “token consumption and performance” results for two alternative training strategies – pruning-and-distillation from earlier checkpoints versus training from scratch – and notes that both options employ “the same 2.1 B architecture,” confirming that the pre-training numbers and experiments all apply directly to the kanana-nano-2.1 B base model.",
  "4-2 (Fine-tuning Data)": "After the base Kanana model has been pre-trained, the authors describe an additional stage in which they build “instruction-tuned models for direct interaction by natural language.”  They say this layer is “building on Kanana pre-trained models,” and that Section 3.2 of their paper “presents the details of the specifics regarding the Supervised Fine-Tuning (SFT) and preference datasets.”  Thus, the fine-tuning phase involves both an SFT corpus and a preference dataset, each designed to adapt the 2.1 B Kanana foundation to instruction-following use-cases, although the quotes do not enumerate the exact size or sources of those datasets.",
  "4-3 (Reinforcement Learning Data)": "",
  "4-4 (Data Filtering)": "The authors repeatedly emphasize that the kanana 2.1 B training pipeline includes “high quality data filtering” as a core ingredient of their compute-efficient recipe.  In the same sentence they list it alongside “staged pre-training, depth up-scaling, and pruning and distillation,” identifying it as one of the principal engineering steps applied before or during training.  Although the excerpts do not expose numerical thresholds or specific classifiers, they explicitly attribute the resulting bilingual performance of Kanana to this rigorous filtering stage, underscoring that careful cleaning was essential for achieving competitive quality with limited computational budget.",
  "4-1 (Pre-training Data)__evidence": [
    {
      "source": "[pdf_text]",
      "quote": "We train Kanana models on 3 trillion tokens, primarily focusing on English and Korean bilingual capabilities. We collect our corpora from various sources and categorize them as English web, Korean web, academic, code, encyclopedic documents, and instruction data."
    },
    {
      "source": "[pdf_text]",
      "quote": "Table 5: Token consumption and performance of pruning & distillation (PD) from preceding models and training from scratch. We use the same 2.1B architecture."
    },
    {
      "source": "[sections/Data]",
      "quote": "We train Kanana models on 3 trillion tokens, primarily focusing on English and Korean bilingual capabilities. We collect our corpora from various sources and categorize them as English web, Korean web, academic, code, encyclopedic documents, and instruction data."
    }
  ],
  "4-2 (Fine-tuning Data)__evidence": [
    {
      "source": "[pdf_text]",
      "quote": "Building on Kanana pre-trained models, we further develop instruction-tuned models for direct interaction by natural language. Section 3.2 presents the details of the specifics regarding the Supervised Fine-Tuning (SFT) and preference datasets."
    }
  ],
  "4-3 (Reinforcement Learning Data)__evidence": [],
  "4-4 (Data Filtering)__evidence": [
    {
      "source": "[abstract]",
      "quote": "We introduce Kanana, a series of bilingual language models ... The report details the techniques employed during pre-training to achieve compute-efficient yet competitive models, including high quality data filtering, staged pre-training, depth up-scaling, and pruning and distillation."
    },
    {
      "source": "[abstract]",
      "quote": "We introduce Kanana, a series of bilingual language models that demonstrate exceeding performance in Korean and competitive performance in English. The report details the techniques employed during pre-training to achieve compute-efficient yet competitive models, including high quality data filtering, staged pre-training, depth up-scaling, and pruning and distillation."
    }
  ]
}