{
  "4-1 (Pre-training Data)": "The only information given about the pre-training corpus for the Kanana family is that the developers pursued \"compute-efficient yet competitive\" training by applying several engineering techniques. The quote says that the report \"details the techniques employed during pre-training … including high quality data filtering, staged pre-training, depth up-scaling, and pruning and distillation.\" From this we can infer that (a) some form of data quality triage preceded training, (b) the material may have been processed in successive stages rather than in one pass, and (c) later architectural or pruning steps were used to lower computational overhead. However, the citation supplies no concrete figures (e.g., number of tokens, percentage retained, domain mix), no explicit data sources, and no licensing or usage-rights discussion. Consequently, the public documentation reveals that sophisticated filtering and curriculum-style staging occurred but leaves the precise make-up, scale, and provenance of the pre-training data undisclosed.",
  "4-2 (Fine-tuning Data)": "The documentation states that after pre-training, the Kanana models underwent \"post-training\" that \"encompass[es] supervised fine-tuning and preference optimization.\" This implies two distinct steps: (1) supervised fine-tuning on curated instruction–response or task-oriented datasets, and (2) a preference-based optimization stage (often RLHF or similar) intended to align model outputs with human judgments of quality or helpfulness. Beyond naming these methods, the quote provides no granular details—there is no disclosure of dataset titles, sizes, domain distributions, licensing status, or example contents. Therefore, the fine-tuning section confirms the existence of supervised and preference-based alignment phases but offers no specifics about the data itself.",
  "4-3 (Reinforcement Learning Data)": "The same sentence covering post-training specifies that the Kanana team performed \"preference optimization\" after supervised fine-tuning. Although the wording does not explicitly use the term \"reinforcement learning,\" in contemporary LLM pipelines preference optimization is normally implemented through reinforcement learning from human feedback (RLHF) or a related algorithm. Thus, the quote indicates that a reinforcement-style stage exists and is focused on user interaction quality, but discloses nothing about how the preference data were collected, how many preference pairs or rankings were used, whether annotators were internal or external, or how the data are stored or licensed. All we know is that such data were leveraged to refine the model’s ability for \"seamless interaction with users.\"",
  "4-4 (Data Filtering)": "The only direct mention of filtering practices appears in the pre-training discussion: the report \"details the techniques employed during pre-training … including high quality data filtering.\" No numeric thresholds, classifier names, duplicate-detection ratios, perplexity cut-offs, or percentage of data removed are supplied in the quote. However, the sentence places filtering as the first element in a pipeline that also lists \"staged pre-training, depth up-scaling, and pruning and distillation,\" implying that filtering was an early, possibly gating, step used to raise data quality before the workload-reduction and model-compression techniques took effect. In short, the developers highlight that rigorous filtering contributed to lower compute cost and competitive performance, but they withhold all concrete criteria or statistics about how the filtering was executed.",
  "4-1 (Pre-training Data)__evidence": [
    {
      "source": "[sections/https://arxiv.org/abs/2502.18934]",
      "quote": "The computational cost of Kanana is significantly lower than that of state-of-the-art models of similar size. The report details the techniques employed during pre-training to achieve compute-efficient yet competitive models, including high quality data filtering, staged pre-training, depth up-scaling, and pruning and distillation."
    }
  ],
  "4-2 (Fine-tuning Data)__evidence": [
    {
      "source": "[sections/https://arxiv.org/abs/2502.18934]",
      "quote": "Furthermore, the report outlines the methodologies utilized during the post-training of the Kanana models, encompassing supervised fine-tuning and preference optimization, aimed at enhancing their capability for seamless interaction with users."
    }
  ],
  "4-3 (Reinforcement Learning Data)__evidence": [
    {
      "source": "[sections/https://arxiv.org/abs/2502.18934]",
      "quote": "Furthermore, the report outlines the methodologies utilized during the post-training of the Kanana models, encompassing supervised fine-tuning and preference optimization, aimed at enhancing their capability for seamless interaction with users."
    }
  ],
  "4-4 (Data Filtering)__evidence": [
    {
      "source": "[sections/https://arxiv.org/abs/2502.18934]",
      "quote": "The computational cost of Kanana is significantly lower than that of state-of-the-art models of similar size. The report details the techniques employed during pre-training to achieve compute-efficient yet competitive models, including high quality data filtering, staged pre-training, depth up-scaling, and pruning and distillation."
    }
  ]
}