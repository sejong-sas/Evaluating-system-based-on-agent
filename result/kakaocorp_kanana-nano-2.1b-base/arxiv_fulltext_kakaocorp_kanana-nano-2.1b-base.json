{
  "model_id": "kakaocorp/kanana-nano-2.1b-base",
  "full_texts": [
    {
      "arxiv_id": "2502.18934",
      "full_text": "March 3, 2025\nKanana: Compute-efficient Bilingual Language Models\nKanana LLM Team ∗\nkanana-llm@kakaocorp.com\nhttps://huggingface.co/kakaocorp\nhttps://github.com/kakao/kanana\nAbstract\nWe introduce Kanana, a series of bilingual language models that demon-\nstrate exceeding performance in Korean and competitive performance in\nEnglish. The computational cost of Kanana is significantly lower than that\nof state-of-the-art models of similar size. The report details the techniques\nemployed during pre-training to achieve compute-efficient yet competitive\nmodels, including high quality data filtering, staged pre-training, depth\nup-scaling, and pruning and distillation. Furthermore, the report outlines\nthe methodologies utilized during the post-training of the Kanana models,\nencompassing supervised fine-tuning and preference optimization, aimed\nat enhancing their capability for seamless interaction with users. Lastly,\nthe report elaborates on plausible approaches used for language model\nadaptation to specific scenarios, such as embedding, retrieval augmented\ngeneration, and function calling. The Kanana model series spans from 2.1B\nto 32.5B parameters with 2.1B models (base, instruct, embedding) publicly\nreleased to promote research on Korean language models.\n1\nIntroduction\nRecent breakthroughs in large language models (LLMs) have been driven by increasing\ntraining data (Hoffmann et al., 2022) and model parameters (Brown et al., 2020; Kaplan et al.,\n2020; Chowdhery et al., 2023). However, advances have also introduced substantial compu-\ntational costs that reach millions of dollars (Grattafiori et al., 2024), which poses a challenge\nto the community on developing LLMs from scratch. As a result, reducing computational\ncost has emerged as a crucial problem in order to popularize the development of LLMs for\nboth academia and industry (Zhao et al., 2024; Fishman et al., 2025; Wang et al., 2025). To this\nend, recent works have presented various solutions to the computation problem in model\narchitectures and scaling (Shao et al., 2024a; Kim et al., 2024a; Muralidharan et al., 2024),\nthrough data (Penedo et al., 2024a; Sachdeva et al., 2024), and through training strategies\n(DeepSeek-AI, 2024; Hu et al., 2024).\nAs the product of our endeavor to address the computational challenges, we introduce\nKanana model family, developed using only a fraction of computational cost while maintain-\ning performance compared to those of the state-of-the-art (SOTA) open LLMs. The family\nof models includes pre-trained base model and post-trained instruction models in sizes of\n{2.1B, 9.8B, 32.5B}. We show in Figure 1 that Kanana models establish a new Pareto frontier\nin the computational cost of the train time versus the performance.\nIn the pre-training phase, as it accounts for the majority of the training costs for LLMs, we\nfocus on reducing its computational demands while maintaining performance. Since the\ncost of the pre-training phase primarily arises from the large dataset size and model scale,\nwe reduce it by improving both data efficiency and training efficiency. To improve data\nefficiency, we carefully curate a training dataset of 3 trillion tokens, enabling our models to\nachieve competitive performance despite using a smaller dataset than SOTA pre-trained\nmodels. For training efficiency, we employ cost-effective techniques such as staged pre-\ntraining (Hu et al., 2024; Ibrahim et al., 2024) and depth up-scaling (Kim et al., 2024a) to\nreduce computational costs associated with model size. From the models obtained, we\n∗A detailed contributor list can be found in the last section of the main paper.\n1\narXiv:2502.18934v3  [cs.CL]  28 Feb 2025\n\n1021\n1022\n1023\n1024\n1025\nApproximate Pre-training Compute (FLOPs)\n25\n30\n35\n40\n45\n50\n55\n60\n65\n70\n75\n80\nMMLU/KMMLU Avg Performance (%)\nKanana Flag 32.5B\nLlama 3.1 70B\nQwen2.5 32B\nGemma 2 27B\nEXAONE-3.5-32B\nKanana Essence 9.8B\nLlama 3.1 8B\nQwen2.5 7B\nGemma 2 9B\nEXAONE-3.5-7.8B\nKanana Nano 2.1B\nLlama 3.2 3B\nQwen2.5 3B\nGemma 2 2B\nEXAONE-3.5-2.4B\nQwen2.5 0.5B\nQwen2.5 1.5B\nLlama 3.2 1B\nOther models (  for instruct)\nKanana\nOriginal Pareto frontier\nKanana Pareto frontier\nFigure 1: Performance to pre-training computational cost for Kanana and comparable models. We\nmeasure computational cost in FLOPs (Floating Point Operations), which is approximately calculated\nas 6 × training tokens × model size (Kaplan et al., 2020). We only calculate student training FLOPs\nfor distillation models. Obviously, Kanana models achieves decent performance given their limited\ncomputational cost.\nextend pruning and distillation technique (Muralidharan et al., 2024) to train smaller models\nusing only a handful subset of the pre-training data.\nLeveraging the strong performances of Kanana base models, we further develop instruction\nand domain-specific adaptation models. To develop instruction models, we apply a post-\ntraining process that includes supervised fine-tuning and preference optimization. As a\nresult, our instruction models achieve competitive performance to that of SOTA models on\nvarious tasks, including English/Korean chat, general knowledge reasoning, instruction\nfollowing, code generation, and mathematical problem-solving. In addition, we adapt\ninstruction models to develop embedding models, retrieval-augmented generation models,\nand function-calling models.\n2\nPre-training\nSince pre-training constitutes the majority of computational costs, we focus on reducing\nthe expenses of this stage and show our results in Section 2.1. To enhance efficiency in\npre-training LLMs, we employ two key strategies: data efficiency and training efficiency.\nIn Section 2.2, we discuss our data curation method to maximize the data efficiency under\nfixed token budget. In Section 2.3, we adopt cost-effective training techniques to minimize\nthe computational overhead associated with model scaling.\n2.1\nPerformance\nWe evaluate our pre-trained models using a series of standard benchmarks designed to\nassess English/Korean general knowledge, code, and mathematical reasoning. For general\n2\n\nModels\nMMLU\nKMMLU\nHAE-RAE\nHumanEval\nMBPP\nGSM8K\nAvg\n5-shot\n5-shot\n5-shot\n0-shot\n3-shot\n5-shot\nKanana Flag 32.5B\n77.68\n62.10\n90.47\n51.22\n63.40\n70.05\n69.15\nQwen2.5 32B\n83.10\n63.15\n75.16\n50.00\n73.40\n82.41\n71.20\nGemma 2 27B\n75.45\n51.16\n69.11\n51.22\n64.60\n74.37\n64.32\nEXAONE-3.5-32B†\n72.68\n46.36\n82.22\n-\n-\n-\n-\nAya Expanse 32B†\n74.52\n49.57\n80.66\n-\n-\n-\n-\nKanana Essence 9.8B\n67.61\n50.57\n84.97\n40.24\n53.60\n63.61\n60.10\nLlama 3.1 8B\n65.18\n41.02\n61.78\n35.37\n48.60\n50.87\n50.47\nQwen2.5 7B\n74.19\n51.68\n67.46\n56.71\n63.20\n83.85\n66.18\nGemma 2 9B\n70.34\n48.18\n66.18\n37.20\n53.60\n68.16\n57.28\nEXAONE-3.5-7.8B†\n65.36\n45.30\n77.54\n-\n-\n-\n-\nAya Expanse 8B†\n62.52\n40.11\n71.95\n-\n-\n-\n-\nKanana Nano 2.1B\n54.83\n44.80\n77.09\n31.10\n46.20\n46.32\n50.06\nLlama 3.2 3B\n56.40\n35.57\n47.66\n25.61\n39.00\n27.37\n38.60\nQwen2.5 3B\n65.57\n45.28\n61.32\n37.80\n55.60\n69.07\n55.77\nGemma 2 2B\n52.89\n30.67\n45.55\n20.12\n28.20\n24.72\n33.69\nEXAONE-3.5-2.4B†\n59.27\n43.58\n68.65\n-\n-\n-\n-\nLlama 3.1 70B\n78.93\n53.00\n76.35\n57.32\n66.60\n81.73\n68.99\nQwen2.5 72B\n86.12\n68.57\n80.84\n55.49\n76.40\n92.04\n76.58\nTable 1: Performance of Kanana base models on a set of standard benchmarks. The best scores are\ndenoted in bold. 70B sized Models have been included for reference purposes. † For these models,\nresults are obtained using instruct models because base model checkpoints are not released.\nknowledge, we employ multiple choice tasks of MMLU (Hendrycks et al., 2021a) for English\nknowledge, and KMMLU (Son et al., 2024a) and HAE-RAE (Son et al., 2024b) for Korean-\nspecific knowledge. To evaluate domain-specific abilities, we use HumanEval (Chen et al.,\n2021) and MBPP (Austin et al., 2021) for code and GSM8K (Cobbe et al., 2021) for mathemat-\nical reasoning. We use log-likelihood for multiple choice tasks, and greedy generation for\ngenerative tasks.\nTo demonstrate the effectiveness of our training strategy, we compare our models with\nrepresentative open-source models in various model sizes (Grattafiori et al., 2024; Team\net al., 2024b; Qwen et al., 2025; Research, 2024b; Dang et al., 2024). For EXAONE and Aya\nExpanse models (Research, 2024b; Dang et al., 2024), we report only the performances on\nmultiple-choice tasks using the same evaluation protocol. This decision is based on the\nobservation that multiple-choice performances largely remain unchanged between the base\nand instruct models, whereas generative tasks exhibit notable divergences (see Appendix\nA.1 for a detailed discussion).\nAs shown in Table 1 and Figure 1, our models demonstrate strong performance in various\ndomains and exhibit impressive Korean language capabilities, while requiring significantly\nless training compute. Kanana Flag 32.5B outperforms Llama 3.1 70B, Gemma 2 27B, and\nEXAONE-3.5-32B on knowledge-intensive natural language understanding benchmarks,\nsuch as MMLU and KMMLU, while consuming substantially fewer computational resources.\nIn particular, the computational cost is even lower than that of Llama 3.1 8B, and is similar\nto Gemma 2 9B and EXAONE-3.5-7.8B. On the HAE-RAE benchmark, all Kanana LLMs\ndemonstrate superior performance compared to other LLMs of similar sizes.\n2.2\nData\nWe train Kanana models on 3 trillion tokens, primarily focusing on English and Korean\nbilingual capabilities. We collect our corpora from various sources and categorize them as\nEnglish web, Korean web, academic, code, encyclopedic documents, and instruction data.\nAll our data come from publicly available sources and do not include data from Kakao’s\nproducts or services.\n3\n\nWe begin by collecting various open-source datasets from multiple high-quality sources such\nas arXiv and Wikipedia. However, we observe that these datasets often suffer quality issues\ndue to suboptimal extraction pipelines, resulting in omissions or incoherent paragraph\nordering (see Appendix A.2 for details). Inherently, we improve source-specific extraction\nprocesses for these sources and re-extract documents with more valuable information\nand higher coherence. For code datasets, we utilize open-source datasets from Li et al.\n(2023b) and Lozhkov et al. (2024). We use only permissively licensed code and exclude any\nwith non-permissive or missing licenses. Following INF-Team (2024)’s observation that\nadding instruction data at the end of pre-training enhances performance after SFT, we also\nincorporate instruction data with decontamination.\nUtilizing the high potential of web as a source of valuable and diverse documents (Li et al.,\n2024; Su et al., 2024; Shao et al., 2024b), we apply series of filtering methods to extract high\nquality data. The first filtering process is cascaded filtering pipeline (AI et al., 2025; Grattafiori\net al., 2024; Li et al., 2024; Team et al., 2024a; Penedo et al., 2024a) consisting of deduplication,\nheuristic filtering, and personally identifiable information (PII) anonymization. After the\ncascaded filtering, we further apply language-specific model-based filtering on high quality\ndocuments (Su et al., 2024; Shao et al., 2024b; Li et al., 2024; Penedo et al., 2024a) separately on\nEnglish and Korean. For English web documents, we utilize a DCLM (Li et al., 2024) classifier.\nFor Korean web documents, due to the lack of publicly available high quality classifiers,\nwe iteratively train edu filter as high quality classifier using FastText (Joulin et al., 2017)\nbased on the FineWeb-Edu pipeline (Penedo et al., 2024a). When applying the FineWeb-Edu\npipeline, we observe that most of the documents are classified as uneducational, leading\nto a distribution imbalance. To address this issue, we iteratively retrain the classifier by\naugmenting educational documents from the previous iteration.\nTo assess the quality of our edu filter and Korean web corpus, we perform experiments by\ncontinual pre-training Llama 3 8B with 25B tokens. As shown in Table 2, the quality of our\nKorean web corpus is comparable to that of FineWeb 2 (Penedo et al., 2024b), which is the\nlargest open-source Korean corpus. Furthermore, when using our edu filter to extract high\nquality data from Korean web corpus, we observe a significant performance improvement in\nthe experimental results through training. Interestingly, we observe that using high quality\nEnglish data, regardless of the quality of Korean data, can improve the scores on Korean\nbenchmarks such as KMMLU and HAE-RAE, as well as the English benchmark MMLU. The\nresults from this experiment make a foundation of our intuition for data mixture strategy in\nthe staged pre-training in the following section.\nEnglish Corpus\nKorean Corpus\nMMLU\nKMMLU\nHAE-RAE\n5-shot\n5-shot\n5-shot\n-\n-\n65.14\n40.29\n61.23\nDCLM random\nFineWeb2 Korean\n64.16\n41.02\n70.39\nDCLM random\nOur Korean web\n63.59\n41.41\n71.31\nDCLM random\nOur Korean web w/ edu filter\n63.47\n43.60\n74.89\nDCLM high\nFineWeb2 Korean\n65.36\n41.78\n71.22\nDCLM high\nOur Korean web\n64.80\n41.96\n72.59\nDCLM high\nOur Korean web w/ edu filter\n65.40\n44.19\n75.99\nTable 2: Performance of Llama 3 8B before and after continual pre-training with only 25B tokens, using\ndifferent combinations of English and Korean corpora at a 1:1 ratio.\nIn summary, we share two insights to consider when building bilingual corpora with\nunderrepresented language for enhanced computational efficiency. (1) Prioritize quality over\nquantity. For languages that do not have vast tokens available, such as Korean, prioritizing\nquality over quantity is an effective solution. (2) Knowledge from English data transfers\nto Korean. Even with quality filtering on Korean dataset, English data remains a primary\nsource of diverse and high-quality knowledge. We observe that, under the same conditions\nfor the quality of Korean data, improving the quality of English data leads to higher scores\non Korean-related benchmarks.\n4\n\n2.3\nTraining Process\nTo enhance computational efficiency in pre-training LLMs, we employ three key techniques:\nstaged pre-training from scratch, depth up-scaling, and pruning and distillation. In Section\n2.3.1, we first train 26.8B and 8B models using a staged pre-training approach, which serves\nas the foundation for obtaining LLMs at various scales. In Section 2.3.2, we describe the\nprocess to obtaining Kanana Flag 32.5B and Kanana Essence 9.8B models by depth up-scaling\nfrom 26.8B and 8B models, respectively. In Section 2.3.3, we derive Kanana Nano 2.1B model\nthrough pruning and distillation from the 8B model, reducing training costs while achieving\nsuperior performance compared to training a model from scratch.\n2.3.1\nStaged Pre-training from Scratch\nEnglish Web (67.8%)\nKorean Web (9.6%)\nAcademic (8.7%)\nCode (12.8%)\nEncyclopedia (1.0%)\n2.7T Tokens\n(a) Stage 1 data\nAcademic (35.8%)\nCode (34.3%)\nKorean Web (20.5%)\nEnglish Web (5.3%)\nEncyclopedia (2.4%)\nInstruction (1.8%)\n300B Tokens\n(b) Stage 2 data\nFigure 2: Kanana’s staged pre-training data mixture.\nModels\nStage\nMMLU\nKMMLU\nHAE-RAE\nHumanEval\nMBPP\nGSM8K\nAvg\n5-shot\n5-shot\n5-shot\n0-shot\n3-shot\n5-shot\n26.8B\nStage 1\n73.38\n54.26\n84.97\n32.32\n47.20\n57.77\n58.32\nStage 2\n74.27\n59.04\n88.45\n51.22\n61.60\n67.48\n67.01\n8B\nStage 1\n63.48\n45.51\n77.27\n23.78\n35.80\n35.03\n46.81\nStage 2\n64.22\n48.30\n83.41\n40.24\n51.40\n57.09\n57.44\nTable 3: Performance of from-scratch Kanana models at the end of each training stage.\nTo maximize performance under fixed compute budget, we adopt the staged pre-training\nstrategy (Hu et al., 2024; Team et al., 2024a; Huang et al., 2024; Wake et al., 2025; Granite Team,\n2024) with two stages. Staged pre-training divides the pre-training process into multiple\nstages, starting with training LLMs on a large amount of moderate-quality data in the initial\nstages, and gradually increasing the proportion of high quality data in the subsequent\nstages.\nWe begin by training 8B from scratch using the diverse 2.7 trillion in stage 1 as shown in\nFigure 2a. In stage 2, we further train the model using 300 billion tokens shown in Figure 2b.\nSpecifically, we set aside high quality data for each category using the available high quality\nclassifiers. Then, we perform lightweight annealing experiments to select candidate datasets\nto search for the data mixture following Grattafiori et al. (2024). Then, the optimal data\nmixture is selected through ablation study. The final model of stage 2 results in a 2.79 point\nincrease in KMMLU and a 10.63 point increase in average performance, demonstrating the\neffectiveness and efficiency of staged pre-training. We apply the same data mixture that was\nused during the training of 8B to 26.8B model. Direct application of the recipe consistently\nyields remarkable performance and stable training as shown in Table 3, demonstrating the\nscalability of our recipe. See Appendix A.3 for our pre-training configurations.\n2.3.2\nDepth Up-scaling\nTo further enhance the model performance within limited resources after pre-training, we\nadopt the depth up-scaling (DUS) which increases model capacity by stacking additional\n5\n\nlayers (Kim et al., 2024a). We apply DUS to expand Kanana 8B into Kanana Essence 9.8B\nand Kanana 26.8B into Kanana Flag 32.5B. After the up-scaling process, each model vari-\nant is further trained on the same data mixtures used in pre-training, with 100 billion\ntokens dedicated to stage 1 and another 100 billion to stage 2. Results of the up-scaling\nstrategy demonstrates that the additional layers consistently contribute to performance\nenhancements as summarized in Table 4.\nModels\nMMLU\nKMMLU\nHAE-RAE\nHumanEval\nMBPP\nGSM8K\nAvg\n5-shot\n5-shot\n5-shot\n0-shot\n3-shot\n5-shot\n26.8B + DUS (32.5B)\n77.68\n62.10\n90.47\n51.22\n63.40\n70.05\n69.15\n26.8B\n74.27\n59.04\n88.45\n51.22\n61.60\n67.48\n67.01\n8B + DUS (9.8B)\n67.61\n50.67\n84.98\n40.24\n53.60\n63.61\n60.10\n8B\n64.22\n48.30\n83.41\n40.24\n51.40\n57.09\n57.44\nTable 4: Performance comparison of Kanana models before and after depth up-scaling.\nTable 4 illustrates the performance improvements achieved through depth up-scaling.\nKanana Essence 9.8B consistently outperforms its non-upscaled version, Kanana 8B with\nthe average score rising from 57.52 to 60.12. This improvement is evident in MMLU,\nKMMLU, HAE-RAE, MBPP, and GSM8K, except for HumanEval. Similarly, Kanana Flag\n32.5B achieves average score of 69.15, notably surpassing the non-upscaled Kanana 26.8B\nmodel. These results emphasize the effectiveness of depth up-scaling in improving various\nbenchmark scores.\nNotably, our strategy saves 11.06% of total computational cost compared to the training\nof 9.8B and 32.5B LLMs from scratch. This strategy of increasing model capacity through\ndepth up-scaling only occupies about 6.67% of the total computing resources across the\nentire training procedure. In combination with pre-training, depth up-scaling offers a\nstrategic approach to significantly enhance model performance without introducing heavy\ncomputational demands of building new models from scratch.\n2.3.3\nPruning and Distillation\nIn opposition to efficiently up-scaling the model size, knowledge distillation is an effective\nmethod to efficiently down-scale the model size (Hinton et al., 2015; Gunter et al., 2024;\nMeta, 2024). Leveraging the 8B model from Section 2.3.1, we efficiently produce smaller\nmodels by improving the pruning and distillation of Minitron (Muralidharan et al., 2024;\nSreenivas et al., 2024). This process allows us to produce models with better performance\nat one-tenth of the data size compared to training from scratch, as shown in Table 5. We\nfurther show that iteratively extending the process beyond two iterations remains effective,\npreserving 87-99% of KMMLU score at only 50% of the model size, as shown in Table 6. Our\nmodels achieve competitive performance to recent open-source models (Allal et al., 2025;\nGrattafiori et al., 2024; Team et al., 2024b; Qwen et al., 2025), as presented in Table 14.\nModels\nTraining\nTokens\nMMLU\nKMMLU\nHAERAE\nHumanEval\nMBPP\nGSM8K\nAvg\n5-shot\n5-shot\n5-shot\n0-shot\n3-shot\n5-shot\n2.1B PD\n0.3T\n54.83\n44.80\n77.09\n31.10\n46.20\n46.32\n50.06\n2.1B\n3T\n50.66\n36.61\n68.74\n24.45\n41.60\n36.69\n43.13\nTable 5: Token consumption and performance of pruning & distillation (PD) from preceding models\nand training from scratch. We use the same 2.1B architecture.\nIn order to improve the pruning and distillation process, we refine Minitron’s width impor-\ntance scoring while preserving its simplicity and efficiency. Its scoring process begins by\nmeasuring the importance of embedding channels, feed-forward neurons, and attention\nheads using activations from a small calibration dataset. Next, we show that summing\nlayer-wise scores plays a crucial role in performance, whereas the prior work performed\nablations along batch and sequence axes. Moreover, for Grouped-Query Attention (GQA)\n6\n\nModels\nMMLU\nKMMLU\nHAERAE\nHumanEval\nMBPP\nGSM8K\nAvg\n5-shot\n5-shot\n5-shot\n0-shot\n3-shot\n5-shot\n8B†\n64.22\n48.30\n83.41\n40.24\n51.40\n57.09\n57.44\n4.5B\n59.74\n48.09\n82.58\n34.76\n48.60\n57.01\n55.13\n2.1B\n54.83\n44.80\n77.09\n31.10\n46.20\n46.32\n50.06\n1.3B\n53.55\n39.91\n72.59\n28.05\n39.60\n36.01\n44.95\n635M\n46.28\n34.60\n62.69\n23.17\n31.40\n19.26\n36.23\n385M\n41.16\n31.70\n47.94\n18.90\n24.00\n10.83\n29.08\n192M\n26.11\n30.16\n19.71\n12.80\n12.40\n2.43\n17.27\nTable 6: Performance through iterative compression beyond two iterations. Each model is pruned\nfrom the preceding model. † Each model is distilled using the 8B model as the teacher.\n(Ainslie et al., 2023), we improve performance by ensuring query-key-value alignment.\nSpecifically, we remove an equal number of query heads within each group, as shown in\nFigure 9. Additionally, since Kanana employs SwiGLU (Shazeer, 2020), we choose between\naveraging gate and up states or using intermediate states, whereas the original formulation\nrelies on pre-activation values. All ablation results for importance scoring are in Table 15.\nWe further enhance the pruning strategies with a focus on intermediate model structures.\nConsistent with the findings from Minitron, we observe that excessive single-step compres-\nsion leads to significant degradation. Although maintaining attention heads is generally\nbeneficial, our experiments reveal that pruning them for smaller models is effective when\ndone earlier at larger scales as presented in Table 16. Additionally, we find that input and\noutput embeddings can be tied by averaging without causing noticeable degradation, which\nwe apply when pruning from 4.5B to 2.1B as shown in Table 17.\nLastly, we observe that the composition of distillation data directly influences the perfor-\nmance, while pruning data is less important. For models larger than 2B, we use high-quality\n300 billion tokens of stage 2 described in Section 2.3.1. However, for smaller models, increas-\ning the proportion of general-domain English data increases both English performance and\nother benchmark scores, as shown in Table 18.\nIn conclusion, our comprehensive pre-training process, which includes staged pre-training,\ndepth up-scaling, and iterative pruning and distillation, offers a compute-efficient strategy\nfor developing high-performing language models. This combined approach not only en-\nhances performance across diverse benchmarks, but also ensures computational efficiency,\ndemonstrating the effectiveness of our strategy in producing a robust family of models\nspanning the range from 2.1B to 32.5B. See Appendix A.4 for our pruning and distillation\nconfigurations.\n3\nPost-training\nBuilding on Kanana pre-trained models, we further develop instruction-tuned models for\ndirect interaction by natural language. In Section 3.1, we highlight the performance of\nKanana instruction-tuned models, demonstrating superior performance on Korean tasks\nand competitive results on other tasks. Section 3.2 presents the details of the specifics\nregarding the Supervised Fine-Tuning (SFT) and preference datasets. Section 3.3 outlines\nthe extensive post-training techniques applied on Kanana instruction models.\n3.1\nPerformance\nWe evaluate our instruction-tuned models across various tasks: chat, instruction following,\ngeneral knowledge, coding, and mathematics and compare their performance to previous\ninstruction-tuned models. For general chat ability, we use MT-Bench (Zheng et al., 2023),\nLogicKor (Park, 2024), KoMT-Bench (Research, 2024a), and WildBench (Lin et al., 2025). To\n7\n\nModels\nChat\nInstruction Following\nMT-Bench\nLogicKor\nKoMT-Bench\nWildBench\nIFEval\nKanana Flag 32.5B\n8.356\n9.524\n8.058\n54.14\n0.856\nQwen2.5 32B\n8.331\n8.988\n7.847\n51.13\n0.822\nGemma 2 27B\n8.088\n8.869\n7.373\n46.46\n0.817\nEXAONE-3.5-32B\n8.375\n9.202\n7.907\n54.30\n0.845\nAya Expanse 32B\n7.788\n8.941\n7.626\n48.36\n0.735\nKanana Essence 9.8B\n7.769\n8.964\n7.706\n47.27\n0.799\nLlama 3.1 8B\n7.500\n6.512\n5.336\n33.20\n0.772\nQwen2.5 7B\n7.625\n7.952\n6.808\n41.31\n0.760\nGemma 2 9B\n7.633\n8.643\n7.029\n40.92\n0.750\nEXAONE-3.5-7.8B\n8.213\n9.357\n8.013\n50.98\n0.826\nAya Expanse 8B\n7.131\n8.357\n7.006\n38.50\n0.645\nKanana Nano 2.1B\n6.400\n7.964\n5.857\n25.41\n0.720\nLlama 3.2 3B\n7.050\n4.452\n3.967\n21.91\n0.767\nQwen2.5 3B\n6.969\n6.488\n5.274\n25.76\n0.355\nGemma 2 2B\n7.225\n5.917\n4.835\n28.71\n0.428\nEXAONE-3.5-2.4B\n7.919\n8.941\n7.223\n41.68\n0.790\nLlama 3.1 70B\n8.275\n8.250\n6.970\n46.50\n0.875\nQwen2.5 72B\n8.619\n9.214\n8.281\n55.25\n0.861\nTable 7: Performance of Kanana and previous instruction-tuned models in general chat and instruction\nfollowing benchmarks. Across all Chat benchmarks, we use gpt-4o-2024-08-06 as a judge model. The\nbest scores are denoted in bold. 70B sized models have been included for reference purposes.\nModels\nGeneral\nCoding\nMathematics\nMMLU\nKMMLU\nHAE-RAE\nHumanEval+\nMBPP+\nGSM8K\nMATH\nKanana Flag 32.5B\n81.08\n64.19\n68.18\n77.44\n69.84\n90.83\n57.82\nQwen2.5 32B\n84.40\n59.37\n48.30\n82.32\n71.96\n95.30\n81.90\nGemma 2 27B\n78.01\n49.98\n46.02\n70.12\n70.90\n91.05\n53.80\nEXAONE-3.5-32B\n78.30\n55.44\n52.27\n78.66\n70.90\n93.56\n76.80\nAya Expanse 32B\n74.49\n42.35\n51.14\n64.63\n65.61\n75.06\n42.82\nKanana Essence 9.8B\n70.64\n50.76\n47.16\n72.56\n69.05\n84.91\n42.24\nLlama 3.1 8B\n71.18\n39.24\n40.91\n60.98\n57.67\n82.71\n49.86\nQwen2.5 7B\n77.23\n46.87\n37.50\n73.78\n70.63\n91.58\n75.22\nGemma 2 9B\n73.47\n44.47\n39.77\n59.76\n64.55\n87.72\n48.10\nEXAONE-3.5-7.8B\n72.62\n52.09\n46.02\n79.27\n66.67\n89.99\n73.50\nAya Expanse 8B\n61.23\n35.78\n39.20\n42.68\n56.88\n78.85\n30.80\nKanana Nano 2.1B\n52.48\n38.51\n33.52\n63.41\n62.43\n72.32\n29.26\nLlama 3.2 3B\n56.09\n3.07\n17.05\n56.71\n50.26\n66.57\n38.18\nQwen2.5 3B\n69.18\n38.33\n32.39\n67.68\n64.02\n84.00\n65.72\nGemma 2 2B\n57.69\n6.99\n7.95\n35.37\n45.24\n49.81\n21.68\nEXAONE-3.5-2.4B\n63.19\n14.27\n14.20\n70.73\n59.79\n83.78\n64.04\nLlama 3.1 70B\n83.48\n39.08\n53.41\n75.61\n66.40\n91.66\n63.98\nQwen2.5 72B\n87.14\n65.78\n60.80\n81.10\n75.66\n95.45\n82.60\nTable 8: Performance of Kanana post-trained models on a set of standard benchmarks. All benchmarks\nunder General category are measured using 0-shot CoT with respective chat-template of each model.\nThe best scores are denoted in bold. 70B sized models have been included for reference purposes.\ntest instruction following ability, we use IFEval1(Zhou et al., 2023). For general knowledge\ntasks, we use MMLU (Hendrycks et al., 2021a), KMMLU (Son et al., 2024a), and HAE-RAE2\n(Son et al., 2024b), with zero-shot chain-of-thought (CoT) (Wei et al., 2022) setting along with\nthe chat template. Employing zero-shot CoT with the chat template, rather than multi-shot\nprompts, allows us to evaluate the inherent capabilities of the instruction model, without\nresidual traces from the pre-trained model. For coding ability, we use HumanEval+ (Liu\net al., 2023) and MBPP+ (Liu et al., 2023). For Mathematical ability, we use GSM8K (Cobbe\n1We report the average of Prompt-level strict-accuracy and Instruct-level strict-accuracy.\n2We report general knowledge category scores in this section.\n8\n\net al., 2021) and MATH (Hendrycks et al., 2021b). See Appendix B.1 for detailed prompts of\nbenchmarks.\nTable 7 and Table 8 show that our models excel similar sized models on Korean tasks. The\n32.5B model achieves the highest performance in Korean chat tasks (LogicKor, KoMT-Bench)\nand Korean knowledge tasks (KMMLU, HAE-RAE). The 9.8B and 2.1B models rank second\nin Korean chat tasks and either best or second-best in Korean knowledge tasks. Additionally,\nour models exhibit competitive performance across other tasks except in math.\n3.2\nData\nWe collect 1.2M instruction data instances in English and Korean to address both languages.\nTo ensure that our post-training data can handle diverse human requests, we define five\ndistinct domains and collect prompts from both public datasets and human contributors. As\na result, our dataset comprises 492K instances for code, 260K for math, 230K for instruction\nfollowing, 120K for general chat, and 96K for safety. The safety dataset includes prompts\nrelated to ethics, privacy, toxicity, and bias.\nFigure 3 depicts the instance size and proportion of each domain. For the preference opti-\nmization stage, we sub-sampled and balanced the data across each domain.\ngeneral chat (10.0%)\ninstruction following (19.2%)\ncode (41.1%)\nsafety (8.0%)\nmath (21.7%)\n 1.2M\n(a) SFT data\ngeneral chat (23.3%)\ninstruction following (32.9%)\ncode (31.7%)\nmath (12.0%)\n407K\n(b) Preference data\nFigure 3: Data size and proportion of each domain.\n3.3\nTraining Process\nWe adopt the widely used multi-stage post-training procedure comprising SFT and a series\nof preference optimization processes (Ouyang et al., 2022; Grattafiori et al., 2024; Qwen\net al., 2025; Team et al., 2024b). In Section 3.3.1, we provide details on the SFT process. In\nSection 3.3.2, we share information on training our reward model from the SFT model for\nthe subsequent preference optimization process. In Section 3.3.3, we perform preference\noptimization on the SFT model, which is a sequential process consisting of offline and online\npreference optimization.\nAs shown in Figure 4, each step of this process quantitatively enhances the instruction-\ntuned model across different model sizes. Qualitatively, we observe that during the SFT\nstage, the model learns to generate structured chat responses while integrating relevant\nknowledge, and this ability persists through subsequent stages. Building on the SFT model,\nthe preference optimization stages further enhance performance by refining the model’s\ntone and manner. Appendix C presents qualitative results and illustrates the evolution of\nmodel completions throughout each phase of post-training.\n3.3.1\nSupervised Fine-Tuning\nDuring the SFT stage, the model develops the ability to generate structured chat responses\nwhile integrating relevant knowledge. In this stage, we train the model using 1.2M data\ninstances, as described in Section 3.2. While optimizing the proportion of domain-specific\ndata, we observed that such data is crucial for achieving high performance in its respective\ndomain and does not negatively impact other domains. Table 9 demonstrates that excluding\n9\n\n5\n10\n15\n20\n25\n30\nModel Size (B)\n0.45\n0.50\n0.55\n0.60\n0.65\n0.70\n0.75\nAVG Score\nSupervised Fine-tuning\nOffline Preference Optimization\nOnline Preference Optimization\nFigure 4: Kanana model performance for each stage of training across different model sizes. The\ny-axis is the average of normalized scores of all benchmarks in Table 7 and Table 8. The normalization\nprocess is done by dividing each score with the maximum possible score.\nDatasets used\nNormalized Scores\nGeneral\nInstruction Following\nCode\nMath\nMT-Bench\nIFEval\nHumanEval+\nMBPP+\nGSM8K\nMATH\n✓\n✓\n✓\n✓\n1.00\n1.00\n1.00\n1.00\n1.00\n1.00\n✓\n✗\n✓\n✓\n0.98\n0.72\n1.06\n0.99\n1.03\n1.07\n✓\n✓\n✗\n✓\n0.99\n1.00\n0.66\n0.72\n1.01\n1.05\n✓\n✓\n✓\n✗\n0.98\n1.00\n1.04\n1.00\n0.60\n0.59\nTable 9: Domain mixture ablation for SFT dataset. All scores are normalized by the score of the SFT\nmodel when datasets of all domains have been included in the training set. We see that removing a\nspecific domain from the training dataset exclusively deteriorates the performance of the respective\ndomain by a significant amount.\ndomain-specific data from total dataset only reduces performance on the corresponding do-\nmain’s benchmark, while performance in other domains remains unaffected. Consequently,\nwe incorporate the full extent of each domain-specific dataset while ensuring balanced\nperformance across all domains.\n3.3.2\nReward Model Training\nWe train a reward model for subsequent online preference optimization process, assuming a\nBradley-Terry model (Bradley & Terry, 1952). The reward model is trained using the offline\npreference data along with additional public preference data. Among various reward models\ntrained with different data proportions and settings, we select the one that demonstrates the\nstrongest best-of-N policy (Gao et al., 2023) performance. The best-of-N policy performance\nis evaluated by generating N responses from the policy model, scoring them with the\nreward model, selecting the highest-scoring response, and then assessing the final response’s\nquality using a benchmark judge. This approach is based on the intuition that the chosen\nreward model should effectively evaluate the response distribution of the online preference\noptimization stage in accordance with the benchmark evaluation criteria.\n3.3.3\nPreference Optimization\nTo further improve the SFT model’s performance on LLM benchmarks, we conduct a pref-\nerence optimization stage. The process begins with offline preference optimization (Meng\net al., 2024; Jung et al., 2024), where we apply direct preference optimization (DPO) (Rafailov\net al., 2023) using the offline preference data.\nWe then conduct online preference optimization, initializing from the offline DPO model.\nDuring training, policy-generated responses are evaluated by the reward model from Section\n3.3.2, providing training data for online DPO (Guo et al., 2024a) with asynchronous response\n10\n\nsampling (Noukhovitch et al., 2025). This approach can be considered as a form of iterative\nDPO (Xiong et al., 2024). However, unlike prior work (Tran et al., 2023), we maintain a fixed\nreference model, specifically the offline DPO model, throughout all iterations. This decision\nis based on our observation that updating the reference model led to undesirable increases\nin response length.\n4\nAdaptations\nIn this section, we show three examples of practical adaptations of Kanana models to popular\napplications of LLMs: embedding models, retrieval-augmented models, and function calling\nmodels. Through experimental results, we show that the performances of Kanana models\nare further improved in each relevant benchmarks when task-specific training techniques\nare further applied, showcasing the possibility of adapting Kanana models to a wide range\nof applications.\n4.1\nEmbedding Models\nText embeddings, or dense vector representations, are essential for capturing the semantic\nessence of text (Karpukhin et al., 2020; Khattab & Zaharia, 2020). Following the success\nof LLMs, decoder-only language models have taken their place as a popular backbone of\nsentence embedding models (Muennighoff, 2022; Wang et al., 2023; Springer et al., 2024;\nMa et al., 2024; BehnamGhader et al., 2024; Xu et al., 2024). In this section, we examine the\ncapabilities of the Kanana model, specifically the Kanana Nano 2.1B, as a robust backbone\nfor embedding by employing LLM2Vec (BehnamGhader et al., 2024). For comparative\nanalysis, we also apply LLM2Vec on models of Llama 3 and Qwen2.5 series with similar\nmodel sizes.\nEmbedding Backbone\nEnglish\nKorean\nAvg\nKanana Nano 2.1B\n51.56\n65.00\n58.28\nLlama 3.2 3B\n53.28\n59.43\n56.35\nQwen2.5 3B\n54.00\n62.10\n58.05\nLlama 3.2 1B\n48.77\n54.68\n51.73\nQwen2.5 1.5B\n50.60\n54.60\n52.60\nTable 10: Performance comparison of embedding models on English and Korean retrieval benchmarks.\nAll embedding models are fine-tuned from instruct models. See Appendix D for detailed evaluations.\nThe embedding models are evaluated on subsets of Massive Text Embedding Benchmark\n(MTEB) (Muennighoff et al., 2023) retrieval tasks, including 10 English tasks sourced from\nthe MTEB v2 leaderboard (Enevoldsen et al., 2025) and 8 Korean tasks curated by Jang et al.\n(2024). Table 10 presents average nDCG@10 scores for English and Korean, summarizing\nthe performance results on retrieval tasks.\nKanana Nano 2.1B consistently demonstrates competitive performance and serves as an\neffective backbone for embedding tasks. As shown in Table 10, our 2.1B model not only\nsignificantly surpasses Llama 3.2 1B and Qwen2.5 1.5B across both English and Korean\nbenchmarks, but also outperforms Llama 3.2 3B and Qwen2.5 3B on Korean evaluations,\ndespite its smaller size. Additionally, it achieves a solid English score and the highest\naverage score among the models, highlighting the strong capacity of Kanana Nano 2.1B\nwhen fine-tuned for retrieval tasks.\n4.2\nRetrieval-Augmented Generation\nRetrieval-Augmented Generation (RAG) methods (Lewis et al., 2021) enable large language\nmodels to access the latest external or proprietary information without altering model\nparameters (Liu et al., 2024a). In order to ensure factual consistency during retrieval, the\ngrounding ability of the model needs to be trained through additional data mixture (Lin\net al., 2024). In this section, we describe a process for developing reliable RAG models with\nenhanced grounding ability from Kanana LLMs.\n11\n\nFor evaluation, we collect RAG scenario benchmarks and evaluate our model on them.\nContextualBench (Nguyen et al., 2024) is set of multi-hop QA, which we specifically include\nto consider the conciseness in evaluation. FACTs (Jacovi et al., 2025) consists of various tasks\nwith contexts such as reasoning, QA, summarization, rewriting, and extraction. 3 IFEval\n(Zhou et al., 2023) measures maintenance of helpfulness of our instruct model. However,\nthese benchmarks are all English-based, making them insufficient to judge the RAG abilities\nin Korean. To this end, we develop an internal FACTs-like Korean RAG benchmark called\nRAG-General-Bench that focuses on measuring factual consistency in Korean. During the\ndevelopment, human annotators manually constructed the dataset with context, instruction,\nand reference answer, to evaluate helpfulness as well. The benchmark consists of a total of\n115 samples with 4 main tasks, categorized into 27 subcategories, providing a diverse set of\nscenarios for evaluation. There are 2 samples of QA task in Appendix E.\n50\n51\n52\n53\n54\n55\n56\n57\nHelpfulness\n35\n40\n45\n50\n55\n60\nGrounding\nGemma2 9B\nQwen2.5 7B\nAya Expanse 8B\nEXAONE3.5 7.8B\nKanana Essence 9.8B\nKanana Essence 9.8B RAG\nGPT-4o\nKanana\nOther instruct models\nFigure 5: Performance Comparison of Various Models Based on averaged helpfulness and grounding\nin RAG-General-Bench.\nDocument\nSynQA-SFT\nQA\nFiltered QA\nSynQA-DPO\nRefined QA\nSynthetic Data\nGeneration\nReward Filter\n(score >= 9)\nRefine Answer\n(w/ reflection)\nReward Filter\n(score == 10)\nBad Response\nGeneration\nFigure 6: QA Generation Pipeline\nTo increase grounding ability, we synthetically generate question-answer pairs using high-\nquality bilingual documents as seed documents, following the pipeline in Figure 6. Then, we\nfilter out instances with low grounding scores and use LLM-judge to reflect and refine the\nlow grounding instances. We call the dataset at this point as SynQA-SFT. With SynQA-SFT,\nwe augment responses with low grounding score to produce preference dataset that we\ncall SynQA-DPO. Along with SynQA datasets, we utilize StructLM (Zhuang et al., 2024)\nand FollowRAG (Dong et al., 2024) to adapt diverse context format and instructions in\nRAG scenarios and replay SFT dataset from Section 3.2 to prevent general capability of the\ninstruction model from degrading during training.\nHowever, we observe a decline in the helpfulness score as the model is trained through\nSFT and DPO in Table 11. In order to address this issue, we merge the DPO model with\nthe instruction model to preserve helpfulness (Kim et al., 2024b). As a result, Kanana\nEssence 9.8B RAG achieves 91.4% of GPT-4o’s grounding performance while maintaining\nour instruct model’s helpfulness in our benchmark as presented in Figure 5.\n4.3\nFunction Calling\nFunction calling is an essential ability for large language models (LLMs) to interact with\nexternal tools and databases, granting them access to up-to-date information stored in\n3We filtered with character length of 20k since our base model was trained with token length limit\nof 8k. This dataset is not labeled golden answer, so we only measure grounding score with it.\n12\n\nModels\nFACTs\nRAG-General-Bench\nContextualBench\nIFEval\nGrounding\nGrounding\nHelpfulness\nExact-match\nKanana Essence 9.8B\n40.66\n32.63\n55.86\n20.22\n79.93\n+ SFT\n62.40\n59.29\n51.60\n48.08\n72.99\n+ DPO\n63.09\n65.33\n52.67\n48.76\n75.00\n+ Merge (Kanana Essence 9.8B RAG)\n53.09\n57.38\n57.32\n48.31\n78.44\nTable 11: Performance change of each phase of recipe. Grounding score is average of two metric\nRAGAS (Es et al., 2023) Faithfulness and rubric based LLM-judge. Helpfulness score is average of two\nmetric RAGAS Answer Relevancy and rubric based LLM-judge. EM means exact matching normalized\nanswer with golden label. IFEval scoring is as same as Section 3.1.\ndynamic or structured formats (Schick et al., 2023). This capability helps integrating real-time\ndata with the static knowledge inherent in LLMs, which is particularly vital in enterprises.\nPrevious works highlight the increasing importance of function calling, which has led to\nvarious efforts in data generation for fine-tuning and model evaluation (Basu et al., 2024;\nGuo et al., 2024b; Qin et al., 2024; Tang et al., 2023; Li et al., 2023a; Rastogi et al., 2020;\nLiu et al., 2024b). However, these efforts predominantly focused on English, making it\nnecessary to create a function calling dataset for low-resource languages. To address this gap\nwithin Korean contexts, we create a fine-tuning dataset, referred to as korean-fc-corpus.\nThe corpus is constructed by: (1) translating two key English function calling corpora, glaive-\nfunction-calling-v1 (gfc-v1) (GlaiveAI, 2023) and the Schema-Guided Dialogue Dataset\n(sgd) (Rastogi et al., 2020), into their Korean equivalents, ko-gfc-v1 and ko-sgd; and (2)\ncreating an in-house function calling dataset (inhouse-fc) specifically tailored for corporate\napplications.\nWe further adopt two-staged training process comprising domain specific pre-training and\nsupervised fine-tuning to adapt instruct-tuned models to function calling specific tokens\nand terminologies. In the domain pre-training phase, we leveraged multiple English-based\nfunction calling datasets, including gfc-v1, glaive-function-calling-v2 (GlaiveAI, 2024), xlam-\nfunction-calling-60k (Liu et al., 2024b), as well as sgd, supplemented by our inhouse-fc. This\nfoundation enabled us to perform supervised fine-tuning exclusively on korean-fc-corpus.\nThis two-stage strategy ensures that models become adequately versed in function calling\nconventions and domain terminologies before focusing on Korean-specific nuances, thereby\nenhancing their performance in Korean function calling tasks.\nModels\nSingle-call\nDialogue\nKanana 8B FC\n0.88\n0.89\ngpt-4-0125-preview\n0.94\n0.94\ngpt-4o-2024-05-13\n0.93\n0.95\nTable 12: Evaluation on FunctionChat-Bench: Single-call and Dialogue Accuracy\nTo evaluate function calling capabilities in corporate environments, we introduce\nFunctionChat-Bench (Lee et al., 2024), a benchmark designed for Korean conversational\nsettings. This benchmark measures performance on two metrics: Single-call accuracy, which\nevaluates how well a model selects and invokes the necessary function from several options,\nand Dialogue accuracy, which examines the model’s capability in multi-turn interactions.\nFor comparative analysis, we evaluate OpenAI’s proprietary models ( gpt-4-0125-preview,\nand gpt-4o-2024-05-13) and Kanana 8B FC model as shown in Table 12.\nThis result indicates that leveraging task specific fine-tuning on moderately sized LLMs,\nwhich are trained at a lower cost, may offer a more cost effective and efficient approach for\naddressing certain tasks.\n13\n\n5\nConclusion\nIn this report, we present Kanana, a family of large language models available in sizes of\n{2.1B, 9.8B, 32.5B}, with a focus on the cost-effective training procedure compared to other\nprominent open models. We emphasize the strong bilingual capability of Kanana models,\nshowcasing state-of-the-art performance on Korean benchmarks of KMMLU, HAE-RAE,\nand KoMT-Bench and competitive results on various English benchmarks. However, we\nalso acknowledge the limitations of Kanana models in overall performance on small scale\nmodels sizes, particularly in math domains. To address the limitations, we plan to improve\nsmall models and the math ability of all models through data quality and mixture. To further\nour commitment in cost-effective training, we intend to explore strategical approaches such\nas formulating scaling laws and other training methodologies as possible future directions.\nAdditionally, we aim to expand the linguistic ability from bilingual to multilingual pri-\noritizing the intuition of treating the underrepresented languages covered in this report.\nBy continuing to build on these efforts, we aspire to make advancements in the field of\nlarge language models, balancing performance with efficiency and broadening the linguistic\nscope of our models.\n14\n\nContributors and Acknowledgements\nPre-training\nYunju Bak, Doohae Jung, Boseop Kim†, Nayeon Kim, Hojin Lee, Jaesun Park, Minho Ryu\nPost-training\nJiyeon Ham, Seungjae Jung, Hyunho Kim, Hyunwoong Ko, Changmin Lee, Daniel Wontae\nNam†, Kyoung-Woon On†‡\nAdaptation\nSeulye Baeg, Junrae Cho, Taegyeong Eo, Sunghee Jung, Jieun Kang, EungGyun Kim†,\nEunhwa Kim, Byeongil Ko, Daniel Lee, Donghun Lee, Minchul Lee, Miok Lee, Shinbok Lee,\nMinho Ryu, Gaeun Seo\nAcknowledgments\nWe thank Donghee Son for the re-extraction of the arXiv data for the pre-training. We\nalso thank Gunsoo Han, Jisang Park, and Byeongjae Son for their contributions in the\nconstruction of the general chat and code data used in the post-training. Finally, we thank\nMyungchul Shin and Byung-hak Kim for their invaluable support for Kanana models.\n†Team leads\n‡Work done at Kakao Corp.\n15\n\nReferences\n01. AI, :, Alex Young, Bei Chen, Chao Li, Chengen Huang, Ge Zhang, Guanwei Zhang,\nGuoyin Wang, Heng Li, Jiangcheng Zhu, Jianqun Chen, Jing Chang, Kaidong Yu, Peng\nLiu, Qiang Liu, Shawn Yue, Senbin Yang, Shiming Yang, Wen Xie, Wenhao Huang, Xiaohui\nHu, Xiaoyi Ren, Xinyao Niu, Pengcheng Nie, Yanpeng Li, Yuchi Xu, Yudong Liu, Yue\nWang, Yuxuan Cai, Zhenyu Gu, Zhiyuan Liu, and Zonghong Dai. Yi: Open foundation\nmodels by 01.ai, 2025. URL https://arxiv.org/abs/2403.04652.\nJoshua Ainslie, James Lee-Thorp, Michiel de Jong, Yury Zemlyanskiy, Federico Lebron,\nand Sumit Sanghai. GQA: Training generalized multi-query transformer models from\nmulti-head checkpoints. In Houda Bouamor, Juan Pino, and Kalika Bali (eds.), Proceedings\nof the 2023 Conference on Empirical Methods in Natural Language Processing, pp. 4895–4901,\nSingapore, December 2023. Association for Computational Linguistics. doi: 10.18653/v1/\n2023.emnlp-main.298. URL https://aclanthology.org/2023.emnlp-main.298/.\nLoubna Ben Allal, Anton Lozhkov, Elie Bakouch, Gabriel Mart´ın Bl´azquez, Guilherme\nPenedo, Lewis Tunstall, Andr´es Marafioti, Hynek Kydl´ıˇcek, Agust´ın Piqueres Lajar´ın,\nVaibhav Srivastav, Joshua Lochner, Caleb Fahlgren, Xuan-Son Nguyen, Cl´ementine Four-\nrier, Ben Burtenshaw, Hugo Larcher, Haojun Zhao, Cyril Zakka, Mathieu Morlon, Colin\nRaffel, Leandro von Werra, and Thomas Wolf. Smollm2: When smol goes big – data-centric\ntraining of a small language model, 2025. URL https://arxiv.org/abs/2502.02737.\nJacob Austin, Augustus Odena, Maxwell Nye, Maarten Bosma, Henryk Michalewski, David\nDohan, Ellen Jiang, Carrie Cai, Michael Terry, Quoc Le, and Charles Sutton. Program\nsynthesis with large language models, 2021. URL https://arxiv.org/abs/2108.07732.\nKinjal Basu, Ibrahim Abdelaziz, Subhajit Chaudhury, Soham Dan, Maxwell Crouse, Asim\nMunawar, Sadhana Kumaravel, Vinod Muthusamy, Pavan Kapanipathi, and Luis A\nLastras. Api-blend: A comprehensive corpora for training and benchmarking api llms.\narXiv preprint arXiv:2402.15491, 2024.\nParishad BehnamGhader, Vaibhav Adlakha, Marius Mosbach, Dzmitry Bahdanau, Nicolas\nChapados, and Siva Reddy. Llm2vec: Large language models are secretly powerful text\nencoders. arXiv preprint arXiv:2404.05961, 2024.\nRalph Allan Bradley and Milton E Terry. Rank analysis of incomplete block designs: I. the\nmethod of paired comparisons. Biometrika, 39(3/4):324–345, 1952.\nTom B. Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared Kaplan, Prafulla\nDhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, Sandhini\nAgarwal, Ariel Herbert-Voss, Gretchen Krueger, Tom Henighan, Rewon Child, Aditya\nRamesh, Daniel M. Ziegler, Jeffrey Wu, Clemens Winter, Christopher Hesse, Mark Chen,\nEric Sigler, Mateusz Litwin, Scott Gray, Benjamin Chess, Jack Clark, Christopher Berner,\nSam McCandlish, Alec Radford, Ilya Sutskever, and Dario Amodei. Language models are\nfew-shot learners. In Proceedings of the 34th International Conference on Neural Information\nProcessing Systems, NIPS ’20, Red Hook, NY, USA, 2020. Curran Associates Inc. ISBN\n9781713829546.\nMark Chen, Jerry Tworek, Heewoo Jun, Qiming Yuan, Henrique Ponde de Oliveira Pinto,\nJared Kaplan, Harri Edwards, Yuri Burda, Nicholas Joseph, Greg Brockman, Alex Ray,\nRaul Puri, Gretchen Krueger, Michael Petrov, Heidy Khlaaf, Girish Sastry, Pamela Mishkin,\nBrooke Chan, Scott Gray, Nick Ryder, Mikhail Pavlov, Alethea Power, Lukasz Kaiser, Mo-\nhammad Bavarian, Clemens Winter, Philippe Tillet, Felipe Petroski Such, Dave Cummings,\nMatthias Plappert, Fotios Chantzis, Elizabeth Barnes, Ariel Herbert-Voss, William Hebgen\nGuss, Alex Nichol, Alex Paino, Nikolas Tezak, Jie Tang, Igor Babuschkin, Suchir Balaji,\nShantanu Jain, William Saunders, Christopher Hesse, Andrew N. Carr, Jan Leike, Josh\nAchiam, Vedant Misra, Evan Morikawa, Alec Radford, Matthew Knight, Miles Brundage,\nMira Murati, Katie Mayer, Peter Welinder, Bob McGrew, Dario Amodei, Sam McCandlish,\nIlya Sutskever, and Wojciech Zaremba. Evaluating large language models trained on code,\n2021. URL https://arxiv.org/abs/2107.03374.\n16\n\nAakanksha Chowdhery, Sharan Narang, Jacob Devlin, Maarten Bosma, Gaurav Mishra,\nAdam Roberts, Paul Barham, Hyung Won Chung, Charles Sutton, Sebastian Gehrmann,\nParker Schuh, Kensen Shi, Sasha Tsvyashchenko, Joshua Maynez, Abhishek Rao, Parker\nBarnes, Yi Tay, Noam Shazeer, Vinodkumar Prabhakaran, Emily Reif, Nan Du, Ben\nHutchinson, Reiner Pope, James Bradbury, Jacob Austin, Michael Isard, Guy Gur-Ari,\nPengcheng Yin, Toju Duke, Anselm Levskaya, Sanjay Ghemawat, Sunipa Dev, Hen-\nryk Michalewski, Xavier Garcia, Vedant Misra, Kevin Robinson, Liam Fedus, Denny\nZhou, Daphne Ippolito, David Luan, Hyeontaek Lim, Barret Zoph, Alexander Spiri-\ndonov, Ryan Sepassi, David Dohan, Shivani Agrawal, Mark Omernick, Andrew M. Dai,\nThanumalayan Sankaranarayana Pillai, Marie Pellat, Aitor Lewkowycz, Erica Moreira,\nRewon Child, Oleksandr Polozov, Katherine Lee, Zongwei Zhou, Xuezhi Wang, Bren-\nnan Saeta, Mark Diaz, Orhan Firat, Michele Catasta, Jason Wei, Kathy Meier-Hellstern,\nDouglas Eck, Jeff Dean, Slav Petrov, and Noah Fiedel. Palm: Scaling language mod-\neling with pathways. Journal of Machine Learning Research, 24(240):1–113, 2023. URL\nhttp://jmlr.org/papers/v24/22-1144.html.\nKarl Cobbe, Vineet Kosaraju, Mohammad Bavarian, Mark Chen, Heewoo Jun, Lukasz\nKaiser, Matthias Plappert, Jerry Tworek, Jacob Hilton, Reiichiro Nakano, Christopher\nHesse, and John Schulman. Training verifiers to solve math word problems. arXiv preprint\narXiv:2110.14168, 2021.\nTogether Computer. Redpajama: An open source recipe to reproduce llama training dataset,\nApril 2023. URL https://github.com/togethercomputer/RedPajama-Data.\nJohn Dang, Shivalika Singh, Daniel D’souza, Arash Ahmadian, Alejandro Salamanca,\nMadeline Smith, Aidan Peppin, Sungjin Hong, Manoj Govindassamy, Terrence Zhao,\nSandra Kublik, Meor Amer, Viraat Aryabumi, Jon Ander Campos, Yi-Chern Tan, Tom\nKocmi, Florian Strub, Nathan Grinsztajn, Yannis Flet-Berliac, Acyr Locatelli, Hangyu Lin,\nDwarak Talupuru, Bharat Venkitesh, David Cairuz, Bowen Yang, Tim Chung, Wei-Yin\nKo, Sylvie Shang Shi, Amir Shukayev, Sammie Bae, Aleksandra Piktus, Roman Castagn´e,\nFelipe Cruz-Salinas, Eddie Kim, Lucas Crawhall-Stein, Adrien Morisot, Sudip Roy, Phil\nBlunsom, Ivan Zhang, Aidan Gomez, Nick Frosst, Marzieh Fadaee, Beyza Ermis, Ahmet\n¨Ust¨un, and Sara Hooker. Aya expanse: Combining research breakthroughs for a new\nmultilingual frontier, 2024. URL https://arxiv.org/abs/2412.04261.\nDeepSeek-AI. Deepseek llm: Scaling open-source language models with longtermism. arXiv\npreprint arXiv:2401.02954, 2024. URL https://github.com/deepseek-ai/DeepSeek-LLM.\nGuanting Dong, Xiaoshuai Song, Yutao Zhu, Runqi Qiao, Zhicheng Dou, and Ji-Rong Wen.\nToward general instruction-following alignment for retrieval-augmented generation, 2024.\nURL https://arxiv.org/abs/2410.09584.\nKenneth Enevoldsen, Isaac Chung, Imene Kerboua, M´arton Kardos, Ashwin Mathur, David\nStap, Jay Gala, Wissam Siblini, Dominik Krzemi´nski, Genta Indra Winata, Saba Sturua,\nSaiteja Utpala, Mathieu Ciancone, Marion Schaeffer, Gabriel Sequeira, Diganta Misra,\nShreeya Dhakal, Jonathan Rystrøm, Roman Solomatin, ¨Omer C¸ a˘gatan, Akash Kundu,\nMartin Bernstorff, Shitao Xiao, Akshita Sukhlecha, Bhavish Pahwa, Rafał Po´swiata, Kran-\nthi Kiran GV, Shawon Ashraf, Daniel Auras, Bj¨orn Pl¨uster, Jan Philipp Harries, Lo¨ıc\nMagne, Isabelle Mohr, Mariya Hendriksen, Dawei Zhu, Hippolyte Gisserot-Boukhlef,\nTom Aarsen, Jan Kostkan, Konrad Wojtasik, Taemin Lee, Marek ˇSuppa, Crystina Zhang,\nRoberta Rocca, Mohammed Hamdy, Andrianos Michail, John Yang, Manuel Faysse, Alek-\nsei Vatolin, Nandan Thakur, Manan Dey, Dipam Vasani, Pranjal Chitale, Simone Tedeschi,\nNguyen Tai, Artem Snegirev, Michael G¨unther, Mengzhou Xia, Weijia Shi, Xing Han L`u,\nJordan Clive, Gayatri Krishnakumar, Anna Maksimova, Silvan Wehrli, Maria Tikhonova,\nHenil Panchal, Aleksandr Abramov, Malte Ostendorff, Zheng Liu, Simon Clematide,\nLester James Miranda, Alena Fenogenova, Guangyu Song, Ruqiya Bin Safi, Wen-Ding Li,\nAlessia Borghini, Federico Cassano, Hongjin Su, Jimmy Lin, Howard Yen, Lasse Hansen,\nSara Hooker, Chenghao Xiao, Vaibhav Adlakha, Orion Weller, Siva Reddy, and Niklas\nMuennighoff. Mmteb: Massive multilingual text embedding benchmark, 2025. URL\nhttps://arxiv.org/abs/2502.13595.\n17\n\nShahul Es, Jithin James, Luis Espinosa-Anke, and Steven Schockaert. Ragas: Automated\nevaluation of retrieval augmented generation, 2023. URL https://arxiv.org/abs/2309.\n15217.\nMaxim Fishman, Brian Chmiel, Ron Banner, and Daniel Soudry. Scaling FP8 training to\ntrillion-token LLMs. In The Thirteenth International Conference on Learning Representations,\n2025. URL https://openreview.net/forum?id=E1EHO0imOb.\nLeo Gao, John Schulman, and Jacob Hilton. Scaling laws for reward model overoptimization.\nIn Andreas Krause, Emma Brunskill, Kyunghyun Cho, Barbara Engelhardt, Sivan Sabato,\nand Jonathan Scarlett (eds.), Proceedings of the 40th International Conference on Machine\nLearning, volume 202 of Proceedings of Machine Learning Research, pp. 10835–10866. PMLR,\n23–29 Jul 2023. URL https://proceedings.mlr.press/v202/gao23h.html.\nGlaiveAI.\nglaive-function-calling.\nhttps://huggingface.co/datasets/glaiveai/\nglaive-function-calling, 2023.\nGlaiveAI.\nglaive-function-calling-v2.\nhttps://huggingface.co/datasets/glaiveai/\nglaive-function-calling-v2, 2024.\nIBM Granite Team. Granite 3.0 language models, 2024.\nAaron Grattafiori, Abhimanyu Dubey, Abhinav Jauhri, Abhinav Pandey, Abhishek Kadian,\nAhmad Al-Dahle, Aiesha Letman, Akhil Mathur, Alan Schelten, Alex Vaughan, Amy\nYang, Angela Fan, Anirudh Goyal, Anthony Hartshorn, Aobo Yang, Archi Mitra, Archie\nSravankumar, Artem Korenev, Arthur Hinsvark, Arun Rao, Aston Zhang, Aurelien Ro-\ndriguez, Austen Gregerson, Ava Spataru, Baptiste Roziere, Bethany Biron, Binh Tang, Bob-\nbie Chern, Charlotte Caucheteux, Chaya Nayak, Chloe Bi, Chris Marra, Chris McConnell,\nChristian Keller, Christophe Touret, Chunyang Wu, Corinne Wong, Cristian Canton Ferrer,\nCyrus Nikolaidis, Damien Allonsius, Daniel Song, Danielle Pintz, Danny Livshits, Danny\nWyatt, David Esiobu, Dhruv Choudhary, Dhruv Mahajan, Diego Garcia-Olano, Diego\nPerino, Dieuwke Hupkes, Egor Lakomkin, Ehab AlBadawy, Elina Lobanova, Emily Dinan,\nEric Michael Smith, Filip Radenovic, Francisco Guzm´an, Frank Zhang, Gabriel Synnaeve,\nGabrielle Lee, Georgia Lewis Anderson, Govind Thattai, Graeme Nail, Gregoire Mialon,\nGuan Pang, Guillem Cucurell, Hailey Nguyen, Hannah Korevaar, Hu Xu, Hugo Touvron,\nIliyan Zarov, Imanol Arrieta Ibarra, Isabel Kloumann, Ishan Misra, Ivan Evtimov, Jack\nZhang, Jade Copet, Jaewon Lee, Jan Geffert, Jana Vranes, Jason Park, Jay Mahadeokar,\nJeet Shah, Jelmer van der Linde, Jennifer Billock, Jenny Hong, Jenya Lee, Jeremy Fu,\nJianfeng Chi, Jianyu Huang, Jiawen Liu, Jie Wang, Jiecao Yu, Joanna Bitton, Joe Spisak,\nJongsoo Park, Joseph Rocca, Joshua Johnstun, Joshua Saxe, Junteng Jia, Kalyan Vasuden\nAlwala, Karthik Prasad, Kartikeya Upasani, Kate Plawiak, Ke Li, Kenneth Heafield, Kevin\nStone, Khalid El-Arini, Krithika Iyer, Kshitiz Malik, Kuenley Chiu, Kunal Bhalla, Kushal\nLakhotia, Lauren Rantala-Yeary, Laurens van der Maaten, Lawrence Chen, Liang Tan, Liz\nJenkins, Louis Martin, Lovish Madaan, Lubo Malo, Lukas Blecher, Lukas Landzaat, Luke\nde Oliveira, Madeline Muzzi, Mahesh Pasupuleti, Mannat Singh, Manohar Paluri, Marcin\nKardas, Maria Tsimpoukelli, Mathew Oldham, Mathieu Rita, Maya Pavlova, Melanie\nKambadur, Mike Lewis, Min Si, Mitesh Kumar Singh, Mona Hassan, Naman Goyal,\nNarjes Torabi, Nikolay Bashlykov, Nikolay Bogoychev, Niladri Chatterji, Ning Zhang,\nOlivier Duchenne, Onur C¸ elebi, Patrick Alrassy, Pengchuan Zhang, Pengwei Li, Petar\nVasic, Peter Weng, Prajjwal Bhargava, Pratik Dubal, Praveen Krishnan, Punit Singh Koura,\nPuxin Xu, Qing He, Qingxiao Dong, Ragavan Srinivasan, Raj Ganapathy, Ramon Calderer,\nRicardo Silveira Cabral, Robert Stojnic, Roberta Raileanu, Rohan Maheswari, Rohit Gird-\nhar, Rohit Patel, Romain Sauvestre, Ronnie Polidoro, Roshan Sumbaly, Ross Taylor, Ruan\nSilva, Rui Hou, Rui Wang, Saghar Hosseini, Sahana Chennabasappa, Sanjay Singh, Sean\nBell, Seohyun Sonia Kim, Sergey Edunov, Shaoliang Nie, Sharan Narang, Sharath Ra-\nparthy, Sheng Shen, Shengye Wan, Shruti Bhosale, Shun Zhang, Simon Vandenhende,\nSoumya Batra, Spencer Whitman, Sten Sootla, Stephane Collot, Suchin Gururangan, Syd-\nney Borodinsky, Tamar Herman, Tara Fowler, Tarek Sheasha, Thomas Georgiou, Thomas\nScialom, Tobias Speckbacher, Todor Mihaylov, Tong Xiao, Ujjwal Karn, Vedanuj Goswami,\nVibhor Gupta, Vignesh Ramanathan, Viktor Kerkez, Vincent Gonguet, Virginie Do, Vish\nVogeti, V´ıtor Albiero, Vladan Petrovic, Weiwei Chu, Wenhan Xiong, Wenyin Fu, Whitney\n18\n\nMeers, Xavier Martinet, Xiaodong Wang, Xiaofang Wang, Xiaoqing Ellen Tan, Xide Xia,\nXinfeng Xie, Xuchao Jia, Xuewei Wang, Yaelle Goldschlag, Yashesh Gaur, Yasmine Babaei,\nYi Wen, Yiwen Song, Yuchen Zhang, Yue Li, Yuning Mao, Zacharie Delpierre Coudert,\nZheng Yan, Zhengxing Chen, Zoe Papakipos, Aaditya Singh, Aayushi Srivastava, Abha\nJain, Adam Kelsey, Adam Shajnfeld, Adithya Gangidi, Adolfo Victoria, Ahuva Goldstand,\nAjay Menon, Ajay Sharma, Alex Boesenberg, Alexei Baevski, Allie Feinstein, Amanda\nKallet, Amit Sangani, Amos Teo, Anam Yunus, Andrei Lupu, Andres Alvarado, Andrew\nCaples, Andrew Gu, Andrew Ho, Andrew Poulton, Andrew Ryan, Ankit Ramchandani,\nAnnie Dong, Annie Franco, Anuj Goyal, Aparajita Saraf, Arkabandhu Chowdhury, Ashley\nGabriel, Ashwin Bharambe, Assaf Eisenman, Azadeh Yazdan, Beau James, Ben Maurer,\nBenjamin Leonhardi, Bernie Huang, Beth Loyd, Beto De Paola, Bhargavi Paranjape, Bing\nLiu, Bo Wu, Boyu Ni, Braden Hancock, Bram Wasti, Brandon Spence, Brani Stojkovic,\nBrian Gamido, Britt Montalvo, Carl Parker, Carly Burton, Catalina Mejia, Ce Liu, Chang-\nhan Wang, Changkyu Kim, Chao Zhou, Chester Hu, Ching-Hsiang Chu, Chris Cai, Chris\nTindal, Christoph Feichtenhofer, Cynthia Gao, Damon Civin, Dana Beaty, Daniel Kreymer,\nDaniel Li, David Adkins, David Xu, Davide Testuggine, Delia David, Devi Parikh, Diana\nLiskovich, Didem Foss, Dingkang Wang, Duc Le, Dustin Holland, Edward Dowling, Eissa\nJamil, Elaine Montgomery, Eleonora Presani, Emily Hahn, Emily Wood, Eric-Tuan Le, Erik\nBrinkman, Esteban Arcaute, Evan Dunbar, Evan Smothers, Fei Sun, Felix Kreuk, Feng\nTian, Filippos Kokkinos, Firat Ozgenel, Francesco Caggioni, Frank Kanayet, Frank Seide,\nGabriela Medina Florez, Gabriella Schwarz, Gada Badeer, Georgia Swee, Gil Halpern,\nGrant Herman, Grigory Sizov, Guangyi, Zhang, Guna Lakshminarayanan, Hakan Inan,\nHamid Shojanazeri, Han Zou, Hannah Wang, Hanwen Zha, Haroun Habeeb, Harrison\nRudolph, Helen Suk, Henry Aspegren, Hunter Goldman, Hongyuan Zhan, Ibrahim\nDamlaj, Igor Molybog, Igor Tufanov, Ilias Leontiadis, Irina-Elena Veliche, Itai Gat, Jake\nWeissman, James Geboski, James Kohli, Janice Lam, Japhet Asher, Jean-Baptiste Gaya,\nJeff Marcus, Jeff Tang, Jennifer Chan, Jenny Zhen, Jeremy Reizenstein, Jeremy Teboul,\nJessica Zhong, Jian Jin, Jingyi Yang, Joe Cummings, Jon Carvill, Jon Shepard, Jonathan\nMcPhie, Jonathan Torres, Josh Ginsburg, Junjie Wang, Kai Wu, Kam Hou U, Karan Sax-\nena, Kartikay Khandelwal, Katayoun Zand, Kathy Matosich, Kaushik Veeraraghavan,\nKelly Michelena, Keqian Li, Kiran Jagadeesh, Kun Huang, Kunal Chawla, Kyle Huang,\nLailin Chen, Lakshya Garg, Lavender A, Leandro Silva, Lee Bell, Lei Zhang, Liangpeng\nGuo, Licheng Yu, Liron Moshkovich, Luca Wehrstedt, Madian Khabsa, Manav Avalani,\nManish Bhatt, Martynas Mankus, Matan Hasson, Matthew Lennie, Matthias Reso, Maxim\nGroshev, Maxim Naumov, Maya Lathi, Meghan Keneally, Miao Liu, Michael L. Seltzer,\nMichal Valko, Michelle Restrepo, Mihir Patel, Mik Vyatskov, Mikayel Samvelyan, Mike\nClark, Mike Macey, Mike Wang, Miquel Jubert Hermoso, Mo Metanat, Mohammad Raste-\ngari, Munish Bansal, Nandhini Santhanam, Natascha Parks, Natasha White, Navyata\nBawa, Nayan Singhal, Nick Egebo, Nicolas Usunier, Nikhil Mehta, Nikolay Pavlovich\nLaptev, Ning Dong, Norman Cheng, Oleg Chernoguz, Olivia Hart, Omkar Salpekar,\nOzlem Kalinli, Parkin Kent, Parth Parekh, Paul Saab, Pavan Balaji, Pedro Rittner, Philip\nBontrager, Pierre Roux, Piotr Dollar, Polina Zvyagina, Prashant Ratanchandani, Pritish Yu-\nvraj, Qian Liang, Rachad Alao, Rachel Rodriguez, Rafi Ayub, Raghotham Murthy, Raghu\nNayani, Rahul Mitra, Rangaprabhu Parthasarathy, Raymond Li, Rebekkah Hogan, Robin\nBattey, Rocky Wang, Russ Howes, Ruty Rinott, Sachin Mehta, Sachin Siby, Sai Jayesh\nBondu, Samyak Datta, Sara Chugh, Sara Hunt, Sargun Dhillon, Sasha Sidorov, Satadru\nPan, Saurabh Mahajan, Saurabh Verma, Seiji Yamamoto, Sharadh Ramaswamy, Shaun\nLindsay, Shaun Lindsay, Sheng Feng, Shenghao Lin, Shengxin Cindy Zha, Shishir Patil,\nShiva Shankar, Shuqiang Zhang, Shuqiang Zhang, Sinong Wang, Sneha Agarwal, Soji\nSajuyigbe, Soumith Chintala, Stephanie Max, Stephen Chen, Steve Kehoe, Steve Satter-\nfield, Sudarshan Govindaprasad, Sumit Gupta, Summer Deng, Sungmin Cho, Sunny Virk,\nSuraj Subramanian, Sy Choudhury, Sydney Goldman, Tal Remez, Tamar Glaser, Tamara\nBest, Thilo Koehler, Thomas Robinson, Tianhe Li, Tianjun Zhang, Tim Matthews, Timothy\nChou, Tzook Shaked, Varun Vontimitta, Victoria Ajayi, Victoria Montanez, Vijai Mohan,\nVinay Satish Kumar, Vishal Mangla, Vlad Ionescu, Vlad Poenaru, Vlad Tiberiu Mihailescu,\nVladimir Ivanov, Wei Li, Wenchen Wang, Wenwen Jiang, Wes Bouaziz, Will Constable,\nXiaocheng Tang, Xiaojian Wu, Xiaolan Wang, Xilun Wu, Xinbo Gao, Yaniv Kleinman,\nYanjun Chen, Ye Hu, Ye Jia, Ye Qi, Yenda Li, Yilin Zhang, Ying Zhang, Yossi Adi, Youngjin\nNam, Yu, Wang, Yu Zhao, Yuchen Hao, Yundi Qian, Yunlu Li, Yuzi He, Zach Rait, Zachary\n19\n\nDeVito, Zef Rosnbrick, Zhaoduo Wen, Zhenyu Yang, Zhiwei Zhao, and Zhiyu Ma. The\nllama 3 herd of models, 2024. URL https://arxiv.org/abs/2407.21783.\nTom Gunter, Zirui Wang, Chong Wang, Ruoming Pang, Andy Narayanan, Aonan Zhang,\nBowen Zhang, Chen Chen, Chung-Cheng Chiu, David Qiu, Deepak Gopinath, Dian Ang\nYap, Dong Yin, Feng Nan, Floris Weers, Guoli Yin, Haoshuo Huang, Jianyu Wang, Jiarui\nLu, John Peebles, Ke Ye, Mark Lee, Nan Du, Qibin Chen, Quentin Keunebroek, Sam Wise-\nman, Syd Evans, Tao Lei, Vivek Rathod, Xiang Kong, Xianzhi Du, Yanghao Li, Yongqiang\nWang, Yuan Gao, Zaid Ahmed, Zhaoyang Xu, Zhiyun Lu, Al Rashid, Albin Madappally\nJose, Alec Doane, Alfredo Bencomo, Allison Vanderby, Andrew Hansen, Ankur Jain, Anu-\npama Mann Anupama, Areeba Kamal, Bugu Wu, Carolina Brum, Charlie Maalouf, Chin-\nguun Erdenebileg, Chris Dulhanty, Dominik Moritz, Doug Kang, Eduardo Jimenez, Evan\nLadd, Fangping Shi, Felix Bai, Frank Chu, Fred Hohman, Hadas Kotek, Hannah Gillis\nColeman, Jane Li, Jeffrey Bigham, Jeffery Cao, Jeff Lai, Jessica Cheung, Jiulong Shan, Joe\nZhou, John Li, Jun Qin, Karanjeet Singh, Karla Vega, Kelvin Zou, Laura Heckman, Lauren\nGardiner, Margit Bowler, Maria Cordell, Meng Cao, Nicole Hay, Nilesh Shahdadpuri,\nOtto Godwin, Pranay Dighe, Pushyami Rachapudi, Ramsey Tantawi, Roman Frigg, Sam\nDavarnia, Sanskruti Shah, Saptarshi Guha, Sasha Sirovica, Shen Ma, Shuang Ma, Simon\nWang, Sulgi Kim, Suma Jayaram, Vaishaal Shankar, Varsha Paidi, Vivek Kumar, Xin Wang,\nXin Zheng, Walker Cheng, Yael Shrager, Yang Ye, Yasu Tanaka, Yihao Guo, Yunsong\nMeng, Zhao Tang Luo, Zhi Ouyang, Alp Aygar, Alvin Wan, Andrew Walkingshaw, Andy\nNarayanan, Antonie Lin, Arsalan Farooq, Brent Ramerth, Colorado Reed, Chris Bartels,\nChris Chaney, David Riazati, Eric Liang Yang, Erin Feldman, Gabriel Hochstrasser, Guil-\nlaume Seguin, Irina Belousova, Joris Pelemans, Karen Yang, Keivan Alizadeh Vahid,\nLiangliang Cao, Mahyar Najibi, Marco Zuliani, Max Horton, Minsik Cho, Nikhil Bhen-\ndawade, Patrick Dong, Piotr Maj, Pulkit Agrawal, Qi Shan, Qichen Fu, Regan Poston, Sam\nXu, Shuangning Liu, Sushma Rao, Tashweena Heeramun, Thomas Merth, Uday Rayala,\nVictor Cui, Vivek Rangarajan Sridhar, Wencong Zhang, Wenqi Zhang, Wentao Wu, Xingyu\nZhou, Xinwen Liu, Yang Zhao, Yin Xia, Zhile Ren, and Zhongzheng Ren. Apple intelli-\ngence foundation language models, 2024. URL https://arxiv.org/abs/2407.21075.\nShangmin Guo, Biao Zhang, Tianlin Liu, Tianqi Liu, Misha Khalman, Felipe Llinares,\nAlexandre Rame, Thomas Mesnard, Yao Zhao, Bilal Piot, Johan Ferret, and Mathieu\nBlondel.\nDirect language model alignment from online ai feedback, 2024a.\nURL\nhttps://arxiv.org/abs/2402.04792.\nZhen Guo, Adriana Meza Soria, Wei Sun, Yikang Shen, and Rameswar Panda. Api pack:\nA massive multi-programming language dataset for api call generation. arXiv preprint\narXiv:2402.09615, 2024b.\nDan Hendrycks, Collin Burns, Steven Basart, Andy Zou, Mantas Mazeika, Dawn Song, and\nJacob Steinhardt. Measuring massive multitask language understanding. Proceedings of\nthe International Conference on Learning Representations (ICLR), 2021a.\nDan Hendrycks, Collin Burns, Saurav Kadavath, Akul Arora, Steven Basart, Eric Tang,\nDawn Song, and Jacob Steinhardt. Measuring mathematical problem solving with the\nmath dataset. NeurIPS, 2021b.\nGeoffrey Hinton, Oriol Vinyals, and Jeff Dean. Distilling the knowledge in a neural network,\n2015. URL https://arxiv.org/abs/1503.02531.\nJordan Hoffmann, Sebastian Borgeaud, Arthur Mensch, Elena Buchatskaya, Trevor Cai,\nEliza Rutherford, Diego de Las Casas, Lisa Anne Hendricks, Johannes Welbl, Aidan Clark,\nTom Hennigan, Eric Noland, Katie Millican, George van den Driessche, Bogdan Damoc,\nAurelia Guy, Simon Osindero, Karen Simonyan, Erich Elsen, Oriol Vinyals, Jack W. Rae,\nand Laurent Sifre. Training compute-optimal large language models. In Proceedings of the\n36th International Conference on Neural Information Processing Systems, NIPS ’22, Red Hook,\nNY, USA, 2022. Curran Associates Inc. ISBN 9781713871088.\nShengding Hu, Yuge Tu, Xu Han, Ganqu Cui, Chaoqun He, Weilin Zhao, Xiang Long,\nZhi Zheng, Yewei Fang, Yuxiang Huang, Xinrong Zhang, Zhen Leng Thai, Chongyi\n20\n\nWang, Yuan Yao, Chenyang Zhao, Jie Zhou, Jie Cai, Zhongwu Zhai, Ning Ding, Chao\nJia, Guoyang Zeng, dahai li, Zhiyuan Liu, and Maosong Sun. MiniCPM: Unveiling the\npotential of small language models with scalable training strategies. In First Conference on\nLanguage Modeling, 2024. URL https://openreview.net/forum?id=3X2L2TFr0f.\nSiming Huang, Tianhao Cheng, J. K. Liu, Jiaran Hao, Liuyihan Song, Yang Xu, J. Yang, J. H.\nLiu, Chenchen Zhang, Linzheng Chai, Ruifeng Yuan, Zhaoxiang Zhang, Jie Fu, Qian Liu,\nGe Zhang, Zili Wang, Yuan Qi, Yinghui Xu, and Wei Chu. Opencoder: The open cookbook\nfor top-tier code large language models, 2024. URL https://arxiv.org/abs/2411.04905.\nAdam Ibrahim, Benjamin Th´erien, Kshitij Gupta, Mats Leon Richter, Quentin Gregory An-\nthony, Eugene Belilovsky, Timoth´ee Lesort, and Irina Rish. Simple and scalable strategies\nto continually pre-train large language models. Transactions on Machine Learning Research,\n2024. ISSN 2835-8856. URL https://openreview.net/forum?id=DimPeeCxKO.\nINF-Team. Inf’s open-source large language models. 2024. URL https://s.infly.cn/f/\nimg/pdf/inf 34b tech report.pdf.\nAlon Jacovi, Andrew Wang, Chris Alberti, Connie Tao, Jon Lipovetz, Kate Olszewska,\nLukas Haas, Michelle Liu, Nate Keating, Adam Bloniarz, Carl Saroufim, Corey Fry,\nDror Marcus, Doron Kukliansky, Gaurav Singh Tomar, James Swirhun, Jinwei Xing, Lily\nWang, Madhu Gurumurthy, Michael Aaron, Moran Ambar, Rachana Fellinger, Rui Wang,\nZizhao Zhang, Sasha Goldshtein, and Dipanjan Das. The facts grounding leaderboard:\nBenchmarking llms’ ability to ground responses to long-form input, 2025. URL https:\n//arxiv.org/abs/2501.03200.\nYoungjoon Jang, Junyoung Son, and Taemin Lee. KURE: Korea university retrieval embed-\nding model, 2024. URL https://github.com/nlpai-lab/KURE.\nArmand Joulin, Edouard Grave, Piotr Bojanowski, and Tomas Mikolov. Bag of tricks for\nefficient text classification. In Mirella Lapata, Phil Blunsom, and Alexander Koller (eds.),\nProceedings of the 15th Conference of the European Chapter of the Association for Computational\nLinguistics: Volume 2, Short Papers, pp. 427–431, Valencia, Spain, April 2017. Association\nfor Computational Linguistics. URL https://aclanthology.org/E17-2068/.\nSeungjae Jung, Gunsoo Han, Daniel Wontae Nam, and Kyoung-Woon On. Binary classifier\noptimization for large language model alignment, 2024. URL https://arxiv.org/abs/\n2404.04656.\nJared Kaplan, Sam McCandlish, Tom Henighan, Tom B. Brown, Benjamin Chess, Rewon\nChild, Scott Gray, Alec Radford, Jeffrey Wu, and Dario Amodei. Scaling laws for neural\nlanguage models, 2020. URL https://arxiv.org/abs/2001.08361.\nVladimir Karpukhin, Barlas O˘guz, Sewon Min, Patrick Lewis, Ledell Wu, Sergey Edunov,\nDanqi Chen, and Wen-tau Yih. Dense passage retrieval for open-domain question answer-\ning. arXiv preprint arXiv:2004.04906, 2020.\nOmar Khattab and Matei Zaharia.\nColbert: Efficient and effective passage search via\ncontextualized late interaction over bert. In Proceedings of the 43rd International ACM SIGIR\nconference on research and development in Information Retrieval, pp. 39–48, 2020.\nSanghoon Kim, Dahyun Kim, Chanjun Park, Wonsung Lee, Wonho Song, Yunsu Kim,\nHyeonwoo Kim, Yungi Kim, Hyeonju Lee, Jihoo Kim, Changbae Ahn, Seonghoon Yang,\nSukyung Lee, Hyunbyung Park, Gyoungjin Gim, Mikyoung Cha, Hwalsuk Lee, and\nSunghun Kim. SOLAR 10.7B: Scaling large language models with simple yet effective\ndepth up-scaling. In Yi Yang, Aida Davani, Avi Sil, and Anoop Kumar (eds.), Proceedings\nof the 2024 Conference of the North American Chapter of the Association for Computational\nLinguistics: Human Language Technologies (Volume 6: Industry Track), pp. 23–35, Mexico City,\nMexico, June 2024a. Association for Computational Linguistics. doi: 10.18653/v1/2024.\nnaacl-industry.3. URL https://aclanthology.org/2024.naacl-industry.3/.\n21\n\nSeungone Kim, Juyoung Suk, Shayne Longpre, Bill Yuchen Lin, Jamin Shin, Sean Welleck,\nGraham Neubig, Moontae Lee, Kyungjae Lee, and Minjoon Seo. Prometheus 2: An open\nsource language model specialized in evaluating other language models, 2024b. URL\nhttps://arxiv.org/abs/2405.01535.\nSolomon Kullback and R. A. Leibler. On information and sufficiency. Annals of Mathematical\nStatistics, 22:79–86, 1951. URL https://api.semanticscholar.org/CorpusID:120349231.\nShinbok Lee, Gaeun Seo, Daniel Lee, Byeongil Ko, Sunghee Jung, and Myeongcheol Shin.\nFunctionchat-bench: Comprehensive evaluation of language models’ generative capabili-\nties in korean tool-use dialogs. arXiv preprint arXiv:2411.14054, 2024.\nPatrick Lewis, Ethan Perez, Aleksandra Piktus, Fabio Petroni, Vladimir Karpukhin, Naman\nGoyal, Heinrich K¨uttler, Mike Lewis, Wen tau Yih, Tim Rockt¨aschel, Sebastian Riedel, and\nDouwe Kiela. Retrieval-augmented generation for knowledge-intensive nlp tasks, 2021.\nURL https://arxiv.org/abs/2005.11401.\nJeffrey Li, Alex Fang, Georgios Smyrnis, Maor Ivgi, Matt Jordan, Samir Yitzhak Gadre, Hritik\nBansal, Etash Kumar Guha, Sedrick Keh, Kushal Arora, Saurabh Garg, Rui Xin, Niklas\nMuennighoff, Reinhard Heckel, Jean Mercat, Mayee F Chen, Suchin Gururangan, Mitchell\nWortsman, Alon Albalak, Yonatan Bitton, Marianna Nezhurina, Amro Kamal Mohamed\nAbbas, Cheng-Yu Hsieh, Dhruba Ghosh, Joshua P Gardner, Maciej Kilian, Hanlin Zhang,\nRulin Shao, Sarah M Pratt, Sunny Sanyal, Gabriel Ilharco, Giannis Daras, Kalyani Marathe,\nAaron Gokaslan, Jieyu Zhang, Khyathi Chandu, Thao Nguyen, Igor Vasiljevic, Sham M.\nKakade, Shuran Song, Sujay Sanghavi, Fartash Faghri, Sewoong Oh, Luke Zettlemoyer,\nKyle Lo, Alaaeldin El-Nouby, Hadi Pouransari, Alexander T Toshev, Stephanie Wang,\nDirk Groeneveld, Luca Soldaini, Pang Wei Koh, Jenia Jitsev, Thomas Kollar, Alex Dimakis,\nYair Carmon, Achal Dave, Ludwig Schmidt, and Vaishaal Shankar. Datacomp-LM: In\nsearch of the next generation of training sets for language models. In The Thirty-eight\nConference on Neural Information Processing Systems Datasets and Benchmarks Track, 2024.\nURL https://openreview.net/forum?id=CNWdWn47IE.\nMinghao Li, Yingxiu Zhao, Bowen Yu, Feifan Song, Hangyu Li, Haiyang Yu, Zhoujun Li,\nFei Huang, and Yongbin Li. API-bank: A comprehensive benchmark for tool-augmented\nLLMs. In Houda Bouamor, Juan Pino, and Kalika Bali (eds.), Proceedings of the 2023\nConference on Empirical Methods in Natural Language Processing, pp. 3102–3116, Singapore,\nDecember 2023a. Association for Computational Linguistics. doi: 10.18653/v1/2023.\nemnlp-main.187. URL https://aclanthology.org/2023.emnlp-main.187/.\nRaymond Li, Loubna Ben allal, Yangtian Zi, Niklas Muennighoff, Denis Kocetkov, Chenghao\nMou, Marc Marone, Christopher Akiki, Jia LI, Jenny Chim, Qian Liu, Evgenii Zheltonozh-\nskii, Terry Yue Zhuo, Thomas Wang, Olivier Dehaene, Joel Lamy-Poirier, Joao Monteiro,\nNicolas Gontier, Ming-Ho Yee, Logesh Kumar Umapathi, Jian Zhu, Ben Lipkin, Muh-\ntasham Oblokulov, Zhiruo Wang, Rudra Murthy, Jason T Stillerman, Siva Sankalp Patel,\nDmitry Abulkhanov, Marco Zocca, Manan Dey, Zhihan Zhang, Urvashi Bhattacharyya,\nWenhao Yu, Sasha Luccioni, Paulo Villegas, Fedor Zhdanov, Tony Lee, Nadav Timor,\nJennifer Ding, Claire S Schlesinger, Hailey Schoelkopf, Jan Ebert, Tri Dao, Mayank Mishra,\nAlex Gu, Carolyn Jane Anderson, Brendan Dolan-Gavitt, Danish Contractor, Siva Reddy,\nDaniel Fried, Dzmitry Bahdanau, Yacine Jernite, Carlos Mu˜noz Ferrandis, Sean Hughes,\nThomas Wolf, Arjun Guha, Leandro Von Werra, and Harm de Vries. Starcoder: may the\nsource be with you! Transactions on Machine Learning Research, 2023b. ISSN 2835-8856.\nURL https://openreview.net/forum?id=KoFOg41haE. Reproducibility Certification.\nBill Yuchen Lin, Yuntian Deng, Khyathi Chandu, Abhilasha Ravichander, Valentina Pyatkin,\nNouha Dziri, Ronan Le Bras, and Yejin Choi. Wildbench: Benchmarking LLMs with\nchallenging tasks from real users in the wild. In The Thirteenth International Conference on\nLearning Representations, 2025. URL https://openreview.net/forum?id=MKEHCx25xp.\nSheng-Chieh Lin, Luyu Gao, Barlas Oguz, Wenhan Xiong, Jimmy Lin, Wen tau Yih, and\nXilun Chen. Flame: Factuality-aware alignment for large language models, 2024. URL\nhttps://arxiv.org/abs/2405.01525.\n22\n\nJiawei Liu, Chunqiu Steven Xia, Yuyao Wang, and Lingming Zhang. Is your code generated\nby chatGPT really correct? rigorous evaluation of large language models for code gen-\neration. In Thirty-seventh Conference on Neural Information Processing Systems, 2023. URL\nhttps://openreview.net/forum?id=1qvx610Cu7.\nZihan Liu, Wei Ping, Rajarshi Roy, Peng Xu, Chankyu Lee, Mohammad Shoeybi, and\nBryan Catanzaro. Chatqa: Surpassing gpt-4 on conversational qa and rag, 2024a. URL\nhttps://arxiv.org/abs/2401.10225.\nZuxin Liu, Thai Quoc Hoang, Jianguo Zhang, Ming Zhu, Tian Lan, Shirley Kokane, Juntao\nTan, Weiran Yao, Zhiwei Liu, Yihao Feng, Rithesh R N, Liangwei Yang, Silvio Savarese,\nJuan Carlos Niebles, Huan Wang, Shelby Heinecke, and Caiming Xiong. APIGen: Au-\ntomated PIpeline for generating verifiable and diverse function-calling datasets. In The\nThirty-eight Conference on Neural Information Processing Systems Datasets and Benchmarks\nTrack, 2024b. URL https://openreview.net/forum?id=Jfg3vw2bjx.\nIlya Loshchilov and Frank Hutter. SGDR: Stochastic gradient descent with warm restarts. In\nInternational Conference on Learning Representations, 2017. URL https://openreview.net/\nforum?id=Skq89Scxx.\nIlya Loshchilov and Frank Hutter. Decoupled weight decay regularization. In International\nConference on Learning Representations, 2019. URL https://openreview.net/forum?id=\nBkg6RiCqY7.\nAnton Lozhkov, Raymond Li, Loubna Ben Allal, Federico Cassano, Joel Lamy-Poirier,\nNouamane Tazi, Ao Tang, Dmytro Pykhtar, Jiawei Liu, Yuxiang Wei, Tianyang Liu, Max\nTian, Denis Kocetkov, Arthur Zucker, Younes Belkada, Zijian Wang, Qian Liu, Dmitry\nAbulkhanov, Indraneil Paul, Zhuang Li, Wen-Ding Li, Megan Risdal, Jia Li, Jian Zhu,\nTerry Yue Zhuo, Evgenii Zheltonozhskii, Nii Osae Osae Dade, Wenhao Yu, Lucas Krauß,\nNaman Jain, Yixuan Su, Xuanli He, Manan Dey, Edoardo Abati, Yekun Chai, Niklas\nMuennighoff, Xiangru Tang, Muhtasham Oblokulov, Christopher Akiki, Marc Marone,\nChenghao Mou, Mayank Mishra, Alex Gu, Binyuan Hui, Tri Dao, Armel Zebaze, Olivier\nDehaene, Nicolas Patry, Canwen Xu, Julian McAuley, Han Hu, Torsten Scholak, Sebastien\nPaquet, Jennifer Robinson, Carolyn Jane Anderson, Nicolas Chapados, Mostofa Patwary,\nNima Tajbakhsh, Yacine Jernite, Carlos Mu˜noz Ferrandis, Lingming Zhang, Sean Hughes,\nThomas Wolf, Arjun Guha, Leandro von Werra, and Harm de Vries. Starcoder 2 and the\nstack v2: The next generation, 2024. URL https://arxiv.org/abs/2402.19173.\nXueguang Ma, Liang Wang, Nan Yang, Furu Wei, and Jimmy Lin. Fine-tuning llama for\nmulti-stage text retrieval. In Proceedings of the 47th International ACM SIGIR Conference on\nResearch and Development in Information Retrieval, pp. 2421–2425, 2024.\nYu Meng, Mengzhou Xia, and Danqi Chen. SimPO: Simple preference optimization with\na reference-free reward. In The Thirty-eighth Annual Conference on Neural Information\nProcessing Systems, 2024. URL https://openreview.net/forum?id=3Tzcot1LKb.\nAI Meta. Llama 3.2: Revolutionizing edge ai and vision with open, customizable models.\nMeta AI Blog. Retrieved December, 20:2024, 2024.\nNiklas Muennighoff. Sgpt: Gpt sentence embeddings for semantic search. arXiv preprint\narXiv:2202.08904, 2022.\nNiklas Muennighoff, Nouamane Tazi, Loic Magne, and Nils Reimers. MTEB: Massive text\nembedding benchmark. In Andreas Vlachos and Isabelle Augenstein (eds.), Proceedings of\nthe 17th Conference of the European Chapter of the Association for Computational Linguistics,\npp. 2014–2037, Dubrovnik, Croatia, May 2023. Association for Computational Linguistics.\ndoi: 10.18653/v1/2023.eacl-main.148. URL https://aclanthology.org/2023.eacl-main.\n148/.\nSaurav Muralidharan, Sharath Turuvekere Sreenivas, Raviraj Bhuminand Joshi, Marcin\nChochowski, Mostofa Patwary, Mohammad Shoeybi, Bryan Catanzaro, Jan Kautz, and\nPavlo Molchanov. Compact language models via pruning and knowledge distillation. In\nThe Thirty-eighth Annual Conference on Neural Information Processing Systems, 2024.\n23\n\nXuan-Phi Nguyen, Shrey Pandit, Senthil Purushwalkam, Austin Xu, Hailin Chen, Yifei Ming,\nZixuan Ke, Silvio Savarese, Caiming Xong, and Shafiq Joty. Sfr-rag: Towards contextually\nfaithful llms, 2024. URL https://arxiv.org/abs/2409.09916.\nMichael Noukhovitch, Shengyi Huang, Sophie Xhonneux, Arian Hosseini, Rishabh Agarwal,\nand Aaron Courville. Faster, more efficient RLHF through off-policy asynchronous\nlearning. In The Thirteenth International Conference on Learning Representations, 2025. URL\nhttps://openreview.net/forum?id=FhTAG591Ve.\nLong Ouyang, Jeffrey Wu, Xu Jiang, Diogo Almeida, Carroll Wainwright, Pamela Mishkin,\nChong Zhang, Sandhini Agarwal, Katarina Slama, Alex Ray, John Schulman, Jacob Hilton,\nFraser Kelton, Luke Miller, Maddie Simens, Amanda Askell, Peter Welinder, Paul F Chris-\ntiano, Jan Leike, and Ryan Lowe. Training language models to follow instructions with\nhuman feedback. In S. Koyejo, S. Mohamed, A. Agarwal, D. Belgrave, K. Cho, and A. Oh\n(eds.), Advances in Neural Information Processing Systems, volume 35, pp. 27730–27744. Cur-\nran Associates, Inc., 2022. URL https://proceedings.neurips.cc/paper files/paper/\n2022/file/b1efde53be364a73914f58805a001731-Paper-Conference.pdf.\nJeonghwan Park. Logickor. Available at https://github.com/instructkr/LogicKor, 2024.\nGuilherme Penedo, Hynek Kydl´ıˇcek, Loubna Ben allal, Anton Lozhkov, Margaret Mitchell,\nColin Raffel, Leandro Von Werra, and Thomas Wolf. The fineweb datasets: Decanting the\nweb for the finest text data at scale. In The Thirty-eight Conference on Neural Information\nProcessing Systems Datasets and Benchmarks Track, 2024a. URL https://openreview.net/\nforum?id=n6SCkn2QaG.\nGuilherme Penedo, Hynek Kydl´ıˇcek, Vinko Sabolˇcec, Bettina Messmer, Negar Foroutan,\nMartin Jaggi, Leandro von Werra, and Thomas Wolf. Fineweb2: A sparkling update\nwith 1000s of languages, December 2024b. URL https://huggingface.co/datasets/\nHuggingFaceFW/fineweb-2.\nYujia Qin, Shihao Liang, Yining Ye, Kunlun Zhu, Lan Yan, Yaxi Lu, Yankai Lin, Xin Cong,\nXiangru Tang, Bill Qian, Sihan Zhao, Lauren Hong, Runchu Tian, Ruobing Xie, Jie Zhou,\nMark Gerstein, dahai li, Zhiyuan Liu, and Maosong Sun. ToolLLM: Facilitating large\nlanguage models to master 16000+ real-world APIs. In The Twelfth International Conference\non Learning Representations, 2024. URL https://openreview.net/forum?id=dHng2O0Jjr.\nQwen, :, An Yang, Baosong Yang, Beichen Zhang, Binyuan Hui, Bo Zheng, Bowen Yu,\nChengyuan Li, Dayiheng Liu, Fei Huang, Haoran Wei, Huan Lin, Jian Yang, Jianhong Tu,\nJianwei Zhang, Jianxin Yang, Jiaxi Yang, Jingren Zhou, Junyang Lin, Kai Dang, Keming\nLu, Keqin Bao, Kexin Yang, Le Yu, Mei Li, Mingfeng Xue, Pei Zhang, Qin Zhu, Rui Men,\nRunji Lin, Tianhao Li, Tianyi Tang, Tingyu Xia, Xingzhang Ren, Xuancheng Ren, Yang\nFan, Yang Su, Yichang Zhang, Yu Wan, Yuqiong Liu, Zeyu Cui, Zhenru Zhang, and Zihan\nQiu. Qwen2.5 technical report, 2025. URL https://arxiv.org/abs/2412.15115.\nRafael Rafailov, Archit Sharma, Eric Mitchell, Christopher D Manning, Stefano Ermon, and\nChelsea Finn. Direct preference optimization: Your language model is secretly a reward\nmodel. In Thirty-seventh Conference on Neural Information Processing Systems, 2023. URL\nhttps://openreview.net/forum?id=HPuSIXJaa9.\nAbhinav Rastogi, Xiaoxue Zang, Srinivas Sunkara, Raghav Gupta, and Pranav Khaitan.\nTowards scalable multi-domain conversational agents: The schema-guided dialogue\ndataset. In Proceedings of the AAAI conference on artificial intelligence, volume 34, pp. 8689–\n8696, 2020.\nLG AI Research.\nKoMT-bench.\nhttps://huggingface.co/datasets/LGAI-EXAONE/\nKoMT-Bench, 2024a.\nLG AI Research. Exaone 3.5: Series of large language models for real-world use cases. arXiv\npreprint arXiv:2412.04862, 2024b.\n24\n\nNoveen Sachdeva, Benjamin Coleman, Wang-Cheng Kang, Jianmo Ni, Lichan Hong,\nEd Huai hsin Chi, James Caverlee, Julian J. McAuley, and Derek Zhiyuan Cheng.\nHow to train data-efficient llms.\nArXiv, abs/2402.09668, 2024.\nURL https://api.\nsemanticscholar.org/CorpusID:267682083.\nTimo Schick, Jane Dwivedi-Yu, Roberto Dessi, Roberta Raileanu, Maria Lomeli, Eric Hambro,\nLuke Zettlemoyer, Nicola Cancedda, and Thomas Scialom. Toolformer: Language models\ncan teach themselves to use tools. In Thirty-seventh Conference on Neural Information\nProcessing Systems, 2023. URL https://openreview.net/forum?id=Yacmpz84TH.\nZhihong Shao, Damai Dai, Daya Guo, Bo Liu (Benjamin Liu), Zihan Wang, and Hua-\njian Xin. Deepseek-v2: A strong, economical, and efficient mixture-of-experts language\nmodel. ArXiv, abs/2405.04434, 2024a. URL https://api.semanticscholar.org/CorpusID:\n269613809.\nZhihong Shao, Peiyi Wang, Qihao Zhu, Runxin Xu, Junxiao Song, Xiao Bi, Haowei Zhang,\nMingchuan Zhang, Y. K. Li, Y. Wu, and Daya Guo. Deepseekmath: Pushing the limits of\nmathematical reasoning in open language models, 2024b. URL https://arxiv.org/abs/\n2402.03300.\nNoam Shazeer. GLU variants improve transformer. CoRR, abs/2002.05202, 2020. URL\nhttps://arxiv.org/abs/2002.05202.\nGuijin Son, Hanwool Lee, Sungdong Kim, Seungone Kim, Niklas Muennighoff, Taekyoon\nChoi, Cheonbok Park, Kang Min Yoo, and Stella Biderman. Kmmlu: Measuring massive\nmultitask language understanding in korean. CoRR, abs/2402.11548, 2024a. URL https:\n//doi.org/10.48550/arXiv.2402.11548.\nGuijin Son, Hanwool Lee, Suwan Kim, Huiseo Kim, Jae cheol Lee, Je Won Yeom, Jihyu\nJung, Jung woo Kim, and Songseong Kim. HAE-RAE bench: Evaluation of Korean\nknowledge in language models. In Nicoletta Calzolari, Min-Yen Kan, Veronique Hoste,\nAlessandro Lenci, Sakriani Sakti, and Nianwen Xue (eds.), Proceedings of the 2024 Joint\nInternational Conference on Computational Linguistics, Language Resources and Evaluation\n(LREC-COLING 2024), pp. 7993–8007, Torino, Italia, May 2024b. ELRA and ICCL. URL\nhttps://aclanthology.org/2024.lrec-main.704/.\nJacob Mitchell Springer, Suhas Kotha, Daniel Fried, Graham Neubig, and Aditi Raghunathan.\nRepetition improves language model embeddings. arXiv preprint arXiv:2402.15449, 2024.\nSharath Turuvekere Sreenivas, Saurav Muralidharan, Raviraj Joshi, Marcin Chochowski,\nMostofa Patwary, Mohammad Shoeybi, Bryan Catanzaro, Jan Kautz, and Pavlo\nMolchanov. Llm pruning and distillation in practice: The minitron approach. arXiv\npreprint arXiv:2408.11796, 2024.\nDan Su, Kezhi Kong, Ying Lin, Joseph Jennings, Brandon Norick, Markus Kliegl, Mostofa\nPatwary, Mohammad Shoeybi, and Bryan Catanzaro. Nemotron-cc: Transforming com-\nmon crawl into a refined long-horizon pretraining dataset. arXiv preprint arXiv:2412.02595,\n2024.\nQiaoyu Tang, Ziliang Deng, Hongyu Lin, Xianpei Han, Qiao Liang, Boxi Cao, and Le Sun.\nToolalpaca: Generalized tool learning for language models with 3000 simulated cases.\narXiv preprint arXiv:2306.05301, 2023.\nGemma Team, Thomas Mesnard, Cassidy Hardin, Robert Dadashi, Surya Bhupatiraju,\nShreya Pathak, Laurent Sifre, Morgane Rivi`ere, Mihir Sanjay Kale, Juliette Love, Pouya\nTafti, L´eonard Hussenot, Pier Giuseppe Sessa, Aakanksha Chowdhery, Adam Roberts,\nAditya Barua, Alex Botev, Alex Castro-Ros, Ambrose Slone, Am´elie H´eliou, Andrea\nTacchetti, Anna Bulanova, Antonia Paterson, Beth Tsai, Bobak Shahriari, Charline Le\nLan, Christopher A. Choquette-Choo, Cl´ement Crepy, Daniel Cer, Daphne Ippolito,\nDavid Reid, Elena Buchatskaya, Eric Ni, Eric Noland, Geng Yan, George Tucker, George-\nChristian Muraru, Grigory Rozhdestvenskiy, Henryk Michalewski, Ian Tenney, Ivan\nGrishchenko, Jacob Austin, James Keeling, Jane Labanowski, Jean-Baptiste Lespiau, Jeff\n25\n\nStanway, Jenny Brennan, Jeremy Chen, Johan Ferret, Justin Chiu, Justin Mao-Jones, Kather-\nine Lee, Kathy Yu, Katie Millican, Lars Lowe Sjoesund, Lisa Lee, Lucas Dixon, Machel\nReid, Maciej Mikuła, Mateo Wirth, Michael Sharman, Nikolai Chinaev, Nithum Thain,\nOlivier Bachem, Oscar Chang, Oscar Wahltinez, Paige Bailey, Paul Michel, Petko Yotov,\nRahma Chaabouni, Ramona Comanescu, Reena Jana, Rohan Anil, Ross McIlroy, Ruibo\nLiu, Ryan Mullins, Samuel L Smith, Sebastian Borgeaud, Sertan Girgin, Sholto Douglas,\nShree Pandya, Siamak Shakeri, Soham De, Ted Klimenko, Tom Hennigan, Vlad Feinberg,\nWojciech Stokowiec, Yu hui Chen, Zafarali Ahmed, Zhitao Gong, Tris Warkentin, Lu-\ndovic Peran, Minh Giang, Cl´ement Farabet, Oriol Vinyals, Jeff Dean, Koray Kavukcuoglu,\nDemis Hassabis, Zoubin Ghahramani, Douglas Eck, Joelle Barral, Fernando Pereira, Eli\nCollins, Armand Joulin, Noah Fiedel, Evan Senter, Alek Andreev, and Kathleen Ke-\nnealy. Gemma: Open models based on gemini research and technology, 2024a. URL\nhttps://arxiv.org/abs/2403.08295.\nGemma Team, Morgane Riviere, Shreya Pathak, Pier Giuseppe Sessa, Cassidy Hardin,\nSurya Bhupatiraju, L´eonard Hussenot, Thomas Mesnard, Bobak Shahriari, Alexandre\nRam´e, Johan Ferret, Peter Liu, Pouya Tafti, Abe Friesen, Michelle Casbon, Sabela Ramos,\nRavin Kumar, Charline Le Lan, Sammy Jerome, Anton Tsitsulin, Nino Vieillard, Piotr\nStanczyk, Sertan Girgin, Nikola Momchev, Matt Hoffman, Shantanu Thakoor, Jean-Bastien\nGrill, Behnam Neyshabur, Olivier Bachem, Alanna Walton, Aliaksei Severyn, Alicia Par-\nrish, Aliya Ahmad, Allen Hutchison, Alvin Abdagic, Amanda Carl, Amy Shen, Andy\nBrock, Andy Coenen, Anthony Laforge, Antonia Paterson, Ben Bastian, Bilal Piot, Bo Wu,\nBrandon Royal, Charlie Chen, Chintu Kumar, Chris Perry, Chris Welty, Christopher A.\nChoquette-Choo, Danila Sinopalnikov, David Weinberger, Dimple Vijaykumar, Dominika\nRogozi´nska, Dustin Herbison, Elisa Bandy, Emma Wang, Eric Noland, Erica Moreira, Evan\nSenter, Evgenii Eltyshev, Francesco Visin, Gabriel Rasskin, Gary Wei, Glenn Cameron, Gus\nMartins, Hadi Hashemi, Hanna Klimczak-Pluci´nska, Harleen Batra, Harsh Dhand, Ivan\nNardini, Jacinda Mein, Jack Zhou, James Svensson, Jeff Stanway, Jetha Chan, Jin Peng\nZhou, Joana Carrasqueira, Joana Iljazi, Jocelyn Becker, Joe Fernandez, Joost van Amers-\nfoort, Josh Gordon, Josh Lipschultz, Josh Newlan, Ju yeong Ji, Kareem Mohamed, Kar-\ntikeya Badola, Kat Black, Katie Millican, Keelin McDonell, Kelvin Nguyen, Kiranbir\nSodhia, Kish Greene, Lars Lowe Sjoesund, Lauren Usui, Laurent Sifre, Lena Heuermann,\nLeticia Lago, Lilly McNealus, Livio Baldini Soares, Logan Kilpatrick, Lucas Dixon, Lu-\nciano Martins, Machel Reid, Manvinder Singh, Mark Iverson, Martin G¨orner, Mat Velloso,\nMateo Wirth, Matt Davidow, Matt Miller, Matthew Rahtz, Matthew Watson, Meg Risdal,\nMehran Kazemi, Michael Moynihan, Ming Zhang, Minsuk Kahng, Minwoo Park, Mofi\nRahman, Mohit Khatwani, Natalie Dao, Nenshad Bardoliwalla, Nesh Devanathan, Neta\nDumai, Nilay Chauhan, Oscar Wahltinez, Pankil Botarda, Parker Barnes, Paul Barham,\nPaul Michel, Pengchong Jin, Petko Georgiev, Phil Culliton, Pradeep Kuppala, Ramona\nComanescu, Ramona Merhej, Reena Jana, Reza Ardeshir Rokni, Rishabh Agarwal, Ryan\nMullins, Samaneh Saadat, Sara Mc Carthy, Sarah Cogan, Sarah Perrin, S´ebastien M. R.\nArnold, Sebastian Krause, Shengyang Dai, Shruti Garg, Shruti Sheth, Sue Ronstrom, Susan\nChan, Timothy Jordan, Ting Yu, Tom Eccles, Tom Hennigan, Tomas Kocisky, Tulsee Doshi,\nVihan Jain, Vikas Yadav, Vilobh Meshram, Vishal Dharmadhikari, Warren Barkley, Wei Wei,\nWenming Ye, Woohyun Han, Woosuk Kwon, Xiang Xu, Zhe Shen, Zhitao Gong, Zichuan\nWei, Victor Cotruta, Phoebe Kirk, Anand Rao, Minh Giang, Ludovic Peran, Tris Warkentin,\nEli Collins, Joelle Barral, Zoubin Ghahramani, Raia Hadsell, D. Sculley, Jeanine Banks,\nAnca Dragan, Slav Petrov, Oriol Vinyals, Jeff Dean, Demis Hassabis, Koray Kavukcuoglu,\nClement Farabet, Elena Buchatskaya, Sebastian Borgeaud, Noah Fiedel, Armand Joulin,\nKathleen Kenealy, Robert Dadashi, and Alek Andreev. Gemma 2: Improving open lan-\nguage models at a practical size, 2024b. URL https://arxiv.org/abs/2408.00118.\nHoang Tran, Chris Glaze, and Braden Hancock. Iterative dpo alignment. Technical report,\nSnorkel AI, 2023.\nAshish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N\nGomez, Ł ukasz Kaiser, and Illia Polosukhin.\nAttention is all you need.\nIn\nAdvances in Neural Information Processing Systems, volume 30. Curran Associates,\nInc., 2017.\nURL https://proceedings.neurips.cc/paper files/paper/2017/file/\n3f5ee243547dee91fbd053c1c4a845aa-Paper.pdf.\n26\n\nAlan Wake, Bei Chen, C. X. Lv, Chao Li, Chengen Huang, Chenglin Cai, Chujie Zheng,\nDaniel Cooper, Fan Zhou, Feng Hu, Ge Zhang, Guoyin Wang, Heng Ji, Howard Qiu,\nJiangcheng Zhu, Jun Tian, Katherine Su, Lihuan Zhang, Liying Li, Ming Song, Mou Li,\nPeng Liu, Qicheng Hu, Shawn Wang, Shijun Zhou, Shiming Yang, Shiyong Li, Tianhang\nZhu, Wen Xie, Wenhao Huang, Xiang He, Xiaobo Chen, Xiaohui Hu, Xiaoyi Ren, Xinyao\nNiu, Yanpeng Li, Yongke Zhao, Yongzhen Luo, Yuchi Xu, Yuxuan Sha, Zhaodong Yan,\nZhiyuan Liu, Zirui Zhang, and Zonghong Dai. Yi-lightning technical report, 2025. URL\nhttps://arxiv.org/abs/2412.01253.\nLiang Wang, Nan Yang, Xiaolong Huang, Linjun Yang, Rangan Majumder, and Furu Wei.\nImproving text embeddings with large language models. arXiv preprint arXiv:2401.00368,\n2023.\nRuizhe Wang, Yeyun Gong, Xiao Liu, Guoshuai Zhao, Ziyue Yang, Baining Guo, Zhengjun\nZha, and Peng Cheng. Optimizing large language model training using fp4 quantization,\n2025. URL https://arxiv.org/abs/2501.17116.\nJason Wei, Xuezhi Wang, Dale Schuurmans, Maarten Bosma, brian ichter, Fei Xia, Ed H.\nChi, Quoc V Le, and Denny Zhou. Chain of thought prompting elicits reasoning in large\nlanguage models. In Alice H. Oh, Alekh Agarwal, Danielle Belgrave, and Kyunghyun Cho\n(eds.), Advances in Neural Information Processing Systems, 2022. URL https://openreview.\nnet/forum?id= VjQlMeSB J.\nMitchell Wortsman, Peter J Liu, Lechao Xiao, Katie E Everett, Alexander A Alemi, Ben Adlam,\nJohn D Co-Reyes, Izzeddin Gur, Abhishek Kumar, Roman Novak, Jeffrey Pennington,\nJascha Sohl-Dickstein, Kelvin Xu, Jaehoon Lee, Justin Gilmer, and Simon Kornblith. Small-\nscale proxies for large-scale transformer training instabilities. In The Twelfth International\nConference on Learning Representations, 2024. URL https://openreview.net/forum?id=\nd8w0pmvXbZ.\nWei Xiong, Hanze Dong, Chenlu Ye, Ziqi Wang, Han Zhong, Heng Ji, Nan Jiang, and Tong\nZhang. Iterative preference learning from human feedback: Bridging theory and practice\nfor RLHF under KL-constraint. In Ruslan Salakhutdinov, Zico Kolter, Katherine Heller,\nAdrian Weller, Nuria Oliver, Jonathan Scarlett, and Felix Berkenkamp (eds.), Proceedings of\nthe 41st International Conference on Machine Learning, volume 235 of Proceedings of Machine\nLearning Research, pp. 54715–54754. PMLR, 21–27 Jul 2024. URL https://proceedings.\nmlr.press/v235/xiong24a.html.\nRan Xu, Wenqi Shi, Yue Yu, Yuchen Zhuang, Yanqiao Zhu, May Dongmei Wang, Joyce C.\nHo, Chao Zhang, and Carl Yang. BMRetriever: Tuning large language models as better\nbiomedical text retrievers. In Yaser Al-Onaizan, Mohit Bansal, and Yun-Nung Chen\n(eds.), Proceedings of the 2024 Conference on Empirical Methods in Natural Language Processing,\npp. 22234–22254, Miami, Florida, USA, November 2024. Association for Computational\nLinguistics. doi: 10.18653/v1/2024.emnlp-main.1241. URL https://aclanthology.org/\n2024.emnlp-main.1241/.\nJiawei Zhao, Zhenyu Zhang, Beidi Chen, Zhangyang Wang, Anima Anandkumar, and\nYuandong Tian. GaLore: Memory-efficient LLM training by gradient low-rank projection.\nIn Ruslan Salakhutdinov, Zico Kolter, Katherine Heller, Adrian Weller, Nuria Oliver,\nJonathan Scarlett, and Felix Berkenkamp (eds.), Proceedings of the 41st International Confer-\nence on Machine Learning, volume 235 of Proceedings of Machine Learning Research, pp. 61121–\n61143. PMLR, 21–27 Jul 2024. URL https://proceedings.mlr.press/v235/zhao24s.html.\nLianmin Zheng, Wei-Lin Chiang, Ying Sheng, Siyuan Zhuang, Zhanghao Wu, Yonghao\nZhuang, Zi Lin, Zhuohan Li, Dacheng Li, Eric Xing, Hao Zhang, Joseph E. Gonzalez, and\nIon Stoica. Judging LLM-as-a-judge with MT-bench and chatbot arena. In Thirty-seventh\nConference on Neural Information Processing Systems Datasets and Benchmarks Track, 2023.\nURL https://openreview.net/forum?id=uccHPGDlao.\nJeffrey Zhou, Tianjian Lu, Swaroop Mishra, Siddhartha Brahma, Sujoy Basu, Yi Luan, Denny\nZhou, and Le Hou. Instruction-following evaluation for large language models, 2023.\nURL https://arxiv.org/abs/2311.07911.\n27\n\nAlex Zhuang, Ge Zhang, Tianyu Zheng, Xinrun Du, Junjie Wang, Weiming Ren, Wenhao\nHuang, Jie Fu, Xiang Yue, and Wenhu Chen. StructLM: Towards building generalist\nmodels for structured knowledge grounding. In First Conference on Language Modeling,\n2024. URL https://openreview.net/forum?id=EKBPn7no4y.\n28\n\nA\nAppendix\nA.1\nComparison between pre-trained models and post-trained models\nModels\nTokens\nCategory\nMMLU\nKMMLU\nHAE-RAE\nHumanEval\nMBPP\nGSM8K\n5-shot\n5-shot\n5-shot\n0-shot\n3-shot\n5-shot\nKanana Flag 32.5B\n3.2T\nbase\n77.68\n62.10\n90.47\n51.22\n63.40\n70.05\ninstruct\n77.84\n62.08\n89.37\n64.63\n73.00\n84.08\nLlama 3.1 70B\n15T\nbase\n78.93\n53.00\n76.35\n57.32\n66.60\n81.73\ninstruct\n82.42\n52.80\n76.08\n78.05\n70.40\n86.66\nQwen2.5 32B\n18T\nbase\n83.10\n63.15\n75.16\n50.00\n73.40\n82.41\ninstruct\n83.41\n61.20\n74.61\n54.88\n73.00\n76.27\nGemma 2 27B\n13T\nbase\n75.45\n51.16\n69.11\n51.22\n64.60\n74.37\ninstruct\n76.39\n51.49\n68.84\n71.34\n66.20\n84.46\nEXAONE-3.5-32B\n6.5T\ninstruct\n72.68\n46.36\n82.22\n74.39\n67.80\n55.50\nAya Expanse 32B\n-\ninstruct\n74.52\n49.57\n80.66\n12.20\n60.40\n85.97\nKanana Essence 9.8B\n3.2T\nbase\n67.61\n50.57\n84.97\n40.24\n53.60\n63.61\ninstruct\n66.45\n49.95\n82.95\n61.59\n51.60\n76.04\nLlama 3.1 8B\n15T\nbase\n65.18\n41.02\n61.78\n35.37\n48.60\n50.87\ninstruct\n68.17\n41.22\n64.44\n59.76\n58.00\n69.52\nQwen2.5 7B\n18T\nbase\n74.19\n51.68\n67.46\n56.71\n63.20\n83.85\ninstruct\n74.23\n50.13\n65.72\n65.85\n31.60\n77.56\nGemma 2 9B\n8T†\nbase\n70.34\n48.18\n66.18\n37.20\n53.60\n68.16\ninstruct\n72.30\n46.56\n66.73\n56.10\n57.60\n80.12\nEXAONE-3.5-7.8B\n9T\ninstruct\n65.36\n45.30\n77.54\n70.73\n61.60\n64.67\nAya Expanse 8B\n-\ninstruct\n62.52\n40.11\n71.95\n7.93\n47.40\n75.97\nKanana Nano 2.1B\n300B†\nbase\n54.83\n44.80\n77.09\n31.10\n46.20\n46.32\ninstruct\n53.67\n42.92\n77.17\n54.88\n55.00\n64.37\nLlama 3.2 3B\n9T†‡\nbase\n56.40\n35.57\n47.66\n25.61\n39.00\n27.37\ninstruct\n60.60\n35.44\n48.21\n49.39\n49.00\n58.76\nQwen2.5 3B\n18T\nbase\n65.57\n45.28\n61.32\n37.80\n55.60\n69.07\ninstruct\n66.47\n44.51\n60.77\n50.61\n54.60\n11.37\nGemma 2 2B\n2T†\nbase\n52.89\n30.67\n45.55\n20.12\n28.20\n24.72\ninstruct\n57.04\n33.48\n49.77\n23.78\n37.80\n44.05\nEXAONE-3.5-2.4B\n6.5T\ninstruct\n59.27\n43.58\n68.65\n63.41\n58.40\n53.07\nTable 13: † For distilled models, distillation tokens are only counted ‡ Information from https:\n//huggingface.co/meta-llama/Llama-3.2-3B\nA.2\nSuboptimal extraction of open-source datasets\n29\n\nExample of suboptimal extraction from arXiv\n(...)\nIn this work, we use sine and cosine functions of different frequencies:\n\\begin{align*}\nPE_{(pos,2i)} = sin(pos / 10000ˆ{2i/d_{\\text{model}}}) \\\\\nPE_{(pos,2i+1)} = cos(pos / 10000ˆ{2i/d_{\\text{model}}})\n\\end{align*}\nwhere $pos$ is the position and $i$ is the dimension.\nThat is, each\ndimension of the positional encoding corresponds to a sinusoid.\nThe\nwavelengths form a geometric progression from $2\\pi$ to $10000 \\cdot 2\\\npi$.\nWe chose this function because we hypothesized it would allow the\nmodel to easily learn to attend by relative positions, since for any\nfixed offset $k$, $PE_{pos+k}$ can be represented as a linear function of\n$PE_{pos}$.\nWe also experimented with using learned positional embeddings \\citep{\nJonasFaceNet2017} instead, and found that the two versions produced\nnearly identical results (see Table˜\\ref{tab:variations} row (E)).\nWe\nchose the sinusoidal version because it may allow the model to\nextrapolate to sequence lengths longer than the ones encountered during\ntraining.\n\\section{Introduction}\n\\input{introduction}\n\\section{Background}\n\\input{background}\n\\section{Model Architecture}\n\\input{model_architecture}\n\\section{Why Self-Attention}\n\\input{why_self_attention}\n\\section{Training}\n\\input{training}\n\\section{Results} \\label{sec:results}\n\\input{results}\n\\section{Conclusion}\nIn this work, we presented the Transformer, the first sequence\ntransduction model based entirely on attention, replacing the recurrent\nlayers most commonly used in encoder-decoder architectures with multi-\nheaded self-attention.\n(...)\nFigure 7: Example of suboptimal extraction from arXiv subset of Computer (2023). The original content\nis from Vaswani et al. (2017).\n30\n\nExample of suboptimal extraction from Wikipedia\nᄇ\nᅡ\nᆯᄉ\nᅢ\nᆼᄋ\nᅯ\nᆫᄋ\nᅵ\nᆫ\nᄒ\nᅬᄌ\nᅥ\nᆫᄌ\nᅪᄑ\nᅭᄀ\nᅨ\nᄒ\nᅬᄌ\nᅥ\nᆫᄌ\nᅪᄑ\nᅭᄀ\nᅨᄌ\nᅪᄑ\nᅭᄀ\nᅨx, y, zᄋ\nᅪᄌ\nᅪᄑ\nᅭᄀ\nᅨx’, y’, z’ᄋ\nᅳ\nᆯᄇ\nᅩᄌ\nᅡᄃ\nᅮᄌ\nᅪᄑ\nᅭᄀ\nᅨᄋ\nᅴᄋ\nᅯ\nᆫᄌ\nᅥ\nᆷᄋ\nᅳ\nᆫᄀ\nᅡ\nᇀᄃ\nᅡ.\nᄀ\nᅡ\nᆨᄀ\nᅡ\nᆨᄋ\nᅴᄀ\nᅧ\nᆼᄋ\nᅮᄋ\nᅦᄃ\nᅢᄒ\nᅢᄇ\nᅦ\nᆨᄐ\nᅥ.ᄋ\nᅳ\nᆫᄃ\nᅮᄌ\nᅪᄑ\nᅭᄀ\nᅨᄋ\nᅦᄉ\nᅥᄃ\nᅡᄋ\nᅳ\nᆷᄀ\nᅪᄀ\nᅡ\nᇀᄋ\nᅵᄑ\nᅭᄉ\nᅵᄃ\nᅬ\nᆫᄃ\nᅡ.\n. (x, y, z ᄌ\nᅪᄑ\nᅭ)\n. (x’, y’, z’ ᄌ\nᅪᄑ\nᅭᄀ\nᅨ)\nᄇ\nᅦ\nᆨᄐ\nᅥᄋ\nᅴᄂ\nᅢᄌ\nᅥ\nᆨᄋ\nᅳ\nᆯᄋ\nᅵᄋ\nᅭ\nᆼᄒ\nᅢx, y, zᄅ\nᅳ\nᆯ(.), (.), (.)ᄋ\nᅳᄅ\nᅩᄑ\nᅭᄒ\nᅧ\nᆫᄒ\nᅡ\nᆯᄉ\nᅮᄋ\nᅵ\nᆻᄃ\nᅡ. ᄂ\nᅢᄌ\nᅥ\nᆨᄋ\nᅴᄇ\nᅡ\nᆼᄇ\nᅥ\nᆸᄋ\nᅳ\nᆫ\nᄃ\nᅡᄋ\nᅳ\nᆷᄀ\nᅪᄀ\nᅡ\nᇀᄃ\nᅡ.\n.\n.\n.\nᄋ\nᅳᄅ\nᅩᄑ\nᅭᄒ\nᅧ\nᆫᄃ\nᅬᄂ\nᅳ\nᆫᄀ\nᅥ\nᆺᄋ\nᅳ\nᆯᄒ\nᅪ\nᆨᄋ\nᅵ\nᆫᄒ\nᅡ\nᆯᄉ\nᅮᄋ\nᅵ\nᆻᄃ\nᅡ.\n(a) Open-source\nExample of improved extraction from Wikipedia\n## ᄇ\nᅡ\nᆯᄉ\nᅢ\nᆼᄋ\nᅯ\nᆫᄋ\nᅵ\nᆫ\n### ᄒ\nᅬᄌ\nᅥ\nᆫᄌ\nᅪᄑ\nᅭᄀ\nᅨ\n#### ᄒ\nᅬᄌ\nᅥ\nᆫᄌ\nᅪᄑ\nᅭᄀ\nᅨ\nᄌ\nᅪᄑ\nᅭᄀ\nᅨx, y, zᄋ\nᅪᄌ\nᅪᄑ\nᅭᄀ\nᅨx’, y’, z’ᄋ\nᅳ\nᆯᄇ\nᅩᄌ\nᅡᄃ\nᅮᄌ\nᅪᄑ\nᅭᄀ\nᅨᄋ\nᅴᄋ\nᅯ\nᆫᄌ\nᅥ\nᆷᄋ\nᅳ\nᆫᄀ\nᅡ\nᇀᄃ\nᅡ. ᄀ\nᅡ\nᆨᄀ\nᅡ\nᆨᄋ\nᅴᄀ\nᅧ\nᆼᄋ\nᅮᄋ\nᅦᄃ\nᅢᄒ\nᅢ\nᄇ\nᅦ\nᆨᄐ\nᅥr.ᄋ\nᅳ\nᆫᄃ\nᅮᄌ\nᅪᄑ\nᅭᄀ\nᅨᄋ\nᅦᄉ\nᅥᄃ\nᅡᄋ\nᅳ\nᆷᄀ\nᅪᄀ\nᅡ\nᇀᄋ\nᅵᄑ\nᅭᄉ\nᅵᄃ\nᅬ\nᆫᄃ\nᅡ.\nr = x ˆx + y ˆy + zˆz. (x, y, z ᄌ\nᅪᄑ\nᅭ)\nr = x′ ˆx′ + y′ ˆy′ + z′ ˆz′. (x’, y’, z’ ᄌ\nᅪᄑ\nᅭᄀ\nᅨ)\nᄇ\nᅦ\nᆨᄐ\nᅥᄋ\nᅴᄂ\nᅢᄌ\nᅥ\nᆨᄋ\nᅳ\nᆯᄋ\nᅵᄋ\nᅭ\nᆼᄒ\nᅢx, y, zᄅ\nᅳ\nᆯ( x′, ˆx′, ˆx.), ( y′, ˆy′, ˆy.), ( z′, ˆz′, ˆz.)ᄋ\nᅳᄅ\nᅩᄑ\nᅭᄒ\nᅧ\nᆫᄒ\nᅡ\nᆯᄉ\nᅮ\nᄋ\nᅵ\nᆻᄃ\nᅡ. ᄂ\nᅢᄌ\nᅥ\nᆨᄋ\nᅴᄇ\nᅡ\nᆼᄇ\nᅥ\nᆸᄋ\nᅳ\nᆫᄃ\nᅡᄋ\nᅳ\nᆷᄀ\nᅪᄀ\nᅡ\nᇀᄃ\nᅡ.\nr ˆx = x = (x′ ˆx′ + y′ ˆy′ + z′ ˆz′)( ˆx) = x′( ˆx′ ˆx) + y′( ˆy′ ˆx) + z′(ˆz′ ˆx).\nr ˆy = y = (x′ ˆx′ + y′ ˆy′ + z′ ˆz′)( ˆy) = x′( ˆx′ ˆy) + y′( ˆy′ ˆy) + z′(ˆz′ ˆy).\nrˆz = z = (x′ ˆx′ + y′ ˆy′ + z′ ˆz′)(ˆz) = x′( ˆx′ ˆz) + y′( ˆy′ ˆz) + z′(ˆz′ ˆz).\nᄋ\nᅳᄅ\nᅩᄑ\nᅭᄒ\nᅧ\nᆫᄃ\nᅬᄂ\nᅳ\nᆫᄀ\nᅥ\nᆺᄋ\nᅳ\nᆯᄒ\nᅪ\nᆨᄋ\nᅵ\nᆫᄒ\nᅡ\nᆯᄉ\nᅮᄋ\nᅵ\nᆻᄃ\nᅡ.\n(b) Improved\nFigure 8: Example of suboptimal and our improved extraction from open-source Wikipedia dataset\n(https://huggingface.co/datasets/wikimedia/wikipedia). The original content is from the Korean\nWikipedia article on the Coriolis effect.\n31\n\nA.3\nDetails of pre-training from scratch\nTo control the effects of architecture and tokenization, and to focus on improving the\ndata scaling curve, we adopt the architecture and tokenizer of Llama 3 (Grattafiori et al.,\n2024). Note that while we use the Llama 3 tokenizer, we do not utilize either the weights\nor the outputs of Llama 3 during the training of Kanana. Based on the observations of\nWortsman et al. (2024), we adopt independent weight decay, which follows the original\nproposal of Loshchilov & Hutter (2019) and differs from the PyTorch implementation, and a\nz-loss (Chowdhery et al., 2023) to obtain effective and stable training across various model\nscales. We set an independent weight decay of 1 × 10−4 and a z-loss coefficient of 5 × 10−6,\nregardless of model size. For peak learning rates, learning rate schedulers, and batch sizes,\nthe hyperparameter scaling law and multi-step scheduler from DeepSeek-AI (2024) are\nemployed.\nA.4\nDetails of pruning and distillation\nThe hyperparameters differ from those used in pre-training from scratch. We apply a cosine\nlearning rate schedule (Loshchilov & Hutter, 2017) with an initial learning rate of 1.2 × 10−4,\nbatch size of 512, sequence length of 8192, and a warmup phase of 100 steps. Following\nthe recommendation of Minitron (Muralidharan et al., 2024; Sreenivas et al., 2024), we\nemploy KL divergence (Kullback & Leibler, 1951) on final logits as the sole loss function.\nAdditionally, we conclude training early during ablation studies, as pruned models quickly\nregain performance and the ranking of ablation options rapidly stabilizes.\nModels\nMMLU\nKMMLU\nHAE-RAE\nHumanEval\nMBPP\nGSM8K\nAvg\n5-shot\n5-shot\n5-shot\n0-shot\n3-shot\n5-shot\nKanana 4.5B\n59.74\n48.09\n82.58\n34.76\n48.60\n57.01\n55.13\nKanana 3B\n58.21\n47.55\n79.19\n34.15\n45.90\n53.75\n53.13\nLlama 3.2 3B\n56.40\n35.57\n47.66\n25.61\n39\n27.37\n38.60\nQwen2.5 3B\n65.57\n45.28\n61.32\n37.80\n55.60\n69.07\n55.77\nKanana 2.1B\n54.83\n44.80\n77.09\n31.10\n46.20\n46.32\n50.06\nKanana 1.3B\n53.55\n39.91\n72.59\n28.05\n39.60\n36.01\n44.95\nGemma 2 2B\n52.89\n30.67\n45.55\n20.12\n28.20\n24.72\n33.69\nSmolLM2-1.7B\n50.08\n24.36\n30.52\n0.61\n34.00\n32.00\n28.60\nQwen2.5 1.5B\n60.86\n36.63\n49.68\n37.20\n44.00\n62.09\n48.41\nLlama 3.2 1B\n31.51\n26.46\n23.10\n18.90\n27.60\n6.14\n22.29\nKanana 635M\n46.28\n34.60\n62.69\n23.17\n31.40\n19.26\n36.23\nKanana 385M\n41.16\n31.70\n47.94\n18.90\n24.00\n10.83\n29.09\nKanana 192M\n26.11\n30.16\n19.71\n12.80\n12.40\n2.43\n17.27\nQwen2.5 0.5B\n47.59\n31.79\n31.44\n28.66\n31.00\n35.10\n34.26\nSmolLM2-360M\n24.84\n15.14\n21.26\n0.00\n19.00\n3.94\n14.03\nSmolLM2-135M\n25.28\n25.73\n20.71\n0.00\n3.40\n1.29\n12.74\nTable 14: Performance of our models obtained with iterative pruning & distillation, compared to\nsimilar-sized open-source base models.\nGQA alignment\nSwiglu importance\nAggregation\nAvg\nLayer\nBatch\nSequence\n✓\nintermediate states\nsum\nl2norm\navg\n36.41\n✗\nintermediate states\nsum\nl2norm\navg\n20.13\n✓\navg of gate, up states\nsum\nl2norm\navg\n36.04\n✓\nintermediate states\n✗\nl2norm\navg\n13.81\n✓\nintermediate states\nsum\navg\navg\n35.65\n✓\nintermediate states\nsum\nl2norm\nl2norm\n34.25\nTable 15: Ablation study on importance scoring details, followed by training the same 1.3B architecture\nwith 25B tokens.\n32\n\nMulti-Head Attention Pruning\nQueries\nValues\nKeys\nQueries\nValues\nKeys\nGrouped-Query Attention Pruning\nQueries\nValues\nKeys\nQueries\nValues\nKeys\nFigure 9: Illustration of ensuring query-key-value alignment in GQA pruning.\nHidden\nIntermediate\nQuery heads\nNon-embedding\nAvg\nparameters\n1280\n5120\n24\n0.96B\n32.81\n1280\n5760\n16\n0.96B\n28.71\n1280\n5760\n24\n1.04B\n34.27\n1536\n4608\n16\n0.98B\n31.99\n1536\n4608\n24\n1.08B\n35.10\n1536\n5376\n8\n0.99B\n24.39\n1536\n5376\n16\n1.09B\n32.39\n1536\n6144\n8\n1.11B\n25.91\n1024\n3072\n24−→16\n1.08B−→504M\n20.85\n1024\n3072\n16−→16\n1.09B−→504M\n21.78\nTable 16: Ablation study on model architectures, using 25B training tokens.\nEmbedding\nMMLU\nKMMLU\nHAERAE\nHumanEval\nMBPP\nGSM8K\nAvg\n5-shot\n5-shot\n5-shot\n0-shot\n3-shot\n5-shot\ntied\n49.07\n40.41\n70.49\n30.49\n40.60\n38.21\n44.88\nuntied\n49.88\n39.61\n70.21\n29.88\n40.20\n36.92\n44.45\nTable 17: Ablation study on tying input and output embeddings by averaging, using 63B training\ntokens. The rest of the architecture remains unchanged, with 1.86B non-embedding parameters.\nModels\nData\nMMLU\nKMMLU\nHAERAE\nHumanEval\nMBPP\nGSM8K\nAvg\n5-shot\n5-shot\n5-shot\n0-shot\n3-shot\n5-shot\n1.3B\nstage2\n39.52\n26.65\n49.77\n25.00\n32.40\n21.00\n32.39\n1.3B\nstage2 en++\n44.00\n33.90\n62.42\n23.78\n33.80\n20.55\n36.41\nTable 18: Ablation study on distillation data, using 25B training tokens.\n33\n\nMMLU prompt (0-shot CoT)\nThe following are multiple choice questions about {mmlu subject}. Summarize your\nreasoning concisely, then conclude with ”Therefore, the answer is: X” where X is one\nof A, B, C, or D.\nQuestion: {question}\nA. {choice A}\nB. {choice B}\nC. {choice C}\nD. {choice D}\n(a) MMLU prompt\nKMMLU prompt (0-shot CoT)\nᄃ\nᅡᄋ\nᅳ\nᆷᄋ\nᅳ\nᆫ{kmmlu subject}ᄋ\nᅦᄀ\nᅪ\nᆫᄒ\nᅡ\nᆫᄀ\nᅢ\nᆨᄀ\nᅪ\nᆫᄉ\nᅵ\nᆨᄆ\nᅮ\nᆫᄌ\nᅦᄋ\nᅵ\nᆸᄂ\nᅵᄃ\nᅡ. ᄃ\nᅡ\nᆼᄉ\nᅵ\nᆫᄋ\nᅴᄎ\nᅮᄅ\nᅩ\nᆫᄀ\nᅪᄌ\nᅥ\nᆼᄋ\nᅳ\nᆯᄀ\nᅡ\nᆫᄀ\nᅧ\nᆯᄒ\nᅡᄀ\nᅦ\nᄋ\nᅭᄋ\nᅣ\nᆨᄒ\nᅡ\nᆫᄒ\nᅮ, ”ᄄ\nᅡᄅ\nᅡᄉ\nᅥ, ᄌ\nᅥ\nᆼᄃ\nᅡ\nᆸᄋ\nᅳ\nᆫ: X”ᄅ\nᅡᄀ\nᅩᄀ\nᅧ\nᆯᄅ\nᅩ\nᆫᄌ\nᅵᄋ\nᅳᄉ\nᅵ\nᆸᄉ\nᅵᄋ\nᅩ. ᄋ\nᅧᄀ\nᅵᄉ\nᅥXᄂ\nᅳ\nᆫA, B, C, D ᄌ\nᅮ\nᆼᄒ\nᅡᄂ\nᅡ\nᄋ\nᅵ\nᆸᄂ\nᅵᄃ\nᅡ.\nᄌ\nᅵ\nᆯᄆ\nᅮ\nᆫ: {question}\nA. {choice A}\nB. {choice B}\nC. {choice C}\nD. {choice D}\n(b) KMMLU prompt\nHAE-RAE (0-shot CoT)\nᄃ\nᅡᄋ\nᅳ\nᆷᄋ\nᅳ\nᆫᄀ\nᅢ\nᆨᄀ\nᅪ\nᆫᄉ\nᅵ\nᆨᄆ\nᅮ\nᆫᄌ\nᅦᄋ\nᅵ\nᆸᄂ\nᅵᄃ\nᅡ. ᄃ\nᅡ\nᆼᄉ\nᅵ\nᆫᄋ\nᅴᄎ\nᅮᄅ\nᅩ\nᆫᄀ\nᅪᄌ\nᅥ\nᆼᄋ\nᅳ\nᆯᄀ\nᅡ\nᆫᄀ\nᅧ\nᆯᄒ\nᅡᄀ\nᅦᄋ\nᅭᄋ\nᅣ\nᆨᄒ\nᅡ\nᆫᄒ\nᅮ, ”ᄄ\nᅡᄅ\nᅡᄉ\nᅥ, ᄌ\nᅥ\nᆼᄃ\nᅡ\nᆸ\nᄋ\nᅳ\nᆫ: X”ᄅ\nᅡᄀ\nᅩᄀ\nᅧ\nᆯᄅ\nᅩ\nᆫᄌ\nᅵᄋ\nᅳᄉ\nᅵ\nᆸᄉ\nᅵᄋ\nᅩ. ᄋ\nᅧᄀ\nᅵᄉ\nᅥXᄂ\nᅳ\nᆫA, B, C, D, E ᄌ\nᅮ\nᆼᄒ\nᅡᄂ\nᅡᄋ\nᅵ\nᆸᄂ\nᅵᄃ\nᅡ.\n{query}\n(c) HAE-RAE prompt\nFigure 10: Evaluation prompts for MMLU, KMMLU, and HAE-RAE. The prompts are used to evaluate\ninstruction-tuned models.\nB\nEvaluation Details\nB.1\nEvaluation prompts for post-trained models\nWe employ 0-shot CoT prompts for a number of evaluations. See Figure 10 for MMLU,\nKMMLU and HAE-RAE. For math-related tasks we employ 0-shot prompt. We refer the\nreaders to Figure 11 for the prompts corresponding to GSM8K and MATH.\n34\n\nGSM8K (0-shot)\nPut your final answer within \\boxed{}.\n{question}\n(a) GSM8K prompt\nMATH (0-shot)\nPut your final answer within \\boxed{}.\n{problem}\n(b) MATH prompt\nFigure 11: Evaluation prompts for GSM8K and MATH. The prompts are used to evaluate instruction-\ntuned models.\nC\nQualititive Results\nIn this section, we include qualitative experimental results. We mainly focus on changes\nmade during post-training. See Table 19 and Table 20 for representative examples.\nPrompt\n1 ᄃ\nᅥ\nᄃ\nᅥ\nᄃ\nᅥᄒ\nᅡ\nᄒ\nᅡ\nᄒ\nᅡᄀ\nᅵ\nᄀ\nᅵ\nᄀ\nᅵ1ᄋ\nᅳ\nᆫ\nᄋ\nᅳ\nᆫ\nᄋ\nᅳ\nᆫᄀ\nᅱ\nᄀ\nᅱ\nᄀ\nᅱᄋ\nᅭ\nᄋ\nᅭ\nᄋ\nᅭᄆ\nᅵ\nᄆ\nᅵ\nᄆ\nᅵᄋ\nᅣ\nᄋ\nᅣ\nᄋ\nᅣ! 1 ᄃ\nᅥ\nᄃ\nᅥ\nᄃ\nᅥᄒ\nᅡ\nᄒ\nᅡ\nᄒ\nᅡᄀ\nᅵ\nᄀ\nᅵ\nᄀ\nᅵ1ᄋ\nᅳ\nᆫ\nᄋ\nᅳ\nᆫ\nᄋ\nᅳ\nᆫ?\nSFT\n1 ᄃ\nᅥᄒ\nᅡᄀ\nᅵ1ᄋ\nᅳ\nᆫ2ᄋ\nᅵ\nᆸᄂ\nᅵᄃ\nᅡ.\nDPO\n1 ᄃ\nᅥᄒ\nᅡᄀ\nᅵ1ᄋ\nᅳ\nᆫ2ᄋ\nᅵ\nᆸᄂ\nᅵᄃ\nᅡ. ᄀ\nᅱᄋ\nᅭᄆ\nᅵᄅ\nᅡᄂ\nᅳ\nᆫᄆ\nᅡ\nᆯᄋ\nᅳ\nᆫᄉ\nᅡ\nᆼᄒ\nᅪ\nᆼᄋ\nᅦᄄ\nᅡᄅ\nᅡᄃ\nᅡᄅ\nᅳᄀ\nᅦᄒ\nᅢᄉ\nᅥ\nᆨᄃ\nᅬ\nᆯᄉ\nᅮᄋ\nᅵ\nᆻ\nᄌ\nᅵᄆ\nᅡ\nᆫ, ᄋ\nᅵ\nᆯᄇ\nᅡ\nᆫᄌ\nᅥ\nᆨᄋ\nᅳᄅ\nᅩᄂ\nᅳ\nᆫ’ᄀ\nᅱᄋ\nᅧᄋ\nᅮ\nᆫᄀ\nᅥ\nᆺ’ᄋ\nᅳ\nᆯᄋ\nᅴᄆ\nᅵᄒ\nᅡ\nᆸᄂ\nᅵᄃ\nᅡ. ᄋ\nᅨᄅ\nᅳ\nᆯᄃ\nᅳ\nᆯᄋ\nᅥ, ᄋ\nᅡᄋ\nᅵᄃ\nᅳ\nᆯᄋ\nᅵᄉ\nᅮᄒ\nᅡ\nᆨ\nᄆ\nᅮ\nᆫᄌ\nᅦᄅ\nᅳ\nᆯᄌ\nᅢᄆ\nᅵᄋ\nᅵ\nᆻᄀ\nᅦᄑ\nᅮ\nᆯᄀ\nᅵᄋ\nᅱᄒ\nᅢ”1 ᄃ\nᅥᄒ\nᅡᄀ\nᅵ1ᄋ\nᅳ\nᆫᄀ\nᅱᄋ\nᅭᄆ\nᅵ” ᄀ\nᅡ\nᇀᄋ\nᅳ\nᆫᄀ\nᅮᄒ\nᅩᄅ\nᅳ\nᆯᄉ\nᅡᄋ\nᅭ\nᆼᄒ\nᅡᄀ\nᅵ\nᄃ\nᅩᄒ\nᅡ\nᆸᄂ\nᅵᄃ\nᅡ. ᄒ\nᅡᄌ\nᅵᄆ\nᅡ\nᆫᄉ\nᅮᄒ\nᅡ\nᆨᄌ\nᅥ\nᆨᄋ\nᅳᄅ\nᅩᄂ\nᅳ\nᆫ1 + 1 = 2ᄋ\nᅵ\nᆸᄂ\nᅵᄃ\nᅡ.\nOnline DPO\n1 ᄃ\nᅥᄒ\nᅡᄀ\nᅵ1ᄋ\nᅳ\nᆫ2ᄋ\nᅵ\nᆸᄂ\nᅵᄃ\nᅡ. ᄀ\nᅱᄋ\nᅭᄆ\nᅵᄅ\nᅡᄂ\nᅳ\nᆫᄑ\nᅭᄒ\nᅧ\nᆫᄋ\nᅳ\nᆫᄉ\nᅮᄒ\nᅡ\nᆨᄌ\nᅥ\nᆨᄋ\nᅵ\nᆫᄀ\nᅨᄉ\nᅡ\nᆫᄇ\nᅩᄃ\nᅡᄂ\nᅳ\nᆫᄀ\nᅡ\nᆷᄌ\nᅥ\nᆼᄌ\nᅥ\nᆨᄋ\nᅵ\nᆫ\nᄑ\nᅭᄒ\nᅧ\nᆫᄋ\nᅳᄅ\nᅩᄇ\nᅩᄋ\nᅵ\nᆸᄂ\nᅵᄃ\nᅡ. ᄒ\nᅩ\nᆨᄉ\nᅵᄃ\nᅡᄅ\nᅳ\nᆫᄌ\nᅵ\nᆯᄆ\nᅮ\nᆫᄋ\nᅵᄂ\nᅡᄎ\nᅮᄀ\nᅡᄌ\nᅥ\nᆨᄋ\nᅵ\nᆫᄉ\nᅥ\nᆯᄆ\nᅧ\nᆼᄋ\nᅵᄑ\nᅵ\nᆯᄋ\nᅭᄒ\nᅡᄉ\nᅵᄆ\nᅧ\nᆫᄋ\nᅡ\nᆯ\nᄅ\nᅧᄌ\nᅮᄉ\nᅦᄋ\nᅭ!\nTable 19: Prompt and completions of each post-training stage.\n35\n\nPrompt\nConvert these dates to YYYY/MM/DD format:\n12/31/2021\n31-12-21\n01/02/2022\n02-01-22\nSFT\nHere is a Python solution for the problem:\ndef convert date ( date ) :\ni f\n'−'\nin date :\ndate parts = date . s p l i t ( '−' )\nreturn\nf ”{date parts [2]}/{ date parts [1]}/{ date parts [0]} ”\nelse :\ndate parts = date . s p l i t ( '/ ' )\nreturn\nf ”{date parts [2]}/{ date parts [0]}/{ date parts [1]} ”\ndates = [ ”12/31/2021” , ”31−12−21” , ”01/02/2022” , ”02−01−22” ]\nfor\ndate in dates :\nprint ( convert date ( date ) )\nWhen you run this code, it will print:\n2021/12/31\n2021/12/31\n2022/02/01\n2022/01/02\nThis code works by checking if the input date contains a ’-’ character. If it\ndoes, it assumes the date is in the format DD-MM-YYYY and converts\nit to YYYY/MM/DD. If the input date does not contain a ’-’ character,\nit assumes the date is in the format MM/DD/YYYY and converts it to\nYYYY/MM/DD.\nContinued on next page\n36\n\nTable 20 – Continued from previous page\nPrompt\nConvert these dates to YYYY/MM/DD format:\n12/31/2021\n31-12-21\n01/02/2022\n02-01-22\nDPO\nHere is a Python solution for the problem:\ndef convert date ( date ) :\ni f\n'/ '\nin date :\nmonth ,\nday ,\nyear = date . s p l i t ( '/ ' )\nreturn\nf ”{year}/{month . z f i l l (2)}/{day . z f i l l (2)} ”\ne l i f\n'−'\nin date :\nday , month ,\nyear = date . s p l i t ( '−' )\nreturn\nf ”{year}/{month . z f i l l (2)}/{day . z f i l l (2)} ”\nelse :\nraise\nValueError ( ” Invalid\ndate format” )\ndates = [ ”12/31/2021” , ”31−12−21” , ”01/02/2022” , ”02−01−22” ]\nfor\ndate in dates :\nprint ( convert date ( date ) )\nThis script will convert the dates to the desired format. The zfill(2)\nfunction is used to add leading zeros if the month or day is a single digit.\nWhen you run this script, it will output:\n2021/12/31\n2021/12/31\n2022/01/02\n2022/01/02\nNote that the dates ”31-12-21” and ”02-01-22” are interpreted as ”31\nDecember 2021” and ”2 January 2022” respectively, because the year is\ngiven as ”21” and ”22”. If these dates are meant to represent ”12-31-2021”\nand ”1-2-2022” (i.e., December 31, 2021 and January 2, 2022), then the\nscript will need to be modified to handle this interpretation.\nOnline DPO\nSure, here are the dates converted to the YYYY/MM/DD format:\n12/31/2021 →2021/12/31\n31-12-21 →2021/12/31\n01/02/2022 →2022/01/02\n02-01-22 →2022/01/02\nNote that the dates ”31-12-21” and ”02-01-22” are ambiguous and could\npotentially mean the opposite year, but based on common sense and the\ncontext provided, the most likely interpretation is the one given above\n(i.e., 2021 and 2022 respectively). If the exact year is unknown, it’s crucial\nto have additional context.\nTable 20: Prompt and completions of each post-training stage.\n37\n\nD\nEvaluation Details of Embedding Models\nBenchmarks\nKanana-Nano-2.1B\nLlama3.2 3B\nQwen2.5 3B\nLlama3.2 1B\nQwen2.5 1.5B\nEnglish (Avg)\n51.56\n53.28\n54.00\n48.77\n50.60\nArguAna\n54.59\n54.36\n56.26\n51.80\n53.49\nCQADupstackGamingRetrieval\n58.37\n60.31\n59.65\n56.13\n57.20\nCQADupstackUnixRetrieval\n43.34\n45.27\n45.41\n39.18\n41.14\nClimateFEVERHardNegatives\n29.64\n30.64\n31.08\n26.93\n27.66\nFEVERHardNegatives\n73.18\n79.09\n80.26\n73.27\n72.09\nFiQA2018\n40.22\n46.47\n47.12\n38.54\n41.08\nHotpotQAHardNegatives\n61.35\n66.10\n66.33\n61.21\n64.18\nSCIDOCS\n21.41\n21.44\n22.14\n18.96\n19.81\nTRECCOVID\n79.85\n81.84\n80.87\n72.67\n75.88\nTouche2020Retrieval.v3\n53.63\n47.26\n50.91\n49.00\n53.50\nKorean (Avg)\n65.00\n59.43\n62.10\n54.68\n54.60\nAutoRAGRetrieval\n79.71\n70.87\n75.64\n71.47\n72.32\nBelebeleRetrieval\n92.35\n87.58\n90.16\n84.44\n83.53\nKo-StrategyQA\n79.98\n73.92\n76.38\n63.46\n64.97\nMIRACLRetrieval\n60.04\n52.25\n56.83\n48.28\n48.68\nMrTidyRetrieval\n49.82\n45.83\n48.48\n35.32\n37.94\nMultiLongDocRetrieval\n30.17\n25.54\n25.75\n20.98\n17.13\nPublicHealthQA\n88.08\n84.12\n86.68\n80.26\n79.71\nXPQARetrieval\n39.88\n35.33\n36.89\n33.24\n32.55\nTable 21: Evaluation details of embedding models on English and Korean retrieval benchmarks.\nE\nRAG-General-Bench Examples\ncontext:\n{doc1}\n□1 마스크 착용 명령 대상\n➊의무화 장소·시설·대상 ○ (마스크 착용 의무)\n- 감염취약시설 중 입소형 시설, 의료기관·약국 및 대중교통수단의 실내*\n* 실내란 버스·택시·기차·선박·항공기, 기타 차량 등 운송수단, 건축물 및\n사방이 구획되어 외부와 분리되어 있는 모든 구조물을 의미\n■감염취약시설 중 입소형 시설\n* (제외장소) 사무동·기숙사 등 입소자의 출입이 필요 없는 구역(단, 건물 또는\n층 단위로 구역이 구분되는 경우만 해당)\n(...)\n{doc2}\n□2 마스크착용의무위반시과태료\n➊대상별 과태료 금액\n○마스크 착용 위반 당사자(행정명령 위반 당사자): 위반 횟수와 관계없이 각각\n10만원 과태료 부과(법 제83조제4항)\n(...)\ninstruction:\nA씨는 최근에 요양병원에 계신 어머님을 두 번 면회하고 왔습니다. \nA씨는 두 번 다 마스크를 착용하지 않았지만 별다른 제지는 없었습니다. \nA씨와 요양병원 원장 B씨가 각각 내야 할 과태료는 얼마인가요?\nreference_answer:\n요양병원은 마스크 착용 의무화된 곳으로 규정되어 있습니다. 따라서 A씨는 두 \n번 다 마스크를 착용하지 않았으므로, 위반 당사자인 A씨는 위반 횟수와 관계 없\n이 각각 10만원의 과태료를 부과받으므로, 총 20만원의 과태료를 내야 합니다.\n요양병원 원장 B씨는 관리자로서 이용자에게 마스크 착용을 안내하고 강제하는\n의무가 있습니다. 만약 B씨가 해당 의무를 다하지 못했다면, 과태료의 부과 기준\n에 따라 1차 위반 시 50만원, 2차 위반 시 100만원의 과태료를 부과받게 됩니다.\n따라서 B씨는 총 150만원의 과태료를 내야합니다.\n(a) Sample 1\ncontext:\n{doc1}\n2023년 교육부 소관 비영리법인 현황\n[{\"연번\":\"1\",\"법인명\":\"한국인문사회총연합회\",\"소관부서\":\"학술연구정책과\"},\n(...)\n{\"연번\":\"24\",\"법인명\":\"소비자교육중앙회\",\"소관부서\":\"디지털소통팀\"}]\n{doc2}\n2024년 교육부 소관 비영리법인 현황\n[{\"연번\":\"1\",\"법인명\":\"한국인문사회총연합회\",\"소관부서\":\"학술연구정책과\"},\n(...)\n{\"연번\":\"24\",\"법인명\":\"소비자교육중앙회\",\"소관부서\":\"학부모정책과\"}]\ninstruction:\n교육부 소관 비영리법인 중 2023년과 비교해서 2024년에 소관부서가 달라진 \n법인을 '법인명, 2023년 소관부서, 2024년 소관부서'로 구분해서 표 형태로 보\n여주세요.\nreference_answer:\n2023년과 비교해서 2024년에 소관부서가 달라진 법인들은 다음과 같습니다.\n| 법인명                  | 2023년 소관부서           | 2024년 소관부서            |\n|------------------------|--------------------\n--------|----------------------------|\n| 한국대학법인협의회     | 대학경영지원과             | 대학경영혁신지원과         |\n| 참교육을위한전국학부모회 | 디지털소통팀                | 학부모정책과               |\n| 소비자교육중앙회       | 디지털소통팀                | 학부모정책과               |\n(b) Sample 2\nFigure 12: RAG-General-Bench example: QA\nF\nFunctionChat-Bench Examples\nF.1\nSingle-call\nSingle-call evaluates how accurately the LM can select and call the necessary function among\nseveral options by providing four single-turn prompts for each of 25 different functions. As\n38\n\nshow in Figure 13, ”1 exact” is that only the target function is provided to the Assistant as a\ncandidate.\nFigure 13: FunctionChat-bench example : Single-call(1 exact)\nF.2\nDialogue\nThe dialog dataset consists of 45 diverse multi-turn interactions between real users and an\nLM, categorized into four situation types to evaluate the model’s response accuracy and\nappropriateness.\n1. Call: An LM must accurately select functions and extract the necessary parameters\nto respond to a user prompt\n2. Completion: An LM must generate appropriate responses based on the results of\nthe tool.\n3. Slot: An LM must query the user for the necessary parameters to make a function\ncall.\n4. Relevance: An LM must generate an appropriate response when it cannot provide\na function for a user prompt.\n39\n\nFigure 14: FunctionChat-bench Example : Dialogue\n40\n"
    }
  ]
}