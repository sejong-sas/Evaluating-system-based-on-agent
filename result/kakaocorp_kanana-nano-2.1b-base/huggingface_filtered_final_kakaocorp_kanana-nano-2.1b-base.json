{
  "1-1 (Weights)": "The quotes explicitly state that, on “📜`2025/02/27`,” Kakao released both a “Technical Report” and “🤗[HF model weights](https://huggingface.co/collections/kakaocorp/kanana-nano-21b-67a326cda1c449c8d4172259).”  In the same block, the Hugging Face URL is provided, signalling that the kanana/nano-series checkpoints can be obtained directly from the Hugging Face Hub.  A second quoted line, “#### Example Usage for `kanana-nano-2.1b-base`,” shows that the specific sub-model kakaocorp/kanana-nano-2.1b-base is included in the publicly listed artifacts and that users are given an example snippet for immediate use, further reinforcing that weights for this exact model variant are hosted and downloadable.",
  "1-1 (Weights)__evidence": [
    {
      "source": "[readme]",
      "quote": "- 📜`2025/02/27`: Released [Technical Report](https://arxiv.org/abs/2502.18934) and 🤗[HF model weights](https://huggingface.co/collections/kakaocorp/kanana-nano-21b-67a326cda1c449c8d4172259)."
    },
    {
      "source": "[readme]",
      "quote": "#### Example Usage for `kanana-nano-2.1b-base`"
    }
  ],
  "1-2 (Code)": "Model-related code is made public via Kakao’s repository: “💻 <a href=\"https://github.com/kakao/kanana\"> Github </a>.”  The quote only supplies the top-level URL and does not distinguish which phases of the pipeline (pre-training, fine-tuning, RL, inference, or serving) are contained there, but it does confirm that an official GitHub repository exists for the kanana project.",
  "1-2 (Code)__evidence": [
    {
      "source": "[readme]",
      "quote": "💻 <a href=\"https://github.com/kakao/kanana\"> Github </a>"
    }
  ],
  "1-3 (License)": "Multiple quotes provide consistent licensing information: “The `Kanana` models are licensed under [CC-BY-NC-4.0](https://spdx.org/licenses/CC-BY-NC-4.0).”  This exact string is repeated verbatim, and the repository metadata snippet reiterates the same: “license: cc-by-nc-4.0.”  Therefore, kakaocorp/kanana-nano-2.1b-base is distributed under the Creative Commons Attribution-NonCommercial 4.0 International license, permitting use and modification with the explicit restriction that it is non-commercial.",
  "1-3 (License)__evidence": [
    {
      "source": "[readme]",
      "quote": "The `Kanana` models are licensed under [CC-BY-NC-4.0](https://spdx.org/licenses/CC-BY-NC-4.0)."
    },
    {
      "source": "[readme]",
      "quote": "license: cc-by-nc-4.0"
    },
    {
      "source": "[readme]",
      "quote": "[readme]\n---\nlanguage:\n- en\n- ko\nlibrary_name: transformers\nlicense: cc-by-nc-4.0\npipeline_tag: text-generation\nmodel_id: kakaocorp/kanana-nano-2.1b-base\nrepo: kakaocorp/kanana-nano-2.1b-base\ndevelopers: Kanana LLM\ntraining_regime: bf"
    },
    {
      "source": "[readme]",
      "quote": "ices, 2) its commitment to sustainability, and 3) its focus on customer experience. Kakao has been recognized as\n```\n\n<br>\n\n## License\n\nThe `Kanana` models are licensed under [CC-BY-NC-4.0](https://spdx.org/licenses/CC-BY-NC-4.0).\n \n<br>\n \n## Citation\n \n```\n@misc{kananallmteam2025kananacomputeefficientbilinguallanguage,\n title={Kanana"
    }
  ],
  "1-4 (Paper)": "Two separate lines confirm the existence of an official technical report: “📜`2025/02/27`: Released Technical Report …” and “📜 <a href=\"https://arxiv.org/abs/2502.18934\">Technical Report</a>.”  Both point to the same ArXiv identifier (2502.18934), establishing that an accompanying paper was published on 27 Feb 2025 to document the kanana-nano model family, including the 2.1 b-parameter variant.",
  "1-4 (Paper)__evidence": [
    {
      "source": "[readme]",
      "quote": "- 📜`2025/02/27`: Released [Technical Report](https://arxiv.org/abs/2502.18934) and 🤗[HF model weights](https://huggingface.co/collections/kakaocorp/kanana-nano-21b-67a326cda1c449c8d4172259)."
    },
    {
      "source": "[readme]",
      "quote": "📜 <a href=\"https://arxiv.org/abs/2502.18934\">Technical Report</a>"
    }
  ],
  "1-5 (Architecture)": "The documentation for kakaocorp/kanana-nano-2.1b-base explicitly states that the Kanana model series spans from 2.1 billion to 32.5 billion parameters, and that the 2.1 billion-parameter variants (which include the “base” release) are the ones made publicly available.  Within this 2.1 B configuration the quoted hyper-parameters are:\n• hidden_size = 1792\n• num_hidden_layers = 32\n• num_attention_heads = 24\n• intermediate_size = 8064\n• max_position_embeddings = 8192\n• vocab_size = 128 256\nTogether, these figures describe a 32-layer transformer encoder-decoder stack (1792-wide hidden states, 24-way attention, 8 064-wide feed-forward projection) that can attend over sequences as long as 8 192 positions while operating on a vocabulary of 128 256 tokens.  The combination of these dimensions is what yields the 2.1 B total trainable parameter count referred to in the Kanana 2.1 B release note.",
  "1-5 (Architecture)__evidence": [
    {
      "source": "[readme]",
      "quote": "The Kanana model series spans from 2.1B to 32.5B parameters with 2.1B models (base, instruct, embedding, function call, and RAG) publicly released to promote research on Korean language models."
    },
    {
      "source": "[config]",
      "quote": "\"hidden_size\": 1792,"
    },
    {
      "source": "[config]",
      "quote": "\"num_hidden_layers\": 32,"
    },
    {
      "source": "[config]",
      "quote": "\"num_attention_heads\": 24,"
    },
    {
      "source": "[config]",
      "quote": "\"intermediate_size\": 8064,"
    },
    {
      "source": "[config]",
      "quote": "\"max_position_embeddings\": 8192,"
    },
    {
      "source": "[config]",
      "quote": "\"vocab_size\": 128256"
    }
  ],
  "1-6 (Tokenizer)": "The model is loaded with the HuggingFace AutoTokenizer interface (tokenizer = AutoTokenizer.from_pretrained(model_name, padding_side=\"left\")), indicating that the tokenizer artefact is bundled and can be downloaded from the same repository.  The configuration shows left-side padding, and immediately after instantiation the pad_token is hard-set to the model’s eos_token (tokenizer.pad_token = tokenizer.eos_token).  Special-token IDs are explicitly listed: bos_token_id = 128 000 and eos_token_id = 128 001.  The tokenizer’s vocabulary is sized at 128 256 entries, mirroring the vocab_size parameter of the model itself, ensuring full alignment between the tokeniser and the kanana-nano-2.1b-base model weights.",
  "1-6 (Tokenizer)__evidence": [
    {
      "source": "[readme]",
      "quote": "tokenizer = AutoTokenizer.from_pretrained(model_name, padding_side=\"left\")"
    },
    {
      "source": "[readme]",
      "quote": "tokenizer.pad_token = tokenizer.eos_token"
    },
    {
      "source": "[config]",
      "quote": "\"bos_token_id\": 128000,"
    },
    {
      "source": "[config]",
      "quote": "\"eos_token_id\": 128001,"
    },
    {
      "source": "[config]",
      "quote": "\"vocab_size\": 128256"
    }
  ],
  "2-1 (Hardware)": "",
  "2-1 (Hardware)__evidence": [],
  "2-2 (Software)": "Running or finetuning kakaocorp/kanana-nano-2.1b-base requires the HuggingFace Transformers library at version 4.45.0 or newer (\"transformers>=4.45.0\"), with the quoted metadata recording the exact build as \"transformers_version\": \"4.45.0.dev0\" and the library_name set to \"transformers\".  No additional training-time software details are provided in the available material.",
  "2-2 (Software)__evidence": [
    {
      "source": "[readme]",
      "quote": "`transformers>=4.45.0` or the latest version is required to run `Kanana` model."
    },
    {
      "source": "[config]",
      "quote": "\"transformers_version\": \"4.45.0.dev0\","
    },
    {
      "source": "[readme]",
      "quote": "library_name: transformers"
    }
  ],
  "2-3 (API)": "",
  "2-3 (API)__evidence": [],
  "3-1 (Pre-training)": "For the kanana-nano-2.1b-base model, the accompanying report says that the pre-training phase was engineered to be \"compute-efficient yet competitive.\" To accomplish this, the workflow combined four named techniques: (1) high-quality data filtering carried out before model training, (2) a staged or multi-phase pre-training schedule, (3) depth up-scaling to gradually increase model depth, and (4) a concluding round of pruning and distillation. Together, these steps form the entire pre-training recipe disclosed for the Kanana series.",
  "3-1 (Pre-training)__evidence": [
    {
      "source": "[readme]",
      "quote": "The report details the techniques employed during pre-training to achieve compute-efficient yet competitive models, including high-quality data filtering, staged pre-training, depth up-scaling, and pruning and distillation."
    }
  ],
  "3-2 (Fine-tuning)": "During the post-training of the Kanana models—including kanana-nano-2.1b-base—the report explicitly lists \"supervised fine-tuning\" as a core component. This supervised phase is intended to improve the model’s aptitude for fluent, user-focused interactions once the base pre-training is finished.",
  "3-2 (Fine-tuning)__evidence": [
    {
      "source": "[readme]",
      "quote": "Furthermore, the report outlines the methodologies utilized during the post-training of the Kanana models, encompassing supervised fine-tuning and preference optimization, aimed at enhancing their capability for seamless interaction with users."
    }
  ],
  "3-3 (Reinforcement Learning)": "The same post-training description also identifies \"preference optimization\" for the Kanana line, which by implication includes kanana-nano-2.1b-base. This preference-optimization step serves as a reinforcement-style procedure that follows supervised fine-tuning and is geared toward aligning the model’s responses with user preferences, thereby enhancing real-world conversational quality.",
  "3-3 (Reinforcement Learning)__evidence": [
    {
      "source": "[readme]",
      "quote": "Furthermore, the report outlines the methodologies utilized during the post-training of the Kanana models, encompassing supervised fine-tuning and preference optimization, aimed at enhancing their capability for seamless interaction with users."
    }
  ],
  "4-1 (Pre-training Data)": "The documentation for the kakaocorp/kanana-nano-2.1b-base model specifies that it belongs to the Kanana series of bilingual language models. Within this series, model sizes range from 2.1 billion to 32.5 billion parameters, and the 2.1 billion-parameter variants—including the \"base, instruct, embedding, function-call, and RAG\" versions—have been publicly released to foster Korean-language-model research. The authors state unambiguously that no Kakao user data appear in either the pre-training or post-training corpora used for Kanana-nano-2.1b-base.",
  "4-1 (Pre-training Data)__evidence": [
    {
      "source": "[readme]",
      "quote": "The Kanana model series spans from 2.1B to 32.5B parameters with 2.1B models (base, instruct, embedding, function call, and RAG) publicly released to promote research on Korean language models. Neither the pre-training nor the post-training data includes Kakao user data."
    }
  ],
  "4-2 (Fine-tuning Data)": "For the Kanana-nano-2.1b-base model, the report says that post-training relied on \"supervised fine-tuning and preference optimization\" in order to improve the system’s ability to engage with users. While no concrete dataset names, sizes, or licenses are disclosed, the quote makes clear that the fine-tuning phase combined standard supervised techniques with preference-based optimization to align the model’s behavior with user-centric interaction goals.",
  "4-2 (Fine-tuning Data)__evidence": [
    {
      "source": "[readme]",
      "quote": "Furthermore, the report outlines the methodologies utilized during the post-training of the Kanana models, encompassing supervised fine-tuning and preference optimization, aimed at enhancing their capability for seamless interaction with users."
    }
  ],
  "4-3 (Reinforcement Learning Data)": "The same passage indicates that, in addition to supervised fine-tuning, the Kanana-nano-2.1b-base post-training pipeline incorporated \"preference optimization\"—a term commonly used to describe reinforcement-learning-from-human-feedback–style data. Thus, the only stated characteristic of the RL data is that it was generated or organized to express user preferences and was employed to optimize the model for smoother human interaction; no further details on size, composition, or public availability are provided.",
  "4-3 (Reinforcement Learning Data)__evidence": [
    {
      "source": "[readme]",
      "quote": "Furthermore, the report outlines the methodologies utilized during the post-training of the Kanana models, encompassing supervised fine-tuning and preference optimization, aimed at enhancing their capability for seamless interaction with users."
    }
  ],
  "4-4 (Data Filtering)": "During the pre-training stage for Kanana, which includes the nano-2.1b-base release, the authors emphasize the use of \"high-quality data filtering\" as part of a larger efficiency-driven strategy that also contained staged pre-training, depth up-scaling, and pruning/distillation steps. Although the quote does not enumerate specific metrics, thresholds, or classifiers, it makes clear that a dedicated filtering phase preceded model training to raise data quality before additional architectural and optimization techniques were applied.",
  "4-4 (Data Filtering)__evidence": [
    {
      "source": "[readme]",
      "quote": "We introduce Kanana, a series of bilingual language models (developed by [Kakao](https://github.com/kakao)) that demonstrate exceeding performance in Korean and competitive performance in English. The report details the techniques employed during pre-training to achieve compute-efficient yet competitive models, including high-quality data filtering, staged pre-training, depth up-scaling, and pruning and distillation."
    }
  ],
  "__usage": {
    "fine_tuning": "used",
    "rl": "used"
  }
}