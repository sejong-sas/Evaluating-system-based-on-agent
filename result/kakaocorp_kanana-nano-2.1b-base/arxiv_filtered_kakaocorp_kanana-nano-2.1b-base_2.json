{
  "1-5 (Architecture)": "The available statements indicate that kanana-nano-2.1b-base belongs to the Kanana model family whose publicly released sizes range from 2.1 B to 32.5 B parameters. For the 2.1 B tier, the same architectural blueprint is reused across the base, instruct and embedding variants, and the authors explicitly say “We use the same 2.1B architecture.” To isolate data-scaling effects from structural factors, the team “adopt[s] the architecture … of Llama 3,” while making it clear that no Llama 3 weights or activations are employed during training. Hence, kanana-nano-2.1b-base is a 2.1 B-parameter transformer that mirrors the Llama 3 architectural specification but is trained entirely from scratch for Korean-centric use cases.",
  "1-6 (Tokenizer)": "For tokenization, kanana-nano-2.1b-base also reuses the Llama 3 tokenizer. The authors note twice that they adopt this tokenizer but do not import any of Llama 3’s weights or generated outputs. No further vocabulary size or BPE/WordPiece details are given in the cited material.",
  "2-1 (Hardware)": "",
  "2-2 (Software)": "During training of kanana-nano-2.1b-base the authors employ a modified optimization stack: (1) they switch to “independent weight decay,” following Loshchilov & Hutter’s original AdamW proposal rather than PyTorch’s built-in variant; and (2) they integrate a z-loss term, as introduced by Chowdhery et al., to improve stability and effectiveness across multiple model scales. No other libraries, frameworks or specific runtime flags appear in the provided excerpts.",
  "1-5 (Architecture)__evidence": [
    {
      "source": "[abstract]",
      "quote": "The Kanana model series spans from 2.1B to 32.5B parameters with 2.1B models (base, instruct, embedding) publicly released to promote research on Korean language models."
    },
    {
      "source": "[pdf_text]",
      "quote": "The family of models includes pre-trained base model and post-trained instruction models in sizes of {2.1B, 9.8B, 32.5B}."
    },
    {
      "source": "[pdf_text]",
      "quote": "We use the same 2.1B architecture."
    },
    {
      "source": "[sections/2502.18934]",
      "quote": "To control the effects of architecture and tokenization, and to focus on improving the data scaling curve, we adopt the architecture and tokenizer of Llama 3 (Grattafiori et al., 2024). Note that while we use the Llama 3 tokenizer, we do not utilize either the weights or the outputs of Llama 3 during the training of Kanana."
    },
    {
      "source": "[sections/Introduction]",
      "quote": "The Kanana model series spans from 2.1B to 32.5B parameters with 2.1B models (base, instruct, embedding) publicly released to promote research on Korean language models."
    },
    {
      "source": "[sections/Introduction]",
      "quote": "The family of models includes pre-trained base model and post-trained instruction models in sizes of {2.1B, 9.8B, 32.5B}."
    },
    {
      "source": "[sections/A.3 Details of pre-training from scratch]",
      "quote": "To control the effects of architecture and tokenization, and to focus on improving the data scaling curve, we adopt the architecture and tokenizer of Llama 3 (Grattafiori et al., 2024). Note that while we use the Llama 3 tokenizer, we do not utilize either the weights or the outputs of Llama 3 during the training of Kanana."
    }
  ],
  "1-6 (Tokenizer)__evidence": [
    {
      "source": "[sections/2502.18934]",
      "quote": "To control the effects of architecture and tokenization, and to focus on improving the data scaling curve, we adopt the architecture and tokenizer of Llama 3 (Grattafiori et al., 2024). Note that while we use the Llama 3 tokenizer, we do not utilize either the weights or the outputs of Llama 3 during the training of Kanana."
    },
    {
      "source": "[sections/A.3 Details of pre-training from scratch]",
      "quote": "To control the effects of architecture and tokenization, and to focus on improving the data scaling curve, we adopt the architecture and tokenizer of Llama 3 (Grattafiori et al., 2024). Note that while we use the Llama 3 tokenizer, we do not utilize either the weights or the outputs of Llama 3 during the training of Kanana."
    }
  ],
  "2-1 (Hardware)__evidence": [],
  "2-2 (Software)__evidence": [
    {
      "source": "[sections/2502.18934]",
      "quote": "To control the effects of architecture and tokenization, and to focus on improving the data scaling curve, we adopt the architecture and tokenizer of Llama 3 (Grattafiori et al., 2024). Note that while we use the Llama 3 tokenizer, we do not utilize either the weights or the outputs of Llama 3 during the training of Kanana. Based on the observations of Wortsman et al. (2024), we adopt independent weight decay, which follows the original proposal of Loshchilov & Hutter (2019) and differs from the PyTorch implementation, and a z-loss (Chowdhery et al., 2023) to obtain effective and stable training across various model scales."
    },
    {
      "source": "[sections/A.3 Details of pre-training from scratch]",
      "quote": "Note that while we use the Llama 3 tokenizer, we do not utilize either the weights or the outputs of Llama 3 during the training of Kanana. Based on the observations of Wortsman et al. (2024), we adopt independent weight decay, which follows the original proposal of Loshchilov & Hutter (2019) and differs from the PyTorch implementation, and a z-loss (Chowdhery et al., 2023) to obtain effective and stable training across various model scales."
    }
  ]
}