{
  "2-3 (API)": "",
  "2-3 (API)__evidence": [],
  "3-1 (Pre-training)": "",
  "3-1 (Pre-training)__evidence": [],
  "3-2 (Fine-tuning)": "The report explicitly explains that, during post-training of the Kanana models, \"supervised fine-tuning\" is combined with \"preference optimization\". These two steps are presented as the key methodologies used to refine Kanana after its initial training, with the stated aim of \"enhancing their capability for seamless interaction with users.\"",
  "3-2 (Fine-tuning)__evidence": [
    {
      "source": "[readme]",
      "quote": "Furthermore, the report outlines the methodologies utilized during the post-training of the Kanana models, encompassing supervised fine-tuning and preference optimization, aimed at enhancing their capability for seamless interaction with users."
    }
  ],
  "3-3 (Reinforcement Learning)": "For reinforcement-learningâ€“based post-training, the documentation notes that \"Kanana-1.5-15.7B-A3B is powered by our newly enhanced post-training strategy, which includes on-policy distillation followed by reinforcement learning.\" This single sentence clarifies that an on-policy distillation stage is run first, and that reinforcement learning is then applied as the subsequent step in the Kanana post-training pipeline.",
  "3-3 (Reinforcement Learning)__evidence": [
    {
      "source": "[readme]",
      "quote": "Furthermore, `Kanana-1.5-15.7B-A3B` is powered by our newly enhanced post-training strategy, which includes on-policy distillation followed by reinforcement learning."
    }
  ]
}