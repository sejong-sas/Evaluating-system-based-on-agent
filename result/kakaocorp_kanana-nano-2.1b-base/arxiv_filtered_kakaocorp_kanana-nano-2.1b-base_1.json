{
  "1-1 (Weights)": "The available quotes repeatedly emphasize that the 2.1 B-parameter Kanana models – including the specific “base” checkpoint that corresponds to kakaocorp/kanana-nano-2.1b-base – are PUBLICLY RELEASED. Concretely, the statements “The Kanana model series spans from 2.1B to 32.5B parameters with 2.1B models (base, instruct, embedding) publicly released to promote research on Korean language models” appear twice, underlining both the range of the series and the fact that every 2.1 B variant (base, instruct, embedding) can be downloaded openly. The disclosure that the models are “publicly released” implies that the weight files themselves are hosted for direct access. The hosting location is specified through the repeated contact block:\n“Kanana LLM Team … https://huggingface.co/kakaocorp”.  This Hugging Face organization page is therefore the official distribution point where users can retrieve the kakaocorp/kanana-nano-2.1b-base weights. No access gate, wait-list, or license pre-approval is mentioned in the quotes, so the summary evidence supports that anyone with a Hugging Face account can download the 2.1 B “base” checkpoint. The same sentences confirm that the public release is meant to “promote research on Korean language models,” suggesting a research-oriented release vision. Because all quoted sentences explicitly reference “2.1B” and “Kanana,” they meet the strict model filter and safely apply to kakaocorp/kanana-nano-2.1b-base.",
  "1-2 (Code)": "Two identical quotes list a public GitHub repository for the project: “Kanana LLM Team … https://github.com/kakao/kanana”.  The presence of the repository link signals that some form of source code is openly available. However, the quotes do not spell out exactly which components live in that repository (e.g., data-preparation scripts, pre-training schedules, fine-tuning notebooks, or inference utilities). They do not separate training versus serving code either. Consequently, the only verifiable fact is that the Kanana team has made code for the Kanana models, including the 2.1 B base variant, accessible on GitHub. Users seeking to reproduce or extend kakaocorp/kanana-nano-2.1b-base should visit that URL to inspect the repository contents and determine whether it contains full training pipelines or only partial resources. The repeated appearance of both “Kanana” and the GitHub address in the quotes satisfies the target-model filter, so the statement unambiguously applies to the 2.1 B base model.",
  "1-3 (License)": "No provided quote contains licensing terms, license identifiers, or any wording about usage, modification, redistribution, or commercial rights for kakaocorp/kanana-nano-2.1b-base. As a result, there is no extractable information on the license from the supplied material.",
  "1-4 (Paper)": "Three distinct quotes give bibliographic-style information about the canonical write-up of Kanana. The phrases “We introduce Kanana, a series of bilingual language models that demonstrate exceeding performance in Korean and competitive performance in English” and “Kanana: Compute-efficient Bilingual Language Models” together function as the abstract and the title of the technical report. A third quote elaborates, stating, “In this report, we present Kanana, a family of large language models available in sizes of {2.1B, 9.8B, 32.5B}, with a focus on the cost-effective training procedure compared to other prominent open models.”  These sentences confirm that an official report or paper exists, titled roughly “Kanana: Compute-efficient Bilingual Language Models,” and that the 2.1 B model is one of three sizes analyzed. The document highlights strong bilingual performance (Korean-first, English-second) and stresses cost-efficiency during training. All sentences explicitly contain the token “Kanana” and, in one sentence, the explicit size “2.1B,” satisfying the strict model filter. Therefore, kakaocorp/kanana-nano-2.1b-base is directly covered by this paper, and interested readers should reference that report for architecture, training setup, and evaluation details.",
  "1-1 (Weights)__evidence": [
    {
      "source": "[abstract]",
      "quote": "The Kanana model series spans from 2.1B to 32.5B parameters with 2.1B models (base, instruct, embedding) publicly released to promote research on Korean language models."
    },
    {
      "source": "[pdf_text]",
      "quote": "Kanana LLM Team ∗\nkanana-llm@kakaocorp.com\nhttps://huggingface.co/kakaocorp"
    },
    {
      "source": "[sections/2502.18934]",
      "quote": "The Kanana model series spans from 2.1B to 32.5B parameters with 2.1B models (base, instruct, embedding) publicly released to promote research on Korean language models."
    },
    {
      "source": "[sections/2502.18934]",
      "quote": "Kanana LLM Team ∗\nkanana-llm@kakaocorp.com\nhttps://huggingface.co/kakaocorp"
    }
  ],
  "1-2 (Code)__evidence": [
    {
      "source": "[pdf_text]",
      "quote": "Kanana LLM Team ∗\nkanana-llm@kakaocorp.com\nhttps://github.com/kakao/kanana"
    },
    {
      "source": "[sections/2502.18934]",
      "quote": "Kanana LLM Team ∗\nkanana-llm@kakaocorp.com\nhttps://github.com/kakao/kanana"
    }
  ],
  "1-3 (License)__evidence": [],
  "1-4 (Paper)__evidence": [
    {
      "source": "[abstract]",
      "quote": "We introduce Kanana, a series of bilingual language models that demonstrate exceeding performance in Korean and competitive performance in English."
    },
    {
      "source": "[sections/2502.18934]",
      "quote": "Kanana: Compute-efficient Bilingual Language Models"
    },
    {
      "source": "[sections/Conclusion]",
      "quote": "In this report, we present Kanana, a family of large language models available in sizes of {2.1B, 9.8B, 32.5B}, with a focus on the cost-effective training procedure compared to other prominent open models."
    }
  ]
}