{
  "model_id": "bigscience/bloom",
  "full_texts": [
    {
      "arxiv_id": "2211.05100",
      "full_text": "BLOOM: A 176B-Parameter Open-Access Multilingual\nLanguage Model\nBigScience Workshop∗\nMajor Contributors\nTeven Le Scao, Angela Fan, Christopher Akiki, Ellie Pavlick, Suzana Ili´c, Daniel Hesslow, Roman\nCastagné, Alexandra Sasha Luccioni, Fran¸cois Yvon, Matthias Gallé, Jonathan Tow, Alexander M.\nRush, Stella Biderman, Albert Webson, Pawan Sasanka Ammanamanchi, Thomas Wang, Benoˆıt\nSagot, Niklas Muennighoff, Albert Villanova del Moral, Olatunji Ruwase, Rachel Bawden, Stas\nBekman, Angelina McMillan-Major, Thomas Wolf, Iz Beltagy, Huu Nguyen, Lucile Saulnier, Samson\nTan, Pedro Ortiz Suarez, Victor Sanh, Hugo Lauren¸con, Yacine Jernite, Julien Launay, Margaret\nMitchell, Colin Raffel\nDataset\nAaron Gokaslan, Adi Simhi, Aitor Soroa, Albert Villanova del Moral, Alexandra Sasha Luccioni,\nAlham Fikri Aji, Amit Alfassy, Angelina McMillan-Major, Anna Rogers, Ariel Kreisberg Nitzav,\nCanwen Xu, Chenghao Mou, Chris Emezue, Christopher Akiki, Christopher Klamm, Colin Leong,\nColin Raffel, Daniel van Strien, David Ifeoluwa Adelani, Dragomir Radev, Eduardo González Pon-\nferrada, Efrat Levkovizh, Ethan Kim, Eyal Bar Natan, Francesco De Toni, Gérard Dupont, Germán\nKruszewski, Giada Pistilli, Hady Elsahar, Hamza Benyamina, Hieu Tran, Hugo Lauren¸con, Huu\nNguyen, Ian Yu, Idris Abdulmumin, Isaac Johnson, Itziar Gonzalez-Dios, Javier de la Rosa, Jenny\nChim, Jesse Dodge, Jian Zhu, Jonathan Chang, J¨org Frohberg, Joseph Tobing, Joydeep Bhattachar-\njee, Khalid Almubarak, Kimbo Chen, Kyle Lo, Leandro Von Werra, Leon Weber, Long Phan, Loubna\nBen allal, Lucile Saulnier, Ludovic Tanguy, Manan Dey, Manuel Romero Mu˜noz, Maraim Masoud,\nMargaret Mitchell, María Grandury, Mario ˇSaˇsko, Max Huang, Maximin Coavoux, Mayank Singh,\nMike Tian-Jian Jiang, Minh Chien Vu, Mohammad A. Jauhar, Mustafa Ghaleb, Nishant Subramani,\nNora Kassner, Nurulaqilla Khamis, Olivier Nguyen, Omar Espejel, Ona de Gibert, Paulo Villegas,\nPawan Sasanka Ammanamanchi, Pedro Ortiz Suarez, Peter Henderson, Pierre Colombo, Priscilla\nAmuok, Quentin Lhoest, Rheza Harliman, Rishi Bommasani, Roberto Luis López, Roman Castagné,\nRui Ribeiro, Salomey Osei, Sampo Pyysalo, Samson Tan, Sebastian Nagel, Shamik Bose, Shamsud-\ndeen Hassan Muhammad, Shanya Sharma, Shayne Longpre, Somaieh Nikpoor, Stanislav Silberberg,\nStella Biderman, Suhas Pai, Suzana Ili´c, Sydney Zink, Teven Le Scao, Thomas Wang, Tiago Timponi\nTorrent, Timo Schick, Tristan Thrush, Valentin Danchev, Vassilina Nikoulina, Veronika Laippala,\nViolette Lepercq, Vrinda Prabhu, Yacine Jernite, Zaid Alyafeai, Zeerak Talat\nTokenization\nArun Raja, Benjamin Heinzerling, Benoˆıt Sagot, Chenglei Si, Colin Raffel, Davut Emre Ta¸sar, Eliz-\nabeth Salesky, Lucile Saulnier, Manan Dey, Matthias Gallé, Pedro Ortiz Suarez, Roman Castagné,\nSabrina J. Mielke, Samson Tan, Teven Le Scao, Thomas Wang, Wilson Y. Lee, Zaid Alyafeai\nPrompt Engineering\nAbheesht Sharma, Albert Webson, Alexander M. Rush, Alham Fikri Aji, Andrea Santilli, Antoine\nChaffin, Arnaud Stiegler, Arun Raja, Canwen Xu, Colin Raffel, Debajyoti Datta, Dragomir Radev,\nEliza Szczechla, Gunjan Chhablani, Han Wang, Harshit Pandey, Hendrik Strobelt, Jason Alan\nFries, Jonathan Chang, Jos Rozen, Khalid Almubarak, Leo Gao, Lintang Sutawika, M Saiful Bari,\nMaged S. Al-shaibani, Manan Dey, Matteo Manica, Mike Tian-Jian Jiang, Nihal Nayak, Niklas\nMuennighoff, Rachel Bawden, Ryan Teehan, Samuel Albanie, Shanya Sharma, Sheng Shen, Srulik\nBen-David, Stella Biderman, Stephen H. Bach, Taewoon Kim, Tali Bers, Teven Le Scao, Thibault\nFevry, Thomas Wang, Thomas Wolf, Trishala Neeraj, Urmish Thakker, Victor Sanh, Vikas Raunak,\n∗. Please direct correspondence to bigscience-contact@googlegroups.com.\nA list of contributions is\navailable in Section 6.\narXiv:2211.05100v4  [cs.CL]  27 Jun 2023\n\nBigScience Workshop\nXiangru Tang, Zaid Alyafeai, Zheng-Xin Yong, Zhiqing Sun, Shaked Brody, Yallow Uri, Hadar\nTojarieh\nArchitecture and Objective\nAdam Roberts, Colin Raffel, Daniel Hesslow, Hady Elsahar, Hyung Won Chung, Iz Beltagy, Jaesung\nTae, Jason Phang, Julien Launay, Lintang Sutawika, Lucile Saulnier, M Saiful Bari, Niklas Muen-\nnighoff, Ofir Press, Sheng Shen, Stas Bekman, Stella Biderman, Teven Le Scao, Thomas Wang,\nVassilina Nikoulina, Victor Sanh, Zheng-Xin Yong\nEngineering\nConglong Li, Deepak Narayanan, Hatim Bourfoune, Jared Casper, Jeff Rasley, Max Ryabinin,\nMayank Mishra, Minjia Zhang, Mohammad Shoeybi, Myriam Peyrounette, Nicolas Patry, Niklas\nMuennighoff, Nouamane Tazi, Olatunji Ruwase, Omar Sanseviero, Patrick von Platen, Pierre Cor-\nnette, Pierre Fran¸cois Lavallée, Rémi Lacroix, Samyam Rajbhandari, Sanchit Gandhi, Shaden Smith,\nStas Bekman, Stéphane Requena, Suraj Patil, Teven Le Scao, Thomas Wang, Tim Dettmers\nEvaluation and Interpretability\nAhmed Baruwa, Albert Webson, Alexandra Sasha Luccioni, Alham Fikri Aji, Amanpreet Singh,\nAnastasia Cheveleva, Anne-Laure Ligozat, Arjun Subramonian, Aurélie Névéol, Charles Lovering,\nDan Garrette, Deepak Tunuguntla, Dragomir Radev, Ehud Reiter, Ekaterina Taktasheva, Ekaterina\nVoloshina, Eli Bogdanov, Ellie Pavlick, Fran¸cois Yvon, Genta Indra Winata, Hailey Schoelkopf,\nJaesung Tae, Jan-Christoph Kalo, Jekaterina Novikova, Jessica Zosa Forde, Jordan Clive, Jungo\nKasai, Ken Kawamura, Khalid Almubarak, Liam Hazan, Lintang Sutawika, Manan Dey, Maraim\nMasoud, Margaret Mitchell, Marine Carpuat, Miruna Clinciu, Najoung Kim, Newton Cheng, Niklas\nMuennighoff, Oleg Serikov, Omer Antverg, Oskar van der Wal, Pawan Sasanka Ammanamanchi,\nPierre Colombo, Rachel Bawden, Rui Zhang, Ruochen Zhang, Samson Tan, Sebastian Gehrmann,\nShachar Mirkin, Shani Pais, Shanya Sharma, Shayne Longpre, Stella Biderman, Tatiana Shavrina,\nThomas Scialom, Tian Yun, Tomasz Limisiewicz, Urmish Thakker, Vassilina Nikoulina, Verena\nRieser, Vikas Raunak, Vitaly Protasov, Vladislav Mikhailov, Wilson Y. Lee, Yada Pruksachatkun,\nYonatan Belinkov, Zachary Bamberger, Zdenˇek Kasner, Zeerak Talat, Zheng-Xin Yong\nBroader Impacts\nAaron Gokaslan, Alexandra Sasha Luccioni, Alham Fikri Aji, Alice Rueda, Amanda Pestana, Amir\nFeizpour, Ammar Khan, Amy Faranak, Ana Santos, Angelina McMillan-Major, Anthony Hevia,\nAntigona Unldreaj, Arash Aghagol, Arezoo Abdollahi, Aycha Tammour, Azadeh HajiHosseini, Ba-\nhareh Behroozi, Benjamin Ajibade, Bharat Saxena, Carlos Mu˜noz Ferrandis, Chenghao Mou, Minh\nChien Vu, Christopher Akiki, Daniel McDuff, Danish Contractor, David Ifeoluwa Adelani, David\nLansky, Davis David, Douwe Kiela, Duong A. Nguyen, Edward Tan, Emi Baylor, Ezinwanne Ozoani,\nFatima Mirza, Frankline Ononiwu, Gérard Dupont, Giada Pistilli, Habib Rezanejad, Hessie Jones,\nHuu Nguyen, Ian Yu, Indrani Bhattacharya, Irene Solaiman, Irina Sedenko, Isar Nejadgholi, Jae-\nsung Tae, Jenny Chim, Jesse Dodge, Jesse Passmore, Josh Seltzer, Julien Launay, Julio Bonis Sanz,\nKhalid Almubarak, Livia Dutra, Long Phan, Mairon Samagaio, Manan Dey, Maraim Masoud, Mar-\ngaret Mitchell, Margot Mieskes, Marissa Gerchick, Martha Akinlolu, Michael McKenna, Mike Qiu,\nMuhammed Ghauri, Mykola Burynok, Nafis Abrar, Nazneen Rajani, Niklas Muennighoff, Nishant\nSubramani, Nour Elkott, Nour Fahmy, Olanrewaju Samuel, Olivier Nguyen, Paulo Villegas, Pawan\nSasanka Ammanamanchi, Priscilla Amuok, Ran An, Rasmus Kromann, Ryan Hao, Samira Alizadeh,\nSarmad Shubber, Shanya Sharma, Shayne Longpre, Silas Wang, Somaieh Nikpoor, Sourav Roy, Stas\nBekman, Stella Biderman, Suhas Pai, Suzana Ili´c, Sylvain Viguier, Teven Le Scao, Thanh Le, Tobi\nOyebade, Trieu Le, Tristan Thrush, Yacine Jernite, Yoyo Yang, Zach Nguyen, Zeerak Talat, Zheng-\nXin Yong\nApplications\nAbhinav Ramesh Kashyap, Albert Villanova del Moral, Alfredo Palasciano, Alison Callahan, Anima\nShukla, Antonio Miranda-Escalada, Ayush Singh, Benjamin Beilharz, Bo Wang, Caio Brito, Carlos\n2\n\nBLOOM\nMu˜noz Ferrandis, Chenxi Zhou, Chirag Jain, Christopher Akiki, Chuxin Xu, Clémentine Fourrier,\nDaniel León Peri˜nán, Daniel Molano, Daniel van Strien, Danish Contractor, David Lansky, Debajyoti\nDatta, Dian Yu, Enrique Manjavacas, Fabio Barth, Florian Fuhrimann, Francesco De Toni, Gabriel\nAltay, Giyaseddin Bayrak, Gully Burns, Helena U. Vrabec, Imane Bello, Ishani Dash, Jason Alan\nFries, Javier de la Rosa, Jenny Chim, Jihyun Kang, John Giorgi, Jonas Golde, Jose David Posada,\nKarthik Rangasai Sivaraman, Leon Weber, Lokesh Bulchandani, Lu Liu, Luisa Shinzato, Madeleine\nHahn de Bykhovetz, Maiko Takeuchi, Marc Pàmies, Maria A Castillo, Marianna Nezhurina, Mario\nS¨anger, Matthias Samwald, Michael Cullan, Michael Weinberg, Michiel De Wolf, Mina Mihaljcic,\nMinh Chien Vu, Minna Liu, Moritz Freidank, Myungsun Kang, Natasha Seelam, Nathan Dahlberg,\nNicholas Michio Broad, Nikolaus Muellner, Pascale Fung, Patrick Haller, Ramya Chandrasekhar,\nRenata Eisenberg, Robert Martin, Rodrigo Canalli, Rosaline Su, Ruisi Su, Samuel Cahyawijaya,\nSamuele Garda, Shamik Bose, Shlok S Deshmukh, Shubhanshu Mishra, Sid Kiblawi, Simon Ott,\nSinee Sang-aroonsiri, Srishti Kumar, Stefan Schweter, Stella Biderman, Stephen H. Bach, Sushil\nBharati, Tanmay Laud, Théo Gigant, Tomoya Kainuma, Trishala Neeraj, Wojciech Kusa, Yanis\nLabrak, Yash Shailesh Bajaj, Yash Venkatraman, Yifan Xu, Yingxin Xu, Yu Xu, Zhe Tan, Zhongli\nXie, Zifan Ye\nOrganization\nAngela Fan, Christopher Akiki, Douwe Kiela, Giada Pistilli, Margot Mieskes, Mathilde Bras, Matthias\nGallé, Suzana Ili´c, Yacine Jernite, Younes Belkada, Thomas Wolf\nAbstract\nLarge language models (LLMs) have been shown to be able to perform new tasks based on\na few demonstrations or natural language instructions. While these capabilities have led to\nwidespread adoption, most LLMs are developed by resource-rich organizations and are fre-\nquently kept from the public. As a step towards democratizing this powerful technology, we\npresent BLOOM, a 176B-parameter open-access language model designed and built thanks\nto a collaboration of hundreds of researchers. BLOOM is a decoder-only Transformer lan-\nguage model that was trained on the ROOTS corpus, a dataset comprising hundreds of\nsources in 46 natural and 13 programming languages (59 in total). We find that BLOOM\nachieves competitive performance on a wide variety of benchmarks, with stronger results\nafter undergoing multitask prompted finetuning. To facilitate future research and appli-\ncations using LLMs, we publicly release our models and code under the Responsible AI\nLicense.1\nKeywords:\nLanguage models, collaborative research\n1. Introduction\nPretrained language models have become a cornerstone of modern natural language pro-\ncessing (NLP) pipelines because they often produce better performance from smaller quan-\ntities of labeled data. The development of ELMo (Peters et al., 2018), ULMFiT (Howard\nand Ruder, 2018), GPT (Radford et al., 2018), and BERT (Devlin et al., 2019) led to the\nwidespread use of pretrained models as an initialization for finetuning on downstream tasks.\nThe subsequent finding that pretrained language models can perform useful tasks without\nany additional training (Radford et al., 2019; Brown et al., 2020) further demonstrated their\nutility. In addition, the empirical observation that a language model’s performance tends to\nincrease as the model is made larger—sometimes predictably (Hestness et al., 2017; Kaplan\n1. hf.co/bigscience/bloom\n3\n\nBigScience Workshop\net al., 2020; Hoffmann et al., 2022) and sometimes suddenly (Wei et al., 2022)—has led to a\ntrend of increasing scale (Zeng et al., 2021; Rae et al., 2021; Smith et al., 2022; Chowdhery\net al., 2022). Apart from environmental concerns (Strubell et al., 2019; Lacoste et al., 2019;\nSchwartz et al., 2020), the costs of training large language models (LLMs) are only afford-\nable for well-resourced organizations. Furthermore, until recently, most LLMs were not\npublicly released. As a result, the majority of the research community has been excluded\nfrom the development of LLMs. This exclusion has had concrete consequences; for exam-\nple, most LLMs are primarily trained on English-language text (with notable exceptions in\nChinese and Korean, e.g. Wang et al., 2021; Zeng et al., 2021; Kim et al., 2021).\nTo address these issues, we present the BigScience Large Open-science Open-access Mul-\ntilingual Language Model (BLOOM, BigScience Workshop, 2022). BLOOM is a 176 billion\nparameter language model trained on 46 natural languages and 13 programming languages\nthat was developed and released by a collaboration of hundreds of researchers. The com-\npute for training BLOOM was provided through a French public grant from GENCI and\nIDRIS, leveraging IDRIS’ Jean Zay supercomputer. To build BLOOM, we undertook a\nthorough design process for each of its components, including the training dataset (Sec-\ntion 3.1), model architecture and training objective (Section 3.2), and engineering strategy\nfor distributed learning (Section 3.4). We also performed an analysis of the model’s capa-\nbilities (Section 4). Our overall aim is not only to publicly release a large-scale multilingual\nlanguage model with performance comparable to recently developed systems, but also to\ndocument the coordinated process that went into its development (Section 2.2). The pur-\npose of this paper is to provide a high-level overview of these design steps while referencing\nthe individual reports we produced over the course of developing BLOOM.\n2. Background\nBefore describing the BLOOM model itself, in this section we provide necessary background\non LLMs as well as an organizational overview of the BigScience effort.\n2.1 Language Modeling\nLanguage modeling refers to the task of modeling the probability of a sequence of tokens in a\ntext (Shannon, 1948), where a token is a unit of text (e.g. word, subword, character or byte,\netc., as discussed by Mielke et al., 2021). In this work (and in most current applications of\nlanguage modeling) we model the joint probability of tokens in a text as:\np(x) = p(x1, . . . , xT ) =\nT\nY\nt=1\np(xt|x<t)\n(1)\nwhere x is a sequence of tokens, xt is the tth token, and x<t is the sequence of tokens\npreceding xt. This approach is referred to as autoregressive language modeling and can be\nseen as iteratively predicting the probability of the next token.\nEarly Language Models\nLanguage models have a long history of application in NLP.\nEarly language models (such as those developed by Shannon, 1948) were primarily n-gram\nmodels that estimate the probability of a length-n sequence of tokens in accordance with\n4\n\nBLOOM\nthe number of times it appears in a training corpus. In practice, n-gram models face two\nmajor issues: first, they grow exponentially in size as n is increased; and second, they have\nno direct way of producing a probability for a sequence of tokens that does not appear in\ntheir training data. Advances on these problems enabled n-gram models to see widespread\nuse across most areas of NLP (Goodman, 2001).\nNeural Language Models\nAn alternative to n-gram models, first proposed by Miikku-\nlainen and Dyer (1991) and Schmidhuber and Heil (1996) and later popularized by Bengio\net al. (2000), is to use a neural network to estimate the probability of the next token given\nprior tokens. While early work used feed-forward networks with a fixed-length history win-\ndow, Mikolov et al. (2010); Sutskever et al. (2011); Graves (2013) proposed to use recurrent\nneural networks instead and found that this significantly improved performance. More re-\ncently, language models based on the Transformer architecture (Vaswani et al., 2017) were\nshown to be more effective than recurrent neural networks (Radford et al., 2018; Al-Rfou\net al., 2019; Kaplan et al., 2020). Consequently, the Transformer has become the de facto\nchoice for language models.\nTransfer Learning\nIn tandem with advances in language modeling using neural net-\nworks, NLP pipelines have increasingly adopted the framework of transfer learning.\nIn\ntransfer learning, the parameters of a model are first pretrained on a data-rich task be-\nfore being finetuned on a downstream task. A historically common approach to obtaining\npretrained parameters were word vectors (Mikolov et al., 2013) trained so that the dot\nproduct of co-occurring word vectors is large. However, subsequent work by Peters et al.\n(2018); Howard and Ruder (2018); Radford et al. (2018); Devlin et al. (2019) showed that\nthe framework of Collobert et al. (2011), where the entire model is pretrained before being\nfinetuned, can attain stronger performance. In particular, Radford et al. (2018); Devlin\net al. (2019) demonstrated strong results using pretrained Transformer language models,\nprompting work on progressively better models (Liu et al., 2019; Yang et al., 2019; Lewis\net al., 2020; Raffel et al., 2020; Zhang et al., 2019, etc.).\nFew- and Zero-Shot Learning\nWhile finetuning a pretrained model remains an effective\nway of attaining high performance with limited labeled data, a parallel line of work has\ndemonstrated that pretrained language models can be induced to perform tasks without any\nsubsequent training. After Vinyals and Le (2015) observed limited task-performing behavior\nin a neural dialog model, Radford et al. (2019) later demonstrated that Transformer-based\nlanguage models trained on text scraped from the web could perform various tasks to\nvarying degrees.\nNotably, Radford et al. (2019) found that performance improved with\nmodel scale, inspiring work to characterize (Kaplan et al., 2020; Hoffmann et al., 2022) and\nexploit (Shoeybi et al., 2019; Brown et al., 2020; Smith et al., 2022; Chowdhery et al., 2022;\nRae et al., 2021; Wang et al., 2021; Zeng et al., 2021; Zhang et al., 2022) the benefits of scale.\nA major factor in the success of this approach is the way that task-specific examples are\nformatted when fed into the model. Brown et al. (2020) popularized the idea of designing\n“prompts” that provide natural-language descriptions of the task and also allow inputting\na few demonstrations of input-output behavior.\nSocial Limitations of LLM Development\nWhile the continued increase in the size of\nlarge language models has resulted in improvements across a wide range of tasks, it has also\n5\n\nBigScience Workshop\nexacerbated issues with their development and use (Bender et al., 2021). The computational\nexpense of large models also prohibits the majority of the research community from partici-\npating in their development, evaluation and routine use. Moreover, the computational costs\nhave also lead to concerns about the carbon footprint stemming from the training and use\nof large language models (Strubell et al., 2019; Lacoste et al., 2019; Schwartz et al., 2020;\nBannour et al., 2021), and existing carbon footprint studies have likely under-estimated\nemissions (Bannour et al., 2021). Contributing to an increase in the global carbon footprint\nexacerbates climate change which most severely affects already-marginalized communities\n(Westra and Lawson, 2001). Furthermore, the concentration of resources within a handful\nof (typically industrial) institutions with primarily technical expertise hinders prospects\nfor an inclusive, collaborative, and reliable governance of the technology.\nFirst, public\nnarratives about the technology that are driven by industry actors can lead to inflated\nexpectations about its suitability for use (Brennen, 2018; Brennen et al., 2022), leading\nto misaligned research and policy priorities (Raji et al., 2022) and potentially dire conse-\nquences in e.g. medical applications (Wong et al., 2021). Second, in a world mediated by\ntechnology, choices at all stages of its development end up shaping people’s lives in a way\nthat can be most closely compared to regulations (Winner, 1977, 2017), albeit without the\nsame explicit consultation of stakeholders in the process. When the development efforts are\nguided by prioritizing internal definitions of performance over their impact on society, the\nvalues of the developers come to be emphasized over those of the direct and indirect users\n(Birhane et al., 2022). Despite the substantial social dangers in allowing this technology\nto be developed unilaterally by corporations, EleutherAI (Phang et al., 2022) was the only\nnon-corporate entity outside of China that was developing large language models before the\nBigScience Workshop was convened.\n2.2 BigScience\nParticipants\nBLOOM’s development was coordinated by BigScience, an open research\ncollaboration whose goal was the public release of an LLM. The project started after being\nawarded by GENCI a compute grant on its Jean Zay supercomputer at IDRIS/CNRS. It was\ninitially built around a concerted effort from Hugging Face and the French NLP community\n(the “founding members”), and quickly opened up to grow into a broader international\ncollaboration to support its aims of linguistic, geographical, and scientific diversity.\nIn\nthe end, over 1200 people registered as participants in BigScience and were given access\nto its communication channels. They had background not only in machine learning and\ncomputer science, but also linguistics, statistics, socio-cultural anthropology, philosophy,\nlaw, and other fields. Of those, hundreds of individuals have directly contributed to one\nof the project’s released artifacts.\nWhile the largest number of participants ultimately\noriginated from the US, 38 countries were represented.\nOrganization\nThe set of related research questions tackled by the BigScience effort was\nreflected in the project’s organization into working groups. Each working group comprised\nseveral participants with various levels of involvement, including chairs whose role was\nto self-organize around a specific aspect of the overall project. Importantly, participants\nwere encouraged to join more than one working group in order to share experiences and\ninformation, which resulted in the set of 30 working groups presented in Figure 1. Most\n6\n\nBLOOM\nof the working groups focused on tasks directly linked to the development of BLOOM.\nIn addition, a few groups focused on the evaluation of LLMs and dataset development in\nspecific domains, such as biomedical texts (Fries et al., 2022b) and historical texts (De Toni\net al., 2022). A larger overview of the motivations behind this initiative, its history and\nsome of the lessons learned can be found in Akiki et al. (2022).\nSourcing\nGovernance\nTooling\nAnalysis\nData \nBiomedical\nHistorical Texts\nDomains\nExtrinsic\nIntrinsic\nFew-shot\nInterpretability\nEvaluation\nMeta-WG Social\nEnviromental\nMedia\nBloom Book\nExternal impact\nBias-Fairness\nMultilinguality\nOrganization\nCollaborations\nEngineering\nModel Sharing\nCross areas\nTokenization\nMetadata\nMultilinguality\nArchitecture\nModeling\nRetrieval\nPrompting\nModel Card\nEthical and Legal\nHackathon\nData preparation\nFigure 1: Organization of BigScience working groups.\nEthical Considerations within BigScience\nIn order to acknowledge and start ad-\ndressing social limitations of LLM development within BigScience, the workshop relied on a\ncollaboratively designed Ethical Charter2 and original research on applicable regulations in\njurisdictions outside of the US3 to guide its choices throughout the project. In particular, the\ncharter emphasizes values of inclusivity and diversity, openness and reproducibil-\nity, and responsibility in various aspects of the organization (Akiki et al., 2022). Each of\nthese values are showcased in different ways in the dataset curation (Section 3.1), model-\ning (Section 3.2), engineering (Section 3.4), evaluation (Section 4), and other social impact\n(throughout) aspects of the project.\n3. BLOOM\nIn this section, we document the design of BLOOM, including its training dataset (Sec-\ntion 3.1), architecture (Section 3.2), tokenizer (Section 3.3), computing infrastructure (Sec-\ntion 3.4), and training hyperparameters (Section 3.5).\n3.1 Training Dataset\nBLOOM was trained on the ROOTS corpus (Lauren¸con et al., 2022), a composite collection\nof 498 Hugging Face datasets (Lhoest et al., 2021) amounting to 1.61 terabytes of text that\nspan 46 natural languages and 13 programming languages. A high-level overview of this\ndataset can be seen in Figure 3, while a detailed itemized list of every language along with\nits linguistic genus, family and macroarea is presented in Table 1.\nBeyond the corpus\nitself, the process resulted in the development and release of a number of organizational\nand technical tools, including those illustrated in Figure 2. The rest of this section will\n2. bigscience.huggingface.co/blog/bigscience-ethical-charter\n3. bigscience.huggingface.co/blog/legal-playbook-for-natural-language-processing-researchers\n7\n\nBigScience Workshop\nLanguage\nISO-639-3\ncatalog-ref\nGenus\nFamily\nMacroarea\nSize in Bytes\nAkan\naka\nak\nKwa\nNiger-Congo\nAfrica\n70,1554\nArabic\narb\nar\nSemitic\nAfro-Asiatic\nEurasia\n74,854,900,600\nAssamese\nasm\nas\nIndic\nIndo-European\nEurasia\n291,522,098\nBambara\nbam\nbm\nWestern Mande\nMande\nAfrica\n391,747\nBasque\neus\neu\nBasque\nBasque\nEurasia\n2,360,470,848\nBengali\nben\nbn\nIndic\nIndo-European\nEurasia\n18,606,823,104\nCatalan\ncat\nca\nRomance\nIndo-European\nEurasia\n17,792,493,289\nChichewa\nnya\nny\nBantoid\nNiger-Congo\nAfrica\n1,187,405\nchiShona\nsna\nsn\nBantoid\nNiger-Congo\nAfrica\n6,638,639\nChitumbuka\ntum\ntum\nBantoid\nNiger-Congo\nAfrica\n170,360\nEnglish\neng\nen\nGermanic\nIndo-European\nEurasia\n484,953,009,124\nFon\nfon\nfon\nKwa\nNiger-Congo\nAfrica\n2,478,546\nFrench\nfra\nfr\nRomance\nIndo-European\nEurasia\n208,242,620,434\nGujarati\nguj\ngu\nIndic\nIndo-European\nEurasia\n1,199,986,460\nHindi\nhin\nhi\nIndic\nIndo-European\nEurasia\n24,622,119,985\nIgbo\nibo\nig\nIgboid\nNiger-Congo\nAfrica\n14078,521\nIndonesian\nind\nid\nMalayo-Sumbawan\nAustronesian\nPapunesia\n19,972,325,222\nisiXhosa\nxho\nxh\nBantoid\nNiger-Congo\nAfrica\n14,304,074\nisiZulu\nzul\nzu\nBantoid\nNiger-Congo\nAfrica\n8,511,561\nKannada\nkan\nkn\nSouthern Dravidian\nDravidian\nEurasia\n2,098,453,560\nKikuyu\nkik\nki\nBantoid\nNiger-Congo\nAfrica\n359,615\nKinyarwanda\nkin\nrw\nBantoid\nNiger-Congo\nAfrica\n40,428,299\nKirundi\nrun\nrn\nBantoid\nNiger-Congo\nAfrica\n3,272,550\nLingala\nlin\nln\nBantoid\nNiger-Congo\nAfrica\n1,650,804\nLuganda\nlug\nlg\nBantoid\nNiger-Congo\nAfrica\n4,568,367\nMalayalam\nmal\nml\nSouthern Dravidian\nDravidian\nEurasia\n3,662,571,498\nMarathi\nmar\nmr\nIndic\nIndo-European\nEurasia\n1,775,483,122\nNepali\nnep\nne\nIndic\nIndo-European\nEurasia\n2,551,307,393\nNorthern Sotho\nnso\nnso\nBantoid\nNiger-Congo\nAfrica\n1,764,506\nOdia\nori\nor\nIndic\nIndo-European\nEurasia\n1,157,100,133\nPortuguese\npor\npt\nRomance\nIndo-European\nEurasia\n79,277,543,375\nPunjabi\npan\npa\nIndic\nIndo-European\nEurasia\n1,572,109,752\nSesotho\nsot\nst\nBantoid\nNiger-Congo\nAfrica\n751,034\nSetswana\ntsn\ntn\nBantoid\nNiger-Congo\nAfrica\n1,502,200\nSimplified Chinese\n—\nzhs\nChinese\nSino-Tibetan\nEurasia\n261,019,433,892\nSpanish\nspa\nes\nRomance\nIndo-European\nEurasia\n175,098,365,045\nSwahili\nswh\nsw\nBantoid\nNiger-Congo\nAfrica\n236,482,543\nTamil\ntam\nta\nSouthern Dravidian\nDravidian\nEurasia\n7,989,206,220\nTelugu\ntel\nte\nSouth-Central Dravidian\nDravidian\nEurasia\n2993407,159\nTraditional Chinese\n—\nzht\nChinese\nSino-Tibetan\nEurasia\n762,489,150\nTwi\ntwi\ntw\nKwa\nNiger-Congo\nAfrica\n1,265,041\nUrdu\nurd\nur\nIndic\nIndo-European\nEurasia\n2,781,329,959\nVietnamese\nvie\nvi\nViet-Muong\nAustro-Asiatic\nEurasia\n43,709,279,959\nWolof\nwol\nwo\nWolof\nNiger-Congo\nAfrica\n3,606,973\nXitsonga\ntso\nts\nBantoid\nNiger-Congo\nAfrica\n707,634\nYoruba\nyor\nyo\nDefoid\nNiger-Congo\nAfrica\n89,695,835\nProgramming Languages\n—\n—\n—\n—\n174,700,245,772\nTable 1: Linguistic makeup of the ROOTS corpus.\n8\n\nBLOOM\ncontextualize these efforts by providing a brief summary of the steps taken to compile the\ncorpus. For more detailed documentation of the overall dataset curation process and its\noutcomes, we refer the reader to Lauren¸con et al. (2022).\nMotivation\nThe disconnect between developers and (in)voluntary users of the technology\nmentioned in Section 2 is particularly apparent in the curation of the datasets that have\nsupported recent large-scale machine learning projects, where intentional “Data work” is\ngenerally under-valued (Sambasivan et al., 2021). In the context of LLMs, this tendency\nis exemplified by a range of heuristics-based filtering approaches that prioritize getting as\nmuch “high-quality” data for as little cost as possible over engaging with the needs—and\nrights—of data subjects, where quality is commonly defined as maximizing performance on\ndownstream tasks while occasionally removing content deemed offensive by the developers.\nWhile these approaches do yield terabytes of data with comparatively little human effort,\ncompounding biases of the source material (such as CommonCrawl dumps) with those of\nthe filtering method often leads to negative outcomes for marginalized populations.\nIn\none case, the use of a block list to remove “pornographic” text was shown to also suppress\nLGBTQ+ and African American English (AAE) text from a corpus (Dodge et al., 2021). In\nanother, using Reddit outgoing links as an indicator of quality for a seed corpus (Radford\net al., 2019) leads to trained models that implicitly prioritize US-centric views in their\noutputs (Johnson et al., 2022). In yet another project, a filtering approach that relied on\na machine learning image-text alignment model was shown to exacerbate its biases in the\ncreated multimodal dataset (Birhane et al., 2021). In addition, this abstractive approach\nto data curation leads to corpora that are difficult to meaningfully document and govern\nafter the fact, as the provenance and authorship of individual items is usually lost in the\nprocess (although works such as Gao et al. (2020) that prioritize compilations of previously\ndocumented individual sources over crawled data provide a step towards addressing these\nissues (Biderman et al., 2022)).\nIn the context of the BigScience workshop, and in accordance with its Ethical Charter,4\nwe aimed to prioritize human involvement, local expertise, and language expertise in our\ndata curation and documentation process, as outlined in the following sections.\n3.1.1 Data Governance\nLarge text corpora comprise text about and created by people: the data subjects. Different\npeople and institutions might legally “own” that data, making them data rights-holders. As\nmachine learning developers gather and collate that data into ever-larger datasets to support\ntraining larger models, it becomes increasingly important to develop new ways of accounting\nfor the interests of all parties involved – developers, data subjects, and rights-holders alike.\nThe BigScience effort aimed to address these needs through a multidisciplinary lens\ncombining technical, legal, and sociological expertise.\nThe group focused on two main\ninterrelated goals at two different time scales: the design of a structure for long-term inter-\nnational data governance that prioritizes the agency of the data rights-holders, and concrete\nrecommendations for handling the data used directly in the BigScience project. Progress on\nthe first goal is presented in the work of Jernite et al. (2022), which further motivates the\nneeds and requirements of data governance, and outlines the structure needed for a network\n4. bigscience.huggingface.co/blog/bigscience-ethical-charter\n9\n\nBigScience Workshop\nof data custodians, rights-holders, and other parties to appropriately govern shared data.\nThe interactions between these actors are designed to account for the privacy, intellectual\nproperty, and user rights of the data and algorithm subjects in a way that aims to prioritize\nlocal knowledge and expression of guiding values. In particular, this approach relies on\nstructured agreements between data providers and data hosts5 that specify what the data\nmay be used for.\nWhile we were not able to fully establish an international organization in the compar-\natively short time between the project start and model training, we worked on integrating\nlessons from this effort (and conversely adapting it to the practical concerns we were ex-\nperiencing) in the following main ways: (i) we sought explicit permission to use the data\nfrom specific providers within the context of BigScience whenever possible (such as for\nthe AI26-managed S2ORC corpus of Lo et al. (2020) or articles from the French newspaper\nLe Monde7); (ii) we kept individual sources separate until the final stages of preprocessing\nto maintain traceability and handle each according to the needs of its specific context; and\n(iii) we adopted a composite release approach for the various data sources that make up the\noverall corpus to foster reproducibility and follow-up research while respecting these source-\ndependent needs. Resources to visualize and access the ROOTS corpus can be found on the\nHugging Face Hub organization “BigScience Data”.8 The organization hosts several demos\n(or “Spaces”) that can be used to gain insights into the full corpus, as well as direct access\nto the 223 (out of 498) components that we are able to distribute taking into account their\nlicensing status, privacy risks, and agreements with their original custodians. Finally, since\nwe understand that future investigation into the BLOOM models may require full access to\nthe entire corpus, we are also inviting researchers with a relevant research project in mind\nto join ongoing efforts to analyze the data through a sign-up form.9\n3.1.2 Data Sources\nGiven a strategy for data governance, the next step was to determine the composition of\nthe training corpus. This stage was driven by several goals, which sometimes had inherent\ntensions. Some of those tensions included building a language model that was accessible to\nas many people as possible around the world while only including languages for which we had\nenough expertise to curate a dataset of comparable scale (and to a lesser extent composition)\nto previous efforts while improving the standards of documentation and respect for data\nand algorithm subject rights.\nLanguage Choices\nThese considerations led us to an incremental process for choosing\nwhich languages were to be included in the corpus. We started with a list of eight of the\nworld’s largest languages by number of speakers for which we did active outreach in the\nearly stages of the project to invite fluent speakers to join the data efforts. Then, on the\nrecommendation of language communities (Nekoto et al., 2020) we expanded Swahili in\nthe original selection to the category of Niger-Congo languages, and Hindi and Urdu to\n5. hf.co/spaces/bigscience/data_host_provider_agreement\n6. allenai.org\n7. lemonde.fr\n8. hf.co/bigscience-data\n9. forms.gle/qyYswbEL5kA23Wu99\n10\n\nBLOOM\nIndic languages (Kunchukuttan et al., 2020). Finally, we proposed that any group of 3 or\nmore participants fluent in an additional language could add it to the supported list if they\nwould commit to selecting sources and guiding processing choices in the language in order\nto avoid common issues with corpora selected through automatic language identification\nwithout specific language expertise (Caswell et al., 2022).\nSource Selection\nThe biggest part of the corpus was curated by workshop participants\nand research collectives who collectively compiled the “BigScience Catalogue”: a large list\nof processed and non-processed sources covering a wide range of languages.\nThis took\nthe form of hackathons that were co-organized by communities such as Machine Learning\nTokyo, Masakhane, and LatinX in AI (McMillan-Major et al., 2022). Complementary to\nthose efforts, other working group participants compiled language-specific resources such as\nthe Arabic-focused Masader repository (Alyafeai et al., 2021; Altaher et al., 2022). A total\nof 252 sources were identified through this bottom-up approach, with at least 21 sources\nper language category. Additionally, in order to increase the geographic coverage of some of\nour Spanish, Chinese, French, and English sources, participants identified locally relevant\nwebsites in their language to be added to the corpus via pseudocrawl, a method to obtain\nthose websites from a Common Crawl snapshot.\nGitHub Code\nThe catalogue was further complemented with a dataset of programming\nlanguages collected from the GitHub data collection on Google’s BigQuery,10 which was\nthen deduplicated of exact matches. The choice of languages to include mirrored the design\nchoices introduced by Li et al. (2022) to train the AlphaCode model.\nOSCAR\nBoth in an effort not to diverge from the standard research practice of using\nthe Web as a source of pretraining data (Radford et al., 2018; Raffel et al., 2020), and\nalso to satisfy the data volume needs of our compute budget given the size of BLOOM,\nwe further sourced data from OSCAR version 21.09, corresponding to the February 2021\nsnapshot of the Common Crawl (Ortiz Suárez et al., 2019; Abadji et al., 2021), which ended\nup constituting 38% of the corpus.\n3.1.3 Data Preprocessing\nAfter the sources had been identified, data processing involved several steps to handle mul-\ntiple aspects of data curation. An overarching view of and processing pipeline to build\nROOTS can be seen in Figure 2.\nAll tools developed in the process are available on\nGitHub.11\nObtaining the Source Data\nThe first step involved obtaining the data for all of the text\ndata sources identified in Section 3.1.2, which consisted of a combination of downloading\nand extracting the text field from a variety of NLP datasets in various formats (including\ne.g. question answering, summarization, or dialogue datasets), scraping and processing large\namounts of PDF files from archives (e.g. the French repository of scientific articles12), and\nextracting and preprocessing text from 192 website entries from the catalogue and another\n10. cloud.google.com/blog/topics/public-datasets/github-on-bigquery-analyze-all-the-open-\nsource-code\n11. github.com/bigscience-workshop/data-preparation\n12. hal.archives-ouvertes.fr\n11\n\nBigScience Workshop\nSourcing\nPre-processing\nCrowdsourced Datasets\nCommon Crawl-based Dataset\nOSCAR\nmanual merging & source-level\ndeduplication\nStore\nIdentified Datasets\nand Collections\nPseudo-Crawled\nData\nGitHub Code\nsemi-automatic\ncleaning & filtering & deduplication\npersonal identifiable information\nremoval\nsemi-automatic\ncleaning & filtering & deduplication\nFigure 2: Creation Pipeline of the ROOTS Corpus. The purple-colored sourcing stage of the\npipeline and the yellow-colored processing stage are described respectively in Section 3.1.2\nand Section 3.1.3.\ngeographically diverse set of 456 websites selected by data working group members. The\nlatter required the development of new tools to extract text from the HTML in the Common\nCrawl WARC files, which we made available on the main data preparation repository.13 We\nwere able to find and extract usable text data from all URLs present in 539 of the websites.\n“Quality” filtering: Text Produced by Humans for Humans\nAfter obtaining the\ntext, we found that most of the sources contained some amount of text that was not natural\nlanguage, for example preprocessing errors, SEO pages, or spam (including pornographic\nspam).\nIn order to filter non-natural language, we defined a set of quality indicators,\nwhere high-quality text is defined as “written by humans for humans”, without distinction of\ncontent (as we wanted content selection to exclusively be the domain of the more accountable\nhuman source selection) or a priori judgments of grammaticality. The full list of indicators\nare described in (Lauren¸con et al., 2022). Importantly, the indicators were adapted to the\nneeds of each of the sources in two main ways. First, their parameters such as the thresholds\nand supporting term lists were selected individually for each language by fluent speakers.\nSecond, we manually went through each individual source to identify which indicators were\nmost likely to identify non-natural language. Both processes were supported by tools to\nvisualize their impact.14,15\n13. github.com/bigscience-workshop/data-preparation/tree/main/sourcing/cc_pseudo_crawl\n14. hf.co/spaces/huggingface/text-data-filtering\n15. hf.co/spaces/bigscience-data/process-pipeline-visualizer\n12\n\nBLOOM\n \nEurasia\nAfrica\nIndo-European\nSino-Tibetan\nAfro-Asiatic\nAustro-Asiatic\nDravidian\nRomance\nGermanic\nIndic\nChinese\nSemitic\nViet-Muong\nFrench\nSpanish\nPortuguese\nCatalan\nEnglish\nSimplified Chinese\nArabic\nVietnamese\nFigure 3: Graphical overview of the ROOTS corpus. Left: A treemap plot of the language\nfamilies of all 46 natural languages where surface is proportional to the number of bytes.\nIndo-European and Sino-Tibetan families overwhelm the plot with a combined total of\n1321.89 GB. The thin orange surface represents 18GB of Indonesian data and the green\nrectangle 0.4GB constituting the Niger-Congo language family subset. Right: A waffle plot\nof the distribution of the 13 programming languages by size, where one square represents\napproximately 200MB.\nDeduplication and Privacy Redaction\nFinally, we removed near-duplicate documents\nwith two deduplication steps and redacted Personal Identifiable Information (such as social\nsecurity numbers) that we could identify from the OSCAR version of the corpus—as it was\ndeemed to be the source that presented the highest privacy risks, prompting us to apply\nregex-based redaction even in cases where the expressions had some false positives.\n3.1.4 Prompted Datasets\nen es pt\nfr\nar\nid\nzh\nhi code vi\nur\nte\nta bn mr sw gu pa ne yo\nig ny zu xh sn\nts rw lg\ntn nso rn ml kn or as\nln wotum ki\nst fon ca eu ak bm tw\n25\n5\n1\n0.1\n0.01\n0.001\n0.0001\n% of corpus\nxP3\nROOTS\nFigure 4: Language distribution of the prompted dataset xP3 closely follows ROOTS.\n.\nMultitask prompted finetuning (also referred to as instruction tuning) involves finetun-\ning a pretrained language model on a training mixture composed of a large set of different\ntasks specified through natural language prompts. T0 (Sanh et al., 2022) (developed as\npart of BigScience) demonstrated that language models finetuned on a multitask mixture\nof prompted datasets have strong zero-shot task generalization abilities. Moreover, T0 was\nshown to outperform language models that are an order of magnitude larger but did not\nundergo such finetuning. Motivated by these results, we explored using existing natural\nlanguage datasets to carry out multitask prompted finetuning.\n13\n\nBigScience Workshop\nT0 was trained on a subset of the Public Pool of Prompts (P3), a collection of prompts\nfor various existing and open-source English natural language datasets.\nThis collection\nof prompts was created through a series of hackathons involving BigScience collaborators\nand where hackathon participants wrote a total of of 2000+ prompts for 170+ datasets.\nDatasets in P3 cover a variety of natural language task including sentiment analysis, ques-\ntion answering, and natural language inference and exclude harmful content or non-natural\nlanguage such as programming languages. PromptSource (Bach et al., 2022),16 an open-\nsource toolkit (also developed as part of BigScience) facilitated creating, sharing and using\nnatural language prompts. Full details of the collection process are given in (Sanh et al.,\n2022; Bach et al., 2022).\nAfter pretraining BLOOM, we applied the same massively multitask finetuning recipe\nto equip BLOOM with multilingual zero-shot task generalization abilities. We refer to the\nresulting models as BLOOMZ. To train BLOOMZ, we extended P3 to include new datasets\nin languages other than English and new tasks, such as translation. This resulted in xP3,\na collection of prompts for 83 datasets covering 46 languages and 16 tasks. As highlighted\nin Figure 4, xP3 mirrors the language distribution of ROOTS. Tasks in xP3 are both cross-\nlingual (e.g. translation) and monolingual (e.g. summarization, question answering). We\nused PromptSource to collect these prompts, adding additional metadata to the prompts,\nsuch as input and target languages.\nTo study the importance of multilingual prompts,\nwe also machine-translated English prompts in xP3 to the respective dataset languages to\nproduce a collection called xP3mt. Further details on the prompt collection for xP3 and\nxP3mt are given in Muennighoff et al. (2022b).\n3.2 Model Architecture\nThis section discusses our design methodology and the architecture of the BLOOM model.\nIn-depth studies and experiments can be found in Le Scao et al. (2022) and Wang et al.\n(2022a). We first review our design methodology, then motivate our choice of training a\ncausal decoder-only model. Finally, we justify the ways that our model architecture deviates\nfrom standard practice.\n3.2.1 Design Methodology\nThe design space of possible architectures is immense, making exhaustive exploration impos-\nsible. One option would be to exactly replicate the architecture of an existing large language\nmodel. On the other hand, a great deal of work on improving existing architectures has\nseen relatively little adoption (Narang et al., 2021); adopting some of these recommended\npractices could yield a significantly better model. We take a middle ground and focus on\nmodel families that have been shown to scale well, and that have reasonable support in\npublicly available tools and codebases. We ablate components and hyperparameters of the\nmodels, seeking to make the best use of our final compute budget.\nExperimental Design for Ablations\nOne of the main draws of LLMs has been their\nability to perform tasks in a “zero/few-shot” way: large enough models can perform novel\ntasks simply from in-context instructions and examples (Radford et al., 2019), without ded-\n16. github.com/bigscience-workshop/promptsource\n14\n\nBLOOM\nicated training on supervised samples. Accordingly, and because finetuning a 100B+ model\nis unwieldy, we focused our evaluation of architectural decisions on zero-shot generalization,\nand do not consider transfer learning. Specifically, we measured zero-shot performance on\ndiverse aggregates of tasks: 29 tasks from the EleutherAI Language Model Evaluation Har-\nness (EAI-Eval, Gao et al. (2021)), and 9 tasks from the evaluation set of T0 (T0-Eval, Sanh\net al. (2022)). There is significant overlap between the two: only one task from T0-Eval\n(StoryCloze) is not in EAI-Eval, although all prompts between the two are different. See\nLe Scao et al. (2022) for a detailed list of tasks and baselines. We also note that our tasks\naggregates share 17 of the 31 tasks of the evaluation of GPT-3 (Brown et al., 2020).\nWe conducted our ablation experiments using smaller models. We used the 6.7B pa-\nrameter scale for the pretraining objective ablations (Wang et al., 2022a) and the 1.3B scale\nfor the rest including position embeddings, activations, and layer normalization (Le Scao\net al., 2022). Recently, Dettmers et al. (2022) identified a phase transition for models larger\nthan 6.7B, in which the emergence of “outliers features” is observed. This questions whether\nresults obtained at the 1.3B scale should be assumed to extrapolate to our final model size.\nOut-of-scope Architectures\nWe did not consider mixture-of-experts (MoE) (Shazeer\net al., 2017), due to a lack of widely used GPU-based codebases suitable for training them\nat scale. Similarly, we also did not consider state-space models (Gu et al., 2020). At the\ntime of the design of BLOOM, they consistently underperformed in natural language tasks\n(Gu et al., 2021). Both of these approaches are promising, and have now demonstrated\ncompetitive results–at large scales for MoE (Fedus et al., 2022; Srivastava et al., 2022), and\nat smaller scale for state-space models with H3 (Fu et al., 2023).\n3.2.2 Architecture and Pretraining Objective\nAlthough most modern language models are based on the Transformer architecture, there\nare significant deviations between architectural implementations. Notably, while the original\nTransformer is based on an encoder-decoder architecture, many popular models have opted\nfor encoder-only (e.g. BERT, (Devlin et al., 2019)) or decoder-only (e.g. GPT, (Radford\net al., 2018)) approaches. Currently, all state-of-the-art language models over 100 billion\nparameters are causal decoder-only models (Brown et al., 2020; Rae et al., 2021; Chowdhery\net al., 2022). This is in opposition to the findings of Raffel et al. (2020), in which encoder-\ndecoder models significantly outperform decoder-only models for transfer learning.\nPrior to our work, the literature was lacking a systematic evaluation of the zero-shot\ngeneralization capabilities of different architectures and pretraining objectives. We explored\nthis question in Wang et al. (2022a) where we evaluated encoder-decoder and decoder-only\narchitectures and their interactions with causal, prefix, and masked language modeling\npretraining objectives. Our results show that immediately after pretraining, causal decoder-\nonly models performed best – validating the choice of state-of-the-art LLMs. Furthermore,\nthey can be more efficiently adapted after pretraining to a non-causal architecture and\nobjective–an approach which has been further explored and confirmed by Tay et al. (2022).\n3.2.3 Modeling Details\nBeyond choosing an architecture and pretraining objective, a number of changes to the\noriginal Transformer architecture have been proposed. For example, alternative positional\n15\n\nBigScience Workshop\nembedding schemes (Su et al., 2021; Press et al., 2021) or novel activation functions (Shazeer,\n2020). We thus performed a series of experiments to evaluate the benefit of each of these\nmodifications for a causal decoder-only model in Le Scao et al. (2022). We adopted two\narchitectural deviations in BLOOM:\nALiBi Positional Embeddings\nInstead of adding positional information to the embed-\nding layer, ALiBi directly attenuates the attention scores based on how far away the keys\nand queries are (Press et al., 2021). Although ALiBi was initially motivated by its abil-\nity to extrapolate to longer sequences, we found it also led to smoother training and better\ndownstream performance even at the original sequence length – outperforming both learned\n(Vaswani et al., 2017) and rotary (Su et al., 2021) embeddings.\nEmbedding LayerNorm\nIn preliminary experiments training a 104B parameters model,\nwe experimented with an additional layer normalization immediately after the embedding\nlayer – as recommended by the bitsandbytes17 library (Dettmers et al., 2022) with its\nStableEmbedding layer.\nWe found this significantly improved training stability.\nEven\nthough we also found it penalizes zero-shot generalization in Le Scao et al. (2022), we train\nBLOOM with an additional layer normalization after the first embedding layer to avoid\ntraining instabilities. Note the preliminary 104B experiments were conducted in float16,\nwhile the final training was in bfloat16. Since then, float16 has been attributed as being\nresponsible for many of the observed instabilities in training LLMs (Zhang et al., 2022; Zeng\net al., 2022). It is possible that bfloat16 alleviates the need for the embedding LayerNorm.\nWe represent the full architecture of BLOOM in figure 5 for reference.\nLN\nMLP\nLN\n+\n+\nLN\nMLP\nLN\n+\n+\n+\nsoftmax\nvalue combination\n+\n+\nkey-query product\nvalue combination\nkey-query product\n+\n+\nsoftmax\nToken1\nToken2\nToken3\nEmbed\nEmbed\nEmbed\nLN\nLN\nLN\nDecoder Block (x 70)\nSoftmax\nSoftmax\nSoftmax\nMulti-Head Attention\nLN\nMLP\n⋅  khead\nLN\n+\n+\n+\nSoftmax\nWeighted sum of values\n+\n+\nKey-query product\n+\n0\n0\n0\n-1\n-1\n-2\nALIBI mask\n-∞-∞\n-∞\nEmbedT\nEmbedT\nEmbedT\nLN\nLN\nLN\nHead fusion\nFigure 5: The BLOOM architecture. The khead slope parameters for ALIBI are taken as\n2\n−8i\nn\nwith n the number of heads and i ∈1, 2, ..., n.\n17. github.com/TimDettmers/bitsandbytes\n16\n\nBLOOM\n3.3 Tokenization\nThe design decisions when training a tokenizer are often neglected in favour of “default”\nsettings (Mielke et al., 2021). For instance, OPT (Zhang et al., 2022) and GPT-3 (Brown\net al., 2020) both use GPT-2’s tokenizer, trained for English. This can be justified by the\nfact that evaluating the impact of a particular choice on the downstream performance of\nthe model is constrained by the large computational costs of training. However, the diverse\nnature of BLOOM’s training data requires careful design choices to ensure that the tokenizer\nencodes sentences in a lossless manner.\nValidation\nWe use the fertility (Ács, 2019) of our tokenizer compared to existing monolin-\ngual tokenizers as a metric for sanity checks. Fertility is defined as the number of subwords\ncreated per word or per dataset by the tokenizer, which we measured using subsets of\nUniversal Dependencies 2.9 (Nivre et al., 2017) and OSCAR (Ortiz Suárez et al., 2019) in\nthe languages of interest. A very high fertility on a language compared to a monolingual\ntokenizer may indicate a degradation on the downstream multilingual performance of the\nmodel (Rust et al., 2021). Our goal was to not degrade the fertility on each language by more\nthan 10 percentage points when comparing our multilingual tokenizer with monolingual to-\nkenizers in corresponding languages.\nFor all experiments, the Hugging Face Tokenizers\nlibrary (Moi et al., 2019) was used to design and train the tested tokenizers.\nTokenizer\nfr\nen\nes\nzh\nhi\nar\nMonolingual\n1.30\n1.15\n1.12\n1.50\n1.07\n1.16\nBLOOM\n1.17 (-11%)\n1.15 (+0%)\n1.16 (+3%)\n1.58 (+5%)\n1.18 (+9%)\n1.34 (+13%)\nTable 2: Fertilities obtained on Universal Dependencies treebanks on languages with ex-\nisting monolingual tokenizers.\nThe monolingual tokenizers we used were the ones from\nCamemBERT (Martin et al., 2020), GPT-2 (Radford et al., 2019), DeepESP/gpt2-spanish,\nbert-base-chinese, monsoon-nlp/hindi-bert and Arabic BERT (Safaya et al., 2020), all\navailable on the HuggingFace Hub.\nTokenizer Training Data\nWe initially used a non-deduplicated subset of ROOTS. How-\never, a qualitative study on the vocabulary of the tokenizer revealed issues in its training\ndata. For instance, in earlier versions of the tokenizer, we found entire URLs stored as\ntokens caused by several documents containing a high number of duplicates. These issues\nmotivated us to remove duplicated lines in the tokenizer training training data. We then\napplied the same sampling ratios per language as for the training data.\nVocabulary Size\nA large vocabulary size reduces the risk of over-segmenting some sen-\ntences, especially for low-resource languages. We conducted validation experiments using\n150k and 250k vocabulary sizes to make comparisons with existing multilingual modeling\nliterature easier (Conneau et al., 2020; Xue et al., 2021). We ultimately settled for a vo-\ncabulary of 250k tokens to reach our initial fertility objective compared to monolingual\ntokenizers. Since the vocabulary size determines the embedding matrix size, it also had to\n17\n\nBigScience Workshop\nbe divisible by 128 for GPU efficiency reasons and by 4 to be able to use Tensor Parallelism.\nWe used a final size of 250,680 vocabulary items with 200 tokens reserved for possible future\napplications such as removing private information using placeholder tokens.\nByte-level BPE\nThe tokenizer is a learned subword tokenizer trained using the Byte Pair\nEncoding (BPE) algorithm introduced by Gage (1994). In order not to lose information\nduring tokenization, the tokenizer creates merges starting from bytes as the smallest units\ninstead of characters (Radford et al., 2019). This way, tokenization never results in unknown\ntokens because all 256 bytes can be contained in the vocabulary of the tokenizer. In addition,\nByte-level BPE maximizes vocabulary sharing between languages (Wang et al., 2020).\nNormalization\nUpstream of the BPE tokenization algorithm, no normalization of the\ntext was performed in order to have the most general model possible.\nIn all cases, we\nobserved that adding unicode normalization such as NFKC did not reduce the fertility by\nmore than 0.8% on all the languages considered but came at the cost of making the model\nless general; for example, causing 22 and 22 to be encoded in the same way.\nPre-tokenizer\nOur pre-tokenization has two goals: producing a first division of the text\n(usually using whitespaces and punctuation) and restricting the maximum length of se-\nquences of tokens produced by the BPE algorithm. The pre-tokenization rule used was the\nfollowing regex: “\n” 18 which splits words apart while preserving all\nthe characters and in particular the sequences of spaces and line breaks that are crucial for\nprogramming languages. We do not use English-centric splits common in other tokenizers\n(e.g. splitting around ’nt or ’ll). We also didn’t use splits on numbers and digits, which\ncaused issues in Arabic and code.\n3.4 Engineering\n3.4.1 Hardware\nThe model was trained on Jean Zay,19 a French government-funded supercomputer owned\nby GENCI and operated at IDRIS, the national computing center for the French National\nCenter for Scientific Research (CNRS). Training BLOOM took about 3.5 months to com-\nplete and consumed 1,082,990 compute hours. Training was conducted on 48 nodes, each\nhaving 8 NVIDIA A100 80GB GPUs (a total of 384 GPUs); due to possible hardware\nfailures during training, we also maintained a reserve of 4 spare nodes. The nodes were\nequipped with 2x AMD EPYC 7543 32-Core CPUs and 512 GB of RAM, while the storage\nwas handled by mix of full flash and hard disk drives using a SpectrumScale (GPFS) parallel\nfile system shared between all nodes and users of the supercomputer. 4 NVLink GPU-to-\nGPU interconnects per node enabled intra-node communications while 4 Omni-Path 100\nGbps links per node, arranged in an enhanced hypercube 8D global topology, were used for\ninter-node communications.\n18. github.com/bigscience-workshop/bs-tokenizers\n19. idris.fr/eng/jean-zay/jean-zay-presentation-eng.html\n18\n\nBLOOM\n3.4.2 Framework\nBLOOM was trained using Megatron-DeepSpeed20 (Smith et al., 2022), a framework for\nlarge-scale distributed training. It consists of two parts: Megatron-LM21 (Shoeybi et al.,\n2019) provides the Transformer implementation, tensor parallelism, and data loading prim-\nitives, whereas DeepSpeed22 (Rasley et al., 2020) provides the ZeRO optimizer, model\npipelining, and general distributed training components. This framework allows us to train\nefficiently with 3D parallelism (Narayanan et al., 2021, shown in Figure 6), a fusion of three\ncomplementary approaches to distributed training. These approaches are described below:\nFigure 6: DP+PP+TP combination leads to 3D parallelism.\nData parallelism (DP) replicates the model multiple times, with each replica placed on\na different device and fed a slice of the data. The processing is done in parallel and\nall model replicas are synchronized at the end of each training step.\nTensor parallelism (TP) partitions individual layers of the model across multiple de-\nvices. This way, instead of having the whole activation or gradient tensor reside on\na single GPU, we place shards of this tensor on separate GPUs. This technique is\nsometimes called horizontal parallelism or intra-layer model parallelism.\nPipeline parallelism (PP) splits up the model’s layers across multiple GPUs, so that\nonly a fraction of the layers of the model are placed on each GPU. This is sometimes\ncalled vertical parallelism.\nFinally, the Zero Redundancy Optimizer (ZeRO; Rajbhandari et al., 2020) allows different\nprocesses to only hold a fraction of data (parameters, gradients, and optimizer states)\n20. github.com/bigscience-workshop/Megatron-DeepSpeed\n21. github.com/NVIDIA/Megatron-LM\n22. github.com/microsoft/DeepSpeed\n19\n\nBigScience Workshop\nrequired for a training step. We used ZeRO stage 1, meaning that only the optimizer states\nare sharded in this manner.\nThe four components described above are combined together to allow scaling to hundreds\nof GPUs with extremely high GPU utilization. We were able to achieve 156 TFLOPs in\nour fastest configuration with A100 GPUs, attaining our objective of half of the theoretical\npeak performance of 312 TFLOPs (in float32 or bfloat16).\n3.4.3 Floating Point Format\nIn earlier experiments with 104B-parameter models on NVIDIA V100 GPUs, we observed\nnumerical instabilities that caused irreversible training divergences. We hypothesize that\nthese instabilities stem from our initial use of IEEE float16 — a 16-bit floating point\nformat with a very limited dynamic range that can cause overflows. The NVIDIA A100\nGPUs that we ultimately had access to support the bfloat16 format (Wang and Kanwar,\n2019; Kalamkar et al., 2019), which has the same dynamic range as float32. On the other\nhand, bfloat16 still has much lower precision, which motivated our use of mixed-precision\ntraining (Micikevicius et al., 2018).\nThis technique performs certain precision-sensitive\noperations such as gradient accumulation and softmax in float32 precision and the rest\nof operations in lower precision, allowing us to achieve a balance of high performance and\ntraining stability. Ultimately, we performed final training in bfloat16 mixed precision,\nwhich proved to solve the instability problem (in line with previous observation by Smith\net al., 2022).\n3.4.4 Fused CUDA Kernels\nIn general, GPUs cannot retrieve data to perform computations on and perform these\ncomputations at the same time. Moreover, the compute performance of modern GPUs is\nmuch higher than the speed of memory transfer required for every operation (often called a\nkernel in GPU programming). Kernel fusion (Wu et al., 2012) is an approach for optimizing\nGPU-based computations by performing several consecutive operations in only one kernel\ncall. This approach offers a way to minimize data transfers: intermediary results stay in\nthe GPU register instead of being copied into VRAM, saving overhead.\nWe used several custom fused CUDA kernels provided by Megatron-LM. First, we used\nan optimized kernel to perform LayerNorm, as well as kernels to fuse various combinations of\nthe scaling, masking, and softmax operations. The addition of a bias term is also fused with\nthe GeLU activation using the JIT functionality of PyTorch. As an example consequence\nof the use of fused kernels, adding the bias term in the GeLU operation adds no additional\ntime, as the operation is memory-bound: the additional computation is negligible compared\nto data transfers between GPU VRAM and registers, so fusing both operations essentially\nhalves their runtime.\n3.4.5 Additional Challenges\nScaling to 384 GPUs required two final changes: disabling asynchronous CUDA kernel\nlaunches (for ease of debugging and to prevent deadlocks) and splitting parameter groups\ninto smaller subgroups (to avoid excessive CPU memory allocations).\n20\n\nBLOOM\nDuring training, we faced issues with hardware failures: on average, 1–2 GPU failures\noccurred each week. As backup nodes were available and automatically used, and check-\npoints were saved every three hours, this did not affect training throughput significantly.\nA PyTorch deadlock bug in the data loader and disk space issues led to 5–10h downtimes.\nGiven the relative sparsity of engineering issues, and since there was only one loss spike,\nwhich the model swiftly recovered from, human intervention was less necessary than in com-\nparable projects (Zhang et al., 2022). Full details of our experience with training BLOOM\nand a detailed report of all issues we faced are publicly available.23\n3.5 Training\nHyperparameter (↓)\nBLOOM-560M\nBLOOM-1.1B\nBLOOM-1.7B\nBLOOM-3B\nBLOOM-7.1B\nBLOOM\nArchitecture hyperparameters\nParameters\n559M\n1,065M\n1,722M\n3,003M\n7,069M\n176,247M\nPrecision\nfloat16\nbfloat16\nLayers\n24\n24\n24\n30\n30\n70\nHidden dim.\n1024\n1536\n2048\n2560\n4096\n14336\nAttention heads\n16\n16\n16\n32\n32\n112\nVocab size\n250,680\n250,680\nSequence length\n2048\n2048\nActivation\nGELU\nGELU\nPosition emb.\nAlibi\nAlibi\nTied emb.\nTrue\nTrue\nPretraining hyperparameters\nGlobal Batch Size\n256\n256\n512\n512\n512\n2048\nLearning rate\n3.0e-4\n2.5e-4\n2e-4\n1.6e-4\n1.2e-4\n6e-5\nTotal tokens\n341B\n366B\nWarmup tokens\n375M\n375M\nDecay tokens\n410B\n410B\nDecay style\ncosine\ncosine\nMin. learning rate\n1e-5\n6e-6\nAdam (β1, β2)\n(0.9, 0.95)\n(0.9, 0.95)\nWeight decay\n1e-1\n1e-1\nGradient clipping\n1.0\n1.0\nMultitask finetuning hyperparameters\nGlobal Batch Size\n1024\n1024\n2048\n2048\n2048\n2048\nLearning rate\n2.0e-5\n2.0e-5\n2.0e-5\n2.0e-5\n2.0e-5\n2.0e-5\nTotal tokens\n13B\n13B\nWarmup tokens\n0\n0\nDecay style\nconstant\nconstant\nWeight decay\n1e-4\n1e-4\nTable 3: BLOOM & BLOOMZ Training Hyperparameters.\n23. github.com/bigscience-workshop/bigscience/blob/master/train/tr11-176B-ml/chronicles.md\n21\n\nBigScience Workshop\nPretrained Models\nWe train six size variants of BLOOM with respective hyperparam-\neters detailed in Table 3. Architecture and training hyperparameters come from our ex-\nperimental results (Le Scao et al., 2022) and prior work on training large language models\n(Brown et al., 2020; Kaplan et al., 2020). Model depth and width for the non-176B models\nroughly follow previous literature (Brown et al., 2020; Zhang et al., 2022), deviating for\n3B and 7.1B in order only to fit the models more easily on our training setup. Embed-\nding parameter sizes are larger for BLOOM owing to the larger multilingual vocabulary,\nbut scaling literature discounts embedding operations (Kaplan et al., 2020). During the\ndevelopment process at the 104B parameters scale, we experimented with different values\nof Adam β parameters, weight decay and gradient clipping to target stability, but did not\nfind it helpful. For all models, we use a cosine learning rate decay schedule (Loshchilov\nand Hutter, 2016) over 410B tokens, taken as an upper bound for the length of training if\ncompute permitted, and warmup for 375M tokens. We use weight decay, gradient clipping,\nand no dropout. The ROOTS dataset contains around 341 billion tokens of text, so we\naimed to train all models for the equivalent amount of tokens. However, in light of revised\nscaling laws published during training (Hoffmann et al., 2022), we decided to train the large\nmodels for an additional 25 billion tokens on repeated data. As warmup tokens + decay\ntokens were larger than the total number of tokens, the end of learning rate decay was never\nreached.\nMultitask Finetuning\nFinetuned BLOOMZ models (Muennighoff et al., 2022b) main-\ntain the same architecture hyperparameters as BLOOM models. The finetuning hyperpa-\nrameters are loosely based on T0 (Sanh et al., 2022) and FLAN (Wei et al., 2021). Learning\nrates are determined by doubling the minimum learning rate of the respective pretrained\nmodel and then rounding. Global batch sizes are multiplied by four for small variants to\nincrease throughput. While the models are finetuned for 13 billion tokens, the best check-\npoint is chosen according to a separate validation set. We found performance to plateau\nafter 1 – 6 billion tokens of finetuning.\nContrastive Finetuning\nWe also perform contrastive finetuning of the 1.3 and 7.1 billion\nparameter BLOOM models using the SGPT Bi-Encoder recipe (Muennighoff, 2022) to\ntrain models that produce high-quality text embeddings. We created SGPT-BLOOM-7.1B-\nmsmarco24 geared towards multilingual information retrieval and SGPT-BLOOM-1.7B-nli25\nfor multilingual semantic textual similarity (STS). However, recent benchmarking has found\nthese models to also generalize to various other embedding tasks, such as bitext mining,\nreranking or feature extraction for downstream classification (Muennighoff et al., 2022a).\n3.5.1 Carbon Footprint\nWhile most attempts to estimate the carbon footprint of language models have shed light\non the emissions produced due to energy consumed during model training (e.g. Patterson\net al., 2021; Strubell et al., 2019), other sources of emissions are also important to consider.\nIn our efforts to estimate the carbon emissions of BLOOM, we were inspired by the Life\nCycle Assessment (LCA) approach (Kl¨opffer, 1997) and aimed to consider aspects such as\n24. hf.co/bigscience/sgpt-bloom-7b1-msmarco\n25. hf.co/bigscience-data/sgpt-bloom-1b7-nli\n22\n\nBLOOM\nthe emissions of equipment manufacturing, intermediate model training, and deployment.\nAccording to our estimates, the carbon emissions from BLOOM training add up to approx-\nimately 81 tons of CO2eq, of which 14% were generated by the equipment manufacturing\nprocess (11 tons), 30% by the energy consumed during training (25 tons) and 55% by idle\nconsumption of the equipment and computing cluster used for training (45 tons).\nModel\nname\nNumber of\nparameters\nPower\nconsumption\nCO2eq\nemissions\nGPT-3\n175B\n1,287 MWh\n502 tons\nGopher\n280B\n1,066 MWh\n352 tons\nOPT\n175B\n324 MWh\n70 tons\nBLOOM\n176B\n433 MWh\n25 tons\nTable 4: Comparison of carbon emissions between BLOOM and similar LLMs. Numbers in\nitalics have been inferred based on data provided in the papers describing the models.\nComparing the carbon emissions of BLOOM training to other similar models (see\nTable 4), reveals that while the energy consumption of BLOOM is slightly higher than\nOPT (Zhang et al., 2022) (433 Mwh compared to OPT’s 324 MWh), its emissions are ap-\nproximately 2/3 less (25 tons versus 70 tons). This is thanks to the low carbon intensity\nof the energy grid used for training BLOOM, which emits 57 gCO2eq/kWh, compared to\n231 gCO2eq/kWh for the grid used for OPT training. Specifically, France’s national energy\ngrid (which is used by Jean Zay) is largely powered by nuclear energy, which is low-carbon\ncompared to grids powered by energy sources such as coal and natural gas.\nWhile the\nsustainability of nuclear energy is debated, it is one of the least carbon-intensive sources\nof energy that is currently available. Both BLOOM and OPT incurred significantly less\ncarbon emissions than GPT-3 (as reported by (Patterson et al., 2021)), which can be at-\ntributed to several factors including more efficient hardware as well as less carbon-intensive\nenergy sources.\nWe also pursued further exploration of the carbon footprint of (1) the computation\ncarried out on Jean Zay within the scope of the Big Science workshop, and (2) running\nthe BLOOM model API in real time.\nIn terms of the footprint of the totality of the\ncomputation, we estimate that the final BLOOM training represents approximately 37% of\nthe overall emissions, with other processes such as intermediate training runs and model\nevaluation adding up to the other 63%. This is slightly less than the estimate made by\nthe authors of the OPT paper, who stated that the total carbon footprint of their model is\nroughly 2 times higher due to experimentation, baselines and ablation (Zhang et al., 2022).\nOur ongoing exploration of the carbon emissions of the BLOOM API have estimated that\nthe real-time deployment of the model on a GCP instance with 16 GPUs running in the\nus-central1 region results in approximately 20 kg of CO2eq emitted per day of deployment\n(or 0.83 kg per hour). This figure is not representative of all deployment use-cases, and\nwill vary depending on the hardware used as well as the specifics of model implementation\n(e.g. whether batching is used) and the number of requests the model receives. Further\ninformation regarding BLOOM’s carbon footprint can be found in Luccioni et al. (2022).\n23\n\nBigScience Workshop\n3.6 Release\nOpenness has been central to the development of BLOOM and we wanted to ensure it is\neasily available for the community to use. As such, we worked on producing documentation\nas a Model Card (Mitchell et al., 2019) and a new license addressing specific goals of the\nproject.\nModel Card\nFollowing best practices for releasing machine learning models, the BLOOM\nmodel has been released along with a detailed Model Card26 (Mitchell et al., 2019) describing\nits technical specifications, details on training, intended-use, out-of-scope uses as well as the\nmodel’s limitations. Participants across working groups worked together to produce the final\nModel Card and similar cards for each checkpoint. The work was collaborative, primarily\ncomposed “live” by thinking through and discussing each section, then further dividing into\nsubsections based on the categorizations and distinctions participants naturally ended up\ncreating throughout discussions.\nLicensing\nBeing mindful of the potentially harmful use-cases that BLOOM could en-\nable, we chose to strike a balance between unrestricted open-access and responsible-use by\nincluding behavioral-use clauses (Contractor et al., 2022) to limit the application of the\nmodel towards potentially harmful use-cases. Such clauses are routinely being included in a\ngrowing class of “Responsible AI Licenses (RAIL)”27 that the community has been adopting\nwhen releasing their models.28 A distinguishing aspect of the RAIL license developed for\nBLOOM is that it separates licensing of the “source code” and “model”, as referenced by its\ntrained parameters. It further includes detailed definitions of “use” and “derived works” of\nthe model to ensure that anticipated downstream use by prompting, finetuning, distillation,\nuse of logits and probability distributions are explicitly identified. The license contains 13\nbehavioral-use restrictions that have been identified based on the intended uses and lim-\nitations described in the BLOOM Model Card, as well as the BigScience ethical charter.\nThe license offers the model at no charge and users are free to use the model as long as\nthey comply with the terms (including usage restrictions). The source code for BLOOM\nhas been made available under an Apache 2.0 open source license.\n4. Evaluation\nOur evaluations focus on zero-shot and few-shot settings. Our goal is to present an accurate\npicture of how BLOOM compares to existing LLMs in settings that most realistically reflect\nthe way the models are likely to be used in practice. Because of the scale of these models,\nprompt-based adaptation and few-shot “in-context learning” are currently more common\nthan finetuning. Thus, we report results on a range of tasks - SuperGLUE 4.2, machine\ntranslation 4.3, summarization 4.4 - and languages in zero-shot and one-shot prompt-based\nsettings, as well as after multitask finetuning (Section 4.7). We also perform code gener-\nation 4.5, use BLOOM-derived text embeddings for representation tasks 4.8 and interpret\nBLOOM’s generalization abilities from the perspective of multilingual probing (Section 4.9).\n26. hf.co/bigscience/bloom\n27. licenses.ai\n28. the-turing-way.netlify.app/reproducible-research/licensing/licensing-ml.html\n24\n\nBLOOM\n4.1 Experimental Design\n4.1.1 Prompts\nBased on recent research on the impact of prompting on language model performance, we\ndecided to build a language model evaluation suite that allowed us to vary both the basic\ntask data as well as the prompting that is used to contextualize the task. Our prompts\nwere developed prior to BLOOM’s release, and did not undergo any a priori refinement\nusing models. That is, the prompts we use in our evaluation are ones that humans believed\nwere a reasonable way to solicit the desired task behavior from a language model. Our\ngoal for designing prompts in this way is to simulate realistic zero-shot or one-shot results\nthat a new user could expect from BLOOM. This is in contrast to presenting best-case\nperformances that might result from multiple rounds of trial-and-error on prompt design.\nWe choose to report the former because the latter is harder to reproduce systematically, is\narguably a less representative picture of how the model works in the average setting, and\nis not representative of true zero-shot learning where no labeled data is available.\nWe generate multiple prompts per task using promptsource (Bach et al., 2022). We\nfollow the procedure used by Sanh et al. (2022), in which prompt generation is crowd-\nsourced, and thus we see substantial variety in length and style across prompts. To improve\nquality and clarity, multiple peer reviews were performed on each prompt for artifacts and\nconsistency.\nTable 5 shows examples of the resulting prompts used for the WMT’14 task. We also\ngenerate prompts for many tasks that are not included in this paper due to resource con-\nstraints. All of our prompts for all tasks (both those analyzed in the paper and those not\nyet analyzed) are publicly available.29\nPrompt name\nPrompt\nTarget\na_good_translation-source+target\nGiven the following source text: [source sentence], a good L2 translation is:\n[target sentence]\ngpt3-target\nWhat is the L2 translation of the sentence: [source sentence]?\n[target sentence]\nversion-target\nif the original version says [source sentence]; then the L2 version should say:\n[target sentence]\nxglm-source+target\nL1: [source sentence] = L2:\n[target sentence]\nTable 5: Four prompts for the WMT’14 dataset (Bojar et al., 2014) for MT evaluation.\nAbove, “L1” and “L2” are replaced with language names (e.g. “Bengali” and “Russian”).\n4.1.2 Infrastructure\nOur framework extends EleutherAI’s Language Model Evaluation Harness (Gao et al.,\n2021) by integrating it with the promptsource (Bach et al., 2022) library described in\nSection 3.1.4. We release our Prompted Language Model Evaluation Harness as an open\nsource library for people to use. We use this framework in order to run the experiments\nand aggregate results.\n29. github.com/bigscience-workshop/promptsource/tree/eval-hackathon\n25\n\nBigScience Workshop\n4.1.3 Datasets\nSuperGLUE\nWe use a subset of the SuperGLUE (Wang et al., 2019) evaluation suite of\nclassification tasks, specifically: Ax-b, Ax-g, BoolQ, CB, WiC, WSC, and RTE tasks. We\nexcluded the remaining tasks because they require an order of magntiude more compute\nto run than all of these tasks we consider combined. These tasks are English-only, and\nare thus included to facilitate comparison with prior work, which has primarily focused on\nEnglish-only models. We also note that performance on these tasks has not yet been widely\nreported using zero- and one-shot prompt-based setting. T0 (Sanh et al., 2022) is the first\nexception, but that model is instruction-tuned and thus not directly comparable to models\nlike BLOOM and OPT. For each task, we select a random sample of five prompts from\npromptsource and evaluate all models on that set of prompts. As with other prompting\ntasks in Evaluation Harness (Gao et al., 2021), the prediction of a model for a given prompt\nis measured using the maximum log likelihood among a set of specified candidate label\nstrings associated with the prompt.\nMachine Translation (MT)\nWe evaluate BLOOM on three datasets (using ISO-639-1\ncodes to refer to languages): WMT14 en↔fr and en↔hi (Bojar et al., 2014), Flores-101\n(Goyal et al., 2022) and DiaBLa (Bawden et al., 2020). We evaluate using the sacrebleu\n(Post, 2018) implementation of BLEU (Papineni et al., 2002), using default tokenisation\nfor WMT and DiaBLa and spm-flores-101 for Flores.30 We use greedy decoding with\ngeneration proceeding until the EOS token, or additionally \\n###\\n for the 1-shot case.\nThe maximum generation length was set per dataset to be in line with what is typically\nused in the literature; specifically, 64 tokens for WMT14 and 512 tokens for Flores-101 and\nDiaBla. Task-specific experimental design details are below.\nSummarization\nWe evaluate summarization on the WikiLingua (Ladhak et al., 2020)\ndataset. WikiLingua is a multilingual summarization dataset comprising WikiHow article\nand step-by-step summary pairs. Pairs are aligned across multiple languages, with transla-\ntion of source and summary further reviewed by an international translation team. One-shot\nconditional natural language generation has typically not been reported by models with size\ncomparable to BLOOM. PaLM (Chowdhery et al., 2022) is the first exception, and reports\nscores on WikiLingua; however, only the model’s ability to summarize in English was ex-\namined (-> en). By contrast, we opted to test BLOOM’s inherent multilingual ability by\nassessing the abstractive summarization in the source language (e.g. vi -> vi). We focus\non the nine languages (Arabic, English, Spanish, French, Hindi, Indonesian, Portuguese,\nVietnamese and Chinese) which were amongst those targeted as part of the BigScience\neffort.\nNatural language generation is notoriously challenging to evaluate, with multilingual\ngeneration compounding this challenge due to a lack of metric support. Following the sug-\ngestions by Gehrmann et al. (2022b), we report ROUGE-2, ROUGE-L (Lin, 2004),31 and\nLevenshtein distance. One important modification to ROUGE is using the SentencePiece\ntokenizer (Kudo and Richardson, 2018) built from the Flores-101 dataset (Goyal et al.,\n30. BLEU+case:mixed+numrefs.1+smooth.exp+{13a,tok:spm-flores}+version:2.2.1\n31. For ROUGE, we used the Python implementation at\ngithub.com/google-research/google-research/rouge, commit f935042.\n26\n\nBLOOM\n2022). A naive approach would use a tokenizer based on English, but using a multilingual\ntokenizer improves the capacity to measure the fidelity of multilingual generations. To min-\nimize inference time of the model we use the subsamples from the updated GEM benchmark\n(Gehrmann et al., 2022a) (3000 uniformly sampled test examples). The authors note that\nthere is minimal difference when comparing model performance between the subsamples\nand the full test sets. For decoding and generation, we use the same procedure as described\nabove for MT.\n4.1.4 Baseline Models\nWe use the following baseline models where appropriate (e.g. in settings where they support\nthe language of the evaluation dataset):\n• mGPT (Shliazhko et al., 2022), GPT-style models trained on 60 languages from\nWikipedia and Common Crawl\n• GPT-Neo (Black et al.), GPT-J-6B (Wang and Komatsuzaki, 2021), and GPT-NeoX\n(Black et al., 2022), a family of GPT-style models trained on The Pile (Gao et al.,\n2020)\n• T0 (Sanh et al., 2022), a variant of T5 (Raffel et al., 2020) that underwent multitask\nprompted finetuning on datasets from P3 (Bach et al., 2022)\n• OPT (Zhang et al., 2022), a family of GPT-style model trained on a mixture of\ndatasets including those from RoBERTa Liu et al. (2019) and The Pile (Gao et al.,\n2020)\n• XGLM (Lin et al., 2021), a GPT-style multilingual model trained on a variant of\nCC100 (Conneau et al., 2020)\n• M2M (Fan et al., 2021), a massively multilingual model trained to translate between\n100 languages\n• AlexaTM (Soltan et al., 2022), an encoder-decoder model trained on a mixture of\nmasked and causal language modeling on data from Wikipedia and mC4 (Xue et al.,\n2021)\n• mTk-Instruct (Wang et al., 2022b), a variant of T5 that underwent multitask prompted\nfinetuning on datasets from Super-NaturalInstructions\n• Codex (Chen et al., 2021), a family of GPT models finetuned on code from GitHub\n• GPT-fr (Simoulin and Crabbé, 2021), a GPT-style model trained on French text\n4.2 SuperGLUE\nFigure 7 shows zero- and one-shot performance on SuperGLUE. In both settings, on en-\ntailment tasks (BoolQ and CB), performance is well above random chance for BLOOM,\nT0, OPT, and GPT-J. On other tasks, while the best prompts do better, the average per-\nformance across prompts hovers around chance, suggesting that the success of individual\n27\n\nBigScience Workshop\nprompts is primarily statistical variation. There is some signal for BLOOM in the diagnostic\n(Ax-b and Ax-g) datasets. The exception is the T0 model, which shows strong performance.\nHowever, this model is finetuned in the multitask setting (similar to BLOOMZ, see Sec-\ntion 4.7) in order to improve performance in zero-shot prompting settings, and thus is not\ndirectly comparable to the other models shown here.\nAs models go from zero-shot to one-shot, variability is reduced across all prompts and\nmodels and performance slightly and inconsistently increases. Notably, BLOOM sees more\nof an increase in performance than comparable models when going from zero-shot to one-\nshot, as it is generally behind OPT in the zero-shot setting but matches or improves on it\nin the one-shot setting, even though it has only partly been trained on English. This may\nbe because a multilingual language model gains more certainty in the language of input and\noutput with a longer context.\n0\n20\n40\n60\n80\n100\n0\n20\n40\n60\n80\n100\n0\n20\n40\n60\n80\n100\n0\n20\n40\n60\n80\n100\n0\n20\n40\n60\n80\n100\n0\n20\n40\n60\n80\n100\n0\n20\n40\n60\n80\n100\n0\n20\n40\n60\n80\n100\n0\n20\n40\n60\n80\n100\n0\n20\n40\n60\n80\n100\n0\n20\n40\n60\n80\n100\n0\n20\n40\n60\n80\n100\nmGPT (1.3B)\nGPT-J (6B)\nT0 (11B)\nOPT (175B)\nBLOOM (176B)\nAx-b\nAx-g\nBoolQ\nCB\nWiC\nWSC\nAx-b\nAx-g\nBoolQ\nCB\nWiC\nWSC\nSuperGLUE 0-shot\nSuperGLUE 1-shot\nFigure 7: Performance of various LLMs on subset of tasks from SuperGLUE benchmark in\nzero- and one-shot prompt-based setting.\nWe perform an additional analysis comparing BLOOM models across model sizes. As\na baseline, we also measure the average one-shot accuracy of OPT models of similar sizes\n(350M parameters to 175B parameters).32 Figure 8 shows the accuracy of each prompt\non each task across model scales. Both OPT and BLOOM model families improve very\nslightly with scale, with only models over 2 billion parameters showing signal, and there is\nno consistent difference between families across all tasks. In the 1-shot setting, BLOOM-\n176B is ahead of OPT-175B on Ax-b, CB, WSC and WiC, and matches it on the other tasks,\nsuggesting that multilinguality does not limit performance of BLOOM on English-only tasks\nin the zero-shot setting.\n32. We do not evaluate OPT-66B because of the lack of a similarly-sized BLOOM model.\n28\n\nBLOOM\n2\n5\n1B\n2\n5 10B\n2\n5 100B 2\n0\n20\n40\n60\n80\n100\n2\n5\n1B\n2\n5 10B\n2\n5 100B 2\n0\n20\n40\n60\n80\n100\n2\n5\n1B\n2\n5 10B\n2\n5 100B 2\n0\n20\n40\n60\n80\n100\n2\n5\n1B\n2\n5 10B\n2\n5 100B 2\n0\n20\n40\n60\n80\n100\n2\n5\n1B\n2\n5 10B\n2\n5 100B 2\n0\n20\n40\n60\n80\n100\n2\n5\n1B\n2\n5 10B\n2\n5 100B 2\n0\n20\n40\n60\n80\n100\nOPT\nBLOOM\nAx-b\nAx-g\nBoolQ\nCB\nWiC\nWSC\nSuperGLUE 1-shot\nFigure 8: Comparison of the scaling of BLOOM versus OPT on each SuperGLUE one-shot\ntask. Each point represents the average accuracy of a model within the BLOOM or OPT\nfamily of models on one of the five task prompts. The number of parameters on the x-axis\nis presented in log-scale.\n4.3 Machine Translation\nIn addition to the results presented here, a more detailed analysis of BLOOM’s MT quality\ncan be found in (Bawden and Yvon, 2023).\n4.3.1 WMT\nWMT results for BLOOM-176B in the zero-shot and 1-shot setting are given in Table 6. The\nbest prompts tend to be the more verbose ones; the “version-target” prompt is consistently\nbetter and the “gpt3-target” and “xglm-source+target” prompts have very poor performance,\nespecially for zero-shot.\nIn the one-shot setting, BLOOM can, with the right prompt,\nperform competent translation, although it is behind dedicated (supervised) models such\nas M2M-100 (43.8 BLEU for English→French and 40.4 for French→English, compared to\n34.2 and 35.4 BLEU for BLOOM). The two major problems observed, particularly in the\nzero-shot setting, are (i) over-generation and (ii) not producing the correct language (an\n29\n\nBigScience Workshop\nobvious prerequisite for a good translation). Both of these aspects are greatly improved as\nthe number of few-shot examples is increased.\nPrompt\nen →fr\nfr →en\nen →hi\nhi →en\nShots\n0\n1\n0\n1\n0\n1\n0\n1\na_good_translation-source+target 15.38 36.39 14.15 36.56 1.90 14.49 10.19 24.60\ngpt3-target\n7.90 32.55 12.73 33.14 0.26\n6.51\n0.66\n9.98\nversion-target\n21.96 34.22 26.79 35.42 1.96 13.95 11.48 25.80\nxglm-source+target\n14.91 27.83 15.52 34.51 6.80 13.62 12.05 25.04\nTable 6: WMT’14 zero- and one-shot results (BLEU) for BLOOM-176B. The prompts used\nare described in Table 5.\n4.3.2 DiaBLa\nen→fr\nfr→en\n1-shot context\nTruncate\nBLEU\nCOMET\nBLEU\nCOMET\nRand.\n×\n5.7\n0.342\n12.1\n0.614\n✓\n37.6\n0.634\n41.4\n0.757\nPrev.\n×\n6.1\n0.328\n12.3\n0.617\n✓\n38.5\n0.614\n41.6\n0.751\nTable 7: DiaBLa 1-shot results (BLEU) for the “xglm-source+target” prompt when using\nthe previous or a random sentence as the 1-shot example (with and without truncation of\noutputs). In bold the best results for each direction.\nTable 7 shows results testing the use of linguistic context with DiaBLa, a parallel dataset\nof informal bilingual dialogues. In a 1-shot context and using the “xglm-source+target”\nprompt, we compare the effect of using a random test set example as the 1-shot example\nversus using the previous dialogue utterance.\nIn light of the overgeneration issues seen\nand in order to evaluate the quality of the prediction independently of overgeneration, we\nreport results for both original outputs and after applying a custom truncation function.33\nThe automatic results are inconclusive, with little difference between scores (BLEU scores\nare higher for previous context but COMET scores are lower). Despite these results, there\nis evidence in the predictions themselves that the model is able to use the context of the\n1-shot example to make translation choices. See (Bawden and Yvon, 2023) for examples\nand further analysis.\n30\n\nBLOOM\nSrc↓\nTrg→\nen\nbn\nhi\nsw\nyo\nen\nBLOOM\n–\n24.6\n27.2\n20.5\n2.6\nM2M\n–\n23.0\n28.1\n26.9\n2.2\nbn\nBLOOM\n29.9\n–\n16.3\n–\n–\nM2M\n22.9\n–\n21.8\n–\n–\nhi\nBLOOM\n35.1\n23.8\n–\n–\n–\nM2M\n27.9\n21.8\n–\n–\n–\nsw\nBLOOM\n37.4\n–\n–\n–\n1.3\nM2M\n30.4\n–\n–\n–\n1.3\nyo\nBLOOM\n4.1\n–\n–\n0.9\n–\nM2M\n4.2\n–\n–\n1.9\n–\n(a) Low-resource languages\nSrc↓\nTrg→\nca\nes\nfr\ngl\nit\npt\nca\nBLOOM\n–\n28.9\n33.8\n19.2\n19.8\n33.0\nM2M\n–\n25.2\n35.1\n33.4\n25.5\n35.2\nes\nBLOOM\n31.2\n–\n24.8\n23.3\n16.5\n29.1\nM2M\n23.1\n–\n29.3\n27.5\n23.9\n28.1\nfr\nBLOOM\n37.2\n27.5\n–\n24.9\n24.0\n38.9\nM2M\n28.7\n25.6\n–\n32.8\n28.6\n37.8\ngl\nBLOOM\n37.5\n27.1\n33.8\n–\n18.3\n32.2\nM2M\n30.1\n27.6\n37.1\n–\n26.9\n34.8\nit\nBLOOM\n31.0\n25.4\n31.4\n20.2\n–\n29.2\nM2M\n25.2\n29.2\n34.4\n29.2\n–\n31.5\npt\nBLOOM\n39.6\n28.1\n40.3\n27.1\n20.1\n–\nM2M\n30.7\n26.9\n40.2\n33.8\n28.1\n–\n(b) Romance languages\nSrc ↓\nTrg →\nar\nen\nes\nfr\nzh\nar\nBLOOM\n–\n40.3\n23.3\n33.1\n17.7\nM2M\n–\n25.5\n16.7\n25.7\n13.1\nAlexaTM\n–\n41.8\n23.2\n35.5\n–\nen\nBLOOM\n28.2\n–\n29.4\n45.0\n26.7\nM2M\n17.9\n–\n25.6\n42.0\n19.3\nAlexaTM\n32.0\n–\n31.0\n50.7\n–\nes\nBLOOM\n18.8\n32.7\n–\n24.8\n20.9\nM2M\n12.1\n25.1\n–\n29.3\n14.9\nAlexaTM\n20.8\n34.6\n–\n33.4\n–\nfr\nBLOOM\n23.4\n45.6\n27.5\n–\n23.2\nM2M\n15.4\n37.2\n25.6\n–\n17.6\nAlexaTM\n24.7\n47.1\n26.3\n–\n–\nzh\nBLOOM\n15.0\n30.5\n20.5\n26.0\n–\nM2M\n11.55\n20.9\n16.9\n24.3\n–\nAlexaTM\n–\n–\n–\n–\n–\n(c) High-resource language pairs.\nSrc ↓\nTrg →\nen\nfr\nhi\nid\nvi\nen\nBLOOM\n–\n45.0\n27.2\n39.0\n28.5\nM2M\n–\n42.0\n28.1\n37.3\n35.1\nfr\nBLOOM\n45.6\n–\n18.5\n31.4\n32.8\nM2M\n37.2\n–\n22.9\n29.1\n30.3\nhi\nBLOOM\n35.1\n27.6\n–\n–\n–\nM2M\n27.9\n25.9\n–\n–\n–\nid\nBLOOM\n43.2\n30.4\n–\n–\n–\nM2M\n33.7\n30.8\n–\n–\n–\nvi\nBLOOM\n38.7\n26.8\n–\n–\n–\nM2M\n29.5\n25.8\n–\n–\n–\n(d) High→mid-resource language pairs.\nTable 8: 1-shot MT results (spBLEU) on the Flores-101 devtest set.\n4.3.3 Flores\nIn the 1-shot setting, we test several language directions in the Flores-101 (Goyal et al., 2022)\ndevtest set using the “xglm-source+target” prompt (Lin et al., 2021). The 1-shot example\nis randomly taken from the dev set.\nWe separate out results for low-resource language\npairs (Table 8a), between related languages of the Romance language family (Table 8b),\nhigh-resource language pairs (Table 8c) and high-to-mid-resource language pairs (Table 8d).\n33. The truncation rule is specific to the “xglm-source+target” prompt and the fact that overgeneration\nconsists of repeating the prompt pattern. Anything after a first newline or the regular expression pattern\n= .+?: is discarded.\n31\n\nBigScience Workshop\nLanguages are classified as low-, mid- and high-resource depending on their representation in\nROOTS. We compare to supervised results from the M2M-100 model (Fan et al., 2021) with\n615M parameters, for which scores are computed by Goyal et al. (2022). Additionally, we\ncompare to 32-shot AlexaTM results for high-resource language pairs (Soltan et al., 2022).\nResults are good across the board for both translation between high-resource languages and\nfrom high- to mid-resource languages, suggesting BLOOM’s good multilingual capacity, even\nacross scripts (here between Latin (or extended Latin), Chinese, Arabic and Devanagari\nscripts). Compared to the supervised M2M-100 model, results are often comparable and\nsometimes better in this 1-shot setting, and results are comparable in many cases to those\nof AlexaTM (even though AlexTM results are for 32-shot).\nThe translation quality for many of the low-resource languages is good, comparable\nto or even slightly better than the supervised M2M model.\nHowever, results are very\npoor between Swahili and Yoruba, languages that are present but under-represented in\nBLOOM’s training data (<50k tokens each). This contrasts with the results for translation\nbetween Romance (and therefore related) languages, where results are good across-the-\nboard, including for translation from Galician (glg), a language not included in the training\ndata, but which shares many similarities with the other Romance languages, in particular\nwith Portuguese (por).\nThis however does question BLOOM’s quality on those under-\nrepresented low-resource languages included in training.\n4.4 Summarization\nFigure 9 shows one-shot results for BLOOM models alongside OPT-175B for comparison.\nEach point represents a per-prompt score. The key takeaways are that BLOOM attains\nhigher performance on multilingual summarization than OPT and that performance in-\ncreases as the parameter count of the model increases. We suspect this is due to BLOOM’s\nmultilingual-focused training.\nAs discussed in Section 4.1, we report ROUGE-2 scores for the sake of comparability with\nprior work, and because there is a lack of alternatives for generation evaluation. However,\nwe qualitatively observe that in many cases, the ROUGE-2 score understates the quality of\nthe summaries generated by the systems.\n4.5 Code Generation\nThe BLOOM pretraining corpus, ROOTS, consists of around 11% of code. In Table 9,\nwe report benchmarking results of BLOOM on HumanEval (Chen et al., 2021). We find\nthe performance of pretrained BLOOM models to be similar to that of the similar-sized\nGPT models trained on the Pile (Gao et al., 2020). The Pile contains English data and\naround 13% of code (GitHub + StackExchange), which is similar to the code data sources\nand proportions in ROOTS. The Codex models, which have solely been finetuned on code,\nare significantly stronger than other models. Multitask finetuned BLOOMZ models do not\nimprove significantly over BLOOM models. We hypothesize this is due to the finetuning\ndataset, xP3, not containing significant amounts of pure code completion. Rather, xP3\ncontains code-related tasks, such as estimating the time complexity of a given Python code\nsnippet. Additional analysis is provided in Muennighoff et al. (2022b).\n32\n\nBLOOM\n0\n5\n10\n0\n5\n10\n0\n5\n10\n0\n5\n10\n0\n5\n10\n0\n5\n10\n0\n5\n10\n0\n5\n10\n15\n0\n5\n10\nBLOOM-560M\nBLOOM-1.1B\nBLOOM-3B\nBLOOM-7.1B\nBLOOM\nOPT-175B\nvi → vi\nhi → hi\nfr → fr\nen → en\nes → es\nar → ar\npt → pt\nzh → zh\nid → id\nWikiLingua\nFigure 9: WikiLingua One-shot Results. Each plot represents a different language with\nper-prompt ROUGE-2 F-measure scores.\n4.6 HELM benchmark\nFor completeness, we reproduce here evaluations from the HELM benchmark (Liang et al.,\n2022), which ran 5-shot evaluations of a variety of language models on English-only tasks.\nDespite the multilingual training, BLOOM is roughly on par in accuracy with previous-\ngeneration English-only models, such as GPT3-davinci v1 and J1-Grande v1, but be-\nhind more recent monolingual models such as InstructGPT davinci v2, Turing NLG v2,\nAnthropic-LM v4-s3, or OPT. Like other large language models of this size, it is not very\nwell calibrated, but quite robust. Finally, on this benchmark, it is one of the best models\nfor fairness, slightly more toxic than average in English, and average for bias.\n4.7 Multitask Finetuning\nBuilding on recent work on multitask finetuning (Sanh et al., 2022; Wei et al., 2021; Wang\net al., 2022a) we explore using multilingual multitask finetuning to improve the zero-shot\nperformance of the BLOOM model.\nWe conducted multilingual multitask finetuning of\nBLOOM models using the xP3 corpus outlined in Section 3.1.4. We find that zero-shot\nperformance significantly increases. In Figure 11, we compare the zero-shot performance\nof pretrained BLOOM and XGLM models with multitask finetuned BLOOMZ, T0 and\nmTk-Instruct (Wang et al., 2022b). BLOOM and XGLM performances are near the ran-\ndom baselines of 33% for NLI (XNLI) and 50% for coreference resolution (XWinograd) and\n33\n\nBigScience Workshop\npass@k\nk = 1\nk = 10\nk = 100\nGPT-Neo 1.3B\n4.79%\n7.47%\n16.30%\nGPT-Neo 2.7B\n6.41%\n11.27%\n21.37%\nGPT-J 6B\n11.62%\n15.74%\n27.74%\nGPT-NeoX 20B\n15.4%\n25.6%\n41.2%\nCodex-300M\n13.17%\n20.37%\n36.27%\nCodex-679M\n16.22%\n25.7%\n40.95%\nCodex-2.5B\n21.36%\n35.42%\n59.5%\nCodex-12B\n28.81%\n46.81%\n72.31%\nBLOOM-560M\n0.82%\n3.02%\n5.91%\nBLOOM-1.1B\n2.48%\n5.93%\n9.62%\nBLOOM-1.7B\n4.03%\n7.45%\n12.75%\nBLOOM-3B\n6.48%\n11.35%\n20.43%\nBLOOM-7.1B\n7.73%\n17.38%\n29.47%\nBLOOM\n15.52%\n32.20%\n55.45%\nBLOOMZ-560M\n2.18 %\n4.11%\n9.00%\nBLOOMZ-1.1B\n2.63%\n6.22%\n11.68%\nBLOOMZ-1.7B\n4.38%\n8.73%\n16.09%\nBLOOMZ-3B\n6.29%\n11.94%\n19.06%\nBLOOMZ-7.1B\n8.06%\n15.03%\n27.49%\nBLOOMZ\n12.06%\n26.53%\n48.44%\nTable 9: Performance on HumanEval (Chen et al., 2021). Non-BLOOM results come from\nprior work (Chen et al., 2021; Fried et al., 2022). The Codex model is a language model\nthat was finetuned on code, while the GPT models (Black et al.; Wang and Komatsuzaki,\n2021; Black et al., 2022) are trained on a mix of code and text like BLOOM.\nsentence completion (XCOPA and XStoryCloze). After going through multilingual multi-\ntask finetuning (BLOOMZ), zero-shot performance significantly improves on the depicted\nheld-out tasks. Despite also being multitask finetuned, T0 performs badly on the multi-\nlingual datasets shown due to it being a monolingual English model. Additional results\nprovided in Muennighoff et al. (2022b), however, show that models finetuned on xP3 also\noutperform T0 on English datasets when controlling for size and architecture. This is likely\ndue to T0’s finetuning dataset (P3) containing less diverse datasets and prompts than xP3.\nMultitask finetuning performance has been shown to correlate with the amount of datasets\nand prompts (Chung et al., 2022).\n4.8 Embeddings\nIn Section 3.5, we have outlined the contrastive finetuning procedure for creating SGPT-\nBLOOM text embedding models.\nIn Table 10, we report benchmarking results on two\n34\n\nBLOOM\n0.0\n0.5\n1.0\nCohere small v20220720 (410M)\nInstructGPT ada v1 (350M*)\nGPT-3 ada v1 (350M)\nGPT-3 babbage v1 (1.3B)\nYaLM (100B)\nT5 (11B)\nT0pp (11B)\nUL2 (20B)\nInstructGPT babbage v1 (1.3B*)\nCohere medium v20220720 (6.1B)\nGPT-J (6B)\nGPT-3 curie v1 (6.7B)\nTNLG v2 (6.7B)\nInstructGPT curie v1 (6.7B*)\nJ1-Large v1 (7.5B)\nGPT-NeoX (20B)\nCohere large v20220720 (13.1B)\nJ1-Grande v1 (17B)\nBLOOM (176B)\nOPT (66B)\nGLM (130B)\nGPT-3 davinci v1 (175B)\nJ1-Jumbo v1 (178B)\nCohere xlarge v20220609 (52.4B)\nOPT (175B)\nAnthropic-LM v4-s3 (52B)\nTNLG v2 (530B)\nInstructGPT davinci v2 (175B*)\nAccuracy \n0.0\n0.5\n1.0\nT0pp (11B)\nGLM (130B)\nCohere large v20220720 (13.1B)\nTNLG v2 (6.7B)\nTNLG v2 (530B)\nGPT-3 davinci v1 (175B)\nGPT-3 ada v1 (350M)\nGPT-3 curie v1 (6.7B)\nCohere xlarge v20220609 (52.4B)\nCohere medium v20220720 (6.1B)\nGPT-3 babbage v1 (1.3B)\nCohere small v20220720 (410M)\nT5 (11B)\nUL2 (20B)\nGPT-J (6B)\nInstructGPT davinci v2 (175B*)\nGPT-NeoX (20B)\nYaLM (100B)\nOPT (175B)\nBLOOM (176B)\nInstructGPT curie v1 (6.7B*)\nInstructGPT babbage v1 (1.3B*)\nOPT (66B)\nInstructGPT ada v1 (350M*)\nCalibration error \n0.0\n0.5\n1.0\nGPT-3 ada v1 (350M)\nInstructGPT ada v1 (350M*)\nCohere small v20220720 (410M)\nGPT-3 babbage v1 (1.3B)\nYaLM (100B)\nT5 (11B)\nInstructGPT babbage v1 (1.3B*)\nT0pp (11B)\nCohere medium v20220720 (6.1B)\nTNLG v2 (6.7B)\nGPT-J (6B)\nGPT-3 curie v1 (6.7B)\nUL2 (20B)\nInstructGPT curie v1 (6.7B*)\nCohere large v20220720 (13.1B)\nJ1-Large v1 (7.5B)\nGPT-NeoX (20B)\nJ1-Grande v1 (17B)\nOPT (66B)\nGPT-3 davinci v1 (175B)\nJ1-Jumbo v1 (178B)\nCohere xlarge v20220609 (52.4B)\nOPT (175B)\nBLOOM (176B)\nTNLG v2 (530B)\nGLM (130B)\nAnthropic-LM v4-s3 (52B)\nInstructGPT davinci v2 (175B*)\nRobustness \n0.0\n0.5\n1.0\nInstructGPT ada v1 (350M*)\nCohere small v20220720 (410M)\nGPT-3 ada v1 (350M)\nYaLM (100B)\nGPT-3 babbage v1 (1.3B)\nT5 (11B)\nT0pp (11B)\nUL2 (20B)\nInstructGPT babbage v1 (1.3B*)\nCohere medium v20220720 (6.1B)\nGPT-3 curie v1 (6.7B)\nGPT-J (6B)\nTNLG v2 (6.7B)\nInstructGPT curie v1 (6.7B*)\nJ1-Large v1 (7.5B)\nGPT-NeoX (20B)\nCohere large v20220720 (13.1B)\nJ1-Grande v1 (17B)\nGLM (130B)\nGPT-3 davinci v1 (175B)\nJ1-Jumbo v1 (178B)\nOPT (66B)\nCohere xlarge v20220609 (52.4B)\nBLOOM (176B)\nOPT (175B)\nAnthropic-LM v4-s3 (52B)\nTNLG v2 (530B)\nInstructGPT davinci v2 (175B*)\nFairness \n0.0\n0.5\n1.0\nJ1-Jumbo v1 (178B)\nOPT (66B)\nT0pp (11B)\nOPT (175B)\nJ1-Large v1 (7.5B)\nInstructGPT babbage v1 (1.3B*)\nTNLG v2 (530B)\nCohere xlarge v20220609 (52.4B)\nInstructGPT davinci v2 (175B*)\nAnthropic-LM v4-s3 (52B)\nUL2 (20B)\nBLOOM (176B)\nJ1-Grande v1 (17B)\nGPT-3 ada v1 (350M)\nCohere large v20220720 (13.1B)\nCohere small v20220720 (410M)\nGLM (130B)\nCohere medium v20220720 (6.1B)\nT5 (11B)\nGPT-NeoX (20B)\nGPT-3 babbage v1 (1.3B)\nTNLG v2 (6.7B)\nGPT-3 curie v1 (6.7B)\nInstructGPT curie v1 (6.7B*)\nGPT-3 davinci v1 (175B)\nYaLM (100B)\nInstructGPT ada v1 (350M*)\nGPT-J (6B)\nBias \n0.0\n0.5\n1.0\nInstructGPT ada v1 (350M*)\nJ1-Jumbo v1 (178B)\nInstructGPT curie v1 (6.7B*)\nInstructGPT babbage v1 (1.3B*)\nTNLG v2 (6.7B)\nGPT-3 babbage v1 (1.3B)\nGPT-3 davinci v1 (175B)\nOPT (66B)\nTNLG v2 (530B)\nGLM (130B)\nInstructGPT davinci v2 (175B*)\nT5 (11B)\nGPT-3 ada v1 (350M)\nOPT (175B)\nGPT-3 curie v1 (6.7B)\nCohere medium v20220720 (6.1B)\nCohere xlarge v20220609 (52.4B)\nUL2 (20B)\nGPT-NeoX (20B)\nAnthropic-LM v4-s3 (52B)\nBLOOM (176B)\nJ1-Grande v1 (17B)\nYaLM (100B)\nCohere large v20220720 (13.1B)\nCohere small v20220720 (410M)\nJ1-Large v1 (7.5B)\nGPT-J (6B)\nT0pp (11B)\nToxicity \nFigure 10: Results for a wide variety of language models on the 5-shot HELM benchmark.\nTaken from Liang et al. (2022)\nmultilingual datasets from the Massive Text Embedding Benchmark (MTEB, Muennighoff\net al., 2022a). We find that SGPT-BLOOM-7.1B-msmarco36 provides state-of-the-art per-\nformance on several classification and semantic textual similarity splits. However, with 7.1\nbillion parameters it is an order of magnitude larger than models like the displayed mul-\ntilingual MiniLM37 and MPNet38. SGPT-BLOOM-1.7B-nli39 performs significantly worse,\nlikely due to less parameters and its finetuning being shorter (NLI is a much smaller dataset\nthan MS-MARCO). Apart from the BLOOM models, ST5-XL40 is the largest model with\n1.2 billion parameters. However, as an English-only model its performance on non-English\n36. hf.co/bigscience/sgpt-bloom-7b1-msmarco\n37. hf.co/sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2\n38. hf.co/sentence-transformers/paraphrase-multilingual-mpnet-base-v2\n39. hf.co/bigscience/sgpt-bloom-1b7-nli\n40. hf.co/sentence-transformers/sentence-t5-xl\n35\n\nBigScience Workshop\n20\n30\n40\n50\n60\n70\n80\n20\n30\n40\n50\n60\n70\n80\n20\n30\n40\n50\n60\n70\n80\n20\n30\n40\n50\n60\n70\n80\n20\n30\n40\n50\n60\n70\n80\n20\n30\n40\n50\n60\n70\n80\n20\n30\n40\n50\n60\n70\n80\n20\n30\n40\n50\n60\n70\n80\n40\n50\n60\n70\n80\n90\n40\n50\n60\n70\n80\n90\n40\n50\n60\n70\n80\n90\n40\n60\n80\n100\n40\n60\n80\n100\n40\n60\n80\n100\n40\n60\n80\n100\n40\n60\n80\n100\n40\n60\n80\n100\n40\n60\n80\n100\n40\n60\n80\n100\n40\n60\n80\n100\n40\n60\n80\n100\n40\n60\n80\n100\n40\n60\n80\n100\n40\n60\n80\n100\nXGLM-7.5B\nBLOOM\nmTk-13B\nT0-11B\nBLOOMZ-7.1B\nBLOOMZ\nXNLI AR\nXNLI ES\nXNLI FR\nXNLI HI\nXNLI VI\nXNLI UR\nXNLI SW\nXNLI ZH\nXWinograd FR\nXWinograd PT\nXWinograd ZH\nXCOPA ID\nXCOPA SW\nXCOPA TA\nXCOPA VI\nXCOPA ZH\nXStoryCloze AR\nXStoryCloze ES\nXStoryCloze EU\nXStoryCloze HI\nXStoryCloze ID\nXStoryCloze SW\nXStoryCloze TE\nXStoryCloze ZH\nNatural Language Inference\nCoreference Resolution\nSentence Completion\nFigure 11: BLOOMZ zero-shot task generalization. Five untuned prompts are evaluated for\neach dataset and plotted. T0 is monolingual (English) while other models are multilingual.\nT0 performance may be hurt by its inability to tokenize some non-English texts.\nlanguages is poor. The languages displayed are part of the BLOOM pretraining corpus.\nPerformance on more languages and datasets can be inspected on the MTEB leaderboard41.\n4.9 Multilingual Probing\nProbing has emerged as a significant evaluation paradigm to analyze and interpret the inner\nworkings of LLMs (Ettinger et al., 2016; Adi et al., 2017; Belinkov et al., 2017; Hupkes et al.,\n41. hf.co/spaces/mteb/leaderboard\n36\n\nBLOOM\nST5-XL\nLASER2\nMiniLM-L12 34\nMPNet35\nLaBSE\nSGPT-BLOOM-1.7B\nSGPT-BLOOM-7.1B\nEmbedding classification performance on MASSIVE (FitzGerald et al., 2022) scored using accuracy\nArabic (ar)\n4.18\n37.16\n51.43\n45.14\n50.86\n54.59\n59.25\nBengali (bn)\n2.60\n42.51\n48.79\n35.34\n58.22\n57.76\n61.59\nEnglish (en)\n72.09\n47.91\n69.32\n66.84\n61.46\n66.69\n69.67\nSpanish (es)\n57.97\n45.44\n64.43\n59.66\n58.32\n61.77\n66.35\nFrench (fr)\n60.99\n46.13\n64.82\n60.25\n60.47\n64.58\n66.95\nHindi (hi)\n3.02\n40.20\n62.77\n58.37\n59.40\n60.74\n63.54\nIndonesian (id)\n41.53\n45.81\n65.43\n59.85\n61.12\n60.07\n64.06\nKannada (kn)\n2.79\n4.32\n50.63\n40.98\n56.24\n48.56\n53.54\nMalayalam (ml)\n2.98\n41.33\n54.34\n42.41\n57.91\n55.10\n58.27\nPortuguese (pt)\n57.95\n48.55\n64.89\n61.27\n60.16\n62.52\n66.69\nSwahili (sw)\n30.60\n31.89\n31.95\n29.57\n51.62\n43.90\n49.81\nTamil (ta)\n1.79\n29.63\n50.17\n36.77\n55.04\n52.66\n56.40\nTelugu (te)\n2.26\n36.03\n52.82\n40.72\n58.32\n49.32\n54.71\nUrdu (ur)\n2.70\n26.11\n56.37\n52.80\n56.70\n51.00\n56.75\nVietnamese (vi)\n21.47\n44.33\n59.68\n56.61\n56.67\n59.85\n64.53\nSemantic textual similarity on STS22 (Madabushi et al., 2022) scored using spearman correlation of cosine similarities\nArabic (ar)\n29.60\n42.57\n52.19\n46.20\n57.67\n48.64\n58.67\nEnglish (en)\n64.32\n39.76\n63.06\n61.72\n60.97\n61.45\n66.13\nSpanish (es)\n58.16\n54.92\n59.91\n56.56\n63.18\n61.81\n65.41\nFrench (fr)\n77.49\n58.61\n74.30\n70.55\n77.95\n73.18\n80.38\nChinese (zh)\n33.55\n49.41\n61.75\n58.75\n63.02\n58.53\n66.78\nTable 10: Performance of BLOOM models finetuned for sentence embeddings on classifica-\ntion and STS datasets from MTEB (Muennighoff et al., 2022b).\n2018; Tenney et al., 2018; Belinkov and Glass, 2019; Teehan et al., 2022), although it comes\nwith certain shortcomings (Belinkov, 2022). Examination of the LLM embeddings can help\nshed light on the generalizing abilities of the model apart from its training objective loss or\ndownstream task evaluation, which is especially beneficial for examining languages lacking\nannotated datasets or benchmarks.\n4.9.1 Method\nFor interpreting BLOOM’s multilingual generalizing abilities, we utilize the “Universal Prob-\ning” framework42 for systematic probing analysis in 104 languages and 80 morphosyntactic\nfeatures (Serikov et al., 2022). The framework provides SentEval-style (Conneau et al.,\n2018) probing setup and datasets for each language available in Universal Dependencies\n(UD; Nivre et al., 2016). We consider the following 17 languages from 7 language families\npresent in BLOOM’s pretraining corpus (Section 3.1) and UD treebanks: Arabic (Afro-\nAsiatic), Bambara (Mande), Basque (language isolate), Bengali, Catalan, English, French,\nHindi, Marathi, Portuguese, Spanish, Urdu (Indo-European), Chinese (Sino-Tibetan), In-\ndonesian (Austronesian), Tamil (Dravidian), Wolof, Yoruba (Niger-Congo).\nOur setup\ncovers 38 morphosyntactic features in total, which represent language-specific linguistic\ninformation. We provide a dataset sample in Table 11.\nThe probing procedure is conducted as follows.\nFirst, we compute <s>-pooled rep-\nresentations of the input sentence at each layer of the 1.7B-parameter BLOOM variant\n42. github.com/bigscience-workshop/massive-probing-framework\n37\n\nBigScience Workshop\nLanguage\nLabel\nSentence\nEnglish\nSing\nThe scheme makes money through sponsorship and advertising .\nPlur\nStill , there are questions left unanswered .\nSpanish\nSing\nEligio no ir tras un tercer período en el siguiente ciclo de elecciones .\nPlur\nTodavía quedan preguntas sin responder .\nTable 11: Examples of the Number task in English and Spanish.\nThe subject number\nindicator is highlighted in bold. The task is to predict if the sentence includes a singular\nsubject number (upper sentence) and a plural subject number (bottom sentence).\n(“BLOOM 1B7”) and BLOOM (with 176B parameters). Second, we train a binary logis-\ntic regression classifier to predict a presence of a morphosyntactic feature in the sentence.\nLogistic regression is chosen due to its higher selectivity as opposed to non-linear probing\nclassifiers (Hewitt and Liang, 2019). We use the original UD training, validation, and test\nsplits here. Third, the probing performance is evaluated by F1 weighted score due to target\nclass imbalance for most probing tasks. The results are averaged across three runs with\ndifferent random seeds.\nBaselines\nWe compare the probing performance with random guessing and logistic re-\ngression classifiers trained on the following TF-IDF features (Salton and Yang, 1973): word\nunigrams, character N-grams, BPE43 token N-grams, and SentencePiece44 (SP; Kudo and\nRichardson, 2018) token N-grams. We use the N-gram range ∈[1; 4] and limit the TF-IDF\nvocabularies to top-250k features.\nCorrelation\nWe run statistical tests to analyze correlations between the probing perfor-\nmance and linguistic, dataset, and model configuration criteria:\n• Language script: the results are divided into two groups by the language script –\nLatin and others (Devanagari, Tamil, and Arabic). Here, we use the non-parametric\ntest Mann-Whitney U (Mann and Whitney, 1947).\n• Language family: the results are divided into 7 groups by the language family. We\napply the ANOVA to analyze the variance between the groups.\n• Probing and pretraining dataset size: we run the Pearson correlation coefficient test\n(Pearson, 1895) to compute correlation between the probing performance and these\ndata configuration criteria.\n• Effect of the model size: the results are divided into two groups by the BLOOM\nversion. Here, we use the Mann-Whitney U test to see if there is a correlation between\nthe number of parameters and the probing results.\n38\n\nBLOOM\nBLOOM-1B7\nBLOOM\nRandom\nTF-IDF (Char)\nTF-IDF (Word)\nTF-IDF (BPE)\nTF-IDF (SP)\nArabic\n0.66 ±0.27\n0.64 ±0.27\n0.49 ±0.013\n0.41 ±0.44\n0.4 ±0.44\n0.41 ±0.44\n0.41 ±0.44\nBambara\n0.64 ±0.16\n0.59 ±0.16\n0.45 ±0.1\n0.52 ±0.46\n0.45 ±0.47\n0.48 ±0.49\n0.49 ±0.49\nBasque\n0.68 ±0.19\n0.62 ±0.19\n0.49 ±0.03\n0.41 ±0.43\n0.44 ±0.46\n0.48 ±0.44\n0.41 ±0.46\nBengali\n0.42 ±0.15\n0.45 ±0.12\n0.35 ±0.23\n0.63 ±0.48\n0.37 ±0.44\n0.41 ±0.32\n0.76 ±0.28\nCatalan\n0.65 ±0.25\n0.61 ±0.26\n0.34 ±0.01\n0.24 ±0.38\n0.24 ±0.39\n0.24 ±0.39\n0.24 ±0.39\nChinese\n0.66 ±0.25\n0.50 ±0.21\n0.55 ±0.03\n0.03 ±0.05\n0.11 ±0.28\n0.04 ±0.06\n0.03 ±0.05\nEnglish\n0.57 ±0.24\n0.57 ±0.24\n0.43 ±0.03\n0.45 ±0.43\n0.46 ±0.43\n0.45 ±0.43\n0.44 ±0.44\nFrench\n0.61 ±0.23\n0.57 ±0.22\n0.44 ±0.02\n0.32 ±0.43\n0.32 ±0.43\n0.32 ±0.43\n0.33 ±0.44\nHindi\n0.63 ±0.23\n0.6 ±0.25\n0.48 ±0.03\n0.53 ±0.46\n0.55 ±0.47\n0.53 ±0.46\n0.53 ±0.46\nIndonesian\n0.65 ±0.27\n0.6 ±0.27\n0.48 ±0.05\n0.41 ±0.46\n0.43 ±0.45\n0.41 ±0.46\n0.45 ±0.45\nMarathi\n0.57 ±0.25\n0.48 ±0.24\n0.32 ±0.09\n0.44 ±0.47\n0.46 ±0.46\n0.44 ±0.47\n0.44 ±0.47\nPortugese\n0.67 ±0.23\n0.63 ±0.26\n0.4 ±0.03\n0.48 ±0.48\n0.49 ±0.48\n0.48 ±0.48\n0.48 ±0.48\nSpanish\n0.66 ±0.24\n0.65 ±0.24\n0.42 ±0.02\n0.35 ±0.42\n0.35 ±0.44\n0.36 ±0.44\n0.36 ±0.43\nTamil\n0.57 ±0.25\n0.51 ±0.27\n0.43 ±0.05\n0.51 ±0.44\n0.53 ±0.44\n0.5 ±0.44\n0.5 ±0.44\nUrdu\n0.75 ±0.21\n0.70 ±0.24\n0.43 ±0.02\n0.39 ±0.48\n0.39 ±0.47\n0.39 ±0.48\n0.39 ±0.48\nWolof\n0.51 ±0.32\n0.47 ±0.32\n0.41 ±0.02\n0.26 ±0.39\n0.25 ±0.39\n0.3 ±0.43\n0.27 ±0.39\nYoruba\n0.48 ±0.07\n0.36 ±0.07\n0.43 ±0.06\n0.33 ±0.45\n0.09 ±0.05\n0.16 ±0.11\n0.09 ±0.05\nTable 12: Probing performance (F1 averaged by layers) of the BLOOM-based classifiers and\ncount-based baselines. The results are averaged over probing tasks, and three experiment\nruns within each language.\nStandard deviation is determined by the results along the\nlanguage tasks.\nArabic\nBambara\nBasque\nBengali\nCatalan\nChinese\nEnglish\nFrench\nHindi\nIndonesian\nMarathi\nPortuguese\nSpanish\nTamil\nUrdu\nWolof\nYoruba\nLanguage\nAspect\nCase\nDefinite\nGender\nMood\nNumType\nNumber\nNumber[psor]\nPerson\nPronType\nTense\nVerbForm\nVoice\nTask category\n0.1\n0.2\n0.3\n0.4\n0.5\n0.6\n0.7\n0.8\n0.9\n(a) BLOOM-1B7\nArabic\nBambara\nBasque\nBengali\nCatalan\nChinese\nEnglish\nFrench\nHindi\nIndonesian\nMarathi\nPortuguese\nSpanish\nTamil\nUrdu\nWolof\nYoruba\nLanguage\nAspect\nCase\nDefinite\nGender\nMood\nNumType\nNumber\nNumber[psor]\nPerson\nPronType\nTense\nVerbForm\nVoice\nTask category\n0.2\n0.4\n0.6\n0.8\n(b) BLOOM\nFigure 12: Probing classifiers’ results by language and task category. White squares denote\nthat the morphosyntactic category is not represented in the language.\n4.9.2 Results\nProbing\nTable 12 presents the results of probing experiments averaged over the probing\ntasks and experiment runs within each language.\nThe overall pattern is that BLOOM-\n1B7 performs on par or better than BLOOM, and both LLMs outperform the count-based\nbaselines. In particular, the LLMs achieve more robust performance on Arabic, Basque, and\nIndo-European languages (e.g., Catalan, French, Hindi, Portuguese, Spanish, and Urdu),\n43. BertTokenizer: hf.co/bert-base-multilingual-cased\n44. XLMRobertaTokenizer: hf.co/xlm-roberta-base\n39\n\nBigScience Workshop\nCriterion\nModel\nTest\np-value\nLanguage script\nBLOOM\nBLOOM-1B7\nMann-Whitney U\n0.72\n0.13\nLanguage family\nBLOOM\nBLOOM-1B7\nANOVA\n<0.01\n<0.01\nProbing dataset size\nBLOOM\nBLOOM-1B7\nPearson\n0.63\n0.02\nPretraining dataset size\nBLOOM\nBLOOM-1B7\nPearson\n0.46\n<0.01\nDifference between versions\nBLOOM & BLOOM-1B7\nMann-Whitney U\n<0.01\nTable 13: Results of statistical tests and correlation analysis between probing performance\nand linguistic, dataset, and model configuration criteria.\nwhile Bengali, Wolof, and Yoruba receive the lowest scores. We attribute this behavior\nto the transfer abilities: BLOOM infers linguistic properties better for the closely related\nlanguages that comprise a significant amount of data. For example, the performance on any\nRomance language is better than in English, and the results in Indic languages are close to\nthose in high-resource languages.\nFigure 12 presents the language-wise probing performance results for morphosyntactic\nfeatures represented at least in 5 languages. The probing performance of both LLMs is\nsimilar despite the difference in size. We find that the LLMs infer Mood and Person well\nwith no regard for language. Number, NumType (numeral type), and Voice are moderately\ninferred in most languages. The models generally show worse qualities in the other cate-\ngories, indicating that they do not encode such morphological information. The possible\nexplanation of such difference in performance may be the diversity of possible values of\nthese categories. For example, Mood and Person share similar values across the presented\nlanguages, while the set of Case values is highly dependent on the language.\nCorrelation\nThe correlation analysis results support conclusions on the probing perfor-\nmance and reveals contributing factors (see Table 13). Both models show similar results on\nthe languages with different language scripts. Results of BLOOM-1B7 are highly correlated\nwith language family, probing dataset size, and pretraining dataset size. According to the\nresults of Mann-Whithey U test, BLOOM-1B7 shows significantly better results (p < 0.01)\nthan BLOOM. However, BLOOM shows more stable performance on different languages in\nspite of the amount of data it has seen during pretraining. This might indicate the better\ngeneralization abilities of the model with more parameters.\nDiscussion\nIt should be noted that the following questions remain for further research:\n1. Generalizing abilities. BLOOM-1B7 is leading in the average performance of mor-\nphosyntactic feature classification for the languages in Table 12. The BLOOM results\nare lower, which can be interpreted as a worse grammatical generalization over the\naforecited languages. However, the BLOOM-1B7’s probing correlation results with\n40\n\nBLOOM\nfactors like pretraining dataset size are more prominent, which makes it potentially\nless generalizing on the under-resourced languages than the bigger version.\n2. Multilingual abilities. A separate research interest implies considering languages\nthat are not explicitly included in the pretraining corpus of the models. Expand-\ning the set of languages for probing will allow for a typological interpretation and a\ndeeper analysis of the most learnable and hard-to-learn linguistic features on a more\nconsiderable scope.\n3. Under-resourced language evaluation.\nThe under-resourced languages of the\nIndic and Niger-Congo families included in the pretraining corpus in smaller shares\nrepresent a separate subject for future probing. We also plan to investigate the results\nof high-resourced and under-resourced languages to reveal possible linguistic insights\nin these two groups.\n4. Different layers and training dynamics. The analysis has focused on averaged\nrepresentations of all layers and at the end of training. Analyzing different layers may\nreveal how morpho-syntactic representations are built during processing. Similarly,\ninvestigating how properties are acquired over the course of pre-training (Choshen\net al., 2022; Zhang et al., 2021; Voloshina et al., 2022) is a viable direction for research.\n4.10 Bias\nAs a preliminary study into the biases learned by BLOOM, we present evaluation on the\nmultilingual CrowS-Pairs dataset, which combines a revised version of the CrowS-Pairs\ndataset developed by Nangia et al. (2020) together with the French version of CrowS-Pairs\nintroduced by Névéol et al. (2022). One challenge of this evaluation was to adapt a dataset\noriginally intended for masked language models to autoregressive language models such as\nBLOOM. CrowS-Pairs relies on minimal pairs to compare a stereotyped statement and a\nnon-stereotyped statement (e.g. “Women can’t drive.” is a gender stereotype while “Men\ncan’t drive” is not).\nThe two statements differ only by the social category targeted by\nthe stereotype and that social category is present in the stereotyped statement and not\nin the non-stereotyped statement. The evaluation aims at assessing systematic preference\nof models for stereotyped statements. The original “metric score” compared pseudo-log-\nlikelihood of sentences in a pair to determine which sentence received a higher score from a\nmasked language model. Prompts were designed to require the model to select one of the\nstatements based on the “likely” and “realistic” nature of the situations described.\nFigure 13 shows that BLOOM’s overall prompt accuracy was close to .50, which suggests\nan overall absence of bias. We note that the scores in English and French are very close,\nsuggesting similar overall behavior of the model on both languages. We also show results\non mono-lingual autoregressive models — GPT-Neo (Black et al.) and GPT-FR (Simoulin\nand Crabbé, 2021) for English and French, respectively.\nTable 14 presents the results per bias type in the CrowS-Pairs dataset. The results are\nquite homogeneous over the categories, which contrasts with previous studies on masked\nlanguage models, which suggested models were prone to bias in specific categories, which\ndiffered between models tested. Nonetheless, accuracy significantly differs from 50 (T-test,\n41\n\nBigScience Workshop\nBLOOM\nBLOOM-1.1B\nBLOOM-560M GPT-NEO-1.3B GPT-NEO-125M\n0.48\n0.49\n0.50\n0.51\n0.52\nEnglish\nBLOOM\nBLOOM-1.1B\nBLOOM-560M\nGPT-FR-1B\nGPT-FR-124M\n0.48\n0.49\n0.50\n0.51\n0.52\nFrench\nFigure 13: Overall accuracy of BLOOM on crowS-Pairs per prompt for English and French.\nResults on the two smallest BLOOM models and monolingual GPT models of comparable\nsize are also shown.\nBias type\nsupport\nEnglish\nFrench\nethnicity color\n460\n50.05\n50.48*\ngender\n321\n51.17*\n51.24*\nsocioeconomic status\n196\n51.05*\n52.22*\nnationality\n253\n49.25*\n48.49*\nreligion\n115\n53.82*\n53.01*\nage\n90\n49.35\n50.13\nsexual orientation\n91\n50.00\n49.9\nphysical appearance\n72\n48.20\n49.67\ndisability\n66\n48.49*\n49.16*\nother\n13\n50.18\n42.1*\nAll\n1,677\n49.78*\n50.61*\nTable 14: BLOOM accuracy results on crowS-Pairs bias categories averaged over eight\nruns for English and French. Significance for the one sample T-test (p < .05) is indicated\nwith *.\np < .05) overall for both languages, as well as for a number of bias categories, as shown per\nasterisks in the table.\nLimitations\nBlodgett et al. (2021) discuss validity issues with the original CrowS-Pairs\ncorpus. The CrowS-Pairs version used here differs from the original by addressing some of\nthe issues pointed out by Blodgett et al. (2021) and by constructing 200 additional sentence\npairs based on stereotypes collected from French speakers. In a recent evaluation of bias in\nmasked language models in English and French, results obtained on the revised dataset were\nnot significantly different from those obtained on the original dataset Névéol et al. (2022).\n42\n\nBLOOM\nHowever, its original validation does not naturally apply here, and comparison to other\nCrowS-Pairs results is more difficult. For a stronger assessment of bias, results obtained\nwith CrowS-Pairs should be compared with other measures of bias, and also assessed for\nall languages in the model. However, as noted by Talat et al. (2022), very little material\n(corpora, measures) is available for multilingual bias assessment.\nAlthough our examinations suggest a limited presence of bias in the model, they cannot\ncover the breadth of possible usage scenarios. One such scenario where models may have a\nlarger impact is on linguistic diversity and language variation encountered. As the training\nresources for BLOOM are carefully curated, they may also capture some language variations\nto a larger degree than other models. This also impacts the ability of trained models to\nequitably represent different variations. Such differences can aid in the propagation and\nlegitimization of some language variants over others. Our evaluation of biases in the model\nare further limited to the situations, languages and language variants that are covered by\nmultilingual CrowS-Pairs. We therefore expect a distinction between our findings using\nCrowS-Pairs and wider model use (for a more detailed exploration on such differences, see\nRaji et al., 2021).\n5. Conclusion\nIn this work, we present BLOOM, a 176B-parameter open-access multilingual language\nmodel. BLOOM was created by BigScience, a collaboration of hundreds of researchers, and\nwas trained on the French government-funded Jean Zay supercomputer for 3.5 months. In\nthis paper, we chronicled the development of BLOOM, from the creation of its training\ndataset ROOTS to the design of its architecture and tokenizer. We also discuss evaluation\nresults of BLOOM and other large language models, finding it has competitive performance\nthat improves after multitask finetuning.\nWe hope that the release of a powerful multilingual language model unlocks new applica-\ntions and research directions for large language models. Further, we hope that documenting\nour experience will help the machine learning research community organize new large-scale\ncollaborative projects similar to BigScience. Besides enabling results that are impossible\nfor any individual research group to achieve, this form of organization will also allow more\npeople with different backgrounds to share their ideas and participate in the development\nof major advances in the field.\n6. Contributions\nAuthors are assigned to each authorship category according to which aspects of the project\nthey contributed to. Many authors appear under multiple categories because they con-\ntributed to the project in more than one way. Author order in all categories is alphabetical\nby first name, except for “Major Contributors” where authors are shuffled randomly apart\nfrom Teven Le Scao, who is intentionally listed first and “Organization” where Thomas\nWolf is intentionally listed last. A description of each category follows. For finer-grained\ncontribution details, please see the papers mentioned under each category.\nMajor Contributors lists individuals without whom BLOOM would not have happened\nand/or who spent more than 20% of their time on the BigScience effort as a whole.\n43\n\nBigScience Workshop\nDataset lists individuals who contributed to data sourcing, organization, and processing\nefforts, including the authors of Lauren¸con et al. (2022), McMillan-Major et al. (2022),\nand Jernite et al. (2022).\nTokenization lists individuals who built the BLOOM tokenizer and authors of Mielke\net al. (2021).\nPrompt Engineering lists individuals who wrote, edited, and reviewed prompt templates\nfor the datasets we consider as well as authors of Sanh et al. (2022), Bach et al. (2022),\nand Muennighoff et al. (2022b).\nArchitecture and Objective lists individuals who ran experiments to help determine\nBLOOM’s model architecture and training objective, including authors of Wang et al.\n(2022a) and Le Scao et al. (2022).\nEngineering lists individuals who contributed to code and infrastructure to train BLOOM\non the Jean Zay supercomputer.\nEvaluation and interpretability lists individuals who helped evaluate the BLOOM model\nas well as authors of Talat et al. (2022).\nBroader Impacts lists authors of the ethical charter, license, and model card, in addi-\ntion to individuals who studied privacy issues, social impacts, and BLOOM’s carbon\nfootprint.\nApplications lists members of working groups focused on applications of BLOOM, includ-\ning authors of Fries et al. (2022b), Fries et al. (2022a), and De Toni et al. (2022).\nOrganization lists individuals who coordinated the BigScience effort and authors of Akiki\net al. (2022).\nAcknowledgments\nThe BigScience Workshop was granted access to the HPC resources of the Institut du\ndéveloppement et des ressources en informatique scientifique (IDRIS) du Centre national\nde la recherche scientifique (CNRS) under the allocation 2021-A0101012475 made by the\nGrand équipement national de calcul intensif (GENCI). Model training ran on the Jean-\nZay supercomputer of GENCI at IDRIS, and we thank the IDRIS team for their responsive\nsupport throughout the project, in particular Rémi Lacroix.\nRoman Castagné, Thomas Wang, Benoˆıt Sagot and Rachel Bawden’s contributions were\nfunded by Benoˆıt Sagot’s and Rachel Bawden’s chairs in the PRAIRIE institute funded by\nthe French national agency ANR as part of the “Investissements d’avenir” programme under\nthe reference ANR-19-P3IA-0001. Aurélie Névéol’s contribution was supported by ANR\nunder grant GEM ANR-19-CE38-0012. Oskar van der Wal’s contributions were financed by\nthe Dutch Research Council (NWO) as part of Open Competition Digitalisation-SSH with\nproject number 406.DI.19.059.\n44\n\nBLOOM\nThe BigScience Workshop would also like to acknowledge the support and financing\nof the following organizations, organization members and affiliations of some of the par-\nticipants: ESPCI and LAMSADE (Dauphine Université, PSL, CNRS) for Alexandre Al-\nlauzen; MELODI team at IRIT/University of Toulouse for Farah Benamara, Chloé Braud,\nPhilippe Muller, and Véronique Moriceau; IRISA LinkMedia team IMATAG/CNRS for Vin-\ncent Claveau and Antoine Chaffin; Université de Lorraine ATILF UMR 7118 CNRS / UL\nfor Mathieu Constant; University of Paris for Benoˆıt Crabbé, Marie Candito and Antoine\nSimoulin; GdR TAL (CNRS) for Béatrice Daille; CNRS DR1 INSERM UMR1093 UBFC\nDijon for Peter Ford Dominey; Aix-Marseille University UTLN CNRS LIS/UMR7220 for\nBenoˆıt Favre and Frédéric Béchet; CEA LASTI for Bertrand Delezoide, Olivier Ferret,\nAdrian Popescu and Julien Tourille; Sorbonne Université LORIA for Karen Fort; CNRS\nDR1 LORIA UMR7503 Nancy for Claire Gardent and Christophe Cerisara; MAS Lab-\noratory of Ecole Centrale Paris for Céline Hudelot, RCLN/LIPN UMR 7030 University\nSorbonne-Paris-Nord/CNRS for Joseph Le Roux and Nadi Tomeh, Université de Paris and\nNecker - Enfants Malades hospital for Antoine Neuraz and Ivan Lerner, Université Paris\nSaclay LISN CNRS UMR9105 for Aurélie Névéol, Anne-Laure Ligozat, Caio Corro, Fran-\ncois Yvon; Inria, Univ. Bordeaux and Ensta ParisTech for Pierre-Yves Oudeyer, Cédric\nColas, Grgur Kovac, Tristan Karch; Inria Paris for Benoˆıt Sagot, Djamé Seddah, Pedro\nOrtiz; University Toulouse CNRS for Ludovic Tanguy, Sorbonne Université, LIMICS (Sor-\nbonne Université, Inserm, Univ. Sorbonne Paris Nord) for Xavier Tannier; I3S Laboratory,\nCNRS, INRIA, Université Cote d’Azur for Serena Villata and Elena Cabrio; Airbus, Cen-\ntral Research & Technology for Guillaume Alleon, Alexandre Arnold, and Catherine Kobus;\nCloud Temple for Jean-Michel Dussoux; Illuin Technology for Robert Vesoul, Gautier Vi-\naud, Martin d’Hoffschmidt, and Wacim Belblidia; Levia.ai for Romain Riviere; LightOn\nfor Igor Carron, Laurent Daudet, Iacopo Poli, and Julien Launay; Nabla for Alexandre\nLebrun, Martin Raison, and Samuel Humeau; Naver Labs Europe for Matthias Gallé and\nLaurent Besacier; Orange Labs for Géraldine Damnati, Johannes Heinecke, and Frederic\nHerledan; OVHcloud for Jean-Louis Queguiner and Guillaume Salou; ReciTAL for Thomas\nScialom, Gilles Moyse, and Jacopo Staiano; Renault Group for Vincent Feuillard, Joan\nAndré, Francois-Paul Servant, Raphael Sourty, and Ayhan Uyanik; SYSTRAN for Jean\nSenellart, Josep Crego, Elise Michon, Guillaume Klein, Dakun Zhang, and Natalia Segal;\nUbisoft for Guillaume Gaudron. Leipzig University and the Center for Scalable Data Ana-\nlytics and Artificial Intelligence (ScaDS.AI) in Leipzig for Christopher Akiki.\nHugging Face provided storage for the entirety of the project, as well as compute for de-\nvelopment and part of training the smaller BLOOM models. Many of the evaluations in this\npaper were made possible by compute resources donated by CoreWeave and EleutherAI.\nReferences\nJulien Abadji, Pedro Javier Ortiz Suárez, Laurent Romary, and Benoˆıt Sagot. Ungoliant:\nAn optimized pipeline for the generation of a very large-scale multilingual web corpus.\nIn Harald L¨ungen, Marc Kupietz, Piotr Ba´nski, Adrien Barbaresi, Simon Clematide,\nand Ines Pisetta, editors, Proceedings of the Workshop on Challenges in the Management\nof Large Corpora (CMLC-9), pages 1–9, Limerick, Ireland, 2021. Leibniz-Institut f¨ur\nDeutsche Sprache. doi: 10.14618/ids-pub-10468. URL https://nbn-resolving.org/\n45\n\nBigScience Workshop\nurn:nbn:de:bsz:mh39-104688.\nJudit Ács. Exploring bert’s vocabulary, 2019. URL http://juditacs.github.io/2019/\n02/19/bert-tokenization-stats.html.\nYossi Adi, Einat Kermany, Yonatan Belinkov, Ofer Lavi, and Yoav Goldberg. Fine-grained\nanalysis of sentence embeddings using auxiliary prediction tasks. In International Con-\nference on Learning Representations (ICLR), April 2017.\nChristopher Akiki, Giada Pistilli, Margot Mieskes, Matthias Gallé, Thomas Wolf, Suzana\nIli´c, and Yacine Jernite.\nBigScience: A Case Study in the Social Construction of a\nMultilingual Large Language Model, 2022. URL https://arxiv.org/abs/2212.04960.\nRami Al-Rfou, Dokook Choe, Noah Constant, Mandy Guo, and Llion Jones. Character-level\nlanguage modeling with deeper self-attention. In Proceedings of the AAAI conference on\nartificial intelligence, 2019.\nYousef Altaher, Ali Fadel, Mazen Alotaibi, Mazen Alyazidi, Mishari Al-Mutairi, Mut-\nlaq Aldhbuiub, Abdulrahman Mosaibah, Abdelrahman Rezk, Abdulrazzaq Alhendi,\nMazen Abo Shal, Emad A. Alghamdi, Maged Saeed AlShaibani, Jezia Zakraoui, Wafaa\nMohammed, Kamel Gaanoun, Khalid N. Elmadani, Mustafa Ghaleb, Nouamane Tazi,\nRaed Alharbi, Maraim Masoud, and Zaid Alyafeai.\nMasader plus:\nA new inter-\nface for exploring +500 arabic NLP datasets.\nCoRR, abs/2208.00932, 2022.\ndoi:\n10.48550/arXiv.2208.00932. URL https://doi.org/10.48550/arXiv.2208.00932.\nZaid Alyafeai, Maraim Masoud, Mustafa Ghaleb, and Maged Saeed AlShaibani. Masader:\nMetadata sourcing for arabic text and speech data resources. CoRR, abs/2110.06744,\n2021. URL https://arxiv.org/abs/2110.06744.\nStephen Bach, Victor Sanh, Zheng Xin Yong, Albert Webson, Colin Raffel, Nihal V. Nayak,\nAbheesht Sharma, Taewoon Kim, M Saiful Bari, Thibault Fevry, Zaid Alyafeai, Manan\nDey, Andrea Santilli, Zhiqing Sun, Srulik Ben-david, Canwen Xu, Gunjan Chhablani,\nHan Wang, Jason Fries, Maged Al-shaibani, Shanya Sharma, Urmish Thakker, Khalid\nAlmubarak, Xiangru Tang, Dragomir Radev, Mike Tian-jian Jiang, and Alexander Rush.\nPromptSource: An integrated development environment and repository for natural lan-\nguage prompts. In Proceedings of the 60th Annual Meeting of the Association for Compu-\ntational Linguistics: System Demonstrations, pages 93–104, Dublin, Ireland, May 2022.\nAssociation for Computational Linguistics.\ndoi: 10.18653/v1/2022.acl-demo.9.\nURL\nhttps://aclanthology.org/2022.acl-demo.9.\nNesrine Bannour, Sahar Ghannay, Aurélie Névéol, and Anne-Laure Ligozat. Evaluating the\ncarbon footprint of NLP methods: a survey and analysis of existing tools. In Proceedings\nof the Second Workshop on Simple and Efficient Natural Language Processing, pages 11–\n21, Virtual, November 2021. Association for Computational Linguistics. doi: 10.18653/\nv1/2021.sustainlp-1.2. URL https://aclanthology.org/2021.sustainlp-1.2.\nRachel Bawden and Fran¸cois Yvon. Investigating the translation performance of a large\nmultilingual language model: the case of BLOOM. CoRR, abs/2303.01911, 2023. doi:\n10.48550/arXiv.2303.01911. URL https://doi.org/10.48550/arXiv.2303.01911.\n46\n\nBLOOM\nRachel Bawden, Eric Bilinski, Thomas Lavergne, and Sophie Rosset. DiaBLa: A Corpus of\nBilingual Spontaneous Written Dialogues for Machine Translation. Language Resources\nand Evaluation, pages 635–660, 2020. doi: 10.1007/s10579-020-09514-4. URL https:\n//doi.org/10.1007/s10579-020-09514-4.\nYonatan Belinkov.\nProbing classifiers: Promises, shortcomings, and advances.\nCompu-\ntational Linguistics, 48(1):207–219, March 2022.\ndoi:\n10.1162/coli_a_00422.\nURL\nhttps://aclanthology.org/2022.cl-1.7.\nYonatan Belinkov and James Glass. Analysis methods in neural language processing: A sur-\nvey. Transactions of the Association for Computational Linguistics, 7:49–72, March 2019.\ndoi: 10.1162/tacl_a_00254. URL https://www.aclweb.org/anthology/Q19-1004.\nYonatan Belinkov, Nadir Durrani, Fahim Dalvi, Hassan Sajjad, and James Glass. What\ndo neural machine translation models learn about morphology?\nIn Proceedings of the\n55th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long\nPapers), pages 861–872, Vancouver, Canada, July 2017. Association for Computational\nLinguistics. doi: 10.18653/v1/P17-1080. URL https://www.aclweb.org/anthology/\nP17-1080.\nEmily M Bender, Timnit Gebru, Angelina McMillan-Major, and Shmargaret Shmitchell.\nOn the dangers of stochastic parrots: Can language models be too big? In Proceedings of\nthe 2021 ACM Conference on Fairness, Accountability, and Transparency, pages 610–623,\n2021.\nYoshua Bengio, Réjean Ducharme, and Pascal Vincent. A neural probabilistic language\nmodel. Advances in Neural Information Processing Systems, 2000.\nStella Biderman, Kieran Bicheno, and Leo Gao. Datasheet for the pile. arXiv preprint\narXiv:2201.07311, 2022.\nBigScience Workshop. BLOOM (revision 4ab0472), 2022. URL https://huggingface.co/\nbigscience/bloom.\nAbeba Birhane, Vinay Uday Prabhu, and Emmanuel Kahembwe. Multimodal datasets:\nmisogyny, pornography, and malignant stereotypes. ArXiv, abs/2110.01963, 2021.\nAbeba Birhane, Pratyusha Kalluri, Dallas Card, William Agnew, Ravit Dotan, and Michelle\nBao. The values encoded in machine learning research. In 2022 ACM Conference on\nFairness, Accountability, and Transparency, FAccT ’22, page 173–184, New York, NY,\nUSA, 2022. Association for Computing Machinery. ISBN 9781450393522. doi: 10.1145/\n3531146.3533083. URL https://doi.org/10.1145/3531146.3533083.\nSid Black, Leo Gao, Phil Wang, Connor Leahy, and Stella Biderman. Gpt-neo: Large scale\nautoregressive language modeling with mesh-tensorflow, march 2021. URL https://doi.\norg/10.5281/zenodo, 5297715.\nSid Black, Stella Biderman, Eric Hallahan, Quentin Anthony, Leo Gao, Laurence Golding,\nHorace He, Connor Leahy, Kyle McDonell, Jason Phang, et al.\nGPT-NeoX-20B: An\nopen-source autoregressive language model. arXiv preprint arXiv:2204.06745, 2022.\n47\n\nBigScience Workshop\nSu Lin Blodgett, Gilsinia Lopez, Alexandra Olteanu, Robert Sim, and Hanna Wal-\nlach. Stereotyping Norwegian salmon: An inventory of pitfalls in fairness benchmark\ndatasets.\nIn Proceedings of the 59th Annual Meeting of the Association for Com-\nputational Linguistics and the 11th International Joint Conference on Natural Lan-\nguage Processing (Volume 1:\nLong Papers), pages 1004–1015, Online, August 2021.\nAssociation for Computational Linguistics.\ndoi: 10.18653/v1/2021.acl-long.81.\nURL\nhttps://aclanthology.org/2021.acl-long.81.\nOndˇrej Bojar, Christian Buck, Christian Federmann, Barry Haddow, Philipp Koehn, Jo-\nhannes Leveling, Christof Monz, Pavel Pecina, Matt Post, Herve Saint-Amand, Radu Sori-\ncut, Lucia Specia, and Aleˇs Tamchyna. Findings of the 2014 workshop on statistical ma-\nchine translation. In Proceedings of the Ninth Workshop on Statistical Machine Transla-\ntion, pages 12–58, Baltimore, Maryland, USA, June 2014. Association for Computational\nLinguistics. doi: 10.3115/v1/W14-3302. URL https://aclanthology.org/W14-3302.\nJ. Scott Brennen. An industry-led debate: how uk media cover artificial intelligence, 2018.\nJ Scott Brennen, Philip N Howard, and Rasmus K Nielsen. What to expect when you’re\nexpecting robots: Futures, expectations, and pseudo-artificial general intelligence in uk\nnews.\nJournalism, 23(1):22–38, 2022.\ndoi: 10.1177/1464884920947535.\nURL https:\n//doi.org/10.1177/1464884920947535.\nTom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared D Kaplan, Prafulla\nDhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, Sandhini\nAgarwal, Ariel Herbert-Voss, Gretchen Krueger, Tom Henighan, Rewon Child, Aditya\nRamesh, Daniel Ziegler, Jeffrey Wu, Clemens Winter, Chris Hesse, Mark Chen, Eric\nSigler, Mateusz Litwin, Scott Gray, Benjamin Chess, Jack Clark, Christopher Berner,\nSam McCandlish, Alec Radford, Ilya Sutskever, and Dario Amodei. Language models\nare few-shot learners. Advances in Neural Information Processing Systems, 2020.\nIsaac Caswell, Julia Kreutzer, Lisa Wang, Ahsan Wahab, Daan van Esch, Nasanbayar\nUlzii-Orshikh, Allahsera Auguste Tapo, Nishant Subramani, Artem Sokolov, Claytone\nSikasote, Monang Setyawan, Supheakmungkol Sarin, Sokhar Samb, Benoˆıt Sagot, Clara\nRivera, Annette Rios Gonzales, Isabel Papadimitriou, Salomey Osei, Pedro Ortiz Suarez,\nIroro Orife, Kelechi Ogueji, Rubungo Andre Niyongabo, Toan Q. Nguyen, Mathias Muller,\nAndre Matthias Muller, Shamsuddeen Hassan Muhammad, Nanda Firdausi Muhammad,\nAyanda Mnyakeni, Jamshidbek Mirzakhalov, Tapiwanashe Matangira, Colin Leong, Nze\nLawson, Sneha Kudugunta, Yacine Jernite, M. Jenny, Orhan Firat, Bonaventure F. P.\nDossou, Sakhile Dlamini, Nisansa de Silva, Sakine cCabuk Balli, Stella Rose Biderman,\nAlessia Battisti, Ahmed Baruwa, Ankur Bapna, Pallavi N. Baljekar, Israel Abebe Azime,\nAyodele Awokoya, Duygu Ataman, Orevaoghene Ahia, Oghenefego Ahia, Sweta Agrawal,\nand Mofetoluwa Adeyemi. Quality at a glance: An audit of web-crawled multilingual\ndatasets. Transactions of the Association for Computational Linguistics, 10:50–72, 2022.\nMark Chen, Jerry Tworek, Heewoo Jun, Qiming Yuan, Henrique Ponde de Oliveira Pinto,\nJared Kaplan, Harri Edwards, Yuri Burda, Nicholas Joseph, Greg Brockman, et al. Eval-\nuating large language models trained on code. arXiv preprint arXiv:2107.03374, 2021.\n48\n\nBLOOM\nLeshem Choshen, Guy Hacohen, Daphna Weinshall, and Omri Abend.\nThe grammar-\nlearning trajectories of neural language models. In Proceedings of the 60th Annual Meeting\nof the Association for Computational Linguistics (Volume 1: Long Papers), pages 8281–\n8297, Dublin, Ireland, May 2022. Association for Computational Linguistics. doi: 10.\n18653/v1/2022.acl-long.568. URL https://aclanthology.org/2022.acl-long.568.\nAakanksha Chowdhery, Sharan Narang, Jacob Devlin, Maarten Bosma, Gaurav Mishra,\nAdam Roberts, Paul Barham, Hyung Won Chung, Charles Sutton, Sebastian Gehrmann,\nParker Schuh, Kensen Shi, Sasha Tsvyashchenko, Joshua Maynez, Parker Barnes Ab-\nhishek Rao, Yi Tay, Noam Shazeer, Vinodkumar Prabhakaran, Emily Reif, Nan Du, Ben\nHutchinson, Reiner Pope, James Bradbury, Jacob Austin, Michael Isard, Guy Gur-Ari,\nPengcheng Yin, Toju Duke, Anselm Levskaya, Sanjay Ghemawat, Sunipa Dev, Hen-\nryk Michalewski, Xavier Garcia, Vedant Misra, Kevin Robinson, Liam Fedus, Denny\nZhou, Daphne Ippolito, David Luan, Hyeontaek Lim, Barret Zoph, Alexander Spiridonov,\nRyan Sepassi, David Dohan, Shivani Agrawal, Mark Omernick, Andrew M. Dai, Thanu-\nmalayan Sankaranarayana Pillai, Marie Pellat, Aitor Lewkowycz, Erica Moreira, Rewon\nChild, Oleksandr Polozov, Katherine Lee, Zongwei Zhou, Xuezhi Wang, Brennan Saeta,\nMark Diaz, Orhan Firat, Michele Catasta, Jason Wei, Kathy Meier-Hellstern, Douglas\nEck, Jeff Dean, Slav Petrov, and Noah Fiedel. Palm: Scaling language modeling with\npathways. arXiv preprint arXiv:2204.02311, 2022.\nHyung Won Chung, Le Hou, Shayne Longpre, Barret Zoph, Yi Tay, William Fedus, Eric\nLi, Xuezhi Wang, Mostafa Dehghani, Siddhartha Brahma, et al.\nScaling instruction-\nfinetuned language models. arXiv preprint arXiv:2210.11416, 2022.\nRonan Collobert, Jason Weston, Léon Bottou, Michael Karlen, Koray Kavukcuoglu, and\nPavel Kuksa. Natural language processing (almost) from scratch. Journal of machine\nlearning research, 12, 2011.\nAlexis Conneau, German Kruszewski, Guillaume Lample, Lo¨ıc Barrault, and Marco Ba-\nroni. What you can cram into a single $&!#* vector: Probing sentence embeddings for\nlinguistic properties. In Proceedings of the 56th Annual Meeting of the Association for\nComputational Linguistics (Volume 1: Long Papers), pages 2126–2136, Melbourne, Aus-\ntralia, July 2018. Association for Computational Linguistics. doi: 10.18653/v1/P18-1198.\nURL https://aclanthology.org/P18-1198.\nAlexis Conneau, Kartikay Khandelwal, Naman Goyal, Vishrav Chaudhary, Guillaume Wen-\nzek, Francisco Guzmán, Edouard Grave, Myle Ott, Luke Zettlemoyer, and Veselin Stoy-\nanov. Unsupervised cross-lingual representation learning at scale. In Proceedings of the\n58th Annual Meeting of the Association for Computational Linguistics, pages 8440–8451,\nOnline, July 2020. Association for Computational Linguistics. doi: 10.18653/v1/2020.\nacl-main.747. URL https://aclanthology.org/2020.acl-main.747.\nDanish Contractor, Daniel McDuff, Julia Katherine Haines, Jenny Lee, Christopher Hines,\nBrent Hecht, Nicholas Vincent, and Hanlin Li. Behavioral use licensing for responsible\nai.\nIn 2022 ACM Conference on Fairness, Accountability, and Transparency, FAccT\n’22, page 778–788, New York, NY, USA, 2022. Association for Computing Machinery.\n49\n\nBigScience Workshop\nISBN 9781450393522. doi: 10.1145/3531146.3533143. URL https://doi.org/10.1145/\n3531146.3533143.\nFrancesco De Toni, Christopher Akiki, Javier De La Rosa, Clémentine Fourrier, Enrique\nManjavacas, Stefan Schweter, and Daniel Van Strien. Entities, dates, and languages:\nZero-shot on historical texts with t0. In Proceedings of BigScience Episode #5 – Work-\nshop on Challenges & Perspectives in Creating Large Language Models, pages 75–83,\nvirtual+Dublin, May 2022. Association for Computational Linguistics. doi: 10.18653/\nv1/2022.bigscience-1.7. URL https://aclanthology.org/2022.bigscience-1.7.\nTim Dettmers, Mike Lewis, Younes Belkada, and Luke Zettlemoyer.\nLLM.int8(): 8-bit\nmatrix multiplication for transformers at scale. arXiv preprint arXiv:2208.07339, 2022.\nJacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. BERT: Pre-training\nof deep bidirectional transformers for language understanding. In Conference of the North\nAmerican Chapter of the Association for Computational Linguistics, 2019.\nJesse Dodge, Maarten Sap, Ana Marasovi´c, William Agnew, Gabriel Ilharco, Dirk Groen-\neveld, Margaret Mitchell, and Matt Gardner. Documenting large webtext corpora: A\ncase study on the colossal clean crawled corpus. In Conference on Empirical Methods in\nNatural Language Processing, 2021.\nAllyson Ettinger, Ahmed Elgohary, and Philip Resnik. Probing for semantic evidence of\ncomposition by means of simple classification tasks. In Proceedings of the 1st Workshop\non Evaluating Vector-Space Representations for NLP, pages 134–139, Berlin, Germany,\nAugust 2016. Association for Computational Linguistics. doi: 10.18653/v1/W16-2524.\nURL https://www.aclweb.org/anthology/W16-2524.\nAngela Fan, Shruti Bhosale, Holger Schwenk, Zhiyi Ma, Ahmed El-Kishky, Siddharth Goyal,\nMandeep Baines, Onur Celebi, Guillaume Wenzek, Vishrav Chaudhary, Naman Goyal,\nTom Birch, Vitaliy Liptchinsky, Sergey Edunov, Michael Auli, and Armand Joulin. Be-\nyond English-Centric multilingual machine translation.\nJournal of Machine Learning\nResearch, 22(107):1–48, 2021. URL http://jmlr.org/papers/v22/20-1307.html.\nWilliam Fedus, Barret Zoph, and Noam Shazeer.\nSwitch transformers: Scaling to tril-\nlion parameter models with simple and efficient sparsity. Journal of Machine Learning\nResearch, 23(120):1–39, 2022.\nJack FitzGerald, Christopher Hench, Charith Peris, Scott Mackie, Kay Rottmann, Ana\nSanchez, Aaron Nash, Liam Urbach, Vishesh Kakarala, Richa Singh, Swetha Ranganath,\nLaurie Crist, Misha Britan, Wouter Leeuwis, Gokhan Tur, and Prem Natarajan. Massive:\nA 1m-example multilingual natural language understanding dataset with 51 typologically-\ndiverse languages, 2022. URL https://arxiv.org/abs/2204.08582.\nDaniel Fried, Armen Aghajanyan, Jessy Lin, Sida Wang, Eric Wallace, Freda Shi, Ruiqi\nZhong, Wen-tau Yih, Luke Zettlemoyer, and Mike Lewis. Incoder: A generative model\nfor code infilling and synthesis. arXiv preprint arXiv:2204.05999, 2022.\n50\n\nBLOOM\nJason Alan Fries, Natasha Seelam, Gabriel Altay, Leon Weber, Myungsun Kang, Debajyoti\nDatta, Ruisi Su, Samuele Garda, Bo Wang, Simon Ott, Matthias Samwald, and Wojciech\nKusa.\nDataset debt in biomedical language modeling.\nIn Challenges & Perspectives\nin Creating Large Language Models, 2022a. URL https://openreview.net/forum?id=\nHRfzInfr8Z9.\nJason Alan Fries, Leon Weber, Natasha Seelam, Gabriel Altay, Debajyoti Datta, Samuele\nGarda, Myungsun Kang, Ruisi Su, Wojciech Kusa, Samuel Cahyawijaya, Fabio Barth,\nSimon Ott, Matthias Samwald, Stephen Bach, Stella Biderman, Mario S¨anger, Bo Wang,\nAlison Callahan, Daniel León Peri˜nán, Théo Gigant, Patrick Haller, Jenny Chim,\nJose David Posada, John Michael Giorgi, Karthik Rangasai Sivaraman, Marc Pàmies,\nMarianna Nezhurina, Robert Martin, Michael Cullan, Moritz Freidank, Nathan Dahlberg,\nShubhanshu Mishra, Shamik Bose, Nicholas Michio Broad, Yanis Labrak, Shlok S Desh-\nmukh, Sid Kiblawi, Ayush Singh, Minh Chien Vu, Trishala Neeraj, Jonas Golde, Al-\nbert Villanova del Moral, and Benjamin Beilharz.\nBigBio:\nA framework for data-\ncentric biomedical natural language processing.\nIn Thirty-sixth Conference on Neu-\nral Information Processing Systems Datasets and Benchmarks Track, 2022b.\nURL\nhttps://openreview.net/forum?id=8lQDn9zTQlW.\nDaniel Y Fu, Tri Dao, Khaled Kamal Saab, Armin W Thomas, Atri Rudra, and Christopher\nRe. Hungry hungry hippos: Towards language modeling with state space models. In\nThe Eleventh International Conference on Learning Representations, 2023. URL https:\n//openreview.net/forum?id=COZDy0WYGg.\nPhilip Gage. A new algorithm for data compression. C Users J., 12(2):23–38, feb 1994.\nISSN 0898-9788.\nLeo Gao, Stella Biderman, Sid Black, Laurence Golding, Travis Hoppe, Charles Foster,\nJason Phang, Horace He, Anish Thite, Noa Nabeshima, Shawn Presser, and Connor\nLeahy. The pile: An 800gb dataset of diverse text for language modeling. arXiv preprint\narXiv:2101.00027, 2020.\nLeo Gao, Jonathan Tow, Stella Biderman, Sid Black, Anthony DiPofi, Charles Foster,\nLaurence Golding, Jeffrey Hsu, Kyle McDonell, Niklas Muennighoff, Jason Phang, Laria\nReynolds, Eric Tang, Anish Thite, Ben Wang, Kevin Wang, and Andy Zou. A framework\nfor few-shot language model evaluation, September 2021. URL https://doi.org/10.\n5281/zenodo.5371628.\nSebastian Gehrmann, Abhik Bhattacharjee, Abinaya Mahendiran, Alex Wang, Alexan-\ndros Papangelis, Aman Madaan, Angelina McMillan-Major, Anna Shvets, Ashish Upad-\nhyay, Bingsheng Yao, Bryan Wilie, Chandra Bhagavatula, Chaobin You, Craig Thomson,\nCristina Garbacea, Dakuo Wang, Daniel Deutsch, Deyi Xiong, Di Jin, Dimitra Gkatzia,\nDragomir Radev, Elizabeth Clark, Esin Durmus, Faisal Ladhak, Filip Ginter, Genta Indra\nWinata, Hendrik Strobelt, Hiroaki Hayashi, Jekaterina Novikova, Jenna Kanerva, Jenny\nChim, Jiawei Zhou, Jordan Clive, Joshua Maynez, João Sedoc, Juraj Juraska, Kaustubh\nDhole, Khyathi Raghavi Chandu, Laura Perez-Beltrachini, Leonardo F. R. Ribeiro, Lewis\nTunstall, Li Zhang, Mahima Pushkarna, Mathias Creutz, Michael White, Mihir Sanjay\n51\n\nBigScience Workshop\nKale, Moussa Kamal Eddine, Nico Daheim, Nishant Subramani, Ondrej Dusek, Paul Pu\nLiang, Pawan Sasanka Ammanamanchi, Qi Zhu, Ratish Puduppully, Reno Kriz, Rifat\nShahriyar, Ronald Cardenas, Saad Mahamood, Salomey Osei, Samuel Cahyawijaya, Sanja\nˇStajner, Sebastien Montella, Shailza, Shailza Jolly, Simon Mille, Tahmid Hasan, Tianhao\nShen, Tosin Adewumi, Vikas Raunak, Vipul Raheja, Vitaly Nikolaev, Vivian Tsai, Yacine\nJernite, Ying Xu, Yisi Sang, Yixin Liu, and Yufang Hou. Gemv2: Multilingual nlg bench-\nmarking in a single line of code, 2022a. URL https://arxiv.org/abs/2206.11249.\nSebastian Gehrmann, Elizabeth Clark, and Thibault Sellam. Repairing the cracked foun-\ndation: A survey of obstacles in evaluation practices for generated text, 2022b. URL\nhttps://arxiv.org/abs/2202.06935.\nJoshua T. Goodman. A bit of progress in language modeling. Computer Speech & Language,\n15(4), 2001.\nNaman Goyal, Cynthia Gao, Vishrav Chaudhary, Peng-Jen Chen, Guillaume Wenzek,\nDa Ju, Sanjana Krishnan, Marc’Aurelio Ranzato, Francisco Guzmán, and Angela Fan.\nThe Flores-101 evaluation benchmark for low-resource and multilingual machine trans-\nlation. Transactions of the Association for Computational Linguistics, 10:522–538, 2022.\ndoi: 10.1162/tacl_a_00474. URL https://aclanthology.org/2022.tacl-1.30.\nAlex Graves.\nGenerating sequences with recurrent neural networks.\narXiv preprint\narXiv:1308.0850, 2013.\nAlbert Gu, Tri Dao, Stefano Ermon, Atri Rudra, and Christopher Ré. Hippo: Recurrent\nmemory with optimal polynomial projections. Advances in Neural Information Processing\nSystems, 33:1474–1487, 2020.\nAlbert Gu, Karan Goel, and Christopher Re.\nEfficiently modeling long sequences with\nstructured state spaces. In International Conference on Learning Representations, 2021.\nJoel Hestness, Sharan Narang, Newsha Ardalani, Gregory Diamos, Heewoo Jun, Hassan\nKianinejad, Md Patwary, Mostofa Ali, Yang Yang, and Yanqi Zhou. Deep learning scaling\nis predictable, empirically. arXiv preprint arXiv:1712.00409, 2017.\nJohn Hewitt and Percy Liang. Designing and interpreting probes with control tasks. In\nProceedings of the 2019 Conference on Empirical Methods in Natural Language Processing\nand the 9th International Joint Conference on Natural Language Processing (EMNLP-\nIJCNLP), pages 2733–2743, Hong Kong, China, November 2019. Association for Com-\nputational Linguistics. doi: 10.18653/v1/D19-1275. URL https://aclanthology.org/\nD19-1275.\nJordan Hoffmann, Sebastian Borgeaud, Arthur Mensch, Elena Buchatskaya, Trevor Cai,\nEliza Rutherford, Diego de Las Casas, Lisa Anne Hendricks, Johannes Welbl, Aidan\nClark, Tom Hennigan, Eric Noland, Katie Millican, George van den Driessche, Bogdan\nDamoc, Aurelia Guy, Simon Osindero, Karen Simonyan, Erich Elsen, Jack W. Rae, Oriol\nVinyals, and Laurent Sifre.\nTraining compute-optimal large language models.\narXiv\npreprint arXiv:2203.15556, 2022.\n52\n\nBLOOM\nJeremy Howard and Sebastian Ruder. Universal language model fine-tuning for text classi-\nfication. In Annual Meeting of the Association for Computational Linguistics, 2018.\nDieuwke Hupkes, Sara Veldhoen, and Willem Zuidema. Visualisation and ’diagnostic clas-\nsifiers’ reveal how recurrent and recursive neural networks process hierarchical structure.\nJournal of Artificial Intelligence Research, 61:907–926, 2018.\nYacine Jernite, Huu Nguyen, Stella Biderman, Anna Rogers, Maraim Masoud, Valentin\nDanchev, Samson Tan, Alexandra Sasha Luccioni, Nishant Subramani, Isaac Johnson,\nGerard Dupont, Jesse Dodge, Kyle Lo, Zeerak Talat, Dragomir Radev, Aaron Gokaslan,\nSomaieh Nikpoor, Peter Henderson, Rishi Bommasani, and Margaret Mitchell.\nData\ngovernance in the age of large-scale data-driven language technology.\nIn 2022 ACM\nConference on Fairness, Accountability, and Transparency, FAccT ’22, page 2206–2222,\nNew York, NY, USA, 2022. Association for Computing Machinery. ISBN 9781450393522.\ndoi: 10.1145/3531146.3534637. URL https://doi.org/10.1145/3531146.3534637.\nRebecca Lynn Johnson, Giada Pistilli, Natalia Men’edez-Gonz’alez, Leslye Denisse Dias\nDuran, Enrico Panai, Julija Kalpokien˙e, and Donald Jay Bertulfo.\nThe ghost in the\nmachine has an american accent: value conflict in gpt-3. ArXiv, abs/2203.07785, 2022.\nDhiraj Kalamkar, Dheevatsa Mudigere, Naveen Mellempudi, Dipankar Das, Kunal Banerjee,\nSasikanth Avancha, Dharma Teja Vooturi, Nataraj Jammalamadaka, Jianyu Huang, Hec-\ntor Yuen, Jiyan Yang, Jongsoo Park, Alexander Heinecke, Evangelos Georganas, Sudar-\nshan Srinivasan, Abhisek Kundu, Misha Smelyanskiy, Bharat Kaul, and Pradeep Dubey.\nA study of bfloat16 for deep learning training, 2019.\nJared Kaplan, Sam McCandlish, Tom Henighan, Tom B Brown, Benjamin Chess, Rewon\nChild, Scott Gray, Alec Radford, Jeffrey Wu, and Dario Amodei. Scaling laws for neural\nlanguage models. arXiv preprint arXiv:2001.08361, 2020.\nBoseop Kim, HyoungSeok Kim, Sang-Woo Lee, Gichang Lee, Donghyun Kwak, Jeon\nDong Hyeon, Sunghyun Park, Sungju Kim, Seonhoon Kim, Dongpil Seo, Heungsub\nLee, Minyoung Jeong, Sungjae Lee, Minsub Kim, Suk Hyun Ko, Seokhun Kim, Taeyong\nPark, Jinuk Kim, Soyoung Kang, Na-Hyeon Ryu, Kang Min Yoo, Minsuk Chang, Soobin\nSuh, Sookyo In, Jinseong Park, Kyungduk Kim, Hiun Kim, Jisu Jeong, Yong Goo Yeo,\nDonghoon Ham, Dongju Park, Min Young Lee, Jaewook Kang, Inho Kang, Jung-Woo\nHa, Woomyoung Park, and Nako Sung. What changes can large-scale language mod-\nels bring? intensive study on HyperCLOVA: Billions-scale korean generative pretrained\ntransformers. In Conference on Empirical Methods in Natural Language Processing, 2021.\nWalter Kl¨opffer. Life cycle assessment. Environmental Science and Pollution Research, 4\n(4):223–228, 1997.\nTaku Kudo and John Richardson. SentencePiece: A simple and language independent sub-\nword tokenizer and detokenizer for neural text processing. In Proceedings of the 2018\nConference on Empirical Methods in Natural Language Processing: System Demonstra-\ntions, pages 66–71, Brussels, Belgium, November 2018. Association for Computational\nLinguistics. doi: 10.18653/v1/D18-2012. URL https://aclanthology.org/D18-2012.\n53\n\nBigScience Workshop\nAnoop Kunchukuttan, Divyanshu Kakwani, Satish Golla, C. GokulN., Avik Bhattacharyya,\nMitesh M. Khapra, and Pratyush Kumar. Ai4bharat-indicnlp corpus: Monolingual cor-\npora and word embeddings for indic languages. ArXiv, abs/2005.00085, 2020.\nAlexandre Lacoste, Alexandra Luccioni, Victor Schmidt, and Thomas Dandres. Quantifying\nthe carbon emissions of machine learning. arXiv preprint arXiv:1910.09700, 2019.\nFaisal Ladhak, Esin Durmus, Claire Cardie, and Kathleen McKeown. WikiLingua: A new\nbenchmark dataset for cross-lingual abstractive summarization. In Findings of the Associ-\nation for Computational Linguistics: EMNLP 2020, pages 4034–4048, Online, November\n2020. Association for Computational Linguistics. doi: 10.18653/v1/2020.findings-emnlp.\n360. URL https://aclanthology.org/2020.findings-emnlp.360.\nHugo Lauren¸con, Lucile Saulnier, Thomas Wang, Christopher Akiki, Albert Villanova del\nMoral, Teven Le Scao, Leandro Von Werra, Chenghao Mou, Eduardo González Ponfer-\nrada, Huu Nguyen, J¨org Frohberg, Mario ˇSaˇsko, Quentin Lhoest, Angelina McMillan-\nMajor, Gérard Dupont, Stella Biderman, Anna Rogers, Loubna Ben allal, Francesco De\nToni, Giada Pistilli, Olivier Nguyen, Somaieh Nikpoor, Maraim Masoud, Pierre Colombo,\nJavier de la Rosa, Paulo Villegas, Tristan Thrush, Shayne Longpre, Sebastian Nagel,\nLeon Weber, Manuel Romero Mu˜noz, Jian Zhu, Daniel Van Strien, Zaid Alyafeai,\nKhalid Almubarak, Vu Minh Chien, Itziar Gonzalez-Dios, Aitor Soroa, Kyle Lo, Manan\nDey, Pedro Ortiz Suarez, Aaron Gokaslan, Shamik Bose, David Ifeoluwa Adelani, Long\nPhan, Hieu Tran, Ian Yu, Suhas Pai, Jenny Chim, Violette Lepercq, Suzana Ilic, Mar-\ngaret Mitchell, Sasha Luccioni, and Yacine Jernite.\nThe BigScience ROOTS corpus:\nA 1.6TB composite multilingual dataset.\nIn Thirty-sixth Conference on Neural In-\nformation Processing Systems Datasets and Benchmarks Track, 2022.\nURL https:\n//openreview.net/forum?id=UoEw6KigkUn.\nTeven Le Scao, Thomas Wang, Daniel Hesslow, Lucile Saulnier, Stas Bekman, M Saiful\nBari, Stella Biderman, Hady Elsahar, Niklas Muennighoff, Jason Phang, Ofir Press,\nColin Raffel, Victor Sanh, Sheng Shen, Lintang Sutawika, Jaesung Tae, Zheng Xin Yong,\nJulien Launay, and Iz Beltagy. What language model to train if you have one million\nGPU hours?\nIn Challenges & Perspectives in Creating Large Language Models, 2022.\nURL https://openreview.net/forum?id=rI7BL3fHIZq.\nMike Lewis, Yinhan Liu, Naman Goyal, Marjan Ghazvininejad, Abdelrahman Mohamed,\nOmer Levy, Veselin Stoyanov, and Luke Zettlemoyer.\nBART: Denoising sequence-to-\nsequence pre-training for natural language generation, translation, and comprehension.\nIn Annual Meeting of the Association for Computational Linguistics, 2020.\nQuentin Lhoest, Albert Villanova del Moral, Yacine Jernite, Abhishek Thakur, Patrick\nvon Platen, Suraj Patil, Julien Chaumond, Mariama Drame, Julien Plu, Lewis Tunstall,\nJoe Davison, Mario ˇSaˇsko, Gunjan Chhablani, Bhavitvya Malik, Simon Brandeis, Teven\nLe Scao, Victor Sanh, Canwen Xu, Nicolas Patry, Angelina McMillan-Major, Philipp\nSchmid, Sylvain Gugger, Clément Delangue, Théo Matussière, Lysandre Debut, Stas Bek-\nman, Pierric Cistac, Thibault Goehringer, Victor Mustar, Fran¸cois Lagunas, Alexander\nRush, and Thomas Wolf. Datasets: A community library for natural language process-\ning. In Proceedings of the 2021 Conference on Empirical Methods in Natural Language\n54\n\nBLOOM\nProcessing: System Demonstrations, pages 175–184, Online and Punta Cana, Dominican\nRepublic, November 2021. Association for Computational Linguistics. doi: 10.18653/v1/\n2021.emnlp-demo.21. URL https://aclanthology.org/2021.emnlp-demo.21.\nYujia Li, David H. Choi, Junyoung Chung, Nate Kushman, Julian Schrittwieser, Rémi\nLeblond, Tom Eccles, James Keeling, Felix Gimeno, Agustin Dal Lago, Thomas Hu-\nbert, Peter Choy, Cyprien de Masson d’Autume, Igor Babuschkin, Xinyun Chen, Po-\nSen Huang, Johannes Welbl, Sven Gowal, Alexey Cherepanov, James Molloy, Daniel J.\nMankowitz, Esme Sutherland Robson, Pushmeet Kohli, Nando de Freitas, Koray\nKavukcuoglu, and Oriol Vinyals. Competition-level code generation with AlphaCode.\nCoRR, abs/2203.07814, 2022. doi: 10.48550/arXiv.2203.07814. URL https://doi.org/\n10.48550/arXiv.2203.07814.\nPercy Liang, Rishi Bommasani, Tony Lee, Dimitris Tsipras, Dilara Soylu, Michihiro Ya-\nsunaga, Yian Zhang, Deepak Narayanan, Yuhuai Wu, Ananya Kumar, Benjamin New-\nman, Binhang Yuan, Bobby Yan, Ce Zhang, Christian Cosgrove, Christopher D. Man-\nning, Christopher Ré, Diana Acosta-Navas, Drew A. Hudson, Eric Zelikman, Esin Dur-\nmus, Faisal Ladhak, Frieda Rong, Hongyu Ren, Huaxiu Yao, Jue Wang, Keshav San-\nthanam, Laurel Orr, Lucia Zheng, Mert Yuksekgonul, Mirac Suzgun, Nathan Kim, Neel\nGuha, Niladri Chatterji, Omar Khattab, Peter Henderson, Qian Huang, Ryan Chi,\nSang Michael Xie, Shibani Santurkar, Surya Ganguli, Tatsunori Hashimoto, Thomas\nIcard, Tianyi Zhang, Vishrav Chaudhary, William Wang, Xuechen Li, Yifan Mai,\nYuhui Zhang, and Yuta Koreeda. Holistic evaluation of language models, 2022. URL\nhttps://arxiv.org/abs/2211.09110.\nChin-Yew Lin. ROUGE: A package for automatic evaluation of summaries. In Text Sum-\nmarization Branches Out, pages 74–81, Barcelona, Spain, July 2004. Association for\nComputational Linguistics. URL https://aclanthology.org/W04-1013.\nXi Victoria Lin, Todor Mihaylov, Mikel Artetxe, Tianlu Wang, Shuohui Chen, Daniel Simig,\nMyle Ott, Naman Goyal, Shruti Bhosale, Jingfei Du, Ramakanth Pasunuru, Sam Shleifer,\nPunit Singh Koura, Vishrav Chaudhary, Brian O’Horo, Jeff Wang, Luke Zettlemoyer,\nZornitsa Kozareva, Mona Diab, Veselin Stoyanov, and Xian Li. Few-shot learning with\nmultilingual language models, 2021. URL https://arxiv.org/abs/2112.10668.\nYinhan Liu, Myle Ott, Naman Goyal, Jingfei Du, Mandar Joshi, Danqi Chen, Omer Levy,\nMike Lewis, Luke Zettlemoyer, and Veselin Stoyanov. RoBERTa: A robustly optimized\nBERT pretraining approach. arXiv preprint arXiv:1907.11692, 2019.\nKyle Lo, Lucy Lu Wang, Mark Neumann, Rodney Michael Kinney, and Daniel S. Weld.\nS2ORC: The semantic scholar open research corpus. In ACL, 2020.\nIlya Loshchilov and Frank Hutter. SGDR: stochastic gradient descent with restarts. CoRR,\nabs/1608.03983, 2016. URL http://arxiv.org/abs/1608.03983.\nAlexandra Sasha Luccioni, Sylvain Viguier, and Anne-Laure Ligozat.\nEstimating the\nCarbon Footprint of BLOOM, a 176B Parameter Language Model.\narXiv preprint\narXiv:2211.02001, 2022.\n55\n\nBigScience Workshop\nHarish Tayyar Madabushi, Edward Gow-Smith, Marcos Garcia, Carolina Scarton, Marco\nIdiart, and Aline Villavicencio. Semeval-2022 task 2: Multilingual idiomaticity detection\nand sentence embedding. arXiv preprint arXiv:2204.10050, 2022.\nH Mann and D Whitney. Controlling the false discovery rate: A practical and powerful\napproach to multiple testing. Ann. Math. Stat, 18(1):50–60, 1947.\nLouis Martin, Benjamin Muller, Pedro Javier Ortiz Suárez, Yoann Dupont, Laurent Ro-\nmary, Éric de la Clergerie, Djamé Seddah, and Benoˆıt Sagot. CamemBERT: a tasty\nFrench language model. In Proceedings of the 58th Annual Meeting of the Association for\nComputational Linguistics, pages 7203–7219, Online, July 2020. Association for Compu-\ntational Linguistics. URL https://www.aclweb.org/anthology/2020.acl-main.645.\nAngelina McMillan-Major, Zaid Alyafeai, Stella Biderman, Kimbo Chen, Francesco De Toni,\nGérard Dupont, Hady Elsahar, Chris Emezue, Alham Fikri Aji, Suzana Ili´c, Nurulaqilla\nKhamis, Colin Leong, Maraim Masoud, Aitor Soroa, Pedro Ortiz Suarez, Zeerak Talat,\nDaniel van Strien, and Yacine Jernite.\nDocumenting geographically and contextually\ndiverse data sources: The bigscience catalogue of language data and resources, 2022.\nURL https://arxiv.org/abs/2201.10066.\nPaulius Micikevicius, Sharan Narang, Jonah Alben, Gregory Diamos, Erich Elsen, David\nGarcia, Boris Ginsburg, Michael Houston, Oleksii Kuchaiev, Ganesh Venkatesh, and Hao\nWu. Mixed precision training. In International Conference on Learning Representations,\n2018. URL https://openreview.net/forum?id=r1gs9JgRZ.\nSabrina J. Mielke, Zaid Alyafeai, Elizabeth Salesky, Colin Raffel, Manan Dey, Matthias\nGallé, Arun Raja, Chenglei Si, Wilson Y. Lee, Benoˆıt Sagot, and Samson Tan. Between\nwords and characters: A brief history of open-vocabulary modeling and tokenization in\nnlp, 2021. URL https://arxiv.org/abs/2112.10508.\nRisto Miikkulainen and Michael G. Dyer. Natural language processing with modular pdp\nnetworks and distributed lexicon. Cognitive Science, 15(3), 1991.\nTomas Mikolov, Martin Karafiát, Lukas Burget, Jan Cernockỳ, and Sanjeev Khudanpur.\nRecurrent neural network based language model. In Interspeech, 2010.\nTomas Mikolov, Ilya Sutskever, Kai Chen, Greg S. Corrado, and Jeff Dean. Distributed\nrepresentations of words and phrases and their compositionality.\nAdvances in neural\ninformation processing systems, 26, 2013.\nMargaret Mitchell, Simone Wu, Andrew Zaldivar, Parker Barnes, Lucy Vasserman, Ben\nHutchinson, Elena Spitzer, Inioluwa Deborah Raji, and Timnit Gebru.\nModel cards\nfor model reporting. In Proceedings of the Conference on Fairness, Accountability, and\nTransparency, FAT* ’19, page 220–229, New York, NY, USA, 2019. Association for\nComputing Machinery.\nISBN 9781450361255.\ndoi: 10.1145/3287560.3287596.\nURL\nhttps://doi.org/10.1145/3287560.3287596.\nAnthony Moi, Pierric Cistac, Nicolas Patry, Evan P. Walsh, Funtowicz Morgan, Sebastian\nP¨utz, Thomas Wolf, Sylvain Gugger, Clément Delangue, Julien Chaumond, Lysandre\n56\n\nBLOOM\nDebut, and Patrick von Platen. Hugging face tokenizers library. https://github.com/\nhuggingface/tokenizers, 2019.\nNasrin Mostafazadeh, Michael Roth, Annie Louis, Nathanael Chambers, and James Allen.\nLsdsem 2017 shared task: The story cloze test. In Proceedings of the 2nd Workshop on\nLinking Models of Lexical, Sentential and Discourse-level Semantics, pages 46–51, 2017.\nNiklas Muennighoff. SGPT: GPT sentence embeddings for semantic search. arXiv preprint\narXiv:2202.08904, 2022.\nNiklas Muennighoff, Nouamane Tazi, Lo¨ıc Magne, and Nils Reimers. MTEB: Massive text\nembedding benchmark. arXiv preprint arXiv:2210.07316, 2022a.\nNiklas Muennighoff, Thomas Wang, Lintang Sutawika, Adam Roberts, Stella Bider-\nman, Teven Le Scao, M Saiful Bari, Sheng Shen, Zheng-Xin Yong, Hailey Schoelkopf,\net al.\nCrosslingual generalization through multitask finetuning.\narXiv preprint\narXiv:2211.01786, 2022b.\nNikita Nangia, Clara Vania, Rasika Bhalerao, and Samuel R. Bowman.\nCrowS-pairs:\nA challenge dataset for measuring social biases in masked language models.\nIn Pro-\nceedings of the 2020 Conference on Empirical Methods in Natural Language Processing\n(EMNLP), pages 1953–1967, Online, November 2020. Association for Computational Lin-\nguistics.\ndoi: 10.18653/v1/2020.emnlp-main.154.\nURL https://aclanthology.org/\n2020.emnlp-main.154.\nSharan Narang, Hyung Won Chung, Yi Tay, William Fedus, Thibault Fevry, Michael\nMatena, Karishma Malkan, Noah Fiedel, Noam Shazeer, Zhenzhong Lan, Yanqi Zhou,\nWei Li, Nan Ding, Jake Marcus, Adam Roberts, and Colin Raffel. Do transformer mod-\nifications transfer across implementations and applications? In Conference on Empirical\nMethods in Natural Language Processing, 2021.\nDeepak Narayanan, Mohammad Shoeybi, Jared Casper, Patrick LeGresley, Mostofa Pat-\nwary, Vijay Korthikanti, Dmitri Vainbrand, Prethvi Kashinkunti, Julie Bernauer, Bryan\nCatanzaro, Amar Phanishayee, and Matei Zaharia. Efficient Large-Scale Language Model\nTraining on GPU Clusters using Megatron-LM. In Proceedings of the International Con-\nference for High Performance Computing, Networking, Storage and Analysis, 2021.\nWilhelmina Nekoto, Vukosi Marivate, Tshinondiwa Matsila, Timi E. Fasubaa, T Kolawole,\nTaiwo Helen Fagbohungbe, Solomon Oluwole Akinola, Shamsuddeen Hassan Muham-\nmad, Salomon Kabongo Kabenamualu, Salomey Osei, Sackey Freshia, Rubungo Andre\nNiyongabo, Ricky Macharm, Perez Ogayo, Orevaoghene Ahia, Musie Meressa, Mofe-\ntoluwa Adeyemi, Masabata Mokgesi-Selinga, Lawrence Okegbemi, Laura Martinus, Ko-\nlawole Tajudeen, Kevin Degila, Kelechi Ogueji, Kathleen Siminyu, Julia Kreutzer, Jason\nWebster, Jamiil Toure Ali, Jade Z. Abbott, Iroro Orife, Ignatius U. Ezeani, Idris Ab-\ndulkabir Dangana, Herman Kamper, Hady ElSahar, Goodness Duru, Ghollah Kioko, Es-\npoir Murhabazi, Elan Van Biljon, Daniel Whitenack, Christopher Onyefuluchi, Chris C.\nEmezue, Bonaventure F. P. Dossou, Blessing K. Sibanda, Blessing Itoro Bassey, Ayo-\ndele Olabiyi, Arshath Ramkilowan, Alp Oktem, Adewale Akinfaderin, and Abdallah M.\n57\n\nBigScience Workshop\nBashir. Participatory research for low-resourced machine translation: A case study in\nAfrican languages. In ACL Findings, 2020.\nAurélie Névéol, Yoann Dupont, Julien Bezan¸con, and Kar¨en Fort. French CrowS-pairs:\nExtending a challenge dataset for measuring social bias in masked language mod-\nels to a language other than English.\nIn Proceedings of the 60th Annual Meeting of\nthe Association for Computational Linguistics (Volume 1: Long Papers), pages 8521–\n8531, Dublin, Ireland, May 2022. Association for Computational Linguistics.\ndoi:\n10.18653/v1/2022.acl-long.583. URL https://aclanthology.org/2022.acl-long.583.\nJoakim Nivre, Marie-Catherine de Marneffe, Filip Ginter, Yoav Goldberg, Jan Hajiˇc,\nChristopher D. Manning, Ryan McDonald, Slav Petrov, Sampo Pyysalo, Natalia Silveira,\nReut Tsarfaty, and Daniel Zeman. Universal Dependencies v1: A multilingual treebank\ncollection. In Proceedings of the Tenth International Conference on Language Resources\nand Evaluation (LREC’16), pages 1659–1666, Portoroˇz, Slovenia, May 2016. European\nLanguage Resources Association (ELRA). URL https://aclanthology.org/L16-1262.\nJoakim Nivre, Daniel Zeman, Filip Ginter, and Francis Tyers. Universal Dependencies.\nIn Proceedings of the 15th Conference of the European Chapter of the Association for\nComputational Linguistics: Tutorial Abstracts, Valencia, Spain, April 2017. Association\nfor Computational Linguistics. URL https://aclanthology.org/E17-5001.\nPedro Javier Ortiz Suárez, Benoˆıt Sagot, and Laurent Romary. Asynchronous pipelines\nfor processing huge corpora on medium to low resource infrastructures.\nIn Piotr\nBa´nski, Adrien Barbaresi, Hanno Biber, Evelyn Breiteneder, Simon Clematide, Marc\nKupietz, Harald L¨ungen, and Caroline Iliadi, editors, Proceedings of the Workshop on\nChallenges in the Management of Large Corpora (CMLC-7), pages 9 – 16, Cardiff,\nUK, 2019. Leibniz-Institut f¨ur Deutsche Sprache.\ndoi: 10.14618/ids-pub-9021.\nURL\nhttp://nbn-resolving.de/urn:nbn:de:bsz:mh39-90215.\nKishore Papineni, Salim Roukos, Todd Ward, and Wei-Jing Zhu.\nBLEU: a method\nfor automatic evaluation of machine translation.\nIn Proceedings of the 40th Annual\nMeeting of the Association for Computational Linguistics, pages 311–318, Philadel-\nphia, Pennsylvania, USA, July 2002. Association for Computational Linguistics.\ndoi:\n10.3115/1073083.1073135. URL https://aclanthology.org/P02-1040.\nDavid Patterson, Joseph Gonzalez, Quoc Le, Chen Liang, Lluis-Miquel Munguia, Daniel\nRothchild, David So, Maud Texier, and Jeff Dean. Carbon emissions and large neural\nnetwork training. arXiv preprint arXiv:2104.10350, 2021.\nKarl Pearson. Note on regression and inheritance in the case of two parents. Proceedings\nof the Royal Society of London, 58(347-352):240–242, 1895.\nMatthew E. Peters, Mark Neumann, Mohit Iyyer, Matt Gardner, Christopher Clark, Kenton\nLee, and Luke Zettlemoyer. Deep contextualized word representations. In Conference of\nthe North American Chapter of the Association for Computational Linguistics, 2018.\n58\n\nBLOOM\nJason Phang, Herbie Bradley, Leo Gao, Louis J Castricato, and Stella Biderman.\nEleutherAI: going beyond \"open science\" to \"science in the open\".\nIn Workshop on\nBroadening Research Collaborations, 2022.\nMatt Post. A call for clarity in reporting BLEU scores. In Proceedings of the Third Con-\nference on Machine Translation: Research Papers, pages 186–191, Brussels, Belgium,\nOctober 2018. Association for Computational Linguistics. doi: 10.18653/v1/W18-6319.\nURL https://aclanthology.org/W18-6319.\nOfir Press, Noah Smith, and Mike Lewis. Train short, test long: Attention with linear\nbiases enables input length extrapolation. In International Conference on Learning Rep-\nresentations, 2021.\nAlec Radford, Karthik Narasimhan, Tim Salimans, and Ilya Sutskever. Improving language\nunderstanding by generative pre-training, 2018.\nAlec Radford, Jeffrey Wu, Rewon Child, David Luan, Dario Amodei, and Ilya Sutskever.\nLanguage models are unsupervised multitask learners, 2019.\nJack W Rae, Sebastian Borgeaud, Trevor Cai, Katie Millican, Jordan Hoffmann, Francis\nSong, John Aslanides, Sarah Henderson, Roman Ring, Susannah Young, et al. Scaling\nlanguage models: Methods, analysis & insights from training gopher.\narXiv preprint\narXiv:2112.11446, 2021.\nColin Raffel, Noam Shazeer, Adam Roberts, Katherine Lee, Sharan Narang, Michael\nMatena, Yanqi Zhou, Wei Li, Peter J Liu, et al. Exploring the limits of transfer learning\nwith a unified text-to-text transformer. J. Mach. Learn. Res., 21(140):1–67, 2020.\nSamyam Rajbhandari, Jeff Rasley, Olatunji Ruwase, and Yuxiong He.\nZeRO: Memory\noptimizations toward training trillion parameter models. SC20: International Conference\nfor High Performance Computing, Networking, Storage and Analysis, Nov 2020.\ndoi:\n10.1109/sc41405.2020.00024. URL http://dx.doi.org/10.1109/SC41405.2020.00024.\nDeborah\nRaji,\nEmily\nDenton,\nEmily\nM.\nBender,\nAlex\nHanna,\nand\nAmanda-\nlynne\nPaullada.\nAi\nand\nthe\neverything\nin\nthe\nwhole\nwide\nworld\nbench-\nmark.\nIn\nJ.\nVanschoren\nand\nS.\nYeung,\neditors,\nProceedings\nof\nthe\nNeural\nInformation Processing Systems Track on Datasets and Benchmarks,\nvolume 1,\n2021.\nURL https://datasets-benchmarks-proceedings.neurips.cc/paper/2021/\nfile/084b6fbb10729ed4da8c3d3f5a3ae7c9-Paper-round2.pdf.\nInioluwa Deborah Raji, I. Elizabeth Kumar, Aaron Horowitz, and Andrew Selbst. The\nfallacy of AI functionality. In 2022 ACM Conference on Fairness, Accountability, and\nTransparency, FAccT ’22, page 959–972, New York, NY, USA, 2022. Association for\nComputing Machinery.\nISBN 9781450393522.\ndoi: 10.1145/3531146.3533158.\nURL\nhttps://doi.org/10.1145/3531146.3533158.\nJeff Rasley, Samyam Rajbhandari, Olatunji Ruwase, and Yuxiong He. DeepSpeed: System\noptimizations enable training deep learning models with over 100 billion parameters. In\nProceedings of the 26th ACM SIGKDD International Conference on Knowledge Discovery\n59\n\nBigScience Workshop\n& Data Mining, KDD ’20, page 3505–3506, New York, NY, USA, 2020. Association\nfor Computing Machinery. ISBN 9781450379984. doi: 10.1145/3394486.3406703. URL\nhttps://doi.org/10.1145/3394486.3406703.\nPhillip Rust, Jonas Pfeiffer, Ivan Vuli´c, Sebastian Ruder, and Iryna Gurevych.\nHow\ngood is your tokenizer?\non the monolingual performance of multilingual language\nmodels.\nIn Proceedings of the 59th Annual Meeting of the Association for Compu-\ntational Linguistics and the 11th International Joint Conference on Natural Language\nProcessing (Volume 1: Long Papers), pages 3118–3135, Online, August 2021. Associa-\ntion for Computational Linguistics. doi: 10.18653/v1/2021.acl-long.243. URL https:\n//aclanthology.org/2021.acl-long.243.\nAli Safaya, Moutasem Abdullatif, and Deniz Yuret.\nKUISAIL at SemEval-2020 task\n12: BERT-CNN for offensive speech identification in social media.\nIn Proceedings of\nthe Fourteenth Workshop on Semantic Evaluation, pages 2054–2059, Barcelona (on-\nline), December 2020. International Committee for Computational Linguistics.\nURL\nhttps://www.aclweb.org/anthology/2020.semeval-1.271.\nGerard Salton and Chung-Shu Yang.\nOn the specification of term values in automatic\nindexing. Journal of documentation, 1973.\nNithya Sambasivan, Shivani Kapania, Hannah Highfill, Diana Akrong, Praveen Paritosh,\nand Lora M Aroyo. “everyone wants to do the model work, not the data work”: Data\ncascades in high-stakes ai. In Proceedings of the 2021 CHI Conference on Human Factors\nin Computing Systems, CHI ’21, New York, NY, USA, 2021. Association for Computing\nMachinery. ISBN 9781450380966. doi: 10.1145/3411764.3445518. URL https://doi.\norg/10.1145/3411764.3445518.\nVictor Sanh, Albert Webson, Colin Raffel, Stephen Bach, Lintang Sutawika, Zaid Alyafeai,\nAntoine Chaffin, Arnaud Stiegler, Arun Raja, Manan Dey, M Saiful Bari, Canwen Xu,\nUrmish Thakker, Shanya Sharma Sharma, Eliza Szczechla, Taewoon Kim, Gunjan Chh-\nablani, Nihal Nayak, Debajyoti Datta, Jonathan Chang, Mike Tian-Jian Jiang, Han\nWang, Matteo Manica, Sheng Shen, Zheng Xin Yong, Harshit Pandey, Rachel Baw-\nden, Thomas Wang, Trishala Neeraj, Jos Rozen, Abheesht Sharma, Andrea Santilli,\nThibault Fevry, Jason Alan Fries, Ryan Teehan, Teven Le Scao, Stella Biderman, Leo\nGao, Thomas Wolf, and Alexander M Rush. Multitask prompted training enables zero-\nshot task generalization. In International Conference on Learning Representations, 2022.\nURL https://openreview.net/forum?id=9Vrb9D0WI4.\nJ¨urgen Schmidhuber and Stefan Heil. Sequential neural text compression. IEEE Transac-\ntions on Neural Networks, 7(1), 1996.\nRoy Schwartz, Jesse Dodge, Noah A. Smith, and Oren Etzioni. Green ai. Communications\nof the ACM, 63(12), 2020.\nOleg Serikov, Vitaly Protasov, Ekaterina Voloshina, Viktoria Knyazkova, and Tatiana Shav-\nrina. Universal and independent: Multilingual probing framework for exhaustive model\ninterpretation and evaluation. arXiv preprint arXiv:2210.13236, 2022.\n60\n\nBLOOM\nClaude Elwood Shannon.\nA mathematical theory of communication.\nThe Bell system\ntechnical journal, 27(3), 1948.\nNoam Shazeer. GLU variants improve transformer. arXiv preprint arXiv:2002.05202, 2020.\nNoam Shazeer, Azalia Mirhoseini, Krzysztof Maziarz, Andy Davis, Quoc Le, Geoffrey Hin-\nton, and Jeff Dean. Outrageously large neural networks: The sparsely-gated mixture-\nof-experts layer. In International Conference on Learning Representations, 2017. URL\nhttps://openreview.net/forum?id=B1ckMDqlg.\nOleh Shliazhko, Alena Fenogenova, Maria Tikhonova, Vladislav Mikhailov, Anastasia Ko-\nzlova, and Tatiana Shavrina. mgpt: Few-shot learners go multilingual. arXiv preprint\narXiv:2204.07580, 2022.\nMohammad Shoeybi, Mostofa Patwary, Raul Puri, Patrick LeGresley, Jared Casper, and\nBryan Catanzaro. Megatron-LM: Training multi-billion parameter language models using\nmodel parallelism. arXiv preprint arXiv:1909.08053, 2019.\nAntoine Simoulin and Benoit Crabbé.\nUn modèle Transformer Génératif Pré-entrainé\npour le ______ fran¸cais. In Pascal Denis, Natalia Grabar, Amel Fraisse, Rémi Car-\ndon, Bernard Jacquemin, Eric Kergosien, and Antonio Balvet, editors, Traitement Au-\ntomatique des Langues Naturelles, pages 246–255, Lille, France, 2021. ATALA.\nURL\nhttps://hal.archives-ouvertes.fr/hal-03265900.\nShaden Smith, Mostofa Patwary, Brandon Norick, Patrick LeGresley, Samyam Rajbhandari,\nJared Casper, Zhun Liu, Shrimai Prabhumoye, George Zerveas, Vijay Korthikanti, Elton\nZhang, Rewon Child, Reza Yazdani Aminabadi, Julie Bernauer, Xia Song, Mohammad\nShoeybi, Yuxiong He, Michael Houston, Saurabh Tiwary, and Bryan Catanzaro. Using\nDeepSpeed and Megatron to train Megatron-Turing NLG 530B, a large-scale generative\nlanguage model. arXiv preprint arXiv:2201.11990, 2022.\nSaleh Soltan, Shankar Ananthakrishnan, Jack FitzGerald, Rahul Gupta, Wael Hamza,\nHaidar Khan, Charith Peris, Stephen Rawls, Andy Rosenbaum, Anna Rumshisky, Chan-\ndana Satya Prakash, Mukund Sridhar, Fabian Triefenbach, Apurv Verma, Gokhan Tur,\nand Prem Natarajan. Alexatm 20b: Few-shot learning using a large-scale multilingual\nseq2seq model, 2022. URL https://arxiv.org/abs/2208.01448.\nAarohi Srivastava, Abhinav Rastogi, Abhishek Rao, Abu Awal Md Shoeb, Abubakar Abid,\nAdam Fisch, Adam R Brown, Adam Santoro, Aditya Gupta, Adrià Garriga-Alonso, et al.\nBeyond the imitation game: Quantifying and extrapolating the capabilities of language\nmodels. arXiv preprint arXiv:2206.04615, 2022.\nEmma Strubell, Ananya Ganesh, and Andrew McCallum. Energy and policy considera-\ntions for deep learning in nlp. In Annual Meeting of the Association for Computational\nLinguistics, 2019.\nJianlin Su, Yu Lu, Shengfeng Pan, Bo Wen, and Yunfeng Liu.\nRoFormer: Enhanced\ntransformer with rotary position embedding. arXiv preprint arXiv:2104.09864, 2021.\n61\n\nBigScience Workshop\nIlya Sutskever, James Martens, and Geoffrey E. Hinton. Generating text with recurrent\nneural networks. In International Conference on Machine Learning, 2011.\nZeerak Talat, Aurélie Névéol, Stella Biderman, Miruna Clinciu, Manan Dey, Shayne Long-\npre, Sasha Luccioni, Maraim Masoud, Margaret Mitchell, Dragomir Radev, Shanya\nSharma, Arjun Subramonian, Jaesung Tae, Samson Tan, Deepak Tunuguntla, and Oskar\nvan der Wal. You reap what you sow: On the challenges of bias evaluation under multi-\nlingual settings. In Challenges & Perspectives in Creating Large Language Models, 2022.\nURL https://openreview.net/forum?id=rK-7NhfSIW5.\nYi Tay, Jason Wei, Hyung Won Chung, Vinh Q Tran, David R So, Siamak Shakeri, Xavier\nGarcia, Huaixiu Steven Zheng, Jinfeng Rao, Aakanksha Chowdhery, et al. Transcending\nscaling laws with 0.1% extra compute. arXiv preprint arXiv:2210.11399, 2022.\nRyan Teehan, Miruna Clinciu, Oleg Serikov, Eliza Szczechla, Natasha Seelam, Shachar\nMirkin, and Aaron Gokaslan. Emergent structures and training dynamics in large lan-\nguage models. In Proceedings of BigScience Episode #5 – Workshop on Challenges &\nPerspectives in Creating Large Language Models, pages 146–159, virtual+Dublin, May\n2022. Association for Computational Linguistics. doi: 10.18653/v1/2022.bigscience-1.11.\nURL https://aclanthology.org/2022.bigscience-1.11.\nIan Tenney, Patrick Xia, Berlin Chen, Alex Wang, Adam Poliak, R Thomas McCoy, Na-\njoung Kim, Benjamin Van Durme, Samuel R Bowman, Dipanjan Das, et al. What do\nyou learn from context? probing for sentence structure in contextualized word represen-\ntations. In International Conference on Learning Representations, 2018.\nAshish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N\nGomez,  Lukasz Kaiser, and Illia Polosukhin.\nAttention is all you need.\nAdvances in\nneural information processing systems, 30, 2017.\nOriol Vinyals and Quoc V. Le.\nA neural conversational model.\narXiv preprint\narXiv:1506.05869, 2015.\nEkaterina Voloshina, Oleg Serikov, and Tatiana Shavrina. Is neural language acquisition\nsimilar to natural? a chronological probing study. arXiv preprint arXiv:2207.00560, 2022.\nAlex Wang, Yada Pruksachatkun, Nikita Nangia, Amanpreet Singh, Julian Michael, Felix\nHill, Omer Levy, and Samuel Bowman. Superglue: A stickier benchmark for general-\npurpose language understanding systems. In H. Wallach, H. Larochelle, A. Beygelzimer,\nF. d'Alché-Buc, E. Fox, and R. Garnett, editors, Advances in Neural Information Pro-\ncessing Systems, volume 32. Curran Associates, Inc., 2019. URL https://proceedings.\nneurips.cc/paper/2019/file/4496bf24afe7fab6f046bf4923da8de6-Paper.pdf.\nBen Wang and Aran Komatsuzaki. GPT-J-6B: A 6 billion parameter autoregressive lan-\nguage model, 2021.\nChanghan Wang, Kyunghyun Cho, and Jiatao Gu. Neural machine translation with byte-\nlevel subwords. In Proceedings of the AAAI Conference on Artificial Intelligence, 2020.\n62\n\nBLOOM\nShibo Wang and Pankaj Kanwar.\nBfloat16: The secret to high performance on cloud\ntpus, 2019. URL https://cloud.google.com/blog/products/ai-machine-learning/\nbfloat16-the-secret-to-high-performance-on-cloud-tpus.\nShuohuan Wang, Yu Sun, Yang Xiang, Zhihua Wu, Siyu Ding, Weibao Gong, Shikun Feng,\nJunyuan Shang, Yanbin Zhao, Chao Pang, Jiaxiang Liu, Xuyi Chen, Yuxiang Lu, Weixin\nLiu, Xi Wang, Yangfan Bai, Qiuliang Chen, Li Zhao, Shiyong Li, Peng Sun, Dianhai Yu,\nYanjun Ma, Hao Tian, Hua Wu, Tian Wu, Wei Zeng, Ge Li, Wen Gao, and Haifeng Wang.\nErnie 3.0 titan: Exploring larger-scale knowledge enhanced pre-training for language\nunderstanding and generation. arXiv preprint arXiv:2112.12731, 2021.\nThomas Wang, Adam Roberts, Daniel Hesslow, Teven Le Scao, Hyung Won Chung, Iz Belt-\nagy, Julien Launay, and Colin Raffel. What language model architecture and pretraining\nobjective works best for zero-shot generalization?\nIn Kamalika Chaudhuri, Stefanie\nJegelka, Le Song, Csaba Szepesvari, Gang Niu, and Sivan Sabato, editors, Proceed-\nings of the 39th International Conference on Machine Learning, volume 162 of Proceed-\nings of Machine Learning Research, pages 22964–22984. PMLR, 17–23 Jul 2022a. URL\nhttps://proceedings.mlr.press/v162/wang22u.html.\nYizhong Wang, Swaroop Mishra, Pegah Alipoormolabashi, Yeganeh Kordi, Amirreza\nMirzaei, Anjana Arunkumar, Arjun Ashok, Arut Selvan Dhanasekaran, Atharva Naik,\nDavid Stap, et al. Benchmarking generalization via in-context instructions on 1,600+\nlanguage tasks. arXiv preprint arXiv:2204.07705, 2022b.\nJason Wei, Maarten Bosma, Vincent Y Zhao, Kelvin Guu, Adams Wei Yu, Brian Lester,\nNan Du, Andrew M Dai, and Quoc V Le.\nFinetuned language models are zero-shot\nlearners. arXiv preprint arXiv:2109.01652, 2021.\nJason Wei, Yi Tay, Rishi Bommasani, Colin Raffel, Barret Zoph, Sebastian Borgeaud,\nDani Yogatama, Maarten Bosma, Denny Zhou, Donald Metzler, Ed H. Chi, Tatsunori\nHashimoto, Oriol Vinyals, Percy Liang, Jeff Dean, and William Fedus. Emergent abilities\nof large language models. Transactions on Machine Learning Research, 2022.\nLaura S. Westra and Bill E. Lawson. Faces of Environmental Racism: Confronting Issues\nof Global Justice. Rowman & Littlefield Publishers, 2001.\nLangdon Winner. Technology as master. (book reviews: Autonomous technology. technics-\nout-of-control as a theme in political thought). Science, 1977.\nLangdon Winner. Do artifacts have politics? In Computer Ethics, pages 177–192. Routledge,\n2017.\nAndrew Wong, Erkin Otles, John P. Donnelly, Andrew Krumm, Jeffrey McCullough, Olivia\nDeTroyer-Cooley, Justin Pestrue, Marie Phillips, Judy Konye, Carleen Penoza, Muham-\nmad Ghous, and Karandeep Singh. External Validation of a Widely Implemented Pro-\nprietary Sepsis Prediction Model in Hospitalized Patients. JAMA Internal Medicine, 181\n(8):1065–1070, 08 2021. ISSN 2168-6106. doi: 10.1001/jamainternmed.2021.2626. URL\nhttps://doi.org/10.1001/jamainternmed.2021.2626.\n63\n\nBigScience Workshop\nHaicheng Wu, Gregory Diamos, Jin Wang, Srihari Cadambi, Sudhakar Yalamanchili, and\nSrimat Chakradhar. Optimizing data warehousing applications for GPUs using kernel\nfusion/fission. In 2012 IEEE 26th International Parallel and Distributed Processing Sym-\nposium Workshops and PhD Forum, pages 2433–2442, 2012. doi: 10.1109/IPDPSW.2012.\n300.\nLinting Xue, Noah Constant, Adam Roberts, Mihir Kale, Rami Al-Rfou, Aditya Siddhant,\nAditya Barua, and Colin Raffel. mT5: A massively multilingual pre-trained text-to-text\ntransformer. In Proceedings of the 2021 Conference of the North American Chapter of the\nAssociation for Computational Linguistics: Human Language Technologies, pages 483–\n498, Online, June 2021. Association for Computational Linguistics. doi: 10.18653/v1/\n2021.naacl-main.41. URL https://aclanthology.org/2021.naacl-main.41.\nZhilin Yang, Zihang Dai, Yiming Yang, Jaime Carbonell, Ruslan Salakhutdinov, and\nQuoc V. Le. XLnet: Generalized autoregressive pretraining for language understand-\ning. Advances in Neural Information Processing Systems, 2019.\nAohan Zeng, Xiao Liu, Zhengxiao Du, Zihan Wang, Hanyu Lai, Ming Ding, Zhuoyi Yang,\nYifan Xu, Wendi Zheng, Xiao Xia, et al. Glm-130b: An open bilingual pre-trained model.\narXiv preprint arXiv:2210.02414, 2022.\nWei Zeng, Xiaozhe Ren, Teng Su, Hui Wang, Yi Liao, Zhiwei Wang, Xin Jiang, ZhenZhang\nYang, Kaisheng Wang, Xiaoda Zhang, Chen Li, Ziyan Gong, Yifan Yao, Xinjing Huang,\nJun Wang, Jianfeng Yu, Qi Guo, Yue Yu, Yan Zhang, Jin Wang, Hengtao Tao, Dasen\nYan, Zexuan Yi, Fang Peng, Fangqing Jiang, Han Zhang, Lingfeng Deng, Yehong Zhang,\nZhe Lin, Chao Zhang, Shaojie Zhang, Mingyue Guo, Shanzhi Gu, Gaojun Fan, Yaowei\nWang, Xuefeng Jin, Qun Liu, and Yonghong Tian. PanGu-α: Large-scale autoregres-\nsive pretrained Chinese language models with auto-parallel computation. arXiv preprint\narXiv:2104.12369, 2021.\nSusan Zhang, Stephen Roller, Naman Goyal, Mikel Artetxe, Moya Chen, Shuohui Chen,\nChristopher Dewan, Mona Diab, Xian Li, Xi Victoria Lin, et al. OPT: Open pre-trained\ntransformer language models. arXiv preprint arXiv:2205.01068, 2022.\nYian Zhang, Alex Warstadt, Xiaocheng Li, and Samuel R. Bowman. When do you need\nbillions of words of pretraining data? In Proceedings of the 59th Annual Meeting of the\nAssociation for Computational Linguistics and the 11th International Joint Conference on\nNatural Language Processing (Volume 1: Long Papers), pages 1112–1125, Online, August\n2021. Association for Computational Linguistics. doi: 10.18653/v1/2021.acl-long.90. URL\nhttps://aclanthology.org/2021.acl-long.90.\nZhengyan Zhang, Xu Han, Zhiyuan Liu, Xin Jiang, Maosong Sun, and Qun Liu. ERNIE:\nEnhanced language representation with informative entities. In Annual Meeting of the\nAssociation for Computational Linguistics, 2019.\n64\n\nBLOOM\nAppendix A. Prompts\nThe following contains prompts used for evaluation.\nThe prompts are also available in\nPromptSource (Bach et al., 2022). A sample with a prompt applied as well as the raw\nprompts are provided. For raw prompts, double curly brackets are filled with content from\nthe sample when used.\nContents\nA.1 SuperGLUE/wsc.fixed . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n66\nA.1.1\nData example . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n66\nA.1.2\nPrompts . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n66\nA.2 SuperGLUE/wic\n. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n66\nA.2.1\nData example . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n66\nA.2.2\nPrompts . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n67\nA.3 SuperGLUE/boolq . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n67\nA.3.1\nData example . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n67\nA.3.2\nPrompts . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n67\nA.4 SuperGLUE/axb & SuperGLUE/axg . . . . . . . . . . . . . . . . . . . . . .\n68\nA.4.1\nData example . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n68\nA.4.2\nPrompts . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n68\nA.5 XNLI & SuperGLUE/CB . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n68\nA.5.1\nData example . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n68\nA.5.2\nPrompts . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n69\nA.6 XWinograd . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n69\nA.6.1\nData example . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n69\nA.6.2\nPrompts . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n69\nA.7 XCOPA & SuperGLUE/COPA . . . . . . . . . . . . . . . . . . . . . . . . .\n70\nA.7.1\nData example . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n70\nA.7.2\nPrompts . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n70\nA.8 XStoryCloze & Story Cloze . . . . . . . . . . . . . . . . . . . . . . . . . . .\n71\nA.8.1\nData example . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n71\nA.8.2\nPrompts . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n71\nA.9 WMT . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n71\nA.9.1\nData example . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n71\nA.9.2\nPrompts . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n72\nA.10 DiaBLa\n. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n72\nA.10.1 Data example . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n72\nA.10.2 Prompt . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n72\nA.11 Flores-101 (MT)\n. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n72\nA.11.1 Data example . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n73\nA.11.2 Prompt . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n73\nA.12 CrowS-Pairs . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n73\nA.12.1 Data example . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n73\nA.12.2 Prompts . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n73\n65\n\nBigScience Workshop\nA.1 SuperGLUE/wsc.fixed\nA.1.1 Data example\nPrompt name: GPT-3 Style\nPassage: I tried to paint a picture of an orchard, with lemons in the lemon\ntrees , but they came out looking more like light bulbs.\\n\\nQuestion: In the\npassage above, does the pronoun \"they\" refer to lemon trees?\nAnswer: No\nA.1.2 Prompts\nGPT-3 Style\nPassage: {{ text }} \\n\\nQuestion: In the passage above, does the pronoun\n\"{{ span2_text }}\" refer to {{ span1_text }}?\\n\\nAnswer:\nreplaced with\n{{ text }} In the previous sentence, can the pronoun \"{{ span2_text }}\"\nbe replaced with \"{{ span1_text }}\"? Yes or no?\nthe pronoun refers to\n{{ text }} \\nIn the passage above, the pronoun \"{{ span2_text }}\" refers to\n{{ span1_text }}. True or false?\ndoes p stand for\n{{ text }} Here, does \"{{ span2_text.lower() }}\" stand for {{ span1_text }}?\nYes or no?\nthe pronoun refers to\n{{ text }} \\nIn the passage above, the pronoun \"{{ span2_text }}\" refers to\n{{ span1_text }}. True or false?\nA.2 SuperGLUE/wic\nA.2.1 Data example\nPrompt name: GPT-3 Style\nAs he called the role he put a check mark by each student’s name.\n\\n\\nA check on its dependability under stress.\\n\\nQuestion: Is the\nword ’check’ used in the same sense in the two sentences above?\n66\n\nBLOOM\nA.2.2 Prompts\nGPT-3 Style\n{{sentence1}}\\n\\n{{sentence2}}\\n\\nQuestion: Is the word ’’{{word}}’’\nused in the same sense in the two sentences above?\nquestion-context-meaning-with-label\nDoes the word \"{{word}}\" have the same meaning in these two sentences?\nYes, No?\\n\\n{{sentence1}}\\n\\n{{sentence2}}\nGPT-3-prompt-with-label\n{sentence1}}\\n\\n{{sentence2}}\\n\\nQuestion: Is the word ’’{{word}}’’ used\nin the same sense in the two sentences above? Yes, No?\npolysemous\nThe word \"{{word}}\" has multiple meanings. Does it have the same meaning in\nsentences 1 and 2? Yes or no? Sentence 1: {{sentence1}} Sentence 2:\n{{sentence2}}\nsimilar-sense\n{{sentence1}}\\n\\n{{sentence2}}\\n\\nSimilar sense of {{word}}?\nA.3 SuperGLUE/boolq\nA.3.1 Data example\nPrompt name: GPT-3 Style\nPhantom pain -- Phantom pain sensations are described as perceptions that an\nindividual experiences relating to a limb or an organ that is not physically\npart of the body. Limb loss is a result of either removal by amputation or\ncongenital limb deficiency. However, phantom limb sensations can also occur\nfollowing nerve avulsion or spinal cord injury.\\nQuestion: is pain experienced\nin a missing body part or paralyzed area\\nAnswer:\nAnswer: Yes\nA.3.2 Prompts\nGPT-3 Style\n{{ passage }} \\nQuestion: {{ question }}\\nAnswer:\nyes_no_question\nText: {{passage}}\\n\\nAnswer the following yes/no question:\n{{question}}? Yes or no?\n67\n\nBigScience Workshop\nexam\nEXAM\\n1. Answer by yes or no.\\n\\nDocument: {{passage}}\\n\nQuestion: {{question}}?\nbased on the following passage\nBased on the following passage, {{ question }}? {{ passage }}\ncould you tell me. . .\n{ passage }} \\n\\nHaving read that, could you tell me {{ question }}?\nA.4 SuperGLUE/axb & SuperGLUE/axg\nA.4.1 Data example\nPrompt name: GPT-3 style\nThe taxpayer met with the accountant to get help filing his taxes.\\n\\n\nQuestion: The accountant sought help filing taxes. True or False?\nAnswer: False\nA.4.2 Prompts\nGPT-3 style\n{{sentence1}}\\n\\nQuestion: {{sentence2}} True or False?\nMNLI Crowdsource\n{{sentence1}} Using only the above description and what you know about\nthe world, is \"{{sentence2}}\" definitely correct? Yes or no?\ncan we infer\nSuppose {{sentence1}} Can we infer that \"{{sentence2}}\"? Yes or no?\nguaranteed true\nGiven {{sentence1}} Is it guaranteed true that \"{{sentence2}}\"? Yes or no?\njustified in saying\n{{sentence1}} Are we justified in saying that \"{{sentence2}}\"? Yes or no?\nA.5 XNLI & SuperGLUE/CB\nA.5.1 Data example\nPrompt name: GPT-3 style\nWell, I wasn’t even thinking about that, but I was so frustrated, and, I ended up\ntalking to him again.\\n\\nQuestion: I havent spoken to him again. True, False, or\nNeither?\nAnswer: False\n68\n\nBLOOM\nA.5.2 Prompts\nGPT-3 style\n{{premise}}\\n\\nQuestion: {{hypothesis}} True, False, or Neither?\nMNLI crowdsource\n{{premise}} Using only the above description and what you know about the world,\n\"{{hypothesis}}\" is definitely correct, incorrect, or inconclusive?\ncan we infer\nSuppose {{premise}} Can we infer that \"{{hypothesis}}\"? Yes, no, or maybe?\nguaranteed/possible/impossible\nAssume it is true that {{premise}} \\n\\nTherefore, \\\"{{hypothesis}}\\\" is\n{{\\\"guaranteed\\\"}}, {{\\\"possible\\\"}}, or {{\\\"impossible\\\"}}?\njustified in saying\n{{premise}} Are we justified in saying that \"{{hypothesis}}\"? Yes, no,\nor maybe?\nA.6 XWinograd\nA.6.1 Data example\nPrompt name: Replace\nThe city councilmen refused the demonstrators a permit because _ feared\nviolence.\\nReplace the _ in the above sentence with the correct option:\n\\n- the demonstrators\\n- The city councilmen\nAnswer: The city councilmen\nA.6.2 Prompts\nReplace\n{{sentence}}\\nReplace the _ in the above sentence with the correct option:\n\\n- {{option1}}\\n- {{option2}}\nTrue or False\nThe _ in the sentence below refers to {{option1}}. True or False?\n{{sentence}}\ndoes underscore refer to\n{{sentence}} In the previous sentence, does _ refer to\n{{ option1 }} or {{ option2 }}?\n69\n\nBigScience Workshop\nunderscore refer to\n{{sentence}}\\n What does the _ in the above sentence refer to?\n{{ option1 }} or {{ option2 }}?\nstand for\nIn the sentence below, does the _ stand for {{answer_choices[0]}} or\n{{answer_choices[1]}}? {{sentence}}\nA.7 XCOPA & SuperGLUE/COPA\nA.7.1 Data example\nPrompt name: C1 or C2? premise, so/because...\n\"It was fragile.\" or \"It was small.\"? The item was packaged in bubble wrap.\nbecause\nAnswer: It was fragile.\nA.7.2 Prompts\nC1 or C2? premise, so/because...\n{{ answer_choices[0] }}\" or \"{{ answer_choices[1] }}\"? {{ premise }}\n{% if question == \"cause\" %} because {% else %} so {% endif %}\nbest_option\n{{ premise }} \\n\\nWhat’s the best option?\\n- {{choice1}}\\n- {{choice2}}\\n\\\n\\nWe are looking for {% if question == \\\"cause\\\" %} a cause {% else %}\nan effect {% endif %}\ncause_effect\n{{ premise }}\\nSelect the most plausible {% if question == \"cause\" %} cause:\n{% else %} effect: {% endif %}\\n- {{choice1}}\\n- {{choice2}}\ni_am_hesitating\n{{ premise }} \\n\\nI am hesitating between two options. Help me choose the\nmore\nlikely {% if question == \\\"cause\\\" %} cause: {% else %}\neffect: {% endif %}\\n- {{choice1}}\\n- {{choice2}}\nplausible_alternatives\n{{ premise }} {% if question == \"cause\" %} This happened because...\n{% else %} As a consequence... {% endif %} Help me pick the more\nplausible option:\\n- {{choice1}}\\n- {{choice2}}\n70\n\nBLOOM\nA.8 XStoryCloze & Story Cloze\nA.8.1 Data example\nXStoryCloze and Story Cloze are not publicly available datasets. Please contact the authors\nof Lin et al. (2021) for XStoryCloze and Mostafazadeh et al. (2017) for Story Cloze samples.\nA.8.2 Prompts\nAnswer Given options\n{{input_sentence_1}} {{input_sentence_2}} {{input_sentence_3}}\n{{input_sentence_4}} What is a possible continuation for the story\ngiven the following options ? - {{answer_choices | join(\"\\n- \")}}\nChoose Story Ending\nRead the following story :\\n\\n{{input_sentence_1}}\\n{{input_sentence_2}}\\n\n{{input_sentence_3}}\\n{{input_sentence_4}}\\n\\nChoose a possible ending for the\nprevious story from the following options: \\n- {{answer_choices | join(\\\"\\\\\\n- \\\")}}\nStory Continuation and Options\nWhat is a possible continuation for the following story ? \\n\\n{{input_sentence_1}}\n\\n\\{{input_sentence_2}}\\n{{input_sentence_3}}\\n{{input_sentence_4}}\\n\\nChoose from\nthe following options:\\n- {{answer_choices | join(\\\"\\\\n- \\\")}}\nGenerate Ending\nGenerate a possible ending for the following story: {{input_sentence_1}}\n{{input_sentence_2}} {{input_sentence_3}} {{input_sentence_4}}\nNovel Correct Ending\nI read the following novel: {{input_sentence_1}} {{input_sentence_2}}\n{{input_sentence_3}} {{input_sentence_4}} What do you think is the most probable\nending? You can choose from the following options: - {{answer_choices | join(\"\\n-\")}}\nA.9 WMT\nPrompts for Section 4.3.1, where we compare prompts in both zero-shot and 1-shot settings\nfor four language directions (en↔{hi,fr}).\nA.9.1 Data example\nThe prompt names and content are specific to the language direction. The prompts below\neach exist in four versions, where “l1” and “l2” are replaced by the language codes of the\nsource and target languages respectively (en, fr or hi) and “L1” and “L2” are replaced by the\nlanguage names of the source and target languages respectively (English, French or Hindi).\nPrompt name: a_good_translation-l1-l2-source+target\nGiven the following source text in English: Spectacular Wingsuit Jump Over\nBogota , a good French translation is:\nAnswer: Spectaculaire saut en \"wingsuit\" au-dessus de Bogota\n71\n\nBigScience Workshop\nA.9.2 Prompts\na_good_translation-l1-l2-source+target\nGiven the following source text in L1:\n{{translation[l1]}} , a\ngood L2 translation is: ||| {{translation[l2]}}\ngpt-3-l1-l2-target\nQ: What is the {{L2}} translation of {{translation[l2]}} A:\nversion-l1-l2-target\nIf the original version says: {{translation[l1]}}; then the L2\nversion should say:\nxglm-l1-l2-source+target\n{{L1}}: {{translation[l1]}} = {{L2}}:\nA.10 DiaBLa\nPrompts for contextual MT results shown in Table 7.\nA.10.1 Data example\nPrompt name: xglm-source+target\nEnglish: We appear to have stopped moving. = French:\nAnswer: J’ai l’impression qu’on s’est arrêtés.\nA.10.2 Prompt\nxglm-source+target\n{% set trg_lang =\"French\" %}{% set src_lang =\"English\" %}\n{% if utterance_meta.lang == \"french\" %}\n{% set trg_lang = \"English\" %}{% set src_lang = \"French\" %}\n{% endif %}\n{{ src_lang }}: {{ orig }} = {{ trg_lang }}: ||| {{ ref }}\nThe dialogue set is bilingual (between native English and native French speakers). In\nfew-shot setups, few-shot examples are by default in either language direction (English→French\nor French→English) regardless of the direction of the current example. We implemented\nsome additional DiaBLa tasks that control the language direction of the few-shot example to\nbe the same or the opposite direction to the current example (diabla_1_shot_context_same\nand diabla_1_shot_context_opposite).\nA.11 Flores-101 (MT)\nThe prompts are specific to each language pair as indicated by the source and target lan-\nguages in the prompt name. Below is an example for French→Catalan translation.\n72\n\nBLOOM\nA.11.1 Data example\nPrompt name: xglm-French-Catalan-source+target\nFrench: L’aménagement vieillot, l’absence d’équipements récents et un certain\ncharme à l’ancienne font également partie de leur caractère. = Catalan: |||\nEls accessoris vintage, la manca de les comoditats modernes i un cert envelliment\nelegant també formen part del seu caràcter\nA.11.2 Prompt\nxglm-French-Catalan-source+target\nFrench: {{ sentence_fra }} = Catalan: ||| {{ sentence_cat }}\nA.12 CrowS-Pairs\nA.12.1 Data example\nSentence A: He couldn’t figure out the issue with the rope, but he tried not to get\ntoo down on himself, most black kids don’t have the best education growing up.\\n\nSentence B: He couldn’t figure out the issue with the rope, but he tried not to get\ntoo down on himself, most white kids don’t have the best education growing up.\\n\nSelect the option that is the most plausible:\\n- Sentence A demonstrates\na stereotype\\n- Sentence B violates a stereotype\nAnswer: Sentence A demonstrates a stereotype\nA.12.2 Prompts\ndemontraste_or_violate\nSentence A: {{sent_more}}\\nSentence B: {{sent_less}}\\nSelect the option\nthat is the most plausible:\\n- {{answer_choices[0]}}\\n- {{answer_choices[1]}}\n73\n"
    },
    {
      "arxiv_id": "1909.08053",
      "full_text": "Megatron-LM: Training Multi-Billion Parameter Language Models Using\nModel Parallelism\nMohammad Shoeybi 1 2 Mostofa Patwary 1 2 Raul Puri 1 2 Patrick LeGresley 2 Jared Casper 2\nBryan Catanzaro 2\nAbstract\nRecent work in language modeling demonstrates\nthat training large transformer models advances\nthe state of the art in Natural Language Processing\napplications. However, very large models can be\nquite difﬁcult to train due to memory constraints.\nIn this work, we present our techniques for train-\ning very large transformer models and implement\na simple, efﬁcient intra-layer model parallel ap-\nproach that enables training transformer models\nwith billions of parameters. Our approach does\nnot require a new compiler or library changes, is\northogonal and complimentary to pipeline model\nparallelism, and can be fully implemented with\nthe insertion of a few communication operations\nin native PyTorch. We illustrate this approach\nby converging transformer based models up to\n8.3 billion parameters using 512 GPUs. We sus-\ntain 15.1 PetaFLOPs across the entire applica-\ntion with 76% scaling efﬁciency when compared\nto a strong single GPU baseline that sustains 39\nTeraFLOPs, which is 30% of peak FLOPs. To\ndemonstrate that large language models can fur-\nther advance the state of the art (SOTA), we train\nan 8.3 billion parameter transformer language\nmodel similar to GPT-2 and a 3.9 billion parame-\nter model similar to BERT. We show that careful\nattention to the placement of layer normalization\nin BERT-like models is critical to achieving in-\ncreased performance as the model size grows. Us-\ning the GPT-2 model we achieve SOTA results\non the WikiText103 (10.8 compared to SOTA per-\nplexity of 15.8) and LAMBADA (66.5% com-\npared to SOTA accuracy of 63.2%) datasets. Our\nBERT model achieves SOTA results on the RACE\ndataset (90.9% compared to SOTA accuracy of\n89.4%).\n1Equal contribution 2NVIDIA. Correspondence to: Mohammad\nShoeybi <mshoeybi@nvidia.com>.\n1. Introduction\nNatural Language Processing (NLP) is advancing quickly in\npart due to an increase in available compute and dataset size.\nThe abundance of compute and data enables training increas-\ningly larger language models via unsupervised pretraining\n(Devlin et al., 2018; Radford et al., 2019). Empirical evi-\ndence indicates that larger language models are dramatically\nmore useful for NLP tasks such as article completion, ques-\ntion answering, and natural language inference (Lan et al.,\n2019; Raffel et al., 2019). By ﬁnetuning these pretrained\nlanguage models on downstream natural language tasks,\none can achieve state of the art results as shown in recent\nwork (Devlin et al., 2018; Peters et al., 2018; Howard &\nRuder, 2018; Radford et al., 2018; 2017; Ramachandran\net al., 2016; Liu et al., 2019b; Dai et al., 2019; Yang et al.,\n2019; Liu et al., 2019a; Lan et al., 2019).\nAs these models become larger, they exceed the memory\nlimit of modern processors, and require additional memory\nmanagement techniques such as activation checkpointing\n(Chen et al., 2016). Widely used optimization algorithms\nsuch as ADAM require additional memory per parameter to\nstore momentum and other optimizer state, which reduces\nthe size of models that can be effectively trained. Several\napproaches to model parallelism overcome this limit by\npartitioning the model such that the weights and their asso-\nciated optimizer state do not need to reside concurrently on\nthe processor. For example, GPipe (Huang et al., 2018) and\nMesh-Tensorﬂow (Shazeer et al., 2018) provide frameworks\nfor model parallelism of different kinds. However, they\nrequire rewriting the model, and rely on custom compilers\nand frameworks that are still under development.\nIn this work, we implement a simple and efﬁcient model\nparallel approach using intra-layer model-parallelism. We\nexploit the inherent structure in transformer based language\nmodels to make a simple model-parallel implementation that\ntrains efﬁciently in PyTorch, with no custom C++ code or\ncompiler required. This approach is orthogonal to pipeline-\nbased model parallelism as advocated by approaches such\nas GPipe (Huang et al., 2018).\nTo demonstrate the scalability of our approach, we establish\narXiv:1909.08053v4  [cs.CL]  13 Mar 2020\n\nMegatron-LM: Training Multi-Billion Parameter Language Models Using Model Parallelism\nFigure 1. Model (blue) and model+data (green) parallel FLOPS\nas a function of number of GPUs. Model parallel (blue): up to\n8-way model parallel weak scaling with approximately 1 billion\nparameters per GPU (e.g. 2 billion for 2 GPUs and 4 billion for\n4 GPUs). Model+data parallel (green): similar conﬁguration as\nmodel parallel combined with 64-way data parallel.\na baseline by training a model of 1.2 billion parameters\non a single NVIDIA V100 32GB GPU, that sustains 39\nTeraFLOPs. This is 30% of the theoretical peak FLOPS\nfor a single GPU as conﬁgured in a DGX-2H server, and\nis thus a strong baseline. Scaling the model to 8.3 billion\nparameters on 512 GPUs with 8-way model parallelism,\nwe achieve up to 15.1 PetaFLOPs per second sustained\nover the entire application. This is 76% scaling efﬁciency\ncompared to the single GPU case. Figure 1 shows more\ndetailed scaling results.\nTo analyze the effect of model size scaling on accuracy,\nwe train both left-to-right GPT-2 (Radford et al., 2019) lan-\nguage models as well as BERT (Devlin et al., 2018) bidi-\nrectional transformers and evaluate them on several down-\nstream tasks. We show that the existing BERT architecture\nresults in model degradation as the size increases. We over-\ncome this challenge by rearranging the layer normalization\nand residual connection in the transformer layers and show\nthat with this change, results for the downstream tasks on\ndevelopment sets improve monotonically as the model size\nincreases. In addition, we show that our models achieve\ntest set state of the art (SOTA) results on WikiText103,\ncloze-style prediction accuracy on LAMBADA, and reading\ncomprehension RACE datasets.\nIn summary, our contributions are as follows:\n• We implement a simple and efﬁcient model parallel\napproach by making only a few targeted modiﬁcations\nto an existing PyTorch transformer implementation.\n• We perform an in-depth empirical analysis of our\nmodel and data parallel technique and demonstrate\nup to 76% scaling efﬁciency using 512 GPUs.\n• We show that careful attention to the placement of\nlayer normalization in BERT-like models is critical to\nachieving increased accuracies as the model grows.\n• We demonstrate that scaling the model size results in\nimproved accuracies for both GPT-2 (studied up to\n8.3 billion parameters) and BERT (studied up to 3.9B\nparameters) models.\n• We showcase that our models achieve state of the art\nresults on test sets: perplexity on WikiText103 (10.8\nppl), accuracy on LAMBADA (66.5%), and accuracy\non RACE (90.9%).\n• We open source our code along with the training\nand evaluation pipelines at https://github.com/\nNVIDIA/Megatron-LM\n2. Background and Challenges\n2.1. Neural Language Model Pretraining\nPretrained language models have become an indispensable\npart of NLP researchers’ toolkits. Leveraging large corpus\npretraining to learn robust neural representations of lan-\nguage is an active area of research that has spanned the\npast decade. Early examples of pretraining and transferring\nneural representations of language demonstrated that pre-\ntrained word embedding tables improve downstream task\nresults compared to word embedding tables learned from\nscratch (Mikolov et al., 2013; Pennington et al., 2014; Turian\net al., 2010). Later work advanced research in this area by\nlearning and transferring neural models that capture contex-\ntual representations of words (Melamud et al., 2016; Mc-\nCann et al., 2017; Peters et al., 2018; Radford et al., 2017;\n2019). Recent parallel work (Ramachandran et al., 2016;\nHoward & Ruder, 2018; Radford et al., 2018; Devlin et al.,\n2018; Liu et al., 2019b; Dai et al., 2019; Yang et al., 2019;\nLiu et al., 2019a; Lan et al., 2019) further builds upon these\nideas by not just transferring the language model to extract\ncontextual word representations, but by also ﬁnetuning the\nlanguage model in an end to end fashion on downstream\ntasks. Through these works, the state of the art has advanced\nfrom transferring just word embedding tables to transferring\nentire multi-billion parameter language models. This pro-\ngression of methods has necessitated the need for hardware,\nsystems techniques, and frameworks that are able to oper-\nate efﬁciently at scale and satisfy increasing computational\nneeds. Our work aims to provide the tools necessary to take\nanother step forward in this trend.\n2.2. Transformer Language Models and Multi-Head\nAttention\nCurrent work in NLP trends towards using transformer mod-\nels (Vaswani et al., 2017) due to their superior accuracy\n\nMegatron-LM: Training Multi-Billion Parameter Language Models Using Model Parallelism\nFigure 2. Transformer Architecture. Purple blocks correspond to\nfully connected layers. Each blue block represents a single trans-\nformer layer that is replicated N times.\nand compute efﬁciency. The original transformer formula-\ntion was designed as a machine translation architecture that\ntransforms an input sequence into another output sequence\nusing two parts, an Encoder and Decoder. However, recent\nwork leveraging transformers for language modeling such as\nBERT (Devlin et al., 2018) and GPT-2 (Radford et al., 2019)\nuse only the Encoder or Decoder depending on their needs.\nThis work explores both a decoder architecture, GPT-2, and\nan encoder architecture, BERT.\nFigure 2 shows a schematic diagram of the model we used.\nWe refer the reader to prior work for a detailed descrip-\ntion of the model architecture (Vaswani et al., 2017; Devlin\net al., 2018; Radford et al., 2019). It is worthwhile to men-\ntion that both GPT-2 and BERT use GeLU (Hendrycks &\nGimpel, 2016) nonlinearities and layer normalization (Ba\net al., 2016) to the input of the multi-head attention and feed\nforward layers, whereas the original transformer (Vaswani\net al., 2017) uses ReLU nonlinearities and applies layer\nnormalization to outputs.\n2.3. Data and Model Parallelism in Deep Learning\nThere are two central paradigms for scaling out deep neu-\nral network training to numerous hardware accelerators:\ndata parallelism (Valiant, 1990) where a training minibatch\nis split across multiple workers, and model parallelism in\nwhich the memory usage and computation of a model is\ndistributed across multiple workers. By increasing the mini-\nbatch size proportionally to the number of available work-\ners (i.e. weak scaling), one observes near linear scaling\nin training data throughput. However, large batch train-\ning introduces complications into the optimization process\nthat can result in reduced accuracy or longer time to conver-\ngence, offsetting the beneﬁt of increased training throughput\n(Keskar et al., 2017). Further research (Goyal et al., 2017;\nYou et al., 2017; 2019) has developed techniques to miti-\ngate these effects and drive down the training time of large\nneural networks. To scale out training even further, parallel\nwork (Chen et al., 2016) has combined data parallelism with\nactivation checkpointing: recomputing activations in the\nbackward pass without storing them in the forward pass to\nreduce memory requirements.\nHowever, these techniques have one fundamental limitation\nin the problem size they can tackle: the model must ﬁt\nentirely on one worker. With language models of increasing\nsize and complexity like BERT and GPT-2, neural networks\nhave approached the memory capacity of modern hardware\naccelerators. One solution to this problem is to employ\nparameter sharing to reduce the memory footprint of the\nmodel (Lan et al., 2019), but this limits the overall capacity\nof the model. Our approach is to utilize model parallelism\nto split the model across multiple accelerators. This not\nonly alleviates the memory pressure, but also increases the\namount of parallelism independently of the microbatch size.\nWithin model parallelism, there are two further paradigms:\nlayer-wise pipeline parallelism, and more general distributed\ntensor computation. In pipeline model parallelism, groups\nof operations are performed on one device before the outputs\nare passed to the next device in the pipeline where a differ-\nent group of operations are performed. Some approaches\n(Harlap et al., 2018; Chen et al., 2018) use a parameter\nserver (Li et al., 2014) in conjunction with pipeline par-\nallelism. However these suffer from inconsistency issues.\nThe GPipe framework for TensorFlow (Huang et al., 2018)\novercomes this inconsistency issue by using synchronous\ngradient decent. This approach requires additional logic to\nhandle the efﬁcient pipelining of these communication and\ncomputation operations, and suffers from pipeline bubbles\nthat reduce efﬁciency, or changes to the optimizer itself\nwhich impact accuracy.\nDistributed tensor computation is an orthogonal and more\ngeneral approach that partitions a tensor operation across\nmultiple devices to accelerate computation or increase\nmodel size. FlexFlow (Jia et al., 2018), a deep learning\nframework orchestrating such parallel computation, pro-\nvides a method to pick the best parallelization strategy. Re-\ncently, Mesh-TensorFlow (Shazeer et al., 2018) introduced\na language for specifying a general class of distributed ten-\nsor computations in TensorFlow (Abadi et al., 2015). The\nparallel dimensions are speciﬁed in the language by the\nend user and the resulting graph is compiled with proper\ncollective primitives. We utilize similar insights to those\nleveraged in Mesh-TensorFlow and exploit parallelism in\ncomputing the transformer’s attention heads to parallelize\nour transformer model. However, rather than implementing\na framework and compiler for model parallelism, we make\nonly a few targeted modiﬁcations to existing PyTorch trans-\nformer implementations. Our approach is simple, does not\n\nMegatron-LM: Training Multi-Billion Parameter Language Models Using Model Parallelism\nrequire any new compiler or code re-writing, and can be\nfully implemented by inserting a few simple primitives, as\ndescribed in the next section.\n3. Model Parallel Transformers\nWe take advantage of the structure of transformer networks\nto create a simple model parallel implementation by adding a\nfew synchronization primitives. A transformer layer consists\nof a self attention block followed by a two-layer, multi-layer\nperceptron (MLP) as shown in Figure 2. We introduce\nmodel parallelism in both of these blocks separately.\nWe start by detailing the MLP block. The ﬁrst part of the\nblock is a GEMM followed by a GeLU nonlinearity:\nY = GeLU(XA)\n(1)\nOne option to parallelize the GEMM is to split the weight\nmatrix A along its rows and input X along its columns as:\nX = [X1, X2], A =\n\u0014A1\nA2\n\u0015\n.\n(2)\nThis partitioning will result in Y\n=\nGeLU(X1A1 +\nX2A2). Since GeLU is a nonlinear function, GeLU(X1A1+\nX2A2) ̸= GeLU(X1A1)+GeLU(X2A2) and this approach\nwill require a synchronization point before the GeLU func-\ntion.\nAnother option is to split A along its columns A = [A1, A2].\nThis partitioning allows the GeLU nonlinearity to be inde-\npendently applied to the output of each partitioned GEMM:\n[Y1, Y2] = [GeLU(XA1), GeLU(XA2)]\n(3)\nThis is advantageous as it removes a synchronization point.\nHence, we partition the ﬁrst GEMM in this column parallel\nfashion and split the second GEMM along its rows so it takes\nthe output of the GeLU layer directly without requiring any\ncommunication as shown in Figure 3a. The output of the\nsecond GEMM is then reduced across the GPUs before\npassing the output to the dropout layer. This approach splits\nboth GEMMs in the MLP block across GPUs and requires\nonly a single all-reduce operation in the forward pass (g\noperator) and a single all-reduce in the backward pass (f\noperator). These two operators are conjugates of each other\nand can be implemented in PyTorch with only a few lines of\ncode. As an example, the implementation of the f operator\nis provided below:\nclass f(torch.autograd.Function):\ndef forward(ctx, x):\nreturn x\ndef backward(ctx, gradient):\nall_reduce(gradient)\nreturn gradient\nCode 1. Implementation of f operator. g is similar to f with\nidentity in the backward and all-reduce in the forward\nfunctions.\n(a) MLP\n(b) Self-Attention\nFigure 3. Blocks of Transformer with Model Parallelism. f and g\nare conjugate. f is an identity operator in the forward pass and all\nreduce in the backward pass while g is an all reduce in the forward\npass and identity in the backward pass.\nAs shown in Figure 3b, for the self attention block we exploit\ninherent parallelism in the multihead attention operation,\npartitioning the GEMMs associated with key (K), query\n(Q), and value (V ) in a column parallel fashion such that\nthe matrix multiply corresponding to each attention head is\ndone locally on one GPU. This allows us to split per atten-\ntion head parameters and workload across the GPUs, and\ndoesnt require any immediate communication to complete\nthe self-attention. The subsequent GEMM from the output\nlinear layer (after self attention) is parallelized along its\nrows and takes the output of the parallel attention layer di-\nrectly, without requiring communication between the GPUs.\nThis approach for both the MLP and self attention layer\nfuses groups of two GEMMs, eliminates a synchronization\npoint in between, and results in better scaling. This enables\nus to perform all GEMMs in a simple transformer layer\nusing only two all-reduces in the forward path and two in\nthe backward path (see Figure 4).\nThe transformer language model has an output embedding\nwith the dimension of hidden-size (H) times vocabulary-\nsize (v). Since the vocabulary size is on the order of tens\nof thousands of tokens for modern language models (for\nexample, GPT-2 used a vocabulary size of 50,257), it is ben-\neﬁcial to parallelize the output embedding GEMM. How-\never, in transformer language models, the output embed-\nding layer shares weights with the input embedding, requir-\ning modiﬁcations to both. We parallelize the input embed-\nding weight matrix EH×v along the vocabulary dimension\nE = [E1, E2] (column-wise). Since each partition now only\n\nMegatron-LM: Training Multi-Billion Parameter Language Models Using Model Parallelism\nFigure 4. Communication operations in a transformer layer. There\nare 4 total communication operations in the forward and backward\npass of a single model parallel transformer layer.\ncontains a portion of the embedding table, an all-reduce (g\noperator) is required after the input embedding. For the\noutput embedding, one approach is to perform the parallel\nGEMM [Y1, Y2] = [XE1, XE2] to obtain the logits, add an\nall-gather Y = all-gather([Y1, Y2]), and send the results to\nthe cross-entropy loss function. However, for this case, the\nall-gather will communicate b × s × v elements (b is the\nbatch-size and s is the sequence length) which is huge due to\nvocabulary size being large. To reduce the communication\nsize, we fuse the output of the parallel GEMM [Y1, Y2] with\nthe cross entropy loss which reduces the dimension to b × s.\nCommunicating scalar losses instead of logits is a huge re-\nduction in communication that improves the efﬁciency of\nour model parallel approach.\nMuch of our model parallel approach can be characterized\nas techniques aimed at reducing communication and keep-\ning the GPUs compute bound. Rather than having one GPU\ncompute part of the dropout, layer normalization, or residual\nconnections and broadcast the results to other GPUs, we\nchoose to duplicate the computation across GPUs. Speciﬁ-\ncally, we maintain duplicate copies of layer normalization\nparameters on each GPU, and take the output of the model\nparallel region and run dropout and residual connection\non these tensors before feeding them as input to the next\nmodel parallel regions. To optimize the model we allow\neach model parallel worker to optimize its own set of pa-\nrameters. Since all values are either local to or duplicated\non a GPU, there is no need for communicating updated\nparameter values in this formulation.\nWe present further details about the hybrid model and data\nparallelism and handling random number generation in Ap-\npendix B for reference. In summary, our approach as de-\nscribed above is simple to implement, requiring only a few\nextra all-reduce operations added to the forward and back-\nward pass. It does not require a compiler, and is orthogonal\nand complementary to the pipeline model parallelism advo-\ncated by approaches such as (Huang et al., 2018).\n4. Setup\nPretrained language understanding models are central tasks\nin natural language processing and language understanding.\nThere are several formulations of language modeling. In\nthis work we focus on GPT-2 (Radford et al., 2019), a left-\nto-right generative transformer based language model, and\nBERT (Devlin et al., 2018), a bi-directional transformer\nmodel based on language model masking. We explain our\nconﬁgurations for these models in the following section and\nrefer to the original papers for more details.\n4.1. Training Dataset\nTo collect a large diverse training set with longterm de-\npendencies we aggregate several of the largest language\nmodeling datasets. We create an aggregate dataset consist-\ning of Wikipedia (Devlin et al., 2018), CC-Stories (Trinh &\nLe, 2018), RealNews (Zellers et al., 2019), and OpenWeb-\ntext (Radford et al., 2019). To avoid training set leakage\ninto our downstream tasks we remove the Wikipedia articles\npresent in the WikiText103 test set (Merity et al., 2016).\nWe also remove unnecessary newlines from the CC-Stories\ncorpus introduced by preprocessing artifacts. For BERT\nmodels we include BooksCorpus (Zhu et al., 2015) in the\ntraining dataset, however, this dataset is excluded for GPT-2\ntrainings as it overlaps with LAMBADA task.\nWe combined all the datasets and then ﬁltered out all the\ndocuments with content length less than 128 tokens from\nthe aggregated dataset. Since similar content might appear\nmultiple times in the aggregated datasets, we used locality-\nsensitive hashing (LSH) to deduplicate content with a jac-\ncard similarity greater than 0.7. The resulting aggregate\ncorpus contains 174 GB of deduplicated text.\n4.2. Training Optimization and Hyperparameters\nTo train our models efﬁciently we utilize mixed precision\ntraining with dynamic loss scaling to take advantage of the\nV100’s Tensor Cores (Micikevicius et al., 2017; NVIDIA,\n2018). We start by initializing our weights W with a sim-\nple normal distribution W ∼N(0, 0.02). We then scale\nweights immediately before residual layers by\n1\n√\n2N where\nN is the number of transformer layers comprised of self at-\ntention and MLP blocks. For our optimizer we utilize Adam\n(Kingma & Ba, 2014) with weight decay (Loshchilov &\nHutter, 2019) λ = 0.01. Additionally, we use global gradi-\nent norm clipping of 1.0 to improve the stability of training\nlarge models. In all cases, a dropout of 0.1 is used. Lastly,\nto better manage our memory footprint we utilize activation\ncheckpointing (Chen et al., 2016) after every transformer\nlayer.\nFor GPT-2 models, all training is performed with sequences\nof 1024 subword units at a batch size of 512 for 300k itera-\n\nMegatron-LM: Training Multi-Billion Parameter Language Models Using Model Parallelism\ntions. Our learning rate of 1.5e-4 utilizes a warmup period\nof 3k iterations before following a single cycle cosine decay\nover the remaining 297k iterations. We stop the decay at a\nminimum learning rate of 1e-5.\nFor BERT models, we largely follow the training process\ndescribed in (Lan et al., 2019). We use the original BERT\ndictionary with vocab size of 30,522. In addition, we re-\nplace the next sentence prediction head with sentence order\nprediction as suggested by (Lan et al., 2019) and use whole\nword n-gram masking of (Joshi et al., 2019). For all cases,\nwe set the batch size to 1024 and use a learning rate of 1.0e-\n4 warmed up over 10,000 iterations and decayed linearly\nover 2 million iterations. Other training parameters are kept\nthe same as (Devlin et al., 2018).\n5. Experiments\nAll of our experiments use up to 32 DGX-2H servers (a total\nof 512 Tesla V100 SXM3 32GB GPUs). Our infrastruc-\nture is optimized for multi-node deep learning applications,\nwith 300 GB/sec bandwidth between GPUs inside a server\nvia NVSwitch and 100 GB/sec of interconnect bandwidth\nbetween servers using 8 InﬁniBand adapters per server.\n5.1. Scaling Analysis\nTo test the scalability of our implementation, we consider\nGPT-2 models with four sets of parameters detailed in Table\n1. To have consistent GEMM sizes in the self attention layer,\nthe hidden size per attention head is kept constant at 96\nwhile the number of heads and layers are varied to obtain\nconﬁgurations ranging from 1 billion to 8 billion parameters.\nThe conﬁguration with 1.2 billion parameters ﬁts on a single\nGPU whereas the 8 billion parameter model requires 8-way\nmodel parallelism (8 GPUs). The original vocabulary size\nwas 50,257, however, to have efﬁcient GEMMs for the logit\nlayer, it is beneﬁcial for the per-GPU vocabulary size to\nbe a multiple of 128. Since we study up to 8-way model\nparallelism, we pad the vocabulary such that it is divisible\nby 128 × 8 = 1024, resulting in a padded vocabulary size\nof 51,200. We study both model and model+data parallel\nscaling. For the model parallel scaling, a ﬁxed batch size of\n8 is used across all conﬁgurations. Data parallel scaling is\nnecessary for training many state of the art models which\ntypically use a much larger global batch size. To this end,\nfor the model+data parallel cases we ﬁx the global batch\nsize to 512 for all experiments which corresponds to 64-way\ndata parallelism.\n5.1.1. MODEL AND DATA PARALLELISM\nThroughout this section, we will showcase weak scaling\nwith respect to the model parameters for both model parallel\nand model+data parallel cases. Weak scaling is typically\nTable 1. Parameters used for scaling studies. Hidden size per atten-\ntion head is kept constant at 96.\nNumber\nNumber\nModel\nModel\nHidden Attention\nof\nof\nparallel\n+data\nSize\nheads\nlayers\nparameters\nGPUs\nparallel\n(billions)\nGPUs\n1536\n16\n40\n1.2\n1\n64\n1920\n20\n54\n2.5\n2\n128\n2304\n24\n64\n4.2\n4\n256\n3072\n32\n72\n8.3\n8\n512\n100%\n95%\n82%\n77%\n96%\n83%\n79%\n74%\n0%\n20%\n40%\n60%\n80%\n100%\n1\n2\n4\n8\n…\n64\n128\n256\n512\nWeak Scaling\nNumber of GPUS\nModel Parallel\nModel + Data Parallel\nFigure 5. Model and model + data parallel weak scaling efﬁciency\nas a function of the number of GPUs.\ndone by scaling the batch-size, however, this approach does\nnot address training large models that do not ﬁt on a single\nGPU and it leads to training convergence degradation for\nlarge batch sizes. In contrast, here we use weak scaling to\ntrain larger models that were not possible otherwise. The\nbaseline for all the scaling numbers is the ﬁrst conﬁguration\n(1.2 billion parameters) in Table 1 running on a single GPU.\nThis is a strong baseline as it achieves 39 TeraFLOPS during\nthe overall training process, which is 30% of the theoretical\npeak FLOPS for a single GPU in a DGX-2H server.\nFigure 5 shows scaling values for both model and\nmodel+data parallelism. We observe excellent scaling num-\nbers in both settings. For example, the 8.3 billion parame-\nters case with 8-way (8 GPU) model parallelism achieves\n77% of linear scaling. Model+data parallelism requires fur-\nther communication of gradients and as a result the scaling\nnumbers drop slightly. However, even for the largest conﬁg-\nuration (8.3 billion parameters) running on 512 GPUs, we\nachieve 74% scaling relative to linear scaling of the strong\nsingle GPU baseline conﬁguration (1.2 billion parameters).\nFurther scaling analysis is provided in Appendix D\n5.2. Language Modeling Results Using GPT-2\nTo demonstrate that large language models can further ad-\nvance the state of the art, we consider training GPT-2 models\nof the sizes and conﬁgurations listed in Table 2. The 355M\nmodel is equivalent in size and conﬁguration of BERT-Large\nmodel (Devlin et al., 2018). The 2.5B model is bigger than\nthe previous largest GPT-2 model, and the 8.3B model is\nlarger than any left-to-right transformer language model\never trained, to the best of our knowledge. To train and eval-\n\nMegatron-LM: Training Multi-Billion Parameter Language Models Using Model Parallelism\nTable 2. Model conﬁgurations used for GPT-2.\nHidden\nTime\nParameter Layers Hidden\nAttn\nSize\nTotal\nper\nCount\nSize\nHeads\nper\nGPUs Epoch\nHead\n(days)\n355M\n24\n1024\n16\n64\n64\n0.86\n2.5B\n54\n1920\n20\n96\n128\n2.27\n8.3B\n72\n3072\n24\n128\n512\n2.10\nTable 3. Zero-shot results. SOTA are from (Khandelwal et al.,\n2019) for Wikitext103 and (Radford et al., 2019) for LAMBADA.\nModel\nWikitext103\nLAMBADA\nPerplexity ↓\nAccuracy ↑\n355M\n19.31\n45.18%\n2.5B\n12.76\n61.73%\n8.3B\n10.81\n66.51%\nPrevious SOTA\n15.79\n63.24%\nuate our language models we use the procedure described in\nsection 4. Table 2 also lists the time it takes to advance one\nepoch which is equivalent to 68,507 iterations. For example,\nfor the 8.3B model on 512 GPUs, each epoch takes around\ntwo days. Compared to the conﬁgurations used for our scal-\ning studies in Table 1, the 2.5B model is the same, the 8.3B\nmodel has 24 attention heads instead of 32, and the 355M is\nmuch smaller than any seen previously while still using 64\nGPUs to train, leading to the much lower time per epoch.\nFigure 6 shows validation perpelixity as a function of num-\nber of iterations. As the model size increases, the validation\nperpelixity decreases and reaches a validation perplexity of\n9.27 for the 8.3B model. We report the zero-shot evaluation\nof the trained models on the LAMBADA and WikiText103\ndatasets in Table 3. For more details on evaluation method-\nology, see Appendix E. We observe the trend that increasing\nmodel size also leads to lower perplexity on WikiText103\nand higher cloze accuracy on LAMBADA. Our 8.3B model\nachieves state of the art perplexity on the WikiText103 test\nset at a properly adjusted perplexity of 10.81. At 66.51%\naccuracy, the 8.3B model similarly surpasses prior cloze\naccuracy results on the LAMBADA task. We have included\nsamples generated from the 8.3 billion parameters model\nin the Appendix C. Recently researchers from Microsoft in\ncollaboration with NVIDIA trained a 17 billion parameter\nGPT-2 model called Turing-NLG (Microsoft, 2020) using\nMegatron and showed that the accuracies further improve\nas they scale the model, highlighting the value of larger\nmodels.\nTo ensure we do not train on any data found in our test sets,\nwe calculate the percentage of test set 8-grams that also\nappear in our training set as done in previous work (Rad-\nford et al., 2019). The WikiText103 test set has at most\nFigure 6. Validation set perplexity. All language models are trained\nfor 300k iterations. Larger language models converge notice-\nably faster and converge to lower validation perplexities than their\nsmaller counterparts.\nTable 4. Model conﬁgurations used for BERT.\nParameter\nLayers\nHidden\nAttention\nTotal\nCount\nSize\nHeads\nGPUs\n336M\n24\n1024\n16\n128\n1.3B\n24\n2048\n32\n256\n3.9B\n48\n2560\n40\n512\n10.8% overlap and the LAMBADA test set (Paperno et al.,\n2016) has at most 1.4% overlap. We should note that the\nWikiText103 test set has already 9.09% overlap with the\nWikiText103 training set (Radford et al., 2019). As these\nare consistent with previous work, we are conﬁdent that no\ndocuments from our test data are inadvertently included in\nour training data.\n5.3. Bi-directional Transformer Results Using BERT\nIn this section, we apply our methodology to BERT-style\ntransformer models and study the effect of model scaling\non several downstream tasks. Prior work (Lan et al., 2019)\nfound that increasing model size beyond BERT-large with\n336M parameters results in unexpected model degradation.\nTo address this degradation, the authors of that work (Lan\net al., 2019) introduced parameter sharing and showed that\nthat their models scale much better compared to the original\nBERT model.\nWe further investigated this behaviour and empirically\ndemonstrated that rearranging the order of the layer nor-\nmalization and the residual connections as shown in Figure\n7 is critical to enable the scaling of the BERT-style mod-\nels beyond BERT-Large. The architecture (b) in Figure 7\neliminates instabilities observed using the original BERT\narchitecture in (a) and also has a lower training loss. To\nthe best of our knowledge, we are the ﬁrst to report such a\nchange enables training larger BERT models.\n\nMegatron-LM: Training Multi-Billion Parameter Language Models Using Model Parallelism\nTable 5. Development set results for MNLI, QQP, SQuAD 1.1 and SQuAD 2.0 and test set results for RACE. The trained tokens represents\nconsumed tokens during model pretraining (proportional to batch size times number of iterations) normalized by consumed tokens during\nmodel pretraining for our 336M model.\nModel\ntrained tokens\nMNLI m/mm\nQQP\nSQuAD 1.1\nSQuAD 2.0\nRACE m/h\nratio\naccuracy\naccuracy\nF1 / EM\nF1 / EM\naccuracy\n(dev set)\n(dev set)\n(dev set)\n(dev set)\n(test set)\nRoBERTa (Liu et al., 2019b)\n2\n90.2 / 90.2\n92.2\n94.6 / 88.9\n89.4 / 86.5\n83.2 (86.5 / 81.8)\nALBERT (Lan et al., 2019)\n3\n90.8\n92.2\n94.8 / 89.3\n90.2 / 87.4\n86.5 (89.0 / 85.5)\nXLNet (Yang et al., 2019)\n2\n90.8 / 90.8\n92.3\n95.1 / 89.7\n90.6 / 87.9\n85.4 (88.6 / 84.0)\nMegatron-336M\n1\n89.7 / 90.0\n92.3\n94.2 / 88.0\n88.1 / 84.8\n83.0 (86.9 / 81.5)\nMegatron-1.3B\n1\n90.9 / 91.0\n92.6\n94.9 / 89.1\n90.2 / 87.1\n87.3 (90.4 / 86.1)\nMegatron-3.9B\n1\n91.4 / 91.4\n92.7\n95.5 / 90.0\n91.2 / 88.5\n89.5 (91.8 / 88.6)\nALBERT ensemble (Lan et al., 2019)\n95.5 / 90.1\n91.4 / 88.9\n89.4 (91.2 / 88.6)\nMegatron-3.9B ensemble\n95.8 / 90.5\n91.7 / 89.0\n90.9 (93.1 / 90.0)\nFigure 7. Training loss for BERT model using the original architec-\nture (a) and the rearranged architecture (b). Left ﬁgure shows the\ntraining loss for 336M and 752M BERT model. While the original\narchitecture performs well on the 336M model, the modiﬁcations\nin (b) enable stable training with lower training loss.\nUsing the architecture change in Figure 7(b), we consider\nthree different cases as detailed in Table 4. The 336M model\nhas the same size as BERT-large. The 1.3B is the same as\nthe BERT-xlarge conﬁguration that was previously shown\nto get worse results than the 336M BERT-large model (Lan\net al., 2019). We further scale the BERT model using both\nlarger hidden size as well as more layers to arrive at the 3.9B\nparameter case. In all cases, the hidden size per attention\nhead is kept constant at 64. 336M and 1.3B models are\ntrained for 2 million iterations while the 3.9B model is\ntrained for 1.5 million iterations and is still training.\nOn a 3% held-out set, 336M, 1.3B, and 3.9B models achieve\nvalidation set perplexity of 1.58, 1.30, and 1.16, respectively,\na monotonic decrease with the model size. We ﬁnetune\nthe trained models on several downstream tasks including\nMNLI and QQP from the GLUE benchmark (Wang et al.,\n2019), SQuAD 1.1 and SQuAD 2.0 from the Stanford Ques-\ntion answering dataset (Rajpurkar et al., 2016; 2018), and\nthe reading comprehension RACE dataset (Lai et al., 2017).\nFor ﬁnetuning, we follow the same procedure as (Liu et al.,\n2019b). We ﬁrst perform hyperparameter tuning on batch\nsize and learning rate. Once we obtain the best values, we\nreport the median development set results over 5 different\nrandom seeds for initialization. The hyperparameters used\nfor each model and task are provided in the Appendix A.\nTable 5 shows the development set results for MNLI, QQP,\nSQuAD 1.1, and SQuAD 2.0 and test set results for RACE.\nFor the test set results of RACE, we ﬁrst use the develop-\nment set to ﬁnd the checkpoint that gives us the median\nscore on the 5 random seeds and we report the results from\nthat checkpoint on the test set. We also report 5-way ensem-\nble results for the development set of SQuAD and test set\nof RACE. From Table 5 we observe that (a) as the model\nsize increases, the downstream task performance improves\nin all cases, (b) our 3.9B model establishes state of the art\nresults on the development set compared to other BERT\nbased models, and (c) our 3.9B model achieves both single\nmodel as well as ensembled SOTA results on RACE test set.\n6. Conclusion and Future Work\nIn this work, we successfully surpassed the limitations posed\nby traditional single-GPU-per-model training by implement-\ning model parallelism with only a few modiﬁcations to\nthe existing PyTorch transformer implementations. We ef-\nﬁciently trained transformer based models up to 8.3 bil-\nlion parameter on 512 NVIDIA V100 GPUs with 8-way\nmodel parallelism and achieved up to 15.1 PetaFLOPs sus-\ntained over the entire application. We also showed that for\nBERT models, careful attention to the placement of layer\nnormalization in BERT-like models is critical to achieving\nincreased accuracies as the model size increases. We study\nthe effect of model size on down-stream task accuracy and\nachieve far superior results on downstream tasks and estab-\nlish new SOTA for WikiText103, LAMBADA, and RACE\ndatasets. Finally, we open sourced our code to enable future\nwork leveraging model parallel transformers.\nThere are several directions for future work. Continuing\nto increase the scale of pretraining is a promising line of\n\nMegatron-LM: Training Multi-Billion Parameter Language Models Using Model Parallelism\ninvestigation that will further test existing deep learning\nhardware and software. To realize this, improvements in\nthe efﬁciency and memory footprint of optimizers will be\nneeded. In addition, training a model with more than 16\nbillion parameters will demand more memory than is avail-\nable within 16 GPUs of a DGX-2H box. For such models, a\nhybrid intra-layer and inter-layer model parallelism along\nwith inter-node model parallelism would be more suitable.\nThree other directions of investigation include (a) pretrain-\ning different model families (XLNet, T5), (b) evaluating per-\nformance of large models across more difﬁcult and diverse\ndownstream tasks (e.g. Generative Question Answering,\nSummarization, and Conversation), and (c) using knowl-\nedge distillation to train small student models from these\nlarge pretrained teacher models.\nReferences\nAbadi, M., Agarwal, A., Barham, P., Brevdo, E., Chen, Z.,\nCitro, C., Corrado, G. S., Davis, A., Dean, J., Devin, M.,\nGhemawat, S., Goodfellow, I., Harp, A., Irving, G., Is-\nard, M., Jia, Y., Jozefowicz, R., Kaiser, L., Kudlur, M.,\nLevenberg, J., Man´e, D., Monga, R., Moore, S., Mur-\nray, D., Olah, C., Schuster, M., Shlens, J., Steiner, B.,\nSutskever, I., Talwar, K., Tucker, P., Vanhoucke, V., Va-\nsudevan, V., Vi´egas, F., Vinyals, O., Warden, P., Watten-\nberg, M., Wicke, M., Yu, Y., and Zheng, X. TensorFlow:\nLarge-scale machine learning on heterogeneous systems,\n2015. URL http://tensorflow.org/. Software\navailable from tensorﬂow.org.\nBa, J. L., Kiros, J. R., and Hinton, G. E. Layernorm. CoRR,\nabs/1607.06450, 2016. URL http://arxiv.org/\nabs/1607.06450.\nChen, C.-C., Yang, C.-L., and Cheng, H.-Y. Efﬁcient and\nrobust parallel dnn training through model parallelism on\nmulti-gpu platform. arXiv:1809.02839, 2018.\nChen, T., Xu, B., Zhang, C., and Guestrin, C.\nTrain-\ning deep nets with sublinear memory cost.\nCoRR,\nabs/1604.06174, 2016. URL http://arxiv.org/\nabs/1604.06174.\nDai, Z., Yang, Z., Yang, Y., Carbonell, J. G., Le, Q. V.,\nand Salakhutdinov, R. Transformer-xl: Attentive lan-\nguage models beyond a ﬁxed-length context.\nCoRR,\nabs/1901.02860, 2019. URL http://arxiv.org/\nabs/1901.02860.\nDevlin, J., Chang, M.-W., Lee, K., and Toutanova, K. Bert:\nPre-training of deep bidirectional transformers for lan-\nguage understanding, 2018.\nGoyal, P., Doll´ar, P., Girshick, R. B., Noordhuis, P.,\nWesolowski, L., Kyrola, A., Tulloch, A., Jia, Y., and\nHe, K. Accurate, large minibatch SGD: training imagenet\nin 1 hour. CoRR, abs/1706.02677, 2017.\nHarlap,\nA.,\nNarayanan,\nD.,\nPhanishayee,\nA.,\nSe-\nshadri, V., Devanur, N., Ganger, G., and Gibbons, P.\nPipedream: Fast and efﬁcient pipeline parallel dnn train-\ning. arXiv:1806.03377, 2018.\nHendrycks, D. and Gimpel, K.\nBridging nonlinearities\nand stochastic regularizers with gaussian error linear\nunits.\nCoRR, abs/1606.08415, 2016.\nURL http:\n//arxiv.org/abs/1606.08415.\nHoward, J. and Ruder, S. Fine-tuned language models for\ntext classiﬁcation. CoRR, abs/1801.06146, 2018.\nHuang, Y., Cheng, Y., Chen, D., Lee, H., Ngiam, J., Le,\nQ. V., and Chen, Z.\nGpipe: Efﬁcient training of gi-\nant neural networks using pipeline parallelism. CoRR,\nabs/1811.06965, 2018. URL http://arxiv.org/\nabs/1811.06965.\nJia, Z., Zaharia, M., and Aiken, A. Beyond data and model\nparallelism for deep neural networks. arXiv:1807.05358,\n2018.\nJoshi, M., Chen, D., Liu, Y., Weld, D. S., Zettlemoyer,\nL., and Levy, O. Spanbert: Improving pre-training by\nrepresenting and predicting spans. arXiv:1907.10529,\n2019.\nKeskar, N. S., Mudigere, D., Nocedal, J., Smelyanskiy,\nM., and Tang, P. T. P. On large- batch training for deep\nlearning: Generalization gap and sharp minima. ICLR,\n2017.\nKhandelwal, U., Levy, O., Jurafsky, D., Zettlemoyer, L., and\nLewis, M. Generalization through memorization: Nearest\nneighbor language models. arXiv:1911.00172, 2019.\nKingma, D. P. and Ba, J. Adam: A method for stochastic\noptimization. arXiv preprint arXiv:1412.6980, 2014.\nLai, G., Xie, Q., Liu, H., Yang, Y., and Hovy, E. Race:\nLarge-scale reading comprehension dataset from exami-\nnations. arXiv:1704.04683, 2017.\nLan, Z., Chen, M., Goodman, S., Gimpel, K., and Soricut, P.\nS. R. Albert: A lite bert for self-supervised learning of\nlanguage representations. arXiv:1909.11942, 2019.\nLi, M., Andersen, D. G., Park, J. W., Smola, A. J., Ahmed,\nA., Josifovski, V., Long, J., Shekita, E. J., and Su, B.-Y.\nScaling distributed machine learning with the parameter\nserver, 2014.\nLiu, X., He, P., Chen, W., and Gao, J. Multi-task deep neu-\nral networks for natural language understanding. CoRR,\nabs/1901.11504, 2019a. URL http://arxiv.org/\nabs/1901.11504.\n\nMegatron-LM: Training Multi-Billion Parameter Language Models Using Model Parallelism\nLiu, Y., Ott, M., Goyal, N., Du, J., Joshi, M., Chen, D., Levy,\nO., Lewis, M., Zettlemoyer, L., and Stoyanov, V. Roberta:\nA robustly optimized BERT pretraining approach. CoRR,\nabs/1907.11692, 2019b. URL http://arxiv.org/\nabs/1907.11692.\nLoshchilov, I. and Hutter, F.\nDecoupled weight de-\ncay regularization.\nIn International Conference on\nLearning Representations, 2019.\nURL https://\nopenreview.net/forum?id=Bkg6RiCqY7.\nMcCann, B., Bradbury, J., Xiong, C., and Socher, R.\nLearned in translation: Contextualized word vectors.\nCoRR, abs/1708.00107, 2017.\nMelamud, O., Goldberger, J., and Dagan, I. context2vec:\nLearning generic context embedding with bidirectional\nlstm. In Proceedings of The 20th SIGNLL Conference on\nComputational Natural Language Learning, pp. 51–61,\n01 2016.\nMerity, S., Xiong, C., Bradbury, J., and Socher, R. Pointer\nsentinel mixture models. CoRR, abs/1609.07843, 2016.\nURL http://arxiv.org/abs/1609.07843.\nMicikevicius, P., Narang, S., Alben, J., Diamos, G. F., Elsen,\nE., Garcia, D., Ginsburg, B., Houston, M., Kuchaiev, O.,\nVenkatesh, G., and Wu, H. Mixed precision training.\nCoRR, abs/1710.03740, 2017.\nMicrosoft.\nTuring-nlg:\nA 17-billion-parameter lan-\nguage model by microsoft, 2020.\nURL https://\nwww.microsoft.com/en-us/research/blog/\nturing - nlg - a - 17 - billion - parameter -\nlanguage-model-by-microsoft/.\nMikolov, T., Deoras, A., Kombrink, S., Burget, L., and\nˇCernock`y, J. Empirical evaluation and combination of ad-\nvanced language modeling techniques. In Twelfth Annual\nConference of the International Speech Communication\nAssociation, 2011.\nMikolov, T., Sutskever, I., Chen, K., Corrado, G., and Dean,\nJ. Distributed representations of words and phrases and\ntheir compositionality. CoRR, abs/1310.4546, 2013.\nNVIDIA. Mixed precision training: Choosing a scaling\nfactor, 2018.\nURL https://docs.nvidia.com/\ndeeplearning / sdk / mixed - precision -\ntraining/index.html#scalefactor.\nPaperno, D., Kruszewski, G., Lazaridou, A., Pham, Q. N.,\nBernardi, R., Pezzelle, S., Baroni, M., Boleda, G., and\nFern´andez, R.\nThe LAMBADA dataset: Word pre-\ndiction requiring a broad discourse context.\nCoRR,\nabs/1606.06031, 2016. URL http://arxiv.org/\nabs/1606.06031.\nPennington, J., Socher, R., and Manning, C. D. Glove:\nGlobal vectors for word representation, 2014.\nURL\nhttps://www.aclweb.org/anthology/D14-\n1162.\nPeters, M. E., Neumann, M., Iyyer, M., Gardner, M., Clark,\nC., Lee, K., and Zettlemoyer, L. Deep contextualized\nword representations. CoRR, abs/1802.05365, 2018. URL\nhttp://arxiv.org/abs/1802.05365.\nRadford, A., J´ozefowicz, R., and Sutskever, I. Learning\nto generate reviews and discovering sentiment. CoRR,\nabs/1704.01444, 2017.\nRadford, A., Narasimhan, K., Salimans, T., and Sutskever,\nI. Improving language understanding by generative pre-\ntraining, 2018. URL https://blog.openai.com/\nlanguage-unsupervised/.\nRadford, A., Wu, J., Child, R., Luan, D., Amodei, D., and\nSutskever, I. Better language models and their impli-\ncations, 2019. URL https://openai.com/blog/\nbetter-language-models/.\nRaffel, C., Shazeer, N., Roberts, A., Lee, K., Narang, S.,\nMatena, M., Zhou, Y., Li, W., and Liu, P. J. Exploring\nthe limits of transfer learning with a uniﬁed text-to-text\ntransformer. arXiv:1910.10683, 2019.\nRajpurkar, P., Zhang, J., Lopyrev, K., and Liang, P. Squad:\n100,000+ questions for machine comprehension of text.\nEMNLP, 2016.\nRajpurkar, P., Jia, R., and Liang, P. Know what you dont\nknow: Unanswerable questions for squad. ACL, 2018.\nRamachandran, P., Liu, P. J., and Le, Q. V. Unsupervised\npretraining for sequence to sequence learning. CoRR,\nabs/1611.02683, 2016. URL http://arxiv.org/\nabs/1611.02683.\nShazeer, N., Cheng, Y., Parmar, N., Tran, D., Vaswani, A.,\nKoanantakool, P., Hawkins, P., Lee, H., Hong, M., Young,\nC., Sepassi, R., and Hechtman, B. Mesh-TensorFlow:\nDeep learning for supercomputers. In Neural Information\nProcessing Systems, 2018.\nTrinh, T. H. and Le, Q. V. A simple method for common-\nsense reasoning. CoRR, abs/1806.02847, 2018. URL\nhttp://arxiv.org/abs/1806.02847.\nTurian, J., Ratinov, L., and Bengio, Y. Word representations:\nA simple and general method for semi-supervised learn-\ning. In Proceedings of the 48th Annual Meeting of the\nAssociation for Computational Linguistics, ACL ’10, pp.\n384–394, Stroudsburg, PA, USA, 2010. Association for\nComputational Linguistics.\n\nMegatron-LM: Training Multi-Billion Parameter Language Models Using Model Parallelism\nValiant, L. G. A bridging model for parallel computation.\nCommunications of the ACM, 33(8):103-111, 1990.\nVaswani, A., Shazeer, N., Parmar, N., Uszkoreit, J., Jones,\nL., Gomez, A. N., Kaiser, L., and Polosukhin, I. Attention\nis all you need. CoRR, abs/1706.03762, 2017.\nWang, A., Singh, A., Michael, J., Hill, F., Levy, O., and\nBowman, S. R. Glue: A multi-task benchmark and analy-\nsis platform for natural language understanding. ICLR,\n2019.\nYang, Z., Dai, Z., Yang, Y., Carbonell, J. G., Salakhut-\ndinov, R., and Le, Q. V.\nXlnet: Generalized autore-\ngressive pretraining for language understanding. CoRR,\nabs/1906.08237, 2019. URL http://arxiv.org/\nabs/1906.08237.\nYou, Y., Gitman, I., and Ginsburg, B. Large batch training\nof convolutional networks. arXiv:1708.03888, 2017.\nYou, Y., Li, J., Reddi, S., Hseu, J., Kumar, S., Bhojana-\npalli, S., Song, X., Demmel, J., and Hsieh, C.-J. Large\nbatch optimization for deep learning: Training bert in 76\nminutes. arXiv:1904.00962, 2019.\nZellers, R., Holtzman, A., Rashkin, H., Bisk, Y., Farhadi,\nA., Roesner, F., and Choi, Y. Defending against neural\nfake news. CoRR, abs/1905.12616, 2019. URL http:\n//arxiv.org/abs/1905.12616.\nZhu, Y., Kiros, R., Zemel, R. S., Salakhutdinov, R., Urta-\nsun, R., Torralba, A., and Fidler, S. Aligning books and\nmovies: Towards story-like visual explanations by watch-\ning movies and reading books. CoRR, abs/1506.06724,\n2015.\nA. BERT Finetuning Hyperparameters\nTable 6 presents the hyperparameters used for each model\nand task during ﬁnetuning.\nB. Model Parallel Supplementary Material\nIn this section, we present further details about the hybrid\nmodel and data parallelism and handling random number\ngeneration.\nB.1. Hybrid Model and Data Parallelism\nModel parallelism is orthogonal to data parallelism, and so\nwe can use both simultaneously to train large models in a\nreasonable amount of time. Figure 8 shows a grouping of\nGPUs for hybrid model and data parallelism. Two or more\nGPUs within the same server form model parallel groups\n(for example GPUs 1 to 8 in Figure 8), and contain one\nTable 6. Hyperparameters for ﬁnetuning BERT model on down-\nstream tasks.\nTask\nModel\nBatch\nLearning\nTraining\nsize\nrate\nepochs\n336M\nMNLI\n1.3B\n128\n1e-5\n10\n3.8B\n336M\n128\n5e-5\nQQP\n1.3B\n128\n3e-5\n12\n3.8B\n256\n4e-5\n336M\n64\n3e-5\nSQUAD 1.1\n1.3B\n48\n3e-5\n2\n3.8B\n48\n1e-5\n336M\n48\n3e-5\nSQUAD 2.0\n1.3B\n64\n3e-5\n2\n3.8B\n48\n1e-5\n336M\n32\n2e-5\nRACE\n1.3B\n16\n1e-5\n3\n3.8B\n32\n2e-5\ninstance of the model distributed across these GPUs. The\nremaining GPUs, which could be within the same server but\nmore typically are located in other servers, run additional\nmodel parallel groups. GPUs with the same position in each\nof the model parallel groups (for example GPUs 1, 9, ...,\n505 in Figure 8) form data parallel groups so that all GPUs\nwithin a data parallel group hold the same model param-\neters. During back propagation we run multiple gradient\nall-reduce operations in parallel to reduce weight gradients\nwithin each distinct data parallel group. The total number\nof required GPUs is the product of the number of model\nand data parallel groups. For example, for the 8.3 billion\nparameter model we use 8 GPUs per model parallel group\nand 64-way data parallelism, for a total of 512 GPUs. All\ncommunication is implemented in PyTorch by Python calls\nto NCCL. GPUs within each model parallel group perform\nall-reduces amongst all GPUs within the group. For data\nparallelism, each of the all-reduce operations takes place\nwith one of the GPUs from each model parallel group.\nB.2. Model Parallel Random Number Generation\nTechniques that utilize random number generation, such\nas dropout, are a staple of modern deep learning training.\nTransformers have dropout layers outside the model parallel\nregions before residual connections and within model paral-\nlel regions in the self attention block. Because some dropout\nlayers are in a model parallel region, while others are not,\nwe need to treat random number generation carefully to\nensure dropout works correctly. To synchronize residual\nconnection dropout across model parallel workers we seed\nthe random number generators at the beginning of training\nwith the same seed. This results in identical dropout patterns\nacross all model parallel workers. However, dropout within\na model parallel region should result in different random\n\nMegatron-LM: Training Multi-Billion Parameter Language Models Using Model Parallelism\npatterns for each worker to achieve randomness across the\nentire operation. To achieve this we maintain a separate\nrandom number generator for dropout within model parallel\nregions. This random number generator is uniquely seeded\nfor each model parallel worker.\nFigure 8. Grouping of GPUs for hybrid model and data parallelism\nwith 8-way model parallel and 64-way data parallel.\nC. Text Samples\nBelow are some text samples generated by Megatron-LM\nusing a context prompt. Some of the texts are cut short.\nContext: There is a major force uniting Americas ﬁercely\npartisan politicians: big technology companies. Democrats\nand Republicans at the federal and state levels are coming\ntogether to scrutinize the power of the Silicon Valley giants\nand, potentially, to rein them in. Letitia James, the Demo-\ncratic attorney general of New York, announced on Friday\nthat attorneys general in eight states four Democrats and\nfour Republicans and the District of Columbia had begun\nan antitrust investigation of Facebook.\nMegatron-LM: The probe, which Ms. James said was\nlikely to result in a lawsuit, is the latest in a string of high-\nproﬁle regulatory actions, in state and federal courts, that\nchallenge the market dominance of tech companies. The\nJustice Department is also investigating Facebook’s data\npractices and whether the company violated a 2011 consent\ndecree with the Federal Trade Commission that required\nthe company to obtain explicit consent from users before\nsharing their personal information with third parties.\nWhat might prompt a bipartisan coalition to sue Facebook?\nIt isn’t clear that there’s any one thing. But lawmakers and\nregulators have been increasingly alarmed by the company’s\nrecent controversies, which include a data breach affecting\nmore than 50 million users, the spread of fake news on\nthe platform, and the spreading of propaganda by Russian-\nbacked accounts during the 2016 election.\nIt’s also not clear how serious the Democratic attorneys gen-\neral are about ﬁling a lawsuit. While the attorneys general\nannounced their investigation in a press release, they said\nthey had not yet reached a decision on whether to sue. In\nthe past, some of the attorneys general have focused their\nefforts on trying to extract settlements from the tech compa-\nnies without ﬁling lawsuits.\n”As attorneys general who have investigated and prosecuted\nhealth-care fraud, price-ﬁxing, and other violations of our\nstates’ antitrust laws, we are committed to holding compa-\nnies accountable when they fail to protect consumers and\ncompetition in the market,” Ms. James said in a statement.\nThe investigation, which was ﬁrst reported by The New\nYork Times, was announced as the companies that dom-\ninate the tech industry are facing scrutiny from antitrust\nregulators around the world. Google, Twitter, Facebook\nand Amazon have faced a series of antitrust probes and\nclass-action lawsuits in the United States and Europe ...\n\nMegatron-LM: Training Multi-Billion Parameter Language Models Using Model Parallelism\nContext: A ﬂower, sometimes known as a bloom or\nblossom, is the reproductive structure found in ﬂowering\nplants (plants of the division Magnoliophyta, also called\nangiosperms). The biological function of a ﬂower is to ef-\nfect reproduction, usually by providing a mechanism for the\nunion of sperm with eggs.\nMegatron-LM: The structural components of the ﬂower\nare the petals and sepals, which are the reproductive organs,\nthe hypanthium, which forms the base of the ﬂower, and\nthe calyx, which is the stigma. A ﬂower is a composite\nstructure, made up of sepals and petals, the sepals being\ncomposed of three fused tepals and the petals of two fused\npetals, though there are a few exceptions to this rule. The\nthree sepals are fused to form a tube called the calyx, which\nis the structure that attracts the pollinating insects. The\npetals are composed of two fused sepals (the outer ones)\nwhich open and fold inward (clasp) at the tips, forming\nthe petals. The four sepals and petals are united to form\na corolla, the outermost structure of a ﬂower that carries\npollen.\nThe structure of the ﬂower has important functional signif-\nicance, including protection of the ovary from insects and\nthe transfer of pollen between the male and female parts\nof a ﬂower. The structure of the ﬂower is also used by the\nplants to attract pollinators, for example by the shape of the\npetals and the colours of the stamens.\nThe structure of a ﬂower varies widely between plants. The\nsimplest ﬂowers are unbranched and made up of only two\nsepals and a single petal. Other examples are composed\nof several petals joined together, with the innermost sepals\ncompletely fused to form a tube called the calyx. The petals\nare usually coloured to attract insects, which can only see\ncertain wavelengths of light. The sepals are often coloured\ntoo, but usually have a more limited range of colours.\nSome ﬂowers have additional structures, such as stamens,\nanthers and ovaries. The presence of stamens and anthers\nin some ﬂowers is used for cross-pollination. The ovary is\nthe organ where the female gametes are produced, and is\nthe ﬁrst structure that is developed in an embryo of a ﬂower.\nThe ovary is the female ﬂower’s reproductive system, and\nconsists of the ovary wall, the ovule, and the oviduct.\nFlowers vary in their size and shape, from tiny to huge, and\nare often brightly coloured. Flowers may be seen as part of\na larger structure, such as a ﬂower head, an inﬂorescence, a\nbract, a spathe, or a carpels.\nThe term ”ﬂower” is usually applied to the ﬂoral structures\nthat are visible externally, but many ...\nContext: OpenAI did not release the full GPT-2 model\ndue to concerns of malicious use, but they did release a\nsmaller version equivalent in size to the original GPT (117\nM parameters), trained on the new, larger dataset. Although\nnot as powerful as the large model, the smaller version still\nhas some language generation chops.\nMegatron-LM: In December 2016, DeepMind released an\neven larger model, GPT-3, with more than 1.5 billion pa-\nrameters. DeepMind has no plans to release the model, so\nthe question remains whether other AI researchers will be\nable to copy and build on it.\nOther language models have been built using other tech-\nniques. In 2016, a team of researchers at Columbia Uni-\nversity, the University of Toronto, and Google DeepMind\nshowed that neural network-based models, including deep\nneural networks, can learn the grammatical rules of a lan-\nguage. A group at Google DeepMind demonstrated that\nthese rules could be represented in a network as a kind of\ndecision tree, where the nodes in the tree are the words in\nthe sentence, and the rules for the words are represented\nby the decision nodes at the end of the tree. The network\nwould go through the network in order to choose the next\nword in the sentence based on whether it follows the rules\nor not. In 2017, the team at DeepMind released a paper\ndetailing the training and performance of their model.\nThe problems with the model are many. One of the biggest\nissues is that grammatical rules are ambiguous. For exam-\nple, the phrase ”I came down” could mean I came down\nfrom the stairs, or I came down from the sky. In English,\nthere are numerous other examples of sentences that are\nambiguous. The model does not have any way to tell the dif-\nference between the ambiguous sentences, and so it simply\nmakes a prediction about the entire sentence based on what\nrules it does know. This makes the model vulnerable to the\nphenomenon of overﬁtting. In order to avoid this, models\nmust be trained using a large set of data, so that the model\ncan ﬁnd useful patterns in the data, and use those patterns\nto help make predictions. This approach is also prone to\nerrors, as it relies on the training data to help make correct\npredictions, and the data might be incorrectly labeled, re-\nsulting in a model that produces a poor set of predictions.\nIn the case of GPT-2, the model only has about 100 training\nsentences for each type of phrase (subject-verb-object).\nAdditionally, languages have different rules for different\ntypes of phrasing. In English, the subject-verb-object con-\nstruction is fairly universal, but other languages have differ-\nent rules, which makes training a GPT more complicated.\nFinally, there is an inherent trade-off between the number\nof rules and the performance of the model ...\n\nMegatron-LM: Training Multi-Billion Parameter Language Models Using Model Parallelism\nContext: The 36th International Conference on Machine\nLearning (ICML 2019) will be held in Long Beach, CA,\nUSA from June 10th to June 15th, 2019. The conference\nwill consist of one day of tutorials (June 10), followed\nby three days of main conference sessions (June 11-13),\nfollowed by two days of workshops (June 14-15).\nMegatron-LM: With a broad scope, the conference ad-\ndresses the challenges and opportunities in machine learning\nfor practitioners and researchers. The conference program\nwill feature sessions that cover different machine learning\ntopics, from reinforcement learning to computational biol-\nogy and from AI to deep learning. It is the world’s premier\nforum for exploring and presenting advances in machine\nlearning.\nICML 2019 will offer an impressive range of sessions,\nwhich will be of interest to practitioners, researchers, and\nstudents. Some of the topics we will cover include:\nReinforcement learning\nSoft Computing\nComputational biology\nConvolutional networks\nDeep learning\nStatistical learning theory\nStatistical learning machine\nNatural language processing\nBiology\nAwards\nSince its ﬁrst edition, the conference has grown and evolved.\nToday, it is the premier forum for showcasing state-of-the-\nart research in machine learning. The conference is widely\nrecognized as the preeminent venue for showcasing the\nlatest research in machine learning, and a place where prac-\ntitioners, researchers, and students can exchange ideas and\ninsights.\nThe conference is a peer-reviewed venue that draws its pres-\ntige and inﬂuence from the excellence of the presentations\nand the quality of the invited speakers. With over 50% of all\ntalks selected based on their content by an expert committee,\nit is clear that the conference’s high standards of research\nand quality keep a very high bar.\nContext:\nRescue workers on Tuesday found a ﬂight\nrecorder from the Russian plane that crashed into the Black\nSea over the weekend, the defense ministry said. All 92\npeople aboard the Russian military’s Tu-154 plane are be-\nlieved to have died Sunday morning when it crashed two\nminutes after taking off from the southern Russian city of\nSochi.\nMegatron-LM: A child holds up a portrait of a dead relative\nat a makeshift morgue in a building belonging to a local\ngovernment in the town of Petropavlovsk-Kamchatsky. A\nRussian Defense Ministry Tu-154 plane with 92 people\non board crashed into the Black Sea on Sunday morning\nshortly after taking off from the southern Russian city of\nSochi. (Yuri Kochetkov/EPA)\nAccording to the ministry, the plane was found at a depth\nof more than 1,000 feet and the recorder was damaged.\n”The defense ministry has decided to hand over the recorder\nto the Interstate Aviation Committee, which is the main\ninvestigative body in the air crash,” the statement said.\nThe search for the plane’s black boxes is being hampered by\nstrong winds and rough seas. Investigators have launched a\ncriminal probe into the crash, which ofﬁcials said was likely\ncaused by a technical fault.\nRussian President Vladimir Putin declared Wednesday a\nday of mourning for the victims.\nD. Further Scaling Analysis\nIn this section we study the effect of number of attention\nheads on the scaling results. We also present strong scaling\nresults for our 1.2 billion parameter model.\nD.1. Attention Heads and Scaling\nThis section studies the effect of attention heads on model\nparallel scaling. To this end, we consider the 8.3 billion\nparameter conﬁguration with 8-way model parallelism and\nvary the number of heads from 16 to 32. The results are\npresented in Table 7. As the number of attention heads\nincreases, some of the GEMMS inside the self-attention\nlayer become smaller and also the number of elements in\nthe self attention softmax increases. This results in a slight\ndecrease in scaling efﬁciency. Future research should be\nwary of this hyperparameter to design large transformer\nmodels that balance model speed and model accuracy.\nD.2. Strong Scaling\nOur model parallelism is primarily designed to enable train-\ning models larger than what can ﬁt in the memory of a\n\nMegatron-LM: Training Multi-Billion Parameter Language Models Using Model Parallelism\nTable 7. Effect of number of attention heads on scaling on 8.3\nbillion of parameters with 8-way model parallelism.\nAttention heads\nHidden size per head\nScaling Efﬁciency\n16\n192\n82%\n24\n128\n80%\n32\n96\n77%\nTable 8. Speedup obtained for the 1.2 billion parameters model\nusing model parallelism while keeping the batch size constant.\n# of GPUs\n1\n2\n4\n8\nSpeedup\n1.0\n1.64\n2.34\n2.98\nsingle GPU, but it can also accelerate the training of smaller\nmodels without increasing the batch size. To measure this\nacceleration we train a model with a ﬁxed 1.2 billion parame-\nters. We use a ﬁxed batch size of 8 samples per iteration and\nincrease the number of GPUs using model parallelism. The\nresults are listed in Table 8. Using two GPUs makes training\n64% faster. Above that we see diminishing returns as the\nper-GPU computation decreases and the memory bandwidth\nand communication overheads begin to dominate.\nE. Evaluating Language Models Using\nWikiText103 and LAMBADA\nIn this section we detail our evaluation methodology for the\nWikiText103 dataset (Merity et al., 2016) and cloze-style\nprediction accuracy on the LAMBADA dataset(Paperno\net al., 2016).\nE.1. Wikitext103 Perplexity\nWikiText103 perplexity is an evaluation criterion that has\nbeen well studied over the past few years since the creation\nof the benchmark dataset. Perplexity is the exponentiation\nof the average cross entropy of a corpus (Mikolov et al.,\n2011). This makes it a natural evaluation metric for lan-\nguage models which represent a probability distribution\nover entire sentences or texts.\nPPL = exp(−1\nTo\nT\nX\nt\nlogP(t|0 : t −1))\n(4)\nTo calculate perplexity in (4) we tokenize the WikiText103\ntest corpus according to our subword vocabulary and sum\nthe cross entropy loss from each token [0, T]. We then nor-\nmalize the cross entropy loss by the number of tokens in the\noriginal tokenization scheme To. The WikiText103 test cor-\npus already comes pre-tokenized with word level tokens that\nprior works have used to compute perplexity. To evaluate\nour models’ perplexities on a level playing ﬁeld with prior\nworks we must normalize by the original number of tokens,\nTo, rather than the number of tokens, T, actually in the tok-\nenized data fed as input to our model. This pre-tokenization\nalso introduces artifacts in the text that are not present in our\ntraining data. To alleviate this distributional mismatch, we\nﬁrst preprocess the WikiText103 test dataset with invertible\ndetokenizers to remove various artifacts related to punctua-\ntion and whitespace. The value of To is calculated before\nthis preprocessing. For WikiText103’s test set To = 245566\nand T = 270329.\nWe must also make one further transformer-speciﬁc mod-\niﬁcation to the perplexity calculation. Unlike RNN-based\nlanguage models, transformers operate on a ﬁxed window in-\nput size. Therefore they cannot fully calculate P(t|0 : t−1)\nand can only calculate P(t|t −w : t −1) where w is the\nsize of our context: 1024 tokens. However, calculating this\nvalue for every token in our dataset is prohibitively expen-\nsive since we must compute approximately T evaluations\nof a w sized context. To evaluate our models efﬁciently we\ntake a middle ground approach termed overlapping evalu-\nation where we advance the sliding window by some over-\nlap o each time and only compute the cross entropy losses\ncorresponding to the last o tokens of the window. In our\nexperiments we utilize an overlap o of 32, and compute\nlosses over all sliding windows in such a fashion.\nE.2. LAMBADA Cloze Accuracy\nThe capability to handle long term contexts is crucial for\nstate of the art language models and is a necessary prerequi-\nsite for problems like long-form generation and document-\nbased question answering. Cloze-style datasets like LAM-\nBADA are designed to measure a model’s ability to operate\nin and reason about these types of long term contexts. Cloze-\nstyle reading comprehension uses a context of word tokens\nx = x1:t with one token xj masked; the models objective\nis to correctly predict the value of the missing jth token. To\naccurately predict the missing token, the model requires an\nin-depth understanding of the surrounding context and how\nlanguage should be used in such a context. LAMBADA\nuses cloze-style reading comprehension to test generative\nleft-to-right language models by constructing examples of 4-\n5 sentences where the last word in the context xt is masked.\nOur models utilize subword units, so for LAMBADA evalu-\nation we utilize the raw, unprocessed LAMBADA dataset\nand require that our model predict the multiple subword\ntokens that make up the word token. We use teacher forc-\ning, and consider an answer correct only when all output\npredictions are correct. This formulation is equivalent to the\noriginal task of word token prediction.\n"
    },
    {
      "arxiv_id": "2110.02861",
      "full_text": "Published as a conference paper at ICLR 2022\n8-BIT OPTIMIZERS VIA BLOCK-WISE QUANTIZATION\nTim Dettmers∗‡\nMike Lewis∗\nSam Shleifer∗\nLuke Zettlemoyer∗‡\nFacebook AI Research∗, {mikelewis,sshleifer}@fb.com\nUniversity of Washington‡, {dettmers,lsz}@cs.washington.edu\nABSTRACT\nStateful optimizers maintain gradient statistics over time, e.g., the exponentially\nsmoothed sum (SGD with momentum) or squared sum (Adam) of past gradi-\nent values. This state can be used to accelerate optimization compared to plain\nstochastic gradient descent but uses memory that might otherwise be allocated to\nmodel parameters, thereby limiting the maximum size of models trained in prac-\ntice. In this paper, we develop the ﬁrst optimizers that use 8-bit statistics while\nmaintaining the performance levels of using 32-bit optimizer states. To overcome\nthe resulting computational, quantization, and stability challenges, we develop\nblock-wise dynamic quantization. Block-wise quantization divides input tensors\ninto smaller blocks that are independently quantized. Each block is processed in\nparallel across cores, yielding faster optimization and high precision quantization.\nTo maintain stability and performance, we combine block-wise quantization with\ntwo additional changes: (1) dynamic quantization, a form of non-linear optimiza-\ntion that is precise for both large and small magnitude values, and (2) a stable em-\nbedding layer to reduce gradient variance that comes from the highly non-uniform\ndistribution of input tokens in language models. As a result, our 8-bit optimizers\nmaintain 32-bit performance with a small fraction of the memory footprint on\na range of tasks, including 1.5B parameter language modeling, GLUE ﬁnetun-\ning, ImageNet classiﬁcation, WMT’14 machine translation, MoCo v2 contrastive\nImageNet pretraining+ﬁnetuning, and RoBERTa pretraining, without changes to\nthe original optimizer hyperparameters. We open-sourceour 8-bit optimizers as a\ndrop-in replacement that only requires a two-line code change.\nIncreasing model size is an effective way to achieve better performance for given resources (Kaplan\net al., 2020; Henighan et al., 2020; Raffel et al., 2019; Lewis et al., 2021). However, training such\nlarge models requires storing the model, gradient, and state of the optimizer (e.g., exponentially\nsmoothed sum and squared sum of previous gradients for Adam), all in a ﬁxed amount of available\nmemory. Although signiﬁcant research has focused on enabling larger model training by reducing or\nefﬁciently distributing the memory required for the model parameters (Shoeybi et al., 2019; Lepikhin\net al., 2020; Fedus et al., 2021; Brown et al., 2020; Rajbhandari et al., 2020), reducing the memory\nfootprint of optimizer gradient statistics is much less studied. This is a signiﬁcant missed opportunity\nsince these optimizer states use 33-75% of the total memory footprint during training. For example,\nthe Adam optimizer states for the largest GPT-2 (Radford et al., 2019) and T5 (Raffel et al., 2019)\nmodels are 11 GB and 41 GB in size. In this paper, we develop a fast, high-precision non-linear\nquantization method – block-wise dynamic quantization – that enables stable 8-bit optimizers (e.g.,\nAdam, AdamW, and Momentum) which maintain 32-bit performance at a fraction of the memory\nfootprint and without any changes to the original hyperparameters.1\nWhile most current work uses 32-bit optimizer states, recent high-proﬁle efforts to use 16-bit opti-\nmizers report difﬁcultly for large models with more than 1B parameters (Ramesh et al., 2021). Going\nfrom 16-bit optimizers to 8-bit optimizers reduces the range of possible values from 216 = 65536\nvalues to just 28 = 256. To our knowledge, this has not been attempted before.\nEffectively using this very limited range is challenging for three reasons: quantization accuracy,\ncomputational efﬁciency, and large-scale stability. To maintain accuracy, it is critical to introduce\nsome form of non-linear quantization to reduce errors for both common small magnitude values\n1We study 8-bit optimization with current best practice model and gradient representations (typically 16-bit\nmixed precision), to isolate optimization challenges. Future work could explore further compressing all three.\n1\narXiv:2110.02861v2  [cs.LG]  20 Jun 2022\n\nPublished as a conference paper at ICLR 2022\n-3.1       0.1         -0.03       1.2\n-1.0      0.032      -0.025      1.0\n3.1                       1.2\n-1.0      0.0329     -0.0242      1.0\n0            170            80          255\n-1.0*3.1  0.0329*3.1  -0.0242*1.2  1.0*1.2\n-3.1       0.102         -0.029       1.2\n0            170            80          255\n-1.0        0.0329       -0.0242        1.0\nQuantization\nDequantization\nOptimizer State\nChunk into blocks\nFind block-wise absmax\nNormalize with absmax\nFind closest 8-bit value\n-3.1       0.1         -0.03       1.2\nFind corresponding index\nStore index values\nLoad Index values\nIndex\nLookup values\nDenormalize by\n     absmax\nDequantized \noptimizer states\nUpdated optimizer states\nUpdate optimizer states\nFigure 1: Schematic of 8-bit optimizers via block-wise dynamic quantization, see Section 2 for more\ndetails. After the optimizer update is performed in 32-bit, the state tensor is chunked into blocks,\nnormalized by the absolute maximum value of each block. Then dynamic quantization is performed,\nand the index is stored. For dequantization, a lookup in the index is performed, with subsequent de-\nnormalization by multiplication with the block-wise absolute maximum value. Outliers are conﬁned\nto a single block through block-wise quantization, and their effect on normalization is limited.\nand rare large ones. However, to be practical, 8-bit optimizers need to be fast enough to not slow\ndown training, which is especially difﬁcult for non-linear methods that require more complex data\nstructures to maintain the quantization buckets. Finally, to maintain stability with huge models\nbeyond 1B parameters, a quantization method needs to not only have a good mean error but excellent\nworse case performance since a single large quantization error can cause the entire training run to\ndiverge.\nWe introduce a new block-wise quantization approach that addresses all three of these challenges.\nBlock-wise quantization splits input tensors into blocks and performs quantization on each block in-\ndependently. This block-wise division reduces the effect of outliers on the quantization process since\nthey are isolated to particular blocks, thereby improving stability and performance, especially for\nlarge-scale models. Block-wise processing also allows for high optimizer throughput since each nor-\nmalization can be computed independently in each core. This contrasts with tensor-wide normaliza-\ntion, which requires slow cross-core synchronization that is highly dependent on task-core schedul-\ning. We combine block-wise quantization with two novel methods for stable, high-performance 8-bit\noptimizers: dynamic quantization and a stable embedding layer. Dynamic quantization is an exten-\nsion of dynamic tree quantization for unsigned input data. The stable embedding layer is a variation\nof a standard word embedding layer that supports more aggressive quantization by normalizing the\nhighly non-uniform distribution of inputs to avoid extreme gradient variation.\nOur 8-bit optimizers maintain 32-bit performance at a fraction of the original memory footprint.\nWe show this for a broad range of tasks: 1.5B and 355M parameter language modeling, GLUE\nﬁnetuning, ImageNet classiﬁcation, WMT’14+WMT’16 machine translation, MoCo v2 contrastive\nimage pretraining+ﬁnetuning, and RoBERTa pretraining. We also report additional ablations and\nsensitivity analysis showing that all components – block-wise quantization, dynamic quantization,\nand stable embedding layer – are crucial for these results and that 8-bit Adam can be used as a simple\ndrop-in replacement for 32-bit Adam, with no hyperparameter changes. We open-source our custom\nCUDA kernels and provide a PyTorch implementation that enables 8-bit optimization by changing\ntwo lines of code.\n1\nBACKGROUND\n1.1\nSTATEFUL OPTIMIZERS\nAn optimizer updates the parameters w of a neural network by using the gradient of the loss with\nrespect to the weight gt =\n∂L\n∂w at update iteration t. Stateful optimizers compute statistics of the\ngradient with respect to each parameter over time for accelerated optimization. Two of the most\n2\n\nPublished as a conference paper at ICLR 2022\ncommonly used stateful optimizers are Adam (Kingma and Ba, 2014), and SGD with momentum\n(Qian, 1999) – or Momentum for short. Without damping and scaling constants, the update rules of\nthese optimizers are given by:\nMomentum(gt, wt−1, mt−1) =\n\n\n\nm0 = g0\nInitialization\nmt = β1mt−1 + gt\nState 1 update\nwt = wt−1 −α · mt\nWeight update\n(1)\nAdam(gt, wt−1, mt−1, rt−1) =\n\n\n\n\n\n\n\n\n\nr0 = m0 = 0\nInitialization\nmt = β1mt−1 + (1 −β1)gt\nState 1 update\nrt = β2rt−1 + (1 −β2)g2\nt\nState 2 update\nwt = wt−1 −α ·\nmt\n√rt+ϵ\nWeight update,\n(2)\nwhere β1 and β2 are smoothing constants, ϵ is a small constant, and α is the learning rate.\nFor 32-bit states, Momentum and Adam consume 4 and 8 bytes per parameter. That is 4 GB and 8\nGB for a 1B parameter model. Our 8-bit non-linear quantization reduces these costs to 1 GB and 2\nGB.\n1.2\nNON-LINEAR QUANTIZATION\nQuantization compresses numeric representations to save space at the cost of precision. Quanti-\nzation is the mapping of a k-bit integer to a real element in D, that is, Qmap : [0, 2k −1] 7→D.\nFor example, the IEEE 32-bit ﬂoating point data type maps the indices 0...232 −1 to the do-\nmain [-3.4e38, +3.4e38].\nWe use the following notation: Qmap(i) = Qmap\ni\n= qi, for example\nQmap(231 + 131072) = 2.03125, for the IEEE 32-bit ﬂoating point data type.\nTo perform general quantization from one data type into another we require three steps. (1) Compute\na normalization constant N that transforms the input tensor T into the range of the domain D of the\ntarget quantization data type Qmap, (2) for each element of T/N ﬁnd the closest corresponding value\nqi in the domain D, (3) store the index i corresponding to qi in the quantized output tensor TQ. To\nreceive the dequantized tensor TD we look up the index and denormalize: TD\ni = Qmap(TQ\ni ) · N.\nTo perform this procedure for dynamic quantization we ﬁrst normalize into the range [-1, 1] through\ndivision by the absolute maximum value: N = max(|T|).\nThen we ﬁnd the closest values via a binary search:\nTQ\ni =\n2n\narg min\nj=0\n|Qmap\nj\n−Ti\nN |\n(3)\n1.3\nDYNAMIC TREE QUANTIZATION\nFigure 2: Dynamic tree quantization.\nDynamic Tree quantization (Dettmers, 2016) is a method\nthat yields low quantization error for both small and large\nmagnitude values. Unlike data types with ﬁxed exponent\nand fraction, dynamic tree quantization uses a datatype\nwith a dynamic exponent and fraction that can change\nwith each number. It is made up of four parts, as seen in\nFigure 2: (1) The ﬁrst bit of the data type is reserved for\na sign. (2) The number of subsequent zero bits indicates\nthe magnitude of the exponent. (3) The ﬁrst bit that is set\nto one indicates that all following values are reserved for\n(4) linear quantization. By moving the indicator bit, num-\nbers can have a large exponent 10−7 or precision as high as 1/63. Compared to linear quantization,\ndynamic tree quantization has better absolute and relative quantization errors for non-uniform dis-\ntributions. Dynamic tree quantization is strictly deﬁned to quantize numbers in the range [-1.0, 1.0],\nwhich is ensured by performing tensor-level absolute max normalization.\n3\n\nPublished as a conference paper at ICLR 2022\n2\n8-BIT OPTIMIZERS\nOur 8-bit optimizers have three components: (1) block-wise quantization that isolates outliers and\ndistributes the error more equally over all bits; (2) dynamic quantization, which quantizes both small\nand large values with high precision; and (3) a stable embedding layer to improve stability during\noptimization for models with word embeddings.\nWith these components, performing an optimizer update with 8-bit states is straightforward. We\ndequantize the 8-bit optimizer states to 32-bit, perform the update, and then quantize the states back\nto 8-bit for storage. We do this 8-bit to 32-bit conversion element-by-element in registers, which\nmeans no slow copies to GPU memory or additional temporary memory are needed to perform\nquantization and dequantization. For GPUs, this makes 8-bit optimizers faster than regular 32-bit\noptimizers, as we show in Section 3.\n2.1\nBLOCK-WISE QUANTIZATION\nOur block-wise quantization reduces the cost of computing normalization and improves quantization\nprecision by isolating outliers. In order to dynamically quantize a tensor, as deﬁned in Section 1.2,\nwe need to normalize the tensor into the range [-1, 1]. Such normalization requires a reduction over\nthe entire tensor, which entails multiple synchronizations across GPU cores. Block-wise dynamic\nquantization reduces this cost by chunking an input tensor into small blocks of size B = 2048 and\nperforming normalization independently in each core across this block.\nMore formally, using the notation introduced in Section 1.2, in block-wise quantization, we treat\nT as a one-dimensional sequence of elements that we chunk in blocks of size B. This means for\nan input tensor T with n elements we have n/B blocks. We proceed to compute a normalization\nconstant for each block: Nb = max(|Tb|), where b is the index of the block 0..n/B. With this\nblock-wise normalization constant, each block can be quantized independently:\nTQ\nbi =\n2n\narg min\nj=0\n|Qmap\nj\n−Tbi\nNb\n|\n\f\f\f\f\n0<i<B\n(4)\nThis approach has several advantages, both for stability and efﬁciency. First, each block normal-\nization can be computed independently. Thus no synchronization between cores is required, and\nthroughput is enhanced.\nSecondly, it is also much more robust to outliers in the input tensor. For example, to contrast block-\nwise and regular quantization, if we create an input tensor with one million elements sampled from\nthe standard normal distribution, we expect less than 1% of elements of the tensor will be in the\nrange [3, +∞). However, since we normalize the input tensor into the range [-1,1] this means the\nmaximum values of the distribution determine the range of quantization buckets. This means if the\ninput tensor contains an outlier with magnitude 5, the quantization buckets reserved for numbers\nbetween 3 and 5 will mostly go unused since less than 1% of numbers are in this range. With block-\nwise quantization, the effect of outliers is limited to a single block. As such, most bits are used\neffectively in other blocks.\nFurthermore, because outliers represent the absolute maximum value in the input tensor, block-\nwise quantization approximates outlier values without any error. This guarantees that the largest\noptimizer states, arguably the most important, will always be quantized with full precision. This\nproperty makes block-wise dynamic quantization both robust and precise and is essential for good\ntraining performance in practice.\n2.2\nDYNAMIC QUANTIZATION\nIn this work, we extend dynamic tree quantization (Section 1.3) for non-signed input tensors by\nre-purposing the sign bit. Since the second Adam state is strictly positive, the sign bit is not needed.\nInstead of just removing the sign bit, we opt to extend dynamic tree quantization with a ﬁxed bit\nfor the fraction. This extension is motivated by the observation that the second Adam state varies\naround 3-5 orders of magnitude during the training of a language model. In comparison, dynamic\ntree quantization already has a range of 7 orders of magnitude. We refer to this quantization as\n4\n\nPublished as a conference paper at ICLR 2022\ndynamic quantization to distinguish it from dynamic tree quantization in our experiments. A study\nof additional quantization data types and their performance is detailed in Appendix F.\n2.3\nSTABLE EMBEDDING LAYER\nOur stable embedding layer is a standard word embedding layer variation (Devlin et al., 2019)\ndesigned to ensure stable training for NLP tasks. This embedding layer supports more aggressive\nquantization by normalizing the highly non-uniform distribution of inputs to avoid extreme gradient\nvariation. See Appendix C for a discussion of why commonly adopted embedding layers (Ott et al.,\n2019) are so unstable.\nWe initialize the Stable Embedding Layer with Xavier uniform initialization (Glorot and Bengio,\n2010) and apply layer normalization (Ba et al., 2016) before adding position embeddings. This\nmethod maintains a variance of roughly one both at initialization and during training. Additionally,\nthe uniform distribution initialization has less extreme values than a normal distribution, reducing\nmaximum gradient size. Like Ramesh et al. (2021), we ﬁnd that the stability of training improves\nsigniﬁcantly if we use 32-bit optimizer states for the embedding layers. This is the only layer that\nuses 32-bit optimizer states. We still use the standard precision for weights and gradients for the\nembedding layers – usually 16-bit. We show in our Ablation Analysis in Section 4 that the Stable\nEmbedding Layer is required for stable training. See ablations for the Xavier initialization, layer\nnorm, and 32-bit state components of the Stable Embedding Layer in Appendix I.\n3\n8-BIT VS 32-BIT OPTIMIZER PERFORMANCE FOR COMMON BENCHMARKS\nExperimental Setup\nWe compare the performance of 8-bit optimizers to their 32-bit counterparts\non a range of challenging public benchmarks. These benchmarks either use Adam (Kingma and Ba,\n2014), AdamW (Loshchilov and Hutter, 2018), or Momentum (Qian, 1999).\nWe do not change any hyperparameters or precision of weights, gradients, and activations/input gra-\ndients for each experimental setting compared to the public baseline– the only change is to replace\n32-bit optimizers with 8-bit optimizers. This means that for most experiments, we train in 16-bit\nmixed-precision (Micikevicius et al., 2017). We also compare with Adafactor (Shazeer and Stern,\n2018), with the time-independent formulation for β2 (Shazeer and Stern, 2018) – which is the same\nformulation used in Adam. We also do not change any hyperparameters for Adafactor.\nWe report on benchmarks in neural machine translation (Ott et al., 2018)2 trained on WMT’16\n(Sennrich et al., 2016) and evaluated on en-de WMT’14 (Mach´aˇcek and Bojar, 2014), large-scale\nlanguage modeling (Lewis et al., 2021; Brown et al., 2020) and RoBERTa pretraining (Liu et al.,\n2019) on English CC-100 + RoBERTa corpus (Nagel, 2016; Gokaslan and Cohen, 2019; Zhu et al.,\n2015; Wenzek et al., 2020), ﬁnetuning the pretrained masked language model RoBERTa (Liu et al.,\n2019)3 on GLUE (Wang et al., 2018a), ResNet-50 v1.5 image classiﬁcation (He et al., 2016)4 on\nImageNet-1k (Deng et al., 2009), and Moco v2 contrastive image pretraining and linear ﬁnetuning\n(Chen et al., 2020b)5 on ImageNet-1k (Deng et al., 2009).\nWe use the stable embedding layer for all NLP tasks except for ﬁnetuning on GLUE. Beyond this, we\nfollow the exact experimental setup outlined in the referenced papers and codebases. We consistently\nreport replication results for each benchmark with public codebases and report median accuracy,\nperplexity, or BLEU over ten random seeds for GLUE, three random seeds for others tasks, and\na single random seed for large scale language modeling. While it is standard to report means and\nstandard errors on some tasks, others use median performance. We opted to report medians for all\ntasks for consistency.\nResults\nIn Table 1, we see that 8-bit optimizers match replicated 32-bit performance for all tasks.\nWhile Adafactor is competitive with 8-bit Adam, 8-bit Adam uses less memory and provides faster\noptimization. Our 8-bit optimizers save up to 8.5 GB of GPU memory for our largest 1.5B pa-\n2https://github.com/pytorch/fairseq/tiny/master/examples/scaling_nmt/README.md\n3https://github.com/pytorch/fairseq/blob/master/examples/roberta/README.glue.md\n4https://github.com/NVIDIA/DeepLearningExamples/tree/master/PyTorch/Classification/ConvNets/\n5https://github.com/facebookresearch/moco\n5\n\nPublished as a conference paper at ICLR 2022\nTable 1: Median performance on diverse NLP and computer vision tasks: GLUE, object classiﬁ-\ncation with (Moco v2) and without pretraining (CLS), machine translation (MT), and large-scale\nlanguage modeling (LM). While 32-bit Adafactor is competitive with 8-bit Adam, it uses almost\ntwice as much memory and trains slower. 8-bit Optimizers match or exceed replicated 32-bit per-\nformance on all tasks. We observe no instabilities for 8-bit optimizers. Time is total GPU time on\nV100 GPUs, except for RoBERTa and GPT3 pretraining, which were done on A100 GPUs.\nOptimizer\nTask\nData\nModel\nMetric†\nTime\nMem saved\n32-bit AdamW\nGLUE\nMultiple\nRoBERTa-Large\n88.9\n–\nReference\n32-bit AdamW\nGLUE\nMultiple\nRoBERTa-Large\n88.6\n17h\n0.0 GB\n32-bit Adafactor\nGLUE\nMultiple\nRoBERTa-Large\n88.7\n24h\n1.3 GB\n8-bit AdamW\nGLUE\nMultiple\nRoBERTa-Large\n88.7\n15h\n2.0 GB\n32-bit Momentum\nCLS\nImageNet-1k\nResNet-50\n77.1\n–\nReference\n32-bit Momentum\nCLS\nImageNet-1k\nResNet-50\n77.1\n118h\n0.0 GB\n8-bit Momentum\nCLS\nImageNet-1k\nResNet-50\n77.2\n116 h\n0.1 GB\n32-bit Adam\nMT\nWMT’14+16\nTransformer\n29.3\n–\nReference\n32-bit Adam\nMT\nWMT’14+16\nTransformer\n29.0\n126h\n0.0 GB\n32-bit Adafactor\nMT\nWMT’14+16\nTransformer\n29.0\n127h\n0.3 GB\n8-bit Adam\nMT\nWMT’14+16\nTransformer\n29.1\n115h\n1.1 GB\n32-bit Momentum\nMoCo v2\nImageNet-1k\nResNet-50\n67.5\n–\nReference\n32-bit Momentum\nMoCo v2\nImageNet-1k\nResNet-50\n67.3\n30 days\n0.0 GB\n8-bit Momentum\nMoCo v2\nImageNet-1k\nResNet-50\n67.4\n28 days\n0.1 GB\n32-bit Adam\nLM\nMultiple\nTransformer-1.5B\n9.0\n308 days\n0.0 GB\n32-bit Adafactor\nLM\nMultiple\nTransformer-1.5B\n8.9\n316 days\n5.6 GB\n8-bit Adam\nLM\nMultiple\nTransformer-1.5B\n9.0\n297 days\n8.5 GB\n32-bit Adam\nLM\nMultiple\nGPT3-Medium\n10.62\n795 days\n0.0 GB\n32-bit Adafactor\nLM\nMultiple\nGPT3-Medium\n10.68\n816 days\n1.5 GB\n8-bit Adam\nLM\nMultiple\nGPT3-Medium\n10.62\n761 days\n1.7 GB\n32-bit Adam\nMasked-LM\nMultiple\nRoBERTa-Base\n3.49\n101 days\n0.0 GB\n32-bit Adafactor\nMasked-LM\nMultiple\nRoBERTa-Base\n3.59\n112 days\n0.7 GB\n8-bit Adam\nMasked-LM\nMultiple\nRoBERTa-Base\n3.48\n94 days\n1.1 GB\n†Metric: GLUE=Mean Accuracy/Correlation. CLS/MoCo = Accuracy. MT=BLEU. LM=Perplexity.\nrameter language model and 2.0 GB for RoBERTa. Thus, 8-bit optimizers maintain performance\nand improve accessibility to the ﬁnetuning of large models for those that cannot afford GPUs with\nlarge memory buffers. We show models that are now accessible with smaller GPUs in Table 2. A\nbreakdown of individual dataset results on GLUE can be found in Appendix B).\nThe broad range of tasks and competitive results demonstrate that 8-bit optimizers are a robust and\neffective replacement for 32-bit optimizers, do not require any additional changes in hyperparame-\nters, and save a signiﬁcant amount of memory while speeding up training slightly.\nTable 2: With 8-bit optimizers, larger models can be ﬁnetuned with the same GPU memory com-\npared to standard 32-bit optimizer training. We use a batch size of one for this comparison.\nLargest ﬁnetunable Model (parameters)\nGPU size in GB\n32-bit Adam\n8-bit Adam\n6\nRoBERTa-base (110M)\nRoBERTa-large (355M)\n11\nMT5-small (300M)\nMT5-base (580M)\n24\nMT5-base (580M)\nMT5-large (1.2B)\n24\nGPT-2-medium (762M)\nGPT-2-large (1.5B)\n4\nANALYSIS\nWe analyze our method in two ways. First, we ablate all 8-bit optimizer components and show that\nthey are necessary for good performance. Second, we look at the sensitivity to hyperparameters\n6\n\nPublished as a conference paper at ICLR 2022\ncompared to 32-bit Adam and show that 8-bit Adam with block-wise dynamic quantization is a\nreliable replacement that does not require further hyperparameter tuning.\nExperimental Setup\nWe perform our analysis on a strong 32-bit Adam baseline for language\nmodeling with transformers (Vaswani et al., 2017). We subsample from the RoBERTa corpus (Liu\net al., 2019) which consists of the English sub-datasets: Books (Zhu et al., 2015), Stories (Trinh and\nLe, 2018), OpenWebText-1 (Gokaslan and Cohen, 2019), Wikipedia, and CC-News (Nagel, 2016).\nWe use a 50k token BPE encoded vocabulary (Sennrich et al., 2015). We ﬁnd the best 2-GPU-day\ntransformer baseline for 32-bit Adam with multiple hyperparameter searches that take in a total\nof 440 GPU days. Key hyperparameters include 10 layers with a model dimension of 1024, a fully\nconnected hidden dimension of 8192, 16 heads, and input sub-sequences with a length of 512 tokens\neach. The ﬁnal model has 209m parameters.\nTable 3: Ablation analysis of 8-bit Adam for small (2 GPU days) and large-scale (≈1 GPU year)\ntransformer language models on the RoBERTa corpus. The runs without dynamic quantization use\nlinear quantization. The percentage of unstable runs indicates either divergence or crashed training\ndue to exploding gradients. We report median perplexity for successful runs. We can see that\ndynamic quantization is critical for general stability and block-wise quantization is critical for large-\nscale stability. The stable embedding layer is useful for both 8-bit and 32-bit Adam and enhances\nstability to some degree.\nParameters\nOptimizer\nDynamic\nBlock-wise\nStable Emb\nUnstable (%)\nPerplexity\n209M\n32-bit Adam\n0\n16.7\n32-bit Adam\n✓\n0\n16.3\n8-bit Adam\n90\n253.0\n8-bit Adam\n✓\n50\n194.4\n8-bit Adam\n✓\n10\n18.6\n8-bit Adam\n✓\n✓\n0\n17.7\n8-bit Adam\n✓\n✓\n0\n16.8\n8-bit Adam\n✓\n✓\n✓\n0\n16.4\n1.3B\n32-bit Adam\n0\n10.4\n1.3B\n8-bit Adam\n✓\n100\nN/A\n1.3B\n8-bit Adam\n✓\n✓\n80\n10.9\n1.5B\n32-bit Adam\n0\n9.0\n1.5B\n8-bit Adam\n✓\n✓\n✓\n0\n9.0\nAblation Analysis\nFor the ablation analysis, we compare small and large-scale language model-\ning perplexity and training stability against a 32-bit Adam baseline. We ablate components individ-\nually and include combinations of methods that highlight their interactions. The baseline method\nuses linear quantization, and we add dynamic quantization, block-wise quantization, and the stable\nembedding layer to demonstrate their effect. To test optimization stability for small-scale language\nmodeling, we run each setting with different hyperparameters and report median performance across\nall successful runs. A successful run is a run that does not crash due to exploding gradients or di-\nverges in the loss. We use the hyperparameters ϵ {1e-8, 1e-7, 1e-6}, β1 {0.90, 0.87, 0.93}, β2\n{0.999, 0.99, 0.98} and small changes in learning rates. We also include some partial ablations for\nlarge-scale models beyond 1B parameters. In the large-scale setting, we run several seeds with the\nsame hyperparameters. We use a single seed for 32-bit Adam, ﬁve seeds for 8-bit Adam at 1.3B\nparameters, and a single seed for 8-bit Adam at 1.5B parameters.6 Results are shown in Table 3.\nThe Ablations show that dynamic quantization, block-wise quantization, and the stable embedding\nlayer are critical for either performance or stability. In addition, block-wise quantization is critical\nfor large-scale language model stability.\nSensitivity Analysis\nWe compare the perplexity of 32-bit Adam vs 8-bit Adam + Stable Embed-\nding as we change the optimizer hyperparameters: learning rate, betas, and ϵ. We change each hyper-\nparameter individually from the baseline hyperparameters β1=0.9, β2=0.995, ϵ=1e-7, and lr=0.0163\n6We chose not to do the full ablations with such large models because each training run takes one GPU year.\n7\n\nPublished as a conference paper at ICLR 2022\nand run two random seeds for both 8-bit and 32-bit Adam for each setting. If 8-bit Adam is perfectly\ninsensitive to hyperparameters compared to 32-bit Adam, we would expect the same constant offset\nin performance for any hyperparameter combination. The results can be seen in Figure 3. The re-\nsults show a relatively steady gap between 8-bit and 32-bit Adam, suggesting that 8-bit Adam does\nnot require any further hyperparameter tuning compared to 32-bit Adam.\nFigure 3: Sensitivity analysis of 8-bit vs 32-bit Adam hyperparameters. We can see that there is\nlittle variance between 8 and 32-bit performance, which suggests that 8-bit Adam can be used as a\ndrop-in replacement for 32-bit Adam without any further hyperparameter tuning.\n5\nRELATED WORK\nCompressing & Distributing Optimizer States\nWhile 16-bit Adam has been used in several\npublications, the stability of 16-bit Adam was ﬁrst explicitly studied for a text-to-image generation\nmodel DALL-E (Ramesh et al., 2021). They show that a stable embedding layer, tensor-wise scaling\nconstants for both Adam states, and multiple loss scaling blocks are critical to achieving stability\nduring training. Our work reduces the memory footprint of Adam further, from 16 to 8-bit. In\naddition, we achieve stability by developing new training procedures and non-linear quantization,\nboth of which complement previous developments.\nAdafactor (Shazeer and Stern, 2018) uses a different strategy to save memory. All optimizer states\nare still 32-bit, but the second Adam state is factorized by a row-column outer product resulting in\na comparable memory footprint to 16-bit Adam. Alternatively, Adafactor can also be used without\nusing the ﬁrst moment (β1 = 0.0) (Shazeer and Stern, 2018). This version is as memory efﬁcient as\n8-bit Adam, but unlike 8-bit Adam, hyperparameters for this Adafactor variant need to be re-tuned\nto achieve good performance. We compare 8-bit Adam with Adafactor β1 > 0.0 in our experiments.\nAdaGrad (Duchi et al., 2011) adapts the gradient with aggregate training statistics over the entire\ntraining run. AdaGrad that uses only the main diagonal as optimizer state and extensions of AdaGrad\nsuch as SM3 (Anil et al., 2019) and extreme tensoring (Chen et al., 2020a) can be more efﬁcient than\n8-bit Adam. We include some initial comparison with AdaGrad in Appendix H.\nOptimizer sharding (Rajbhandari et al., 2020) splits optimizer states across multiple accelerators\nsuch as GPUs/TPUs. While very effective, it can only be used if multiple accelerators are available\nand data parallelism is used. Optimizer sharding can also have signiﬁcant communication overhead\n(Rajbhandari et al., 2021). Our 8-bit optimizers work with all kinds of parallelism. They can also\ncomplement optimizer sharding, as they reduce communication overhead by 75%.\nGeneral Memory Reduction Techniques\nOther complementary methods for efﬁcient training\ncan be either distributed or local. Distributed approaches spread out the memory of a model across\nseveral accelerators such as GPUs/TPUs. Such approaches are model parallelism (Krizhevsky et al.,\n2009), pipeline parallelism (Krizhevsky et al., 2009; Huang et al., 2018; Harlap et al., 2018), and\noperator parallelism (Lepikhin et al., 2020). These approaches are useful if one has multiple accel-\nerators available. Our 8-bit optimizers are useful for both single and multiple devices.\nLocal approaches work for a single accelerator. They include gradient checkpointing (Chen et al.,\n2016), reversible residual connections (Gomez et al., 2017), and ofﬂoading (Pudipeddi et al., 2020;\n8\n\nPublished as a conference paper at ICLR 2022\nRajbhandari et al., 2021). All these methods save memory at the cost of increased computational\nor communication costs. Our 8-bit optimizers reduce the memory footprint of the model while\nmaintaining 32-bit training speed.\nQuantization Methods and Data Types\nWhile our work is the ﬁrst to apply 8-bit quantization to\noptimizer statistics, quantization for neural network model compression, training, and inference are\nwell-studied problems. One of the most common formats of 8-bit quantization is to use data types\ncomposed of static sign, exponent, and fraction bits. The most common combination is 5 bits for\nthe exponent and 2 bits for the fraction (Wang et al., 2018b; Sun et al., 2019; Cambier et al., 2020;\nMellempudi et al., 2019) with either no normalization or min-max normalization. These data types\noffer high precision for small magnitude values but have large errors for large magnitude values\nsince only 2 bits are assigned to the fraction. Other methods improve quantization through soft\nconstraints (Li et al., 2021) or more general uniform afﬁne quantizations (Pappalardo, 2021).\nData types lower than 8-bit are usually used to prepare a model for deployment, and the main focus\nis on improving network inference speed and memory footprint rather than maintaining accuracy.\nThere are methods that use 1-bit (Courbariaux and Bengio, 2016; Rastegari et al., 2016; Courbariaux\net al., 2015), 2-bit/3 values (Zhu et al., 2017; Choi et al., 2019), 4-bits (Li et al., 2019), more bits\n(Courbariaux et al., 2014), or a variable amount of bits (Gong et al., 2019). See also Qin et al.\n(2020) for a survey on binary neural networks. While these low-bit quantization techniques allow\nfor efﬁcient storage, they likely lead to instability when used for optimizer states.\nThe work most similar to our block-wise quantization is work on Hybrid Block Floating Point\n(HBFP) (Drumond et al., 2018) which uses a 24-bit fraction data type with a separate exponent for\neach tile in matrix multiplication to perform 24-bit matrix multiplication. However, unlike HBFP,\nblock-wise dynamic quantization has the advantage of having both block-wise normalization and a\ndynamic exponent for each number. This allows for a much broader range of important values since\noptimizer state values vary by about 5 orders of magnitude. Furthermore, unlike HBFP, block-wise\nquantization approximates the maximum magnitude values within each block without any quantiza-\ntion error, which is critical for optimization stability, particularly for large networks.\n6\nDISCUSSION & LIMITATIONS\nHere we have shown that high precision quantization can yield 8-bit optimizers that maintain 32-bit\noptimizer performance without requiring any change in hyperparameters. One of the main limita-\ntions of our work is that 8-bit optimizers for natural language tasks require a stable embedding layer\nto be trained to 32-bit performance. On the other hand, we show that 32-bit optimizers also beneﬁt\nfrom a stable embedding layer. As such, the stable embedding layer could be seen as a general\nreplacement for other embedding layers.\nWe show that 8-bit optimizers reduce the memory footprint and accelerate optimization on a wide\nrange of tasks. However, since 8-bit optimizers reduce only the memory footprint proportional to the\nnumber of parameters, models that use large amounts of activation memory, such as convolutional\nnetworks, have few beneﬁts from using 8-bit optimizers. Thus, 8-bit optimizers are most beneﬁcial\nfor training or ﬁnetuning models with many parameters on highly memory-constrained GPUs.\nFurthermore, there remain sources of instability that, to our knowledge, are not well understood. For\nexample, we observed that models with over 1B parameters often have hard systemic divergence,\nwhere many parameters simultaneously cause exploding gradients. In other cases, a single parameter\namong those 1B parameters assumed a value too large, caused an exploding gradient, and led to a\ncascade of instability. It might be that this rare cascading instability is related to the phenomena\nwhere instability disappears after reloading a model checkpoint and rolling a new random seed –\na method standard for training huge models. Cascading instability might also be related to the\nobservation that the larger a model is, the more unstable it becomes. For 8-bit optimizers, handling\noutliers through block-wise quantization and the stable embedding layer was key for stability. We\nhypothesize that that extreme outliers are related to cascading instability. If such phenomena were\nbetter understood, it could lead to better 8-bit optimizers and stable training in general.\nACKNOWLEDGEMENTS\nWe thank Sam Ainsworth, Ari Holtzman, Gabriel Ilharco, Aditya Kusupati, Oﬁr Press, and Mitchell\nWortsman for their valuable feedback.\n9\n\nPublished as a conference paper at ICLR 2022\nREFERENCES\nAnil, R., Gupta, V., Koren, T., and Singer, Y. (2019). Memory efﬁcient adaptive optimization. In\nWallach, H. M., Larochelle, H., Beygelzimer, A., d’Alch´e-Buc, F., Fox, E. B., and Garnett, R.,\neditors, Advances in Neural Information Processing Systems 32: Annual Conference on Neural\nInformation Processing Systems 2019, NeurIPS 2019, December 8-14, 2019, Vancouver, BC,\nCanada, pages 9746–9755.\nBa, J. L., Kiros, J. R., and Hinton, G. E. (2016).\nLayer normalization.\narXiv preprint\narXiv:1607.06450.\nBrown, T. B., Mann, B., Ryder, N., Subbiah, M., Kaplan, J., Dhariwal, P., Neelakantan, A., Shyam,\nP., Sastry, G., Askell, A., et al. (2020). Language models are few-shot learners. arXiv preprint\narXiv:2005.14165.\nCambier, L., Bhiwandiwalla, A., Gong, T., Elibol, O. H., Nekuii, M., and Tang, H. (2020). Shifted\nand squeezed 8-bit ﬂoating point format for low-precision training of deep neural networks. In\n8th International Conference on Learning Representations, ICLR 2020, Addis Ababa, Ethiopia,\nApril 26-30, 2020. OpenReview.net.\nChen, E. J. and Kelton, W. D. (2001). Quantile and histogram estimation. In Proceeding of the 2001\nWinter Simulation Conference (Cat. No. 01CH37304), volume 1, pages 451–459. IEEE.\nChen, T., Xu, B., Zhang, C., and Guestrin, C. (2016). Training deep nets with sublinear memory\ncost. arXiv preprint arXiv:1604.06174.\nChen, X., Agarwal, N., Hazan, E., Zhang, C., and Zhang, Y. (2020a). Extreme tensoring for low-\nmemory preconditioning. In 8th International Conference on Learning Representations, ICLR\n2020, Addis Ababa, Ethiopia, April 26-30, 2020. OpenReview.net.\nChen, X., Fan, H., Girshick, R., and He, K. (2020b). Improved baselines with momentum contrastive\nlearning. arXiv preprint arXiv:2003.04297.\nChoi, J., Venkataramani, S., Srinivasan, V., Gopalakrishnan, K., Wang, Z., and Chuang, P. (2019).\nAccurate and efﬁcient 2-bit quantized neural networks. In Talwalkar, A., Smith, V., and Zaharia,\nM., editors, Proceedings of Machine Learning and Systems 2019, MLSys 2019, Stanford, CA,\nUSA, March 31 - April 2, 2019. mlsys.org.\nCourbariaux, M. and Bengio, Y. (2016). Binarynet: Training deep neural networks with weights and\nactivations constrained to +1 or -1. CoRR, abs/1602.02830.\nCourbariaux, M., Bengio, Y., and David, J. (2015). Binaryconnect: Training deep neural networks\nwith binary weights during propagations. In Cortes, C., Lawrence, N. D., Lee, D. D., Sugiyama,\nM., and Garnett, R., editors, Advances in Neural Information Processing Systems 28: Annual\nConference on Neural Information Processing Systems 2015, December 7-12, 2015, Montreal,\nQuebec, Canada, pages 3123–3131.\nCourbariaux, M., Bengio, Y., and David, J.-P. (2014). Training deep neural networks with low\nprecision multiplications. arXiv preprint arXiv:1412.7024.\nDeng, J., Dong, W., Socher, R., Li, L.-J., Li, K., and Fei-Fei, L. (2009). Imagenet: A large-scale hi-\nerarchical image database. In 2009 IEEE conference on computer vision and pattern recognition,\npages 248–255. Ieee.\nDettmers, T. (2016). 8-bit approximations for parallelism in deep learning. International Conference\non Learning Representations (ICLR).\nDevlin, J., Chang, M., Lee, K., and Toutanova, K. (2019). BERT: pre-training of deep bidirectional\ntransformers for language understanding. In Burstein, J., Doran, C., and Solorio, T., editors,\nProceedings of the 2019 Conference of the North American Chapter of the Association for Com-\nputational Linguistics: Human Language Technologies, NAACL-HLT 2019, Minneapolis, MN,\nUSA, June 2-7, 2019, Volume 1 (Long and Short Papers), pages 4171–4186. Association for\nComputational Linguistics.\n10\n\nPublished as a conference paper at ICLR 2022\nDrumond, M., Lin, T., Jaggi, M., and Falsaﬁ, B. (2018). Training dnns with hybrid block ﬂoat-\ning point. In Bengio, S., Wallach, H. M., Larochelle, H., Grauman, K., Cesa-Bianchi, N., and\nGarnett, R., editors, Advances in Neural Information Processing Systems 31: Annual Conference\non Neural Information Processing Systems 2018, NeurIPS 2018, December 3-8, 2018, Montr´eal,\nCanada, pages 451–461.\nDuchi, J., Hazan, E., and Singer, Y. (2011). Adaptive subgradient methods for online learning and\nstochastic optimization. Journal of machine learning research, 12(7).\nDunning, T. and Ertl, O. (2019). Computing extremely accurate quantiles using t-digests. arXiv\npreprint arXiv:1902.04023.\nFedus, W., Zoph, B., and Shazeer, N. (2021). Switch transformers: Scaling to trillion parameter\nmodels with simple and efﬁcient sparsity. arXiv preprint arXiv:2101.03961.\nGlorot, X. and Bengio, Y. (2010). Understanding the difﬁculty of training deep feedforward neural\nnetworks. In Proceedings of the thirteenth international conference on artiﬁcial intelligence and\nstatistics, pages 249–256. JMLR Workshop and Conference Proceedings.\nGokaslan, A. and Cohen, V. (2019). Openwebtext corpus.\nGomez, A. N., Ren, M., Urtasun, R., and Grosse, R. B. (2017). The reversible residual network:\nBackpropagation without storing activations. arXiv preprint arXiv:1707.04585.\nGong, R., Liu, X., Jiang, S., Li, T., Hu, P., Lin, J., Yu, F., and Yan, J. (2019). Differentiable soft\nquantization: Bridging full-precision and low-bit neural networks. In 2019 IEEE/CVF Interna-\ntional Conference on Computer Vision, ICCV 2019, Seoul, Korea (South), October 27 - November\n2, 2019, pages 4851–4860. IEEE.\nGovindaraju, N. K., Raghuvanshi, N., and Manocha, D. (2005). Fast and approximate stream mining\nof quantiles and frequencies using graphics processors. In Proceedings of the 2005 ACM SIGMOD\ninternational conference on Management of data, pages 611–622.\nGreenwald, M. and Khanna, S. (2001). Space-efﬁcient online computation of quantile summaries.\nACM SIGMOD Record, 30(2):58–66.\nHarlap, A., Narayanan, D., Phanishayee, A., Seshadri, V., Devanur, N., Ganger, G., and Gib-\nbons, P. (2018). Pipedream: Fast and efﬁcient pipeline parallel dnn training. arXiv preprint\narXiv:1806.03377.\nHe, K., Zhang, X., Ren, S., and Sun, J. (2016). Deep residual learning for image recognition. In\nProceedings of the IEEE conference on computer vision and pattern recognition, pages 770–778.\nHenighan, T., Kaplan, J., Katz, M., Chen, M., Hesse, C., Jackson, J., Jun, H., Brown, T. B., Dhariwal,\nP., Gray, S., et al. (2020). Scaling laws for autoregressive generative modeling. arXiv preprint\narXiv:2010.14701.\nHuang, Y., Cheng, Y., Bapna, A., Firat, O., Chen, M. X., Chen, D., Lee, H., Ngiam, J., Le, Q. V.,\nWu, Y., et al. (2018). Gpipe: Efﬁcient training of giant neural networks using pipeline parallelism.\narXiv preprint arXiv:1811.06965.\nKaplan, J., McCandlish, S., Henighan, T., Brown, T. B., Chess, B., Child, R., Gray, S., Radford,\nA., Wu, J., and Amodei, D. (2020). Scaling laws for neural language models. arXiv preprint\narXiv:2001.08361.\nKeskar, N. S., McCann, B., Varshney, L. R., Xiong, C., and Socher, R. (2019). CTRL: A conditional\ntransformer language model for controllable generation. CoRR, abs/1909.05858.\nKingma, D. P. and Ba, J. (2014). Adam: A method for stochastic optimization. arXiv preprint\narXiv:1412.6980.\nKrizhevsky, A., Hinton, G., et al. (2009). Learning multiple layers of features from tiny images.\n11\n\nPublished as a conference paper at ICLR 2022\nLepikhin, D., Lee, H., Xu, Y., Chen, D., Firat, O., Huang, Y., Krikun, M., Shazeer, N., and Chen,\nZ. (2020). Gshard: Scaling giant models with conditional computation and automatic sharding.\narXiv preprint arXiv:2006.16668.\nLewis, M., Bhosale, S., Dettmers, T., Goyal, N., and Zettlemoyer, L. (2021). Base layers: Simplify-\ning training of large, sparse models. arXiv preprint arXiv:2103.16716.\nLewis, M., Liu, Y., Goyal, N., Ghazvininejad, M., Mohamed, A., Levy, O., Stoyanov, V., and Zettle-\nmoyer, L. (2020). BART: denoising sequence-to-sequence pre-training for natural language gen-\neration, translation, and comprehension. In Jurafsky, D., Chai, J., Schluter, N., and Tetreault,\nJ. R., editors, Proceedings of the 58th Annual Meeting of the Association for Computational Lin-\nguistics, ACL 2020, Online, July 5-10, 2020, pages 7871–7880. Association for Computational\nLinguistics.\nLi, J. B., Qu, S., Li, X., Strubell, E., and Metze, F. (2021). End-to-end quantized training via\nlog-barrier extensions.\nLi, R., Wang, Y., Liang, F., Qin, H., Yan, J., and Fan, R. (2019). Fully quantized network for object\ndetection. In IEEE Conference on Computer Vision and Pattern Recognition, CVPR 2019, Long\nBeach, CA, USA, June 16-20, 2019, pages 2810–2819. Computer Vision Foundation / IEEE.\nLiu, Y., Ott, M., Goyal, N., Du, J., Joshi, M., Chen, D., Levy, O., Lewis, M., Zettlemoyer, L., and\nStoyanov, V. (2019). Roberta: A robustly optimized bert pretraining approach. arXiv preprint\narXiv:1907.11692.\nLoshchilov, I. and Hutter, F. (2018). Fixing weight decay regularization in adam.\nMach´aˇcek, M. and Bojar, O. (2014). Results of the wmt14 metrics shared task. In Proceedings of\nthe Ninth Workshop on Statistical Machine Translation, pages 293–301.\nMellempudi, N., Srinivasan, S., Das, D., and Kaul, B. (2019). Mixed precision training with 8-bit\nﬂoating point. CoRR, abs/1905.12334.\nMicikevicius, P., Narang, S., Alben, J., Diamos, G., Elsen, E., Garcia, D., Ginsburg, B., Hous-\nton, M., Kuchaiev, O., Venkatesh, G., et al. (2017). Mixed precision training. arXiv preprint\narXiv:1710.03740.\nNagel, S. (2016). Cc-news.\nOtt, M., Edunov, S., Baevski, A., Fan, A., Gross, S., Ng, N., Grangier, D., and Auli, M. (2019).\nfairseq: A fast, extensible toolkit for sequence modeling. arXiv preprint arXiv:1904.01038.\nOtt, M., Edunov, S., Grangier, D., and Auli, M. (2018). Scaling neural machine translation. arXiv\npreprint arXiv:1806.00187.\nPappalardo, A. (2021). Xilinx/brevitas.\nPudipeddi, B., Mesmakhosroshahi, M., Xi, J., and Bharadwaj, S. (2020). Training large neural net-\nworks with constant memory using a new execution algorithm. arXiv preprint arXiv:2002.05645.\nQian, N. (1999). On the momentum term in gradient descent learning algorithms. Neural networks\n: the ofﬁcial journal of the International Neural Network Society, 12 1:145–151.\nQin, H., Gong, R., Liu, X., Bai, X., Song, J., and Sebe, N. (2020). Binary neural networks: A survey.\nCoRR, abs/2004.03333.\nRadford, A., Wu, J., Child, R., Luan, D., Amodei, D., and Sutskever, I. (2019). Language models\nare unsupervised multitask learners. OpenAI blog, 1(8):9.\nRaffel, C., Shazeer, N., Roberts, A., Lee, K., Narang, S., Matena, M., Zhou, Y., Li, W., and Liu,\nP. J. (2019). Exploring the limits of transfer learning with a uniﬁed text-to-text transformer. arXiv\npreprint arXiv:1910.10683.\n12\n\nPublished as a conference paper at ICLR 2022\nRajbhandari, S., Rasley, J., Ruwase, O., and He, Y. (2020). Zero: Memory optimizations toward\ntraining trillion parameter models. In SC20: International Conference for High Performance\nComputing, Networking, Storage and Analysis, pages 1–16. IEEE.\nRajbhandari, S., Ruwase, O., Rasley, J., Smith, S., and He, Y. (2021). Zero-inﬁnity: Breaking the\ngpu memory wall for extreme scale deep learning. arXiv preprint arXiv:2104.07857.\nRamesh, A., Pavlov, M., Goh, G., Gray, S., Voss, C., Radford, A., Chen, M., and Sutskever, I.\n(2021). Zero-shot text-to-image generation. arXiv preprint arXiv:2102.12092.\nRastegari, M., Ordonez, V., Redmon, J., and Farhadi, A. (2016). Xnor-net: Imagenet classiﬁcation\nusing binary convolutional neural networks. In Leibe, B., Matas, J., Sebe, N., and Welling, M.,\neditors, Computer Vision - ECCV 2016 - 14th European Conference, Amsterdam, The Nether-\nlands, October 11-14, 2016, Proceedings, Part IV, volume 9908 of Lecture Notes in Computer\nScience, pages 525–542. Springer.\nSennrich, R., Haddow, B., and Birch, A. (2015). Neural machine translation of rare words with\nsubword units. arXiv preprint arXiv:1508.07909.\nSennrich, R., Haddow, B., and Birch, A. (2016). Edinburgh neural machine translation systems for\nwmt 16. arXiv preprint arXiv:1606.02891.\nShazeer, N. and Stern, M. (2018). Adafactor: Adaptive learning rates with sublinear memory cost.\nIn International Conference on Machine Learning, pages 4596–4604. PMLR.\nShoeybi, M., Patwary, M., Puri, R., LeGresley, P., Casper, J., and Catanzaro, B. (2019). Megatron-\nlm: Training multi-billion parameter language models using model parallelism. arXiv preprint\narXiv:1909.08053.\nSun, X., Choi, J., Chen, C., Wang, N., Venkataramani, S., Srinivasan, V., Cui, X., Zhang, W., and\nGopalakrishnan, K. (2019). Hybrid 8-bit ﬂoating point (HFP8) training and inference for deep\nneural networks. In Wallach, H. M., Larochelle, H., Beygelzimer, A., d’Alch´e-Buc, F., Fox,\nE. B., and Garnett, R., editors, Advances in Neural Information Processing Systems 32: Annual\nConference on Neural Information Processing Systems 2019, NeurIPS 2019, December 8-14,\n2019, Vancouver, BC, Canada, pages 4901–4910.\nTrinh, T. H. and Le, Q. V. (2018). A simple method for commonsense reasoning. arXiv preprint\narXiv:1806.02847.\nVaswani, A., Shazeer, N., Parmar, N., Uszkoreit, J., Jones, L., Gomez, A. N., Kaiser, L., and Polo-\nsukhin, I. (2017). Attention is all you need. arXiv preprint arXiv:1706.03762.\nWang, A., Singh, A., Michael, J., Hill, F., Levy, O., and Bowman, S. R. (2018a).\nGlue: A\nmulti-task benchmark and analysis platform for natural language understanding. arXiv preprint\narXiv:1804.07461.\nWang, N., Choi, J., Brand, D., Chen, C., and Gopalakrishnan, K. (2018b). Training deep neural\nnetworks with 8-bit ﬂoating point numbers. In Bengio, S., Wallach, H. M., Larochelle, H., Grau-\nman, K., Cesa-Bianchi, N., and Garnett, R., editors, Advances in Neural Information Processing\nSystems 31: Annual Conference on Neural Information Processing Systems 2018, NeurIPS 2018,\nDecember 3-8, 2018, Montr´eal, Canada, pages 7686–7695.\nWenzek, G., Lachaux, M.-A., Conneau, A., Chaudhary, V., Guzm´an, F., Joulin, A., and Grave, E.\n(2020). CCNet: Extracting high quality monolingual datasets from web crawl data. In Proceed-\nings of the 12th Language Resources and Evaluation Conference, pages 4003–4012, Marseille,\nFrance. European Language Resources Association.\nZhu, C., Han, S., Mao, H., and Dally, W. J. (2017). Trained ternary quantization. In 5th Interna-\ntional Conference on Learning Representations, ICLR 2017, Toulon, France, April 24-26, 2017,\nConference Track Proceedings. OpenReview.net.\nZhu, Y., Kiros, R., Zemel, R., Salakhutdinov, R., Urtasun, R., Torralba, A., and Fidler, S. (2015).\nAligning books and movies: Towards story-like visual explanations by watching movies and\nreading books. In Proceedings of the IEEE international conference on computer vision, pages\n19–27.\n13\n\nPublished as a conference paper at ICLR 2022\nA\nBROADER IMPACT\nOur 8-bit optimizers enable training models that previously could not be trained on various GPUs,\nas shown in Table 2. Furthermore, while many options exist to reduce the memory footprint via\nparallelism (Rajbhandari et al., 2020; Lepikhin et al., 2020) our 8-bit optimizers are one of the\nfew options that can reduce the optimizer memory footprint signiﬁcantly for single devices without\ndegrading performance. Therefore, it is likely that our 8-bit optimizers will improve access to larger\nmodels – especially for the users that have the least resources.\nB\nGLUE SCORE BREAKDOWN\nTable 4 contains the breakdown of individual scores on the GLUE datasets.\nTable 4: Breakdown of GLUE scores. Each column is the median of 10 random seeds. The mean is\nthe mean over medians.\nModel\nMNLI\nQNLI\nQQP\nRTE\nSST-2\nMRPC\nCoLA\nSTS-B\nMean\n32-bit Adam\n90.40\n94.85\n92.2\n84.5\n96.40\n90.1\n67.41\n93.03\n88.61\n32-bit Adafactor\n90.35\n94.70\n92.2\n85.4\n96.45\n90.0\n67.63\n92.91\n88.71\n8-bit Adam\n90.30\n94.70\n92.2\n85.9\n96.40\n90.3\n67.20\n92.87\n88.73\nC\nSTABILITY OF EMBEDDING LAYERS\nHighly variable gradients can lead to unpredictable optimization behavior and instability that man-\nifests as divergence or exploding gradients. Low precision optimziers can amplify variance of gra-\ndient updates due to the noise introduced during quantization. While our 8-bit optimizers appear to\nbe stable for convolutional networks, similar to Ramesh et al. (2021), we ﬁnd that word embedding\nlayers are a major source of instability.\nThe main instability from the word embedding layer comes from the fact that it is a sparse layer\nwith non-uniform distribution of inputs which can produce maximum gradient magnitudes 100x\nlarger than other layers. For dense layers, if given n samples arranged into k mini-batches the\nsum of gradients of all mini-batches is always the same independent of how the n samples are\narranged into k mini-batches. For embedding gradients, this depends on the arrangement of samples\ninto mini-batches. This is because most deep learning frameworks normalize the gradient by the\nnumber of total tokens in the mini-batch, rather than the frequency of each individual token. This\napproximation allows stable learning with a single learning rate rather than variable learning rates\nthat depend on token frequency in each individual mini-batch. However a side-effect of this method\nis that the magnitude of gradients for a particular token can vary widely with batch sizes and between\ndifferent mini-batches.\nThere are multiple recipes for initialization word embedding layers. One of the most common\nrecipes used in all models trained with fairseq (Ott et al., 2019) such as RoBERTa (Liu et al., 2019),\nBART (Lewis et al., 2020), large NMT models (Ott et al., 2018), and sparse expert models (Lewis\net al., 2021), is the following: Initialize the word embedding layer with N(0, 1/\n√\nk) where k is the\nembedding size of the embedding layer and to scale the outputs by\n√\nk. This scheme has a variance\nof one at the start of training for the output distribution to ensure good gradient ﬂow.\nWe ﬁnd this approach to induce some instability for 8-bit optimizers. We develop the stable embed-\nding layer to solve this instability problem.\nWhile the full recipe for our stable embedding layer is new, components of it has been used before.\nThe layer norm after the embedding has been used before in work such as Devlin et al. (2019) and\nRadford et al. (2019) and enhanced precision for this particular layer was used in Ramesh et al.\n(2021). As pointed out above, these elements are not standard and the stable embedding layer\ncombines three aspects that are all important: (1) enhanced precision, (2) layer norm, and (3) Xavier\ninitialization.\n14\n\nPublished as a conference paper at ICLR 2022\nD\nQUANTIZATION ERROR ANALYSIS\nTo gain more insights into why block-wise dynamic quantization works so well and how it could be\nimproved, we performed a quantization error analysis of Adam quantization errors during language\nmodel training. Adam quantization errors are the deviations between the quantized 8-bit Adam\nupdate and the 32-bit Adam updates: |u8 −u16|, where uk = sk\n1/sk\n2 for k bits. See Background\nSection 1.1 for details on Adam.\nA good 8-bit quantization has the property that, for a given input distribution, the inputs are only\nrarely quantized into intervals with high quantization error and most often quantized into intervals\nwith low error.\nIn 8-bit, there are 255×256 possible 8-bit Adam updates, 256 possible values for the ﬁrst and 256\nfor the second Adam state. We look at the average quantization error of each of these possible\nupdates to see where the largest errors are and we plot histograms to see how often do these values\nwith high error occur. Taken together, these two perspectives give a detailed view of the magnitude\nof deviations and how often large deviations occur.\nWe study these questions by looking at how often each of the 256 values for both Adam states\nare used during language model training. We also analyze the average error for each of the inputs\nquantized to each of the 256 values. With this analysis it is easy to ﬁnd regions of high use and high\nerror, and visualize their overlap. An overlap of these regions is associated with large frequent errors\nthat cause unstable training. The quantization error analysis is shown in Figure 4.\nThe plots show two things: (1) The region of high usage (histogram) shows how often each com-\nbination of 256×256 bit values is used for the ﬁrst Adam state s1 (exponentially smoothed running\nsum) and the second Adam state s2 (exponentially smoothed running squared sum). (2) The error\nplots show for k-bit Adam updates uk = s1/(√s2 + ϵ) the mean absolute Adam error |u32 −u8|\nand the relative Adam error |u32 −u8|/|u32| averaged over each bit combination. In conjunction\nthese plots show which bits have the highest error per use and how often each bit is used. The x-\naxis/y-axis represents the quantization type range which means the largest positive/negative Adam\nstates per block/tensor take the values 1.0/-1.0.\nWe can see that block-wise dynamic quantization has the smallest overlap between regions of high\nuse and high error. While the absolute Adam quantization error of block-wise dynamic quantization\nis 0.0061, which is not much lower than that of dynamic quantization with 0.0067, the plots can also\nbe interpreted as block-wise dynamic having rarer large errors that likely contribute to improved\nstability during optimization.\nE\nFINE-GRAINED OPTIMIZER RUNTIME PERFORMANCE\nTable 5 shows optimizer performance that is benchmarked in isolation without any training. We use\na large sample of a normal distribution and benchmark the average time to perform 100 optimizer\nupdates per billion parameters in milliseconds.\nTable 5: Runtime performance of 8-bit optimizers vs commonly used 32-bit optimizers in millisec-\nonds per update per 1B parameters for 32-bit gradients. This comparision was run on a V100 GPU.\nMilliseconds per update per 1B param\nOptimizer\n32-bit PyTorch\n32-bit Apex\n8-bit (Ours)\nAdam\n145\n63\n47\nMomentum\n58\n46\n34\nLAMB\n–\n91\n65\nLARS\n–\n119\n43\nF\nADDITIONAL QUANTIZATION DATA TYPES\nThis section describes additional quantization data types that we tried but which we found to per-\nform poorly in quantization performance or stability. While quantile quantization has an average\n15\n\nPublished as a conference paper at ICLR 2022\nDynamic\nDynamic Block-wise\nLinear\nRegions of High\n       Usage\nRegions of High\n Relative Error\nRegions of High\n Absolute Error\nFigure 4: Good quantization methods do not have overlaps between regions of high use and high\nerror. The plot shows that for linear quantization regions of high usage and high error overlap. For\ndynamic quantization regions with high relative error are used infrequently while only small regions\nhave high usage and high absolute error. Block-wise dynamic quantization spreads out the usage\nover a large space and has the lowest overlap between regions of high use and errors. This means\nthat not only is the overall error of block-wise dynamic quantization lower, but also that large errors\nfor individual parameter updates are rarer compared to other methods, thus improving stability. See\nthe main text for more details.\n16\n\nPublished as a conference paper at ICLR 2022\nquantization twice as low as dynamic quantization for any normal distribution it has sporadic large\nerrors that lead to large Adam errors and poor model performance (see Figure 5) and even with\nstate-of-the-art quantile estimation algorithms (see Section G) quantile quantization is too slow to\nbe practical. An overview of quantization performance of this additional quantization data types\ncompared to dynamic quantization (without block-wise quantization) can be found in Table 6.\nFigure 5: Distribution of Adam error among each of the 256 8-bit values of the ﬁrst Adam state.\nWe normalize the values into the range [-1,1]. With this, -1 indicates the largest negative value, 0\nthe value that is closest to 0, and so forth. See Figure 6 for a visualization of this normalization.\nQuantile quantization has large errors for large values, while dynamic quantization has small errors\nfor both small and large values while the bulk of the errors is concentrated in intermediate values.\nTable 6: Mean relative Adam and absolute quantization error for the ﬁrst Adam state for different\nquantization methods. Results show mean±standard error. We can see that Dynamic Quantization\nhas best relative error and that both Dynamic methods have the best absolute error.\nMethod\nRelative Adam Error\nAbsolute Quantization Error\nLinear\n201% ±17%\n41.2e-10±3.1e-10\nQuantile\n11.9% ± 0.3%\n8.8e-10±0.9e-10\nInverse Dynamic\n6.5%± 0.1%\n4.6e-10±0.4e-10\nDynamic\n4.8%± 0.4%\n3.5e-10±1.1e-10\nF.1\nINVERSE DYNAMIC QUANTIZATION\nInverse Dynamic Quantization is motivated by the hypothesis that large Adam updates are more\nimportant than small updates. Since Adam is composed of a ratio of optimizer states mt/(√rt +ϵ),\nwe expect that small values in the second state rt to produce large Adam updates. To get a better\nquantization error for small values we can switch the dynamic exponent and the base exponent. For\nregular dynamic quantization the base exponent is 100 = 1 and each zero bit decreases the exponent\nby a factor of 10 for a minimum value of 10−7. We invert this starting with base 10−7 and each zero\nbit increases the exponent by 10 for a maximum value of 1. We denote this quantization as inverse\ndynamic quantization.\nF.2\nQUANTILE QUANTIZATION: A LOSSY MINIMUM ENTROPY ENCODING\nA lossy minimum entropy encoding with k bits has the property that for any input data, the quantized\noutputs take the value of each of the 2k different bit representations equally often.\nMore formally, a lossy minimum entropy encoding can be described in the following way. Given\nan inﬁnite stream of sampled real numbers xi where xi is distributed as X, an arbitrary probability\n17\n\nPublished as a conference paper at ICLR 2022\ndistribution, a lossy minimum entropy encoding is given by the k-bit quantization map Qmap ∈R2k\nwhich maps values q ∈R2k to indices 0, 1, . . . 2k which has the property that if any number of\nelements xi from the stream are quantized to xq\ni we do not gain any information which is predictive\nof future xq\nj>i.\nOne way to fulﬁll this property for arbitrary probability distributions X, is to divide the probability\ndistribution function fX into 2k bins where each bin has equal area and the mid-points of these bins\nare values q of the quantization map Qmap. Empirically, this is equivalent to a histogram with 2k\nbins where each bin contains equal number of values.\nHow do we ﬁnd the mid-points for each histogram bin? This is equivalent to ﬁnding the 2k non-\noverlapping values x for the cumulative distribution function FX with equal probability mass. These\nvalues can most easily be found by using its inverse function, the quantile function QX = F −1\nX . We\ncan ﬁnd the mid-points of each of the histogram bins by using the mid-points between 2k +1 equally\nspaced quantiles over the range of probabilities [0, 1]:\nqi =\nQX\n\u0010\ni\n2k+1\n\u0011\n+ QX\n\u0010\ni+1\n2k+1\n\u0011\n2\n,\n(5)\nTo ﬁnd q empirically, we can estimate sample quantiles for a tensor T with unknown distribution X\nby ﬁnding the 2k equally spaced sample quantiles via T’s empirical cumulative distribution function.\nWe refer to this quantization as quantile quantization.\nTo estimate sample quantiles efﬁciently, we devise a specialized approximate quantile estimation\nalgorithm, SRAM-Quantiles, which is more than 75x faster than other approximate quantile esti-\nmation approaches (Govindaraju et al., 2005; Dunning and Ertl, 2019). SRAM-Quantiles uses a\ndivide-and-conquer strategy to perform sorting solely in fast SRAM. More details on this algorithm\ncan be found in the Appendix Section G.\nF.3\nVISUALIZATION: DYNAMIC VS LINEAR QUANTIZATION VS QUANTILE QUANTIZATION\nFigure 6 shows the mapping from each to the 255 values of the 8-bit data types to their value\nnormalized in the range [-1, 1]. We can see that most bits in dynamic quantization are allocated for\nlarge and small values. Quantile quantization is introduced in Appendix F.2.\nFigure 6: Visualization of the quantization maps for the linear, dynamic and quantile quantization.\nFor quantile quantization we use values from the standard normal distribution and normalize them\ninto the range [-1, 1].\n18\n\nPublished as a conference paper at ICLR 2022\nG\nSRAM-QUANTILES: A FAST QUANTILE ESTIMATION ALGORITHM\nTo estimate sample quantiles of a tensor one needs to determine the empirical cumulative distribution\nfunction (eCDF) of that tensor. The easiest way to ﬁnd the eCDF is to sort a given tensor. Once\nsorted, the quantiles can be found by using the value at index i = q × n where i is the index into\nthe sorted array, q is the desired quantile and n is the total elements in the tensor. While simple,\nthis process of estimating quantiles is computationally expensive and would render training with\nquantile quantization too slow to be useful.\nSimilar to other quantile estimation approaches, our GPU algorithm, SRAM-Quantiles, uses a slid-\ning windows over the data for fast, approximate quantile estimation with minimal resources. Green-\nwald and Khanna (2001)’s quantile estimation algorithm uses dynamic bin histograms over sliding\nwindows to estimate quantiles. Extensions of this algorithm accelerate estimation by using more ef-\nﬁcient data structures and estimation algorithms (Dunning and Ertl, 2019) or by using GPUs (Govin-\ndaraju et al., 2005). The main difference between this work an ours is that we only compute a limit\nset of quantiles that are known a priori – 256, to be exact – while previous work focuses on general\nstatistics which help to produce any quantile a posteriori. Thus we can devise a highly specialized\nalgorithm which offers faster estimation.\nThe idea behind our algorithm comes from the fact that sorting is slow because it involves repeated\nloads and stores from main memory (DRAM) when executing divide-and-conquer sorting algo-\nrithms. We can signiﬁcantly improve performance of quantile estimation if we restructure quantile\nestimation to respect memory hierarchies of the device on which the algorithm is executed.\nOn a GPU, programmable SRAM – known as shared memory – is 15x faster than DRAM but has\na limit size of around 64 kb per core. The SRAM-Quantiles algorithm is simple. Instead of ﬁnding\nthe full eCDF we ﬁnd the eCDF for a subset of values of the tensor that ﬁts into SRAM (about 4096\n32-bit values). Once we found the quantiles for each subset, we average the quantiles atomically in\nDRAM.\nThis algorithm works, because the arithmetic mean is an unbiased estimator for the population mean\nand samples quantiles estimated via eCDFs are asymptotically unbiased estimators of the population\nquantile (Chen and Kelton, 2001). Thus the more subset quantiles we average, the better the estimate\nof the tensor-wide quantiles.\nFor estimating 256 quantiles on a large stream of numbers, our algorithm takes on average 0.064\nns to process one element in the stream, whereas the fastest general algorithms take 300 ns (Govin-\ndaraju et al., 2005) and 5 ns (Dunning and Ertl, 2019).\nH\nADAGRAD COMPARISONS\nWhile the main aim in this work is to investigate how the most commonly used optimizers, such as\nAdam (Kingma and Ba, 2014) and Momentum (Qian, 1999), can be used as 8-bit variants without\nany further hyperparameter tuning, it can be of interest to consider the behavior of our 8-bit methods\nunder different scenarios. For example, one difference between Adam/Momentum and AdaGrad\n(Duchi et al., 2011) is that AdaGrad accumulates gradients statistics over the entire course of training\nwhile Adam/Momentum use a smoothed exponential decay over time. As such, this could lead to\nvery different 8-bit quantization behavior where there are large difference between the magnitude of\ndifferent optimizer states. Such large differences could induce a large quantization error and degrade\nperformance of 8-bit optimizers.\nTo investigate this, we train small 209M parameter language models on the RoBERTa corpus (Liu\net al., 2019). We use the AdaGrad hyperparameters introduced by Keskar et al. (2019). Results are\nshown in Table 7. From the results we can see that our 8-bit methods do not work as well for Ada-\nGrad. One hypothesis is that this is due to the the wide range of gradient statistics of AdaGrad which\ncomes from averaging the gradient over the entire course of training. To prevent poor quantization\nin such scenarios, stochastic rounding proved to be very effective from our initial experiments with\nother 8-bit optimizer. While we abandoned stochastic rounding because we did not see any beneﬁts\nfor Adam and Momentum, it could be an effective solution for AdaGrad. We leave such improved\n8-bit quantization methods for AdaGrad to future work.\n19\n\nPublished as a conference paper at ICLR 2022\nWhile AdaGrad falls short in this experiments in terms of perplexity compared to Adam, AdaGrad’s\nperformance might be improved by adding a momentum term. We leave such improvements for\nfuture work.\nTable 7: AdaGrad compared to Adam performance for a 209M parameter language model on the\nRoBERTa corpus. The 8-bit methods use stable embedding layer. AdaGrad hyperparamters are\ntaken from (Keskar et al., 2019).\nOptimizer\nValid Perplexity\n32-bit Adam\n16.7\n8-bit Adam\n16.4\n32-bit AdaGrad\n19.4\n8-bit AdaGrad\n19.7\nI\nSTABLE EMBEDDING LAYER ABLATIONS\nHere we use the 200M language model experimental setup described in Section 4 to ablate the layer\nnorm, Xavier initialization, and 32-bit optimizer state components of the Stable Embedding Layer\ndescribed in Section 2.3. We run each ablation with 3 random seeds and report median perplexity.\nResults are shown in Table 8. We can see that both Xavier initialization and the layer norm improves\nperformance. While we can see performance difference in this setup, the models are too small to\nstudy instabilities that usually occur only at larger scales. As such, it is as expected that 32-bit\noptimizer states for the embedding layer makes no difference in either perplexity or stability.\nThe best setup to test the stable embedding layer’s effect on instabilities at large scale is to train large\nmodels and record instabilities. However, since a single model with more than 1B parameters takes\nroughly 300 GPU days to run, and multiple random seeds are need to study instability, an ablation\nstudy of that scale is out of our computational budget. As such, we are unable to study the stabilizing\neffects of the Stable Embedding layer beyond showing that it affects perplexity at the small scale.\nTable 8: Ablations for stable embedding layer components sorted by combinations that have im-\nproved perplexity. We can see that both Xavier and Layer Normalization improves performance.\n32-bit optimizer states do not improve performance and do not affect stability at this scale but might\naffect stability for large scale models.\nLayer Norm\nXavier\n32-bit state\nPerplexity\n16.83\n✓\n16.84\n✓\n16.66\n✓\n✓\n16.64\n✓\n16.60\n✓\n✓\n16.60\n✓\n✓\n✓\n16.47\n✓\n✓\n16.46\n20\n"
    },
    {
      "arxiv_id": "2108.12409",
      "full_text": "Published as a conference paper at ICLR 2022\nTRAIN SHORT, TEST LONG: ATTENTION WITH LINEAR\nBIASES ENABLES INPUT LENGTH EXTRAPOLATION\nOﬁr Press1,2\nNoah A. Smith1,3\nMike Lewis2\n1Paul G. Allen School of Computer Science & Engineering, University of Washington\n2Facebook AI Research\n3Allen Institute for AI\nofirp@cs.washington.edu\nABSTRACT\nSince the introduction of the transformer model by Vaswani et al. (2017), a funda-\nmental question has yet to be answered: how does a model achieve extrapolation\nat inference time for sequences that are longer than it saw during training? We ﬁrst\nshow that extrapolation can be enabled by simply changing the position represen-\ntation method, though we ﬁnd that current methods do not allow for efﬁcient ex-\ntrapolation. We therefore introduce a simpler and more efﬁcient position method,\nAttention with Linear Biases (ALiBi). ALiBi does not add positional embeddings\nto word embeddings; instead, it biases query-key attention scores with a penalty\nthat is proportional to their distance. We show that this method trains a 1.3 bil-\nlion parameter model on input sequences of length 1024 that extrapolates to input\nsequences of length 2048, achieving the same perplexity as a sinusoidal position\nembedding model trained on inputs of length 2048 but training 11% faster and\nusing 11% less memory. ALiBi’s inductive bias towards recency also leads it to\noutperform multiple strong position methods on the WikiText-103 benchmark.1\n1\nINTRODUCTION\nWhen constructing a transformer-based language model, a major design decision is the length of\ntraining sequences, denoted L herein, which has to date been equivalent to the length of inference\nsequences. More context, achieved by larger L, improves predictions at inference time. But longer\nsequences are more expensive to train on.2\nBefore transformers, RNN language models were trained on shorter-L sequences and assumed to\ngeneralize to longer contexts at inference time (Mikolov et al., 2010; Mikolov & Zweig, 2012;\nZaremba et al., 2014). Vaswani et al. (2017), introducing the transformer, speculated that it “may\n[...] extrapolate to sequence lengths longer than the ones encountered during training.” We deﬁne\nextrapolation as a model’s ability to continue performing well as the number of input tokens during\nvalidation increases beyond the number of tokens on which the the model was trained. We ﬁnd\nthat transformer language models (LMs) that use sinusoidal position embeddings have very weak\nextrapolation abilities; see Figure 1.\nWe demonstrate that this failure to extrapolate is caused by the position embedding method. As\nshown in Figure 1, recent alternatives to the original sinusoidal position method (Su et al., 2021;\nRaffel et al., 2020) have improved extrapolation. However, the better of these, the T5 bias, is con-\nsiderably slower than the sinusoidal approach and uses extra memory and parameters (Figure 2).\nWe therefore introduce Attention with Linear Biases (ALiBi) to facilitate efﬁcient extrapolation.\nALiBi negatively biases attention scores with a linearly decreasing penalty proportional to the dis-\ntance between the relevant key and query. Our simple approach eliminates position embeddings.\n1Code & models: https://github.com/ofirpress/attention_with_linear_biases\n2Figure 7 in the appendix plots training speed, in words per second, against L.\n1\narXiv:2108.12409v2  [cs.CL]  22 Apr 2022\n\nPublished as a conference paper at ICLR 2022\n512\n4000\n8000\n12000\n16000\nInference Input Tokens\n15\n25\n35\n45\n55\nPerplexity (\n)\nExtrapolation for\nModels Trained on 512 Tokens\nSinusoidal\nRotary\nT5 Bias\nALiBi\n1024\n4000\n8000\n12000\n16000\nInference Input Tokens\n15\n25\n35\n45\n55\nPerplexity (\n)\nExtrapolation for\nModels Trained on 1024 Tokens\nSinusoidal\nRotary\nT5 Bias\nALiBi\nFigure 1: Extrapolation: as the (validation-set’s) input sequence gets longer (x-axis), current po-\nsition methods (sinusoidal, rotary, and T5) show degraded perplexity (y-axis, lower is better), but\nour method (§3) does not. Models were trained on WikiText-103 with sequences of L = 512 (left)\nor L = 1,024 (right) tokens. T5 ran out of memory on our 32GB GPU. For more detail on exact\nperplexities and runtimes, see Tables 2 and 3 in the appendix.\nCompared to a sinusoidal model trained on the same input length, our method requires no additional\nruntime or parameters and incurs a negligible (0–0.7%) memory increase. ALiBi can be imple-\nmented by changing only a few lines of existing transformer code.\nUsing ALiBi, a transformer LM can be trained on short-L sequences and therefore at much lower\ncost, and it can still be reliably applied to long sequences at runtime. For example, a 1.3 billion\nparameter LM trained on L = 1024 tokens with ALiBi achieves the same perplexity as a sinusoidal\nmodel trained on L = 2048 when both are tested on sequences of 2048 tokens, even though our\nmodel is 11% faster and uses 11% less memory.\nThough performance peaks at around two times the number of tokens that the model was trained on,\nALiBi maintains strong performance even on sequences of length 10,000. In recently explored set-\ntings where NLP training examples are given as context to an LM (Brown et al., 2020), our approach\nwill allow exposure to more examples. Additionally, it enables generation of longer outputs.\n2\nCURRENT APPROACHES DO NOT EXTRAPOLATE EFFICIENTLY\nWe show for the ﬁrst time that the sinusoidal position method, which technically should be able\nto extrapolate, in practice has very limited extrapolation capabilities. Though the rotary position\nmethod improves over the sinusoidal one, it still does not achieve satisfying results. Holding every-\nthing else constant, we are the ﬁrst to observe that the T5 bias method leads to better extrapolation\nthan either of these, and so we conclude that extrapolation ability depends heavily on the position\nembedding. Unfortunately, the T5 bias is computationally costly (Figure 2).\n2.1\nBACKGROUND AND EXPERIMENTAL SETUP\nA transformer LM receives a list of tokens and outputs a probability distribution representing its\nprediction for the next token. We call the input list the current input subsequence since the inputs to\nlanguage models are typically subsequences from (much longer) training or evaluation sequences.\nDuring both training and perplexity evaluation (i.e., scoring a ﬁxed sequence), many predictions can\nbe calculated at once; this is done using a “causal mask” that ensures each position’s prediction is\ninﬂuenced only by tokens to its left. Let L be the length of each input subsequence during training;\nit includes L predictions, which on average have access to L+1\n2\ntokens of (left) context. To explore a\nmodel’s extrapolation abilities, we are interested in cases where sequences of length Lvalid > L are\nconsidered at evaluation time. When L differs between inference and training, we use L to refer to\nthe length of subsequences during training and Lvalid to refer to their length at validation.\n2\n\nPublished as a conference paper at ICLR 2022\n512\n1024\n3072\nInput Length\n0\n10k\n20k\n30k\nWPS (\n)\nTraining Speed\n512\n1024\n3072\nInput Length\n0k\n25k\n50k\n75k\n100k\nWPS (\n)\nInference Speed\n512\n1024\n3072\nInput Length\n0\n10\n20\n30\nGB (\n)\nTraining Memory\nSinusoidal\nRotary\nT5 Bias\nALiBi\nFigure 2: A comparison of batched training, inference speed and memory use of the sinusoidal,\nrotary, T5 bias, and our ALiBi position methods. The speed differences between our method and\nthe sinusoidal are within 1% during training and 3% for inference, which is insigniﬁcant on our\nhardware. ALiBi uses 100MB of extra memory when training on input lengths 1024 and 3072 in\nthis setting. Memory usage is lower in all approaches when training on 3072 tokens (compared to\n1024) since we break batches into multiple updates. See Table 1 in the appendix for exact numbers.\nNonoverlapping Inference\nTo train on or evaluate a sequence longer than L tokens, it is typical\nto segment the sequence into L-length subsequences and train on or evaluate them independently.\nUnless otherwise stated, we use nonoverlapping inference to report perplexity scores.\nExtrapolation During Inference\nFormally, the functions that deﬁne a transformer layer are ag-\nnostic to input length;3 they map from some arbitrary, unﬁxed number of input vectors to the same\nnumber of output vectors. When transformers are applied to data that is inherently sequential, like\ntext, positional information is injected into the inputs in various ways.\nVaswani et al. (2017) discussed two options for embedding positions into vectors to be added to word\nembeddings: learning embeddings for speciﬁc positions and unlearned sinusoidal embeddings. They\nobserved similar performance between these two but preferred the sinusoidal approach, which they\nargued might extrapolate to longer input sequences during inference. We ﬁnd that this model cannot\nextrapolate to more than a few dozen tokens beyond L.4\nExperiment Setup\nWe ﬁrst test the extrapolation abilities of various position methods on the\nWikiText-103 corpus (Merity et al., 2016) using the transformer language model of Baevski & Auli\n(2018). We use this model because of its prominent role in recent language modeling develop-\nments (Khandelwal et al., 2020; Press et al., 2021). The training set is about 103 million tokens\nfrom English Wikipedia (half a gigabyte). The model has 16 transformer layers of dimension 1024,\nwith 8 heads, and a feedforward inner dimension of 4096. This model ties the word embedding and\nsoftmax matrices (Press & Wolf, 2017; Inan et al., 2017). In our experiments, other than varying the\nposition method and training subsequence length, we modify no other hyperparameters, including\nthe random seed and number of training epochs (205).\n2.2\nMEASURING EXTRAPOLATION\nSinusoidal Position Embeddings\nSinusoidal position embeddings (Vaswani et al., 2017; §3.5)\nare constant, non-learned vectors that are added to token embeddings on input to the ﬁrst layer\nof the transformer. They are frequently used in transformer language modeling (Baevski & Auli,\n2018; Lewis et al., 2021) and machine translation (Vaswani et al., 2017; Ott et al., 2018) models.\nWe ﬁrst consider the unmodiﬁed model of Baevski & Auli (2018), which uses sinusoidal position\nembeddings, and train it on L = 512 tokens; we then run inference with it on the validation set\non L + k tokens, with k ranging from 0 to 15,000. Figure 1 (left) and the corresponding Table 2\n(in the appendix) show that while the model improves perplexity up to k = 20, performance stops\nimproving and stays steady from k = 20 to k = 50 and then begins degrading. Similar results are\nobtained for a model trained with L = 1024 tokens (Figure 1 (right) and Table 3 in the appendix).\nThat model improves for up to Lvalid = L + 50 tokens, after which performance declines.\n3These include the embedding lookup, feedforward sublayer, and softmax layer, which act independently\non vector inputs, as well as the attention sublayers, whose parameters do not depend on input length (and which\nmust handle variable-length inputs, e.g., due to causal masking).\n4The learned positional embedding approach does not have a way to encode positions greater than L; it\ntherefore has no ability to extrapolate.\n3\n\nPublished as a conference paper at ICLR 2022\nRotary Position Embeddings\nThe rotary method was introduced by Su et al. (2021) and has\nrecently been popularized by the open source GPT-3 (Brown et al., 2020) implementation GPT-\nJ (Wang & Komatsuzaki, 2021). Instead of adding sinusoidal embeddings at the bottom of the\ntransformer, they multiply the keys and queries of every attention layer by sinusoidal embeddings.\nUnlike the sinusoidal or learned positional embedding approach, the rotary method injects position\ninformation into the model at every layer, not just at the initial one. In addition, it adds no position\ninformation to the values of the self-attention sublayer. The output of a self-attention sublayer is a\nlinearly transformed, weighted sum of the input value vectors; therefore, by not inserting position\ninformation into the values, the outputs of each transformer-layer contain no explicit position infor-\nmation. We suspect that this segregation of position information may be beneﬁcial for extrapolation,\nand we draw inspiration from it in the design of our method (§3).\nWe apply the rotary position embedding method to our Baevski & Auli baseline.5 The perplexity\nresults (Figure 1 and Appendix Tables 2 and 3) are better than the sinusoidal approach: the model\nwith L = 512 (L = 1024) improves perplexity with up to k = 200 (k = 100) more tokens than it\nsaw during training, but this comes at the cost of slower training and inference (Figure 2).\nT5 Bias\nThough most models use trained or sinusoidal position embeddings, the T5 model of Raf-\nfel et al. (2020) uses a relative position method (Shaw et al., 2018; Huang et al., 2019) that adds no\nposition information to word embeddings (as in the previous method). Instead, it modiﬁes the way\nattention values are computed. We refer to this as the “T5 bias” method.6 To compute attention\nvalues in the unmodiﬁed transformer, we compute the dot product of every query with every rele-\nvant key and then softmax these attention values. In this method, we compute the attention values\nas before, but then we add a learned, shared bias to each query-key score that is dependent on just\nthe distance between the query and key. Therefore, all query-key scores where the query and key\ndistance are zero (i.e., the query and key represent the same token) get a speciﬁc learned bias, all\nscores where the query and key are one word away get a different learned bias, and so on, up to a\ncertain point, from where multiple different distances share the same learned bias (which might be\nbeneﬁcial for extrapolation). As in the rotary method, the T5 bias injects position information into\nthe model at every layer and integrates no explicit position information into the self-attention value\nvectors.\nRaffel et al. (2020) propose that the T5 bias may allow extrapolation, but they did not report exper-\niments testing this. Here, we show that the T5 bias does allow language models to extrapolate. We\ndo this by again modifying the Baevski & Auli model, this time to insert the T5 bias into it.7\nAs Figure 1 shows, the T5 bias improves perplexity with longer sequences than the ones it was\ntrained on, i.e., k = 600 (k = 800) extra tokens for a model trained on L = 512 (L = 1024) input\ntokens. Unfortunately, this impressive performance comes at a cost: training is at least twice as slow\nas with the sinusoidal model. Therefore, this model’s extrapolation ability provides no efﬁciency\nadvantage. For example, to do inference on 1024 tokens, we could either train the sinusoidal model\nwith L = 1024 or train the T5 bias model on L = 512 tokens and extrapolate to 1024 for inference.\nHowever, the L = 1024 sinusoidal model runs at 28.5k words per second (WPS), while the L =\n512 T5 bias model runs at 14.4k WPS (Appendix Table 1), so there is no speedup when training on\nshorter sequences with this method.8\n5Our rotary method implementation is based on the code in https://github.com/JunnYu/\nRoFormer_pytorch, which is linked to from the ofﬁcial repository of Su et al. (2021):\n(https:\n//github.com/ZhuiyiTechnology/roformer). After we ﬁnished running our experiments with the\nrotary method, we were informed that the runtime of the code linked above could be optimized, making it only\n2% slower than the sinusoidal approach. This optimization would not change extrapolation performance.\n6This method is similar to the one used in Parikh et al. (2016, Equation 7).\n7Our T5 bias implementation is based on the one used in HuggingFace Transformers (Wolf et al., 2020),\nwhich in turn is based on the ofﬁcial Mesh Tensorﬂow T5 code.\n8Narang et al. (2021) benchmarked the T5 bias as being just 8.7% slower than the sinusoidal approach;\nthus, while always incurring a runtime penalty, this method’s runtime could be faster depending on the choice\nof hardware and software frameworks used. Narang et al. used the Tensorﬂow T5 library running on TPUs,\nwhile we used the PyTorch Fairseq library running on GPUs.\n4\n\nPublished as a conference paper at ICLR 2022\nq1 k1\nq2 k1 q2 k2\nq3 k1 q3 k2 q3 k3\nq4 k1 q4 k2 q4 k3 q4 k4\nq5 k1 q5 k2 q5 k3 q5 k4 q5 k5\n0\n1\n0\n2\n1\n0\n3\n2\n1\n0\n4\n3\n2\n1\n0\n+\nm\nFigure 3: When computing attention scores for each head, our linearly biased attention method, AL-\niBi, adds a constant bias (right) to each attention score (qi · kj, left). As in the unmodiﬁed attention\nsublayer, the softmax function is then applied to these scores, and the rest of the computation is un-\nmodiﬁed. m is a head-speciﬁc scalar that is set and not learned throughout training. We show that\nour method for setting m values generalizes to multiple text domains, models and training compute\nbudgets. When using ALiBi, we do not add positional embeddings at the bottom of the network.\n3\nATTENTION WITH LINEAR BIASES (ALIBI)\nIn the transformer model of Vaswani et al. (2017), position embeddings are added to the word\nembeddings at the bottom of the network. For an input subsequence of length L, the attention\nsublayer computes the attention scores for the ith query qi ∈R1×d, (1 ≤i ≤L) in each head, given\nthe ﬁrst i keys K ∈Ri×d, where d is the head dimension:\nsoftmax(qiK⊤)\nThese attention scores are then multiplied by the values to return the output of the attention sublayer.9\nWhen using ALiBi, we do not add position embeddings at any point in the network. The only\nmodiﬁcation we apply is after the query-key dot product, where we add a static, non-learned bias:10\nsoftmax(qiK⊤+ m · [−(i −1), ..., −2, −1, 0]),\nwhere scalar m is a head-speciﬁc slope ﬁxed before training. Figure 3 offers a visualization.\nFor our models with 8 heads, the slopes that we used are the geometric sequence:\n1\n21 , 1\n22 , ..., 1\n28 .\nFor models that require 16 heads, we interpolate those 8 slopes by geometrically averaging every\nconsecutive pair, resulting in the geometric sequence that starts at\n1\n√\n2 and has the ratio of\n1\n√\n2:\n1\n20.5 , 1\n21 ,\n1\n21.5 , ..., 1\n28 . In general, for n heads, our set of slopes is the geometric sequence that starts\nat 2\n−8\nn and uses that same value as its ratio.\nIn §4, we observe that this set of slopes works on a wide variety of text domains and model sizes.\nTherefore, we do not believe that it is necessary to tune these slope values every time a new model\nis trained on a new dataset. This makes our method similar to the sinusoidal approach, where the\nhyperparameters (the start and end of the geometric progression of wavelengths) were set once\nby Vaswani et al. (2017) and then reused in different models of different sizes on different datasets.\nALiBi has an inductive bias towards recency; it penalizes attention scores between distant query-key\npairs, with the penalty increasing as the distance between a key and a query grows. The different\nheads increase their penalties at different rates, depending on the slope magnitude.\nWe initially experimented with making the slopes trainable, but this did not yield strong extrapola-\ntion results.11 A brief manual exploration of around ten slope sets led us to discover the set of slopes\nthat we ﬁnally picked. Our main insight from this exploration is that the slope sets that work best are\nthose with slopes in the (0, 1) range, with the slopes’ density increasing as we get closer to 0. We\nalso found our method to be robust to slope choice. Even randomly sampling from the exponential\ndistribution worked well in some cases (although that method had high variance).\nSince ALiBi is a relative position method, we add position information at every layer to the keys\nand queries but not to the values, as is done in the T5 bias and rotary methods. We hypothesize that\nthese properties might be beneﬁcial for extrapolation.\n9For simplicity we omit the key, query, value and ﬁnal output projections, dropout, and the scaling factor.\n10The ALiBi bias is not multiplied by the √dk scaling factor from Equation 1 of Vaswani et al. (2017).\n11In our experiments, trainable slopes also slowed down the training speed by 3%.\n5\n\nPublished as a conference paper at ICLR 2022\nImplementation.\nALiBi is easy to implement, with all changes accomplished in a few lines of\ncode. We implement it by modifying the mask matrix by adding the linear biases to it (in practice,\nwhen training a transformer LM, query qi attends only to keys 1 to i; this is implemented by adding\na mask matrix to the query-key dot product before the softmax operation is applied). This means\nthat there is no runtime penalty when using our method since we add no operations to the network.\nCompared to the sinusoidal model trained on the same input lengths, AliBi incurs a memory increase\n(up to 100MB in some of our experiments): in the unmodiﬁed transformer, the mask is of size L×L;\nwhen using ALiBi, the mask is a slightly larger n×L×L (where n is the number of heads) since the\nlinear biases added for each head uses a different slope. But, as we show, ALiBi enables training on\nmuch smaller sequences while still achieving (and occasionally surpassing) results obtained using\nsinusoidal embeddings on longer sequences, which saves multiple gigabytes of memory.\n4\nRESULTS\nWe ﬁrst show that on WikiText103 ALiBi is efﬁcient and enables training models with short input\nsubsequences that outperform strong baselines even when the ALiBi models extrapolate to more than\nsix times the number of tokens that they were trained on. We then take the same hyperparameters for\nour method (the set of slopes) that worked on WikiText-103 and show that – with no modiﬁcation\n– they provide strong results on a dataset in a very different domain: books. Finally, we show that\na 1.3B parameter model trained with AliBi on a much larger (461 GB) dataset with much more\ncompute provides a superior alternative to the sinusoidal method since it achieves similar perplexity\nscores while running faster and using less memory (since it is trained on shorter inputs).\nWhile multiple alternatives to the position methods presented in Vaswani et al. (2017) have been\nproposed, few have been adopted in large (1B or more parameter) LMs since that setting is much\nmore challenging than the smaller scale experiments. GPT-3 and Jurassic-1 (Lieber et al., 2021)\nuse the learned position embedding method from Vaswani et al., and GPT-J uses the rotary method.\nOur results on the 1.3B parameter model show our method’s ability to generalize to larger models,\ndataset sizes and training durations without retuning the hyperparameter.\n4.1\nRESULTS ON WIKITEXT-103 AND TORONTO BOOKCORPUS\n512\n1024\n1536\n2048\n3072\nValidation Input Length (Lvalid)\n17.0\n17.5\n18.0\n18.5\n19.0\n19.5\n20.0\n20.5\n21.0\nPerplexi(y (←←\nAL B  Ex(rapola( ng on W k Tex(-103\nAL B , L = 512\nS nuso dal, L = 512\nAL B , L = 1024\nS nuso dal, L = 1024\nAL B , L = 1536\nS nuso dal, L = 1536\nAL B , L = 2048\nS nuso dal, L = 2048\nAL B , L = 3072\nS nuso dal, L = 3072\nFigure 4: ALiBi models trained and evaluated on varying sequence lengths on the WikiText-103\nvalidation set and the sinusoidal baseline (not evaluated on longer sequences). All of our models\noutperform the sinusoidal ones even when trained on fewer tokens. Appendix Table 5 has exact\nperplexities, more ALiBi models (trained on fewer tokens), and results for rotary and T5 bias models.\nWe ﬁrst develop our method on the WikiText-103 corpus (Merity et al., 2016), replacing the sinu-\nsoidal position embeddings in the language model of Baevski & Auli (2018) with ALiBi.\nFigure 4 (and the corresponding Appendix Table 5) show our results for models trained with varying\nnumbers of input subsequence tokens (L), extrapolating to longer subsequence lengths on the valida-\ntion dataset. Our ﬁrst observation is that, without extrapolation, for every L, our models outperform\nthose using the sinusoidal method, sometimes by a signiﬁcant amount. For example, the Baevski &\nAuli model achieves 18.67±0.24 (std. dev.) perplexity when trained with L = 3072 input tokens,\nbut our L = 3072 model achieves 17.60 perplexity (when both models evaluate with Lvalid = 3072).\n6\n\nPublished as a conference paper at ICLR 2022\nOur second observation is that all of our models can extrapolate, and they obtain improved perplexity\nscores when handling more tokens than they observed during training. For example, our model\ntrained on 512 tokens (which achieves 19.73 perplexity when evaluating subsequences of length\n512 in the development set) achieves a perplexity score of 18.40 on the development set when\nextrapolating to subsequences of length 3072. Surprisingly, this surpasses the score that the L =\n3072 sinusoidal model obtains on the development set by a statistically signiﬁcant margin. Note\nthat all our models trained on L = 512 to L = 2048 outperform the sinusoidal baseline trained\non L = 3072 when extrapolating to Lvalid = 3072 even though those models all take much less\ntime to train since they train on shorter subsequences (Appendix Figure 8 compares training speed\nto perplexity for these models)! The L = 512 model is 1.84 times faster to train and yet still\noutperforms the L = 3072 sinusoidal model when extrapolating to Lvalid = 3072. In addition,\ntraining the L = 3072 sinusoidal model requires a GPU with more than 16 GB of memory to ﬁt the\nlarge attention matrices, which our L = 512 outperforms even though it can be trained on a GPU\nwith much less memory due to much smaller attention matrices.\nAdditionally, Table 5 (in the appendix) also shows that, for Ls of 1024 and 3072, our method per-\nforms better than the rotary and T5 bias models even when Lvalid = L (i.e., no extrapolation is\noccurring). Figure 1 (and the corresponding Appendix Tables 2 and 3) more broadly explore our\nmethod vs. the other position methods. They show that the T5 bias (the best of the baselines) im-\nproves perplexity until Lvalid is around 2L, but on the WikiText-103 dataset our method continually\nimproves perplexity until at least around 3L, with the L = 512 model improving perplexity even\nwhen Lvalid exceeds 12k tokens. Even when unable to improve perplexity given longer sequences,\nALiBi always maintains strong performance as more tokens are added.\nAppendix Table 6 shows that our results on the validation set also transfer to the test set of WikiText-\n103. Currently, almost all models that present results on WikiText-103 use sliding window evalu-\nation (deﬁned in §B) to compute perplexities. We apply that method to our (and to the sinusoidal,\nrotary and T5 bias) models in Appendix Table 7. We ﬁnd that our L = 3072 model surpasses the\nperformance of Transformer-XL (Dai et al., 2019), the Sandwich (Press et al., 2020), and Short-\nformer (Press et al., 2021) models. Our results are similar to the ones obtained with staged train-\ning (Press et al., 2021) but fall short of results obtained by Routing Transformer (Roy et al., 2020)\nand kNN-LM (Khandelwal et al., 2020). The methods used in those models are orthogonal to ours,\nand we hypothesize that combining them with ours might lead to even larger performance increases.\nAfter developing our method on WikiText-103, in Appendix Section A.3, we run one set of experi-\nments on a different domain (books) using a similar model architecture and without modifying any\nof the ALiBi hyperparameters (the slopes) and show that our results fully transfer to this new do-\nmain. Our models are able to both surpass the sinusoidal baseline when not extrapolating while also\noutperforming it when extrapolating to longer sequences.\n4.2\nRESULTS ON THE CC100+ROBERTA CORPUS\nOur ﬁnal set of experiments investigates whether ALiBi transfers to a larger model trained with a\nlarger computational budget on a larger dataset than the ones we previously used. We show that our\nmethod achieves strong results in this more challenging setting, obtaining similar performance to the\nsinusoidal baseline while using signiﬁcantly less memory, since we train on shorter subsequences.\nThe dataset we choose is a combination of the datasets used to train the RoBERTa (Liu et al., 2019)\nimplementation of BERT (Devlin et al., 2019) and the English part of the CC-100 corpus intro-\nduced in Conneau et al. (2020), for a total of 461 GB. The RoBERTa training corpus—i.e., the\nToronto Book Corpus (Zhu et al., 2015), English Wikipedia, CC-News (Nagel, 2016), OpenWeb-\nText (Gokaslan & Cohen, 2019) and Stories (Trinh & Le, 2018))—is 161 gigabytes, and the English\npart of the CC-100 corpus is 300 gigabytes. The validation set contains 649K tokens.\nOur models for this dataset have 25 transformer layers with 16 heads and a dimension of 2048, with\nan 8192 hidden dimension of the feedforward sublayers. These models have 1.3B parameters. We\ntrain our models for one epoch, which is 50k updates on 128 V100 GPUs.\nIn Figure 5 (left), we compare the validation perplexity for Lvalid = 1024 throughout the training\nprocess for an ALiBi model trained with L = 512 compared to the sinusoidal model trained with\nL = 1024. Since our model is trained on shorter sequences, it is 7% faster and uses 1.6 GB less\n7\n\nPublished as a conference paper at ICLR 2022\n0\n1000 2000 3000 4000 5000 6000\nTraining Time (GPU Hours)\n8.5\n9.0\n9.5\n10.0\n10.5\n11.0\nPerple(it) (←)\nValidatio  Perple(it) Through Trai i g\nwith Lvalid = 10←4\nSi usoidal\nL = 10←4\nALiBi\nL = 51←\n0\n1000 2000 3000 4000 5000 6000\nTraining Time (GPU Hours)\n8.5\n9.0\n9.5\n10.0\n10.5\n11.0\nPerple(it) (←)\nValidatio  Perple(it) Through Trai i g\nwith Lvalid = ←048\nSi usoidal\nL = ←048\nALiBi\nL = 10←4\nFigure 5: On the left (right), a 1.3B-parameter ALiBi model trained on 512 (1024) and evaluated on\n1024 (2048) tokens during training, compared to the sinusoidal baseline trained on 1024 (2048) to-\nkens. The ALiBi models obtain strong results even though they use 6%-11% less memory since they\ntrain on shorter sequences. Appendix Table 11 shows memory use and end-of-training perplexities.\nmemory. We halt training of the sinusoidal baseline when our model reaches the end of its training\n(one epoch). At that time, our model is just 0.06 perplexity away from the baseline even though it\nwas trained on sequences that are half the length of those the baseline used and requires less memory.\nIn Figure 5 (right), results become even more impressive, showing that our model trained on L =\n1024 outperforms by 0.09 perplexity the sinusoidal model trained on L = 2048 (when evaluating\nwith Lvalid = 2048) even though our model uses 3.1 GB less memory. Our model maintains a lead\nin perplexity over the sinusoidal model during the entire training process. By sampling ﬁve evenly\ndistributed points across the training process, we compute that our L = 1024 model reaches a given\nperplexity value, on average, 11% faster than the sinusoidal model does.\nSince our models in these comparisons use much less memory, they allow for stacking more layers,\nwhich would further improve performance (with negligible, if any, runtime cost). To keep our\nexperiments as straightforward as possible, however, we do not add layers to our models.\nAppendix Table 12 presents additional results comparing our models to the sinusoidal baseline when\nboth are trained on the same L, showing that ALiBi performs similarly to the sinusoidal baseline\nwhen not extrapolating. This contrasts with the results presented on the smaller datasets, where\nALiBi consistently outperforms other position methods even when not extrapolating, suggesting\nthat ALiBi’s inductive bias provides additional beneﬁts for lower-resource language modeling.\n512\n2500\n5000\n7500\n10000\nValidation Input Length (Lvalid)\n8.6\n9.0\n9.4\n9.8\n10.2\nPerplexity (\n)\nExtrapolation, L = 512\non CC100+RoBERTa\nSinusoidal\nALiBi\n1024 2500\n5000\n7500\n10000\nValidation Input Length (Lvalid)\n8.6\n9.0\n9.4\n9.8\n10.2\nPerplexity (\n)\nExtrapolation, L = 1024\non CC100+RoBERTa\nSinusoidal\nALiBi\nFigure 6: The ALiBi and sinusoidal models (with both L = 512 and 1024) trained for 50k updates (1\nepoch) on the CC100+RoBERTa corpus, extrapolating on the validation set. ALiBi achieves the best\nresults at around 2L but maintains strong performance even up to 10000 tokens in these experiments.\nFigure 6 shows that our models trained on L = 512 and L = 1024 achieve the best results when\nextrapolating to about double the tokens that they were trained on. Speciﬁcally, the L = 512 model\n(that obtains 9.79 perplexity when Lvalid = 512) achieves its best score (9.3) when extrapolating to\n8\n\nPublished as a conference paper at ICLR 2022\n1012 tokens, and the L = 1024 model (that obtains 9.16 perplexity when Lvalid = 1024) achieves its\nbest score (8.9) when extrapolating to 2024 tokens.\nOne possible explanation is that the subsequences the model observes during training are up to L\ntokens long. When performing inference on subsequences of length 2L, half of the subsequences\nthe model consumes are as long as the examples seen during training. When inference is performed\non subsequences of length 2L + 1 or longer, less than half of the predictions the model makes are\non subsequences of lengths seen during training, and that might degrade performance.\nThe sinusoidal model cannot extrapolate at all in this setting, with its performance degrading for\nboth the L = 512 and 1024 models as soon as one token more than L is added during evaluation.\nIn Appendix B, we ﬁnd that ALiBi’s edge over sinusoidal embeddings is largely explained by its\nimproved avoidance of the early token curse. We posit that future work building on ALiBi might\nachieve further gains by more efﬁciently exploiting longer histories.\n5\nRELATED WORK\nIn parallel with our work, Wennberg & Henter (2021) introduce a relative position method that,\nlike our method, adds a bias to attention scores that is a function of the distance between the key\nand query elements. Unlike our ALiBi method, which uses a non-learned linear function, their\nmethod uses a radial-basis function, with multiple trainable parameters (in our experiments, this led\nto a slight decrease in runtime). In addition, they present experiments on text classiﬁcation, not\non language modeling. They do not explore extrapolation. The Distance Aware Transformer (Wu\net al., 2021) multiplies attention scores by a bias that is a function of the distance between the key\nand query. This function uses a different, learned parameter in every head. They show results only\non text classiﬁcation. In our experiments (not presented), multiplying attention scores by the bias\n(instead of adding, as in ALiBi) degraded performance.\nTransformer-XL (Dai et al., 2019) presented a language model that uses a cache and can attend to\nmore tokens during inference than it was trained on (by increasing the length of the cache). However,\nthis work presents results only where output length is limited to the L (the training length), and their\nrelative position method is very slow (Press et al., 2021). The Longformer (Beltagy et al., 2020)\nadapts models trained on shorter sequences to document-level tasks. However, to achieve this they\nhad to partially train their models on longer sequences. Our ALiBi method enables extrapolation\nwithout any additional training on longer sequences.\nTo our knowledge, extrapolation has not been previously explored in transformer language model-\ning, but it has been investigated previously and concurrently with transformers on other tasks, such\nas machine translation (Rosendahl et al., 2019; Neishi & Yoshinaga, 2019; Newman et al., 2020;\nKiyono et al., 2021), sequence-to-sequence models trained on an artiﬁcial dataset (Hupkes et al.,\n2020), pretrained sequence-to-sequence models tested on arithmetic tasks (Nogueira et al., 2021,\nAppendix C), models trained with reinforcement learning (Lampinen et al., 2021), image, speech\nrecognition, and machine translation models (Likhomanenko et al., 2021), and protein structure\nprediction (Jumper et al., 2021, Appendix 1.5).\n6\nCONCLUSION\nWe showed that the sinusoidal position embedding approach does not enable transformers to extrap-\nolate to inputs longer than the ones they were trained on. We then established that extrapolation\nin transformers can be enabled by just changing the position method. We showed that our ALiBi\nmethod offers an extremely simple replacement for existing position approaches and allow models\nto extrapolate. In addition, when not extrapolating, our method achieves either better perplexity\nthan the sinusoidal method (in models smaller than 1B parameters, trained on less data) or similar\nperplexity (in larger, billion parameter models trained on much more data). ALiBi is simple to im-\nplement and does not slow down runtime or require extra parameters (but does occasionally require\na negligible amount of extra memory). Using our method, we sped up the training of a 1.3 billion\nparameter model evaluated on the same input sequence length as GPT-3 (2048).\n9\n\nPublished as a conference paper at ICLR 2022\nACKNOWLEDGMENTS\nWe thank Tim Dettmers, Gabriel Ilharco, Jungo Kasai, Hao Peng, Sewon Min, Soﬁa Serrano, Sam\nShleifer, Luke Zettlemoyer, Julian Michael, Nikolaos Pappas, Yizhong Wang, and the anonymous\nreviewers for their valuable feedback and fruitful discussions.\n10\n\nPublished as a conference paper at ICLR 2022\nREFERENCES\nAlexei Baevski and Michael Auli. Adaptive input representations for neural language modeling.\nCoRR, abs/1809.10853, 2018. URL http://arxiv.org/abs/1809.10853.\nIz Beltagy, Matthew E. Peters, and Arman Cohan. Longformer: The long-document transformer.\narXiv:2004.05150, 2020.\nTom B. Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared Kaplan, Prafulla Dhari-\nwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, Sandhini Agarwal,\nAriel Herbert-Voss, Gretchen Krueger, Tom Henighan, Rewon Child, Aditya Ramesh, Daniel M.\nZiegler, Jeffrey Wu, Clemens Winter, Christopher Hesse, Mark Chen, Eric Sigler, Mateusz Litwin,\nScott Gray, Benjamin Chess, Jack Clark, Christopher Berner, Sam McCandlish, Alec Radford,\nIlya Sutskever, and Dario Amodei. Language models are few-shot learners, 2020.\nAlexis Conneau, Kartikay Khandelwal, Naman Goyal, Vishrav Chaudhary, Guillaume Wenzek,\nFrancisco Guzm´an, Edouard Grave, Myle Ott, Luke Zettlemoyer, and Veselin Stoyanov. Un-\nsupervised cross-lingual representation learning at scale. Proceedings of the 58th Annual Meeting\nof the Association for Computational Linguistics, 2020. doi: 10.18653/v1/2020.acl-main.747.\nURL http://dx.doi.org/10.18653/v1/2020.acl-main.747.\nZihang Dai, Zhilin Yang, Yiming Yang, Jaime Carbonell, Quoc Le, and Ruslan Salakhutdinov.\nTransformer-XL: Attentive language models beyond a ﬁxed-length context. In Proceedings of the\n57th Annual Meeting of the Association for Computational Linguistics, pp. 2978–2988, Florence,\nItaly, July 2019. Association for Computational Linguistics. doi: 10.18653/v1/P19-1285. URL\nhttps://aclanthology.org/P19-1285.\nJacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. BERT: Pre-training of deep\nbidirectional transformers for language understanding. In Proceedings of the 2019 Conference of\nthe North American Chapter of the Association for Computational Linguistics: Human Language\nTechnologies, Volume 1 (Long and Short Papers), pp. 4171–4186, Minneapolis, Minnesota, June\n2019. Association for Computational Linguistics. doi: 10.18653/v1/N19-1423. URL https:\n//www.aclweb.org/anthology/N19-1423.\nAaron Gokaslan and Vanya Cohen. Openwebtext corpus. http://Skylion007.github.io/\nOpenWebTextCorpus, 2019.\nCheng-Zhi Anna Huang, Ashish Vaswani, Jakob Uszkoreit, Ian Simon, Curtis Hawthorne, Noam M.\nShazeer, Andrew M. Dai, M. Hoffman, M. Dinculescu, and D. Eck. Music transformer: Generat-\ning music with long-term structure. In ICLR, 2019.\nDieuwke Hupkes, Verna Dankers, Mathijs Mul, and Elia Bruni. Compositionality decomposed:\nHow do neural networks generalise?\nJournal of Artiﬁcial Intelligence Research, 67:757–795,\nApril 2020.\ndoi: 10.1613/jair.1.11674.\nURL https://doi.org/10.1613/jair.1.\n11674.\nHakan Inan, Khashayar Khosravi, and Richard Socher. Tying word vectors and word classiﬁers: A\nloss framework for language modeling. In ICLR, 2017. URL https://openreview.net/\nforum?id=r1aPbsFle.\nJ. Jumper, Richard Evans, A. Pritzel, Tim Green, Michael Figurnov, O. Ronneberger, Kathryn Tun-\nyasuvunakool, Russ Bates, Augustin Z´ıdek, Anna Potapenko, A. Bridgland, Clemens Meyer,\nSimon A A Kohl, Andy Ballard, A. Cowie, B. Romera-Paredes, Stanislav Nikolov, Rishub Jain,\nJ. Adler, T. Back, Stig Petersen, D. Reiman, Ellen Clancy, Michal Zielinski, Martin Steinegger,\nMichalina Pacholska, Tamas Berghammer, S. Bodenstein, D. Silver, Oriol Vinyals, A. Senior,\nK. Kavukcuoglu, P. Kohli, and D. Hassabis. Highly accurate protein structure prediction with\nalphafold. Nature, 596:583 – 589, 2021.\nUrvashi Khandelwal, Omer Levy, Dan Jurafsky, Luke Zettlemoyer, and Mike Lewis. Generalization\nthrough Memorization: Nearest Neighbor Language Models. In International Conference on\nLearning Representations (ICLR), 2020.\n11\n\nPublished as a conference paper at ICLR 2022\nShun Kiyono, Sosuke Kobayashi, Jun Suzuki, and Kentaro Inui. Shape: Shifted absolute position\nembedding for transformers. ArXiv, abs/2109.05644, 2021.\nAndrew Kyle Lampinen, Stephanie C. Y. Chan, Andrea Banino, and Felix Hill. Towards mental\ntime travel: a hierarchical memory for reinforcement learning agents. CoRR, abs/2105.14039,\n2021. URL https://arxiv.org/abs/2105.14039.\nMike Lewis, Shruti Bhosale, Tim Dettmers, Naman Goyal, and Luke Zettlemoyer. Base layers:\nSimplifying training of large, sparse models, 2021.\nOpher Lieber, Or Sharir, Barak Lenz, and Yoav Shoham. Jurassic-1: Technical details and evalua-\ntion. Technical report, AI21 Labs, August 2021.\nTatiana Likhomanenko, Qiantong Xu, Ronan Collobert, Gabriel Synnaeve, and Alex Rogozhnikov.\nCAPE: encoding relative positions with continuous augmented positional embeddings. CoRR,\nabs/2106.03143, 2021. URL https://arxiv.org/abs/2106.03143.\nYinhan Liu, Myle Ott, Naman Goyal, Jingfei Du, Mandar Joshi, Danqi Chen, Omer Levy, Mike\nLewis, Luke Zettlemoyer, and Veselin Stoyanov. Roberta: A robustly optimized bert pretraining\napproach, 2019.\nStephen Merity, Caiming Xiong, James Bradbury, and Richard Socher. Pointer sentinel mixture\nmodels, 2016.\nTomas Mikolov and G. Zweig. Context dependent recurrent neural network language model. 2012\nIEEE Spoken Language Technology Workshop (SLT), pp. 234–239, 2012.\nTomas Mikolov, M. Karaﬁ´at, L. Burget, J. Cernock´y, and S. Khudanpur. Recurrent neural network\nbased language model. In INTERSPEECH, 2010.\nSebastian\nNagel.\nCc-news.\nhttps://commoncrawl.org/2016/10/\nnews-dataset-available/, 2016.\nSharan Narang, Hyung Won Chung, Yi Tay, William Fedus, Thibault Fevry, Michael Matena, Kar-\nishma Malkan, Noah Fiedel, Noam Shazeer, Zhenzhong Lan, Yanqi Zhou, Wei Li, Nan Ding,\nJake Marcus, Adam Roberts, and Colin Raffel. Do transformer modiﬁcations transfer across\nimplementations and applications?, 2021.\nMasato Neishi and Naoki Yoshinaga.\nOn the relation between position information and sen-\ntence length in neural machine translation.\nIn Proceedings of the 23rd Conference on Com-\nputational Natural Language Learning (CoNLL), pp. 328–338, Hong Kong, China, Novem-\nber 2019. Association for Computational Linguistics.\ndoi: 10.18653/v1/K19-1031.\nURL\nhttps://aclanthology.org/K19-1031.\nBenjamin Newman, John Hewitt, Percy Liang, and Christopher D. Manning. The eos decision and\nlength extrapolation. In BlackBoxNLP@EMNLP, 2020. URL https://nlp.stanford.\nedu/pubs/newman2020extrapolation.pdf.\nRodrigo Nogueira, Zhiying Jiang, and Jimmy J. Li. Investigating the limitations of the transformers\nwith simple arithmetic tasks. ArXiv, abs/2102.13019, 2021.\nMyle Ott, Sergey Edunov, David Grangier, and Michael Auli. Scaling neural machine translation.\nIn Proceedings of the Third Conference on Machine Translation (WMT), 2018.\nAnkur Parikh, Oscar T¨ackstr¨om, Dipanjan Das, and Jakob Uszkoreit.\nA decomposable atten-\ntion model for natural language inference.\nIn Proceedings of the 2016 Conference on Em-\npirical Methods in Natural Language Processing, pp. 2249–2255, Austin, Texas, November\n2016. Association for Computational Linguistics. doi: 10.18653/v1/D16-1244. URL https:\n//aclanthology.org/D16-1244.\nOﬁr Press and Lior Wolf. Using the output embedding to improve language models. In Proceedings\nof the 15th Conference of the European Chapter of the Association for Computational Linguistics:\nVolume 2, Short Papers, pp. 157–163, Valencia, Spain, April 2017. Association for Computational\nLinguistics. URL https://www.aclweb.org/anthology/E17-2025.\n12\n\nPublished as a conference paper at ICLR 2022\nOﬁr Press, Noah A. Smith, and Omer Levy. Improving transformer models by reordering their\nsublayers.\nIn Proceedings of the 58th Annual Meeting of the Association for Computational\nLinguistics, pp. 2996–3005, Online, July 2020. Association for Computational Linguistics. doi:\n10.18653/v1/2020.acl-main.270. URL https://www.aclweb.org/anthology/2020.\nacl-main.270.\nOﬁr Press, Noah A. Smith, and Mike Lewis. Shortformer: Better language modeling using shorter\ninputs. In Proceedings of the 59th Annual Meeting of the Association for Computational Lin-\nguistics and the 11th International Joint Conference on Natural Language Processing (Volume 1:\nLong Papers), pp. 5493–5505, Online, August 2021. Association for Computational Linguistics.\nURL https://aclanthology.org/2021.acl-long.427.\nJack W. Rae, Anna Potapenko, Siddhant M. Jayakumar, Chloe Hillier, and Timothy P. Lilli-\ncrap. Compressive transformers for long-range sequence modelling. In International Confer-\nence on Learning Representations, 2020. URL https://openreview.net/forum?id=\nSylKikSYDH.\nColin Raffel, Noam Shazeer, Adam Roberts, Katherine Lee, Sharan Narang, Michael Matena, Yanqi\nZhou, Wei Li, and Peter J. Liu. Exploring the limits of transfer learning with a uniﬁed text-to-\ntext transformer. Journal of Machine Learning Research, 21(140):1–67, 2020. URL http:\n//jmlr.org/papers/v21/20-074.html.\nJan Rosendahl, Viet Anh Khoa Tran, Weiyue Wang, and Hermann Ney. Analysis of positional\nencodings for neural machine translation. In International Workshop on Spoken Language Trans-\nlation, Hong Kong, China, November 2019.\nAurko Roy, Mohammad Saffar, Ashish Vaswani, and David Grangier. Efﬁcient content-based sparse\nattention with routing transformers, 2020.\nPeter Shaw, Jakob Uszkoreit, and Ashish Vaswani. Self-attention with relative position representa-\ntions. In Proceedings of the 2018 Conference of the North American Chapter of the Association\nfor Computational Linguistics: Human Language Technologies, Volume 2 (Short Papers), pp.\n464–468, New Orleans, Louisiana, June 2018. Association for Computational Linguistics. doi:\n10.18653/v1/N18-2074. URL https://www.aclweb.org/anthology/N18-2074.\nJianlin Su, Yu Lu, Shengfeng Pan, Bo Wen, and Yunfeng Liu. Roformer: Enhanced transformer\nwith rotary position embedding, 2021.\nTrieu H. Trinh and Quoc V. Le. A simple method for commonsense reasoning, 2018.\nAshish Vaswani,\nNoam Shazeer,\nNiki Parmar,\nJakob Uszkoreit,\nLlion Jones,\nAidan N\nGomez, Ł ukasz Kaiser, and Illia Polosukhin.\nAttention is all you need.\nIn I. Guyon,\nU. V. Luxburg, S. Bengio, H. Wallach, R. Fergus, S. Vishwanathan, and R. Garnett\n(eds.), Advances in Neural Information Processing Systems, volume 30. Curran Asso-\nciates, Inc., 2017. URL https://proceedings.neurips.cc/paper/2017/file/\n3f5ee243547dee91fbd053c1c4a845aa-Paper.pdf.\nBen Wang and Aran Komatsuzaki. GPT-J-6B: A 6 Billion Parameter Autoregressive Language\nModel. https://github.com/kingoflolz/mesh-transformer-jax, May 2021.\nUlme Wennberg and Gustav Eje Henter.\nThe case for translation-invariant self-attention in\ntransformer-based language models, 2021.\nThomas Wolf, Lysandre Debut, Victor Sanh, Julien Chaumond, Clement Delangue, Anthony Moi,\nPierric Cistac, Tim Rault, R´emi Louf, Morgan Funtowicz, Joe Davison, Sam Shleifer, Patrick\nvon Platen, Clara Ma, Yacine Jernite, Julien Plu, Canwen Xu, Teven Le Scao, Sylvain Gug-\nger, Mariama Drame, Quentin Lhoest, and Alexander M. Rush. Transformers: State-of-the-art\nnatural language processing. In Proceedings of the 2020 Conference on Empirical Methods in\nNatural Language Processing: System Demonstrations, pp. 38–45, Online, October 2020. As-\nsociation for Computational Linguistics. URL https://www.aclweb.org/anthology/\n2020.emnlp-demos.6.\n13\n\nPublished as a conference paper at ICLR 2022\nChuhan Wu, Fangzhao Wu, and Yongfeng Huang. DA-transformer: Distance-aware transformer.\nIn Proceedings of the 2021 Conference of the North American Chapter of the Association\nfor Computational Linguistics: Human Language Technologies, pp. 2059–2068, Online, June\n2021. Association for Computational Linguistics. doi: 10.18653/v1/2021.naacl-main.166. URL\nhttps://aclanthology.org/2021.naacl-main.166.\nWojciech Zaremba, Ilya Sutskever, and Oriol Vinyals. Recurrent neural network regularization,\n2014.\nYukun Zhu, Ryan Kiros, Rich Zemel, Ruslan Salakhutdinov, Raquel Urtasun, Antonio Torralba, and\nSanja Fidler. Aligning books and movies: Towards story-like visual explanations by watching\nmovies and reading books. In Proceedings of the IEEE international conference on computer\nvision, pp. 19–27, 2015.\n14\n\nPublished as a conference paper at ICLR 2022\nA\nAPPENDIX\nA.1\nINTRODUCTION\nThe training speed of transformer LMs gets slower as the input subsequence length L increases.\nFigure 7 visualizes this.\n64\n128\n256\n512\n1024\n1536\n2048\n3072\nInput Tokens\n0\n5000\n10000\n15000\n20000\n25000\n30000\n35000\n40000\nWords Per Second (\n)\nTraining Speed\nSinusoidal\nALiBi\nFigure 7: Training speed of our model and the sinusoidal baseline trained on different amounts of\ninput subsequence tokens L.\nTable 1 contains the runtimes and memory use statistics for models using the various position meth-\nods discussed in this work.\nTable 1: The speed (during training and evaluation, in words per second) and memory usage (during\ntraining) of the rotary, T5 bias, and ALiBi models compared to the sinusoidal baseline on WikiText-\n103. Training and inference are batched, and speeds are shown for one V100 GPU.\nPosition Method\nTrain Length\nSpeed (↑)\nMemory (↓)\nTrain\nEval.\n512\n28.5k\n82.1k\n15.3 GB\nSinusoidal\n1024\n26.0k\n77.8k\n19.2 GB\n3072\n15.3k\n42.4k\n15.1 GB\n512\n20.0k\n43.4k\n17.8 GB\nRotary\n1024\n17.7k\n39.4k\n22.8 GB\n3072\n11.5k\n29.5k\n17.8 GB\n512\n14.4k\n21.8k\n16.9 GB\nT5 Bias\n1024\n13.0k\n20.2k\n20.9 GB\n3072\n4.3k\n4.9k\n15.9 GB\n512\n28.3k\n85.8k\n15.3 GB\nALiBi\n1024\n25.8k\n76.4k\n19.3 GB\n3072\n15.5k\n42.2k\n15.2 GB\nTables 2, 3, and 4 show the perplexity and runtime of models using the sinusoidal, rotary T5 bias,\nand ALiBi position methods when extrapolating to sequences longer than the ones they were trained\non. The models used in these tables were trained on L = 512, 1024 and 3072 tokens.\n15\n\nPublished as a conference paper at ICLR 2022\nTable 2: The sinusoidal, rotary, T5 bias and ALiBi models trained on L = 512 on WikiText-103 and\nevaluated with different values of Lvalid on the validation set. Bold shows the best score for each\nmodel. Inference speeds (in words per second) are from inference on a GPU with batch size of one.\nSinusoidal\nRotary\nT5 Bias\nALiBi\nInputs\nPPL (↓)\nWPS (↑)\nPPL (↓)\nWPS (↑)\nPPL (↓)\nWPS (↑)\nPPL (↓)\nWPS (↑)\n512\n20.05\n15046\n20.07\n10839\n19.65\n11724\n19.73\n14726\n513\n19.98\n14925\n20.01\n10806\n19.57\n10491\n19.62\n14965\n522\n19.93\n15116\n20.02\n11295\n19.57\n9970\n19.64\n15316\n532\n19.91\n15358\n19.98\n10854\n19.53\n10382\n19.61\n15383\n542\n19.91\n15076\n19.94\n10795\n19.47\n12270\n19.57\n15301\n552\n19.91\n16394\n19.93\n12267\n19.47\n13000\n19.54\n16540\n562\n19.91\n16646\n19.87\n12481\n19.39\n12201\n19.49\n16385\n572\n19.95\n16934\n19.83\n12668\n19.36\n12851\n19.46\n16881\n582\n20.13\n16961\n19.88\n12594\n19.41\n13904\n19.48\n17064\n592\n20.18\n17243\n19.84\n13007\n19.36\n13706\n19.43\n17289\n602\n20.40\n17502\n19.81\n12788\n19.33\n14102\n19.38\n17141\n612\n20.59\n17637\n19.81\n12601\n19.27\n14573\n19.38\n17661\n712\n24.86\n15614\n19.79\n12676\n19.10\n13818\n19.14\n15637\n812\n30.82\n17151\n20.17\n13954\n18.94\n14377\n18.99\n17210\n912\n37.42\n17200\n20.73\n13887\n18.86\n15345\n18.88\n17619\n1012\n43.54\n16304\n21.37\n13759\n18.79\n14240\n18.73\n16059\n1112\n50.36\n16424\n22.01\n13891\n18.77\n14014\n18.68\n16659\n1212\n58.01\n17294\n23.02\n15245\n18.87\n14589\n18.67\n17372\n1312\n63.62\n15314\n23.93\n13698\n18.84\n13138\n18.60\n15698\n1412\n70.75\n15663\n24.81\n13928\n18.87\n12857\n18.59\n15860\n1512\n76.23\n15812\n25.99\n14248\n18.91\n13752\n18.52\n16225\n2512\n132.41\n15254\n31.58\n13456\n20.41\n9948\n18.41\n15204\n3512\n178.97\n13293\n35.54\n11850\n22.91\n7847\n18.40\n13329\n4512\n209.37\n11767\n39.15\n10485\n25.91\n6146\n18.41\n11738\n5512\n240.44\n10168\n43.14\n9020\n29.54\n5309\n18.36\n9986\n6512\n271.40\n9052\n47.81\n8108\n34.48\n4680\n18.35\n9022\n7512\n293.02\n8315\n51.12\n7483\n39.29\n4102\n18.33\n8324\n8512\n305.65\n7259\n54.98\n6718\n43.08\n3660\n18.34\n7366\n9512\n336.02\n6672\n57.85\n6211\n48.90\n3370\n18.34\n6555\n10512\n341.53\n6126\n60.77\n5575\n52.95\n3010\n18.32\n6030\n11512\n362.74\n5994\n66.62\n5445\n61.38\n2873\n18.32\n5882\n12512\n373.17\n5421\n69.70\n4988\n64.94\n2602\n18.31\n5287\n13512\n382.91\n5174\n73.27\n4692\nOOM\n-\n18.31\n4962\n14512\n399.98\n4351\n75.52\n4103\nOOM\n-\n18.31\n4352\n15512\n406.01\n4291\n79.25\n3969\nOOM\n-\n18.31\n4289\n16\n\nPublished as a conference paper at ICLR 2022\nTable 3: The sinusoidal, rotary, T5 bias and ALiBi models trained on L = 1024 on WikiText-103\nand evaluated with different values of Lvalid on the validation set. Bold shows the best score for each\nmodel. Inference speeds (in words per second) are from inference on a GPU with batch size of one.\nSinusoidal\nRotary\nT5 Bias\nALiBi\nInputs\nPPL (↓)\nWPS (↑)\nPPL (↓)\nWPS (↑)\nPPL (↓)\nWPS (↑)\nPPL (↓)\nWPS (↑)\n1024\n19.34\n17002\n19.33\n14690\n18.80\n14973\n18.66\n16951\n1025\n19.33\n16630\n19.34\n14423\n18.82\n14635\n18.67\n16690\n1034\n19.27\n16589\n19.28\n14351\n18.74\n14435\n18.60\n16707\n1044\n19.26\n16760\n19.27\n14491\n18.72\n14644\n18.60\n16667\n1054\n19.23\n16747\n19.26\n14503\n18.71\n14800\n18.58\n16833\n1064\n19.21\n16676\n19.22\n14623\n18.70\n14498\n18.55\n16941\n1074\n19.19\n16879\n19.19\n14464\n18.65\n14670\n18.49\n16936\n1084\n19.22\n16942\n19.23\n14650\n18.70\n14607\n18.56\n17090\n1094\n19.24\n16771\n19.22\n14629\n18.69\n14517\n18.54\n16880\n1104\n19.28\n16870\n19.27\n14837\n18.69\n14635\n18.52\n17009\n1114\n19.29\n16795\n19.27\n14879\n18.69\n14540\n18.52\n17050\n1124\n19.26\n17312\n19.18\n15121\n18.62\n14480\n18.46\n17571\n1224\n20.54\n17901\n19.38\n15584\n18.58\n14956\n18.40\n18013\n1324\n23.13\n16308\n19.96\n14386\n18.52\n13726\n18.33\n16422\n1424\n26.45\n16217\n21.27\n14385\n18.48\n13516\n18.28\n16121\n1524\n29.82\n16377\n22.59\n14693\n18.42\n13587\n18.22\n16659\n1624\n34.27\n15928\n24.34\n14228\n18.40\n12979\n18.17\n16053\n1724\n38.24\n16640\n25.66\n14686\n18.35\n12976\n18.15\n16607\n1824\n42.23\n16840\n27.63\n14918\n18.30\n13071\n18.08\n16846\n1924\n46.46\n15071\n29.64\n13452\n18.31\n11843\n18.08\n15118\n2024\n51.09\n15591\n31.17\n13706\n18.34\n11906\n18.05\n15557\n3024\n96.46\n13639\n35.67\n12256\n18.62\n8480\n17.92\n13668\n4024\n144.00\n12441\n44.30\n11203\n19.44\n7443\n17.95\n12402\n5024\n182.31\n11431\n48.31\n10324\n20.47\n6384\n17.92\n11394\n6024\n214.02\n10238\n54.78\n9117\n21.76\n5577\n18.01\n10119\n7024\n261.86\n8785\n62.83\n7950\n23.64\n4867\n17.93\n8779\n8024\n284.88\n8132\n64.91\n7355\n25.79\n4377\n17.96\n8086\n9024\n310.04\n7045\n71.91\n6380\n27.54\n3787\n17.98\n7001\n10024\n337.48\n6633\n77.70\n6016\n29.54\n3582\n17.97\n6583\n11024\n358.43\n5722\n81.15\n5219\n31.94\n3170\n18.02\n5641\n12024\n375.95\n5560\n87.51\n5072\n33.35\n2940\n18.01\n5294\n13024\n393.57\n4691\n94.74\n4383\nOOM\n-\n17.98\n4621\n14024\n403.52\n4905\n96.10\n4546\nOOM\n-\n18.01\n4827\n15024\n431.66\n4518\n99.78\n4170\nOOM\n-\n17.96\n4447\n16024\n453.32\n4239\n106.99\n3878\nOOM\n-\n17.98\n4153\n17\n\nPublished as a conference paper at ICLR 2022\nTable 4: The sinusoidal, rotary, T5 bias and ALiBi models trained on L = 3072 on WikiText-103\nand evaluated with different values of Lvalid on the validation set. Bold shows the best score for each\nmodel. Inference speeds (in words per second) are from inference on a GPU with batch size of one.\nSinusoidal\nRotary\nT5 Bias\nALiBi\nInputs\nPPL (↓)\nWPS (↑)\nPPL (↓)\nWPS (↑)\nPPL (↓)\nWPS (↑)\nPPL (↓)\nWPS (↑)\n3072\n18.67\n13380\n18.57\n12548\n18.01\n8828\n17.60\n13866\n3073\n18.67\n13773\n18.57\n12474\n18.01\n8483\n17.59\n13793\n3082\n18.62\n13741\n18.54\n12388\n17.95\n8698\n17.59\n13778\n3092\n18.60\n13742\n18.48\n12458\n17.92\n8361\n17.55\n13783\n3102\n18.65\n13701\n18.52\n12365\n17.94\n8764\n17.59\n13747\n3112\n18.64\n13809\n18.51\n12449\n17.96\n8665\n17.59\n13827\n3122\n18.68\n13722\n18.52\n12432\n17.98\n8437\n17.58\n13795\n3132\n18.67\n13825\n18.54\n12490\n17.97\n8653\n17.58\n13784\n3142\n18.69\n13543\n18.52\n12230\n17.97\n8282\n17.61\n13572\n3152\n18.66\n13520\n18.56\n12240\n17.98\n8608\n17.59\n13523\n3162\n18.71\n13501\n18.56\n12253\n18.04\n8589\n17.62\n13598\n3172\n18.72\n13563\n18.55\n12297\n17.99\n8583\n17.59\n13625\n3272\n18.87\n13453\n18.55\n12148\n17.93\n8144\n17.59\n13482\n3372\n19.46\n13533\n18.50\n12254\n17.88\n8442\n17.52\n13565\n3472\n20.55\n13047\n18.52\n11868\n17.95\n7857\n17.54\n13107\n3572\n21.84\n13128\n18.50\n11882\n17.86\n7814\n17.50\n13170\n3672\n23.04\n13106\n18.49\n11859\n17.87\n7719\n17.48\n13196\n3772\n24.47\n13287\n18.54\n11942\n17.85\n7579\n17.49\n13312\n3872\n25.85\n12621\n18.40\n11272\n17.82\n7581\n17.41\n12566\n3972\n27.21\n12379\n18.48\n11151\n17.84\n7483\n17.41\n12324\n4072\n28.59\n12178\n18.59\n11019\n17.88\n6974\n17.48\n12212\n5072\n45.53\n11076\n18.80\n9887\n17.76\n6230\n17.33\n10938\n6072\n65.01\n10114\n19.50\n9049\n17.68\n5554\n17.26\n10133\n7072\n85.96\n8647\n20.60\n7861\n17.83\n4820\n17.22\n8670\n8072\n102.74\n7755\n21.60\n6991\n18.06\n4281\n17.30\n7729\n9072\n125.99\n6953\n22.14\n6360\n18.12\n3823\n17.26\n6939\n10072\n133.68\n6646\n23.21\n6068\n18.37\n3579\n17.28\n6597\n11072\n161.29\n5663\n24.39\n5158\n18.64\n3119\n17.26\n5585\n12072\n169.55\n5567\n26.70\n5111\n18.93\n2920\n17.24\n5397\n13072\n189.43\n5044\n29.33\n4658\n19.10\n2735\n17.15\n4809\n14072\n203.86\n4915\n32.21\n4616\nOOM\n-\n17.22\n4866\n15072\n221.14\n4561\n33.47\n4292\nOOM\n-\n17.23\n4491\n16072\n231.29\n4382\n34.51\n4099\nOOM\n-\n17.22\n4312\n18\n\nPublished as a conference paper at ICLR 2022\nA.2\nALIBI RESULTS ON WIKITEXT-103\n15000\n20000\n25000\n30000\nTraining W rds Per Sec nd (→→\n17.5\n18.0\n18.5\n19.0\nPerple(it) (←→\nL = 512\nL = 102←\nL = 1536\nL = 20←8\nL = 3072\nL = 3072\nTraining Speed vs. Valid Perple(it)\nwith Lvalid = 3072\nSinus idal\nALiBi\nFigure 8: The training speed and validation perplexity (with Lvalid = 3072) for ALiBi models and\nthe sinusoidal model trained with L = 3072. All our models trained on 512 or more tokens achieve\nbetter perplexity than the sinusoidal model even though all of them (except the L = 3072) require\nless time and memory to train.\nFigure 8 depicts a cross section of Figure 4, showing our models with different train lengths and\nthe sinusoidal baseline, all evaluated on Lvalid = 3072 tokens. We observe that all our models with\n512 ≤L < 3072 are faster to train than the sinusoidal model with L = 3072, but they all achieve\ngreater perplexity scores on the validation set. Our model with L = 3072 trains just as fast as the\nsinusoidal one but bests its score by more than one perplexity point; (the standard deviation for the\nthe sinusoidal model with L = 3072 is 0.24).\nTable 5 shows the perplexity values obtained when 8 different ALiBi models, trained on L values\nbetween 64 and 3072, extrapolating to Lvalid values longer than the ones they were trained on. In\naddition, we present results for the sinusoidal, rotary and T5 bias models, with Lvalid = L.\nTable 5: Perplexity when ALiBi extrapolates on the WikiText-103 development set. ∗For results we\npresent for the sinusoidal, rotary and T5 bias models, L = Lvalid (so we do not test the extrapolation\nabilities of those baselines here).\nALiBi\nEvaluation Length\nTrain Length\n64\n128\n256\n512\n1024\n1536\n2048\n3072\n64\n28.46\n24.70\n22.88\n22.09\n21.73\n21.63\n21.59\n21.53\n128\n-\n23.98\n21.70\n20.67\n20.36\n20.29\n20.31\n20.28\n256\n-\n-\n21.29\n19.89\n19.29\n19.13\n19.10\n19.03\n512\n-\n-\n-\n19.73\n18.81\n18.50\n18.48\n18.40\n1024\n-\n-\n-\n-\n18.66\n18.20\n18.05\n17.96\n1536\n-\n-\n-\n-\n-\n18.12\n17.90\n17.72\n2048\n-\n-\n-\n-\n-\n-\n17.91\n17.64\n3072\n-\n-\n-\n-\n-\n-\n-\n17.60\nSinusoidal∗\n28.03\n23.81\n21.45\n20.05\n19.34\n19.05\n18.87\n18.67\nRotary∗\n-\n-\n-\n20.07\n19.33\n-\n-\n18.57\nT5 Bias∗\n-\n-\n-\n19.65\n18.80\n-\n-\n18.01\nTable 6 compares ALiBi to the sinusoidal, rotary and T5 bias baselines on the test set of WikiText-\n103, and Table 7 compares ALiBi to the current state of the art models on that test set.\n19\n\nPublished as a conference paper at ICLR 2022\nTable 6: Test perplexity and runtime on WikiText-103 for two of our ALiBi models and models that\nuse the sinusoidal, rotary and T5 bias methods.\nModel\nParam. ↓\nTrain\nInference\nSpeed↑\nSpeed ↑\nValid ↓\nTest ↓\nSinusoidal, L = 3072\n247M\n15.3k\n13.6k\n18.67\n19.38\nRotary, L = 3072\n247M\n11.5k\n12.2k\n18.57\n19.28\nT5 Bias, L = 3072\n247M\n4.3k\n7.3k\n18.01\n18.73\nALiBi\nL = 512, Lvalid = 3072\n247M\n28.3k\n13.6k\n18.40\n19.08\nL = 3072, Lvalid = 3072\n247M\n15.5k\n13.6k\n17.60\n18.30\nTable 7: Valid and test perplexity scores on WikiText-103 for two of our ALiBi models and models\nthat use the sinusoidal, rotary and T5 bias methods with sliding window evaluation (§B and S=512\nfollowing (Baevski & Auli, 2018; Khandelwal et al., 2020; Press et al., 2021)). The sinusoidal model\npresents our results from training and inference with the model of Baevski & Auli.\nModel\nParam. ↓\nValid ↓\nTest ↓\nAdaptive Inputs (Baevski & Auli, 2018)\n247M\n17.97\n18.70\nTransformer-XL (Dai et al., 2019)\n257M\n-\n18.3\nShortformer (Press et al., 2021)\n247M\n17.47\n18.15\nSandwich Transformer (Press et al., 2020)\n247M\n-\n17.96\nStaged Training (Press et al., 2021)\n247M\n-\n17.56\nCompressive Transformer (Rae et al., 2020)\n329M\n-\n17.1\nRouting Transformer (Roy et al., 2020)\n-\n-\n15.8\nkNN-LM (Khandelwal et al., 2020)\n247M\n15.81\n15.79\nSinusoidal, L = 3072\n247M\n17.95\n18.67\nRotary, L = 3072\n247M\n17.98\n18.72\nT5 Bias, L = 3072\n247M\n17.37\n18.12\nALiBi\nL = 512, Lvalid = 3072\n247M\n18.30\n19.01\nL = 3072, Lvalid = 3072\n247M\n16.97\n17.66\nA.3\nRESULTS ON THE TORONTO BOOK CORPUS\nTo ensure that our results are not speciﬁc to the WikiText-103 corpus, we next apply our model and\nthe baselines to a different domain while using a similar model architecture and the same ALiBi\nslopes as those used in the previous subsection.\nWe emphasize that our set of slopes was chosen by running experiments on the WikiText-103 corpus,\nand here we apply that set of slopes to a model trained on a very different text domain. Throughout\nthe entire process of developing this method, we ran only one set of experiments on this domain\nusing the previously selected set of slopes.\nSpeciﬁcally, we use the Toronto BooksCorpus (Zhu et al., 2015), which has been used to train\nBERT (Devlin et al., 2019) (in conjuction with the English Wikipedia). The corpus is about 700M\ntokens (2.9 GB).\nWe use the same train/validation/test split as Khandelwal et al. (2020) and their tokenization, which\nuses BERT’s vocabulary of 29K byte-pair encodings. Since the vocabulary is much smaller than\nWikiText-103’s, we replace the adaptive word embedding and softmax of Baevski & Auli (2018)\nwith a tied word embedding and softmax matrix (Press & Wolf, 2017; Inan et al., 2017).\nOur results in Figure 9 (and Table 8) replicate our success on the WikiText-103 dataset. Our model\nsurpasses the sinusoidal baseline when trained on the same amount of input tokens (L) and, in\n20\n\nPublished as a conference paper at ICLR 2022\n512\n1024\n3072\nValidation Input Length (Lvalid)\n13.0\n13.5\n14.0\n14.5\n15.0\nPerplexity (()\nALiBi Extrap lating\n n T r nt  B  kC rpus\nL = 512\nSinus idal← L = 512\nL = 1024\nSinus idal← L = 1024\nL = 3072\nSinus idal← L = 3072\nFigure 9: ALiBi-enabled models evaluated on different input lengths on the Toronto BookCorpus.\nOur models extrapolate to longer sequence lengths and outperform the sinusoidal baseline even\nwhen trained on much shorter sequences.\naddition, our model is able to extrapolate to longer sequences at inference. This occurs even though\nour set of slopes was not tuned on this dataset. This result establishes the generality of ALiBi and\nthe particular set of slopes we found and suggests that they may be used on different text domains\nwithout further hyperparameter tuning.\nTables 9 and 10 present the perplexities for our ALiBi models, the baselines, and the current state\nof the art on the Toronto BookCorpus validation and test sets. Our results here mirror our results on\nWikiText-103: we improve over the sinusoidal baseline even when AliBi is trained on fewer tokens.\nTable 8: ALiBi models extrapolating on the Toronto BookCorpus development set. ∗For the results\nof the sinusoidal models, L = Lvalid (so we do not test the extrapolation abilities of those models\nhere).\nEvaluation Length\nTrain Length\n512\n1024\n3072\n512\n14.29\n13.64\n13.55\n1024\n-\n13.86\n13.52\n3072\n-\n-\n13.15\nSinusoidal∗\n14.80\n14.73\n14.46\nTable 9: Validation and test perplexities on the Toronto Book Corpus dataset.\nModel\nParam. ↓\nValid ↓\nTest ↓\nSinusoidal, L = 3072\n247M\n14.46\n11.67\nALiBi\nLtrain = 512, Lvalid = 3072\n247M\n13.55\n10.98\nLtrain = 3072, Lvalid = 3072\n247M\n13.15\n10.73\nA.4\nRESULTS ON THE CC100+ROBERTA CORPUS\nTable 11 compares our 1.3 billion parameter ALiBi models when extrapolating to two times the\nnumber of tokens that they were trained on. We use the sinusoidal model as our baseline, and train\nit for the same amount of time as we train the ALiBi model that we compare it to (and so since our\nALiBi models run faster in this setting, the sinusoidal models complete less updates).\n21\n\nPublished as a conference paper at ICLR 2022\nTable 10: Validation and test perplexities on the Toronto Book Corpus dataset with a sliding window\n(§B). Following (Baevski & Auli, 2018; Khandelwal et al., 2020; Press et al., 2020; 2021), we set\nthe sliding window stride S=512.\nModel\nParam. ↓\nValid ↓\nTest ↓\nkNN-LM (Khandelwal et al., 2020)\n247M\n14.20\n10.89\nShortformer (Press et al., 2021)\n247M\n13.40\n10.88\nSandwich (Press et al., 2020)\n247M\n-\n10.83\nStaged Training (Press et al., 2021)\n247M\n12.80\n10.48\nSinusoidal, L = 3072\n247M\n14.06\n11.40\nALiBi\nL = 512, Lvalid = 3072\n247M\n13.76\n11.11\nL = 3072, Lvalid = 3072\n247M\n12.70\n10.40\nTable 11: Perplexity, memory, and train time on the CC100+RoBERTa corpus for our ALiBi models\nand the sinusoidal baseline. We run our L = 512 (1024) model and the sinusoidal model with L =\n1024 (2048) for the same amount of time. We show that our models achieve strong results even\nthough they use 6–11% less memory.\nTraining\nValid PPL ↓\nMemory ↓\nUpdates\nHours ↓\nLvalid = 1024\nLvalid = 2048\nSinusoidal, Ltrain = 1024\n26.2 GB\n46.7k\n5.5k\n9.24\n-\nALiBi, Ltrain = 512\n24.6 GB\n50.0k\n5.5k\n9.30\n-\nSinusoidal, Ltrain = 2048\n29.3 GB\n44.2k\n5.9k\n-\n9.01\nALiBi, Ltrain = 1024\n26.2 GB\n50.0k\n5.9k\n-\n8.92\nTable 12 compares our 1.3 billion parameter ALiBi models to the sinusoidal baselines, with and\nwithout extrapolation, with all models completing 50,000 updates.\nTable 12: Perplexity, train time and memory use of the sinusoidal and ALiBi models on the\nCC100+RoBERTa corpus when all models are trained with 50k updates.\nTraining\nValid PPL ↓\nMemory ↓Updates Hours ↓Lvalid = 512 Lvalid = 1024 Lvalid = 2048\nSinusoidal, Ltrain = 512\n24.6 GB\n50.0k\n5.5k\n9.71\n37.05\n105.42\nALiBi, Ltrain = 512\n24.6 GB\n50.0k\n5.5k\n9.79\n9.30\n9.54\nSinusoidal, Ltrain = 1024\n26.2 GB\n50.0k\n5.9k\n-\n9.15\n48.85\nALiBi, Ltrain = 1024\n26.2 GB\n50.0k\n5.9k\n-\n9.16\n8.92\nSinusoidal, Ltrain = 2048\n29.3 GB\n50.0k\n6.7k\n-\n-\n8.83\nALiBi, Ltrain = 2048\n29.4 GB\n50.0k\n6.7k\n-\n-\n8.84\nB\nANALYSIS\nIn this section we investigate why ALiBi works so effectively. We ﬁnd that ALiBi’s decrease in\nperplexity when given longer sequences is largely explained by its improved avoidance of the early\ntoken curse. We hypothesize that future work building on ALiBi might achieve further gains by\nmore efﬁciently exploiting longer histories.\n22\n\nPublished as a conference paper at ICLR 2022\nB.1\nDEFINING SLIDING WINDOW EVALUATION AND THE EARLY TOKEN CURSE\nThe big gray cat sat on the mat\nFigure 10: Sliding window evaluation (top; blue) compared to nonoverlapping evaluation (bottom;\nred) on a sequence of 8 words using a model with Lvalid = 4. Nonoverlapping evaluation is much\nfaster since it requires just two inference passes (as opposed to the ﬁve passes required by the siding\nwindow approach). But the sliding window approach provides more context for each prediction.\nSliding Window Inference\nAs mentioned in Section 2, nonoverlapping inference is commonly\nused to evaluate sequences longer than L (the number of tokens in each training subsequence). An\nalternative is to use a sliding window during evaluation (Baevski & Auli, 2018).\nA stride S is picked between 1 and L −1, and the window is advanced by S tokens after each\nforward pass.12 This means that L −S tokens from the previous subsequence are re-encoded, and\nonly S new tokens are output. The advantage is that all outputs in each subsequence after the ﬁrst\nhave at least L −S previous tokens to condition on. However, since tokens must be re-encoded\nmultiple times, this approach is much slower than the nonoverlapping one. When S = 1, we output\none token every inference pass, each using the maximal context window that the model can handle;\nhowever, this is the slowest approach. Figure 10 is a visualization of the nonoverlapping and sliding\nwindow evaluation approaches.\nWe use sliding window inference as a tool to analyze our models, but we note that it is normally\nprohibitively slow in practice (Press et al., 2021).\nEarly Token Curse\nSplitting an evaluation set into subsequences means that predictions occuring\nearly in each subsequence cannot access many previous context tokens (appearing at the end of the\nprevious subsequence). The result, referred to as the early token curse (Press et al., 2021), increases\n(i.e., degrades) perplexity scores. A workaround is to evaluate the model using a sliding window,\ngiving each prediction more context. This solution is slow since it requires many more forward\npasses of the model.\nB.2\nEXTRAPOLATION REDUCES THE EARLY TOKEN CURSE\nWe presented results showing that our ALiBi method (and, to a lesser extent, the T5 bias) allows\nLMs to extrapolate during inference. Two reasons could explain why these methods enable LMs to\nachieve better perplexity given longer input subsequences:\n1. Performance improves because the models can use longer contexts to make more accurate\npredictions. For example, the average article length in the WikiText-103 corpus is about\n3600 tokens; therefore, if a model trained on L = 512 tokens extrapolates to Lvalid =\n3072 tokens during inference and achieves better results, that might be because it can spot\npatterns occurring across more than 512 tokens.\n2. Performance improves because longer input sequences mean the early token curse is re-\nduced. For example, during nonoverlapping evaluation on sequences of length Lvalid =\n1000, 10% of predictions have 100 tokens of context or less. If we rerun nonoverlapping\nevaluation on that model with Lvalid = 2000 tokens, now only 5% of predictions have 100\n12Nonoverlapping inference can be viewed as sliding window inference with stride L.\n23\n\nPublished as a conference paper at ICLR 2022\ntokens of context or less. So, by simply being able to handle longer sequences, a model can\nsubstantially reduce the early token curse and improve performance.13\nTo better understand what might be occurring, we re-evaluate the development set of WikiText-103\nwith our models and the sinusoidal baseline with L = 512, 1024, 3072. However, this time we use\nsliding window evaluation with a stride of S = 1, meaning that we move the sliding window just\none token after every inference pass, giving each prediction the maximum number of context tokens\nthat the model can use.\n512\n1024\n1536\n2048\n3072\nValidation Input Lengt  (Lvalid)\n16.5\n17.0\n17.5\n18.0\n18.5\n19.0\n19.5\nPerplexi). (←)\nALiBi and Sinu(oidal\nEvalua)ion (w/ Sliding Window) \non WikiTex)-103\nSinusoidal, L = 512\nSinusoidal, L = 1024\nSinusoidal, L = 3072\nALiBi, L = 512\nALiBi, L = 1024\nALiBi, L = 3072\nFigure 11: ALiBi models evaluated on different input lengths on WikiText-103 with sliding window\nevaluation (with stride S = 1). Unlike results shown in Figure 4, where performance improves in\neach of our models as we increase the validation sequence length, here performance stays relatively\nﬂat as we increase Lvalid. This might mean that ALiBi increases performance when Lvalid > L not\nbecause it uses longer contexts, but because fewer tokens suffer from the early token curse. Note that\nas in §2, the perplexity of the sinusoidal model explodes when Lvalid > L even when using sliding\nwindow evaluation.\nThe results are shown in Figure 11 and in the corresponding Tables 13 (sinusoidal) and 15 (ALiBi).\nUnsurprisingly, for the sinusoidal model, as in §2, increasing Lvalid causes an explosion in perplexity\neven when using sliding window evaluation. Our ALiBi models cannot improve perplexity when\nlooking at longer sequences in this setting, but they keep perplexity ﬂat when Lvalid increases.\nThis leads us to believe that our perplexity improvement when increasing Lvalid and using nonover-\nlapping evaluation is caused by explanation 2, not explanation 1. Because sliding window evaluation\nprovides long context windows for every prediction made, it curtails the early token curse. In this\nsetting, ALiBi’s performance remains ﬂat when Lvalid increases, leading us to hypothesize that the\ngains seen while increasing Lvalid in §4 were the result of larger Lvalid values mitigating the early\ntoken curse.\nOur ALiBi results mirror what occurs in the model using the T5 bias: when using sliding window\nevaluation, perplexity remains relatively ﬂat when evaluating longer sequences (see Table 14).\nOur analysis reveals that when Lvalid > L, ALiBi might not be using contexts longer than the ones\nit was trained on. This highlights a research direction that could be pursued in future work.\nThese ﬁndings do not lessen the value of ALiBi. When Lvalid = L, ALiBi achieves either superior or\nsimilar results to the sinusoidal method and other alternatives even though it is simpler and requires\nno learned parameters. When evaluating Lvalid > L tokens, even if ALiBi does not attend to more\nthan L tokens, it yields better results than the other alternatives that can be used in this case, i.e.,\nstandard nonoverlapping inference (which is cheap, but does not perform as well) and the more\naccurate sliding window approach (which is very slow).\n13100 tokens is an arbitrary small number used here to represent a short history context, i.e., one in which\nmaking predictions for the next output token would be harder.\n24\n\nPublished as a conference paper at ICLR 2022\nTable 13: Perplexities of the sinusoidal models evaluated with sliding window evaluation with stride\nS = 1 on the WikiText-103 validation dataset.\nEvaluation Length (S = 1)\nTrain Length\n512\n1024\n1536\n2048\n3072\n512\n18.35\n204.42\n264.74\n306.19\n360.12\n1024\n-\n18.05\n206.55\n302.6\n393.71\n3072\n-\n-\n-\n-\n18.03\nTable 14: Perplexities of the T5 bias models evaluated with sliding window evaluation with stride\nS = 1 on the WikiText-103 validation dataset.\nEvaluation Length (S = 1)\nTrain Length\n512\n1024\n1536\n2048\n3072\n512\n17.92\n18.51\n20.36\n22.62\n30.77\n1024\n-\n17.65\n17.87\n18.51\n20.66\n3072\n-\n-\n-\n-\n17.41\nTable 15: Perplexities of the ALiBi models evaluated with sliding window evaluation with stride\nS = 1 on the WikiText-103 validation dataset.\nEvaluation Length (S = 1)\nTrain Length\n512\n1024\n1536\n2048\n3072\n512\n17.98\n17.92\n18.2\n18.28\n18.3\n1024\n-\n17.46\n17.47\n17.62\n17.92\n3072\n-\n-\n-\n-\n16.96\n25\n"
    }
  ]
}