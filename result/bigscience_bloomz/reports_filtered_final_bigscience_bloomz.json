{
  "1-1 (Weights)": "The set of excerpts supplied contains no sentences that mention the availability, location, or access conditions of the BLOOMZ model weights. Consequently, the quotes provide no evidence about whether the weights are public or how they might be obtained. No information is given on hosting services, checkpoints, download procedures, or any restrictions. Therefore, based solely on the provided material, no public‐facing details about the weights can be summarized.",
  "1-2 (Code)": "None of the supplied excerpts discuss code of any kind—whether full training pipelines, data-preparation scripts, configuration files, or inference utilities—for BLOOMZ. There are no references to repositories, open-source releases, or partial disclosures (e.g., fine-tuning only). Thus, the quotes give no insight into the existence, scope, or openness of training code associated with BLOOMZ.",
  "1-3 (License)": "The provided quotes are silent on licensing. They do not specify a license name, version, or any usage clauses (research-only, non-commercial, no-redistribution, etc.). As a result, no licensing terms—covering use, modification, redistribution, or commercial exploitation—can be extracted for BLOOMZ from the given material.",
  "1-4 (Paper)": "The quotes indicate that the BLOOMZ model is documented in a source whose Section II is titled “BLOOMZ & mT0: Instruction-tuned multilingual LLMs with up to 176 B parameters.” They further state that the creators applied “MTF” (multi-task finetuning) to pretrained multilingual BLOOM and mT5 families, producing finetuned variants named BLOOMZ and mT0. Together, these lines reveal two key paper-level facts: (1) BLOOMZ is described alongside mT0 in a technical or academic write-up, and (2) the work focuses on instruction-tuning very large multilingual language models—up to 176 billion parameters—via an MTF procedure that begins from the original BLOOM and mT5 checkpoints.",
  "1-1 (Weights)__evidence": [],
  "1-2 (Code)__evidence": [],
  "1-3 (License)__evidence": [],
  "1-4 (Paper)__evidence": [
    {
      "source": "[sections/https://arxiv.org/abs/2211.01786]",
      "quote": "We apply MTF to the pretrained multilingual BLOOM and mT5 model families to produce finetuned variants called BLOOMZ and mT0."
    },
    {
      "source": "[sections/https://raw.githubusercontent.com/bigscience-workshop/xmtf/master/plotstables/XMTF_ACL2023_POSTER.pdf]",
      "quote": "II. BLOOMZ & mT0: Instruction-tuned multilingual LLMs with up to 176B parameters"
    }
  ],
  "1-5 (Architecture)": "The provided source sentence identifies BLOOMZ as part of the “BLOOMZ & mT0” line of instruction-tuned, multilingual large language models and states that models in this family scale “up to 176 B parameters.” From this, one can conclude that BLOOMZ follows an instruction-tuning paradigm and that its parameter count may be as large as 176 billion, situating it among very-high-capacity LLM architectures.",
  "1-6 (Tokenizer)": "",
  "2-1 (Hardware)": "",
  "2-2 (Software)": "",
  "1-5 (Architecture)__evidence": [
    {
      "source": "[sections/https://raw.githubusercontent.com/bigscience-workshop/xmtf/master/plotstables/XMTF_ACL2023_POSTER.pdf]",
      "quote": "II. BLOOMZ & mT0: Instruction-tuned multilingual LLMs with up to 176B parameters"
    }
  ],
  "1-6 (Tokenizer)__evidence": [],
  "2-1 (Hardware)__evidence": [],
  "2-2 (Software)__evidence": [],
  "2-3 (API)": "The provided material contains no sentences that mention BLOOMZ in the context of an externally accessible inference API, public endpoints, authentication keys, pricing tiers, or developer documentation. Consequently, based solely on the available quotes, there is no evidence that an official or third-party API exists for BLOOMZ, nor any description of how such an API might be accessed or used.",
  "3-1 (Pre-training)": "The only pre-training-related statement explicitly referencing BLOOMZ is: \"We apply MTF to the pretrained multilingual BLOOM and mT5 model families to produce finetuned variants called BLOOMZ and mT0.\" From this we can infer that BLOOMZ is not itself pretrained from scratch; instead, it inherits its core weights from the already-pretrained multilingual BLOOM model family. The quote confirms the existence of a prior BLOOM pre-training phase but supplies no further details on that phase’s corpus composition, training duration, tokenizer settings, model architecture hyperparameters, or optimization schedule. All that can be concluded is that BLOOMZ’s starting point is a multilingual BLOOM checkpoint that underwent an additional method (“MTF”) after the original BLOOM pre-training.",
  "3-2 (Fine-tuning)": "Fine-tuning information is conveyed in two complementary quotes. First, \"We apply MTF to the pretrained multilingual BLOOM and mT5 model families to produce finetuned variants called BLOOMZ and mT0,\" indicating that the fine-tuning technique—referred to as MTF—is applied on top of BLOOM to yield BLOOMZ. Second, the headline-style sentence \"BLOOMZ & mT0: Instruction-tuned multilingual LLMs with up to 176B parameters\" clarifies the goal and scale of this stage: BLOOMZ is an instruction-tuned model, and its parameter count can reach as high as 176 billion. Combining both quotes, the fine-tuning pipeline can be summarized as follows: starting from a pretrained multilingual BLOOM checkpoint, the authors perform an MTF-based instruction-tuning procedure that transforms BLOOM into BLOOMZ, producing a multilingual, instruction-capable language model whose largest variant contains up to 176 B parameters. No further specifics on MTF’s data sources, number of training steps, batch sizes, learning rates, or evaluation checkpoints are provided in the supplied text.",
  "3-3 (Reinforcement Learning)": "There are no sentences in the provided material that discuss reinforcement-learning-based post-training—such as RLHF, RLAIF, RLEF, DPO, or any other preference optimization—applied to BLOOMZ. As a result, no information on policy optimization objectives, reward modeling datasets, human feedback collection, or iteration schedules can be extracted.",
  "2-3 (API)__evidence": [],
  "3-1 (Pre-training)__evidence": [
    {
      "source": "[sections/https://arxiv.org/abs/2211.01786]",
      "quote": "We apply MTF to the pretrained multilingual BLOOM and mT5 model families to produce finetuned variants called BLOOMZ and mT0."
    }
  ],
  "3-2 (Fine-tuning)__evidence": [
    {
      "source": "[sections/https://arxiv.org/abs/2211.01786]",
      "quote": "We apply MTF to the pretrained multilingual BLOOM and mT5 model families to produce finetuned variants called BLOOMZ and mT0."
    },
    {
      "source": "[sections/https://raw.githubusercontent.com/bigscience-workshop/xmtf/master/plotstables/XMTF_ACL2023_POSTER.pdf]",
      "quote": "BLOOMZ & mT0: Instruction-tuned multilingual LLMs with up to 176B parameters"
    }
  ],
  "3-3 (Reinforcement Learning)__evidence": [],
  "4-1 (Pre-training Data)": "",
  "4-2 (Fine-tuning Data)": "Fine-tuning for BLOOMZ was carried out with the MTF (Multitask Fine-tuning) procedure applied to the pretrained multilingual BLOOM family. The fine-tuning relied on xP3, a composite collection of supervised datasets that spans 46 languages and combines both original English prompts and machine-translated prompts. As a consequence of the examples present in these fine-tuning datasets, BLOOMZ exhibits a tendency toward producing short answers, reflecting the generally short length of most training instances.",
  "4-3 (Reinforcement Learning Data)": "",
  "4-4 (Data Filtering)": "",
  "4-1 (Pre-training Data)__evidence": [],
  "4-2 (Fine-tuning Data)__evidence": [
    {
      "source": "[sections/https://arxiv.org/abs/2211.01786]",
      "quote": "We apply MTF to the pretrained multilingual BLOOM and mT5 model families to produce finetuned variants called BLOOMZ and mT0. In addition, we introduce xP3, a composite of supervised datasets in 46 languages with English and machine-translated prompts."
    },
    {
      "source": "[sections/https://raw.githubusercontent.com/bigscience-workshop/xmtf/master/plotstables/XMTF_ACL2023_POSTER.pdf]",
      "quote": "BLOOMZ is biased towards short answers as most training examples are short ⚠"
    }
  ],
  "4-3 (Reinforcement Learning Data)__evidence": [],
  "4-4 (Data Filtering)__evidence": [],
  "__usage": {
    "fine_tuning": "used",
    "rl": "not_used"
  }
}