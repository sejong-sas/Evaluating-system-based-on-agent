{
  "1-5 (Architecture)": "The provided source sentence identifies BLOOMZ as part of the “BLOOMZ & mT0” line of instruction-tuned, multilingual large language models and states that models in this family scale “up to 176 B parameters.” From this, one can conclude that BLOOMZ follows an instruction-tuning paradigm and that its parameter count may be as large as 176 billion, situating it among very-high-capacity LLM architectures.",
  "1-6 (Tokenizer)": "",
  "2-1 (Hardware)": "",
  "2-2 (Software)": "",
  "1-5 (Architecture)__evidence": [
    {
      "source": "[sections/https://raw.githubusercontent.com/bigscience-workshop/xmtf/master/plotstables/XMTF_ACL2023_POSTER.pdf]",
      "quote": "II. BLOOMZ & mT0: Instruction-tuned multilingual LLMs with up to 176B parameters"
    }
  ],
  "1-6 (Tokenizer)__evidence": [],
  "2-1 (Hardware)__evidence": [],
  "2-2 (Software)__evidence": []
}