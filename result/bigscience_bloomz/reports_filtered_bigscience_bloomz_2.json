{
  "1-5 (Architecture)": "The available quote specifies that BLOOMZ is an instruction-tuned multilingual large language model whose scale reaches up to 176 billion parameters. No further architectural details are given in the provided material.",
  "1-6 (Tokenizer)": "",
  "2-1 (Hardware)": "",
  "2-2 (Software)": "",
  "1-5 (Architecture)__evidence": [
    {
      "source": "[sections/https://raw.githubusercontent.com/bigscience-workshop/xmtf/master/plotstables/XMTF_ACL2023_POSTER.pdf]",
      "quote": "II. BLOOMZ & mT0: Instruction-tuned multilingual LLMs with up to 176B parameters"
    }
  ],
  "1-6 (Tokenizer)__evidence": [],
  "2-1 (Hardware)__evidence": [],
  "2-2 (Software)__evidence": []
}