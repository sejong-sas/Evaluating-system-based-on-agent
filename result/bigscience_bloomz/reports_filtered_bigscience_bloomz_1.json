{
  "1-1 (Weights)": "The set of excerpts supplied contains no sentences that mention the availability, location, or access conditions of the BLOOMZ model weights. Consequently, the quotes provide no evidence about whether the weights are public or how they might be obtained. No information is given on hosting services, checkpoints, download procedures, or any restrictions. Therefore, based solely on the provided material, no public‐facing details about the weights can be summarized.",
  "1-2 (Code)": "None of the supplied excerpts discuss code of any kind—whether full training pipelines, data-preparation scripts, configuration files, or inference utilities—for BLOOMZ. There are no references to repositories, open-source releases, or partial disclosures (e.g., fine-tuning only). Thus, the quotes give no insight into the existence, scope, or openness of training code associated with BLOOMZ.",
  "1-3 (License)": "The provided quotes are silent on licensing. They do not specify a license name, version, or any usage clauses (research-only, non-commercial, no-redistribution, etc.). As a result, no licensing terms—covering use, modification, redistribution, or commercial exploitation—can be extracted for BLOOMZ from the given material.",
  "1-4 (Paper)": "The quotes indicate that the BLOOMZ model is documented in a source whose Section II is titled “BLOOMZ & mT0: Instruction-tuned multilingual LLMs with up to 176 B parameters.” They further state that the creators applied “MTF” (multi-task finetuning) to pretrained multilingual BLOOM and mT5 families, producing finetuned variants named BLOOMZ and mT0. Together, these lines reveal two key paper-level facts: (1) BLOOMZ is described alongside mT0 in a technical or academic write-up, and (2) the work focuses on instruction-tuning very large multilingual language models—up to 176 billion parameters—via an MTF procedure that begins from the original BLOOM and mT5 checkpoints.",
  "1-1 (Weights)__evidence": [],
  "1-2 (Code)__evidence": [],
  "1-3 (License)__evidence": [],
  "1-4 (Paper)__evidence": [
    {
      "source": "[sections/https://arxiv.org/abs/2211.01786]",
      "quote": "We apply MTF to the pretrained multilingual BLOOM and mT5 model families to produce finetuned variants called BLOOMZ and mT0."
    },
    {
      "source": "[sections/https://raw.githubusercontent.com/bigscience-workshop/xmtf/master/plotstables/XMTF_ACL2023_POSTER.pdf]",
      "quote": "II. BLOOMZ & mT0: Instruction-tuned multilingual LLMs with up to 176B parameters"
    }
  ]
}