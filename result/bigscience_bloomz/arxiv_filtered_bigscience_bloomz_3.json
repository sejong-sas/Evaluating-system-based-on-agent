{
  "2-3 (API)": "",
  "3-1 (Pre-training)": "The only explicit pre-training detail mentioned for BLOOMZ is comparative: researchers note that mT0 \"consistently outperforms BLOOMZ on Swahili (SW), possibly due to it being a larger part of its pretraining corpus.\"  From this we can infer that BLOOMZ’s own pre-training corpus contains proportionally less Swahili than mT0’s, and that corpus-language balance can directly affect downstream performance on that language.  No additional information about architecture, optimization schedule, or other hyper-parameters is provided in the available quotes.",
  "3-2 (Fine-tuning)": "Fine-tuning is carried out with the MTF procedure applied to the pretrained multilingual BLOOM and mT5 families, yielding the BLOOMZ and mT0 model lines.  Three principal BLOOMZ variants are produced, each offered in multiple sizes:\n• BLOOMZ-P3 – fine-tuned solely on the English-only P3 collection.\n• BLOOMZ – fine-tuned on xP3, a multilingual dataset that keeps English prompts.\n• BLOOMZ-MT – fine-tuned on xP3mt, which augments xP3 with machine-translated prompts in many languages.\n\nEmpirical findings highlighted in the quotes show several important behaviors:\n• \"Finetuned BLOOMZ and BLOOMZ-P3 models significantly improve over BLOOM and XGLM on held-out tasks,\" demonstrating that the MTF fine-tuning step materially boosts performance compared with the original BLOOM checkpoints and with another strong multilingual baseline (XGLM).\n• Although the base BLOOMZ models were not explicitly trained on non-English prompts, they \"are still able to handle them,\" but performance \"drops significantly when translating the prompts to the respective unseen languages.\"  In unseen-language settings, BLOOMZ-MT can actually underperform the base BLOOMZ because it \"has not been finetuned on prompts in these languages.\"\n• Quantitatively, \"Table 1 shows that BLOOMZ performs much better on English than on non-English prompts,\" whereas BLOOMZ-MT \"significantly improves on multilingual prompts,\" confirming the benefit of machine-translated prompt fine-tuning for cross-lingual generalization when the prompt language is represented during fine-tuning.\n\nReproducibility and artifacts: The project supplies an \"additional repository containing the final optimizer states for training with Megatron-Deepspeed\"—users can access these by appending \"-optimizer-states\" to the normal model URL.  This makes it easier for practitioners to resume or further adapt the fine-tuned checkpoints.",
  "3-3 (Reinforcement Learning)": "",
  "2-3 (API)__evidence": [],
  "3-1 (Pre-training)__evidence": [
    {
      "source": "[sections/Results]",
      "quote": "We also find that mT0 consistently outperforms BLOOMZ on Swahili (SW), possibly due to it being a larger part of its pretraining corpus (see Figure 2 and §4.6)."
    },
    {
      "source": "[sections/B Task generalization breakdown]",
      "quote": "We also find that mT0 consistently outperforms BLOOMZ on Swahili (SW), possibly due to it being a larger part of its pretraining corpus (see Figure 2 and §4.6)."
    }
  ],
  "3-2 (Fine-tuning)__evidence": [
    {
      "source": "[abstract]",
      "quote": "We apply MTF to the pretrained multilingual BLOOM and mT5 model families to produce finetuned variants called BLOOMZ and mT0."
    },
    {
      "source": "[pdf_text]",
      "quote": "We produce three core model variants available in different sizes: • BLOOMZ-P3 / mT0-P3: Models finetuned on the English-only P3. • BLOOMZ / mT0: Models finetuned on xP3, which consists of multilingual datasets with English prompts. • BLOOMZ-MT / mT0-MT: Models fine- tuned on xP3mt, which consists of multilingual datasets with English and machine- translated prompts."
    },
    {
      "source": "[pdf_text]",
      "quote": "BLOOMZ and mT0-13B have not been trained on non-English prompts, but are still able to handle them."
    },
    {
      "source": "[pdf_text]",
      "quote": "For BLOOMZ, performance drops significantly when translating the prompts to the respective unseen languages. Unlike on translated prompts for seen languages (§4.3), BLOOMZ-MT performs worse than BLOOMZ for machine-translated prompts in unseen languages. This is likely because BLOOMZ-MT has not been finetuned on prompts in these languages."
    },
    {
      "source": "[sections/Artifacts]",
      "quote": "BLOOMZ models have an additional repository containing the final optimizer states for training with Megatron-Deepspeed that can be found by appending “-optimizer-states\" to the respective URL."
    },
    {
      "source": "[sections/Models]",
      "quote": "We produce three core model variants available in different sizes: • BLOOMZ-P3 / mT0-P3: Models finetuned on the English-only P3. • BLOOMZ / mT0: Models finetuned on xP3, which consists of multilingual datasets with English prompts. • BLOOMZ-MT / mT0-MT: Models fine- tuned on xP3mt, which consists of multilingual datasets with English and machine- translated prompts."
    },
    {
      "source": "[sections/Results]",
      "quote": "Finetuned BLOOMZ and BLOOMZ-P3 models significantly improve over BLOOM and XGLM on held-out tasks."
    },
    {
      "source": "[sections/Multilingual prompting]",
      "quote": "Table 1 shows that BLOOMZ performs much better on English than on non- English prompts. BLOOMZ-MT, which is fine- tuned on xP3mt, significantly improves on multilingual prompts."
    }
  ],
  "3-3 (Reinforcement Learning)__evidence": []
}