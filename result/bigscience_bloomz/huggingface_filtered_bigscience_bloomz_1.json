{
  "1-1 (Weights)": "The only two lines that talk about weight availability both explicitly point to the Hugging Face Hub entry that is named after the target model. The first line sets the variable: “checkpoint = \"bigscience/bloomz\"”.  The very next line shows that those weights can be fetched simply by calling the Hugging Face auto-loader: “model = AutoModelForCausalLM.from_pretrained(checkpoint)”.  Taken together, these quotes make it clear that the BLOOMZ weights are publicly hosted under the identifier bigscience/bloomz and can be downloaded through the standard `from_pretrained` call without any additional authentication steps mentioned in the text.",
  "1-1 (Weights)__evidence": [
    {
      "source": "[readme]",
      "quote": "checkpoint = \"bigscience/bloomz\""
    },
    {
      "source": "[readme]",
      "quote": "model = AutoModelForCausalLM.from_pretrained(checkpoint)"
    }
  ],
  "1-2 (Code)": "The quotes indicate that there is public code support surrounding BLOOMZ.  First, the sentence “We present BLOOMZ & mT0, a family of models capable of following human instructions in dozens of languages zero-shot.” explicitly introduces BLOOMZ in the context of code and tooling.  Immediately after, two concrete code locations are given: “- **Repository:** [bigscience-workshop/xmtf](https://github.com/bigscience-workshop/xmtf)” and “- **Orchestration:** [Megatron-DeepSpeed](https://github.com/bigscience-workshop/Megatron-DeepSpeed)”.  These references show that (i) a GitHub repository called xmtf contains project code relevant to BLOOMZ, and (ii) the training / scaling orchestration relies on the Megatron-DeepSpeed stack.  Although the quotes do not explicitly separate pre-training, fine-tuning, or inference stages, they do demonstrate that code for working with BLOOMZ exists and is publicly linked.",
  "1-2 (Code)__evidence": [
    {
      "source": "[readme]",
      "quote": "We present BLOOMZ & mT0, a family of models capable of following human instructions in dozens of languages zero-shot."
    },
    {
      "source": "[readme]",
      "quote": "- **Repository:** [bigscience-workshop/xmtf](https://github.com/bigscience-workshop/xmtf)"
    },
    {
      "source": "[readme]",
      "quote": "- **Orchestration:** [Megatron-DeepSpeed](https://github.com/bigscience-workshop/Megatron-DeepSpeed)"
    }
  ],
  "1-3 (License)": "Licensing information is conveyed in a single, direct line: “license: bigscience-bloom-rail-1.0”.  This tells the reader that BLOOMZ falls under the BigScience BLOOM RAIL 1.0 license.  No additional constraints or permissions are spelled out in the quoted material, but the presence of this exact tag documents the official license designation attached to the model.",
  "1-3 (License)__evidence": [
    {
      "source": "[readme]",
      "quote": "license: bigscience-bloom-rail-1.0"
    }
  ],
  "1-4 (Paper)": "Two sentences supply publication details.  The bullet “- **Paper:** [Crosslingual Generalization through Multitask Finetuning](https://arxiv.org/abs/2211.01786)” provides the formal title and the direct arXiv link.  A follow-up line—“We refer to Table 7 from our [paper](https://arxiv.org/abs/2211.01786) & [bigscience/evaluation-results](https://huggingface.co/datasets/bigscience/evaluation-results) for zero-shot results on unseen tasks.”—connects that same paper to quantitative evaluation tables and a companion Hugging Face dataset.  Collectively, these quotes confirm that BLOOMZ is documented in the cited technical report and that additional empirical results are publicly accessible.",
  "1-4 (Paper)__evidence": [
    {
      "source": "[readme]",
      "quote": "- **Paper:** [Crosslingual Generalization through Multitask Finetuning](https://arxiv.org/abs/2211.01786)"
    },
    {
      "source": "[readme]",
      "quote": "We refer to Table 7 from our [paper](https://arxiv.org/abs/2211.01786) & [bigscience/evaluation-results](https://huggingface.co/datasets/bigscience/evaluation-results) for zero-shot results on unseen tasks."
    }
  ]
}