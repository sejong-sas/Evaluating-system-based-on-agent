{
  "model": "bigscience/bloomz",
  "scores": {
    "1-2 Code": {
      "score": 0.0,
      "reason": "No training pipeline files; README mentions are ignored."
    },
    "1-3 License": {
      "score": 1.0,
      "reason": "The model is released under “bigscience-bloom-rail-1.0”, which the rubric lists among the licenses that allow all four rights with only minor Responsible-AI restrictions; therefore it is considered Open."
    },
    "1-4 Paper": {
      "score": 1.0,
      "reason": "Provider-authored paper “Cross-lingual Generalization through Multitask Finetuning” (arXiv:2211.01786) is linked in the official model card."
    },
    "1-5 Architecture": {
      "score": 1.0,
      "reason": "Hugging Face presence detected; forced Open per FORCE_HF_CORE_OPEN."
    },
    "1-6 Tokenizer": {
      "score": 1.0,
      "reason": "Hugging Face presence detected; forced Open per FORCE_HF_CORE_OPEN."
    },
    "1-1 Weights": {
      "score": 1.0,
      "reason": "Hugging Face presence detected; forced Open per FORCE_HF_CORE_OPEN."
    },
    "2-1 Hardware": {
      "score": 1.0,
      "reason": "README states training used 36 nodes × 8 A100-80 GB = 288 GPUs plus node specs (AMD CPUs, 512 GB RAM, NVLink-4, OmniPath).  Both type and quantity are disclosed."
    },
    "2-2 Software": {
      "score": 1.0,
      "reason": "README lists the full training stack: Megatron-DeepSpeed (orchestration), DeepSpeed (optimizer/parallelism), PyTorch-1.11 + CUDA-11.5, NVIDIA Apex for FP16."
    },
    "2-3 API": {
      "score": 0.0,
      "reason": "Only generic or non-official mentions; no qualifying official API docs.  No qualifying official API docs via web search."
    },
    "3-1 Pre-training": {
      "score": 0.0,
      "reason": "No quoted material details the BLOOM pre-training pipeline, objectives, or schedules."
    },
    "3-2 Fine-tuning": {
      "score": 0.5,
      "reason": "Quotes describe the Multitask-Finetuning procedure (MTF) and the three dataset mixtures (P3, xP3, xP3mt) but do not give full hyper-parameters; therefore partial disclosure."
    },
    "3-3 Reinforcement Learning": {
      "score": 0.0,
      "reason": "No RL training is mentioned in the evidence. No direct quote evidence; defaulting to Closed by strict rule."
    },
    "4-1 Pre-training Data": {
      "score": 0.5,
      "reason": "BLOOMZ inherits BLOOM’s ROOTS corpus (listed as hundreds of sources in 46 natural + 13 programming languages).  This is a partial but not complete disclosure (no sizes/licensing breakdowns)."
    },
    "4-2 Fine-tuning Data": {
      "score": 0.5,
      "reason": "Datasets P3, xP3 and xP3mt are identified, but counts, licenses and full composition are not given—partial disclosure."
    },
    "4-3 Reinforcement Learning Data": {
      "score": 0.0,
      "reason": "No RL data mentioned. No direct quote evidence; defaulting to Closed by strict rule."
    },
    "4-4 Data Filtering": {
      "score": 0.0,
      "reason": "No information on filtering or cleaning pipelines is provided in the quotes. No direct quote evidence; defaulting to Closed by strict rule."
    }
  },
  "included_scores": {
    "1-2 Code": {
      "score": 0.0,
      "reason": "No training pipeline files; README mentions are ignored."
    },
    "1-3 License": {
      "score": 1.0,
      "reason": "The model is released under “bigscience-bloom-rail-1.0”, which the rubric lists among the licenses that allow all four rights with only minor Responsible-AI restrictions; therefore it is considered Open."
    },
    "1-4 Paper": {
      "score": 1.0,
      "reason": "Provider-authored paper “Cross-lingual Generalization through Multitask Finetuning” (arXiv:2211.01786) is linked in the official model card."
    },
    "1-5 Architecture": {
      "score": 1.0,
      "reason": "Hugging Face presence detected; forced Open per FORCE_HF_CORE_OPEN."
    },
    "1-6 Tokenizer": {
      "score": 1.0,
      "reason": "Hugging Face presence detected; forced Open per FORCE_HF_CORE_OPEN."
    },
    "1-1 Weights": {
      "score": 1.0,
      "reason": "Hugging Face presence detected; forced Open per FORCE_HF_CORE_OPEN."
    },
    "2-1 Hardware": {
      "score": 1.0,
      "reason": "README states training used 36 nodes × 8 A100-80 GB = 288 GPUs plus node specs (AMD CPUs, 512 GB RAM, NVLink-4, OmniPath).  Both type and quantity are disclosed."
    },
    "2-2 Software": {
      "score": 1.0,
      "reason": "README lists the full training stack: Megatron-DeepSpeed (orchestration), DeepSpeed (optimizer/parallelism), PyTorch-1.11 + CUDA-11.5, NVIDIA Apex for FP16."
    },
    "2-3 API": {
      "score": 0.0,
      "reason": "Only generic or non-official mentions; no qualifying official API docs.  No qualifying official API docs via web search."
    },
    "3-1 Pre-training": {
      "score": 0.0,
      "reason": "No quoted material details the BLOOM pre-training pipeline, objectives, or schedules."
    },
    "3-2 Fine-tuning": {
      "score": 0.5,
      "reason": "Quotes describe the Multitask-Finetuning procedure (MTF) and the three dataset mixtures (P3, xP3, xP3mt) but do not give full hyper-parameters; therefore partial disclosure."
    },
    "4-1 Pre-training Data": {
      "score": 0.5,
      "reason": "BLOOMZ inherits BLOOM’s ROOTS corpus (listed as hundreds of sources in 46 natural + 13 programming languages).  This is a partial but not complete disclosure (no sizes/licensing breakdowns)."
    },
    "4-2 Fine-tuning Data": {
      "score": 0.5,
      "reason": "Datasets P3, xP3 and xP3mt are identified, but counts, licenses and full composition are not given—partial disclosure."
    },
    "4-4 Data Filtering": {
      "score": 0.0,
      "reason": "No information on filtering or cleaning pipelines is provided in the quotes. No direct quote evidence; defaulting to Closed by strict rule."
    }
  },
  "final_score_10pt": 6.071,
  "meta": {
    "usage_from_dispatch": {
      "fine_tuning": "used",
      "rl": "not_used"
    },
    "excluded": [
      "3-3 Reinforcement Learning",
      "4-3 Reinforcement Learning Data"
    ],
    "denominator": 14,
    "raw_sum": 8.5,
    "scale": "10/14",
    "code_detection_reason": "No training pipeline files; README mentions are ignored.",
    "pretrain_sources_used": true
  }
}