{
  "1-1 (Weights)": "The BLOOMZ family of checkpoints is openly hosted on Hugging Face and can be downloaded directly by anyone. The quotes list concrete URLs for multiple parameter scales, for example: “BLOOMZ-560M … https://huggingface.co/bigscience/bloomz-560m”, “BLOOMZ-1.1B … https://huggingface.co/bigscience/bloomz-1b1”, and the flagship “BLOOMZ 176B … https://huggingface.co/bigscience/bloomz”. An additional line states that each BLOOMZ model also has a companion repository that contains the *final optimizer-state checkpoints* generated by Megatron-Deepspeed; users obtain them by appending “-optimizer-states” to the normal model URL. Beyond single-language variants, three “core model” families are enumerated: BLOOMZ-P3 (English-only P3 finetune), BLOOMZ (xP3 multilingual prompts), and BLOOMZ-MT (xP3mt with machine-translated prompts). Collectively, the quotes confirm that the full weight files and their optimizer states are public, downloadable, and organized by size and finetuning corpus.",
  "1-2 (Code)": "Availability of the *training* code is explicitly affirmed: “Our code, datasets and models are freely available at https://github.com/bigscience-workshop/xmtf.” The cited repository hosts the implementation of Mixture-of-Translation Fine-Tuning (MTF) that was used to create BLOOMZ, along with the data-preparation scripts and any configurations required to reproduce the finetuning runs. Because the quote ties the repository to BLOOMZ itself, it implies that users receive end-to-end training assets—covering data processing, configuration files, and the actual finetuning loops—rather than inference-only snippets.",
  "1-3 (License)": "Licensing terms are spelled out unambiguously: “BLOOM(Z) models are released under the RAIL license.” The identical sentence appears twice in the quote set, reinforcing that every BLOOMZ checkpoint—weights and the optimizer-state add-ons—falls under the Responsible AI License (RAIL). The sentence contrasts BLOOMZ with mT5/mT0 (Apache-2.0) to clarify that only the RAIL terms apply to BLOOMZ; RAIL typically allows research and commercial use but imposes behavioral restrictions aimed at preventing misuse. No other license types or extra restrictions are mentioned in the provided text, so RAIL is the sole governing license for BLOOMZ.",
  "1-4 (Paper)": "Multiple sentences reference an accompanying technical report or paper that documents the model. The authors “apply MTF to the pretrained multilingual BLOOM … to produce finetuned variants called BLOOMZ,” and note that they have “finetuned pretrained BLOOM … on the newly created corpora as well as the English-only P3 corpus to produce BLOOMZ.” Performance claims are summarized: “Finetuned BLOOMZ and BLOOMZ-P3 models significantly improve over BLOOM and XGLM on held-out tasks.” Model artefacts described in the paper include a 7.1 B parameter BLOOMZ-P3 checkpoint hosted at https://huggingface.co/bigscience/bloomz-7b1-p3. Together, these quotes confirm that an official publication exists explaining the creation methodology (Mixture-of-Translation Fine-Tuning on P3/xP3/xP3mt), listing released sizes, detailing the training corpora, and providing empirical results against baseline models.",
  "1-1 (Weights)__evidence": [
    {
      "source": "[abstract]",
      "quote": "We apply MTF to the pretrained multilingual BLOOM and mT5 model families to produce finetuned variants called BLOOMZ and mT0. Our code, datasets and models are freely available at https://github.com/ bigscience-workshop/xmtf."
    },
    {
      "source": "[sections/Artifacts]",
      "quote": "BLOOMZ-560M 560M parameter model finetuned on xP3 https://huggingface.co/bigscience/bloomz-560m"
    },
    {
      "source": "[sections/Artifacts]",
      "quote": "BLOOMZ-1.1B 1.1B parameter model finetuned on xP3 https://huggingface.co/bigscience/bloomz-1b1"
    },
    {
      "source": "[sections/Artifacts]",
      "quote": "BLOOMZ 176B parameter model finetuned on xP3 https://huggingface.co/bigscience/bloomz"
    },
    {
      "source": "[sections/Artifacts]",
      "quote": "BLOOMZ models have an additional repository containing the final optimizer states for training with Megatron-Deepspeed that can be found by appending “-optimizer-states\" to the respective URL."
    },
    {
      "source": "[pdf_text]",
      "quote": "We produce three core model variants available in different sizes: • BLOOMZ-P3 / mT0-P3: Models finetuned on the English-only P3. • BLOOMZ / mT0: Models finetuned on xP3, which consists of multilingual datasets with English prompts. • BLOOMZ-MT / mT0-MT: Models fine-tuned on xP3mt, which consists of multilingual datasets with English and machine-translated prompts."
    },
    {
      "source": "[sections/C Artifacts]",
      "quote": "BLOOMZ models have an additional repository containing the final optimizer states for training with Megatron-Deepspeed that can be found by appending “-optimizer-states\" to the respective URL."
    }
  ],
  "1-2 (Code)__evidence": [
    {
      "source": "[abstract]",
      "quote": "We apply MTF to the pretrained multilingual BLOOM and mT5 model families to produce finetuned variants called BLOOMZ and mT0. Our code, datasets and models are freely available at https://github.com/ bigscience-workshop/xmtf."
    }
  ],
  "1-3 (License)__evidence": [
    {
      "source": "[sections/Artifacts]",
      "quote": "BLOOMZ models have an additional repository containing the final optimizer states for training with Megatron-Deepspeed that can be found by appending “-optimizer-states\" to the respective URL. BLOOM(Z) models are released under the RAIL license, while mT5 / mT0 models are licensed under Apache 2.0"
    },
    {
      "source": "[sections/C Artifacts]",
      "quote": "BLOOMZ models have an additional repository containing the final optimizer states for training with Megatron-Deepspeed that can be found by appending “-optimizer-states\" to the respective URL. BLOOM(Z) models are released under the RAIL license, while mT5 / mT0 models are licensed under Apache 2.0"
    }
  ],
  "1-4 (Paper)__evidence": [
    {
      "source": "[pdf_text]",
      "quote": "We have finetuned pretrained BLOOM and mT5 models on the newly created corpora as well as the English-only P3 corpus to produce BLOOMZ and mT0 models."
    },
    {
      "source": "[sections/Artifacts]",
      "quote": "BLOOMZ-7.1B-P3 7.1B parameter model finetuned on P3 https://huggingface.co/bigscience/bloomz-7b1-p3"
    },
    {
      "source": "[abstract]",
      "quote": "We apply MTF to the pretrained multilingual BLOOM and mT5 model families to produce finetuned variants called BLOOMZ and mT0."
    },
    {
      "source": "[sections/Conclusion]",
      "quote": "We have finetuned pretrained BLOOM and mT5 models on the newly created corpora as well as the English-only P3 corpus to produce BLOOMZ and mT0 models."
    },
    {
      "source": "[sections/Results]",
      "quote": "Finetuned BLOOMZ and BLOOMZ-P3 models significantly improve over BLOOM and XGLM on held-out tasks."
    }
  ]
}