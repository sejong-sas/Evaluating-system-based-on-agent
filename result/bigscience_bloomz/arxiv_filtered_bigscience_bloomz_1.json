{
  "1-1 (Weights)": "The quotes state that multiple checkpoints of the model are already hosted on Hugging Face.  One sentence specifies: “BLOOMZ-560M  560M parameter model finetuned on xP3  https://huggingface.co/bigscience/bloomz-560m”, confirming that the 560 million-parameter version is publicly posted at that URL.  Another sentence discloses the large 176 billion-parameter edition in exactly the same way: “BLOOMZ  176B parameter model finetuned on xP3  https://huggingface.co/bigscience/bloomz”.  The material also clarifies how to download optimizer checkpoints produced during training: “BLOOMZ models have an additional repository containing the final optimizer states for training with Megatron-Deepspeed that can be found by appending “-optimizer-states\" to the respective URL.”  Regarding breadth of releases, the authors write: “We produce three core model variants available in different sizes: • BLOOMZ-P3 / mT0-P3: Models finetuned on the English-only P3. • BLOOMZ / mT0: Models finetuned on xP3, which consists of multilingual datasets with English prompts. • BLOOMZ-MT / mT0-MT: Models fine-tuned on xP3mt, which consists of multilingual datasets with English and machine-translated prompts.”  Together, these statements reveal that anyone can obtain weight files for several BLOOMZ sizes and sub-variants directly from Hugging Face, and that associated Megatron-Deepspeed optimizer state directories are also publicly exposed via a systematic “-optimizer-states” suffix.",
  "1-2 (Code)": "No sentence in the supplied material mentions BLOOMZ together with any form of publicly released training pipeline, data-processing script, configuration file, or fine-tuning code.  Therefore, based solely on the given quotes, there is **no evidence** that training code—whether for pre-training, fine-tuning, or reinforcement learning—has been open-sourced for BLOOMZ.",
  "1-3 (License)": "The only licensing information that explicitly names the model says: “BLOOMZ models have an additional repository containing the final optimizer states for training with Megatron-Deepspeed that can be found by appending “-optimizer-states\" to the respective URL. BLOOM(Z) models are released under the RAIL license, while mT5 / mT0 models are licensed under Apache 2.0”.  From this, we learn that every BLOOMZ release—weights and optimizer states alike—is distributed under the Responsible AI License (RAIL), and **not** under standard permissive licenses such as Apache 2.0.  No further text spells out the RAIL’s specific clauses, but the passage establishes that BLOOMZ inherits the RAIL’s conditions for use, modification, redistribution, and any potential commercial activity.",
  "1-4 (Paper)": "The single paper-related quote reads: “We apply MTF to the pretrained multilingual BLOOM and mT5 model families to produce finetuned variants called BLOOMZ and mT0.”  This indicates that an academic or technical write-up exists where the authors describe Multitask Finetuning (MTF) applied to BLOOM, giving rise to BLOOMZ.  No direct DOI, arXiv link, or conference citation is included in the supplied sentences, but the statement confirms the presence of an official publication that documents the methodology and positioning of BLOOMZ as the finetuned successor of BLOOM within the multilingual model family.",
  "1-1 (Weights)__evidence": [
    {
      "source": "[sections/C Artifacts]",
      "quote": "BLOOMZ-560M  560M parameter model finetuned on xP3  https://huggingface.co/bigscience/bloomz-560m"
    },
    {
      "source": "[sections/C Artifacts]",
      "quote": "BLOOMZ  176B parameter model finetuned on xP3  https://huggingface.co/bigscience/bloomz"
    },
    {
      "source": "[sections/C Artifacts]",
      "quote": "BLOOMZ models have an additional repository containing the final optimizer states for training with Megatron-Deepspeed that can be found by appending “-optimizer-states\" to the respective URL."
    },
    {
      "source": "[sections/Models]",
      "quote": "We produce three core model variants available in different sizes: • BLOOMZ-P3 / mT0-P3: Models finetuned on the English-only P3. • BLOOMZ / mT0: Models finetuned on xP3, which consists of multilingual datasets with English prompts. • BLOOMZ-MT / mT0-MT: Models fine-tuned on xP3mt, which consists of multilingual datasets with English and machine-translated prompts."
    },
    {
      "source": "[sections/Artifacts]",
      "quote": "BLOOMZ models have an additional repository containing the final optimizer states for training with Megatron-Deepspeed that can be found by appending “-optimizer-states\" to the respective URL."
    }
  ],
  "1-2 (Code)__evidence": [],
  "1-3 (License)__evidence": [
    {
      "source": "[sections/Artifacts]",
      "quote": "BLOOMZ models have an additional repository containing the final optimizer states for training with Megatron-Deepspeed that can be found by appending “-optimizer-states\" to the respective URL. BLOOM(Z) models are released under the RAIL license, while mT5 / mT0 models are licensed under Apache 2.0"
    }
  ],
  "1-4 (Paper)__evidence": [
    {
      "source": "[abstract]",
      "quote": "We apply MTF to the pretrained multilingual BLOOM and mT5 model families to produce finetuned variants called BLOOMZ and mT0."
    }
  ]
}