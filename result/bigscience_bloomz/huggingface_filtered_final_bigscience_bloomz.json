{
  "1-1 (Weights)": "The only two lines that talk about weight availability both explicitly point to the Hugging Face Hub entry that is named after the target model. The first line sets the variable: “checkpoint = \"bigscience/bloomz\"”.  The very next line shows that those weights can be fetched simply by calling the Hugging Face auto-loader: “model = AutoModelForCausalLM.from_pretrained(checkpoint)”.  Taken together, these quotes make it clear that the BLOOMZ weights are publicly hosted under the identifier bigscience/bloomz and can be downloaded through the standard `from_pretrained` call without any additional authentication steps mentioned in the text.",
  "1-1 (Weights)__evidence": [
    {
      "source": "[readme]",
      "quote": "checkpoint = \"bigscience/bloomz\""
    },
    {
      "source": "[readme]",
      "quote": "model = AutoModelForCausalLM.from_pretrained(checkpoint)"
    }
  ],
  "1-2 (Code)": "The quotes indicate that there is public code support surrounding BLOOMZ.  First, the sentence “We present BLOOMZ & mT0, a family of models capable of following human instructions in dozens of languages zero-shot.” explicitly introduces BLOOMZ in the context of code and tooling.  Immediately after, two concrete code locations are given: “- **Repository:** [bigscience-workshop/xmtf](https://github.com/bigscience-workshop/xmtf)” and “- **Orchestration:** [Megatron-DeepSpeed](https://github.com/bigscience-workshop/Megatron-DeepSpeed)”.  These references show that (i) a GitHub repository called xmtf contains project code relevant to BLOOMZ, and (ii) the training / scaling orchestration relies on the Megatron-DeepSpeed stack.  Although the quotes do not explicitly separate pre-training, fine-tuning, or inference stages, they do demonstrate that code for working with BLOOMZ exists and is publicly linked.",
  "1-2 (Code)__evidence": [
    {
      "source": "[readme]",
      "quote": "We present BLOOMZ & mT0, a family of models capable of following human instructions in dozens of languages zero-shot."
    },
    {
      "source": "[readme]",
      "quote": "- **Repository:** [bigscience-workshop/xmtf](https://github.com/bigscience-workshop/xmtf)"
    },
    {
      "source": "[readme]",
      "quote": "- **Orchestration:** [Megatron-DeepSpeed](https://github.com/bigscience-workshop/Megatron-DeepSpeed)"
    }
  ],
  "1-3 (License)": "Licensing information is conveyed in a single, direct line: “license: bigscience-bloom-rail-1.0”.  This tells the reader that BLOOMZ falls under the BigScience BLOOM RAIL 1.0 license.  No additional constraints or permissions are spelled out in the quoted material, but the presence of this exact tag documents the official license designation attached to the model.",
  "1-3 (License)__evidence": [
    {
      "source": "[readme]",
      "quote": "license: bigscience-bloom-rail-1.0"
    }
  ],
  "1-4 (Paper)": "Two sentences supply publication details.  The bullet “- **Paper:** [Crosslingual Generalization through Multitask Finetuning](https://arxiv.org/abs/2211.01786)” provides the formal title and the direct arXiv link.  A follow-up line—“We refer to Table 7 from our [paper](https://arxiv.org/abs/2211.01786) & [bigscience/evaluation-results](https://huggingface.co/datasets/bigscience/evaluation-results) for zero-shot results on unseen tasks.”—connects that same paper to quantitative evaluation tables and a companion Hugging Face dataset.  Collectively, these quotes confirm that BLOOMZ is documented in the cited technical report and that additional empirical results are publicly accessible.",
  "1-4 (Paper)__evidence": [
    {
      "source": "[readme]",
      "quote": "- **Paper:** [Crosslingual Generalization through Multitask Finetuning](https://arxiv.org/abs/2211.01786)"
    },
    {
      "source": "[readme]",
      "quote": "We refer to Table 7 from our [paper](https://arxiv.org/abs/2211.01786) & [bigscience/evaluation-results](https://huggingface.co/datasets/bigscience/evaluation-results) for zero-shot results on unseen tasks."
    }
  ],
  "1-5 (Architecture)": "The bigscience/bloomz model follows the numerical configuration spelled out in its public config: it stacks 70 transformer layers, each layer operates with 112 self-attention heads, and the model’s hidden/embedding width is 14 336. These figures capture the depth (n_layer = 70), the parallel attention capacity (num_attention_heads = 112), and the width of every vector flowing through the network (n_embed = 14 336), defining the core architectural scale for BLOOMZ.",
  "1-5 (Architecture)__evidence": [
    {
      "source": "[readme]",
      "quote": "- **Architecture:** Same as [bloom](https://huggingface.co/bigscience/bloom), also refer to the `config.json` file"
    },
    {
      "source": "[config]",
      "quote": "\"n_layer\": 70,"
    },
    {
      "source": "[config]",
      "quote": "\"num_attention_heads\": 112,"
    },
    {
      "source": "[config]",
      "quote": "\"n_embed\": 14336,"
    }
  ],
  "1-6 (Tokenizer)": "The BLOOMZ checkpoint (checkpoint = \"bigscience/bloomz\") ships with a tokenizer.json file whose vocabulary size is fixed at 250 880 tokens. The presence of tokenizer.json indicates the full tokenizer definition can be downloaded and loaded locally, giving users the exact same sub-word inventory—250 880 entries—that was used during pre-training and instruction tuning of bigscience/bloomz.",
  "1-6 (Tokenizer)__evidence": [
    {
      "source": "[config]",
      "quote": "\"vocab_size\": 250880"
    },
    {
      "source": "[readme]",
      "quote": "checkpoint = \"bigscience/bloomz\""
    },
    {
      "source": "[files]",
      "quote": "tokenizer.json"
    }
  ],
  "2-1 (Hardware)": "Model pre-training / fine-tuning for bigscience/bloomz was carried out on a multi-node cluster of AMD CPU hosts, each node equipped with 512 GB of system memory. GPU acceleration relied on 36 nodes, each containing 8 NVIDIA A100 80 GB cards, for an aggregate of 288 A100 80 GB GPUs. Within every node the eight GPUs are linked by NVLink 4, and across nodes the fabric is extended by four OmniPath links, giving the bandwidth needed for large-scale distributed training.",
  "2-1 (Hardware)__evidence": [
    {
      "source": "[readme]",
      "quote": "- **CPUs:** AMD CPUs with 512GB memory per node"
    },
    {
      "source": "[readme]",
      "quote": "- **GPUs:** 288 A100 80GB GPUs with 8 GPUs per node (36 nodes) using NVLink 4 inter-gpu connects, 4 OmniPath links"
    }
  ],
  "2-2 (Software)": "The training stack for bigscience/bloomz combines several open-source components: Megatron-DeepSpeed provides the overall orchestration layer; DeepSpeed itself supplies the optimizer implementation and distributed/parallelism strategies; core neural network operations run on PyTorch (pytorch-1.11 compiled with CUDA-11.5); and, where half-precision is employed, NVIDIA Apex is used to handle FP16 computation.",
  "2-2 (Software)__evidence": [
    {
      "source": "[readme]",
      "quote": "- **Orchestration:** [Megatron-DeepSpeed](https://github.com/bigscience-workshop/Megatron-DeepSpeed)"
    },
    {
      "source": "[readme]",
      "quote": "- **Optimizer & parallelism:** [DeepSpeed](https://github.com/microsoft/DeepSpeed)"
    },
    {
      "source": "[readme]",
      "quote": "- **Neural networks:** [PyTorch](https://github.com/pytorch/pytorch) (pytorch-1.11 w/ CUDA-11.5)"
    },
    {
      "source": "[readme]",
      "quote": "- **FP16 if applicable:** [apex](https://github.com/NVIDIA/apex)"
    }
  ],
  "2-3 (API)": "The available material contains a code-level reference indicating that the model can be reached programmatically through a checkpoint string: “checkpoint = \"bigscience/bloomz\"”. From this single line we can infer that any user who wishes to load or call the model via a library-style API (for example a typical Python or REST-style interface that expects a model identifier) simply passes the exact name \"bigscience/bloomz\". The presence of this explicit checkpoint tag implies that the model is published under that handle, discoverable, and loadable without further indirection, and that all subsequent API calls will recognize that identifier as the canonical entry point for BLOOMZ.",
  "2-3 (API)__evidence": [
    {
      "source": "[readme]",
      "quote": "checkpoint = \"bigscience/bloomz\""
    }
  ],
  "3-1 (Pre-training)": "",
  "3-1 (Pre-training)__evidence": [],
  "3-2 (Fine-tuning)": "The information states: “We present BLOOMZ & mT0, a family of models capable of following human instructions in dozens of languages zero-shot. We finetune BLOOM & mT5 pretrained multilingual language models on our crosslingual task mixture (xP3) and find the resulting models capable of crosslingual generalization to unseen tasks & languages.” From this, one can give a detailed picture of the fine-tuning phase. First, the base models are BLOOM (and its mT5 counterpart). Second, these base models undergo an instruction-finetuning procedure that uses the xP3 ‘cross-lingual task mixture’. The finetuning objective is explicitly to teach the models to “follow human instructions”, not just to continue text, and to do so across “dozens of languages”. The xP3 mixture therefore provides diverse, multilingual, task-oriented prompts during training. The result of the process is re-branded models—BLOOMZ and mT0—which, after this single finetuning stage, demonstrate “crosslingual generalization to unseen tasks & languages”, meaning they perform zero-shot instruction following outside the languages and tasks directly observed during finetuning.",
  "3-2 (Fine-tuning)__evidence": [
    {
      "source": "[readme]",
      "quote": "We present BLOOMZ & mT0, a family of models capable of following human instructions in dozens of languages zero-shot. We finetune BLOOM & mT5 pretrained multilingual language models on our crosslingual task mixture (xP3) and find the resulting models capable of crosslingual generalization to unseen tasks & languages."
    }
  ],
  "3-3 (Reinforcement Learning)": "",
  "3-3 (Reinforcement Learning)__evidence": [],
  "4-1 (Pre-training Data)": "The provided quote set contains no sentences that mention \"bloomz\" together with any facts about its pre-training corpus. As a result, the available material does not disclose the types of data, sources, licenses, quantities, or any other characteristics of the pre-training data used for BLOOMZ.",
  "4-1 (Pre-training Data)__evidence": [],
  "4-2 (Fine-tuning Data)": "The quotes indicate that BLOOMZ is obtained by fine-tuning the original BLOOM (and, in parallel work, mT5) on a cross-lingual, instruction-following task mixture referred to as xP3 (dataset identifier: \"bigscience/xP3\"). This mixture is expressly designed to teach the model to follow human instructions across dozens of languages. The fine-tuning procedure leverages the multilingual coverage of BLOOM and mT5 to produce BLOOMZ, a family of models that demonstrate zero-shot generalization to previously unseen tasks and languages. No further composition details, licensing terms, or example counts are provided beyond the identification of the xP3 dataset itself.",
  "4-2 (Fine-tuning Data)__evidence": [
    {
      "source": "[readme]",
      "quote": "We present BLOOMZ & mT0, a family of models capable of following human instructions in dozens of languages zero-shot. We finetune BLOOM & mT5 pretrained multilingual language models on our crosslingual task mixture (xP3) and find the resulting models capable of crosslingual generalization to unseen tasks & languages."
    },
    {
      "source": "[readme]",
      "quote": "datasets:\n- bigscience/xP3"
    }
  ],
  "4-3 (Reinforcement Learning Data)": "No sentences in the supplied quotes mention any reinforcement-learning (RL) data, sources, or generation procedures for BLOOMZ. Consequently, there is no information available regarding the presence or absence of an RL training stage for this model.",
  "4-3 (Reinforcement Learning Data)__evidence": [],
  "4-4 (Data Filtering)": "The quote set contains no BLOOMZ-specific sentences that describe data filtering or cleaning pipelines, tools, classifiers, threshold values, or the proportion of data removed. Therefore, no information can be reported on BLOOMZ’s data-filtering criteria or their impact on the final training corpus.",
  "4-4 (Data Filtering)__evidence": [],
  "__usage": {
    "fine_tuning": "used",
    "rl": "not_used"
  }
}