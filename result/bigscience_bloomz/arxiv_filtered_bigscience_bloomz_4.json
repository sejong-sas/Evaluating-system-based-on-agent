{
  "4-1 (Pre-training Data)": "The only explicit information that mentions BLOOMZ and relates to its pre-training data concerns the relative presence of Swahili in the corpus. Two separate sentences report the same observation: “We also find that mT0 consistently outperforms BLOOMZ on Swahili (SW), possibly due to it being a larger part of its pretraining corpus (see Figure 2 and §4.6).” From this we can state that the authors attribute BLOOMZ’s weaker Swahili performance to Swahili occupying a smaller share of BLOOMZ’s pre-training data compared with mT0. No other details—such as the exact size of the corpus, its total number of tokens, individual data sources, licensing terms, or the overall language mix—are provided in the available quotes. Consequently, the sole pre-training insight is a qualitative comparison indicating that Swahili is under-represented for BLOOMZ relative to mT0, which in turn is hypothesized to affect downstream evaluation scores.",
  "4-2 (Fine-tuning Data)": "Multiple quotes give a richer picture of BLOOMZ’s fine-tuning stage. The paper states: “We apply MTF to the pretrained multilingual BLOOM and mT5 model families to produce finetuned variants called BLOOMZ and mT0.” It then details three fine-tuning configurations, each tied to a specific dataset blend: (i) BLOOMZ-P3 models are fine-tuned on the English-only P3 collection; (ii) BLOOMZ models (without a suffix) are fine-tuned on xP3, which is explicitly described as a multilingual dataset in which every task has an English prompt; and (iii) BLOOMZ-MT models are fine-tuned on xP3mt, a multilingual suite that includes both English prompts and automatically machine-translated prompts. Concrete model checkpoints are enumerated: BLOOMZ-560M (560 M parameters, fine-tuned on xP3), BLOOMZ-7.1B (7.1 B parameters, fine-tuned on xP3), BLOOMZ-7.1B-P3 (7.1 B parameters, fine-tuned on P3), BLOOMZ-7.1B-MT (7.1 B parameters, fine-tuned on xP3mt), and BLOOMZ-P3 (a 176 B-parameter model fine-tuned on P3). The quotes emphasize that the presence or absence of multilingual prompts, and whether those prompts are human-written or machine-translated, are the defining axes that separate the P3, xP3, and xP3mt fine-tuning datasets. No explicit numerical breakdown of dataset sizes, license terms, or example counts is offered in the quoted material, but the dataset taxonomy and the mapping from dataset to model name are clearly spelled out.",
  "4-3 (Reinforcement Learning Data)": "",
  "4-4 (Data Filtering)": "",
  "4-1 (Pre-training Data)__evidence": [
    {
      "source": "[sections/Results §4.2 Language generalization]",
      "quote": "We also find that mT0 consistently outperforms BLOOMZ on Swahili (SW), possibly due to it being a larger part of its pretraining corpus (see Figure 2 and §4.6)."
    },
    {
      "source": "[sections/Task generalization breakdown]",
      "quote": "Performance by prompt varies substantially highlighting that prompt engineering may still be necessary after MTF. We also find that mT0 consistently outperforms BLOOMZ on Swahili (SW), possibly due to it being a larger part of its pretraining corpus (see Figure 2 and §4.6)."
    }
  ],
  "4-2 (Fine-tuning Data)__evidence": [
    {
      "source": "[pdf_text]",
      "quote": "We produce three core model variants available in different sizes: • BLOOMZ-P3 / mT0-P3: Models finetuned on the English-only P3. • BLOOMZ / mT0: Models finetuned on xP3, which consists of multilingual datasets with English prompts. • BLOOMZ-MT / mT0-MT: Models fine-tuned on xP3mt, which consists of multilingual datasets with English and machine-translated prompts."
    },
    {
      "source": "[sections/Artifacts/Table 3]",
      "quote": "BLOOMZ-7.1B 7.1B parameter model finetuned on xP3"
    },
    {
      "source": "[sections/Artifacts/Table 3]",
      "quote": "BLOOMZ-P3 176B parameter model finetuned on P3"
    },
    {
      "source": "[abstract]",
      "quote": "We apply MTF to the pretrained multilingual BLOOM and mT5 model families to produce finetuned variants called BLOOMZ and mT0."
    },
    {
      "source": "[sections/Models]",
      "quote": "We produce three core model variants available in different sizes:\n• BLOOMZ-P3 / mT0-P3: Models finetuned on the English-only P3.\n• BLOOMZ / mT0: Models finetuned on xP3, which consists of multilingual datasets with English prompts.\n• BLOOMZ-MT / mT0-MT: Models fine- tuned on xP3mt, which consists of multilingual datasets with English and machine- translated prompts."
    },
    {
      "source": "[sections/Artifacts]",
      "quote": "BLOOMZ-560M 560M parameter model finetuned on xP3"
    },
    {
      "source": "[sections/Artifacts]",
      "quote": "BLOOMZ-7.1B-P3 7.1B parameter model finetuned on P3"
    },
    {
      "source": "[sections/Artifacts]",
      "quote": "BLOOMZ-7.1B-MT 7.1B parameter model finetuned on xP3mt"
    }
  ],
  "4-3 (Reinforcement Learning Data)__evidence": [],
  "4-4 (Data Filtering)__evidence": []
}