{
  "4-1 (Pre-training Data)": "",
  "4-2 (Fine-tuning Data)": "Fine-tuning for BLOOMZ was carried out with the MTF (Multitask Fine-tuning) procedure applied to the pretrained multilingual BLOOM family. The fine-tuning relied on xP3, a composite collection of supervised datasets that spans 46 languages and combines both original English prompts and machine-translated prompts. As a consequence of the examples present in these fine-tuning datasets, BLOOMZ exhibits a tendency toward producing short answers, reflecting the generally short length of most training instances.",
  "4-3 (Reinforcement Learning Data)": "",
  "4-4 (Data Filtering)": "",
  "4-1 (Pre-training Data)__evidence": [],
  "4-2 (Fine-tuning Data)__evidence": [
    {
      "source": "[sections/https://arxiv.org/abs/2211.01786]",
      "quote": "We apply MTF to the pretrained multilingual BLOOM and mT5 model families to produce finetuned variants called BLOOMZ and mT0. In addition, we introduce xP3, a composite of supervised datasets in 46 languages with English and machine-translated prompts."
    },
    {
      "source": "[sections/https://raw.githubusercontent.com/bigscience-workshop/xmtf/master/plotstables/XMTF_ACL2023_POSTER.pdf]",
      "quote": "BLOOMZ is biased towards short answers as most training examples are short âš "
    }
  ],
  "4-3 (Reinforcement Learning Data)__evidence": [],
  "4-4 (Data Filtering)__evidence": []
}