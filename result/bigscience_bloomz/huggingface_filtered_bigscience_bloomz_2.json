{
  "1-5 (Architecture)": "The bigscience/bloomz model follows the numerical configuration spelled out in its public config: it stacks 70 transformer layers, each layer operates with 112 self-attention heads, and the model’s hidden/embedding width is 14 336. These figures capture the depth (n_layer = 70), the parallel attention capacity (num_attention_heads = 112), and the width of every vector flowing through the network (n_embed = 14 336), defining the core architectural scale for BLOOMZ.",
  "1-5 (Architecture)__evidence": [
    {
      "source": "[readme]",
      "quote": "- **Architecture:** Same as [bloom](https://huggingface.co/bigscience/bloom), also refer to the `config.json` file"
    },
    {
      "source": "[config]",
      "quote": "\"n_layer\": 70,"
    },
    {
      "source": "[config]",
      "quote": "\"num_attention_heads\": 112,"
    },
    {
      "source": "[config]",
      "quote": "\"n_embed\": 14336,"
    }
  ],
  "1-6 (Tokenizer)": "The BLOOMZ checkpoint (checkpoint = \"bigscience/bloomz\") ships with a tokenizer.json file whose vocabulary size is fixed at 250 880 tokens. The presence of tokenizer.json indicates the full tokenizer definition can be downloaded and loaded locally, giving users the exact same sub-word inventory—250 880 entries—that was used during pre-training and instruction tuning of bigscience/bloomz.",
  "1-6 (Tokenizer)__evidence": [
    {
      "source": "[config]",
      "quote": "\"vocab_size\": 250880"
    },
    {
      "source": "[readme]",
      "quote": "checkpoint = \"bigscience/bloomz\""
    },
    {
      "source": "[files]",
      "quote": "tokenizer.json"
    }
  ],
  "2-1 (Hardware)": "Model pre-training / fine-tuning for bigscience/bloomz was carried out on a multi-node cluster of AMD CPU hosts, each node equipped with 512 GB of system memory. GPU acceleration relied on 36 nodes, each containing 8 NVIDIA A100 80 GB cards, for an aggregate of 288 A100 80 GB GPUs. Within every node the eight GPUs are linked by NVLink 4, and across nodes the fabric is extended by four OmniPath links, giving the bandwidth needed for large-scale distributed training.",
  "2-1 (Hardware)__evidence": [
    {
      "source": "[readme]",
      "quote": "- **CPUs:** AMD CPUs with 512GB memory per node"
    },
    {
      "source": "[readme]",
      "quote": "- **GPUs:** 288 A100 80GB GPUs with 8 GPUs per node (36 nodes) using NVLink 4 inter-gpu connects, 4 OmniPath links"
    }
  ],
  "2-2 (Software)": "The training stack for bigscience/bloomz combines several open-source components: Megatron-DeepSpeed provides the overall orchestration layer; DeepSpeed itself supplies the optimizer implementation and distributed/parallelism strategies; core neural network operations run on PyTorch (pytorch-1.11 compiled with CUDA-11.5); and, where half-precision is employed, NVIDIA Apex is used to handle FP16 computation.",
  "2-2 (Software)__evidence": [
    {
      "source": "[readme]",
      "quote": "- **Orchestration:** [Megatron-DeepSpeed](https://github.com/bigscience-workshop/Megatron-DeepSpeed)"
    },
    {
      "source": "[readme]",
      "quote": "- **Optimizer & parallelism:** [DeepSpeed](https://github.com/microsoft/DeepSpeed)"
    },
    {
      "source": "[readme]",
      "quote": "- **Neural networks:** [PyTorch](https://github.com/pytorch/pytorch) (pytorch-1.11 w/ CUDA-11.5)"
    },
    {
      "source": "[readme]",
      "quote": "- **FP16 if applicable:** [apex](https://github.com/NVIDIA/apex)"
    }
  ]
}