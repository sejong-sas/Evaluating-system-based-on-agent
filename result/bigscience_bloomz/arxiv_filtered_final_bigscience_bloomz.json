{
  "1-1 (Weights)": "The quotes state that multiple checkpoints of the model are already hosted on Hugging Face.  One sentence specifies: “BLOOMZ-560M  560M parameter model finetuned on xP3  https://huggingface.co/bigscience/bloomz-560m”, confirming that the 560 million-parameter version is publicly posted at that URL.  Another sentence discloses the large 176 billion-parameter edition in exactly the same way: “BLOOMZ  176B parameter model finetuned on xP3  https://huggingface.co/bigscience/bloomz”.  The material also clarifies how to download optimizer checkpoints produced during training: “BLOOMZ models have an additional repository containing the final optimizer states for training with Megatron-Deepspeed that can be found by appending “-optimizer-states\" to the respective URL.”  Regarding breadth of releases, the authors write: “We produce three core model variants available in different sizes: • BLOOMZ-P3 / mT0-P3: Models finetuned on the English-only P3. • BLOOMZ / mT0: Models finetuned on xP3, which consists of multilingual datasets with English prompts. • BLOOMZ-MT / mT0-MT: Models fine-tuned on xP3mt, which consists of multilingual datasets with English and machine-translated prompts.”  Together, these statements reveal that anyone can obtain weight files for several BLOOMZ sizes and sub-variants directly from Hugging Face, and that associated Megatron-Deepspeed optimizer state directories are also publicly exposed via a systematic “-optimizer-states” suffix.",
  "1-2 (Code)": "No sentence in the supplied material mentions BLOOMZ together with any form of publicly released training pipeline, data-processing script, configuration file, or fine-tuning code.  Therefore, based solely on the given quotes, there is **no evidence** that training code—whether for pre-training, fine-tuning, or reinforcement learning—has been open-sourced for BLOOMZ.",
  "1-3 (License)": "The only licensing information that explicitly names the model says: “BLOOMZ models have an additional repository containing the final optimizer states for training with Megatron-Deepspeed that can be found by appending “-optimizer-states\" to the respective URL. BLOOM(Z) models are released under the RAIL license, while mT5 / mT0 models are licensed under Apache 2.0”.  From this, we learn that every BLOOMZ release—weights and optimizer states alike—is distributed under the Responsible AI License (RAIL), and **not** under standard permissive licenses such as Apache 2.0.  No further text spells out the RAIL’s specific clauses, but the passage establishes that BLOOMZ inherits the RAIL’s conditions for use, modification, redistribution, and any potential commercial activity.",
  "1-4 (Paper)": "The single paper-related quote reads: “We apply MTF to the pretrained multilingual BLOOM and mT5 model families to produce finetuned variants called BLOOMZ and mT0.”  This indicates that an academic or technical write-up exists where the authors describe Multitask Finetuning (MTF) applied to BLOOM, giving rise to BLOOMZ.  No direct DOI, arXiv link, or conference citation is included in the supplied sentences, but the statement confirms the presence of an official publication that documents the methodology and positioning of BLOOMZ as the finetuned successor of BLOOM within the multilingual model family.",
  "1-1 (Weights)__evidence": [
    {
      "source": "[sections/C Artifacts]",
      "quote": "BLOOMZ-560M  560M parameter model finetuned on xP3  https://huggingface.co/bigscience/bloomz-560m"
    },
    {
      "source": "[sections/C Artifacts]",
      "quote": "BLOOMZ  176B parameter model finetuned on xP3  https://huggingface.co/bigscience/bloomz"
    },
    {
      "source": "[sections/C Artifacts]",
      "quote": "BLOOMZ models have an additional repository containing the final optimizer states for training with Megatron-Deepspeed that can be found by appending “-optimizer-states\" to the respective URL."
    },
    {
      "source": "[sections/Models]",
      "quote": "We produce three core model variants available in different sizes: • BLOOMZ-P3 / mT0-P3: Models finetuned on the English-only P3. • BLOOMZ / mT0: Models finetuned on xP3, which consists of multilingual datasets with English prompts. • BLOOMZ-MT / mT0-MT: Models fine-tuned on xP3mt, which consists of multilingual datasets with English and machine-translated prompts."
    },
    {
      "source": "[sections/Artifacts]",
      "quote": "BLOOMZ models have an additional repository containing the final optimizer states for training with Megatron-Deepspeed that can be found by appending “-optimizer-states\" to the respective URL."
    }
  ],
  "1-2 (Code)__evidence": [],
  "1-3 (License)__evidence": [
    {
      "source": "[sections/Artifacts]",
      "quote": "BLOOMZ models have an additional repository containing the final optimizer states for training with Megatron-Deepspeed that can be found by appending “-optimizer-states\" to the respective URL. BLOOM(Z) models are released under the RAIL license, while mT5 / mT0 models are licensed under Apache 2.0"
    }
  ],
  "1-4 (Paper)__evidence": [
    {
      "source": "[abstract]",
      "quote": "We apply MTF to the pretrained multilingual BLOOM and mT5 model families to produce finetuned variants called BLOOMZ and mT0."
    }
  ],
  "1-5 (Architecture)": "The BLOOMZ family is presented entirely in terms of model sizes and their fine-tuning status.  At the top end, the release references “BLOOMZ 176B”, explicitly described twice as a 176-billion-parameter model that has been finetuned on the xP3 supervision mixture.  A cross-model comparison sentence provides an external size reference, stating that mT0 has 13 billion parameters and is “an order of magnitude fewer parameters” than BLOOMZ 176B, anchoring the BLOOMZ flagship at 176 B parameters.  Beneath this flagship, several smaller checkpoints are enumerated: “BLOOMZ-7.1B” (7.1 B parameters) which is also reported in two different contexts—one ordinary listing, and one fuller description (“7.1 B parameter model finetuned on xP3mt”).  A table caption further emphasizes experimentation on that 7.1 B variant, noting that “Table 5” benchmarks several 7.1 B BLOOMZ models with unspecified “various modifications” on the MultiEURLEX English–French translation task.  Even smaller distilled versions are named in sequence—“BLOOMZ-3B”, “BLOOMZ-1.7B”, “BLOOMZ-1.1B”, and “BLOOMZ-560M”.  The 560 M checkpoint receives an explicit description paralleling the flagship line: “BLOOMZ-560M 560M parameter model finetuned on xP3”.  Collectively, the quotes therefore establish a tiered architecture line-up consisting of at least six discrete parameter scales (560 M, 1.1 B, 1.7 B, 3 B, 7.1 B, and 176 B), and indicate that each of these checkpoints is the result of a supervised fine-tuning pass (either on xP3 or xP3mt).  No other structural details—such as layer counts, hidden sizes, or attention configuration—are given in the extracted sentences; only the total parameter counts, the finetuning dataset labels, and the acknowledgment that modifications were explored on the 7.1 B variant appear in the source material.",
  "1-6 (Tokenizer)": "None of the extracted BLOOMZ sentences contain any reference to the tokenizer, its type, configuration, vocabulary, or download location, so no tokenizer details can be summarized from the available quotes.",
  "2-1 (Hardware)": "The quote set contains no explicit mention of training hardware—there are no references to GPU/TPU model names, counts, or total compute—so hardware information cannot be reported.",
  "2-2 (Software)": "One sentence provides software-level insight: “BLOOMZ models have an additional repository containing the final optimizer states for training with Megatron-Deepspeed that can be found by appending “-optimizer-states” to the respective URL.”  From this we can conclude that the training stack used the Megatron-Deepspeed framework, and that the team made the full optimizer checkpoints available in a dedicated repository path ending in “-optimizer-states”.  No other software components, framework versions, or training flags are mentioned in the provided material.",
  "1-5 (Architecture)__evidence": [
    {
      "source": "[pdf_text]",
      "quote": "Despite an order of magnitude fewer parameters, mT0 (13 billion parameters) is ahead of BLOOMZ (176 billion parameters)."
    },
    {
      "source": "[sections/C Artifacts]",
      "quote": "BLOOMZ 176B parameter model finetuned on xP3"
    },
    {
      "source": "[sections/C Artifacts]",
      "quote": "BLOOMZ-7.1B 7.1B parameter model finetuned on xP3mt"
    },
    {
      "source": "[sections/G Increasing generation length]",
      "quote": "Table 5: 7.1 billion parameter BLOOMZ models with various modifications benchmarked on MultiEURLEX English-French translation (Chalkidis et al., 2021)."
    },
    {
      "source": "[pdf_text]",
      "quote": "BLOOMZ-560M"
    },
    {
      "source": "[pdf_text]",
      "quote": "BLOOMZ-1.1B"
    },
    {
      "source": "[pdf_text]",
      "quote": "BLOOMZ-1.7B"
    },
    {
      "source": "[pdf_text]",
      "quote": "BLOOMZ-3B"
    },
    {
      "source": "[pdf_text]",
      "quote": "BLOOMZ-7.1B"
    },
    {
      "source": "[sections/4.1 Task generalization]",
      "quote": "Despite an order of magnitude fewer parameters, mT0 (13 billion parameters) is ahead of BLOOMZ (176 billion parameters)."
    },
    {
      "source": "[pdf_text/Artifacts]",
      "quote": "BLOOMZ-560M 560M parameter model finetuned on xP3"
    },
    {
      "source": "[pdf_text/Artifacts]",
      "quote": "BLOOMZ 176B parameter model finetuned on xP3"
    }
  ],
  "1-6 (Tokenizer)__evidence": [],
  "2-1 (Hardware)__evidence": [],
  "2-2 (Software)__evidence": [
    {
      "source": "[pdf_text/Artifacts]",
      "quote": "BLOOMZ models have an additional repository containing the final optimizer states for training with Megatron-Deepspeed that can be found by appending “-optimizer-states\" to the respective URL."
    }
  ],
  "2-3 (API)": "",
  "3-1 (Pre-training)": "The only explicit pre-training detail mentioned for BLOOMZ is comparative: researchers note that mT0 \"consistently outperforms BLOOMZ on Swahili (SW), possibly due to it being a larger part of its pretraining corpus.\"  From this we can infer that BLOOMZ’s own pre-training corpus contains proportionally less Swahili than mT0’s, and that corpus-language balance can directly affect downstream performance on that language.  No additional information about architecture, optimization schedule, or other hyper-parameters is provided in the available quotes.",
  "3-2 (Fine-tuning)": "Fine-tuning is carried out with the MTF procedure applied to the pretrained multilingual BLOOM and mT5 families, yielding the BLOOMZ and mT0 model lines.  Three principal BLOOMZ variants are produced, each offered in multiple sizes:\n• BLOOMZ-P3 – fine-tuned solely on the English-only P3 collection.\n• BLOOMZ – fine-tuned on xP3, a multilingual dataset that keeps English prompts.\n• BLOOMZ-MT – fine-tuned on xP3mt, which augments xP3 with machine-translated prompts in many languages.\n\nEmpirical findings highlighted in the quotes show several important behaviors:\n• \"Finetuned BLOOMZ and BLOOMZ-P3 models significantly improve over BLOOM and XGLM on held-out tasks,\" demonstrating that the MTF fine-tuning step materially boosts performance compared with the original BLOOM checkpoints and with another strong multilingual baseline (XGLM).\n• Although the base BLOOMZ models were not explicitly trained on non-English prompts, they \"are still able to handle them,\" but performance \"drops significantly when translating the prompts to the respective unseen languages.\"  In unseen-language settings, BLOOMZ-MT can actually underperform the base BLOOMZ because it \"has not been finetuned on prompts in these languages.\"\n• Quantitatively, \"Table 1 shows that BLOOMZ performs much better on English than on non-English prompts,\" whereas BLOOMZ-MT \"significantly improves on multilingual prompts,\" confirming the benefit of machine-translated prompt fine-tuning for cross-lingual generalization when the prompt language is represented during fine-tuning.\n\nReproducibility and artifacts: The project supplies an \"additional repository containing the final optimizer states for training with Megatron-Deepspeed\"—users can access these by appending \"-optimizer-states\" to the normal model URL.  This makes it easier for practitioners to resume or further adapt the fine-tuned checkpoints.",
  "3-3 (Reinforcement Learning)": "",
  "2-3 (API)__evidence": [],
  "3-1 (Pre-training)__evidence": [
    {
      "source": "[sections/Results]",
      "quote": "We also find that mT0 consistently outperforms BLOOMZ on Swahili (SW), possibly due to it being a larger part of its pretraining corpus (see Figure 2 and §4.6)."
    },
    {
      "source": "[sections/B Task generalization breakdown]",
      "quote": "We also find that mT0 consistently outperforms BLOOMZ on Swahili (SW), possibly due to it being a larger part of its pretraining corpus (see Figure 2 and §4.6)."
    }
  ],
  "3-2 (Fine-tuning)__evidence": [
    {
      "source": "[abstract]",
      "quote": "We apply MTF to the pretrained multilingual BLOOM and mT5 model families to produce finetuned variants called BLOOMZ and mT0."
    },
    {
      "source": "[pdf_text]",
      "quote": "We produce three core model variants available in different sizes: • BLOOMZ-P3 / mT0-P3: Models finetuned on the English-only P3. • BLOOMZ / mT0: Models finetuned on xP3, which consists of multilingual datasets with English prompts. • BLOOMZ-MT / mT0-MT: Models fine- tuned on xP3mt, which consists of multilingual datasets with English and machine- translated prompts."
    },
    {
      "source": "[pdf_text]",
      "quote": "BLOOMZ and mT0-13B have not been trained on non-English prompts, but are still able to handle them."
    },
    {
      "source": "[pdf_text]",
      "quote": "For BLOOMZ, performance drops significantly when translating the prompts to the respective unseen languages. Unlike on translated prompts for seen languages (§4.3), BLOOMZ-MT performs worse than BLOOMZ for machine-translated prompts in unseen languages. This is likely because BLOOMZ-MT has not been finetuned on prompts in these languages."
    },
    {
      "source": "[sections/Artifacts]",
      "quote": "BLOOMZ models have an additional repository containing the final optimizer states for training with Megatron-Deepspeed that can be found by appending “-optimizer-states\" to the respective URL."
    },
    {
      "source": "[sections/Models]",
      "quote": "We produce three core model variants available in different sizes: • BLOOMZ-P3 / mT0-P3: Models finetuned on the English-only P3. • BLOOMZ / mT0: Models finetuned on xP3, which consists of multilingual datasets with English prompts. • BLOOMZ-MT / mT0-MT: Models fine- tuned on xP3mt, which consists of multilingual datasets with English and machine- translated prompts."
    },
    {
      "source": "[sections/Results]",
      "quote": "Finetuned BLOOMZ and BLOOMZ-P3 models significantly improve over BLOOM and XGLM on held-out tasks."
    },
    {
      "source": "[sections/Multilingual prompting]",
      "quote": "Table 1 shows that BLOOMZ performs much better on English than on non- English prompts. BLOOMZ-MT, which is fine- tuned on xP3mt, significantly improves on multilingual prompts."
    }
  ],
  "3-3 (Reinforcement Learning)__evidence": [],
  "4-1 (Pre-training Data)": "The only explicit information that mentions BLOOMZ and relates to its pre-training data concerns the relative presence of Swahili in the corpus. Two separate sentences report the same observation: “We also find that mT0 consistently outperforms BLOOMZ on Swahili (SW), possibly due to it being a larger part of its pretraining corpus (see Figure 2 and §4.6).” From this we can state that the authors attribute BLOOMZ’s weaker Swahili performance to Swahili occupying a smaller share of BLOOMZ’s pre-training data compared with mT0. No other details—such as the exact size of the corpus, its total number of tokens, individual data sources, licensing terms, or the overall language mix—are provided in the available quotes. Consequently, the sole pre-training insight is a qualitative comparison indicating that Swahili is under-represented for BLOOMZ relative to mT0, which in turn is hypothesized to affect downstream evaluation scores.",
  "4-2 (Fine-tuning Data)": "Multiple quotes give a richer picture of BLOOMZ’s fine-tuning stage. The paper states: “We apply MTF to the pretrained multilingual BLOOM and mT5 model families to produce finetuned variants called BLOOMZ and mT0.” It then details three fine-tuning configurations, each tied to a specific dataset blend: (i) BLOOMZ-P3 models are fine-tuned on the English-only P3 collection; (ii) BLOOMZ models (without a suffix) are fine-tuned on xP3, which is explicitly described as a multilingual dataset in which every task has an English prompt; and (iii) BLOOMZ-MT models are fine-tuned on xP3mt, a multilingual suite that includes both English prompts and automatically machine-translated prompts. Concrete model checkpoints are enumerated: BLOOMZ-560M (560 M parameters, fine-tuned on xP3), BLOOMZ-7.1B (7.1 B parameters, fine-tuned on xP3), BLOOMZ-7.1B-P3 (7.1 B parameters, fine-tuned on P3), BLOOMZ-7.1B-MT (7.1 B parameters, fine-tuned on xP3mt), and BLOOMZ-P3 (a 176 B-parameter model fine-tuned on P3). The quotes emphasize that the presence or absence of multilingual prompts, and whether those prompts are human-written or machine-translated, are the defining axes that separate the P3, xP3, and xP3mt fine-tuning datasets. No explicit numerical breakdown of dataset sizes, license terms, or example counts is offered in the quoted material, but the dataset taxonomy and the mapping from dataset to model name are clearly spelled out.",
  "4-3 (Reinforcement Learning Data)": "",
  "4-4 (Data Filtering)": "",
  "4-1 (Pre-training Data)__evidence": [
    {
      "source": "[sections/Results §4.2 Language generalization]",
      "quote": "We also find that mT0 consistently outperforms BLOOMZ on Swahili (SW), possibly due to it being a larger part of its pretraining corpus (see Figure 2 and §4.6)."
    },
    {
      "source": "[sections/Task generalization breakdown]",
      "quote": "Performance by prompt varies substantially highlighting that prompt engineering may still be necessary after MTF. We also find that mT0 consistently outperforms BLOOMZ on Swahili (SW), possibly due to it being a larger part of its pretraining corpus (see Figure 2 and §4.6)."
    }
  ],
  "4-2 (Fine-tuning Data)__evidence": [
    {
      "source": "[pdf_text]",
      "quote": "We produce three core model variants available in different sizes: • BLOOMZ-P3 / mT0-P3: Models finetuned on the English-only P3. • BLOOMZ / mT0: Models finetuned on xP3, which consists of multilingual datasets with English prompts. • BLOOMZ-MT / mT0-MT: Models fine-tuned on xP3mt, which consists of multilingual datasets with English and machine-translated prompts."
    },
    {
      "source": "[sections/Artifacts/Table 3]",
      "quote": "BLOOMZ-7.1B 7.1B parameter model finetuned on xP3"
    },
    {
      "source": "[sections/Artifacts/Table 3]",
      "quote": "BLOOMZ-P3 176B parameter model finetuned on P3"
    },
    {
      "source": "[abstract]",
      "quote": "We apply MTF to the pretrained multilingual BLOOM and mT5 model families to produce finetuned variants called BLOOMZ and mT0."
    },
    {
      "source": "[sections/Models]",
      "quote": "We produce three core model variants available in different sizes:\n• BLOOMZ-P3 / mT0-P3: Models finetuned on the English-only P3.\n• BLOOMZ / mT0: Models finetuned on xP3, which consists of multilingual datasets with English prompts.\n• BLOOMZ-MT / mT0-MT: Models fine- tuned on xP3mt, which consists of multilingual datasets with English and machine- translated prompts."
    },
    {
      "source": "[sections/Artifacts]",
      "quote": "BLOOMZ-560M 560M parameter model finetuned on xP3"
    },
    {
      "source": "[sections/Artifacts]",
      "quote": "BLOOMZ-7.1B-P3 7.1B parameter model finetuned on P3"
    },
    {
      "source": "[sections/Artifacts]",
      "quote": "BLOOMZ-7.1B-MT 7.1B parameter model finetuned on xP3mt"
    }
  ],
  "4-3 (Reinforcement Learning Data)__evidence": [],
  "4-4 (Data Filtering)__evidence": [],
  "__usage": {
    "fine_tuning": "used",
    "rl": "not_used"
  }
}