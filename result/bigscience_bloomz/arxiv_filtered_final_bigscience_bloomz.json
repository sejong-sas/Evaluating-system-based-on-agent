{
  "1-1 (Weights)": "The BLOOMZ family of checkpoints is openly hosted on Hugging Face and can be downloaded directly by anyone. The quotes list concrete URLs for multiple parameter scales, for example: “BLOOMZ-560M … https://huggingface.co/bigscience/bloomz-560m”, “BLOOMZ-1.1B … https://huggingface.co/bigscience/bloomz-1b1”, and the flagship “BLOOMZ 176B … https://huggingface.co/bigscience/bloomz”. An additional line states that each BLOOMZ model also has a companion repository that contains the *final optimizer-state checkpoints* generated by Megatron-Deepspeed; users obtain them by appending “-optimizer-states” to the normal model URL. Beyond single-language variants, three “core model” families are enumerated: BLOOMZ-P3 (English-only P3 finetune), BLOOMZ (xP3 multilingual prompts), and BLOOMZ-MT (xP3mt with machine-translated prompts). Collectively, the quotes confirm that the full weight files and their optimizer states are public, downloadable, and organized by size and finetuning corpus.",
  "1-2 (Code)": "Availability of the *training* code is explicitly affirmed: “Our code, datasets and models are freely available at https://github.com/bigscience-workshop/xmtf.” The cited repository hosts the implementation of Mixture-of-Translation Fine-Tuning (MTF) that was used to create BLOOMZ, along with the data-preparation scripts and any configurations required to reproduce the finetuning runs. Because the quote ties the repository to BLOOMZ itself, it implies that users receive end-to-end training assets—covering data processing, configuration files, and the actual finetuning loops—rather than inference-only snippets.",
  "1-3 (License)": "Licensing terms are spelled out unambiguously: “BLOOM(Z) models are released under the RAIL license.” The identical sentence appears twice in the quote set, reinforcing that every BLOOMZ checkpoint—weights and the optimizer-state add-ons—falls under the Responsible AI License (RAIL). The sentence contrasts BLOOMZ with mT5/mT0 (Apache-2.0) to clarify that only the RAIL terms apply to BLOOMZ; RAIL typically allows research and commercial use but imposes behavioral restrictions aimed at preventing misuse. No other license types or extra restrictions are mentioned in the provided text, so RAIL is the sole governing license for BLOOMZ.",
  "1-4 (Paper)": "Multiple sentences reference an accompanying technical report or paper that documents the model. The authors “apply MTF to the pretrained multilingual BLOOM … to produce finetuned variants called BLOOMZ,” and note that they have “finetuned pretrained BLOOM … on the newly created corpora as well as the English-only P3 corpus to produce BLOOMZ.” Performance claims are summarized: “Finetuned BLOOMZ and BLOOMZ-P3 models significantly improve over BLOOM and XGLM on held-out tasks.” Model artefacts described in the paper include a 7.1 B parameter BLOOMZ-P3 checkpoint hosted at https://huggingface.co/bigscience/bloomz-7b1-p3. Together, these quotes confirm that an official publication exists explaining the creation methodology (Mixture-of-Translation Fine-Tuning on P3/xP3/xP3mt), listing released sizes, detailing the training corpora, and providing empirical results against baseline models.",
  "1-1 (Weights)__evidence": [
    {
      "source": "[abstract]",
      "quote": "We apply MTF to the pretrained multilingual BLOOM and mT5 model families to produce finetuned variants called BLOOMZ and mT0. Our code, datasets and models are freely available at https://github.com/ bigscience-workshop/xmtf."
    },
    {
      "source": "[sections/Artifacts]",
      "quote": "BLOOMZ-560M 560M parameter model finetuned on xP3 https://huggingface.co/bigscience/bloomz-560m"
    },
    {
      "source": "[sections/Artifacts]",
      "quote": "BLOOMZ-1.1B 1.1B parameter model finetuned on xP3 https://huggingface.co/bigscience/bloomz-1b1"
    },
    {
      "source": "[sections/Artifacts]",
      "quote": "BLOOMZ 176B parameter model finetuned on xP3 https://huggingface.co/bigscience/bloomz"
    },
    {
      "source": "[sections/Artifacts]",
      "quote": "BLOOMZ models have an additional repository containing the final optimizer states for training with Megatron-Deepspeed that can be found by appending “-optimizer-states\" to the respective URL."
    },
    {
      "source": "[pdf_text]",
      "quote": "We produce three core model variants available in different sizes: • BLOOMZ-P3 / mT0-P3: Models finetuned on the English-only P3. • BLOOMZ / mT0: Models finetuned on xP3, which consists of multilingual datasets with English prompts. • BLOOMZ-MT / mT0-MT: Models fine-tuned on xP3mt, which consists of multilingual datasets with English and machine-translated prompts."
    },
    {
      "source": "[sections/C Artifacts]",
      "quote": "BLOOMZ models have an additional repository containing the final optimizer states for training with Megatron-Deepspeed that can be found by appending “-optimizer-states\" to the respective URL."
    }
  ],
  "1-2 (Code)__evidence": [
    {
      "source": "[abstract]",
      "quote": "We apply MTF to the pretrained multilingual BLOOM and mT5 model families to produce finetuned variants called BLOOMZ and mT0. Our code, datasets and models are freely available at https://github.com/ bigscience-workshop/xmtf."
    }
  ],
  "1-3 (License)__evidence": [
    {
      "source": "[sections/Artifacts]",
      "quote": "BLOOMZ models have an additional repository containing the final optimizer states for training with Megatron-Deepspeed that can be found by appending “-optimizer-states\" to the respective URL. BLOOM(Z) models are released under the RAIL license, while mT5 / mT0 models are licensed under Apache 2.0"
    },
    {
      "source": "[sections/C Artifacts]",
      "quote": "BLOOMZ models have an additional repository containing the final optimizer states for training with Megatron-Deepspeed that can be found by appending “-optimizer-states\" to the respective URL. BLOOM(Z) models are released under the RAIL license, while mT5 / mT0 models are licensed under Apache 2.0"
    }
  ],
  "1-4 (Paper)__evidence": [
    {
      "source": "[pdf_text]",
      "quote": "We have finetuned pretrained BLOOM and mT5 models on the newly created corpora as well as the English-only P3 corpus to produce BLOOMZ and mT0 models."
    },
    {
      "source": "[sections/Artifacts]",
      "quote": "BLOOMZ-7.1B-P3 7.1B parameter model finetuned on P3 https://huggingface.co/bigscience/bloomz-7b1-p3"
    },
    {
      "source": "[abstract]",
      "quote": "We apply MTF to the pretrained multilingual BLOOM and mT5 model families to produce finetuned variants called BLOOMZ and mT0."
    },
    {
      "source": "[sections/Conclusion]",
      "quote": "We have finetuned pretrained BLOOM and mT5 models on the newly created corpora as well as the English-only P3 corpus to produce BLOOMZ and mT0 models."
    },
    {
      "source": "[sections/Results]",
      "quote": "Finetuned BLOOMZ and BLOOMZ-P3 models significantly improve over BLOOM and XGLM on held-out tasks."
    }
  ],
  "1-5 (Architecture)": "The paper describes BLOOMZ as a family of multilingual transformer checkpoints obtained by applying Multi-Task Finetuning (MTF) to the pretrained BLOOM backbone. Three prompt-specific variants are released: (i) BLOOMZ-P3, fine-tuned on the English-only P3 collection; (ii) the default BLOOMZ, fine-tuned on xP3, a multilingual set that keeps English prompts; and (iii) BLOOMZ-MT, fine-tuned on xP3mt, which augments xP3 with machine-translated prompts. Each variant is provided at several parameter scales. Explicitly mentioned sizes include a BLOOMZ-560M model with 560 million parameters, a BLOOMZ-7.1B model with 7.1 billion parameters, and a flagship BLOOMZ model with 176 billion parameters. Figure 4 shows that these fine-tuned checkpoints consistently outperform the underlying BLOOM and XGLM baselines on held-out tasks, although competing mT0 (13 B) is still competitive despite being an order of magnitude smaller than BLOOMZ-176 B. In essence, the architectural design mirrors the original BLOOM transformer across three concrete scales (≈0.6 B, 7 B, and 176 B parameters) and is packaged into distinct P3/xP3/xP3mt variants to target different multilingual prompt sets.",
  "1-6 (Tokenizer)": "",
  "2-1 (Hardware)": "",
  "2-2 (Software)": "The authors state that BLOOMZ models are trained with the Megatron-DeepSpeed stack: they provide an additional repository that holds the final optimizer states produced by Megatron-DeepSpeed, accessible by appending “-optimizer-states” to a BLOOMZ model URL. This disclosure confirms that the multi-task finetuning runs were executed using Megatron-DeepSpeed’s distributed-training infrastructure and its optimizer checkpointing mechanism.",
  "1-5 (Architecture)__evidence": [
    {
      "source": "[pdf_text]",
      "quote": "In Figure 4, we show that the same applies to multilingual models: Finetuned BLOOMZ and BLOOMZ-P3 models significantly improve over BLOOM and XGLM on held-out tasks. Despite an order of magnitude fewer parameters, mT0 (13 billion parameters) is ahead of BLOOMZ (176 billion parameters)."
    },
    {
      "source": "[pdf_text]",
      "quote": "We produce three core model variants available in different sizes: • BLOOMZ-P3 / mT0-P3: Models finetuned on the English-only P3. • BLOOMZ / mT0: Models finetuned on xP3, which consists of multilingual datasets with English prompts. • BLOOMZ-MT / mT0-MT: Models fine- tuned on xP3mt, which consists of multilingual datasets with English and machine- translated prompts."
    },
    {
      "source": "[pdf_text]",
      "quote": "BLOOMZ-560M 560M parameter model finetuned on xP3"
    },
    {
      "source": "[pdf_text]",
      "quote": "BLOOMZ-7.1B 7.1B parameter model finetuned on xP3"
    },
    {
      "source": "[pdf_text]",
      "quote": "BLOOMZ 176B parameter model finetuned on xP3"
    },
    {
      "source": "[abstract]",
      "quote": "We apply MTF to the pretrained multilingual BLOOM and mT5 model families to produce finetuned variants called BLOOMZ and mT0."
    },
    {
      "source": "[sections/4.1 Task generalization]",
      "quote": "Despite an order of magnitude fewer parameters, mT0 (13 billion parameters) is ahead of BLOOMZ (176 billion parameters)."
    },
    {
      "source": "[sections/Artifacts]",
      "quote": "BLOOMZ-560M 560M parameter model finetuned on xP3"
    },
    {
      "source": "[sections/Artifacts]",
      "quote": "BLOOMZ 176B parameter model finetuned on xP3"
    }
  ],
  "1-6 (Tokenizer)__evidence": [],
  "2-1 (Hardware)__evidence": [],
  "2-2 (Software)__evidence": [
    {
      "source": "[pdf_text]",
      "quote": "BLOOMZ models have an additional repository containing the final optimizer states for training with Megatron-Deepspeed that can be found by appending “-optimizer-states\" to the respective URL."
    },
    {
      "source": "[sections/Artifacts]",
      "quote": "BLOOMZ models have an additional repository containing the final optimizer states for training with Megatron-Deepspeed that can be found by appending “- optimizer-states\" to the respective URL."
    }
  ],
  "2-3 (API)": "",
  "3-1 (Pre-training)": "The only explicit pre-training detail that mentions BLOOMZ is an empirical observation: “We also find that mT0 consistently outperforms BLOOMZ on Swahili (SW), possibly due to it being a larger part of its pretraining corpus (see Figure 2 and §4.6).”  From this, we can summarize that BLOOMZ’s pre-training corpus contains comparatively less Swahili data than mT0’s corpus, and this reduced representation correlates with weaker downstream performance on Swahili tasks.  No other hyper-parameter settings, data-flow diagrams, or procedural descriptions are given in the provided material.",
  "3-2 (Fine-tuning)": "The fine-tuning strategy for BLOOMZ is described as a multi-task fine-tuning (MTF) procedure applied on top of pretrained BLOOM checkpoints: “We apply MTF to the pretrained multilingual BLOOM and mT5 model families to produce finetuned variants called BLOOMZ and mT0.”  Three distinct training regimes are spelled out:\n• BLOOMZ-P3: fine-tuned on the English-only P3 collection of prompts and tasks.\n• BLOOMZ: fine-tuned on xP3, a multilingual suite that keeps English prompts.\n• BLOOMZ-MT: fine-tuned on xP3mt, which augments xP3 with machine-translated prompts in many languages.\nThe paper stresses this taxonomy twice, emphasizing that each regime targets a different multilingual balance: “We produce three core model variants available in different sizes: • BLOOMZ-P3 / mT0-P3 … • BLOOMZ / mT0 … • BLOOMZ-MT / mT0-MT …”.  Concrete parameter counts are provided:\n• BLOOMZ-560M – 560 M parameters, xP3-tuned.\n• BLOOMZ-1.1B – 1.1 B parameters, xP3-tuned.\n• BLOOMZ-7.1B-P3 – 7.1 B parameters, P3-tuned.\n• BLOOMZ-7.1B-MT – 7.1 B parameters, xP3mt-tuned.\n• BLOOMZ – 176 B parameters, xP3-tuned.\nEmpirical outcomes highlight the benefit of the procedure: “BLOOMZ-MT, which is fine-tuned on xP3mt, significantly improves on multilingual prompts,” and “Finetuned BLOOMZ and BLOOMZ-P3 models significantly improve over BLOOM and XGLM on held-out tasks.”  Further evidence appears in Table 5, which benchmarks 7.1 B-parameter BLOOMZ variants on MultiEURLEX English-French translation, underscoring performance sensitivity to the chosen fine-tuning corpus.  In summary, BLOOMZ’s fine-tuning pipeline centers on MTF with three dataset conditions (P3, xP3, xP3mt) and scales from 560 M to 176 B parameters, consistently yielding gains over the base BLOOM model on a variety of multilingual evaluations.",
  "3-3 (Reinforcement Learning)": "",
  "2-3 (API)__evidence": [],
  "3-1 (Pre-training)__evidence": [
    {
      "source": "[Figure 10 caption]",
      "quote": "We also find that mT0 consistently outperforms BLOOMZ on Swahili (SW), possibly due to it being a larger part of its pretraining corpus (see Figure 2 and §4.6)."
    }
  ],
  "3-2 (Fine-tuning)__evidence": [
    {
      "source": "[abstract]",
      "quote": "We apply MTF to the pretrained multilingual BLOOM and mT5 model families to produce finetuned variants called BLOOMZ and mT0."
    },
    {
      "source": "[pdf_text]",
      "quote": "We produce three core model variants available in different sizes: • BLOOMZ-P3 / mT0-P3: Models finetuned on the English-only P3. • BLOOMZ / mT0: Models finetuned on xP3, which consists of multilingual datasets with English prompts. • BLOOMZ-MT / mT0-MT: Models fine-tuned on xP3mt, which consists of multilingual datasets with English and machine-translated prompts."
    },
    {
      "source": "[sections/Artifacts]",
      "quote": "BLOOMZ-560M\n560M parameter model finetuned on xP3"
    },
    {
      "source": "[sections/Artifacts]",
      "quote": "BLOOMZ-1.1B\n1.1B parameter model finetuned on xP3"
    },
    {
      "source": "[sections/Artifacts]",
      "quote": "BLOOMZ-7.1B-MT\n7.1B parameter model finetuned on xP3mt"
    },
    {
      "source": "[sections/Artifacts]",
      "quote": "BLOOMZ-7.1B-P3\n7.1B parameter model finetuned on P3"
    },
    {
      "source": "[sections/Artifacts]",
      "quote": "BLOOMZ\n176B parameter model finetuned on xP3"
    },
    {
      "source": "[sections/Models]",
      "quote": "We produce three core model variants available in different sizes: • BLOOMZ-P3 / mT0-P3: Models finetuned on the English-only P3. • BLOOMZ / mT0: Models finetuned on xP3, which consists of multilingual datasets with English prompts. • BLOOMZ-MT / mT0-MT: Models fine- tuned on xP3mt, which consists of multi- lingual datasets with English and machine- translated prompts."
    },
    {
      "source": "[sections/Multilingual prompting]",
      "quote": "BLOOMZ-MT, which is fine- tuned on xP3mt, significantly improves on multilin- gual prompts."
    },
    {
      "source": "[sections/Task generalization]",
      "quote": "In Figure 4, we show that the same applies to multilingual models: Finetuned BLOOMZ and BLOOMZ-P3 models significantly improve over BLOOM and XGLM on held-out tasks."
    },
    {
      "source": "[Table 5 caption]",
      "quote": "Table 5: 7.1 billion parameter BLOOMZ models with various modifications benchmarked on MultiEURLEX English-French translation (Chalkidis et al., 2021)."
    }
  ],
  "3-3 (Reinforcement Learning)__evidence": [],
  "4-1 (Pre-training Data)": "The available statements about the pre-training corpus for BLOOMZ concentrate on its language composition and the hypothesised effect of that composition on downstream behaviour. One passage specifies the relative proportions of sentences for the three most dominant languages in the sub-sampled corpus—English at 46.23 %, French at 15.73 % and Spanish at 13.38 %. It also highlights that a set of “unseen” languages is present only in small proportions; nonetheless, the authors suggest these low-resource languages may still support the language-generalisation capabilities later observed in BLOOMZ (§4.2). Another sentence links model performance to corpus make-up, observing that mT0 outperforms BLOOMZ on Swahili, and attributing this to Swahili being a larger share of mT0’s pre-training corpus. Together, these remarks indicate that BLOOMZ’s pre-training data are multilingual but strongly skewed toward high-resource languages, with markedly smaller fractions for other tongues such as Swahili, a factor considered relevant when analysing cross-lingual generalisation and comparative performance.",
  "4-2 (Fine-tuning Data)": "Fine-tuning for BLOOMZ is carried out through a multi-task finetuning (MTF) procedure applied to the multilingual BLOOM family, giving rise to several clearly defined variants that differ in the data mixture and prompt languages used. The paper enumerates three principal data configurations: (1) BLOOMZ-P3 models, finetuned exclusively on the English-only P3 collection; (2) BLOOMZ models, finetuned on xP3, a multilingual suite whose instances are paired with English prompts; and (3) BLOOMZ-MT models, finetuned on xP3mt, which augments xP3 with machine-translated prompts, making both the underlying datasets and the prompts multilingual.\n\nMultiple sizes are released for each configuration. Explicitly named checkpoints include: BLOOMZ-560M (560 M parameters, xP3); BLOOMZ-7.1B (7.1 B parameters, xP3); BLOOMZ-7.1B-P3 (7.1 B parameters, P3); BLOOMZ-7.1B-MT (7.1 B parameters, xP3mt); BLOOMZ-MT (the full 176 B-parameter model, xP3mt); and BLOOMZ-P3 (176 B parameters, P3). The authors further remark that the presence or absence of code data in the fine-tuning mix can significantly shift performance: when no code data are included in BLOOMZ-P3, results drop notably, whereas meaningful gains over the base BLOOM model appear mainly for smaller checkpoints such as BLOOMZ-560M. In summary, the fine-tuning pipeline for BLOOMZ relies on carefully curated, instruction-style corpora (P3, xP3, xP3mt) with English-only, multilingual-English-prompt and multilingual-plus-MT-prompt variants, and it produces a spectrum of parameter-scaled checkpoints whose empirical behaviour is sensitive to both the chosen data mixture and the presence of auxiliary code examples.",
  "4-3 (Reinforcement Learning Data)": "",
  "4-4 (Data Filtering)": "",
  "4-1 (Pre-training Data)__evidence": [
    {
      "source": "[sections/ROOTS language contamination]",
      "quote": "These “unseen” languages only have small sentence proportions in our subsample compared to English (en: 46.23%), French (fr: 15.73%) and Spanish (es: 13.38%). Yet, they may help the language generalization of BLOOMZ models described in §4.2."
    },
    {
      "source": "[sections/B Figure 10 caption]",
      "quote": "In Figure 10, we visualize task generalization to multilingual datasets. We also find that mT0 consistently outperforms BLOOMZ on Swahili (SW), possibly due to it being a larger part of its pretraining corpus (see Figure 2 and §4.6)."
    }
  ],
  "4-2 (Fine-tuning Data)__evidence": [
    {
      "source": "[pdf_text]",
      "quote": "We produce three core model variants available in different sizes: • BLOOMZ-P3 / mT0-P3: Models finetuned on the English-only P3. • BLOOMZ / mT0: Models finetuned on xP3, which consists of multilingual datasets with English prompts. • BLOOMZ-MT / mT0-MT: Models fine-tuned on xP3mt, which consists of multilingual datasets with English and machine-translated prompts."
    },
    {
      "source": "[pdf_text]",
      "quote": "Only for small models, such as BLOOMZ-560M vs. BLOOM-560M, there are meaningful performance gains. When no code data is included in finetuning (BLOOMZ-P3) performance decreases significantly."
    },
    {
      "source": "[sections/Artifacts]",
      "quote": "BLOOMZ-560M\n560M parameter model finetuned on xP3"
    },
    {
      "source": "[sections/Artifacts]",
      "quote": "BLOOMZ-7.1B\n7.1B parameter model finetuned on xP3"
    },
    {
      "source": "[sections/Artifacts]",
      "quote": "BLOOMZ-7.1B-MT\n7.1B parameter model finetuned on xP3mt"
    },
    {
      "source": "[sections/Artifacts]",
      "quote": "BLOOMZ-7.1B-P3\n7.1B parameter model finetuned on P3"
    },
    {
      "source": "[abstract]",
      "quote": "We apply MTF to the pretrained multilingual BLOOM and mT5 model families to produce finetuned variants called BLOOMZ and mT0."
    },
    {
      "source": "[sections/Models]",
      "quote": "We produce three core model variants available in different sizes: • BLOOMZ-P3 / mT0-P3: Models finetuned on the English-only P3. • BLOOMZ / mT0: Models finetuned on xP3, which consists of multilingual datasets with English prompts. • BLOOMZ-MT / mT0-MT: Models fine- tuned on xP3mt, which consists of multilingual datasets with English and machine-translated prompts."
    },
    {
      "source": "[sections/Artifacts]",
      "quote": "BLOOMZ-560M 560M parameter model finetuned on xP3"
    },
    {
      "source": "[sections/Artifacts]",
      "quote": "BLOOMZ-7.1B-P3 7.1B parameter model finetuned on P3"
    },
    {
      "source": "[sections/Artifacts]",
      "quote": "BLOOMZ-7.1B-MT 7.1B parameter model finetuned on xP3mt"
    },
    {
      "source": "[sections/Artifacts]",
      "quote": "BLOOMZ-MT 176B parameter model finetuned on xP3mt"
    },
    {
      "source": "[sections/Artifacts]",
      "quote": "BLOOMZ-P3 176B parameter model finetuned on P3"
    }
  ],
  "4-3 (Reinforcement Learning Data)__evidence": [],
  "4-4 (Data Filtering)__evidence": [],
  "__usage": {
    "fine_tuning": "used",
    "rl": "not_used"
  }
}