{
  "1-5 (Architecture)": "The BLOOMZ family is presented entirely in terms of model sizes and their fine-tuning status.  At the top end, the release references “BLOOMZ 176B”, explicitly described twice as a 176-billion-parameter model that has been finetuned on the xP3 supervision mixture.  A cross-model comparison sentence provides an external size reference, stating that mT0 has 13 billion parameters and is “an order of magnitude fewer parameters” than BLOOMZ 176B, anchoring the BLOOMZ flagship at 176 B parameters.  Beneath this flagship, several smaller checkpoints are enumerated: “BLOOMZ-7.1B” (7.1 B parameters) which is also reported in two different contexts—one ordinary listing, and one fuller description (“7.1 B parameter model finetuned on xP3mt”).  A table caption further emphasizes experimentation on that 7.1 B variant, noting that “Table 5” benchmarks several 7.1 B BLOOMZ models with unspecified “various modifications” on the MultiEURLEX English–French translation task.  Even smaller distilled versions are named in sequence—“BLOOMZ-3B”, “BLOOMZ-1.7B”, “BLOOMZ-1.1B”, and “BLOOMZ-560M”.  The 560 M checkpoint receives an explicit description paralleling the flagship line: “BLOOMZ-560M 560M parameter model finetuned on xP3”.  Collectively, the quotes therefore establish a tiered architecture line-up consisting of at least six discrete parameter scales (560 M, 1.1 B, 1.7 B, 3 B, 7.1 B, and 176 B), and indicate that each of these checkpoints is the result of a supervised fine-tuning pass (either on xP3 or xP3mt).  No other structural details—such as layer counts, hidden sizes, or attention configuration—are given in the extracted sentences; only the total parameter counts, the finetuning dataset labels, and the acknowledgment that modifications were explored on the 7.1 B variant appear in the source material.",
  "1-6 (Tokenizer)": "None of the extracted BLOOMZ sentences contain any reference to the tokenizer, its type, configuration, vocabulary, or download location, so no tokenizer details can be summarized from the available quotes.",
  "2-1 (Hardware)": "The quote set contains no explicit mention of training hardware—there are no references to GPU/TPU model names, counts, or total compute—so hardware information cannot be reported.",
  "2-2 (Software)": "One sentence provides software-level insight: “BLOOMZ models have an additional repository containing the final optimizer states for training with Megatron-Deepspeed that can be found by appending “-optimizer-states” to the respective URL.”  From this we can conclude that the training stack used the Megatron-Deepspeed framework, and that the team made the full optimizer checkpoints available in a dedicated repository path ending in “-optimizer-states”.  No other software components, framework versions, or training flags are mentioned in the provided material.",
  "1-5 (Architecture)__evidence": [
    {
      "source": "[pdf_text]",
      "quote": "Despite an order of magnitude fewer parameters, mT0 (13 billion parameters) is ahead of BLOOMZ (176 billion parameters)."
    },
    {
      "source": "[sections/C Artifacts]",
      "quote": "BLOOMZ 176B parameter model finetuned on xP3"
    },
    {
      "source": "[sections/C Artifacts]",
      "quote": "BLOOMZ-7.1B 7.1B parameter model finetuned on xP3mt"
    },
    {
      "source": "[sections/G Increasing generation length]",
      "quote": "Table 5: 7.1 billion parameter BLOOMZ models with various modifications benchmarked on MultiEURLEX English-French translation (Chalkidis et al., 2021)."
    },
    {
      "source": "[pdf_text]",
      "quote": "BLOOMZ-560M"
    },
    {
      "source": "[pdf_text]",
      "quote": "BLOOMZ-1.1B"
    },
    {
      "source": "[pdf_text]",
      "quote": "BLOOMZ-1.7B"
    },
    {
      "source": "[pdf_text]",
      "quote": "BLOOMZ-3B"
    },
    {
      "source": "[pdf_text]",
      "quote": "BLOOMZ-7.1B"
    },
    {
      "source": "[sections/4.1 Task generalization]",
      "quote": "Despite an order of magnitude fewer parameters, mT0 (13 billion parameters) is ahead of BLOOMZ (176 billion parameters)."
    },
    {
      "source": "[pdf_text/Artifacts]",
      "quote": "BLOOMZ-560M 560M parameter model finetuned on xP3"
    },
    {
      "source": "[pdf_text/Artifacts]",
      "quote": "BLOOMZ 176B parameter model finetuned on xP3"
    }
  ],
  "1-6 (Tokenizer)__evidence": [],
  "2-1 (Hardware)__evidence": [],
  "2-2 (Software)__evidence": [
    {
      "source": "[pdf_text/Artifacts]",
      "quote": "BLOOMZ models have an additional repository containing the final optimizer states for training with Megatron-Deepspeed that can be found by appending “-optimizer-states\" to the respective URL."
    }
  ]
}