{
  "model_id": "bigscience/bloomz",
  "full_texts": [
    {
      "arxiv_id": "2211.01786",
      "full_text": "Crosslingual Generalization through Multitask Finetuning\nNiklas Muennighoff1\nThomas Wang1\nLintang Sutawika2,3\nAdam Roberts4\nStella Biderman3,5\nTeven Le Scao1\nM Saiful Bari6\nSheng Shen7\nZheng-Xin Yong8\nHailey Schoelkopf 3,9\nXiangru Tang 9\nDragomir Radev9\nAlham Fikri Aji10\nKhalid Almubarak11\nSamuel Albanie12\nZaid Alyafeai13\nAlbert Webson8\nEdward Raff5\nColin Raffel1\n1Hugging Face\n2Datasaur.ai\n3EleutherAI\n4Google Research, Brain Team\n5Booz Allen Hamilton\n6Nanyang Technological University\n7 UC Berkeley\n8 Brown University\n9 Yale University\n10 MBZUAI\n11 PSAU\n12 University of Cambridge 13 KFUPM\nniklas@hf.co\nAbstract\nMultitask prompted finetuning (MTF) has been\nshown to help large language models gener-\nalize to new tasks in a zero-shot setting, but\nso far explorations of MTF have focused on\nEnglish data and models. We apply MTF to\nthe pretrained multilingual BLOOM and mT5\nmodel families to produce finetuned variants\ncalled BLOOMZ and mT0. We find finetuning\nlarge multilingual language models on English\ntasks with English prompts allows for task gen-\neralization to non-English languages that ap-\npear only in the pretraining corpus. Finetun-\ning on multilingual tasks with English prompts\nfurther improves performance on English and\nnon-English tasks leading to various state-of-\nthe-art zero-shot results. We also investigate\nfinetuning on multilingual tasks with prompts\nthat have been machine-translated from En-\nglish to match the language of each dataset.\nWe find training on these machine-translated\nprompts leads to better performance on human-\nwritten prompts in the respective languages.\nSurprisingly, we find models are capable of\nzero-shot generalization to tasks in languages\nthey have never intentionally seen. We conjec-\nture that the models are learning higher-level\ncapabilities that are both task- and language-\nagnostic.\nIn addition, we introduce xP3, a\ncomposite of supervised datasets in 46 lan-\nguages with English and machine-translated\nprompts.\nOur code, datasets and models\nare freely available at https://github.com/\nbigscience-workshop/xmtf.\n1\nIntroduction\nLarge language models pretrained on vast amounts\nof text show some capability of solving tasks ex-\npressed in natural language, even without explicit\ntraining on these tasks (Brown et al., 2020). Fine-\ntuning on groups of language tasks has been shown\nto significantly boost this zero-shot task general-\nization of language models (Wei et al., 2021; Sanh\net al., 2022; Min et al., 2021). For example, Sanh\net al. (2022) finetune on tasks like summarization\nand question answering leading to better perfor-\nmance on unseen tasks like natural language in-\nference. Previous work has focused on multitask\nfinetuning in the context of large English language\nmodels and tasks.\nMultilingual large language models show the\nsame zero-shot learning capabilities for both mono-\nlingual and crosslingual tasks (Goyal et al., 2021a;\nLin et al., 2021; Patel et al., 2022; Soltan et al.,\n2022). However, zero-shot performance tends to\nbe significantly lower than finetuned performance.\nThus, task-specific or language-specific transfer\nlearning via finetuning remains the predominant\npractice (Devlin et al., 2018; Conneau et al., 2019;\nAribandi et al., 2021). This is particularly chal-\nlenging for low-resource languages or tasks with\nlimited data available, such as writing a fable that\nteaches a specified moral. In the spirit of multitask\nfinetuning, it would be desirable to improve the\nzero-shot task generalization of multilingual mod-\nels to make them usable on tasks from low-resource\nlanguages without requiring further finetuning.\nTo address this goal, we focus on crosslingual\nmultitask finetuning. Due to the difficulty of col-\nlecting supervised task data in low-resource lan-\nguages, previous work typically aims to transfer\ncapabilities learned from finetuning on English\ndata, which can improve performance on non-\nEnglish language tasks (Wu and Dredze, 2019;\nPhang et al., 2020; Chalkidis et al., 2021; Vu et al.,\n2022). We investigate whether English-only mul-\ntitask finetuning also improves performance on\nnon-English held-out tasks using the multilingual\nBLOOM (Scao et al., 2022a) and mT5 (Xue et al.,\n2020) models. We find that after finetuning on the\nEnglish-only multitask mixture used for T0 (Sanh\net al., 2022) (P3), performance on a diverse set of\nnon-English held-out tasks increases.\nTo investigate whether multilingual task data can\nfurther improve performance, we extend P3 to xP3\narXiv:2211.01786v2  [cs.CL]  29 May 2023\n\nSummarization\nSentiment\nParaphrase\nIdentification\nCoreference\nResolution\nQQP\nMRPC\nPAWS\nYelp\nRotten Tomatoes\nApp Reviews\nIMDB\nAmazon\nTopic Classification\nAG News\nDBPedia\nTREC\nStructure-To-Text\nWiki Bio\nCommon Gen\nMultiNews\nGigaword\nXSum\nSamSum\nCNN Daily Mail\nClosed-Book QA\nHotpot QA\nWiki QA\nExtractive QA\nROPES\nAdversarial QA\nDuoRC\nMultiple-Choice QA\nDREAM\nQuAIL\nQuaRTz\nSocial IQA\nCosmos QA\nQASC\nWiQA\nSciQ\nQuaRel\nCOPA\nSentence \nCompletion\nStoryCloze\nNatural Language\nInference\nANLI\nCB\nRTE\nWinogrande\nWord Sense\nDisambiguation\nWiC\nQuoref\nWiki Hop\nPAWS-X\nXWinograd\nXNLI\nXCOPA\nWiki-Lingua\nxQuAD\nTranslation\nTatoeba\nFlores-200\nProgram Synthesis\nHumanEval\nMBPP\nXL-WiC\nXLSum\nXStoryCloze\nNeuralCodeSearch\nGreatCode\nMLQA\nTyDi QA\nCode Misc.\nCodeComplex\nState Changes\nAPPS\nXLCoST\nCodeContests\nDocstring Corpus\nCSL\nJupyterCodePairs\nC3\nCMRC2018\nDRCD\nTNEWS\nSimplification\nBiSECT\nRACE\nBoolQ\nARC (AI2)\nTriviaQA\nSQuAD (V2)\nReCoRD\nMultiRC\nPiQA\nWebQuestions\nCoS-E\nOpenBookQA\nFigure 1: An overview of datasets in xP3. Datasets added to P3 in this work are marked bold. Yellow datasets are\ntrained on. Green datasets are held out for evaluation.\nen es pt\nfr\nar\nid\nzh\nhi code vi\nur\nte\nta bn mr sw gu pa ne yo ig ny zu xh sn\nts rw lg\ntn nso rn ml kn or as\nln wotum ki\nst fon ca eu ak bm tw\n25\n5\n1\n0.1\n0.01\n0.001\n0.0001\n% of corpus\nxP3\nROOTS\nmT5 corpus\nFigure 2: Language composition of xP3, ROOTS, and the corpus of mT5. All ROOTS and xP3 languages are\ndepicted. The mT5 corpus covers additional languages that are not included in the graph.\nby adding datasets from 46 different languages that\ncover tasks previously not present in P3 (such as\ntranslation and program synthesis). Finetuning on\nxP3 leads to even better zero-shot task generaliza-\ntion in both English and non-English compared\nto the P3-trained baseline. Models finetuned on\nxP3 perform best on English prompts, even for\nnon-English samples. Hypothesizing that better\nperformance could be attained by training on non-\nEnglish prompts, we construct a variant of xP3\nwith machine-translated prompts called xP3mt. We\nfind that finetuning on machine-translated prompts\nis enough to significantly increase performance\non held-out tasks with non-English human-written\nprompts. However, reducing the number of En-\nglish prompts in the finetuning also worsens En-\nglish prompt performance on multilingual tasks.\nNotably, we also find that models finetuned on\nxP3 generalize to held-out tasks in languages never\nintentionally seen during pretraining nor finetun-\ning. We conduct a contamination analysis and find\nthat only small amounts of these languages were\nincluded in the pretraining corpus. Thus, we hy-\npothesize the models learn some language- and\ntask-agnostic capabilities.\nWe publicly release all our datasets and models\n(URLs in Appendix §C).\n2\nRelated work\n2.1\nMultitask learning\nMultitask finetuning (Sanh et al., 2022) (or instruc-\ntion tuning (Wei et al., 2021)) has emerged as a\nrecipe for improving the zero-shot task generaliza-\ntion of large language models. Typically, these\nworks define a task as a collection of datasets that\n\nSentence1 \nHe was a scholar in Metaphysical \nLiterature, Theology and Classical \nsciences.\nSentence2\nHe was a scholar in metaphysical \nliterature, theology, and classical \nscience.\nLabel\n1\nSentence1 \nHe was a scholar in Metaphysical \nLiterature, Theology and Classical \nsciences.\nSentence2\nHe was a scholar in metaphysical \nliterature, theology, and classical \nscience.\nLabel\n1\nSentence1 \nHe was a scholar in Metaphysical \nLiterature, Theology and Classical \nsciences.\nSentence2\nHe was a scholar in metaphysical \nliterature, theology, and classical \nscience.\nLabel\n1\nP3\nsentence1 \nHe was a scholar in Metaphysical \nLiterature, Theology and Classical \nsciences.\nsentence2\nHe was a scholar in metaphysical \nliterature, theology, and classical \nscience.\nlabel\n1\nOración 1: {{sentence1}}\nOración 2: {{sentence2}}\nPregunta: ¿La oración 1 parafrasea la \noración 2? ¿Si o no? {{Choices[label]}}\nSentence 1: {{sentence1}}\nSentence 2: {{sentence2}}\nQuestion: Can we rewrite Sentence 1 to \nSentence 2? Yes or No? {{Choices[label]}}\nChoices=[No,Sí]\nxP3\nSentence1 \nHe was a scholar in Metaphysical \nLiterature, Theology and Classical \nsciences.\nSentence2\nHe was a scholar in metaphysical \nliterature, theology, and classical \nscience.\nLabel\n1\nSentence1 \nHe was a scholar in Metaphysical \nLiterature, Theology and Classical \nsciences.\nSentence2\nHe was a scholar in metaphysical \nliterature, theology, and classical \nscience.\nLabel\n1\nSentence1 \nHe was a scholar in Metaphysical \nLiterature, Theology and Classical \nsciences.\nSentence2\nHe was a scholar in metaphysical \nliterature, theology, and classical \nscience.\nLabel\n1\nsentence1 \nFue académico en literatura metafísica, \nteología y ciencias clásicas.\"\nsentence2\nFue académico en literatura metafísica, \nteología y ciencia clásica.\nlabel\n1\nxP3mt\nSentence1 \nHe was a scholar in Metaphysical \nLiterature, Theology and Classical \nsciences.\nSentence2\nHe was a scholar in metaphysical \nliterature, theology, and classical \nscience.\nLabel\n1\nSentence1 \nHe was a scholar in Metaphysical \nLiterature, Theology and Classical \nsciences.\nSentence2\nHe was a scholar in metaphysical \nliterature, theology, and classical \nscience.\nLabel\n1\nSentence1 \nHe was a scholar in Metaphysical \nLiterature, Theology and Classical \nsciences.\nSentence2\nHe was a scholar in metaphysical \nliterature, theology, and classical \nscience.\nLabel\n1\nsentence1 \nFue académico en literatura metafísica, \nteología y ciencias clásicas.\"\nsentence2\nFue académico en literatura metafísica, \nteología y ciencia clásica.\nlabel\n1\nChoices=[No,Yes]\nSentence 1: {{sentence1}}\nSentence 2: {{sentence2}}\nQuestion: Can we rewrite Sentence 1 to \nSentence 2? Yes or No? {{Choices[label]}}\nChoices=[No,Yes]\nFigure 3: Comparison of dataset variants P3, xP3, and xP3mt on a sample from PAWS for P3 (Zhang et al., 2019)\nand PAWS-X (Yang et al., 2019) for xP3 and xP3mt. P3 pairs English datasets with English prompts, xP3 pairs\nmultilingual datasets with English prompts and xP3mt pairs multilingual datasets with prompts machine-translated\nfrom English to match the dataset language. Expressions in curly brackets are replaced, e.g. for xP3mt the target\nshown as {{Choices[label]}}) becomes Sí.\nrequire a certain set of skills. To inform large lan-\nguage models which task to perform given an in-\nput, a prompt is used to add natural language in-\nstructions to dataset instances (Schick and Schütze,\n2020; Scao and Rush, 2021). In this line of work,\nzero-shot task generalization refers to the ability to\nperform a held-out task based on prompted instruc-\ntions alone. Our work builds on T0 (Sanh et al.,\n2022), a variant of T5 (Raffel et al., 2020) that un-\nderwent MTF and was subsequently shown to have\nstrong zero-shot task generalization capabilities.\nIncreasing the number and diversity of finetun-\ning tasks and datasets has been shown to increase\nmodel performance (Min et al., 2021; Fries et al.,\n2022; Wang et al., 2022d; Scialom et al., 2022;\nChung et al., 2022; Mishra et al., 2021b). Prompt-\nSource (Bach et al., 2022) is a software application\nthat provides a framework for developing and ap-\nplying prompts. PromptSource was used to con-\nstruct P3, the training dataset of T0. While most\nprior work has focused on using English prompts\non English datasets, Wang et al. (2022c) trained\nboth English and multilingual models on prompted\ndatasets. Their multilingual model, called mTk-\nInstruct, attains strong crosslingual performance.\nIn contrast with Wang et al. (2022c), our sole focus\nis crosslingual zero-shot generalization. Therefore,\nwe consider a wider variety of prompting settings\nand perform a more detailed evaluation of multilin-\ngual capabilities. Separately, Radford et al. (2019)\nfind that accidental inclusion of non-English text\ngave the GPT-2 model a limited ability to process\nand generate non-English text. We similarly dis-\ncover that our finetuned models can process text in\nlanguages not intentionally trained on.\n2.2\nMultilingual models\nMany language models are pretrained on English\ndata only. Multilingual pretrained language mod-\nels (Lample and Conneau, 2019; Conneau et al.,\n2019; Fan et al., 2021) aim to enable processing\na wide variety of non-English languages. Unlike\nmonolingual models, multilingual models can also\nbe used for crosslingual tasks, such as translation.\nFor language generation, recent efforts have fo-\ncused on two different model architectures based\non the Transformer (Vaswani et al., 2017). On the\none hand, encoder-decoder transformers trained\nwith a denoising objective such as mBART (Liu\net al., 2020) and mT5 (Xue et al., 2020) learn to\npredict tokens masked out in the input sequence.\nPredicting masked tokens is only a pretraining task\nand these models are generally finetuned on down-\nstream datasets before being used. On the other\nhand, decoder-only models pretrained on next to-\nken prediction such as mGPT (Shliazhko et al.,\n2022), XGLM (Lin et al., 2021) and BLOOM\n(Scao et al., 2022a) can be used to solve tasks ex-\npressed in natural language directly in a zero-shot\nor few-shot setting (Brown et al., 2020). XGLM\ndemonstrated competitive few-shot performance\neven when the model was prompted in a language\ndifferent than the sample being processed. In par-\nticular, using English prompts for multilingual\ndatasets provides better performance with XGLM\nthan human-translating the English prompt to the\ndataset language.\nIn this work, we use the BLOOM models (Scao\net al., 2022a,b), which were pretrained on the\nROOTS corpus (Laurençon et al., 2022) in 46 natu-\nral languages and 13 programming languages. We\n\nalso finetune mT5 (Xue et al., 2020) to compare\nencoder-decoder and decoder-only performance.\nmT5 is pretrained on a corpus sampled from mC4\ncovering 101 languages.\n3\nFinetuning data and models\nTo study crosslingual multitask prompted finetun-\ning, we create xP3 by extending the P3 dataset\ncollection with additional non-English tasks. We\nfinetune both BLOOM and mT5 models on xP3.\nWe refer to Appendix §C for public links to re-\nleased models and datasets.\n3.1\nFinetuning data\nWe build on the P3 (Sanh et al., 2022) task taxon-\nomy and add 30 new multilingual datasets illus-\ntrated in Figure 1. We define four task clusters\npreviously not present in P3: translation, simplifi-\ncation, program synthesis, and miscellaneous code\ndatasets. As 11% of BLOOM’s pretraining data is\ncode, we add code datasets classified as program\nsynthesis (text-to-code) or miscellaneous. The lat-\nter includes tasks such as estimating the computa-\ntional complexity of a provided code snippet and\ngenerating a name for a given function. We extend\nthe XWinograd dataset (Tikhonov and Ryabinin,\n2021) with winograd schemas from CLUE (Xu\net al., 2020) to increase its Chinese samples from\n16 to 504. Similar to P3, a fraction of our prompts\ninvert the task at hand. For example, a prompt\nmay invert a closed-book QA sample by asking the\nmodel to generate a question given an answer.\nWith xP3 we aim to replicate the language dis-\ntribution of the ROOTS corpus (Laurençon et al.,\n2022) used to pretrain BLOOM. Thus, xP3 con-\nsists of the same 46 natural languages and code as\nROOTS. ROOTS, xP3 and the mT5 corpus (Xue\net al., 2020) language distributions are visualized in\nFigure 2. 39% of xP3 data is English, slightly more\nthan the 30% of English data in ROOTS. Various\nAfrican languages such as Twi (tw) and Bambara\n(bm) form the tail of xP3’s language distribution.\nMany of them are not included in the mT5 pretrain-\ning corpus. In xP3, Twi and others are represented\nsolely as a translation task using data from Flores-\n200 (NLLB Team et al., 2022).\nTo study the importance of non-English prompts,\nwe construct a machine-translated variant of xP3,\nxP3mt.\nWe translate prompts of monolingual\ndatasets into the respective dataset language. For\nexample, for the Chinese dataset C3 (Sun et al.,\n2020) prompts in xP3mt are in Chinese instead of\nEnglish in xP3. For crosslingual datasets prompts\nremain in English in xP3mt (such as Wiki-Lingua,\nwhich involves producing a summary in one lan-\nguage based on text in another language). We use\nthe Google Cloud API for machine translation1.\nFigure 3 compares the dataset variants we train on.\n3.2\nModels\nWe use publicly available pretrained BLOOM mod-\nels ranging from 560 million to 176 billion pa-\nrameters. BLOOM models are large decoder-only\nlanguage models pretrained for around 350 bil-\nlion tokens with an architecture similar to GPT-3\n(Brown et al., 2020). We finetune the models for\nan additional 13 billion tokens with loss only being\ncomputed on target tokens. For example, given\nthe input “Translate to English: Je t’aime.\" and\na space-separated target “I love you.\", the model\nis trained to predict only the targets. As targets\nvary in length from just one to hundreds of tokens,\nwe downscale the loss of each token by the length\nof the target it belongs to. This ensures short tar-\ngets (e.g. for multiple-choice QA) get the same\nweight as long targets (e.g. for translation). We\nskip samples longer than 2048 tokens and use pack-\ning to train efficiently on multiple samples at a time\n(Kosec et al., 2021). We select the final checkpoint\nbased on validation performance.\nFor mT5 models,\nwe finetune using the\nT5X (Roberts et al., 2022) framework on TPUs.\nmT5 uses the same encoder-decoder architecture,\npretraining objective (masked language modeling),\nand pretraining length (1 trillion tokens) as T5 (Raf-\nfel et al., 2020). For finetuning mT5, we follow the\nsame procedure as described above for BLOOM,\nexcept that inputs are fed into the encoder and thus\nare not space-separated from targets.\nWe produce three core model variants available\nin different sizes:\n• BLOOMZ-P3 / mT0-P3: Models finetuned\non the English-only P3.\n• BLOOMZ / mT0: Models finetuned on xP3,\nwhich consists of multilingual datasets with\nEnglish prompts.\n• BLOOMZ-MT / mT0-MT: Models fine-\ntuned on xP3mt, which consists of multi-\nlingual datasets with English and machine-\ntranslated prompts.\n1https://cloud.google.com/translate\n\nSentence Completion (XCOPA & XStoryCloze)\n51.8\n52.9\n50.4\n80.5\n82.6\n86.0\nNatural Language Inference (XNLI)\n33.3\n33.6\n34.8\n47.4\n55.3\n57.7\nMultilingual Multitask Generalization\nXGLM-7.5B\nBLOOM\nmTk-Instruct-13B\nBLOOMZ-P3\nBLOOMZ\nmT0-13B\nCoreference Resolution (XWinograd)\n50.3\n50.6\n54.7\n54.6\n66.3\n74.6\nFigure 4: Zero-shot multilingual task generalization with English prompts. BLOOM models have 176 billion\nparameters. Scores are the language average for each task. Appendix §B breaks down performance by language.\nWe evaluate on three held-out tasks: corefer-\nence resolution, sentence completion and natural\nlanguage inference (NLI) as depicted in Figure 1.\nWe also evaluate on HumanEval due to its popu-\nlarity for code evaluations using the pass@k met-\nric (Chen et al., 2021). For datasets that involve\nchoosing the correct completion from several op-\ntions, we follow prior work (Sanh et al., 2022;\nBrown et al., 2020) and use rank classification:\nWe compute the log-likelihood of each possible\ncompletion and select the highest scoring option.\nFor each evaluation dataset, we select 5 prompts at\nrandom from PromptSource and use them for all\nlanguage splits of the dataset. We report the me-\ndian of the 5 prompts for results per language split.\nThus, in constrast to XGLM (Lin et al., 2021), we\ndo not tune prompts based on performance on vali-\ndation data. A selection of prompts can be found in\nAppendix §M. For evaluation on generative tasks,\nsuch as translation, we use lm-evaluation-harness\n(Gao et al., 2021) and report BLEU scores (Pap-\nineni et al., 2002).\n4\nResults\nWe first examine generalization to new tasks in\nlanguages included in finetuning in §4.1. Then,\nin §4.2, we look at language generalization: Can\nmodels generalize to tasks in languages that (a)\nthey have only seen during pretraining and (b) they\nhave never seen intentionally? In §4.3, we inves-\ntigate performance on multilingual prompts and\nfinetuning on xP3mt. Scaling laws are analyzed\nin §4.4. Finally, §4.5 looks at performance on\ngenerative tasks and §4.6 at the effect of language\nproportions on performance.\n4.1\nTask generalization\nPrevious work has shown that large language mod-\nels finetuned on prompted multitask mixtures gen-\neralize to unseen tasks (Zhong et al., 2021; Wei\net al., 2021; Mishra et al., 2021b,a; Wang et al.,\n2022c). In Figure 4, we show that the same applies\nto multilingual models: Finetuned BLOOMZ and\nBLOOMZ-P3 models significantly improve over\nBLOOM and XGLM on held-out tasks. Despite\nan order of magnitude fewer parameters, mT0 (13\nbillion parameters) is ahead of BLOOMZ (176 bil-\nlion parameters). We attribute this to the encoder-\ndecoder architecture paired with a masked lan-\nguage modeling pretraining objective (Wang et al.,\n2022a; Tay et al., 2022a) as well as the longer pre-\ntraining of mT5 (Hoffmann et al., 2022; Su et al.,\n2022) (1 trillion tokens for mT5 vs. 366 billion\nfor BLOOM). Despite also having gone through\ncrosslingual multitask finetuning, mTk-Instruct per-\nforms significantly worse than the same-sized mT0.\nWe attribute this to our prompting style, which aims\nto replicate natural human communication. mTk-\nInstruct is finetuned on more structured prompts\nwith specific “Definition\", “Input\" and “Output\"\nfields. Similarly, Wang et al. (2022c) find that T0\nperforms worse than Tk-Instruct on their prompts.\nWe also find models finetuned on the 39% En-\nglish xP3 (BLOOMZ, mT0-13B) outperform mod-\nels finetuned on the 100% English P3 (BLOOMZ-\nP3, mT0-13B-P3) on English tasks (Appendix §B).\nEven the fully English T0-11B model (Sanh et al.,\n2022) is outperformed by our mT0-13B model on\nentirely English tasks. Ignoring embedding param-\neters T0-11B and mT0-13B have about the same\nsize. This is likely due to xP3 adding additional\ntasks and prompts, which has been shown to help\n\n20\n30\n40\n50\n60\n20\n30\n40\n50\n60\n20\n30\n40\n50\n60\n20\n30\n40\n50\n60\n20\n30\n40\n50\n60\n20\n30\n40\n50\n60\n40\n50\n60\n70\n40\n50\n60\n70\n40\n50\n60\n70\n80\n40\n50\n60\n70\n80\n40\n50\n60\n70\n80\n40\n50\n60\n70\n80\n40\n50\n60\n70\n80\n40\n50\n60\n70\n80\n40\n50\n60\n70\n80\n90\n40\n50\n60\n70\n80\n90\nBLOOM-7.1B\nBLOOM\nBLOOMZ-7.1B\nBLOOMZ\nXNLI BG\nXNLI DE\nXNLI EL\nXNLI RU\nXNLI TH\nXNLI TR\nXWinograd JP\nXWinograd RU\nXCOPA ET\nXCOPA HT\nXCOPA IT\nXCOPA QU\nXCOPA TH\nXCOPA TR\nXStoryCloze MY\nXStoryCloze RU\nNatural Language Inference\nCoreference Resolution\nSentence Completion\nFigure 5: Zero-shot task and language generalization using English prompts on tasks and languages not intentionally\nseen during pretraining nor finetuning. Language codes are ISO 639-1, except for JP (Japanese).\ngeneralization (Chung et al., 2022; Iyer et al., 2022).\nmT0-13B beating T0-11B indicates that the benefit\nof scaling tasks is larger than the benefit of pre-\ntraining and finetuning on relatively more English\ntokens.\n4.2\nLanguage generalization\nHere we add another layer of generalization: lan-\nguages. Figure 4 already shows that finetuning on\nEnglish data only (P3) leads to better performance\non non-English data: For example, BLOOMZ-P3\nimproves by over 50% on multilingual sentence\ncompletion compared to BLOOM. Thus, zero-shot\ntask performance in languages only seen during\npretraining improves after finetuning on English.\nThis has major practical benefits as it can be more\ndifficult to collect data for low-resource languages.\nNext, we investigate performance on languages\nthe model has never intentionally seen. Due to\nthe scale of large language model pretraining, it\nis difficult to label tasks or languages as strictly\nunseen. It is likely that the training data uninten-\ntionally includes small fractions of these languages\n(just as many tasks might appear “implicitly” in the\npretraining corpus (Sanh et al., 2022)). In Figure 5\nwe show that after multitask finetuning on xP3,\nthe models can perform unseen tasks in languages\nthat were not intentionally trained on. After prob-\ning the pretraining corpus of BLOOM, we do find\nsmall amounts of these languages that were unin-\ntentionally included (Appendix §D). However, for\nXNLI, performance increases across all languages,\nmany of which only show up in tiny fractions in\nour language contamination analysis, such as Thai\nwith 0.006%. If we extrapolate this proportion to\nthe entire ROOTS corpus, the BLOOM models\nwould have seen a mere 20 million tokens of Thai\nduring pretraining. One possibility is that better-\nthan-random XNLI performance can be attained\nwith little or no language understanding. In Ap-\npendix §H, we investigate edit distances of XNLI\nsamples and find that there are differences across\nlabels, however, likely not significant enough to\nenable this kind of generalization.\n4.3\nMultilingual prompting\nTask\nPrompt\nAverage accuracy\nBLOOMZ\nBLOOMZ-MT\nmT0-13B\nmT0-13B-MT\nXNLI\nEN\n52.99\n49.01\n48.24\n51.29\nMT\n37.56\n41.16\n39.31\n41.66\nHT\n40.4\n43.88\n44.95\n46.87\nXCOPA\nEN\n72.52\n73.24\n81.4\n80.36\nMT\n70.04\n71.84\n81.16\n79.64\nXStoryCloze\nEN\n81.73\n81.39\n81.99\n82.3\nMT\n80.89\n81.76\n83.37\n82.86\nXWinograd\nEN\n60.07\n59.15\n70.49\n73.24\nMT\n58.48\n60.14\n66.89\n72.33\nTable 1:\nComparison between EN (English), MT\n(machine-translated)\nand\nHT\n(human-translated)\nprompts for 176B BLOOMZ and 13B mT0 models\nfinetuned on either only English or English and\nmachine-translated multilingual prompts (-MT).\nSince all prompts in xP3 are in English (even for\nmultilingual datasets), we created xP3mt, an exten-\n\nsion with machine-translated prompts. To investi-\ngate performance on non-English prompts, we addi-\ntionally human- and machine-translated the English\nevaluation prompts from Figure 4. In Table 1, we\nreport performance on these. Results on machine-\ntranslated prompts in languages that are not part\nof the finetuning corpus, such as those in Figure 5,\nare in Appendix §I. Table 1 shows that BLOOMZ\nperforms much better on English than on non-\nEnglish prompts. BLOOMZ-MT, which is fine-\ntuned on xP3mt, significantly improves on multilin-\ngual prompts. On XNLI, BLOOMZ-MT raises the\naverage performance on human-translated prompts\nfrom 41.13 to 45.55. This comes at the cost of a\nreduction in its performance on English prompts,\nfrom 53.58 to 49.74. For mT0, the MT version\nprovides similar performance gains on XNLI and\nXWinograd non-English prompts, while results\non XCOPA and XStoryCloze are mixed. Simi-\nlar to Lin et al. (2021), we also find that models\nperform better on human-translated prompts than\nmachine-translated ones for XNLI.\n4.4\nScaling\n0.561 1.7 3\n7.1\n176\n40\n60\n80\n100\nAverage accuracy (%)\nXCOPA\n0.561 1.7 3\n7.1\n176\n30\n40\n50\n60\nXNLI\n0.561 1.7 3\n7.1\n176\nModel parameters (billions)\n40\n60\n80\n100\nAverage accuracy (%)\nXStoryCloze\n0.561 1.7 3\n7.1\n176\nModel parameters (billions)\n45\n55\n65\n75\n85\nXWinograd\nBLOOM\nBLOOMZ\nmT0\nFigure 6: Aggregate performance vs. size. Transparent\nlines correspond to individual languages, while thick\nlines are average accuracy scores.\nIn Figure 4, the average performance of BLOOM\nis near the random baselines of 0.50 for Sen-\ntence Completion and Coreference Resolution and\n0.33 for NLI. We think this is due to all of our\nexperiments being zero-shot and using untuned\nprompts (Perez et al., 2021a). We find in Figure 6\nthat even at 560M parameters, multitask finetun-\ning improves zero-shot generalization. The gap\nbetween pretrained and multitask finetuned models\ngrows significantly as parameters increase. Scaling\nup parameters benefits all languages evaluated.\n4.5\nGeneration tasks\n30\n40\n50\n60\nAverage acc. (%)\nBLOOMZ\nBLOOMZ-7.1B\nmT0-13B\n0\n2\n4\n6\n8\nTraining tokens (billions)\n0\n10\n20\nAverage BLEU (%)\nFigure 7: Validation performance during training on\nnatural language understanding (NLU) and natural lan-\nguage generation (NLG) tasks. The former are scored\nusing accuracy and the latter using BLEU (Papineni\net al., 2002). The NLG tasks measured are translation\nand summarization. For BLOOMZ(-7.1B) the perfor-\nmance at 0 training tokens corresponds to the perfor-\nmance of BLOOM(-7.1B). For mT0 there is no data\npoint at 0 tokens, as its base model, mT5, is not suitable\nfor evaluation without finetuning. Performance on indi-\nvidual tasks is in Appendix §K.\nIn this section, we investigate the impact of mul-\ntitask finetuning on generative tasks. In Figure\n7, we plot validation performance throughout the\ntraining process. We find that while performance\non natural language understanding tasks contin-\nues to increase, generative performance jumps ini-\ntially and then decreases. Relatedly, in Table 2,\nwe find that multitask finetuning does not improve\nperformance on HumanEval (Chen et al., 2021).\nOnly for small models, such as BLOOM-560M\nvs. BLOOMZ-560M, there are meaningful perfor-\nmance gains. When no code data is included in\nfinetuning (BLOOMZ-P3) performance decreases\nsignificantly. mT0 models, which have not been\npretrained on code, fail to solve any HumanEval\nproblems (see full results in Appendix §K). Given\na Python docstring, HumanEval requires models to\ncomplete a function. Inspecting generations reveals\nthat the multitask finetuned models are biased to-\nwards short generations. In Appendix §E, we show\nexample solutions from HumanEval and compute\naverage length statistics. BLOOMZ tries to solve\n\nproblems with 70% fewer characters than BLOOM.\nPass@k\nk = 1\nk = 10\nk = 100\nGPT-Neo 1.3B\n4.79%\n7.47%\n16.30%\nGPT-Neo 2.7B\n6.41%\n11.27%\n21.37%\nGPT-J 6B\n11.62%\n15.74%\n27.74%\nGPT-NeoX 20B\n15.4%\n25.6%\n41.2%\nCodex-300M\n13.17%\n20.37%\n36.27%\nCodex-679M\n16.22%\n25.7%\n40.95%\nCodex-2.5B\n21.36%\n35.42%\n59.5%\nCodex-12B\n28.81%\n46.81%\n72.31%\nBLOOM-560M\n0.82%\n3.02%\n5.91%\nBLOOM-1.1B\n2.48%\n5.93%\n9.62%\nBLOOM-1.7B\n4.03%\n7.45%\n12.75%\nBLOOM-3B\n6.48%\n11.35%\n20.43%\nBLOOM-7.1B\n7.73%\n17.38%\n29.47%\nBLOOM\n15.52%\n32.20%\n55.45%\nBLOOMZ-560M\n2.18 %\n4.11%\n9.00%\nBLOOMZ-1.1B\n2.63%\n6.22%\n11.68%\nBLOOMZ-1.7B\n4.38%\n8.73%\n16.09%\nBLOOMZ-3B\n6.29%\n11.94%\n19.06%\nBLOOMZ-7.1B\n8.06%\n15.03%\n27.49%\nBLOOMZ\n12.06%\n26.53%\n48.44%\nBLOOMZ-P3\n6.13%\n11.79%\n18.73%\nTable 2: Code continuation on HumanEval.\nNon-\nBLOOM results come from prior work (Chen et al.,\n2021; Fried et al., 2022). Codex is a language model\nfinetuned on code, while the GPT models (Black et al.,\n2021; Wang and Komatsuzaki, 2021; Black et al., 2022)\nare trained on a mix of code and text like BLOOM. Fol-\nlowing Chen et al. (2021) we generate 200 samples for\neach problem with top p = 0.95 and compute pass rates.\nWe perform this evaluation three times for temperatures\n0.2, 0.6 and 0.8 and pick the best pass rate.\nThis bias towards short answers and the perfor-\nmance drop on generative tasks come from fine-\ntuning on short texts. Most tasks in our finetuning\ndataset, xP3, are single sentences. We show in Ap-\npendix §G that finetuning on fewer short tasks via\nearly stopping, adding long tasks or upweighting\nlong tasks leads to longer generations and slightly\nbetter performance. We find it most effective, how-\never, to force a minimum generation length at infer-\nence. This is done by ignoring any probability mass\nthe model assigns to its end-of-sequence token for\na desired number of tokens. Only after the genera-\ntion has reached the desired length, can the model\ngenerate the end-of-sequence token, thus finish-\ning the generation. Forcing a minimum generation\nlength improves the BLEU score on a translation\ntask by 9 points, see Appendix §G for quantitative\nand Figure 15 for qualitative results.\n0.01\n0.1\n1\n5 10 20\n55\n65\n75\n85\nAverage accuracy (%)\nXCOPA\n0.01\n0.1\n1\n5 1020\n35\n45\n55\n65\nXNLI\n0.01\n0.1\n1\n5 10 20\nPre-training size in BLOOM (%)\n50\n70\n90\nAverage accuracy (%)\nXStoryCloze\n5\n10\n20\n30\nPre-training size in BLOOM (%)\n50\n55\n60\n65\n70\nXWinograd\nBLOOM\nBLOOMZ-P3\nBLOOMZ-MT\nBLOOMZ\nFigure 8: Performance across languages by size in the\nBLOOM pretraining corpus, ROOTS.\n4.6\nEffect of language proportions\nIn Figure 8, we find that finetuned BLOOM mod-\nels perform better on languages seen extensively\nduring pretraining. As the language distribution\nin the finetuning dataset, xP3, closely follows that\nof pretraining, these languages are also seen most\nfrequently during finetuning. Specifically, XCOPA\nand XNLI show significantly better performance\non these high-resource languages, such as English,\nSpanish or French, which all make up more than\n10% of pretraining individually. The trend is less\nconsistent for XWinograd. This may be caused\nby the fact that XWinograd language subsets are\nnot translations of each other and have a signifi-\ncantly different number of samples. Thus, some\nlanguage subsets of XWinograd may be inherently\nmore difficult than others.\n5\nConclusion\nIn this work we investigated crosslingual multitask\nfinetuning. We developed xP3, a corpus consisting\nof tasks in 46 languages. Further, we have extended\nxP3 to xP3mt with machine-translated prompts.\nWe have finetuned pretrained BLOOM and mT5\nmodels on the newly created corpora as well as the\nEnglish-only P3 corpus to produce BLOOMZ and\nmT0 models.\nWe found that English-only finetuning suffices\nfor a multilingual pretrained large language model\nto generalize to tasks in other pretrained languages.\n\nHowever, finetuning on multiple languages using\nxP3 provided even better performance. We have\nfurther observed finetuned models to be capable\nof generalization to new tasks in languages they\nhave never intentionally seen. We investigated\nmultilingual prompting and found performance af-\nter finetuning on English prompts only to be poor.\nHowever, finetuning on a corpus with machine-\ntranslated prompts (xP3mt) lead to significantly\nbetter performance on human-written non-English\nprompts. Comparing models from 560 million up\nto 176 billion parameters revealed that the perfor-\nmance gap between only pretraining and finetuning\nwidens as parameters increase. Lastly, we found\nmultitask finetuning on billions of short targets bi-\nases models to produce short answers, which can\nhurt performance on generative tasks. We proposed\na simple workaround by forcing a minimum gener-\nation length at inference.\nTo contribute to future progress on improving\nzero-shot generalization, we release all datasets\nand models introduced in this work.\n6\nLimitations\nWe highlight several limitations of our work:\nUnnatural prompting format\nThe choice to\nseparate inputs and targets using a space charac-\nter has proven effective to multitask finetune our\ndecoder-only models. Nonetheless, poorly format-\nted prompts may result in undesirable behavior. For\nexample, given the following prompt: “Translate\nto English: Je t’aime\", the model may continue the\ninput with additional French content before start-\ning to solve the task, i.e. translating the input from\nFrench to English. This can be mitigated by im-\nproving the prompts with a trailing full stop or a\nnewline symbol. Encoder-decoder models, such as\nour mT0, do not suffer from this problem, as inputs\nand targets are fed into different parts of the model.\nLimited languages in xP3\nThe pretraining cor-\npus of mT0 contains more than 101 languages (Xue\net al., 2020), however, we finetune on only 46\nlanguages. Likely, finetuning on the full 101 lan-\nguages mT0 has seen during pretraining would lead\nto better performance. However, we decided to use\nonly the languages of BLOOM in order to study lan-\nguage generalization (§4.2). Similarly, one could\nlikely attain better performance by enhancing xP3\nwith more datasets, such as via BIG-Bench (Sri-\nvastava et al., 2022; Suzgun et al., 2022), or more\nprompts, such as via NL-Augmenter (Dhole et al.,\n2021). We have released an extended version of\nxP3 dubbed xP3x that covers 277 languages and\nis around ten times larger than xP3, but are yet to\nfinetune models on it.\nPerformance\nWhile our models show strong ca-\npabilities of performing tasks zero-shot, there re-\nmain numerous failure modes that are common\nin large language models (Rae et al., 2021; Bom-\nmasani et al., 2021; Zhang et al., 2022; Smith et al.,\n2022; Ouyang et al., 2022; Taylor et al., 2022;\nChowdhery et al., 2022; Biderman et al., 2023;\nAllal et al., 2023; Li et al., 2023). In Figure 16\nof Appendix §F, BLOOMZ fails to understand the\nmoral of a fable resulting in an undesirable genera-\ntion. Similarly, in Figure 15, mT0-13B is asked to\nprovide an explanation, but answers with a question.\nWe have made several modifications to the multi-\ntask finetuning recipe, such as loss weighting, mix-\ning in long tasks, and various multilingual aspects,\nleading to the strong zero-shot performance of our\nmodels. However, there are many other changes to\nthe multitask finetuning procedure that are worth\nexploring to get better models (Honovich et al.,\n2022; Wang et al., 2022b; Longpre et al., 2023a;\nLiu et al., 2023; Dettmers et al., 2023). Further,\nthe pre-trained models we use, BLOOM and mT5,\nare suboptimal in many aspects such as compute\nallocation (Hoffmann et al., 2022; Muennighoff\net al., 2023), pre-training datasets (Longpre et al.,\n2023b; Touvron et al., 2023; Chung et al., 2023),\npre-training objective (Tay et al., 2022b) and possi-\nbly model architecture (Komatsuzaki et al., 2022;\nShen et al., 2023). Future work should investigate\nmultitask finetuning better base models.\nLearning new languages during finetuning\nWhile we have investigated generalization to lan-\nguages only seen during pretraining, we did not\ninvestigate generalization to languages only seen\nduring finetuning. Our mT0 models are finetuned\non several new languages not seen in pretraining\n(see Figure 2). Out of those, we only evaluated on\ncode (HumanEval), where mT0 performed at the\nrandom baseline (0.00 in Table 10). We point to\nfollow-up work that has investigated the question\nof teaching BLOOMZ new languages (Yong et al.,\n2022; Cahyawijaya et al., 2023) and work investi-\ngating adaptation of BLOOM (Ennen et al., 2023;\nYong and Nikoulina, 2022).\n\nAcknowledgments\nThis work was granted access to the HPC resources\nof Institut du développement et des ressources en\ninformatique scientifique (IDRIS) du Centre na-\ntional de la recherche scientifique (CNRS) under\nthe allocation 2021-A0101012475 made by Grand\néquipement national de calcul intensif (GENCI). In\nparticular, all the evaluations and data processing\nran on the Jean Zay cluster of IDRIS, and we want\nto thank the IDRIS team for responsive support\nthroughout the project, in particular Rémi Lacroix.\nWe thank the XGLM team for providing access\nto XStoryCloze. We thank volunteers who human-\ntranslated XNLI prompts. We thank Noah Constant\nand Douwe Kiela for feedback on drafts of this\npaper. We thank Victor Sanh, Stephen Bach, Sasha\nRush and Jordan Clive for support throughout the\nproject.\nReferences\n2018. Neural code search evaluation dataset. page\narXiv:1908.09804 [cs.SE].\n2020. Wikilingua: A new benchmark dataset for mul-\ntilingual abstractive summarization. arXiv preprint\narXiv:2010.03093.\nLoubna Ben Allal, Raymond Li, Denis Kocetkov,\nChenghao Mou, Christopher Akiki, Carlos Munoz\nFerrandis, Niklas Muennighoff, Mayank Mishra,\nAlex Gu, Manan Dey, et al. 2023. Santacoder: don’t\nreach for the stars! arXiv preprint arXiv:2301.03988.\nVamsi Aribandi, Yi Tay, Tal Schuster, Jinfeng Rao,\nHuaixiu Steven Zheng, Sanket Vaibhav Mehta, Hon-\nglei Zhuang, Vinh Q. Tran, Dara Bahri, Jianmo\nNi, Jai Prakash Gupta, Kai Hui, Sebastian Ruder,\nand Donald Metzler. 2021.\nExt5: Towards ex-\ntreme multi-task scaling for transfer learning. CoRR,\nabs/2111.10952.\nMikel Artetxe, Sebastian Ruder, and Dani Yogatama.\n2019. On the cross-lingual transferability of mono-\nlingual representations. CoRR, abs/1910.11856.\nJacob Austin, Augustus Odena, Maxwell Nye, Maarten\nBosma, Henryk Michalewski, David Dohan, Ellen\nJiang, Carrie Cai, Michael Terry, Quoc Le, et al. 2021.\nProgram synthesis with large language models. arXiv\npreprint arXiv:2108.07732.\nStephen H. Bach, Victor Sanh, Zheng-Xin Yong, Albert\nWebson, Colin Raffel, Nihal V. Nayak, Abheesht\nSharma, Taewoon Kim, M Saiful Bari, Thibault\nFevry, Zaid Alyafeai, Manan Dey, Andrea San-\ntilli, Zhiqing Sun, Srulik Ben-David, Canwen Xu,\nGunjan Chhablani, Han Wang, Jason Alan Fries,\nMaged S. Al-shaibani, Shanya Sharma, Urmish\nThakker, Khalid Almubarak, Xiangru Tang, Xian-\ngru Tang, Mike Tian-Jian Jiang, and Alexander M.\nRush. 2022. Promptsource: An integrated develop-\nment environment and repository for natural language\nprompts.\nStella Biderman, Hailey Schoelkopf, Quentin Anthony,\nHerbie Bradley, Kyle O’Brien, Eric Hallahan, Mo-\nhammad Aflah Khan, Shivanshu Purohit, USVSN Sai\nPrashanth, Edward Raff, et al. 2023. Pythia: A suite\nfor analyzing large language models across training\nand scaling. arXiv preprint arXiv:2304.01373.\nSid Black, Stella Biderman, Eric Hallahan, Quentin\nAnthony, Leo Gao, Laurence Golding, Horace He,\nConnor Leahy, Kyle McDonell, Jason Phang, et al.\n2022. Gpt-neox-20b: An open-source autoregressive\nlanguage model. arXiv preprint arXiv:2204.06745.\nSid Black, Leo Gao, Phil Wang, Connor Leahy, and\nStella Biderman. 2021. Gpt-neo: Large scale autore-\ngressive language modeling with mesh-tensorflow.\nIf you use this software, please cite it using these\nmetadata, 58.\nRishi Bommasani, Drew A Hudson, Ehsan Adeli,\nRuss Altman, Simran Arora, Sydney von Arx,\nMichael S Bernstein, Jeannette Bohg, Antoine Bosse-\nlut, Emma Brunskill, et al. 2021. On the opportuni-\nties and risks of foundation models. arXiv preprint\narXiv:2108.07258.\nTom Brown, Benjamin Mann, Nick Ryder, Melanie\nSubbiah, Jared D Kaplan, Prafulla Dhariwal, Arvind\nNeelakantan, Pranav Shyam, Girish Sastry, Amanda\nAskell, Sandhini Agarwal, Ariel Herbert-Voss,\nGretchen Krueger, Tom Henighan, Rewon Child,\nAditya Ramesh, Daniel Ziegler, Jeffrey Wu, Clemens\nWinter, Chris Hesse, Mark Chen, Eric Sigler, Ma-\nteusz Litwin, Scott Gray, Benjamin Chess, Jack\nClark, Christopher Berner, Sam McCandlish, Alec\nRadford, Ilya Sutskever, and Dario Amodei. 2020.\nLanguage models are few-shot learners.\nIn Ad-\nvances in Neural Information Processing Systems,\nvolume 33, pages 1877–1901. Curran Associates,\nInc.\nSamuel Cahyawijaya, Holy Lovenia, Tiezheng Yu,\nWilly Chung, and Pascale Fung. 2023.\nInstruct-\nalign: Teaching novel languages with to llms through\nalignment-based cross-lingual instruction.\narXiv\npreprint arXiv:2305.13627.\nIlias Chalkidis, Manos Fergadiotis, and Ion Androut-\nsopoulos. 2021.\nMultieurlex–a multi-lingual and\nmulti-label legal document classification dataset for\nzero-shot cross-lingual transfer.\narXiv preprint\narXiv:2109.00904.\nMark Chen, Jerry Tworek, Heewoo Jun, Qiming\nYuan, Henrique Ponde de Oliveira Pinto, Jared Ka-\nplan, Harri Edwards, Yuri Burda, Nicholas Joseph,\nGreg Brockman, et al. 2021.\nEvaluating large\nlanguage models trained on code. arXiv preprint\narXiv:2107.03374.\n\nAakanksha Chowdhery, Sharan Narang, Jacob Devlin,\nMaarten Bosma, Gaurav Mishra, Adam Roberts,\nPaul Barham, Hyung Won Chung, Charles Sutton,\nSebastian Gehrmann, et al. 2022. Palm: Scaling\nlanguage modeling with pathways. arXiv preprint\narXiv:2204.02311.\nHyung Won Chung, Noah Constant, Xavier Garcia,\nAdam Roberts, Yi Tay, Sharan Narang, and Orhan\nFirat. 2023. Unimax: Fairer and more effective lan-\nguage sampling for large-scale multilingual pretrain-\ning. arXiv preprint arXiv:2304.09151.\nHyung Won Chung, Le Hou, Shayne Longpre, Barret\nZoph, Yi Tay, William Fedus, Eric Li, Xuezhi Wang,\nMostafa Dehghani, Siddhartha Brahma, Albert Web-\nson, Shixiang Shane Gu, Zhuyun Dai, Mirac Suz-\ngun, Xinyun Chen, Aakanksha Chowdhery, Sharan\nNarang, Gaurav Mishra, Adams Yu, Vincent Zhao,\nYanping Huang, Andrew Dai, Hongkun Yu, Slav\nPetrov, Ed H. Chi, Jeff Dean, Jacob Devlin, Adam\nRoberts, Denny Zhou, Quoc V. Le, and Jason Wei.\n2022. Scaling instruction-finetuned language models.\narXiv preprint arXiv:2210.11416.\nAlexis Conneau, Kartikay Khandelwal, Naman Goyal,\nVishrav Chaudhary, Guillaume Wenzek, Francisco\nGuzmán, Edouard Grave, Myle Ott, Luke Zettle-\nmoyer, and Veselin Stoyanov. 2019. Unsupervised\ncross-lingual representation learning at scale. arXiv\npreprint arXiv:1911.02116.\nAlexis Conneau, Ruty Rinott, Guillaume Lample, Ad-\nina Williams, Samuel R. Bowman, Holger Schwenk,\nand Veselin Stoyanov. 2018. Xnli: Evaluating cross-\nlingual sentence representations. In Proceedings of\nthe 2018 Conference on Empirical Methods in Natu-\nral Language Processing. Association for Computa-\ntional Linguistics.\nYiming Cui, Ting Liu, Li Xiao, Zhipeng Chen, Wen-\ntao Ma, Wanxiang Che, Shijin Wang, and Guoping\nHu. 2018.\nA span-extraction dataset for chinese\nmachine reading comprehension.\narXiv preprint\narXiv:1810.07366.\nTim Dettmers, Artidoro Pagnoni, Ari Holtzman, and\nLuke Zettlemoyer. 2023. Qlora: Efficient finetuning\nof quantized llms. arXiv preprint arXiv:2305.14314.\nJacob Devlin, Ming-Wei Chang, Kenton Lee, and\nKristina Toutanova. 2018. Bert: Pre-training of deep\nbidirectional transformers for language understand-\ning. arXiv preprint arXiv:1810.04805.\nKaustubh D Dhole, Varun Gangal, Sebastian Gehrmann,\nAadesh Gupta, Zhenhao Li, Saad Mahamood, Abi-\nnaya Mahendiran, Simon Mille, Ashish Srivastava,\nSamson Tan, et al. 2021. Nl-augmenter: A frame-\nwork for task-sensitive natural language augmenta-\ntion. arXiv preprint arXiv:2112.02721.\nNing Ding, Yujia Qin, Guang Yang, Fuchao Wei, Zong-\nhan Yang, Yusheng Su, Shengding Hu, Yulin Chen,\nChi-Min Chan, Weize Chen, et al. 2022. Delta tuning:\nA comprehensive study of parameter efficient meth-\nods for pre-trained language models. arXiv preprint\narXiv:2203.06904.\nPhilipp Ennen, Po-Chun Hsu, Chan-Jan Hsu, Chang-Le\nLiu, Yen-Chen Wu, Yin-Hsiang Liao, Chin-Tung Lin,\nDa-Shan Shiu, and Wei-Yun Ma. 2023. Extending\nthe pre-training of bloom for improved support of tra-\nditional chinese: Models, methods and results. arXiv\npreprint arXiv:2303.04715.\nAngela Fan, Shruti Bhosale, Holger Schwenk, Zhiyi\nMa, Ahmed El-Kishky, Siddharth Goyal, Mandeep\nBaines, Onur Celebi, Guillaume Wenzek, Vishrav\nChaudhary, et al. 2021. Beyond english-centric mul-\ntilingual machine translation. J. Mach. Learn. Res.,\n22(107):1–48.\nDaniel Fried, Armen Aghajanyan, Jessy Lin, Sida Wang,\nEric Wallace, Freda Shi, Ruiqi Zhong, Wen-tau Yih,\nLuke Zettlemoyer, and Mike Lewis. 2022. Incoder:\nA generative model for code infilling and synthesis.\narXiv preprint arXiv:2204.05999.\nJason Alan Fries, Leon Weber, Natasha Seelam, Gabriel\nAltay, Debajyoti Datta, Samuele Garda, Myungsun\nKang, Ruisi Su, Wojciech Kusa, Samuel Cahyaw-\nijaya, et al. 2022.\nBigbio:\nA framework for\ndata-centric biomedical natural language processing.\narXiv preprint arXiv:2206.15076.\nLeo Gao, Jonathan Tow, Stella Biderman, Sid Black,\nAnthony DiPofi, Charles Foster, Laurence Golding,\nJeffrey Hsu, Kyle McDonell, Niklas Muennighoff,\nJason Phang, Laria Reynolds, Eric Tang, Anish Thite,\nBen Wang, Kevin Wang, and Andy Zou. 2021. A\nframework for few-shot language model evaluation.\nNaman Goyal, Jingfei Du, Myle Ott, Giri Ananthara-\nman, and Alexis Conneau. 2021a. Larger-scale trans-\nformers for multilingual masked language modeling.\narXiv preprint arXiv:2105.00572.\nNaman Goyal, Cynthia Gao, Vishrav Chaudhary, Peng-\nJen Chen, Guillaume Wenzek, Da Ju, Sanjana Kr-\nishnan, Marc’Aurelio Ranzato, Francisco Guzm’an,\nand Angela Fan. 2021b. The flores-101 evaluation\nbenchmark for low-resource and multilingual ma-\nchine translation.\nFrancisco Guzm’an, Peng-Jen Chen, Myle Ott, Juan\nPino, Guillaume Lample, Philipp Koehn, Vishrav\nChaudhary, and Marc’Aurelio Ranzato. 2019. Two\nnew evaluation datasets for low-resource machine\ntranslation: Nepali-english and sinhala-english.\nTahmid Hasan, Abhik Bhattacharjee, Md. Saiful Is-\nlam, Kazi Mubasshir, Yuan-Fang Li, Yong-Bin Kang,\nM. Sohel Rahman, and Rifat Shahriyar. 2021. XL-\nsum: Large-scale multilingual abstractive summariza-\ntion for 44 languages. In Findings of the Association\nfor Computational Linguistics: ACL-IJCNLP 2021,\npages 4693–4703, Online. Association for Computa-\ntional Linguistics.\n\nVincent J. Hellendoorn, Charles Sutton, Rishabh Singh,\nPetros Maniatis, and David Bieber. 2020. Global re-\nlational models of source code. In 8th International\nConference on Learning Representations, ICLR 2020,\nAddis Ababa, Ethiopia, April 26-30, 2020. OpenRe-\nview.net.\nDan Hendrycks, Steven Basart, Saurav Kadavath, Man-\ntas Mazeika, Akul Arora, Ethan Guo, Collin Burns,\nSamir Puranik, Horace He, Dawn Song, and Jacob\nSteinhardt. 2021. Measuring coding challenge com-\npetence with apps. NeurIPS.\nJordan Hoffmann, Sebastian Borgeaud, Arthur Men-\nsch, Elena Buchatskaya, Trevor Cai, Eliza Ruther-\nford, Diego de Las Casas, Lisa Anne Hendricks,\nJohannes Welbl, Aidan Clark, et al. 2022. Train-\ning compute-optimal large language models. arXiv\npreprint arXiv:2203.15556.\nOr Honovich, Thomas Scialom, Omer Levy, and Timo\nSchick. 2022. Unnatural instructions: Tuning lan-\nguage models with (almost) no human labor. arXiv\npreprint arXiv:2212.09689.\nEdward J Hu, Yelong Shen, Phillip Wallis, Zeyuan\nAllen-Zhu, Yuanzhi Li, Shean Wang, Lu Wang,\nand Weizhu Chen. 2021.\nLora: Low-rank adap-\ntation of large language models.\narXiv preprint\narXiv:2106.09685.\nSrinivasan Iyer, Xi Victoria Lin, Ramakanth Pasunuru,\nTodor Mihaylov, Dániel Simig, Ping Yu, Kurt Shus-\nter, Tianlu Wang, Qing Liu, Punit Singh Koura, et al.\n2022.\nOpt-iml: Scaling language model instruc-\ntion meta learning through the lens of generalization.\narXiv preprint arXiv:2212.12017.\nJoongwon Kim, Mounica Maddela, Reno Kriz, Wei Xu,\nand Chris Callison-Burch. 2021. BiSECT: Learning\nto split and rephrase sentences with bitexts. In Pro-\nceedings of the 2021 Conference on Empirical Meth-\nods in Natural Language Processing, pages 6193–\n6209, Online and Punta Cana, Dominican Republic.\nAssociation for Computational Linguistics.\nAran Komatsuzaki, Joan Puigcerver, James Lee-Thorp,\nCarlos Riquelme Ruiz, Basil Mustafa, Joshua Ainslie,\nYi Tay, Mostafa Dehghani, and Neil Houlsby.\n2022.\nSparse upcycling:\nTraining mixture-of-\nexperts from dense checkpoints.\narXiv preprint\narXiv:2212.05055.\nMatej Kosec, Sheng Fu, and Mario Michael Krell. 2021.\nPacking: Towards 2x nlp bert acceleration. arXiv\npreprint arXiv:2107.02027.\nGuillaume Lample and Alexis Conneau. 2019. Cross-\nlingual language model pretraining.\nHugo Laurençon, Lucile Saulnier, Thomas Wang,\nChristopher Akiki, Albert Villanova del Moral, Teven\nLe Scao, Leandro Von Werra, Chenghao Mou, Ed-\nuardo González Ponferrada, Huu Nguyen, et al. 2022.\nThe bigscience roots corpus: A 1.6 tb composite\nmultilingual dataset. In Thirty-sixth Conference on\nNeural Information Processing Systems Datasets and\nBenchmarks Track.\nVladimir I Levenshtein et al. 1966. Binary codes capa-\nble of correcting deletions, insertions, and reversals.\nIn Soviet physics doklady, volume 10, pages 707–710.\nSoviet Union.\nPatrick Lewis, Barlas Oguz, Ruty Rinott, Sebastian\nRiedel, and Holger Schwenk. 2019. Mlqa: Eval-\nuating cross-lingual extractive question answering.\narXiv preprint arXiv:1910.07475.\nRaymond Li, Loubna Ben Allal, Yangtian Zi, Niklas\nMuennighoff, Denis Kocetkov, Chenghao Mou, Marc\nMarone, Christopher Akiki, Jia Li, Jenny Chim, et al.\n2023. Starcoder: may the source be with you! arXiv\npreprint arXiv:2305.06161.\nXi Victoria Lin, Todor Mihaylov, Mikel Artetxe, Tianlu\nWang, Shuohui Chen, Daniel Simig, Myle Ott, Na-\nman Goyal, Shruti Bhosale, Jingfei Du, et al. 2021.\nFew-shot learning with multilingual language models.\narXiv preprint arXiv:2112.10668.\nHaokun Liu, Derek Tam, Mohammed Muqeeth, Jay Mo-\nhta, Tenghao Huang, Mohit Bansal, and Colin Raffel.\n2022. Few-shot parameter-efficient fine-tuning is\nbetter and cheaper than in-context learning. arXiv\npreprint arXiv:2205.05638.\nQian Liu, Fan Zhou, Zhengbao Jiang, Longxu Dou, and\nMin Lin. 2023. From zero to hero: Examining the\npower of symbolic tasks in instruction tuning. arXiv\npreprint arXiv:2304.07995.\nYinhan Liu, Jiatao Gu, Naman Goyal, Xian Li, Sergey\nEdunov, Marjan Ghazvininejad, Mike Lewis, and\nLuke Zettlemoyer. 2020. Multilingual denoising pre-\ntraining for neural machine translation.\nTransac-\ntions of the Association for Computational Linguis-\ntics, 8:726–742.\nRobert L Logan, Ivana Balaževi´c, Eric Wallace, Fabio\nPetroni, Sameer Singh, and Sebastian Riedel. 2021.\nCutting down on prompts and parameters: Sim-\nple few-shot learning with language models. arXiv\npreprint arXiv:2106.13353.\nShayne Longpre, Le Hou, Tu Vu, Albert Webson,\nHyung Won Chung, Yi Tay, Denny Zhou, Quoc V\nLe, Barret Zoph, Jason Wei, et al. 2023a. The flan\ncollection: Designing data and methods for effective\ninstruction tuning. arXiv preprint arXiv:2301.13688.\nShayne Longpre, Gregory Yauney, Emily Reif, Kather-\nine Lee, Adam Roberts, Barret Zoph, Denny Zhou,\nJason Wei, Kevin Robinson, David Mimno, et al.\n2023b. A pretrainer’s guide to training data: Measur-\ning the effects of data age, domain coverage, quality,\n& toxicity. arXiv preprint arXiv:2305.13169.\nSewon Min, Mike Lewis, Luke Zettlemoyer, and Han-\nnaneh Hajishirzi. 2021. Metaicl: Learning to learn in\ncontext. arXiv preprint arXiv:2110.15943.\n\nSwaroop Mishra, Daniel Khashabi, Chitta Baral, and\nHannaneh Hajishirzi. 2021a. Cross-task generaliza-\ntion via natural language crowdsourcing instructions.\narXiv preprint arXiv:2104.08773.\nSwaroop Mishra, Daniel Khashabi, Chitta Baral, and\nHannaneh Hajishirzi. 2021b. Natural instructions:\nBenchmarking generalization to new tasks from nat-\nural language instructions. CoRR, abs/2104.08773.\nNiklas Muennighoff. 2022.\nSgpt:\nGpt sentence\nembeddings for semantic search.\narXiv preprint\narXiv:2202.08904.\nNiklas Muennighoff, Alexander M. Rush, Boaz Barak,\nTeven Le Scao, Aleksandra Piktus, Nouamane Tazi,\nSampo Pyysalo, Thomas Wolf, and Colin Raffel.\n2023. Scaling data-constrained language models.\nNiklas Muennighoff, Nouamane Tazi, Loïc Magne, and\nNils Reimers. 2022. Mteb: Massive text embedding\nbenchmark. arXiv preprint arXiv:2210.07316.\nNLLB Team, Marta R. Costa-jussà, James Cross, Onur\nÇelebi, Maha Elbayad, Kenneth Heafield, Kevin Hef-\nfernan, Elahe Kalbassi, Janice Lam, Daniel Licht,\nJean Maillard, Anna Sun, Skyler Wang, Guillaume\nWenzek, Al Youngblood, Bapi Akula, Loic Bar-\nrault, Gabriel Mejia Gonzalez, Prangthip Hansanti,\nJohn Hoffman, Semarley Jarrett, Kaushik Ram\nSadagopan, Dirk Rowe, Shannon Spruit, Chau\nTran, Pierre Andrews, Necip Fazil Ayan, Shruti\nBhosale, Sergey Edunov, Angela Fan, Cynthia\nGao, Vedanuj Goswami, Francisco Guzmán, Philipp\nKoehn, Alexandre Mourachko, Christophe Rop-\ners, Safiyyah Saleem, Holger Schwenk, and Jeff\nWang. 2022.\nNo language left behind: Scaling\nhuman-centered machine translation. arXiv preprint\n2207.04672.\nLong Ouyang, Jeff Wu, Xu Jiang, Diogo Almeida, Car-\nroll L Wainwright, Pamela Mishkin, Chong Zhang,\nSandhini Agarwal, Katarina Slama, Alex Ray, et al.\n2022.\nTraining language models to follow in-\nstructions with human feedback.\narXiv preprint\narXiv:2203.02155.\nKishore Papineni, Salim Roukos, Todd Ward, and Wei-\nJing Zhu. 2002. Bleu: a method for automatic evalu-\nation of machine translation. In Proceedings of the\n40th annual meeting of the Association for Computa-\ntional Linguistics, pages 311–318.\nAjay Patel, Bryan Li, Mohammad Sadegh Rasooli,\nNoah Constant, Colin Raffel, and Chris Callison-\nBurch. 2022. Bidirectional language models are also\nfew-shot learners. arXiv preprint arXiv:2209.14500.\nEthan Perez, Douwe Kiela, and Kyunghyun Cho. 2021a.\nTrue few-shot learning with language models. Ad-\nvances in Neural Information Processing Systems,\n34:11054–11070.\nEthan Perez, Douwe Kiela, and Kyunghyun Cho. 2021b.\nTrue few-shot learning with language models. CoRR,\nabs/2105.11447.\nJason Phang, Iacer Calixto, Phu Mon Htut, Yada Pruk-\nsachatkun, Haokun Liu, Clara Vania, Katharina Kann,\nand Samuel R Bowman. 2020. English intermediate-\ntask training improves zero-shot cross-lingual trans-\nfer too. arXiv preprint arXiv:2005.13013.\nEdoardo M. Ponti, Goran Glavas, Olga Majewska,\nQianchu Liu, Ivan Vuli’c, and Anna Korhonen. 2020.\nXCOPA: A multilingual dataset for causal common-\nsense reasoning. arXiv preprint.\nAlec Radford, Jeffrey Wu, Rewon Child, David Luan,\nDario Amodei, Ilya Sutskever, et al. 2019. Language\nmodels are unsupervised multitask learners. OpenAI\nblog, 1(8):9.\nJack W Rae, Sebastian Borgeaud, Trevor Cai, Katie\nMillican, Jordan Hoffmann, Francis Song, John\nAslanides, Sarah Henderson, Roman Ring, Susan-\nnah Young, et al. 2021. Scaling language models:\nMethods, analysis & insights from training gopher.\narXiv preprint arXiv:2112.11446.\nColin Raffel, Noam Shazeer, Adam Roberts, Katherine\nLee, Sharan Narang, Michael Matena, Yanqi Zhou,\nWei Li, Peter J Liu, et al. 2020. Exploring the limits\nof transfer learning with a unified text-to-text trans-\nformer. J. Mach. Learn. Res., 21(140):1–67.\nAlessandro Raganato, Tommaso Pasini, Jose Camacho-\nCollados, and Mohammad Taher Pilehvar. 2020. Xl-\nwic: A multilingual benchmark for evaluating seman-\ntic contextualization. In Proceedings of the 2020\nConference on Empirical Methods in Natural Lan-\nguage Processing (EMNLP), pages 7193–7206.\nAdam Roberts, Hyung Won Chung, Anselm Levskaya,\nGaurav Mishra, James Bradbury, Daniel Andor, Sha-\nran Narang, Brian Lester, Colin Gaffney, Afroz\nMohiuddin, Curtis Hawthorne, Aitor Lewkowycz,\nAlex Salcianu, Marc van Zee, Jacob Austin, Se-\nbastian Goodman, Livio Baldini Soares, Haitang\nHu, Sasha Tsvyashchenko, Aakanksha Chowdh-\nery, Jasmijn Bastings, Jannis Bulian, Xavier Gar-\ncia, Jianmo Ni, Andrew Chen, Kathleen Kenealy,\nJonathan H. Clark, Stephan Lee, Dan Garrette, James\nLee-Thorp, Colin Raffel, Noam Shazeer, Marvin\nRitter, Maarten Bosma, Alexandre Passos, Jeremy\nMaitin-Shepard, Noah Fiedel, Mark Omernick, Bren-\nnan Saeta, Ryan Sepassi, Alexander Spiridonov,\nJoshua Newlan, and Andrea Gesmundo. 2022. Scal-\ning up models and data with t5x and seqio. arXiv\npreprint arXiv:2203.17189.\nMelissa Roemmele, Cosmin Adrian Bejan, and An-\ndrew S Gordon. 2011.\nChoice of plausible alter-\nnatives: An evaluation of commonsense causal rea-\nsoning. In 2011 AAAI Spring Symposium Series.\nVictor Sanh, Albert Webson, Colin Raffel, Stephen\nBach, Lintang Sutawika, Zaid Alyafeai, Antoine\nChaffin, Arnaud Stiegler, Teven Le Scao, Arun Raja,\net al. 2022. Multitask prompted training enables zero-\nshot task generalization. In The Tenth International\nConference on Learning Representations.\n\nTeven Le Scao, Angela Fan, Christopher Akiki, El-\nlie Pavlick, Suzana Ili´c, Daniel Hesslow, Roman\nCastagné, Alexandra Sasha Luccioni, François Yvon,\nMatthias Gallé, et al. 2022a.\nBloom: A 176b-\nparameter open-access multilingual language model.\narXiv preprint arXiv:2211.05100.\nTeven Le Scao and Alexander M Rush. 2021. How\nmany data points is a prompt worth? arXiv preprint\narXiv:2103.08493.\nTeven Le Scao, Thomas Wang, Daniel Hesslow, Lu-\ncile Saulnier, Stas Bekman, M Saiful Bari, Stella\nBideman, Hady Elsahar, Niklas Muennighoff, Jason\nPhang, et al. 2022b. What language model to train\nif you have one million gpu hours? arXiv preprint\narXiv:2210.15424.\nTimo Schick and Hinrich Schütze. 2020.\nExploit-\ning cloze questions for few shot text classification\nand natural language inference.\narXiv preprint\narXiv:2001.07676.\nTimo Schick and Hinrich Schütze. 2020. Exploiting\ncloze questions for few-shot text classification and\nnatural language inference. CoRR, abs/2001.07676.\nThomas Scialom, Tuhin Chakrabarty, and Smaranda\nMuresan. 2022. Continual-t0: Progressively instruct-\ning 50+ tasks to language models without forgetting.\narXiv preprint arXiv:2205.12393.\nSheng Shen, Le Hou, Yanqi Zhou, Nan Du, Shayne\nLongpre, Jason Wei, Hyung Won Chung, Barret\nZoph, William Fedus, Xinyun Chen, et al. 2023.\nFlan-moe: Scaling instruction-finetuned language\nmodels with sparse mixture of experts. arXiv preprint\narXiv:2305.14705.\nOleh Shliazhko, Alena Fenogenova, Maria Tikhonova,\nVladislav Mikhailov, Anastasia Kozlova, and Tatiana\nShavrina. 2022. mgpt: Few-shot learners go multilin-\ngual. arXiv preprint arXiv:2204.07580.\nShaden Smith, Mostofa Patwary, Brandon Norick,\nPatrick LeGresley, Samyam Rajbhandari, Jared\nCasper, Zhun Liu, Shrimai Prabhumoye, George\nZerveas, Vijay Korthikanti, et al. 2022. Using deep-\nspeed and megatron to train megatron-turing nlg\n530b, a large-scale generative language model. arXiv\npreprint arXiv:2201.11990.\nSaleh Soltan, Shankar Ananthakrishnan, Jack FitzGer-\nald, Rahul Gupta, Wael Hamza, Haidar Khan, Charith\nPeris, Stephen Rawls, Andy Rosenbaum, Anna\nRumshisky, Chandana Satya Prakash, Mukund Srid-\nhar, Fabian Triefenbach, Apurv Verma, Gokhan Tur,\nand Prem Natarajan. 2022. Alexatm 20b: Few-shot\nlearning using a large-scale multilingual seq2seq\nmodel.\nAarohi Srivastava, Abhinav Rastogi, Abhishek Rao,\nAbu Awal Md Shoeb, Abubakar Abid, Adam Fisch,\nAdam R Brown, Adam Santoro, Aditya Gupta,\nAdrià Garriga-Alonso, et al. 2022.\nBeyond the\nimitation game: Quantifying and extrapolating the\ncapabilities of language models.\narXiv preprint\narXiv:2206.04615.\nHui Su, Xiao Zhou, Houjing Yu, Yuwen Chen, Zilin\nZhu, Yang Yu, and Jie Zhou. 2022. Welm: A well-\nread pre-trained language model for chinese. arXiv\npreprint arXiv:2209.10372.\nKai Sun, Dian Yu, Dong Yu, and Claire Cardie. 2020.\nInvestigating prior knowledge for challenging chi-\nnese machine reading comprehension. Trans. Assoc.\nComput. Linguistics, 8:141–155.\nMirac Suzgun, Nathan Scales, Nathanael Schärli, Se-\nbastian Gehrmann, Yi Tay, Hyung Won Chung,\nAakanksha Chowdhery, Quoc V Le, Ed H Chi, Denny\nZhou, et al. 2022. Challenging big-bench tasks and\nwhether chain-of-thought can solve them.\narXiv\npreprint arXiv:2210.09261.\nYi Tay, Mostafa Dehghani, Vinh Q Tran, Xavier Gar-\ncia, Dara Bahri, Tal Schuster, Huaixiu Steven Zheng,\nNeil Houlsby, and Donald Metzler. 2022a. Unify-\ning language learning paradigms.\narXiv preprint\narXiv:2205.05131.\nYi Tay, Mostafa Dehghani, Vinh Q Tran, Xavier Gar-\ncia, Jason Wei, Xuezhi Wang, Hyung Won Chung,\nDara Bahri, Tal Schuster, Steven Zheng, et al. 2022b.\nUl2: Unifying language learning paradigms. In The\nEleventh International Conference on Learning Rep-\nresentations.\nYi Tay, Jason Wei, Hyung Won Chung, Vinh Q\nTran, David R So, Siamak Shakeri, Xavier Gar-\ncia, Huaixiu Steven Zheng, Jinfeng Rao, Aakanksha\nChowdhery, et al. 2022c.\nTranscending scaling\nlaws with 0.1% extra compute.\narXiv preprint\narXiv:2210.11399.\nRoss Taylor, Marcin Kardas, Guillem Cucurull, Thomas\nScialom, Anthony Hartshorn, Elvis Saravia, Andrew\nPoulton, Viktor Kerkez, and Robert Stojnic. 2022.\nGalactica: A large language model for science. arXiv\npreprint arXiv:2211.09085.\nJ\"org Tiedemann. 2020. The Tatoeba Translation Chal-\nlenge – Realistic data sets for low resource and mul-\ntilingual MT. In Proceedings of the Fifth Conference\non Machine Translation, pages 1174–1182. Associa-\ntion for Computational Linguistics.\nAlexey Tikhonov and Max Ryabinin. 2021. It’s all in\nthe heads: Using attention heads as a baseline for\ncross-lingual transfer in commonsense reasoning.\nHugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier\nMartinet, Marie-Anne Lachaux, Timothée Lacroix,\nBaptiste Rozière, Naman Goyal, Eric Hambro,\nFaisal Azhar, et al. 2023. Llama: Open and effi-\ncient foundation language models. arXiv preprint\narXiv:2302.13971.\nAshish Vaswani, Noam Shazeer, Niki Parmar, Jakob\nUszkoreit, Llion Jones, Aidan N Gomez, Łukasz\nKaiser, and Illia Polosukhin. 2017. Attention is all\n\nyou need. Advances in neural information processing\nsystems, 30.\nTu Vu, Aditya Barua, Brian Lester, Daniel Cer, Mo-\nhit Iyyer, and Noah Constant. 2022. Overcoming\ncatastrophic forgetting in zero-shot cross-lingual gen-\neration. arXiv preprint arXiv:2205.12647.\nBen Wang and Aran Komatsuzaki. 2021. Gpt-j-6b: A 6\nbillion parameter autoregressive language model.\nThomas Wang,\nAdam Roberts,\nDaniel Hesslow,\nTeven Le Scao, Hyung Won Chung, Iz Beltagy, Julien\nLaunay, and Colin Raffel. 2022a. What language\nmodel architecture and pretraining objective work\nbest for zero-shot generalization?\narXiv preprint\narXiv:2204.05832.\nYizhong Wang, Yeganeh Kordi, Swaroop Mishra, Al-\nisa Liu, Noah A Smith, Daniel Khashabi, and Han-\nnaneh Hajishirzi. 2022b. Self-instruct: Aligning lan-\nguage model with self generated instructions. arXiv\npreprint arXiv:2212.10560.\nYizhong Wang, Swaroop Mishra, Pegah Alipoor-\nmolabashi,\nYeganeh Kordi,\nAmirreza Mirzaei,\nAnjana Arunkumar, Arjun Ashok, Arut Selvan\nDhanasekaran, Atharva Naik, David Stap, Eshaan\nPathak, Giannis Karamanolakis, Haizhi Gary Lai, Is-\nhan Purohit, Ishani Mondal, Jacob Anderson, Kirby\nKuznia, Krima Doshi, Maitreya Patel, Kuntal Ku-\nmar Pal, Mehrad Moradshahi, Mihir Parmar, Mi-\nrali Purohit, Neeraj Varshney, Phani Rohitha Kaza,\nPulkit Verma, Ravsehaj Singh Puri, Rushang Karia,\nShailaja Keyur Sampat, Savan Doshi, Siddhartha\nMishra, Sujan Reddy, Sumanta Patro, Tanay Dixit,\nXudong Shen, Chitta Baral, Yejin Choi, Noah A.\nSmith, Hannaneh Hajishirzi, and Daniel Khashabi.\n2022c. Super-naturalinstructions: Generalization via\ndeclarative instructions on 1600+ nlp tasks. arXiv\npreprint arXiv:2204.07705.\nZhenhailong Wang, Xiaoman Pan, Dian Yu, Dong Yu,\nJianshu Chen, and Heng Ji. 2022d. Zemi: Learn-\ning zero-shot semi-parametric language models from\nmultiple tasks. arXiv preprint arXiv:2210.00185.\nAlbert Webson and Ellie Pavlick. 2021. Do prompt-\nbased models really understand the meaning of their\nprompts?\nJason Wei, Maarten Bosma, Vincent Y Zhao, Kelvin\nGuu, Adams Wei Yu, Brian Lester, Nan Du, An-\ndrew M Dai, and Quoc V Le. 2021. Finetuned lan-\nguage models are zero-shot learners. arXiv preprint\narXiv:2109.01652.\nAdina Williams, Nikita Nangia, and Samuel Bowman.\n2018. A broad-coverage challenge corpus for sen-\ntence understanding through inference. In Proceed-\nings of the 2018 Conference of the North American\nChapter of the Association for Computational Lin-\nguistics: Human Language Technologies, Volume 1\n(Long Papers), pages 1112–1122. Association for\nComputational Linguistics.\nShijie Wu and Mark Dredze. 2019. Beto, bentz, becas:\nThe surprising cross-lingual effectiveness of BERT.\nIn Proceedings of the 2019 Conference on Empirical\nMethods in Natural Language Processing and the 9th\nInternational Joint Conference on Natural Language\nProcessing (EMNLP-IJCNLP), pages 833–844, Hong\nKong, China. Association for Computational Linguis-\ntics.\nLiang Xu, Hai Hu, Xuanwei Zhang, Lu Li, Chenjie\nCao, Yudong Li, Yechen Xu, Kai Sun, Dian Yu,\nCong Yu, et al. 2020. Clue: A chinese language\nunderstanding evaluation benchmark. arXiv preprint\narXiv:2004.05986.\nLinting Xue, Noah Constant, Adam Roberts, Mihir Kale,\nRami Al-Rfou, Aditya Siddhant, Aditya Barua, and\nColin Raffel. 2020. mt5: A massively multilingual\npre-trained text-to-text transformer. arXiv preprint\narXiv:2010.11934.\nYinfei Yang, Yuan Zhang, Chris Tar, and Jason\nBaldridge. 2019. PAWS-X: A Cross-lingual Adver-\nsarial Dataset for Paraphrase Identification. In Proc.\nof EMNLP.\nZheng-Xin Yong and Vassilina Nikoulina. 2022. Adapt-\ning bigscience multilingual model to unseen lan-\nguages. arXiv preprint arXiv:2204.04873.\nZheng-Xin Yong, Hailey Schoelkopf, Niklas Muen-\nnighoff, Alham Fikri Aji, David Ifeoluwa Adelani,\nKhalid Almubarak, M Saiful Bari, Lintang Sutawika,\nJungo Kasai, Ahmed Baruwa, et al. 2022. Bloom+\n1: Adding language support to bloom for zero-shot\nprompting. arXiv preprint arXiv:2212.09535.\nElad Ben Zaken, Shauli Ravfogel, and Yoav Gold-\nberg. 2021.\nBitfit:\nSimple parameter-efficient\nfine-tuning for transformer-based masked language-\nmodels. arXiv preprint arXiv:2106.10199.\nAohan Zeng, Xiao Liu, Zhengxiao Du, Zihan Wang,\nHanyu Lai, Ming Ding, Zhuoyi Yang, Yifan Xu,\nWendi Zheng, Xiao Xia, et al. 2022.\nGlm-130b:\nAn open bilingual pre-trained model. arXiv preprint\narXiv:2210.02414.\nSusan Zhang, Stephen Roller, Naman Goyal, Mikel\nArtetxe, Moya Chen, Shuohui Chen, Christopher De-\nwan, Mona Diab, Xian Li, Xi Victoria Lin, et al. 2022.\nOpt: Open pre-trained transformer language models.\narXiv preprint arXiv:2205.01068.\nYuan Zhang, Jason Baldridge, and Luheng He. 2019.\nPaws: Paraphrase adversaries from word scrambling.\narXiv preprint arXiv:1904.01130.\nRuiqi Zhong, Kristy Lee, Zheng Zhang, and Dan Klein.\n2021.\nMeta-tuning language models to answer\nprompts better. CoRR, abs/2104.04670.\nMing Zhu, Aneesh Jain, Karthik Suresh, Roshan Ravin-\ndran, Sindhu Tipirneni, and Chandan K. Reddy. 2022.\nXlcost: A benchmark dataset for cross-lingual code\nintelligence.\n\nContents\n1\nIntroduction\n1\n2\nRelated work\n2\n2.1\nMultitask learning . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n2\n2.2\nMultilingual models . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n3\n3\nFinetuning data and models\n4\n3.1\nFinetuning data . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n4\n3.2\nModels\n. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n4\n4\nResults\n5\n4.1\nTask generalization . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n5\n4.2\nLanguage generalization\n. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n6\n4.3\nMultilingual prompting . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n6\n4.4\nScaling\n. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n7\n4.5\nGeneration tasks . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n7\n4.6\nEffect of language proportions . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n8\n5\nConclusion\n8\n6\nLimitations\n9\nA Contributions\n17\nB\nTask generalization breakdown\n17\nC Artifacts\n19\nD ROOTS language contamination\n19\nE\nCode generations\n20\nF\nQualitative examples\n20\nG Increasing generation length\n23\nH XNLI edit distances\n23\nI\nMultilingual prompting in unseen languages\n24\nJ\nIdeas that did not work\n25\nK Full results\n25\nL Version control\n28\nM Prompts used\n28\n\nA\nContributions\nThis research was conducted under the BigScience project for open research, a year-long initiative\ntargeting the study of large models and datasets. The goal of the project is to research language models\nin a public environment. The project has hundreds of researchers from more than 50 countries and\nover 250 institutions. The BigScience project was initiated by Thomas Wolf at Hugging Face, and this\ncollaboration would not have been possible without his effort. In the following, we list contributions\nmade to this work.\nNiklas Muennighoff evaluated all models, created xP3 and wrote most of the paper.\nNiklas Muennighoff, Thomas Wang, Lintang Sutawika, Adam Roberts and Hailey Schoelkopf wrote\nthe training and evaluation code.\nNiklas Muennighoff and Adam Roberts trained the models.\nNiklas Muennighoff, Teven Le Scao, Hailey Schoelkopf, Zheng-Xin Yong, Thomas Wang, Khalid\nAlmubarak, Alham Fikri Aji, M Saiful Bari and Zaid Alyafeai contributed prompts or datasets.\nLintang Sutawika, Stella Biderman, Zheng-Xin Yong, Khalid Almubarak, M Saiful Bari and Albert\nWebson initiated the project.\nSheng Shen conducted the contamination analysis.\nSamuel Albanie wrote the prompt appendix.\nThomas Wang and Zheng-Xin Yong converted checkpoints.\nColin Raffel, Thomas Wang, Teven Le Scao, M Saiful Bari, Edward Raff and Dragomir Radev\nadvised the project.\nNiklas Muennighoff, Lintang Sutawika, Teven Le Scao, Colin Raffel, Stella Biderman, Alham Fikri\nAji, Adam Roberts, Samuel Albanie, Sheng Shen, M Saiful Bari, Albert Webson, Xiangru Tang,\nDragomir Radev and Edward Raff contributed to the paper.\nB\nTask generalization breakdown\nIn Figure 9, we compare performance on English held-out tasks. We find that (a) finetuning on xP3\noutperforms P3 (b) multilingual mT0 is better than monolingual T0 on English tasks. We think both\nimprovements come from xP3 having more prompts and datasets than P3 (Chung et al., 2022).\n60\n70\n80\n90\n100\n0\n20\n40\n60\n80\n100\n20\n30\n40\n50\n20\n30\n40\n50\n20\n30\n40\n50\n20\n30\n40\n50\n60\n70\n40\n50\n60\n70\n80\n90\n40\n50\n60\n70\n80\n90\n50\n60\n70\n80\n90\n100\n50\n60\n70\n80\n90\n100\nBLOOMZ-P3\nBLOOMZ\nmT0-13B-P3\nmT0-13B\nT0-11B\nRTE\nCB\nANLI R1\nANLI R2\nANLI R3\nXNLI EN\nWinogrande\nXWinograd EN\nCOPA\nStoryCloze\nNatural Language Inference\nCoreference Resolution\nSentence Completion\nFigure 9: Zero-shot English task generalization. Each dot represents performance on one English evaluation prompt.\n\nIn Figure 10, we visualize task generalization to multilingual datasets. The same data is aggregated\nin Figure 4. Performance by prompt varies substantially highlighting that prompt engineering may still\nbe necessary after MTF. We also find that mT0 consistently outperforms BLOOMZ on Swahili (SW),\npossibly due to it being a larger part of its pretraining corpus (see Figure 2 and §4.6).\n20\n30\n40\n50\n60\n70\n80\n20\n30\n40\n50\n60\n70\n80\n20\n30\n40\n50\n60\n70\n80\n20\n30\n40\n50\n60\n70\n80\n20\n30\n40\n50\n60\n70\n80\n20\n30\n40\n50\n60\n70\n80\n20\n30\n40\n50\n60\n70\n80\n20\n30\n40\n50\n60\n70\n80\n40\n50\n60\n70\n80\n90\n40\n50\n60\n70\n80\n90\n40\n50\n60\n70\n80\n90\n40\n60\n80\n100\n40\n60\n80\n100\n40\n60\n80\n100\n40\n60\n80\n100\n40\n60\n80\n100\n40\n60\n80\n100\n40\n60\n80\n100\n40\n60\n80\n100\n40\n60\n80\n100\n40\n60\n80\n100\n40\n60\n80\n100\n40\n60\n80\n100\n40\n60\n80\n100\nXGLM-7.5B\nBLOOM\nmTk-13B\nBLOOMZ\nmT0-13B\nXNLI AR\nXNLI ES\nXNLI FR\nXNLI HI\nXNLI VI\nXNLI UR\nXNLI SW\nXNLI ZH\nXWinograd FR\nXWinograd PT\nXWinograd ZH\nXCOPA ID\nXCOPA SW\nXCOPA TA\nXCOPA VI\nXCOPA ZH\nXStoryCloze AR\nXStoryCloze ES\nXStoryCloze EU\nXStoryCloze HI\nXStoryCloze ID\nXStoryCloze SW\nXStoryCloze TE\nXStoryCloze ZH\nNatural Language Inference\nCoreference Resolution\nSentence Completion\nFigure 10: Zero-shot multilingual task generalization on languages seen during pretraining and finetuning. Each dot\nrepresents performance on one English evaluation prompt.\n\nC\nArtifacts\nTable 3 lists all artifacts used or released in this work. We make all our work accessible under the most\npermissive licenses available to us.\nArtifact\nExplanation\nPublic link\nROOTS\nMultilingual pretraining corpus of BLOOM\nhttps://huggingface.co/bigscience-data\nmC4\nMultilingual pretraining corpus used for mT5\nhttps://huggingface.co/datasets/mc4\nP3\nMultitask finetuning dataset with English data & English prompts\nhttps://huggingface.co/datasets/bigscience/P3\nxP3\nMultitask finetuning dataset with multilingual data & English prompts\nhttps://huggingface.co/datasets/bigscience/xP3\nxP3all\nSame as xP3 with held-out evaluation sets\nhttps://huggingface.co/datasets/bigscience/xP3all\nxP3mt\nSame as xP3 with English & multilingual machine-translated prompts\nhttps://huggingface.co/datasets/bigscience/xP3mt\nxP3megds\nProcessed version of xP3 for easy usage with Megatron-DeepSpeed\nhttps://huggingface.co/datasets/bigscience/xP3megds\nxP3x\nExtension of xP3 to 277 languages\nhttps://huggingface.co/datasets/Muennighoff/xP3x\nXGLM-7.5B\n7.5B parameter pretrained multilingual transformer\nhttps://huggingface.co/facebook/xglm-7.5B\nT0-11B\n11B parameter model finetuned on P3\nhttps://huggingface.co/bigscience/t0\nmTk-Instruct-3.7B\n3.7B parameter multitask finetuned multilingual transformer\nhttps://huggingface.co/allenai/mtk-instruct-3b-def-pos\nmTk-Instruct-13B\n13B parameter multitask finetuned multilingual transformer\nhttps://huggingface.co/allenai/mtk-instruct-11b-def-pos\nBLOOM-560M\n560M parameter model pretrained on ROOTS\nhttps://huggingface.co/bigscience/bloom-560m\nBLOOM-1.1B\n1.1B parameter model pretrained on ROOTS\nhttps://huggingface.co/bigscience/bloom-1b1\nBLOOM-1.7B\n1.7B parameter model pretrained on ROOTS\nhttps://huggingface.co/bigscience/bloom-1b7\nBLOOM-3B\n3B parameter model pretrained on ROOTS\nhttps://huggingface.co/bigscience/bloom-3b\nBLOOM-7.1B\n7.1B parameter model pretrained on ROOTS\nhttps://huggingface.co/bigscience/bloom-7b1\nBLOOM\n176B parameter model pretrained on ROOTS\nhttps://huggingface.co/bigscience/bloom\nBLOOMZ-560M\n560M parameter model finetuned on xP3\nhttps://huggingface.co/bigscience/bloomz-560m\nBLOOMZ-1.1B\n1.1B parameter model finetuned on xP3\nhttps://huggingface.co/bigscience/bloomz-1b1\nBLOOMZ-1.7B\n1.7B parameter model finetuned on xP3\nhttps://huggingface.co/bigscience/bloomz-1b7\nBLOOMZ-3B\n3B parameter model finetuned on xP3\nhttps://huggingface.co/bigscience/bloomz-3b\nBLOOMZ-7.1B\n7.1B parameter model finetuned on xP3\nhttps://huggingface.co/bigscience/bloomz-7b1\nBLOOMZ-7.1B-MT\n7.1B parameter model finetuned on xP3mt\nhttps://huggingface.co/bigscience/bloomz-7b1-mt\nBLOOMZ-7.1B-P3\n7.1B parameter model finetuned on P3\nhttps://huggingface.co/bigscience/bloomz-7b1-p3\nBLOOMZ\n176B parameter model finetuned on xP3\nhttps://huggingface.co/bigscience/bloomz\nBLOOMZ-MT\n176B parameter model finetuned on xP3mt\nhttps://huggingface.co/bigscience/bloomz-mt\nBLOOMZ-P3\n176B parameter model finetuned on P3\nhttps://huggingface.co/bigscience/bloomz-p3\nmT5-300M\n300M parameter model pretrained on a sampled version of mC4\nhttps://huggingface.co/google/mt5-small\nmT5-580M\n580M parameter model pretrained on a sampled version of mC4\nhttps://huggingface.co/google/mt5-base\nmT5-1.2B\n1.2B parameter model pretrained on a sampled version of mC4\nhttps://huggingface.co/google/mt5-large\nmT5-3.7B\n3.7B parameter model pretrained on a sampled version of mC4\nhttps://huggingface.co/google/mt5-xl\nmT5-13B\n13B parameter model pretrained on a sampled version of mC4\nhttps://huggingface.co/google/mt5-xxl\nmT0-300M\n300M parameter model finetuned on xP3\nhttps://huggingface.co/bigscience/mt0-small\nmT0-580M\n580M parameter model finetuned on xP3\nhttps://huggingface.co/bigscience/mt0-base\nmT0-1.2B\n1.2B parameter model finetuned on xP3\nhttps://huggingface.co/bigscience/mt0-large\nmT0-3.7B\n3.7B parameter model finetuned on xP3\nhttps://huggingface.co/bigscience/mt0-xl\nmT0-13B\n13B parameter model finetuned on xP3\nhttps://huggingface.co/bigscience/mt0-xxl\nmT0-13B-MT\n13B parameter model finetuned on xP3mt\nhttps://huggingface.co/bigscience/mt0-xxl-mt\nmT0-13B-P3\n13B parameter model finetuned on P3\nhttps://huggingface.co/bigscience/mt0-xxl-p3\nTable 3: Links to all models & datasets used as part of this work. BLOOMZ models have an additional repository\ncontaining the final optimizer states for training with Megatron-Deepspeed that can be found by appending “-\noptimizer-states\" to the respective URL. BLOOM(Z) models are released under the RAIL license, while mT5 / mT0\nmodels are licensed under Apache 2.0\nD\nROOTS language contamination\nWhile the BLOOM ROOTS corpus (Laurençon et al., 2022) was collected from 46 natural languages\nand 13 programming languages, we find that sentences from the same document do not always belong to\nthe collected (meta) language. Some sentences use languages like Russian or Japanese that were not the\nintentionally collected parts. This “language contamination” may stem from “code-mixing” or different\nlanguages being used in code comments. To investigate the extent of contamination, we randomly sample\n1% of the documents from ROOTS for a total of 51M documents. For each document, we use cld32 (Xue\net al., 2020) to identify the languages used in each sentence and compare them with the meta language of\nthe document. We summarize our results in Figure 11. It shows that ROOTS contains unintentionally\ncollected languages, such as Burmese (my: 0.00003%), Thai (th: 0.006%), Turkish (tr: 0.03%), Greek\n(el: 0.03%), Russian (ru: 0.03%), Bulgarian (bg: 0.05%), Estonian (et: 0.06%), Haitian (ht: 0.12%),\nGerman (de: 0.21%), Italian (it: 0.28%) and Japanese (ja: 0.54%). These “unseen” languages only have\n2https://github.com/google/cld3\n\nen fr es pt vi ar ca id zh gl ja hi ms la it eu ta da no de bn nl lb fy af cy eo ht mt ml te sr sv mrmgne gd ga ro zu hu jv kn fil co sn ig et pl su slhawbg sm ur ceb lt gu xh sw sq lv pa sk cshmnru uk el so fi az ny mi tr ha ko st yo uz is fa be sd tg ku mk ky kk ps mn yi iw th hy am ka si kmmy lo\n25\n5\n1\n0.1\n0.01\n0.001\n0.0001\n% of corpus\nROOTS-IDENTIFY-1%\nROOTS-1%\nmT5 corpus\nFigure 11: Language composition of ROOTS-IDENTIFY-1%, ROOTS-1% and the mT5 corpus. All mT5 languages\nare depicted. ROOTS-1% is a random 1% sample of ROOTS with its assigned meta-languages. ROOTS-IDENTIFY-\n1% are the actual languages in ROOTS-1% re-identified using cld3.\nsmall sentence proportions in our subsample compared to English (en: 46.23%), French (fr: 15.73%) and\nSpanish (es: 13.38%). Yet, they may help the language generalization of BLOOMZ models described\nin §4.2. Japanese is mostly mixed in the meta English documents (47%), meta Code documents (8%)\nand meta Chinese documents (5%). Meanwhile, Russian is mostly mixed in the meta English documents\n(52%), meta Code documents (19%) and meta French documents (11%).\nE\nCode generations\nTable 4 provides statistics on code generations and code data. We find that BLOOM generates on average\n70% more characters and 17x more comments than BLOOMZ for a given problem from HumanEval.\nFigure 12 compares an example solution from BLOOM and BLOOMZ. While both solutions are correct,\nBLOOMZ is biased towards short and concise answers.\n(a) BLOOM\n(b) BLOOMZ\nFigure 12: Code generations of BLOOM and BLOOMZ on HumanEval. The model is prompted to generate after\nthe final ”””. The generation is stopped after an end-of-sequence token or a return statement followed by a newline.\nData (→)\nHumanEval generations\nFine-tuning data\nBLOOM\nBLOOMZ\nin xP3 (code data)\nAverage characters\n247\n144\n531\nAverage Python comments (#)\n0.69\n0.04\n0.85\nTable 4: Number of characters and comments for generations and fine-tuning data. For finetuning data, the statistics\nare computed for the targets that the model is tasked to generate, not the input.\nF\nQualitative examples\n\n一个传奇的开端，一个不\n灭的神话，这不仅仅是一\n部电影，而是作为一个走\n进新时代的标签，永远彪\n炳史册。Would you rate\nthe previous review as\npositive, neutral or\nnegative?\nInput:\npositive\nBLOOMZ:\npositive\nBLOOMZ-MT:\nmT0-13B:\npositive\n(a) English prompt\n一个传奇的开端，一个不\n灭的神话，这不仅仅是一\n部电影，而是作为一个走\n进新时代的标签，永远彪\n炳史册。你认为这句话的\n立场是赞扬、中立还是批\n评？\nInput:\npositive\nBLOOMZ:\n赞扬\nBLOOMZ-MT:\nmT0-13B:\n赞扬\n(b) Non-English prompt\nFigure 13: Greedy generations for sentiment analysis, a task trained on. BLOOMZ and mT0-13B have not been\ntrained on non-English prompts, but are still able to handle them. BLOOMZ, however, answers in English. The\nreview is a five star review of Star Wars Episode IV.\nSuggest at least five\nrelated search terms to\n\"Mạng neural nhân\ntạo\".\nInput:\nartificial neural network,\nneural network, artificial\nintelligence, machine\nlearning, deep learning\nBLOOMZ:\ntrí tuệ nhân tạo, mạng thần\nkinh, mạng nơ-ron, mạng nơ-\nron nhân tạo, mạng nơ-ron\nsâu\nBLOOMZ-MT:\nmT0-13B:\nmạng neural, tự động hóa, tự\nđộng hóa mạng neural\n(a) English prompt\nProposez au moins\ncinq mots clés\nconcernant «Réseau de\nneurones artificiels».\nInput:\nartificial neural network,\nartificial intelligence,\nmachine learning, deep\nlearning, neural network\nBLOOMZ:\nBLOOMZ-MT:\nmT0-13B:\napprentissage, réseau,\nintelligence artificielle\nréseau de neurones\nartificiels, réseau de\nneurones, apprentissage\nautomatique, apprentissage\nprofond, apprentissage en\nprofondeur\n(b) Non-English prompt\nFigure 14: Greedy generations for zero-shot query expansion, a task not trained on. The models sometimes fail to\noutput at least five terms as requested in the prompt.\n\nExplain in a sentence in\nTelugu what is\nbackpropagation in\nneural networks.\nInput:\nఒక న్యూట్రానిక్ నెట్వర్క్ లో\nబ్యాక్ ప్రాసెసింగ్ అనేది ఒక\nరకమైన అభ్యసన విధానం, ఇది\nరీజియన్ ల యొక్క గత\nఅభ్యసన ఫలితాలను\nఉపయోగించి, రీజియన్ ల\nయొక్క ప్రస్తుత అభ్యసన\nఫలితాలను సవరిస్తుంది.\nBLOOMZ:\nఒక న్యూట్రాన్‌కు తిరిగి పంపిన\nప్రతిబింబాన్ని ఒక రకమైన\nబ్యాక్ ప్రాసెసింగ్ అంటారు.\nBLOOMZ-MT:\nmT0-13B:\nనేరుగా నేరుగా న్యూరోలాజికల్\nనెట్ వర్క్ లలో బేక్ పోర్షన్\nఅంటే ఏమిటి?\n(a) English prompt\nWhy is the sky blue?\nInput:\nRayleigh scattering.\nBLOOMZ:\nRayleigh scattering. The blue\ncolor of the sky is due to\nRayleigh scattering of\nsunlight by atmospheric\nmolecules.\nBLOOMZ with 10\ntokens min length:\nmT0-13B:\nbecause of the sun\nbecause of the sun's\nradiation sensitivity to light\nmT0-13B with 10\ntokens min length:\nBLOOM:\n\"Why is the sky blue?\"\n\"Why is the sky blue?\"\n\"Why is the sky blue?\"...\n(b) English prompt\nFigure 15: Greedy generations on question answering, a task trained on. Left: Specifying the language in the\nprompt is an effective way to force the output language. Right: Setting a minimum token length as a generation\nhyperparameter is an effective way to force long generations. The output of BLOOM is shortened (marked with ...).\nWrite a fairy tale about a troll saving a princess from a\ndangerous dragon. The fairy tale is a masterpiece that\nhas achieved praise worldwide and its moral is \"Heroes\nCome in All Shapes and Sizes\". Story (in Spanish):\nInput:\nUn reino en el bosque estaba bajo un terrible\ndragón. Todos los que intentaron derrotar al\ndragón murieron. El rey y la reina le pedían al\npueblo que les diera un niño, que les gustaría\nsalvar el reino de la maldición del dragón. Un\ndía, la reina dio a luz a un niño con una cabeza\nenorme y ojos que brillaban como estrellas. El\nrey y la reina llamaron al niño Elliot. Elliot\ncrecería y se convertiría en un joven gigante. Él\nsalvaría al pueblo.\nBLOOMZ:\nEn la noche de la primera tempestad, un gigante\ndragón, llamado Odín, golpeó a una princesa al\ndedo. Así que un troll, llamado Odín, fue a salvar\na la princesa. Luego, este fue querido por todos.\nmT0:\n(a) English prompt\nWrite a fable about wood elves living in a forest that is\nsuddenly invaded by ogres. The fable is a masterpiece that\nhas achieved praise worldwide and its moral is \"Violence is\nthe last refuge of the incompetent\". Fable (in Hindi):\nInput:\nएक समय की बात है, देवदार के वृक्षों से आच्छादित एक वन में\nएक समुदाय रहता था जिनमें से प्रत्येक व्यक्ति का कद एक फ़ीट\nका चार इंच ही था, परंतु वे लोग बहुत ही बुद्धिमान और सभ्य थे।\nउनके बीच परस्पर प्रेम और भाइचारे की भावना थी। अपने\nपड़ोसियों के प्रति उनके व्यवहार और आदर को देखकर लोग\nआश्चर्य में पड़ जाते थे। वे वृक्षों की छाल के बने हुए छोटे-छोटे\nकमरों में रहते थे। अपनी बुद्धिमत्ता के कारण तो वे लोग प्रसिद्ध\nथे ही, पर विशेष रूप से अपने सदगुणों के कारण उनका नाम\nदूर-दूर तक मशहूर था। वे लोग स्वयं को डंगऑल (Dongal)\nकहते थे। वे जिस वन में रहते थे, उसी वन की हरियाली और\nपेड़-पौधों के कारण ही तो वह वन इतना आकर्षक था। डंगऑल\nउस वन के स्वामी नहीं थे। उनकी बस एक शर्त थी कि वे हर एक\nपेड़ पर केवल एक ही घर बनाएँगे। एक दिन उस वन में घुस आए\nकुछ लोग जो स्वभाव से ही जंगली और असभ्य थे। उन्होंने उस\nवन में अपना आधिपत्य जमा लिया। वे बहुत विशाल कद के थे\nऔर उनकी आँखें सुर्ख़ लाल रंग की थे। उन्होंने उस वन की\nहरियाली नष्ट कर दी और वृक्षों को काटना शुरू कर दिया।\nBLOOMZ:\n(b) English prompt\nFigure 16: Non-greedy fable generations given a moral, a task not trained on. The generations are cherry-picked\nfrom 16 outputs with no minimum length, a temperature of 0.9 and top k of 40. Left: BLOOMZ generates an\ninteresting fable with the desired moral. mT0 is significantly worse at writing stories likely due to its different\npretraining objective. Right: BLOOMZ does not seem to understand the moral correctly.\n\nG\nIncreasing generation length\nIn §4.5, we found performance on generative tasks to worsen in later stages of training. To investigate this\nproblem further, we study a 7.1 billion parameter BLOOM model that is finetuned for 13 billion tokens,\nwhich results in a low BLEU score of 0 and very short generations as shown in Table 5 (Default). We\ncan solve this problem with two high-level strategies: (a) Reducing short tasks during finetuning and (b)\nForcing a minimum generation length.\nFor (a), we do so by either early stopping, upweighting long tasks or adding new long tasks. As\nthe majority of our finetuning data are single sentences, early stopping has the effect of finetuning on\nfewer short sentences. Upweighting long tasks is done by removing the loss normalization explained in\n§3.2. This has the effect of each token getting equal weight regardless of the task, which upweights long\ntasks, as they have more tokens. Finally, for adding long tasks, we add tasks that require multi-sentence\ngenerations, such as generating an entire news article given a title. These long tasks collectively make up\n10% of finetuning data for this ablation. All three solutions result in longer average generations as shown\nin Table 5 and slightly better BLEU scores, albeit effects are still small.\nFor (b), we force the model to generate a minimum number of tokens at inference. Our benchmarking\ntask, MultiEURLEX (Chalkidis et al., 2021), requires multi-sentence generations with an average target\nlength of 1965 characters (about 491 tokens). By forcing the model to generate at least 768 tokens, we\nensure that the generation is at least as long as the target. This boosts the BLEU score significantly to\n9.05. This approach is thus an effective strategy to maintain long generations of good quality.\nFor our final models, we employ early stopping, adding of long tasks and recommend forcing a\nminimum generation length at inference for long generations. We do not upweight longer tasks, as it\nworsens accuracy on our NLU validation tasks by 10%. The number of tokens our final models are\nfine-tuned for are displayed in Table 6.\nModel\nFinetuning tokens\nBLEU Score\nAverage generation length (characters)\nDefault\n13 billion\n0.00\n122\nEarly stopping\n6 billion\n0.00\n155\nUpweight longer tasks\n13 billion\n0.06\n364\nAdd more long tasks\n13 billion\n0.06\n136\nForcing 768 tokens at inference\n13 billion\n9.05\n3072\nTable 5: 7.1 billion parameter BLOOMZ models with various modifications benchmarked on MultiEURLEX\nEnglish-French translation (Chalkidis et al., 2021). We benchmark three prompts on both English to French and\nFrench to English translation. We then take the median performance across the three prompts for each translation\ndirection and average the two scores to arrive at the BLEU score reported.\nModel\nmT0-300M\nmT0-560M\nmT0-1.2B\nmT0-3.7B\nmT0-13B\nTokens\n4.62\n4.62\n4.62\n1.85\n1.29\nModel\nBLOOMZ-560M\nBLOOMZ-1.1B\nBLOOMZ-1.7B\nBLOOMZ-3B\nBLOOMZ-7.1B\nBLOOMZ\nTokens\n3.67\n0.502\n8.39\n8.39\n4.19\n2.09\nTable 6: Tokens in billions that final models are finetuned for. We early-stop models based on validation performance.\nFor -MT and -P3 variants we take the checkpoint after the same number of steps as for their default versions.\nH\nXNLI edit distances\nAs models are surprisingly capable of solving XNLI in languages they were never intentionally trained on\n(§4.2), we investigate whether XNLI can be solved without any language understanding. To do so, we\ncompute edit distances using the Levenshtein methodology (Levenshtein et al., 1966) between premise\n\nPremise\nHypothesis\nLev. distance\nLabel\nprobably so probably so um-hum\nprobably yes so uh-huh\n13\nEntailment\nequivalent to increasing national saving to 19 .\nNational savings are 18 now .\n34\nNeutral\nThe Inglethorps did not appear .\nThe Inglethorps were the first ones to turn up .\n26\nContradiction\nTable 7: Three samples from the English XNLI split. To solve XNLI models need to classify whether the premise\nentails, is neutral to or contradicts the hypothesis. Samples are cherry-picked.\nand hypothesis. Table 7 shows three samples from the English XNLI and their edit distances. Our\nhypothesis is that entailment pairs generally need to cover similar content, and thus have similar distance.\nContradiction pairs still need to cover similar content but differ in at least one major way. Meanwhile\nfor neutral pairs, hypothesis and premise may be about completely different topics, hence they should\nhave the highest distance. In Table 8 we compute distances across all Thai, Turkish and Greek samples,\nthree languages where we found language generalization to occur for BLOOMZ. Results confirm our\nhypothesis that distances are generally largest for neutral samples and smallest for entailment samples.\nHowever, the aggregate differences are very small with only a few edits difference. For example, Thai\ncontradiction samples only have 2.5 edits more on average than entailment samples. Thus, comparing\ncharacters based on edit distance alone is likely not sufficient to fully explain the language generalization\nof models in §4.2.\nLabel (→)\nEntailment\nNeutral\nContradiction\nLanguage (↓)\nThai (th)\n79.08\n82.64\n81.52\nTurkish (tr)\n76.93\n80.59\n80.24\nGreek (el)\n90.90\n95.10\n93.93\nTable 8: Levenshtein distances between hypothesis and premise averaged across samples from different XNLI\nlabels. Each label has 830 samples per language subset.\nI\nMultilingual prompting in unseen languages\nTable 9 shows aggregate performances on languages not intentionally seen during pretraining nor fine-\ntuning for BLOOMZ and only seen during pretraining for mT0. For BLOOMZ, performance drops\nsignificantly when translating the prompts to the respective unseen languages. Unlike on translated\nprompts for seen languages (§4.3), BLOOMZ-MT performs worse than BLOOMZ for machine-translated\nprompts in unseen languages. This is likely because BLOOMZ-MT has not been finetuned on prompts in\nthese languages. For mT0 differences are less significant.\n\nTask\nPrompt\nAverage accuracy\nBLOOMZ\nBLOOMZ-MT\nmT0-13B\nmT0-13B-MT\nXNLI\nEN\n45.65\n43.2\n48.52\n51.33\nMT\n36.48\n35.67\n41.86\n39.78\nXCOPA\nEN\n54.27\n53.67\n72.67\n71.6\nMT\n53.2\n53.0\n71.57\n70.87\nXStoryCloze\nEN\n61.59\n61.36\n79.31\n80.13\nMT\n60.5\n59.91\n80.21\n80.28\nXWinograd\nEN\n55.98\n54.54\n70.81\n72.0\nMT\n53.11\n52.46\n67.86\n70.45\nTable 9: Comparison between EN (English) and MT (machine-translated) prompts for 176B BLOOMZ and 13B\nmT0 models finetuned on either only English or English and machine-translated multilingual prompts (-MT).\nFor BLOOMZ the evaluation languages averaged are never intentionally seen, such as Japanese and Russian for\nXWinograd (see Figure 5). For mT0 the evaluation languages are only seen during pretraining.\nJ\nIdeas that did not work\nWe list several experiments that did not improve over baseline results:\nNon-causal\nIn a non-causal or prefix language model, the model attends bidirectionally over input\ntokens and only causally over target tokens. Given a pretrained causal decoder, other work found that\nmultitask finetuning in a non-causal setup performed better than causal finetuning (Wang et al., 2022a; Tay\net al., 2022c). However, in our experiments, non-causal finetuning did not improve over causal finetuning.\nSpecial tokens\nInstead of separating inputs and targets with a space, we experimented with special\ntokens. Using the end-of-sequence token as a separator or a completely new token that the model would\nlearn during finetuning significantly worsened results. The models may need to train on more tokens,\npossibly even during pretraining, to learn these new special tokens (Zeng et al., 2022).\nFixing prompts\nPromptSource has been written with encoder-decoder models in mind, where inputs\nand targets are fed into different models. As a consequence, human-written prompts in PromptSource\noften lack separators between input and target. For our decoder models, we decided to separate them\nwith a space. We additionally experimented with leaving them as is or rewriting a significant amount of\nprompts, but neither improved significantly over space separation.\nBitFit\nPrevious work has shown bias-only finetuning (Zaken et al., 2021) of large language models to\nbe sufficient for strong downstream performance (Logan et al., 2021; Hu et al., 2021; Muennighoff, 2022;\nLiu et al., 2022; Ding et al., 2022; Muennighoff et al., 2022). We found multitask finetuning of only biases\nto perform 15 absolute percentage points worse on the average of held-out tasks for BLOOMZ-7.1B.\nK\nFull results\nTable 10 shows all evaluation results on test datasets. Table 11 displays evaluation results on validation\ndatasets which we use for checkpoint selection.\n\nPretrained\nPretrained + Multitask finetuned\nTask\nDataset\nConfig\nSplit\nPrompt\nMetric\nXGLM-7.5B\nBLOOM-560M\nBLOOM-1.1B\nBLOOM-1.7B\nBLOOM-3B\nBLOOM-7.1B\nBLOOM\nT0-11B\nmTk-Instruct-3.7B\nmTk-Instruct-13B\nmT0-300M\nmT0-560M\nmT0-1.2B\nmT0-3.7B\nmT0-13B\nmT0-13B-MT\nmT0-13B-P3\nBLOOMZ-560M\nBLOOMZ-1.1B\nBLOOMZ-1.7B\nBLOOMZ-3B\nBLOOMZ-7.1B\nBLOOMZ-7.1B-MT\nBLOOMZ-7.1B-P3\nBLOOMZ\nBLOOMZ-MT\nBLOOMZ-P3\nCoref. resolution\nwinogrande\nxl\nvalidation\nEN\nMedian acc.\n49.25\n49.88\n50.99\n49.57\n49.96\n49.41\n48.62\n60.46\n50.99\n52.33\n49.57\n51.62\n50.51\n52.01\n62.27\n62.51\n56.91\n49.80\n51.07\n50.75\n51.78\n55.41\n55.88\n51.78\n58.41\n58.64\n55.64\nCoref. resolution\nwinogrande\nxl\nvalidation\nEN\nMax acc.\n50.12\n50.99\n51.62\n50.91\n51.46\n50.91\n49.64\n63.61\n51.14\n54.54\n50.51\n53.28\n51.78\n52.49\n63.38\n62.67\n58.56\n52.41\n52.33\n51.14\n53.67\n55.80\n56.51\n54.06\n59.27\n59.98\n57.06\nCoref. resolution\nxwinograd\nen\ntest\nEN\nMedian acc.\n50.88\n50.62\n51.10\n50.67\n50.97\n50.15\n50.28\n62.75\n52.22\n52.77\n50.11\n51.01\n52.30\n57.94\n79.91\n81.33\n59.87\n50.24\n50.15\n52.09\n54.84\n60.09\n59.31\n52.26\n67.87\n64.73\n59.74\nCoref. resolution\nxwinograd\nen\ntest\nEN\nMax acc.\n51.61\n51.53\n51.57\n51.66\n51.70\n50.71\n51.27\n70.71\n53.12\n60.82\n51.31\n51.40\n54.80\n61.89\n81.29\n83.31\n70.71\n51.01\n50.49\n56.34\n59.23\n66.02\n65.76\n53.72\n69.08\n69.33\n60.65\nCoref. resolution\nxwinograd\nfr\ntest\nEN\nMedian acc.\n50.60\n46.99\n48.19\n50.60\n46.99\n50.60\n51.81\n54.22\n50.60\n53.01\n50.60\n51.81\n49.40\n56.63\n77.11\n73.49\n55.42\n49.40\n53.01\n51.81\n49.40\n53.01\n53.01\n53.01\n65.06\n59.04\n53.01\nCoref. resolution\nxwinograd\nfr\ntest\nEN\nMax acc.\n51.81\n51.81\n56.63\n55.42\n54.22\n51.81\n53.01\n56.63\n53.01\n60.24\n53.01\n55.42\n56.63\n59.04\n78.31\n78.31\n61.45\n51.81\n56.63\n55.42\n53.01\n57.83\n55.42\n55.42\n68.67\n68.67\n59.04\nCoref. resolution\nxwinograd\nfr\ntest\nMT\nMedian acc.\n46.99\n48.19\n53.01\n48.19\n46.99\n50.60\n49.40\n54.22\n50.60\n53.01\n49.40\n53.01\n53.01\n56.63\n68.67\n75.90\n53.01\n48.19\n50.60\n50.60\n50.60\n51.81\n55.42\n51.81\n56.63\n57.83\n53.01\nCoref. resolution\nxwinograd\nfr\ntest\nMT\nMax acc.\n51.81\n51.81\n55.42\n53.01\n55.42\n51.81\n50.60\n59.04\n57.83\n63.86\n54.22\n55.42\n55.42\n59.04\n75.90\n75.90\n61.45\n51.81\n59.04\n51.81\n50.60\n57.83\n57.83\n54.22\n65.06\n66.27\n56.63\nCoref. resolution\nxwinograd\njp\ntest\nEN\nMedian acc.\n49.22\n50.36\n50.89\n51.62\n51.41\n50.89\n50.26\n51.51\n52.66\n53.18\n50.89\n50.68\n51.41\n56.93\n74.35\n77.37\n60.27\n49.74\n49.84\n49.95\n50.26\n50.68\n49.64\n50.36\n57.46\n55.47\n51.09\nCoref. resolution\nxwinograd\njp\ntest\nEN\nMax acc.\n52.03\n51.09\n52.03\n52.35\n52.24\n52.76\n50.99\n51.82\n53.18\n56.20\n52.14\n51.41\n52.24\n60.27\n78.62\n78.62\n65.59\n50.57\n51.09\n52.55\n52.45\n52.87\n51.62\n51.93\n59.65\n58.39\n56.00\nCoref. resolution\nxwinograd\njp\ntest\nMT\nMedian acc.\n48.91\n50.89\n50.26\n50.78\n51.93\n49.53\n51.72\n51.51\n51.20\n53.28\n51.41\n50.05\n50.26\n55.27\n73.31\n78.42\n61.00\n50.78\n50.57\n49.64\n50.68\n49.95\n50.26\n50.36\n52.87\n52.66\n50.89\nCoref. resolution\nxwinograd\njp\ntest\nMT\nMax acc.\n50.99\n52.03\n52.03\n52.24\n52.97\n50.99\n53.18\n52.03\n53.70\n56.41\n52.45\n51.09\n53.08\n59.02\n78.21\n80.19\n66.11\n52.03\n51.82\n49.95\n52.14\n52.76\n51.82\n51.51\n53.91\n53.60\n54.33\nCoref. resolution\nxwinograd\npt\ntest\nEN\nMedian acc.\n50.57\n51.33\n51.71\n51.71\n50.19\n48.67\n50.95\n52.47\n52.09\n56.27\n49.81\n49.81\n53.61\n58.17\n72.24\n76.05\n56.27\n50.19\n50.19\n50.95\n52.47\n53.99\n54.37\n51.33\n63.50\n60.08\n53.99\nCoref. resolution\nxwinograd\npt\ntest\nEN\nMax acc.\n53.99\n53.99\n53.99\n53.99\n54.37\n50.19\n51.33\n54.75\n52.09\n58.56\n50.57\n52.09\n55.13\n60.84\n76.43\n80.99\n61.98\n52.09\n51.33\n53.23\n53.61\n57.79\n57.41\n53.99\n64.26\n64.64\n60.46\nCoref. resolution\nxwinograd\npt\ntest\nMT\nMedian acc.\n50.95\n52.09\n50.57\n49.81\n50.57\n50.57\n53.23\n52.47\n53.23\n52.47\n49.81\n47.15\n52.47\n54.75\n71.48\n75.67\n55.89\n52.47\n50.57\n49.81\n50.19\n52.85\n53.61\n51.33\n60.46\n59.70\n54.75\nCoref. resolution\nxwinograd\npt\ntest\nMT\nMax acc.\n53.99\n53.99\n53.99\n53.99\n53.99\n53.99\n53.99\n56.65\n54.37\n55.89\n51.71\n52.09\n56.27\n66.16\n77.95\n80.61\n64.26\n53.99\n54.75\n53.23\n52.47\n53.99\n55.51\n52.09\n64.26\n62.74\n59.32\nCoref. resolution\nxwinograd\nru\ntest\nEN\nMedian acc.\n53.33\n51.43\n52.38\n54.29\n52.70\n54.29\n54.29\n51.43\n53.97\n56.83\n49.52\n51.11\n52.38\n56.83\n74.29\n73.97\n56.51\n52.06\n49.52\n51.75\n52.38\n53.97\n53.02\n48.57\n57.78\n56.51\n52.70\nCoref. resolution\nxwinograd\nru\ntest\nEN\nMax acc.\n53.97\n53.97\n53.97\n56.19\n54.92\n55.24\n57.14\n53.33\n55.56\n60.32\n53.65\n52.70\n55.56\n59.05\n76.51\n79.05\n62.22\n53.97\n50.48\n53.33\n53.97\n54.92\n55.87\n49.21\n60.95\n60.32\n56.19\nCoref. resolution\nxwinograd\nru\ntest\nMT\nMedian acc.\n53.33\n51.75\n52.38\n53.97\n52.06\n53.97\n52.70\n50.16\n53.33\n54.29\n52.06\n51.75\n52.70\n52.38\n66.98\n71.43\n55.87\n51.43\n51.43\n53.02\n49.52\n52.06\n52.70\n47.62\n54.29\n55.87\n54.92\nCoref. resolution\nxwinograd\nru\ntest\nMT\nMax acc.\n54.60\n53.97\n53.97\n54.60\n54.92\n55.56\n55.87\n52.70\n54.92\n58.73\n54.29\n53.97\n54.60\n54.60\n72.06\n75.24\n58.41\n53.97\n53.97\n55.24\n53.97\n53.33\n54.92\n53.97\n60.32\n57.14\n57.14\nCoref. resolution\nxwinograd\nzh\ntest\nEN\nMedian acc.\n49.01\n49.21\n48.81\n50.20\n50.00\n50.60\n49.21\n49.21\n52.18\n56.75\n52.78\n52.18\n51.59\n57.54\n69.25\n76.19\n58.53\n54.17\n53.97\n51.39\n55.16\n57.94\n54.37\n52.18\n68.65\n62.10\n51.59\nCoref. resolution\nxwinograd\nzh\ntest\nEN\nMax acc.\n50.79\n52.18\n52.78\n53.77\n55.16\n55.36\n52.98\n49.40\n54.76\n57.14\n54.17\n53.77\n54.17\n62.90\n77.38\n79.17\n65.67\n54.76\n55.16\n56.15\n60.91\n63.69\n62.70\n52.98\n69.05\n70.63\n55.95\nCoref. resolution\nxwinograd\nzh\ntest\nHT\nMedian acc.\n-\n-\n-\n-\n-\n-\n-\n-\n-\n-\n-\n-\n-\n-\n-\n-\n-\n-\n-\n-\n-\n50.99\n-\n49.40\n-\n-\n-\nCoref. resolution\nxwinograd\nzh\ntest\nHT\nMax acc.\n-\n-\n-\n-\n-\n-\n-\n-\n-\n-\n-\n-\n-\n-\n-\n-\n-\n-\n-\n-\n-\n59.72\n-\n52.18\n-\n-\n-\nCoref. resolution\nxwinograd\nzh\ntest\nMT\nMedian acc.\n48.02\n49.01\n49.01\n49.40\n49.60\n50.79\n49.60\n49.21\n53.17\n53.17\n51.19\n51.79\n50.60\n56.35\n67.86\n72.42\n57.74\n50.79\n51.19\n51.79\n52.98\n52.38\n57.94\n50.40\n62.70\n67.46\n57.14\nCoref. resolution\nxwinograd\nzh\ntest\nMT\nMax acc.\n49.21\n55.56\n53.17\n56.15\n53.57\n56.94\n57.74\n49.21\n54.56\n57.74\n53.37\n53.97\n54.37\n62.10\n72.82\n82.34\n64.09\n51.98\n54.17\n54.17\n55.16\n60.71\n62.50\n52.38\n70.24\n76.39\n60.71\nNLI\nanli\nr1\nvalidation\nEN\nMedian acc.\n33.30\n33.60\n33.50\n33.40\n32.90\n33.40\n36.20\n44.50\n29.90\n34.20\n33.30\n31.30\n30.70\n37.50\n48.00\n48.50\n44.90\n29.60\n29.10\n33.10\n38.60\n40.90\n40.10\n34.50\n46.00\n45.60\n40.60\nNLI\nanli\nr1\nvalidation\nEN\nMax acc.\n33.50\n34.40\n33.70\n33.80\n33.40\n33.70\n37.60\n45.00\n34.80\n35.40\n34.70\n33.30\n33.30\n38.20\n49.50\n49.50\n47.30\n33.40\n33.30\n34.00\n40.10\n42.10\n42.60\n35.10\n48.60\n49.70\n41.70\nNLI\nanli\nr2\nvalidation\nEN\nMedian acc.\n33.40\n33.20\n33.10\n33.30\n33.20\n33.30\n33.70\n39.30\n32.40\n32.50\n33.20\n33.30\n32.50\n34.40\n41.70\n40.60\n37.90\n32.00\n33.20\n34.30\n34.60\n38.20\n37.60\n33.90\n41.90\n41.00\n37.80\nNLI\nanli\nr2\nvalidation\nEN\nMax acc.\n35.00\n33.70\n33.50\n36.00\n34.90\n33.40\n34.80\n39.60\n33.20\n34.20\n34.00\n33.50\n34.70\n34.80\n43.00\n42.00\n40.20\n33.40\n33.50\n36.10\n36.80\n39.50\n39.40\n35.40\n44.10\n45.00\n39.30\nNLI\nanli\nr3\nvalidation\nEN\nMedian acc.\n32.92\n33.50\n33.42\n33.17\n33.33\n33.08\n34.58\n41.33\n32.83\n33.33\n33.00\n33.00\n33.50\n37.42\n44.83\n46.25\n40.50\n33.25\n33.08\n35.42\n37.75\n38.00\n38.92\n34.08\n42.67\n41.33\n40.08\nNLI\nanli\nr3\nvalidation\nEN\nMax acc.\n34.25\n35.58\n33.50\n33.67\n33.58\n33.58\n36.33\n43.75\n33.00\n34.83\n33.83\n33.33\n34.75\n39.00\n46.08\n48.17\n44.17\n33.50\n34.50\n37.08\n40.00\n41.00\n42.00\n37.58\n45.50\n45.58\n42.83\nNLI\nsuper_glue\ncb\nvalidation\nEN\nMedian acc.\n39.29\n42.86\n42.86\n28.57\n32.14\n44.64\n33.93\n76.79\n44.64\n26.79\n44.64\n46.43\n50.00\n69.64\n82.14\n87.50\n67.86\n51.79\n53.57\n58.93\n67.86\n57.14\n71.43\n53.57\n76.79\n76.79\n75.00\nNLI\nsuper_glue\ncb\nvalidation\nEN\nMax acc.\n41.07\n60.71\n48.21\n42.86\n51.79\n57.14\n42.86\n78.57\n51.79\n57.14\n50.00\n50.00\n51.79\n85.71\n85.71\n87.50\n76.79\n53.57\n58.93\n71.43\n75.00\n80.36\n83.93\n62.50\n82.14\n87.50\n85.71\nNLI\nsuper_glue\nrte\nvalidation\nEN\nMedian acc.\n52.71\n53.07\n47.65\n49.46\n54.15\n52.35\n50.18\n83.39\n56.68\n51.26\n58.84\n65.70\n62.09\n76.90\n83.03\n83.75\n80.14\n64.26\n53.07\n73.29\n72.56\n79.06\n79.06\n67.15\n81.95\n81.23\n73.65\nNLI\nsuper_glue\nrte\nvalidation\nEN\nMax acc.\n53.07\n54.15\n52.71\n53.79\n57.40\n55.96\n54.15\n84.84\n61.37\n55.23\n61.01\n66.43\n64.26\n78.70\n85.56\n84.84\n83.03\n67.15\n65.70\n76.17\n76.17\n84.12\n82.67\n78.70\n85.56\n85.92\n85.20\nNLI\nxnli\nar\nvalidation\nEN\nMedian acc.\n33.33\n33.82\n33.57\n33.98\n35.94\n33.82\n33.78\n33.90\n35.10\n35.10\n33.90\n40.84\n38.15\n49.72\n56.51\n57.63\n54.82\n39.80\n41.33\n46.99\n48.23\n51.20\n49.28\n47.99\n54.38\n51.85\n46.39\nNLI\nxnli\nar\nvalidation\nEN\nMax acc.\n34.98\n36.95\n34.78\n35.90\n36.59\n37.99\n34.46\n34.22\n39.72\n38.31\n37.43\n41.85\n42.61\n51.85\n57.91\n58.03\n56.06\n44.46\n46.59\n50.04\n53.29\n53.25\n55.58\n50.64\n60.68\n58.03\n55.22\nNLI\nxnli\nar\nvalidation\nHT\nMedian acc.\n34.30\n33.37\n33.33\n34.06\n33.37\n33.33\n33.37\n33.33\n33.65\n35.62\n32.57\n33.37\n34.34\n39.32\n42.93\n49.16\n47.55\n34.78\n35.30\n35.82\n39.36\n41.85\n39.00\n36.35\n38.63\n37.31\n50.32\nNLI\nxnli\nar\nvalidation\nHT\nMax acc.\n37.31\n33.45\n33.41\n35.38\n35.26\n37.79\n34.18\n33.65\n35.50\n37.79\n36.02\n34.14\n35.14\n49.56\n50.04\n55.22\n54.82\n36.47\n38.27\n45.06\n47.39\n48.07\n46.10\n43.21\n42.29\n43.01\n56.71\nNLI\nxnli\nar\nvalidation\nMT\nMedian acc.\n33.33\n33.33\n33.33\n33.33\n33.49\n35.06\n33.61\n33.33\n33.25\n33.78\n33.29\n33.37\n33.33\n33.41\n33.33\n35.14\n33.37\n33.25\n33.33\n33.53\n33.90\n33.49\n34.66\n33.53\n33.33\n41.97\n33.37\nNLI\nxnli\nar\nvalidation\nMT\nMax acc.\n34.22\n33.45\n35.42\n33.69\n34.54\n36.67\n36.95\n33.45\n34.10\n36.27\n33.33\n33.53\n34.22\n33.94\n34.18\n42.85\n39.48\n36.18\n40.32\n35.94\n41.89\n49.12\n48.55\n42.89\n35.50\n45.42\n47.51\nNLI\nxnli\nbg\nvalidation\nEN\nMedian acc.\n33.37\n33.33\n33.33\n33.41\n33.33\n33.37\n33.13\n34.66\n34.30\n34.50\n33.86\n40.44\n41.49\n52.65\n59.24\n59.80\n56.79\n37.27\n35.46\n38.59\n39.36\n43.49\n41.20\n41.65\n47.19\n43.69\n41.16\nNLI\nxnli\nbg\nvalidation\nEN\nMax acc.\n37.23\n33.45\n34.66\n34.78\n34.62\n34.66\n33.90\n35.66\n39.92\n36.59\n37.55\n42.33\n43.94\n54.18\n59.88\n59.92\n58.23\n39.76\n40.40\n42.17\n43.82\n43.61\n44.90\n43.98\n48.43\n46.75\n46.63\nNLI\nxnli\nbg\nvalidation\nMT\nMedian acc.\n33.33\n33.33\n33.33\n33.37\n33.33\n33.09\n33.05\n33.33\n34.34\n36.63\n33.33\n33.69\n34.10\n40.72\n38.39\n46.67\n41.93\n33.33\n34.74\n33.33\n33.33\n33.49\n33.65\n33.86\n34.10\n33.41\n33.41\nNLI\nxnli\nbg\nvalidation\nMT\nMax acc.\n33.33\n33.73\n33.33\n33.45\n34.62\n34.70\n33.49\n33.33\n37.19\n39.44\n33.65\n35.54\n38.31\n48.55\n54.70\n52.53\n48.23\n33.33\n39.80\n34.10\n36.27\n35.02\n34.66\n34.58\n39.20\n37.43\n43.98\nNLI\nxnli\nde\nvalidation\nEN\nMedian acc.\n33.37\n33.37\n34.14\n33.29\n33.45\n33.33\n33.09\n43.94\n36.10\n35.14\n33.94\n41.37\n42.25\n52.93\n60.12\n59.84\n57.03\n37.71\n36.27\n41.12\n40.76\n45.86\n44.22\n41.85\n53.13\n45.18\n46.43\nNLI\nxnli\nde\nvalidation\nEN\nMax acc.\n36.10\n34.02\n35.02\n33.65\n35.18\n34.26\n33.65\n45.90\n40.52\n35.50\n35.78\n42.41\n44.18\n54.78\n60.64\n60.16\n58.59\n39.36\n40.12\n42.73\n45.26\n46.83\n48.92\n47.03\n54.38\n53.69\n50.16\nNLI\nxnli\nde\nvalidation\nMT\nMedian acc.\n33.13\n33.29\n33.37\n33.37\n33.33\n33.41\n33.41\n33.94\n34.30\n39.32\n33.33\n33.33\n33.41\n37.87\n37.51\n35.26\n33.41\n33.41\n34.78\n33.61\n34.26\n33.69\n33.86\n35.70\n39.56\n36.79\n39.32\nNLI\nxnli\nde\nvalidation\nMT\nMax acc.\n36.06\n33.45\n33.45\n33.65\n34.54\n34.58\n36.39\n40.08\n35.74\n41.45\n33.86\n33.41\n34.54\n47.47\n51.37\n45.82\n50.56\n35.50\n35.58\n38.27\n37.63\n36.22\n37.99\n36.47\n42.13\n38.96\n44.70\nNLI\nxnli\nel\nvalidation\nEN\nMedian acc.\n33.29\n33.33\n33.45\n33.37\n33.33\n33.33\n33.49\n33.57\n34.50\n34.82\n34.62\n39.76\n41.57\n52.57\n58.80\n58.88\n56.10\n37.95\n35.50\n38.43\n40.36\n41.08\n40.92\n40.68\n45.94\n42.29\n39.12\nNLI\nxnli\nel\nvalidation\nEN\nMax acc.\n36.83\n33.90\n34.02\n34.58\n35.54\n34.42\n33.73\n34.10\n40.08\n37.31\n37.43\n40.92\n43.94\n53.78\n59.00\n59.20\n57.35\n40.96\n39.32\n41.81\n42.61\n41.53\n42.89\n41.89\n47.43\n46.55\n43.05\nNLI\nxnli\nel\nvalidation\nMT\nMedian acc.\n33.33\n33.33\n33.33\n33.33\n33.33\n33.37\n33.33\n33.33\n33.53\n34.70\n33.21\n33.33\n33.33\n33.29\n45.06\n34.78\n33.49\n33.33\n33.33\n33.41\n33.33\n33.78\n33.33\n33.37\n34.82\n34.74\n33.61\nNLI\nxnli\nel\nvalidation\nMT\nMax acc.\n34.22\n33.41\n34.62\n33.33\n33.37\n34.90\n33.37\n34.70\n39.08\n36.22\n33.73\n33.33\n39.28\n34.98\n51.24\n42.37\n35.58\n33.49\n33.37\n35.50\n33.37\n34.82\n34.38\n33.94\n37.19\n37.43\n35.70\nNLI\nxnli\nen\nvalidation\nEN\nMedian acc.\n33.49\n34.38\n33.61\n33.61\n33.57\n33.29\n33.49\n59.12\n36.79\n35.66\n34.10\n43.13\n40.60\n55.18\n61.24\n61.93\n60.36\n44.62\n39.04\n49.84\n52.21\n55.38\n54.38\n46.35\n60.92\n57.47\n55.02\nNLI\nxnli\nen\nvalidation\nEN\nMax acc.\n36.79\n35.34\n34.74\n35.22\n37.83\n33.45\n36.02\n60.16\n41.00\n36.83\n38.47\n43.78\n44.26\n56.83\n62.01\n62.25\n61.00\n46.43\n47.11\n55.02\n57.31\n59.68\n58.92\n55.90\n67.47\n61.81\n59.72\nNLI\nxnli\nes\nvalidation\nEN\nMedian acc.\n33.33\n33.37\n33.33\n33.49\n33.90\n33.86\n34.10\n45.70\n34.26\n35.58\n34.26\n40.36\n42.49\n51.97\n60.32\n60.72\n58.03\n43.90\n41.24\n46.91\n50.32\n52.53\n46.10\n45.34\n58.51\n43.98\n52.09\nNLI\nxnli\nes\nvalidation\nEN\nMax acc.\n33.41\n34.38\n33.98\n33.86\n35.98\n35.94\n39.48\n48.67\n38.96\n36.27\n36.75\n41.93\n45.34\n54.78\n60.80\n60.92\n59.40\n44.98\n47.55\n52.97\n56.14\n55.10\n57.35\n53.73\n61.24\n59.12\n59.32\nNLI\nxnli\nes\nvalidation\nHT\nMedian acc.\n33.37\n33.33\n33.33\n33.45\n33.33\n33.33\n33.33\n33.33\n34.66\n36.75\n33.33\n33.41\n33.37\n35.66\n38.76\n54.74\n33.33\n33.37\n34.10\n33.33\n33.45\n33.33\n37.63\n33.33\n33.37\n46.55\n33.37\nNLI\nxnli\nes\nvalidation\nHT\nMax acc.\n33.49\n34.86\n33.33\n34.86\n34.94\n33.37\n34.94\n43.05\n35.22\n42.05\n33.90\n33.78\n36.02\n39.04\n58.15\n60.76\n51.65\n37.23\n38.96\n48.03\n53.09\n48.76\n53.13\n52.97\n56.83\n56.99\n58.76\nNLI\nxnli\nes\nvalidation\nMT\nMedian acc.\n33.45\n33.33\n33.33\n33.33\n33.33\n33.33\n33.61\n34.22\n33.82\n36.27\n33.29\n33.33\n33.37\n33.53\n45.54\n55.34\n33.45\n33.33\n33.45\n33.33\n33.45\n33.33\n39.80\n34.50\n33.37\n49.52\n33.90\nNLI\nxnli\nes\nvalidation\nMT\nMax acc.\n34.22\n35.38\n34.10\n34.90\n34.02\n35.38\n36.14\n37.27\n40.08\n39.00\n33.53\n33.57\n34.14\n47.67\n55.38\n60.12\n47.23\n43.21\n34.46\n43.01\n51.24\n55.10\n53.86\n52.21\n57.71\n53.82\n58.96\nNLI\nxnli\nfr\nvalidation\nEN\nMedian acc.\n33.33\n33.82\n33.90\n33.45\n34.50\n34.46\n33.45\n45.38\n35.78\n35.26\n34.94\n40.36\n40.36\n52.37\n59.52\n59.56\n58.15\n44.22\n42.65\n48.59\n50.52\n52.41\n51.00\n47.39\n57.71\n53.69\n51.81\nNLI\nxnli\nfr\nvalidation\nEN\nMax acc.\n35.46\n35.26\n34.86\n33.90\n37.47\n37.27\n36.95\n46.71\n36.75\n35.94\n37.15\n42.45\n42.01\n54.22\n59.88\n59.88\n58.47\n45.54\n48.51\n52.21\n55.78\n55.26\n56.67\n53.37\n61.37\n59.12\n57.99\nNLI\nxnli\nfr\nvalidation\nHT\nMedian acc.\n33.49\n33.82\n33.53\n34.82\n34.22\n33.98\n33.90\n33.61\n35.26\n36.06\n33.41\n33.98\n34.58\n35.90\n39.76\n55.86\n33.94\n33.33\n35.14\n37.59\n34.58\n46.55\n48.92\n42.73\n49.52\n51.53\n48.59\nNLI\nxnli\nfr\nvalidation\nHT\nMax acc.\n36.27\n34.02\n35.70\n35.50\n35.82\n34.34\n35.06\n47.59\n41.45\n36.83\n34.14\n38.92\n37.79\n47.31\n57.07\n58.80\n50.92\n43.53\n47.75\n46.83\n51.89\n53.21\n53.45\n47.11\n58.47\n56.95\n55.50\nNLI\nxnli\nfr\nvalidation\nMT\nMedian acc.\n34.10\n33.33\n33.33\n33.90\n32.69\n33.45\n33.49\n33.61\n37.07\n35.58\n33.53\n34.78\n34.38\n35.10\n37.79\n55.66\n34.98\n33.73\n34.26\n37.83\n36.67\n41.41\n47.23\n41.85\n46.95\n52.01\n49.00\nNLI\nxnli\nfr\nvalidation\nMT\nMax acc.\n34.98\n34.54\n35.22\n34.98\n33.49\n34.70\n35.66\n34.46\n43.37\n35.90\n35.26\n38.27\n36.51\n44.02\n57.83\n58.71\n50.32\n40.48\n46.35\n42.13\n51.41\n48.84\n52.41\n47.91\n57.91\n54.82\n55.86\nNLI\nxnli\nhi\nvalidation\nEN\nMedian acc.\n33.33\n33.41\n33.49\n34.54\n33.78\n33.33\n33.53\n33.33\n34.18\n34.02\n34.18\n38.88\n38.19\n47.87\n56.14\n57.31\n54.94\n39.08\n37.59\n44.54\n46.18\n48.35\n44.70\n41.41\n53.53\n44.38\n45.78\nNLI\nxnli\nhi\nvalidation\nEN\nMax acc.\n34.86\n34.94\n34.34\n35.10\n37.07\n35.38\n36.75\n33.65\n39.48\n34.86\n35.38\n39.76\n41.89\n50.24\n57.23\n57.47\n55.46\n41.81\n42.89\n48.07\n51.49\n50.88\n53.45\n49.84\n56.83\n52.53\n55.02\nNLI\nxnli\nhi\nvalidation\nHT\nMedian acc.\n33.41\n33.33\n33.29\n33.33\n33.33\n33.33\n34.18\n33.33\n34.22\n34.22\n33.29\n36.79\n37.59\n39.24\n47.99\n44.78\n34.62\n37.27\n35.50\n36.59\n34.50\n40.56\n37.31\n43.90\n44.54\n49.64\n47.15\nNLI\nxnli\nhi\nvalidation\nHT\nMax acc.\n34.62\n34.46\n34.14\n33.33\n33.37\n33.41\n35.98\n35.22\n38.67\n37.23\n34.94\n37.99\n41.69\n48.67\n56.06\n56.39\n53.41\n40.32\n41.12\n41.24\n43.41\n42.61\n47.03\n49.28\n60.20\n52.37\n52.21\nNLI\nxnli\nhi\nvalidation\nMT\nMedian acc.\n33.29\n33.37\n33.33\n33.33\n33.45\n33.37\n34.22\n33.33\n35.22\n33.33\n33.37\n34.86\n34.26\n41.61\n47.59\n36.39\n33.45\n34.54\n37.39\n36.71\n33.33\n33.94\n34.50\n33.90\n38.51\n44.14\n38.07\nNLI\nxnli\nhi\nvalidation\nMT\nMax acc.\n33.73\n33.98\n33.45\n34.14\n33.61\n33.45\n36.22\n33.33\n38.31\n36.91\n34.30\n36.79\n39.36\n47.23\n50.24\n48.59\n33.69\n37.15\n39.04\n39.08\n36.18\n36.27\n36.55\n39.88\n41.61\n49.32\n43.94\nNLI\nxnli\nru\nvalidation\nEN\nMedian acc.\n33.33\n33.33\n33.37\n33.33\n33.61\n33.33\n33.29\n38.15\n33.86\n35.30\n33.73\n41.16\n41.37\n49.84\n57.95\n57.67\n55.10\n36.99\n35.78\n41.45\n43.73\n46.67\n44.62\n43.90\n52.33\n46.75\n44.74\nNLI\nxnli\nru\nvalidation\nEN\nMax acc.\n36.10\n34.34\n35.02\n33.82\n35.10\n35.26\n34.66\n41.24\n38.84\n37.99\n37.35\n41.93\n42.13\n53.09\n58.88\n58.67\n56.55\n39.64\n42.81\n45.10\n47.11\n47.75\n50.24\n46.55\n54.02\n52.85\n50.12\nNLI\nxnli\nru\nvalidation\nMT\nMedian acc.\n33.41\n33.33\n33.33\n33.37\n33.41\n34.14\n33.37\n33.37\n34.10\n36.10\n35.26\n33.57\n33.29\n39.44\n39.72\n33.94\n34.14\n33.33\n33.49\n33.73\n33.29\n33.78\n34.46\n33.94\n42.89\n38.59\n41.49\nNLI\nxnli\nru\nvalidation\nMT\nMax acc.\n33.98\n33.53\n33.61\n33.82\n34.86\n36.79\n33.45\n33.65\n37.63\n38.27\n36.67\n35.86\n35.42\n42.93\n56.71\n48.03\n38.11\n33.86\n33.57\n36.63\n33.57\n38.96\n37.31\n34.94\n45.34\n46.31\n43.49\nNLI\nxnli\nsw\nvalidation\nEN\nMedian acc.\n33.25\n33.82\n33.45\n33.53\n33.98\n33.33\n33.21\n33.73\n34.82\n34.14\n33.94\n38.23\n39.64\n45.78\n55.46\n55.70\n53.25\n37.19\n35.50\n41.20\n41.77\n43.90\n42.29\n36.51\n50.36\n43.98\n37.27\nNLI\nxnli\nsw\nvalidation\nEN\nMax acc.\n34.82\n34.94\n35.46\n34.46\n36.75\n36.55\n33.73\n33.78\n37.79\n35.46\n35.18\n39.68\n40.08\n49.60\n55.66\n56.79\n53.73\n38.35\n41.29\n44.34\n47.83\n46.63\n48.27\n43.49\n52.09\n50.36\n50.04\nNLI\nxnli\nsw\nvalidation\nHT\nMedian acc.\n33.45\n33.33\n33.41\n33.33\n33.33\n33.37\n34.54\n33.94\n35.02\n34.58\n33.41\n33.33\n33.57\n37.75\n41.73\n46.95\n43.25\n34.54\n33.53\n34.02\n33.41\n35.70\n33.61\n34.10\n34.98\n39.40\n34.54\nNLI\nxnli\nsw\nvalidation\nHT\nMax acc.\n35.54\n34.50\n33.45\n33.41\n33.37\n35.02\n35.94\n34.58\n35.42\n37.19\n34.02\n35.46\n36.59\n46.31\n52.37\n49.60\n49.68\n35.14\n33.98\n34.94\n36.10\n35.94\n35.58\n37.19\n37.71\n42.85\n35.02\nNLI\nxnli\nsw\nvalidation\nMT\nMedian acc.\n33.57\n33.33\n33.33\n33.33\n33.33\n33.33\n33.41\n33.33\n33.25\n35.18\n33.33\n33.33\n33.33\n35.98\n33.33\n33.37\n34.58\n33.33\n33.53\n33.57\n32.97\n33.41\n33.37\n33.33\n35.82\n35.10\n33.37\nNLI\nxnli\nsw\nvalidation\nMT\nMax acc.\n34.22\n33.33\n33.45\n34.38\n33.37\n34.62\n35.46\n34.06\n35.02\n37.11\n33.57\n34.82\n34.78\n38.03\n39.64\n37.55\n41.45\n34.34\n34.98\n33.94\n33.53\n37.03\n33.41\n33.41\n36.27\n36.83\n33.57\nNLI\nxnli\nth\nvalidation\nEN\nMedian acc.\n33.37\n33.73\n33.37\n33.41\n33.33\n33.41\n33.33\n33.69\n34.74\n34.58\n33.41\n40.92\n39.68\n45.34\n56.31\n57.19\n54.98\n34.14\n33.61\n38.88\n38.19\n39.00\n39.48\n41.65\n41.45\n41.33\n37.31\nNLI\nxnli\nth\nvalidation\nEN\nMax acc.\n35.22\n33.86\n33.90\n34.46\n34.02\n33.82\n36.31\n34.70\n39.88\n35.46\n37.55\n41.97\n40.80\n52.13\n57.43\n58.03\n56.02\n35.50\n42.93\n40.36\n42.93\n40.12\n41.08\n43.17\n43.78\n43.98\n42.29\nNLI\nxnli\nth\nvalidation\nMT\nMedian acc.\n33.53\n33.33\n33.37\n33.33\n33.33\n33.33\n34.58\n33.33\n34.06\n35.54\n33.25\n35.34\n33.57\n35.78\n38.55\n39.20\n40.32\n33.57\n33.33\n33.29\n33.65\n32.53\n33.78\n33.94\n33.29\n34.10\n34.02\nNLI\nxnli\nth\nvalidation\nMT\nMax acc.\n35.46\n33.69\n33.69\n33.41\n35.18\n36.39\n40.44\n33.33\n35.98\n36.06\n33.57\n35.82\n33.69\n40.84\n52.45\n57.95\n49.08\n34.94\n34.30\n34.22\n33.98\n34.78\n34.90\n35.38\n36.27\n37.63\n36.99\nNLI\nxnli\ntr\nvalidation\nEN\nMedian acc.\n33.33\n33.33\n33.33\n33.33\n33.37\n33.33\n34.22\n34.14\n35.26\n34.62\n34.26\n39.16\n40.76\n48.71\n56.75\n56.39\n54.34\n36.95\n33.98\n35.46\n36.06\n36.14\n37.47\n38.31\n42.73\n39.76\n39.08\nNLI\nxnli\ntr\nvalidation\nEN\nMax acc.\n36.51\n33.73\n33.57\n33.37\n33.41\n35.18\n34.66\n34.90\n40.24\n36.79\n36.51\n40.28\n41.29\n50.56\n57.59\n57.67\n55.38\n37.31\n37.51\n37.15\n37.23\n37.55\n38.71\n40.44\n45.70\n43.78\n43.78\nNLI\nxnli\ntr\nvalidation\nMT\nMedian acc.\n33.33\n33.33\n33.41\n33.33\n33.33\n33.21\n33.37\n33.49\n34.34\n34.02\n33.29\n33.21\n33.94\n34.06\n38.80\n38.67\n38.67\n33.49\n33.33\n33.33\n33.25\n33.33\n33.37\n34.46\n33.33\n33.37\n33.82\nNLI\nxnli\ntr\nvalidation\nMT\nMax acc.\n33.45\n33.41\n34.62\n34.34\n33.98\n33.49\n34.02\n34.02\n34.86\n38.80\n34.26\n35.94\n34.46\n37.63\n48.35\n54.98\n46.99\n36.67\n33.61\n35.34\n34.46\n33.73\n33.98\n36.18\n37.27\n37.27\n40.88\nNLI\nxnli\nur\nvalidation\nEN\nMedian acc.\n33.05\n33.61\n33.37\n33.69\n33.41\n34.42\n34.02\n33.13\n33.29\n34.14\n34.50\n36.59\n37.07\n46.67\n54.70\n54.58\n53.57\n36.67\n35.26\n39.88\n42.89\n43.94\n40.84\n40.12\n49.96\n45.86\n40.28\nNLI\nxnli\nur\nvalidation\nEN\nMax acc.\n34.10\n33.69\n34.58\n34.78\n34.30\n35.82\n34.26\n33.33\n38.43\n34.62\n35.78\n38.71\n39.80\n47.91\n55.42\n55.98\n54.02\n38.96\n41.37\n44.38\n49.04\n46.51\n49.48\n45.18\n50.80\n51.24\n51.81\nNLI\nxnli\nur\nvalidation\nHT\nMedian acc.\n33.90\n33.61\n33.09\n33.53\n32.93\n31.69\n33.69\n33.25\n34.86\n34.10\n34.30\n34.70\n35.74\n35.50\n48.92\n45.42\n35.66\n34.18\n33.37\n37.95\n38.35\n34.26\n35.62\n36.10\n34.62\n40.44\n38.92\nNLI\nxnli\nur\nvalidation\nHT\nMax acc.\n34.98\n34.06\n33.37\n33.78\n33.53\n33.13\n35.06\n33.53\n35.78\n35.86\n35.14\n37.23\n39.88\n41.12\n52.17\n53.82\n46.87\n36.79\n33.86\n39.92\n41.49\n41.77\n40.00\n37.67\n41.77\n46.39\n46.95\nNLI\nxnli\nur\nvalidation\nMT\nMedian acc.\n33.33\n33.33\n33.33\n33.25\n33.25\n33.25\n33.29\n33.45\n33.33\n33.49\n33.61\n35.02\n33.49\n40.08\n40.32\n36.10\n33.49\n34.14\n33.33\n33.29\n33.33\n33.33\n33.37\n33.37\n33.33\n33.82\n36.67\nNLI\nxnli\nur\nvalidation\nMT\nMax acc.\n34.02\n33.45\n33.45\n33.53\n33.29\n33.33\n34.66\n33.57\n34.06\n34.78\n34.22\n35.46\n38.88\n42.69\n53.78\n51.81\n49.80\n36.02\n33.33\n33.33\n33.45\n35.66\n33.78\n37.51\n35.50\n35.86\n37.87\nNLI\nxnli\nvi\nvalidation\nEN\nMedian acc.\n33.33\n34.42\n33.37\n33.45\n33.57\n33.69\n33.49\n34.70\n34.98\n34.58\n35.46\n39.60\n39.76\n52.45\n58.19\n58.35\n56.39\n43.65\n40.52\n46.35\n46.22\n50.08\n46.51\n43.61\n55.78\n48.27\n49.80\nNLI\nxnli\nvi\nvalidation\nEN\nMax acc.\n37.79\n35.26\n34.54\n34.82\n37.91\n37.19\n33.73\n35.58\n38.27\n35.14\n36.95\n40.20\n41.81\n53.21\n58.51\n58.92\n57.11\n44.74\n47.19\n51.08\n53.98\n52.93\n54.50\n51.97\n61.00\n55.82\n57.27\nNLI\nxnli\nvi\nvalidation\nHT\nMedian acc.\n33.41\n33.05\n33.41\n33.29\n33.37\n32.97\n33.73\n33.21\n34.02\n37.63\n33.13\n33.78\n34.98\n41.37\n43.57\n33.45\n45.78\n37.23\n37.79\n35.94\n40.92\n41.24\n50.28\n33.57\n39.60\n46.55\n33.61\nNLI\nxnli\nvi\nvalidation\nHT\nMax acc.\n34.86\n33.53\n33.61\n33.78\n34.14\n33.53\n34.46\n33.25\n37.51\n38.63\n33.61\n37.99\n39.56\n46.31\n56.14\n56.75\n49.24\n39.20\n42.21\n44.14\n43.29\n47.59\n52.65\n39.76\n46.99\n54.82\n48.03\nNLI\nxnli\nvi\nvalidation\nMT\nMedian acc.\n33.33\n33.33\n33.33\n33.33\n33.33\n33.33\n33.33\n33.33\n33.41\n33.98\n33.33\n33.33\n33.33\n33.33\n33.78\n33.33\n33.73\n33.57\n33.33\n33.33\n33.33\n33.33\n33.33\n33.33\n33.33\n33.33\n33.33\nNLI\nxnli\nvi\nvalidation\nMT\nMax acc.\n34.62\n33.41\n33.82\n34.66\n34.14\n34.02\n35.02\n34.06\n33.69\n34.70\n33.37\n33.94\n34.34\n33.53\n36.79\n33.94\n35.82\n37.03\n34.66\n34.42\n33.57\n33.94\n38.88\n34.26\n39.20\n33.33\n33.90\nNLI\nxnli\nzh\nvalidation\nEN\nMedian acc.\n33.41\n35.06\n33.41\n33.33\n35.54\n34.10\n33.73\n33.33\n34.62\n34.74\n36.14\n41.37\n38.92\n44.54\n57.11\n57.39\n53.90\n43.94\n39.40\n48.11\n47.71\n51.24\n47.07\n47.39\n56.22\n44.02\n48.47\nNLI\nxnli\nzh\nvalidation\nEN\nMax acc.\n35.14\n36.31\n34.22\n36.95\n36.67\n37.27\n34.86\n33.82\n41.85\n35.10\n37.07\n42.49\n40.84\n50.64\n59.12\n58.71\n55.22\n44.66\n47.63\n51.12\n54.18\n53.61\n54.30\n52.29\n56.91\n55.50\n56.95\nNLI\nxnli\nzh\nvalidation\nHT\nMedian acc.\n33.45\n33.33\n33.33\n33.33\n33.33\n33.37\n33.53\n33.33\n33.25\n34.70\n33.13\n33.78\n34.10\n40.24\n46.83\n50.40\n39.00\n34.70\n33.37\n33.45\n33.69\n35.38\n34.46\n39.32\n33.69\n36.95\n52.89\nNLI\nxnli\nzh\nvalidation\nHT\nMax acc.\n33.86\n33.37\n33.45\n33.57\n34.74\n35.02\n36.91\n33.33\n37.63\n37.51\n34.02\n35.22\n35.66\n41.89\n55.98\n56.99\n51.49\n37.43\n38.96\n34.06\n42.09\n44.10\n40.64\n45.10\n41.37\n49.24\n53.69\nNLI\nxnli\nzh\nvalidation\nMT\nMedian acc.\n33.69\n33.25\n33.25\n32.61\n34.38\n33.41\n33.82\n33.33\n34.42\n34.46\n32.85\n33.69\n34.54\n36.31\n48.92\n54.86\n33.33\n33.82\n33.98\n34.30\n34.06\n34.98\n46.35\n35.10\n34.38\n49.56\n38.96\nNLI\nxnli\nzh\nvalidation\nMT\nMax acc.\n35.62\n33.49\n33.45\n33.86\n34.54\n34.02\n34.70\n33.33\n36.63\n35.30\n34.10\n34.82\n35.90\n39.12\n51.49\n56.87\n39.28\n35.58\n36.95\n36.63\n39.16\n42.41\n48.92\n39.60\n50.60\n52.25\n44.14\nProgram synthesis\nopenai_humaneval\nNone\ntest\nEN\nPass@1\n-\n0.82\n2.48\n4.03\n6.48\n7.73\n15.52\n-\n-\n-\n-\n-\n-\n0.00\n-\n-\n-\n2.18\n2.62\n4.38\n6.29\n8.06\n7.23\n1.55\n12.06\n13.55\n6.13\nProgram synthesis\nopenai_humaneval\nNone\ntest\nEN\nPass@10\n-\n3.02\n5.93\n7.45\n11.35\n17.38\n32.20\n-\n-\n-\n-\n-\n-\n0.00\n-\n-\n-\n4.11\n6.22\n8.73\n11.94\n15.03\n14.46\n4.12\n26.53\n26.26\n11.79\nProgram synthesis\nopenai_humaneval\nNone\ntest\nEN\nPass@100\n-\n6.23\n9.62\n12.75\n20.43\n29.47\n55.45\n-\n-\n-\n-\n-\n-\n0.00\n-\n-\n-\n9.00\n11.68\n16.09\n19.06\n27.49\n25.86\n9.60\n48.44\n47.01\n18.73\nSent. completion\nstory_cloze\n2016\nvalidation\nEN\nMedian acc.\n51.68\n50.08\n47.25\n48.48\n47.51\n49.44\n50.99\n94.71\n46.71\n48.21\n52.38\n57.14\n58.69\n77.61\n95.40\n93.85\n96.31\n58.52\n59.01\n79.64\n85.20\n89.10\n88.51\n84.66\n95.67\n95.83\n94.01\nSent. completion\nstory_cloze\n2016\nvalidation\nEN\nMax acc.\n66.27\n59.43\n62.05\n64.30\n66.44\n70.92\n76.22\n94.92\n52.27\n57.08\n54.36\n57.83\n59.49\n79.10\n96.04\n94.66\n96.63\n60.29\n62.75\n82.90\n87.33\n90.43\n89.58\n87.07\n96.26\n96.69\n94.66\nSent. completion\nsuper_glue\ncopa\nvalidation\nEN\nMedian acc.\n55.00\n55.00\n57.00\n54.00\n62.00\n69.00\n55.00\n93.00\n53.00\n58.00\n51.00\n53.00\n65.00\n66.00\n90.00\n86.00\n90.00\n51.00\n58.00\n66.00\n73.00\n83.00\n80.00\n78.00\n88.00\n90.00\n89.00\nSent. completion\nsuper_glue\ncopa\nvalidation\nEN\nMax acc.\n67.00\n67.00\n62.00\n65.00\n66.00\n78.00\n75.00\n94.00\n54.00\n66.00\n57.00\n55.00\n65.00\n72.00\n93.00\n88.00\n91.00\n52.00\n63.00\n69.00\n76.00\n86.00\n84.00\n81.00\n91.00\n91.00\n91.00\nSent. completion\nxcopa\net\nvalidation\nEN\nMedian acc.\n57.00\n53.00\n50.00\n53.00\n50.00\n53.00\n51.00\n53.00\n52.00\n51.00\n53.00\n49.00\n53.00\n65.00\n72.00\n79.00\n75.00\n48.00\n49.00\n48.00\n50.00\n49.00\n51.00\n52.00\n48.00\n52.00\n49.00\nSent. completion\nxcopa\net\nvalidation\nEN\nMax acc.\n58.00\n58.00\n56.00\n57.00\n57.00\n57.00\n52.00\n55.00\n56.00\n61.00\n57.00\n51.00\n56.00\n70.00\n75.00\n81.00\n79.00\n53.00\n55.00\n50.00\n51.00\n50.00\n51.00\n57.00\n50.00\n54.00\n53.00\nSent. completion\nxcopa\net\nvalidation\nMT\nMedian acc.\n56.00\n56.00\n54.00\n51.00\n53.00\n53.00\n46.00\n49.00\n55.00\n56.00\n53.00\n50.00\n54.00\n60.00\n74.00\n76.00\n75.00\n48.00\n53.00\n47.00\n49.00\n47.00\n47.00\n48.00\n49.00\n48.00\n46.00\nSent. completion\nxcopa\net\nvalidation\nMT\nMax acc.\n57.00\n60.00\n58.00\n55.00\n61.00\n56.00\n54.00\n54.00\n58.00\n64.00\n56.00\n52.00\n55.00\n69.00\n79.00\n79.00\n77.00\n50.00\n54.00\n48.00\n53.00\n48.00\n52.00\n52.00\n51.00\n52.00\n53.00\nSent. completion\nxcopa\nht\nvalidation\nEN\nMedian acc.\n52.00\n48.00\n48.00\n52.00\n53.00\n52.00\n56.00\n47.00\n53.00\n47.00\n55.00\n55.00\n59.00\n60.00\n76.00\n76.00\n75.00\n42.00\n46.00\n48.00\n52.00\n49.00\n51.00\n51.00\n55.00\n51.00\n51.00\nSent. completion\nxcopa\nht\nvalidation\nEN\nMax acc.\n53.00\n53.00\n56.00\n56.00\n57.00\n59.00\n57.00\n51.00\n63.00\n54.00\n60.00\n59.00\n62.00\n66.00\n79.00\n79.00\n77.00\n49.00\n52.00\n51.00\n62.00\n54.00\n52.00\n55.00\n58.00\n55.00\n56.00\nSent. completion\nxcopa\nht\nvalidation\nMT\nMedian acc.\n53.00\n52.00\n54.00\n50.00\n55.00\n53.00\n57.00\n49.00\n58.00\n45.00\n54.00\n55.00\n54.00\n60.00\n72.00\n75.00\n73.00\n45.00\n44.00\n47.00\n49.00\n50.00\n51.00\n54.00\n53.00\n50.00\n56.00\nSent. completion\nxcopa\nht\nvalidation\nMT\nMax acc.\n57.00\n53.00\n62.00\n57.00\n59.00\n56.00\n66.00\n56.00\n58.00\n52.00\n60.00\n60.00\n58.00\n61.00\n81.00\n78.00\n80.00\n47.00\n51.00\n54.00\n64.00\n52.00\n54.00\n56.00\n56.00\n54.00\n58.00\nSent. completion\nxcopa\nid\nvalidation\nEN\nMedian acc.\n51.00\n53.00\n51.00\n56.00\n50.00\n54.00\n54.00\n55.00\n48.00\n51.00\n56.00\n50.00\n59.00\n65.00\n90.00\n88.00\n84.00\n50.00\n59.00\n58.00\n66.00\n70.00\n70.00\n65.00\n79.00\n83.00\n78.00\nSent. completion\nxcopa\nid\nvalidation\nEN\nMax acc.\n54.00\n56.00\n61.00\n58.00\n53.00\n59.00\n62.00\n58.00\n52.00\n53.00\n58.00\n54.00\n59.00\n70.00\n92.00\n90.00\n86.00\n57.00\n60.00\n61.00\n70.00\n76.00\n73.00\n67.00\n86.00\n87.00\n82.00\nSent. completion\nxcopa\nid\nvalidation\nMT\nMedian acc.\n52.00\n55.00\n51.00\n59.00\n53.00\n53.00\n56.00\n56.00\n47.00\n53.00\n53.00\n53.00\n55.00\n59.00\n90.00\n88.00\n84.00\n53.00\n56.00\n56.00\n60.00\n62.00\n64.00\n64.00\n76.00\n83.00\n75.00\nSent. completion\nxcopa\nid\nvalidation\nMT\nMax acc.\n58.00\n60.00\n60.00\n61.00\n58.00\n57.00\n61.00\n59.00\n51.00\n57.00\n59.00\n55.00\n61.00\n71.00\n91.00\n89.00\n87.00\n54.00\n57.00\n59.00\n64.00\n67.00\n71.00\n70.00\n82.00\n84.00\n87.00\nSent. completion\nxcopa\nit\nvalidation\nEN\nMedian acc.\n49.00\n56.00\n50.00\n46.00\n55.00\n50.00\n54.00\n66.00\n55.00\n54.00\n53.00\n55.00\n60.00\n64.00\n87.00\n85.00\n87.00\n51.00\n51.00\n45.00\n50.00\n59.00\n57.00\n57.00\n72.00\n69.00\n69.00\nSent. completion\nxcopa\nit\nvalidation\nEN\nMax acc.\n52.00\n59.00\n55.00\n55.00\n60.00\n55.00\n56.00\n68.00\n58.00\n55.00\n57.00\n61.00\n61.00\n69.00\n90.00\n88.00\n90.00\n52.00\n55.00\n48.00\n53.00\n61.00\n62.00\n60.00\n74.00\n72.00\n74.00\nSent. completion\nxcopa\nit\nvalidation\nMT\nMedian acc.\n53.00\n53.00\n53.00\n45.00\n54.00\n52.00\n54.00\n63.00\n53.00\n56.00\n57.00\n54.00\n59.00\n66.00\n84.00\n84.00\n85.00\n49.00\n54.00\n43.00\n48.00\n55.00\n57.00\n55.00\n69.00\n69.00\n68.00\nSent. completion\nxcopa\nit\nvalidation\nMT\nMax acc.\n55.00\n58.00\n55.00\n48.00\n57.00\n55.00\n57.00\n72.00\n59.00\n57.00\n59.00\n56.00\n63.00\n70.00\n88.00\n86.00\n88.00\n52.00\n56.00\n49.00\n51.00\n57.00\n60.00\n58.00\n73.00\n74.00\n71.00\nSent. completion\nxcopa\nqu\nvalidation\nEN\nMedian acc.\n59.00\n54.00\n48.00\n56.00\n59.00\n61.00\n56.00\n52.00\n48.00\n48.00\n47.00\n52.00\n54.00\n53.00\n58.00\n54.00\n48.00\n54.00\n44.00\n52.00\n51.00\n45.00\n48.00\n50.00\n49.00\n51.00\n51.00\nSent. completion\nxcopa\nqu\nvalidation\nEN\nMax acc.\n61.00\n56.00\n56.00\n58.00\n59.00\n65.00\n59.00\n58.00\n55.00\n53.00\n51.00\n53.00\n57.00\n55.00\n58.00\n56.00\n49.00\n55.00\n50.00\n56.00\n56.00\n60.00\n60.00\n54.00\n56.00\n53.00\n54.00\nSent. completion\nxcopa\nqu\nvalidation\nMT\nMedian acc.\n60.00\n49.00\n50.00\n55.00\n52.00\n60.00\n51.00\n57.00\n47.00\n53.00\n49.00\n53.00\n54.00\n54.00\n56.00\n54.00\n53.00\n53.00\n47.00\n50.00\n45.00\n51.00\n46.00\n50.00\n49.00\n50.00\n51.00\nSent. completion\nxcopa\nqu\nvalidation\nMT\nMax acc.\n63.00\n60.00\n58.00\n57.00\n55.00\n63.00\n60.00\n60.00\n51.00\n55.00\n54.00\n55.00\n56.00\n56.00\n59.00\n55.00\n56.00\n55.00\n56.00\n54.00\n50.00\n60.00\n61.00\n51.00\n53.00\n56.00\n57.00\nSent. completion\nxcopa\nsw\nvalidation\nEN\nMedian acc.\n50.00\n51.00\n52.00\n53.00\n48.00\n52.00\n58.00\n57.00\n45.00\n47.00\n50.00\n54.00\n53.00\n48.00\n70.00\n76.00\n71.00\n52.00\n56.00\n53.00\n55.00\n47.00\n54.00\n48.00\n60.00\n64.00\n56.00\nSent. completion\nxcopa\nsw\nvalidation\nEN\nMax acc.\n56.00\n59.00\n61.00\n61.00\n58.00\n59.00\n65.00\n58.00\n58.00\n50.00\n53.00\n59.00\n54.00\n53.00\n73.00\n81.00\n74.00\n55.00\n64.00\n55.00\n66.00\n63.00\n60.00\n58.00\n64.00\n66.00\n58.00\nSent. completion\nxcopa\nsw\nvalidation\nMT\nMedian acc.\n53.00\n49.00\n49.00\n49.00\n53.00\n53.00\n60.00\n57.00\n46.00\n47.00\n50.00\n54.00\n51.00\n52.00\n77.00\n76.00\n72.00\n53.00\n57.00\n53.00\n54.00\n59.00\n59.00\n56.00\n63.00\n62.00\n58.00\nSent. completion\nxcopa\nsw\nvalidation\nMT\nMax acc.\n57.00\n60.00\n62.00\n59.00\n57.00\n55.00\n62.00\n59.00\n54.00\n52.00\n55.00\n58.00\n54.00\n53.00\n79.00\n78.00\n75.00\n56.00\n61.00\n57.00\n62.00\n60.00\n61.00\n62.00\n64.00\n67.00\n61.00\nSent. completion\nxcopa\nta\nvalidation\nEN\nMedian acc.\n52.00\n48.00\n54.00\n53.00\n58.00\n54.00\n53.00\n55.00\n57.00\n50.00\n57.00\n60.00\n59.00\n60.00\n84.00\n78.00\n79.00\n48.00\n54.00\n52.00\n55.00\n55.00\n59.00\n69.00\n67.00\n66.00\n66.00\nSent. completion\nxcopa\nta\nvalidation\nEN\nMax acc.\n61.00\n55.00\n54.00\n59.00\n61.00\n56.00\n63.00\n62.00\n61.00\n59.00\n59.00\n63.00\n61.00\n62.00\n84.00\n79.00\n84.00\n50.00\n57.00\n56.00\n59.00\n57.00\n62.00\n71.00\n69.00\n70.00\n69.00\nSent. completion\nxcopa\nta\nvalidation\nMT\nMedian acc.\n54.00\n44.00\n55.00\n53.00\n57.00\n56.00\n59.00\n55.00\n50.00\n52.00\n57.00\n60.00\n61.00\n55.00\n77.00\n74.00\n71.00\n46.00\n52.00\n50.00\n54.00\n61.00\n56.00\n63.00\n62.00\n63.00\n63.00\nSent. completion\nxcopa\nta\nvalidation\nMT\nMax acc.\n58.00\n55.00\n60.00\n55.00\n62.00\n57.00\n68.00\n58.00\n60.00\n62.00\n58.00\n61.00\n62.00\n64.00\n80.00\n81.00\n82.00\n58.00\n55.00\n54.00\n57.00\n64.00\n60.00\n66.00\n64.00\n64.00\n69.00\nSent. completion\nxcopa\nth\nvalidation\nEN\nMedian acc.\n54.00\n53.00\n53.00\n54.00\n52.00\n50.00\n55.00\n55.00\n52.00\n55.00\n60.00\n50.00\n51.00\n56.00\n73.00\n71.00\n74.00\n54.00\n55.00\n56.00\n54.00\n57.00\n55.00\n56.00\n54.00\n51.00\n51.00\nSent. completion\nxcopa\nth\nvalidation\nEN\nMax acc.\n55.00\n57.00\n53.00\n56.00\n54.00\n52.00\n59.00\n55.00\n56.00\n57.00\n65.00\n51.00\n53.00\n60.00\n74.00\n74.00\n77.00\n58.00\n59.00\n60.00\n55.00\n57.00\n61.00\n63.00\n58.00\n53.00\n59.00\nSent. completion\nxcopa\nth\nvalidation\nMT\nMedian acc.\n53.00\n52.00\n52.00\n52.00\n48.00\n45.00\n55.00\n55.00\n52.00\n58.00\n56.00\n51.00\n53.00\n54.00\n71.00\n72.00\n72.00\n54.00\n50.00\n56.00\n55.00\n51.00\n51.00\n55.00\n52.00\n51.00\n54.00\nSent. completion\nxcopa\nth\nvalidation\nMT\nMax acc.\n55.00\n54.00\n59.00\n55.00\n52.00\n54.00\n57.00\n55.00\n58.00\n63.00\n59.00\n55.00\n57.00\n58.00\n77.00\n76.00\n76.00\n57.00\n58.00\n59.00\n63.00\n56.00\n56.00\n56.00\n57.00\n53.00\n61.00\nSent. completion\nxcopa\ntr\nvalidation\nEN\nMedian acc.\n48.00\n56.00\n54.00\n51.00\n52.00\n49.00\n49.00\n49.00\n48.00\n49.00\n51.00\n55.00\n47.00\n55.00\n73.00\n73.00\n74.00\n55.00\n47.00\n55.00\n53.00\n49.00\n55.00\n53.00\n54.00\n51.00\n54.00\nSent. completion\nxcopa\ntr\nvalidation\nEN\nMax acc.\n56.00\n60.00\n55.00\n55.00\n55.00\n52.00\n50.00\n51.00\n51.00\n56.00\n58.00\n58.00\n49.00\n57.00\n79.00\n76.00\n76.00\n58.00\n55.00\n59.00\n58.00\n53.00\n56.00\n55.00\n57.00\n54.00\n55.00\nSent. completion\nxcopa\ntr\nvalidation\nMT\nMedian acc.\n53.00\n55.00\n50.00\n50.00\n48.00\n42.00\n51.00\n49.00\n50.00\n49.00\n52.00\n56.00\n50.00\n55.00\n77.00\n73.00\n73.00\n55.00\n52.00\n58.00\n54.00\n44.00\n48.00\n54.00\n51.00\n50.00\n55.00\nSent. completion\nxcopa\ntr\nvalidation\nMT\nMax acc.\n56.00\n57.00\n56.00\n58.00\n54.00\n49.00\n55.00\n53.00\n56.00\n58.00\n57.00\n60.00\n57.00\n58.00\n79.00\n76.00\n74.00\n61.00\n54.00\n59.00\n61.00\n48.00\n51.00\n58.00\n54.00\n53.00\n56.00\nSent. completion\nxcopa\nvi\nvalidation\nEN\nMedian acc.\n51.00\n60.00\n50.00\n59.00\n54.00\n59.00\n56.00\n49.00\n51.00\n54.00\n43.00\n51.00\n57.00\n62.00\n85.00\n83.00\n82.00\n56.00\n58.00\n68.00\n73.00\n78.00\n71.00\n65.00\n84.00\n84.00\n74.00\nSent. completion\nxcopa\nvi\nvalidation\nEN\nMax acc.\n56.00\n62.00\n59.00\n64.00\n57.00\n62.00\n59.00\n53.00\n52.00\n61.00\n54.00\n52.00\n63.00\n68.00\n87.00\n85.00\n84.00\n61.00\n63.00\n70.00\n77.00\n79.00\n72.00\n67.00\n87.00\n91.00\n79.00\nSent. completion\nxcopa\nvi\nvalidation\nMT\nMedian acc.\n52.00\n61.00\n52.00\n57.00\n59.00\n62.00\n57.00\n61.00\n50.00\n57.00\n48.00\n50.00\n57.00\n60.00\n87.00\n84.00\n81.00\n54.00\n57.00\n63.00\n64.00\n68.00\n72.00\n61.00\n82.00\n85.00\n76.00\nSent. completion\nxcopa\nvi\nvalidation\nMT\nMax acc.\n57.00\n66.00\n65.00\n65.00\n69.00\n67.00\n68.00\n65.00\n54.00\n61.00\n52.00\n52.00\n63.00\n64.00\n88.00\n84.00\n83.00\n57.00\n57.00\n67.00\n65.00\n74.00\n77.00\n64.00\n84.00\n89.00\n81.00\nSent. completion\nxcopa\nzh\nvalidation\nEN\nMedian acc.\n55.00\n51.00\n49.00\n57.00\n51.00\n60.00\n56.00\n55.00\n51.00\n53.00\n53.00\n53.00\n56.00\n63.00\n85.00\n83.00\n79.00\n55.00\n55.00\n62.00\n72.00\n76.00\n77.00\n72.00\n86.00\n84.00\n80.00\nSent. completion\nxcopa\nzh\nvalidation\nEN\nMax acc.\n58.00\n61.00\n63.00\n73.00\n66.00\n68.00\n72.00\n55.00\n52.00\n65.00\n54.00\n58.00\n58.00\n65.00\n89.00\n86.00\n79.00\n61.00\n61.00\n66.00\n73.00\n80.00\n80.00\n77.00\n90.00\n86.00\n82.00\nSent. completion\nxcopa\nzh\nvalidation\nHT\nMedian acc.\n-\n-\n-\n-\n-\n-\n-\n-\n-\n-\n-\n-\n-\n-\n-\n-\n-\n-\n-\n-\n-\n76.00\n-\n75.00\n-\n-\n-\nSent. completion\nxcopa\nzh\nvalidation\nHT\nMax acc.\n-\n-\n-\n-\n-\n-\n-\n-\n-\n-\n-\n-\n-\n-\n-\n-\n-\n-\n-\n-\n-\n78.00\n-\n79.00\n-\n-\n-\nSent. completion\nxcopa\nzh\nvalidation\nMT\nMedian acc.\n54.00\n52.00\n49.00\n57.00\n52.00\n61.00\n53.00\n55.00\n50.00\n56.00\n48.00\n51.00\n55.00\n57.00\n83.00\n83.00\n77.00\n54.00\n54.00\n59.00\n57.00\n72.00\n74.00\n72.00\n86.00\n83.00\n82.00\nSent. completion\nxcopa\nzh\nvalidation\nMT\nMax acc.\n63.00\n62.00\n58.00\n67.00\n66.00\n67.00\n73.00\n55.00\n52.00\n58.00\n56.00\n52.00\n58.00\n59.00\n88.00\n87.00\n79.00\n59.00\n55.00\n67.00\n61.00\n81.00\n80.00\n76.00\n90.00\n86.00\n83.00\nSent. completion\nxstory_cloze\nar\nvalidation\nEN\nMedian acc.\n51.69\n49.44\n49.57\n49.57\n50.23\n50.36\n52.42\n48.11\n47.98\n49.97\n48.11\n53.61\n54.53\n65.92\n85.37\n87.23\n88.29\n52.75\n52.08\n69.49\n78.23\n81.47\n82.13\n75.18\n91.26\n91.59\n91.86\nSent. completion\nxstory_cloze\nar\nvalidation\nEN\nMax acc.\n52.95\n51.36\n52.08\n54.20\n56.25\n59.17\n65.52\n49.64\n49.04\n51.62\n48.71\n54.53\n56.59\n67.50\n90.93\n90.60\n88.95\n53.67\n53.54\n73.33\n80.61\n83.26\n83.79\n77.50\n92.65\n92.92\n92.12\nSent. completion\nxstory_cloze\nar\nvalidation\nMT\nMedian acc.\n51.62\n48.91\n49.11\n49.24\n51.49\n49.37\n52.61\n51.69\n48.71\n51.36\n47.58\n53.08\n54.86\n66.71\n90.54\n90.14\n88.29\n52.55\n52.28\n62.48\n67.24\n80.01\n82.46\n75.12\n91.13\n91.73\n91.79\nSent. completion\nxstory_cloze\nar\nvalidation\nMT\nMax acc.\n53.01\n51.75\n52.48\n54.20\n55.72\n58.37\n65.12\n53.47\n49.90\n53.01\n48.78\n54.20\n55.06\n70.09\n91.07\n91.00\n89.15\n54.40\n53.08\n70.48\n78.03\n83.06\n83.85\n78.69\n92.79\n94.04\n92.46\nSent. completion\nxstory_cloze\nes\nvalidation\nEN\nMedian acc.\n52.28\n50.89\n46.86\n47.72\n48.58\n49.83\n50.56\n81.80\n47.19\n48.11\n53.28\n54.33\n55.33\n73.73\n89.94\n89.68\n93.32\n55.33\n55.99\n69.56\n81.67\n86.76\n86.76\n78.89\n93.18\n94.77\n92.85\nSent. completion\nxstory_cloze\nes\nvalidation\nEN\nMax acc.\n59.36\n55.20\n58.77\n60.95\n63.07\n65.25\n72.34\n83.32\n49.97\n52.35\n55.20\n54.93\n55.72\n74.39\n92.52\n93.32\n93.58\n55.86\n58.04\n77.96\n85.90\n88.88\n88.62\n82.93\n94.31\n95.23\n93.91\nSent. completion\nxstory_cloze\nes\nvalidation\nMT\nMedian acc.\n52.08\n50.56\n46.72\n49.97\n49.77\n50.69\n50.63\n80.08\n49.44\n48.05\n53.87\n55.00\n54.60\n74.45\n91.20\n92.85\n92.72\n55.53\n56.06\n55.72\n66.25\n84.98\n86.57\n78.49\n92.98\n94.90\n92.85\nSent. completion\nxstory_cloze\nes\nvalidation\nMT\nMax acc.\n60.56\n54.93\n57.84\n60.89\n62.81\n65.78\n72.47\n82.13\n50.63\n53.47\n54.86\n55.39\n55.33\n77.17\n92.52\n93.38\n93.98\n56.45\n58.37\n72.53\n84.51\n88.95\n88.82\n81.60\n94.37\n95.50\n94.44\nSent. completion\nxstory_cloze\neu\nvalidation\nEN\nMedian acc.\n51.56\n49.31\n48.25\n50.30\n50.36\n48.38\n50.96\n49.11\n45.14\n49.31\n51.82\n51.82\n50.50\n64.13\n86.04\n84.38\n90.87\n49.44\n46.72\n57.05\n67.44\n70.81\n70.02\n67.17\n85.24\n85.04\n85.04\nSent. completion\nxstory_cloze\neu\nvalidation\nEN\nMax acc.\n54.86\n52.95\n54.14\n54.00\n55.00\n56.52\n62.61\n51.36\n50.83\n53.01\n52.95\n52.88\n52.55\n66.64\n89.48\n89.68\n91.33\n50.56\n52.22\n60.49\n70.95\n73.33\n72.67\n70.42\n86.90\n85.90\n86.70\nSent. completion\nxstory_cloze\neu\nvalidation\nMT\nMedian acc.\n49.83\n50.69\n48.64\n50.50\n49.31\n48.44\n50.89\n50.83\n46.46\n50.30\n51.42\n52.55\n51.29\n65.92\n89.54\n87.16\n91.20\n47.39\n45.14\n49.83\n50.17\n59.89\n65.59\n66.38\n81.40\n82.59\n82.73\nSent. completion\nxstory_cloze\neu\nvalidation\nMT\nMax acc.\n55.13\n52.22\n52.95\n54.20\n54.60\n56.32\n62.67\n52.15\n50.96\n53.47\n52.28\n53.67\n52.61\n69.03\n90.60\n91.13\n91.66\n47.52\n52.35\n57.91\n64.13\n70.81\n73.26\n68.63\n86.76\n86.83\n84.25\nSent. completion\nxstory_cloze\nhi\nvalidation\nEN\nMedian acc.\n50.96\n48.11\n47.65\n50.17\n51.42\n49.44\n52.42\n49.70\n46.33\n52.22\n51.09\n52.55\n50.89\n67.84\n89.81\n87.76\n88.68\n53.34\n51.82\n68.03\n75.98\n75.31\n74.85\n68.17\n87.03\n87.43\n87.09\nSent. completion\nxstory_cloze\nhi\nvalidation\nEN\nMax acc.\n55.33\n52.95\n54.53\n56.78\n56.39\n59.70\n63.80\n50.23\n52.02\n53.61\n51.62\n53.87\n52.15\n70.68\n92.32\n90.73\n89.41\n53.74\n55.20\n72.87\n78.89\n79.48\n78.82\n72.20\n87.89\n88.68\n88.35\nSent. completion\nxstory_cloze\nhi\nvalidation\nMT\nMedian acc.\n49.64\n46.99\n47.72\n50.23\n52.35\n50.56\n51.09\n48.78\n47.12\n52.08\n51.89\n54.60\n50.17\n69.89\n91.79\n88.95\n87.82\n54.80\n50.89\n55.46\n65.06\n73.33\n75.84\n68.83\n86.70\n87.36\n86.23\nSent. completion\nxstory_cloze\nhi\nvalidation\nMT\nMax acc.\n56.59\n53.87\n54.40\n56.78\n57.31\n60.23\n65.39\n50.76\n53.14\n54.86\n53.01\n55.00\n51.16\n71.08\n92.19\n90.14\n89.15\n55.79\n55.92\n70.75\n75.25\n80.61\n80.41\n71.61\n88.42\n89.15\n88.09\nSent. completion\nxstory_cloze\nid\nvalidation\nEN\nMedian acc.\n52.42\n48.97\n45.86\n47.85\n50.63\n52.28\n52.02\n70.28\n46.86\n48.58\n50.76\n53.54\n54.14\n72.34\n90.80\n90.54\n91.86\n55.79\n55.79\n64.00\n71.81\n82.40\n78.16\n74.45\n90.67\n90.87\n91.40\nSent. completion\nxstory_cloze\nid\nvalidation\nEN\nMax acc.\n59.36\n55.00\n57.51\n58.77\n60.29\n63.53\n69.03\n73.06\n49.90\n51.03\n52.35\n54.40\n54.67\n73.86\n93.25\n93.05\n92.46\n57.25\n57.97\n74.92\n82.99\n84.25\n83.32\n77.10\n92.12\n92.06\n92.59\nSent. completion\nxstory_cloze\nid\nvalidation\nMT\nMedian acc.\n52.15\n49.90\n48.58\n49.70\n51.03\n52.81\n51.89\n68.96\n47.85\n48.05\n50.83\n54.73\n54.14\n74.98\n92.46\n91.73\n91.66\n52.75\n54.67\n53.14\n58.97\n68.96\n82.26\n73.46\n89.28\n90.87\n90.07\nSent. completion\nxstory_cloze\nid\nvalidation\nMT\nMax acc.\n59.63\n55.33\n57.97\n60.29\n60.89\n63.60\n68.76\n70.15\n49.97\n51.49\n53.08\n57.38\n54.27\n75.71\n93.51\n92.19\n92.65\n57.84\n56.98\n69.29\n78.89\n83.32\n84.58\n75.12\n91.46\n92.79\n91.40\nSent. completion\nxstory_cloze\nmy\nvalidation\nEN\nMedian acc.\n51.42\n51.49\n47.32\n52.02\n49.64\n52.48\n52.68\n52.75\n46.00\n48.91\n50.89\n51.16\n51.16\n63.20\n82.79\n84.78\n86.96\n46.46\n46.00\n48.31\n46.99\n49.70\n49.17\n50.63\n49.97\n51.89\n50.63\nSent. completion\nxstory_cloze\nmy\nvalidation\nEN\nMax acc.\n53.21\n52.61\n48.44\n52.95\n50.56\n52.61\n52.95\n54.80\n50.89\n50.56\n50.96\n51.42\n51.69\n65.65\n87.49\n86.70\n88.35\n47.05\n46.39\n51.03\n49.90\n50.43\n51.42\n51.03\n52.35\n52.35\n52.68\nSent. completion\nxstory_cloze\nmy\nvalidation\nMT\nMedian acc.\n49.83\n50.03\n46.86\n50.50\n49.31\n51.62\n52.28\n48.18\n45.47\n47.39\n49.57\n52.28\n50.03\n63.07\n83.45\n81.07\n84.32\n46.06\n46.06\n47.39\n47.58\n51.42\n50.56\n49.90\n49.83\n50.23\n51.22\nSent. completion\nxstory_cloze\nmy\nvalidation\nMT\nMax acc.\n52.48\n51.69\n47.58\n52.61\n50.56\n52.68\n53.41\n50.17\n50.50\n50.83\n51.82\n52.75\n50.23\n64.66\n85.57\n85.90\n85.44\n46.59\n47.05\n51.09\n49.04\n52.55\n51.56\n51.49\n50.56\n51.42\n51.95\nSent. completion\nxstory_cloze\nru\nvalidation\nEN\nMedian acc.\n50.23\n49.70\n50.63\n52.88\n50.89\n50.10\n50.36\n65.12\n46.19\n49.04\n48.38\n51.82\n52.75\n69.49\n87.49\n86.30\n83.65\n51.29\n48.44\n54.00\n57.78\n63.27\n62.08\n64.06\n79.02\n77.56\n79.42\nSent. completion\nxstory_cloze\nru\nvalidation\nEN\nMax acc.\n60.09\n50.69\n51.22\n54.14\n52.08\n52.35\n56.06\n67.50\n51.42\n56.39\n49.24\n53.87\n53.08\n71.14\n90.80\n90.87\n84.51\n53.14\n50.23\n56.39\n61.42\n65.32\n64.26\n66.45\n81.73\n79.09\n79.62\nSent. completion\nxstory_cloze\nru\nvalidation\nMT\nMedian acc.\n49.50\n50.23\n51.09\n52.02\n52.35\n50.50\n49.83\n61.95\n47.52\n49.24\n48.97\n50.63\n53.01\n71.28\n89.54\n90.01\n82.86\n51.03\n49.11\n50.17\n50.10\n58.31\n58.77\n61.02\n78.42\n74.19\n75.98\nSent. completion\nxstory_cloze\nru\nvalidation\nMT\nMax acc.\n60.42\n50.30\n51.42\n53.14\n53.41\n52.15\n55.46\n63.93\n51.82\n55.39\n49.70\n53.01\n53.74\n74.85\n91.40\n91.66\n84.91\n52.22\n50.30\n55.13\n56.65\n63.40\n62.41\n64.13\n79.09\n78.16\n76.57\nSent. completion\nxstory_cloze\nsw\nvalidation\nEN\nMedian acc.\n52.08\n49.90\n49.64\n49.83\n53.08\n51.89\n49.31\n49.31\n46.59\n49.04\n53.61\n53.21\n53.94\n67.11\n86.17\n87.76\n89.15\n49.24\n49.24\n55.59\n67.44\n67.70\n66.31\n58.84\n77.83\n79.42\n75.71\nSent. completion\nxstory_cloze\nsw\nvalidation\nEN\nMax acc.\n56.32\n50.03\n50.30\n51.62\n55.06\n53.94\n60.42\n51.49\n49.31\n53.21\n54.53\n53.34\n54.73\n68.83\n88.82\n89.61\n89.61\n51.36\n49.24\n61.28\n69.69\n71.67\n71.01\n60.82\n79.81\n81.14\n77.76\nSent. completion\nxstory_cloze\nsw\nvalidation\nMT\nMedian acc.\n50.69\n50.83\n49.83\n50.76\n51.49\n50.30\n48.84\n51.56\n46.46\n47.92\n52.81\n53.14\n53.81\n69.69\n87.36\n88.15\n89.08\n48.64\n49.17\n49.24\n50.23\n53.28\n58.37\n55.79\n70.81\n73.00\n70.28\nSent. completion\nxstory_cloze\nsw\nvalidation\nMT\nMax acc.\n55.33\n51.62\n50.69\n51.69\n53.01\n53.67\n60.56\n52.28\n49.37\n53.94\n54.20\n54.40\n55.53\n71.14\n89.41\n89.15\n89.34\n50.50\n49.97\n58.44\n63.73\n68.43\n69.69\n57.18\n78.36\n80.01\n72.60\nSent. completion\nxstory_cloze\nte\nvalidation\nEN\nMedian acc.\n51.29\n49.90\n51.95\n50.23\n52.68\n50.83\n51.69\n48.44\n49.04\n49.97\n53.21\n54.67\n56.32\n64.86\n85.04\n86.10\n86.37\n52.02\n51.36\n63.40\n70.28\n72.01\n70.09\n62.08\n80.61\n80.15\n77.70\nSent. completion\nxstory_cloze\nte\nvalidation\nEN\nMax acc.\n57.38\n55.33\n55.92\n55.79\n57.58\n57.91\n61.61\n49.17\n54.00\n53.34\n53.34\n55.72\n57.18\n68.70\n89.54\n90.40\n87.29\n53.61\n55.26\n66.25\n73.66\n74.72\n73.06\n63.14\n81.20\n82.40\n79.88\nSent. completion\nxstory_cloze\nte\nvalidation\nMT\nMedian acc.\n49.11\n51.03\n52.68\n49.44\n52.61\n50.89\n50.76\n49.50\n49.77\n48.97\n53.21\n56.25\n55.13\n67.50\n87.16\n86.90\n82.20\n51.16\n50.36\n55.33\n54.73\n63.73\n66.98\n58.31\n76.17\n78.29\n73.79\nSent. completion\nxstory_cloze\nte\nvalidation\nMT\nMax acc.\n57.05\n55.79\n56.12\n56.32\n58.11\n57.64\n62.41\n49.83\n53.61\n54.00\n53.67\n56.92\n56.85\n68.89\n90.54\n89.41\n85.51\n54.86\n55.86\n61.88\n66.38\n73.59\n71.54\n59.30\n80.28\n80.54\n77.37\nSent. completion\nxstory_cloze\nzh\nvalidation\nEN\nMedian acc.\n52.08\n49.83\n47.39\n47.85\n49.17\n50.30\n51.36\n49.70\n47.65\n53.21\n56.25\n54.00\n56.32\n68.17\n91.20\n91.79\n92.92\n54.53\n57.18\n76.84\n82.59\n84.91\n83.85\n76.70\n91.99\n92.32\n91.13\nSent. completion\nxstory_cloze\nzh\nvalidation\nEN\nMax acc.\n55.59\n54.47\n56.45\n58.04\n59.89\n61.22\n65.65\n50.89\n49.04\n53.94\n56.78\n54.60\n58.84\n71.74\n92.72\n93.05\n93.18\n56.52\n58.17\n78.69\n84.32\n85.04\n85.84\n79.62\n93.12\n92.85\n92.26\nSent. completion\nxstory_cloze\nzh\nvalidation\nHT\nMedian acc.\n-\n-\n-\n-\n-\n-\n-\n-\n-\n-\n-\n-\n-\n-\n-\n-\n-\n-\n-\n-\n-\n81.67\n-\n77.10\n-\n-\n-\nSent. completion\nxstory_cloze\nzh\nvalidation\nHT\nMax acc.\n-\n-\n-\n-\n-\n-\n-\n-\n-\n-\n-\n-\n-\n-\n-\n-\n-\n-\n-\n-\n-\n85.37\n-\n79.48\n-\n-\n-\nSent. completion\nxstory_cloze\nzh\nvalidation\nMT\nMedian acc.\n52.15\n49.24\n47.45\n47.65\n50.23\n51.89\n53.01\n48.05\n46.99\n52.02\n55.00\n54.27\n57.71\n72.01\n92.59\n91.79\n91.79\n55.59\n56.45\n70.88\n74.26\n81.20\n84.65\n78.42\n91.86\n91.40\n90.40\nSent. completion\nxstory_cloze\nzh\nvalidation\nMT\nMax acc.\n56.12\n54.33\n56.59\n57.38\n60.09\n61.22\n66.64\n50.03\n48.97\n54.20\n57.78\n55.72\n59.50\n72.93\n93.85\n93.05\n93.58\n56.45\n56.85\n77.17\n80.87\n85.11\n85.90\n80.34\n92.39\n92.52\n91.93\nTable 10: Evaluation results. Results per prompt can be found at https://huggingface.co/datasets/bigscience/evaluation-results\n\nTask\nDataset\nConfig\nSplit\nPrompt\nMetric\nmT0-300M\nmT0-560M\nmT0-1.2B\nmT0-3.7B\nmT0-13B\nBLOOMZ-560M\nBLOOMZ-1.1B\nBLOOMZ-1.7B\nBLOOMZ-3B\nBLOOMZ-7.1B\nBLOOMZ\nExtractive QA\ncraigslist_bargains\nbargains\nvalidation\nEN\nMedian acc.\n30.49\n23.95\n22.61\n39.61\n25.96\n38.94\n47.99\n28.14\n22.86\n46.48\n26.47\nExtractive QA\ncraigslist_bargains\nbargains\nvalidation\nEN\nMax acc.\n49.41\n28.14\n31.32\n50.92\n40.54\n72.53\n72.36\n46.90\n31.32\n60.47\n51.76\nGrammar Correction\nblimp_adjunct\nisland\nvalidation\nEN\nMedian acc.\n50.40\n51.60\n51.80\n53.80\n55.10\n51.60\n52.30\n50.60\n49.20\n49.90\n49.80\nGrammar Correction\nblimp_adjunct\nisland\nvalidation\nEN\nMax acc.\n50.90\n57.00\n58.00\n59.10\n56.80\n77.10\n60.90\n62.30\n59.90\n57.60\n51.60\nGrammar Correction\nglue\ncola\nvalidation\nEN\nMedian acc.\n30.97\n38.26\n56.57\n35.19\n45.83\n31.26\n57.81\n31.16\n31.35\n33.27\n44.58\nGrammar Correction\nglue\ncola\nvalidation\nEN\nMax acc.\n64.33\n51.01\n62.80\n47.17\n58.29\n41.71\n67.98\n46.40\n65.39\n56.86\n63.37\nMultiple-Choice QA\naqua_rat\nraw\nvalidation\nEN\nMedian acc.\n27.95\n25.20\n24.80\n20.47\n16.14\n19.29\n22.83\n22.05\n22.44\n24.41\n27.56\nMultiple-Choice QA\naqua_rat\nraw\nvalidation\nEN\nMax acc.\n29.53\n26.38\n25.59\n21.65\n18.90\n20.08\n24.80\n22.83\n22.83\n25.20\n28.35\nMultiple-Choice QA\ncodah\ncodah\ntrain\nEN\nMedian acc.\n25.25\n25.43\n26.48\n55.04\n75.58\n24.93\n24.35\n57.17\n64.12\n73.60\n80.66\nMultiple-Choice QA\ncodah\ncodah\ntrain\nEN\nMax acc.\n25.32\n26.15\n27.13\n55.44\n76.22\n25.04\n24.60\n57.31\n64.41\n73.67\n80.91\nMultiple-Choice QA\ncommonsense_qa\nqa\nvalidation\nEN\nMedian acc.\n31.20\n37.43\n36.61\n56.35\n69.53\n43.98\n38.90\n69.86\n84.44\n83.05\n80.26\nMultiple-Choice QA\ncommonsense_qa\nqa\nvalidation\nEN\nMax acc.\n31.53\n37.51\n39.72\n60.03\n69.94\n44.47\n42.42\n72.40\n84.60\n84.36\n83.05\nMultiple-Choice QA\nhead_qa\nen\nvalidation\nEN\nMedian acc.\n24.89\n24.38\n23.43\n27.53\n36.02\n26.72\n27.16\n27.53\n30.01\n38.58\n53.15\nMultiple-Choice QA\nhead_qa\nen\nvalidation\nEN\nMax acc.\n25.55\n25.62\n26.87\n31.55\n36.16\n27.75\n27.67\n33.31\n35.21\n40.92\n53.95\nMultiple-Choice QA\nhead_qa\nes\nvalidation\nEN\nMedian acc.\n24.60\n24.45\n23.94\n27.89\n34.92\n26.94\n25.04\n24.45\n26.21\n34.41\n50.81\nMultiple-Choice QA\nhead_qa\nes\nvalidation\nEN\nMax acc.\n26.21\n26.21\n24.74\n29.50\n37.04\n28.26\n26.28\n29.87\n33.02\n39.75\n51.76\nMultiple-Choice QA\nmath_qa\nqa\ntest\nEN\nMedian acc.\n21.11\n20.00\n22.18\n23.25\n23.69\n19.66\n21.21\n20.97\n21.81\n21.14\n21.84\nMultiple-Choice QA\nmath_qa\nqa\ntest\nEN\nMax acc.\n22.21\n26.03\n35.64\n24.89\n26.60\n45.56\n27.94\n35.24\n43.28\n38.12\n47.37\nMultiple-Choice QA\nmwsc\nmwsc\nvalidation\nEN\nMedian acc.\n50.00\n52.44\n54.88\n60.98\n74.39\n53.66\n52.44\n56.10\n58.54\n62.20\n71.95\nMultiple-Choice QA\nmwsc\nmwsc\nvalidation\nEN\nMax acc.\n52.44\n53.66\n57.32\n65.85\n79.27\n58.54\n57.32\n58.54\n63.41\n69.51\n80.49\nMultiple-Choice QA\npubmed_qa\nlabeled\ntrain\nEN\nMedian acc.\n45.55\n54.50\n55.75\n58.35\n65.35\n55.75\n58.90\n66.75\n66.80\n67.15\n71.80\nMultiple-Choice QA\npubmed_qa\nlabeled\ntrain\nEN\nMax acc.\n48.60\n57.60\n58.30\n58.60\n66.20\n57.50\n63.50\n72.10\n69.80\n69.50\n74.40\nMultiple-Choice QA\nriddle_sense\nsense\nvalidation\nEN\nMedian acc.\n24.39\n22.04\n23.41\n29.63\n43.14\n22.87\n24.53\n30.02\n35.11\n39.47\n50.64\nMultiple-Choice QA\nriddle_sense\nsense\nvalidation\nEN\nMax acc.\n34.48\n33.30\n33.01\n39.18\n47.50\n37.41\n39.86\n43.58\n47.60\n48.09\n59.26\nSentiment\namazon_reviews_multi\nen\nvalidation\nEN\nMedian acc.\n40.60\n50.80\n51.12\n49.00\n53.24\n46.52\n42.46\n50.48\n49.88\n51.00\n50.90\nSentiment\namazon_reviews_multi\nen\nvalidation\nEN\nMax acc.\n41.34\n53.88\n54.18\n55.92\n57.04\n50.44\n47.74\n55.94\n53.74\n55.08\n54.16\nSentiment\namazon_reviews_multi\nes\nvalidation\nEN\nMedian acc.\n39.56\n48.70\n49.02\n47.56\n52.30\n37.60\n38.92\n45.08\n45.32\n44.44\n43.26\nSentiment\namazon_reviews_multi\nes\nvalidation\nEN\nMax acc.\n42.66\n51.00\n50.42\n50.68\n53.58\n39.10\n40.24\n47.98\n46.28\n47.76\n44.48\nSentiment\namazon_reviews_multi\nfr\nvalidation\nEN\nMedian acc.\n38.74\n48.44\n48.32\n46.12\n51.12\n38.78\n38.38\n44.36\n45.84\n44.92\n43.92\nSentiment\namazon_reviews_multi\nfr\nvalidation\nEN\nMax acc.\n40.66\n49.64\n49.70\n49.30\n52.40\n41.16\n40.04\n46.66\n46.80\n47.42\n44.90\nSentiment\namazon_reviews_multi\nzh\nvalidation\nEN\nMedian acc.\n34.74\n42.38\n42.58\n39.66\n45.30\n37.54\n34.44\n41.10\n38.78\n44.78\n40.48\nSentiment\namazon_reviews_multi\nzh\nvalidation\nEN\nMax acc.\n37.88\n44.36\n44.74\n43.66\n47.14\n39.48\n35.24\n43.52\n39.64\n47.12\n42.10\nSentiment\nfinancial_phrasebank\nallagree\ntrain\nEN\nMedian acc.\n18.33\n28.98\n28.09\n25.44\n35.25\n31.10\n29.28\n34.76\n35.91\n34.89\n24.82\nSentiment\nfinancial_phrasebank\nallagree\ntrain\nEN\nMax acc.\n22.22\n57.51\n52.25\n68.15\n37.77\n44.79\n34.81\n54.37\n59.23\n37.15\n37.23\nSentiment\nglue\nsst2\nvalidation\nEN\nMedian acc.\n79.70\n83.49\n83.37\n82.80\n93.58\n87.96\n83.72\n92.09\n94.50\n94.04\n93.92\nSentiment\nglue\nsst2\nvalidation\nEN\nMax acc.\n81.88\n87.96\n86.81\n91.51\n94.84\n92.89\n89.79\n94.15\n95.87\n94.61\n95.07\nSentiment\nlince\nspaeng\nvalidation\nEN\nMedian acc.\n43.63\n43.09\n49.11\n41.69\n54.81\n58.04\n53.85\n52.82\n50.19\n58.15\n59.60\nSentiment\nlince\nspaeng\nvalidation\nEN\nMax acc.\n56.91\n56.05\n56.37\n55.78\n56.80\n58.53\n55.35\n56.37\n54.60\n58.47\n60.09\nSentiment\nmovie_rationales\nrationales\nvalidation\nEN\nMedian acc.\n63.50\n78.00\n81.00\n69.50\n90.00\n93.50\n97.50\n98.50\n98.00\n97.50\n98.50\nSentiment\nmovie_rationales\nrationales\nvalidation\nEN\nMax acc.\n94.50\n95.50\n98.50\n99.50\n100.00\n98.50\n97.50\n100.00\n99.50\n99.00\n99.50\nSentiment\npoem_sentiment\nsentiment\nvalidation\nEN\nMedian acc.\n17.14\n18.10\n16.19\n16.19\n26.67\n20.95\n29.52\n24.76\n24.76\n22.86\n23.81\nSentiment\npoem_sentiment\nsentiment\nvalidation\nEN\nMax acc.\n18.10\n23.81\n20.00\n27.62\n27.62\n22.86\n33.33\n29.52\n31.43\n29.52\n24.76\nSummarization\nmlsum\nes\nvalidation\nEN\nMedian BLEU\n0.18\n0.18\n0.18\n0.19\n0.19\n0.20\n0.18\n0.19\n0.19\n0.20\n0.19\nSummarization\nmlsum\nes\nvalidation\nEN\nMax BLEU\n2.91\n3.51\n3.46\n3.72\n4.21\n3.62\n2.87\n3.23\n3.84\n4.82\n4.16\nText Classification\nart\nart\nvalidation\nEN\nMedian acc.\n50.85\n50.85\n50.46\n53.33\n68.99\n51.50\n50.07\n52.68\n54.57\n58.42\n66.58\nText Classification\nart\nart\nvalidation\nEN\nMax acc.\n51.04\n51.83\n51.76\n56.07\n69.71\n52.68\n50.65\n54.24\n57.31\n61.10\n67.43\nText Classification\nclimate_fever\nfever\ntest\nEN\nMedian acc.\n10.62\n25.28\n10.94\n26.78\n29.97\n45.34\n10.36\n51.92\n10.81\n43.97\n18.63\nText Classification\nclimate_fever\nfever\ntest\nEN\nMax acc.\n42.41\n43.78\n20.98\n43.32\n51.01\n63.97\n30.94\n65.54\n32.12\n47.69\n36.61\nText Classification\nconv_ai_3\n3\nvalidation\nEN\nMedian acc.\n35.15\n38.52\n37.79\n39.04\n39.04\n39.04\n39.04\n39.04\n39.04\n39.04\n39.04\nText Classification\nconv_ai_3\n3\nvalidation\nEN\nMax acc.\n60.35\n60.96\n55.69\n60.96\n60.96\n60.96\n60.96\n60.96\n60.96\n60.96\n60.96\nText Classification\nemotion\nemotion\ntest\nEN\nMedian acc.\n20.75\n23.83\n42.20\n32.38\n31.35\n34.72\n35.57\n29.93\n39.77\n33.05\n36.70\nText Classification\nemotion\nemotion\ntest\nEN\nMax acc.\n32.40\n24.65\n46.25\n33.05\n34.65\n46.70\n42.40\n49.20\n49.35\n50.25\n45.20\nText Classification\nhealth_fact\nfact\nvalidation\nEN\nMedian acc.\n31.59\n27.27\n31.10\n43.67\n54.78\n42.04\n45.63\n44.00\n32.41\n31.51\n47.92\nText Classification\nhealth_fact\nfact\nvalidation\nEN\nMax acc.\n50.61\n43.02\n42.53\n44.16\n59.59\n54.78\n56.82\n63.76\n62.53\n57.55\n61.31\nText Classification\nhlgd\nhlgd\nvalidation\nEN\nMedian acc.\n50.65\n59.45\n52.88\n78.15\n80.72\n72.89\n68.63\n64.14\n65.39\n70.57\n67.57\nText Classification\nhlgd\nhlgd\nvalidation\nEN\nMax acc.\n63.80\n73.71\n65.83\n79.36\n84.92\n74.92\n72.50\n73.37\n68.15\n81.83\n78.44\nText Classification\nhyperpartisan_news_detection\nbyarticle\ntrain\nEN\nMedian acc.\n46.20\n49.15\n52.87\n52.87\n43.26\n62.95\n63.10\n63.10\n63.10\n63.10\n63.10\nText Classification\nhyperpartisan_news_detection\nbyarticle\ntrain\nEN\nMax acc.\n49.15\n50.39\n54.57\n53.64\n44.96\n63.10\n63.26\n63.10\n63.41\n63.10\n63.72\nText Classification\nliar\nliar\nvalidation\nEN\nMedian acc.\n19.47\n18.07\n20.40\n17.68\n17.91\n17.60\n19.31\n19.39\n15.19\n20.79\n20.87\nText Classification\nliar\nliar\nvalidation\nEN\nMax acc.\n19.47\n18.07\n20.40\n17.68\n17.91\n17.60\n19.31\n19.39\n15.19\n20.79\n20.87\nText Classification\nonestop_english\nenglish\ntrsin\nEN\nMedian acc.\n48.32\n48.15\n33.33\n58.20\n48.32\n43.39\n33.51\n35.80\n45.33\n54.67\n41.80\nText Classification\nonestop_english\nenglish\ntrsin\nEN\nMax acc.\n56.26\n58.73\n46.74\n65.61\n56.44\n55.56\n34.57\n41.80\n63.32\n64.02\n53.09\nText Classification\nscicite\nscicite\nvalidation\nEN\nMedian acc.\n13.97\n24.56\n23.14\n33.08\n39.63\n33.08\n17.90\n21.62\n30.57\n34.28\n54.91\nText Classification\nscicite\nscicite\nvalidation\nEN\nMax acc.\n25.11\n37.23\n30.57\n66.16\n66.16\n54.91\n25.98\n44.10\n57.21\n50.33\n63.43\nTopic Classification\nbanking77\nbanking77\ntest\nEN\nMedian acc.\n11.30\n11.53\n16.27\n19.51\n30.10\n14.38\n19.29\n20.81\n24.19\n25.39\n28.57\nTopic Classification\nbanking77\nbanking77\ntest\nEN\nMax acc.\n15.10\n12.99\n19.94\n23.83\n30.94\n16.10\n20.45\n26.04\n28.90\n26.36\n29.06\nTopic Classification\nblbooksgenre_title_genre\nclassifiction\nvalidation\nEN\nMedian acc.\n26.21\n35.43\n35.83\n49.14\n32.03\n41.47\n25.17\n30.47\n27.13\n74.94\n77.07\nTopic Classification\nblbooksgenre_title_genre\nclassifiction\nvalidation\nEN\nMax acc.\n33.93\n43.78\n73.10\n74.88\n85.43\n74.31\n74.94\n73.62\n71.72\n84.56\n86.41\nTopic Classification\nselqa\nanalysis\nvalidation\nEN\nMedian acc.\n88.34\n88.54\n90.00\n89.30\n92.61\n89.81\n87.71\n91.08\n90.83\n89.24\n91.46\nTopic Classification\nselqa\nanalysis\nvalidation\nEN\nMax acc.\n91.59\n90.32\n91.08\n91.97\n94.39\n92.36\n88.66\n92.36\n92.48\n91.21\n94.27\nTopic Classification\nsnips_built_in_intents\nintents\ntrain\nEN\nMedian acc.\n35.37\n45.73\n34.15\n62.20\n82.62\n27.13\n39.63\n11.89\n25.91\n39.94\n70.12\nTopic Classification\nsnips_built_in_intents\nintents\ntrain\nEN\nMax acc.\n39.02\n54.27\n42.07\n64.63\n92.68\n46.34\n53.96\n17.68\n33.84\n58.23\n78.66\nTranslation\nwmt14_fr_en\nen\nvalidation\nEN\nMedian BLEU\n5.47\n11.33\n17.00\n23.92\n29.87\n4.70\n4.28\n6.10\n12.29\n8.03\n26.07\nTranslation\nwmt14_fr_en\nen\nvalidation\nEN\nMax BLEU\n10.84\n19.23\n23.15\n29.63\n33.65\n21.24\n25.38\n26.19\n27.93\n29.54\n33.71\nTranslation\nwmt14_hi_en\nen\nvalidation\nEN\nMedian BLEU\n1.02\n3.35\n5.11\n9.14\n18.43\n1.36\n0.39\n1.11\n1.84\n3.62\n10.05\nTranslation\nwmt14_hi_en\nen\nvalidation\nEN\nMax BLEU\n2.47\n7.57\n12.96\n19.80\n26.13\n9.02\n11.09\n12.02\n14.82\n17.02\n21.18\nTable 11: Evaluation results on validation datasets used for checkpoint selection. Results of the chosen checkpoint are shown. Results per prompt can be found at https://huggingface.co/datasets/bigscience/evaluation-results\n\nL\nVersion control\nV1 →V2:\n• Added evaluation results for the validation datasets used for checkpoint selection (Appendix §K)\n• Added a section on the effect on generation length (Appendix §G) and rewrote parts of §4.5\n• Added a mention of xP3x, the extension of xP3 to 277 languages in Appendix §C\n• Added an example of XNLI to Appendix §H\nM\nPrompts used\nThis section describes the prompts used for training and evaluation.\n\nIn the following, dataset naming conventions follow those used in the Hugging Face datasets\nlibrary. Since xP3 expands upon the P3 dataset employed by Sanh et al. (2022), we refer the\nreader to that work for example prompts from datasets that fall within P3. Here, we provide\nprompts curated for datasets that belong to xP3 but not to P3. The prompts provided are\nnot exhaustive. Code will be released to provide a canonical reference. For each dataset\nconsidered, a dataset example is provided for additional context. Next, it is noted if the\nprompt does not match the original task formulation of the dataset. This is followed by a\nreference for the data, an input template and a target template. For prompts with predefined\nanswer choices, these are also included. To provide examples of both human-translated and\nmachine-translated prompts, samples of each kind are included for the xnli es dataset.\nContents\n1\nPrompts\n1.1\nSimplification\n. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n1.1.1\nGEM/BiSECT en\n. . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n1.1.2\nGEM/BiSECT es . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n1.1.3\nGEM/BiSECT fr . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n1.2\nSummarization . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n1.2.1\nGEM/wiki_lingua en\n. . . . . . . . . . . . . . . . . . . . . . . . . . .\n1.2.2\nGEM/wiki_lingua es . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n1.2.3\nGEM/xlsum bengali . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n1.2.4\nGEM/xlsum english . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n1.3\nTranslation\n. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n1.3.1\nHelsinki-NLP/tatoeba_mt ben-eng . . . . . . . . . . . . . . . . . . . .\n1.3.2\nHelsinki-NLP/tatoeba_mt eng-fra\n. . . . . . . . . . . . . . . . . . . .\n1.3.3\nfacebook/flores ben_Beng-eng_Latn . . . . . . . . . . . . . . . . . . .\n1.3.4\nfacebook/flores ben_Beng-fra_Latn . . . . . . . . . . . . . . . . . . .\n1.4\nProgram Synthesis . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n1.4.1\nMuennighoff/mbpp sanitized\n. . . . . . . . . . . . . . . . . . . . . . .\n1.4.2\ncodeparrot/apps all\n. . . . . . . . . . . . . . . . . . . . . . . . . . . .\n1.4.3\ncodeparrot/github-jupyter-text-code-pairs . . . . . . . . . . . . . . . .\n1.4.4\ncodeparrot/xlcost-text-to-code C++-program-level . . . . . . . . . . .\n1.4.5\ncodeparrot/xlcost-text-to-code C-program-level . . . . . . . . . . . . .\n1.4.6\ncodeparrot/xlcost-text-to-code Csharp-program-level . . . . . . . . . .\n1.4.7\ncodeparrot/xlcost-text-to-code Java-program-level\n. . . . . . . . . . .\n1.4.8\ncodeparrot/xlcost-text-to-code Javascript-program-level . . . . . . . .\n1.4.9\ncodeparrot/xlcost-text-to-code PHP-program-level . . . . . . . . . . .\n1.4.10 codeparrot/xlcost-text-to-code Python-program-level . . . . . . . . . .\n1.4.11 neural_code_search evaluation_dataset . . . . . . . . . . . . . . . . .\n1.4.12 teven/code_contests . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n1.5\nCoreference Resolution\n. . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n\n1.5.1\nMuennighoff/xwinograd en\n. . . . . . . . . . . . . . . . . . . . . . . .\n1.5.2\nMuennighoff/xwinograd fr . . . . . . . . . . . . . . . . . . . . . . . . .\n1.6\nQuestion Answering Multiple Choice\n. . . . . . . . . . . . . . . . . . . . . .\n1.6.1\nclue c3 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n1.7\nQuestion Answering Extractive . . . . . . . . . . . . . . . . . . . . . . . . . .\n1.7.1\nclue cmrc2018 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n1.7.2\nclue drcd\n. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n1.7.3\nmlqa mlqa.vi.vi . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n1.7.4\nmlqa mlqa.zh.zh\n. . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n1.7.5\nxquad xquad.vi . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n1.7.6\nxquad xquad.zh . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n1.8\nTopic Classification\n. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n1.8.1\nclue csl\n. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n1.8.2\nclue tnews . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n1.9\nCode Misc.\n. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n1.9.1\ncodeparrot/codecomplex codeparrot–codecomplex\n. . . . . . . . . . .\n1.9.2\ngreat_code . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n1.9.3\nteven/code_docstring_corpus top_level . . . . . . . . . . . . . . . . .\n1.10 Word Sense Disambiguation\n. . . . . . . . . . . . . . . . . . . . . . . . . . .\n1.10.1 pasinit/xlwic xlwic_en_zh\n. . . . . . . . . . . . . . . . . . . . . . . .\n1.10.2 pasinit/xlwic xlwic_fr_fr . . . . . . . . . . . . . . . . . . . . . . . . .\n1.11 Paraphrase Identification\n. . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n1.11.1 paws-x en . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n1.11.2 paws-x es . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n1.12 Sentence Completion\n. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n1.12.1 xcopa vi . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n1.12.2 xcopa zh . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n1.13 Natural Language Inference . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n1.13.1 xnli en . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n1.13.2 xnli es . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n1.13.2.1 Human-translated prompts . . . . . . . . . . . . . . . . . . .\n1.13.2.2 Machine-translated prompts\n. . . . . . . . . . . . . . . . . .\n1\nPrompts\n1.1\nSimplification\n1.1.1\nGEM/BiSECT en\nDataset from Kim et al. (2021). Used in training.\n\nData Example\nKey\nValue\ngem_id\nBiSECT_en-train-1\nsource\nTo view any of the video clips belo...\ntarget\nIf you want to watch one of the vid...\nreferences\nIf you want to watch one of the vid...\nPrompts\nInput Template:\nSplit and simplify the following sentence while retaining its full meaning:\n{{source}}\nSimplified version:\nTarget Template:\n{{target}}\nInput Template:\n{{source}}\nThe above sentence is very complicated. Please provide me a simplified synonymous\nversion consisting of multiple sentences:\nTarget Template:\n{{target}}\nInput Template:\n{{source}}. This sentence is hard to understand. A simpler version with equivalent\nmeaning is the following:\nTarget Template:\n{{target}}\n1.1.2\nGEM/BiSECT es\nData Example\n\nKey\nValue\ngem_id\nBiSECT_es-train-1\nsource\nAl final de la Santa Misa , mientra...\ntarget\nAl finalizar la santa misa , mientr...\nreferences\nAl finalizar la santa misa , mientr...\nPrompts\nInput Template:\n{{source}}. Esta frase es difícil de entender. Una versión más simple con\nsignificado equivalente es la siguiente:\nTarget Template:\n{{target}}\nInput Template:\nDivida y simplifique la siguiente oración conservando su significado completo:\n{{source}}\nVersión simplificada:\nTarget Template:\n{{target}}\nInput Template:\n{{source}}\nLa frase anterior es muy complicada. Por favor, proporcione una versión sinónima\nsimplificada que consta de varias oraciones:\nTarget Template:\n{{target}}\n1.1.3\nGEM/BiSECT fr\nData Example\nPrompts\nInput Template:\n\nKey\nValue\ngem_id\nBiSECT_fr-train-1\nsource\nN'ayez pas peur de poser des questi...\ntarget\nIl ne faut pas avoir peur de poser ...\nreferences\nIl ne faut pas avoir peur de poser ...\nDivisez et simplifiez la phrase suivante tout en conservant son sens complet :\n{{source}}\nVersion simplifiée :\nTarget Template:\n{{target}}\nInput Template:\n{{source}}\nLa phrase ci-dessus est très compliquée. Veuillez me fournir une version synonyme\nsimplifiée composée de plusieurs phrases :\nTarget Template:\n{{target}}\nInput Template:\n{{source}}. Cette phrase est difficile à comprendre. Une version plus simple avec\nune signification équivalente est la suivante :\nTarget Template:\n{{target}}\n1.2\nSummarization\n1.2.1\nGEM/wiki_lingua en\nDataset from lad (2020). Used in training.\nData Example\nPrompts\nNotes: xsum DOC_write_summary_of_above template\n\nKey\nValue\ngem_id\nwikilingua_multilingual-train-42437...\ngem_parent_id\nwikilingua_multilingual-train-42437...\nsource_language\nen\ntarget_language\nen\nsource\nGo online and simply search “Decor ...\ntarget\nTake a quiz online to find your sty...\nreferences\nTake a quiz online to find your sty...\nInput Template:\n{{source}}\n===\nWrite a summary of the text above in English :\nTarget Template:\n{{target}}\nNotes: xsum ’article_DOC_summary’ template\nInput Template:\nArticle in English: {{source}}\nSummary in English:\nTarget Template:\n{{target}}\nNotes: xsum ’DOC_how_would_you_rephrase_few_words’ template\nInput Template:\n{{source}}\nHow would you rephrase that briefly in English?\nTarget Template:\n{{target}}\nNotes: xsum ’DOC_tldr’ template\nInput Template:\n\n{{source}}\nTL;DR in English:\nTarget Template:\n{{target}}\nNotes: xsum ’read_below_DOC_write_abstract’ template\nInput Template:\nFirst, read the English article below.\n{{source}}\nNow, please write a short abstract for it in English.\nTarget Template:\n{{target}}\nInput Template:\n{{target}}\nGiven the above abstract, write an English article for it.\nTarget Template:\n{{source}}\nInput Template:\n{{target}}\nI'm interested in that, but I only have a few mins. Can you give me the first 500\ncharacters of an article about that?\nTarget Template:\n{{source[:500]}}\n1.2.2\nGEM/wiki_lingua es\nData Example\n\nKey\nValue\ngem_id\nwikilingua_multilingual-train-34808...\ngem_parent_id\nwikilingua_multilingual-train-34808...\nsource_language\nes\ntarget_language\nes\nsource\nNavega en la web y simplemente busc...\ntarget\nHaz un cuestionario en línea para e...\nreferences\nHaz un cuestionario en línea para e...\nPrompts\nNotes: xsum templates\nInput Template:\n{{source}}\n===\nWrite a summary of the text above in Spanish:\nTarget Template:\n{{target}}\nNotes: xsum templates\nInput Template:\nFirst, read the Spanish article below.\n{{source}}\nNow, please write a short abstract for it in Spanish.\nTarget Template:\n{{target}}\nNotes: xsum templates\nInput Template:\n{{source}}\nTL;DR in Spanish:\nTarget Template:\n\n{{target}}\nNotes: xsum templates\nInput Template:\nArticle in Spanish: {{source}}\nSummary in Spanish:\nTarget Template:\n{{target}}\nNotes: xsum templates\nInput Template:\n{{source}}\nHow would you rephrase that briefly in Spanish?\nTarget Template:\n{{target}}\n1.2.3\nGEM/xlsum bengali\nDataset from Hasan et al. (2021). Used in training.\nData Example\nKey\nValue\ngem_id\nxlsum_bengali-train-2\nurl\nhttps://www.bbc.com/bengali/news-50...\ntitle\nরািশয়ায়ক্ষমতার২০বছরেযভােবেকট...\ntarget\nভ্লািদিমরপুিতনতাঁরক্ষমতায়থাকার...\nreferences\nভ্লািদিমরপুিতনতাঁরক্ষমতায়থাকার...\ntext\nগত২০বছেরিতিনরািশয়ারেপৰ্িসেডন্...\nPrompts\nInput Template:\n\nএকিটিনবেন্ধরনীেচরিশেরানামএবংসারাংশেদওয়া, একিটেছাটিনবন্ধৈতিরকরুনবাতােদরসােথেযেত\nএকিটদীঘর্িনবেন্ধরশুরুকরুন।িশেরানাম: {{title}}সারাংশ: {{target}}\nTarget Template:\n{{text[:500]}}\nInput Template:\nিবষয়বস্তু: {{text[:7000]}}\nTarget Template:\n{{target}}\nInput Template:\nডকসংিক্ষপ্তকরারজনয্: {{text[:8500]}}\nTarget Template:\n{{target}}\nInput Template:\n...{{text[3000:3500]}}\nTarget Template:\n{{text[5000:]}}\nInput Template:\nিশেরানাম: {{title}}\nTarget Template:\n{{text[:7000]}}\nInput Template:\n{{text}}\nTarget Template:\n{{title}}\nInput Template:\n\nিশেরানাম: {{title}}\nTarget Template:\n{{text[:7000]}}\nInput Template:\n{{title}}{{text[:5000]}}\nTarget Template:\n{{target}}\nInput Template:\n{{text[:1000]}}\nTarget Template:\n{{text[1000:5000]}}\n1.2.4\nGEM/xlsum english\nData Example\nKey\nValue\ngem_id\nxlsum_english-train-2\nurl\nhttps://www.bbc.com/news/uk-scotlan...\ntitle\nHuge tidal turbine installed at Ork...\ntarget\nThe massive tidal turbine AK1000 ha...\nreferences\nThe massive tidal turbine AK1000 ha...\ntext\nAtlantis Resources unveiled the mar...\nPrompts\nInput Template:\nDoc to summarize: {{text[:8500]}}\\nSummary in the same language as the doc:\nTarget Template:\n{{target}}\nInput Template:\n\nContent: {{text[:7000]}}\\nThe previous content can be summarized as follows:\nTarget Template:\n{{target}}\nInput Template:\n{{title}}\\n{{text[:5000]}}\\n\\ntl;dr:\nTarget Template:\n{{target}}\nInput Template:\n{{text}} \\n\\nGive me a good title for the article above.\nTarget Template:\n{{title}}\nInput Template:\nGiven the below title and summary of an article, generate a short article or the\nbeginning of a long article to go along with them. Title: {{title}}\\nSummary:\n{{target}}\\nArticle (Max 500 characters):\nTarget Template:\n{{text[:500]}}\nInput Template:\nTitle: {{title}}\\nGiven the above title of an imaginary article, imagine the\narticle.\\n\nTarget Template:\n{{text[:7000]}}\nInput Template:\n\nTitle: {{title}}\\nGiven the above title of an imaginary article, imagine the\narticle.\\n\nTarget Template:\n{{text[:7000]}}\nInput Template:\n{{text[:1000]}}... Continue the article for another 4000 characters max:\nTarget Template:\n{{text[1000:5000]}}\nInput Template:\n...{{text[3000:3500]}}... Write the rest of the article:\nTarget Template:\n{{text[5000:]}}\n1.3\nTranslation\n1.3.1\nHelsinki-NLP/tatoeba_mt ben-eng\nDataset from Tiedemann (2020). Used in training.\nData Example\nKey\nValue\nsourceLang\nben\ntargetlang\neng\nsourceString\nTatoebaরঅথর্কী?\ntargetString\nWhat does \"Tatoeba\" mean?\nPrompts\nInput Template:\nTranslate the following text from English to Bengali {{ targetString }}\n\nTarget Template:\n{{ sourceString }}\nInput Template:\nTranslate the following text from Bengali to English {{ sourceString }}\nTarget Template:\n{{ targetString }}\n1.3.2\nHelsinki-NLP/tatoeba_mt eng-fra\nData Example\nKey\nValue\nsourceLang\neng\ntargetlang\nfra\nsourceString\nAah. Now I understand.\ntargetString\nAh ! Maintenant, je comprends.\nPrompts\nInput Template:\nTranslate the following text from French to English {{ targetString }}\nTarget Template:\n{{ sourceString }}\nInput Template:\nTranslate the following text from English to French {{ sourceString }}\nTarget Template:\n{{ targetString }}\n\n1.3.3\nfacebook/flores ben_Beng-eng_Latn\nDataset from NLLB (2022). Used in training.\nData Example\nKey\nValue\nid\n2\nURL\nhttps://en.wikinews.org/wiki/Scient...\ndomain\nwikinews\ntopic\nhealth\nhas_image\n0\nhas_hyperlink\n0\nsentence_ben_Beng\nশীষর্গেবষকরাবলেছন, এিটিনম্ন-আেয়...\nsentence_eng_Latn\nLead researchers say this may bring...\nPrompts\nInput Template:\n{{sentence_ben_Beng}}\nTarget Template:\n{{sentence_eng_Latn}}\nInput Template:\nA text in Bengali: {{sentence_ben_Beng}}\nTarget Template:\n{{sentence_eng_Latn}}\nInput Template:\n{{sentence_ben_Beng}}\nTarget Template:\n{{sentence_eng_Latn}}\n1.3.4\nfacebook/flores ben_Beng-fra_Latn\nData Example\n\nKey\nValue\nid\n2\nURL\nhttps://en.wikinews.org/wiki/Scient...\ndomain\nwikinews\ntopic\nhealth\nhas_image\n0\nhas_hyperlink\n0\nsentence_ben_Beng\nশীষর্গেবষকরাবলেছন, এিটিনম্ন-আেয়...\nsentence_fra_Latn\nSelon les chercheurs principaux, ce...\nPrompts\nInput Template:\n{{sentence_ben_Beng}}\nTarget Template:\n{{sentence_fra_Latn}}\nInput Template:\n{{sentence_ben_Beng}}\nTarget Template:\n{{sentence_fra_Latn}}\nInput Template:\nA text in Bengali: {{sentence_ben_Beng}}\nTarget Template:\n{{sentence_fra_Latn}}\n1.4\nProgram Synthesis\n1.4.1\nMuennighoff/mbpp sanitized\nDataset from Austin et al. (2021). Used in training.\nData Example\nPrompts\nInput Template:\n\nKey\nValue\nsource_file\nBenchmark Questions Verification V2...\ntask_id\n3\nprompt\nWrite a python function to identify...\ncode\nimport math\ndef is_not_prime(n):\n...\ntest_imports\ntest_list\nassert is_not_prime(2) == False;ass...\n{{ prompt }} Here is a solution in Python:\nTarget Template:\n{{ code }}\nNote: the prompt does not correspond to the original task intended by the dataset authors.\nInput Template:\n{{ prompt }} This can be solved in Python with the following code:\nTarget Template:\n{{ code }}\n1.4.2\ncodeparrot/apps all\nDataset from Hendrycks et al. (2021). Used in training.\nData Example\nKey\nValue\nproblem_id\n1\nquestion\nMikhail walks on a Cartesian plane....\nsolutions\n[\"q=int(input())\\n\\nfor e in range(...\ninput_output\n{\n\"inputs\": [\n\"3\\n2 2 3\\n4 3 ...\ndiﬀiculty\ninterview\nurl\nhttps://codeforces.com/problemset/p...\nstarter_code\nPrompts\nInput Template:\n\nSolve in Python:\n{{ question }}\nTarget Template:\n{{ solution }}\nInput Template:\n{{ question }}\nCan you solve the above problem using Python?\nTarget Template:\n{{ solution }}\nInput Template:\nI found an interesting problem on {{url}}:\n{{ question }}\nI tried it in Python, but could not do it. Can you solve it?\nTarget Template:\n{{ solution }}\n1.4.3\ncodeparrot/github-jupyter-text-code-pairs\nData Example\nKey\nValue\nmarkdown\nExtract the dataset from the compre...\ncode\nnum_classes = 10\nnp.random.seed(133...\npath\nmachine-learning/deep-learning/udac...\nrepo_name\npk-ai/training\nlicense\nmit\nPrompts\nInput Template:\n\n\"{{ markdown }}\"\nPlease write code following the instructions in jupyter notebook style.\nTarget Template:\n{{ code }}\nInput Template:\nI am working on the file \"{{ path }}\".\nThe first task is:\n{{ markdown }}\nCan you write Python code for it?\nTarget Template:\n{{ code }}\nNote: the prompt does not correspond to the original task intended by the dataset authors.\nInput Template:\n{{ markdown }}\nTarget Template:\n{{ code }}\nNote: the prompt does not correspond to the original task intended by the dataset authors.\nInput Template:\n{{ code }}\nGiven the above code, generate some markdown instructions for it.\nTarget Template:\n{{ markdown }}\n1.4.4\ncodeparrot/xlcost-text-to-code C++-program-level\nDataset from Zhu et al. (2022). Used in training.\nData Example\n\nKey\nValue\ntext\nCheck if a number can be represente...\ncode\n#include <bits/stdc++.h> NEW_LINE u...\nPrompts\nInput Template:\n\"{{ text }}\"\nSolution in C++:\nTarget Template:\n{{ code_clean }}\nInput Template:\n\"{{ text }}\"\nHow can the above be solved in C++?\nTarget Template:\n{{ code_clean }}\n1.4.5\ncodeparrot/xlcost-text-to-code C-program-level\nData Example\nKey\nValue\ntext\nLogarithm tricks for Competitive Pr...\ncode\n#include <stdio.h> NEW_LINE #includ...\nPrompts\nInput Template:\n\"{{ text }}\"\nSolution in C:\nTarget Template:\n{{ code_clean }}\n\nInput Template:\n{{ text }}\nHow can the above be solved in C?\nTarget Template:\n{{ code_clean }}\n1.4.6\ncodeparrot/xlcost-text-to-code Csharp-program-level\nData Example\nKey\nValue\ntext\nCheck if a number can be represente...\ncode\nusing System ; class GFG { static b...\nPrompts\nInput Template:\n\"{{ text }}\"\nSolution in C#:\nTarget Template:\n{{ code_clean }}\nInput Template:\n\"{{ text }}\"\nHow can the above be solved in C-Sharp?\nTarget Template:\n{{ code_clean }}\n1.4.7\ncodeparrot/xlcost-text-to-code Java-program-level\nData Example\n\nKey\nValue\ntext\nCheck if a number can be represente...\ncode\nimport java . io . * ; class GFG { ...\nPrompts\nInput Template:\n\"{{ text }}\"\nSolution in Java:\nTarget Template:\n{{ code_clean }}\nInput Template:\n\"{{ text }}\"\nHow can the above be solved in Java?\nTarget Template:\n{{ code_clean }}\n1.4.8\ncodeparrot/xlcost-text-to-code Javascript-program-level\nData Example\nKey\nValue\ntext\nCheck if a number can be represente...\ncode\nfunction sumOfTwoCubes ( n ) { var ...\nPrompts\nInput Template:\n\"{{ text }}\"\nSolution in Javascript:\nTarget Template:\n{{ code_clean }}\n\nInput Template:\n\"{{ text }}\"\nHow can the above be solved in JS?\nTarget Template:\n{{ code_clean }}\n1.4.9\ncodeparrot/xlcost-text-to-code PHP-program-level\nData Example\nKey\nValue\ntext\nRearrange the array to maximize the...\ncode\n< ? php function solve ( $ a , $ n ...\nPrompts\nInput Template:\n\"{{ text }}\"\nSolution in php:\nTarget Template:\n{{ code_clean }}\nInput Template:\n\"{{ text }}\"\nHow can the above be solved in PHP?\nTarget Template:\n{{ code_clean }}\n1.4.10\ncodeparrot/xlcost-text-to-code Python-program-level\nData Example\n\nKey\nValue\ntext\nCheck if a number can be represente...\ncode\nimport math NEW_LINE def sumOfTwoCu...\nPrompts\nInput Template:\n\"{{ text }}\"\nSolution in Python:\nTarget Template:\n{{ code_clean }}\nInput Template:\n\"{{ text }}\"\nHow can the above be solved in Python?\nTarget Template:\n{{ code_clean }}\n1.4.11\nneural_code_search evaluation_dataset\nDataset from hug (2018). Used in training.\nData Example\nKey\nValue\nstackoverflow_id\n4616095\nquestion\nHow to get the build/version number...\nquestion_url\nhttps://stackoverflow.com/questions...\nquestion_author\nFahad Ali Shaikh\nquestion_author_url\nhttps://stackoverflow.com/users/565...\nanswer\ntry {\nPackageInfo pInfo = this.ge...\nanswer_url\nhttps://stackoverflow.com/a/6593822\nanswer_author\nplus-\nanswer_author_url\nhttps://stackoverflow.com/users/709...\nexamples\n4130029;3398176;2320640\nexamples_url\nhttps://github.com/altanizio/Concei...\nPrompts\n\nNote: the prompt does not correspond to the original task intended by the dataset authors.\nInput Template:\nDescription:\n{{ question }}\nImplementation:\nTarget Template:\n{{ answer }}\nNote: the prompt does not correspond to the original task intended by the dataset authors.\nInput Template:\nGiven the following code:\n{{ answer }}\nDescribe it:\nTarget Template:\n{{ question }}\n1.4.12\nteven/code_contests\nData Example\nKey\nValue\nname\n1575_A. Another Sorting Problem\ndescription\nAndi and Budi were given an assignm...\nsource\n2\ndiﬀiculty\n7\nsolution\n#include <bits/stdc++.h>\nusing name...\nlanguage\nCPP\nPrompts\nInput Template:\n{{description}}\nTarget Template:\n{{solution}}\n\nInput Template:\nCan you solve the below in {{language}}?\n{{description}}\nTarget Template:\n{{solution}}\nInput Template:\n{{description}}\nThe above is tricky. Write me a correct solution in {{language}}.\nTarget Template:\n{{solution}}\nInput Template:\n{{description}}\nSolve the task in {{language}}.\nTarget Template:\n{{solution}}\nInput Template:\n{{description}}\nUsing {{language | lower}} can you solve the prior task?\nTarget Template:\n{{solution}}\nInput Template:\n{{description}}\n{{solution[:5]}}\nTarget Template:\n\n{{solution[5:]}}\nInput Template:\n{{language}} solution for \"{{description}}\":\nTarget Template:\n{{solution}}\n1.5\nCoreference Resolution\n1.5.1\nMuennighoff/xwinograd en\nDataset from Tikhonov and Ryabinin (2021). Used in evaluation.\nData Example\nKey\nValue\nsentence\nThe city councilmen refused the dem...\noption1\nThe city councilmen\noption2\nthe demonstrators\nanswer\n2\nPrompts\nInput Template:\n{{sentence}}\nReplace the _ in the above sentence with the correct option:\n- {{option1}}\n- {{option2}}\nTarget Template:\n{% if answer == '1' %} {{option1}} {% else %} {{ option2 }} {% endif %}\nAnswer Choices Template:\n{{option1}} ||| {{option2}}\nInput Template:\n\nFill in the _ in the below sentence:\n{{sentence}}\nChoices:\n- {{ option1 }}\n- {{ option2 }}\nAnswer:\nTarget Template:\n{% if answer == '1' %} {{option1}} {% else %} {{ option2 }} {% endif %}\nAnswer Choices Template:\n{{option1}} ||| {{option2}}\nNote: the prompt does not correspond to the original task intended by the dataset authors.\nInput Template:\nThe _ in the sentence below refers to {{option1}}. True or False?\n{{sentence}}\nTarget Template:\n{{answer_choices[answer|int - 1]}}\nAnswer Choices Template:\nTrue ||| False\nInput Template:\n{{ sentence }} In the previous sentence, does _ refer to {{ option1 }} or\n{{\noption2 }}?\nTarget Template:\n{% if answer == '1' %} {{option1}} {% else %} {{ option2 }} {% endif %}\nAnswer Choices Template:\n{{ option1 }} ||| {{ option2 }}\nInput Template:\n\n{{sentence}}\nWhat does the _ in the above sentence refer to? {{ option1 }} or {{ option2 }}?\nTarget Template:\n{% if answer == '1' %} {{option1}} {% else %} {{ option2 }} {% endif %}\nAnswer Choices Template:\n{{option1}} ||| {{option2}}\nInput Template:\nIn the sentence below, does the _ stand for {{answer_choices[0]}} or\n{{answer_choices[1]}}?\n{{sentence}}\nTarget Template:\n{{answer_choices[answer | int - 1]}}\nAnswer Choices Template:\n{{option1}} ||| {{option2}}\n1.5.2\nMuennighoff/xwinograd fr\nData Example\nKey\nValue\nsentence\nLa coupe n'entre pas dans la valise...\noption1\nLa coupe\noption2\nla valise\nanswer\n2\nPrompts\nInput Template:\n{{ sentence }} Dans la phrase précédente, _ fait-il référence à {{ option1 }} ou\n{{ option2 }} ?\nTarget Template:\n\n{% if answer == '1' %} {{option1}} {% else %} {{ option2 }} {% endif %}\nAnswer Choices Template:\n{{ option1 }} ||| {{ option2 }}\nInput Template:\nDans la phrase ci-dessous, le _ signifie-t-il {{answer_choices[0]}} ou\n{{answer_choices[1]}} ?\n{{sentence}}\nTarget Template:\n{{answer_choices[answer | int - 1]}}\nAnswer Choices Template:\n{{option1}} ||| {{option2}}\nInput Template:\n{{sentence}}\nRemplacez le _ dans la phrase ci-dessus par l'option correcte :\n- {{option1}}\n- {{option2}}\nTarget Template:\n{% if answer == '1' %} {{option1}} {% else %} {{ option2 }} {% endif %}\nAnswer Choices Template:\n{{option1}} ||| {{option2}}\nInput Template:\n{{sentence}}\nÀ quoi le _ dans la phrase ci-dessus fait-il référence ? {{ option1 }} ou {{\noption2 }} ?\nTarget Template:\n{% if answer == '1' %} {{option1}} {% else %} {{ option2 }} {% endif %}\n\nAnswer Choices Template:\n{{option1}} ||| {{option2}}\nInput Template:\nLe _ dans la phrase ci-dessous fait référence à {{option1}}. Vrai ou faux?\n{{sentence}}\nTarget Template:\n{{answer_choices[answer|int - 1]}}\nAnswer Choices Template:\nVrai ||| Faux\n1.6\nQuestion Answering Multiple Choice\n1.6.1\nclue c3\nDataset from Sun et al. (2020). Used in training.\nData Example\nKey\nValue\nid\n1\ncontext\n男：足球比赛是明天上午八点开始吧?;女：因为天气不好，比赛改到后天下午...\nquestion\n根据对话，可以知道什么?\nchoice\n今天天气不好;比赛时间变了;校长忘了时间\nanswer\n比赛时间变了\nPrompts\nInput Template:\n{% for statement in context %}\n{{ statement }}\n{% endfor %}\n鉴于上面的对话/段落，问题“{{question}}”的答案是什么\nTarget Template:\n{{ answer }}\n\nInput Template:\n段落：{% for statement in context %}\n{{ statement }}\n{% endfor %}\n什么样的问题会引起{{ answer }} 的回答响应？\nTarget Template:\n{{ question }}\nInput Template:\n{% for statement in context %}\n{{ statement }}\n{% endfor %}\nGiven the dialogue / passage above, use the following options to answer the\nquestion \"{{question}}\".\nOptions:\n- {{ answer_choices | join('\\n- ') }}\nTarget Template:\n{{ answer }}\nAnswer Choices Template:\n{{ choice | join(\" ||| \") }}\nInput Template:\n{% for statement in context %}\n{{ statement }}\n{% endfor %}\n鉴于上面的对话/段落，使用以下选项回答问题“{{question}}”。\n选项：\n- {{ answer_choices | join('\n- ') }}\nTarget Template:\n{{ answer }}\nAnswer Choices Template:\n{{ choice | join(\" ||| \") }}\n\nInput Template:\nPassage: {% for statement in context %}\n{{ statement }}\n{% endfor %}\nQuestion: \"{{question}}\"\nAnswer choices: {{ answer_choices[:-1] | join(', ') }}, or\n{{ answer_choices[-1]\n}}?\nTarget Template:\n{{ answer }}\nAnswer Choices Template:\n{{ choice | join(\" ||| \") }}\nNote: the prompt does not correspond to the original task intended by the dataset authors.\nInput Template:\nPassage: {% for statement in context %}\n{{ statement }}\n{% endfor %}\nWhat kind of question would elicit an answer response of {{ answer }}?\nTarget Template:\n{{ question }}\nInput Template:\n段落：{% for statement in context %}\n{{ statement }}\n{% endfor %}\n问题：“{{question}}”\n答案选择：{{ answer_choices[:-1] | join(', ') }} 还是{{ answer_choices[-1] }}？\nTarget Template:\n{{ answer }}\nAnswer Choices Template:\n{{ choice | join(' ||| ') }}\nInput Template:\n\n{% for statement in context %}\n{{ statement }}\n{% endfor %}\nGiven the dialogue / passage above, what is the answer for the question\n\"{{question}}\"\nAnswer choices: {{ answer_choices[:-1] | join(', ') }}, or\n{{ answer_choices[-1]\n}}?\nTarget Template:\n{{ answer }}\nAnswer Choices Template:\n{{ choice | join(' ||| ') }}\nInput Template:\n{% for statement in context %}\n{{ statement }}\n{% endfor %}\n鉴于上面的对话/段落，问题“{{question}}”的答案是什么\n答案选择：{{ answer_choices[:-1] | join(', ') }} 还是{{ answer_choices[-1] }}？\nTarget Template:\n{{ answer }}\nAnswer Choices Template:\n{{ choice | join(' ||| ') }}\nNote: the prompt does not correspond to the original task intended by the dataset authors.\nInput Template:\n{% for statement in context %}\n{{ statement }}\n{% endfor %}\nGiven the dialogue / passage above, what is the answer for the question\n\"{{question}}\"\nTarget Template:\n{{ answer }}\n\n1.7\nQuestion Answering Extractive\n1.7.1\nclue cmrc2018\nDataset from Cui et al. (2018). Used in training.\nData Example\nKey\nValue\nid\nTRAIN_186_QUERY_1\ncontext\n范廷颂枢机（，），圣名保禄·若瑟（），是越南罗马天主教枢机。1963年...\nquestion\n1990年，范廷颂担任什么职务？\nanswers\n{'text': ['1990年被擢升为天主教河内总教区宗座署理'],...\nPrompts\nInput Template:\n问：{{ question }}你能写一些上下文来回答这个问题吗？\nTarget Template:\n{{ context }}\nNote: the prompt does not correspond to the original task intended by the dataset authors.\nInput Template:\nGiven this context \"{{ context }}\", generate a question that would return the\nanswer of \"{{ answers['text'][0] }}\".\nTarget Template:\n{{ question }}\nInput Template:\n{{ context }}\n{{ question }} 的答案在上面的段落中。它是什么？\nTarget Template:\n{{ answers['text'][0] }}\nInput Template:\n\nIn an exam, you are asked {{ question }}, and you are tasked to find the answer\nfrom the following passage.\n{{ context }}\nWhat's the answer?\nTarget Template:\n{{ answers['text'][0] }}\nInput Template:\n{{ context }}\nThe answer to {{ question }} is in the passage above. What is it?\nTarget Template:\n{{ answers['text'][0] }}\nInput Template:\nAnswer the question using the given context.\nQuestion: {{ question }}\nContext: {{ context }}\nAnswer:\nTarget Template:\n{{ answers['text'][0] }}\nInput Template:\nQ: {{ question }} Can you write some context to answer the question?\nTarget Template:\n{{ context }}\nInput Template:\n{{ context[:answers[\"answer_start\"][0]-5] }}... How would you continue the prior\ntext to answer \"{{ question }}\"?\nTarget Template:\n\n{{ context[answers[\"answer_start\"][0]-5:] }}\nInput Template:\n{{ context[:answers[\"answer_start\"][0]-5] }}... 你将如何继续前面的文本来回答“{{\nquestion }}”？\nTarget Template:\n{{ context[answers[\"answer_start\"][0]-5:] }}\nInput Template:\n使用给定的上下文回答问题。\n问题：{{ question }}\n上下文：{{ context }}\n答案：\nTarget Template:\n{{ answers['text'][0] }}\nInput Template:\n在考试中，你被问到{{ question }}，你的任务是从以下段落中找到答案。\n{{ context }}\n答案是什么？\nTarget Template:\n{{ answers['text'][0] }}\nInput Template:\n给定这个上下文“{{ context }}”，生成一个返回“{{ answers['text'][0] }}”答案的问题。\nTarget Template:\n{{ question }}\n1.7.2\nclue drcd\nDataset from Xu et al. (2020). Used in training.\n\nData Example\nKey\nValue\nid\n1001-10-2\ncontext\n2010年引進的廣州快速公交運輸系統，屬世界第二大快速公交系統，日常載...\nquestion\n從哪一天開始在廣州市\u0000騎摩托車會被\u0000收？\nanswers\n{'text': ['2007年1月16日'], 'answer_st...\nPrompts\nInput Template:\n{{ context }}\n{{ question }} 的答案在上面的段落中。它是什么？\nTarget Template:\n{{ answers['text'][0] }}\nInput Template:\nAnswer the question using the given context.\nQuestion: {{ question }}\nContext: {{ context }}\nAnswer:\nTarget Template:\n{{ answers['text'][0] }}\nInput Template:\n{{context[:answers[\"answer_start\"]-5]}}... 你将如何继续前面的文本来回答“{{\nquestion}}”？\nTarget Template:\n{{context[answers[\"answer_start\"]-5:]}}\nInput Template:\n在考试中，你被问到{{ question }}，你的任务是找到回答问题的段落。写这样一段话：\nTarget Template:\n\n{{ context }}\nInput Template:\n{{ context }}\nThe answer to {{ question }} is in the passage above. What is it?\nTarget Template:\n{{ answers['text'][0] }}\nInput Template:\n在考试中，你被问到{{ question }}，你的任务是从以下段落中找到答案。\n{{ context }}\n答案是什么？\nTarget Template:\n{{ answers['text'][0] }}\nInput Template:\n给定这个上下文“{{ context }}”，生成一个返回“{{ answers['text'][0] }}”答案的问题。\nTarget Template:\n{{ question }}\nInput Template:\n{{context[:answers[\"answer_start\"]-5]}}... How would you continue the prior text\nto answer \"{{ question }}\"?\nTarget Template:\n{{context[answers[\"answer_start\"]-5:]}}\nInput Template:\n使用给定的上下文回答问题。\n问题：{{ question }}\n上下文：{{ context }}\n答案：\n\nTarget Template:\n{{ answers['text'][0] }}\nNote: the prompt does not correspond to the original task intended by the dataset authors.\nInput Template:\nGiven this context \"{{ context }}\", generate a question that would return the\nanswer of \"{{ answers['text'][0] }}\".\nTarget Template:\n{{ question }}\nInput Template:\nIn an exam, you are asked {{ question }}, and you are tasked to find the answer\nfrom the following passage.\n{{ context }}\nWhat's the answer?\nTarget Template:\n{{ answers['text'][0] }}\nInput Template:\nIn an exam, you are asked {{ question }}, and you are tasked to find a passage\nanswering the question. Write such a passage:\nTarget Template:\n{{ context }}\n1.7.3\nmlqa mlqa.vi.vi\nDataset from Lewis et al. (2019). Used in training.\nData Example\n\nKey\nValue\ncontext\nThành phốMiêu Lật tiếng Trung:苗栗市,...\nquestion\nMiaoli có tỷlệcao loại người nào?\nanswers\n{'answer_start': [311], 'text': ['K...\nid\n2f0d6ff162619164bb113c0cadbcca06a50...\nPrompts\nInput Template:\n{{ context[:answers.answer_start[0]-5]}} ... Tiếp tục ởtrên, sao cho nó trảlời\n\"{{question}}\":\nTarget Template:\n{{ context[answers.answer_start[0]-5:]}}\nInput Template:\n{{context}}\nVới sựtham chiếu đến ngữcảnh trên, {{question}}\nTarget Template:\n{{answers.text[0]}}\nInput Template:\n{{context}}\nH: {{question}}\nĐềcập đến đoạn văn trên, câu trảlời đúng cho câu hỏi đã cho trong ngôn ngữcủa\nđoạn văn là\nTarget Template:\n{{answers[\"text\"][0]}}\nInput Template:\nCâu hỏi: {{question}}\nNgữcảnh: {{context}}\nCâu trảlời từngữcảnh:\nTarget Template:\n\n{{answers.text[0]}}\nInput Template:\nTham khảo đoạn văn dưới đây và sau đó trảlời câu hỏi sau đó bằng ngôn ngữtương\ntựnhư đoạn văn:\nĐoạn: {{context}}\nCâu hỏi: {{question}}\nTarget Template:\n{{answers[\"text\"][0]}}\nInput Template:\nTôi đã tìm thấy một văn bản trảlời \"{{question}}\" bằng {{answers.text[0]}}. Nó\nbắt đầu bằng \"{{ context[:10] }}\". Bạn có thểtiếp tục nó không?\nTarget Template:\n{{ context[10:] }}\nInput Template:\nĐọc đoạn văn sau và sau đó trảlời câu hỏi tiếp theo bằng cách trích một phần đúng\ntrong đoạn văn:\n{{context}}\n{{question}}\nTarget Template:\n{{answers.text[0]}}\nInput Template:\nD: {{context}}\nH: {{question}}\nA:\nTarget Template:\n\n{{answers[\"text\"][0]}}\n1.7.4\nmlqa mlqa.zh.zh\nData Example\nKey\nValue\ncontext\n楚河州包括有整个楚河河谷及邻近的山脉与峡谷。河谷的黑土非常肥沃，而且被...\nquestion\n哪水体有助土地如此多产？\nanswers\n{'answer_start': [36], 'text': ['楚河...\nid\n1aee17dd937cc1043e3ff47c38396541fc3...\nPrompts\nInput Template:\n阅读下面的短文，然后从短文中选出正确的部分来回答下面的问题：\n{{context}}\n{{question}}\nTarget Template:\n{{answers.text[0]}}\nInput Template:\n{{ context[:answers.answer_start[0]-5]}}... 继续上述操作，使其回答“{{question}}”：\nTarget Template:\n{{ context[answers.answer_start[0]-5:]}}\nInput Template:\nD：{{context}}\n问：{{question}}\n答：\nTarget Template:\n{{answers[\"text\"][0]}}\n\nInput Template:\n我找到了一个用{{answers.text[0]}} 回答“{{answers.text[0]}}”的文本。它以“{{\ncontext[:10] }}”开头。可以继续吗？\nTarget Template:\n{{ context[10:] }}\nInput Template:\n问题：{{question}}\n上下文：{{context}}\n从上下文中回答：\nTarget Template:\n{{answers.text[0]}}\nInput Template:\n参考下面的段落，然后用与段落相同的语言回答问题：\n段落：{{context}}\n问题：{{question}}\nTarget Template:\n{{answers[\"text\"][0]}}\nInput Template:\n{{context}}\n参考上述上下文，{{question}}\nTarget Template:\n{{answers.text[0]}}\nInput Template:\n\n{{context}}\n问：{{question}}\n参考上面的段落，用该段落的语言对给定问题的正确答案是\nTarget Template:\n{{answers[\"text\"][0]}}\n1.7.5\nxquad xquad.vi\nDataset from Artetxe et al. (2019). Used in training.\nData Example\nKey\nValue\nid\n56beb4343aeaaa14008c925c\ncontext\nĐội thủcủa Panthers chỉthua 308 đ...\nquestion\nJared Allen có bao nhiêu lần vật ng...\nanswers\n{'text': ['136'], 'answer_start': [...\nPrompts\nInput Template:\n{{context}}\nVới sựtham chiếu đến ngữcảnh trên, {{question}}\nTarget Template:\n{{answers.text[0]}}\nInput Template:\nĐưa ra câu trảlời {{answers.text[0]}} cho {{question}}, hãy viết một văn bản giải\nthích điều này. Câu trảlời phải bắt đầu ởsốký tự{{answers.answer_start[0]}}.\nVăn bản:\nTarget Template:\n{{context}}\nInput Template:\n\n{{question}} Rõ ràng là {{answers.text[0]}}. Bạn có thểcung cấp cho tôi một số\nbối cảnh?\nTarget Template:\n{{context}}\nInput Template:\n{{context}}\nH: {{question}}\nĐềcập đến đoạn văn trên, câu trảlời chính xác cho câu hỏi được đưa ra là\nTarget Template:\n{{answers[\"text\"][0]}}\nInput Template:\n{{context}}\nH: {{question}}\nA:\nTarget Template:\n{{answers[\"text\"][0]}}\nInput Template:\nĐọc đoạn văn sau và trảlời câu hỏi sau:\n{{context}}\n{{question}}\nTarget Template:\n{{answers.text[0]}}\nInput Template:\n\nTham khảo đoạn văn dưới đây và trảlời câu hỏi sau:\nĐoạn: {{context}}\nCâu hỏi: {{question}}\nTarget Template:\n{{answers[\"text\"][0]}}\nInput Template:\n{{context}}\nTừđoạn văn trên, một câu hỏi hợp lý với \"{{answers[\"text\"][0]}}\" như câu trảlời\nsẽlà:\nTarget Template:\n{{question}}\nInput Template:\n{{context}}\nTạo câu hỏi từđoạn văn trên:\nTarget Template:\n{{question}}\n1.7.6\nxquad xquad.zh\nData Example\nKey\nValue\nid\n56beb4343aeaaa14008c925c\ncontext\n黑豹队的防守只丢了308分，在联赛中排名第六，同时也以24 次拦截...\nquestion\n贾里德在职业生涯中有多少次擒杀？\nanswers\n{'text': ['136 次'], 'answer_start':...\nPrompts\nInput Template:\n\n阅读下面的短文，回答下面的问题：\n{{context}}\n{{question}}\nTarget Template:\n{{answers.text[0]}}\nInput Template:\n{{context}}\n问：{{question}}\n参考上面的段落，给定问题的正确答案是\nTarget Template:\n{{answers[\"text\"][0]}}\nInput Template:\n参考下面的短文，回答下列问题：\n段落：{{context}}\n问题：{{question}}\nTarget Template:\n{{answers[\"text\"][0]}}\nInput Template:\n{{context}}\n从上面的段落中，一个以“{{answers[\"text\"][0]}}\" 为答案的合理问题将是：\nTarget Template:\n{{question}}\nInput Template:\n\n{{context}}\n从上面的段落中产生一个问题：\nTarget Template:\n{{question}}\nInput Template:\n{{context}}\n参考上述上下文，{{question}}\nTarget Template:\n{{answers.text[0]}}\nInput Template:\n{{context}}\n问：{{question}}\n答：\nTarget Template:\n{{answers[\"text\"][0]}}\n1.8\nTopic Classification\n1.8.1\nclue csl\nData Example\nKey\nValue\nidx\n1\ncorpus_id\n2565\nabst\n针对核函数参数选择的重要性,提出了粒子群(PSO)模式搜索算法来搜索最...\nlabel\n-1\nkeyword\n模式搜索;支持向量机;核参数选取\nPrompts\n\nInput Template:\nAfter John wrote the abstract \"{{abst}}\", he wrote these keywords \"{{ keyword |\njoin(', ') }}\". Do you think his choice of keywords was correct? Answer {{\nanswer_choices[1]}} or {{ answer_choices[0]}}.\nTarget Template:\n{{ answer_choices[label] }}\nAnswer Choices Template:\nno ||| yes\nInput Template:\nDo these keywords \"{{ keyword | join(', ') }}\" represent key concepts in the\nabstract \"{{ abst }}\"?\nTarget Template:\n{{ answer_choices[label] }}\nAnswer Choices Template:\nno ||| yes\nNote: the prompt does not correspond to the original task intended by the dataset authors.\nInput Template:\nGiven the abstract {{abst}}, list out {{ keyword | length }} keywords for it.\nTarget Template:\n{% if label == 1 %}\n{{ keyword | join(', ') }}\n{% endif %}\nInput Template:\n一位学者使用“{{ keyword | join(', ') }}”作为搜索词。你认为搜索引擎会返回摘要\n“{{abst}}”吗？回答{{ answer_choices[1] }} 或{{ answer_choices[0] }}。\nTarget Template:\n\n{{ answer_choices[label] }}\nAnswer Choices Template:\n不||| 是的\nInput Template:\n给定抽象{{abst}}，列出{{ keyword | length }} 关键字。\nTarget Template:\n{% if label == 1 %}\n{{ keyword | join(', ') }}\n{% endif %}\nInput Template:\n写一篇关于“{{ keyword | join(', ') }}”的摘要：\nTarget Template:\n{% if label == 1 %} {{abst}} {% endif %}\nAnswer Choices Template:\n不||| 是的\nInput Template:\n在约翰写完摘要“{{abst}}”之后，他写了这些关键字“{{ keyword | join(', ') }}”。你认为\n他选择的关键词是正确的吗？回答{{ answer_choices[1]}} 或{{ answer_choices[0]}}。\nTarget Template:\n{{ answer_choices[label] }}\nAnswer Choices Template:\n不||| 是的\nInput Template:\n\n这些关键字“{{ keyword | join(', ') }}”是否代表抽象“{{ abst }}”中的关键概念？\nTarget Template:\n{{ answer_choices[label] }}\nAnswer Choices Template:\n不||| 是的\nInput Template:\nA scholar used \"{{ keyword | join(', ') }}\" as search terms. Do you think the\nsearch engine would return the abstract \"{{abst}}\"? Answer {{ answer_choices[1] }}\nor {{ answer_choices[0] }}.\nTarget Template:\n{{ answer_choices[label] }}\nAnswer Choices Template:\nno ||| yes\nInput Template:\nWrite an abstract about \"{{ keyword | join(', ') }}\":\nTarget Template:\n{% if label == 1 %} {{abst}} {% endif %}\nAnswer Choices Template:\nno ||| yes\n1.8.2\nclue tnews\nData Example\nPrompts\nInput Template:\n\nKey\nValue\nsentence\n买套房不香吗？为什么会有人愿花600万买部手机？\nlabel\n-1\nidx\n1\n将标题“{{ sentence }}”分为以下主题：\n- {{ answer_choices | join('\\n- ') }}\n主题：\nTarget Template:\n{{ answer_choices[label] }}\nAnswer Choices Template:\n故事||| 文化||| 娱乐||| 运动的||| 金融||| 房地产||| 车||| 教育||| 技术|||\n军队||| 旅行||| 世界新闻||| 股票||| 农业||| 游戏\nInput Template:\nClassify the title \"{{ sentence }}\" into the following topics:\n- {{ answer_choices | join('\\n- ') }}\nTopic:\nTarget Template:\n{{ answer_choices[label] }}\nAnswer Choices Template:\nstory ||| culture ||| entertainment ||| sports ||| finance ||| real estate ||| car\n||| education ||| tech ||| military ||| travel ||| world news ||| stock |||\nagriculture ||| game\nInput Template:\nGiven the topics of {{answer_choices[:-1] | join(', ') }}, and {{\nanswer_choices[-1] }}, specify which of them best represents the following\nsentence:\n{{ sentence }}\nBest:\nTarget Template:\n{{ answer_choices[label] }}\n\nAnswer Choices Template:\nstory ||| culture ||| entertainment ||| sports ||| finance ||| real estate ||| car\n||| education ||| tech ||| military ||| travel ||| world news ||| stock |||\nagriculture ||| game\nInput Template:\n以下新闻标题“{{ sentence }}”属于什么主题？{{ answer_choices[0] | capitalize }},\n{{ answer_choices[1:-1] | join(', ') }} 还是{{ answer_choices[-1] }}？\nTarget Template:\n{{ answer_choices[label] }}\nAnswer Choices Template:\n故事||| 文化||| 娱乐||| 运动的||| 金融||| 房地产||| 车||| 教育||| 技术|||\n军队||| 旅行||| 世界新闻||| 股票||| 农业||| 游戏\nInput Template:\n鉴于{{answer_choices[:-1] | join(', ') }} 和{{ answer_choices[-1] }}，指定它们中\n的哪一个最能代表以下句子：\n{{ sentence }}\n最佳：\nTarget Template:\n{{ answer_choices[label] }}\nAnswer Choices Template:\n故事||| 文化||| 娱乐||| 运动的||| 金融||| 房地产||| 车||| 教育||| 技术|||\n军队||| 旅行||| 世界新闻||| 股票||| 农业||| 游戏\nInput Template:\nWhat topic does the following news title \"{{ sentence }}\" belong to? {{\nanswer_choices[0] | capitalize }}, {{ answer_choices[1:-1] | join(', ') }}, or {{\nanswer_choices[-1] }}?\nTarget Template:\n{{ answer_choices[label] }}\n\nAnswer Choices Template:\nstory ||| culture ||| entertainment ||| sports ||| finance ||| real estate ||| car\n||| education ||| tech ||| military ||| travel ||| world news ||| stock |||\nagriculture ||| game\n1.9\nCode Misc.\n1.9.1\ncodeparrot/codecomplex codeparrot–codecomplex\nData Example\nKey\nValue\nsrc\nimport java.util.Scanner;\npublic ...\ncomplexity\nlinear\nproblem\n1197_B. Pillars\nfrom\nCODEFORCES\nPrompts\nInput Template:\n{{ code }} What is the time complexity of the previous code?\nTarget Template:\n{{ complexity }}\nNote: the prompt does not correspond to the original task intended by the dataset authors.\nInput Template:\nIdentify the time complexity of the following code as constant, linear, quadratic,\ncubic, log(n), nlog(n) or NP-hard. {{ code }} Complexity:\nTarget Template:\n{{ complexity }}\nNote: the prompt does not correspond to the original task intended by the dataset authors.\nInput Template:\n{{ code }} Which one is the correct time complexity of the code snippet: constant,\nlinear, quadratic, cubic, log(n), nlog(n) or NP-hard?\n\nTarget Template:\n{{ complexity }}\n1.9.2\ngreat_code\nDataset from Hellendoorn et al. (2020). Used in training.\nData Example\nKey\nValue\nid\n1\nsource_tokens\n#NEWLINE#;def test_get_params(;self...\nhas_bug\nTrue\nerror_location\n76\nrepair_candidates\n2;76;4;11;18;22;30;40;48;58;66;80;8...\nbug_kind\n1\nbug_kind_name\nVARIABLE_MISUSE\nrepair_targets\n4;11;18;22;30;40;48;58;66;80;88;103...\nedges\n[{'before_index': 1, 'after_index':...\nprovenances\n{'datasetProvenance': {'datasetName...\nPrompts\nNote: the prompt does not correspond to the original task intended by the dataset authors.\nInput Template:\n{% set mask = 'def <FUNC_NAME> (' %}\n{% set indent = '\n' %}\n{% set ns = namespace(indent_size=0, result=[], masked=false, target='') %}\n{% for token in source_tokens\n%}\n{% if ns.masked is false and token.startswith('def') %}\n{% set ns.target = token.split('def ')[1][:-1] %}\n{% set token = mask %}\n{% set ns.masked = true %}\n{% endif%}\n{% if token== '#INDENT#' %}\n{% set ns.indent_size = ns.indent_size + 1 %}\n{% set ns.result = ns.result + [indent * ns.indent_size] %}\n{% elif token == '#NEWLINE#' %}\n{% set ns.result = ns.result + [\"\\n\"] %}\n{% elif token == '#UNINDENT#' %}\n{% set ns.indent_size = ns.indent_size - 1 %}\n{% else %}\n{% if not loop.first and loop.previtem == '#NEWLINE#' %}\n{% set ns.result = ns.result + [indent * ns.indent_size] %}\n{% endif %}\n{% set ns.result = ns.result + [token | replace('\\\\n', '\\n'), \" \"] %}\n{% endif %}\n{% endfor %}\n{{ns.result | join(\"\") | replace(\" . \", \".\") | replace(\" , \", \", \")\n| replace(\"(\n\", \"(\") | replace(\" )\", \")\") | replace(\"[ \", \"[\") | replace(\" ]\", \"]\")}}\nWhat is the function name?\n\nTarget Template:\n{{ ns.target }}\nNote: the prompt does not correspond to the original task intended by the dataset authors.\nInput Template:\n{% set result = \"\" %}\n{% set indent = '\n' %}\n{% set ns = namespace(indent_size=0, line_number=0, buggy_line=0, bug_location=0,\nbug_len=0, result=[], result_lines=[]) %}\n{% set fixed_token = source_tokens[repair_targets[0]] %}\n{% set buggy_line_content = \"\" %}\n{% set fixed_buggy_line_content = \"\" %}\n{% if has_bug and (repair_targets | length > 0) %}\n{% for token in source_tokens\n%}\n{% if loop.index0 == error_location %}\n{% set ns.buggy_line = ns.line_number %}\n{% set ns.bug_location = (ns.result | join(\"\") | length) %}\n{% set ns.bug_len = (token | length) %}\n{% endif%}\n{% if token== '#INDENT#' %}\n{% set ns.indent_size = ns.indent_size + 1 %}\n{% set ns.result = ns.result + [indent * ns.indent_size] %}\n{% elif token == '#NEWLINE#' %}\n{% set ns.result_lines = ns.result_lines + [ns.result | join(\"\")] %}\n{% set ns.result = [] %}\n{% set ns.line_number = ns.line_number + 1 %}\n{% elif token == '#UNINDENT#' %}\n{% set ns.indent_size = ns.indent_size - 1 %}\n{% else %}\n{% if not loop.first and loop.previtem == '#NEWLINE#' %}\n{% set ns.result = ns.result + [indent * ns.indent_size] %}\n{% endif %}\n{% set ns.result = ns.result + [token | replace('\\\\n', '\\n'), \" \"]\n%}\n{% endif %}\n{% endfor %}\n{% set ns.result_lines = ns.result_lines + [ns.result | join(\"\")] %}\n{% set result = ns.result_lines | join(\"\\n\") %}\n{{result | replace(\" . \", \".\") | replace(\" , \", \", \")\n| replace(\"( \", \"(\") |\nreplace(\" )\", \")\") | replace(\"[ \", \"[\") | replace(\" ]\", \"]\")}}\n{% set buggy_line_content = ns.result_lines[ns.buggy_line] | trim | replace(\"\n. \", \".\") | replace(\" , \", \", \")\n| replace(\"( \", \"(\") | replace(\" )\", \")\") |\nreplace(\"[ \", \"[\") | replace(\" ]\", \"]\") %}\n{% set fixed_buggy_line_content =\n(ns.result_lines[ns.buggy_line][:ns.bug_location] + fixed_token +\nns.result_lines[ns.buggy_line][ns.bug_location + ns.bug_len:]) | trim | replace(\"\n. \", \".\") | replace(\" , \", \", \")\n| replace(\"( \", \"(\") | replace(\" )\", \")\") |\nreplace(\"[ \", \"[\") | replace(\" ]\", \"]\")%}\nFix the buggy line: {{buggy_line_content}}\nTarget Template:\n\n{{fixed_buggy_line_content}}\n{% endif %}\nInput Template:\n{% set mask = '<MASK>' %}\n{% set indent = '\n' %}\n{% set ns = namespace(indent_size=0, result=[]) %}\n{% if has_bug %}\n{% for token in source_tokens\n%}\n{% if loop.index0 == error_location %}\n{% set token = mask %}\n{% endif%}\n{% if token== '#INDENT#' %}\n{% set ns.indent_size = ns.indent_size + 1 %}\n{% set ns.result = ns.result + [indent * ns.indent_size] %}\n{% elif token == '#NEWLINE#' %}\n{% set ns.result = ns.result + [\"\\n\"] %}\n{% elif token == '#UNINDENT#' %}\n{% set ns.indent_size = ns.indent_size - 1 %}\n{% else %}\n{% if not loop.first and loop.previtem == '#NEWLINE#' %}\n{% set ns.result = ns.result + [indent * ns.indent_size] %}\n{% endif %}\n{% set ns.result = ns.result + [token | replace('\\\\n', '\\n'), \" \"]\n%}\n{% endif %}\n{% endfor %}\n{{ns.result | join(\"\") | replace(\" . \", \".\") | replace(\" , \", \", \")\n|\nreplace(\"( \", \"(\") | replace(\" )\", \")\") | replace(\"[ \", \"[\") | replace(\" ]\",\n\"]\")}}\nGiven the code above, what is a proper replacement for {{mask}}?\nTarget Template:\n{{source_tokens[repair_targets[0]]}}\n{% endif %}\nNote: the prompt does not correspond to the original task intended by the dataset authors.\nInput Template:\n{% set indent = '\n' %}\n{% set ns = namespace(indent_size=0, result=[]) %}\n{% for token in source_tokens\n%}\n{% if token== '#INDENT#' %}\n{% set ns.indent_size = ns.indent_size + 1 %}\n{% set ns.result = ns.result + [indent * ns.indent_size] %}\n{% elif token == '#NEWLINE#' %}\n{% set ns.result = ns.result + [\"\\n\"] %}\n{% elif token == '#UNINDENT#' %}\n{% set ns.indent_size = ns.indent_size - 1 %}\n{% else %}\n{% if not loop.first and loop.previtem == '#NEWLINE#' %}\n{% set ns.result = ns.result + [indent * ns.indent_size] %}\n\n{% endif %}\n{% set ns.result = ns.result + [token | replace('\\\\n', '\\n'), \" \"] %}\n{% endif %}\n{% endfor %}\n{{ns.result | join(\"\") | replace(\" . \", \".\") | replace(\" , \", \", \")\n| replace(\"(\n\", \"(\") | replace(\" )\", \")\") | replace(\"[ \", \"[\") | replace(\" ]\", \"]\")}}\nIs there a bug in the code above?\nTarget Template:\n{{ {True: \"Yes\", False: \"No\"}[has_bug] }}\nAnswer Choices Template:\nYes ||| No\nNote: the prompt does not correspond to the original task intended by the dataset authors.\nInput Template:\n{% set mask = '<MASK>' %}\n{% set indent = '\n' %}\n{% set ns = namespace(indent_size=0, result=[]) %}\n{% if has_bug %}\n{% for token in source_tokens\n%}\n{% if loop.index0 == error_location %}\n{% set token = mask %}\n{% endif%}\n{% if token== '#INDENT#' %}\n{% set ns.indent_size = ns.indent_size + 1 %}\n{% set ns.result = ns.result + [indent * ns.indent_size] %}\n{% elif token == '#NEWLINE#' %}\n{% set ns.result = ns.result + [\"\\n\"] %}\n{% elif token == '#UNINDENT#' %}\n{% set ns.indent_size = ns.indent_size - 1 %}\n{% else %}\n{% if not loop.first and loop.previtem == '#NEWLINE#' %}\n{% set ns.result = ns.result + [indent * ns.indent_size] %}\n{% endif %}\n{% set ns.result = ns.result + [token | replace('\\\\n', '\\n'), \" \"]\n%}\n{% endif %}\n{% endfor %}\n{{ns.result | join(\"\") | replace(\" . \", \".\") | replace(\" , \", \", \")\n|\nreplace(\"( \", \"(\") | replace(\" )\", \")\") | replace(\"[ \", \"[\") | replace(\" ]\",\n\"]\")}}\nGiven the code above, what is a proper replacement for {{mask}}? Choose among:\n{{answer_choices | join(\", \")}}\nTarget Template:\n{{source_tokens[repair_targets[0]]}}\n{% endif %}\n\nAnswer Choices Template:\n{% if has_bug %}\n{% set nss = namespace(choices=[]) %}\n{% for i in\nrepair_candidates\n%}\n{% set nss.choices = nss.choices + [source_tokens[(i\n| int)]] %}\n{% endfor %}\n{{nss.choices | unique | join(\" ||| \")}} {% endif\n%}\n1.9.3\nteven/code_docstring_corpus top_level\nData Example\nKey\nValue\ndesc\n'XXX22: This has to be present'\ndecl\ndef XXX11():\nbodies\npass\nPrompts\nInput Template:\nComplete the below\n{{decl}}\n'''{{desc | replace('\n', '\n')}}'''\nTarget Template:\n{{bodies}}\nInput Template:\nI wrote the below code\n{{bodies}}\nWhat's a good function header?\nTarget Template:\n{{decl}}\nInput Template:\n{{decl}}\n\nTarget Template:\n\"\"\"{{desc | replace('\n', '\n') | replace(\"'\", '')}}\"\"\"\n{{bodies}}\n1.10\nWord Sense Disambiguation\n1.10.1\npasinit/xlwic xlwic_en_zh\nDataset from Raganato et al. (2020). Used in training.\nData Example\nKey\nValue\nid\nEN_1\ncontext_1\nWe like to summer in the Mediterran...\ncontext_2\nWe summered in Kashmir.\ntarget_word\nsummer\npos\nV\ntarget_word_location_1\n{'char_start': 11, 'char_end': 17}\ntarget_word_location_2\n{'char_start': 3, 'char_end': 11}\nlanguage\nEN\nlabel\n1\nPrompts\nInput Template:\n第1 句：{{context_1}}\n句子2：{{context_2}}\n确定单词“{{target_word}}”在两个句子中的使用是否相同。是还是不是？\nTarget Template:\n{% if label != -1%}\n{{answer_choices[label]}}\n{% endif %}\nAnswer Choices Template:\n不||| 是的\nInput Template:\n\n家庭作业\n判断“{{target_word}}”这个词在以下两个句子中的含义是否相同。回答是或否。\n{{context_1}}\n{{context_2}}\nTarget Template:\n{% if label != -1%}\n{{answer_choices[label]}}\n{% endif %}\nAnswer Choices Template:\n不||| 是的\nInput Template:\n{{context_1}}\n{{context_2}}\n{{target_word}} 的类似意义？\nTarget Template:\n{% if label != -1%}\n{{answer_choices[label]}}\n{% endif %}\nAnswer Choices Template:\n不||| 是的\nInput Template:\n“{{target_word}}”这个词有多种含义。第1 句和第2 句的意思相同吗？是还是不是？\n第1 句：{{context_1}}\n句子2：{{context_2}}\nTarget Template:\n{% if label != -1%}\n{{answer_choices[label]}}\n{% endif %}\nAnswer Choices Template:\n\n不||| 是的\nInput Template:\n确定以下两个句子中是否以相同的方式使用了单词“{{target_word}}”。\n{{context_1}}\n{{context_2}}\nTarget Template:\n{% if label != -1%}\n{{answer_choices[label]}}\n{% endif %}\nAnswer Choices Template:\n不||| 是的\nInput Template:\n{{context_1}}\n{{context_2}}\n问题：“{{target_word}}”这个词在上面两个句子中的含义是否相同？\nTarget Template:\n{% if label != -1%}\n{{answer_choices[label]}}\n{% endif %}\nAnswer Choices Template:\n不||| 是的\nInput Template:\n“{{target_word}}”这个词在这两个句子中是否具有相同的含义？是的，不是吗？\n{{context_1}}\n{{context_2}}\nTarget Template:\n{% if label != -1%}\n{{answer_choices[label]}}\n{% endif %}\nAnswer Choices Template:\n\n不||| 是的\nInput Template:\n“{{target_word}}”这个词在这两个句子中是否具有相同的含义？\n{{context_1}}\n{{context_2}}\nTarget Template:\n{% if label != -1%}\n{{answer_choices[label]}}\n{% endif %}\nAnswer Choices Template:\n不||| 是的\nInput Template:\n句子A：{{context_1}}\n句子B：{{context_2}}\n“{{target_word}}”在句子A 和B 中具有相似的含义。对还是错？\nTarget Template:\n{% if label != -1%}\n{{answer_choices[label]}}\n{% endif %}\nAnswer Choices Template:\n错误的||| 真的\nInput Template:\n{{context_1}}\n{{context_2}}\n问题：“{{target_word}}”这个词在上面两个句子中的含义是否相同？是的，不是吗？\nTarget Template:\n{% if label != -1%}\n{{answer_choices[label]}}\n{% endif %}\n\nAnswer Choices Template:\n不||| 是的\n1.10.2\npasinit/xlwic xlwic_fr_fr\nData Example\nKey\nValue\nid\nFR_1\ncontext_1\nL’éclaircie est généralement une co...\ncontext_2\nAméliorations utiles.\ntarget_word\namélioration\npos\nN\ntarget_word_location_1\n{'char_start': 41, 'char_end': 53}\ntarget_word_location_2\n{'char_start': 0, 'char_end': 13}\nlanguage\nFR\nlabel\n1\nPrompts\nInput Template:\n{{context_1}}\n{{context_2}}\nSens similaire de {{target_word}} ?\nTarget Template:\n{% if label != -1%}\n{{answer_choices[label]}}\n{% endif %}\nAnswer Choices Template:\nNon ||| Oui\nInput Template:\nDevoirs\nDécidez si le mot \"{{target_word}}\" est utilisé avec le même sens dans les deux\nphrases suivantes. Répondez par oui ou non.\n{{context_1}}\n{{context_2}}\nTarget Template:\n\n{% if label != -1%}\n{{answer_choices[label]}}\n{% endif %}\nAnswer Choices Template:\nNon ||| Oui\nInput Template:\nLe mot \"{{target_word}}\" a-t-il le même sens dans ces deux phrases ? Oui Non?\n{{context_1}}\n{{context_2}}\nTarget Template:\n{% if label != -1%}\n{{answer_choices[label]}}\n{% endif %}\nAnswer Choices Template:\nNon ||| Oui\nInput Template:\n{{context_1}}\n{{context_2}}\nQuestion : Le mot '{{target_word}}' est-il utilisé dans le même sens dans les deux\nphrases ci-dessus ? Oui Non?\nTarget Template:\n{% if label != -1%}\n{{answer_choices[label]}}\n{% endif %}\nAnswer Choices Template:\nNon ||| Oui\nInput Template:\nLe mot \"{{target_word}}\" a plusieurs significations. A-t-il le même sens dans les\nphrases 1 et 2 ? Oui ou non?\nPhrase 1 : {{context_1}}\nPhrase 2 : {{context_2}}\n\nTarget Template:\n{% if label != -1%}\n{{answer_choices[label]}}\n{% endif %}\nAnswer Choices Template:\nNon ||| Oui\nInput Template:\nPhrase 1 : {{context_1}}\nPhrase 2 : {{context_2}}\nDéterminez si le mot \"{{target_word}}\" est utilisé dans le même sens dans les deux\nphrases. Oui ou non?\nTarget Template:\n{% if label != -1%}\n{{answer_choices[label]}}\n{% endif %}\nAnswer Choices Template:\nNon ||| Oui\nInput Template:\nLe mot \"{{target_word}}\" a-t-il le même sens dans ces deux phrases ?\n{{context_1}}\n{{context_2}}\nTarget Template:\n{% if label != -1%}\n{{answer_choices[label]}}\n{% endif %}\nAnswer Choices Template:\nNon ||| Oui\nInput Template:\n\nPhrase A : {{context_1}}\nPhrase B : {{context_2}}\n\"{{target_word}}\" a une signification similaire dans les phrases A et B. Vrai ou\nfaux ?\nTarget Template:\n{% if label != -1%}\n{{answer_choices[label]}}\n{% endif %}\nAnswer Choices Template:\nFaux ||| Vrai\nInput Template:\nDéterminez si le mot '{{target_word}}' est utilisé de la même manière dans les\ndeux phrases ci-dessous.\n{{context_1}}\n{{context_2}}\nTarget Template:\n{% if label != -1%}\n{{answer_choices[label]}}\n{% endif %}\nAnswer Choices Template:\nNon ||| Oui\nInput Template:\n{{context_1}}\n{{context_2}}\nQuestion : Le mot '{{target_word}}' est-il utilisé dans le même sens dans les deux\nphrases ci-dessus ?\nTarget Template:\n{% if label != -1%}\n{{answer_choices[label]}}\n{% endif %}\nAnswer Choices Template:\n\nNon ||| Oui\n1.11\nParaphrase Identification\n1.11.1\npaws-x en\nDataset from Yang et al. (2019). Used in training.\nData Example\nKey\nValue\nid\n2\nsentence1\nThe NBA season of 1975 -- 76 was th...\nsentence2\nThe 1975 -- 76 season of the Nation...\nlabel\n1\nPrompts\nNotes: Generalized prompt format, task_description-input.\nInput Template:\nDetermine if the following two sentences paraphrase each other or not.\nSent 1: {{sentence1}}\nSent 2: {{sentence2}}\nTarget Template:\n{{answer_choices[label]}}\nAnswer Choices Template:\nNo ||| Yes\nNotes: Natural question.\nInput Template:\nSentence 1: {{sentence1}}\nSentence 2: {{sentence2}}\nQuestion: Do Sentence 1 and Sentence 2 express the same meaning? Yes or No?\nTarget Template:\n{{answer_choices[label]}}\nAnswer Choices Template:\n\nNo ||| Yes\nNotes: Generalized prompt format, context-question without any label.\nInput Template:\n{{sentence1}}\nIs that a paraphrase of the following sentence?\n{{sentence2}}?\nTarget Template:\n{{answer_choices[label]}}\nAnswer Choices Template:\nNo ||| Yes\nNotes: Natural Question without label.\nInput Template:\nSentence 1: {{sentence1}}\nSentence 2: {{sentence2}}\nQuestion: Can we rewrite Sentence 1 to Sentence 2?\nTarget Template:\n{{answer_choices[label]}}\nAnswer Choices Template:\nNo ||| Yes\nNotes: Generalized prompt format, context-question.\nInput Template:\n{{sentence1}}\nIs that a paraphrase of the following sentence?\n{{sentence2}}?\nYes or No.\nTarget Template:\n{{answer_choices[label]}}\nAnswer Choices Template:\n\nNo ||| Yes\nNotes: Concatenation of sentence 1 and sentence 2.\nInput Template:\nSentence 1: {{sentence1}}\nSentence 2: {{sentence2}}\nQuestion: Does Sentence 1 paraphrase Sentence 2? Yes or No?\nTarget Template:\n{{answer_choices[label]}}\nAnswer Choices Template:\nNo ||| Yes\nNote: the prompt does not correspond to the original task intended by the dataset authors.\nNotes: Create a generative paraphrase task.\nInput Template:\n{% if label == 1 %}\nParaphrase the sentence: {{sentence1}}\nTarget Template:\n{{sentence2}}\n{% endif %}\nNotes: Concatenation of sentence 1 and sentence 2 without any label.\nInput Template:\nSentence 1: {{sentence1}}\nSentence 2: {{sentence2}}\nQuestion: Does Sentence 1 paraphrase Sentence 2?\nTarget Template:\n{{answer_choices[label]}}\nAnswer Choices Template:\nNo ||| Yes\n\nNotes: Natural question without label.\nInput Template:\nSentence 1: {{sentence1}}\nSentence 2: {{sentence2}}\nQuestion: Do Sentence 1 and Sentence 2 express the same meaning?\nTarget Template:\n{{answer_choices[label]}}\nAnswer Choices Template:\nNo ||| Yes\nPrompt from Brown et al. (2020) Notes: ANLI prompt format from Table G7 in the GPT3\npaper Brown et al. (2020)\nInput Template:\n{{sentence1}} Question: {{sentence2}} True or False?\nTarget Template:\n{{answer_choices[label]}}\nAnswer Choices Template:\nFalse ||| True\nNotes: Natural Question.\nInput Template:\nSentence 1: {{sentence1}}\nSentence 2: {{sentence2}}\nQuestion: Can we rewrite Sentence 1 to Sentence 2? Yes or No?\nTarget Template:\n{{answer_choices[label]}}\nAnswer Choices Template:\nNo ||| Yes\n\nPrompt from Brown et al. (2020) Notes: ANLI prompt format from Table G7 in the GPT3\npaper Brown et al. (2020). Additionally added task information without any label.\nInput Template:\n{{sentence1}} Question: {{sentence2}} Paraphrase or not?\nTarget Template:\n{{answer_choices[label]}}\nAnswer Choices Template:\nNo ||| Yes\n1.11.2\npaws-x es\nData Example\nKey\nValue\nid\n2\nsentence1\nLa temporada de la NBA de 1975: 76 ...\nsentence2\nLa temporada 1975 - 76 de la Asocia...\nlabel\n1\nPrompts\nInput Template:\nOración 1: {{sentence1}}\nOración 2: {{sentence2}}\nPregunta: ¿La oración 1 parafrasea la oración 2? ¿Si o no?\nTarget Template:\n{{answer_choices[label]}}\nAnswer Choices Template:\nNo ||| Sí\nInput Template:\n{{sentence1}} Pregunta: {{sentence2}} ¿Parafrasear o no?\n\nTarget Template:\n{{answer_choices[label]}}\nAnswer Choices Template:\nNo ||| Sí\nInput Template:\n{{sentence1}}\n¿Es una paráfrasis de la siguiente oración?\n{{sentence2}}?\nSi o no.\nTarget Template:\n{{answer_choices[label]}}\nAnswer Choices Template:\nNo ||| Sí\nInput Template:\nOración 1: {{sentence1}}\nOración 2: {{sentence2}}\nPregunta: ¿La Oración 1 y la Oración 2 expresan el mismo significado?\nTarget Template:\n{{answer_choices[label]}}\nAnswer Choices Template:\nNo ||| Sí\nInput Template:\n{% if label == 1 %}\nParafrasea la oración: {{sentence1}}\nTarget Template:\n\n{{sentence2}}\n{% endif %}\nInput Template:\n{{sentence1}} Pregunta: {{sentence2}} ¿Verdadero o falso?\nTarget Template:\n{{answer_choices[label]}}\nAnswer Choices Template:\nFalso ||| Verdadero\nInput Template:\nOración 1: {{sentence1}}\nOración 2: {{sentence2}}\nPregunta: ¿La oración 1 parafrasea la oración 2?\nTarget Template:\n{{answer_choices[label]}}\nAnswer Choices Template:\nNo ||| Sí\nInput Template:\nDetermina si las siguientes dos oraciones se parafrasean entre sí o no.\nEnviado 1: {{sentence1}}\nEnviado 2: {{sentence2}}\nTarget Template:\n{{answer_choices[label]}}\nAnswer Choices Template:\nNo ||| Sí\nInput Template:\n\nOración 1: {{sentence1}}\nOración 2: {{sentence2}}\nPregunta: ¿La Oración 1 y la Oración 2 expresan el mismo significado? ¿Si o no?\nTarget Template:\n{{answer_choices[label]}}\nAnswer Choices Template:\nNo ||| Sí\nInput Template:\nOración 1: {{sentence1}}\nOración 2: {{sentence2}}\nPregunta: ¿Podemos reescribir la Oración 1 a la Oración 2? ¿Si o no?\nTarget Template:\n{{answer_choices[label]}}\nAnswer Choices Template:\nNo ||| Sí\nInput Template:\n{{sentence1}}\n¿Es una paráfrasis de la siguiente oración?\n{{sentence2}}?\nTarget Template:\n{{answer_choices[label]}}\nAnswer Choices Template:\nNo ||| Sí\nInput Template:\nOración 1: {{sentence1}}\nOración 2: {{sentence2}}\nPregunta: ¿Podemos reescribir la Oración 1 a la Oración 2?\n\nTarget Template:\n{{answer_choices[label]}}\nAnswer Choices Template:\nNo ||| Sí\n1.12\nSentence Completion\n1.12.1\nxcopa vi\nDataset from Ponti et al. (2020). Used in evaluation.\nData Example\nKey\nValue\npremise\nCô gái tìm thấy con bọtrong ngũ cố...\nchoice1\nCô đổsữa vào bát.\nchoice2\nCô mất cảm giác ngon miệng.\nquestion\neffect\nlabel\n1\nidx\n1\nchanged\nFalse\nPrompts\nInput Template:\n{{ premise }}\nTôi đang lưỡng lựgiữa hai lựa chọn. Giúp tôi chọn nguyên nhân {% if question ==\n\"cause\" %} có khảnăng xảy ra cao hơn: {% else %} effect: {% endif %}\n- {{choice1}}\n- {{choice2}}\nTarget Template:\n{% if label != -1 %} {{ answer_choices[label] }} {% endif %}\nAnswer Choices Template:\n{{choice1}} ||| {{choice2}}\nInput Template:\n\n{{ premise }}\nLựa chọn tốt nhất là gì?\n- {{choice1}}\n- {{choice2}}\nChúng tôi đang tìm kiếm {% if question == \"cause\" %} một nguyên nhân {% else %}\nmột ảnh hưởng {% endif %}\nTarget Template:\n{% if label != -1 %} {{answer_choices[label]}} {% endif %}\nAnswer Choices Template:\n{{choice1}} ||| {{choice2}}\nInput Template:\n{{ premise }} {% if question == \"cause\" %} Điều này xảy ra vì ... {% else %} Do đó\n... {% endif %}\nGiúp tôi chọn tùy chọn hợp lý hơn:\n- {{choice1}}\n- {{choice2}}\nTarget Template:\n{% if label != -1 %} {{ answer_choices[label] }} {% endif %}\nAnswer Choices Template:\n{{choice1}} ||| {{choice2}}\nInput Template:\n\"{{ answer_choices[0] }}\" hay \"{{ answer_choices[1] }}\"? {{ premise }} {% if\nquestion == \"cause\" %} bởi vì {% else %} nên {% endif %}\nTarget Template:\n{% if label != -1 %} {{ answer_choices[label] }} {% endif %}\nAnswer Choices Template:\n{{choice1 }} ||| {{choice2}}\nInput Template:\n\n{{ premise }}\nChọn nguyên nhân {% if question == \"cause\" %} hợp lý nhất: {% else %} effect: {%\nendif %}\n- {{choice1}}\n- {{choice2}}\nTarget Template:\n{% if label != -1 %} {{ answer_choices[label] }} {% endif %}\nAnswer Choices Template:\n{{choice1}} ||| {{choice2}}\n1.12.2\nxcopa zh\nData Example\nKey\nValue\npremise\n这个女孩在麦片粥中发现了一个虫子。\nchoice1\n她向碗里倒了牛奶。\nchoice2\n她没了食欲。\nquestion\neffect\nlabel\n1\nidx\n1\nchanged\nFalse\nPrompts\nInput Template:\n{{ premise }} {% if question == \"cause\" %} 这是因为... {% else %} 结果... {% endif\n%}\n帮助我选择更合理的选项：\n- {{choice1}}\n- {{choice2}}\nTarget Template:\n{% if label != -1 %}{{ answer_choices[label] }}{%endif%}\nAnswer Choices Template:\n{{choice1}} ||| {{choice2}}\nInput Template:\n\n{{ premise }}\n选择最合理的{% if question == \"cause\" %} 原因：{% else %} 效果：{% endif %}\n- {{choice1}}\n- {{choice2}}\nTarget Template:\n{% if label != -1 %}{{ answer_choices[label] }}{%endif%}\nAnswer Choices Template:\n{{choice1}} ||| {{choice2}}\nInput Template:\n“{{ answer_choices[0] }}”还是“{{ answer_choices[1] }}”？{{ premise }} {% if\nquestion == \"cause\" %} 因为{% else %} 所以{% endif %}\nTarget Template:\n{% if label != -1 %}{{ answer_choices[label] }}{% endif %}\nAnswer Choices Template:\n{{choice1 }} ||| {{choice2}}\nInput Template:\n{{ premise }}\n最好的选择是什么？\n- {{choice1}}\n- {{choice2}}\n我们正在寻找{% if question == \"cause\" %} 一个原因{% else %} 一个结果{% endif %}\nTarget Template:\n{% if label != -1 %}{{answer_choices[label]}}{%endif%}\nAnswer Choices Template:\n{{choice1}} ||| {{choice2}}\nInput Template:\n\n{{ premise }}\n我在两个选项之间犹豫不决。帮我选择更有可能的{% if question == \"cause\" %} 原因：{%\nelse %} 效果：{% endif %}\n- {{choice1}}\n- {{choice2}}\nTarget Template:\n{% if label != -1 %}{{ answer_choices[label] }}{%endif%}\nAnswer Choices Template:\n{{choice1}} ||| {{choice2}}\nInput Template:\n{{ premise }}\n我正在考虑两个选项。请帮我最有可能的{% if question == \"cause\" %}导因：{% else %}后果：\n{% endif %}\n- {{choice1}}\n- {{choice2}}\nTarget Template:\n{% if label != -1 %}{{ answer_choices[label] }}{%endif%}\nAnswer Choices Template:\n{{choice1}} ||| {{choice2}}\nInput Template:\n{{ premise }} {% if question == \"cause\" %}这个会发生是因为... {% else %}结果是...\n{% endif %}\n帮我挑选合适的选项：\n- {{choice1}}\n- {{choice2}}\nTarget Template:\n{% if label != -1 %}{{ answer_choices[label] }}{%endif%}\nAnswer Choices Template:\n\n{{choice1}} ||| {{choice2}}\nNotes: Adapted from Perez et al. (2021) and Schick and Schütze (2020).\nInput Template:\n\"{{ answer_choices[0] }}\" 还是\"{{ answer_choices[1] }}\"? {{ premise }} {% if\nquestion == \"cause\" %}因为{% else %}所以{% endif %}\nTarget Template:\n{% if label != -1 %}{{ answer_choices[label] }}{% endif %}\nAnswer Choices Template:\n{{choice1}} ||| {{choice2}}\nInput Template:\n{{ premise }}\n哪个是最好的答案?\n- {{choice1}}\n- {{choice2}}\n我们正在考虑{% if question == \"cause\" %}起因{% else %}后果{% endif %}\nTarget Template:\n{% if label != -1 %}{{answer_choices[label]}}{%endif%}\nAnswer Choices Template:\n{{choice1}} ||| {{choice2}}\nInput Template:\n{{ premise }}\n请选择最贴切的答案：{% if question == \"cause\" %}导因:{% else %}结果: {% endif %}\n- {{choice1}}\n- {{choice2}}\nTarget Template:\n{% if label != -1 %}{{ answer_choices[label] }}{%endif%}\nAnswer Choices Template:\n\n{{choice1}} ||| {{choice2}}\n1.13\nNatural Language Inference\n1.13.1\nxnli en\nDataset from Conneau et al. (2018). Used in evaluation.\nData Example\nKey\nValue\npremise\nyou know during the season and i gu...\nhypothesis\nYou lose the things to the followin...\nlabel\n0\nPrompts\nNotes: Sanh et al. (2022)\nInput Template:\nTake the following as truth: {{premise}}\nThen the following statement: \"{{hypothesis}}\" is {{\"true\"}}, {{\"false\"}}, or\n{{\"inconclusive\"}}?\nTarget Template:\n{{ answer_choices[label] }}\nAnswer Choices Template:\nTrue ||| Inconclusive ||| False\nNotes: Sanh et al. (2022)\nInput Template:\n{{premise}}\nQuestion: Does this imply that \"{{hypothesis}}\"? Yes, no, or maybe?\nTarget Template:\n{{answer_choices[label]}}\nAnswer Choices Template:\n\nYes ||| Maybe ||| No\nNotes: Same as reported in Figure G7 of Brown et al. (2020), except that there is no task identifying\ntokens like ”anli R1: ”.\nInput Template:\n{{premise}}\nQuestion: {{hypothesis}} True, False, or Neither?\nTarget Template:\n{{ answer_choices[label] }}\nAnswer Choices Template:\nTrue ||| Neither ||| False\nNotes: Sanh et al. (2022)\nInput Template:\nGiven that {{premise}} Does it follow that {{hypothesis}} Yes, no, or maybe?\nTarget Template:\n{{ answer_choices[label] }}\nAnswer Choices Template:\nYes ||| Maybe ||| No\nNotes: Adapted from the BoolQ prompts in Schick and Schütze (2020).\nInput Template:\n{{premise}} Based on the previous passage, is it true that \"{{hypothesis}}\"? Yes,\nno, or maybe?\nTarget Template:\n{{ answer_choices[label] }}\nAnswer Choices Template:\nYes ||| Maybe ||| No\n\nNotes: Webson and Pavlick (2021)\nInput Template:\nGiven {{premise}} Is it guaranteed true that \"{{hypothesis}}\"? Yes, no, or maybe?\nTarget Template:\n{{ answer_choices[label] }}\nAnswer Choices Template:\nYes ||| Maybe ||| No\nNotes: Webson and Pavlick (2021)\nInput Template:\nGiven {{premise}} Should we assume that \"{{hypothesis}}\" is true? Yes, no, or\nmaybe?\nTarget Template:\n{{ answer_choices[label] }}\nAnswer Choices Template:\nYes ||| Maybe ||| No\nNotes: Sanh et al. (2022)\nInput Template:\nGiven that {{premise}} Therefore, it must be true that \"{{hypothesis}}\"? Yes, no,\nor maybe?\nTarget Template:\n{{ answer_choices[label] }}\nAnswer Choices Template:\nYes ||| Maybe ||| No\nNotes: Webson and Pavlick (2021)\nInput Template:\n\nSuppose {{premise}} Can we infer that \"{{hypothesis}}\"? Yes, no, or maybe?\nTarget Template:\n{{ answer_choices[label] }}\nAnswer Choices Template:\nYes ||| Maybe ||| No\nNotes: Webson and Pavlick (2021)\nInput Template:\n{{premise}} Are we justified in saying that \"{{hypothesis}}\"? Yes, no, or maybe?\nTarget Template:\n{{ answer_choices[label] }}\nAnswer Choices Template:\nYes ||| Maybe ||| No\nNotes: Sanh et al. (2022)\nInput Template:\n{{premise}} Based on that information, is the claim: \"{{hypothesis}}\" {{\"true\"}},\n{{\"false\"}}, or {{\"inconclusive\"}}?\nTarget Template:\n{{ answer_choices[label] }}\nAnswer Choices Template:\nTrue ||| Inconclusive ||| False\nNotes: Sanh et al. (2022)\nInput Template:\n{{premise}}\nKeeping in mind the above text, consider: {{hypothesis}} Is this {{\"always\"}},\n{{\"sometimes\"}}, or {{\"never\"}} correct?\n\nTarget Template:\n{{ answer_choices[label] }}\nAnswer Choices Template:\nAlways ||| Sometimes ||| Never\nNotes: Sanh et al. (2022)\nInput Template:\nSuppose it's true that {{premise}} Then, is \"{{hypothesis}}\" {{\"always\"}},\n{{\"sometimes\"}}, or {{\"never\"}} true?\nTarget Template:\n{{ answer_choices[label] }}\nAnswer Choices Template:\nAlways ||| Sometimes ||| Never\nNotes: Sanh et al. (2022)\nInput Template:\nAssume it is true that {{premise}}\nTherefore, \"{{hypothesis}}\" is {{\"guaranteed\"}}, {{\"possible\"}}, or\n{{\"impossible\"}}?\nTarget Template:\n{{ answer_choices[label] }}\nAnswer Choices Template:\nGuaranteed ||| Possible ||| Impossible\nNotes: Adapted from Williams et al. (2018) instructions to crowdsourcing workers.\nInput Template:\n{{premise}} Using only the above description and what you know about the world,\n\"{{hypothesis}}\" is definitely correct, incorrect, or inconclusive?\nTarget Template:\n\n{{ answer_choices[label] }}\nAnswer Choices Template:\nCorrect ||| Inconclusive ||| Incorrect\n1.13.2\nxnli es\nData Example\nKey\nValue\npremise\nUsted sabe durante la temporada y s...\nhypothesis\nPierdes las cosas al siguiente nive...\nlabel\n0\nPrompts\n1.13.2.1\nHuman-translated prompts\nNotes: Same as reported in Figure G7 of Brown et al. (2020), except that there is no task identifying\ntokens like ”anli R1: ”.\nInput Template:\n{{premise}}\nPregunta: {{hypothesis}} Verdadero, Falso, o Ninguno?\nTarget Template:\n{{ answer_choices[label] }}\nAnswer Choices Template:\nVerdadero ||| Ninguno ||| Falso\nNotes: Webson and Pavlick (2021)\nInput Template:\nSupongamos {{premise}} Podemos inferir que \"{{hypothesis}}\"? Si, no, o tal vez?\nTarget Template:\n{{ answer_choices[label] }}\n\nAnswer Choices Template:\nSí ||| Tal vez ||| No\nNotes: Webson and Pavlick (2021)\nInput Template:\n{{premise}} Estamos justificados en decir que \"{{hypothesis}}\"? Si, no, o tal vez?\nTarget Template:\n{{ answer_choices[label] }}\nAnswer Choices Template:\nSí ||| Tal vez ||| No\nNotes: Sanh et al. (2022)\nInput Template:\nSupongamos que es cierto que {{premise}}\npor lo tanto, \"{{hypothesis}}\" es {{\"garantizado\"}}, {{\"posible\"}}, o\n{{\"imposible\"}}?\nTarget Template:\n{{ answer_choices[label] }}\nAnswer Choices Template:\nGarantizado ||| Posible ||| Imposible\nNotes: Adapted from Williams et al. (2018) instructions to crowdsourcing workers.\nInput Template:\n{{premise}} Usando solo la descripción anterior y lo que sabe sobre el mundo,\n\"{{hypothesis}}\" es definitivamente correcto, incorrecto o no concluyente?\nTarget Template:\n{{ answer_choices[label] }}\nAnswer Choices Template:\n\nCorrecto ||| No concluyente ||| Incorrecto\n1.13.2.2\nMachine-translated prompts\nInput Template:\n{{premise}} ¿Estamos justificados al decir que &quot;{{hypothesis}}&quot;? ¿Sí, no\no tal vez?\nTarget Template:\n{{ answer_choices[label] }}\nAnswer Choices Template:\nSí ||| Quizás ||| No\nInput Template:\n{{premise}} Pregunta: {{hypothesis}} ¿Verdadero, falso o ninguno?\nTarget Template:\n{{ answer_choices[label] }}\nAnswer Choices Template:\nVerdadero ||| Ninguno de los dos ||| Falso\nInput Template:\n{{premise}} Usando solo la descripción anterior y lo que sabe sobre el mundo,\n&quot;{{hypothesis}}&quot; es definitivamente correcta, incorrecta o no\nconcluyente.\nTarget Template:\n{{ answer_choices[label] }}\nAnswer Choices Template:\nCorrecto ||| Poco concluyente ||| Incorrecto\nInput Template:\n\nSupongamos {{premise}} ¿Podemos inferir que &quot;{{hypothesis}}&quot;? ¿Sí, no o\ntal vez?\nTarget Template:\n{{ answer_choices[label] }}\nAnswer Choices Template:\nSí ||| Quizás ||| No\nInput Template:\nSupongamos que es cierto que {{premise}} Por lo tanto, &quot;{{hypothesis}}&quot;\nes {{\"guaranteed\"}}, {{\"possible\"}} o {{\"impossible\"}}.\nTarget Template:\n{{ answer_choices[label] }}\nAnswer Choices Template:\ngarantizado ||| Posible ||| Imposible\n"
    }
  ]
}