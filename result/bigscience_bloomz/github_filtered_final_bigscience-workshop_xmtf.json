{
  "1-1 (Weights)": "No statements about the availability, location, or access method for the model weights appear in the provided quotes, so no information can be extracted on whether the bigscience-workshop/xmtf weights are public or how they may be obtained.",
  "1-1 (Weights)__evidence": [],
  "1-2 (Code)": "The quote set contains no sentences that discuss training code, data-preparation scripts, configuration files, or inference/serving code for bigscience-workshop/xmtf. Consequently, there is no evidence in the supplied material about whether any portion of the training pipeline (pre-training, fine-tuning, RL, etc.) is open-sourced or where it might be hosted.",
  "1-2 (Code)__evidence": [],
  "1-3 (License)": "The project is released under the Apache License, Version 2.0. Key excerpts from the license file explicitly grant broad rights:\n• \"Apache License Version 2.0, January 2004 http://www.apache.org/licenses/\" – identifies the license and its official URL.\n• \"Licensed under the Apache License, Version 2.0 (the \\\"License\\\"); you may not use this file except in compliance with the License.\" – establishes that all use must follow the Apache 2.0 terms.\n• \"Subject to the terms and conditions of this License, each Contributor hereby grants to You a perpetual, worldwide, non-exclusive, no-charge, royalty-free, irrevocable copyright license to reproduce, prepare Derivative Works of, publicly display, publicly perform, sublicense, and distribute the Work and such Derivative Works in Source or Object form.\" – explicitly grants the rights to use, modify, redistribute, and sublicense, including for commercial purposes, provided the Apache 2.0 conditions are met.\n• \"This License does not grant permission to use the trade names, trademarks, service marks, or product names of the Licensor,\" – clarifies that trademark usage is not covered.\nUnder Apache 2.0, users have full rights for (a) use, (b) modification, (c) redistribution, and (d) commercial use, as long as they preserve copyright notices, include a copy of the license, and provide attribution.",
  "1-3 (License)__evidence": [
    {
      "source": "[license_files]",
      "quote": "Apache License\n                           Version 2.0, January 2004\n                        http://www.apache.org/licenses/"
    },
    {
      "source": "[license_files]",
      "quote": "Licensed under the Apache License, Version 2.0 (the \"License\");\n   you may not use this file except in compliance with the License."
    },
    {
      "source": "[license_files]",
      "quote": "Subject to the terms and conditions of this License, each Contributor hereby grants to You a perpetual,\n      worldwide, non-exclusive, no-charge, royalty-free, irrevocable\n      copyright license to reproduce, prepare Derivative Works of,\n      publicly display, publicly perform, sublicense, and distribute the\n      Work and such Derivative Works in Source or Object form."
    },
    {
      "source": "[license_files]",
      "quote": "This License does not grant permission to use the trade names, trademarks, service marks, or product names of the Licensor,"
    }
  ],
  "1-4 (Paper)": "One official reference is provided: \"https://arxiv.org/abs/2211.01786)\". This URL points to an arXiv preprint that serves as the technical report or paper associated with the bigscience-workshop/xmtf model.",
  "1-4 (Paper)__evidence": [
    {
      "source": "readme",
      "quote": "https://arxiv.org/abs/2211.01786)."
    }
  ],
  "1-5 (Architecture)": "",
  "1-5 (Architecture)__evidence": [],
  "1-6 (Tokenizer)": "",
  "1-6 (Tokenizer)__evidence": [],
  "2-1 (Hardware)": "",
  "2-1 (Hardware)__evidence": [],
  "2-2 (Software)": "",
  "2-2 (Software)__evidence": [],
  "2-3 (API)": "",
  "2-3 (API)__evidence": [],
  "3-1 (Pre-training)": "",
  "3-1 (Pre-training)__evidence": [],
  "3-2 (Fine-tuning)": "",
  "3-2 (Fine-tuning)__evidence": [],
  "3-3 (Reinforcement Learning)": "",
  "3-3 (Reinforcement Learning)__evidence": [],
  "4-1 (Pre-training Data)": "No quotations were supplied that discuss the pre-training data for the bigscience-workshop/xmtf model. The excerpt set contains no statements about the kinds of corpora, their sizes, sources, licenses, or any other descriptive statistics. Therefore, no summary of pre-training data content, provenance, or permitted usage can be produced from the provided material.",
  "4-1 (Pre-training Data)__evidence": [],
  "4-2 (Fine-tuning Data)": "The quote collection contains no sentences that mention, describe, or allude to any fine-tuning datasets for bigscience-workshop/xmtf. As a result, there is no information available about the sources, composition, size, or public accessibility of fine-tuning data, nor any illustrative examples thereof.",
  "4-2 (Fine-tuning Data)__evidence": [],
  "4-3 (Reinforcement Learning Data)": "None of the supplied excerpts reference reinforcement-learning-specific data for bigscience-workshop/xmtf. Consequently, there is no basis for summarizing the composition, generation process, accessibility, or provenance of any RL datasets.",
  "4-3 (Reinforcement Learning Data)__evidence": [],
  "4-4 (Data Filtering)": "The single quotation provided—\"detect_res = cld3.get_language(sentence)\"—shows that the data-filtering pipeline for bigscience-workshop/xmtf invokes the Compact Language Detector v3 (CLD3) at the sentence level. Each sentence is passed to cld3’s get_language function, yielding a language prediction stored in detect_res. While the excerpt does not spell out subsequent decision rules, its presence indicates a filtering step in which language identification results are used to accept, reject, or route data according to linguistic criteria. No additional numeric thresholds, class probabilities, Jaccard ratios, perplexity limits, or multi-stage filtering details are disclosed in the provided material.",
  "4-4 (Data Filtering)__evidence": [
    {
      "source": "[py_files/plotstables/contamination/probe_contamination.py]",
      "quote": "detect_res = cld3.get_language(sentence)"
    }
  ],
  "__usage": {
    "fine_tuning": "unknown",
    "rl": "unknown"
  }
}