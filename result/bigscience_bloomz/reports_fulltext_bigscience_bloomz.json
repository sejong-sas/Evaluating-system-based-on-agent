{
  "model_id": "bigscience/bloomz",
  "full_texts": [
    {
      "arxiv_id": "https://arxiv.org/abs/2211.01786",
      "full_text": " [2211.01786] Crosslingual Generalization through Multitask Finetuning Skip to main content We gratefully acknowledge support from the Simons Foundation, member institutions , and all contributors. Donate &gt; cs &gt; arXiv:2211.01786 Help | Advanced Search All fields Title Author Abstract Comments Journal reference ACM classification MSC classification Report number arXiv identifier DOI ORCID arXiv author ID Help pages Full text Search open search GO open navigation menu quick links Login Help Pages About --> Computer Science > Computation and Language arXiv:2211.01786 (cs) [Submitted on 3 Nov 2022 ( v1 ), last revised 29 May 2023 (this version, v2)] Title: Crosslingual Generalization through Multitask Finetuning Authors: Niklas Muennighoff , Thomas Wang , Lintang Sutawika , Adam Roberts , Stella Biderman , Teven Le Scao , M Saiful Bari , Sheng Shen , Zheng-Xin Yong , Hailey Schoelkopf , Xiangru Tang , Dragomir Radev , Alham Fikri Aji , Khalid Almubarak , Samuel Albanie , Zaid Alyafeai , Albert Webson , Edward Raff , Colin Raffel View a PDF of the paper titled Crosslingual Generalization through Multitask Finetuning, by Niklas Muennighoff and 18 other authors View PDF Abstract: Multitask prompted finetuning (MTF) has been shown to help large language models generalize to new tasks in a zero-shot setting, but so far explorations of MTF have focused on English data and models. We apply MTF to the pretrained multilingual BLOOM and mT5 model families to produce finetuned variants called BLOOMZ and mT0. We find finetuning large multilingual language models on English tasks with English prompts allows for task generalization to non-English languages that appear only in the pretraining corpus. Finetuning on multilingual tasks with English prompts further improves performance on English and non-English tasks leading to various state-of-the-art zero-shot results. We also investigate finetuning on multilingual tasks with prompts that have been machine-translated from English to match the language of each dataset. We find training on these machine-translated prompts leads to better performance on human-written prompts in the respective languages. Surprisingly, we find models are capable of zero-shot generalization to tasks in languages they have never intentionally seen. We conjecture that the models are learning higher-level capabilities that are both task- and language-agnostic. In addition, we introduce xP3, a composite of supervised datasets in 46 languages with English and machine-translated prompts. Our code, datasets and models are freely available at this https URL . Comments: 9 main pages (119 with appendix), 16 figures and 11 tables Subjects: Computation and Language (cs.CL) ; Artificial Intelligence (cs.AI); Machine Learning (cs.LG) Cite as: arXiv:2211.01786 [cs.CL] &nbsp; (or arXiv:2211.01786v2 [cs.CL] for this version) &nbsp; https://doi.org/10.48550/arXiv.2211.01786 Focus to learn more arXiv-issued DOI via DataCite Submission history From: Niklas Muennighoff [ view email ] [v1] Thu, 3 Nov 2022 13:19:32 UTC (8,533 KB) [v2] Mon, 29 May 2023 16:40:37 UTC (8,518 KB) Full-text links: Access Paper: View a PDF of the paper titled Crosslingual Generalization through Multitask Finetuning, by Niklas Muennighoff and 18 other authors View PDF TeX Source Other Formats view license Current browse context: cs.CL &lt;&nbsp;prev &nbsp; | &nbsp; next&nbsp;&gt; new | recent | 2022-11 Change to browse by: cs cs.AI cs.LG References &amp; Citations NASA ADS Google Scholar Semantic Scholar a export BibTeX citation Loading... BibTeX formatted citation &times; loading... Data provided by: Bookmark Bibliographic Tools Bibliographic and Citation Tools Bibliographic Explorer Toggle Bibliographic Explorer ( What is the Explorer? ) Connected Papers Toggle Connected Papers ( What is Connected Papers? ) Litmaps Toggle Litmaps ( What is Litmaps? ) scite.ai Toggle scite Smart Citations ( What are Smart Citations? ) Code, Data, Media Code, Data and Media Associated with this Article alphaXiv Toggle alphaXiv ( What is alphaXiv? ) Links to Code Toggle CatalyzeX Code Finder for Papers ( What is CatalyzeX? ) DagsHub Toggle DagsHub ( What is DagsHub? ) GotitPub Toggle Gotit.pub ( What is GotitPub? ) Huggingface Toggle Hugging Face ( What is Huggingface? ) Links to Code Toggle Papers with Code ( What is Papers with Code? ) ScienceCast Toggle ScienceCast ( What is ScienceCast? ) Demos Demos Replicate Toggle Replicate ( What is Replicate? ) Spaces Toggle Hugging Face Spaces ( What is Spaces? ) Spaces Toggle TXYZ.AI ( What is TXYZ.AI? ) Related Papers Recommenders and Search Tools Link to Influence Flower Influence Flower ( What are Influence Flowers? ) Core recommender toggle CORE Recommender ( What is CORE? ) Author Venue Institution Topic About arXivLabs arXivLabs: experimental projects with community collaborators arXivLabs is a framework that allows collaborators to develop and share new arXiv features directly on our website. Both individuals and organizations that work with arXivLabs have embraced and accepted our values of openness, community, excellence, and user data privacy. arXiv is committed to these values and only works with partners that adhere to them. Have an idea for a project that will add value for arXiv's community? Learn more about arXivLabs . Which authors of this paper are endorsers? | Disable MathJax ( What is MathJax? ) About Help contact arXiv Click here to contact arXiv Contact subscribe to arXiv mailings Click here to subscribe Subscribe Copyright Privacy Policy Web Accessibility Assistance arXiv Operational Status Get status notifications via email or slack ",
      "fetch_method": "direct-html"
    },
    {
      "arxiv_id": "https://arxiv.org/abs/2211.01786",
      "full_text": " [2211.01786] Crosslingual Generalization through Multitask Finetuning Skip to main content We gratefully acknowledge support from the Simons Foundation, member institutions , and all contributors. Donate &gt; cs &gt; arXiv:2211.01786 Help | Advanced Search All fields Title Author Abstract Comments Journal reference ACM classification MSC classification Report number arXiv identifier DOI ORCID arXiv author ID Help pages Full text Search open search GO open navigation menu quick links Login Help Pages About --> Computer Science > Computation and Language arXiv:2211.01786 (cs) [Submitted on 3 Nov 2022 ( v1 ), last revised 29 May 2023 (this version, v2)] Title: Crosslingual Generalization through Multitask Finetuning Authors: Niklas Muennighoff , Thomas Wang , Lintang Sutawika , Adam Roberts , Stella Biderman , Teven Le Scao , M Saiful Bari , Sheng Shen , Zheng-Xin Yong , Hailey Schoelkopf , Xiangru Tang , Dragomir Radev , Alham Fikri Aji , Khalid Almubarak , Samuel Albanie , Zaid Alyafeai , Albert Webson , Edward Raff , Colin Raffel View a PDF of the paper titled Crosslingual Generalization through Multitask Finetuning, by Niklas Muennighoff and 18 other authors View PDF Abstract: Multitask prompted finetuning (MTF) has been shown to help large language models generalize to new tasks in a zero-shot setting, but so far explorations of MTF have focused on English data and models. We apply MTF to the pretrained multilingual BLOOM and mT5 model families to produce finetuned variants called BLOOMZ and mT0. We find finetuning large multilingual language models on English tasks with English prompts allows for task generalization to non-English languages that appear only in the pretraining corpus. Finetuning on multilingual tasks with English prompts further improves performance on English and non-English tasks leading to various state-of-the-art zero-shot results. We also investigate finetuning on multilingual tasks with prompts that have been machine-translated from English to match the language of each dataset. We find training on these machine-translated prompts leads to better performance on human-written prompts in the respective languages. Surprisingly, we find models are capable of zero-shot generalization to tasks in languages they have never intentionally seen. We conjecture that the models are learning higher-level capabilities that are both task- and language-agnostic. In addition, we introduce xP3, a composite of supervised datasets in 46 languages with English and machine-translated prompts. Our code, datasets and models are freely available at this https URL . Comments: 9 main pages (119 with appendix), 16 figures and 11 tables Subjects: Computation and Language (cs.CL) ; Artificial Intelligence (cs.AI); Machine Learning (cs.LG) Cite as: arXiv:2211.01786 [cs.CL] &nbsp; (or arXiv:2211.01786v2 [cs.CL] for this version) &nbsp; https://doi.org/10.48550/arXiv.2211.01786 Focus to learn more arXiv-issued DOI via DataCite Submission history From: Niklas Muennighoff [ view email ] [v1] Thu, 3 Nov 2022 13:19:32 UTC (8,533 KB) [v2] Mon, 29 May 2023 16:40:37 UTC (8,518 KB) Full-text links: Access Paper: View a PDF of the paper titled Crosslingual Generalization through Multitask Finetuning, by Niklas Muennighoff and 18 other authors View PDF TeX Source Other Formats view license Current browse context: cs.CL &lt;&nbsp;prev &nbsp; | &nbsp; next&nbsp;&gt; new | recent | 2022-11 Change to browse by: cs cs.AI cs.LG References &amp; Citations NASA ADS Google Scholar Semantic Scholar a export BibTeX citation Loading... BibTeX formatted citation &times; loading... Data provided by: Bookmark Bibliographic Tools Bibliographic and Citation Tools Bibliographic Explorer Toggle Bibliographic Explorer ( What is the Explorer? ) Connected Papers Toggle Connected Papers ( What is Connected Papers? ) Litmaps Toggle Litmaps ( What is Litmaps? ) scite.ai Toggle scite Smart Citations ( What are Smart Citations? ) Code, Data, Media Code, Data and Media Associated with this Article alphaXiv Toggle alphaXiv ( What is alphaXiv? ) Links to Code Toggle CatalyzeX Code Finder for Papers ( What is CatalyzeX? ) DagsHub Toggle DagsHub ( What is DagsHub? ) GotitPub Toggle Gotit.pub ( What is GotitPub? ) Huggingface Toggle Hugging Face ( What is Huggingface? ) Links to Code Toggle Papers with Code ( What is Papers with Code? ) ScienceCast Toggle ScienceCast ( What is ScienceCast? ) Demos Demos Replicate Toggle Replicate ( What is Replicate? ) Spaces Toggle Hugging Face Spaces ( What is Spaces? ) Spaces Toggle TXYZ.AI ( What is TXYZ.AI? ) Related Papers Recommenders and Search Tools Link to Influence Flower Influence Flower ( What are Influence Flowers? ) Core recommender toggle CORE Recommender ( What is CORE? ) Author Venue Institution Topic About arXivLabs arXivLabs: experimental projects with community collaborators arXivLabs is a framework that allows collaborators to develop and share new arXiv features directly on our website. Both individuals and organizations that work with arXivLabs have embraced and accepted our values of openness, community, excellence, and user data privacy. arXiv is committed to these values and only works with partners that adhere to them. Have an idea for a project that will add value for arXiv's community? Learn more about arXivLabs . Which authors of this paper are endorsers? | Disable MathJax ( What is MathJax? ) About Help contact arXiv Click here to contact arXiv Contact subscribe to arXiv mailings Click here to subscribe Subscribe Copyright Privacy Policy Web Accessibility Assistance arXiv Operational Status Get status notifications via email or slack ",
      "fetch_method": "direct-html"
    },
    {
      "arxiv_id": "https://huggingface.co/blog/bloom-megatron-deepspeed",
      "full_text": " The Technology Behind BLOOM Training Hugging Face Models Datasets Spaces Community Docs Enterprise Pricing Log In Sign Up Back to Articles The Technology Behind BLOOM Training Published July 14, 2022 Update on GitHub Upvote 43 +37 Stas Bekman stas Follow People Overview Megatron-DeepSpeed Data Parallelism ZeRO Data Parallelism Tensor Parallelism Pipeline Parallelism DP+PP DP+PP+TP ZeRO DP+PP+TP BF16Optimizer Fused CUDA Kernels Datasets Embedding LayerNorm Positional Encoding Training Difficulties Conclusion Resources Important links Papers and Articles Blog credits In recent years, training ever larger language models has become the norm. While the issues of those models&#39; not being released for further study is frequently discussed, the hidden knowledge about how to train such models rarely gets any attention. This article aims to change this by shedding some light on the technology and engineering behind training such models both in terms of hardware and software on the example of the 176B parameter language model BLOOM . But first we would like to thank the companies and key people and groups that made the amazing feat of training a 176 Billion parameter model by a small group of dedicated people possible. Then the hardware setup and main technological components will be discussed. Here&#39;s a quick summary of project: Hardware 384 80GB A100 GPUs Software Megatron-DeepSpeed Architecture GPT3 w/ extras Dataset 350B tokens of 59 Languages Training time 3.5 months People The project was conceived by Thomas Wolf (co-founder and CSO - Hugging Face), who dared to compete with the huge corporations not only to train one of the largest multilingual models, but also to make the final result accessible to all people, thus making what was but a dream to most people a reality. This article focuses specifically on the engineering side of the training of the model. The most important part of the technology behind BLOOM were the people and companies who shared their expertise and helped us with coding and training. There are 6 main groups of people to thank: The HuggingFace&#39;s BigScience team who dedicated more than half a dozen full time employees to figure out and run the training from inception to the finishing line and provided and paid for all the infrastructure beyond the Jean Zay&#39;s compute. The Microsoft DeepSpeed team, who developed DeepSpeed and later integrated it with Megatron-LM, and whose developers spent many weeks working on the needs of the project and provided lots of awesome practical experiential advice before and during the training. The NVIDIA Megatron-LM team, who developed Megatron-LM and who were super helpful answering our numerous questions and providing first class experiential advice. The IDRIS / GENCI team managing the Jean Zay supercomputer, who donated to the project an insane amount of compute and great system administration support. The PyTorch team who created a super powerful framework, on which the rest of the software was based, and who were very supportive to us during the preparation for the training, fixing multiple bugs and improving the usability of the PyTorch components we relied on during the training. The volunteers in the BigScience Engineering workgroup It&#39;d be very difficult to name all the amazing people who contributed to the engineering side of the project, so I will just name a few key people outside of Hugging Face who were the engineering foundation of this project for the last 14 months: Olatunji Ruwase, Deepak Narayanan, Jeff Rasley, Jared Casper, Samyam Rajbhandari and Rémi Lacroix Also we are grateful to all the companies who allowed their employees to contribute to this project. Overview BLOOM&#39;s architecture is very similar to GPT3 with a few added improvements as will be discussed later in this article. The model was trained on Jean Zay , the French government-funded super computer that is managed by GENCI and installed at IDRIS , the national computing center for the French National Center for Scientific Research (CNRS). The compute was generously donated to the project by GENCI (grant 2021-A0101012475). The following hardware was used during the training: GPUs: 384 NVIDIA A100 80GB GPUs (48 nodes) + 32 spare gpus 8 GPUs per node Using NVLink 4 inter-gpu connects, 4 OmniPath links CPU: AMD EPYC 7543 32-Core Processor CPU memory: 512GB per node GPU memory: 640GB per node Inter-node connect: Omni-Path Architecture (OPA) w/ non-blocking fat tree NCCL-communications network: a fully dedicated subnet Disc IO network: GPFS shared with other nodes and users Checkpoints: main checkpoints each checkpoint with fp32 optim states and bf16+fp32 weights is 2.3TB - just the bf16 weights are 329GB. Datasets: 46 Languages in 1.5TB of deduplicated massively cleaned up text, converted into 350B unique tokens Vocabulary size of the model is 250,680 tokens For full details please see The BigScience Corpus A 1.6TB Composite Multilingual Dataset The training of the 176B BLOOM model occurred over Mar-Jul 2022 and took about 3.5 months to complete (approximately 1M compute hours). Megatron-DeepSpeed The 176B BLOOM model has been trained using Megatron-DeepSpeed , which is a combination of 2 main technologies: DeepSpeed is a deep learning optimization library that makes distributed training easy, efficient, and effective. Megatron-LM is a large, powerful transformer model framework developed by the Applied Deep Learning Research team at NVIDIA. The DeepSpeed team developed a 3D parallelism based implementation by combining ZeRO sharding and pipeline parallelism from the DeepSpeed library with Tensor Parallelism from Megatron-LM. More details about each component can be seen in the table below. Please note that the BigScience&#39;s Megatron-DeepSpeed is a fork of the original Megatron-DeepSpeed repository, to which we added multiple additions. Here is a table of which components were provided by which framework to train BLOOM: Component DeepSpeed Megatron-LM ZeRO Data Parallelism V Tensor Parallelism V Pipeline Parallelism V BF16Optimizer V Fused CUDA Kernels V DataLoader V Please note that both Megatron-LM and DeepSpeed have Pipeline Parallelism and BF16 Optimizer implementations, but we used the ones from DeepSpeed as they are integrated with ZeRO. Megatron-DeepSpeed implements 3D Parallelism to allow huge models to train in a very efficient way. Let’s briefly discuss the 3D components. DataParallel (DP) - the same setup is replicated multiple times, and each being fed a slice of the data. The processing is done in parallel and all setups are synchronized at the end of each training step. TensorParallel (TP) - each tensor is split up into multiple chunks, so instead of having the whole tensor reside on a single GPU, each shard of the tensor resides on its designated GPU. During processing each shard gets processed separately and in parallel on different GPUs and the results are synced at the end of the step. This is what one may call horizontal parallelism, as the splitting happens on a horizontal level. PipelineParallel (PP) - the model is split up vertically (layer-level) across multiple GPUs, so that only one or several layers of the model are placed on a single GPU. Each GPU processes in parallel different stages of the pipeline and works on a small chunk of the batch. Zero Redundancy Optimizer (ZeRO) - also performs sharding of the tensors somewhat similar to TP, except the whole tensor gets reconstructed in time for a forward or backward computation, therefore the model doesn&#39;t need to be modified. It also supports various offloading techniques to compensate for limited GPU memory. Data Parallelism Most users with just a few GPUs are likely to be familiar with DistributedDataParallel (DDP) PyTorch documentation . In this method the model is fully replicated to each GPU and then after each iteration all the models synchronize their states with each other. This approach allows training speed up but throwing more resources at the problem, but it only works if the model can fit onto a single GPU. ZeRO Data Parallelism ZeRO-powered data parallelism (ZeRO-DP) is described on the following diagram from this blog post It can be difficult to wrap one&#39;s head around it, but in reality, the concept is quite simple. This is just the usual DDP, except, instead of replicating the full model params, gradients and optimizer states, each GPU stores only a slice of it. And then at run-time when the full layer params are needed just for the given layer, all GPUs synchronize to give each other parts that they miss - this is it. This component is implemented by DeepSpeed. Tensor Parallelism In Tensor Parallelism (TP) each GPU processes only a slice of a tensor and only aggregates the full tensor for operations that require the whole thing. In this section we use concepts and diagrams from the Megatron-LM paper: Efficient Large-Scale Language Model Training on GPU Clusters . The main building block of any transformer is a fully connected nn.Linear followed by a nonlinear activation GeLU . Following the Megatron paper&#39;s notation, we can write the dot-product part of it as Y = GeLU(XA) , where X and Y are the input and output vectors, and A is the weight matrix. If we look at the computation in matrix form, it&#39;s easy to see how the matrix multiplication can be split between multiple GPUs: If we split the weight matrix A column-wise across N GPUs and perform matrix multiplications XA_1 through XA_n in parallel, then we will end up with N output vectors Y_1, Y_2, ..., Y_n which can be fed into GeLU independently: . Notice with the Y matrix split along the columns, we can split the second GEMM along its rows so that it takes the output of the GeLU directly without any extra communication. Using this principle, we can update an MLP of arbitrary depth, while synchronizing the GPUs after each row-column sequence. The Megatron-LM paper authors provide a helpful illustration for that: Here f is an identity operator in the forward pass and all reduce in the backward pass while g is an all reduce in the forward pass and identity in the backward pass. Parallelizing the multi-headed attention layers is even simpler, since they are already inherently parallel, due to having multiple independent heads! Special considerations: Due to the two all reduces per layer in both the forward and backward passes, TP requires a very fast interconnect between devices. Therefore it&#39;s not advisable to do TP across more than one node, unless you have a very fast network. In our case the inter-node was much slower than PCIe. Practically, if a node has 4 GPUs, the highest TP degree is therefore 4. If you need a TP degree of 8, you need to use nodes that have at least 8 GPUs. This component is implemented by Megatron-LM. Megatron-LM has recently expanded tensor parallelism to include sequence parallelism that splits the operations that cannot be split as above, such as LayerNorm, along the sequence dimension. The paper Reducing Activation Recomputation in Large Transformer Models provides details for this technique. Sequence parallelism was developed after BLOOM was trained so not used in the BLOOM training. Pipeline Parallelism Naive Pipeline Parallelism (naive PP) is where one spreads groups of model layers across multiple GPUs and simply moves data along from GPU to GPU as if it were one large composite GPU. The mechanism is relatively simple - switch the desired layers .to() the desired devices and now whenever the data goes in and out those layers switch the data to the same device as the layer and leave the rest unmodified. This performs a vertical model parallelism, because if you remember how most models are drawn, we slice the layers vertically. For example, if the following diagram shows an 8-layer model: =================== =================== | 0 | 1 | 2 | 3 | | 4 | 5 | 6 | 7 | =================== =================== GPU0 GPU1 we just sliced it in 2 vertically, placing layers 0-3 onto GPU0 and 4-7 to GPU1. Now while data travels from layer 0 to 1, 1 to 2 and 2 to 3 this is just like the forward pass of a normal model on a single GPU. But when data needs to pass from layer 3 to layer 4 it needs to travel from GPU0 to GPU1 which introduces a communication overhead. If the participating GPUs are on the same compute node (e.g. same physical machine) this copying is pretty fast, but if the GPUs are located on different compute nodes (e.g. multiple machines) the communication overhead could be significantly larger. Then layers 4 to 5 to 6 to 7 are as a normal model would have and when the 7th layer completes we often need to send the data back to layer 0 where the labels are (or alternatively send the labels to the last layer). Now the loss can be computed and the optimizer can do its work. Problems: the main deficiency and why this one is called &quot;naive&quot; PP, is that all but one GPU is idle at any given moment. So if 4 GPUs are used, it&#39;s almost identical to quadrupling the amount of memory of a single GPU, and ignoring the rest of the hardware. Plus there is the overhead of copying the data between devices. So 4x 6GB cards will be able to accommodate the same size as 1x 24GB card using naive PP, except the latter will complete the training faster, since it doesn&#39;t have the data copying overhead. But, say, if you have 40GB cards and need to fit a 45GB model you can with 4x 40GB cards (but barely because of the gradient and optimizer states). shared embeddings may need to get copied back and forth between GPUs. Pipeline Parallelism (PP) is almost identical to a naive PP described above, but it solves the GPU idling problem, by chunking the incoming batch into micro-batches and artificially creating a pipeline, which allows different GPUs to concurrently participate in the computation process. The following illustration from the GPipe paper shows the naive PP on the top, and PP on the bottom: It&#39;s easy to see from the bottom diagram how PP has fewer dead zones, where GPUs are idle. The idle parts are referred to as the &quot;bubble&quot;. Both parts of the diagram show parallelism that is of degree 4. That is 4 GPUs are participating in the pipeline. So there is the forward path of 4 pipe stages F0, F1, F2 and F3 and then the return reverse order backward path of B3, B2, B1 and B0. PP introduces a new hyper-parameter to tune that is called chunks . It defines how many chunks of data are sent in a sequence through the same pipe stage. For example, in the bottom diagram, you can see that chunks=4 . GPU0 performs the same forward path on chunk 0, 1, 2 and 3 (F0,0, F0,1, F0,2, F0,3) and then it waits for other GPUs to do their work and only when their work is starting to be complete, does GPU0 start to work again doing the backward path for chunks 3, 2, 1 and 0 (B0,3, B0,2, B0,1, B0,0). Note that conceptually this is the same concept as gradient accumulation steps (GAS). PyTorch uses chunks , whereas DeepSpeed refers to the same hyper-parameter as GAS. Because of the chunks, PP introduces the concept of micro-batches (MBS). DP splits the global data batch size into mini-batches, so if you have a DP degree of 4, a global batch size of 1024 gets split up into 4 mini-batches of 256 each (1024/4). And if the number of chunks (or GAS) is 32 we end up with a micro-batch size of 8 (256/32). Each Pipeline stage works with a single micro-batch at a time. To calculate the global batch size of the DP + PP setup we then do: mbs*chunks*dp_degree ( 8*32*4=1024 ). Let&#39;s go back to the diagram. With chunks=1 you end up with the naive PP, which is very inefficient. With a very large chunks value you end up with tiny micro-batch sizes which could be not very efficient either. So one has to experiment to find the value that leads to the highest efficient utilization of the GPUs. While the diagram shows that there is a bubble of &quot;dead&quot; time that can&#39;t be parallelized because the last forward stage has to wait for backward to complete the pipeline, the purpose of finding the best value for chunks is to enable a high concurrent GPU utilization across all participating GPUs which translates to minimizing the size of the bubble. This scheduling mechanism is known as all forward all backward . Some other alternatives are one forward one backward and interleaved one forward one backward . While both Megatron-LM and DeepSpeed have their own implementation of the PP protocol, Megatron-DeepSpeed uses the DeepSpeed implementation as it&#39;s integrated with other aspects of DeepSpeed. One other important issue here is the size of the word embedding matrix. While normally a word embedding matrix consumes less memory than the transformer block, in our case with a huge 250k vocabulary, the embedding layer needed 7.2GB in bf16 weights and the transformer block is just 4.9GB. Therefore, we had to instruct Megatron-Deepspeed to consider the embedding layer as a transformer block. So we had a pipeline of 72 layers, 2 of which were dedicated to the embedding (first and last). This allowed to balance out the GPU memory consumption. If we didn&#39;t do it, we would have had the first and the last stages consume most of the GPU memory, and 95% of GPUs would be using much less memory and thus the training would be far from being efficient. DP+PP The following diagram from the DeepSpeed pipeline tutorial demonstrates how one combines DP with PP. Here it&#39;s important to see how DP rank 0 doesn&#39;t see GPU2 and DP rank 1 doesn&#39;t see GPU3. To DP there are just GPUs 0 and 1 where it feeds data as if there were just 2 GPUs. GPU0 &quot;secretly&quot; offloads some of its load to GPU2 using PP. And GPU1 does the same by enlisting GPU3 to its aid. Since each dimension requires at least 2 GPUs, here you&#39;d need at least 4 GPUs. DP+PP+TP To get an even more efficient training PP is combined with TP and DP which is called 3D parallelism. This can be seen in the following diagram. This diagram is from a blog post 3D parallelism: Scaling to trillion-parameter models , which is a good read as well. Since each dimension requires at least 2 GPUs, here you&#39;d need at least 8 GPUs for full 3D parallelism. ZeRO DP+PP+TP One of the main features of DeepSpeed is ZeRO, which is a super-scalable extension of DP. It has already been discussed in ZeRO Data Parallelism . Normally it&#39;s a standalone feature that doesn&#39;t require PP or TP. But it can be combined with PP and TP. When ZeRO-DP is combined with PP (and optionally TP) it typically enables only ZeRO stage 1, which shards only optimizer states. ZeRO stage 2 additionally shards gradients, and stage 3 also shards the model weights. While it&#39;s theoretically possible to use ZeRO stage 2 with Pipeline Parallelism, it will have bad performance impacts. There would need to be an additional reduce-scatter collective for every micro-batch to aggregate the gradients before sharding, which adds a potentially significant communication overhead. By nature of Pipeline Parallelism, small micro-batches are used and instead the focus is on trying to balance arithmetic intensity (micro-batch size) with minimizing the Pipeline bubble (number of micro-batches). Therefore those communication costs are going to hurt. In addition, there are already fewer layers than normal due to PP and so the memory savings won&#39;t be huge. PP already reduces gradient size by 1/PP , and so gradient sharding savings on top of that are less significant than pure DP. ZeRO stage 3 can also be used to train models at this scale, however, it requires more communication than the DeepSpeed 3D parallel implementation. After careful evaluation in our environment which happened a year ago we found Megatron-DeepSpeed 3D parallelism performed best. Since then ZeRO stage 3 performance has dramatically improved and if we were to evaluate it today perhaps we would have chosen stage 3 instead. BF16Optimizer Training huge LLM models in FP16 is a no-no. We have proved it to ourselves by spending several months training a 104B model which as you can tell from the tensorboard was but a complete failure. We learned a lot of things while fighting the ever diverging lm-loss: and we also got the same advice from the Megatron-LM and DeepSpeed teams after they trained the 530B model . The recent release of OPT-175B too reported that they had a very difficult time training in FP16. So back in January as we knew we would be training on A100s which support the BF16 format Olatunji Ruwase developed a BF16Optimizer which we used to train BLOOM. If you are not familiar with this data format, please have a look at the bits layout . The key to BF16 format is that it has the same exponent as FP32 and thus doesn&#39;t suffer from overflow FP16 suffers from a lot! With FP16, which has a max numerical range of 64k, you can only multiply small numbers. e.g. you can do 250*250=62500 , but if you were to try 255*255=65025 you got yourself an overflow, which is what causes the main problems during training. This means your weights have to remain tiny. A technique called loss scaling can help with this problem, but the limited range of FP16 is still an issue when models become very large. BF16 has no such problem, you can easily do 10_000*10_000=100_000_000 and it&#39;s no problem. Of course, since BF16 and FP16 have the same size of 2 bytes, one doesn&#39;t get a free lunch and one pays with really bad precision when using BF16. However, if you remember the training using stochastic gradient descent and its variations is a sort of stumbling walk, so if you don&#39;t get the perfect direction immediately it&#39;s no problem, you will correct yourself in the next steps. Regardless of whether one uses BF16 or FP16 there is also a copy of weights which is always in FP32 - this is what gets updated by the optimizer. So the 16-bit formats are only used for the computation, the optimizer updates the FP32 weights with full precision and then casts them into the 16-bit format for the next iteration. All PyTorch components have been updated to ensure that they perform any accumulation in FP32, so no loss happening there. One crucial issue is gradient accumulation, and it&#39;s one of the main features of pipeline parallelism as the gradients from each microbatch processing get accumulated. It&#39;s crucial to implement gradient accumulation in FP32 to keep the training precise, and this is what BF16Optimizer does. Besides other improvements we believe that using BF16 mixed precision training turned a potential nightmare into a relatively smooth process which can be observed from the following lm loss graph: Fused CUDA Kernels The GPU performs two things. It can copy data to/from memory and perform computations on that data. While the GPU is busy copying the GPU&#39;s computations units idle. If we want to efficiently utilize the GPU we want to minimize the idle time. A kernel is a set of instructions that implements a specific PyTorch operation. For example, when you call torch.add , it goes through a PyTorch dispatcher which looks at the input tensor(s) and various other things and decides which code it should run, and then runs it. A CUDA kernel is a specific implementation that uses the CUDA API library and can only run on NVIDIA GPUs. Now, when instructing the GPU to compute c = torch.add(a, b); e = torch.max([c,d]) , a naive approach, and what PyTorch will do unless instructed otherwise, is to launch two separate kernels, one to perform the addition of a and b and another to find the maximum value between c and d . In this case, the GPU fetches from its memory a and b , performs the addition, and then copies the result back into the memory. It then fetches c and d and performs the max operation and again copies the result back into the memory. If we were to fuse these two operations, i.e. put them into a single &quot;fused kernel&quot;, and just launch that one kernel we won&#39;t copy the intermediary result c to the memory, but leave it in the GPU registers and only need to fetch d to complete the last computation. This saves a lot of overhead and prevents GPU idling and makes the whole operation much more efficient. Fused kernels are just that. Primarily they replace multiple discrete computations and data movements to/from memory into fused computations that have very few memory movements. Additionally, some fused kernels rewrite the math so that certain groups of computations can be performed faster. To train BLOOM fast and efficiently it was necessary to use several custom fused CUDA kernels provided by Megatron-LM. In particular there is an optimized kernel to perform LayerNorm as well as kernels to fuse various combinations of the scaling, masking, and softmax operations. The addition of a bias term is also fused with the GeLU operation using PyTorch&#39;s JIT functionality. These operations are all memory bound, so it is important to fuse them to maximize the amount of computation done once a value has been retrieved from memory. So, for example, adding the bias term while already doing the memory bound GeLU operation adds no additional time. These kernels are all available in the Megatron-LM repository . Datasets Another important feature from Megatron-LM is the efficient data loader. During start up of the initial training each data set is split into samples of the requested sequence length (2048 for BLOOM) and index is created to number each sample. Based on the training parameters the number of epochs for a dataset is calculated and an ordering for that many epochs is created and then shuffled. For example, if a dataset has 10 samples and should be gone through twice, the system first lays out the samples indices in order [0, ..., 9, 0, ..., 9] and then shuffles that order to create the final global order for the dataset. Notice that this means that training will not simply go through the entire dataset and then repeat, it is possible to see the same sample twice before seeing another sample at all, but at the end of training the model will have seen each sample twice. This helps ensure a smooth training curve through the entire training process. These indices, including the offsets into the base dataset of each sample, are saved to a file to avoid recomputing them each time a training process is started. Several of these datasets can then be blended with varying weights into the final data seen by the training process. Embedding LayerNorm While we were fighting with trying to stop 104B from diverging we discovered that adding an additional LayerNorm right after the first word embedding made the training much more stable. This insight came from experimenting with bitsandbytes which contains a StableEmbedding which is a normal Embedding with layernorm and it uses a uniform xavier initialization. Positional Encoding We also replaced the usual positional embedding with an AliBi - based on the paper: Train Short, Test Long: Attention with Linear Biases Enables Input Length Extrapolation , which allows to extrapolate for longer input sequences than the ones the model was trained on. So even though we train on sequences with length 2048 the model can also deal with much longer sequences during inference. Training Difficulties With the architecture, hardware and software in place we were able to start training in early March 2022. However, it was not just smooth sailing from there. In this section we discuss some of the main hurdles we encountered. There were a lot of issues to figure out before the training started. In particular we found several issues that manifested themselves only once we started training on 48 nodes, and won&#39;t appear at small scale. E.g., CUDA_LAUNCH_BLOCKING=1 was needed to prevent the framework from hanging, and we needed to split the optimizer groups into smaller groups, otherwise the framework would again hang. You can read about those in detail in the training prequel chronicles . The main type of issue encountered during training were hardware failures. As this was a new cluster with about 400 GPUs, on average we were getting 1-2 GPU failures a week. We were saving a checkpoint every 3h (100 iterations) so on average we would lose 1.5h of training on hardware crash. The Jean Zay sysadmins would then replace the faulty GPUs and bring the node back up. Meanwhile we had backup nodes to use instead. We have run into a variety of other problems that led to 5-10h downtime several times, some related to a deadlock bug in PyTorch, others due to running out of disk space. If you are curious about specific details please see training chronicles . We were planning for all these downtimes when deciding on the feasibility of training this model - we chose the size of the model to match that feasibility and the amount of data we wanted the model to consume. With all the downtimes we managed to finish the training in our estimated time. As mentioned earlier it took about 1M compute hours to complete. One other issue was that SLURM wasn&#39;t designed to be used by a team of people. A SLURM job is owned by a single user and if they aren&#39;t around, the other members of the group can&#39;t do anything to the running job. We developed a kill-switch workaround that allowed other users in the group to kill the current process without requiring the user who started the process to be present. This worked well in 90% of the issues. If SLURM designers read this - please add a concept of Unix groups, so that a SLURM job can be owned by a group. As the training was happening 24/7 we needed someone to be on call - but since we had people both in Europe and West Coast Canada overall there was no need for someone to carry a pager, we would just overlap nicely. Of course, someone had to watch the training on the weekends as well. We automated most things, including recovery from hardware crashes, but sometimes a human intervention was needed as well. Conclusion The most difficult and intense part of the training was the 2 months leading to the start of the training. We were under a lot of pressure to start training ASAP, since the resources allocation was limited in time and we didn&#39;t have access to A100s until the very last moment. So it was a very difficult time, considering that the BF16Optimizer was written in the last moment and we needed to debug it and fix various bugs. And as explained in the previous section we discovered new problems that manifested themselves only once we started training on 48 nodes, and won&#39;t appear at small scale. But once we sorted those out, the training itself was surprisingly smooth and without major problems. Most of the time we had one person monitoring the training and only a few times several people were involved to troubleshoot. We enjoyed great support from Jean Zay&#39;s administration who quickly addressed most needs that emerged during the training. Overall it was a super-intense but very rewarding experience. Training large language models is still a challenging task, but we hope by building and sharing this technology in the open others can build on top of our experience. Resources Important links main training document tensorboard training slurm script training chronicles Papers and Articles We couldn&#39;t have possibly explained everything in detail in this article, so if the technology presented here piqued your curiosity and you&#39;d like to know more here are the papers to read: Megatron-LM: Efficient Large-Scale Language Model Training on GPU Clusters . Reducing Activation Recomputation in Large Transformer Models DeepSpeed: ZeRO: Memory Optimizations Toward Training Trillion Parameter Models ZeRO-Offload: Democratizing Billion-Scale Model Training ZeRO-Infinity: Breaking the GPU Memory Wall for Extreme Scale Deep Learning DeepSpeed: Extreme-scale model training for everyone Joint Megatron-LM and Deepspeeed: Using DeepSpeed and Megatron to Train Megatron-Turing NLG 530B, A Large-Scale Generative Language Model . ALiBi: Train Short, Test Long: Attention with Linear Biases Enables Input Length Extrapolation What Language Model to Train if You Have One Million GPU Hours? - there you will find the experiments that lead to us choosing ALiBi. BitsNBytes: 8-bit Optimizers via Block-wise Quantization (in the context of Embedding LayerNorm but the rest of the paper and the technology is amazing - the only reason were weren&#39;t using the 8-bit optimizer is because we were already saving the optimizer memory with DeepSpeed-ZeRO). Blog credits Huge thanks to the following kind folks who asked good questions and helped improve the readability of the article (listed in alphabetical order): Britney Muller, Douwe Kiela, Jared Casper, Jeff Rasley, Julien Launay, Leandro von Werra, Omar Sanseviero, Stefan Schweter and Thomas Wang. The main graphics was created by Chunte Lee. More Articles from our Blog mmBERT: ModernBERT goes Multilingual By orionweller September 9, 2025 • 79 Seq vs Seq: the Ettin Suite of Paired Encoders and Decoders By orionweller July 16, 2025 • 67 Community Edit Preview Upload images, audio, and videos by dragging in the text input, pasting, or clicking here . Tap or paste here to upload images Comment · Sign up or log in to comment Upvote 43 +31 System theme Company TOS Privacy About Jobs Website Models Datasets Spaces Pricing Docs ",
      "fetch_method": "direct-html"
    },
    {
      "arxiv_id": "https://github.com/google-research/t5x/blob/main/docs/usage/finetune.md",
      "full_text": " t5x/docs/usage/finetune.md at main · google-research/t5x · GitHub Skip to content Navigation Menu Toggle navigation Sign in Appearance settings Platform GitHub Copilot Write better code with AI GitHub Spark New Build and deploy intelligent apps GitHub Models New Manage and compare prompts GitHub Advanced Security Find and fix vulnerabilities Actions Automate any workflow Codespaces Instant dev environments Issues Plan and track work Code Review Manage code changes Discussions Collaborate outside of code Code Search Find more, search less Explore Why GitHub Documentation GitHub Skills Blog Integrations GitHub Marketplace View all features Solutions By company size Enterprises Small and medium teams Startups Nonprofits By use case DevSecOps DevOps CI/CD View all use cases By industry Healthcare Financial services Manufacturing Government View all industries View all solutions Resources Topics AI DevOps Security Software Development View all Explore Learning Pathways Events &amp; Webinars Ebooks &amp; Whitepapers Customer Stories Partners Executive Insights Open Source GitHub Sponsors Fund open source developers The ReadME Project GitHub community articles Repositories Topics Trending Collections Enterprise Enterprise platform AI-powered developer platform Available add-ons GitHub Advanced Security Enterprise-grade security features Copilot for business Enterprise-grade AI features Premium Support Enterprise-grade 24/7 support Pricing Search or jump to... Search code, repositories, users, issues, pull requests... --> Search Clear Search syntax tips Provide feedback --> We read every piece of feedback, and take your input very seriously. Include my email address so I can be contacted Cancel Submit feedback Saved searches Use saved searches to filter your results more quickly --> Name Query To see all available qualifiers, see our documentation . Cancel Create saved search Sign in Sign up Appearance settings Resetting focus You signed in with another tab or window. Reload to refresh your session. You signed out in another tab or window. Reload to refresh your session. You switched accounts on another tab or window. Reload to refresh your session. Dismiss alert {{ message }} google-research / t5x Public Notifications You must be signed in to change notification settings Fork 328 Star 2.9k Code Issues 88 Pull requests 96 Discussions Actions Security Uh oh! There was an error while loading. Please reload this page . Insights Additional navigation options Code Issues Pull requests Discussions Actions Security Insights Footer &copy; 2025 GitHub,&nbsp;Inc. Footer navigation Terms Privacy Security Status Docs Contact Manage cookies Do not share my personal information You can’t perform that action at this time. ",
      "fetch_method": "direct-html"
    },
    {
      "arxiv_id": "https://arxiv.org/abs/2203.17189",
      "full_text": " [2203.17189] Scaling Up Models and Data with $\\texttt{t5x}$ and $\\texttt{seqio}$ Skip to main content We gratefully acknowledge support from the Simons Foundation, member institutions , and all contributors. Donate &gt; cs &gt; arXiv:2203.17189 Help | Advanced Search All fields Title Author Abstract Comments Journal reference ACM classification MSC classification Report number arXiv identifier DOI ORCID arXiv author ID Help pages Full text Search open search GO open navigation menu quick links Login Help Pages About --> Computer Science > Machine Learning arXiv:2203.17189 (cs) [Submitted on 31 Mar 2022] Title: Scaling Up Models and Data with $\\texttt{t5x}$ and $\\texttt{seqio}$ Authors: Adam Roberts , Hyung Won Chung , Anselm Levskaya , Gaurav Mishra , James Bradbury , Daniel Andor , Sharan Narang , Brian Lester , Colin Gaffney , Afroz Mohiuddin , Curtis Hawthorne , Aitor Lewkowycz , Alex Salcianu , Marc van Zee , Jacob Austin , Sebastian Goodman , Livio Baldini Soares , Haitang Hu , Sasha Tsvyashchenko , Aakanksha Chowdhery , Jasmijn Bastings , Jannis Bulian , Xavier Garcia , Jianmo Ni , Andrew Chen , Kathleen Kenealy , Jonathan H. Clark , Stephan Lee , Dan Garrette , James Lee-Thorp , Colin Raffel , Noam Shazeer , Marvin Ritter , Maarten Bosma , Alexandre Passos , Jeremy Maitin-Shepard , Noah Fiedel , Mark Omernick , Brennan Saeta , Ryan Sepassi , Alexander Spiridonov , Joshua Newlan , Andrea Gesmundo View a PDF of the paper titled Scaling Up Models and Data with $\\texttt{t5x}$ and $\\texttt{seqio}$, by Adam Roberts and 42 other authors View PDF Abstract: Recent neural network-based language models have benefited greatly from scaling up the size of training datasets and the number of parameters in the models themselves. Scaling can be complicated due to various factors including the need to distribute computation on supercomputer clusters (e.g., TPUs), prevent bottlenecks when infeeding data, and ensure reproducible results. In this work, we present two software libraries that ease these issues: $\\texttt{t5x}$ simplifies the process of building and training large language models at scale while maintaining ease of use, and $\\texttt{seqio}$ provides a task-based API for simple creation of fast and reproducible training data and evaluation pipelines. These open-source libraries have been used to train models with hundreds of billions of parameters on datasets with multiple terabytes of training data. Along with the libraries, we release configurations and instructions for T5-like encoder-decoder models as well as GPT-like decoder-only architectures. $\\texttt{t5x}$ and $\\texttt{seqio}$ are open source and available at this https URL and this https URL , respectively. Subjects: Machine Learning (cs.LG) ; Computation and Language (cs.CL) Cite as: arXiv:2203.17189 [cs.LG] &nbsp; (or arXiv:2203.17189v1 [cs.LG] for this version) &nbsp; https://doi.org/10.48550/arXiv.2203.17189 Focus to learn more arXiv-issued DOI via DataCite Submission history From: Hyung Won Chung [ view email ] [v1] Thu, 31 Mar 2022 17:12:13 UTC (545 KB) Full-text links: Access Paper: View a PDF of the paper titled Scaling Up Models and Data with $\\texttt{t5x}$ and $\\texttt{seqio}$, by Adam Roberts and 42 other authors View PDF TeX Source Other Formats view license Current browse context: cs.LG &lt;&nbsp;prev &nbsp; | &nbsp; next&nbsp;&gt; new | recent | 2022-03 Change to browse by: cs cs.CL References &amp; Citations NASA ADS Google Scholar Semantic Scholar export BibTeX citation Loading... BibTeX formatted citation &times; loading... Data provided by: Bookmark Bibliographic Tools Bibliographic and Citation Tools Bibliographic Explorer Toggle Bibliographic Explorer ( What is the Explorer? ) Connected Papers Toggle Connected Papers ( What is Connected Papers? ) Litmaps Toggle Litmaps ( What is Litmaps? ) scite.ai Toggle scite Smart Citations ( What are Smart Citations? ) Code, Data, Media Code, Data and Media Associated with this Article alphaXiv Toggle alphaXiv ( What is alphaXiv? ) Links to Code Toggle CatalyzeX Code Finder for Papers ( What is CatalyzeX? ) DagsHub Toggle DagsHub ( What is DagsHub? ) GotitPub Toggle Gotit.pub ( What is GotitPub? ) Huggingface Toggle Hugging Face ( What is Huggingface? ) Links to Code Toggle Papers with Code ( What is Papers with Code? ) ScienceCast Toggle ScienceCast ( What is ScienceCast? ) Demos Demos Replicate Toggle Replicate ( What is Replicate? ) Spaces Toggle Hugging Face Spaces ( What is Spaces? ) Spaces Toggle TXYZ.AI ( What is TXYZ.AI? ) Related Papers Recommenders and Search Tools Link to Influence Flower Influence Flower ( What are Influence Flowers? ) Core recommender toggle CORE Recommender ( What is CORE? ) IArxiv recommender toggle IArxiv Recommender ( What is IArxiv? ) Author Venue Institution Topic About arXivLabs arXivLabs: experimental projects with community collaborators arXivLabs is a framework that allows collaborators to develop and share new arXiv features directly on our website. Both individuals and organizations that work with arXivLabs have embraced and accepted our values of openness, community, excellence, and user data privacy. arXiv is committed to these values and only works with partners that adhere to them. Have an idea for a project that will add value for arXiv's community? Learn more about arXivLabs . Which authors of this paper are endorsers? | Disable MathJax ( What is MathJax? ) About Help contact arXiv Click here to contact arXiv Contact subscribe to arXiv mailings Click here to subscribe Subscribe Copyright Privacy Policy Web Accessibility Assistance arXiv Operational Status Get status notifications via email or slack ",
      "fetch_method": "direct-html"
    },
    {
      "arxiv_id": "https://colab.research.google.com/drive/1yRDXktu030DnipFBj6-dwOGNVIdgktA9?usp=sharing",
      "full_text": " Google Colab Sign in ",
      "fetch_method": "direct-html"
    },
    {
      "arxiv_id": "https://docs.google.com/drawings/d/1wSt_X0olUFcOFQ5D1UnMv1V-LKMr3WZIRIgaFypTP24/edit?usp=sharing",
      "full_text": " xp3_variants - Google 드로잉 자바스크립트가 브라우저에서 활성화되어 있지 않아 이 파일을 열 수 없습니다. 활성화하고 새로고침하세요. xp3_variants &nbsp; 외부 &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; 공유 로그인 파일 수정 보기 삽입 도움말 접근성 디버그 &nbsp; &nbsp; 드라이브에 변경사항이 저장되지 않음 &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; 접근성 &nbsp; &nbsp; 댓글 전용 &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; ",
      "fetch_method": "direct-html"
    },
    {
      "arxiv_id": "https://colab.research.google.com/drive/1bz083LuBJi0-pLOqdr4_ycEctn6obYST?usp=sharing",
      "full_text": " Google Colab Sign in ",
      "fetch_method": "direct-html"
    },
    {
      "arxiv_id": "https://colab.research.google.com/drive/1lFFR6_ijR_iWJQnqIW5y5-LuRnRoRTS3?usp=sharing",
      "full_text": " Google Colab Sign in ",
      "fetch_method": "direct-html"
    },
    {
      "arxiv_id": "https://colab.research.google.com/drive/19GcYT5SJFpyu8B0RrewN462w3i461mZ5?usp=sharing",
      "full_text": " Google Colab Sign in ",
      "fetch_method": "direct-html"
    },
    {
      "arxiv_id": "https://colab.research.google.com/drive/1FWW7LMKC9kQNLgCLZXl_dBER5wBSPGMu?usp=sharing",
      "full_text": " Google Colab Sign in ",
      "fetch_method": "direct-html"
    },
    {
      "arxiv_id": "https://colab.research.google.com/drive/1hpW6xEnU56Ed7DmXrREzczGwEeNV8KJ2?usp=sharing",
      "full_text": " Google Colab Sign in ",
      "fetch_method": "direct-html"
    },
    {
      "arxiv_id": "https://colab.research.google.com/drive/1ankXUcTqjPantCzIfUSwAjYfAhkR7M6o?usp=sharing",
      "full_text": " Google Colab Sign in ",
      "fetch_method": "direct-html"
    },
    {
      "arxiv_id": "https://colab.research.google.com/drive/1ZhwHDaHBPUlZiTp-ZZxy7axuWgE68FkW?usp=sharing",
      "full_text": " Google Colab Sign in ",
      "fetch_method": "direct-html"
    },
    {
      "arxiv_id": "https://colab.research.google.com/drive/1WCUgfjToVJ9b_fJHzkWKsuGzVofqv38x?usp=sharing",
      "full_text": " Google Colab Sign in ",
      "fetch_method": "direct-html"
    },
    {
      "arxiv_id": "https://colab.research.google.com/drive/1AWJk3jbrD1VpiMARW-xATalrupwFzZN-?usp=sharing",
      "full_text": " Google Colab Sign in ",
      "fetch_method": "direct-html"
    },
    {
      "arxiv_id": "https://colab.research.google.com/drive/14t9w6QSf2K5BQP0cInyGsreAhY271DLB?usp=sharing",
      "full_text": " Google Colab Sign in ",
      "fetch_method": "direct-html"
    },
    {
      "arxiv_id": "https://raw.githubusercontent.com/bigscience-workshop/xmtf/master/plotstables/XMTF_ACL2023_POSTER.pdf",
      "full_text": "I. xP3: Crosslingual Corpus of prompted tasks in up to 277 languages\nII. BLOOMZ & mT0: Instruction-tuned multilingual LLMs with up to 176B parameters\nCrosslingual Generalization through Multitask Finetuning\nGeneralization to unseen tasks\nScaling from 560M to 176B parameters\n& importance of language distributions\nNiklas Muennighoff, Thomas Wang, Lintang Sutawika, Adam Roberts, Stella Biderman, Teven Le Scao, M Saiful Bari, \nSheng Shen, Zheng-Xin Yong, Hailey Schoelkopf, Xiangru Tang, Dragomir Radev, Alham Fikri Aji, \nKhalid Almubarak, Samuel Albanie, Zaid Alyafeai, Albert Webson, Edward Raff, Colin Raffel\nPAPER\nxP3 datasets & tasks divided into training \n(yellow) & evaluation (green)\nxP3 language distribution\nxP3 variants\nxP3 eXtension:\n●\n🌍277 languages\n●\n🎡700GB\n●\n🤯500M samples\n+ xP3x (2023)\nGeneralization to unseen tasks in “unseen” languages\nGeneration\nBLOOMZ is biased towards short \nanswers as most training examples \nare short ⚠\n➡Workaround: \nForce minimum generation length💡\n•\nTraining on xP3mt influences \ngeneration language 🇨🇳🇬🇧🇫🇷\n•\nBLOOMZ capable of creative \nwriting following instructions, \ne.g. generating fables 👺\n•\nModels still display some \nfailure modes like not clearly \nfollowing instructions ⛔\nGenerative performance degrades if fine-tuning too long! 💀\nCode Generation on HumanEval 💻\nSummarization\nSentiment\nParaphrase\nIdentification\nCoreference\nResolution\nQQP\nMRPC\nPAWS\nYelp\nRotten Tomatoes\nApp Reviews\nIMDB\nAmazon\nTopic Classification\nAG News\nDBPedia\nTREC\nStructure-To-Text\nWiki Bio\nCommon Gen\nMultiNews\nGigaword\nXSum\nSamSum\nCNN Daily Mail\nClosed-Book QA\nHotpot QA\nWiki QA\nExtractive QA\nROPES\nAdversarial QA\nDuoRC\nMultiple-Choice QA\nDREAM\nQuAIL\nQuaRTz\nSocial IQA\nCosmos QA\nQASC\nWiQA\nSciQ\nQuaRel\nCOPA\nSentence \nCompletion\nStoryCloze\nNatural Language\nInference\nANLI\nCB\nRTE\nWinogrande\nWord Sense\nDisambiguation\nWiC\nQuoref\nWiki Hop\nPAWS-X\nXWinograd\nXNLI\nXCOPA\nWiki-Lingua\nxQuAD\nTranslation\nTatoeba\nFlores-200\nProgram Synthesis\nHumanEval\nMBPP\nXL-WiC\nXLSum\nXStoryCloze\nNeuralCodeSearch\nGreatCode\nMLQA\nTyDi QA\nCode Misc.\nCodeComplex\nState Changes\nAPPS\nXLCoST\nCodeContests\nDocstring Corpus\nCSL\nJupyterCodePairs\nC3\nCMRC2018\nDRCD\nTNEWS\nSimplification\nBiSECT\nRACE\nBoolQ\nARC (AI2)\nTriviaQA\nSQuAD (V2)\nReCoRD\nMultiRC\nPiQA\nWebQuestions\nCoS-E\nOpenBookQA\nmT0 beats T0 on English Tasks\nBLOOMZ:\nBLOOM:\nWhy is the sky blue?\nInput:\nRayleigh scattering.\nBLOOMZ:\nRayleigh scattering. The blue\ncolor of the sky is due to\nRayleigh scattering of\nsunlight by atmospheric\nmolecules.\nBLOOMZ with 10\ntokens min length:\nmT0-13B:\nbecause of the sun\nbecause of the sun's\nradiation sensitivity to light\nmT0-13B with 10\ntokens min length:\nBLOOM:\n\"Why is the sky blue?\"\n\"Why is the sky blue?\"\n\"Why is the sky blue?\"...\nWrite a fairy tale about a troll saving a princess from a\ndangerous dragon. The fairy tale is a masterpiece that\nhas achieved praise worldwide and its moral is \"Heroes\nCome in All Shapes and Sizes\". Story (in Spanish):\nInput:\nUn reino en el bosque estaba bajo un terrible\ndragón. Todos los que intentaron derrotar al\ndragón murieron. El rey y la reina le pedían al\npueblo que les diera un niño, que les gustaría\nsalvar el reino de la maldición del dragón. Un\ndía, la reina dio a luz a un niño con una cabeza\nenorme y ojos que brillaban como estrellas. El\nrey y la reina llamaron al niño Elliot. Elliot\ncrecería y se convertiría en un joven gigante. Él\nsalvaría al pueblo.\nBLOOMZ:\nEn la noche de la primera tempestad, un gigante\ndragón, llamado Odín, golpeó a una princesa al\ndedo. Así que un troll, llamado Odín, fue a salvar\na la princesa. Luego, este fue querido por todos.\nmT0:\n一个传奇的开端，一个不\n灭的神话，这不仅仅是一\n部电影，而是作为一个走\n进新时代的标签，永远彪\n炳史册。你认为这句话的\n立场是赞扬、中立还是批\n评？\nInput:\npositive\nBLOOMZ:\n赞扬\nBLOOMZ-MT:\nmT0-13B:\n赞扬\nProposez au moins\ncinq mots clés\nconcernant «Réseau de\nneurones artificiels».\nInput:\nartificial neural network,\nartificial intelligence,\nmachine learning, deep\nlearning, neural network\nBLOOMZ:\nBLOOMZ-MT:\nmT0-13B:\napprentissage, réseau,\nintelligence artificielle\nréseau de neurones\nartificiels, réseau de\nneurones, apprentissage\nautomatique, apprentissage\nprofond, apprentissage en\nprofondeur\n🇧🇬\n🇩🇪\n🇬🇷\n🇷🇺\n🇹🇭\n🇹🇷\n🇯🇵\n🇷🇺\n🇪🇪\n🇭🇹\n🇮🇹\n🇵🇪\n🇹🇭\n🇹🇷\n🇲🇲\n🇷🇺\nunseen = not pretrained nor finetuned on\n",
      "fetch_method": "direct-pdf"
    },
    {
      "arxiv_id": "https://raw.githubusercontent.com/bigscience-workshop/xmtf/master/plotstables/XMTF_ACL2023_PRES.pdf",
      "full_text": "Crosslingual Generalization through \nMultitask Finetuning\nNiklas Muennighoff, Thomas Wang, Lintang Sutawika, \nAdam Roberts, Stella Biderman, Teven Le Scao, M \nSaiful Bari, Sheng Shen, Zheng-Xin Yong, Hailey \nSchoelkopf, Xiangru Tang, Dragomir Radev, Alham Fikri \nAji, Khalid Almubarak, Samuel Albanie, Zaid Alyafeai, \nAlbert Webson, Edward Raff, Colin Raffel\nPAPER\n& xP3\n\nMotivation: Instruction Tuning & Multilingual Models\nInstruction-tuning teaches models to perform natural language tasks to make them useful.\nCrosslingual Generalization through Multitask Finetuning (Muennighoff et al., 2022)\nPre-trained Model:   , Where is the capital of Japan? How many islands are there in Japan? These are all great      \n                                   questions that require a more complex answer than a name or a number and…\n Instruction-tuned:   Tokyo\nWhat is the capital of Japan?\n Sanh et al. (2021), Wei et al. (2021), Ouyang et al. (2022), inter alia\nMultilingual models can write in dozens of languages\nT5: 󰏅                                                                  \nmT5: 󰏅󰐮󰎼󰎲󰏃󰏢󰐨󰐤󰐗󰑍󰏦󰑜󰏙󰎱󰎩…          \nBLOOM: 󰏅󰎩󰏃󰎼💻󰐨…                                   \n\nxP3: Crosslingual Corpus of prompted tasks in up to 277 languages\nxP3 Task Taxonomy | Yellow: Training | Green: Evaluation\nCrosslingual Generalization through Multitask Finetuning (Muennighoff et al., 2022)\n\nxP3: Crosslingual Corpus of prompted tasks in up to 277 languages\nDataset variations\nCrosslingual Generalization through Multitask Finetuning (Muennighoff et al., 2022)\nxP3 eXtension:\n●\n🌍  277 languages\n●\n🎡 700GB\n●\n🤯  500M samples\n+\nxP3x (2023)\n\nMultitask Fine-tuning / Instruction tuning\nCrosslingual Generalization through Multitask Finetuning (Muennighoff et al., 2022)\n1. Pretrained \nLanguage Model (LM)\nBLOOM: 560M - 176B parameter \ndecoder LMs trained on 48 \nlanguages\nmT5: 300M - 13B parameter \nencoder-decoder LMs trained on \n101 languages\n2. Finetune on many \ntasks with instructions\nP3: English instructions; English data\nxP3: English instructions; Multilingual \ndata\nxP3mt: Multilingual instructions; \nMultilingual data\n3. Evaluate on tasks \nNOT trained on\nSentence Completion: XCOPA & \nXStoryCloze\nNatural Language Inference: \nXNLI\nCoreference resolution: \nXWinograd\n Sanh et al. (2021), Wei et al. (2021), Ouyang et al. (2022), inter alia\n\nBLOOMZ & mT0: Generalization to unseen Tasks\nCrosslingual Generalization through Multitask Finetuning (Muennighoff et al., 2022)\n\nBLOOMZ & mT0: Generalization to unseen Tasks\nCrosslingual Generalization through Multitask Finetuning (Muennighoff et al., 2022)\n\nBLOOMZ & mT0: Generalization to unseen Tasks in \n“unseen” Languages\nCrosslingual Generalization through Multitask Finetuning (Muennighoff et al., 2022)\n󰎲\n󰎒\n󰏏\n󰐮\n󰑆\n󰑍\n󰏦\n󰐮\n󰎹\n󰏗\n󰏢\n󰐟\n󰑆\n󰑍\n󰐅\n󰐮\n\nBLOOMZ & mT0: Generalization to unseen Tasks\nCrosslingual Generalization through Multitask Finetuning (Muennighoff et al., 2022)\n\nCheck the paper for more details on:\n●\nImpact of non-English instructions (xP3mt)\n●\nGenerative tasks such as code generation\n●\nImpact of pre-training language distribution\n●\nLanguage contamination\n…\nNiklas Muennighoff, Thomas Wang, Lintang Sutawika, Adam Roberts, Stella \nBiderman, Teven Le Scao, M Saiful Bari, Sheng Shen, Zheng-Xin Yong, Hailey \nSchoelkopf, Xiangru Tang, Dragomir Radev, Alham Fikri Aji, Khalid Almubarak, \nSamuel Albanie, Zaid Alyafeai, Albert Webson, Edward Raff, Colin Raffel\nPaper:\narxiv.org/abs/2211.01786\n& xP3\nOpen-source code/models/data: \ngithub.com/bigscience-workshop/xmtf\nThanks!\n",
      "fetch_method": "direct-pdf"
    },
    {
      "arxiv_id": "https://raw.githubusercontent.com/bigscience-workshop/xmtf/master/plotstables/contamination/language_plot_identify.pdf",
      "full_text": "en fr es pt vi ar ca id zh gl ja hi ms la\nit eu ta da no de bn nl lb fy af cy eo ht mt ml te sr sv mrmg ne gd ga ro zu hu jv kn fil co sn ig et pl su sl hawbg sm ur ceb lt gu xh sw sq lv pa sk cshmnru uk el so fi az ny mi tr ha ko st yo uz is fa be sd tg ku mk ky kk ps mn yi iw th hy am ka si kmmy lo\n25\n5\n1\n0.1\n0.01\n0.001\n0.0001\n% of corpus\nROOTS-IDENTIFY-1%\nROOTS-1%\nmC4\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\n",
      "fetch_method": "direct-pdf"
    },
    {
      "arxiv_id": "https://raw.githubusercontent.com/bigscience-workshop/xmtf/master/plotstables/english_task_generalization.pdf",
      "full_text": "60\n70\n80\n90\n100\n0\n20\n40\n60\n80\n100\n20\n30\n40\n50\n20\n30\n40\n50\n20\n30\n40\n50\n20\n30\n40\n50\n60\n70\n40\n50\n60\n70\n80\n90\n40\n50\n60\n70\n80\n90\n50\n60\n70\n80\n90\n100\n50\n60\n70\n80\n90\n100\nBLOOMZ-P3\nBLOOMZ\nmT0-13B-P3\nmT0-13B\nT0-11B\nRTE\nCB\nANLI R1\nANLI R2\nANLI R3\nXNLI EN\nWinogrande\nXWinograd EN\nCOPA\nStoryCloze\nNatural Language Inference\nCoreference Resolution\nSentence Completion\n",
      "fetch_method": "direct-pdf"
    },
    {
      "arxiv_id": "https://r.jina.ai/https://raw.githubusercontent.com/bigscience-workshop/xmtf/master/plotstables/examples/bloom_code_light.pdf",
      "full_text": "Title: \n\nURL Source: https://raw.githubusercontent.com/bigscience-workshop/xmtf/master/plotstables/examples/bloom_code_light.pdf\n\nMarkdown Content:\n\n",
      "fetch_method": "jina-reader"
    },
    {
      "arxiv_id": "https://raw.githubusercontent.com/bigscience-workshop/xmtf/master/plotstables/examples/bloom_code_light.pdf",
      "full_text": "",
      "fetch_method": "link-only"
    },
    {
      "arxiv_id": "https://r.jina.ai/https://raw.githubusercontent.com/bigscience-workshop/xmtf/master/plotstables/examples/bloomz_code_light.pdf",
      "full_text": "Title: \n\nURL Source: https://raw.githubusercontent.com/bigscience-workshop/xmtf/master/plotstables/examples/bloomz_code_light.pdf\n\nMarkdown Content:\n\n",
      "fetch_method": "jina-reader"
    },
    {
      "arxiv_id": "https://raw.githubusercontent.com/bigscience-workshop/xmtf/master/plotstables/examples/bloomz_code_light.pdf",
      "full_text": "",
      "fetch_method": "link-only"
    },
    {
      "arxiv_id": "https://raw.githubusercontent.com/bigscience-workshop/xmtf/master/plotstables/examples/exp_one.pdf",
      "full_text": "Explain in a sentence in\nTelugu what is\nbackpropagation in\nneural networks.\nInput:\nఒక న్యూట్రానిక్ నెట్వర్క్ లో\nబ్యాక్ ప్రాసెసింగ్ అనేది ఒక\nరకమైన అభ్యసన విధానం, ఇది\nరీజియన్ ల యొక్క గత\nఅభ్యసన ఫలితాలను\nఉపయోగించి, రీజియన్ ల\nయొక్క ప్రస్తుత అభ్యసన\nఫలితాలను సవరిస్తుంది.\nBLOOMZ:\nఒక న్యూట్రాన్‌కు తిరిగి పంపిన\nప్రతిబింబాన్ని ఒక రకమైన\nబ్యాక్ ప్రాసెసింగ్ అంటారు.\nBLOOMZ-MT:\nmT0-13B:\nనేరుగా నేరుగా న్యూరోలాజికల్\nనెట్ వర్క్ లలో బేక్ పోర్షన్\nఅంటే ఏమిటి?\n",
      "fetch_method": "direct-pdf"
    },
    {
      "arxiv_id": "https://raw.githubusercontent.com/bigscience-workshop/xmtf/master/plotstables/examples/exp_two.pdf",
      "full_text": "Why is the sky blue?\nInput:\nRayleigh scattering.\nBLOOMZ:\nRayleigh scattering. The blue\ncolor of the sky is due to\nRayleigh scattering of\nsunlight by atmospheric\nmolecules.\nBLOOMZ with 10\ntokens min length:\nmT0-13B:\nbecause of the sun\nbecause of the sun's\nradiation sensitivity to light\nmT0-13B with 10\ntokens min length:\nBLOOM:\n\"Why is the sky blue?\"\n\"Why is the sky blue?\"\n\"Why is the sky blue?\"...\n",
      "fetch_method": "direct-pdf"
    },
    {
      "arxiv_id": "https://raw.githubusercontent.com/bigscience-workshop/xmtf/master/plotstables/examples/fable_hindi.pdf",
      "full_text": "Write a fable about wood elves living in a forest that is\nsuddenly invaded by ogres. The fable is a masterpiece that\nhas achieved praise worldwide and its moral is \"Violence is\nthe last refuge of the incompetent\". Fable (in Hindi):\nInput:\nएक समय की बात है, देवदार के वृक्षों से आच्छादित एक वन में\nएक समुदाय रहता था जिनमें से प्रत्येक व्यक्ति का कद एक फ़ीट\nका चार इंच ही था, परंतु वे लोग बहुत ही बुद्धिमान और सभ्य थे।\nउनके बीच परस्पर प्रेम और भाइचारे की भावना थी। अपने\nपड़ोसियों के प्रति उनके व्यवहार और आदर को देखकर लोग\nआश्चर्य में पड़ जाते थे। वे वृक्षों की छाल के बने हुए छोटे-छोटे\nकमरों में रहते थे। अपनी बुद्धिमत्ता के कारण तो वे लोग प्रसिद्ध\nथे ही, पर विशेष रूप से अपने सदगुणों के कारण उनका नाम\nदूर-दूर तक मशहूर था। वे लोग स्वयं को डंगऑल (Dongal)\nकहते थे। वे जिस वन में रहते थे, उसी वन की हरियाली और\nपेड़-पौधों के कारण ही तो वह वन इतना आकर्षक था। डंगऑल\nउस वन के स्वामी नहीं थे। उनकी बस एक शर्त थी कि वे हर एक\nपेड़ पर केवल एक ही घर बनाएँगे। एक दिन उस वन में घुस आए\nकुछ लोग जो स्वभाव से ही जंगली और असभ्य थे। उन्होंने उस\nवन में अपना आधिपत्य जमा लिया। वे बहुत विशाल कद के थे\nऔर उनकी आँखें सुर्ख़ लाल रंग की थे। उन्होंने उस वन की\nहरियाली नष्ट कर दी और वृक्षों को काटना शुरू कर दिया।\nBLOOMZ:\n",
      "fetch_method": "direct-pdf"
    },
    {
      "arxiv_id": "https://raw.githubusercontent.com/bigscience-workshop/xmtf/master/plotstables/examples/fable_spanish.pdf",
      "full_text": "Write a fairy tale about a troll saving a princess from a\ndangerous dragon. The fairy tale is a masterpiece that\nhas achieved praise worldwide and its moral is \"Heroes\nCome in All Shapes and Sizes\". Story (in Spanish):\nInput:\nUn reino en el bosque estaba bajo un terrible\ndragón. Todos los que intentaron derrotar al\ndragón murieron. El rey y la reina le pedían al\npueblo que les diera un niño, que les gustaría\nsalvar el reino de la maldición del dragón. Un\ndía, la reina dio a luz a un niño con una cabeza\nenorme y ojos que brillaban como estrellas. El\nrey y la reina llamaron al niño Elliot. Elliot\ncrecería y se convertiría en un joven gigante. Él\nsalvaría al pueblo.\nBLOOMZ:\nEn la noche de la primera tempestad, un gigante\ndragón, llamado Odín, golpeó a una princesa al\ndedo. Así que un troll, llamado Odín, fue a salvar\na la princesa. Luego, este fue querido por todos.\nmT0:\n",
      "fetch_method": "direct-pdf"
    },
    {
      "arxiv_id": "https://raw.githubusercontent.com/bigscience-workshop/xmtf/master/plotstables/examples/language_plot_identify.pdf",
      "full_text": "en fr es pt vi ar ca id zh gl ja hi ms la it eu ta da no de bn nl lb fy af cy eo ht mt ml te sr sv mrmgne gd ga ro zu hu jv kn fil co sn ig et pl su slhawbg sm ur ceb lt gu xh sw sq lv pa sk cshmnru uk el so fi az ny mi tr ha ko st yo uz is fa be sd tg ku mk ky kk ps mn yi iw th hy am ka si kmmy lo\n25\n5\n1\n0.1\n0.01\n0.001\n0.0001\n% of corpus\nROOTS-IDENTIFY-1%\nROOTS-1%\nmT5 corpus\n",
      "fetch_method": "direct-pdf"
    },
    {
      "arxiv_id": "https://raw.githubusercontent.com/bigscience-workshop/xmtf/master/plotstables/examples/query_one.pdf",
      "full_text": "Suggest at least five\nrelated search terms to\n\"Mạng neural nhân\ntạo\".\nInput:\nartificial neural network,\nneural network, artificial\nintelligence, machine\nlearning, deep learning\nBLOOMZ:\ntrí tuệ nhân tạo, mạng thần\nkinh, mạng nơ-ron, mạng nơ-\nron nhân tạo, mạng nơ-ron\nsâu\nBLOOMZ-MT:\nmT0-13B:\nmạng neural, tự động hóa, tự\nđộng hóa mạng neural\n",
      "fetch_method": "direct-pdf"
    },
    {
      "arxiv_id": "https://raw.githubusercontent.com/bigscience-workshop/xmtf/master/plotstables/examples/query_two.pdf",
      "full_text": "Proposez au moins\ncinq mots clés\nconcernant «Réseau de\nneurones artificiels».\nInput:\nartificial neural network,\nartificial intelligence,\nmachine learning, deep\nlearning, neural network\nBLOOMZ:\nBLOOMZ-MT:\nmT0-13B:\napprentissage, réseau,\nintelligence artificielle\nréseau de neurones\nartificiels, réseau de\nneurones, apprentissage\nautomatique, apprentissage\nprofond, apprentissage en\nprofondeur\n",
      "fetch_method": "direct-pdf"
    },
    {
      "arxiv_id": "https://raw.githubusercontent.com/bigscience-workshop/xmtf/master/plotstables/examples/sent_one.pdf",
      "full_text": "一个传奇的开端，一个不\n灭的神话，这不仅仅是一\n部电影，而是作为一个走\n进新时代的标签，永远彪\n炳史册。Would you rate\nthe previous review as\npositive, neutral or\nnegative?\nInput:\npositive\nBLOOMZ:\npositive\nBLOOMZ-MT:\nmT0-13B:\npositive\n",
      "fetch_method": "direct-pdf"
    },
    {
      "arxiv_id": "https://raw.githubusercontent.com/bigscience-workshop/xmtf/master/plotstables/examples/sent_two.pdf",
      "full_text": "一个传奇的开端，一个不\n灭的神话，这不仅仅是一\n部电影，而是作为一个走\n进新时代的标签，永远彪\n炳史册。你认为这句话的\n立场是赞扬、中立还是批\n评？\nInput:\npositive\nBLOOMZ:\n赞扬\nBLOOMZ-MT:\nmT0-13B:\n赞扬\n",
      "fetch_method": "direct-pdf"
    },
    {
      "arxiv_id": "https://raw.githubusercontent.com/bigscience-workshop/xmtf/master/plotstables/lang_generalization.pdf",
      "full_text": "20\n30\n40\n50\n60\n20\n30\n40\n50\n60\n20\n30\n40\n50\n60\n20\n30\n40\n50\n60\n20\n30\n40\n50\n60\n20\n30\n40\n50\n60\n40\n50\n60\n70\n40\n50\n60\n70\n40\n50\n60\n70\n80\n40\n50\n60\n70\n80\n40\n50\n60\n70\n80\n40\n50\n60\n70\n80\n40\n50\n60\n70\n80\n40\n50\n60\n70\n80\n40\n50\n60\n70\n80\n90\n40\n50\n60\n70\n80\n90\nBLOOM-7.1B\nBLOOM\nBLOOMZ-7.1B\nBLOOMZ\nXNLI BG\nXNLI DE\nXNLI EL\nXNLI RU\nXNLI TH\nXNLI TR\nXWinograd JP\nXWinograd RU\nXCOPA ET\nXCOPA HT\nXCOPA IT\nXCOPA QU\nXCOPA TH\nXCOPA TR\nXStoryCloze MY\nXStoryCloze RU\nNatural Language Inference\nCoreference Resolution\nSentence Completion\n",
      "fetch_method": "direct-pdf"
    },
    {
      "arxiv_id": "https://raw.githubusercontent.com/bigscience-workshop/xmtf/master/plotstables/pretraining_sizes.pdf",
      "full_text": "0.01\n0.1\n1\n5 10 20\n55\n65\n75\n85\nAverage accuracy (%)\nXCOPA\n0.01\n0.1\n1\n5 1020\n35\n45\n55\n65\nXNLI\n0.01\n0.1\n1\n5 10 20\nPre-training size in BLOOM (%)\n50\n70\n90\nAverage accuracy (%)\nXStoryCloze\n5\n10\n20\n30\nPre-training size in BLOOM (%)\n50\n55\n60\n65\n70\nXWinograd\nBLOOM\nBLOOMZ-P3\nBLOOMZ-MT\nBLOOMZ\n",
      "fetch_method": "direct-pdf"
    },
    {
      "arxiv_id": "https://raw.githubusercontent.com/bigscience-workshop/xmtf/master/plotstables/roots_xp3_languages_bloom_paper.pdf",
      "full_text": "en es pt\nfr\nar\nid\nzh\nhi code vi\nur\nte\nta bn mr sw gu pa ne yo\nig ny zu xh sn\nts rw lg\ntn nso rn ml kn or as\nln wotum ki\nst fon ca eu ak bm tw\n25\n5\n1\n0.1\n0.01\n0.001\n0.0001\n% of corpus\nxP3\nROOTS\n",
      "fetch_method": "direct-pdf"
    },
    {
      "arxiv_id": "https://raw.githubusercontent.com/bigscience-workshop/xmtf/master/plotstables/scale.pdf",
      "full_text": "0.561 1.7 3\n7.1\n176\n40\n60\n80\n100\nAverage accuracy (%)\nXCOPA\n0.561 1.7 3\n7.1\n176\n30\n40\n50\n60\nXNLI\n0.561 1.7 3\n7.1\n176\nModel parameters (billions)\n40\n60\n80\n100\nAverage accuracy (%)\nXStoryCloze\n0.561 1.7 3\n7.1\n176\nModel parameters (billions)\n45\n55\n65\n75\n85\nXWinograd\nBLOOM\nBLOOMZ\nmT0\n",
      "fetch_method": "direct-pdf"
    },
    {
      "arxiv_id": "https://raw.githubusercontent.com/bigscience-workshop/xmtf/master/plotstables/task_generalization.pdf",
      "full_text": "20\n30\n40\n50\n60\n70\n80\n20\n30\n40\n50\n60\n70\n80\n20\n30\n40\n50\n60\n70\n80\n20\n30\n40\n50\n60\n70\n80\n20\n30\n40\n50\n60\n70\n80\n20\n30\n40\n50\n60\n70\n80\n20\n30\n40\n50\n60\n70\n80\n20\n30\n40\n50\n60\n70\n80\n40\n50\n60\n70\n80\n90\n40\n50\n60\n70\n80\n90\n40\n50\n60\n70\n80\n90\n40\n60\n80\n100\n40\n60\n80\n100\n40\n60\n80\n100\n40\n60\n80\n100\n40\n60\n80\n100\n40\n60\n80\n100\n40\n60\n80\n100\n40\n60\n80\n100\n40\n60\n80\n100\n40\n60\n80\n100\n40\n60\n80\n100\n40\n60\n80\n100\n40\n60\n80\n100\nXGLM-7.5B\nBLOOM\nmTk-13B\nBLOOMZ\nmT0-13B\nXNLI AR\nXNLI ES\nXNLI FR\nXNLI HI\nXNLI VI\nXNLI UR\nXNLI SW\nXNLI ZH\nXWinograd FR\nXWinograd PT\nXWinograd ZH\nXCOPA ID\nXCOPA SW\nXCOPA TA\nXCOPA VI\nXCOPA ZH\nXStoryCloze AR\nXStoryCloze ES\nXStoryCloze EU\nXStoryCloze HI\nXStoryCloze ID\nXStoryCloze SW\nXStoryCloze TE\nXStoryCloze ZH\nNatural Language Inference\nCoreference Resolution\nSentence Completion\n",
      "fetch_method": "direct-pdf"
    },
    {
      "arxiv_id": "https://raw.githubusercontent.com/bigscience-workshop/xmtf/master/plotstables/task_generalization_bar.pdf",
      "full_text": "Sentence Completion (XCOPA & XStoryCloze)\n51.8\n52.9\n50.4\n80.5\n82.6\n86.0\nNatural Language Inference (XNLI)\n33.3\n33.6\n34.8\n47.4\n55.3\n57.7\nMultilingual Multitask Generalization\nXGLM-7.5B\nBLOOM\nmTk-Instruct-13B\nBLOOMZ-P3\nBLOOMZ\nmT0-13B\nCoreference Resolution (XWinograd)\n50.3\n50.6\n54.7\n54.6\n66.3\n74.6\n",
      "fetch_method": "direct-pdf"
    },
    {
      "arxiv_id": "https://raw.githubusercontent.com/bigscience-workshop/xmtf/master/plotstables/task_generalization_bloom_paper.pdf",
      "full_text": "20\n30\n40\n50\n60\n70\n80\n20\n30\n40\n50\n60\n70\n80\n20\n30\n40\n50\n60\n70\n80\n20\n30\n40\n50\n60\n70\n80\n20\n30\n40\n50\n60\n70\n80\n20\n30\n40\n50\n60\n70\n80\n20\n30\n40\n50\n60\n70\n80\n20\n30\n40\n50\n60\n70\n80\n40\n50\n60\n70\n80\n90\n40\n50\n60\n70\n80\n90\n40\n50\n60\n70\n80\n90\n40\n60\n80\n100\n40\n60\n80\n100\n40\n60\n80\n100\n40\n60\n80\n100\n40\n60\n80\n100\n40\n60\n80\n100\n40\n60\n80\n100\n40\n60\n80\n100\n40\n60\n80\n100\n40\n60\n80\n100\n40\n60\n80\n100\n40\n60\n80\n100\n40\n60\n80\n100\nXGLM-7.5B\nBLOOM\nmTk-13B\nT0-11B\nBLOOMZ-7.1B\nBLOOMZ\nXNLI AR\nXNLI ES\nXNLI FR\nXNLI HI\nXNLI VI\nXNLI UR\nXNLI SW\nXNLI ZH\nXWinograd FR\nXWinograd PT\nXWinograd ZH\nXCOPA ID\nXCOPA SW\nXCOPA TA\nXCOPA VI\nXCOPA ZH\nXStoryCloze AR\nXStoryCloze ES\nXStoryCloze EU\nXStoryCloze HI\nXStoryCloze ID\nXStoryCloze SW\nXStoryCloze TE\nXStoryCloze ZH\nNatural Language Inference\nCoreference Resolution\nSentence Completion\n",
      "fetch_method": "direct-pdf"
    },
    {
      "arxiv_id": "https://raw.githubusercontent.com/bigscience-workshop/xmtf/master/plotstables/validation.pdf",
      "full_text": "30\n40\n50\n60\nAverage acc. (%)\nBLOOMZ\nBLOOMZ-7.1B\nmT0-13B\n0\n2\n4\n6\n8\nTraining tokens (billions)\n0\n10\n20\nAverage BLEU (%)\n",
      "fetch_method": "direct-pdf"
    },
    {
      "arxiv_id": "https://raw.githubusercontent.com/bigscience-workshop/xmtf/master/plotstables/xp3_languages.pdf",
      "full_text": "en es pt\nfr\nar\nid\nzh\nhi code vi\nur\nte\nta bn mr sw gu pa ne yo ig ny zu xh sn\nts rw lg\ntn nso rn ml kn or as\nln wotum ki\nst fon ca eu ak bm tw\n25\n5\n1\n0.1\n0.01\n0.001\n0.0001\n% of corpus\nxP3\nROOTS\nmT5 corpus\n",
      "fetch_method": "direct-pdf"
    },
    {
      "arxiv_id": "https://raw.githubusercontent.com/bigscience-workshop/xmtf/master/plotstables/xp3_taxonomy.pdf",
      "full_text": "Summarization\nSentiment\nParaphrase\nIdentification\nCoreference\nResolution\nQQP\nMRPC\nPAWS\nYelp\nRotten Tomatoes\nApp Reviews\nIMDB\nAmazon\nTopic Classification\nAG News\nDBPedia\nTREC\nStructure-To-Text\nWiki Bio\nCommon Gen\nMultiNews\nGigaword\nXSum\nSamSum\nCNN Daily Mail\nClosed-Book QA\nHotpot QA\nWiki QA\nExtractive QA\nROPES\nAdversarial QA\nDuoRC\nMultiple-Choice QA\nDREAM\nQuAIL\nQuaRTz\nSocial IQA\nCosmos QA\nQASC\nWiQA\nSciQ\nQuaRel\nCOPA\nSentence \nCompletion\nStoryCloze\nNatural Language\nInference\nANLI\nCB\nRTE\nWinogrande\nWord Sense\nDisambiguation\nWiC\nQuoref\nWiki Hop\nPAWS-X\nXWinograd\nXNLI\nXCOPA\nWiki-Lingua\nxQuAD\nTranslation\nTatoeba\nFlores-200\nProgram Synthesis\nHumanEval\nMBPP\nXL-WiC\nXLSum\nXStoryCloze\nNeuralCodeSearch\nGreatCode\nMLQA\nTyDi QA\nCode Misc.\nCodeComplex\nState Changes\nAPPS\nXLCoST\nCodeContests\nDocstring Corpus\nCSL\nJupyterCodePairs\nC3\nCMRC2018\nDRCD\nTNEWS\nSimplification\nBiSECT\nRACE\nBoolQ\nARC (AI2)\nTriviaQA\nSQuAD (V2)\nReCoRD\nMultiRC\nPiQA\nWebQuestions\nCoS-E\nOpenBookQA\n",
      "fetch_method": "direct-pdf"
    },
    {
      "arxiv_id": "https://raw.githubusercontent.com/bigscience-workshop/xmtf/master/plotstables/xp3_variants.pdf",
      "full_text": "Sentence1 \nHe was a scholar in Metaphysical \nLiterature, Theology and Classical \nsciences.\nSentence2\nHe was a scholar in metaphysical \nliterature, theology, and classical \nscience.\nLabel\n1\nSentence1 \nHe was a scholar in Metaphysical \nLiterature, Theology and Classical \nsciences.\nSentence2\nHe was a scholar in metaphysical \nliterature, theology, and classical \nscience.\nLabel\n1\nSentence1 \nHe was a scholar in Metaphysical \nLiterature, Theology and Classical \nsciences.\nSentence2\nHe was a scholar in metaphysical \nliterature, theology, and classical \nscience.\nLabel\n1\nP3\nsentence1 \nHe was a scholar in Metaphysical \nLiterature, Theology and Classical \nsciences.\nsentence2\nHe was a scholar in metaphysical \nliterature, theology, and classical \nscience.\nlabel\n1\nOración 1: {{sentence1}}\nOración 2: {{sentence2}}\nPregunta: ¿La oración 1 parafrasea la \noración 2? ¿Si o no? {{Choices[label]}}\nSentence 1: {{sentence1}}\nSentence 2: {{sentence2}}\nQuestion: Can we rewrite Sentence 1 to \nSentence 2? Yes or No? {{Choices[label]}}\nChoices=[No,Sí]\nxP3\nSentence1 \nHe was a scholar in Metaphysical \nLiterature, Theology and Classical \nsciences.\nSentence2\nHe was a scholar in metaphysical \nliterature, theology, and classical \nscience.\nLabel\n1\nSentence1 \nHe was a scholar in Metaphysical \nLiterature, Theology and Classical \nsciences.\nSentence2\nHe was a scholar in metaphysical \nliterature, theology, and classical \nscience.\nLabel\n1\nSentence1 \nHe was a scholar in Metaphysical \nLiterature, Theology and Classical \nsciences.\nSentence2\nHe was a scholar in metaphysical \nliterature, theology, and classical \nscience.\nLabel\n1\nsentence1 \nFue académico en literatura metafísica, \nteología y ciencias clásicas.\"\nsentence2\nFue académico en literatura metafísica, \nteología y ciencia clásica.\nlabel\n1\nxP3mt\nSentence1 \nHe was a scholar in Metaphysical \nLiterature, Theology and Classical \nsciences.\nSentence2\nHe was a scholar in metaphysical \nliterature, theology, and classical \nscience.\nLabel\n1\nSentence1 \nHe was a scholar in Metaphysical \nLiterature, Theology and Classical \nsciences.\nSentence2\nHe was a scholar in metaphysical \nliterature, theology, and classical \nscience.\nLabel\n1\nSentence1 \nHe was a scholar in Metaphysical \nLiterature, Theology and Classical \nsciences.\nSentence2\nHe was a scholar in metaphysical \nliterature, theology, and classical \nscience.\nLabel\n1\nsentence1 \nFue académico en literatura metafísica, \nteología y ciencias clásicas.\"\nsentence2\nFue académico en literatura metafísica, \nteología y ciencia clásica.\nlabel\n1\nChoices=[No,Yes]\nSentence 1: {{sentence1}}\nSentence 2: {{sentence2}}\nQuestion: Can we rewrite Sentence 1 to \nSentence 2? Yes or No? {{Choices[label]}}\nChoices=[No,Yes]\n",
      "fetch_method": "direct-pdf"
    }
  ]
}