{
  "1-1 (Weights)": "The project explicitly states that the weights for **Hunyuan-A13B-Instruct** are publicly downloadable. The quoted line “* 2025.6.27 We have open-sourced  **Hunyuan-A13B-Pretrain** , **Hunyuan-A13B-Instruct** , **Hunyuan-A13B-Instruct-FP8** , **Hunyuan-A13B-Instruct-GPTQ-Int4** on Hugging Face.” confirms that the base ‘Pretrain’ checkpoint as well as several instruction-tuned and quantised variants (FP8 and GPTQ-Int4) were released simultaneously on 27 June 2025. A command snippet shows the exact repository name --model tencent/Hunyuan-A13B-Instruct\\, establishing that the primary distribution channel is the Hugging Face Hub under that namespace. The file list excerpt (“model-00001-of-00033.safetensors” and the companion “model.safetensors.index.json”) indicates that the full-precision checkpoint is split across 33 *.safetensors shards with an index file, a conventional Hugging Face format that can be fetched by anyone who meets the licence terms. Taken together, the quotes demonstrate that the weights are openly hosted, versioned, and that multiple precision/format options are included for immediate use or further fine-tuning.",
  "1-1 (Weights)__evidence": [
    {
      "source": "[readme]",
      "quote": "* 2025.6.27 We have open-sourced  **Hunyuan-A13B-Pretrain** , **Hunyuan-A13B-Instruct** , **Hunyuan-A13B-Instruct-FP8** , **Hunyuan-A13B-Instruct-GPTQ-Int4** on Hugging Face."
    },
    {
      "source": "[readme]",
      "quote": "--model tencent/Hunyuan-A13B-Instruct  \\"
    },
    {
      "source": "[files]",
      "quote": "model-00001-of-00033.safetensors"
    },
    {
      "source": "[files]",
      "quote": "model.safetensors.index.json"
    }
  ],
  "1-2 (Code)": "The quotes reference a public GitHub repository: “<a href=\"https://github.com/Tencent-Hunyuan/Hunyuan-A13B\"><b>GITHUB</b></a>”. Inside that repo a source file named “modeling_hunyuan.py” is highlighted and begins with the header “\"\"\" PyTorch HunYuan model.\"\"\"”, signalling that at least the model definition and inference utilities are available in PyTorch form. While the snippets do not explicitly enumerate the entire training pipeline, they confirm that core model code has been released. Therefore: (1) Inference/model-loading code is public, (2) the repository is located under the Tencent-Hunyuan organisation on GitHub, and (3) the implementation is specifically tailored for the Hunyuan-A13B family, making it directly usable with the tencent/Hunyuan-A13B-Instruct checkpoint. No sentence in the provided material claims that full pre-training scripts, RLHF pipelines, or data-generation code are shared, so only the availability of model implementation (and likely basic fine-tuning scripts) can be asserted on the basis of the quotes.",
  "1-2 (Code)__evidence": [
    {
      "source": "[readme]",
      "quote": "<a href=\"https://github.com/Tencent-Hunyuan/Hunyuan-A13B\"><b>GITHUB</b></a>"
    },
    {
      "source": "[files]",
      "quote": "modeling_hunyuan.py"
    },
    {
      "source": "[py_files/modeling_hunyuan.py]",
      "quote": "\"\"\" PyTorch HunYuan model.\"\"\""
    }
  ],
  "1-3 (License)": "All artefacts are distributed under the “TENCENT HUNYUAN COMMUNITY LICENSE AGREEMENT”, referenced multiple times: “license_name: tencent-hunyuan-a13b”, “# Licensed under the TENCENT HUNYUAN COMMUNITY LICENSE AGREEMENT (the \"License\");”. Core terms quoted include: “We grant You … a non-exclusive, non-transferable and royalty-free limited license … to use, reproduce, distribute, create derivative works of (including Model Derivatives), and make modifications to the Materials, only in accordance with the terms of this Agreement and the Acceptable Use Policy.” The licence places explicit limitations: “You must not use the Tencent Hunyuan Works or any Output or results of the Tencent Hunyuan Works to improve any other AI model (other than Tencent Hunyuan or Model Derivatives thereof).” It is territorially restricted: “THIS LICENSE AGREEMENT DOES NOT APPLY IN THE EUROPEAN UNION, UNITED KINGDOM AND SOUTH KOREA AND IS EXPRESSLY LIMITED TO THE TERRITORY…”. It introduces a usage-scale clause: “If, on the Tencent Hunyuan version release date, the monthly active users of all products or services made available by or for Licensee is greater than 100 million … You must request a license from Tencent.” Trademark permissions are narrow: “Tencent hereby grants You a license to use ‘Tencent Hunyuan’ (the ‘Mark’) in the Territory solely as required…”. A direct link to the full text is provided (“https://github.com/Tencent/Tencent-Hunyuan-Large/blob/main/License.docx”) and an email contact (hunyuan_opensource@tencent.com) is supplied for licensing questions. Finally, the repository contains an explicit LICENSE file (“LICENSE file present: LICENSE”), ensuring that the legal terms accompany every download.",
  "1-3 (License)__evidence": [
    {
      "source": "[readme]",
      "quote": "license_name: tencent-hunyuan-a13b"
    },
    {
      "source": "[license_file]",
      "quote": "TENCENT HUNYUAN COMMUNITY LICENSE AGREEMENT"
    },
    {
      "source": "[license_file]",
      "quote": "We grant You, for the Territory only, a non-exclusive, non-transferable and royalty-free limited license under Tencent’s intellectual property or other rights owned by Us embodied in or utilized by the Materials to use, reproduce, distribute, create derivative works of (including Model Derivatives), and make modifications to the Materials, only in accordance with the terms of this Agreement and the Acceptable Use Policy"
    },
    {
      "source": "[license_file]",
      "quote": "You must not use the Tencent Hunyuan Works or any Output or results of the Tencent Hunyuan Works to improve any other AI model (other than Tencent Hunyuan or Model Derivatives thereof)."
    },
    {
      "source": "[license_file]",
      "quote": "THIS LICENSE AGREEMENT DOES NOT APPLY IN THE EUROPEAN UNION, UNITED KINGDOM AND SOUTH KOREA AND IS EXPRESSLY LIMITED TO THE TERRITORY, AS DEFINED BELOW."
    },
    {
      "source": "[license_file]",
      "quote": "If, on the Tencent Hunyuan version release date, the monthly active users of all products or services made available by or for Licensee is greater than 100 million monthly active users in the preceding calendar month, You must request a license from Tencent"
    },
    {
      "source": "[py_files/modeling_hunyuan.py]",
      "quote": "# Licensed under the TENCENT HUNYUAN COMMUNITY LICENSE AGREEMENT (the \"License\");\n# you may not use this file except in compliance with the License."
    },
    {
      "source": "[py_files/modeling_hunyuan.py]",
      "quote": "# You may obtain a copy of the License at\n#     https://github.com/Tencent/Tencent-Hunyuan-Large/blob/main/License.docx"
    },
    {
      "source": "[license_file]",
      "quote": "ia email (hunyuan_opensource@tencent.com).\n\n\n[license_file]\nTENCENT HUNYUAN COMMUNITY LICENSE AGREEMENT\r\nTencent Hunyuan A13B Release Date: June 27, 2025\r\nTHIS LICENSE AGREEMENT DOES NOT APPLY IN THE EUROPEAN UNION, UNITED KINGDOM AND SOUTH KOREA AND IS EXPRESSLY LIMITED TO THE TERRITORY, AS DEFINED BELOW.\r\nBy clicking to agree or by using, reproducing, modifying, distributing, performing or displaying any portion or element"
    },
    {
      "source": "[readme]",
      "quote": "ny of its affiliates, except as required for reasonable and customary use in describing and distributing the Tencent Hunyuan Works. Tencent hereby grants You a license to use “Tencent Hunyuan” (the “Mark”) in the Territory solely as required to comply with the provisions of Section 3(c), provided that You comply with any applicable laws related to trademark protection. All goodwill arising out of Your use o"
    },
    {
      "source": "[files]",
      "quote": "LICENSE file present: LICENSE"
    }
  ],
  "1-4 (Paper)": "The official technical documentation is supplied through a GitHub-hosted PDF: “<a href=\"https://github.com/Tencent-Hunyuan/Hunyuan-A13B/blob/main/report/Hunyuan_A13B_Technical_Report.pdf\"><b>Technical Report</b> </a>”. Although only the link is quoted, its placement in the repository indicates that a full technical report dedicated to the Hunyuan-A13B series exists, providing empirical results, architecture details, and design rationales for the same major version as the Instruct model.",
  "1-4 (Paper)__evidence": [
    {
      "source": "[readme]",
      "quote": "<a href=\"https://github.com/Tencent-Hunyuan/Hunyuan-A13B/blob/main/report/Hunyuan_A13B_Technical_Report.pdf\"><b>Technical Report</b> </a>"
    }
  ],
  "1-5 (Architecture)": "The official repository describes \"Hunyuan-A13B\" as \"an innovative and open-source large language model (LLM) built on a fine-grained Mixture-of-Experts (MoE) architecture.\"  It is further detailed that \"the newly introduced Hunyuan-A13B model features a total of 80 billion parameters with 13 billion active parameters,\" and that \"with only 13 billion active parameters (out of a total of 80 billion), the model delivers competitive performance on a wide range of benchmark tasks, rivaling much larger models.\"  For sequence processing it is stated that \"the Hunyuan A13B model supports a maximum context length of 256K tokens (262,144 tokens).\"  The code comments add that this is \"the bare HunYuan Model outputting raw hidden-states without any specific head on top,\" realized as a \"Transformer decoder consisting of *config.num_hidden_layers* layers. Each layer is a [`HunYuanDecoderLayer`].\"",
  "1-5 (Architecture)__evidence": [
    {
      "source": "[readme]",
      "quote": "Welcome to the official repository of **Hunyuan-A13B**, an innovative and open-source large language model (LLM) built on a fine-grained Mixture-of-Experts (MoE) architecture."
    },
    {
      "source": "[readme]",
      "quote": "The newly introduced Hunyuan-A13B model features a total of 80 billion parameters with 13 billion active parameters."
    },
    {
      "source": "[readme]",
      "quote": "With only 13 billion active parameters (out of a total of 80 billion), the model delivers competitive performance on a wide range of benchmark tasks, rivaling much larger models."
    },
    {
      "source": "[readme]",
      "quote": "The Hunyuan A13B model supports a maximum context length of **256K tokens (262,144 tokens)**."
    },
    {
      "source": "[py_files/modeling_hunyuan.py]",
      "quote": "The bare HunYuan Model outputting raw hidden-states without any specific head on top."
    },
    {
      "source": "[py_files/modeling_hunyuan.py]",
      "quote": "Transformer decoder consisting of *config.num_hidden_layers* layers. Each layer is a [`HunYuanDecoderLayer`]"
    }
  ],
  "1-6 (Tokenizer)": "Tokenizer configuration snippets specify a \"vocab_size\": 128 167 together with token identifiers \"bos_token_id\": 1, \"eos_token_id\": 127 960, and \"pad_token_id\": 127 961.  The positional capacity on the tokenizer side is indicated by \"max_position_embeddings\": 32 768.",
  "1-6 (Tokenizer)__evidence": [
    {
      "source": "[config]",
      "quote": "\"vocab_size\": 128167,"
    },
    {
      "source": "[config]",
      "quote": "\"bos_token_id\": 1,"
    },
    {
      "source": "[config]",
      "quote": "\"eos_token_id\": 127960,"
    },
    {
      "source": "[config]",
      "quote": "\"pad_token_id\": 127961,"
    },
    {
      "source": "[config]",
      "quote": "\"max_position_embeddings\": 32768,"
    }
  ],
  "2-1 (Hardware)": "For 256 K-context deployment, the documentation recommends systems \"equipped with NVIDIA H20 GPUs (96 GB VRAM).\"",
  "2-1 (Hardware)__evidence": [
    {
      "source": "[readme]",
      "quote": "The following configuration is recommended for deploying the model with 256K context length support on systems equipped with **NVIDIA H20 GPUs (96GB VRAM)**:"
    }
  ],
  "2-2 (Software)": "The maintainers supply containerized runtimes: \"We provide a pre-built Docker image based on the latest version of TensorRT-LLM,\" obtainable via the command \"docker pull hunyuaninfer/hunyuan-a13b:hunyuan-moe-A13B-trtllm.\"  They also note, \"We provide a pre-built Docker image containing vLLM 0.8.5 with full support for this model.\"",
  "2-2 (Software)__evidence": [
    {
      "source": "[readme]",
      "quote": "We provide a pre-built Docker image based on the latest version of TensorRT-LLM."
    },
    {
      "source": "[readme]",
      "quote": "docker pull hunyuaninfer/hunyuan-a13b:hunyuan-moe-A13B-trtllm"
    },
    {
      "source": "[readme]",
      "quote": "We provide a pre-built Docker image containing vLLM 0.8.5 with full support for this model."
    }
  ],
  "2-3 (API)": "The only explicit information about an end-user or developer-facing API for tencent/hunyuan-a13b-instruct appears in a single Docker-based launch example. The sentence shows that an OpenAI-compatible HTTP server can be stood up through the vLLM runtime by running a container image named “hunyuaninfer/hunyuan-infer-vllm-cuda12.4:v1”. The command passes the entry-point “python3 -m vllm.entrypoints.openai.api_server”, binds the server to 0.0.0.0, exposes it on port 8000, and enables 4-way tensor parallelism via “--tensor-parallel-size 4”. The model identifier fed to vLLM is exactly “tencent/Hunyuan-A13B-Instruct”, and the switch “--trust_remote_code” indicates that the implementation relies on the remote Hugging Face repository. The container is granted full GPU access (“--gpus=all”) and host networking (“--net=host”), with the seccomp profile disabled to avoid sandbox restrictions. A cache volume from the host’s ~/.cache directory is mounted into /root/.cache inside the container. Taken together, the quote confirms that tencent/hunyuan-a13b-instruct can be served through an OpenAI-style REST API using vLLM, that the maintainers provide an official CUDA 12.4 image, and that multi-GPU tensor parallelism is expected.",
  "2-3 (API)__evidence": [
    {
      "source": "[readme]",
      "quote": "docker run --rm  --ipc=host \\\n        -v ~/.cache:/root/.cache/ \\\n        --security-opt seccomp=unconfined \\\n        --net=host \\\n        --gpus=all \\\n        -it \\\n        --entrypoint python3 hunyuaninfer/hunyuan-infer-vllm-cuda12.4:v1 \\\n        -m vllm.entrypoints.openai.api_server \\\n        --host 0.0.0.0 \\\n        --tensor-parallel-size 4 \\\n        --port 8000 \\\n        --model tencent/Hunyuan-A13B-Instruct  \\\n        --trust_remote_code"
    }
  ],
  "3-1 (Pre-training)": "One sentence provides the entirety of public pre-training details. It states that on “2025.6.27” Tencent released a family of checkpoints on Hugging Face: “Hunyuan-A13B-Pretrain”, “Hunyuan-A13B-Instruct”, “Hunyuan-A13B-Instruct-FP8”, and “Hunyuan-A13B-Instruct-GPTQ-Int4”. While no hyperparameters, corpus descriptions, or training procedures are enumerated, the quote does establish that there is a dedicated pre-training checkpoint (“Hunyuan-A13B-Pretrain”) separate from instruction-tuned variants. Furthermore, multiple precision formats (full precision, FP8, and GPTQ 4-bit quantization) are available, implying that the base pre-training run was completed early enough to derive these post-processed artifacts. The open-sourcing date also anchors the release timeline of the pre-trained backbone relative to the instruction-tuned model.",
  "3-1 (Pre-training)__evidence": [
    {
      "source": "[readme]",
      "quote": "2025.6.27 We have open-sourced  **Hunyuan-A13B-Pretrain** , **Hunyuan-A13B-Instruct** , **Hunyuan-A13B-Instruct-FP8** , **Hunyuan-A13B-Instruct-GPTQ-Int4** on Hugging Face."
    }
  ],
  "3-2 (Fine-tuning)": "Fine-tuning information is conveyed through a legal definition extracted from Tencent’s license. The clause clarifies that “Tencent Hunyuan” encompasses all large language models and associated software, explicitly including fine-tuning-enabling code. It further cites “Tencent Hunyuan A13B released at https://github.com/Tencent-Hunyuan/Hunyuan-A13B” as part of that family. While procedural details are absent, the wording confirms (1) that fine-tuning support code is publicly distributed, (2) that the tencent/hunyuan-a13b-instruct weights belong to the licensable Hunyuan suite, and (3) that users are permitted to obtain and run fine-tuning pipelines hosted in the referenced GitHub repository.",
  "3-2 (Fine-tuning)__evidence": [
    {
      "source": "[license_file]",
      "quote": "j. “Tencent Hunyuan” shall mean the large language models, text/image/video/audio/3D generation models, and multimodal large language models and their software and algorithms, including trained model weights, parameters (including optimizer states), machine-learning model code, inference-enabling code, training-enabling code, fine-tuning enabling code and other elements of the foregoing made publicly available by Us, including, without limitation to, Tencent Hunyuan A13B released at [https://github.com/Tencent-Hunyuan/Hunyuan-A13B]."
    }
  ],
  "3-3 (Reinforcement Learning)": "",
  "3-3 (Reinforcement Learning)__evidence": [],
  "4-1 (Pre-training Data)": "",
  "4-1 (Pre-training Data)__evidence": [],
  "4-2 (Fine-tuning Data)": "",
  "4-2 (Fine-tuning Data)__evidence": [],
  "4-3 (Reinforcement Learning Data)": "",
  "4-3 (Reinforcement Learning Data)__evidence": [],
  "4-4 (Data Filtering)": "",
  "4-4 (Data Filtering)__evidence": [],
  "__usage": {
    "fine_tuning": "unknown",
    "rl": "unknown"
  }
}