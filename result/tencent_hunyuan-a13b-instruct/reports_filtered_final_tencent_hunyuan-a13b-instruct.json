{
  "1-1 (Weights)": "The authors state explicitly that \"We release Hunyuan-A13B to the open-source community\" and describe the model as \"an efficient and open-source large language model built upon the MoE architecture.\" The same claim is reiterated when they note that it is being released \"to facilitate efficient, research-driven practical applications\" and highlight that the model \"exhibits superior inference throughput\" for latency-sensitive uses. Although the excerpts do not give a download URL, hosting platform, or access procedure, the wording confirms an intention to publish the full model weights without restriction. A related citation—\"Hunyuan-large: An open-source MoE model with 52 billion activated parameters\"—shows a precedent of open weight release within the Hunyuan family, reinforcing the expectation that Hunyuan-A13B weights are likewise publicly available.",
  "1-2 (Code)": "None of the supplied sentences declare that Hunyuan-A13B’s training pipeline (data preparation, configuration files, pre-training scripts, fine-tuning or RLHF code) has been released. The only concrete code artefact mentioned is the GitHub repository for \"PenguinScrolls: A user-aligned fine-grained benchmark for long-context language model evaluation\" (URL: https://github.com/Penguin-Scrolls/PenguinScrolls), which pertains to evaluation rather than training. An arXiv preprint describing \"Hunyuan-large\" is branded \"open-source,\" implying some implementation material exists for that sibling model, but no link or repository is provided here. On the basis of the available quotations, we can confirm public availability of an evaluation benchmark but have no evidence that the end-to-end training code for Hunyuan-A13B has been made public.",
  "1-3 (License)": "The quote set contains no references to a licence name (e.g., Apache-2.0, MIT), no usage clauses such as \"non-commercial\" or \"research only,\" and no statements about redistribution or derivative works. Accordingly, no licensing information for Hunyuan-A13B or its assets can be extracted from the provided material.",
  "1-4 (Paper)": "Multiple written resources are cited. The cornerstone is the \"2025-06-26 Hunyuan-A13B Technical Report\" issued by the Tencent Hunyuan Team. One sentence notes: \"This paper presents Hunyuan-A13B, an efficient and open-source large language model built upon the MoE architecture,\" indicating that a standalone research paper accompanies the model. In the broader Hunyuan ecosystem, two further arXiv preprints are referenced: \"Hunyuan-turbos\" (arXiv:2505.15431, 2025) and \"Hunyuan-large\" (arXiv:2411.02265, 2024), both of which document related large-scale MoE models from Tencent. Finally, the team publicises an evaluation resource, \"PenguinScrolls\" (2024, GitHub URL supplied), which, while not a model paper, forms part of the official documentation set. Collectively, these documents supply technical details, architectural insights and evaluation methodologies for Hunyuan-A13B and its companion models.",
  "1-1 (Weights)__evidence": [
    {
      "source": "[pdf_text]",
      "quote": "We release Hunyuan-A13B to the open-source community to encourage ongoing advancements in LLMs and to facilitate efficient, research-driven practical applications."
    },
    {
      "source": "[pdf_text]",
      "quote": "This paper presents Hunyuan-A13B, an efficient and open-source large language model built upon the MoE architecture."
    },
    {
      "source": "[pdf_text]",
      "quote": "Notably, our model exhibits superior inference throughput, making it particularly suitable for latency-sensitive applications. We release Hunyuan-A13B to the open-source community to encourage ongoing advancements in LLMs and to facilitate efficient, research-driven practical applications."
    },
    {
      "source": "[pdf_text]",
      "quote": "Xingwu Sun, Yanfeng Chen, Yiqing Huang, Ruobing Xie, Jiaqi Zhu, Kai Zhang, Shuaipeng Li, Zhen Yang,\nJonny Han, Xiaobo Shu, et al. Hunyuan-large: An open-source moe model with 52 billion activated\nparameters by tencent. arXiv preprint arXiv:2411.02265, 2024."
    }
  ],
  "1-2 (Code)__evidence": [
    {
      "source": "[pdf_text]",
      "quote": "Tencent Hunyuan. PenguinScrolls: A user-aligned fine-grained benchmark for long-context language\nmodel evaluation, 2024. URL https://github.com/Penguin-Scrolls/PenguinScrolls."
    },
    {
      "source": "[pdf_text]",
      "quote": "Xingwu Sun, Yanfeng Chen, Yiqing Huang, Ruobing Xie, Jiaqi Zhu, Kai Zhang, Shuaipeng Li, Zhen Yang,\nJonny Han, Xiaobo Shu, et al. Hunyuan-large: An open-source moe model with 52 billion activated\nparameters by tencent. arXiv preprint arXiv:2411.02265, 2024."
    }
  ],
  "1-3 (License)__evidence": [],
  "1-4 (Paper)__evidence": [
    {
      "source": "[pdf_text]",
      "quote": "2025-06-26 Hunyuan-A13B Technical Report Tencent Hunyuan Team"
    },
    {
      "source": "[pdf_text]",
      "quote": "This paper presents Hunyuan-A13B, an efficient and open-source large language model built upon the MoE architecture."
    },
    {
      "source": "[pdf_text]",
      "quote": "Ao Liu, Botong Zhou, Can Xu, Chayse Zhou, ChenChen Zhang, Chengcheng Xu, Chenhao Wang,\nDecheng Wu, Dengpeng Wu, Dian Jiao, et al. Hunyuan-turbos: Advancing large language models\nthrough mamba-transformer synergy and adaptive chain-of-thought. arXiv preprint arXiv:2505.15431,\n2025."
    },
    {
      "source": "[pdf_text]",
      "quote": "Xingwu Sun, Yanfeng Chen, Yiqing Huang, Ruobing Xie, Jiaqi Zhu, Kai Zhang, Shuaipeng Li, Zhen Yang,\nJonny Han, Xiaobo Shu, et al. Hunyuan-large: An open-source moe model with 52 billion activated\nparameters by tencent. arXiv preprint arXiv:2411.02265, 2024."
    },
    {
      "source": "[pdf_text]",
      "quote": "Tencent Hunyuan. PenguinScrolls: A user-aligned fine-grained benchmark for long-context language\nmodel evaluation, 2024. URL https://github.com/Penguin-Scrolls/PenguinScrolls."
    }
  ],
  "1-5 (Architecture)": "According to the provided descriptions, Hunyuan-A13B is an open-source large language model that “employs a Mixture-of-Experts (MoE) architecture that optimizes the trade-off between computational efficiency and model performance.”  Concretely, it is “designed with a Sparse Mixture-of-Experts architecture comprising 80 billion total parameters, yet activating only 13 billion parameters per input.”  The implementation is “fine-grained”: it contains “1 shared expert and 64 fine-grained non-shared experts, all operating with identical intermediate dimension.”  During training, “the shared expert remains perpetually active, while only 8 non-shared experts are activated simultaneously,” yielding “13 billion active parameters within a total parameter count of 80 billion.”  Beyond the MoE layout, the model “incorporates Grouped-Query Attention (GQA … 2023) in its attention layers to enhance KV Cache memory efficiency.”  (For context on the broader family, the citation list also references “Hunyuan-large … with 52 billion activated parameters” and “Hunyuan-turbos …,” but no additional architectural numbers are given for A13B.)",
  "1-6 (Tokenizer)": "The tokenizer that ships with Hunyuan-A13B is explicitly stated to be “the same as Hunyuan-Large … with a vocabulary size of 128 K.”  No further structural or availability details are provided in the quoted material.",
  "2-1 (Hardware)": "",
  "2-2 (Software)": "For training, the authors state: “In the pre-training stage, we first train Hunyuan-A13B on a high-quality dataset consisting of more than 20 T tokens. Subsequently, we process a fast annealing stage to enhance its overall performance and a long-context stage to scale the model’s context window to 256 K.”  The quotes do not mention the underlying ML framework, optimizer, distributed-training libraries, or any other software components beyond these three curriculum phases.",
  "1-5 (Architecture)__evidence": [
    {
      "source": "[pdf_text]",
      "quote": "Hunyuan-A13B is designed with a Sparse Mixture-of-Experts architecture comprising 80 billion total parameters, yet activating only 13 billion parameters per input."
    },
    {
      "source": "[pdf_text]",
      "quote": "The Hunyuan-A13B model employs a fine-grained MoE architecture (Dai et al., 2024). Specifically, it consists of 1 shared expert and 64 fine-grained non-shared experts, all operating with identical intermediate dimension."
    },
    {
      "source": "[sections/https://raw.githubusercontent.com/Tencent-Hunyuan/Hunyuan-A13B/main/report/Hunyuan_A13B_Technical_Report.pdf]",
      "quote": "We present Hunyuan-A13B, an open-source large language model employing a Mixture-of-Experts (MoE) architecture that optimizes the trade-off between computational efficiency and model performance."
    },
    {
      "source": "[sections/https://raw.githubusercontent.com/Tencent-Hunyuan/Hunyuan-A13B/main/report/Hunyuan_A13B_Technical_Report.pdf]",
      "quote": "The Hunyuan-A13B model employs a fine-grained MoE architecture (Dai et al., 2024). Specifically, it consists of 1 shared expert and 64 fine-grained non-shared experts, all operating with identical intermediate dimension. During the training stage, the shared expert remains perpetually active, while only 8 non-shared experts are activated simultaneously. The model features 13 billion active parameters within a total parameter count of 80 billion."
    },
    {
      "source": "[sections/https://raw.githubusercontent.com/Tencent-Hunyuan/Hunyuan-A13B/main/report/Hunyuan_A13B_Technical_Report.pdf]",
      "quote": "Hunyuan-A13B incorporates Grouped-Query Attention (GQA, Ainslie et al., 2023) in its attention layers to enhance KV Cache memory efficiency."
    },
    {
      "source": "[pdf_text]",
      "quote": "Xingwu Sun, Yanfeng Chen, Yiqing Huang, Ruobing Xie, Jiaqi Zhu, Kai Zhang, Shuaipeng Li, Zhen Yang, Jonny Han, Xiaobo Shu, et al. Hunyuan-large: An open-source moe model with 52 billion activated parameters by tencent. arXiv preprint arXiv:2411.02265, 2024."
    },
    {
      "source": "[pdf_text]",
      "quote": "Ao Liu, Botong Zhou, Can Xu, Chayse Zhou, ChenChen Zhang, Chengcheng Xu, Chenhao Wang, Decheng Wu, Dengpeng Wu, Dian Jiao, et al. Hunyuan-turbos: Advancing large language models through mamba-transformer synergy and adaptive chain-of-thought. arXiv preprint arXiv:2505.15431, 2025."
    }
  ],
  "1-6 (Tokenizer)__evidence": [
    {
      "source": "[pdf_text]",
      "quote": "The tokenizer of Hunyuan-A13B is the same as Hunyuan-Large (Sun et al., 2024), with a vocabulary size of 128K."
    },
    {
      "source": "[sections/https://raw.githubusercontent.com/Tencent-Hunyuan/Hunyuan-A13B/main/report/Hunyuan_A13B_Technical_Report.pdf]",
      "quote": "The tokenizer of Hunyuan-A13B is the same as Hunyuan-Large (Sun et al., 2024), with a vocabulary size of 128K."
    }
  ],
  "2-1 (Hardware)__evidence": [],
  "2-2 (Software)__evidence": [
    {
      "source": "[sections/https://raw.githubusercontent.com/Tencent-Hunyuan/Hunyuan-A13B/main/report/Hunyuan_A13B_Technical_Report.pdf]",
      "quote": "In the pre-training stage, we first train Hunyuan-A13B on a high-quality dataset consisting of more than 20T tokens. Subsequently, we process a fast annealing stage to enhance its overall performance and a long-context stage to scale the model’s context window to 256K."
    }
  ],
  "2-3 (API)": "The material explicitly states that Hunyuan-A13B has been released to the open-source community, which signals that the model weights, code, or both are publicly accessible rather than restricted to an internal research preview. Although the excerpts do not describe a cloud-hosted REST or chat API in the style of GPT or Gemini, they do clarify how the model can be programmatically consumed: Hunyuan-A13B \"integrates seamlessly with mainstream open-source inference frameworks, including vLLM, SGLang, and TensorRT-LLM.\" These tool names imply that users can load the checkpoint in those libraries and expose their own endpoints or pipelines with \"straightforward one-click deployment for W16A16 precision inference.\" Hence, developers gain practical, code-level access through widely adopted inference stacks rather than a proprietary HTTP interface. The public release combined with plug-and-play compatibility effectively constitutes an accessible, community-oriented API surface, supported by documentation and examples native to vLLM, SGLang, and TensorRT-LLM ecosystems.",
  "3-1 (Pre-training)": "Pre-training for Hunyuan-A13B follows a large-scale, multi-phase regimen. The authors state that they \"first train Hunyuan-A13B on [a] high-quality dataset consisting of more than 20T tokens,\" indicating an enormous corpus size well above the common trillion-token scale. They further break the process into \"three sequential stages,\" of which the \"Foundation Training Stage\" alone processes \"a total of 20T tokens.\" The engineering backbone is a \"fine-grained MoE architecture\" containing \"1 shared expert and 64 fine-grained non-shared experts, all operating with identical intermediate dimension.\" This configuration, together with GQA (mentioned elsewhere for inference), underpins efficient scaling while preserving capacity. Domain-specific augmentation is highlighted by the extraction of \"250 billion tokens of high-quality STEM pre-training corpus\" that is folded into the broader training mix. Although the citation list also references related Hunyuan variants such as \"Hunyuan-turbos\" (arXiv:2505.15431) and \"Hunyuan-large\" (arXiv:2411.02265), the quotes focus on the A13B model’s own data volume, staged curriculum, and MoE topology, giving a concrete picture of its pre-training methodology and hyper-scale data pipeline.",
  "3-2 (Fine-tuning)": "After pre-training, the team executes a \"structured, multi-stage\" post-training pipeline. The first step is \"supervised fine-tuning (SFT)\" that leverages \"high-quality data\" and concentrates specifically on \"reasoning tasks,\" suggesting curated instruction or task-oriented datasets. This supervised pass aligns the model with downstream expectations before any reinforcement learning is applied. The approach is expressly comparative: it demonstrates that Hunyuan-A13B \"shows superior performance when compared to other representative MoE or Dense model[s]\" of similar or larger scale, implying that the fine-tuning regimen materially boosts real-world metrics. The authors indicate that these SFT efforts are part of a larger post-training suite that ultimately \"comprehensively [enhances] the model’s performance in all dimensions,\" laying the groundwork for the subsequent RL stage.",
  "3-3 (Reinforcement Learning)": "Reinforcement learning forms the second pillar of the post-training workflow. Following SFT, the team embarks on \"large-scale reinforcement learning (RL), systematically enhancing reasoning capabilities through iterative optimization.\" They describe this as \"targeted RL optimization\" that is explicitly designed to \"enhance reasoning capabilities,\" implying task-reward shaping or preference-based fine-tuning that iteratively refines the model. The text underscores that RL is not a token afterthought but \"comprehensively [enhances] the model’s performance in all dimensions,\" integrating with earlier SFT to form a unified post-training regime. Although hyperparameters, reward models, or algorithmic variants (e.g., PPO, DPO) are not enumerated in the snippets, the repeated emphasis on \"large-scale\" and \"iterative optimization\" conveys that the RLHF-style phase is extensive and central to elevating Hunyuan-A13B’s final reasoning proficiency.",
  "2-3 (API)__evidence": [
    {
      "source": "[pdf_text]",
      "quote": "We release Hunyuan-A13B to the open-source community to encourage ongoing advancements in LLMs and to facilitate efficient, research-driven practical applications."
    },
    {
      "source": "[pdf_text]",
      "quote": "Hunyuan-A13B employs GQA and MoE architectures to markedly enhance reasoning efficiency without sacrificing model performance. It integrates seamlessly with mainstream open-source inference frameworks, including vLLM, SGLang, and TensorRT-LLM, enabling straightforward one-click deployment for W16A16 precision inference."
    }
  ],
  "3-1 (Pre-training)__evidence": [
    {
      "source": "[pdf_text]",
      "quote": "In the pre-training stage, we first train Hunyuan-A13B on high-quality dataset consisting of more than 20T tokens."
    },
    {
      "source": "[pdf_text]",
      "quote": "The training process of Hunyuan-A13B consisted of three sequential stages. Foundation Training Stage: This stage processed a total of 20T tokens."
    },
    {
      "source": "[pdf_text]",
      "quote": "Hunyuan-A13B employs a fine-grained MoE architecture (Dai et al., 2024). Specifically, it consists of 1 shared expert and 64 fine-grained non-shared experts, all operating with identical intermediate dimension."
    },
    {
      "source": "[pdf_text]",
      "quote": "We successfully extract 250 billion tokens of high-quality STEM pre-training corpus, which is incorporated into the training of Hunyuan-A13B."
    },
    {
      "source": "[pdf_text]",
      "quote": "The Hunyuan-A13B model employs a fine-grained MoE architecture (Dai et al., 2024). Specifically, it consists of 1 shared expert and 64 fine-grained non-shared experts, all operating with identical intermediate dimension."
    },
    {
      "source": "[pdf_text]",
      "quote": "Ao Liu, Botong Zhou, Can Xu, Chayse Zhou, ChenChen Zhang, Chengcheng Xu, Chenhao Wang, Decheng Wu, Dengpeng Wu, Dian Jiao, et al. Hunyuan-turbos: Advancing large language models through mamba-transformer synergy and adaptive chain-of-thought. arXiv preprint arXiv:2505.15431, 2025."
    },
    {
      "source": "[pdf_text]",
      "quote": "Xingwu Sun, Yanfeng Chen, Yiqing Huang, Ruobing Xie, Jiaqi Zhu, Kai Zhang, Shuaipeng Li, Zhen Yang, Jonny Han, Xiaobo Shu, et al. Hunyuan-large: An open-source moe model with 52 billion activated parameters by tencent. arXiv preprint arXiv:2411.02265, 2024."
    }
  ],
  "3-2 (Fine-tuning)__evidence": [
    {
      "source": "[pdf_text]",
      "quote": "Hunyuan-A13B shows superior performance when compared to other representative MoE or Dense model with comparable or larger activated or total parameter sizes. The post-training procedure involved a structured, multi-stage approach. Initially, we conducted supervised fine-tuning focused on reasoning tasks, followed by targeted RL optimization to further enhance reasoning capabilities."
    },
    {
      "source": "[pdf_text]",
      "quote": "In the post-training stage, Hunyuan-A13B utilizes high-quality data for supervised fine-tuning (SFT) and large-scale reinforcement learning, comprehensively enhancing the model’s performance in all dimensions."
    }
  ],
  "3-3 (Reinforcement Learning)__evidence": [
    {
      "source": "[pdf_text]",
      "quote": "Hunyuan-A13B incorporates several innovative elements that collectively enhance its reasoning performance, flexibility, and inference efficiency. Subsequently, we conducted large-scale reinforcement learning (RL), systematically enhancing reasoning capabilities through iterative optimization."
    },
    {
      "source": "[pdf_text]",
      "quote": "In the post-training stage, Hunyuan-A13B utilizes high-quality data for supervised fine-tuning (SFT) and large-scale reinforcement learning, comprehensively enhancing the model’s performance in all dimensions."
    }
  ],
  "4-1 (Pre-training Data)": "According to the provided statements, the pre-training of Hunyuan-A13B relies on a very large, carefully screened corpus. Two separate descriptions fix the overall scale at “more than 20 T tokens” (sometimes rounded to “20 T”); this figure represents the total volume of raw text that survived all upstream filtering steps before the model’s initial training began. Within that overall pool, STEM (science, technology, engineering, mathematics) material received special emphasis. The team explicitly “enhance[d] the STEM data acquisition and cleaning processes,” and—after the full pipeline of deduplication, denoising, topic labeling and semantic‐level checks—“successfully extract[ed] 250 billion tokens of high-quality STEM pre-training corpus,” which were then folded into the larger 20 T-token mix. Although the individual data sources are not enumerated, the repeated references to a “high-quality dataset,” “plain-text extraction,” and reuse of the Hunyuan-TurboS curation pipeline imply heterogeneous web-scale text, segmented, labeled, and quality-screened by automated modules. No restrictive licensing or usage limits are mentioned, suggesting the data is either public-domain or under terms that allow research and commercial use once it passes the internal quality gates. Overall, the composition can be summarized as: (1) a general-domain portion comprising the bulk of the >20 T tokens, already deduplicated and denoised; and (2) a strategically enlarged, 250 B-token STEM subset, curated for factual rigor and reasoning coverage, all of which is finally merged into the training mixture that seeds the Hunyuan-A13B weights.",
  "4-2 (Fine-tuning Data)": "Fine-tuning for Hunyuan-A13B is carried out in a “structured, multi-stage” post-training pipeline that starts with supervised fine-tuning (SFT) and is later augmented by reinforcement learning. During the SFT phase, the authors gathered “high-quality, long-CoT” (chain-of-thought) datasets explicitly aimed at strengthening step-by-step logic and complex reasoning. The supervision set therefore appears to be dominated by reasoning-oriented question-answer pairs or task-specific demonstrations where extended rationale chains are provided as targets. No numeric token count is disclosed for this stage, but the phrasing “high-quality” and “long-CoT” signal that the examples are carefully filtered for correctness and explanatory depth rather than raw volume. The data is framed as proprietary or internally collected (the quotes do not indicate a public release), and it is processed in successive sub-phases so that the model first learns from supervised demonstrations and later transitions to other optimization regimes. In total, the fine-tuning data serves to “significantly boost the model’s logic and complex reasoning performance,” aligning Hunyuan-A13B with specialized reasoning benchmarks while keeping the vocabulary and style inherited from the much larger pre-training mix.",
  "4-3 (Reinforcement Learning Data)": "After supervised fine-tuning, the team performs “large-scale reinforcement learning” intended to “systematically enhance reasoning capabilities through iterative optimization.” Although the exact sources and formats of RL data are not itemized, the wording indicates that feedback-based data—likely in the form of synthetic dialogues, preference pairs, or reward signals generated from model interactions—forms the backbone of this phase. Because the RL step directly follows the SFT stage inside the same “post-training” umbrella, we can infer that the initial prompts are either sampled from, or stylistically consistent with, the supervised fine-tuning corpora. The phrase “high-quality data” is used again here, emphasizing that the prompts, responses, and reward signals were screened to maintain factuality and transparency. The overarching goal is to refine Hunyuan-A13B’s reasoning depth, flexibility, and response safety through repeated policy-gradient or similar reinforcement loops, thereby “comprehensively enhancing the model’s performance in all dimensions.”",
  "4-4 (Data Filtering)": "Data filtering for Hunyuan-A13B is described as both rigorous and multi-layered, inheriting and then improving on the three-module pipeline originally published for Hunyuan-TurboS. The pipeline operates in three primary stages: (1) a Data-Preprocessing module that performs automatic deduplication, low-quality filtering, denoising, and topic labeling; (2) a Model-Based Extraction module that extracts plain text from the preprocessed material, presumably discarding non-text or malformed segments; and (3) a Post-Processing module that repeats low-quality filtering and adds \"semantic-level deduplication\" to remove near-duplicate passages with different surface forms. For Hunyuan-A13B specifically, the developers “optimize the sub-modules within this pipeline,” placing extra focus on STEM-related acquisition and cleaning so that the resulting corpus is more reliable on science and mathematics topics. The end product is a “rigorously filtered 20 T token corpus,” within which 250 B tokens constitute a specialized, high-quality STEM subset. Although the quotes do not list numerical thresholds such as perplexity cut-offs or Jaccard distances, the explicit mention of deduplication passes, low-quality screens, denoising routines, topic labeling, and semantic-level checks makes clear that data is admitted only after passing multiple automated gates at different stages of the pipeline. This multi-stage filtration is credited with “significantly improv[ing] factual reliability and reasoning abilities,” tying the cleanliness of the corpus directly to the observed performance benefits.",
  "4-1 (Pre-training Data)__evidence": [
    {
      "source": "[pdf_text]",
      "quote": "Specifically, Hunyuan-A13B is pretrained on a rigorously filtered 20T token corpus with enhanced STEM-focused data curation, which significantly improves its factual reliability and reasoning abilities."
    },
    {
      "source": "[pdf_text]",
      "quote": "In the pre-training stage, we first train Hunyuan-A13B on high-quality dataset consisting of more than 20T tokens."
    },
    {
      "source": "[pdf_text]",
      "quote": "As a result, we successfully extract 250 billion tokens of high-quality STEM pre-training corpus, which is incorporated into the training of Hunyuan-A13B."
    },
    {
      "source": "[pdf_text]",
      "quote": "For the pre-training data processing of Hunyuan-A13B, we optimize the sub-modules within this pipeline. Specifically, we enhance the STEM data acquisition and cleaning processes, significantly improving the quality of STEM-related data. As a result, we successfully extract 250 billion tokens of high-quality STEM pre-training corpus, which is incorporated into the training of Hunyuan-A13B."
    },
    {
      "source": "[pdf_text]",
      "quote": "We reuse the data curation pipeline of Hunyuan-TurboS (Liu et al., 2025), which consists of the following modules to obtain high-quality pre-training corpus: (1) Data preprocessing module, which completes data deduplication, low-quality filtering, data denoising and data topic labeling. (2) Model-based extraction module, extracting plain text from data processed by previous modules. (3) Post-processing module, which performs low-quality filtering and semantic level deduplication on the extracted corpus."
    }
  ],
  "4-2 (Fine-tuning Data)__evidence": [
    {
      "source": "[pdf_text]",
      "quote": "Hunyuan-A13B shows superior performance when compared to other representative MoE or Dense model with comparable or larger activated or total parameter sizes. The post-training procedure involved a structured, multi-stage approach. Initially, we conducted supervised fine-tuning focused on reasoning tasks, followed by targeted RL optimization to further enhance reasoning capabilities."
    },
    {
      "source": "[pdf_text]",
      "quote": "Hunyuan-A13B incorporates several innovative elements that collectively enhance its reasoning performance, flexibility, and inference efficiency. Second, we collected and utilized high-quality, long-CoT SFT data to significantly boost the model’s logic and complex reasoning performance."
    },
    {
      "source": "[pdf_text]",
      "quote": "In the post-training stage, Hunyuan-A13B utilizes high-quality data for supervised fine-tuning (SFT) and large-scale reinforcement learning, comprehensively enhancing the model’s performance in all dimensions."
    }
  ],
  "4-3 (Reinforcement Learning Data)__evidence": [
    {
      "source": "[pdf_text]",
      "quote": "Hunyuan-A13B incorporates several innovative elements that collectively enhance its reasoning performance, flexibility, and inference efficiency. Subsequently, we conducted large-scale reinforcement learning (RL), systematically enhancing reasoning capabilities through iterative optimization."
    },
    {
      "source": "[pdf_text]",
      "quote": "In the post-training stage, Hunyuan-A13B utilizes high-quality data for supervised fine-tuning (SFT) and large-scale reinforcement learning, comprehensively enhancing the model’s performance in all dimensions."
    }
  ],
  "4-4 (Data Filtering)__evidence": [
    {
      "source": "[pdf_text]",
      "quote": "Specifically, Hunyuan-A13B is pretrained on a rigorously filtered 20T token corpus with enhanced STEM-focused data curation, which significantly improves its factual reliability and reasoning abilities."
    },
    {
      "source": "[pdf_text]",
      "quote": "We reuse the data curation pipeline of Hunyuan-TurboS (Liu et al., 2025), which consists of the following modules to obtain high-quality pre-training corpus: (1) Data preprocessing module, which completes data deduplication, low-quality filtering, data denoising and data topic labeling. (2) Model-based extraction module, extracting plain text from data processed by previous modules. (3) Post-processing module, which performs low-quality filtering and semantic level deduplication on the extracted corpus. For the pre-training data processing of Hunyuan-A13B, we optimize the sub-modules within this pipeline."
    },
    {
      "source": "[pdf_text]",
      "quote": "We reuse the data curation pipeline of Hunyuan-TurboS (Liu et al., 2025), which consists of the following modules to obtain high-quality pre-training corpus: (1) Data preprocessing module, which completes data deduplication, low-quality filtering, data denoising and data topic labeling. (2) Model-based extraction module, extracting plain text from data processed by previous modules. (3) Post-processing module, which performs low-quality filtering and semantic level deduplication on the extracted corpus."
    },
    {
      "source": "[pdf_text]",
      "quote": "For the pre-training data processing of Hunyuan-A13B, we optimize the sub-modules within this pipeline. Specifically, we enhance the STEM data acquisition and cleaning processes, significantly improving the quality of STEM-related data."
    }
  ],
  "__usage": {
    "fine_tuning": "used",
    "rl": "used"
  }
}