{
  "1-1 (Weights)": "The project states that its model weights are publicly released and downloadable. A dated bullet point says “2025.6.27 We have open-sourced  **Hunyuan-A13B-Pretrain** , **Hunyuan-A13B-Instruct** , **Hunyuan-A13B-Instruct-FP8** , **Hunyuan-A13B-Instruct-GPTQ-Int4** on Hugging Face.”  This makes clear that both the base pre-training checkpoint and several instruction-tuned or compressed variants (FP8 and GPTQ-Int4) are available.  A separate hyperlink “<a href=\"https://huggingface.co/tencent/Hunyuan-A13B-Instruct\"><b>Hugging Face</b></a>” indicates that the official hosting location is the Tencent organization page on Hugging Face.  Another sentence confirms direct, unauthenticated download of a quantized version: “You can also directly download our quantization completed open source model to use [Hunyuan-A13B-Instruct-FP8](https://huggingface.co/tencent/Hunyuan-A13B-Instruct-FP8).”  Therefore anyone can fetch full-precision and multiple quantized weights (FP8 and 4-bit GPTQ) from Hugging Face as of 27 June 2025, without mention of paywalls or request forms.",
  "1-1 (Weights)__evidence": [
    {
      "source": "[readme]",
      "quote": "* 2025.6.27 We have open-sourced  **Hunyuan-A13B-Pretrain** , **Hunyuan-A13B-Instruct** , **Hunyuan-A13B-Instruct-FP8** , **Hunyuan-A13B-Instruct-GPTQ-Int4** on Hugging Face."
    },
    {
      "source": "[readme]",
      "quote": "<a href=\"https://huggingface.co/tencent/Hunyuan-A13B-Instruct\"><b>Hugging Face</b></a>"
    },
    {
      "source": "[readme]",
      "quote": "You can also directly download our quantization completed open source model to use [Hunyuan-A13B-Instruct-FP8](https://huggingface.co/tencent/Hunyuan-A13B-Instruct-FP8)."
    }
  ],
  "1-2 (Code)": "Training-related source code is explicitly provided.  One line says “Hunyuan-A13B provides processes related to model training. Please refer to [Training](train/README.md) for model training purposes.”  This shows that scripts, configs, or instructions for running the training pipeline (not just inference) are present in the repository.  Internally, modular Python files are referenced: the repository contains “models.modeling_hunyuan” with classes “HunYuanMoEV1ForCausalLM, HunYuanMoE”, and a config class in “models.configuration_hunyuan” (“HunYuanConfig”).  The snippets “\"\"\" PyTorch HunYuan model.\"\"\"” (appearing twice) and “\"\"\"hunyuan tokenizer.\"\"\"” confirm that both model architecture and tokenizer implementations are in PyTorch.  A configuration constant shows “model_type='hunyuan',” indicating that the code registers a custom model type with the Transformers ecosystem.  Together these quotes demonstrate that end-to-end training components—configuration objects, model/optimizer definitions, tokenizer, and a dedicated Training README—are public, going beyond inference-only wrappers.",
  "1-2 (Code)__evidence": [
    {
      "source": "[readme]",
      "quote": "Hunyuan-A13B provides processes related to model training. Please refer to [Training](train/README.md) for model training purposes."
    },
    {
      "source": "[py_files/models/configuration_hunyuan.py]",
      "quote": "This is the configuration class to store the configuration of a [`HunYuanModel`]. It is used to instantiate an HunYuan model according to the specified arguments, defining the model architecture."
    },
    {
      "source": "[py_files/models/hunyuan.py]",
      "quote": "\"\"\" PyTorch HunYuan model.\"\"\""
    },
    {
      "source": "[py_files/models/modeling_hunyuan.py]",
      "quote": "\"\"\" PyTorch HunYuan model.\"\"\""
    },
    {
      "source": "[py_files/tokenization_hy.py]",
      "quote": "\"\"\"hunyuan tokenizer.\"\"\""
    },
    {
      "source": "[py_files/train/train.py]",
      "quote": "from models.modeling_hunyuan import HunYuanMoEV1ForCausalLM, HunYuanMoE"
    },
    {
      "source": "[py_files/train/train.py]",
      "quote": "from models.configuration_hunyuan import HunYuanConfig"
    },
    {
      "source": "[py_files/train/train.py]",
      "quote": "model_type='hunyuan',"
    }
  ],
  "1-3 (License)": "The repository is primarily distributed under the “TENCENT HUNYUAN COMMUNITY LICENSE AGREEMENT.”  The header records the specific edition: “Tencent Hunyuan A13B Release Date: June 27, 2025.”  The grant of rights is spelled out: “We grant You, for the Territory only, a non-exclusive, non-transferable and royalty-free limited license … to use, reproduce, distribute, create derivative works of (including Model Derivatives), and make modifications to the Materials, only in accordance with the terms of this Agreement and the Acceptable Use Policy.”  Geographic restrictions follow: “You must not use, reproduce, modify, distribute, or display the Tencent Hunyuan Works, Output or results of the Tencent Hunyuan Works outside the Territory.”  Additional volume-based gating is specified: “If, on the Tencent Hunyuan version release date, the monthly active users of all products or services made available by or for Licensee is greater than 100 million monthly active users in the preceding calendar month, You must request a license from Tencent.”  Source files embed the notice “# Licensed under the TENCENT HUNYUAN COMMUNITY LICENSE AGREEMENT (the \"License\");” and provide the download link “#     https://github.com/Tencent/Tencent-Hunyuan-Large/blob/main/License.docx”.  Some individual files also carry the standard permissive header “# Licensed under the Apache License, Version 2.0 (the \"License\");  #     http://www.apache.org/licenses/LICENSE-2.0”, implying that certain components (likely code utilities) are dual-licensed or separately licensed under Apache-2.0 while the model weights and other ‘Materials’ remain under the Tencent Hunyuan Community License.  Consequently:  (a) use and modification are allowed only within the stated Territory and AUP; (b) redistribution and derivative works are permitted but subject to the same agreement; (c) very high-scale deployments (>100 M MAU) need a separate Tencent license; and (d) no explicit blanket ban on commercial use is present, but usage outside the Territory or exceeding the user-count threshold is restricted.",
  "1-3 (License)__evidence": [
    {
      "source": "[license_files]",
      "quote": "TENCENT HUNYUAN COMMUNITY LICENSE AGREEMENT"
    },
    {
      "source": "[license_files]",
      "quote": "Tencent Hunyuan A13B Release Date: June 27, 2025"
    },
    {
      "source": "[license_files]",
      "quote": "We grant You, for the Territory only, a non-exclusive, non-transferable and royalty-free limited license under Tencent’s intellectual property or other rights owned by Us embodied in or utilized by the Materials to use, reproduce, distribute, create derivative works of (including Model Derivatives), and make modifications to the Materials, only in accordance with the terms of this Agreement and the Acceptable Use Policy"
    },
    {
      "source": "[license_files]",
      "quote": "You must not use, reproduce, modify, distribute, or display the Tencent Hunyuan Works, Output or results of the Tencent Hunyuan Works outside the Territory."
    },
    {
      "source": "[license_files]",
      "quote": "If, on the Tencent Hunyuan version release date, the monthly active users of all products or services made available by or for Licensee is greater than 100 million monthly active users in the preceding calendar month, You must request a license from Tencent"
    },
    {
      "source": "[py_files/models/modeling_hunyuan.py]",
      "quote": "# Licensed under the TENCENT HUNYUAN COMMUNITY LICENSE AGREEMENT (the \"License\");"
    },
    {
      "source": "[py_files/models/modeling_hunyuan.py]",
      "quote": "# You may obtain a copy of the License at"
    },
    {
      "source": "[py_files/models/modeling_hunyuan.py]",
      "quote": "#     https://github.com/Tencent/Tencent-Hunyuan-Large/blob/main/License.docx"
    },
    {
      "source": "[py_files/train/train.py]",
      "quote": "# Licensed under the Apache License, Version 2.0 (the \"License\");"
    },
    {
      "source": "[py_files/train/train.py]",
      "quote": "#     http://www.apache.org/licenses/LICENSE-2.0"
    }
  ],
  "1-4 (Paper)": "The official technical documentation is provided as a PDF: “<a href=\"report/Hunyuan_A13B_Technical_Report.pdf\"><b>Technical Report</b> </a>” (mirrored as “report/Hunyuan_A13B_Technical_Report.pdf”).  This is the authoritative paper/report for the Hunyuan-A13B family.",
  "1-4 (Paper)__evidence": [
    {
      "source": "[readme]",
      "quote": "<a href=\"report/Hunyuan_A13B_Technical_Report.pdf\"><b>Technical Report</b> </a>"
    },
    {
      "source": "[files]",
      "quote": "report/Hunyuan_A13B_Technical_Report.pdf"
    }
  ],
  "1-5 (Architecture)": "Multiple repository excerpts explicitly characterise Hunyuan-A13B as “an innovative and open-source large language model (LLM) built on a fine-grained Mixture-of-Experts (MoE) architecture,” establishing that the core design uses expert sparsity. One sentence quantifies the scale: “The newly introduced Hunyuan-A13B model features a total of 80 billion parameters with 13 billion active parameters,” which implies that, at inference time, only a 13 B-parameter subset of the 80 B parameter pool is selected. The code base reinforces this MoE-centric structure through dedicated classes such as “class HunYuanMoEV1ForCausalLM” and a model-specific rotary-embedding implementation (“class HunYuanRotaryEmbedding(nn.Module)”). Architectural hyper-parameters are stored via “HunYuanConfig,” whose docstring states that it “is used to instantiate an HunYuan model according to the specified arguments, defining the model architecture.” Among these arguments is “vocab_size … defaults to 32000,” signalling that the embedding layer is built for a 32 K token vocabulary. Collectively, the quotes depict Hunyuan-A13B as an 80 B parameter, 13 B-active, MoE-based causal language model implemented in bespoke PyTorch classes with rotary positional embeddings and a formal configuration object used to recreate the full architecture.",
  "1-5 (Architecture)__evidence": [
    {
      "source": "[readme]",
      "quote": "Welcome to the official repository of **Hunyuan-A13B**, an innovative and open-source large language model (LLM) built on a fine-grained Mixture-of-Experts (MoE) architecture."
    },
    {
      "source": "[readme]",
      "quote": "The newly introduced Hunyuan-A13B model features a total of 80 billion parameters with 13 billion active parameters."
    },
    {
      "source": "[py_files/models/configuration_hunyuan.py]",
      "quote": "This is the configuration class to store the configuration of a [`HunYuanModel`]. It is used to instantiate an HunYuan model according to the specified arguments, defining the model architecture."
    },
    {
      "source": "[py_files/models/configuration_hunyuan.py]",
      "quote": "vocab_size (`int`, *optional*, defaults to 32000): Vocabulary size of the HunYuan model."
    },
    {
      "source": "[py_files/models/modeling_hunyuan.py]",
      "quote": "class HunYuanMoEV1ForCausalLM(HunYuanPreTrainedModel):"
    },
    {
      "source": "[py_files/models/modeling_hunyuan.py]",
      "quote": "class HunYuanRotaryEmbedding(nn.Module):"
    },
    {
      "source": "[py_files/train/train.py]",
      "quote": "from models.configuration_hunyuan import HunYuanConfig"
    },
    {
      "source": "[py_files/train/train.py]",
      "quote": "config = HunYuanConfig("
    }
  ],
  "1-6 (Tokenizer)": "Tokenizer support is provided through a dedicated class: “class HYTokenizer(PreTrainedTokenizer): \\\"\\\"\\\"hunyuan tokenizer.\\\"\\\"\\\",” indicating a custom tokenizer subclass of the standard Hugging Face interface. The configuration snippet repeats that the model’s “vocab_size … defaults to 32000,” tying the tokenizer’s vocabulary to 32 K tokens. An additional code fragment shows the tokenizer is built on the tiktoken library: “enc = tiktoken.Encoding( \\n            \\\"HunYuan\\\", … ),” which names the underlying byte-pair-encoding scheme “HunYuan.” These details together show that Hunyuan-A13B employs a purpose-built HYTokenizer with a 32 000-token vocabulary and a tiktoken-based encoding labelled “HunYuan.”",
  "1-6 (Tokenizer)__evidence": [
    {
      "source": "[py_files/models/configuration_hunyuan.py]",
      "quote": "vocab_size (`int`, *optional*, defaults to 32000): Vocabulary size of the HunYuan model."
    },
    {
      "source": "[py_files/tokenization_hy.py]",
      "quote": "class HYTokenizer(PreTrainedTokenizer):\n    \"\"\"hunyuan tokenizer.\"\"\""
    },
    {
      "source": "[py_files/tokenization_hy.py]",
      "quote": "enc = tiktoken.Encoding(\n            \"HunYuan\","
    }
  ],
  "2-1 (Hardware)": "No training-hardware information is disclosed in the provided quotes.",
  "2-1 (Hardware)__evidence": [],
  "2-2 (Software)": "The repository explicitly notes that “Hunyuan-A13B provides processes related to model training,” and the code header labels it a “PyTorch HunYuan model,” confirming PyTorch as the primary machine-learning framework for training. Import lines such as “from models.modeling_hunyuan import HunYuanMoEV1ForCausalLM, HunYuanMoE” further show that the model’s MoE implementation is handled through custom PyTorch modules located within the same repository. Although no explicit version numbers or distributed-training libraries are named, the quotes collectively establish that the training software stack for Hunyuan-A13B is built around PyTorch with in-house Python modules that implement the MoE architecture and training routines.",
  "2-2 (Software)__evidence": [
    {
      "source": "[readme]",
      "quote": "Hunyuan-A13B provides processes related to model training."
    },
    {
      "source": "[py_files/models/modeling_hunyuan.py]",
      "quote": "\"\"\" PyTorch HunYuan model.\"\"\""
    },
    {
      "source": "[py_files/train/train.py]",
      "quote": "from models.modeling_hunyuan import HunYuanMoEV1ForCausalLM, HunYuanMoE"
    }
  ],
  "2-3 (API)": "The single relevant quote explicitly presents a hyperlink labeled \"HunyuanAPI\" that points to https://cloud.tencent.com/product/hunyuan.  This establishes that an official, cloud-hosted application-programming interface exists for the Tencent-Hunyuan/Hunyuan-A13B model family and is publicly advertised through Tencent Cloud.  No additional implementation details or usage examples are provided in the available material, but the presence of the dedicated API link confirms that users can access the model via an online service rather than only through downloadable weights or libraries.",
  "2-3 (API)__evidence": [
    {
      "source": "[readme]",
      "quote": "🕖 <a href=\"https://cloud.tencent.com/product/hunyuan\"><b>HunyuanAPI</b></a>"
    }
  ],
  "3-1 (Pre-training)": "The quotes indicate that Tencent released several pre-training–related artifacts for the Hunyuan-A13B series on 2025-06-27.  Specifically, the organization publicly open-sourced multiple variants—\"Hunyuan-A13B-Pretrain, Hunyuan-A13B-Instruct, Hunyuan-A13B-Instruct-FP8, and Hunyuan-A13B-Instruct-GPTQ-Int4\"—on Hugging Face, making both base and instruction-tuned checkpoints, as well as reduced-precision FP8 and INT4 quantized versions, freely available.  Alongside the model weights, Tencent published (a) a technical report and (b) a training and inference operation manual, both said to contain “detailed information about the model’s capabilities as well as the operations for training and inference.”  A repository path “[Training](train/README.md)” is referenced, indicating that step-by-step processes for reproducing or continuing training are documented.  Code snippets show that users can import the model classes through\n   from models.modeling_hunyuan import HunYuanMoEV1ForCausalLM, HunYuanMoE\nand configure them with\n   config = HunYuanConfig(\nThis confirms that configuration files and Python modules necessary for large-scale pre-training (and inference) are packaged within the release, providing a reproducible setup for the Hunyuan-A13B architecture.",
  "3-1 (Pre-training)__evidence": [
    {
      "source": "[readme]",
      "quote": "2025.6.27 We have open-sourced  **Hunyuan-A13B-Pretrain** , **Hunyuan-A13B-Instruct** , **Hunyuan-A13B-Instruct-FP8** , **Hunyuan-A13B-Instruct-GPTQ-Int4** on Hugging Face."
    },
    {
      "source": "[readme]",
      "quote": "In addition, we have released a technical report and a training and inference operation manual, which provide detailed information about the model’s capabilities as well as the operations for training and inference."
    },
    {
      "source": "[readme]",
      "quote": "Hunyuan-A13B provides processes related to model training. Please refer to [Training](train/README.md) for model training purposes."
    },
    {
      "source": "[py_files/train/train.py]",
      "quote": "from models.modeling_hunyuan import HunYuanMoEV1ForCausalLM, HunYuanMoE"
    },
    {
      "source": "[py_files/train/train.py]",
      "quote": "config = HunYuanConfig("
    }
  ],
  "3-2 (Fine-tuning)": "A licensing excerpt defines “Tencent Hunyuan” to include “fine-tuning enabling code” for the family of large language and multimodal models, explicitly naming “Tencent Hunyuan A13B” and citing its public GitHub repository (https://github.com/Tencent-Hunyuan/Hunyuan-A13B).  This shows that Tencent distributes not only the trained weights but also the software components required to run additional supervised instruction tuning or domain adaptation on the A13B checkpoint.  Although concrete hyperparameters are not given in the provided sentence, the legal text confirms that the fine-tuning pipeline—including optimizer states, training scripts, and other relevant elements—forms an integral, openly available part of the Hunyuan-A13B release, enabling users to reproduce or extend the model’s training.",
  "3-2 (Fine-tuning)__evidence": [
    {
      "source": "[license_files]",
      "quote": "j.\t“Tencent Hunyuan” shall mean the large language models, text/image/video/audio/3D generation models, and multimodal large language models and their software and algorithms, including trained model weights, parameters (including optimizer states), machine-learning model code, inference-enabling code, training-enabling code, fine-tuning enabling code and other elements of the foregoing made publicly available by Us, including, without limitation to, Tencent Hunyuan A13B released at [https://github.com/Tencent-Hunyuan/Hunyuan-A13B]."
    }
  ],
  "3-3 (Reinforcement Learning)": "",
  "3-3 (Reinforcement Learning)__evidence": [],
  "4-1 (Pre-training Data)": "",
  "4-1 (Pre-training Data)__evidence": [],
  "4-2 (Fine-tuning Data)": "",
  "4-2 (Fine-tuning Data)__evidence": [],
  "4-3 (Reinforcement Learning Data)": "",
  "4-3 (Reinforcement Learning Data)__evidence": [],
  "4-4 (Data Filtering)": "",
  "4-4 (Data Filtering)__evidence": [],
  "__usage": {
    "fine_tuning": "unknown",
    "rl": "not_used"
  }
}