{
  "model_id": "tencent/hunyuan-a13b-instruct",
  "full_texts": [
    {
      "arxiv_id": "https://github.com/Tencent-Hunyuan/Hunyuan-A13B/blob/main/report/Hunyuan_A13B_Technical_Report.pdf",
      "full_text": "Skip to content\n Toggle navigation\nNavigation Menu\nSign in\n Appearance settings\n‚Ä¢  Platform \n‚óã  \nGitHub Copilot\nWrite better code with AI\n‚óã  \nGitHub Spark New\nBuild and deploy intelligent apps\n‚óã  \nGitHub Models New\nManage and compare prompts\n‚óã  \nGitHub Advanced Security\nFind and fix vulnerabilities\n‚óã  \nActions\nAutomate any workflow\n‚óã  \nCodespaces\nInstant dev environments\n‚óã  \nIssues\n\nPlan and track work\n‚óã  \nCode Review\nManage code changes\n‚óã  \nDiscussions\nCollaborate outside of code\n‚óã  \nCode Search\nFind more, search less\nExplore\n‚óã  Why GitHub\n‚óã  Documentation \n‚óã  GitHub Skills \n‚óã  Blog \nIntegrations\n‚óã  GitHub Marketplace\n‚óã  MCP Registry\nView all features \n‚Ä¢  Solutions \nBy company size\n‚óã  Enterprises\n‚óã  Small and medium teams\n‚óã  Startups\n‚óã  Nonprofits\nBy use case\n‚óã  DevSecOps\n‚óã  DevOps\n‚óã  CI/CD\n‚óã  View all use cases\n\nBy industry\n‚óã  Healthcare\n‚óã  Financial services\n‚óã  Manufacturing\n‚óã  Government\n‚óã  View all industries\nView all solutions \n‚Ä¢  Resources \nTopics\n‚óã  AI\n‚óã  DevOps\n‚óã  Security\n‚óã  Software Development\n‚óã  View all\nExplore\n‚óã  Learning Pathways \n‚óã  Events & Webinars\n‚óã  Ebooks & Whitepapers\n‚óã  Customer Stories\n‚óã  Partners \n‚óã  Executive Insights\n‚Ä¢  Open Source \n‚óã  GitHub Sponsors\nFund open source developers\n‚óã  The ReadME Project\nGitHub community articles\nRepositories\n‚óã  Topics\n‚óã  Trending\n‚óã  Collections\n\n‚Ä¢  Enterprise \n‚óã  \nEnterprise platform\nAI-powered developer platform\nAvailable add-ons\n‚óã  \nGitHub Advanced Security\nEnterprise-grade security features\n‚óã  \nCopilot for business\nEnterprise-grade AI features\n‚óã  \nPremium Support\nEnterprise-grade 24/7 support\n‚Ä¢  Pricing\n Search or jump to...\nSearch code, repositories,\nusers, issues, pull requests...\nSearch\nClear \n\nSearch syntax tips\nProvide feedback\n Include my email address so I can be contacted\nWe read every piece of feedback, and take your input very\nseriously.\nCancel Submit feedback\nSaved searches\nUse saved searches to filter your results\nmore quickly\nName\nQuery\nTo see all available qualifiers, see our documentation.\nCancel Create saved search\nSign in\nSign up\n Appearance settings\nResetting focus\n You signed in with another tab or window. Reload to refresh\nyour session. You signed out in another tab or window. Reload to\nrefresh your session. You switched accounts on another tab or\nwindow. Reload to refresh your session. \n Dismiss alert\n\n Tencent-Hunyuan / Hunyuan-A13B Public\n‚Ä¢  \nNotifications You must be signed in to change notification\nsettings\n‚Ä¢  \nFork 93\n‚Ä¢  \n Star 748\n‚Ä¢  \n Code\n‚Ä¢  \n Issues 13\n‚Ä¢  \n Pull requests 0\n‚Ä¢  \n Actions\n‚Ä¢  \n Projects 0\n‚Ä¢  \n Security\nUh oh!\nThere was an error while loading. Please reload this page.\n‚Ä¢  \n Insights\n\nFooter\n ¬© 2025 GitHub,¬†Inc.\nFooter navigation\n‚Ä¢  Terms\n‚Ä¢  Privacy\n‚Ä¢  Security\n‚Ä¢  Status\n‚Ä¢  Docs\n‚Ä¢  Contact\n‚Ä¢  Manage cookies\n‚Ä¢  Do not share my personal information\n \n You can‚Äôt perform that action at this time.\n",
      "fetch_method": "direct-pdf"
    },
    {
      "arxiv_id": "https://angelslim.readthedocs.io/",
      "full_text": " ÊñáÊ°£ &#8212; AngelSlim Skip to main content Back to top Ctrl + K ÂÖ•Èó®ÊåáÂçó ÂÆâË£ÖÊïôÁ®ã Âø´ÈÄüÂºÄÂßã ÁÆóÊ≥ïÁâπÊÄß ÈáèÂåñ ÊäïÊú∫ÈááÊ†∑ Cache Ê®°ÂûãÊîØÊåÅ Ê∑∑ÂÖÉÊ®°ÂûãÈáèÂåñ DeepSeekÈáèÂåñ QwenÈáèÂåñ FLUX CacheÂä†ÈÄü ÈÉ®ÁΩ≤ÊñáÊ°£ ÂéãÁº©Ê®°ÂûãÈÉ®ÁΩ≤ ÊÄßËÉΩË°®Áé∞ ÈáèÂåñbenchmark ÊäïÊú∫ÈááÊ†∑Benchmark ËÆæËÆ°ÊñáÊ°£ ËÆæËÆ°ÊñáÊ°£ Repository Suggest edit .md .pdf ÊñáÊ°£ ÁõÆÂΩï ÊñáÊ°£ Êõ¥Â§ö Efficient LLM Compression Toolkit Star Watch Fork AngelSlimÊòØËÖæËÆØËá™Á†îÁöÑÔºåËá¥Âäõ‰∫éÊâìÈÄ†Êõ¥ÊòìÁî®„ÄÅÊõ¥ÂÖ®Èù¢ÂíåÊõ¥È´òÊïàÁöÑÂ§ßËØ≠Ë®ÄÊ®°ÂûãÂéãÁº©Â∑•ÂÖ∑ÂåÖ„ÄÇÊàë‰ª¨Â∞ÜÂºÄÊ∫êÈáèÂåñ„ÄÅÊäïÊú∫ÈááÊ†∑„ÄÅÁ®ÄÁñèÂåñÂíåËí∏È¶èÁ≠âÂéãÁº©ÁÆóÊ≥ï„ÄÇË¶ÜÁõñ‰∏ªÊµÅÊúÄÂâçÊ≤øÁöÑÂ§ßÊ®°ÂûãÔºåÂπ∂‰∏îÁ´ØÂà∞Á´ØÊâìÈÄö‰ªéÂéãÁº©Âà∞ÈÉ®ÁΩ≤ÁöÑÂÖ®ÊµÅÁ®ã„ÄÇ (AngelSlim, developed by Tencent, is a large language model compression toolkit engineered for enhanced usability, comprehensiveness, and efficiency. We will open-source compression algorithms including quantization, speculative decoding, pruning, and distillation. Supporting cutting-edge mainstream LLMs, the toolkit streamlines the complete end-to-end workflow from compression to deployment.) üåüAngelSlim‰∏ªË¶ÅÁâπÊÄßÊúâÔºö È´òÂ∫¶ÈõÜÊàêÂåñ ÔºöÊú¨Â∑•ÂÖ∑Â∞Ü‰∏ªÊµÅÁöÑÂéãÁº©ÁÆóÊ≥ïÈõÜÊàêÂà∞Â∑•ÂÖ∑ÔºåÂºÄÂèëËÄÖÂèØ‰∏ÄÈîÆÂºèË∞ÉÁî®ÔºåÂÖ∑ÊúâÂæàÂ•ΩÁöÑÊòìÁî®ÊÄß„ÄÇ ÊåÅÁª≠ÁÆóÊ≥ïÂàõÊñ∞ ÔºöÊú¨Â∑•ÂÖ∑Èô§‰∫ÜÈõÜÊàêÂ∑•‰∏öÁïå‰ΩøÁî®ÊúÄÂπøÁöÑÁÆóÊ≥ïÔºåËøòÊåÅÁª≠Ëá™Á†îÊõ¥Â•ΩÁöÑÂéãÁº©ÁÆóÊ≥ïÔºåÂπ∂‰∏î‰ºöÈôÜÁª≠ÂºÄÊ∫ê„ÄÇ ËøΩÊ±ÇÊûÅËá¥ÊÄßËÉΩ ÔºöÂú®Ê®°ÂûãÂéãÁº©ÊµÅÁ®ã„ÄÅÂéãÁº©ÁÆóÊ≥ïÈÉ®ÁΩ≤ÊñπÈù¢ÔºåÊú¨Â∑•ÂÖ∑ÊåÅÁª≠Á´ØÂà∞Á´Ø‰ºòÂåñÔºåËá¥Âäõ‰∫éÁî®Êõ¥Â∞ëÁöÑÊàêÊú¨ÂéãÁº©ÈÉ®ÁΩ≤Â§ßÊ®°Âûã„ÄÇ ÁõÆÂâçÊîØÊåÅÁöÑÂéãÁº©Á≠ñÁï•Ôºö ÈáèÂåñ ÔºöÂä®ÊÄÅINT8„ÄÅÈùôÊÄÅFP8„ÄÅÂä®ÊÄÅFP8„ÄÅINT4-GPTQ„ÄÅINT4-AWQÁ≠âÊñπÊ≥ïÔºõ ÊäïÊú∫ÈááÊ†∑ ÔºöEAGLE2„ÄÅEAGLE3Á≠âÊñπÊ≥ï„ÄÇ ÊñáÊ°£ # ÂÖ•Èó®ÊåáÂçó ÂÆâË£ÖÊïôÁ®ã Âø´ÈÄüÂºÄÂßã ÁÆóÊ≥ïÁâπÊÄß ÈáèÂåñ ÊäïÊú∫ÈááÊ†∑ Cache Ê®°ÂûãÊîØÊåÅ Ê∑∑ÂÖÉÊ®°ÂûãÈáèÂåñ DeepSeekÈáèÂåñ QwenÈáèÂåñ FLUX CacheÂä†ÈÄü ÈÉ®ÁΩ≤ÊñáÊ°£ ÂéãÁº©Ê®°ÂûãÈÉ®ÁΩ≤ ÊÄßËÉΩË°®Áé∞ ÈáèÂåñbenchmark ÊäïÊú∫ÈááÊ†∑Benchmark ËÆæËÆ°ÊñáÊ°£ ËÆæËÆ°ÊñáÊ°£ Êõ¥Â§ö # ÊÉ≥‰∫ÜËß£Êõ¥Â§ö‰ø°ÊÅØÔºåÂèØ‰ª•ÁªôÊàë‰ª¨Âú® GitHub Issues ‰∏äÁïôË®ÄÔºå‰πüÂèØ‰ª•Âä†ÂÖ•Êàë‰ª¨ÁöÑ ÂæÆ‰ø°‰∫§ÊµÅÁæ§ ËÆ®ËÆ∫Êõ¥Â§öÁöÑÊäÄÊúØÈóÆÈ¢ò„ÄÇ ‰∏ã‰∏ÄÈ°µ ÂÆâË£ÖÊïôÁ®ã ÁõÆÂΩï ÊñáÊ°£ Êõ¥Â§ö ‰ΩúËÄÖÔºö the AngelSlim Team ¬© Copyright 2025, AngelSlim Team. so the DOM is not blocked --> ",
      "fetch_method": "direct-html"
    },
    {
      "arxiv_id": "https://raw.githubusercontent.com/Tencent-Hunyuan/Hunyuan-A13B/main/report/Hunyuan_A13B_Technical_Report.pdf",
      "full_text": "2025-06-26\nHunyuan-A13B Technical Report\nTencent Hunyuan Team\nAbstract\nWe present Hunyuan-A13B, an open-source large language model employing a Mixture-\nof-Experts (MoE) architecture that optimizes the trade-off between computational effi-\nciency and model performance. The architecture leverages 80 billion total parameters\nwhile activating only 13 billion parameters during inference, enabling cost-effective de-\nployment without compromising capability. Specifically, Hunyuan-A13B is pretrained on\na rigorously filtered 20T token corpus with enhanced STEM-focused data curation, which\nsignificantly improves its factual reliability and reasoning abilities. In the post-training\nstage, Hunyuan-A13B utilizes high-quality data for supervised fine-tuning (SFT) and\nlarge-scale reinforcement learning, comprehensively enhancing the model‚Äôs performance\nin all dimensions. To enhance computational efficiency, we implement a dual-mode\nChain-of-Thought (CoT) framework that dynamically adjusts reasoning depth based on\ntask requirements. This framework provides a rapid ‚Äúfast-thinking‚Äù mode that handles\nroutine inquiries with low latency, and a deeper ‚Äúslow-thinking‚Äù mode for more involved\nmulti-step reasoning problems. Benchmark evaluations demonstrate that the Hunyuan-\nA13B achieves competitive performance across diverse domains including mathematical\nand scientific reasoning, programming, and general language tasks. In mathematical and\nscientific reasoning tasks, it achieves results comparable to state-of-the-art models with\nsubstantially larger parameter counts. The model also approaches the programming\ncompetency of these larger counterparts while exhibiting robust agent capabilities in task\nplanning and tool utilization scenarios. Notably, our model exhibits superior inference\nthroughput, making it particularly suitable for latency-sensitive applications. We release\nHunyuan-A13B to the open-source community to encourage ongoing advancements in\nLLMs and to facilitate efficient, research-driven practical applications.\n1\nIntroduction\nLarge Language Models have rapidly advanced in recent years, driven by the emergence of foundational\nmodels progressively approaching capabilities associated with Artificial General Intelligence (AGI).\nState-of-the-art systems, such as GPT-4o (OpenAI, 2024), o1 (OpenAI, 2024), o3 (OpenAI, 2025), Gemini\n2.5 (DeepMind, 2025), DeepSeek-R1 (Guo et al., 2025), and Qwen3 (Yang et al., 2025b), demonstrate\nincreasingly sophisticated capabilities, narrowing the gap toward the general intelligence that researchers\nhave long pursued. However, deploying these advanced models typically demands significant computa-\ntional resources, imposing high inference latency and considerable hardware costs, which limits broad\naccessibility.\nTo alleviate these computational challenges, we propose Hunyuan-A13B, an efficient and accessible open-\nsource LLM. Unlike many leading open-source models that adopt dense architectures with uniformly\nhigh parameter activation, Hunyuan-A13B is designed with a Sparse Mixture-of-Experts architecture\ncomprising 80 billion total parameters, yet activating only 13 billion parameters per input. By selectively\nactivating relevant model components for each input, Hunyuan-A13B achieves performance comparable\nwith cutting-edge LLMs, substantially reducing inference latency and computational overhead relative\nto dense models of similar scale. Consequently, Hunyuan-A13B offers researchers and practitioners a\nscalable and efficient alternative, significantly lowering deployment costs while preserving advanced\nlanguage modeling capabilities.\nHunyuan-A13B incorporates several innovative elements that collectively enhance its reasoning per-\nformance, flexibility, and inference efficiency. First, we constructed a high-quality pre-training corpus,\ncarefully curated from diverse domains to form a robust 20T tokens. We placed particular emphasis\non rigorous quality standards for data related to STEM disciplines, thereby elevating the upper bound\nfor the model‚Äôs reasoning abilities. Second, we collected and utilized high-quality, long-CoT SFT data\nto significantly boost the model‚Äôs logic and complex reasoning performance. Subsequently, we con-\nducted large-scale reinforcement learning (RL), systematically enhancing reasoning capabilities through\niterative optimization. Third, Hunyuan-A13B employs a dual-chain-of-thought (dual-CoT) reasoning\nstrategy, offering concise short-CoT for simpler queries and detailed long-CoT for complex tasks. Users\ncan flexibly select between these two modes according to their application‚Äôs complexity and resource\nconstraints. Lastly, significant enhancements in inference optimization substantially increase token\n1\n\nthroughput and inference performance. These improvements enable our models to effectively tackle\nreal-time, resource-constrained scenarios requiring fast, reliable predictions.\nIn the pre-training stage, we first train Hunyuan-A13B on high-quality dataset consisting of more than\n20T tokens. Subsequently, we process a fast annealing stage to enhance its overall performance and a\nlong-context stage to scale the model‚Äôs context window to 256K. To improve the data diversity and quality,\nwe optimize the acquisition and cleaning process of STEM data and build a refined data labeling system.\nHunyuan-A13B shows superior performance when compared to other representative MoE or Dense\nmodel with comparable or larger activated or total parameter sizes. The post-training procedure involved\na structured, multi-stage approach. Initially, we conducted supervised fine-tuning focused on reasoning\ntasks, followed by targeted RL optimization to further enhance reasoning capabilities. Subsequently,\ngeneral supervised fine-tuning was conducted across diverse domain-specific tasks, succeeded by a\ngeneralized RL training phase aimed at enhancing broader instruction-following abilities. Furthermore,\nwe incorporated a dual-CoT schema. This schema offers two distinct modes: a rapid ‚Äúfast-thinking‚Äù\nmode for efficient handling of routine inquiries, and a deeper ‚Äúslow-thinking‚Äù mode specifically tailored\nfor complex problems requiring multi-step reasoning.\nTo validate the capabilities and efficiency of Hunyuan-A13B, we conducted comprehensive evaluations\ncovering both widely recognized public benchmarks and newly developed internal test sets, enabling a\nrigorous assessment while minimizing potential biases from data contamination. Experimental results\nshow that our model exhibits strong performance in mathematical, scientific, and logical reasoning tasks.\nOn these reasoning tasks, Hunyuan-A13B achieves accuracy comparable to state-of-the-art language\nmodels that have notably larger parameter sizes. In coding-related benchmarks, the model similarly\nobtains competitive results, performing close to established leading models. Importantly, our model\nsignificantly outperforms larger alternatives in agent-oriented tasks, demonstrating superior proficiency\nin complex decision-making scenarios and task-oriented reasoning. Furthermore, efficiency evaluations\nreveal that Hunyuan-A13B achieves high throughput, underscoring its computational efficiency without\ncompromising reasoning quality, accuracy, or generalization across a broad range of challenging tasks.\n2\nPre-Training\nIn this section, we will introduce the details of the pre-training of Hunyuan-A13B, including data\nallocation, model structure and pre-training stage.\n2.1\nData for Pre-training\nWe reuse the data curation pipeline of Hunyuan-TurboS (Liu et al., 2025), which consists of the following\nmodules to obtain high-quality pre-training corpus: (1) Data preprocessing module, which completes data\ndeduplication, low-quality filtering, data denoising and data topic labeling. (2) Model-based extraction\nmodule, extracting plain text from data processed by previous modules. (3) Post-processing module,\nwhich performs low-quality filtering and semantic level deduplication on the extracted corpus. For\nthe pre-training data processing of Hunyuan-A13B, we optimize the sub-modules within this pipeline.\nSpecifically, we enhance the STEM data acquisition and cleaning processes, significantly improving the\nquality of STEM-related data. As a result, we successfully extract 250 billion tokens of high-quality STEM\npre-training corpus, which is incorporated into the training of Hunyuan-A13B. For data labeling, we\ndesigned a refined knowledge labeling system to improve the accuracy of knowledge representation\nin labels. Furthermore, we design a multi-dimensional difficulty grading framework to facilitate the\nefficient selection and filtering of multi-dimensional corpora within the training dataset.\n2.2\nModel Architechture\nThe Hunyuan-A13B model employs a fine-grained MoE architecture (Dai et al., 2024). Specifically, it con-\nsists of 1 shared expert and 64 fine-grained non-shared experts, all operating with identical intermediate\ndimension. This design is inspired by our extensive experiments on the scaling laws of MoE architectures.\nThrough these experiments, we observe that the presence of a shared expert has a noticeable impact on\nmodel performance‚Äîmodels without any shared expert tend to underperform compared to those with\nat least one. However, increasing the number of shared experts beyond one yields diminishing returns,\nwith only marginal improvements (or even fluctuations) in model effectiveness.\nDuring the training stage, the shared expert remains perpetually active, while only 8 non-shared experts\nare activated simultaneously. The model features 13 billion active parameters within a total parameter\ncount of 80 billion.\nFor the activation function, we adopt SWiGLU (Shazeer, 2020), maintaining consistency with both\n2\n\nConfiguration\nHunyuan-A13B\n# Layers\n32\n# Attention Heads\n32\n# Key/Value Heads\n8\n# Shared Experts\n1\n# Specialized Experts\n64\n# Activated Specialized Experts\n8\nActivation Function\nSwiGLU\nVocabulary Size\n128K\nHidden Size\n4096\nFFN Hidden Size\n3072\nTable 1: Overview of the key hyper-parameters of Hunyuan-A13B.\nPretrain \nModel\nStage1\nReasoning-oriented \nSFT\nStage2\nReasoning-oriented \nRL\nStage3\nAll-Scenarios\nSFT\nStage4\nAll-Scenarios\nRL\nFigure 1: A diagram illustrating the four steps of Hunyuan-A13B post-training.\nHunyuan-Large (Sun et al., 2024) and Hunyuan-TurboS (Liu et al., 2025). Hunyuan-A13B incorporates\nGrouped-Query Attention(GQA, Ainslie et al., 2023) in its attention layers to enhance KV Cache memory\nefficiency. The tokenizer of Hunyuan-A13B is the same as Hunyuan-Large (Sun et al., 2024), with a\nvocabulary size of 128K. Table 1 provides the key architectural features of our model.\n2.3\nPre-training Stage\nThe training process of Hunyuan-A13B consisted of three sequential stages.\nFoundation Training Stage: This stage processed a total of 20T tokens. The learning rate schedule follows\na three-phase approach: The warmup phase linearly scales the learning rate from 0 to the maximum value\nof 3 √ó 10‚àí4. In cosine decay stage, we decrease the learning rate from 3 √ó 10‚àí4 to the minimum value of\n3 √ó 10‚àí5 over 13.5 trillion tokens, then maintains the minimum learning rate for the remaining training\nsteps. Throughout this stage, a fixed 4096 context window was employed for all training sequences.\nFast Annealing Stage: Initiating from the minimum learning rate of 3 √ó 10‚àí5, this stage implemented a\nrapid cosine decay over 300B tokens to reach 8 √ó 10‚àí6. The context window was increased to 8192 tokens\nduring annealing stage training.\nLong-Context Training Stage: Following the annealing phase, Hunyuan-A13B progressed through two\nsequential phases to expand its context window to 32K tokens and then to 256K tokens. Both phases\nadopt the NTK-aware (Peng & Quesnelle, 2023) positional encoding identical to Hunyuan-TurboS, with\nalpha values set to 50 (32K) and 1000 (256K) respectively.\n3\nPost-training\nAs depicted in Figure 1, we propose a structured post-training approach designed to substantially\nenhance the capabilities of LLMs. This framework consists of two complementary fine-tuning phases:\nreasoning-oriented fine-tuning and general-purpose (all-scenarios) fine-tuning.\n3.1\nReasoning-oriented Fine-Tuning\nThe reasoning-oriented fine-tuning stage aims specifically at strengthening the model‚Äôs proficiency in\ncomplex reasoning-oriented tasks, such as mathematical reasoning, logical inference, code generation, and\nscientific analysis. In this phase, supervised fine-tuning is conducted using carefully curated instruction-\nresponse datasets, composed of explicit reasoning processes and detailed chain-of-thought solutions.\nReinforcement learning in this stage leverages feedback signals generated directly from correctness\nevaluations of final outputs, thereby explicitly promoting higher accuracy and logical rigor.\n3\n\n3.1.1\nReasoning-oriented SFT Stage\nThe methodologies for acquiring and processing supervised fine-tuning data in each domain are detailed\nas follows:\n(1) Mathematical Reasoning: Mathematical problems are collected from diverse educational re-\nsources, such as textbooks, standardized tests, and mathematics competitions, covering levels\nranging from basic arithmetic up to advanced Olympiad-level mathematics. State-of-the-art gen-\nerative reward models and automated solution verification mechanisms are employed iteratively\nto assess and refine CoT-based examples. Only rigorously verified mathematical reasoning pairs\nare retained in the final dataset.\n(2) Code-Based Reasoning: Programming reasoning data originate from carefully selected open-\nsource repositories (e.g., GitHub). A mature data-generation pipeline (Wei et al., 2024) sys-\ntematically transforms code snippets into structured instructional reasoning pairs spanning\nvarious tasks, programming languages, and problem types. Multi-stage validation, involving\ncritic models and sandbox execution tests, ensures correctness, logical coherence, and practical\nexecutability of the final reasoning examples.\n(3) Logical Reasoning: Logical reasoning data are derived from copyrighted and publicly available\npuzzle collections. An automated data synthesis methodology, inspired by ZebraLogic (Lin et al.,\n2025b), is adopted for scalable augmentation of the dataset. Logical tasks are systematically\ncategorized by problem type and difficulty. Quality assurance relies on a two-tiered validation\napproach, employing automated CoT evaluation models for standard scenarios, and human\nannotators for verifying complex cases, thus ensuring high data quality and optimal resource\nutilization.\n(4) Scientific Reasoning: Scientific reasoning tasks encompass a broad range of disciplines, including\nphysics, chemistry, and biology, incorporating questions from middle school level to advanced\ngraduate-level difficulty. Large language models are utilized for assessing question difficulty\nand quality. Especially complex items, such as advanced university exam questions and science\nOlympiads problems, undergo rigorous scrutiny through an advanced LLM-based verifier. This\nverification procedure is designed to identify and correct subtle scientific discrepancies involving\nunit conversions, numerical approximations, and chemical notations. Ultimately, only samples\nsuccessfully validated through rigorous rejection sampling are included in the final dataset.\n3.1.2\nReasoning-oriented RL Stage\nIn this stage, reasoning capabilities in the four domains are further enhanced through reinforcement\nlearning on top of the supervised fine-tuned foundation based on the Group Relative Policy Optimization\n(GRPO) (Shao et al., 2024), leveraging two types of reward:\n(1) Outcome Reward Model: This lightweight language model-based verifier assesses the alignment\nbetween the generated final answer and the reference solution, yielding a binary reward (1 for\nalignment, 0 otherwise). It is designed to normalize superficial discrepancies‚Äîsuch as formatting,\nunit conversions, or synonyms-to minimize false negatives, and is applied across mathematics,\nlogic, and science evaluations.\n(2) Sandbox Feedback: A multilingual code sandbox supporting 36 programming languages‚Äîsuch\nas Python, C++, Go, and Java‚Äîhas been developed. Deployed on a distributed CPU cluster, the\nsandbox enables over 1000 concurrent executions. Strict security measures, including file and\nnetwork isolation, are implemented to prevent malicious code execution.\nBuilding on these reward mechanisms, the RL stage samples prompts from cases where the SFT model\nshows unstable performance. The dataset includes 150K samples (Mathematics : Coding : Logic : Science\nratio 2:2:1:1), with 10% overlapping with SFT training data and 90% novel cases. We exclude multiple-\nchoice, true/false, and proof-based problems to avoid rewarding guesswork and ensure verifiable\noutcomes.\nRL training progresses through two contextual length phases inspired by Luo et al. to systematically\nenhance reasoning depth: Phase 1 uses 24K-length contexts, while Phase 2 expands to 32K. Notably, the\ntraining architecture removes the KL divergence constraint as referenced in Yu et al., enabling more\nflexible policy updates. Other effective configurations include (1) on-policy learning strategies, (2) large\nbatch sizes, (3) increased rollout counts, and (4) a relatively low sampling temperature (0.6‚Äì0.8). These\nmeasures collectively benefit the RL training process.\n4\n\n3.2\nAll-Scenarios Fine-Tuning\nThe all-scenarios fine-tuning stage broadens the model‚Äôs competence across diverse practical scenarios,\nincluding creative writing, knowledge-based question-answering, instruction-following, and multi-turn\nconversational tasks. This stage similarly involves supervised fine-tuning on diverse instruction-response\ndatasets. In contrast to the first phase, reinforcement learning here employs a dual-signal optimization\nmethod‚Äîevaluating both correctness of final outputs and assessments of stylistic quality, coherence, and\nadaptability provided by a larger LLM functioning as a proxy evaluator. This comprehensive evaluation\nstrategy allows the model to achieve improved accuracy along with enhanced usability within varied\napplication contexts.\n3.2.1\nAll-Scenarios SFT Stage\nBuilding upon the model‚Äôs demonstrated proficiency in complex reasoning tasks, this phase aims to\nfurther broaden its adaptability. To achieve this, we sampled portions from specialized reasoning datasets\nand combined them simultaneously with general-domain examples for SFT. These supplementary\ndatasets target a wider range of capabilities:\n(1) Language Understanding Tasks: This domain targets foundational language processing ca-\npabilities, including comprehension, accurate translation, and fluent text generation. Datasets\nundergo rigorous screening to exclude unclear and ambiguous instructions. Responses are\nvalidated through advanced response scoring models, explicitly designed to discourage reward\nmanipulation and encourage genuinely helpful outputs. The resulting language instruction data\nfurther undergoes iterative expert rewriting for quality improvement.\n(2) Creative Writing: Creative generation data are annotated along multiple dimensions, such as\ngenre, style, tone, and narrative structure, to ensure content diversity. Low-quality or unstable\nsamples are filtered out through discriminative reward modeling. Final dataset refinement\ninvolves iterative self-improvement methodologies combined with expert-assisted rewriting to\nachieve high-quality creative outputs.\n(3) Multilingual Tasks: To broaden linguistic capability, representative instructional datasets encom-\npassing standard English and various other languages were synthesized leveraging advanced\naugmentation and instruction-evolution methodologies, back-translation techniques. Dedicated\nlinguistic experts supervised annotation and validation processes to ensure language accuracy,\nfluency, and cultural appropriateness.\n(4) Complex Instruction Scenarios: To enhance the model‚Äôs proficiency with multifaceted tasks,\ndatasets were synthesized to present varying constraints, extensive context integration, and\nagent-driven requirements. Rule-based validation methodologies guarantee constraint adher-\nence within generated responses. Long-context tasks were developed specifically to require\nsynthesizing information across diverse textual segments. Complex interactions demanding tool\nuse, strategic planning, reflection, and multi-stage reasoning were purposefully crafted to enrich\nagentic behaviors.\n(5) Role-based Interaction: Diverse character profiles formulated around distinct personality traits\nserve as a foundation for generating role-play scenarios. Dialogue samples consistent with\ndefined personas result from sophisticated prompt-engineering approaches. Comprehensive\nresponse evaluation metrics, including trait accuracy, instruction adherence, and emotional\nempathy, were systematically applied to ensure fidelity of role-based interactions.\n(6) Knowledge-Based QA: To ensure improved accuracy and mitigate model hallucinations in\nknowledge-intensive domains, multi-layer validation strategies were deployed. Specialized\ncritic models systematically filtered inaccurate and ungrounded information, selecting rigorously\nsubstantiated responses.\n(7) Multi-Turn Dialogues: Multi-turn dialogue datasets encompass varied interaction modes, in-\ncluding task-oriented conversations, social dialogues, and question-answer exchanges. Data\ncompilation involves strategic integration from open-source data, vendor-sourced materials,\ncontrolled data synthesis, and advanced pseudo-dialogue techniques designed specifically to\nsimulate realistic multi-turn exchanges with depth and contrastive variety.\n(8) Agent: We take a series of data construction measures to enhance the model dialogue interaction\nskills, like planning, tool invocation, and reflection. First, we develop a multi-role synthetic data\nengine with five roles, including user, planner, tool, agent and checker. This engine simulates\nreal multi-party conversations to generate training data. Next, we integrate three data sources\nto achieve tool responses: sandbox tools, model context protocols (MCPs), and synthetic tools.\nThis integration solves the issues of high real data acquisition cost and limited tool variety,\n5\n\ngenerating diverse environmental feedback. To deal with the imbalance between fast and slow\nthinking data in the dataset, we optimize prompt strategies and train a special model to generate\nthinking processes. At the same time, we design over 30 types of agent system instructions. We\ncombine format changes of tools, actions, and responses to create 20,000 format combinations,\ngeneralizing some training data to improve the model‚Äôs generalization ability. Finally, based on\nagent characteristics, we focus on enhancing the model‚Äôs ability to handle high-frequency tasks,\nsuch as Excel processing and deep search.\n3.2.2\nAll-Scenarios RL Stage\nIn this stage, optimization is extended to general tasks, with a focus on balancing performance across\nvarious domains. This requires integrating diverse reward mechanisms and task-specific frameworks to\nensure robust generalization.\nFollowing Zhang et al.; Mahan et al.; Xu et al., we employ a generative reward model (GRM) that\ncompares candidate answers against a reference answer, treating it as a semantic anchor for open-ended\ntasks (e.g., creative writing) and using ground-truth for deterministic problems like closed-book QA.\nThe GRM can optionally ingest CoT reasoning traces to improve judgment accuracy on multi-step\nreasoning tasks, and may invoke external tools, report length statistics, or check outputs against task\nconstraints‚Äîvia carefully crafted prompts‚Äîto assess correctness. This flexible architecture enables\nnuanced reward signaling across task types.\nTo ensure balanced general capabilities, every subsequent track is supported by a dedicated reward\nservice, corresponding preference datasets and rules, and specially designed data construction pipelines.\nThese components work in tandem to address the unique challenges of each domain:\n(1) Text Understanding. Two reward models are utilized: a consistency model for objective Q&A and\na comparative GRM for subjective or open-ended tasks, aligning with the duality of text-based\nreasoning.\n(2) Translation. Domain experts annotate parallel corpora; GRMs trained on these annotations\nprovide faithful reward signals, ensuring semantic fidelity across languages.\n(3) Long Context. For long-context tasks, an additional hallucination-focused reward model and\nonline RL further enhance stability, addressing the complexity of extended reasoning chains.\n(4) Creative Writing. Paired GRMs based on relative preference judgments mitigate reward hacking,\nwhile creative rewards are blended with automated checks for instruction adherence to balance\ninnovation and compliance.\n(5) Agents. We employ sandbox tools and MCPs to facilitate information feedback. Additionally, we\nconstruct rule-based rewards in reinforcement training. The reward function has two components:\nformat reward (1 for correct special markers and order, 0 otherwise) ensuring parseability,\nand correctness reward evaluating tool/parameter/value consistency between predictions and\nreferences.\n(6) Multi-Turn Dialogue. Dialogue-specific critic models and general rewards are refined by mining\nunstable conversations, enhancing contextual coherence in interactive scenarios.\n(7) Complex Instructions. Constraint-extraction and satisfaction tools, complemented by general\ncritic and reward models, ensure precise execution of multi-faceted commands.\n(8) Role-Playing. Evaluation of instruction comprehension, character consistency, and empathy\nguides data generation via generalized critic and reward models, fostering immersive interac-\ntions.\n(9) Safety. Safe response pairs are identified using classifiers and refusal heuristics, integrating\nsafety alignment directly into preference datasets to mitigate risks.\n(10) Knowledge QA. Hallucination detection models (with and without reference access) and user-\nexperience-focused models are jointly optimized to reduce factual errors in knowledge-based\nresponses.\n(11) Multilingual. GRMs sample and score SFT answers, retaining prompts with high diversity,\nanswer variance, and quality to enhance cross-lingual generalization.\n(12) Finance, Legal, and Medical Domains. Consistency-based rewards identify unstable items\nfrom professional exams, supplying preference data for targeted improvements in specialized\nknowledge domains.\nThis All-Scenarios RL stage unifies a flexible GRM with domain-specific pipelines across 16 sub-topics\nand over 30 scoring services. Adversarial prompt filtering and dynamic sampling ensure cross-domain\ngeneralization, enabling versatile reasoning in deterministic, creative, and professional scenarios.\n6\n\n3.3\nDual-Mode Chain-of-Thought\nThe Dual-Mode CoT approach allows LLMs to dynamically adjust reasoning depth according to task\ncomplexity and user needs by integrating two complementary inference modes: fast-thinking and slow-\nthinking. The fast-thinking mode provides concise, efficient outputs ideal for simpler tasks requiring\nspeed and minimal computational overhead. In contrast, the slow-thinking mode involves deeper, more\ncomprehensive reasoning steps such as reflection and backtracking, leading to longer CoT and increased\ntoken use, which substantially improves accuracy and robustness in handling complex reasoning tasks.\nBy flexibly selecting the appropriate reasoning mode, Dual-Mode CoT optimizes computational resource\nallocation, enabling users to strike a preferred balance between efficiency and task-specific accuracy.\nDuring the post-training stage, a unified training structure is employed to simultaneously optimize two\nreasoning modes for flexible and integrated usage. To standardize model outputs, both fast and slow\nthinking training examples adopt a common structured format differentiated by the presence or absence\nof detailed reasoning steps within a dedicated ‚Äú<think>‚Äù content block. Specifically, the ‚Äú<think>‚Äù block\nremains intentionally empty (‚Äú<think>\\n\\n<think>‚Äù) for fast-thinking, whereas it explicitly contains\nstep-by-step reasoning for slow-thinking. Users select the desired reasoning mode by specifying control\ntags: use ‚Äú/no think‚Äù for fast-thinking mode and ‚Äú/think‚Äù for slow-thinking mode. If no control tag is\nprovided, the system defaults to the slow-thinking mode.\n4\nEvaluation\n4.1\nEvaluations on Pre-Trained Model\n4.1.1\nBenchmarks\nWe evaluated Hunyuan-A13B from four core capability dimensions: general tasks, coding, mathematics,\nand multilingual capabilities. These capability dimensions include the following benchmark sets:\n‚Ä¢ General Tasks: include MMLU (Hendrycks et al., 2021)(5-shot), MMLU-Pro (Wang et al., 2024)(5-\nshot), MMLU-Redux (Gema et al., 2024))(5-shot), BBH (Suzgun et al., 2022)(3-shot), SuperGPQA\n(Du et al., 2025)(5-shot, CoT).\n‚Ä¢ Coding Tasks: include EvalPlus (Chen et al., 2021) (0-shot) , MultiPL-E (Cassano et al., 2022)\n(0-shot) (Python, C++, JAVA, PHP, TypeScript, C#, Bash, JavaScript), MBPP-3shot (Austin et al.,\n2021), and CRUXEval (Liu et al., 2023)(1-shot).\n‚Ä¢ Math & STEM Tasks: include MATH (Hendrycks et al., 2021) (4-shot, CoT), CMATH (Wei et al.,\n2023), GSM8K (Cobbe et al., 2021) (4-shot, CoT), and GPQA (Rein et al., 2024) (5-shot, CoT).\n4.1.2\nEvaluation Results\nTable 2 presents a comparative analysis of benchmark indicators between Hunyuan-A13B and several\nother representative MoE and Dense models with comparable or larger (activated) parameter sizes,\nincluding Hunyuan-Large (Sun et al., 2024), Qwen2.5-72B (Yang et al., 2025c) and Qwen3-235B-A22B(Yang\net al., 2025a)(hereafter referred to as Qwen3-A22B). Notably, When compared to other competitive base\nmodels, Hunyuan-A13B shows superior performance.\n(1) Compared with Hunyuan team‚Äôs earlier open-source MoE model Hunyuan-Large, which was\nfirst released in 2024, due to the improvements in our training strategy, model architecture\nand the enhancement of data quality, Hunyuan-A13B performs better on 12 of 14 evaluation\nbenchmarks with only 1/4 activated parameters and about 1/5 total parameters.\n(2) Compared with Qwen2.5-72B, which has similar total parameter size, Hunyuan-A13B acheieves\nhigher scores on almost all the evaluation benchmarks.\n(3) Compared with Qwen3-A22B, the recently released MoE model with about 3 times the total of\nparameters and about 2 times activated parameters, Hunyuan-A13B outperforms the latter on 7\nof 12 evaluation benchmarks, exhibitting comparable capabilities across multiple dimensions.\n4.2\nEvaluations on Post-Trained Model\n4.2.1\nBenchmarks\nTable 3 and Table 4 provide a comprehensive comparative analysis, assessing the performance of our\nproposed Hunyuan-A13B model against several prominent baseline models, including open-source\n7\n\nModel\nHunyuan-Large-1116\nQwen2.5-72B\nQwen3-A22B\nHunyuan-A13B\nArchitecture\nMoE\nDense\nMoE\nMoE\n# Activated Params\n52B\n72B\n22B\n13B\n# Total Params\n389B\n72B\n235B\n80B\nContext Length\n256K\n128K\n128K\n256K\nGeneral Tasks\nMMLU\n88.40\n86.10\n87.81\n88.17\nMMLU-Pro\n60.20\n58.10\n68.18\n67.23\nMMLU-Redux\n87.47\n83.90\n87.40\n87.67\nBBH\n86.30\n85.80\n88.87\n87.56\nSuperGPQA\n38.90\n36.20\n44.06\n41.32\nCoding Tasks\nEvalPlus\n75.69\n65.93\n77.60\n78.64\nMultiPL-E\n59.13\n60.50\n65.94\n69.33\nMBPP\n72.60\n76.00\n81.40\n83.86\nCRUX-I\n57.00\n57.63\n-\n70.13\nCRUX-O\n60.63\n66.20\n79.00\n77.00\nMath & STEM Tasks\nMATH\n69.80\n62.12\n71.84\n72.35\nCMATH\n91.30\n84.80\n-\n91.17\nGSM8k\n92.80\n91.50\n94.39\n91.83\nGPQA\n25.18\n45.90\n47.47\n49.12\nTable 2: Performance of Hunyuan-A13B and other representative MoE and Dense base models. The\nhighest scores are shown in bold.\nmodels (Hunyuan-Large, Qwen2.5-72B-Instruct, Qwen3-A22B, and DeepSeekR1) as well as the closed-\nsource model openAI-o1. The evaluation comprehensively assesses multiple essential competencies,\nincluding mathematical reasoning, scientific knowledge, coding capability, logical reasoning, instruction-\nfollowing accuracy, text comprehension, text-generation quality, execution of complex instructions, and\ntool-calling ability. To systematically benchmark the aforementioned capabilities, we employ a variety of\nestablished evaluation frameworks, explicitly as follows:\n‚Ä¢ Mathematics: Mathematical reasoning proficiency is assessed using established benchmarks,\nincluding the MATH dataset (Lightman et al., 2023) and advanced-level competition datasets\nsuch as the American Invitational Mathematics Examination (AIME).\n‚Ä¢ Science: Scientific knowledge and reasoning capabilities are evaluated using more challenging\nbenchmark tests, specifically GPQA-Diamond (Rein et al., 2024) and OlympiadBench (He et al.,\n2024).\n‚Ä¢ Coding: We assess coding performance using standard benchmarks.\nThese include Live-\nCodeBench (Jain et al., 2024), FullstackBench (Cheng et al., 2024), and McEval (Chai et al., 2024).\nAdditionally, we introduce ArtifactsBench, a new benchmark designed to evaluate frontend code\ngeneration abilities of LLMs. ArtifactsBench will be made publicly available soon.\n‚Ä¢ Reasoning: Logical deduction and cognitive reasoning capabilities are systematically assessed\nvia recognized tests, notably the BIG-Bench Hard (BBH) (Suzgun et al., 2022), ZebraLogic (Lin\net al., 2025a) and DROP (Dua et al., 2019) benchmarks. Performance on the DROP benchmark is\nreported using the F1 metric to suitably reflect the nuanced capabilities of logical reasoning and\nreading comprehension.\n‚Ä¢ Instruction Following: Proficiency in instruction comprehension and adherence is evaluated\nthrough standardized benchmarks such as IFEval (Zhou et al., 2023) and SysBench (Qin et al.,\n2024).\n‚Ä¢ Text Generation: To evaluate the text generation capabilities of LLMs, we introduce two internal\nbenchmarks: LengthCtrl and InsCtrl. Specifically, LengthCtrl measures the ability of a model\nto precisely adhere to predefined output length constraints, whereas InsCtrl assesses a model‚Äôs\nproficiency in accurately interpreting and executing complex textual instructions.\n‚Ä¢ NLU: We introduce two novel internal benchmarks to comprehensively assess the natural\nlanguage understanding capabilities of LLMs. The first benchmark, named ComplexNLU,\nconsists of advanced and long-tail reasoning tasks specifically designed to evaluate deeper levels\nof linguistic comprehension, presenting intricate linguistic structures and sophisticated semantic\nchallenges. The second benchmark, termed Word-Task, aims to examine fine-grained lexical and\nsemantic understanding at the word level.\n‚Ä¢ Agents: To comprehensively assess tool-calling ability, we choose four challenging benchmarks:\nBFCL v3 (Charlie Cheng-Jie Ji), œÑ-Bench (Yao et al., 2025), ComplexFuncBench (Zhong et al., 2025),\nand C3-Bench (Yu et al., 2025). These benchmarks simulate various complex scenarios, including\nmulti-turn tasks, long contexts, intricate tool interactions, and diverse calling strategies.\n8\n\nOpenAI-o1-1217\nDeepseek-R1-0120\nQwen3-A22B\nHunyuan-A13B\nMathematics\nAIME2024\n74.3\n79.8\n85.7\n87.3\nAIME2025\n79.2\n70\n81.5\n76.8\nMATH\n96.4\n94.9\n94\n94.3\nScience\nGPQA-Diamond\n78\n71.5\n71.1\n71.2\nOlympiadBench\n83.1\n82.4\n85.7\n82.7\nCoding\nLiveCodeBench\n63.9\n65.9\n70.7\n63.9\nFullstackBench\n64.6\n71.6\n65.6\n67.8\nArtifactsBench\n38.6\n44.6\n44.6\n43\nReasoning\nBBH\n80.4\n83.7\n88.9\n89.1\nDROP\n90.2\n92.2\n90.3\n91.1\nZebraLogic\n81\n78.7\n80.3\n84.7\nInstruction\nIF-Eval\n91.8\n83.3\n83.4\n84.7\nFollowing\nSysBench\n82.5\n77.7\n74.2\n76.1\nText\nLengthCtrl\n60.1\n55.9\n53.3\n55.4\nGeneration\nInsCtrl\n74.8\n69\n73.7\n71.9\nNLU\nComplexNLU\n64.7\n64.5\n59.8\n61.2\nWord-Task\n67.1\n76.3\n56.4\n62.9\nAGENT\nBFCL v3\n67.8\n56.9\n70.8\n78.3\nœÑ-Bench\n60.4\n43.8\n44.6\n54.7\nComplexFuncBench\n47.6\n41.1\n40.6\n61.2\nC3-Bench\n58.8\n55.3\n51.7\n63.5\nTable 3: Comparison of Hunyuan-A13B with other AI models evaluating performance on slow-thinking\nmode. The highest and second-best scores are shown in bold and underlined, respectively.\nHunyuan-Large-1116\nQwen2.5-72B-instruct\nQwen3-A22B\nHunyuan-A13B\nMathematics\nAIME2024\n23.3\n18.9\n40.1\n30.6\nAIME2025\n8.3\n15\n24.7\n19.2\nMATH\n77.4\n83.1\n87.2\n85.4\nScience\nGPQA-Diamond\n42.4\n49\n62.9\n61.8\nOlympiadBench\n44.8\n52.9\n69.9\n64.1\nCoding\nLiveCodeBench\n18.1\n25.4\n35.3\n27.4\nFullstackBench\n45.1\n55.2\n57.8\n58.3\nMcEval\n42.3\n58.7\n63.5\n59.6\nReasoning\nBBH\n86.5\n80.5\n68.1\n87\nDROP\n88.6\n75.7\n82.4\n86.5\nZebraLogic\n20.5\n26.6\n37.7\n36.5\nInstruction\nIF-Eval\n85.8\n84.1\n83.2\n84.4\nFollowing\nSysBench\n57.6\n70.4\n72.1\n70.2\nText\nLengthCtrl\n52.1\n53.2\n54.7\n53.9\nGeneration\nInsCtrl\n67.4\n65.2\n71.2\n68.9\nNLU\nComplexNLU\n50.2\n53.9\n56.7\n54.5\nWord-Task\n40.5\n41\n56.4\n53.4\nAGENT\nBFCL v3\n66.1\n61.4\n68.0\n65.9\nœÑ-Bench\n17.3\n41.8\n36.4\n42.6\nComplexFuncBench\n25.2\n40.1\n38.1\n74.0\nC3-Bench\n45.4\n52.1\n48.4\n65.4\nTable 4: Comparison of Hunyuan-A13B with other AI models evaluating performance on fast-thinking\nmode. The highest and second-best scores are shown in bold and underlined, respectively.\n4.2.2\nEvaluation Results\nAs shown in Table 3 and Table 4, despite its smaller model size compared to other LLMs in comparison,\nHunyuan-A13B demonstrates remarkable capabilities across various benchmarks, frequently achieving\ntop or near-top performance. It particularly excels in advanced mathematics, securing the highest score\non AIME2024, and also shows impressive logical reasoning skills, leading on ZebraLogic and BBH.\nBeyond these areas, Hunyuan-A13B demonstrates strong performance in scientific knowledge and\ninstruction following tasks, frequently ranking second and notably outperforming significantly larger\nmodels. Although its overall coding performance is slightly lower, it achieves results comparable to\nsubstantially larger LLMs on code evaluation tasks such as FullstackBench and ArtifactsBench, reflecting\nstrong practical coding capability. Across multiple domains and especially in the fast-thinking scenario,\nHunyuan-A13B consistently exhibits excellent performance, often clearly outperforming larger models,\nemphasizing its efficiency and capabilities relative to model size.\nBesides, Hunyuan-A13B demonstrated a leading performance on BFCL-v3, œÑ-Bench, ComplexFuncbench\nand C3-Bench. This suggests Hunyuan-A13B has strong tool-calling capabilities and adapts to a broad\nrange of application scenarios. Notably, C3-Bench simulates various changes in task combinations. In\nthis case, Hunyuan-A13B still maintains high accuracy, fully proving its ability to handle open problems\nin the real world.\n9\n\nModel\nGemini 2.5 Pro\nDeepSeek R1\nQwen3-A22B\nHunyuan-A13B\nPenguinScrolls\n88.3\n87.5\n87.1\n87.7\nLongBench-v2\n60.9\n53.8\n48.4\n55.0\nFRAMES\n80.1\n85.7\n84.0\n81.1\nTable 5: Performance comparison of Hunyuan-A13B and other representative open-source models on key\nlong-context benchmarks. The highest scores are bolded.\n4.3\nLong-Context Evaluation\nTo rigorously assess Hunyuan-A13B‚Äôs long-context understanding capabilities, we conducted evaluations\non a suite of challenging benchmarks designed to test various aspects of long-text comprehension. The\nselection of these specific benchmarks was driven by their distinct strengths in covering diverse and\ncritical scenarios for real-world LLM performance:\n‚Ä¢ PenguinScrolls (Hunyuan, 2024): This high-quality, manually annotated dataset is specifically\ndesigned to assess performance in real-world user experience scenarios. It features four common\ntypes of long-text tasks: Information Extraction, Information Localization, Qualitative Analysis,\nand Numerical Reasoning. Its inclusion ensures that Hunyuan-A13B‚Äôs capabilities are evaluated\nagainst practical challenges, using diverse natural long-form texts such as books, financial reports,\nlegal documents, and academic papers.\n‚Ä¢ LongBench-v2 (Bai et al., 2024): This benchmark is crucial for evaluating general long-context\nunderstanding across a wide array of realistic tasks. It encompasses six diverse tasks: single-\ndocument QA, multi-document QA, long in-context learning, long-dialogue history understand-\ning, code repository understanding, and long structured data understanding. Its comprehensive\nnature with a focus on deep understanding and reasoning rather than simple retrieval, makes it\na robust measure of an LLM‚Äôs versatility in handling complex long texts.\n‚Ä¢ FRAMES (Krishna et al., 2024): Retrieval-Augmented Generation (RAG) represents a critical\napplication in long-document scenarios. FRAMES provides an essential, unified framework for\nevaluating RAG systems by simultaneously assessing factuality, retrieval accuracy, and reasoning\nin end-to-end scenarios. Unlike benchmarks that evaluate components in isolation, FRAMES\nemploys multi-hop questions requiring information integration from multiple sources. This\ndesign critically tests an LLM‚Äôs ability to retrieve relevant information and synthesize accurate,\nfactual responses from extensive contexts.\n‚Ä¢ RULER (Hsieh et al., 2024): This benchmark is paramount for understanding how model\nperformance scales with increasing context lengths. RULER generates synthetic examples with\nconfigurable sequence lengths and task complexities, allowing for a systematic analysis of\nperformance degradation as context length increases. By focusing on the Question Answering\n(QA) sub-task, which closely reflects real-world scenarios and minimizes task shortcuts, RULER\nprovides insights into an LLM‚Äôs ability to retain and utilize information effectively across very\nlong contexts, addressing a key challenge in long-context modeling.\n4.3.1\nPerformance on Key Long-Context Benchmarks\nTable 5 summarizes Hunyuan-A13B‚Äôs performance against other representative open-source models on\nPenguinScrolls, LongBench-v2, and FRAMES.\nPenguinScrolls: Hunyuan-A13B achieved a strong score of 87.7, performing comparably to Gemini\n2.5 Pro (88.3) and slightly outperforming DeepSeek R1 (87.5) and Qwen3-A22B (87.1). These results\nunderscore Hunyuan-A13B‚Äôs robust capabilities in real-world long-text applications.\nLongBench-v2: On LongBench-v2, Hunyuan-A13B achieved a score of 55.0. This places it as the second-\nhighest performing model in the comparison, trailing only Gemini 2.5 Pro, which scored 60.9. Notably,\nHunyuan-A13B outperformed both DeepSeek R1 (53.8) and Qwen3-A22B (48.4) on this benchmark.\nFRAMES: On the FRAMES benchmark, Hunyuan-A13B achieved a competitive score of 81.1. This\nresult is noteworthy as it surpasses Gemini 2.5 Pro (80.1). However, Hunyuan-A13B‚Äôs performance on\nFRAMES is not as strong as DeepSeek R1 (85.7) and Qwen3-A22B (84.0), indicating there‚Äôs still room for\nimprovement in its RAG-specific long-context processing capabilities.\n4.3.2\nPerformance Scaling with Increasing Context Lengths\nTable 6 presents Hunyuan-A13B‚Äôs performance on the RULER benchmark‚Äôs QA sub-task across varying\ninput lengths. The RULER benchmark is crucial for understanding how model performance scales with\n10\n\nModel\nRULER\nAvg.\n0-8K\n8K-32K\n32K-64K\n64K-128K\nGemini 2.5 Pro\n81.7\n83.2\n80.0\n83.3\n80.2\nDeepSeek R1\n72.0\n75.4\n72.1\n72.1\n65.6\nQwen3-A22B\n73.0\n76.6\n72.8\n72.9\n66.6\nHunyuan-A13B\n76.7\n78.7\n75.3\n78.0\n73.9\nTable 6: Performance comparison of Hunyuan-A13B and other representative open-source models on\nthe RULER benchmark‚Äôs QA task. This comparison demonstrates the models‚Äô long-context performance\nacross various input lengths, specifically highlighting Hunyuan-A13B‚Äôs effectiveness as context length\nincreases.\nincreasing context length, given its configurable sequence length and task complexity. Here, we select\nthe Question Answering (QA) sub-task of RULER because it more closely reflects real-world scenarios\nand is considerably more difficult due to the scarcity of task shortcuts. Hunyuan-A13B demonstrates\ncompetitive performance on the RULER benchmark‚Äôs QA task, with an impressive average of 76.7. While\nGemini 2.5 Pro leads across all context lengths, Hunyuan-A13B exhibits remarkable long-range stability,\nwith its performance decay second only to Gemini 2.5 Pro. Notably, as the context length increased\nto 64K-128K, Hunyuan-A13B maintained a strong performance of 73.9, significantly outperforming\nDeepSeek R1 (65.6) and Qwen3-A22B (66.6). This demonstrates Hunyuan-A13B‚Äôs exceptional ability\nto retain and utilize information effectively even in very long contexts, showcasing a more graceful\ndegradation in performance compared to its counterparts as context length increases.\n4.4\nInference efficiency\nHunyuan-A13B employs GQA and MoE architectures to markedly enhance reasoning efficiency without\nsacrificing model performance. It integrates seamlessly with mainstream open-source inference frame-\nworks, including vLLM, SGLang, and TensorRT-LLM, enabling straightforward one-click deployment for\nW16A16 precision inference. Additionally, it utilizes service-layer features like Auto Prefix Caching and\nChunk Prefill to facilitate high-performance inference while maintaining compatibility within prevalent\ninference ecosystems. Furthermore, Hunyuan-A13B supports various lossless quantization formats, such\nas Weight Only INT8, W8A8, and KV Cache FP8, which can be effectively paired with native acceleration\nmechanisms provided by the frameworks, including Tensor Parallel (TP), Expert Parallel (EP), and\nFusedMoE. Table 7 presents the throughput performance of Hunyuan-A13B under the A16W16C16\nprecision setting.\nBatch\nInput Length\nOutput Length\nThroughput (tokens/s)\n1\n2048\n14336\n190.84\n16\n2048\n14336\n1246.54\n32\n2048\n14336\n1981.99\n32\n2048\n22528\n1725.95\nTable 7: Model Throughput of Hunyuan-A13B\n5\nConclusion\nThis paper presents Hunyuan-A13B, an efficient and open-source large language model built upon the\nMoE architecture. Despite having fewer active parameters, Hunyuan-A13B demonstrates comparable\nperformance to much larger LLMs across a variety of benchmark tasks. To enhance the model‚Äôs capabili-\nties, a meticulously assembled 20T pre-training dataset was employed. Structured supervised fine-tuning\nand reinforcement-learning-based optimization strategies were then adopted. Furthermore, targeted\narchitectural enhancements were implemented to boost inference speed and throughput, improving the\nmodel‚Äôs performance in a wide range of tasks. Consequently, Hunyuan-A13B attains highly efficient infer-\nence performance without substantially sacrificing effectiveness compared to the previous state-of-the-art\nLLMs. Additionally, scenario-specific optimizations for agent-oriented tasks empower Hunyuan-A13B to\neffectively manage complex decision-making. Overall, these improvements establish Hunyuan-A13B as\nan effective, scalable, and computationally efficient large language model, rendering it especially suitable\nfor advanced reasoning and general-purpose deployment in resource-constrained settings.\n11\n\nReferences\nAIME. Aime problems and solutions. https://artofproblemsolving.com/wiki/index.php/AIME Probl\nems and Solutions.\nJoshua Ainslie, James Lee-Thorp, Michiel de Jong, Yury Zemlyanskiy, Federico Lebr¬¥on, and Sumit\nSanghai. GQA: Training generalized multi-query transformer models from multi-head checkpoints. In\nProceedings of EMNLP, 2023.\nJacob Austin, Augustus Odena, Maxwell Nye, Maarten Bosma, Henryk Michalewski, David Dohan, Ellen\nJiang, Carrie Cai, Michael Terry, Quoc Le, et al. Program synthesis with large language models. arXiv\npreprint arXiv:2108.07732, 2021.\nYushi Bai, Shangqing Tu, Jiajie Zhang, Hao Peng, Xiaozhi Wang, Xin Lv, Shulin Cao, Jiazheng Xu, Lei\nHou, Yuxiao Dong, et al. Longbench v2: Towards deeper understanding and reasoning on realistic\nlong-context multitasks. arXiv preprint arXiv:2412.15204, 2024.\nFederico Cassano, John Gouwar, Daniel Nguyen, Sydney Nguyen, Luna Phipps-Costin, Donald Pinckney,\nMing-Ho Yee, Yangtian Zi, Carolyn Jane Anderson, Molly Q Feldman, et al. Multipl-e: A scalable and\nextensible approach to benchmarking neural code generation. arXiv preprint arXiv:2208.08227, 2022.\nLinzheng Chai, Shukai Liu, Jian Yang, Yuwei Yin, Ke Jin, Jiaheng Liu, Tao Sun, Ge Zhang, Changyu Ren,\nHongcheng Guo, et al. Mceval: Massively multilingual code evaluation. arXiv preprint arXiv:2406.07436,\n2024.\nFanjia Yan Shishir G. Patil Tianjun Zhang Ion Stoica Joseph E. Gonzalez Charlie Cheng-Jie Ji, Huanzhi Mao.\nGorilla bfvl v3. https://gorilla.cs.berkeley.edu/leaderboard.html. Accessed: 2025-01-17.\nMark Chen, Jerry Tworek, Heewoo Jun, Qiming Yuan, Henrique Ponde De Oliveira Pinto, Jared Kaplan,\nHarri Edwards, Yuri Burda, Nicholas Joseph, Greg Brockman, et al. Evaluating large language models\ntrained on code. arXiv preprint arXiv:2107.03374, 2021.\nYao Cheng, Jianfeng Chen, Jie Chen, Li Chen, Liyu Chen, Wentao Chen, Zhengyu Chen, Shijie Geng,\nAoyan Li, Bo Li, et al. Fullstack bench: Evaluating llms as full stack coders. arXiv e-prints, pp.\narXiv‚Äì2412, 2024.\nKarl Cobbe, Vineet Kosaraju, Mohammad Bavarian, Mark Chen, Heewoo Jun, Lukasz Kaiser, Matthias\nPlappert, Jerry Tworek, Jacob Hilton, Reiichiro Nakano, et al. Training verifiers to solve math word\nproblems. arXiv preprint arXiv:2110.14168, 2021.\nDamai Dai, Chengqi Deng, Chenggang Zhao, RX Xu, Huazuo Gao, Deli Chen, Jiashi Li, Wangding Zeng,\nXingkai Yu, Y Wu, et al. DeepSeekMoE: Towards ultimate expert specialization in mixture-of-experts\nlanguage models. arXiv preprint arXiv:2401.06066, 2024.\nGoogle DeepMind. Gemini 2.5, 2025. URL https://blog.google/technology/google-deepmind/gemini\n-model-thinking-updates-march-2025/.\nXinrun Du, Yifan Yao, Kaijing Ma, Bingli Wang, Tianyu Zheng, King Zhu, Minghao Liu, Yiming Liang,\nXiaolong Jin, Zhenlin Wei, et al. Supergpqa: Scaling llm evaluation across 285 graduate disciplines.\narXiv preprint arXiv:2502.14739, 2025.\nDheeru Dua, Yizhong Wang, Pradeep Dasigi, Gabriel Stanovsky, Sameer Singh, and Matt Gardner. Drop:\nA reading comprehension benchmark requiring discrete reasoning over paragraphs. arXiv preprint\narXiv:1903.00161, 2019.\nAryo Pradipta Gema, Joshua Ong Jun Leang, Giwon Hong, Alessio Devoto, Alberto Carlo Maria Mancino,\nRohit Saxena, Xuanli He, Yu Zhao, Xiaotang Du, Mohammad Reza Ghasemi Madani, et al. Are we\ndone with mmlu? arXiv preprint arXiv:2406.04127, 2024.\nDaya Guo, Dejian Yang, Haowei Zhang, Junxiao Song, Ruoyu Zhang, Runxin Xu, Qihao Zhu, Shirong Ma,\nPeiyi Wang, Xiao Bi, et al. DeepSeek-R1: Incentivizing reasoning capability in LLMs via reinforcement\nlearning. arXiv preprint arXiv:2501.12948, 2025.\nChaoqun He, Renjie Luo, Yuzhuo Bai, Shengding Hu, Zhen Leng Thai, Junhao Shen, Jinyi Hu, Xu Han,\nYujie Huang, Yuxiang Zhang, et al. Olympiadbench: A challenging benchmark for promoting agi with\nolympiad-level bilingual multimodal scientific problems. arXiv preprint arXiv:2402.14008, 2024.\nDan Hendrycks, Collin Burns, Saurav Kadavath, Akul Arora, Steven Basart, Eric Tang, Dawn Song, and\nJacob Steinhardt. Measuring mathematical problem solving with the math dataset. arXiv preprint\narXiv:2103.03874, 2021.\n12\n\nCheng-Ping Hsieh, Simeng Sun, Samuel Kriman, Shantanu Acharya, Dima Rekesh, Fei Jia, Yang Zhang,\nand Boris Ginsburg. Ruler: What‚Äôs the real context size of your long-context language models? arXiv\npreprint arXiv:2404.06654, 2024.\nTencent Hunyuan. PenguinScrolls: A user-aligned fine-grained benchmark for long-context language\nmodel evaluation, 2024. URL https://github.com/Penguin-Scrolls/PenguinScrolls.\nNaman Jain, King Han, Alex Gu, Wen-Ding Li, Fanjia Yan, Tianjun Zhang, Sida Wang, Armando Solar-\nLezama, Koushik Sen, and Ion Stoica. Livecodebench: Holistic and contamination free evaluation of\nlarge language models for code. arXiv preprint arXiv:2403.07974, 2024.\nSatyapriya Krishna, Kalpesh Krishna, Anhad Mohananey, Steven Schwarcz, Adam Stambler, Shyam\nUpadhyay, and Manaal Faruqui. Fact, fetch, and reason: A unified evaluation of retrieval-augmented\ngeneration, 2024. URL https://arxiv.org/abs/2409.12941.\nHunter Lightman, Vineet Kosaraju, Yuri Burda, Harrison Edwards, Bowen Baker, Teddy Lee, Jan Leike,\nJohn Schulman, Ilya Sutskever, and Karl Cobbe. Let‚Äôs verify step by step. In The Twelfth International\nConference on Learning Representations, 2023.\nBill Yuchen Lin, Ronan Le Bras, Kyle Richardson, Ashish Sabharwal, Radha Poovendran, Peter Clark, and\nYejin Choi. Zebralogic: On the scaling limits of llms for logical reasoning. arXiv preprint arXiv:2502.01100,\n2025a.\nBill Yuchen Lin, Ronan Le Bras, Kyle Richardson, Ashish Sabharwal, Radha Poovendran, Peter Clark,\nand Yejin Choi. Zebralogic: On the scaling limits of llms for logical reasoning, 2025b. URL https:\n//arxiv.org/abs/2502.01100.\nAo Liu, Botong Zhou, Can Xu, Chayse Zhou, ChenChen Zhang, Chengcheng Xu, Chenhao Wang,\nDecheng Wu, Dengpeng Wu, Dian Jiao, et al. Hunyuan-turbos: Advancing large language models\nthrough mamba-transformer synergy and adaptive chain-of-thought. arXiv preprint arXiv:2505.15431,\n2025.\nJiawei Liu, Chunqiu Steven Xia, Yuyao Wang, and Lingming Zhang. Is your code generated by chatgpt\nreally correct? rigorous evaluation of large language models for code generation. Advances in Neural\nInformation Processing Systems, 36:21558‚Äì21572, 2023.\nMichael Luo, Sijun Tan, Justin Wong, Xiaoxiang Shi, William Y. Tang, Manan Roongta, Colin Cai, Jeffrey\nLuo, Li Erran Li, Raluca Ada Popa, and Ion Stoica. Deepscaler: Surpassing o1-preview with a 1.5b\nmodel by scaling rl. https://pretty-radio-b75.notion.site/DeepScaleR-Surpassing-O1-Preview\n-with-a-1-5B-Model-by-Scaling-RL-19681902c1468005bed8ca303013a4e2, 2025. Notion Blog.\nDakota Mahan, Duy Van Phung, Rafael Rafailov, Chase Blagden, Nathan Lile, Louis Castricato, Jan-\nPhilipp Fr¬®anken, Chelsea Finn, and Alon Albalak.\nGenerative reward models.\narXiv preprint\narXiv:2410.12832, 2024.\nOpenAI. Introducing openai o1, 2024. URL https://openai.com/o1/.\nOpenAI. Hello GPT-4o, 2024. URL https://openai.com/index/hello-gpt-4o/.\nOpenAI. Introducing openai o3 and o4-mini, 2025. URL https://openai.com/index/introducing-o3-a\nnd-o4-mini/.\nBowen Peng and Jeffrey Quesnelle. Ntk-aware scaled rope allows llama models to have extended (8k+)\ncontext size without any fine-tuning and minimal perplexity degradation, 2023.\nYanzhao Qin, Tao Zhang, Yanjun Shen, Wenjing Luo, Haoze Sun, Yan Zhang, Yujing Qiao, Weipeng Chen,\nZenan Zhou, Wentao Zhang, et al. Sysbench: Can large language models follow system messages?\narXiv preprint arXiv:2408.10943, 2024.\nDavid Rein, Betty Li Hou, Asa Cooper Stickland, Jackson Petty, Richard Yuanzhe Pang, Julien Dirani,\nJulian Michael, and Samuel R Bowman. Gpqa: A graduate-level google-proof q&a benchmark. In First\nConference on Language Modeling, 2024.\nZhihong Shao, Peiyi Wang, Qihao Zhu, Runxin Xu, Junxiao Song, Xiao Bi, Haowei Zhang, Mingchuan\nZhang, YK Li, Y Wu, et al. Deepseekmath: Pushing the limits of mathematical reasoning in open\nlanguage models. arXiv preprint arXiv:2402.03300, 2024.\nNoam Shazeer. GLU variants improve transformer. arXiv preprint arXiv:2002.05202, 2020.\n13\n\nXingwu Sun, Yanfeng Chen, Yiqing Huang, Ruobing Xie, Jiaqi Zhu, Kai Zhang, Shuaipeng Li, Zhen Yang,\nJonny Han, Xiaobo Shu, et al. Hunyuan-large: An open-source moe model with 52 billion activated\nparameters by tencent. arXiv preprint arXiv:2411.02265, 2024.\nMirac Suzgun, Nathan Scales, Nathanael Sch¬®arli, Sebastian Gehrmann, Yi Tay, Hyung Won Chung,\nAakanksha Chowdhery, Quoc V Le, Ed H Chi, Denny Zhou, et al. Challenging big-bench tasks and\nwhether chain-of-thought can solve them. arXiv preprint arXiv:2210.09261, 2022.\nYubo Wang, Xueguang Ma, Ge Zhang, Yuansheng Ni, Abhranil Chandra, Shiguang Guo, Weiming Ren,\nAaran Arulraj, Xuan He, Ziyan Jiang, et al. MMLU-Pro: A more robust and challenging multi-task\nlanguage understanding benchmark. In Proceedings of NeurIPS, 2024.\nTianwen Wei, Jian Luan, Wei Liu, Shuang Dong, and Bin Wang. CMATH: Can your language model pass\nchinese elementary school math test? arXiv preprint arXiv:2306.16636, 2023.\nYuxiang Wei, Zhe Wang, Jiawei Liu, Yifeng Ding, and Lingming Zhang. Magicoder: Empowering code\ngeneration with OSS-instruct. In Proceedings of the 41st International Conference on Machine Learning,\nvolume 235 of Proceedings of Machine Learning Research, pp. 52632‚Äì52657. PMLR, 21‚Äì27 Jul 2024. URL\nhttps://proceedings.mlr.press/v235/wei24h.html.\nWenyuan Xu, Xiaochen Zuo, Chao Xin, Yu Yue, Lin Yan, and Yonghui Wu. A unified pairwise framework\nfor rlhf: Bridging generative reward modeling and policy optimization, 2025. URL https://arxiv.or\ng/abs/2504.04950.\nAn Yang, Anfeng Li, Baosong Yang, Beichen Zhang, Binyuan Hui, Bo Zheng, Bowen Yu, Chang Gao,\nChengen Huang, Chenxu Lv, Chujie Zheng, Dayiheng Liu, Fan Zhou, Fei Huang, Feng Hu, Hao Ge,\nHaoran Wei, Huan Lin, Jialong Tang, Jian Yang, Jianhong Tu, Jianwei Zhang, Jianxin Yang, Jiaxi Yang,\nJing Zhou, Jingren Zhou, Junyang Lin, Kai Dang, Keqin Bao, Kexin Yang, Le Yu, Lianghao Deng,\nMei Li, Mingfeng Xue, Mingze Li, Pei Zhang, Peng Wang, Qin Zhu, Rui Men, Ruize Gao, Shixuan\nLiu, Shuang Luo, Tianhao Li, Tianyi Tang, Wenbiao Yin, Xingzhang Ren, Xinyu Wang, Xinyu Zhang,\nXuancheng Ren, Yang Fan, Yang Su, Yichang Zhang, Yinger Zhang, Yu Wan, Yuqiong Liu, Zekun\nWang, Zeyu Cui, Zhenru Zhang, Zhipeng Zhou, and Zihan Qiu. Qwen3 technical report, 2025a. URL\nhttps://arxiv.org/abs/2505.09388.\nAn Yang, Anfeng Li, Baosong Yang, Beichen Zhang, Binyuan Hui, Bo Zheng, Bowen Yu, Chang Gao,\nChengen Huang, Chenxu Lv, et al. Qwen3 technical report. arXiv preprint arXiv:2505.09388, 2025b.\nAn Yang, Baosong Yang, Beichen Zhang, Binyuan Hui, Bo Zheng, Bowen Yu, Chengyuan Li, Dayiheng\nLiu, Fei Huang, Haoran Wei, Huan Lin, Jian Yang, Jianhong Tu, Jianwei Zhang, Jianxin Yang, Jiaxi Yang,\nJingren Zhou, Junyang Lin, Kai Dang, Keming Lu, Keqin Bao, Kexin Yang, Le Yu, Mei Li, Mingfeng\nXue, Pei Zhang, Qin Zhu, Rui Men, Runji Lin, Tianhao Li, Tianyi Tang, Tingyu Xia, Xingzhang Ren,\nXuancheng Ren, Yang Fan, Yang Su, Yichang Zhang, Yu Wan, Yuqiong Liu, Zeyu Cui, Zhenru Zhang,\nand Zihan Qiu. Qwen2.5 technical report, 2025c. URL https://arxiv.org/abs/2412.15115.\nShunyu Yao, Noah Shinn, Pedram Razavi, and Karthik Narasimhan. œÑ-bench: A benchmark for tool-\nagent-user interaction in real-world domains. In The Thirteenth International Conference on Learning\nRepresentations, 2025.\nPeijie Yu, Yifan Yang, Jinjian Li, Zelong Zhang, Haorui Wang, Xiao Feng, and Feng Zhang. c3-bench: The\nthings real disturbing llm based agent in multi-tasking. arXiv preprint arXiv:2505.18746, 2025. URL\nhttps://arxiv.org/abs/2505.18746.\nQiying Yu, Zheng Zhang, Ruofei Zhu, Yufeng Yuan, Xiaochen Zuo, Yu Yue, Tiantian Fan, Gaohong Liu,\nLingjun Liu, Xin Liu, et al. Dapo: An open-source llm reinforcement learning system at scale, 2025.\nURL https://arxiv. org/abs/2503.14476.\nLunjun Zhang, Arian Hosseini, Hritik Bansal, Mehran Kazemi, Aviral Kumar, and Rishabh Agarwal.\nGenerative verifiers: Reward modeling as next-token prediction. arXiv preprint arXiv:2408.15240, 2024.\nLucen Zhong, Zhengxiao Du, Xiaohan Zhang, Haiyi Hu, and Jie Tang. Complexfuncbench: Exploring\nmulti-step and constrained function calling under long-context scenario. arXiv preprint arXiv:2501.10132,\n2025.\nJeffrey Zhou, Tianjian Lu, Swaroop Mishra, Siddhartha Brahma, Sujoy Basu, Yi Luan, Denny Zhou, and\nLe Hou. Instruction-following evaluation for large language models. arXiv preprint arXiv:2311.07911,\n2023.\n14\n",
      "fetch_method": "direct-pdf"
    }
  ]
}