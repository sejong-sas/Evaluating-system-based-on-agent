{
  "2-3 (API)": "The material explicitly notes that Cohere exposes Command-series models through an \"API or enterprise contexts.\"  Because the authors deliberately evaluate \"without a system preamble to simulate cases outside of Cohere’s API,\" the text confirms (a) the existence of a production-grade Cohere API through which users normally invoke coherelabs/c4ai-command-a-03-2025, and (b) that the same model can also be run in settings that bypass the default, safety-oriented system preamble applied by the API.  No further implementation details, documentation links, pricing, or example endpoints are provided in the supplied quotation.",
  "3-1 (Pre-training)": "According to the quotations, the pre-training of coherelabs/c4ai-command-a-03-2025 (\"Command A\" series) relies on a large, multilingual mixture of data types and multiple quality-control passes.  The corpus is assembled from: (1) publicly scraped text and code from the web; (2) internally generated synthetic datasets; (3) instruction-tuning sets created by human annotators; and (4) \"high-quality data\" purchased from specialised vendors.  To rebalance topic coverage the team \"enhance[s] the ratio of educational samples\"—material considered sparse on the public internet—while simultaneously down-sampling low-quality content that has been identified through ML-based quality classifiers.  The pipeline also performs careful de-duplication and safety-oriented heuristic filtering before training.  Long-context ability is cultivated by sampling segments up to 8 192 tokens from the pre-training corpus and prompting another internal model (\"Command R+ Refresh\") to generate paired question-answer examples, thereby enriching the training set with context-length-aware supervision data.",
  "3-2 (Fine-tuning)": "After base pre-training, coherelabs/c4ai-command-a-03-2025 undergoes an extensive, multi-stage \"post-training\" regimen that alternates between centralised and decentralised phases.  In centralised phases a single checkpoint is fine-tuned; in decentralised phases multiple specialised \"expert\" models are trained in parallel to maximise performance on different domains.  Their parameters are subsequently merged—first into a supervised-fine-tuning (SFT) \"model soup\" that collects the outputs of experts trained with standard supervised learning, then into an \"off-pref soup\" that combines experts optimised with offline preference-based objectives.  A dedicated stage called the \"Instruct Model\" is produced early in the workflow, giving the base model core instruction-following capabilities via supervised learning.  Overall, the process embodies a sequential recipe of SFT followed by preference-tuning, with model-merging used at two decisive checkpoints to consolidate diverse capabilities contributed by separate teams working asynchronously.",
  "3-3 (Reinforcement Learning)": "The reinforcement-learning portion of the post-training pipeline creates six \"RL Expert Models\" by starting from the SFT Soup checkpoint and applying RL algorithms tailored to each target domain.  Depending on the domain, the experts are trained with either pairwise human-preference comparisons or objective, \"verifiable\" reward functions.  Training then proceeds in a \"ping-pong\" cycle that alternates between offline preference optimisation and online RL (labelled CoPG/SRPO in the cited figure).  At each iteration human-preference win-rates are measured—e.g., against GPT-4o baselines—and the back-and-forth continues until those win-rates plateau, yielding the final Command A model.  The authors emphasise that interleaving the two optimisation styles improves stability, because regressions introduced by one phase are likely to be corrected in the next.",
  "2-3 (API)__evidence": [
    {
      "source": "[sections/Default Safety]",
      "quote": "In the default setting, we evaluate the safety of the model without a system preamble to simulate cases outside of Cohere’s API or enterprise contexts. Command A shows strong performance in various categories of unsafe content."
    }
  ],
  "3-1 (Pre-training)__evidence": [
    {
      "source": "[pdf_text]",
      "quote": "Command A models are trained on multilingual data (also see Section 3.3.3.1) from various sources including publicly available text and code data from the web, a collection of synthetic datasets generated internally, instruction-tuning datasets obtained from human annotators, and high quality data sourced from specialised data vendors. We optimise the web text data by enhancing the ratio of educational samples that are relatively sparse on the internet, and down-sampling low-quality samples identified by Machine Learning (ML)-based quality filters after careful de-duplication and heuristic filtering for safety and quality."
    },
    {
      "source": "[sections/2504.00698]",
      "quote": "Command A models are trained on multilingual data (also see Section 3.3.3.1) from various sources including publicly available text and code data from the web, a collection of synthetic datasets generated internally, instruction-tuning datasets obtained from human annotators, and high quality data sourced from specialised data vendors. We optimise the web text data by enhancing the ratio of educational samples that are relatively sparse on the internet, and down-sampling low-quality samples identified by Machine Learning (ML)-based quality filters after careful de-duplication and heuristic filtering for safety and quality."
    },
    {
      "source": "[sections/Long Context Data]",
      "quote": "We sample from our long-context pretraining dataset and prompt Command R+ Refresh to generate question-answer pairs based on randomly selected fragments within 8,192 tokens (Xiong et al., 2024)."
    }
  ],
  "3-2 (Fine-tuning)__evidence": [
    {
      "source": "[pdf_text]",
      "quote": "Command A is post-trained using a novel decentralised approach to maximise and control its performance over a wide spectrum of domains and capability areas. More precisely, Command A is trained by alternating centralised training stages, where a single model is fine-tuned, and decentralised training stages, where multiple expert models are trained separately to maximise domain-wise performance before merging their parameters."
    },
    {
      "source": "[pdf_text]",
      "quote": "We divide the global Command A post-training recipe into several sub-stages, each producing intermediary model artifacts: • Instruct Model: We train an initial Instruct model with supervised learning on top of the base model to provide the core basic capabilities of the model."
    },
    {
      "source": "[sections/Expert Merging]",
      "quote": "Model merging enables multiple teams to work asynchronously on improving different capabilities, with their contributions merged together in parameter space. The capabilities exhibited by Command A cover a wide range of data scales, that would be non-trivial to combine into a single dataset and optimisation strategy. We apply merging at two points in the overall training pipeline: firstly, to combine a set of expert models trained using SFT into an initial ‘SFT model soup’; secondly, to combine a set of experts that were trained using offline preference optimisation techniques on top of the SFT soup, giving an ‘off-pref soup’."
    },
    {
      "source": "[sections/2504.00698]",
      "quote": "We divide the global Command A post-training recipe into several sub-stages, each producing intermediary model artifacts: • Instruct Model: We train an initial Instruct model with supervised learning on top of the base model to provide the core basic capabilities of the model."
    },
    {
      "source": "[sections/2504.00698]",
      "quote": "In the context of instruction-following we post-train Command A in sequence with SFT and preference tuning."
    }
  ],
  "3-3 (Reinforcement Learning)__evidence": [
    {
      "source": "[pdf_text]",
      "quote": "We divide the global Command A post-training recipe into several sub-stages, each producing intermediary model artifacts: • RL Expert Models: We train six RL experts on top of the SFT Soup checkpoint using RL algorithms tailored to each domain, using pairwise comparisons or verifiable rewards."
    },
    {
      "source": "[sections/Polishing]",
      "quote": "In Figure 13, we show Command A win rates against GPT-4o (1120) according to a pool of LLM judges at each phase of the CoPG/SRPO ping-pong. While this method allows us to achieve high win rates, we also observe that interleaving the two methods tends to increase overall training stability: a potential regression occurring at a particular phase is likely to be corrected by the next one."
    },
    {
      "source": "[sections/2504.00698]",
      "quote": "We divide the global Command A post-training recipe into several sub-stages, each producing intermediary model artifacts: • RL Expert Models: We train six RL experts on top of the SFT Soup checkpoint using RL algorithms tailored to each domain, using pairwise comparisons or verifiable rewards."
    },
    {
      "source": "[sections/2504.00698]",
      "quote": "Then, we alternate between offline preference and online RL optimisation in a ping-pong approach, iterating as required until we observe a human preference performance plateau, to obtain the final Command A model."
    },
    {
      "source": "[sections/Polishing]",
      "quote": "In Figure 13, we show Command A win rates against GPT-4o (1120) according to a pool of LLM judges at each phase of the CoPG/SRPO ping-pong."
    }
  ]
}