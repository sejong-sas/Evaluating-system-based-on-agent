{
  "4-1 (Pre-training Data)": "The pre-training corpus for coherelabs/c4ai-command-a-03-2025 (referred to in the documentation as “Command A”) is intentionally broad and multilingual. According to the quoted material, it is assembled from four main streams: (1) publicly-available web text and source-code, (2) internally-generated synthetic data, (3) instruction-tuning sets written by human annotators, and (4) specialised, high-quality data purchased from commercial vendors. A long-context sub-corpus is also maintained; fragments of up to 8,192 tokens are sampled from this set and then used to prompt a sister model (“Command R+ Refresh”) to create additional question–answer pairs that become part of the pre-training mix. The engineering team employs a “model-merging” approach so that different research groups can train partial models on different slices of the data and later merge their checkpoints in parameter space. This strategy lets them combine capabilities that arise from very different data scales without having to concatenate everything into one monolithic dataset or adopt a single optimisation schedule.",
  "4-2 (Fine-tuning Data)": "For supervised fine-tuning, the team concentrates on boosting code-related and multilingual instruction-following skills. One major component is a code-centric mixture that emphasises eight languages—Python, Java, C++, Rust, Go, JavaScript, TypeScript and COBOL—as well as five SQL dialects (SQLite, MySQL, PostgreSQL, Microsoft T-SQL and Oracle PL/SQL). Separate synthetic pipelines generate diverse “preambles” (policy, style or context directives) that are prepended to real user prompts so that the model learns to respect pre-conversation instructions. Beyond code, the post-training recipe deliberately covers many domains: factual Q&A, formatting tasks, STEM reasoning, tabular or other structured data manipulation, and general compliance. Multilingual quality is improved through iterative, synthetic data harvesting that uses a best-of-N sampling strategy; the method repeats generation in multiple languages, ranks the outputs and keeps only the top candidates for further fine-tuning.",
  "4-3 (Reinforcement Learning Data)": "Reinforcement learning is applied after supervised fine-tuning in a multi-stage post-training pipeline. The process starts from an SFT “Soup” checkpoint and branches into six separate RL experts, each optimised for one domain with an RL algorithm that is explicitly ‘tailored’ to that domain. The quoted text indicates two reward-collection methods: (i) pairwise human or model comparisons, and (ii) automatically-verifiable reward functions. Although no concrete datasets are listed, the structure implies that each RL expert gathers or is fed preference-comparison data specific to its specialty before the experts are later merged or distilled back into the main Command A model.",
  "4-4 (Data Filtering)": "The documentation reports a multi-layered filtering regime. During raw-web corpus construction, the team raises the proportion of educational documents—judged to be under-represented online—while aggressively down-sampling low-quality pages. The down-sampling decision is driven by ML-based quality classifiers that run after a de-duplication pass and a sequence of safety-oriented heuristic rules. For prompt-generation pipelines, they use the sister model “Command R+ Refresh” to automatically score or remove prompts along several complexity axes; although they concede the filters may be overly strict in edge cases, empirical checks show that the surviving prompts remain challenging for state-of-the-art systems. When building long-context training examples, multiple candidate answers are produced and a proprietary reward model selects the single best answer for inclusion, effectively acting as another quality-filtering stage.",
  "4-1 (Pre-training Data)__evidence": [
    {
      "source": "[pdf_text]",
      "quote": "Command A models are trained on multilingual data (also see Section 3.3.3.1) from various sources including publicly available text and code data from the web, a collection of synthetic datasets generated internally, instruction-tuning datasets obtained from human annotators, and high quality data sourced from specialised data vendors."
    },
    {
      "source": "[sections/Expert Merging]",
      "quote": "Model merging enables multiple teams to work asynchronously on improving different capabilities, with their contributions merged together in parameter space. The capabilities exhibited by Command A cover a wide range of data scales, that would be non-trivial to combine into a single dataset and optimisation strategy."
    },
    {
      "source": "[sections/2.2 Data]",
      "quote": "Command A models are trained on multilingual data (also see Section 3.3.3.1) from various sources including publicly available text and code data from the web, a collection of synthetic datasets generated internally, instruction-tuning datasets obtained from human annotators, and high quality data sourced from specialised data vendors."
    },
    {
      "source": "[sections/Long Context]",
      "quote": "We sample from our long-context pretraining dataset and prompt Command R+ Refresh to generate question-answer pairs based on randomly selected fragments within 8,192 tokens (Xiong et al., 2024)."
    }
  ],
  "4-2 (Fine-tuning Data)__evidence": [
    {
      "source": "[pdf_text]",
      "quote": "Generating and understanding code is a fundamental requirement for any enterprise LLM. We invest in the code capabilities of Command A to assist the software development cycle and improve user coding experience. Data Mixture. Our data mix focuses on 8 priority programming languages (Python, Java, C++, Rust, Go, JavaScript, TypeScript, COBOL) and 5 dialects of SQL (SQLite, MySQL, PostgreSQL, Microsoft T-SQL, Oracle PL/SQL)."
    },
    {
      "source": "[pdf_text]",
      "quote": "To train Command A to follow preamble instructions, we develop methods based on synthetic data generation to create diverse preambles that are attached to prompts flowing into the above-described pipeline."
    },
    {
      "source": "[sections/3.3.1 Instruction-Following]",
      "quote": "As such, in the Command A post-training recipe, we teach the model to follow instructions across a wide range of topics and domains, including but not limited to generalist instruction-following (e.g., factual knowledge requests), formatting, STEM-specific tasks (e.g., tabular reasoning, structured data manipulation), and preamble compliance."
    },
    {
      "source": "[sections/3.3.3 Multilingual]",
      "quote": "To further improve the multilingual quality of Command A, we conduct iterative synthetic data collection through multilingual best-of-N (Stiennon et al., 2020; Eisenstein et al., 2024)."
    }
  ],
  "4-3 (Reinforcement Learning Data)__evidence": [
    {
      "source": "[pdf_text]",
      "quote": "We divide the global Command A post-training recipe into several sub-stages, each producing intermediary model artifacts: • RL Expert Models: We train six RL experts on top of the SFT Soup checkpoint using RL algorithms tailored to each domain, using pairwise comparisons or verifiable rewards."
    },
    {
      "source": "[sections/3 Post-training Overview]",
      "quote": "We divide the global Command A post-training recipe into several sub-stages, each producing intermediary model artifacts: … • RL Expert Models: We train six RL experts on top of the SFT Soup checkpoint using RL algorithms tailored to each domain, using pairwise comparisons or verifiable rewards."
    }
  ],
  "4-4 (Data Filtering)__evidence": [
    {
      "source": "[pdf_text]",
      "quote": "Command A models are trained on multilingual data (also see Section 3.3.3.1) from various sources including publicly available text and code data from the web, a collection of synthetic datasets generated internally, instruction-tuning datasets obtained from human annotators, and high quality data sourced from specialised data vendors. We optimise the web text data by enhancing the ratio of educational samples that are relatively sparse on the internet, and down-sampling low-quality samples identified by Machine Learning (ML)-based quality filters after careful de-duplication and heuristic filtering for safety and quality."
    },
    {
      "source": "[sections/4.11.1 Evaluation Data]",
      "quote": "We automatically filter prompts with the help of Command R+ Refresh along several axes, building up a more comprehensive notion of prompt complexity. While the filtering may be overly strict in some instances, our findings show that the resulting pool of prompts is sufficiently difficult even for state-of-the-art models."
    },
    {
      "source": "[sections/2.2 Data]",
      "quote": "Command A models are trained on multilingual data … . We optimise the web text data by enhancing the ratio of educational samples that are relatively sparse on the internet, and down-sampling low-quality samples identified by Machine Learning (ML)-based quality filters after careful de-duplication and heuristic filtering for safety and quality."
    },
    {
      "source": "[sections/Long Context]",
      "quote": "We sample from our long-context pretraining dataset and prompt Command R+ Refresh to generate question-answer pairs based on randomly selected fragments within 8,192 tokens (Xiong et al., 2024). To ensure high-quality, we use our reward model to select the best generation from a pool of candidates."
    },
    {
      "source": "[sections/Human Evaluation]",
      "quote": "We automatically filter prompts with the help of Command R+ Refresh along several axes, building up a more comprehensive notion of prompt complexity."
    }
  ]
}