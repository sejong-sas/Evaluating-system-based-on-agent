{
  "2-3 (API)": "All publicly-exposed interaction pathways described for coherelabs/c4ai-command-a-03-2025 are delivered through the standard Hugging Face Transformers API. The model can be pulled with the usual Python loaders: AutoTokenizer.from_pretrained() and AutoModelForCausalLM.from_pretrained(), using the model identifier \"CohereForAI/c4ai-command-a-03-2025\". The documentation explains how to structure system instructions and how to invoke the model in multiple usage scenarios—Chat, Retrieval-Augmented Generation (RAG), Tool Use and Agent frameworks—directly from the Hugging Face “Copy” page. For grounded (document-aware) generation, the SDK exposes apply_chat_template(); users pass an explicit documents parameter that injects external snippets into the prompt. The same chat-template mechanism underlies tool invocation: specialised prompt blocks can be generated for tools, and model outputs are post-processed with tool results. Therefore, every quoted instruction shows that Command A is accessible as a hosted model endpoint or local checkpoint through the standard, public HF API rather than a private library.",
  "3-1 (Pre-training)": "The authors characterise Command A as a \"powerful large language model\" created for real-world enterprise tasks through a decentralised training approach. The pre-training phase combines self-refinement algorithms and model-merging techniques. A dedicated technical report \"details our original training pipeline\" and provides an \"extensive evaluation\" across enterprise benchmarks, emphasising both performance and efficiency. No explicit numeric hyper-parameters are disclosed in the quotes, but the pipeline description establishes that Command A’s base capability arises from self-refinement (iterative self-improvement) plus the aggregation of multiple sub-models via model merging within a distributed training workflow.",
  "3-2 (Fine-tuning)": "After the base model is built, Command A undergoes multiple rounds of supervised fine-tuning and preference fine-tuning. The objective set includes summarisation, the final step of Retrieval-Augmented Generation (RAG), and explicit tool-use behaviours. The same blend of supervised data and preference signals, coupled with a \"specific prompt template,\" is applied to teach structured tool calls. Hence, the fine-tuning stage is task-oriented: it aligns the model with enterprise-centric downstream goals (summaries, RAG answers, tool workflows) through supervised instruction datasets complemented by preference optimisation.",
  "3-3 (Reinforcement Learning)": "Preference fine-tuning (i.e., learning from preference comparisons, a form of RLHF-style optimisation) is repeatedly highlighted as a core component of Command A’s post-training stack. Both its summarisation/RAG abilities and its tool-use policies are \"trained into the model via a mixture of supervised fine-tuning and preference fine-tuning.\" Although concrete reward functions or batch sizes are not enumerated, the text confirms that preference-based reinforcement steps are interleaved with supervised updates to sculpt model behaviour toward user-preferred outputs.",
  "2-3 (API)__evidence": [
    {
      "source": "[sections/Using Command A on Hugging Face]",
      "quote": "Grounded generation in Command A is supported through chat templates in Transformers. Simply provide document snippets using the documents parameter of Hugging Face’s apply_chat_template()."
    },
    {
      "source": "[sections/https://docs.cohere.com/docs/command-a-hf#obtaining-non-interactive-behavior]",
      "quote": "Using Command A on Hugging Face Copy page This page contains detailed instructions about: How to set system instructions for Command A in Hugging Face How to run Command A in Hugging Face for Chat, RAG, Tool Use and Agents use cases."
    },
    {
      "source": "[sections/https://docs.cohere.com/docs/command-a-hf#obtaining-non-interactive-behavior]",
      "quote": "Grounded generation in Command A is supported through chat templates in Transformers. Simply provide document snippets using the documents parameter of Hugging Face’s apply_chat_template() ."
    },
    {
      "source": "[sections/https://docs.cohere.com/docs/command-a-hf#obtaining-non-interactive-behavior]",
      "quote": "Tool use in Command A is supported through chat templates in Transformers."
    },
    {
      "source": "[sections/https://huggingface.co/docs/transformers/main/en/chat_templating#advanced-retrieval-augmented-generation]",
      "quote": "Usage: Generate the Tool Use prompt with tool results in the conversation  PYTHON 1 from transformers import AutoTokenizer, AutoModelForCausalLM 2 3 # Load the model and tokenizer 4 model_id = \"CohereForAI/c4ai-command-a-03-2025\" 5 tokenizer = AutoTokenizer.from_pretrained(model_id) 6 model = AutoModelForCausalLM.from_pretrained(model_id)"
    }
  ],
  "3-1 (Pre-training)__evidence": [
    {
      "source": "[abstract]",
      "quote": "In this report we describe the development of Command A, a powerful large language model purpose-built to excel at real-world enterprise use cases. These abilities are achieved through a decentralised training approach, including self-refinement algorithms and model merging techniques."
    },
    {
      "source": "[sections/https://arxiv.org/abs/2504.00698]",
      "quote": "In this report we describe the development of Command A, a powerful large language model purpose-built to excel at real-world enterprise use cases. These abilities are achieved through a decentralised training approach, including self-refinement algorithms and model merging techniques."
    },
    {
      "source": "[sections/https://arxiv.org/abs/2504.00698]",
      "quote": "This technical report details our original training pipeline and presents an extensive evaluation of our models across a suite of enterprise-relevant tasks and public benchmarks, demonstrating excellent performance and efficiency."
    }
  ],
  "3-2 (Fine-tuning)__evidence": [
    {
      "source": "[sections/Using Command A on Hugging Face]",
      "quote": "Command A has been trained specifically for tasks like summarization and the final step of Retrieval Augmented Generation (RAG). This behavior has been trained into the model via a mixture of supervised fine-tuning and preference fine-tuning."
    },
    {
      "source": "[sections/https://docs.cohere.com/docs/command-a-hf#obtaining-non-interactive-behavior]",
      "quote": "Command A has been trained specifically for tasks like summarization and the final step of Retrieval Augmented Generation (RAG). This behavior has been trained into the model via a mixture of supervised fine-tuning and preference fine-tuning."
    },
    {
      "source": "[sections/https://docs.cohere.com/docs/command-a-hf#obtaining-non-interactive-behavior]",
      "quote": "These tool use capabilities have been trained into the model via a mixture of supervised fine-tuning and preference fine-tuning, using a specific prompt template."
    }
  ],
  "3-3 (Reinforcement Learning)__evidence": [
    {
      "source": "[sections/Using Command A on Hugging Face]",
      "quote": "Command A has been trained specifically for tasks like summarization and the final step of Retrieval Augmented Generation (RAG). This behavior has been trained into the model via a mixture of supervised fine-tuning and preference fine-tuning."
    },
    {
      "source": "[sections/https://docs.cohere.com/docs/command-a-hf#obtaining-non-interactive-behavior]",
      "quote": "This behavior has been trained into the model via a mixture of supervised fine-tuning and preference fine-tuning."
    },
    {
      "source": "[sections/https://docs.cohere.com/docs/command-a-hf#obtaining-non-interactive-behavior]",
      "quote": "These tool use capabilities have been trained into the model via a mixture of supervised fine-tuning and preference fine-tuning, using a specific prompt template."
    }
  ]
}