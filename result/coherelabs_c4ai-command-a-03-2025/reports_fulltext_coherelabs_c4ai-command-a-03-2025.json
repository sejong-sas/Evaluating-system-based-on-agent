{
  "model_id": "coherelabs/c4ai-command-a-03-2025",
  "full_texts": [
    {
      "arxiv_id": "https://docs.cohere.com/docs/cohere-labs-acceptable-use-policy",
      "full_text": " Cohere Labs Acceptable Use Policy | Cohere Docs v2 API v2 API DASHBOARD PLAYGROUND DOCS COMMUNITY LOG IN Guides and concepts API Reference Release Notes LLMU Cookbooks Search / Ask AI Guides and concepts API Reference Release Notes LLMU Cookbooks Get Started Introduction Installation Creating a client Quickstart Playground FAQs Models An Overview of Cohere&#x27;s Models Command Embed Rerank Aya Text Generation Introduction to Text Generation at Cohere Using the Chat API Reasoning Image Inputs Streaming Responses Structured Outputs Predictable Outputs Advanced Generation Parameters Retrieval Augmented Generation (RAG) Tool Use Tokens and Tokenizers Summarizing Text Safety Modes Embeddings (Vectors, Search, Retrieval) Introduction to Embeddings at Cohere Semantic Search with Embeddings Multimodal Embeddings Batch Embedding Jobs Reranking Going to Production API Keys and Rate Limits Going Live Deprecations How Does Cohere&#x27;s Pricing Work? Integrations Integrating Embedding Models with Other Tools Cohere and LangChain LlamaIndex and Cohere Deployment Options Overview SDK Compatibility Private Deployment Cloud AI Services Tutorials Cookbooks LLM University Build Things with Cohere! Agentic RAG Cohere on Azure Responsible Use Security Usage Policy Command R and Command R+ Model Card Cohere Labs Cohere Labs Acceptable Use Policy More Resources Cohere Toolkit Datasets Improve Cohere Docs DASHBOARD PLAYGROUND DOCS COMMUNITY LOG IN Light Cohere Labs Cohere Labs Acceptable Use Policy Copy page We believe that independent and open machine learning research is vital to realizing the benefits of generative AI equitably and ensuring robust assessments of risks of generative AI use. We expect users of our models or model derivatives to comply with all applicable local and international laws and regulations. Additionally, you may not use or allow others to use our models or model derivatives in connection with any of the following strictly prohibited use cases: Violence and harm: engaging in, promoting, or inciting violence, threats, hate speech self-harm, sexual exploitation, or targeting of individuals based on protected characteristics. Harassment and abuse: engaging in, promoting, facilitating, or inciting activities that harass or abuse individuals or groups. Sexual exploitation, harm, or abuse: encouraging, facilitating, or carrying out any activities that exploit, abuse, or endanger individuals, and particularly children, including soliciting, creating, or distributing child exploitative content or Child Sexual Abuse Material. Sensitive information: collecting, disclosing, or inferring health, demographic, or other sensitive personal or private information about individuals without lawful authority or obtaining all rights and consents required by applicable laws. Fraud and deception: misrepresenting generated content from models as human-created or allowing individuals to create false identities for malicious purposes, deception, or to cause harm, through methods including: propagation of spam, fraudulent activities such as catfishing, phishing, or generation of false reviews; creation or promotion of false representations of or defamatory content about real people, such as deepfakes; or creation or promotion of intentionally false claims or misinformation. Synthetic data for commercial uses: generating synthetic data outputs for commercial purposes, including to train, improve, benchmark, enhance or otherwise develop model derivatives, or any products or services in connection with the foregoing. Was this page helpful? Yes No Edit this page Previous How to Start with the Cohere Toolkit Next Built with ",
      "fetch_method": "direct-html"
    },
    {
      "arxiv_id": "https://arxiv.org/abs/2504.00698",
      "full_text": " [2504.00698] Command A: An Enterprise-Ready Large Language Model Skip to main content We gratefully acknowledge support from the Simons Foundation, member institutions , and all contributors. Donate &gt; cs &gt; arXiv:2504.00698 Help | Advanced Search All fields Title Author Abstract Comments Journal reference ACM classification MSC classification Report number arXiv identifier DOI ORCID arXiv author ID Help pages Full text Search open search GO open navigation menu quick links Login Help Pages About --> Computer Science > Computation and Language arXiv:2504.00698 (cs) [Submitted on 1 Apr 2025 ( v1 ), last revised 14 Apr 2025 (this version, v2)] Title: Command A: An Enterprise-Ready Large Language Model Authors: Team Cohere : Aakanksha , Arash Ahmadian , Marwan Ahmed , Jay Alammar , Milad Alizadeh , Yazeed Alnumay , Sophia Althammer , Arkady Arkhangorodsky , Viraat Aryabumi , Dennis Aumiller , Raphaël Avalos , Zahara Aviv , Sammie Bae , Saurabh Baji , Alexandre Barbet , Max Bartolo , Björn Bebensee , Neeral Beladia , Walter Beller-Morales , Alexandre Bérard , Andrew Berneshawi , Anna Bialas , Phil Blunsom , Matt Bobkin , Adi Bongale , Sam Braun , Maxime Brunet , Samuel Cahyawijaya , David Cairuz , Jon Ander Campos , Cassie Cao , Kris Cao , Roman Castagné , Julián Cendrero , Leila Chan Currie , Yash Chandak , Diane Chang , Giannis Chatziveroglou , Hongyu Chen , Claire Cheng , Alexis Chevalier , Justin T. Chiu , Eugene Cho , Eugene Choi , Eujeong Choi , Tim Chung , Volkan Cirik , Ana Cismaru , Pierre Clavier , Henry Conklin , Lucas Crawhall-Stein , Devon Crouse , Andres Felipe Cruz-Salinas , Ben Cyrus , Daniel D&#39;souza , Hugo Dalla-Torre , John Dang , William Darling , Omar Darwiche Domingues , Saurabh Dash , Antoine Debugne , Théo Dehaze , Shaan Desai , Joan Devassy , Rishit Dholakia , Kyle Duffy , Ali Edalati , Ace Eldeib , Abdullah Elkady , Sarah Elsharkawy , Irem Ergün , Beyza Ermis , Marzieh Fadaee , Boyu Fan , Lucas Fayoux , Yannis Flet-Berliac , Nick Frosst , Matthias Gallé , Wojciech Galuba , Utsav Garg , Matthieu Geist , Mohammad Gheshlaghi Azar , Ellen Gilsenan-McMahon , Seraphina Goldfarb-Tarrant , Tomas Goldsack , Aidan Gomez , Victor Machado Gonzaga , Nithya Govindarajan , Manoj Govindassamy , Nathan Grinsztajn , Nikolas Gritsch , Patrick Gu , Shangmin Guo , Kilian Haefeli , Rod Hajjar , Tim Hawes , Jingyi He , Sebastian Hofstätter , Sungjin Hong , Sara Hooker , Tom Hosking , Stephanie Howe , Eric Hu , Renjie Huang , Hemant Jain , Ritika Jain , Nick Jakobi , Madeline Jenkins , JJ Jordan , Dhruti Joshi , Jason Jung , Trushant Kalyanpur , Siddhartha Rao Kamalakara , Julia Kedrzycki , Gokce Keskin , Edward Kim , Joon Kim , Wei-Yin Ko , Tom Kocmi , Michael Kozakov , Wojciech Kryściński , Arnav Kumar Jain , Komal Kumar Teru , Sander Land , Michael Lasby , Olivia Lasche , Justin Lee , Patrick Lewis , Jeffrey Li , Jonathan Li , Hangyu Lin , Acyr Locatelli , Kevin Luong , Raymond Ma , Lukáš Mach , Marina Machado , Joanne Magbitang , Brenda Malacara Lopez , Aryan Mann , Kelly Marchisio , Olivia Markham , Alexandre Matton , Alex McKinney , Dominic McLoughlin , Jozef Mokry , Adrien Morisot , Autumn Moulder , Harry Moynehan , Maximilian Mozes , Vivek Muppalla , Lidiya Murakhovska , Hemangani Nagarajan , Alekhya Nandula , Hisham Nasir , Shauna Nehra , Josh Netto-Rosen , Daniel Ohashi , James Owers-Bardsley , Jason Ozuzu , Dennis Padilla , Gloria Park , Sam Passaglia , Jeremy Pekmez , Laura Penstone , Aleksandra Piktus , Case Ploeg , Andrew Poulton , Youran Qi , Shubha Raghvendra , Miguel Ramos , Ekagra Ranjan , Pierre Richemond , Cécile Robert-Michon , Aurélien Rodriguez , Sudip Roy , Sebastian Ruder , Laura Ruis , Louise Rust , Anubhav Sachan , Alejandro Salamanca , Kailash Karthik Saravanakumar , Isha Satyakam , Alice Schoenauer Sebag , Priyanka Sen , Sholeh Sepehri , Preethi Seshadri , Ye Shen , Tom Sherborne , Sylvie Shang Shi , Sanal Shivaprasad , Vladyslav Shmyhlo , Anirudh Shrinivason , Inna Shteinbuk , Amir Shukayev , Mathieu Simard , Ella Snyder , Ava Spataru , Victoria Spooner , Trisha Starostina , Florian Strub , Yixuan Su , Jimin Sun , Dwarak Talupuru , Eugene Tarassov , Elena Tommasone , Jennifer Tracey , Billy Trend , Evren Tumer , Ahmet Üstün , Bharat Venkitesh , David Venuto , Pat Verga , Maxime Voisin , Alex Wang , Donglu Wang , Shijian Wang , Edmond Wen , Naomi White , Jesse Willman , Marysia Winkels , Chen Xia , Jessica Xie , Minjie Xu , Bowen Yang , Tan Yi-Chern , Ivan Zhang , Zhenyu Zhao , Zhoujie Zhao et al. (129 additional authors not shown) &nbsp;You must enable JavaScript to view entire author list. View a PDF of the paper titled Command A: An Enterprise-Ready Large Language Model, by Team Cohere: Aakanksha and 227 other authors View PDF Abstract: In this report we describe the development of Command A, a powerful large language model purpose-built to excel at real-world enterprise use cases. Command A is an agent-optimised and multilingual-capable model, with support for 23 languages of global business, and a novel hybrid architecture balancing efficiency with top of the range performance. It offers best-in-class Retrieval Augmented Generation (RAG) capabilities with grounding and tool use to automate sophisticated business processes. These abilities are achieved through a decentralised training approach, including self-refinement algorithms and model merging techniques. We also include results for Command R7B which shares capability and architectural similarities to Command A. Weights for both models have been released for research purposes. This technical report details our original training pipeline and presents an extensive evaluation of our models across a suite of enterprise-relevant tasks and public benchmarks, demonstrating excellent performance and efficiency. Comments: 55 pages Subjects: Computation and Language (cs.CL) ; Artificial Intelligence (cs.AI); Machine Learning (cs.LG) Cite as: arXiv:2504.00698 [cs.CL] &nbsp; (or arXiv:2504.00698v2 [cs.CL] for this version) &nbsp; https://doi.org/10.48550/arXiv.2504.00698 Focus to learn more arXiv-issued DOI via DataCite Submission history From: Seraphina Goldfarb-Tarrant [ view email ] [v1] Tue, 1 Apr 2025 12:08:07 UTC (4,689 KB) [v2] Mon, 14 Apr 2025 12:37:51 UTC (4,689 KB) Full-text links: Access Paper: View a PDF of the paper titled Command A: An Enterprise-Ready Large Language Model, by Team Cohere: Aakanksha and 227 other authors View PDF Other Formats view license Current browse context: cs.CL &lt;&nbsp;prev &nbsp; | &nbsp; next&nbsp;&gt; new | recent | 2025-04 Change to browse by: cs cs.AI cs.LG References &amp; Citations NASA ADS Google Scholar Semantic Scholar a export BibTeX citation Loading... BibTeX formatted citation &times; loading... Data provided by: Bookmark Bibliographic Tools Bibliographic and Citation Tools Bibliographic Explorer Toggle Bibliographic Explorer ( What is the Explorer? ) Connected Papers Toggle Connected Papers ( What is Connected Papers? ) Litmaps Toggle Litmaps ( What is Litmaps? ) scite.ai Toggle scite Smart Citations ( What are Smart Citations? ) Code, Data, Media Code, Data and Media Associated with this Article alphaXiv Toggle alphaXiv ( What is alphaXiv? ) Links to Code Toggle CatalyzeX Code Finder for Papers ( What is CatalyzeX? ) DagsHub Toggle DagsHub ( What is DagsHub? ) GotitPub Toggle Gotit.pub ( What is GotitPub? ) Huggingface Toggle Hugging Face ( What is Huggingface? ) Links to Code Toggle Papers with Code ( What is Papers with Code? ) ScienceCast Toggle ScienceCast ( What is ScienceCast? ) Demos Demos Replicate Toggle Replicate ( What is Replicate? ) Spaces Toggle Hugging Face Spaces ( What is Spaces? ) Spaces Toggle TXYZ.AI ( What is TXYZ.AI? ) Related Papers Recommenders and Search Tools Link to Influence Flower Influence Flower ( What are Influence Flowers? ) Core recommender toggle CORE Recommender ( What is CORE? ) Author Venue Institution Topic About arXivLabs arXivLabs: experimental projects with community collaborators arXivLabs is a framework that allows collaborators to develop and share new arXiv features directly on our website. Both individuals and organizations that work with arXivLabs have embraced and accepted our values of openness, community, excellence, and user data privacy. arXiv is committed to these values and only works with partners that adhere to them. Have an idea for a project that will add value for arXiv's community? Learn more about arXivLabs . Which authors of this paper are endorsers? | Disable MathJax ( What is MathJax? ) About Help contact arXiv Click here to contact arXiv Contact subscribe to arXiv mailings Click here to subscribe Subscribe Copyright Privacy Policy Web Accessibility Assistance arXiv Operational Status Get status notifications via email or slack ",
      "fetch_method": "direct-html"
    },
    {
      "arxiv_id": "https://docs.cohere.com/docs/command-a-hf#obtaining-non-interactive-behavior",
      "full_text": " Using Command A on Hugging Face | Cohere Docs v2 API v2 API DASHBOARD PLAYGROUND DOCS COMMUNITY LOG IN Guides and concepts API Reference Release Notes LLMU Cookbooks Search / Ask AI Guides and concepts API Reference Release Notes LLMU Cookbooks Get Started Introduction Installation Creating a client Quickstart Playground FAQs Models An Overview of Cohere&#x27;s Models Command Embed Rerank Aya Text Generation Introduction to Text Generation at Cohere Using the Chat API Reasoning Image Inputs Streaming Responses Structured Outputs Predictable Outputs Advanced Generation Parameters Retrieval Augmented Generation (RAG) Tool Use Tokens and Tokenizers Summarizing Text Safety Modes Embeddings (Vectors, Search, Retrieval) Introduction to Embeddings at Cohere Semantic Search with Embeddings Multimodal Embeddings Batch Embedding Jobs Reranking Going to Production API Keys and Rate Limits Going Live Deprecations How Does Cohere&#x27;s Pricing Work? Integrations Integrating Embedding Models with Other Tools Cohere and LangChain LlamaIndex and Cohere Deployment Options Overview SDK Compatibility Private Deployment Cloud AI Services Tutorials Cookbooks LLM University Build Things with Cohere! Agentic RAG Cohere on Azure Responsible Use Security Usage Policy Command R and Command R+ Model Card Cohere Labs Cohere Labs Acceptable Use Policy More Resources Cohere Toolkit Datasets Improve Cohere Docs Using Command A on Hugging Face DASHBOARD PLAYGROUND DOCS COMMUNITY LOG IN Light On this page Chat Capabilities Conversational Mode Obtaining non-interactive behavior Safety modes Grounded Generation and RAG Capabilities: Option 1: Grounded Generation Option 2: Regular Generation Tool use, Function Calling &amp; Agent capabilities Using Command A on Hugging Face Copy page This page contains detailed instructions about: How to set system instructions for Command A in Hugging Face How to run Command A in Hugging Face for Chat, RAG, Tool Use and Agents use cases. Chat Capabilities Conversational Mode Command A is configured as a conversational model, meaning it is optimized for interactive experiences, such as chatbots, where the model engages in dialogue. This kind of behavior is conditioned with a system message; system messages vary for different models, and in the case of Command A, it is written such that the model will reply in a conversational fashion, provide introductory statements and follow-up questions, and use Markdown as well as LaTeX where appropriate. The (conversational) system instruction for Command A looks like this: 1 # System Preamble 2 {Safety Preamble} 3 4 Your information cutoff date is June 2024. 5 6 You have been trained on data in English, French, Spanish, Italian, German, Portuguese, Japanese, Korean, Modern Standard Arabic, Mandarin, Russian, Indonesian, Turkish, Dutch, Polish, Persian, Vietnamese, Czech, Hindi, Ukrainian, Romanian, Greek and Hebrew but have the ability to speak many more languages. 7 8 # Default Preamble 9 The following instructions are your defaults unless specified elsewhere in a developer system message or user prompt. 10 - Your name is Command. 11 - You are a large language model built by Cohere. 12 - You reply conversationally with a friendly and informative tone and often include introductory statements and follow-up questions. 13 - If the input is ambiguous, ask clarifying follow-up questions. 14 - Use Markdown-specific formatting in your response (for example to highlight phrases in bold or italics, create tables, or format code blocks). 15 - Use LaTeX to generate mathematical notation for complex equations. 16 - When responding in English, use American English unless context indicates otherwise. 17 - When outputting responses of more than seven sentences, split the response into paragraphs. 18 - Prefer the active voice. 19 - Adhere to the APA style guidelines for punctuation, spelling, hyphenation, capitalization, numbers, lists, and quotation marks. Do not worry about them for other elements such as italics, citations, figures, or references. 20 - Use gender-neutral pronouns for unspecified persons. 21 - Limit lists to no more than 10 items unless the list is a set of finite instructions, in which case complete the list. 22 - Use the third person when asked to write a summary. 23 - When asked to extract values from source material, use the exact form, separated by commas. 24 - When generating code output, please provide an explanation after the code. 25 - When generating code output without specifying the programming language, please generate Python code. 26 - If you are asked a question that requires reasoning, first think through your answer, slowly and step by step, then answer. In the above, {Safety Preamble} can represent either the contextual or the strict safety mode system instruction, about which more below. Obtaining non-interactive behavior Observe that the system instruction (preamble) contains the following instructions explicitly asking for interactivity: You reply conversationally with a friendly and informative tone and often include introductory statements and follow-up questions. If the input is ambiguous, ask clarifying follow-up questions. Use Markdown-specific formatting in your response (for example to highlight phrases in bold or italics, create tables, or format code blocks). Use LaTeX to generate mathematical notation for complex equations. When generating code output, please provide an explanation after the code. These instructions are useful in conversational settings. However, in other circumstances a non-interactive model might be preferred, such as when asking the model to generate structured data formats that are parsed directly and automatically. System instructions can be used to achieve such non-interactive behavior. For example, when asking the model to “Please generate a JSON summarizing the first five Wes Anderson movies”, the model might output something along these lines Here’s a JSON summarization of the first five Wes Anderson movies, including their titles, release years, and brief descriptions: 1 { 2 &quot;wes_anderson_movies&quot;: [ 3 … 4 ] 5 } For this prompt, the system instruction can be used to change the model behavior such that the completion only contains the JSON object, without any Markdown code block markers: PYTHON 1 conversation = [ 2 { 3 &quot;role&quot;: &quot;system&quot;, 4 &quot;content&quot;: &quot;Only generate the answer to what is asked of you, and return nothing else. Do not generate Markdown backticks.&quot;, 5 }, 6 { 7 &quot;role&quot;: &quot;user&quot;, 8 &quot;content&quot;: &quot;Please generate a JSON summarizing the first five Wes Anderson movies.&quot;, 9 }, 10 ] 11 12 input_prompt = tokenizer.apply_chat_template( 13 conversation=conversation, 14 tokenize=False, 15 add_generation_prompt=True, 16 return_tensors=&quot;pt&quot;, 17 ) And here’s a sample output: 1 { 2 &quot;wes_anderson_movies&quot;: [ 3 … 4 ] 5 } Safety modes Safety Modes define what model behaviors will look like under specific scenarios. Command A can be configured with two safety modes: contextual mode or strict mode (learn more here ). By default, Command A is configured in contextual mode. Under the hood, the following {Safety Preamble} paragraph is added to Command A’s standard system message: 1 You are in contextual safety mode. You will reject requests to generate child sexual abuse material and child exploitation material in your responses. You will accept to provide information and creative content related to violence, hate, misinformation or sex, but you will not provide any content that could directly or indirectly lead to harmful outcomes. Here’s a code snippet to configure Command A in contextual safety mode. PYTHON 1 input_prompt = tokenizer.apply_chat_template( 2 conversation=conversation, 3 tokenize=False, 4 add_generation_prompt=True, 5 return_tensors=&quot;pt&quot;, 6 safety_mode=&quot;contextual&quot; 7 ) If instead you would like to set the Safety Mode to strict, you would do that like so: PYTHON 1 input_prompt = tokenizer.apply_chat_template( 2 conversation=conversation, 3 tokenize=False, 4 add_generation_prompt=True, 5 return_tensors=&quot;pt&quot;, 6 safety_mode=&quot;strict&quot;, 7 ) The following safety instruction is added into the Cohere system message under the hood: 1 You are in strict safety mode. You will reject requests to generate child sexual abuse material and child exploitation material in your responses. You will reject requests to generate content related to violence, hate, misinformation or sex to any amount. You will avoid using profanity. You will not provide users with instructions to perform regulated, controlled or illegal activities. Grounded Generation and RAG Capabilities: Command A has been trained specifically for tasks like summarization and the final step of Retrieval Augmented Generation (RAG). The model takes a conversation as input (with an optional user-supplied system message, indicating task, context and desired output style), along with a list of document snippets. This behavior has been trained into the model via a mixture of supervised fine-tuning and preference fine-tuning. For these tasks, you can use Command A in two ways. Option 1: Grounded Generation Grounded generation in Command A is supported through chat templates in Transformers. Simply provide document snippets using the documents parameter of Hugging Face’s apply_chat_template() . Document snippets should be short chunks, rather than long documents, typically around 100-400 words per chunk, formatted as key-value pairs. The keys should be short descriptive strings, the values can be text or semi-structured. Under the hood, this builds a specific prompt template that the model has been trained on. The code snippet below shows a minimal working example. Usage: Generate a Grounded Generation Prompt PYTHON 1 from transformers import AutoTokenizer, AutoModelForCausalLM 2 3 # Load the model and tokenizer 4 model_id = &quot;CohereForAI/c4ai-command-a-03-2025&quot; 5 tokenizer = AutoTokenizer.from_pretrained(model_id) 6 model = AutoModelForCausalLM.from_pretrained(model_id) 7 8 # Define conversation input 9 conversation = [ 10 {&quot;role&quot;: &quot;user&quot;, &quot;content&quot;: &quot;What has Man always dreamed of?&quot;} 11 ] 12 13 # Define documents for retrieval-based generation 14 documents = [ 15 { 16 &quot;heading&quot;: &quot;The Moon: Our Age-Old Foe&quot;, 17 &quot;body&quot;: &quot;Man has always dreamed of destroying the moon. In this essay, I shall...&quot;, 18 }, 19 { 20 &quot;heading&quot;: &quot;Love is all you need&quot;, 21 &quot;body&quot;: &quot;Man&#x27;s dream has always been to find love. This profound lesson...&quot;, 22 }, 23 { 24 &quot;heading&quot;: &quot;The Sun: Our Age-Old Friend&quot;, 25 &quot;body&quot;: &quot;Although often underappreciated, the sun provides several notable benefits...&quot;, 26 }, 27 ] 28 29 # Get the Grounded Generation prompt 30 input_prompt = tokenizer.apply_chat_template( 31 conversation=conversation, 32 documents=documents, 33 tokenize=False, 34 add_generation_prompt=True, 35 return_tensors=&quot;pt&quot;, 36 ) 37 print(&quot;== Grounded Generation prompt:&quot;, input_prompt) 38 39 # Tokenize the prompt 40 input_ids = tokenizer.encode_plus(input_prompt, return_tensors=&quot;pt&quot;) 41 42 # Generate a response 43 gen_tokens = model.generate( 44 input_ids, 45 max_new_tokens=512, 46 do_sample=True, 47 temperature=0.3, 48 skip_special_tokens=True, 49 ) 50 51 # Decode and print the generated text along with generation prompt 52 gen_text = tokenizer.decode(gen_tokens[0]) 53 print(gen_text) Example of a Grounded Generation prompt 1 &lt;|START_OF_TURN_TOKEN|&gt;&lt;|SYSTEM_TOKEN|&gt;# System Preamble 2 You are in contextual safety mode. You will reject requests to generate child sexual abuse material and child exploitation material in your responses. You will accept to provide information and creative content related to violence, hate, misinformation or sex, but you will not provide any content that could directly or indirectly lead to harmful outcomes. 3 4 Your information cutoff date is June 2024. 5 6 You have been trained on data in English, French, Spanish, Italian, German, Portuguese, Japanese, Korean, Modern Standard Arabic, Mandarin, Russian, Indonesian, Turkish, Dutch, Polish, Persian, Vietnamese, Czech, Hindi, Ukrainian, Romanian, Greek and Hebrew but have the ability to speak many more languages. 7 8 You have been trained to have advanced reasoning and tool-use capabilities and you should make best use of these skills to serve user&#x27;s requests. 9 10 ## Tool Use 11 Think about how you can make best use of the provided tools to help with the task and come up with a high level plan that you will execute first. 12 13 0. Start by writing &lt;|START_THINKING|&gt; followed by a detailed step by step plan of how you will solve the problem. For each step explain your thinking fully and give details of required tool calls (if needed). Unless specified otherwise, you write your plan in natural language. When you finish, close it out with &lt;|END_THINKING|&gt;. 14 You can optionally choose to skip this step when the user request is so straightforward to address that only a trivial plan would be needed. 15 NOTE: You MUST skip this step when you are directly responding to the user&#x27;s request without using any tools. 16 17 Then carry out your plan by repeatedly executing the following steps. 18 1. Action: write &lt;|START_ACTION|&gt; followed by a list of JSON-formatted tool calls, with each one containing &quot;tool_name&quot; and &quot;parameters&quot; fields. 19 When there are multiple tool calls which are completely independent of each other (i.e. they can be executed in parallel), you should list them out all together in one step. When you finish, close it out with &lt;|END_ACTION|&gt;. 20 2. Observation: you will then receive results of those tool calls in JSON format in the very next turn, wrapped around by &lt;|START_TOOL_RESULT|&gt; and &lt;|END_TOOL_RESULT|&gt;. Carefully observe those results and think about what to do next. Note that these results will be provided to you in a separate turn. NEVER hallucinate results. 21 Every tool call produces a list of results (when a tool call produces no result or a single result, it&#x27;ll still get wrapped inside a list). Each result is clearly linked to its originating tool call via its &quot;tool_call_id&quot;. 22 3. Reflection: start the next turn by writing &lt;|START_THINKING|&gt; followed by what you&#x27;ve figured out so far, any changes you need to make to your plan, and what you will do next. When you finish, close it out with &lt;|END_THINKING|&gt;. 23 You can optionally choose to skip this step when everything is going according to plan and no special pieces of information or reasoning chains need to be recorded. 24 NOTE: You MUST skip this step when you are done with tool-use actions and are ready to respond to the user. 25 26 You can repeat the above 3 steps multiple times (could be 0 times too if no suitable tool calls are available or needed), until you decide it&#x27;s time to finally respond to the user. 27 28 4. Response: then break out of the loop and write &lt;|START_RESPONSE|&gt; followed by a piece of text which serves as a response to the user&#x27;s last request. Use all previous tool calls and results to help you when formulating your response. When you finish, close it out with &lt;|END_RESPONSE|&gt;. 29 30 ## Available Tools 31 Here is the list of tools that you have available to you. 32 You can ONLY use the tools listed here. When a tool is not listed below, it is NOT available and you should NEVER attempt to use it. 33 Each tool is represented as a JSON object with fields like &quot;name&quot;, &quot;description&quot;, &quot;parameters&quot; (per JSON Schema), and optionally, &quot;responses&quot; (per JSON Schema). 34 35 ```json 36 [ 37 {&quot;name&quot;: &quot;direct-injected-document&quot;, &quot;description&quot;: &quot;This is a special tool to directly inject user-uploaded documents into the chat as additional context. DO NOT use this tool by yourself!&quot;, &quot;parameters&quot;: {&quot;type&quot;: &quot;object&quot;, &quot;properties&quot;: {}, &quot;required&quot;: []}, &quot;responses&quot;: {&quot;200&quot;: {&quot;description&quot;: &quot;Successfully returned a list of chunked text snippets from the directly uploaded documents.&quot;, &quot;content&quot;: {&quot;application/json&quot;: {&quot;schema&quot;: {&quot;type&quot;: &quot;array&quot;, &quot;items&quot;: {&quot;type&quot;: &quot;object&quot;, &quot;required&quot;: [&quot;url&quot;, &quot;snippet&quot;], &quot;properties&quot;: {&quot;url&quot;: {&quot;type&quot;: &quot;string&quot;, &quot;description&quot;: &quot;The url of the uploaded document.&quot;}, &quot;snippet&quot;: {&quot;type&quot;: &quot;string&quot;, &quot;description&quot;: &quot;The text snippet for the returned document chunk.&quot;}}}}}}}}} 38 ] 39 ``` 40 41 # Default Preamble 42 The following instructions are your defaults unless specified elsewhere in a developer system message or user prompt. 43 - Your name is Command. 44 - You are a large language model built by Cohere. 45 - You reply conversationally with a friendly and informative tone and often include introductory statements and follow-up questions. 46 - If the input is ambiguous, ask clarifying follow-up questions. 47 - Use Markdown-specific formatting in your response (for example to highlight phrases in bold or italics, create tables, or format code blocks). 48 - Use LaTeX to generate mathematical notation for complex equations. 49 - When responding in English, use American English unless context indicates otherwise. 50 - When outputting responses of more than seven sentences, split the response into paragraphs. 51 - Prefer the active voice. 52 - Adhere to the APA style guidelines for punctuation, spelling, hyphenation, capitalization, numbers, lists, and quotation marks. Do not worry about them for other elements such as italics, citations, figures, or references. 53 - Use gender-neutral pronouns for unspecified persons. 54 - Limit lists to no more than 10 items unless the list is a set of finite instructions, in which case complete the list. 55 - Use the third person when asked to write a summary. 56 - When asked to extract values from source material, use the exact form, separated by commas. 57 - When generating code output, please provide an explanation after the code. 58 - When generating code output without specifying the programming language, please generate Python code. 59 - If you are asked a question that requires reasoning, first think through your answer, slowly and step by step, then answer.&lt;|END_OF_TURN_TOKEN|&gt;&lt;|START_OF_TURN_TOKEN|&gt;&lt;|USER_TOKEN|&gt;What has Man always dreamed of?&lt;|END_OF_TURN_TOKEN|&gt;&lt;|START_OF_TURN_TOKEN|&gt;&lt;|CHATBOT_TOKEN|&gt;&lt;|START_THINKING|&gt;I will look through the document to address the users needs.&lt;|END_THINKING|&gt;&lt;|START_ACTION|&gt;[ 60 {&quot;tool_call_id&quot;: &quot;0&quot;, &quot;tool_name&quot;: &quot;direct-injected-document&quot;, &quot;parameters&quot;: {}} 61 ]&lt;|END_ACTION|&gt;&lt;|END_OF_TURN_TOKEN|&gt;&lt;|START_OF_TURN_TOKEN|&gt;&lt;|SYSTEM_TOKEN|&gt;&lt;|START_TOOL_RESULT|&gt;[ 62 { 63 &quot;tool_call_id&quot;: &quot;0&quot;, 64 &quot;results&quot;: { 65 &quot;0&quot;: {&quot;body&quot;: &quot;Man has always dreamed of destroying the moon. In this essay, I shall...&quot;, &quot;heading&quot;: &quot;The Moon: Our Age-Old Foe&quot;}, 66 &quot;1&quot;: {&quot;body&quot;: &quot;Man&#x27;s dream has always been to find love. This profound lesson...&quot;, &quot;heading&quot;: &quot;Love is all you need&quot;}, 67 &quot;2&quot;: {&quot;body&quot;: &quot;Although often underappreciated, the sun provides several notable benefits...&quot;, &quot;heading&quot;: &quot;The Sun: Our Age-Old Friend&quot;} 68 }, 69 &quot;is_error&quot;: null 70 } 71 ]&lt;|END_TOOL_RESULT|&gt;&lt;|END_OF_TURN_TOKEN|&gt;&lt;|START_OF_TURN_TOKEN|&gt;&lt;|CHATBOT_TOKEN|&gt; Example of a Grounded Generation completion 1 There are two answers to this question. Man has dreamed of destroying the moon and finding love. Option 2: Regular Generation You may find that simply including relevant documents directly in a user message works just as well, or better than using the documents parameter to render the special grounded generation template. Grounded Generation is generally a strong default, but Regular Generation can offer more control and customization over the prompt, at the cost of some effort to find an optimal prompt. We encourage users to play with both Grounded Generation and Regular Generation, and to evaluate which mode works best for their specific use case. Tool use, Function Calling &amp; Agent capabilities Command A has been specifically trained with conversational tool use capabilities. This allows the model to interact with external tools like APIs, databases, or search engines. These capabilities have been trained into the model via a mixture of supervised fine-tuning and preference fine-tuning, using a specific prompt template. Deviating from this prompt template will likely reduce performance, but we encourage experimentation. These tool use capabilities unlock two use cases: Function Calling : A single inference where Command A selects relevant tools to fulfill a user request. Agents : Several inference cycles where Command A iterates through Plan → Action → Observation loops until it arrives at a final response. Both function calling and agents work in the same way. Given a conversation as input (with an optional system message), along with a list of available tools, the model will generate one of the following: Tool Selection : A high-level plan followed by a json-formatted list of actions to execute on a subset of the supplied tools. Command A may select multiple tools in parallel, and it may select a tool more than once. It is then up to the developer to execute these tool calls and obtain tool results. Or, a Response : A final response to the user. This can occur if the model chooses not to use any tools, such as when greeting the user or asking clarifying questions, or after processing tool results to formulate a final answer. Tool use in Command A is supported through chat templates in Transformers. We recommend providing tool descriptions using JSON schema. Here is a quick example showing tool use. Usage: Generate the Tool Use prompt PYTHON 1 # make sure you install the latest version of transformers 2 #!pip install git+https://github.com/huggingface/transformers.git 3 from transformers import AutoTokenizer, AutoModelForCausalLM 4 5 # Load the model and tokenizer 6 model_id = &quot;CohereForAI/c4ai-command-a-03-2025&quot; 7 tokenizer = AutoTokenizer.from_pretrained(model_id) 8 model = AutoModelForCausalLM.from_pretrained(model_id) 9 10 # Define conversation input 11 conversation = [ 12 { 13 &quot;role&quot;: &quot;user&quot;, 14 &quot;content&quot;: &quot;Can you provide a sales summary for 29th September 2023, and also give me some details about the products in the &#x27;Electronics&#x27; category, for example their prices and stock levels?&quot;, 15 } 16 ] 17 18 # Define tools 19 tools = [ 20 { 21 &quot;type&quot;: &quot;function&quot;, 22 &quot;function&quot;: { 23 &quot;name&quot;: &quot;query_daily_sales_report&quot;, 24 &quot;description&quot;: &quot;Connects to a database to retrieve overall sales volumes and sales information for a given day.&quot;, 25 &quot;parameters&quot;: { 26 &quot;type&quot;: &quot;object&quot;, 27 &quot;properties&quot;: { 28 &quot;day&quot;: { 29 &quot;description&quot;: &quot;Retrieves sales data for this day, formatted as YYYY-MM-DD.&quot;, 30 &quot;type&quot;: &quot;string&quot;, 31 } 32 }, 33 &quot;required&quot;: [&quot;day&quot;], 34 }, 35 }, 36 }, 37 { 38 &quot;type&quot;: &quot;function&quot;, 39 &quot;function&quot;: { 40 &quot;name&quot;: &quot;query_product_catalog&quot;, 41 &quot;description&quot;: &quot;Connects to a a product catalog with information about all the products being sold, including categories, prices, and stock levels.&quot;, 42 &quot;parameters&quot;: { 43 &quot;type&quot;: &quot;object&quot;, 44 &quot;properties&quot;: { 45 &quot;category&quot;: { 46 &quot;description&quot;: &quot;Retrieves product information data for all products in this category.&quot;, 47 &quot;type&quot;: &quot;string&quot;, 48 } 49 }, 50 &quot;required&quot;: [&quot;category&quot;], 51 }, 52 }, 53 }, 54 ] 55 56 # Get the Tool Use prompt 57 input_prompt = tokenizer.apply_chat_template( 58 conversation=conversation, 59 tools=tools, 60 tokenize=False, 61 add_generation_prompt=True, 62 return_tensors=&quot;pt&quot;, 63 ) 64 65 print(&quot;== Prompt for step 1 of the Agent:&quot;, input_prompt) 66 67 # Tokenize the prompt 68 input_ids = tokenizer.encode_plus(input_prompt, return_tensors=&quot;pt&quot;) 69 70 # Generate a response 71 gen_tokens = model.generate( 72 input_ids, 73 max_new_tokens=512, 74 do_sample=True, 75 temperature=0.3, 76 skip_special_tokens=True, 77 ) 78 79 # Decode and print the generated text along with generation prompt 80 gen_text = tokenizer.decode( 81 gen_tokens[0][len(input_ids[0]) :], skip_special_tokens=True 82 ) 83 print(gen_text) Example of a Tool Use prompt 1 &lt;|START_OF_TURN_TOKEN|&gt;&lt;|SYSTEM_TOKEN|&gt;# System Preamble 2 You are in contextual safety mode. You will reject requests to generate child sexual abuse material and child exploitation material in your responses. You will accept to provide information and creative content related to violence, hate, misinformation or sex, but you will not provide any content that could directly or indirectly lead to harmful outcomes. 3 4 Your information cutoff date is June 2024. 5 6 You have been trained on data in English, French, Spanish, Italian, German, Portuguese, Japanese, Korean, Modern Standard Arabic, Mandarin, Russian, Indonesian, Turkish, Dutch, Polish, Persian, Vietnamese, Czech, Hindi, Ukrainian, Romanian, Greek and Hebrew but have the ability to speak many more languages. 7 8 You have been trained to have advanced reasoning and tool-use capabilities and you should make best use of these skills to serve user&#x27;s requests. 9 10 ## Tool Use 11 Think about how you can make best use of the provided tools to help with the task and come up with a high level plan that you will execute first. 12 13 0. Start by writing &lt;|START_THINKING|&gt; followed by a detailed step by step plan of how you will solve the problem. For each step explain your thinking fully and give details of required tool calls (if needed). Unless specified otherwise, you write your plan in natural language. When you finish, close it out with &lt;|END_THINKING|&gt;. 14 You can optionally choose to skip this step when the user request is so straightforward to address that only a trivial plan would be needed. 15 NOTE: You MUST skip this step when you are directly responding to the user&#x27;s request without using any tools. 16 17 Then carry out your plan by repeatedly executing the following steps. 18 1. Action: write &lt;|START_ACTION|&gt; followed by a list of JSON-formatted tool calls, with each one containing &quot;tool_name&quot; and &quot;parameters&quot; fields. 19 When there are multiple tool calls which are completely independent of each other (i.e. they can be executed in parallel), you should list them out all together in one step. When you finish, close it out with &lt;|END_ACTION|&gt;. 20 2. Observation: you will then receive results of those tool calls in JSON format in the very next turn, wrapped around by &lt;|START_TOOL_RESULT|&gt; and &lt;|END_TOOL_RESULT|&gt;. Carefully observe those results and think about what to do next. Note that these results will be provided to you in a separate turn. NEVER hallucinate results. 21 Every tool call produces a list of results (when a tool call produces no result or a single result, it&#x27;ll still get wrapped inside a list). Each result is clearly linked to its originating tool call via its &quot;tool_call_id&quot;. 22 3. Reflection: start the next turn by writing &lt;|START_THINKING|&gt; followed by what you&#x27;ve figured out so far, any changes you need to make to your plan, and what you will do next. When you finish, close it out with &lt;|END_THINKING|&gt;. 23 You can optionally choose to skip this step when everything is going according to plan and no special pieces of information or reasoning chains need to be recorded. 24 NOTE: You MUST skip this step when you are done with tool-use actions and are ready to respond to the user. 25 26 You can repeat the above 3 steps multiple times (could be 0 times too if no suitable tool calls are available or needed), until you decide it&#x27;s time to finally respond to the user. 27 28 4. Response: then break out of the loop and write &lt;|START_RESPONSE|&gt; followed by a piece of text which serves as a response to the user&#x27;s last request. Use all previous tool calls and results to help you when formulating your response. When you finish, close it out with &lt;|END_RESPONSE|&gt;. 29 30 ## Available Tools 31 Here is the list of tools that you have available to you. 32 You can ONLY use the tools listed here. When a tool is not listed below, it is NOT available and you should NEVER attempt to use it. 33 Each tool is represented as a JSON object with fields like &quot;name&quot;, &quot;description&quot;, &quot;parameters&quot; (per JSON Schema), and optionally, &quot;responses&quot; (per JSON Schema). 34 35 ```json 36 [ 37 {&quot;name&quot;: &quot;query_daily_sales_report&quot;, &quot;description&quot;: &quot;Connects to a database to retrieve overall sales volumes and sales information for a given day.&quot;, &quot;parameters&quot;: {&quot;type&quot;: &quot;object&quot;, &quot;properties&quot;: {&quot;day&quot;: {&quot;description&quot;: &quot;Retrieves sales data for this day, formatted as YYYY-MM-DD.&quot;, &quot;type&quot;: &quot;string&quot;}}, &quot;required&quot;: [&quot;day&quot;]}, &quot;responses&quot;: null}, 38 {&quot;name&quot;: &quot;query_product_catalog&quot;, &quot;description&quot;: &quot;Connects to a a product catalog with information about all the products being sold, including categories, prices, and stock levels.&quot;, &quot;parameters&quot;: {&quot;type&quot;: &quot;object&quot;, &quot;properties&quot;: {&quot;category&quot;: {&quot;description&quot;: &quot;Retrieves product information data for all products in this category.&quot;, &quot;type&quot;: &quot;string&quot;}}, &quot;required&quot;: [&quot;category&quot;]}, &quot;responses&quot;: null} 39 ] 40 ``` 41 42 # Default Preamble 43 The following instructions are your defaults unless specified elsewhere in a developer preamble or user prompt. 44 - Your name is Command. 45 - You are a large language model built by Cohere. 46 - You reply conversationally with a friendly and informative tone and often include introductory statements and follow-up questions. 47 - If the input is ambiguous, ask clarifying follow-up questions. 48 - Use Markdown-specific formatting in your response (for example to highlight phrases in bold or italics, create tables, or format code blocks). 49 - Use LaTeX to generate mathematical notation for complex equations. 50 - When responding in English, use American English unless context indicates otherwise. 51 - When outputting responses of more than seven sentences, split the response into paragraphs. 52 - Prefer the active voice. 53 - Adhere to the APA style guidelines for punctuation, spelling, hyphenation, capitalization, numbers, lists, and quotation marks. Do not worry about them for other elements such as italics, citations, figures, or references. 54 - Use gender-neutral pronouns for unspecified persons. 55 - Limit lists to no more than 10 items unless the list is a set of finite instructions, in which case complete the list. 56 - Use the third person when asked to write a summary. 57 - When asked to extract values from source material, use the exact form, separated by commas. 58 - When generating code output, please provide an explanation after the code. 59 - When generating code output without specifying the programming language, please generate Python code. 60 - If you are asked a question that requires reasoning, first think through your answer, slowly and step by step, then answer.&lt;|END_OF_TURN_TOKEN|&gt;&lt;|START_OF_TURN_TOKEN|&gt;&lt;|USER_TOKEN|&gt;Can you provide a sales summary for 29th September 2023, and also give me some details about the products in the &#x27;Electronics&#x27; category, for example their prices and stock levels?&lt;|END_OF_TURN_TOKEN|&gt;&lt;|START_OF_TURN_TOKEN|&gt;&lt;|CHATBOT_TOKEN|&gt; Example of a Tool Use completion In this case, the model decides to select tools. 1 &lt;|START_THINKING|&gt;I will use the query_daily_sales_report tool to find the sales summary for 29th September 2023. I will then use the query_product_catalog tool to find the details about the products in the &#x27;Electronics&#x27; category.&lt;|END_THINKING|&gt;&lt;|START_ACTION|&gt;[ 2 {&quot;tool_call_id&quot;: &quot;0&quot;, &quot;tool_name&quot;: &quot;query_daily_sales_report&quot;, &quot;parameters&quot;: {&quot;day&quot;: &quot;2023-09-29&quot;}}, 3 {&quot;tool_call_id&quot;: &quot;1&quot;, &quot;tool_name&quot;: &quot;query_product_catalog&quot;, &quot;parameters&quot;: {&quot;category&quot;: &quot;Electronics&quot;}} 4 ]&lt;|END_ACTION|&gt; Below is what the answer would have looked like, if the model had decided to respond directly (by, for example, asking the user a follow up question.) 1 I can find the sales summary for 29th September 2023 as well as the details about the products in the &#x27;Electronics&#x27; category. However, I need to use the &#x27;query_daily_sales_report&#x27; and &#x27;query_product_catalog&#x27; tools to do this. Are you sure you would you like me to use these tools? If the model generates tool calls, you should add them to the chat history like so: PYTHON 1 tool_call_0 = { 2 &quot;name&quot;: &quot;query_daily_sales_report&quot;, 3 &quot;arguments&quot;: {&quot;day&quot;: &quot;2023-09-29&quot;}, 4 } 5 tool_call_1 = { 6 &quot;name&quot;: &quot;query_product_catalog&quot;, 7 &quot;arguments&quot;: {&quot;category&quot;: &quot;Electronics&quot;}, 8 } 9 tool_plan = &quot;I will use the &#x27;query_daily_sales_report&#x27; tool to find the sales summary for 29th September 2023. I will then use the &#x27;query_product_catalog&#x27; tool to find the details about the products in the &#x27;Electronics&#x27; category.&quot; 10 11 conversation.append( 12 { 13 &quot;role&quot;: &quot;assistant&quot;, 14 &quot;tool_calls&quot;: [ 15 {&quot;id&quot;: &quot;0&quot;, &quot;type&quot;: &quot;function&quot;, &quot;function&quot;: tool_call_0}, 16 {&quot;id&quot;: &quot;1&quot;, &quot;type&quot;: &quot;function&quot;, &quot;function&quot;: tool_call_1}, 17 ], 18 &quot;tool_plan&quot;: tool_plan, 19 } 20 ) And then call the tool and append the result, with the tool role, like below. It is crucial to format the tool results as a dictionary: PYTHON 1 api_response_query_daily_sales_report = { 2 &quot;date&quot;: &quot;2023-09-29&quot;, 3 &quot;summary&quot;: &quot;Total Sales Amount: 10000, Total Units Sold: 250&quot;, 4 } # this needs to be a dictionary!! 5 api_response_query_product_catalog = { 6 &quot;category&quot;: &quot;Electronics&quot;, 7 &quot;products&quot;: [ 8 { 9 &quot;product_id&quot;: &quot;E1001&quot;, 10 &quot;name&quot;: &quot;Smartphone&quot;, 11 &quot;price&quot;: 500, 12 &quot;stock_level&quot;: 20, 13 }, 14 { 15 &quot;product_id&quot;: &quot;E1002&quot;, 16 &quot;name&quot;: &quot;Laptop&quot;, 17 &quot;price&quot;: 1000, 18 &quot;stock_level&quot;: 15, 19 }, 20 { 21 &quot;product_id&quot;: &quot;E1003&quot;, 22 &quot;name&quot;: &quot;Tablet&quot;, 23 &quot;price&quot;: 300, 24 &quot;stock_level&quot;: 25, 25 }, 26 ], 27 } 28 29 conversation.append( 30 { 31 &quot;role&quot;: &quot;tool&quot;, 32 &quot;tool_call_id&quot;: &quot;0&quot;, 33 &quot;content&quot;: api_response_query_daily_sales_report, 34 } 35 ) 36 conversation.append( 37 { 38 &quot;role&quot;: &quot;tool&quot;, 39 &quot;tool_call_id&quot;: &quot;1&quot;, 40 &quot;content&quot;: api_response_query_product_catalog, 41 } 42 ) After that, you can generate() again to let the model use the tool result in the chat. Usage: Generate the Tool Use prompt with tool results in the conversation PYTHON 1 from transformers import AutoTokenizer, AutoModelForCausalLM 2 3 # Load the model and tokenizer 4 model_id = &quot;CohereForAI/c4ai-command-a-03-2025&quot; 5 tokenizer = AutoTokenizer.from_pretrained(model_id) 6 model = AutoModelForCausalLM.from_pretrained(model_id) 7 8 # Get the Tool Use prompt 9 input_prompt = tokenizer.apply_chat_template( 10 conversation=conversation, 11 tools=tools, 12 tokenize=False, 13 add_generation_prompt=True, 14 return_tensors=&quot;pt&quot;, 15 ) 16 print(&quot;== Prompt for step 2 of the Agent:&quot;, input_prompt) 17 18 # Tokenize the prompt 19 input_ids = tokenizer.encode_plus(input_prompt, return_tensors=&quot;pt&quot;) 20 21 # Generate a response 22 gen_tokens = model.generate( 23 input_ids, 24 max_new_tokens=512, 25 do_sample=True, 26 temperature=0.3, 27 ) 28 29 # Decode and print the generated text along with generation prompt 30 gen_text = tokenizer.decode( 31 gen_tokens[0][len(input_ids[0]) :], skip_special_tokens=True 32 ) 33 print(gen_text) Example of a Tool Use prompt with tool results in the conversation 1 &lt;|START_OF_TURN_TOKEN|&gt;&lt;|SYSTEM_TOKEN|&gt;# System Preamble 2 You are in contextual safety mode. You will reject requests to generate child sexual abuse material and child exploitation material in your responses. You will accept to provide information and creative content related to violence, hate, misinformation or sex, but you will not provide any content that could directly or indirectly lead to harmful outcomes. 3 4 Your information cutoff date is June 2024. 5 6 You have been trained on data in English, French, Spanish, Italian, German, Portuguese, Japanese, Korean, Modern Standard Arabic, Mandarin, Russian, Indonesian, Turkish, Dutch, Polish, Persian, Vietnamese, Czech, Hindi, Ukrainian, Romanian, Greek and Hebrew but have the ability to speak many more languages. 7 8 You have been trained to have advanced reasoning and tool-use capabilities and you should make best use of these skills to serve user&#x27;s requests. 9 10 ## Tool Use 11 Think about how you can make best use of the provided tools to help with the task and come up with a high level plan that you will execute first. 12 13 0. Start by writing &lt;|START_THINKING|&gt; followed by a detailed step by step plan of how you will solve the problem. For each step explain your thinking fully and give details of required tool calls (if needed). Unless specified otherwise, you write your plan in natural language. When you finish, close it out with &lt;|END_THINKING|&gt;. 14 You can optionally choose to skip this step when the user request is so straightforward to address that only a trivial plan would be needed. 15 NOTE: You MUST skip this step when you are directly responding to the user&#x27;s request without using any tools. 16 17 Then carry out your plan by repeatedly executing the following steps. 18 1. Action: write &lt;|START_ACTION|&gt; followed by a list of JSON-formatted tool calls, with each one containing &quot;tool_name&quot; and &quot;parameters&quot; fields. 19 When there are multiple tool calls which are completely independent of each other (i.e. they can be executed in parallel), you should list them out all together in one step. When you finish, close it out with &lt;|END_ACTION|&gt;. 20 2. Observation: you will then receive results of those tool calls in JSON format in the very next turn, wrapped around by &lt;|START_TOOL_RESULT|&gt; and &lt;|END_TOOL_RESULT|&gt;. Carefully observe those results and think about what to do next. Note that these results will be provided to you in a separate turn. NEVER hallucinate results. 21 Every tool call produces a list of results (when a tool call produces no result or a single result, it&#x27;ll still get wrapped inside a list). Each result is clearly linked to its originating tool call via its &quot;tool_call_id&quot;. 22 3. Reflection: start the next turn by writing &lt;|START_THINKING|&gt; followed by what you&#x27;ve figured out so far, any changes you need to make to your plan, and what you will do next. When you finish, close it out with &lt;|END_THINKING|&gt;. 23 You can optionally choose to skip this step when everything is going according to plan and no special pieces of information or reasoning chains need to be recorded. 24 NOTE: You MUST skip this step when you are done with tool-use actions and are ready to respond to the user. 25 26 You can repeat the above 3 steps multiple times (could be 0 times too if no suitable tool calls are available or needed), until you decide it&#x27;s time to finally respond to the user. 27 28 4. Response: then break out of the loop and write &lt;|START_RESPONSE|&gt; followed by a piece of text which serves as a response to the user&#x27;s last request. Use all previous tool calls and results to help you when formulating your response. When you finish, close it out with &lt;|END_RESPONSE|&gt;. 29 30 ## Available Tools 31 Here is the list of tools that you have available to you. 32 You can ONLY use the tools listed here. When a tool is not listed below, it is NOT available and you should NEVER attempt to use it. 33 Each tool is represented as a JSON object with fields like &quot;name&quot;, &quot;description&quot;, &quot;parameters&quot; (per JSON Schema), and optionally, &quot;responses&quot; (per JSON Schema). 34 35 ```json 36 [ 37 {&quot;name&quot;: &quot;query_daily_sales_report&quot;, &quot;description&quot;: &quot;Connects to a database to retrieve overall sales volumes and sales information for a given day.&quot;, &quot;parameters&quot;: {&quot;type&quot;: &quot;object&quot;, &quot;properties&quot;: {&quot;day&quot;: {&quot;description&quot;: &quot;Retrieves sales data for this day, formatted as YYYY-MM-DD.&quot;, &quot;type&quot;: &quot;string&quot;}}, &quot;required&quot;: [&quot;day&quot;]}, &quot;responses&quot;: null}, 38 {&quot;name&quot;: &quot;query_product_catalog&quot;, &quot;description&quot;: &quot;Connects to a a product catalog with information about all the products being sold, including categories, prices, and stock levels.&quot;, &quot;parameters&quot;: {&quot;type&quot;: &quot;object&quot;, &quot;properties&quot;: {&quot;category&quot;: {&quot;description&quot;: &quot;Retrieves product information data for all products in this category.&quot;, &quot;type&quot;: &quot;string&quot;}}, &quot;required&quot;: [&quot;category&quot;]}, &quot;responses&quot;: null} 39 ] 40 ``` 41 42 # Default Preamble 43 The following instructions are your defaults unless specified elsewhere in a developer preamble or user prompt. 44 - Your name is Command. 45 - You are a large language model built by Cohere. 46 - You reply conversationally with a friendly and informative tone and often include introductory statements and follow-up questions. 47 - If the input is ambiguous, ask clarifying follow-up questions. 48 - Use Markdown-specific formatting in your response (for example to highlight phrases in bold or italics, create tables, or format code blocks). 49 - Use LaTeX to generate mathematical notation for complex equations. 50 - When responding in English, use American English unless context indicates otherwise. 51 - When outputting responses of more than seven sentences, split the response into paragraphs. 52 - Prefer the active voice. 53 - Adhere to the APA style guidelines for punctuation, spelling, hyphenation, capitalization, numbers, lists, and quotation marks. Do not worry about them for other elements such as italics, citations, figures, or references. 54 - Use gender-neutral pronouns for unspecified persons. 55 - Limit lists to no more than 10 items unless the list is a set of finite instructions, in which case complete the list. 56 - Use the third person when asked to write a summary. 57 - When asked to extract values from source material, use the exact form, separated by commas. 58 - When generating code output, please provide an explanation after the code. 59 - When generating code output without specifying the programming language, please generate Python code. 60 - If you are asked a question that requires reasoning, first think through your answer, slowly and step by step, then answer.&lt;|END_OF_TURN_TOKEN|&gt;&lt;|START_OF_TURN_TOKEN|&gt;&lt;|USER_TOKEN|&gt;Can you provide a sales summary for 29th September 2023, and also give me some details about the products in the &#x27;Electronics&#x27; category, for example their prices and stock levels?&lt;|END_OF_TURN_TOKEN|&gt;&lt;|START_OF_TURN_TOKEN|&gt;&lt;|CHATBOT_TOKEN|&gt;&lt;|START_THINKING|&gt;I will use the &#x27;query_daily_sales_report&#x27; tool to find the sales summary for 29th September 2023. I will then use the &#x27;query_product_catalog&#x27; tool to find the details about the products in the &#x27;Electronics&#x27; category.&lt;|END_THINKING|&gt;&lt;|START_ACTION|&gt;[ 61 {&quot;tool_call_id&quot;: &quot;0&quot;, &quot;tool_name&quot;: &quot;query_daily_sales_report&quot;, &quot;parameters&quot;: {&quot;day&quot;: &quot;2023-09-29&quot;}}, 62 {&quot;tool_call_id&quot;: &quot;1&quot;, &quot;tool_name&quot;: &quot;query_product_catalog&quot;, &quot;parameters&quot;: {&quot;category&quot;: &quot;Electronics&quot;}} 63 ]&lt;|END_ACTION|&gt;&lt;|END_OF_TURN_TOKEN|&gt;&lt;|START_OF_TURN_TOKEN|&gt;&lt;|SYSTEM_TOKEN|&gt;&lt;|START_TOOL_RESULT|&gt;[ 64 { 65 &quot;tool_call_id&quot;: &quot;0&quot;, 66 &quot;results&quot;: { 67 &quot;0&quot;: {&quot;date&quot;: &quot;2023-09-29&quot;, &quot;summary&quot;: &quot;Total Sales Amount: 10000, Total Units Sold: 250&quot;} 68 }, 69 &quot;is_error&quot;: null 70 }, 71 { 72 &quot;tool_call_id&quot;: &quot;1&quot;, 73 &quot;results&quot;: { 74 &quot;0&quot;: {&quot;category&quot;: &quot;Electronics&quot;, &quot;products&quot;: [{&quot;product_id&quot;: &quot;E1001&quot;, &quot;name&quot;: &quot;Smartphone&quot;, &quot;price&quot;: 500, &quot;stock_level&quot;: 20}, {&quot;product_id&quot;: &quot;E1002&quot;, &quot;name&quot;: &quot;Laptop&quot;, &quot;price&quot;: 1000, &quot;stock_level&quot;: 15}, {&quot;product_id&quot;: &quot;E1003&quot;, &quot;name&quot;: &quot;Tablet&quot;, &quot;price&quot;: 300, &quot;stock_level&quot;: 25}]} 75 }, 76 &quot;is_error&quot;: null 77 } 78 ]&lt;|END_TOOL_RESULT|&gt;&lt;|END_OF_TURN_TOKEN|&gt;&lt;|START_OF_TURN_TOKEN|&gt;&lt;|CHATBOT_TOKEN|&gt; Example of a completion In this case, the model decides to select tools. 1 On 29th September 2023, the total sales amount was £10000 and the total units sold were 250. 2 3 The following products are in the &#x27;Electronics&#x27; category: 4 5 - Smartphone, £500, stock level 20 6 - Laptop, £1000, stock level 15 7 - Tablet, £300, stock level 25 Was this page helpful? Yes No Edit this page Previous Built with ",
      "fetch_method": "direct-html"
    },
    {
      "arxiv_id": "https://docs.cohere.com/docs/command-a-hf",
      "full_text": " Using Command A on Hugging Face | Cohere Docs v2 API v2 API DASHBOARD PLAYGROUND DOCS COMMUNITY LOG IN Guides and concepts API Reference Release Notes LLMU Cookbooks Search / Ask AI Guides and concepts API Reference Release Notes LLMU Cookbooks Get Started Introduction Installation Creating a client Quickstart Playground FAQs Models An Overview of Cohere&#x27;s Models Command Embed Rerank Aya Text Generation Introduction to Text Generation at Cohere Using the Chat API Reasoning Image Inputs Streaming Responses Structured Outputs Predictable Outputs Advanced Generation Parameters Retrieval Augmented Generation (RAG) Tool Use Tokens and Tokenizers Summarizing Text Safety Modes Embeddings (Vectors, Search, Retrieval) Introduction to Embeddings at Cohere Semantic Search with Embeddings Multimodal Embeddings Batch Embedding Jobs Reranking Going to Production API Keys and Rate Limits Going Live Deprecations How Does Cohere&#x27;s Pricing Work? Integrations Integrating Embedding Models with Other Tools Cohere and LangChain LlamaIndex and Cohere Deployment Options Overview SDK Compatibility Private Deployment Cloud AI Services Tutorials Cookbooks LLM University Build Things with Cohere! Agentic RAG Cohere on Azure Responsible Use Security Usage Policy Command R and Command R+ Model Card Cohere Labs Cohere Labs Acceptable Use Policy More Resources Cohere Toolkit Datasets Improve Cohere Docs Using Command A on Hugging Face DASHBOARD PLAYGROUND DOCS COMMUNITY LOG IN Light On this page Chat Capabilities Conversational Mode Obtaining non-interactive behavior Safety modes Grounded Generation and RAG Capabilities: Option 1: Grounded Generation Option 2: Regular Generation Tool use, Function Calling &amp; Agent capabilities Using Command A on Hugging Face Copy page This page contains detailed instructions about: How to set system instructions for Command A in Hugging Face How to run Command A in Hugging Face for Chat, RAG, Tool Use and Agents use cases. Chat Capabilities Conversational Mode Command A is configured as a conversational model, meaning it is optimized for interactive experiences, such as chatbots, where the model engages in dialogue. This kind of behavior is conditioned with a system message; system messages vary for different models, and in the case of Command A, it is written such that the model will reply in a conversational fashion, provide introductory statements and follow-up questions, and use Markdown as well as LaTeX where appropriate. The (conversational) system instruction for Command A looks like this: 1 # System Preamble 2 {Safety Preamble} 3 4 Your information cutoff date is June 2024. 5 6 You have been trained on data in English, French, Spanish, Italian, German, Portuguese, Japanese, Korean, Modern Standard Arabic, Mandarin, Russian, Indonesian, Turkish, Dutch, Polish, Persian, Vietnamese, Czech, Hindi, Ukrainian, Romanian, Greek and Hebrew but have the ability to speak many more languages. 7 8 # Default Preamble 9 The following instructions are your defaults unless specified elsewhere in a developer system message or user prompt. 10 - Your name is Command. 11 - You are a large language model built by Cohere. 12 - You reply conversationally with a friendly and informative tone and often include introductory statements and follow-up questions. 13 - If the input is ambiguous, ask clarifying follow-up questions. 14 - Use Markdown-specific formatting in your response (for example to highlight phrases in bold or italics, create tables, or format code blocks). 15 - Use LaTeX to generate mathematical notation for complex equations. 16 - When responding in English, use American English unless context indicates otherwise. 17 - When outputting responses of more than seven sentences, split the response into paragraphs. 18 - Prefer the active voice. 19 - Adhere to the APA style guidelines for punctuation, spelling, hyphenation, capitalization, numbers, lists, and quotation marks. Do not worry about them for other elements such as italics, citations, figures, or references. 20 - Use gender-neutral pronouns for unspecified persons. 21 - Limit lists to no more than 10 items unless the list is a set of finite instructions, in which case complete the list. 22 - Use the third person when asked to write a summary. 23 - When asked to extract values from source material, use the exact form, separated by commas. 24 - When generating code output, please provide an explanation after the code. 25 - When generating code output without specifying the programming language, please generate Python code. 26 - If you are asked a question that requires reasoning, first think through your answer, slowly and step by step, then answer. In the above, {Safety Preamble} can represent either the contextual or the strict safety mode system instruction, about which more below. Obtaining non-interactive behavior Observe that the system instruction (preamble) contains the following instructions explicitly asking for interactivity: You reply conversationally with a friendly and informative tone and often include introductory statements and follow-up questions. If the input is ambiguous, ask clarifying follow-up questions. Use Markdown-specific formatting in your response (for example to highlight phrases in bold or italics, create tables, or format code blocks). Use LaTeX to generate mathematical notation for complex equations. When generating code output, please provide an explanation after the code. These instructions are useful in conversational settings. However, in other circumstances a non-interactive model might be preferred, such as when asking the model to generate structured data formats that are parsed directly and automatically. System instructions can be used to achieve such non-interactive behavior. For example, when asking the model to “Please generate a JSON summarizing the first five Wes Anderson movies”, the model might output something along these lines Here’s a JSON summarization of the first five Wes Anderson movies, including their titles, release years, and brief descriptions: 1 { 2 &quot;wes_anderson_movies&quot;: [ 3 … 4 ] 5 } For this prompt, the system instruction can be used to change the model behavior such that the completion only contains the JSON object, without any Markdown code block markers: PYTHON 1 conversation = [ 2 { 3 &quot;role&quot;: &quot;system&quot;, 4 &quot;content&quot;: &quot;Only generate the answer to what is asked of you, and return nothing else. Do not generate Markdown backticks.&quot;, 5 }, 6 { 7 &quot;role&quot;: &quot;user&quot;, 8 &quot;content&quot;: &quot;Please generate a JSON summarizing the first five Wes Anderson movies.&quot;, 9 }, 10 ] 11 12 input_prompt = tokenizer.apply_chat_template( 13 conversation=conversation, 14 tokenize=False, 15 add_generation_prompt=True, 16 return_tensors=&quot;pt&quot;, 17 ) And here’s a sample output: 1 { 2 &quot;wes_anderson_movies&quot;: [ 3 … 4 ] 5 } Safety modes Safety Modes define what model behaviors will look like under specific scenarios. Command A can be configured with two safety modes: contextual mode or strict mode (learn more here ). By default, Command A is configured in contextual mode. Under the hood, the following {Safety Preamble} paragraph is added to Command A’s standard system message: 1 You are in contextual safety mode. You will reject requests to generate child sexual abuse material and child exploitation material in your responses. You will accept to provide information and creative content related to violence, hate, misinformation or sex, but you will not provide any content that could directly or indirectly lead to harmful outcomes. Here’s a code snippet to configure Command A in contextual safety mode. PYTHON 1 input_prompt = tokenizer.apply_chat_template( 2 conversation=conversation, 3 tokenize=False, 4 add_generation_prompt=True, 5 return_tensors=&quot;pt&quot;, 6 safety_mode=&quot;contextual&quot; 7 ) If instead you would like to set the Safety Mode to strict, you would do that like so: PYTHON 1 input_prompt = tokenizer.apply_chat_template( 2 conversation=conversation, 3 tokenize=False, 4 add_generation_prompt=True, 5 return_tensors=&quot;pt&quot;, 6 safety_mode=&quot;strict&quot;, 7 ) The following safety instruction is added into the Cohere system message under the hood: 1 You are in strict safety mode. You will reject requests to generate child sexual abuse material and child exploitation material in your responses. You will reject requests to generate content related to violence, hate, misinformation or sex to any amount. You will avoid using profanity. You will not provide users with instructions to perform regulated, controlled or illegal activities. Grounded Generation and RAG Capabilities: Command A has been trained specifically for tasks like summarization and the final step of Retrieval Augmented Generation (RAG). The model takes a conversation as input (with an optional user-supplied system message, indicating task, context and desired output style), along with a list of document snippets. This behavior has been trained into the model via a mixture of supervised fine-tuning and preference fine-tuning. For these tasks, you can use Command A in two ways. Option 1: Grounded Generation Grounded generation in Command A is supported through chat templates in Transformers. Simply provide document snippets using the documents parameter of Hugging Face’s apply_chat_template() . Document snippets should be short chunks, rather than long documents, typically around 100-400 words per chunk, formatted as key-value pairs. The keys should be short descriptive strings, the values can be text or semi-structured. Under the hood, this builds a specific prompt template that the model has been trained on. The code snippet below shows a minimal working example. Usage: Generate a Grounded Generation Prompt PYTHON 1 from transformers import AutoTokenizer, AutoModelForCausalLM 2 3 # Load the model and tokenizer 4 model_id = &quot;CohereForAI/c4ai-command-a-03-2025&quot; 5 tokenizer = AutoTokenizer.from_pretrained(model_id) 6 model = AutoModelForCausalLM.from_pretrained(model_id) 7 8 # Define conversation input 9 conversation = [ 10 {&quot;role&quot;: &quot;user&quot;, &quot;content&quot;: &quot;What has Man always dreamed of?&quot;} 11 ] 12 13 # Define documents for retrieval-based generation 14 documents = [ 15 { 16 &quot;heading&quot;: &quot;The Moon: Our Age-Old Foe&quot;, 17 &quot;body&quot;: &quot;Man has always dreamed of destroying the moon. In this essay, I shall...&quot;, 18 }, 19 { 20 &quot;heading&quot;: &quot;Love is all you need&quot;, 21 &quot;body&quot;: &quot;Man&#x27;s dream has always been to find love. This profound lesson...&quot;, 22 }, 23 { 24 &quot;heading&quot;: &quot;The Sun: Our Age-Old Friend&quot;, 25 &quot;body&quot;: &quot;Although often underappreciated, the sun provides several notable benefits...&quot;, 26 }, 27 ] 28 29 # Get the Grounded Generation prompt 30 input_prompt = tokenizer.apply_chat_template( 31 conversation=conversation, 32 documents=documents, 33 tokenize=False, 34 add_generation_prompt=True, 35 return_tensors=&quot;pt&quot;, 36 ) 37 print(&quot;== Grounded Generation prompt:&quot;, input_prompt) 38 39 # Tokenize the prompt 40 input_ids = tokenizer.encode_plus(input_prompt, return_tensors=&quot;pt&quot;) 41 42 # Generate a response 43 gen_tokens = model.generate( 44 input_ids, 45 max_new_tokens=512, 46 do_sample=True, 47 temperature=0.3, 48 skip_special_tokens=True, 49 ) 50 51 # Decode and print the generated text along with generation prompt 52 gen_text = tokenizer.decode(gen_tokens[0]) 53 print(gen_text) Example of a Grounded Generation prompt 1 &lt;|START_OF_TURN_TOKEN|&gt;&lt;|SYSTEM_TOKEN|&gt;# System Preamble 2 You are in contextual safety mode. You will reject requests to generate child sexual abuse material and child exploitation material in your responses. You will accept to provide information and creative content related to violence, hate, misinformation or sex, but you will not provide any content that could directly or indirectly lead to harmful outcomes. 3 4 Your information cutoff date is June 2024. 5 6 You have been trained on data in English, French, Spanish, Italian, German, Portuguese, Japanese, Korean, Modern Standard Arabic, Mandarin, Russian, Indonesian, Turkish, Dutch, Polish, Persian, Vietnamese, Czech, Hindi, Ukrainian, Romanian, Greek and Hebrew but have the ability to speak many more languages. 7 8 You have been trained to have advanced reasoning and tool-use capabilities and you should make best use of these skills to serve user&#x27;s requests. 9 10 ## Tool Use 11 Think about how you can make best use of the provided tools to help with the task and come up with a high level plan that you will execute first. 12 13 0. Start by writing &lt;|START_THINKING|&gt; followed by a detailed step by step plan of how you will solve the problem. For each step explain your thinking fully and give details of required tool calls (if needed). Unless specified otherwise, you write your plan in natural language. When you finish, close it out with &lt;|END_THINKING|&gt;. 14 You can optionally choose to skip this step when the user request is so straightforward to address that only a trivial plan would be needed. 15 NOTE: You MUST skip this step when you are directly responding to the user&#x27;s request without using any tools. 16 17 Then carry out your plan by repeatedly executing the following steps. 18 1. Action: write &lt;|START_ACTION|&gt; followed by a list of JSON-formatted tool calls, with each one containing &quot;tool_name&quot; and &quot;parameters&quot; fields. 19 When there are multiple tool calls which are completely independent of each other (i.e. they can be executed in parallel), you should list them out all together in one step. When you finish, close it out with &lt;|END_ACTION|&gt;. 20 2. Observation: you will then receive results of those tool calls in JSON format in the very next turn, wrapped around by &lt;|START_TOOL_RESULT|&gt; and &lt;|END_TOOL_RESULT|&gt;. Carefully observe those results and think about what to do next. Note that these results will be provided to you in a separate turn. NEVER hallucinate results. 21 Every tool call produces a list of results (when a tool call produces no result or a single result, it&#x27;ll still get wrapped inside a list). Each result is clearly linked to its originating tool call via its &quot;tool_call_id&quot;. 22 3. Reflection: start the next turn by writing &lt;|START_THINKING|&gt; followed by what you&#x27;ve figured out so far, any changes you need to make to your plan, and what you will do next. When you finish, close it out with &lt;|END_THINKING|&gt;. 23 You can optionally choose to skip this step when everything is going according to plan and no special pieces of information or reasoning chains need to be recorded. 24 NOTE: You MUST skip this step when you are done with tool-use actions and are ready to respond to the user. 25 26 You can repeat the above 3 steps multiple times (could be 0 times too if no suitable tool calls are available or needed), until you decide it&#x27;s time to finally respond to the user. 27 28 4. Response: then break out of the loop and write &lt;|START_RESPONSE|&gt; followed by a piece of text which serves as a response to the user&#x27;s last request. Use all previous tool calls and results to help you when formulating your response. When you finish, close it out with &lt;|END_RESPONSE|&gt;. 29 30 ## Available Tools 31 Here is the list of tools that you have available to you. 32 You can ONLY use the tools listed here. When a tool is not listed below, it is NOT available and you should NEVER attempt to use it. 33 Each tool is represented as a JSON object with fields like &quot;name&quot;, &quot;description&quot;, &quot;parameters&quot; (per JSON Schema), and optionally, &quot;responses&quot; (per JSON Schema). 34 35 ```json 36 [ 37 {&quot;name&quot;: &quot;direct-injected-document&quot;, &quot;description&quot;: &quot;This is a special tool to directly inject user-uploaded documents into the chat as additional context. DO NOT use this tool by yourself!&quot;, &quot;parameters&quot;: {&quot;type&quot;: &quot;object&quot;, &quot;properties&quot;: {}, &quot;required&quot;: []}, &quot;responses&quot;: {&quot;200&quot;: {&quot;description&quot;: &quot;Successfully returned a list of chunked text snippets from the directly uploaded documents.&quot;, &quot;content&quot;: {&quot;application/json&quot;: {&quot;schema&quot;: {&quot;type&quot;: &quot;array&quot;, &quot;items&quot;: {&quot;type&quot;: &quot;object&quot;, &quot;required&quot;: [&quot;url&quot;, &quot;snippet&quot;], &quot;properties&quot;: {&quot;url&quot;: {&quot;type&quot;: &quot;string&quot;, &quot;description&quot;: &quot;The url of the uploaded document.&quot;}, &quot;snippet&quot;: {&quot;type&quot;: &quot;string&quot;, &quot;description&quot;: &quot;The text snippet for the returned document chunk.&quot;}}}}}}}}} 38 ] 39 ``` 40 41 # Default Preamble 42 The following instructions are your defaults unless specified elsewhere in a developer system message or user prompt. 43 - Your name is Command. 44 - You are a large language model built by Cohere. 45 - You reply conversationally with a friendly and informative tone and often include introductory statements and follow-up questions. 46 - If the input is ambiguous, ask clarifying follow-up questions. 47 - Use Markdown-specific formatting in your response (for example to highlight phrases in bold or italics, create tables, or format code blocks). 48 - Use LaTeX to generate mathematical notation for complex equations. 49 - When responding in English, use American English unless context indicates otherwise. 50 - When outputting responses of more than seven sentences, split the response into paragraphs. 51 - Prefer the active voice. 52 - Adhere to the APA style guidelines for punctuation, spelling, hyphenation, capitalization, numbers, lists, and quotation marks. Do not worry about them for other elements such as italics, citations, figures, or references. 53 - Use gender-neutral pronouns for unspecified persons. 54 - Limit lists to no more than 10 items unless the list is a set of finite instructions, in which case complete the list. 55 - Use the third person when asked to write a summary. 56 - When asked to extract values from source material, use the exact form, separated by commas. 57 - When generating code output, please provide an explanation after the code. 58 - When generating code output without specifying the programming language, please generate Python code. 59 - If you are asked a question that requires reasoning, first think through your answer, slowly and step by step, then answer.&lt;|END_OF_TURN_TOKEN|&gt;&lt;|START_OF_TURN_TOKEN|&gt;&lt;|USER_TOKEN|&gt;What has Man always dreamed of?&lt;|END_OF_TURN_TOKEN|&gt;&lt;|START_OF_TURN_TOKEN|&gt;&lt;|CHATBOT_TOKEN|&gt;&lt;|START_THINKING|&gt;I will look through the document to address the users needs.&lt;|END_THINKING|&gt;&lt;|START_ACTION|&gt;[ 60 {&quot;tool_call_id&quot;: &quot;0&quot;, &quot;tool_name&quot;: &quot;direct-injected-document&quot;, &quot;parameters&quot;: {}} 61 ]&lt;|END_ACTION|&gt;&lt;|END_OF_TURN_TOKEN|&gt;&lt;|START_OF_TURN_TOKEN|&gt;&lt;|SYSTEM_TOKEN|&gt;&lt;|START_TOOL_RESULT|&gt;[ 62 { 63 &quot;tool_call_id&quot;: &quot;0&quot;, 64 &quot;results&quot;: { 65 &quot;0&quot;: {&quot;body&quot;: &quot;Man has always dreamed of destroying the moon. In this essay, I shall...&quot;, &quot;heading&quot;: &quot;The Moon: Our Age-Old Foe&quot;}, 66 &quot;1&quot;: {&quot;body&quot;: &quot;Man&#x27;s dream has always been to find love. This profound lesson...&quot;, &quot;heading&quot;: &quot;Love is all you need&quot;}, 67 &quot;2&quot;: {&quot;body&quot;: &quot;Although often underappreciated, the sun provides several notable benefits...&quot;, &quot;heading&quot;: &quot;The Sun: Our Age-Old Friend&quot;} 68 }, 69 &quot;is_error&quot;: null 70 } 71 ]&lt;|END_TOOL_RESULT|&gt;&lt;|END_OF_TURN_TOKEN|&gt;&lt;|START_OF_TURN_TOKEN|&gt;&lt;|CHATBOT_TOKEN|&gt; Example of a Grounded Generation completion 1 There are two answers to this question. Man has dreamed of destroying the moon and finding love. Option 2: Regular Generation You may find that simply including relevant documents directly in a user message works just as well, or better than using the documents parameter to render the special grounded generation template. Grounded Generation is generally a strong default, but Regular Generation can offer more control and customization over the prompt, at the cost of some effort to find an optimal prompt. We encourage users to play with both Grounded Generation and Regular Generation, and to evaluate which mode works best for their specific use case. Tool use, Function Calling &amp; Agent capabilities Command A has been specifically trained with conversational tool use capabilities. This allows the model to interact with external tools like APIs, databases, or search engines. These capabilities have been trained into the model via a mixture of supervised fine-tuning and preference fine-tuning, using a specific prompt template. Deviating from this prompt template will likely reduce performance, but we encourage experimentation. These tool use capabilities unlock two use cases: Function Calling : A single inference where Command A selects relevant tools to fulfill a user request. Agents : Several inference cycles where Command A iterates through Plan → Action → Observation loops until it arrives at a final response. Both function calling and agents work in the same way. Given a conversation as input (with an optional system message), along with a list of available tools, the model will generate one of the following: Tool Selection : A high-level plan followed by a json-formatted list of actions to execute on a subset of the supplied tools. Command A may select multiple tools in parallel, and it may select a tool more than once. It is then up to the developer to execute these tool calls and obtain tool results. Or, a Response : A final response to the user. This can occur if the model chooses not to use any tools, such as when greeting the user or asking clarifying questions, or after processing tool results to formulate a final answer. Tool use in Command A is supported through chat templates in Transformers. We recommend providing tool descriptions using JSON schema. Here is a quick example showing tool use. Usage: Generate the Tool Use prompt PYTHON 1 # make sure you install the latest version of transformers 2 #!pip install git+https://github.com/huggingface/transformers.git 3 from transformers import AutoTokenizer, AutoModelForCausalLM 4 5 # Load the model and tokenizer 6 model_id = &quot;CohereForAI/c4ai-command-a-03-2025&quot; 7 tokenizer = AutoTokenizer.from_pretrained(model_id) 8 model = AutoModelForCausalLM.from_pretrained(model_id) 9 10 # Define conversation input 11 conversation = [ 12 { 13 &quot;role&quot;: &quot;user&quot;, 14 &quot;content&quot;: &quot;Can you provide a sales summary for 29th September 2023, and also give me some details about the products in the &#x27;Electronics&#x27; category, for example their prices and stock levels?&quot;, 15 } 16 ] 17 18 # Define tools 19 tools = [ 20 { 21 &quot;type&quot;: &quot;function&quot;, 22 &quot;function&quot;: { 23 &quot;name&quot;: &quot;query_daily_sales_report&quot;, 24 &quot;description&quot;: &quot;Connects to a database to retrieve overall sales volumes and sales information for a given day.&quot;, 25 &quot;parameters&quot;: { 26 &quot;type&quot;: &quot;object&quot;, 27 &quot;properties&quot;: { 28 &quot;day&quot;: { 29 &quot;description&quot;: &quot;Retrieves sales data for this day, formatted as YYYY-MM-DD.&quot;, 30 &quot;type&quot;: &quot;string&quot;, 31 } 32 }, 33 &quot;required&quot;: [&quot;day&quot;], 34 }, 35 }, 36 }, 37 { 38 &quot;type&quot;: &quot;function&quot;, 39 &quot;function&quot;: { 40 &quot;name&quot;: &quot;query_product_catalog&quot;, 41 &quot;description&quot;: &quot;Connects to a a product catalog with information about all the products being sold, including categories, prices, and stock levels.&quot;, 42 &quot;parameters&quot;: { 43 &quot;type&quot;: &quot;object&quot;, 44 &quot;properties&quot;: { 45 &quot;category&quot;: { 46 &quot;description&quot;: &quot;Retrieves product information data for all products in this category.&quot;, 47 &quot;type&quot;: &quot;string&quot;, 48 } 49 }, 50 &quot;required&quot;: [&quot;category&quot;], 51 }, 52 }, 53 }, 54 ] 55 56 # Get the Tool Use prompt 57 input_prompt = tokenizer.apply_chat_template( 58 conversation=conversation, 59 tools=tools, 60 tokenize=False, 61 add_generation_prompt=True, 62 return_tensors=&quot;pt&quot;, 63 ) 64 65 print(&quot;== Prompt for step 1 of the Agent:&quot;, input_prompt) 66 67 # Tokenize the prompt 68 input_ids = tokenizer.encode_plus(input_prompt, return_tensors=&quot;pt&quot;) 69 70 # Generate a response 71 gen_tokens = model.generate( 72 input_ids, 73 max_new_tokens=512, 74 do_sample=True, 75 temperature=0.3, 76 skip_special_tokens=True, 77 ) 78 79 # Decode and print the generated text along with generation prompt 80 gen_text = tokenizer.decode( 81 gen_tokens[0][len(input_ids[0]) :], skip_special_tokens=True 82 ) 83 print(gen_text) Example of a Tool Use prompt 1 &lt;|START_OF_TURN_TOKEN|&gt;&lt;|SYSTEM_TOKEN|&gt;# System Preamble 2 You are in contextual safety mode. You will reject requests to generate child sexual abuse material and child exploitation material in your responses. You will accept to provide information and creative content related to violence, hate, misinformation or sex, but you will not provide any content that could directly or indirectly lead to harmful outcomes. 3 4 Your information cutoff date is June 2024. 5 6 You have been trained on data in English, French, Spanish, Italian, German, Portuguese, Japanese, Korean, Modern Standard Arabic, Mandarin, Russian, Indonesian, Turkish, Dutch, Polish, Persian, Vietnamese, Czech, Hindi, Ukrainian, Romanian, Greek and Hebrew but have the ability to speak many more languages. 7 8 You have been trained to have advanced reasoning and tool-use capabilities and you should make best use of these skills to serve user&#x27;s requests. 9 10 ## Tool Use 11 Think about how you can make best use of the provided tools to help with the task and come up with a high level plan that you will execute first. 12 13 0. Start by writing &lt;|START_THINKING|&gt; followed by a detailed step by step plan of how you will solve the problem. For each step explain your thinking fully and give details of required tool calls (if needed). Unless specified otherwise, you write your plan in natural language. When you finish, close it out with &lt;|END_THINKING|&gt;. 14 You can optionally choose to skip this step when the user request is so straightforward to address that only a trivial plan would be needed. 15 NOTE: You MUST skip this step when you are directly responding to the user&#x27;s request without using any tools. 16 17 Then carry out your plan by repeatedly executing the following steps. 18 1. Action: write &lt;|START_ACTION|&gt; followed by a list of JSON-formatted tool calls, with each one containing &quot;tool_name&quot; and &quot;parameters&quot; fields. 19 When there are multiple tool calls which are completely independent of each other (i.e. they can be executed in parallel), you should list them out all together in one step. When you finish, close it out with &lt;|END_ACTION|&gt;. 20 2. Observation: you will then receive results of those tool calls in JSON format in the very next turn, wrapped around by &lt;|START_TOOL_RESULT|&gt; and &lt;|END_TOOL_RESULT|&gt;. Carefully observe those results and think about what to do next. Note that these results will be provided to you in a separate turn. NEVER hallucinate results. 21 Every tool call produces a list of results (when a tool call produces no result or a single result, it&#x27;ll still get wrapped inside a list). Each result is clearly linked to its originating tool call via its &quot;tool_call_id&quot;. 22 3. Reflection: start the next turn by writing &lt;|START_THINKING|&gt; followed by what you&#x27;ve figured out so far, any changes you need to make to your plan, and what you will do next. When you finish, close it out with &lt;|END_THINKING|&gt;. 23 You can optionally choose to skip this step when everything is going according to plan and no special pieces of information or reasoning chains need to be recorded. 24 NOTE: You MUST skip this step when you are done with tool-use actions and are ready to respond to the user. 25 26 You can repeat the above 3 steps multiple times (could be 0 times too if no suitable tool calls are available or needed), until you decide it&#x27;s time to finally respond to the user. 27 28 4. Response: then break out of the loop and write &lt;|START_RESPONSE|&gt; followed by a piece of text which serves as a response to the user&#x27;s last request. Use all previous tool calls and results to help you when formulating your response. When you finish, close it out with &lt;|END_RESPONSE|&gt;. 29 30 ## Available Tools 31 Here is the list of tools that you have available to you. 32 You can ONLY use the tools listed here. When a tool is not listed below, it is NOT available and you should NEVER attempt to use it. 33 Each tool is represented as a JSON object with fields like &quot;name&quot;, &quot;description&quot;, &quot;parameters&quot; (per JSON Schema), and optionally, &quot;responses&quot; (per JSON Schema). 34 35 ```json 36 [ 37 {&quot;name&quot;: &quot;query_daily_sales_report&quot;, &quot;description&quot;: &quot;Connects to a database to retrieve overall sales volumes and sales information for a given day.&quot;, &quot;parameters&quot;: {&quot;type&quot;: &quot;object&quot;, &quot;properties&quot;: {&quot;day&quot;: {&quot;description&quot;: &quot;Retrieves sales data for this day, formatted as YYYY-MM-DD.&quot;, &quot;type&quot;: &quot;string&quot;}}, &quot;required&quot;: [&quot;day&quot;]}, &quot;responses&quot;: null}, 38 {&quot;name&quot;: &quot;query_product_catalog&quot;, &quot;description&quot;: &quot;Connects to a a product catalog with information about all the products being sold, including categories, prices, and stock levels.&quot;, &quot;parameters&quot;: {&quot;type&quot;: &quot;object&quot;, &quot;properties&quot;: {&quot;category&quot;: {&quot;description&quot;: &quot;Retrieves product information data for all products in this category.&quot;, &quot;type&quot;: &quot;string&quot;}}, &quot;required&quot;: [&quot;category&quot;]}, &quot;responses&quot;: null} 39 ] 40 ``` 41 42 # Default Preamble 43 The following instructions are your defaults unless specified elsewhere in a developer preamble or user prompt. 44 - Your name is Command. 45 - You are a large language model built by Cohere. 46 - You reply conversationally with a friendly and informative tone and often include introductory statements and follow-up questions. 47 - If the input is ambiguous, ask clarifying follow-up questions. 48 - Use Markdown-specific formatting in your response (for example to highlight phrases in bold or italics, create tables, or format code blocks). 49 - Use LaTeX to generate mathematical notation for complex equations. 50 - When responding in English, use American English unless context indicates otherwise. 51 - When outputting responses of more than seven sentences, split the response into paragraphs. 52 - Prefer the active voice. 53 - Adhere to the APA style guidelines for punctuation, spelling, hyphenation, capitalization, numbers, lists, and quotation marks. Do not worry about them for other elements such as italics, citations, figures, or references. 54 - Use gender-neutral pronouns for unspecified persons. 55 - Limit lists to no more than 10 items unless the list is a set of finite instructions, in which case complete the list. 56 - Use the third person when asked to write a summary. 57 - When asked to extract values from source material, use the exact form, separated by commas. 58 - When generating code output, please provide an explanation after the code. 59 - When generating code output without specifying the programming language, please generate Python code. 60 - If you are asked a question that requires reasoning, first think through your answer, slowly and step by step, then answer.&lt;|END_OF_TURN_TOKEN|&gt;&lt;|START_OF_TURN_TOKEN|&gt;&lt;|USER_TOKEN|&gt;Can you provide a sales summary for 29th September 2023, and also give me some details about the products in the &#x27;Electronics&#x27; category, for example their prices and stock levels?&lt;|END_OF_TURN_TOKEN|&gt;&lt;|START_OF_TURN_TOKEN|&gt;&lt;|CHATBOT_TOKEN|&gt; Example of a Tool Use completion In this case, the model decides to select tools. 1 &lt;|START_THINKING|&gt;I will use the query_daily_sales_report tool to find the sales summary for 29th September 2023. I will then use the query_product_catalog tool to find the details about the products in the &#x27;Electronics&#x27; category.&lt;|END_THINKING|&gt;&lt;|START_ACTION|&gt;[ 2 {&quot;tool_call_id&quot;: &quot;0&quot;, &quot;tool_name&quot;: &quot;query_daily_sales_report&quot;, &quot;parameters&quot;: {&quot;day&quot;: &quot;2023-09-29&quot;}}, 3 {&quot;tool_call_id&quot;: &quot;1&quot;, &quot;tool_name&quot;: &quot;query_product_catalog&quot;, &quot;parameters&quot;: {&quot;category&quot;: &quot;Electronics&quot;}} 4 ]&lt;|END_ACTION|&gt; Below is what the answer would have looked like, if the model had decided to respond directly (by, for example, asking the user a follow up question.) 1 I can find the sales summary for 29th September 2023 as well as the details about the products in the &#x27;Electronics&#x27; category. However, I need to use the &#x27;query_daily_sales_report&#x27; and &#x27;query_product_catalog&#x27; tools to do this. Are you sure you would you like me to use these tools? If the model generates tool calls, you should add them to the chat history like so: PYTHON 1 tool_call_0 = { 2 &quot;name&quot;: &quot;query_daily_sales_report&quot;, 3 &quot;arguments&quot;: {&quot;day&quot;: &quot;2023-09-29&quot;}, 4 } 5 tool_call_1 = { 6 &quot;name&quot;: &quot;query_product_catalog&quot;, 7 &quot;arguments&quot;: {&quot;category&quot;: &quot;Electronics&quot;}, 8 } 9 tool_plan = &quot;I will use the &#x27;query_daily_sales_report&#x27; tool to find the sales summary for 29th September 2023. I will then use the &#x27;query_product_catalog&#x27; tool to find the details about the products in the &#x27;Electronics&#x27; category.&quot; 10 11 conversation.append( 12 { 13 &quot;role&quot;: &quot;assistant&quot;, 14 &quot;tool_calls&quot;: [ 15 {&quot;id&quot;: &quot;0&quot;, &quot;type&quot;: &quot;function&quot;, &quot;function&quot;: tool_call_0}, 16 {&quot;id&quot;: &quot;1&quot;, &quot;type&quot;: &quot;function&quot;, &quot;function&quot;: tool_call_1}, 17 ], 18 &quot;tool_plan&quot;: tool_plan, 19 } 20 ) And then call the tool and append the result, with the tool role, like below. It is crucial to format the tool results as a dictionary: PYTHON 1 api_response_query_daily_sales_report = { 2 &quot;date&quot;: &quot;2023-09-29&quot;, 3 &quot;summary&quot;: &quot;Total Sales Amount: 10000, Total Units Sold: 250&quot;, 4 } # this needs to be a dictionary!! 5 api_response_query_product_catalog = { 6 &quot;category&quot;: &quot;Electronics&quot;, 7 &quot;products&quot;: [ 8 { 9 &quot;product_id&quot;: &quot;E1001&quot;, 10 &quot;name&quot;: &quot;Smartphone&quot;, 11 &quot;price&quot;: 500, 12 &quot;stock_level&quot;: 20, 13 }, 14 { 15 &quot;product_id&quot;: &quot;E1002&quot;, 16 &quot;name&quot;: &quot;Laptop&quot;, 17 &quot;price&quot;: 1000, 18 &quot;stock_level&quot;: 15, 19 }, 20 { 21 &quot;product_id&quot;: &quot;E1003&quot;, 22 &quot;name&quot;: &quot;Tablet&quot;, 23 &quot;price&quot;: 300, 24 &quot;stock_level&quot;: 25, 25 }, 26 ], 27 } 28 29 conversation.append( 30 { 31 &quot;role&quot;: &quot;tool&quot;, 32 &quot;tool_call_id&quot;: &quot;0&quot;, 33 &quot;content&quot;: api_response_query_daily_sales_report, 34 } 35 ) 36 conversation.append( 37 { 38 &quot;role&quot;: &quot;tool&quot;, 39 &quot;tool_call_id&quot;: &quot;1&quot;, 40 &quot;content&quot;: api_response_query_product_catalog, 41 } 42 ) After that, you can generate() again to let the model use the tool result in the chat. Usage: Generate the Tool Use prompt with tool results in the conversation PYTHON 1 from transformers import AutoTokenizer, AutoModelForCausalLM 2 3 # Load the model and tokenizer 4 model_id = &quot;CohereForAI/c4ai-command-a-03-2025&quot; 5 tokenizer = AutoTokenizer.from_pretrained(model_id) 6 model = AutoModelForCausalLM.from_pretrained(model_id) 7 8 # Get the Tool Use prompt 9 input_prompt = tokenizer.apply_chat_template( 10 conversation=conversation, 11 tools=tools, 12 tokenize=False, 13 add_generation_prompt=True, 14 return_tensors=&quot;pt&quot;, 15 ) 16 print(&quot;== Prompt for step 2 of the Agent:&quot;, input_prompt) 17 18 # Tokenize the prompt 19 input_ids = tokenizer.encode_plus(input_prompt, return_tensors=&quot;pt&quot;) 20 21 # Generate a response 22 gen_tokens = model.generate( 23 input_ids, 24 max_new_tokens=512, 25 do_sample=True, 26 temperature=0.3, 27 ) 28 29 # Decode and print the generated text along with generation prompt 30 gen_text = tokenizer.decode( 31 gen_tokens[0][len(input_ids[0]) :], skip_special_tokens=True 32 ) 33 print(gen_text) Example of a Tool Use prompt with tool results in the conversation 1 &lt;|START_OF_TURN_TOKEN|&gt;&lt;|SYSTEM_TOKEN|&gt;# System Preamble 2 You are in contextual safety mode. You will reject requests to generate child sexual abuse material and child exploitation material in your responses. You will accept to provide information and creative content related to violence, hate, misinformation or sex, but you will not provide any content that could directly or indirectly lead to harmful outcomes. 3 4 Your information cutoff date is June 2024. 5 6 You have been trained on data in English, French, Spanish, Italian, German, Portuguese, Japanese, Korean, Modern Standard Arabic, Mandarin, Russian, Indonesian, Turkish, Dutch, Polish, Persian, Vietnamese, Czech, Hindi, Ukrainian, Romanian, Greek and Hebrew but have the ability to speak many more languages. 7 8 You have been trained to have advanced reasoning and tool-use capabilities and you should make best use of these skills to serve user&#x27;s requests. 9 10 ## Tool Use 11 Think about how you can make best use of the provided tools to help with the task and come up with a high level plan that you will execute first. 12 13 0. Start by writing &lt;|START_THINKING|&gt; followed by a detailed step by step plan of how you will solve the problem. For each step explain your thinking fully and give details of required tool calls (if needed). Unless specified otherwise, you write your plan in natural language. When you finish, close it out with &lt;|END_THINKING|&gt;. 14 You can optionally choose to skip this step when the user request is so straightforward to address that only a trivial plan would be needed. 15 NOTE: You MUST skip this step when you are directly responding to the user&#x27;s request without using any tools. 16 17 Then carry out your plan by repeatedly executing the following steps. 18 1. Action: write &lt;|START_ACTION|&gt; followed by a list of JSON-formatted tool calls, with each one containing &quot;tool_name&quot; and &quot;parameters&quot; fields. 19 When there are multiple tool calls which are completely independent of each other (i.e. they can be executed in parallel), you should list them out all together in one step. When you finish, close it out with &lt;|END_ACTION|&gt;. 20 2. Observation: you will then receive results of those tool calls in JSON format in the very next turn, wrapped around by &lt;|START_TOOL_RESULT|&gt; and &lt;|END_TOOL_RESULT|&gt;. Carefully observe those results and think about what to do next. Note that these results will be provided to you in a separate turn. NEVER hallucinate results. 21 Every tool call produces a list of results (when a tool call produces no result or a single result, it&#x27;ll still get wrapped inside a list). Each result is clearly linked to its originating tool call via its &quot;tool_call_id&quot;. 22 3. Reflection: start the next turn by writing &lt;|START_THINKING|&gt; followed by what you&#x27;ve figured out so far, any changes you need to make to your plan, and what you will do next. When you finish, close it out with &lt;|END_THINKING|&gt;. 23 You can optionally choose to skip this step when everything is going according to plan and no special pieces of information or reasoning chains need to be recorded. 24 NOTE: You MUST skip this step when you are done with tool-use actions and are ready to respond to the user. 25 26 You can repeat the above 3 steps multiple times (could be 0 times too if no suitable tool calls are available or needed), until you decide it&#x27;s time to finally respond to the user. 27 28 4. Response: then break out of the loop and write &lt;|START_RESPONSE|&gt; followed by a piece of text which serves as a response to the user&#x27;s last request. Use all previous tool calls and results to help you when formulating your response. When you finish, close it out with &lt;|END_RESPONSE|&gt;. 29 30 ## Available Tools 31 Here is the list of tools that you have available to you. 32 You can ONLY use the tools listed here. When a tool is not listed below, it is NOT available and you should NEVER attempt to use it. 33 Each tool is represented as a JSON object with fields like &quot;name&quot;, &quot;description&quot;, &quot;parameters&quot; (per JSON Schema), and optionally, &quot;responses&quot; (per JSON Schema). 34 35 ```json 36 [ 37 {&quot;name&quot;: &quot;query_daily_sales_report&quot;, &quot;description&quot;: &quot;Connects to a database to retrieve overall sales volumes and sales information for a given day.&quot;, &quot;parameters&quot;: {&quot;type&quot;: &quot;object&quot;, &quot;properties&quot;: {&quot;day&quot;: {&quot;description&quot;: &quot;Retrieves sales data for this day, formatted as YYYY-MM-DD.&quot;, &quot;type&quot;: &quot;string&quot;}}, &quot;required&quot;: [&quot;day&quot;]}, &quot;responses&quot;: null}, 38 {&quot;name&quot;: &quot;query_product_catalog&quot;, &quot;description&quot;: &quot;Connects to a a product catalog with information about all the products being sold, including categories, prices, and stock levels.&quot;, &quot;parameters&quot;: {&quot;type&quot;: &quot;object&quot;, &quot;properties&quot;: {&quot;category&quot;: {&quot;description&quot;: &quot;Retrieves product information data for all products in this category.&quot;, &quot;type&quot;: &quot;string&quot;}}, &quot;required&quot;: [&quot;category&quot;]}, &quot;responses&quot;: null} 39 ] 40 ``` 41 42 # Default Preamble 43 The following instructions are your defaults unless specified elsewhere in a developer preamble or user prompt. 44 - Your name is Command. 45 - You are a large language model built by Cohere. 46 - You reply conversationally with a friendly and informative tone and often include introductory statements and follow-up questions. 47 - If the input is ambiguous, ask clarifying follow-up questions. 48 - Use Markdown-specific formatting in your response (for example to highlight phrases in bold or italics, create tables, or format code blocks). 49 - Use LaTeX to generate mathematical notation for complex equations. 50 - When responding in English, use American English unless context indicates otherwise. 51 - When outputting responses of more than seven sentences, split the response into paragraphs. 52 - Prefer the active voice. 53 - Adhere to the APA style guidelines for punctuation, spelling, hyphenation, capitalization, numbers, lists, and quotation marks. Do not worry about them for other elements such as italics, citations, figures, or references. 54 - Use gender-neutral pronouns for unspecified persons. 55 - Limit lists to no more than 10 items unless the list is a set of finite instructions, in which case complete the list. 56 - Use the third person when asked to write a summary. 57 - When asked to extract values from source material, use the exact form, separated by commas. 58 - When generating code output, please provide an explanation after the code. 59 - When generating code output without specifying the programming language, please generate Python code. 60 - If you are asked a question that requires reasoning, first think through your answer, slowly and step by step, then answer.&lt;|END_OF_TURN_TOKEN|&gt;&lt;|START_OF_TURN_TOKEN|&gt;&lt;|USER_TOKEN|&gt;Can you provide a sales summary for 29th September 2023, and also give me some details about the products in the &#x27;Electronics&#x27; category, for example their prices and stock levels?&lt;|END_OF_TURN_TOKEN|&gt;&lt;|START_OF_TURN_TOKEN|&gt;&lt;|CHATBOT_TOKEN|&gt;&lt;|START_THINKING|&gt;I will use the &#x27;query_daily_sales_report&#x27; tool to find the sales summary for 29th September 2023. I will then use the &#x27;query_product_catalog&#x27; tool to find the details about the products in the &#x27;Electronics&#x27; category.&lt;|END_THINKING|&gt;&lt;|START_ACTION|&gt;[ 61 {&quot;tool_call_id&quot;: &quot;0&quot;, &quot;tool_name&quot;: &quot;query_daily_sales_report&quot;, &quot;parameters&quot;: {&quot;day&quot;: &quot;2023-09-29&quot;}}, 62 {&quot;tool_call_id&quot;: &quot;1&quot;, &quot;tool_name&quot;: &quot;query_product_catalog&quot;, &quot;parameters&quot;: {&quot;category&quot;: &quot;Electronics&quot;}} 63 ]&lt;|END_ACTION|&gt;&lt;|END_OF_TURN_TOKEN|&gt;&lt;|START_OF_TURN_TOKEN|&gt;&lt;|SYSTEM_TOKEN|&gt;&lt;|START_TOOL_RESULT|&gt;[ 64 { 65 &quot;tool_call_id&quot;: &quot;0&quot;, 66 &quot;results&quot;: { 67 &quot;0&quot;: {&quot;date&quot;: &quot;2023-09-29&quot;, &quot;summary&quot;: &quot;Total Sales Amount: 10000, Total Units Sold: 250&quot;} 68 }, 69 &quot;is_error&quot;: null 70 }, 71 { 72 &quot;tool_call_id&quot;: &quot;1&quot;, 73 &quot;results&quot;: { 74 &quot;0&quot;: {&quot;category&quot;: &quot;Electronics&quot;, &quot;products&quot;: [{&quot;product_id&quot;: &quot;E1001&quot;, &quot;name&quot;: &quot;Smartphone&quot;, &quot;price&quot;: 500, &quot;stock_level&quot;: 20}, {&quot;product_id&quot;: &quot;E1002&quot;, &quot;name&quot;: &quot;Laptop&quot;, &quot;price&quot;: 1000, &quot;stock_level&quot;: 15}, {&quot;product_id&quot;: &quot;E1003&quot;, &quot;name&quot;: &quot;Tablet&quot;, &quot;price&quot;: 300, &quot;stock_level&quot;: 25}]} 75 }, 76 &quot;is_error&quot;: null 77 } 78 ]&lt;|END_TOOL_RESULT|&gt;&lt;|END_OF_TURN_TOKEN|&gt;&lt;|START_OF_TURN_TOKEN|&gt;&lt;|CHATBOT_TOKEN|&gt; Example of a completion In this case, the model decides to select tools. 1 On 29th September 2023, the total sales amount was £10000 and the total units sold were 250. 2 3 The following products are in the &#x27;Electronics&#x27; category: 4 5 - Smartphone, £500, stock level 20 6 - Laptop, £1000, stock level 15 7 - Tablet, £300, stock level 25 Was this page helpful? Yes No Edit this page Previous Built with ",
      "fetch_method": "direct-html"
    },
    {
      "arxiv_id": "https://huggingface.co/docs/transformers/main/en/chat_templating#advanced-retrieval-augmented-generation",
      "full_text": " Chat templates Hugging Face Models Datasets Spaces Community Docs Enterprise Pricing Log In Sign Up Transformers documentation Chat templates Transformers 🏡 View all docs AWS Trainium &amp; Inferentia Accelerate Argilla AutoTrain Bitsandbytes Chat UI Dataset viewer Datasets Deploying on AWS Diffusers Distilabel Evaluate Gradio Hub Hub Python Library Huggingface.js Inference Endpoints (dedicated) Inference Providers Kernels LeRobot Leaderboards Lighteval Microsoft Azure Optimum PEFT Safetensors Sentence Transformers TRL Tasks Text Embeddings Inference Text Generation Inference Tokenizers Trackio Transformers Transformers.js smolagents timm Search documentation main v4.56.2 v4.55.4 v4.53.3 v4.52.3 v4.51.3 v4.50.0 v4.49.0 v4.48.2 v4.47.1 v4.46.3 v4.45.2 v4.44.2 v4.43.4 v4.42.4 v4.41.2 v4.40.2 v4.39.3 v4.38.2 v4.37.2 v4.36.1 v4.35.2 v4.34.1 v4.33.3 v4.32.1 v4.31.0 v4.30.0 v4.29.1 v4.28.1 v4.27.2 v4.26.1 v4.25.1 v4.24.0 v4.23.1 v4.22.2 v4.21.3 v4.20.1 v4.19.4 v4.18.0 v4.17.0 v4.16.2 v4.15.0 v4.14.1 v4.13.0 v4.12.5 v4.11.3 v4.10.1 v4.9.2 v4.8.2 v4.7.0 v4.6.0 v4.5.1 v4.4.2 v4.3.3 v4.2.2 v4.1.1 v4.0.1 v3.5.1 v3.4.0 v3.3.1 v3.2.0 v3.1.0 v3.0.2 v2.11.0 v2.10.0 v2.9.1 v2.8.0 v2.7.0 v2.6.0 v2.5.1 v2.4.1 v2.3.0 v2.2.2 v2.1.1 v2.0.0 v1.2.0 v1.1.0 v1.0.0 doc-builder-html AR DE EN ES FR HI IT JA KO PT TE TR ZH Get started Transformers Installation Quickstart Base classes Inference Pipeline API LLMs Chat with models Chat basics Chat templates Multimodal chat templates Tool use Writing a chat template Serving Optimization Agents Tools Inference server backends Training Quantization Export to production Resources Contribute API You are viewing main version, which requires installation from source . If you&#39;d like regular pip install, checkout the latest stable version ( v4.56.2 ). Join the Hugging Face community and get access to the augmented documentation experience Collaborate on models, datasets and Spaces Faster examples with accelerated inference Switch between documentation themes Sign Up to get started Chat templates The chat basics guide covers how to store chat histories and generate text from chat models using TextGenerationPipeline . This guide is intended for more advanced users, and covers the underlying classes and methods, as well as the key concepts for understanding what’s actually going on when you chat with a model. The critical insight needed to understand chat models is this: All causal LMs, whether chat-trained or not, continue a sequence of tokens. When causal LMs are trained, the training usually begins with “pre-training” on a huge corpus of text, which creates a “base” model. These base models are then often “fine-tuned” for chat, which means training them on data that is formatted as a sequence of messages. The chat is still just a sequence of tokens, though! The list of role and content dictionaries that you pass to a chat model get converted to a token sequence, often with control tokens like &lt;|user|&gt; or &lt;|assistant|&gt; or &lt;|end_of_message|&gt; , which allow the model to see the chat structure. There are many possible chat formats, and different models may use different formats or control tokens, even if they were fine-tuned from the same base model! Don’t panic, though - you don’t need to memorize every possible chat format in order to use chat models. Chat models come with chat templates , which indicate how they expect chats to be formatted. You can access these with the apply_chat_template method. Let’s see two examples. Both of these models are fine-tuned from the same Mistral-7B base model: Mistral Zephyr Copied from transformers import AutoTokenizer tokenizer = AutoTokenizer.from_pretrained( &quot;mistralai/Mistral-7B-Instruct-v0.1&quot; ) chat = [ { &quot;role&quot; : &quot;user&quot; , &quot;content&quot; : &quot;Hello, how are you?&quot; }, { &quot;role&quot; : &quot;assistant&quot; , &quot;content&quot; : &quot;I&#x27;m doing great. How can I help you today?&quot; }, { &quot;role&quot; : &quot;user&quot; , &quot;content&quot; : &quot;I&#x27;d like to show off how chat templating works!&quot; }, ] tokenizer.apply_chat_template(chat, tokenize= False ) Copied &lt; s &gt; [INST] Hello, how are you? [/INST]I&#x27;m doing great. How can I help you today? &lt;/ s &gt; [INST] I&#x27;d like to show off how chat templating works! [/INST] Mistral-7B-Instruct uses [INST] and [/INST] tokens to indicate the start and end of user messages, while Zephyr-7B uses &lt;|user|&gt; and &lt;|assistant|&gt; tokens to indicate speaker roles. This is why chat templates are important - with the wrong control tokens, these models would have drastically worse performance. Using apply_chat_template The input to apply_chat_template should be structured as a list of dictionaries with role and content keys. The role key specifies the speaker, and the content key contains the message. The common roles are: user for messages from the user assistant for messages from the model system for directives on how the model should act (usually placed at the beginning of the chat) apply_chat_template takes this list and returns a formatted sequence. Set tokenize=True if you want to tokenize the sequence. Copied import torch from transformers import AutoModelForCausalLM, AutoTokenizer tokenizer = AutoTokenizer.from_pretrained( &quot;HuggingFaceH4/zephyr-7b-beta&quot; ) model = AutoModelForCausalLM.from_pretrained( &quot;HuggingFaceH4/zephyr-7b-beta&quot; , device_map= &quot;auto&quot; , dtype=torch.bfloat16) messages = [ { &quot;role&quot; : &quot;system&quot; , &quot;content&quot; : &quot;You are a friendly chatbot who always responds in the style of a pirate&quot; ,}, { &quot;role&quot; : &quot;user&quot; , &quot;content&quot; : &quot;How many helicopters can a human eat in one sitting?&quot; }, ] tokenized_chat = tokenizer.apply_chat_template(messages, tokenize= True , add_generation_prompt= True , return_tensors= &quot;pt&quot; ) print (tokenizer.decode(tokenized_chat[ 0 ])) Copied &lt;|system|&gt; You are a friendly chatbot who always responds in the style of a pirate &lt;/ s &gt; &lt;|user|&gt; How many helicopters can a human eat in one sitting? &lt;/ s &gt; &lt;|assistant|&gt; Pass the tokenized chat to generate() to generate a response. Copied outputs = model.generate(tokenized_chat, max_new_tokens= 128 ) print (tokenizer.decode(outputs[ 0 ])) Copied &lt;|system|&gt; You are a friendly chatbot who always responds in the style of a pirate &lt;/ s &gt; &lt;|user|&gt; How many helicopters can a human eat in one sitting? &lt;/ s &gt; &lt;|assistant|&gt; Matey, I&#x27;m afraid I must inform ye that humans cannot eat helicopters. Helicopters are not food, they are flying machines. Food is meant to be eaten, like a hearty plate o&#x27; grog, a savory bowl o&#x27; stew, or a delicious loaf o&#x27; bread. But helicopters, they be for transportin&#x27; and movin&#x27; around, not for eatin&#x27;. So, I&#x27;d say none, me hearties. None at all. Some tokenizers add special &lt;bos&gt; and &lt;eos&gt; tokens. Chat templates should already include all the necessary special tokens, and adding additional special tokens is often incorrect or duplicated, hurting model performance. When you format text with apply_chat_template(tokenize=False) , make sure you set add_special_tokens=False if you tokenize later to avoid duplicating these tokens. This isn’t an issue if you use apply_chat_template(tokenize=True) , which means it’s usually the safer option! add_generation_prompt You may have noticed the add_generation_prompt argument in the above examples. This argument adds tokens to the end of the chat that indicate the start of an assistant response. Remember: Beneath all the chat abstractions, chat models are still just language models that continue a sequence of tokens! If you include tokens that tell it that it’s now in an assistant response, it will correctly write a response, but if you don’t include these tokens, the model may get confused and do something strange, like continuing the user’s message instead of replying to it! Let’s see an example to understand what add_generation_prompt is actually doing. First, let’s format a chat without add_generation_prompt : Copied tokenized_chat = tokenizer.apply_chat_template(messages, tokenize= False , add_generation_prompt= False ) tokenized_chat Copied &lt;|im _start|&gt;user Hi there!&lt;|im_ end|&gt; &lt;|im _start|&gt;assistant Nice to meet you!&lt;|im_ end|&gt; &lt;|im _start|&gt;user Can I ask a question?&lt;|im_ end|&gt; Now, let’s format the same chat with add_generation_prompt=True : Copied tokenized_chat = tokenizer.apply_chat_template(messages, tokenize= False , add_generation_prompt= True ) tokenized_chat Copied &lt;|im _start|&gt;user Hi there!&lt;|im_ end|&gt; &lt;|im _start|&gt;assistant Nice to meet you!&lt;|im_ end|&gt; &lt;|im _start|&gt;user Can I ask a question?&lt;|im_ end|&gt; &lt;|im _start|&gt;assistant When add_generation_prompt=True , &lt;|im_start|&gt;assistant is added at the end to indicate the start of an assistant message. This lets the model know an assistant response is next. Not all models require generation prompts, and some models, like Llama , don’t have any special tokens before the assistant response. In these cases, add_generation_prompt has no effect. continue_final_message The continue_final_message parameter controls whether the final message in the chat should be continued or not instead of starting a new one. It removes end of sequence tokens so that the model continues generation from the final message. This is useful for “prefilling” a model response. In the example below, the model generates text that continues the JSON string rather than starting a new message. It can be very useful for improving the accuracy of instruction following when you know how to start its replies. Copied chat = [ { &quot;role&quot; : &quot;user&quot; , &quot;content&quot; : &quot;Can you format the answer in JSON?&quot; }, { &quot;role&quot; : &quot;assistant&quot; , &quot;content&quot; : &#x27;{&quot;name&quot;: &quot;&#x27; }, ] formatted_chat = tokenizer.apply_chat_template(chat, tokenize= True , return_dict= True , continue_final_message= True ) model.generate(**formatted_chat) You shouldn’t use add_generation_prompt and continue_final_message together. The former adds tokens that start a new message, while the latter removes end of sequence tokens. Using them together returns an error. TextGenerationPipeline sets add_generation_prompt to True by default to start a new message. However, if the final message in the chat has the assistant role, it assumes the message is a prefill and switches to continue_final_message=True . This is because most models don’t support multiple consecutive assistant messages. To override this behavior, explicitly pass the continue_final_message argument to the pipeline. Model training Training a model with a chat template is a good way to ensure the template matches the tokens the model was trained on. Apply the chat template as a preprocessing step to your dataset. Set add_generation_prompt=False because the additional tokens to prompt an assistant response aren’t helpful during training. An example of preprocessing a dataset with a chat template is shown below. Copied from transformers import AutoTokenizer from datasets import Dataset tokenizer = AutoTokenizer.from_pretrained( &quot;HuggingFaceH4/zephyr-7b-beta&quot; ) chat1 = [ { &quot;role&quot; : &quot;user&quot; , &quot;content&quot; : &quot;Which is bigger, the moon or the sun?&quot; }, { &quot;role&quot; : &quot;assistant&quot; , &quot;content&quot; : &quot;The sun.&quot; } ] chat2 = [ { &quot;role&quot; : &quot;user&quot; , &quot;content&quot; : &quot;Which is bigger, a virus or a bacterium?&quot; }, { &quot;role&quot; : &quot;assistant&quot; , &quot;content&quot; : &quot;A bacterium.&quot; } ] dataset = Dataset.from_dict({ &quot;chat&quot; : [chat1, chat2]}) dataset = dataset. map ( lambda x: { &quot;formatted_chat&quot; : tokenizer.apply_chat_template(x[ &quot;chat&quot; ], tokenize= False , add_generation_prompt= False )}) print (dataset[ &#x27;formatted_chat&#x27; ][ 0 ]) Copied &lt;|user|&gt; Which is bigger, the moon or the sun? &lt;/ s &gt; &lt;|assistant|&gt; The sun. &lt;/ s &gt; After this step, you can continue following the training recipe for causal language models using the formatted_chat column. &lt; &gt; Update on GitHub ← Chat basics Multimodal chat templates → Chat templates Using apply_chat_template add_generation_prompt continue_final_message Model training ",
      "fetch_method": "direct-html"
    },
    {
      "arxiv_id": "https://huggingface.co/docs/transformers/main/chat_templating#advanced-retrieval-augmented-generation",
      "full_text": " Chat templates Hugging Face Models Datasets Spaces Community Docs Enterprise Pricing Log In Sign Up Transformers documentation Chat templates Transformers 🏡 View all docs AWS Trainium &amp; Inferentia Accelerate Argilla AutoTrain Bitsandbytes Chat UI Dataset viewer Datasets Deploying on AWS Diffusers Distilabel Evaluate Gradio Hub Hub Python Library Huggingface.js Inference Endpoints (dedicated) Inference Providers Kernels LeRobot Leaderboards Lighteval Microsoft Azure Optimum PEFT Safetensors Sentence Transformers TRL Tasks Text Embeddings Inference Text Generation Inference Tokenizers Trackio Transformers Transformers.js smolagents timm Search documentation main v4.56.2 v4.55.4 v4.53.3 v4.52.3 v4.51.3 v4.50.0 v4.49.0 v4.48.2 v4.47.1 v4.46.3 v4.45.2 v4.44.2 v4.43.4 v4.42.4 v4.41.2 v4.40.2 v4.39.3 v4.38.2 v4.37.2 v4.36.1 v4.35.2 v4.34.1 v4.33.3 v4.32.1 v4.31.0 v4.30.0 v4.29.1 v4.28.1 v4.27.2 v4.26.1 v4.25.1 v4.24.0 v4.23.1 v4.22.2 v4.21.3 v4.20.1 v4.19.4 v4.18.0 v4.17.0 v4.16.2 v4.15.0 v4.14.1 v4.13.0 v4.12.5 v4.11.3 v4.10.1 v4.9.2 v4.8.2 v4.7.0 v4.6.0 v4.5.1 v4.4.2 v4.3.3 v4.2.2 v4.1.1 v4.0.1 v3.5.1 v3.4.0 v3.3.1 v3.2.0 v3.1.0 v3.0.2 v2.11.0 v2.10.0 v2.9.1 v2.8.0 v2.7.0 v2.6.0 v2.5.1 v2.4.1 v2.3.0 v2.2.2 v2.1.1 v2.0.0 v1.2.0 v1.1.0 v1.0.0 doc-builder-html AR DE EN ES FR HI IT JA KO PT TE TR ZH Get started Transformers Installation Quickstart Base classes Inference Pipeline API LLMs Chat with models Chat basics Chat templates Multimodal chat templates Tool use Writing a chat template Serving Optimization Agents Tools Inference server backends Training Quantization Export to production Resources Contribute API You are viewing main version, which requires installation from source . If you&#39;d like regular pip install, checkout the latest stable version ( v4.56.2 ). Join the Hugging Face community and get access to the augmented documentation experience Collaborate on models, datasets and Spaces Faster examples with accelerated inference Switch between documentation themes Sign Up to get started Chat templates The chat basics guide covers how to store chat histories and generate text from chat models using TextGenerationPipeline . This guide is intended for more advanced users, and covers the underlying classes and methods, as well as the key concepts for understanding what’s actually going on when you chat with a model. The critical insight needed to understand chat models is this: All causal LMs, whether chat-trained or not, continue a sequence of tokens. When causal LMs are trained, the training usually begins with “pre-training” on a huge corpus of text, which creates a “base” model. These base models are then often “fine-tuned” for chat, which means training them on data that is formatted as a sequence of messages. The chat is still just a sequence of tokens, though! The list of role and content dictionaries that you pass to a chat model get converted to a token sequence, often with control tokens like &lt;|user|&gt; or &lt;|assistant|&gt; or &lt;|end_of_message|&gt; , which allow the model to see the chat structure. There are many possible chat formats, and different models may use different formats or control tokens, even if they were fine-tuned from the same base model! Don’t panic, though - you don’t need to memorize every possible chat format in order to use chat models. Chat models come with chat templates , which indicate how they expect chats to be formatted. You can access these with the apply_chat_template method. Let’s see two examples. Both of these models are fine-tuned from the same Mistral-7B base model: Mistral Zephyr Copied from transformers import AutoTokenizer tokenizer = AutoTokenizer.from_pretrained( &quot;mistralai/Mistral-7B-Instruct-v0.1&quot; ) chat = [ { &quot;role&quot; : &quot;user&quot; , &quot;content&quot; : &quot;Hello, how are you?&quot; }, { &quot;role&quot; : &quot;assistant&quot; , &quot;content&quot; : &quot;I&#x27;m doing great. How can I help you today?&quot; }, { &quot;role&quot; : &quot;user&quot; , &quot;content&quot; : &quot;I&#x27;d like to show off how chat templating works!&quot; }, ] tokenizer.apply_chat_template(chat, tokenize= False ) Copied &lt; s &gt; [INST] Hello, how are you? [/INST]I&#x27;m doing great. How can I help you today? &lt;/ s &gt; [INST] I&#x27;d like to show off how chat templating works! [/INST] Mistral-7B-Instruct uses [INST] and [/INST] tokens to indicate the start and end of user messages, while Zephyr-7B uses &lt;|user|&gt; and &lt;|assistant|&gt; tokens to indicate speaker roles. This is why chat templates are important - with the wrong control tokens, these models would have drastically worse performance. Using apply_chat_template The input to apply_chat_template should be structured as a list of dictionaries with role and content keys. The role key specifies the speaker, and the content key contains the message. The common roles are: user for messages from the user assistant for messages from the model system for directives on how the model should act (usually placed at the beginning of the chat) apply_chat_template takes this list and returns a formatted sequence. Set tokenize=True if you want to tokenize the sequence. Copied import torch from transformers import AutoModelForCausalLM, AutoTokenizer tokenizer = AutoTokenizer.from_pretrained( &quot;HuggingFaceH4/zephyr-7b-beta&quot; ) model = AutoModelForCausalLM.from_pretrained( &quot;HuggingFaceH4/zephyr-7b-beta&quot; , device_map= &quot;auto&quot; , dtype=torch.bfloat16) messages = [ { &quot;role&quot; : &quot;system&quot; , &quot;content&quot; : &quot;You are a friendly chatbot who always responds in the style of a pirate&quot; ,}, { &quot;role&quot; : &quot;user&quot; , &quot;content&quot; : &quot;How many helicopters can a human eat in one sitting?&quot; }, ] tokenized_chat = tokenizer.apply_chat_template(messages, tokenize= True , add_generation_prompt= True , return_tensors= &quot;pt&quot; ) print (tokenizer.decode(tokenized_chat[ 0 ])) Copied &lt;|system|&gt; You are a friendly chatbot who always responds in the style of a pirate &lt;/ s &gt; &lt;|user|&gt; How many helicopters can a human eat in one sitting? &lt;/ s &gt; &lt;|assistant|&gt; Pass the tokenized chat to generate() to generate a response. Copied outputs = model.generate(tokenized_chat, max_new_tokens= 128 ) print (tokenizer.decode(outputs[ 0 ])) Copied &lt;|system|&gt; You are a friendly chatbot who always responds in the style of a pirate &lt;/ s &gt; &lt;|user|&gt; How many helicopters can a human eat in one sitting? &lt;/ s &gt; &lt;|assistant|&gt; Matey, I&#x27;m afraid I must inform ye that humans cannot eat helicopters. Helicopters are not food, they are flying machines. Food is meant to be eaten, like a hearty plate o&#x27; grog, a savory bowl o&#x27; stew, or a delicious loaf o&#x27; bread. But helicopters, they be for transportin&#x27; and movin&#x27; around, not for eatin&#x27;. So, I&#x27;d say none, me hearties. None at all. Some tokenizers add special &lt;bos&gt; and &lt;eos&gt; tokens. Chat templates should already include all the necessary special tokens, and adding additional special tokens is often incorrect or duplicated, hurting model performance. When you format text with apply_chat_template(tokenize=False) , make sure you set add_special_tokens=False if you tokenize later to avoid duplicating these tokens. This isn’t an issue if you use apply_chat_template(tokenize=True) , which means it’s usually the safer option! add_generation_prompt You may have noticed the add_generation_prompt argument in the above examples. This argument adds tokens to the end of the chat that indicate the start of an assistant response. Remember: Beneath all the chat abstractions, chat models are still just language models that continue a sequence of tokens! If you include tokens that tell it that it’s now in an assistant response, it will correctly write a response, but if you don’t include these tokens, the model may get confused and do something strange, like continuing the user’s message instead of replying to it! Let’s see an example to understand what add_generation_prompt is actually doing. First, let’s format a chat without add_generation_prompt : Copied tokenized_chat = tokenizer.apply_chat_template(messages, tokenize= False , add_generation_prompt= False ) tokenized_chat Copied &lt;|im _start|&gt;user Hi there!&lt;|im_ end|&gt; &lt;|im _start|&gt;assistant Nice to meet you!&lt;|im_ end|&gt; &lt;|im _start|&gt;user Can I ask a question?&lt;|im_ end|&gt; Now, let’s format the same chat with add_generation_prompt=True : Copied tokenized_chat = tokenizer.apply_chat_template(messages, tokenize= False , add_generation_prompt= True ) tokenized_chat Copied &lt;|im _start|&gt;user Hi there!&lt;|im_ end|&gt; &lt;|im _start|&gt;assistant Nice to meet you!&lt;|im_ end|&gt; &lt;|im _start|&gt;user Can I ask a question?&lt;|im_ end|&gt; &lt;|im _start|&gt;assistant When add_generation_prompt=True , &lt;|im_start|&gt;assistant is added at the end to indicate the start of an assistant message. This lets the model know an assistant response is next. Not all models require generation prompts, and some models, like Llama , don’t have any special tokens before the assistant response. In these cases, add_generation_prompt has no effect. continue_final_message The continue_final_message parameter controls whether the final message in the chat should be continued or not instead of starting a new one. It removes end of sequence tokens so that the model continues generation from the final message. This is useful for “prefilling” a model response. In the example below, the model generates text that continues the JSON string rather than starting a new message. It can be very useful for improving the accuracy of instruction following when you know how to start its replies. Copied chat = [ { &quot;role&quot; : &quot;user&quot; , &quot;content&quot; : &quot;Can you format the answer in JSON?&quot; }, { &quot;role&quot; : &quot;assistant&quot; , &quot;content&quot; : &#x27;{&quot;name&quot;: &quot;&#x27; }, ] formatted_chat = tokenizer.apply_chat_template(chat, tokenize= True , return_dict= True , continue_final_message= True ) model.generate(**formatted_chat) You shouldn’t use add_generation_prompt and continue_final_message together. The former adds tokens that start a new message, while the latter removes end of sequence tokens. Using them together returns an error. TextGenerationPipeline sets add_generation_prompt to True by default to start a new message. However, if the final message in the chat has the assistant role, it assumes the message is a prefill and switches to continue_final_message=True . This is because most models don’t support multiple consecutive assistant messages. To override this behavior, explicitly pass the continue_final_message argument to the pipeline. Model training Training a model with a chat template is a good way to ensure the template matches the tokens the model was trained on. Apply the chat template as a preprocessing step to your dataset. Set add_generation_prompt=False because the additional tokens to prompt an assistant response aren’t helpful during training. An example of preprocessing a dataset with a chat template is shown below. Copied from transformers import AutoTokenizer from datasets import Dataset tokenizer = AutoTokenizer.from_pretrained( &quot;HuggingFaceH4/zephyr-7b-beta&quot; ) chat1 = [ { &quot;role&quot; : &quot;user&quot; , &quot;content&quot; : &quot;Which is bigger, the moon or the sun?&quot; }, { &quot;role&quot; : &quot;assistant&quot; , &quot;content&quot; : &quot;The sun.&quot; } ] chat2 = [ { &quot;role&quot; : &quot;user&quot; , &quot;content&quot; : &quot;Which is bigger, a virus or a bacterium?&quot; }, { &quot;role&quot; : &quot;assistant&quot; , &quot;content&quot; : &quot;A bacterium.&quot; } ] dataset = Dataset.from_dict({ &quot;chat&quot; : [chat1, chat2]}) dataset = dataset. map ( lambda x: { &quot;formatted_chat&quot; : tokenizer.apply_chat_template(x[ &quot;chat&quot; ], tokenize= False , add_generation_prompt= False )}) print (dataset[ &#x27;formatted_chat&#x27; ][ 0 ]) Copied &lt;|user|&gt; Which is bigger, the moon or the sun? &lt;/ s &gt; &lt;|assistant|&gt; The sun. &lt;/ s &gt; After this step, you can continue following the training recipe for causal language models using the formatted_chat column. &lt; &gt; Update on GitHub ← Chat basics Multimodal chat templates → Chat templates Using apply_chat_template add_generation_prompt continue_final_message Model training ",
      "fetch_method": "direct-html"
    },
    {
      "arxiv_id": "https://huggingface.co/docs/transformers/main/en/chat_templating#advanced-tool-use--function-calling",
      "full_text": " Chat templates Hugging Face Models Datasets Spaces Community Docs Enterprise Pricing Log In Sign Up Transformers documentation Chat templates Transformers 🏡 View all docs AWS Trainium &amp; Inferentia Accelerate Argilla AutoTrain Bitsandbytes Chat UI Dataset viewer Datasets Deploying on AWS Diffusers Distilabel Evaluate Gradio Hub Hub Python Library Huggingface.js Inference Endpoints (dedicated) Inference Providers Kernels LeRobot Leaderboards Lighteval Microsoft Azure Optimum PEFT Safetensors Sentence Transformers TRL Tasks Text Embeddings Inference Text Generation Inference Tokenizers Trackio Transformers Transformers.js smolagents timm Search documentation main v4.56.2 v4.55.4 v4.53.3 v4.52.3 v4.51.3 v4.50.0 v4.49.0 v4.48.2 v4.47.1 v4.46.3 v4.45.2 v4.44.2 v4.43.4 v4.42.4 v4.41.2 v4.40.2 v4.39.3 v4.38.2 v4.37.2 v4.36.1 v4.35.2 v4.34.1 v4.33.3 v4.32.1 v4.31.0 v4.30.0 v4.29.1 v4.28.1 v4.27.2 v4.26.1 v4.25.1 v4.24.0 v4.23.1 v4.22.2 v4.21.3 v4.20.1 v4.19.4 v4.18.0 v4.17.0 v4.16.2 v4.15.0 v4.14.1 v4.13.0 v4.12.5 v4.11.3 v4.10.1 v4.9.2 v4.8.2 v4.7.0 v4.6.0 v4.5.1 v4.4.2 v4.3.3 v4.2.2 v4.1.1 v4.0.1 v3.5.1 v3.4.0 v3.3.1 v3.2.0 v3.1.0 v3.0.2 v2.11.0 v2.10.0 v2.9.1 v2.8.0 v2.7.0 v2.6.0 v2.5.1 v2.4.1 v2.3.0 v2.2.2 v2.1.1 v2.0.0 v1.2.0 v1.1.0 v1.0.0 doc-builder-html AR DE EN ES FR HI IT JA KO PT TE TR ZH Get started Transformers Installation Quickstart Base classes Inference Pipeline API LLMs Chat with models Chat basics Chat templates Multimodal chat templates Tool use Writing a chat template Serving Optimization Agents Tools Inference server backends Training Quantization Export to production Resources Contribute API You are viewing main version, which requires installation from source . If you&#39;d like regular pip install, checkout the latest stable version ( v4.56.2 ). Join the Hugging Face community and get access to the augmented documentation experience Collaborate on models, datasets and Spaces Faster examples with accelerated inference Switch between documentation themes Sign Up to get started Chat templates The chat basics guide covers how to store chat histories and generate text from chat models using TextGenerationPipeline . This guide is intended for more advanced users, and covers the underlying classes and methods, as well as the key concepts for understanding what’s actually going on when you chat with a model. The critical insight needed to understand chat models is this: All causal LMs, whether chat-trained or not, continue a sequence of tokens. When causal LMs are trained, the training usually begins with “pre-training” on a huge corpus of text, which creates a “base” model. These base models are then often “fine-tuned” for chat, which means training them on data that is formatted as a sequence of messages. The chat is still just a sequence of tokens, though! The list of role and content dictionaries that you pass to a chat model get converted to a token sequence, often with control tokens like &lt;|user|&gt; or &lt;|assistant|&gt; or &lt;|end_of_message|&gt; , which allow the model to see the chat structure. There are many possible chat formats, and different models may use different formats or control tokens, even if they were fine-tuned from the same base model! Don’t panic, though - you don’t need to memorize every possible chat format in order to use chat models. Chat models come with chat templates , which indicate how they expect chats to be formatted. You can access these with the apply_chat_template method. Let’s see two examples. Both of these models are fine-tuned from the same Mistral-7B base model: Mistral Zephyr Copied from transformers import AutoTokenizer tokenizer = AutoTokenizer.from_pretrained( &quot;mistralai/Mistral-7B-Instruct-v0.1&quot; ) chat = [ { &quot;role&quot; : &quot;user&quot; , &quot;content&quot; : &quot;Hello, how are you?&quot; }, { &quot;role&quot; : &quot;assistant&quot; , &quot;content&quot; : &quot;I&#x27;m doing great. How can I help you today?&quot; }, { &quot;role&quot; : &quot;user&quot; , &quot;content&quot; : &quot;I&#x27;d like to show off how chat templating works!&quot; }, ] tokenizer.apply_chat_template(chat, tokenize= False ) Copied &lt; s &gt; [INST] Hello, how are you? [/INST]I&#x27;m doing great. How can I help you today? &lt;/ s &gt; [INST] I&#x27;d like to show off how chat templating works! [/INST] Mistral-7B-Instruct uses [INST] and [/INST] tokens to indicate the start and end of user messages, while Zephyr-7B uses &lt;|user|&gt; and &lt;|assistant|&gt; tokens to indicate speaker roles. This is why chat templates are important - with the wrong control tokens, these models would have drastically worse performance. Using apply_chat_template The input to apply_chat_template should be structured as a list of dictionaries with role and content keys. The role key specifies the speaker, and the content key contains the message. The common roles are: user for messages from the user assistant for messages from the model system for directives on how the model should act (usually placed at the beginning of the chat) apply_chat_template takes this list and returns a formatted sequence. Set tokenize=True if you want to tokenize the sequence. Copied import torch from transformers import AutoModelForCausalLM, AutoTokenizer tokenizer = AutoTokenizer.from_pretrained( &quot;HuggingFaceH4/zephyr-7b-beta&quot; ) model = AutoModelForCausalLM.from_pretrained( &quot;HuggingFaceH4/zephyr-7b-beta&quot; , device_map= &quot;auto&quot; , dtype=torch.bfloat16) messages = [ { &quot;role&quot; : &quot;system&quot; , &quot;content&quot; : &quot;You are a friendly chatbot who always responds in the style of a pirate&quot; ,}, { &quot;role&quot; : &quot;user&quot; , &quot;content&quot; : &quot;How many helicopters can a human eat in one sitting?&quot; }, ] tokenized_chat = tokenizer.apply_chat_template(messages, tokenize= True , add_generation_prompt= True , return_tensors= &quot;pt&quot; ) print (tokenizer.decode(tokenized_chat[ 0 ])) Copied &lt;|system|&gt; You are a friendly chatbot who always responds in the style of a pirate &lt;/ s &gt; &lt;|user|&gt; How many helicopters can a human eat in one sitting? &lt;/ s &gt; &lt;|assistant|&gt; Pass the tokenized chat to generate() to generate a response. Copied outputs = model.generate(tokenized_chat, max_new_tokens= 128 ) print (tokenizer.decode(outputs[ 0 ])) Copied &lt;|system|&gt; You are a friendly chatbot who always responds in the style of a pirate &lt;/ s &gt; &lt;|user|&gt; How many helicopters can a human eat in one sitting? &lt;/ s &gt; &lt;|assistant|&gt; Matey, I&#x27;m afraid I must inform ye that humans cannot eat helicopters. Helicopters are not food, they are flying machines. Food is meant to be eaten, like a hearty plate o&#x27; grog, a savory bowl o&#x27; stew, or a delicious loaf o&#x27; bread. But helicopters, they be for transportin&#x27; and movin&#x27; around, not for eatin&#x27;. So, I&#x27;d say none, me hearties. None at all. Some tokenizers add special &lt;bos&gt; and &lt;eos&gt; tokens. Chat templates should already include all the necessary special tokens, and adding additional special tokens is often incorrect or duplicated, hurting model performance. When you format text with apply_chat_template(tokenize=False) , make sure you set add_special_tokens=False if you tokenize later to avoid duplicating these tokens. This isn’t an issue if you use apply_chat_template(tokenize=True) , which means it’s usually the safer option! add_generation_prompt You may have noticed the add_generation_prompt argument in the above examples. This argument adds tokens to the end of the chat that indicate the start of an assistant response. Remember: Beneath all the chat abstractions, chat models are still just language models that continue a sequence of tokens! If you include tokens that tell it that it’s now in an assistant response, it will correctly write a response, but if you don’t include these tokens, the model may get confused and do something strange, like continuing the user’s message instead of replying to it! Let’s see an example to understand what add_generation_prompt is actually doing. First, let’s format a chat without add_generation_prompt : Copied tokenized_chat = tokenizer.apply_chat_template(messages, tokenize= False , add_generation_prompt= False ) tokenized_chat Copied &lt;|im _start|&gt;user Hi there!&lt;|im_ end|&gt; &lt;|im _start|&gt;assistant Nice to meet you!&lt;|im_ end|&gt; &lt;|im _start|&gt;user Can I ask a question?&lt;|im_ end|&gt; Now, let’s format the same chat with add_generation_prompt=True : Copied tokenized_chat = tokenizer.apply_chat_template(messages, tokenize= False , add_generation_prompt= True ) tokenized_chat Copied &lt;|im _start|&gt;user Hi there!&lt;|im_ end|&gt; &lt;|im _start|&gt;assistant Nice to meet you!&lt;|im_ end|&gt; &lt;|im _start|&gt;user Can I ask a question?&lt;|im_ end|&gt; &lt;|im _start|&gt;assistant When add_generation_prompt=True , &lt;|im_start|&gt;assistant is added at the end to indicate the start of an assistant message. This lets the model know an assistant response is next. Not all models require generation prompts, and some models, like Llama , don’t have any special tokens before the assistant response. In these cases, add_generation_prompt has no effect. continue_final_message The continue_final_message parameter controls whether the final message in the chat should be continued or not instead of starting a new one. It removes end of sequence tokens so that the model continues generation from the final message. This is useful for “prefilling” a model response. In the example below, the model generates text that continues the JSON string rather than starting a new message. It can be very useful for improving the accuracy of instruction following when you know how to start its replies. Copied chat = [ { &quot;role&quot; : &quot;user&quot; , &quot;content&quot; : &quot;Can you format the answer in JSON?&quot; }, { &quot;role&quot; : &quot;assistant&quot; , &quot;content&quot; : &#x27;{&quot;name&quot;: &quot;&#x27; }, ] formatted_chat = tokenizer.apply_chat_template(chat, tokenize= True , return_dict= True , continue_final_message= True ) model.generate(**formatted_chat) You shouldn’t use add_generation_prompt and continue_final_message together. The former adds tokens that start a new message, while the latter removes end of sequence tokens. Using them together returns an error. TextGenerationPipeline sets add_generation_prompt to True by default to start a new message. However, if the final message in the chat has the assistant role, it assumes the message is a prefill and switches to continue_final_message=True . This is because most models don’t support multiple consecutive assistant messages. To override this behavior, explicitly pass the continue_final_message argument to the pipeline. Model training Training a model with a chat template is a good way to ensure the template matches the tokens the model was trained on. Apply the chat template as a preprocessing step to your dataset. Set add_generation_prompt=False because the additional tokens to prompt an assistant response aren’t helpful during training. An example of preprocessing a dataset with a chat template is shown below. Copied from transformers import AutoTokenizer from datasets import Dataset tokenizer = AutoTokenizer.from_pretrained( &quot;HuggingFaceH4/zephyr-7b-beta&quot; ) chat1 = [ { &quot;role&quot; : &quot;user&quot; , &quot;content&quot; : &quot;Which is bigger, the moon or the sun?&quot; }, { &quot;role&quot; : &quot;assistant&quot; , &quot;content&quot; : &quot;The sun.&quot; } ] chat2 = [ { &quot;role&quot; : &quot;user&quot; , &quot;content&quot; : &quot;Which is bigger, a virus or a bacterium?&quot; }, { &quot;role&quot; : &quot;assistant&quot; , &quot;content&quot; : &quot;A bacterium.&quot; } ] dataset = Dataset.from_dict({ &quot;chat&quot; : [chat1, chat2]}) dataset = dataset. map ( lambda x: { &quot;formatted_chat&quot; : tokenizer.apply_chat_template(x[ &quot;chat&quot; ], tokenize= False , add_generation_prompt= False )}) print (dataset[ &#x27;formatted_chat&#x27; ][ 0 ]) Copied &lt;|user|&gt; Which is bigger, the moon or the sun? &lt;/ s &gt; &lt;|assistant|&gt; The sun. &lt;/ s &gt; After this step, you can continue following the training recipe for causal language models using the formatted_chat column. &lt; &gt; Update on GitHub ← Chat basics Multimodal chat templates → Chat templates Using apply_chat_template add_generation_prompt continue_final_message Model training ",
      "fetch_method": "direct-html"
    },
    {
      "arxiv_id": "https://r.jina.ai/https://docs.cohere.com/docs/command-a-hf&sa=D&source=docs&ust=1741857329583678&usg=AOvVaw3sS-2eIfLzShS6c9VWXJWa",
      "full_text": "Title: \n\nURL Source: https://docs.cohere.com/docs/command-a-hf&sa=D&source=docs&ust=1741857329583678&usg=AOvVaw3sS-2eIfLzShS6c9VWXJWa\n\nWarning: Target URL returned error 404: Not Found\n\nMarkdown Content:\nWe use cookies to enhance your browsing experience, serve personalized content, and analyze our traffic. By clicking \"Accept All\", you consent to our use of cookies.\n\nManage settings Reject All Accept All\n\nCustomize Consent Preferences![Image 1: Close](https://cdn-cookieyes.com/assets/images/close.svg)\n\nWe use cookies to help you navigate efficiently and perform certain functions. You will find detailed information about all cookies under each consent category below.\n\nThe cookies that are categorized as \"Necessary\" are stored on your browser as they are essential for enabling the basic functionalities of the site. ...Show more\n\nNecessary Always Active\n\nNecessary cookies are required to enable the basic features of this site, such as providing secure log-in or adjusting your consent preferences. These cookies do not store any personally identifiable data.\n\nFunctional\n\n- [x] \n\nFunctional cookies help perform certain functionalities like sharing the content of the website on social media platforms, collecting feedback, and other third-party features.\n\nAnalytics\n\n- [x] \n\nAnalytical cookies are used to understand how visitors interact with the website. These cookies help provide information on metrics such as the number of visitors, bounce rate, traffic source, etc.\n\nPerformance\n\n- [x] \n\nPerformance cookies are used to understand and analyze the key performance indexes of the website which helps in delivering a better user experience for the visitors.\n\nAdvertisement\n\n- [x] \n\nAdvertisement cookies are used to provide visitors with customized advertisements based on the pages you visited previously and to analyze the effectiveness of the ad campaigns.\n\nOthers\n\n- [x] \n\nOther uncategorized cookies are those that are being analyzed and have not been classified into a category as yet.\n\nReject All Save My Preferences Accept All\n\n[![Image 2: Logo](https://files.buildwithfern.com/cohere.docs.buildwithfern.com/2025-09-19T21:38:35.992Z/assets/logo.svg)![Image 3: Logo](https://files.buildwithfern.com/cohere.docs.buildwithfern.com/2025-09-19T21:38:35.992Z/assets/logo-dark.svg)Docs](https://docs.cohere.com/)\n\n[DASHBOARD](https://dashboard.cohere.com/)[PLAYGROUND](https://dashboard.cohere.com/playground/generate)[DOCS](https://docs.cohere.com/)[COMMUNITY](https://discord.com/invite/co-mmunity)[LOG IN](https://dashboard.cohere.com/login)\n\nSearch\n\n/\n\nAsk AI\n\n[DASHBOARD](https://dashboard.cohere.com/)[PLAYGROUND](https://dashboard.cohere.com/playground/generate)[DOCS](https://docs.cohere.com/)[COMMUNITY](https://discord.com/invite/co-mmunity)[LOG IN](https://dashboard.cohere.com/login)\n\nSystem\n\nPage not found!\n===============\n\nWe're sorry, we couldn't find the page you were looking for.\n\n[Return home](https://docs.cohere.com/)\n\nAsk AI \n\nAssistant\n\nHi, I'm an AI assistant with access to documentation and other content.\n\nTip: you can toggle this pane with\n\n⌘\n+\n\n/\n\nSuggestions\n\nWhat are the recommended replacement models for the deprecated Command models?\n\nHow do I migrate from the deprecated /v1/summarize endpoint to the new alternatives?\n\nWhat are the key features and capabilities of Command A Vision for processing images and documents?\n\nHow can I use the new Compatibility API to switch from OpenAI SDK to Cohere's models?\n\nWhat languages are supported by Command A Translate and what are its technical specifications?\n\n![Image 4](https://t.co/1/i/adsct?bci=4&dv=UTC%26en-US%26Google%20Inc.%26Linux%20x86_64%26255%26800%26600%264%2624%26800%26600%260%26na&eci=3&event=%7B%7D&event_id=47bf78ab-0b7e-484d-8989-3a755fb80c3e&integration=gtm&p_id=Twitter&p_user_id=0&pl_id=eab6350c-f3a6-498c-bb9f-86317296e671&tw_document_href=https%3A%2F%2Fdocs.cohere.com%2Fdocs%2Fcommand-a-hf%26sa%3DD%26source%3Ddocs%26ust%3D1741857329583678%26usg%3DAOvVaw3sS-2eIfLzShS6c9VWXJWa&tw_iframe_status=0&txn_id=o8y8u&type=javascript&version=2.3.34)![Image 5](https://analytics.twitter.com/1/i/adsct?bci=4&dv=UTC%26en-US%26Google%20Inc.%26Linux%20x86_64%26255%26800%26600%264%2624%26800%26600%260%26na&eci=3&event=%7B%7D&event_id=47bf78ab-0b7e-484d-8989-3a755fb80c3e&integration=gtm&p_id=Twitter&p_user_id=0&pl_id=eab6350c-f3a6-498c-bb9f-86317296e671&tw_document_href=https%3A%2F%2Fdocs.cohere.com%2Fdocs%2Fcommand-a-hf%26sa%3DD%26source%3Ddocs%26ust%3D1741857329583678%26usg%3DAOvVaw3sS-2eIfLzShS6c9VWXJWa&tw_iframe_status=0&txn_id=o8y8u&type=javascript&version=2.3.34)![Image 6](https://id.rlcdn.com/464526.gif)\n",
      "fetch_method": "jina-reader"
    },
    {
      "arxiv_id": "https://huggingface.co/docs/transformers/main/chat_templating#advanced-tool-use--function-calling",
      "full_text": " Chat templates Hugging Face Models Datasets Spaces Community Docs Enterprise Pricing Log In Sign Up Transformers documentation Chat templates Transformers 🏡 View all docs AWS Trainium &amp; Inferentia Accelerate Argilla AutoTrain Bitsandbytes Chat UI Dataset viewer Datasets Deploying on AWS Diffusers Distilabel Evaluate Gradio Hub Hub Python Library Huggingface.js Inference Endpoints (dedicated) Inference Providers Kernels LeRobot Leaderboards Lighteval Microsoft Azure Optimum PEFT Safetensors Sentence Transformers TRL Tasks Text Embeddings Inference Text Generation Inference Tokenizers Trackio Transformers Transformers.js smolagents timm Search documentation main v4.56.2 v4.55.4 v4.53.3 v4.52.3 v4.51.3 v4.50.0 v4.49.0 v4.48.2 v4.47.1 v4.46.3 v4.45.2 v4.44.2 v4.43.4 v4.42.4 v4.41.2 v4.40.2 v4.39.3 v4.38.2 v4.37.2 v4.36.1 v4.35.2 v4.34.1 v4.33.3 v4.32.1 v4.31.0 v4.30.0 v4.29.1 v4.28.1 v4.27.2 v4.26.1 v4.25.1 v4.24.0 v4.23.1 v4.22.2 v4.21.3 v4.20.1 v4.19.4 v4.18.0 v4.17.0 v4.16.2 v4.15.0 v4.14.1 v4.13.0 v4.12.5 v4.11.3 v4.10.1 v4.9.2 v4.8.2 v4.7.0 v4.6.0 v4.5.1 v4.4.2 v4.3.3 v4.2.2 v4.1.1 v4.0.1 v3.5.1 v3.4.0 v3.3.1 v3.2.0 v3.1.0 v3.0.2 v2.11.0 v2.10.0 v2.9.1 v2.8.0 v2.7.0 v2.6.0 v2.5.1 v2.4.1 v2.3.0 v2.2.2 v2.1.1 v2.0.0 v1.2.0 v1.1.0 v1.0.0 doc-builder-html AR DE EN ES FR HI IT JA KO PT TE TR ZH Get started Transformers Installation Quickstart Base classes Inference Pipeline API LLMs Chat with models Chat basics Chat templates Multimodal chat templates Tool use Writing a chat template Serving Optimization Agents Tools Inference server backends Training Quantization Export to production Resources Contribute API You are viewing main version, which requires installation from source . If you&#39;d like regular pip install, checkout the latest stable version ( v4.56.2 ). Join the Hugging Face community and get access to the augmented documentation experience Collaborate on models, datasets and Spaces Faster examples with accelerated inference Switch between documentation themes Sign Up to get started Chat templates The chat basics guide covers how to store chat histories and generate text from chat models using TextGenerationPipeline . This guide is intended for more advanced users, and covers the underlying classes and methods, as well as the key concepts for understanding what’s actually going on when you chat with a model. The critical insight needed to understand chat models is this: All causal LMs, whether chat-trained or not, continue a sequence of tokens. When causal LMs are trained, the training usually begins with “pre-training” on a huge corpus of text, which creates a “base” model. These base models are then often “fine-tuned” for chat, which means training them on data that is formatted as a sequence of messages. The chat is still just a sequence of tokens, though! The list of role and content dictionaries that you pass to a chat model get converted to a token sequence, often with control tokens like &lt;|user|&gt; or &lt;|assistant|&gt; or &lt;|end_of_message|&gt; , which allow the model to see the chat structure. There are many possible chat formats, and different models may use different formats or control tokens, even if they were fine-tuned from the same base model! Don’t panic, though - you don’t need to memorize every possible chat format in order to use chat models. Chat models come with chat templates , which indicate how they expect chats to be formatted. You can access these with the apply_chat_template method. Let’s see two examples. Both of these models are fine-tuned from the same Mistral-7B base model: Mistral Zephyr Copied from transformers import AutoTokenizer tokenizer = AutoTokenizer.from_pretrained( &quot;mistralai/Mistral-7B-Instruct-v0.1&quot; ) chat = [ { &quot;role&quot; : &quot;user&quot; , &quot;content&quot; : &quot;Hello, how are you?&quot; }, { &quot;role&quot; : &quot;assistant&quot; , &quot;content&quot; : &quot;I&#x27;m doing great. How can I help you today?&quot; }, { &quot;role&quot; : &quot;user&quot; , &quot;content&quot; : &quot;I&#x27;d like to show off how chat templating works!&quot; }, ] tokenizer.apply_chat_template(chat, tokenize= False ) Copied &lt; s &gt; [INST] Hello, how are you? [/INST]I&#x27;m doing great. How can I help you today? &lt;/ s &gt; [INST] I&#x27;d like to show off how chat templating works! [/INST] Mistral-7B-Instruct uses [INST] and [/INST] tokens to indicate the start and end of user messages, while Zephyr-7B uses &lt;|user|&gt; and &lt;|assistant|&gt; tokens to indicate speaker roles. This is why chat templates are important - with the wrong control tokens, these models would have drastically worse performance. Using apply_chat_template The input to apply_chat_template should be structured as a list of dictionaries with role and content keys. The role key specifies the speaker, and the content key contains the message. The common roles are: user for messages from the user assistant for messages from the model system for directives on how the model should act (usually placed at the beginning of the chat) apply_chat_template takes this list and returns a formatted sequence. Set tokenize=True if you want to tokenize the sequence. Copied import torch from transformers import AutoModelForCausalLM, AutoTokenizer tokenizer = AutoTokenizer.from_pretrained( &quot;HuggingFaceH4/zephyr-7b-beta&quot; ) model = AutoModelForCausalLM.from_pretrained( &quot;HuggingFaceH4/zephyr-7b-beta&quot; , device_map= &quot;auto&quot; , dtype=torch.bfloat16) messages = [ { &quot;role&quot; : &quot;system&quot; , &quot;content&quot; : &quot;You are a friendly chatbot who always responds in the style of a pirate&quot; ,}, { &quot;role&quot; : &quot;user&quot; , &quot;content&quot; : &quot;How many helicopters can a human eat in one sitting?&quot; }, ] tokenized_chat = tokenizer.apply_chat_template(messages, tokenize= True , add_generation_prompt= True , return_tensors= &quot;pt&quot; ) print (tokenizer.decode(tokenized_chat[ 0 ])) Copied &lt;|system|&gt; You are a friendly chatbot who always responds in the style of a pirate &lt;/ s &gt; &lt;|user|&gt; How many helicopters can a human eat in one sitting? &lt;/ s &gt; &lt;|assistant|&gt; Pass the tokenized chat to generate() to generate a response. Copied outputs = model.generate(tokenized_chat, max_new_tokens= 128 ) print (tokenizer.decode(outputs[ 0 ])) Copied &lt;|system|&gt; You are a friendly chatbot who always responds in the style of a pirate &lt;/ s &gt; &lt;|user|&gt; How many helicopters can a human eat in one sitting? &lt;/ s &gt; &lt;|assistant|&gt; Matey, I&#x27;m afraid I must inform ye that humans cannot eat helicopters. Helicopters are not food, they are flying machines. Food is meant to be eaten, like a hearty plate o&#x27; grog, a savory bowl o&#x27; stew, or a delicious loaf o&#x27; bread. But helicopters, they be for transportin&#x27; and movin&#x27; around, not for eatin&#x27;. So, I&#x27;d say none, me hearties. None at all. Some tokenizers add special &lt;bos&gt; and &lt;eos&gt; tokens. Chat templates should already include all the necessary special tokens, and adding additional special tokens is often incorrect or duplicated, hurting model performance. When you format text with apply_chat_template(tokenize=False) , make sure you set add_special_tokens=False if you tokenize later to avoid duplicating these tokens. This isn’t an issue if you use apply_chat_template(tokenize=True) , which means it’s usually the safer option! add_generation_prompt You may have noticed the add_generation_prompt argument in the above examples. This argument adds tokens to the end of the chat that indicate the start of an assistant response. Remember: Beneath all the chat abstractions, chat models are still just language models that continue a sequence of tokens! If you include tokens that tell it that it’s now in an assistant response, it will correctly write a response, but if you don’t include these tokens, the model may get confused and do something strange, like continuing the user’s message instead of replying to it! Let’s see an example to understand what add_generation_prompt is actually doing. First, let’s format a chat without add_generation_prompt : Copied tokenized_chat = tokenizer.apply_chat_template(messages, tokenize= False , add_generation_prompt= False ) tokenized_chat Copied &lt;|im _start|&gt;user Hi there!&lt;|im_ end|&gt; &lt;|im _start|&gt;assistant Nice to meet you!&lt;|im_ end|&gt; &lt;|im _start|&gt;user Can I ask a question?&lt;|im_ end|&gt; Now, let’s format the same chat with add_generation_prompt=True : Copied tokenized_chat = tokenizer.apply_chat_template(messages, tokenize= False , add_generation_prompt= True ) tokenized_chat Copied &lt;|im _start|&gt;user Hi there!&lt;|im_ end|&gt; &lt;|im _start|&gt;assistant Nice to meet you!&lt;|im_ end|&gt; &lt;|im _start|&gt;user Can I ask a question?&lt;|im_ end|&gt; &lt;|im _start|&gt;assistant When add_generation_prompt=True , &lt;|im_start|&gt;assistant is added at the end to indicate the start of an assistant message. This lets the model know an assistant response is next. Not all models require generation prompts, and some models, like Llama , don’t have any special tokens before the assistant response. In these cases, add_generation_prompt has no effect. continue_final_message The continue_final_message parameter controls whether the final message in the chat should be continued or not instead of starting a new one. It removes end of sequence tokens so that the model continues generation from the final message. This is useful for “prefilling” a model response. In the example below, the model generates text that continues the JSON string rather than starting a new message. It can be very useful for improving the accuracy of instruction following when you know how to start its replies. Copied chat = [ { &quot;role&quot; : &quot;user&quot; , &quot;content&quot; : &quot;Can you format the answer in JSON?&quot; }, { &quot;role&quot; : &quot;assistant&quot; , &quot;content&quot; : &#x27;{&quot;name&quot;: &quot;&#x27; }, ] formatted_chat = tokenizer.apply_chat_template(chat, tokenize= True , return_dict= True , continue_final_message= True ) model.generate(**formatted_chat) You shouldn’t use add_generation_prompt and continue_final_message together. The former adds tokens that start a new message, while the latter removes end of sequence tokens. Using them together returns an error. TextGenerationPipeline sets add_generation_prompt to True by default to start a new message. However, if the final message in the chat has the assistant role, it assumes the message is a prefill and switches to continue_final_message=True . This is because most models don’t support multiple consecutive assistant messages. To override this behavior, explicitly pass the continue_final_message argument to the pipeline. Model training Training a model with a chat template is a good way to ensure the template matches the tokens the model was trained on. Apply the chat template as a preprocessing step to your dataset. Set add_generation_prompt=False because the additional tokens to prompt an assistant response aren’t helpful during training. An example of preprocessing a dataset with a chat template is shown below. Copied from transformers import AutoTokenizer from datasets import Dataset tokenizer = AutoTokenizer.from_pretrained( &quot;HuggingFaceH4/zephyr-7b-beta&quot; ) chat1 = [ { &quot;role&quot; : &quot;user&quot; , &quot;content&quot; : &quot;Which is bigger, the moon or the sun?&quot; }, { &quot;role&quot; : &quot;assistant&quot; , &quot;content&quot; : &quot;The sun.&quot; } ] chat2 = [ { &quot;role&quot; : &quot;user&quot; , &quot;content&quot; : &quot;Which is bigger, a virus or a bacterium?&quot; }, { &quot;role&quot; : &quot;assistant&quot; , &quot;content&quot; : &quot;A bacterium.&quot; } ] dataset = Dataset.from_dict({ &quot;chat&quot; : [chat1, chat2]}) dataset = dataset. map ( lambda x: { &quot;formatted_chat&quot; : tokenizer.apply_chat_template(x[ &quot;chat&quot; ], tokenize= False , add_generation_prompt= False )}) print (dataset[ &#x27;formatted_chat&#x27; ][ 0 ]) Copied &lt;|user|&gt; Which is bigger, the moon or the sun? &lt;/ s &gt; &lt;|assistant|&gt; The sun. &lt;/ s &gt; After this step, you can continue following the training recipe for causal language models using the formatted_chat column. &lt; &gt; Update on GitHub ← Chat basics Multimodal chat templates → Chat templates Using apply_chat_template add_generation_prompt continue_final_message Model training ",
      "fetch_method": "direct-html"
    },
    {
      "arxiv_id": "https://r.jina.ai/https://arxiv.org/abs/2504.00698},",
      "full_text": "Title: [2504.00698},] Article identifier not recognized\n\nURL Source: https://arxiv.org/abs/2504.00698%7D,\n\nWarning: Target URL returned error 404: Not Found\n\nMarkdown Content:\n[2504.00698},] Article identifier not recognized\n\n===============\n\n[Skip to main content](https://arxiv.org/abs/2504.00698%7D,#content)\n\n[![Image 1: Cornell University Logo](https://arxiv.org/static/browse/0.3.4/images/icons/cu/cornell-reduced-white-SMALL.svg)](https://www.cornell.edu/)\n\nWe gratefully acknowledge support from the Simons Foundation, [member institutions](https://info.arxiv.org/about/ourmembers.html), and all contributors.[Donate](https://info.arxiv.org/about/donate.html)\n\n[](https://arxiv.org/IgnoreMe)\n![Image 2: arxiv logo](https://arxiv.org/static/browse/0.3.4/images/arxiv-logo-one-color-white.svg)\n===================================================================================================\n\n[Help](https://info.arxiv.org/help) | [Advanced Search](https://arxiv.org/search/advanced)\n\nSearch\n\n[![Image 3: arXiv logo](https://arxiv.org/static/browse/0.3.4/images/arxiv-logomark-small-white.svg)](https://arxiv.org/)\n\n[![Image 4: Cornell University Logo](https://arxiv.org/static/browse/0.3.4/images/icons/cu/cornell-reduced-white-SMALL.svg)](https://www.cornell.edu/)\n\nGO\n\nquick links\n-----------\n\n*   [Login](https://arxiv.org/login)\n*   [Help Pages](https://info.arxiv.org/help)\n*   [About](https://info.arxiv.org/about)\n\nArticle identifier '2504.00698},' not recognized\n================================================\n\nYou might instead try to [search for articles](https://arxiv.org/search) using title or author information.\n\nFor additional help on arXiv identifiers, see [understanding the arXiv identifier](https://info.arxiv.org/help/arxiv_identifier.html).\n\n*   [About](https://info.arxiv.org/about)\n*   [Help](https://info.arxiv.org/help)\n\n*   [Contact](https://info.arxiv.org/help/contact.html)\n*   [Subscribe](https://info.arxiv.org/help/subscribe)\n\n*   [Copyright](https://info.arxiv.org/help/license/index.html)\n*   [Privacy Policy](https://info.arxiv.org/help/policies/privacy_policy.html)\n\n*   [Web Accessibility Assistance](https://info.arxiv.org/help/web_accessibility.html)\n*   [arXiv Operational Status](https://status.arxiv.org/)\n\n Get status notifications via [email](https://subscribe.sorryapp.com/24846f03/email/new) or [slack](https://subscribe.sorryapp.com/24846f03/slack/new)\n",
      "fetch_method": "jina-reader"
    }
  ]
}