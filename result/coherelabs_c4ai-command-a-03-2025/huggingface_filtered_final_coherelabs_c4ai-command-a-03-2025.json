{
  "1-1 (Weights)": "The quotes state that “Cohere Labs Command A is an open weights research release of a 111 billion parameter model optimized for demanding enterprises that require fast, secure, and high-quality AI.”  They further explain that users are invited to “try out Cohere Labs Command A before downloading the weights in our hosted Hugging Face Space (https://coherelabs-c4ai-command.hf.space/models/command-a-03-2025).”  Finally, the authors say, “We hope that the release of this model will make community-based research efforts more accessible, by releasing the weights of a highly performant 111 billion parameter model to researchers all over the world.”  Taken together, the quotes confirm that the weights of the target model (c4ai-command-a-03-2025) are publicly downloadable (after an initial ‘try-out’ option), constitute an “open weights research release,” and are explicitly intended for broad, global research use.",
  "1-1 (Weights)__evidence": [
    {
      "source": "[readme]",
      "quote": "Cohere Labs Command A is an open weights research release of a 111 billion parameter model optimized for demanding enterprises that require fast, secure, and high-quality AI."
    },
    {
      "source": "[readme]",
      "quote": "You can try out Cohere Labs Command A before downloading the weights in our hosted Hugging Face Space (https://coherelabs-c4ai-command.hf.space/models/command-a-03-2025)."
    },
    {
      "source": "[readme]",
      "quote": "We hope that the release of this model will make community-based research efforts more accessible, by releasing the weights of a highly performant 111 billion parameter model to researchers all over the world."
    }
  ],
  "1-2 (Code)": "Only one statement about code is provided: “Please install transformers from the source repository that includes the necessary changes for this model.”  This indicates that inference/serving support for c4ai-command-a-03-2025 requires a fork or branch of the Transformers library containing model-specific modifications.  The quotes give no information about the availability of any training, fine-tuning, or RLHF code, implying that such training code is not publicly released; only the modified inference library is referenced.",
  "1-2 (Code)__evidence": [
    {
      "source": "[readme]",
      "quote": "Please install transformers from the source repository that includes the necessary changes for this model."
    }
  ],
  "1-3 (License)": "Multiple excerpts consistently specify that the model is released under the Creative Commons Attribution-NonCommercial license: “* License:[CC-BY-NC](https://cohere.com/cohere-labs-cc-by-nc-license)” and “license: cc-by-nc-4.0.”  All quotes emphasize the non-commercial restriction—e.g., “I agree to use this model for non-commercial use ONLY: checkbox.”  In addition to the CC-BY-NC terms, users must also comply with “Cohere Lab's Acceptable Use Policy,” which is hyperlinked in the same sentences.  Access is gated: the model card lists “extra_gated_fields” (Name, Affiliation, Country) and requires confirmation of adherence to the License Agreement, underscoring that redistribution or commercial deployment is prohibited unless separately licensed by Cohere.  Thus, usage is limited to non-commercial purposes under CC-BY-NC 4.0 plus the Cohere Labs AUP, with mandatory acceptance via a gated form.",
  "1-3 (License)__evidence": [
    {
      "source": "[readme]",
      "quote": "* License:[CC-BY-NC](https://cohere.com/cohere-labs-cc-by-nc-license), requires also adhering to [Cohere Lab's Acceptable Use Policy](https://docs.cohere.com/docs/cohere-labs-acceptable-use-policy)"
    },
    {
      "source": "[readme]",
      "quote": "This model is governed by a [CC-BY-NC](https://cohere.com/cohere-labs-cc-by-nc-license), requires also adhering to [Cohere Lab's Acceptable Use Policy](https://docs.cohere.com/docs/cohere-labs-acceptable-use-policy)"
    },
    {
      "source": "[readme]",
      "quote": "I agree to use this model for non-commercial use ONLY: checkbox"
    },
    {
      "source": "[readme]",
      "quote": "license: cc-by-nc-4.0"
    },
    {
      "source": "[readme]",
      "quote": ": false\nlibrary_name: transformers\nlanguage:\n- en\n- fr\n- de\n- es\n- it\n- pt\n- ja\n- ko\n- zh\n- ar\n- el\n- fa\n- pl\n- id\n- cs\n- he\n- hi\n- nl\n- ro\n- ru\n- tr\n- uk\n- vi\nlicense: cc-by-nc-4.0\nextra_gated_prompt: \"By submitting this form, you agree to the [License Agreement](https://cohere.com/c4ai-cc-by-nc-license) and acknowledge that the i"
    },
    {
      "source": "[readme]",
      "quote": "two GPUs.\n\nDeveloped by: [Cohere](https://cohere.com/) and [Cohere Labs](https://cohere.for.ai/)\n\n* Point of Contact: [Cohere Labs](https://cohere.for.ai/) \n* License:[CC-BY-NC](https://cohere.com/cohere-labs-cc-by-nc-license), requires also adhering to [Cohere Lab's Acceptable Use Policy](https://docs.cohere.com/docs/cohere-labs-a"
    },
    {
      "source": "[readme]",
      "quote": "ribe at any time.\" \nextra_gated_fields:\n Name: text\n Affiliation: text\n Country: country\n I agree to use this model for non-commercial use ONLY: checkbox\n---\n\n# **Model Card for Cohere Labs Command A**\n\n## **Model Summary**\n\nCohere Labs Command A is an o"
    }
  ],
  "1-4 (Paper)": "The documentation points to an official technical report: “For more details on how this model was developed, check out our [Tech Report](https://arxiv.org/abs/2504.00698).”  A citation is also provided: “@misc{cohere2025commandaenterprisereadylarge, title={Command A: An Enterprise-Ready Large Language Model}, …”  These quotes confirm that a dedicated paper/tech report exists on arXiv (identifier 2504.00698) specifically covering Command A, offering deeper discussion of its development and enterprise readiness.",
  "1-4 (Paper)__evidence": [
    {
      "source": "[readme]",
      "quote": "For more details on how this model was developed, check out our [Tech Report](https://arxiv.org/abs/2504.00698)."
    },
    {
      "source": "[readme]",
      "quote": "@misc{cohere2025commandaenterprisereadylarge,\n      title={Command A: An Enterprise-Ready Large Language Model},"
    }
  ],
  "1-5 (Architecture)": "The provided documentation repeatedly identifies the model as “c4ai-command-a-03-2025.” In the architecture bullet list, it explicitly states “Model Size: 111 billion parameters,” establishing that the network contains 111 B learnable weights. In the same list, as well as in a separate reiterated sentence, the text specifies a “Context length: 256K,” and restates that “Command A supports a context length of 256 K.” Taken together, these two facts give the entirety of the architectural detail disclosed in the quotes: (1) the parameter count of 111 B and (2) the extremely large maximum window of 256 000 tokens that the architecture is designed to accept in a single forward pass. No further layer‐wise, head‐count, or hyper-parameter breakdowns are present in the supplied excerpts.",
  "1-5 (Architecture)__evidence": [
    {
      "source": "[readme]",
      "quote": "* Model: c4ai-command-a-03-2025  \n* Model Size: 111 billion parameters  \n* Context length: 256K"
    },
    {
      "source": "[readme]",
      "quote": "**Context Length**: Command A supports a context length of 256K."
    }
  ],
  "1-6 (Tokenizer)": "All tokenizer information centers on how users should load and format messages for the target model. The snippet shows that messages should be prepared “with the c4ai-command-a-03-2025 chat template,” implying a dedicated prompt-formatting convention bundled with the release. Programmatically, the tokenizer is pulled directly from HuggingFace via:\n\nmodel_id = \"CohereLabs/c4ai-command-a-03-2025\"\ntokenizer = AutoTokenizer.from_pretrained(model_id)\n\nThis line demonstrates that the tokenizer artefact is published under the same repository name on the Hub and can be downloaded with the standard AutoTokenizer class, suggesting compatibility with the Transformers ecosystem. The quote, however, does not reveal vocabulary size, model type (BPE, SentencePiece, etc.), or any special token details beyond the existence of a chat template.",
  "1-6 (Tokenizer)__evidence": [
    {
      "source": "[readme]",
      "quote": "# Format message with the c4ai-command-a-03-2025 chat template"
    },
    {
      "source": "[readme]",
      "quote": "model_id = \"CohereLabs/c4ai-command-a-03-2025\"\ntokenizer = AutoTokenizer.from_pretrained(model_id)"
    }
  ],
  "2-1 (Hardware)": "The only hardware-related statement given is that Cohere Labs positions Command A as “deployable on just two GPUs,” despite its 111 billion parameter scale. While framed in terms of deployment rather than training, this still conveys that the model can run inference on a dual-GPU setup, highlighting cost-effective operational requirements. No other specifics—such as GPU model (A100, H100, etc.), memory footprint, training cluster size, or total FLOPs—are disclosed in the provided material.",
  "2-1 (Hardware)__evidence": [
    {
      "source": "[readme]",
      "quote": "Cohere Labs Command A is an open weights research release of a 111 billion parameter model optimized for demanding enterprises that require fast, secure, and high-quality AI. Compared to other leading proprietary and open-weights models Command A delivers maximum performance with minimum hardware costs, excelling on business-critical agentic and multilingual tasks while‬ being deployable on just two GPUs."
    }
  ],
  "2-2 (Software)": "",
  "2-2 (Software)__evidence": [],
  "2-3 (API)": "Command A has been specifically trained with conversational tool use capabilities, allowing the model to interact with external tools such as APIs, databases, or search engines.",
  "2-3 (API)__evidence": [
    {
      "source": "[readme]",
      "quote": "Command A has been specifically trained with conversational tool use capabilities. This allows the model to interact with external tools like APIs, databases, or search engines."
    }
  ],
  "3-1 (Pre-training)": "",
  "3-1 (Pre-training)__evidence": [
    {
      "source": "[readme]",
      "quote": "Model Architecture: This is an auto-regressive language model that uses an optimized transformer architecture. After pretraining, this model uses supervised fine-tuning (SFT) and preference training to align model behavior to human preferences for helpfulness and safety."
    }
  ],
  "3-2 (Fine-tuning)": "",
  "3-2 (Fine-tuning)__evidence": [
    {
      "source": "[readme]",
      "quote": "After pretraining, this model uses supervised fine-tuning (SFT) and preference training to align model behavior to human preferences for helpfulness and safety."
    }
  ],
  "3-3 (Reinforcement Learning)": "",
  "3-3 (Reinforcement Learning)__evidence": [
    {
      "source": "[readme]",
      "quote": "After pretraining, this model uses supervised fine-tuning (SFT) and preference training to align model behavior to human preferences for helpfulness and safety."
    }
  ],
  "4-1 (Pre-training Data)": "The documentation for coherelabs/c4ai-command-a-03-2025 states that the pre-training corpus spans 23 distinct natural languages. Specifically, the training mixture includes English, French, Spanish, Italian, German, Portuguese, Japanese, Korean, Arabic, Chinese, Russian, Polish, Turkish, Vietnamese, Dutch, Czech, Indonesian, Ukrainian, Romanian, Greek, Hindi, Hebrew, and Persian. The quote emphasizes the breadth of linguistic coverage, implying that the raw pre-training data was intentionally assembled to represent a multilingual distribution rather than an English-only set. No numerical token counts, domain breakdowns, or license descriptions are provided, but the explicit enumeration of the 23 languages confirms that the model’s foundational corpus is multilingual and that representation was a design objective during data collection for the 03-2025 Command series model.",
  "4-1 (Pre-training Data)__evidence": [
    {
      "source": "[readme]",
      "quote": "**Languages covered**: The model has been trained on 23 languages: English, French, Spanish, Italian, German, Portuguese, Japanese, Korean, Arabic, Chinese, Russian, Polish, Turkish, Vietnamese, Dutch, Czech, Indonesian, Ukrainian, Romanian, Greek, Hindi, Hebrew, and Persian."
    }
  ],
  "4-2 (Fine-tuning Data)": "For the fine-tuning stage, the only disclosed information is that, after pre-training, coherelabs/c4ai-command-a-03-2025 undergoes supervised fine-tuning (SFT) and subsequent preference training. These two steps are carried out to ‘align model behavior to human preferences for helpfulness and safety’. While the quote does not enumerate concrete datasets, domains, or licensing terms, it clearly conveys the pipeline structure: first an SFT pass—implying a curated set of instruction–response pairs—and then a preference-based optimization phase where human (or human-like) feedback signals are used to steer the model toward preferred answers. Thus, compositionally, the fine-tuning data consist of (i) supervision examples used in SFT and (ii) preference-annotated examples used in alignment, both selected for the twin goals of helpfulness and safety.",
  "4-2 (Fine-tuning Data)__evidence": [
    {
      "source": "[readme]",
      "quote": "After pretraining, this model uses supervised fine-tuning (SFT) and preference training to align model behavior to human preferences for helpfulness and safety."
    }
  ],
  "4-3 (Reinforcement Learning Data)": "The same sentence indicates that, in addition to SFT, coherelabs/c4ai-command-a-03-2025 receives ‘preference training’. In typical alignment pipelines, this preference training corresponds to a reinforcement-learning-from-human-feedback (RLHF) or similar reward-model approach. The quote clarifies that the preference data are grounded in ‘human preferences for helpfulness and safety’. Although no numeric details or accessibility statements are provided, the disclosure confirms that the RL dataset is human-labelled (or derived from human feedback) and that its purpose is to optimize the model’s responses toward safety-conscious, helpful outputs. Therefore, the reinforcement-learning data component is a set of preference comparisons or ratings that teach the model which answers humans consider superior under the safety and usefulness criteria.",
  "4-3 (Reinforcement Learning Data)__evidence": [
    {
      "source": "[readme]",
      "quote": "After pretraining, this model uses supervised fine-tuning (SFT) and preference training to align model behavior to human preferences for helpfulness and safety."
    }
  ],
  "4-4 (Data Filtering)": "The only filtering-related information released for coherelabs/c4ai-command-a-03-2025 centers on its configurable safety modes. The quote explains that ‘Command A can be configured with two safety modes’, labelled ‘contextual mode’ and ‘strict mode’. Contextual mode is intended for ‘wide-ranging interactions with fewer constraints on output’ yet still maintains ‘core protections by rejecting harmful or illegal suggestions’. Although explicit numeric thresholds or classifier names are not furnished, the existence of a strict-versus-contextual toggle indicates an internal filtering or guard-rail subsystem controlling generation. The designers therefore built two operational policies: (1) a looser, context-aware filter that allows more open-ended content while screening out disallowed material, and (2) a stricter policy that presumably applies tighter constraints. This dual-mode guardrail mechanism represents the principal documented data (or output) filtering feature for the 03-2025 Command model.",
  "4-4 (Data Filtering)__evidence": [
    {
      "source": "[readme]",
      "quote": "Besides, Command A can be configured with two safety modes, which enable users to set guardrails that are both safe and suitable to their needs: contextual mode, or strict mode. Contextual mode is appropriate for wide-ranging interactions with fewer constraints on output, while maintaining core protections by rejecting harmful or illegal suggestions."
    }
  ],
  "__usage": {
    "fine_tuning": "used",
    "rl": "unknown"
  }
}