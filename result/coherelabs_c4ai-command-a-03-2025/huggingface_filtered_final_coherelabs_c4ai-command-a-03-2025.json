{
  "1-1 (Weights)": "The provided material repeatedly stresses that \"Cohere Labs Command A is an open weights research release\" and identifies the snapshot as a 111 billion-parameter model. Access is explicitly offered in two ways: (1) users may \"try out Cohere Labs Command A before downloading the weights in our hosted Hugging Face Space (https://coherelabs-c4ai-command.hf.space/models/command-a-03-2025),\" and (2) they can fetch the individual weight shards that follow the pattern \"model-00001-of-00049.safetensors\" (the quoted filename confirms that at least 49 sharded .safetensors files are distributed). All sentences that describe the weights label the release as ‘open,’ signalling that the full parameter tensors are downloadable, not merely held behind an API. No paywall or enterprise-only gate is mentioned in the quotes; the only referenced prerequisite is visiting the Hugging Face Space, where both an interactive demo and the raw checkpoints are made available for the \"command-a-03-2025\" checkpoint.",
  "1-1 (Weights)__evidence": [
    {
      "source": "[readme]",
      "quote": "Cohere Labs Command A is an open weights research release of a 111 billion parameter model optimized for demanding enterprises that require fast, secure, and high-quality AI."
    },
    {
      "source": "[readme]",
      "quote": "You can try out Cohere Labs Command A before downloading the weights in our hosted [Hugging Face Space](https://coherelabs-c4ai-command.hf.space/models/command-a-03-2025)."
    },
    {
      "source": "[files]",
      "quote": "model-00001-of-00049.safetensors"
    }
  ],
  "1-2 (Code)": "The only statement about code is: \"Please install transformers from the source repository that includes the necessary changes for this model.\" From this we can infer that inference/serving requires a fork or a bleeding-edge commit of the Hugging Face Transformers library that already integrates the Command-A architecture or tokenizer changes. Crucially, there is NO line indicating that the pre-training, fine-tuning, or RLHF training scripts themselves have been open-sourced; the quote talks solely about installing a modified inference library. Therefore, with the evidence given, training code for any stage (pre-training, supervised fine-tune, or RL) is not publicly provided, while minimal inference support has been upstreamed to a special branch of Transformers.",
  "1-2 (Code)__evidence": [
    {
      "source": "[readme]",
      "quote": "Please install transformers from the source repository that includes the necessary changes for this model."
    }
  ],
  "1-3 (License)": "Multiple overlapping passages establish that the model is released under Creative Commons Non-Commercial terms. Exact quoted phrases include \"license: cc-by-nc-4.0\", \"* License:[CC-BY-NC](https://cohere.com/cohere-labs-cc-by-nc-license), requires also adhering to [Cohere Lab's Acceptable Use Policy]\", and \"This model is governed by a [CC-BY-NC](https://cohere.com/cohere-labs-cc-by-nc-license)\".  A gated download form presents an \"extra_gated_prompt\" where users must agree that \"By submitting this form, you agree to the [License Agreement]\"; additional gated fields request \"Name\", \"Affiliation\", \"Country\", and a mandatory checkbox that states \"I agree to use this model for non-commercial use ONLY\".  Together these clauses mean:  • Redistribution and modification are allowed only under CC-BY-NC 4.0;  • Commercial use is expressly disallowed;  • Users must also follow Cohere Labs’ AUP, forming an extra layer of restrictions;  • The project is co-developed by \"[Cohere](https://cohere.com/) and [Cohere Labs](https://cohere.for.ai/)\" who serve as the points of contact. No text offers an enterprise/commercial license path inside these quotes, so the sole officially documented permission set is the CC-BY-NC 4.0 plus AUP regime.",
  "1-3 (License)__evidence": [
    {
      "source": "[readme]",
      "quote": "license: cc-by-nc-4.0"
    },
    {
      "source": "[readme]",
      "quote": "* License:[CC-BY-NC](https://cohere.com/cohere-labs-cc-by-nc-license), requires also adhering to [Cohere Lab's Acceptable Use Policy](https://docs.cohere.com/docs/cohere-labs-acceptable-use-policy)"
    },
    {
      "source": "[readme]",
      "quote": "This model is governed by a [CC-BY-NC](https://cohere.com/cohere-labs-cc-by-nc-license), requires also adhering to [Cohere Lab's Acceptable Use Policy](https://docs.cohere.com/docs/cohere-labs-acceptable-use-policy)"
    },
    {
      "source": "[readme]",
      "quote": ": false\nlibrary_name: transformers\nlanguage:\n- en\n- fr\n- de\n- es\n- it\n- pt\n- ja\n- ko\n- zh\n- ar\n- el\n- fa\n- pl\n- id\n- cs\n- he\n- hi\n- nl\n- ro\n- ru\n- tr\n- uk\n- vi\nlicense: cc-by-nc-4.0\nextra_gated_prompt: \"By submitting this form, you agree to the [License Agreement](https://cohere.com/c4ai-cc-by-nc-license) and acknowledge that the i"
    },
    {
      "source": "[readme]",
      "quote": "two GPUs.\n\nDeveloped by: [Cohere](https://cohere.com/) and [Cohere Labs](https://cohere.for.ai/)\n\n* Point of Contact: [Cohere Labs](https://cohere.for.ai/) \n* License:[CC-BY-NC](https://cohere.com/cohere-labs-cc-by-nc-license), requires also adhering to [Cohere Lab's Acceptable Use Policy](https://docs.cohere.com/docs/cohere-labs-a"
    },
    {
      "source": "[readme]",
      "quote": "ribe at any time.\" \nextra_gated_fields:\n Name: text\n Affiliation: text\n Country: country\n I agree to use this model for non-commercial use ONLY: checkbox\n---\n\n# **Model Card for Cohere Labs Command A**\n\n## **Model Summary**\n\nCohere Labs Command A is an o"
    }
  ],
  "1-4 (Paper)": "One concise sentence links to formal documentation: \"For more details on how this model was developed, check out our [Tech Report](https://arxiv.org/abs/2504.00698).\" Hence the canonical write-up for Command-A-03-2025 is an arXiv technical report at the above URL, which readers are directed to for methodology, training details, evaluation, and further background.",
  "1-4 (Paper)__evidence": [
    {
      "source": "[readme]",
      "quote": "For more details on how this model was developed, check out our [Tech Report](https://arxiv.org/abs/2504.00698)."
    }
  ],
  "1-5 (Architecture)": "The quotes state that the model explicitly identified as “c4ai-command-a-03-2025” (also referred to as Cohere Labs Command A) is an open-weights research release with 111 billion parameters. It is an auto-regressive transformer language model whose context window is 256 K tokens. Design details given in the quoted material describe four distinct attention layers: the first three layers rely on sliding-window attention with a window size of 4096 tokens and employ RoPE (Rotary Positional Embeddings) for relative positional encoding, while a fourth layer switches to global attention that completely removes positional embeddings so tokens can interact freely across the entire sequence. After the core pre-training, the model undergoes supervised fine-tuning (SFT) followed by preference-based alignment to make its behaviour helpful and safe for enterprise use. In short, c4ai-command-a-03-2025 is a 111 B-parameter, 256 K-context, autoregressive transformer that combines sliding-window + RoPE attention with a final global-attention layer, then gains additional alignment through SFT and preference training.",
  "1-5 (Architecture)__evidence": [
    {
      "source": "[readme]",
      "quote": "Cohere Labs Command A is an open weights research release of a 111 billion parameter model optimized for demanding enterprises that require fast, secure, and high-quality AI."
    },
    {
      "source": "[readme]",
      "quote": "* Model: c4ai-command-a-03-2025  \n* Model Size: 111 billion parameters  \n* Context length: 256K"
    },
    {
      "source": "[readme]",
      "quote": "**Model Architecture**: This is an auto-regressive language model that uses an optimized transformer architecture. After pretraining, this model uses supervised fine-tuning (SFT) and preference training to align model behavior to human preferences for helpfulness and safety. The model features three layers with **sliding window attention** (window size 4096) and **RoPE** for efficient local context modeling and relative positional encoding. A fourth layer uses **global attention** without positional embeddings, enabling unrestricted token interactions across the entire sequence."
    }
  ],
  "1-6 (Tokenizer)": "The only tokenizer-related information in the quotes is that users must “format messages with the c4ai-command-a-03-2025 chat template,” and that Retrieval-Augmented Generation (RAG) with Command A is supported through chat templates in Hugging Face Transformers. No further specifics (vocabulary size, BPE vs. SentencePiece, download location, etc.) are provided in the supplied quotes.",
  "1-6 (Tokenizer)__evidence": [
    {
      "source": "[readme]",
      "quote": "# Format message with the c4ai-command-a-03-2025 chat template"
    },
    {
      "source": "[readme]",
      "quote": "RAG with Command A is supported through chat templates in Transformers."
    }
  ],
  "2-1 (Hardware)": "",
  "2-1 (Hardware)__evidence": [],
  "2-2 (Software)": "",
  "2-2 (Software)__evidence": [],
  "2-3 (API)": "The quotes explicitly state two publicly accessible entry points for working with coherelabs/c4ai-command-a-03-2025. First, “You can try Command A chat in the playground [here](https://dashboard.cohere.com/playground/chat?model=command-a-03-2025),” confirming that Cohere’s own dashboard playground hosts an interactive chat interface tied to the exact model string “command-a-03-2025.” Second, “You can try out Cohere Labs Command A before downloading the weights in our hosted Hugging Face Space(https://coherelabs-c4ai-command.hf.space/models/command-a-03-2025),” establishes that a Hugging Face Space also exposes an online demo of the same model. Together these two sentences show the existence of an API-like, point-and-click interface (Cohere Playground) and an additional browser-based endpoint (Hugging Face Space) where users can send prompts and receive completions without running local code. Although no explicit token limits, authentication flow, or REST/RPC specification is listed in the quotes, the presence of both a Cohere dashboard URL and a Hugging Face Space URL strongly implies that developers can experiment interactively and download weights after testing. Thus, from the provided text we can summarize that the model is publicly reachable through Cohere’s playground and through a hosted Hugging Face Space, giving immediate hands-on access before any local deployment.",
  "2-3 (API)__evidence": [
    {
      "source": "[readme]",
      "quote": "You can try Command A chat in the playground [here](https://dashboard.cohere.com/playground/chat?model=command-a-03-2025)."
    },
    {
      "source": "[readme]",
      "quote": "You can try out Cohere Labs Command A before downloading the weights in our hosted Hugging Face Space(https://coherelabs-c4ai-command.hf.space/models/command-a-03-2025)."
    }
  ],
  "3-1 (Pre-training)": "The only direct information about pre-training for coherelabs/c4ai-command-a-03-2025 is contained in the sentence: “After pretraining, this model uses supervised fine-tuning (SFT) and preference training to align model behavior to human preferences for helpfulness and safety.” The wording confirms that a distinct pre-training phase occurred prior to alignment work, but it does not enumerate data sources, corpus size, token counts, model hyper-parameters, curriculum order, or training duration. Nonetheless, the sentence does establish a clear chronological boundary: pre-training finished first, and subsequent alignment stages (SFT and preference training) were layered on top. Therefore, the summary is limited to acknowledging the completion of an undisclosed pre-training process that produced a base model sufficiently capable to move on to supervised fine-tuning and preference-based alignment.",
  "3-1 (Pre-training)__evidence": [
    {
      "source": "[readme]",
      "quote": "After pretraining, this model uses supervised fine-tuning (SFT) and preference training to align model behavior to human preferences for helpfulness and safety."
    }
  ],
  "3-2 (Fine-tuning)": "Fine-tuning information comes from the same statement: “After pretraining, this model uses supervised fine-tuning (SFT) and preference training to align model behavior to human preferences for helpfulness and safety.” From this we can derive that coherelabs/c4ai-command-a-03-2025 underwent a supervised fine-tuning (SFT) stage immediately after its base pre-training. The stated goal of this SFT was explicit: to adjust the model so that its responses are aligned with human preferences, particularly around being helpful and safe. The quote groups SFT together with “preference training,” implying that SFT formed the first half of a broader alignment pipeline. No dataset names, number of examples, or hyper-parameters (learning rate, batch size, epochs) are provided in the quotes, but we can definitively report that supervised fine-tuning was employed and that its alignment objectives centered on helpfulness and safety.",
  "3-2 (Fine-tuning)__evidence": [
    {
      "source": "[readme]",
      "quote": "After pretraining, this model uses supervised fine-tuning (SFT) and preference training to align model behavior to human preferences for helpfulness and safety."
    }
  ],
  "3-3 (Reinforcement Learning)": "The reinforcement-learning or preference-optimization stage is again referenced in the sentence: “After pretraining, this model uses supervised fine-tuning (SFT) and preference training to align model behavior to human preferences for helpfulness and safety.” Here, the term “preference training” is the only clue pointing to an RL-type alignment step—likely Reinforcement Learning from Human Feedback (RLHF) or a comparable approach—whose purpose was to nudge the model toward outputs rated higher by human evaluators for helpfulness and safety. While the quote does not spell out the algorithmic variant (e.g., PPO, DPO) or specific reward models, it affirms that some form of preference-based reinforcement learning occurred after SFT. Thus, the summary: coherelabs/c4ai-command-a-03-2025, having completed pre-training and SFT, received an additional preference-training phase that leverages human feedback to further align its behavior, specifically optimizing for helpfulness and safety in its responses.",
  "3-3 (Reinforcement Learning)__evidence": [
    {
      "source": "[readme]",
      "quote": "After pretraining, this model uses supervised fine-tuning (SFT) and preference training to align model behavior to human preferences for helpfulness and safety."
    }
  ],
  "4-1 (Pre-training Data)": "The quotes indicate that the coherelabs/c4ai-command-a-03-2025 model was exposed, during its pre-training stage, to a multilingual corpus that explicitly covered 23 languages: English, French, Spanish, Italian, German, Portuguese, Japanese, Korean, Arabic, Chinese, Russian, Polish, Turkish, Vietnamese, Dutch, Czech, Indonesian, Ukrainian, Romanian, Greek, Hindi, Hebrew, and Persian. No other details about data source types, licensing, or exact corpus sizes are disclosed in the available material.",
  "4-1 (Pre-training Data)__evidence": [
    {
      "source": "[readme]",
      "quote": "**Languages covered**: The model has been trained on 23 languages: English, French, Spanish, Italian, German, Portuguese, Japanese, Korean, Arabic, Chinese, Russian, Polish, Turkish, Vietnamese, Dutch, Czech, Indonesian, Ukrainian, Romanian, Greek, Hindi, Hebrew, and Persian."
    }
  ],
  "4-2 (Fine-tuning Data)": "According to the quoted material, once the base pre-training was complete the coherelabs/c4ai-command-a-03-2025 model underwent supervised fine-tuning (SFT) followed by preference training. Both steps were aimed at aligning the system’s behavior with human preferences for helpfulness and safety. The source composition, concrete examples, or public availability of these fine-tuning datasets are not described in the provided quotes.",
  "4-2 (Fine-tuning Data)__evidence": [
    {
      "source": "[readme]",
      "quote": "After pretraining, this model uses supervised fine-tuning (SFT) and preference training to align model behavior to human preferences for helpfulness and safety."
    }
  ],
  "4-3 (Reinforcement Learning Data)": "The same quoted sentence states that, in addition to supervised fine-tuning, the model also received preference training designed to align output with human judgments of helpfulness and safety. This implies a reinforcement-learning-from-human-feedback (RLHF) style dataset, but the specific composition, accessibility, or generation method (e.g., size, annotator demographics, or sampling strategy) is not revealed in the supplied excerpts.",
  "4-3 (Reinforcement Learning Data)__evidence": [
    {
      "source": "[readme]",
      "quote": "After pretraining, this model uses supervised fine-tuning (SFT) and preference training to align model behavior to human preferences for helpfulness and safety."
    }
  ],
  "4-4 (Data Filtering)": "",
  "4-4 (Data Filtering)__evidence": [],
  "__usage": {
    "fine_tuning": "used",
    "rl": "used"
  }
}