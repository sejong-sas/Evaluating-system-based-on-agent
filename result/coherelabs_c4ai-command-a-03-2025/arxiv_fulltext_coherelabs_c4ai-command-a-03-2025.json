{
  "model_id": "coherelabs/c4ai-command-a-03-2025",
  "full_texts": [
    {
      "arxiv_id": "2504.00698",
      "full_text": "Command A:\nAn Enterprise-Ready Large Language Model\nCohere1\nAbstract\nIn this report we describe the development of Command A, a powerful large language model purpose-built to\nexcel at real-world enterprise use cases. Command A is an agent-optimised and multilingual-capable model,\nwith support for 23 languages of global business, and a novel hybrid architecture balancing efficiency with top\nof the range performance. It offers best-in-class Retrieval Augmented Generation (RAG) capabilities with\ngrounding and tool use to automate sophisticated business processes. These abilities are achieved through\na decentralised training approach, including self-refinement algorithms and model merging techniques. We\nalso include results for Command R7B which shares capability and architectural similarities to Command A.\nWeights for both models have been released for research purposes. This technical report details our original\ntraining pipeline and presents an extensive evaluation of our models across a suite of enterprise-relevant tasks\nand public benchmarks, demonstrating excellent performance and efficiency.\n1\nIntroduction\nLarge Language Models (LLMs) are Artificial Intelligence (AI) models designed to understand and gener-\nate human-like text conditioned on the input they receive. Recent advancements have led to remarkable\nbreakthroughs in their ability to comprehend and produce human language with unparalleled accuracy and\nfluency. This progress has been instrumental in their widespread adoption across various real-world and\nenterprise environments, where they significantly boost operational efficiency and deepen understanding.\nThis technical report describes the development of Command A and Command R7B, two LLMs designed to\nexcel in real-world enterprise settings. Both the 111B parameter Command A and Command R7B perform\nbest-in-class across a suite of established benchmarks for their respective model sizes. We also highlight\nkey innovations and technical contributions including data and architectural optimisations, self-refinement\nalgorithms, and a model merging-based approach optimised to bring out expert-level performance across\ncapabilities within a single set of model weights, providing fast and efficient performance.\nCommand A is tailored for excellent performance in enterprise-relevant settings such as Retrieval Augmented\nGeneration (RAG), where models can interact with, understand, and process information distributed across a\nwide range of documents. As part of this focus, our models also excel in the multilingual setting, supporting 23\nkey languages of global business: English, French, Spanish, Italian, German, Portuguese, Japanese, Korean,\nArabic, Chinese, Russian, Polish, Turkish, Vietnamese, Dutch, Czech, Indonesian, Ukrainian, Romanian,\nGreek, Hindi, Hebrew, and Persian.\nAlong with its impressive overall performance, achieving best-in-class results for any model in its size and\nefficiency range on common benchmarks such as MATH, Command A outperforms across an extensive\nsuite of human evaluation tasks as shown in Figure 1. Furthermore, Command A achieves strong results on\nenterprise-relevant agentic benchmarks such as Taubench, as shown in Table 1.\nCommand A focuses on delivering competitive performance as efficiently as possible. With a serving footprint\nof just two A100s or H100s, Command A requires considerably less computational overhead than comparable\nmodels. This is of particular importance for privacy-preserving enterprise settings and on-premises deploy-\nments. Command A can deliver tokens at a rate of up to 156 tokens/sec which is 1.75x higher than GPT-4o\nand 2.4x higher than DeepSeek V3.\n1Please cite this technical report as “Cohere (2025)”. A full author list can be found at the end of this document.\n1\nReleased as a preprint on April 7, 2025\n\nHuman Preference Evaluation\nCommand A vs GPT-4o (Nov)\nCommand A vs DeepSeek-V3\nGeneral \nBusiness\nSTEM\nCode\nGeneral \nBusiness\nSTEM\nCode\n50.4%​\n51.4%​\n46.8%​\n49.6%​\n48.6%​\n53.2%​\n49.0%​\n49.3%​\n54.7%​\n51.0%​\n50.7%​\n45.3%​\nFigure 1: Head-to-head human evaluation win rates. All examples are blind-annotated by specially trained\nhuman annotators, assessing enterprise-focused accuracy, instruction following, and style.\nCapability\nBenchmark\nCommand A\nDeepSeek V3\nGPT-4o\nLlama 3.3 70B\nCommand R7B\nLlama 3.1 8B\nMinistral 8B\nAcademic\nMMLU\n85.5\n88.5\n85.7\n86.0\n65.2\n71.1\n71.1\nMATH\n80.0\n70.2\n68.5\n77.0\n59.1\n51.9\n54.5\nIFEval\n90.9\n86.1\n83.8\n92.1\n77.9\n78.6\n59.0\nGPQA\n50.8\n59.1\n53.6\n50.5\n26.3\n23.4\n23.4\nAgents\nTaubench\n51.7\n39.1\n51.2\n21.0\nBFCL\n63.8\n58.6\n72.1\n51.4\n52.2\n50.9\n51.8\nCode\nMBPP+\n86.2\n89.9\n86.5\n84.4\n72.0\n72.8\n61.1\nBird-SQL\n59.5\n53.1\n50.5\n58.0\n42.2\n41.9\n33.2\nRepoQA\n92.6\n92.2\n91.2\n85.6\n69.6\n73.6\n62.0\nMultilingual\nNTREX\n68.8\n69.8\n71.0\n62.5\n48.1\n49.2\n36.8\nTable 1: Command A and Command R7B results on key academic, agentic, code and multilingual bench-\nmarks, in comparison to relevant external models.\nWe also release model weights to the research community to facilitate community-based exploration under\na CC-BY-NC License (Non-Commercial) with an acceptable use addendum. The model checkpoints are\navailable on the HuggingFace model hub.\n2\nPre-training\n2.1\nOverview\nPre-training language models involves training a model on trillions of tokens of unlabelled text data to learn\ngeneral language patterns, syntax, and semantics, enabling it to generate contextually relevant responses.\nThis foundational step leverages self-supervised learning techniques, such as next-token prediction, to build\na versatile representation of language that can subsequently be fine-tuned for specific downstream tasks. Pre-\ntraining is computationally intensive but essential for achieving state-of-the-art performance across diverse\nnatural language processing applications.\n2\n\n2.2\nData\nCommand A models are trained on multilingual data (also see Section 3.3.3.1) from various sources including\npublicly available text and code data from the web, a collection of synthetic datasets generated internally,\ninstruction-tuning datasets obtained from human annotators, and high quality data sourced from specialised\ndata vendors. We optimise the web text data by enhancing the ratio of educational samples that are relatively\nsparse on the internet, and down-sampling low-quality samples identified by Machine Learning (ML)-based\nquality filters after careful de-duplication and heuristic filtering for safety and quality. The final data mixture\nis determined by running a series of ablations using smaller models.\n2.3\nModel Architecture\nSliding Window \nSelf-A!ention \n - Grouped-query a!ention \n - RoPE positional embeddings\nMLP \n - SwiGLU activation \n - No bias terms\n Self-A!ention\nMLP\n. . .\n  Tokenizer\n - Vocabulary size : 255,000 \n - Multilingual \n - Special tokens for chat turns, tool calls.\n  LM Head\n  Output embeddings\n  Input embeddings\nCommand A\nCommand A Transformer Block (SWA)\nShared input and output embeddings\nInterleaved SWA and \nfull a!ention (3:1 ratio)\n  Transformer Block 4\n Self-A!ention\n  Transformer Block 2\n(SWA)\n Self-A!ention\nMLP\n  Transformer Block 3\n(SWA)\n(Full)\n Self-A!ention\nMLP\n  Transformer Block 1\n(SWA)\nMLP\n  Transformer Block N\n Self-A!ention\n(Full)\nMLP\nFull  \nSelf-A!ention   \n - Grouped-query a!ention \n - No positional embeddings\nMLP \n - SwiGLU activation \n - No bias terms\nCommand A Transformer Block (Full)\nFigure 2: Schematic of the Command A model architecture.\nWe use a decoder-only Transformer architecture (Vaswani et al., 2017) as depicted in Figure 2. We highlight\na few key architectural decisions below:\n• SwiGLU. The SwiGLU activation (Shazeer, 2020) demonstrates performance improvements over other\nactivation functions.\n• Interleaved attention layers. We use interleaved layers of sliding window attention and full attention\nin 3:1 ratio. Each sliding window layer uses Rotary Positional Embeddings (RoPE) (Su et al., 2021) and\nevery full attention layer uses No Positional Embeddings (NoPE) (Kazemnejad et al., 2023). Further\ndetails can be found in Yang et al. (2025).\n• GQA. We use grouped-query attention (GQA; (Ainslie et al., 2023)) to increase serving throughput.\nWe use document masking to ensure that each individual sequence in a batch can only attend to itself.\n• Parallel transformer block. This shows equivalent performance but significant improvement in\nthroughput compared to the vanilla transformer block.\n• No bias. Similar to PaLM (Chowdhery et al., 2023), we do not employ bias terms, which improves\ntraining stability at larger scales.\n• Input and output embeddings. We share the input and output embedding matrices, which provides\na large reduction in memory requirements due to our large vocabulary size. We do not observe any\nperformance degradation across ablations.\n3\n\n2.4\nPre-training Recipe\nWe perform most of our hyperparameter optimisation at considerably smaller scales than those representing\nour final family of models. We use µP and µTransfer (Yang et al., 2021) to tune hyper-parameters on smaller\nmodels and zero-shot transfer them to our larger models. Sweeps are performed for each model size as they\nassume a fixed number of layers.\n2.4.1\nDistributed Training\nWe train all our models on our NVIDIA H100 GPU cluster using our internal JAX-based (Frostig et al.,\n2018) distributed training framework. Our framework leverages JAX’s GSPMD (Xu et al., 2021) as the\nbackbone to implement complex sharding strategies. We split the available GPUs into a mesh with four axes\nfor each of the following sharding schemes:\n• Data Parallel (DP) axis shards the activations along the batch dimension, which behaves as standard\ndata parallel training when all GPUs are allocated to it.\n• Fully Sharded Data Parallel (FSDP) axis shards both the activations along the batch dimension\nand model states along a specified dimension. The model states are replicated across the data parallel\naxis to contain the communication costs to the FSDP submesh.\n• Sequence Parallel (SP). Given the restrictions on critical batch-size of LLMs, scaling the number of\nGPUs in pure FSDP/DP scenarios is infeasible. We thus use sequence parallelism (Li et al., 2023b) to\nshard activations along the sequence dimension. The activations after the QKV projection are sharded\nalong the heads dimension to remove communication costs during the attention computation. The\nattention outputs are sharded on the outer dimension, and the weight matrix of the final attention\noutput transformation is sharded along the contracting dimension as in Megatron-style (Shoeybi et al.,\n2019) sharding. This allows us to operate using a single all-gather and a single reduce-scatter for the\nactivations, while only gathering QKV and attention outputs along the FSDP axis. At the feed forward\nblock, the FFN transformation is independent along the sequence axis, therefore there is no need for\nany activation communication. Moreover, since we use parallel attention and a FFN block setup, we\ncompletely overlap the computation of the FFN expansion layer and the all-gather of the attention\nactivations. The reduce-scatter after the attention block is further overlapped with the execution of the\nFFN reduction layer. Since all other major operations such as layer norm, input and output embedding\nlayers are independent along the sequence axis, they incur no communications along the activations.\n• Tensor Parallel (TP) axis for a pure Megatron-style sharding, where two complementary matrix\nmultiplications are sharded such that the activations are all-gathered before the first matrix multipli-\ncation (where the weight is sharded on the outer axis, resulting in the activations being sharded on the\nouter axis as well), and one all-reduce after the second matrix multiplication (to sum the partial out-\nputs). Pure TP is desirable when moving activations between devices as it is much cheaper compared\nto moving weights, a layout class sometimes referred to as weight-stationary (Pope et al., 2023). We\nuse pure TP for fast decoding and in low batch-size scenarios.\nOur models are trained with varying combinations of the parallelism strategies mentioned above. During\npre-training, since we are in the high batch-size and throughput regime, we opt for a combination of DP,\nFSDP and SP to minimise activation communication. Furthermore, we can unroll the model’s forward loop\nto overlap the communication of the weights of the next layer with the execution of the current layer.\nWe leverage Hopper-specific features such as FP8 tensor cores (Micikevicius et al., 2022) to further improve\nthroughput. While many works have reported instability while training with FP8 precision for long training\nhorizons (Fishman et al., 2025), we observe no such instability. In fact, we observe minimal run interventions\ndue to loss spikes and optimisation instability. We keep our main weights and optimiser states in FP32\nprecision, and cast the model weights to BF16 or FP8 prior to the computation. We keep sensitive operations\nsuch as exponentials, softmaxes, layer norms, and output embeddings in FP32 precision, and run the attention\ncomputation in BF16 precision. While we do not observe any training instabilities with FP8 matmuls, we\nnotice that there is a small but non-trivial degradation in downstream performance if the entire training\nrun is in FP8. To mitigate this effect, we first perform a number of steps in BF16 precision, which brings\nperformance back to the full BF16 trained model’s performance range.\n4\n\nModel \nMerging\nRAG, Tool Use & \nAgents  Expert \n(SFT)\nMultilingual Expert \n(SFT)\nCode Expert \n(SFT)\nMath & Reasoning \nExpert \n(SFT)\nLong Context Expert \n(SFT)\nSafety Expert \n(SFT)\nModel \nMerging\nReinforcement \nLearning\nSupervised \nFine-Tuning\nPolishing\nInstruction-Tuned \nCheckpoint\nMerged Checkpoint \n(SFT Soup Model)\nInstruction-Following \nExpert \n(SFT)\nExpert \nWeights\nRAG, Tool Use & \nAgents  Expert \n(RL)\nMultilingual Expert \n(RL)\nCode Expert \n(RL)\nMath & Reasoning \nExpert \n(RL)\nLong Context Expert \n(RL)\nSafety Expert \n(RL)\nInstruction-Following \nExpert \n(RL)\nSupervised Fine-Tuning (SFT)\nOﬄine-Preference Reinforcement Learning (RL)\nReinforcement Learning (RLHF)\nOﬄine-Preference Reinforcement Learning (RL)\nRAG & Agents\nMultilingual\nCode\nReasoning\nLong-Context\nSafety\nInstruction\nExpert \nWeights\nRAG & Agents\nMultilingual\nCode\nReasoning\nLong-Context\nSafety\nInstruction\nMerged Checkpoint \n(RL Soup Model)\nFigure 3: Command A goes through multiple post-training phases including two weighted model merging\nsteps, and a model polishing phase.\n2.5\nCooldown\nWe linearly anneal the learning rate from 2.5 × 10−4 to 1 × 10−6 for 50,000 steps in BF16 precision with\npurposely curated high quality datasets. The context length is initially maintained at 8k tokens for the\nfirst 30,000 steps, then extended to 32k, 128k, and 256k for 10,000, 5,000, and 5,000 steps respectively by\ninterleaving long context pre-training data every fourth step. During the long-context stages, we adjust the\noverall ratio of datasets to ensure a balanced mixture across domains (Fu et al., 2024), while maintaining a\nsufficient proportion of long-context data.\n3\nPost-training\n3.1\nOverview\nCommand A is post-trained using a novel decentralised approach to maximise and control its performance\nover a wide spectrum of domains and capability areas. More precisely, Command A is trained by alternating\ncentralised training stages, where a single model is fine-tuned, and decentralised training stages, where\nmultiple expert models are trained separately to maximise domain-wise performance before merging their\nparameters. Although the classic post-training paradigm involves training a single model sequentially with\nvarying data-mixtures (Dubey et al., 2024; Team et al., 2024), Command A is the first large-scale attempt\nto combine multiple parallel expert tracks, with parameter merging techniques playing a central role. This\nsection details the high-level post-training recipe of Command A, illustrated in Figure 3.\nWe divide the global Command A post-training recipe into several sub-stages, each producing intermediary\nmodel artifacts:\n• Instruct Model: We train an initial Instruct model with supervised learning on top of the base model\nto provide the core basic capabilities of the model.\n• SFT Expert Models: We train six SFT experts on top of the Instruct checkpoint with specialised\ndata mixtures to maximise capability-specific performance.\n• SFT Soup Model: We merge the six model experts into a Soup model with parameter-merging\nmethods (see Section 3.4) to produce a single SFT aggregate model.\n5\n\n• RL Expert Models: We train six RL experts on top of the SFT Soup checkpoint using RL algorithms\ntailored to each domain, using pairwise comparisons or verifiable rewards.\n• RL Soup Model: We merge the six RL experts into a RL Soup model with parameter-merging\nmethods to produce a single RL aggregate model.\n• Polished Model: We perform a final stage on the RL Soup model to enhance human interaction\nperformance by alternating between best-of-N methods, offline preference, and online RL algorithms.\nSix expert models are created at each expert stage: Code, Safety, RAG, Math, Multilingual, and a General\nLong-Context expert. This approach allows us to adapt each expert’s training procedure, tailoring it to the\nspecific capability or domain of interest. This allows fine-grained hyperparameter tuning, specialised data\nmixture optimisation, local optimisation (e.g., seed merging), and the capability-specific selection of the\nmost appropriate algorithms. This becomes even more crucial during the RL stage, as different domains\ndemand distinct RL techniques — for example, verifiable rewards for Math and Code, or preference pairs\nfor Safety and Multilingual. Our late-merging procedure allows us to re-balance Soup model performance a\nposteriori without requiring additional training (§3.4). From an organisational perspective, merging allows\ncontributors to collaborate closely in parallel, fostering a unique model development synergy. Overall, this\ndecentralised training procedure maximises individual expert performance while controlling the final global\nmodel capacity, allowing us to optimise both model performance and efficiency.\nFinally, the model undergoes a polishing phase to improve its writing style. First, we apply a best-of-N\nsupervised training stage to the RL Soup model. Then, we alternate between offline preference and online RL\noptimisation in a ping-pong approach, iterating as required until we observe a human preference performance\nplateau, to obtain the final Command A model.\nIn the following sections, we introduce the methods and algorithms that we use at various stages of the\npost-training process. We detail individual expert recipe considerations and provide further technical details\non the merging techniques applied. Finally, we discuss key features of the model polishing phase.\n3.2\nMethods\n3.2.1\nSupervised Fine-Tuning\nIn all cases, the first stage of our post-training pipeline involves finetuning the pretrained model to follow\ninstructions and operate in a conversational setting. We structure Supervised Fine-Tuning (SFT) (Wei et al.,\n2021; Sanh et al., 2021) datapoints as prompts and completions. Prompts are model input sequences that may\ncontain information such as preambles or system prompts defining expected model behaviour, tool specifi-\ncations, conversational history, special tokens (e.g., <|START_OF_TURN_TOKEN|> or <|SYSTEM_TOKEN|>), and\nqueries or instructions. Completions refer to the sequence of tokens that the model is trained to generate\nconditioned on a given prompt. We train the model using a cross-entropy loss with the loss corresponding\nto prompt tokens masked out. Depending on the specific setting, we may choose to regularise training either\nby including some small proportion of pretraining data, or a parameter-based L2 penalty to the pretrained\nmodel. We optimise using the AdamW algorithm (Loshchilov & Hutter, 2017) with decoupled weight decay.\n3.2.2\nReinforcement Learning\nDepending on the stage and task, we perform direct alignment using preference training (Rafailov et al.,\n2024; Azar et al., 2024) or we optimise for a reward signal through reinforcement learning (RL) (Sutton et al.,\n2023), either offline or online. This reward signal can be the learnt reward model, or a verifiable reward (for\nexample, based on unit tests for code generation or on response correctness for reasoning).\n3.2.2.1\nPreference Training with Self-refinement\nWe consider preference training methods for learning offline from preference datasets: Sequence Likelihood\nCalibration, or SLiC (Zhao et al., 2023), Direct Preference Optimisation, or DPO (Rafailov et al., 2024), and\nIdentity Preference Optimisation, or IPO (Azar et al., 2024). In addition to these conventional preference-\ntraining methods, our model-training pipeline incorporates our novel Self-improving Robust Preference Op-\ntimisation (SRPO) (Choi et al., 2025). This recently-developed approach represents a significant departure\n6\n\nfrom traditional preference training techniques, introducing a novel mechanism for continuously enhancing\nmodel alignment and robustness. It amounts to solving the following min-max optimisation problem:\nmin\nπ max\nπ† ExEy1∼π(·|x),y2∼π†(·|x,y1) [P(y2 ≻y1|x) −β KL(π†||πref|x, y1) + β KL(π||πref|x)] .\nThis objective function aims to learn a self-improvement policy π† that can improve generations from π,\naccording to the preference model P without deviating too much from a reference model πref, and at the\nsame time at learning a policy π of which generations cannot be improved by π†. SRPO’s novelty partly\nlies in its robustness: unlike classical methods, it does not depend on the sampling distribution of the\npreference dataset. This independence ensures greater generalisation and stability in varied deployment\nscenarios. Furthermore, SRPO uniquely enables iterative self-revision at inference time, a process where the\nmodel sequentially refines its output: Given an initial prompt, the model first generates an initial completion\nusing the generative policy π, followed by multiple sequential refinements through the self-refinement policy\nπ†, each progressively improving the quality and alignment of the final output. This iterative refinement\ncapability is not present in conventional alignment pipelines, underscoring SRPO’s innovative contribution.\n3.2.2.2\nOptimising the Reward Model with RL\nWhen given a reward function, be it the reward model or a verifiable reward, we consider the classic KL-\nregularised reinforcement learning objective, J(π) = ExEy∼π(·|x)[R(x, y) −β KL(π||πref|x)]. In all settings\n(offline or online), we optimise it using the recent Contrastive Policy Gradient approach, or CoPG (Flet-\nBerliac et al., 2024). For a prompt x and k > 1 completions y1, . . . , yk of arbitrary origin, the corresponding\nloss to be minimised is\nℓ(x, y1, . . . , yk; π) =\n1\nk −1\nX\ni>j\n\u0000Rπ\nβ(x, yi) −Rπ\nβ(x, yj)\n\u00012 with Rπ\nβ(x, y) = R(x, y) −β ln π(y|x)\nπref(y|x).\nThe CoPG loss can be used in both the offline and online cases. In the online case, it can be used with\na replay buffer, possibly combined with additional datasets, or in a pure on-policy fashion, in which case\nit is equivalent to Reinforce Leave-One Out, or RLOO (Ahmadian et al., 2024). Furthermore, Flet-Berliac\net al. (2024) show that the gradient of this loss is a form of (negative) off-policy policy gradient, not relying\non importance sampling, clipping, or on an additional value network. It also comes with strong theoretical\nguarantees, notably estimating the KL-regularised optimal policy π∗even in the offline case, and generalising\npolicy gradient and some preference training approaches. In the offline case, it can be used with any dataset,\nas long as there is more than one completion per prompt, and that we can compute the associated rewards.\nWe mostly use CoPG offline and online on-policy.\n3.2.3\nReward Models\nWe train a Bradley-Terry reward model for use in online preference learning, evaluation, and data filtering.\nSimilar to Gunter et al. (2024), we use a cross-entropy loss with soft labels as targets.\nWe find that reward models tend to suffer from high memorisation, causing catastrophic collapse in perfor-\nmance on a second epoch over the same data; so, we train the model in two stages. The first stage consists\nof approximately 4 million samples designated as “lower quality” and relabelled using an ensemble of reward\nmodels. Training is carried out for one epoch with a batch size of 1024 with a cosine learning rate schedule\nwith a peak of 4 × 10−5. The second stage consists of approximately 350,000 high quality samples, with\nlabels derived from the strength of human preferences, ensembles of models (with labels inconsistent with\nhuman annotations moved to the first stage), or a constant label value of 0.999 for gold-standard data and\n0.5 for gold-standard tied pairs. This stage uses a smaller batch size of 16, and a lower maximum learning\nrate of 3×10−6. Both stages use packed data, where multiple pairs of preference data are encoded in a single\ntraining sample for efficiency, using attention masking to avoid different (non-packed) samples influencing\neach other. The pairs are left-padded to align their <EOS_TOKEN>, and distributed to aim for a 75% fill while\nkeeping the number of samples per row as uniform as possible, ensuring an equal loss contribution.\nOur internal reward model scores 92.7% on RewardBench (Lambert et al., 2024), and achieves an average\nscore of 72.3% on RMB (Zhou et al., 2024).\n7\n\n3.3\nCapabilities\n3.3.1\nInstruction-Following\nCore instruction-following capabilities are crucial for LLMs to solve tasks across areas and domains. We\ntherefore consider instruction-following a prerequisite for more specific model capabilities focusing on ad-\nvanced topics such as code, multilingual, and reasoning. As such, in the Command A post-training recipe, we\nteach the model to follow instructions across a wide range of topics and domains, including but not limited to\ngeneralist instruction-following (e.g., factual knowledge requests), formatting, STEM-specific tasks (e.g., tab-\nular reasoning, structured data manipulation), and preamble compliance. Instruction-following capabilities\nare acquired both via SFT and offline preference tuning.\n3.3.1.1\nData collection\nOur data collection approach can be divided based on the post-training method, i.e., SFT or preference\ntuning. To collect datasets that serve both of these, we primarily rely on synthetic prompt generation in\nconjunction with human annotation, and explore various sampling and filtering techniques (Bartolo et al.,\n2021). Specifically, we synthetically generate diverse sets of prompts covering a range of instructions tailored\nto individual domains (such domains are mostly enterprise-oriented) and generate two completions per\nprompt sampled with different temperatures. We then ask human annotators to provide discrete ratings for\nboth completions. This process is repeated over multiple turns, resulting in a multi-turn dataset. If the two\ncompletion ratings are not tied and the better completion does not obtain the highest possible rating, we\nask the annotator to improve the better completion.\nSFT samples. SFT datasets are constructed using the human rewrites obtained from the process mentioned\nabove, to ensure the highest completion quality.\nPreference pairs. We construct preference pairs directly from the obtained samples by considering com-\npletions with different ratings (including the human rewrites), with ties excluded. It is worth noting that the\nobtained preference samples are used to train both Command A and our reward model itself.\n3.3.1.2\nIterative Reward-based Sample Refinement\nWe further experiment with reward-based sample refinement approaches to obtain both SFT and preference\npairs using the synthetically-generated prompts. Similar to Dubey et al. (2024), we use internal reward\nmodels trained on our most recent Command A checkpoints in conjunction with a collection of both human-\nwritten completions and completions generated from different checkpoints under different conditions (e.g.,\nvarying temperature values) during post-training This implies that the resulting dataset does not contain\npurely synthetic completions, and human-written completions are retained for inputs where the models fail\nto generate high-quality completions. We approach this in an iterative fashion, where we use the most recent\ncheckpoints at a given point in time to generate completions, score those completions using our reward\nmodel, create preference pairs and SFT samples using the scores, re-train our models, and repeat.\n3.3.1.3\nPreambles\nA specific focus of the instruction-following post-training of Command A lies in the model’s ability to follow\npreamble (or system prompt) requirements. Preambles are designed to contain instructions that should apply\nto an entire conversation and potentially multiple model inputs, meaning that instead of having to repeat\ninstructions in every prompt, they can be defined directly in the preamble. Such system instructions could\nspecify what language the model should reply in (e.g., “Always respond in French.”), the desired format of\nmodel generations (e.g., “Always use JSON.”, “Do not generate Markdown.”), or the exclusion of specific words\nand phrases (e.g., “Do not use the word ‘LLM’ in your response.”). To train Command A to follow preamble\ninstructions, we develop methods based on synthetic data generation to create diverse preambles that are\nattached to prompts flowing into the above-described pipeline. The preambles are then taken into account\nwhen creating the respective completions and preferences, i.e., preamble-augmented data is used during\nboth SFT and preference tuning. During preamble generation, we aim to maximise instruction diversity to\nencourage robustness to a wide range of instructions at inference time.\n8\n\n3.3.1.4\nModel training\nIn the context of instruction-following we post-train Command A in sequence with SFT and preference\ntuning. For preference tuning, we experiment with a range of methods, including SLiC, IPO, DPO, and\nSRPO (for further details, see Section 3.2.2). We find that SRPO performs best across evaluation tasks and\nselect a checkpoint trained using SRPO for the final Instruct model.\n3.3.2\nRetrieval Augmented Generation, Tool Use and Agents\nRecent breakthroughs have propelled LLMs beyond simple chatbots, transforming them into versatile agents\ncapable of navigating complex environments. At the heart of this evolution is their ability to use tools\nstrategically: invoking APIs, analysing results, and iterating dynamically to accomplish goals. This agen-\ntic behaviour is pivotal for two key advancements. First, integrating knowledge sources outside of model\nparameters (e.g., via Retrieval-Augmented Generation, or RAG) mitigates hallucinations and ensures accu-\nrate information beyond the timespan of model training. Second, agentic frameworks empower models to\norchestrate vast action chains, potentially executing hundreds of API calls to automate intricate workflows.\nTogether, these capabilities expand LLMs’ operational horizons, enabling them to tackle tasks once deemed\nbeyond their reach.\n3.3.2.1\nAgentic Tool-Use\nEmpowering LLMs with Tools. LLMs have demonstrated remarkable proficiency in leveraging external\ntools to enhance their capabilities (Schick et al., 2023). By generating API calls, models can execute specific\nactions—like performing calculations or retrieving information—to solve tasks more effectively. This process\ntypically involves providing a set of tool definitions in the model’s preamble. When faced with a task, the\nmodel selects and invokes the appropriate tools, and the results are fed back to inform its final response.\nA prime example of this is Retrieval-Augmented Generation (RAG). In RAG (Lewis et al., 2020), the model\nhas access to a search tool (e.g. a dense embedding index) to answer information-seeking queries. It generates\nsearch queries, retrieves relevant snippets from the selected knowledge source, and uses this context to craft\na well-informed response.\nAgents. For more intricate tasks, models may need to orchestrate multiple tools across several steps. This\nrequires a structured approach to halt generation, extract tool calls, execute them, and reintroduce results\ninto the model’s workflow—a process repeated until the task is resolved.\nWe roughly follow the ReAct framework (Yao et al., 2022), a widely adopted method for guiding LLMs\nthrough dynamic problem-solving. ReAct enables models to interleave reasoning and action: they first artic-\nulate their thought process, outlining plans and tool requirements, then either execute a tool (via structured\noutputs like JSON) or deliver a final answer. This iterative loop enables adaptive planning, reflection, and\ninteraction with external systems, making it ideal for complex, multi-step tasks.\n3.3.2.2\nData and Training\nWe train our model on a combination of human-annotated and synthetically generated data. We collect\ndatapoints in multiple languages to directly supervise on, as well as datapoints with preference judgments\nfor multiple completions. The data covers areas of code tool execution, user-uploaded documents, and general\nAPI environments. Training consists of an SFT step on agentic datasets followed by offline preference tuning\nusing the Contrastive Policy Gradient loss (§3.2.2.2).\nData format. Each datapoint contains a user prompt along with a set of available tools and potentially\ncustom model instructions that the user has provided to the model. The datapoint also contains a reasoning\nstep, where the model reasons about which tools to use to fulfil the user request and how to fill in the input\nparameters of the tools. This is followed by tool calls and tool outputs, which can be concurrent or sequential.\nThe datapoint concludes with a model response that includes citations to the tool outputs.\nData collection. Annotation is performed by internal annotators specifically trained in ReAct-style data.\nAll annotated data used for SFT is reviewed multiple times by different annotators to ensure correctness and\n9\n\nquality. For preference data, we use a majority vote of at least 3 annotators to collect preference judgments.\nSynthetic data. We also generate synthetic training data containing whole trajectories of reasoning and\ntool calls. We verify the quality of the trajectories using internal LLMs-as-a-judge.\n3.3.3\nMultilingual\nThe ability to communicate in and understand multiple languages is a fundamental component of enterprise\nLLMs. Command A is designed to handle a wide array of languages, ensuring that information can be\naccessed and shared seamlessly across different linguistic communities. By incorporating solid multilingual\ncapabilities in 23 languages, Command A enables businesses and individuals to reach a broader audience,\nfostering inclusivity and accessibility. Moreover, the multilingual aspect of Command A facilitates better\nunderstanding and collaboration among international teams, driving innovation and efficiency.\n3.3.3.1\nData Mixture\nWe focus our data mixture on 23 languages: English, French, Spanish, Italian, German, Portuguese, Japanese,\nKorean, Arabic, Chinese, Russian, Polish, Turkish, Vietnamese, Dutch, Czech, Indonesian, Ukrainian, Roma-\nnian, Greek, Hindi, Hebrew, and Persian. This ensures coverage to expand state-of-the-art language modelling\ncapabilities to approximately half of the world’s population (Aryabumi et al., 2024; Üstün et al., 2024). Our\nmultilingual data mixture spans a diverse set of domains and tasks, covering machine translation, multilin-\ngual safety, multilingual reasoning, multilingual robust controllability, multilingual RAG, multilingual agents,\netc; ensuring that Command A possesses strong generalisation capabilities across languages. The datasets\nare collected through various means including human annotation, multilingual data arbitrage (Odumakinde\net al., 2024; Dang et al., 2024), templated public datasets, or machine translation. Our data mixture is\nspecifically tailored to handle multilingual learning through SFT and offline preference tuning.\n3.3.3.2\nMultilingual Data Annotation\nMultilingual data annotation is performed by internal and external multilingual annotators who are expertly\ntrained for annotation within various tasks. It can be divided into two distinct processes, i.e., regionally-\nrelevant data annotation and complex multilingual task annotation, which cover use cases for both SFT\nand preference tuning. For complex tasks such as domain-specific RAG, long-context reasoning, or agentic\ntool-use tasks, we conduct human annotation using two different approaches: 1) LLM-generated response\nwith human post-editing; and 2) manually annotated human data. The prior helps us scale the quantity of\ndata, while the latter helps develop high-quality multilingual data for tackling complex tasks. We develop\na customised in-house data annotation platform that can support both of these use cases. The high-quality\ndata generated from human annotations also helps to further improve the quality of the machine-generated\nresponses providing a positive feedback loop within the annotation process.\nMultilingual Best-of-N. To further improve the multilingual quality of Command A, we conduct iterative\nsynthetic data collection through multilingual best-of-N (Stiennon et al., 2020; Eisenstein et al., 2024). Using\na collection of high-quality prompts, we collect responses from all expert models and score them using our\ninternal reward models and select the best response to be used in our iterative training. This approach is very\nsimilar to multilingual arbitrage where the model is trained on responses from diverse teacher models. Using\nthis approach, we observe from human evaluation that LLMs can produce responses that are comparable or\neven better than the human-written gold label provided in many multilingual datasets.\n3.3.3.3\nTraining\nThe multilingual expert model is trained via both SFT and Preference Tuning (full details in Appendix B.2).\nWe find that training several models with the same configuration (but a different random seed) and uniformly\nmerging them gives a slight performance boost for the expert at the SFT stage, but does not help at the\npreference tuning stage.\n10\n\n3.3.4\nCode\nGenerating and understanding code is a fundamental requirement for any enterprise LLM. We invest in\nthe code capabilities of Command A to assist the software development cycle and improve user coding\nexperience. Command A’s success in code-oriented tasks is also a precise measurement of capability in\ninstruction-following, pragmatic inference in interpreting prompts, and procedural reasoning. Our models\nexcel in challenging business environments, including understanding and translating legacy programs in\nCOBOL, and using SQL to interface with relational databases.\n3.3.4.1\nData\nData Mixture. Our data mix focuses on 8 priority programming languages (Python, Java, C++, Rust,\nGo, JavaScript, TypeScript, COBOL) and 5 dialects of SQL (SQLite, MySQL, PostgreSQL, Microsoft T-\nSQL, Oracle PL/SQL). Across these languages, we target a wide range of tasks including code generation\n(i.e., NL-to-code), code translation, and code optimisation. Within these tasks, we include diverse domains\nsuch as interview-style questions; repository-level queries; and enterprise-specific demands (including high-\ndimensional arrays, complex debugging, data processing, and visualisation).\nPrompts and completions are sourced from annotation campaigns and synthetic generation pipelines. We\nenrich prompt-completion pairs with additional information including execution feedback, explanations, para-\nphrases, stack traces, database context (Chang & Fosler-Lussier, 2023), diff patch formats, and unit-testing\nrequirements. We prioritize candidate data with positive execution-based validation to filter erroneous or\nunverifiable code. This includes passing gold-standard unit tests or correct and error-free execution within\na database. We use a multi-language code execution sandbox to evaluate code correctness in an isolated\nenvironment similar to Guo et al. (2024) and Team et al. (2025).\nDuring pretraining, we perform execution-based code data enrichment. We isolate self-contained functions\nand files and add print statements of some variables and generate synthetically valid input parameters. The\nresulting code is executed in a sandbox and the output is appended to the enriched source code, adding several\nbillion pretraining tokens. There is the added benefit that a subset of code repositories can be formatted as\na very long document where files are linearised following a graph traversal defined by import links.\nIn the RL stage, we jointly optimise for code correctness and annotated preferences between code comple-\ntions. This approach enhances both the functional accuracy of generated code and reduces edit times of\ntechnically correct but suboptimal or dispreferred generations. We quantify performance using the propor-\ntion of unit tests passed in our code execution sandbox, where a reward of 1.0 indicates 100% test success\nand 0.2 represents 20% test success. When no valid code block is detected, we assign a reward of −1.0 to\nexplicitly penalize non-code output. We use synthetic unit-test generation ensuring all code completions have\na minimum of 4 tests per sample. Our synthetic test generation pipeline is similar to Zeng et al. (2025) with\nmore robust unittest tests over assert statements. The preferred SQL completions are canonicalised using\nstatic query analysis. Beyond verifiable metrics, we incorporate DPO-style preference pairs (Rafailov et al.,\n2024) to optimise for code style conventions, documentation structure, and formatting consistency.\nSynthetic Data. We experiment with synthetic data pipelines for post-training data enrichment. As a\nresult, a high proportion of our data are verified synthetic examples in many coding languages. For synthetic\ngeneration sources, we exploit our highest performing models for code, and generalist models for explanations\nand reasoning. We experiment with both novel data synthesis and conditional synthetic data augmentation.\nOur novel data synthesis efforts include generating examples taking inspiration from concepts, similar to\nStarCoder (Wei et al., 2024), and sampling pretraining programming queries. We explore pipelines where\nour synthesis is Python-only followed by translating code and unit tests into additional languages, or direct\ngeneration into any programming language. While the former is useful for generating parallel corpora and\ntargeting Python benchmarks, the latter pipeline proves valuable for problems using absent or uncommon\nfeatures of Python (e.g., multithreading or memory management for C++). We use our execution sandbox to\nverify all synthetic completions — ensuring that any synthetic example teaches a novel skill via verified code.\nThis approach to data synthesis only improves performance for small models (i.e., data-based distillation from\nlarger models). Novel synthesis methods yield negligible improvement for larger models, instead requiring\n11\n\nhuman annotation and synthetic data augmentation to advance our most capable coding experts.\nWe rely on synthetic data augmentation to diversify the style, syntax, and complexity of our code data. Our\ndata augmentation pipeline includes prompt paraphrasing, injecting stricter requirements into prompts for\nmore precise instruction following, and complexifying prompts similar to Magicoder (Wei et al., 2023). In\nverifiable scenarios, we use execution feedback to build data for code repair or translation, where iterative\nfeedback provides guidance until the repaired code passes all tests. In a similar scenario for SQL, a repaired\nor translated SQL is adequate when it returns an equivalent answer from a target language database. This\noffline pipeline can generate prompt-completion pairs, but we also cast this iterative process into multi-turn\ndata to simulate conversational code repair.\nWe also use synthetic augmentation to improve non-verifiable aspects of our data. This includes code ex-\nplanations, markdown style formatting, technical precision, and global completion structure. We use reward\nmodelling and majority voting to score these non-verifiable code completions. We also elicit feedback from\nhuman annotators to guide our data synthesis pipeline towards developer preferences for code style, struc-\nture, and explanations. This regularises against overfitting to the preferred style of any LLM judge, and our\ngenerations’ target style and structural features are actually preferred by human raters.\n3.3.4.2\nTraining\nThe code expert is trained in three stages (hyperparameters and full details in Appendix B.3):\nStage 1 is large-scale supervised learning, with the code data mixture described above. This stage includes\ndata for all relevant tasks to optimise a high level of coding capability. To mitigate variance in initialisation,\nlearning trajectory, and performance on small evaluation datasets we use linear merging over the top k\nseeds (Izmailov et al., 2018; Team et al., 2024; Yadav et al., 2024; Aakanksha et al., 2024; Khalifa et al.,\n2025) where k is typically 2 or 3. We observe that this merged model is a strictly superior initialisation for\ncontinued training with additional fine-tuning or RL.\nStage 2 is supervised learning on only high-quality data. From the first stage fine-tuning, we further\nstrengthen our code expert with additional fine-tuning on our “highest-quality” code generation datasets.\nWe define “high-quality” as verifiable human or synthetic data from our best experts, or data rated highly by\ninternal reward models. As before, we train multiple candidate models and merge across random seeds to pro-\nduce the final SFT code expert. This secondary fine-tuning stage increased our key benchmark performance\nwith negligible regression in tasks only present in stage 1 training (e.g., SQL query optimisation).\nStage 3 is RL over scored or preference-pair data. We train the expert with the offline Contrastive Policy\nGradient algorithm (§3.2.2), to train with execution feedback and DPO-style preference pairs as described\nabove. To ensure stable RL, we introduce three regularisation schemas. First, we repeat the Stage 2 high-\nquality supervised learning and merging process on any non-code expert model (e.g., a merge of multiple\nexperts). CoPG on a merged checkpoint was strictly more stable and yielded better results than RL on top\nof an individual SFT/merge. Second, we introduce a hybrid cross-entropy loss function on top of CoPG to\nsample steps of typical supervised learning from the same Stage 2 data mix. Third, we use WARP-style\nmerging (Ramé et al., 2024) to combine the final model trained with RL to the parent checkpoint. This\nhybrid approach ensures stable reinforcement learning optimisation to improve both our code experts for\nuser preference, and improving our performance on intrinsic code generation capabilities.\n3.3.5\nMath and Reasoning\nSophisticated reasoning abilities are a necessary competency area for generalisation in LLMs (Guo et al.,\n2025; Team et al., 2025; Toshniwal et al., 2024). We focus primarily on core mathematical reasoning as it\nis both intrinsically useful (e.g., in financial use cases) and yields out-of-distribution improvements in other\nknowledge-intensive tasks such as coding and data manipulation (Islam et al., 2023; Shao et al., 2024).\n12\n\n3.3.5.1\nData\nWe find that training on synthetic data outperforms human-written data, so our approach is heavily weighted\ntowards the use of synthetic examples. We use carefully-curated seed prompts for few-shot generation of\nnovel mathematical problems, and LLM-as-judge techniques to determine the correctness of novel problem-\nsolution pairs. We find that the choices of prompt seeds, correctness validation, and final dataset filtering\nhave a substantial impact on the quality of our reasoning-expert models.\n3.3.5.2\nTraining\nSupervised Fine-Tuning. For SFT, we leverage synthetic mathematical reasoning datasets that have\nundergone extensive LLM-driven filtering for solution correctness. We find, similar to\nToshniwal et al.\n(2024), that strict correctness cut-offs are not needed for optimal SFT performance.\nPreference Tuning. We employ preference tuning following SFT across one of two datasets, dependent on\nthe downstream model training stages: The first dataset is comprised of human-rated preferences on paired\ncompletions to reasoning prompts. The second, fully-synthetic dataset comprises correct and incorrect paired\nsolutions to reasoning prompts. We find that, unlike in SFT, solution correctness is of critical importance\nin preference training (e.g., so that preferences are not accidentally inverted), and in the absence of human-\nwritten ratings, we use a mixture of programmatic and model-driven verifiers to evaluate solution correctness.\nMerging. We find that using candidate models exhibiting maximal reasoning performance is sometimes\ndetrimental to the cross-capability merge under particular merging strategies. We observe that optimal\nmerged performance is achieved when first merging various reasoning-tuned and instruction-tuned expert\nmodels, and this yields a sufficiently high-signal proxy for selecting Pareto-optimal candidates to merge with\na broader mix of models downstream. We employ this selection for our final set of candidate models, with\nthe exact selection criteria along the Pareto frontier being dependent on downstream merging strategies.\nTraining hyperparameters for SFT and preference tuning are in Appendix B.4.\n3.3.6\nLong Context\nData. Given the complexity of human annotation for long-context tasks, we synthetically generate long-\ncontext examples. We sample from our long-context pretraining dataset and prompt Command R+ Refresh\nto generate question-answer pairs based on randomly selected fragments within 8,192 tokens (Xiong et al.,\n2024). To ensure high-quality, we use our reward model to select the best generation from a pool of candidates.\nThe selected question-answer pairs are then concatenated to the original samples to construct our synthetic\ndata.\nTraining. We perform one stage of SFT on top of the pretrained model, following a similar approach to our\ncooldown phase. We use an interleaved training regime with datasets of 16k and 256k sequence lengths at a\n3:1 interleaving ratio. Hyperparameters are in Appendix B.5.\n3.3.7\nSafety\nAI Safety focuses on quantifying and mitigating harms that can occur from using a given AI model, either\nto the end user, to the company deploying it, or to society at large. Harms can arise from a single piece of\ngenerated content (e.g. hate speech). They can also be distribution-based, which is the case when the model\nis biased towards certain groups. This section focuses on model safety at the instance level, that is, how we\ndecrease the risks stemming from single generative instances of a given model. We include a distribution-based\nevaluation (§4.6) and consider it to be a form of robustness (Seshadri & Goldfarb-Tarrant, 2025).\nCohere’s core Safety behaviour. We focus on practical safety considerations, driven both by model\ncapabilities and deployment use cases. We consider two main settings in which our models can be deployed:\n• The Default setting, in which the model is used entirely outside of Cohere (e.g. open weights release).\nIn this scenario, we lack control of the preamble structure and deployment context. We ensure that\nthe model behaves according to Cohere’s Core Safety behaviour in this general setting.\n• The Enterprise setting, in which the model is deployed by Cohere to one of our enterprise partners.\n13\n\nHere the safety behaviour of the model is controllable by the preamble, to meet different enterprise\nneeds exceeding Core Safety behaviour. The controllable behaviours are called \"Safety modes\". There\nare currently two safety modes; contextual, and strict.\nOur Core Safety behaviour focuses of five key areas where we want to prevent the purposeful propagation of\nharmful content online: Violence and hate, Misinformation, Self-harm, Child Sexual Exploitation and Abuse\n(CSEA) and Sexual content. In the default setting, we expect the model to be able to answer requests for\ninformation on those topics (covering factual elements such as statistics, educational content); however it\nshould not generate any unsafe content, that is, supporting, encouraging or otherwise enabling harm. In the\nenterprise setting, the contextual mode is similar, but allows sexual content. The model behaviour can be\nmade stricter by using the strict mode, which prevents the model from covering any topic related to our key\nfocus areas, as well as from generating profanity.\n3.3.7.1\nData\nPretraining. We perform two stages of safety-related pretraining filtering: first, we remove known domains\nfor CSEA and sexual content, and second, we use a classifier to remove generally low quality content, including\nsexual content.\nPost-training. In post-training, we use both SFT and preference datasets, with a combination of manual and\nautomated labels. Safety annotation is performed by internal annotators and specialist external vendors, who\nare specifically trained for our Safety concepts and tasks. Our close interaction with internal Safety annotators\nprovides additional benefits due to the potentially distressing nature of the data. We increase the diversity\nof our post-training data via both LLM ‘personas’ and LLM-based reformulations. We generate completions\ncorresponding to different styles, identities and belief systems via diverse LLM personas. Additionally, we\nuse our LLM to reformulate content (preserving overall semantics but changing form), thus increasing data\ndiversity and making sure that the preferred completions are consistent with our refusal policy (in particular,\nthe model should not apologise for refusing to generate unsafe content, which creates a common dataset\nartifact (Chen & Goldfarb-Tarrant, 2025)).\nBalancing safety and refusal behaviour. Ensuring that the model cannot produce harmful content\nmeans that a lot of training data shows refusal as the preferred behaviour. It is crucial to balance such data\npoints with authorised user requests and compliant completions to prevent the model from over-relying on\nrefusal – as previously referred to in the literature as the balance between harmlessness and helpfulness (Bai\net al., 2022). The balancing prompts can be split into two sets: user requests which are information requests\non safety topics, and benign user requests with similar vocabulary and structure as unsafe prompts.\n3.3.7.2\nTraining\nImproving overall model safety means finding a fine balance between over- and under-refusal. We find it\ncrucial to split datasets in two: namely in their safety-increasing (where the model should refuse) and\nhelpfulness-inducing (where the model should answer) components. This allows us to balance these aspects\ndifferently during training. We use both SFT and offline preference tuning. We find offline preference tuning\ncrucial in limiting over-refusal, however, it is less efficient than SFT at making the model safer. We observe\nthis behaviour both on 8B models and 111B models, with the main difference between the two regimes being\nthe effect of regularisation, with larger models more prone to overfitting. Overall, the biggest impact on our\nmodel’s ability to respond safely and helpfully is achieved in the polishing process described in Section 3.5.\nThe Safety expert differs from other experts in that during the preference tuning stage we combine an offline\npreference loss with an equally weighted SFT loss. Preference tuning focuses on reinforcing helpfulness via\nhelpfulness preference pairs, while SFT focuses on reinforcing safety via safety-inducing data. We find that\nIPO and DPO perform similarly, with SLiC showing a worse trade-off between over- and under-refusal, so\nwe use IPO. Full details on SFT and preference hyperparameters are in Appendix B.6.\n14\n\n3.4\nMerging\n3.4.1\nDefinition\nModel merging refers to the process of combining a set of model parameters θi for i ∈[1, K], into a single\ncombined model θmerged = f(θ1, ..., θK), where f(·) is some merging function. The merging function can\nrange in complexity from simple averaging (Izmailov et al., 2018; Wortsman et al., 2022; Li et al., 2022) to\nmethods based on Fisher information (Matena & Raffel, 2022) and sign agreement between models (Yadav\net al., 2023; Yu et al., 2024). Model merging produces a single set of model parameters, resulting in faster\ninference than ensembling and lower memory requirements than runtime query routing.\nExpert 1 \n(Model Checkpoint )\nExpert 2 \n(Model Checkpoint )\nMerged Model\nCapability 1\nCapability 2\nCapability 1\nCapability 2\nCapability 1\nCapability 2\n+\n=\nFigure 4: Model merging allows teams to build domain expert models that excel at different capabilities\nindependently. These experts are merged into a single model that retains close-to-expert capability levels\nacross multiple domains or capabilities.\n3.4.2\nMerging Taxonomy\nWe here list the different merging techniques, each using different models and having different goals.\nExpert merging. Expert merging refers to the process of combining a set of models with different capabili-\nties to produce a single monolithic model with those capabilities. In this setting, the input models will likely\nbe trained on various datasets and exhibit performance along a single domain only. The aim is to produce\na single set of parameters that preserves as much of the individual ‘expert’ performance as possible. Expert\nmerging is a core feature of the Command A training pipeline, and we describe it in more detail in §3.4.3.\nMerging as Polyak averaging. Merging may be used to achieve a form of Polyak averaging (Ruppert,\n1988; Polyak & Juditsky, 1992). Here, the input models are checkpoints from different points along a single\ntraining run, and merging acts as a smoothing operation that reduces the effects of noise inherent in stochastic\ngradient-based optimisation.\nSeed merging. Merging may also reduce the effects of random seeds (e.g., for initialization or data loader\nordering). Merging the final checkpoints from multiple equivalent training runs with different random seeds\ncan reduce the risk of overfitting and lead to a more robust model.\nInterpolation for capability recovery. We observe multiple instances of capability forgetting Kirkpatrick\net al. (2017), whereby training an expert on one capability degrades performance on other capabilities. This\nis a particular issue for long-context abilities since experts are generally trained on top of a long-context\ncapable model but with training schemes that use short context lengths. In this situation, merging an expert\nwith the original base model can recover a significant proportion of the original capability while retaining\nthe new expert capability. This setting is closely related to the WARP approach (Ramé et al., 2024).\n15\n\n3.4.3\nExpert Merging\nThe overall goal for an enterprise-ready LLM is a single monolithic model, with multiple capabilities. These\ncapabilities can sometimes be orthogonal (e.g., code and safety competencies have very different data distri-\nbutions) and may involve different scales of training data. For example, it is more straightforward to generate\nhigh volumes of synthetic data in more easily verifiable domains, such as code and reasoning, compared to\ndomains like safety or RAG, where human-annotated data is more prevalent. These differences introduce\ntechnical and operational challenges: how can we enable asynchronous development of model capabilities, and\njointly optimise for a range of capabilities with highly varied training dynamics?\nModel merging enables multiple teams to work asynchronously on improving different capabilities, with\ntheir contributions merged together in parameter space. The capabilities exhibited by Command A cover\na wide range of data scales, that would be non-trivial to combine into a single dataset and optimisation\nstrategy. Merging allows each team to separately optimise hyperparameters and data scale for peak per-\nformance on their capability of interest. Our final model was informed by 500 separate evaluation metrics,\nwhich would have been significantly less practical in a more centralised organisational structure. Merging is\ncomputationally cheap, allowing us to quickly and easily rebalance the capabilities of the final model.\nWe apply merging at two points in the overall training pipeline: firstly, to combine a set of expert models\ntrained using SFT into an initial ‘SFT model soup’; secondly, to combine a set of experts that were trained\nusing offline preference optimisation techniques on top of the SFT soup, giving an ‘off-pref soup’. At both\nstages, our aim is to jointly maintain as high a proportion of the expert capability as possible while also\nallowing for rebalancing of the overall capabilities of the final model.\n3.4.3.1\nLinear merging is simple but effective\nWe employ linear merging (also known as weight averaging), with weights chosen by manual search. We find\nthat, broadly speaking, the interaction between expert weights and resulting model performance is fairly\nintuitive; increasing the weight of a domain expert is likely to increase the performance in that domain.\nHowever, this relationship is not perfect, and the corresponding degradation in performance of other (implic-\nitly downweighted2) domains is much less predictable. We therefore search across merging weights using a\ncombination of heuristics (i.e., upweight experts for domains in which a merge candidate is underperforming)\nand brute force search (i.e., perturb the weights for each expert, centred around the current candidate). We\nexperimented with more complex merging methods (e.g., SLERP (Ramé et al., 2024) and task vectors (Il-\nharco et al., 2023)) but found no significant performance improvements, at the cost of increased complexity.\nIn addition, linear merging is associative, meaning that a linear merge of linear merges can be expressed as\na single merge operation, improving the interpretability of a complex training pipeline.\n3.4.3.2\nConsistency is more important than optimality\nAll expert models are initialised from a common ‘general instruction following’ model, for two reasons.\nFirstly, some domain experts make use of special tokens (e.g., tool calls) whose embeddings otherwise remain\nuntrained. We find that selectively merging these embeddings only from checkpoints where they are trained\nis beneficial, but suboptimal. Using a shared generalised instruction-following model as initialisation for\neach expert and merging the special token embeddings as normal performs much better, even though these\nembeddings are likely to be lower quality. Secondly, we find that the post-training process generally degrades\nlong-context performance, and that this is challenging to recover. Starting from a generalised model that is\n‘long-context capable’ preserves long-context performance more easily throughout the training pipeline.\nWe find it valuable to include ‘leave-one-out’ merges as part of the search process, to reveal instances where\none expert model causes performance degradation of others, or ‘collisions’. To address this, we include a small\namount of cross-domain data in each expert’s training, to act as a regulariser and ensure that each expert\nremains ‘compatible’ with the other experts. We also observe that collisions can be caused by small incon-\nsistencies in the style or formatting of the common data between experts. In combination this implies that\nmaintaining some consistency between expert models is more important than absolute expert performance.\n2For linear merging, the weights must sum to 1. Increasing the weight of one expert therefore requires reducing the weight\nof one or more of the other experts.\n16\n\n3.4.3.3\nMerging is cheap, evaluation is expensive\nWe note that most prior work on model merging assumes that the set of input experts is fixed, and seeks to\nfind a single merging method that optimises some value function, generally a single metric or small number\nof metrics (e.g., Wang et al., 2024a). These methods often involve a large number of hyperparameters\n(Ilharco et al., 2023) or extensive search over merge weights (Khalifa et al., 2025). By contrast, our goal is\nto optimise for a wide range of capabilities and many metrics. This introduces a further challenge generally\nnot acknowledged by the literature; while model merging is cheap and fast, evaluating each merge requires\nsignificant inference time and compute. Evaluation is therefore a significant bottleneck when applying model\nmerging in a production context. The set of input models is also not fixed, and a significant portion of the\neffort towards successful merging involves making changes to the training scheme used by the experts.\n3.5\nPolishing\nModel merging provides a powerful mechanism for combining a diverse set of experts into a single model.\nHowever, combining experts trained to target specific capabilities does not guarantee the final model’s\nalignment with human preferences. To address this, we introduce a polishing phase as the final post-training\nstep. This phase serves two critical purposes: fixing any artifacts introduced during model merging and\naligning the final model with human preferences.\nUnlike other specific capabilities such as coding or instruction-following, human alignment has a cross-\ndomain effect and influences every aspect of the model’s behaviour. The polishing phase ensures that the\nmodel adheres to human expectations, including tone and style, without sacrificing technical competence.\nPolishing is divided into three steps. First, we apply SFT on a subset of our highest quality datasets. Second,\nwe apply offline preference tuning, and finally, we apply online Reinforcement Learning from Human Feedback\n(RLHF). We find that ping-pong-style interleaving of offline and online preference learning helps improve\nalignment with human preferences while avoiding regressions and mitigating reward model hacking.\nSupervised Fine-Tuning (SFT). We employ a best-of-N SFT approach (Stiennon et al., 2020) where we\nsynthetically generate four candidate completions for each prompt. We leverage our reward model (§3.2.3)\ntrained on human preference data to rank these completions. We then apply SFT using the highest-ranked\ncompletions, ensuring that the model learns from the most highly rewarded responses.\nPreference Tuning. We use offline preference training to align our model with human preferences. We\nselect completions with the highest reward scores as preferred completions, and use the completions with\nthe lowest reward scores as dis-preferred. Additionally, we refine the dataset by filtering out prompts ex-\nhibiting a low average reward. To further improve the model’s proficiency in instruction-following, mathe-\nmatical reasoning, and coding, we incorporate domain-specific preference data into our training mixture. For\ninstruction-following, completions that correctly adhere to all instructions are considered preferred, while\ncompletions failing to meet all instruction criterion are labelled as dis-preferred. To construct preference\ndata for mathematical reasoning, we categorise completions that yield correct answers as preferred and those\nfailing to produce accurate solutions as dis-preferred. Similarly, for code generation tasks, code snippets\npassing all unit tests serve as preferred completions, while those failing the tests are used as dis-preferred\ncompletions. We also filter these preference datasets by removing samples for which the preferred completions\nare assigned a lower score than the dis-preferred completions by our reward model. We rely again on the\nSRPO loss due to its robustness and its self-refinement abilities (§3.2.2.1). In our implementation of SRPO,\nfollowing Grinsztajn et al. (2024), we average the log-likelihoods of preferred and dispreferred completions\nto control for variations in the completion length.\nReinforcement Learning from Human Feedback (RLHF). To enhance the alignment of our model\nwith human preferences, we further employ Reinforcement Learning from Human Feedback (RLHF). We\nuse online CoPG (§3.2.2.2) with two generations per prompt. The prompts used for RLHF training are\nderived from a subset of those previously used during SFT, including reasoning, multilingual, coding, and\npreference-based tasks prompts. We regularize training using an auxiliary L2 loss with the reference policy,\nand an SFT loss using a high-quality subset of post-training data.\n17\n\nArea\nBenchmarks\nAcademic, General Knowledge and\nInstruction Following (§4.1)\nMMLU; MMLU-Pro; GPQA; IFEval; InFoBench\nAgents and Tool-Use (§4.2)\nTauBench; BFCL.\nMultilingual (§4.3)\nMMMLU; NTREX; FLoReS; MGSM; mArenaHard (LLM-as-a-Judge); Language\nConfusion Benchmark; Al-Qasida; INCLUDE 44; mTauBench.\nCode (§4.4)\nLBPP; HumanEvalPack; MBPP+; Spider; Bird SQL; RepoQA; LiveCodeBench; Big-\nCodeBench; SWE-Bench Diff Generation; Aider Polyglot; internal datasets.\nMath and Reasoning (§4.5)\nMATH; AIME; LiveBenchMath; Waterloo; OpenQuant; FinanceBench; OmniMath.\nSafety (§4.6)\nXSTest; internal datasets.\nLong-Context (§4.8)\nNeedle-in-a-Haystack; RULER; RulerQA.\nTable 2: Benchmark datasets used to evaluate Command A models, grouped by area.\n4\nResults\nWe report results from a diverse and extensive set of evaluations benchmarking the performance of Command\nA and Command R7B. We evaluate a broad range of capabilities using public academic datasets and internal\nevaluations. Table 2 gives an overview of the capability areas we focus on and the corresponding benchmarks.\nWe present a snapshot of results on a representative subset of these evaluations in Table 1 opening this report.\nFull details for each dataset are available in the corresponding sections.\nWe compare our models against open and closed models in similar parameter count ranges. Wherever possible,\nwe show externally reported results with comparable evaluation settings. Where these are not available, we\nattempt to internally reproduce these results as faithfully as possible given the information provided publicly.\n4.1\nStandard Benchmarks\nWhile our primary aim is to build a highly performant model for enterprise use cases (§4.7), we also measure\nperformance on standard academic datasets to evaluate baseline model knowledge and capabilities. Where\napplicable (MMLU, MMLU-Pro, GPQA), we follow the simple-evals implementation, including data, task\nsettings, prompting, and answer parsing. More details can be found in Appendix B.7.\nModel\nMMLU\nMMLU-Pro\nGPQA\nIFEval\nInFoBench\nCommand A\n85.5\n69.6\n50.8\n90.9\n94.9\nGPT-4o\n89.2\n77.9\n53.6\n83.8\n94.0\nDeepSeek V3\n88.5\n75.9\n59.1\n86.1∗\n94.3\nLlama 3.3 70B Instruct\n86.0\n66.0\n50.5\n92.1\n92.8\nLlama 3.1 405B Instruct\n88.6\n73.0\n49.0\n88.6\n93.9\nMistral Large 2\n85.2\n67.9\n48.6\n83.8\n93.3\nClaude 3.5 Sonnet\n89.5\n78.0\n65.0\n90.2\n93.9\nGemini 2.0 Pro\n89.3\n79.1\n64.7\n87.3\n92.2\nCommand R7B\n65.2\n42.4\n26.3\n77.9\n85.6\nLlama 3.1 8B Instruct\n71.1\n46.5\n23.4\n78.6\n90.1\nMinistral 8B\n71.1\n43.0\n23.4\n59.0\n88.3\nGemma 2 9B Instruct\n73.5\n50.6\n31.3\n74.4\n87.2\nGemini 1.5 Flash-8B\n74.8\n48.4\n31.6\n88.0\n88.3\nTable 3: Results for Command A and Command R7B on standard academic benchmarks. ∗Note that for\nIFEval, Liu et al. (2024a) report only the prompt-level strict accuracy. We report the average of the prompt-\nand instruction-level strict accuracies for all other models (see Appendix B.7).\nWe note that academic benchmarks have various limitations such as saturation, bias and alignment to\nreal-world performance (Kiela et al., 2021). Human assessment of model capabilities can be undesirably\n18\n\ninfluenced by confounders (Hosking et al., 2024), be subject to idiosyncratic, conversational and demographic\nvariance (Kirk et al., 2024), and demonstrates imperfect correlation to academic benchmarks (Schaeffer et al.,\n2025). Enterprise-relevant capabilities are often not well-represented in these benchmarks, so we augment our\nevaluations with enterprise-oriented signal (e.g. §4.2, §4.7), and human annotation based evaluation (§4.11).\nTable 3 shows results on these selected benchmarks. Command A is competitive across all benchmarks,\ngenerally outperforming similarly-sized models while remaining competitive with considerably larger and\nless-efficient models. On the instruction-following benchmarks, we observe that Command A performs com-\npetitively across both IFEval and InFoBench. Specifically, it outperforms all similarly sized models on In-\nFoBench and is outperformed only by Llama 3.3 70B Instruct on IFEval. We also note that Command A\nrepresents a substantial improvement over our previous Command R+ Refresh model.\n4.2\nAgentic Tool Use\nModel\nChatRAGBench\nStrategyQA\nBamboogle\nDROP\nHotPotQA\nCommand A\n72.9\n76.7\n76.0\n91.1\n92.1\nGPT-4o\n66.6\n81.2\n76.0\n89.5\n92.1\nDeepSeek V3\n40.3\n73.8\n70.4\n85.7\n90.1\nTable 4: Standard RAG evaluations. Correctness is determined following the procedure in Verga et al.\n(2024) where a panel of LLMs judges the model’s generation against a reference answer.\nModel\nBFCL Overall\nLive AST\nMulti-turn\nCommand A\n63.8\n80.5\n25.5\nLlama 3.3 70B Instruct\n51.4\n62.8\n6.9\nMistral Large 2\n58.5\n69.9\n23.8\nQwen 2.5 72B Instruct\n63.5\n79.0\n24.6\nClaude 3.5 Sonnet\n56.5\n78.9\n41.0\nClaude 3.7 Sonnet\n58.3\n78.4\n48.4\nDeepSeek V3\n58.6\n68.4\n18.6\nGPT-4o\n72.1\n79.8\n47.6\nCommand R7B\n52.2\n69.2\n5.0\nLlama 3.1 8B Instruct\n50.9\n61.1\n9.6\nGemma 2 9B Instruct\n51.6\n68.0\n1.6\nMinistral 8B\n51.8\n64.9\n11.4\nQwen 2.5 7B Instruct\n53.7\n67.4\n7.6\nTable 5: BFCL Results. All numbers taken from official leaderboard. Where leaderboard entries exist for\nboth function calling and prompted, we take the larger of the two reported values.\nModel\nTaubench Retail\nTaubench Airline\nP@1\nP@2\nP@3\nP@4\nP@1\nP@2\nP@3\nP@4\nCommand A\n60.0\n49.8\n44.1\n40.4\n45.3\n36.9\n32.2\n29.0\nLlama 3.3 70B Instruct\n6.2\n5.7\n5.49\n5.3\n35.3\n33.6\n32.4\n31.5\nMistral Large 2\n53.3\n37.8\n29.0\n23.1\n27.2\n14.2\n9.4\n7.1\nLlama 3.1 405B Instruct\n29.1\n17.5\n12.8\n10.4\n26.0\n17.3\n13.5\n12.0\nDeepSeek V3\n54.8\n41.2\n34.1\n30.4\n25.5\n14.0\n12.0\n12.0\nGPT-4o\n60.6\n49.0\n42.4\n37.7\n43.0\n31.8\n26.3\n22.3\nClaude 3.5 Sonnet\n69.2\n57.6\n50.9\n46.2\n46.0\n32.6\n26.3\n22.5\nTable 6: Taubench Results. We follow the original experimental setup from Yao et al. (2024). Pass@k (P@k)\nevaluates a model’s consistency; for example, Pass@4 is the probability that a model answers the same\nquestion correctly 4 times. Scores are aggregated over 10 runs.\n.\n19\n\nStandard RAG Benchmarks. We evaluate on several RAG benchmarks that test the model’s ability to an-\nswer questions conditioned on source documents. In DROP (Dua et al., 2019) and HotPotQA-distractor (Yang\net al., 2018), the model is given a question and set of pre-retrieved relevant documents. Bamboogle (Press\net al., 2022) and StrategyQA (Geva et al., 2021) are multi-hop question answering datasets where models\nmust submit one or more sequential or parallel queries to a search engine to gather documents and arrive at\nthe answer. Finally, we show results averaged over the ten datasets in ChatRAGBench (Liu et al., 2024d)\nthat cover a variety of domains situated in a multi-turn conversation. Results are shown in Table 4.\nBerkeley Function-Calling Leaderboard (BFCL). BFCL is one of the most widely used evaluations of\nLLM tool use / function calling capabilities and maintains an independently run leaderboard (Yan et al.,\n2024). Evaluations include simple single step tool calls, measures of tool irrelevance, and a multi-turn subset\nwhich simulates much harder scenarios over long action trajectories. Results are shown in Table 5.\nTaubench. Taubench is a complex agentic tool-use benchmark that simulates a customer support agent in\ntwo settings: airline and retail (Yao et al., 2024). The agent model has access to a set of tools for reading\nand writing to a provided database and must help a simulated user in accomplishing a given task such as\nchanging flight or returning a product order. Results are shown in Table 6.\n4.3\nMultilingual\nCommand A supports 23 key languages of global business: English, French, Spanish, Italian, German, Por-\ntuguese, Japanese, Korean, Arabic, Chinese, Russian, Polish, Turkish, Vietnamese, Dutch, Czech, Indone-\nsian, Ukrainian, Romanian, Greek, Hindi, Hebrew, and Persian. We evaluate performance on many of these\nlanguages (and beyond) on both academic and internal enterprise focused benchmarks, as well as public\nbenchmarks important for business use such as language consistency and steerability, and dialect awareness.\nWe assess the general multilingual capability of Command A through machine translation via NTREX-\n128 (Federmann et al., 2022), which contains human translated news domain documents, FLORES-200 (Team\net al., 2022; Goyal et al., 2022); and multilingual mathematical reasoning (MGSM; Shi et al. (2022)). We\nfurther evaluate Command A’s understanding of regional contexts through INCLUDE (Romanou et al.,\n2025), a large-scale region-specific evaluation suite in 44 languages.\nResults for machine translation on NTREX are shown in Table 7. We use the COMET-20 metric (Rei et al.,\n2020), one of the top performing MT metrics (Freitag et al., 2023). Rather than mark single winning models,\nwe mark winning clusters of models by taking into account the effect size of the metric. A model is in a\nwinning cluster if its score difference to the best model is smaller than 1.67 points. This threshold equates\nto 75% agreement with humans (Kocmi et al., 2024): humans will agree with automatic metric on 3 out of\n4 pairwise system comparisons that have a difference of 1.67 COMET-20. Further academic results (MGSM\nand INCLUDE-44) are in Appendix B.2.\nTo evaluate more general and diverse capabilities, we ran an LLM-as-judge arena-like evaluation of responses\nto mArenaHard, a dataset of 500 challenging queries from Chatbot Arena, originally in English, translated\ninto 23 languages (Dang et al., 2024). As shown in Table 8, Command A is preferred across all 23 languages\nversus Llama 3.3 70B Instruct, Llama 3.1 405B Instruct, and DeepSeek V3.\nWe also conduct a human-annotated arena-like evaluation. Figure 5 shows the results of an internal evaluation\nset consisting of 100 translated prompts 3 from English that focus on instruction-following ability. Command\nA performs favourably in multilingual head-to-head human evaluations against comparable models across 9\npriority languages. Command A outperforms the Llama 3.3 70B Instruct and Llama 3.1 405B Instruct across\nall evaluated languages. Versus DeepSeek V3 and Mistral Large 2, Command A is favoured across 8 of 9\nlanguages. Notably, Command A is favoured in Chinese compared to DeepSeek V3 and is favoured in French\ncompared to Mistral Large 2. It is also favoured against GPT-4o on Arabic and Korean, and is competitive\non Spanish, German, Italian, and Chinese.\n3Models commonly had issues generating completions for one of the prompts across different languages, the win/tie/loss\nrates for these are based on 99 prompt-completion pairs\n20\n\nar\ncs\nde\nel\nes\nfa\nfr\nhe\nhi\nid\nit\nja\nko\nnl\npl\npt\nro\nru\ntr\nuk\nvi\nzh\nAvg.\nFLORES\nCommand A\n60.0\n84.3\n60.2\n79.5\n74.2\n56.3\n67.2\n66.2\n67.8\n78.7\n74.2\n60.6\n66.8\n63.2\n73.3\n74.7\n74.1\n64.2\n80.1\n68.3\n63.7\n56.3\n68.8\n81.2\nGPT-4o\n60.8\n85.9\n61.3\n83.0\n74.7\n57.9\n68.6\n70.8\n70.7\n82.2\n76.1\n64.1\n68.9\n64.5\n75.4\n76.8\n76.8\n65.0\n83.9\n70.7\n66.1\n56.9\n71.0\n83.0\nGemini 2.0 Flash\n58.9\n85.1\n61.7\n82.2\n75.0\n57.6\n67.9\n67.3\n71.5\n81.6\n75.4\n62.9\n66.9\n64.6\n75.2\n75.8\n76.8\n65.9\n84.8\n71.5\n65.9\n56.4\n70.5\n82.8\nGemini 1.5 Pro\n58.4\n85.3\n61.3\n83.1\n74.1\n57.2\n68.0\n68.8\n71.4\n81.2\n75.9\n61.6\n65.2\n63.8\n75.7\n76.4\n77.6\n65.7\n84.2\n72.2\n66.0\n56.5\n70.4\n82.8\nClaude 3.7 Sonnet\n59.5\n86.1\n61.5\n80.7\n73.0\n55.9\n67.3\n70.2\n69.6\n81.2\n75.7\n63.6\n69.0\n64.0\n75.3\n75.3\n75.4\n65.5\n82.6\n71.6\n65.5\n56.8\n70.2\n82.7\nDeepSeek V3\n59.8\n85.0\n61.0\n76.7\n73.3\n55.3\n67.6\n68.3\n70.9\n81.6\n74.8\n64.4\n68.2\n63.8\n74.3\n76.0\n74.6\n64.9\n83.0\n69.0\n65.8\n58.5\n69.8\n76.3\nLlama 3.1 405B Instruct\n52.0\n81.3\n59.0\n71.5\n71.4\n49.1\n64.3\n63.9\n64.8\n78.5\n73.3\n59.8\n63.3\n62.9\n70.3\n72.3\n73.2\n60.9\n77.7\n62.7\n60.2\n54.3\n65.8\n79.1\nMistral Large 2\n52.1\n77.7\n59.4\n69.7\n71.4\n45.5\n65.7\n63.2\n61.3\n73.3\n73.8\n58.9\n63.2\n59.2\n67.9\n74.1\n69.7\n60.7\n68.5\n63.8\n58.3\n53.2\n64.1\n77.1\nLlama 3.3 70B Instruct\n45.2\n76.1\n57.6\n64.7\n69.1\n43.6\n62.7\n59.9\n63.4\n75.6\n70.7\n57.0\n57.3\n61.4\n67.7\n70.9\n70.1\n58.1\n70.5\n61.2\n58.3\n53.5\n62.5\n75.8\nQwen 2.5 72B Instruct Turbo\n48.3\n70.8\n56.0\n41.8\n70.2\n28.8\n63.4\n19.0\n49.6\n74.3\n69.5\n57.7\n51.8\n57.4\n60.2\n73.6\n56.4\n58.3\n65.0\n49.6\n55.9\n54.5\n56.0\n69.8\nCommand R7B\n50.6\n61.8\n54.7\n30.2\n69.2\n32.8\n61.4\n-15.1\n35.9\n58.5\n68.4\n50.7\n52.2\n48.4\n46.6\n69.1\n58.4\n39.8\n50.1\n40.7\n46.0\n47.6\n48.1\n58.6\nGemini 1.5 Flash-8B\n47.5\n76.0\n57.1\n73.0\n71.3\n47.7\n64.3\n51.6\n64.8\n78.0\n71.2\n56.1\n60.2\n59.5\n67.6\n72.3\n70.2\n59.8\n75.1\n63.6\n61.5\n50.9\n63.6\n77.0\nClaude 3 Haiku\n46.4\n79.2\n56.8\n62.9\n68.4\n44.2\n62.1\n50.1\n59.0\n76.9\n69.7\n53.9\n63.7\n59.2\n66.5\n70.7\n67.9\n59.5\n73.9\n63.7\n58.6\n51.6\n62.0\n76.3\nGemma 2 9B Instruct\n42.2\n73.0\n56.3\n61.4\n70.2\n40.3\n62.4\n36.0\n60.0\n76.3\n69.9\n54.5\n53.8\n57.2\n66.3\n72.0\n66.4\n56.9\n70.4\n60.2\n57.5\n51.0\n59.7\n72.6\nLlama 3.1 8B Instruct\n26.7\n61.2\n50.1\n40.8\n63.8\n27.1\n54.7\n30.2\n46.9\n67.7\n63.9\n42.5\n41.0\n52.6\n53.6\n64.3\n57.5\n47.0\n48.8\n47.0\n51.1\n44.6\n49.2\n63.8\nMinistral 8B\n-12.2\n42.5\n52.0\n34.9\n65.4\n-10.3\n56.9\n9.2\n18.0\n55.5\n65.2\n38.2\n34.6\n44.8\n31.4\n67.4\n34.5\n48.1\n23.3\n38.7\n33.6\n38.6\n36.8\n52.3\nQwen 2.5 7B Instruct Turbo\n2.7\n29.5\n39.0\n-37.5\n59.6\n-34.5\n49.5\n-68.4\n-12.9\n56.1\n54.4\n32.7\n11.3\n38.0\n20.1\n61.2\n15.3\n26.2\n17.1\n-5.3\n39.1\n47.0\n20.0\n33.7\nTable 7: Machine translation (COMET-20) scores on NTREX. Average over FLORES (COMET-20) is also shown. System scores in the\nwinning cluster are bold per language.\nAvg.\nar\ncs\nde\nel\nen\nes\nfa\nfr\nhe\nhi\nid\nit\nja\nko\nnl\npl\npt\nro\nru\ntr\nuk\nvi\nzh\nCommand A vs. Llama 3.3 70B Instruct\n78.9\n81.2\n77.3\n81.4\n78.7\n72.8\n76.4\n81.6\n76.1\n84.0\n81.9\n81.6\n74.9\n84.9\n84.6\n78.8\n79.9\n76.7\n77.0\n69.7\n79.0\n80.0\n82.6\n74.4\nCommand A vs. Llama 3.1 405B Instruct\n78.7\n80.4\n77.4\n81.2\n78.7\n73.6\n82.7\n78.4\n79.3\n85.2\n81.1\n81.7\n81.2\n82.7\n79.3\n76.3\n77.6\n82.5\n78.0\n73.6\n70.6\n72.1\n78.2\n78.8\nCommand A vs. Mistral Large 2\n65.9\n68.4\n64.8\n64.4\n68.5\n64.1\n63.8\n68.6\n61.0\n68.0\n65.6\n66.8\n62.9\n66.6\n71.2\n64.8\n64.9\n61.7\n65.5\n64.2\n67.6\n64.5\n70.2\n66.9\nCommand A vs. DeepSeek V3\n53.4\n54.5\n54.5\n55.9\n54.5\n52.5\n52.7\n52.7\n53.7\n54.0\n51.6\n53.9\n51.8\n56.4\n53.2\n52.7\n50.2\n54.2\n51.9\n52.0\n50.4\n55.2\n55.7\n54.7\nTable 8: Command A mArenaHard winrates on 23 languages against open-weights models.\n21\n\n0.0\n20.0\n40.0\n60.0\n80.0\n100.0\nJA\n47\n5\n48\nAR\n65.66\n1.01\n33.33\nKO\n61\n2\n2\n37\nPT\n70\n28\n2\nES\n70\n29\n1\nFR\n62\n3\n35\nDE\n64.65\n2.02\n33.33\nIT\n56\n3\n41\nZH\n58\n0\n42\nCommand A Win Rates\nTie Rates\nLoss Rates\n50% Threshold\nCommand A vs DeepSeek V3\n0.0\n20.0\n40.0\n60.0\n80.0\n100.0\nJA\n47.5\n7.1\n45.5\nAR\n48.0\n3.0\n49.0\nKO\n53.5\n0.0\n46.5\nPT\n53.0\n46.0\n1.0\nES\n58.0\n40.0\n2.0\nFR\n41.0\n1.0\n58.0\nDE\n56.0\n1.0\n43.0\nIT\n53.0\n3.0\n44.0\nZH\n55.6\n44.4\n0.0\nCommand A Win Rates\nTie Rates\nLoss Rates\n50% Threshold\nCommand A vs Gemini 2.0 Pro-exp\n0.0\n20.0\n40.0\n60.0\n80.0\n100.0\nJA\n69\n2\n29\nAR\n75\n3\n22\nKO\n76\n1\n23\nPT\n58\n40\n2\nES\n58.59\n33.33\n8.08\nFR\n67\n1\n32\nDE\n60\n2\n38\nIT\n63.64\n2.02\n34.34\nZH\n70.71\n28.28\n1.01\nCommand A Win Rates\nTie Rates\nLoss Rates\n50% Threshold\nCommand A vs Llama 3.3 70B\n0.0\n20.0\n40.0\n60.0\n80.0\n100.0\nJA\n64\n6\n30\nAR\n69.23\n3.85\n26.92\nKO\n73.21\n0\n26.79\nPT\n65\n34\n1\nES\n58.59\n35.35\n6.06\nFR\n58\n0\n42\nDE\n69\n1\n30\nIT\n58.59\n3.03\n38.38\nZH\n55\n44\n1\nCommand A Win Rates\nTie Rates\nLoss Rates\n50% Threshold\nCommand A vs Llama 3.1 405B\n0.0\n20.0\n40.0\n60.0\n80.0\n100.0\nJA\n55\n2\n43\nAR\n67.68\n2.02\n30.03\nKO\n62\n5\n33\nPT\n59.6\n39.39\n1.01\nES\n48\n49\n3\nFR\n55\n2\n43\nDE\n55.56\n2.02\n42.42\nIT\n54.55\n0\n45.45\nZH\n67.68\n30.03\n2.02\nCommand A Win Rates\nTie Rates\nLoss Rates\n50% Threshold\nCommand A vs Mistral Large\n0.0\n20.0\n40.0\n60.0\n80.0\n100.0\nJA\n37\n5\n58\nAR\n51\n0\n49\nKO\n51\n0\n49\nPT\n39\n57\n4\nES\n44\n44\n12\nFR\n41.41\n5.05\n53.54\nDE\n47\n2\n51\nIT\n46\n2\n52\nZH\n47\n53\n0\nCommand A Win Rates\nTie Rates\nLoss Rates\n50% Threshold\nCommand A vs GPT-4o (Nov)\nFigure 5: Head-to-head human evaluations against comparable models.\n22\n\nMTaubench Retail\nMTaubench Airline\nAvg.\nen\nfr\nar\nja\nko\nAvg.\nen\nfr\nar\nja\nko\nCommand A\n34.3\n60.0\n36.5\n28.5\n24.4\n22.0\n43.4\n45.3\n52.7\n47.3\n38.0\n33.8\nGPT-4o\n37.3\n59.7\n41.7\n29.6\n28.7\n26.7\n45.2\n47.3\n38.7\n50.7\n41.3\n48.0\nGemini 1.5 Pro\n26.4\n49.0\n28.4\n20.3\n16.2\n18.8\n41.4\n31.7\n36.9\n46.0\n47.6\n45.0\nGemini 2.0 Flash\n26.0\n44.1\n29.6\n20.6\n17.1\n18.8\n33.7\n35.3\n36.0\n34.0\n29.3\n34.0\nMistral Large 2\n25.8\n54.1\n30.0\n18.7\n11.8\n14.4\n27.0\n28.6\n32.0\n30.9\n21.6\n21.9\nTable 9: Multilingual Taubench Results: We follow the original experimental setup from Yao et al.\n(2024). We report the per language pass@1 (P@1) score. Scores aggregated over 3 runs.\nAvg\nar\nde\nes\nfr\nhi\nid\nit\nja\nko\npt\nru\ntr\nvi\nzh\nCommand A\n93.0\n98.2\n95.5\n94.8\n93.5\n94.6\n84.2\n94.9\n93.2\n93.4\n89.6\n93.7\n93.6\n92.2\n90.3\nCommand R+ Refresh\n95.5\n94.8\n98.2\n97.2\n97.2\n96.5\n89.0\n97.5\n96.2\n94.7\n91.6\n96.9\n97.8\n98.3\n90.6\nQwen 2.5 72B Instruct Turbo\n93.0\n96.4\n94.0\n94.3\n93.7\n94.1\n86.6\n94.0\n91.3\n93.0\n87.9\n95.8\n95.4\n95.7\n89.2\nClaude 3.7 Sonnet\n91.8\n94.5\n95.5\n93.2\n94.2\n93.6\n78.9\n94.0\n93.6\n93.9\n84.1\n95.9\n92.6\n95.3\n86.2\nLlama 3.3 70B Instruct\n91.3\n90.5\n94.7\n92.9\n93.0\n98.2\n92.1\n93.3\n83.3\n78.9\n91.2\n95.0\n92.9\n95.9\n86.5\nGemini 1.5 Pro\n90.6\n91.7\n94.4\n94.5\n92.0\n93.5\n82.9\n93.3\n86.1\n90.6\n86.7\n93.4\n94.8\n95.6\n79.1\nDeepSeek V3\n90.6\n94.9\n93.8\n94.2\n93.5\n92.2\n82.6\n92.8\n85.5\n91.5\n85.3\n92.9\n92.3\n91.8\n84.3\nGPT-4o\n88.9\n92.2\n91.0\n94.9\n91.7\n91.9\n80.9\n90.4\n85.3\n87.4\n87.0\n87.9\n90.7\n88.0\n85.5\nMistral Large 2\n75.9\n85.3\n64.7\n83.4\n78.3\n86.9\n65.0\n69.6\n82.6\n76.1\n75.2\n75.6\n69.3\n73.1\n77.2\nTable 10: Crosslingual line-level pass rate (LPR) from the Language Confusion Benchmark (Marchisio et al.,\n2024). Models are prompted in English with an instruction to reply in a different language. LPR measures\nthe percentage of answers with all lines in the requested language.\nMonolingual\nCrosslingual\nCommand A\n24.2\n33.5\nGemini 1.5 Pro\n19.3\n26.4\nGPT-4o\n15.8\n24.7\nClaude 3.7 Sonnet\n8.5\n23.1\nDeepSeek V3\n15.7\n15.7\nLlama 3.3 70B Instruct\n15.2\n8.3\nQwen 2.5 72B Instruct Turbo\n9.9\n9.6\nMistral Large 2\n6.9\n7.9\nCommand R+ Refresh\n1.9\n6.1\nTable 11: ADI2 score over monolingual and crosslingual prompts in 4 Arabic dialects (Egyptian, Saudi,\nSyrian, Moroccan) from Robinson et al. (2024). Higher scores indicate greater desired dialect adherence.\nBeyond instruction-following, agentic capabilities are important for enterprise use. We evaluate Command\nA on our own human translated version of τ-bench (Yao et al., 2024).4 As shown in Table 9, Command A\noutperforms other widely-adopted LLMs agentic solutions such as Mistral Large 2 and Gemini 1.5 Pro, while\nbeing competitive with GPT-4o.\nThe Language Confusion Benchmark (Marchisio et al., 2024) measures a model’s ability to appropriately\nrespond in the desired language of the user. In Table 10, we measure line-level pass-rate (LPR) on crosslingual\nprompts. Concretely, models are prompted with an English request and an instruction to reply in another\nlanguage. LPR is the percentage of responses where all lines were in the user’s desired language. Command\nA and its predecessor, Command R+ Refresh, perform very strongly across languages, with the highest and\nsecond highest aggregate scores.\nWe measure Command A’s sensitivity to regional dialect in Table 11, which shows ADI2 scores over mono-\nlingual and crosslingual prompts in 4 Arabic dialects (Egyptian, Saudi, Syrian, Moroccan) from Robinson\net al. (2024). Higher scores indicate more adherence to the desired Arabic dialect. We observe that Command\n4The number may differ slightly from the official implementation due to extensions for our multilingual evaluation pipeline.\n23\n\nA strongly outperforms comparison models in its ability to adhere to dialect.\n4.4\nCode\nWe evaluate the code capabilities of Command A across code understanding, code editing, and SQL\ngeneration benchmarks.\nPython\nMulti-language\nCOBOL\nRepoQA\nMBPP+\nLiveCodeBench BigCodeBench\nLBPP(All) HE(All)\nHE\n→Python\nCommand A\n86.2\n26.9\n45.4\n51.5\n76.2\n25.3\n55.7\n92.6\nCommand A Expert\n87.0\n24.9\n47.4\n50.8\n77.5\n29.8\n64.6\n91.8\nCommand A Agentic\n—\n32.9\n59.7∗\n65.4\n—\n—\n—\n—\nCommand R7B\n72.0\n9.0\n30.9\n21.9\n50.7\n7.0\n35.4\n69.6\nCommand R Refresh\n74.3\n11.0\n34.3\n24.7\n54.7\n1.9\n34.2\n73.2\nCommand R+ Refresh\n78.8\n14.4\n25.8\n25.6\n54.4\n2.5\n43.7\n77.0\nLlama 3.3 70B Instruct\n86.0 / 81.0\n32.9\n46.9 / 41.9\n47.8\n75.5\n3.2\n46.2\n85.6\nMistral Large 2\n84.7\n26.7\n44.7\n54.0\n82.9\n10.8\n46.8\n88.0\nQwen 2.5 72B Instruct\n88.6\n26.3\n45.8 / 43.6\n48.3\n78.5\n6.3\n55.7\n83.2\nLlama 3.1 405B Instruct\n88.6 / 87.0\n29.3\n46.2\n52.7\n76.7\n3.2\n59.5\n90.4\nDeepSeek V3\n90.0\n33.5\n50.0 / 48.6\n61.5\n83.5\n15.2\n63.3\n92.2\nTable 12: Code Understanding Benchmarks across Python, Multi-language, and COBOL groups re-\nporting 1-shot pass@1 and RepoQA reporting match accuracy. HE is HumanEval. All results are internal\nreproductions using an identical prompt except where ‘/’ indicates external value first and internal repro-\nduction second. Best score ±1% is bolded. ∗For BigCodeBench, we use 3 tool-use execution feedback tests.\nSWE-Bench Verified\nAider Polyglot\nCommand A\n26.8\n14.7\nCommand A Expert\n23.4\n8.9\nCommand R7B\n3.6\n2.7\nCommand R Refresh\n11.6\n1.8\nCommand R+ Refresh\n17.0\n2.2\nLlama 3.3 70B Instruct\n29.4\n8.4\nMistral Large 2\n30.0\n16.0\nQwen 2.5 72B Instruct\n33.0\n8.0\nLlama 3.1 405B Instruct\n33.4\n13.8\nDeepSeek V3\n42.0 / 45.8\n49.6 / 51.6\nTable 13: Code Editing Benchmarks. All results are internal reproductions using an identical prompt\nexcept where ‘/’ indicates externally reported value first and internal reproduction second.\nCode Understanding evaluates code generation across multiple languages. For Python generation, we\nreport on MBPP+ (Austin et al., 2021; Liu et al., 2024c), LiveCodeBench (Jain et al., 2024, Version 5\n10/24-2/25), BigCodeBench (Zhuo et al., 2024, Instruct), and RepoQA (Liu et al., 2024b, 32K context length,\nthreshold 0.8). For multi-language generation, we report HumanEval (Chen et al., 2021; Muennighoff et al.,\n2023) scores in Python, C++, Java, Javascript, Go, and Rust. We also extend our earlier uncontaminated\nPython benchmark, Less Basic Python Problems (Matton et al., 2024, LBPP), with parallel versions in\nC++, Java, Javascript, Go and Rust for uncontaminated generation evaluation across enterprise-critical\nprogramming languages.5\nTo assist in future advancements in COBOL understanding, we also develop a parallel version of HumanEval\nin COBOL (i.e., HumanEval-COBOL). We evaluate direct generation of COBOL, and translation of COBOL\n5We will release this dataset in an update to huggingface.co/datasets/CohereForAI/lbpp\n24\n\nSpider\nBird\nInternal\nDev\nTest\nDev\nAvg.\nSQLite\nPostgreSQL\nMySQL\nPL/SQL\nT-SQL\nCommand A\n79.5\n80.2\n59.5\n55.3\n48.7\n58.0\n56.0\n58.7\n55.3\nCommand A Expert\n85.5\n85.4\n58.5\n56.1\n49.3\n60.0\n55.3\n58.0\n58.0\nCommand R7B\n78.1\n77.6\n42.2\n34.4\n27.3\n36.0\n34.7\n38.7\n35.3\nCommand R Refresh\n76.5\n78.1\n47.3\n42.8\n36.7\n48.0\n42.7\n43.3\n43.3\nCommand R+ Refresh\n82.0\n81.7\n52.7\n44.4\n40.7\n47.3\n40.0\n52.0\n42.0\nLlama 3.3 70B Instruct\n81.1\n84.8\n58.0\n45.9\n41.3\n48.0\n43.3\n50.0\n46.7\nMistral Large 2\n78.8\n76.3\n50.0\n53.3\n54.0\n54.7\n50.7\n53.3\n54.0\nQwen 2.5 72B Instruct\n83.5\n83.8\n50.1\n53.7\n52.7\n54.7\n56.7\n49.3\n55.3\nLlama 3.1 405B Instruct\n83.0\n86.7\n59.4\n49.2\n54.0\n58.7\n50.7\n34.0\n48.7\nDeepSeek V3\n81.7\n81.7\n53.1\n60.8\n56.7\n66.0\n60.7\n58.7\n62.0\nTable 14: SQL Generation Benchmarks reporting execution accuracy against gold databases. All results\nare internal reproductions using an identical prompt. Avg. is the sample-weighted average across internal\nmulti-dialect evaluation datasets. Best score ±1% is bolded.\nto Python similar to Muennighoff et al. (2023). The translation setting tests model capability to update\nlegacy codebases into a modern language.\nCode Understanding metrics are outlined in Table 12. Sources of externally reported values are in Ap-\npendix B.3. Command A provides strong Python and multi-language performance compared to similar and\nlarger models. Table 26 details the complete performance for HumanEval and LBPP in all languages, high-\nlighting competitive accuracy in many business-critical programming languages. In the hardest benchmarks,\nLiveCodeBench and BigCodeBench, Command A surpasses many competitors and can be further improved\nwith agentic tool-use discussed below. Command A also leads in RepoQA performance compared to all\ncompetitors. Finally, Command A offers state-of-the-art capabilities in COBOL for both direct generation,\nvia HumanEval-COBOL, and translation from HumanEval-COBOL to HumanEval-Python. These strengths\nhighlight that Command A offers accurate code understanding in the complex environment of navigating\nlegacy enterprise codebases.\nWe also investigate the performance of Command A as a code agent using multi-hop tool use similar\nto the setup for RAG in Section 4.2. Command A can now access a code execution tool and receives\nfeedback on code generation via execution results from gold-standard unit tests similar to Gehring et al.\n(2025). We evaluate 3 datasets in this regime: LiveCodeBench, BigCodeBench, and LBPP(all languages). In\nLiveCodeBench, we use the public unit tests for execution feedback and private tests for final evaluation.\nFor BigCodeBench and LBPP, we simulate the unit-test split by using 3 unit tests for execution feedback\nand all remaining tests for final evaluation.6 Table 12 shows how using Command A as an agent easily\nsurpasses direct code generation across all datasets—achieving a pass@1 gain over Command A of +5.9% for\nLiveCodeBench, +14.3% for BigCodeBench, and +12.3% for LBPP across all languages. Notably, Command\nA achieves 71.4% in LBPP-Python surpassing all competitors by 4.3% and surpasses all other models in the\nBigCodeBench leaderboard at the time of publication.7\nCode Editing evaluates the model capability to generate precise code line-level changes to edit and update\na codebase. We evaluate our models on the SWEBench Verified Patch Generation task in Python (Jimenez\net al., 2024), and the Aider Polyglot benchmark8 for multi-language code editing in Python, C++, Java,\nJavascript, and Rust. Table 13 demonstrates Command A is competitively capable in repository-level under-\nstanding and solving pull-requests or building code fixes via patch generation. We note that these results are\nfrom post-hoc investigations into code-editing behaviour in our model as we did not target these functions\n6As the prompt design for LBPP includes 3 unit-tests, this setup does not leak any further testing requirements to the\nmodel.\n7The current best model is GPT-4o-2024-05-13 with 51.1 pass@1 https://bigcode-bench.github.io/\n8aider.chat/2024/12/21/polyglot.html\n25\n\n20.0%\n100\n200\n300\n400\n500\n600\n700\n40.0%\n30.0%\n50.0%\n70.0%\n60.0%\nLBPP All\nDataset\nModel\nBigCodeBench\nBird SQL\nCode Performance Against Model Size\nSize (#Billion Params)\nCommand R Refresh\nCommand A 111B Expert\nCommand R+ Refresh\nCommand A 111B\nCommand A 111B Agentic\nLlama 3.3 70B Instruct\nDeepSeek V3\nQwen 2.5 72B Instruct\nMistral Large 2\nLlama 3.1 405B\nPerformance (pass@1 / execution acc.)\nFigure 6: Code Performance against Model Size. Command A provides state-of-the-art performance\ncompared to models of similar size, and often significantly larger models. Command A Agent improves even\nfurther to set a new standard for performance at 111B size with tool-use in code.\nin developing Command A. Similar to our investigation into a code agents described above, we share these\nresults as early signposts for future objectives of code expert development.\nSQL Generation evaluates model capability in understanding user requests using a partially observed\ndatabase context. Understanding SQL and reasoning with databases is critical for Command A to succeed\nas an enterprise model. We evaluate SQLite performance using Spider (Yu et al., 2018, Dev & Test) and the\nmore recent Bird SQL benchmark (Li et al., 2023a, Dev). To ensure Command A can accurately generate SQL\nin an enterprise database context, we also report results for an internal benchmark in SQLite, PostgreSQL,\nMySQL, Oracle PL/SQL, and Microsoft T-SQL. Performance on these dialects better reflects real usage of\nSQL to access commercial database systems.\nTable 14 demonstrates that Command A offers state-of-the-art performance across multiple datasets. Com-\nmand A leads in both Spider Dev, and Bird to provide accurate SQL generation to solve challenging queries\nin even “dirty” database contexts. Across models of similar size, Command A also demonstrates the strongest\naverage performance across 5 enterprise-critical SQL dialects in our internal benchmark. This further punc-\ntuates the capability of Command A in both academic and enterprise scenarios for SQL.\nWe highlight the performance benefit of Command A relative to size in Figure 6. Across 3 datasets, Command\nA and Command A Code Expert provide best-in-class performance, often surpassing similar and larger\nmodels. Command A offers a unique trade-off for enterprise capability in accurate code and SQL generation.\nUsing Command A as an agent for code further enhances the model for state-of-the-art capabilities across\nchallenging benchmarks.\n4.5\nMath and Reasoning\nWe evaluate the reasoning capability of our model on key mathematical reasoning benchmarks, and compare\nthis to publicly-reported metrics (where available) in Table 15. We find that Command A performs especially\nwell on mathematical benchmarks, and that merging models preserves reasoning performance (compared to\nreasoning-expert models) within a few percentage points across most benchmarks (§4.9).\n26\n\nMATH (all)\nAIME (2024)\nGPQA (Diamond)\nCommand A\n80.0\n23.3\n50.8\nGPT-4o\n68.5\n9.3\n46.0\nLlama 3.3 70B\n77.0\n20.0∗\n50.5\nLlama 3.3 405B\n73.9\n20.0∗\n49.0\nMistral Large 2\n71.3∗\n11.0\n48.6\nTable 15: Reasoning performance of Command A compared to similarly-sized models. Benchmarks are MATH\n(Hendrycks et al., 2021), the 2024 AIME mathematics competition, and GPQA Diamond (Rein et al., 2023).\nResults for external models are taken from officially-reported sources, unless indicated with an asterisk (*),\nwhich denotes internal evaluation since official public results were not available.\nIn our qualitative assessments, we also find that reasoning-expert models provide generalised gains in coding\nand structured data manipulation tasks, and that these are additive in the final Command A model.\n4.6\nSafety\nOur safety evaluation methodology combines human and automated assessments. Due to speed and cost\nconsiderations, we mainly rely on automated evaluations. We use human annotations as a baseline to ensure\nour automated evaluations align with human judgment. These are triply annotated by an internal team of\nspecialist safety annotators. To further strengthen the reliability of our automated evaluation, we assess the\nsuitability of evaluators based on their robustness to artifacts (Chen & Goldfarb-Tarrant, 2025).\nWe measure both absolute and relative safety. Absolute safety evaluation tests models with potentially\neliciting prompts from the categories of our core safety behaviour (§3.3.7), and then computes the rate of\nunsafe content in the model output, using an LLM-as-a-judge setup. The absolute safety aggregate score is\nthe average of each categorical rate, where each category is weighted equally. Relative safety evaluation uses\nthe same prompts, but considers how the safety of each response compares to the safety of another model’s\nresponse for the same prompt. If both responses are equally safe, the higher quality response is chosen as\nthe winner. Relative safety is more challenging, so we rely on a jury of LLM evaluators (Verga et al., 2024),\nwhich achieves human agreement scores of 77.7% and Cohen’s Kappa of 0.55 in relative safety evaluations.\nWe also measure over-refusal rate; how frequently models refuse to answer a prompt that should be an-\nswered. These prompts fall into two categories: word sense disambiguation and requests for information\nabout safety topics. We use an LLM-as-a-judge setup, as we find refusal classification a much easier task\nthan safety, with very high accuracy and human agreement\n4.6.1\nEnterprise Safety\nIn enterprise contexts, safety priorities and risk assessments differ significantly from those in default contexts.\nWe consider two elements that are of strong concern for Enterprise usage: Controllability, the ability of\nthe model to be customised for different safety needs, and Demographic Fairness, the robustness of the\nmodel to demographic perturbations in tasks involving real human data.\n4.6.1.1\nControllability\nIn Enterprise Safety, the notion of safety itself is context-dependent. Some core safety behaviour is consistent\nacross all contexts (§3.3.7.1), but much of it varies between different deployments. The boundaries of content\nthat an LLM should generate when used as an LLM-editor for a journalist are very different than the content\nboundaries of a customer service chatbot. Therefore, we evaluate the model’s ability to accurately condition\non different safety instructions, under our two safety modes: contextual and strict (§3.3.7.1). For each mode\nwe compose two evaluation sets: one that should always be answered (over-refusal evaluation) and one that\nshould always be refused (safety mode control), which allows us to optimise the trade-off between these two\nscenarios.9 Safety mode accuracy is the mean of these sets for a given mode.\n9We note that the over-refusal evaluation set was created by red-teaming Command R+ Refresh.\n27\n\n75\n0\n20\n40\n60\n80\n100\n85\n80\n90\n100\n95\n75\n0\n20\n40\n60\n80\n100\n85\n80\n90\n100\n95\nLlama 3.1 405B\nCommand A\nMistral Large\nQwen 2.5 72B\nGPT-4o\nLlama 3.3 70B\nDeepSeek V3\nClaude 3.5 Sonnet\nCommand R+ Refresh\nStrict Mode\nCorrectly Answering\nCorrectly Refusing\nCorrectly Refusing\nCorrectly Answering\nContextual Mode\nFigure 7: The Pareto frontier between correctly answering and refusing for our enterprise safety modes.\nFigure 7 shows that the Command A model is on the Pareto frontier between answering and refusing for both\nsafety modes. Results for competitor models can be found in appendix Table 27. Each competitor targets\ndifferent markets and behaviours, so we consider different modes to have effectively different competitors. In\ncontextual mode, the relevant competitors are Mistral Large 2, Qwen 2.5 72B Instruct and Llama 3.3 70B\nInstruct, while in strict mode the relevant competitors are GPT-4o and Claude 3.5 Sonnet.\n4.6.1.2\nDemographic Fairness\nLLMs are used in various hiring software systems in the market, and we evaluate demographic fairness in\nthis context. The model is tasked with summarising the suitability of resumes with respect to a given job\ndescription. We follow Seshadri & Goldfarb-Tarrant (2025) for both our method and our metric. We permute\nthe demographics of the resume and measure meaningful differences in generated summaries for candidates\nwhen their race or gender has changed. A perfect model would have no meaningful differences, i.e. would be\ninvariant to the perturbation. The bias metric is defined as the proportion of measurements (including reading\nease, subjectivity and regard, as outlined in Seshadri & Goldfarb-Tarrant (2025) for which the null invariance\nhypothesis is rejected when comparing the original and perturbed summaries. To account for variability in\ngenerations (Chen & Goldfarb-Tarrant (2025) observed this even at temperature 0), we generate responses\nusing each model five times per sample and plot the distribution of bias rates across all runs. The results\nfor gender and race are shown in Figure 8. We report with both Bonferroni (bonf) and Benjamini-Hochberg\n(bh) corrections to account for the multiple measurements on the same summaries and to allow the reader\nto select whichever correction is more applicable – bonf to minimise false positives (finding a demographic\nfairness issue when there is none), and bh to minimise false negatives.\nWe note two broad patterns across all models: models tend towards much stronger racial bias than gender\nbias, and smaller models tend to have greater bias than larger models. In particular, the Command A\nmodels show impressive robustness to demographic perturbations. Command A is entirely robust to gender\nperturbations and very resilient to race ones (only 1% failures). Command R7B similarly is entirely robust to\ngender in this evaluation, and competitive for a small model at robustness to race, with around 4% failures.\nWe don’t observe significant gender bias for large models in this domain in our testing setup. Command A,\nLlama 3.3 70B Instruct, and Mistral Large 2 all exhibit minimal racial bias, each failing a median of 1% of\ninvariance tests, while Claude 3.5 Sonnet has the lowest, at 0%. Small models are significantly less robust.\n28\n\n0.0\n0.4\n0.2\n0.6\n1.2\n1.0\n0.8\nGender\nBonferroni\nBenjamini-Hochberg\n0\n1\n2\n5\n4\n3\nGender\n0\n2\n4\n8\n6\nRace\nMistral Large\nLlama 3.3 70B\nGPT-4o\nCommand A\nClaude 3.5 Sonnet\n0\n2.5\n5.0\n20.0\n17.5\n15.0\n12.5\n10.0\n7.5\nRace\nCommand R7B\nCommand R Refresh\nMistal 8B\nMistral 8x7B\nLlama 3.1 8B\nGemma 2 9B\nFigure 8: Boxplots of gender and racial bias rates in model-generated resume summaries for Command A\n(left) and Command R7B (right) compared to similarly sized models, respectively, using either Bonferroni\nor Benjamini-Hochberg correction. The Command A models show impressive robustness to demographic\nperturbations. Command A is robust to gender perturbations and very resilient to race ones (only 1%\nfailures). Command R7B similarly is robust to gender in this evaluation, and competitive for a small model\nat robustness to race, with around 4% failures.\nMost small models remain robust to gender, with the exception of Llama 3.1 8B Instruct and Ministral 8B,\nwhich fail 1-5% of invariance tests. Interestingly, Ministral 8B lacks robustness to gender, but is robust to\nrace, whereas Mixtral lacks robustness to race, but is robust to gender.\nOverall, our models offer excellent coverage of robustness across different demographic categories, for multiple\nsizes. We note that, though generation does contribute, total demographic fairness in a hiring pipeline is\ndominated by the retrieval stage (Seshadri & Goldfarb-Tarrant, 2025). Here we measure only the generation\nstage, but our embedding model for the retrieval stage is also the most robust to perturbations.\n4.6.2\nDefault Safety\nIn the default setting, we evaluate the safety of the model without a system preamble to simulate cases\noutside of Cohere’s API or enterprise contexts.\nCommand A shows strong performance in various categories of unsafe content. As shown in Figure 9, Com-\nmand A significantly outperforms all competitors in relative safety evaluations. Additionally, it attains an\nabsolute safety score of 70.4%, ranking third among large models, closely following Claude 3.5 Sonnet and\nQwen 2.5 72B Instruct (Table 16). It excels at avoiding violence and hate speech, with a 89.7% safe response\nrate, and performs well in areas such as not generating CSEA (87.5%) and not promoting misinformation\n(67.9%) (Figure 15). While Command R7B shows lower overall performance, it still maintains a notable\npresence in certain categories, such as avoiding violence and hate speech (76.3%) and not promoting CSEA\n(67.0%) (Figure 16). These results highlight the effectiveness of Command A in mitigating unsafe content\ngeneration even in situations where we cannot add system preamble guardrails.\nAlthough the relative and absolute safety performance of Command A may initially seem contradictory, this\n29\n\n0.0\n20.0\n40.0\n60.0\n80.0\n100.0\nCommand A Win Rates\nCompetitor Win\nClaude 3.5 Sonnet\nDeepSeek V3\nGPT-4o\nLlama 3.1 405B\nLlama 3.3 70B\nMistral Large\nQwen 2.5 72B\nCommand R+\n(a) Large Models\n0.0\n20.0\n40.0\n60.0\n80.0\n100.0\nCommand R7B Win Rates\nCompetitor Win\nGemma 2 9B\nLlama 3.1 8B\nQwen 2.5 7B\nCommand R\n(b) Small Models\nFigure 9: Default relative safety performance. Winner is assigned by a panel of LLM judges. When\nboth responses are equally safe, the winner is chosen based on which response is higher quality.\nRelative\nSafety(↑)\nAbsolute\nSafety(↑)\nMisinfo(↑)\nSelf-\nharm(↑)\nCSEA(↑)\nSexual\nContent(↑)\nViolence &\nHate(↑)\nCommand A\n49.5\n70.4\n67.9\n61.2\n87.5\n63.1\n89.7\nClaude 3.5 Sonnet\n26.4\n80.0\n76.9\n90.4\n98.5\n94.4\n93.1\nDeepSeek V3\n23.7\n49.7\n50.0\n37.0\n74.1\n34.8\n74.0\nGPT-4o\n26.6\n65.6\n76.9\n69.9\n33.7\n95.5\n84.4\nLlama 3.1 405B\n15.9\n41.8\n42.3\n28.8\n63.0\n34.3\n62.2\nLlama 3.3 70B\n14.9\n40.5\n50.0\n42.5\n63.0\n6.7\n61.8\nMistral Large 2\n22.0\n45.7\n57.7\n37.0\n74.8\n8.0\n71.0\nQwen 2.5 72B\n32.4\n71.4\n61.5\n60.3\n86.3\n91.6\n90.0\nCommand R+ Refresh\n16.3\n30.2\n42.3\n21.9\n45.9\n5.1\n47.3\nCommand R7B\n49.7\n58.2\n50.0\n50.7\n67.0\n47.2\n76.3\nGemma 2 9B\n81.3\n87.3\n76.9\n82.2\n94.8\n85.4\n96.9\nLlama 3.1 8B\n35.9\n63.6\n57.7\n58.9\n60.7\n66.3\n74.4\nQwen 2.5 7B\n59.2\n71.5\n65.4\n50.7\n71.9\n87.6\n82.1\nCommand R Refresh\n30.4\n31.3\n38.5\n26.0\n37.8\n3.9\n50.4\nTable 16: Default safety performance of Command A and Command R7B compared to similarly sized\nmodels across various categories of unsafe content. Relative safety is the winrate vs. Command A. Absolute\nsafety score is computed as an average of safe response rates for all categories. Large models are shown in\nthe top half of the table, while small models are shown in the bottom half. The top performing model for\neach size category is bolded in each column. As indicated by the upwards-pointing arrows, higher winrates\nand higher safe response rates correspond to better performance for each competitor.\noccurs because the relative safety evaluation considers the intersection of safety and quality. Critically, in\nthe event that both models provide a safe response, the relative safety evaluations then consider the winner\nto be the model that provides a higher quality response. Rather than simply refusing to answer, Command\nA engages meaningfully with queries that relate to potentially unsafe topics. Many other models, such as\nClaude 3.5 Sonnet, provide non-specific refusals.\nWe also measure over-refusal rates for the default setting on the XSTest benchmark (Röttger et al., 2024).\nCommand A shows refusal rates under 3%, which is considerably better than other closed-source models,\nnamely Claude 3.5 Sonnet and GPT-4o; and marginally better than open-access models such as Llama 3.3\n30\n\nXSTest Refusal (↓)\nOver-Refusal (↓)\nFull\nPartial\nXSTest\nInternal\nCommand A\n1.1\n0.0\n1.1\n7.1\nClaude 3.5 Sonnet\n3.6\n0.0\n3.6\n4.1\nDeepSeek V3\n0.8\n0.0\n0.8\n1.2\nGPT-4o\n5.6\n0.0\n5.6\n7.1\nLlama 3.1 405B\n1.2\n0.0\n1.2\n2.4\nLlama 3.3 70B\n1.2\n0.0\n1.2\n3.5\nMistral Large 2\n0.8\n0.0\n0.8\n2.4\nQwen 2.5 72B\n0.4\n0.0\n0.4\n1.2\nCommand R+ Refresh\n3.6\n1.2\n4.8\n5.3\nCommand R7B\n4.0\n0.8\n4.8\n11.8\nGemma 2 9B\n2.8\n4.0\n6.8\n24.1\nLlama 3.1 8B\n6.4\n0.0\n6.4\n8.8\nQwen 2.5 7B\n0.8\n0.0\n0.8\n9.4\nCommand R Refresh\n3.2\n0.0\n3.2\n2.4\nTable 17: Over-refusal rate of Command A and Command R7B, based on the XSTest benchmark, which\ndistinguishes between full refusal and partial refusal. We sum both to obtain the over-refusal rate.\n0.0\n0.2\n0.1\n0.6\n0.5\n0.4\n0.3\n0.8\n0.7\nArabic\nJapanese\nKorean\nChinese\nFrench\nLanguage\nSpanish\nItalian\nCommand R+ Refresh\nLlama 3.3 70B\nLlama 3.1 405B\nDeepSeek V3\nMistral Large\nCommand A\nPortuguese\nGerman\nFigure 10: Safety scores (rate of safe responses averaged over Misinformation and Violence and Hate) across\n9 languages. The dashed lines show the English score.\n70B Instruct (see Table 17). As XSTest is saturated, we also report default over-refusal based on our internal\ntest set from red-teaming our previous model.\n4.6.3\nMultilingual Safety\nWe evaluate safety on nine priority languages (results in Figure 10). The safety score is the rate of safe\nresponses measured on the same set of prompts across all languages, thus allowing for direct comparisons.\nThis set is the Misinformation and Violence and Hate categories from English, translated automatically,\nthen corrected and naturalised by multilingual annotators. We use LLM-as-a-judge evaluation. For some\nlanguages, completions are translated into English before being evaluated, and some are retained in the\noriginal language, based on which showed the best performance on a development set.\nWe also evaluate over-refusal via a prompt set collected through red teaming with the multilingual annotators.\nThe English over-refusal set was translated into each language, then refined and augmented with more natural\n31\n\nprompts. Command A is on the Pareto frontier between safety and over-refusal in 5 languages (Japanese,\nChinese, German, Korean and Arabic), whilst remaining competitive in all other measured languages.\n4.7\nEnterprise-specific Benchmarks\nOur enterprise evaluation process focuses on testing model capabilities in generative and retrieval-augmented\nuse cases that mirror typical enterprise applications.\nGenerative use cases. We use rule-based checks and LLM-as-a-judge evaluations due to the complexity\nand nuances of the differing enterprise use cases. We show an example prompt for reference in Figure 11.\nExample Prompt\nCreate a job ad for the role of Social Media Manager at the BuzzMedia, located in New York, NY.\nThe ad should be creative and engaging to attract the best talent. It should include a catchy and\ncreative title, a brief role overview, and a list of at least 5 employee benefits offered by the company.\nThe overview should be at least 50 words.\nFormat the response as a JSON object in the following format.\n{\n\"title\": <title>,\n\"role_overview\": <role_overview>,\n\"employee_benefits\": [\n<Benefit 1>,\n<Benefit 2>,\n<Benefit 3>,\n<Benefit 4>,\n<Benefit 5>\n]\n}\nFigure 11: Example prompt for an enterprise generative use case.\nWe break down success criteria into rule-based and LLM-based checks:\n• Rule-Based Checks. Checks for attributes like word count, valid JSON output, count of expected\nitems, and similar.\n• LLM-Based Checks. We use a panel of judges, similar to the approach described in Verga et al.\n(2024), to evaluate more nuanced criteria like tone and natural language quality.\nThe final per-example score is an average of the rule- and LLM-based scores. Our generative enterprise\nbenchmark consists of 22 tasks covering use cases including chat and meeting summarisation, information\nextraction, and FAQ generation. A summary of the results across all tasks can be found in Table 18. Command\nA achieves the highest pass rate at 94.2% across all generative use cases. Command R7B scores 71.9%, which\nis the highest performance among similarly-sized models.\nRAG use cases. For enterprise RAG evaluation tasks, we assess question-answering use cases involving\ntechnical documentation and workplace policies, including internal rules and company benefits. These tasks\noften involve user queries on long document snippets that can exceed 10,000 tokens. Some questions require\nsynthesizing information from multiple snippets, while others cannot be directly answered using the given\ndocuments. Our evaluation set includes ground-truth answers annotated by humans.\nTo assess the performance of our models, we use two key evaluation metrics:\n• Correctness: Measured using Llama Index Correctness, this evaluates the validity of a model’s re-\nsponse. It ensures that generated answers align with the provided context and are factually correct.\n• Answerability Accuracy: Assessed by LLM judges, this metric measures the model’s ability to\ndiscern between answerable and unanswerable questions.\n32\n\nModel\nEnterprise Pass Rate (%)\nCommand A\n94.2\nCommand R+ Refresh\n87.4\nClaude 3.5 Sonnet v2\n84.2\nDeepSeek V3\n81.3\nGPT-4o\n79.1\nLlama 3.3 70B Instruct\n65.2\nLlama 3.1 405B Instruct\n77.8\nCommand R7B\n71.9\nGemma 2 9B\n65.7\nLlama 3.1 8B\n60.4\nTable 18: Enterprise generative evaluation performance across all 22 tasks.\nModel\nWorkplace Policies QA\nTechnical QA\nAvg.\nCommand A\n4.59\n4.86\n4.73\nCommand R+ Refresh\n4.08\n4.42\n4.25\nClaude 3.5 Sonnet v2\n4.63\n4.81\n4.72\nDeepSeek V3\n4.52\n4.64\n4.58\nGPT-4o\n4.50\n4.81\n4.66\nLlama 3.3 70B Instruct\n4.45\n4.78\n4.61\nLlama 3.1 405B Instruct\n4.41\n4.62\n4.52\nCommand R7B\n4.16\n4.48\n4.32\nGemma 2 9B\n3.23\n4.66\n3.95\nLlama 3.1 8B\n3.99\n4.37\n4.18\nTable 19: Enterprise RAG evaluation performances of Command A models. The table shows the LLama\nIndex Correctness metric which ranges from 1-5. Best performances are marked in bold.\nModel\nAnswerable Acc. (%)\nUnanswerable Acc. (%)\nCommand A\n96\n91\nCommand R+ Refresh\n78\n95\nClaude 3.5 Sonnet v2\n89\n92\nDeepSeek V3\n93\n86\nGPT-4o\n94\n88\nLlama 3.3 70B Instruct\n95\n87\nLlama 3.1 405B Instruct\n94\n90\nCommand R7B\n86\n76\nGemma 2 9B\n90\n90\nLlama 3.1 8B\n88\n91\nTable 20: Enterprise RAG Answerable and Unanswerable Accuracy.\nThese metrics ensure that the model generates accurate responses while demonstrating robust judgement in\nhandling questions beyond the scope of the provided context. Table 19 shows the LLama Index Correctness\nperformance on RAG use cases and Table 20 shows the Answerable and Unanswerable Accuracy across tasks.\nThe accuracy across answerable and unanswerable questions is typically a trade-off as we do not want the\nmodel to over-refuse when the question is actually answerable, given the context.\nCommand A has the best Llama Index Correctness Average at 4.73, Answerable Accuracy of 96% and an\nUnanswerable Accuracy of 91%, indicating that it responds when intended, while keeping hallucination rates\nlow for unanswerable questions. Command R7B has the best Llama Index Correctness Average score at 4.32\n33\n\ncompared to models of similar size, an Answerable Accuracy of 86% and an Unanswerable Accuracy of 76%.\n4.8\nLong-Context Benchmarks\nTo assess long-context understanding capability, we employ two extensive long-context benchmark datasets:\nRULER (Hsieh et al., 2024) and LongBench-V2 (Bai et al., 2025). RULER comprises 13 distinct tasks,\nincluding retrieval, question-answering, multi-hop tracing and aggregation tasks. It is designed to evaluate a\nmodel’s ability to retrieve and reason over longer context inputs. The evaluation is conducted on sequences of\nup to 256k tokens. LongBench-V2 includes a diverse set of question-answering tasks spanning various levels\nof difficulty and multiple context types, such as single- and multi-document contexts, multi-turn dialogues,\ncode repositories, and long structured data.\n4k\n8k\n16k\n32k\n64k\n128k\n256k\nAvg\nwAvg.\nwAvg.\n(≤128k)\n(inc)\n(dec)\nCommand A\n97.2\n96.9\n96.7\n95.9\n93.3\n90.0\n84.6\n95.0\n93.9\n96.1\nMistral Large 2\n96.4\n96.3\n95.3\n94.0\n85.9\n48.1\n-\n86.0\n79.5\n92.5\nLlama 3.1 70B\n96.5\n95.8\n95.4\n94.8\n88.4\n66.6\n-\n89.6\n85.5\n93.7\nCommand R+ Refresh\n96.0\n95.1\n94.0\n92.4\n85.4\n64.6\n-\n87.9\n83.4\n92.4\nGPT-4o(11-20)\n97.0\n92.1\n89.0\n88.8\n88.4\n-\n-\n-\n-\n-\nClaude 3.5 Sonnet (10-22)\n96.5\n96.0\n95.7\n95.0\n95.2\n93.8\n-\n95.4\n95.0\n95.8\nGemini-1.5-Pro (002)\n96.2\n96.0\n96.0\n95.8\n93.8\n91.7\n91.6\n94.9\n94.2\n95.6\nGemini-2.0-Flash (exp)\n96.0\n96.0\n95.1\n95.7\n93.7\n86.0\n79.7\n93.8\n92.4\n95.1\nTable 21: Results on the RULER long context benchmark. Not all models support contexts up to 256k.\nOverall\nShort\nMedium\nLong\nEasy\nHard\nCommand A\n43.4\n44.1\n47.4\n34.3\n45.3\n42.3\nMistral Large 2\n34.4\n41.7\n30.7\n29.6\n38.0\n32.2\nLlama 3.1 70B\n31.8\n37.2\n28.8\n28.7\n35.4\n29.6\nLlama 3.1 405B\n37.8\n39.7\n39.0\n32.1\n39.0\n37.0\nCommand R+ Refresh\n27.8\n36.7\n23.7\n21.3\n30.2\n26.4\nGPT-4o(11-20)\n46.0\n42.9\n52.1\n35.3\n52.1\n42.2\nClaude 3.5 Sonnet (10-22)\n40.7\n44.9\n41.3\n31.6\n45.8\n38.6\nTable 22: LongBench-V2 results for Command A.\nTables 21 and 22 highlight Command A’s exceptional long-context capabilities. Our hybrid architecture\nenables this level of performance while requiring significantly less KV cache memory compared to models\nwith a full attention architecture. For instance, at an 8k sequence length, Command A requires only 75%\nof the KV cache memory used by Llama 3.3 70B Instruct, 23.8% of that used by Llama 3.1 405B Instruct,\nand 45.5% of that used by Mistral Large. At a 128k sequence length, these decrease to 32.9%, 10.4%, and\n19.9%, respectively. This reduction in KV cache memory usage can significantly decrease latency and memory\nconsumption while enhancing throughput during inference, particularly for longer contexts.\n4.9\nMerging\n4.9.1\nExpert performance is largely preserved\nWe find that model merging is an effective method for combining capabilities from a set of expert models\ninto a single model, and that linear merging is sufficient to preserve expert performance with only a 1.8%\naverage drop. Figure 12a shows the distribution of changes in score for the metrics tracked during merging.\nWe find that the overwhelming majority of metrics are preserved to within 2.5% of the best expert score, with\nsome metrics actually improving after merging. Figure 12b shows the average degree of metric preservation\nbetween expert and merge, grouped by domain. RAG and general performance are generally best preserved,\nwith code performance the least well preserved during merging.\n34\n\nNumber of Metrics\nMerge Score - Expert Score\n(a) Distribution of metric values for the merged\nmodel, as a percentage of the score for the best in-\nput expert. The overwhelming majority of metrics\ntracked are preserved to within 2.5% of the best ex-\npert score.\nMerge - Expert Score, Percentage\nDomain\nReasoning\nSafety\nGeneral\nCode\nMultilingual\nRAG\n(b) Average degree of metric preservation between ex-\npert and merge, by domain. Error bars denote stan-\ndard deviation. RAG and general performance are\nbest preserved, with code the least well preserved.\nNote that the higher RAG standard deviation is a\nproperty of RAG metrics (rather than the merge) as\nTauBench has very high variance across runs.\nFigure 12\nWe additionally find that, at the 111B scale, model merging is very stable. Large changes to expert weights\nresult in relatively small (2-3%) changes in domain performance, with very few candidates displaying catas-\ntrophic failures at any capability.\n4.9.2\nMerging is a coarse operation\nModel merging is a coarse operation, with no guarantee that a merged checkpoint will fall at a local optimum\nin the loss landscape. We find that applying a small number of SFT steps with low learning rate10 on top of\na merged checkpoint leads to significant improvements in model capability, with many metrics reaching or\nexceeding 100% of the expert performance. Intuitively, model merging appears to be effective at combining\ncapabilities but may leave them ‘misaligned’ in encoding space; a small number of gradient-based updates\ngently adjusts these encodings and allows the capabilities to be fully surfaced.\n4.10\nPolishing\nThe polishing effort includes several phases to improve model style and overall alignment with human pref-\nerences. We use the same evaluation setting as the multilingual mArenaHard restricted to English (Dang\net al., 2024). In Figure 13, we show Command A win rates against GPT-4o (1120) according to a pool of\nLLM judges at each phase of the CoPG/SRPO ping-pong. While this method allows us to achieve high\nwin rates, we also observe that interleaving the two methods tends to increase overall training stability: a\npotential regression occurring at a particular phase is likely to be corrected by the next one.\nIn addition, polishing helps improve both the overall alignment with human preferences and recovery of any\ndegraded scores during merging. For some of the metrics, it also improves the score over the expert models.\nFigure 14 provides the difference in metrics across many domains between the polished and the RL soup\nmodel, showing that some domains benefit significantly from polishing, with human preferences benefiting\nthe most.\n10Roughly 10-20x lower than was used for the main SFT training.\n35\n\nAuto Arena Hard\n0\n0.2\n0.1\n0.3\n0.4\n0.5\nSoup\nBON\nSRPO\nOnline CoPG\nSRPO\nOnline CoPG\nSRPO\nFigure 13: Command A win rates against GPT-4o as the polishing phase progresses.\nImprovements from Polishing\nPolished - Soup\nBFCL \nMGSM\nMMLU\nLBPP\nMATH (all)\nHumanEval\nIFEval\n(Not) over-refusal\nHuman Eval Win Rates\nmArenaHard\nAuto Arena Hard\n-5\n5\n0\n10\n20\n15\n25\nSafety Controllability\nFigure 14: Performance improvements across various domains during polishing between the inital RL soup\nmodel and the polished model.\n4.11\nHuman Evaluation\nWhile automated evaluation benchmarks provide quick feedback and allow for efficient hill-climbing in specific\ntask settings, automatically assessing the perceived quality of models remains challenging (Ni et al., 2024).\nTo validate broader model performance, our most promising model candidates are additionally evaluated by\nhuman annotators. This section describes our human evaluation setup, including details on the curation of\nour internal evaluation dataset, an overview of our annotation strategy, as well as the results for Command\nA in head-to-head evaluation against competitors.\n36\n\n4.11.1\nEvaluation Data\nChatbot Arena (Chiang et al., 2024) is a popular LLM benchmark that involves crowd-sourced human an-\nnotations. The framework relies on human preference judgments between two model-generated completions\nover user-provided prompts, from which the authors then derive Elo scores to build up-to-date leader-\nboards (Boubdir et al., 2024). While Chatbot Arena-like evaluation provides an extremely useful quality\nsignal on user-perceived model quality, it has several drawbacks for the efficient evaluation of internal model\ncandidates. Two challenges are the long ramp-up time required to provide performance signal on a new\nmodel (often requiring multiple thousands of ratings before producing a reliable Elo score), as well as the\nreliance on user-provided prompts that often end up skewing towards simpler topics. In order to provide a\nmore immediate and targeted feedback signal on human-perceived model performance, we instead curate a\nstatic collection of single-turn prompts for the internal evaluation of our models versus competitors.\nPrompts in our internal collection are primarily curated from scratch by our pool of annotators to avoid\naccidental contamination for competitor models by re-using existing prompt collections. Instructions are\nspecifically targeted towards creating complex real-world prompts that span a broad range of typical LLM-\nassisted tasks. For our purposes, we define complexity as a function of the number of different asks contained\nin a single prompt. As an example, asking for a summary with a certain length requirement would be assigned\na complexity score of 2. The first point for invoking a particular task (summarisation), and the second point\nfor additionally restricting the output (length constraint). We automatically filter prompts with the help of\nCommand R+ Refresh along several axes, building up a more comprehensive notion of prompt complexity.\nWhile the filtering may be overly strict in some instances, our findings show that the resulting pool of\nprompts is sufficiently difficult even for state-of-the-art models. We prune the final dataset to around 800\nprompts, further subset into “general” (350 prompts), “reasoning” (150 prompts), and “code” (300 prompts).\nThe “general” split focuses on general-purpose tasks that do not require deep technical expertise. This can\ninclude open-ended document generation or idea generation requests, and at most requires basic under-\nstanding of formatting languages such as CSV or JSON. The “reasoning” split includes more reasoning- and\nmath-heavy prompts that generally require undergraduate-level understanding of one STEM discipline. For\nthe “code” split, we specifically instruct annotators to curate prompts across a number of target programming\nlanguages, with a focus on analysis and debugging rather than code generation.\n4.11.2\nAnnotation Methodology\nOur annotation process comprises a relatively straightforward pairwise preference evaluation setup. For each\nannotation task, we present annotators with one prompt and two completions from competing models.\nAnnotators are first tasked with evaluating the quality of each completion separately, assigning a score from\n1 (flawed) to 5 (perfect). They are then given the option to label common failure modes. Finally, annotators\nprovide their preference between the two completions. The choices correspond to “Completion A is much\nbetter”, “Completion A is slightly better”, “Tie”, “Completion B is slightly better”, and “Completion B is\nmuch better”. To avoid positional bias, we randomly shuffle the order in which completions are shown to\nannotators. We compare different models’ performance based on their win rate versus a fixed competitor\nmodel. We assign the win rate in a pairwise matchup as a single score, distributing ties, as:\nwinrate = wins + (0.5 × ties)\nwins + ties + losses\nWe group the strong and weak preferences for wins or losses together and find that computing a win rate\nover the 5-point scale does not change model rankings, but helps annotators gain confidence in their given\npreference ratings. For a complete pairwise evaluation of 800 samples across all three subsets, on average 65\nannotators contribute to a single evaluation run.\n4.11.3\nResults\nTable 23 shows the results for pairwise human annotation runs against different competitor models. 11 Our\nresults show that Command A is competitive with frontier models on both general and reasoning subsets.\n11We obtain completions for GPT-family models directly through the OpenAI API. We use the TogetherAI endpoints for\nLlama 3.3 and DeepSeek V3.\n37\n\nCommand A Win Rate (%)\nvs.\nGeneral\nReasoning\nCode\nGPT-4o\n50.4\n51.4\n46.8\nGPT-4.5 Preview\n47.2\n30.7\n38.3\nDeepSeek V3\n49.0\n49.3\n54.7\nLlama 3.3 70B Instruct\n68.8\n71.7\n63.4\nLlama 3.1 405B Instruct\n61.6\n64.0\n61.6\nTable 23: Win rate of human-annotated pairwise evaluation between Command A and different competitor\nmodels on our internal test set. Win rates are from the perspective of Command A (50% is tie, higher\nnumbers mean Command A wins by a larger margin).\nInterestingly, we also observe that GPT-4.5-preview improves significantly over its predecessor on reasoning-\nheavy prompts.\nAnnotators strongly prefer Command A over Llama 3.3 70B Instruct for all subsets, including code. Our\nanalysis across evaluation results indicates that humans particularly prefer Command A’s ability to respect\nrequests for a particular formatting or style.\nTo further illustrate the importance of polishing as a critical step towards human preference optimisation\n(see Section 3.5), we compare the human evaluation results of an earlier internal checkpoint, a result of\nthe expert merging before polishing. Evaluating both our earlier checkpoint and the final model candidate\nagainst GPT-4o, we see substantial gains as reflected in human preference. For our final model, we manage\nan absolute improvement of more than seven percentage points in win rate on the general subset (43.2 →\n50.4), 10 points on reasoning (41.4 →51.4), and almost 17 points (30.0 →46.8) on code.\nOverall, Command A is significantly more preferred by human evaluators to models such as Llama 3.1 405B\nInstruct across all subsets, and is competitive with state-of-the-art models such as GPT-4o and DeepSeek\nV3 while being considerably more efficient.\n5\nConclusion\nThis technical report detailed the development of Command A, shared extensive performance evaluations\nacross many domains and languages, and shared additional results for Command R7B. Command A repre-\nsents a significant advancement in LLMs for enterprise, achieving best-in-class performance across a wide\nrange of tasks with optimal efficiency. Our models excel in enterprise-relevant tasks such as agentic workflows,\nmultilingual understanding and generation, and instruction-following. Key innovations introduced include\ndata and architectural optimisations, self-refinement algorithms, and a model merging-based approach that\nensures expert-level performance across diverse capabilities within a single model.\nCommand A outperforms comparable models in both efficiency and computational overhead, requiring fewer\nresources for serving, making it easy to deploy on-premises or in private cloud environments on just two A100\nor H100 GPUs, and delivering tokens at a higher rate. The release of model weights under a non-commercial\nlicense further facilitates community-based exploration and research. Command A sets a new standard for\nLLMs in enterprise applications, balancing performance, efficiency, and versatility — and providing maximum\nperformance for minimal compute.\n38\n\nReferences\nAakanksha, Arash Ahmadian, Seraphina Goldfarb-Tarrant, Beyza Ermis, Marzieh Fadaee, and Sara Hooker.\nMix data or merge models? optimizing for performance and safety in multilingual contexts. In Neurips\nSafe Generative AI Workshop 2024, 2024. URL https://openreview.net/forum?id=L1Hxp8ktiT.\nArash Ahmadian, Chris Cremer, Matthias Gallé, Marzieh Fadaee, Julia Kreutzer, Olivier Pietquin, Ahmet\nÜstün, and Sara Hooker. Back to basics: Revisiting REINFORCE-style optimization for learning from\nhuman feedback in LLMs. In Lun-Wei Ku, Andre Martins, and Vivek Srikumar (eds.), Proceedings of\nthe 62nd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pp.\n12248–12267, Bangkok, Thailand, August 2024. Association for Computational Linguistics. doi: 10.18653\n/v1/2024.acl-long.662. URL https://aclanthology.org/2024.acl-long.662.\nJoshua Ainslie, James Lee-Thorp, Michiel de Jong, Yury Zemlyanskiy, Federico Lebrón, and Sumit Sanghai.\nGqa: Training generalized multi-query transformer models from multi-head checkpoints, 2023. URL https:\n//arxiv.org/abs/2305.13245.\nViraat Aryabumi, John Dang, Dwarak Talupuru, Saurabh Dash, David Cairuz, Hangyu Lin, Bharat\nVenkitesh, Madeline Smith, Jon Ander Campos, Yi Chern Tan, Kelly Marchisio, Max Bartolo, Sebas-\ntian Ruder, Acyr Locatelli, Julia Kreutzer, Nick Frosst, Aidan Gomez, Phil Blunsom, Marzieh Fadaee,\nAhmet Üstün, and Sara Hooker. Aya 23: Open weight releases to further multilingual progress, 2024. URL\nhttps://arxiv.org/abs/2405.15032.\nJacob Austin, Augustus Odena, Maxwell Nye, Maarten Bosma, Henryk Michalewski, David Dohan, Ellen\nJiang, Carrie Cai, Michael Terry, Quoc Le, and Charles Sutton. Program synthesis with large language\nmodels, 2021. URL https://arxiv.org/abs/2108.07732.\nMohammad Gheshlaghi Azar, Zhaohan Daniel Guo, Bilal Piot, Remi Munos, Mark Rowland, Michal Valko,\nand Daniele Calandriello. A general theoretical paradigm to understand learning from human preferences.\nIn International Conference on Artificial Intelligence and Statistics, pp. 4447–4455. PMLR, 2024.\nYuntao Bai, Andy Jones, Kamal Ndousse, Amanda Askell, Anna Chen, Nova DasSarma, Dawn Drain,\nStanislav Fort, Deep Ganguli, Tom Henighan, Nicholas Joseph, Saurav Kadavath, Jackson Kernion, Tom\nConerly, Sheer El-Showk, Nelson Elhage, Zac Hatfield-Dodds, Danny Hernandez, Tristan Hume, Scott\nJohnston, Shauna Kravec, Liane Lovitt, Neel Nanda, Catherine Olsson, Dario Amodei, Tom Brown, Jack\nClark, Sam McCandlish, Chris Olah, Ben Mann, and Jared Kaplan. Training a helpful and harmless assis-\ntant with reinforcement learning from human feedback, 2022. URL https://arxiv.org/abs/2204.05862.\nYushi Bai, Shangqing Tu, Jiajie Zhang, Hao Peng, Xiaozhi Wang, Xin Lv, Shulin Cao, Jiazheng Xu, Lei\nHou, Yuxiao Dong, Jie Tang, and Juanzi Li. Longbench v2: Towards deeper understanding and reasoning\non realistic long-context multitasks, 2025. URL https://arxiv.org/abs/2412.15204.\nMax Bartolo, Tristan Thrush, Robin Jia, Sebastian Riedel, Pontus Stenetorp, and Douwe Kiela. Improving\nquestion answering model robustness with synthetic adversarial data generation. In Marie-Francine Moens,\nXuanjing Huang, Lucia Specia, and Scott Wen-tau Yih (eds.), Proceedings of the 2021 Conference on\nEmpirical Methods in Natural Language Processing, pp. 8830–8848, Online and Punta Cana, Dominican\nRepublic, November 2021. Association for Computational Linguistics. doi: 10.18653/v1/2021.emnlp-mai\nn.696. URL https://aclanthology.org/2021.emnlp-main.696.\nMeriem Boubdir, Edward Kim, Beyza Ermis, Sara Hooker, and Marzieh Fadaee. Elo uncovered: Robustness\nand best practices in language model evaluation. In Amir Globersons, Lester Mackey, Danielle Belgrave,\nAngela Fan, Ulrich Paquet, Jakub M. Tomczak, and Cheng Zhang (eds.), Advances in Neural Information\nProcessing Systems 38: Annual Conference on Neural Information Processing Systems 2024, NeurIPS\n2024, Vancouver, BC, Canada, December 10 - 15, 2024, 2024. URL http://papers.nips.cc/paper_f\niles/paper/2024/hash/bfba8efb806a970455b83b852c9cf846-Abstract-Conference.html.\nShuaichen Chang and Eric Fosler-Lussier. How to prompt llms for text-to-sql: A study in zero-shot, single-\ndomain, and cross-domain settings, 2023. URL https://arxiv.org/abs/2305.11853.\n39\n\nHongyu Chen and Seraphina Goldfarb-Tarrant. Safer or luckier? llms as safety evaluators are not robust to\nartifacts, 2025. URL https://arxiv.org/abs/2503.09347.\nMark Chen, Jerry Tworek, Heewoo Jun, Qiming Yuan, Henrique Ponde de Oliveira Pinto, Jared Kaplan, Harri\nEdwards, Yuri Burda, Nicholas Joseph, Greg Brockman, Alex Ray, Raul Puri, Gretchen Krueger, Michael\nPetrov, Heidy Khlaaf, Girish Sastry, Pamela Mishkin, Brooke Chan, Scott Gray, Nick Ryder, Mikhail\nPavlov, Alethea Power, Lukasz Kaiser, Mohammad Bavarian, Clemens Winter, Philippe Tillet, Felipe Pet-\nroski Such, Dave Cummings, Matthias Plappert, Fotios Chantzis, Elizabeth Barnes, Ariel Herbert-Voss,\nWilliam Hebgen Guss, Alex Nichol, Alex Paino, Nikolas Tezak, Jie Tang, Igor Babuschkin, Suchir Balaji,\nShantanu Jain, William Saunders, Christopher Hesse, Andrew N. Carr, Jan Leike, Josh Achiam, Vedant\nMisra, Evan Morikawa, Alec Radford, Matthew Knight, Miles Brundage, Mira Murati, Katie Mayer, Pe-\nter Welinder, Bob McGrew, Dario Amodei, Sam McCandlish, Ilya Sutskever, and Wojciech Zaremba.\nEvaluating large language models trained on code, 2021. URL https://arxiv.org/abs/2107.03374.\nWei-Lin Chiang, Lianmin Zheng, Ying Sheng, Anastasios Nikolas Angelopoulos, Tianle Li, Dacheng Li,\nBanghua Zhu, Hao Zhang, Michael I. Jordan, Joseph E. Gonzalez, and Ion Stoica. Chatbot arena: An\nopen platform for evaluating llms by human preference. In Forty-first International Conference on Machine\nLearning, ICML 2024, Vienna, Austria, July 21-27, 2024. OpenReview.net, 2024. URL https://openre\nview.net/forum?id=3MW8GKNyzI.\nEugene Choi, Arash Ahmadian, Matthieu Geist, Olivier Pietquin, and Mohammad Gheshlaghi Azar. Self-\nimproving robust preference optimization. In The Thirteenth International Conference on Learning Rep-\nresentations, 2025. URL https://openreview.net/forum?id=ZSdubdbOoi.\nAakanksha Chowdhery, Sharan Narang, Jacob Devlin, Maarten Bosma, Gaurav Mishra, Adam Roberts, Paul\nBarham, Hyung Won Chung, Charles Sutton, Sebastian Gehrmann, Parker Schuh, Kensen Shi, Sasha\nTsvyashchenko, Joshua Maynez, Abhishek Rao, Parker Barnes, Yi Tay, Noam Shazeer, Vinodkumar Prab-\nhakaran, Emily Reif, Nan Du, Ben Hutchinson, Reiner Pope, James Bradbury, Jacob Austin, Michael Is-\nard, Guy Gur-Ari, Pengcheng Yin, Toju Duke, Anselm Levskaya, Sanjay Ghemawat, Sunipa Dev, Henryk\nMichalewski, Xavier Garcia, Vedant Misra, Kevin Robinson, Liam Fedus, Denny Zhou, Daphne Ippolito,\nDavid Luan, Hyeontaek Lim, Barret Zoph, Alexander Spiridonov, Ryan Sepassi, David Dohan, Shivani\nAgrawal, Mark Omernick, Andrew M. Dai, Thanumalayan Sankaranarayana Pillai, Marie Pellat, Aitor\nLewkowycz, Erica Moreira, Rewon Child, Oleksandr Polozov, Katherine Lee, Zongwei Zhou, Xuezhi Wang,\nBrennan Saeta, Mark Diaz, Orhan Firat, Michele Catasta, Jason Wei, Kathy Meier-Hellstern, Douglas Eck,\nJeff Dean, Slav Petrov, and Noah Fiedel. Palm: Scaling language modeling with pathways. Journal of\nMachine Learning Research, 24(240):1–113, 2023. URL http://jmlr.org/papers/v24/22-1144.html.\nJohn Dang, Shivalika Singh, Daniel D’souza, Arash Ahmadian, Alejandro Salamanca, Madeline Smith,\nAidan Peppin, Sungjin Hong, Manoj Govindassamy, Terrence Zhao, Sandra Kublik, Meor Amer, Viraat\nAryabumi, Jon Ander Campos, Yi-Chern Tan, Tom Kocmi, Florian Strub, Nathan Grinsztajn, Yannis\nFlet-Berliac, Acyr Locatelli, Hangyu Lin, Dwarak Talupuru, Bharat Venkitesh, David Cairuz, Bowen\nYang, Tim Chung, Wei-Yin Ko, Sylvie Shang Shi, Amir Shukayev, Sammie Bae, Aleksandra Piktus, Ro-\nman Castagné, Felipe Cruz-Salinas, Eddie Kim, Lucas Crawhall-Stein, Adrien Morisot, Sudip Roy, Phil\nBlunsom, Ivan Zhang, Aidan Gomez, Nick Frosst, Marzieh Fadaee, Beyza Ermis, Ahmet Üstün, and Sara\nHooker. Aya expanse: Combining research breakthroughs for a new multilingual frontier, 2024. URL\nhttps://arxiv.org/abs/2412.04261.\nDheeru Dua, Yizhong Wang, Pradeep Dasigi, Gabriel Stanovsky, Sameer Singh, and Matt Gardner. DROP:\nA reading comprehension benchmark requiring discrete reasoning over paragraphs. In Jill Burstein, Christy\nDoran, and Thamar Solorio (eds.), Proceedings of the 2019 Conference of the North American Chapter of\nthe Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long and Short\nPapers), pp. 2368–2378, Minneapolis, Minnesota, June 2019. Association for Computational Linguistics.\ndoi: 10.18653/v1/N19-1246. URL https://aclanthology.org/N19-1246.\nAbhimanyu Dubey, Abhinav Jauhri, Abhinav Pandey, Abhishek Kadian, Ahmad Al-Dahle, Aiesha Letman,\nAkhil Mathur, Alan Schelten, Amy Yang, Angela Fan, et al. The llama 3 herd of models. arXiv preprint\narXiv:2407.21783, 2024.\n40\n\nJacob Eisenstein, Chirag Nagpal, Alekh Agarwal, Ahmad Beirami, Alexander Nicholas D’Amour, Krish-\nnamurthy Dj Dvijotham, Adam Fisch, Katherine A Heller, Stephen Robert Pfohl, Deepak Ramachan-\ndran, Peter Shaw, and Jonathan Berant.\nHelping or herding? reward model ensembles mitigate but\ndo not eliminate reward hacking.\nIn First Conference on Language Modeling, 2024.\nURL https:\n//openreview.net/forum?id=5u1GpUkKtG.\nChristian Federmann, Tom Kocmi, and Ying Xin. NTREX-128 – news test references for MT evaluation\nof 128 languages.\nIn Kabir Ahuja, Antonios Anastasopoulos, Barun Patra, Graham Neubig, Monojit\nChoudhury, Sandipan Dandapat, Sunayana Sitaram, and Vishrav Chaudhary (eds.), Proceedings of the\nFirst Workshop on Scaling Up Multilingual Evaluation, pp. 21–24, Online, November 2022. Association for\nComputational Linguistics. URL https://aclanthology.org/2022.sumeval-1.4.\nMaxim Fishman, Brian Chmiel, Ron Banner, and Daniel Soudry. Scaling fp8 training to trillion-token llms,\n2025. URL https://arxiv.org/abs/2409.12517.\nYannis Flet-Berliac, Nathan Grinsztajn, Florian Strub, Eugene Choi, Bill Wu, Chris Cremer, Arash Ahma-\ndian, Yash Chandak, Mohammad Gheshlaghi Azar, Olivier Pietquin, and Matthieu Geist. Contrastive\npolicy gradient: Aligning LLMs on sequence-level scores in a supervised-friendly fashion. In Yaser Al-\nOnaizan, Mohit Bansal, and Yun-Nung Chen (eds.), Proceedings of the 2024 Conference on Empirical\nMethods in Natural Language Processing, pp. 21353–21370, Miami, Florida, USA, November 2024. Asso-\nciation for Computational Linguistics. URL https://aclanthology.org/2024.emnlp-main.1190.\nMarkus Freitag, Nitika Mathur, Chi-kiu Lo, Eleftherios Avramidis, Ricardo Rei, Brian Thompson, Tom\nKocmi, Frederic Blain, Daniel Deutsch, Craig Stewart, Chrysoula Zerva, Sheila Castilho, Alon Lavie,\nand George Foster. Results of WMT23 metrics shared task: Metrics might be guilty but references are\nnot innocent. In Philipp Koehn, Barry Haddow, Tom Kocmi, and Christof Monz (eds.), Proceedings of\nthe Eighth Conference on Machine Translation, pp. 578–628, Singapore, December 2023. Association for\nComputational Linguistics. doi: 10.18653/v1/2023.wmt-1.51. URL https://aclanthology.org/2023.\nwmt-1.51.\nRoy Frostig, Matthew Johnson, and Chris Leary. Compiling machine learning programs via high-level tracing,\n2018. URL https://mlsys.org/Conferences/doc/2018/146.pdf.\nYao Fu, Rameswar Panda, Xinyao Niu, Xiang Yue, Hannaneh Hajishirzi, Yoon Kim, and Hao Peng. Data en-\ngineering for scaling language models to 128k context. In Proceedings of the 41st International Conference\non Machine Learning, ICML’24. JMLR.org, 2024.\nJonas Gehring, Kunhao Zheng, Jade Copet, Vegard Mella, Quentin Carbonneaux, Taco Cohen, and Gabriel\nSynnaeve.\nRlef: Grounding code llms in execution feedback with reinforcement learning, 2025.\nURL\nhttps://arxiv.org/abs/2410.02089.\nMor Geva, Daniel Khashabi, Elad Segal, Tushar Khot, Dan Roth, and Jonathan Berant.\nDid aristotle\nuse a laptop? a question answering benchmark with implicit reasoning strategies. Transactions of the\nAssociation for Computational Linguistics, 9:346–361, 2021. doi: 10.1162/tacl_a_00370. URL https:\n//aclanthology.org/2021.tacl-1.21.\nNaman Goyal, Cynthia Gao, Vishrav Chaudhary, Peng-Jen Chen, Guillaume Wenzek, Da Ju, Sanjana Kr-\nishnan, Marc’Aurelio Ranzato, Francisco Guzmán, and Angela Fan. The Flores-101 evaluation benchmark\nfor low-resource and multilingual machine translation. Transactions of the Association for Computational\nLinguistics, 10:522–538, 2022. doi: 10.1162/tacl_a_00474. URL https://aclanthology.org/2022.ta\ncl-1.30.\nNathan Grinsztajn, Yannis Flet-Berliac, Mohammad Gheshlaghi Azar, Florian Strub, Bill Wu, Eugene Choi,\nChris Cremer, Arash Ahmadian, Yash Chandak, Olivier Pietquin, and Matthieu Geist. Averaging log-\nlikelihoods in direct alignment. arXiv preprint arXiv:2406.19188, 2024.\nTom Gunter, Zirui Wang, Chong Wang, Ruoming Pang, Andy Narayanan, Aonan Zhang, Bowen Zhang,\nChen Chen, Chung-Cheng Chiu, David Qiu, Deepak Gopinath, Dian Ang Yap, Dong Yin, Feng Nan,\n41\n\nFloris Weers, Guoli Yin, Haoshuo Huang, Jianyu Wang, Jiarui Lu, John Peebles, Ke Ye, Mark Lee, Nan\nDu, Qibin Chen, Quentin Keunebroek, Sam Wiseman, Syd Evans, Tao Lei, Vivek Rathod, Xiang Kong,\nXianzhi Du, Yanghao Li, Yongqiang Wang, Yuan Gao, Zaid Ahmed, Zhaoyang Xu, Zhiyun Lu, Al Rashid,\nAlbin Madappally Jose, Alec Doane, Alfredo Bencomo, Allison Vanderby, Andrew Hansen, Ankur Jain,\nAnupama Mann Anupama, Areeba Kamal, Bugu Wu, Carolina Brum, Charlie Maalouf, Chinguun Er-\ndenebileg, Chris Dulhanty, Dominik Moritz, Doug Kang, Eduardo Jimenez, Evan Ladd, Fangping Shi, Felix\nBai, Frank Chu, Fred Hohman, Hadas Kotek, Hannah Gillis Coleman, Jane Li, Jeffrey Bigham, Jeffery Cao,\nJeff Lai, Jessica Cheung, Jiulong Shan, Joe Zhou, John Li, Jun Qin, Karanjeet Singh, Karla Vega, Kelvin\nZou, Laura Heckman, Lauren Gardiner, Margit Bowler, Maria Cordell, Meng Cao, Nicole Hay, Nilesh\nShahdadpuri, Otto Godwin, Pranay Dighe, Pushyami Rachapudi, Ramsey Tantawi, Roman Frigg, Sam\nDavarnia, Sanskruti Shah, Saptarshi Guha, Sasha Sirovica, Shen Ma, Shuang Ma, Simon Wang, Sulgi Kim,\nSuma Jayaram, Vaishaal Shankar, Varsha Paidi, Vivek Kumar, Xin Wang, Xin Zheng, Walker Cheng, Yael\nShrager, Yang Ye, Yasu Tanaka, Yihao Guo, Yunsong Meng, Zhao Tang Luo, Zhi Ouyang, Alp Aygar, Alvin\nWan, Andrew Walkingshaw, Andy Narayanan, Antonie Lin, Arsalan Farooq, Brent Ramerth, Colorado\nReed, Chris Bartels, Chris Chaney, David Riazati, Eric Liang Yang, Erin Feldman, Gabriel Hochstrasser,\nGuillaume Seguin, Irina Belousova, Joris Pelemans, Karen Yang, Keivan Alizadeh Vahid, Liangliang Cao,\nMahyar Najibi, Marco Zuliani, Max Horton, Minsik Cho, Nikhil Bhendawade, Patrick Dong, Piotr Maj,\nPulkit Agrawal, Qi Shan, Qichen Fu, Regan Poston, Sam Xu, Shuangning Liu, Sushma Rao, Tashweena\nHeeramun, Thomas Merth, Uday Rayala, Victor Cui, Vivek Rangarajan Sridhar, Wencong Zhang, Wenqi\nZhang, Wentao Wu, Xingyu Zhou, Xinwen Liu, Yang Zhao, Yin Xia, Zhile Ren, and Zhongzheng Ren.\nApple Intelligence Foundation Language Models, 2024. URL https://arxiv.org/abs/2407.21075.\nDaya Guo, Qihao Zhu, Dejian Yang, Zhenda Xie, Kai Dong, Wentao Zhang, Guanting Chen, Xiao Bi,\nY Wu, YK Li, et al. Deepseek-coder: When the large language model meets programming–the rise of code\nintelligence. arXiv preprint arXiv:2401.14196, 2024.\nDaya Guo, Dejian Yang, Haowei Zhang, Junxiao Song, Ruoyu Zhang, Runxin Xu, Qihao Zhu, Shirong Ma,\nPeiyi Wang, Xiao Bi, et al.\nDeepseek-r1: Incentivizing reasoning capability in llms via reinforcement\nlearning. arXiv preprint arXiv:2501.12948, 2025.\nDan Hendrycks, Steven Basart, Saurav Kadavath, Mantas Mazeika, Akul Arora, Ethan Guo, Collin Burns,\nSamir Puranik, Horace He, Dawn Song, et al. Measuring coding challenge competence with apps. arXiv\npreprint arXiv:2105.09938, 2021.\nTom Hosking, Phil Blunsom, and Max Bartolo. Human feedback is not gold standard. In The Twelfth\nInternational Conference on Learning Representations, 2024. URL https://openreview.net/forum?i\nd=7W3GLNImfS.\nCheng-Ping Hsieh, Simeng Sun, Samuel Kriman, Shantanu Acharya, Dima Rekesh, Fei Jia, Yang Zhang, and\nBoris Ginsburg. Ruler: What’s the real context size of your long-context language models?, 2024. URL\nhttps://arxiv.org/abs/2404.06654.\nBinyuan Hui, Jian Yang, Zeyu Cui, Jiaxi Yang, Dayiheng Liu, Lei Zhang, Tianyu Liu, Jiajun Zhang, Bowen\nYu, Keming Lu, Kai Dang, Yang Fan, Yichang Zhang, An Yang, Rui Men, Fei Huang, Bo Zheng, Yibo\nMiao, Shanghaoran Quan, Yunlong Feng, Xingzhang Ren, Xuancheng Ren, Jingren Zhou, and Junyang\nLin. Qwen2.5-coder technical report, 2024. URL https://arxiv.org/abs/2409.12186.\nGabriel Ilharco, Marco Tulio Ribeiro, Mitchell Wortsman, Ludwig Schmidt, Hannaneh Hajishirzi, and Ali\nFarhadi. Editing models with task arithmetic. In The Eleventh International Conference on Learning\nRepresentations, 2023. URL https://openreview.net/forum?id=6t0Kwf8-jrj.\nPranab Islam, Anand Kannappan, Douwe Kiela, Rebecca Qian, Nino Scherrer, and Bertie Vidgen.\nFi-\nnancebench: A new benchmark for financial question answering. arXiv preprint arXiv:2311.11944, 2023.\nPavel Izmailov, Dmitrii Podoprikhin, Timur Garipov, Dmitry Vetrov, and Andrew Gordon Wilson. Averaging\nweights leads to wider optima and better generalization. In 34th Conference on Uncertainty in Artificial\nIntelligence 2018, UAI 2018, pp. 876–885. Association For Uncertainty in Artificial Intelligence (AUAI),\n2018.\n42\n\nNaman Jain, King Han, Alex Gu, Wen-Ding Li, Fanjia Yan, Tianjun Zhang, Sida Wang, Armando Solar-\nLezama, Koushik Sen, and Ion Stoica. Livecodebench: Holistic and contamination free evaluation of large\nlanguage models for code. arXiv preprint arXiv:2403.07974, 2024.\nCarlos E Jimenez, John Yang, Alexander Wettig, Shunyu Yao, Kexin Pei, Ofir Press, and Karthik R\nNarasimhan. SWE-bench: Can language models resolve real-world github issues?\nIn The Twelfth In-\nternational Conference on Learning Representations, 2024. URL https://openreview.net/forum?id=\nVTF8yNQM66.\nAmirhossein Kazemnejad, Inkit Padhi, Karthikeyan Natesan Ramamurthy, Payel Das, and Siva Reddy.\nThe impact of positional encoding on length generalization in transformers.\nIn A. Oh, T. Naumann,\nA. Globerson, K. Saenko, M. Hardt, and S. Levine (eds.), Advances in Neural Information Processing\nSystems, volume 36, pp. 24892–24928. Curran Associates, Inc., 2023. URL https://proceedings.neurip\ns.cc/paper_files/paper/2023/file/4e85362c02172c0c6567ce593122d31c-Paper-Conference.pdf.\nMuhammad Khalifa, Yi-Chern Tan, Arash Ahmadian, Tom Hosking, Honglak Lee, Lu Wang, Ahmet Üstün,\nTom Sherborne, and Matthias Gallé. If you can’t use them, recycle them: Optimizing merging at scale\nmitigates performance tradeoffs, 2025. URL https://arxiv.org/abs/2412.04144.\nDouwe Kiela, Max Bartolo, Yixin Nie, Divyansh Kaushik, Atticus Geiger, Zhengxuan Wu, Bertie Vid-\ngen, Grusha Prasad, Amanpreet Singh, Pratik Ringshia, Zhiyi Ma, Tristan Thrush, Sebastian Riedel,\nZeerak Waseem, Pontus Stenetorp, Robin Jia, Mohit Bansal, Christopher Potts, and Adina Williams.\nDynabench: Rethinking benchmarking in NLP. In Kristina Toutanova, Anna Rumshisky, Luke Zettle-\nmoyer, Dilek Hakkani-Tur, Iz Beltagy, Steven Bethard, Ryan Cotterell, Tanmoy Chakraborty, and\nYichao Zhou (eds.), Proceedings of the 2021 Conference of the North American Chapter of the As-\nsociation for Computational Linguistics: Human Language Technologies, pp. 4110–4124, Online, June\n2021. Association for Computational Linguistics.\ndoi: 10.18653/v1/2021.naacl-main.324.\nURL\nhttps://aclanthology.org/2021.naacl-main.324.\nHannah Rose Kirk, Alexander Whitefield, Paul Röttger, Andrew Michael Bean, Katerina Margatina, Rafael\nMosquera, Juan Manuel Ciro, Max Bartolo, Adina Williams, He He, Bertie Vidgen, and Scott A. Hale.\nThe PRISM alignment dataset: What participatory, representative and individualised human feedback\nreveals about the subjective and multicultural alignment of large language models. In The Thirty-eight\nConference on Neural Information Processing Systems Datasets and Benchmarks Track, 2024. URL https:\n//openreview.net/forum?id=DFr5hteojx.\nJames Kirkpatrick, Razvan Pascanu, Neil Rabinowitz, Joel Veness, Guillaume Desjardins, Andrei A Rusu,\nKieran Milan, John Quan, Tiago Ramalho, Agnieszka Grabska-Barwinska, et al. Overcoming catastrophic\nforgetting in neural networks. Proceedings of the national academy of sciences, 114(13):3521–3526, 2017.\nTom Kocmi, Vilém Zouhar, Christian Federmann, and Matt Post. Navigating the metrics maze: Reconciling\nscore magnitudes and accuracies. In Lun-Wei Ku, Andre Martins, and Vivek Srikumar (eds.), Proceedings\nof the 62nd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers),\npp. 1999–2014, Bangkok, Thailand, August 2024. Association for Computational Linguistics. doi: 10.186\n53/v1/2024.acl-long.110. URL https://aclanthology.org/2024.acl-long.110.\nNathan Lambert, Valentina Pyatkin, Jacob Morrison, LJ Miranda, Bill Yuchen Lin, Khyathi Chandu, Nouha\nDziri, Sachin Kumar, Tom Zick, Yejin Choi, Noah A. Smith, and Hannaneh Hajishirzi. RewardBench:\nEvaluating Reward Models for Language Modeling, 2024. URL https://arxiv.org/abs/2403.13787.\nPatrick Lewis, Ethan Perez, Aleksandra Piktus, Fabio Petroni, Vladimir Karpukhin, Naman Goyal, Heinrich\nKüttler, Mike Lewis, Wen-tau Yih, Tim Rocktäschel, Sebastian Riedel, and Douwe Kiela.\nRetrieval-\naugmented generation for knowledge-intensive nlp tasks. In H. Larochelle, M. Ranzato, R. Hadsell, M.F.\nBalcan, and H. Lin (eds.), Advances in Neural Information Processing Systems, volume 33, pp. 9459–9474.\nCurran Associates, Inc., 2020. URL https://proceedings.neurips.cc/paper_files/paper/2020/fi\nle/6b493230205f780e1bc26945df7481e5-Paper.pdf.\n43\n\nJinyang Li, Binyuan Hui, Ge Qu, Jiaxi Yang, Binhua Li, Bowen Li, Bailin Wang, Bowen Qin, Ruiying Geng,\nNan Huo, Xuanhe Zhou, Chenhao Ma, Guoliang Li, Kevin C.C. Chang, Fei Huang, Reynold Cheng, and\nYongbin Li. Can llm already serve as a database interface? a big bench for large-scale database grounded\ntext-to-sqls. In Proceedings of the 37th International Conference on Neural Information Processing Sys-\ntems, NIPS ’23, Red Hook, NY, USA, 2023a. Curran Associates Inc.\nMargaret Li, Suchin Gururangan, Tim Dettmers, Mike Lewis, Tim Althoff, Noah A. Smith, and Luke Zettle-\nmoyer. Branch-train-merge: Embarrassingly parallel training of expert language models. In First Workshop\non Interpolation Regularizers and Beyond at NeurIPS 2022, 2022. URL https://openreview.net/for\num?id=SQgVgE2Sq4.\nShenggui Li, Fuzhao Xue, Chaitanya Baranwal, Yongbin Li, and Yang You. Sequence parallelism: Long\nsequence training from system perspective. In Anna Rogers, Jordan Boyd-Graber, and Naoaki Okazaki\n(eds.), Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics (Volume\n1: Long Papers), pp. 2391–2404, Toronto, Canada, July 2023b. Association for Computational Linguistics.\ndoi: 10.18653/v1/2023.acl-long.134. URL https://aclanthology.org/2023.acl-long.134.\nAixin Liu, Bei Feng, Bing Xue, Bingxuan Wang, Bochao Wu, Chengda Lu, Chenggang Zhao, Chengqi Deng,\nChenyu Zhang, Chong Ruan, et al. Deepseek-v3 technical report. arXiv preprint 2412.19437, 2024a.\nJiawei Liu, Jia Le Tian, Vijay Daita, Yuxiang Wei, Yifeng Ding, Yuhan Katherine Wang, Jun Yang, and\nLingming Zhang. Repoqa: Evaluating long context code understanding. arXiv preprint arXiv:2406.06025,\n2024b.\nJiawei Liu, Chunqiu Steven Xia, Yuyao Wang, and Lingming Zhang. Is your code generated by chatgpt really\ncorrect? rigorous evaluation of large language models for code generation. Advances in Neural Information\nProcessing Systems, 36, 2024c.\nZihan Liu, Wei Ping, Rajarshi Roy, Peng Xu, Chankyu Lee, Mohammad Shoeybi, and Bryan Catanzaro.\nChatqa: Surpassing gpt-4 on conversational qa and rag. arXiv preprint arXiv:2401.10225, 2024d.\nIlya Loshchilov and Frank Hutter. Fixing weight decay regularization in adam. CoRR, abs/1711.05101,\n2017. URL http://arxiv.org/abs/1711.05101.\nKelly Marchisio, Wei-Yin Ko, Alexandre Berard, Théo Dehaze, and Sebastian Ruder. Understanding and\nmitigating language confusion in LLMs. In Yaser Al-Onaizan, Mohit Bansal, and Yun-Nung Chen (eds.),\nProceedings of the 2024 Conference on Empirical Methods in Natural Language Processing, pp. 6653–\n6677, Miami, Florida, USA, November 2024. Association for Computational Linguistics. URL https:\n//aclanthology.org/2024.emnlp-main.380.\nMichael Matena and Colin Raffel. Merging models with fisher-weighted averaging. In Proceedings of the 36th\nInternational Conference on Neural Information Processing Systems, NIPS ’22, Red Hook, NY, USA, 2022.\nCurran Associates Inc. ISBN 9781713871088.\nAlexandre Matton, Tom Sherborne, Dennis Aumiller, Elena Tommasone, Milad Alizadeh, Jingyi He, Ray-\nmond Ma, Maxime Voisin, Ellen Gilsenan-McMahon, and Matthias Gallé. On leakage of code generation\nevaluation datasets. In Yaser Al-Onaizan, Mohit Bansal, and Yun-Nung Chen (eds.), Findings of the As-\nsociation for Computational Linguistics: EMNLP 2024, pp. 13215–13223, Miami, Florida, USA, November\n2024. Association for Computational Linguistics. URL https://aclanthology.org/2024.findings-e\nmnlp.772.\nPaulius Micikevicius, Dusan Stosic, Neil Burgess, Marius Cornea, Pradeep Dubey, Richard Grisenthwaite,\nSangwon Ha, Alexander Heinecke, Patrick Judd, John Kamalu, Naveen Mellempudi, Stuart Oberman,\nMohammad Shoeybi, Michael Siu, and Hao Wu.\nFp8 formats for deep learning, 2022.\nURL https:\n//arxiv.org/abs/2209.05433.\nNiklas Muennighoff, Qian Liu, Armel Zebaze, Qinkai Zheng, Binyuan Hui, Terry Yue Zhuo, Swayam Singh,\nXiangru Tang, Leandro Von Werra, and Shayne Longpre. Octopack: Instruction tuning code large language\nmodels. arXiv preprint arXiv:2308.07124, 2023.\n44\n\nJinjie Ni, Fuzhao Xue, Xiang Yue, Yuntian Deng, Mahir Shah, Kabir Jain, Graham Neubig, and Yang\nYou.\nMixeval: Deriving wisdom of the crowd from LLM benchmark mixtures.\nIn Amir Globersons,\nLester Mackey, Danielle Belgrave, Angela Fan, Ulrich Paquet, Jakub M. Tomczak, and Cheng Zhang\n(eds.), Advances in Neural Information Processing Systems 38: Annual Conference on Neural Information\nProcessing Systems 2024, NeurIPS 2024, Vancouver, BC, Canada, December 10 - 15, 2024, 2024. URL\nhttp://papers.nips.cc/paper_files/paper/2024/hash/b1f34d7b4a03a3d80be8e72eb430dd81-Abs\ntract-Conference.html.\nAyomide Odumakinde, Daniel D’souza, Pat Verga, Beyza Ermis, and Sara Hooker. Multilingual arbitrage:\nOptimizing data pools to accelerate multilingual progress, 2024. URL https://arxiv.org/abs/2408.1\n4960.\nB. T. Polyak and A. B. Juditsky. Acceleration of stochastic approximation by averaging. SIAM Journal on\nControl and Optimization, 30(4):838–855, 1992. doi: 10.1137/0330046. URL https://doi.org/10.1137/\n0330046.\nReiner Pope, Sholto Douglas, Aakanksha Chowdhery, Jacob Devlin, James Bradbury, Jonathan Heek, Kefan\nXiao, Shivani Agrawal, and Jeff Dean. Efficiently scaling transformer inference. In D. Song, M. Carbin,\nand T. Chen (eds.), Proceedings of Machine Learning and Systems, volume 5, pp. 606–624. Curan, 2023.\nURL https://proceedings.mlsys.org/paper_files/paper/2023/file/c4be71ab8d24cdfb45e3d06d\nbfca2780-Paper-mlsys2023.pdf.\nOfir Press, Muru Zhang, Sewon Min, Ludwig Schmidt, Noah A Smith, and Mike Lewis. Measuring and\nnarrowing the compositionality gap in language models. arXiv preprint arXiv:2210.03350, 2022.\nYiwei Qin, Kaiqiang Song, Yebowen Hu, Wenlin Yao, Sangwoo Cho, Xiaoyang Wang, Xuansheng Wu, Fei\nLiu, Pengfei Liu, and Dong Yu. InFoBench: Evaluating instruction following ability in large language\nmodels.\nIn Lun-Wei Ku, Andre Martins, and Vivek Srikumar (eds.), Findings of the Association for\nComputational Linguistics: ACL 2024, pp. 13025–13048, Bangkok, Thailand, August 2024. Association\nfor Computational Linguistics. doi: 10.18653/v1/2024.findings-acl.772. URL https://aclanthology.o\nrg/2024.findings-acl.772.\nRafael Rafailov, Archit Sharma, Eric Mitchell, Christopher D Manning, Stefano Ermon, and Chelsea Finn.\nDirect preference optimization: Your language model is secretly a reward model.\nAdvances in Neural\nInformation Processing Systems, 36, 2024.\nAlexandre Ramé, Johan Ferret, Nino Vieillard, Robert Dadashi, Léonard Hussenot, Pierre-Louis Cedoz,\nPier Giuseppe Sessa, Sertan Girgin, Arthur Douillard, and Olivier Bachem. Warp: On the benefits of\nweight averaged rewarded policies, 2024. URL https://arxiv.org/abs/2406.16768.\nRicardo Rei, Craig Stewart, Ana C Farinha, and Alon Lavie. COMET: A neural framework for MT eval-\nuation. In Bonnie Webber, Trevor Cohn, Yulan He, and Yang Liu (eds.), Proceedings of the 2020 Con-\nference on Empirical Methods in Natural Language Processing (EMNLP), pp. 2685–2702, Online, Novem-\nber 2020. Association for Computational Linguistics. doi: 10.18653/v1/2020.emnlp-main.213. URL\nhttps://aclanthology.org/2020.emnlp-main.213.\nDavid Rein, Betty Li Hou, Asa Cooper Stickland, Jackson Petty, Richard Yuanzhe Pang, Julien Dirani,\nJulian Michael, and Samuel R Bowman. Gpqa: A graduate-level google-proof q&a benchmark. arXiv\npreprint arXiv:2311.12022, 2023.\nNathaniel R Robinson, Shahd Abdelmoneim, Kelly Marchisio, and Sebastian Ruder. Al-qasida: Analyzing\nllm quality and accuracy systematically in dialectal arabic. arXiv preprint arXiv:2412.04193, 2024.\nAngelika Romanou, Negar Foroutan, Anna Sotnikova, Sree Harsha Nelaturu, Shivalika Singh, Rishabh Ma-\nheshwary, Micol Altomare, Zeming Chen, Mohamed A. Haggag, Snegha A, Alfonso Amayuelas, Azril Hafizi\nAmirudin, Danylo Boiko, Michael Chang, Jenny Chim, Gal Cohen, Aditya Kumar Dalmia, Abraham\nDiress, Sharad Duwal, Daniil Dzenhaliou, Daniel Fernando Erazo Florez, Fabian Farestam, Joseph Mar-\nvin Imperial, Shayekh Bin Islam, Perttu Isotalo, Maral Jabbarishiviari, Börje F. Karlsson, Eldar Khalilov,\n45\n\nChristopher Klamm, Fajri Koto, Dominik Krzemiński, Gabriel Adriano de Melo, Syrielle Montariol, Yiyang\nNan, Joel Niklaus, Jekaterina Novikova, Johan Samir Obando Ceron, Debjit Paul, Esther Ploeger, Je-\nbish Purbey, Swati Rajwal, Selvan Sunitha Ravi, Sara Rydell, Roshan Santhosh, Drishti Sharma, Mar-\njana Prifti Skenduli, Arshia Soltani Moakhar, Bardia soltani moakhar, Ayush Kumar Tarun, Azmine Tou-\nshik Wasi, Thenuka Ovin Weerasinghe, Serhan Yilmaz, Mike Zhang, Imanol Schlag, Marzieh Fadaee,\nSara Hooker, and Antoine Bosselut. INCLUDE: Evaluating multilingual language understanding with\nregional knowledge. In The Thirteenth International Conference on Learning Representations, 2025. URL\nhttps://openreview.net/forum?id=k3gCieTXeY.\nPaul Röttger, Hannah Kirk, Bertie Vidgen, Giuseppe Attanasio, Federico Bianchi, and Dirk Hovy. XSTest:\nA test suite for identifying exaggerated safety behaviours in large language models. In Kevin Duh, Helena\nGomez, and Steven Bethard (eds.), Proceedings of the 2024 Conference of the North American Chapter of\nthe Association for Computational Linguistics: Human Language Technologies (Volume 1: Long Papers),\npp. 5377–5400, Mexico City, Mexico, June 2024. Association for Computational Linguistics. doi: 10.18653\n/v1/2024.naacl-long.301. URL https://aclanthology.org/2024.naacl-long.301.\nDavid Ruppert. Efficient estimations from a slowly convergent robbins-monro process. Technical report,\nCornell University Operations Research and Industrial Engineering, 1988.\nVictor Sanh, Albert Webson, Colin Raffel, Stephen H Bach, Lintang Sutawika, Zaid Alyafeai, Antoine Chaffin,\nArnaud Stiegler, Teven Le Scao, Arun Raja, et al. Multitask prompted training enables zero-shot task\ngeneralization. arXiv preprint arXiv:2110.08207, 2021.\nRylan Schaeffer, Punit Singh Koura, Binh Tang, Ranjan Subramanian, Aaditya K Singh, Todor Mihaylov,\nPrajjwal Bhargava, Lovish Madaan, Niladri S. Chatterji, Vedanuj Goswami, Sergey Edunov, Dieuwke\nHupkes, Sanmi Koyejo, and Sharan Narang. Correlating and predicting human evaluations of language\nmodels from natural language processing benchmarks, 2025. URL https://arxiv.org/abs/2502.18339.\nTimo Schick, Jane Dwivedi-Yu, Roberto Dessi, Roberta Raileanu, Maria Lomeli, Eric Hambro, Luke Zettle-\nmoyer, Nicola Cancedda, and Thomas Scialom. Toolformer: Language models can teach themselves to\nuse tools. In A. Oh, T. Naumann, A. Globerson, K. Saenko, M. Hardt, and S. Levine (eds.), Advances in\nNeural Information Processing Systems, volume 36, pp. 68539–68551. Curran Associates, Inc., 2023. URL\nhttps://proceedings.neurips.cc/paper_files/paper/2023/file/d842425e4bf79ba039352da0f65\n8a906-Paper-Conference.pdf.\nPreethi Seshadri and Seraphina Goldfarb-Tarrant. Who does the giant number pile like best: Analyzing\nfairness in hiring contexts, 2025. URL https://arxiv.org/abs/2501.04316.\nZhihong Shao, Peiyi Wang, Qihao Zhu, Runxin Xu, Junxiao Song, Xiao Bi, Haowei Zhang, Mingchuan Zhang,\nYK Li, Y Wu, et al. Deepseekmath: Pushing the limits of mathematical reasoning in open language models.\narXiv preprint arXiv:2402.03300, 2024.\nNoam Shazeer. GLU variants improve transformer. arXiv preprint arXiv:2002.05202, 2020.\nFreda Shi, Mirac Suzgun, Markus Freitag, Xuezhi Wang, Suraj Srivats, Soroush Vosoughi, Hyung Won\nChung, Yi Tay, Sebastian Ruder, Denny Zhou, Dipanjan Das, and Jason Wei.\nLanguage models are\nmultilingual chain-of-thought reasoners, 2022.\nMohammad Shoeybi, Mostofa Patwary, Raul Puri, Patrick LeGresley, Jared Casper, and Bryan Catan-\nzaro. Megatron-lm: Training multi-billion parameter language models using model parallelism. ArXiv,\nabs/1909.08053, 2019. URL https://api.semanticscholar.org/CorpusID:202660670.\nNisan Stiennon, Long Ouyang, Jeff Wu, Daniel M. Ziegler, Ryan Lowe, Chelsea Voss, Alec Radford, Dario\nAmodei, and Paul Christiano. Learning to summarize from human feedback. In Proceedings of the 34th\nInternational Conference on Neural Information Processing Systems, NIPS ’20, Red Hook, NY, USA, 2020.\nCurran Associates Inc. ISBN 9781713829546.\nJianlin Su, Yu Lu, Shengfeng Pan, Bo Wen, and Yunfeng Liu. Roformer: Enhanced transformer with rotary\nposition embedding, 2021.\n46\n\nAdam Sutton, Almog Simchon, Matthew Edwards, and Stephan Lewandowsky. You are what you read:\nInferring personality from consumed textual content. In Jeremy Barnes, Orphée De Clercq, and Roman\nKlinger (eds.), Proceedings of the 13th Workshop on Computational Approaches to Subjectivity, Senti-\nment, & Social Media Analysis, pp. 28–38, Toronto, Canada, July 2023. Association for Computational\nLinguistics. doi: 10.18653/v1/2023.wassa-1.4. URL https://aclanthology.org/2023.wassa-1.4.\nGemma Team, Morgane Riviere, Shreya Pathak, Pier Giuseppe Sessa, Cassidy Hardin, Surya Bhupatiraju,\nLéonard Hussenot, Thomas Mesnard, Bobak Shahriari, Alexandre Ramé, et al. Gemma 2: Improving open\nlanguage models at a practical size, 2024. URL https://arxiv. org/abs/2408.00118, 1(3), 2024.\nKimi Team, Angang Du, Bofei Gao, Bowei Xing, Changjiu Jiang, Cheng Chen, Cheng Li, Chenjun Xiao,\nChenzhuang Du, Chonghua Liao, et al. Kimi k1. 5: Scaling reinforcement learning with llms. arXiv preprint\narXiv:2501.12599, 2025.\nNLLB Team, Marta Ruiz Costa-jussà, James Cross, Onur Çelebi, Maha Elbayad, Kenneth Heafield, Kevin\nHeffernan, Elahe Kalbassi, Janice Lam, Daniel Licht, Jean Maillard, Anna Sun, Skyler Wang, Guillaume\nWenzek, Alison Youngblood, Bapi Akula, Loïc Barrault, Gabriel Mejia Gonzalez, Prangthip Hansanti,\nJohn Hoffman, Semarley Jarrett, Kaushik Ram Sadagopan, Dirk Rowe, Shannon L. Spruit, C. Tran,\nPierre Yves Andrews, Necip Fazil Ayan, Shruti Bhosale, Sergey Edunov, Angela Fan, Cynthia Gao, Vedanuj\nGoswami, Francisco Guzmán, Philipp Koehn, Alexandre Mourachko, Christophe Ropers, Safiyyah Saleem,\nHolger Schwenk, and Jeff Wang. No language left behind: Scaling human-centered machine translation.\nArXiv, abs/2207.04672, 2022. URL https://api.semanticscholar.org/CorpusID:250425961.\nShubham Toshniwal, Wei Du, Ivan Moshkov, Branislav Kisacanin, Alexan Ayrapetyan, and Igor Gitman.\nOpenmathinstruct-2: Accelerating ai for math with massive open-source instruction data. arXiv preprint\narXiv:2410.01560, 2024.\nAhmet Üstün, Viraat Aryabumi, Zheng Yong, Wei-Yin Ko, Daniel D’souza, Gbemileke Onilude, Neel Bhan-\ndari, Shivalika Singh, Hui-Lee Ooi, Amr Kayid, Freddie Vargus, Phil Blunsom, Shayne Longpre, Niklas\nMuennighoff, Marzieh Fadaee, Julia Kreutzer, and Sara Hooker. Aya model: An instruction finetuned\nopen-access multilingual language model. In Lun-Wei Ku, Andre Martins, and Vivek Srikumar (eds.),\nProceedings of the 62nd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long\nPapers), pp. 15894–15939, Bangkok, Thailand, August 2024. Association for Computational Linguistics.\ndoi: 10.18653/v1/2024.acl-long.845. URL https://aclanthology.org/2024.acl-long.845.\nAshish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez, Ł ukasz Kaiser,\nand Illia Polosukhin. Attention is all you need. In I. Guyon, U. Von Luxburg, S. Bengio, H. Wallach,\nR. Fergus, S. Vishwanathan, and R. Garnett (eds.), Advances in Neural Information Processing Systems,\nvolume 30. Curran Associates, Inc., 2017. URL https://proceedings.neurips.cc/paper_files/pap\ner/2017/file/3f5ee243547dee91fbd053c1c4a845aa-Paper.pdf.\nPat Verga, Sebastian Hofstatter, Sophia Althammer, Yixuan Su, Aleksandra Piktus, Arkady Arkhangorod-\nsky, Minjie Xu, Naomi White, and Patrick Lewis. Replacing judges with juries: Evaluating llm generations\nwith a panel of diverse models. arXiv preprint arXiv:2404.18796, 2024.\nKe Wang, Nikolaos Dimitriadis, Guillermo Ortiz-Jiménez, François Fleuret, and Pascal Frossard. Localizing\ntask information for improved model merging and compression. In International Conference on Machine\nLearning, 2024a.\nYubo Wang, Xueguang Ma, Ge Zhang, Yuansheng Ni, Abhranil Chandra, Shiguang Guo, Weiming Ren,\nAaran Arulraj, Xuan He, Ziyan Jiang, et al. Mmlu-Pro: A more robust and challenging multi-task language\nunderstanding benchmark. In The Thirty-eight Conference on Neural Information Processing Systems\nDatasets and Benchmarks Track, 2024b.\nJason Wei, Maarten Bosma, Vincent Y Zhao, Kelvin Guu, Adams Wei Yu, Brian Lester, Nan Du, Andrew M\nDai, and Quoc V Le. Finetuned language models are zero-shot learners. arXiv preprint arXiv:2109.01652,\n2021.\n47\n\nYuxiang Wei, Zhe Wang, Jiawei Liu, Yifeng Ding, and Lingming Zhang. Magicoder: Source code is all you\nneed. arXiv preprint arXiv:2312.02120, 2023.\nYuxiang Wei, Federico Cassano, Yifeng Ding, Naman Jain, Harm de Vries, Leandro von Werra, Arjun\nGuha, and Lingming Zhang. Starcoder2-instruct: Fully transparent and permissive self-alignment for code\ngeneration, 2024. URL https://huggingface.co/blog/sc2-instruct.\nMitchell Wortsman, Gabriel Ilharco, Samir Ya Gadre, Rebecca Roelofs, Raphael Gontijo-Lopes, Ari S Morcos,\nHongseok Namkoong, Ali Farhadi, Yair Carmon, Simon Kornblith, and Ludwig Schmidt. Model soups:\naveraging weights of multiple fine-tuned models improves accuracy without increasing inference time. In\nKamalika Chaudhuri, Stefanie Jegelka, Le Song, Csaba Szepesvari, Gang Niu, and Sivan Sabato (eds.),\nProceedings of the 39th International Conference on Machine Learning, volume 162 of Proceedings of\nMachine Learning Research, pp. 23965–23998. PMLR, 17–23 Jul 2022. URL https://proceedings.ml\nr.press/v162/wortsman22a.html.\nWenhan Xiong, Jingyu Liu, Igor Molybog, Hejia Zhang, Prajjwal Bhargava, Rui Hou, Louis Martin, Rashi\nRungta, Karthik Abinav Sankararaman, Barlas Oguz, Madian Khabsa, Han Fang, Yashar Mehdad, Sharan\nNarang, Kshitiz Malik, Angela Fan, Shruti Bhosale, Sergey Edunov, Mike Lewis, Sinong Wang, and Hao\nMa.\nEffective long-context scaling of foundation models.\nIn Kevin Duh, Helena Gomez, and Steven\nBethard (eds.), Proceedings of the 2024 Conference of the North American Chapter of the Association\nfor Computational Linguistics: Human Language Technologies (Volume 1: Long Papers), pp. 4643–4663,\nMexico City, Mexico, June 2024. Association for Computational Linguistics. doi: 10.18653/v1/2024.naacl\n-long.260. URL https://aclanthology.org/2024.naacl-long.260.\nYuanzhong Xu, HyoukJoong Lee, Dehao Chen, Blake A. Hechtman, Yanping Huang, Rahul Joshi, Maxim\nKrikun, Dmitry Lepikhin, Andy Ly, Marcello Maggioni, Ruoming Pang, Noam Shazeer, Shibo Wang, Tao\nWang, Yonghui Wu, and Zhifeng Chen. GSPMD: general and scalable parallelization for ML computation\ngraphs. CoRR, abs/2105.04663, 2021. URL https://arxiv.org/abs/2105.04663.\nPrateek Yadav, Derek Tam, Leshem Choshen, Colin Raffel, and Mohit Bansal. TIES-merging: Resolving in-\nterference when merging models. In Thirty-seventh Conference on Neural Information Processing Systems,\n2023. URL https://openreview.net/forum?id=xtaX3WyCj1.\nPrateek Yadav, Tu Vu, Jonathan Lai, Alexandra Chronopoulou, Manaal Faruqui, Mohit Bansal, and Tsend-\nsuren Munkhdalai. What matters for model merging at scale? arXiv preprint arXiv:2410.03617, 2024.\nFanjia Yan, Huanzhi Mao, Charlie Cheng-Jie Ji, Tianjun Zhang, Shishir G. Patil, Ion Stoica, and Joseph E.\nGonzalez. Berkeley function calling leaderboard. https://gorilla.cs.berkeley.edu/blogs/8_berkel\ney_function_calling_leaderboard.html, 2024.\nBowen Yang, Bharat Venkitesh, Dwarak Talupuru, Hangyu Lin, David Cairuz, Phil Blunsom, and Acyr\nLocatelli. Rope to nope and back again: A new hybrid attention strategy, 2025. URL https://arxiv.or\ng/abs/2501.18795.\nGreg Yang, Edward J Hu, Igor Babuschkin, Szymon Sidor, Xiaodong Liu, David Farhi, Nick Ryder, Jakub\nPachocki, Weizhu Chen, and Jianfeng Gao. Tuning large neural networks via zero-shot hyperparameter\ntransfer. In A. Beygelzimer, Y. Dauphin, P. Liang, and J. Wortman Vaughan (eds.), Advances in Neural\nInformation Processing Systems, 2021. URL https://openreview.net/forum?id=Bx6qKuBM2AD.\nZhilin Yang, Peng Qi, Saizheng Zhang, Yoshua Bengio, William Cohen, Ruslan Salakhutdinov, and Christo-\npher D. Manning.\nHotpotQA: A dataset for diverse, explainable multi-hop question answering.\nIn\nEllen Riloff, David Chiang, Julia Hockenmaier, and Jun’ichi Tsujii (eds.), Proceedings of the 2018\nConference on Empirical Methods in Natural Language Processing, pp. 2369–2380, Brussels, Belgium,\nOctober-November 2018. Association for Computational Linguistics. doi: 10.18653/v1/D18-1259. URL\nhttps://aclanthology.org/D18-1259.\nShunyu Yao, Jeffrey Zhao, Dian Yu, Nan Du, Izhak Shafran, Karthik Narasimhan, and Yuan Cao. React:\nSynergizing reasoning and acting in language models. arXiv preprint arXiv:2210.03629, 2022.\n48\n\nShunyu Yao, Noah Shinn, Pedram Razavi, and Karthik Narasimhan. τ-bench: A benchmark for tool-agent-\nuser interaction in real-world domains, 2024. URL https://arxiv.org/abs/2406.12045.\nLe Yu, Bowen Yu, Haiyang Yu, Fei Huang, and Yongbin Li. Language models are super mario: absorbing\nabilities from homologous models as a free lunch. In Proceedings of the 41st International Conference on\nMachine Learning, ICML’24. JMLR.org, 2024.\nTao Yu, Rui Zhang, Kai Yang, Michihiro Yasunaga, Dongxu Wang, Zifan Li, James Ma, Irene Li, Qingning\nYao, Shanelle Roman, Zilin Zhang, and Dragomir Radev. Spider: A large-scale human-labeled dataset for\ncomplex and cross-domain semantic parsing and text-to-SQL task. In Ellen Riloff, David Chiang, Julia\nHockenmaier, and Jun’ichi Tsujii (eds.), Proceedings of the 2018 Conference on Empirical Methods in\nNatural Language Processing, pp. 3911–3921, Brussels, Belgium, October-November 2018. Association for\nComputational Linguistics. doi: 10.18653/v1/D18-1425. URL https://aclanthology.org/D18-1425.\nHuaye Zeng, Dongfu Jiang, Haozhe Wang, Ping Nie, Xiaotong Chen, and Wenhu Chen. Acecoder: Acing\ncoder rl via automated test-case synthesis, 2025. URL https://arxiv.org/abs/2502.01718.\nYao Zhao, Rishabh Joshi, Tianqi Liu, Misha Khalman, Mohammad Saleh, and Peter J Liu. Slic-hf: Sequence\nlikelihood calibration with human feedback. arXiv preprint arXiv:2305.10425, 2023.\nEnyu Zhou, Guodong Zheng, Binghai Wang, Zhiheng Xi, Shihan Dou, Rong Bao, Wei Shen, Limao Xiong,\nJessica Fan, Yurong Mou, Rui Zheng, Tao Gui, Qi Zhang, and Xuanjing Huang. RMB: Comprehensively\nBenchmarking Reward Models in LLM Alignment, 2024. URL https://arxiv.org/abs/2410.09893.\nJeffrey Zhou, Tianjian Lu, Swaroop Mishra, Siddhartha Brahma, Sujoy Basu, Yi Luan, Denny Zhou, and\nLe Hou. Instruction-following evaluation for large language models. arXiv preprint arXiv:2311.07911,\n2023.\nTerry Yue Zhuo, Minh Chien Vu, Jenny Chim, Han Hu, Wenhao Yu, Ratnadira Widyasari, Imam Nur Bani\nYusuf, Haolan Zhan, Junda He, Indraneil Paul, Simon Brunner, Chen Gong, Thong Hoang, Armel Randy\nZebaze, Xiaoheng Hong, Wen-Ding Li, Jean Kaddour, Ming Xu, Zhihan Zhang, Prateek Yadav, Naman\nJain, Alex Gu, Zhoujun Cheng, Jiawei Liu, Qian Liu, Zijian Wang, David Lo, Binyuan Hui, Niklas Muen-\nnighoff, Daniel Fried, Xiaoning Du, Harm de Vries, and Leandro Von Werra.\nBigcodebench: Bench-\nmarking code generation with diverse function calls and complex instructions, 2024.\nURL https:\n//arxiv.org/abs/2406.15877.\n49\n\nA\nAuthors\nTeam Cohere: Aakanksha, Arash Ahmadian, Marwan Ahmed, Jay Alammar, Milad Alizadeh, Yazeed Al-\nnumay, Sophia Althammer, Arkady Arkhangorodsky, Viraat Aryabumi, Dennis Aumiller, Raphaël Avalos,\nZahara Aviv, Sammie Bae, Saurabh Baji, Alexandre Barbet, Max Bartolo, Björn Bebensee, Neeral Be-\nladia, Walter Beller-Morales, Alexandre Bérard, Andrew Berneshawi, Anna Bialas, Phil Blunsom, Matt\nBobkin, Adi Bongale, Sam Braun, Maxime Brunet, Samuel Cahyawijaya, David Cairuz, Jon Ander Cam-\npos, Cassie Cao, Kris Cao, Roman Castagné, Julián Cendrero, Leila Chan Currie, Yash Chandak, Diane\nChang, Giannis Chatziveroglou, Hongyu Chen, Claire Cheng, Alexis Chevalier, Justin T. Chiu, Eugene\nCho, Eugene Choi, Eujeong Choi, Tim Chung, Volkan Cirik, Ana Cismaru, Pierre Clavier, Henry Conklin,\nLucas Crawhall-Stein, Devon Crouse, Andres Felipe Cruz-Salinas, Ben Cyrus, Daniel D’souza, Hugo Dalla-\nTorre, John Dang, William Darling, Omar Darwiche Domingues, Saurabh Dash, Antoine Debugne, Théo\nDehaze, Shaan Desai, Joan Devassy, Rishit Dholakia, Kyle Duffy, Ali Edalati, Ace Eldeib, Abdullah Elkady,\nSarah Elsharkawy, Irem Ergün, Beyza Ermis, Marzieh Fadaee, Boyu Fan, Lucas Fayoux, Yannis Flet-Berliac,\nNick Frosst, Matthias Gallé, Wojciech Galuba, Utsav Garg, Matthieu Geist, Mohammad Gheshlaghi Azar,\nEllen Gilsenan-McMahon, Seraphina Goldfarb-Tarrant, Tomas Goldsack, Aidan Gomez, Victor Machado\nGonzaga, Nithya Govindarajan, Manoj Govindassamy, Nathan Grinsztajn, Nikolas Gritsch, Patrick Gu,\nShangmin Guo, Kilian Haefeli, Rod Hajjar, Tim Hawes, Jingyi He, Sebastian Hofstätter, Sungjin Hong, Sara\nHooker, Tom Hosking, Stephanie Howe, Eric Hu, Renjie Huang, Hemant Jain, Ritika Jain, Nick Jakobi,\nMadeline Jenkins, JJ Jordan, Dhruti Joshi, Jason Jung, Trushant Kalyanpur, Siddhartha Rao Kamalakara,\nJulia Kedrzycki, Gokce Keskin, Edward Kim, Joon Kim, Wei-Yin Ko, Tom Kocmi, Michael Kozakov, Wo-\njciech Kryściński, Arnav Kumar Jain, Komal Kumar Teru, Sander Land, Michael Lasby, Olivia Lasche,\nJustin Lee, Patrick Lewis, Jeffrey Li, Jonathan Li, Hangyu Lin, Acyr Locatelli, Kevin Luong, Raymond Ma,\nLukáš Mach, Marina Machado, Joanne Magbitang, Brenda Malacara Lopez, Aryan Mann, Kelly Marchisio,\nOlivia Markham, Alexandre Matton, Alex McKinney, Dominic McLoughlin, Jozef Mokry, Adrien Morisot,\nAutumn Moulder, Harry Moynehan, Maximilian Mozes, Vivek Muppalla, Lidiya Murakhovska, Heman-\ngani Nagarajan, Alekhya Nandula, Hisham Nasir, Shauna Nehra, Josh Netto-Rosen, Daniel Ohashi, James\nOwers-Bardsley, Jason Ozuzu, Dennis Padilla, Gloria Park, Sam Passaglia, Jeremy Pekmez, Laura Penstone,\nAleksandra Piktus, Case Ploeg, Andrew Poulton, Youran Qi, Shubha Raghvendra, Miguel Ramos, Ekagra\nRanjan, Pierre Richemond, Cécile Robert-Michon, Aurélien Rodriguez, Sudip Roy, Sebastian Ruder, Laura\nRuis, Louise Rust, Anubhav Sachan, Alejandro Salamanca, Kailash Karthik Saravanakumar, Isha Satyakam,\nAlice Schoenauer Sebag, Priyanka Sen, Sholeh Sepehri, Preethi Seshadri, Ye Shen, Tom Sherborne, Sylvie\nShang Shi, Sanal Shivaprasad, Vladyslav Shmyhlo, Anirudh Shrinivason, Inna Shteinbuk, Amir Shukayev,\nMathieu Simard, Ella Snyder, Ava Spataru, Victoria Spooner, Trisha Starostina, Florian Strub, Yixuan Su,\nJimin Sun, Dwarak Talupuru, Eugene Tarassov, Elena Tommasone, Jennifer Tracey, Billy Trend, Evren\nTumer, Ahmet Üstün, Bharat Venkitesh, David Venuto, Pat Verga, Maxime Voisin, Alex Wang, Donglu\nWang, Shijian Wang, Edmond Wen, Naomi White, Jesse Willman, Marysia Winkels, Chen Xia, Jessica Xie,\nMinjie Xu, Bowen Yang, Tan Yi-Chern, Ivan Zhang, Zhenyu Zhao, and Zhoujie Zhao.\nA.1\nAcknowledgements\nWe would also like to acknowledge the contributions of the following people who helped make this work\npossible: Robert Li, Olivier Pietquin, Karina Waluk, Bill Wu, and James Zhou. We would also like to thank\nour talented team of internal annotators and model trainers.\nThis technical report was written with the assistance of the models described in this technical report.\n50\n\nB\nAppendices\nB.1\nInstruction-Following\nWe use the following custom preamble override for internal IFEval evaluation for Command A:\nYou are in non-interactive mode. Please be as comprehensive and accurate as possible and do not\nintroduce your response and / or ask follow-up questions.\nB.2\nMultilingual\nB.2.1\nExpert Training Considerations\nSupervised Fine-Tuning (SFT). We employ all SFT datasets as mentioned in Section 3.3.3.1. To train\nthe model, we use the Adam optimiser with a peak learning rate of 2.5 × 10−5, cosine decay to 1.25 × 10−5,\nβ1 = 0.9, β2 = 0.95 and a weight decay of 0.1. We merge several models with the same configuration (but a\ndifferent random seed) at this stage.\nPreference Tuning. Following the SFT stage, we perform offline preference tuning using both human-\nannotated and synthetically generated multilingual preference data (with best-of-N or machine translation).\nWe use DPO with the Adam optimiser, a peak learning rate of 1.25 × 10−5 decaying to 1.25 × 10−6, β = 0.2\nand use SFT regularisation with the same data mixture we used in the SFT stage. We do not merge over\nmultiple seeds at this stage.\nB.2.2\nResults\nWe show additional results on MGSM in Table 24, and INCLUDE-44 in Table 25 — highlighting the highly\ncompetitive multilingual capabilities of our models.\nAvg.\nde\nes\nfr\nja\nru\nzh\nCommand A\n90.1\n92.0\n92.4\n85.1⋆\n87.1\n94.0\n90.0\nDeepSeek V3\n92.3\n92.4\n96.0\n90.4\n89.6\n94.0\n91.6\nClaude 3.7 Sonnet\n92.1\n93.6\n96.4\n85.5\n89.2\n95.6\n92.4\nLlama 3.3 70B Instruct\n91.5\n92.8\n91.6\n90.0\n89.6\n95.2\n90.0\nQwen 2.5 72B Instruct Turbo\n90.5\n92.0\n94.4\n87.6\n87.1\n92.4\n89.6\nMistral Large 2\n90.1\n90.0\n90.4\n85.9\n90.3\n92.8\n91.5\nGemini 2.0 Flash\n90.0\n90.8\n92.8\n83.1\n87.1\n95.2\n90.8\nGPT-4o\n89.6\n92.0\n90.8\n83.1\n87.1\n92.0\n92.8\nGemini 1.5 Pro\n88.4\n91.2\n93.2\n77.5\n86.7\n92.4\n89.6\nLlama 3.1 405B Instruct\n74.1\n89.8\n82.4\n7.7\n85.8\n90.3\n88.6\nCommand R7B\n75.0\n77.1\n78.3\n71.5\n66.7\n79.9\n76.3\nGemma 2 9B Instruct\n80.7\n83.1\n85.9\n76.7\n72.3\n86.3\n79.9\nGemini 1.5 Flash-8B\n80.3\n79.9\n80.7\n76.7\n79.1\n84.3\n80.7\nClaude 3 Haiku\n76.9\n77.9\n77.9\n73.5\n74.7\n80.3\n77.1\nMinistral 8B\n76.1\n81.0\n80.6\n74.6\n64.4\n81.1\n74.7\nLlama 3.1 8B Instruct\n70.4\n73.9\n67.1\n65.9\n62.7\n77.9\n75.1\nQwen 2.5 7B Instruct Turbo\n70.3\n81.9\n75.9\n27.3\n72.7\n82.3\n81.5\nTable 24: MGSM scores using simple-evals. ⋆: we did not train with the simple-evals template and our French\noutputs sometimes contain a comma (used as a decimal separator), which simple-evals counts as wrong. With\nour internal implementation of MGSM, we score 90.0% on French.\n51\n\nAvg.\nar\nde\nel\nes\nfa\nfr\nhe\nhi\nid\nit\nja\nko\nnl\npl\npt\nru\ntr\nuk\nvi\nzh\nCommand A\n74.3\n73.2\n67.6\n68.0\n79.3\n61.3\n77.6\n75.5\n67.6\n72.7\n86.5\n86.8\n72.4\n81.3\n78.6\n77.5\n69.2\n67.7\n76.5\n70.7\n75.8\nClaude 3.7 Sonnet\n81.0\n79.2\n66.9\n74.7\n85.1\n73.4\n84.2\n87.8\n77.3\n81.6\n91.4\n93.4\n73.6\n87.1\n87.0\n80.9\n78.8\n68.4\n87.1\n80.5\n80.7\nGemini 2.0 Flash\n78.8\n79.0\n69.1\n71.8\n83.1\n64.6\n79.5\n83.3\n80.4\n77.8\n89.8\n91.2\n79.4\n86.0\n85.0\n79.1\n73.7\n66.2\n80.4\n77.5\n78.3\nGPT-4o\n78.7\n79.3\n68.3\n72.5\n83.1\n63.3\n82.3\n83.1\n76.4\n78.2\n89.2\n93.6\n73.8\n85.8\n82.7\n78.9\n74.8\n66.2\n82.5\n79.5\n79.3\nDeepSeek V3\n76.9\n73.9\n64.7\n66.5\n82.5\n63.5\n83.1\n80.2\n75.0\n76.5\n88.5\n91.8\n71.4\n83.7\n80.3\n77.9\n72.8\n66.8\n77.8\n78.0\n83.9\nGemini 1.5 Pro\n76.4\n72.5\n66.2\n70.7\n79.6\n65.3\n79.4\n78.2\n77.1\n76.0\n87.2\n90.8\n72.3\n86.2\n80.4\n75.7\n71.7\n67.7\n78.5\n75.1\n77.0\nLlama 3.1 405B Instruct\n75.3\n70.7\n71.7\n62.2\n81.4\n60.0\n77.4\n77.8\n73.9\n76.4\n76.8\n88.5\n68.4\n85.8\n84.7\n75.8\n72.0\n71.9\n76.5\n77.6\n76.9\nLlama 3.3 70B Instruct\n75.0\n71.7\n65.5\n64.7\n78.0\n59.5\n75.2\n78.5\n72.4\n76.4\n89.2\n85.2\n67.0\n83.5\n86.5\n77.9\n69.6\n73.0\n74.5\n76.4\n74.9\nQwen 2.5 72B Instruct Turbo\n72.6\n70.5\n64.0\n57.1\n76.9\n57.1\n77.3\n70.5\n71.5\n73.1\n85.8\n86.0\n70.6\n84.0\n71.9\n76.2\n69.4\n64.8\n71.3\n70.0\n84.4\nMistral Large 2\n72.0\n68.3\n66.2\n66.0\n79.5\n55.3\n76.6\n74.9\n69.3\n72.2\n87.0\n85.8\n65.0\n82.4\n74.3\n75.3\n68.3\n65.3\n71.8\n66.2\n71.0\nCommand R7B\n55.8\n58.0\n51.8\n41.3\n64.0\n41.4\n58.7\n53.6\n47.2\n58.4\n69.2\n61.9\n58.2\n59.5\n71.0\n49.9\n47.3\n62.0\n58.9\n52.2\n51.9\nGemini 1.5 Flash-8B\n66.0\n61.4\n54.7\n61.7\n71.2\n49.3\n69.4\n68.9\n65.4\n67.8\n79.4\n77.4\n64.0\n74.6\n67.5\n67.3\n62.1\n63.0\n66.5\n63.1\n65.9\nClaude 3 Haiku\n64.0\n65.9\n50.4\n54.2\n71.5\n46.9\n67.1\n68.0\n60.1\n67.1\n76.6\n75.0\n62.8\n72.6\n69.3\n64.1\n62.3\n58.6\n69.3\n56.0\n61.3\nGemma 2 9B Instruct\n61.7\n57.1\n58.3\n55.3\n65.5\n44.9\n64.0\n66.5\n58.1\n63.6\n72.4\n73.7\n56.2\n70.2\n64.1\n65.0\n57.8\n59.3\n66.0\n60.2\n56.5\nQwen 2.5 7B Instruct Turbo\n59.9\n57.1\n53.2\n49.3\n65.8\n43.4\n63.5\n58.4\n56.7\n63.5\n72.8\n71.9\n61.4\n69.7\n59.5\n61.5\n52.5\n52.6\n57.1\n56.4\n72.8\nLlama 3.1 8B Instruct\n54.9\n55.1\n53.2\n33.5\n62.7\n43.8\n59.2\n54.5\n49.2\n60.5\n70.6\n58.3\n50.2\n63.7\n55.1\n59.5\n49.1\n56.0\n54.7\n55.5\n53.6\nMinistral 8B\n50.2\n43.8\n42.4\n41.6\n59.5\n38.7\n57.0\n49.5\n46.6\n49.5\n57.5\n57.3\n51.0\n58.4\n55.8\n49.2\n51.1\n46.7\n54.0\n44.5\n50.6\nTable 25: INCLUDE-44 scores for individual languages.\nB.3\nCode\nB.3.1\nExpert Training Considerations\nStage 1 large-scale supervised learning. This expert is trained using Adam optimisation, a learning\nrate peak of 5×10−5, cosine decay to 5×10−6, beta values (β1 = 0.9, β2 = 0.95), weight decay factor of 0.1,\nand gradient norm clipping at peak 1.0. Our regularisation is inspired by similar code expert training such\nas Qwen 2.5 Coder (Hui et al., 2024).\nStage 2 supervised learning on only high-quality data. We follow a similar schedule for fine-tuning\nof the merged model described in Stage 1.\nStage 3 RL over scored or preference-pair data. We use a constant learning rate of 2 × 10−6, a\nregularisation parameter β of 0.06, and otherwise match the hyperparameters used for SFT.\nB.3.2\nResults\nWe additionally show a breakdown of results across programming languages for HumanEval and Less Basic\nPython Problems (LBPP) in Table 26.\nHumanEval\nLess Basic Python Problems (LBPP)\nAvg.\nPython\nC++\nRust\nJava\nJavaScript\nGo\nCOBOL\nAvg.\nPython\nC++\nRust\nJava\nJavaScript\nGo\nCommand A\n76.2\n85.4\n81.1\n73.2\n86.0\n64.6\n72.0\n25.3\n51.5\n58.4\n50.3\n41.6\n50.0\n58.2\n50.3\nCommand A Expert\n77.5\n86.6\n76.8\n74.4\n76.2\n79.9\n71.3\n29.8\n50.8\n59.6\n51.6\n39.6\n53.2\n54.3\n46.6\nCommand A Agentic\n—\n—\n—\n—\n—\n—\n—\n—\n65.4\n71.4\n68.8\n57.7\n63.9\n68.0\n62.7\nCommand R7B\n50.7\n61.0\n54.9\n39.0\n52.4\n53.7\n43.3\n7.0\n21.9\n26.1\n21.1\n13.4\n23.4\n29.4\n17.4\nCommand R Refresh\n54.7\n67.1\n56.1\n47.6\n59.2\n64.6\n33.5\n1.9\n24.7\n33.5\n25.5\n16.1\n22.2\n26.8\n23.6\nCommand R+ Refresh\n54.4\n72.0\n54.3\n50.6\n67.1\n62.8\n19.5\n2.5\n25.6\n38.5\n26.7\n21.5\n30.4\n28.1\n8.1\nLlama 3.3 70B Instruct\n75.5\n80.5 / 85.4\n75.0\n63.4\n83.4\n81.7\n62.2\n3.2\n47.8\n55.3\n42.2\n32.9\n56.3\n56.2\n43.5\nMistral Large 2\n82.9\n94.5\n86.0\n72.6\n84.8\n87.2\n72.6\n10.8\n54.0\n62.7\n53.4\n36.2\n51.3\n52.9\n42.2\nQwen 2.5 72B Instruct\n78.5\n86.6 / 86.6\n76.8\n72.0\n82.9\n81.1\n71.3\n6.3\n48.3\n59.6\n46.6\n32.9\n49.4\n60.8\n39.8\nLlama 3.1 405B Instruct\n76.7\n89.0 / 88.4\n81.7\n53.7\n87.8\n79.3\n69.5\n3.2\n52.7\n59.6\n50.9\n38.9\n57.6\n61.4\n47.2\nDeepSeek V3\n83.5\n92.1\n90.2\n70.7\n85.4\n85.4\n77.4\n15.2\n61.5\n67.1\n59.0\n57.1\n67.1\n66.0\n52.8\nTable 26: Full pass@1 results for HumanEval and LBPP across Python, C++, Rust Java, Javascript, Go.\nand COBOL. All results are internal reproductions using an identical prompt except where ‘/’ indicates an\nexternal value first, and footnote citation, and the internal reproduction second. Average (Avg.) results are\na sample-weighted average across Python, C++, Rust Java, Javascript and Go languages.\nSources of externally reported numbers in all code-related tables in this report are as follows:\nCode Understanding Benchmarks (Table 12)\n• Llama 3.3 70B Instruct: https://github.com/meta-llama/llama-models/blob/main/models/lla\n52\n\nma3_3/MODEL_CARD.md, https://bigcode-bench.github.io/\n• Mistral Large 2: https://livecodebench.github.io/\n• Qwen 2.5 72B Instruct: https://bigcode-bench.github.io/\n• Llama 3.1 405B Instruct: https://github.com/meta-llama/llama-models/blob/main/models/lla\nma3_3/MODEL_CARD.md\n• DeepSeek V3: https://livecodebench.github.io/, https://bigcode-bench.github.io/\nCode Editing Benchmarks (Table 13)\n• DeepSeek V3: https://www.deepseek.com/\nHumanEval and LBPP (Table 26)\n• Llama 3.3 70B Instruct and Llama 3.1 405B Instruct: https://github.com/meta-llama/llama-mod\nels/blob/main/models/llama3_3/MODEL_CARD.md\n• Qwen 2.5 72B Instruct: From Hui et al. (2024).\nB.4\nReasoning\nSupervised Fine-Tuning. We train using the Adam optimiser with peak learning rate of 2.5×10−5, cosine\ndecay to 2.5 × 10−6, β1 = 0.9, β2 = 0.95, weight decay of 0.01, and gradient norm clipping peak at 1.0.\nPreference Tuning. We train using CoPG with the Adam optimiser, using a learning rate of 2×10−6 with\nno decay, β1 = 0.9, β2 = 0.95, and gradient norm clipping peak at 1.0.\nB.5\nLong Context\nTraining is conducted using the Adam optimiser with a peak learning rate of 2.5 × 10−5, cosine decay to\n2.5 × 10−6, β1 = 0.9, β2 = 0.95, weight decay of 0.01, and gradient norm clipping peak at 1.0.\nB.6\nSafety\nContextual\nOver-Refusal\nContextual\nAccuracy\nStrict\nOver-Refusal\nStrict Accuracy\nCommand A\n8.3\n75.4\n10.2\n87.5\nClaude 3.5 Sonnet\n10.1\n82.5\n10.4\n89.6\nDeepSeek V3\n3.6\n66.6\n1.7\n68.7\nGPT-4o\n21.3\n77.8\n10.4\n86.9\nLlama 3.1 405B\n5.9\n70.3\n5.8\n80.5\nLlama 3.3 70B\n3.6\n60.9\n4.0\n67.9\nMistral Large Latest\n8.9\n70.6\n8.7\n81.1\nQwen 2.5 72B\n10.1\n72.2\n1.2\n78.3\nCommand R+ Refresh\n8.9\n61.8\n12.7\n70.0\nCommand R7B\n14.2\n60.9\n7.5\n62.3\nGemini 2.0 Flash\n26.6\n69.7\n14.7\n79.0\nGemma 2 9B\n39.1\n72.5\n14.5\n81.8\nLlama 3.1 8B\n4.1\n70.7\n6.9\n81.0\nQwen 2.5 7B\n18.3\n70.3\n8.7\n74.7\nCommand R Refresh\n9.5\n63.1\n10.4\n68.0\nTable 27: Safety mode performance compared to similarly sized models. We pass the instructions for each\nsafety mode to competitors as system message or first message to allow for the mode comparison. Note\nthat the over-refusal set was developed by red-teaming Command R+ Refresh, and therefore is specifically\nchallenging for Command A models. The top performing model for each size category is bolded in each\ncolumn. Higher accuracy and lower over-refusal rates correspond to better performance.\n53\n\n0\n40\n20\n80\n60\n100\nMisinfo\nSelf-harm\nCSEA\nSexual Content\nViolence & Hate\nClaude 3.5 Sonnet\nDeepSeek V3\nGPT-4o\nLlama 3.1 405B\nLlama 3.3 70B\nMistral Large Latest\nQwen 2.5 72B\nCommand R+\nCommand A\nFigure 15: Absolute safety performance for large models in the default setting.\n0\n40\n20\n80\n60\n100\nMisinfo\nSelf-harm\nCSEA\nSexual Content\nViolence & Hate\nGemini 1.5 Flash\nGemini 2.0 Flash\nGemma 2 9B\nLlama 3.1 8B\nQwen 2.5 7B\nCommand R7B\nCommand R\nFigure 16: Absolute safety performance for small models in the default setting.\nB.6.1\nExpert Training Considerations\nSupervised fine-tuning. During the SFT stage, we train the safety expert using the Adam optimiser with\nβ1 = 0.9, β2 = 0.95, and a peak learning rate of 10−4 decayed to 10−5 with a cosine schedule. Gradient norm\nis clipped to 1, and weight decay is weighted by 10−3.\nOffline preference tuning. We train the Safety expert using the same hyper-parameters as above, except\nthe peak learning rate is 10−6 decayed to 10−7. We use IPO with a KL regularisation parameter, β = 0.03.\nB.6.2\nResults\nTable 27 provides additional results for the safety mode performance of the Command A models, while\nFigures 15 and 16 show absolute safety performance for large and small models respectively in the default\nsafety setting, further highlighting our models’ competitive safety performance.\nB.7\nEvaluation on Standard Benchmarks\nWe provide further details about how we measure performance on standard benchmarks:\n• MMLU (Hendrycks et al., 2021) measures university-level academic knowledge across a diverse range\nof subjects. We run 5-shot, with Chain-of-Thought (CoT).\n• MMLU-Pro (Wang et al., 2024b) is an enhanced version of MMLU, designed to evaluate knowledge\nacross a diverse range of professional and academic domains, including law, medicine, and engineering.\n54\n\nWe run 5-shot, with CoT.\n• GPQA (Rein et al., 2023) measures graduate-level academic knowledge in specialized STEM topics.\nWe run on 0-shot, with CoT, and we report results only on the diamond subset.\n• IFEval (Zhou et al., 2023) measures instruction-following ability across 25 types of verifiable instruc-\ntions (e.g. output length, keyword inclusion/exclusion, formatting). We compute the average of the\nprompt-level strict accuracy (i.e., the fraction of dataset prompts where all verifiable instructions are\nfollowed) and the instruction-level strict accuracy (i.e., the fraction of verifiable instructions that are\nfollowed, this allows partial credit as most prompts include multiple instructions).\n• InFoBench (Qin et al., 2024) also measures instruction-following across five broad types of instruc-\ntions: content, linguistic, style, format, and number. Each prompt in InFoBench is paired with a set\nof yes-no evaluation questions, and we use GPT-4o to answer these questions. We compute the overall\naverage accuracy across the fraction of correctly answered questions per prompt.\n55\n"
    }
  ]
}