{
  "1-1 (Weights)": "The provided material repeatedly stresses that \"Cohere Labs Command A is an open weights research release\" and identifies the snapshot as a 111 billion-parameter model. Access is explicitly offered in two ways: (1) users may \"try out Cohere Labs Command A before downloading the weights in our hosted Hugging Face Space (https://coherelabs-c4ai-command.hf.space/models/command-a-03-2025),\" and (2) they can fetch the individual weight shards that follow the pattern \"model-00001-of-00049.safetensors\" (the quoted filename confirms that at least 49 sharded .safetensors files are distributed). All sentences that describe the weights label the release as ‘open,’ signalling that the full parameter tensors are downloadable, not merely held behind an API. No paywall or enterprise-only gate is mentioned in the quotes; the only referenced prerequisite is visiting the Hugging Face Space, where both an interactive demo and the raw checkpoints are made available for the \"command-a-03-2025\" checkpoint.",
  "1-1 (Weights)__evidence": [
    {
      "source": "[readme]",
      "quote": "Cohere Labs Command A is an open weights research release of a 111 billion parameter model optimized for demanding enterprises that require fast, secure, and high-quality AI."
    },
    {
      "source": "[readme]",
      "quote": "You can try out Cohere Labs Command A before downloading the weights in our hosted [Hugging Face Space](https://coherelabs-c4ai-command.hf.space/models/command-a-03-2025)."
    },
    {
      "source": "[files]",
      "quote": "model-00001-of-00049.safetensors"
    }
  ],
  "1-2 (Code)": "The only statement about code is: \"Please install transformers from the source repository that includes the necessary changes for this model.\" From this we can infer that inference/serving requires a fork or a bleeding-edge commit of the Hugging Face Transformers library that already integrates the Command-A architecture or tokenizer changes. Crucially, there is NO line indicating that the pre-training, fine-tuning, or RLHF training scripts themselves have been open-sourced; the quote talks solely about installing a modified inference library. Therefore, with the evidence given, training code for any stage (pre-training, supervised fine-tune, or RL) is not publicly provided, while minimal inference support has been upstreamed to a special branch of Transformers.",
  "1-2 (Code)__evidence": [
    {
      "source": "[readme]",
      "quote": "Please install transformers from the source repository that includes the necessary changes for this model."
    }
  ],
  "1-3 (License)": "Multiple overlapping passages establish that the model is released under Creative Commons Non-Commercial terms. Exact quoted phrases include \"license: cc-by-nc-4.0\", \"* License:[CC-BY-NC](https://cohere.com/cohere-labs-cc-by-nc-license), requires also adhering to [Cohere Lab's Acceptable Use Policy]\", and \"This model is governed by a [CC-BY-NC](https://cohere.com/cohere-labs-cc-by-nc-license)\".  A gated download form presents an \"extra_gated_prompt\" where users must agree that \"By submitting this form, you agree to the [License Agreement]\"; additional gated fields request \"Name\", \"Affiliation\", \"Country\", and a mandatory checkbox that states \"I agree to use this model for non-commercial use ONLY\".  Together these clauses mean:  • Redistribution and modification are allowed only under CC-BY-NC 4.0;  • Commercial use is expressly disallowed;  • Users must also follow Cohere Labs’ AUP, forming an extra layer of restrictions;  • The project is co-developed by \"[Cohere](https://cohere.com/) and [Cohere Labs](https://cohere.for.ai/)\" who serve as the points of contact. No text offers an enterprise/commercial license path inside these quotes, so the sole officially documented permission set is the CC-BY-NC 4.0 plus AUP regime.",
  "1-3 (License)__evidence": [
    {
      "source": "[readme]",
      "quote": "license: cc-by-nc-4.0"
    },
    {
      "source": "[readme]",
      "quote": "* License:[CC-BY-NC](https://cohere.com/cohere-labs-cc-by-nc-license), requires also adhering to [Cohere Lab's Acceptable Use Policy](https://docs.cohere.com/docs/cohere-labs-acceptable-use-policy)"
    },
    {
      "source": "[readme]",
      "quote": "This model is governed by a [CC-BY-NC](https://cohere.com/cohere-labs-cc-by-nc-license), requires also adhering to [Cohere Lab's Acceptable Use Policy](https://docs.cohere.com/docs/cohere-labs-acceptable-use-policy)"
    },
    {
      "source": "[readme]",
      "quote": ": false\nlibrary_name: transformers\nlanguage:\n- en\n- fr\n- de\n- es\n- it\n- pt\n- ja\n- ko\n- zh\n- ar\n- el\n- fa\n- pl\n- id\n- cs\n- he\n- hi\n- nl\n- ro\n- ru\n- tr\n- uk\n- vi\nlicense: cc-by-nc-4.0\nextra_gated_prompt: \"By submitting this form, you agree to the [License Agreement](https://cohere.com/c4ai-cc-by-nc-license) and acknowledge that the i"
    },
    {
      "source": "[readme]",
      "quote": "two GPUs.\n\nDeveloped by: [Cohere](https://cohere.com/) and [Cohere Labs](https://cohere.for.ai/)\n\n* Point of Contact: [Cohere Labs](https://cohere.for.ai/) \n* License:[CC-BY-NC](https://cohere.com/cohere-labs-cc-by-nc-license), requires also adhering to [Cohere Lab's Acceptable Use Policy](https://docs.cohere.com/docs/cohere-labs-a"
    },
    {
      "source": "[readme]",
      "quote": "ribe at any time.\" \nextra_gated_fields:\n Name: text\n Affiliation: text\n Country: country\n I agree to use this model for non-commercial use ONLY: checkbox\n---\n\n# **Model Card for Cohere Labs Command A**\n\n## **Model Summary**\n\nCohere Labs Command A is an o"
    }
  ],
  "1-4 (Paper)": "One concise sentence links to formal documentation: \"For more details on how this model was developed, check out our [Tech Report](https://arxiv.org/abs/2504.00698).\" Hence the canonical write-up for Command-A-03-2025 is an arXiv technical report at the above URL, which readers are directed to for methodology, training details, evaluation, and further background.",
  "1-4 (Paper)__evidence": [
    {
      "source": "[readme]",
      "quote": "For more details on how this model was developed, check out our [Tech Report](https://arxiv.org/abs/2504.00698)."
    }
  ]
}