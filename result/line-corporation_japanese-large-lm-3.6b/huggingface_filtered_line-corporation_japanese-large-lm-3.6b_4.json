{
  "4-1 (Pre-training Data)": "The documentation for the japanese-large-lm-3.6b model explains that its pre-training material is entirely Japanese-language text. According to the authors, “Our training corpus consists of the Japanese portions of publicly available corpus such as C4, CC-100, and Oscar.” In addition to these well-known public datasets, they “incorporated the Web texts crawled by in-house system.” When combined, the total size of the pre-training corpus is reported to be “about 650 GB.” No other sources, licenses, or quantitative breakdowns are disclosed in the provided quotation.",
  "4-1 (Pre-training Data)__evidence": [
    {
      "source": "[readme]",
      "quote": "Our training corpus consists of the Japanese portions of publicly available corpus such as C4, CC-100, and Oscar. We also incorporated the Web texts crawled by in-house system. The total size of our training corpus is about 650 GB."
    }
  ],
  "4-2 (Fine-tuning Data)": "",
  "4-2 (Fine-tuning Data)__evidence": [],
  "4-3 (Reinforcement Learning Data)": "",
  "4-3 (Reinforcement Learning Data)__evidence": [],
  "4-4 (Data Filtering)": "",
  "4-4 (Data Filtering)__evidence": []
}