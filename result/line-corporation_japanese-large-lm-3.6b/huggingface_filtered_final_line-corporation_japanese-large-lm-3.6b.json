{
  "1-1 (Weights)": "The single sentence provided—“This repository provides a 3.6B parameters Japanese language model, trained by [LINE Corporation](https://linecorp.com/ja/)”—states that the weights for the target model are hosted directly in the same repository that carries its README. The wording “provides” indicates that the full set of model parameters (3.6 billion in total) is made available there rather than merely being described or referred to, implying that a user visiting the repository can obtain them without further gate-keeping. No other sentences expand on mirrors, checkpoints, or special access conditions, so all that can be asserted from the quotes is that the repository itself is the place where the weights live and that they are exposed to the public in association with LINE Corporation’s release of a Japanese-language model of that exact scale (3.6 B).",
  "1-1 (Weights)__evidence": [
    {
      "source": "[readme]",
      "quote": "This repository provides a 3.6B parameters Japanese language model, trained by [LINE Corporation](https://linecorp.com/ja/)."
    }
  ],
  "1-2 (Code)": "None of the supplied quotes contain any sentence mentioning the release status of training code, fine-tuning scripts, inference utilities, or pipeline specifics for the 3.6 B Japanese model. Because no quote addresses code at all, there is no basis to claim that any portion—pre-training, fine-tuning, RL, or otherwise—is public or private. The available evidence is therefore silent on the availability of training or serving code.",
  "1-2 (Code)__evidence": [],
  "1-3 (License)": "Every quoted license reference is consistent and explicit: “license: apache-2.0,” “## License  [Apache License, Version 2.0](https://www.apache.org/licenses/LICENSE-2.0),” and the YAML header fragment that likewise lists “license: apache-2.0.”  These statements confirm the model is distributed under the Apache License, Version 2.0. The Apache-2.0 license is a permissive open-source license that, by its own terms, grants broad rights to use, modify, distribute and sublicense, including for commercial purposes, provided that copyright notices and the license text itself are preserved. No quote introduces additional restrictions or carve-outs beyond those standard Apache-2.0 terms, so the model inherits the full set of customary freedoms (commercial redistribution, derivative works, patent grant, etc.) and obligations (notice retention, no trademark use without permission) set forth in that license.",
  "1-3 (License)__evidence": [
    {
      "source": "[readme]",
      "quote": "license: apache-2.0"
    },
    {
      "source": "[readme]",
      "quote": "## License\n[Apache License, Version 2.0](https://www.apache.org/licenses/LICENSE-2.0)"
    },
    {
      "source": "[readme]",
      "quote": "[readme]\n---\nlicense: apache-2.0\ndatasets:\n- wikipedia\n- mc4\n- cc100\n- oscar\nlanguage:\n- ja\n---\n# japanese-large-lm-3.6b\n\nThis repository provides a 3.6B parameters Japanese language model, tr"
    }
  ],
  "1-4 (Paper)": "The single multi-sentence quote states: “This repository provides a 3.6B parameters Japanese language model, trained by [LINE Corporation](https://linecorp.com/ja/).  [Tech Blog](https://engineering.linecorp.com/ja/blog/3.6-billion-parameter-japanese-language-model) explains details.”  From this we can summarize that there is no academic paper cited, but there is an official engineering or technology blog post published by LINE Corporation that serves as the primary technical reference or report for the model. The blog link is explicitly provided and is presented as the vehicle that “explains details,” indicating that readers should consult that article for methodology, architecture choices, training data description, evaluation results, or any other technical exposition associated with the 3.6 B Japanese language model.",
  "1-4 (Paper)__evidence": [
    {
      "source": "[readme]",
      "quote": "This repository provides a 3.6B parameters Japanese language model, trained by [LINE Corporation](https://linecorp.com/ja/).\n\n[Tech Blog](https://engineering.linecorp.com/ja/blog/3.6-billion-parameter-japanese-language-model) explains details."
    }
  ],
  "1-5 (Architecture)": "The line-corporation/japanese-large-lm-3.6b repository explicitly notes: “This repository provides a 3.6B parameters Japanese language model, trained by LINE Corporation.”  A concise specification line captures the main numerical design choices: “3.6B  | 51200 | GPTNeoX | RoPE | 30 | 3072 | 32”.  The model config JSON reinforces this by setting \"architectures\": [\"GPTNeoXForCausalLM\"], while also recording \"num_hidden_layers\": 30 and \"hidden_size\": 3072.  Collectively, these quoted facts state that japanese-large-lm-3.6b is a 3.6-billion-parameter GPTNeoXForCausalLM transformer with 30 hidden layers, each of width 3 072, that uses Rotary Positional Embeddings (RoPE).  The same summary row additionally lists the values 51 200 and 32, presented without further labels, but included here because they appear directly in the architecture table published for this model.",
  "1-5 (Architecture)__evidence": [
    {
      "source": "[readme]",
      "quote": "This repository provides a 3.6B parameters Japanese language model, trained by [LINE Corporation](https://linecorp.com/ja/)."
    },
    {
      "source": "[readme]",
      "quote": "| 3.6B  | 51200      | GPTNeoX      | RoPE          | 30     | 3072       | 32 |"
    },
    {
      "source": "[config]",
      "quote": "\"architectures\": [\n    \"GPTNeoXForCausalLM\"\n  ]"
    },
    {
      "source": "[config]",
      "quote": "\"num_hidden_layers\": 30,"
    },
    {
      "source": "[config]",
      "quote": "\"hidden_size\": 3072,"
    }
  ],
  "1-6 (Tokenizer)": "Tokenizer details are given in four separate quotes.  The authors state: “We use a sentencepiece tokenizer with a unigram language model and byte-fallback.”  They emphatically add, “We **do not** apply pre-tokenization with Japanese tokenizer.”  A usage example shows how it is instantiated: `tokenizer = AutoTokenizer.from_pretrained(\"line-corporation/japanese-large-lm-3.6b\", use_fast=False)`, confirming that the slow implementation is chosen.  Finally, the vocabulary/model file itself is named “spiece.model”.  Combining these points, japanese-large-lm-3.6b relies on a SentencePiece unigram tokenizer that supports byte-fallback, skips any external Japanese-specific pre-tokeniser, is shipped as spiece.model, and is normally loaded through AutoTokenizer with the fast path disabled.",
  "1-6 (Tokenizer)__evidence": [
    {
      "source": "[readme]",
      "quote": "We use a sentencepiece tokenizer with a unigram language model and byte-fallback."
    },
    {
      "source": "[readme]",
      "quote": "We **do not** apply pre-tokenization with Japanese tokenizer."
    },
    {
      "source": "[readme]",
      "quote": "tokenizer = AutoTokenizer.from_pretrained(\"line-corporation/japanese-large-lm-3.6b\", use_fast=False)"
    },
    {
      "source": "[files]",
      "quote": "spiece.model"
    }
  ],
  "2-1 (Hardware)": "",
  "2-1 (Hardware)__evidence": [],
  "2-2 (Software)": "The software environment is indicated by two direct snippets.  The configuration JSON lists \"transformers_version\": \"4.29.2\", while the loading example uses `model = AutoModelForCausalLM.from_pretrained(\"line-corporation/japanese-large-lm-3.6b\", torch_dtype=torch.float16)`.  These quotes show that japanese-large-lm-3.6b was prepared with Hugging Face Transformers version 4.29.2 and that typical usage relies on the AutoModelForCausalLM helper, often loading the weights in half-precision (torch.float16).",
  "2-2 (Software)__evidence": [
    {
      "source": "[config]",
      "quote": "\"transformers_version\": \"4.29.2\","
    },
    {
      "source": "[readme]",
      "quote": "model = AutoModelForCausalLM.from_pretrained(\"line-corporation/japanese-large-lm-3.6b\", torch_dtype=torch.float16)"
    }
  ],
  "2-3 (API)": "The only API-level guidance provided is a minimal example that relies on the standard Hugging Face Transformers interface. The snippet illustrates that users can pull the repository \"line-corporation/japanese-large-lm-3.6b\" directly with\n  model = AutoModelForCausalLM.from_pretrained(\"line-corporation/japanese-large-lm-3.6b\", torch_dtype=torch.float16)\nfor the weights and\n  tokenizer = AutoTokenizer.from_pretrained(\"line-corporation/japanese-large-lm-3.6b\", use_fast=False)\nfor the vocabulary. From these two lines we can infer that the model is publicly hosted, can be instantiated with half-precision (float16) to save memory, and that the authors recommend (or require) the slow tokenizer implementation (use_fast=False) for full compatibility. No additional client libraries, rate limits, authentication steps, or hosted-service endpoints are mentioned; the example implicitly assumes local inference after download.",
  "2-3 (API)__evidence": [
    {
      "source": "[readme]",
      "quote": "model = AutoModelForCausalLM.from_pretrained(\"line-corporation/japanese-large-lm-3.6b\", torch_dtype=torch.float16)"
    },
    {
      "source": "[readme]",
      "quote": "tokenizer = AutoTokenizer.from_pretrained(\"line-corporation/japanese-large-lm-3.6b\", use_fast=False)"
    }
  ],
  "3-1 (Pre-training)": "The documentation explicitly states that \"This repository provides a 3.6B parameters Japanese language model, trained by LINE Corporation,\" establishing both the parameter count and the creator. A compact specification row — \"| 3.6B  | 51200      | GPTNeoX      | RoPE          | 30     | 3072       | 32 |\" — enumerates technical details: model size 3.6 B parameters, a numeric value 51 200 (likely a context or vocabulary size), the base architecture GPT-NeoX, rotary positional embeddings (RoPE) for position encoding, and three further numeric hyper-parameters (30, 3072, 32) that correspond to depth, hidden-size/FFN dimension, and number of attention heads, respectively, although the table does not label the columns. Training data are drawn exclusively from Japanese text. The corpus combines Japanese subsets of open-source datasets (C4, CC-100, Oscar) with additional web pages gathered by an in-house crawler. The aggregate volume reaches \"about 650 GB\" of raw text. No other preprocessing, batching, or optimization details are included in the provided material.",
  "3-1 (Pre-training)__evidence": [
    {
      "source": "[readme]",
      "quote": "This repository provides a 3.6B parameters Japanese language model, trained by [LINE Corporation](https://linecorp.com/ja/)."
    },
    {
      "source": "[readme]",
      "quote": "| 3.6B  | 51200      | GPTNeoX      | RoPE          | 30     | 3072       | 32 |"
    },
    {
      "source": "[readme]",
      "quote": "Our training corpus consists of the Japanese portions of publicly available corpus such as C4, CC-100, and Oscar.\nWe also incorporated the Web texts crawled by in-house system.\nThe total size of our training corpus is about 650 GB."
    }
  ],
  "3-2 (Fine-tuning)": "",
  "3-2 (Fine-tuning)__evidence": [],
  "3-3 (Reinforcement Learning)": "",
  "3-3 (Reinforcement Learning)__evidence": [],
  "4-1 (Pre-training Data)": "The documentation for the japanese-large-lm-3.6b model explains that its pre-training material is entirely Japanese-language text. According to the authors, “Our training corpus consists of the Japanese portions of publicly available corpus such as C4, CC-100, and Oscar.” In addition to these well-known public datasets, they “incorporated the Web texts crawled by in-house system.” When combined, the total size of the pre-training corpus is reported to be “about 650 GB.” No other sources, licenses, or quantitative breakdowns are disclosed in the provided quotation.",
  "4-1 (Pre-training Data)__evidence": [
    {
      "source": "[readme]",
      "quote": "Our training corpus consists of the Japanese portions of publicly available corpus such as C4, CC-100, and Oscar. We also incorporated the Web texts crawled by in-house system. The total size of our training corpus is about 650 GB."
    }
  ],
  "4-2 (Fine-tuning Data)": "",
  "4-2 (Fine-tuning Data)__evidence": [],
  "4-3 (Reinforcement Learning Data)": "",
  "4-3 (Reinforcement Learning Data)__evidence": [],
  "4-4 (Data Filtering)": "",
  "4-4 (Data Filtering)__evidence": [],
  "__usage": {
    "fine_tuning": "unknown",
    "rl": "unknown"
  }
}