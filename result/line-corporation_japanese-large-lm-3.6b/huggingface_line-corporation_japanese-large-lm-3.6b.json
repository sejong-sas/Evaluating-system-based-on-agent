{
    "model_id": "line-corporation/japanese-large-lm-3.6b",
    "files": [
        ".gitattributes",
        "README.md",
        "config.json",
        "model.safetensors",
        "pytorch_model.bin",
        "spiece.model",
        "tokenizer_config.json"
    ],
    "readme": "---\nlicense: apache-2.0\ndatasets:\n- wikipedia\n- mc4\n- cc100\n- oscar\nlanguage:\n- ja\n---\n# japanese-large-lm-3.6b\n\nThis repository provides a 3.6B parameters Japanese language model, trained by [LINE Corporation](https://linecorp.com/ja/).\n\n[Tech Blog](https://engineering.linecorp.com/ja/blog/3.6-billion-parameter-japanese-language-model) explains details.\n\n## How to use\n\n```\nimport torch\nfrom transformers import AutoModelForCausalLM, AutoTokenizer, pipeline, set_seed\n \nmodel = AutoModelForCausalLM.from_pretrained(\"line-corporation/japanese-large-lm-3.6b\", torch_dtype=torch.float16)\ntokenizer = AutoTokenizer.from_pretrained(\"line-corporation/japanese-large-lm-3.6b\", use_fast=False)\ngenerator = pipeline(\"text-generation\", model=model, tokenizer=tokenizer, device=0)\nset_seed(101)\n \ntext = generator(\n    \"おはようございます、今日の天気は\",\n    max_length=30,\n    do_sample=True,\n    pad_token_id=tokenizer.pad_token_id,\n    num_return_sequences=5,\n)\n \nfor t in text:\n  print(t)\n \n# 下記は生成される出力の例\n# [{'generated_text': 'おはようございます、今日の天気は雨模様ですね。梅雨のこの時期の 朝は洗濯物が乾きにくいなど、主婦にとっては悩みどころですね。 では、'},\n#  {'generated_text': 'おはようございます、今日の天気は晴れ。 気温は8°C位です。 朝晩は結構冷え込むようになりました。 寒くなってくると、...'},\n#  {'generated_text': 'おはようございます、今日の天気は曇りです。 朝起きたら雪が軽く積もっていた。 寒さもそれほどでもありません。 日中は晴れるみたいですね。'},\n#  {'generated_text': 'おはようございます、今日の天気は☁のち☀です。 朝の気温5°C、日中も21°Cと 暖かい予報です'},\n#  {'generated_text': 'おはようございます、今日の天気は晴天ですが涼しい1日です、気温は午後になり低くなり25°Cくらい、風も強いようですので、'}]\n```\n\n## Model architecture\n| Model | Vocab size | Architecture | Position type | Layers | Hidden dim | Attention heads |\n| :---: | :--------: | :----------- | :-----------: | :----: | :--------: | :-------------: |\n| 1.7B  | 51200      | GPT2         | Absolute      | 24     | 2304       | 24 |\n| 3.6B  | 51200      | GPTNeoX      | RoPE          | 30     | 3072       | 32 |\n\n## Training Corpus\nOur training corpus consists of the Japanese portions of publicly available corpus such as C4, CC-100, and Oscar.\nWe also incorporated the Web texts crawled by in-house system.\nThe total size of our training corpus is about 650 GB.\nThe trained model achieves 7.50 perplexity on the internal validation sets of Japanese C4.\n\n## Tokenization\nWe use a sentencepiece tokenizer with a unigram language model and byte-fallback.\nWe **do not** apply pre-tokenization with Japanese tokenizer.\nThus, a user may directly feed raw sentences into the tokenizer.\n\n\n## License\n[Apache License, Version 2.0](https://www.apache.org/licenses/LICENSE-2.0)",
    "config": "{\n  \"architectures\": [\n    \"GPTNeoXForCausalLM\"\n  ],\n  \"bos_token_id\": 2,\n  \"classifier_dropout\": 0.1,\n  \"eos_token_id\": 2,\n  \"hidden_act\": \"gelu\",\n  \"hidden_size\": 3072,\n  \"initializer_range\": 0.02,\n  \"intermediate_size\": 12288,\n  \"layer_norm_eps\": 1e-05,\n  \"max_position_embeddings\": 2048,\n  \"model_type\": \"gpt_neox\",\n  \"num_attention_heads\": 32,\n  \"num_hidden_layers\": 30,\n  \"rotary_emb_base\": 10000,\n  \"rotary_pct\": 1.0,\n  \"tie_word_embeddings\": true,\n  \"torch_dtype\": \"float16\",\n  \"transformers_version\": \"4.29.2\",\n  \"use_cache\": true,\n  \"use_parallel_residual\": false,\n  \"vocab_size\": 51200\n}\n",
    "generation_config": "",
    "license_file": "",
    "py_files": {}
}