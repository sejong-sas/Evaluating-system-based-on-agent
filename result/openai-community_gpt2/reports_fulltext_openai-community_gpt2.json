{
  "model_id": "openai-community/gpt2",
  "full_texts": [
    {
      "arxiv_id": "https://cdn.openai.com/better-language-models/language_models_are_unsupervised_multitask_learners.pdf",
      "full_text": "Language Models are Unsupervised Multitask Learners\nAlec Radford * 1 Jeffrey Wu * 1 Rewon Child 1 David Luan 1 Dario Amodei ** 1 Ilya Sutskever ** 1\nAbstract\nNatural language processing tasks, such as ques-\ntion answering, machine translation, reading com-\nprehension, and summarization, are typically\napproached with supervised learning on task-\nspeciﬁc datasets. We demonstrate that language\nmodels begin to learn these tasks without any ex-\nplicit supervision when trained on a new dataset\nof millions of webpages called WebText. When\nconditioned on a document plus questions, the an-\nswers generated by the language model reach 55\nF1 on the CoQA dataset - matching or exceeding\nthe performance of 3 out of 4 baseline systems\nwithout using the 127,000+ training examples.\nThe capacity of the language model is essential\nto the success of zero-shot task transfer and in-\ncreasing it improves performance in a log-linear\nfashion across tasks. Our largest model, GPT-2,\nis a 1.5B parameter Transformer that achieves\nstate of the art results on 7 out of 8 tested lan-\nguage modeling datasets in a zero-shot setting\nbut still underﬁts WebText. Samples from the\nmodel reﬂect these improvements and contain co-\nherent paragraphs of text. These ﬁndings suggest\na promising path towards building language pro-\ncessing systems which learn to perform tasks from\ntheir naturally occurring demonstrations.\n1. Introduction\nMachine learning systems now excel (in expectation) at\ntasks they are trained for by using a combination of large\ndatasets, high-capacity models, and supervised learning\n(Krizhevsky et al., 2012) (Sutskever et al., 2014) (Amodei\net al., 2016). Yet these systems are brittle and sensitive to\nslight changes in the data distribution (Recht et al., 2018)\nand task speciﬁcation (Kirkpatrick et al., 2017). Current sys-\ntems are better characterized as narrow experts rather than\n*, **Equal contribution\n1OpenAI, San Francisco, Califor-\nnia,\nUnited States.\nCorrespondence to:\nAlec Radford\n<alec@openai.com>.\ncompetent generalists. We would like to move towards more\ngeneral systems which can perform many tasks – eventually\nwithout the need to manually create and label a training\ndataset for each one.\nThe dominant approach to creating ML systems is to col-\nlect a dataset of training examples demonstrating correct\nbehavior for a desired task, train a system to imitate these\nbehaviors, and then test its performance on independent\nand identically distributed (IID) held-out examples. This\nhas served well to make progress on narrow experts. But\nthe often erratic behavior of captioning models (Lake et al.,\n2017), reading comprehension systems (Jia & Liang, 2017),\nand image classiﬁers (Alcorn et al., 2018) on the diversity\nand variety of possible inputs highlights some of the short-\ncomings of this approach.\nOur suspicion is that the prevalence of single task training\non single domain datasets is a major contributor to the lack\nof generalization observed in current systems. Progress\ntowards robust systems with current architectures is likely\nto require training and measuring performance on a wide\nrange of domains and tasks. Recently, several benchmarks\nhave been proposed such as GLUE (Wang et al., 2018) and\ndecaNLP (McCann et al., 2018) to begin studying this.\nMultitask learning (Caruana, 1997) is a promising frame-\nwork for improving general performance. However, mul-\ntitask training in NLP is still nascent. Recent work re-\nports modest performance improvements (Yogatama et al.,\n2019) and the two most ambitious efforts to date have\ntrained on a total of 10 and 17 (dataset, objective)\npairs respectively (McCann et al., 2018) (Bowman et al.,\n2018). From a meta-learning perspective, each (dataset,\nobjective) pair is a single training example sampled\nfrom the distribution of datasets and objectives. Current\nML systems need hundreds to thousands of examples to\ninduce functions which generalize well. This suggests that\nmultitask training many need just as many effective training\npairs to realize its promise with current approaches. It will\nbe very difﬁcult to continue to scale the creation of datasets\nand the design of objectives to the degree that may be re-\nquired to brute force our way there with current techniques.\nThis motivates exploring additional setups for performing\nmultitask learning.\nThe current best performing systems on language tasks\n\nLanguage Models are Unsupervised Multitask Learners\nFigure 1. Zero-shot task performance of WebText LMs as a function of model size on many NLP tasks. Reading Comprehension results\nare on CoQA (Reddy et al., 2018), translation on WMT-14 Fr-En (Artetxe et al., 2017), summarization on CNN and Daily Mail (See et al.,\n2017), and Question Answering on Natural Questions (Kwiatkowski et al., 2019). Section 3 contains detailed descriptions of each result.\nutilize a combination of pre-training and supervised ﬁne-\ntuning. This approach has a long history with a trend to-\nwards more ﬂexible forms of transfer. First, word vectors\nwere learned and used as inputs to task-speciﬁc architec-\ntures (Mikolov et al., 2013) (Collobert et al., 2011), then\nthe contextual representations of recurrent networks were\ntransferred (Dai & Le, 2015) (Peters et al., 2018), and re-\ncent work suggests that task-speciﬁc architectures are no\nlonger necessary and transferring many self-attention blocks\nis sufﬁcient (Radford et al., 2018) (Devlin et al., 2018).\nThese methods still require supervised training in order\nto perform a task. When only minimal or no supervised\ndata is available, another line of work has demonstrated\nthe promise of language models to perform speciﬁc tasks,\nsuch as commonsense reasoning (Schwartz et al., 2017) and\nsentiment analysis (Radford et al., 2017).\nIn this paper, we connect these two lines of work and con-\ntinue the trend of more general methods of transfer. We\ndemonstrate language models can perform down-stream\ntasks in a zero-shot setting – without any parameter or archi-\ntecture modiﬁcation. We demonstrate this approach shows\npotential by highlighting the ability of language models to\nperform a wide range of tasks in a zero-shot setting. We\nachieve promising, competitive, and state of the art results\ndepending on the task.\n2. Approach\nAt the core of our approach is language modeling. Lan-\nguage modeling is usually framed as unsupervised distri-\nbution estimation from a set of examples (x1, x2, ..., xn)\neach composed of variable length sequences of symbols\n(s1, s2, ..., sn). Since language has a natural sequential or-\ndering, it is common to factorize the joint probabilities over\nsymbols as the product of conditional probabilities (Jelinek\n& Mercer, 1980) (Bengio et al., 2003):\np(x) =\nn\nY\ni=1\np(sn|s1, ..., sn−1)\n(1)\nThis approach allows for tractable sampling from and es-\ntimation of p(x) as well as any conditionals of the form\np(sn−k, ..., sn|s1, ..., sn−k−1). In recent years, there have\nbeen signiﬁcant improvements in the expressiveness of mod-\nels that can compute these conditional probabilities, such as\nself-attention architectures like the Transformer (Vaswani\net al., 2017).\nLearning to perform a single task can be expressed in a\nprobabilistic framework as estimating a conditional distri-\nbution p(output|input). Since a general system should be\nable to perform many different tasks, even for the same\ninput, it should condition not only on the input but also\non the task to be performed.\nThat is, it should model\np(output|input, task). This has been variously formalized\nin multitask and meta-learning settings. Task conditioning\nis often implemented at an architectural level, such as the\ntask speciﬁc encoders and decoders in (Kaiser et al., 2017)\nor at an algorithmic level such as the inner and outer loop\noptimization framework of MAML (Finn et al., 2017). But\nas exempliﬁed in McCann et al. (2018), language provides\na ﬂexible way to specify tasks, inputs, and outputs all as a\nsequence of symbols. For example, a translation training\nexample can be written as the sequence (translate to\nfrench, english text, french text). Like-\nwise, a reading comprehension training example can\nbe written as (answer the question, document,\nquestion, answer). McCann et al. (2018) demon-\nstrated it was possible to train a single model, the MQAN,\n\nLanguage Models are Unsupervised Multitask Learners\nto infer and perform many different tasks on examples with\nthis type of format.\nLanguage modeling is also able to, in principle, learn the\ntasks of McCann et al. (2018) without the need for explicit\nsupervision of which symbols are the outputs to be pre-\ndicted. Since the supervised objective is the the same as the\nunsupervised objective but only evaluated on a subset of the\nsequence, the global minimum of the unsupervised objective\nis also the global minimum of the supervised objective. In\nthis slightly toy setting, the concerns with density estimation\nas a principled training objective discussed in (Sutskever\net al., 2015) are side stepped. The problem instead becomes\nwhether we are able to, in practice, optimize the unsuper-\nvised objective to convergence. Preliminary experiments\nconﬁrmed that sufﬁciently large language models are able to\nperform multitask learning in this toy-ish setup but learning\nis much slower than in explicitly supervised approaches.\nWhile it is a large step from the well-posed setup described\nabove to the messiness of “language in the wild”, Weston\n(2016) argues, in the context of dialog, for the need to\ndevelop systems capable of learning from natural language\ndirectly and demonstrated a proof of concept – learning a\nQA task without a reward signal by using forward prediction\nof a teacher’s outputs. While dialog is an attractive approach,\nwe worry it is overly restrictive. The internet contains a vast\namount of information that is passively available without\nthe need for interactive communication. Our speculation is\nthat a language model with sufﬁcient capacity will begin\nto learn to infer and perform the tasks demonstrated in\nnatural language sequences in order to better predict them,\nregardless of their method of procurement. If a language\nmodel is able to do this it will be, in effect, performing\nunsupervised multitask learning. We test whether this is the\ncase by analyzing the performance of language models in a\nzero-shot setting on a wide variety of tasks.\n2.1. Training Dataset\nMost prior work trained language models on a single do-\nmain of text, such as news articles (Jozefowicz et al., 2016),\nWikipedia (Merity et al., 2016), or ﬁction books (Kiros\net al., 2015). Our approach motivates building as large and\ndiverse a dataset as possible in order to collect natural lan-\nguage demonstrations of tasks in as varied of domains and\ncontexts as possible.\nA promising source of diverse and nearly unlimited text is\nweb scrapes such as Common Crawl. While these archives\nare many orders of magnitude larger than current language\nmodeling datasets, they have signiﬁcant data quality issues.\nTrinh & Le (2018) used Common Crawl in their work on\ncommonsense reasoning but noted a large amount of doc-\numents “whose content are mostly unintelligible”. We ob-\nserved similar data issues in our initial experiments with\n”I’m not the cleverest man in the world, but like they say in\nFrench: Je ne suis pas un imbecile [I’m not a fool].\nIn a now-deleted post from Aug. 16, Soheil Eid, Tory candidate\nin the riding of Joliette, wrote in French: ”Mentez mentez,\nil en restera toujours quelque chose,” which translates as,\n”Lie lie and something will always remain.”\n“I hate the word ‘perfume,”’ Burr says. ‘It’s somewhat better\nin French: ‘parfum.’\nIf listened carefully at 29:55, a conversation can be heard\nbetween two guys in French: “-Comment on fait pour aller\nde l’autre cot´e? -Quel autre cot´e?”, which means “- How\ndo you get to the other side? - What side?”.\nIf this sounds like a bit of a stretch, consider this ques-\ntion in French: As-tu aller au cin´ema?, or Did you go to\nthe movies?, which literally translates as Have-you to go to\nmovies/theater?\n“Brevet Sans Garantie Du Gouvernement”, translated to\nEnglish: “Patented without government warranty”.\nTable 1. Examples of naturally occurring demonstrations of En-\nglish to French and French to English translation found throughout\nthe WebText training set.\nCommon Crawl. Trinh & Le (2018)’s best results were\nachieved using a small subsample of Common Crawl which\nincluded only documents most similar to their target dataset,\nthe Winograd Schema Challenge. While this is a pragmatic\napproach to improve performance on a speciﬁc task, we\nwant to avoid making assumptions about the tasks to be\nperformed ahead of time.\nInstead, we created a new web scrape which emphasizes\ndocument quality. To do this we only scraped web pages\nwhich have been curated/ﬁltered by humans. Manually\nﬁltering a full web scrape would be exceptionally expensive\nso as a starting point, we scraped all outbound links from\nReddit, a social media platform, which received at least 3\nkarma. This can be thought of as a heuristic indicator for\nwhether other users found the link interesting, educational,\nor just funny.\nThe resulting dataset, WebText, contains the text subset\nof these 45 million links. To extract the text from HTML\nresponses we use a combination of the Dragnet (Peters &\nLecocq, 2013) and Newspaper1 content extractors. All re-\nsults presented in this paper use a preliminary version of\nWebText which does not include links created after Dec\n2017 and which after de-duplication and some heuristic\nbased cleaning contains slightly over 8 million documents\nfor a total of 40 GB of text. We removed all Wikipedia\ndocuments from WebText since it is a common data source\nfor other datasets and could complicate analysis due to over-\n1https://github.com/codelucas/newspaper\n\nLanguage Models are Unsupervised Multitask Learners\nlapping training data with test evaluation tasks.\n2.2. Input Representation\nA general language model (LM) should be able to compute\nthe probability of (and also generate) any string. Current\nlarge scale LMs include pre-processing steps such as lower-\ncasing, tokenization, and out-of-vocabulary tokens which\nrestrict the space of model-able strings. While processing\nUnicode strings as a sequence of UTF-8 bytes elegantly ful-\nﬁlls this requirement as exempliﬁed in work such as Gillick\net al. (2015), current byte-level LMs are not competitive\nwith word-level LMs on large scale datasets such as the\nOne Billion Word Benchmark (Al-Rfou et al., 2018). We\nobserved a similar performance gap in our own attempts to\ntrain standard byte-level LMs on WebText.\nByte Pair Encoding (BPE) (Sennrich et al., 2015) is a\npractical middle ground between character and word level\nlanguage modeling which effectively interpolates between\nword level inputs for frequent symbol sequences and char-\nacter level inputs for infrequent symbol sequences. Despite\nits name, reference BPE implementations often operate on\nUnicode code points and not byte sequences. These imple-\nmentations would require including the full space of Uni-\ncode symbols in order to model all Unicode strings. This\nwould result in a base vocabulary of over 130,000 before\nany multi-symbol tokens are added. This is prohibitively\nlarge compared to the 32,000 to 64,000 token vocabularies\noften used with BPE. In contrast, a byte-level version of\nBPE only requires a base vocabulary of size 256. However,\ndirectly applying BPE to the byte sequence results in sub-\noptimal merges due to BPE using a greedy frequency based\nheuristic for building the token vocabulary. We observed\nBPE including many versions of common words like dog\nsince they occur in many variations such as dog. dog!\ndog? . This results in a sub-optimal allocation of limited\nvocabulary slots and model capacity. To avoid this, we pre-\nvent BPE from merging across character categories for any\nbyte sequence. We add an exception for spaces which sig-\nniﬁcantly improves the compression efﬁciency while adding\nonly minimal fragmentation of words across multiple vocab\ntokens.\nThis input representation allows us to combine the empirical\nbeneﬁts of word-level LMs with the generality of byte-level\napproaches. Since our approach can assign a probability to\nany Unicode string, this allows us to evaluate our LMs on\nany dataset regardless of pre-processing, tokenization, or\nvocab size.\n2.3. Model\nWe use a Transformer (Vaswani et al., 2017) based archi-\ntecture for our LMs. The model largely follows the details\nof the OpenAI GPT model (Radford et al., 2018) with a\nParameters\nLayers\ndmodel\n117M\n12\n768\n345M\n24\n1024\n762M\n36\n1280\n1542M\n48\n1600\nTable 2. Architecture hyperparameters for the 4 model sizes.\nfew modiﬁcations. Layer normalization (Ba et al., 2016)\nwas moved to the input of each sub-block, similar to a\npre-activation residual network (He et al., 2016) and an\nadditional layer normalization was added after the ﬁnal self-\nattention block. A modiﬁed initialization which accounts\nfor the accumulation on the residual path with model depth\nis used. We scale the weights of residual layers at initial-\nization by a factor of 1/\n√\nN where N is the number of\nresidual layers. The vocabulary is expanded to 50,257. We\nalso increase the context size from 512 to 1024 tokens and\na larger batchsize of 512 is used.\n3. Experiments\nWe trained and benchmarked four LMs with approximately\nlog-uniformly spaced sizes. The architectures are summa-\nrized in Table 2. The smallest model is equivalent to the\noriginal GPT, and the second smallest equivalent to the\nlargest model from BERT (Devlin et al., 2018). Our largest\nmodel, which we call GPT-2, has over an order of magni-\ntude more parameters than GPT. The learning rate of each\nmodel was manually tuned for the best perplexity on a 5%\nheld-out sample of WebText. All models still underﬁt Web-\nText and held-out perplexity has as of yet improved given\nmore training time.\n3.1. Language Modeling\nAs an initial step towards zero-shot task transfer, we are\ninterested in understanding how WebText LM’s perform\nat zero-shot domain transfer on the primary task they are\ntrained for – language modeling. Since our model operates\non a byte level and does not require lossy pre-processing\nor tokenization, we can evaluate it on any language model\nbenchmark. Results on language modeling datasets are\ncommonly reported in a quantity which is a scaled or ex-\nponentiated version of the average negative log probability\nper canonical prediction unit - usually a character, a byte, or\na word. We evaluate the same quantity by computing the\nlog-probability of a dataset according to a WebText LM and\ndividing by the number of canonical units. For many of these\ndatasets, WebText LMs would be tested signiﬁcantly out-\nof-distribution, having to predict aggressively standardized\ntext, tokenization artifacts such as disconnected punctuation\nand contractions, shufﬂed sentences, and even the string\n\nLanguage Models are Unsupervised Multitask Learners\nLAMBADA\nLAMBADA\nCBT-CN\nCBT-NE\nWikiText2\nPTB\nenwik8\ntext8\nWikiText103\n1BW\n(PPL)\n(ACC)\n(ACC)\n(ACC)\n(PPL)\n(PPL)\n(BPB)\n(BPC)\n(PPL)\n(PPL)\nSOTA\n99.8\n59.23\n85.7\n82.3\n39.14\n46.54\n0.99\n1.08\n18.3\n21.8\n117M\n35.13\n45.99\n87.65\n83.4\n29.41\n65.85\n1.16\n1.17\n37.50\n75.20\n345M\n15.60\n55.48\n92.35\n87.1\n22.76\n47.33\n1.01\n1.06\n26.37\n55.72\n762M\n10.87\n60.12\n93.45\n88.0\n19.93\n40.31\n0.97\n1.02\n22.05\n44.575\n1542M\n8.63\n63.24\n93.30\n89.05\n18.34\n35.76\n0.93\n0.98\n17.48\n42.16\nTable 3. Zero-shot results on many datasets. No training or ﬁne-tuning was performed for any of these results. PTB and WikiText-2\nresults are from (Gong et al., 2018). CBT results are from (Bajgar et al., 2016). LAMBADA accuracy result is from (Hoang et al., 2018)\nand LAMBADA perplexity result is from (Grave et al., 2016). Other results are from (Dai et al., 2019).\n<UNK> which is extremely rare in WebText - occurring\nonly 26 times in 40 billion bytes. We report our main re-\nsults in Table 3 using invertible de-tokenizers which remove\nas many of these tokenization / pre-processing artifacts as\npossible. Since these de-tokenizers are invertible, we can\nstill calculate the log probability of a dataset and they can\nbe thought of as a simple form of domain adaptation. We\nobserve gains of 2.5 to 5 perplexity for GPT-2 with these\nde-tokenizers.\nWebText LMs transfer well across domains and datasets,\nimproving the state of the art on 7 out of the 8 datasets in a\nzero-shot setting. Large improvements are noticed on small\ndatasets such as Penn Treebank and WikiText-2 which have\nonly 1 to 2 million training tokens. Large improvements\nare also noticed on datasets created to measure long-term\ndependencies like LAMBADA (Paperno et al., 2016) and\nthe Children’s Book Test (Hill et al., 2015). Our model is\nstill signiﬁcantly worse than prior work on the One Billion\nWord Benchmark (Chelba et al., 2013). This is likely due\nto a combination of it being both the largest dataset and\nhaving some of the most destructive pre-processing - 1BW’s\nsentence level shufﬂing removes all long-range structure.\n3.2. Children’s Book Test\nFigure 2. Performance on the Children’s Book Test as a function of\nmodel capacity. Human performance are from Bajgar et al. (2016),\ninstead of the much lower estimates from the original paper.\nThe Children’s Book Test (CBT) (Hill et al., 2015) was\ncreated to examine the performance of LMs on different cat-\negories of words: named entities, nouns, verbs, and preposi-\ntions. Rather than reporting perplexity as an evaluation met-\nric, CBT reports accuracy on an automatically constructed\ncloze test where the task is to predict which of 10 possible\nchoices for an omitted word is correct. Following the LM\napproach introduced in the original paper, we compute the\nprobability of each choice and the rest of the sentence con-\nditioned on this choice according to the LM, and predict\nthe one with the highest probability. As seen in Figure 2\nperformance steadily improves as model size is increased\nand closes the majority of the gap to human performance\non this test. Data overlap analysis showed one of the CBT\ntest set books, The Jungle Book by Rudyard Kipling, is in\nWebText, so we report results on the validation set which\nhas no signiﬁcant overlap. GPT-2 achieves new state of the\nart results of 93.3% on common nouns and 89.1% on named\nentities. A de-tokenizer was applied to remove PTB style\ntokenization artifacts from CBT.\n3.3. LAMBADA\nThe LAMBADA dataset (Paperno et al., 2016) tests the\nability of systems to model long-range dependencies in\ntext. The task is to predict the ﬁnal word of sentences\nwhich require at least 50 tokens of context for a human to\nsuccessfully predict. GPT-2 improves the state of the art\nfrom 99.8 (Grave et al., 2016) to 8.6 perplexity and increases\nthe accuracy of LMs on this test from 19% (Dehghani et al.,\n2018) to 52.66%. Investigating GPT-2’s errors showed most\npredictions are valid continuations of the sentence, but are\nnot valid ﬁnal words. This suggests that the LM is not\nusing the additional useful constraint that the word must be\nthe ﬁnal of the sentence. Adding a stop-word ﬁlter as an\napproximation to this further increases accuracy to 63.24%,\nimproving the overall state of the art on this task by 4%. The\nprevious state of the art (Hoang et al., 2018) used a different\nrestricted prediction setting where the outputs of the model\nwere constrained to only words that appeared in the context.\nFor GPT-2, this restriction is harmful rather than helpful\n\nLanguage Models are Unsupervised Multitask Learners\nsince 19% of answers are not in context. We use a version\nof the dataset without preprocessing.\n3.4. Winograd Schema Challenge\nFigure 3. Performance on the Winograd Schema Challenge as a\nfunction of model capacity.\nThe Winograd Schema challenge (Levesque et al., 2012)\nwas constructed to measure the capability of a system to\nperform commonsense reasoning by measuring its ability\nto resolve ambiguities in text. Recently Trinh & Le (2018)\ndemonstrated signiﬁcant progress on this challenge using\nLMs, by predicting the resolution of the ambiguity with\nhigher probability. We follow their problem formulation and\nvisualize the performance of our models with both full and\npartial scoring techniques in Figure 3. GPT-2 improves state\nof the art accuracy by 7%, achieving 70.70%. The dataset\nis quite small with only 273 examples so we recommend\nreading Trichelair et al. (2018) to help contextualize this\nresult.\n3.5. Reading Comprehension\nThe Conversation Question Answering dataset (CoQA)\nReddy et al. (2018) consists of documents from 7 different\ndomains paired with natural language dialogues between a\nquestion asker and a question answerer about the document.\nCoQA tests reading comprehension capabilities and also\nthe ability of models to answer questions that depend on\nconversation history (such as “Why?”).\nGreedy decoding from GPT-2 when conditioned on a doc-\nument, the history of the associated conversation, and a\nﬁnal token A: achieves 55 F1 on the development set. This\nmatches or exceeds the performance of 3 out of 4 base-\nline systems without using the 127,000+ manually collected\nquestion answer pairs those baselines were trained on. The\nsupervised SOTA, a BERT based system (Devlin et al.,\nR-1\nR-2\nR-L\nR-AVG\nBottom-Up Sum\n41.22\n18.68\n38.34\n32.75\nLede-3\n40.38\n17.66\n36.62\n31.55\nSeq2Seq + Attn\n31.33\n11.81\n28.83\n23.99\nGPT-2 TL;DR:\n29.34\n8.27\n26.58\n21.40\nRandom-3\n28.78\n8.63\n25.52\n20.98\nGPT-2 no hint\n21.58\n4.03\n19.47\n15.03\nTable 4. Summarization performance as measured by ROUGE F1\nmetrics on the CNN and Daily Mail dataset. Bottom-Up Sum is\nthe SOTA model from (Gehrmann et al., 2018)\n2018), is nearing the 89 F1 performance of humans. While\nGPT-2’s performance is exciting for a system without any su-\npervised training, some inspection of its answers and errors\nsuggests GPT-2 often uses simple retrieval based heuristics\nsuch as answer with a name from the document in response\nto a who question.\n3.6. Summarization\nWe test GPT-2’s ability to perform summarization on the\nCNN and Daily Mail dataset (Nallapati et al., 2016). To in-\nduce summarization behavior we add the text TL;DR: after\nthe article and generate 100 tokens with Top-k random sam-\npling (Fan et al., 2018) with k = 2 which reduces repetition\nand encourages more abstractive summaries than greedy de-\ncoding. We use the ﬁrst 3 generated sentences in these 100\ntokens as the summary. While qualitatively the generations\nresemble summaries, as shown in Table 14, they often focus\non recent content from the article or confuse speciﬁc details\nsuch as how many cars were involved in a crash or whether\na logo was on a hat or shirt. On the commonly reported\nROUGE 1,2,L metrics the generated summaries only begin\nto approach the performance of classic neural baselines and\njust barely outperforms selecting 3 random sentences from\nthe article. GPT-2’s performance drops by 6.4 points on\nthe aggregate metric when the task hint is removed which\ndemonstrates the ability to invoke task speciﬁc behavior in\na language model with natural language.\n3.7. Translation\nWe test whether GPT-2 has begun to learn how to translate\nfrom one language to another. In order to help it infer that\nthis is the desired task, we condition the language model\non a context of example pairs of the format english\nsentence = french sentence and then after a ﬁ-\nnal prompt of english sentence = we sample from\nthe model with greedy decoding and use the ﬁrst generated\nsentence as the translation. On the WMT-14 English-French\ntest set, GPT-2 gets 5 BLEU, which is slightly worse than\na word-by-word substitution with a bilingual lexicon in-\nferred in previous work on unsupervised word translation\n\nLanguage Models are Unsupervised Multitask Learners\nQuestion\nGenerated Answer\nCorrect\nProbability\nWho wrote the book the origin of species?\nCharles Darwin\n\u0013\n83.4%\nWho is the founder of the ubuntu project?\nMark Shuttleworth\n\u0013\n82.0%\nWho is the quarterback for the green bay packers?\nAaron Rodgers\n\u0013\n81.1%\nPanda is a national animal of which country?\nChina\n\u0013\n76.8%\nWho came up with the theory of relativity?\nAlbert Einstein\n\u0013\n76.4%\nWhen was the ﬁrst star wars ﬁlm released?\n1977\n\u0013\n71.4%\nWhat is the most common blood type in sweden?\nA\n\u0017\n70.6%\nWho is regarded as the founder of psychoanalysis?\nSigmund Freud\n\u0013\n69.3%\nWho took the ﬁrst steps on the moon in 1969?\nNeil Armstrong\n\u0013\n66.8%\nWho is the largest supermarket chain in the uk?\nTesco\n\u0013\n65.3%\nWhat is the meaning of shalom in english?\npeace\n\u0013\n64.0%\nWho was the author of the art of war?\nSun Tzu\n\u0013\n59.6%\nLargest state in the us by land mass?\nCalifornia\n\u0017\n59.2%\nGreen algae is an example of which type of reproduction?\nparthenogenesis\n\u0017\n56.5%\nVikram samvat calender is ofﬁcial in which country?\nIndia\n\u0013\n55.6%\nWho is mostly responsible for writing the declaration of independence?\nThomas Jefferson\n\u0013\n53.3%\nWhat us state forms the western boundary of montana?\nMontana\n\u0017\n52.3%\nWho plays ser davos in game of thrones?\nPeter Dinklage\n\u0017\n52.1%\nWho appoints the chair of the federal reserve system?\nJanet Yellen\n\u0017\n51.5%\nState the process that divides one nucleus into two genetically identical nuclei?\nmitosis\n\u0013\n50.7%\nWho won the most mvp awards in the nba?\nMichael Jordan\n\u0017\n50.2%\nWhat river is associated with the city of rome?\nthe Tiber\n\u0013\n48.6%\nWho is the ﬁrst president to be impeached?\nAndrew Johnson\n\u0013\n48.3%\nWho is the head of the department of homeland security 2017?\nJohn Kelly\n\u0013\n47.0%\nWhat is the name given to the common currency to the european union?\nEuro\n\u0013\n46.8%\nWhat was the emperor name in star wars?\nPalpatine\n\u0013\n46.5%\nDo you have to have a gun permit to shoot at a range?\nNo\n\u0013\n46.4%\nWho proposed evolution in 1859 as the basis of biological development?\nCharles Darwin\n\u0013\n45.7%\nNuclear power plant that blew up in russia?\nChernobyl\n\u0013\n45.7%\nWho played john connor in the original terminator?\nArnold Schwarzenegger\n\u0017\n45.2%\nTable 5. The 30 most conﬁdent answers generated by GPT-2 on the development set of Natural Questions sorted by their probability\naccording to GPT-2. None of these questions appear in WebText according to the procedure described in Section 4.\n(Conneau et al., 2017b). On the WMT-14 French-English\ntest set, GPT-2 is able to leverage its very strong English\nlanguage model to perform signiﬁcantly better, achieving\n11.5 BLEU. This outperforms several unsupervised machine\ntranslation baselines from (Artetxe et al., 2017) and (Lample\net al., 2017) but is still much worse than the 33.5 BLEU of\nthe current best unsupervised machine translation approach\n(Artetxe et al., 2019). Performance on this task was sur-\nprising to us, since we deliberately removed non-English\nwebpages from WebText as a ﬁltering step. In order to con-\nﬁrm this, we ran a byte-level language detector2 on WebText\nwhich detected only 10MB of data in the French language\nwhich is approximately 500x smaller than the monolingual\nFrench corpus common in prior unsupervised machine trans-\nlation research.\n3.8. Question Answering\nA potential way to test what information is contained within\na language model is to evaluate how often it generates the\ncorrect answer to factoid-style questions. Previous showcas-\ning of this behavior in neural systems where all information\nis stored in parameters such as A Neural Conversational\nModel (Vinyals & Le, 2015) reported qualitative results due\nto the lack of high-quality evaluation datasets. The recently\nintroduced Natural Questions dataset (Kwiatkowski et al.,\n2https://github.com/CLD2Owners/cld2\n2019) is a promising resource to test this more quantita-\ntively. Similar to translation, the context of the language\nmodel is seeded with example question answer pairs which\nhelps the model infer the short answer style of the dataset.\nGPT-2 answers 4.1% of questions correctly when evalu-\nated by the exact match metric commonly used on reading\ncomprehension datasets like SQUAD.3 As a comparison\npoint, the smallest model does not exceed the 1.0% accu-\nracy of an incredibly simple baseline which returns the most\ncommon answer for each question type (who, what, where,\netc...). GPT-2 answers 5.3 times more questions correctly,\nsuggesting that model capacity has been a major factor in\nthe poor performance of neural systems on this kind of task\nas of yet. The probability GPT-2 assigns to its generated\nanswers is well calibrated and GPT-2 has an accuracy of\n63.1% on the 1% of questions it is most conﬁdent in. The\n30 most conﬁdent answers generated by GPT-2 on develop-\nment set questions are shown in Table 5. The performance\nof GPT-2 is still much, much, worse than the 30 to 50%\nrange of open domain question answering systems which\nhybridize information retrieval with extractive document\nquestion answering (Alberti et al., 2019).\n3Alec, who previously thought of himself as good at random\ntrivia, answered 17 of 100 randomly sampled examples correctly\nwhen tested in the same setting as GPT-2. He actually only got 14 right but he\nshould have gotten those other 3\n\nLanguage Models are Unsupervised Multitask Learners\nPTB\nWikiText-2\nenwik8\ntext8\nWikitext-103\n1BW\nDataset train\n2.67%\n0.66%\n7.50%\n2.34%\n9.09%\n13.19%\nWebText train\n0.88%\n1.63%\n6.31%\n3.94%\n2.42%\n3.75%\nTable 6. Percentage of test set 8 grams overlapping with training sets.\n4. Generalization vs Memorization\nRecent work in computer vision has shown that common im-\nage datasets contain a non-trivial amount of near-duplicate\nimages. For instance CIFAR-10 has 3.3% overlap between\ntrain and test images (Barz & Denzler, 2019). This results in\nan over-reporting of the generalization performance of ma-\nchine learning systems. As the size of datasets increases this\nissue becomes increasingly likely which suggests a similar\nphenomena could be happening with WebText. Therefore it\nis important to analyze how much test data also shows up in\nthe training data.\nTo study this we created Bloom ﬁlters containing 8-grams\nof WebText training set tokens. To improve recall, strings\nwere normalized to contain only lower-cased alphanumeric\nwords with a single space as a delimiter. The Bloom ﬁlters\nwere constructed such that the false positive rate is upper\nbounded by\n1\n108 . We further veriﬁed the low false positive\nrate by generating 1M strings, of which zero were found by\nthe ﬁlter.\nThese Bloom ﬁlters let us calculate, given a dataset, the\npercentage of 8-grams from that dataset that are also found\nin the WebText training set. Table 6 shows this overlap anal-\nysis for the test sets of common LM benchmarks. Common\nLM datasets’ test sets have between 1-6% overlap with Web-\nText train, with an average of overlap of 3.2%. Somewhat\nsurprisingly, many datasets have larger overlaps with their\nown training splits, with an average of 5.9% overlap.\nOur approach optimizes for recall, and while manual inspec-\ntion of the overlaps shows many common phrases, there are\nmany longer matches that are due to duplicated data. This is\nnot unique to WebText. For instance, we discovered that the\ntest set of WikiText-103 has an article which is also in the\ntraining dataset. Since there are only 60 articles in the test\nset there is at least an overlap of 1.6%.4 Potentially more\nworryingly, 1BW has an overlap of nearly 13.2% with its\nown training set according to our procedure.\nFor the Winograd Schema Challenge, we found only 10\nschemata which had any 8-gram overlaps with the WebText\ntraining set. Of these, 2 were spurious matches. Of the\nremaining 8, only 1 schema appeared in any contexts that\n4A signiﬁcant portion of additional overlap is due to editors\nreusing some paragraphs across multiple articles with a shared\ntheme such as various battles in the Korean War.\ngave away the answer.\nFor CoQA, about 15% of documents in the news domain\nare already in WebText and the model performs about 3\nF1 better on these. CoQA’s development set metric reports\nthe average performance over 5 different domains and we\nmeasure a gain of about 0.5-1.0 F1 due to overlap across the\nvarious domains. However, no actual training questions or\nanswers are in WebText since CoQA was released after the\ncutoff date for links in WebText.\nOn LAMBADA, the average overlap is 1.2%. GPT-2 per-\nforms about 2 perplexity better on examples with greater\nthan 15% overlap. Recalculating metrics when excluding\nall examples with any overlap shifts results from 8.6 to 8.7\nperplexity and reduces accuracy from 63.2% to 62.9%. This\nvery small change in overall results is likely due to only 1\nin 200 examples having signiﬁcant overlap.\nOverall, our analysis suggests that data overlap between\nWebText training data and speciﬁc evaluation datasets pro-\nvides a small but consistent beneﬁt to reported results. How-\never, for most datasets we do not notice signiﬁcantly larger\noverlaps than those already existing between standard train-\ning and test sets, as Table 6 highlights.\nUnderstanding and quantifying how highly similar text im-\npacts performance is an important research question. Better\nde-duplication techniques such as scalable fuzzy matching\ncould also help better answer these questions. For now, we\nrecommend the use of n-gram overlap based de-duplication\nas an important veriﬁcation step and sanity check during the\ncreation of training and test splits for new NLP datasets.\nAnother potential way of determining whether the perfor-\nmance of WebText LMs is attributable to memorization is\ninspecting their performance on their own held-out set. As\nshown in Figure 4, performance on both the training and\ntest sets of WebText are similar and improve together as\nmodel size is increased. This suggests even GPT-2 is still\nunderﬁtting on WebText in many ways.\nGPT-2 is also able to write news articles about the discovery\nof talking unicorns. An example is provided in Table 13.\n5. Related Work\nA signiﬁcant portion of this work measured the performance\nof larger language models trained on larger datasets. This\n\nLanguage Models are Unsupervised Multitask Learners\nFigure 4. The performance of LMs trained on WebText as a func-\ntion of model size.\nis similar to the work of Jozefowicz et al. (2016) which\nscaled RNN based language models on the 1 Billion Word\nBenchmark. Bajgar et al. (2016) also previously improved\nresults on the Children’s Book Test by creating a much larger\ntraining dataset out of Project Gutenberg to supplement the\nstandard training dataset. Hestness et al. (2017) conducted\na thorough analysis of how the performance of various deep\nlearning models changes as a function of both model capac-\nity and dataset size. Our experiments, while much noisier\nacross tasks, suggest similar trends hold for sub-tasks of an\nobjective and continue into the 1B+ parameter regime.\nInteresting learned functionality in generative models\nhas been documented before such as the cells in an\nRNN language model performing line-width tracking and\nquote/comment detection Karpathy et al. (2015). More in-\nspirational to our work was the observation of Liu et al.\n(2018) that a model trained to generate Wikipedia articles\nalso learned to translate names between languages.\nPrevious work has explored alternative approaches to ﬁlter-\ning and constructing a large text corpus of web pages, such\nas the iWeb Corpus (Davies, 2018).\nThere has been extensive work on pre-training methods\nfor language tasks. In addition to those mentioned in the\nintroduction, GloVe (Pennington et al., 2014) scaled word\nvector representation learning to all of Common Crawl. An\ninﬂuential early work on deep representation learning for\ntext was Skip-thought Vectors (Kiros et al., 2015). McCann\net al. (2017) explored the use of representations derived from\nmachine translation models and Howard & Ruder (2018)\nimproved the RNN based ﬁne-tuning approaches of (Dai\n& Le, 2015). (Conneau et al., 2017a) studied the transfer\nperformance of representations learned by natural language\ninference models and (Subramanian et al., 2018) explored\nlarge-scale multitask training.\n(Ramachandran et al., 2016) demonstrated that seq2seq mod-\nels beneﬁt from being initialized with pre-trained language\nmodels as encoders and decoders. More recent work has\nshown that LM pre-training is helpful when ﬁne-tuned for\ndifﬁcult generation tasks like chit-chat dialog and dialog\nbased question answering systems as well (Wolf et al., 2019)\n(Dinan et al., 2018).\n6. Discussion\nMuch research has been dedicated to learning (Hill et al.,\n2016), understanding (Levy & Goldberg, 2014), and criti-\ncally evaluating (Wieting & Kiela, 2019) the representations\nof both supervised and unsupervised pre-training methods.\nOur results suggest that unsupervised task learning is an\nadditional promising area of research to explore. These\nﬁndings potentially help explain the widespread success of\npre-training techniques for down-stream NLP tasks as we\nshow that, in the limit, one of these pre-training techniques\nbegins to learn to perform tasks directly without the need\nfor supervised adaption or modiﬁcation.\nOn reading comprehension the performance of GPT-2 is\ncompetitive with supervised baselines in a zero-shot setting.\nHowever, on other tasks such as summarization, while it\nis qualitatively performing the task, its performance is still\nonly rudimentary according to quantitative metrics. While\nsuggestive as a research result, in terms of practical applica-\ntions, the zero-shot performance of GPT-2 is still far from\nuse-able.\nWe have studied the zero-shot performance of WebText\nLMs on many canonical NLP tasks, but there are many addi-\ntional tasks that could be evaluated. There are undoubtedly\nmany practical tasks where the performance of GPT-2 is\nstill no better than random. Even on common tasks that we\nevaluated on, such as question answering and translation,\nlanguage models only begin to outperform trivial baselines\nwhen they have sufﬁcient capacity.\nWhile zero-shot performance establishes a baseline of the\npotential performance of GPT-2 on many tasks, it is not\nclear where the ceiling is with ﬁnetuning. On some tasks,\nGPT-2’s fully abstractive output is a signiﬁcant departure\nfrom the extractive pointer network (Vinyals et al., 2015)\nbased outputs which are currently state of the art on many\nquestion answering and reading comprehension datasets.\nGiven the prior success of ﬁne-tuning GPT, we plan to in-\nvestigate ﬁne-tuning on benchmarks such as decaNLP and\nGLUE, especially since it is unclear whether the additional\n\nLanguage Models are Unsupervised Multitask Learners\ntraining data and capacity of GPT-2 is sufﬁcient to over-\ncome the inefﬁciencies of uni-directional representations\ndemonstrated by BERT (Devlin et al., 2018).\n7. Conclusion\nWhen a large language model is trained on a sufﬁciently\nlarge and diverse dataset it is able to perform well across\nmany domains and datasets. GPT-2 zero-shots to state of\nthe art performance on 7 out of 8 tested language model-\ning datasets. The diversity of tasks the model is able to\nperform in a zero-shot setting suggests that high-capacity\nmodels trained to maximize the likelihood of a sufﬁciently\nvaried text corpus begin to learn how to perform a surprising\namount of tasks without the need for explicit supervision.5\nAcknowledgements\nThanks to everyone who wrote the text, shared the links,\nand upvoted the content in WebText. Many millions of\npeople were involved in creating the data that GPT-2 was\ntrained on. Also thanks to all the Googlers who helped us\nwith training infrastructure, including Zak Stone, JS Riehl,\nJonathan Hseu, Russell Power, Youlong Cheng, Noam\nShazeer, Solomon Boulos, Michael Banﬁeld, Aman Gupta,\nDaniel Sohn, and many more. Finally thanks to the people\nwho gave feedback on drafts of the paper: Jacob Steinhardt,\nSam Bowman, Geoffrey Irving, and Madison May.\nReferences\nAl-Rfou, R., Choe, D., Constant, N., Guo, M., and Jones, L.\nCharacter-level language modeling with deeper self-attention.\narXiv preprint arXiv:1808.04444, 2018.\nAlberti, C., Lee, K., and Collins, M. A bert baseline for the natural\nquestions. arXiv preprint arXiv:1901.08634, 2019.\nAlcorn, M. A., Li, Q., Gong, Z., Wang, C., Mai, L., Ku, W.-S., and\nNguyen, A. Strike (with) a pose: Neural networks are easily\nfooled by strange poses of familiar objects. arXiv preprint\narXiv:1811.11553, 2018.\nAmodei, D., Ananthanarayanan, S., Anubhai, R., Bai, J., Batten-\nberg, E., Case, C., Casper, J., Catanzaro, B., Cheng, Q., Chen,\nG., et al. Deep speech 2: End-to-end speech recognition in\nenglish and mandarin. In International Conference on Machine\nLearning, pp. 173–182, 2016.\nArtetxe, M., Labaka, G., Agirre, E., and Cho, K. Unsupervised\nneural machine translation. arXiv preprint arXiv:1710.11041,\n2017.\nArtetxe, M., Labaka, G., and Agirre, E.\nAn effective ap-\nproach to unsupervised machine translation. arXiv preprint\narXiv:1902.01313, 2019.\n5Preliminary code for downloading and using the small model\nis available at https://github.com/openai/gpt-2\nBa, J. L., Kiros, J. R., and Hinton, G. E. Layer normalization.\narXiv preprint arXiv:1607.06450, 2016.\nBajgar, O., Kadlec, R., and Kleindienst, J. Embracing data abun-\ndance: Booktest dataset for reading comprehension. arXiv\npreprint arXiv:1610.00956, 2016.\nBarz, B. and Denzler, J. Do we train on test data? purging cifar of\nnear-duplicates. arXiv preprint arXiv:1902.00423, 2019.\nBengio, Y., Ducharme, R., Vincent, P., and Jauvin, C. A neural\nprobabilistic language model. Journal of machine learning\nresearch, 3(Feb):1137–1155, 2003.\nBowman, S. R., Pavlick, E., Grave, E., Van Durme, B., Wang, A.,\nHula, J., Xia, P., Pappagari, R., McCoy, R. T., Patel, R., et al.\nLooking for elmo’s friends: Sentence-level pretraining beyond\nlanguage modeling. arXiv preprint arXiv:1812.10860, 2018.\nCaruana, R. Multitask learning. Machine learning, 28(1):41–75,\n1997.\nChelba, C., Mikolov, T., Schuster, M., Ge, Q., Brants, T., Koehn,\nP., and Robinson, T. One billion word benchmark for measur-\ning progress in statistical language modeling. arXiv preprint\narXiv:1312.3005, 2013.\nCollobert, R., Weston, J., Bottou, L., Karlen, M., Kavukcuoglu,\nK., and Kuksa, P. Natural language processing (almost) from\nscratch. Journal of Machine Learning Research, 12(Aug):2493–\n2537, 2011.\nConneau, A., Kiela, D., Schwenk, H., Barrault, L., and Bor-\ndes, A. Supervised learning of universal sentence represen-\ntations from natural language inference data. arXiv preprint\narXiv:1705.02364, 2017a.\nConneau, A., Lample, G., Ranzato, M., Denoyer, L., and J´egou,\nH.\nWord translation without parallel data.\narXiv preprint\narXiv:1710.04087, 2017b.\nDai, A. M. and Le, Q. V. Semi-supervised sequence learning. In\nAdvances in neural information processing systems, pp. 3079–\n3087, 2015.\nDai, Z., Yang, Z., Yang, Y., Cohen, W. W., Carbonell, J., Le,\nQ. V., and Salakhutdinov, R. Transformer-xl: Attentive lan-\nguage models beyond a ﬁxed-length context. arXiv preprint\narXiv:1901.02860, 2019.\nDavies,\nM.\nThe\n14\nbillion\nword\niweb\ncorpus.\nhttps://corpus.byu.edu/iWeb/, 2018.\nDehghani, M., Gouws, S., Vinyals, O., Uszkoreit, J., and Kaiser,\nŁ. Universal transformers. arXiv preprint arXiv:1807.03819,\n2018.\nDevlin, J., Chang, M.-W., Lee, K., and Toutanova, K. Bert: Pre-\ntraining of deep bidirectional transformers for language under-\nstanding. arXiv preprint arXiv:1810.04805, 2018.\nDinan, E., Roller, S., Shuster, K., Fan, A., Auli, M., and Weston,\nJ. Wizard of wikipedia: Knowledge-powered conversational\nagents. arXiv preprint arXiv:1811.01241, 2018.\nFan, A., Lewis, M., and Dauphin, Y. Hierarchical neural story\ngeneration. arXiv preprint arXiv:1805.04833, 2018.\n\nLanguage Models are Unsupervised Multitask Learners\nFinn, C., Abbeel, P., and Levine, S.\nModel-agnostic meta-\nlearning for fast adaptation of deep networks. arXiv preprint\narXiv:1703.03400, 2017.\nGehrmann, S., Deng, Y., and Rush, A. M. Bottom-up abstractive\nsummarization. arXiv preprint arXiv:1808.10792, 2018.\nGillick, D., Brunk, C., Vinyals, O., and Subramanya, A. Mul-\ntilingual language processing from bytes.\narXiv preprint\narXiv:1512.00103, 2015.\nGong, C., He, D., Tan, X., Qin, T., Wang, L., and Liu, T.-Y. Frage:\nfrequency-agnostic word representation. In Advances in Neural\nInformation Processing Systems, pp. 1341–1352, 2018.\nGrave, E., Joulin, A., and Usunier, N.\nImproving neural\nlanguage models with a continuous cache.\narXiv preprint\narXiv:1612.04426, 2016.\nHe, K., Zhang, X., Ren, S., and Sun, J. Identity mappings in deep\nresidual networks. In European conference on computer vision,\npp. 630–645. Springer, 2016.\nHestness, J., Narang, S., Ardalani, N., Diamos, G., Jun, H., Kian-\ninejad, H., Patwary, M., Ali, M., Yang, Y., and Zhou, Y. Deep\nlearning scaling is predictable, empirically.\narXiv preprint\narXiv:1712.00409, 2017.\nHill, F., Bordes, A., Chopra, S., and Weston, J. The goldilocks\nprinciple: Reading children’s books with explicit memory rep-\nresentations. arXiv preprint arXiv:1511.02301, 2015.\nHill, F., Cho, K., and Korhonen, A. Learning distributed repre-\nsentations of sentences from unlabelled data. arXiv preprint\narXiv:1602.03483, 2016.\nHoang, L., Wiseman, S., and Rush, A. M. Entity tracking im-\nproves cloze-style reading comprehension.\narXiv preprint\narXiv:1810.02891, 2018.\nHoward, J. and Ruder, S. Universal language model ﬁne-tuning for\ntext classiﬁcation. In Proceedings of the 56th Annual Meeting\nof the Association for Computational Linguistics (Volume 1:\nLong Papers), volume 1, pp. 328–339, 2018.\nJelinek, F. and Mercer, R. L. Interpolated estimation of markov\nsource parameters from sparse data. In Proceedings of the\nWorkshop on Pattern Recognition in Practice, Amsterdam, The\nNetherlands: North-Holland, May., 1980.\nJia, R. and Liang, P. Adversarial examples for evaluating read-\ning comprehension systems. arXiv preprint arXiv:1707.07328,\n2017.\nJozefowicz, R., Vinyals, O., Schuster, M., Shazeer, N., and Wu,\nY. Exploring the limits of language modeling. arXiv preprint\narXiv:1602.02410, 2016.\nKaiser, L., Gomez, A. N., Shazeer, N., Vaswani, A., Parmar, N.,\nJones, L., and Uszkoreit, J. One model to learn them all. arXiv\npreprint arXiv:1706.05137, 2017.\nKarpathy, A., Johnson, J., and Fei-Fei, L. Visualizing and under-\nstanding recurrent networks. arXiv preprint arXiv:1506.02078,\n2015.\nKirkpatrick, J., Pascanu, R., Rabinowitz, N., Veness, J., Desjardins,\nG., Rusu, A. A., Milan, K., Quan, J., Ramalho, T., Grabska-\nBarwinska, A., et al. Overcoming catastrophic forgetting in\nneural networks. Proceedings of the national academy of sci-\nences, pp. 201611835, 2017.\nKiros, R., Zhu, Y., Salakhutdinov, R. R., Zemel, R., Urtasun, R.,\nTorralba, A., and Fidler, S. Skip-thought vectors. In Advances\nin neural information processing systems, pp. 3294–3302, 2015.\nKrizhevsky, A., Sutskever, I., and Hinton, G. E. Imagenet classiﬁ-\ncation with deep convolutional neural networks. In Advances in\nneural information processing systems, pp. 1097–1105, 2012.\nKwiatkowski, T., Palomaki, J., Rhinehart, O., Collins, M., Parikh,\nA., Alberti, C., Epstein, D., Polosukhin, I., Kelcey, M., Devlin,\nJ., et al. Natural questions: a benchmark for question answering\nresearch. 2019.\nLake, B. M., Ullman, T. D., Tenenbaum, J. B., and Gershman, S. J.\nBuilding machines that learn and think like people. Behavioral\nand Brain Sciences, 40, 2017.\nLample, G., Conneau, A., Denoyer, L., and Ranzato, M. Unsu-\npervised machine translation using monolingual corpora only.\narXiv preprint arXiv:1711.00043, 2017.\nLevesque, H., Davis, E., and Morgenstern, L.\nThe winograd\nschema challenge. In Thirteenth International Conference on\nthe Principles of Knowledge Representation and Reasoning,\n2012.\nLevy, O. and Goldberg, Y. Neural word embedding as implicit ma-\ntrix factorization. In Advances in neural information processing\nsystems, pp. 2177–2185, 2014.\nLiu, P. J., Saleh, M., Pot, E., Goodrich, B., Sepassi, R., Kaiser, L.,\nand Shazeer, N. Generating wikipedia by summarizing long\nsequences. arXiv preprint arXiv:1801.10198, 2018.\nMcCann, B., Bradbury, J., Xiong, C., and Socher, R. Learned\nin translation: Contextualized word vectors. In Advances in\nNeural Information Processing Systems, pp. 6294–6305, 2017.\nMcCann, B., Keskar, N. S., Xiong, C., and Socher, R. The natural\nlanguage decathlon: Multitask learning as question answering.\narXiv preprint arXiv:1806.08730, 2018.\nMerity, S., Xiong, C., Bradbury, J., and Socher, R. Pointer sentinel\nmixture models. arXiv preprint arXiv:1609.07843, 2016.\nMikolov, T., Sutskever, I., Chen, K., Corrado, G. S., and Dean,\nJ. Distributed representations of words and phrases and their\ncompositionality. In Advances in neural information processing\nsystems, pp. 3111–3119, 2013.\nNallapati, R., Zhou, B., Gulcehre, C., Xiang, B., et al. Abstrac-\ntive text summarization using sequence-to-sequence rnns and\nbeyond. arXiv preprint arXiv:1602.06023, 2016.\nPaperno, D., Kruszewski, G., Lazaridou, A., Pham, Q. N., Bernardi,\nR., Pezzelle, S., Baroni, M., Boleda, G., and Fern´andez, R. The\nlambada dataset: Word prediction requiring a broad discourse\ncontext. arXiv preprint arXiv:1606.06031, 2016.\nPennington, J., Socher, R., and Manning, C. Glove: Global vectors\nfor word representation. In Proceedings of the 2014 conference\non empirical methods in natural language processing (EMNLP),\npp. 1532–1543, 2014.\n\nLanguage Models are Unsupervised Multitask Learners\nPeters, M. E. and Lecocq, D. Content extraction using diverse fea-\nture sets. In Proceedings of the 22nd International Conference\non World Wide Web, pp. 89–90. ACM, 2013.\nPeters, M. E., Neumann, M., Iyyer, M., Gardner, M., Clark, C.,\nLee, K., and Zettlemoyer, L. Deep contextualized word repre-\nsentations. arXiv preprint arXiv:1802.05365, 2018.\nRadford, A., Jozefowicz, R., and Sutskever, I.\nLearning to\ngenerate reviews and discovering sentiment. arXiv preprint\narXiv:1704.01444, 2017.\nRadford, A., Narasimhan, K., Salimans, T., and Sutskever, I.\nImproving language understanding by generative pre-training.\n2018.\nRamachandran, P., Liu, P. J., and Le, Q. V. Unsupervised pre-\ntraining for sequence to sequence learning.\narXiv preprint\narXiv:1611.02683, 2016.\nRecht, B., Roelofs, R., Schmidt, L., and Shankar, V.\nDo\ncifar-10 classiﬁers generalize to cifar-10?\narXiv preprint\narXiv:1806.00451, 2018.\nReddy, S., Chen, D., and Manning, C. D. Coqa: A conversational\nquestion answering challenge. arXiv preprint arXiv:1808.07042,\n2018.\nSchwartz, R., Sap, M., Konstas, I., Zilles, L., Choi, Y., and Smith,\nN. A. Story cloze task: Uw nlp system. In Proceedings of the\n2nd Workshop on Linking Models of Lexical, Sentential and\nDiscourse-level Semantics, pp. 52–55, 2017.\nSee, A., Liu, P. J., and Manning, C. D. Get to the point: Sum-\nmarization with pointer-generator networks. arXiv preprint\narXiv:1704.04368, 2017.\nSennrich, R., Haddow, B., and Birch, A. Neural machine trans-\nlation of rare words with subword units.\narXiv preprint\narXiv:1508.07909, 2015.\nSubramanian, S., Trischler, A., Bengio, Y., and Pal, C. J. Learning\ngeneral purpose distributed sentence representations via large\nscale multi-task learning. arXiv preprint arXiv:1804.00079,\n2018.\nSutskever, I., Vinyals, O., and Le, Q. V. Sequence to sequence\nlearning with neural networks. In Advances in neural informa-\ntion processing systems, pp. 3104–3112, 2014.\nSutskever, I., Jozefowicz, R., Gregor, K., Rezende, D., Lillicrap,\nT., and Vinyals, O. Towards principled unsupervised learning.\narXiv preprint arXiv:1511.06440, 2015.\nTrichelair, P., Emami, A., Cheung, J. C. K., Trischler, A., Sule-\nman, K., and Diaz, F. On the evaluation of common-sense\nreasoning in natural language understanding. arXiv preprint\narXiv:1811.01778, 2018.\nTrinh, T. H. and Le, Q. V. A simple method for commonsense\nreasoning. arXiv preprint arXiv:1806.02847, 2018.\nVaswani, A., Shazeer, N., Parmar, N., Uszkoreit, J., Jones, L.,\nGomez, A. N., Kaiser, Ł., and Polosukhin, I. Attention is\nall you need. In Advances in Neural Information Processing\nSystems, pp. 5998–6008, 2017.\nVinyals, O. and Le, Q. A neural conversational model. arXiv\npreprint arXiv:1506.05869, 2015.\nVinyals, O., Fortunato, M., and Jaitly, N. Pointer networks. In\nAdvances in Neural Information Processing Systems, pp. 2692–\n2700, 2015.\nWang, A., Singh, A., Michael, J., Hill, F., Levy, O., and Bow-\nman, S. R.\nGlue: A multi-task benchmark and analysis\nplatform for natural language understanding. arXiv preprint\narXiv:1804.07461, 2018.\nWeston, J. E. Dialog-based language learning. In Advances in\nNeural Information Processing Systems, pp. 829–837, 2016.\nWieting, J. and Kiela, D.\nNo training required: Exploring\nrandom encoders for sentence classiﬁcation. arXiv preprint\narXiv:1901.10444, 2019.\nWolf, T., Sanh, V., Chaumond, J., and Delangue, C. Transfer-\ntransfo: A transfer learning approach for neural network based\nconversational agents. arXiv preprint arXiv:1901.08149, 2019.\nYogatama, D., d’Autume, C. d. M., Connor, J., Kocisky, T.,\nChrzanowski, M., Kong, L., Lazaridou, A., Ling, W., Yu, L.,\nDyer, C., et al. Learning and evaluating general linguistic intel-\nligence. arXiv preprint arXiv:1901.11373, 2019.\n\nLanguage Models are Unsupervised Multitask Learners\n8. Appendix A: Samples\n8.1. Model capacity\nTo complement the reported perplexity gains of bigger LMs on\nWebText show in Figure 4, Tables 7 through 11 show side-by-side\ncompletions of the smallest WebText LM and GPT-2 on random\nunseen WebText test set articles.\n8.2. Text Memorization\nWe observe some memorizing behavior in GPT-2 on longer strings\nthat are repeated many times in the dataset such as famous quotes\nor speeches. For example, when conditioned on the ﬁrst sentence\nand a half of the Gettysburg Address (which occurs approximately\n40 times throughout WebText), an argmax decode from GPT-2\nrecovers the speech. Even when sampling without truncation, we\nﬁnd that the model copies the speech for awhile before drifting,\nalbeit in a similar style. It typically drifts within 100-200 tokens,\nand displays widening diversity once it drifts.\nTo quantify how often exact memorization shows up in samples,\nwe generated samples from GPT-2 conditioned on WebText test\nset articles and compared the overlap rates of GPT-2’s generations\nto the overlap rates of the ground-truth completions. The results of\nthis analysis are shown below and suggest that GPT-2 repeats text\nfrom the training set less often then the baseline rate of held-out\narticles.\nFigure 5. CDF of percentage 8-gram overlap with WebText train-\ning set, for both WebText test set and samples (conditioned on\nWebText test set, with top-k truncated random sampling with\nk = 40). Most samples have less than 1% overlap, including over\n30% of samples with no overlap, whereas the median for test set is\n2.6% overlap.\n8.3. Diversity\nTable 12 shows multiple completions of the same random WebText\ntest set context, showing the diversity of completions with standard\nsampling settings.\n8.4. Robustness\nTable 13 shows the previously mentioned talking unicorns news\narticle. We ﬁnd the model to be capable of handling out of dis-\ntribution contexts, but the quality of these samples is generally\nlower.\n\nLanguage Models are Unsupervised Multitask Learners\nContext (WebText test)\nOutﬁt 1: Typical\nThis pairing was the ﬁrst outﬁt I thought of when I bought the shoes.\nIt’s like a summer version of this\nJake Grantham outﬁt; in fact, my shoes are close to the colors of his Nike Racers! Instead of a heavy Harris Tweed\njacket and denim shirt, I’m wearing a cotton DB jacket and and a linen shirt. Both fabrics (in these colors) are an\nabsolute must for summer, as they go with both dark and and light pants! As you can see, they pair wonderfully\nwith the dark jeans and shoes. It’s a pseudo menswear/prep outﬁt.\nOverall, this is a very casual outﬁt which is why I paired my sneakers with it.\nI’m not about wearing a\nfull wool suit with sneakers (as GQ shows a lot) but I’m deﬁnitely open to keeping things casual, like this cotton\nDB. Casual fabrics are key to pulling off your sneakers in a dressed down menswear outﬁt. I’d even suggest to\nwear these sneakers with a khaki chino suit or a white linen suit. Just be sure to ditch the tie or wear a tee or polo;\nwearing a tie with sneakers is a bit too much\nSmall model completion\n.\nOutﬁt 2: High End\nThis outﬁt was just in the eye of the storm.\nIt’s a\nhybrid of what ASICS does with streetwear (like J. Crew)\nand what J. Crew does with casual pieces that are a bit\nmore traditional. Again, as I mentioned before, it’s a low\nbudget item and they can be very expensive. These will\ndeﬁnitely be an issue if you’re trying to work as a casual\nsneakermaker, and that’s OK but don’t hesitate if you\nwant to try the lower price versions of their outﬁts!\nThis version features a very low cut-cut neckline\n(which is not a bad thing), slightly longer legs, and a pair\nof long legs to keep things nice. My preferred color of\ncolor these items are blue-gray (which is my ﬁrst colors,\nas they are a light white), or black (which is my favorite\nof my designs). As always, my pick of colors from this\nstore have nothing to do with that other than the de-\nsign. For a high end, I would recommend this to everyone!\nOutﬁt 3: High Light\nThis was a ”high light” look while I had a chance\nto get some color. I think the color is based on\nGPT-2 completion\nfor me!\nOutﬁt 2: Casual Jacket\nI know that some people might be opposed to wearing\nsneakers in a turtleneck, but I wanted to be true to myself,\nso I went with a slim ﬁtting turtleneck in a color more\nsimilar to my favorite color of the day. Overall, it’s a\nclassic turtleneck, with nothing too ﬂashy. I did swap my\nshoes for these ASICS Tiger Killshots and I wish I hadn’t\ndid that. The shoe itself is very comfortable, however,\nI found that wearing them in these sneakers made my\nfeet look a bit larger. I’d wear them with denim, chinos,\nwhatever, but would never recommend wearing them\nalone. There are just too many variables involved.\nOutﬁt 3: Dress\nThis is the ﬁnal outﬁt I wore the ASICS Tiger\nKillshots. This version of the ASICS is a much more\nmodern and streamlined version. The shoe has the most\nmodern and minimal design that I can think of! With a lot\nof black, these sneakers look very cool. They’ll deﬁnitely\nbe very comfortable for summer.\nTo me, wearing them with a suit is the easiest way\nto have a good pair of sneaker shoes. That’s pretty much\nit! If\nTable 7. Random unseen contexts (top), and non-cherry-picked completions from both the smallest (left) and largest (right) models.\nContexts are 768 tokens, with approximately 256 tokens worth of paragraphs shown. Completions are 256 tokens and fully shown. Top-k\nrandom sampling with k = 40 was used for generation.\n\nLanguage Models are Unsupervised Multitask Learners\nContext (WebText test)\nSome of the most glorious historical attractions in Spain date from the period of Muslim rule, including The\nMezquita, built as the Great Mosque of C´ordoba and the Medina Azahara, also in C´ordoba and now in ruins but\nstill visitable as such and built as the Madinat al-Zahra, the Palace of al-Andalus; and the Alhambra in Granada, a\nsplendid, intact palace. There are also two synagogues still standing that were built during the era of Muslim Spain:\nSanta Mar´ıa la Blanca in Toledo and the Synagogue of C´ordoba, in the Old City.\nReconquista and Imperial era [ edit ]\nSmall model completion\nThe era of Muslim Spain is very different from that of\nother parts of the world, with a rather extensive medieval\nhistory. While the Spanish Empire existed during the\nsecond half of the 9th century BC it was quite slowly\ntransformed to a small, secularized empire by 1150 A.D.,\nin a much more prosperous and diverse state. In 1050 CE\nthe emperor of Naples, at least partially in characterised by\na series of wars and invasions from the north-western coast\nto the western coast of the region (e.g., on the Danube\nRiver which extended all the way from Gibraltar into the\nsouth of Spain, but was quickly destroyed on the same\nday by the Roman army), assumed the title of Emperor\nof Naples. He made a large contribution over the last few\ndecades to restore and maintain the empire, the ﬁrst of\nwhich to the west. His successors were Ferdinand, who\ntook power from the Papal Throne in 1148, while the ﬁrst\nimperial successor was King Alfred II of Sweden. The\nempire was again made one of the few independent states\nin the European Continent and although no more than 200\nmillion people lived on its territory during this period the\npeople had much to say to the Emperor. From 1720 on the\nMuslims made up half the population of Spain.\nGPT-2 completion\nThe Reconquista (literally, ”The return to the country”)\nof 1092 and 1095 saw the establishment in the Iberian\npeninsula of a united kingdom of Castile and Aragon in\nwhich a Muslim dynasty would exist. This was the last\ntime in Europe where Muslim rule would be considered\n”native” and the last occasion where Christian-inﬂamed\nand organized conﬂicts would make themselves known in\nEurope.\nThe reign of Muslim Moors in Spain is often re-\nferred to,\nespecially among non-Muslims,\nas the\n”Conquest of Granada” and Spain is sometimes referred\nto as Spain in ”Granada”. The last years of Muslim rule\nwere followed by the Spanish Reconquista where the\nMuslims were driven or compelled out of the Iberian\nPeninsula with the victory of Ferdinand IX. in the Battle\nof Poitiers in March 1189 and the conquest of Jerusalem\nin 1187 by Saladin.\nThe Reconquista period in Spain came to an end\nwhen the Moors retreated north. However the Moorish\nrule would not be fully replaced by the Christian rule until\nthe Reconquista era resumed when the Reconquista lasted\nfrom 1229 to 1492.\nThe last Muslim state in Spain, Granada, was con-\nquered by the\nTable 8. Random unseen contexts (top), and non-cherry-picked completions from both the smallest (left) and largest (right) models.\nContexts are 768 tokens, with approximately 256 tokens worth of paragraphs shown. Completions are 256 tokens and fully shown. Top-k\nrandom sampling with k = 40 was used for generation.\n\nLanguage Models are Unsupervised Multitask Learners\nContext (WebText test)\nPinky\nThe pink ghost’s AI is designed to ”feel” opposite of the red ghost’s behavior. Pinky actually attempts to\nget out in front of Pac-Man. This is accomplished by setting the target 4 tiles ahead of Pac-Man’s current location in\nthe direction that Pac-Man is travelling. One exception to this is when Pac-Man is traveling up. Due to an overﬂow\nbug in the code, the calculation includes a left offset equal to the expected up offset.\nThe use of Pac-Man’s expected position allows the player to exploit the navigation code for Pinky. This was most\noften seen in situations where Pac-Man and Pinky are in close proximity to each other. For example, if they were\nheaded toward one another, once they are less than 4 tiles apart, the target tile is now behind Pinky. If he encounters\nan intersection while in this conﬁguration, he will attempt to double-back (by turning) in order to reach the target tile.\nPlayer’s would often perform what was referred to as a ”head fake” by changing directions rapidly just\nas Pinky was approaching an intersection. By displacing the target tile just as Pinky\nSmall model completion\nwas coming to the left, they would get the ”correct”\ndirection.\nIf the location is actually farther from what Pac-\nMan was at, the player’s will not respond to the ghost’s\nmovement, and the ghost will instead return to the player’s\ncurrent location. This is because Pac-Man does not move\nby turning.\nThe ghost will have to go to another location of\nsome kind to return where he began, and this is achieved\nin many ways. It will often simply leave the location, as\nfor example if Pac-Man did jump off a cliff, he would\nreturn to the spot where he was before vanishing into\nspace. There is a good number of possibilities these ghost\npaths may be possible, but the most obvious ones are if\nthe player wants to avoid getting caught up in the game,\nwith a goal as easy as keeping Pac-Man in the game for\n1/3 as long as he was.\nChasing\nIn Pac-Man’s case, this would be similar to how\nPac-Man can ”catch up” in a ﬁght.\nA ghost could chase through enemies.\nThis is use-\nful for many reasons, both for Pac-Man and his AI.\nUnlike most Ghost games, this ghost simply travels in the\ndirection from\nGPT-2 completion\nwas about to cross the intersection, Pac-Man would be\nable to dodge the ghosts’ projectiles and return to the safe\nlocation of the safe house.\nBuster\nThe yellow ghost’s AI is speciﬁcally designed to\ntry to avoid Pac-Man from leaving the safe house. The\nplayer must press the SPACE bar to reveal the safe\nhouse’s doors, then go into them to ﬁnd the yellow ghost\nhiding in the next room.\nThe player must also note that the ghost will not\nattempt to exit the safe house if it sees Pac-Man already\nthere. In fact, the ghost will not attempt an exit in the ﬁrst\nplace unless they’re already in the safe house.\nThese two behaviors are based on the fact that the\nplayer does not always attempt to reach the safe house\nbefore Pac-Man (or Blinky).\nThe player must also beware of this glitch if a\nghost (or another player) is already in the safe house\nand attempting to get out as soon as Pac-Man or Blinky\nis gone. At this point, the ghost will attempt to exit\nthe safe house in a similar manner as it is in ”Chase” mode.\nPac-Man\nThere are some ghosts that follow Pac-Man. These ghosts\nare\nTable 9. Random unseen contexts (top), and non-cherry-picked completions from both the smallest (left) and largest (right) models.\nContexts are 768 tokens, with approximately 256 tokens worth of paragraphs shown. Completions are 256 tokens and fully shown. Top-k\nrandom sampling with k = 40 was used for generation.\n\nLanguage Models are Unsupervised Multitask Learners\nContext (WebText test)\nCorporal Michael P. Goeldin was an unskilled laborer from Ireland when he enlisted in Company A in November\n1860. Goldein survived the war. Corporal Patrick O’Neal, also from Ireland, ﬁrst enlisted in 1854 and served with\nCompany L, 3d U.S. Artillery, in Oregon. He returned to the East Coast and enlisted in the company in 1860.\nO’Neal served until 1874, when he was named superintendent of the National Cemetery at Willets Point, New York.\nCorporal Benjamin Browne was a shoemaker from Orange County, New York. In August 1862, he enlisted in the\nnewly formed 124th New York Volunteers, and was one of sixty-one men who transferred into Company A that\nOctober. Browne reenlisted in the company in February 1864 while it was camped at Brandy Station. He returned\nto civilian life after completing his enlistment in 1867.\nOn 10 June, Artiﬁcer William Collins was promoted to corporal, probably to ﬁll a combat leadership\nvoid for the crossing of the James River. Collins’s service record does not reﬂect the qualities he demonstrated to\nearn this promotion, but he had obviously overcome some serious problems. Born in Sacketts Harbor, New York,\nCollins enlisted in the company in December 1853 at the age of twenty-two, and reenlisted in December 1858. Just\na month before the war began in April 1861, Collins went ”over the hill” and was not caught until three years later.\nReturned to the company on 22 March 1864, he was tried\nSmall model completion\non military charges and was discharged from the U.S.\nMilitary. He returned to the company, and was assigned\nto Division of Internal Affairs during the Civil War.\nAs an engineer, Collins also had training in engi-\nneering from his native New York. After graduate studies\nin the Union College of Engineering, he was employed\nin the United States Army for two years at the same\ntime. There he worked under Captain Daniel Pfeiffer\nand was promoted to colonel in May 1870, three years\nafter Captain Pfeiffer’s enlistment.\nCollins returned\nhome to England after completing his ﬁrst year in the\nFirst Division to serve the next two years in the Army\nCorps of Engineers, and by his second year, he began to\nexperience his service duty.¡—endoftext—¿MOSCOW,\nJuly 5 (Reuters) - U.K.-based Russian state media and\ngovernment sources and experts say Moscow has sought\nto downplay the ”military buildup” of the Syrian oppo-\nsition while maintaining support for President Donald\nTrump’s plan to build a wall along the U.S.-Mexico\nborder, calling it a ”military coup”.\nU.S.-led Syria war veteran and Syria envoy Michael\nFlynn (R), right, gestures with U.S.-based Syrian political\nanalyst Ahmet Koryat, with\nGPT-2 completion\nin June of that year. Sentenced to be shot for desertion, he\nremained on active duty.\nCaptain James J. Dall was born in Virginia in 1829 during\nthe Revolution and joined his father in the Army in 1836.\nHe served with the 2d Virginia Infantry until 1845, when\nhe went to the Western Reserve Regiment; and, on his\ndischarge, he reenlisted with Company I, 25th New York\nVolunteers (which in 1845 numbered about 4,500 men).\nOne of his brothers, James, served with his company\nin the same capacity in the same brigade. While the\nregiment remained in Connecticut through 1846, Captain\nDall served with Company I at various times before\nhis discharge. A veteran of the Mexican War, he had\nserved two years on the staff of Brigadier General John J.\nSullivan at San Antonio. During the Peninsula Campaign\nof 1863, he commanded a company of twenty-three and\nwas in charge of the battery of ﬁfty-four heavy mortars\nand the gunboat Pinta of the Battery, a gunboat and\nseveral other boats. Captain Dall was on active duty with\nCompany I in the fall of 1865. Two years later, he was\npromoted to second lieutenant.\nCaptain John A. O’\nTable 10. Random unseen contexts (top), and non-cherry-picked completions from both the smallest (left) and largest (right) models.\nContexts are 768 tokens, with approximately 256 tokens worth of paragraphs shown. Completions are 256 tokens and fully shown. Top-k\nrandom sampling with k = 40 was used for generation.\n\nLanguage Models are Unsupervised Multitask Learners\nContext (WebText test)\nXmonad, Xresources, and Fonts\nXresources can be an absolute pain (they were for me). So, this a bit of help:\nFirst of all, the way that fonts are generally handled in Xmonad is through XFT . Xft is the X FreeType\nInterface library and is a library that handles the actual rendering of fonts in Xmonad/urxvt (Xresources speciﬁes\na bunch of resources to things that launch under the X window manager. Settings for individual programs are\nspeciﬁed in Xresources like so: Urxvt.background:background settins here ).\nAnyway, fonts are set using this syntax in Xresources :\nurxvt*font:xft:your font name goes here:size=the size of the font goes here,xft:fall-back font 1 goes here\nSo, thats the most basic part of fonts in urxvt speciﬁed via Xresources.\nFixing /.ssh/ Permissions in Cygwin\nIn Cygwin on Windows I found that I could not set the permissions of my /.ssh/ folder to be 0600, as is\nrequired for ssh to allow you to use keys. The symptom I had was that I no matter what I did, it always modﬁed the\nowner and the group\nSmall model completion\nwas set to /.ssh/ , which was the way things were. This\ncould not be ﬁxed completely. I found I could make that\nmuch more workable with just typing the username with\nyour mouse then trying to set permissions to be ”0600”\nfor the group, which is what my system is now conﬁgured\nto do.\nHowever, as of Cygwin 7 on Linux I have found\nthat doing so is now ”unnecessary” for my system! I also\ninstalled a package called chroot. It does that by allowing\nyou to install/upgrade to your system directory.\nSo, as of Windows 8 and up, installing that can re-\nquire chroot, so I tried and tried and found that it wasn’t\nso bad when installing the package on Linux and then\nuninstalling it. As of my last write-up on Cygwin, the\nscript just does its job (the sudo commands do the rest), if\nyou try it you will have to log the change.\nIf that wasn’t confusing enough, this is what I tried and\njust went back to my /.ssh/ directory and the chroot script.\nI found I was ﬁnally running out of permissions (because\ni forgot to create a symlink to ssh from the ﬁle system and\nGPT-2 completion\n, making these ﬁles read-only. I tried just starting ssh\nand using a program like passwd which modiﬁes the\nowner/group to 0644 permissions, but it didn’t work. It\nwas annoying but not nearly as bad as it could be, just\nannoying to have to ﬁddle with permissions.\nSo I decided to do this as the reverse:\nFor each .ssh folder in your Cygwin directory, set\nthe permission of each ﬁle and directory to 2644. Then\nstart ssh with: ssh-add /.ssh/\nThe ﬁrst time you connect,\nyour permissions of\nthe\n/.ssh folder that you just had in Cygwin will be\nset to 0600. Then every time you connect again, your\npermissions will change to 0644.\nMakeﬁle for Xresources as used in /.xsessionrc\nThese are made in my .xsessionrc :\n.xproﬁle: .xsession-errors: color=””\nFor example:\n.xproﬁle: color=blue\nAs the colors are stored in a separate ﬁle, the color in your\n/.xsession-errors will look like cyan instead of blue.\nThis is mostly because, as of now, only the color\nis assigned to your\nTable 11. Random unseen contexts (top), and non-cherry-picked completions from both the smallest (left) and largest (right) models.\nContexts are 768 tokens, with approximately 256 tokens worth of paragraphs shown. Completions are 256 tokens and fully shown. Top-k\nrandom sampling with k = 40 was used for generation.\n\nLanguage Models are Unsupervised Multitask Learners\nContext (WebText test)\nAaron loves mint chocolate cake, but he requires that it be paired with mini chocolate chips, so I threw some of\nthose in between the layers. I also had a few Peppermint Jo Jos on hand so I crushed them up and threw some of\nthose in along with some crushed meringue cookies because, why not? It’s a total smorgasbord of minty chocolate\nchippy cookie crunchy goodness. I didn’t measure how much of each topping I used, but after I tasted the ﬁnished\nproduct, I wish I had added more. You can add anything you want- crushed candy canes, peppermint bark, etc. And\ndon’t be afraid to use a heavy hand. Texture = good.\nIf you don’t have 7-inch cake pans, you can get 3 shorter 8-inch layers out of this\nGPT-2 Completion 1\nrecipe. I made the longer version,\nthen used the pan for a layer of\nbuttercream frosting and stuck 1/3\nof it into the fridge for a frosting\ntopper. Just make sure you have all\nof this frosting prepared earlier than\nyou make this cake and you won’t\nhave to wait on it as your egg whites\nsit on the bottom of your mixing\nbowl.\nYou want the egg whites whipped\nto stiff peaks? Then put your cake\ndish over a pot of simmering water\nand heat it up until it’s just barely\nwarm. It doesn’t take more than 3-5\nminutes; just enough to get it started\nGPT-2 Completion 2\nrecipe. If you do, and you know\nhow to make that cake, then your\nlife will never be the same.\nI have two very simple rules\nwhen it comes to cookie crumbs\nor chocolate chips. 1) When I buy\nraw sugar, I do not use it until it’s\ncooled down and has at least half a\nteaspoon of sugar on it. That’s my\nrule. And 2) If I have a recipe that\ncalls for something and I want to\nuse it in a recipe, I do not substitute\nit with something else on-line. Like\nthis.\nI\nfound\nthis\nreally\ncute\ncard\nwith cute little kittens on\nGPT-2 Completion 3\nrecipe, which will likely yield about\nthe same amount of cake.\nChocolate\nButtermilk\nCake\nPrint Chunks of chocolate cake,\ncoated in buttermilk and topped\nwith crunchy chopped chocolate,\ndrizzled\nwith\nchocolate\nsauce.\nSimple but a nice change from\nchocolate cake! Author: Momma\nMade Ingredients For the chocolate\nbuttermilk cake 1 1/2 cup unsalted\nbutter , room temperature\n1 1/2 cup granulated sugar\n3 large eggs plus 1 egg yolk For\nthe chocolate glaze 1/3 cup cocoa\npowder\n1 3/4 cups powdered sugar\n6 ounces semis\nGPT-2 Completion 4\nrecipe.\nMeringue Cookies\n1 3/4 cup butter, softened\n1 cup sugar\n1 egg yolk\n3 T. heavy cream\n3 1/2-4 cups ﬂour\npinch of salt\nPeppermint-Jojo Topping\n1 1/4 cups powdered sugar\n1 cup chopped pecans\n1/2 cup ﬁnely chopped mint leaves\n1/2 cup chopped fresh mint (about\n1/2 sheet)\n1 tsp. peppermint extract\n1/4 tsp. sea salt\npinch of sea\nGPT-2 Completion 5\nrecipe.\nMy 2 failures at Christmas morning\nwere not due to poor frosting skills.\nThat was another success story.\nThe cake was beautifully moist,\nand well, just a beautiful cake. It’s\ngot very little air in it or moisture,\nbut all the chocolate and mint was\nperfectly blended together.\nOne thing that made me cringe a\nbit was that all those ingredients-\negg yolks, sugar, melted butter,\nchocolate chips, melted chocolate,\nand mint- had to be mixed right\ninto the cake batter,\nwhich is\nsuper time-consuming.\nA friend\nsuggested that if I put the batter in a\nseparate bowl, I\nGPT-2 Completion 6\nrecipe, which will give you about\n6-9 servings of delicious chocolate\ncake.\nTo prevent your layers\nfrom sticking together, I used my\n8-inch round tart pan, lined it with\nparchment paper, then cut up 4\n7-inch round cakes from it and\nsandwiched the layers between\nthem (this also worked well for\nthe 1-layer chocolate cake as well).\nWhen you’re ready to bake this\ncake, remove the parchment paper,\npreheat oven to 350 degrees and\nbake for about 30 – 40 minutes,\nuntil the top is golden and a tester\ncomes out clean.\nChocolate\nButtermilk\nLayer\nCake with Peppermint Jo\nTable 12. Non-cherry-picked completions from GPT-2 generated from the same context (from WebText test). Context is 384 tokens\n(shown truncated), and generations are 128 tokens. Top-k random sampling with k = 40 was used for generation.\n\nLanguage Models are Unsupervised Multitask Learners\nContext (human-written): In a shocking ﬁnding, scientist discovered a herd of unicorns living\nin a remote, previously unexplored valley, in the Andes Mountains. Even more surprising to the\nresearchers was the fact that the unicorns spoke perfect English.\nGPT-2: The scientist named the population, after their distinctive horn, Ovid’s Unicorn. These\nfour-horned, silver-white unicorns were previously unknown to science.\nNow, after almost two centuries, the mystery of what sparked this odd phenomenon is ﬁ-\nnally solved.\nDr.\nJorge P´erez, an evolutionary biologist from the University of La Paz, and several com-\npanions, were exploring the Andes Mountains when they found a small valley, with no other animals\nor humans. P´erez noticed that the valley had what appeared to be a natural fountain, surrounded by\ntwo peaks of rock and silver snow.\nP´erez and the others then ventured further into the valley.\n“By the time we reached the top\nof one peak, the water looked blue, with some crystals on top,” said P´erez.\nP´erez and his friends were astonished to see the unicorn herd.\nThese creatures could be\nseen from the air without having to move too much to see them – they were so close they could touch\ntheir horns.\nWhile examining these bizarre creatures the scientists discovered that the creatures also\nspoke some fairly regular English. P´erez stated, “We can see, for example, that they have a common\n‘language,’ something like a dialect or dialectic.”\nDr.\nP´erez believes that the unicorns may have originated in Argentina, where the animals\nwere believed to be descendants of a lost race of people who lived there before the arrival of humans\nin those parts of South America.\nWhile their origins are still unclear, some believe that perhaps the creatures were created\nwhen a human and a unicorn met each other in a time before human civilization. According to P´erez,\n“In South America, such incidents seem to be quite common.”\nHowever, P´erez also pointed out that it is likely that the only way of knowing for sure if\nunicorns are indeed the descendants of a lost alien race is through DNA. “But they seem to be able to\ncommunicate in English quite well, which I believe is a sign of evolution, or at least a change in\nsocial organization,” said the scientist.\nTable 13. Conditional generation on an out-of-distribution context by GPT-2. Cherry pick of 10 samples generated with k = 40.\n\nLanguage Models are Unsupervised Multitask Learners\nArticle: Prehistoric man sketched an incredible array of prehistoric beasts on the rough limestone walls of a cave in\nmodern day France 36,000 years ago.\nNow, with the help of cutting-edge technology, those works of art in the Chauvet-Pont-d’Arc Cave have been\nreproduced to create the biggest replica cave in the world.\nThe manmade cavern named the Caverne du Pont-d’Arc has been built a few miles from the original site in\nVallon-Pont-D’arc in Southern France and contains 1,000 painstakingly-reproduced drawings as well as around 450\nbones and other features...\nCavemen and women sketched an incredible array of prehistoric beasts on the rough limestone walls of a cave\n36,000 years ago and now a replica has been created (pictured)\n...\nGPT-2: The original site in Vallon-Pont-D’arc in South-\nern France is a Unesco World Heritage site and is the\noldest known and the best preserved cave decorated by\nman. The replica cave was built a few miles from the orig-\ninal site in Vallon-Pont-D’Arc in Southern France. The\ncave contains images of 14 different species of animals\nincluding woolly rhinoceros, mammoths, and big cats.\nReference: Cave mimics famous Caverne du Pont-d’Arc\nin France, the oldest cave decorated by man and the\nbest preserved.\nThe replica contains all 1,000 paint-\nings which include 425 such as a woolly rhinoceros and\nmammoths. Minute details were copied using 3D mod-\nelling and anamorphic techniques, often used to shoot\nwidescreen images. The modern cave also includes replica\npaw prints of bears, bones and details preserved in the\noriginal cave.\nArticle: ‘If you can ﬁnd my submarine, it’s yours,’ Russian oil billionaire Roman Abramovich once said.\nAnd it seems the oligarch and Chelsea FC owner, whose submarine is just one of the extras that came with his\n£300million superyacht Eclipse (perfect for getting to shore undetected), is not the only wealthy businessman\nsplashing out on underwater exploration.\nDubbed Earth’s real ‘ﬁnal frontier’, the oceans are still so little-explored that billionaires are queuing up to buy\nvessels that give them a glimpse of the dramatic seascapes and incredible wildlife of the world’s oceans.\nSo if you have a spare few million in the bank and want some holiday snaps and Instagram posts that will really\ntrump everyone else, you may want to snap up one of these...\nWhale of a time: The OrcaSub takes you 2000 feet beneath the surface in two pressurised Perspex viewing domes\nfor optimum exploration\n...\nGPT-2: The U-Boat Worx submarine can dive to 984 feet\n(300 metres) and is capable of diving to the bottom of\nthe ocean. It can be used for diving, snorkelling and even\nsnorkel diving. The submarine is also equipped with a\nsnorkelling platform.\nReference: Feel like a James Bond villain with the most\nadvanced submerged ocean vehicles. Some convert from\na yacht into a submarine, and others detached off for a\nspeedy exploration. Oliver’s Travels offers a Mile Low\npackage on its submarine, complete with chef and butler\nfor £175,000 a night.\nArticle: Amina Ali Qassim is sitting with her youngest grandchild on her lap, wiping away tears with her headscarf.\nOnly a few months old, this is the baby girl whose ears she desperately tried to cover the night the aerial bombardment\nstarted. She lay awake, she says, in a village mosque on the Yemeni island of Birim, counting explosions as the\nbaby cried.\nIt could have been worse though. They could have still been in their house when the ﬁrst missile landed.\n”Our neighbor shouted to my husband ’you have to leave, they’re coming.’ And we just ran. As soon as we left the\nhouse, the ﬁrst missile fell right by it and then a second on it. It burned everything to the ground,” Qassim tells us\n...\nGPT-2: Yemen is in the middle of a civil war. Saudi\nArabia is leading the coalition bombing campaign. It’s\nbeen bombing Yemen for more than two months now.\nReference: Amina Ali Qassim’s family sought shelter in\na mosque before ﬂeeing Yemen. Thousands like them are\nboarding boats to sail to Djibouti. Saudi Arabia has been\npounding Yemen in a bid to defeat Houthi rebels.\nTable 14. Summaries generated by GPT-2 and reference summaries on the test set of the CNN and Daily Mail dataset.\n\nLanguage Models are Unsupervised Multitask Learners\nEnglish reference\nGPT-2 French translation\nThis re-release, titled The Next Day Extra, was presented\nin the form of three disks: the original album, unpublished\nstudio sessions and remixes, plus a DVD containing the\nfour clips that have already been unveiled.\nLes nouvelles re-releases, tout en premier disc, n´ecessaire\nde l’album, un studio session et remixes, plus une DVD\nde l’´ecran de quelques clips qui ont ´et´e d´ej`a ´echapp´es.\nFrench reference\nGPT-2 English translation\nUn homme expliquait que le fonctionnement de la hernia\nfonctionnelle qu’il avait reconnaˆıt avant de faire, le fonc-\ntionnement de la hernia fonctionnelle que j’ai r´eussi, j’ai\nr´eussi.\nThis release, originally released on three discs: the origi-\nnal album, sessions studio in´edites and remixes, a DVD\ncontaining the four clips that were released on the previous\nday.\nEnglish reference\nGPT-2 French translation\nOne man explained that the free hernia surgery he’d re-\nceived will allow him to work again.\nUn homme expliquait que le fonctionnement de la hernia\nfonctionnelle qu’il avait reconnaˆıt avant de faire, le fonc-\ntionnement de la hernia fonctionnelle que j’ai r´eussi, j’ai\nr´eussi.\nFrench reference\nGPT-2 English translation\nUn homme a expliqu´e que l’op´eration gratuite qu’il avait\nsubie pour soigner une hernie lui permettrait de travailler\n`a nouveau.\nA man told me that the operation gratuity he had been\npromised would not allow him to travel.\nEnglish reference\nGPT-2 French translation\nSpeaking to an open government conference in London\nvia video link, Mr Kerry said: ”There is no question that\nthe President and I and others in government have actually\nlearned of some things that had been happening on an\nautomatic pilot because the ability has been there, going\nback to World War Two and to the very difﬁcult years of\nthe Cold War, and then, of course, 9/11.”\nLe premier conference de l’´edition internationale en ligne\nsur la vide, le ministre de l’´Edition internationale a ´et´e\nvide, lui dit : ”Il n’y a qu’une question que le pr´esident\net moi et tous les autres personnes d’une administration\nqui a ´et´e ´evident par la fois, par la part de la guerre, par la\npart de la guer\nFrench reference\nGPT-2 English translation\nS’exprimant lors d’une conf´erence intergouvernementale\n`a Londres par liaison vid´eo, M. Kerry a d´eclar´e: ”Il est\nind´eniable que le Pr´esident, moi-mˆeme et d’autres mem-\nbres du gouvernement avons pris connaissance de cer-\ntaines choses en mode pilote automatique parce que nous\nen avions la possibilit´e, d`es la Seconde guerre mondiale et\njusqu’aux ann´ees difﬁciles de la Guerre froide, puis bien\nsˆur le 11 septembre.”\nIn a conf´erence between the United States and London,\nSecretary of State John Kerry said: ”It is indeniable that\nthe President, myself and others of the government have\nbeen aware of certain certain choices that have been made\nin the past in order to be able to do certain things in a more\nautomated way.”\nTable 15. English to French and French to English translations generated by GPT-2.\n\nLanguage Models are Unsupervised Multitask Learners\nContext (passage and previous question/answer pairs)\nThe 2008 Summer Olympics torch relay was run from March 24 until August 8, 2008, prior to the 2008 Summer\nOlympics, with the theme of “one world, one dream”. Plans for the relay were announced on April 26, 2007, in\nBeijing, China. The relay, also called by the organizers as the “Journey of Harmony”, lasted 129 days and carried\nthe torch 137,000 km (85,000 mi) – the longest distance of any Olympic torch relay since the tradition was started\nahead of the 1936 Summer Olympics.\nAfter being lit at the birthplace of the Olympic Games in Olympia, Greece on March 24, the torch trav-\neled to the Panathinaiko Stadium in Athens, and then to Beijing, arriving on March 31. From Beijing, the torch was\nfollowing a route passing through six continents. The torch has visited cities along the Silk Road, symbolizing\nancient links between China and the rest of the world. The relay also included an ascent with the ﬂame to the top of\nMount Everest on the border of Nepal and Tibet, China from the Chinese side, which was closed specially for the\nevent.\nQ: What was the theme\nA: “one world, one dream”.\nQ: What was the length of the race?\nA: 137,000 km\nQ: Was it larger than previous ones?\nA: No\nQ: Where did the race begin?\nA: Olympia, Greece\nQ: Is there anything notable about that place?\nA: birthplace of Olympic Games\nQ: Where did they go after?\nA: Athens\nQ: How many days was the race?\nA: seven\nQ: Did they visit any notable landmarks?\nA: Panathinaiko Stadium\nQ: And did they climb any mountains?\nA:\nModel answer: Everest\nTurker answers: unknown, yes, Yes, yes\nTable 16. Selected CoQA completion.\n\nLanguage Models are Unsupervised Multitask Learners\nContext (passage and previous question/answer pairs)\nTom goes everywhere with Catherine Green, a 54-year-old secretary. He moves around her ofﬁce at work and goes\nshopping with her. ”Most people don’t seem to mind Tom,” says Catherine, who thinks he is wonderful. ”He’s my\nfourth child,” she says. She may think of him and treat him that way as her son. He moves around buying his food,\npaying his health bills and his taxes, but in fact Tom is a dog.\nCatherine and Tom live in Sweden, a country where everyone is expected to lead an orderly life accord-\ning to rules laid down by the government, which also provides a high level of care for its people. This level of care\ncosts money.\nPeople in Sweden pay taxes on everything, so aren’t surprised to ﬁnd that owning a dog means more\ntaxes. Some people are paying as much as 500 Swedish kronor in taxes a year for the right to keep their dog, which\nis spent by the government on dog hospitals and sometimes medical treatment for a dog that falls ill. However, most\nsuch treatment is expensive, so owners often decide to offer health and even life\nfor their dog.\nIn Sweden dog owners must pay for any damage their dog does.\nA Swedish Kennel Club ofﬁcial ex-\nplains what this means: if your dog runs out on the road and gets hit by a passing car, you, as the owner, have to pay\nfor any damage done to the car, even if your dog has been killed in the accident.\nQ: How old is Catherine?\nA: 54\nQ: where does she live?\nA:\nModel answer: Stockholm\nTurker answers: Sweden, Sweden, in Sweden, Sweden\nTable 17. Selected CoQA completion.\n",
      "fetch_method": "direct-pdf"
    },
    {
      "arxiv_id": "https://r.jina.ai/https://openai.com/blog/better-language-models/",
      "full_text": "Title: Better language models and their implications\n\nURL Source: https://openai.com/blog/better-language-models/\n\nMarkdown Content:\nBetter language models and their implications | OpenAI\n\nListen to article\n\nWe’ve trained a large-scale unsupervised language model which generates coherent paragraphs of text, achieves state-of-the-art performance on many language modeling benchmarks, and performs rudimentary reading comprehension, machine translation, question answering, and summarization—all without task-specific training.\n\nOur model, called GPT‑2 (a successor to[GPT⁠](https://openai.com/index/language-unsupervised/)), was trained simply to predict the next word in 40GB of Internet text. Due to our concerns about malicious applications of the technology, we are not releasing the trained model. As an experiment in responsible disclosure, we are instead releasing a much[smaller model⁠(opens in a new window)](https://github.com/openai/gpt-2)for researchers to experiment with, as well as a[technical paper⁠(opens in a new window)](https://cdn.openai.com/better-language-models/language_models_are_unsupervised_multitask_learners.pdf).\n\nGPT‑2 is a large[transformer⁠(opens in a new window)](https://arxiv.org/abs/1706.03762)-based language model with 1.5 billion parameters, trained on a dataset[A](https://openai.com/blog/better-language-models/#citation-bottom-A) of 8 million web pages. GPT‑2 is trained with a simple objective: predict the next word, given all of the previous words within some text. The diversity of the dataset causes this simple goal to contain naturally occurring demonstrations of many tasks across diverse domains. GPT‑2 is a direct scale-up of GPT, with more than 10X the parameters and trained on more than 10X the amount of data.\n\nGPT‑2 displays a broad set of capabilities, including the ability to generate conditional synthetic text samples of unprecedented quality, where we prime the model with an input and have it generate a lengthy continuation. In addition, GPT‑2 outperforms other language models trained on specific domains (like Wikipedia, news, or books) without needing to use these domain-specific training datasets. On language tasks like question answering, reading comprehension, summarization, and translation, GPT‑2 begins to learn these tasks from the raw text, using no task-specific training data. While scores on these downstream tasks are far from state-of-the-art, they suggest that the tasks can benefit from unsupervised techniques, given sufficient (unlabeled) data and compute.\n\nSamples\n-------\n\nGPT‑2 generates synthetic text samples in response to the model being primed with an arbitrary input. The model is chameleon-like—it adapts to the style and content of the conditioning text. This allows the user to generate realistic and coherent continuations about a topic of their choosing, as seen by the following select samples.[B](https://openai.com/blog/better-language-models/#citation-bottom-B)\n\n#### System Prompt (human-written)\n\n_In a shocking finding, scientist discovered a herd of unicorns living in a remote, previously unexplored valley, in the Andes Mountains. Even more surprising to the researchers was the fact that the unicorns spoke perfect English._\n\n#### Model Completion (machine-written, 10 tries)\n\nThe scientist named the population, after their distinctive horn, Ovid’s Unicorn. These four-horned, silver-white unicorns were previously unknown to science.\nNow, after almost two centuries, the mystery of what sparked this odd phenomenon is finally solved.\n\nDr. Jorge Pérez, an evolutionary biologist from the University of La Paz, and several companions, were exploring the Andes Mountains when they found a small valley, with no other animals or humans. Pérez noticed that the valley had what appeared to be a natural fountain, surrounded by two peaks of rock and silver snow.\n\nPérez and the others then ventured further into the valley. “By the time we reached the top of one peak, the water looked blue, with some crystals on top,” said Pérez.\n\nPérez and his friends were astonished to see the unicorn herd. These creatures could be seen from the air without having to move too much to see them – they were so close they could touch their horns.\n\nWhile examining these bizarre creatures the scientists discovered that the creatures also spoke some fairly regular English. Pérez stated, “We can see, for example, that they have a common ‘language,’ something like a dialect or dialectic.”\n\nDr. Pérez believes that the unicorns may have originated in Argentina, where the animals were believed to be descendants of a lost race of people who lived there before the arrival of humans in those parts of South America.\n\nWhile their origins are still unclear, some believe that perhaps the creatures were created when a human and a unicorn met each other in a time before human civilization. According to Pérez, “In South America, such incidents seem to be quite common.”\n\nHowever, Pérez also pointed out that it is likely that the only way of knowing for sure if unicorns are indeed the descendants of a lost alien race is through DNA. “But they seem to be able to communicate in English quite well, which I believe is a sign of evolution, or at least a change in social organization,” said the scientist.\n\nAs the above samples show, our model is capable of generating samples from a variety of prompts that feel close to human quality and show coherence over a page or more of text. Nevertheless, we have observed various failure modes, such as repetitive text, world modeling failures (e.g., the model sometimes writes about _fires happening under water_), and unnatural topic switching. Exploring these types of weaknesses of language models is an[active⁠(opens in a new window)](https://arxiv.org/abs/1601.04811)[area⁠(opens in a new window)](https://arxiv.org/abs/1510.03055)of[research⁠(opens in a new window)](https://arxiv.org/abs/1701.03185)in the natural language processing community.\n\nOverall, we find that it takes a few tries to get a good sample, with the number of tries depending on how familiar the model is with the context. When prompted with topics that are highly represented in the data (Brexit, Miley Cyrus, Lord of the Rings, and so on), it seems to be capable of generating reasonable samples about 50% of the time. The opposite is also true: on highly technical or esoteric types of content, the model can perform poorly. Fine-tuning offers the potential for even more detailed control over generated samples—for example, we can fine-tune GPT‑2 on the Amazon Reviews dataset and use this to let us write reviews conditioned on things like star rating and category.\n\nThese samples have substantial policy implications: large language models are becoming increasingly easy to steer towards scalable, customized, coherent text generation, which in turn could be used in a number of beneficial as well as malicious ways. We’ll discuss these implications below in more detail, and outline a publication experiment we are taking in light of such considerations.\n\nZero-shot\n---------\n\nGPT‑2 achieves state-of-the-art scores on a variety of domain-specific language modeling tasks. Our model is not trained on any of the data specific to any of these tasks and is only evaluated on them as a final test; this is known as the “zero-shot” setting. GPT‑2 outperforms models trained on domain-specific datasets (e.g. Wikipedia, news, books) when evaluated on those same datasets. The following table shows all our state-of-the-art zero-shot results.\n\n_(+) means a higher score is better for this domain. (–) means a lower score is better._\n\n**Dataset****Metric****Our result****Previous record****Human**\nWinograd Schema Challenge accuracy (+)70.70%63.7%[[secondary]]92%+[[/secondary]]\nLAMBADA accuracy (+)63.24%59.23%[[secondary]]95%+[[/secondary]]\nLAMBADA perplexity (–)8.6 99[[secondary]]~1–2[[/secondary]]\nChildren’s Book Test Common Nouns (validation accuracy)accuracy (+)93.30%85.7%[[secondary]]96%[[/secondary]]\nChildren’s Book Test Named Entities (validation accuracy)accuracy (+)89.05%82.3%[[secondary]]92%[[/secondary]]\nPenn Tree Bank perplexity (–)35.76 46.54[[secondary]]unknown[[/secondary]]\nWikiText-2 perplexity (–)18.34 39.14[[secondary]]unknown[[/secondary]]\nenwik8 bits per character (–)0.93 0.99[[secondary]]unknown[[/secondary]]\ntext8 bits per character (–)0.98 1.08[[secondary]]unknown[[/secondary]]\nWikiText-103 perplexity (–)17.48 18.3[[secondary]]unknown[[/secondary]]\n\nGPT‑2 achieves state-of-the-art on Winograd Schema, LAMBADA, and other language modeling tasks.\n\nOn other language tasks like question answering, reading comprehension, summarization, and translation, we are able to get surprising results without any fine-tuning of our models, simply by prompting the trained model in the right way (see below for examples of how we do this), though we do still fall short of state-of-the-art for specialized systems.\n\n#### Task\n\n**Reading Comprehension:** answer questions about given passages\n\n#### Dataset\n\nCOQA\n\n#### Example\n\n_The 2008 Summer Olympics torch relay was run from March 24 until August 8, 2008, prior to the 2008 Summer Olympics, with the theme of “one world, one dream”. Plans for the relay were announced on April 26, 2007, in Beijing, China. The relay, also called by the organizers as the “Journey of Harmony”, lasted 129 days and carried the torch 137,000 km (85,000 mi) – the longest distance of any Olympic torch relay since the tradition was started ahead of the 1936 Summer Olympics.\nAfter being lit at the birthplace of the Olympic Games in Olympia, Greece on March 24, the torch traveled to the Panathinaiko Stadium in Athens, and then to Beijing, arriving on March 31. From Beijing, the torch was following a route passing through six continents. The torch has visited cities along the Silk Road, symbolizing ancient links between China and the rest of the world. The relay also included an ascent with the flame to the top of Mount Everest on the border of Nepal and Tibet, China from the Chinese side, which was closed specially for the event.\n\nQ: What was the theme?\n\nA: “one world, one dream”.\n\nQ: What was the length of the race?\n\nA: 137,000 km Q: Was it larger than previous ones?\n\nA: No Q: Where did the race begin?\n\nA: Olympia, Greece Q: Is there anything notable about that place?\n\nA: birthplace of Olympic Games Q: Where did they go after?\n\nA: Athens Q: How many days was the race?\n\nA: seven Q: Did they visit any notable landmarks?\n\nA: Panathinaiko Stadium Q: And did they climb any mountains?\n\nA:_\n\n**Target answers:** unknown or yes\n\n**Model answer:** Everest\n\n#### Performance\n\n![Image 1](https://cdn.openai.com/better-language-models/reading-comprehension-1.svg)\n\nWe hypothesize that since these tasks are a subset of general language modeling, we can expect performance to increase further with more compute and data. Others have published[similar hypotheses⁠(opens in a new window)](https://arxiv.org/abs/1901.11373). We also expect fine-tuning to help performance on downstream tasks, though we have yet to do thorough experiments.\n\nPolicy implications\n-------------------\n\nLarge, general language models could have significant societal impacts, and also have many near-term applications. We can anticipate how systems like GPT‑2 could be used to create:\n\n*   AI writing assistants\n*   More capable dialogue agents\n*   Unsupervised translation between languages\n*   Better speech recognition systems\n\nWe can also imagine the application of these models for[malicious purposes⁠](https://openai.com/index/preparing-for-malicious-uses-of-ai/), including the following (or other applications we can’t yet anticipate):\n\n*   Generate misleading news articles\n*   Impersonate others online\n*   Automate the production of abusive or faked content to post on social media\n*   Automate the production of spam/phishing content\n\nThese findings, combined with earlier results on synthetic imagery, audio, and video, imply that technologies are reducing the cost of generating fake content and waging disinformation campaigns. The public at large will need to become more skeptical of text they find online, just as the “[deep fakes⁠(opens in a new window)](https://papers.ssrn.com/sol3/papers.cfm?abstract_id=3213954)” phenomenon calls for more skepticism about images.[C](https://openai.com/blog/better-language-models/#citation-bottom-C)\n\nToday, malicious actors—some of which are political in nature—have already begun to target the shared online commons, using [things like⁠(opens in a new window)](https://www.washingtonpost.com/amphtml/opinions/global-opinions/the-new-censors-wont-delete-your-words--theyll-drown-them-out/2019/02/08/c8a926a2-2b27-11e9-984d-9b8fba003e81_story.html)“robotic tools, fake accounts and dedicated teams to troll individuals with hateful commentary or smears that make them afraid to speak, or difficult to be heard or believed.” We should consider how research into the generation of synthetic images, videos, audio, and text may further combine to unlock new as-yet-unanticipated capabilities for these actors, and should seek to create better technical and non-technical countermeasures. Furthermore, the underlying technical innovations inherent to these systems are core to fundamental artificial intelligence research, so it is not possible to control research in these domains without slowing down the progress of AI as a whole.\n\nRelease strategy\n----------------\n\nDue to concerns about large language models being used to generate deceptive, biased, or abusive language at scale, we are only releasing a[much smaller version of GPT‑2 along with sampling code⁠(opens in a new window)](https://github.com/openai/gpt-2/). We are not releasing the dataset, training code, or GPT‑2 model weights. Nearly a year ago we wrote in the[OpenAI Charter⁠](https://openai.com/charter/): “we expect that safety and security concerns will reduce our traditional publishing in the future, while increasing the importance of sharing safety, policy, and standards research,” and we see this current work as potentially representing the early beginnings of such concerns, which we expect may grow over time. This decision, as well as our discussion of it, is an experiment: while we are not sure that it is the right decision today, we believe that the AI community will eventually need to tackle the issue of publication norms in a thoughtful way in certain research areas. Other disciplines such as biotechnology and cybersecurity have long had active debates about responsible publication in cases with clear misuse potential, and we hope that our experiment will serve as a case study for more nuanced discussions of model and code release decisions in the AI community.\n\nWe are aware that some researchers have the technical capacity to reproduce and open source our results. We believe our release strategy limits the initial set of organizations who may choose to do this, and gives the AI community more time to have a discussion about the implications of such systems.\n\nWe also think governments should consider expanding or commencing initiatives to more systematically monitor the societal impact and diffusion of AI technologies, and to measure the progression in the capabilities of such systems. If pursued, these efforts could yield a better evidence base for decisions by AI labs and governments regarding publication decisions and AI policy more broadly.\n\nWe will further publicly discuss this strategy in six months. If you’d like to discuss large language models and their implications, please email us at:[languagequestions@openai.com⁠](mailto:languagequestions@openai.com). And if you’re excited about working on cutting-edge language models (and thinking through their policy implications),[we’re hiring⁠](https://openai.com/careers/).\n\n_GPT‑2 Interim Update, May 2019_\n\nWe’re implementing two mechanisms to responsibly publish GPT‑2 and hopefully future releases: staged release and partnership-based sharing. We’re now[releasing⁠(opens in a new window)](https://github.com/openai/gpt-2)a larger 345M version of GPT‑2 as a next step in staged release, and are sharing the 762M and 1.5B versions with partners in the AI and security communities who are working to improve societal preparedness for large language models.\n\nStaged release\n--------------\n\nStaged release involves the gradual release of a family of models over time. The purpose of our staged release of GPT‑2 is to give people time to assess the properties of these models, discuss their societal implications, and evaluate the impacts of release after each stage.\n\nAs the next step in our staged release strategy, we are releasing the 345M parameter version of GPT‑2. This model features improved performance relative to the 117M version, though falls short of the 1.5B version with respect to the ease of generating coherent text. We have been excited to see so many positive uses of GPT‑2-117M, and hope that 345M will yield still more benefits.\n\nWhile the misuse risk of 345M is higher than that of 117M, we believe it is substantially lower than that of 1.5B, and we believe that training systems of similar capability to GPT‑2-345M is well within the reach of many actors already; this evolving replication landscape has informed our decision-making about what is appropriate to release.\n\nIn making our 345M release decision, some of the factors we considered include: the ease of use (by various users) of different model sizes for generating coherent text, the role of humans in the text generation process, the likelihood and timing of future replication and publication by others, evidence of use in the wild and expert-informed inferences about unobservable uses, proofs of concept such as the review generator mentioned in the original blog post, the strength of demand for the models for beneficial purposes, and the input of stakeholders and experts. We remain uncertain about some of these variables and continue to welcome input on how to make appropriate language model publication decisions.\n\nWe hope that ongoing research on bias, detection, and misuse will give us the confidence to publish larger models in a timely manner, and at the six month mark we will share a fuller analysis of language models’ societal implications and our heuristics for release decisions.\n\nOutput dataset\n--------------\n\nWe’re[releasing⁠(opens in a new window)](https://github.com/openai/gpt-2-output-dataset)a dataset of GPT‑2 outputs from all 4 model sizes, with and without top-k truncation, as well as a subset of the WebText corpus used to train GPT‑2. The output dataset features approximately 250,000 samples per model/hyperparameter pair, which we expect is sufficient to help a wider range of researchers perform quantitative and qualitative analysis on the three topics above. Alongside these datasets, we are including a baseline analysis of some detection-related properties of the models, which we hope others will be able to quickly build on.\n\nTalk to us\n----------\n\nWe are interested in collaborating with researchers working on language model output detection, bias, and publication norms, and with organizations potentially affected by large language models: please reach out via our[Google Form⁠(opens in a new window)](https://forms.gle/A7WBSbTY2EkKdroPA). Additionally, OpenAI’s language, safety, and policy teams will be at[ICLR⁠(opens in a new window)](https://iclr.cc/)next week, including at the Reproducibility workshop and the OpenAI booth. In particular, we will be discussing this release strategy at the[AI for Social Good⁠(opens in a new window)](https://aiforsocialgood.github.io/iclr2019/schedule.htm)workshop.\n\n*   [GPT](https://openai.com/research/index/?tags=gpt)\n*   [Language](https://openai.com/research/index/?tags=language)\n*   [Generative Models](https://openai.com/research/index/?tags=generative-models)\n*   [Ethics & Safety](https://openai.com/research/index/?tags=ethics-safety)\n*   [Transformers](https://openai.com/research/index/?tags=transformers)\n\nFootnotes\n---------\n\n1.   A\nWe created a new dataset which emphasizes diversity of content, by scraping content from the Internet. In order to preserve document quality, we used only pages which have been curated/filtered by humans—specifically, we used outbound links from Reddit which received at least 3 karma. This can be thought of as a heuristic indicator for whether other users found the link interesting (whether educational or funny), leading to higher data quality than other similar datasets, such as CommonCrawl.\n\n2.   B\nNote that while we have hand-chosen these samples, and are thus engaging in some meta-cherry-picking, we believe they are not too unrepresentative of the sampling process. We are simply using top-k truncated sampling, and have yet to explore more advanced methods of sampling (such as beam-search methods).\n\n3.   C\nPoliticians may want to consider introducing penalties for the misuse of such systems, as some have proposed for deep fakes.\n\nAcknowledgments\n---------------\n\nThanks to David Luan and Rewon Child for their work on GPT-2.\n\nWe also thank the following for feedback on drafts of this post: Greg Brockman, Kai-Fu Lee, Tasha McCauley, Jeffrey Ding, Brian Tse, Allan Dafoe, Rebecca Crootof, Sam Bowman, Ryan Calo, Nick Cammarata and John Schulman.\n\nEditor: Ashley Pilipiszyn\n\nDesign: Justin Jay Wang\n\nCover artwork: Ben Barry\n",
      "fetch_method": "jina-reader"
    },
    {
      "arxiv_id": "https://cdn.openai.com/better-language-models/language-models.pdf",
      "full_text": "Language Models are Unsupervised Multitask Learners\nAlec Radford * 1 Jeffrey Wu * 1 Rewon Child 1 David Luan 1 Dario Amodei ** 1 Ilya Sutskever ** 1\nAbstract\nNatural language processing tasks, such as ques-\ntion answering, machine translation, reading com-\nprehension, and summarization, are typically\napproached with supervised learning on task-\nspeciﬁc datasets. We demonstrate that language\nmodels begin to learn these tasks without any ex-\nplicit supervision when trained on a new dataset\nof millions of webpages called WebText. When\nconditioned on a document plus questions, the an-\nswers generated by the language model reach 55\nF1 on the CoQA dataset - matching or exceeding\nthe performance of 3 out of 4 baseline systems\nwithout using the 127,000+ training examples.\nThe capacity of the language model is essential\nto the success of zero-shot task transfer and in-\ncreasing it improves performance in a log-linear\nfashion across tasks. Our largest model, GPT-2,\nis a 1.5B parameter Transformer that achieves\nstate of the art results on 7 out of 8 tested lan-\nguage modeling datasets in a zero-shot setting\nbut still underﬁts WebText. Samples from the\nmodel reﬂect these improvements and contain co-\nherent paragraphs of text. These ﬁndings suggest\na promising path towards building language pro-\ncessing systems which learn to perform tasks from\ntheir naturally occurring demonstrations.\n1. Introduction\nMachine learning systems now excel (in expectation) at\ntasks they are trained for by using a combination of large\ndatasets, high-capacity models, and supervised learning\n(Krizhevsky et al., 2012) (Sutskever et al., 2014) (Amodei\net al., 2016). Yet these systems are brittle and sensitive to\nslight changes in the data distribution (Recht et al., 2018)\nand task speciﬁcation (Kirkpatrick et al., 2017). Current sys-\ntems are better characterized as narrow experts rather than\n*, **Equal contribution\n1OpenAI, San Francisco, Califor-\nnia,\nUnited States.\nCorrespondence to:\nAlec Radford\n<alec@openai.com>.\ncompetent generalists. We would like to move towards more\ngeneral systems which can perform many tasks – eventually\nwithout the need to manually create and label a training\ndataset for each one.\nThe dominant approach to creating ML systems is to col-\nlect a dataset of training examples demonstrating correct\nbehavior for a desired task, train a system to imitate these\nbehaviors, and then test its performance on independent\nand identically distributed (IID) held-out examples. This\nhas served well to make progress on narrow experts. But\nthe often erratic behavior of captioning models (Lake et al.,\n2017), reading comprehension systems (Jia & Liang, 2017),\nand image classiﬁers (Alcorn et al., 2018) on the diversity\nand variety of possible inputs highlights some of the short-\ncomings of this approach.\nOur suspicion is that the prevalence of single task training\non single domain datasets is a major contributor to the lack\nof generalization observed in current systems. Progress\ntowards robust systems with current architectures is likely\nto require training and measuring performance on a wide\nrange of domains and tasks. Recently, several benchmarks\nhave been proposed such as GLUE (Wang et al., 2018) and\ndecaNLP (McCann et al., 2018) to begin studying this.\nMultitask learning (Caruana, 1997) is a promising frame-\nwork for improving general performance. However, mul-\ntitask training in NLP is still nascent. Recent work re-\nports modest performance improvements (Yogatama et al.,\n2019) and the two most ambitious efforts to date have\ntrained on a total of 10 and 17 (dataset, objective)\npairs respectively (McCann et al., 2018) (Bowman et al.,\n2018). From a meta-learning perspective, each (dataset,\nobjective) pair is a single training example sampled\nfrom the distribution of datasets and objectives. Current\nML systems need hundreds to thousands of examples to\ninduce functions which generalize well. This suggests that\nmultitask training many need just as many effective training\npairs to realize its promise with current approaches. It will\nbe very difﬁcult to continue to scale the creation of datasets\nand the design of objectives to the degree that may be re-\nquired to brute force our way there with current techniques.\nThis motivates exploring additional setups for performing\nmultitask learning.\nThe current best performing systems on language tasks\n\nLanguage Models are Unsupervised Multitask Learners\nFigure 1. Zero-shot task performance of WebText LMs as a function of model size on many NLP tasks. Reading Comprehension results\nare on CoQA (Reddy et al., 2018), translation on WMT-14 Fr-En (Artetxe et al., 2017), summarization on CNN and Daily Mail (See et al.,\n2017), and Question Answering on Natural Questions (Kwiatkowski et al., 2019). Section 3 contains detailed descriptions of each result.\nutilize a combination of pre-training and supervised ﬁne-\ntuning. This approach has a long history with a trend to-\nwards more ﬂexible forms of transfer. First, word vectors\nwere learned and used as inputs to task-speciﬁc architec-\ntures (Mikolov et al., 2013) (Collobert et al., 2011), then\nthe contextual representations of recurrent networks were\ntransferred (Dai & Le, 2015) (Peters et al., 2018), and re-\ncent work suggests that task-speciﬁc architectures are no\nlonger necessary and transferring many self-attention blocks\nis sufﬁcient (Radford et al., 2018) (Devlin et al., 2018).\nThese methods still require supervised training in order\nto perform a task. When only minimal or no supervised\ndata is available, another line of work has demonstrated\nthe promise of language models to perform speciﬁc tasks,\nsuch as commonsense reasoning (Schwartz et al., 2017) and\nsentiment analysis (Radford et al., 2017).\nIn this paper, we connect these two lines of work and con-\ntinue the trend of more general methods of transfer. We\ndemonstrate language models can perform down-stream\ntasks in a zero-shot setting – without any parameter or archi-\ntecture modiﬁcation. We demonstrate this approach shows\npotential by highlighting the ability of language models to\nperform a wide range of tasks in a zero-shot setting. We\nachieve promising, competitive, and state of the art results\ndepending on the task.\n2. Approach\nAt the core of our approach is language modeling. Lan-\nguage modeling is usually framed as unsupervised distri-\nbution estimation from a set of examples (x1, x2, ..., xn)\neach composed of variable length sequences of symbols\n(s1, s2, ..., sn). Since language has a natural sequential or-\ndering, it is common to factorize the joint probabilities over\nsymbols as the product of conditional probabilities (Jelinek\n& Mercer, 1980) (Bengio et al., 2003):\np(x) =\nn\nY\ni=1\np(sn|s1, ..., sn−1)\n(1)\nThis approach allows for tractable sampling from and es-\ntimation of p(x) as well as any conditionals of the form\np(sn−k, ..., sn|s1, ..., sn−k−1). In recent years, there have\nbeen signiﬁcant improvements in the expressiveness of mod-\nels that can compute these conditional probabilities, such as\nself-attention architectures like the Transformer (Vaswani\net al., 2017).\nLearning to perform a single task can be expressed in a\nprobabilistic framework as estimating a conditional distri-\nbution p(output|input). Since a general system should be\nable to perform many different tasks, even for the same\ninput, it should condition not only on the input but also\non the task to be performed.\nThat is, it should model\np(output|input, task). This has been variously formalized\nin multitask and meta-learning settings. Task conditioning\nis often implemented at an architectural level, such as the\ntask speciﬁc encoders and decoders in (Kaiser et al., 2017)\nor at an algorithmic level such as the inner and outer loop\noptimization framework of MAML (Finn et al., 2017). But\nas exempliﬁed in McCann et al. (2018), language provides\na ﬂexible way to specify tasks, inputs, and outputs all as a\nsequence of symbols. For example, a translation training\nexample can be written as the sequence (translate to\nfrench, english text, french text). Like-\nwise, a reading comprehension training example can\nbe written as (answer the question, document,\nquestion, answer). McCann et al. (2018) demon-\nstrated it was possible to train a single model, the MQAN,\n\nLanguage Models are Unsupervised Multitask Learners\nto infer and perform many different tasks on examples with\nthis type of format.\nLanguage modeling is also able to, in principle, learn the\ntasks of McCann et al. (2018) without the need for explicit\nsupervision of which symbols are the outputs to be pre-\ndicted. Since the supervised objective is the the same as the\nunsupervised objective but only evaluated on a subset of the\nsequence, the global minimum of the unsupervised objective\nis also the global minimum of the supervised objective. In\nthis slightly toy setting, the concerns with density estimation\nas a principled training objective discussed in (Sutskever\net al., 2015) are side stepped. The problem instead becomes\nwhether we are able to, in practice, optimize the unsuper-\nvised objective to convergence. Preliminary experiments\nconﬁrmed that sufﬁciently large language models are able to\nperform multitask learning in this toy-ish setup but learning\nis much slower than in explicitly supervised approaches.\nWhile it is a large step from the well-posed setup described\nabove to the messiness of “language in the wild”, Weston\n(2016) argues, in the context of dialog, for the need to\ndevelop systems capable of learning from natural language\ndirectly and demonstrated a proof of concept – learning a\nQA task without a reward signal by using forward prediction\nof a teacher’s outputs. While dialog is an attractive approach,\nwe worry it is overly restrictive. The internet contains a vast\namount of information that is passively available without\nthe need for interactive communication. Our speculation is\nthat a language model with sufﬁcient capacity will begin\nto learn to infer and perform the tasks demonstrated in\nnatural language sequences in order to better predict them,\nregardless of their method of procurement. If a language\nmodel is able to do this it will be, in effect, performing\nunsupervised multitask learning. We test whether this is the\ncase by analyzing the performance of language models in a\nzero-shot setting on a wide variety of tasks.\n2.1. Training Dataset\nMost prior work trained language models on a single do-\nmain of text, such as news articles (Jozefowicz et al., 2016),\nWikipedia (Merity et al., 2016), or ﬁction books (Kiros\net al., 2015). Our approach motivates building as large and\ndiverse a dataset as possible in order to collect natural lan-\nguage demonstrations of tasks in as varied of domains and\ncontexts as possible.\nA promising source of diverse and nearly unlimited text is\nweb scrapes such as Common Crawl. While these archives\nare many orders of magnitude larger than current language\nmodeling datasets, they have signiﬁcant data quality issues.\nTrinh & Le (2018) used Common Crawl in their work on\ncommonsense reasoning but noted a large amount of doc-\numents “whose content are mostly unintelligible”. We ob-\nserved similar data issues in our initial experiments with\n”I’m not the cleverest man in the world, but like they say in\nFrench: Je ne suis pas un imbecile [I’m not a fool].\nIn a now-deleted post from Aug. 16, Soheil Eid, Tory candidate\nin the riding of Joliette, wrote in French: ”Mentez mentez,\nil en restera toujours quelque chose,” which translates as,\n”Lie lie and something will always remain.”\n“I hate the word ‘perfume,”’ Burr says. ‘It’s somewhat better\nin French: ‘parfum.’\nIf listened carefully at 29:55, a conversation can be heard\nbetween two guys in French: “-Comment on fait pour aller\nde l’autre cot´e? -Quel autre cot´e?”, which means “- How\ndo you get to the other side? - What side?”.\nIf this sounds like a bit of a stretch, consider this ques-\ntion in French: As-tu aller au cin´ema?, or Did you go to\nthe movies?, which literally translates as Have-you to go to\nmovies/theater?\n“Brevet Sans Garantie Du Gouvernement”, translated to\nEnglish: “Patented without government warranty”.\nTable 1. Examples of naturally occurring demonstrations of En-\nglish to French and French to English translation found throughout\nthe WebText training set.\nCommon Crawl. Trinh & Le (2018)’s best results were\nachieved using a small subsample of Common Crawl which\nincluded only documents most similar to their target dataset,\nthe Winograd Schema Challenge. While this is a pragmatic\napproach to improve performance on a speciﬁc task, we\nwant to avoid making assumptions about the tasks to be\nperformed ahead of time.\nInstead, we created a new web scrape which emphasizes\ndocument quality. To do this we only scraped web pages\nwhich have been curated/ﬁltered by humans. Manually\nﬁltering a full web scrape would be exceptionally expensive\nso as a starting point, we scraped all outbound links from\nReddit, a social media platform, which received at least 3\nkarma. This can be thought of as a heuristic indicator for\nwhether other users found the link interesting, educational,\nor just funny.\nThe resulting dataset, WebText, contains the text subset\nof these 45 million links. To extract the text from HTML\nresponses we use a combination of the Dragnet (Peters &\nLecocq, 2013) and Newspaper1 content extractors. All re-\nsults presented in this paper use a preliminary version of\nWebText which does not include links created after Dec\n2017 and which after de-duplication and some heuristic\nbased cleaning contains slightly over 8 million documents\nfor a total of 40 GB of text. We removed all Wikipedia\ndocuments from WebText since it is a common data source\nfor other datasets and could complicate analysis due to over-\n1https://github.com/codelucas/newspaper\n\nLanguage Models are Unsupervised Multitask Learners\nlapping training data with test evaluation tasks.\n2.2. Input Representation\nA general language model (LM) should be able to compute\nthe probability of (and also generate) any string. Current\nlarge scale LMs include pre-processing steps such as lower-\ncasing, tokenization, and out-of-vocabulary tokens which\nrestrict the space of model-able strings. While processing\nUnicode strings as a sequence of UTF-8 bytes elegantly ful-\nﬁlls this requirement as exempliﬁed in work such as Gillick\net al. (2015), current byte-level LMs are not competitive\nwith word-level LMs on large scale datasets such as the\nOne Billion Word Benchmark (Al-Rfou et al., 2018). We\nobserved a similar performance gap in our own attempts to\ntrain standard byte-level LMs on WebText.\nByte Pair Encoding (BPE) (Sennrich et al., 2015) is a\npractical middle ground between character and word level\nlanguage modeling which effectively interpolates between\nword level inputs for frequent symbol sequences and char-\nacter level inputs for infrequent symbol sequences. Despite\nits name, reference BPE implementations often operate on\nUnicode code points and not byte sequences. These imple-\nmentations would require including the full space of Uni-\ncode symbols in order to model all Unicode strings. This\nwould result in a base vocabulary of over 130,000 before\nany multi-symbol tokens are added. This is prohibitively\nlarge compared to the 32,000 to 64,000 token vocabularies\noften used with BPE. In contrast, a byte-level version of\nBPE only requires a base vocabulary of size 256. However,\ndirectly applying BPE to the byte sequence results in sub-\noptimal merges due to BPE using a greedy frequency based\nheuristic for building the token vocabulary. We observed\nBPE including many versions of common words like dog\nsince they occur in many variations such as dog. dog!\ndog? . This results in a sub-optimal allocation of limited\nvocabulary slots and model capacity. To avoid this, we pre-\nvent BPE from merging across character categories for any\nbyte sequence. We add an exception for spaces which sig-\nniﬁcantly improves the compression efﬁciency while adding\nonly minimal fragmentation of words across multiple vocab\ntokens.\nThis input representation allows us to combine the empirical\nbeneﬁts of word-level LMs with the generality of byte-level\napproaches. Since our approach can assign a probability to\nany Unicode string, this allows us to evaluate our LMs on\nany dataset regardless of pre-processing, tokenization, or\nvocab size.\n2.3. Model\nWe use a Transformer (Vaswani et al., 2017) based archi-\ntecture for our LMs. The model largely follows the details\nof the OpenAI GPT model (Radford et al., 2018) with a\nParameters\nLayers\ndmodel\n117M\n12\n768\n345M\n24\n1024\n762M\n36\n1280\n1542M\n48\n1600\nTable 2. Architecture hyperparameters for the 4 model sizes.\nfew modiﬁcations. Layer normalization (Ba et al., 2016)\nwas moved to the input of each sub-block, similar to a\npre-activation residual network (He et al., 2016) and an\nadditional layer normalization was added after the ﬁnal self-\nattention block. A modiﬁed initialization which accounts\nfor the accumulation on the residual path with model depth\nis used. We scale the weights of residual layers at initial-\nization by a factor of 1/\n√\nN where N is the number of\nresidual layers. The vocabulary is expanded to 50,257. We\nalso increase the context size from 512 to 1024 tokens and\na larger batchsize of 512 is used.\n3. Experiments\nWe trained and benchmarked four LMs with approximately\nlog-uniformly spaced sizes. The architectures are summa-\nrized in Table 2. The smallest model is equivalent to the\noriginal GPT, and the second smallest equivalent to the\nlargest model from BERT (Devlin et al., 2018). Our largest\nmodel, which we call GPT-2, has over an order of magni-\ntude more parameters than GPT. The learning rate of each\nmodel was manually tuned for the best perplexity on a 5%\nheld-out sample of WebText. All models still underﬁt Web-\nText and held-out perplexity has as of yet improved given\nmore training time.\n3.1. Language Modeling\nAs an initial step towards zero-shot task transfer, we are\ninterested in understanding how WebText LM’s perform\nat zero-shot domain transfer on the primary task they are\ntrained for – language modeling. Since our model operates\non a byte level and does not require lossy pre-processing\nor tokenization, we can evaluate it on any language model\nbenchmark. Results on language modeling datasets are\ncommonly reported in a quantity which is a scaled or ex-\nponentiated version of the average negative log probability\nper canonical prediction unit - usually a character, a byte, or\na word. We evaluate the same quantity by computing the\nlog-probability of a dataset according to a WebText LM and\ndividing by the number of canonical units. For many of these\ndatasets, WebText LMs would be tested signiﬁcantly out-\nof-distribution, having to predict aggressively standardized\ntext, tokenization artifacts such as disconnected punctuation\nand contractions, shufﬂed sentences, and even the string\n\nLanguage Models are Unsupervised Multitask Learners\nLAMBADA\nLAMBADA\nCBT-CN\nCBT-NE\nWikiText2\nPTB\nenwik8\ntext8\nWikiText103\n1BW\n(PPL)\n(ACC)\n(ACC)\n(ACC)\n(PPL)\n(PPL)\n(BPB)\n(BPC)\n(PPL)\n(PPL)\nSOTA\n99.8\n59.23\n85.7\n82.3\n39.14\n46.54\n0.99\n1.08\n18.3\n21.8\n117M\n35.13\n45.99\n87.65\n83.4\n29.41\n65.85\n1.16\n1.17\n37.50\n75.20\n345M\n15.60\n55.48\n92.35\n87.1\n22.76\n47.33\n1.01\n1.06\n26.37\n55.72\n762M\n10.87\n60.12\n93.45\n88.0\n19.93\n40.31\n0.97\n1.02\n22.05\n44.575\n1542M\n8.63\n63.24\n93.30\n89.05\n18.34\n35.76\n0.93\n0.98\n17.48\n42.16\nTable 3. Zero-shot results on many datasets. No training or ﬁne-tuning was performed for any of these results. PTB and WikiText-2\nresults are from (Gong et al., 2018). CBT results are from (Bajgar et al., 2016). LAMBADA accuracy result is from (Hoang et al., 2018)\nand LAMBADA perplexity result is from (Grave et al., 2016). Other results are from (Dai et al., 2019).\n<UNK> which is extremely rare in WebText - occurring\nonly 26 times in 40 billion bytes. We report our main re-\nsults in Table 3 using invertible de-tokenizers which remove\nas many of these tokenization / pre-processing artifacts as\npossible. Since these de-tokenizers are invertible, we can\nstill calculate the log probability of a dataset and they can\nbe thought of as a simple form of domain adaptation. We\nobserve gains of 2.5 to 5 perplexity for GPT-2 with these\nde-tokenizers.\nWebText LMs transfer well across domains and datasets,\nimproving the state of the art on 7 out of the 8 datasets in a\nzero-shot setting. Large improvements are noticed on small\ndatasets such as Penn Treebank and WikiText-2 which have\nonly 1 to 2 million training tokens. Large improvements\nare also noticed on datasets created to measure long-term\ndependencies like LAMBADA (Paperno et al., 2016) and\nthe Children’s Book Test (Hill et al., 2015). Our model is\nstill signiﬁcantly worse than prior work on the One Billion\nWord Benchmark (Chelba et al., 2013). This is likely due\nto a combination of it being both the largest dataset and\nhaving some of the most destructive pre-processing - 1BW’s\nsentence level shufﬂing removes all long-range structure.\n3.2. Children’s Book Test\nFigure 2. Performance on the Children’s Book Test as a function of\nmodel capacity. Human performance are from Bajgar et al. (2016),\ninstead of the much lower estimates from the original paper.\nThe Children’s Book Test (CBT) (Hill et al., 2015) was\ncreated to examine the performance of LMs on different cat-\negories of words: named entities, nouns, verbs, and preposi-\ntions. Rather than reporting perplexity as an evaluation met-\nric, CBT reports accuracy on an automatically constructed\ncloze test where the task is to predict which of 10 possible\nchoices for an omitted word is correct. Following the LM\napproach introduced in the original paper, we compute the\nprobability of each choice and the rest of the sentence con-\nditioned on this choice according to the LM, and predict\nthe one with the highest probability. As seen in Figure 2\nperformance steadily improves as model size is increased\nand closes the majority of the gap to human performance\non this test. Data overlap analysis showed one of the CBT\ntest set books, The Jungle Book by Rudyard Kipling, is in\nWebText, so we report results on the validation set which\nhas no signiﬁcant overlap. GPT-2 achieves new state of the\nart results of 93.3% on common nouns and 89.1% on named\nentities. A de-tokenizer was applied to remove PTB style\ntokenization artifacts from CBT.\n3.3. LAMBADA\nThe LAMBADA dataset (Paperno et al., 2016) tests the\nability of systems to model long-range dependencies in\ntext. The task is to predict the ﬁnal word of sentences\nwhich require at least 50 tokens of context for a human to\nsuccessfully predict. GPT-2 improves the state of the art\nfrom 99.8 (Grave et al., 2016) to 8.6 perplexity and increases\nthe accuracy of LMs on this test from 19% (Dehghani et al.,\n2018) to 52.66%. Investigating GPT-2’s errors showed most\npredictions are valid continuations of the sentence, but are\nnot valid ﬁnal words. This suggests that the LM is not\nusing the additional useful constraint that the word must be\nthe ﬁnal of the sentence. Adding a stop-word ﬁlter as an\napproximation to this further increases accuracy to 63.24%,\nimproving the overall state of the art on this task by 4%. The\nprevious state of the art (Hoang et al., 2018) used a different\nrestricted prediction setting where the outputs of the model\nwere constrained to only words that appeared in the context.\nFor GPT-2, this restriction is harmful rather than helpful\n\nLanguage Models are Unsupervised Multitask Learners\nsince 19% of answers are not in context. We use a version\nof the dataset without preprocessing.\n3.4. Winograd Schema Challenge\nFigure 3. Performance on the Winograd Schema Challenge as a\nfunction of model capacity.\nThe Winograd Schema challenge (Levesque et al., 2012)\nwas constructed to measure the capability of a system to\nperform commonsense reasoning by measuring its ability\nto resolve ambiguities in text. Recently Trinh & Le (2018)\ndemonstrated signiﬁcant progress on this challenge using\nLMs, by predicting the resolution of the ambiguity with\nhigher probability. We follow their problem formulation and\nvisualize the performance of our models with both full and\npartial scoring techniques in Figure 3. GPT-2 improves state\nof the art accuracy by 7%, achieving 70.70%. The dataset\nis quite small with only 273 examples so we recommend\nreading Trichelair et al. (2018) to help contextualize this\nresult.\n3.5. Reading Comprehension\nThe Conversation Question Answering dataset (CoQA)\nReddy et al. (2018) consists of documents from 7 different\ndomains paired with natural language dialogues between a\nquestion asker and a question answerer about the document.\nCoQA tests reading comprehension capabilities and also\nthe ability of models to answer questions that depend on\nconversation history (such as “Why?”).\nGreedy decoding from GPT-2 when conditioned on a doc-\nument, the history of the associated conversation, and a\nﬁnal token A: achieves 55 F1 on the development set. This\nmatches or exceeds the performance of 3 out of 4 base-\nline systems without using the 127,000+ manually collected\nquestion answer pairs those baselines were trained on. The\nsupervised SOTA, a BERT based system (Devlin et al.,\nR-1\nR-2\nR-L\nR-AVG\nBottom-Up Sum\n41.22\n18.68\n38.34\n32.75\nLede-3\n40.38\n17.66\n36.62\n31.55\nSeq2Seq + Attn\n31.33\n11.81\n28.83\n23.99\nGPT-2 TL;DR:\n29.34\n8.27\n26.58\n21.40\nRandom-3\n28.78\n8.63\n25.52\n20.98\nGPT-2 no hint\n21.58\n4.03\n19.47\n15.03\nTable 4. Summarization performance as measured by ROUGE F1\nmetrics on the CNN and Daily Mail dataset. Bottom-Up Sum is\nthe SOTA model from (Gehrmann et al., 2018)\n2018), is nearing the 89 F1 performance of humans. While\nGPT-2’s performance is exciting for a system without any su-\npervised training, some inspection of its answers and errors\nsuggests GPT-2 often uses simple retrieval based heuristics\nsuch as answer with a name from the document in response\nto a who question.\n3.6. Summarization\nWe test GPT-2’s ability to perform summarization on the\nCNN and Daily Mail dataset (Nallapati et al., 2016). To in-\nduce summarization behavior we add the text TL;DR: after\nthe article and generate 100 tokens with Top-k random sam-\npling (Fan et al., 2018) with k = 2 which reduces repetition\nand encourages more abstractive summaries than greedy de-\ncoding. We use the ﬁrst 3 generated sentences in these 100\ntokens as the summary. While qualitatively the generations\nresemble summaries, as shown in Table 14, they often focus\non recent content from the article or confuse speciﬁc details\nsuch as how many cars were involved in a crash or whether\na logo was on a hat or shirt. On the commonly reported\nROUGE 1,2,L metrics the generated summaries only begin\nto approach the performance of classic neural baselines and\njust barely outperforms selecting 3 random sentences from\nthe article. GPT-2’s performance drops by 6.4 points on\nthe aggregate metric when the task hint is removed which\ndemonstrates the ability to invoke task speciﬁc behavior in\na language model with natural language.\n3.7. Translation\nWe test whether GPT-2 has begun to learn how to translate\nfrom one language to another. In order to help it infer that\nthis is the desired task, we condition the language model\non a context of example pairs of the format english\nsentence = french sentence and then after a ﬁ-\nnal prompt of english sentence = we sample from\nthe model with greedy decoding and use the ﬁrst generated\nsentence as the translation. On the WMT-14 English-French\ntest set, GPT-2 gets 5 BLEU, which is slightly worse than\na word-by-word substitution with a bilingual lexicon in-\nferred in previous work on unsupervised word translation\n\nLanguage Models are Unsupervised Multitask Learners\nQuestion\nGenerated Answer\nCorrect\nProbability\nWho wrote the book the origin of species?\nCharles Darwin\n\u0013\n83.4%\nWho is the founder of the ubuntu project?\nMark Shuttleworth\n\u0013\n82.0%\nWho is the quarterback for the green bay packers?\nAaron Rodgers\n\u0013\n81.1%\nPanda is a national animal of which country?\nChina\n\u0013\n76.8%\nWho came up with the theory of relativity?\nAlbert Einstein\n\u0013\n76.4%\nWhen was the ﬁrst star wars ﬁlm released?\n1977\n\u0013\n71.4%\nWhat is the most common blood type in sweden?\nA\n\u0017\n70.6%\nWho is regarded as the founder of psychoanalysis?\nSigmund Freud\n\u0013\n69.3%\nWho took the ﬁrst steps on the moon in 1969?\nNeil Armstrong\n\u0013\n66.8%\nWho is the largest supermarket chain in the uk?\nTesco\n\u0013\n65.3%\nWhat is the meaning of shalom in english?\npeace\n\u0013\n64.0%\nWho was the author of the art of war?\nSun Tzu\n\u0013\n59.6%\nLargest state in the us by land mass?\nCalifornia\n\u0017\n59.2%\nGreen algae is an example of which type of reproduction?\nparthenogenesis\n\u0017\n56.5%\nVikram samvat calender is ofﬁcial in which country?\nIndia\n\u0013\n55.6%\nWho is mostly responsible for writing the declaration of independence?\nThomas Jefferson\n\u0013\n53.3%\nWhat us state forms the western boundary of montana?\nMontana\n\u0017\n52.3%\nWho plays ser davos in game of thrones?\nPeter Dinklage\n\u0017\n52.1%\nWho appoints the chair of the federal reserve system?\nJanet Yellen\n\u0017\n51.5%\nState the process that divides one nucleus into two genetically identical nuclei?\nmitosis\n\u0013\n50.7%\nWho won the most mvp awards in the nba?\nMichael Jordan\n\u0017\n50.2%\nWhat river is associated with the city of rome?\nthe Tiber\n\u0013\n48.6%\nWho is the ﬁrst president to be impeached?\nAndrew Johnson\n\u0013\n48.3%\nWho is the head of the department of homeland security 2017?\nJohn Kelly\n\u0013\n47.0%\nWhat is the name given to the common currency to the european union?\nEuro\n\u0013\n46.8%\nWhat was the emperor name in star wars?\nPalpatine\n\u0013\n46.5%\nDo you have to have a gun permit to shoot at a range?\nNo\n\u0013\n46.4%\nWho proposed evolution in 1859 as the basis of biological development?\nCharles Darwin\n\u0013\n45.7%\nNuclear power plant that blew up in russia?\nChernobyl\n\u0013\n45.7%\nWho played john connor in the original terminator?\nArnold Schwarzenegger\n\u0017\n45.2%\nTable 5. The 30 most conﬁdent answers generated by GPT-2 on the development set of Natural Questions sorted by their probability\naccording to GPT-2. None of these questions appear in WebText according to the procedure described in Section 4.\n(Conneau et al., 2017b). On the WMT-14 French-English\ntest set, GPT-2 is able to leverage its very strong English\nlanguage model to perform signiﬁcantly better, achieving\n11.5 BLEU. This outperforms several unsupervised machine\ntranslation baselines from (Artetxe et al., 2017) and (Lample\net al., 2017) but is still much worse than the 33.5 BLEU of\nthe current best unsupervised machine translation approach\n(Artetxe et al., 2019). Performance on this task was sur-\nprising to us, since we deliberately removed non-English\nwebpages from WebText as a ﬁltering step. In order to con-\nﬁrm this, we ran a byte-level language detector2 on WebText\nwhich detected only 10MB of data in the French language\nwhich is approximately 500x smaller than the monolingual\nFrench corpus common in prior unsupervised machine trans-\nlation research.\n3.8. Question Answering\nA potential way to test what information is contained within\na language model is to evaluate how often it generates the\ncorrect answer to factoid-style questions. Previous showcas-\ning of this behavior in neural systems where all information\nis stored in parameters such as A Neural Conversational\nModel (Vinyals & Le, 2015) reported qualitative results due\nto the lack of high-quality evaluation datasets. The recently\nintroduced Natural Questions dataset (Kwiatkowski et al.,\n2https://github.com/CLD2Owners/cld2\n2019) is a promising resource to test this more quantita-\ntively. Similar to translation, the context of the language\nmodel is seeded with example question answer pairs which\nhelps the model infer the short answer style of the dataset.\nGPT-2 answers 4.1% of questions correctly when evalu-\nated by the exact match metric commonly used on reading\ncomprehension datasets like SQUAD.3 As a comparison\npoint, the smallest model does not exceed the 1.0% accu-\nracy of an incredibly simple baseline which returns the most\ncommon answer for each question type (who, what, where,\netc...). GPT-2 answers 5.3 times more questions correctly,\nsuggesting that model capacity has been a major factor in\nthe poor performance of neural systems on this kind of task\nas of yet. The probability GPT-2 assigns to its generated\nanswers is well calibrated and GPT-2 has an accuracy of\n63.1% on the 1% of questions it is most conﬁdent in. The\n30 most conﬁdent answers generated by GPT-2 on develop-\nment set questions are shown in Table 5. The performance\nof GPT-2 is still much, much, worse than the 30 to 50%\nrange of open domain question answering systems which\nhybridize information retrieval with extractive document\nquestion answering (Alberti et al., 2019).\n3Alec, who previously thought of himself as good at random\ntrivia, answered 17 of 100 randomly sampled examples correctly\nwhen tested in the same setting as GPT-2. He actually only got 14 right but he\nshould have gotten those other 3\n\nLanguage Models are Unsupervised Multitask Learners\nPTB\nWikiText-2\nenwik8\ntext8\nWikitext-103\n1BW\nDataset train\n2.67%\n0.66%\n7.50%\n2.34%\n9.09%\n13.19%\nWebText train\n0.88%\n1.63%\n6.31%\n3.94%\n2.42%\n3.75%\nTable 6. Percentage of test set 8 grams overlapping with training sets.\n4. Generalization vs Memorization\nRecent work in computer vision has shown that common im-\nage datasets contain a non-trivial amount of near-duplicate\nimages. For instance CIFAR-10 has 3.3% overlap between\ntrain and test images (Barz & Denzler, 2019). This results in\nan over-reporting of the generalization performance of ma-\nchine learning systems. As the size of datasets increases this\nissue becomes increasingly likely which suggests a similar\nphenomena could be happening with WebText. Therefore it\nis important to analyze how much test data also shows up in\nthe training data.\nTo study this we created Bloom ﬁlters containing 8-grams\nof WebText training set tokens. To improve recall, strings\nwere normalized to contain only lower-cased alphanumeric\nwords with a single space as a delimiter. The Bloom ﬁlters\nwere constructed such that the false positive rate is upper\nbounded by\n1\n108 . We further veriﬁed the low false positive\nrate by generating 1M strings, of which zero were found by\nthe ﬁlter.\nThese Bloom ﬁlters let us calculate, given a dataset, the\npercentage of 8-grams from that dataset that are also found\nin the WebText training set. Table 6 shows this overlap anal-\nysis for the test sets of common LM benchmarks. Common\nLM datasets’ test sets have between 1-6% overlap with Web-\nText train, with an average of overlap of 3.2%. Somewhat\nsurprisingly, many datasets have larger overlaps with their\nown training splits, with an average of 5.9% overlap.\nOur approach optimizes for recall, and while manual inspec-\ntion of the overlaps shows many common phrases, there are\nmany longer matches that are due to duplicated data. This is\nnot unique to WebText. For instance, we discovered that the\ntest set of WikiText-103 has an article which is also in the\ntraining dataset. Since there are only 60 articles in the test\nset there is at least an overlap of 1.6%.4 Potentially more\nworryingly, 1BW has an overlap of nearly 13.2% with its\nown training set according to our procedure.\nFor the Winograd Schema Challenge, we found only 10\nschemata which had any 8-gram overlaps with the WebText\ntraining set. Of these, 2 were spurious matches. Of the\nremaining 8, only 1 schema appeared in any contexts that\n4A signiﬁcant portion of additional overlap is due to editors\nreusing some paragraphs across multiple articles with a shared\ntheme such as various battles in the Korean War.\ngave away the answer.\nFor CoQA, about 15% of documents in the news domain\nare already in WebText and the model performs about 3\nF1 better on these. CoQA’s development set metric reports\nthe average performance over 5 different domains and we\nmeasure a gain of about 0.5-1.0 F1 due to overlap across the\nvarious domains. However, no actual training questions or\nanswers are in WebText since CoQA was released after the\ncutoff date for links in WebText.\nOn LAMBADA, the average overlap is 1.2%. GPT-2 per-\nforms about 2 perplexity better on examples with greater\nthan 15% overlap. Recalculating metrics when excluding\nall examples with any overlap shifts results from 8.6 to 8.7\nperplexity and reduces accuracy from 63.2% to 62.9%. This\nvery small change in overall results is likely due to only 1\nin 200 examples having signiﬁcant overlap.\nOverall, our analysis suggests that data overlap between\nWebText training data and speciﬁc evaluation datasets pro-\nvides a small but consistent beneﬁt to reported results. How-\never, for most datasets we do not notice signiﬁcantly larger\noverlaps than those already existing between standard train-\ning and test sets, as Table 6 highlights.\nUnderstanding and quantifying how highly similar text im-\npacts performance is an important research question. Better\nde-duplication techniques such as scalable fuzzy matching\ncould also help better answer these questions. For now, we\nrecommend the use of n-gram overlap based de-duplication\nas an important veriﬁcation step and sanity check during the\ncreation of training and test splits for new NLP datasets.\nAnother potential way of determining whether the perfor-\nmance of WebText LMs is attributable to memorization is\ninspecting their performance on their own held-out set. As\nshown in Figure 4, performance on both the training and\ntest sets of WebText are similar and improve together as\nmodel size is increased. This suggests even GPT-2 is still\nunderﬁtting on WebText in many ways.\nGPT-2 is also able to write news articles about the discovery\nof talking unicorns. An example is provided in Table 13.\n5. Related Work\nA signiﬁcant portion of this work measured the performance\nof larger language models trained on larger datasets. This\n\nLanguage Models are Unsupervised Multitask Learners\nFigure 4. The performance of LMs trained on WebText as a func-\ntion of model size.\nis similar to the work of Jozefowicz et al. (2016) which\nscaled RNN based language models on the 1 Billion Word\nBenchmark. Bajgar et al. (2016) also previously improved\nresults on the Children’s Book Test by creating a much larger\ntraining dataset out of Project Gutenberg to supplement the\nstandard training dataset. Hestness et al. (2017) conducted\na thorough analysis of how the performance of various deep\nlearning models changes as a function of both model capac-\nity and dataset size. Our experiments, while much noisier\nacross tasks, suggest similar trends hold for sub-tasks of an\nobjective and continue into the 1B+ parameter regime.\nInteresting learned functionality in generative models\nhas been documented before such as the cells in an\nRNN language model performing line-width tracking and\nquote/comment detection Karpathy et al. (2015). More in-\nspirational to our work was the observation of Liu et al.\n(2018) that a model trained to generate Wikipedia articles\nalso learned to translate names between languages.\nPrevious work has explored alternative approaches to ﬁlter-\ning and constructing a large text corpus of web pages, such\nas the iWeb Corpus (Davies, 2018).\nThere has been extensive work on pre-training methods\nfor language tasks. In addition to those mentioned in the\nintroduction, GloVe (Pennington et al., 2014) scaled word\nvector representation learning to all of Common Crawl. An\ninﬂuential early work on deep representation learning for\ntext was Skip-thought Vectors (Kiros et al., 2015). McCann\net al. (2017) explored the use of representations derived from\nmachine translation models and Howard & Ruder (2018)\nimproved the RNN based ﬁne-tuning approaches of (Dai\n& Le, 2015). (Conneau et al., 2017a) studied the transfer\nperformance of representations learned by natural language\ninference models and (Subramanian et al., 2018) explored\nlarge-scale multitask training.\n(Ramachandran et al., 2016) demonstrated that seq2seq mod-\nels beneﬁt from being initialized with pre-trained language\nmodels as encoders and decoders. More recent work has\nshown that LM pre-training is helpful when ﬁne-tuned for\ndifﬁcult generation tasks like chit-chat dialog and dialog\nbased question answering systems as well (Wolf et al., 2019)\n(Dinan et al., 2018).\n6. Discussion\nMuch research has been dedicated to learning (Hill et al.,\n2016), understanding (Levy & Goldberg, 2014), and criti-\ncally evaluating (Wieting & Kiela, 2019) the representations\nof both supervised and unsupervised pre-training methods.\nOur results suggest that unsupervised task learning is an\nadditional promising area of research to explore. These\nﬁndings potentially help explain the widespread success of\npre-training techniques for down-stream NLP tasks as we\nshow that, in the limit, one of these pre-training techniques\nbegins to learn to perform tasks directly without the need\nfor supervised adaption or modiﬁcation.\nOn reading comprehension the performance of GPT-2 is\ncompetitive with supervised baselines in a zero-shot setting.\nHowever, on other tasks such as summarization, while it\nis qualitatively performing the task, its performance is still\nonly rudimentary according to quantitative metrics. While\nsuggestive as a research result, in terms of practical applica-\ntions, the zero-shot performance of GPT-2 is still far from\nuse-able.\nWe have studied the zero-shot performance of WebText\nLMs on many canonical NLP tasks, but there are many addi-\ntional tasks that could be evaluated. There are undoubtedly\nmany practical tasks where the performance of GPT-2 is\nstill no better than random. Even on common tasks that we\nevaluated on, such as question answering and translation,\nlanguage models only begin to outperform trivial baselines\nwhen they have sufﬁcient capacity.\nWhile zero-shot performance establishes a baseline of the\npotential performance of GPT-2 on many tasks, it is not\nclear where the ceiling is with ﬁnetuning. On some tasks,\nGPT-2’s fully abstractive output is a signiﬁcant departure\nfrom the extractive pointer network (Vinyals et al., 2015)\nbased outputs which are currently state of the art on many\nquestion answering and reading comprehension datasets.\nGiven the prior success of ﬁne-tuning GPT, we plan to in-\nvestigate ﬁne-tuning on benchmarks such as decaNLP and\nGLUE, especially since it is unclear whether the additional\n\nLanguage Models are Unsupervised Multitask Learners\ntraining data and capacity of GPT-2 is sufﬁcient to over-\ncome the inefﬁciencies of uni-directional representations\ndemonstrated by BERT (Devlin et al., 2018).\n7. Conclusion\nWhen a large language model is trained on a sufﬁciently\nlarge and diverse dataset it is able to perform well across\nmany domains and datasets. GPT-2 zero-shots to state of\nthe art performance on 7 out of 8 tested language model-\ning datasets. The diversity of tasks the model is able to\nperform in a zero-shot setting suggests that high-capacity\nmodels trained to maximize the likelihood of a sufﬁciently\nvaried text corpus begin to learn how to perform a surprising\namount of tasks without the need for explicit supervision.5\nAcknowledgements\nThanks to everyone who wrote the text, shared the links,\nand upvoted the content in WebText. Many millions of\npeople were involved in creating the data that GPT-2 was\ntrained on. Also thanks to all the Googlers who helped us\nwith training infrastructure, including Zak Stone, JS Riehl,\nJonathan Hseu, Russell Power, Youlong Cheng, Noam\nShazeer, Solomon Boulos, Michael Banﬁeld, Aman Gupta,\nDaniel Sohn, and many more. Finally thanks to the people\nwho gave feedback on drafts of the paper: Jacob Steinhardt,\nSam Bowman, Geoffrey Irving, and Madison May.\nReferences\nAl-Rfou, R., Choe, D., Constant, N., Guo, M., and Jones, L.\nCharacter-level language modeling with deeper self-attention.\narXiv preprint arXiv:1808.04444, 2018.\nAlberti, C., Lee, K., and Collins, M. A bert baseline for the natural\nquestions. arXiv preprint arXiv:1901.08634, 2019.\nAlcorn, M. A., Li, Q., Gong, Z., Wang, C., Mai, L., Ku, W.-S., and\nNguyen, A. Strike (with) a pose: Neural networks are easily\nfooled by strange poses of familiar objects. arXiv preprint\narXiv:1811.11553, 2018.\nAmodei, D., Ananthanarayanan, S., Anubhai, R., Bai, J., Batten-\nberg, E., Case, C., Casper, J., Catanzaro, B., Cheng, Q., Chen,\nG., et al. Deep speech 2: End-to-end speech recognition in\nenglish and mandarin. In International Conference on Machine\nLearning, pp. 173–182, 2016.\nArtetxe, M., Labaka, G., Agirre, E., and Cho, K. Unsupervised\nneural machine translation. arXiv preprint arXiv:1710.11041,\n2017.\nArtetxe, M., Labaka, G., and Agirre, E.\nAn effective ap-\nproach to unsupervised machine translation. arXiv preprint\narXiv:1902.01313, 2019.\n5Preliminary code for downloading and using the small model\nis available at https://github.com/openai/gpt-2\nBa, J. L., Kiros, J. R., and Hinton, G. E. Layer normalization.\narXiv preprint arXiv:1607.06450, 2016.\nBajgar, O., Kadlec, R., and Kleindienst, J. Embracing data abun-\ndance: Booktest dataset for reading comprehension. arXiv\npreprint arXiv:1610.00956, 2016.\nBarz, B. and Denzler, J. Do we train on test data? purging cifar of\nnear-duplicates. arXiv preprint arXiv:1902.00423, 2019.\nBengio, Y., Ducharme, R., Vincent, P., and Jauvin, C. A neural\nprobabilistic language model. Journal of machine learning\nresearch, 3(Feb):1137–1155, 2003.\nBowman, S. R., Pavlick, E., Grave, E., Van Durme, B., Wang, A.,\nHula, J., Xia, P., Pappagari, R., McCoy, R. T., Patel, R., et al.\nLooking for elmo’s friends: Sentence-level pretraining beyond\nlanguage modeling. arXiv preprint arXiv:1812.10860, 2018.\nCaruana, R. Multitask learning. Machine learning, 28(1):41–75,\n1997.\nChelba, C., Mikolov, T., Schuster, M., Ge, Q., Brants, T., Koehn,\nP., and Robinson, T. One billion word benchmark for measur-\ning progress in statistical language modeling. arXiv preprint\narXiv:1312.3005, 2013.\nCollobert, R., Weston, J., Bottou, L., Karlen, M., Kavukcuoglu,\nK., and Kuksa, P. Natural language processing (almost) from\nscratch. Journal of Machine Learning Research, 12(Aug):2493–\n2537, 2011.\nConneau, A., Kiela, D., Schwenk, H., Barrault, L., and Bor-\ndes, A. Supervised learning of universal sentence represen-\ntations from natural language inference data. arXiv preprint\narXiv:1705.02364, 2017a.\nConneau, A., Lample, G., Ranzato, M., Denoyer, L., and J´egou,\nH.\nWord translation without parallel data.\narXiv preprint\narXiv:1710.04087, 2017b.\nDai, A. M. and Le, Q. V. Semi-supervised sequence learning. In\nAdvances in neural information processing systems, pp. 3079–\n3087, 2015.\nDai, Z., Yang, Z., Yang, Y., Cohen, W. W., Carbonell, J., Le,\nQ. V., and Salakhutdinov, R. Transformer-xl: Attentive lan-\nguage models beyond a ﬁxed-length context. arXiv preprint\narXiv:1901.02860, 2019.\nDavies,\nM.\nThe\n14\nbillion\nword\niweb\ncorpus.\nhttps://corpus.byu.edu/iWeb/, 2018.\nDehghani, M., Gouws, S., Vinyals, O., Uszkoreit, J., and Kaiser,\nŁ. Universal transformers. arXiv preprint arXiv:1807.03819,\n2018.\nDevlin, J., Chang, M.-W., Lee, K., and Toutanova, K. Bert: Pre-\ntraining of deep bidirectional transformers for language under-\nstanding. arXiv preprint arXiv:1810.04805, 2018.\nDinan, E., Roller, S., Shuster, K., Fan, A., Auli, M., and Weston,\nJ. Wizard of wikipedia: Knowledge-powered conversational\nagents. arXiv preprint arXiv:1811.01241, 2018.\nFan, A., Lewis, M., and Dauphin, Y. Hierarchical neural story\ngeneration. arXiv preprint arXiv:1805.04833, 2018.\n\nLanguage Models are Unsupervised Multitask Learners\nFinn, C., Abbeel, P., and Levine, S.\nModel-agnostic meta-\nlearning for fast adaptation of deep networks. arXiv preprint\narXiv:1703.03400, 2017.\nGehrmann, S., Deng, Y., and Rush, A. M. Bottom-up abstractive\nsummarization. arXiv preprint arXiv:1808.10792, 2018.\nGillick, D., Brunk, C., Vinyals, O., and Subramanya, A. Mul-\ntilingual language processing from bytes.\narXiv preprint\narXiv:1512.00103, 2015.\nGong, C., He, D., Tan, X., Qin, T., Wang, L., and Liu, T.-Y. Frage:\nfrequency-agnostic word representation. In Advances in Neural\nInformation Processing Systems, pp. 1341–1352, 2018.\nGrave, E., Joulin, A., and Usunier, N.\nImproving neural\nlanguage models with a continuous cache.\narXiv preprint\narXiv:1612.04426, 2016.\nHe, K., Zhang, X., Ren, S., and Sun, J. Identity mappings in deep\nresidual networks. In European conference on computer vision,\npp. 630–645. Springer, 2016.\nHestness, J., Narang, S., Ardalani, N., Diamos, G., Jun, H., Kian-\ninejad, H., Patwary, M., Ali, M., Yang, Y., and Zhou, Y. Deep\nlearning scaling is predictable, empirically.\narXiv preprint\narXiv:1712.00409, 2017.\nHill, F., Bordes, A., Chopra, S., and Weston, J. The goldilocks\nprinciple: Reading children’s books with explicit memory rep-\nresentations. arXiv preprint arXiv:1511.02301, 2015.\nHill, F., Cho, K., and Korhonen, A. Learning distributed repre-\nsentations of sentences from unlabelled data. arXiv preprint\narXiv:1602.03483, 2016.\nHoang, L., Wiseman, S., and Rush, A. M. Entity tracking im-\nproves cloze-style reading comprehension.\narXiv preprint\narXiv:1810.02891, 2018.\nHoward, J. and Ruder, S. Universal language model ﬁne-tuning for\ntext classiﬁcation. In Proceedings of the 56th Annual Meeting\nof the Association for Computational Linguistics (Volume 1:\nLong Papers), volume 1, pp. 328–339, 2018.\nJelinek, F. and Mercer, R. L. Interpolated estimation of markov\nsource parameters from sparse data. In Proceedings of the\nWorkshop on Pattern Recognition in Practice, Amsterdam, The\nNetherlands: North-Holland, May., 1980.\nJia, R. and Liang, P. Adversarial examples for evaluating read-\ning comprehension systems. arXiv preprint arXiv:1707.07328,\n2017.\nJozefowicz, R., Vinyals, O., Schuster, M., Shazeer, N., and Wu,\nY. Exploring the limits of language modeling. arXiv preprint\narXiv:1602.02410, 2016.\nKaiser, L., Gomez, A. N., Shazeer, N., Vaswani, A., Parmar, N.,\nJones, L., and Uszkoreit, J. One model to learn them all. arXiv\npreprint arXiv:1706.05137, 2017.\nKarpathy, A., Johnson, J., and Fei-Fei, L. Visualizing and under-\nstanding recurrent networks. arXiv preprint arXiv:1506.02078,\n2015.\nKirkpatrick, J., Pascanu, R., Rabinowitz, N., Veness, J., Desjardins,\nG., Rusu, A. A., Milan, K., Quan, J., Ramalho, T., Grabska-\nBarwinska, A., et al. Overcoming catastrophic forgetting in\nneural networks. Proceedings of the national academy of sci-\nences, pp. 201611835, 2017.\nKiros, R., Zhu, Y., Salakhutdinov, R. R., Zemel, R., Urtasun, R.,\nTorralba, A., and Fidler, S. Skip-thought vectors. In Advances\nin neural information processing systems, pp. 3294–3302, 2015.\nKrizhevsky, A., Sutskever, I., and Hinton, G. E. Imagenet classiﬁ-\ncation with deep convolutional neural networks. In Advances in\nneural information processing systems, pp. 1097–1105, 2012.\nKwiatkowski, T., Palomaki, J., Rhinehart, O., Collins, M., Parikh,\nA., Alberti, C., Epstein, D., Polosukhin, I., Kelcey, M., Devlin,\nJ., et al. Natural questions: a benchmark for question answering\nresearch. 2019.\nLake, B. M., Ullman, T. D., Tenenbaum, J. B., and Gershman, S. J.\nBuilding machines that learn and think like people. Behavioral\nand Brain Sciences, 40, 2017.\nLample, G., Conneau, A., Denoyer, L., and Ranzato, M. Unsu-\npervised machine translation using monolingual corpora only.\narXiv preprint arXiv:1711.00043, 2017.\nLevesque, H., Davis, E., and Morgenstern, L.\nThe winograd\nschema challenge. In Thirteenth International Conference on\nthe Principles of Knowledge Representation and Reasoning,\n2012.\nLevy, O. and Goldberg, Y. Neural word embedding as implicit ma-\ntrix factorization. In Advances in neural information processing\nsystems, pp. 2177–2185, 2014.\nLiu, P. J., Saleh, M., Pot, E., Goodrich, B., Sepassi, R., Kaiser, L.,\nand Shazeer, N. Generating wikipedia by summarizing long\nsequences. arXiv preprint arXiv:1801.10198, 2018.\nMcCann, B., Bradbury, J., Xiong, C., and Socher, R. Learned\nin translation: Contextualized word vectors. In Advances in\nNeural Information Processing Systems, pp. 6294–6305, 2017.\nMcCann, B., Keskar, N. S., Xiong, C., and Socher, R. The natural\nlanguage decathlon: Multitask learning as question answering.\narXiv preprint arXiv:1806.08730, 2018.\nMerity, S., Xiong, C., Bradbury, J., and Socher, R. Pointer sentinel\nmixture models. arXiv preprint arXiv:1609.07843, 2016.\nMikolov, T., Sutskever, I., Chen, K., Corrado, G. S., and Dean,\nJ. Distributed representations of words and phrases and their\ncompositionality. In Advances in neural information processing\nsystems, pp. 3111–3119, 2013.\nNallapati, R., Zhou, B., Gulcehre, C., Xiang, B., et al. Abstrac-\ntive text summarization using sequence-to-sequence rnns and\nbeyond. arXiv preprint arXiv:1602.06023, 2016.\nPaperno, D., Kruszewski, G., Lazaridou, A., Pham, Q. N., Bernardi,\nR., Pezzelle, S., Baroni, M., Boleda, G., and Fern´andez, R. The\nlambada dataset: Word prediction requiring a broad discourse\ncontext. arXiv preprint arXiv:1606.06031, 2016.\nPennington, J., Socher, R., and Manning, C. Glove: Global vectors\nfor word representation. In Proceedings of the 2014 conference\non empirical methods in natural language processing (EMNLP),\npp. 1532–1543, 2014.\n\nLanguage Models are Unsupervised Multitask Learners\nPeters, M. E. and Lecocq, D. Content extraction using diverse fea-\nture sets. In Proceedings of the 22nd International Conference\non World Wide Web, pp. 89–90. ACM, 2013.\nPeters, M. E., Neumann, M., Iyyer, M., Gardner, M., Clark, C.,\nLee, K., and Zettlemoyer, L. Deep contextualized word repre-\nsentations. arXiv preprint arXiv:1802.05365, 2018.\nRadford, A., Jozefowicz, R., and Sutskever, I.\nLearning to\ngenerate reviews and discovering sentiment. arXiv preprint\narXiv:1704.01444, 2017.\nRadford, A., Narasimhan, K., Salimans, T., and Sutskever, I.\nImproving language understanding by generative pre-training.\n2018.\nRamachandran, P., Liu, P. J., and Le, Q. V. Unsupervised pre-\ntraining for sequence to sequence learning.\narXiv preprint\narXiv:1611.02683, 2016.\nRecht, B., Roelofs, R., Schmidt, L., and Shankar, V.\nDo\ncifar-10 classiﬁers generalize to cifar-10?\narXiv preprint\narXiv:1806.00451, 2018.\nReddy, S., Chen, D., and Manning, C. D. Coqa: A conversational\nquestion answering challenge. arXiv preprint arXiv:1808.07042,\n2018.\nSchwartz, R., Sap, M., Konstas, I., Zilles, L., Choi, Y., and Smith,\nN. A. Story cloze task: Uw nlp system. In Proceedings of the\n2nd Workshop on Linking Models of Lexical, Sentential and\nDiscourse-level Semantics, pp. 52–55, 2017.\nSee, A., Liu, P. J., and Manning, C. D. Get to the point: Sum-\nmarization with pointer-generator networks. arXiv preprint\narXiv:1704.04368, 2017.\nSennrich, R., Haddow, B., and Birch, A. Neural machine trans-\nlation of rare words with subword units.\narXiv preprint\narXiv:1508.07909, 2015.\nSubramanian, S., Trischler, A., Bengio, Y., and Pal, C. J. Learning\ngeneral purpose distributed sentence representations via large\nscale multi-task learning. arXiv preprint arXiv:1804.00079,\n2018.\nSutskever, I., Vinyals, O., and Le, Q. V. Sequence to sequence\nlearning with neural networks. In Advances in neural informa-\ntion processing systems, pp. 3104–3112, 2014.\nSutskever, I., Jozefowicz, R., Gregor, K., Rezende, D., Lillicrap,\nT., and Vinyals, O. Towards principled unsupervised learning.\narXiv preprint arXiv:1511.06440, 2015.\nTrichelair, P., Emami, A., Cheung, J. C. K., Trischler, A., Sule-\nman, K., and Diaz, F. On the evaluation of common-sense\nreasoning in natural language understanding. arXiv preprint\narXiv:1811.01778, 2018.\nTrinh, T. H. and Le, Q. V. A simple method for commonsense\nreasoning. arXiv preprint arXiv:1806.02847, 2018.\nVaswani, A., Shazeer, N., Parmar, N., Uszkoreit, J., Jones, L.,\nGomez, A. N., Kaiser, Ł., and Polosukhin, I. Attention is\nall you need. In Advances in Neural Information Processing\nSystems, pp. 5998–6008, 2017.\nVinyals, O. and Le, Q. A neural conversational model. arXiv\npreprint arXiv:1506.05869, 2015.\nVinyals, O., Fortunato, M., and Jaitly, N. Pointer networks. In\nAdvances in Neural Information Processing Systems, pp. 2692–\n2700, 2015.\nWang, A., Singh, A., Michael, J., Hill, F., Levy, O., and Bow-\nman, S. R.\nGlue: A multi-task benchmark and analysis\nplatform for natural language understanding. arXiv preprint\narXiv:1804.07461, 2018.\nWeston, J. E. Dialog-based language learning. In Advances in\nNeural Information Processing Systems, pp. 829–837, 2016.\nWieting, J. and Kiela, D.\nNo training required: Exploring\nrandom encoders for sentence classiﬁcation. arXiv preprint\narXiv:1901.10444, 2019.\nWolf, T., Sanh, V., Chaumond, J., and Delangue, C. Transfer-\ntransfo: A transfer learning approach for neural network based\nconversational agents. arXiv preprint arXiv:1901.08149, 2019.\nYogatama, D., d’Autume, C. d. M., Connor, J., Kocisky, T.,\nChrzanowski, M., Kong, L., Lazaridou, A., Ling, W., Yu, L.,\nDyer, C., et al. Learning and evaluating general linguistic intel-\nligence. arXiv preprint arXiv:1901.11373, 2019.\n\nLanguage Models are Unsupervised Multitask Learners\n8. Appendix A: Samples\n8.1. Model capacity\nTo complement the reported perplexity gains of bigger LMs on\nWebText show in Figure 4, Tables 7 through 11 show side-by-side\ncompletions of the smallest WebText LM and GPT-2 on random\nunseen WebText test set articles.\n8.2. Text Memorization\nWe observe some memorizing behavior in GPT-2 on longer strings\nthat are repeated many times in the dataset such as famous quotes\nor speeches. For example, when conditioned on the ﬁrst sentence\nand a half of the Gettysburg Address (which occurs approximately\n40 times throughout WebText), an argmax decode from GPT-2\nrecovers the speech. Even when sampling without truncation, we\nﬁnd that the model copies the speech for awhile before drifting,\nalbeit in a similar style. It typically drifts within 100-200 tokens,\nand displays widening diversity once it drifts.\nTo quantify how often exact memorization shows up in samples,\nwe generated samples from GPT-2 conditioned on WebText test\nset articles and compared the overlap rates of GPT-2’s generations\nto the overlap rates of the ground-truth completions. The results of\nthis analysis are shown below and suggest that GPT-2 repeats text\nfrom the training set less often then the baseline rate of held-out\narticles.\nFigure 5. CDF of percentage 8-gram overlap with WebText train-\ning set, for both WebText test set and samples (conditioned on\nWebText test set, with top-k truncated random sampling with\nk = 40). Most samples have less than 1% overlap, including over\n30% of samples with no overlap, whereas the median for test set is\n2.6% overlap.\n8.3. Diversity\nTable 12 shows multiple completions of the same random WebText\ntest set context, showing the diversity of completions with standard\nsampling settings.\n8.4. Robustness\nTable 13 shows the previously mentioned talking unicorns news\narticle. We ﬁnd the model to be capable of handling out of dis-\ntribution contexts, but the quality of these samples is generally\nlower.\n\nLanguage Models are Unsupervised Multitask Learners\nContext (WebText test)\nOutﬁt 1: Typical\nThis pairing was the ﬁrst outﬁt I thought of when I bought the shoes.\nIt’s like a summer version of this\nJake Grantham outﬁt; in fact, my shoes are close to the colors of his Nike Racers! Instead of a heavy Harris Tweed\njacket and denim shirt, I’m wearing a cotton DB jacket and and a linen shirt. Both fabrics (in these colors) are an\nabsolute must for summer, as they go with both dark and and light pants! As you can see, they pair wonderfully\nwith the dark jeans and shoes. It’s a pseudo menswear/prep outﬁt.\nOverall, this is a very casual outﬁt which is why I paired my sneakers with it.\nI’m not about wearing a\nfull wool suit with sneakers (as GQ shows a lot) but I’m deﬁnitely open to keeping things casual, like this cotton\nDB. Casual fabrics are key to pulling off your sneakers in a dressed down menswear outﬁt. I’d even suggest to\nwear these sneakers with a khaki chino suit or a white linen suit. Just be sure to ditch the tie or wear a tee or polo;\nwearing a tie with sneakers is a bit too much\nSmall model completion\n.\nOutﬁt 2: High End\nThis outﬁt was just in the eye of the storm.\nIt’s a\nhybrid of what ASICS does with streetwear (like J. Crew)\nand what J. Crew does with casual pieces that are a bit\nmore traditional. Again, as I mentioned before, it’s a low\nbudget item and they can be very expensive. These will\ndeﬁnitely be an issue if you’re trying to work as a casual\nsneakermaker, and that’s OK but don’t hesitate if you\nwant to try the lower price versions of their outﬁts!\nThis version features a very low cut-cut neckline\n(which is not a bad thing), slightly longer legs, and a pair\nof long legs to keep things nice. My preferred color of\ncolor these items are blue-gray (which is my ﬁrst colors,\nas they are a light white), or black (which is my favorite\nof my designs). As always, my pick of colors from this\nstore have nothing to do with that other than the de-\nsign. For a high end, I would recommend this to everyone!\nOutﬁt 3: High Light\nThis was a ”high light” look while I had a chance\nto get some color. I think the color is based on\nGPT-2 completion\nfor me!\nOutﬁt 2: Casual Jacket\nI know that some people might be opposed to wearing\nsneakers in a turtleneck, but I wanted to be true to myself,\nso I went with a slim ﬁtting turtleneck in a color more\nsimilar to my favorite color of the day. Overall, it’s a\nclassic turtleneck, with nothing too ﬂashy. I did swap my\nshoes for these ASICS Tiger Killshots and I wish I hadn’t\ndid that. The shoe itself is very comfortable, however,\nI found that wearing them in these sneakers made my\nfeet look a bit larger. I’d wear them with denim, chinos,\nwhatever, but would never recommend wearing them\nalone. There are just too many variables involved.\nOutﬁt 3: Dress\nThis is the ﬁnal outﬁt I wore the ASICS Tiger\nKillshots. This version of the ASICS is a much more\nmodern and streamlined version. The shoe has the most\nmodern and minimal design that I can think of! With a lot\nof black, these sneakers look very cool. They’ll deﬁnitely\nbe very comfortable for summer.\nTo me, wearing them with a suit is the easiest way\nto have a good pair of sneaker shoes. That’s pretty much\nit! If\nTable 7. Random unseen contexts (top), and non-cherry-picked completions from both the smallest (left) and largest (right) models.\nContexts are 768 tokens, with approximately 256 tokens worth of paragraphs shown. Completions are 256 tokens and fully shown. Top-k\nrandom sampling with k = 40 was used for generation.\n\nLanguage Models are Unsupervised Multitask Learners\nContext (WebText test)\nSome of the most glorious historical attractions in Spain date from the period of Muslim rule, including The\nMezquita, built as the Great Mosque of C´ordoba and the Medina Azahara, also in C´ordoba and now in ruins but\nstill visitable as such and built as the Madinat al-Zahra, the Palace of al-Andalus; and the Alhambra in Granada, a\nsplendid, intact palace. There are also two synagogues still standing that were built during the era of Muslim Spain:\nSanta Mar´ıa la Blanca in Toledo and the Synagogue of C´ordoba, in the Old City.\nReconquista and Imperial era [ edit ]\nSmall model completion\nThe era of Muslim Spain is very different from that of\nother parts of the world, with a rather extensive medieval\nhistory. While the Spanish Empire existed during the\nsecond half of the 9th century BC it was quite slowly\ntransformed to a small, secularized empire by 1150 A.D.,\nin a much more prosperous and diverse state. In 1050 CE\nthe emperor of Naples, at least partially in characterised by\na series of wars and invasions from the north-western coast\nto the western coast of the region (e.g., on the Danube\nRiver which extended all the way from Gibraltar into the\nsouth of Spain, but was quickly destroyed on the same\nday by the Roman army), assumed the title of Emperor\nof Naples. He made a large contribution over the last few\ndecades to restore and maintain the empire, the ﬁrst of\nwhich to the west. His successors were Ferdinand, who\ntook power from the Papal Throne in 1148, while the ﬁrst\nimperial successor was King Alfred II of Sweden. The\nempire was again made one of the few independent states\nin the European Continent and although no more than 200\nmillion people lived on its territory during this period the\npeople had much to say to the Emperor. From 1720 on the\nMuslims made up half the population of Spain.\nGPT-2 completion\nThe Reconquista (literally, ”The return to the country”)\nof 1092 and 1095 saw the establishment in the Iberian\npeninsula of a united kingdom of Castile and Aragon in\nwhich a Muslim dynasty would exist. This was the last\ntime in Europe where Muslim rule would be considered\n”native” and the last occasion where Christian-inﬂamed\nand organized conﬂicts would make themselves known in\nEurope.\nThe reign of Muslim Moors in Spain is often re-\nferred to,\nespecially among non-Muslims,\nas the\n”Conquest of Granada” and Spain is sometimes referred\nto as Spain in ”Granada”. The last years of Muslim rule\nwere followed by the Spanish Reconquista where the\nMuslims were driven or compelled out of the Iberian\nPeninsula with the victory of Ferdinand IX. in the Battle\nof Poitiers in March 1189 and the conquest of Jerusalem\nin 1187 by Saladin.\nThe Reconquista period in Spain came to an end\nwhen the Moors retreated north. However the Moorish\nrule would not be fully replaced by the Christian rule until\nthe Reconquista era resumed when the Reconquista lasted\nfrom 1229 to 1492.\nThe last Muslim state in Spain, Granada, was con-\nquered by the\nTable 8. Random unseen contexts (top), and non-cherry-picked completions from both the smallest (left) and largest (right) models.\nContexts are 768 tokens, with approximately 256 tokens worth of paragraphs shown. Completions are 256 tokens and fully shown. Top-k\nrandom sampling with k = 40 was used for generation.\n\nLanguage Models are Unsupervised Multitask Learners\nContext (WebText test)\nPinky\nThe pink ghost’s AI is designed to ”feel” opposite of the red ghost’s behavior. Pinky actually attempts to\nget out in front of Pac-Man. This is accomplished by setting the target 4 tiles ahead of Pac-Man’s current location in\nthe direction that Pac-Man is travelling. One exception to this is when Pac-Man is traveling up. Due to an overﬂow\nbug in the code, the calculation includes a left offset equal to the expected up offset.\nThe use of Pac-Man’s expected position allows the player to exploit the navigation code for Pinky. This was most\noften seen in situations where Pac-Man and Pinky are in close proximity to each other. For example, if they were\nheaded toward one another, once they are less than 4 tiles apart, the target tile is now behind Pinky. If he encounters\nan intersection while in this conﬁguration, he will attempt to double-back (by turning) in order to reach the target tile.\nPlayer’s would often perform what was referred to as a ”head fake” by changing directions rapidly just\nas Pinky was approaching an intersection. By displacing the target tile just as Pinky\nSmall model completion\nwas coming to the left, they would get the ”correct”\ndirection.\nIf the location is actually farther from what Pac-\nMan was at, the player’s will not respond to the ghost’s\nmovement, and the ghost will instead return to the player’s\ncurrent location. This is because Pac-Man does not move\nby turning.\nThe ghost will have to go to another location of\nsome kind to return where he began, and this is achieved\nin many ways. It will often simply leave the location, as\nfor example if Pac-Man did jump off a cliff, he would\nreturn to the spot where he was before vanishing into\nspace. There is a good number of possibilities these ghost\npaths may be possible, but the most obvious ones are if\nthe player wants to avoid getting caught up in the game,\nwith a goal as easy as keeping Pac-Man in the game for\n1/3 as long as he was.\nChasing\nIn Pac-Man’s case, this would be similar to how\nPac-Man can ”catch up” in a ﬁght.\nA ghost could chase through enemies.\nThis is use-\nful for many reasons, both for Pac-Man and his AI.\nUnlike most Ghost games, this ghost simply travels in the\ndirection from\nGPT-2 completion\nwas about to cross the intersection, Pac-Man would be\nable to dodge the ghosts’ projectiles and return to the safe\nlocation of the safe house.\nBuster\nThe yellow ghost’s AI is speciﬁcally designed to\ntry to avoid Pac-Man from leaving the safe house. The\nplayer must press the SPACE bar to reveal the safe\nhouse’s doors, then go into them to ﬁnd the yellow ghost\nhiding in the next room.\nThe player must also note that the ghost will not\nattempt to exit the safe house if it sees Pac-Man already\nthere. In fact, the ghost will not attempt an exit in the ﬁrst\nplace unless they’re already in the safe house.\nThese two behaviors are based on the fact that the\nplayer does not always attempt to reach the safe house\nbefore Pac-Man (or Blinky).\nThe player must also beware of this glitch if a\nghost (or another player) is already in the safe house\nand attempting to get out as soon as Pac-Man or Blinky\nis gone. At this point, the ghost will attempt to exit\nthe safe house in a similar manner as it is in ”Chase” mode.\nPac-Man\nThere are some ghosts that follow Pac-Man. These ghosts\nare\nTable 9. Random unseen contexts (top), and non-cherry-picked completions from both the smallest (left) and largest (right) models.\nContexts are 768 tokens, with approximately 256 tokens worth of paragraphs shown. Completions are 256 tokens and fully shown. Top-k\nrandom sampling with k = 40 was used for generation.\n\nLanguage Models are Unsupervised Multitask Learners\nContext (WebText test)\nCorporal Michael P. Goeldin was an unskilled laborer from Ireland when he enlisted in Company A in November\n1860. Goldein survived the war. Corporal Patrick O’Neal, also from Ireland, ﬁrst enlisted in 1854 and served with\nCompany L, 3d U.S. Artillery, in Oregon. He returned to the East Coast and enlisted in the company in 1860.\nO’Neal served until 1874, when he was named superintendent of the National Cemetery at Willets Point, New York.\nCorporal Benjamin Browne was a shoemaker from Orange County, New York. In August 1862, he enlisted in the\nnewly formed 124th New York Volunteers, and was one of sixty-one men who transferred into Company A that\nOctober. Browne reenlisted in the company in February 1864 while it was camped at Brandy Station. He returned\nto civilian life after completing his enlistment in 1867.\nOn 10 June, Artiﬁcer William Collins was promoted to corporal, probably to ﬁll a combat leadership\nvoid for the crossing of the James River. Collins’s service record does not reﬂect the qualities he demonstrated to\nearn this promotion, but he had obviously overcome some serious problems. Born in Sacketts Harbor, New York,\nCollins enlisted in the company in December 1853 at the age of twenty-two, and reenlisted in December 1858. Just\na month before the war began in April 1861, Collins went ”over the hill” and was not caught until three years later.\nReturned to the company on 22 March 1864, he was tried\nSmall model completion\non military charges and was discharged from the U.S.\nMilitary. He returned to the company, and was assigned\nto Division of Internal Affairs during the Civil War.\nAs an engineer, Collins also had training in engi-\nneering from his native New York. After graduate studies\nin the Union College of Engineering, he was employed\nin the United States Army for two years at the same\ntime. There he worked under Captain Daniel Pfeiffer\nand was promoted to colonel in May 1870, three years\nafter Captain Pfeiffer’s enlistment.\nCollins returned\nhome to England after completing his ﬁrst year in the\nFirst Division to serve the next two years in the Army\nCorps of Engineers, and by his second year, he began to\nexperience his service duty.¡—endoftext—¿MOSCOW,\nJuly 5 (Reuters) - U.K.-based Russian state media and\ngovernment sources and experts say Moscow has sought\nto downplay the ”military buildup” of the Syrian oppo-\nsition while maintaining support for President Donald\nTrump’s plan to build a wall along the U.S.-Mexico\nborder, calling it a ”military coup”.\nU.S.-led Syria war veteran and Syria envoy Michael\nFlynn (R), right, gestures with U.S.-based Syrian political\nanalyst Ahmet Koryat, with\nGPT-2 completion\nin June of that year. Sentenced to be shot for desertion, he\nremained on active duty.\nCaptain James J. Dall was born in Virginia in 1829 during\nthe Revolution and joined his father in the Army in 1836.\nHe served with the 2d Virginia Infantry until 1845, when\nhe went to the Western Reserve Regiment; and, on his\ndischarge, he reenlisted with Company I, 25th New York\nVolunteers (which in 1845 numbered about 4,500 men).\nOne of his brothers, James, served with his company\nin the same capacity in the same brigade. While the\nregiment remained in Connecticut through 1846, Captain\nDall served with Company I at various times before\nhis discharge. A veteran of the Mexican War, he had\nserved two years on the staff of Brigadier General John J.\nSullivan at San Antonio. During the Peninsula Campaign\nof 1863, he commanded a company of twenty-three and\nwas in charge of the battery of ﬁfty-four heavy mortars\nand the gunboat Pinta of the Battery, a gunboat and\nseveral other boats. Captain Dall was on active duty with\nCompany I in the fall of 1865. Two years later, he was\npromoted to second lieutenant.\nCaptain John A. O’\nTable 10. Random unseen contexts (top), and non-cherry-picked completions from both the smallest (left) and largest (right) models.\nContexts are 768 tokens, with approximately 256 tokens worth of paragraphs shown. Completions are 256 tokens and fully shown. Top-k\nrandom sampling with k = 40 was used for generation.\n\nLanguage Models are Unsupervised Multitask Learners\nContext (WebText test)\nXmonad, Xresources, and Fonts\nXresources can be an absolute pain (they were for me). So, this a bit of help:\nFirst of all, the way that fonts are generally handled in Xmonad is through XFT . Xft is the X FreeType\nInterface library and is a library that handles the actual rendering of fonts in Xmonad/urxvt (Xresources speciﬁes\na bunch of resources to things that launch under the X window manager. Settings for individual programs are\nspeciﬁed in Xresources like so: Urxvt.background:background settins here ).\nAnyway, fonts are set using this syntax in Xresources :\nurxvt*font:xft:your font name goes here:size=the size of the font goes here,xft:fall-back font 1 goes here\nSo, thats the most basic part of fonts in urxvt speciﬁed via Xresources.\nFixing /.ssh/ Permissions in Cygwin\nIn Cygwin on Windows I found that I could not set the permissions of my /.ssh/ folder to be 0600, as is\nrequired for ssh to allow you to use keys. The symptom I had was that I no matter what I did, it always modﬁed the\nowner and the group\nSmall model completion\nwas set to /.ssh/ , which was the way things were. This\ncould not be ﬁxed completely. I found I could make that\nmuch more workable with just typing the username with\nyour mouse then trying to set permissions to be ”0600”\nfor the group, which is what my system is now conﬁgured\nto do.\nHowever, as of Cygwin 7 on Linux I have found\nthat doing so is now ”unnecessary” for my system! I also\ninstalled a package called chroot. It does that by allowing\nyou to install/upgrade to your system directory.\nSo, as of Windows 8 and up, installing that can re-\nquire chroot, so I tried and tried and found that it wasn’t\nso bad when installing the package on Linux and then\nuninstalling it. As of my last write-up on Cygwin, the\nscript just does its job (the sudo commands do the rest), if\nyou try it you will have to log the change.\nIf that wasn’t confusing enough, this is what I tried and\njust went back to my /.ssh/ directory and the chroot script.\nI found I was ﬁnally running out of permissions (because\ni forgot to create a symlink to ssh from the ﬁle system and\nGPT-2 completion\n, making these ﬁles read-only. I tried just starting ssh\nand using a program like passwd which modiﬁes the\nowner/group to 0644 permissions, but it didn’t work. It\nwas annoying but not nearly as bad as it could be, just\nannoying to have to ﬁddle with permissions.\nSo I decided to do this as the reverse:\nFor each .ssh folder in your Cygwin directory, set\nthe permission of each ﬁle and directory to 2644. Then\nstart ssh with: ssh-add /.ssh/\nThe ﬁrst time you connect,\nyour permissions of\nthe\n/.ssh folder that you just had in Cygwin will be\nset to 0600. Then every time you connect again, your\npermissions will change to 0644.\nMakeﬁle for Xresources as used in /.xsessionrc\nThese are made in my .xsessionrc :\n.xproﬁle: .xsession-errors: color=””\nFor example:\n.xproﬁle: color=blue\nAs the colors are stored in a separate ﬁle, the color in your\n/.xsession-errors will look like cyan instead of blue.\nThis is mostly because, as of now, only the color\nis assigned to your\nTable 11. Random unseen contexts (top), and non-cherry-picked completions from both the smallest (left) and largest (right) models.\nContexts are 768 tokens, with approximately 256 tokens worth of paragraphs shown. Completions are 256 tokens and fully shown. Top-k\nrandom sampling with k = 40 was used for generation.\n\nLanguage Models are Unsupervised Multitask Learners\nContext (WebText test)\nAaron loves mint chocolate cake, but he requires that it be paired with mini chocolate chips, so I threw some of\nthose in between the layers. I also had a few Peppermint Jo Jos on hand so I crushed them up and threw some of\nthose in along with some crushed meringue cookies because, why not? It’s a total smorgasbord of minty chocolate\nchippy cookie crunchy goodness. I didn’t measure how much of each topping I used, but after I tasted the ﬁnished\nproduct, I wish I had added more. You can add anything you want- crushed candy canes, peppermint bark, etc. And\ndon’t be afraid to use a heavy hand. Texture = good.\nIf you don’t have 7-inch cake pans, you can get 3 shorter 8-inch layers out of this\nGPT-2 Completion 1\nrecipe. I made the longer version,\nthen used the pan for a layer of\nbuttercream frosting and stuck 1/3\nof it into the fridge for a frosting\ntopper. Just make sure you have all\nof this frosting prepared earlier than\nyou make this cake and you won’t\nhave to wait on it as your egg whites\nsit on the bottom of your mixing\nbowl.\nYou want the egg whites whipped\nto stiff peaks? Then put your cake\ndish over a pot of simmering water\nand heat it up until it’s just barely\nwarm. It doesn’t take more than 3-5\nminutes; just enough to get it started\nGPT-2 Completion 2\nrecipe. If you do, and you know\nhow to make that cake, then your\nlife will never be the same.\nI have two very simple rules\nwhen it comes to cookie crumbs\nor chocolate chips. 1) When I buy\nraw sugar, I do not use it until it’s\ncooled down and has at least half a\nteaspoon of sugar on it. That’s my\nrule. And 2) If I have a recipe that\ncalls for something and I want to\nuse it in a recipe, I do not substitute\nit with something else on-line. Like\nthis.\nI\nfound\nthis\nreally\ncute\ncard\nwith cute little kittens on\nGPT-2 Completion 3\nrecipe, which will likely yield about\nthe same amount of cake.\nChocolate\nButtermilk\nCake\nPrint Chunks of chocolate cake,\ncoated in buttermilk and topped\nwith crunchy chopped chocolate,\ndrizzled\nwith\nchocolate\nsauce.\nSimple but a nice change from\nchocolate cake! Author: Momma\nMade Ingredients For the chocolate\nbuttermilk cake 1 1/2 cup unsalted\nbutter , room temperature\n1 1/2 cup granulated sugar\n3 large eggs plus 1 egg yolk For\nthe chocolate glaze 1/3 cup cocoa\npowder\n1 3/4 cups powdered sugar\n6 ounces semis\nGPT-2 Completion 4\nrecipe.\nMeringue Cookies\n1 3/4 cup butter, softened\n1 cup sugar\n1 egg yolk\n3 T. heavy cream\n3 1/2-4 cups ﬂour\npinch of salt\nPeppermint-Jojo Topping\n1 1/4 cups powdered sugar\n1 cup chopped pecans\n1/2 cup ﬁnely chopped mint leaves\n1/2 cup chopped fresh mint (about\n1/2 sheet)\n1 tsp. peppermint extract\n1/4 tsp. sea salt\npinch of sea\nGPT-2 Completion 5\nrecipe.\nMy 2 failures at Christmas morning\nwere not due to poor frosting skills.\nThat was another success story.\nThe cake was beautifully moist,\nand well, just a beautiful cake. It’s\ngot very little air in it or moisture,\nbut all the chocolate and mint was\nperfectly blended together.\nOne thing that made me cringe a\nbit was that all those ingredients-\negg yolks, sugar, melted butter,\nchocolate chips, melted chocolate,\nand mint- had to be mixed right\ninto the cake batter,\nwhich is\nsuper time-consuming.\nA friend\nsuggested that if I put the batter in a\nseparate bowl, I\nGPT-2 Completion 6\nrecipe, which will give you about\n6-9 servings of delicious chocolate\ncake.\nTo prevent your layers\nfrom sticking together, I used my\n8-inch round tart pan, lined it with\nparchment paper, then cut up 4\n7-inch round cakes from it and\nsandwiched the layers between\nthem (this also worked well for\nthe 1-layer chocolate cake as well).\nWhen you’re ready to bake this\ncake, remove the parchment paper,\npreheat oven to 350 degrees and\nbake for about 30 – 40 minutes,\nuntil the top is golden and a tester\ncomes out clean.\nChocolate\nButtermilk\nLayer\nCake with Peppermint Jo\nTable 12. Non-cherry-picked completions from GPT-2 generated from the same context (from WebText test). Context is 384 tokens\n(shown truncated), and generations are 128 tokens. Top-k random sampling with k = 40 was used for generation.\n\nLanguage Models are Unsupervised Multitask Learners\nContext (human-written): In a shocking ﬁnding, scientist discovered a herd of unicorns living\nin a remote, previously unexplored valley, in the Andes Mountains. Even more surprising to the\nresearchers was the fact that the unicorns spoke perfect English.\nGPT-2: The scientist named the population, after their distinctive horn, Ovid’s Unicorn. These\nfour-horned, silver-white unicorns were previously unknown to science.\nNow, after almost two centuries, the mystery of what sparked this odd phenomenon is ﬁ-\nnally solved.\nDr.\nJorge P´erez, an evolutionary biologist from the University of La Paz, and several com-\npanions, were exploring the Andes Mountains when they found a small valley, with no other animals\nor humans. P´erez noticed that the valley had what appeared to be a natural fountain, surrounded by\ntwo peaks of rock and silver snow.\nP´erez and the others then ventured further into the valley.\n“By the time we reached the top\nof one peak, the water looked blue, with some crystals on top,” said P´erez.\nP´erez and his friends were astonished to see the unicorn herd.\nThese creatures could be\nseen from the air without having to move too much to see them – they were so close they could touch\ntheir horns.\nWhile examining these bizarre creatures the scientists discovered that the creatures also\nspoke some fairly regular English. P´erez stated, “We can see, for example, that they have a common\n‘language,’ something like a dialect or dialectic.”\nDr.\nP´erez believes that the unicorns may have originated in Argentina, where the animals\nwere believed to be descendants of a lost race of people who lived there before the arrival of humans\nin those parts of South America.\nWhile their origins are still unclear, some believe that perhaps the creatures were created\nwhen a human and a unicorn met each other in a time before human civilization. According to P´erez,\n“In South America, such incidents seem to be quite common.”\nHowever, P´erez also pointed out that it is likely that the only way of knowing for sure if\nunicorns are indeed the descendants of a lost alien race is through DNA. “But they seem to be able to\ncommunicate in English quite well, which I believe is a sign of evolution, or at least a change in\nsocial organization,” said the scientist.\nTable 13. Conditional generation on an out-of-distribution context by GPT-2. Cherry pick of 10 samples generated with k = 40.\n\nLanguage Models are Unsupervised Multitask Learners\nArticle: Prehistoric man sketched an incredible array of prehistoric beasts on the rough limestone walls of a cave in\nmodern day France 36,000 years ago.\nNow, with the help of cutting-edge technology, those works of art in the Chauvet-Pont-d’Arc Cave have been\nreproduced to create the biggest replica cave in the world.\nThe manmade cavern named the Caverne du Pont-d’Arc has been built a few miles from the original site in\nVallon-Pont-D’arc in Southern France and contains 1,000 painstakingly-reproduced drawings as well as around 450\nbones and other features...\nCavemen and women sketched an incredible array of prehistoric beasts on the rough limestone walls of a cave\n36,000 years ago and now a replica has been created (pictured)\n...\nGPT-2: The original site in Vallon-Pont-D’arc in South-\nern France is a Unesco World Heritage site and is the\noldest known and the best preserved cave decorated by\nman. The replica cave was built a few miles from the orig-\ninal site in Vallon-Pont-D’Arc in Southern France. The\ncave contains images of 14 different species of animals\nincluding woolly rhinoceros, mammoths, and big cats.\nReference: Cave mimics famous Caverne du Pont-d’Arc\nin France, the oldest cave decorated by man and the\nbest preserved.\nThe replica contains all 1,000 paint-\nings which include 425 such as a woolly rhinoceros and\nmammoths. Minute details were copied using 3D mod-\nelling and anamorphic techniques, often used to shoot\nwidescreen images. The modern cave also includes replica\npaw prints of bears, bones and details preserved in the\noriginal cave.\nArticle: ‘If you can ﬁnd my submarine, it’s yours,’ Russian oil billionaire Roman Abramovich once said.\nAnd it seems the oligarch and Chelsea FC owner, whose submarine is just one of the extras that came with his\n£300million superyacht Eclipse (perfect for getting to shore undetected), is not the only wealthy businessman\nsplashing out on underwater exploration.\nDubbed Earth’s real ‘ﬁnal frontier’, the oceans are still so little-explored that billionaires are queuing up to buy\nvessels that give them a glimpse of the dramatic seascapes and incredible wildlife of the world’s oceans.\nSo if you have a spare few million in the bank and want some holiday snaps and Instagram posts that will really\ntrump everyone else, you may want to snap up one of these...\nWhale of a time: The OrcaSub takes you 2000 feet beneath the surface in two pressurised Perspex viewing domes\nfor optimum exploration\n...\nGPT-2: The U-Boat Worx submarine can dive to 984 feet\n(300 metres) and is capable of diving to the bottom of\nthe ocean. It can be used for diving, snorkelling and even\nsnorkel diving. The submarine is also equipped with a\nsnorkelling platform.\nReference: Feel like a James Bond villain with the most\nadvanced submerged ocean vehicles. Some convert from\na yacht into a submarine, and others detached off for a\nspeedy exploration. Oliver’s Travels offers a Mile Low\npackage on its submarine, complete with chef and butler\nfor £175,000 a night.\nArticle: Amina Ali Qassim is sitting with her youngest grandchild on her lap, wiping away tears with her headscarf.\nOnly a few months old, this is the baby girl whose ears she desperately tried to cover the night the aerial bombardment\nstarted. She lay awake, she says, in a village mosque on the Yemeni island of Birim, counting explosions as the\nbaby cried.\nIt could have been worse though. They could have still been in their house when the ﬁrst missile landed.\n”Our neighbor shouted to my husband ’you have to leave, they’re coming.’ And we just ran. As soon as we left the\nhouse, the ﬁrst missile fell right by it and then a second on it. It burned everything to the ground,” Qassim tells us\n...\nGPT-2: Yemen is in the middle of a civil war. Saudi\nArabia is leading the coalition bombing campaign. It’s\nbeen bombing Yemen for more than two months now.\nReference: Amina Ali Qassim’s family sought shelter in\na mosque before ﬂeeing Yemen. Thousands like them are\nboarding boats to sail to Djibouti. Saudi Arabia has been\npounding Yemen in a bid to defeat Houthi rebels.\nTable 14. Summaries generated by GPT-2 and reference summaries on the test set of the CNN and Daily Mail dataset.\n\nLanguage Models are Unsupervised Multitask Learners\nEnglish reference\nGPT-2 French translation\nThis re-release, titled The Next Day Extra, was presented\nin the form of three disks: the original album, unpublished\nstudio sessions and remixes, plus a DVD containing the\nfour clips that have already been unveiled.\nLes nouvelles re-releases, tout en premier disc, n´ecessaire\nde l’album, un studio session et remixes, plus une DVD\nde l’´ecran de quelques clips qui ont ´et´e d´ej`a ´echapp´es.\nFrench reference\nGPT-2 English translation\nUn homme expliquait que le fonctionnement de la hernia\nfonctionnelle qu’il avait reconnaˆıt avant de faire, le fonc-\ntionnement de la hernia fonctionnelle que j’ai r´eussi, j’ai\nr´eussi.\nThis release, originally released on three discs: the origi-\nnal album, sessions studio in´edites and remixes, a DVD\ncontaining the four clips that were released on the previous\nday.\nEnglish reference\nGPT-2 French translation\nOne man explained that the free hernia surgery he’d re-\nceived will allow him to work again.\nUn homme expliquait que le fonctionnement de la hernia\nfonctionnelle qu’il avait reconnaˆıt avant de faire, le fonc-\ntionnement de la hernia fonctionnelle que j’ai r´eussi, j’ai\nr´eussi.\nFrench reference\nGPT-2 English translation\nUn homme a expliqu´e que l’op´eration gratuite qu’il avait\nsubie pour soigner une hernie lui permettrait de travailler\n`a nouveau.\nA man told me that the operation gratuity he had been\npromised would not allow him to travel.\nEnglish reference\nGPT-2 French translation\nSpeaking to an open government conference in London\nvia video link, Mr Kerry said: ”There is no question that\nthe President and I and others in government have actually\nlearned of some things that had been happening on an\nautomatic pilot because the ability has been there, going\nback to World War Two and to the very difﬁcult years of\nthe Cold War, and then, of course, 9/11.”\nLe premier conference de l’´edition internationale en ligne\nsur la vide, le ministre de l’´Edition internationale a ´et´e\nvide, lui dit : ”Il n’y a qu’une question que le pr´esident\net moi et tous les autres personnes d’une administration\nqui a ´et´e ´evident par la fois, par la part de la guerre, par la\npart de la guer\nFrench reference\nGPT-2 English translation\nS’exprimant lors d’une conf´erence intergouvernementale\n`a Londres par liaison vid´eo, M. Kerry a d´eclar´e: ”Il est\nind´eniable que le Pr´esident, moi-mˆeme et d’autres mem-\nbres du gouvernement avons pris connaissance de cer-\ntaines choses en mode pilote automatique parce que nous\nen avions la possibilit´e, d`es la Seconde guerre mondiale et\njusqu’aux ann´ees difﬁciles de la Guerre froide, puis bien\nsˆur le 11 septembre.”\nIn a conf´erence between the United States and London,\nSecretary of State John Kerry said: ”It is indeniable that\nthe President, myself and others of the government have\nbeen aware of certain certain choices that have been made\nin the past in order to be able to do certain things in a more\nautomated way.”\nTable 15. English to French and French to English translations generated by GPT-2.\n\nLanguage Models are Unsupervised Multitask Learners\nContext (passage and previous question/answer pairs)\nThe 2008 Summer Olympics torch relay was run from March 24 until August 8, 2008, prior to the 2008 Summer\nOlympics, with the theme of “one world, one dream”. Plans for the relay were announced on April 26, 2007, in\nBeijing, China. The relay, also called by the organizers as the “Journey of Harmony”, lasted 129 days and carried\nthe torch 137,000 km (85,000 mi) – the longest distance of any Olympic torch relay since the tradition was started\nahead of the 1936 Summer Olympics.\nAfter being lit at the birthplace of the Olympic Games in Olympia, Greece on March 24, the torch trav-\neled to the Panathinaiko Stadium in Athens, and then to Beijing, arriving on March 31. From Beijing, the torch was\nfollowing a route passing through six continents. The torch has visited cities along the Silk Road, symbolizing\nancient links between China and the rest of the world. The relay also included an ascent with the ﬂame to the top of\nMount Everest on the border of Nepal and Tibet, China from the Chinese side, which was closed specially for the\nevent.\nQ: What was the theme\nA: “one world, one dream”.\nQ: What was the length of the race?\nA: 137,000 km\nQ: Was it larger than previous ones?\nA: No\nQ: Where did the race begin?\nA: Olympia, Greece\nQ: Is there anything notable about that place?\nA: birthplace of Olympic Games\nQ: Where did they go after?\nA: Athens\nQ: How many days was the race?\nA: seven\nQ: Did they visit any notable landmarks?\nA: Panathinaiko Stadium\nQ: And did they climb any mountains?\nA:\nModel answer: Everest\nTurker answers: unknown, yes, Yes, yes\nTable 16. Selected CoQA completion.\n\nLanguage Models are Unsupervised Multitask Learners\nContext (passage and previous question/answer pairs)\nTom goes everywhere with Catherine Green, a 54-year-old secretary. He moves around her ofﬁce at work and goes\nshopping with her. ”Most people don’t seem to mind Tom,” says Catherine, who thinks he is wonderful. ”He’s my\nfourth child,” she says. She may think of him and treat him that way as her son. He moves around buying his food,\npaying his health bills and his taxes, but in fact Tom is a dog.\nCatherine and Tom live in Sweden, a country where everyone is expected to lead an orderly life accord-\ning to rules laid down by the government, which also provides a high level of care for its people. This level of care\ncosts money.\nPeople in Sweden pay taxes on everything, so aren’t surprised to ﬁnd that owning a dog means more\ntaxes. Some people are paying as much as 500 Swedish kronor in taxes a year for the right to keep their dog, which\nis spent by the government on dog hospitals and sometimes medical treatment for a dog that falls ill. However, most\nsuch treatment is expensive, so owners often decide to offer health and even life\nfor their dog.\nIn Sweden dog owners must pay for any damage their dog does.\nA Swedish Kennel Club ofﬁcial ex-\nplains what this means: if your dog runs out on the road and gets hit by a passing car, you, as the owner, have to pay\nfor any damage done to the car, even if your dog has been killed in the accident.\nQ: How old is Catherine?\nA: 54\nQ: where does she live?\nA:\nModel answer: Stockholm\nTurker answers: Sweden, Sweden, in Sweden, Sweden\nTable 17. Selected CoQA completion.\n",
      "fetch_method": "direct-pdf"
    },
    {
      "arxiv_id": "https://r.jina.ai/https://openai.com/research/better-language-models/",
      "full_text": "Title: Better language models and their implications\n\nURL Source: https://openai.com/research/better-language-models/\n\nMarkdown Content:\nBetter language models and their implications | OpenAI\n\nListen to article\n\nWe’ve trained a large-scale unsupervised language model which generates coherent paragraphs of text, achieves state-of-the-art performance on many language modeling benchmarks, and performs rudimentary reading comprehension, machine translation, question answering, and summarization—all without task-specific training.\n\nOur model, called GPT‑2 (a successor to[GPT⁠](https://openai.com/index/language-unsupervised/)), was trained simply to predict the next word in 40GB of Internet text. Due to our concerns about malicious applications of the technology, we are not releasing the trained model. As an experiment in responsible disclosure, we are instead releasing a much[smaller model⁠(opens in a new window)](https://github.com/openai/gpt-2)for researchers to experiment with, as well as a[technical paper⁠(opens in a new window)](https://cdn.openai.com/better-language-models/language_models_are_unsupervised_multitask_learners.pdf).\n\nGPT‑2 is a large[transformer⁠(opens in a new window)](https://arxiv.org/abs/1706.03762)-based language model with 1.5 billion parameters, trained on a dataset[A](https://openai.com/research/better-language-models/#citation-bottom-A) of 8 million web pages. GPT‑2 is trained with a simple objective: predict the next word, given all of the previous words within some text. The diversity of the dataset causes this simple goal to contain naturally occurring demonstrations of many tasks across diverse domains. GPT‑2 is a direct scale-up of GPT, with more than 10X the parameters and trained on more than 10X the amount of data.\n\nGPT‑2 displays a broad set of capabilities, including the ability to generate conditional synthetic text samples of unprecedented quality, where we prime the model with an input and have it generate a lengthy continuation. In addition, GPT‑2 outperforms other language models trained on specific domains (like Wikipedia, news, or books) without needing to use these domain-specific training datasets. On language tasks like question answering, reading comprehension, summarization, and translation, GPT‑2 begins to learn these tasks from the raw text, using no task-specific training data. While scores on these downstream tasks are far from state-of-the-art, they suggest that the tasks can benefit from unsupervised techniques, given sufficient (unlabeled) data and compute.\n\nSamples\n-------\n\nGPT‑2 generates synthetic text samples in response to the model being primed with an arbitrary input. The model is chameleon-like—it adapts to the style and content of the conditioning text. This allows the user to generate realistic and coherent continuations about a topic of their choosing, as seen by the following select samples.[B](https://openai.com/research/better-language-models/#citation-bottom-B)\n\n#### System Prompt (human-written)\n\n_In a shocking finding, scientist discovered a herd of unicorns living in a remote, previously unexplored valley, in the Andes Mountains. Even more surprising to the researchers was the fact that the unicorns spoke perfect English._\n\n#### Model Completion (machine-written, 10 tries)\n\nThe scientist named the population, after their distinctive horn, Ovid’s Unicorn. These four-horned, silver-white unicorns were previously unknown to science.\nNow, after almost two centuries, the mystery of what sparked this odd phenomenon is finally solved.\n\nDr. Jorge Pérez, an evolutionary biologist from the University of La Paz, and several companions, were exploring the Andes Mountains when they found a small valley, with no other animals or humans. Pérez noticed that the valley had what appeared to be a natural fountain, surrounded by two peaks of rock and silver snow.\n\nPérez and the others then ventured further into the valley. “By the time we reached the top of one peak, the water looked blue, with some crystals on top,” said Pérez.\n\nPérez and his friends were astonished to see the unicorn herd. These creatures could be seen from the air without having to move too much to see them – they were so close they could touch their horns.\n\nWhile examining these bizarre creatures the scientists discovered that the creatures also spoke some fairly regular English. Pérez stated, “We can see, for example, that they have a common ‘language,’ something like a dialect or dialectic.”\n\nDr. Pérez believes that the unicorns may have originated in Argentina, where the animals were believed to be descendants of a lost race of people who lived there before the arrival of humans in those parts of South America.\n\nWhile their origins are still unclear, some believe that perhaps the creatures were created when a human and a unicorn met each other in a time before human civilization. According to Pérez, “In South America, such incidents seem to be quite common.”\n\nHowever, Pérez also pointed out that it is likely that the only way of knowing for sure if unicorns are indeed the descendants of a lost alien race is through DNA. “But they seem to be able to communicate in English quite well, which I believe is a sign of evolution, or at least a change in social organization,” said the scientist.\n\nAs the above samples show, our model is capable of generating samples from a variety of prompts that feel close to human quality and show coherence over a page or more of text. Nevertheless, we have observed various failure modes, such as repetitive text, world modeling failures (e.g., the model sometimes writes about _fires happening under water_), and unnatural topic switching. Exploring these types of weaknesses of language models is an[active⁠(opens in a new window)](https://arxiv.org/abs/1601.04811)[area⁠(opens in a new window)](https://arxiv.org/abs/1510.03055)of[research⁠(opens in a new window)](https://arxiv.org/abs/1701.03185)in the natural language processing community.\n\nOverall, we find that it takes a few tries to get a good sample, with the number of tries depending on how familiar the model is with the context. When prompted with topics that are highly represented in the data (Brexit, Miley Cyrus, Lord of the Rings, and so on), it seems to be capable of generating reasonable samples about 50% of the time. The opposite is also true: on highly technical or esoteric types of content, the model can perform poorly. Fine-tuning offers the potential for even more detailed control over generated samples—for example, we can fine-tune GPT‑2 on the Amazon Reviews dataset and use this to let us write reviews conditioned on things like star rating and category.\n\nThese samples have substantial policy implications: large language models are becoming increasingly easy to steer towards scalable, customized, coherent text generation, which in turn could be used in a number of beneficial as well as malicious ways. We’ll discuss these implications below in more detail, and outline a publication experiment we are taking in light of such considerations.\n\nZero-shot\n---------\n\nGPT‑2 achieves state-of-the-art scores on a variety of domain-specific language modeling tasks. Our model is not trained on any of the data specific to any of these tasks and is only evaluated on them as a final test; this is known as the “zero-shot” setting. GPT‑2 outperforms models trained on domain-specific datasets (e.g. Wikipedia, news, books) when evaluated on those same datasets. The following table shows all our state-of-the-art zero-shot results.\n\n_(+) means a higher score is better for this domain. (–) means a lower score is better._\n\n**Dataset****Metric****Our result****Previous record****Human**\nWinograd Schema Challenge accuracy (+)70.70%63.7%[[secondary]]92%+[[/secondary]]\nLAMBADA accuracy (+)63.24%59.23%[[secondary]]95%+[[/secondary]]\nLAMBADA perplexity (–)8.6 99[[secondary]]~1–2[[/secondary]]\nChildren’s Book Test Common Nouns (validation accuracy)accuracy (+)93.30%85.7%[[secondary]]96%[[/secondary]]\nChildren’s Book Test Named Entities (validation accuracy)accuracy (+)89.05%82.3%[[secondary]]92%[[/secondary]]\nPenn Tree Bank perplexity (–)35.76 46.54[[secondary]]unknown[[/secondary]]\nWikiText-2 perplexity (–)18.34 39.14[[secondary]]unknown[[/secondary]]\nenwik8 bits per character (–)0.93 0.99[[secondary]]unknown[[/secondary]]\ntext8 bits per character (–)0.98 1.08[[secondary]]unknown[[/secondary]]\nWikiText-103 perplexity (–)17.48 18.3[[secondary]]unknown[[/secondary]]\n\nGPT‑2 achieves state-of-the-art on Winograd Schema, LAMBADA, and other language modeling tasks.\n\nOn other language tasks like question answering, reading comprehension, summarization, and translation, we are able to get surprising results without any fine-tuning of our models, simply by prompting the trained model in the right way (see below for examples of how we do this), though we do still fall short of state-of-the-art for specialized systems.\n\n#### Task\n\n**Reading Comprehension:** answer questions about given passages\n\n#### Dataset\n\nCOQA\n\n#### Example\n\n_The 2008 Summer Olympics torch relay was run from March 24 until August 8, 2008, prior to the 2008 Summer Olympics, with the theme of “one world, one dream”. Plans for the relay were announced on April 26, 2007, in Beijing, China. The relay, also called by the organizers as the “Journey of Harmony”, lasted 129 days and carried the torch 137,000 km (85,000 mi) – the longest distance of any Olympic torch relay since the tradition was started ahead of the 1936 Summer Olympics.\nAfter being lit at the birthplace of the Olympic Games in Olympia, Greece on March 24, the torch traveled to the Panathinaiko Stadium in Athens, and then to Beijing, arriving on March 31. From Beijing, the torch was following a route passing through six continents. The torch has visited cities along the Silk Road, symbolizing ancient links between China and the rest of the world. The relay also included an ascent with the flame to the top of Mount Everest on the border of Nepal and Tibet, China from the Chinese side, which was closed specially for the event.\n\nQ: What was the theme?\n\nA: “one world, one dream”.\n\nQ: What was the length of the race?\n\nA: 137,000 km Q: Was it larger than previous ones?\n\nA: No Q: Where did the race begin?\n\nA: Olympia, Greece Q: Is there anything notable about that place?\n\nA: birthplace of Olympic Games Q: Where did they go after?\n\nA: Athens Q: How many days was the race?\n\nA: seven Q: Did they visit any notable landmarks?\n\nA: Panathinaiko Stadium Q: And did they climb any mountains?\n\nA:_\n\n**Target answers:** unknown or yes\n\n**Model answer:** Everest\n\n#### Performance\n\n![Image 1](https://cdn.openai.com/better-language-models/reading-comprehension-1.svg)\n\nWe hypothesize that since these tasks are a subset of general language modeling, we can expect performance to increase further with more compute and data. Others have published[similar hypotheses⁠(opens in a new window)](https://arxiv.org/abs/1901.11373). We also expect fine-tuning to help performance on downstream tasks, though we have yet to do thorough experiments.\n\nPolicy implications\n-------------------\n\nLarge, general language models could have significant societal impacts, and also have many near-term applications. We can anticipate how systems like GPT‑2 could be used to create:\n\n*   AI writing assistants\n*   More capable dialogue agents\n*   Unsupervised translation between languages\n*   Better speech recognition systems\n\nWe can also imagine the application of these models for[malicious purposes⁠](https://openai.com/index/preparing-for-malicious-uses-of-ai/), including the following (or other applications we can’t yet anticipate):\n\n*   Generate misleading news articles\n*   Impersonate others online\n*   Automate the production of abusive or faked content to post on social media\n*   Automate the production of spam/phishing content\n\nThese findings, combined with earlier results on synthetic imagery, audio, and video, imply that technologies are reducing the cost of generating fake content and waging disinformation campaigns. The public at large will need to become more skeptical of text they find online, just as the “[deep fakes⁠(opens in a new window)](https://papers.ssrn.com/sol3/papers.cfm?abstract_id=3213954)” phenomenon calls for more skepticism about images.[C](https://openai.com/research/better-language-models/#citation-bottom-C)\n\nToday, malicious actors—some of which are political in nature—have already begun to target the shared online commons, using [things like⁠(opens in a new window)](https://www.washingtonpost.com/amphtml/opinions/global-opinions/the-new-censors-wont-delete-your-words--theyll-drown-them-out/2019/02/08/c8a926a2-2b27-11e9-984d-9b8fba003e81_story.html)“robotic tools, fake accounts and dedicated teams to troll individuals with hateful commentary or smears that make them afraid to speak, or difficult to be heard or believed.” We should consider how research into the generation of synthetic images, videos, audio, and text may further combine to unlock new as-yet-unanticipated capabilities for these actors, and should seek to create better technical and non-technical countermeasures. Furthermore, the underlying technical innovations inherent to these systems are core to fundamental artificial intelligence research, so it is not possible to control research in these domains without slowing down the progress of AI as a whole.\n\nRelease strategy\n----------------\n\nDue to concerns about large language models being used to generate deceptive, biased, or abusive language at scale, we are only releasing a[much smaller version of GPT‑2 along with sampling code⁠(opens in a new window)](https://github.com/openai/gpt-2/). We are not releasing the dataset, training code, or GPT‑2 model weights. Nearly a year ago we wrote in the[OpenAI Charter⁠](https://openai.com/charter/): “we expect that safety and security concerns will reduce our traditional publishing in the future, while increasing the importance of sharing safety, policy, and standards research,” and we see this current work as potentially representing the early beginnings of such concerns, which we expect may grow over time. This decision, as well as our discussion of it, is an experiment: while we are not sure that it is the right decision today, we believe that the AI community will eventually need to tackle the issue of publication norms in a thoughtful way in certain research areas. Other disciplines such as biotechnology and cybersecurity have long had active debates about responsible publication in cases with clear misuse potential, and we hope that our experiment will serve as a case study for more nuanced discussions of model and code release decisions in the AI community.\n\nWe are aware that some researchers have the technical capacity to reproduce and open source our results. We believe our release strategy limits the initial set of organizations who may choose to do this, and gives the AI community more time to have a discussion about the implications of such systems.\n\nWe also think governments should consider expanding or commencing initiatives to more systematically monitor the societal impact and diffusion of AI technologies, and to measure the progression in the capabilities of such systems. If pursued, these efforts could yield a better evidence base for decisions by AI labs and governments regarding publication decisions and AI policy more broadly.\n\nWe will further publicly discuss this strategy in six months. If you’d like to discuss large language models and their implications, please email us at:[languagequestions@openai.com⁠](mailto:languagequestions@openai.com). And if you’re excited about working on cutting-edge language models (and thinking through their policy implications),[we’re hiring⁠](https://openai.com/careers/).\n\n_GPT‑2 Interim Update, May 2019_\n\nWe’re implementing two mechanisms to responsibly publish GPT‑2 and hopefully future releases: staged release and partnership-based sharing. We’re now[releasing⁠(opens in a new window)](https://github.com/openai/gpt-2)a larger 345M version of GPT‑2 as a next step in staged release, and are sharing the 762M and 1.5B versions with partners in the AI and security communities who are working to improve societal preparedness for large language models.\n\nStaged release\n--------------\n\nStaged release involves the gradual release of a family of models over time. The purpose of our staged release of GPT‑2 is to give people time to assess the properties of these models, discuss their societal implications, and evaluate the impacts of release after each stage.\n\nAs the next step in our staged release strategy, we are releasing the 345M parameter version of GPT‑2. This model features improved performance relative to the 117M version, though falls short of the 1.5B version with respect to the ease of generating coherent text. We have been excited to see so many positive uses of GPT‑2-117M, and hope that 345M will yield still more benefits.\n\nWhile the misuse risk of 345M is higher than that of 117M, we believe it is substantially lower than that of 1.5B, and we believe that training systems of similar capability to GPT‑2-345M is well within the reach of many actors already; this evolving replication landscape has informed our decision-making about what is appropriate to release.\n\nIn making our 345M release decision, some of the factors we considered include: the ease of use (by various users) of different model sizes for generating coherent text, the role of humans in the text generation process, the likelihood and timing of future replication and publication by others, evidence of use in the wild and expert-informed inferences about unobservable uses, proofs of concept such as the review generator mentioned in the original blog post, the strength of demand for the models for beneficial purposes, and the input of stakeholders and experts. We remain uncertain about some of these variables and continue to welcome input on how to make appropriate language model publication decisions.\n\nWe hope that ongoing research on bias, detection, and misuse will give us the confidence to publish larger models in a timely manner, and at the six month mark we will share a fuller analysis of language models’ societal implications and our heuristics for release decisions.\n\nOutput dataset\n--------------\n\nWe’re[releasing⁠(opens in a new window)](https://github.com/openai/gpt-2-output-dataset)a dataset of GPT‑2 outputs from all 4 model sizes, with and without top-k truncation, as well as a subset of the WebText corpus used to train GPT‑2. The output dataset features approximately 250,000 samples per model/hyperparameter pair, which we expect is sufficient to help a wider range of researchers perform quantitative and qualitative analysis on the three topics above. Alongside these datasets, we are including a baseline analysis of some detection-related properties of the models, which we hope others will be able to quickly build on.\n\nTalk to us\n----------\n\nWe are interested in collaborating with researchers working on language model output detection, bias, and publication norms, and with organizations potentially affected by large language models: please reach out via our[Google Form⁠(opens in a new window)](https://forms.gle/A7WBSbTY2EkKdroPA). Additionally, OpenAI’s language, safety, and policy teams will be at[ICLR⁠(opens in a new window)](https://iclr.cc/)next week, including at the Reproducibility workshop and the OpenAI booth. In particular, we will be discussing this release strategy at the[AI for Social Good⁠(opens in a new window)](https://aiforsocialgood.github.io/iclr2019/schedule.htm)workshop.\n\n*   [GPT](https://openai.com/research/index/?tags=gpt)\n*   [Language](https://openai.com/research/index/?tags=language)\n*   [Generative Models](https://openai.com/research/index/?tags=generative-models)\n*   [Ethics & Safety](https://openai.com/research/index/?tags=ethics-safety)\n*   [Transformers](https://openai.com/research/index/?tags=transformers)\n\nFootnotes\n---------\n\n1.   A\nWe created a new dataset which emphasizes diversity of content, by scraping content from the Internet. In order to preserve document quality, we used only pages which have been curated/filtered by humans—specifically, we used outbound links from Reddit which received at least 3 karma. This can be thought of as a heuristic indicator for whether other users found the link interesting (whether educational or funny), leading to higher data quality than other similar datasets, such as CommonCrawl.\n\n2.   B\nNote that while we have hand-chosen these samples, and are thus engaging in some meta-cherry-picking, we believe they are not too unrepresentative of the sampling process. We are simply using top-k truncated sampling, and have yet to explore more advanced methods of sampling (such as beam-search methods).\n\n3.   C\nPoliticians may want to consider introducing penalties for the misuse of such systems, as some have proposed for deep fakes.\n\nAcknowledgments\n---------------\n\nThanks to David Luan and Rewon Child for their work on GPT-2.\n\nWe also thank the following for feedback on drafts of this post: Greg Brockman, Kai-Fu Lee, Tasha McCauley, Jeffrey Ding, Brian Tse, Allan Dafoe, Rebecca Crootof, Sam Bowman, Ryan Calo, Nick Cammarata and John Schulman.\n\nEditor: Ashley Pilipiszyn\n\nDesign: Justin Jay Wang\n\nCover artwork: Ben Barry\n",
      "fetch_method": "jina-reader"
    },
    {
      "arxiv_id": "https://r.jina.ai/https://openai.com/blog/gpt-2-6-month-follow-up/",
      "full_text": "Title: GPT-2: 6-month follow-up\n\nURL Source: https://openai.com/blog/gpt-2-6-month-follow-up/\n\nMarkdown Content:\nGPT-2: 6-month follow-up | OpenAI\n\n===============\n\n[Skip to main content](https://openai.com/blog/gpt-2-6-month-follow-up/#main)\n\nLog in\n\n[](https://openai.com/)\n\nSwitch to\n\n*   [ChatGPT(opens in a new window)](https://chatgpt.com/?openaicom-did=c9c54dfe-7a01-47fc-991f-394b07cdb30e&openaicom_referred=true)\n*   [Sora(opens in a new window)](https://sora.com/)\n*   [API Platform(opens in a new window)](https://platform.openai.com/)\n\n*   [Research](https://openai.com/research/index/) \n*   [Safety](https://openai.com/safety/) \n*   [For Business](https://openai.com/business/) \n*   [For Developers](https://openai.com/api/) \n*   [ChatGPT](https://chatgpt.com/overview?openaicom-did=c9c54dfe-7a01-47fc-991f-394b07cdb30e&openaicom_referred=true) \n*   [Sora](https://openai.com/sora/) \n*   [Stories](https://openai.com/stories/) \n*   [Company](https://openai.com/about/) \n*   [News](https://openai.com/news/) \n\n*   Research\n\nBack to main menu  \n\n    *   [Research Index](https://openai.com/research/index/)\n    *   [Research Overview](https://openai.com/research/)\n    *   [Research Residency](https://openai.com/residency/)\n    *   Latest Advancements\n    *   [GPT-5](https://openai.com/index/introducing-gpt-5/)\n    *   [OpenAI o3 and o4-mini](https://openai.com/index/introducing-o3-and-o4-mini/)\n    *   [GPT-4.5](https://openai.com/index/introducing-gpt-4-5/)\n    *   [OpenAI o1](https://openai.com/o1/)\n    *   [GPT-4o](https://openai.com/index/gpt-4o-system-card/)\n    *   [Sora](https://openai.com/index/sora-system-card/)\n\n*   Safety\n\nBack to main menu  \n\n    *   [Safety Approach](https://openai.com/safety/)\n    *   [Security & Privacy](https://openai.com/security-and-privacy/)\n\n*   [For Business](https://openai.com/business/)\n\nBack to main menu  \n\n    *   [Business Overview](https://openai.com/business/)\n    *   [Startups](https://openai.com/startups/)\n    *   [Solutions](https://openai.com/solutions/)\n    *   [ChatGPT Pricing](https://openai.com/business/chatgpt-pricing/)\n    *   [API Pricing](https://openai.com/api/pricing/)\n    *   [Contact Sales](https://openai.com/contact-sales/)\n\n*   For Developers\n\nBack to main menu  \n\n    *   [API Platform](https://openai.com/api/)\n    *   [API Pricing](https://openai.com/api/pricing/)\n    *   [Codex](https://openai.com/codex/)\n    *   [Open Models](https://openai.com/open-models/)\n    *   [Community(opens in a new window)](https://community.openai.com/)\n\n*   [ChatGPT](https://chatgpt.com/overview?openaicom-did=c9c54dfe-7a01-47fc-991f-394b07cdb30e&openaicom_referred=true)\n\nBack to main menu  \n\n    *   [Explore ChatGPT](https://chatgpt.com/overview?openaicom-did=c9c54dfe-7a01-47fc-991f-394b07cdb30e&openaicom_referred=true)\n    *   [Business](https://chatgpt.com/for-business/team?openaicom-did=c9c54dfe-7a01-47fc-991f-394b07cdb30e&openaicom_referred=true)\n    *   [Enterprise](https://chatgpt.com/for-business/enterprise?openaicom-did=c9c54dfe-7a01-47fc-991f-394b07cdb30e&openaicom_referred=true)\n    *   [Education](https://chatgpt.com/for-business/education?openaicom-did=c9c54dfe-7a01-47fc-991f-394b07cdb30e&openaicom_referred=true)\n    *   [Pricing](https://chatgpt.com/pricing?openaicom-did=c9c54dfe-7a01-47fc-991f-394b07cdb30e&openaicom_referred=true)\n    *   [Download](https://chatgpt.com/download?openaicom-did=c9c54dfe-7a01-47fc-991f-394b07cdb30e&openaicom_referred=true)\n\n*   Sora\n\nBack to main menu  \n\n    *   [Sora Overview](https://openai.com/sora/)\n    *   [Features](https://openai.com/sora/#features)\n    *   [Pricing](https://openai.com/sora/#pricing)\n    *   [Help Center(opens in a new window)](https://help.openai.com/en/articles/9957612-generating-videos-on-sora)\n    *   [Sora Log in(opens in a new window)](https://sora.com/)\n\n*   [Stories](https://openai.com/stories/)\n*   Company\n\nBack to main menu  \n\n    *   [About Us](https://openai.com/about/)\n    *   [Our Charter](https://openai.com/charter/)\n    *   [Careers](https://openai.com/careers/)\n    *   [Brand Guidelines](https://openai.com/brand/)\n\n*   [News](https://openai.com/news/)\n\nLog in\n\nOpenAI\n\nTable of contents\n\n*   [Key things we’ve learned](https://openai.com/blog/gpt-2-6-month-follow-up/#key-things-weve-learned)\n*   [Partnerships](https://openai.com/blog/gpt-2-6-month-follow-up/#partnerships)\n*   [Future release decisions](https://openai.com/blog/gpt-2-6-month-follow-up/#future-release-decisions)\n\nAugust 20, 2019\n\n[Publication](https://openai.com/research/index/publication/)\n\nGPT-2: 6-month follow-up\n========================\n\n[Read paper(opens in a new window)](https://arxiv.org/abs/1908.09203)[View code(opens in a new window)](https://github.com/openai/gpt-2)[Legal agreement(opens in a new window)](https://cdn.openai.com/Software%20Access%20Agreement%20Template.docx)\n\n![Image 1: GPT-2 6 Month Follow Up](https://images.ctfassets.net/kftzwdyauwt9/67ab40e4-ca9b-4509-be5089a865ea/097fcea6729c109151e81177db7a4ff2/gpt-2-6-month-follow-up.jpg?w=3840&q=90&fm=webp)\n\nIllustration:Ben Barry\n\nLoading…\n\nShare\n\nWe’re releasing the 774 million parameter GPT‑2 language model after the release of our small[124M model⁠](https://openai.com/index/better-language-models/)in February, staged release of our medium[355M model⁠](https://openai.com/index/better-language-models/#update)in May, and subsequent research with partners and the AI community into the model’s potential for misuse and societal benefit. We’re also releasing an open-source legal agreement to make it easier for organizations to initiate model-sharing partnerships with each other, and are publishing a technical report about our experience in coordinating with the wider AI research community on publication norms.\n\nKey things we’ve learned\n------------------------\n\n**1. Coordination is difficult, but possible**.To date, there hasn’t been a public release of a 1558M parameter language model, though multiple organizations have developed the systems to train them, or have publicly discussed how to train larger models. For example, teams from both NLP developer[Hugging Face⁠(opens in a new window)](https://medium.com/huggingface/ethical-analysis-of-the-open-sourcing-of-a-state-of-the-art-conversational-ai-852113c324b2)and the[Allen Institute for Artificial Intelligence⁠(opens in a new window)](https://allenai.org/)(AI2) with the University of Washington[have explicitly adopted similar staged release approaches to us⁠(opens in a new window)](https://arxiv.org/abs/1905.12616). Since February, we’ve spoken with more than five groups who have replicated GPT‑2.[A](https://openai.com/blog/gpt-2-6-month-follow-up/#citation-bottom-A)\n\n**2. Humans can be convinced by synthetic text**.Research from our research partners Sarah Kreps and Miles McCain at Cornell[published in _Foreign Affairs_⁠(opens in a new window)](https://www.foreignaffairs.com/articles/2019-08-02/not-your-fathers-bots)says people find GPT‑2 synthetic text samples almost as convincing (72% in one cohort judged the articles to be credible) as real articles from the New York Times (83%).[B](https://openai.com/blog/gpt-2-6-month-follow-up/#citation-bottom-B)Additionally, research from AI2/UW has shown that news written by a system called “GROVER” can be[more plausible than human-written propaganda⁠(opens in a new window)](https://arxiv.org/abs/1905.12616). These research results make us generally more cautious about releasing language models.\n\n**3. Detection isn’t simple**.In practice, we expect detectors to need to detect a significant fraction of generations with very few false positives. Malicious actors may use a variety of sampling techniques (including rejection sampling) or fine-tune models to evade detection methods. A deployed system likely needs to be highly accurate (99.9%–99.99%) on a variety of generations. Our research suggests that current ML-based methods only achieve low to mid–90s accuracy, and that fine-tuning the language models decreases accuracy further. There are promising paths forward (see especially those advocated by the developers of “[GROVER⁠(opens in a new window)](https://arxiv.org/abs/1905.12616)”) but it’s a genuinely difficult research problem. We believe that statistical detection of text needs to be supplemented with human judgment and metadata related to the text in order to effectively combat misuse of language models.\n\nPartnerships\n------------\n\nWe’ve partnered with four leading research organizations to analyze both the newly-released 774M parameter GPT‑2 model and the unreleased full-size GPT‑2 model. We’ve included some preliminary results from them in our technical report, and their ongoing analysis will factor into the potential release of the 1558M model. We’ve also developed a non-commercial legal agreement to facilitate the sharing of models between organizations and are publishing it here to help others initiate such sharing schemes.\n\n*   **Cornell University**is studying human susceptibility to digital disinformation generated by language models.\n*   **The Middlebury Institute of International Studies**Center on Terrorism, Extremism, and Counterterrorism (CTEC) is exploring how GPT‑2 could be misused by terrorists and extremists online.\n*   **The University of Oregon**is developing a series of “bias probes” to analyze bias within GPT‑2.\n*   **The University of Texas at Austin**is studying the statistical detectability of GPT‑2 outputs after fine-tuning the model on domain-specific datasets, as well as the extent of detection transfer across different language models.\n\nFuture release decisions\n------------------------\n\nResearch from these partners will factor into our future release decisions, as will observing how the 774M model is used, and discussing language models with researchers and policymakers to understand the considerations around larger models. As part of our staged release strategy, our current plan is to release the 1558M parameter model in a few months, but it’s plausible that findings from a partner, or malicious usage of our 774M model, could change this.\n\nWe think that a combination of staged release and partnership-based model sharing is likely to be a key foundation of responsible publication in AI, particularly in the context of powerful generative models. The issues inherent to large models are going to grow, rather than diminish, over time. We hope that our work on GPT‑2, discussed further in the[technical report⁠(opens in a new window)](https://cdn.openai.com/GPT_2_August_Report.pdf)we’re publishing, will help provide evidence the AI community can draw on when thinking about the publication challenges inherent to some parts of AI research.\n\n### Timeline\n\n*   **January 2019**\n\nOpenAI publishes a[blog post⁠](https://openai.com/index/better-language-models/)and[paper⁠(opens in a new window)](https://cdn.openai.com/better-language-models/language_models_are_unsupervised_multitask_learners.pdf)on GPT‑2.\n\nReleased small parameter (124M) GPT‑2 model. \n*   **February 2019**\n\nThe Partnership on AI co-hosts a dinner with OpenAI to[discuss publication norms⁠(opens in a new window)](https://www.partnershiponai.org/when-is-it-appropriate-to-publish-high-stakes-ai-research/), then publishes a blog summarizing the discussion. \n*   **April 2019**\n\nReleased medium parameter (355M) model.\n\nReleased dataset of outputs from large-scale models.\n\nReleased a detection baseline to help people understand how to detect outputs of models like GPT‑2.\n\nThe original blog post is[updated⁠](https://openai.com/index/better-language-models/#update)to reflect these changes.\n\nAdam King[launches⁠(opens in a new window)](https://twitter.com/adamdanielking/status/1125831730848571392?lang=en)“TalktoTransformer.com”, giving people an interface to play with the newly released models.\n\nHugging Face releases a conversational AI demo based on GPT‑2 models, discusses some of the ethical considerations in the release decision, and[decides not to release the large GPT‑2 model⁠(opens in a new window)](https://medium.com/huggingface/ethical-analysis-of-the-open-sourcing-of-a-state-of-the-art-conversational-ai-852113c324b2).\n\nResearchers with the University of Washington and Allen Institute for AI Research[reveal GROVER⁠(opens in a new window)](https://arxiv.org/abs/1905.12616), a GPT‑2–style language model; they do not release the large versions of the model, and conduct research into the detection of the outputs of such models. \n*   **May 2019**\n\n[OpenAI testifies in Congress⁠(opens in a new window)](https://www.youtube.com/watch?v=tdLS9MlIWOk)about the implications of synthetic media, including a discussion of synthetic text.\n\nDeepMind discusses GPT‑2 and the importance of appropriate publication norms for generative models in their recent[discussion⁠(opens in a new window)](https://deepmind.com/blog/article/unsupervised-learning)of unsupervised learning.\n\nOpenAI commences a research collaboration with the[Partnership on AI⁠(opens in a new window)](https://www.partnershiponai.org/)for publication norms in AI research. We’re trying to work with a diverse set of AI research organizations to come up with questions scientists may want to ask ahead of publication, and potential frameworks they can use to make publication decisions. \n*   **June 2019**\n\n[DeepTabNine develops a code autocompleter⁠(opens in a new window)](https://tabnine.com/blog/deep)based on GPT‑2.\n\n[Multi-turn Dialogue Response Generation with Autoregressive Transformer Models⁠(opens in a new window)](https://arxiv.org/abs/1908.01841)\n\n[GLTR: Statistical Detection and Visualization of Generated Text⁠(opens in a new window)](https://www.aclweb.org/anthology/P19-3019) \n*   **July 2019**\n\nResearchers with the Thoughtful Technology Project and the University of Cambridge published a working paper on “[Reducing malicious use of synthetic media research: Considerations and potential release practices for machine learning⁠(opens in a new window)](https://arxiv.org/abs/1907.11274)”.\n\n[Hello, It’s GPT‑2—How Can I Help You? Towards the Use of Pretrained Language Models for Task-Oriented Dialogue Systems⁠(opens in a new window)](https://arxiv.org/abs/1907.05774)\n\nAI startup AI21 Labs releases[HAIM⁠(opens in a new window)](https://www.ai21.com/haim-post), a neural text generator; they only release a 345M variant of the model, “equivalent in size to the publicly released versions of Grover and GPT‑2.”\n\nNVIDIA Research[trains⁠(opens in a new window)](https://nv-adlr.github.io/MegatronLM)8.3 billion parameter GPT‑2 model.\n\nReleased larger parameter (774M)model. \n\n*   [GPT](https://openai.com/research/index/?tags=gpt)\n*   [Community & Collaboration](https://openai.com/research/index/?tags=community-collaboration)\n*   [Reasonings & Policy](https://openai.com/research/index/?tags=reasoning-policy)\n*   [Ethics & Safety](https://openai.com/research/index/?tags=ethics-safety)\n\nFootnotes\n---------\n\n1.   A\nHaving these conversations is difficult, as it involves talking candidly about proprietary systems and it’s unclear who to reach out to in specific organizations to discuss such models and what the appropriate processes are for inter-org discussion about unreleased research.\n\n2.   B\nThese samples were generated via a “human-in-the-loop” process meant to simulate contemporary disinformation operations, where a human generated samples and periodically selected some for exposure to people.\n\nRelated articles\n----------------\n\n[View all](https://openai.com/news/publication/)\n\n![Image 2: Democratic Inputs To AI Grant Program Update](https://images.ctfassets.net/kftzwdyauwt9/f50ce1d2-4f61-4ed2-e560c624d631/6f4dd4542898a35d0a91b137f85c9834/Democratic_inputs_to_AI_grant_program_lessons_learned_and_implementation_plans.jpg?w=3840&q=90&fm=webp)\n\n[Democratic inputs to AI grant program: lessons learned and implementation plans Safety Jan 16, 2024](https://openai.com/index/democratic-inputs-to-ai-grant-program-update/)\n\n![Image 3: Three farmers using a mobile app outside](https://images.ctfassets.net/kftzwdyauwt9/7b26cc3b-46d8-45b0-f67f6de0c8f8/8bc94d083a5a147609cddad159243ef7/digital_green.png?w=3840&q=90&fm=webp)\n\n[Building agricultural database for farmers API Jan 12, 2024](https://openai.com/index/digital-green/)\n\n![Image 4: Wix cover image](https://images.ctfassets.net/kftzwdyauwt9/6E3QyNLzuWwK3EGBHPw7nQ/fbd0c5a34cc4f4ba096028adbdda8934/oai_Wix_1x1.png?w=3840&q=90&fm=webp)\n\n[Creating websites in minutes with AI Website Builder API May 29, 2025](https://openai.com/index/wix/)\n\nOur Research\n*   [Research Index](https://openai.com/research/index/)\n*   [Research Overview](https://openai.com/research/)\n*   [Research Residency](https://openai.com/residency/)\n\nLatest Advancements\n*   [GPT-5](https://openai.com/gpt-5/)\n*   [OpenAI o3](https://openai.com/index/introducing-o3-and-o4-mini/)\n*   [OpenAI o4-mini](https://openai.com/index/introducing-o3-and-o4-mini/)\n*   [GPT-4o](https://openai.com/index/gpt-4o-system-card/)\n*   [GPT-4o mini](https://openai.com/index/gpt-4o-mini-advancing-cost-efficient-intelligence/)\n*   [Sora](https://openai.com/index/sora-system-card/)\n\nSafety\n*   [Safety Approach](https://openai.com/safety/)\n*   [Security & Privacy](https://openai.com/security-and-privacy/)\n*   [Trust & Transparency](https://openai.com/trust-and-transparency/)\n\nChatGPT\n*   [Explore ChatGPT(opens in a new window)](https://chatgpt.com/overview?openaicom-did=c9c54dfe-7a01-47fc-991f-394b07cdb30e&openaicom_referred=true)\n*   [Business](https://chatgpt.com/business/business-plan?openaicom-did=c9c54dfe-7a01-47fc-991f-394b07cdb30e&openaicom_referred=true)\n*   [Enterprise](https://chatgpt.com/business/enterprise?openaicom-did=c9c54dfe-7a01-47fc-991f-394b07cdb30e&openaicom_referred=true)\n*   [Education](https://chatgpt.com/business/education?openaicom-did=c9c54dfe-7a01-47fc-991f-394b07cdb30e&openaicom_referred=true)\n*   [Pricing(opens in a new window)](https://chatgpt.com/pricing?openaicom-did=c9c54dfe-7a01-47fc-991f-394b07cdb30e&openaicom_referred=true)\n*   [Download(opens in a new window)](https://chatgpt.com/download?openaicom-did=c9c54dfe-7a01-47fc-991f-394b07cdb30e&openaicom_referred=true)\n\nSora\n*   [Sora Overview](https://openai.com/sora/)\n*   [Features](https://openai.com/sora/#features)\n*   [Pricing](https://openai.com/sora/#pricing)\n*   [Sora log in(opens in a new window)](https://sora.com/)\n\nAPI Platform\n*   [Platform Overview](https://openai.com/api/)\n*   [Pricing](https://openai.com/api/pricing/)\n*   [API log in(opens in a new window)](https://platform.openai.com/login)\n*   [Documentation(opens in a new window)](https://platform.openai.com/docs/overview)\n*   [Developer Forum(opens in a new window)](https://community.openai.com/)\n\nFor Business\n*   [Business Overview](https://openai.com/business/)\n*   [Solutions](https://openai.com/solutions/)\n*   [Contact Sales](https://openai.com/contact-sales/)\n\nCompany\n*   [About Us](https://openai.com/about/)\n*   [Our Charter](https://openai.com/charter/)\n*   [Careers](https://openai.com/careers/)\n*   [Brand](https://openai.com/brand/)\n\nSupport\n*   [Help Center(opens in a new window)](https://help.openai.com/)\n\nMore\n*   [News](https://openai.com/news/)\n*   [Stories](https://openai.com/stories/)\n*   [Livestreams](https://openai.com/live/)\n*   [Podcast](https://openai.com/podcast/)\n\nTerms & Policies\n*   [Terms of Use](https://openai.com/policies/terms-of-use/)\n*   [Privacy Policy](https://openai.com/policies/privacy-policy/)\n*   [Other Policies](https://openai.com/policies/)\n\n[(opens in a new window)](https://x.com/OpenAI)[(opens in a new window)](https://www.youtube.com/OpenAI)[(opens in a new window)](https://www.linkedin.com/company/openai)[(opens in a new window)](https://github.com/openai)[(opens in a new window)](https://www.instagram.com/openai/)[(opens in a new window)](https://www.tiktok.com/@openai)[(opens in a new window)](https://discord.gg/openai)\n\nOpenAI © 2015–2025 Manage Cookies\n\nEnglish United States\n",
      "fetch_method": "jina-reader"
    },
    {
      "arxiv_id": "https://r.jina.ai/https://www.openai.com/blog/gpt-2-1-5b-release/",
      "full_text": "Title: GPT-2: 1.5B release\n\nURL Source: https://www.openai.com/blog/gpt-2-1-5b-release/\n\nMarkdown Content:\nGPT-2: 1.5B release | OpenAI\n\n===============\n\n[Skip to main content](https://www.openai.com/blog/gpt-2-1-5b-release/#main)\n\nLog in\n\n[](https://www.openai.com/)\n\nSwitch to\n\n*   [ChatGPT(opens in a new window)](https://chatgpt.com/?openaicom-did=f0734c87-cec3-4a9a-ab0a-d8f0e104236d&openaicom_referred=true)\n*   [Sora(opens in a new window)](https://sora.com/)\n*   [API Platform(opens in a new window)](https://platform.openai.com/)\n\n*   [Research](https://www.openai.com/research/index/) \n*   [Safety](https://www.openai.com/safety/) \n*   [For Business](https://www.openai.com/business/) \n*   [For Developers](https://www.openai.com/api/) \n*   [ChatGPT](https://chatgpt.com/overview?openaicom-did=f0734c87-cec3-4a9a-ab0a-d8f0e104236d&openaicom_referred=true) \n*   [Sora](https://www.openai.com/sora/) \n*   [Stories](https://www.openai.com/stories/) \n*   [Company](https://www.openai.com/about/) \n*   [News](https://www.openai.com/news/) \n\n*   Research\n\nBack to main menu  \n\n    *   [Research Index](https://www.openai.com/research/index/)\n    *   [Research Overview](https://www.openai.com/research/)\n    *   [Research Residency](https://www.openai.com/residency/)\n    *   Latest Advancements\n    *   [GPT-5](https://www.openai.com/index/introducing-gpt-5/)\n    *   [OpenAI o3 and o4-mini](https://www.openai.com/index/introducing-o3-and-o4-mini/)\n    *   [GPT-4.5](https://www.openai.com/index/introducing-gpt-4-5/)\n    *   [OpenAI o1](https://www.openai.com/o1/)\n    *   [GPT-4o](https://www.openai.com/index/gpt-4o-system-card/)\n    *   [Sora](https://www.openai.com/index/sora-system-card/)\n\n*   Safety\n\nBack to main menu  \n\n    *   [Safety Approach](https://www.openai.com/safety/)\n    *   [Security & Privacy](https://www.openai.com/security-and-privacy/)\n\n*   [For Business](https://www.openai.com/business/)\n\nBack to main menu  \n\n    *   [Business Overview](https://www.openai.com/business/)\n    *   [Startups](https://www.openai.com/startups/)\n    *   [Solutions](https://www.openai.com/solutions/)\n    *   [ChatGPT Pricing](https://www.openai.com/business/chatgpt-pricing/)\n    *   [API Pricing](https://www.openai.com/api/pricing/)\n    *   [Contact Sales](https://www.openai.com/contact-sales/)\n\n*   For Developers\n\nBack to main menu  \n\n    *   [API Platform](https://www.openai.com/api/)\n    *   [API Pricing](https://www.openai.com/api/pricing/)\n    *   [Codex](https://www.openai.com/codex/)\n    *   [Open Models](https://www.openai.com/open-models/)\n    *   [Community(opens in a new window)](https://community.openai.com/)\n\n*   [ChatGPT](https://chatgpt.com/overview?openaicom-did=f0734c87-cec3-4a9a-ab0a-d8f0e104236d&openaicom_referred=true)\n\nBack to main menu  \n\n    *   [Explore ChatGPT](https://chatgpt.com/overview?openaicom-did=f0734c87-cec3-4a9a-ab0a-d8f0e104236d&openaicom_referred=true)\n    *   [Business](https://chatgpt.com/for-business/team?openaicom-did=f0734c87-cec3-4a9a-ab0a-d8f0e104236d&openaicom_referred=true)\n    *   [Enterprise](https://chatgpt.com/for-business/enterprise?openaicom-did=f0734c87-cec3-4a9a-ab0a-d8f0e104236d&openaicom_referred=true)\n    *   [Education](https://chatgpt.com/for-business/education?openaicom-did=f0734c87-cec3-4a9a-ab0a-d8f0e104236d&openaicom_referred=true)\n    *   [Pricing](https://chatgpt.com/pricing?openaicom-did=f0734c87-cec3-4a9a-ab0a-d8f0e104236d&openaicom_referred=true)\n    *   [Download](https://chatgpt.com/download?openaicom-did=f0734c87-cec3-4a9a-ab0a-d8f0e104236d&openaicom_referred=true)\n\n*   Sora\n\nBack to main menu  \n\n    *   [Sora Overview](https://www.openai.com/sora/)\n    *   [Features](https://www.openai.com/sora/#features)\n    *   [Pricing](https://www.openai.com/sora/#pricing)\n    *   [Help Center(opens in a new window)](https://help.openai.com/en/articles/9957612-generating-videos-on-sora)\n    *   [Sora Log in(opens in a new window)](https://sora.com/)\n\n*   [Stories](https://www.openai.com/stories/)\n*   Company\n\nBack to main menu  \n\n    *   [About Us](https://www.openai.com/about/)\n    *   [Our Charter](https://www.openai.com/charter/)\n    *   [Careers](https://www.openai.com/careers/)\n    *   [Brand Guidelines](https://www.openai.com/brand/)\n\n*   [News](https://www.openai.com/news/)\n\nLog in\n\nGPT-2: 1.5B release | OpenAI\n\nTable of contents\n\n*   [Our findings](https://www.openai.com/blog/gpt-2-1-5b-release/#our-findings)\n*   [Next steps](https://www.openai.com/blog/gpt-2-1-5b-release/#next-steps)\n\nNovember 5, 2019\n\n[Release](https://www.openai.com/research/index/release/)\n\nGPT-2: 1.5B release\n===================\n\n[Read paper(opens in a new window)](https://arxiv.org/abs/1908.09203)[GPT-2 model(opens in a new window)](https://github.com/openai/gpt-2)[Detector model(opens in a new window)](https://github.com/openai/gpt-2-output-dataset/tree/master/detector)\n\n![Image 1: GPT-2 1.5B Release](https://images.ctfassets.net/kftzwdyauwt9/5ca4df8a-bd0c-47e0-7efe1b15187a/f891c43eaec1e52760e3cf7c9902a819/gpt-2-1-5b-release.jpg?w=3840&q=90&fm=webp)\n\nIllustration:Ben Barry\n\nMore Resources\n\n[Model card(opens in a new window)](https://github.com/openai/gpt-2/blob/master/model_card.md)\n\nListen to article\n\nShare\n\nAs the final model release of[GPT‑2⁠](https://www.openai.com/index/better-language-models/)’s[staged release⁠](https://www.openai.com/index/gpt-2-6-month-follow-up/), we’re releasing the largest version (1.5B parameters) of GPT‑2 along with[code and model weights⁠(opens in a new window)](https://github.com/openai/gpt-2-output-dataset)to facilitate detection of outputs of GPT‑2 models. While there have been larger language models released since August, we’ve continued with our original staged release plan in order to provide the community with a test case of a full staged release process. We hope that this test case will be useful to developers of future powerful models, and we’re actively continuing the conversation with the AI community on responsible publication.\n\nWhile there have been larger language models released since August, we’ve continued with our original staged release plan in order to provide the community with a test case of a full staged release process. We hope that this test case will be useful to developers of future powerful models, and we’re actively continuing the conversation with the AI community on responsible publication.\n\nOur findings\n------------\n\n**1. Humans find GPT‑2 outputs convincing**.Our partners at Cornell University surveyed people to assign GPT‑2 text a credibility score across model sizes. People gave the 1.5B model a “credibility score” of 6.91 out of 10. This is marginally greater than outputs from the 774M model (6.72) and significantly above the medium 355M model (6.07). These results make us more inclined to release the 1.5B model, as the incremental increase in human-perceived credibility relative to 774M seems low.\n\n**2. GPT‑2 can be fine-tuned for misuse**.Our partners at the Middlebury Institute of International Studies’ Center on Terrorism, Extremism, and Counterterrorism (CTEC) found that extremist groups can use GPT‑2 for misuse, specifically by fine-tuning GPT‑2 models on four ideological positions: white supremacy, Marxism, jihadist Islamism, and anarchism. CTEC demonstrated that it’s possible to create models that can generate synthetic propaganda for these ideologies. They also show that, despite having low detection accuracy on synthetic outputs, ML-based detection methods can give experts reasonable suspicion that an actor is generating synthetic text.\n\n**3. Detection is challenging**.We expect that content-based detection of synthetic text is a long-term challenge. To test whether machine learning approaches may help today, we conducted in-house detection research and developed a[detection model⁠(opens in a new window)](https://github.com/openai/gpt-2-output-dataset)that has detection rates of ~95% for detecting 1.5B GPT‑2-generated text.[A](https://www.openai.com/blog/gpt-2-1-5b-release/#citation-bottom-A) We believe this is not high enough accuracy for standalone detection and needs to be paired with metadata-based approaches, human judgment, and public education to be more effective. We are releasing this model to aid the study of research into the detection of synthetic text, although this does let adversaries with access better evade detection.\n\nWhile we found detection accuracy depends heavily on the sampling methods used in training and testing, we also found detection to be more reliable when training across a range of sampling techniques. As seen in the figure below, we observed that larger models’ outputs are more difficult to classify, but training on larger models’ outputs makes detection results more accurate and robust. We expect this trend to continue and that detection will be more challenging with increased model size.\n\nTransferred model accuracy (nucleus samples)\n============================================\n\n| Trained on | Tested on Small (124M) | Medium (355M) | Large (774M) | XL (1.5B) |\n| --- | --- | --- | --- | --- |\n| Small (124M) | 99.3% | 96.6% | 90.9% | 79.3% |\n| Medium (355M) | 99.0% | 98.5% | 96.9% | 91.8% |\n| Large (774M) | 98.4% | 97.9% | 97.9% | 95.7% |\n| XL (1.5B) | 96.9% | 96.7% | 96.6% | 96.0% |\n\n**4. We’ve seen no strong evidence of misuse so far**.While we’ve seen some discussion around GPT‑2’s potential to augment high-volume/low-yield operations like spam and phishing, we haven’t seen evidence of writing code, documentation, or instances of misuse. We think synthetic text generators have a higher chance of being misused if their outputs become more reliable and coherent. We acknowledge that we cannot be aware of all threats, and that motivated actors can replicate language models without model release.\n\n**5. We need standards for studying bias**.Language models have biases. Working out how to study these biases, discuss them, and address them, is a challenge for the AI research community. We’ve approached the challenge of bias in two ways:\n\n*   Publishing a[model card⁠(opens in a new window)](https://github.com/openai/gpt-2/blob/master/model_card.md)[B](https://www.openai.com/blog/gpt-2-1-5b-release/#citation-bottom-B) alongside our models on GitHub to give people a sense of the issues inherent to language models such as GPT‑2.\n*   Performing a qualitative, in-house evaluation of some of the biases in GPT‑2: We probed GPT‑2 for some gender, race, and religious biases, using those findings to inform our model card. These probes are not comprehensive and raise the need for collaboration on bias analysis frameworks.\n\nNext steps\n----------\n\nOur experience with GPT‑2 over the past 9 months has given us valuable insight into the challenges and opportunities for creating responsible publication norms in AI. We’re continuing our work on this issue via participation in the Partnership on AI’s “Responsible Publication Norms for Machine Learning” project and discussions with our colleagues in the research community.\n\n_If you’d like to develop large-scale AI systems and think about their implications,_[_we’re hiring_⁠](https://www.openai.com/careers/)_._\n\n*   [GPT](https://www.openai.com/research/index/?tags=gpt)\n*   [Ethics & Safety](https://www.openai.com/research/index/?tags=ethics-safety)\n*   [Generative Models](https://www.openai.com/research/index/?tags=generative-models)\n*   [Language](https://www.openai.com/research/index/?tags=language)\n*   [Transformers](https://www.openai.com/research/index/?tags=transformers)\n\nFootnotes\n---------\n\n1.   A\nSpecifically, we based a sequence classifier on RoBERTaBASE(125 million parameters) and RoBERTaLARGE(355 million parameters) and fine-tuned it to classify the outputs from the 1.5B GPT-2 model versus WebText, the dataset we used to train the GPT-2 model.\n\n2.   B\nWhich we’ve based on “[Model Cards for Model Reporting⁠(opens in a new window)](https://arxiv.org/abs/1810.03993)” by Mitchell et al.\n\nRelated articles\n----------------\n\n[View all](https://www.openai.com/news/release/)\n\n![Image 2: Democratic Inputs To AI Grant Program Update](https://images.ctfassets.net/kftzwdyauwt9/f50ce1d2-4f61-4ed2-e560c624d631/6f4dd4542898a35d0a91b137f85c9834/Democratic_inputs_to_AI_grant_program_lessons_learned_and_implementation_plans.jpg?w=3840&q=90&fm=webp)\n\n[Democratic inputs to AI grant program: lessons learned and implementation plans Safety Jan 16, 2024](https://www.openai.com/index/democratic-inputs-to-ai-grant-program-update/)\n\n![Image 3: Three farmers using a mobile app outside](https://images.ctfassets.net/kftzwdyauwt9/7b26cc3b-46d8-45b0-f67f6de0c8f8/8bc94d083a5a147609cddad159243ef7/digital_green.png?w=3840&q=90&fm=webp)\n\n[Building agricultural database for farmers API Jan 12, 2024](https://www.openai.com/index/digital-green/)\n\n![Image 4: Wix cover image](https://images.ctfassets.net/kftzwdyauwt9/6E3QyNLzuWwK3EGBHPw7nQ/fbd0c5a34cc4f4ba096028adbdda8934/oai_Wix_1x1.png?w=3840&q=90&fm=webp)\n\n[Creating websites in minutes with AI Website Builder API May 29, 2025](https://www.openai.com/index/wix/)\n\nOur Research\n*   [Research Index](https://www.openai.com/research/index/)\n*   [Research Overview](https://www.openai.com/research/)\n*   [Research Residency](https://www.openai.com/residency/)\n\nLatest Advancements\n*   [GPT-5](https://www.openai.com/gpt-5/)\n*   [OpenAI o3](https://www.openai.com/index/introducing-o3-and-o4-mini/)\n*   [OpenAI o4-mini](https://www.openai.com/index/introducing-o3-and-o4-mini/)\n*   [GPT-4o](https://www.openai.com/index/gpt-4o-system-card/)\n*   [GPT-4o mini](https://www.openai.com/index/gpt-4o-mini-advancing-cost-efficient-intelligence/)\n*   [Sora](https://www.openai.com/index/sora-system-card/)\n\nSafety\n*   [Safety Approach](https://www.openai.com/safety/)\n*   [Security & Privacy](https://www.openai.com/security-and-privacy/)\n*   [Trust & Transparency](https://www.openai.com/trust-and-transparency/)\n\nChatGPT\n*   [Explore ChatGPT(opens in a new window)](https://chatgpt.com/overview?openaicom-did=f0734c87-cec3-4a9a-ab0a-d8f0e104236d&openaicom_referred=true)\n*   [Business](https://chatgpt.com/business/business-plan?openaicom-did=f0734c87-cec3-4a9a-ab0a-d8f0e104236d&openaicom_referred=true)\n*   [Enterprise](https://chatgpt.com/business/enterprise?openaicom-did=f0734c87-cec3-4a9a-ab0a-d8f0e104236d&openaicom_referred=true)\n*   [Education](https://chatgpt.com/business/education?openaicom-did=f0734c87-cec3-4a9a-ab0a-d8f0e104236d&openaicom_referred=true)\n*   [Pricing(opens in a new window)](https://chatgpt.com/pricing?openaicom-did=f0734c87-cec3-4a9a-ab0a-d8f0e104236d&openaicom_referred=true)\n*   [Download(opens in a new window)](https://chatgpt.com/download?openaicom-did=f0734c87-cec3-4a9a-ab0a-d8f0e104236d&openaicom_referred=true)\n\nSora\n*   [Sora Overview](https://www.openai.com/sora/)\n*   [Features](https://www.openai.com/sora/#features)\n*   [Pricing](https://www.openai.com/sora/#pricing)\n*   [Sora log in(opens in a new window)](https://sora.com/)\n\nAPI Platform\n*   [Platform Overview](https://www.openai.com/api/)\n*   [Pricing](https://www.openai.com/api/pricing/)\n*   [API log in(opens in a new window)](https://platform.openai.com/login)\n*   [Documentation(opens in a new window)](https://platform.openai.com/docs/overview)\n*   [Developer Forum(opens in a new window)](https://community.openai.com/)\n\nFor Business\n*   [Business Overview](https://www.openai.com/business/)\n*   [Solutions](https://www.openai.com/solutions/)\n*   [Contact Sales](https://www.openai.com/contact-sales/)\n\nCompany\n*   [About Us](https://www.openai.com/about/)\n*   [Our Charter](https://www.openai.com/charter/)\n*   [Careers](https://www.openai.com/careers/)\n*   [Brand](https://www.openai.com/brand/)\n\nSupport\n*   [Help Center(opens in a new window)](https://help.openai.com/)\n\nMore\n*   [News](https://www.openai.com/news/)\n*   [Stories](https://www.openai.com/stories/)\n*   [Livestreams](https://www.openai.com/live/)\n*   [Podcast](https://www.openai.com/podcast/)\n\nTerms & Policies\n*   [Terms of Use](https://www.openai.com/policies/terms-of-use/)\n*   [Privacy Policy](https://www.openai.com/policies/privacy-policy/)\n*   [Other Policies](https://www.openai.com/policies/)\n\n[(opens in a new window)](https://x.com/OpenAI)[(opens in a new window)](https://www.youtube.com/OpenAI)[(opens in a new window)](https://www.linkedin.com/company/openai)[(opens in a new window)](https://github.com/openai)[(opens in a new window)](https://www.instagram.com/openai/)[(opens in a new window)](https://www.tiktok.com/@openai)[(opens in a new window)](https://discord.gg/openai)\n\nOpenAI © 2015–2025 Manage Cookies\n\nEnglish United States\n",
      "fetch_method": "jina-reader"
    }
  ]
}