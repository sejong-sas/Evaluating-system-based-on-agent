{
  "model": "openai-community/gpt2",
  "scores": {
    "1-1 Weights": {
      "score": 1,
      "reason": "Weights files present in the repository (e.g., *.safetensors/bin/pt/ckpt)."
    },
    "1-2 Code": {
      "score": 0.0,
      "reason": "No training pipeline files; README mentions are ignored."
    },
    "1-3 License": {
      "score": 1.0,
      "reason": "The checkpoint is distributed under the MIT licence, which permits use, modification, redistribution, and commercial use."
    },
    "1-4 Paper": {
      "score": 1.0,
      "reason": "GPT-2 is documented in the public technical report “Language Models are Unsupervised Multitask Learners” (Radford et al., 2019)."
    },
    "1-5 Architecture": {
      "score": 1,
      "reason": "Architecture evidence present in extracted quotes."
    },
    "1-6 Tokenizer": {
      "score": 1.0,
      "reason": "Tokenizer files downloadable in repository."
    },
    "2-1 Hardware": {
      "score": 1.0,
      "reason": "Quote: “The larger model was trained on 256 Cloud TPU v3 cores,” disclosing both hardware type and quantity."
    },
    "2-2 Software": {
      "score": 0.0,
      "reason": "Only the generic Transformers version (\"4.26.0.dev0\") is mentioned; no training stack, libraries, or configs are described."
    },
    "2-3 API": {
      "score": 1.0,
      "reason": "Only generic/irrelevant mentions; no qualifying API docs.  Web search found API docs: https://openrouter.ai/docs/community/open-ai-sdk, https://fireworks.ai/docs/tools-sdks/openai-compatibility, https://openrouter.ai/docs/overview/models"
    },
    "3-1 Pre-training": {
      "score": 0.5,
      "reason": "Objective (next-word prediction) and dataset size/source (40 GB WebText) are given, but hyper-parameters, schedules, and full pipeline are not."
    },
    "3-2 Fine-tuning": {
      "score": 0.0,
      "reason": "Only generic statements that GPT-2 *can* be fine-tuned; no concrete procedure, datasets, or hyper-parameters are provided."
    },
    "3-3 Reinforcement Learning": {
      "score": 0.0,
      "reason": "No information or claims about any RL training for GPT-2 appear in the supplied quotes. No direct quote evidence; defaulting to Closed by strict rule."
    },
    "4-1 Pre-training Data": {
      "score": 0.5,
      "reason": "Source (Reddit outbound links ≥3 karma) and size (40 GB) are disclosed, but the corpus itself is unreleased and not fully enumerated."
    },
    "4-2 Fine-tuning Data": {
      "score": 0.0,
      "reason": "No concrete fine-tuning datasets or sizes are disclosed in the evidence."
    },
    "4-3 Reinforcement Learning Data": {
      "score": 0.0,
      "reason": "No RL data is described. No direct quote evidence; defaulting to Closed by strict rule."
    },
    "4-4 Data Filtering": {
      "score": 0.5,
      "reason": "Some filtering steps are mentioned (Wikipedia removal, non-English removal), but the complete filtering pipeline is not detailed."
    }
  },
  "included_scores": {
    "1-1 Weights": {
      "score": 1,
      "reason": "Weights files present in the repository (e.g., *.safetensors/bin/pt/ckpt)."
    },
    "1-2 Code": {
      "score": 0.0,
      "reason": "No training pipeline files; README mentions are ignored."
    },
    "1-3 License": {
      "score": 1.0,
      "reason": "The checkpoint is distributed under the MIT licence, which permits use, modification, redistribution, and commercial use."
    },
    "1-4 Paper": {
      "score": 1.0,
      "reason": "GPT-2 is documented in the public technical report “Language Models are Unsupervised Multitask Learners” (Radford et al., 2019)."
    },
    "1-5 Architecture": {
      "score": 1,
      "reason": "Architecture evidence present in extracted quotes."
    },
    "1-6 Tokenizer": {
      "score": 1.0,
      "reason": "Tokenizer files downloadable in repository."
    },
    "2-1 Hardware": {
      "score": 1.0,
      "reason": "Quote: “The larger model was trained on 256 Cloud TPU v3 cores,” disclosing both hardware type and quantity."
    },
    "2-2 Software": {
      "score": 0.0,
      "reason": "Only the generic Transformers version (\"4.26.0.dev0\") is mentioned; no training stack, libraries, or configs are described."
    },
    "2-3 API": {
      "score": 1.0,
      "reason": "Only generic/irrelevant mentions; no qualifying API docs.  Web search found API docs: https://openrouter.ai/docs/community/open-ai-sdk, https://fireworks.ai/docs/tools-sdks/openai-compatibility, https://openrouter.ai/docs/overview/models"
    },
    "3-1 Pre-training": {
      "score": 0.5,
      "reason": "Objective (next-word prediction) and dataset size/source (40 GB WebText) are given, but hyper-parameters, schedules, and full pipeline are not."
    },
    "4-1 Pre-training Data": {
      "score": 0.5,
      "reason": "Source (Reddit outbound links ≥3 karma) and size (40 GB) are disclosed, but the corpus itself is unreleased and not fully enumerated."
    },
    "4-4 Data Filtering": {
      "score": 0.5,
      "reason": "Some filtering steps are mentioned (Wikipedia removal, non-English removal), but the complete filtering pipeline is not detailed."
    }
  },
  "final_score_10pt": 7.083,
  "meta": {
    "usage_from_dispatch": {
      "fine_tuning": "not_used",
      "rl": "not_used"
    },
    "excluded": [
      "3-2 Fine-tuning",
      "3-3 Reinforcement Learning",
      "4-2 Fine-tuning Data",
      "4-3 Reinforcement Learning Data"
    ],
    "denominator": 12,
    "raw_sum": 8.5,
    "scale": "10/12",
    "code_detection_reason": "No training pipeline files; README mentions are ignored.",
    "pretrain_sources_used": false
  }
}