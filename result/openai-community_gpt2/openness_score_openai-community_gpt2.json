{
  "model": "openai-community/gpt2",
  "scores": {
    "1-2 Code": {
      "score": 0.0,
      "reason": "No training pipeline files; README mentions are ignored."
    },
    "1-3 License": {
      "score": 1.0,
      "reason": "The repository is released under the MIT licence which grants use, modification, redistribution and commercial use with no substantive restrictions."
    },
    "1-4 Paper": {
      "score": 1.0,
      "reason": "The official OpenAI paper “Language Models are Unsupervised Multitask Learners” is linked in the model card and is authored by OpenAI researchers, matching the target model."
    },
    "1-5 Architecture": {
      "score": 1.0,
      "reason": "Hugging Face presence detected; forced Open per FORCE_HF_CORE_OPEN."
    },
    "1-6 Tokenizer": {
      "score": 1.0,
      "reason": "Hugging Face presence detected; forced Open per FORCE_HF_CORE_OPEN."
    },
    "2-1 Hardware": {
      "score": 1.0,
      "reason": "Quote states: “The larger model was trained on 256 cloud TPU v3 cores,” giving both hardware type (TPU v3) and quantity (256 cores)."
    },
    "2-2 Software": {
      "score": 0.0,
      "reason": "Only a Transformers library version (\"4.26.0.dev0\") is mentioned; no details of the training stack beyond the base framework are provided."
    },
    "2-3 API": {
      "score": 0.0,
      "reason": "Only generic or non-official mentions; no qualifying official API docs.  No qualifying official API docs via web search."
    },
    "3-1 Pre-training": {
      "score": 0.5,
      "reason": "Quotes describe self-supervised next-word prediction on WebText, but do not supply full, reproducible training schedules or hyper-parameter settings."
    },
    "3-2 Fine-tuning": {
      "score": 0.0,
      "reason": "Only generic statements that GPT-2 *can* be fine-tuned are given; no methodological details of any author-performed fine-tuning are disclosed."
    },
    "3-3 Reinforcement Learning": {
      "score": 0.0,
      "reason": "No quotes describe any RL-based training or data. No direct quote evidence; defaulting to Closed by strict rule."
    },
    "4-1 Pre-training Data": {
      "score": 0.5,
      "reason": "Source (Reddit outbound links ≥ 3 karma) and size (~40 GB WebText) are disclosed, but the corpus is not released and full composition/licensing details are absent."
    },
    "4-2 Fine-tuning Data": {
      "score": 0.0,
      "reason": "No specific datasets, sizes or availability for fine-tuning are provided in the quoted material."
    },
    "4-3 Reinforcement Learning Data": {
      "score": 0.0,
      "reason": "No RL data are mentioned in the evidence. No direct quote evidence; defaulting to Closed by strict rule."
    },
    "4-4 Data Filtering": {
      "score": 0.5,
      "reason": "Partial filtering details are given (Wikipedia pages and non-English pages were removed) but the full multi-stage filtering pipeline is not described."
    },
    "1-1 Weights": {
      "score": 1.0,
      "reason": "Hugging Face presence detected; forced Open per FORCE_HF_CORE_OPEN."
    }
  },
  "included_scores": {
    "1-2 Code": {
      "score": 0.0,
      "reason": "No training pipeline files; README mentions are ignored."
    },
    "1-3 License": {
      "score": 1.0,
      "reason": "The repository is released under the MIT licence which grants use, modification, redistribution and commercial use with no substantive restrictions."
    },
    "1-4 Paper": {
      "score": 1.0,
      "reason": "The official OpenAI paper “Language Models are Unsupervised Multitask Learners” is linked in the model card and is authored by OpenAI researchers, matching the target model."
    },
    "1-5 Architecture": {
      "score": 1.0,
      "reason": "Hugging Face presence detected; forced Open per FORCE_HF_CORE_OPEN."
    },
    "1-6 Tokenizer": {
      "score": 1.0,
      "reason": "Hugging Face presence detected; forced Open per FORCE_HF_CORE_OPEN."
    },
    "2-1 Hardware": {
      "score": 1.0,
      "reason": "Quote states: “The larger model was trained on 256 cloud TPU v3 cores,” giving both hardware type (TPU v3) and quantity (256 cores)."
    },
    "2-2 Software": {
      "score": 0.0,
      "reason": "Only a Transformers library version (\"4.26.0.dev0\") is mentioned; no details of the training stack beyond the base framework are provided."
    },
    "2-3 API": {
      "score": 0.0,
      "reason": "Only generic or non-official mentions; no qualifying official API docs.  No qualifying official API docs via web search."
    },
    "3-1 Pre-training": {
      "score": 0.5,
      "reason": "Quotes describe self-supervised next-word prediction on WebText, but do not supply full, reproducible training schedules or hyper-parameter settings."
    },
    "4-1 Pre-training Data": {
      "score": 0.5,
      "reason": "Source (Reddit outbound links ≥ 3 karma) and size (~40 GB WebText) are disclosed, but the corpus is not released and full composition/licensing details are absent."
    },
    "4-4 Data Filtering": {
      "score": 0.5,
      "reason": "Partial filtering details are given (Wikipedia pages and non-English pages were removed) but the full multi-stage filtering pipeline is not described."
    },
    "1-1 Weights": {
      "score": 1.0,
      "reason": "Hugging Face presence detected; forced Open per FORCE_HF_CORE_OPEN."
    }
  },
  "final_score_10pt": 6.25,
  "meta": {
    "usage_from_dispatch": {
      "fine_tuning": "not_used",
      "rl": "not_used"
    },
    "excluded": [
      "3-2 Fine-tuning",
      "3-3 Reinforcement Learning",
      "4-2 Fine-tuning Data",
      "4-3 Reinforcement Learning Data"
    ],
    "denominator": 12,
    "raw_sum": 7.5,
    "scale": "10/12",
    "code_detection_reason": "No training pipeline files; README mentions are ignored.",
    "pretrain_sources_used": false
  }
}