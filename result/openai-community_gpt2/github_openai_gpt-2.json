{
    "repo": "openai/gpt-2",
    "branch": "master",
    "files": [
        ".gitattributes",
        ".gitignore",
        "CONTRIBUTORS.md",
        "DEVELOPERS.md",
        "Dockerfile.cpu",
        "Dockerfile.gpu",
        "LICENSE",
        "README.md",
        "domains.txt",
        "download_model.py",
        "model_card.md",
        "requirements.txt",
        "src/encoder.py",
        "src/generate_unconditional_samples.py",
        "src/interactive_conditional_samples.py",
        "src/model.py",
        "src/sample.py"
    ],
    "license_files": {
        "LICENSE": "Modified MIT License\n\nSoftware Copyright (c) 2019 OpenAI\n\nWe don’t claim ownership of the content you create with GPT-2, so it is yours to do with as you please.\nWe only ask that you use GPT-2 responsibly and clearly indicate your content was created using GPT-2.\n\nPermission is hereby granted, free of charge, to any person obtaining a copy of this software and\nassociated documentation files (the \"Software\"), to deal in the Software without restriction,\nincluding without limitation the rights to use, copy, modify, merge, publish, distribute, sublicense,\nand/or sell copies of the Software, and to permit persons to whom the Software is furnished to do so,\nsubject to the following conditions:\n\nThe above copyright notice and this permission notice shall be included\nin all copies or substantial portions of the Software.\nThe above copyright notice and this permission notice need not be included\nwith content created by the Software.\n\nTHE SOFTWARE IS PROVIDED \"AS IS\", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR IMPLIED,\nINCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,\nFITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE AUTHORS OR COPYRIGHT HOLDERS\nBE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER LIABILITY, WHETHER IN AN ACTION OF CONTRACT,\nTORT OR OTHERWISE, ARISING FROM, OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE\nOR OTHER DEALINGS IN THE SOFTWARE.\n"
    },
    "readme": "**Status:** Archive (code is provided as-is, no updates expected)\n\n# gpt-2\n\nCode and models from the paper [\"Language Models are Unsupervised Multitask Learners\"](https://d4mucfpksywv.cloudfront.net/better-language-models/language-models.pdf).\n\nYou can read about GPT-2 and its staged release in our [original blog post](https://openai.com/research/better-language-models/), [6 month follow-up post](https://openai.com/blog/gpt-2-6-month-follow-up/), and [final post](https://www.openai.com/blog/gpt-2-1-5b-release/).\n\nWe have also [released a dataset](https://github.com/openai/gpt-2-output-dataset) for researchers to study their behaviors.\n\n<sup>*</sup> *Note that our original parameter counts were wrong due to an error (in our previous blog posts and paper).  Thus you may have seen small referred to as 117M and medium referred to as 345M.*\n\n## Usage\n\nThis repository is meant to be a starting point for researchers and engineers to experiment with GPT-2.\n\nFor basic information, see our [model card](./model_card.md).\n\n### Some caveats\n\n- GPT-2 models' robustness and worst case behaviors are not well-understood.  As with any machine-learned model, carefully evaluate GPT-2 for your use case, especially if used without fine-tuning or in safety-critical applications where reliability is important.\n- The dataset our GPT-2 models were trained on contains many texts with [biases](https://twitter.com/TomerUllman/status/1101485289720242177) and factual inaccuracies, and thus GPT-2 models are likely to be biased and inaccurate as well.\n- To avoid having samples mistaken as human-written, we recommend clearly labeling samples as synthetic before wide dissemination.  Our models are often incoherent or inaccurate in subtle ways, which takes more than a quick read for a human to notice.\n\n### Work with us\n\nPlease [let us know](mailto:languagequestions@openai.com) if you’re doing interesting research with or working on applications of GPT-2!  We’re especially interested in hearing from and potentially working with those who are studying\n- Potential malicious use cases and defenses against them (e.g. the detectability of synthetic text)\n- The extent of problematic content (e.g. bias) being baked into the models and effective mitigations\n\n## Development\n\nSee [DEVELOPERS.md](./DEVELOPERS.md)\n\n## Contributors\n\nSee [CONTRIBUTORS.md](./CONTRIBUTORS.md)\n\n## Citation\n\nPlease use the following bibtex entry:\n```\n@article{radford2019language,\n  title={Language Models are Unsupervised Multitask Learners},\n  author={Radford, Alec and Wu, Jeff and Child, Rewon and Luan, David and Amodei, Dario and Sutskever, Ilya},\n  year={2019}\n}\n```\n\n## Future work\n\nWe may release code for evaluating the models on various benchmarks.\n\nWe are still considering release of the larger models.\n\n## License\n\n[Modified MIT](./LICENSE)\n",
    "py_files": {
        "download_model.py": "import os\nimport sys\nimport requests\nfrom tqdm import tqdm\n\nif len(sys.argv) != 2:\n    print('You must enter the model name as a parameter, e.g.: download_model.py 124M')\n    sys.exit(1)\n\nmodel = sys.argv[1]\n\nsubdir = os.path.join('models', model)\nif not os.path.exists(subdir):\n    os.makedirs(subdir)\nsubdir = subdir.replace('\\\\','/') # needed for Windows\n\nfor filename in ['checkpoint','encoder.json','hparams.json','model.ckpt.data-00000-of-00001', 'model.ckpt.index', 'model.ckpt.meta', 'vocab.bpe']:\n\n    r = requests.get(\"https://openaipublic.blob.core.windows.net/gpt-2/\" + subdir + \"/\" + filename, stream=True)\n\n    with open(os.path.join(subdir, filename), 'wb') as f:\n        file_size = int(r.headers[\"content-length\"])\n        chunk_size = 1000\n        with tqdm(ncols=100, desc=\"Fetching \" + filename, total=file_size, unit_scale=True) as pbar:\n            # 1k for chunk_size, since Ethernet packet size is around 1500 bytes\n            for chunk in r.iter_content(chunk_size=chunk_size):\n                f.write(chunk)\n                pbar.update(chunk_size)\n",
        "src/encoder.py": "\"\"\"Byte pair encoding utilities\"\"\"\n\nimport os\nimport json\nimport regex as re\nfrom functools import lru_cache\n\n@lru_cache()\ndef bytes_to_unicode():\n    \"\"\"\n    Returns list of utf-8 byte and a corresponding list of unicode strings.\n    The reversible bpe codes work on unicode strings.\n    This means you need a large # of unicode characters in your vocab if you want to avoid UNKs.\n    When you're at something like a 10B token dataset you end up needing around 5K for decent coverage.\n    This is a signficant percentage of your normal, say, 32K bpe vocab.\n    To avoid that, we want lookup tables between utf-8 bytes and unicode strings.\n    And avoids mapping to whitespace/control characters the bpe code barfs on.\n    \"\"\"\n    bs = list(range(ord(\"!\"), ord(\"~\")+1))+list(range(ord(\"¡\"), ord(\"¬\")+1))+list(range(ord(\"®\"), ord(\"ÿ\")+1))\n    cs = bs[:]\n    n = 0\n    for b in range(2**8):\n        if b not in bs:\n            bs.append(b)\n            cs.append(2**8+n)\n            n += 1\n    cs = [chr(n) for n in cs]\n    return dict(zip(bs, cs))\n\ndef get_pairs(word):\n    \"\"\"Return set of symbol pairs in a word.\n\n    Word is represented as tuple of symbols (symbols being variable-length strings).\n    \"\"\"\n    pairs = set()\n    prev_char = word[0]\n    for char in word[1:]:\n        pairs.add((prev_char, char))\n        prev_char = char\n    return pairs\n\nclass Encoder:\n    def __init__(self, encoder, bpe_merges, errors='replace'):\n        self.encoder = encoder\n        self.decoder = {v:k for k,v in self.encoder.items()}\n        self.errors = errors # how to handle errors in decoding\n        self.byte_encoder = bytes_to_unicode()\n        self.byte_decoder = {v:k for k, v in self.byte_encoder.items()}\n        self.bpe_ranks = dict(zip(bpe_merges, range(len(bpe_merges))))\n        self.cache = {}\n\n        # Should haved added re.IGNORECASE so BPE merges can happen for capitalized versions of contractions\n        self.pat = re.compile(r\"\"\"'s|'t|'re|'ve|'m|'ll|'d| ?\\p{L}+| ?\\p{N}+| ?[^\\s\\p{L}\\p{N}]+|\\s+(?!\\S)|\\s+\"\"\")\n\n    def bpe(self, token):\n        if token in self.cache:\n            return self.cache[token]\n        word = tuple(token)\n        pairs = get_pairs(word)\n\n        if not pairs:\n            return token\n\n        while True:\n            bigram = min(pairs, key = lambda pair: self.bpe_ranks.get(pair, float('inf')))\n            if bigram not in self.bpe_ranks:\n                break\n            first, second = bigram\n            new_word = []\n            i = 0\n            while i < len(word):\n                try:\n                    j = word.index(first, i)\n                    new_word.extend(word[i:j])\n                    i = j\n                except:\n                    new_word.extend(word[i:])\n                    break\n\n                if word[i] == first and i < len(word)-1 and word[i+1] == second:\n                    new_word.append(first+second)\n                    i += 2\n                else:\n                    new_word.append(word[i])\n                    i += 1\n            new_word = tuple(new_word)\n            word = new_word\n            if len(word) == 1:\n                break\n            else:\n                pairs = get_pairs(word)\n        word = ' '.join(word)\n        self.cache[token] = word\n        return word\n\n    def encode(self, text):\n        bpe_tokens = []\n        for token in re.findall(self.pat, text):\n            token = ''.join(self.byte_encoder[b] for b in token.encode('utf-8'))\n            bpe_tokens.extend(self.encoder[bpe_token] for bpe_token in self.bpe(token).split(' '))\n        return bpe_tokens\n\n    def decode(self, tokens):\n        text = ''.join([self.decoder[token] for token in tokens])\n        text = bytearray([self.byte_decoder[c] for c in text]).decode('utf-8', errors=self.errors)\n        return text\n\ndef get_encoder(model_name, models_dir):\n    with open(os.path.join(models_dir, model_name, 'encoder.json'), 'r') as f:\n        encoder = json.load(f)\n    with open(os.path.join(models_dir, model_name, 'vocab.bpe'), 'r', encoding=\"utf-8\") as f:\n        bpe_data = f.read()\n    bpe_merges = [tuple(merge_str.split()) for merge_str in bpe_data.split('\\n')[1:-1]]\n    return Encoder(\n        encoder=encoder,\n        bpe_merges=bpe_merges,\n    )\n",
        "src/generate_unconditional_samples.py": "#!/usr/bin/env python3\n\nimport fire\nimport json\nimport os\nimport numpy as np\nimport tensorflow as tf\n\nimport model, sample, encoder\n\ndef sample_model(\n    model_name='124M',\n    seed=None,\n    nsamples=0,\n    batch_size=1,\n    length=None,\n    temperature=1,\n    top_k=0,\n    top_p=1,\n    models_dir='models',\n):\n    \"\"\"\n    Run the sample_model\n    :model_name=124M : String, which model to use\n    :seed=None : Integer seed for random number generators, fix seed to\n     reproduce results\n    :nsamples=0 : Number of samples to return, if 0, continues to\n     generate samples indefinately.\n    :batch_size=1 : Number of batches (only affects speed/memory).\n    :length=None : Number of tokens in generated text, if None (default), is\n     determined by model hyperparameters\n    :temperature=1 : Float value controlling randomness in boltzmann\n     distribution. Lower temperature results in less random completions. As the\n     temperature approaches zero, the model will become deterministic and\n     repetitive. Higher temperature results in more random completions.\n    :top_k=0 : Integer value controlling diversity. 1 means only 1 word is\n     considered for each step (token), resulting in deterministic completions,\n     while 40 means 40 words are considered at each step. 0 (default) is a\n     special setting meaning no restrictions. 40 generally is a good value.\n     :models_dir : path to parent folder containing model subfolders\n     (i.e. contains the <model_name> folder)\n    \"\"\"\n    models_dir = os.path.expanduser(os.path.expandvars(models_dir))\n    enc = encoder.get_encoder(model_name, models_dir)\n    hparams = model.default_hparams()\n    with open(os.path.join(models_dir, model_name, 'hparams.json')) as f:\n        hparams.override_from_dict(json.load(f))\n\n    if length is None:\n        length = hparams.n_ctx\n    elif length > hparams.n_ctx:\n        raise ValueError(\"Can't get samples longer than window size: %s\" % hparams.n_ctx)\n\n    with tf.Session(graph=tf.Graph()) as sess:\n        np.random.seed(seed)\n        tf.set_random_seed(seed)\n\n        output = sample.sample_sequence(\n            hparams=hparams, length=length,\n            start_token=enc.encoder['<|endoftext|>'],\n            batch_size=batch_size,\n            temperature=temperature, top_k=top_k, top_p=top_p\n        )[:, 1:]\n\n        saver = tf.train.Saver()\n        ckpt = tf.train.latest_checkpoint(os.path.join(models_dir, model_name))\n        saver.restore(sess, ckpt)\n\n        generated = 0\n        while nsamples == 0 or generated < nsamples:\n            out = sess.run(output)\n            for i in range(batch_size):\n                generated += batch_size\n                text = enc.decode(out[i])\n                print(\"=\" * 40 + \" SAMPLE \" + str(generated) + \" \" + \"=\" * 40)\n                print(text)\n\nif __name__ == '__main__':\n    fire.Fire(sample_model)\n\n",
        "src/interactive_conditional_samples.py": "#!/usr/bin/env python3\n\nimport fire\nimport json\nimport os\nimport numpy as np\nimport tensorflow as tf\n\nimport model, sample, encoder\n\ndef interact_model(\n    model_name='124M',\n    seed=None,\n    nsamples=1,\n    batch_size=1,\n    length=None,\n    temperature=1,\n    top_k=0,\n    top_p=1,\n    models_dir='models',\n):\n    \"\"\"\n    Interactively run the model\n    :model_name=124M : String, which model to use\n    :seed=None : Integer seed for random number generators, fix seed to reproduce\n     results\n    :nsamples=1 : Number of samples to return total\n    :batch_size=1 : Number of batches (only affects speed/memory).  Must divide nsamples.\n    :length=None : Number of tokens in generated text, if None (default), is\n     determined by model hyperparameters\n    :temperature=1 : Float value controlling randomness in boltzmann\n     distribution. Lower temperature results in less random completions. As the\n     temperature approaches zero, the model will become deterministic and\n     repetitive. Higher temperature results in more random completions.\n    :top_k=0 : Integer value controlling diversity. 1 means only 1 word is\n     considered for each step (token), resulting in deterministic completions,\n     while 40 means 40 words are considered at each step. 0 (default) is a\n     special setting meaning no restrictions. 40 generally is a good value.\n     :models_dir : path to parent folder containing model subfolders\n     (i.e. contains the <model_name> folder)\n    \"\"\"\n    models_dir = os.path.expanduser(os.path.expandvars(models_dir))\n    if batch_size is None:\n        batch_size = 1\n    assert nsamples % batch_size == 0\n\n    enc = encoder.get_encoder(model_name, models_dir)\n    hparams = model.default_hparams()\n    with open(os.path.join(models_dir, model_name, 'hparams.json')) as f:\n        hparams.override_from_dict(json.load(f))\n\n    if length is None:\n        length = hparams.n_ctx // 2\n    elif length > hparams.n_ctx:\n        raise ValueError(\"Can't get samples longer than window size: %s\" % hparams.n_ctx)\n\n    with tf.Session(graph=tf.Graph()) as sess:\n        context = tf.placeholder(tf.int32, [batch_size, None])\n        np.random.seed(seed)\n        tf.set_random_seed(seed)\n        output = sample.sample_sequence(\n            hparams=hparams, length=length,\n            context=context,\n            batch_size=batch_size,\n            temperature=temperature, top_k=top_k, top_p=top_p\n        )\n\n        saver = tf.train.Saver()\n        ckpt = tf.train.latest_checkpoint(os.path.join(models_dir, model_name))\n        saver.restore(sess, ckpt)\n\n        while True:\n            raw_text = input(\"Model prompt >>> \")\n            while not raw_text:\n                print('Prompt should not be empty!')\n                raw_text = input(\"Model prompt >>> \")\n            context_tokens = enc.encode(raw_text)\n            generated = 0\n            for _ in range(nsamples // batch_size):\n                out = sess.run(output, feed_dict={\n                    context: [context_tokens for _ in range(batch_size)]\n                })[:, len(context_tokens):]\n                for i in range(batch_size):\n                    generated += 1\n                    text = enc.decode(out[i])\n                    print(\"=\" * 40 + \" SAMPLE \" + str(generated) + \" \" + \"=\" * 40)\n                    print(text)\n            print(\"=\" * 80)\n\nif __name__ == '__main__':\n    fire.Fire(interact_model)\n\n",
        "src/model.py": "import numpy as np\nimport tensorflow as tf\nfrom tensorflow.contrib.training import HParams\n\ndef default_hparams():\n    return HParams(\n        n_vocab=0,\n        n_ctx=1024,\n        n_embd=768,\n        n_head=12,\n        n_layer=12,\n    )\n\ndef shape_list(x):\n    \"\"\"Deal with dynamic shape in tensorflow cleanly.\"\"\"\n    static = x.shape.as_list()\n    dynamic = tf.shape(x)\n    return [dynamic[i] if s is None else s for i, s in enumerate(static)]\n\ndef softmax(x, axis=-1):\n    x = x - tf.reduce_max(x, axis=axis, keepdims=True)\n    ex = tf.exp(x)\n    return ex / tf.reduce_sum(ex, axis=axis, keepdims=True)\n\ndef gelu(x):\n    return 0.5*x*(1+tf.tanh(np.sqrt(2/np.pi)*(x+0.044715*tf.pow(x, 3))))\n\ndef norm(x, scope, *, axis=-1, epsilon=1e-5):\n    \"\"\"Normalize to mean = 0, std = 1, then do a diagonal affine transform.\"\"\"\n    with tf.variable_scope(scope):\n        n_state = x.shape[-1].value\n        g = tf.get_variable('g', [n_state], initializer=tf.constant_initializer(1))\n        b = tf.get_variable('b', [n_state], initializer=tf.constant_initializer(0))\n        u = tf.reduce_mean(x, axis=axis, keepdims=True)\n        s = tf.reduce_mean(tf.square(x-u), axis=axis, keepdims=True)\n        x = (x - u) * tf.rsqrt(s + epsilon)\n        x = x*g + b\n        return x\n\ndef split_states(x, n):\n    \"\"\"Reshape the last dimension of x into [n, x.shape[-1]/n].\"\"\"\n    *start, m = shape_list(x)\n    return tf.reshape(x, start + [n, m//n])\n\ndef merge_states(x):\n    \"\"\"Smash the last two dimensions of x into a single dimension.\"\"\"\n    *start, a, b = shape_list(x)\n    return tf.reshape(x, start + [a*b])\n\ndef conv1d(x, scope, nf, *, w_init_stdev=0.02):\n    with tf.variable_scope(scope):\n        *start, nx = shape_list(x)\n        w = tf.get_variable('w', [1, nx, nf], initializer=tf.random_normal_initializer(stddev=w_init_stdev))\n        b = tf.get_variable('b', [nf], initializer=tf.constant_initializer(0))\n        c = tf.reshape(tf.matmul(tf.reshape(x, [-1, nx]), tf.reshape(w, [-1, nf]))+b, start+[nf])\n        return c\n\ndef attention_mask(nd, ns, *, dtype):\n    \"\"\"1's in the lower triangle, counting from the lower right corner.\n\n    Same as tf.matrix_band_part(tf.ones([nd, ns]), -1, ns-nd), but doesn't produce garbage on TPUs.\n    \"\"\"\n    i = tf.range(nd)[:,None]\n    j = tf.range(ns)\n    m = i >= j - ns + nd\n    return tf.cast(m, dtype)\n\n\ndef attn(x, scope, n_state, *, past, hparams):\n    assert x.shape.ndims == 3  # Should be [batch, sequence, features]\n    assert n_state % hparams.n_head == 0\n    if past is not None:\n        assert past.shape.ndims == 5  # Should be [batch, 2, heads, sequence, features], where 2 is [k, v]\n\n    def split_heads(x):\n        # From [batch, sequence, features] to [batch, heads, sequence, features]\n        return tf.transpose(split_states(x, hparams.n_head), [0, 2, 1, 3])\n\n    def merge_heads(x):\n        # Reverse of split_heads\n        return merge_states(tf.transpose(x, [0, 2, 1, 3]))\n\n    def mask_attn_weights(w):\n        # w has shape [batch, heads, dst_sequence, src_sequence], where information flows from src to dst.\n        _, _, nd, ns = shape_list(w)\n        b = attention_mask(nd, ns, dtype=w.dtype)\n        b = tf.reshape(b, [1, 1, nd, ns])\n        w = w*b - tf.cast(1e10, w.dtype)*(1-b)\n        return w\n\n    def multihead_attn(q, k, v):\n        # q, k, v have shape [batch, heads, sequence, features]\n        w = tf.matmul(q, k, transpose_b=True)\n        w = w * tf.rsqrt(tf.cast(v.shape[-1].value, w.dtype))\n\n        w = mask_attn_weights(w)\n        w = softmax(w)\n        a = tf.matmul(w, v)\n        return a\n\n    with tf.variable_scope(scope):\n        c = conv1d(x, 'c_attn', n_state*3)\n        q, k, v = map(split_heads, tf.split(c, 3, axis=2))\n        present = tf.stack([k, v], axis=1)\n        if past is not None:\n            pk, pv = tf.unstack(past, axis=1)\n            k = tf.concat([pk, k], axis=-2)\n            v = tf.concat([pv, v], axis=-2)\n        a = multihead_attn(q, k, v)\n        a = merge_heads(a)\n        a = conv1d(a, 'c_proj', n_state)\n        return a, present\n\n\ndef mlp(x, scope, n_state, *, hparams):\n    with tf.variable_scope(scope):\n        nx = x.shape[-1].value\n        h = gelu(conv1d(x, 'c_fc', n_state))\n        h2 = conv1d(h, 'c_proj', nx)\n        return h2\n\n\ndef block(x, scope, *, past, hparams):\n    with tf.variable_scope(scope):\n        nx = x.shape[-1].value\n        a, present = attn(norm(x, 'ln_1'), 'attn', nx, past=past, hparams=hparams)\n        x = x + a\n        m = mlp(norm(x, 'ln_2'), 'mlp', nx*4, hparams=hparams)\n        x = x + m\n        return x, present\n\ndef past_shape(*, hparams, batch_size=None, sequence=None):\n    return [batch_size, hparams.n_layer, 2, hparams.n_head, sequence, hparams.n_embd // hparams.n_head]\n\ndef expand_tile(value, size):\n    \"\"\"Add a new axis of given size.\"\"\"\n    value = tf.convert_to_tensor(value, name='value')\n    ndims = value.shape.ndims\n    return tf.tile(tf.expand_dims(value, axis=0), [size] + [1]*ndims)\n\ndef positions_for(tokens, past_length):\n    batch_size = tf.shape(tokens)[0]\n    nsteps = tf.shape(tokens)[1]\n    return expand_tile(past_length + tf.range(nsteps), batch_size)\n\n\ndef model(hparams, X, past=None, scope='model', reuse=False):\n    with tf.variable_scope(scope, reuse=reuse):\n        results = {}\n        batch, sequence = shape_list(X)\n\n        wpe = tf.get_variable('wpe', [hparams.n_ctx, hparams.n_embd],\n                             initializer=tf.random_normal_initializer(stddev=0.01))\n        wte = tf.get_variable('wte', [hparams.n_vocab, hparams.n_embd],\n                             initializer=tf.random_normal_initializer(stddev=0.02))\n        past_length = 0 if past is None else tf.shape(past)[-2]\n        h = tf.gather(wte, X) + tf.gather(wpe, positions_for(X, past_length))\n\n        # Transformer\n        presents = []\n        pasts = tf.unstack(past, axis=1) if past is not None else [None] * hparams.n_layer\n        assert len(pasts) == hparams.n_layer\n        for layer, past in enumerate(pasts):\n            h, present = block(h, 'h%d' % layer, past=past, hparams=hparams)\n            presents.append(present)\n        results['present'] = tf.stack(presents, axis=1)\n        h = norm(h, 'ln_f')\n\n        # Language model loss.  Do tokens <n predict token n?\n        h_flat = tf.reshape(h, [batch*sequence, hparams.n_embd])\n        logits = tf.matmul(h_flat, wte, transpose_b=True)\n        logits = tf.reshape(logits, [batch, sequence, hparams.n_vocab])\n        results['logits'] = logits\n        return results\n",
        "src/sample.py": "import tensorflow as tf\n\nimport model\n\ndef top_k_logits(logits, k):\n    if k == 0:\n        # no truncation\n        return logits\n\n    def _top_k():\n        values, _ = tf.nn.top_k(logits, k=k)\n        min_values = values[:, -1, tf.newaxis]\n        return tf.where(\n            logits < min_values,\n            tf.ones_like(logits, dtype=logits.dtype) * -1e10,\n            logits,\n        )\n    return tf.cond(\n       tf.equal(k, 0),\n       lambda: logits,\n       lambda: _top_k(),\n    )\n\n\ndef top_p_logits(logits, p):\n    \"\"\"Nucleus sampling\"\"\"\n    batch, _ = logits.shape.as_list()\n    sorted_logits = tf.sort(logits, direction='DESCENDING', axis=-1)\n    cumulative_probs = tf.cumsum(tf.nn.softmax(sorted_logits, axis=-1), axis=-1)\n    indices = tf.stack([\n        tf.range(0, batch),\n        # number of indices to include\n        tf.maximum(tf.reduce_sum(tf.cast(cumulative_probs <= p, tf.int32), axis=-1) - 1, 0),\n    ], axis=-1)\n    min_values = tf.gather_nd(sorted_logits, indices)\n    return tf.where(\n        logits < min_values,\n        tf.ones_like(logits) * -1e10,\n        logits,\n    )\n\n\ndef sample_sequence(*, hparams, length, start_token=None, batch_size=None, context=None, temperature=1, top_k=0, top_p=1):\n    if start_token is None:\n        assert context is not None, 'Specify exactly one of start_token and context!'\n    else:\n        assert context is None, 'Specify exactly one of start_token and context!'\n        context = tf.fill([batch_size, 1], start_token)\n\n    def step(hparams, tokens, past=None):\n        lm_output = model.model(hparams=hparams, X=tokens, past=past, reuse=tf.AUTO_REUSE)\n\n        logits = lm_output['logits'][:, :, :hparams.n_vocab]\n        presents = lm_output['present']\n        presents.set_shape(model.past_shape(hparams=hparams, batch_size=batch_size))\n        return {\n            'logits': logits,\n            'presents': presents,\n        }\n\n    with tf.name_scope('sample_sequence'):\n        def body(past, prev, output):\n            next_outputs = step(hparams, prev, past=past)\n            logits = next_outputs['logits'][:, -1, :]  / tf.to_float(temperature)\n            logits = top_k_logits(logits, k=top_k)\n            logits = top_p_logits(logits, p=top_p)\n            samples = tf.multinomial(logits, num_samples=1, output_dtype=tf.int32)\n            return [\n                next_outputs['presents'] if past is None else tf.concat([past, next_outputs['presents']], axis=-2),\n                samples,\n                tf.concat([output, samples], axis=1)\n            ]\n\n        past, prev, output = body(None, context, context)\n\n        def cond(*args):\n            return True\n\n        _, _, tokens = tf.while_loop(\n            cond=cond, body=body,\n            maximum_iterations=length - 1,\n            loop_vars=[\n                past,\n                prev,\n                output\n            ],\n            shape_invariants=[\n                tf.TensorShape(model.past_shape(hparams=hparams, batch_size=batch_size)),\n                tf.TensorShape([batch_size, None]),\n                tf.TensorShape([batch_size, None]),\n            ],\n            back_prop=False,\n        )\n\n        return tokens\n"
    }
}