{
  "1-1 (Weights)": "The available statements repeatedly emphasize that the full HyperCLOVA X THINK weights are not yet public, but that an open-source release is actively being prepared. Authors say that they \"plan to open-source a pruned and distilled version of HyperCLOVA X THINK\" and that such a model is \"under preparation to be open-sourced.\" They explain that the enabling technology is a \"pruning and distillation technique\" which they \"will soon [apply] to HyperCLOVA X THINK for an open-source and business-friendly foundation model.\"  As proof-of-concept, other family members have already been published on Hugging Face: \"HyperCLOVA X SEED 0.5B, recently released on HuggingFace, is the first open-source model in the HyperCLOVA X series,\" and the larger \"HyperCLOVA X SEED Vision Instruct 3B\" is likewise \"Available on Hugging Face Hub\" at https://huggingface.co/naver-hyperclovax/HyperCLOVAX-SEED-Vision-Instruct-3B.  Together, the quotes show: (1) no public checkpoint yet for THINK, (2) a concrete plan to publish a smaller pruned/distilled THINK variant, (3) precedent of earlier SEED checkpoints already downloadable from Hugging Face, demonstrating that the project uses that platform to host weights.",
  "1-2 (Code)": "Only one explicit statement addresses training code. The authors say they \"share a practical pruning-distillation pipeline and commit to apply it for an open-source version of THINK—fostering further research and commercial deployment, even under more resource-constrained settings.\"  This indicates that the code for the pruning/distillation stage of training (not merely inference) is intended to be published, but no files, repositories, or configuration specifics are yet named in the material provided.",
  "1-3 (License)": "Licensing details are still high-level and prospective. The same sentence that outlines release plans for the pruned THINK model also clarifies intent: \"We plan to open-source release this model under a business-friendly license.\"  The phrase \"business-friendly\" implies that commercial use will be permitted, but no formal license name, version, or clause text has been disclosed in the supplied excerpts.",
  "1-4 (Paper)": "The official technical report is already on arXiv: “arXiv:2506.22403v2 [cs.CL]  1 Jul 2025  HyperCLOVA X THINK.”  Throughout the text the authors describe the work: “We introduce HyperCLOVA X THINK, the first reasoning-focused large language model in the HyperCLOVA X family,\" highlighting that it is \"pre-trained on roughly 6 trillion high-quality Korean and English tokens, augmented with targeted synthetic Korean data.\"  They summarize that in this report they \"introduced HyperCLOVA X THINK, the first reasoning-focused LLM within [the] HyperCLOVA X family.\"  These quotes confirm both the existence of a dedicated paper and outline its scope—emphasizing reasoning objectives, massive bilingual token counts, and the model’s position as the flagship reasoning model in the HyperCLOVA X line.",
  "1-1 (Weights)__evidence": [
    {
      "source": "[pdf_text]",
      "quote": "Lastly, we present a pruning and distillation technique that will soon be applied to HyperCLOVA X THINK for an open-source and business-friendly foundation model."
    },
    {
      "source": "[pdf_text]",
      "quote": "This technique will soon be applied to THINK itself to produce a model suitable for limited resource settings. We plan to open-source release this model under a business-friendly license."
    },
    {
      "source": "[pdf_text]",
      "quote": "HyperCLOVA X SEED 0.5B, recently released on HuggingFace, is the first open-source model in the HyperCLOVA X series trained using pruning and knowledge distillation."
    },
    {
      "source": "[pdf_text]",
      "quote": "Lastly, we plan to open-source a pruned and distilled version of HyperCLOVA X THINK."
    },
    {
      "source": "[pdf_text]",
      "quote": "NAVER Cloud HyperCLOVA X Team. 2024. HyperCLOVA X SEED Vision Instruct 3B. https://huggingface.co/naver-hyperclovax/HyperCLOVAX-SEED-Vision-Instruct-3B. Available on Hugging Face Hub."
    },
    {
      "source": "[sections/https://arxiv.org/pdf/2506.22403]",
      "quote": "Lastly, we present a pruning and distillation technique that will soon be applied to HyperCLOVA X THINK for an open-source and business-friendly foundation model."
    },
    {
      "source": "[sections/https://arxiv.org/pdf/2506.22403]",
      "quote": "To ensure that academic and industry partners can benefit from the model, we introduce a pruning- and-distillation recipe that reduces parameter count while preserving accuracy. This technique will soon be applied to THINK itself to produce a model suitable for limited resource settings. We plan to open-source release this model under a business-friendly license."
    },
    {
      "source": "[pdf_text]",
      "quote": "As a real-world example, HyperCLOVA X SEED 0.5B, recently released on HuggingFace, is the first open-source model in the HyperCLOVA X series trained using pruning and knowledge distillation."
    },
    {
      "source": "[pdf_text]",
      "quote": "Currently, a pruned and distilled version of THINK is under preparation to be open-sourced."
    }
  ],
  "1-2 (Code)__evidence": [
    {
      "source": "[sections/https://arxiv.org/pdf/2506.22403]",
      "quote": "Third, we share a practical pruning-distillation pipeline and commit to apply it for an open-source version of THINK—fostering further research and commercial deployment, even under more resource-constrained settings."
    }
  ],
  "1-3 (License)__evidence": [
    {
      "source": "[pdf_text]",
      "quote": "This technique will soon be applied to THINK itself to produce a model suitable for limited resource settings. We plan to open-source release this model under a business-friendly license."
    },
    {
      "source": "[web:https://www.bigcode-project.org/docs/pages/model-license/]",
      "quote": "https://www.bigcode-project.org/docs/pages/model-license/"
    },
    {
      "source": "[web:https://huggingface.co/tiiuae/falcon-40b/blob/main/LICENSE]",
      "quote": "https://huggingface.co/tiiuae/falcon-40b/blob/main/LICENSE"
    }
  ],
  "1-4 (Paper)__evidence": [
    {
      "source": "[pdf_text]",
      "quote": "arXiv:2506.22403v2  [cs.CL]  1 Jul 2025\nHyperCLOVA X THINK"
    },
    {
      "source": "[pdf_text]",
      "quote": "We introduce HyperCLOVA X THINK, the first reasoning-focused large language model in the HyperCLOVA X family, pre-trained on roughly 6 trillion high-quality Korean, and English tokens, augmented with targeted synthetic Korean data."
    },
    {
      "source": "[sections/https://arxiv.org/pdf/2506.22403]",
      "quote": "We introduce HyperCLOVA X THINK, the first reasoning-focused large language model in the HyperCLOVA X family, pre-trained on roughly 6 trillion high-quality Korean, and English tokens, augmented with targeted synthetic Korean data."
    },
    {
      "source": "[pdf_text]",
      "quote": "In this report, we introduced HyperCLOVA X THINK, the first reasoning-focused LLM within HyperCLOVA X family."
    }
  ],
  "1-5 (Architecture)": "The available information on the architecture comes from two passages that explicitly name the target‐family tokens “THINK” and “SEED.”  First, we are told that “HyperCLOVA X THINK” is a reasoning-focused large language model that belongs to the HyperCLOVA X family.  It is implemented as “a compute-memory-balanced Peri-LN Transformer scaled with µP,” and it is “pre-trained through a three-stage curriculum that expands the context window to 128 K tokens.”  The same sentence notes that roughly “6 trillion high-quality Korean and English tokens,” augmented with targeted synthetic Korean data, are used in pre-training.  After this curriculum, the model is “post-trained via supervised fine-tuning with Reinforcement Learning from Verifiable Rewards,” enabling it to operate in “detailed rationale” or “concise-answer” modes.  Second, another sentence highlights a related model in the family: “HyperCLOVA X SEED 0.5B … is the first open-source model in the HyperCLOVA X series trained using pruning and knowledge distillation.”  Together, these quotes establish that the THINK variant relies on a Peri-LN Transformer architecture with µP scaling, very large context lengths (up to 128 K tokens), and a curriculum/post-training pipeline aimed at high-quality reasoning, while the SEED sub-model showcases the family’s use of pruning and distillation techniques.",
  "1-6 (Tokenizer)": "",
  "2-1 (Hardware)": "The only explicit hardware detail appears in the sentence that accompanies Figure 3 and names the model: during the post-training phase of “HyperCLOVA X THINK,” “a sequence of fine-tuning procedures … is executed across a large-scale GPU cluster.”  No additional numbers or GPU classes are provided, but the quote confirms that a sizable, multi-GPU setup is employed for the training/fine-tuning stages that follow pre-training.",
  "2-2 (Software)": "For the software‐side training procedures, the quote that references “THINK” explains that pre-training follows “a three-stage curriculum” which gradually grows the context window until it reaches 128 K tokens, allowing the model to process long documents and perform multi-step reasoning in a single pass.  After this curriculum, the post-training stack combines “supervised fine-tuning on carefully designed reasoning tasks” with “Reinforcement Learning from Verifiable Rewards.”  These two ingredients—SFT and RL with verifiable reward signals—form the core of the software pipeline that adapts the pre-trained model into its final reasoning-capable form.",
  "1-5 (Architecture)__evidence": [
    {
      "source": "[pdf_text]",
      "quote": "We introduce HyperCLOVA X THINK, the first reasoning-focused large language model in the HyperCLOVA X family, pre-trained on roughly 6 trillion high-quality Korean, and English tokens, augmented with targeted synthetic Korean data. It was implemented as a compute-memory-balanced Peri-LN Transformer scaled with µP, pre-trained through a three-stage curriculum that expands the context window to 128K tokens, and post-trained via supervised fine-tuning with Rein- forcement Learning from Verifiable Rewards supports both detailed rationale and concise-answer modes."
    },
    {
      "source": "[pdf_text]",
      "quote": "As illustrated in Figure 2, the Peri-LN model exhibits fewer gradient and loss spikes than its Pre-LN counterpart, reproducing the large-scale stability benefits reported by Kim et al. (2025). Furthermore, the Peri-LN configuration attains, on average, a 15 % lower training loss within the same wall-clock budget. These findings confirm that Peri-LN delivers superior stability and performance without incurring additional computational cost, and thus we adopt it as the default normalization scheme in the THINK architecture."
    },
    {
      "source": "[sections/https://arxiv.org/pdf/2506.22403]",
      "quote": "We introduce HyperCLOVA X THINK, the first reasoning-focused large language model in the HyperCLOVA X family, pre-trained on roughly 6 trillion high-quality Korean, and English tokens, augmented with targeted synthetic Korean data. It was implemented as a compute-memory-balanced Peri-LN Transformer scaled with µP, pre-trained through a three-stage curriculum that expands the context window to 128K tokens, and post-trained via supervised fine-tuning with Rein- forcement Learning from Verifiable Rewards supports both detailed rationale and concise-answer modes."
    },
    {
      "source": "[pdf_text]",
      "quote": "As a real-world example, HyperCLOVA X SEED 0.5B, recently released on HuggingFace, is the\nfirst open-source model in the HyperCLOVA X series trained using pruning and knowledge distil-\nlation."
    }
  ],
  "1-6 (Tokenizer)__evidence": [],
  "2-1 (Hardware)__evidence": [
    {
      "source": "[sections/https://arxiv.org/pdf/2506.22403]",
      "quote": "Figure 3: Post-training pipeline of HyperCLOVA X THINK. ... (2) Training Phase: A sequence of fine-tuning procedures—including Su- pervised Fine-Tuning (SFT), Reward Modeling (RM), Reinforcement Learning with Verifiable Re- wards (RLVR), training for reasoning Length Controllability (LC), and Reinforcement Learning from Human Feedback (RLHF) —is executed across a large-scale GPU cluster."
    }
  ],
  "2-2 (Software)__evidence": [
    {
      "source": "[sections/https://arxiv.org/pdf/2506.22403]",
      "quote": "During pre-training, A three-stage curriculum gradually increases the context window, culminating in 128k tokens, which enables THINK to process long documents and perform multi-step reason- ing within a single pass. Then, for post-training, we combine supervised fine-tuning on carefully designed reasoning tasks with Reinforcement Learning from Verifiable Rewards."
    }
  ],
  "2-3 (API)": "",
  "3-1 (Pre-training)": "The pre-training of HyperCLOVA X THINK is built around a deliberately Korean-centric, scalable data pipeline that is further strengthened with targeted synthetic corpora. The model adopts a compute- and memory-efficient Transformer architecture whose stability is promoted through scale-invariant parameterization principles. Training progresses through a three-stage curriculum: the first stage builds basic linguistic knowledge, the second refines that knowledge with higher-fidelity data, and the third expands the model’s contextual capacity so that it can carry out long-form reasoning. Across the curriculum, the context window is expanded step-wise until it reaches 128 K tokens, enabling the model to ingest very long documents and complete multi-step reasoning in a single forward pass. HyperCLOVA X THINK is the first reasoning-focused member of the HyperCLOVA X family and is pre-trained on roughly six trillion high-quality tokens in Korean and English, further augmented by specially generated synthetic Korean data.",
  "3-2 (Fine-tuning)": "For post-training, HyperCLOVA X THINK follows a two-part strategy. First, a supervised fine-tuning (SFT) phase explicitly injects core reasoning patterns and a variety of task-specific skills. Second, a multi-stage reinforcement-learning pipeline is applied; it brings in verifiable reward signals, mechanisms for controlling output length, and rounds of human feedback so that the final model is both efficient and well aligned. Once fine-tuned, THINK can seamlessly toggle between a verbose ‘reasoning mode’—suitable for elaborate, multi-step problems—and a concise ‘non-reasoning mode’ for faster, context-driven answers. Within the same family, HyperCLOVA X SEED 0.5 B has already been released on HuggingFace after undergoing pruning and knowledge-distillation, and a similarly pruned, distilled version of THINK is currently being readied for open-source release.",
  "3-3 (Reinforcement Learning)": "Reinforcement-learning is tightly integrated into THINK’s post-training pipeline. After the SFT seed stage, the model is subjected to a multi-stage RL process that combines verifiable rewards, explicit penalties or bonuses to steer output length, and iterative human feedback for alignment and safety. To make length controllability (LC) a native capability, the training incorporates the length-penalized reward functions proposed by Aggarwal and Welleck (2025). This combination of verifiable, length-aware rewards and human-in-the-loop tuning is aimed at producing an aligned, efficient, and scalable reasoner.",
  "2-3 (API)__evidence": [],
  "3-1 (Pre-training)__evidence": [
    {
      "source": "[pdf_text]",
      "quote": "This section outlines the pre-training methodology behind THINK: a scalable, Korean-centric data pipeline enriched with targeted synthetic corpora (Section 2.1); a compute–memory-efficient yet stability-oriented Transformer, instantiated with scale-invariant parameterization principles (Section 2.2); and a three-stage curriculum that sequentially builds foundational linguistic knowledge, refines competence with higher-fidelity data, and expands contextual capacity to support long-form reasoning (Section 2.3)."
    },
    {
      "source": "[pdf_text]",
      "quote": "During pre-training, A three-stage curriculum gradually increases the context window, culminating in 128K tokens, which enables THINK to process long documents and perform multi-step reasoning within a single pass."
    },
    {
      "source": "[pdf_text]",
      "quote": "Each model is trained on 30 billion tokens. To ensure reliable validation, we pre-train each model with five different training seeds in all experiments."
    },
    {
      "source": "[pdf_text]",
      "quote": "In particular, when we sweep over training seeds and learning rates, Pre-LN frequently exhibits spikes in the gradient-norm curve, whereas Peri-LN shows comparatively few, thereby supporting Proposition 3.1."
    },
    {
      "source": "[pdf_text]",
      "quote": "In most of our experiments across different random seeds, the Pre-LN architecture exhibited early-stage instability. Although we initially suspected that a high learning rate might be the root cause, lowering it did not substantially mitigate these issues."
    },
    {
      "source": "[sections/https://arxiv.org/pdf/2506.22403]",
      "quote": "We introduce HyperCLOVA X THINK, the first reasoning-focused large language model in the HyperCLOVA X family, pre-trained on roughly 6 trillion high-quality Korean, and English tokens, augmented with targeted synthetic Korean data."
    },
    {
      "source": "[sections/https://arxiv.org/pdf/2506.22403]",
      "quote": "This section outlines the pre-training methodology behind THINK: a scalable, Korean-centric data pipeline enriched with targeted synthetic corpora (Section 2.1); a compute–memory-efficient yet stability-oriented Transformer, instantiated with scale-invariant parameterization principles (Section 2.2); and a three-stage curriculum that sequentially builds foundational linguistic knowledge, refines competence with higher-fidelity data, and expands contextual capacity to support long-form reasoning (Section 2.3)."
    },
    {
      "source": "[pdf_text]",
      "quote": "In this report, we introduced HyperCLOVA X THINK, the first reasoning-focused LLM within HyperCLOVA X family. Its pre-training dataset comprises approximately 6 trillion high-quality tokens spanning Korean, English, and further enhanced by targeted synthetic Korean data."
    }
  ],
  "3-2 (Fine-tuning)__evidence": [
    {
      "source": "[pdf_text]",
      "quote": "This section outlines the post-training methodology of THINK: a supervised fine-tuning (SFT) phase that injects core reasoning patterns and task-specific capabilities (Section 3.1); and a multi-stage reinforcement learning pipeline that incorporates verifiable rewards, length-controllability, and human feedback to achieve aligned, efficient, and scalable reasoning (Sections 3.2–3.4)."
    },
    {
      "source": "[sections/https://arxiv.org/pdf/2506.22403]",
      "quote": "This section outlines the post-training methodology of THINK: a supervised fine-tuning (SFT) phase that injects core reasoning patterns and task-specific capabilities (Section 3.1); and a multi-stage reinforcement learning pipeline that incorporates verifiable rewards, length-controllability, and human feedback to achieve aligned, efficient, and scalable reasoning (Sections 3.2–3.4)."
    },
    {
      "source": "[sections/https://arxiv.org/pdf/2506.22403]",
      "quote": "THINK is trained to operate in an integrated manner, allowing for dynamic switching between a detailed ‘reasoning mode’ for complex, multi-step reasoning and a more direct ‘non-reasoning mode’ for rapid, context-driven responses."
    },
    {
      "source": "[pdf_text]",
      "quote": "As a real-world example, HyperCLOVA X SEED 0.5B, recently released on HuggingFace, is the first open-source model in the HyperCLOVA X series trained using pruning and knowledge distillation."
    },
    {
      "source": "[pdf_text]",
      "quote": "Currently, a pruned and distilled version of THINK is under preparation to be open-sourced."
    }
  ],
  "3-3 (Reinforcement Learning)__evidence": [
    {
      "source": "[pdf_text]",
      "quote": "This section outlines the post-training methodology of THINK: a supervised fine-tuning (SFT) phase that injects core reasoning patterns and task-specific capabilities (Section 3.1); and a multi-stage reinforcement learning pipeline that incorporates verifiable rewards, length-controllability, and human feedback to achieve aligned, efficient, and scalable reasoning (Sections 3.2–3.4)."
    },
    {
      "source": "[sections/https://arxiv.org/pdf/2506.22403]",
      "quote": "To induce LC in HyperCLOVA X THINK, we additionally incorporate the length-penalized reward functions introduced by Aggarwal and Welleck, 2025."
    }
  ],
  "4-1 (Pre-training Data)": "The pre-training of the target model family is centered on HyperCLOVA X THINK, described as “the first reasoning-focused large language model in the HyperCLOVA X family.”  According to the disclosure, the model is “pre-trained on roughly 6 trillion high-quality Korean and English tokens, augmented with targeted synthetic Korean data.”  The language mix therefore spans at least two natural-language sources (Korean and English) plus an additional tranche of synthetic Korean material generated specifically to strengthen coverage of that language.  During the pre-training stage, “each model was trained under a controlled random seed.”  Progress is monitored at “iteration 14,000—corresponding to the completion of 30 B tokens,” whose training loss is used as the principal reference point.  A smaller experimental configuration is also mentioned: “We pre-train the 400 M-parameter Transformers on 30 B tokens each under the controlled same training seed.”  Although individual corpus titles or licensing terms are not enumerated, the quotes jointly specify (i) overall scale (≈6 T tokens), (ii) language composition (Korean, English, synthetic Korean), (iii) the existence of strict seeding for reproducibility, and (iv) a 30 B-token milestone that anchors empirical evaluation during the pre-training run.",
  "4-2 (Fine-tuning Data)": "Fine-tuning is situated in a broader “post-training pipeline” and begins with a supervised fine-tuning (SFT) phase.  The pipeline explicitly enumerates the data-driven stages that follow: “Supervised Fine-Tuning (SFT), Reward Modeling (RM), Reinforcement Learning with Verifiable Rewards (RLVR), training for reasoning Length Controllability (LC), and Reinforcement Learning from Human Feedback (RLHF).”  A second statement reiterates that THINK is “post-trained via supervised fine-tuning with Reinforcement Learning from Verifiable Rewards,” and emphasizes that the resulting model can operate in “both detailed rationale and concise-answer modes.”  Together, the quotes establish that fine-tuning datasets are purpose-built for (a) injecting core reasoning patterns, (b) supplying reward-model supervision, and (c) enabling controllable answer length.  They further clarify that the data are processed on “a large-scale GPU cluster,” implying sizeable dataset volume, although exact token counts, public availability, or individual dataset names are not provided.",
  "4-3 (Reinforcement Learning Data)": "After the initial SFT stage, reinforcement learning data shape the model through a “multi-stage reinforcement learning pipeline that incorporates verifiable rewards, length-controllability, and human feedback.”  The curriculum begins with supervised fine-tuning “on carefully designed reasoning tasks,” then transitions to reinforcement learning where the reward signal is explicitly described as “verifiable.”  Human-authored feedback is folded in via RLHF, and additional stages enforce explicit control over answer length.  These data collectively aim to “achieve aligned, efficient, and scalable reasoning.”  While no numeric dataset sizes are supplied, the disclosure affirms that the reinforcement data are multi-sourced (verifiable evaluations, length control signals, and human feedback) and specifically engineered around complex reasoning use-cases.",
  "4-4 (Data Filtering)": "Data quality is enforced before training through a dedicated preparation and filtering pipeline.  In the “Data-Preparation Phase,” the system “collects raw corpora, carries out cleansing, language identification, deduplication, and masking; attaches quantitative quality signals, applies filtering, synthesizes targeted data, and serializes the resulting shards.”  A follow-up description adds three guiding principles—“scalability, reusability, and quick refresh”—and explains that the pipeline first performs schema standardization, then a separate “quality assessment and filtering” stage.  During annotation, it “attaches quantitative quality signals, including structural and linguistic metrics, and applies masking to all personally identifiable information (PII).”  Finally, “the filtering stage then materializes stage-specific corpora by applying threshold rules to the annotated data and serializes the result into shard files optimized for streaming.”  Although explicit numeric thresholds are not divulged, the cited steps reveal concrete criteria and tooling: (1) language ID classifiers, (2) deduplication procedures, (3) PII masking, (4) structural & linguistic quality metrics, and (5) rule-based thresholds that decide corpus inclusion.  The result is a set of pre-filtered, versioned shards ready for large-scale training.",
  "4-1 (Pre-training Data)__evidence": [
    {
      "source": "[pdf_text]",
      "quote": "HyperCLOVA X THINK, the first reasoning-focused large language model in the HyperCLOVA X family, pre-trained on roughly 6 trillion high-quality Korean, and English tokens, augmented with targeted synthetic Korean data."
    },
    {
      "source": "[pdf_text]",
      "quote": "During the pretraining stage, each model was trained under a controlled random seed. We used the training loss at iteration 14, 000—corresponding to the completion of 30B tokens—as our main reference point."
    },
    {
      "source": "[pdf_text]",
      "quote": "We pre-train the 400M-parameter Transformers on 30B tokens each under the controlled same training seed."
    },
    {
      "source": "[sections/https://arxiv.org/pdf/2506.22403]",
      "quote": "We introduce HyperCLOVA X THINK, the first reasoning-focused large language model in the HyperCLOVA X family, pre-trained on roughly 6 trillion high-quality Korean, and English tokens, augmented with targeted synthetic Korean data."
    }
  ],
  "4-2 (Fine-tuning Data)__evidence": [
    {
      "source": "[pdf_text]",
      "quote": "Figure 3: Post-training pipeline of HyperCLOVA X THINK. (2) Training Phase: A sequence of fine-tuning procedures—including Supervised Fine-Tuning (SFT), Reward Modeling (RM), Reinforcement Learning with Verifiable Rewards (RLVR), training for reasoning Length Controllability (LC), and Reinforcement Learning from Human Feedback (RLHF) —is executed across a large-scale GPU cluster."
    },
    {
      "source": "[sections/https://arxiv.org/pdf/2506.22403]",
      "quote": "We introduce HyperCLOVA X THINK, the first reasoning-focused large language model in the HyperCLOVA X family, pre-trained on roughly 6 trillion high-quality Korean, and English tokens, augmented with targeted synthetic Korean data. It was implemented as a compute-memory-balanced Peri-LN Transformer scaled with µP, pre-trained through a three-stage curriculum that expands the context window to 128K tokens, and post-trained via supervised fine-tuning with Reinforcement Learning from Verifiable Rewards supports both detailed rationale and concise-answer modes."
    }
  ],
  "4-3 (Reinforcement Learning Data)__evidence": [
    {
      "source": "[pdf_text]",
      "quote": "During pre-training, A three-stage curriculum gradually increases the context window, culminating in 128k tokens, which enables THINK to process long documents and perform multi-step reasoning within a single pass. Then, for post-training, we combine supervised fine-tuning on carefully designed reasoning tasks with Reinforcement Learning from Verifiable Rewards supports both detailed rationale and concise-answer modes."
    },
    {
      "source": "[sections/https://arxiv.org/pdf/2506.22403]",
      "quote": "This section outlines the post-training methodology of THINK: a supervised fine-tuning (SFT) phase that injects core reasoning patterns and task-specific capabilities (Section 3.1); and a multi-stage reinforcement learning pipeline that incorporates verifiable rewards, length-controllability, and human feedback to achieve aligned, efficient, and scalable reasoning (Sections 3.2–3.4)."
    }
  ],
  "4-4 (Data Filtering)__evidence": [
    {
      "source": "[pdf_text]",
      "quote": "Figure 1: Pre-training pipeline of HyperCLOVA X THINK. (1) Data-Preparation Phase: A scalable pipeline collects raw corpora, carries out cleansing, language identification, deduplication, and masking; attaches quantitative quality signals, applies filtering, synthesizes targeted data, and serializes the resulting shards"
    },
    {
      "source": "[sections/https://arxiv.org/pdf/2506.22403]",
      "quote": "The data pipeline for THINK is designed around three guiding principles: scalability, reusability, and quick refresh, so that new corpora can be incorporated with minimal latency while maintaining strict quality guarantees. Following Weber et al. (2024a), the pipeline separates schema standardization from quality assessment and filtering. The subsequent annotation stage attaches quantitative quality signals, including structural and linguistic metrics, and applies masking to all personally identifiable information (PII). The filtering stage then materializes stage-specific corpora by applying threshold rules to the annotated data and serializes the result into shard files optimized for streaming."
    }
  ],
  "__usage": {
    "fine_tuning": "used",
    "rl": "used"
  }
}