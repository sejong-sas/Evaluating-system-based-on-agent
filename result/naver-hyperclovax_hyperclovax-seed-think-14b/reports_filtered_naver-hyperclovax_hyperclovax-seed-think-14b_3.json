{
  "2-3 (API)": "",
  "3-1 (Pre-training)": "The pre-training of HyperCLOVA X THINK is built around a deliberately Korean-centric, scalable data pipeline that is further strengthened with targeted synthetic corpora. The model adopts a compute- and memory-efficient Transformer architecture whose stability is promoted through scale-invariant parameterization principles. Training progresses through a three-stage curriculum: the first stage builds basic linguistic knowledge, the second refines that knowledge with higher-fidelity data, and the third expands the model’s contextual capacity so that it can carry out long-form reasoning. Across the curriculum, the context window is expanded step-wise until it reaches 128 K tokens, enabling the model to ingest very long documents and complete multi-step reasoning in a single forward pass. HyperCLOVA X THINK is the first reasoning-focused member of the HyperCLOVA X family and is pre-trained on roughly six trillion high-quality tokens in Korean and English, further augmented by specially generated synthetic Korean data.",
  "3-2 (Fine-tuning)": "For post-training, HyperCLOVA X THINK follows a two-part strategy. First, a supervised fine-tuning (SFT) phase explicitly injects core reasoning patterns and a variety of task-specific skills. Second, a multi-stage reinforcement-learning pipeline is applied; it brings in verifiable reward signals, mechanisms for controlling output length, and rounds of human feedback so that the final model is both efficient and well aligned. Once fine-tuned, THINK can seamlessly toggle between a verbose ‘reasoning mode’—suitable for elaborate, multi-step problems—and a concise ‘non-reasoning mode’ for faster, context-driven answers. Within the same family, HyperCLOVA X SEED 0.5 B has already been released on HuggingFace after undergoing pruning and knowledge-distillation, and a similarly pruned, distilled version of THINK is currently being readied for open-source release.",
  "3-3 (Reinforcement Learning)": "Reinforcement-learning is tightly integrated into THINK’s post-training pipeline. After the SFT seed stage, the model is subjected to a multi-stage RL process that combines verifiable rewards, explicit penalties or bonuses to steer output length, and iterative human feedback for alignment and safety. To make length controllability (LC) a native capability, the training incorporates the length-penalized reward functions proposed by Aggarwal and Welleck (2025). This combination of verifiable, length-aware rewards and human-in-the-loop tuning is aimed at producing an aligned, efficient, and scalable reasoner.",
  "2-3 (API)__evidence": [],
  "3-1 (Pre-training)__evidence": [
    {
      "source": "[pdf_text]",
      "quote": "This section outlines the pre-training methodology behind THINK: a scalable, Korean-centric data pipeline enriched with targeted synthetic corpora (Section 2.1); a compute–memory-efficient yet stability-oriented Transformer, instantiated with scale-invariant parameterization principles (Section 2.2); and a three-stage curriculum that sequentially builds foundational linguistic knowledge, refines competence with higher-fidelity data, and expands contextual capacity to support long-form reasoning (Section 2.3)."
    },
    {
      "source": "[pdf_text]",
      "quote": "During pre-training, A three-stage curriculum gradually increases the context window, culminating in 128K tokens, which enables THINK to process long documents and perform multi-step reasoning within a single pass."
    },
    {
      "source": "[pdf_text]",
      "quote": "Each model is trained on 30 billion tokens. To ensure reliable validation, we pre-train each model with five different training seeds in all experiments."
    },
    {
      "source": "[pdf_text]",
      "quote": "In particular, when we sweep over training seeds and learning rates, Pre-LN frequently exhibits spikes in the gradient-norm curve, whereas Peri-LN shows comparatively few, thereby supporting Proposition 3.1."
    },
    {
      "source": "[pdf_text]",
      "quote": "In most of our experiments across different random seeds, the Pre-LN architecture exhibited early-stage instability. Although we initially suspected that a high learning rate might be the root cause, lowering it did not substantially mitigate these issues."
    },
    {
      "source": "[sections/https://arxiv.org/pdf/2506.22403]",
      "quote": "We introduce HyperCLOVA X THINK, the first reasoning-focused large language model in the HyperCLOVA X family, pre-trained on roughly 6 trillion high-quality Korean, and English tokens, augmented with targeted synthetic Korean data."
    },
    {
      "source": "[sections/https://arxiv.org/pdf/2506.22403]",
      "quote": "This section outlines the pre-training methodology behind THINK: a scalable, Korean-centric data pipeline enriched with targeted synthetic corpora (Section 2.1); a compute–memory-efficient yet stability-oriented Transformer, instantiated with scale-invariant parameterization principles (Section 2.2); and a three-stage curriculum that sequentially builds foundational linguistic knowledge, refines competence with higher-fidelity data, and expands contextual capacity to support long-form reasoning (Section 2.3)."
    },
    {
      "source": "[pdf_text]",
      "quote": "In this report, we introduced HyperCLOVA X THINK, the first reasoning-focused LLM within HyperCLOVA X family. Its pre-training dataset comprises approximately 6 trillion high-quality tokens spanning Korean, English, and further enhanced by targeted synthetic Korean data."
    }
  ],
  "3-2 (Fine-tuning)__evidence": [
    {
      "source": "[pdf_text]",
      "quote": "This section outlines the post-training methodology of THINK: a supervised fine-tuning (SFT) phase that injects core reasoning patterns and task-specific capabilities (Section 3.1); and a multi-stage reinforcement learning pipeline that incorporates verifiable rewards, length-controllability, and human feedback to achieve aligned, efficient, and scalable reasoning (Sections 3.2–3.4)."
    },
    {
      "source": "[sections/https://arxiv.org/pdf/2506.22403]",
      "quote": "This section outlines the post-training methodology of THINK: a supervised fine-tuning (SFT) phase that injects core reasoning patterns and task-specific capabilities (Section 3.1); and a multi-stage reinforcement learning pipeline that incorporates verifiable rewards, length-controllability, and human feedback to achieve aligned, efficient, and scalable reasoning (Sections 3.2–3.4)."
    },
    {
      "source": "[sections/https://arxiv.org/pdf/2506.22403]",
      "quote": "THINK is trained to operate in an integrated manner, allowing for dynamic switching between a detailed ‘reasoning mode’ for complex, multi-step reasoning and a more direct ‘non-reasoning mode’ for rapid, context-driven responses."
    },
    {
      "source": "[pdf_text]",
      "quote": "As a real-world example, HyperCLOVA X SEED 0.5B, recently released on HuggingFace, is the first open-source model in the HyperCLOVA X series trained using pruning and knowledge distillation."
    },
    {
      "source": "[pdf_text]",
      "quote": "Currently, a pruned and distilled version of THINK is under preparation to be open-sourced."
    }
  ],
  "3-3 (Reinforcement Learning)__evidence": [
    {
      "source": "[pdf_text]",
      "quote": "This section outlines the post-training methodology of THINK: a supervised fine-tuning (SFT) phase that injects core reasoning patterns and task-specific capabilities (Section 3.1); and a multi-stage reinforcement learning pipeline that incorporates verifiable rewards, length-controllability, and human feedback to achieve aligned, efficient, and scalable reasoning (Sections 3.2–3.4)."
    },
    {
      "source": "[sections/https://arxiv.org/pdf/2506.22403]",
      "quote": "To induce LC in HyperCLOVA X THINK, we additionally incorporate the length-penalized reward functions introduced by Aggarwal and Welleck, 2025."
    }
  ]
}