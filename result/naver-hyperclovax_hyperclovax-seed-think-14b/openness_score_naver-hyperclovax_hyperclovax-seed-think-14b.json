{
  "model": "naver-hyperclovax/HyperCLOVAX-SEED-Think-14B",
  "scores": {
    "1-1 Weights": {
      "score": 1,
      "reason": "Weights files present in the repository (e.g., *.safetensors/bin/pt/ckpt)."
    },
    "1-2 Code": {
      "score": 0.0,
      "reason": "No training pipeline files; README mentions are ignored."
    },
    "1-3 License": {
      "score": 1.0,
      "reason": "The bespoke “HyperCLOVA X SEED” license allows use, modification, redistribution and commercial use for ordinary users (restrictions only apply to very large competitors), meeting the rubric’s ‘Open’ criterion."
    },
    "1-4 Paper": {
      "score": 1.0,
      "reason": "An official technical report dedicated to this exact model is on arXiv (e-print 2506.22403)."
    },
    "1-5 Architecture": {
      "score": 1,
      "reason": "Architecture evidence present in extracted quotes."
    },
    "1-6 Tokenizer": {
      "score": 1.0,
      "reason": "Tokenizer files downloadable in repository."
    },
    "2-1 Hardware": {
      "score": 0.5,
      "reason": "Quotes reveal the GPU type used for training/inference (A100-80 GB) but give no quantities or total compute."
    },
    "2-2 Software": {
      "score": 0.0,
      "reason": "Only the base framework (Transformers 4.52.4/4.53+) is mentioned; no additional training-stack components are specified."
    },
    "2-3 API": {
      "score": 0.0,
      "reason": "Only generic or non-official mentions; no qualifying official API docs.  No qualifying official API docs via web search."
    },
    "3-1 Pre-training": {
      "score": 0.5,
      "reason": "Curriculum, token count and high-level objectives are described, but not enough hyper-parameter detail to reproduce exactly."
    },
    "3-2 Fine-tuning": {
      "score": 0.5,
      "reason": "Multi-stage SFT pipeline is outlined, yet lacks concrete settings (batch sizes, LR, epochs) for full reproduction."
    },
    "3-3 Reinforcement Learning": {
      "score": 0.5,
      "reason": "Stages (RLVR, LC, RLHF) are explained, but implementation specifics are missing."
    },
    "4-1 Pre-training Data": {
      "score": 0.5,
      "reason": "Languages, synthetic augmentation and 6 T-token scale are given, but individual sources and proportions are not."
    },
    "4-2 Fine-tuning Data": {
      "score": 0.5,
      "reason": "Domains and annotation schema are described, yet datasets are not enumerated or released."
    },
    "4-3 Reinforcement Learning Data": {
      "score": 0.5,
      "reason": "Prompt mixture and reward types are explained, but the underlying data are not fully disclosed."
    },
    "4-4 Data Filtering": {
      "score": 1.0,
      "reason": "Paper details a multi-stage pipeline: schema standardisation, quantitative quality signals, MinHash dedup, PII masking, and two-tier Korean-specific filtering."
    }
  },
  "included_scores": {
    "1-1 Weights": {
      "score": 1,
      "reason": "Weights files present in the repository (e.g., *.safetensors/bin/pt/ckpt)."
    },
    "1-2 Code": {
      "score": 0.0,
      "reason": "No training pipeline files; README mentions are ignored."
    },
    "1-3 License": {
      "score": 1.0,
      "reason": "The bespoke “HyperCLOVA X SEED” license allows use, modification, redistribution and commercial use for ordinary users (restrictions only apply to very large competitors), meeting the rubric’s ‘Open’ criterion."
    },
    "1-4 Paper": {
      "score": 1.0,
      "reason": "An official technical report dedicated to this exact model is on arXiv (e-print 2506.22403)."
    },
    "1-5 Architecture": {
      "score": 1,
      "reason": "Architecture evidence present in extracted quotes."
    },
    "1-6 Tokenizer": {
      "score": 1.0,
      "reason": "Tokenizer files downloadable in repository."
    },
    "2-1 Hardware": {
      "score": 0.5,
      "reason": "Quotes reveal the GPU type used for training/inference (A100-80 GB) but give no quantities or total compute."
    },
    "2-2 Software": {
      "score": 0.0,
      "reason": "Only the base framework (Transformers 4.52.4/4.53+) is mentioned; no additional training-stack components are specified."
    },
    "2-3 API": {
      "score": 0.0,
      "reason": "Only generic or non-official mentions; no qualifying official API docs.  No qualifying official API docs via web search."
    },
    "3-1 Pre-training": {
      "score": 0.5,
      "reason": "Curriculum, token count and high-level objectives are described, but not enough hyper-parameter detail to reproduce exactly."
    },
    "3-2 Fine-tuning": {
      "score": 0.5,
      "reason": "Multi-stage SFT pipeline is outlined, yet lacks concrete settings (batch sizes, LR, epochs) for full reproduction."
    },
    "3-3 Reinforcement Learning": {
      "score": 0.5,
      "reason": "Stages (RLVR, LC, RLHF) are explained, but implementation specifics are missing."
    },
    "4-1 Pre-training Data": {
      "score": 0.5,
      "reason": "Languages, synthetic augmentation and 6 T-token scale are given, but individual sources and proportions are not."
    },
    "4-2 Fine-tuning Data": {
      "score": 0.5,
      "reason": "Domains and annotation schema are described, yet datasets are not enumerated or released."
    },
    "4-3 Reinforcement Learning Data": {
      "score": 0.5,
      "reason": "Prompt mixture and reward types are explained, but the underlying data are not fully disclosed."
    },
    "4-4 Data Filtering": {
      "score": 1.0,
      "reason": "Paper details a multi-stage pipeline: schema standardisation, quantitative quality signals, MinHash dedup, PII masking, and two-tier Korean-specific filtering."
    }
  },
  "final_score_10pt": 5.938,
  "meta": {
    "usage_from_dispatch": {
      "fine_tuning": "used",
      "rl": "used"
    },
    "excluded": [],
    "denominator": 16,
    "raw_sum": 9.5,
    "scale": "10/16",
    "code_detection_reason": "No training pipeline files; README mentions are ignored.",
    "pretrain_sources_used": false
  }
}