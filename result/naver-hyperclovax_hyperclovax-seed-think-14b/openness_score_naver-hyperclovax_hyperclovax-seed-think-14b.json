{
  "model": "naver-hyperclovax/HyperCLOVAX-SEED-Think-14B",
  "scores": {
    "1-1 Weights": {
      "score": 1.0,
      "reason": "Hugging Face presence detected; forced Open per FORCE_HF_CORE_OPEN."
    },
    "1-2 Code": {
      "score": 0.0,
      "reason": "No training pipeline files; README mentions are ignored."
    },
    "1-3 License": {
      "score": 1.0,
      "reason": "The bespoke “HyperCLOVA X SEED Model License Agreement” grants use, modification, redistribution and commercial use. Extra duties (attribution, ‘Powered by HyperCLOVA X’, waiver for >10 M-MAU competitors) are minor under the rubric (similar to Qwen), so it is treated as open."
    },
    "1-4 Paper": {
      "score": 1.0,
      "reason": "Official provider-authored technical report on arXiv (e-print 2506.22403) specifically about HyperCLOVA X THINK."
    },
    "1-5 Architecture": {
      "score": 1.0,
      "reason": "Hugging Face presence detected; forced Open per FORCE_HF_CORE_OPEN."
    },
    "1-6 Tokenizer": {
      "score": 1.0,
      "reason": "Hugging Face presence detected; forced Open per FORCE_HF_CORE_OPEN."
    },
    "2-1 Hardware": {
      "score": 0.5,
      "reason": "Quotes mention training on an “A100-80 GB” cluster and a table headed “GPU Hours (A100-80GB, MFU 50 %)”, but no concrete GPU-hour number or card count."
    },
    "2-2 Software": {
      "score": 0.0,
      "reason": "Only inference stack (Transformers ≥ 4.53, timm) is quoted; nothing about the TRAINING software stack."
    },
    "2-3 API": {
      "score": 1.0,
      "reason": "Only generic or non-official mentions; no qualifying official API docs.  apifind: API_OPEN via https://api.ncloud-docs.com/docs/en/ai-naver-clovastudio-summary, https://naver.github.io/naver-openapi-guide/apilist.html. CLOVA Studio API key issuance page; official API documentation for HyperCLOVA X models."
    },
    "3-1 Pre-training": {
      "score": 0.5,
      "reason": "Three-stage curriculum, μP scaling, 128 K context and 6 T-token corpus described, but not fully reproducible (no hyper-params, batch sizes, LR schedule)."
    },
    "3-2 Fine-tuning": {
      "score": 0.5,
      "reason": "Pipeline listed (SFT → RLVR → LC → RLHF + RLVR) yet lacks concrete reproducing details."
    },
    "3-3 Reinforcement Learning": {
      "score": 0.5,
      "reason": "Multi-stage RL strategy (verifiable rewards, length-penalised rewards, RLHF) explained, but no algorithmic hyper-parameters; therefore partial."
    },
    "4-1 Pre-training Data": {
      "score": 0.5,
      "reason": "Discloses size (≈6 T tokens) and language mix (Korean, English, synthetic Korean) but no dataset names or proportions."
    },
    "4-2 Fine-tuning Data": {
      "score": 0.5,
      "reason": "States SFT data aggregated from maths, coding, STEM, general abilities and schema (prompt / think / response); no concrete dataset lists or sizes."
    },
    "4-3 Reinforcement Learning Data": {
      "score": 0.5,
      "reason": "Mentions mixture of reasoning / non-reasoning prompts for RLHF and verifiable-reward sets, yet gives no dataset specifics."
    },
    "4-4 Data Filtering": {
      "score": 1.0,
      "reason": "Provides a multi-step pipeline: schema standardisation, language ID, cleansing, PII masking, MinHash near-duplicate removal, and quantitative quality signals (symbol-to-word ratio, mean word length, sentence count, masked-PII ratio, normalised-length ratio) – enough detail to re-implement."
    }
  },
  "included_scores": {
    "1-1 Weights": {
      "score": 1.0,
      "reason": "Hugging Face presence detected; forced Open per FORCE_HF_CORE_OPEN."
    },
    "1-2 Code": {
      "score": 0.0,
      "reason": "No training pipeline files; README mentions are ignored."
    },
    "1-3 License": {
      "score": 1.0,
      "reason": "The bespoke “HyperCLOVA X SEED Model License Agreement” grants use, modification, redistribution and commercial use. Extra duties (attribution, ‘Powered by HyperCLOVA X’, waiver for >10 M-MAU competitors) are minor under the rubric (similar to Qwen), so it is treated as open."
    },
    "1-4 Paper": {
      "score": 1.0,
      "reason": "Official provider-authored technical report on arXiv (e-print 2506.22403) specifically about HyperCLOVA X THINK."
    },
    "1-5 Architecture": {
      "score": 1.0,
      "reason": "Hugging Face presence detected; forced Open per FORCE_HF_CORE_OPEN."
    },
    "1-6 Tokenizer": {
      "score": 1.0,
      "reason": "Hugging Face presence detected; forced Open per FORCE_HF_CORE_OPEN."
    },
    "2-1 Hardware": {
      "score": 0.5,
      "reason": "Quotes mention training on an “A100-80 GB” cluster and a table headed “GPU Hours (A100-80GB, MFU 50 %)”, but no concrete GPU-hour number or card count."
    },
    "2-2 Software": {
      "score": 0.0,
      "reason": "Only inference stack (Transformers ≥ 4.53, timm) is quoted; nothing about the TRAINING software stack."
    },
    "2-3 API": {
      "score": 1.0,
      "reason": "Only generic or non-official mentions; no qualifying official API docs.  apifind: API_OPEN via https://api.ncloud-docs.com/docs/en/ai-naver-clovastudio-summary, https://naver.github.io/naver-openapi-guide/apilist.html. CLOVA Studio API key issuance page; official API documentation for HyperCLOVA X models."
    },
    "3-1 Pre-training": {
      "score": 0.5,
      "reason": "Three-stage curriculum, μP scaling, 128 K context and 6 T-token corpus described, but not fully reproducible (no hyper-params, batch sizes, LR schedule)."
    },
    "3-2 Fine-tuning": {
      "score": 0.5,
      "reason": "Pipeline listed (SFT → RLVR → LC → RLHF + RLVR) yet lacks concrete reproducing details."
    },
    "3-3 Reinforcement Learning": {
      "score": 0.5,
      "reason": "Multi-stage RL strategy (verifiable rewards, length-penalised rewards, RLHF) explained, but no algorithmic hyper-parameters; therefore partial."
    },
    "4-1 Pre-training Data": {
      "score": 0.5,
      "reason": "Discloses size (≈6 T tokens) and language mix (Korean, English, synthetic Korean) but no dataset names or proportions."
    },
    "4-2 Fine-tuning Data": {
      "score": 0.5,
      "reason": "States SFT data aggregated from maths, coding, STEM, general abilities and schema (prompt / think / response); no concrete dataset lists or sizes."
    },
    "4-3 Reinforcement Learning Data": {
      "score": 0.5,
      "reason": "Mentions mixture of reasoning / non-reasoning prompts for RLHF and verifiable-reward sets, yet gives no dataset specifics."
    },
    "4-4 Data Filtering": {
      "score": 1.0,
      "reason": "Provides a multi-step pipeline: schema standardisation, language ID, cleansing, PII masking, MinHash near-duplicate removal, and quantitative quality signals (symbol-to-word ratio, mean word length, sentence count, masked-PII ratio, normalised-length ratio) – enough detail to re-implement."
    }
  },
  "final_score_10pt": 6.562,
  "meta": {
    "usage_from_dispatch": {
      "fine_tuning": "used",
      "rl": "used"
    },
    "excluded": [],
    "denominator": 16,
    "raw_sum": 10.5,
    "scale": "10/16",
    "code_detection_reason": "No training pipeline files; README mentions are ignored.",
    "pretrain_sources_used": false
  }
}