{
  "1-1 (Weights)": "\"The total size of DeepSeek-V3 models on Hugging Face is 685B, which includes 671B of the Main Model weights and 14B of the Multi-Token Prediction (MTP) Module weights.\"  \n\"Download the model weights from Hugging Face, and put them into `/path/to/DeepSeek-V3` folder.\"",
  "1-1 (Weights)__evidence": [
    {
      "source": "[readme]",
      "quote": "The total size of DeepSeek-V3 models on Hugging Face is 685B, which includes 671B of the Main Model weights and 14B of the Multi-Token Prediction (MTP) Module weights."
    },
    {
      "source": "[readme]",
      "quote": "Download the model weights from Hugging Face, and put them into `/path/to/DeepSeek-V3` folder."
    }
  ],
  "1-2 (Code)": "\"1. **DeepSeek-Infer Demo**: We provide a simple and lightweight demo for FP8 and BF16 inference.\"",
  "1-2 (Code)__evidence": [
    {
      "source": "[readme]",
      "quote": "1. **DeepSeek-Infer Demo**: We provide a simple and lightweight demo for FP8 and BF16 inference."
    }
  ],
  "1-3 (License)": "\"This code repository is licensed under [the MIT License](LICENSE-CODE).\"  \n\"The use of DeepSeek-V3 Base/Chat models is subject to [the Model License](LICENSE-MODEL).\"  \n\"DeepSeek-V3 series (including Base and Chat) supports commercial use.\"  \n\"MIT License\"  \n\"Permission is hereby granted, free of charge, to any person obtaining a copy of this software and associated documentation files (the \\\"Software\\\"), to deal in the Software without restriction, including without limitation the rights to use, copy, modify, merge, publish, distribute, sublicense, and/or sell copies of the Software, and to permit persons to whom the Software is furnished to do so, subject to the following conditions:\"  \n\"DeepSeek hereby grants to You a perpetual, worldwide, non-exclusive, no-charge, royalty-free, irrevocable copyright license to reproduce, prepare, publicly display, publicly perform, sublicense, and distribute the Complementary Material, the Model, and Derivatives of the Model.\"",
  "1-3 (License)__evidence": [
    {
      "source": "[readme]",
      "quote": "This code repository is licensed under [the MIT License](LICENSE-CODE)."
    },
    {
      "source": "[readme]",
      "quote": "The use of DeepSeek-V3 Base/Chat models is subject to [the Model License](LICENSE-MODEL)."
    },
    {
      "source": "[readme]",
      "quote": "DeepSeek-V3 series (including Base and Chat) supports commercial use."
    },
    {
      "source": "[license_files]",
      "quote": "MIT License"
    },
    {
      "source": "[license_files]",
      "quote": "Permission is hereby granted, free of charge, to any person obtaining a copy of this software and associated documentation files (the \"Software\"), to deal in the Software without restriction, including without limitation the rights to use, copy, modify, merge, publish, distribute, sublicense, and/or sell copies of the Software, and to permit persons to whom the Software is furnished to do so, subject to the following conditions:"
    },
    {
      "source": "[license_files]",
      "quote": "DeepSeek hereby grants to You a perpetual, worldwide, non-exclusive, no-charge, royalty-free, irrevocable copyright license to reproduce, prepare, publicly display, publicly perform, sublicense, and distribute the Complementary Material, the Model, and Derivatives of the Model."
    }
  ],
  "1-4 (Paper)": "\"@misc{deepseekai2024deepseekv3technicalreport,\"  \n\"title={DeepSeek-V3 Technical Report},\"",
  "1-4 (Paper)__evidence": [
    {
      "source": "[readme]",
      "quote": "@misc{deepseekai2024deepseekv3technicalreport,"
    },
    {
      "source": "[readme]",
      "quote": "title={DeepSeek-V3 Technical Report},"
    }
  ],
  "1-5 (Architecture)": "\"We present DeepSeek-V3, a strong Mixture-of-Experts (MoE) language model with 671B total parameters with 37B activated for each token.\" \"To achieve efficient inference and cost-effective training, DeepSeek-V3 adopts Multi-head Latent Attention (MLA) and DeepSeekMoE architectures, which were thoroughly validated in DeepSeek-V2.\"",
  "1-5 (Architecture)__evidence": [
    {
      "source": "[readme]",
      "quote": "We present DeepSeek-V3, a strong Mixture-of-Experts (MoE) language model with 671B total parameters with 37B activated for each token."
    },
    {
      "source": "[readme]",
      "quote": "To achieve efficient inference and cost-effective training, DeepSeek-V3 adopts Multi-head Latent Attention (MLA) and DeepSeekMoE architectures, which were thoroughly validated in DeepSeek-V2."
    }
  ],
  "1-6 (Tokenizer)": "",
  "1-6 (Tokenizer)__evidence": [],
  "2-1 (Hardware)": "\"Despite its excellent performance, DeepSeek-V3 requires only 2.788M H800 GPU hours for its full training.\" \"- At an economical cost of only 2.664M H800 GPU hours, we complete the pre-training of DeepSeek-V3 on 14.8T tokens, producing the currently strongest open-source base model.\"",
  "2-1 (Hardware)__evidence": [
    {
      "source": "[readme]",
      "quote": "Despite its excellent performance, DeepSeek-V3 requires only 2.788M H800 GPU hours for its full training."
    },
    {
      "source": "[readme]",
      "quote": "- At an economical cost of only 2.664M H800 GPU hours, we complete the pre-training of DeepSeek-V3 on 14.8T tokens, producing the currently strongest open-source base model."
    }
  ],
  "2-2 (Software)": "",
  "2-2 (Software)__evidence": [],
  "2-3 (API)": "The provided materials explicitly note two public-facing access points for DeepSeek-V3. First, \"You can chat with DeepSeek-V3 on DeepSeek's official website: [chat.deepseek.com](https://chat.deepseek.com/sign_in)\", which confirms the existence of an interactive chat front-end. Second, the developers state, \"We also provide OpenAI-Compatible API at DeepSeek Platform: [platform.deepseek.com](https://platform.deepseek.com/)\", establishing that an openly accessible, OpenAI-style REST API is available through the DeepSeek Platform. Together, these sentences document both a browser-based chat interface and an API endpoint that claims compatibility with the OpenAI specification.",
  "2-3 (API)__evidence": [
    {
      "source": "[readme]",
      "quote": "You can chat with DeepSeek-V3 on DeepSeek's official website: [chat.deepseek.com](https://chat.deepseek.com/sign_in)"
    },
    {
      "source": "[readme]",
      "quote": "We also provide OpenAI-Compatible API at DeepSeek Platform: [platform.deepseek.com](https://platform.deepseek.com/)"
    }
  ],
  "3-1 (Pre-training)": "\"We pre-train DeepSeek-V3 on 14.8 trillion diverse and high-quality tokens, followed by Supervised Fine-Tuning and Reinforcement Learning stages to fully harness its capabilities.\"  In addition to the massive token count, the authors highlight the efficiency of the run: \"At an economical cost of only 2.664M H800 GPU hours, we complete the pre-training of DeepSeek-V3 on 14.8T tokens, producing the currently strongest open-source base model.\"  These two statements collectively establish (i) the scale of the corpus (14.8 T tokens), (ii) the hardware budget (2.664 M H800 GPU-hours), and (iii) the claim of state-of-the-art strength among open-source base models, all of which characterize the core pre-training effort for DeepSeek-V3.",
  "3-1 (Pre-training)__evidence": [
    {
      "source": "[readme]",
      "quote": "We pre-train DeepSeek-V3 on 14.8 trillion diverse and high-quality tokens, followed by Supervised Fine-Tuning and Reinforcement Learning stages to fully harness its capabilities."
    },
    {
      "source": "[readme]",
      "quote": "At an economical cost of only 2.664M H800 GPU hours, we complete the pre-training of DeepSeek-V3 on 14.8T tokens, producing the currently strongest open-source base model."
    }
  ],
  "3-2 (Fine-tuning)": "Fine-tuning is described as a two-step process layered on top of the large-scale pre-training. First, the authors note that after pre-training on \"14.8 trillion diverse and high-quality tokens\" the model undergoes \"Supervised Fine-Tuning and Reinforcement Learning stages to fully harness its capabilities.\"  Second, they articulate a specific technique aimed at reasoning: \"We introduce an innovative methodology to distill reasoning capabilities from the long-Chain-of-Thought (CoT) model, specifically from one of the DeepSeek R1 series models, into standard LLMs, particularly DeepSeek-V3.\"  Together these quotations indicate (a) the existence of a supervised fine-tuning phase, (b) an additional RL-based phase, and (c) a specialized knowledge-distillation pipeline that transfers long CoT reasoning from DeepSeek R1 to DeepSeek-V3.",
  "3-2 (Fine-tuning)__evidence": [
    {
      "source": "[readme]",
      "quote": "We pre-train DeepSeek-V3 on 14.8 trillion diverse and high-quality tokens, followed by Supervised Fine-Tuning and Reinforcement Learning stages to fully harness its capabilities."
    },
    {
      "source": "[readme]",
      "quote": "We introduce an innovative methodology to distill reasoning capabilities from the long-Chain-of-Thought (CoT) model, specifically from one of the DeepSeek R1 series models, into standard LLMs, particularly DeepSeek-V3."
    }
  ],
  "3-3 (Reinforcement Learning)": "Reinforcement learning is explicitly listed as a core component of the post-pre-training workflow: \"We pre-train DeepSeek-V3 on 14.8 trillion diverse and high-quality tokens, followed by Supervised Fine-Tuning and Reinforcement Learning stages to fully harness its capabilities.\"  This sentence confirms that, after supervised fine-tuning, an RL stage (implicitly RLHF or a related paradigm) is used to further improve DeepSeek-V3.",
  "3-3 (Reinforcement Learning)__evidence": [
    {
      "source": "[readme]",
      "quote": "We pre-train DeepSeek-V3 on 14.8 trillion diverse and high-quality tokens, followed by Supervised Fine-Tuning and Reinforcement Learning stages to fully harness its capabilities."
    }
  ],
  "4-1 (Pre-training Data)": "The source material offers two explicit statements on DeepSeek-V3â€™s pre-training setup. First: \"We pre-train DeepSeek-V3 on 14.8 trillion diverse and high-quality tokens, followed by Supervised Fine-Tuning and Reinforcement Learning stages to fully harness its capabilities.\" Second: \"At an economical cost of only 2.664M H800 GPU hours, we complete the pre-training of DeepSeek-V3 on 14.8T tokens, producing the currently strongest open-source base model.\" These quotations establish that the pre-training corpus comprises \"14.8 trillion diverse and high-quality tokens,\" and that the process required \"2.664M H800 GPU hours.\"",
  "4-1 (Pre-training Data)__evidence": [
    {
      "source": "[readme]",
      "quote": "We pre-train DeepSeek-V3 on 14.8 trillion diverse and high-quality tokens, followed by Supervised Fine-Tuning and Reinforcement Learning stages to fully harness its capabilities."
    },
    {
      "source": "[readme]",
      "quote": "At an economical cost of only 2.664M H800 GPU hours, we complete the pre-training of DeepSeek-V3 on 14.8T tokens, producing the currently strongest open-source base model."
    }
  ],
  "4-2 (Fine-tuning Data)": "Fine-tuning is mentioned in the sentence: \"We pre-train DeepSeek-V3 on 14.8 trillion diverse and high-quality tokens, followed by Supervised Fine-Tuning and Reinforcement Learning stages to fully harness its capabilities.\" This quote affirms the presence of a \"Supervised Fine-Tuning\" phase directly after pre-training for DeepSeek-V3.",
  "4-2 (Fine-tuning Data)__evidence": [
    {
      "source": "[readme]",
      "quote": "We pre-train DeepSeek-V3 on 14.8 trillion diverse and high-quality tokens, followed by Supervised Fine-Tuning and Reinforcement Learning stages to fully harness its capabilities."
    }
  ],
  "4-3 (Reinforcement Learning Data)": "Reinforcement learning is referenced in the same line: \"We pre-train DeepSeek-V3 on 14.8 trillion diverse and high-quality tokens, followed by Supervised Fine-Tuning and Reinforcement Learning stages to fully harness its capabilities.\" This establishes that a \"Reinforcement Learning\" stage follows pre-training within the DeepSeek-V3 training pipeline.",
  "4-3 (Reinforcement Learning Data)__evidence": [
    {
      "source": "[readme]",
      "quote": "We pre-train DeepSeek-V3 on 14.8 trillion diverse and high-quality tokens, followed by Supervised Fine-Tuning and Reinforcement Learning stages to fully harness its capabilities."
    }
  ],
  "4-4 (Data Filtering)": "",
  "4-4 (Data Filtering)__evidence": [],
  "__usage": {
    "fine_tuning": "used",
    "rl": "used"
  }
}