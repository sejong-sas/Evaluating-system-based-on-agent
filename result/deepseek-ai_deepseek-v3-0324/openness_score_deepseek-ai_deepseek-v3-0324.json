{
  "model": "deepseek-ai/DeepSeek-V3-0324",
  "scores": {
    "1-2 Code": {
      "score": 0.0,
      "reason": "No training pipeline files; README mentions are ignored."
    },
    "1-3 License": {
      "score": 1.0,
      "reason": "Recognized permissive license (e.g., MIT/Apache-2.0/BSD/MPL/CC0/CC-BY) in quotes."
    },
    "1-4 Paper": {
      "score": 1.0,
      "reason": "An official technical report entitled “DeepSeek-V3 Technical Report” is on arXiv (url quoted). It is specifically about this model version."
    },
    "1-5 Architecture": {
      "score": 1,
      "reason": "Architecture evidence present in extracted quotes."
    },
    "1-6 Tokenizer": {
      "score": 1.0,
      "reason": "Tokenizer files downloadable in repository."
    },
    "2-1 Hardware": {
      "score": 1.0,
      "reason": "Quotes give BOTH type and quantity: “trained on a cluster equipped with 2 048 NVIDIA H800 GPUs … 180 K H800-GPU hours per trillion tokens … 2.788 M H800-GPU hours total”."
    },
    "2-2 Software": {
      "score": 0.5,
      "reason": "Training stack is partially described (HAI-LLM framework, 16-way PP, 64-way EP, ZeRO-1 DP, FP8 kernels, AdamW, YaRN, GRPO) but versions/configs are incomplete, so it is ‘Semi-Open’."
    },
    "2-3 API": {
      "score": 1.0,
      "reason": "Only third-party or generic API mentions; no official owner-hosted API.  Web search found official API docs: https://api-docs.deepseek.com/news/news250325, https://api-docs.deepseek.com/, https://api-docs.deepseek.com/api/deepseek-api"
    },
    "3-1 Pre-training": {
      "score": 0.5,
      "reason": "Partial methodology disclosed (techniques/algorithms mentioned) but not fully reproducible."
    },
    "3-2 Fine-tuning": {
      "score": 0.5,
      "reason": "Partial methodology disclosed (techniques/algorithms mentioned) but not fully reproducible."
    },
    "3-3 Reinforcement Learning": {
      "score": 0.5,
      "reason": "Partial methodology disclosed (techniques/algorithms mentioned) but not fully reproducible."
    },
    "4-1 Pre-training Data": {
      "score": 0.5,
      "reason": "States size (14.8 T tokens) and high-level composition changes (more maths/code, broader multilingual), but does not list concrete sources or licences. Partial disclosure."
    },
    "4-2 Fine-tuning Data": {
      "score": 0.5,
      "reason": "Explains that reasoning data come from DeepSeek-R1, non-reasoning from DeepSeek-V2.5 + human vetting, but no dataset dumps or exact sizes are provided. Partial."
    },
    "4-3 Reinforcement Learning Data": {
      "score": 0.5,
      "reason": "Mentions prompt domains and that reward data are built from SFT checkpoints with chain-of-thought labels, but no full dataset specification. Partial."
    },
    "4-4 Data Filtering": {
      "score": 0.5,
      "reason": "Partial filtering details disclosed (dedup/safety/langid/quality etc.)."
    },
    "1-1 Weights": {
      "score": 1,
      "reason": "Weights files present in the repository (e.g., *.safetensors/bin/pt/ckpt)."
    }
  },
  "included_scores": {
    "1-2 Code": {
      "score": 0.0,
      "reason": "No training pipeline files; README mentions are ignored."
    },
    "1-3 License": {
      "score": 1.0,
      "reason": "Recognized permissive license (e.g., MIT/Apache-2.0/BSD/MPL/CC0/CC-BY) in quotes."
    },
    "1-4 Paper": {
      "score": 1.0,
      "reason": "An official technical report entitled “DeepSeek-V3 Technical Report” is on arXiv (url quoted). It is specifically about this model version."
    },
    "1-5 Architecture": {
      "score": 1,
      "reason": "Architecture evidence present in extracted quotes."
    },
    "1-6 Tokenizer": {
      "score": 1.0,
      "reason": "Tokenizer files downloadable in repository."
    },
    "2-1 Hardware": {
      "score": 1.0,
      "reason": "Quotes give BOTH type and quantity: “trained on a cluster equipped with 2 048 NVIDIA H800 GPUs … 180 K H800-GPU hours per trillion tokens … 2.788 M H800-GPU hours total”."
    },
    "2-2 Software": {
      "score": 0.5,
      "reason": "Training stack is partially described (HAI-LLM framework, 16-way PP, 64-way EP, ZeRO-1 DP, FP8 kernels, AdamW, YaRN, GRPO) but versions/configs are incomplete, so it is ‘Semi-Open’."
    },
    "2-3 API": {
      "score": 1.0,
      "reason": "Only third-party or generic API mentions; no official owner-hosted API.  Web search found official API docs: https://api-docs.deepseek.com/news/news250325, https://api-docs.deepseek.com/, https://api-docs.deepseek.com/api/deepseek-api"
    },
    "3-1 Pre-training": {
      "score": 0.5,
      "reason": "Partial methodology disclosed (techniques/algorithms mentioned) but not fully reproducible."
    },
    "3-2 Fine-tuning": {
      "score": 0.5,
      "reason": "Partial methodology disclosed (techniques/algorithms mentioned) but not fully reproducible."
    },
    "3-3 Reinforcement Learning": {
      "score": 0.5,
      "reason": "Partial methodology disclosed (techniques/algorithms mentioned) but not fully reproducible."
    },
    "4-1 Pre-training Data": {
      "score": 0.5,
      "reason": "States size (14.8 T tokens) and high-level composition changes (more maths/code, broader multilingual), but does not list concrete sources or licences. Partial disclosure."
    },
    "4-2 Fine-tuning Data": {
      "score": 0.5,
      "reason": "Explains that reasoning data come from DeepSeek-R1, non-reasoning from DeepSeek-V2.5 + human vetting, but no dataset dumps or exact sizes are provided. Partial."
    },
    "4-3 Reinforcement Learning Data": {
      "score": 0.5,
      "reason": "Mentions prompt domains and that reward data are built from SFT checkpoints with chain-of-thought labels, but no full dataset specification. Partial."
    },
    "4-4 Data Filtering": {
      "score": 0.5,
      "reason": "Partial filtering details disclosed (dedup/safety/langid/quality etc.)."
    },
    "1-1 Weights": {
      "score": 1,
      "reason": "Weights files present in the repository (e.g., *.safetensors/bin/pt/ckpt)."
    }
  },
  "final_score_10pt": 6.875,
  "meta": {
    "usage_from_dispatch": {
      "fine_tuning": "used",
      "rl": "used"
    },
    "excluded": [],
    "denominator": 16,
    "raw_sum": 11.0,
    "scale": "10/16",
    "code_detection_reason": "No training pipeline files; README mentions are ignored.",
    "pretrain_sources_used": true
  }
}