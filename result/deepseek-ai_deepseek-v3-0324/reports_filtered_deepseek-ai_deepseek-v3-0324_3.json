{
  "2-3 (API)": "",
  "3-1 (Pre-training)": "DeepSeek-V3 undergoes an extensive pre-training phase that is repeatedly characterized in the source material as both massive in scale and carefully engineered for efficiency. The model is trained on 14.8 trillion ‚Äúhigh-quality and diverse‚Äù tokens, a corpus that is explicitly optimized to increase the share of mathematical and programming content while also broadening multilingual coverage beyond just English and Chinese. Tokenization relies on a Byte-level BPE vocabulary expanded to 128 K tokens.  \n\nArchitecturally, DeepSeek-V3 is a 61-layer Transformer with a hidden size of 7 168. Although the total parameter count reaches 671 billion, the model follows a Mixture-of-Experts design in which only 37 billion parameters are activated per token. Several training innovations are highlighted: incorporation of the Fill-in-the-Middle (FIM) strategy, the use of FP8 precision, an auxiliary-loss-free load-balancing method for MoE routing, and a multi-token prediction objective.  \n\nThe optimization setup employs AdamW with Œ≤‚ÇÅ = 0.9, Œ≤‚ÇÇ = 0.95, and weight_decay = 0.1. Sequences of up to 4 000 tokens are processed during pre-training. Cost figures are given with unusual transparency: every trillion tokens consumes roughly 180 K H800 GPU-hours, so the full 14.8 T tokens require 2.664 million H800 GPU-hours‚Äîequivalent to about 3.7 days on a 2 048-GPU H800 cluster. The authors repeatedly emphasize that these efficiencies make DeepSeek-V3 ‚Äúthe currently strongest open-source base model.‚Äù Finally, the pre-training phase is presented as the first stage of a larger pipeline that will subsequently apply Supervised Fine-Tuning and Reinforcement Learning to ‚Äúfully harness‚Äù the model‚Äôs capabilities.",
  "3-2 (Fine-tuning)": "After the base model is trained, the DeepSeek-V3 team performs a dedicated post-training stage centered on Supervised Fine-Tuning (SFT). They explicitly state that DeepSeek-V3-Base is fine-tuned for two full epochs on an SFT dataset under a cosine-decay learning-rate schedule that starts at 5 √ó 10‚Åª‚Å∂ and decays to 1 √ó 10‚Åª‚Å∂. A key goal of this stage is to align the model with human preferences while preserving a balance between answer accuracy and generation length.  \n\nThe fine-tuning pipeline also incorporates knowledge distillation: reasoning abilities are distilled from the DeepSeek-R1 model family into DeepSeek-V3, with the authors claiming a ‚Äúsuccess‚Äù in transferring these capabilities. Moreover, checkpoints produced during SFT serve as the initialization point for both the reward model and the subsequent reinforcement-learning stage. Preference data used for reward-model training is constructed to include not only final preference labels but also the chain-of-thought that leads to those preferences, signaling an intention to make the reward signal richer and more reliable. Collectively, these measures comprise a reproducible, well-specified SFT routine that prepares the base model for RLHF-style alignment.",
  "3-3 (Reinforcement Learning)": "Reinforcement learning constitutes the final alignment layer for DeepSeek-V3. The authors describe a post-training sequence that combines SFT with RL ‚Äúto align it with human preferences and further unlock its potential.‚Äù They adopt Group Relative Policy Optimization (GRPO), an algorithm that dispenses with a same-size critic model and instead derives baselines from group scores, thereby reducing computational overhead compared with standard PPO-style methods.  \n\nThe reward model used during RL is itself trained from DeepSeek-V3 SFT checkpoints, leveraging preference data that contains detailed chain-of-thought annotations. Beyond GRPO, the team applies a constitutional-AI style mechanism in which DeepSeek-V3‚Äôs own voting evaluations act as a feedback signal, broadening the sources of alignment feedback. Throughout the RL stage, the explicit objective is to fine-tune the policy so that model outputs better satisfy human and constitutional preferences while retaining the strong capabilities inherited from pre-training and SFT.",
  "2-3 (API)__evidence": [],
  "3-1 (Pre-training)__evidence": [
    {
      "source": "[pdf_text]",
      "quote": "We pre-train DeepSeek-V3 on 14.8 trillion diverse and high-quality tokens, followed by Supervised Fine-Tuning and Reinforcement Learning stages to fully harness its capabilities."
    },
    {
      "source": "[pdf_text]",
      "quote": "During pre-training, we train DeepSeek-V3 on 14.8T high-quality and diverse tokens."
    },
    {
      "source": "[pdf_text]",
      "quote": "During the pre-training stage, training DeepSeek-V3 on each trillion tokens requires only 180K H800 GPU hours, i.e., 3.7 days on our cluster with 2048 H800 GPUs."
    },
    {
      "source": "[pdf_text]",
      "quote": "Finally, the training corpus for DeepSeek-V3 consists of 14.8T high-quality and diverse tokens in our tokenizer."
    },
    {
      "source": "[pdf_text]",
      "quote": "In alignment with DeepSeekCoder-V2, we also incorporate the FIM strategy in the pre-training of DeepSeek-V3."
    },
    {
      "source": "[pdf_text]",
      "quote": "The tokenizer for DeepSeek-V3 employs Byte-level BPE (Shibata et al., 1999) with an extended vocabulary of 128K tokens."
    },
    {
      "source": "[pdf_text]",
      "quote": "We set the number of Transformer layers to 61 and the hidden dimension to 7168. Under this configuration, DeepSeek-V3 comprises 671B total parameters, of which 37B are activated for each token."
    },
    {
      "source": "[pdf_text]",
      "quote": "Training Hyper-Parameters. We employ the AdamW optimizer (Loshchilov and Hutter, 2017) with hyper-parameters set to ùõΩ1 = 0.9, ùõΩ2 = 0.95, and weight_decay = 0.1. We set the maximum sequence length to 4K during pre-training, and pre-train DeepSeek-V3 on 14.8T tokens."
    },
    {
      "source": "[pdf_text]",
      "quote": "Compared with DeepSeek-V2, we optimize the pre-training corpus by enhancing the ratio of mathematical and programming samples, while expanding multilingual coverage beyond English and Chinese."
    },
    {
      "source": "[pdf_text]",
      "quote": "We set the maximum sequence length to 4K during pre-training, and pre-train DeepSeek-V3 on 14.8T tokens."
    },
    {
      "source": "[pdf_text]",
      "quote": "Under this configuration, DeepSeek-V3 comprises 671B total parameters, of which 37B are activated for each token."
    },
    {
      "source": "[pdf_text]",
      "quote": "In this paper, we introduce DeepSeek-V3, a large MoE language model with 671B total parameters and 37B activated parameters, trained on 14.8T tokens. In addition to the MLA and DeepSeekMoE architectures, it also pioneers an auxiliary-loss-free strategy for load balancing and sets a multi-token prediction training objective for stronger performance."
    },
    {
      "source": "[pdf_text]",
      "quote": "The training of DeepSeek-V3 is cost-effective due to the support of FP8 training and meticulous engineering optimizations."
    },
    {
      "source": "[sections/2412.19437]",
      "quote": "We pre-train DeepSeek-V3 on 14.8 trillion diverse and high-quality tokens, followed by Supervised Fine-Tuning and Reinforcement Learning stages to fully harness its capabilities."
    },
    {
      "source": "[pdf_text]",
      "quote": "During the pre-training stage, training DeepSeek-V3 on each trillion tokens requires only 180K H800 GPU hours."
    },
    {
      "source": "[pdf_text]",
      "quote": "At an economical cost of only 2.664M H800 GPU hours, we complete the pre-training of DeepSeek-V3 on 14.8T tokens, producing the currently strongest open-source base model."
    },
    {
      "source": "[pdf_text]",
      "quote": "We employ the AdamW optimizer (Loshchilov and Hutter, 2017) with hyper-parameters set to ùõΩ1 = 0.9, ùõΩ2 = 0.95, and weight_decay = 0.1. We set the maximum sequence length to 4K during pre-training, and pre-train DeepSeek-V3 on 14.8T tokens."
    }
  ],
  "3-2 (Fine-tuning)__evidence": [
    {
      "source": "[pdf_text]",
      "quote": "Following this, we conduct post-training, including Supervised Fine-Tuning (SFT) and Reinforcement Learning (RL) on the base model of DeepSeek-V3, to align it with human preferences and further unlock its potential."
    },
    {
      "source": "[pdf_text]",
      "quote": "We fine-tune DeepSeek-V3-Base for two epochs using the SFT dataset, using the cosine decay learning rate scheduling that starts at 5 √ó 10‚àí6 and gradually decreases to 1 √ó 10‚àí6."
    },
    {
      "source": "[pdf_text]",
      "quote": "During the post-training stage, we distill the reasoning capability from the DeepSeek-R1 series of models, and meanwhile carefully maintain the balance between model accuracy and generation length."
    },
    {
      "source": "[pdf_text]",
      "quote": "The reward model is trained from the DeepSeek-V3 SFT checkpoints. To enhance its reliability, we construct preference data that not only provides the final reward but also includes the chain-of-thought leading to the reward."
    },
    {
      "source": "[pdf_text]",
      "quote": "The post-training also makes a success in distilling the reasoning capability from the DeepSeek-R1 series of models."
    },
    {
      "source": "[sections/2412.19437]",
      "quote": "We pre-train DeepSeek-V3 on 14.8 trillion diverse and high-quality tokens, followed by Supervised Fine-Tuning and Reinforcement Learning stages to fully harness its capabilities."
    },
    {
      "source": "[pdf_text]",
      "quote": "SFT Settings. We fine-tune DeepSeek-V3-Base for two epochs using the SFT dataset, using the cosine decay learning rate scheduling that starts at 5 √ó 10‚àí6 and gradually decreases to 1 √ó 10‚àí6."
    }
  ],
  "3-3 (Reinforcement Learning)__evidence": [
    {
      "source": "[pdf_text]",
      "quote": "Following this, we conduct post-training, including Supervised Fine-Tuning (SFT) and Reinforcement Learning (RL) on the base model of DeepSeek-V3, to align it with human preferences and further unlock its potential."
    },
    {
      "source": "[pdf_text]",
      "quote": "Similar to DeepSeek-V2 (DeepSeek-AI, 2024c), we adopt Group Relative Policy Optimization (GRPO) (Shao et al., 2024), which foregoes the critic model that is typically with the same size as the policy model, and estimates the baseline from group scores instead."
    },
    {
      "source": "[pdf_text]",
      "quote": "The reward model is trained from the DeepSeek-V3 SFT checkpoints."
    },
    {
      "source": "[pdf_text]",
      "quote": "We pre-train DeepSeek-V3 on 14.8 trillion diverse and high-quality tokens, followed by Supervised Fine-Tuning and Reinforcement Learning stages to fully harness its capabilities."
    },
    {
      "source": "[pdf_text]",
      "quote": "During the development of DeepSeek-V3, for these broader contexts, we employ the constitutional AI approach (Bai et al., 2022), leveraging the voting evaluation results of DeepSeek-V3 itself as a feedback source."
    },
    {
      "source": "[sections/2412.19437]",
      "quote": "We pre-train DeepSeek-V3 on 14.8 trillion diverse and high-quality tokens, followed by Supervised Fine-Tuning and Reinforcement Learning stages to fully harness its capabilities."
    }
  ]
}