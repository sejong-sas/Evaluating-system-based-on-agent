{
  "4-1 (Pre-training Data)": "DeepSeek-V3 is pre-trained on 14.8 trillion high-quality and diverse tokens. Multiple passages repeatedly stress that the entire pre-training corpus contains “14.8T tokens” (or “14.8 trillion” tokens) and that these tokens are judged to be high quality and to come from diverse sources. Compared with DeepSeek-V2, the creators explicitly “enhance the ratio of mathematical and programming samples” and also “expand multilingual coverage beyond English and Chinese,” while simultaneously “assign[ing] more training tokens to learn Chinese knowledge.” In total, therefore, DeepSeek-V3’s corpus is not only larger but re-weighted toward mathematics, coding, and a broader set of languages, with special emphasis on additional Chinese data.  \nThe model’s tokenizer uses “Byte-level BPE … with an extended vocabulary of 128 K tokens,” and the paper notes that DeepSeek-V3 is a “large MoE language model with 671 B total parameters and 37 B activated parameters,” all of which are exposed to the same 14.8 T-token corpus.  \nFinally, the pre-training stage also “incorporate[s] the FIM strategy,” echoing the approach used in DeepSeekCoder-V2, meaning fill-in-the-middle sampling was part of pre-training. In short, the pre-training data strategy centers on a 14.8 T-token, quality-filtered, math- and code-enriched, multilingual corpus processed by a 128 K-vocabulary byte-BPE tokenizer, augmented with FIM sampling.",
  "4-2 (Fine-tuning Data)": "After the base DeepSeek-V3 model is trained, the authors run a post-training phase “including Supervised Fine-Tuning (SFT) and Reinforcement Learning (RL).”  \n• Reasoning data: “For reasoning-related datasets, including those focused on mathematics, code competition problems, and logic puzzles, we generate the data by leveraging an internal DeepSeek-R1 model.” Separate lines re-affirm that these reasoning datasets are distilled from DeepSeek-R1 and that, during post-training, they “distill the reasoning capability from the DeepSeek-R1 series of models.”  \n• Non-reasoning data: “For non-reasoning data, such as creative writing, role-play, and simple question answering, we utilize DeepSeek-V2.5 to generate responses and enlist human annotators to verify the accuracy and correctness of the data.” The same point is made twice, underscoring that human verification is applied to the V2.5-generated answers.  \nThe reward model that will be used later for RL “is trained from the DeepSeek-V3 SFT checkpoints,” so the fine-tuning data directly seeds both the alignment model and the reward model.  \nOverall, SFT data are a mixture of internally-generated reasoning samples (DeepSeek-R1 origin) and non-reasoning conversational/creative samples (DeepSeek-V2.5 origin, filtered by human annotators), all intended to align DeepSeek-V3 with human preferences before RL.",
  "4-3 (Reinforcement Learning Data)": "Reinforcement Learning follows SFT: “Following this, we conduct post-training, including … Reinforcement Learning (RL) on the base model of DeepSeek-V3, to align it with human preferences.”  \n• Reward model: “The reward model is trained from the DeepSeek-V3 SFT checkpoints.” Another quote clarifies that the reward data “not only provides the final reward but also includes the chain-of-thought leading to the reward,” giving richer supervision signals.  \n• Algorithm: “Similar to DeepSeek-V2 … we adopt Group Relative Policy Optimization (GRPO),” which removes the usual large critic; instead, a baseline is “estimated from group scores.”  \nThus, the RL dataset consists of preference pairs (with explicit chains of thought) that are scored by a reward model initialized from the SFT stage, and the policy is optimized with GRPO to further align DeepSeek-V3 with user preferences.",
  "4-4 (Data Filtering)": "The paper provides several explicit statements about how data are filtered or left untouched for DeepSeek-V3.  \n1. Token retention: “Due to the effective load balancing strategy, DeepSeek-V3 keeps a good load balance during its full training. Therefore, DeepSeek-V3 does not drop any tokens during training.” A second sentence reiterates the same point that “DeepSeek-V3 does not drop any tokens.”  \n2. Corpus de-duplication / redundancy minimization: “Compared with DeepSeek-V2, we optimize the pre-training corpus … Also, our data processing pipeline is refined to minimize redundancy while maintaining corpus diversity.” The quote is repeated twice in the source, emphasizing redundancy removal as a filtering goal.  \n3. Vocabulary-level bias mitigation: The tokenizer “employs Byte-level BPE … with an extended vocabulary of 128 K tokens,” but this “may introduce the token boundary bias … when the model processes multi-line prompts.” To counteract that, the authors “randomly split a certain proportion of such combined tokens during training,” thereby exposing the model to more boundary cases and “mitigat[ing] this bias.” A second quote adds that the new pre-tokenizer “introduces tokens that combine punctuations and line breaks” and repeats the same random-splitting mitigation strategy.  \n4. Content re-balancing as implicit filtering: The same corpus-optimization quote notes that they “enhanc[e] the ratio of mathematical and programming samples” and “expand multilingual coverage,” which acts as a form of content-level filtering toward desired domains and languages.  \nConcrete criteria such as exact Jaccard or perplexity thresholds are not disclosed, but the authors explicitly state the following actionable filtering rules: (a) zero token dropping thanks to load balancing, (b) pipeline stages that de-duplicate to “minimize redundancy,” (c) tokenizer-level random splitting of a fraction of boundary tokens to combat bias, and (d) corpus re-weighting for math, code, and multilingual data.",
  "4-1 (Pre-training Data)__evidence": [
    {
      "source": "[pdf_text]",
      "quote": "During pre-training, we train DeepSeek-V3 on 14.8T high-quality and diverse tokens."
    },
    {
      "source": "[abstract]",
      "quote": "We pre-train DeepSeek-V3 on 14.8 trillion diverse and high-quality tokens, followed by Supervised Fine-Tuning and Reinforcement Learning stages to fully harness its capabilities."
    },
    {
      "source": "[pdf_text]",
      "quote": "Compared with DeepSeek-V2, we optimize the pre-training corpus by enhancing the ratio of mathematical and programming samples, while expanding multilingual coverage beyond English and Chinese. Finally, the training corpus for DeepSeek-V3 consists of 14.8T high-quality and diverse tokens in our tokenizer."
    },
    {
      "source": "[pdf_text]",
      "quote": "In alignment with DeepSeekCoder-V2, we also incorporate the FIM strategy in the pre-training of DeepSeek-V3."
    },
    {
      "source": "[pdf_text]",
      "quote": "We pre-train DeepSeek-V3 on 14.8 trillion diverse and high-quality tokens, followed by Supervised Fine-Tuning and Reinforcement Learning stages to fully harness its capabilities."
    },
    {
      "source": "[sections/4.1 Data Construction]",
      "quote": "Compared with DeepSeek-V2, we optimize the pre-training corpus by enhancing the ratio of mathematical and programming samples, while expanding multilingual coverage beyond English and Chinese. Finally, the training corpus for DeepSeek-V3 consists of 14.8T high-quality and diverse tokens in our tokenizer."
    },
    {
      "source": "[sections/4.1 Data Construction]",
      "quote": "The tokenizer for DeepSeek-V3 employs Byte-level BPE (Shibata et al., 1999) with an extended vocabulary of 128K tokens."
    },
    {
      "source": "[pdf_text]",
      "quote": "In this paper, we introduce DeepSeek-V3, a large MoE language model with 671B total parameters and 37B activated parameters, trained on 14.8T tokens."
    },
    {
      "source": "[pdf_text]",
      "quote": "DeepSeek-V3 assigns more training tokens to learn Chinese knowledge, leading to exceptional performance on the C-SimpleQA."
    },
    {
      "source": "[pdf_text]",
      "quote": "Finally, the training corpus for DeepSeek-V3 consists of 14.8T high-quality and diverse tokens in our tokenizer."
    },
    {
      "source": "[pdf_text]",
      "quote": "Compared with DeepSeek-V2, we optimize the pre-training corpus by enhancing the ratio of mathematical and programming samples, while expanding multilingual coverage beyond English and Chinese."
    }
  ],
  "4-2 (Fine-tuning Data)__evidence": [
    {
      "source": "[pdf_text]",
      "quote": "Following this, we conduct post-training, including Supervised Fine-Tuning (SFT) and Reinforcement Learning (RL) on the base model of DeepSeek-V3, to align it with human preferences and further unlock its potential."
    },
    {
      "source": "[pdf_text]",
      "quote": "For reasoning-related datasets, including those focused on mathematics, code competition problems, and logic puzzles, we generate the data by leveraging an internal DeepSeek-R1 model."
    },
    {
      "source": "[pdf_text]",
      "quote": "For non-reasoning data, such as creative writing, role-play, and simple question answering, we utilize DeepSeek-V2.5 to generate responses and enlist human annotators to verify the accuracy and correctness of the data."
    },
    {
      "source": "[sections/5.1 Supervised Fine-Tuning]",
      "quote": "For reasoning-related datasets, including those focused on mathematics, code competition problems, and logic puzzles, we generate the data by leveraging an internal DeepSeek-R1 model."
    },
    {
      "source": "[sections/5.1 Supervised Fine-Tuning]",
      "quote": "For non-reasoning data, such as creative writing, role-play, and simple question answering, we utilize DeepSeek-V2.5 to generate responses and enlist human annotators to verify the accuracy and correctness of the data."
    },
    {
      "source": "[pdf_text]",
      "quote": "The reward model is trained from the DeepSeek-V3 SFT checkpoints."
    },
    {
      "source": "[pdf_text]",
      "quote": "During the post-training stage, we distill the reasoning capability from the DeepSeek-R1 series of models, and meanwhile carefully maintain the balance between model accuracy and generation length."
    },
    {
      "source": "[pdf_text]",
      "quote": "Reasoning Data. For reasoning-related datasets, including those focused on mathematics, code competition problems, and logic puzzles, we generate the data by leveraging an internal DeepSeek-R1 model."
    },
    {
      "source": "[pdf_text]",
      "quote": "Non-Reasoning Data. For non-reasoning data, such as creative writing, role-play, and simple question answering, we utilize DeepSeek-V2.5 to generate responses and enlist human annotators to verify the accuracy and correctness of the data."
    }
  ],
  "4-3 (Reinforcement Learning Data)__evidence": [
    {
      "source": "[pdf_text]",
      "quote": "Following this, we conduct post-training, including Supervised Fine-Tuning (SFT) and Reinforcement Learning (RL) on the base model of DeepSeek-V3, to align it with human preferences and further unlock its potential."
    },
    {
      "source": "[pdf_text]",
      "quote": "The reward model is trained from the DeepSeek-V3 SFT checkpoints."
    },
    {
      "source": "[sections/5.2 Reinforcement Learning]",
      "quote": "The reward model is trained from the DeepSeek-V3 SFT checkpoints."
    },
    {
      "source": "[pdf_text]",
      "quote": "The reward model is trained from the DeepSeek-V3 SFT checkpoints. To enhance its reliability, we construct preference data that not only provides the final reward but also includes the chain-of-thought leading to the reward."
    },
    {
      "source": "[pdf_text]",
      "quote": "Similar to DeepSeek-V2 (DeepSeek-AI, 2024c), we adopt Group Relative Policy Optimization (GRPO) (Shao et al., 2024), which foregoes the critic model that is typically with the same size as the policy model, and estimates the baseline from group scores instead."
    }
  ],
  "4-4 (Data Filtering)__evidence": [
    {
      "source": "[pdf_text]",
      "quote": "No Token-Dropping. Due to the effective load balancing strategy, DeepSeek-V3 keeps a good load balance during its full training. Therefore, DeepSeek-V3 does not drop any tokens during training."
    },
    {
      "source": "[pdf_text]",
      "quote": "Compared with DeepSeek-V2, we optimize the pre-training corpus by enhancing the ratio of mathematical and programming samples, while expanding multilingual coverage beyond English and Chinese. Also, our data processing pipeline is refined to minimize redundancy while maintaining corpus diversity."
    },
    {
      "source": "[pdf_text]",
      "quote": "In addition, compared with DeepSeek-V2, the new pretokenizer introduces tokens that combine punctuations and line breaks. To address this issue, we randomly split a certain proportion of such combined tokens during training, which exposes the model to a wider array of special cases and mitigates this bias."
    },
    {
      "source": "[sections/4.1 Data Construction]",
      "quote": "Compared with DeepSeek-V2, we optimize the pre-training corpus by enhancing the ratio of mathematical and programming samples, while expanding multilingual coverage beyond English and Chinese. Also, our data processing pipeline is refined to minimize redundancy while maintaining corpus diversity."
    },
    {
      "source": "[sections/4.1 Data Construction]",
      "quote": "The tokenizer for DeepSeek-V3 employs Byte-level BPE (Shibata et al., 1999) with an extended vocabulary of 128K tokens. However, this trick may introduce the token boundary bias (Lundberg, 2023) when the model processes multi-line prompts without terminal line breaks, particularly for few-shot evaluation prompts. To address this issue, we randomly split a certain proportion of such combined tokens during training, which exposes the model to a wider array of special cases and mitigates this bias."
    },
    {
      "source": "[pdf_text]",
      "quote": "Due to the effective load balancing strategy, DeepSeek-V3 keeps a good load balance during its full training. Therefore, DeepSeek-V3 does not drop any tokens during training."
    },
    {
      "source": "[pdf_text]",
      "quote": "In addition, compared with DeepSeek-V2, the new pretokenizer introduces tokens that combine punctuations and line breaks. However, this trick may introduce the token boundary bias (Lundberg, 2023) when the model processes multi-line prompts without terminal line breaks, particularly for few-shot evaluation prompts. To address this issue, we randomly split a certain proportion of such combined tokens during training, which exposes the model to a wider array of special cases and mitigates this bias."
    }
  ]
}