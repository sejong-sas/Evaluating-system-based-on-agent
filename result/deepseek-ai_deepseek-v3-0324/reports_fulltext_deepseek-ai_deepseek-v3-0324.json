{
  "model_id": "deepseek-ai/deepseek-v3-0324",
  "full_texts": [
    {
      "arxiv_id": "https://r.jina.ai/https://arxiv.org/abs/2412.19437},",
      "full_text": "Title: [2412.19437},] Article identifier not recognized\n\nURL Source: https://arxiv.org/abs/2412.19437%7D,\n\nWarning: Target URL returned error 404: Not Found\n\nMarkdown Content:\n[2412.19437},] Article identifier not recognized\n\n===============\n\n[Skip to main content](https://arxiv.org/abs/2412.19437%7D,#content)\n\n[![Image 1: Cornell University Logo](https://arxiv.org/static/browse/0.3.4/images/icons/cu/cornell-reduced-white-SMALL.svg)](https://www.cornell.edu/)\n\nWe gratefully acknowledge support from the Simons Foundation, [member institutions](https://info.arxiv.org/about/ourmembers.html), and all contributors.[Donate](https://info.arxiv.org/about/donate.html)\n\n[](https://arxiv.org/IgnoreMe)\n![Image 2: arxiv logo](https://arxiv.org/static/browse/0.3.4/images/arxiv-logo-one-color-white.svg)\n===================================================================================================\n\n[Help](https://info.arxiv.org/help) | [Advanced Search](https://arxiv.org/search/advanced)\n\nSearch\n\n[![Image 3: arXiv logo](https://arxiv.org/static/browse/0.3.4/images/arxiv-logomark-small-white.svg)](https://arxiv.org/)\n\n[![Image 4: Cornell University Logo](https://arxiv.org/static/browse/0.3.4/images/icons/cu/cornell-reduced-white-SMALL.svg)](https://www.cornell.edu/)\n\nGO\n\nquick links\n-----------\n\n*   [Login](https://arxiv.org/login)\n*   [Help Pages](https://info.arxiv.org/help)\n*   [About](https://info.arxiv.org/about)\n\nArticle identifier '2412.19437},' not recognized\n================================================\n\nYou might instead try to [search for articles](https://arxiv.org/search) using title or author information.\n\nFor additional help on arXiv identifiers, see [understanding the arXiv identifier](https://info.arxiv.org/help/arxiv_identifier.html).\n\n*   [About](https://info.arxiv.org/about)\n*   [Help](https://info.arxiv.org/help)\n\n*   [Contact](https://info.arxiv.org/help/contact.html)\n*   [Subscribe](https://info.arxiv.org/help/subscribe)\n\n*   [Copyright](https://info.arxiv.org/help/license/index.html)\n*   [Privacy Policy](https://info.arxiv.org/help/policies/privacy_policy.html)\n\n*   [Web Accessibility Assistance](https://info.arxiv.org/help/web_accessibility.html)\n*   [arXiv Operational Status](https://status.arxiv.org/)\n\n Get status notifications via [email](https://subscribe.sorryapp.com/24846f03/email/new) or [slack](https://subscribe.sorryapp.com/24846f03/slack/new)\n",
      "fetch_method": "jina-reader"
    },
    {
      "arxiv_id": "https://arxiv.org/pdf/2412.19437",
      "full_text": "DeepSeek-V3 Technical Report\nDeepSeek-AI\nresearch@deepseek.com\nAbstract\nWe present DeepSeek-V3, a strong Mixture-of-Experts (MoE) language model with 671B total\nparameters with 37B activated for each token. To achieve efficient inference and cost-effective\ntraining, DeepSeek-V3 adopts Multi-head Latent Attention (MLA) and DeepSeekMoE architec-\ntures, which were thoroughly validated in DeepSeek-V2. Furthermore, DeepSeek-V3 pioneers\nan auxiliary-loss-free strategy for load balancing and sets a multi-token prediction training\nobjective for stronger performance. We pre-train DeepSeek-V3 on 14.8 trillion diverse and\nhigh-quality tokens, followed by Supervised Fine-Tuning and Reinforcement Learning stages to\nfully harness its capabilities. Comprehensive evaluations reveal that DeepSeek-V3 outperforms\nother open-source models and achieves performance comparable to leading closed-source\nmodels. Despite its excellent performance, DeepSeek-V3 requires only 2.788M H800 GPU hours\nfor its full training. In addition, its training process is remarkably stable. Throughout the entire\ntraining process, we did not experience any irrecoverable loss spikes or perform any rollbacks.\nThe model checkpoints are available at https://github.com/deepseek-ai/DeepSeek-V3.\nMMLU-Pro\n(EM)\nGPQA-Diamond\n(Pass@1)\nMATH 500\n(EM)\nAIME 2024\n(Pass@1)\nCodeforces\n(Percentile)\nSWE-bench Verified\n(Resolved)\n0\n20\n40\n60\n80\n100\nAccuracy / Percentile (%)\n75.9\n59.1\n90.2\n39.2\n51.6\n42.0\n66.2\n41.3\n74.7\n16.7\n35.6\n22.6\n71.6\n49.0\n80.0\n23.3\n24.8\n23.8\n73.3\n51.1\n73.8\n23.3\n25.3\n24.5\n72.6\n49.9\n74.6\n9.3\n23.6\n38.8\n78.0\n65.0\n78.3\n16.0\n20.3\n50.8\nDeepSeek-V3\nDeepSeek-V2.5\nQwen2.5-72B-Inst\nLlama-3.1-405B-Inst\nGPT-4o-0513\nClaude-3.5-Sonnet-1022\nFigure 1 | Benchmark performance of DeepSeek-V3 and its counterparts.\narXiv:2412.19437v2  [cs.CL]  18 Feb 2025\n\nContents\n1\nIntroduction\n4\n2\nArchitecture\n6\n2.1\nBasic Architecture . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n6\n2.1.1\nMulti-Head Latent Attention . . . . . . . . . . . . . . . . . . . . . . . . . .\n7\n2.1.2\nDeepSeekMoE with Auxiliary-Loss-Free Load Balancing . . . . . . . . . .\n8\n2.2\nMulti-Token Prediction . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n10\n3\nInfrastructures\n11\n3.1\nCompute Clusters . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n11\n3.2\nTraining Framework\n. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n12\n3.2.1\nDualPipe and Computation-Communication Overlap . . . . . . . . . . . .\n12\n3.2.2\nEfficient Implementation of Cross-Node All-to-All Communication . . . .\n13\n3.2.3\nExtremely Memory Saving with Minimal Overhead . . . . . . . . . . . . .\n14\n3.3\nFP8 Training . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n14\n3.3.1\nMixed Precision Framework\n. . . . . . . . . . . . . . . . . . . . . . . . . .\n15\n3.3.2\nImproved Precision from Quantization and Multiplication . . . . . . . . .\n16\n3.3.3\nLow-Precision Storage and Communication\n. . . . . . . . . . . . . . . . .\n18\n3.4\nInference and Deployment . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n18\n3.4.1\nPrefilling . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n19\n3.4.2\nDecoding\n. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n19\n3.5\nSuggestions on Hardware Design . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n20\n3.5.1\nCommunication Hardware\n. . . . . . . . . . . . . . . . . . . . . . . . . . .\n20\n3.5.2\nCompute Hardware\n. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n20\n4\nPre-Training\n21\n4.1\nData Construction . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n21\n4.2\nHyper-Parameters . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n22\n4.3\nLong Context Extension\n. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n23\n4.4\nEvaluations\n. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n24\n4.4.1\nEvaluation Benchmarks . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n24\n4.4.2\nEvaluation Results . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n24\n4.5\nDiscussion . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n26\n4.5.1\nAblation Studies for Multi-Token Prediction\n. . . . . . . . . . . . . . . . .\n26\n4.5.2\nAblation Studies for the Auxiliary-Loss-Free Balancing Strategy . . . . . .\n26\n2\n\n4.5.3\nBatch-Wise Load Balance VS. Sequence-Wise Load Balance . . . . . . . . .\n27\n5\nPost-Training\n28\n5.1\nSupervised Fine-Tuning\n. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n28\n5.2\nReinforcement Learning . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n29\n5.2.1\nReward Model\n. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n29\n5.2.2\nGroup Relative Policy Optimization . . . . . . . . . . . . . . . . . . . . . .\n30\n5.3\nEvaluations\n. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n30\n5.3.1\nEvaluation Settings . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n30\n5.3.2\nStandard Evaluation . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n31\n5.3.3\nOpen-Ended Evaluation . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n33\n5.3.4\nDeepSeek-V3 as a Generative Reward Model . . . . . . . . . . . . . . . . .\n33\n5.4\nDiscussion . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n34\n5.4.1\nDistillation from DeepSeek-R1\n. . . . . . . . . . . . . . . . . . . . . . . . .\n34\n5.4.2\nSelf-Rewarding . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n34\n5.4.3\nMulti-Token Prediction Evaluation . . . . . . . . . . . . . . . . . . . . . . .\n35\n6\nConclusion, Limitations, and Future Directions\n35\nA Contributions and Acknowledgments\n45\nB Ablation Studies for Low-Precision Training\n47\nB.1\nFP8 v.s. BF16 Training\n. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n47\nB.2\nDiscussion About Block-Wise Quantization . . . . . . . . . . . . . . . . . . . . . .\n47\nC Expert Specialization Patterns of the 16B Aux-Loss-Based and Aux-Loss-Free Models 48\n3\n\n1. Introduction\nIn recent years, Large Language Models (LLMs) have been undergoing rapid iteration and\nevolution (Anthropic, 2024; Google, 2024; OpenAI, 2024a), progressively diminishing the gap to-\nwards Artificial General Intelligence (AGI). Beyond closed-source models, open-source models,\nincluding DeepSeek series (DeepSeek-AI, 2024a,b,c; Guo et al., 2024), LLaMA series (AI@Meta,\n2024a,b; Touvron et al., 2023a,b), Qwen series (Qwen, 2023, 2024a,b), and Mistral series (Jiang\net al., 2023; Mistral, 2024), are also making significant strides, endeavoring to close the gap with\ntheir closed-source counterparts. To further push the boundaries of open-source model capa-\nbilities, we scale up our models and introduce DeepSeek-V3, a large Mixture-of-Experts (MoE)\nmodel with 671B parameters, of which 37B are activated for each token.\nWith a forward-looking perspective, we consistently strive for strong model performance\nand economical costs. Therefore, in terms of architecture, DeepSeek-V3 still adopts Multi-head\nLatent Attention (MLA) (DeepSeek-AI, 2024c) for efficient inference and DeepSeekMoE (Dai\net al., 2024) for cost-effective training. These two architectures have been validated in DeepSeek-\nV2 (DeepSeek-AI, 2024c), demonstrating their capability to maintain robust model performance\nwhile achieving efficient training and inference. Beyond the basic architecture, we implement\ntwo additional strategies to further enhance the model capabilities. Firstly, DeepSeek-V3 pi-\noneers an auxiliary-loss-free strategy (Wang et al., 2024a) for load balancing, with the aim of\nminimizing the adverse impact on model performance that arises from the effort to encourage\nload balancing. Secondly, DeepSeek-V3 employs a multi-token prediction training objective,\nwhich we have observed to enhance the overall performance on evaluation benchmarks.\nIn order to achieve efficient training, we support the FP8 mixed precision training and\nimplement comprehensive optimizations for the training framework. Low-precision training\nhas emerged as a promising solution for efficient training (Dettmers et al., 2022; Kalamkar et al.,\n2019; Narang et al., 2017; Peng et al., 2023b), its evolution being closely tied to advancements in\nhardware capabilities (Luo et al., 2024; Micikevicius et al., 2022; Rouhani et al., 2023a). In this\nwork, we introduce an FP8 mixed precision training framework and, for the first time, validate\nits effectiveness on an extremely large-scale model. Through the support for FP8 computation\nand storage, we achieve both accelerated training and reduced GPU memory usage. As for\nthe training framework, we design the DualPipe algorithm for efficient pipeline parallelism,\nwhich has fewer pipeline bubbles and hides most of the communication during training through\ncomputation-communication overlap. This overlap ensures that, as the model further scales up,\nas long as we maintain a constant computation-to-communication ratio, we can still employ\nfine-grained experts across nodes while achieving a near-zero all-to-all communication overhead.\nIn addition, we also develop efficient cross-node all-to-all communication kernels to fully utilize\nInfiniBand (IB) and NVLink bandwidths. Furthermore, we meticulously optimize the memory\nfootprint, making it possible to train DeepSeek-V3 without using costly tensor parallelism.\nCombining these efforts, we achieve high training efficiency.\nDuring pre-training, we train DeepSeek-V3 on 14.8T high-quality and diverse tokens. The\npre-training process is remarkably stable. Throughout the entire training process, we did not\nencounter any irrecoverable loss spikes or have to roll back. Next, we conduct a two-stage\ncontext length extension for DeepSeek-V3. In the first stage, the maximum context length is\nextended to 32K, and in the second stage, it is further extended to 128K. Following this, we\nconduct post-training, including Supervised Fine-Tuning (SFT) and Reinforcement Learning (RL)\non the base model of DeepSeek-V3, to align it with human preferences and further unlock its\npotential. During the post-training stage, we distill the reasoning capability from the DeepSeek-\nR1 series of models, and meanwhile carefully maintain the balance between model accuracy\n4\n\nTraining Costs\nPre-Training\nContext Extension\nPost-Training\nTotal\nin H800 GPU Hours\n2664K\n119K\n5K\n2788K\nin USD\n$5.328M\n$0.238M\n$0.01M\n$5.576M\nTable 1 | Training costs of DeepSeek-V3, assuming the rental price of H800 is $2 per GPU hour.\nand generation length.\nWe evaluate DeepSeek-V3 on a comprehensive array of benchmarks. Despite its economical\ntraining costs, comprehensive evaluations reveal that DeepSeek-V3-Base has emerged as the\nstrongest open-source base model currently available, especially in code and math. Its chat\nversion also outperforms other open-source models and achieves performance comparable to\nleading closed-source models, including GPT-4o and Claude-3.5-Sonnet, on a series of standard\nand open-ended benchmarks.\nLastly, we emphasize again the economical training costs of DeepSeek-V3, summarized in\nTable 1, achieved through our optimized co-design of algorithms, frameworks, and hardware.\nDuring the pre-training stage, training DeepSeek-V3 on each trillion tokens requires only 180K\nH800 GPU hours, i.e., 3.7 days on our cluster with 2048 H800 GPUs. Consequently, our pre-\ntraining stage is completed in less than two months and costs 2664K GPU hours. Combined\nwith 119K GPU hours for the context length extension and 5K GPU hours for post-training,\nDeepSeek-V3 costs only 2.788M GPU hours for its full training. Assuming the rental price of\nthe H800 GPU is $2 per GPU hour, our total training costs amount to only $5.576M. Note that\nthe aforementioned costs include only the official training of DeepSeek-V3, excluding the costs\nassociated with prior research and ablation experiments on architectures, algorithms, or data.\nOur main contribution includes:\nArchitecture: Innovative Load Balancing Strategy and Training Objective\n• On top of the efficient architecture of DeepSeek-V2, we pioneer an auxiliary-loss-free\nstrategy for load balancing, which minimizes the performance degradation that arises\nfrom encouraging load balancing.\n• We investigate a Multi-Token Prediction (MTP) objective and prove it beneficial to model\nperformance. It can also be used for speculative decoding for inference acceleration.\nPre-Training: Towards Ultimate Training Efficiency\n• We design an FP8 mixed precision training framework and, for the first time, validate the\nfeasibility and effectiveness of FP8 training on an extremely large-scale model.\n• Through the co-design of algorithms, frameworks, and hardware, we overcome the\ncommunication bottleneck in cross-node MoE training, achieving near-full computation-\ncommunication overlap. This significantly enhances our training efficiency and reduces the\ntraining costs, enabling us to further scale up the model size without additional overhead.\n• At an economical cost of only 2.664M H800 GPU hours, we complete the pre-training of\nDeepSeek-V3 on 14.8T tokens, producing the currently strongest open-source base model.\nThe subsequent training stages after pre-training require only 0.1M GPU hours.\nPost-Training: Knowledge Distillation from DeepSeek-R1\n• We introduce an innovative methodology to distill reasoning capabilities from the long-\nChain-of-Thought (CoT) model, specifically from one of the DeepSeek R1 series models,\ninto standard LLMs, particularly DeepSeek-V3. Our pipeline elegantly incorporates the\n5\n\nverification and reflection patterns of R1 into DeepSeek-V3 and notably improves its\nreasoning performance. Meanwhile, we also maintain control over the output style and\nlength of DeepSeek-V3.\nSummary of Core Evaluation Results\n• Knowledge: (1) On educational benchmarks such as MMLU, MMLU-Pro, and GPQA,\nDeepSeek-V3 outperforms all other open-source models, achieving 88.5 on MMLU, 75.9\non MMLU-Pro, and 59.1 on GPQA. Its performance is comparable to leading closed-source\nmodels like GPT-4o and Claude-Sonnet-3.5, narrowing the gap between open-source\nand closed-source models in this domain. (2) For factuality benchmarks, DeepSeek-V3\ndemonstrates superior performance among open-source models on both SimpleQA and\nChinese SimpleQA. While it trails behind GPT-4o and Claude-Sonnet-3.5 in English factual\nknowledge (SimpleQA), it surpasses these models in Chinese factual knowledge (Chinese\nSimpleQA), highlighting its strength in Chinese factual knowledge.\n• Code, Math, and Reasoning: (1) DeepSeek-V3 achieves state-of-the-art performance on\nmath-related benchmarks among all non-long-CoT open-source and closed-source models.\nNotably, it even outperforms o1-preview on specific benchmarks, such as MATH-500,\ndemonstrating its robust mathematical reasoning capabilities. (2) On coding-related tasks,\nDeepSeek-V3 emerges as the top-performing model for coding competition benchmarks,\nsuch as LiveCodeBench, solidifying its position as the leading model in this domain. For\nengineering-related tasks, while DeepSeek-V3 performs slightly below Claude-Sonnet-3.5,\nit still outpaces all other models by a significant margin, demonstrating its competitiveness\nacross diverse technical benchmarks.\nIn the remainder of this paper, we first present a detailed exposition of our DeepSeek-V3\nmodel architecture (Section 2). Subsequently, we introduce our infrastructures, encompassing\nour compute clusters, the training framework, the support for FP8 training, the inference\ndeployment strategy, and our suggestions on future hardware design. Next, we describe our\npre-training process, including the construction of training data, hyper-parameter settings, long-\ncontext extension techniques, the associated evaluations, as well as some discussions (Section 4).\nThereafter, we discuss our efforts on post-training, which include Supervised Fine-Tuning (SFT),\nReinforcement Learning (RL), the corresponding evaluations, and discussions (Section 5). Lastly,\nwe conclude this work, discuss existing limitations of DeepSeek-V3, and propose potential\ndirections for future research (Section 6).\n2. Architecture\nWe first introduce the basic architecture of DeepSeek-V3, featured by Multi-head Latent Atten-\ntion (MLA) (DeepSeek-AI, 2024c) for efficient inference and DeepSeekMoE (Dai et al., 2024)\nfor economical training. Then, we present a Multi-Token Prediction (MTP) training objective,\nwhich we have observed to enhance the overall performance on evaluation benchmarks. For\nother minor details not explicitly mentioned, DeepSeek-V3 adheres to the settings of DeepSeek-\nV2 (DeepSeek-AI, 2024c).\n2.1. Basic Architecture\nThe basic architecture of DeepSeek-V3 is still within the Transformer (Vaswani et al., 2017)\nframework. For efficient inference and economical training, DeepSeek-V3 also adopts MLA\nand DeepSeekMoE, which have been thoroughly validated by DeepSeek-V2. Compared with\nDeepSeek-V2, an exception is that we additionally introduce an auxiliary-loss-free load balancing\n6\n\n…\nRouter\nInput Hidden 𝐮𝐮𝑡𝑡\nOutput Hidden 𝐡𝐡𝑡𝑡\n′\n1\n𝑁𝑁𝑠𝑠\n1\n2\n𝑁𝑁𝑟𝑟-1\n𝑁𝑁𝑟𝑟\nShared Expert\nRouted Expert\nTop-𝐾𝐾𝑟𝑟\nAttention\nFeed-Forward Network\n…\n3\n4\nRMSNorm\nRMSNorm\nTransformer Block ×𝐿𝐿\nDeepSeekMoE\n0\nInput Hidden 𝐡𝐡𝑡𝑡\nMulti-Head Latent Attention (MLA)\n0\n{𝐪𝐪𝑡𝑡,𝑖𝑖\n𝐶𝐶}\n{𝐯𝐯𝑡𝑡,𝑖𝑖\n𝐶𝐶}\n{𝐤𝐤𝑡𝑡,𝑖𝑖\n𝐶𝐶}\nLatent 𝐜𝐜𝑡𝑡\n𝐾𝐾𝐾𝐾\nLatent 𝐜𝐜𝑡𝑡\n𝑄𝑄\n{𝐪𝐪𝑡𝑡,𝑖𝑖\n𝑅𝑅}\n𝐤𝐤𝑡𝑡\n𝑅𝑅\nCached During Inference\nMulti-Head Attention\nconcatenate\nconcatenate\n{[𝐪𝐪𝑡𝑡,𝑖𝑖\n𝐶𝐶; 𝐪𝐪𝑡𝑡,𝑖𝑖\n𝑅𝑅]}\n{[𝐤𝐤𝑡𝑡,𝑖𝑖\n𝐶𝐶; 𝐤𝐤𝑡𝑡\n𝑅𝑅]}\n…\nOutput Hidden 𝐮𝐮𝑡𝑡\n…\n… …\n… …\n1\n… …\n… …\napply\nRoPE\napply\nRoPE\nFigure 2 | Illustration of the basic architecture of DeepSeek-V3. Following DeepSeek-V2, we\nadopt MLA and DeepSeekMoE for efficient inference and economical training.\nstrategy (Wang et al., 2024a) for DeepSeekMoE to mitigate the performance degradation induced\nby the effort to ensure load balance. Figure 2 illustrates the basic architecture of DeepSeek-V3,\nand we will briefly review the details of MLA and DeepSeekMoE in this section.\n2.1.1. Multi-Head Latent Attention\nFor attention, DeepSeek-V3 adopts the MLA architecture. Let 𝑑denote the embedding dimen-\nsion, 𝑛ℎdenote the number of attention heads, 𝑑ℎdenote the dimension per head, and h𝑡∈R𝑑\ndenote the attention input for the 𝑡-th token at a given attention layer. The core of MLA is the\nlow-rank joint compression for attention keys and values to reduce Key-Value (KV) cache during\ninference:\nc𝐾𝑉\n𝑡\n= 𝑊𝐷𝐾𝑉h𝑡,\n(1)\n[k𝐶\n𝑡,1; k𝐶\n𝑡,2; ...; k𝐶\n𝑡,𝑛ℎ] = k𝐶\n𝑡= 𝑊𝑈𝐾c𝐾𝑉\n𝑡,\n(2)\nk𝑅\n𝑡\n= RoPE(𝑊𝐾𝑅h𝑡),\n(3)\nk𝑡,𝑖= [k𝐶\n𝑡,𝑖; k𝑅\n𝑡],\n(4)\n[v𝐶\n𝑡,1; v𝐶\n𝑡,2; ...; v𝐶\n𝑡,𝑛ℎ] = v𝐶\n𝑡= 𝑊𝑈𝑉c𝐾𝑉\n𝑡,\n(5)\n7\n\nwhere c𝐾𝑉\n𝑡\n∈R𝑑𝑐is the compressed latent vector for keys and values; 𝑑𝑐(≪𝑑ℎ𝑛ℎ) indicates the KV\ncompression dimension; 𝑊𝐷𝐾𝑉∈R𝑑𝑐×𝑑denotes the down-projection matrix; 𝑊𝑈𝐾,𝑊𝑈𝑉∈R𝑑ℎ𝑛ℎ×𝑑𝑐\nare the up-projection matrices for keys and values, respectively; 𝑊𝐾𝑅∈R𝑑𝑅\nℎ×𝑑is the matrix used\nto produce the decoupled key that carries Rotary Positional Embedding (RoPE) (Su et al., 2024);\nRoPE(·) denotes the operation that applies RoPE matrices; and [·; ·] denotes concatenation. Note\nthat for MLA, only the blue-boxed vectors (i.e., c𝐾𝑉\n𝑡\nand k𝑅\n𝑡) need to be cached during generation,\nwhich results in significantly reduced KV cache while maintaining performance comparable to\nstandard Multi-Head Attention (MHA) (Vaswani et al., 2017).\nFor the attention queries, we also perform a low-rank compression, which can reduce the\nactivation memory during training:\nc𝑄\n𝑡= 𝑊𝐷𝑄h𝑡,\n(6)\n[q𝐶\n𝑡,1; q𝐶\n𝑡,2; ...; q𝐶\n𝑡,𝑛ℎ] = q𝐶\n𝑡= 𝑊𝑈𝑄c𝑄\n𝑡,\n(7)\n[q𝑅\n𝑡,1; q𝑅\n𝑡,2; ...; q𝑅\n𝑡,𝑛ℎ] = q𝑅\n𝑡= RoPE(𝑊𝑄𝑅c𝑄\n𝑡),\n(8)\nq𝑡,𝑖= [q𝐶\n𝑡,𝑖; q𝑅\n𝑡,𝑖],\n(9)\nwhere c𝑄\n𝑡\n∈R𝑑′\n𝑐is the compressed latent vector for queries; 𝑑′\n𝑐(≪𝑑ℎ𝑛ℎ) denotes the query\ncompression dimension; 𝑊𝐷𝑄∈R𝑑′\n𝑐×𝑑,𝑊𝑈𝑄∈R𝑑ℎ𝑛ℎ×𝑑′\n𝑐are the down-projection and up-projection\nmatrices for queries, respectively; and 𝑊𝑄𝑅∈R𝑑𝑅\nℎ𝑛ℎ×𝑑′\n𝑐is the matrix to produce the decoupled\nqueries that carry RoPE.\nUltimately, the attention queries (q𝑡,𝑖), keys (k𝑗,𝑖), and values (v𝐶\n𝑗,𝑖) are combined to yield the\nfinal attention output u𝑡:\no𝑡,𝑖=\n𝑡∑︁\n𝑗=1\nSoftmax𝑗(\nq𝑇\n𝑡,𝑖k𝑗,𝑖\n√︃\n𝑑ℎ+ 𝑑𝑅\nℎ\n)v𝐶\n𝑗,𝑖,\n(10)\nu𝑡= 𝑊𝑂[o𝑡,1; o𝑡,2; ...; o𝑡,𝑛ℎ],\n(11)\nwhere 𝑊𝑂∈R𝑑×𝑑ℎ𝑛ℎdenotes the output projection matrix.\n2.1.2. DeepSeekMoE with Auxiliary-Loss-Free Load Balancing\nBasic Architecture of DeepSeekMoE.\nFor Feed-Forward Networks (FFNs), DeepSeek-V3\nemploys the DeepSeekMoE architecture (Dai et al., 2024). Compared with traditional MoE\narchitectures like GShard (Lepikhin et al., 2021), DeepSeekMoE uses finer-grained experts and\nisolates some experts as shared ones. Let u𝑡denote the FFN input of the 𝑡-th token, we compute\nthe FFN output h′\n𝑡as follows:\nh′\n𝑡= u𝑡+\n𝑁𝑠\n∑︁\n𝑖=1\nFFN(𝑠)\n𝑖\n(u𝑡) +\n𝑁𝑟\n∑︁\n𝑖=1\n𝑔𝑖,𝑡FFN(𝑟)\n𝑖\n(u𝑡),\n(12)\n𝑔𝑖,𝑡=\n𝑔′\n𝑖,𝑡\nÍ𝑁𝑟\n𝑗=1 𝑔′\n𝑗,𝑡\n,\n(13)\n𝑔′\n𝑖,𝑡=\n(\n𝑠𝑖,𝑡,\n𝑠𝑖,𝑡∈Topk({𝑠𝑗,𝑡|1 ⩽𝑗⩽𝑁𝑟}, 𝐾𝑟),\n0,\notherwise,\n(14)\n𝑠𝑖,𝑡= Sigmoid \u0000u𝑡\n𝑇e𝑖\n\u0001 ,\n(15)\n8\n\nwhere 𝑁𝑠and 𝑁𝑟denote the numbers of shared experts and routed experts, respectively; FFN(𝑠)\n𝑖\n(·)\nand FFN(𝑟)\n𝑖\n(·) denote the 𝑖-th shared expert and the 𝑖-th routed expert, respectively; 𝐾𝑟denotes\nthe number of activated routed experts; 𝑔𝑖,𝑡is the gating value for the 𝑖-th expert; 𝑠𝑖,𝑡is the\ntoken-to-expert affinity; e𝑖is the centroid vector of the 𝑖-th routed expert; and Topk(·, 𝐾) denotes\nthe set comprising 𝐾highest scores among the affinity scores calculated for the 𝑡-th token and\nall routed experts. Slightly different from DeepSeek-V2, DeepSeek-V3 uses the sigmoid function\nto compute the affinity scores, and applies a normalization among all selected affinity scores to\nproduce the gating values.\nAuxiliary-Loss-Free Load Balancing.\nFor MoE models, an unbalanced expert load will lead to\nrouting collapse (Shazeer et al., 2017) and diminish computational efficiency in scenarios with\nexpert parallelism. Conventional solutions usually rely on the auxiliary loss (Fedus et al., 2021;\nLepikhin et al., 2021) to avoid unbalanced load. However, too large an auxiliary loss will impair\nthe model performance (Wang et al., 2024a). To achieve a better trade-off between load balance\nand model performance, we pioneer an auxiliary-loss-free load balancing strategy (Wang et al.,\n2024a) to ensure load balance. To be specific, we introduce a bias term 𝑏𝑖for each expert and\nadd it to the corresponding affinity scores 𝑠𝑖,𝑡to determine the top-K routing:\n𝑔′\n𝑖,𝑡=\n(\n𝑠𝑖,𝑡,\n𝑠𝑖,𝑡+ 𝑏𝑖∈Topk({𝑠𝑗,𝑡+ 𝑏𝑗|1 ⩽𝑗⩽𝑁𝑟}, 𝐾𝑟),\n0,\notherwise.\n(16)\nNote that the bias term is only used for routing. The gating value, which will be multiplied with\nthe FFN output, is still derived from the original affinity score 𝑠𝑖,𝑡. During training, we keep\nmonitoring the expert load on the whole batch of each training step. At the end of each step,\nwe will decrease the bias term by 𝛾if its corresponding expert is overloaded, and increase it by\n𝛾if its corresponding expert is underloaded, where 𝛾is a hyper-parameter called bias update\nspeed. Through the dynamic adjustment, DeepSeek-V3 keeps balanced expert load during\ntraining, and achieves better performance than models that encourage load balance through\npure auxiliary losses.\nComplementary Sequence-Wise Auxiliary Loss.\nAlthough DeepSeek-V3 mainly relies on the\nauxiliary-loss-free strategy for load balance, to prevent extreme imbalance within any single\nsequence, we also employ a complementary sequence-wise balance loss:\nLBal = 𝛼\n𝑁𝑟\n∑︁\n𝑖=1\n𝑓𝑖𝑃𝑖,\n(17)\n𝑓𝑖= 𝑁𝑟\n𝐾𝑟𝑇\n𝑇\n∑︁\n𝑡=1\n1 \u0000𝑠𝑖,𝑡∈Topk({𝑠𝑗,𝑡|1 ⩽𝑗⩽𝑁𝑟}, 𝐾𝑟)\u0001 ,\n(18)\n𝑠′\n𝑖,𝑡=\n𝑠𝑖,𝑡\nÍ𝑁𝑟\n𝑗=1 𝑠𝑗,𝑡\n,\n(19)\n𝑃𝑖= 1\n𝑇\n𝑇\n∑︁\n𝑡=1\n𝑠′\n𝑖,𝑡,\n(20)\nwhere the balance factor 𝛼is a hyper-parameter, which will be assigned an extremely small\nvalue for DeepSeek-V3; 1(·) denotes the indicator function; and 𝑇denotes the number of tokens\nin a sequence. The sequence-wise balance loss encourages the expert load on each sequence to\nbe balanced.\n9\n\nMain Model\n(Next Token Prediction)\nEmbedding Layer\nOutput Head\nOutput Head\nTransformer Block \nEmbedding Layer\n𝑡𝑡2\n𝑡𝑡3\n𝑡𝑡4\n𝑡𝑡1\n𝑡𝑡3\n𝑡𝑡4\n𝑡𝑡5\n𝑡𝑡2\nRMSNorm\nRMSNorm\nLinear Projection\nMTP Module 1\n(Next2 Token Prediction)\nShared\nShared\nconcatenation\nOutput Head\nTransformer Block \nEmbedding Layer\nLinear Projection\nMTP Module 2\n(Next3 Token Prediction)\nconcatenation\nShared\nShared\n𝑡𝑡3\n𝑡𝑡4\n𝑡𝑡5\n𝑡𝑡2\n𝑡𝑡4\n𝑡𝑡5\n𝑡𝑡6\n𝑡𝑡3\n𝑡𝑡5\n𝑡𝑡6\n𝑡𝑡7\n𝑡𝑡4\n𝑡𝑡4\n𝑡𝑡5\n𝑡𝑡6\n𝑡𝑡3\nTransformer Block × 𝐿𝐿 \nTransformer Block × 𝐿𝐿 \nTransformer Block × 𝐿𝐿 \nTransformer Block × 𝐿𝐿 \nTransformer Block × 𝐿𝐿 \n···\nCross-Entropy Loss\nCross-Entropy Loss\nCross-Entropy Loss\nInput Tokens\nTarget Tokens\nRMSNorm\nRMSNorm\nℒMTP\n1\nℒMTP\n2\nℒ𝑀𝑀𝑀𝑀𝑀𝑀𝑀𝑀\nFigure 3 | Illustration of our Multi-Token Prediction (MTP) implementation. We keep the\ncomplete causal chain for the prediction of each token at each depth.\nNode-Limited Routing.\nLike the device-limited routing used by DeepSeek-V2, DeepSeek-V3\nalso uses a restricted routing mechanism to limit communication costs during training. In short,\nwe ensure that each token will be sent to at most 𝑀nodes, which are selected according to\nthe sum of the highest 𝐾𝑟\n𝑀affinity scores of the experts distributed on each node. Under this\nconstraint, our MoE training framework can nearly achieve full computation-communication\noverlap.\nNo Token-Dropping.\nDue to the effective load balancing strategy, DeepSeek-V3 keeps a good\nload balance during its full training. Therefore, DeepSeek-V3 does not drop any tokens during\ntraining. In addition, we also implement specific deployment strategies to ensure inference load\nbalance, so DeepSeek-V3 also does not drop tokens during inference.\n2.2. Multi-Token Prediction\nInspired by Gloeckle et al. (2024), we investigate and set a Multi-Token Prediction (MTP)\nobjective for DeepSeek-V3, which extends the prediction scope to multiple future tokens at each\nposition. On the one hand, an MTP objective densifies the training signals and may improve\ndata efficiency. On the other hand, MTP may enable the model to pre-plan its representations\nfor better prediction of future tokens. Figure 3 illustrates our implementation of MTP. Different\nfrom Gloeckle et al. (2024), which parallelly predicts 𝐷additional tokens using independent\noutput heads, we sequentially predict additional tokens and keep the complete causal chain at\neach prediction depth. We introduce the details of our MTP implementation in this section.\nMTP Modules.\nTo be specific, our MTP implementation uses 𝐷sequential modules to predict 𝐷\nadditional tokens. The 𝑘-th MTP module consists of a shared embedding layer Emb(·), a shared\noutput head OutHead(·), a Transformer block TRM𝑘(·), and a projection matrix 𝑀𝑘∈R𝑑×2𝑑. For\nthe 𝑖-th input token 𝑡𝑖, at the 𝑘-th prediction depth, we first combine the representation of the 𝑖-th\ntoken at the (𝑘−1)-th depth h𝑘−1\n𝑖\n∈R𝑑and the embedding of the (𝑖+ 𝑘)-th token 𝐸𝑚𝑏(𝑡𝑖+𝑘) ∈R𝑑\n10\n\nwith the linear projection:\nh′𝑘\n𝑖= 𝑀𝑘[RMSNorm(h𝑘−1\n𝑖\n); RMSNorm(Emb(𝑡𝑖+𝑘))],\n(21)\nwhere [·; ·] denotes concatenation. Especially, when 𝑘= 1, h𝑘−1\n𝑖\nrefers to the representation given\nby the main model. Note that for each MTP module, its embedding layer is shared with the\nmain model. The combined h′𝑘\n𝑖serves as the input of the Transformer block at the 𝑘-th depth to\nproduce the output representation at the current depth h𝑘\n𝑖:\nh𝑘\n1:𝑇−𝑘= TRM𝑘(h′𝑘\n1:𝑇−𝑘),\n(22)\nwhere 𝑇represents the input sequence length and 𝑖:𝑗denotes the slicing operation (inclusive of\nboth the left and right boundaries). Finally, taking h𝑘\n𝑖as the input, the shared output head will\ncompute the probability distribution for the 𝑘-th additional prediction token 𝑃𝑘\n𝑖+1+𝑘∈R𝑉, where\n𝑉is the vocabulary size:\n𝑃𝑘\n𝑖+𝑘+1 = OutHead(h𝑘\n𝑖).\n(23)\nThe output head OutHead(·) linearly maps the representation to logits and subsequently applies\nthe Softmax(·) function to compute the prediction probabilities of the 𝑘-th additional token.\nAlso, for each MTP module, its output head is shared with the main model. Our principle of\nmaintaining the causal chain of predictions is similar to that of EAGLE (Li et al., 2024b), but its\nprimary objective is speculative decoding (Leviathan et al., 2023; Xia et al., 2023), whereas we\nutilize MTP to improve training.\nMTP Training Objective.\nFor each prediction depth, we compute a cross-entropy loss L𝑘\nMTP:\nL𝑘\nMTP = CrossEntropy(𝑃𝑘\n2+𝑘:𝑇+1, 𝑡2+𝑘:𝑇+1) = −1\n𝑇\n𝑇+1\n∑︁\n𝑖=2+𝑘\nlog 𝑃𝑘\n𝑖[𝑡𝑖],\n(24)\nwhere 𝑇denotes the input sequence length, 𝑡𝑖denotes the ground-truth token at the 𝑖-th position,\nand 𝑃𝑘\n𝑖[𝑡𝑖] denotes the corresponding prediction probability of 𝑡𝑖, given by the 𝑘-th MTP module.\nFinally, we compute the average of the MTP losses across all depths and multiply it by a\nweighting factor 𝜆to obtain the overall MTP loss LMTP, which serves as an additional training\nobjective for DeepSeek-V3:\nLMTP = 𝜆\n𝐷\n𝐷\n∑︁\n𝑘=1\nL𝑘\nMTP.\n(25)\nMTP in Inference.\nOur MTP strategy mainly aims to improve the performance of the main\nmodel, so during inference, we can directly discard the MTP modules and the main model can\nfunction independently and normally. Additionally, we can also repurpose these MTP modules\nfor speculative decoding to further improve the generation latency.\n3. Infrastructures\n3.1. Compute Clusters\nDeepSeek-V3 is trained on a cluster equipped with 2048 NVIDIA H800 GPUs. Each node in\nthe H800 cluster contains 8 GPUs connected by NVLink and NVSwitch within nodes. Across\ndifferent nodes, InfiniBand (IB) interconnects are utilized to facilitate communications.\n11\n\nComputation\nMLP(B)▲\nMLP(W)▲\nMLP(F)△\nATTN(B)▲\nATTN(W)▲\nATTN(F)△\nCommunication\nDISPATCH(F)△\nDISPATCH(B)▲\nCOMBINE(F)△\nPP\nCOMBINE(B)▲\nTime\n➔\n△ Forward chunk\n▲ Backward chunk\nFigure 4 | Overlapping strategy for a pair of individual forward and backward chunks (the\nboundaries of the transformer blocks are not aligned). Orange denotes forward, green denotes\n\"backward for input\", blue denotes \"backward for weights\", purple denotes PP communication,\nand red denotes barriers. Both all-to-all and PP communication can be fully hidden.\n3.2. Training Framework\nThe training of DeepSeek-V3 is supported by the HAI-LLM framework, an efficient and\nlightweight training framework crafted by our engineers from the ground up. On the whole,\nDeepSeek-V3 applies 16-way Pipeline Parallelism (PP) (Qi et al., 2023a), 64-way Expert Paral-\nlelism (EP) (Lepikhin et al., 2021) spanning 8 nodes, and ZeRO-1 Data Parallelism (DP) (Rajb-\nhandari et al., 2020).\nIn order to facilitate efficient training of DeepSeek-V3, we implement meticulous engineering\noptimizations. Firstly, we design the DualPipe algorithm for efficient pipeline parallelism.\nCompared with existing PP methods, DualPipe has fewer pipeline bubbles. More importantly, it\noverlaps the computation and communication phases across forward and backward processes,\nthereby addressing the challenge of heavy communication overhead introduced by cross-node\nexpert parallelism. Secondly, we develop efficient cross-node all-to-all communication kernels\nto fully utilize IB and NVLink bandwidths and conserve Streaming Multiprocessors (SMs)\ndedicated to communication. Finally, we meticulously optimize the memory footprint during\ntraining, thereby enabling us to train DeepSeek-V3 without using costly Tensor Parallelism (TP).\n3.2.1. DualPipe and Computation-Communication Overlap\nFor DeepSeek-V3, the communication overhead introduced by cross-node expert parallelism\nresults in an inefficient computation-to-communication ratio of approximately 1:1. To tackle this\nchallenge, we design an innovative pipeline parallelism algorithm called DualPipe, which not\nonly accelerates model training by effectively overlapping forward and backward computation-\ncommunication phases, but also reduces the pipeline bubbles.\nThe key idea of DualPipe is to overlap the computation and communication within a pair of\nindividual forward and backward chunks. To be specific, we divide each chunk into four compo-\nnents: attention, all-to-all dispatch, MLP, and all-to-all combine. Specially, for\na backward chunk, both attention and MLP are further split into two parts, backward for\ninput and backward for weights, like in ZeroBubble (Qi et al., 2023b). In addition, we\nhave a PP communication component. As illustrated in Figure 4, for a pair of forward and\nbackward chunks, we rearrange these components and manually adjust the ratio of GPU SMs\ndedicated to communication versus computation. In this overlapping strategy, we can ensure\nthat both all-to-all and PP communication can be fully hidden during execution. Given the\nefficient overlapping strategy, the full DualPipe scheduling is illustrated in Figure 5. It employs\na bidirectional pipeline scheduling, which feeds micro-batches from both ends of the pipeline\nsimultaneously and a significant portion of communications can be fully overlapped. This\noverlap also ensures that, as the model further scales up, as long as we maintain a constant\ncomputation-to-communication ratio, we can still employ fine-grained experts across nodes\nwhile achieving a near-zero all-to-all communication overhead.\n12\n\nDevice 0 0\n1\n2\n3\n4\n5\n6\n7\n0\n8\n1\n9\n2\n3\n4\n5\n6\n6\n7\n7\n8\n8\n9\n9\nDevice 1\n0\n1\n2\n3\n4\n5\n6\n0\n7\n1\n8\n2\n9\n3\n4\n5\n6\n7\n8\n7\n9\n8\n9\nDevice 2\n0\n1\n2\n3\n4\n5\n0\n6\n1\n7\n2\n8\n3\n9\n4\n5\n6\n7\n8\n7\n9\n8\n9\nDevice 3\n0\n1\n2\n3\n4\n0\n5\n1\n6\n2\n7\n3\n8\n4\n9\n5\n6\n7\n8\n9\n8\n9\nDevice 4\n0\n1\n2\n3\n0\n4\n1\n5\n2\n6\n3\n7\n4\n8\n5\n9\n6\n7\n8\n9\n8\n9\nDevice 5\n0\n1\n2\n0\n0\n3\n1\n4\n2\n5\n3\n6\n4\n7\n5\n8\n6\n9\n7\n8\n9\n9\nDevice 6\n0\n1\n0\n0\n2\n1\n1\n3\n2\n4\n3\n5\n4\n6\n5\n7\n6\n8\n7\n9\n8\n9\n9\nDevice 7\n0\n0\n0\n1\n1\n1\n2\n2\n2\n3\n3\n4\n4\n5\n5\n6\n6\n7\n7\n8\n8\n9\n9\nTime\n➔\nForward\nBackward\nBackward for input\nBackward for weights\nOverlapped forward & Backward\nFigure 5 | Example DualPipe scheduling for 8 PP ranks and 20 micro-batches in two directions.\nThe micro-batches in the reverse direction are symmetric to those in the forward direction, so\nwe omit their batch ID for illustration simplicity. Two cells enclosed by a shared black border\nhave mutually overlapped computation and communication.\nMethod\nBubble\nParameter\nActivation\n1F1B\n(𝑃𝑃−1)(𝐹+ 𝐵)\n1×\n𝑃𝑃\nZB1P\n(𝑃𝑃−1)(𝐹+ 𝐵−2𝑊)\n1×\n𝑃𝑃\nDualPipe (Ours)\n( 𝑃𝑃\n2 −1)(𝐹&𝐵+ 𝐵−3𝑊)\n2×\n𝑃𝑃+ 1\nTable 2 | Comparison of pipeline bubbles and memory usage across different pipeline parallel\nmethods. 𝐹denotes the execution time of a forward chunk, 𝐵denotes the execution time of a\nfull backward chunk, 𝑊denotes the execution time of a \"backward for weights\" chunk, and 𝐹&𝐵\ndenotes the execution time of two mutually overlapped forward and backward chunks.\nIn addition, even in more general scenarios without a heavy communication burden, Du-\nalPipe still exhibits efficiency advantages. In Table 2, we summarize the pipeline bubbles and\nmemory usage across different PP methods. As shown in the table, compared with ZB1P (Qi\net al., 2023b) and 1F1B (Harlap et al., 2018), DualPipe significantly reduces the pipeline bubbles\nwhile only increasing the peak activation memory by\n1\n𝑃𝑃times. Although DualPipe requires\nkeeping two copies of the model parameters, this does not significantly increase the memory\nconsumption since we use a large EP size during training. Compared with Chimera (Li and\nHoefler, 2021), DualPipe only requires that the pipeline stages and micro-batches be divisible by\n2, without requiring micro-batches to be divisible by pipeline stages. In addition, for DualPipe,\nneither the bubbles nor activation memory will increase as the number of micro-batches grows.\n3.2.2. Efficient Implementation of Cross-Node All-to-All Communication\nIn order to ensure sufficient computational performance for DualPipe, we customize efficient\ncross-node all-to-all communication kernels (including dispatching and combining) to conserve\nthe number of SMs dedicated to communication. The implementation of the kernels is co-\ndesigned with the MoE gating algorithm and the network topology of our cluster. To be specific,\nin our cluster, cross-node GPUs are fully interconnected with IB, and intra-node communications\nare handled via NVLink. NVLink offers a bandwidth of 160 GB/s, roughly 3.2 times that of IB\n(50 GB/s). To effectively leverage the different bandwidths of IB and NVLink, we limit each\ntoken to be dispatched to at most 4 nodes, thereby reducing IB traffic. For each token, when its\nrouting decision is made, it will first be transmitted via IB to the GPUs with the same in-node\nindex on its target nodes. Once it reaches the target nodes, we will endeavor to ensure that it is\ninstantaneously forwarded via NVLink to specific GPUs that host their target experts, without\nbeing blocked by subsequently arriving tokens. In this way, communications via IB and NVLink\nare fully overlapped, and each token can efficiently select an average of 3.2 experts per node\nwithout incurring additional overhead from NVLink. This implies that, although DeepSeek-V3\n13\n\nselects only 8 routed experts in practice, it can scale up this number to a maximum of 13 experts\n(4 nodes × 3.2 experts/node) while preserving the same communication cost. Overall, under\nsuch a communication strategy, only 20 SMs are sufficient to fully utilize the bandwidths of IB\nand NVLink.\nIn detail, we employ the warp specialization technique (Bauer et al., 2014) and partition\n20 SMs into 10 communication channels. During the dispatching process, (1) IB sending, (2)\nIB-to-NVLink forwarding, and (3) NVLink receiving are handled by respective warps. The\nnumber of warps allocated to each communication task is dynamically adjusted according to the\nactual workload across all SMs. Similarly, during the combining process, (1) NVLink sending,\n(2) NVLink-to-IB forwarding and accumulation, and (3) IB receiving and accumulation are also\nhandled by dynamically adjusted warps. In addition, both dispatching and combining kernels\noverlap with the computation stream, so we also consider their impact on other SM computation\nkernels. Specifically, we employ customized PTX (Parallel Thread Execution) instructions and\nauto-tune the communication chunk size, which significantly reduces the use of the L2 cache\nand the interference to other SMs.\n3.2.3. Extremely Memory Saving with Minimal Overhead\nIn order to reduce the memory footprint during training, we employ the following techniques.\nRecomputation of RMSNorm and MLA Up-Projection.\nWe recompute all RMSNorm op-\nerations and MLA up-projections during back-propagation, thereby eliminating the need to\npersistently store their output activations. With a minor overhead, this strategy significantly\nreduces memory requirements for storing activations.\nExponential Moving Average in CPU.\nDuring training, we preserve the Exponential Mov-\ning Average (EMA) of the model parameters for early estimation of the model performance\nafter learning rate decay. The EMA parameters are stored in CPU memory and are updated\nasynchronously after each training step. This method allows us to maintain EMA parameters\nwithout incurring additional memory or time overhead.\nShared Embedding and Output Head for Multi-Token Prediction.\nWith the DualPipe strategy,\nwe deploy the shallowest layers (including the embedding layer) and deepest layers (including\nthe output head) of the model on the same PP rank. This arrangement enables the physical\nsharing of parameters and gradients, of the shared embedding and output head, between the\nMTP module and the main model. This physical sharing mechanism further enhances our\nmemory efficiency.\n3.3. FP8 Training\nInspired by recent advances in low-precision training (Dettmers et al., 2022; Noune et al., 2022;\nPeng et al., 2023b), we propose a fine-grained mixed precision framework utilizing the FP8\ndata format for training DeepSeek-V3. While low-precision training holds great promise, it\nis often limited by the presence of outliers in activations, weights, and gradients (Fishman\net al., 2024; He et al.; Sun et al., 2024). Although significant progress has been made in in-\nference quantization (Frantar et al., 2022; Xiao et al., 2023), there are relatively few studies\ndemonstrating successful application of low-precision techniques in large-scale language model\n14\n\nΣ\nFprop\nFP32\nInput\nTo FP8\nBF16\nWeight\nΣ\nDgrad\nFP32\nInput \nGradient\nOutput\nOutput \nGradient\nBF16\nTo FP8\nΣ\nWgrad\nFP32\nTo FP8\nTo FP8\nWeight \nGradient\nOptimizer \nStates\n  To\nBF16\nMaster \nWeight\nTo FP8\nTo BF16\nTo BF16\nTo FP32\nFP32\nFigure 6 | The overall mixed precision framework with FP8 data format. For clarification, only\nthe Linear operator is illustrated.\npre-training (Fishman et al., 2024). To address this challenge and effectively extend the dynamic\nrange of the FP8 format, we introduce a fine-grained quantization strategy: tile-wise grouping\nwith 1 × 𝑁𝑐elements or block-wise grouping with 𝑁𝑐× 𝑁𝑐elements. The associated dequantiza-\ntion overhead is largely mitigated under our increased-precision accumulation process, a critical\naspect for achieving accurate FP8 General Matrix Multiplication (GEMM). Moreover, to further\nreduce memory and communication overhead in MoE training, we cache and dispatch activa-\ntions in FP8, while storing low-precision optimizer states in BF16. We validate the proposed FP8\nmixed precision framework on two model scales similar to DeepSeek-V2-Lite and DeepSeek-\nV2, training for approximately 1 trillion tokens (see more details in Appendix B.1). Notably,\ncompared with the BF16 baseline, the relative loss error of our FP8-training model remains\nconsistently below 0.25%, a level well within the acceptable range of training randomness.\n3.3.1. Mixed Precision Framework\nBuilding upon widely adopted techniques in low-precision training (Kalamkar et al., 2019;\nNarang et al., 2017), we propose a mixed precision framework for FP8 training. In this frame-\nwork, most compute-density operations are conducted in FP8, while a few key operations\nare strategically maintained in their original data formats to balance training efficiency and\nnumerical stability. The overall framework is illustrated in Figure 6.\nFirstly, in order to accelerate model training, the majority of core computation kernels, i.e.,\nGEMM operations, are implemented in FP8 precision. These GEMM operations accept FP8\ntensors as inputs and produce outputs in BF16 or FP32. As depicted in Figure 6, all three GEMMs\nassociated with the Linear operator, namely Fprop (forward pass), Dgrad (activation backward\npass), and Wgrad (weight backward pass), are executed in FP8. This design theoretically doubles\nthe computational speed compared with the original BF16 method. Additionally, the FP8 Wgrad\nGEMM allows activations to be stored in FP8 for use in the backward pass. This significantly\nreduces memory consumption.\nDespite the efficiency advantage of the FP8 format, certain operators still require a higher\nprecision due to their sensitivity to low-precision computations. Besides, some low-cost opera-\ntors can also utilize a higher precision with a negligible overhead to the overall training cost. For\nthis reason, after careful investigations, we maintain the original precision (e.g., BF16 or FP32)\nfor the following components: the embedding module, the output head, MoE gating modules,\nnormalization operators, and attention operators. These targeted retentions of high precision\nensure stable training dynamics for DeepSeek-V3. To further guarantee numerical stability, we\nstore the master weights, weight gradients, and optimizer states in higher precision. While\n15\n\nScaling Factor\n…\n…\n…\n…\nTensor Core\nCUDA Core\nInput\nScaling \nFactor\nWeight\nScaling \nFactor\nOutput\nTensor Core\nWGMMA 1\nWGMMA 4\nLow Prec Acc\nCUDA Core\nFP32 Register\nInterval\nOutput\n/\nGEMM Input\n(b) Increasing accumulation precision\n(a) Fine-grained quantization\nFigure 7 | (a) We propose a fine-grained quantization method to mitigate quantization errors\ncaused by feature outliers; for illustration simplicity, only Fprop is illustrated. (b) In conjunction\nwith our quantization strategy, we improve the FP8 GEMM precision by promoting to CUDA\nCores at an interval of 𝑁𝐶= 128 elements MMA for the high-precision accumulation.\nthese high-precision components incur some memory overheads, their impact can be minimized\nthrough efficient sharding across multiple DP ranks in our distributed training system.\n3.3.2. Improved Precision from Quantization and Multiplication\nBased on our mixed precision FP8 framework, we introduce several strategies to enhance low-\nprecision training accuracy, focusing on both the quantization method and the multiplication\nprocess.\nFine-Grained Quantization.\nIn low-precision training frameworks, overflows and underflows\nare common challenges due to the limited dynamic range of the FP8 format, which is constrained\nby its reduced exponent bits. As a standard practice, the input distribution is aligned to the\nrepresentable range of the FP8 format by scaling the maximum absolute value of the input\ntensor to the maximum representable value of FP8 (Narang et al., 2017). This method makes low-\nprecision training highly sensitive to activation outliers, which can heavily degrade quantization\naccuracy. To solve this, we propose a fine-grained quantization method that applies scaling\nat a more granular level. As illustrated in Figure 7 (a), (1) for activations, we group and\nscale elements on a 1x128 tile basis (i.e., per token per 128 channels); and (2) for weights, we\ngroup and scale elements on a 128x128 block basis (i.e., per 128 input channels per 128 output\nchannels). This approach ensures that the quantization process can better accommodate outliers\nby adapting the scale according to smaller groups of elements. In Appendix B.2, we further\ndiscuss the training instability when we group and scale activations on a block basis in the same\nway as weights quantization.\nOne key modification in our method is the introduction of per-group scaling factors along\nthe inner dimension of GEMM operations. This functionality is not directly supported in the\nstandard FP8 GEMM. However, combined with our precise FP32 accumulation strategy, it can\n16\n\nbe efficiently implemented.\nNotably, our fine-grained quantization strategy is highly consistent with the idea of mi-\ncroscaling formats (Rouhani et al., 2023b), while the Tensor Cores of NVIDIA next-generation\nGPUs (Blackwell series) have announced the support for microscaling formats with smaller\nquantization granularity (NVIDIA, 2024a). We hope our design can serve as a reference for\nfuture work to keep pace with the latest GPU architectures.\nIncreasing Accumulation Precision.\nLow-precision GEMM operations often suffer from un-\nderflow issues, and their accuracy largely depends on high-precision accumulation, which is\ncommonly performed in an FP32 precision (Kalamkar et al., 2019; Narang et al., 2017). However,\nwe observe that the accumulation precision of FP8 GEMM on NVIDIA H800 GPUs is limited to\nretaining around 14 bits, which is significantly lower than FP32 accumulation precision. This\nproblem will become more pronounced when the inner dimension K is large (Wortsman et al.,\n2023), a typical scenario in large-scale model training where the batch size and model width\nare increased. Taking GEMM operations of two random matrices with K = 4096 for example, in\nour preliminary test, the limited accumulation precision in Tensor Cores results in a maximum\nrelative error of nearly 2%. Despite these problems, the limited accumulation precision is still\nthe default option in a few FP8 frameworks (NVIDIA, 2024b), severely constraining the training\naccuracy.\nIn order to address this issue, we adopt the strategy of promotion to CUDA Cores for\nhigher precision (Thakkar et al., 2023). The process is illustrated in Figure 7 (b). To be specific,\nduring MMA (Matrix Multiply-Accumulate) execution on Tensor Cores, intermediate results\nare accumulated using the limited bit width. Once an interval of 𝑁𝐶is reached, these partial\nresults will be copied to FP32 registers on CUDA Cores, where full-precision FP32 accumulation\nis performed. As mentioned before, our fine-grained quantization applies per-group scaling\nfactors along the inner dimension K. These scaling factors can be efficiently multiplied on the\nCUDA Cores as the dequantization process with minimal additional computational cost.\nIt is worth noting that this modification reduces the WGMMA (Warpgroup-level Matrix\nMultiply-Accumulate) instruction issue rate for a single warpgroup. However, on the H800\narchitecture, it is typical for two WGMMA to persist concurrently: while one warpgroup\nperforms the promotion operation, the other is able to execute the MMA operation. This design\nenables overlapping of the two operations, maintaining high utilization of Tensor Cores. Based\non our experiments, setting 𝑁𝐶= 128 elements, equivalent to 4 WGMMAs, represents the\nminimal accumulation interval that can significantly improve precision without introducing\nsubstantial overhead.\nMantissa over Exponents.\nIn contrast to the hybrid FP8 format adopted by prior work\n(NVIDIA, 2024b; Peng et al., 2023b; Sun et al., 2019b), which uses E4M3 (4-bit exponent and\n3-bit mantissa) in Fprop and E5M2 (5-bit exponent and 2-bit mantissa) in Dgrad and Wgrad,\nwe adopt the E4M3 format on all tensors for higher precision. We attribute the feasibility of\nthis approach to our fine-grained quantization strategy, i.e., tile and block-wise scaling. By\noperating on smaller element groups, our methodology effectively shares exponent bits among\nthese grouped elements, mitigating the impact of the limited dynamic range.\nOnline Quantization.\nDelayed quantization is employed in tensor-wise quantization frame-\nworks (NVIDIA, 2024b; Peng et al., 2023b), which maintains a history of the maximum absolute\n17\n\nvalues across prior iterations to infer the current value. In order to ensure accurate scales and\nsimplify the framework, we calculate the maximum absolute value online for each 1x128 acti-\nvation tile or 128x128 weight block. Based on it, we derive the scaling factor and then quantize\nthe activation or weight online into the FP8 format.\n3.3.3. Low-Precision Storage and Communication\nIn conjunction with our FP8 training framework, we further reduce the memory consumption\nand communication overhead by compressing cached activations and optimizer states into\nlower-precision formats.\nLow-Precision Optimizer States.\nWe adopt the BF16 data format instead of FP32 to track the\nfirst and second moments in the AdamW (Loshchilov and Hutter, 2017) optimizer, without\nincurring observable performance degradation. However, the master weights (stored by the\noptimizer) and gradients (used for batch size accumulation) are still retained in FP32 to ensure\nnumerical stability throughout training.\nLow-Precision Activation.\nAs illustrated in Figure 6, the Wgrad operation is performed in FP8.\nTo reduce the memory consumption, it is a natural choice to cache activations in FP8 format\nfor the backward pass of the Linear operator. However, special considerations are taken on\nseveral operators for low-cost high-precision training:\n(1) Inputs of the Linear after the attention operator. These activations are also\nused in the backward pass of the attention operator, which makes it sensitive to\nprecision. We adopt a customized E5M6 data format exclusively for these activations.\nAdditionally, these activations will be converted from an 1x128 quantization tile to\nan 128x1 tile in the backward pass. To avoid introducing extra quantization error,\nall the scaling factors are round scaled, i.e., integral power of 2.\n(2) Inputs of the SwiGLU operator in MoE. To further reduce the memory cost, we\ncache the inputs of the SwiGLU operator and recompute its output in the backward\npass. These activations are also stored in FP8 with our fine-grained quantization\nmethod, striking a balance between memory efficiency and computational accuracy.\nLow-Precision Communication.\nCommunication bandwidth is a critical bottleneck in the\ntraining of MoE models. To alleviate this challenge, we quantize the activation before MoE\nup-projections into FP8 and then apply dispatch components, which is compatible with\nFP8 Fprop in MoE up-projections. Like the inputs of the Linear after the attention operator,\nscaling factors for this activation are integral power of 2. A similar strategy is applied to the\nactivation gradient before MoE down-projections. For both the forward and backward combine\ncomponents, we retain them in BF16 to preserve training precision in critical parts of the training\npipeline.\n3.4. Inference and Deployment\nWe deploy DeepSeek-V3 on the H800 cluster, where GPUs within each node are interconnected\nusing NVLink, and all GPUs across the cluster are fully interconnected via IB. To simultaneously\nensure both the Service-Level Objective (SLO) for online services and high throughput, we\nemploy the following deployment strategy that separates the prefilling and decoding stages.\n18\n\n3.4.1. Prefilling\nThe minimum deployment unit of the prefilling stage consists of 4 nodes with 32 GPUs. The\nattention part employs 4-way Tensor Parallelism (TP4) with Sequence Parallelism (SP), com-\nbined with 8-way Data Parallelism (DP8). Its small TP size of 4 limits the overhead of TP\ncommunication. For the MoE part, we use 32-way Expert Parallelism (EP32), which ensures that\neach expert processes a sufficiently large batch size, thereby enhancing computational efficiency.\nFor the MoE all-to-all communication, we use the same method as in training: first transferring\ntokens across nodes via IB, and then forwarding among the intra-node GPUs via NVLink. In\nparticular, we use 1-way Tensor Parallelism for the dense MLPs in shallow layers to save TP\ncommunication.\nTo achieve load balancing among different experts in the MoE part, we need to ensure that\neach GPU processes approximately the same number of tokens. To this end, we introduce a\ndeployment strategy of redundant experts, which duplicates high-load experts and deploys them\nredundantly. The high-load experts are detected based on statistics collected during the online\ndeployment and are adjusted periodically (e.g., every 10 minutes). After determining the set\nof redundant experts, we carefully rearrange experts among GPUs within a node based on the\nobserved loads, striving to balance the load across GPUs as much as possible without increasing\nthe cross-node all-to-all communication overhead. For the deployment of DeepSeek-V3, we set\n32 redundant experts for the prefilling stage. For each GPU, besides the original 8 experts it\nhosts, it will also host one additional redundant expert.\nFurthermore, in the prefilling stage, to improve the throughput and hide the overhead of\nall-to-all and TP communication, we simultaneously process two micro-batches with similar\ncomputational workloads, overlapping the attention and MoE of one micro-batch with the\ndispatch and combine of another.\nFinally, we are exploring a dynamic redundancy strategy for experts, where each GPU hosts\nmore experts (e.g., 16 experts), but only 9 will be activated during each inference step. Before\nthe all-to-all operation at each layer begins, we compute the globally optimal routing scheme\non the fly. Given the substantial computation involved in the prefilling stage, the overhead of\ncomputing this routing scheme is almost negligible.\n3.4.2. Decoding\nDuring decoding, we treat the shared expert as a routed one. From this perspective, each token\nwill select 9 experts during routing, where the shared expert is regarded as a heavy-load one\nthat will always be selected. The minimum deployment unit of the decoding stage consists\nof 40 nodes with 320 GPUs. The attention part employs TP4 with SP, combined with DP80,\nwhile the MoE part uses EP320. For the MoE part, each GPU hosts only one expert, and 64 GPUs\nare responsible for hosting redundant experts and shared experts. All-to-all communication\nof the dispatch and combine parts is performed via direct point-to-point transfers over IB to\nachieve low latency. Additionally, we leverage the IBGDA (NVIDIA, 2022) technology to further\nminimize latency and enhance communication efficiency.\nSimilar to prefilling, we periodically determine the set of redundant experts in a certain\ninterval, based on the statistical expert load from our online service. However, we do not need\nto rearrange experts since each GPU only hosts one expert. We are also exploring the dynamic\nredundancy strategy for decoding. However, this requires more careful optimization of the\nalgorithm that computes the globally optimal routing scheme and the fusion with the dispatch\nkernel to reduce overhead.\n19\n\nAdditionally, to enhance throughput and hide the overhead of all-to-all communication,\nwe are also exploring processing two micro-batches with similar computational workloads\nsimultaneously in the decoding stage. Unlike prefilling, attention consumes a larger portion\nof time in the decoding stage. Therefore, we overlap the attention of one micro-batch with\nthe dispatch+MoE+combine of another. In the decoding stage, the batch size per expert\nis relatively small (usually within 256 tokens), and the bottleneck is memory access rather\nthan computation. Since the MoE part only needs to load the parameters of one expert, the\nmemory access overhead is minimal, so using fewer SMs will not significantly affect the overall\nperformance. Therefore, to avoid impacting the computation speed of the attention part, we\ncan allocate only a small portion of SMs to dispatch+MoE+combine.\n3.5. Suggestions on Hardware Design\nBased on our implementation of the all-to-all communication and FP8 training scheme, we\npropose the following suggestions on chip design to AI hardware vendors.\n3.5.1. Communication Hardware\nIn DeepSeek-V3, we implement the overlap between computation and communication to hide\nthe communication latency during computation. This significantly reduces the dependency\non communication bandwidth compared to serial computation and communication. However,\nthe current communication implementation relies on expensive SMs (e.g., we allocate 20 out of\nthe 132 SMs available in the H800 GPU for this purpose), which will limit the computational\nthroughput. Moreover, using SMs for communication results in significant inefficiencies, as\ntensor cores remain entirely under-utilized.\nCurrently, the SMs primarily perform the following tasks for all-to-all communication:\n• Forwarding data between the IB (InfiniBand) and NVLink domain while aggregating IB\ntraffic destined for multiple GPUs within the same node from a single GPU.\n• Transporting data between RDMA buffers (registered GPU memory regions) and in-\nput/output buffers.\n• Executing reduce operations for all-to-all combine.\n• Managing fine-grained memory layout during chunked data transferring to multiple\nexperts across the IB and NVLink domain.\nWe aspire to see future vendors developing hardware that offloads these communication\ntasks from the valuable computation unit SM, serving as a GPU co-processor or a network\nco-processor like NVIDIA SHARP Graham et al. (2016). Furthermore, to reduce application\nprogramming complexity, we aim for this hardware to unify the IB (scale-out) and NVLink\n(scale-up) networks from the perspective of the computation units. With this unified interface,\ncomputation units can easily accomplish operations such as read, write, multicast, and\nreduce across the entire IB-NVLink-unified domain via submitting communication requests\nbased on simple primitives.\n3.5.2. Compute Hardware\nHigher FP8 GEMM Accumulation Precision in Tensor Cores.\nIn the current Tensor Core\nimplementation of the NVIDIA Hopper architecture, FP8 GEMM suffers from limited accumula-\ntion precision. After aligning 32 mantissa products by right-shifting based on the maximum\nexponent, the Tensor Core only uses the highest 14 bits of each mantissa product for addition,\n20\n\nand truncates bits exceeding this range. The accumulation of addition results into registers also\nemploys 14-bit precision. Our implementation partially mitigates the limitation by accumulating\nthe addition results of 128 FP8×FP8 multiplications into registers with FP32 precision in the\nCUDA core. Although helpful in achieving successful FP8 training, it is merely a compromise\ndue to the Hopper architecture’s hardware deficiency in FP8 GEMM accumulation precision.\nFuture chips need to adopt higher precision.\nSupport for Tile- and Block-Wise Quantization.\nCurrent GPUs only support per-tensor\nquantization, lacking the native support for fine-grained quantization like our tile- and block-\nwise quantization. In the current implementation, when the 𝑁𝐶interval is reached, the partial\nresults will be copied from Tensor Cores to CUDA cores, multiplied by the scaling factors, and\nadded to FP32 registers on CUDA cores. Although the dequantization overhead is significantly\nmitigated combined with our precise FP32 accumulation strategy, the frequent data movements\nbetween Tensor Cores and CUDA cores still limit the computational efficiency. Therefore, we\nrecommend future chips to support fine-grained quantization by enabling Tensor Cores to\nreceive scaling factors and implement MMA with group scaling. In this way, the whole partial\nsum accumulation and dequantization can be completed directly inside Tensor Cores until the\nfinal result is produced, avoiding frequent data movements.\nSupport for Online Quantization.\nThe current implementations struggle to effectively support\nonline quantization, despite its effectiveness demonstrated in our research. In the existing\nprocess, we need to read 128 BF16 activation values (the output of the previous computation)\nfrom HBM (High Bandwidth Memory) for quantization, and the quantized FP8 values are\nthen written back to HBM, only to be read again for MMA. To address this inefficiency, we\nrecommend that future chips integrate FP8 cast and TMA (Tensor Memory Accelerator) access\ninto a single fused operation, so quantization can be completed during the transfer of activations\nfrom global memory to shared memory, avoiding frequent memory reads and writes. We also\nrecommend supporting a warp-level cast instruction for speedup, which further facilitates the\nbetter fusion of layer normalization and FP8 cast. Alternatively, a near-memory computing\napproach can be adopted, where compute logic is placed near the HBM. In this case, BF16\nelements can be cast to FP8 directly as they are read from HBM into the GPU, reducing off-chip\nmemory access by roughly 50%.\nSupport for Transposed GEMM Operations.\nThe current architecture makes it cumbersome\nto fuse matrix transposition with GEMM operations. In our workflow, activations during the\nforward pass are quantized into 1x128 FP8 tiles and stored. During the backward pass, the\nmatrix needs to be read out, dequantized, transposed, re-quantized into 128x1 tiles, and stored\nin HBM. To reduce memory operations, we recommend future chips to enable direct transposed\nreads of matrices from shared memory before MMA operation, for those precisions required\nin both training and inference. Combined with the fusion of FP8 format conversion and TMA\naccess, this enhancement will significantly streamline the quantization workflow.\n4. Pre-Training\n4.1. Data Construction\nCompared with DeepSeek-V2, we optimize the pre-training corpus by enhancing the ratio\nof mathematical and programming samples, while expanding multilingual coverage beyond\n21\n\nEnglish and Chinese. Also, our data processing pipeline is refined to minimize redundancy\nwhile maintaining corpus diversity. Inspired by Ding et al. (2024), we implement the document\npacking method for data integrity but do not incorporate cross-sample attention masking during\ntraining. Finally, the training corpus for DeepSeek-V3 consists of 14.8T high-quality and diverse\ntokens in our tokenizer.\nIn the training process of DeepSeekCoder-V2 (DeepSeek-AI, 2024a), we observe that the\nFill-in-Middle (FIM) strategy does not compromise the next-token prediction capability while\nenabling the model to accurately predict middle text based on contextual cues. In alignment with\nDeepSeekCoder-V2, we also incorporate the FIM strategy in the pre-training of DeepSeek-V3. To\nbe specific, we employ the Prefix-Suffix-Middle (PSM) framework to structure data as follows:\n<|fim_begin|> 𝑓pre<|fim_hole|> 𝑓suf<|fim_end|> 𝑓middle<|eos_token|>.\nThis structure is applied at the document level as a part of the pre-packing process. The FIM\nstrategy is applied at a rate of 0.1, consistent with the PSM framework.\nThe tokenizer for DeepSeek-V3 employs Byte-level BPE (Shibata et al., 1999) with an extended\nvocabulary of 128K tokens. The pretokenizer and training data for our tokenizer are modified\nto optimize multilingual compression efficiency. In addition, compared with DeepSeek-V2,\nthe new pretokenizer introduces tokens that combine punctuations and line breaks. However,\nthis trick may introduce the token boundary bias (Lundberg, 2023) when the model processes\nmulti-line prompts without terminal line breaks, particularly for few-shot evaluation prompts.\nTo address this issue, we randomly split a certain proportion of such combined tokens during\ntraining, which exposes the model to a wider array of special cases and mitigates this bias.\n4.2. Hyper-Parameters\nModel Hyper-Parameters.\nWe set the number of Transformer layers to 61 and the hidden\ndimension to 7168. All learnable parameters are randomly initialized with a standard deviation\nof 0.006. In MLA, we set the number of attention heads 𝑛ℎto 128 and the per-head dimension 𝑑ℎ\nto 128. The KV compression dimension 𝑑𝑐is set to 512, and the query compression dimension 𝑑′\n𝑐\nis set to 1536. For the decoupled queries and key, we set the per-head dimension 𝑑𝑅\nℎto 64. We\nsubstitute all FFNs except for the first three layers with MoE layers. Each MoE layer consists of 1\nshared expert and 256 routed experts, where the intermediate hidden dimension of each expert\nis 2048. Among the routed experts, 8 experts will be activated for each token, and each token\nwill be ensured to be sent to at most 4 nodes. The multi-token prediction depth 𝐷is set to 1, i.e.,\nbesides the exact next token, each token will predict one additional token. As DeepSeek-V2,\nDeepSeek-V3 also employs additional RMSNorm layers after the compressed latent vectors,\nand multiplies additional scaling factors at the width bottlenecks. Under this configuration,\nDeepSeek-V3 comprises 671B total parameters, of which 37B are activated for each token.\nTraining Hyper-Parameters.\nWe employ the AdamW optimizer (Loshchilov and Hutter, 2017)\nwith hyper-parameters set to 𝛽1 = 0.9, 𝛽2 = 0.95, and weight_decay = 0.1. We set the maximum\nsequence length to 4K during pre-training, and pre-train DeepSeek-V3 on 14.8T tokens. As for\nthe learning rate scheduling, we first linearly increase it from 0 to 2.2 × 10−4 during the first\n2K steps. Then, we keep a constant learning rate of 2.2 × 10−4 until the model consumes 10T\ntraining tokens. Subsequently, we gradually decay the learning rate to 2.2 × 10−5 in 4.3T tokens,\nfollowing a cosine decay curve. During the training of the final 500B tokens, we keep a constant\nlearning rate of 2.2 × 10−5 in the first 333B tokens, and switch to another constant learning rate\n22\n\nof 7.3 × 10−6 in the remaining 167B tokens. The gradient clipping norm is set to 1.0. We employ\na batch size scheduling strategy, where the batch size is gradually increased from 3072 to 15360\nin the training of the first 469B tokens, and then keeps 15360 in the remaining training. We\nleverage pipeline parallelism to deploy different layers of a model on different GPUs, and for\neach layer, the routed experts will be uniformly deployed on 64 GPUs belonging to 8 nodes.\nAs for the node-limited routing, each token will be sent to at most 4 nodes (i.e., 𝑀= 4). For\nauxiliary-loss-free load balancing, we set the bias update speed 𝛾to 0.001 for the first 14.3T\ntokens, and to 0.0 for the remaining 500B tokens. For the balance loss, we set 𝛼to 0.0001, just to\navoid extreme imbalance within any single sequence. The MTP loss weight 𝜆is set to 0.3 for the\nfirst 10T tokens, and to 0.1 for the remaining 4.8T tokens.\n2K\n11K\n20K\n29K\n38K\n47K\n56K\n65K\n74K\n83K\n92K 101K 110K 119K 128K\nContext Length (#Tokens)\n0\n7\n14\n21\n29\n36\n43\n50\n57\n64\n71\n79\n86\n93\n100\nDocument Depth Percent (%)\nPressure Testing DeepSeek-V3 128K Context via \"Needle In A HayStack\"\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\nScore\nFigure 8 | Evaluation results on the ”Needle In A Haystack” (NIAH) tests. DeepSeek-V3\nperforms well across all context window lengths up to 128K.\n4.3. Long Context Extension\nWe adopt a similar approach to DeepSeek-V2 (DeepSeek-AI, 2024c) to enable long context\ncapabilities in DeepSeek-V3. After the pre-training stage, we apply YaRN (Peng et al., 2023a)\nfor context extension and perform two additional training phases, each comprising 1000 steps,\nto progressively expand the context window from 4K to 32K and then to 128K. The YaRN\nconfiguration is consistent with that used in DeepSeek-V2, being applied exclusively to the\ndecoupled shared key k𝑅\n𝑡. The hyper-parameters remain identical across both phases, with the\nscale 𝑠= 40, 𝛼= 1, 𝛽= 32, and the scaling factor √\n𝑡= 0.1 ln 𝑠+ 1. In the first phase, the sequence\nlength is set to 32K, and the batch size is 1920. During the second phase, the sequence length is\nincreased to 128K, and the batch size is reduced to 480. The learning rate for both phases is set\nto 7.3 × 10−6, matching the final learning rate from the pre-training stage.\nThrough this two-phase extension training, DeepSeek-V3 is capable of handling inputs up to\n128K in length while maintaining strong performance. Figure 8 illustrates that DeepSeek-V3,\nfollowing supervised fine-tuning, achieves notable performance on the \"Needle In A Haystack\"\n(NIAH) test, demonstrating consistent robustness across context window lengths up to 128K.\n23\n\n4.4. Evaluations\n4.4.1. Evaluation Benchmarks\nThe base model of DeepSeek-V3 is pretrained on a multilingual corpus with English and Chinese\nconstituting the majority, so we evaluate its performance on a series of benchmarks primarily\nin English and Chinese, as well as on a multilingual benchmark. Our evaluation is based\non our internal evaluation framework integrated in our HAI-LLM framework. Considered\nbenchmarks are categorized and listed as follows, where underlined benchmarks are in Chinese\nand double-underlined benchmarks are multilingual ones:\nMulti-subject multiple-choice datasets include MMLU (Hendrycks et al., 2020), MMLU-\nRedux (Gema et al., 2024), MMLU-Pro (Wang et al., 2024b), MMMLU (OpenAI, 2024b), C-Eval\n(Huang et al., 2023), and CMMLU (Li et al., 2023).\nLanguage understanding and reasoning datasets include HellaSwag (Zellers et al., 2019),\nPIQA (Bisk et al., 2020), ARC (Clark et al., 2018), and BigBench Hard (BBH) (Suzgun et al., 2022).\nClosed-book question answering datasets include TriviaQA (Joshi et al., 2017) and Natu-\nralQuestions (Kwiatkowski et al., 2019).\nReading comprehension datasets include RACE Lai et al. (2017), DROP (Dua et al., 2019),\nC3 (Sun et al., 2019a), and CMRC (Cui et al., 2019).\nReference disambiguation datasets include CLUEWSC (Xu et al., 2020) and WinoGrande\nSakaguchi et al. (2019).\nLanguage modeling datasets include Pile (Gao et al., 2020).\nChinese understanding and culture datasets include CCPM (Li et al., 2021).\nMath datasets include GSM8K (Cobbe et al., 2021), MATH (Hendrycks et al., 2021), MGSM\n(Shi et al., 2023), and CMath (Wei et al., 2023).\nCode datasets include HumanEval (Chen et al., 2021), LiveCodeBench-Base (0801-1101) (Jain\net al., 2024), MBPP (Austin et al., 2021), and CRUXEval (Gu et al., 2024).\nStandardized exams include AGIEval (Zhong et al., 2023). Note that AGIEval includes both\nEnglish and Chinese subsets.\nFollowing our previous work (DeepSeek-AI, 2024b,c), we adopt perplexity-based eval-\nuation for datasets including HellaSwag, PIQA, WinoGrande, RACE-Middle, RACE-High,\nMMLU, MMLU-Redux, MMLU-Pro, MMMLU, ARC-Easy, ARC-Challenge, C-Eval, CMMLU,\nC3, and CCPM, and adopt generation-based evaluation for TriviaQA, NaturalQuestions, DROP,\nMATH, GSM8K, MGSM, HumanEval, MBPP, LiveCodeBench-Base, CRUXEval, BBH, AGIEval,\nCLUEWSC, CMRC, and CMath. In addition, we perform language-modeling-based evaluation\nfor Pile-test and use Bits-Per-Byte (BPB) as the metric to guarantee fair comparison among\nmodels using different tokenizers.\n4.4.2. Evaluation Results\nIn Table 3, we compare the base model of DeepSeek-V3 with the state-of-the-art open-source base\nmodels, including DeepSeek-V2-Base (DeepSeek-AI, 2024c) (our previous release), Qwen2.5 72B\nBase (Qwen, 2024b), and LLaMA-3.1 405B Base (AI@Meta, 2024b). We evaluate all these models\nwith our internal evaluation framework, and ensure that they share the same evaluation setting.\nNote that due to the changes in our evaluation framework over the past months, the performance\n24\n\nBenchmark (Metric)\n# Shots\nDeepSeek-V2\nQwen2.5\nLLaMA-3.1\nDeepSeek-V3\nBase\n72B Base\n405B Base\nBase\nArchitecture\n-\nMoE\nDense\nDense\nMoE\n# Activated Params\n-\n21B\n72B\n405B\n37B\n# Total Params\n-\n236B\n72B\n405B\n671B\nEnglish\nPile-test (BPB)\n-\n0.606\n0.638\n0.542\n0.548\nBBH (EM)\n3-shot\n78.8\n79.8\n82.9\n87.5\nMMLU (EM)\n5-shot\n78.4\n85.0\n84.4\n87.1\nMMLU-Redux (EM)\n5-shot\n75.6\n83.2\n81.3\n86.2\nMMLU-Pro (EM)\n5-shot\n51.4\n58.3\n52.8\n64.4\nDROP (F1)\n3-shot\n80.4\n80.6\n86.0\n89.0\nARC-Easy (EM)\n25-shot\n97.6\n98.4\n98.4\n98.9\nARC-Challenge (EM)\n25-shot\n92.2\n94.5\n95.3\n95.3\nHellaSwag (EM)\n10-shot\n87.1\n84.8\n89.2\n88.9\nPIQA (EM)\n0-shot\n83.9\n82.6\n85.9\n84.7\nWinoGrande (EM)\n5-shot\n86.3\n82.3\n85.2\n84.9\nRACE-Middle (EM)\n5-shot\n73.1\n68.1\n74.2\n67.1\nRACE-High (EM)\n5-shot\n52.6\n50.3\n56.8\n51.3\nTriviaQA (EM)\n5-shot\n80.0\n71.9\n82.7\n82.9\nNaturalQuestions (EM)\n5-shot\n38.6\n33.2\n41.5\n40.0\nAGIEval (EM)\n0-shot\n57.5\n75.8\n60.6\n79.6\nCode\nHumanEval (Pass@1)\n0-shot\n43.3\n53.0\n54.9\n65.2\nMBPP (Pass@1)\n3-shot\n65.0\n72.6\n68.4\n75.4\nLiveCodeBench-Base (Pass@1)\n3-shot\n11.6\n12.9\n15.5\n19.4\nCRUXEval-I (EM)\n2-shot\n52.5\n59.1\n58.5\n67.3\nCRUXEval-O (EM)\n2-shot\n49.8\n59.9\n59.9\n69.8\nMath\nGSM8K (EM)\n8-shot\n81.6\n88.3\n83.5\n89.3\nMATH (EM)\n4-shot\n43.4\n54.4\n49.0\n61.6\nMGSM (EM)\n8-shot\n63.6\n76.2\n69.9\n79.8\nCMath (EM)\n3-shot\n78.7\n84.5\n77.3\n90.7\nChinese\nCLUEWSC (EM)\n5-shot\n82.0\n82.5\n83.0\n82.7\nC-Eval (EM)\n5-shot\n81.4\n89.2\n72.5\n90.1\nCMMLU (EM)\n5-shot\n84.0\n89.5\n73.7\n88.8\nCMRC (EM)\n1-shot\n77.4\n75.8\n76.0\n76.3\nC3 (EM)\n0-shot\n77.4\n76.7\n79.7\n78.6\nCCPM (EM)\n0-shot\n93.0\n88.5\n78.6\n92.0\nMultilingual\nMMMLU-non-English (EM)\n5-shot\n64.0\n74.8\n73.8\n79.4\nTable 3 | Comparison among DeepSeek-V3-Base and other representative open-source base\nmodels. All models are evaluated in our internal framework and share the same evaluation\nsetting. Scores with a gap not exceeding 0.3 are considered to be at the same level. DeepSeek-\nV3-Base achieves the best performance on most benchmarks, especially on math and code tasks.\nof DeepSeek-V2-Base exhibits a slight difference from our previously reported results. Overall,\nDeepSeek-V3-Base comprehensively outperforms DeepSeek-V2-Base and Qwen2.5 72B Base,\nand surpasses LLaMA-3.1 405B Base in the majority of benchmarks, essentially becoming the\nstrongest open-source model.\nFrom a more detailed perspective, we compare DeepSeek-V3-Base with the other open-source\nbase models individually. (1) Compared with DeepSeek-V2-Base, due to the improvements in\nour model architecture, the scale-up of the model size and training tokens, and the enhancement\nof data quality, DeepSeek-V3-Base achieves significantly better performance as expected. (2)\nCompared with Qwen2.5 72B Base, the state-of-the-art Chinese open-source model, with only\nhalf of the activated parameters, DeepSeek-V3-Base also demonstrates remarkable advantages,\n25\n\nespecially on English, multilingual, code, and math benchmarks. As for Chinese benchmarks,\nexcept for CMMLU, a Chinese multi-subject multiple-choice task, DeepSeek-V3-Base also shows\nbetter performance than Qwen2.5 72B. (3) Compared with LLaMA-3.1 405B Base, the largest\nopen-source model with 11 times the activated parameters, DeepSeek-V3-Base also exhibits\nmuch better performance on multilingual, code, and math benchmarks. As for English and\nChinese language benchmarks, DeepSeek-V3-Base shows competitive or better performance,\nand is especially good on BBH, MMLU-series, DROP, C-Eval, CMMLU, and CCPM.\nDue to our efficient architectures and comprehensive engineering optimizations, DeepSeek-\nV3 achieves extremely high training efficiency. Under our training framework and infrastruc-\ntures, training DeepSeek-V3 on each trillion tokens requires only 180K H800 GPU hours, which\nis much cheaper than training 72B or 405B dense models.\nBenchmark (Metric)\n# Shots\nSmall MoE\nSmall MoE\nLarge MoE\nLarge MoE\nBaseline\nw/ MTP\nBaseline\nw/ MTP\n# Activated Params (Inference)\n-\n2.4B\n2.4B\n20.9B\n20.9B\n# Total Params (Inference)\n-\n15.7B\n15.7B\n228.7B\n228.7B\n# Training Tokens\n-\n1.33T\n1.33T\n540B\n540B\nPile-test (BPB)\n-\n0.729\n0.729\n0.658\n0.657\nBBH (EM)\n3-shot\n39.0\n41.4\n70.0\n70.7\nMMLU (EM)\n5-shot\n50.0\n53.3\n67.5\n66.6\nDROP (F1)\n1-shot\n39.2\n41.3\n68.5\n70.6\nTriviaQA (EM)\n5-shot\n56.9\n57.7\n67.0\n67.3\nNaturalQuestions (EM)\n5-shot\n22.7\n22.3\n27.2\n28.5\nHumanEval (Pass@1)\n0-shot\n20.7\n26.8\n44.5\n53.7\nMBPP (Pass@1)\n3-shot\n35.8\n36.8\n61.6\n62.2\nGSM8K (EM)\n8-shot\n25.4\n31.4\n72.3\n74.0\nMATH (EM)\n4-shot\n10.7\n12.6\n38.6\n39.8\nTable 4 | Ablation results for the MTP strategy. The MTP strategy consistently enhances the\nmodel performance on most of the evaluation benchmarks.\n4.5. Discussion\n4.5.1. Ablation Studies for Multi-Token Prediction\nIn Table 4, we show the ablation results for the MTP strategy. To be specific, we validate the\nMTP strategy on top of two baseline models across different scales. At the small scale, we train\na baseline MoE model comprising 15.7B total parameters on 1.33T tokens. At the large scale,\nwe train a baseline MoE model comprising 228.7B total parameters on 540B tokens. On top\nof them, keeping the training data and the other architectures the same, we append a 1-depth\nMTP module onto them and train two models with the MTP strategy for comparison. Note that\nduring inference, we directly discard the MTP module, so the inference costs of the compared\nmodels are exactly the same. From the table, we can observe that the MTP strategy consistently\nenhances the model performance on most of the evaluation benchmarks.\n4.5.2. Ablation Studies for the Auxiliary-Loss-Free Balancing Strategy\nIn Table 5, we show the ablation results for the auxiliary-loss-free balancing strategy. We\nvalidate this strategy on top of two baseline models across different scales. At the small scale,\nwe train a baseline MoE model comprising 15.7B total parameters on 1.33T tokens. At the\nlarge scale, we train a baseline MoE model comprising 228.7B total parameters on 578B tokens.\n26\n\nBenchmark (Metric)\n# Shots\nSmall MoE\nSmall MoE\nLarge MoE\nLarge MoE\nAux-Loss-Based\nAux-Loss-Free\nAux-Loss-Based\nAux-Loss-Free\n# Activated Params\n-\n2.4B\n2.4B\n20.9B\n20.9B\n# Total Params\n-\n15.7B\n15.7B\n228.7B\n228.7B\n# Training Tokens\n-\n1.33T\n1.33T\n578B\n578B\nPile-test (BPB)\n-\n0.727\n0.724\n0.656\n0.652\nBBH (EM)\n3-shot\n37.3\n39.3\n66.7\n67.9\nMMLU (EM)\n5-shot\n51.0\n51.8\n68.3\n67.2\nDROP (F1)\n1-shot\n38.1\n39.0\n67.1\n67.1\nTriviaQA (EM)\n5-shot\n58.3\n58.5\n66.7\n67.7\nNaturalQuestions (EM)\n5-shot\n23.2\n23.4\n27.1\n28.1\nHumanEval (Pass@1)\n0-shot\n22.0\n22.6\n40.2\n46.3\nMBPP (Pass@1)\n3-shot\n36.6\n35.8\n59.2\n61.2\nGSM8K (EM)\n8-shot\n27.1\n29.6\n70.7\n74.5\nMATH (EM)\n4-shot\n10.9\n11.1\n37.2\n39.6\nTable 5 | Ablation results for the auxiliary-loss-free balancing strategy. Compared with the\npurely auxiliary-loss-based method, the auxiliary-loss-free strategy consistently achieves better\nmodel performance on most of the evaluation benchmarks.\nBoth of the baseline models purely use auxiliary losses to encourage load balance, and use the\nsigmoid gating function with top-K affinity normalization. Their hyper-parameters to control\nthe strength of auxiliary losses are the same as DeepSeek-V2-Lite and DeepSeek-V2, respectively.\nOn top of these two baseline models, keeping the training data and the other architectures the\nsame, we remove all auxiliary losses and introduce the auxiliary-loss-free balancing strategy for\ncomparison. From the table, we can observe that the auxiliary-loss-free strategy consistently\nachieves better model performance on most of the evaluation benchmarks.\n4.5.3. Batch-Wise Load Balance VS. Sequence-Wise Load Balance\nThe key distinction between auxiliary-loss-free balancing and sequence-wise auxiliary loss lies\nin their balancing scope: batch-wise versus sequence-wise. Compared with the sequence-wise\nauxiliary loss, batch-wise balancing imposes a more flexible constraint, as it does not enforce\nin-domain balance on each sequence. This flexibility allows experts to better specialize in\ndifferent domains. To validate this, we record and analyze the expert load of a 16B auxiliary-\nloss-based baseline and a 16B auxiliary-loss-free model on different domains in the Pile test set.\nAs illustrated in Figure 9, we observe that the auxiliary-loss-free model demonstrates greater\nexpert specialization patterns as expected.\nTo further investigate the correlation between this flexibility and the advantage in model\nperformance, we additionally design and validate a batch-wise auxiliary loss that encourages\nload balance on each training batch instead of on each sequence. The experimental results show\nthat, when achieving a similar level of batch-wise load balance, the batch-wise auxiliary loss\ncan also achieve similar model performance to the auxiliary-loss-free method. To be specific,\nin our experiments with 1B MoE models, the validation losses are: 2.258 (using a sequence-\nwise auxiliary loss), 2.253 (using the auxiliary-loss-free method), and 2.253 (using a batch-wise\nauxiliary loss). We also observe similar results on 3B MoE models: the model using a sequence-\nwise auxiliary loss achieves a validation loss of 2.085, and the models using the auxiliary-loss-free\nmethod or a batch-wise auxiliary loss achieve the same validation loss of 2.080.\nIn addition, although the batch-wise load balancing methods show consistent performance\nadvantages, they also face two potential challenges in efficiency: (1) load imbalance within\n27\n\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64\nWikipedia (en)\nGithub\nDM Mathematics\nAux-Loss-Based Layer 9\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64\nWikipedia (en)\nGithub\nDM Mathematics\nAux-Loss-Free Layer 9\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64\nWikipedia (en)\nGithub\nDM Mathematics\nAux-Loss-Based Layer 18\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64\nWikipedia (en)\nGithub\nDM Mathematics\nAux-Loss-Free Layer 18\n0\n2\n4\n6\n8\n10\nRelative Expert Load\nFigure 9 | Expert load of auxiliary-loss-free and auxiliary-loss-based models on three domains in\nthe Pile test set. The auxiliary-loss-free model shows greater expert specialization patterns than\nthe auxiliary-loss-based one. The relative expert load denotes the ratio between the actual expert\nload and the theoretically balanced expert load. Due to space constraints, we only present the\nresults of two layers as an example, with the results of all layers provided in Appendix C.\ncertain sequences or small batches, and (2) domain-shift-induced load imbalance during infer-\nence. The first challenge is naturally addressed by our training framework that uses large-scale\nexpert parallelism and data parallelism, which guarantees a large size of each micro-batch. For\nthe second challenge, we also design and implement an efficient inference framework with\nredundant expert deployment, as described in Section 3.4, to overcome it.\n5. Post-Training\n5.1. Supervised Fine-Tuning\nWe curate our instruction-tuning datasets to include 1.5M instances spanning multiple domains,\nwith each domain employing distinct data creation methods tailored to its specific requirements.\nReasoning Data.\nFor reasoning-related datasets, including those focused on mathematics,\ncode competition problems, and logic puzzles, we generate the data by leveraging an internal\nDeepSeek-R1 model. Specifically, while the R1-generated data demonstrates strong accuracy, it\nsuffers from issues such as overthinking, poor formatting, and excessive length. Our objective is\nto balance the high accuracy of R1-generated reasoning data and the clarity and conciseness of\nregularly formatted reasoning data.\nTo establish our methodology, we begin by developing an expert model tailored to a specific\ndomain, such as code, mathematics, or general reasoning, using a combined Supervised Fine-\nTuning (SFT) and Reinforcement Learning (RL) training pipeline. This expert model serves as a\ndata generator for the final model. The training process involves generating two distinct types\nof SFT samples for each instance: the first couples the problem with its original response in\nthe format of <problem, original response>, while the second incorporates a system prompt\n28\n\nalongside the problem and the R1 response in the format of <system prompt, problem, R1\nresponse>.\nThe system prompt is meticulously designed to include instructions that guide the model\ntoward producing responses enriched with mechanisms for reflection and verification. During\nthe RL phase, the model leverages high-temperature sampling to generate responses that\nintegrate patterns from both the R1-generated and original data, even in the absence of explicit\nsystem prompts. After hundreds of RL steps, the intermediate RL model learns to incorporate\nR1 patterns, thereby enhancing overall performance strategically.\nUpon completing the RL training phase, we implement rejection sampling to curate high-\nquality SFT data for the final model, where the expert models are used as data generation\nsources. This method ensures that the final training data retains the strengths of DeepSeek-R1\nwhile producing responses that are concise and effective.\nNon-Reasoning Data.\nFor non-reasoning data, such as creative writing, role-play, and sim-\nple question answering, we utilize DeepSeek-V2.5 to generate responses and enlist human\nannotators to verify the accuracy and correctness of the data.\nSFT Settings.\nWe fine-tune DeepSeek-V3-Base for two epochs using the SFT dataset, using the\ncosine decay learning rate scheduling that starts at 5 × 10−6 and gradually decreases to 1 × 10−6.\nDuring training, each single sequence is packed from multiple samples. However, we adopt a\nsample masking strategy to ensure that these examples remain isolated and mutually invisible.\n5.2. Reinforcement Learning\n5.2.1. Reward Model\nWe employ a rule-based Reward Model (RM) and a model-based RM in our RL process.\nRule-Based RM.\nFor questions that can be validated using specific rules, we adopt a rule-\nbased reward system to determine the feedback. For instance, certain math problems have\ndeterministic results, and we require the model to provide the final answer within a designated\nformat (e.g., in a box), allowing us to apply rules to verify the correctness. Similarly, for LeetCode\nproblems, we can utilize a compiler to generate feedback based on test cases. By leveraging\nrule-based validation wherever possible, we ensure a higher level of reliability, as this approach\nis resistant to manipulation or exploitation.\nModel-Based RM.\nFor questions with free-form ground-truth answers, we rely on the reward\nmodel to determine whether the response matches the expected ground-truth. Conversely, for\nquestions without a definitive ground-truth, such as those involving creative writing, the reward\nmodel is tasked with providing feedback based on the question and the corresponding answer\nas inputs. The reward model is trained from the DeepSeek-V3 SFT checkpoints. To enhance its\nreliability, we construct preference data that not only provides the final reward but also includes\nthe chain-of-thought leading to the reward. This approach helps mitigate the risk of reward\nhacking in specific tasks.\n29\n\n5.2.2. Group Relative Policy Optimization\nSimilar to DeepSeek-V2 (DeepSeek-AI, 2024c), we adopt Group Relative Policy Optimiza-\ntion (GRPO) (Shao et al., 2024), which foregoes the critic model that is typically with the same\nsize as the policy model, and estimates the baseline from group scores instead. Specifically, for\neach question 𝑞, GRPO samples a group of outputs {𝑜1, 𝑜2, · · · , 𝑜𝐺} from the old policy model\n𝜋𝜃𝑜𝑙𝑑and then optimizes the policy model 𝜋𝜃by maximizing the following objective:\nJ𝐺𝑅𝑃𝑂(𝜃) = E[𝑞∼𝑃(𝑄), {𝑜𝑖}𝐺\n𝑖=1 ∼𝜋𝜃𝑜𝑙𝑑(𝑂|𝑞)]\n1\n𝐺\n𝐺\n∑︁\n𝑖=1\n\u0012\nmin\n\u0012 𝜋𝜃(𝑜𝑖|𝑞)\n𝜋𝜃𝑜𝑙𝑑(𝑜𝑖|𝑞) 𝐴𝑖, clip\n\u0012 𝜋𝜃(𝑜𝑖|𝑞)\n𝜋𝜃𝑜𝑙𝑑(𝑜𝑖|𝑞) , 1 −𝜀, 1 + 𝜀\n\u0013\n𝐴𝑖\n\u0013\n−𝛽D𝐾𝐿\n\u0000𝜋𝜃||𝜋𝑟𝑒𝑓\n\u0001\u0013\n,\n(26)\nD𝐾𝐿\n\u0000𝜋𝜃||𝜋𝑟𝑒𝑓\n\u0001 =\n𝜋𝑟𝑒𝑓(𝑜𝑖|𝑞)\n𝜋𝜃(𝑜𝑖|𝑞) −log\n𝜋𝑟𝑒𝑓(𝑜𝑖|𝑞)\n𝜋𝜃(𝑜𝑖|𝑞) −1,\n(27)\nwhere 𝜀and 𝛽are hyper-parameters; 𝜋𝑟𝑒𝑓is the reference model; and 𝐴𝑖is the advantage, derived\nfrom the rewards {𝑟1, 𝑟2, . . . , 𝑟𝐺} corresponding to the outputs within each group:\n𝐴𝑖= 𝑟𝑖−mean({𝑟1, 𝑟2, · · · , 𝑟𝐺})\nstd({𝑟1, 𝑟2, · · · , 𝑟𝐺})\n.\n(28)\nWe incorporate prompts from diverse domains, such as coding, math, writing, role-playing,\nand question answering, during the RL process. This approach not only aligns the model more\nclosely with human preferences but also enhances performance on benchmarks, especially in\nscenarios where available SFT data are limited.\n5.3. Evaluations\n5.3.1. Evaluation Settings\nEvaluation Benchmarks.\nApart from the benchmark we used for base model testing, we\nfurther evaluate instructed models on IFEval (Zhou et al., 2023), FRAMES (Krishna et al.,\n2024), LongBench v2 (Bai et al., 2024), GPQA (Rein et al., 2023), SimpleQA (OpenAI, 2024c), C-\nSimpleQA (He et al., 2024), SWE-Bench Verified (OpenAI, 2024d), Aider 1, LiveCodeBench (Jain\net al., 2024) (questions from August 2024 to November 2024), Codeforces 2, Chinese National\nHigh School Mathematics Olympiad (CNMO 2024)3, and American Invitational Mathematics\nExamination 2024 (AIME 2024) (MAA, 2024).\nCompared Baselines.\nWe conduct comprehensive evaluations of our chat model against sev-\neral strong baselines, including DeepSeek-V2-0506, DeepSeek-V2.5-0905, Qwen2.5 72B Instruct,\nLLaMA-3.1 405B Instruct, Claude-Sonnet-3.5-1022, and GPT-4o-0513. For the DeepSeek-V2\nmodel series, we select the most representative variants for comparison. For closed-source\nmodels, evaluations are performed through their respective APIs.\nDetailed Evaluation Configurations.\nFor standard benchmarks including MMLU, DROP,\nGPQA, and SimpleQA, we adopt the evaluation prompts from the simple-evals framework4.\n1https://aider.chat\n2https://codeforces.com\n3https://www.cms.org.cn/Home/comp/comp/cid/12.html\n4https://github.com/openai/simple-evals\n30\n\nWe utilize the Zero-Eval prompt format (Lin, 2024) for MMLU-Redux in a zero-shot setting.\nFor other datasets, we follow their original evaluation protocols with default prompts as pro-\nvided by the dataset creators. For code and math benchmarks, the HumanEval-Mul dataset\nincludes 8 mainstream programming languages (Python, Java, Cpp, C#, JavaScript, TypeScript,\nPHP, and Bash) in total. We use CoT and non-CoT methods to evaluate model performance\non LiveCodeBench, where the data are collected from August 2024 to November 2024. The\nCodeforces dataset is measured using the percentage of competitors. SWE-Bench verified is\nevaluated using the agentless framework (Xia et al., 2024). We use the “diff” format to evaluate\nthe Aider-related benchmarks. For mathematical assessments, AIME and CNMO 2024 are\nevaluated with a temperature of 0.7, and the results are averaged over 16 runs, while MATH-500\nemploys greedy decoding. We allow all models to output a maximum of 8192 tokens for each\nbenchmark.\nBenchmark (Metric)\nDeepSeek DeepSeek Qwen2.5 LLaMA-3.1 Claude-3.5- GPT-4o DeepSeek\nV2-0506\nV2.5-0905 72B-Inst. 405B-Inst. Sonnet-1022\n0513\nV3\nArchitecture\nMoE\nMoE\nDense\nDense\n-\n-\nMoE\n# Activated Params\n21B\n21B\n72B\n405B\n-\n-\n37B\n# Total Params\n236B\n236B\n72B\n405B\n-\n-\n671B\nEnglish\nMMLU (EM)\n78.2\n80.6\n85.3\n88.6\n88.3\n87.2\n88.5\nMMLU-Redux (EM)\n77.9\n80.3\n85.6\n86.2\n88.9\n88.0\n89.1\nMMLU-Pro (EM)\n58.5\n66.2\n71.6\n73.3\n78.0\n72.6\n75.9\nDROP (3-shot F1)\n83.0\n87.8\n76.7\n88.7\n88.3\n83.7\n91.6\nIF-Eval (Prompt Strict)\n57.7\n80.6\n84.1\n86.0\n86.5\n84.3\n86.1\nGPQA-Diamond (Pass@1)\n35.3\n41.3\n49.0\n51.1\n65.0\n49.9\n59.1\nSimpleQA (Correct)\n9.0\n10.2\n9.1\n17.1\n28.4\n38.2\n24.9\nFRAMES (Acc.)\n66.9\n65.4\n69.8\n70.0\n72.5\n80.5\n73.3\nLongBench v2 (Acc.)\n31.6\n35.4\n39.4\n36.1\n41.0\n48.1\n48.7\nCode\nHumanEval-Mul (Pass@1)\n69.3\n77.4\n77.3\n77.2\n81.7\n80.5\n82.6\nLiveCodeBench (Pass@1-COT)\n18.8\n29.2\n31.1\n28.4\n36.3\n33.4\n40.5\nLiveCodeBench (Pass@1)\n20.3\n28.4\n28.7\n30.1\n32.8\n34.2\n37.6\nCodeforces (Percentile)\n17.5\n35.6\n24.8\n25.3\n20.3\n23.6\n51.6\nSWE Verified (Resolved)\n-\n22.6\n23.8\n24.5\n50.8\n38.8\n42.0\nAider-Edit (Acc.)\n60.3\n71.6\n65.4\n63.9\n84.2\n72.9\n79.7\nAider-Polyglot (Acc.)\n-\n18.2\n7.6\n5.8\n45.3\n16.0\n49.6\nMath\nAIME 2024 (Pass@1)\n4.6\n16.7\n23.3\n23.3\n16.0\n9.3\n39.2\nMATH-500 (EM)\n56.3\n74.7\n80.0\n73.8\n78.3\n74.6\n90.2\nCNMO 2024 (Pass@1)\n2.8\n10.8\n15.9\n6.8\n13.1\n10.8\n43.2\nChinese\nCLUEWSC (EM)\n89.9\n90.4\n91.4\n84.7\n85.4\n87.9\n90.9\nC-Eval (EM)\n78.6\n79.5\n86.1\n61.5\n76.7\n76.0\n86.5\nC-SimpleQA (Correct)\n48.5\n54.1\n48.4\n50.4\n51.3\n59.3\n64.8\nTable 6 | Comparison between DeepSeek-V3 and other representative chat models. All models\nare evaluated in a configuration that limits the output length to 8K. Benchmarks containing\nfewer than 1000 samples are tested multiple times using varying temperature settings to derive\nrobust final results. DeepSeek-V3 stands as the best-performing open-source model, and also\nexhibits competitive performance against frontier closed-source models.\n5.3.2. Standard Evaluation\nTable 6 presents the evaluation results, showcasing that DeepSeek-V3 stands as the best-\nperforming open-source model. Additionally, it is competitive against frontier closed-source\nmodels like GPT-4o and Claude-3.5-Sonnet.\n31\n\nEnglish Benchmarks.\nMMLU is a widely recognized benchmark designed to assess the perfor-\nmance of large language models, across diverse knowledge domains and tasks. DeepSeek-V3\ndemonstrates competitive performance, standing on par with top-tier models such as LLaMA-\n3.1-405B, GPT-4o, and Claude-Sonnet 3.5, while significantly outperforming Qwen2.5 72B.\nMoreover, DeepSeek-V3 excels in MMLU-Pro, a more challenging educational knowledge\nbenchmark, where it closely trails Claude-Sonnet 3.5. On MMLU-Redux, a refined version of\nMMLU with corrected labels, DeepSeek-V3 surpasses its peers. In addition, on GPQA-Diamond,\na PhD-level evaluation testbed, DeepSeek-V3 achieves remarkable results, ranking just behind\nClaude 3.5 Sonnet and outperforming all other competitors by a substantial margin.\nIn long-context understanding benchmarks such as DROP, LongBench v2, and FRAMES,\nDeepSeek-V3 continues to demonstrate its position as a top-tier model. It achieves an impressive\n91.6 F1 score in the 3-shot setting on DROP, outperforming all other models in this category.\nOn FRAMES, a benchmark requiring question-answering over 100k token contexts, DeepSeek-\nV3 closely trails GPT-4o while outperforming all other models by a significant margin. This\ndemonstrates the strong capability of DeepSeek-V3 in handling extremely long-context tasks.\nThe long-context capability of DeepSeek-V3 is further validated by its best-in-class performance\non LongBench v2, a dataset that was released just a few weeks before the launch of DeepSeek\nV3. On the factual knowledge benchmark, SimpleQA, DeepSeek-V3 falls behind GPT-4o and\nClaude-Sonnet, primarily due to its design focus and resource allocation. DeepSeek-V3 assigns\nmore training tokens to learn Chinese knowledge, leading to exceptional performance on the\nC-SimpleQA. On the instruction-following benchmark, DeepSeek-V3 significantly outperforms\nits predecessor, DeepSeek-V2-series, highlighting its improved ability to understand and adhere\nto user-defined format constraints.\nCode and Math Benchmarks.\nCoding is a challenging and practical task for LLMs, encom-\npassing engineering-focused tasks like SWE-Bench-Verified and Aider, as well as algorithmic\ntasks such as HumanEval and LiveCodeBench. In engineering tasks, DeepSeek-V3 trails behind\nClaude-Sonnet-3.5-1022 but significantly outperforms open-source models. The open-source\nDeepSeek-V3 is expected to foster advancements in coding-related engineering tasks. By pro-\nviding access to its robust capabilities, DeepSeek-V3 can drive innovation and improvement\nin areas such as software engineering and algorithm development, empowering developers\nand researchers to push the boundaries of what open-source models can achieve in coding\ntasks. In algorithmic tasks, DeepSeek-V3 demonstrates superior performance, outperforming\nall baselines on benchmarks like HumanEval-Mul and LiveCodeBench. This success can be\nattributed to its advanced knowledge distillation technique, which effectively enhances its code\ngeneration and problem-solving capabilities in algorithm-focused tasks.\nOn math benchmarks, DeepSeek-V3 demonstrates exceptional performance, significantly\nsurpassing baselines and setting a new state-of-the-art for non-o1-like models. Specifically, on\nAIME, MATH-500, and CNMO 2024, DeepSeek-V3 outperforms the second-best model, Qwen2.5\n72B, by approximately 10% in absolute scores, which is a substantial margin for such challenging\nbenchmarks. This remarkable capability highlights the effectiveness of the distillation technique\nfrom DeepSeek-R1, which has been proven highly beneficial for non-o1-like models.\nChinese Benchmarks.\nQwen and DeepSeek are two representative model series with robust\nsupport for both Chinese and English. On the factual benchmark Chinese SimpleQA, DeepSeek-\nV3 surpasses Qwen2.5-72B by 16.4 points, despite Qwen2.5 being trained on a larger corpus\ncompromising 18T tokens, which are 20% more than the 14.8T tokens that DeepSeek-V3 is\n32\n\nModel\nArena-Hard\nAlpacaEval 2.0\nDeepSeek-V2.5-0905\n76.2\n50.5\nQwen2.5-72B-Instruct\n81.2\n49.1\nLLaMA-3.1 405B\n69.3\n40.5\nGPT-4o-0513\n80.4\n51.1\nClaude-Sonnet-3.5-1022\n85.2\n52.0\nDeepSeek-V3\n85.5\n70.0\nTable 7 | English open-ended conversation evaluations. For AlpacaEval 2.0, we use the length-\ncontrolled win rate as the metric.\npre-trained on.\nOn C-Eval, a representative benchmark for Chinese educational knowledge evaluation, and\nCLUEWSC (Chinese Winograd Schema Challenge), DeepSeek-V3 and Qwen2.5-72B exhibit\nsimilar performance levels, indicating that both models are well-optimized for challenging\nChinese-language reasoning and educational tasks.\n5.3.3. Open-Ended Evaluation\nIn addition to standard benchmarks, we also evaluate our models on open-ended generation\ntasks using LLMs as judges, with the results shown in Table 7. Specifically, we adhere to\nthe original configurations of AlpacaEval 2.0 (Dubois et al., 2024) and Arena-Hard (Li et al.,\n2024a), which leverage GPT-4-Turbo-1106 as judges for pairwise comparisons. On Arena-Hard,\nDeepSeek-V3 achieves an impressive win rate of over 86% against the baseline GPT-4-0314,\nperforming on par with top-tier models like Claude-Sonnet-3.5-1022. This underscores the\nrobust capabilities of DeepSeek-V3, especially in dealing with complex prompts, including\ncoding and debugging tasks. Furthermore, DeepSeek-V3 achieves a groundbreaking milestone\nas the first open-source model to surpass 85% on the Arena-Hard benchmark. This achievement\nsignificantly bridges the performance gap between open-source and closed-source models,\nsetting a new standard for what open-source models can accomplish in challenging domains.\nSimilarly, DeepSeek-V3 showcases exceptional performance on AlpacaEval 2.0, outperform-\ning both closed-source and open-source models. This demonstrates its outstanding proficiency in\nwriting tasks and handling straightforward question-answering scenarios. Notably, it surpasses\nDeepSeek-V2.5-0905 by a significant margin of 20%, highlighting substantial improvements in\ntackling simple tasks and showcasing the effectiveness of its advancements.\n5.3.4. DeepSeek-V3 as a Generative Reward Model\nWe compare the judgment ability of DeepSeek-V3 with state-of-the-art models, namely GPT-4o\nand Claude-3.5. Table 8 presents the performance of these models in RewardBench (Lambert\net al., 2024). DeepSeek-V3 achieves performance on par with the best versions of GPT-4o-0806\nand Claude-3.5-Sonnet-1022, while surpassing other versions. Additionally, the judgment ability\nof DeepSeek-V3 can also be enhanced by the voting technique. Therefore, we employ DeepSeek-\nV3 along with voting to offer self-feedback on open-ended questions, thereby improving the\neffectiveness and robustness of the alignment process.\n33\n\nModel\nChat\nChat-Hard\nSafety\nReasoning\nAverage\nGPT-4o-0513\n96.6\n70.4\n86.7\n84.9\n84.7\nGPT-4o-0806\n96.1\n76.1\n88.1\n86.6\n86.7\nGPT-4o-1120\n95.8\n71.3\n86.2\n85.2\n84.6\nClaude-3.5-sonnet-0620\n96.4\n74.0\n81.6\n84.7\n84.2\nClaude-3.5-sonnet-1022\n96.4\n79.7\n91.1\n87.6\n88.7\nDeepSeek-V3\n96.9\n79.8\n87.0\n84.3\n87.0\nDeepSeek-V3 (maj@6)\n96.9\n82.6\n89.5\n89.2\n89.6\nTable 8 | Performances of GPT-4o, Claude-3.5-sonnet and DeepSeek-V3 on RewardBench.\nModel\nLiveCodeBench-CoT\nMATH-500\nPass@1\nLength\nPass@1\nLength\nDeepSeek-V2.5 Baseline\n31.1\n718\n74.6\n769\nDeepSeek-V2.5 +R1 Distill\n37.4\n783\n83.2\n1510\nTable 9 | The contribution of distillation from DeepSeek-R1. The evaluation settings of Live-\nCodeBench and MATH-500 are the same as in Table 6.\n5.4. Discussion\n5.4.1. Distillation from DeepSeek-R1\nWe ablate the contribution of distillation from DeepSeek-R1 based on DeepSeek-V2.5. The\nbaseline is trained on short CoT data, whereas its competitor uses data generated by the expert\ncheckpoints described above.\nTable 9 demonstrates the effectiveness of the distillation data, showing significant improve-\nments in both LiveCodeBench and MATH-500 benchmarks. Our experiments reveal an inter-\nesting trade-off: the distillation leads to better performance but also substantially increases the\naverage response length. To maintain a balance between model accuracy and computational\nefficiency, we carefully selected optimal settings for DeepSeek-V3 in distillation.\nOur research suggests that knowledge distillation from reasoning models presents a promis-\ning direction for post-training optimization. While our current work focuses on distilling data\nfrom mathematics and coding domains, this approach shows potential for broader applications\nacross various task domains. The effectiveness demonstrated in these specific areas indicates\nthat long-CoT distillation could be valuable for enhancing model performance in other cogni-\ntive tasks requiring complex reasoning. Further exploration of this approach across different\ndomains remains an important direction for future research.\n5.4.2. Self-Rewarding\nRewards play a pivotal role in RL, steering the optimization process. In domains where verifica-\ntion through external tools is straightforward, such as some coding or mathematics scenarios, RL\ndemonstrates exceptional efficacy. However, in more general scenarios, constructing a feedback\nmechanism through hard coding is impractical. During the development of DeepSeek-V3, for\nthese broader contexts, we employ the constitutional AI approach (Bai et al., 2022), leveraging\nthe voting evaluation results of DeepSeek-V3 itself as a feedback source. This method has\n34\n\nproduced notable alignment effects, significantly enhancing the performance of DeepSeek-V3\nin subjective evaluations. By integrating additional constitutional inputs, DeepSeek-V3 can\noptimize towards the constitutional direction. We believe that this paradigm, which combines\nsupplementary information with LLMs as a feedback source, is of paramount importance. The\nLLM serves as a versatile processor capable of transforming unstructured information from\ndiverse scenarios into rewards, ultimately facilitating the self-improvement of LLMs. Beyond\nself-rewarding, we are also dedicated to uncovering other general and scalable rewarding\nmethods to consistently advance the model capabilities in general scenarios.\n5.4.3. Multi-Token Prediction Evaluation\nInstead of predicting just the next single token, DeepSeek-V3 predicts the next 2 tokens through\nthe MTP technique. Combined with the framework of speculative decoding (Leviathan et al.,\n2023; Xia et al., 2023), it can significantly accelerate the decoding speed of the model. A natural\nquestion arises concerning the acceptance rate of the additionally predicted token. Based on\nour evaluation, the acceptance rate of the second token prediction ranges between 85% and 90%\nacross various generation topics, demonstrating consistent reliability. This high acceptance rate\nenables DeepSeek-V3 to achieve a significantly improved decoding speed, delivering 1.8 times\nTPS (Tokens Per Second).\n6. Conclusion, Limitations, and Future Directions\nIn this paper, we introduce DeepSeek-V3, a large MoE language model with 671B total pa-\nrameters and 37B activated parameters, trained on 14.8T tokens. In addition to the MLA and\nDeepSeekMoE architectures, it also pioneers an auxiliary-loss-free strategy for load balancing\nand sets a multi-token prediction training objective for stronger performance. The training of\nDeepSeek-V3 is cost-effective due to the support of FP8 training and meticulous engineering op-\ntimizations. The post-training also makes a success in distilling the reasoning capability from the\nDeepSeek-R1 series of models. Comprehensive evaluations demonstrate that DeepSeek-V3 has\nemerged as the strongest open-source model currently available, and achieves performance com-\nparable to leading closed-source models like GPT-4o and Claude-3.5-Sonnet. Despite its strong\nperformance, it also maintains economical training costs. It requires only 2.788M H800 GPU\nhours for its full training, including pre-training, context length extension, and post-training.\nWhile acknowledging its strong performance and cost-effectiveness, we also recognize that\nDeepSeek-V3 has some limitations, especially on the deployment. Firstly, to ensure efficient\ninference, the recommended deployment unit for DeepSeek-V3 is relatively large, which might\npose a burden for small-sized teams. Secondly, although our deployment strategy for DeepSeek-\nV3 has achieved an end-to-end generation speed of more than two times that of DeepSeek-V2,\nthere still remains potential for further enhancement. Fortunately, these limitations are expected\nto be naturally addressed with the development of more advanced hardware.\nDeepSeek consistently adheres to the route of open-source models with longtermism, aiming\nto steadily approach the ultimate goal of AGI (Artificial General Intelligence). In the future, we\nplan to strategically invest in research across the following directions.\n• We will consistently study and refine our model architectures, aiming to further improve\nboth the training and inference efficiency, striving to approach efficient support for infinite\ncontext length. Additionally, we will try to break through the architectural limitations of\nTransformer, thereby pushing the boundaries of its modeling capabilities.\n35\n\n• We will continuously iterate on the quantity and quality of our training data, and explore\nthe incorporation of additional training signal sources, aiming to drive data scaling across\na more comprehensive range of dimensions.\n• We will consistently explore and iterate on the deep thinking capabilities of our models,\naiming to enhance their intelligence and problem-solving abilities by expanding their\nreasoning length and depth.\n• We will explore more comprehensive and multi-dimensional model evaluation methods to\nprevent the tendency towards optimizing a fixed set of benchmarks during research, which\nmay create a misleading impression of the model capabilities and affect our foundational\nassessment.\nReferences\nAI@Meta. Llama 3 model card, 2024a. URL https://github.com/meta-llama/llama3/bl\nob/main/MODEL_CARD.md.\nAI@Meta. Llama 3.1 model card, 2024b. URL https://github.com/meta-llama/llama-m\nodels/blob/main/models/llama3_1/MODEL_CARD.md.\nAnthropic. Claude 3.5 sonnet, 2024. URL https://www.anthropic.com/news/claude-3\n-5-sonnet.\nJ. Austin, A. Odena, M. Nye, M. Bosma, H. Michalewski, D. Dohan, E. Jiang, C. Cai, M. Terry,\nQ. Le, et al. Program synthesis with large language models. arXiv preprint arXiv:2108.07732,\n2021.\nY. Bai, S. Kadavath, S. Kundu, A. Askell, J. Kernion, A. Jones, A. Chen, A. Goldie, A. Mirhoseini,\nC. McKinnon, et al. Constitutional AI: Harmlessness from AI feedback. arXiv preprint\narXiv:2212.08073, 2022.\nY. Bai, S. Tu, J. Zhang, H. Peng, X. Wang, X. Lv, S. Cao, J. Xu, L. Hou, Y. Dong, J. Tang, and\nJ. Li. LongBench v2: Towards deeper understanding and reasoning on realistic long-context\nmultitasks. arXiv preprint arXiv:2412.15204, 2024.\nM. Bauer, S. Treichler, and A. Aiken. Singe: leveraging warp specialization for high performance\non GPUs. In Proceedings of the 19th ACM SIGPLAN Symposium on Principles and Practice\nof Parallel Programming, PPoPP ’14, page 119–130, New York, NY, USA, 2014. Association\nfor Computing Machinery. ISBN 9781450326568. doi: 10.1145/2555243.2555258. URL\nhttps://doi.org/10.1145/2555243.2555258.\nY. Bisk, R. Zellers, R. L. Bras, J. Gao, and Y. Choi. PIQA: reasoning about physical commonsense\nin natural language. In The Thirty-Fourth AAAI Conference on Artificial Intelligence, AAAI\n2020, The Thirty-Second Innovative Applications of Artificial Intelligence Conference, IAAI\n2020, The Tenth AAAI Symposium on Educational Advances in Artificial Intelligence, EAAI\n2020, New York, NY, USA, February 7-12, 2020, pages 7432–7439. AAAI Press, 2020. doi:\n10.1609/aaai.v34i05.6239. URL https://doi.org/10.1609/aaai.v34i05.6239.\nM. Chen, J. Tworek, H. Jun, Q. Yuan, H. P. de Oliveira Pinto, J. Kaplan, H. Edwards, Y. Burda,\nN. Joseph, G. Brockman, A. Ray, R. Puri, G. Krueger, M. Petrov, H. Khlaaf, G. Sastry, P. Mishkin,\nB. Chan, S. Gray, N. Ryder, M. Pavlov, A. Power, L. Kaiser, M. Bavarian, C. Winter, P. Tillet,\nF. P. Such, D. Cummings, M. Plappert, F. Chantzis, E. Barnes, A. Herbert-Voss, W. H. Guss,\nA. Nichol, A. Paino, N. Tezak, J. Tang, I. Babuschkin, S. Balaji, S. Jain, W. Saunders, C. Hesse,\n36\n\nA. N. Carr, J. Leike, J. Achiam, V. Misra, E. Morikawa, A. Radford, M. Knight, M. Brundage,\nM. Murati, K. Mayer, P. Welinder, B. McGrew, D. Amodei, S. McCandlish, I. Sutskever, and\nW. Zaremba. Evaluating large language models trained on code. CoRR, abs/2107.03374, 2021.\nURL https://arxiv.org/abs/2107.03374.\nP. Clark, I. Cowhey, O. Etzioni, T. Khot, A. Sabharwal, C. Schoenick, and O. Tafjord. Think you\nhave solved question answering? try arc, the AI2 reasoning challenge. CoRR, abs/1803.05457,\n2018. URL http://arxiv.org/abs/1803.05457.\nK. Cobbe, V. Kosaraju, M. Bavarian, M. Chen, H. Jun, L. Kaiser, M. Plappert, J. Tworek,\nJ. Hilton, R. Nakano, et al. Training verifiers to solve math word problems. arXiv preprint\narXiv:2110.14168, 2021.\nY. Cui, T. Liu, W. Che, L. Xiao, Z. Chen, W. Ma, S. Wang, and G. Hu. A span-extraction\ndataset for Chinese machine reading comprehension. In K. Inui, J. Jiang, V. Ng, and X. Wan,\neditors, Proceedings of the 2019 Conference on Empirical Methods in Natural Language\nProcessing and the 9th International Joint Conference on Natural Language Processing\n(EMNLP-IJCNLP), pages 5883–5889, Hong Kong, China, Nov. 2019. Association for Computa-\ntional Linguistics. doi: 10.18653/v1/D19-1600. URL https://aclanthology.org/D19-1\n600.\nD. Dai, C. Deng, C. Zhao, R. X. Xu, H. Gao, D. Chen, J. Li, W. Zeng, X. Yu, Y. Wu, Z. Xie, Y. K.\nLi, P. Huang, F. Luo, C. Ruan, Z. Sui, and W. Liang. Deepseekmoe: Towards ultimate expert\nspecialization in mixture-of-experts language models. CoRR, abs/2401.06066, 2024. URL\nhttps://doi.org/10.48550/arXiv.2401.06066.\nDeepSeek-AI. Deepseek-coder-v2: Breaking the barrier of closed-source models in code intelli-\ngence. CoRR, abs/2406.11931, 2024a. URL https://doi.org/10.48550/arXiv.2406.11\n931.\nDeepSeek-AI. Deepseek LLM: scaling open-source language models with longtermism. CoRR,\nabs/2401.02954, 2024b. URL https://doi.org/10.48550/arXiv.2401.02954.\nDeepSeek-AI. Deepseek-v2: A strong, economical, and efficient mixture-of-experts language\nmodel. CoRR, abs/2405.04434, 2024c. URL https://doi.org/10.48550/arXiv.2405.\n04434.\nT. Dettmers, M. Lewis, Y. Belkada, and L. Zettlemoyer. Gpt3. int8 (): 8-bit matrix multiplication\nfor transformers at scale. Advances in Neural Information Processing Systems, 35:30318–\n30332, 2022.\nH. Ding, Z. Wang, G. Paolini, V. Kumar, A. Deoras, D. Roth, and S. Soatto. Fewer truncations\nimprove language modeling. arXiv preprint arXiv:2404.10830, 2024.\nD. Dua, Y. Wang, P. Dasigi, G. Stanovsky, S. Singh, and M. Gardner. DROP: A reading compre-\nhension benchmark requiring discrete reasoning over paragraphs. In J. Burstein, C. Doran, and\nT. Solorio, editors, Proceedings of the 2019 Conference of the North American Chapter of the\nAssociation for Computational Linguistics: Human Language Technologies, NAACL-HLT\n2019, Minneapolis, MN, USA, June 2-7, 2019, Volume 1 (Long and Short Papers), pages 2368–\n2378. Association for Computational Linguistics, 2019. doi: 10.18653/V1/N19-1246. URL\nhttps://doi.org/10.18653/v1/n19-1246.\nY. Dubois, B. Galambosi, P. Liang, and T. B. Hashimoto. Length-controlled alpacaeval: A simple\nway to debias automatic evaluators. arXiv preprint arXiv:2404.04475, 2024.\n37\n\nW. Fedus, B. Zoph, and N. Shazeer. Switch transformers: Scaling to trillion parameter models\nwith simple and efficient sparsity. CoRR, abs/2101.03961, 2021. URL https://arxiv.org/\nabs/2101.03961.\nM. Fishman, B. Chmiel, R. Banner, and D. Soudry. Scaling FP8 training to trillion-token llms.\narXiv preprint arXiv:2409.12517, 2024.\nE. Frantar, S. Ashkboos, T. Hoefler, and D. Alistarh. Gptq: Accurate post-training quantization\nfor generative pre-trained transformers. arXiv preprint arXiv:2210.17323, 2022.\nL. Gao, S. Biderman, S. Black, L. Golding, T. Hoppe, C. Foster, J. Phang, H. He, A. Thite,\nN. Nabeshima, et al. The Pile: An 800GB dataset of diverse text for language modeling. arXiv\npreprint arXiv:2101.00027, 2020.\nA. P. Gema, J. O. J. Leang, G. Hong, A. Devoto, A. C. M. Mancino, R. Saxena, X. He, Y. Zhao,\nX. Du, M. R. G. Madani, C. Barale, R. McHardy, J. Harris, J. Kaddour, E. van Krieken, and\nP. Minervini. Are we done with mmlu? CoRR, abs/2406.04127, 2024. URL https://doi.or\ng/10.48550/arXiv.2406.04127.\nF. Gloeckle, B. Y. Idrissi, B. Rozière, D. Lopez-Paz, and G. Synnaeve. Better & faster large\nlanguage models via multi-token prediction.\nIn Forty-first International Conference on\nMachine Learning, ICML 2024, Vienna, Austria, July 21-27, 2024. OpenReview.net, 2024. URL\nhttps://openreview.net/forum?id=pEWAcejiU2.\nGoogle. Our next-generation model: Gemini 1.5, 2024. URL https://blog.google/techno\nlogy/ai/google-gemini-next-generation-model-february-2024.\nR. L. Graham, D. Bureddy, P. Lui, H. Rosenstock, G. Shainer, G. Bloch, D. Goldenerg, M. Dubman,\nS. Kotchubievsky, V. Koushnir, et al. Scalable hierarchical aggregation protocol (SHArP): A\nhardware architecture for efficient data reduction. In 2016 First International Workshop on\nCommunication Optimizations in HPC (COMHPC), pages 1–10. IEEE, 2016.\nA. Gu, B. Rozière, H. Leather, A. Solar-Lezama, G. Synnaeve, and S. I. Wang. Cruxeval: A\nbenchmark for code reasoning, understanding and execution, 2024.\nD. Guo, Q. Zhu, D. Yang, Z. Xie, K. Dong, W. Zhang, G. Chen, X. Bi, Y. Wu, Y. K. Li, F. Luo,\nY. Xiong, and W. Liang. Deepseek-coder: When the large language model meets programming\n- the rise of code intelligence. CoRR, abs/2401.14196, 2024. URL https://doi.org/10.485\n50/arXiv.2401.14196.\nA. Harlap, D. Narayanan, A. Phanishayee, V. Seshadri, N. Devanur, G. Ganger, and P. Gibbons.\nPipedream: Fast and efficient pipeline parallel dnn training, 2018. URL https://arxiv.or\ng/abs/1806.03377.\nB. He, L. Noci, D. Paliotta, I. Schlag, and T. Hofmann. Understanding and minimising out-\nlier features in transformer training. In The Thirty-eighth Annual Conference on Neural\nInformation Processing Systems.\nY. He, S. Li, J. Liu, Y. Tan, W. Wang, H. Huang, X. Bu, H. Guo, C. Hu, B. Zheng, et al. Chi-\nnese simpleqa: A chinese factuality evaluation for large language models. arXiv preprint\narXiv:2411.07140, 2024.\nD. Hendrycks, C. Burns, S. Basart, A. Zou, M. Mazeika, D. Song, and J. Steinhardt. Measuring\nmassive multitask language understanding. arXiv preprint arXiv:2009.03300, 2020.\n38\n\nD. Hendrycks, C. Burns, S. Kadavath, A. Arora, S. Basart, E. Tang, D. Song, and J. Steinhardt. Mea-\nsuring mathematical problem solving with the math dataset. arXiv preprint arXiv:2103.03874,\n2021.\nY. Huang, Y. Bai, Z. Zhu, J. Zhang, J. Zhang, T. Su, J. Liu, C. Lv, Y. Zhang, J. Lei, et al. C-Eval: A\nmulti-level multi-discipline chinese evaluation suite for foundation models. arXiv preprint\narXiv:2305.08322, 2023.\nN. Jain, K. Han, A. Gu, W. Li, F. Yan, T. Zhang, S. Wang, A. Solar-Lezama, K. Sen, and I. Stoica.\nLivecodebench: Holistic and contamination free evaluation of large language models for code.\nCoRR, abs/2403.07974, 2024. URL https://doi.org/10.48550/arXiv.2403.07974.\nA. Q. Jiang, A. Sablayrolles, A. Mensch, C. Bamford, D. S. Chaplot, D. d. l. Casas, F. Bressand,\nG. Lengyel, G. Lample, L. Saulnier, et al. Mistral 7b. arXiv preprint arXiv:2310.06825, 2023.\nM. Joshi, E. Choi, D. Weld, and L. Zettlemoyer. TriviaQA: A large scale distantly supervised chal-\nlenge dataset for reading comprehension. In R. Barzilay and M.-Y. Kan, editors, Proceedings of\nthe 55th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long\nPapers), pages 1601–1611, Vancouver, Canada, July 2017. Association for Computational\nLinguistics. doi: 10.18653/v1/P17-1147. URL https://aclanthology.org/P17-1147.\nD. Kalamkar, D. Mudigere, N. Mellempudi, D. Das, K. Banerjee, S. Avancha, D. T. Vooturi,\nN. Jammalamadaka, J. Huang, H. Yuen, et al. A study of bfloat16 for deep learning training.\narXiv preprint arXiv:1905.12322, 2019.\nS. Krishna, K. Krishna, A. Mohananey, S. Schwarcz, A. Stambler, S. Upadhyay, and M. Faruqui.\nFact, fetch, and reason: A unified evaluation of retrieval-augmented generation. CoRR,\nabs/2409.12941, 2024. doi: 10.48550/ARXIV.2409.12941. URL https://doi.org/10.485\n50/arXiv.2409.12941.\nT. Kwiatkowski, J. Palomaki, O. Redfield, M. Collins, A. P. Parikh, C. Alberti, D. Epstein,\nI. Polosukhin, J. Devlin, K. Lee, K. Toutanova, L. Jones, M. Kelcey, M. Chang, A. M. Dai,\nJ. Uszkoreit, Q. Le, and S. Petrov. Natural questions: a benchmark for question answering\nresearch. Trans. Assoc. Comput. Linguistics, 7:452–466, 2019. doi: 10.1162/tacl\\_a\\_00276.\nURL https://doi.org/10.1162/tacl_a_00276.\nG. Lai, Q. Xie, H. Liu, Y. Yang, and E. H. Hovy. RACE: large-scale reading comprehension\ndataset from examinations. In M. Palmer, R. Hwa, and S. Riedel, editors, Proceedings of\nthe 2017 Conference on Empirical Methods in Natural Language Processing, EMNLP 2017,\nCopenhagen, Denmark, September 9-11, 2017, pages 785–794. Association for Computational\nLinguistics, 2017. doi: 10.18653/V1/D17-1082. URL https://doi.org/10.18653/v1/d1\n7-1082.\nN. Lambert, V. Pyatkin, J. Morrison, L. Miranda, B. Y. Lin, K. Chandu, N. Dziri, S. Kumar,\nT. Zick, Y. Choi, et al. Rewardbench: Evaluating reward models for language modeling. arXiv\npreprint arXiv:2403.13787, 2024.\nD. Lepikhin, H. Lee, Y. Xu, D. Chen, O. Firat, Y. Huang, M. Krikun, N. Shazeer, and Z. Chen.\nGshard: Scaling giant models with conditional computation and automatic sharding. In 9th\nInternational Conference on Learning Representations, ICLR 2021. OpenReview.net, 2021.\nURL https://openreview.net/forum?id=qrwe7XHTmYb.\n39\n\nY. Leviathan, M. Kalman, and Y. Matias. Fast inference from transformers via speculative\ndecoding. In International Conference on Machine Learning, ICML 2023, 23-29 July 2023,\nHonolulu, Hawaii, USA, volume 202 of Proceedings of Machine Learning Research, pages\n19274–19286. PMLR, 2023. URL https://proceedings.mlr.press/v202/leviathan23\na.html.\nH. Li, Y. Zhang, F. Koto, Y. Yang, H. Zhao, Y. Gong, N. Duan, and T. Baldwin. CMMLU: Measur-\ning massive multitask language understanding in Chinese. arXiv preprint arXiv:2306.09212,\n2023.\nS. Li and T. Hoefler. Chimera: efficiently training large-scale neural networks with bidirectional\npipelines. In Proceedings of the International Conference for High Performance Computing,\nNetworking, Storage and Analysis, SC ’21, page 1–14. ACM, Nov. 2021. doi: 10.1145/345881\n7.3476145. URL http://dx.doi.org/10.1145/3458817.3476145.\nT. Li, W.-L. Chiang, E. Frick, L. Dunlap, T. Wu, B. Zhu, J. E. Gonzalez, and I. Stoica. From\ncrowdsourced data to high-quality benchmarks: Arena-hard and benchbuilder pipeline. arXiv\npreprint arXiv:2406.11939, 2024a.\nW. Li, F. Qi, M. Sun, X. Yi, and J. Zhang. Ccpm: A chinese classical poetry matching dataset,\n2021.\nY. Li, F. Wei, C. Zhang, and H. Zhang. EAGLE: speculative sampling requires rethinking\nfeature uncertainty. In Forty-first International Conference on Machine Learning, ICML 2024,\nVienna, Austria, July 21-27, 2024. OpenReview.net, 2024b. URL https://openreview.net\n/forum?id=1NdN7eXyb4.\nB. Y. Lin. ZeroEval: A Unified Framework for Evaluating Language Models, July 2024. URL\nhttps://github.com/WildEval/ZeroEval.\nI. Loshchilov and F. Hutter.\nDecoupled weight decay regularization.\narXiv preprint\narXiv:1711.05101, 2017.\nS. Lundberg. The art of prompt design: Prompt boundaries and token healing, 2023. URL\nhttps://towardsdatascience.com/the-art-of-prompt-design-prompt-bound\naries-and-token-healing-3b2448b0be38.\nY. Luo, Z. Zhang, R. Wu, H. Liu, Y. Jin, K. Zheng, M. Wang, Z. He, G. Hu, L. Chen, et al. Ascend\nHiFloat8 format for deep learning. arXiv preprint arXiv:2409.16626, 2024.\nMAA.\nAmerican invitational mathematics examination - aime.\nIn American Invitational\nMathematics Examination - AIME 2024, February 2024. URL https://maa.org/math\n-competitions/american-invitational-mathematics-examination-aime.\nP. Micikevicius, D. Stosic, N. Burgess, M. Cornea, P. Dubey, R. Grisenthwaite, S. Ha, A. Heinecke,\nP. Judd, J. Kamalu, et al. FP8 formats for deep learning. arXiv preprint arXiv:2209.05433, 2022.\nMistral. Cheaper, better, faster, stronger: Continuing to push the frontier of ai and making it\naccessible to all, 2024. URL https://mistral.ai/news/mixtral-8x22b.\nS. Narang, G. Diamos, E. Elsen, P. Micikevicius, J. Alben, D. Garcia, B. Ginsburg, M. Hous-\nton, O. Kuchaiev, G. Venkatesh, et al. Mixed precision training. In Int. Conf. on Learning\nRepresentation, 2017.\n40\n\nB. Noune, P. Jones, D. Justus, D. Masters, and C. Luschi. 8-bit numerical formats for deep neural\nnetworks. arXiv preprint arXiv:2206.02915, 2022.\nNVIDIA. Improving network performance of HPC systems using NVIDIA Magnum IO NVSH-\nMEM and GPUDirect Async. https://developer.nvidia.com/blog/improving-net\nwork-performance-of-hpc-systems-using-nvidia-magnum-io-nvshmem-and-g\npudirect-async, 2022.\nNVIDIA. Blackwell architecture. https://www.nvidia.com/en-us/data-center/tech\nnologies/blackwell-architecture/, 2024a.\nNVIDIA. TransformerEngine, 2024b. URL https://github.com/NVIDIA/TransformerE\nngine. Accessed: 2024-11-19.\nOpenAI. Hello GPT-4o, 2024a. URL https://openai.com/index/hello-gpt-4o/.\nOpenAI. Multilingual massive multitask language understanding (mmmlu), 2024b. URL\nhttps://huggingface.co/datasets/openai/MMMLU.\nOpenAI. Introducing SimpleQA, 2024c. URL https://openai.com/index/introducing\n-simpleqa/.\nOpenAI. Introducing SWE-bench verified we’re releasing a human-validated subset of swe-\nbench that more, 2024d. URL https://openai.com/index/introducing-swe-bench\n-verified/.\nB. Peng, J. Quesnelle, H. Fan, and E. Shippole. Yarn: Efficient context window extension of large\nlanguage models. arXiv preprint arXiv:2309.00071, 2023a.\nH. Peng, K. Wu, Y. Wei, G. Zhao, Y. Yang, Z. Liu, Y. Xiong, Z. Yang, B. Ni, J. Hu, et al. FP8-LM:\nTraining FP8 large language models. arXiv preprint arXiv:2310.18313, 2023b.\nP. Qi, X. Wan, G. Huang, and M. Lin.\nZero bubble pipeline parallelism.\narXiv preprint\narXiv:2401.10241, 2023a.\nP. Qi, X. Wan, G. Huang, and M. Lin. Zero bubble pipeline parallelism, 2023b. URL https:\n//arxiv.org/abs/2401.10241.\nQwen. Qwen technical report. arXiv preprint arXiv:2309.16609, 2023.\nQwen. Introducing Qwen1.5, 2024a. URL https://qwenlm.github.io/blog/qwen1.5.\nQwen. Qwen2.5: A party of foundation models, 2024b. URL https://qwenlm.github.io/b\nlog/qwen2.5.\nS. Rajbhandari, J. Rasley, O. Ruwase, and Y. He. Zero: Memory optimizations toward training tril-\nlion parameter models. In SC20: International Conference for High Performance Computing,\nNetworking, Storage and Analysis, pages 1–16. IEEE, 2020.\nD. Rein, B. L. Hou, A. C. Stickland, J. Petty, R. Y. Pang, J. Dirani, J. Michael, and S. R. Bowman.\nGPQA: A graduate-level google-proof q&a benchmark. arXiv preprint arXiv:2311.12022, 2023.\nB. D. Rouhani, R. Zhao, A. More, M. Hall, A. Khodamoradi, S. Deng, D. Choudhary, M. Cornea,\nE. Dellinger, K. Denolf, et al. Microscaling data formats for deep learning. arXiv preprint\narXiv:2310.10537, 2023a.\n41\n\nB. D. Rouhani, R. Zhao, A. More, M. Hall, A. Khodamoradi, S. Deng, D. Choudhary, M. Cornea,\nE. Dellinger, K. Denolf, et al. Microscaling data formats for deep learning. arXiv preprint\narXiv:2310.10537, 2023b.\nK. Sakaguchi, R. L. Bras, C. Bhagavatula, and Y. Choi. Winogrande: An adversarial winograd\nschema challenge at scale, 2019.\nZ. Shao, P. Wang, Q. Zhu, R. Xu, J. Song, M. Zhang, Y. Li, Y. Wu, and D. Guo. Deepseekmath:\nPushing the limits of mathematical reasoning in open language models. arXiv preprint\narXiv:2402.03300, 2024.\nN. Shazeer, A. Mirhoseini, K. Maziarz, A. Davis, Q. V. Le, G. E. Hinton, and J. Dean. Outrageously\nlarge neural networks: The sparsely-gated mixture-of-experts layer. In 5th International\nConference on Learning Representations, ICLR 2017. OpenReview.net, 2017. URL https:\n//openreview.net/forum?id=B1ckMDqlg.\nF. Shi, M. Suzgun, M. Freitag, X. Wang, S. Srivats, S. Vosoughi, H. W. Chung, Y. Tay, S. Ruder,\nD. Zhou, D. Das, and J. Wei. Language models are multilingual chain-of-thought reasoners.\nIn The Eleventh International Conference on Learning Representations, ICLR 2023, Kigali,\nRwanda, May 1-5, 2023. OpenReview.net, 2023. URL https://openreview.net/forum?i\nd=fR3wGCk-IXp.\nY. Shibata, T. Kida, S. Fukamachi, M. Takeda, A. Shinohara, T. Shinohara, and S. Arikawa. Byte\npair encoding: A text compression scheme that accelerates pattern matching. 1999.\nJ. Su, M. Ahmed, Y. Lu, S. Pan, W. Bo, and Y. Liu. Roformer: Enhanced transformer with rotary\nposition embedding. Neurocomputing, 568:127063, 2024.\nK. Sun, D. Yu, D. Yu, and C. Cardie. Investigating prior knowledge for challenging chinese\nmachine reading comprehension, 2019a.\nM. Sun, X. Chen, J. Z. Kolter, and Z. Liu. Massive activations in large language models. arXiv\npreprint arXiv:2402.17762, 2024.\nX. Sun, J. Choi, C.-Y. Chen, N. Wang, S. Venkataramani, V. V. Srinivasan, X. Cui, W. Zhang, and\nK. Gopalakrishnan. Hybrid 8-bit floating point (HFP8) training and inference for deep neural\nnetworks. Advances in neural information processing systems, 32, 2019b.\nM. Suzgun, N. Scales, N. Schärli, S. Gehrmann, Y. Tay, H. W. Chung, A. Chowdhery, Q. V. Le,\nE. H. Chi, D. Zhou, et al. Challenging big-bench tasks and whether chain-of-thought can solve\nthem. arXiv preprint arXiv:2210.09261, 2022.\nV. Thakkar, P. Ramani, C. Cecka, A. Shivam, H. Lu, E. Yan, J. Kosaian, M. Hoemmen, H. Wu,\nA. Kerr, M. Nicely, D. Merrill, D. Blasig, F. Qiao, P. Majcher, P. Springer, M. Hohnerbach,\nJ. Wang, and M. Gupta. CUTLASS, Jan. 2023. URL https://github.com/NVIDIA/cutlas\ns.\nH. Touvron, T. Lavril, G. Izacard, X. Martinet, M.-A. Lachaux, T. Lacroix, B. Rozière, N. Goyal,\nE. Hambro, F. Azhar, et al. LLaMA: Open and efficient foundation language models. arXiv\npreprint arXiv:2302.13971, 2023a.\nH. Touvron, L. Martin, K. Stone, P. Albert, A. Almahairi, Y. Babaei, N. Bashlykov, S. Batra,\nP. Bhargava, S. Bhosale, D. Bikel, L. Blecher, C. Canton-Ferrer, M. Chen, G. Cucurull, D. Esiobu,\nJ. Fernandes, J. Fu, W. Fu, B. Fuller, C. Gao, V. Goswami, N. Goyal, A. Hartshorn, S. Hosseini,\n42\n\nR. Hou, H. Inan, M. Kardas, V. Kerkez, M. Khabsa, I. Kloumann, A. Korenev, P. S. Koura,\nM. Lachaux, T. Lavril, J. Lee, D. Liskovich, Y. Lu, Y. Mao, X. Martinet, T. Mihaylov, P. Mishra,\nI. Molybog, Y. Nie, A. Poulton, J. Reizenstein, R. Rungta, K. Saladi, A. Schelten, R. Silva, E. M.\nSmith, R. Subramanian, X. E. Tan, B. Tang, R. Taylor, A. Williams, J. X. Kuan, P. Xu, Z. Yan,\nI. Zarov, Y. Zhang, A. Fan, M. Kambadur, S. Narang, A. Rodriguez, R. Stojnic, S. Edunov, and\nT. Scialom. Llama 2: Open foundation and fine-tuned chat models. CoRR, abs/2307.09288,\n2023b. doi: 10.48550/arXiv.2307.09288. URL https://doi.org/10.48550/arXiv.2307.\n09288.\nA. Vaswani, N. Shazeer, N. Parmar, J. Uszkoreit, L. Jones, A. N. Gomez, Ł. Kaiser, and I. Polo-\nsukhin. Attention is all you need. Advances in neural information processing systems, 30,\n2017.\nL. Wang, H. Gao, C. Zhao, X. Sun, and D. Dai. Auxiliary-loss-free load balancing strategy for\nmixture-of-experts. CoRR, abs/2408.15664, 2024a. URL https://doi.org/10.48550/arX\niv.2408.15664.\nY. Wang, X. Ma, G. Zhang, Y. Ni, A. Chandra, S. Guo, W. Ren, A. Arulraj, X. He, Z. Jiang, T. Li,\nM. Ku, K. Wang, A. Zhuang, R. Fan, X. Yue, and W. Chen. Mmlu-pro: A more robust and\nchallenging multi-task language understanding benchmark. CoRR, abs/2406.01574, 2024b.\nURL https://doi.org/10.48550/arXiv.2406.01574.\nT. Wei, J. Luan, W. Liu, S. Dong, and B. Wang. Cmath: Can your language model pass chinese\nelementary school math test?, 2023.\nM. Wortsman, T. Dettmers, L. Zettlemoyer, A. Morcos, A. Farhadi, and L. Schmidt. Stable\nand low-precision training for large-scale vision-language models. Advances in Neural\nInformation Processing Systems, 36:10271–10298, 2023.\nH. Xi, C. Li, J. Chen, and J. Zhu. Training transformers with 4-bit integers. Advances in Neural\nInformation Processing Systems, 36:49146–49168, 2023.\nC. S. Xia, Y. Deng, S. Dunn, and L. Zhang.\nAgentless: Demystifying llm-based software\nengineering agents. arXiv preprint, 2024.\nH. Xia, T. Ge, P. Wang, S. Chen, F. Wei, and Z. Sui. Speculative decoding: Exploiting spec-\nulative execution for accelerating seq2seq generation. In Findings of the Association for\nComputational Linguistics: EMNLP 2023, Singapore, December 6-10, 2023, pages 3909–3925.\nAssociation for Computational Linguistics, 2023. URL https://doi.org/10.18653/v1/\n2023.findings-emnlp.257.\nG. Xiao, J. Lin, M. Seznec, H. Wu, J. Demouth, and S. Han. Smoothquant: Accurate and efficient\npost-training quantization for large language models. In International Conference on Machine\nLearning, pages 38087–38099. PMLR, 2023.\nL. Xu, H. Hu, X. Zhang, L. Li, C. Cao, Y. Li, Y. Xu, K. Sun, D. Yu, C. Yu, Y. Tian, Q. Dong, W. Liu,\nB. Shi, Y. Cui, J. Li, J. Zeng, R. Wang, W. Xie, Y. Li, Y. Patterson, Z. Tian, Y. Zhang, H. Zhou,\nS. Liu, Z. Zhao, Q. Zhao, C. Yue, X. Zhang, Z. Yang, K. Richardson, and Z. Lan. CLUE: A chi-\nnese language understanding evaluation benchmark. In D. Scott, N. Bel, and C. Zong, editors,\nProceedings of the 28th International Conference on Computational Linguistics, COLING\n2020, Barcelona, Spain (Online), December 8-13, 2020, pages 4762–4772. International Com-\nmittee on Computational Linguistics, 2020. doi: 10.18653/V1/2020.COLING-MAIN.419. URL\nhttps://doi.org/10.18653/v1/2020.coling-main.419.\n43\n\nR. Zellers, A. Holtzman, Y. Bisk, A. Farhadi, and Y. Choi. HellaSwag: Can a machine really finish\nyour sentence? In A. Korhonen, D. R. Traum, and L. Màrquez, editors, Proceedings of the 57th\nConference of the Association for Computational Linguistics, ACL 2019, Florence, Italy, July\n28- August 2, 2019, Volume 1: Long Papers, pages 4791–4800. Association for Computational\nLinguistics, 2019. doi: 10.18653/v1/p19-1472. URL https://doi.org/10.18653/v1/p1\n9-1472.\nW. Zhong, R. Cui, Y. Guo, Y. Liang, S. Lu, Y. Wang, A. Saied, W. Chen, and N. Duan. AGIEval: A\nhuman-centric benchmark for evaluating foundation models. CoRR, abs/2304.06364, 2023.\ndoi: 10.48550/arXiv.2304.06364. URL https://doi.org/10.48550/arXiv.2304.06364.\nJ. Zhou, T. Lu, S. Mishra, S. Brahma, S. Basu, Y. Luan, D. Zhou, and L. Hou. Instruction-following\nevaluation for large language models. arXiv preprint arXiv:2311.07911, 2023.\n44\n\nAppendix\nA. Contributions and Acknowledgments\nResearch & Engineering\nAixin Liu\nBing Xue\nBingxuan Wang\nBochao Wu\nChengda Lu\nChenggang Zhao\nChengqi Deng\nChenyu Zhang*\nChong Ruan\nDamai Dai\nDaya Guo\nDejian Yang\nDeli Chen\nErhang Li\nFangyun Lin\nFucong Dai\nFuli Luo*\nGuangbo Hao\nGuanting Chen\nGuowei Li\nH. Zhang\nHan Bao*\nHanwei Xu\nHaocheng Wang*\nHaowei Zhang\nHonghui Ding\nHuajian Xin*\nHuazuo Gao\nHui Qu\nJianzhong Guo\nJiashi Li\nJiawei Wang*\nJingchang Chen\nJingyang Yuan\nJunjie Qiu\nJunlong Li\nJunxiao Song\nKai Dong\nKai Hu*\nKaige Gao\nKang Guan\nKexin Huang\nKuai Yu\nLean Wang\nLecong Zhang\nLiang Zhao\nLitong Wang\nLiyue Zhang\nMingchuan Zhang\nMinghua Zhang\nMinghui Tang\nPanpan Huang\nPeiyi Wang\nQiancheng Wang\nQihao Zhu\nQinyu Chen\nQiushi Du\nRuiqi Ge\nRuisong Zhang\nRuizhe Pan\nRunji Wang\nRunxin Xu\nRuoyu Zhang\nShanghao Lu\nShangyan Zhou\nShanhuang Chen\nShengfeng Ye\nShirong Ma\nShiyu Wang\nShuiping Yu\nShunfeng Zhou\nShuting Pan\nTao Yun\nTian Pei\nWangding Zeng\nWanjia Zhao*\nWen Liu\nWenfeng Liang\nWenjun Gao\nWenqin Yu\nWentao Zhang\nXiao Bi\nXiaodong Liu\nXiaohan Wang\nXiaokang Chen\nXiaokang Zhang\nXiaotao Nie\nXin Cheng\nXin Liu\n45\n\nXin Xie\nXingchao Liu\nXingkai Yu\nXinyu Yang\nXinyuan Li\nXuecheng Su\nXuheng Lin\nY.K. Li\nY.Q. Wang\nY.X. Wei\nYang Zhang\nYanhong Xu\nYao Li\nYao Zhao\nYaofeng Sun\nYaohui Wang\nYi Yu\nYichao Zhang\nYifan Shi\nYiliang Xiong\nYing He\nYishi Piao\nYisong Wang\nYixuan Tan\nYiyang Ma*\nYiyuan Liu\nYongqiang Guo\nYu Wu\nYuan Ou\nYuduan Wang\nYue Gong\nYuheng Zou\nYujia He\nYunfan Xiong\nYuxiang Luo\nYuxiang You\nYuxuan Liu\nYuyang Zhou\nZ.F. Wu\nZ.Z. Ren\nZehui Ren\nZhangli Sha\nZhe Fu\nZhean Xu\nZhenda Xie\nZhengyan Zhang\nZhewen Hao\nZhibin Gou\nZhicheng Ma\nZhigang Yan\nZhihong Shao\nZhiyu Wu\nZhuoshu Li\nZihui Gu\nZijia Zhu\nZijun Liu*\nZilin Li\nZiwei Xie\nZiyang Song\nZiyi Gao\nZizheng Pan\nData Annotation\nBei Feng\nHui Li\nJ.L. Cai\nJiaqi Ni\nLei Xu\nMeng Li\nNing Tian\nR.J. Chen\nR.L. Jin\nRuyi Chen\nS.S. Li\nShuang Zhou\nTianyu Sun\nX.Q. Li\nXiangyue Jin\nXiaojin Shen\nXiaosha Chen\nXiaowen Sun\nXiaoxiang Wang\nXinnan Song\nXinyi Zhou\nY.X. Zhu\nYanhong Xu\nYanping Huang\nYaohui Li\nYi Zheng\nYuchen Zhu\nYunxian Ma\nZhen Huang\nZhipeng Xu\nZhongyu Zhang\nBusiness & Compliance\nDongjie Ji\n46\n\nJian Liang\nJin Chen\nLeyi Xia\nMiaojun Wang\nMingming Li\nPeng Zhang\nShaoqing Wu\nShengfeng Ye\nT. Wang\nW.L. Xiao\nWei An\nXianzu Wang\nXinxia Shan\nYing Tang\nYukun Zha\nYuting Yan\nZhen Zhang\nWithin each role, authors are listed alphabetically by the first name. Names marked with *\ndenote individuals who have departed from our team.\nB. Ablation Studies for Low-Precision Training\nFigure 10 | Loss curves comparison between BF16 and FP8 training. Results are smoothed by\nExponential Moving Average (EMA) with a coefficient of 0.9.\nB.1. FP8 v.s. BF16 Training\nWe validate our FP8 mixed precision framework with a comparison to BF16 training on top of\ntwo baseline models across different scales. At the small scale, we train a baseline MoE model\ncomprising approximately 16B total parameters on 1.33T tokens. At the large scale, we train a\nbaseline MoE model comprising approximately 230B total parameters on around 0.9T tokens.\nWe show the training curves in Figure 10 and demonstrate that the relative error remains below\n0.25% with our high-precision accumulation and fine-grained quantization strategies.\nB.2. Discussion About Block-Wise Quantization\nAlthough our tile-wise fine-grained quantization effectively mitigates the error introduced\nby feature outliers, it requires different groupings for activation quantization, i.e., 1x128 in\nforward pass and 128x1 for backward pass. A similar process is also required for the activation\ngradient. A straightforward strategy is to apply block-wise quantization per 128x128 elements\nlike the way we quantize the model weights. In this way, only transposition is required for\nbackward. Therefore, we conduct an experiment where all tensors associated with Dgrad are\nquantized on a block-wise basis. The results reveal that the Dgrad operation which computes\nthe activation gradients and back-propagates to shallow layers in a chain-like manner, is highly\nsensitive to precision. Specifically, block-wise quantization of activation gradients leads to\n47\n\nmodel divergence on an MoE model comprising approximately 16B total parameters, trained for\naround 300B tokens. We hypothesize that this sensitivity arises because activation gradients are\nhighly imbalanced among tokens, resulting in token-correlated outliers (Xi et al., 2023). These\noutliers cannot be effectively managed by a block-wise quantization approach.\nC. Expert Specialization Patterns of the 16B Aux-Loss-Based and Aux-Loss-\nFree Models\nWe record the expert load of the 16B auxiliary-loss-based baseline and the auxiliary-loss-free\nmodel on the Pile test set. The auxiliary-loss-free model tends to have greater expert specializa-\ntion across all layers, as demonstrated in Figure 10.\n48\n\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64\nWikipedia (en)\nGithub\nDM Mathematics\nAux-Loss-Based Layer 1\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64\nWikipedia (en)\nGithub\nDM Mathematics\nAux-Loss-Free Layer 1\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64\nWikipedia (en)\nGithub\nDM Mathematics\nAux-Loss-Based Layer 2\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64\nWikipedia (en)\nGithub\nDM Mathematics\nAux-Loss-Free Layer 2\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64\nWikipedia (en)\nGithub\nDM Mathematics\nAux-Loss-Based Layer 3\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64\nWikipedia (en)\nGithub\nDM Mathematics\nAux-Loss-Free Layer 3\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64\nWikipedia (en)\nGithub\nDM Mathematics\nAux-Loss-Based Layer 4\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64\nWikipedia (en)\nGithub\nDM Mathematics\nAux-Loss-Free Layer 4\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64\nWikipedia (en)\nGithub\nDM Mathematics\nAux-Loss-Based Layer 5\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64\nWikipedia (en)\nGithub\nDM Mathematics\nAux-Loss-Free Layer 5\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64\nWikipedia (en)\nGithub\nDM Mathematics\nAux-Loss-Based Layer 6\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64\nWikipedia (en)\nGithub\nDM Mathematics\nAux-Loss-Free Layer 6\n0\n2\n4\n6\n8\n10\nRelative Expert Load\n(a) Layers 1-7\n49\n\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64\nWikipedia (en)\nGithub\nDM Mathematics\nAux-Loss-Based Layer 7\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64\nWikipedia (en)\nGithub\nDM Mathematics\nAux-Loss-Free Layer 7\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64\nWikipedia (en)\nGithub\nDM Mathematics\nAux-Loss-Based Layer 8\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64\nWikipedia (en)\nGithub\nDM Mathematics\nAux-Loss-Free Layer 8\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64\nWikipedia (en)\nGithub\nDM Mathematics\nAux-Loss-Based Layer 9\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64\nWikipedia (en)\nGithub\nDM Mathematics\nAux-Loss-Free Layer 9\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64\nWikipedia (en)\nGithub\nDM Mathematics\nAux-Loss-Based Layer 10\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64\nWikipedia (en)\nGithub\nDM Mathematics\nAux-Loss-Free Layer 10\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64\nWikipedia (en)\nGithub\nDM Mathematics\nAux-Loss-Based Layer 11\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64\nWikipedia (en)\nGithub\nDM Mathematics\nAux-Loss-Free Layer 11\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64\nWikipedia (en)\nGithub\nDM Mathematics\nAux-Loss-Based Layer 12\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64\nWikipedia (en)\nGithub\nDM Mathematics\nAux-Loss-Free Layer 12\n0\n2\n4\n6\n8\n10\nRelative Expert Load\n(b) Layers 7-13\n50\n\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64\nWikipedia (en)\nGithub\nDM Mathematics\nAux-Loss-Based Layer 13\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64\nWikipedia (en)\nGithub\nDM Mathematics\nAux-Loss-Free Layer 13\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64\nWikipedia (en)\nGithub\nDM Mathematics\nAux-Loss-Based Layer 14\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64\nWikipedia (en)\nGithub\nDM Mathematics\nAux-Loss-Free Layer 14\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64\nWikipedia (en)\nGithub\nDM Mathematics\nAux-Loss-Based Layer 15\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64\nWikipedia (en)\nGithub\nDM Mathematics\nAux-Loss-Free Layer 15\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64\nWikipedia (en)\nGithub\nDM Mathematics\nAux-Loss-Based Layer 16\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64\nWikipedia (en)\nGithub\nDM Mathematics\nAux-Loss-Free Layer 16\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64\nWikipedia (en)\nGithub\nDM Mathematics\nAux-Loss-Based Layer 17\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64\nWikipedia (en)\nGithub\nDM Mathematics\nAux-Loss-Free Layer 17\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64\nWikipedia (en)\nGithub\nDM Mathematics\nAux-Loss-Based Layer 18\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64\nWikipedia (en)\nGithub\nDM Mathematics\nAux-Loss-Free Layer 18\n0\n2\n4\n6\n8\n10\nRelative Expert Load\n(c) Layers 13-19\n51\n\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64\nWikipedia (en)\nGithub\nDM Mathematics\nAux-Loss-Based Layer 19\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64\nWikipedia (en)\nGithub\nDM Mathematics\nAux-Loss-Free Layer 19\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64\nWikipedia (en)\nGithub\nDM Mathematics\nAux-Loss-Based Layer 20\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64\nWikipedia (en)\nGithub\nDM Mathematics\nAux-Loss-Free Layer 20\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64\nWikipedia (en)\nGithub\nDM Mathematics\nAux-Loss-Based Layer 21\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64\nWikipedia (en)\nGithub\nDM Mathematics\nAux-Loss-Free Layer 21\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64\nWikipedia (en)\nGithub\nDM Mathematics\nAux-Loss-Based Layer 22\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64\nWikipedia (en)\nGithub\nDM Mathematics\nAux-Loss-Free Layer 22\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64\nWikipedia (en)\nGithub\nDM Mathematics\nAux-Loss-Based Layer 23\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64\nWikipedia (en)\nGithub\nDM Mathematics\nAux-Loss-Free Layer 23\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64\nWikipedia (en)\nGithub\nDM Mathematics\nAux-Loss-Based Layer 24\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64\nWikipedia (en)\nGithub\nDM Mathematics\nAux-Loss-Free Layer 24\n0\n2\n4\n6\n8\n10\nRelative Expert Load\n(d) Layers 19-25\n52\n\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64\nWikipedia (en)\nGithub\nDM Mathematics\nAux-Loss-Based Layer 25\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64\nWikipedia (en)\nGithub\nDM Mathematics\nAux-Loss-Free Layer 25\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64\nWikipedia (en)\nGithub\nDM Mathematics\nAux-Loss-Based Layer 26\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64\nWikipedia (en)\nGithub\nDM Mathematics\nAux-Loss-Free Layer 26\n0\n2\n4\n6\n8\n10\nRelative Expert Load\n(e) Layers 25-27\nFigure 10 | Expert load of auxiliary-loss-free and auxiliary-loss-based models on three domains\nin the Pile test set. The auxiliary-loss-free model shows greater expert specialization patterns\nthan the auxiliary-loss-based one. The relative expert load denotes the ratio between the actual\nexpert load and the theoretically balanced expert load.\n53\n",
      "fetch_method": "direct-pdf"
    },
    {
      "arxiv_id": "https://lmsys.org/blog/2024-09-04-sglang-v0-3/#deepseek-multi-head-latent-attention-mla-throughput-optimizations",
      "full_text": " SGLang v0.3 Release: 7x Faster DeepSeek MLA, 1.5x Faster torch.compile, Multi-Image/Video LLaVA-OneVision | LMSYS Org LMSYS ORG Projects Blog About Donations Chatbot Arena (graduated) Open Menu Projects Blog About Donations Chatbot Arena (graduated) Close Menu SGLang v0.3 Release: 7x Faster DeepSeek MLA, 1.5x Faster torch.compile, Multi-Image/Video LLaVA-OneVision by: The SGLang Team , Sep 04, 2024 We're excited to announce the release of SGLang v0.3 , which brings significant performance enhancements and expanded support for novel model architectures. Here are the key updates: Up to 7x higher throughput for DeepSeek Multi-head Latent Attention (MLA) Up to 1.5x lower latency with torch.compile on small batch sizes Support for interleaved text and multi-image/video in LLaVA-OneVision Support for interleaved window attention and 2x longer context length in Gemma-2 In this blog post, we'll walk you through these key features. Please do not hesitate to report any issues or contribute ideas and code. DeepSeek Multi-head Latent Attention (MLA) Throughput Optimizations Multi-head Latent Attention (MLA) is a new attention variant introduced by the DeepSeek team to improve inference efficiency. Due to its differences from standard attention mechanisms, existing open-source libraries have not fully optimized this operation. In SGLang v0.3, we implemented various optimizations for MLA, including weight absorption, grouped decoding kernels, FP8 batched MatMul, and FP8 KV cache quantization. Benchmark results show that SGLang v0.3 with MLA optimizations achieves 3x to 7x higher throughput than the baseline system. The benchmark measures the peak output throughput of these models with BF16 and FP8 on H100 GPUs (tensor-parallelism=1 for lite models and tensor-parallelism=8 for big models) on the ShareGPT datasets. Reproducible instructions are in the appendix. While encouraging, there is still much room for improvement. We are actively working on more optimizations to fully reproduce the results from the DeepSeek paper. Related PRs: #905 , #1060 , #1138 , #469 , #1285 , #1286 . Torch.compile Latency Optimizations Torch.compile is a major feature of PyTorch 2.0. On NVIDIA GPUs, it performs aggressive fusion and generates highly efficient Triton kernels. We've integrated torch.compile into SGLang for linear/norm/activation layers, combining it with FlashInfer attention and sampling kernels. We turn on torch.compile for batch sizes 1 to 32, where we observed the most acceleration. With this combination, SGLang is faster than gpt-fast at batch size 1 and supports all online serving features, including continuous batching and RadixAttention for prefix caching. We are actively collaborating with the torch.compile and torchao teams to incorporate their latest optimizations into SGLang. To use torch.compile in SGLang, add --enable-torch-compile when launching the server. SGLang w/ torch.compile yields up to a 1.5x speedup in the following benchmark. Reproducible instructions are in the appendix. LLaVA-OneVision Support with Interleaved Text, Multi-Image, and Video LLaVA-OneVision is the first open model to achieve state-of-the-art performance in three important computer vision scenarios: single-image, multi-image, and video tasks. We collaborated with the LLaVA team to integrate these capabilities into SGLang v0.3. You can launch a server and query it using the OpenAI-compatible vision API, which supports interleaved text, multi-image, and video formats. Usage details are available here . The authors validated the model's accuracy and reported benchmark results on the VideoDetailDescriptions and LLaVA-in-the-wild datasets (see #1123 ). SGLang archives up to 4.5x speedup than the authors’ original implementation in HuggingFace/transformers. Gemma-2 Support with Interleaved Window Attention Google's Gemma-2 model uses interleaved window attention to reduce computational complexity for long contexts, alternating between local sliding window attention (4K context length) and global attention (8K context length) in every other layer. We enhanced SGLang v0.3 to fully support the 8K context length by leveraging the optimized window attention kernel from FlashInfer kernels (which skips computation instead of masking) and refining our KV cache manager. Other libraries that lack this feature can only run with a 4K context length. You can launch the model with python3 -m sglang .launch_server --model-path google/gemma- 2 b Acknowledgment The DeepSeek MLA optimizations were contributed by Ke Bao and Yineng Zhang. The torch.compile optimizations were contributed by Liangsheng Yin. The LLaVA-OneVision contributions were made by Kaichen Zhang and Bo Li. The interleaved window attention was contributed by Ying Sheng. We also thank all 90+ open-source contributors . Appendix Benchmark Instructions for DeepSeek MLA # DeepSeekCoder-V2-Lite (BF16) ## Launch a server python3 - m sglang . launch_server --model deepseek-ai / DeepSeek-Coder-V2-Lite-Instruct --enable-mla --disable-radix --trust-remote-code python3 - m vllm . entrypoints . openai . api_server --model deepseek-ai / DeepSeek-Coder-V2-Lite-Instruct --disable-log-requests --trust-remote-code --max-model-len 4096 ## Run benchmark python3 - m sglang . bench_serving --backend sglang --num-prompts 5000 python3 - m sglang . bench_serving --backend vllm --num-prompts 5000 # DeepSeekCoder-V2 (BF16) ## Launch a server python3 - m sglang . launch_server --model deepseek-ai / DeepSeek-Coder-V2-Instruct --disable-radix --tp 8 --trust-remote-code --enable-mla python3 - m vllm . entrypoints . openai . api_server --model deepseek-ai / DeepSeek-Coder-V2-Instruct --disable-log-requests --tensor-parallel-size 8 --trust-remote-code --max-model-len 4096 ## Run benchmark python3 - m sglang . bench_serving --backend sglang --num-prompts 5000 python3 - m sglang . bench_serving --backend vllm --num-prompts 5000 # DeepSeekCoder-V2 (FP8) ## Launch a server python3 - m sglang . launch_server --model neuralmagic / DeepSeek-Coder-V2-Instruct-FP8 --enable-mla --quantization fp8 --kv-cache-dtype fp8_e5m2 --disable-radix --tp 8 --trust-remote-code python3 - m vllm . entrypoints . openai . api_server --model neuralmagic / DeepSeek-Coder-V2-Instruct-FP8 --quantization fp8 --disable-log-requests --tensor-parallel-size 8 --trust-remote-code --max-model-len 4096 ## Run benchmark python3 - m sglang . bench_serving --backend sglang --num-prompts 5000 python3 - m sglang . bench_serving --backend vllm --num-prompts 5000 Benchmark Instructions for torch.compile # SGLang ## Launch a server python3 - m sglang . launch_server --model meta-llama / Meta-Llama-3-8B --enable-torch-compile ## Run benchmark python3 - m sglang . bench_serving --backend sglang --dataset-name random --random-input-len 128 --random-output-len 512 --random-range-ratio 1 --num-prompts 1 # vLLM ## Launch a server python3 - m vllm . entrypoints . openai . api_server --model meta-llama / Meta-Llama-3-8B --disable-log-requests ## Run benchmark python3 - m sglang . bench_serving --backend vllm --dataset-name random --random-input-len 128 --random-output-len 512 --random-range-ratio 1 --num-prompts 1 ",
      "fetch_method": "direct-html"
    },
    {
      "arxiv_id": "https://lmsys.org/blog/2024-12-04-sglang-v0-4/#data-parallelism-attention-for-deepseek-models",
      "full_text": " SGLang v0.4: Zero-Overhead Batch Scheduler, Cache-Aware Load Balancer, Faster Structured Outputs | LMSYS Org LMSYS ORG Projects Blog About Donations Chatbot Arena (graduated) Open Menu Projects Blog About Donations Chatbot Arena (graduated) Close Menu SGLang v0.4: Zero-Overhead Batch Scheduler, Cache-Aware Load Balancer, Faster Structured Outputs by: The SGLang Team , Dec 04, 2024 We’re excited to release SGLang v0.4 , featuring significant performance improvements and new features: Zero-overhead batch scheduler: 1.1x increase in throughput. Cache-aware load balancer: up to 1.9x increase in throughput with 3.8x higher cache hit rate. Data parallelism attention for DeepSeek models: up to 1.9x decoding throughput improvement. Fast structured outputs with xgrammar: up to 10x faster. This blog provides a walkthrough of these updates. We welcome your feedback and contributions! Zero-Overhead Batch Scheduler While LLM inference runs on GPUs, there is substantial work that also needs to be done by the CPU, such as batch scheduling, memory allocation, and prefix matching. An unoptimized inference engine can spend as much as half of its time on CPU overhead . SGLang has been known for its efficient batch scheduler from the start. In this new version, we pushed it to the extreme and achieved a near zero-overhead batch scheduler. This idea is simple and has been proposed in NanoFlow . Basically, we can overlap the CPU scheduling with the GPU computation. The scheduler runs one batch ahead and prepares all the metadata required for the next batch. By doing this, we can keep the GPUs always busy and hide expensive overheads such as the radix cache operations. The related code is here . The implementation details involve resolving dependencies by creating future tokens and carefully scheduling CUDA events and synchronization. Below is an illustration of the overlapped CPU scheduler and GPU worker. We verified the zero-overhead claim by using the Nsight profiling system. In the figure below, there are 5 consecutive decoding batches, and you can see there is no single idle time on the GPU. (NOTE: This profile is obtained with the Triton attention backend; there is still a minor gap with the FlashInfer backend, which will be resolved in the next FlashInfer release.) With this optimization, SGLang v0.4 can now squeeze the last bit of performance from the GPU and achieves a 1.1x speedup against its previous version and a 1.3x speedup against other state-of-the-art baselines. The speedup is most significant on small models and large tensor parallelism sizes. Usage : It is turned on by default, so you do not need to change anything! Reproduce benchmark : # zero-overhead batch scheduler (v0.4) python3 - m sglang . launch_server --model meta-llama / Llama-3 . 2-3B-Instruct python3 - m sglang . bench_serving --backend sglang --dataset-name random --num-prompts 500 --random-input 4096 --random-output 2048 # old batch scheduler (v0.3) python3 - m sglang . launch_server --model meta-llama / Llama-3 . 2-3B-Instruct --disable-overlap python3 - m sglang . bench_serving --backend sglang --dataset-name random --num-prompts 500 --random-input 4096 --random-output 2048 Cache-Aware Load Balancer SGLang v0.4 introduces a cache-aware load balancer for LLM inference engines. The load balancer predicts prefix KV cache hit rates on workers and selects those with the highest match rates. Testing shows a up to 1.9x throughput increase and 3.8x hit rate improvement , with benefits scaling as worker count increases. The figure below shows how a cache-aware load balancer is different from a naive round-robin load balancer for data parallelism. The cache-aware load balancer maintains an approximate radix tree of the actual radix tree on the workers. The tree is lazily updated with almost no overhead. Here are some benchmark results. The new cache-aware router significantly improves throughput. SGLang v0.3 SGLang v0.4 Throughput (token/s) 82665 158596 Cache hit rate 20% 75% The benchmark is conducted on a workload that has multiple long prefix groups, and each group is perfectly balanced. The performance might vary based on the characteristics of the workload, but it should improve the cache hit rate significantly The key features of this router includes Multi-Node Support : Deploy workers across multiple machines, connect a single router to distributed workers, allowing for easy horizontal scaling while preserving cache awareness in a distributed setup. Cache-Aware Routing : Requests are sent to workers with a higher hit rate, and load balancing is performed to avoid imbalance. Communication-Free Design : No worker synchronization is required for cache state; instead, it uses passed information to simulate an &quot;approximate tree&quot;. High-Performance Implementation : Built in pure Rust for high concurrency, with a low overhead design, offering a 2x speedup compared to Python-based alternatives. Standalone Package : Published as &quot;sglang-router&quot;, includes Python bindings, and features a CLI interface for easy usage. Usage Installation: pip install sglang-router Co-launch Workers and Router Drop-in replacement for existing --dp-size parameter: python -m sglang_router.launch_server \\ --model-path meta-llama/Meta-Llama- 3 . 1 - 8 B-Instruct \\ --dp-size 8 Router-Only Launch Ideal for multi-node distributed processing: python -m sglang_router.launch_router \\ --worker-urls http://worker1: 8000 http://worker2: 8000 Reproduce benchmark: # Hardware: 8x A100 80GB GPUs # Run benchmark python bench_serving.py \\ --host 127.0.0.1 \\ --port 30000 \\ --dataset-name generated-shared-prefix # Launch with router python -m sglang_router.launch_server \\ --model-path meta-llama/Meta-Llama- 3 . 1 - 8 B-Instruct \\ --dp-size 8 # Launch without router (baseline) python -m sglang.launch_server \\ --model-path meta-llama/Meta-Llama- 3 . 1 - 8 B-Instruct \\ --dp-size 8 Learn more by reading the deep dive thread . There is also a related paper (with a different design and implementation), Preble , which is also built on top of SGLang. Data Parallelism Attention For DeepSeek Models The most common parallelism strategy for inference is tensor parallelism. However, it might not be the most efficient strategy for certain models. For example, DeepSeek models use MLA and only have one KV head. If we use tensor parallelism on 8 GPUs, it will lead to duplicated KV cache and unwanted memory usage. To overcome this, we've implemented data parallelism (DP) for the multi-head latent attention (MLA) mechanism to improve throughput for DeepSeek models. By adopting DP for the attention component, the KV cache is significantly reduced, allowing for larger batch sizes. In our DP attention implementation, each DP worker handles different types of batches (prefill, decode, idle) independently. The attention-processed data will be all-gathered among all workers before entering the Mixture-of-Experts (MoE) layer, and after processing through the MoE, the data will be redistributed back to each worker. The figure below illustrates this idea. Here are the benchmark results on 8 x H100 80GB GPUs. With this optimization, SGLang v0.4 achieved 1.9x decoding throughput compared to SGLang v0.3. We are working on further improving the throughput by integrating expert parallelism for the MoE layers. You can check out the related PRs for data parallelism and expert parallelism . Usage: Add --enable-dp-attention option to turn on this feature. Currently, it’s only supported for DeepSeek models. Reproduce benchmark: # Hardware: 8x H100 80GB GPUs # If you see out-of-memory, please try to reduce `--mem-fraction-static` to a smaller value such as 0.75. # SGLang w/ DP attention (v0.4) python3 - m sglang . launch_server --model-path neuralmagic / DeepSeek-Coder-V2-Instruct-FP8 --disable-radix-cache --trust-remote-code --tp 8 --enable-dp-attention --mem-fraction-static 0 . 78 python3 - m sglang . bench_serving --backend sglang --dataset-name random --random-input 1 --random-output 512 --random-range-ratio 1 --num-prompts 10000 # SGLang w/o DP attention (v0.3) python3 - m sglang . launch_server --model-path neuralmagic / DeepSeek-Coder-V2-Instruct-FP8 --disable-radix-cache --trust-remote-code --tp 8 --mem-fraction-static 0 . 78 python3 - m sglang . bench_serving --backend sglang --dataset-name random --random-input 1 --random-output 512 --random-range-ratio 1 --num-prompts 10000 Fast Structured Outputs with XGrammar SGLang has been the fastest inference engine for JSON decoding with its Compressed Finite State Machine . With this new release, it becomes even faster by integrating a faster grammar backend, xgrammar. According to the benchmark results, SGLang + xgrammar can be up to 10x faster than other open-source solutions for JSON decoding tasks . You can learn more in the xgrammar blog post: https://blog.mlc.ai/2024/11/22/achieving-efficient-flexible-portable-structured-generation-with-xgrammar . Usage : Add `--grammar-backend xgrammar` when launching the server. python3 -m sglang.launch_server --model-path meta-llama/Llama- 3 . 1 - 8 B-Instruct --grammar-backend xgrammar You can then query it with the OpenAI-compatible API. See an example at https://sgl-project.github.io/backend/openai_api_completions.html#JSON . Acknowledgment The work in this blog post is mainly contributed by Byron Hsu, Ke Bao, Lianmin Zheng, Yineng Zhang, and Ziyi Xu. We thank Zhiqiang Xie, Liangsheng Yin, Shuo Yang, and Yilong Zhao for their discussions on the zero-overhead scheduler; Ying Sheng, Yichuan Wang, and Shiyi Cao for their discussions on the cache-aware load balancer; Jiashi Li for their discussion on data parallelism attention; and Yixin Dong for the amazing xgrammar library. Roadmap It has been a great year, and we delivered many features following our roadmap . The community is also growing healthily with more developers and adoption. The focus of the next release will be on disaggregated prefill-decode, speculative decoding, multi-level radix cache, sequence parallelism, and more! ",
      "fetch_method": "direct-html"
    },
    {
      "arxiv_id": "https://docs.vllm.ai/en/latest/serving/distributed_serving.html",
      "full_text": " Parallelism and Scaling - vLLM Skip to content You are viewing the latest developer preview docs. Click here to view docs for the latest stable release. vLLM Parallelism and Scaling Initializing search GitHub Home User Guide Developer Guide API Reference CLI Reference Community vLLM GitHub Home User Guide User Guide Getting Started Getting Started Quickstart Installation Installation GPU CPU Google TPU AWS Neuron Examples Examples Offline Inference Offline Inference Async LLM Streaming Audio Language Automatic Prefix Caching Basic Batch LLM Inference Chat With Tools Context Extension Convert Model To Seq Cls Data Parallel Disaggregated Prefill V1 Disaggregated Prefill Dolphin Embed Jina Embeddings V3 Embed Matryoshka Fy Encoder Decoder Encoder Decoder Multimodal LLM Engine Example Load Sharded State LoRA With Quantization Inference Metrics Mistral-Small MLPSpeculator MultiLoRA Inference Offline Inference with the OpenAI Batch file format Prefix Caching Prithvi Geospatial MAE Prithvi Geospatial MAE Io Processor Profiling vLLM TPU Profiling Prompt Embed Inference Qwen2.5-Omni Offline Inference Examples Qwen3 Reranker Qwen 1M Reproducibility RLHF RLHF Colocate RLHF Utils Save Sharded State Simple Profiling Skip Loading Weights In Engine Init Spec Decode Structured Outputs Torchrun Example TPU Vision Language Vision Language Multi Image Vision Language Pooling Online Serving Online Serving API Client Helm Charts Cohere Rerank Client Disaggregated Prefill Disaggregated Serving Gradio OpenAI Chatbot Webserver Gradio Webserver Jinaai Rerank Client Kv Events Subscriber Multi-Node-Serving Multi Instance Data Parallel OpenAI Chat Completion Client OpenAI Chat Completion Client For Multimodal OpenAI Chat Completion Client With Tools OpenAI Chat Completion Client With Tools Required OpenAI Chat Completion Client With Tools Xlam OpenAI Chat Completion Client With Tools Xlam Streaming OpenAI Chat Completion Tool Calls With Reasoning OpenAI Chat Completion With Reasoning OpenAI Chat Completion With Reasoning Streaming OpenAI Chat Embedding Client For Multimodal OpenAI Classification Client OpenAI Completion Client OpenAI Cross Encoder Score OpenAI Cross Encoder Score For Multimodal OpenAI Embedding Client Long Text Embedding with Chunked Processing OpenAI Embedding Matryoshka Fy OpenAI Pooling Client OpenAI Transcription Client OpenAI Translation Client Setup OpenTelemetry POC Prithvi Geospatial MAE Prometheus and Grafana Prompt Embed Inference With OpenAI Client Ray Serve Deepseek Retrieval Augmented Generation With Langchain Retrieval Augmented Generation With Llamaindex Run Cluster Sagemaker-Entrypoint Streamlit OpenAI Chatbot Webserver Structured Outputs Utils Others Others LMCache Examples Logging Configuration Tensorize vLLM Model General General vLLM V1 Frequently Asked Questions Production Metrics Reproducibility Security Troubleshooting Usage Stats Collection Inference and Serving Inference and Serving Offline Inference OpenAI-Compatible Server Data Parallel Deployment Troubleshooting distributed deployments Expert Parallel Deployment Parallelism and Scaling Parallelism and Scaling Table of contents Distributed inference strategies for a single-model replica Distributed serving of Mixture of Experts (MoE) models Single-node deployment Multi-node deployment What is Ray? Ray cluster setup with containers Running vLLM on a Ray cluster Optimizing network communication for tensor parallelism Enabling GPUDirect RDMA Troubleshooting distributed deployments Integrations Integrations LangChain LlamaIndex Deployment Deployment Using Docker Using Kubernetes Using Nginx Frameworks Frameworks Anyscale AnythingLLM AutoGen BentoML Cerebrium Chatbox Dify dstack Haystack Helm LiteLLM Lobe Chat LWS Modal Open WebUI Retrieval-Augmented Generation SkyPilot Streamlit NVIDIA Triton Integrations Integrations KServe KubeAI KubeRay Llama Stack llmaz Production stack Training Training Reinforcement Learning from Human Feedback Transformers Reinforcement Learning Configuration Configuration Conserving Memory Engine Arguments Environment Variables Model Resolution Optimization and Tuning Server Arguments TPU Optimization Tips Models Models Supported Models Generative Models Pooling Models Extensions Extensions Loading Model weights with fastsafetensors Loading models with Run:ai Model Streamer Loading models with CoreWeave's Tensorizer Hardware Supported Models Hardware Supported Models TPU Features Features Automatic Prefix Caching Disaggregated Prefilling (experimental) LoRA Adapters Multimodal Inputs Prompt Embedding Inputs Reasoning Outputs Sleep Mode Speculative Decoding Structured Outputs Tool Calling Quantization Quantization AutoAWQ AutoRound BitBLAS BitsAndBytes FP8 W8A8 GGUF GPTQModel FP8 INC INT4 W4A16 INT8 W8A8 NVIDIA TensorRT Model Optimizer Quantized KV Cache AMD Quark TorchAO Developer Guide Developer Guide General General Benchmark Suites Deprecation Policy Dockerfile Incremental Compilation Workflow Profiling vLLM Vulnerability Management Model Implementation Model Implementation Basic Model Registering a Model Unit Testing Multi-Modal Support Speech-to-Text (Transcription/Translation) Support CI CI CI Failures Update PyTorch version on vLLM OSS CI/CD Design Documents Design Documents Architecture Overview Fused MoE Modular Kernel Integration with Hugging Face Hybrid KV Cache Manager IO Processor Plugins Metrics Multi-Modal Data Processing Python Multiprocessing P2P NCCL Connector Paged Attention Plugin System Automatic Prefix Caching torch.compile integration API Reference API Reference vllm vllm beam_search collect_env connections env_override envs forward_context logger logits_process logprobs outputs pooling_params sampling_params scalar_type scripts sequence tasks test_utils tracing version adapter_commons adapter_commons layers models request utils worker_manager assets assets audio base image video attention attention layer selector backends backends abstract differential_flash_attn dual_chunk_flash_attn flash_attn flashmla placeholder_attn rocm_aiter_mla rocm_flash_attn triton_mla utils xformers mla mla common layers layers chunked_local_attention cross_attention encoder_only_attention ops ops chunked_prefill_paged_decode common flashmla merge_attn_states paged_attn pallas_kv_cache_update prefix_prefill rocm_aiter_mla rocm_aiter_paged_attn triton_decode_attention triton_flash_attention triton_merge_attn_states triton_unified_attention utils utils fa_utils kv_sharing_utils benchmarks benchmarks datasets latency serve throughput lib lib endpoint_request_func ready_checker utils compilation compilation activation_quant_fusion backends base_static_graph collective_fusion compiler_interface counter cuda_graph cuda_piecewise_backend decorators fix_functionalization fusion fusion_attn fx_utils inductor_pass monitor multi_output_match noop_elimination pass_manager sequence_parallelism torch25_custom_graph_pass vllm_inductor_pass wrapper config config cache compilation kv_events kv_transfer load lora parallel scheduler utils core core block_manager evictor interfaces placeholder_block_space_manager scheduler block block block_table common cpu_gpu_block_allocator interfaces naive_block prefix_caching_block utils device_allocator device_allocator cumem distributed distributed communication_op kv_events parallel_state tpu_distributed_utils utils device_communicators device_communicators all2all all_reduce_utils base_device_communicator cpu_communicator cuda_communicator cuda_wrapper custom_all_reduce pynccl pynccl_wrapper quick_all_reduce ray_communicator shm_broadcast symm_mem tpu_communicator xpu_communicator eplb eplb eplb_state rebalance_algo rebalance_execute kv_transfer kv_transfer kv_transfer_state kv_connector kv_connector base factory utils v1 v1 base lmcache_connector multi_connector nixl_connector shared_storage_connector p2p p2p p2p_nccl_connector p2p_nccl_engine tensor_memory_pool kv_lookup_buffer kv_lookup_buffer base mooncake_store simple_buffer kv_pipe kv_pipe base mooncake_pipe pynccl_pipe engine engine arg_utils async_llm_engine async_timeout llm_engine metrics metrics_types protocol multiprocessing multiprocessing client engine output_processor output_processor interfaces single_step stop_checker util entrypoints entrypoints api_server chat_utils constants context harmony_utils launcher llm logger renderer score_utils ssl tool tool_server utils cli cli collect_env main openai run_batch serve types benchmark benchmark base latency main serve throughput openai openai api_server cli_args logits_processors protocol run_batch serving_chat serving_classification serving_completion serving_embedding serving_engine serving_models serving_pooling serving_responses serving_score serving_tokenization serving_transcription speech_to_text tool_parsers tool_parsers abstract_tool_parser deepseekv3_tool_parser deepseekv31_tool_parser glm4_moe_tool_parser granite_20b_fc_tool_parser granite_tool_parser hermes_tool_parser hunyuan_a13b_tool_parser internlm2_tool_parser jamba_tool_parser kimi_k2_tool_parser llama4_pythonic_tool_parser llama_tool_parser minimax_tool_parser mistral_tool_parser openai_tool_parser phi4mini_tool_parser pythonic_tool_parser qwen3coder_tool_parser seed_oss_tool_parser step3_tool_parser utils xlam_tool_parser executor executor executor_base mp_distributed_executor msgspec_utils multiproc_worker_utils ray_distributed_executor ray_utils uniproc_executor inputs inputs data parse preprocess registry logging_utils logging_utils dump_input formatter lora lora lora models peft_helper request resolver utils worker_manager layers layers base base_linear column_parallel_linear logits_processor qkv_x_parallel_linear replicated_linear row_parallel_linear utils vocal_parallel_embedding ops ops ipex_ops ipex_ops lora_ops torch_ops torch_ops lora_ops triton_ops triton_ops kernel_utils lora_expand_op lora_kernel_metadata lora_shrink_op utils xla_ops xla_ops lora_ops punica_wrapper punica_wrapper punica_base punica_cpu punica_gpu punica_selector punica_tpu punica_xpu utils model_executor model_executor custom_op parameter sampling_metadata utils layers layers activation attention_layer_base layernorm lightning_attn linear logits_processor mla pooler resampler sampler utils vocab_parallel_embedding fla fla ops ops chunk chunk_delta_h chunk_o chunk_scaled_dot_kkt cumsum fused_recurrent index l2norm layernorm_guard op solve_tril utils wy_fast fused_moe fused_moe batched_deep_gemm_moe batched_triton_or_deep_gemm_moe config cpu_fused_moe cutlass_moe deep_gemm_moe deep_gemm_utils deepep_ht_prepare_finalize deepep_ll_prepare_finalize flashinfer_cutlass_moe flashinfer_cutlass_prepare_finalize fused_batched_moe fused_marlin_moe fused_moe gpt_oss_triton_kernels_moe layer modular_kernel moe_align_block_size moe_pallas moe_permute_unpermute moe_torch_iterative pplx_prepare_finalize prepare_finalize rocm_aiter_fused_moe routing_simulator topk_weight_and_reduce triton_deep_gemm_moe trtllm_moe utils mamba mamba abstract linear_attn mamba2_metadata mamba_mixer mamba_mixer2 mamba_utils short_conv ops ops causal_conv1d layernorm_gated mamba_ssm ssd_bmm ssd_chunk_scan ssd_chunk_state ssd_combined ssd_state_passing quantization quantization auto_round awq awq_marlin awq_triton base_config bitblas bitsandbytes deepgemm deepspeedfp experts_int8 fbgemm_fp8 fp8 gguf gptq gptq_bitblas gptq_marlin gptq_marlin_24 hqq_marlin inc input_quant_fp8 ipex_quant kv_cache modelopt moe_wna16 mxfp4 petit ptpc_fp8 rtn schema torchao tpu_int8 compressed_tensors compressed_tensors compressed_tensors compressed_tensors_moe triton_scaled_mm utils schemes schemes compressed_tensors_24 compressed_tensors_scheme compressed_tensors_w4a4_nvfp4 compressed_tensors_w4a8_fp8 compressed_tensors_w4a8_int compressed_tensors_w4a16_24 compressed_tensors_w4a16_nvfp4 compressed_tensors_w8a8_fp8 compressed_tensors_w8a8_int8 compressed_tensors_w8a16_fp8 compressed_tensors_wNa16 kernels kernels mixed_precision mixed_precision allspark bitblas conch cutlass dynamic_4bit exllama MPLinearKernel machete marlin scaled_mm scaled_mm aiter cpu cutlass ScaledMMLinearKernel triton xla quark quark quark quark_moe utils schemes schemes quark_scheme quark_w4a4_mxfp4 quark_w8a8_fp8 quark_w8a8_int8 utils utils allspark_utils bitblas_utils flashinfer_fp4_moe flashinfer_utils fp8_utils gptq_utils int8_utils layer_utils machete_utils marlin_utils marlin_utils_fp4 marlin_utils_fp8 marlin_utils_test marlin_utils_test_24 mxfp4_utils mxfp8_utils nvfp4_emulation_utils nvfp4_moe_support petit_utils quant_utils w8a8_utils rotary_embedding rotary_embedding base common deepseek_scaling_rope dual_chunk_rope dynamic_ntk_alpha_rope dynamic_ntk_scaling_rope ernie45_vl_rope linear_scaling_rope llama3_rope llama4_vision_rope mrope ntk_scaling_rope phi3_long_rope_scaled_rope yarn_scaling_rope shared_fused_moe shared_fused_moe shared_fused_moe model_loader model_loader base_loader bitsandbytes_loader default_loader dummy_loader gguf_loader runai_streamer_loader sharded_state_loader tensorizer tensorizer_loader tpu utils weight_utils models models adapters aimv2 apertus arcee arctic aria aya_vision baichuan bailing_moe bamba bart bert bert_with_rope blip blip2 bloom chameleon chatglm clip cohere2_vision commandr config constant_size_cache dbrx deepseek deepseek_eagle deepseek_mtp deepseek_v2 deepseek_vl2 donut dots1 ernie45 ernie45_moe ernie45_vl ernie45_vl_moe ernie_mtp exaone exaone4 fairseq2_llama falcon falcon_h1 florence2 fuyu gemma gemma2 gemma3 gemma3_mm gemma3n gemma3n_mm glm glm4 glm4_1v glm4_moe glm4_moe_mtp glm4v gpt2 gpt_bigcode gpt_j gpt_neox gpt_oss granite granite_speech granitemoe granitemoehybrid granitemoeshared gritlm grok1 h2ovl hunyuan_v1 hyperclovax_vision idefics2_vision_model idefics3 interfaces interfaces_base intern_vit internlm2 internlm2_ve interns1 interns1_vit internvl jais jamba jina_vl keye keye_vl1_5 kimi_vl lfm2 llama llama4 llama4_eagle llama_eagle llama_eagle3 llava llava_next llava_next_video llava_onevision mamba mamba2 mamba_cache medusa midashenglm mimo mimo_mtp minicpm minicpm3 minicpm_eagle minicpmo minicpmv minimax_cache minimax_text_01 minimax_vl_01 mistral3 mixtral mllama mllama4 mlp_speculator modernbert module_mapping molmo moonvit motif mpt nano_nemotron_vl nemotron nemotron_h nemotron_nas nemotron_vl nvlm_d olmo olmo2 olmoe opt orion ovis ovis2_5 paligemma persimmon phi phi3 phi3v phi4_multimodal phi4flash phi4mm phi4mm_audio phi4mm_utils phimoe pixtral plamo2 qwen qwen2 qwen2_5_omni_thinker qwen2_5_vl qwen2_audio qwen2_moe qwen2_rm qwen2_vl qwen3 qwen3_moe qwen3_next qwen3_next_mtp qwen_vl registry roberta rvl seed_oss siglip siglip2navit skyworkr1v smolvlm solar stablelm starcoder2 step3_text step3_vl swin tarsier telechat2 teleflm terratorch transformers ultravox utils vision voxtral whisper zamba2 warmup warmup deep_gemm_warmup kernel_warmup multimodal multimodal audio base cache hasher image inputs parse processing profiling registry utils video platforms platforms cpu cuda interface rocm tpu xpu plugins plugins io_processors io_processors interface lora_resolvers lora_resolvers filesystem_resolver profiler profiler layerwise_profile utils ray ray lazy_utils ray_env reasoning reasoning abs_reasoning_parsers deepseek_r1_reasoning_parser glm4_moe_reasoning_parser gptoss_reasoning_parser granite_reasoning_parser hunyuan_a13b_reasoning_parser mistral_reasoning_parser qwen3_reasoning_parser step3_reasoning_parser transformers_utils transformers_utils config config_parser_base detokenizer detokenizer_utils dynamic_module processor runai_utils s3_utils tokenizer tokenizer_base tokenizer_group utils chat_templates chat_templates registry configs configs arctic chatglm deepseek_vl2 eagle falcon jais kimi_vl medusa midashenglm mistral mlp_speculator moonvit nemotron nemotron_h nemotron_vl ovis qwen3_next step3_vl ultravox speculators speculators algos base processors processors deepseek_vl2 ovis ovis2_5 tokenizers tokenizers mistral triton_utils triton_utils importing usage usage usage_lib utils utils deep_gemm flashinfer jsontree tensor_schema v1 v1 cudagraph_dispatcher kv_cache_interface outputs request serial_utils utils attention attention backends backends cpu_attn flash_attn flashinfer flex_attention gdn_attn linear_attn mamba1_attn mamba2_attn mamba_attn pallas rocm_aiter_fa short_conv_attn tree_attn triton_attn utils xformers mla mla common cutlass_mla flashattn_mla flashinfer_mla flashmla rocm_aiter_mla triton_mla core core block_pool encoder_cache_manager kv_cache_coordinator kv_cache_manager kv_cache_utils single_type_kv_cache_manager sched sched async_scheduler interface output request_queue scheduler utils engine engine async_llm coordinator core core_client detokenizer exceptions llm_engine logprobs output_processor parallel_sampling processor utils executor executor abstract multiproc_executor ray_distributed_executor metrics metrics loggers prometheus ray_wrappers reader stats pool pool metadata sample sample metadata rejection_sampler sampler logits_processor logits_processor builtin interface state ops ops bad_words logprobs penalties topk_topp_sampler tpu tpu metadata sampler spec_decode spec_decode eagle medusa metadata metrics ngram_proposer utils structured_output structured_output backend_guidance backend_lm_format_enforcer backend_outlines backend_types backend_xgrammar request utils worker worker block_table cpu_model_runner cpu_worker gpu_input_batch gpu_model_runner gpu_worker kv_connector_model_runner_mixin lora_model_runner_mixin tpu_input_batch tpu_model_runner tpu_worker utils worker_base xpu_model_runner xpu_worker worker worker cache_engine enc_dec_model_runner model_runner model_runner_base utils worker worker_base CLI Reference CLI Reference vllm serve vllm chat vllm complete vllm run-batch vllm bench vllm bench vllm bench latency vllm bench serve vllm bench throughput Community Community Contact Us Meetups Sponsors Blog Forum Slack Table of contents Distributed inference strategies for a single-model replica Distributed serving of Mixture of Experts (MoE) models Single-node deployment Multi-node deployment What is Ray? Ray cluster setup with containers Running vLLM on a Ray cluster Optimizing network communication for tensor parallelism Enabling GPUDirect RDMA Troubleshooting distributed deployments Parallelism and Scaling ¶ Distributed inference strategies for a single-model replica ¶ To choose a distributed inference strategy for a single-model replica, use the following guidelines: Single GPU (no distributed inference): if the model fits on a single GPU, distributed inference is probably unnecessary. Run inference on that GPU. Single-node multi-GPU using tensor parallel inference: if the model is too large for a single GPU but fits on a single node with multiple GPUs, use tensor parallelism . For example, set tensor_parallel_size=4 when using a node with 4 GPUs. Multi-node multi-GPU using tensor parallel and pipeline parallel inference: if the model is too large for a single node, combine tensor parallelism with pipeline parallelism . Set tensor_parallel_size to the number of GPUs per node and pipeline_parallel_size to the number of nodes. For example, set tensor_parallel_size=8 and pipeline_parallel_size=2 when using 2 nodes with 8 GPUs per node. Increase the number of GPUs and nodes until there is enough GPU memory for the model. Set tensor_parallel_size to the number of GPUs per node and pipeline_parallel_size to the number of nodes. After you provision sufficient resources to fit the model, run vllm . Look for log messages like: INFO 07-23 13:56:04 [kv_cache_utils.py:775] GPU KV cache size: 643,232 tokens INFO 07-23 13:56:04 [kv_cache_utils.py:779] Maximum concurrency for 40,960 tokens per request: 15.70x The GPU KV cache size line reports the total number of tokens that can be stored in the GPU KV cache at once. The Maximum concurrency line provides an estimate of how many requests can be served concurrently if each request requires the specified number of tokens (40,960 in the example above). The tokens-per-request number is taken from the model configuration's maximum sequence length, ModelConfig.max_model_len . If these numbers are lower than your throughput requirements, add more GPUs or nodes to your cluster. Edge case: uneven GPU splits If the model fits within a single node but the GPU count doesn't evenly divide the model size, enable pipeline parallelism, which splits the model along layers and supports uneven splits. In this scenario, set tensor_parallel_size=1 and pipeline_parallel_size to the number of GPUs. Furthermore, if the GPUs on the node do not have NVLINK interconnect (e.g. L40S), leverage pipeline parallelism instead of tensor parallelism for higher throughput and lower communication overhead. Distributed serving of Mixture of Experts ( MoE ) models ¶ It's often advantageous to exploit the inherent parallelism of experts by using a separate parallelism strategy for the expert layers. vLLM supports large-scale deployment combining Data Parallel attention with Expert or Tensor Parallel MoE layers. For more information, see Data Parallel Deployment . Single-node deployment ¶ vLLM supports distributed tensor-parallel and pipeline-parallel inference and serving. The implementation includes Megatron-LM's tensor parallel algorithm . The default distributed runtimes are Ray for multi-node inference and native Python multiprocessing for single-node inference. You can override the defaults by setting distributed_executor_backend in the LLM class or --distributed-executor-backend in the API server. Use mp for multiprocessing or ray for Ray. For multi-GPU inference, set tensor_parallel_size in the LLM class to the desired GPU count. For example, to run inference on 4 GPUs: from vllm import LLM llm = LLM ( \"facebook/opt-13b\" , tensor_parallel_size = 4 ) output = llm . generate ( \"San Francisco is a\" ) For multi-GPU serving, include --tensor-parallel-size when starting the server. For example, to run the API server on 4 GPUs: vllm serve facebook/opt-13b \\ --tensor-parallel-size 4 To enable pipeline parallelism, add --pipeline-parallel-size . For example, to run the API server on 8 GPUs with pipeline parallelism and tensor parallelism: # Eight GPUs total vllm serve gpt2 \\ --tensor-parallel-size 4 \\ --pipeline-parallel-size 2 Multi-node deployment ¶ If a single node lacks sufficient GPUs to hold the model, deploy vLLM across multiple nodes. Ensure that every node provides an identical execution environment, including the model path and Python packages. Using container images is recommended because they provide a convenient way to keep environments consistent and to hide host heterogeneity. What is Ray? ¶ Ray is a distributed computing framework for scaling Python programs. Multi-node vLLM deployments require Ray as the runtime engine. vLLM uses Ray to manage the distributed execution of tasks across multiple nodes and control where execution happens. Ray also offers high-level APIs for large-scale offline batch inference and online serving that can leverage vLLM as the engine. These APIs add production-grade fault tolerance, scaling, and distributed observability to vLLM workloads. For details, see the Ray documentation . Ray cluster setup with containers ¶ The helper script examples/online_serving/run_cluster.sh starts containers across nodes and initializes Ray. By default, the script runs Docker without administrative privileges, which prevents access to the GPU performance counters when profiling or tracing. To enable admin privileges, add the --cap-add=CAP_SYS_ADMIN flag to the Docker command. Choose one node as the head node and run: bash run_cluster.sh \\ vllm/vllm-openai \\ &lt;HEAD_NODE_IP&gt; \\ --head \\ /path/to/the/huggingface/home/in/this/node \\ -e VLLM_HOST_IP = &lt;HEAD_NODE_IP&gt; On each worker node, run: bash run_cluster.sh \\ vllm/vllm-openai \\ &lt;HEAD_NODE_IP&gt; \\ --worker \\ /path/to/the/huggingface/home/in/this/node \\ -e VLLM_HOST_IP = &lt;WORKER_NODE_IP&gt; Note that VLLM_HOST_IP is unique for each worker. Keep the shells running these commands open; closing any shell terminates the cluster. Ensure that all nodes can communicate with each other through their IP addresses. Network security For security, set VLLM_HOST_IP to an address on a private network segment. Traffic sent over this network is unencrypted, and the endpoints exchange data in a format that can be exploited to execute arbitrary code if an adversary gains network access. Ensure that untrusted parties cannot reach the network. From any node, enter a container and run ray status and ray list nodes to verify that Ray finds the expected number of nodes and GPUs. Tip Alternatively, set up the Ray cluster using KubeRay. For more information, see KubeRay vLLM documentation . Running vLLM on a Ray cluster ¶ Tip If Ray is running inside containers, run the commands in the remainder of this guide inside the containers , not on the host. To open a shell inside a container, connect to a node and use docker exec -it &lt;container_name&gt; /bin/bash . Once a Ray cluster is running, use vLLM as you would in a single-node setting. All resources across the Ray cluster are visible to vLLM, so a single vllm command on a single node is sufficient. The common practice is to set the tensor parallel size to the number of GPUs in each node, and the pipeline parallel size to the number of nodes. For example, if you have 16 GPUs across 2 nodes (8 GPUs per node), set the tensor parallel size to 8 and the pipeline parallel size to 2: vllm serve /path/to/the/model/in/the/container \\ --tensor-parallel-size 8 \\ --pipeline-parallel-size 2 Alternatively, you can set tensor_parallel_size to the total number of GPUs in the cluster: vllm serve /path/to/the/model/in/the/container \\ --tensor-parallel-size 16 Optimizing network communication for tensor parallelism ¶ Efficient tensor parallelism requires fast inter-node communication, preferably through high-speed network adapters such as InfiniBand. To set up the cluster to use InfiniBand, append additional arguments like --privileged -e NCCL_IB_HCA=mlx5 to the examples/online_serving/run_cluster.sh helper script. Contact your system administrator for more information about the required flags. Enabling GPUDirect RDMA ¶ GPUDirect RDMA (Remote Direct Memory Access) is an NVIDIA technology that allows network adapters to directly access GPU memory, bypassing the CPU and system memory. This direct access reduces latency and CPU overhead, which is beneficial for large data transfers between GPUs across nodes. To enable GPUDirect RDMA with vLLM, configure the following settings: IPC_LOCK security context: add the IPC_LOCK capability to the container's security context to lock memory pages and prevent swapping to disk. Shared memory with /dev/shm : mount /dev/shm in the pod spec to provide shared memory for interprocess communication (IPC). If you use Docker, set up the container as follows: docker run --gpus all \\ --ipc = host \\ --shm-size = 16G \\ -v /dev/shm:/dev/shm \\ vllm/vllm-openai If you use Kubernetes, set up the pod spec as follows: ... spec : containers : - name : vllm image : vllm/vllm-openai securityContext : capabilities : add : [ \"IPC_LOCK\" ] volumeMounts : - mountPath : /dev/shm name : dshm resources : limits : nvidia.com/gpu : 8 requests : nvidia.com/gpu : 8 volumes : - name : dshm emptyDir : medium : Memory ... Confirm GPUDirect RDMA operation To confirm your InfiniBand card is using GPUDirect RDMA, run vLLM with detailed NCCL logs: NCCL_DEBUG=TRACE vllm serve ... . Then look for the NCCL version and the network used. If you find [send] via NET/IB/GDRDMA in the logs, then NCCL is using InfiniBand with GPUDirect RDMA, which is efficient. If you find [send] via NET/Socket in the logs, NCCL used a raw TCP socket, which is not efficient for cross-node tensor parallelism. Pre-download Hugging Face models If you use Hugging Face models, downloading the model before starting vLLM is recommended. Download the model on every node to the same path, or store the model on a distributed file system accessible by all nodes. Then pass the path to the model in place of the repository ID. Otherwise, supply a Hugging Face token by appending -e HF_TOKEN=&lt;TOKEN&gt; to run_cluster.sh . Troubleshooting distributed deployments ¶ For information about distributed debugging, see Troubleshooting distributed deployments . September 8, 2025 Back to top Made with Material for MkDocs ",
      "fetch_method": "direct-html"
    },
    {
      "arxiv_id": "https://lightllm-en.readthedocs.io/en/latest/getting_started/quickstart.html",
      "full_text": " Quick Start &#8212; Lightllm Skip to main content Back to top Ctrl + K Quick Start Installation Guide Quick Start Performance Benchmark Deployment Tutorials DeepSeek R1 Deployment Multimodal Deployment Reward Model Deployment OpenAI api Usage APIServer Parameters Lightllm API Introduction Model Support Supported Models List Adding New Models Architecture Introduction Architecture Overview Token Attention Efficient Router .rst .pdf Quick Start Contents 1. Prepare Model Files 2. Start Model Service 3. Test Model Service Quick Start # Deploying models with Lightllm is very simple, requiring only two steps at minimum: Prepare model weight files supported by Lightllm. Use command line to start the model service. (Optional) Test the model service. Note Before continuing with this tutorial, please ensure you have completed the Installation Guide . 1. Prepare Model Files # Download Qwen3-8B first. Below is an example code for downloading the model: (Optional) Create folder $ mkdirs ~/models &amp;&amp; cd ~/models Install huggingface_hub $ pip install -U huggingface_hub Download model files $ huggingface-cli download Qwen/Qwen3-8B --local-dir Qwen3-8B 2. Start Model Service # After downloading the Qwen3-8B model, use the following code in the terminal to deploy the API service: $ python -m lightllm.server.api_server --model_dir ~/models/Qwen3-8B Note The --model_dir parameter in the above code needs to be modified to your actual local model path. 3. Test Model Service # $ curl http://127.0.0.1:8000/generate \\ -H &quot;Content-Type: application/json&quot; \\ -d &#39;{ &quot;inputs&quot;: &quot;What is AI?&quot;, &quot;parameters&quot;:{ &quot;max_new_tokens&quot;:17, &quot;frequency_penalty&quot;:1 } }&#39; previous Installation Guide next Benchmark Testing Guide Contents 1. Prepare Model Files 2. Start Model Service 3. Test Model Service By the Lightllm Team © Copyright 2024, Lightllm Team. so the DOM is not blocked --> ",
      "fetch_method": "direct-html"
    },
    {
      "arxiv_id": "https://r.jina.ai/https://arxiv.org/abs/2412.19437},",
      "full_text": "Title: [2412.19437},] Article identifier not recognized\n\nURL Source: https://arxiv.org/abs/2412.19437%7D,\n\nWarning: Target URL returned error 404: Not Found\n\nMarkdown Content:\n[2412.19437},] Article identifier not recognized\n\n===============\n\n[Skip to main content](https://arxiv.org/abs/2412.19437%7D,#content)\n\n[![Image 1: Cornell University Logo](https://arxiv.org/static/browse/0.3.4/images/icons/cu/cornell-reduced-white-SMALL.svg)](https://www.cornell.edu/)\n\nWe gratefully acknowledge support from the Simons Foundation, [member institutions](https://info.arxiv.org/about/ourmembers.html), and all contributors.[Donate](https://info.arxiv.org/about/donate.html)\n\n[](https://arxiv.org/IgnoreMe)\n![Image 2: arxiv logo](https://arxiv.org/static/browse/0.3.4/images/arxiv-logo-one-color-white.svg)\n===================================================================================================\n\n[Help](https://info.arxiv.org/help) | [Advanced Search](https://arxiv.org/search/advanced)\n\nSearch\n\n[![Image 3: arXiv logo](https://arxiv.org/static/browse/0.3.4/images/arxiv-logomark-small-white.svg)](https://arxiv.org/)\n\n[![Image 4: Cornell University Logo](https://arxiv.org/static/browse/0.3.4/images/icons/cu/cornell-reduced-white-SMALL.svg)](https://www.cornell.edu/)\n\nGO\n\nquick links\n-----------\n\n*   [Login](https://arxiv.org/login)\n*   [Help Pages](https://info.arxiv.org/help)\n*   [About](https://info.arxiv.org/about)\n\nArticle identifier '2412.19437},' not recognized\n================================================\n\nYou might instead try to [search for articles](https://arxiv.org/search) using title or author information.\n\nFor additional help on arXiv identifiers, see [understanding the arXiv identifier](https://info.arxiv.org/help/arxiv_identifier.html).\n\n*   [About](https://info.arxiv.org/about)\n*   [Help](https://info.arxiv.org/help)\n\n*   [Contact](https://info.arxiv.org/help/contact.html)\n*   [Subscribe](https://info.arxiv.org/help/subscribe)\n\n*   [Copyright](https://info.arxiv.org/help/license/index.html)\n*   [Privacy Policy](https://info.arxiv.org/help/policies/privacy_policy.html)\n\n*   [Web Accessibility Assistance](https://info.arxiv.org/help/web_accessibility.html)\n*   [arXiv Operational Status](https://status.arxiv.org/)\n\n Get status notifications via [email](https://subscribe.sorryapp.com/24846f03/email/new) or [slack](https://subscribe.sorryapp.com/24846f03/slack/new)\n",
      "fetch_method": "jina-reader"
    }
  ]
}