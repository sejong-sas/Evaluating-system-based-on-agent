{
  "2-3 (API)": "",
  "3-1 (Pre-training)": "DeepSeek-V3 is initially pre-trained as a 671 B-parameter Mixture-of-Experts model (37 B active parameters per token) that integrates Multi-head Latent Attention, DeepSeekMoE, extra RMSNorm layers and width-bottleneck scaling, an auxiliary-loss-free load-balancing strategy, and a multi-token-prediction objective.  The team builds a 14.8-trillion-token high-quality, diverse corpus whose composition is deliberately adjusted to contain more mathematics, programming and broader multilingual content than DeepSeek-V2.  Training is performed with a 4 K maximum sequence length and is reported as highly stable: each trillion tokens costs about 180 K H800 GPU-hours (≈3.7 days on a 2 048-H800 cluster) and the full 14.8 T corpus consumes 2.664 M H800 GPU-hours.  Counting later context-length and post-training work, the entire life-cycle totals 2.788 M H800 hours.  The result is claimed to be the strongest open-source base model available, rivaling leading closed models such as GPT-4o and Claude-3.5-Sonnet.",
  "3-2 (Fine-tuning)": "After pre-training, DeepSeek-V3 undergoes a multi-stage post-training pipeline.  Supervised Fine-Tuning (SFT) is run for two epochs on a curated dataset with a cosine-decay learning-rate schedule that begins at 5 × 10⁻⁶ and tapers to 1 × 10⁻⁶.  The SFT corpus is split by task type: reasoning data (math, code-competition, logic puzzles) are generated with internal DeepSeek-R1 models, whereas non-reasoning data (creative writing, role-play, general Q&A) are produced by DeepSeek-V2.5 and human-verified for correctness.  An additional distillation step explicitly transfers reasoning capability from the DeepSeek-R1 family while keeping a balance between answer accuracy and output length.  Context-window extension is applied with YaRN: two 1 000-step phases expand the model’s context length from the pre-training 4 K to 32 K and then to 128 K tokens, after which the model shows strong \"Needle-In-A-Haystack\" robustness across the full 128 K window.  Following the subsequent RL phase, rejection sampling with expert models filters and refines the SFT data so the final checkpoint preserves DeepSeek-R1 strengths yet produces concise, effective responses.",
  "3-3 (Reinforcement Learning)": "Reinforcement Learning is the second half of the post-training cycle for DeepSeek-V3.  A reward model is first trained from the DeepSeek-V3 SFT checkpoints.  Policy optimization then employs Group Relative Policy Optimization (GRPO), a variant that eliminates the need for a same-size critic by computing the baseline from group scores, thereby reducing compute overhead compared with traditional PPO.  For broader behavioral alignment the team also incorporates a Constitutional-AI style loop: DeepSeek-V3’s own voted evaluations supply feedback that guides further updates.  Collectively, the RL stage aligns the base model with human preferences and \"unlocks\" additional capability that is later consolidated through rejection-sampled SFT data.",
  "2-3 (API)__evidence": [],
  "3-1 (Pre-training)__evidence": [
    {
      "source": "[abstract]",
      "quote": "We pre-train DeepSeek-V3 on 14.8 trillion diverse and high-quality tokens, followed by Supervised Fine-Tuning and Reinforcement Learning stages to fully harness its capabilities."
    },
    {
      "source": "[sections/Introduction]",
      "quote": "During pre-training, we train DeepSeek-V3 on 14.8T high-quality and diverse tokens."
    },
    {
      "source": "[sections/Introduction]",
      "quote": "During the pre-training stage, training DeepSeek-V3 on each trillion tokens requires only 180K H800 GPU hours, i.e., 3.7 days on our cluster with 2048 H800 GPUs."
    },
    {
      "source": "[sections/Introduction]",
      "quote": "At an economical cost of only 2.664M H800 GPU hours, we complete the pre-training of DeepSeek-V3 on 14.8T tokens, producing the currently strongest open-source base model."
    },
    {
      "source": "[sections/Pre-Training]",
      "quote": "Compared with DeepSeek-V2, we optimize the pre-training corpus by enhancing the ratio of mathematical and programming samples, while expanding multilingual coverage beyond English and Chinese."
    },
    {
      "source": "[sections/Pre-Training]",
      "quote": "Finally, the training corpus for DeepSeek-V3 consists of 14.8T high-quality and diverse tokens in our tokenizer."
    },
    {
      "source": "[sections/Pre-Training]",
      "quote": "As DeepSeek-V2, DeepSeek-V3 also employs additional RMSNorm layers after the compressed latent vectors, and multiplies additional scaling factors at the width bottlenecks."
    },
    {
      "source": "[sections/Pre-Training]",
      "quote": "Under this configuration, DeepSeek-V3 comprises 671B total parameters, of which 37B are activated for each token."
    },
    {
      "source": "[abstract]",
      "quote": "We present DeepSeek-V3, a strong Mixture-of-Experts (MoE) language model with 671B total parameters with 37B activated for each token. To achieve efficient inference and cost-effective training, DeepSeek-V3 adopts Multi-head Latent Attention (MLA) and DeepSeekMoE architectures, which were thoroughly validated in DeepSeek-V2. Furthermore, DeepSeek-V3 pioneers an auxiliary-loss-free strategy for load balancing and sets a multi-token prediction training objective for stronger performance. We pre-train DeepSeek-V3 on 14.8 trillion diverse and high-quality tokens, followed by Supervised Fine-Tuning and Reinforcement Learning stages to fully harness its capabilities."
    },
    {
      "source": "[sections/Introduction]",
      "quote": "During pre-training, we train DeepSeek-V3 on 14.8T high-quality and diverse tokens. The pre-training process is remarkably stable."
    },
    {
      "source": "[sections/Pre-Training]",
      "quote": "We set the maximum sequence length to 4K during pre-training, and pre-train DeepSeek-V3 on 14.8T tokens."
    },
    {
      "source": "[sections/Conclusion]",
      "quote": "In this paper, we introduce DeepSeek-V3, a large MoE language model with 671B total parameters and 37B activated parameters, trained on 14.8T tokens."
    },
    {
      "source": "[sections/Conclusion]",
      "quote": "Comprehensive evaluations demonstrate that DeepSeek-V3 has emerged as the strongest open-source model currently available, and achieves performance comparable to leading closed-source models like GPT-4o and Claude-3.5-Sonnet. It requires only 2.788M H800 GPU hours for its full training, including pre-training, context length extension, and post-training."
    }
  ],
  "3-2 (Fine-tuning)__evidence": [
    {
      "source": "[sections/Introduction]",
      "quote": "Following this, we conduct post-training, including Supervised Fine-Tuning (SFT) and Reinforcement Learning (RL) on the base model of DeepSeek-V3, to align it with human preferences and further unlock its potential."
    },
    {
      "source": "[sections/Introduction]",
      "quote": "During the post-training stage, we distill the reasoning capability from the DeepSeek-R1 series of models, and meanwhile carefully maintain the balance between model accuracy and generation length."
    },
    {
      "source": "[sections/Supervised Fine-Tuning]",
      "quote": "We fine-tune DeepSeek-V3-Base for two epochs using the SFT dataset, using the cosine decay learning rate scheduling that starts at 5 × 10−6 and gradually decreases to 1 × 10−6."
    },
    {
      "source": "[sections/Supervised Fine-Tuning]",
      "quote": "Reasoning Data. For reasoning-related datasets, including those focused on mathematics, code competition problems, and logic puzzles, we generate the data by leveraging an internal DeepSeek-R1 model."
    },
    {
      "source": "[sections/Supervised Fine-Tuning]",
      "quote": "Non-Reasoning Data. For non-reasoning data, such as creative writing, role-play, and simple question answering, we utilize DeepSeek-V2.5 to generate responses and enlist human annotators to verify the accuracy and correctness of the data."
    },
    {
      "source": "[sections/Long Context Extension]",
      "quote": "We adopt a similar approach to DeepSeek-V2 (DeepSeek-AI, 2024c) to enable long context capabilities in DeepSeek-V3. After the pre-training stage, we apply YaRN (Peng et al., 2023a) for context extension and perform two additional training phases, each comprising 1000 steps, to progressively expand the context window from 4K to 32K and then to 128K."
    },
    {
      "source": "[sections/Long Context Extension]",
      "quote": "Figure 8 illustrates that DeepSeek-V3, following supervised fine-tuning, achieves notable performance on the \"Needle In A HayStack\" (NIAH) test, demonstrating consistent robustness across context window lengths up to 128K."
    },
    {
      "source": "[sections/5.1 Supervised Fine-Tuning]",
      "quote": "We fine-tune DeepSeek-V3-Base for two epochs using the SFT dataset, using the cosine decay learning rate scheduling that starts at 5 × 10−6 and gradually decreases to 1 × 10−6."
    },
    {
      "source": "[sections/5.1 Supervised Fine-Tuning]",
      "quote": "After completing the RL training phase, we implement rejection sampling to curate high-quality SFT data for the final model, where the expert models are used as data generation sources. This method ensures that the final training data retains the strengths of DeepSeek-R1 while producing responses that are concise and effective."
    },
    {
      "source": "[sections/5.1 Supervised Fine-Tuning]",
      "quote": "For non-reasoning data, such as creative writing, role-play, and simple question answering, we utilize DeepSeek-V2.5 to generate responses and enlist human annotators to verify the accuracy and correctness of the data."
    }
  ],
  "3-3 (Reinforcement Learning)__evidence": [
    {
      "source": "[sections/Introduction]",
      "quote": "Following this, we conduct post-training, including Supervised Fine-Tuning (SFT) and Reinforcement Learning (RL) on the base model of DeepSeek-V3, to align it with human preferences and further unlock its potential."
    },
    {
      "source": "[sections/Reinforcement Learning]",
      "quote": "The reward model is trained from the DeepSeek-V3 SFT checkpoints."
    },
    {
      "source": "[sections/Reinforcement Learning]",
      "quote": "Similar to DeepSeek-V2 (DeepSeek-AI, 2024c), we adopt Group Relative Policy Optimization (GRPO) (Shao et al., 2024), which foregoes the critic model that is typically with the same size as the policy model, and estimates the baseline from group scores instead."
    },
    {
      "source": "[sections/Reinforcement Learning]",
      "quote": "During the development of DeepSeek-V3, for these broader contexts, we employ the constitutional AI approach (Bai et al., 2022), leveraging the voting evaluation results of DeepSeek-V3 itself as a feedback source."
    },
    {
      "source": "[sections/5.2 Reinforcement Learning – Reward Model]",
      "quote": "The reward model is trained from the DeepSeek-V3 SFT checkpoints."
    },
    {
      "source": "[sections/5.2 Reinforcement Learning – GRPO]",
      "quote": "Similar to DeepSeek-V2 (DeepSeek-AI, 2024c), we adopt Group Relative Policy Optimization (GRPO) (Shao et al., 2024), which foregoes the critic model that is typically with the same size as the policy model, and estimates the baseline from group scores instead."
    }
  ]
}