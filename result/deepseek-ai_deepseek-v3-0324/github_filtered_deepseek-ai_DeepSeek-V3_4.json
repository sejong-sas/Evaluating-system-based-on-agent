{
  "4-1 (Pre-training Data)": "The source material offers two explicit statements on DeepSeek-V3â€™s pre-training setup. First: \"We pre-train DeepSeek-V3 on 14.8 trillion diverse and high-quality tokens, followed by Supervised Fine-Tuning and Reinforcement Learning stages to fully harness its capabilities.\" Second: \"At an economical cost of only 2.664M H800 GPU hours, we complete the pre-training of DeepSeek-V3 on 14.8T tokens, producing the currently strongest open-source base model.\" These quotations establish that the pre-training corpus comprises \"14.8 trillion diverse and high-quality tokens,\" and that the process required \"2.664M H800 GPU hours.\"",
  "4-1 (Pre-training Data)__evidence": [
    {
      "source": "[readme]",
      "quote": "We pre-train DeepSeek-V3 on 14.8 trillion diverse and high-quality tokens, followed by Supervised Fine-Tuning and Reinforcement Learning stages to fully harness its capabilities."
    },
    {
      "source": "[readme]",
      "quote": "At an economical cost of only 2.664M H800 GPU hours, we complete the pre-training of DeepSeek-V3 on 14.8T tokens, producing the currently strongest open-source base model."
    }
  ],
  "4-2 (Fine-tuning Data)": "Fine-tuning is mentioned in the sentence: \"We pre-train DeepSeek-V3 on 14.8 trillion diverse and high-quality tokens, followed by Supervised Fine-Tuning and Reinforcement Learning stages to fully harness its capabilities.\" This quote affirms the presence of a \"Supervised Fine-Tuning\" phase directly after pre-training for DeepSeek-V3.",
  "4-2 (Fine-tuning Data)__evidence": [
    {
      "source": "[readme]",
      "quote": "We pre-train DeepSeek-V3 on 14.8 trillion diverse and high-quality tokens, followed by Supervised Fine-Tuning and Reinforcement Learning stages to fully harness its capabilities."
    }
  ],
  "4-3 (Reinforcement Learning Data)": "Reinforcement learning is referenced in the same line: \"We pre-train DeepSeek-V3 on 14.8 trillion diverse and high-quality tokens, followed by Supervised Fine-Tuning and Reinforcement Learning stages to fully harness its capabilities.\" This establishes that a \"Reinforcement Learning\" stage follows pre-training within the DeepSeek-V3 training pipeline.",
  "4-3 (Reinforcement Learning Data)__evidence": [
    {
      "source": "[readme]",
      "quote": "We pre-train DeepSeek-V3 on 14.8 trillion diverse and high-quality tokens, followed by Supervised Fine-Tuning and Reinforcement Learning stages to fully harness its capabilities."
    }
  ],
  "4-4 (Data Filtering)": "",
  "4-4 (Data Filtering)__evidence": []
}