{
  "4-1 (Pre-training Data)": "Across every description, the authors repeatedly state that the deepseek-v3 pre-training corpus contains 14.8 trillion “high-quality and diverse tokens.”  They emphasise that, relative to DeepSeek-V2, the mixture of data was deliberately re-balanced: the share of mathematical and programming examples was increased, and multilingual coverage was extended beyond just English and Chinese.  DeepSeek-V3 is trained with sequences up to 4 k tokens long, and all references underline that the same 14.8 T-token corpus is used in the model’s own tokenizer.  Thus, the publicly disclosed picture is: a single, very large (14.8 T) corpus, explicitly optimised for diversity, maths, coding and broader multilinguality, forms the entirety of the pre-training data from which the 671 B-parameter, 37 B-expert-active MoE model is learned.",
  "4-2 (Fine-tuning Data)": "After the base deepseek-v3 model is trained, a post-training pipeline of Supervised Fine-Tuning (SFT) is applied.  The SFT stage draws on two clearly separated data streams.  (1) “Reasoning” data—mathematics, code-competition problems and logic puzzles—is generated with an internal DeepSeek-R1 model and then used to transfer long chain-of-thought capability to DeepSeek-V3 through an explicit knowledge-distillation procedure.  (2) “Non-reasoning” data—creative writing, role-play and ordinary Q&A—is produced with DeepSeek-V2.5; human annotators subsequently check these responses for accuracy and correctness before they are admitted into the SFT set.  DeepSeek-V3-Base is then fine-tuned for two epochs on the resulting SFT mixture, using a cosine-decay learning-rate schedule that starts at 5 × 10⁻⁶ and drops to 1 × 10⁻⁶.  The authors frame the objective of the entire SFT corpus as balancing accuracy with adequate generation length while transplanting the reasoning power of DeepSeek-R1 into the new model.",
  "4-3 (Reinforcement Learning Data)": "Reinforcement Learning for deepseek-v3 begins after the SFT checkpoints are available.  A reward model is first trained “from the DeepSeek-V3 SFT checkpoints.”  For the policy-optimisation stage, the team adopts Group Relative Policy Optimisation (GRPO), mirroring the recipe used in DeepSeek-V2.  GRPO discards the usual full-size critic network and instead estimates baselines from group scores.  The RL prompt distribution is deliberately broad, covering coding, math, writing, role-play and standard question-answering so that human-preference alignment spans multiple domains.  The paper also notes that constitutional-AI style feedback is injected by leveraging “voting” evaluations produced by DeepSeek-V3 itself, giving an additional automatic preference signal during longer-context development.",
  "4-4 (Data Filtering)": "For deepseek-v3 the authors claim a refined data-processing pipeline whose chief aim is to “minimize redundancy while maintaining corpus diversity,” an explicit contrast with the previous DeepSeek-V2 pipeline.  One concrete cleaning step tackles a bias introduced by a new pre-tokeniser that fuses punctuation with line breaks: during training a random fraction of these combined tokens is split apart so that the model sees more varied special-token patterns.  Owing to an effective MoE load-balancing strategy, the team reports “No Token-Dropping” at all during the full training run.  After RL is finished, the project still performs an extra filtering pass for the SFT data: expert DeepSeek-R1 models generate candidate answers and “rejection sampling” is applied so that only high-quality responses—shorter, concise and effective—are kept for the final release set.  Collectively, these measures constitute the entire published description of filtering and cleaning for DeepSeek-V3.",
  "4-1 (Pre-training Data)__evidence": [
    {
      "source": "[sections/Introduction]",
      "quote": "During pre-training, we train DeepSeek-V3 on 14.8T high-quality and diverse tokens."
    },
    {
      "source": "[sections/4.1 Data Construction]",
      "quote": "Compared with DeepSeek-V2, we optimize the pre-training corpus by enhancing the ratio of mathematical and programming samples, while expanding multilingual coverage beyond English and Chinese."
    },
    {
      "source": "[sections/4.1 Data Construction]",
      "quote": "Finally, the training corpus for DeepSeek-V3 consists of 14.8T high-quality and diverse tokens in our tokenizer."
    },
    {
      "source": "[sections/4.2 Hyper-Parameters]",
      "quote": "We set the maximum sequence length to 4K during pre-training, and pre-train DeepSeek-V3 on 14.8T tokens."
    },
    {
      "source": "[sections/2412.19437]",
      "quote": "We pre-train DeepSeek-V3 on 14.8 trillion diverse and high-quality tokens, followed by Supervised Fine-Tuning and Reinforcement Learning stages to fully harness its capabilities."
    },
    {
      "source": "[sections/Pre-Training – Data Construction]",
      "quote": "Compared with DeepSeek-V2, we optimize the pre-training corpus by enhancing the ratio of mathematical and programming samples, while expanding multilingual coverage beyond English and Chinese."
    },
    {
      "source": "[sections/Pre-Training – Data Construction]",
      "quote": "Finally, the training corpus for DeepSeek-V3 consists of 14.8T high-quality and diverse tokens in our tokenizer."
    },
    {
      "source": "[sections/Conclusion]",
      "quote": "In this paper, we introduce DeepSeek-V3, a large MoE language model with 671B total parameters and 37B activated parameters, trained on 14.8T tokens."
    }
  ],
  "4-2 (Fine-tuning Data)__evidence": [
    {
      "source": "[sections/Introduction]",
      "quote": "Following this, we conduct post-training, including Supervised Fine-Tuning (SFT) and Reinforcement Learning (RL) on the base model of DeepSeek-V3, to align it with human preferences and further unlock its potential."
    },
    {
      "source": "[sections/5.1 Supervised Fine-Tuning]",
      "quote": "Reasoning Data. For reasoning-related datasets, including those focused on mathematics, code competition problems, and logic puzzles, we generate the data by leveraging an internal DeepSeek-R1 model."
    },
    {
      "source": "[sections/5.1 Supervised Fine-Tuning]",
      "quote": "Non-Reasoning Data. For non-reasoning data, such as creative writing, role-play, and simple question answering, we utilize DeepSeek-V2.5 to generate responses and enlist human annotators to verify the accuracy and correctness of the data."
    },
    {
      "source": "[sections/Introduction]",
      "quote": "During the post-training stage, we distill the reasoning capability from the DeepSeek-R1 series of models, and meanwhile carefully maintain the balance between model accuracy and generation length."
    },
    {
      "source": "[sections/Introduction]",
      "quote": "Post-Training: Knowledge Distillation from DeepSeek-R1 • We introduce an innovative methodology to distill reasoning capabilities from the long-Chain-of-Thought (CoT) model, specifically from one of the DeepSeek R1 series models, into standard LLMs, particularly DeepSeek-V3."
    },
    {
      "source": "[sections/5.1 Supervised Fine-Tuning]",
      "quote": "We fine-tune DeepSeek-V3-Base for two epochs using the SFT dataset, using the cosine decay learning rate scheduling that starts at 5 × 10−6 and gradually decreases to 1 × 10−6."
    },
    {
      "source": "[sections/5.1 Supervised Fine-Tuning]",
      "quote": "For non-reasoning data, such as creative writing, role-play, and simple question answering, we utilize DeepSeek-V2.5 to generate responses and enlist human annotators to verify the accuracy and correctness of the data."
    },
    {
      "source": "[sections/5.1 Supervised Fine-Tuning]",
      "quote": "For reasoning-related datasets, including those focused on mathematics, code competition problems, and logic puzzles, we generate the data by leveraging an internal DeepSeek-R1 model."
    }
  ],
  "4-3 (Reinforcement Learning Data)__evidence": [
    {
      "source": "[sections/Introduction]",
      "quote": "Following this, we conduct post-training, including Supervised Fine-Tuning (SFT) and Reinforcement Learning (RL) on the base model of DeepSeek-V3, to align it with human preferences and further unlock its potential."
    },
    {
      "source": "[sections/5.2.2 Group Relative Policy Optimization]",
      "quote": "Similar to DeepSeek-V2 (DeepSeek-AI, 2024c), we adopt Group Relative Policy Optimization (GRPO) (Shao et al., 2024), which foregoes the critic model that is typically with the same size as the policy model, and estimates the baseline from group scores instead. We incorporate prompts from diverse domains, such as coding, math, writing, role-playing, and question answering, during the RL process."
    },
    {
      "source": "[sections/5.2.1 Reward Model]",
      "quote": "The reward model is trained from the DeepSeek-V3 SFT checkpoints."
    },
    {
      "source": "[sections/Introduction]",
      "quote": "We conduct post-training, including Supervised Fine-Tuning (SFT) and Reinforcement Learning (RL) on the base model of DeepSeek-V3, to align it with human preferences and further unlock its potential."
    },
    {
      "source": "[sections/5.2 Reinforcement Learning]",
      "quote": "The reward model is trained from the DeepSeek-V3 SFT checkpoints."
    },
    {
      "source": "[sections/5.4.2 Self-Rewarding]",
      "quote": "During the development of DeepSeek-V3, for these broader contexts, we employ the constitutional AI approach (Bai et al., 2022), leveraging the voting evaluation results of DeepSeek-V3 itself as a feedback source."
    }
  ],
  "4-4 (Data Filtering)__evidence": [
    {
      "source": "[sections/4.1 Data Construction]",
      "quote": "Compared with DeepSeek-V2, we optimize the pre-training corpus by enhancing the ratio of mathematical and programming samples, while expanding multilingual coverage beyond English and Chinese. Also, our data processing pipeline is refined to minimize redundancy while maintaining corpus diversity."
    },
    {
      "source": "[sections/4.1 Data Construction]",
      "quote": "In addition, compared with DeepSeek-V2, the new pretokenizer introduces tokens that combine punctuations and line breaks. To address this issue, we randomly split a certain proportion of such combined tokens during training, which exposes the model to a wider array of special cases and mitigates this bias."
    },
    {
      "source": "[sections/Pre-Training – Data Construction]",
      "quote": "Also, our data processing pipeline is refined to minimize redundancy while maintaining corpus diversity."
    },
    {
      "source": "[pdf_text]",
      "quote": "No Token-Dropping. Due to the effective load balancing strategy, DeepSeek-V3 keeps a good load balance during its full training. Therefore, DeepSeek-V3 does not drop any tokens during training."
    },
    {
      "source": "[sections/Pre-Training – Data Construction]",
      "quote": "In addition, compared with DeepSeek-V2, the new pretokenizer introduces tokens that combine punctuations and line breaks."
    },
    {
      "source": "[sections/5.1 Supervised Fine-Tuning]",
      "quote": "Upon completing the RL training phase, we implement rejection sampling to curate high-quality SFT data for the final model, where the expert models are used as data generation sources. This method ensures that the final training data retains the strengths of DeepSeek-R1 while producing responses that are concise and effective."
    }
  ]
}