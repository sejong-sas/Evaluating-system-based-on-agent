{
  "1-1 (Weights)": "Multiple sentences explicitly state that the DeepSeek-V3 model checkpoints (i.e., the full weight files) are publicly downloadable. The core quote says: “We present DeepSeek-V3, a strong Mixture-of-Experts (MoE) language model with 671B total parameters with 37B activated for each token. The model checkpoints are available at https://github.com/deepseek-ai/DeepSeek-V3.”  The same GitHub URL is repeated in several other DeepSeek-V3-specific sentences such as “The model checkpoints are available at https://github.com/deepseek-ai/DeepSeek-V3,” and a longer variant embedded in a performance claim: “Comprehensive evaluations reveal that DeepSeek-V3 outperforms other open-source models … The model checkpoints are available at https://github.com/deepseek-ai/DeepSeek-V3.”  Taken together, the only concrete information given is that anyone can fetch the full checkpoints from that public GitHub repository; no access gate, authentication, or additional download mechanism is mentioned in the provided material.",
  "1-2 (Code)": "None of the supplied DeepSeek-V3 quotes mention training scripts, data-prep code, configuration files, or any other form of training-pipeline source code. Therefore, the quotes provide no evidence that DeepSeek-V3’s training code (pre-training, fine-tuning, or RL stages) is public or accessible.",
  "1-3 (License)": "No quote that contains a DeepSeek-related token (e.g., “deepseek” or “DeepSeek-V3”) discusses license terms. Consequently, the provided material contains no DeepSeek-V3-specific statements about usage rights, modification, redistribution, or commercial use.",
  "1-4 (Paper)": "The existence of an official technical report is confirmed repeatedly: “DeepSeek-V3 Technical Report\\nDeepSeek-AI\\nresearch@deepseek.com” and again simply “DeepSeek-V3 Technical Report.”  The report introduces the model: “In this paper, we introduce DeepSeek-V3, a large MoE language model with 671B total parameters and 37B activated parameters, trained on 14.8T tokens.”  Hardware details are also cited: “DeepSeek-V3 is trained on a cluster equipped with 2048 NVIDIA H800 GPUs.”  Methodological highlights include a context-length extension procedure: “Through this two-phase extension training, DeepSeek-V3 is capable of handling inputs up to 128K in length while maintaining strong performance.”  Comparative positioning appears in the sentence: “In Table 3, we compare the base model of DeepSeek-V3 with the state-of-the-art open-source base models, including DeepSeek-V2-Base (DeepSeek-AI, 2024c) … Qwen2.5 72B Base … and LLaMA-3.1 405B Base.”  The set of quotes also references related DeepSeek work: “Z. Shao … Deepseekmath: Pushing the limits of mathematical reasoning in open language models. arXiv preprint arXiv:2402.03300, 2024.”  Collectively, these sentences confirm that (1) an official “DeepSeek-V3 Technical Report” exists, (2) it provides architecture, scaling, training-token, and hardware details, (3) the report positions the model against both earlier DeepSeek releases and other open models, and (4) further research outputs such as “Deepseekmath” are linked to the same research program.",
  "1-1 (Weights)__evidence": [
    {
      "source": "[pdf_text]",
      "quote": "We present DeepSeek-V3, a strong Mixture-of-Experts (MoE) language model with 671B total parameters with 37B activated for each token. The model checkpoints are available at https://github.com/deepseek-ai/DeepSeek-V3."
    },
    {
      "source": "[pdf_text]",
      "quote": "Overall, DeepSeek-V3-Base comprehensively outperforms DeepSeek-V2-Base and Qwen2.5 72B Base, and surpasses LLaMA-3.1 405B Base in the majority of benchmarks, essentially becoming the strongest open-source model."
    },
    {
      "source": "[pdf_text]",
      "quote": "DeepSeek-V3 stands as the best-performing open-source model, and also exhibits competitive performance against frontier closed-source models."
    },
    {
      "source": "[pdf_text]",
      "quote": "Comprehensive evaluations reveal that DeepSeek-V3 outperforms other open-source models and achieves performance comparable to leading closed-source models. ... The model checkpoints are available at https://github.com/deepseek-ai/DeepSeek-V3."
    },
    {
      "source": "[sections/2412.19437]",
      "quote": "The model checkpoints are available at https://github.com/deepseek-ai/DeepSeek-V3."
    },
    {
      "source": "[pdf_text]",
      "quote": "Table 6 presents the evaluation results, showcasing that DeepSeek-V3 stands as the best-performing open-source model."
    },
    {
      "source": "[pdf_text]",
      "quote": "Comprehensive evaluations demonstrate that DeepSeek-V3 has emerged as the strongest open-source model currently available"
    }
  ],
  "1-2 (Code)__evidence": [],
  "1-3 (License)__evidence": [
    {
      "source": "[web:https://www.bigcode-project.org/docs/pages/model-license/]",
      "quote": "https://www.bigcode-project.org/docs/pages/model-license/"
    },
    {
      "source": "[web:https://ai.meta.com/llama/license/]",
      "quote": "https://ai.meta.com/llama/license/"
    },
    {
      "source": "[web:https://huggingface.co/tiiuae/falcon-40b/blob/main/LICENSE]",
      "quote": "https://huggingface.co/tiiuae/falcon-40b/blob/main/LICENSE"
    }
  ],
  "1-4 (Paper)__evidence": [
    {
      "source": "[pdf_text]",
      "quote": "DeepSeek-V3 Technical Report\nDeepSeek-AI\nresearch@deepseek.com"
    },
    {
      "source": "[pdf_text]",
      "quote": "In this paper, we introduce DeepSeek-V3, a large MoE language model with 671B total parameters and 37B activated parameters, trained on 14.8T tokens."
    },
    {
      "source": "[title]",
      "quote": "DeepSeek-V3 Technical Report"
    },
    {
      "source": "[abstract]",
      "quote": "We present DeepSeek-V3, a strong Mixture-of-Experts (MoE) language model with 671B total parameters with 37B activated for each token."
    },
    {
      "source": "[pdf_text]",
      "quote": "DeepSeek-V3 is trained on a cluster equipped with 2048 NVIDIA H800 GPUs."
    },
    {
      "source": "[pdf_text]",
      "quote": "In Table 3, we compare the base model of DeepSeek-V3 with the state-of-the-art open-source base models, including DeepSeek-V2-Base (DeepSeek-AI, 2024c) (our previous release), Qwen2.5 72B Base (Qwen, 2024b), and LLaMA-3.1 405B Base (AI@Meta, 2024b)."
    },
    {
      "source": "[pdf_text]",
      "quote": "Through this two-phase extension training, DeepSeek-V3 is capable of handling inputs up to 128K in length while maintaining strong performance."
    },
    {
      "source": "[sections/2412.19437]",
      "quote": "DeepSeek-V3 Technical Report\nDeepSeek-AI"
    },
    {
      "source": "[sections/2412.19437]",
      "quote": "We present DeepSeek-V3, a strong Mixture-of-Experts (MoE) language model with 671B total parameters with 37B activated for each token."
    },
    {
      "source": "[pdf_text]",
      "quote": "To further push the boundaries of open-source model capabilities, we scale up our models and introduce DeepSeek-V3, a large Mixture-of-Experts (MoE) model with 671B parameters, of which 37B are activated for each token."
    },
    {
      "source": "[pdf_text]",
      "quote": "In the remainder of this paper, we first present a detailed exposition of our DeepSeek-V3 model architecture (Section 2)."
    },
    {
      "source": "[pdf_text]",
      "quote": "Z. Shao, P. Wang, Q. Zhu, R. Xu, J. Song, M. Zhang, Y. Li, Y. Wu, and D. Guo. Deepseekmath: Pushing the limits of mathematical reasoning in open language models. arXiv preprint arXiv:2402.03300, 2024."
    }
  ]
}