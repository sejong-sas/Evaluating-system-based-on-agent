{
  "1-1 (Weights)": "The project explicitly provides public download links and checksums for both the Base and Instruct variants of the 7-billion-parameter model. Quoted lines:\n• \"| 7B Instruct | https://models.mistralcdn.com/mistral-7b-v0-3/mistral-7B-Instruct-v0.3.tar | `80b71fcb6416085bcb4efad86dfb4d52` |\" \n• \"| 7B Base | https://models.mistralcdn.com/mistral-7b-v0-3/mistral-7B-v0.3.tar | `0663b293810d7571dad25dae2f2a5806` |\"  \nThese lines show that anyone can retrieve the weight archives directly from the listed URLs and verify integrity by matching the MD5/SHA-style strings. The documentation also highlights that earlier revisions remain available: \"You can download the previous versions of our models from our [docs](https://docs.mistral.ai/getting-started/open_weight_models/#downloading).\"  Together, these quotes indicate that weights for multiple versions are openly hosted, downloadable via HTTPS, and accompanied by hash values for reproducibility.",
  "1-1 (Weights)__evidence": [
    {
      "source": "[readme]",
      "quote": "| 7B Instruct | https://models.mistralcdn.com/mistral-7b-v0-3/mistral-7B-Instruct-v0.3.tar | `80b71fcb6416085bcb4efad86dfb4d52` |"
    },
    {
      "source": "[readme]",
      "quote": "| 7B Base | https://models.mistralcdn.com/mistral-7b-v0-3/mistral-7B-v0.3.tar | `0663b293810d7571dad25dae2f2a5806` |"
    },
    {
      "source": "[readme]",
      "quote": "You can download the previous versions of our models from our [docs](https://docs.mistral.ai/getting-started/open_weight_models/#downloading)."
    }
  ],
  "1-2 (Code)": "The repository accompanying the model publishes only the portions needed for running/inference, not full training pipelines. Its own description states: \"This repository contains minimal code to run Mistral models.\"  A dedicated deployment directory is supplied: \"The `deploy` folder contains code to build a [vLLM](https://M7B_DIR.com/vllm-project/vllm) image with the required dependencies to serve the Mistral AI model.\"  Users can quickly validate installation by executing an included utility: \"To test that a model works in your setup, you can run the `mistral-demo` command.\"  No material about pre-training, fine-tuning, or RLHF scripts is mentioned, so the public code scope is limited to inference/serving.",
  "1-2 (Code)__evidence": [
    {
      "source": "[readme]",
      "quote": "This repository contains minimal code to run Mistral models."
    },
    {
      "source": "[readme]",
      "quote": "The `deploy` folder contains code to build a [vLLM](https://M7B_DIR.com/vllm-project/vllm) image with the required dependencies to serve the Mistral AI model."
    },
    {
      "source": "[readme]",
      "quote": "To test that a model works in your setup, you can run the `mistral-demo` command."
    }
  ],
  "1-3 (License)": "All quoted excerpts reproduce the text of the Apache License 2.0, January 2004, which governs the repository: \n• \"Apache License\\n                           Version 2.0, January 2004\" \n• \"Licensed under the Apache License, Version 2.0 (the \\\"License\\\");   you may not use this file except in compliance with the License.\" \n• \"Subject to the terms and conditions of       this License, each Contributor hereby grants to You a perpetual,       worldwide, non-exclusive, no-charge, royalty-free, irrevocable       copyright license to reproduce, prepare Derivative Works of,       publicly display, publicly perform, sublicense, and distribute the       Work and such Derivative Works in Source or Object form.\"  These statements confirm that the model’s accompanying artifacts and code are covered by an OSI-approved permissive license, granting broad rights for use, modification, redistribution, and commercial exploitation, subject only to the standard Apache-2.0 conditions (notice preservation, patent clause, etc.).",
  "1-3 (License)__evidence": [
    {
      "source": "[license_files]",
      "quote": "Apache License\n                           Version 2.0, January 2004"
    },
    {
      "source": "[license_files]",
      "quote": "Licensed under the Apache License, Version 2.0 (the \"License\");   you may not use this file except in compliance with the License."
    },
    {
      "source": "[license_files]",
      "quote": "Subject to the terms and conditions of       this License, each Contributor hereby grants to You a perpetual,       worldwide, non-exclusive, no-charge, royalty-free, irrevocable       copyright license to reproduce, prepare Derivative Works of,       publicly display, publicly perform, sublicense, and distribute the       Work and such Derivative Works in Source or Object form."
    }
  ],
  "1-4 (Paper)": "No formal academic paper is quoted, but two official Mistral AI blog posts serve as technical write-ups and announcements: \n• \"Blog 7B: https://mistral.ai/news/announcing-mistral-7b/\" \n• \"Blog 8x7B: https://mistral.ai/news/mixtral-of-experts/\"  These links provide architecture details, training choices, evaluation results, and usage guidance for the 7 B model family (and its 8×7 B mixture-of-experts sibling).",
  "1-4 (Paper)__evidence": [
    {
      "source": "[readme]",
      "quote": "Blog 7B: [https://mistral.ai/news/announcing-mistral-7b/](https://mistral.ai/news/announcing-mistral-7b/)\\"
    },
    {
      "source": "[readme]",
      "quote": "Blog 8x7B: [https://mistral.ai/news/mixtral-of-experts/](https://mistral.ai/news/mixtral-of-experts/)\\"
    }
  ],
  "1-5 (Architecture)": "",
  "1-5 (Architecture)__evidence": [],
  "1-6 (Tokenizer)": "All available lines point to the same family-specific tokenizer. The code snippets repeatedly import the class `MistralTokenizer` from the `mistral_common.tokens.tokenizers.mistral` module and then instantiate it directly from a local JSON vocabulary file. One example loads the tokenizer with `MistralTokenizer.from_file(\"./mistral-nemo-instruct-v0.1/tekken.json\")`, while others pass in a path assembled from `model_path` (e.g. `mistral_tokenizer = MistralTokenizer.from_file(str(model_path / tokenizer[0]))`). A helper call, `mistral_tokenizer: MistralTokenizer = load_tokenizer(Path(model_path))`, shows an alternate wrapper that resolves the same object, and the final reference (`tokenizer: Tokenizer = mistral_tokenizer.instruct_tokenizer.tokenizer`) illustrates that the raw tokenizer returned by the instruction-mode wrapper is the one ultimately exposed for tokenisation. Altogether the quotes make clear that the model family relies on the dedicated, downloadable \"MistralTokenizer\", loaded from a JSON file kept next to the model weights, with convenience helpers for both standard and instruction variants.",
  "1-6 (Tokenizer)__evidence": [
    {
      "source": "[readme]",
      "quote": "from mistral_common.tokens.tokenizers.mistral import MistralTokenizer"
    },
    {
      "source": "[readme]",
      "quote": "tokenizer = MistralTokenizer.from_file(\"./mistral-nemo-instruct-v0.1/tekken.json\")  # change to extracted tokenizer file"
    },
    {
      "source": "[py_files/src/mistral_inference/main.py]",
      "quote": "mistral_tokenizer = MistralTokenizer.from_file(str(model_path / tokenizer[0]))"
    },
    {
      "source": "[py_files/src/mistral_inference/main.py]",
      "quote": "mistral_tokenizer: MistralTokenizer = load_tokenizer(Path(model_path))"
    },
    {
      "source": "[py_files/src/mistral_inference/main.py]",
      "quote": "tokenizer: Tokenizer = mistral_tokenizer.instruct_tokenizer.tokenizer"
    }
  ],
  "2-1 (Hardware)": "The only explicit hardware details concern GPUs. One note states that a GPU is needed even for the installation step because `mistral-inference` pulls in the `xformers` package, and \"`xformers` itself needs a GPU for installation.\" A launch command is also provided—`torchrun --nproc-per-node 2 --no-python mistral-demo $M8x7B_DIR`—which shows the authors running two processes per node, implicitly mapping to two GPUs during execution. Nothing else about node count, accelerator model, or total compute is disclosed in the supplied text.",
  "2-1 (Hardware)__evidence": [
    {
      "source": "[readme]",
      "quote": "Note: You will use a GPU to install `mistral-inference`, as it currently requires `xformers` to be installed and `xformers` itself needs a GPU for installation."
    },
    {
      "source": "[readme]",
      "quote": "torchrun --nproc-per-node 2 --no-python mistral-demo $M8x7B_DIR"
    }
  ],
  "2-2 (Software)": "The same GPU-installation advisory doubles as the only direct insight into the software stack: installing the `mistral-inference` Python package pulls in `xformers`, indicating a PyTorch environment compiled with that CUDA-level attention optimisation. A second quote adds that the `deploy` folder \"contains code to build a vLLM image with the required dependencies to serve the Mistral AI model,\" explicitly naming the vLLM runtime and implying a containerised deployment recipe. Beyond the presence of PyTorch-compatible `xformers` and the vLLM-based serving image, no additional libraries, versions, or distributed-training frameworks are mentioned.",
  "2-2 (Software)__evidence": [
    {
      "source": "[readme]",
      "quote": "Note: You will use a GPU to install `mistral-inference`, as it currently requires `xformers` to be installed and `xformers` itself needs a GPU for installation."
    },
    {
      "source": "[readme]",
      "quote": "The `deploy` folder contains code to build a [vLLM](https://M7B_DIR.com/vllm-project/vllm) image with the required dependencies to serve the Mistral AI model."
    }
  ],
  "2-3 (API)": "The only explicit statement concerning an external interface says: “Use Mistral models on [Mistral AI official API](https://console.mistral.ai/) (La Plateforme).” From this we can conclude that the mistralai/mistral-inference model family is exposed through an officially supported, publicly documented web-based API hosted at console.mistral.ai, informally called “La Plateforme.” No further details on endpoints, authentication, rate limits, example requests, or pricing are disclosed in the supplied material.",
  "2-3 (API)__evidence": [
    {
      "source": "[readme]",
      "quote": "Use Mistral models on [Mistral AI official API](https://console.mistral.ai/) (La Plateforme)"
    }
  ],
  "3-1 (Pre-training)": "",
  "3-1 (Pre-training)__evidence": [],
  "3-2 (Fine-tuning)": "",
  "3-2 (Fine-tuning)__evidence": [],
  "3-3 (Reinforcement Learning)": "",
  "3-3 (Reinforcement Learning)__evidence": [],
  "4-1 (Pre-training Data)": "No information regarding the sources, quantities, types, or composition of pre-training data is disclosed in the provided quotes.",
  "4-1 (Pre-training Data)__evidence": [],
  "4-2 (Fine-tuning Data)": "The supplied quote set contains no statements about the origin, makeup, or public availability of any fine-tuning datasets.",
  "4-2 (Fine-tuning Data)__evidence": [],
  "4-3 (Reinforcement Learning Data)": "There are no quotes describing the composition, sourcing, or generation of reinforcement-learning datasets.",
  "4-3 (Reinforcement Learning Data)__evidence": [],
  "4-4 (Data Filtering)": "The only available quote is a single reference to Mistral’s guardrailing documentation: “Guardrailing: https://docs.mistral.ai/usage/guardrailing.”  This citation indicates that the model’s data-filtering or safety pipeline is addressed under a dedicated “guardrailing” section of Mistral’s official usage guide.  However, the quote itself provides no concrete filtering criteria, numeric thresholds, classifier names, or stage-by-stage descriptions.  It merely signals that such information—presumably covering the mechanisms used to detect and mitigate unsafe or low-quality content—exists in the linked documentation, but those details are not included in the excerpt supplied here.",
  "4-4 (Data Filtering)__evidence": [
    {
      "source": "[readme]",
      "quote": "Guardrailing: [https://docs.mistral.ai/usage/guardrailing](https://docs.mistral.ai/usage/guardrailing)"
    }
  ],
  "__usage": {
    "fine_tuning": "unknown",
    "rl": "unknown"
  }
}