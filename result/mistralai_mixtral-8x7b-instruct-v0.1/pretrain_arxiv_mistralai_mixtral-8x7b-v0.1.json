{
  "pretrain_method": "Mixtral is based on a transformer architecture [31] and uses the same modifications as described in [18], with the notable exceptions that Mixtral supports a fully dense context length of 32k tokens, and the feed‐forward blocks are replaced by Mixture-of-Expert layers (Section 2.1).",
  "pretrain_data": "Mixtral is pretrained with multilingual data using a context size of 32k tokens.",
  "__evidence": [
    {
      "source": "arxiv:2401.04088",
      "quote": "Mixtral is based on a transformer architecture [31] and uses the same modifications as described in [18], with the notable exceptions that Mixtral supports a fully dense context length of 32k tokens, and the feed‐forward blocks are replaced by Mixture-of-Expert layers (Section 2.1)."
    },
    {
      "source": "arxiv:2401.04088",
      "quote": "Mixtral is pretrained with multilingual data using a context size of 32k tokens."
    }
  ]
}