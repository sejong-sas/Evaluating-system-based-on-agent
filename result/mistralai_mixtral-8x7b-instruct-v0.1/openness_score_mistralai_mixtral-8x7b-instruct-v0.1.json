{
  "model": "mistralai/Mixtral-8x7B-Instruct-v0.1",
  "scores": {
    "1-2 Code": {
      "score": 0.0,
      "reason": "No training pipeline files; README mentions are ignored."
    },
    "1-3 License": {
      "score": 1.0,
      "reason": "Released under Apache-2.0, granting use, modification, redistribution, and commercial use with no extra restrictions."
    },
    "1-4 Paper": {
      "score": 1.0,
      "reason": "An official technical report (PDF / web page) specifically about Mixtral-8×7B is linked and quoted."
    },
    "1-5 Architecture": {
      "score": 1,
      "reason": "Architecture evidence present in extracted quotes."
    },
    "1-6 Tokenizer": {
      "score": 1.0,
      "reason": "Tokenizer files downloadable in repository."
    },
    "2-1 Hardware": {
      "score": 0.0,
      "reason": "No quoted information about training hardware types or quantities."
    },
    "2-2 Software": {
      "score": 0.0,
      "reason": "No quotes detail the training software stack beyond generic method names; frameworks/versions/configs are absent."
    },
    "2-3 API": {
      "score": 0.0,
      "reason": "Only generic/irrelevant mentions; no qualifying API docs.  No qualifying API docs via web search."
    },
    "3-1 Pre-training": {
      "score": 0.5,
      "reason": "Some method details (multilingual data, 32k context, up-sampling strategy) are given, but not all hyper-parameters or a reproducible pipeline."
    },
    "3-2 Fine-tuning": {
      "score": 0.5,
      "reason": "Procedure (SFT then DPO) is described, but hyper-parameters, dataset specifics, and full configs are missing."
    },
    "3-3 Reinforcement Learning": {
      "score": 0.5,
      "reason": "Use of Direct Preference Optimization on paired feedback data is stated, but the process is not fully specified."
    },
    "4-1 Pre-training Data": {
      "score": 0.5,
      "reason": "Only that the corpus is ‘multilingual’ is disclosed; no sources, sizes, or licensing details."
    },
    "4-2 Fine-tuning Data": {
      "score": 0.5,
      "reason": "Acknowledges an instruction dataset and a paired-feedback dataset but gives no composition or sourcing information."
    },
    "4-3 Reinforcement Learning Data": {
      "score": 0.5,
      "reason": "Mentions a paired feedback dataset for DPO, without further disclosure."
    },
    "4-4 Data Filtering": {
      "score": 0.0,
      "reason": "No statements about data filtering, cleaning, or moderation during training."
    },
    "1-1 Weights": {
      "score": 1,
      "reason": "Weights files present in the repository (e.g., *.safetensors/bin/pt/ckpt)."
    }
  },
  "included_scores": {
    "1-2 Code": {
      "score": 0.0,
      "reason": "No training pipeline files; README mentions are ignored."
    },
    "1-3 License": {
      "score": 1.0,
      "reason": "Released under Apache-2.0, granting use, modification, redistribution, and commercial use with no extra restrictions."
    },
    "1-4 Paper": {
      "score": 1.0,
      "reason": "An official technical report (PDF / web page) specifically about Mixtral-8×7B is linked and quoted."
    },
    "1-5 Architecture": {
      "score": 1,
      "reason": "Architecture evidence present in extracted quotes."
    },
    "1-6 Tokenizer": {
      "score": 1.0,
      "reason": "Tokenizer files downloadable in repository."
    },
    "2-1 Hardware": {
      "score": 0.0,
      "reason": "No quoted information about training hardware types or quantities."
    },
    "2-2 Software": {
      "score": 0.0,
      "reason": "No quotes detail the training software stack beyond generic method names; frameworks/versions/configs are absent."
    },
    "2-3 API": {
      "score": 0.0,
      "reason": "Only generic/irrelevant mentions; no qualifying API docs.  No qualifying API docs via web search."
    },
    "3-1 Pre-training": {
      "score": 0.5,
      "reason": "Some method details (multilingual data, 32k context, up-sampling strategy) are given, but not all hyper-parameters or a reproducible pipeline."
    },
    "3-2 Fine-tuning": {
      "score": 0.5,
      "reason": "Procedure (SFT then DPO) is described, but hyper-parameters, dataset specifics, and full configs are missing."
    },
    "3-3 Reinforcement Learning": {
      "score": 0.5,
      "reason": "Use of Direct Preference Optimization on paired feedback data is stated, but the process is not fully specified."
    },
    "4-1 Pre-training Data": {
      "score": 0.5,
      "reason": "Only that the corpus is ‘multilingual’ is disclosed; no sources, sizes, or licensing details."
    },
    "4-2 Fine-tuning Data": {
      "score": 0.5,
      "reason": "Acknowledges an instruction dataset and a paired-feedback dataset but gives no composition or sourcing information."
    },
    "4-3 Reinforcement Learning Data": {
      "score": 0.5,
      "reason": "Mentions a paired feedback dataset for DPO, without further disclosure."
    },
    "4-4 Data Filtering": {
      "score": 0.0,
      "reason": "No statements about data filtering, cleaning, or moderation during training."
    },
    "1-1 Weights": {
      "score": 1,
      "reason": "Weights files present in the repository (e.g., *.safetensors/bin/pt/ckpt)."
    }
  },
  "final_score_10pt": 5.0,
  "meta": {
    "usage_from_dispatch": {
      "fine_tuning": "used",
      "rl": "used"
    },
    "excluded": [],
    "denominator": 16,
    "raw_sum": 8.0,
    "scale": "10/16",
    "code_detection_reason": "No training pipeline files; README mentions are ignored.",
    "pretrain_sources_used": false,
    "trusted_router_hosts": [
      "openrouter.ai",
      "together.ai",
      "fireworks.ai",
      "perplexity.ai"
    ]
  }
}