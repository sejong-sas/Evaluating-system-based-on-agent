{
  "model": "google/gemma-2-2b-it",
  "scores": {
    "1-1 Weights": {
      "score": 1,
      "reason": "Weights files present in the repository (e.g., *.safetensors/bin/pt/ckpt)."
    },
    "1-2 Code": {
      "score": 0.0,
      "reason": "No training pipeline files; README mentions are ignored."
    },
    "1-3 License": {
      "score": 1.0,
      "reason": "The model is under the “Gemma” licence, which Google explicitly lists among the open licences that allow use, modification, redistribution and commercial use with only minor policy restrictions."
    },
    "1-4 Paper": {
      "score": 1.0,
      "reason": "A dedicated technical report, “Gemma 2: Improving Open Language Models at a Practical Size,” describes this exact model family."
    },
    "1-5 Architecture": {
      "score": 1,
      "reason": "Architecture evidence present in extracted quotes."
    },
    "1-6 Tokenizer": {
      "score": 1.0,
      "reason": "Tokenizer files downloadable in repository."
    },
    "2-1 Hardware": {
      "score": 1.0,
      "reason": "Quotes give both type (TPUv5e) and quantity (2 × 16 × 16 = 512 chips, 512-way data replication, 1-way model sharding)."
    },
    "2-2 Software": {
      "score": 0.5,
      "reason": "Training stack components beyond the base framework are named (JAX, ML Pathways, GSPMD partitioner, MegaScale XLA) but no precise versions/configs, so partial disclosure."
    },
    "2-3 API": {
      "score": 1.0,
      "reason": "Only generic or non-official mentions; no qualifying official API docs.  Official API docs via web: https://cloud.google.com/kubernetes-engine/docs/deprecations/serve-gemma-tpu-saxml, https://cloud.google.com/dataflow/docs/notebooks/run_inference_gemma, https://cloud.google.com/vertex-ai/generative-ai/docs/open-models/use-gemma"
    },
    "3-1 Pre-training": {
      "score": 0.5,
      "reason": "Method details such as 2 T tokens, knowledge-distillation objective and teacher-student setup are given, but not enough to reproduce the full pipeline."
    },
    "3-2 Fine-tuning": {
      "score": 0.5,
      "reason": "The paper outlines control-token schema and SFT procedure, but omits full hyper-parameters and scripts."
    },
    "3-3 Reinforcement Learning": {
      "score": 0.5,
      "reason": "RLHF is described at a high level (reward model size, conversational focus), but not in reproducible detail."
    },
    "4-1 Pre-training Data": {
      "score": 0.5,
      "reason": "Sources (web, code, science) and token counts are given, yet exact corpora, licences and proportions are not."
    },
    "4-2 Fine-tuning Data": {
      "score": 0.5,
      "reason": "Lists data types and one named set (LMSYS-chat-1M prompts) but not full dataset inventory or sizes."
    },
    "4-3 Reinforcement Learning Data": {
      "score": 0.5,
      "reason": "States that English preference data on the same prompts was used, but provides no dataset size or sourcing details."
    },
    "4-4 Data Filtering": {
      "score": 0.5,
      "reason": "Describes multi-stage filtering for safety and PII with the Google Cloud Sensitive Data Protection Tool, but lacks concrete thresholds or removal ratios."
    }
  },
  "included_scores": {
    "1-1 Weights": {
      "score": 1,
      "reason": "Weights files present in the repository (e.g., *.safetensors/bin/pt/ckpt)."
    },
    "1-2 Code": {
      "score": 0.0,
      "reason": "No training pipeline files; README mentions are ignored."
    },
    "1-3 License": {
      "score": 1.0,
      "reason": "The model is under the “Gemma” licence, which Google explicitly lists among the open licences that allow use, modification, redistribution and commercial use with only minor policy restrictions."
    },
    "1-4 Paper": {
      "score": 1.0,
      "reason": "A dedicated technical report, “Gemma 2: Improving Open Language Models at a Practical Size,” describes this exact model family."
    },
    "1-5 Architecture": {
      "score": 1,
      "reason": "Architecture evidence present in extracted quotes."
    },
    "1-6 Tokenizer": {
      "score": 1.0,
      "reason": "Tokenizer files downloadable in repository."
    },
    "2-1 Hardware": {
      "score": 1.0,
      "reason": "Quotes give both type (TPUv5e) and quantity (2 × 16 × 16 = 512 chips, 512-way data replication, 1-way model sharding)."
    },
    "2-2 Software": {
      "score": 0.5,
      "reason": "Training stack components beyond the base framework are named (JAX, ML Pathways, GSPMD partitioner, MegaScale XLA) but no precise versions/configs, so partial disclosure."
    },
    "2-3 API": {
      "score": 1.0,
      "reason": "Only generic or non-official mentions; no qualifying official API docs.  Official API docs via web: https://cloud.google.com/kubernetes-engine/docs/deprecations/serve-gemma-tpu-saxml, https://cloud.google.com/dataflow/docs/notebooks/run_inference_gemma, https://cloud.google.com/vertex-ai/generative-ai/docs/open-models/use-gemma"
    },
    "3-1 Pre-training": {
      "score": 0.5,
      "reason": "Method details such as 2 T tokens, knowledge-distillation objective and teacher-student setup are given, but not enough to reproduce the full pipeline."
    },
    "3-2 Fine-tuning": {
      "score": 0.5,
      "reason": "The paper outlines control-token schema and SFT procedure, but omits full hyper-parameters and scripts."
    },
    "3-3 Reinforcement Learning": {
      "score": 0.5,
      "reason": "RLHF is described at a high level (reward model size, conversational focus), but not in reproducible detail."
    },
    "4-1 Pre-training Data": {
      "score": 0.5,
      "reason": "Sources (web, code, science) and token counts are given, yet exact corpora, licences and proportions are not."
    },
    "4-2 Fine-tuning Data": {
      "score": 0.5,
      "reason": "Lists data types and one named set (LMSYS-chat-1M prompts) but not full dataset inventory or sizes."
    },
    "4-3 Reinforcement Learning Data": {
      "score": 0.5,
      "reason": "States that English preference data on the same prompts was used, but provides no dataset size or sourcing details."
    },
    "4-4 Data Filtering": {
      "score": 0.5,
      "reason": "Describes multi-stage filtering for safety and PII with the Google Cloud Sensitive Data Protection Tool, but lacks concrete thresholds or removal ratios."
    }
  },
  "final_score_10pt": 6.875,
  "meta": {
    "usage_from_dispatch": {
      "fine_tuning": "used",
      "rl": "used"
    },
    "excluded": [],
    "denominator": 16,
    "raw_sum": 11.0,
    "scale": "10/16",
    "code_detection_reason": "No training pipeline files; README mentions are ignored.",
    "pretrain_sources_used": false
  }
}