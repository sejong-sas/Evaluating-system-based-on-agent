{
  "1-1 (Weights)": "The quotes state that “Gemma is a family of lightweight, state-of-the-art open models from Google … with open weights for both pre-trained variants and instruction-tuned variants.”  For the target google/gemma-2-2b-it checkpoint this means that the weights themselves are openly released.  Distribution is occurring on Hugging Face, but “To access Gemma on Hugging Face, you’re required to review and agree to Google’s usage license,” so download is gated behind an electronic license click-through.  The model files are provided in `bfloat16` format (“The native weights of this model were exported in `bfloat16` precision.”) and at least one shard is explicitly listed (“model-00001-of-00002.safetensors”), implying a multi-file safetensors upload.  Functionally, the model is described as a “text-to-text, decoder-only large language model … available in English,” making clear that the released weights correspond to a decoder-only architecture suitable for text generation or completion tasks.",
  "1-1 (Weights)__evidence": [
    {
      "source": "[readme]",
      "quote": "Gemma is a family of lightweight, state-of-the-art open models from Google, built from the same research and technology used to create the Gemini models. They are text-to-text, decoder-only large language models, available in English, with open weights for both pre-trained variants and instruction-tuned variants."
    },
    {
      "source": "[readme]",
      "quote": "To access Gemma on Hugging Face, you’re required to review and agree to Google’s usage license."
    },
    {
      "source": "[readme]",
      "quote": "The native weights of this model were exported in `bfloat16` precision."
    },
    {
      "source": "[files]",
      "quote": "model-00001-of-00002.safetensors"
    }
  ],
  "1-2 (Code)": "Inference/serving code is available: “The local-gemma repository contains a lightweight wrapper around Transformers for running Gemma 2 through a command line interface, or CLI.” This public repository simplifies local execution of the 2-billion-parameter Gemma 2 models.  The training stack is mentioned—“Training was done using JAX and ML Pathways”—indicating that Google trained the model with internal JAX/Pathways infrastructure, but the quote does not indicate that this training code has been released.  Consequently, only inference/serving utilities (via the local-gemma wrapper) are openly published, while the end-to-end pre-training or fine-tuning scripts used internally remain undisclosed according to the provided material.",
  "1-2 (Code)__evidence": [
    {
      "source": "[readme]",
      "quote": "The [local-gemma](https://github.com/huggingface/local-gemma) repository contains a lightweight wrapper around Transformers for running Gemma 2 through a command line interface, or CLI."
    },
    {
      "source": "[readme]",
      "quote": "Training was done using [JAX][jax] and [ML Pathways][ml-pathways]."
    }
  ],
  "1-3 (License)": "The model is distributed under a custom license explicitly labeled in the metadata as “license: gemma.”  Users must accept additional legal terms: “**Terms of Use**: [Terms][terms]” and, in practice, download is conditioned on agreement—“To access Gemma on Hugging Face, you’re required to review and agree to Google’s usage license.”  The quotes do not enumerate the clauses, but they make clear that Google’s proprietary ‘Gemma’ terms govern use, redistribution, and modification, and an affirmative click-through is mandatory before obtaining the weights.",
  "1-3 (License)__evidence": [
    {
      "source": "[readme]",
      "quote": "license: gemma"
    },
    {
      "source": "[readme]",
      "quote": "**Terms of Use**: [Terms][terms]"
    },
    {
      "source": "[readme]",
      "quote": "To access Gemma on Hugging Face, you’re required to review and agree to Google’s usage license."
    }
  ],
  "1-4 (Paper)": "Two public technical references are cited.  First, performance and safety benchmarking are covered in “All evaluations are described in detail in [Evaluating Frontier Models for Dangerous Capabilities] and in brief in the [Gemma 2 technical report].” Second, a bibliographic entry is provided for a 2024 article: “@article{gemma_2024, title={Gemma}, url={https://www.kaggle.com/m/3301}, DOI={10.34740/KAGGLE/M/3301}, publisher={Kaggle}, author={Gemma Team}, year={2024}}.”  Together these indicate that the model family, including the 2-2b-it variant, is documented in both a dedicated Gemma 2 technical report and an external Kaggle-hosted article, with additional evaluation details appearing in a frontier-model safety study.",
  "1-4 (Paper)__evidence": [
    {
      "source": "[readme]",
      "quote": "All evaluations are described in detail in [Evaluating Frontier Models for Dangerous Capabilities][eval-danger] and in brief in the [Gemma 2 technical report][tech-report]."
    },
    {
      "source": "[readme]",
      "quote": "@article{gemma_2024,\n    title={Gemma},\n    url={https://www.kaggle.com/m/3301},\n    DOI={10.34740/KAGGLE/M/3301},\n    publisher={Kaggle},\n    author={Gemma Team},\n    year={2024}\n}"
    }
  ],
  "1-5 (Architecture)": "The documentation that explicitly names the target model states: \"The Gemma-2 2b model can be run up to 6x faster by leveraging torch compile.\"  This sentence directly attributes a concrete runtime characteristic—an up-to-six-fold speed-up—to the google/gemma-2-2b-it architecture when the PyTorch compilation path is enabled.  A second sentence provides the only disclosed data-volume figure that also mentions the target size class: \"The 27B model was trained with 13 trillion tokens, the 9B model was trained with 8 trillion tokens, and 2B model was trained with 2 trillion tokens.\"  From that line, the relevant fragment for the 2 B variant is that the model \"was trained with 2 trillion tokens.\"  Taken together, these two quotes yield all public architectural and design details: the model belongs to the 2-billion-parameter class, achieves up to a 6× speed-up under torch compile, and was pre-trained on a corpus containing approximately two trillion tokens.",
  "1-5 (Architecture)__evidence": [
    {
      "source": "[readme]",
      "quote": "The Gemma-2 2b model can be run up to 6x faster by leveraging torch compile."
    },
    {
      "source": "[readme]",
      "quote": "The 27B model was trained with 13 trillion tokens, the 9B model was trained with 8 trillion tokens, and 2B model was trained with 2 trillion tokens."
    }
  ],
  "1-6 (Tokenizer)": "Tokenizer use is demonstrated through the explicit loading example: \"tokenizer = AutoTokenizer.from_pretrained(\\\"google/gemma-2-2b-it\\\")\".  The same section immediately stresses correct chat formatting: \"You can ensure the correct chat template is applied by using `tokenizer.apply_chat_template` as follows:\".  These two sentences convey that a dedicated, downloadable tokenizer artifact is published under the exact model identifier and that proper inference requires applying the bundled chat template through `apply_chat_template`.",
  "1-6 (Tokenizer)__evidence": [
    {
      "source": "[readme]",
      "quote": "tokenizer = AutoTokenizer.from_pretrained(\"google/gemma-2-2b-it\")"
    },
    {
      "source": "[readme]",
      "quote": "You can ensure the correct chat template is applied by using `tokenizer.apply_chat_template` as follows:"
    }
  ],
  "2-1 (Hardware)": "\"Gemma was trained using the latest generation of [Tensor Processing Unit (TPU)][tpu] hardware (TPUv5p).\"  This single statement confirms that google/gemma-2-2b-it was trained on TPUv5p accelerators.",
  "2-1 (Hardware)__evidence": [
    {
      "source": "[readme]",
      "quote": "Gemma was trained using the latest generation of [Tensor Processing Unit (TPU)][tpu] hardware (TPUv5p)."
    }
  ],
  "2-2 (Software)": "\"Training was done using [JAX][jax] and [ML Pathways][ml-pathways].\"  This direct quote establishes that the training stack for gemma-2-2b-it relied on the JAX framework and the ML Pathways system.",
  "2-2 (Software)__evidence": [
    {
      "source": "[readme]",
      "quote": "Training was done using [JAX][jax] and [ML Pathways][ml-pathways]."
    }
  ],
  "2-3 (API)": "The only public conduit that is explicitly tied to gemma-2-2b-it is the open-source “local-gemma” GitHub repository. The project offers “a lightweight wrapper around Transformers for running Gemma 2 through a command line interface, or CLI.” In practice, this means that anyone who has the model weights on disk can launch an interactive prompt from a terminal and send requests directly to the Gemma runtime without needing any remote server, web dashboard, or proprietary endpoint. All interaction—model loading, prompt submission, and text generation—flows through this single CLI entry point, giving developers a self-contained, GPT-style experience while remaining entirely local.",
  "2-3 (API)__evidence": [
    {
      "source": "[readme]",
      "quote": "The [local-gemma](https://github.com/huggingface/local-gemma) repository contains a lightweight wrapper around Transformers\nfor running Gemma 2 through a command line interface, or CLI."
    }
  ],
  "3-1 (Pre-training)": "Pre-training statistics for gemma-2-2b-it are disclosed alongside the rest of the Gemma family: “the 2B model was trained with 2 trillion tokens.” The training run was executed on Google’s latest “TPUv5p” hardware and harnessed a modern JAX software stack coordinated through the “ML Pathways” distributed training system. In short, the 2-billion-parameter checkpoint consumed a 2-trillion-token corpus and was optimized on large-scale TPU infrastructure managed by JAX/Pathways, the same environment that underpins Google’s flagship research models.",
  "3-1 (Pre-training)__evidence": [
    {
      "source": "[readme]",
      "quote": "The 27B model was trained with 13 trillion tokens, the 9B model was trained with 8 trillion tokens, and 2B model was trained with 2 trillion tokens."
    },
    {
      "source": "[readme]",
      "quote": "Gemma was trained using the latest generation of\n[Tensor Processing Unit (TPU)][tpu] hardware (TPUv5p)."
    },
    {
      "source": "[readme]",
      "quote": "Training was done using [JAX][jax] and [ML Pathways][ml-pathways]."
    }
  ],
  "3-2 (Fine-tuning)": "Gemma-2-2b-it belongs to “a family of lightweight, state-of-the-art open models from Google, built from the same research and technology used to create the Gemini models.” Within this family, the models are “text-to-text, decoder-only large language models, available in English,” and Google releases “open weights for both pre-trained variants and instruction-tuned variants.” Consequently, fine-tuning for the 2 B model is delivered as an instruction-tuned checkpoint that is publicly available beside the base model, enabling users to reproduce or extend the instruction-tuning stage with their own datasets.",
  "3-2 (Fine-tuning)__evidence": [
    {
      "source": "[readme]",
      "quote": "Gemma is a family of lightweight, state-of-the-art open models from Google,\nbuilt from the same research and technology used to create the Gemini models.\nThey are text-to-text, decoder-only large language models, available in English,\nwith open weights for both pre-trained variants and instruction-tuned variants."
    }
  ],
  "3-3 (Reinforcement Learning)": "",
  "3-3 (Reinforcement Learning)__evidence": [],
  "4-1 (Pre-training Data)": "The provided quote indicates that the gemma-2-2b model (“2B model”) was pre-trained on 2 trillion tokens drawn from several curated source classes. Those classes explicitly listed are: (1) Web Documents – primarily English web text selected to give the model broad topical and stylistic coverage; (2) Code – snippets and repositories that teach programming-language syntax and patterns for code understanding and generation; and (3) Mathematics – mathematical text that supplies symbolic expressions and logical-reasoning examples. The same quote also places the 2 B parameter model in the context of a family of larger variants (9 B and 27 B) that were trained on proportionately larger corpora (8 T and 13 T tokens, respectively), but the statement confirms that the 2 B variant specifically saw 2 T tokens during pre-training.",
  "4-1 (Pre-training Data)__evidence": [
    {
      "source": "[readme]",
      "quote": "These models were trained on a dataset of text data that includes a wide variety of sources. The 27B model was trained with 13 trillion tokens, the 9B model was trained with 8 trillion tokens, and 2B model was trained with 2 trillion tokens.\nHere are the key components:\n\n* Web Documents: A diverse collection of web text ensures the model is exposed\n  to a broad range of linguistic styles, topics, and vocabulary. Primarily\n  English-language content.\n* Code: Exposing the model to code helps it to learn the syntax and patterns of\n  programming languages, which improves its ability to generate code or\n  understand code-related questions.\n* Mathematics: Training on mathematical text helps the model learn logical\n  reasoning, symbolic representation, and to address mathematical queries."
    }
  ],
  "4-2 (Fine-tuning Data)": "",
  "4-2 (Fine-tuning Data)__evidence": [],
  "4-3 (Reinforcement Learning Data)": "",
  "4-3 (Reinforcement Learning Data)__evidence": [],
  "4-4 (Data Filtering)": "Two filtering practices are explicitly described for the gemma-2-2b pre-training pipeline. First, “Sensitive Data Filtering” employed automated techniques to strip out personal information and other sensitive content before the model saw the data, with the goal of keeping the model safe and reliable. Second, the team applied further filtering “based on content quality and safety” aligned with the project’s stated safety policies, indicating an additional quality-screening stage beyond the sensitive-data removal.",
  "4-4 (Data Filtering)__evidence": [
    {
      "source": "[readme]",
      "quote": "* Sensitive Data Filtering: As part of making Gemma pre-trained models safe and\n  reliable, automated techniques were used to filter out certain personal\n  information and other sensitive data from training sets.\n* Additional methods: Filtering based on content quality and safety in line with\n  [our policies][safety-policies]."
    }
  ],
  "__usage": {
    "fine_tuning": "used",
    "rl": "not_used"
  }
}