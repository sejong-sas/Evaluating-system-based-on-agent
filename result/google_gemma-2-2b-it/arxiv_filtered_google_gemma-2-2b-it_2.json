{
  "1-5 (Architecture)": "Google’s gemma-2-2b-it follows the architectural pattern laid out for the Gemma 2 family. According to the cited text, all Gemma 2 models, including the 2 B parameter variant, are “based on a decoder-only transformer architecture.” The design retains several core elements from the original Gemma release: a fixed context window of 8 192 tokens, Rotary Position Embeddings (RoPE) for positional encoding, and the approximated GeGLU non-linearity. Beyond these carry-overs, Gemma 2 introduces additional transformer refinements—most notably the interleaving of local-global attention blocks and the use of group-query attention. These modifications are intended to improve efficiency and long-context reasoning while keeping the model lightweight enough to fit in the 2 B parameter budget. As a result, gemma-2-2b-it sits in a family that spans 2 B–27 B parameters but keeps a common decoder-only backbone augmented with the techniques listed above.",
  "1-6 (Tokenizer)": "The gemma-2-2b-it model employs the same tokenizer that shipped with Gemma 1 and Google’s Gemini models: a SentencePiece tokenizer configured with three specific preprocessing rules—split digits, preserve whitespace, and encode at the byte level. This tokenizer yields a vocabulary of 256 k subword pieces. The Gemma team highlights that this shared tokenizer is a cornerstone for multilingual support; they cite its role in downstream projects such as the Navarasa 2.0 model covering 15 Indian languages. Because Gemma 2 kept the identical tokenizer spec, any user already relying on the Gemma 1 or Gemini vocabulary can plug into gemma-2-2b-it without conversion or re-training of tokenization assets.",
  "2-1 (Hardware)": "Training of the 2 B model was performed on TPUv5e hardware in a \"2 × 16 × 16\" topology. This layout amounts to 512 TPU chips. The compute strategy applies 512-way data replication with a single shard for the model weights (\"1-way model sharding\"), indicating that the entire 2 B parameter network fits inside the memory of each device replica while data parallelism supplies scale-out throughput.",
  "2-2 (Software)": "gemma-2-2b-it was trained with Google’s GSPMD partitioner orchestrating each training step and compiled through the MegaScale XLA compiler. In line with Gemma 1, GSPMD handles logical partitioning of tensors and operations across the 512 TPUv5e chips, while MegaScale XLA produces the optimized executable kernels. For downstream alignment, the Gemma team states that their fine-tuning pipeline blends Supervised Fine-Tuning (SFT) with Reinforcement Learning from Human Feedback (RLHF) so that the resulting instruction-tuned checkpoint adheres to Google safety policies.",
  "1-5 (Architecture)__evidence": [
    {
      "source": "[sections/Model Architecture]",
      "quote": "Similar to previous Gemma models (Gemma Team, 2024), the Gemma 2 models are based on a decoder-only transformer architecture (Vaswani et al., 2017). A few architectural elements are similar to the first version of Gemma models; namely, a context length of 8192 tokens, the use of Rotary Position Embeddings (RoPE) (Su et al., 2021), and the approximated GeGLU non-linearity (Shazeer, 2020)."
    },
    {
      "source": "[sections/2408.00118]",
      "quote": "In this work, we introduce Gemma 2, a new addition to the Gemma family of lightweight, state-of-the-art open models, ranging in scale from 2 billion to 27 billion parameters. In this new version, we apply several known technical modifications to the Transformer architecture, such as interleaving local-global attentions (Beltagy et al., 2020a) and group-query attention (Ainslie et al., 2023)."
    },
    {
      "source": "[sections/2408.00118]",
      "quote": "Similar to previous Gemma models (Gemma Team, 2024), the Gemma 2 models are based on a decoder-only transformer architecture (Vaswani et al., 2017)."
    }
  ],
  "1-6 (Tokenizer)__evidence": [
    {
      "source": "[sections/Pre-training/Training Data]",
      "quote": "Tokenizer. We use the same tokenizer as Gemma 1 and Gemini: a SentencePiece tokenizer with split digits, preserved whitespace, and byte-level encodings (Kudo and Richardson, 2018). The resulting vocabulary has 256k entries."
    },
    {
      "source": "[sections/2408.00118]",
      "quote": "Tokenizer. We use the same tokenizer as Gemma 1 and Gemini: a SentencePiece tokenizer with split digits, preserved whitespace, and byte-level encodings (Kudo and Richardson, 2018)."
    },
    {
      "source": "[sections/8.1 Impact assessment]",
      "quote": "Since the launch of Gemma 1, we have seen our Gemma models drive a number of socially beneficial applications, relying on Gemma’s unique technologies like its tokenizer to facilitate the creation of multilingual models, such as for Navarasa 2.0, a Gemma tuned model for 15 Indian languages."
    }
  ],
  "2-1 (Hardware)__evidence": [
    {
      "source": "[sections/Compute Infrastructure]",
      "quote": "For the 2B model, we train on a 2×16×16 configuration of TPUv5e, totaling 512 chips, with 512-way data replication and 1-way model sharding."
    },
    {
      "source": "[sections/2408.00118]",
      "quote": "For the 2B model, we train on a 2×16×16 configuration of TPUv5e, totaling 512 chips, with 512-way data replication and 1-way model sharding."
    }
  ],
  "2-2 (Software)__evidence": [
    {
      "source": "[sections/Compute Infrastructure]",
      "quote": "As in Gemma 1, we use the GSPMD partitioner (Xu et al., 2021) for training step computation and the MegaScale XLA compiler (XLA, 2019)."
    },
    {
      "source": "[sections/2408.00118]",
      "quote": "As in Gemma 1, we use the GSPMD partitioner (Xu et al., 2021) for training step computation and the MegaScale XLA compiler (XLA, 2019)."
    },
    {
      "source": "[sections/8.2 Safety policies and train-time mitigations]",
      "quote": "A key pillar of Gemma’s approach to safety is to align fine-tuned models with Google’s safety policies, in line with Gemini models (Gemini Team, 2023). For fine-tuned models, we also use both SFT and RLHF to steer the model away from undesirable behavior."
    }
  ]
}