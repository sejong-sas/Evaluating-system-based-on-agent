{
  "1-5 (Architecture)": "Gemma 2 retains the same fundamental design principles as the earlier Gemma line: it is a decoder-only Transformer (Vaswani et al., 2017). The public description emphasizes several concrete hyper-parameter choices that mirror the first Gemma release. Each 2 B parameter variant (and the larger siblings in the same family) is built to handle a context window of 8 192 tokens. Rotary Position Embeddings (RoPE) are used for positional encoding, and the feed-forward blocks rely on the approximated GeGLU non-linearity. The paper also signals that the main layer counts, hidden sizes, and other core figures are summarized in an accompanying “Table 1.” Beyond the text-only backbone, the authors report combining the SigLIP-So400m vision encoder—previously used in PaliGemma—with every Gemma 2 size, from the 2 B up to a 27 B variant. Multi-stage training is carried out at three image resolutions (224 px, 448 px, and 896 px) to furnish the models with transferable multimodal capabilities. In short, the 2 B configuration inherits Gemma’s decoder-only Transformer core, 8 192-token context, RoPE, and GeGLU, and it can be paired with a SigLIP-So400m vision front-end for multimodal use cases.",
  "1-6 (Tokenizer)": "For Gemma 2-2B-IT, no new tokenizer is introduced; instead, the model re-uses the identical SentencePiece tokenizer deployed in Gemma 1 and the Gemini series. That tokenizer keeps split digits, preserves white-space tokens, and operates with byte-level encodings. The published vocabulary size is 256 000 tokens, so downstream users inherit the same 256 k sub-word inventory that ships with prior Gemma checkpoints.",
  "2-1 (Hardware)": "Training was conducted exclusively on Google TPUs. The authors cite experiments on TPU v4, TPU v5e, and TPU v5p. For the Gemma 2 2 B model specifically, they provide an explicit topology: a 2 × 16 × 16 mesh of TPU v5e devices, which totals 512 separate chips. The run distributed the workload with 512-way data parallelism (one replica per chip) and performed no intra-model partitioning beyond that: ‘1-way model sharding.’",
  "2-2 (Software)": "The Gemma 2 training stack follows the same recipe as Gemma 1. Computation is partitioned with GSPMD, and the low-level compilation is handled by the MegaScale XLA compiler. No other frameworks or optimizer libraries are enumerated in the excerpt, indicating the in-house JAX/XLA workflow remained unchanged for the 2 B training runs.",
  "1-5 (Architecture)__evidence": [
    {
      "source": "[pdf_text]",
      "quote": "Similar to previous Gemma models (Gemma Team, 2024), the Gemma 2 models are based on a decoder-only transformer architecture (Vaswani et al., 2017)."
    },
    {
      "source": "[pdf_text]",
      "quote": "A few architectural elements are similar to the first version of Gemma models; namely, a context length of 8192 tokens, the use of Rotary Position Embeddings (RoPE) (Su et al., 2021), and the approximated GeGLU non-linearity (Shazeer, 2020)."
    },
    {
      "source": "[sections/https://storage.googleapis.com/deepmind-media/gemma/gemma-2-report.pdf]",
      "quote": "Similar to previous Gemma models (Gemma Team, 2024), the Gemma 2 models are based on a decoder-only transformer architecture (Vaswani et al., 2017). We summarize the main parameters and architecture choices in Table 1."
    },
    {
      "source": "[sections/https://storage.googleapis.com/deepmind-media/gemma/gemma-2-report.pdf]",
      "quote": "A few architectural elements are similar to the first version of Gemma models; namely, a context\nlength of 8192 tokens, the use of Rotary Posi-\ntion Embeddings (RoPE) (Su et al., 2021), and\nthe approximated GeGLU non-linearity (Shazeer,\n2020)."
    },
    {
      "source": "[sections/https://arxiv.org/abs/2412.03555]",
      "quote": "We combine the SigLIP-So400m vision encoder that was also used by PaliGemma with the whole range of Gemma 2 models, from the 2B one all the way up to the 27B model. We train these models at three resolutions (224px, 448px, and 896px) in multiple stages to equip them with broad knowledge for transfer via fine-tuning."
    }
  ],
  "1-6 (Tokenizer)__evidence": [
    {
      "source": "[pdf_text]",
      "quote": "Tokenizer. We use the same tokenizer as Gemma 1 and Gemini: a SentencePiece tokenizer with split digits, preserved whitespace, and byte-level encodings (Kudo and Richardson, 2018). The resulting vocabulary has 256k entries."
    },
    {
      "source": "[sections/https://storage.googleapis.com/deepmind-media/gemma/gemma-2-report.pdf]",
      "quote": "Tokenizer. We use the same tokenizer as Gemma 1 and Gemini: a SentencePiece tokenizer with split digits, preserved whitespace, and byte-level encodings (Kudo and Richardson, 2018). The resulting vocabulary has 256k entries."
    }
  ],
  "2-1 (Hardware)__evidence": [
    {
      "source": "[pdf_text]",
      "quote": "We train our models with TPUv4, TPUv5e, and TPUv5p as outlined in Table 3. For the 2B model, we train on a 2×16×16 configuration of TPUv5e, totaling 512 chips, with 512-way data replication and 1-way model sharding."
    },
    {
      "source": "[sections/https://storage.googleapis.com/deepmind-media/gemma/gemma-2-report.pdf]",
      "quote": "For the 2B model, we train on a 2x16x16 configuration of TPUv5e, totaling 512 chips, with 512-way data replication and 1-way model sharding."
    }
  ],
  "2-2 (Software)__evidence": [
    {
      "source": "[pdf_text]",
      "quote": "As in Gemma 1, we use the GSPMD partitioner (Xu et al., 2021) for training step computation and the MegaScale XLA compiler (XLA, 2019)."
    },
    {
      "source": "[sections/https://storage.googleapis.com/deepmind-media/gemma/gemma-2-report.pdf]",
      "quote": "As in Gemma 1, we use the GSPMD partitioner (Xu et al., 2021) for training step computation and the MegaScale XLA compiler (XLA, 2019)."
    }
  ]
}