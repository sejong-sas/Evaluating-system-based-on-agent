{
  "model_id": "google/gemma-3-4b-it",
  "full_texts": [
    {
      "arxiv_id": "https://ai.google.dev/gemma/docs/core",
      "full_text": " Gemma 3 model overview &nbsp;|&nbsp; Google AI for Developers Skip to main content Models Gemini About Docs API reference Pricing Imagen About Docs Pricing Veo About Docs Pricing Gemma About Docs Gemmaverse Solutions Build with Gemini Gemini API Google AI Studio Customize Gemma open models Gemma open models Multi-framework with Keras Fine-tune in Colab Run on-device Google AI Edge Gemini Nano on Android Chrome built-in web APIs Build responsibly Responsible GenAI Toolkit Secure AI Framework Code assistance Android Studio Chrome DevTools Colab Firebase Google Cloud JetBrains Jules VS Code Showcase Gemini Showcase Gemini API Developer Competition Community Google AI Forum Gemini for Research / English Deutsch Español – América Latina Français Indonesia Italiano Polski Português – Brasil Shqip Tiếng Việt Türkçe Русский עברית العربيّة فارسی हिंदी বাংলা ภาษาไทย 中文 – 简体 中文 – 繁體 日本語 한국어 Sign in Gemma Docs Models More Gemma Docs Solutions More Code assistance More Showcase More Community More Overview Get started Releases Models Gemma 3 Overview Model card Gemma 2 model card Gemma 1 model card Gemma 3n Overview Model card EmbeddingGemma Overview Model card Generate embeddings with Sentence Transformers Fine-tune EmbeddingGemma CodeGemma Overview Model card Generate code with Keras Generate code with JAX and Flax Code assist with Keras Prompt and system instructions PaliGemma 2 Overview v2 model card v1 model card Generate output with Keras Fine-tune with JAX and Flax Prompt and system instructions ShieldGemma 2 Overview ShieldGemma 2 Model card ShieldGemma 1 Model card Run Gemma Overview Hugging Face Transformers Ollama Gemma library Keras PyTorch Gemma.cpp Gemini API Cloud GKE Cloud Run Prompt and system instructions Gemma setup Capabilities Function calling Visual data processing Overview Image interpretation Video understanding Content creation Audio data processing Tuning guides Overview Tune using LoRA and Keras Tune using Gemma library Tune using Hugging Face Transformers and QLoRA Vision Tune using Hugging Face Transformers and QLoRA Full model fine-tune using Hugging Face Transformers Distributed tuning using Keras Application guides Personal code assistant Business email assistant Spoken language tasks Chatbot using Python Meme Generator Conversion guides Convert Hugging Face Safetensors to MediaPipe Task Deployment guides Web Mobile Google Cloud LangChain Research and tools RecurrentGemma Overview Inference using JAX and Flax Fine-tune using JAX and Flax Model card DataGemma Gemma Scope Gemma-APS Community Gemmaverse Discord Legal Terms of use Prohibited use Intended use statement Gemini About Docs API reference Pricing Imagen About Docs Pricing Veo About Docs Pricing Gemma About Docs Gemmaverse Build with Gemini Gemini API Google AI Studio Customize Gemma open models Gemma open models Multi-framework with Keras Fine-tune in Colab Run on-device Google AI Edge Gemini Nano on Android Chrome built-in web APIs Build responsibly Responsible GenAI Toolkit Secure AI Framework Android Studio Chrome DevTools Colab Firebase Google Cloud JetBrains Jules VS Code Gemini Showcase Gemini API Developer Competition Google AI Forum Gemini for Research Gemma 3n released with audio input and optimized for use in everyday devices! Learn more Home Gemma Models Docs Send feedback Gemma 3 model overview Gemma is a family of generative artificial intelligence (AI) models and you can use them in a wide variety of generation tasks, including question answering, summarization, and reasoning. Gemma models are provided with open weights and permit responsible commercial use , allowing you to tune and deploy them in your own projects and applications. The Gemma 3 release includes the following key features. Try it in AI Studio : Image and text input : Multimodal capabilities let you input images and text to understand and analyze visual data. Start building 128K token context : Significantly large input context for analyzing more data and solving more complex problems. Function calling : Build natural language interfaces for working with programming interfaces. Start building Wide language support : Work in your language or expand your AI application&#39;s language capabilities with support for over 140 languages. Start building Developer friendly model sizes : Choose a model size (270M, 1B, 4B, 12B, 27B) and precision level that works best for your task and compute resources. You can download Gemma 3 models from Kaggle and Hugging Face . For more technical details on Gemma 3, see the Model Card and Technical Report . Earlier versions of Gemma core models are also available for download. For more information, see Previous Gemma models . Try Gemma 3 Get it on Kaggle Get it on Hugging Face Multimodal image and text input You can tackle complex analysis and generation tasks with Gemma 3 with its ability to handle image and text data. You can use the model to interpret image data, identify objects, extract text data, and complete many other visual input to text output tasks. Start building Important: The Gemma 3 270M and 1B models are text only and do not support image input . 128K token context window Gemma 3 models (4B, 12B, and 27B) can handle prompt inputs up to 128K tokens, a 16x larger context window than previous Gemma models. The large number of tokens means you can process several, multi page articles, larger single articles, or hundreds of images in a single prompt. Important: The Gemma 3 270M and 1B models can process up to 32k tokens. Wide language support Work in your own language with built-in support for over 140 languages. Gemma 3 is trained to support a large number of languages compared to previous Gemma versions, letting you take on more visual and text tasks in the languages your customers use. Start building Function calling Build intelligent, natural language controls for programming interfaces. Gemma 3 lets you define coding functions with specific syntax and constraints, and the model can call these functions to complete tasks. Start building Parameter sizes and quantization Gemma 3 models are available in 5 parameter sizes: 270M, 1B, 4B, 12B, and 27B. The models can be used with their default precision (16-bit) or with a lower precision using quantization. The different sizes and precisions represent a set of trade-offs for your AI application. Models with higher parameters and bit counts (higher precision) are generally more capable, but are more expensive to run in terms of processing cycles, memory cost and power consumption. Models with lower parameters and bit counts (lower precision) have less capabilities, but may be sufficient for your AI task. For all Gemma 3 models, Quantization-Aware Trained checkpoints are provided, which allow quantizing (reducing the precision), while preserving high-quality. The following table details the approximate GPU or TPU memory requirements for running inference with each size of the Gemma 3 model versions. Note that the numbers may changed based on inference tool. Parameters BF16 (16-bit) SFP8 (8-bit) Q4_0 (4-bit) Gemma 3 270M ( text only ) 400 MB 297 MB 240 MB Gemma 3 1B ( text only ) 1.5 GB 1.1 GB 892 MB Gemma 3 4B 6.4 GB 4.4 GB 3.4 GB Gemma 3 12B 20 GB 12.2 GB 8.7 GB Gemma 3 27B 46.4 GB 29.1 GB 21 GB Table 1. Approximate GPU or TPU memory required to load Gemma 3 models based on parameter count and quantization level. Caution: These estimates only include the memory required to load the models. They don&#39;t include the additional memory required for the prompt tokens or supporting software. Memory consumption increases based on the total number of tokens required for the prompt you run. The larger the number of tokens required to process your prompt, the higher the memory required, which is in addition to the memory required to load the model. Note: Memory requirements for fine-tuning Gemma models are significantly higher than running inference. The requirements depend on the development framework and tuning technique you use, such as Low Rank Adapter (LoRA) versus full-precision tuning. Previous Gemma models You can work with previous generations of Gemma models, which are also available from Kaggle and Hugging Face . For more technical details about previous Gemma models, see the following model card pages: Gemma 2 Model Card Gemma 1 Model Card Ready to start building? Get started with Gemma models! Send feedback Except as otherwise noted, the content of this page is licensed under the Creative Commons Attribution 4.0 License , and code samples are licensed under the Apache 2.0 License . For details, see the Google Developers Site Policies . Java is a registered trademark of Oracle and/or its affiliates. Last updated 2025-08-14 UTC. Need to tell us more? [[[\"Easy to understand\",\"easyToUnderstand\",\"thumb-up\"],[\"Solved my problem\",\"solvedMyProblem\",\"thumb-up\"],[\"Other\",\"otherUp\",\"thumb-up\"]],[[\"Missing the information I need\",\"missingTheInformationINeed\",\"thumb-down\"],[\"Too complicated / too many steps\",\"tooComplicatedTooManySteps\",\"thumb-down\"],[\"Out of date\",\"outOfDate\",\"thumb-down\"],[\"Samples / code issue\",\"samplesCodeIssue\",\"thumb-down\"],[\"Other\",\"otherDown\",\"thumb-down\"]],[\"Last updated 2025-08-14 UTC.\"],[],[],null,[]] Terms Privacy Manage cookies English Deutsch Español – América Latina Français Indonesia Italiano Polski Português – Brasil Shqip Tiếng Việt Türkçe Русский עברית العربيّة فارسی हिंदी বাংলা ภาษาไทย 中文 – 简体 中文 – 繁體 日本語 한국어 ",
      "fetch_method": "direct-html"
    },
    {
      "arxiv_id": "https://arxiv.org/abs/1905.07830",
      "full_text": " [1905.07830] HellaSwag: Can a Machine Really Finish Your Sentence? Skip to main content We gratefully acknowledge support from the Simons Foundation, member institutions , and all contributors. Donate &gt; cs &gt; arXiv:1905.07830 Help | Advanced Search All fields Title Author Abstract Comments Journal reference ACM classification MSC classification Report number arXiv identifier DOI ORCID arXiv author ID Help pages Full text Search open search GO open navigation menu quick links Login Help Pages About --> Computer Science > Computation and Language arXiv:1905.07830 (cs) [Submitted on 19 May 2019] Title: HellaSwag: Can a Machine Really Finish Your Sentence? Authors: Rowan Zellers , Ari Holtzman , Yonatan Bisk , Ali Farhadi , Yejin Choi View a PDF of the paper titled HellaSwag: Can a Machine Really Finish Your Sentence?, by Rowan Zellers and 4 other authors View PDF Abstract: Recent work by Zellers et al. (2018) introduced a new task of commonsense natural language inference: given an event description such as &#34;A woman sits at a piano,&#34; a machine must select the most likely followup: &#34;She sets her fingers on the keys.&#34; With the introduction of BERT, near human-level performance was reached. Does this mean that machines can perform human level commonsense inference? In this paper, we show that commonsense inference still proves difficult for even state-of-the-art models, by presenting HellaSwag, a new challenge dataset. Though its questions are trivial for humans (&gt;95% accuracy), state-of-the-art models struggle (&lt;48%). We achieve this via Adversarial Filtering (AF), a data collection paradigm wherein a series of discriminators iteratively select an adversarial set of machine-generated wrong answers. AF proves to be surprisingly robust. The key insight is to scale up the length and complexity of the dataset examples towards a critical &#39;Goldilocks&#39; zone wherein generated text is ridiculous to humans, yet often misclassified by state-of-the-art models. Our construction of HellaSwag, and its resulting difficulty, sheds light on the inner workings of deep pretrained models. More broadly, it suggests a new path forward for NLP research, in which benchmarks co-evolve with the evolving state-of-the-art in an adversarial way, so as to present ever-harder challenges. Comments: ACL 2019. Project page at this https URL Subjects: Computation and Language (cs.CL) Cite as: arXiv:1905.07830 [cs.CL] &nbsp; (or arXiv:1905.07830v1 [cs.CL] for this version) &nbsp; https://doi.org/10.48550/arXiv.1905.07830 Focus to learn more arXiv-issued DOI via DataCite Submission history From: Rowan Zellers [ view email ] [v1] Sun, 19 May 2019 23:57:23 UTC (1,063 KB) Full-text links: Access Paper: View a PDF of the paper titled HellaSwag: Can a Machine Really Finish Your Sentence?, by Rowan Zellers and 4 other authors View PDF TeX Source Other Formats view license Current browse context: cs.CL &lt;&nbsp;prev &nbsp; | &nbsp; next&nbsp;&gt; new | recent | 2019-05 Change to browse by: cs References &amp; Citations NASA ADS Google Scholar Semantic Scholar 1 blog link ( what is this? ) DBLP - CS Bibliography listing | bibtex Rowan Zellers Ari Holtzman Yonatan Bisk Ali Farhadi Yejin Choi export BibTeX citation Loading... BibTeX formatted citation &times; loading... Data provided by: Bookmark Bibliographic Tools Bibliographic and Citation Tools Bibliographic Explorer Toggle Bibliographic Explorer ( What is the Explorer? ) Connected Papers Toggle Connected Papers ( What is Connected Papers? ) Litmaps Toggle Litmaps ( What is Litmaps? ) scite.ai Toggle scite Smart Citations ( What are Smart Citations? ) Code, Data, Media Code, Data and Media Associated with this Article alphaXiv Toggle alphaXiv ( What is alphaXiv? ) Links to Code Toggle CatalyzeX Code Finder for Papers ( What is CatalyzeX? ) DagsHub Toggle DagsHub ( What is DagsHub? ) GotitPub Toggle Gotit.pub ( What is GotitPub? ) Huggingface Toggle Hugging Face ( What is Huggingface? ) Links to Code Toggle Papers with Code ( What is Papers with Code? ) ScienceCast Toggle ScienceCast ( What is ScienceCast? ) Demos Demos Replicate Toggle Replicate ( What is Replicate? ) Spaces Toggle Hugging Face Spaces ( What is Spaces? ) Spaces Toggle TXYZ.AI ( What is TXYZ.AI? ) Related Papers Recommenders and Search Tools Link to Influence Flower Influence Flower ( What are Influence Flowers? ) Core recommender toggle CORE Recommender ( What is CORE? ) Author Venue Institution Topic About arXivLabs arXivLabs: experimental projects with community collaborators arXivLabs is a framework that allows collaborators to develop and share new arXiv features directly on our website. Both individuals and organizations that work with arXivLabs have embraced and accepted our values of openness, community, excellence, and user data privacy. arXiv is committed to these values and only works with partners that adhere to them. Have an idea for a project that will add value for arXiv's community? Learn more about arXivLabs . Which authors of this paper are endorsers? | Disable MathJax ( What is MathJax? ) About Help contact arXiv Click here to contact arXiv Contact subscribe to arXiv mailings Click here to subscribe Subscribe Copyright Privacy Policy Web Accessibility Assistance arXiv Operational Status Get status notifications via email or slack ",
      "fetch_method": "direct-html"
    },
    {
      "arxiv_id": "https://arxiv.org/abs/1905.10044",
      "full_text": " [1905.10044] BoolQ: Exploring the Surprising Difficulty of Natural Yes/No Questions Skip to main content We gratefully acknowledge support from the Simons Foundation, member institutions , and all contributors. Donate &gt; cs &gt; arXiv:1905.10044 Help | Advanced Search All fields Title Author Abstract Comments Journal reference ACM classification MSC classification Report number arXiv identifier DOI ORCID arXiv author ID Help pages Full text Search open search GO open navigation menu quick links Login Help Pages About --> Computer Science > Computation and Language arXiv:1905.10044 (cs) [Submitted on 24 May 2019] Title: BoolQ: Exploring the Surprising Difficulty of Natural Yes/No Questions Authors: Christopher Clark , Kenton Lee , Ming-Wei Chang , Tom Kwiatkowski , Michael Collins , Kristina Toutanova View a PDF of the paper titled BoolQ: Exploring the Surprising Difficulty of Natural Yes/No Questions, by Christopher Clark and 5 other authors View PDF Abstract: In this paper we study yes/no questions that are naturally occurring --- meaning that they are generated in unprompted and unconstrained settings. We build a reading comprehension dataset, BoolQ, of such questions, and show that they are unexpectedly challenging. They often query for complex, non-factoid information, and require difficult entailment-like inference to solve. We also explore the effectiveness of a range of transfer learning baselines. We find that transferring from entailment data is more effective than transferring from paraphrase or extractive QA data, and that it, surprisingly, continues to be very beneficial even when starting from massive pre-trained language models such as BERT. Our best method trains BERT on MultiNLI and then re-trains it on our train set. It achieves 80.4% accuracy compared to 90% accuracy of human annotators (and 62% majority-baseline), leaving a significant gap for future work. Comments: In NAACL 2019 Subjects: Computation and Language (cs.CL) Cite as: arXiv:1905.10044 [cs.CL] &nbsp; (or arXiv:1905.10044v1 [cs.CL] for this version) &nbsp; https://doi.org/10.48550/arXiv.1905.10044 Focus to learn more arXiv-issued DOI via DataCite Submission history From: Christopher Clark [ view email ] [v1] Fri, 24 May 2019 05:48:49 UTC (43 KB) Full-text links: Access Paper: View a PDF of the paper titled BoolQ: Exploring the Surprising Difficulty of Natural Yes/No Questions, by Christopher Clark and 5 other authors View PDF TeX Source Other Formats view license Current browse context: cs.CL &lt;&nbsp;prev &nbsp; | &nbsp; next&nbsp;&gt; new | recent | 2019-05 Change to browse by: cs References &amp; Citations NASA ADS Google Scholar Semantic Scholar 1 blog link ( what is this? ) DBLP - CS Bibliography listing | bibtex Christopher Clark Kenton Lee Ming-Wei Chang Tom Kwiatkowski Michael Collins &hellip; export BibTeX citation Loading... BibTeX formatted citation &times; loading... Data provided by: Bookmark Bibliographic Tools Bibliographic and Citation Tools Bibliographic Explorer Toggle Bibliographic Explorer ( What is the Explorer? ) Connected Papers Toggle Connected Papers ( What is Connected Papers? ) Litmaps Toggle Litmaps ( What is Litmaps? ) scite.ai Toggle scite Smart Citations ( What are Smart Citations? ) Code, Data, Media Code, Data and Media Associated with this Article alphaXiv Toggle alphaXiv ( What is alphaXiv? ) Links to Code Toggle CatalyzeX Code Finder for Papers ( What is CatalyzeX? ) DagsHub Toggle DagsHub ( What is DagsHub? ) GotitPub Toggle Gotit.pub ( What is GotitPub? ) Huggingface Toggle Hugging Face ( What is Huggingface? ) Links to Code Toggle Papers with Code ( What is Papers with Code? ) ScienceCast Toggle ScienceCast ( What is ScienceCast? ) Demos Demos Replicate Toggle Replicate ( What is Replicate? ) Spaces Toggle Hugging Face Spaces ( What is Spaces? ) Spaces Toggle TXYZ.AI ( What is TXYZ.AI? ) Related Papers Recommenders and Search Tools Link to Influence Flower Influence Flower ( What are Influence Flowers? ) Core recommender toggle CORE Recommender ( What is CORE? ) Author Venue Institution Topic About arXivLabs arXivLabs: experimental projects with community collaborators arXivLabs is a framework that allows collaborators to develop and share new arXiv features directly on our website. Both individuals and organizations that work with arXivLabs have embraced and accepted our values of openness, community, excellence, and user data privacy. arXiv is committed to these values and only works with partners that adhere to them. Have an idea for a project that will add value for arXiv's community? Learn more about arXivLabs . Which authors of this paper are endorsers? | Disable MathJax ( What is MathJax? ) About Help contact arXiv Click here to contact arXiv Contact subscribe to arXiv mailings Click here to subscribe Subscribe Copyright Privacy Policy Web Accessibility Assistance arXiv Operational Status Get status notifications via email or slack ",
      "fetch_method": "direct-html"
    },
    {
      "arxiv_id": "https://arxiv.org/abs/1911.11641",
      "full_text": " [1911.11641] PIQA: Reasoning about Physical Commonsense in Natural Language Skip to main content We gratefully acknowledge support from the Simons Foundation, member institutions , and all contributors. Donate &gt; cs &gt; arXiv:1911.11641 Help | Advanced Search All fields Title Author Abstract Comments Journal reference ACM classification MSC classification Report number arXiv identifier DOI ORCID arXiv author ID Help pages Full text Search open search GO open navigation menu quick links Login Help Pages About --> Computer Science > Computation and Language arXiv:1911.11641 (cs) [Submitted on 26 Nov 2019] Title: PIQA: Reasoning about Physical Commonsense in Natural Language Authors: Yonatan Bisk , Rowan Zellers , Ronan Le Bras , Jianfeng Gao , Yejin Choi View a PDF of the paper titled PIQA: Reasoning about Physical Commonsense in Natural Language, by Yonatan Bisk and 4 other authors View PDF Abstract: To apply eyeshadow without a brush, should I use a cotton swab or a toothpick? Questions requiring this kind of physical commonsense pose a challenge to today&#39;s natural language understanding systems. While recent pretrained models (such as BERT) have made progress on question answering over more abstract domains - such as news articles and encyclopedia entries, where text is plentiful - in more physical domains, text is inherently limited due to reporting bias. Can AI systems learn to reliably answer physical common-sense questions without experiencing the physical world? In this paper, we introduce the task of physical commonsense reasoning and a corresponding benchmark dataset Physical Interaction: Question Answering or PIQA. Though humans find the dataset easy (95% accuracy), large pretrained models struggle (77%). We provide analysis about the dimensions of knowledge that existing models lack, which offers significant opportunities for future research. Comments: AAAI 2020 Subjects: Computation and Language (cs.CL) ; Artificial Intelligence (cs.AI); Machine Learning (cs.LG) Cite as: arXiv:1911.11641 [cs.CL] &nbsp; (or arXiv:1911.11641v1 [cs.CL] for this version) &nbsp; https://doi.org/10.48550/arXiv.1911.11641 Focus to learn more arXiv-issued DOI via DataCite Submission history From: Yonatan Bisk [ view email ] [v1] Tue, 26 Nov 2019 15:31:46 UTC (854 KB) Full-text links: Access Paper: View a PDF of the paper titled PIQA: Reasoning about Physical Commonsense in Natural Language, by Yonatan Bisk and 4 other authors View PDF TeX Source Other Formats view license Current browse context: cs.CL &lt;&nbsp;prev &nbsp; | &nbsp; next&nbsp;&gt; new | recent | 2019-11 Change to browse by: cs cs.AI cs.LG References &amp; Citations NASA ADS Google Scholar Semantic Scholar DBLP - CS Bibliography listing | bibtex Yonatan Bisk Rowan Zellers Ronan Le Bras Jianfeng Gao Yejin Choi a export BibTeX citation Loading... BibTeX formatted citation &times; loading... Data provided by: Bookmark Bibliographic Tools Bibliographic and Citation Tools Bibliographic Explorer Toggle Bibliographic Explorer ( What is the Explorer? ) Connected Papers Toggle Connected Papers ( What is Connected Papers? ) Litmaps Toggle Litmaps ( What is Litmaps? ) scite.ai Toggle scite Smart Citations ( What are Smart Citations? ) Code, Data, Media Code, Data and Media Associated with this Article alphaXiv Toggle alphaXiv ( What is alphaXiv? ) Links to Code Toggle CatalyzeX Code Finder for Papers ( What is CatalyzeX? ) DagsHub Toggle DagsHub ( What is DagsHub? ) GotitPub Toggle Gotit.pub ( What is GotitPub? ) Huggingface Toggle Hugging Face ( What is Huggingface? ) Links to Code Toggle Papers with Code ( What is Papers with Code? ) ScienceCast Toggle ScienceCast ( What is ScienceCast? ) Demos Demos Replicate Toggle Replicate ( What is Replicate? ) Spaces Toggle Hugging Face Spaces ( What is Spaces? ) Spaces Toggle TXYZ.AI ( What is TXYZ.AI? ) Related Papers Recommenders and Search Tools Link to Influence Flower Influence Flower ( What are Influence Flowers? ) Core recommender toggle CORE Recommender ( What is CORE? ) Author Venue Institution Topic About arXivLabs arXivLabs: experimental projects with community collaborators arXivLabs is a framework that allows collaborators to develop and share new arXiv features directly on our website. Both individuals and organizations that work with arXivLabs have embraced and accepted our values of openness, community, excellence, and user data privacy. arXiv is committed to these values and only works with partners that adhere to them. Have an idea for a project that will add value for arXiv's community? Learn more about arXivLabs . Which authors of this paper are endorsers? | Disable MathJax ( What is MathJax? ) About Help contact arXiv Click here to contact arXiv Contact subscribe to arXiv mailings Click here to subscribe Subscribe Copyright Privacy Policy Web Accessibility Assistance arXiv Operational Status Get status notifications via email or slack ",
      "fetch_method": "direct-html"
    },
    {
      "arxiv_id": "https://arxiv.org/abs/1904.09728",
      "full_text": " [1904.09728] SocialIQA: Commonsense Reasoning about Social Interactions Skip to main content We gratefully acknowledge support from the Simons Foundation, member institutions , and all contributors. Donate &gt; cs &gt; arXiv:1904.09728 Help | Advanced Search All fields Title Author Abstract Comments Journal reference ACM classification MSC classification Report number arXiv identifier DOI ORCID arXiv author ID Help pages Full text Search open search GO open navigation menu quick links Login Help Pages About --> Computer Science > Computation and Language arXiv:1904.09728 (cs) [Submitted on 22 Apr 2019 ( v1 ), last revised 9 Sep 2019 (this version, v3)] Title: SocialIQA: Commonsense Reasoning about Social Interactions Authors: Maarten Sap , Hannah Rashkin , Derek Chen , Ronan LeBras , Yejin Choi View a PDF of the paper titled SocialIQA: Commonsense Reasoning about Social Interactions, by Maarten Sap and 4 other authors View PDF Abstract: We introduce Social IQa, the first largescale benchmark for commonsense reasoning about social situations. Social IQa contains 38,000 multiple choice questions for probing emotional and social intelligence in a variety of everyday situations (e.g., Q: &#34;Jordan wanted to tell Tracy a secret, so Jordan leaned towards Tracy. Why did Jordan do this?&#34; A: &#34;Make sure no one else could hear&#34;). Through crowdsourcing, we collect commonsense questions along with correct and incorrect answers about social interactions, using a new framework that mitigates stylistic artifacts in incorrect answers by asking workers to provide the right answer to a different but related question. Empirical results show that our benchmark is challenging for existing question-answering models based on pretrained language models, compared to human performance (&gt;20% gap). Notably, we further establish Social IQa as a resource for transfer learning of commonsense knowledge, achieving state-of-the-art performance on multiple commonsense reasoning tasks (Winograd Schemas, COPA). Comments: the first two authors contributed equally; accepted to EMNLP 2019; camera ready version Subjects: Computation and Language (cs.CL) Cite as: arXiv:1904.09728 [cs.CL] &nbsp; (or arXiv:1904.09728v3 [cs.CL] for this version) &nbsp; https://doi.org/10.48550/arXiv.1904.09728 Focus to learn more arXiv-issued DOI via DataCite Submission history From: Maarten Sap [ view email ] [v1] Mon, 22 Apr 2019 05:36:37 UTC (313 KB) [v2] Sat, 17 Aug 2019 00:10:30 UTC (785 KB) [v3] Mon, 9 Sep 2019 17:29:55 UTC (358 KB) Full-text links: Access Paper: View a PDF of the paper titled SocialIQA: Commonsense Reasoning about Social Interactions, by Maarten Sap and 4 other authors View PDF TeX Source Other Formats view license Current browse context: cs.CL &lt;&nbsp;prev &nbsp; | &nbsp; next&nbsp;&gt; new | recent | 2019-04 Change to browse by: cs References &amp; Citations NASA ADS Google Scholar Semantic Scholar DBLP - CS Bibliography listing | bibtex Maarten Sap Hannah Rashkin Derek Chen Ronan LeBras Yejin Choi a export BibTeX citation Loading... BibTeX formatted citation &times; loading... Data provided by: Bookmark Bibliographic Tools Bibliographic and Citation Tools Bibliographic Explorer Toggle Bibliographic Explorer ( What is the Explorer? ) Connected Papers Toggle Connected Papers ( What is Connected Papers? ) Litmaps Toggle Litmaps ( What is Litmaps? ) scite.ai Toggle scite Smart Citations ( What are Smart Citations? ) Code, Data, Media Code, Data and Media Associated with this Article alphaXiv Toggle alphaXiv ( What is alphaXiv? ) Links to Code Toggle CatalyzeX Code Finder for Papers ( What is CatalyzeX? ) DagsHub Toggle DagsHub ( What is DagsHub? ) GotitPub Toggle Gotit.pub ( What is GotitPub? ) Huggingface Toggle Hugging Face ( What is Huggingface? ) Links to Code Toggle Papers with Code ( What is Papers with Code? ) ScienceCast Toggle ScienceCast ( What is ScienceCast? ) Demos Demos Replicate Toggle Replicate ( What is Replicate? ) Spaces Toggle Hugging Face Spaces ( What is Spaces? ) Spaces Toggle TXYZ.AI ( What is TXYZ.AI? ) Related Papers Recommenders and Search Tools Link to Influence Flower Influence Flower ( What are Influence Flowers? ) Core recommender toggle CORE Recommender ( What is CORE? ) Author Venue Institution Topic About arXivLabs arXivLabs: experimental projects with community collaborators arXivLabs is a framework that allows collaborators to develop and share new arXiv features directly on our website. Both individuals and organizations that work with arXivLabs have embraced and accepted our values of openness, community, excellence, and user data privacy. arXiv is committed to these values and only works with partners that adhere to them. Have an idea for a project that will add value for arXiv's community? Learn more about arXivLabs . Which authors of this paper are endorsers? | Disable MathJax ( What is MathJax? ) About Help contact arXiv Click here to contact arXiv Contact subscribe to arXiv mailings Click here to subscribe Subscribe Copyright Privacy Policy Web Accessibility Assistance arXiv Operational Status Get status notifications via email or slack ",
      "fetch_method": "direct-html"
    },
    {
      "arxiv_id": "https://arxiv.org/abs/1705.03551",
      "full_text": " [1705.03551] TriviaQA: A Large Scale Distantly Supervised Challenge Dataset for Reading Comprehension Skip to main content We gratefully acknowledge support from the Simons Foundation, member institutions , and all contributors. Donate &gt; cs &gt; arXiv:1705.03551 Help | Advanced Search All fields Title Author Abstract Comments Journal reference ACM classification MSC classification Report number arXiv identifier DOI ORCID arXiv author ID Help pages Full text Search open search GO open navigation menu quick links Login Help Pages About --> Computer Science > Computation and Language arXiv:1705.03551 (cs) [Submitted on 9 May 2017 ( v1 ), last revised 13 May 2017 (this version, v2)] Title: TriviaQA: A Large Scale Distantly Supervised Challenge Dataset for Reading Comprehension Authors: Mandar Joshi , Eunsol Choi , Daniel S. Weld , Luke Zettlemoyer View a PDF of the paper titled TriviaQA: A Large Scale Distantly Supervised Challenge Dataset for Reading Comprehension, by Mandar Joshi and 3 other authors View PDF Abstract: We present TriviaQA, a challenging reading comprehension dataset containing over 650K question-answer-evidence triples. TriviaQA includes 95K question-answer pairs authored by trivia enthusiasts and independently gathered evidence documents, six per question on average, that provide high quality distant supervision for answering the questions. We show that, in comparison to other recently introduced large-scale datasets, TriviaQA (1) has relatively complex, compositional questions, (2) has considerable syntactic and lexical variability between questions and corresponding answer-evidence sentences, and (3) requires more cross sentence reasoning to find answers. We also present two baseline algorithms: a feature-based classifier and a state-of-the-art neural network, that performs well on SQuAD reading comprehension. Neither approach comes close to human performance (23% and 40% vs. 80%), suggesting that TriviaQA is a challenging testbed that is worth significant future study. Data and code available at -- this http URL Comments: Added references, fixed typos, minor baseline update Subjects: Computation and Language (cs.CL) Cite as: arXiv:1705.03551 [cs.CL] &nbsp; (or arXiv:1705.03551v2 [cs.CL] for this version) &nbsp; https://doi.org/10.48550/arXiv.1705.03551 Focus to learn more arXiv-issued DOI via DataCite Submission history From: Mandar Joshi [ view email ] [v1] Tue, 9 May 2017 21:35:07 UTC (1,148 KB) [v2] Sat, 13 May 2017 21:12:37 UTC (1,149 KB) Full-text links: Access Paper: View a PDF of the paper titled TriviaQA: A Large Scale Distantly Supervised Challenge Dataset for Reading Comprehension, by Mandar Joshi and 3 other authors View PDF TeX Source Other Formats view license Current browse context: cs.CL &lt;&nbsp;prev &nbsp; | &nbsp; next&nbsp;&gt; new | recent | 2017-05 Change to browse by: cs References &amp; Citations NASA ADS Google Scholar Semantic Scholar DBLP - CS Bibliography listing | bibtex Mandar Joshi Eunsol Choi Daniel S. Weld Luke Zettlemoyer a export BibTeX citation Loading... BibTeX formatted citation &times; loading... Data provided by: Bookmark Bibliographic Tools Bibliographic and Citation Tools Bibliographic Explorer Toggle Bibliographic Explorer ( What is the Explorer? ) Connected Papers Toggle Connected Papers ( What is Connected Papers? ) Litmaps Toggle Litmaps ( What is Litmaps? ) scite.ai Toggle scite Smart Citations ( What are Smart Citations? ) Code, Data, Media Code, Data and Media Associated with this Article alphaXiv Toggle alphaXiv ( What is alphaXiv? ) Links to Code Toggle CatalyzeX Code Finder for Papers ( What is CatalyzeX? ) DagsHub Toggle DagsHub ( What is DagsHub? ) GotitPub Toggle Gotit.pub ( What is GotitPub? ) Huggingface Toggle Hugging Face ( What is Huggingface? ) Links to Code Toggle Papers with Code ( What is Papers with Code? ) ScienceCast Toggle ScienceCast ( What is ScienceCast? ) Demos Demos Replicate Toggle Replicate ( What is Replicate? ) Spaces Toggle Hugging Face Spaces ( What is Spaces? ) Spaces Toggle TXYZ.AI ( What is TXYZ.AI? ) Related Papers Recommenders and Search Tools Link to Influence Flower Influence Flower ( What are Influence Flowers? ) Core recommender toggle CORE Recommender ( What is CORE? ) Author Venue Institution Topic About arXivLabs arXivLabs: experimental projects with community collaborators arXivLabs is a framework that allows collaborators to develop and share new arXiv features directly on our website. Both individuals and organizations that work with arXivLabs have embraced and accepted our values of openness, community, excellence, and user data privacy. arXiv is committed to these values and only works with partners that adhere to them. Have an idea for a project that will add value for arXiv's community? Learn more about arXivLabs . Which authors of this paper are endorsers? | Disable MathJax ( What is MathJax? ) About Help contact arXiv Click here to contact arXiv Contact subscribe to arXiv mailings Click here to subscribe Subscribe Copyright Privacy Policy Web Accessibility Assistance arXiv Operational Status Get status notifications via email or slack ",
      "fetch_method": "direct-html"
    },
    {
      "arxiv_id": "https://arxiv.org/abs/1911.01547",
      "full_text": " [1911.01547] On the Measure of Intelligence Skip to main content We gratefully acknowledge support from the Simons Foundation, member institutions , and all contributors. Donate &gt; cs &gt; arXiv:1911.01547 Help | Advanced Search All fields Title Author Abstract Comments Journal reference ACM classification MSC classification Report number arXiv identifier DOI ORCID arXiv author ID Help pages Full text Search open search GO open navigation menu quick links Login Help Pages About --> Computer Science > Artificial Intelligence arXiv:1911.01547 (cs) [Submitted on 5 Nov 2019 ( v1 ), last revised 25 Nov 2019 (this version, v2)] Title: On the Measure of Intelligence Authors: François Chollet View a PDF of the paper titled On the Measure of Intelligence, by Fran\\c{c}ois Chollet View PDF Abstract: To make deliberate progress towards more intelligent and more human-like artificial systems, we need to be following an appropriate feedback signal: we need to be able to define and evaluate intelligence in a way that enables comparisons between two systems, as well as comparisons with humans. Over the past hundred years, there has been an abundance of attempts to define and measure intelligence, across both the fields of psychology and AI. We summarize and critically assess these definitions and evaluation approaches, while making apparent the two historical conceptions of intelligence that have implicitly guided them. We note that in practice, the contemporary AI community still gravitates towards benchmarking intelligence by comparing the skill exhibited by AIs and humans at specific tasks such as board games and video games. We argue that solely measuring skill at any given task falls short of measuring intelligence, because skill is heavily modulated by prior knowledge and experience: unlimited priors or unlimited training data allow experimenters to &#34;buy&#34; arbitrary levels of skills for a system, in a way that masks the system&#39;s own generalization power. We then articulate a new formal definition of intelligence based on Algorithmic Information Theory, describing intelligence as skill-acquisition efficiency and highlighting the concepts of scope, generalization difficulty, priors, and experience. Using this definition, we propose a set of guidelines for what a general AI benchmark should look like. Finally, we present a benchmark closely following these guidelines, the Abstraction and Reasoning Corpus (ARC), built upon an explicit set of priors designed to be as close as possible to innate human priors. We argue that ARC can be used to measure a human-like form of general fluid intelligence and that it enables fair general intelligence comparisons between AI systems and humans. Subjects: Artificial Intelligence (cs.AI) Cite as: arXiv:1911.01547 [cs.AI] &nbsp; (or arXiv:1911.01547v2 [cs.AI] for this version) &nbsp; https://doi.org/10.48550/arXiv.1911.01547 Focus to learn more arXiv-issued DOI via DataCite Submission history From: Francois Chollet [ view email ] [v1] Tue, 5 Nov 2019 00:31:38 UTC (771 KB) [v2] Mon, 25 Nov 2019 13:02:04 UTC (770 KB) Full-text links: Access Paper: View a PDF of the paper titled On the Measure of Intelligence, by Fran\\c{c}ois Chollet View PDF TeX Source Other Formats view license Current browse context: cs.AI &lt;&nbsp;prev &nbsp; | &nbsp; next&nbsp;&gt; new | recent | 2019-11 Change to browse by: cs References &amp; Citations NASA ADS Google Scholar Semantic Scholar 2 blog links ( what is this? ) DBLP - CS Bibliography listing | bibtex François Chollet a export BibTeX citation Loading... BibTeX formatted citation &times; loading... Data provided by: Bookmark Bibliographic Tools Bibliographic and Citation Tools Bibliographic Explorer Toggle Bibliographic Explorer ( What is the Explorer? ) Connected Papers Toggle Connected Papers ( What is Connected Papers? ) Litmaps Toggle Litmaps ( What is Litmaps? ) scite.ai Toggle scite Smart Citations ( What are Smart Citations? ) Code, Data, Media Code, Data and Media Associated with this Article alphaXiv Toggle alphaXiv ( What is alphaXiv? ) Links to Code Toggle CatalyzeX Code Finder for Papers ( What is CatalyzeX? ) DagsHub Toggle DagsHub ( What is DagsHub? ) GotitPub Toggle Gotit.pub ( What is GotitPub? ) Huggingface Toggle Hugging Face ( What is Huggingface? ) Links to Code Toggle Papers with Code ( What is Papers with Code? ) ScienceCast Toggle ScienceCast ( What is ScienceCast? ) Demos Demos Replicate Toggle Replicate ( What is Replicate? ) Spaces Toggle Hugging Face Spaces ( What is Spaces? ) Spaces Toggle TXYZ.AI ( What is TXYZ.AI? ) Related Papers Recommenders and Search Tools Link to Influence Flower Influence Flower ( What are Influence Flowers? ) Core recommender toggle CORE Recommender ( What is CORE? ) Author Venue Institution Topic About arXivLabs arXivLabs: experimental projects with community collaborators arXivLabs is a framework that allows collaborators to develop and share new arXiv features directly on our website. Both individuals and organizations that work with arXivLabs have embraced and accepted our values of openness, community, excellence, and user data privacy. arXiv is committed to these values and only works with partners that adhere to them. Have an idea for a project that will add value for arXiv's community? Learn more about arXivLabs . Which authors of this paper are endorsers? | Disable MathJax ( What is MathJax? ) About Help contact arXiv Click here to contact arXiv Contact subscribe to arXiv mailings Click here to subscribe Subscribe Copyright Privacy Policy Web Accessibility Assistance arXiv Operational Status Get status notifications via email or slack ",
      "fetch_method": "direct-html"
    },
    {
      "arxiv_id": "https://arxiv.org/abs/1907.10641",
      "full_text": " [1907.10641] WinoGrande: An Adversarial Winograd Schema Challenge at Scale Skip to main content We gratefully acknowledge support from the Simons Foundation, member institutions , and all contributors. Donate &gt; cs &gt; arXiv:1907.10641 Help | Advanced Search All fields Title Author Abstract Comments Journal reference ACM classification MSC classification Report number arXiv identifier DOI ORCID arXiv author ID Help pages Full text Search open search GO open navigation menu quick links Login Help Pages About --> Computer Science > Computation and Language arXiv:1907.10641 (cs) [Submitted on 24 Jul 2019 ( v1 ), last revised 21 Nov 2019 (this version, v2)] Title: WinoGrande: An Adversarial Winograd Schema Challenge at Scale Authors: Keisuke Sakaguchi , Ronan Le Bras , Chandra Bhagavatula , Yejin Choi View a PDF of the paper titled WinoGrande: An Adversarial Winograd Schema Challenge at Scale, by Keisuke Sakaguchi and 3 other authors View PDF Abstract: The Winograd Schema Challenge (WSC) (Levesque, Davis, and Morgenstern 2011), a benchmark for commonsense reasoning, is a set of 273 expert-crafted pronoun resolution problems originally designed to be unsolvable for statistical models that rely on selectional preferences or word associations. However, recent advances in neural language models have already reached around 90% accuracy on variants of WSC. This raises an important question whether these models have truly acquired robust commonsense capabilities or whether they rely on spurious biases in the datasets that lead to an overestimation of the true capabilities of machine commonsense. To investigate this question, we introduce WinoGrande, a large-scale dataset of 44k problems, inspired by the original WSC design, but adjusted to improve both the scale and the hardness of the dataset. The key steps of the dataset construction consist of (1) a carefully designed crowdsourcing procedure, followed by (2) systematic bias reduction using a novel AfLite algorithm that generalizes human-detectable word associations to machine-detectable embedding associations. The best state-of-the-art methods on WinoGrande achieve 59.4-79.1%, which are 15-35% below human performance of 94.0%, depending on the amount of the training data allowed. Furthermore, we establish new state-of-the-art results on five related benchmarks - WSC (90.1%), DPR (93.1%), COPA (90.6%), KnowRef (85.6%), and Winogender (97.1%). These results have dual implications: on one hand, they demonstrate the effectiveness of WinoGrande when used as a resource for transfer learning. On the other hand, they raise a concern that we are likely to be overestimating the true capabilities of machine commonsense across all these benchmarks. We emphasize the importance of algorithmic bias reduction in existing and future benchmarks to mitigate such overestimation. Subjects: Computation and Language (cs.CL) Cite as: arXiv:1907.10641 [cs.CL] &nbsp; (or arXiv:1907.10641v2 [cs.CL] for this version) &nbsp; https://doi.org/10.48550/arXiv.1907.10641 Focus to learn more arXiv-issued DOI via DataCite Submission history From: Keisuke Sakaguchi [ view email ] [v1] Wed, 24 Jul 2019 18:11:59 UTC (2,699 KB) [v2] Thu, 21 Nov 2019 19:01:32 UTC (1,474 KB) Full-text links: Access Paper: View a PDF of the paper titled WinoGrande: An Adversarial Winograd Schema Challenge at Scale, by Keisuke Sakaguchi and 3 other authors View PDF TeX Source Other Formats view license Current browse context: cs.CL &lt;&nbsp;prev &nbsp; | &nbsp; next&nbsp;&gt; new | recent | 2019-07 Change to browse by: cs References &amp; Citations NASA ADS Google Scholar Semantic Scholar 1 blog link ( what is this? ) DBLP - CS Bibliography listing | bibtex Keisuke Sakaguchi Ronan Le Bras Chandra Bhagavatula Yejin Choi a export BibTeX citation Loading... BibTeX formatted citation &times; loading... Data provided by: Bookmark Bibliographic Tools Bibliographic and Citation Tools Bibliographic Explorer Toggle Bibliographic Explorer ( What is the Explorer? ) Connected Papers Toggle Connected Papers ( What is Connected Papers? ) Litmaps Toggle Litmaps ( What is Litmaps? ) scite.ai Toggle scite Smart Citations ( What are Smart Citations? ) Code, Data, Media Code, Data and Media Associated with this Article alphaXiv Toggle alphaXiv ( What is alphaXiv? ) Links to Code Toggle CatalyzeX Code Finder for Papers ( What is CatalyzeX? ) DagsHub Toggle DagsHub ( What is DagsHub? ) GotitPub Toggle Gotit.pub ( What is GotitPub? ) Huggingface Toggle Hugging Face ( What is Huggingface? ) Links to Code Toggle Papers with Code ( What is Papers with Code? ) ScienceCast Toggle ScienceCast ( What is ScienceCast? ) Demos Demos Replicate Toggle Replicate ( What is Replicate? ) Spaces Toggle Hugging Face Spaces ( What is Spaces? ) Spaces Toggle TXYZ.AI ( What is TXYZ.AI? ) Related Papers Recommenders and Search Tools Link to Influence Flower Influence Flower ( What are Influence Flowers? ) Core recommender toggle CORE Recommender ( What is CORE? ) Author Venue Institution Topic About arXivLabs arXivLabs: experimental projects with community collaborators arXivLabs is a framework that allows collaborators to develop and share new arXiv features directly on our website. Both individuals and organizations that work with arXivLabs have embraced and accepted our values of openness, community, excellence, and user data privacy. arXiv is committed to these values and only works with partners that adhere to them. Have an idea for a project that will add value for arXiv's community? Learn more about arXivLabs . Which authors of this paper are endorsers? | Disable MathJax ( What is MathJax? ) About Help contact arXiv Click here to contact arXiv Contact subscribe to arXiv mailings Click here to subscribe Subscribe Copyright Privacy Policy Web Accessibility Assistance arXiv Operational Status Get status notifications via email or slack ",
      "fetch_method": "direct-html"
    },
    {
      "arxiv_id": "https://paperswithcode.com/dataset/bbh",
      "full_text": " Trending Papers - Hugging Face Hugging Face Models Datasets Spaces Community Docs Enterprise Pricing Log In Sign Up new Get trending papers in your email inbox once a day! Get trending papers in your email inbox! Subscribe Trending Papers by AK and the research community Daily Weekly Monthly Trending Papers Submitted by Haozhan72 SimpleVLA-RL: Scaling VLA Training via Reinforcement Learning Vision-Language-Action (VLA) models have recently emerged as a powerful paradigm for robotic manipulation. Despite substantial progress enabled by large-scale pretraining and supervised fine-tuning (SFT), these models face two fundamental challenges: (i) the scarcity and high cost of large-scale human-operated robotic trajectories required for SFT scaling, and (ii) limited generalization to tasks involving distribution shift. Recent breakthroughs in Large Reasoning Models (LRMs) demonstrate that reinforcement learning (RL) can dramatically enhance step-by-step reasoning capabilities, raising a natural question: Can RL similarly improve the long-horizon step-by-step action planning of VLA? In this work, we introduce SimpleVLA-RL, an efficient RL framework tailored for VLA models. Building upon veRL, we introduce VLA-specific trajectory sampling, scalable parallelization, multi-environment rendering, and optimized loss computation. When applied to OpenVLA-OFT, SimpleVLA-RL achieves SoTA performance on LIBERO and even outperforms pi_0 on RoboTwin 1.0\\&amp;2.0 with the exploration-enhancing strategies we introduce. SimpleVLA-RL not only reduces dependence on large-scale data and enables robust generalization, but also remarkably surpasses SFT in real-world tasks. Moreover, we identify a novel phenomenon ``pushcut'' during RL training, wherein the policy discovers previously unseen patterns beyond those seen in the previous training process. Github: https://github.com/PRIME-RL/SimpleVLA-RL 21 authors · Published on Sep 11, 2025 Upvote 48 GitHub 446 arXiv Page Submitted by Haozhan72 SimpleVLA-RL: Scaling VLA Training via Reinforcement Learning Vision-Language-Action (VLA) models have recently emerged as a powerful paradigm for robotic manipulation. Despite substantial progress enabled by large-scale pretraining and supervised fine-tuning (SFT), these models face two fundamental challenges: (i) the scarcity and high cost of large-scale human-operated robotic trajectories required for SFT scaling, and (ii) limited generalization to tasks involving distribution shift. Recent breakthroughs in Large Reasoning Models (LRMs) demonstrate that reinforcement learning (RL) can dramatically enhance step-by-step reasoning capabilities, raising a natural question: Can RL similarly improve the long-horizon step-by-step action planning of VLA? In this work, we introduce SimpleVLA-RL, an efficient RL framework tailored for VLA models. Building upon veRL, we introduce VLA-specific trajectory sampling, scalable parallelization, multi-environment rendering, and optimized loss computation. When applied to OpenVLA-OFT, SimpleVLA-RL achieves SoTA performance on LIBERO and even outperforms pi_0 on RoboTwin 1.0\\&amp;2.0 with the exploration-enhancing strategies we introduce. SimpleVLA-RL not only reduces dependence on large-scale data and enables robust generalization, but also remarkably surpasses SFT in real-world tasks. Moreover, we identify a novel phenomenon ``pushcut'' during RL training, wherein the policy discovers previously unseen patterns beyond those seen in the previous training process. Github: https://github.com/PRIME-RL/SimpleVLA-RL 21 authors · Sep 11, 2025 Upvote 48 GitHub 446 arXiv Page Submitted by taesiri AgentScope 1.0: A Developer-Centric Framework for Building Agentic Applications Driven by rapid advancements of Large Language Models (LLMs), agents are empowered to combine intrinsic knowledge with dynamic tool use, greatly enhancing their capacity to address real-world tasks. In line with such an evolution, AgentScope introduces major improvements in a new version (1.0), towards comprehensively supporting flexible and efficient tool-based agent-environment interactions for building agentic applications. Specifically, we abstract foundational components essential for agentic applications and provide unified interfaces and extensible modules, enabling developers to easily leverage the latest progress, such as new models and MCPs. Furthermore, we ground agent behaviors in the ReAct paradigm and offer advanced agent-level infrastructure based on a systematic asynchronous design, which enriches both human-agent and agent-agent interaction patterns while improving execution efficiency. Building on this foundation, we integrate several built-in agents tailored to specific practical scenarios. AgentScope also includes robust engineering support for developer-friendly experiences. We provide a scalable evaluation module with a visual studio interface, making the development of long-trajectory agentic applications more manageable and easier to trace. In addition, AgentScope offers a runtime sandbox to ensure safe agent execution and facilitates rapid deployment in production environments. With these enhancements, AgentScope provides a practical foundation for building scalable, adaptive, and effective agentic applications. 23 authors · Published on Aug 22, 2025 Upvote 37 GitHub 11.1k arXiv Page Submitted by taesiri AgentScope 1.0: A Developer-Centric Framework for Building Agentic Applications Driven by rapid advancements of Large Language Models (LLMs), agents are empowered to combine intrinsic knowledge with dynamic tool use, greatly enhancing their capacity to address real-world tasks. In line with such an evolution, AgentScope introduces major improvements in a new version (1.0), towards comprehensively supporting flexible and efficient tool-based agent-environment interactions for building agentic applications. Specifically, we abstract foundational components essential for agentic applications and provide unified interfaces and extensible modules, enabling developers to easily leverage the latest progress, such as new models and MCPs. Furthermore, we ground agent behaviors in the ReAct paradigm and offer advanced agent-level infrastructure based on a systematic asynchronous design, which enriches both human-agent and agent-agent interaction patterns while improving execution efficiency. Building on this foundation, we integrate several built-in agents tailored to specific practical scenarios. AgentScope also includes robust engineering support for developer-friendly experiences. We provide a scalable evaluation module with a visual studio interface, making the development of long-trajectory agentic applications more manageable and easier to trace. In addition, AgentScope offers a runtime sandbox to ensure safe agent execution and facilitates rapid deployment in production environments. With these enhancements, AgentScope provides a practical foundation for building scalable, adaptive, and effective agentic applications. 23 authors · Aug 22, 2025 Upvote 37 GitHub 11.1k arXiv Page Submitted by taesiri AgentGym-RL: Training LLM Agents for Long-Horizon Decision Making through Multi-Turn Reinforcement Learning Developing autonomous LLM agents capable of making a series of intelligent decisions to solve complex, real-world tasks is a fast-evolving frontier. Like human cognitive development, agents are expected to acquire knowledge and skills through exploration and interaction with the environment. Despite advances, the community still lacks a unified, interactive reinforcement learning (RL) framework that can effectively train such agents from scratch -- without relying on supervised fine-tuning (SFT) -- across diverse and realistic environments. To bridge this gap, we introduce AgentGym-RL, a new framework to train LLM agents for multi-turn interactive decision-making through RL. The framework features a modular and decoupled architecture, ensuring high flexibility and extensibility. It encompasses a wide variety of real-world scenarios, and supports mainstream RL algorithms. Furthermore, we propose ScalingInter-RL, a training approach designed for exploration-exploitation balance and stable RL optimization. In early stages, it emphasizes exploitation by restricting the number of interactions, and gradually shifts towards exploration with larger horizons to encourage diverse problem-solving strategies. In this way, the agent develops more diverse behaviors and is less prone to collapse under long horizons. We perform extensive experiments to validate the stability and effectiveness of both the AgentGym-RL framework and the ScalingInter-RL approach. Our agents match or surpass commercial models on 27 tasks across diverse environments. We offer key insights and will open-source the complete AgentGym-RL framework -- including code and datasets -- to empower the research community in developing the next generation of intelligent agents. 23 authors · Published on Sep 10, 2025 Upvote 23 GitHub 141 arXiv Page Submitted by taesiri AgentGym-RL: Training LLM Agents for Long-Horizon Decision Making through Multi-Turn Reinforcement Learning Developing autonomous LLM agents capable of making a series of intelligent decisions to solve complex, real-world tasks is a fast-evolving frontier. Like human cognitive development, agents are expected to acquire knowledge and skills through exploration and interaction with the environment. Despite advances, the community still lacks a unified, interactive reinforcement learning (RL) framework that can effectively train such agents from scratch -- without relying on supervised fine-tuning (SFT) -- across diverse and realistic environments. To bridge this gap, we introduce AgentGym-RL, a new framework to train LLM agents for multi-turn interactive decision-making through RL. The framework features a modular and decoupled architecture, ensuring high flexibility and extensibility. It encompasses a wide variety of real-world scenarios, and supports mainstream RL algorithms. Furthermore, we propose ScalingInter-RL, a training approach designed for exploration-exploitation balance and stable RL optimization. In early stages, it emphasizes exploitation by restricting the number of interactions, and gradually shifts towards exploration with larger horizons to encourage diverse problem-solving strategies. In this way, the agent develops more diverse behaviors and is less prone to collapse under long horizons. We perform extensive experiments to validate the stability and effectiveness of both the AgentGym-RL framework and the ScalingInter-RL approach. Our agents match or surpass commercial models on 27 tasks across diverse environments. We offer key insights and will open-source the complete AgentGym-RL framework -- including code and datasets -- to empower the research community in developing the next generation of intelligent agents. 23 authors · Sep 10, 2025 Upvote 23 GitHub 141 arXiv Page Submitted by taesiri FLUX-Reason-6M &amp; PRISM-Bench: A Million-Scale Text-to-Image Reasoning Dataset and Comprehensive Benchmark The advancement of open-source text-to-image (T2I) models has been hindered by the absence of large-scale, reasoning-focused datasets and comprehensive evaluation benchmarks, resulting in a performance gap compared to leading closed-source systems. To address this challenge, We introduce FLUX-Reason-6M and PRISM-Bench (Precise and Robust Image Synthesis Measurement Benchmark). FLUX-Reason-6M is a massive dataset consisting of 6 million high-quality FLUX-generated images and 20 million bilingual (English and Chinese) descriptions specifically designed to teach complex reasoning. The image are organized according to six key characteristics: Imagination, Entity, Text rendering, Style, Affection, and Composition, and design explicit Generation Chain-of-Thought (GCoT) to provide detailed breakdowns of image generation steps. The whole data curation takes 15,000 A100 GPU days, providing the community with a resource previously unattainable outside of large industrial labs. PRISM-Bench offers a novel evaluation standard with seven distinct tracks, including a formidable Long Text challenge using GCoT. Through carefully designed prompts, it utilizes advanced vision-language models for nuanced human-aligned assessment of prompt-image alignment and image aesthetics. Our extensive evaluation of 19 leading models on PRISM-Bench reveals critical performance gaps and highlights specific areas requiring improvement. Our dataset, benchmark, and evaluation code are released to catalyze the next wave of reasoning-oriented T2I generation. Project page: https://flux-reason-6m.github.io/ . 10 authors · Published on Sep 11, 2025 Upvote 22 GitHub 24 arXiv Page Submitted by taesiri FLUX-Reason-6M &amp; PRISM-Bench: A Million-Scale Text-to-Image Reasoning Dataset and Comprehensive Benchmark The advancement of open-source text-to-image (T2I) models has been hindered by the absence of large-scale, reasoning-focused datasets and comprehensive evaluation benchmarks, resulting in a performance gap compared to leading closed-source systems. To address this challenge, We introduce FLUX-Reason-6M and PRISM-Bench (Precise and Robust Image Synthesis Measurement Benchmark). FLUX-Reason-6M is a massive dataset consisting of 6 million high-quality FLUX-generated images and 20 million bilingual (English and Chinese) descriptions specifically designed to teach complex reasoning. The image are organized according to six key characteristics: Imagination, Entity, Text rendering, Style, Affection, and Composition, and design explicit Generation Chain-of-Thought (GCoT) to provide detailed breakdowns of image generation steps. The whole data curation takes 15,000 A100 GPU days, providing the community with a resource previously unattainable outside of large industrial labs. PRISM-Bench offers a novel evaluation standard with seven distinct tracks, including a formidable Long Text challenge using GCoT. Through carefully designed prompts, it utilizes advanced vision-language models for nuanced human-aligned assessment of prompt-image alignment and image aesthetics. Our extensive evaluation of 19 leading models on PRISM-Bench reveals critical performance gaps and highlights specific areas requiring improvement. Our dataset, benchmark, and evaluation code are released to catalyze the next wave of reasoning-oriented T2I generation. Project page: https://flux-reason-6m.github.io/ . 10 authors · Sep 11, 2025 Upvote 22 GitHub 24 arXiv Page Submitted by iseesaw A Survey of Reinforcement Learning for Large Reasoning Models In this paper, we survey recent advances in Reinforcement Learning (RL) for reasoning with Large Language Models (LLMs). RL has achieved remarkable success in advancing the frontier of LLM capabilities, particularly in addressing complex logical tasks such as mathematics and coding. As a result, RL has emerged as a foundational methodology for transforming LLMs into LRMs. With the rapid progress of the field, further scaling of RL for LRMs now faces foundational challenges not only in computational resources but also in algorithm design, training data, and infrastructure. To this end, it is timely to revisit the development of this domain, reassess its trajectory, and explore strategies to enhance the scalability of RL toward Artificial SuperIntelligence (ASI). In particular, we examine research applying RL to LLMs and LRMs for reasoning abilities, especially since the release of DeepSeek-R1, including foundational components, core problems, training resources, and downstream applications, to identify future opportunities and directions for this rapidly evolving area. We hope this review will promote future research on RL for broader reasoning models. Github: https://github.com/TsinghuaC3I/Awesome-RL-for-LRMs 39 authors · Published on Sep 10, 2025 Upvote 114 GitHub 941 arXiv Page Submitted by iseesaw A Survey of Reinforcement Learning for Large Reasoning Models In this paper, we survey recent advances in Reinforcement Learning (RL) for reasoning with Large Language Models (LLMs). RL has achieved remarkable success in advancing the frontier of LLM capabilities, particularly in addressing complex logical tasks such as mathematics and coding. As a result, RL has emerged as a foundational methodology for transforming LLMs into LRMs. With the rapid progress of the field, further scaling of RL for LRMs now faces foundational challenges not only in computational resources but also in algorithm design, training data, and infrastructure. To this end, it is timely to revisit the development of this domain, reassess its trajectory, and explore strategies to enhance the scalability of RL toward Artificial SuperIntelligence (ASI). In particular, we examine research applying RL to LLMs and LRMs for reasoning abilities, especially since the release of DeepSeek-R1, including foundational components, core problems, training resources, and downstream applications, to identify future opportunities and directions for this rapidly evolving area. We hope this review will promote future research on RL for broader reasoning models. Github: https://github.com/TsinghuaC3I/Awesome-RL-for-LRMs 39 authors · Sep 10, 2025 Upvote 114 GitHub 941 arXiv Page Submitted by taesiri 3D and 4D World Modeling: A Survey World modeling has become a cornerstone in AI research, enabling agents to understand, represent, and predict the dynamic environments they inhabit. While prior work largely emphasizes generative methods for 2D image and video data, they overlook the rapidly growing body of work that leverages native 3D and 4D representations such as RGB-D imagery, occupancy grids, and LiDAR point clouds for large-scale scene modeling. At the same time, the absence of a standardized definition and taxonomy for ``world models'' has led to fragmented and sometimes inconsistent claims in the literature. This survey addresses these gaps by presenting the first comprehensive review explicitly dedicated to 3D and 4D world modeling and generation. We establish precise definitions, introduce a structured taxonomy spanning video-based (VideoGen), occupancy-based (OccGen), and LiDAR-based (LiDARGen) approaches, and systematically summarize datasets and evaluation metrics tailored to 3D/4D settings. We further discuss practical applications, identify open challenges, and highlight promising research directions, aiming to provide a coherent and foundational reference for advancing the field. A systematic summary of existing literature is available at https://github.com/worldbench/survey 23 authors · Published on Sep 4, 2025 Upvote 43 GitHub 234 arXiv Page Submitted by taesiri 3D and 4D World Modeling: A Survey World modeling has become a cornerstone in AI research, enabling agents to understand, represent, and predict the dynamic environments they inhabit. While prior work largely emphasizes generative methods for 2D image and video data, they overlook the rapidly growing body of work that leverages native 3D and 4D representations such as RGB-D imagery, occupancy grids, and LiDAR point clouds for large-scale scene modeling. At the same time, the absence of a standardized definition and taxonomy for ``world models'' has led to fragmented and sometimes inconsistent claims in the literature. This survey addresses these gaps by presenting the first comprehensive review explicitly dedicated to 3D and 4D world modeling and generation. We establish precise definitions, introduce a structured taxonomy spanning video-based (VideoGen), occupancy-based (OccGen), and LiDAR-based (LiDARGen) approaches, and systematically summarize datasets and evaluation metrics tailored to 3D/4D settings. We further discuss practical applications, identify open challenges, and highlight promising research directions, aiming to provide a coherent and foundational reference for advancing the field. A systematic summary of existing literature is available at https://github.com/worldbench/survey 23 authors · Sep 4, 2025 Upvote 43 GitHub 234 arXiv Page Submitted by akhaliq LlamaFactory: Unified Efficient Fine-Tuning of 100+ Language Models Efficient fine-tuning is vital for adapting large language models (LLMs) to downstream tasks. However, it requires non-trivial efforts to implement these methods on different models. We present LlamaFactory, a unified framework that integrates a suite of cutting-edge efficient training methods. It allows users to flexibly customize the fine-tuning of 100+ LLMs without the need for coding through the built-in web UI LlamaBoard. We empirically validate the efficiency and effectiveness of our framework on language modeling and text generation tasks. It has been released at https://github.com/hiyouga/LLaMA-Factory and already received over 13,000 stars and 1,600 forks. 5 authors · Published on Mar 20, 2024 Upvote 138 GitHub 57.9k arXiv Page Submitted by akhaliq LlamaFactory: Unified Efficient Fine-Tuning of 100+ Language Models Efficient fine-tuning is vital for adapting large language models (LLMs) to downstream tasks. However, it requires non-trivial efforts to implement these methods on different models. We present LlamaFactory, a unified framework that integrates a suite of cutting-edge efficient training methods. It allows users to flexibly customize the fine-tuning of 100+ LLMs without the need for coding through the built-in web UI LlamaBoard. We empirically validate the efficiency and effectiveness of our framework on language modeling and text generation tasks. It has been released at https://github.com/hiyouga/LLaMA-Factory and already received over 13,000 stars and 1,600 forks. 5 authors · Mar 20, 2024 Upvote 138 GitHub 57.9k arXiv Page Submitted by imryanxu ComfyUI-Copilot: An Intelligent Assistant for Automated Workflow Development We introduce ComfyUI-Copilot, a large language model-powered plugin designed to enhance the usability and efficiency of ComfyUI, an open-source platform for AI-driven art creation. Despite its flexibility and user-friendly interface, ComfyUI can present challenges to newcomers, including limited documentation, model misconfigurations, and the complexity of workflow design. ComfyUI-Copilot addresses these challenges by offering intelligent node and model recommendations, along with automated one-click workflow construction. At its core, the system employs a hierarchical multi-agent framework comprising a central assistant agent for task delegation and specialized worker agents for different usages, supported by our curated ComfyUI knowledge bases to streamline debugging and deployment. We validate the effectiveness of ComfyUI-Copilot through both offline quantitative evaluations and online user feedback, showing that it accurately recommends nodes and accelerates workflow development. Additionally, use cases illustrate that ComfyUI-Copilot lowers entry barriers for beginners and enhances workflow efficiency for experienced users. The ComfyUI-Copilot installation package and a demo video are available at https://github.com/AIDC-AI/ComfyUI-Copilot. 10 authors · Published on Jun 5, 2025 Upvote 78 GitHub 2.8k arXiv Page Submitted by imryanxu ComfyUI-Copilot: An Intelligent Assistant for Automated Workflow Development We introduce ComfyUI-Copilot, a large language model-powered plugin designed to enhance the usability and efficiency of ComfyUI, an open-source platform for AI-driven art creation. Despite its flexibility and user-friendly interface, ComfyUI can present challenges to newcomers, including limited documentation, model misconfigurations, and the complexity of workflow design. ComfyUI-Copilot addresses these challenges by offering intelligent node and model recommendations, along with automated one-click workflow construction. At its core, the system employs a hierarchical multi-agent framework comprising a central assistant agent for task delegation and specialized worker agents for different usages, supported by our curated ComfyUI knowledge bases to streamline debugging and deployment. We validate the effectiveness of ComfyUI-Copilot through both offline quantitative evaluations and online user feedback, showing that it accurately recommends nodes and accelerates workflow development. Additionally, use cases illustrate that ComfyUI-Copilot lowers entry barriers for beginners and enhances workflow efficiency for experienced users. The ComfyUI-Copilot installation package and a demo video are available at https://github.com/AIDC-AI/ComfyUI-Copilot. 10 authors · Jun 5, 2025 Upvote 78 GitHub 2.8k arXiv Page Submitted by taesiri Mini-o3: Scaling Up Reasoning Patterns and Interaction Turns for Visual Search Recent advances in large multimodal models have leveraged image-based tools with reinforcement learning to tackle visual problems. However, existing open-source approaches often exhibit monotonous reasoning patterns and allow only a limited number of interaction turns, making them inadequate for difficult tasks that require trial-and-error exploration. In this work, we address this limitation by scaling up tool-based interactions and introduce Mini-o3, a system that executes deep, multi-turn reasoning -- spanning tens of steps -- and achieves state-of-the-art performance on challenging visual search tasks. Our recipe for reproducing OpenAI o3-style behaviors comprises three key components. First, we construct the Visual Probe Dataset, a collection of thousands of challenging visual search problems designed for exploratory reasoning. Second, we develop an iterative data collection pipeline to obtain cold-start trajectories that exhibit diverse reasoning patterns, including depth-first search, trial-and-error, and goal maintenance. Third, we propose an over-turn masking strategy that prevents penalization of over-turn responses (those that hit the maximum number of turns) during reinforcement learning, thereby balancing training-time efficiency with test-time scalability. Despite training with an upper bound of only six interaction turns, our model generates trajectories that naturally scale to tens of turns at inference time, with accuracy improving as the number of turns increases. Extensive experiments demonstrate that Mini-o3 produces rich reasoning patterns and deep thinking paths, effectively solving challenging visual search problems. 6 authors · Published on Sep 9, 2025 Upvote 50 GitHub 193 arXiv Page Submitted by taesiri Mini-o3: Scaling Up Reasoning Patterns and Interaction Turns for Visual Search Recent advances in large multimodal models have leveraged image-based tools with reinforcement learning to tackle visual problems. However, existing open-source approaches often exhibit monotonous reasoning patterns and allow only a limited number of interaction turns, making them inadequate for difficult tasks that require trial-and-error exploration. In this work, we address this limitation by scaling up tool-based interactions and introduce Mini-o3, a system that executes deep, multi-turn reasoning -- spanning tens of steps -- and achieves state-of-the-art performance on challenging visual search tasks. Our recipe for reproducing OpenAI o3-style behaviors comprises three key components. First, we construct the Visual Probe Dataset, a collection of thousands of challenging visual search problems designed for exploratory reasoning. Second, we develop an iterative data collection pipeline to obtain cold-start trajectories that exhibit diverse reasoning patterns, including depth-first search, trial-and-error, and goal maintenance. Third, we propose an over-turn masking strategy that prevents penalization of over-turn responses (those that hit the maximum number of turns) during reinforcement learning, thereby balancing training-time efficiency with test-time scalability. Despite training with an upper bound of only six interaction turns, our model generates trajectories that naturally scale to tens of turns at inference time, with accuracy improving as the number of turns increases. Extensive experiments demonstrate that Mini-o3 produces rich reasoning patterns and deep thinking paths, effectively solving challenging visual search problems. 6 authors · Sep 9, 2025 Upvote 50 GitHub 193 arXiv Page Submitted by simingfu TempFlow-GRPO: When Timing Matters for GRPO in Flow Models Recent flow matching models for text-to-image generation have achieved remarkable quality, yet their integration with reinforcement learning for human preference alignment remains suboptimal, hindering fine-grained reward-based optimization. We observe that the key impediment to effective GRPO training of flow models is the temporal uniformity assumption in existing approaches: sparse terminal rewards with uniform credit assignment fail to capture the varying criticality of decisions across generation timesteps, resulting in inefficient exploration and suboptimal convergence. To remedy this shortcoming, we introduce TempFlow-GRPO (Temporal Flow GRPO), a principled GRPO framework that captures and exploits the temporal structure inherent in flow-based generation. TempFlow-GRPO introduces two key innovations: (i) a trajectory branching mechanism that provides process rewards by concentrating stochasticity at designated branching points, enabling precise credit assignment without requiring specialized intermediate reward models; and (ii) a noise-aware weighting scheme that modulates policy optimization according to the intrinsic exploration potential of each timestep, prioritizing learning during high-impact early stages while ensuring stable refinement in later phases. These innovations endow the model with temporally-aware optimization that respects the underlying generative dynamics, leading to state-of-the-art performance in human preference alignment and standard text-to-image benchmarks. 8 authors · Published on Aug 6, 2025 Upvote 12 GitHub 654 arXiv Page Submitted by simingfu TempFlow-GRPO: When Timing Matters for GRPO in Flow Models Recent flow matching models for text-to-image generation have achieved remarkable quality, yet their integration with reinforcement learning for human preference alignment remains suboptimal, hindering fine-grained reward-based optimization. We observe that the key impediment to effective GRPO training of flow models is the temporal uniformity assumption in existing approaches: sparse terminal rewards with uniform credit assignment fail to capture the varying criticality of decisions across generation timesteps, resulting in inefficient exploration and suboptimal convergence. To remedy this shortcoming, we introduce TempFlow-GRPO (Temporal Flow GRPO), a principled GRPO framework that captures and exploits the temporal structure inherent in flow-based generation. TempFlow-GRPO introduces two key innovations: (i) a trajectory branching mechanism that provides process rewards by concentrating stochasticity at designated branching points, enabling precise credit assignment without requiring specialized intermediate reward models; and (ii) a noise-aware weighting scheme that modulates policy optimization according to the intrinsic exploration potential of each timestep, prioritizing learning during high-impact early stages while ensuring stable refinement in later phases. These innovations endow the model with temporally-aware optimization that respects the underlying generative dynamics, leading to state-of-the-art performance in human preference alignment and standard text-to-image benchmarks. 8 authors · Aug 6, 2025 Upvote 12 GitHub 654 arXiv Page Submitted by sanaka87 Reconstruction Alignment Improves Unified Multimodal Models Unified multimodal models (UMMs) unify visual understanding and generation within a single architecture. However, conventional training relies on image-text pairs (or sequences) whose captions are typically sparse and miss fine-grained visual details--even when they use hundreds of words to describe a simple image. We introduce Reconstruction Alignment (RecA), a resource-efficient post-training method that leverages visual understanding encoder embeddings as dense \"text prompts,\" providing rich supervision without captions. Concretely, RecA conditions a UMM on its own visual understanding embeddings and optimizes it to reconstruct the input image with a self-supervised reconstruction loss, thereby realigning understanding and generation. Despite its simplicity, RecA is broadly applicable: across autoregressive, masked-autoregressive, and diffusion-based UMMs, it consistently improves generation and editing fidelity. With only 27 GPU-hours, post-training with RecA substantially improves image generation performance on GenEval (0.73rightarrow0.90) and DPGBench (80.93rightarrow88.15), while also boosting editing benchmarks (ImgEdit 3.38rightarrow3.75, GEdit 6.94rightarrow7.25). Notably, RecA surpasses much larger open-source models and applies broadly across diverse UMM architectures, establishing it as an efficient and general post-training alignment strategy for UMMs 4 authors · Published on Sep 8, 2025 Upvote 36 GitHub 107 arXiv Page Submitted by sanaka87 Reconstruction Alignment Improves Unified Multimodal Models Unified multimodal models (UMMs) unify visual understanding and generation within a single architecture. However, conventional training relies on image-text pairs (or sequences) whose captions are typically sparse and miss fine-grained visual details--even when they use hundreds of words to describe a simple image. We introduce Reconstruction Alignment (RecA), a resource-efficient post-training method that leverages visual understanding encoder embeddings as dense \"text prompts,\" providing rich supervision without captions. Concretely, RecA conditions a UMM on its own visual understanding embeddings and optimizes it to reconstruct the input image with a self-supervised reconstruction loss, thereby realigning understanding and generation. Despite its simplicity, RecA is broadly applicable: across autoregressive, masked-autoregressive, and diffusion-based UMMs, it consistently improves generation and editing fidelity. With only 27 GPU-hours, post-training with RecA substantially improves image generation performance on GenEval (0.73rightarrow0.90) and DPGBench (80.93rightarrow88.15), while also boosting editing benchmarks (ImgEdit 3.38rightarrow3.75, GEdit 6.94rightarrow7.25). Notably, RecA surpasses much larger open-source models and applies broadly across diverse UMM architectures, establishing it as an efficient and general post-training alignment strategy for UMMs 4 authors · Sep 8, 2025 Upvote 36 GitHub 107 arXiv Page Submitted by nielsr DINOv3 Self-supervised learning holds the promise of eliminating the need for manual data annotation, enabling models to scale effortlessly to massive datasets and larger architectures. By not being tailored to specific tasks or domains, this training paradigm has the potential to learn visual representations from diverse sources, ranging from natural to aerial images -- using a single algorithm. This technical report introduces DINOv3, a major milestone toward realizing this vision by leveraging simple yet effective strategies. First, we leverage the benefit of scaling both dataset and model size by careful data preparation, design, and optimization. Second, we introduce a new method called Gram anchoring, which effectively addresses the known yet unsolved issue of dense feature maps degrading during long training schedules. Finally, we apply post-hoc strategies that further enhance our models' flexibility with respect to resolution, model size, and alignment with text. As a result, we present a versatile vision foundation model that outperforms the specialized state of the art across a broad range of settings, without fine-tuning. DINOv3 produces high-quality dense features that achieve outstanding performance on various vision tasks, significantly surpassing previous self- and weakly-supervised foundation models. We also share the DINOv3 suite of vision models, designed to advance the state of the art on a wide spectrum of tasks and data by providing scalable solutions for diverse resource constraints and deployment scenarios. 26 authors · Published on Aug 13, 2025 Upvote 248 GitHub 6.84k arXiv Page Submitted by nielsr DINOv3 Self-supervised learning holds the promise of eliminating the need for manual data annotation, enabling models to scale effortlessly to massive datasets and larger architectures. By not being tailored to specific tasks or domains, this training paradigm has the potential to learn visual representations from diverse sources, ranging from natural to aerial images -- using a single algorithm. This technical report introduces DINOv3, a major milestone toward realizing this vision by leveraging simple yet effective strategies. First, we leverage the benefit of scaling both dataset and model size by careful data preparation, design, and optimization. Second, we introduce a new method called Gram anchoring, which effectively addresses the known yet unsolved issue of dense feature maps degrading during long training schedules. Finally, we apply post-hoc strategies that further enhance our models' flexibility with respect to resolution, model size, and alignment with text. As a result, we present a versatile vision foundation model that outperforms the specialized state of the art across a broad range of settings, without fine-tuning. DINOv3 produces high-quality dense features that achieve outstanding performance on various vision tasks, significantly surpassing previous self- and weakly-supervised foundation models. We also share the DINOv3 suite of vision models, designed to advance the state of the art on a wide spectrum of tasks and data by providing scalable solutions for diverse resource constraints and deployment scenarios. 26 authors · Aug 13, 2025 Upvote 248 GitHub 6.84k arXiv Page Submitted by xhyandwyy Mobile-Agent-v3: Foundamental Agents for GUI Automation This paper introduces GUI-Owl, a foundational GUI agent model that achieves state-of-the-art performance among open-source end-to-end models on ten GUI benchmarks across desktop and mobile environments, covering grounding, question answering, planning, decision-making, and procedural knowledge. GUI-Owl-7B achieves 66.4 on AndroidWorld and 29.4 on OSWorld. Building on this, we propose Mobile-Agent-v3, a general-purpose GUI agent framework that further improves performance to 73.3 on AndroidWorld and 37.7 on OSWorld, setting a new state-of-the-art for open-source GUI agent frameworks. GUI-Owl incorporates three key innovations: (1) Large-scale Environment Infrastructure: a cloud-based virtual environment spanning Android, Ubuntu, macOS, and Windows, enabling our Self-Evolving GUI Trajectory Production framework. This generates high-quality interaction data via automated query generation and correctness validation, leveraging GUI-Owl to refine trajectories iteratively, forming a self-improving loop. It supports diverse data pipelines and reduces manual annotation. (2) Diverse Foundational Agent Capabilities: by integrating UI grounding, planning, action semantics, and reasoning patterns, GUI-Owl supports end-to-end decision-making and can act as a modular component in multi-agent systems. (3) Scalable Environment RL: we develop a scalable reinforcement learning framework with fully asynchronous training for real-world alignment. We also introduce Trajectory-aware Relative Policy Optimization (TRPO) for online RL, achieving 34.9 on OSWorld. GUI-Owl and Mobile-Agent-v3 are open-sourced at https://github.com/X-PLUG/MobileAgent. 15 authors · Published on Aug 21, 2025 Upvote 61 GitHub 5.59k arXiv Page Submitted by xhyandwyy Mobile-Agent-v3: Foundamental Agents for GUI Automation This paper introduces GUI-Owl, a foundational GUI agent model that achieves state-of-the-art performance among open-source end-to-end models on ten GUI benchmarks across desktop and mobile environments, covering grounding, question answering, planning, decision-making, and procedural knowledge. GUI-Owl-7B achieves 66.4 on AndroidWorld and 29.4 on OSWorld. Building on this, we propose Mobile-Agent-v3, a general-purpose GUI agent framework that further improves performance to 73.3 on AndroidWorld and 37.7 on OSWorld, setting a new state-of-the-art for open-source GUI agent frameworks. GUI-Owl incorporates three key innovations: (1) Large-scale Environment Infrastructure: a cloud-based virtual environment spanning Android, Ubuntu, macOS, and Windows, enabling our Self-Evolving GUI Trajectory Production framework. This generates high-quality interaction data via automated query generation and correctness validation, leveraging GUI-Owl to refine trajectories iteratively, forming a self-improving loop. It supports diverse data pipelines and reduces manual annotation. (2) Diverse Foundational Agent Capabilities: by integrating UI grounding, planning, action semantics, and reasoning patterns, GUI-Owl supports end-to-end decision-making and can act as a modular component in multi-agent systems. (3) Scalable Environment RL: we develop a scalable reinforcement learning framework with fully asynchronous training for real-world alignment. We also introduce Trajectory-aware Relative Policy Optimization (TRPO) for online RL, achieving 34.9 on OSWorld. GUI-Owl and Mobile-Agent-v3 are open-sourced at https://github.com/X-PLUG/MobileAgent. 15 authors · Aug 21, 2025 Upvote 61 GitHub 5.59k arXiv Page Submitted by xhyandwyy PC-Agent: A Hierarchical Multi-Agent Collaboration Framework for Complex Task Automation on PC In the field of MLLM-based GUI agents, compared to smartphones, the PC scenario not only features a more complex interactive environment, but also involves more intricate intra- and inter-app workflows. To address these issues, we propose a hierarchical agent framework named PC-Agent. Specifically, from the perception perspective, we devise an Active Perception Module (APM) to overcome the inadequate abilities of current MLLMs in perceiving screenshot content. From the decision-making perspective, to handle complex user instructions and interdependent subtasks more effectively, we propose a hierarchical multi-agent collaboration architecture that decomposes decision-making processes into Instruction-Subtask-Action levels. Within this architecture, three agents (i.e., Manager, Progress and Decision) are set up for instruction decomposition, progress tracking and step-by-step decision-making respectively. Additionally, a Reflection agent is adopted to enable timely bottom-up error feedback and adjustment. We also introduce a new benchmark PC-Eval with 25 real-world complex instructions. Empirical results on PC-Eval show that our PC-Agent achieves a 32% absolute improvement of task success rate over previous state-of-the-art methods. The code will be publicly available. 11 authors · Published on Feb 20, 2025 Upvote 28 GitHub 5.59k arXiv Page Submitted by xhyandwyy PC-Agent: A Hierarchical Multi-Agent Collaboration Framework for Complex Task Automation on PC In the field of MLLM-based GUI agents, compared to smartphones, the PC scenario not only features a more complex interactive environment, but also involves more intricate intra- and inter-app workflows. To address these issues, we propose a hierarchical agent framework named PC-Agent. Specifically, from the perception perspective, we devise an Active Perception Module (APM) to overcome the inadequate abilities of current MLLMs in perceiving screenshot content. From the decision-making perspective, to handle complex user instructions and interdependent subtasks more effectively, we propose a hierarchical multi-agent collaboration architecture that decomposes decision-making processes into Instruction-Subtask-Action levels. Within this architecture, three agents (i.e., Manager, Progress and Decision) are set up for instruction decomposition, progress tracking and step-by-step decision-making respectively. Additionally, a Reflection agent is adopted to enable timely bottom-up error feedback and adjustment. We also introduce a new benchmark PC-Eval with 25 real-world complex instructions. Empirical results on PC-Eval show that our PC-Agent achieves a 32% absolute improvement of task success rate over previous state-of-the-art methods. The code will be publicly available. 11 authors · Feb 20, 2025 Upvote 28 GitHub 5.59k arXiv Page Submitted by xhyandwyy Look Before You Leap: A GUI-Critic-R1 Model for Pre-Operative Error Diagnosis in GUI Automation In recent years, Multimodal Large Language Models (MLLMs) have been extensively utilized for multimodal reasoning tasks, including Graphical User Interface (GUI) automation. Unlike general offline multimodal tasks, GUI automation is executed in online interactive environments, necessitating step-by-step decision-making based on real-time status of the environment. This task has a lower tolerance for decision-making errors at each step, as any mistakes may cumulatively disrupt the process and potentially lead to irreversible outcomes like deletions or payments. To address these issues, we introduce a pre-operative critic mechanism that provides effective feedback prior to the actual execution, by reasoning about the potential outcome and correctness of actions. Specifically, we propose a Suggestion-aware Gradient Relative Policy Optimization (S-GRPO) strategy to construct our pre-operative critic model GUI-Critic-R1, incorporating a novel suggestion reward to enhance the reliability of the model's feedback. Furthermore, we develop a reasoning-bootstrapping based data collection pipeline to create a GUI-Critic-Train and a GUI-Critic-Test, filling existing gaps in GUI critic data. Static experiments on the GUI-Critic-Test across both mobile and web domains reveal that our GUI-Critic-R1 offers significant advantages in critic accuracy compared to current MLLMs. Dynamic evaluation on GUI automation benchmark further highlights the effectiveness and superiority of our model, as evidenced by improved success rates and operational efficiency. 12 authors · Published on Jun 5, 2025 Upvote 18 GitHub 5.59k arXiv Page Submitted by xhyandwyy Look Before You Leap: A GUI-Critic-R1 Model for Pre-Operative Error Diagnosis in GUI Automation In recent years, Multimodal Large Language Models (MLLMs) have been extensively utilized for multimodal reasoning tasks, including Graphical User Interface (GUI) automation. Unlike general offline multimodal tasks, GUI automation is executed in online interactive environments, necessitating step-by-step decision-making based on real-time status of the environment. This task has a lower tolerance for decision-making errors at each step, as any mistakes may cumulatively disrupt the process and potentially lead to irreversible outcomes like deletions or payments. To address these issues, we introduce a pre-operative critic mechanism that provides effective feedback prior to the actual execution, by reasoning about the potential outcome and correctness of actions. Specifically, we propose a Suggestion-aware Gradient Relative Policy Optimization (S-GRPO) strategy to construct our pre-operative critic model GUI-Critic-R1, incorporating a novel suggestion reward to enhance the reliability of the model's feedback. Furthermore, we develop a reasoning-bootstrapping based data collection pipeline to create a GUI-Critic-Train and a GUI-Critic-Test, filling existing gaps in GUI critic data. Static experiments on the GUI-Critic-Test across both mobile and web domains reveal that our GUI-Critic-R1 offers significant advantages in critic accuracy compared to current MLLMs. Dynamic evaluation on GUI automation benchmark further highlights the effectiveness and superiority of our model, as evidenced by improved success rates and operational efficiency. 12 authors · Jun 5, 2025 Upvote 18 GitHub 5.59k arXiv Page Submitted by XionghuiWang OneReward: Unified Mask-Guided Image Generation via Multi-Task Human Preference Learning In this paper, we introduce OneReward, a unified reinforcement learning framework that enhances the model's generative capabilities across multiple tasks under different evaluation criteria using only One Reward model. By employing a single vision-language model (VLM) as the generative reward model, which can distinguish the winner and loser for a given task and a given evaluation criterion, it can be effectively applied to multi-task generation models, particularly in contexts with varied data and diverse task objectives. We utilize OneReward for mask-guided image generation, which can be further divided into several sub-tasks such as image fill, image extend, object removal, and text rendering, involving a binary mask as the edit area. Although these domain-specific tasks share same conditioning paradigm, they differ significantly in underlying data distributions and evaluation metrics. Existing methods often rely on task-specific supervised fine-tuning (SFT), which limits generalization and training efficiency. Building on OneReward, we develop Seedream 3.0 Fill, a mask-guided generation model trained via multi-task reinforcement learning directly on a pre-trained base model, eliminating the need for task-specific SFT. Experimental results demonstrate that our unified edit model consistently outperforms both commercial and open-source competitors, such as Ideogram, Adobe Photoshop, and FLUX Fill [Pro], across multiple evaluation dimensions. Code and model are available at: https://one-reward.github.io 6 authors · Published on Aug 28, 2025 Upvote 12 GitHub 180 arXiv Page Submitted by XionghuiWang OneReward: Unified Mask-Guided Image Generation via Multi-Task Human Preference Learning In this paper, we introduce OneReward, a unified reinforcement learning framework that enhances the model's generative capabilities across multiple tasks under different evaluation criteria using only One Reward model. By employing a single vision-language model (VLM) as the generative reward model, which can distinguish the winner and loser for a given task and a given evaluation criterion, it can be effectively applied to multi-task generation models, particularly in contexts with varied data and diverse task objectives. We utilize OneReward for mask-guided image generation, which can be further divided into several sub-tasks such as image fill, image extend, object removal, and text rendering, involving a binary mask as the edit area. Although these domain-specific tasks share same conditioning paradigm, they differ significantly in underlying data distributions and evaluation metrics. Existing methods often rely on task-specific supervised fine-tuning (SFT), which limits generalization and training efficiency. Building on OneReward, we develop Seedream 3.0 Fill, a mask-guided generation model trained via multi-task reinforcement learning directly on a pre-trained base model, eliminating the need for task-specific SFT. Experimental results demonstrate that our unified edit model consistently outperforms both commercial and open-source competitors, such as Ideogram, Adobe Photoshop, and FLUX Fill [Pro], across multiple evaluation dimensions. Code and model are available at: https://one-reward.github.io 6 authors · Aug 28, 2025 Upvote 12 GitHub 180 arXiv Page Submitted by henggg The Landscape of Agentic Reinforcement Learning for LLMs: A Survey The emergence of agentic reinforcement learning (Agentic RL) marks a paradigm shift from conventional reinforcement learning applied to large language models (LLM RL), reframing LLMs from passive sequence generators into autonomous, decision-making agents embedded in complex, dynamic worlds. This survey formalizes this conceptual shift by contrasting the degenerate single-step Markov Decision Processes (MDPs) of LLM-RL with the temporally extended, partially observable Markov decision processes (POMDPs) that define Agentic RL. Building on this foundation, we propose a comprehensive twofold taxonomy: one organized around core agentic capabilities, including planning, tool use, memory, reasoning, self-improvement, and perception, and the other around their applications across diverse task domains. Central to our thesis is that reinforcement learning serves as the critical mechanism for transforming these capabilities from static, heuristic modules into adaptive, robust agentic behavior. To support and accelerate future research, we consolidate the landscape of open-source environments, benchmarks, and frameworks into a practical compendium. By synthesizing over five hundred recent works, this survey charts the contours of this rapidly evolving field and highlights the opportunities and challenges that will shape the development of scalable, general-purpose AI agents. 25 authors · Published on Sep 2, 2025 Upvote 182 GitHub 579 arXiv Page Submitted by henggg The Landscape of Agentic Reinforcement Learning for LLMs: A Survey The emergence of agentic reinforcement learning (Agentic RL) marks a paradigm shift from conventional reinforcement learning applied to large language models (LLM RL), reframing LLMs from passive sequence generators into autonomous, decision-making agents embedded in complex, dynamic worlds. This survey formalizes this conceptual shift by contrasting the degenerate single-step Markov Decision Processes (MDPs) of LLM-RL with the temporally extended, partially observable Markov decision processes (POMDPs) that define Agentic RL. Building on this foundation, we propose a comprehensive twofold taxonomy: one organized around core agentic capabilities, including planning, tool use, memory, reasoning, self-improvement, and perception, and the other around their applications across diverse task domains. Central to our thesis is that reinforcement learning serves as the critical mechanism for transforming these capabilities from static, heuristic modules into adaptive, robust agentic behavior. To support and accelerate future research, we consolidate the landscape of open-source environments, benchmarks, and frameworks into a practical compendium. By synthesizing over five hundred recent works, this survey charts the contours of this rapidly evolving field and highlights the opportunities and challenges that will shape the development of scalable, general-purpose AI agents. 25 authors · Sep 2, 2025 Upvote 182 GitHub 579 arXiv Page Submitted by Weiyun1025 InternVL3: Exploring Advanced Training and Test-Time Recipes for Open-Source Multimodal Models We introduce InternVL3, a significant advancement in the InternVL series featuring a native multimodal pre-training paradigm. Rather than adapting a text-only large language model (LLM) into a multimodal large language model (MLLM) that supports visual inputs, InternVL3 jointly acquires multimodal and linguistic capabilities from both diverse multimodal data and pure-text corpora during a single pre-training stage. This unified training paradigm effectively addresses the complexities and alignment challenges commonly encountered in conventional post-hoc training pipelines for MLLMs. To further improve performance and scalability, InternVL3 incorporates variable visual position encoding (V2PE) to support extended multimodal contexts, employs advanced post-training techniques such as supervised fine-tuning (SFT) and mixed preference optimization (MPO), and adopts test-time scaling strategies alongside an optimized training infrastructure. Extensive empirical evaluations demonstrate that InternVL3 delivers superior performance across a wide range of multi-modal tasks. In particular, InternVL3-78B achieves a score of 72.2 on the MMMU benchmark, setting a new state-of-the-art among open-source MLLMs. Its capabilities remain highly competitive with leading proprietary models, including ChatGPT-4o, Claude 3.5 Sonnet, and Gemini 2.5 Pro, while also maintaining strong pure-language proficiency. In pursuit of open-science principles, we will publicly release both the training data and model weights to foster further research and development in next-generation MLLMs. 47 authors · Published on Apr 14, 2025 Upvote 288 arXiv Page Submitted by Weiyun1025 InternVL3: Exploring Advanced Training and Test-Time Recipes for Open-Source Multimodal Models We introduce InternVL3, a significant advancement in the InternVL series featuring a native multimodal pre-training paradigm. Rather than adapting a text-only large language model (LLM) into a multimodal large language model (MLLM) that supports visual inputs, InternVL3 jointly acquires multimodal and linguistic capabilities from both diverse multimodal data and pure-text corpora during a single pre-training stage. This unified training paradigm effectively addresses the complexities and alignment challenges commonly encountered in conventional post-hoc training pipelines for MLLMs. To further improve performance and scalability, InternVL3 incorporates variable visual position encoding (V2PE) to support extended multimodal contexts, employs advanced post-training techniques such as supervised fine-tuning (SFT) and mixed preference optimization (MPO), and adopts test-time scaling strategies alongside an optimized training infrastructure. Extensive empirical evaluations demonstrate that InternVL3 delivers superior performance across a wide range of multi-modal tasks. In particular, InternVL3-78B achieves a score of 72.2 on the MMMU benchmark, setting a new state-of-the-art among open-source MLLMs. Its capabilities remain highly competitive with leading proprietary models, including ChatGPT-4o, Claude 3.5 Sonnet, and Gemini 2.5 Pro, while also maintaining strong pure-language proficiency. In pursuit of open-science principles, we will publicly release both the training data and model weights to foster further research and development in next-generation MLLMs. 47 authors · Apr 14, 2025 Upvote 288 arXiv Page Submitted by taesiri AgentFly: Fine-tuning LLM Agents without Fine-tuning LLMs In this paper, we introduce a novel learning paradigm for adaptive Large Language Model (LLM) agents that eliminates the need for fine-tuning the underlying LLMs. Existing approaches are often either rigid, relying on static, handcrafted reflection workflows, or computationally intensive, requiring gradient updates of LLM model parameters. In contrast, our method enables low-cost continual adaptation via memory-based online reinforcement learning. We formalise this as a Memory-augmented Markov Decision Process (M-MDP), equipped with a neural case-selection policy to guide action decisions. Past experiences are stored in an episodic memory, either differentiable or non-parametric. The policy is continually updated based on environmental feedback through a memory rewriting mechanism, whereas policy improvement is achieved through efficient memory reading (retrieval). We instantiate our agent model in the deep research setting, namely AgentFly, which attains top-1 on GAIA validation (87.88% Pass@3) and 79.40% on the test set. It reaches 66.6% F1 and 80.4% PM on the DeepResearcher dataset, outperforming the state-of-the-art training-based method, while case-based memory adds 4.7% to 9.6% absolute points on out-of-distribution tasks. Our approach offers a scalable and efficient pathway for developing generalist LLM agents capable of continuous, real-time learning without gradient updates, advancing machine learning towards open-ended skill acquisition and deep research scenarios. The code is available at https://github.com/Agent-on-the-Fly/AgentFly. 11 authors · Published on Aug 22, 2025 Upvote 146 GitHub 1.39k arXiv Page Submitted by taesiri AgentFly: Fine-tuning LLM Agents without Fine-tuning LLMs In this paper, we introduce a novel learning paradigm for adaptive Large Language Model (LLM) agents that eliminates the need for fine-tuning the underlying LLMs. Existing approaches are often either rigid, relying on static, handcrafted reflection workflows, or computationally intensive, requiring gradient updates of LLM model parameters. In contrast, our method enables low-cost continual adaptation via memory-based online reinforcement learning. We formalise this as a Memory-augmented Markov Decision Process (M-MDP), equipped with a neural case-selection policy to guide action decisions. Past experiences are stored in an episodic memory, either differentiable or non-parametric. The policy is continually updated based on environmental feedback through a memory rewriting mechanism, whereas policy improvement is achieved through efficient memory reading (retrieval). We instantiate our agent model in the deep research setting, namely AgentFly, which attains top-1 on GAIA validation (87.88% Pass@3) and 79.40% on the test set. It reaches 66.6% F1 and 80.4% PM on the DeepResearcher dataset, outperforming the state-of-the-art training-based method, while case-based memory adds 4.7% to 9.6% absolute points on out-of-distribution tasks. Our approach offers a scalable and efficient pathway for developing generalist LLM agents capable of continuous, real-time learning without gradient updates, advancing machine learning towards open-ended skill acquisition and deep research scenarios. The code is available at https://github.com/Agent-on-the-Fly/AgentFly. 11 authors · Aug 22, 2025 Upvote 146 GitHub 1.39k arXiv Page Submitted by Lingaaaaaaa Revolutionizing Reinforcement Learning Framework for Diffusion Large Language Models We propose TraceRL, a trajectory-aware reinforcement learning framework for diffusion language models (DLMs) that incorporates preferred inference trajectory into post-training, and is applicable across different architectures. Equipped with a diffusion-based value model that enhances training stability, we demonstrate improved reasoning performance on complex math and coding tasks. Besides, it can also be applied to adapt block-specific models to larger blocks, which improves sampling flexibility. Employing TraceRL, we derive a series of state-of-the-art diffusion language models, namely TraDo. Although smaller than 7B-scale AR models, TraDo-4B-Instruct still consistently outperforms them across complex math reasoning tasks. TraDo-8B-Instruct achieves relative accuracy improvements of 6.1% over Qwen2.5-7B-Instruct and 51.3% over Llama3.1-8B-Instruct on mathematical reasoning benchmarks. Through curriculum learning, we also derive the first long-CoT DLM, outperforming Qwen2.5-7B-Instruct on MATH500 with an 18.1% relative accuracy gain. To facilitate reproducible research and practical applications, we release a comprehensive open-source framework for building, training, and deploying diffusion LLMs across diverse architectures. The framework integrates accelerated KV-cache techniques and inference engines for both inference and reinforcement learning, and includes implementations of various supervised fine-tuning and RL methods for mathematics, coding, and general tasks. Code and Models: https://github.com/Gen-Verse/dLLM-RL 6 authors · Published on Sep 8, 2025 Upvote 49 GitHub 162 arXiv Page Submitted by Lingaaaaaaa Revolutionizing Reinforcement Learning Framework for Diffusion Large Language Models We propose TraceRL, a trajectory-aware reinforcement learning framework for diffusion language models (DLMs) that incorporates preferred inference trajectory into post-training, and is applicable across different architectures. Equipped with a diffusion-based value model that enhances training stability, we demonstrate improved reasoning performance on complex math and coding tasks. Besides, it can also be applied to adapt block-specific models to larger blocks, which improves sampling flexibility. Employing TraceRL, we derive a series of state-of-the-art diffusion language models, namely TraDo. Although smaller than 7B-scale AR models, TraDo-4B-Instruct still consistently outperforms them across complex math reasoning tasks. TraDo-8B-Instruct achieves relative accuracy improvements of 6.1% over Qwen2.5-7B-Instruct and 51.3% over Llama3.1-8B-Instruct on mathematical reasoning benchmarks. Through curriculum learning, we also derive the first long-CoT DLM, outperforming Qwen2.5-7B-Instruct on MATH500 with an 18.1% relative accuracy gain. To facilitate reproducible research and practical applications, we release a comprehensive open-source framework for building, training, and deploying diffusion LLMs across diverse architectures. The framework integrates accelerated KV-cache techniques and inference engines for both inference and reinforcement learning, and includes implementations of various supervised fine-tuning and RL methods for mathematics, coding, and general tasks. Code and Models: https://github.com/Gen-Verse/dLLM-RL 6 authors · Sep 8, 2025 Upvote 49 GitHub 162 arXiv Page Submitted by LooperXX Qwen-Image Technical Report We present Qwen-Image, an image generation foundation model in the Qwen series that achieves significant advances in complex text rendering and precise image editing. To address the challenges of complex text rendering, we design a comprehensive data pipeline that includes large-scale data collection, filtering, annotation, synthesis, and balancing. Moreover, we adopt a progressive training strategy that starts with non-text-to-text rendering, evolves from simple to complex textual inputs, and gradually scales up to paragraph-level descriptions. This curriculum learning approach substantially enhances the model's native text rendering capabilities. As a result, Qwen-Image not only performs exceptionally well in alphabetic languages such as English, but also achieves remarkable progress on more challenging logographic languages like Chinese. To enhance image editing consistency, we introduce an improved multi-task training paradigm that incorporates not only traditional text-to-image (T2I) and text-image-to-image (TI2I) tasks but also image-to-image (I2I) reconstruction, effectively aligning the latent representations between Qwen2.5-VL and MMDiT. Furthermore, we separately feed the original image into Qwen2.5-VL and the VAE encoder to obtain semantic and reconstructive representations, respectively. This dual-encoding mechanism enables the editing module to strike a balance between preserving semantic consistency and maintaining visual fidelity. Qwen-Image achieves state-of-the-art performance, demonstrating its strong capabilities in both image generation and editing across multiple benchmarks. 39 authors · Published on Aug 4, 2025 Upvote 245 GitHub 4.94k arXiv Page Submitted by LooperXX Qwen-Image Technical Report We present Qwen-Image, an image generation foundation model in the Qwen series that achieves significant advances in complex text rendering and precise image editing. To address the challenges of complex text rendering, we design a comprehensive data pipeline that includes large-scale data collection, filtering, annotation, synthesis, and balancing. Moreover, we adopt a progressive training strategy that starts with non-text-to-text rendering, evolves from simple to complex textual inputs, and gradually scales up to paragraph-level descriptions. This curriculum learning approach substantially enhances the model's native text rendering capabilities. As a result, Qwen-Image not only performs exceptionally well in alphabetic languages such as English, but also achieves remarkable progress on more challenging logographic languages like Chinese. To enhance image editing consistency, we introduce an improved multi-task training paradigm that incorporates not only traditional text-to-image (T2I) and text-image-to-image (TI2I) tasks but also image-to-image (I2I) reconstruction, effectively aligning the latent representations between Qwen2.5-VL and MMDiT. Furthermore, we separately feed the original image into Qwen2.5-VL and the VAE encoder to obtain semantic and reconstructive representations, respectively. This dual-encoding mechanism enables the editing module to strike a balance between preserving semantic consistency and maintaining visual fidelity. Qwen-Image achieves state-of-the-art performance, demonstrating its strong capabilities in both image generation and editing across multiple benchmarks. 39 authors · Aug 4, 2025 Upvote 245 GitHub 4.94k arXiv Page Submitted by hpouransari FastVLM: Efficient Vision Encoding for Vision Language Models Scaling the input image resolution is essential for enhancing the performance of Vision Language Models (VLMs), particularly in text-rich image understanding tasks. However, popular visual encoders such as ViTs become inefficient at high resolutions due to the large number of tokens and high encoding latency caused by stacked self-attention layers. At different operational resolutions, the vision encoder of a VLM can be optimized along two axes: reducing encoding latency and minimizing the number of visual tokens passed to the LLM, thereby lowering overall latency. Based on a comprehensive efficiency analysis of the interplay between image resolution, vision latency, token count, and LLM size, we introduce FastVLM, a model that achieves an optimized trade-off between latency, model size and accuracy. FastVLM incorporates FastViTHD, a novel hybrid vision encoder designed to output fewer tokens and significantly reduce encoding time for high-resolution images. Unlike previous methods, FastVLM achieves the optimal balance between visual token count and image resolution solely by scaling the input image, eliminating the need for additional token pruning and simplifying the model design. In the LLaVA-1.5 setup, FastVLM achieves 3.2times improvement in time-to-first-token (TTFT) while maintaining similar performance on VLM benchmarks compared to prior works. Compared to LLaVa-OneVision at the highest resolution (1152times1152), FastVLM achieves comparable performance on key benchmarks like SeedBench and MMMU, using the same 0.5B LLM, but with 85times faster TTFT and a vision encoder that is 3.4times smaller. 11 authors · Published on Dec 17, 2024 Upvote 65 GitHub 6.43k arXiv Page Submitted by hpouransari FastVLM: Efficient Vision Encoding for Vision Language Models Scaling the input image resolution is essential for enhancing the performance of Vision Language Models (VLMs), particularly in text-rich image understanding tasks. However, popular visual encoders such as ViTs become inefficient at high resolutions due to the large number of tokens and high encoding latency caused by stacked self-attention layers. At different operational resolutions, the vision encoder of a VLM can be optimized along two axes: reducing encoding latency and minimizing the number of visual tokens passed to the LLM, thereby lowering overall latency. Based on a comprehensive efficiency analysis of the interplay between image resolution, vision latency, token count, and LLM size, we introduce FastVLM, a model that achieves an optimized trade-off between latency, model size and accuracy. FastVLM incorporates FastViTHD, a novel hybrid vision encoder designed to output fewer tokens and significantly reduce encoding time for high-resolution images. Unlike previous methods, FastVLM achieves the optimal balance between visual token count and image resolution solely by scaling the input image, eliminating the need for additional token pruning and simplifying the model design. In the LLaVA-1.5 setup, FastVLM achieves 3.2times improvement in time-to-first-token (TTFT) while maintaining similar performance on VLM benchmarks compared to prior works. Compared to LLaVa-OneVision at the highest resolution (1152times1152), FastVLM achieves comparable performance on key benchmarks like SeedBench and MMMU, using the same 0.5B LLM, but with 85times faster TTFT and a vision encoder that is 3.4times smaller. 11 authors · Dec 17, 2024 Upvote 65 GitHub 6.43k arXiv Page Submitted by chujiezheng Qwen3 Technical Report In this work, we present Qwen3, the latest version of the Qwen model family. Qwen3 comprises a series of large language models (LLMs) designed to advance performance, efficiency, and multilingual capabilities. The Qwen3 series includes models of both dense and Mixture-of-Expert (MoE) architectures, with parameter scales ranging from 0.6 to 235 billion. A key innovation in Qwen3 is the integration of thinking mode (for complex, multi-step reasoning) and non-thinking mode (for rapid, context-driven responses) into a unified framework. This eliminates the need to switch between different models--such as chat-optimized models (e.g., GPT-4o) and dedicated reasoning models (e.g., QwQ-32B)--and enables dynamic mode switching based on user queries or chat templates. Meanwhile, Qwen3 introduces a thinking budget mechanism, allowing users to allocate computational resources adaptively during inference, thereby balancing latency and performance based on task complexity. Moreover, by leveraging the knowledge from the flagship models, we significantly reduce the computational resources required to build smaller-scale models, while ensuring their highly competitive performance. Empirical evaluations demonstrate that Qwen3 achieves state-of-the-art results across diverse benchmarks, including tasks in code generation, mathematical reasoning, agent tasks, etc., competitive against larger MoE models and proprietary models. Compared to its predecessor Qwen2.5, Qwen3 expands multilingual support from 29 to 119 languages and dialects, enhancing global accessibility through improved cross-lingual understanding and generation capabilities. To facilitate reproducibility and community-driven research and development, all Qwen3 models are publicly accessible under Apache 2.0. 60 authors · Published on May 14, 2025 Upvote 290 GitHub 24.5k arXiv Page Submitted by chujiezheng Qwen3 Technical Report In this work, we present Qwen3, the latest version of the Qwen model family. Qwen3 comprises a series of large language models (LLMs) designed to advance performance, efficiency, and multilingual capabilities. The Qwen3 series includes models of both dense and Mixture-of-Expert (MoE) architectures, with parameter scales ranging from 0.6 to 235 billion. A key innovation in Qwen3 is the integration of thinking mode (for complex, multi-step reasoning) and non-thinking mode (for rapid, context-driven responses) into a unified framework. This eliminates the need to switch between different models--such as chat-optimized models (e.g., GPT-4o) and dedicated reasoning models (e.g., QwQ-32B)--and enables dynamic mode switching based on user queries or chat templates. Meanwhile, Qwen3 introduces a thinking budget mechanism, allowing users to allocate computational resources adaptively during inference, thereby balancing latency and performance based on task complexity. Moreover, by leveraging the knowledge from the flagship models, we significantly reduce the computational resources required to build smaller-scale models, while ensuring their highly competitive performance. Empirical evaluations demonstrate that Qwen3 achieves state-of-the-art results across diverse benchmarks, including tasks in code generation, mathematical reasoning, agent tasks, etc., competitive against larger MoE models and proprietary models. Compared to its predecessor Qwen2.5, Qwen3 expands multilingual support from 29 to 119 languages and dialects, enhancing global accessibility through improved cross-lingual understanding and generation capabilities. To facilitate reproducibility and community-driven research and development, all Qwen3 models are publicly accessible under Apache 2.0. 60 authors · May 14, 2025 Upvote 290 GitHub 24.5k arXiv Page Submitted by deleted RepoMaster: Autonomous Exploration and Understanding of GitHub Repositories for Complex Task Solving The ultimate goal of code agents is to solve complex tasks autonomously. Although large language models (LLMs) have made substantial progress in code generation, real-world tasks typically demand full-fledged code repositories rather than simple scripts. Building such repositories from scratch remains a major challenge. Fortunately, GitHub hosts a vast, evolving collection of open-source repositories, which developers frequently reuse as modular components for complex tasks. Yet, existing frameworks like OpenHands and SWE-Agent still struggle to effectively leverage these valuable resources. Relying solely on README files provides insufficient guidance, and deeper exploration reveals two core obstacles: overwhelming information and tangled dependencies of repositories, both constrained by the limited context windows of current LLMs. To tackle these issues, we propose RepoMaster, an autonomous agent framework designed to explore and reuse GitHub repositories for solving complex tasks. For efficient understanding, RepoMaster constructs function-call graphs, module-dependency graphs, and hierarchical code trees to identify essential components, providing only identified core elements to the LLMs rather than the entire repository. During autonomous execution, it progressively explores related components using our exploration tools and prunes information to optimize context usage. Evaluated on the adjusted MLE-bench, RepoMaster achieves a 110% relative boost in valid submissions over the strongest baseline OpenHands. On our newly released GitTaskBench, RepoMaster lifts the task-pass rate from 40.7% to 62.9% while reducing token usage by 95%. Our code and demonstration materials are publicly available at https://github.com/QuantaAlpha/RepoMaster. 14 authors · Published on May 27, 2025 Upvote 2 GitHub 277 arXiv Page Submitted by deleted RepoMaster: Autonomous Exploration and Understanding of GitHub Repositories for Complex Task Solving The ultimate goal of code agents is to solve complex tasks autonomously. Although large language models (LLMs) have made substantial progress in code generation, real-world tasks typically demand full-fledged code repositories rather than simple scripts. Building such repositories from scratch remains a major challenge. Fortunately, GitHub hosts a vast, evolving collection of open-source repositories, which developers frequently reuse as modular components for complex tasks. Yet, existing frameworks like OpenHands and SWE-Agent still struggle to effectively leverage these valuable resources. Relying solely on README files provides insufficient guidance, and deeper exploration reveals two core obstacles: overwhelming information and tangled dependencies of repositories, both constrained by the limited context windows of current LLMs. To tackle these issues, we propose RepoMaster, an autonomous agent framework designed to explore and reuse GitHub repositories for solving complex tasks. For efficient understanding, RepoMaster constructs function-call graphs, module-dependency graphs, and hierarchical code trees to identify essential components, providing only identified core elements to the LLMs rather than the entire repository. During autonomous execution, it progressively explores related components using our exploration tools and prunes information to optimize context usage. Evaluated on the adjusted MLE-bench, RepoMaster achieves a 110% relative boost in valid submissions over the strongest baseline OpenHands. On our newly released GitTaskBench, RepoMaster lifts the task-pass rate from 40.7% to 62.9% while reducing token usage by 95%. Our code and demonstration materials are publicly available at https://github.com/QuantaAlpha/RepoMaster. 14 authors · May 27, 2025 Upvote 2 GitHub 277 arXiv Page Submitted by Kaichengalex Gradient-Attention Guided Dual-Masking Synergetic Framework for Robust Text-based Person Retrieval Although Contrastive Language-Image Pre-training (CLIP) exhibits strong performance across diverse vision tasks, its application to person representation learning faces two critical challenges: (i) the scarcity of large-scale annotated vision-language data focused on person-centric images, and (ii) the inherent limitations of global contrastive learning, which struggles to maintain discriminative local features crucial for fine-grained matching while remaining vulnerable to noisy text tokens. This work advances CLIP for person representation learning through synergistic improvements in data curation and model architecture. First, we develop a noise-resistant data construction pipeline that leverages the in-context learning capabilities of MLLMs to automatically filter and caption web-sourced images. This yields WebPerson, a large-scale dataset of 5M high-quality person-centric image-text pairs. Second, we introduce the GA-DMS (Gradient-Attention Guided Dual-Masking Synergetic) framework, which improves cross-modal alignment by adaptively masking noisy textual tokens based on the gradient-attention similarity score. Additionally, we incorporate masked token prediction objectives that compel the model to predict informative text tokens, enhancing fine-grained semantic representation learning. Extensive experiments show that GA-DMS achieves state-of-the-art performance across multiple benchmarks. 6 authors · Published on Sep 11, 2025 Upvote 5 GitHub 9 arXiv Page Submitted by Kaichengalex Gradient-Attention Guided Dual-Masking Synergetic Framework for Robust Text-based Person Retrieval Although Contrastive Language-Image Pre-training (CLIP) exhibits strong performance across diverse vision tasks, its application to person representation learning faces two critical challenges: (i) the scarcity of large-scale annotated vision-language data focused on person-centric images, and (ii) the inherent limitations of global contrastive learning, which struggles to maintain discriminative local features crucial for fine-grained matching while remaining vulnerable to noisy text tokens. This work advances CLIP for person representation learning through synergistic improvements in data curation and model architecture. First, we develop a noise-resistant data construction pipeline that leverages the in-context learning capabilities of MLLMs to automatically filter and caption web-sourced images. This yields WebPerson, a large-scale dataset of 5M high-quality person-centric image-text pairs. Second, we introduce the GA-DMS (Gradient-Attention Guided Dual-Masking Synergetic) framework, which improves cross-modal alignment by adaptively masking noisy textual tokens based on the gradient-attention similarity score. Additionally, we incorporate masked token prediction objectives that compel the model to predict informative text tokens, enhancing fine-grained semantic representation learning. Extensive experiments show that GA-DMS achieves state-of-the-art performance across multiple benchmarks. 6 authors · Sep 11, 2025 Upvote 5 GitHub 9 arXiv Page Submitted by Hanyuezhuohua Parallel-R1: Towards Parallel Thinking via Reinforcement Learning Parallel thinking has emerged as a novel approach for enhancing the reasoning capabilities of large language models (LLMs) by exploring multiple reasoning paths concurrently. However, activating such capabilities through training remains challenging, as existing methods predominantly rely on supervised fine-tuning (SFT) over synthetic data, which encourages teacher-forced imitation rather than exploration and generalization. Different from them, we propose Parallel-R1, the first reinforcement learning (RL) framework that enables parallel thinking behaviors for complex real-world reasoning tasks. Our framework employs a progressive curriculum that explicitly addresses the cold-start problem in training parallel thinking with RL. We first use SFT on prompt-generated trajectories from easier tasks to instill the parallel thinking ability, then transition to RL to explore and generalize this skill on harder problems. Experiments on various math benchmarks, including MATH, AMC23, and AIME, show that Parallel-R1 successfully instills parallel thinking, leading to 8.4% accuracy improvements over the sequential thinking model trained directly on challenging tasks with RL. Further analysis reveals a clear shift in the model's thinking behavior: at an early stage, it uses parallel thinking as an exploration strategy, while in a later stage, it uses the same capability for multi-perspective verification. Most significantly, we validate parallel thinking as a mid-training exploration scaffold, where this temporary exploratory phase unlocks a higher performance ceiling after RL, yielding a 42.9% improvement over the baseline on AIME25. Our model, data, and code will be open-source at https://github.com/zhengkid/Parallel-R1. 11 authors · Published on Sep 9, 2025 Upvote 84 GitHub 58 arXiv Page Submitted by Hanyuezhuohua Parallel-R1: Towards Parallel Thinking via Reinforcement Learning Parallel thinking has emerged as a novel approach for enhancing the reasoning capabilities of large language models (LLMs) by exploring multiple reasoning paths concurrently. However, activating such capabilities through training remains challenging, as existing methods predominantly rely on supervised fine-tuning (SFT) over synthetic data, which encourages teacher-forced imitation rather than exploration and generalization. Different from them, we propose Parallel-R1, the first reinforcement learning (RL) framework that enables parallel thinking behaviors for complex real-world reasoning tasks. Our framework employs a progressive curriculum that explicitly addresses the cold-start problem in training parallel thinking with RL. We first use SFT on prompt-generated trajectories from easier tasks to instill the parallel thinking ability, then transition to RL to explore and generalize this skill on harder problems. Experiments on various math benchmarks, including MATH, AMC23, and AIME, show that Parallel-R1 successfully instills parallel thinking, leading to 8.4% accuracy improvements over the sequential thinking model trained directly on challenging tasks with RL. Further analysis reveals a clear shift in the model's thinking behavior: at an early stage, it uses parallel thinking as an exploration strategy, while in a later stage, it uses the same capability for multi-perspective verification. Most significantly, we validate parallel thinking as a mid-training exploration scaffold, where this temporary exploratory phase unlocks a higher performance ceiling after RL, yielding a 42.9% improvement over the baseline on AIME25. Our model, data, and code will be open-source at https://github.com/zhengkid/Parallel-R1. 11 authors · Sep 9, 2025 Upvote 84 GitHub 58 arXiv Page Submitted by tulvgengenr MixGRPO: Unlocking Flow-based GRPO Efficiency with Mixed ODE-SDE Although GRPO substantially enhances flow matching models in human preference alignment of image generation, methods such as FlowGRPO still exhibit inefficiency due to the necessity of sampling and optimizing over all denoising steps specified by the Markov Decision Process (MDP). In this paper, we propose MixGRPO, a novel framework that leverages the flexibility of mixed sampling strategies through the integration of stochastic differential equations (SDE) and ordinary differential equations (ODE). This streamlines the optimization process within the MDP to improve efficiency and boost performance. Specifically, MixGRPO introduces a sliding window mechanism, using SDE sampling and GRPO-guided optimization only within the window, while applying ODE sampling outside. This design confines sampling randomness to the time-steps within the window, thereby reducing the optimization overhead, and allowing for more focused gradient updates to accelerate convergence. Additionally, as time-steps beyond the sliding window are not involved in optimization, higher-order solvers are supported for sampling. So we present a faster variant, termed MixGRPO-Flash, which further improves training efficiency while achieving comparable performance. MixGRPO exhibits substantial gains across multiple dimensions of human preference alignment, outperforming DanceGRPO in both effectiveness and efficiency, with nearly 50% lower training time. Notably, MixGRPO-Flash further reduces training time by 71%. Codes and models are available at https://github.com/Tencent-Hunyuan/MixGRPO{MixGRPO}. 7 authors · Published on Jul 29, 2025 Upvote 12 GitHub 572 arXiv Page Submitted by tulvgengenr MixGRPO: Unlocking Flow-based GRPO Efficiency with Mixed ODE-SDE Although GRPO substantially enhances flow matching models in human preference alignment of image generation, methods such as FlowGRPO still exhibit inefficiency due to the necessity of sampling and optimizing over all denoising steps specified by the Markov Decision Process (MDP). In this paper, we propose MixGRPO, a novel framework that leverages the flexibility of mixed sampling strategies through the integration of stochastic differential equations (SDE) and ordinary differential equations (ODE). This streamlines the optimization process within the MDP to improve efficiency and boost performance. Specifically, MixGRPO introduces a sliding window mechanism, using SDE sampling and GRPO-guided optimization only within the window, while applying ODE sampling outside. This design confines sampling randomness to the time-steps within the window, thereby reducing the optimization overhead, and allowing for more focused gradient updates to accelerate convergence. Additionally, as time-steps beyond the sliding window are not involved in optimization, higher-order solvers are supported for sampling. So we present a faster variant, termed MixGRPO-Flash, which further improves training efficiency while achieving comparable performance. MixGRPO exhibits substantial gains across multiple dimensions of human preference alignment, outperforming DanceGRPO in both effectiveness and efficiency, with nearly 50% lower training time. Notably, MixGRPO-Flash further reduces training time by 71%. Codes and models are available at https://github.com/Tencent-Hunyuan/MixGRPO{MixGRPO}. 7 authors · Jul 29, 2025 Upvote 12 GitHub 572 arXiv Page Submitted by Nickyang Hunyuan-MT Technical Report In this report, we introduce Hunyuan-MT-7B, our first open-source multilingual translation model, which supports bidirectional translation across 33 major languages and places a special emphasis on translation between Mandarin and several ethnic minority languages as well as dialects. Furthermore, to serve and address diverse translation scenarios and enhance model performance at test time, we introduce Hunyuan-MT-Chimera-7B, a translation model inspired by the slow thinking mode. This model integrates multiple outputs generated by the Hunyuan-MT-7B model under varying parameter settings, thereby achieving performance superior to that of conventional slow-thinking models based on Chain-of-Thought (CoT). The development of our models follows a holistic training process specifically engineered for multilingual translation, which begins with general and MT-oriented pre-training to build foundational capabilities, proceeds to Supervised Fine-Tuning (SFT) for task-specific adaptation, and culminates in advanced alignment through Reinforcement Learning (RL) and weak-to-strong RL. Through comprehensive experimentation, we demonstrate that both Hunyuan-MT-7B and Hunyuan-MT-Chimera-7B significantly outperform all translation-specific models of comparable parameter size and most of the SOTA large models, particularly on the task of translation between Mandarin and minority languages as well as dialects. In the WMT2025 shared task (General Machine Translation), our models demonstrate state-of-the-art performance, ranking first in 30 out of 31 language pairs. This result highlights the robustness of our models across a diverse linguistic spectrum, encompassing high-resource languages such as Chinese, English, and Japanese, as well as low-resource languages including Czech, Marathi, Estonian, and Icelandic. 7 authors · Published on Sep 5, 2025 Upvote 9 GitHub 492 arXiv Page Submitted by Nickyang Hunyuan-MT Technical Report In this report, we introduce Hunyuan-MT-7B, our first open-source multilingual translation model, which supports bidirectional translation across 33 major languages and places a special emphasis on translation between Mandarin and several ethnic minority languages as well as dialects. Furthermore, to serve and address diverse translation scenarios and enhance model performance at test time, we introduce Hunyuan-MT-Chimera-7B, a translation model inspired by the slow thinking mode. This model integrates multiple outputs generated by the Hunyuan-MT-7B model under varying parameter settings, thereby achieving performance superior to that of conventional slow-thinking models based on Chain-of-Thought (CoT). The development of our models follows a holistic training process specifically engineered for multilingual translation, which begins with general and MT-oriented pre-training to build foundational capabilities, proceeds to Supervised Fine-Tuning (SFT) for task-specific adaptation, and culminates in advanced alignment through Reinforcement Learning (RL) and weak-to-strong RL. Through comprehensive experimentation, we demonstrate that both Hunyuan-MT-7B and Hunyuan-MT-Chimera-7B significantly outperform all translation-specific models of comparable parameter size and most of the SOTA large models, particularly on the task of translation between Mandarin and minority languages as well as dialects. In the WMT2025 shared task (General Machine Translation), our models demonstrate state-of-the-art performance, ranking first in 30 out of 31 language pairs. This result highlights the robustness of our models across a diverse linguistic spectrum, encompassing high-resource languages such as Chinese, English, and Japanese, as well as low-resource languages including Czech, Marathi, Estonian, and Icelandic. 7 authors · Sep 5, 2025 Upvote 9 GitHub 492 arXiv Page Submitted by hiyouga Easy Dataset: A Unified and Extensible Framework for Synthesizing LLM Fine-Tuning Data from Unstructured Documents Large language models (LLMs) have shown impressive performance on general-purpose tasks, yet adapting them to specific domains remains challenging due to the scarcity of high-quality domain data. Existing data synthesis tools often struggle to extract reliable fine-tuning data from heterogeneous documents effectively. To address this limitation, we propose Easy Dataset, a unified framework for synthesizing fine-tuning data from unstructured documents via an intuitive graphical user interface (GUI). Specifically, Easy Dataset allows users to easily configure text extraction models and chunking strategies to transform raw documents into coherent text chunks. It then leverages a persona-driven prompting approach to generate diverse question-answer pairs using public-available LLMs. Throughout the pipeline, a human-in-the-loop visual interface facilitates the review and refinement of intermediate outputs to ensure data quality. Experiments on a financial question-answering task show that fine-tuning LLMs on the synthesized dataset significantly improves domain-specific performance while preserving general knowledge. The source code and installable package are available at https://github.com/ConardLi/easy-dataset and have garnered over 9,000 GitHub stars. 7 authors · Published on Jul 5, 2025 Upvote 42 GitHub 10.7k arXiv Page Submitted by hiyouga Easy Dataset: A Unified and Extensible Framework for Synthesizing LLM Fine-Tuning Data from Unstructured Documents Large language models (LLMs) have shown impressive performance on general-purpose tasks, yet adapting them to specific domains remains challenging due to the scarcity of high-quality domain data. Existing data synthesis tools often struggle to extract reliable fine-tuning data from heterogeneous documents effectively. To address this limitation, we propose Easy Dataset, a unified framework for synthesizing fine-tuning data from unstructured documents via an intuitive graphical user interface (GUI). Specifically, Easy Dataset allows users to easily configure text extraction models and chunking strategies to transform raw documents into coherent text chunks. It then leverages a persona-driven prompting approach to generate diverse question-answer pairs using public-available LLMs. Throughout the pipeline, a human-in-the-loop visual interface facilitates the review and refinement of intermediate outputs to ensure data quality. Experiments on a financial question-answering task show that fine-tuning LLMs on the synthesized dataset significantly improves domain-specific performance while preserving general knowledge. The source code and installable package are available at https://github.com/ConardLi/easy-dataset and have garnered over 9,000 GitHub stars. 7 authors · Jul 5, 2025 Upvote 42 GitHub 10.7k arXiv Page Submitted by fenfan UMO: Scaling Multi-Identity Consistency for Image Customization via Matching Reward Recent advancements in image customization exhibit a wide range of application prospects due to stronger customization capabilities. However, since we humans are more sensitive to faces, a significant challenge remains in preserving consistent identity while avoiding identity confusion with multi-reference images, limiting the identity scalability of customization models. To address this, we present UMO, a Unified Multi-identity Optimization framework, designed to maintain high-fidelity identity preservation and alleviate identity confusion with scalability. With \"multi-to-multi matching\" paradigm, UMO reformulates multi-identity generation as a global assignment optimization problem and unleashes multi-identity consistency for existing image customization methods generally through reinforcement learning on diffusion models. To facilitate the training of UMO, we develop a scalable customization dataset with multi-reference images, consisting of both synthesised and real parts. Additionally, we propose a new metric to measure identity confusion. Extensive experiments demonstrate that UMO not only improves identity consistency significantly, but also reduces identity confusion on several image customization methods, setting a new state-of-the-art among open-source methods along the dimension of identity preserving. Code and model: https://github.com/bytedance/UMO 6 authors · Published on Sep 8, 2025 Upvote 27 GitHub 70 arXiv Page Submitted by fenfan UMO: Scaling Multi-Identity Consistency for Image Customization via Matching Reward Recent advancements in image customization exhibit a wide range of application prospects due to stronger customization capabilities. However, since we humans are more sensitive to faces, a significant challenge remains in preserving consistent identity while avoiding identity confusion with multi-reference images, limiting the identity scalability of customization models. To address this, we present UMO, a Unified Multi-identity Optimization framework, designed to maintain high-fidelity identity preservation and alleviate identity confusion with scalability. With \"multi-to-multi matching\" paradigm, UMO reformulates multi-identity generation as a global assignment optimization problem and unleashes multi-identity consistency for existing image customization methods generally through reinforcement learning on diffusion models. To facilitate the training of UMO, we develop a scalable customization dataset with multi-reference images, consisting of both synthesised and real parts. Additionally, we propose a new metric to measure identity confusion. Extensive experiments demonstrate that UMO not only improves identity consistency significantly, but also reduces identity confusion on several image customization methods, setting a new state-of-the-art among open-source methods along the dimension of identity preserving. Code and model: https://github.com/bytedance/UMO 6 authors · Sep 8, 2025 Upvote 27 GitHub 70 arXiv Page Submitted by deleted Killing Two Birds with One Stone:Efficient and Robust Training of Face Recognition CNNs by Partial FC Learning discriminative deep feature embeddings by using million-scale in-the-wild datasets and margin-based softmax loss is the current state-of-the-art approach for face recognition. However, the memory and computing cost of the Fully Connected (FC) layer linearly scales up to the number of identities in the training set. Besides, the large-scale training data inevitably suffers from inter-class conflict and long-tailed distribution. In this paper, we propose a sparsely updating variant of the FC layer, named Partial FC (PFC). In each iteration, positive class centers and a random subset of negative class centers are selected to compute the margin-based softmax loss. All class centers are still maintained throughout the whole training process, but only a subset is selected and updated in each iteration. Therefore, the computing requirement, the probability of inter-class conflict, and the frequency of passive update on tail class centers, are dramatically reduced. Extensive experiments across different training data and backbones (e.g. CNN and ViT) confirm the effectiveness, robustness and efficiency of the proposed PFC. The source code is available at \\https://github.com/deepinsight/insightface/tree/master/recognition. 7 authors · Published on Mar 28, 2022 Upvote 2 GitHub 26.5k arXiv Page Submitted by deleted Killing Two Birds with One Stone:Efficient and Robust Training of Face Recognition CNNs by Partial FC Learning discriminative deep feature embeddings by using million-scale in-the-wild datasets and margin-based softmax loss is the current state-of-the-art approach for face recognition. However, the memory and computing cost of the Fully Connected (FC) layer linearly scales up to the number of identities in the training set. Besides, the large-scale training data inevitably suffers from inter-class conflict and long-tailed distribution. In this paper, we propose a sparsely updating variant of the FC layer, named Partial FC (PFC). In each iteration, positive class centers and a random subset of negative class centers are selected to compute the margin-based softmax loss. All class centers are still maintained throughout the whole training process, but only a subset is selected and updated in each iteration. Therefore, the computing requirement, the probability of inter-class conflict, and the frequency of passive update on tail class centers, are dramatically reduced. Extensive experiments across different training data and backbones (e.g. CNN and ViT) confirm the effectiveness, robustness and efficiency of the proposed PFC. The source code is available at \\https://github.com/deepinsight/insightface/tree/master/recognition. 7 authors · Mar 28, 2022 Upvote 2 GitHub 26.5k arXiv Page Submitted by xianbao GLM-4.5: Agentic, Reasoning, and Coding (ARC) Foundation Models We present GLM-4.5, an open-source Mixture-of-Experts (MoE) large language model with 355B total parameters and 32B activated parameters, featuring a hybrid reasoning method that supports both thinking and direct response modes. Through multi-stage training on 23T tokens and comprehensive post-training with expert model iteration and reinforcement learning, GLM-4.5 achieves strong performance across agentic, reasoning, and coding (ARC) tasks, scoring 70.1% on TAU-Bench, 91.0% on AIME 24, and 64.2% on SWE-bench Verified. With much fewer parameters than several competitors, GLM-4.5 ranks 3rd overall among all evaluated models and 2nd on agentic benchmarks. We release both GLM-4.5 (355B parameters) and a compact version, GLM-4.5-Air (106B parameters), to advance research in reasoning and agentic AI systems. Code, models, and more information are available at https://github.com/zai-org/GLM-4.5. 171 authors · Published on Aug 8, 2025 Upvote 178 GitHub 2.57k arXiv Page Submitted by xianbao GLM-4.5: Agentic, Reasoning, and Coding (ARC) Foundation Models We present GLM-4.5, an open-source Mixture-of-Experts (MoE) large language model with 355B total parameters and 32B activated parameters, featuring a hybrid reasoning method that supports both thinking and direct response modes. Through multi-stage training on 23T tokens and comprehensive post-training with expert model iteration and reinforcement learning, GLM-4.5 achieves strong performance across agentic, reasoning, and coding (ARC) tasks, scoring 70.1% on TAU-Bench, 91.0% on AIME 24, and 64.2% on SWE-bench Verified. With much fewer parameters than several competitors, GLM-4.5 ranks 3rd overall among all evaluated models and 2nd on agentic benchmarks. We release both GLM-4.5 (355B parameters) and a compact version, GLM-4.5-Air (106B parameters), to advance research in reasoning and agentic AI systems. Code, models, and more information are available at https://github.com/zai-org/GLM-4.5. 171 authors · Aug 8, 2025 Upvote 178 GitHub 2.57k arXiv Page Submitted by JianyuanWang VGGT: Visual Geometry Grounded Transformer We present VGGT, a feed-forward neural network that directly infers all key 3D attributes of a scene, including camera parameters, point maps, depth maps, and 3D point tracks, from one, a few, or hundreds of its views. This approach is a step forward in 3D computer vision, where models have typically been constrained to and specialized for single tasks. It is also simple and efficient, reconstructing images in under one second, and still outperforming alternatives that require post-processing with visual geometry optimization techniques. The network achieves state-of-the-art results in multiple 3D tasks, including camera parameter estimation, multi-view depth estimation, dense point cloud reconstruction, and 3D point tracking. We also show that using pretrained VGGT as a feature backbone significantly enhances downstream tasks, such as non-rigid point tracking and feed-forward novel view synthesis. Code and models are publicly available at https://github.com/facebookresearch/vggt. 6 authors · Published on Mar 14, 2025 Upvote 29 GitHub 10.9k arXiv Page Submitted by JianyuanWang VGGT: Visual Geometry Grounded Transformer We present VGGT, a feed-forward neural network that directly infers all key 3D attributes of a scene, including camera parameters, point maps, depth maps, and 3D point tracks, from one, a few, or hundreds of its views. This approach is a step forward in 3D computer vision, where models have typically been constrained to and specialized for single tasks. It is also simple and efficient, reconstructing images in under one second, and still outperforming alternatives that require post-processing with visual geometry optimization techniques. The network achieves state-of-the-art results in multiple 3D tasks, including camera parameter estimation, multi-view depth estimation, dense point cloud reconstruction, and 3D point tracking. We also show that using pretrained VGGT as a feature backbone significantly enhances downstream tasks, such as non-rigid point tracking and feed-forward novel view synthesis. Code and models are publicly available at https://github.com/facebookresearch/vggt. 6 authors · Mar 14, 2025 Upvote 29 GitHub 10.9k arXiv Page Submitted by callanwu WebDancer: Towards Autonomous Information Seeking Agency Addressing intricate real-world problems necessitates in-depth information seeking and multi-step reasoning. Recent progress in agentic systems, exemplified by Deep Research, underscores the potential for autonomous multi-step research. In this work, we present a cohesive paradigm for building end-to-end agentic information seeking agents from a data-centric and training-stage perspective. Our approach consists of four key stages: (1) browsing data construction, (2) trajectories sampling, (3) supervised fine-tuning for effective cold start, and (4) reinforcement learning for enhanced generalisation. We instantiate this framework in a web agent based on the ReAct, WebDancer. Empirical evaluations on the challenging information seeking benchmarks, GAIA and WebWalkerQA, demonstrate the strong performance of WebDancer, achieving considerable results and highlighting the efficacy of our training paradigm. Further analysis of agent training provides valuable insights and actionable, systematic pathways for developing more capable agentic models. The codes and demo will be released in https://github.com/Alibaba-NLP/WebAgent. 12 authors · Published on May 28, 2025 Upvote 30 GitHub 6.54k arXiv Page Submitted by callanwu WebDancer: Towards Autonomous Information Seeking Agency Addressing intricate real-world problems necessitates in-depth information seeking and multi-step reasoning. Recent progress in agentic systems, exemplified by Deep Research, underscores the potential for autonomous multi-step research. In this work, we present a cohesive paradigm for building end-to-end agentic information seeking agents from a data-centric and training-stage perspective. Our approach consists of four key stages: (1) browsing data construction, (2) trajectories sampling, (3) supervised fine-tuning for effective cold start, and (4) reinforcement learning for enhanced generalisation. We instantiate this framework in a web agent based on the ReAct, WebDancer. Empirical evaluations on the challenging information seeking benchmarks, GAIA and WebWalkerQA, demonstrate the strong performance of WebDancer, achieving considerable results and highlighting the efficacy of our training paradigm. Further analysis of agent training provides valuable insights and actionable, systematic pathways for developing more capable agentic models. The codes and demo will be released in https://github.com/Alibaba-NLP/WebAgent. 12 authors · May 28, 2025 Upvote 30 GitHub 6.54k arXiv Page Submitted by callanwu WebShaper: Agentically Data Synthesizing via Information-Seeking Formalization The advent of Large Language Model (LLM)-powered agents has revolutionized artificial intelligence by enabling solutions to complex, open-ended tasks through web-based information-seeking (IS) capabilities. The scarcity of high-quality training data has limited the development of IS agents. Existing approaches typically adopt an information-driven paradigm that first collects web data and then generates questions based on the retrieval. However, this may lead to inconsistency between information structure and reasoning structure, question and answer. To mitigate, we propose a formalization-driven IS data synthesis framework WebShaper to construct a dataset. WebShaper systematically formalizes IS tasks through set theory. Central to the formalization is the concept of Knowledge Projections (KP), which enables precise control over reasoning structure by KP operation compositions. During synthesis, we begin by creating seed tasks, then use a multi-step expansion process. At each step, an agentic Expander expands the current formal question more complex with retrieval and validation tools based on our formalization. We train our model on the synthesized dataset. Experiment results demonstrate that WebShaper achieves state-of-the-art performance among open-sourced IS agents on GAIA and WebWalkerQA benchmarks. 13 authors · Published on Jul 20, 2025 Upvote 54 GitHub 6.54k arXiv Page Submitted by callanwu WebShaper: Agentically Data Synthesizing via Information-Seeking Formalization The advent of Large Language Model (LLM)-powered agents has revolutionized artificial intelligence by enabling solutions to complex, open-ended tasks through web-based information-seeking (IS) capabilities. The scarcity of high-quality training data has limited the development of IS agents. Existing approaches typically adopt an information-driven paradigm that first collects web data and then generates questions based on the retrieval. However, this may lead to inconsistency between information structure and reasoning structure, question and answer. To mitigate, we propose a formalization-driven IS data synthesis framework WebShaper to construct a dataset. WebShaper systematically formalizes IS tasks through set theory. Central to the formalization is the concept of Knowledge Projections (KP), which enables precise control over reasoning structure by KP operation compositions. During synthesis, we begin by creating seed tasks, then use a multi-step expansion process. At each step, an agentic Expander expands the current formal question more complex with retrieval and validation tools based on our formalization. We train our model on the synthesized dataset. Experiment results demonstrate that WebShaper achieves state-of-the-art performance among open-sourced IS agents on GAIA and WebWalkerQA benchmarks. 13 authors · Jul 20, 2025 Upvote 54 GitHub 6.54k arXiv Page Submitted by richardxp888 WebWatcher: Breaking New Frontier of Vision-Language Deep Research Agent Web agents such as Deep Research have demonstrated superhuman cognitive abilities, capable of solving highly challenging information-seeking problems. However, most research remains primarily text-centric, overlooking visual information in the real world. This makes multimodal Deep Research highly challenging, as such agents require much stronger reasoning abilities in perception, logic, knowledge, and the use of more sophisticated tools compared to text-based agents. To address this limitation, we introduce WebWatcher, a multi-modal Agent for Deep Research equipped with enhanced visual-language reasoning capabilities. It leverages high-quality synthetic multimodal trajectories for efficient cold start training, utilizes various tools for deep reasoning, and further enhances generalization through reinforcement learning. To better evaluate the capabilities of multimodal agents, we propose BrowseComp-VL, a benchmark with BrowseComp-style that requires complex information retrieval involving both visual and textual information. Experimental results show that WebWatcher significantly outperforms proprietary baseline, RAG workflow and open-source agents in four challenging VQA benchmarks, which paves the way for solving complex multimodal information-seeking tasks. 14 authors · Published on Aug 7, 2025 Upvote 124 GitHub 6.54k arXiv Page Submitted by richardxp888 WebWatcher: Breaking New Frontier of Vision-Language Deep Research Agent Web agents such as Deep Research have demonstrated superhuman cognitive abilities, capable of solving highly challenging information-seeking problems. However, most research remains primarily text-centric, overlooking visual information in the real world. This makes multimodal Deep Research highly challenging, as such agents require much stronger reasoning abilities in perception, logic, knowledge, and the use of more sophisticated tools compared to text-based agents. To address this limitation, we introduce WebWatcher, a multi-modal Agent for Deep Research equipped with enhanced visual-language reasoning capabilities. It leverages high-quality synthetic multimodal trajectories for efficient cold start training, utilizes various tools for deep reasoning, and further enhances generalization through reinforcement learning. To better evaluate the capabilities of multimodal agents, we propose BrowseComp-VL, a benchmark with BrowseComp-style that requires complex information retrieval involving both visual and textual information. Experimental results show that WebWatcher significantly outperforms proprietary baseline, RAG workflow and open-source agents in four challenging VQA benchmarks, which paves the way for solving complex multimodal information-seeking tasks. 14 authors · Aug 7, 2025 Upvote 124 GitHub 6.54k arXiv Page Submitted by taesiri ST-Raptor: LLM-Powered Semi-Structured Table Question Answering Semi-structured tables, widely used in real-world applications (e.g., financial reports, medical records, transactional orders), often involve flexible and complex layouts (e.g., hierarchical headers and merged cells). These tables generally rely on human analysts to interpret table layouts and answer relevant natural language questions, which is costly and inefficient. To automate the procedure, existing methods face significant challenges. First, methods like NL2SQL require converting semi-structured tables into structured ones, which often causes substantial information loss. Second, methods like NL2Code and multi-modal LLM QA struggle to understand the complex layouts of semi-structured tables and cannot accurately answer corresponding questions. To this end, we propose ST-Raptor, a tree-based framework for semi-structured table question answering using large language models. First, we introduce the Hierarchical Orthogonal Tree (HO-Tree), a structural model that captures complex semi-structured table layouts, along with an effective algorithm for constructing the tree. Second, we define a set of basic tree operations to guide LLMs in executing common QA tasks. Given a user question, ST-Raptor decomposes it into simpler sub-questions, generates corresponding tree operation pipelines, and conducts operation-table alignment for accurate pipeline execution. Third, we incorporate a two-stage verification mechanism: forward validation checks the correctness of execution steps, while backward validation evaluates answer reliability by reconstructing queries from predicted answers. To benchmark the performance, we present SSTQA, a dataset of 764 questions over 102 real-world semi-structured tables. Experiments show that ST-Raptor outperforms nine baselines by up to 20% in answer accuracy. The code is available at https://github.com/weAIDB/ST-Raptor. 9 authors · Published on Aug 25, 2025 Upvote 6 GitHub 46 arXiv Page Submitted by taesiri ST-Raptor: LLM-Powered Semi-Structured Table Question Answering Semi-structured tables, widely used in real-world applications (e.g., financial reports, medical records, transactional orders), often involve flexible and complex layouts (e.g., hierarchical headers and merged cells). These tables generally rely on human analysts to interpret table layouts and answer relevant natural language questions, which is costly and inefficient. To automate the procedure, existing methods face significant challenges. First, methods like NL2SQL require converting semi-structured tables into structured ones, which often causes substantial information loss. Second, methods like NL2Code and multi-modal LLM QA struggle to understand the complex layouts of semi-structured tables and cannot accurately answer corresponding questions. To this end, we propose ST-Raptor, a tree-based framework for semi-structured table question answering using large language models. First, we introduce the Hierarchical Orthogonal Tree (HO-Tree), a structural model that captures complex semi-structured table layouts, along with an effective algorithm for constructing the tree. Second, we define a set of basic tree operations to guide LLMs in executing common QA tasks. Given a user question, ST-Raptor decomposes it into simpler sub-questions, generates corresponding tree operation pipelines, and conducts operation-table alignment for accurate pipeline execution. Third, we incorporate a two-stage verification mechanism: forward validation checks the correctness of execution steps, while backward validation evaluates answer reliability by reconstructing queries from predicted answers. To benchmark the performance, we present SSTQA, a dataset of 764 questions over 102 real-world semi-structured tables. Experiments show that ST-Raptor outperforms nine baselines by up to 20% in answer accuracy. The code is available at https://github.com/weAIDB/ST-Raptor. 9 authors · Aug 25, 2025 Upvote 6 GitHub 46 arXiv Page Submitted by hba123 Ark: An Open-source Python-based Framework for Robot Learning Robotics has made remarkable hardware strides-from DARPA's Urban and Robotics Challenges to the first humanoid-robot kickboxing tournament-yet commercial autonomy still lags behind progress in machine learning. A major bottleneck is software: current robot stacks demand steep learning curves, low-level C/C++ expertise, fragmented tooling, and intricate hardware integration, in stark contrast to the Python-centric, well-documented ecosystems that propelled modern AI. We introduce ARK, an open-source, Python-first robotics framework designed to close that gap. ARK presents a Gym-style environment interface that allows users to collect data, preprocess it, and train policies using state-of-the-art imitation-learning algorithms (e.g., ACT, Diffusion Policy) while seamlessly toggling between high-fidelity simulation and physical robots. A lightweight client-server architecture provides networked publisher-subscriber communication, and optional C/C++ bindings ensure real-time performance when needed. ARK ships with reusable modules for control, SLAM, motion planning, system identification, and visualization, along with native ROS interoperability. Comprehensive documentation and case studies-from manipulation to mobile navigation-demonstrate rapid prototyping, effortless hardware swapping, and end-to-end pipelines that rival the convenience of mainstream machine-learning workflows. By unifying robotics and AI practices under a common Python umbrella, ARK lowers entry barriers and accelerates research and commercial deployment of autonomous robots. 13 authors · Published on Jun 24, 2025 Upvote 16 GitHub 214 arXiv Page Submitted by hba123 Ark: An Open-source Python-based Framework for Robot Learning Robotics has made remarkable hardware strides-from DARPA's Urban and Robotics Challenges to the first humanoid-robot kickboxing tournament-yet commercial autonomy still lags behind progress in machine learning. A major bottleneck is software: current robot stacks demand steep learning curves, low-level C/C++ expertise, fragmented tooling, and intricate hardware integration, in stark contrast to the Python-centric, well-documented ecosystems that propelled modern AI. We introduce ARK, an open-source, Python-first robotics framework designed to close that gap. ARK presents a Gym-style environment interface that allows users to collect data, preprocess it, and train policies using state-of-the-art imitation-learning algorithms (e.g., ACT, Diffusion Policy) while seamlessly toggling between high-fidelity simulation and physical robots. A lightweight client-server architecture provides networked publisher-subscriber communication, and optional C/C++ bindings ensure real-time performance when needed. ARK ships with reusable modules for control, SLAM, motion planning, system identification, and visualization, along with native ROS interoperability. Comprehensive documentation and case studies-from manipulation to mobile navigation-demonstrate rapid prototyping, effortless hardware swapping, and end-to-end pipelines that rival the convenience of mainstream machine-learning workflows. By unifying robotics and AI practices under a common Python umbrella, ARK lowers entry barriers and accelerates research and commercial deployment of autonomous robots. 13 authors · Jun 24, 2025 Upvote 16 GitHub 214 arXiv Page Submitted by NCJ RenderFormer: Transformer-based Neural Rendering of Triangle Meshes with Global Illumination We present RenderFormer, a neural rendering pipeline that directly renders an image from a triangle-based representation of a scene with full global illumination effects and that does not require per-scene training or fine-tuning. Instead of taking a physics-centric approach to rendering, we formulate rendering as a sequence-to-sequence transformation where a sequence of tokens representing triangles with reflectance properties is converted to a sequence of output tokens representing small patches of pixels. RenderFormer follows a two stage pipeline: a view-independent stage that models triangle-to-triangle light transport, and a view-dependent stage that transforms a token representing a bundle of rays to the corresponding pixel values guided by the triangle-sequence from the view-independent stage. Both stages are based on the transformer architecture and are learned with minimal prior constraints. We demonstrate and evaluate RenderFormer on scenes with varying complexity in shape and light transport. 5 authors · Published on May 28, 2025 Upvote 37 GitHub 759 arXiv Page Submitted by NCJ RenderFormer: Transformer-based Neural Rendering of Triangle Meshes with Global Illumination We present RenderFormer, a neural rendering pipeline that directly renders an image from a triangle-based representation of a scene with full global illumination effects and that does not require per-scene training or fine-tuning. Instead of taking a physics-centric approach to rendering, we formulate rendering as a sequence-to-sequence transformation where a sequence of tokens representing triangles with reflectance properties is converted to a sequence of output tokens representing small patches of pixels. RenderFormer follows a two stage pipeline: a view-independent stage that models triangle-to-triangle light transport, and a view-dependent stage that transforms a token representing a bundle of rays to the corresponding pixel values guided by the triangle-sequence from the view-independent stage. Both stages are based on the transformer architecture and are learned with minimal prior constraints. We demonstrate and evaluate RenderFormer on scenes with varying complexity in shape and light transport. 5 authors · May 28, 2025 Upvote 37 GitHub 759 arXiv Page Submitted by aopolin-lv F1: A Vision-Language-Action Model Bridging Understanding and Generation to Actions Executing language-conditioned tasks in dynamic visual environments remains a central challenge in embodied AI. Existing Vision-Language-Action (VLA) models predominantly adopt reactive state-to-action mappings, often leading to short-sighted behaviors and poor robustness in dynamic scenes. In this paper, we introduce F1, a pretrained VLA framework which integrates the visual foresight generation into decision-making pipeline. F1 adopts a Mixture-of-Transformer architecture with dedicated modules for perception, foresight generation, and control, thereby bridging understanding, generation, and actions. At its core, F1 employs a next-scale prediction mechanism to synthesize goal-conditioned visual foresight as explicit planning targets. By forecasting plausible future visual states, F1 reformulates action generation as a foresight-guided inverse dynamics problem, enabling actions that implicitly achieve visual goals. To endow F1 with robust and generalizable capabilities, we propose a three-stage training recipe on an extensive dataset comprising over 330k trajectories across 136 diverse tasks. This training scheme enhances modular reasoning and equips the model with transferable visual foresight, which is critical for complex and dynamic environments. Extensive evaluations on real-world tasks and simulation benchmarks demonstrate F1 consistently outperforms existing approaches, achieving substantial gains in both task success rate and generalization ability. 10 authors · Published on Sep 8, 2025 Upvote 25 GitHub 63 arXiv Page Submitted by aopolin-lv F1: A Vision-Language-Action Model Bridging Understanding and Generation to Actions Executing language-conditioned tasks in dynamic visual environments remains a central challenge in embodied AI. Existing Vision-Language-Action (VLA) models predominantly adopt reactive state-to-action mappings, often leading to short-sighted behaviors and poor robustness in dynamic scenes. In this paper, we introduce F1, a pretrained VLA framework which integrates the visual foresight generation into decision-making pipeline. F1 adopts a Mixture-of-Transformer architecture with dedicated modules for perception, foresight generation, and control, thereby bridging understanding, generation, and actions. At its core, F1 employs a next-scale prediction mechanism to synthesize goal-conditioned visual foresight as explicit planning targets. By forecasting plausible future visual states, F1 reformulates action generation as a foresight-guided inverse dynamics problem, enabling actions that implicitly achieve visual goals. To endow F1 with robust and generalizable capabilities, we propose a three-stage training recipe on an extensive dataset comprising over 330k trajectories across 136 diverse tasks. This training scheme enhances modular reasoning and equips the model with transferable visual foresight, which is critical for complex and dynamic environments. Extensive evaluations on real-world tasks and simulation benchmarks demonstrate F1 consistently outperforms existing approaches, achieving substantial gains in both task success rate and generalization ability. 10 authors · Sep 8, 2025 Upvote 25 GitHub 63 arXiv Page Submitted by ashawkey Efficient Part-level 3D Object Generation via Dual Volume Packing Recent progress in 3D object generation has greatly improved both the quality and efficiency. However, most existing methods generate a single mesh with all parts fused together, which limits the ability to edit or manipulate individual parts. A key challenge is that different objects may have a varying number of parts. To address this, we propose a new end-to-end framework for part-level 3D object generation. Given a single input image, our method generates high-quality 3D objects with an arbitrary number of complete and semantically meaningful parts. We introduce a dual volume packing strategy that organizes all parts into two complementary volumes, allowing for the creation of complete and interleaved parts that assemble into the final object. Experiments show that our model achieves better quality, diversity, and generalization than previous image-based part-level generation methods. 10 authors · Published on Jun 11, 2025 Upvote 8 GitHub 672 arXiv Page Submitted by ashawkey Efficient Part-level 3D Object Generation via Dual Volume Packing Recent progress in 3D object generation has greatly improved both the quality and efficiency. However, most existing methods generate a single mesh with all parts fused together, which limits the ability to edit or manipulate individual parts. A key challenge is that different objects may have a varying number of parts. To address this, we propose a new end-to-end framework for part-level 3D object generation. Given a single input image, our method generates high-quality 3D objects with an arbitrary number of complete and semantically meaningful parts. We introduce a dual volume packing strategy that organizes all parts into two complementary volumes, allowing for the creation of complete and interleaved parts that assemble into the final object. Experiments show that our model achieves better quality, diversity, and generalization than previous image-based part-level generation methods. 10 authors · Jun 11, 2025 Upvote 8 GitHub 672 arXiv Page Submitted by X-iZhang A Comprehensive Survey of Self-Evolving AI Agents: A New Paradigm Bridging Foundation Models and Lifelong Agentic Systems Recent advances in large language models have sparked growing interest in AI agents capable of solving complex, real-world tasks. However, most existing agent systems rely on manually crafted configurations that remain static after deployment, limiting their ability to adapt to dynamic and evolving environments. To this end, recent research has explored agent evolution techniques that aim to automatically enhance agent systems based on interaction data and environmental feedback. This emerging direction lays the foundation for self-evolving AI agents, which bridge the static capabilities of foundation models with the continuous adaptability required by lifelong agentic systems. In this survey, we provide a comprehensive review of existing techniques for self-evolving agentic systems. Specifically, we first introduce a unified conceptual framework that abstracts the feedback loop underlying the design of self-evolving agentic systems. The framework highlights four key components: System Inputs, Agent System, Environment, and Optimisers, serving as a foundation for understanding and comparing different strategies. Based on this framework, we systematically review a wide range of self-evolving techniques that target different components of the agent system. We also investigate domain-specific evolution strategies developed for specialised fields such as biomedicine, programming, and finance, where optimisation objectives are tightly coupled with domain constraints. In addition, we provide a dedicated discussion on the evaluation, safety, and ethical considerations for self-evolving agentic systems, which are critical to ensuring their effectiveness and reliability. This survey aims to provide researchers and practitioners with a systematic understanding of self-evolving AI agents, laying the foundation for the development of more adaptive, autonomous, and lifelong agentic systems. 15 authors · Published on Aug 10, 2025 Upvote 92 GitHub 899 arXiv Page Submitted by X-iZhang A Comprehensive Survey of Self-Evolving AI Agents: A New Paradigm Bridging Foundation Models and Lifelong Agentic Systems Recent advances in large language models have sparked growing interest in AI agents capable of solving complex, real-world tasks. However, most existing agent systems rely on manually crafted configurations that remain static after deployment, limiting their ability to adapt to dynamic and evolving environments. To this end, recent research has explored agent evolution techniques that aim to automatically enhance agent systems based on interaction data and environmental feedback. This emerging direction lays the foundation for self-evolving AI agents, which bridge the static capabilities of foundation models with the continuous adaptability required by lifelong agentic systems. In this survey, we provide a comprehensive review of existing techniques for self-evolving agentic systems. Specifically, we first introduce a unified conceptual framework that abstracts the feedback loop underlying the design of self-evolving agentic systems. The framework highlights four key components: System Inputs, Agent System, Environment, and Optimisers, serving as a foundation for understanding and comparing different strategies. Based on this framework, we systematically review a wide range of self-evolving techniques that target different components of the agent system. We also investigate domain-specific evolution strategies developed for specialised fields such as biomedicine, programming, and finance, where optimisation objectives are tightly coupled with domain constraints. In addition, we provide a dedicated discussion on the evaluation, safety, and ethical considerations for self-evolving agentic systems, which are critical to ensuring their effectiveness and reliability. This survey aims to provide researchers and practitioners with a systematic understanding of self-evolving AI agents, laying the foundation for the development of more adaptive, autonomous, and lifelong agentic systems. 15 authors · Aug 10, 2025 Upvote 92 GitHub 899 arXiv Page Submitted by lynazhang rStar2-Agent: Agentic Reasoning Technical Report We introduce rStar2-Agent, a 14B math reasoning model trained with agentic reinforcement learning to achieve frontier-level performance. Beyond current long CoT, the model demonstrates advanced cognitive behaviors, such as thinking carefully before using Python coding tools and reflecting on code execution feedback to autonomously explore, verify, and refine intermediate steps in complex problem-solving. This capability is enabled through three key innovations that makes agentic RL effective at scale: (i) an efficient RL infrastructure with a reliable Python code environment that supports high-throughput execution and mitigates the high rollout costs, enabling training on limited GPU resources (64 MI300X GPUs); (ii) GRPO-RoC, an agentic RL algorithm with a Resample-on-Correct rollout strategy that addresses the inherent environment noises from coding tools, allowing the model to reason more effectively in a code environment; (iii) An efficient agent training recipe that starts with non-reasoning SFT and progresses through multi-RL stages, yielding advanced cognitive abilities with minimal compute cost. To this end, rStar2-Agent boosts a pre-trained 14B model to state of the art in only 510 RL steps within one week, achieving average pass@1 scores of 80.6% on AIME24 and 69.8% on AIME25, surpassing DeepSeek-R1 (671B) with significantly shorter responses. Beyond mathematics, rStar2-Agent-14B also demonstrates strong generalization to alignment, scientific reasoning, and agentic tool-use tasks. Code and training recipes are available at https://github.com/microsoft/rStar. 15 authors · Published on Aug 28, 2025 Upvote 102 GitHub 1.15k arXiv Page Submitted by lynazhang rStar2-Agent: Agentic Reasoning Technical Report We introduce rStar2-Agent, a 14B math reasoning model trained with agentic reinforcement learning to achieve frontier-level performance. Beyond current long CoT, the model demonstrates advanced cognitive behaviors, such as thinking carefully before using Python coding tools and reflecting on code execution feedback to autonomously explore, verify, and refine intermediate steps in complex problem-solving. This capability is enabled through three key innovations that makes agentic RL effective at scale: (i) an efficient RL infrastructure with a reliable Python code environment that supports high-throughput execution and mitigates the high rollout costs, enabling training on limited GPU resources (64 MI300X GPUs); (ii) GRPO-RoC, an agentic RL algorithm with a Resample-on-Correct rollout strategy that addresses the inherent environment noises from coding tools, allowing the model to reason more effectively in a code environment; (iii) An efficient agent training recipe that starts with non-reasoning SFT and progresses through multi-RL stages, yielding advanced cognitive abilities with minimal compute cost. To this end, rStar2-Agent boosts a pre-trained 14B model to state of the art in only 510 RL steps within one week, achieving average pass@1 scores of 80.6% on AIME24 and 69.8% on AIME25, surpassing DeepSeek-R1 (671B) with significantly shorter responses. Beyond mathematics, rStar2-Agent-14B also demonstrates strong generalization to alignment, scientific reasoning, and agentic tool-use tasks. Code and training recipes are available at https://github.com/microsoft/rStar. 15 authors · Aug 28, 2025 Upvote 102 GitHub 1.15k arXiv Page Submitted by tqliu 4DNeX: Feed-Forward 4D Generative Modeling Made Easy We present 4DNeX, the first feed-forward framework for generating 4D (i.e., dynamic 3D) scene representations from a single image. In contrast to existing methods that rely on computationally intensive optimization or require multi-frame video inputs, 4DNeX enables efficient, end-to-end image-to-4D generation by fine-tuning a pretrained video diffusion model. Specifically, 1) to alleviate the scarcity of 4D data, we construct 4DNeX-10M, a large-scale dataset with high-quality 4D annotations generated using advanced reconstruction approaches. 2) we introduce a unified 6D video representation that jointly models RGB and XYZ sequences, facilitating structured learning of both appearance and geometry. 3) we propose a set of simple yet effective adaptation strategies to repurpose pretrained video diffusion models for 4D modeling. 4DNeX produces high-quality dynamic point clouds that enable novel-view video synthesis. Extensive experiments demonstrate that 4DNeX outperforms existing 4D generation methods in efficiency and generalizability, offering a scalable solution for image-to-4D modeling and laying the foundation for generative 4D world models that simulate dynamic scene evolution. 9 authors · Published on Aug 18, 2025 Upvote 58 GitHub 449 arXiv Page Submitted by tqliu 4DNeX: Feed-Forward 4D Generative Modeling Made Easy We present 4DNeX, the first feed-forward framework for generating 4D (i.e., dynamic 3D) scene representations from a single image. In contrast to existing methods that rely on computationally intensive optimization or require multi-frame video inputs, 4DNeX enables efficient, end-to-end image-to-4D generation by fine-tuning a pretrained video diffusion model. Specifically, 1) to alleviate the scarcity of 4D data, we construct 4DNeX-10M, a large-scale dataset with high-quality 4D annotations generated using advanced reconstruction approaches. 2) we introduce a unified 6D video representation that jointly models RGB and XYZ sequences, facilitating structured learning of both appearance and geometry. 3) we propose a set of simple yet effective adaptation strategies to repurpose pretrained video diffusion models for 4D modeling. 4DNeX produces high-quality dynamic point clouds that enable novel-view video synthesis. Extensive experiments demonstrate that 4DNeX outperforms existing 4D generation methods in efficiency and generalizability, offering a scalable solution for image-to-4D modeling and laying the foundation for generative 4D world models that simulate dynamic scene evolution. 9 authors · Aug 18, 2025 Upvote 58 GitHub 449 arXiv Page Submitted by skicy Step-Audio 2 Technical Report This paper presents Step-Audio~2, an end-to-end multi-modal large language model designed for industry-strength audio understanding and speech conversation. By integrating a latent audio encoder and reasoning-centric reinforcement learning (RL), Step-Audio 2 achieves promising performance in automatic speech recognition (ASR) and audio understanding. To facilitate genuine end-to-end speech conversation, Step-Audio 2 incorporates the generation of discrete audio tokens into language modeling, significantly enhancing its responsiveness to paralinguistic information such as speaking styles and emotions. To effectively leverage the rich textual and acoustic knowledge in real-world data, Step-Audio 2 integrates retrieval-augmented generation (RAG) and is able to call external tools such as web search to mitigate hallucination and audio search to switch timbres. Trained on millions of hours of speech and audio data, Step-Audio 2 delivers intelligence and expressiveness across diverse conversational scenarios. Evaluation results demonstrate that Step-Audio 2 achieves state-of-the-art performance on various audio understanding and conversational benchmarks compared to other open-source and commercial solutions. Please visit https://github.com/stepfun-ai/Step-Audio2 for more information. 109 authors · Published on Jul 22, 2025 Upvote 71 GitHub 1.03k arXiv Page Submitted by skicy Step-Audio 2 Technical Report This paper presents Step-Audio~2, an end-to-end multi-modal large language model designed for industry-strength audio understanding and speech conversation. By integrating a latent audio encoder and reasoning-centric reinforcement learning (RL), Step-Audio 2 achieves promising performance in automatic speech recognition (ASR) and audio understanding. To facilitate genuine end-to-end speech conversation, Step-Audio 2 incorporates the generation of discrete audio tokens into language modeling, significantly enhancing its responsiveness to paralinguistic information such as speaking styles and emotions. To effectively leverage the rich textual and acoustic knowledge in real-world data, Step-Audio 2 integrates retrieval-augmented generation (RAG) and is able to call external tools such as web search to mitigate hallucination and audio search to switch timbres. Trained on millions of hours of speech and audio data, Step-Audio 2 delivers intelligence and expressiveness across diverse conversational scenarios. Evaluation results demonstrate that Step-Audio 2 achieves state-of-the-art performance on various audio understanding and conversational benchmarks compared to other open-source and commercial solutions. Please visit https://github.com/stepfun-ai/Step-Audio2 for more information. 109 authors · Jul 22, 2025 Upvote 71 GitHub 1.03k arXiv Page Submitted by taesiri InternVL3.5: Advancing Open-Source Multimodal Models in Versatility, Reasoning, and Efficiency We introduce InternVL 3.5, a new family of open-source multimodal models that significantly advances versatility, reasoning capability, and inference efficiency along the InternVL series. A key innovation is the Cascade Reinforcement Learning (Cascade RL) framework, which enhances reasoning through a two-stage process: offline RL for stable convergence and online RL for refined alignment. This coarse-to-fine training strategy leads to substantial improvements on downstream reasoning tasks, e.g., MMMU and MathVista. To optimize efficiency, we propose a Visual Resolution Router (ViR) that dynamically adjusts the resolution of visual tokens without compromising performance. Coupled with ViR, our Decoupled Vision-Language Deployment (DvD) strategy separates the vision encoder and language model across different GPUs, effectively balancing computational load. These contributions collectively enable InternVL3.5 to achieve up to a +16.0\\% gain in overall reasoning performance and a 4.05times inference speedup compared to its predecessor, i.e., InternVL3. In addition, InternVL3.5 supports novel capabilities such as GUI interaction and embodied agency. Notably, our largest model, i.e., InternVL3.5-241B-A28B, attains state-of-the-art results among open-source MLLMs across general multimodal, reasoning, text, and agentic tasks -- narrowing the performance gap with leading commercial models like GPT-5. All models and code are publicly released. 61 authors · Published on Aug 25, 2025 Upvote 185 GitHub 9.11k arXiv Page Submitted by taesiri InternVL3.5: Advancing Open-Source Multimodal Models in Versatility, Reasoning, and Efficiency We introduce InternVL 3.5, a new family of open-source multimodal models that significantly advances versatility, reasoning capability, and inference efficiency along the InternVL series. A key innovation is the Cascade Reinforcement Learning (Cascade RL) framework, which enhances reasoning through a two-stage process: offline RL for stable convergence and online RL for refined alignment. This coarse-to-fine training strategy leads to substantial improvements on downstream reasoning tasks, e.g., MMMU and MathVista. To optimize efficiency, we propose a Visual Resolution Router (ViR) that dynamically adjusts the resolution of visual tokens without compromising performance. Coupled with ViR, our Decoupled Vision-Language Deployment (DvD) strategy separates the vision encoder and language model across different GPUs, effectively balancing computational load. These contributions collectively enable InternVL3.5 to achieve up to a +16.0\\% gain in overall reasoning performance and a 4.05times inference speedup compared to its predecessor, i.e., InternVL3. In addition, InternVL3.5 supports novel capabilities such as GUI interaction and embodied agency. Notably, our largest model, i.e., InternVL3.5-241B-A28B, attains state-of-the-art results among open-source MLLMs across general multimodal, reasoning, text, and agentic tasks -- narrowing the performance gap with leading commercial models like GPT-5. All models and code are publicly released. 61 authors · Aug 25, 2025 Upvote 185 GitHub 9.11k arXiv Page Submitted by yifanzhang114 Aligning Multimodal LLM with Human Preference: A Survey Large language models (LLMs) can handle a wide variety of general tasks with simple prompts, without the need for task-specific training. Multimodal Large Language Models (MLLMs), built upon LLMs, have demonstrated impressive potential in tackling complex tasks involving visual, auditory, and textual data. However, critical issues related to truthfulness, safety, o1-like reasoning, and alignment with human preference remain insufficiently addressed. This gap has spurred the emergence of various alignment algorithms, each targeting different application scenarios and optimization goals. Recent studies have shown that alignment algorithms are a powerful approach to resolving the aforementioned challenges. In this paper, we aim to provide a comprehensive and systematic review of alignment algorithms for MLLMs. Specifically, we explore four key aspects: (1) the application scenarios covered by alignment algorithms, including general image understanding, multi-image, video, and audio, and extended multimodal applications; (2) the core factors in constructing alignment datasets, including data sources, model responses, and preference annotations; (3) the benchmarks used to evaluate alignment algorithms; and (4) a discussion of potential future directions for the development of alignment algorithms. This work seeks to help researchers organize current advancements in the field and inspire better alignment methods. The project page of this paper is available at https://github.com/BradyFU/Awesome-Multimodal-Large-Language-Models/tree/Alignment. 17 authors · Published on Mar 18, 2025 Upvote 26 GitHub 16.2k arXiv Page Submitted by yifanzhang114 Aligning Multimodal LLM with Human Preference: A Survey Large language models (LLMs) can handle a wide variety of general tasks with simple prompts, without the need for task-specific training. Multimodal Large Language Models (MLLMs), built upon LLMs, have demonstrated impressive potential in tackling complex tasks involving visual, auditory, and textual data. However, critical issues related to truthfulness, safety, o1-like reasoning, and alignment with human preference remain insufficiently addressed. This gap has spurred the emergence of various alignment algorithms, each targeting different application scenarios and optimization goals. Recent studies have shown that alignment algorithms are a powerful approach to resolving the aforementioned challenges. In this paper, we aim to provide a comprehensive and systematic review of alignment algorithms for MLLMs. Specifically, we explore four key aspects: (1) the application scenarios covered by alignment algorithms, including general image understanding, multi-image, video, and audio, and extended multimodal applications; (2) the core factors in constructing alignment datasets, including data sources, model responses, and preference annotations; (3) the benchmarks used to evaluate alignment algorithms; and (4) a discussion of potential future directions for the development of alignment algorithms. This work seeks to help researchers organize current advancements in the field and inspire better alignment methods. The project page of this paper is available at https://github.com/BradyFU/Awesome-Multimodal-Large-Language-Models/tree/Alignment. 17 authors · Mar 18, 2025 Upvote 26 GitHub 16.2k arXiv Page Submitted by Chevalier A Survey of Context Engineering for Large Language Models The performance of Large Language Models (LLMs) is fundamentally determined by the contextual information provided during inference. This survey introduces Context Engineering, a formal discipline that transcends simple prompt design to encompass the systematic optimization of information payloads for LLMs. We present a comprehensive taxonomy decomposing Context Engineering into its foundational components and the sophisticated implementations that integrate them into intelligent systems. We first examine the foundational components: context retrieval and generation, context processing and context management. We then explore how these components are architecturally integrated to create sophisticated system implementations: retrieval-augmented generation (RAG), memory systems and tool-integrated reasoning, and multi-agent systems. Through this systematic analysis of over 1300 research papers, our survey not only establishes a technical roadmap for the field but also reveals a critical research gap: a fundamental asymmetry exists between model capabilities. While current models, augmented by advanced context engineering, demonstrate remarkable proficiency in understanding complex contexts, they exhibit pronounced limitations in generating equally sophisticated, long-form outputs. Addressing this gap is a defining priority for future research. Ultimately, this survey provides a unified framework for both researchers and engineers advancing context-aware AI. 15 authors · Published on Jul 17, 2025 Upvote 252 GitHub 2.18k arXiv Page Submitted by Chevalier A Survey of Context Engineering for Large Language Models The performance of Large Language Models (LLMs) is fundamentally determined by the contextual information provided during inference. This survey introduces Context Engineering, a formal discipline that transcends simple prompt design to encompass the systematic optimization of information payloads for LLMs. We present a comprehensive taxonomy decomposing Context Engineering into its foundational components and the sophisticated implementations that integrate them into intelligent systems. We first examine the foundational components: context retrieval and generation, context processing and context management. We then explore how these components are architecturally integrated to create sophisticated system implementations: retrieval-augmented generation (RAG), memory systems and tool-integrated reasoning, and multi-agent systems. Through this systematic analysis of over 1300 research papers, our survey not only establishes a technical roadmap for the field but also reveals a critical research gap: a fundamental asymmetry exists between model capabilities. While current models, augmented by advanced context engineering, demonstrate remarkable proficiency in understanding complex contexts, they exhibit pronounced limitations in generating equally sophisticated, long-form outputs. Addressing this gap is a defining priority for future research. Ultimately, this survey provides a unified framework for both researchers and engineers advancing context-aware AI. 15 authors · Jul 17, 2025 Upvote 252 GitHub 2.18k arXiv Page Submitted by luojunyu Large Language Model Agent: A Survey on Methodology, Applications and Challenges The era of intelligent agents is upon us, driven by revolutionary advancements in large language models. Large Language Model (LLM) agents, with goal-driven behaviors and dynamic adaptation capabilities, potentially represent a critical pathway toward artificial general intelligence. This survey systematically deconstructs LLM agent systems through a methodology-centered taxonomy, linking architectural foundations, collaboration mechanisms, and evolutionary pathways. We unify fragmented research threads by revealing fundamental connections between agent design principles and their emergent behaviors in complex environments. Our work provides a unified architectural perspective, examining how agents are constructed, how they collaborate, and how they evolve over time, while also addressing evaluation methodologies, tool applications, practical challenges, and diverse application domains. By surveying the latest developments in this rapidly evolving field, we offer researchers a structured taxonomy for understanding LLM agents and identify promising directions for future research. The collection is available at https://github.com/luo-junyu/Awesome-Agent-Papers. 26 authors · Published on Mar 27, 2025 Upvote 82 GitHub 1.63k arXiv Page Submitted by luojunyu Large Language Model Agent: A Survey on Methodology, Applications and Challenges The era of intelligent agents is upon us, driven by revolutionary advancements in large language models. Large Language Model (LLM) agents, with goal-driven behaviors and dynamic adaptation capabilities, potentially represent a critical pathway toward artificial general intelligence. This survey systematically deconstructs LLM agent systems through a methodology-centered taxonomy, linking architectural foundations, collaboration mechanisms, and evolutionary pathways. We unify fragmented research threads by revealing fundamental connections between agent design principles and their emergent behaviors in complex environments. Our work provides a unified architectural perspective, examining how agents are constructed, how they collaborate, and how they evolve over time, while also addressing evaluation methodologies, tool applications, practical challenges, and diverse application domains. By surveying the latest developments in this rapidly evolving field, we offer researchers a structured taxonomy for understanding LLM agents and identify promising directions for future research. The collection is available at https://github.com/luo-junyu/Awesome-Agent-Papers. 26 authors · Mar 27, 2025 Upvote 82 GitHub 1.63k arXiv Page Submitted by taesiri Paper2Agent: Reimagining Research Papers As Interactive and Reliable AI Agents We introduce Paper2Agent, an automated framework that converts research papers into AI agents. Paper2Agent transforms research output from passive artifacts into active systems that can accelerate downstream use, adoption, and discovery. Conventional research papers require readers to invest substantial effort to understand and adapt a paper's code, data, and methods to their own work, creating barriers to dissemination and reuse. Paper2Agent addresses this challenge by automatically converting a paper into an AI agent that acts as a knowledgeable research assistant. It systematically analyzes the paper and the associated codebase using multiple agents to construct a Model Context Protocol (MCP) server, then iteratively generates and runs tests to refine and robustify the resulting MCP. These paper MCPs can then be flexibly connected to a chat agent (e.g. Claude Code) to carry out complex scientific queries through natural language while invoking tools and workflows from the original paper. We demonstrate Paper2Agent's effectiveness in creating reliable and capable paper agents through in-depth case studies. Paper2Agent created an agent that leverages AlphaGenome to interpret genomic variants and agents based on ScanPy and TISSUE to carry out single-cell and spatial transcriptomics analyses. We validate that these paper agents can reproduce the original paper's results and can correctly carry out novel user queries. By turning static papers into dynamic, interactive AI agents, Paper2Agent introduces a new paradigm for knowledge dissemination and a foundation for the collaborative ecosystem of AI co-scientists. 4 authors · Published on Sep 8, 2025 Upvote 21 GitHub 44 arXiv Page Submitted by taesiri Paper2Agent: Reimagining Research Papers As Interactive and Reliable AI Agents We introduce Paper2Agent, an automated framework that converts research papers into AI agents. Paper2Agent transforms research output from passive artifacts into active systems that can accelerate downstream use, adoption, and discovery. Conventional research papers require readers to invest substantial effort to understand and adapt a paper's code, data, and methods to their own work, creating barriers to dissemination and reuse. Paper2Agent addresses this challenge by automatically converting a paper into an AI agent that acts as a knowledgeable research assistant. It systematically analyzes the paper and the associated codebase using multiple agents to construct a Model Context Protocol (MCP) server, then iteratively generates and runs tests to refine and robustify the resulting MCP. These paper MCPs can then be flexibly connected to a chat agent (e.g. Claude Code) to carry out complex scientific queries through natural language while invoking tools and workflows from the original paper. We demonstrate Paper2Agent's effectiveness in creating reliable and capable paper agents through in-depth case studies. Paper2Agent created an agent that leverages AlphaGenome to interpret genomic variants and agents based on ScanPy and TISSUE to carry out single-cell and spatial transcriptomics analyses. We validate that these paper agents can reproduce the original paper's results and can correctly carry out novel user queries. By turning static papers into dynamic, interactive AI agents, Paper2Agent introduces a new paradigm for knowledge dissemination and a foundation for the collaborative ecosystem of AI co-scientists. 4 authors · Sep 8, 2025 Upvote 21 GitHub 44 arXiv Page System theme Company TOS Privacy About Jobs Website Models Datasets Spaces Pricing Docs ",
      "fetch_method": "direct-html"
    },
    {
      "arxiv_id": "https://arxiv.org/abs/1903.00161",
      "full_text": " [1903.00161] DROP: A Reading Comprehension Benchmark Requiring Discrete Reasoning Over Paragraphs Skip to main content We gratefully acknowledge support from the Simons Foundation, member institutions , and all contributors. Donate &gt; cs &gt; arXiv:1903.00161 Help | Advanced Search All fields Title Author Abstract Comments Journal reference ACM classification MSC classification Report number arXiv identifier DOI ORCID arXiv author ID Help pages Full text Search open search GO open navigation menu quick links Login Help Pages About --> Computer Science > Computation and Language arXiv:1903.00161 (cs) [Submitted on 1 Mar 2019 ( v1 ), last revised 16 Apr 2019 (this version, v2)] Title: DROP: A Reading Comprehension Benchmark Requiring Discrete Reasoning Over Paragraphs Authors: Dheeru Dua , Yizhong Wang , Pradeep Dasigi , Gabriel Stanovsky , Sameer Singh , Matt Gardner View a PDF of the paper titled DROP: A Reading Comprehension Benchmark Requiring Discrete Reasoning Over Paragraphs, by Dheeru Dua and 5 other authors View PDF Abstract: Reading comprehension has recently seen rapid progress, with systems matching humans on the most popular datasets for the task. However, a large body of work has highlighted the brittleness of these systems, showing that there is much work left to be done. We introduce a new English reading comprehension benchmark, DROP, which requires Discrete Reasoning Over the content of Paragraphs. In this crowdsourced, adversarially-created, 96k-question benchmark, a system must resolve references in a question, perhaps to multiple input positions, and perform discrete operations over them (such as addition, counting, or sorting). These operations require a much more comprehensive understanding of the content of paragraphs than what was necessary for prior datasets. We apply state-of-the-art methods from both the reading comprehension and semantic parsing literature on this dataset and show that the best systems only achieve 32.7% F1 on our generalized accuracy metric, while expert human performance is 96.0%. We additionally present a new model that combines reading comprehension methods with simple numerical reasoning to achieve 47.0% F1. Subjects: Computation and Language (cs.CL) Cite as: arXiv:1903.00161 [cs.CL] &nbsp; (or arXiv:1903.00161v2 [cs.CL] for this version) &nbsp; https://doi.org/10.48550/arXiv.1903.00161 Focus to learn more arXiv-issued DOI via DataCite Submission history From: Dheeru Dua [ view email ] [v1] Fri, 1 Mar 2019 05:32:01 UTC (2,543 KB) [v2] Tue, 16 Apr 2019 21:22:39 UTC (3,145 KB) Full-text links: Access Paper: View a PDF of the paper titled DROP: A Reading Comprehension Benchmark Requiring Discrete Reasoning Over Paragraphs, by Dheeru Dua and 5 other authors View PDF TeX Source Other Formats view license Current browse context: cs.CL &lt;&nbsp;prev &nbsp; | &nbsp; next&nbsp;&gt; new | recent | 2019-03 Change to browse by: cs References &amp; Citations NASA ADS Google Scholar Semantic Scholar DBLP - CS Bibliography listing | bibtex Dheeru Dua Yizhong Wang Pradeep Dasigi Gabriel Stanovsky Sameer Singh &hellip; a export BibTeX citation Loading... BibTeX formatted citation &times; loading... Data provided by: Bookmark Bibliographic Tools Bibliographic and Citation Tools Bibliographic Explorer Toggle Bibliographic Explorer ( What is the Explorer? ) Connected Papers Toggle Connected Papers ( What is Connected Papers? ) Litmaps Toggle Litmaps ( What is Litmaps? ) scite.ai Toggle scite Smart Citations ( What are Smart Citations? ) Code, Data, Media Code, Data and Media Associated with this Article alphaXiv Toggle alphaXiv ( What is alphaXiv? ) Links to Code Toggle CatalyzeX Code Finder for Papers ( What is CatalyzeX? ) DagsHub Toggle DagsHub ( What is DagsHub? ) GotitPub Toggle Gotit.pub ( What is GotitPub? ) Huggingface Toggle Hugging Face ( What is Huggingface? ) Links to Code Toggle Papers with Code ( What is Papers with Code? ) ScienceCast Toggle ScienceCast ( What is ScienceCast? ) Demos Demos Replicate Toggle Replicate ( What is Replicate? ) Spaces Toggle Hugging Face Spaces ( What is Spaces? ) Spaces Toggle TXYZ.AI ( What is TXYZ.AI? ) Related Papers Recommenders and Search Tools Link to Influence Flower Influence Flower ( What are Influence Flowers? ) Core recommender toggle CORE Recommender ( What is CORE? ) Author Venue Institution Topic About arXivLabs arXivLabs: experimental projects with community collaborators arXivLabs is a framework that allows collaborators to develop and share new arXiv features directly on our website. Both individuals and organizations that work with arXivLabs have embraced and accepted our values of openness, community, excellence, and user data privacy. arXiv is committed to these values and only works with partners that adhere to them. Have an idea for a project that will add value for arXiv's community? Learn more about arXivLabs . Which authors of this paper are endorsers? | Disable MathJax ( What is MathJax? ) About Help contact arXiv Click here to contact arXiv Contact subscribe to arXiv mailings Click here to subscribe Subscribe Copyright Privacy Policy Web Accessibility Assistance arXiv Operational Status Get status notifications via email or slack ",
      "fetch_method": "direct-html"
    },
    {
      "arxiv_id": "https://arxiv.org/abs/2009.03300",
      "full_text": " [2009.03300] Measuring Massive Multitask Language Understanding Skip to main content We gratefully acknowledge support from the Simons Foundation, member institutions , and all contributors. Donate &gt; cs &gt; arXiv:2009.03300 Help | Advanced Search All fields Title Author Abstract Comments Journal reference ACM classification MSC classification Report number arXiv identifier DOI ORCID arXiv author ID Help pages Full text Search open search GO open navigation menu quick links Login Help Pages About --> Computer Science > Computers and Society arXiv:2009.03300 (cs) [Submitted on 7 Sep 2020 ( v1 ), last revised 12 Jan 2021 (this version, v3)] Title: Measuring Massive Multitask Language Understanding Authors: Dan Hendrycks , Collin Burns , Steven Basart , Andy Zou , Mantas Mazeika , Dawn Song , Jacob Steinhardt View a PDF of the paper titled Measuring Massive Multitask Language Understanding, by Dan Hendrycks and 6 other authors View PDF Abstract: We propose a new test to measure a text model&#39;s multitask accuracy. The test covers 57 tasks including elementary mathematics, US history, computer science, law, and more. To attain high accuracy on this test, models must possess extensive world knowledge and problem solving ability. We find that while most recent models have near random-chance accuracy, the very largest GPT-3 model improves over random chance by almost 20 percentage points on average. However, on every one of the 57 tasks, the best models still need substantial improvements before they can reach expert-level accuracy. Models also have lopsided performance and frequently do not know when they are wrong. Worse, they still have near-random accuracy on some socially important subjects such as morality and law. By comprehensively evaluating the breadth and depth of a model&#39;s academic and professional understanding, our test can be used to analyze models across many tasks and to identify important shortcomings. Comments: ICLR 2021; the test and code is available at this https URL Subjects: Computers and Society (cs.CY) ; Artificial Intelligence (cs.AI); Computation and Language (cs.CL); Machine Learning (cs.LG) Cite as: arXiv:2009.03300 [cs.CY] &nbsp; (or arXiv:2009.03300v3 [cs.CY] for this version) &nbsp; https://doi.org/10.48550/arXiv.2009.03300 Focus to learn more arXiv-issued DOI via DataCite Submission history From: Dan Hendrycks [ view email ] [v1] Mon, 7 Sep 2020 17:59:25 UTC (2,172 KB) [v2] Mon, 21 Sep 2020 05:06:57 UTC (2,402 KB) [v3] Tue, 12 Jan 2021 18:57:11 UTC (2,578 KB) Full-text links: Access Paper: View a PDF of the paper titled Measuring Massive Multitask Language Understanding, by Dan Hendrycks and 6 other authors View PDF TeX Source Other Formats view license Current browse context: cs.CY &lt;&nbsp;prev &nbsp; | &nbsp; next&nbsp;&gt; new | recent | 2020-09 Change to browse by: cs cs.AI cs.CL cs.LG References &amp; Citations NASA ADS Google Scholar Semantic Scholar 4 blog links ( what is this? ) DBLP - CS Bibliography listing | bibtex Dan Hendrycks Collin Burns Steven Basart Mantas Mazeika Dawn Song &hellip; a export BibTeX citation Loading... BibTeX formatted citation &times; loading... Data provided by: Bookmark Bibliographic Tools Bibliographic and Citation Tools Bibliographic Explorer Toggle Bibliographic Explorer ( What is the Explorer? ) Connected Papers Toggle Connected Papers ( What is Connected Papers? ) Litmaps Toggle Litmaps ( What is Litmaps? ) scite.ai Toggle scite Smart Citations ( What are Smart Citations? ) Code, Data, Media Code, Data and Media Associated with this Article alphaXiv Toggle alphaXiv ( What is alphaXiv? ) Links to Code Toggle CatalyzeX Code Finder for Papers ( What is CatalyzeX? ) DagsHub Toggle DagsHub ( What is DagsHub? ) GotitPub Toggle Gotit.pub ( What is GotitPub? ) Huggingface Toggle Hugging Face ( What is Huggingface? ) Links to Code Toggle Papers with Code ( What is Papers with Code? ) ScienceCast Toggle ScienceCast ( What is ScienceCast? ) Demos Demos Replicate Toggle Replicate ( What is Replicate? ) Spaces Toggle Hugging Face Spaces ( What is Spaces? ) Spaces Toggle TXYZ.AI ( What is TXYZ.AI? ) Related Papers Recommenders and Search Tools Link to Influence Flower Influence Flower ( What are Influence Flowers? ) Core recommender toggle CORE Recommender ( What is CORE? ) Author Venue Institution Topic About arXivLabs arXivLabs: experimental projects with community collaborators arXivLabs is a framework that allows collaborators to develop and share new arXiv features directly on our website. Both individuals and organizations that work with arXivLabs have embraced and accepted our values of openness, community, excellence, and user data privacy. arXiv is committed to these values and only works with partners that adhere to them. Have an idea for a project that will add value for arXiv's community? Learn more about arXivLabs . Which authors of this paper are endorsers? | Disable MathJax ( What is MathJax? ) About Help contact arXiv Click here to contact arXiv Contact subscribe to arXiv mailings Click here to subscribe Subscribe Copyright Privacy Policy Web Accessibility Assistance arXiv Operational Status Get status notifications via email or slack ",
      "fetch_method": "direct-html"
    },
    {
      "arxiv_id": "https://arxiv.org/abs/2304.06364",
      "full_text": " [2304.06364] AGIEval: A Human-Centric Benchmark for Evaluating Foundation Models Skip to main content We gratefully acknowledge support from the Simons Foundation, member institutions , and all contributors. Donate &gt; cs &gt; arXiv:2304.06364 Help | Advanced Search All fields Title Author Abstract Comments Journal reference ACM classification MSC classification Report number arXiv identifier DOI ORCID arXiv author ID Help pages Full text Search open search GO open navigation menu quick links Login Help Pages About --> Computer Science > Computation and Language arXiv:2304.06364 (cs) [Submitted on 13 Apr 2023 ( v1 ), last revised 18 Sep 2023 (this version, v2)] Title: AGIEval: A Human-Centric Benchmark for Evaluating Foundation Models Authors: Wanjun Zhong , Ruixiang Cui , Yiduo Guo , Yaobo Liang , Shuai Lu , Yanlin Wang , Amin Saied , Weizhu Chen , Nan Duan View a PDF of the paper titled AGIEval: A Human-Centric Benchmark for Evaluating Foundation Models, by Wanjun Zhong and 7 other authors View PDF Abstract: Evaluating the general abilities of foundation models to tackle human-level tasks is a vital aspect of their development and application in the pursuit of Artificial General Intelligence (AGI). Traditional benchmarks, which rely on artificial datasets, may not accurately represent human-level capabilities. In this paper, we introduce AGIEval, a novel benchmark specifically designed to assess foundation model in the context of human-centric standardized exams, such as college entrance exams, law school admission tests, math competitions, and lawyer qualification tests. We evaluate several state-of-the-art foundation models, including GPT-4, ChatGPT, and Text-Davinci-003, using this benchmark. Impressively, GPT-4 surpasses average human performance on SAT, LSAT, and math competitions, attaining a 95% accuracy rate on the SAT Math test and a 92.5% accuracy on the English test of the Chinese national college entrance exam. This demonstrates the extraordinary performance of contemporary foundation models. In contrast, we also find that GPT-4 is less proficient in tasks that require complex reasoning or specific domain knowledge. Our comprehensive analyses of model capabilities (understanding, knowledge, reasoning, and calculation) reveal these models&#39; strengths and limitations, providing valuable insights into future directions for enhancing their general capabilities. By concentrating on tasks pertinent to human cognition and decision-making, our benchmark delivers a more meaningful and robust evaluation of foundation models&#39; performance in real-world scenarios. The data, code, and all model outputs are released in this https URL . Comments: 19 pages Subjects: Computation and Language (cs.CL) ; Artificial Intelligence (cs.AI) Cite as: arXiv:2304.06364 [cs.CL] &nbsp; (or arXiv:2304.06364v2 [cs.CL] for this version) &nbsp; https://doi.org/10.48550/arXiv.2304.06364 Focus to learn more arXiv-issued DOI via DataCite Submission history From: Wanjun Zhong [ view email ] [v1] Thu, 13 Apr 2023 09:39:30 UTC (13,276 KB) [v2] Mon, 18 Sep 2023 14:23:02 UTC (6,512 KB) Full-text links: Access Paper: View a PDF of the paper titled AGIEval: A Human-Centric Benchmark for Evaluating Foundation Models, by Wanjun Zhong and 7 other authors View PDF TeX Source Other Formats view license Current browse context: cs.CL &lt;&nbsp;prev &nbsp; | &nbsp; next&nbsp;&gt; new | recent | 2023-04 Change to browse by: cs cs.AI References &amp; Citations NASA ADS Google Scholar Semantic Scholar a export BibTeX citation Loading... BibTeX formatted citation &times; loading... Data provided by: Bookmark Bibliographic Tools Bibliographic and Citation Tools Bibliographic Explorer Toggle Bibliographic Explorer ( What is the Explorer? ) Connected Papers Toggle Connected Papers ( What is Connected Papers? ) Litmaps Toggle Litmaps ( What is Litmaps? ) scite.ai Toggle scite Smart Citations ( What are Smart Citations? ) Code, Data, Media Code, Data and Media Associated with this Article alphaXiv Toggle alphaXiv ( What is alphaXiv? ) Links to Code Toggle CatalyzeX Code Finder for Papers ( What is CatalyzeX? ) DagsHub Toggle DagsHub ( What is DagsHub? ) GotitPub Toggle Gotit.pub ( What is GotitPub? ) Huggingface Toggle Hugging Face ( What is Huggingface? ) Links to Code Toggle Papers with Code ( What is Papers with Code? ) ScienceCast Toggle ScienceCast ( What is ScienceCast? ) Demos Demos Replicate Toggle Replicate ( What is Replicate? ) Spaces Toggle Hugging Face Spaces ( What is Spaces? ) Spaces Toggle TXYZ.AI ( What is TXYZ.AI? ) Related Papers Recommenders and Search Tools Link to Influence Flower Influence Flower ( What are Influence Flowers? ) Core recommender toggle CORE Recommender ( What is CORE? ) Author Venue Institution Topic About arXivLabs arXivLabs: experimental projects with community collaborators arXivLabs is a framework that allows collaborators to develop and share new arXiv features directly on our website. Both individuals and organizations that work with arXivLabs have embraced and accepted our values of openness, community, excellence, and user data privacy. arXiv is committed to these values and only works with partners that adhere to them. Have an idea for a project that will add value for arXiv's community? Learn more about arXivLabs . Which authors of this paper are endorsers? | Disable MathJax ( What is MathJax? ) About Help contact arXiv Click here to contact arXiv Contact subscribe to arXiv mailings Click here to subscribe Subscribe Copyright Privacy Policy Web Accessibility Assistance arXiv Operational Status Get status notifications via email or slack ",
      "fetch_method": "direct-html"
    },
    {
      "arxiv_id": "https://arxiv.org/abs/2103.03874",
      "full_text": " [2103.03874] Measuring Mathematical Problem Solving With the MATH Dataset arXiv Is Hiring a DevOps Engineer Work on one of the world's most important websites and make an impact on open science. View Jobs Skip to main content arXiv Is Hiring a DevOps Engineer View Jobs We gratefully acknowledge support from the Simons Foundation, member institutions , and all contributors. Donate &gt; cs &gt; arXiv:2103.03874 Help | Advanced Search All fields Title Author Abstract Comments Journal reference ACM classification MSC classification Report number arXiv identifier DOI ORCID arXiv author ID Help pages Full text Search open search GO open navigation menu quick links Login Help Pages About --> Computer Science > Machine Learning arXiv:2103.03874 (cs) [Submitted on 5 Mar 2021 ( v1 ), last revised 8 Nov 2021 (this version, v2)] Title: Measuring Mathematical Problem Solving With the MATH Dataset Authors: Dan Hendrycks , Collin Burns , Saurav Kadavath , Akul Arora , Steven Basart , Eric Tang , Dawn Song , Jacob Steinhardt View a PDF of the paper titled Measuring Mathematical Problem Solving With the MATH Dataset, by Dan Hendrycks and Collin Burns and Saurav Kadavath and Akul Arora and Steven Basart and Eric Tang and Dawn Song and Jacob Steinhardt View PDF Abstract: Many intellectual endeavors require mathematical problem solving, but this skill remains beyond the capabilities of computers. To measure this ability in machine learning models, we introduce MATH, a new dataset of 12,500 challenging competition mathematics problems. Each problem in MATH has a full step-by-step solution which can be used to teach models to generate answer derivations and explanations. To facilitate future research and increase accuracy on MATH, we also contribute a large auxiliary pretraining dataset which helps teach models the fundamentals of mathematics. Even though we are able to increase accuracy on MATH, our results show that accuracy remains relatively low, even with enormous Transformer models. Moreover, we find that simply increasing budgets and model parameter counts will be impractical for achieving strong mathematical reasoning if scaling trends continue. While scaling Transformers is automatically solving most other text-based tasks, scaling is not currently solving MATH. To have more traction on mathematical problem solving we will likely need new algorithmic advancements from the broader research community. Comments: NeurIPS 2021. Code and the MATH dataset is available at this https URL Subjects: Machine Learning (cs.LG) ; Artificial Intelligence (cs.AI); Computation and Language (cs.CL) Cite as: arXiv:2103.03874 [cs.LG] &nbsp; (or arXiv:2103.03874v2 [cs.LG] for this version) &nbsp; https://doi.org/10.48550/arXiv.2103.03874 Focus to learn more arXiv-issued DOI via DataCite Submission history From: Dan Hendrycks [ view email ] [v1] Fri, 5 Mar 2021 18:59:39 UTC (1,943 KB) [v2] Mon, 8 Nov 2021 21:30:18 UTC (1,581 KB) Full-text links: Access Paper: View a PDF of the paper titled Measuring Mathematical Problem Solving With the MATH Dataset, by Dan Hendrycks and Collin Burns and Saurav Kadavath and Akul Arora and Steven Basart and Eric Tang and Dawn Song and Jacob Steinhardt View PDF TeX Source Other Formats view license Current browse context: cs.LG &lt;&nbsp;prev &nbsp; | &nbsp; next&nbsp;&gt; new | recent | 2021-03 Change to browse by: cs cs.AI cs.CL References &amp; Citations NASA ADS Google Scholar Semantic Scholar 1 blog link ( what is this? ) DBLP - CS Bibliography listing | bibtex Dan Hendrycks Collin Burns Saurav Kadavath Steven Basart Dawn Song &hellip; a export BibTeX citation Loading... BibTeX formatted citation &times; loading... Data provided by: Bookmark Bibliographic Tools Bibliographic and Citation Tools Bibliographic Explorer Toggle Bibliographic Explorer ( What is the Explorer? ) Connected Papers Toggle Connected Papers ( What is Connected Papers? ) Litmaps Toggle Litmaps ( What is Litmaps? ) scite.ai Toggle scite Smart Citations ( What are Smart Citations? ) Code, Data, Media Code, Data and Media Associated with this Article alphaXiv Toggle alphaXiv ( What is alphaXiv? ) Links to Code Toggle CatalyzeX Code Finder for Papers ( What is CatalyzeX? ) DagsHub Toggle DagsHub ( What is DagsHub? ) GotitPub Toggle Gotit.pub ( What is GotitPub? ) Huggingface Toggle Hugging Face ( What is Huggingface? ) Links to Code Toggle Papers with Code ( What is Papers with Code? ) ScienceCast Toggle ScienceCast ( What is ScienceCast? ) Demos Demos Replicate Toggle Replicate ( What is Replicate? ) Spaces Toggle Hugging Face Spaces ( What is Spaces? ) Spaces Toggle TXYZ.AI ( What is TXYZ.AI? ) Related Papers Recommenders and Search Tools Link to Influence Flower Influence Flower ( What are Influence Flowers? ) Core recommender toggle CORE Recommender ( What is CORE? ) IArxiv recommender toggle IArxiv Recommender ( What is IArxiv? ) Author Venue Institution Topic About arXivLabs arXivLabs: experimental projects with community collaborators arXivLabs is a framework that allows collaborators to develop and share new arXiv features directly on our website. Both individuals and organizations that work with arXivLabs have embraced and accepted our values of openness, community, excellence, and user data privacy. arXiv is committed to these values and only works with partners that adhere to them. Have an idea for a project that will add value for arXiv's community? Learn more about arXivLabs . Which authors of this paper are endorsers? | Disable MathJax ( What is MathJax? ) About Help contact arXiv Click here to contact arXiv Contact subscribe to arXiv mailings Click here to subscribe Subscribe Copyright Privacy Policy Web Accessibility Assistance arXiv Operational Status Get status notifications via email or slack ",
      "fetch_method": "direct-html"
    },
    {
      "arxiv_id": "https://arxiv.org/abs/2110.14168",
      "full_text": " [2110.14168] Training Verifiers to Solve Math Word Problems Skip to main content We gratefully acknowledge support from the Simons Foundation, member institutions , and all contributors. Donate &gt; cs &gt; arXiv:2110.14168 Help | Advanced Search All fields Title Author Abstract Comments Journal reference ACM classification MSC classification Report number arXiv identifier DOI ORCID arXiv author ID Help pages Full text Search open search GO open navigation menu quick links Login Help Pages About --> Computer Science > Machine Learning arXiv:2110.14168 (cs) [Submitted on 27 Oct 2021 ( v1 ), last revised 18 Nov 2021 (this version, v2)] Title: Training Verifiers to Solve Math Word Problems Authors: Karl Cobbe , Vineet Kosaraju , Mohammad Bavarian , Mark Chen , Heewoo Jun , Lukasz Kaiser , Matthias Plappert , Jerry Tworek , Jacob Hilton , Reiichiro Nakano , Christopher Hesse , John Schulman View a PDF of the paper titled Training Verifiers to Solve Math Word Problems, by Karl Cobbe and 11 other authors View PDF Abstract: State-of-the-art language models can match human performance on many tasks, but they still struggle to robustly perform multi-step mathematical reasoning. To diagnose the failures of current models and support research, we introduce GSM8K, a dataset of 8.5K high quality linguistically diverse grade school math word problems. We find that even the largest transformer models fail to achieve high test performance, despite the conceptual simplicity of this problem distribution. To increase performance, we propose training verifiers to judge the correctness of model completions. At test time, we generate many candidate solutions and select the one ranked highest by the verifier. We demonstrate that verification significantly improves performance on GSM8K, and we provide strong empirical evidence that verification scales more effectively with increased data than a finetuning baseline. Subjects: Machine Learning (cs.LG) ; Computation and Language (cs.CL) Cite as: arXiv:2110.14168 [cs.LG] &nbsp; (or arXiv:2110.14168v2 [cs.LG] for this version) &nbsp; https://doi.org/10.48550/arXiv.2110.14168 Focus to learn more arXiv-issued DOI via DataCite Submission history From: Karl Cobbe [ view email ] [v1] Wed, 27 Oct 2021 04:49:45 UTC (3,262 KB) [v2] Thu, 18 Nov 2021 00:23:45 UTC (3,262 KB) Full-text links: Access Paper: View a PDF of the paper titled Training Verifiers to Solve Math Word Problems, by Karl Cobbe and 11 other authors View PDF TeX Source Other Formats view license Current browse context: cs.LG &lt;&nbsp;prev &nbsp; | &nbsp; next&nbsp;&gt; new | recent | 2021-10 Change to browse by: cs cs.CL References &amp; Citations NASA ADS Google Scholar Semantic Scholar 3 blog links ( what is this? ) DBLP - CS Bibliography listing | bibtex Karl Cobbe Vineet Kosaraju Mohammad Bavarian Reiichiro Nakano Christopher Hesse &hellip; a export BibTeX citation Loading... BibTeX formatted citation &times; loading... Data provided by: Bookmark Bibliographic Tools Bibliographic and Citation Tools Bibliographic Explorer Toggle Bibliographic Explorer ( What is the Explorer? ) Connected Papers Toggle Connected Papers ( What is Connected Papers? ) Litmaps Toggle Litmaps ( What is Litmaps? ) scite.ai Toggle scite Smart Citations ( What are Smart Citations? ) Code, Data, Media Code, Data and Media Associated with this Article alphaXiv Toggle alphaXiv ( What is alphaXiv? ) Links to Code Toggle CatalyzeX Code Finder for Papers ( What is CatalyzeX? ) DagsHub Toggle DagsHub ( What is DagsHub? ) GotitPub Toggle Gotit.pub ( What is GotitPub? ) Huggingface Toggle Hugging Face ( What is Huggingface? ) Links to Code Toggle Papers with Code ( What is Papers with Code? ) ScienceCast Toggle ScienceCast ( What is ScienceCast? ) Demos Demos Replicate Toggle Replicate ( What is Replicate? ) Spaces Toggle Hugging Face Spaces ( What is Spaces? ) Spaces Toggle TXYZ.AI ( What is TXYZ.AI? ) Related Papers Recommenders and Search Tools Link to Influence Flower Influence Flower ( What are Influence Flowers? ) Core recommender toggle CORE Recommender ( What is CORE? ) IArxiv recommender toggle IArxiv Recommender ( What is IArxiv? ) Author Venue Institution Topic About arXivLabs arXivLabs: experimental projects with community collaborators arXivLabs is a framework that allows collaborators to develop and share new arXiv features directly on our website. Both individuals and organizations that work with arXivLabs have embraced and accepted our values of openness, community, excellence, and user data privacy. arXiv is committed to these values and only works with partners that adhere to them. Have an idea for a project that will add value for arXiv's community? Learn more about arXivLabs . Which authors of this paper are endorsers? | Disable MathJax ( What is MathJax? ) About Help contact arXiv Click here to contact arXiv Contact subscribe to arXiv mailings Click here to subscribe Subscribe Copyright Privacy Policy Web Accessibility Assistance arXiv Operational Status Get status notifications via email or slack ",
      "fetch_method": "direct-html"
    },
    {
      "arxiv_id": "https://arxiv.org/abs/2311.12022",
      "full_text": " [2311.12022] GPQA: A Graduate-Level Google-Proof Q&amp;A Benchmark Skip to main content We gratefully acknowledge support from the Simons Foundation, member institutions , and all contributors. Donate &gt; cs &gt; arXiv:2311.12022 Help | Advanced Search All fields Title Author Abstract Comments Journal reference ACM classification MSC classification Report number arXiv identifier DOI ORCID arXiv author ID Help pages Full text Search open search GO open navigation menu quick links Login Help Pages About --> Computer Science > Artificial Intelligence arXiv:2311.12022 (cs) [Submitted on 20 Nov 2023] Title: GPQA: A Graduate-Level Google-Proof Q&amp;A Benchmark Authors: David Rein , Betty Li Hou , Asa Cooper Stickland , Jackson Petty , Richard Yuanzhe Pang , Julien Dirani , Julian Michael , Samuel R. Bowman View a PDF of the paper titled GPQA: A Graduate-Level Google-Proof Q&amp;A Benchmark, by David Rein and 7 other authors View PDF Abstract: We present GPQA, a challenging dataset of 448 multiple-choice questions written by domain experts in biology, physics, and chemistry. We ensure that the questions are high-quality and extremely difficult: experts who have or are pursuing PhDs in the corresponding domains reach 65% accuracy (74% when discounting clear mistakes the experts identified in retrospect), while highly skilled non-expert validators only reach 34% accuracy, despite spending on average over 30 minutes with unrestricted access to the web (i.e., the questions are &#34;Google-proof&#34;). The questions are also difficult for state-of-the-art AI systems, with our strongest GPT-4 based baseline achieving 39% accuracy. If we are to use future AI systems to help us answer very hard questions, for example, when developing new scientific knowledge, we need to develop scalable oversight methods that enable humans to supervise their outputs, which may be difficult even if the supervisors are themselves skilled and knowledgeable. The difficulty of GPQA both for skilled non-experts and frontier AI systems should enable realistic scalable oversight experiments, which we hope can help devise ways for human experts to reliably get truthful information from AI systems that surpass human capabilities. Comments: 28 pages, 5 figures, 7 tables Subjects: Artificial Intelligence (cs.AI) ; Computation and Language (cs.CL) Cite as: arXiv:2311.12022 [cs.AI] &nbsp; (or arXiv:2311.12022v1 [cs.AI] for this version) &nbsp; https://doi.org/10.48550/arXiv.2311.12022 Focus to learn more arXiv-issued DOI via DataCite Submission history From: David Rein [ view email ] [v1] Mon, 20 Nov 2023 18:57:34 UTC (7,790 KB) Full-text links: Access Paper: View a PDF of the paper titled GPQA: A Graduate-Level Google-Proof Q&amp;A Benchmark, by David Rein and 7 other authors View PDF TeX Source Other Formats view license Current browse context: cs.AI &lt;&nbsp;prev &nbsp; | &nbsp; next&nbsp;&gt; new | recent | 2023-11 Change to browse by: cs cs.CL References &amp; Citations NASA ADS Google Scholar Semantic Scholar a export BibTeX citation Loading... BibTeX formatted citation &times; loading... Data provided by: Bookmark Bibliographic Tools Bibliographic and Citation Tools Bibliographic Explorer Toggle Bibliographic Explorer ( What is the Explorer? ) Connected Papers Toggle Connected Papers ( What is Connected Papers? ) Litmaps Toggle Litmaps ( What is Litmaps? ) scite.ai Toggle scite Smart Citations ( What are Smart Citations? ) Code, Data, Media Code, Data and Media Associated with this Article alphaXiv Toggle alphaXiv ( What is alphaXiv? ) Links to Code Toggle CatalyzeX Code Finder for Papers ( What is CatalyzeX? ) DagsHub Toggle DagsHub ( What is DagsHub? ) GotitPub Toggle Gotit.pub ( What is GotitPub? ) Huggingface Toggle Hugging Face ( What is Huggingface? ) Links to Code Toggle Papers with Code ( What is Papers with Code? ) ScienceCast Toggle ScienceCast ( What is ScienceCast? ) Demos Demos Replicate Toggle Replicate ( What is Replicate? ) Spaces Toggle Hugging Face Spaces ( What is Spaces? ) Spaces Toggle TXYZ.AI ( What is TXYZ.AI? ) Related Papers Recommenders and Search Tools Link to Influence Flower Influence Flower ( What are Influence Flowers? ) Core recommender toggle CORE Recommender ( What is CORE? ) Author Venue Institution Topic About arXivLabs arXivLabs: experimental projects with community collaborators arXivLabs is a framework that allows collaborators to develop and share new arXiv features directly on our website. Both individuals and organizations that work with arXivLabs have embraced and accepted our values of openness, community, excellence, and user data privacy. arXiv is committed to these values and only works with partners that adhere to them. Have an idea for a project that will add value for arXiv's community? Learn more about arXivLabs . Which authors of this paper are endorsers? | Disable MathJax ( What is MathJax? ) About Help contact arXiv Click here to contact arXiv Contact subscribe to arXiv mailings Click here to subscribe Subscribe Copyright Privacy Policy Web Accessibility Assistance arXiv Operational Status Get status notifications via email or slack ",
      "fetch_method": "direct-html"
    },
    {
      "arxiv_id": "https://arxiv.org/abs/2108.07732",
      "full_text": " [2108.07732] Program Synthesis with Large Language Models Skip to main content We gratefully acknowledge support from the Simons Foundation, member institutions , and all contributors. Donate &gt; cs &gt; arXiv:2108.07732 Help | Advanced Search All fields Title Author Abstract Comments Journal reference ACM classification MSC classification Report number arXiv identifier DOI ORCID arXiv author ID Help pages Full text Search open search GO open navigation menu quick links Login Help Pages About --> Computer Science > Programming Languages arXiv:2108.07732 (cs) [Submitted on 16 Aug 2021] Title: Program Synthesis with Large Language Models Authors: Jacob Austin , Augustus Odena , Maxwell Nye , Maarten Bosma , Henryk Michalewski , David Dohan , Ellen Jiang , Carrie Cai , Michael Terry , Quoc Le , Charles Sutton View a PDF of the paper titled Program Synthesis with Large Language Models, by Jacob Austin and 10 other authors View PDF Abstract: This paper explores the limits of the current generation of large language models for program synthesis in general purpose programming languages. We evaluate a collection of such models (with between 244M and 137B parameters) on two new benchmarks, MBPP and MathQA-Python, in both the few-shot and fine-tuning regimes. Our benchmarks are designed to measure the ability of these models to synthesize short Python programs from natural language descriptions. The Mostly Basic Programming Problems (MBPP) dataset contains 974 programming tasks, designed to be solvable by entry-level programmers. The MathQA-Python dataset, a Python version of the MathQA benchmark, contains 23914 problems that evaluate the ability of the models to synthesize code from more complex text. On both datasets, we find that synthesis performance scales log-linearly with model size. Our largest models, even without finetuning on a code dataset, can synthesize solutions to 59.6 percent of the problems from MBPP using few-shot learning with a well-designed prompt. Fine-tuning on a held-out portion of the dataset improves performance by about 10 percentage points across most model sizes. On the MathQA-Python dataset, the largest fine-tuned model achieves 83.8 percent accuracy. Going further, we study the model&#39;s ability to engage in dialog about code, incorporating human feedback to improve its solutions. We find that natural language feedback from a human halves the error rate compared to the model&#39;s initial prediction. Additionally, we conduct an error analysis to shed light on where these models fall short and what types of programs are most difficult to generate. Finally, we explore the semantic grounding of these models by fine-tuning them to predict the results of program execution. We find that even our best models are generally unable to predict the output of a program given a specific input. Comments: Jacob and Augustus contributed equally Subjects: Programming Languages (cs.PL) ; Machine Learning (cs.LG) Cite as: arXiv:2108.07732 [cs.PL] &nbsp; (or arXiv:2108.07732v1 [cs.PL] for this version) &nbsp; https://doi.org/10.48550/arXiv.2108.07732 Focus to learn more arXiv-issued DOI via DataCite Submission history From: Augustus Odena [ view email ] [v1] Mon, 16 Aug 2021 03:57:30 UTC (2,247 KB) Full-text links: Access Paper: View a PDF of the paper titled Program Synthesis with Large Language Models, by Jacob Austin and 10 other authors View PDF TeX Source Other Formats view license Current browse context: cs.PL &lt;&nbsp;prev &nbsp; | &nbsp; next&nbsp;&gt; new | recent | 2021-08 Change to browse by: cs cs.LG References &amp; Citations NASA ADS Google Scholar Semantic Scholar 1 blog link ( what is this? ) DBLP - CS Bibliography listing | bibtex Augustus Odena Maxwell Nye Henryk Michalewski David Dohan Carrie J. Cai &hellip; export BibTeX citation Loading... BibTeX formatted citation &times; loading... Data provided by: Bookmark Bibliographic Tools Bibliographic and Citation Tools Bibliographic Explorer Toggle Bibliographic Explorer ( What is the Explorer? ) Connected Papers Toggle Connected Papers ( What is Connected Papers? ) Litmaps Toggle Litmaps ( What is Litmaps? ) scite.ai Toggle scite Smart Citations ( What are Smart Citations? ) Code, Data, Media Code, Data and Media Associated with this Article alphaXiv Toggle alphaXiv ( What is alphaXiv? ) Links to Code Toggle CatalyzeX Code Finder for Papers ( What is CatalyzeX? ) DagsHub Toggle DagsHub ( What is DagsHub? ) GotitPub Toggle Gotit.pub ( What is GotitPub? ) Huggingface Toggle Hugging Face ( What is Huggingface? ) Links to Code Toggle Papers with Code ( What is Papers with Code? ) ScienceCast Toggle ScienceCast ( What is ScienceCast? ) Demos Demos Replicate Toggle Replicate ( What is Replicate? ) Spaces Toggle Hugging Face Spaces ( What is Spaces? ) Spaces Toggle TXYZ.AI ( What is TXYZ.AI? ) Related Papers Recommenders and Search Tools Link to Influence Flower Influence Flower ( What are Influence Flowers? ) Core recommender toggle CORE Recommender ( What is CORE? ) Author Venue Institution Topic About arXivLabs arXivLabs: experimental projects with community collaborators arXivLabs is a framework that allows collaborators to develop and share new arXiv features directly on our website. Both individuals and organizations that work with arXivLabs have embraced and accepted our values of openness, community, excellence, and user data privacy. arXiv is committed to these values and only works with partners that adhere to them. Have an idea for a project that will add value for arXiv's community? Learn more about arXivLabs . Which authors of this paper are endorsers? | Disable MathJax ( What is MathJax? ) About Help contact arXiv Click here to contact arXiv Contact subscribe to arXiv mailings Click here to subscribe Subscribe Copyright Privacy Policy Web Accessibility Assistance arXiv Operational Status Get status notifications via email or slack ",
      "fetch_method": "direct-html"
    },
    {
      "arxiv_id": "https://arxiv.org/abs/2107.03374",
      "full_text": " [2107.03374] Evaluating Large Language Models Trained on Code Skip to main content We gratefully acknowledge support from the Simons Foundation, member institutions , and all contributors. Donate &gt; cs &gt; arXiv:2107.03374 Help | Advanced Search All fields Title Author Abstract Comments Journal reference ACM classification MSC classification Report number arXiv identifier DOI ORCID arXiv author ID Help pages Full text Search open search GO open navigation menu quick links Login Help Pages About --> Computer Science > Machine Learning arXiv:2107.03374 (cs) [Submitted on 7 Jul 2021 ( v1 ), last revised 14 Jul 2021 (this version, v2)] Title: Evaluating Large Language Models Trained on Code Authors: Mark Chen , Jerry Tworek , Heewoo Jun , Qiming Yuan , Henrique Ponde de Oliveira Pinto , Jared Kaplan , Harri Edwards , Yuri Burda , Nicholas Joseph , Greg Brockman , Alex Ray , Raul Puri , Gretchen Krueger , Michael Petrov , Heidy Khlaaf , Girish Sastry , Pamela Mishkin , Brooke Chan , Scott Gray , Nick Ryder , Mikhail Pavlov , Alethea Power , Lukasz Kaiser , Mohammad Bavarian , Clemens Winter , Philippe Tillet , Felipe Petroski Such , Dave Cummings , Matthias Plappert , Fotios Chantzis , Elizabeth Barnes , Ariel Herbert-Voss , William Hebgen Guss , Alex Nichol , Alex Paino , Nikolas Tezak , Jie Tang , Igor Babuschkin , Suchir Balaji , Shantanu Jain , William Saunders , Christopher Hesse , Andrew N. Carr , Jan Leike , Josh Achiam , Vedant Misra , Evan Morikawa , Alec Radford , Matthew Knight , Miles Brundage , Mira Murati , Katie Mayer , Peter Welinder , Bob McGrew , Dario Amodei , Sam McCandlish , Ilya Sutskever , Wojciech Zaremba View a PDF of the paper titled Evaluating Large Language Models Trained on Code, by Mark Chen and 57 other authors View PDF Abstract: We introduce Codex, a GPT language model fine-tuned on publicly available code from GitHub, and study its Python code-writing capabilities. A distinct production version of Codex powers GitHub Copilot. On HumanEval, a new evaluation set we release to measure functional correctness for synthesizing programs from docstrings, our model solves 28.8% of the problems, while GPT-3 solves 0% and GPT-J solves 11.4%. Furthermore, we find that repeated sampling from the model is a surprisingly effective strategy for producing working solutions to difficult prompts. Using this method, we solve 70.2% of our problems with 100 samples per problem. Careful investigation of our model reveals its limitations, including difficulty with docstrings describing long chains of operations and with binding operations to variables. Finally, we discuss the potential broader impacts of deploying powerful code generation technologies, covering safety, security, and economics. Comments: corrected typos, added references, added authors, added acknowledgements Subjects: Machine Learning (cs.LG) Cite as: arXiv:2107.03374 [cs.LG] &nbsp; (or arXiv:2107.03374v2 [cs.LG] for this version) &nbsp; https://doi.org/10.48550/arXiv.2107.03374 Focus to learn more arXiv-issued DOI via DataCite Submission history From: Mark Chen [ view email ] [v1] Wed, 7 Jul 2021 17:41:24 UTC (1,466 KB) [v2] Wed, 14 Jul 2021 17:16:02 UTC (1,467 KB) Full-text links: Access Paper: View a PDF of the paper titled Evaluating Large Language Models Trained on Code, by Mark Chen and 57 other authors View PDF TeX Source Other Formats view license Current browse context: cs.LG &lt;&nbsp;prev &nbsp; | &nbsp; next&nbsp;&gt; new | recent | 2021-07 Change to browse by: cs References &amp; Citations NASA ADS Google Scholar Semantic Scholar 4 blog links ( what is this? ) DBLP - CS Bibliography listing | bibtex Heewoo Jun Jared Kaplan Harrison Edwards Yuri Burda Greg Brockman &hellip; export BibTeX citation Loading... BibTeX formatted citation &times; loading... Data provided by: Bookmark Bibliographic Tools Bibliographic and Citation Tools Bibliographic Explorer Toggle Bibliographic Explorer ( What is the Explorer? ) Connected Papers Toggle Connected Papers ( What is Connected Papers? ) Litmaps Toggle Litmaps ( What is Litmaps? ) scite.ai Toggle scite Smart Citations ( What are Smart Citations? ) Code, Data, Media Code, Data and Media Associated with this Article alphaXiv Toggle alphaXiv ( What is alphaXiv? ) Links to Code Toggle CatalyzeX Code Finder for Papers ( What is CatalyzeX? ) DagsHub Toggle DagsHub ( What is DagsHub? ) GotitPub Toggle Gotit.pub ( What is GotitPub? ) Huggingface Toggle Hugging Face ( What is Huggingface? ) Links to Code Toggle Papers with Code ( What is Papers with Code? ) ScienceCast Toggle ScienceCast ( What is ScienceCast? ) Demos Demos Replicate Toggle Replicate ( What is Replicate? ) Spaces Toggle Hugging Face Spaces ( What is Spaces? ) Spaces Toggle TXYZ.AI ( What is TXYZ.AI? ) Related Papers Recommenders and Search Tools Link to Influence Flower Influence Flower ( What are Influence Flowers? ) Core recommender toggle CORE Recommender ( What is CORE? ) IArxiv recommender toggle IArxiv Recommender ( What is IArxiv? ) Author Venue Institution Topic About arXivLabs arXivLabs: experimental projects with community collaborators arXivLabs is a framework that allows collaborators to develop and share new arXiv features directly on our website. Both individuals and organizations that work with arXivLabs have embraced and accepted our values of openness, community, excellence, and user data privacy. arXiv is committed to these values and only works with partners that adhere to them. Have an idea for a project that will add value for arXiv's community? Learn more about arXivLabs . Which authors of this paper are endorsers? | Disable MathJax ( What is MathJax? ) About Help contact arXiv Click here to contact arXiv Contact subscribe to arXiv mailings Click here to subscribe Subscribe Copyright Privacy Policy Web Accessibility Assistance arXiv Operational Status Get status notifications via email or slack ",
      "fetch_method": "direct-html"
    },
    {
      "arxiv_id": "https://arxiv.org/abs/2210.03057",
      "full_text": " [2210.03057] Language Models are Multilingual Chain-of-Thought Reasoners Skip to main content We gratefully acknowledge support from the Simons Foundation, member institutions , and all contributors. Donate &gt; cs &gt; arXiv:2210.03057 Help | Advanced Search All fields Title Author Abstract Comments Journal reference ACM classification MSC classification Report number arXiv identifier DOI ORCID arXiv author ID Help pages Full text Search open search GO open navigation menu quick links Login Help Pages About --> Computer Science > Computation and Language arXiv:2210.03057 (cs) [Submitted on 6 Oct 2022] Title: Language Models are Multilingual Chain-of-Thought Reasoners Authors: Freda Shi , Mirac Suzgun , Markus Freitag , Xuezhi Wang , Suraj Srivats , Soroush Vosoughi , Hyung Won Chung , Yi Tay , Sebastian Ruder , Denny Zhou , Dipanjan Das , Jason Wei View a PDF of the paper titled Language Models are Multilingual Chain-of-Thought Reasoners, by Freda Shi and 11 other authors View PDF Abstract: We evaluate the reasoning abilities of large language models in multilingual settings. We introduce the Multilingual Grade School Math (MGSM) benchmark, by manually translating 250 grade-school math problems from the GSM8K dataset (Cobbe et al., 2021) into ten typologically diverse languages. We find that the ability to solve MGSM problems via chain-of-thought prompting emerges with increasing model scale, and that models have strikingly strong multilingual reasoning abilities, even in underrepresented languages such as Bengali and Swahili. Finally, we show that the multilingual reasoning abilities of language models extend to other tasks such as commonsense reasoning and word-in-context semantic judgment. The MGSM benchmark is publicly available at this https URL . Subjects: Computation and Language (cs.CL) ; Artificial Intelligence (cs.AI); Machine Learning (cs.LG) Cite as: arXiv:2210.03057 [cs.CL] &nbsp; (or arXiv:2210.03057v1 [cs.CL] for this version) &nbsp; https://doi.org/10.48550/arXiv.2210.03057 Focus to learn more arXiv-issued DOI via DataCite Submission history From: Freda Shi [ view email ] [v1] Thu, 6 Oct 2022 17:03:34 UTC (544 KB) Full-text links: Access Paper: View a PDF of the paper titled Language Models are Multilingual Chain-of-Thought Reasoners, by Freda Shi and 11 other authors View PDF TeX Source Other Formats view license Current browse context: cs.CL &lt;&nbsp;prev &nbsp; | &nbsp; next&nbsp;&gt; new | recent | 2022-10 Change to browse by: cs cs.AI cs.LG References &amp; Citations NASA ADS Google Scholar Semantic Scholar a export BibTeX citation Loading... BibTeX formatted citation &times; loading... Data provided by: Bookmark Bibliographic Tools Bibliographic and Citation Tools Bibliographic Explorer Toggle Bibliographic Explorer ( What is the Explorer? ) Connected Papers Toggle Connected Papers ( What is Connected Papers? ) Litmaps Toggle Litmaps ( What is Litmaps? ) scite.ai Toggle scite Smart Citations ( What are Smart Citations? ) Code, Data, Media Code, Data and Media Associated with this Article alphaXiv Toggle alphaXiv ( What is alphaXiv? ) Links to Code Toggle CatalyzeX Code Finder for Papers ( What is CatalyzeX? ) DagsHub Toggle DagsHub ( What is DagsHub? ) GotitPub Toggle Gotit.pub ( What is GotitPub? ) Huggingface Toggle Hugging Face ( What is Huggingface? ) Links to Code Toggle Papers with Code ( What is Papers with Code? ) ScienceCast Toggle ScienceCast ( What is ScienceCast? ) Demos Demos Replicate Toggle Replicate ( What is Replicate? ) Spaces Toggle Hugging Face Spaces ( What is Spaces? ) Spaces Toggle TXYZ.AI ( What is TXYZ.AI? ) Related Papers Recommenders and Search Tools Link to Influence Flower Influence Flower ( What are Influence Flowers? ) Core recommender toggle CORE Recommender ( What is CORE? ) Author Venue Institution Topic About arXivLabs arXivLabs: experimental projects with community collaborators arXivLabs is a framework that allows collaborators to develop and share new arXiv features directly on our website. Both individuals and organizations that work with arXivLabs have embraced and accepted our values of openness, community, excellence, and user data privacy. arXiv is committed to these values and only works with partners that adhere to them. Have an idea for a project that will add value for arXiv's community? Learn more about arXivLabs . Which authors of this paper are endorsers? | Disable MathJax ( What is MathJax? ) About Help contact arXiv Click here to contact arXiv Contact subscribe to arXiv mailings Click here to subscribe Subscribe Copyright Privacy Policy Web Accessibility Assistance arXiv Operational Status Get status notifications via email or slack ",
      "fetch_method": "direct-html"
    },
    {
      "arxiv_id": "https://arxiv.org/abs/2106.03193",
      "full_text": " [2106.03193] The FLORES-101 Evaluation Benchmark for Low-Resource and Multilingual Machine Translation Skip to main content We gratefully acknowledge support from the Simons Foundation, member institutions , and all contributors. Donate &gt; cs &gt; arXiv:2106.03193 Help | Advanced Search All fields Title Author Abstract Comments Journal reference ACM classification MSC classification Report number arXiv identifier DOI ORCID arXiv author ID Help pages Full text Search open search GO open navigation menu quick links Login Help Pages About --> Computer Science > Computation and Language arXiv:2106.03193 (cs) [Submitted on 6 Jun 2021] Title: The FLORES-101 Evaluation Benchmark for Low-Resource and Multilingual Machine Translation Authors: Naman Goyal , Cynthia Gao , Vishrav Chaudhary , Peng-Jen Chen , Guillaume Wenzek , Da Ju , Sanjana Krishnan , Marc&#39;Aurelio Ranzato , Francisco Guzman , Angela Fan View a PDF of the paper titled The FLORES-101 Evaluation Benchmark for Low-Resource and Multilingual Machine Translation, by Naman Goyal and 9 other authors View PDF Abstract: One of the biggest challenges hindering progress in low-resource and multilingual machine translation is the lack of good evaluation benchmarks. Current evaluation benchmarks either lack good coverage of low-resource languages, consider only restricted domains, or are low quality because they are constructed using semi-automatic procedures. In this work, we introduce the FLORES-101 evaluation benchmark, consisting of 3001 sentences extracted from English Wikipedia and covering a variety of different topics and domains. These sentences have been translated in 101 languages by professional translators through a carefully controlled process. The resulting dataset enables better assessment of model quality on the long tail of low-resource languages, including the evaluation of many-to-many multilingual translation systems, as all translations are multilingually aligned. By publicly releasing such a high-quality and high-coverage dataset, we hope to foster progress in the machine translation community and beyond. Subjects: Computation and Language (cs.CL) ; Artificial Intelligence (cs.AI) Cite as: arXiv:2106.03193 [cs.CL] &nbsp; (or arXiv:2106.03193v1 [cs.CL] for this version) &nbsp; https://doi.org/10.48550/arXiv.2106.03193 Focus to learn more arXiv-issued DOI via DataCite Submission history From: Angela Fan [ view email ] [v1] Sun, 6 Jun 2021 17:58:12 UTC (1,898 KB) Full-text links: Access Paper: View a PDF of the paper titled The FLORES-101 Evaluation Benchmark for Low-Resource and Multilingual Machine Translation, by Naman Goyal and 9 other authors View PDF TeX Source Other Formats view license Current browse context: cs.CL &lt;&nbsp;prev &nbsp; | &nbsp; next&nbsp;&gt; new | recent | 2021-06 Change to browse by: cs cs.AI References &amp; Citations NASA ADS Google Scholar Semantic Scholar DBLP - CS Bibliography listing | bibtex Naman Goyal Vishrav Chaudhary Peng-Jen Chen Guillaume Wenzek Da Ju &hellip; a export BibTeX citation Loading... BibTeX formatted citation &times; loading... Data provided by: Bookmark Bibliographic Tools Bibliographic and Citation Tools Bibliographic Explorer Toggle Bibliographic Explorer ( What is the Explorer? ) Connected Papers Toggle Connected Papers ( What is Connected Papers? ) Litmaps Toggle Litmaps ( What is Litmaps? ) scite.ai Toggle scite Smart Citations ( What are Smart Citations? ) Code, Data, Media Code, Data and Media Associated with this Article alphaXiv Toggle alphaXiv ( What is alphaXiv? ) Links to Code Toggle CatalyzeX Code Finder for Papers ( What is CatalyzeX? ) DagsHub Toggle DagsHub ( What is DagsHub? ) GotitPub Toggle Gotit.pub ( What is GotitPub? ) Huggingface Toggle Hugging Face ( What is Huggingface? ) Links to Code Toggle Papers with Code ( What is Papers with Code? ) ScienceCast Toggle ScienceCast ( What is ScienceCast? ) Demos Demos Replicate Toggle Replicate ( What is Replicate? ) Spaces Toggle Hugging Face Spaces ( What is Spaces? ) Spaces Toggle TXYZ.AI ( What is TXYZ.AI? ) Related Papers Recommenders and Search Tools Link to Influence Flower Influence Flower ( What are Influence Flowers? ) Core recommender toggle CORE Recommender ( What is CORE? ) Author Venue Institution Topic About arXivLabs arXivLabs: experimental projects with community collaborators arXivLabs is a framework that allows collaborators to develop and share new arXiv features directly on our website. Both individuals and organizations that work with arXivLabs have embraced and accepted our values of openness, community, excellence, and user data privacy. arXiv is committed to these values and only works with partners that adhere to them. Have an idea for a project that will add value for arXiv's community? Learn more about arXivLabs . Which authors of this paper are endorsers? | Disable MathJax ( What is MathJax? ) About Help contact arXiv Click here to contact arXiv Contact subscribe to arXiv mailings Click here to subscribe Subscribe Copyright Privacy Policy Web Accessibility Assistance arXiv Operational Status Get status notifications via email or slack ",
      "fetch_method": "direct-html"
    },
    {
      "arxiv_id": "https://arxiv.org/abs/1910.11856v3",
      "full_text": " [1910.11856v3] On the Cross-lingual Transferability of Monolingual Representations Skip to main content We gratefully acknowledge support from the Simons Foundation, member institutions , and all contributors. Donate &gt; cs &gt; arXiv:1910.11856v3 Help | Advanced Search All fields Title Author Abstract Comments Journal reference ACM classification MSC classification Report number arXiv identifier DOI ORCID arXiv author ID Help pages Full text Search open search GO open navigation menu quick links Login Help Pages About --> Computer Science > Computation and Language arXiv:1910.11856v3 (cs) [Submitted on 25 Oct 2019 ( v1 ), last revised 26 May 2020 (this version, v3)] Title: On the Cross-lingual Transferability of Monolingual Representations Authors: Mikel Artetxe , Sebastian Ruder , Dani Yogatama View a PDF of the paper titled On the Cross-lingual Transferability of Monolingual Representations, by Mikel Artetxe and 2 other authors View PDF Abstract: State-of-the-art unsupervised multilingual models (e.g., multilingual BERT) have been shown to generalize in a zero-shot cross-lingual setting. This generalization ability has been attributed to the use of a shared subword vocabulary and joint training across multiple languages giving rise to deep multilingual abstractions. We evaluate this hypothesis by designing an alternative approach that transfers a monolingual model to new languages at the lexical level. More concretely, we first train a transformer-based masked language model on one language, and transfer it to a new language by learning a new embedding matrix with the same masked language modeling objective, freezing parameters of all other layers. This approach does not rely on a shared vocabulary or joint training. However, we show that it is competitive with multilingual BERT on standard cross-lingual classification benchmarks and on a new Cross-lingual Question Answering Dataset (XQuAD). Our results contradict common beliefs of the basis of the generalization ability of multilingual models and suggest that deep monolingual models learn some abstractions that generalize across languages. We also release XQuAD as a more comprehensive cross-lingual benchmark, which comprises 240 paragraphs and 1190 question-answer pairs from SQuAD v1.1 translated into ten languages by professional translators. Comments: ACL 2020 Subjects: Computation and Language (cs.CL) ; Artificial Intelligence (cs.AI); Machine Learning (cs.LG) Cite as: arXiv:1910.11856 [cs.CL] &nbsp; (or arXiv:1910.11856v3 [cs.CL] for this version) &nbsp; https://doi.org/10.48550/arXiv.1910.11856 Focus to learn more arXiv-issued DOI via DataCite Related DOI : https://doi.org/10.18653/v1/2020.acl-main.421 Focus to learn more DOI(s) linking to related resources Submission history From: Mikel Artetxe [ view email ] [v1] Fri, 25 Oct 2019 17:30:20 UTC (164 KB) [v2] Thu, 30 Apr 2020 16:55:33 UTC (166 KB) [v3] Tue, 26 May 2020 22:45:22 UTC (167 KB) Full-text links: Access Paper: View a PDF of the paper titled On the Cross-lingual Transferability of Monolingual Representations, by Mikel Artetxe and 2 other authors View PDF TeX Source Other Formats view license Current browse context: cs.CL &lt;&nbsp;prev &nbsp; | &nbsp; next&nbsp;&gt; new | recent | 2019-10 Change to browse by: cs cs.AI cs.LG References &amp; Citations NASA ADS Google Scholar Semantic Scholar 3 blog links ( what is this? ) DBLP - CS Bibliography listing | bibtex Mikel Artetxe Sebastian Ruder Dani Yogatama a export BibTeX citation Loading... BibTeX formatted citation &times; loading... Data provided by: Bookmark Bibliographic Tools Bibliographic and Citation Tools Bibliographic Explorer Toggle Bibliographic Explorer ( What is the Explorer? ) Connected Papers Toggle Connected Papers ( What is Connected Papers? ) Litmaps Toggle Litmaps ( What is Litmaps? ) scite.ai Toggle scite Smart Citations ( What are Smart Citations? ) Code, Data, Media Code, Data and Media Associated with this Article alphaXiv Toggle alphaXiv ( What is alphaXiv? ) Links to Code Toggle CatalyzeX Code Finder for Papers ( What is CatalyzeX? ) DagsHub Toggle DagsHub ( What is DagsHub? ) GotitPub Toggle Gotit.pub ( What is GotitPub? ) Huggingface Toggle Hugging Face ( What is Huggingface? ) Links to Code Toggle Papers with Code ( What is Papers with Code? ) ScienceCast Toggle ScienceCast ( What is ScienceCast? ) Demos Demos Replicate Toggle Replicate ( What is Replicate? ) Spaces Toggle Hugging Face Spaces ( What is Spaces? ) Spaces Toggle TXYZ.AI ( What is TXYZ.AI? ) Related Papers Recommenders and Search Tools Link to Influence Flower Influence Flower ( What are Influence Flowers? ) Core recommender toggle CORE Recommender ( What is CORE? ) Author Venue Institution Topic About arXivLabs arXivLabs: experimental projects with community collaborators arXivLabs is a framework that allows collaborators to develop and share new arXiv features directly on our website. Both individuals and organizations that work with arXivLabs have embraced and accepted our values of openness, community, excellence, and user data privacy. arXiv is committed to these values and only works with partners that adhere to them. Have an idea for a project that will add value for arXiv's community? Learn more about arXivLabs . Which authors of this paper are endorsers? | Disable MathJax ( What is MathJax? ) About Help contact arXiv Click here to contact arXiv Contact subscribe to arXiv mailings Click here to subscribe Subscribe Copyright Privacy Policy Web Accessibility Assistance arXiv Operational Status Get status notifications via email or slack ",
      "fetch_method": "direct-html"
    },
    {
      "arxiv_id": "https://arxiv.org/abs/2502.12404v1",
      "full_text": " [2502.12404v1] WMT24++: Expanding the Language Coverage of WMT24 to 55 Languages &amp; Dialects Skip to main content We gratefully acknowledge support from the Simons Foundation, member institutions , and all contributors. Donate &gt; cs &gt; arXiv:2502.12404v1 Help | Advanced Search All fields Title Author Abstract Comments Journal reference ACM classification MSC classification Report number arXiv identifier DOI ORCID arXiv author ID Help pages Full text Search open search GO open navigation menu quick links Login Help Pages About --> Computer Science > Computation and Language arXiv:2502.12404v1 (cs) [Submitted on 18 Feb 2025] Title: WMT24++: Expanding the Language Coverage of WMT24 to 55 Languages &amp; Dialects Authors: Daniel Deutsch , Eleftheria Briakou , Isaac Caswell , Mara Finkelstein , Rebecca Galor , Juraj Juraska , Geza Kovacs , Alison Lui , Ricardo Rei , Jason Riesa , Shruti Rijhwani , Parker Riley , Elizabeth Salesky , Firas Trabelsi , Stephanie Winkler , Biao Zhang , Markus Freitag View a PDF of the paper titled WMT24++: Expanding the Language Coverage of WMT24 to 55 Languages &amp; Dialects, by Daniel Deutsch and 16 other authors View PDF HTML (experimental) Abstract: As large language models (LLM) become more and more capable in languages other than English, it is important to collect benchmark datasets in order to evaluate their multilingual performance, including on tasks like machine translation (MT). In this work, we extend the WMT24 dataset to cover 55 languages by collecting new human-written references and post-edits for 46 new languages and dialects in addition to post-edits of the references in 8 out of 9 languages in the original WMT24 dataset. The dataset covers four domains: literary, news, social, and speech. We benchmark a variety of MT providers and LLMs on the collected dataset using automatic metrics and find that LLMs are the best-performing MT systems in all 55 languages. These results should be confirmed using a human-based evaluation, which we leave for future work. Subjects: Computation and Language (cs.CL) Cite as: arXiv:2502.12404 [cs.CL] &nbsp; (or arXiv:2502.12404v1 [cs.CL] for this version) &nbsp; https://doi.org/10.48550/arXiv.2502.12404 Focus to learn more arXiv-issued DOI via DataCite Submission history From: Daniel Deutsch [ view email ] [v1] Tue, 18 Feb 2025 00:39:30 UTC (4,940 KB) Full-text links: Access Paper: View a PDF of the paper titled WMT24++: Expanding the Language Coverage of WMT24 to 55 Languages &amp; Dialects, by Daniel Deutsch and 16 other authors View PDF HTML (experimental) TeX Source Other Formats view license Current browse context: cs.CL &lt;&nbsp;prev &nbsp; | &nbsp; next&nbsp;&gt; new | recent | 2025-02 Change to browse by: cs References &amp; Citations NASA ADS Google Scholar Semantic Scholar export BibTeX citation Loading... BibTeX formatted citation &times; loading... Data provided by: Bookmark Bibliographic Tools Bibliographic and Citation Tools Bibliographic Explorer Toggle Bibliographic Explorer ( What is the Explorer? ) Connected Papers Toggle Connected Papers ( What is Connected Papers? ) Litmaps Toggle Litmaps ( What is Litmaps? ) scite.ai Toggle scite Smart Citations ( What are Smart Citations? ) Code, Data, Media Code, Data and Media Associated with this Article alphaXiv Toggle alphaXiv ( What is alphaXiv? ) Links to Code Toggle CatalyzeX Code Finder for Papers ( What is CatalyzeX? ) DagsHub Toggle DagsHub ( What is DagsHub? ) GotitPub Toggle Gotit.pub ( What is GotitPub? ) Huggingface Toggle Hugging Face ( What is Huggingface? ) Links to Code Toggle Papers with Code ( What is Papers with Code? ) ScienceCast Toggle ScienceCast ( What is ScienceCast? ) Demos Demos Replicate Toggle Replicate ( What is Replicate? ) Spaces Toggle Hugging Face Spaces ( What is Spaces? ) Spaces Toggle TXYZ.AI ( What is TXYZ.AI? ) Related Papers Recommenders and Search Tools Link to Influence Flower Influence Flower ( What are Influence Flowers? ) Core recommender toggle CORE Recommender ( What is CORE? ) Author Venue Institution Topic About arXivLabs arXivLabs: experimental projects with community collaborators arXivLabs is a framework that allows collaborators to develop and share new arXiv features directly on our website. Both individuals and organizations that work with arXivLabs have embraced and accepted our values of openness, community, excellence, and user data privacy. arXiv is committed to these values and only works with partners that adhere to them. Have an idea for a project that will add value for arXiv's community? Learn more about arXivLabs . Which authors of this paper are endorsers? | Disable MathJax ( What is MathJax? ) About Help contact arXiv Click here to contact arXiv Contact subscribe to arXiv mailings Click here to subscribe Subscribe Copyright Privacy Policy Web Accessibility Assistance arXiv Operational Status Get status notifications via email or slack ",
      "fetch_method": "direct-html"
    },
    {
      "arxiv_id": "https://arxiv.org/abs/2502.21228",
      "full_text": " [2502.21228] ECLeKTic: a Novel Challenge Set for Evaluation of Cross-Lingual Knowledge Transfer Skip to main content We gratefully acknowledge support from the Simons Foundation, member institutions , and all contributors. Donate &gt; cs &gt; arXiv:2502.21228 Help | Advanced Search All fields Title Author Abstract Comments Journal reference ACM classification MSC classification Report number arXiv identifier DOI ORCID arXiv author ID Help pages Full text Search open search GO open navigation menu quick links Login Help Pages About --> Computer Science > Computation and Language arXiv:2502.21228 (cs) [Submitted on 28 Feb 2025 ( v1 ), last revised 3 Mar 2025 (this version, v2)] Title: ECLeKTic: a Novel Challenge Set for Evaluation of Cross-Lingual Knowledge Transfer Authors: Omer Goldman , Uri Shaham , Dan Malkin , Sivan Eiger , Avinatan Hassidim , Yossi Matias , Joshua Maynez , Adi Mayrav Gilady , Jason Riesa , Shruti Rijhwani , Laura Rimell , Idan Szpektor , Reut Tsarfaty , Matan Eyal View a PDF of the paper titled ECLeKTic: a Novel Challenge Set for Evaluation of Cross-Lingual Knowledge Transfer, by Omer Goldman and 13 other authors View PDF HTML (experimental) Abstract: To achieve equitable performance across languages, multilingual large language models (LLMs) must be able to abstract knowledge beyond the language in which it was acquired. However, the current literature lacks reliable ways to measure LLMs&#39; capability of cross-lingual knowledge transfer. To that end, we present ECLeKTic, a multilingual closed-book QA (CBQA) dataset that Evaluates Cross-Lingual Knowledge Transfer in a simple, black-box manner. We detected information with uneven coverage across languages by controlling for presence and absence of Wikipedia articles in 12 languages. We generated knowledge-seeking questions in a source language, for which the answer appears in a relevant Wikipedia article and translated them to all other 11 languages, for which the respective Wikipedias lack equivalent articles. Assuming that Wikipedia reflects the prominent knowledge in the LLM&#39;s training data, to solve ECLeKTic&#39;s CBQA task the model is required to transfer knowledge between languages. Experimenting with 8 LLMs, we show that SOTA models struggle to effectively share knowledge across, languages even if they can predict the answer well for queries in the same language the knowledge was acquired in. Subjects: Computation and Language (cs.CL) ; Artificial Intelligence (cs.AI) Cite as: arXiv:2502.21228 [cs.CL] &nbsp; (or arXiv:2502.21228v2 [cs.CL] for this version) &nbsp; https://doi.org/10.48550/arXiv.2502.21228 Focus to learn more arXiv-issued DOI via DataCite Submission history From: Omer Goldman [ view email ] [v1] Fri, 28 Feb 2025 16:59:30 UTC (1,339 KB) [v2] Mon, 3 Mar 2025 09:11:46 UTC (1,332 KB) Full-text links: Access Paper: View a PDF of the paper titled ECLeKTic: a Novel Challenge Set for Evaluation of Cross-Lingual Knowledge Transfer, by Omer Goldman and 13 other authors View PDF HTML (experimental) TeX Source Other Formats view license Current browse context: cs.CL &lt;&nbsp;prev &nbsp; | &nbsp; next&nbsp;&gt; new | recent | 2025-02 Change to browse by: cs cs.AI References &amp; Citations NASA ADS Google Scholar Semantic Scholar export BibTeX citation Loading... BibTeX formatted citation &times; loading... Data provided by: Bookmark Bibliographic Tools Bibliographic and Citation Tools Bibliographic Explorer Toggle Bibliographic Explorer ( What is the Explorer? ) Connected Papers Toggle Connected Papers ( What is Connected Papers? ) Litmaps Toggle Litmaps ( What is Litmaps? ) scite.ai Toggle scite Smart Citations ( What are Smart Citations? ) Code, Data, Media Code, Data and Media Associated with this Article alphaXiv Toggle alphaXiv ( What is alphaXiv? ) Links to Code Toggle CatalyzeX Code Finder for Papers ( What is CatalyzeX? ) DagsHub Toggle DagsHub ( What is DagsHub? ) GotitPub Toggle Gotit.pub ( What is GotitPub? ) Huggingface Toggle Hugging Face ( What is Huggingface? ) Links to Code Toggle Papers with Code ( What is Papers with Code? ) ScienceCast Toggle ScienceCast ( What is ScienceCast? ) Demos Demos Replicate Toggle Replicate ( What is Replicate? ) Spaces Toggle Hugging Face Spaces ( What is Spaces? ) Spaces Toggle TXYZ.AI ( What is TXYZ.AI? ) Related Papers Recommenders and Search Tools Link to Influence Flower Influence Flower ( What are Influence Flowers? ) Core recommender toggle CORE Recommender ( What is CORE? ) Author Venue Institution Topic About arXivLabs arXivLabs: experimental projects with community collaborators arXivLabs is a framework that allows collaborators to develop and share new arXiv features directly on our website. Both individuals and organizations that work with arXivLabs have embraced and accepted our values of openness, community, excellence, and user data privacy. arXiv is committed to these values and only works with partners that adhere to them. Have an idea for a project that will add value for arXiv's community? Learn more about arXivLabs . Which authors of this paper are endorsers? | Disable MathJax ( What is MathJax? ) About Help contact arXiv Click here to contact arXiv Contact subscribe to arXiv mailings Click here to subscribe Subscribe Copyright Privacy Policy Web Accessibility Assistance arXiv Operational Status Get status notifications via email or slack ",
      "fetch_method": "direct-html"
    },
    {
      "arxiv_id": "https://arxiv.org/abs/2404.16816",
      "full_text": " [2404.16816] IndicGenBench: A Multilingual Benchmark to Evaluate Generation Capabilities of LLMs on Indic Languages Skip to main content We gratefully acknowledge support from the Simons Foundation, member institutions , and all contributors. Donate &gt; cs &gt; arXiv:2404.16816 Help | Advanced Search All fields Title Author Abstract Comments Journal reference ACM classification MSC classification Report number arXiv identifier DOI ORCID arXiv author ID Help pages Full text Search open search GO open navigation menu quick links Login Help Pages About --> Computer Science > Computation and Language arXiv:2404.16816 (cs) [Submitted on 25 Apr 2024 ( v1 ), last revised 7 Aug 2024 (this version, v2)] Title: IndicGenBench: A Multilingual Benchmark to Evaluate Generation Capabilities of LLMs on Indic Languages Authors: Harman Singh , Nitish Gupta , Shikhar Bharadwaj , Dinesh Tewari , Partha Talukdar View a PDF of the paper titled IndicGenBench: A Multilingual Benchmark to Evaluate Generation Capabilities of LLMs on Indic Languages, by Harman Singh and 4 other authors View PDF HTML (experimental) Abstract: As large language models (LLMs) see increasing adoption across the globe, it is imperative for LLMs to be representative of the linguistic diversity of the world. India is a linguistically diverse country of 1.4 Billion people. To facilitate research on multilingual LLM evaluation, we release IndicGenBench - the largest benchmark for evaluating LLMs on user-facing generation tasks across a diverse set 29 of Indic languages covering 13 scripts and 4 language families. IndicGenBench is composed of diverse generation tasks like cross-lingual summarization, machine translation, and cross-lingual question answering. IndicGenBench extends existing benchmarks to many Indic languages through human curation providing multi-way parallel evaluation data for many under-represented Indic languages for the first time. We evaluate a wide range of proprietary and open-source LLMs including GPT-3.5, GPT-4, PaLM-2, mT5, Gemma, BLOOM and LLaMA on IndicGenBench in a variety of settings. The largest PaLM-2 models performs the best on most tasks, however, there is a significant performance gap in all languages compared to English showing that further research is needed for the development of more inclusive multilingual language models. IndicGenBench is released at this http URL Comments: ACL 2024 Subjects: Computation and Language (cs.CL) Cite as: arXiv:2404.16816 [cs.CL] &nbsp; (or arXiv:2404.16816v2 [cs.CL] for this version) &nbsp; https://doi.org/10.48550/arXiv.2404.16816 Focus to learn more arXiv-issued DOI via DataCite Submission history From: Harman Singh [ view email ] [v1] Thu, 25 Apr 2024 17:57:36 UTC (3,171 KB) [v2] Wed, 7 Aug 2024 19:47:21 UTC (3,224 KB) Full-text links: Access Paper: View a PDF of the paper titled IndicGenBench: A Multilingual Benchmark to Evaluate Generation Capabilities of LLMs on Indic Languages, by Harman Singh and 4 other authors View PDF HTML (experimental) TeX Source Other Formats view license Current browse context: cs.CL &lt;&nbsp;prev &nbsp; | &nbsp; next&nbsp;&gt; new | recent | 2024-04 Change to browse by: cs References &amp; Citations NASA ADS Google Scholar Semantic Scholar export BibTeX citation Loading... BibTeX formatted citation &times; loading... Data provided by: Bookmark Bibliographic Tools Bibliographic and Citation Tools Bibliographic Explorer Toggle Bibliographic Explorer ( What is the Explorer? ) Connected Papers Toggle Connected Papers ( What is Connected Papers? ) Litmaps Toggle Litmaps ( What is Litmaps? ) scite.ai Toggle scite Smart Citations ( What are Smart Citations? ) Code, Data, Media Code, Data and Media Associated with this Article alphaXiv Toggle alphaXiv ( What is alphaXiv? ) Links to Code Toggle CatalyzeX Code Finder for Papers ( What is CatalyzeX? ) DagsHub Toggle DagsHub ( What is DagsHub? ) GotitPub Toggle Gotit.pub ( What is GotitPub? ) Huggingface Toggle Hugging Face ( What is Huggingface? ) Links to Code Toggle Papers with Code ( What is Papers with Code? ) ScienceCast Toggle ScienceCast ( What is ScienceCast? ) Demos Demos Replicate Toggle Replicate ( What is Replicate? ) Spaces Toggle Hugging Face Spaces ( What is Spaces? ) Spaces Toggle TXYZ.AI ( What is TXYZ.AI? ) Related Papers Recommenders and Search Tools Link to Influence Flower Influence Flower ( What are Influence Flowers? ) Core recommender toggle CORE Recommender ( What is CORE? ) Author Venue Institution Topic About arXivLabs arXivLabs: experimental projects with community collaborators arXivLabs is a framework that allows collaborators to develop and share new arXiv features directly on our website. Both individuals and organizations that work with arXivLabs have embraced and accepted our values of openness, community, excellence, and user data privacy. arXiv is committed to these values and only works with partners that adhere to them. Have an idea for a project that will add value for arXiv's community? Learn more about arXivLabs . Which authors of this paper are endorsers? | Disable MathJax ( What is MathJax? ) About Help contact arXiv Click here to contact arXiv Contact subscribe to arXiv mailings Click here to subscribe Subscribe Copyright Privacy Policy Web Accessibility Assistance arXiv Operational Status Get status notifications via email or slack ",
      "fetch_method": "direct-html"
    },
    {
      "arxiv_id": "https://arxiv.org/abs/2104.12756",
      "full_text": " [2104.12756] InfographicVQA Skip to main content We gratefully acknowledge support from the Simons Foundation, member institutions , and all contributors. Donate &gt; cs &gt; arXiv:2104.12756 Help | Advanced Search All fields Title Author Abstract Comments Journal reference ACM classification MSC classification Report number arXiv identifier DOI ORCID arXiv author ID Help pages Full text Search open search GO open navigation menu quick links Login Help Pages About --> Computer Science > Computer Vision and Pattern Recognition arXiv:2104.12756 (cs) [Submitted on 26 Apr 2021 ( v1 ), last revised 22 Aug 2021 (this version, v2)] Title: InfographicVQA Authors: Minesh Mathew , Viraj Bagal , Rubèn Pérez Tito , Dimosthenis Karatzas , Ernest Valveny , C.V Jawahar View a PDF of the paper titled InfographicVQA, by Minesh Mathew and 5 other authors View PDF Abstract: Infographics are documents designed to effectively communicate information using a combination of textual, graphical and visual elements. In this work, we explore the automatic understanding of infographic images by using Visual Question Answering this http URL this end, we present InfographicVQA, a new dataset that comprises a diverse collection of infographics along with natural language questions and answers annotations. The collected questions require methods to jointly reason over the document layout, textual content, graphical elements, and data visualizations. We curate the dataset with emphasis on questions that require elementary reasoning and basic arithmetic skills. Finally, we evaluate two strong baselines based on state of the art multi-modal VQA models, and establish baseline performance for the new task. The dataset, code and leaderboard will be made available at this http URL Subjects: Computer Vision and Pattern Recognition (cs.CV) ; Computation and Language (cs.CL) Cite as: arXiv:2104.12756 [cs.CV] &nbsp; (or arXiv:2104.12756v2 [cs.CV] for this version) &nbsp; https://doi.org/10.48550/arXiv.2104.12756 Focus to learn more arXiv-issued DOI via DataCite Submission history From: Minesh Mathew [ view email ] [v1] Mon, 26 Apr 2021 17:45:54 UTC (8,825 KB) [v2] Sun, 22 Aug 2021 04:27:56 UTC (9,222 KB) Full-text links: Access Paper: View a PDF of the paper titled InfographicVQA, by Minesh Mathew and 5 other authors View PDF TeX Source Other Formats view license Current browse context: cs.CV &lt;&nbsp;prev &nbsp; | &nbsp; next&nbsp;&gt; new | recent | 2021-04 Change to browse by: cs cs.CL References &amp; Citations NASA ADS Google Scholar Semantic Scholar DBLP - CS Bibliography listing | bibtex Minesh Mathew Dimosthenis Karatzas Ernest Valveny C. V. Jawahar a export BibTeX citation Loading... BibTeX formatted citation &times; loading... Data provided by: Bookmark Bibliographic Tools Bibliographic and Citation Tools Bibliographic Explorer Toggle Bibliographic Explorer ( What is the Explorer? ) Connected Papers Toggle Connected Papers ( What is Connected Papers? ) Litmaps Toggle Litmaps ( What is Litmaps? ) scite.ai Toggle scite Smart Citations ( What are Smart Citations? ) Code, Data, Media Code, Data and Media Associated with this Article alphaXiv Toggle alphaXiv ( What is alphaXiv? ) Links to Code Toggle CatalyzeX Code Finder for Papers ( What is CatalyzeX? ) DagsHub Toggle DagsHub ( What is DagsHub? ) GotitPub Toggle Gotit.pub ( What is GotitPub? ) Huggingface Toggle Hugging Face ( What is Huggingface? ) Links to Code Toggle Papers with Code ( What is Papers with Code? ) ScienceCast Toggle ScienceCast ( What is ScienceCast? ) Demos Demos Replicate Toggle Replicate ( What is Replicate? ) Spaces Toggle Hugging Face Spaces ( What is Spaces? ) Spaces Toggle TXYZ.AI ( What is TXYZ.AI? ) Related Papers Recommenders and Search Tools Link to Influence Flower Influence Flower ( What are Influence Flowers? ) Core recommender toggle CORE Recommender ( What is CORE? ) Author Venue Institution Topic About arXivLabs arXivLabs: experimental projects with community collaborators arXivLabs is a framework that allows collaborators to develop and share new arXiv features directly on our website. Both individuals and organizations that work with arXivLabs have embraced and accepted our values of openness, community, excellence, and user data privacy. arXiv is committed to these values and only works with partners that adhere to them. Have an idea for a project that will add value for arXiv's community? Learn more about arXivLabs . Which authors of this paper are endorsers? | Disable MathJax ( What is MathJax? ) About Help contact arXiv Click here to contact arXiv Contact subscribe to arXiv mailings Click here to subscribe Subscribe Copyright Privacy Policy Web Accessibility Assistance arXiv Operational Status Get status notifications via email or slack ",
      "fetch_method": "direct-html"
    },
    {
      "arxiv_id": "https://arxiv.org/abs/2311.16502",
      "full_text": " [2311.16502] MMMU: A Massive Multi-discipline Multimodal Understanding and Reasoning Benchmark for Expert AGI Skip to main content We gratefully acknowledge support from the Simons Foundation, member institutions , and all contributors. Donate &gt; cs &gt; arXiv:2311.16502 Help | Advanced Search All fields Title Author Abstract Comments Journal reference ACM classification MSC classification Report number arXiv identifier DOI ORCID arXiv author ID Help pages Full text Search open search GO open navigation menu quick links Login Help Pages About --> Computer Science > Computation and Language arXiv:2311.16502 (cs) [Submitted on 27 Nov 2023 ( v1 ), last revised 13 Jun 2024 (this version, v4)] Title: MMMU: A Massive Multi-discipline Multimodal Understanding and Reasoning Benchmark for Expert AGI Authors: Xiang Yue , Yuansheng Ni , Kai Zhang , Tianyu Zheng , Ruoqi Liu , Ge Zhang , Samuel Stevens , Dongfu Jiang , Weiming Ren , Yuxuan Sun , Cong Wei , Botao Yu , Ruibin Yuan , Renliang Sun , Ming Yin , Boyuan Zheng , Zhenzhu Yang , Yibo Liu , Wenhao Huang , Huan Sun , Yu Su , Wenhu Chen View a PDF of the paper titled MMMU: A Massive Multi-discipline Multimodal Understanding and Reasoning Benchmark for Expert AGI, by Xiang Yue and 21 other authors View PDF Abstract: We introduce MMMU: a new benchmark designed to evaluate multimodal models on massive multi-discipline tasks demanding college-level subject knowledge and deliberate reasoning. MMMU includes 11.5K meticulously collected multimodal questions from college exams, quizzes, and textbooks, covering six core disciplines: Art &amp; Design, Business, Science, Health &amp; Medicine, Humanities &amp; Social Science, and Tech &amp; Engineering. These questions span 30 subjects and 183 subfields, comprising 30 highly heterogeneous image types, such as charts, diagrams, maps, tables, music sheets, and chemical structures. Unlike existing benchmarks, MMMU focuses on advanced perception and reasoning with domain-specific knowledge, challenging models to perform tasks akin to those faced by experts. The evaluation of 14 open-source LMMs as well as the proprietary GPT-4V(ision) and Gemini highlights the substantial challenges posed by MMMU. Even the advanced GPT-4V and Gemini Ultra only achieve accuracies of 56% and 59% respectively, indicating significant room for improvement. We believe MMMU will stimulate the community to build next-generation multimodal foundation models towards expert artificial general intelligence. Comments: CVPR 2024 Oral Subjects: Computation and Language (cs.CL) ; Artificial Intelligence (cs.AI); Computer Vision and Pattern Recognition (cs.CV) Cite as: arXiv:2311.16502 [cs.CL] &nbsp; (or arXiv:2311.16502v4 [cs.CL] for this version) &nbsp; https://doi.org/10.48550/arXiv.2311.16502 Focus to learn more arXiv-issued DOI via DataCite Submission history From: Xiang Yue [ view email ] [v1] Mon, 27 Nov 2023 17:33:21 UTC (16,490 KB) [v2] Mon, 18 Dec 2023 03:47:39 UTC (17,526 KB) [v3] Thu, 21 Dec 2023 04:06:49 UTC (17,526 KB) [v4] Thu, 13 Jun 2024 15:02:39 UTC (17,534 KB) Full-text links: Access Paper: View a PDF of the paper titled MMMU: A Massive Multi-discipline Multimodal Understanding and Reasoning Benchmark for Expert AGI, by Xiang Yue and 21 other authors View PDF TeX Source Other Formats view license Current browse context: cs.CL &lt;&nbsp;prev &nbsp; | &nbsp; next&nbsp;&gt; new | recent | 2023-11 Change to browse by: cs cs.AI cs.CV References &amp; Citations NASA ADS Google Scholar Semantic Scholar a export BibTeX citation Loading... BibTeX formatted citation &times; loading... Data provided by: Bookmark Bibliographic Tools Bibliographic and Citation Tools Bibliographic Explorer Toggle Bibliographic Explorer ( What is the Explorer? ) Connected Papers Toggle Connected Papers ( What is Connected Papers? ) Litmaps Toggle Litmaps ( What is Litmaps? ) scite.ai Toggle scite Smart Citations ( What are Smart Citations? ) Code, Data, Media Code, Data and Media Associated with this Article alphaXiv Toggle alphaXiv ( What is alphaXiv? ) Links to Code Toggle CatalyzeX Code Finder for Papers ( What is CatalyzeX? ) DagsHub Toggle DagsHub ( What is DagsHub? ) GotitPub Toggle Gotit.pub ( What is GotitPub? ) Huggingface Toggle Hugging Face ( What is Huggingface? ) Links to Code Toggle Papers with Code ( What is Papers with Code? ) ScienceCast Toggle ScienceCast ( What is ScienceCast? ) Demos Demos Replicate Toggle Replicate ( What is Replicate? ) Spaces Toggle Hugging Face Spaces ( What is Spaces? ) Spaces Toggle TXYZ.AI ( What is TXYZ.AI? ) Related Papers Recommenders and Search Tools Link to Influence Flower Influence Flower ( What are Influence Flowers? ) Core recommender toggle CORE Recommender ( What is CORE? ) Author Venue Institution Topic About arXivLabs arXivLabs: experimental projects with community collaborators arXivLabs is a framework that allows collaborators to develop and share new arXiv features directly on our website. Both individuals and organizations that work with arXivLabs have embraced and accepted our values of openness, community, excellence, and user data privacy. arXiv is committed to these values and only works with partners that adhere to them. Have an idea for a project that will add value for arXiv's community? Learn more about arXivLabs . Which authors of this paper are endorsers? | Disable MathJax ( What is MathJax? ) About Help contact arXiv Click here to contact arXiv Contact subscribe to arXiv mailings Click here to subscribe Subscribe Copyright Privacy Policy Web Accessibility Assistance arXiv Operational Status Get status notifications via email or slack ",
      "fetch_method": "direct-html"
    },
    {
      "arxiv_id": "https://paperswithcode.com/dataset/realworldqa",
      "full_text": " Trending Papers - Hugging Face Hugging Face Models Datasets Spaces Community Docs Enterprise Pricing Log In Sign Up new Get trending papers in your email inbox once a day! Get trending papers in your email inbox! Subscribe Trending Papers by AK and the research community Daily Weekly Monthly Trending Papers Submitted by Haozhan72 SimpleVLA-RL: Scaling VLA Training via Reinforcement Learning Vision-Language-Action (VLA) models have recently emerged as a powerful paradigm for robotic manipulation. Despite substantial progress enabled by large-scale pretraining and supervised fine-tuning (SFT), these models face two fundamental challenges: (i) the scarcity and high cost of large-scale human-operated robotic trajectories required for SFT scaling, and (ii) limited generalization to tasks involving distribution shift. Recent breakthroughs in Large Reasoning Models (LRMs) demonstrate that reinforcement learning (RL) can dramatically enhance step-by-step reasoning capabilities, raising a natural question: Can RL similarly improve the long-horizon step-by-step action planning of VLA? In this work, we introduce SimpleVLA-RL, an efficient RL framework tailored for VLA models. Building upon veRL, we introduce VLA-specific trajectory sampling, scalable parallelization, multi-environment rendering, and optimized loss computation. When applied to OpenVLA-OFT, SimpleVLA-RL achieves SoTA performance on LIBERO and even outperforms pi_0 on RoboTwin 1.0\\&amp;2.0 with the exploration-enhancing strategies we introduce. SimpleVLA-RL not only reduces dependence on large-scale data and enables robust generalization, but also remarkably surpasses SFT in real-world tasks. Moreover, we identify a novel phenomenon ``pushcut'' during RL training, wherein the policy discovers previously unseen patterns beyond those seen in the previous training process. Github: https://github.com/PRIME-RL/SimpleVLA-RL 21 authors · Published on Sep 11, 2025 Upvote 48 GitHub 446 arXiv Page Submitted by Haozhan72 SimpleVLA-RL: Scaling VLA Training via Reinforcement Learning Vision-Language-Action (VLA) models have recently emerged as a powerful paradigm for robotic manipulation. Despite substantial progress enabled by large-scale pretraining and supervised fine-tuning (SFT), these models face two fundamental challenges: (i) the scarcity and high cost of large-scale human-operated robotic trajectories required for SFT scaling, and (ii) limited generalization to tasks involving distribution shift. Recent breakthroughs in Large Reasoning Models (LRMs) demonstrate that reinforcement learning (RL) can dramatically enhance step-by-step reasoning capabilities, raising a natural question: Can RL similarly improve the long-horizon step-by-step action planning of VLA? In this work, we introduce SimpleVLA-RL, an efficient RL framework tailored for VLA models. Building upon veRL, we introduce VLA-specific trajectory sampling, scalable parallelization, multi-environment rendering, and optimized loss computation. When applied to OpenVLA-OFT, SimpleVLA-RL achieves SoTA performance on LIBERO and even outperforms pi_0 on RoboTwin 1.0\\&amp;2.0 with the exploration-enhancing strategies we introduce. SimpleVLA-RL not only reduces dependence on large-scale data and enables robust generalization, but also remarkably surpasses SFT in real-world tasks. Moreover, we identify a novel phenomenon ``pushcut'' during RL training, wherein the policy discovers previously unseen patterns beyond those seen in the previous training process. Github: https://github.com/PRIME-RL/SimpleVLA-RL 21 authors · Sep 11, 2025 Upvote 48 GitHub 446 arXiv Page Submitted by taesiri AgentScope 1.0: A Developer-Centric Framework for Building Agentic Applications Driven by rapid advancements of Large Language Models (LLMs), agents are empowered to combine intrinsic knowledge with dynamic tool use, greatly enhancing their capacity to address real-world tasks. In line with such an evolution, AgentScope introduces major improvements in a new version (1.0), towards comprehensively supporting flexible and efficient tool-based agent-environment interactions for building agentic applications. Specifically, we abstract foundational components essential for agentic applications and provide unified interfaces and extensible modules, enabling developers to easily leverage the latest progress, such as new models and MCPs. Furthermore, we ground agent behaviors in the ReAct paradigm and offer advanced agent-level infrastructure based on a systematic asynchronous design, which enriches both human-agent and agent-agent interaction patterns while improving execution efficiency. Building on this foundation, we integrate several built-in agents tailored to specific practical scenarios. AgentScope also includes robust engineering support for developer-friendly experiences. We provide a scalable evaluation module with a visual studio interface, making the development of long-trajectory agentic applications more manageable and easier to trace. In addition, AgentScope offers a runtime sandbox to ensure safe agent execution and facilitates rapid deployment in production environments. With these enhancements, AgentScope provides a practical foundation for building scalable, adaptive, and effective agentic applications. 23 authors · Published on Aug 22, 2025 Upvote 37 GitHub 11.1k arXiv Page Submitted by taesiri AgentScope 1.0: A Developer-Centric Framework for Building Agentic Applications Driven by rapid advancements of Large Language Models (LLMs), agents are empowered to combine intrinsic knowledge with dynamic tool use, greatly enhancing their capacity to address real-world tasks. In line with such an evolution, AgentScope introduces major improvements in a new version (1.0), towards comprehensively supporting flexible and efficient tool-based agent-environment interactions for building agentic applications. Specifically, we abstract foundational components essential for agentic applications and provide unified interfaces and extensible modules, enabling developers to easily leverage the latest progress, such as new models and MCPs. Furthermore, we ground agent behaviors in the ReAct paradigm and offer advanced agent-level infrastructure based on a systematic asynchronous design, which enriches both human-agent and agent-agent interaction patterns while improving execution efficiency. Building on this foundation, we integrate several built-in agents tailored to specific practical scenarios. AgentScope also includes robust engineering support for developer-friendly experiences. We provide a scalable evaluation module with a visual studio interface, making the development of long-trajectory agentic applications more manageable and easier to trace. In addition, AgentScope offers a runtime sandbox to ensure safe agent execution and facilitates rapid deployment in production environments. With these enhancements, AgentScope provides a practical foundation for building scalable, adaptive, and effective agentic applications. 23 authors · Aug 22, 2025 Upvote 37 GitHub 11.1k arXiv Page Submitted by taesiri AgentGym-RL: Training LLM Agents for Long-Horizon Decision Making through Multi-Turn Reinforcement Learning Developing autonomous LLM agents capable of making a series of intelligent decisions to solve complex, real-world tasks is a fast-evolving frontier. Like human cognitive development, agents are expected to acquire knowledge and skills through exploration and interaction with the environment. Despite advances, the community still lacks a unified, interactive reinforcement learning (RL) framework that can effectively train such agents from scratch -- without relying on supervised fine-tuning (SFT) -- across diverse and realistic environments. To bridge this gap, we introduce AgentGym-RL, a new framework to train LLM agents for multi-turn interactive decision-making through RL. The framework features a modular and decoupled architecture, ensuring high flexibility and extensibility. It encompasses a wide variety of real-world scenarios, and supports mainstream RL algorithms. Furthermore, we propose ScalingInter-RL, a training approach designed for exploration-exploitation balance and stable RL optimization. In early stages, it emphasizes exploitation by restricting the number of interactions, and gradually shifts towards exploration with larger horizons to encourage diverse problem-solving strategies. In this way, the agent develops more diverse behaviors and is less prone to collapse under long horizons. We perform extensive experiments to validate the stability and effectiveness of both the AgentGym-RL framework and the ScalingInter-RL approach. Our agents match or surpass commercial models on 27 tasks across diverse environments. We offer key insights and will open-source the complete AgentGym-RL framework -- including code and datasets -- to empower the research community in developing the next generation of intelligent agents. 23 authors · Published on Sep 10, 2025 Upvote 23 GitHub 141 arXiv Page Submitted by taesiri AgentGym-RL: Training LLM Agents for Long-Horizon Decision Making through Multi-Turn Reinforcement Learning Developing autonomous LLM agents capable of making a series of intelligent decisions to solve complex, real-world tasks is a fast-evolving frontier. Like human cognitive development, agents are expected to acquire knowledge and skills through exploration and interaction with the environment. Despite advances, the community still lacks a unified, interactive reinforcement learning (RL) framework that can effectively train such agents from scratch -- without relying on supervised fine-tuning (SFT) -- across diverse and realistic environments. To bridge this gap, we introduce AgentGym-RL, a new framework to train LLM agents for multi-turn interactive decision-making through RL. The framework features a modular and decoupled architecture, ensuring high flexibility and extensibility. It encompasses a wide variety of real-world scenarios, and supports mainstream RL algorithms. Furthermore, we propose ScalingInter-RL, a training approach designed for exploration-exploitation balance and stable RL optimization. In early stages, it emphasizes exploitation by restricting the number of interactions, and gradually shifts towards exploration with larger horizons to encourage diverse problem-solving strategies. In this way, the agent develops more diverse behaviors and is less prone to collapse under long horizons. We perform extensive experiments to validate the stability and effectiveness of both the AgentGym-RL framework and the ScalingInter-RL approach. Our agents match or surpass commercial models on 27 tasks across diverse environments. We offer key insights and will open-source the complete AgentGym-RL framework -- including code and datasets -- to empower the research community in developing the next generation of intelligent agents. 23 authors · Sep 10, 2025 Upvote 23 GitHub 141 arXiv Page Submitted by taesiri FLUX-Reason-6M &amp; PRISM-Bench: A Million-Scale Text-to-Image Reasoning Dataset and Comprehensive Benchmark The advancement of open-source text-to-image (T2I) models has been hindered by the absence of large-scale, reasoning-focused datasets and comprehensive evaluation benchmarks, resulting in a performance gap compared to leading closed-source systems. To address this challenge, We introduce FLUX-Reason-6M and PRISM-Bench (Precise and Robust Image Synthesis Measurement Benchmark). FLUX-Reason-6M is a massive dataset consisting of 6 million high-quality FLUX-generated images and 20 million bilingual (English and Chinese) descriptions specifically designed to teach complex reasoning. The image are organized according to six key characteristics: Imagination, Entity, Text rendering, Style, Affection, and Composition, and design explicit Generation Chain-of-Thought (GCoT) to provide detailed breakdowns of image generation steps. The whole data curation takes 15,000 A100 GPU days, providing the community with a resource previously unattainable outside of large industrial labs. PRISM-Bench offers a novel evaluation standard with seven distinct tracks, including a formidable Long Text challenge using GCoT. Through carefully designed prompts, it utilizes advanced vision-language models for nuanced human-aligned assessment of prompt-image alignment and image aesthetics. Our extensive evaluation of 19 leading models on PRISM-Bench reveals critical performance gaps and highlights specific areas requiring improvement. Our dataset, benchmark, and evaluation code are released to catalyze the next wave of reasoning-oriented T2I generation. Project page: https://flux-reason-6m.github.io/ . 10 authors · Published on Sep 11, 2025 Upvote 22 GitHub 24 arXiv Page Submitted by taesiri FLUX-Reason-6M &amp; PRISM-Bench: A Million-Scale Text-to-Image Reasoning Dataset and Comprehensive Benchmark The advancement of open-source text-to-image (T2I) models has been hindered by the absence of large-scale, reasoning-focused datasets and comprehensive evaluation benchmarks, resulting in a performance gap compared to leading closed-source systems. To address this challenge, We introduce FLUX-Reason-6M and PRISM-Bench (Precise and Robust Image Synthesis Measurement Benchmark). FLUX-Reason-6M is a massive dataset consisting of 6 million high-quality FLUX-generated images and 20 million bilingual (English and Chinese) descriptions specifically designed to teach complex reasoning. The image are organized according to six key characteristics: Imagination, Entity, Text rendering, Style, Affection, and Composition, and design explicit Generation Chain-of-Thought (GCoT) to provide detailed breakdowns of image generation steps. The whole data curation takes 15,000 A100 GPU days, providing the community with a resource previously unattainable outside of large industrial labs. PRISM-Bench offers a novel evaluation standard with seven distinct tracks, including a formidable Long Text challenge using GCoT. Through carefully designed prompts, it utilizes advanced vision-language models for nuanced human-aligned assessment of prompt-image alignment and image aesthetics. Our extensive evaluation of 19 leading models on PRISM-Bench reveals critical performance gaps and highlights specific areas requiring improvement. Our dataset, benchmark, and evaluation code are released to catalyze the next wave of reasoning-oriented T2I generation. Project page: https://flux-reason-6m.github.io/ . 10 authors · Sep 11, 2025 Upvote 22 GitHub 24 arXiv Page Submitted by iseesaw A Survey of Reinforcement Learning for Large Reasoning Models In this paper, we survey recent advances in Reinforcement Learning (RL) for reasoning with Large Language Models (LLMs). RL has achieved remarkable success in advancing the frontier of LLM capabilities, particularly in addressing complex logical tasks such as mathematics and coding. As a result, RL has emerged as a foundational methodology for transforming LLMs into LRMs. With the rapid progress of the field, further scaling of RL for LRMs now faces foundational challenges not only in computational resources but also in algorithm design, training data, and infrastructure. To this end, it is timely to revisit the development of this domain, reassess its trajectory, and explore strategies to enhance the scalability of RL toward Artificial SuperIntelligence (ASI). In particular, we examine research applying RL to LLMs and LRMs for reasoning abilities, especially since the release of DeepSeek-R1, including foundational components, core problems, training resources, and downstream applications, to identify future opportunities and directions for this rapidly evolving area. We hope this review will promote future research on RL for broader reasoning models. Github: https://github.com/TsinghuaC3I/Awesome-RL-for-LRMs 39 authors · Published on Sep 10, 2025 Upvote 114 GitHub 941 arXiv Page Submitted by iseesaw A Survey of Reinforcement Learning for Large Reasoning Models In this paper, we survey recent advances in Reinforcement Learning (RL) for reasoning with Large Language Models (LLMs). RL has achieved remarkable success in advancing the frontier of LLM capabilities, particularly in addressing complex logical tasks such as mathematics and coding. As a result, RL has emerged as a foundational methodology for transforming LLMs into LRMs. With the rapid progress of the field, further scaling of RL for LRMs now faces foundational challenges not only in computational resources but also in algorithm design, training data, and infrastructure. To this end, it is timely to revisit the development of this domain, reassess its trajectory, and explore strategies to enhance the scalability of RL toward Artificial SuperIntelligence (ASI). In particular, we examine research applying RL to LLMs and LRMs for reasoning abilities, especially since the release of DeepSeek-R1, including foundational components, core problems, training resources, and downstream applications, to identify future opportunities and directions for this rapidly evolving area. We hope this review will promote future research on RL for broader reasoning models. Github: https://github.com/TsinghuaC3I/Awesome-RL-for-LRMs 39 authors · Sep 10, 2025 Upvote 114 GitHub 941 arXiv Page Submitted by taesiri 3D and 4D World Modeling: A Survey World modeling has become a cornerstone in AI research, enabling agents to understand, represent, and predict the dynamic environments they inhabit. While prior work largely emphasizes generative methods for 2D image and video data, they overlook the rapidly growing body of work that leverages native 3D and 4D representations such as RGB-D imagery, occupancy grids, and LiDAR point clouds for large-scale scene modeling. At the same time, the absence of a standardized definition and taxonomy for ``world models'' has led to fragmented and sometimes inconsistent claims in the literature. This survey addresses these gaps by presenting the first comprehensive review explicitly dedicated to 3D and 4D world modeling and generation. We establish precise definitions, introduce a structured taxonomy spanning video-based (VideoGen), occupancy-based (OccGen), and LiDAR-based (LiDARGen) approaches, and systematically summarize datasets and evaluation metrics tailored to 3D/4D settings. We further discuss practical applications, identify open challenges, and highlight promising research directions, aiming to provide a coherent and foundational reference for advancing the field. A systematic summary of existing literature is available at https://github.com/worldbench/survey 23 authors · Published on Sep 4, 2025 Upvote 43 GitHub 234 arXiv Page Submitted by taesiri 3D and 4D World Modeling: A Survey World modeling has become a cornerstone in AI research, enabling agents to understand, represent, and predict the dynamic environments they inhabit. While prior work largely emphasizes generative methods for 2D image and video data, they overlook the rapidly growing body of work that leverages native 3D and 4D representations such as RGB-D imagery, occupancy grids, and LiDAR point clouds for large-scale scene modeling. At the same time, the absence of a standardized definition and taxonomy for ``world models'' has led to fragmented and sometimes inconsistent claims in the literature. This survey addresses these gaps by presenting the first comprehensive review explicitly dedicated to 3D and 4D world modeling and generation. We establish precise definitions, introduce a structured taxonomy spanning video-based (VideoGen), occupancy-based (OccGen), and LiDAR-based (LiDARGen) approaches, and systematically summarize datasets and evaluation metrics tailored to 3D/4D settings. We further discuss practical applications, identify open challenges, and highlight promising research directions, aiming to provide a coherent and foundational reference for advancing the field. A systematic summary of existing literature is available at https://github.com/worldbench/survey 23 authors · Sep 4, 2025 Upvote 43 GitHub 234 arXiv Page Submitted by akhaliq LlamaFactory: Unified Efficient Fine-Tuning of 100+ Language Models Efficient fine-tuning is vital for adapting large language models (LLMs) to downstream tasks. However, it requires non-trivial efforts to implement these methods on different models. We present LlamaFactory, a unified framework that integrates a suite of cutting-edge efficient training methods. It allows users to flexibly customize the fine-tuning of 100+ LLMs without the need for coding through the built-in web UI LlamaBoard. We empirically validate the efficiency and effectiveness of our framework on language modeling and text generation tasks. It has been released at https://github.com/hiyouga/LLaMA-Factory and already received over 13,000 stars and 1,600 forks. 5 authors · Published on Mar 20, 2024 Upvote 138 GitHub 57.9k arXiv Page Submitted by akhaliq LlamaFactory: Unified Efficient Fine-Tuning of 100+ Language Models Efficient fine-tuning is vital for adapting large language models (LLMs) to downstream tasks. However, it requires non-trivial efforts to implement these methods on different models. We present LlamaFactory, a unified framework that integrates a suite of cutting-edge efficient training methods. It allows users to flexibly customize the fine-tuning of 100+ LLMs without the need for coding through the built-in web UI LlamaBoard. We empirically validate the efficiency and effectiveness of our framework on language modeling and text generation tasks. It has been released at https://github.com/hiyouga/LLaMA-Factory and already received over 13,000 stars and 1,600 forks. 5 authors · Mar 20, 2024 Upvote 138 GitHub 57.9k arXiv Page Submitted by imryanxu ComfyUI-Copilot: An Intelligent Assistant for Automated Workflow Development We introduce ComfyUI-Copilot, a large language model-powered plugin designed to enhance the usability and efficiency of ComfyUI, an open-source platform for AI-driven art creation. Despite its flexibility and user-friendly interface, ComfyUI can present challenges to newcomers, including limited documentation, model misconfigurations, and the complexity of workflow design. ComfyUI-Copilot addresses these challenges by offering intelligent node and model recommendations, along with automated one-click workflow construction. At its core, the system employs a hierarchical multi-agent framework comprising a central assistant agent for task delegation and specialized worker agents for different usages, supported by our curated ComfyUI knowledge bases to streamline debugging and deployment. We validate the effectiveness of ComfyUI-Copilot through both offline quantitative evaluations and online user feedback, showing that it accurately recommends nodes and accelerates workflow development. Additionally, use cases illustrate that ComfyUI-Copilot lowers entry barriers for beginners and enhances workflow efficiency for experienced users. The ComfyUI-Copilot installation package and a demo video are available at https://github.com/AIDC-AI/ComfyUI-Copilot. 10 authors · Published on Jun 5, 2025 Upvote 78 GitHub 2.8k arXiv Page Submitted by imryanxu ComfyUI-Copilot: An Intelligent Assistant for Automated Workflow Development We introduce ComfyUI-Copilot, a large language model-powered plugin designed to enhance the usability and efficiency of ComfyUI, an open-source platform for AI-driven art creation. Despite its flexibility and user-friendly interface, ComfyUI can present challenges to newcomers, including limited documentation, model misconfigurations, and the complexity of workflow design. ComfyUI-Copilot addresses these challenges by offering intelligent node and model recommendations, along with automated one-click workflow construction. At its core, the system employs a hierarchical multi-agent framework comprising a central assistant agent for task delegation and specialized worker agents for different usages, supported by our curated ComfyUI knowledge bases to streamline debugging and deployment. We validate the effectiveness of ComfyUI-Copilot through both offline quantitative evaluations and online user feedback, showing that it accurately recommends nodes and accelerates workflow development. Additionally, use cases illustrate that ComfyUI-Copilot lowers entry barriers for beginners and enhances workflow efficiency for experienced users. The ComfyUI-Copilot installation package and a demo video are available at https://github.com/AIDC-AI/ComfyUI-Copilot. 10 authors · Jun 5, 2025 Upvote 78 GitHub 2.8k arXiv Page Submitted by taesiri Mini-o3: Scaling Up Reasoning Patterns and Interaction Turns for Visual Search Recent advances in large multimodal models have leveraged image-based tools with reinforcement learning to tackle visual problems. However, existing open-source approaches often exhibit monotonous reasoning patterns and allow only a limited number of interaction turns, making them inadequate for difficult tasks that require trial-and-error exploration. In this work, we address this limitation by scaling up tool-based interactions and introduce Mini-o3, a system that executes deep, multi-turn reasoning -- spanning tens of steps -- and achieves state-of-the-art performance on challenging visual search tasks. Our recipe for reproducing OpenAI o3-style behaviors comprises three key components. First, we construct the Visual Probe Dataset, a collection of thousands of challenging visual search problems designed for exploratory reasoning. Second, we develop an iterative data collection pipeline to obtain cold-start trajectories that exhibit diverse reasoning patterns, including depth-first search, trial-and-error, and goal maintenance. Third, we propose an over-turn masking strategy that prevents penalization of over-turn responses (those that hit the maximum number of turns) during reinforcement learning, thereby balancing training-time efficiency with test-time scalability. Despite training with an upper bound of only six interaction turns, our model generates trajectories that naturally scale to tens of turns at inference time, with accuracy improving as the number of turns increases. Extensive experiments demonstrate that Mini-o3 produces rich reasoning patterns and deep thinking paths, effectively solving challenging visual search problems. 6 authors · Published on Sep 9, 2025 Upvote 50 GitHub 193 arXiv Page Submitted by taesiri Mini-o3: Scaling Up Reasoning Patterns and Interaction Turns for Visual Search Recent advances in large multimodal models have leveraged image-based tools with reinforcement learning to tackle visual problems. However, existing open-source approaches often exhibit monotonous reasoning patterns and allow only a limited number of interaction turns, making them inadequate for difficult tasks that require trial-and-error exploration. In this work, we address this limitation by scaling up tool-based interactions and introduce Mini-o3, a system that executes deep, multi-turn reasoning -- spanning tens of steps -- and achieves state-of-the-art performance on challenging visual search tasks. Our recipe for reproducing OpenAI o3-style behaviors comprises three key components. First, we construct the Visual Probe Dataset, a collection of thousands of challenging visual search problems designed for exploratory reasoning. Second, we develop an iterative data collection pipeline to obtain cold-start trajectories that exhibit diverse reasoning patterns, including depth-first search, trial-and-error, and goal maintenance. Third, we propose an over-turn masking strategy that prevents penalization of over-turn responses (those that hit the maximum number of turns) during reinforcement learning, thereby balancing training-time efficiency with test-time scalability. Despite training with an upper bound of only six interaction turns, our model generates trajectories that naturally scale to tens of turns at inference time, with accuracy improving as the number of turns increases. Extensive experiments demonstrate that Mini-o3 produces rich reasoning patterns and deep thinking paths, effectively solving challenging visual search problems. 6 authors · Sep 9, 2025 Upvote 50 GitHub 193 arXiv Page Submitted by simingfu TempFlow-GRPO: When Timing Matters for GRPO in Flow Models Recent flow matching models for text-to-image generation have achieved remarkable quality, yet their integration with reinforcement learning for human preference alignment remains suboptimal, hindering fine-grained reward-based optimization. We observe that the key impediment to effective GRPO training of flow models is the temporal uniformity assumption in existing approaches: sparse terminal rewards with uniform credit assignment fail to capture the varying criticality of decisions across generation timesteps, resulting in inefficient exploration and suboptimal convergence. To remedy this shortcoming, we introduce TempFlow-GRPO (Temporal Flow GRPO), a principled GRPO framework that captures and exploits the temporal structure inherent in flow-based generation. TempFlow-GRPO introduces two key innovations: (i) a trajectory branching mechanism that provides process rewards by concentrating stochasticity at designated branching points, enabling precise credit assignment without requiring specialized intermediate reward models; and (ii) a noise-aware weighting scheme that modulates policy optimization according to the intrinsic exploration potential of each timestep, prioritizing learning during high-impact early stages while ensuring stable refinement in later phases. These innovations endow the model with temporally-aware optimization that respects the underlying generative dynamics, leading to state-of-the-art performance in human preference alignment and standard text-to-image benchmarks. 8 authors · Published on Aug 6, 2025 Upvote 12 GitHub 654 arXiv Page Submitted by simingfu TempFlow-GRPO: When Timing Matters for GRPO in Flow Models Recent flow matching models for text-to-image generation have achieved remarkable quality, yet their integration with reinforcement learning for human preference alignment remains suboptimal, hindering fine-grained reward-based optimization. We observe that the key impediment to effective GRPO training of flow models is the temporal uniformity assumption in existing approaches: sparse terminal rewards with uniform credit assignment fail to capture the varying criticality of decisions across generation timesteps, resulting in inefficient exploration and suboptimal convergence. To remedy this shortcoming, we introduce TempFlow-GRPO (Temporal Flow GRPO), a principled GRPO framework that captures and exploits the temporal structure inherent in flow-based generation. TempFlow-GRPO introduces two key innovations: (i) a trajectory branching mechanism that provides process rewards by concentrating stochasticity at designated branching points, enabling precise credit assignment without requiring specialized intermediate reward models; and (ii) a noise-aware weighting scheme that modulates policy optimization according to the intrinsic exploration potential of each timestep, prioritizing learning during high-impact early stages while ensuring stable refinement in later phases. These innovations endow the model with temporally-aware optimization that respects the underlying generative dynamics, leading to state-of-the-art performance in human preference alignment and standard text-to-image benchmarks. 8 authors · Aug 6, 2025 Upvote 12 GitHub 654 arXiv Page Submitted by sanaka87 Reconstruction Alignment Improves Unified Multimodal Models Unified multimodal models (UMMs) unify visual understanding and generation within a single architecture. However, conventional training relies on image-text pairs (or sequences) whose captions are typically sparse and miss fine-grained visual details--even when they use hundreds of words to describe a simple image. We introduce Reconstruction Alignment (RecA), a resource-efficient post-training method that leverages visual understanding encoder embeddings as dense \"text prompts,\" providing rich supervision without captions. Concretely, RecA conditions a UMM on its own visual understanding embeddings and optimizes it to reconstruct the input image with a self-supervised reconstruction loss, thereby realigning understanding and generation. Despite its simplicity, RecA is broadly applicable: across autoregressive, masked-autoregressive, and diffusion-based UMMs, it consistently improves generation and editing fidelity. With only 27 GPU-hours, post-training with RecA substantially improves image generation performance on GenEval (0.73rightarrow0.90) and DPGBench (80.93rightarrow88.15), while also boosting editing benchmarks (ImgEdit 3.38rightarrow3.75, GEdit 6.94rightarrow7.25). Notably, RecA surpasses much larger open-source models and applies broadly across diverse UMM architectures, establishing it as an efficient and general post-training alignment strategy for UMMs 4 authors · Published on Sep 8, 2025 Upvote 36 GitHub 107 arXiv Page Submitted by sanaka87 Reconstruction Alignment Improves Unified Multimodal Models Unified multimodal models (UMMs) unify visual understanding and generation within a single architecture. However, conventional training relies on image-text pairs (or sequences) whose captions are typically sparse and miss fine-grained visual details--even when they use hundreds of words to describe a simple image. We introduce Reconstruction Alignment (RecA), a resource-efficient post-training method that leverages visual understanding encoder embeddings as dense \"text prompts,\" providing rich supervision without captions. Concretely, RecA conditions a UMM on its own visual understanding embeddings and optimizes it to reconstruct the input image with a self-supervised reconstruction loss, thereby realigning understanding and generation. Despite its simplicity, RecA is broadly applicable: across autoregressive, masked-autoregressive, and diffusion-based UMMs, it consistently improves generation and editing fidelity. With only 27 GPU-hours, post-training with RecA substantially improves image generation performance on GenEval (0.73rightarrow0.90) and DPGBench (80.93rightarrow88.15), while also boosting editing benchmarks (ImgEdit 3.38rightarrow3.75, GEdit 6.94rightarrow7.25). Notably, RecA surpasses much larger open-source models and applies broadly across diverse UMM architectures, establishing it as an efficient and general post-training alignment strategy for UMMs 4 authors · Sep 8, 2025 Upvote 36 GitHub 107 arXiv Page Submitted by nielsr DINOv3 Self-supervised learning holds the promise of eliminating the need for manual data annotation, enabling models to scale effortlessly to massive datasets and larger architectures. By not being tailored to specific tasks or domains, this training paradigm has the potential to learn visual representations from diverse sources, ranging from natural to aerial images -- using a single algorithm. This technical report introduces DINOv3, a major milestone toward realizing this vision by leveraging simple yet effective strategies. First, we leverage the benefit of scaling both dataset and model size by careful data preparation, design, and optimization. Second, we introduce a new method called Gram anchoring, which effectively addresses the known yet unsolved issue of dense feature maps degrading during long training schedules. Finally, we apply post-hoc strategies that further enhance our models' flexibility with respect to resolution, model size, and alignment with text. As a result, we present a versatile vision foundation model that outperforms the specialized state of the art across a broad range of settings, without fine-tuning. DINOv3 produces high-quality dense features that achieve outstanding performance on various vision tasks, significantly surpassing previous self- and weakly-supervised foundation models. We also share the DINOv3 suite of vision models, designed to advance the state of the art on a wide spectrum of tasks and data by providing scalable solutions for diverse resource constraints and deployment scenarios. 26 authors · Published on Aug 13, 2025 Upvote 248 GitHub 6.84k arXiv Page Submitted by nielsr DINOv3 Self-supervised learning holds the promise of eliminating the need for manual data annotation, enabling models to scale effortlessly to massive datasets and larger architectures. By not being tailored to specific tasks or domains, this training paradigm has the potential to learn visual representations from diverse sources, ranging from natural to aerial images -- using a single algorithm. This technical report introduces DINOv3, a major milestone toward realizing this vision by leveraging simple yet effective strategies. First, we leverage the benefit of scaling both dataset and model size by careful data preparation, design, and optimization. Second, we introduce a new method called Gram anchoring, which effectively addresses the known yet unsolved issue of dense feature maps degrading during long training schedules. Finally, we apply post-hoc strategies that further enhance our models' flexibility with respect to resolution, model size, and alignment with text. As a result, we present a versatile vision foundation model that outperforms the specialized state of the art across a broad range of settings, without fine-tuning. DINOv3 produces high-quality dense features that achieve outstanding performance on various vision tasks, significantly surpassing previous self- and weakly-supervised foundation models. We also share the DINOv3 suite of vision models, designed to advance the state of the art on a wide spectrum of tasks and data by providing scalable solutions for diverse resource constraints and deployment scenarios. 26 authors · Aug 13, 2025 Upvote 248 GitHub 6.84k arXiv Page Submitted by xhyandwyy Mobile-Agent-v3: Foundamental Agents for GUI Automation This paper introduces GUI-Owl, a foundational GUI agent model that achieves state-of-the-art performance among open-source end-to-end models on ten GUI benchmarks across desktop and mobile environments, covering grounding, question answering, planning, decision-making, and procedural knowledge. GUI-Owl-7B achieves 66.4 on AndroidWorld and 29.4 on OSWorld. Building on this, we propose Mobile-Agent-v3, a general-purpose GUI agent framework that further improves performance to 73.3 on AndroidWorld and 37.7 on OSWorld, setting a new state-of-the-art for open-source GUI agent frameworks. GUI-Owl incorporates three key innovations: (1) Large-scale Environment Infrastructure: a cloud-based virtual environment spanning Android, Ubuntu, macOS, and Windows, enabling our Self-Evolving GUI Trajectory Production framework. This generates high-quality interaction data via automated query generation and correctness validation, leveraging GUI-Owl to refine trajectories iteratively, forming a self-improving loop. It supports diverse data pipelines and reduces manual annotation. (2) Diverse Foundational Agent Capabilities: by integrating UI grounding, planning, action semantics, and reasoning patterns, GUI-Owl supports end-to-end decision-making and can act as a modular component in multi-agent systems. (3) Scalable Environment RL: we develop a scalable reinforcement learning framework with fully asynchronous training for real-world alignment. We also introduce Trajectory-aware Relative Policy Optimization (TRPO) for online RL, achieving 34.9 on OSWorld. GUI-Owl and Mobile-Agent-v3 are open-sourced at https://github.com/X-PLUG/MobileAgent. 15 authors · Published on Aug 21, 2025 Upvote 61 GitHub 5.59k arXiv Page Submitted by xhyandwyy Mobile-Agent-v3: Foundamental Agents for GUI Automation This paper introduces GUI-Owl, a foundational GUI agent model that achieves state-of-the-art performance among open-source end-to-end models on ten GUI benchmarks across desktop and mobile environments, covering grounding, question answering, planning, decision-making, and procedural knowledge. GUI-Owl-7B achieves 66.4 on AndroidWorld and 29.4 on OSWorld. Building on this, we propose Mobile-Agent-v3, a general-purpose GUI agent framework that further improves performance to 73.3 on AndroidWorld and 37.7 on OSWorld, setting a new state-of-the-art for open-source GUI agent frameworks. GUI-Owl incorporates three key innovations: (1) Large-scale Environment Infrastructure: a cloud-based virtual environment spanning Android, Ubuntu, macOS, and Windows, enabling our Self-Evolving GUI Trajectory Production framework. This generates high-quality interaction data via automated query generation and correctness validation, leveraging GUI-Owl to refine trajectories iteratively, forming a self-improving loop. It supports diverse data pipelines and reduces manual annotation. (2) Diverse Foundational Agent Capabilities: by integrating UI grounding, planning, action semantics, and reasoning patterns, GUI-Owl supports end-to-end decision-making and can act as a modular component in multi-agent systems. (3) Scalable Environment RL: we develop a scalable reinforcement learning framework with fully asynchronous training for real-world alignment. We also introduce Trajectory-aware Relative Policy Optimization (TRPO) for online RL, achieving 34.9 on OSWorld. GUI-Owl and Mobile-Agent-v3 are open-sourced at https://github.com/X-PLUG/MobileAgent. 15 authors · Aug 21, 2025 Upvote 61 GitHub 5.59k arXiv Page Submitted by xhyandwyy PC-Agent: A Hierarchical Multi-Agent Collaboration Framework for Complex Task Automation on PC In the field of MLLM-based GUI agents, compared to smartphones, the PC scenario not only features a more complex interactive environment, but also involves more intricate intra- and inter-app workflows. To address these issues, we propose a hierarchical agent framework named PC-Agent. Specifically, from the perception perspective, we devise an Active Perception Module (APM) to overcome the inadequate abilities of current MLLMs in perceiving screenshot content. From the decision-making perspective, to handle complex user instructions and interdependent subtasks more effectively, we propose a hierarchical multi-agent collaboration architecture that decomposes decision-making processes into Instruction-Subtask-Action levels. Within this architecture, three agents (i.e., Manager, Progress and Decision) are set up for instruction decomposition, progress tracking and step-by-step decision-making respectively. Additionally, a Reflection agent is adopted to enable timely bottom-up error feedback and adjustment. We also introduce a new benchmark PC-Eval with 25 real-world complex instructions. Empirical results on PC-Eval show that our PC-Agent achieves a 32% absolute improvement of task success rate over previous state-of-the-art methods. The code will be publicly available. 11 authors · Published on Feb 20, 2025 Upvote 28 GitHub 5.59k arXiv Page Submitted by xhyandwyy PC-Agent: A Hierarchical Multi-Agent Collaboration Framework for Complex Task Automation on PC In the field of MLLM-based GUI agents, compared to smartphones, the PC scenario not only features a more complex interactive environment, but also involves more intricate intra- and inter-app workflows. To address these issues, we propose a hierarchical agent framework named PC-Agent. Specifically, from the perception perspective, we devise an Active Perception Module (APM) to overcome the inadequate abilities of current MLLMs in perceiving screenshot content. From the decision-making perspective, to handle complex user instructions and interdependent subtasks more effectively, we propose a hierarchical multi-agent collaboration architecture that decomposes decision-making processes into Instruction-Subtask-Action levels. Within this architecture, three agents (i.e., Manager, Progress and Decision) are set up for instruction decomposition, progress tracking and step-by-step decision-making respectively. Additionally, a Reflection agent is adopted to enable timely bottom-up error feedback and adjustment. We also introduce a new benchmark PC-Eval with 25 real-world complex instructions. Empirical results on PC-Eval show that our PC-Agent achieves a 32% absolute improvement of task success rate over previous state-of-the-art methods. The code will be publicly available. 11 authors · Feb 20, 2025 Upvote 28 GitHub 5.59k arXiv Page Submitted by xhyandwyy Look Before You Leap: A GUI-Critic-R1 Model for Pre-Operative Error Diagnosis in GUI Automation In recent years, Multimodal Large Language Models (MLLMs) have been extensively utilized for multimodal reasoning tasks, including Graphical User Interface (GUI) automation. Unlike general offline multimodal tasks, GUI automation is executed in online interactive environments, necessitating step-by-step decision-making based on real-time status of the environment. This task has a lower tolerance for decision-making errors at each step, as any mistakes may cumulatively disrupt the process and potentially lead to irreversible outcomes like deletions or payments. To address these issues, we introduce a pre-operative critic mechanism that provides effective feedback prior to the actual execution, by reasoning about the potential outcome and correctness of actions. Specifically, we propose a Suggestion-aware Gradient Relative Policy Optimization (S-GRPO) strategy to construct our pre-operative critic model GUI-Critic-R1, incorporating a novel suggestion reward to enhance the reliability of the model's feedback. Furthermore, we develop a reasoning-bootstrapping based data collection pipeline to create a GUI-Critic-Train and a GUI-Critic-Test, filling existing gaps in GUI critic data. Static experiments on the GUI-Critic-Test across both mobile and web domains reveal that our GUI-Critic-R1 offers significant advantages in critic accuracy compared to current MLLMs. Dynamic evaluation on GUI automation benchmark further highlights the effectiveness and superiority of our model, as evidenced by improved success rates and operational efficiency. 12 authors · Published on Jun 5, 2025 Upvote 18 GitHub 5.59k arXiv Page Submitted by xhyandwyy Look Before You Leap: A GUI-Critic-R1 Model for Pre-Operative Error Diagnosis in GUI Automation In recent years, Multimodal Large Language Models (MLLMs) have been extensively utilized for multimodal reasoning tasks, including Graphical User Interface (GUI) automation. Unlike general offline multimodal tasks, GUI automation is executed in online interactive environments, necessitating step-by-step decision-making based on real-time status of the environment. This task has a lower tolerance for decision-making errors at each step, as any mistakes may cumulatively disrupt the process and potentially lead to irreversible outcomes like deletions or payments. To address these issues, we introduce a pre-operative critic mechanism that provides effective feedback prior to the actual execution, by reasoning about the potential outcome and correctness of actions. Specifically, we propose a Suggestion-aware Gradient Relative Policy Optimization (S-GRPO) strategy to construct our pre-operative critic model GUI-Critic-R1, incorporating a novel suggestion reward to enhance the reliability of the model's feedback. Furthermore, we develop a reasoning-bootstrapping based data collection pipeline to create a GUI-Critic-Train and a GUI-Critic-Test, filling existing gaps in GUI critic data. Static experiments on the GUI-Critic-Test across both mobile and web domains reveal that our GUI-Critic-R1 offers significant advantages in critic accuracy compared to current MLLMs. Dynamic evaluation on GUI automation benchmark further highlights the effectiveness and superiority of our model, as evidenced by improved success rates and operational efficiency. 12 authors · Jun 5, 2025 Upvote 18 GitHub 5.59k arXiv Page Submitted by XionghuiWang OneReward: Unified Mask-Guided Image Generation via Multi-Task Human Preference Learning In this paper, we introduce OneReward, a unified reinforcement learning framework that enhances the model's generative capabilities across multiple tasks under different evaluation criteria using only One Reward model. By employing a single vision-language model (VLM) as the generative reward model, which can distinguish the winner and loser for a given task and a given evaluation criterion, it can be effectively applied to multi-task generation models, particularly in contexts with varied data and diverse task objectives. We utilize OneReward for mask-guided image generation, which can be further divided into several sub-tasks such as image fill, image extend, object removal, and text rendering, involving a binary mask as the edit area. Although these domain-specific tasks share same conditioning paradigm, they differ significantly in underlying data distributions and evaluation metrics. Existing methods often rely on task-specific supervised fine-tuning (SFT), which limits generalization and training efficiency. Building on OneReward, we develop Seedream 3.0 Fill, a mask-guided generation model trained via multi-task reinforcement learning directly on a pre-trained base model, eliminating the need for task-specific SFT. Experimental results demonstrate that our unified edit model consistently outperforms both commercial and open-source competitors, such as Ideogram, Adobe Photoshop, and FLUX Fill [Pro], across multiple evaluation dimensions. Code and model are available at: https://one-reward.github.io 6 authors · Published on Aug 28, 2025 Upvote 12 GitHub 180 arXiv Page Submitted by XionghuiWang OneReward: Unified Mask-Guided Image Generation via Multi-Task Human Preference Learning In this paper, we introduce OneReward, a unified reinforcement learning framework that enhances the model's generative capabilities across multiple tasks under different evaluation criteria using only One Reward model. By employing a single vision-language model (VLM) as the generative reward model, which can distinguish the winner and loser for a given task and a given evaluation criterion, it can be effectively applied to multi-task generation models, particularly in contexts with varied data and diverse task objectives. We utilize OneReward for mask-guided image generation, which can be further divided into several sub-tasks such as image fill, image extend, object removal, and text rendering, involving a binary mask as the edit area. Although these domain-specific tasks share same conditioning paradigm, they differ significantly in underlying data distributions and evaluation metrics. Existing methods often rely on task-specific supervised fine-tuning (SFT), which limits generalization and training efficiency. Building on OneReward, we develop Seedream 3.0 Fill, a mask-guided generation model trained via multi-task reinforcement learning directly on a pre-trained base model, eliminating the need for task-specific SFT. Experimental results demonstrate that our unified edit model consistently outperforms both commercial and open-source competitors, such as Ideogram, Adobe Photoshop, and FLUX Fill [Pro], across multiple evaluation dimensions. Code and model are available at: https://one-reward.github.io 6 authors · Aug 28, 2025 Upvote 12 GitHub 180 arXiv Page Submitted by henggg The Landscape of Agentic Reinforcement Learning for LLMs: A Survey The emergence of agentic reinforcement learning (Agentic RL) marks a paradigm shift from conventional reinforcement learning applied to large language models (LLM RL), reframing LLMs from passive sequence generators into autonomous, decision-making agents embedded in complex, dynamic worlds. This survey formalizes this conceptual shift by contrasting the degenerate single-step Markov Decision Processes (MDPs) of LLM-RL with the temporally extended, partially observable Markov decision processes (POMDPs) that define Agentic RL. Building on this foundation, we propose a comprehensive twofold taxonomy: one organized around core agentic capabilities, including planning, tool use, memory, reasoning, self-improvement, and perception, and the other around their applications across diverse task domains. Central to our thesis is that reinforcement learning serves as the critical mechanism for transforming these capabilities from static, heuristic modules into adaptive, robust agentic behavior. To support and accelerate future research, we consolidate the landscape of open-source environments, benchmarks, and frameworks into a practical compendium. By synthesizing over five hundred recent works, this survey charts the contours of this rapidly evolving field and highlights the opportunities and challenges that will shape the development of scalable, general-purpose AI agents. 25 authors · Published on Sep 2, 2025 Upvote 182 GitHub 579 arXiv Page Submitted by henggg The Landscape of Agentic Reinforcement Learning for LLMs: A Survey The emergence of agentic reinforcement learning (Agentic RL) marks a paradigm shift from conventional reinforcement learning applied to large language models (LLM RL), reframing LLMs from passive sequence generators into autonomous, decision-making agents embedded in complex, dynamic worlds. This survey formalizes this conceptual shift by contrasting the degenerate single-step Markov Decision Processes (MDPs) of LLM-RL with the temporally extended, partially observable Markov decision processes (POMDPs) that define Agentic RL. Building on this foundation, we propose a comprehensive twofold taxonomy: one organized around core agentic capabilities, including planning, tool use, memory, reasoning, self-improvement, and perception, and the other around their applications across diverse task domains. Central to our thesis is that reinforcement learning serves as the critical mechanism for transforming these capabilities from static, heuristic modules into adaptive, robust agentic behavior. To support and accelerate future research, we consolidate the landscape of open-source environments, benchmarks, and frameworks into a practical compendium. By synthesizing over five hundred recent works, this survey charts the contours of this rapidly evolving field and highlights the opportunities and challenges that will shape the development of scalable, general-purpose AI agents. 25 authors · Sep 2, 2025 Upvote 182 GitHub 579 arXiv Page Submitted by Weiyun1025 InternVL3: Exploring Advanced Training and Test-Time Recipes for Open-Source Multimodal Models We introduce InternVL3, a significant advancement in the InternVL series featuring a native multimodal pre-training paradigm. Rather than adapting a text-only large language model (LLM) into a multimodal large language model (MLLM) that supports visual inputs, InternVL3 jointly acquires multimodal and linguistic capabilities from both diverse multimodal data and pure-text corpora during a single pre-training stage. This unified training paradigm effectively addresses the complexities and alignment challenges commonly encountered in conventional post-hoc training pipelines for MLLMs. To further improve performance and scalability, InternVL3 incorporates variable visual position encoding (V2PE) to support extended multimodal contexts, employs advanced post-training techniques such as supervised fine-tuning (SFT) and mixed preference optimization (MPO), and adopts test-time scaling strategies alongside an optimized training infrastructure. Extensive empirical evaluations demonstrate that InternVL3 delivers superior performance across a wide range of multi-modal tasks. In particular, InternVL3-78B achieves a score of 72.2 on the MMMU benchmark, setting a new state-of-the-art among open-source MLLMs. Its capabilities remain highly competitive with leading proprietary models, including ChatGPT-4o, Claude 3.5 Sonnet, and Gemini 2.5 Pro, while also maintaining strong pure-language proficiency. In pursuit of open-science principles, we will publicly release both the training data and model weights to foster further research and development in next-generation MLLMs. 47 authors · Published on Apr 14, 2025 Upvote 288 arXiv Page Submitted by Weiyun1025 InternVL3: Exploring Advanced Training and Test-Time Recipes for Open-Source Multimodal Models We introduce InternVL3, a significant advancement in the InternVL series featuring a native multimodal pre-training paradigm. Rather than adapting a text-only large language model (LLM) into a multimodal large language model (MLLM) that supports visual inputs, InternVL3 jointly acquires multimodal and linguistic capabilities from both diverse multimodal data and pure-text corpora during a single pre-training stage. This unified training paradigm effectively addresses the complexities and alignment challenges commonly encountered in conventional post-hoc training pipelines for MLLMs. To further improve performance and scalability, InternVL3 incorporates variable visual position encoding (V2PE) to support extended multimodal contexts, employs advanced post-training techniques such as supervised fine-tuning (SFT) and mixed preference optimization (MPO), and adopts test-time scaling strategies alongside an optimized training infrastructure. Extensive empirical evaluations demonstrate that InternVL3 delivers superior performance across a wide range of multi-modal tasks. In particular, InternVL3-78B achieves a score of 72.2 on the MMMU benchmark, setting a new state-of-the-art among open-source MLLMs. Its capabilities remain highly competitive with leading proprietary models, including ChatGPT-4o, Claude 3.5 Sonnet, and Gemini 2.5 Pro, while also maintaining strong pure-language proficiency. In pursuit of open-science principles, we will publicly release both the training data and model weights to foster further research and development in next-generation MLLMs. 47 authors · Apr 14, 2025 Upvote 288 arXiv Page Submitted by taesiri AgentFly: Fine-tuning LLM Agents without Fine-tuning LLMs In this paper, we introduce a novel learning paradigm for adaptive Large Language Model (LLM) agents that eliminates the need for fine-tuning the underlying LLMs. Existing approaches are often either rigid, relying on static, handcrafted reflection workflows, or computationally intensive, requiring gradient updates of LLM model parameters. In contrast, our method enables low-cost continual adaptation via memory-based online reinforcement learning. We formalise this as a Memory-augmented Markov Decision Process (M-MDP), equipped with a neural case-selection policy to guide action decisions. Past experiences are stored in an episodic memory, either differentiable or non-parametric. The policy is continually updated based on environmental feedback through a memory rewriting mechanism, whereas policy improvement is achieved through efficient memory reading (retrieval). We instantiate our agent model in the deep research setting, namely AgentFly, which attains top-1 on GAIA validation (87.88% Pass@3) and 79.40% on the test set. It reaches 66.6% F1 and 80.4% PM on the DeepResearcher dataset, outperforming the state-of-the-art training-based method, while case-based memory adds 4.7% to 9.6% absolute points on out-of-distribution tasks. Our approach offers a scalable and efficient pathway for developing generalist LLM agents capable of continuous, real-time learning without gradient updates, advancing machine learning towards open-ended skill acquisition and deep research scenarios. The code is available at https://github.com/Agent-on-the-Fly/AgentFly. 11 authors · Published on Aug 22, 2025 Upvote 146 GitHub 1.39k arXiv Page Submitted by taesiri AgentFly: Fine-tuning LLM Agents without Fine-tuning LLMs In this paper, we introduce a novel learning paradigm for adaptive Large Language Model (LLM) agents that eliminates the need for fine-tuning the underlying LLMs. Existing approaches are often either rigid, relying on static, handcrafted reflection workflows, or computationally intensive, requiring gradient updates of LLM model parameters. In contrast, our method enables low-cost continual adaptation via memory-based online reinforcement learning. We formalise this as a Memory-augmented Markov Decision Process (M-MDP), equipped with a neural case-selection policy to guide action decisions. Past experiences are stored in an episodic memory, either differentiable or non-parametric. The policy is continually updated based on environmental feedback through a memory rewriting mechanism, whereas policy improvement is achieved through efficient memory reading (retrieval). We instantiate our agent model in the deep research setting, namely AgentFly, which attains top-1 on GAIA validation (87.88% Pass@3) and 79.40% on the test set. It reaches 66.6% F1 and 80.4% PM on the DeepResearcher dataset, outperforming the state-of-the-art training-based method, while case-based memory adds 4.7% to 9.6% absolute points on out-of-distribution tasks. Our approach offers a scalable and efficient pathway for developing generalist LLM agents capable of continuous, real-time learning without gradient updates, advancing machine learning towards open-ended skill acquisition and deep research scenarios. The code is available at https://github.com/Agent-on-the-Fly/AgentFly. 11 authors · Aug 22, 2025 Upvote 146 GitHub 1.39k arXiv Page Submitted by Lingaaaaaaa Revolutionizing Reinforcement Learning Framework for Diffusion Large Language Models We propose TraceRL, a trajectory-aware reinforcement learning framework for diffusion language models (DLMs) that incorporates preferred inference trajectory into post-training, and is applicable across different architectures. Equipped with a diffusion-based value model that enhances training stability, we demonstrate improved reasoning performance on complex math and coding tasks. Besides, it can also be applied to adapt block-specific models to larger blocks, which improves sampling flexibility. Employing TraceRL, we derive a series of state-of-the-art diffusion language models, namely TraDo. Although smaller than 7B-scale AR models, TraDo-4B-Instruct still consistently outperforms them across complex math reasoning tasks. TraDo-8B-Instruct achieves relative accuracy improvements of 6.1% over Qwen2.5-7B-Instruct and 51.3% over Llama3.1-8B-Instruct on mathematical reasoning benchmarks. Through curriculum learning, we also derive the first long-CoT DLM, outperforming Qwen2.5-7B-Instruct on MATH500 with an 18.1% relative accuracy gain. To facilitate reproducible research and practical applications, we release a comprehensive open-source framework for building, training, and deploying diffusion LLMs across diverse architectures. The framework integrates accelerated KV-cache techniques and inference engines for both inference and reinforcement learning, and includes implementations of various supervised fine-tuning and RL methods for mathematics, coding, and general tasks. Code and Models: https://github.com/Gen-Verse/dLLM-RL 6 authors · Published on Sep 8, 2025 Upvote 49 GitHub 162 arXiv Page Submitted by Lingaaaaaaa Revolutionizing Reinforcement Learning Framework for Diffusion Large Language Models We propose TraceRL, a trajectory-aware reinforcement learning framework for diffusion language models (DLMs) that incorporates preferred inference trajectory into post-training, and is applicable across different architectures. Equipped with a diffusion-based value model that enhances training stability, we demonstrate improved reasoning performance on complex math and coding tasks. Besides, it can also be applied to adapt block-specific models to larger blocks, which improves sampling flexibility. Employing TraceRL, we derive a series of state-of-the-art diffusion language models, namely TraDo. Although smaller than 7B-scale AR models, TraDo-4B-Instruct still consistently outperforms them across complex math reasoning tasks. TraDo-8B-Instruct achieves relative accuracy improvements of 6.1% over Qwen2.5-7B-Instruct and 51.3% over Llama3.1-8B-Instruct on mathematical reasoning benchmarks. Through curriculum learning, we also derive the first long-CoT DLM, outperforming Qwen2.5-7B-Instruct on MATH500 with an 18.1% relative accuracy gain. To facilitate reproducible research and practical applications, we release a comprehensive open-source framework for building, training, and deploying diffusion LLMs across diverse architectures. The framework integrates accelerated KV-cache techniques and inference engines for both inference and reinforcement learning, and includes implementations of various supervised fine-tuning and RL methods for mathematics, coding, and general tasks. Code and Models: https://github.com/Gen-Verse/dLLM-RL 6 authors · Sep 8, 2025 Upvote 49 GitHub 162 arXiv Page Submitted by LooperXX Qwen-Image Technical Report We present Qwen-Image, an image generation foundation model in the Qwen series that achieves significant advances in complex text rendering and precise image editing. To address the challenges of complex text rendering, we design a comprehensive data pipeline that includes large-scale data collection, filtering, annotation, synthesis, and balancing. Moreover, we adopt a progressive training strategy that starts with non-text-to-text rendering, evolves from simple to complex textual inputs, and gradually scales up to paragraph-level descriptions. This curriculum learning approach substantially enhances the model's native text rendering capabilities. As a result, Qwen-Image not only performs exceptionally well in alphabetic languages such as English, but also achieves remarkable progress on more challenging logographic languages like Chinese. To enhance image editing consistency, we introduce an improved multi-task training paradigm that incorporates not only traditional text-to-image (T2I) and text-image-to-image (TI2I) tasks but also image-to-image (I2I) reconstruction, effectively aligning the latent representations between Qwen2.5-VL and MMDiT. Furthermore, we separately feed the original image into Qwen2.5-VL and the VAE encoder to obtain semantic and reconstructive representations, respectively. This dual-encoding mechanism enables the editing module to strike a balance between preserving semantic consistency and maintaining visual fidelity. Qwen-Image achieves state-of-the-art performance, demonstrating its strong capabilities in both image generation and editing across multiple benchmarks. 39 authors · Published on Aug 4, 2025 Upvote 245 GitHub 4.94k arXiv Page Submitted by LooperXX Qwen-Image Technical Report We present Qwen-Image, an image generation foundation model in the Qwen series that achieves significant advances in complex text rendering and precise image editing. To address the challenges of complex text rendering, we design a comprehensive data pipeline that includes large-scale data collection, filtering, annotation, synthesis, and balancing. Moreover, we adopt a progressive training strategy that starts with non-text-to-text rendering, evolves from simple to complex textual inputs, and gradually scales up to paragraph-level descriptions. This curriculum learning approach substantially enhances the model's native text rendering capabilities. As a result, Qwen-Image not only performs exceptionally well in alphabetic languages such as English, but also achieves remarkable progress on more challenging logographic languages like Chinese. To enhance image editing consistency, we introduce an improved multi-task training paradigm that incorporates not only traditional text-to-image (T2I) and text-image-to-image (TI2I) tasks but also image-to-image (I2I) reconstruction, effectively aligning the latent representations between Qwen2.5-VL and MMDiT. Furthermore, we separately feed the original image into Qwen2.5-VL and the VAE encoder to obtain semantic and reconstructive representations, respectively. This dual-encoding mechanism enables the editing module to strike a balance between preserving semantic consistency and maintaining visual fidelity. Qwen-Image achieves state-of-the-art performance, demonstrating its strong capabilities in both image generation and editing across multiple benchmarks. 39 authors · Aug 4, 2025 Upvote 245 GitHub 4.94k arXiv Page Submitted by hpouransari FastVLM: Efficient Vision Encoding for Vision Language Models Scaling the input image resolution is essential for enhancing the performance of Vision Language Models (VLMs), particularly in text-rich image understanding tasks. However, popular visual encoders such as ViTs become inefficient at high resolutions due to the large number of tokens and high encoding latency caused by stacked self-attention layers. At different operational resolutions, the vision encoder of a VLM can be optimized along two axes: reducing encoding latency and minimizing the number of visual tokens passed to the LLM, thereby lowering overall latency. Based on a comprehensive efficiency analysis of the interplay between image resolution, vision latency, token count, and LLM size, we introduce FastVLM, a model that achieves an optimized trade-off between latency, model size and accuracy. FastVLM incorporates FastViTHD, a novel hybrid vision encoder designed to output fewer tokens and significantly reduce encoding time for high-resolution images. Unlike previous methods, FastVLM achieves the optimal balance between visual token count and image resolution solely by scaling the input image, eliminating the need for additional token pruning and simplifying the model design. In the LLaVA-1.5 setup, FastVLM achieves 3.2times improvement in time-to-first-token (TTFT) while maintaining similar performance on VLM benchmarks compared to prior works. Compared to LLaVa-OneVision at the highest resolution (1152times1152), FastVLM achieves comparable performance on key benchmarks like SeedBench and MMMU, using the same 0.5B LLM, but with 85times faster TTFT and a vision encoder that is 3.4times smaller. 11 authors · Published on Dec 17, 2024 Upvote 65 GitHub 6.43k arXiv Page Submitted by hpouransari FastVLM: Efficient Vision Encoding for Vision Language Models Scaling the input image resolution is essential for enhancing the performance of Vision Language Models (VLMs), particularly in text-rich image understanding tasks. However, popular visual encoders such as ViTs become inefficient at high resolutions due to the large number of tokens and high encoding latency caused by stacked self-attention layers. At different operational resolutions, the vision encoder of a VLM can be optimized along two axes: reducing encoding latency and minimizing the number of visual tokens passed to the LLM, thereby lowering overall latency. Based on a comprehensive efficiency analysis of the interplay between image resolution, vision latency, token count, and LLM size, we introduce FastVLM, a model that achieves an optimized trade-off between latency, model size and accuracy. FastVLM incorporates FastViTHD, a novel hybrid vision encoder designed to output fewer tokens and significantly reduce encoding time for high-resolution images. Unlike previous methods, FastVLM achieves the optimal balance between visual token count and image resolution solely by scaling the input image, eliminating the need for additional token pruning and simplifying the model design. In the LLaVA-1.5 setup, FastVLM achieves 3.2times improvement in time-to-first-token (TTFT) while maintaining similar performance on VLM benchmarks compared to prior works. Compared to LLaVa-OneVision at the highest resolution (1152times1152), FastVLM achieves comparable performance on key benchmarks like SeedBench and MMMU, using the same 0.5B LLM, but with 85times faster TTFT and a vision encoder that is 3.4times smaller. 11 authors · Dec 17, 2024 Upvote 65 GitHub 6.43k arXiv Page Submitted by chujiezheng Qwen3 Technical Report In this work, we present Qwen3, the latest version of the Qwen model family. Qwen3 comprises a series of large language models (LLMs) designed to advance performance, efficiency, and multilingual capabilities. The Qwen3 series includes models of both dense and Mixture-of-Expert (MoE) architectures, with parameter scales ranging from 0.6 to 235 billion. A key innovation in Qwen3 is the integration of thinking mode (for complex, multi-step reasoning) and non-thinking mode (for rapid, context-driven responses) into a unified framework. This eliminates the need to switch between different models--such as chat-optimized models (e.g., GPT-4o) and dedicated reasoning models (e.g., QwQ-32B)--and enables dynamic mode switching based on user queries or chat templates. Meanwhile, Qwen3 introduces a thinking budget mechanism, allowing users to allocate computational resources adaptively during inference, thereby balancing latency and performance based on task complexity. Moreover, by leveraging the knowledge from the flagship models, we significantly reduce the computational resources required to build smaller-scale models, while ensuring their highly competitive performance. Empirical evaluations demonstrate that Qwen3 achieves state-of-the-art results across diverse benchmarks, including tasks in code generation, mathematical reasoning, agent tasks, etc., competitive against larger MoE models and proprietary models. Compared to its predecessor Qwen2.5, Qwen3 expands multilingual support from 29 to 119 languages and dialects, enhancing global accessibility through improved cross-lingual understanding and generation capabilities. To facilitate reproducibility and community-driven research and development, all Qwen3 models are publicly accessible under Apache 2.0. 60 authors · Published on May 14, 2025 Upvote 290 GitHub 24.5k arXiv Page Submitted by chujiezheng Qwen3 Technical Report In this work, we present Qwen3, the latest version of the Qwen model family. Qwen3 comprises a series of large language models (LLMs) designed to advance performance, efficiency, and multilingual capabilities. The Qwen3 series includes models of both dense and Mixture-of-Expert (MoE) architectures, with parameter scales ranging from 0.6 to 235 billion. A key innovation in Qwen3 is the integration of thinking mode (for complex, multi-step reasoning) and non-thinking mode (for rapid, context-driven responses) into a unified framework. This eliminates the need to switch between different models--such as chat-optimized models (e.g., GPT-4o) and dedicated reasoning models (e.g., QwQ-32B)--and enables dynamic mode switching based on user queries or chat templates. Meanwhile, Qwen3 introduces a thinking budget mechanism, allowing users to allocate computational resources adaptively during inference, thereby balancing latency and performance based on task complexity. Moreover, by leveraging the knowledge from the flagship models, we significantly reduce the computational resources required to build smaller-scale models, while ensuring their highly competitive performance. Empirical evaluations demonstrate that Qwen3 achieves state-of-the-art results across diverse benchmarks, including tasks in code generation, mathematical reasoning, agent tasks, etc., competitive against larger MoE models and proprietary models. Compared to its predecessor Qwen2.5, Qwen3 expands multilingual support from 29 to 119 languages and dialects, enhancing global accessibility through improved cross-lingual understanding and generation capabilities. To facilitate reproducibility and community-driven research and development, all Qwen3 models are publicly accessible under Apache 2.0. 60 authors · May 14, 2025 Upvote 290 GitHub 24.5k arXiv Page Submitted by deleted RepoMaster: Autonomous Exploration and Understanding of GitHub Repositories for Complex Task Solving The ultimate goal of code agents is to solve complex tasks autonomously. Although large language models (LLMs) have made substantial progress in code generation, real-world tasks typically demand full-fledged code repositories rather than simple scripts. Building such repositories from scratch remains a major challenge. Fortunately, GitHub hosts a vast, evolving collection of open-source repositories, which developers frequently reuse as modular components for complex tasks. Yet, existing frameworks like OpenHands and SWE-Agent still struggle to effectively leverage these valuable resources. Relying solely on README files provides insufficient guidance, and deeper exploration reveals two core obstacles: overwhelming information and tangled dependencies of repositories, both constrained by the limited context windows of current LLMs. To tackle these issues, we propose RepoMaster, an autonomous agent framework designed to explore and reuse GitHub repositories for solving complex tasks. For efficient understanding, RepoMaster constructs function-call graphs, module-dependency graphs, and hierarchical code trees to identify essential components, providing only identified core elements to the LLMs rather than the entire repository. During autonomous execution, it progressively explores related components using our exploration tools and prunes information to optimize context usage. Evaluated on the adjusted MLE-bench, RepoMaster achieves a 110% relative boost in valid submissions over the strongest baseline OpenHands. On our newly released GitTaskBench, RepoMaster lifts the task-pass rate from 40.7% to 62.9% while reducing token usage by 95%. Our code and demonstration materials are publicly available at https://github.com/QuantaAlpha/RepoMaster. 14 authors · Published on May 27, 2025 Upvote 2 GitHub 277 arXiv Page Submitted by deleted RepoMaster: Autonomous Exploration and Understanding of GitHub Repositories for Complex Task Solving The ultimate goal of code agents is to solve complex tasks autonomously. Although large language models (LLMs) have made substantial progress in code generation, real-world tasks typically demand full-fledged code repositories rather than simple scripts. Building such repositories from scratch remains a major challenge. Fortunately, GitHub hosts a vast, evolving collection of open-source repositories, which developers frequently reuse as modular components for complex tasks. Yet, existing frameworks like OpenHands and SWE-Agent still struggle to effectively leverage these valuable resources. Relying solely on README files provides insufficient guidance, and deeper exploration reveals two core obstacles: overwhelming information and tangled dependencies of repositories, both constrained by the limited context windows of current LLMs. To tackle these issues, we propose RepoMaster, an autonomous agent framework designed to explore and reuse GitHub repositories for solving complex tasks. For efficient understanding, RepoMaster constructs function-call graphs, module-dependency graphs, and hierarchical code trees to identify essential components, providing only identified core elements to the LLMs rather than the entire repository. During autonomous execution, it progressively explores related components using our exploration tools and prunes information to optimize context usage. Evaluated on the adjusted MLE-bench, RepoMaster achieves a 110% relative boost in valid submissions over the strongest baseline OpenHands. On our newly released GitTaskBench, RepoMaster lifts the task-pass rate from 40.7% to 62.9% while reducing token usage by 95%. Our code and demonstration materials are publicly available at https://github.com/QuantaAlpha/RepoMaster. 14 authors · May 27, 2025 Upvote 2 GitHub 277 arXiv Page Submitted by Kaichengalex Gradient-Attention Guided Dual-Masking Synergetic Framework for Robust Text-based Person Retrieval Although Contrastive Language-Image Pre-training (CLIP) exhibits strong performance across diverse vision tasks, its application to person representation learning faces two critical challenges: (i) the scarcity of large-scale annotated vision-language data focused on person-centric images, and (ii) the inherent limitations of global contrastive learning, which struggles to maintain discriminative local features crucial for fine-grained matching while remaining vulnerable to noisy text tokens. This work advances CLIP for person representation learning through synergistic improvements in data curation and model architecture. First, we develop a noise-resistant data construction pipeline that leverages the in-context learning capabilities of MLLMs to automatically filter and caption web-sourced images. This yields WebPerson, a large-scale dataset of 5M high-quality person-centric image-text pairs. Second, we introduce the GA-DMS (Gradient-Attention Guided Dual-Masking Synergetic) framework, which improves cross-modal alignment by adaptively masking noisy textual tokens based on the gradient-attention similarity score. Additionally, we incorporate masked token prediction objectives that compel the model to predict informative text tokens, enhancing fine-grained semantic representation learning. Extensive experiments show that GA-DMS achieves state-of-the-art performance across multiple benchmarks. 6 authors · Published on Sep 11, 2025 Upvote 5 GitHub 9 arXiv Page Submitted by Kaichengalex Gradient-Attention Guided Dual-Masking Synergetic Framework for Robust Text-based Person Retrieval Although Contrastive Language-Image Pre-training (CLIP) exhibits strong performance across diverse vision tasks, its application to person representation learning faces two critical challenges: (i) the scarcity of large-scale annotated vision-language data focused on person-centric images, and (ii) the inherent limitations of global contrastive learning, which struggles to maintain discriminative local features crucial for fine-grained matching while remaining vulnerable to noisy text tokens. This work advances CLIP for person representation learning through synergistic improvements in data curation and model architecture. First, we develop a noise-resistant data construction pipeline that leverages the in-context learning capabilities of MLLMs to automatically filter and caption web-sourced images. This yields WebPerson, a large-scale dataset of 5M high-quality person-centric image-text pairs. Second, we introduce the GA-DMS (Gradient-Attention Guided Dual-Masking Synergetic) framework, which improves cross-modal alignment by adaptively masking noisy textual tokens based on the gradient-attention similarity score. Additionally, we incorporate masked token prediction objectives that compel the model to predict informative text tokens, enhancing fine-grained semantic representation learning. Extensive experiments show that GA-DMS achieves state-of-the-art performance across multiple benchmarks. 6 authors · Sep 11, 2025 Upvote 5 GitHub 9 arXiv Page Submitted by Hanyuezhuohua Parallel-R1: Towards Parallel Thinking via Reinforcement Learning Parallel thinking has emerged as a novel approach for enhancing the reasoning capabilities of large language models (LLMs) by exploring multiple reasoning paths concurrently. However, activating such capabilities through training remains challenging, as existing methods predominantly rely on supervised fine-tuning (SFT) over synthetic data, which encourages teacher-forced imitation rather than exploration and generalization. Different from them, we propose Parallel-R1, the first reinforcement learning (RL) framework that enables parallel thinking behaviors for complex real-world reasoning tasks. Our framework employs a progressive curriculum that explicitly addresses the cold-start problem in training parallel thinking with RL. We first use SFT on prompt-generated trajectories from easier tasks to instill the parallel thinking ability, then transition to RL to explore and generalize this skill on harder problems. Experiments on various math benchmarks, including MATH, AMC23, and AIME, show that Parallel-R1 successfully instills parallel thinking, leading to 8.4% accuracy improvements over the sequential thinking model trained directly on challenging tasks with RL. Further analysis reveals a clear shift in the model's thinking behavior: at an early stage, it uses parallel thinking as an exploration strategy, while in a later stage, it uses the same capability for multi-perspective verification. Most significantly, we validate parallel thinking as a mid-training exploration scaffold, where this temporary exploratory phase unlocks a higher performance ceiling after RL, yielding a 42.9% improvement over the baseline on AIME25. Our model, data, and code will be open-source at https://github.com/zhengkid/Parallel-R1. 11 authors · Published on Sep 9, 2025 Upvote 84 GitHub 58 arXiv Page Submitted by Hanyuezhuohua Parallel-R1: Towards Parallel Thinking via Reinforcement Learning Parallel thinking has emerged as a novel approach for enhancing the reasoning capabilities of large language models (LLMs) by exploring multiple reasoning paths concurrently. However, activating such capabilities through training remains challenging, as existing methods predominantly rely on supervised fine-tuning (SFT) over synthetic data, which encourages teacher-forced imitation rather than exploration and generalization. Different from them, we propose Parallel-R1, the first reinforcement learning (RL) framework that enables parallel thinking behaviors for complex real-world reasoning tasks. Our framework employs a progressive curriculum that explicitly addresses the cold-start problem in training parallel thinking with RL. We first use SFT on prompt-generated trajectories from easier tasks to instill the parallel thinking ability, then transition to RL to explore and generalize this skill on harder problems. Experiments on various math benchmarks, including MATH, AMC23, and AIME, show that Parallel-R1 successfully instills parallel thinking, leading to 8.4% accuracy improvements over the sequential thinking model trained directly on challenging tasks with RL. Further analysis reveals a clear shift in the model's thinking behavior: at an early stage, it uses parallel thinking as an exploration strategy, while in a later stage, it uses the same capability for multi-perspective verification. Most significantly, we validate parallel thinking as a mid-training exploration scaffold, where this temporary exploratory phase unlocks a higher performance ceiling after RL, yielding a 42.9% improvement over the baseline on AIME25. Our model, data, and code will be open-source at https://github.com/zhengkid/Parallel-R1. 11 authors · Sep 9, 2025 Upvote 84 GitHub 58 arXiv Page Submitted by tulvgengenr MixGRPO: Unlocking Flow-based GRPO Efficiency with Mixed ODE-SDE Although GRPO substantially enhances flow matching models in human preference alignment of image generation, methods such as FlowGRPO still exhibit inefficiency due to the necessity of sampling and optimizing over all denoising steps specified by the Markov Decision Process (MDP). In this paper, we propose MixGRPO, a novel framework that leverages the flexibility of mixed sampling strategies through the integration of stochastic differential equations (SDE) and ordinary differential equations (ODE). This streamlines the optimization process within the MDP to improve efficiency and boost performance. Specifically, MixGRPO introduces a sliding window mechanism, using SDE sampling and GRPO-guided optimization only within the window, while applying ODE sampling outside. This design confines sampling randomness to the time-steps within the window, thereby reducing the optimization overhead, and allowing for more focused gradient updates to accelerate convergence. Additionally, as time-steps beyond the sliding window are not involved in optimization, higher-order solvers are supported for sampling. So we present a faster variant, termed MixGRPO-Flash, which further improves training efficiency while achieving comparable performance. MixGRPO exhibits substantial gains across multiple dimensions of human preference alignment, outperforming DanceGRPO in both effectiveness and efficiency, with nearly 50% lower training time. Notably, MixGRPO-Flash further reduces training time by 71%. Codes and models are available at https://github.com/Tencent-Hunyuan/MixGRPO{MixGRPO}. 7 authors · Published on Jul 29, 2025 Upvote 12 GitHub 572 arXiv Page Submitted by tulvgengenr MixGRPO: Unlocking Flow-based GRPO Efficiency with Mixed ODE-SDE Although GRPO substantially enhances flow matching models in human preference alignment of image generation, methods such as FlowGRPO still exhibit inefficiency due to the necessity of sampling and optimizing over all denoising steps specified by the Markov Decision Process (MDP). In this paper, we propose MixGRPO, a novel framework that leverages the flexibility of mixed sampling strategies through the integration of stochastic differential equations (SDE) and ordinary differential equations (ODE). This streamlines the optimization process within the MDP to improve efficiency and boost performance. Specifically, MixGRPO introduces a sliding window mechanism, using SDE sampling and GRPO-guided optimization only within the window, while applying ODE sampling outside. This design confines sampling randomness to the time-steps within the window, thereby reducing the optimization overhead, and allowing for more focused gradient updates to accelerate convergence. Additionally, as time-steps beyond the sliding window are not involved in optimization, higher-order solvers are supported for sampling. So we present a faster variant, termed MixGRPO-Flash, which further improves training efficiency while achieving comparable performance. MixGRPO exhibits substantial gains across multiple dimensions of human preference alignment, outperforming DanceGRPO in both effectiveness and efficiency, with nearly 50% lower training time. Notably, MixGRPO-Flash further reduces training time by 71%. Codes and models are available at https://github.com/Tencent-Hunyuan/MixGRPO{MixGRPO}. 7 authors · Jul 29, 2025 Upvote 12 GitHub 572 arXiv Page Submitted by Nickyang Hunyuan-MT Technical Report In this report, we introduce Hunyuan-MT-7B, our first open-source multilingual translation model, which supports bidirectional translation across 33 major languages and places a special emphasis on translation between Mandarin and several ethnic minority languages as well as dialects. Furthermore, to serve and address diverse translation scenarios and enhance model performance at test time, we introduce Hunyuan-MT-Chimera-7B, a translation model inspired by the slow thinking mode. This model integrates multiple outputs generated by the Hunyuan-MT-7B model under varying parameter settings, thereby achieving performance superior to that of conventional slow-thinking models based on Chain-of-Thought (CoT). The development of our models follows a holistic training process specifically engineered for multilingual translation, which begins with general and MT-oriented pre-training to build foundational capabilities, proceeds to Supervised Fine-Tuning (SFT) for task-specific adaptation, and culminates in advanced alignment through Reinforcement Learning (RL) and weak-to-strong RL. Through comprehensive experimentation, we demonstrate that both Hunyuan-MT-7B and Hunyuan-MT-Chimera-7B significantly outperform all translation-specific models of comparable parameter size and most of the SOTA large models, particularly on the task of translation between Mandarin and minority languages as well as dialects. In the WMT2025 shared task (General Machine Translation), our models demonstrate state-of-the-art performance, ranking first in 30 out of 31 language pairs. This result highlights the robustness of our models across a diverse linguistic spectrum, encompassing high-resource languages such as Chinese, English, and Japanese, as well as low-resource languages including Czech, Marathi, Estonian, and Icelandic. 7 authors · Published on Sep 5, 2025 Upvote 9 GitHub 492 arXiv Page Submitted by Nickyang Hunyuan-MT Technical Report In this report, we introduce Hunyuan-MT-7B, our first open-source multilingual translation model, which supports bidirectional translation across 33 major languages and places a special emphasis on translation between Mandarin and several ethnic minority languages as well as dialects. Furthermore, to serve and address diverse translation scenarios and enhance model performance at test time, we introduce Hunyuan-MT-Chimera-7B, a translation model inspired by the slow thinking mode. This model integrates multiple outputs generated by the Hunyuan-MT-7B model under varying parameter settings, thereby achieving performance superior to that of conventional slow-thinking models based on Chain-of-Thought (CoT). The development of our models follows a holistic training process specifically engineered for multilingual translation, which begins with general and MT-oriented pre-training to build foundational capabilities, proceeds to Supervised Fine-Tuning (SFT) for task-specific adaptation, and culminates in advanced alignment through Reinforcement Learning (RL) and weak-to-strong RL. Through comprehensive experimentation, we demonstrate that both Hunyuan-MT-7B and Hunyuan-MT-Chimera-7B significantly outperform all translation-specific models of comparable parameter size and most of the SOTA large models, particularly on the task of translation between Mandarin and minority languages as well as dialects. In the WMT2025 shared task (General Machine Translation), our models demonstrate state-of-the-art performance, ranking first in 30 out of 31 language pairs. This result highlights the robustness of our models across a diverse linguistic spectrum, encompassing high-resource languages such as Chinese, English, and Japanese, as well as low-resource languages including Czech, Marathi, Estonian, and Icelandic. 7 authors · Sep 5, 2025 Upvote 9 GitHub 492 arXiv Page Submitted by hiyouga Easy Dataset: A Unified and Extensible Framework for Synthesizing LLM Fine-Tuning Data from Unstructured Documents Large language models (LLMs) have shown impressive performance on general-purpose tasks, yet adapting them to specific domains remains challenging due to the scarcity of high-quality domain data. Existing data synthesis tools often struggle to extract reliable fine-tuning data from heterogeneous documents effectively. To address this limitation, we propose Easy Dataset, a unified framework for synthesizing fine-tuning data from unstructured documents via an intuitive graphical user interface (GUI). Specifically, Easy Dataset allows users to easily configure text extraction models and chunking strategies to transform raw documents into coherent text chunks. It then leverages a persona-driven prompting approach to generate diverse question-answer pairs using public-available LLMs. Throughout the pipeline, a human-in-the-loop visual interface facilitates the review and refinement of intermediate outputs to ensure data quality. Experiments on a financial question-answering task show that fine-tuning LLMs on the synthesized dataset significantly improves domain-specific performance while preserving general knowledge. The source code and installable package are available at https://github.com/ConardLi/easy-dataset and have garnered over 9,000 GitHub stars. 7 authors · Published on Jul 5, 2025 Upvote 42 GitHub 10.7k arXiv Page Submitted by hiyouga Easy Dataset: A Unified and Extensible Framework for Synthesizing LLM Fine-Tuning Data from Unstructured Documents Large language models (LLMs) have shown impressive performance on general-purpose tasks, yet adapting them to specific domains remains challenging due to the scarcity of high-quality domain data. Existing data synthesis tools often struggle to extract reliable fine-tuning data from heterogeneous documents effectively. To address this limitation, we propose Easy Dataset, a unified framework for synthesizing fine-tuning data from unstructured documents via an intuitive graphical user interface (GUI). Specifically, Easy Dataset allows users to easily configure text extraction models and chunking strategies to transform raw documents into coherent text chunks. It then leverages a persona-driven prompting approach to generate diverse question-answer pairs using public-available LLMs. Throughout the pipeline, a human-in-the-loop visual interface facilitates the review and refinement of intermediate outputs to ensure data quality. Experiments on a financial question-answering task show that fine-tuning LLMs on the synthesized dataset significantly improves domain-specific performance while preserving general knowledge. The source code and installable package are available at https://github.com/ConardLi/easy-dataset and have garnered over 9,000 GitHub stars. 7 authors · Jul 5, 2025 Upvote 42 GitHub 10.7k arXiv Page Submitted by fenfan UMO: Scaling Multi-Identity Consistency for Image Customization via Matching Reward Recent advancements in image customization exhibit a wide range of application prospects due to stronger customization capabilities. However, since we humans are more sensitive to faces, a significant challenge remains in preserving consistent identity while avoiding identity confusion with multi-reference images, limiting the identity scalability of customization models. To address this, we present UMO, a Unified Multi-identity Optimization framework, designed to maintain high-fidelity identity preservation and alleviate identity confusion with scalability. With \"multi-to-multi matching\" paradigm, UMO reformulates multi-identity generation as a global assignment optimization problem and unleashes multi-identity consistency for existing image customization methods generally through reinforcement learning on diffusion models. To facilitate the training of UMO, we develop a scalable customization dataset with multi-reference images, consisting of both synthesised and real parts. Additionally, we propose a new metric to measure identity confusion. Extensive experiments demonstrate that UMO not only improves identity consistency significantly, but also reduces identity confusion on several image customization methods, setting a new state-of-the-art among open-source methods along the dimension of identity preserving. Code and model: https://github.com/bytedance/UMO 6 authors · Published on Sep 8, 2025 Upvote 27 GitHub 70 arXiv Page Submitted by fenfan UMO: Scaling Multi-Identity Consistency for Image Customization via Matching Reward Recent advancements in image customization exhibit a wide range of application prospects due to stronger customization capabilities. However, since we humans are more sensitive to faces, a significant challenge remains in preserving consistent identity while avoiding identity confusion with multi-reference images, limiting the identity scalability of customization models. To address this, we present UMO, a Unified Multi-identity Optimization framework, designed to maintain high-fidelity identity preservation and alleviate identity confusion with scalability. With \"multi-to-multi matching\" paradigm, UMO reformulates multi-identity generation as a global assignment optimization problem and unleashes multi-identity consistency for existing image customization methods generally through reinforcement learning on diffusion models. To facilitate the training of UMO, we develop a scalable customization dataset with multi-reference images, consisting of both synthesised and real parts. Additionally, we propose a new metric to measure identity confusion. Extensive experiments demonstrate that UMO not only improves identity consistency significantly, but also reduces identity confusion on several image customization methods, setting a new state-of-the-art among open-source methods along the dimension of identity preserving. Code and model: https://github.com/bytedance/UMO 6 authors · Sep 8, 2025 Upvote 27 GitHub 70 arXiv Page Submitted by deleted Killing Two Birds with One Stone:Efficient and Robust Training of Face Recognition CNNs by Partial FC Learning discriminative deep feature embeddings by using million-scale in-the-wild datasets and margin-based softmax loss is the current state-of-the-art approach for face recognition. However, the memory and computing cost of the Fully Connected (FC) layer linearly scales up to the number of identities in the training set. Besides, the large-scale training data inevitably suffers from inter-class conflict and long-tailed distribution. In this paper, we propose a sparsely updating variant of the FC layer, named Partial FC (PFC). In each iteration, positive class centers and a random subset of negative class centers are selected to compute the margin-based softmax loss. All class centers are still maintained throughout the whole training process, but only a subset is selected and updated in each iteration. Therefore, the computing requirement, the probability of inter-class conflict, and the frequency of passive update on tail class centers, are dramatically reduced. Extensive experiments across different training data and backbones (e.g. CNN and ViT) confirm the effectiveness, robustness and efficiency of the proposed PFC. The source code is available at \\https://github.com/deepinsight/insightface/tree/master/recognition. 7 authors · Published on Mar 28, 2022 Upvote 2 GitHub 26.5k arXiv Page Submitted by deleted Killing Two Birds with One Stone:Efficient and Robust Training of Face Recognition CNNs by Partial FC Learning discriminative deep feature embeddings by using million-scale in-the-wild datasets and margin-based softmax loss is the current state-of-the-art approach for face recognition. However, the memory and computing cost of the Fully Connected (FC) layer linearly scales up to the number of identities in the training set. Besides, the large-scale training data inevitably suffers from inter-class conflict and long-tailed distribution. In this paper, we propose a sparsely updating variant of the FC layer, named Partial FC (PFC). In each iteration, positive class centers and a random subset of negative class centers are selected to compute the margin-based softmax loss. All class centers are still maintained throughout the whole training process, but only a subset is selected and updated in each iteration. Therefore, the computing requirement, the probability of inter-class conflict, and the frequency of passive update on tail class centers, are dramatically reduced. Extensive experiments across different training data and backbones (e.g. CNN and ViT) confirm the effectiveness, robustness and efficiency of the proposed PFC. The source code is available at \\https://github.com/deepinsight/insightface/tree/master/recognition. 7 authors · Mar 28, 2022 Upvote 2 GitHub 26.5k arXiv Page Submitted by xianbao GLM-4.5: Agentic, Reasoning, and Coding (ARC) Foundation Models We present GLM-4.5, an open-source Mixture-of-Experts (MoE) large language model with 355B total parameters and 32B activated parameters, featuring a hybrid reasoning method that supports both thinking and direct response modes. Through multi-stage training on 23T tokens and comprehensive post-training with expert model iteration and reinforcement learning, GLM-4.5 achieves strong performance across agentic, reasoning, and coding (ARC) tasks, scoring 70.1% on TAU-Bench, 91.0% on AIME 24, and 64.2% on SWE-bench Verified. With much fewer parameters than several competitors, GLM-4.5 ranks 3rd overall among all evaluated models and 2nd on agentic benchmarks. We release both GLM-4.5 (355B parameters) and a compact version, GLM-4.5-Air (106B parameters), to advance research in reasoning and agentic AI systems. Code, models, and more information are available at https://github.com/zai-org/GLM-4.5. 171 authors · Published on Aug 8, 2025 Upvote 178 GitHub 2.57k arXiv Page Submitted by xianbao GLM-4.5: Agentic, Reasoning, and Coding (ARC) Foundation Models We present GLM-4.5, an open-source Mixture-of-Experts (MoE) large language model with 355B total parameters and 32B activated parameters, featuring a hybrid reasoning method that supports both thinking and direct response modes. Through multi-stage training on 23T tokens and comprehensive post-training with expert model iteration and reinforcement learning, GLM-4.5 achieves strong performance across agentic, reasoning, and coding (ARC) tasks, scoring 70.1% on TAU-Bench, 91.0% on AIME 24, and 64.2% on SWE-bench Verified. With much fewer parameters than several competitors, GLM-4.5 ranks 3rd overall among all evaluated models and 2nd on agentic benchmarks. We release both GLM-4.5 (355B parameters) and a compact version, GLM-4.5-Air (106B parameters), to advance research in reasoning and agentic AI systems. Code, models, and more information are available at https://github.com/zai-org/GLM-4.5. 171 authors · Aug 8, 2025 Upvote 178 GitHub 2.57k arXiv Page Submitted by JianyuanWang VGGT: Visual Geometry Grounded Transformer We present VGGT, a feed-forward neural network that directly infers all key 3D attributes of a scene, including camera parameters, point maps, depth maps, and 3D point tracks, from one, a few, or hundreds of its views. This approach is a step forward in 3D computer vision, where models have typically been constrained to and specialized for single tasks. It is also simple and efficient, reconstructing images in under one second, and still outperforming alternatives that require post-processing with visual geometry optimization techniques. The network achieves state-of-the-art results in multiple 3D tasks, including camera parameter estimation, multi-view depth estimation, dense point cloud reconstruction, and 3D point tracking. We also show that using pretrained VGGT as a feature backbone significantly enhances downstream tasks, such as non-rigid point tracking and feed-forward novel view synthesis. Code and models are publicly available at https://github.com/facebookresearch/vggt. 6 authors · Published on Mar 14, 2025 Upvote 29 GitHub 10.9k arXiv Page Submitted by JianyuanWang VGGT: Visual Geometry Grounded Transformer We present VGGT, a feed-forward neural network that directly infers all key 3D attributes of a scene, including camera parameters, point maps, depth maps, and 3D point tracks, from one, a few, or hundreds of its views. This approach is a step forward in 3D computer vision, where models have typically been constrained to and specialized for single tasks. It is also simple and efficient, reconstructing images in under one second, and still outperforming alternatives that require post-processing with visual geometry optimization techniques. The network achieves state-of-the-art results in multiple 3D tasks, including camera parameter estimation, multi-view depth estimation, dense point cloud reconstruction, and 3D point tracking. We also show that using pretrained VGGT as a feature backbone significantly enhances downstream tasks, such as non-rigid point tracking and feed-forward novel view synthesis. Code and models are publicly available at https://github.com/facebookresearch/vggt. 6 authors · Mar 14, 2025 Upvote 29 GitHub 10.9k arXiv Page Submitted by callanwu WebDancer: Towards Autonomous Information Seeking Agency Addressing intricate real-world problems necessitates in-depth information seeking and multi-step reasoning. Recent progress in agentic systems, exemplified by Deep Research, underscores the potential for autonomous multi-step research. In this work, we present a cohesive paradigm for building end-to-end agentic information seeking agents from a data-centric and training-stage perspective. Our approach consists of four key stages: (1) browsing data construction, (2) trajectories sampling, (3) supervised fine-tuning for effective cold start, and (4) reinforcement learning for enhanced generalisation. We instantiate this framework in a web agent based on the ReAct, WebDancer. Empirical evaluations on the challenging information seeking benchmarks, GAIA and WebWalkerQA, demonstrate the strong performance of WebDancer, achieving considerable results and highlighting the efficacy of our training paradigm. Further analysis of agent training provides valuable insights and actionable, systematic pathways for developing more capable agentic models. The codes and demo will be released in https://github.com/Alibaba-NLP/WebAgent. 12 authors · Published on May 28, 2025 Upvote 30 GitHub 6.54k arXiv Page Submitted by callanwu WebDancer: Towards Autonomous Information Seeking Agency Addressing intricate real-world problems necessitates in-depth information seeking and multi-step reasoning. Recent progress in agentic systems, exemplified by Deep Research, underscores the potential for autonomous multi-step research. In this work, we present a cohesive paradigm for building end-to-end agentic information seeking agents from a data-centric and training-stage perspective. Our approach consists of four key stages: (1) browsing data construction, (2) trajectories sampling, (3) supervised fine-tuning for effective cold start, and (4) reinforcement learning for enhanced generalisation. We instantiate this framework in a web agent based on the ReAct, WebDancer. Empirical evaluations on the challenging information seeking benchmarks, GAIA and WebWalkerQA, demonstrate the strong performance of WebDancer, achieving considerable results and highlighting the efficacy of our training paradigm. Further analysis of agent training provides valuable insights and actionable, systematic pathways for developing more capable agentic models. The codes and demo will be released in https://github.com/Alibaba-NLP/WebAgent. 12 authors · May 28, 2025 Upvote 30 GitHub 6.54k arXiv Page Submitted by callanwu WebShaper: Agentically Data Synthesizing via Information-Seeking Formalization The advent of Large Language Model (LLM)-powered agents has revolutionized artificial intelligence by enabling solutions to complex, open-ended tasks through web-based information-seeking (IS) capabilities. The scarcity of high-quality training data has limited the development of IS agents. Existing approaches typically adopt an information-driven paradigm that first collects web data and then generates questions based on the retrieval. However, this may lead to inconsistency between information structure and reasoning structure, question and answer. To mitigate, we propose a formalization-driven IS data synthesis framework WebShaper to construct a dataset. WebShaper systematically formalizes IS tasks through set theory. Central to the formalization is the concept of Knowledge Projections (KP), which enables precise control over reasoning structure by KP operation compositions. During synthesis, we begin by creating seed tasks, then use a multi-step expansion process. At each step, an agentic Expander expands the current formal question more complex with retrieval and validation tools based on our formalization. We train our model on the synthesized dataset. Experiment results demonstrate that WebShaper achieves state-of-the-art performance among open-sourced IS agents on GAIA and WebWalkerQA benchmarks. 13 authors · Published on Jul 20, 2025 Upvote 54 GitHub 6.54k arXiv Page Submitted by callanwu WebShaper: Agentically Data Synthesizing via Information-Seeking Formalization The advent of Large Language Model (LLM)-powered agents has revolutionized artificial intelligence by enabling solutions to complex, open-ended tasks through web-based information-seeking (IS) capabilities. The scarcity of high-quality training data has limited the development of IS agents. Existing approaches typically adopt an information-driven paradigm that first collects web data and then generates questions based on the retrieval. However, this may lead to inconsistency between information structure and reasoning structure, question and answer. To mitigate, we propose a formalization-driven IS data synthesis framework WebShaper to construct a dataset. WebShaper systematically formalizes IS tasks through set theory. Central to the formalization is the concept of Knowledge Projections (KP), which enables precise control over reasoning structure by KP operation compositions. During synthesis, we begin by creating seed tasks, then use a multi-step expansion process. At each step, an agentic Expander expands the current formal question more complex with retrieval and validation tools based on our formalization. We train our model on the synthesized dataset. Experiment results demonstrate that WebShaper achieves state-of-the-art performance among open-sourced IS agents on GAIA and WebWalkerQA benchmarks. 13 authors · Jul 20, 2025 Upvote 54 GitHub 6.54k arXiv Page Submitted by richardxp888 WebWatcher: Breaking New Frontier of Vision-Language Deep Research Agent Web agents such as Deep Research have demonstrated superhuman cognitive abilities, capable of solving highly challenging information-seeking problems. However, most research remains primarily text-centric, overlooking visual information in the real world. This makes multimodal Deep Research highly challenging, as such agents require much stronger reasoning abilities in perception, logic, knowledge, and the use of more sophisticated tools compared to text-based agents. To address this limitation, we introduce WebWatcher, a multi-modal Agent for Deep Research equipped with enhanced visual-language reasoning capabilities. It leverages high-quality synthetic multimodal trajectories for efficient cold start training, utilizes various tools for deep reasoning, and further enhances generalization through reinforcement learning. To better evaluate the capabilities of multimodal agents, we propose BrowseComp-VL, a benchmark with BrowseComp-style that requires complex information retrieval involving both visual and textual information. Experimental results show that WebWatcher significantly outperforms proprietary baseline, RAG workflow and open-source agents in four challenging VQA benchmarks, which paves the way for solving complex multimodal information-seeking tasks. 14 authors · Published on Aug 7, 2025 Upvote 124 GitHub 6.54k arXiv Page Submitted by richardxp888 WebWatcher: Breaking New Frontier of Vision-Language Deep Research Agent Web agents such as Deep Research have demonstrated superhuman cognitive abilities, capable of solving highly challenging information-seeking problems. However, most research remains primarily text-centric, overlooking visual information in the real world. This makes multimodal Deep Research highly challenging, as such agents require much stronger reasoning abilities in perception, logic, knowledge, and the use of more sophisticated tools compared to text-based agents. To address this limitation, we introduce WebWatcher, a multi-modal Agent for Deep Research equipped with enhanced visual-language reasoning capabilities. It leverages high-quality synthetic multimodal trajectories for efficient cold start training, utilizes various tools for deep reasoning, and further enhances generalization through reinforcement learning. To better evaluate the capabilities of multimodal agents, we propose BrowseComp-VL, a benchmark with BrowseComp-style that requires complex information retrieval involving both visual and textual information. Experimental results show that WebWatcher significantly outperforms proprietary baseline, RAG workflow and open-source agents in four challenging VQA benchmarks, which paves the way for solving complex multimodal information-seeking tasks. 14 authors · Aug 7, 2025 Upvote 124 GitHub 6.54k arXiv Page Submitted by taesiri ST-Raptor: LLM-Powered Semi-Structured Table Question Answering Semi-structured tables, widely used in real-world applications (e.g., financial reports, medical records, transactional orders), often involve flexible and complex layouts (e.g., hierarchical headers and merged cells). These tables generally rely on human analysts to interpret table layouts and answer relevant natural language questions, which is costly and inefficient. To automate the procedure, existing methods face significant challenges. First, methods like NL2SQL require converting semi-structured tables into structured ones, which often causes substantial information loss. Second, methods like NL2Code and multi-modal LLM QA struggle to understand the complex layouts of semi-structured tables and cannot accurately answer corresponding questions. To this end, we propose ST-Raptor, a tree-based framework for semi-structured table question answering using large language models. First, we introduce the Hierarchical Orthogonal Tree (HO-Tree), a structural model that captures complex semi-structured table layouts, along with an effective algorithm for constructing the tree. Second, we define a set of basic tree operations to guide LLMs in executing common QA tasks. Given a user question, ST-Raptor decomposes it into simpler sub-questions, generates corresponding tree operation pipelines, and conducts operation-table alignment for accurate pipeline execution. Third, we incorporate a two-stage verification mechanism: forward validation checks the correctness of execution steps, while backward validation evaluates answer reliability by reconstructing queries from predicted answers. To benchmark the performance, we present SSTQA, a dataset of 764 questions over 102 real-world semi-structured tables. Experiments show that ST-Raptor outperforms nine baselines by up to 20% in answer accuracy. The code is available at https://github.com/weAIDB/ST-Raptor. 9 authors · Published on Aug 25, 2025 Upvote 6 GitHub 46 arXiv Page Submitted by taesiri ST-Raptor: LLM-Powered Semi-Structured Table Question Answering Semi-structured tables, widely used in real-world applications (e.g., financial reports, medical records, transactional orders), often involve flexible and complex layouts (e.g., hierarchical headers and merged cells). These tables generally rely on human analysts to interpret table layouts and answer relevant natural language questions, which is costly and inefficient. To automate the procedure, existing methods face significant challenges. First, methods like NL2SQL require converting semi-structured tables into structured ones, which often causes substantial information loss. Second, methods like NL2Code and multi-modal LLM QA struggle to understand the complex layouts of semi-structured tables and cannot accurately answer corresponding questions. To this end, we propose ST-Raptor, a tree-based framework for semi-structured table question answering using large language models. First, we introduce the Hierarchical Orthogonal Tree (HO-Tree), a structural model that captures complex semi-structured table layouts, along with an effective algorithm for constructing the tree. Second, we define a set of basic tree operations to guide LLMs in executing common QA tasks. Given a user question, ST-Raptor decomposes it into simpler sub-questions, generates corresponding tree operation pipelines, and conducts operation-table alignment for accurate pipeline execution. Third, we incorporate a two-stage verification mechanism: forward validation checks the correctness of execution steps, while backward validation evaluates answer reliability by reconstructing queries from predicted answers. To benchmark the performance, we present SSTQA, a dataset of 764 questions over 102 real-world semi-structured tables. Experiments show that ST-Raptor outperforms nine baselines by up to 20% in answer accuracy. The code is available at https://github.com/weAIDB/ST-Raptor. 9 authors · Aug 25, 2025 Upvote 6 GitHub 46 arXiv Page Submitted by hba123 Ark: An Open-source Python-based Framework for Robot Learning Robotics has made remarkable hardware strides-from DARPA's Urban and Robotics Challenges to the first humanoid-robot kickboxing tournament-yet commercial autonomy still lags behind progress in machine learning. A major bottleneck is software: current robot stacks demand steep learning curves, low-level C/C++ expertise, fragmented tooling, and intricate hardware integration, in stark contrast to the Python-centric, well-documented ecosystems that propelled modern AI. We introduce ARK, an open-source, Python-first robotics framework designed to close that gap. ARK presents a Gym-style environment interface that allows users to collect data, preprocess it, and train policies using state-of-the-art imitation-learning algorithms (e.g., ACT, Diffusion Policy) while seamlessly toggling between high-fidelity simulation and physical robots. A lightweight client-server architecture provides networked publisher-subscriber communication, and optional C/C++ bindings ensure real-time performance when needed. ARK ships with reusable modules for control, SLAM, motion planning, system identification, and visualization, along with native ROS interoperability. Comprehensive documentation and case studies-from manipulation to mobile navigation-demonstrate rapid prototyping, effortless hardware swapping, and end-to-end pipelines that rival the convenience of mainstream machine-learning workflows. By unifying robotics and AI practices under a common Python umbrella, ARK lowers entry barriers and accelerates research and commercial deployment of autonomous robots. 13 authors · Published on Jun 24, 2025 Upvote 16 GitHub 214 arXiv Page Submitted by hba123 Ark: An Open-source Python-based Framework for Robot Learning Robotics has made remarkable hardware strides-from DARPA's Urban and Robotics Challenges to the first humanoid-robot kickboxing tournament-yet commercial autonomy still lags behind progress in machine learning. A major bottleneck is software: current robot stacks demand steep learning curves, low-level C/C++ expertise, fragmented tooling, and intricate hardware integration, in stark contrast to the Python-centric, well-documented ecosystems that propelled modern AI. We introduce ARK, an open-source, Python-first robotics framework designed to close that gap. ARK presents a Gym-style environment interface that allows users to collect data, preprocess it, and train policies using state-of-the-art imitation-learning algorithms (e.g., ACT, Diffusion Policy) while seamlessly toggling between high-fidelity simulation and physical robots. A lightweight client-server architecture provides networked publisher-subscriber communication, and optional C/C++ bindings ensure real-time performance when needed. ARK ships with reusable modules for control, SLAM, motion planning, system identification, and visualization, along with native ROS interoperability. Comprehensive documentation and case studies-from manipulation to mobile navigation-demonstrate rapid prototyping, effortless hardware swapping, and end-to-end pipelines that rival the convenience of mainstream machine-learning workflows. By unifying robotics and AI practices under a common Python umbrella, ARK lowers entry barriers and accelerates research and commercial deployment of autonomous robots. 13 authors · Jun 24, 2025 Upvote 16 GitHub 214 arXiv Page Submitted by NCJ RenderFormer: Transformer-based Neural Rendering of Triangle Meshes with Global Illumination We present RenderFormer, a neural rendering pipeline that directly renders an image from a triangle-based representation of a scene with full global illumination effects and that does not require per-scene training or fine-tuning. Instead of taking a physics-centric approach to rendering, we formulate rendering as a sequence-to-sequence transformation where a sequence of tokens representing triangles with reflectance properties is converted to a sequence of output tokens representing small patches of pixels. RenderFormer follows a two stage pipeline: a view-independent stage that models triangle-to-triangle light transport, and a view-dependent stage that transforms a token representing a bundle of rays to the corresponding pixel values guided by the triangle-sequence from the view-independent stage. Both stages are based on the transformer architecture and are learned with minimal prior constraints. We demonstrate and evaluate RenderFormer on scenes with varying complexity in shape and light transport. 5 authors · Published on May 28, 2025 Upvote 37 GitHub 759 arXiv Page Submitted by NCJ RenderFormer: Transformer-based Neural Rendering of Triangle Meshes with Global Illumination We present RenderFormer, a neural rendering pipeline that directly renders an image from a triangle-based representation of a scene with full global illumination effects and that does not require per-scene training or fine-tuning. Instead of taking a physics-centric approach to rendering, we formulate rendering as a sequence-to-sequence transformation where a sequence of tokens representing triangles with reflectance properties is converted to a sequence of output tokens representing small patches of pixels. RenderFormer follows a two stage pipeline: a view-independent stage that models triangle-to-triangle light transport, and a view-dependent stage that transforms a token representing a bundle of rays to the corresponding pixel values guided by the triangle-sequence from the view-independent stage. Both stages are based on the transformer architecture and are learned with minimal prior constraints. We demonstrate and evaluate RenderFormer on scenes with varying complexity in shape and light transport. 5 authors · May 28, 2025 Upvote 37 GitHub 759 arXiv Page Submitted by aopolin-lv F1: A Vision-Language-Action Model Bridging Understanding and Generation to Actions Executing language-conditioned tasks in dynamic visual environments remains a central challenge in embodied AI. Existing Vision-Language-Action (VLA) models predominantly adopt reactive state-to-action mappings, often leading to short-sighted behaviors and poor robustness in dynamic scenes. In this paper, we introduce F1, a pretrained VLA framework which integrates the visual foresight generation into decision-making pipeline. F1 adopts a Mixture-of-Transformer architecture with dedicated modules for perception, foresight generation, and control, thereby bridging understanding, generation, and actions. At its core, F1 employs a next-scale prediction mechanism to synthesize goal-conditioned visual foresight as explicit planning targets. By forecasting plausible future visual states, F1 reformulates action generation as a foresight-guided inverse dynamics problem, enabling actions that implicitly achieve visual goals. To endow F1 with robust and generalizable capabilities, we propose a three-stage training recipe on an extensive dataset comprising over 330k trajectories across 136 diverse tasks. This training scheme enhances modular reasoning and equips the model with transferable visual foresight, which is critical for complex and dynamic environments. Extensive evaluations on real-world tasks and simulation benchmarks demonstrate F1 consistently outperforms existing approaches, achieving substantial gains in both task success rate and generalization ability. 10 authors · Published on Sep 8, 2025 Upvote 25 GitHub 63 arXiv Page Submitted by aopolin-lv F1: A Vision-Language-Action Model Bridging Understanding and Generation to Actions Executing language-conditioned tasks in dynamic visual environments remains a central challenge in embodied AI. Existing Vision-Language-Action (VLA) models predominantly adopt reactive state-to-action mappings, often leading to short-sighted behaviors and poor robustness in dynamic scenes. In this paper, we introduce F1, a pretrained VLA framework which integrates the visual foresight generation into decision-making pipeline. F1 adopts a Mixture-of-Transformer architecture with dedicated modules for perception, foresight generation, and control, thereby bridging understanding, generation, and actions. At its core, F1 employs a next-scale prediction mechanism to synthesize goal-conditioned visual foresight as explicit planning targets. By forecasting plausible future visual states, F1 reformulates action generation as a foresight-guided inverse dynamics problem, enabling actions that implicitly achieve visual goals. To endow F1 with robust and generalizable capabilities, we propose a three-stage training recipe on an extensive dataset comprising over 330k trajectories across 136 diverse tasks. This training scheme enhances modular reasoning and equips the model with transferable visual foresight, which is critical for complex and dynamic environments. Extensive evaluations on real-world tasks and simulation benchmarks demonstrate F1 consistently outperforms existing approaches, achieving substantial gains in both task success rate and generalization ability. 10 authors · Sep 8, 2025 Upvote 25 GitHub 63 arXiv Page Submitted by ashawkey Efficient Part-level 3D Object Generation via Dual Volume Packing Recent progress in 3D object generation has greatly improved both the quality and efficiency. However, most existing methods generate a single mesh with all parts fused together, which limits the ability to edit or manipulate individual parts. A key challenge is that different objects may have a varying number of parts. To address this, we propose a new end-to-end framework for part-level 3D object generation. Given a single input image, our method generates high-quality 3D objects with an arbitrary number of complete and semantically meaningful parts. We introduce a dual volume packing strategy that organizes all parts into two complementary volumes, allowing for the creation of complete and interleaved parts that assemble into the final object. Experiments show that our model achieves better quality, diversity, and generalization than previous image-based part-level generation methods. 10 authors · Published on Jun 11, 2025 Upvote 8 GitHub 672 arXiv Page Submitted by ashawkey Efficient Part-level 3D Object Generation via Dual Volume Packing Recent progress in 3D object generation has greatly improved both the quality and efficiency. However, most existing methods generate a single mesh with all parts fused together, which limits the ability to edit or manipulate individual parts. A key challenge is that different objects may have a varying number of parts. To address this, we propose a new end-to-end framework for part-level 3D object generation. Given a single input image, our method generates high-quality 3D objects with an arbitrary number of complete and semantically meaningful parts. We introduce a dual volume packing strategy that organizes all parts into two complementary volumes, allowing for the creation of complete and interleaved parts that assemble into the final object. Experiments show that our model achieves better quality, diversity, and generalization than previous image-based part-level generation methods. 10 authors · Jun 11, 2025 Upvote 8 GitHub 672 arXiv Page Submitted by X-iZhang A Comprehensive Survey of Self-Evolving AI Agents: A New Paradigm Bridging Foundation Models and Lifelong Agentic Systems Recent advances in large language models have sparked growing interest in AI agents capable of solving complex, real-world tasks. However, most existing agent systems rely on manually crafted configurations that remain static after deployment, limiting their ability to adapt to dynamic and evolving environments. To this end, recent research has explored agent evolution techniques that aim to automatically enhance agent systems based on interaction data and environmental feedback. This emerging direction lays the foundation for self-evolving AI agents, which bridge the static capabilities of foundation models with the continuous adaptability required by lifelong agentic systems. In this survey, we provide a comprehensive review of existing techniques for self-evolving agentic systems. Specifically, we first introduce a unified conceptual framework that abstracts the feedback loop underlying the design of self-evolving agentic systems. The framework highlights four key components: System Inputs, Agent System, Environment, and Optimisers, serving as a foundation for understanding and comparing different strategies. Based on this framework, we systematically review a wide range of self-evolving techniques that target different components of the agent system. We also investigate domain-specific evolution strategies developed for specialised fields such as biomedicine, programming, and finance, where optimisation objectives are tightly coupled with domain constraints. In addition, we provide a dedicated discussion on the evaluation, safety, and ethical considerations for self-evolving agentic systems, which are critical to ensuring their effectiveness and reliability. This survey aims to provide researchers and practitioners with a systematic understanding of self-evolving AI agents, laying the foundation for the development of more adaptive, autonomous, and lifelong agentic systems. 15 authors · Published on Aug 10, 2025 Upvote 92 GitHub 899 arXiv Page Submitted by X-iZhang A Comprehensive Survey of Self-Evolving AI Agents: A New Paradigm Bridging Foundation Models and Lifelong Agentic Systems Recent advances in large language models have sparked growing interest in AI agents capable of solving complex, real-world tasks. However, most existing agent systems rely on manually crafted configurations that remain static after deployment, limiting their ability to adapt to dynamic and evolving environments. To this end, recent research has explored agent evolution techniques that aim to automatically enhance agent systems based on interaction data and environmental feedback. This emerging direction lays the foundation for self-evolving AI agents, which bridge the static capabilities of foundation models with the continuous adaptability required by lifelong agentic systems. In this survey, we provide a comprehensive review of existing techniques for self-evolving agentic systems. Specifically, we first introduce a unified conceptual framework that abstracts the feedback loop underlying the design of self-evolving agentic systems. The framework highlights four key components: System Inputs, Agent System, Environment, and Optimisers, serving as a foundation for understanding and comparing different strategies. Based on this framework, we systematically review a wide range of self-evolving techniques that target different components of the agent system. We also investigate domain-specific evolution strategies developed for specialised fields such as biomedicine, programming, and finance, where optimisation objectives are tightly coupled with domain constraints. In addition, we provide a dedicated discussion on the evaluation, safety, and ethical considerations for self-evolving agentic systems, which are critical to ensuring their effectiveness and reliability. This survey aims to provide researchers and practitioners with a systematic understanding of self-evolving AI agents, laying the foundation for the development of more adaptive, autonomous, and lifelong agentic systems. 15 authors · Aug 10, 2025 Upvote 92 GitHub 899 arXiv Page Submitted by lynazhang rStar2-Agent: Agentic Reasoning Technical Report We introduce rStar2-Agent, a 14B math reasoning model trained with agentic reinforcement learning to achieve frontier-level performance. Beyond current long CoT, the model demonstrates advanced cognitive behaviors, such as thinking carefully before using Python coding tools and reflecting on code execution feedback to autonomously explore, verify, and refine intermediate steps in complex problem-solving. This capability is enabled through three key innovations that makes agentic RL effective at scale: (i) an efficient RL infrastructure with a reliable Python code environment that supports high-throughput execution and mitigates the high rollout costs, enabling training on limited GPU resources (64 MI300X GPUs); (ii) GRPO-RoC, an agentic RL algorithm with a Resample-on-Correct rollout strategy that addresses the inherent environment noises from coding tools, allowing the model to reason more effectively in a code environment; (iii) An efficient agent training recipe that starts with non-reasoning SFT and progresses through multi-RL stages, yielding advanced cognitive abilities with minimal compute cost. To this end, rStar2-Agent boosts a pre-trained 14B model to state of the art in only 510 RL steps within one week, achieving average pass@1 scores of 80.6% on AIME24 and 69.8% on AIME25, surpassing DeepSeek-R1 (671B) with significantly shorter responses. Beyond mathematics, rStar2-Agent-14B also demonstrates strong generalization to alignment, scientific reasoning, and agentic tool-use tasks. Code and training recipes are available at https://github.com/microsoft/rStar. 15 authors · Published on Aug 28, 2025 Upvote 102 GitHub 1.15k arXiv Page Submitted by lynazhang rStar2-Agent: Agentic Reasoning Technical Report We introduce rStar2-Agent, a 14B math reasoning model trained with agentic reinforcement learning to achieve frontier-level performance. Beyond current long CoT, the model demonstrates advanced cognitive behaviors, such as thinking carefully before using Python coding tools and reflecting on code execution feedback to autonomously explore, verify, and refine intermediate steps in complex problem-solving. This capability is enabled through three key innovations that makes agentic RL effective at scale: (i) an efficient RL infrastructure with a reliable Python code environment that supports high-throughput execution and mitigates the high rollout costs, enabling training on limited GPU resources (64 MI300X GPUs); (ii) GRPO-RoC, an agentic RL algorithm with a Resample-on-Correct rollout strategy that addresses the inherent environment noises from coding tools, allowing the model to reason more effectively in a code environment; (iii) An efficient agent training recipe that starts with non-reasoning SFT and progresses through multi-RL stages, yielding advanced cognitive abilities with minimal compute cost. To this end, rStar2-Agent boosts a pre-trained 14B model to state of the art in only 510 RL steps within one week, achieving average pass@1 scores of 80.6% on AIME24 and 69.8% on AIME25, surpassing DeepSeek-R1 (671B) with significantly shorter responses. Beyond mathematics, rStar2-Agent-14B also demonstrates strong generalization to alignment, scientific reasoning, and agentic tool-use tasks. Code and training recipes are available at https://github.com/microsoft/rStar. 15 authors · Aug 28, 2025 Upvote 102 GitHub 1.15k arXiv Page Submitted by tqliu 4DNeX: Feed-Forward 4D Generative Modeling Made Easy We present 4DNeX, the first feed-forward framework for generating 4D (i.e., dynamic 3D) scene representations from a single image. In contrast to existing methods that rely on computationally intensive optimization or require multi-frame video inputs, 4DNeX enables efficient, end-to-end image-to-4D generation by fine-tuning a pretrained video diffusion model. Specifically, 1) to alleviate the scarcity of 4D data, we construct 4DNeX-10M, a large-scale dataset with high-quality 4D annotations generated using advanced reconstruction approaches. 2) we introduce a unified 6D video representation that jointly models RGB and XYZ sequences, facilitating structured learning of both appearance and geometry. 3) we propose a set of simple yet effective adaptation strategies to repurpose pretrained video diffusion models for 4D modeling. 4DNeX produces high-quality dynamic point clouds that enable novel-view video synthesis. Extensive experiments demonstrate that 4DNeX outperforms existing 4D generation methods in efficiency and generalizability, offering a scalable solution for image-to-4D modeling and laying the foundation for generative 4D world models that simulate dynamic scene evolution. 9 authors · Published on Aug 18, 2025 Upvote 58 GitHub 449 arXiv Page Submitted by tqliu 4DNeX: Feed-Forward 4D Generative Modeling Made Easy We present 4DNeX, the first feed-forward framework for generating 4D (i.e., dynamic 3D) scene representations from a single image. In contrast to existing methods that rely on computationally intensive optimization or require multi-frame video inputs, 4DNeX enables efficient, end-to-end image-to-4D generation by fine-tuning a pretrained video diffusion model. Specifically, 1) to alleviate the scarcity of 4D data, we construct 4DNeX-10M, a large-scale dataset with high-quality 4D annotations generated using advanced reconstruction approaches. 2) we introduce a unified 6D video representation that jointly models RGB and XYZ sequences, facilitating structured learning of both appearance and geometry. 3) we propose a set of simple yet effective adaptation strategies to repurpose pretrained video diffusion models for 4D modeling. 4DNeX produces high-quality dynamic point clouds that enable novel-view video synthesis. Extensive experiments demonstrate that 4DNeX outperforms existing 4D generation methods in efficiency and generalizability, offering a scalable solution for image-to-4D modeling and laying the foundation for generative 4D world models that simulate dynamic scene evolution. 9 authors · Aug 18, 2025 Upvote 58 GitHub 449 arXiv Page Submitted by skicy Step-Audio 2 Technical Report This paper presents Step-Audio~2, an end-to-end multi-modal large language model designed for industry-strength audio understanding and speech conversation. By integrating a latent audio encoder and reasoning-centric reinforcement learning (RL), Step-Audio 2 achieves promising performance in automatic speech recognition (ASR) and audio understanding. To facilitate genuine end-to-end speech conversation, Step-Audio 2 incorporates the generation of discrete audio tokens into language modeling, significantly enhancing its responsiveness to paralinguistic information such as speaking styles and emotions. To effectively leverage the rich textual and acoustic knowledge in real-world data, Step-Audio 2 integrates retrieval-augmented generation (RAG) and is able to call external tools such as web search to mitigate hallucination and audio search to switch timbres. Trained on millions of hours of speech and audio data, Step-Audio 2 delivers intelligence and expressiveness across diverse conversational scenarios. Evaluation results demonstrate that Step-Audio 2 achieves state-of-the-art performance on various audio understanding and conversational benchmarks compared to other open-source and commercial solutions. Please visit https://github.com/stepfun-ai/Step-Audio2 for more information. 109 authors · Published on Jul 22, 2025 Upvote 71 GitHub 1.03k arXiv Page Submitted by skicy Step-Audio 2 Technical Report This paper presents Step-Audio~2, an end-to-end multi-modal large language model designed for industry-strength audio understanding and speech conversation. By integrating a latent audio encoder and reasoning-centric reinforcement learning (RL), Step-Audio 2 achieves promising performance in automatic speech recognition (ASR) and audio understanding. To facilitate genuine end-to-end speech conversation, Step-Audio 2 incorporates the generation of discrete audio tokens into language modeling, significantly enhancing its responsiveness to paralinguistic information such as speaking styles and emotions. To effectively leverage the rich textual and acoustic knowledge in real-world data, Step-Audio 2 integrates retrieval-augmented generation (RAG) and is able to call external tools such as web search to mitigate hallucination and audio search to switch timbres. Trained on millions of hours of speech and audio data, Step-Audio 2 delivers intelligence and expressiveness across diverse conversational scenarios. Evaluation results demonstrate that Step-Audio 2 achieves state-of-the-art performance on various audio understanding and conversational benchmarks compared to other open-source and commercial solutions. Please visit https://github.com/stepfun-ai/Step-Audio2 for more information. 109 authors · Jul 22, 2025 Upvote 71 GitHub 1.03k arXiv Page Submitted by taesiri InternVL3.5: Advancing Open-Source Multimodal Models in Versatility, Reasoning, and Efficiency We introduce InternVL 3.5, a new family of open-source multimodal models that significantly advances versatility, reasoning capability, and inference efficiency along the InternVL series. A key innovation is the Cascade Reinforcement Learning (Cascade RL) framework, which enhances reasoning through a two-stage process: offline RL for stable convergence and online RL for refined alignment. This coarse-to-fine training strategy leads to substantial improvements on downstream reasoning tasks, e.g., MMMU and MathVista. To optimize efficiency, we propose a Visual Resolution Router (ViR) that dynamically adjusts the resolution of visual tokens without compromising performance. Coupled with ViR, our Decoupled Vision-Language Deployment (DvD) strategy separates the vision encoder and language model across different GPUs, effectively balancing computational load. These contributions collectively enable InternVL3.5 to achieve up to a +16.0\\% gain in overall reasoning performance and a 4.05times inference speedup compared to its predecessor, i.e., InternVL3. In addition, InternVL3.5 supports novel capabilities such as GUI interaction and embodied agency. Notably, our largest model, i.e., InternVL3.5-241B-A28B, attains state-of-the-art results among open-source MLLMs across general multimodal, reasoning, text, and agentic tasks -- narrowing the performance gap with leading commercial models like GPT-5. All models and code are publicly released. 61 authors · Published on Aug 25, 2025 Upvote 185 GitHub 9.11k arXiv Page Submitted by taesiri InternVL3.5: Advancing Open-Source Multimodal Models in Versatility, Reasoning, and Efficiency We introduce InternVL 3.5, a new family of open-source multimodal models that significantly advances versatility, reasoning capability, and inference efficiency along the InternVL series. A key innovation is the Cascade Reinforcement Learning (Cascade RL) framework, which enhances reasoning through a two-stage process: offline RL for stable convergence and online RL for refined alignment. This coarse-to-fine training strategy leads to substantial improvements on downstream reasoning tasks, e.g., MMMU and MathVista. To optimize efficiency, we propose a Visual Resolution Router (ViR) that dynamically adjusts the resolution of visual tokens without compromising performance. Coupled with ViR, our Decoupled Vision-Language Deployment (DvD) strategy separates the vision encoder and language model across different GPUs, effectively balancing computational load. These contributions collectively enable InternVL3.5 to achieve up to a +16.0\\% gain in overall reasoning performance and a 4.05times inference speedup compared to its predecessor, i.e., InternVL3. In addition, InternVL3.5 supports novel capabilities such as GUI interaction and embodied agency. Notably, our largest model, i.e., InternVL3.5-241B-A28B, attains state-of-the-art results among open-source MLLMs across general multimodal, reasoning, text, and agentic tasks -- narrowing the performance gap with leading commercial models like GPT-5. All models and code are publicly released. 61 authors · Aug 25, 2025 Upvote 185 GitHub 9.11k arXiv Page Submitted by yifanzhang114 Aligning Multimodal LLM with Human Preference: A Survey Large language models (LLMs) can handle a wide variety of general tasks with simple prompts, without the need for task-specific training. Multimodal Large Language Models (MLLMs), built upon LLMs, have demonstrated impressive potential in tackling complex tasks involving visual, auditory, and textual data. However, critical issues related to truthfulness, safety, o1-like reasoning, and alignment with human preference remain insufficiently addressed. This gap has spurred the emergence of various alignment algorithms, each targeting different application scenarios and optimization goals. Recent studies have shown that alignment algorithms are a powerful approach to resolving the aforementioned challenges. In this paper, we aim to provide a comprehensive and systematic review of alignment algorithms for MLLMs. Specifically, we explore four key aspects: (1) the application scenarios covered by alignment algorithms, including general image understanding, multi-image, video, and audio, and extended multimodal applications; (2) the core factors in constructing alignment datasets, including data sources, model responses, and preference annotations; (3) the benchmarks used to evaluate alignment algorithms; and (4) a discussion of potential future directions for the development of alignment algorithms. This work seeks to help researchers organize current advancements in the field and inspire better alignment methods. The project page of this paper is available at https://github.com/BradyFU/Awesome-Multimodal-Large-Language-Models/tree/Alignment. 17 authors · Published on Mar 18, 2025 Upvote 26 GitHub 16.2k arXiv Page Submitted by yifanzhang114 Aligning Multimodal LLM with Human Preference: A Survey Large language models (LLMs) can handle a wide variety of general tasks with simple prompts, without the need for task-specific training. Multimodal Large Language Models (MLLMs), built upon LLMs, have demonstrated impressive potential in tackling complex tasks involving visual, auditory, and textual data. However, critical issues related to truthfulness, safety, o1-like reasoning, and alignment with human preference remain insufficiently addressed. This gap has spurred the emergence of various alignment algorithms, each targeting different application scenarios and optimization goals. Recent studies have shown that alignment algorithms are a powerful approach to resolving the aforementioned challenges. In this paper, we aim to provide a comprehensive and systematic review of alignment algorithms for MLLMs. Specifically, we explore four key aspects: (1) the application scenarios covered by alignment algorithms, including general image understanding, multi-image, video, and audio, and extended multimodal applications; (2) the core factors in constructing alignment datasets, including data sources, model responses, and preference annotations; (3) the benchmarks used to evaluate alignment algorithms; and (4) a discussion of potential future directions for the development of alignment algorithms. This work seeks to help researchers organize current advancements in the field and inspire better alignment methods. The project page of this paper is available at https://github.com/BradyFU/Awesome-Multimodal-Large-Language-Models/tree/Alignment. 17 authors · Mar 18, 2025 Upvote 26 GitHub 16.2k arXiv Page Submitted by Chevalier A Survey of Context Engineering for Large Language Models The performance of Large Language Models (LLMs) is fundamentally determined by the contextual information provided during inference. This survey introduces Context Engineering, a formal discipline that transcends simple prompt design to encompass the systematic optimization of information payloads for LLMs. We present a comprehensive taxonomy decomposing Context Engineering into its foundational components and the sophisticated implementations that integrate them into intelligent systems. We first examine the foundational components: context retrieval and generation, context processing and context management. We then explore how these components are architecturally integrated to create sophisticated system implementations: retrieval-augmented generation (RAG), memory systems and tool-integrated reasoning, and multi-agent systems. Through this systematic analysis of over 1300 research papers, our survey not only establishes a technical roadmap for the field but also reveals a critical research gap: a fundamental asymmetry exists between model capabilities. While current models, augmented by advanced context engineering, demonstrate remarkable proficiency in understanding complex contexts, they exhibit pronounced limitations in generating equally sophisticated, long-form outputs. Addressing this gap is a defining priority for future research. Ultimately, this survey provides a unified framework for both researchers and engineers advancing context-aware AI. 15 authors · Published on Jul 17, 2025 Upvote 252 GitHub 2.18k arXiv Page Submitted by Chevalier A Survey of Context Engineering for Large Language Models The performance of Large Language Models (LLMs) is fundamentally determined by the contextual information provided during inference. This survey introduces Context Engineering, a formal discipline that transcends simple prompt design to encompass the systematic optimization of information payloads for LLMs. We present a comprehensive taxonomy decomposing Context Engineering into its foundational components and the sophisticated implementations that integrate them into intelligent systems. We first examine the foundational components: context retrieval and generation, context processing and context management. We then explore how these components are architecturally integrated to create sophisticated system implementations: retrieval-augmented generation (RAG), memory systems and tool-integrated reasoning, and multi-agent systems. Through this systematic analysis of over 1300 research papers, our survey not only establishes a technical roadmap for the field but also reveals a critical research gap: a fundamental asymmetry exists between model capabilities. While current models, augmented by advanced context engineering, demonstrate remarkable proficiency in understanding complex contexts, they exhibit pronounced limitations in generating equally sophisticated, long-form outputs. Addressing this gap is a defining priority for future research. Ultimately, this survey provides a unified framework for both researchers and engineers advancing context-aware AI. 15 authors · Jul 17, 2025 Upvote 252 GitHub 2.18k arXiv Page Submitted by luojunyu Large Language Model Agent: A Survey on Methodology, Applications and Challenges The era of intelligent agents is upon us, driven by revolutionary advancements in large language models. Large Language Model (LLM) agents, with goal-driven behaviors and dynamic adaptation capabilities, potentially represent a critical pathway toward artificial general intelligence. This survey systematically deconstructs LLM agent systems through a methodology-centered taxonomy, linking architectural foundations, collaboration mechanisms, and evolutionary pathways. We unify fragmented research threads by revealing fundamental connections between agent design principles and their emergent behaviors in complex environments. Our work provides a unified architectural perspective, examining how agents are constructed, how they collaborate, and how they evolve over time, while also addressing evaluation methodologies, tool applications, practical challenges, and diverse application domains. By surveying the latest developments in this rapidly evolving field, we offer researchers a structured taxonomy for understanding LLM agents and identify promising directions for future research. The collection is available at https://github.com/luo-junyu/Awesome-Agent-Papers. 26 authors · Published on Mar 27, 2025 Upvote 82 GitHub 1.63k arXiv Page Submitted by luojunyu Large Language Model Agent: A Survey on Methodology, Applications and Challenges The era of intelligent agents is upon us, driven by revolutionary advancements in large language models. Large Language Model (LLM) agents, with goal-driven behaviors and dynamic adaptation capabilities, potentially represent a critical pathway toward artificial general intelligence. This survey systematically deconstructs LLM agent systems through a methodology-centered taxonomy, linking architectural foundations, collaboration mechanisms, and evolutionary pathways. We unify fragmented research threads by revealing fundamental connections between agent design principles and their emergent behaviors in complex environments. Our work provides a unified architectural perspective, examining how agents are constructed, how they collaborate, and how they evolve over time, while also addressing evaluation methodologies, tool applications, practical challenges, and diverse application domains. By surveying the latest developments in this rapidly evolving field, we offer researchers a structured taxonomy for understanding LLM agents and identify promising directions for future research. The collection is available at https://github.com/luo-junyu/Awesome-Agent-Papers. 26 authors · Mar 27, 2025 Upvote 82 GitHub 1.63k arXiv Page Submitted by taesiri Paper2Agent: Reimagining Research Papers As Interactive and Reliable AI Agents We introduce Paper2Agent, an automated framework that converts research papers into AI agents. Paper2Agent transforms research output from passive artifacts into active systems that can accelerate downstream use, adoption, and discovery. Conventional research papers require readers to invest substantial effort to understand and adapt a paper's code, data, and methods to their own work, creating barriers to dissemination and reuse. Paper2Agent addresses this challenge by automatically converting a paper into an AI agent that acts as a knowledgeable research assistant. It systematically analyzes the paper and the associated codebase using multiple agents to construct a Model Context Protocol (MCP) server, then iteratively generates and runs tests to refine and robustify the resulting MCP. These paper MCPs can then be flexibly connected to a chat agent (e.g. Claude Code) to carry out complex scientific queries through natural language while invoking tools and workflows from the original paper. We demonstrate Paper2Agent's effectiveness in creating reliable and capable paper agents through in-depth case studies. Paper2Agent created an agent that leverages AlphaGenome to interpret genomic variants and agents based on ScanPy and TISSUE to carry out single-cell and spatial transcriptomics analyses. We validate that these paper agents can reproduce the original paper's results and can correctly carry out novel user queries. By turning static papers into dynamic, interactive AI agents, Paper2Agent introduces a new paradigm for knowledge dissemination and a foundation for the collaborative ecosystem of AI co-scientists. 4 authors · Published on Sep 8, 2025 Upvote 21 GitHub 44 arXiv Page Submitted by taesiri Paper2Agent: Reimagining Research Papers As Interactive and Reliable AI Agents We introduce Paper2Agent, an automated framework that converts research papers into AI agents. Paper2Agent transforms research output from passive artifacts into active systems that can accelerate downstream use, adoption, and discovery. Conventional research papers require readers to invest substantial effort to understand and adapt a paper's code, data, and methods to their own work, creating barriers to dissemination and reuse. Paper2Agent addresses this challenge by automatically converting a paper into an AI agent that acts as a knowledgeable research assistant. It systematically analyzes the paper and the associated codebase using multiple agents to construct a Model Context Protocol (MCP) server, then iteratively generates and runs tests to refine and robustify the resulting MCP. These paper MCPs can then be flexibly connected to a chat agent (e.g. Claude Code) to carry out complex scientific queries through natural language while invoking tools and workflows from the original paper. We demonstrate Paper2Agent's effectiveness in creating reliable and capable paper agents through in-depth case studies. Paper2Agent created an agent that leverages AlphaGenome to interpret genomic variants and agents based on ScanPy and TISSUE to carry out single-cell and spatial transcriptomics analyses. We validate that these paper agents can reproduce the original paper's results and can correctly carry out novel user queries. By turning static papers into dynamic, interactive AI agents, Paper2Agent introduces a new paradigm for knowledge dissemination and a foundation for the collaborative ecosystem of AI co-scientists. 4 authors · Sep 8, 2025 Upvote 21 GitHub 44 arXiv Page System theme Company TOS Privacy About Jobs Website Models Datasets Spaces Pricing Docs ",
      "fetch_method": "direct-html"
    },
    {
      "arxiv_id": "https://arxiv.org/html/2406.09175v1",
      "full_text": " ReMI: A Dataset for Reasoning with Multiple Images 1 Introduction 2 Related Work 3 The 𝖱𝖾𝖬𝖨 𝖱𝖾𝖬𝖨 \\mathsf{ReMI} sansserif_ReMI Dataset 4 Experiments 4.1 Human Baseline Substantially Beats SoTA Models in Multi-Image Reasoning 4.2 Single-Image vs Multi-Image Reasoning 4.3 Failure Analysis 4.4 Performance as a Function of Task Properties 4.5 Zeroshot vs Fewshot Performance 5 Conclusion A Further Failure Analysis B Statistics about 𝖱𝖾𝖬𝖨 𝖱𝖾𝖬𝖨 \\mathsf{ReMI} sansserif_ReMI C Details about the Tasks in 𝖱𝖾𝖬𝖨 𝖱𝖾𝖬𝖨 \\mathsf{ReMI} sansserif_ReMI D Model Performance vs Human Time E Experimental Setup F Limitations \\correspondingauthor mehrankazemi@google.com ReMI: A Dataset for Reasoning with Multiple Images Mehran Kazemi Google DeepMind Nishanth Dikkala Google Research Ankit Anand Google DeepMind Petar Devic Google Ishita Dasgupta Google DeepMind Fangyu Liu Google DeepMind Bahare Fatemi Google Research Pranjal Awasthi Google Research Dee Guo Google Research Sreenivas Gollapudi Google Research Ahmed Qureshi Google Abstract With the continuous advancement of large language models (LLMs), it is essential to create new benchmarks to effectively evaluate their expanding capabilities and identify areas for improvement. This work focuses on multi-image reasoning, an emerging capability in state-of-the-art LLMs. We introduce 𝖱𝖾𝖬𝖨 𝖱𝖾𝖬𝖨 \\mathsf{ReMI} sansserif_ReMI , a dataset designed to assess LLMs’ ability to Re ason with M ultiple I mages. This dataset encompasses a diverse range of tasks, spanning various reasoning domains such as math, physics, logic, code, table/chart understanding, and spatial and temporal reasoning. It also covers a broad spectrum of characteristics found in multi-image reasoning scenarios. We have benchmarked several cutting-edge LLMs using 𝖱𝖾𝖬𝖨 𝖱𝖾𝖬𝖨 \\mathsf{ReMI} sansserif_ReMI and found a substantial gap between their performance and human-level proficiency. This highlights the challenges in multi-image reasoning and the need for further research. Our analysis also reveals the strengths and weaknesses of different models, shedding light on the types of reasoning that are currently attainable and areas where future models require improvement. To foster further research in this area, we are open-sourcing 𝖱𝖾𝖬𝖨 𝖱𝖾𝖬𝖨 \\mathsf{ReMI} sansserif_ReMI : https://huggingface.co/datasets/mehrankazemi/ReMI . 1 Introduction Large Language Models (LLMs) have demonstrated an extraordinary evolution, not only in their output quality but also in their burgeoning capabilities. A significant direction of development has been models’ ability to perform increasingly general forms of reasoning that were previously not possible. The emergence of these novel capabilities necessitates the development of robust evaluation benchmarks and metrics to measure and enhance model performance in these specific areas. The ability of LLMs to reason over text has improved in leaps and bounds, and has been studied extensively (Lewkowycz et al., 2022 ; Wei et al., 2022 ; Rajani et al., 2019 ) . More recent developments in multi-modal models has opened up a new space of reasoning problems, moving toward the capability to reason across multiple, potentially disparate, sources of information presented in various formats (Reid et al., 2024 ; Team et al., 2023 ; Achiam et al., 2023 ; Anthropic, 2024 ) . This multi-modal reasoning capability has numerous applications, from complex problem-solving to information synthesis. In this paper, we focus on a specific aspect of this capability: multi-image reasoning. A large portion of the current benchmarks for multi-modal evaluation are based on a single image (Lu et al., 2023 , 2022 ; Kazemi et al., 2023a ; Lu et al., 2021 ; Liu et al., 2023 ; Lindström and Abraham, 2022 ; Fu et al., 2023 ; Antol et al., 2015 ; Goyal et al., 2017 ; Marino et al., 2019 ) . We address the lack of dedicated evaluation frameworks in this domain by introducing a comprehensive benchmark designed to specifically assess and improve this skill in LLMs. Figure 1: Model performances on 𝖱𝖾𝖬𝖨 𝖱𝖾𝖬𝖨 \\mathsf{ReMI} sansserif_ReMI . We focus specifically on reasoning problems where besides visual understanding, one needs to find a step-by-step solution to a problem. This process often involves combining information across text and multiple images – a skill that is currently not extensively evaluated in existing benchmarks. This contribution aims to catalyze progress in multi-image reasoning, ultimately enabling LLMs to better navigate and extract insights from the increasingly complex information landscape of our digital world. We introduce 𝖱𝖾𝖬𝖨 𝖱𝖾𝖬𝖨 \\mathsf{ReMI} sansserif_ReMI , a new benchmark designed for Re asoning with M ultiple I mages. Our goal is to cover a broad spectrum of domains where integrating information across multiple modalities is necessary, as well as various key properties unique to multi-image reasoning. To achieve this, we have developed 13 tasks that span a range of domains and properties. The domains covered in 𝖱𝖾𝖬𝖨 𝖱𝖾𝖬𝖨 \\mathsf{ReMI} sansserif_ReMI include algebra, calculus, geometry, graph theory, physics, temporal and spatial/maps reasoning, tabular and chart understanding, coding, and logic. The properties covered by 𝖱𝖾𝖬𝖨 𝖱𝖾𝖬𝖨 \\mathsf{ReMI} sansserif_ReMI include sequential vs set consumption of image information, problems that require reasoning over images demonstrating a similar concept (e.g., two charts) or different concepts (e.g., geometry shape and a table), images that are interleaved or not interleaved with the text, and the number of separate images provided as input. Our tasks require reasoning over up to six images, with all tasks requiring reasoning over at least two images. Table 1 outlines the tasks, domains and properties. Our images comprise a variety of heterogeneous image types including charts, tables, equations, emojis, graphs, shapes, maps, clocks, physical objects, LaTeX diagrams, functions, etc. We evaluate state-of-the-art LLMs on 𝖱𝖾𝖬𝖨 𝖱𝖾𝖬𝖨 \\mathsf{ReMI} sansserif_ReMI and compare their performance to humans, showing that model performances remain substantially behind human performance (see Fig 1 ). Interestingly, our results also reveal that models may perform better when multiple images are fed to them separately as opposed to all in one image; this is especially true in the case where the images are interleaved with the question text. A detailed failure analysis reveals model shortcomings that can guide future improvement efforts. Table 1: The reasoning domain and properties of the tasks in 𝖱𝖾𝖬𝖨 𝖱𝖾𝖬𝖨 \\mathsf{ReMI} sansserif_ReMI . Task Name Reasoning Domain(s) Sequence or Set Same/Diff. Concept Interleaved Max #Images 𝖤𝗆𝗈𝗃𝗂𝖠𝗅𝗀𝖾𝖻𝗋𝖺 𝖤𝗆𝗈𝗃𝗂𝖠𝗅𝗀𝖾𝖻𝗋𝖺 \\mathsf{EmojiAlgebra} sansserif_EmojiAlgebra Algebra Seq Same Yes 6 𝖥𝗎𝗇𝖼𝖱𝖾𝖺𝖽 𝖥𝗎𝗇𝖼𝖱𝖾𝖺𝖽 \\mathsf{FuncRead} sansserif_FuncRead Calculus Mix Same Yes 3 𝖦𝖾𝗈𝗆𝖲𝗁𝖺𝗉𝖾𝗌 𝖦𝖾𝗈𝗆𝖲𝗁𝖺𝗉𝖾𝗌 \\mathsf{GeomShapes} sansserif_GeomShapes Geometry Seq Same No 2 𝖦𝖾𝗈𝗆𝖢𝗈𝗌𝗍 𝖦𝖾𝗈𝗆𝖢𝗈𝗌𝗍 \\mathsf{GeomCost} sansserif_GeomCost Geometry, Tabular Seq Diff Yes 2 𝖢𝗈𝗅𝗅𝗂𝗌𝗂𝗈𝗇𝗌 𝖢𝗈𝗅𝗅𝗂𝗌𝗂𝗈𝗇𝗌 \\mathsf{Collisions} sansserif_Collisions Physics Set Same No 2 𝖢𝗅𝗈𝖼𝗄𝗌 𝖢𝗅𝗈𝖼𝗄𝗌 \\mathsf{Clocks} sansserif_Clocks Time Arithmetic Set Same No 2 𝖲𝖼𝗁𝖾𝖽𝗎𝗅𝖾 𝖲𝖼𝗁𝖾𝖽𝗎𝗅𝖾 \\mathsf{Schedule} sansserif_Schedule Time, Tabular Seq Diff Yes 2 𝖢𝗁𝖺𝗋𝗍𝗌 𝖢𝗁𝖺𝗋𝗍𝗌 \\mathsf{Charts} sansserif_Charts Charts Set Same No 2 𝖢𝗈𝖽𝖾𝖤𝖽𝗂𝗍 𝖢𝗈𝖽𝖾𝖤𝖽𝗂𝗍 \\mathsf{CodeEdit} sansserif_CodeEdit Code Seq Same Yes 2 𝖨𝗌𝗈𝗆𝗈𝗋𝗉𝗁𝗂𝗌𝗆 𝖨𝗌𝗈𝗆𝗈𝗋𝗉𝗁𝗂𝗌𝗆 \\mathsf{Isomorphism} sansserif_Isomorphism Graph Theory Set Same Yes 2 𝖬𝖺𝗉𝗌 𝖬𝖺𝗉𝗌 \\mathsf{Maps} sansserif_Maps Spatial, Maps Mix Same Yes 4 𝖱𝖾𝖿𝖢𝖮𝖢𝖮 𝖱𝖾𝖿𝖢𝖮𝖢𝖮 \\mathsf{RefCOCO} sansserif_RefCOCO Spatial Seq Diff No 2 𝖨𝖰 𝖨𝖰 \\mathsf{IQ} sansserif_IQ Logic Mix Same Yes 5 2 Related Work Vision-language foundation models. In our work, we focus on vision language generation models, i.e. models that produce open-ended text conditioned on text and images. Frozen (Tsimpoukelli et al., 2021 ) and Flamingo (Alayrac et al., 2022 ) first transformed LLMs into vision-language models by adding a vision transformer tower and training cross/self-attention layers to enable LLMs to perceive visual information. Subsequently, a large volume of research emerged focusing on the approach of stitching a pretrained visual encoder (usually vision transformer) to a pretrained langauge model. PaLI (Chen et al., 2022 ) , BLIP (Li et al., 2023b ) , LLaVA (Liu et al., 2024 ) , OpenFlamingo (Awadalla et al., 2023 ) , PaLIGemma (Beyer* et al., 2024 ) all follow similar techniques. The latest closed-source frontier models such as GPT-4 (Achiam et al., 2023 ) , Gemini (Team et al., 2023 ) and Claude 3 Anthropic ( 2024 ) all have vision input support and are also reported to be the best performing models across popular vision-language reasoning benchmarks (Lu et al., 2023 ) . These frontier models are able to condition fairly arbitrarily on sequences of interleaved image and text. However, most vision-language benchmarks test models’ performance on a single image-text pair; the focus of this paper is to take a step toward evaluating more flexible vision-language abilities. Reasoning Benchmarks. Reasoning has been a core area of interest for NLP systems. The initial benchmarks focused on ‘simpler’ reasoning tasks which largely involve language understanding (e.g. SuperGLUE (Wang et al., 2019 ) , HellaSwag (Zellers et al., 2019 ) , Lambada (Paperno et al., 2016 ) ). With LLMs making remarkable strides in recent years, a plethora of benchmarks requiring much stronger reasoning abilities have emerged. Some of these like MMLU (Hendrycks et al., 2020 ) and ARC (Clark et al., 2018 ) focus on science questions. MATH (Hendrycks et al., 2021 ) , GSM8K (Cobbe et al., 2021 ) and MGSM (Shi et al., 2022 ) focus on mathematical problem solving. There is also a line of works (Tafjord et al., 2021 ; Saparov et al., 2024 ; Kazemi et al., 2023b ) which construct semi-synthetic benchmarks to evaluate the logical deductive reasoning abilities of LLMs. In addition, the BIG-Bench (Srivastava et al., 2022 ) suite of tasks contains many which focus on reasoning. Vision-language reasoning benchmarks. Some recent benchmarks such as Fu et al. ( 2023 ); Yue et al. ( 2023 ); Lu et al. ( 2023 ); Kazemi et al. ( 2023a ) present reasoning problems that require conditioning on images; however, they predominantly require only a single image, and do not directly measure how well the model can integrate information across different images. Cross-image reasoning benchmarks exist but are restricted to the entailment task or focus on limited number of domains. NLVR (Suhr et al., 2017 ) creates pairs of images composed of synthetic 2D objects and the task is identifying whether the caption is entailed from the image. NLVR2 (Suhr et al., 2019 ) extends NLVR by replacing synthetic images with pairs of images sampled from MS COCO (Lin et al., 2014 ) . MaRVL (Liu et al., 2021 ) expands a similar idea to multi-cultural and multilingual scenarios and only focuses on the natural image domain. SEED-Bench-2 Li et al. ( 2023a ) proposes a hierarchy of different vision-language datasets including multi-image datasets composed of frames extracted from videos. BLINK (Fu et al., 2024 ) is a collection of 14 visual perception tasks where some of the tasks involve multiple images, e.g. visual similarity and multi-view reasoning. None of these mentioned benchmarks aim to test vision-language models for complex reasoning in multi-image scenarios. We aim to propose a holistic benchmark covering a wide range of visual information in the world and focuses on complex reasoning of multi-images. (a) 𝖢𝗅𝗈𝖼𝗄𝗌 𝖢𝗅𝗈𝖼𝗄𝗌 \\mathsf{Clocks} sansserif_Clocks (b) 𝖥𝗎𝗇𝖼𝖱𝖾𝖺𝖽 𝖥𝗎𝗇𝖼𝖱𝖾𝖺𝖽 \\mathsf{FuncRead} sansserif_FuncRead (c) 𝖢𝗁𝖺𝗋𝗍𝗌 𝖢𝗁𝖺𝗋𝗍𝗌 \\mathsf{Charts} sansserif_Charts (d) 𝖱𝖾𝖿𝖢𝖮𝖢𝖮 𝖱𝖾𝖿𝖢𝖮𝖢𝖮 \\mathsf{RefCOCO} sansserif_RefCOCO (e) 𝖤𝗆𝗈𝗃𝗂𝖠𝗅𝗀𝖾𝖻𝗋𝖺 𝖤𝗆𝗈𝗃𝗂𝖠𝗅𝗀𝖾𝖻𝗋𝖺 \\mathsf{EmojiAlgebra} sansserif_EmojiAlgebra (f) 𝖦𝖾𝗈𝗆𝖲𝗁𝖺𝗉𝖾𝗌 𝖦𝖾𝗈𝗆𝖲𝗁𝖺𝗉𝖾𝗌 \\mathsf{GeomShapes} sansserif_GeomShapes (g) 𝖦𝖾𝗈𝗆𝖢𝗈𝗌𝗍 𝖦𝖾𝗈𝗆𝖢𝗈𝗌𝗍 \\mathsf{GeomCost} sansserif_GeomCost (h) 𝖨𝖰 𝖨𝖰 \\mathsf{IQ} sansserif_IQ (i) 𝖨𝗌𝗈𝗆𝗈𝗋𝗉𝗁𝗂𝗌𝗆 𝖨𝗌𝗈𝗆𝗈𝗋𝗉𝗁𝗂𝗌𝗆 \\mathsf{Isomorphism} sansserif_Isomorphism (j) 𝖬𝖺𝗉𝗌 𝖬𝖺𝗉𝗌 \\mathsf{Maps} sansserif_Maps (k) 𝖢𝗈𝗅𝗅𝗂𝗌𝗂𝗈𝗇𝗌 𝖢𝗈𝗅𝗅𝗂𝗌𝗂𝗈𝗇𝗌 \\mathsf{Collisions} sansserif_Collisions (l) 𝖲𝖼𝗁𝖾𝖽𝗎𝗅𝖾 𝖲𝖼𝗁𝖾𝖽𝗎𝗅𝖾 \\mathsf{Schedule} sansserif_Schedule (m) 𝖢𝗈𝖽𝖾𝖤𝖽𝗂𝗍 𝖢𝗈𝖽𝖾𝖤𝖽𝗂𝗍 \\mathsf{CodeEdit} sansserif_CodeEdit Figure 2: Sample problems from the tasks in 𝖱𝖾𝖬𝖨 𝖱𝖾𝖬𝖨 \\mathsf{ReMI} sansserif_ReMI . Note that the images are provided to the models separately in place of the &lt;image i&gt; markers. 3 The 𝖱𝖾𝖬𝖨 𝖱𝖾𝖬𝖨 \\mathsf{ReMI} sansserif_ReMI Dataset Multi-image reasoning can arise in many domains and the problems involving reasoning over multiple images may differ in some key properties. We aim to create a benchmark that exhibits many domains and covers those key properties as much as possible. To this end, we included 13 tasks in our benchmark that covers the following domains: Algebra , Calculus , Geometry , Tabular Reasoning , Time Arithmetic , Logic , Physics , Spatial Reasoning , Graph Theory , Charts , Maps , and Coding . We also identified the following key properties specific to multi-image reasoning and aimed for having tasks that provide a good coverage of them: • Sequential vs Set: In some tasks, the provided images have to be consumed in a sequence (e.g., computing a quantity from one image and then using that quantity in the second image), whereas in some other tasks, the provided images constitute a set. When more than two images are provided, they may be grouped into subsets that have to be consumed sequentially. • Same vs Different Concept: In some multi-image reasoning problems, the provided images all correspond to the same concept (e.g., all of them are charts, or function graphs) whereas in some other problems, the provided images may correspond to different concepts (e.g., one image might be a geometry shape, and the other might be a table). • Interleaving: For all our tasks, we can either provide all the images first and then ask a question about them, or the images can be interleaved with the question task when they are referred to. To enable experimenting for both settings, we make a subset of the tasks interleaved while for the others we provide the image at the beginning of the prompt. • Number of images: In some tasks, a variable number of images may be provided as input. Solving our tasks requires parsing and understanding the information in the images and text of the question provided as input, which is often followed by the model having to reason using this information to arrive at the correct answer. We provide a brief description of each task below and a more detailed description in the Appendix. In Figure 2 , we illustrate a sample from each of the tasks in 𝖱𝖾𝖬𝖨 𝖱𝖾𝖬𝖨 \\mathsf{ReMI} sansserif_ReMI . Moreover, in Table 1 , we specify the domain and properties for each of the tasks in 𝖱𝖾𝖬𝖨 𝖱𝖾𝖬𝖨 \\mathsf{ReMI} sansserif_ReMI . (1) 𝖤𝗆𝗈𝗃𝗂𝖠𝗅𝗀𝖾𝖻𝗋𝖺 𝖤𝗆𝗈𝗃𝗂𝖠𝗅𝗀𝖾𝖻𝗋𝖺 \\mathsf{EmojiAlgebra} sansserif_EmojiAlgebra : Solve a system of linear equations involving digits and emojis. Each image contains an equation or the final expression to be computed. (2) 𝖥𝗎𝗇𝖼𝖱𝖾𝖺𝖽 𝖥𝗎𝗇𝖼𝖱𝖾𝖺𝖽 \\mathsf{FuncRead} sansserif_FuncRead : Given multiple function graphs in separate images, answer questions about them. (3) 𝖦𝖾𝗈𝗆𝖲𝗁𝖺𝗉𝖾𝗌 𝖦𝖾𝗈𝗆𝖲𝗁𝖺𝗉𝖾𝗌 \\mathsf{GeomShapes} sansserif_GeomShapes : Given two shapes (in two different images) with a common property, compute a missing value of one of the shapes. (4) 𝖦𝖾𝗈𝗆𝖢𝗈𝗌𝗍 𝖦𝖾𝗈𝗆𝖢𝗈𝗌𝗍 \\mathsf{GeomCost} sansserif_GeomCost : Given the shape of an object (in one image) on which an operation is to be done and a table of various costs (in a different image), compute the total cost of the operation. (5) 𝖢𝗈𝗅𝗅𝗂𝗌𝗂𝗈𝗇𝗌 𝖢𝗈𝗅𝗅𝗂𝗌𝗂𝗈𝗇𝗌 \\mathsf{Collisions} sansserif_Collisions : Given the before and after snapshots of two objects colliding (each in a separate image), answer questions about their state. (6) 𝖢𝗅𝗈𝖼𝗄𝗌 𝖢𝗅𝗈𝖼𝗄𝗌 \\mathsf{Clocks} sansserif_Clocks : Given two clocks with different designs (each in a separate image), compute the time difference between them. (7) 𝖲𝖼𝗁𝖾𝖽𝗎𝗅𝖾 𝖲𝖼𝗁𝖾𝖽𝗎𝗅𝖾 \\mathsf{Schedule} sansserif_Schedule : Given the current time (in one image) and a table of train schedules (in another image), answer questions about the next scheduled train. (8) 𝖢𝗁𝖺𝗋𝗍𝗌 𝖢𝗁𝖺𝗋𝗍𝗌 \\mathsf{Charts} sansserif_Charts : Given two charts (each in a separate image), possibly in different formats – e.g., one bar chart and one pie chart, identify the differences between the reported values or reason jointly from values in both charts. (9) 𝖢𝗈𝖽𝖾𝖤𝖽𝗂𝗍 𝖢𝗈𝖽𝖾𝖤𝖽𝗂𝗍 \\mathsf{CodeEdit} sansserif_CodeEdit : Given a TikZ code, the rendered image, and the goal image, determine which line of code should be removed to get to the goal image. (10) 𝖨𝗌𝗈𝗆𝗈𝗋𝗉𝗁𝗂𝗌𝗆 𝖨𝗌𝗈𝗆𝗈𝗋𝗉𝗁𝗂𝗌𝗆 \\mathsf{Isomorphism} sansserif_Isomorphism : Given two graphs (in two images), determine if they are isomorphic or not. (11) 𝖬𝖺𝗉𝗌 𝖬𝖺𝗉𝗌 \\mathsf{Maps} sansserif_Maps : Given a description of a navigation and four navigation routes on a map (each in a different image), determine which one corresponds to the one in the description. (12) 𝖱𝖾𝖿𝖢𝖮𝖢𝖮 𝖱𝖾𝖿𝖢𝖮𝖢𝖮 \\mathsf{RefCOCO} sansserif_RefCOCO : Given a real-world image and another image of same dimensions with non-overlapping circles marked on it, determine which circle overlaps the most with a target entity in the real image. (13) 𝖨𝖰 𝖨𝖰 \\mathsf{IQ} sansserif_IQ : Given a matrix of shapes that have a logical connection and with one missing value, predict the shape that goes into the missing part. Figure 3: Average time spent on each problem by humans for the human performance results. Table 2: The performance of SoTA models on 𝖱𝖾𝖬𝖨 𝖱𝖾𝖬𝖨 \\mathsf{ReMI} sansserif_ReMI and its individual tasks. The winner for each task is in bold and the second winner is underlined. Task Name Naive Baseline Claude3 Sonnet Gemini Ultra Gemini Flash Gemini 1.5 GPT4 Turbo Human 𝖤𝗆𝗈𝗃𝗂𝖠𝗅𝗀𝖾𝖻𝗋𝖺 𝖤𝗆𝗈𝗃𝗂𝖠𝗅𝗀𝖾𝖻𝗋𝖺 \\mathsf{EmojiAlgebra} sansserif_EmojiAlgebra 0.0 28.0 2.5 15.0 44.5 57.5 100.0 𝖥𝗎𝗇𝖼𝖱𝖾𝖺𝖽 𝖥𝗎𝗇𝖼𝖱𝖾𝖺𝖽 \\mathsf{FuncRead} sansserif_FuncRead 5.5 24.0 15.0 36.0 40.0 26.0 100.0 𝖦𝖾𝗈𝗆𝖲𝗁𝖺𝗉𝖾𝗌 𝖦𝖾𝗈𝗆𝖲𝗁𝖺𝗉𝖾𝗌 \\mathsf{GeomShapes} sansserif_GeomShapes 0.0 17.5 14.5 34.0 51.5 32.5 100.0 𝖦𝖾𝗈𝗆𝖢𝗈𝗌𝗍 𝖦𝖾𝗈𝗆𝖢𝗈𝗌𝗍 \\mathsf{GeomCost} sansserif_GeomCost 0.0 58.5 47.0 75.0 81.5 70.5 90.0 𝖢𝗈𝗅𝗅𝗂𝗌𝗂𝗈𝗇𝗌 𝖢𝗈𝗅𝗅𝗂𝗌𝗂𝗈𝗇𝗌 \\mathsf{Collisions} sansserif_Collisions 30.8 51.5 36.5 56.5 50.5 62.0 100.0 𝖢𝗅𝗈𝖼𝗄𝗌 𝖢𝗅𝗈𝖼𝗄𝗌 \\mathsf{Clocks} sansserif_Clocks 2.0 5.0 4.0 4.0 2.5 4.0 80.0 𝖲𝖼𝗁𝖾𝖽𝗎𝗅𝖾 𝖲𝖼𝗁𝖾𝖽𝗎𝗅𝖾 \\mathsf{Schedule} sansserif_Schedule 0.0 36.0 33.0 43.0 40.5 49.5 90.0 𝖢𝗁𝖺𝗋𝗍𝗌 𝖢𝗁𝖺𝗋𝗍𝗌 \\mathsf{Charts} sansserif_Charts 2.5 40.0 30.0 53.0 54.0 44.0 95.0 𝖢𝗈𝖽𝖾𝖤𝖽𝗂𝗍 𝖢𝗈𝖽𝖾𝖤𝖽𝗂𝗍 \\mathsf{CodeEdit} sansserif_CodeEdit 14.9 20.0 24.5 46.0 41.0 42.0 95.0 𝖨𝗌𝗈𝗆𝗈𝗋𝗉𝗁𝗂𝗌𝗆 𝖨𝗌𝗈𝗆𝗈𝗋𝗉𝗁𝗂𝗌𝗆 \\mathsf{Isomorphism} sansserif_Isomorphism 50.0 57.0 65.0 67.0 72.0 71.5 100.0 𝖬𝖺𝗉𝗌 𝖬𝖺𝗉𝗌 \\mathsf{Maps} sansserif_Maps 28.0 39.5 39.0 47.0 47.0 36.5 100.0 𝖱𝖾𝖿𝖢𝖮𝖢𝖮 𝖱𝖾𝖿𝖢𝖮𝖢𝖮 \\mathsf{RefCOCO} sansserif_RefCOCO 12.0 30.0 31.0 49.0 56.0 37.5 95.0 𝖨𝖰 𝖨𝖰 \\mathsf{IQ} sansserif_IQ 25.0 50.5 30.0 53.0 76.0 62.5 100.0 𝖱𝖾𝖬𝖨 𝖱𝖾𝖬𝖨 \\mathsf{ReMI} sansserif_ReMI 13.1 35.2 28.6 44.5 50.5 45.8 95.8 4 Experiments We report the performance of multiple state-of-the-art models on our benchmark. Metrics: We mainly report accuracy for our tasks. For textual outputs, we compute exact match while handling slight variations such as spacing issues, lowercase vs uppercase, etc. For numeric answers, we compute a relaxed accuracy with 1% tolerance, mainly to avoid penalizing rounding errors. In the case of relaxed accuracy with tolerance ϵ italic-ϵ \\epsilon italic_ϵ , a numeric prediction p 𝑝 p italic_p is considered correct if ( 1 − ϵ ) ⁢ l ≤ p ≤ ( 1 + ϵ ) ⁢ l 1 italic-ϵ 𝑙 𝑝 1 italic-ϵ 𝑙 (1-\\epsilon)l\\leq p\\leq(1+\\epsilon)l ( 1 - italic_ϵ ) italic_l ≤ italic_p ≤ ( 1 + italic_ϵ ) italic_l where l 𝑙 l italic_l is the label. Following the original GeomVerse paper (Kazemi et al., 2023a ) , we report relaxed accuracy with 3% tolerance for our 𝖦𝖾𝗈𝗆𝖲𝗁𝖺𝗉𝖾𝗌 𝖦𝖾𝗈𝗆𝖲𝗁𝖺𝗉𝖾𝗌 \\mathsf{GeomShapes} sansserif_GeomShapes and 𝖦𝖾𝗈𝗆𝖢𝗈𝗌𝗍 𝖦𝖾𝗈𝗆𝖢𝗈𝗌𝗍 \\mathsf{GeomCost} sansserif_GeomCost tasks as intermediate operations are also rounded and different operation orders lead to slight variations in the final result. For the 𝖢𝗅𝗈𝖼𝗄𝗌 𝖢𝗅𝗈𝖼𝗄𝗌 \\mathsf{Clocks} sansserif_Clocks , we allow 10 minutes tolerance to account for slight variations in reading times from analog clocks. In our analyses, we also use a metric named error reduction percentage(ERP) with respect to a baseline, which corresponds to how much a model reduces the error with respect to a baseline. We define the ERP of a model M 𝑀 M italic_M for a task T 𝑇 T italic_T with respect to a baseline B 𝐵 B italic_B as follows: E ⁢ R ⁢ P T ⁢ ( B , M ) = 100 ∗ E ⁢ r ⁢ r ⁢ o ⁢ r T ⁢ ( B ) − E ⁢ r ⁢ r ⁢ o ⁢ r T ⁢ ( M ) E ⁢ r ⁢ r ⁢ o ⁢ r T ⁢ ( B ) 𝐸 𝑅 subscript 𝑃 𝑇 𝐵 𝑀 100 𝐸 𝑟 𝑟 𝑜 subscript 𝑟 𝑇 𝐵 𝐸 𝑟 𝑟 𝑜 subscript 𝑟 𝑇 𝑀 𝐸 𝑟 𝑟 𝑜 subscript 𝑟 𝑇 𝐵 ERP_{T}(B,M)=100*\\frac{Error_{T}(B)-Error_{T}(M)}{Error_{T}(B)} italic_E italic_R italic_P start_POSTSUBSCRIPT italic_T end_POSTSUBSCRIPT ( italic_B , italic_M ) = 100 ∗ divide start_ARG italic_E italic_r italic_r italic_o italic_r start_POSTSUBSCRIPT italic_T end_POSTSUBSCRIPT ( italic_B ) - italic_E italic_r italic_r italic_o italic_r start_POSTSUBSCRIPT italic_T end_POSTSUBSCRIPT ( italic_M ) end_ARG start_ARG italic_E italic_r italic_r italic_o italic_r start_POSTSUBSCRIPT italic_T end_POSTSUBSCRIPT ( italic_B ) end_ARG Conceptually, the numerator corresponds to how much of the error has been reduced compared to the baseline, and the denominator normalizes by how much room for error reduction existed. Naive Baseline: We provide the expected accuracy for a naive baseline that predicts the answers without looking at the images, by only guessing the final answer based on the text of the question. For multi-choice questions, we assume this baseline will predict the answer correctly with 1 / c 1 𝑐 1/c 1 / italic_c chance where c 𝑐 c italic_c is the number of choices (for 𝖢𝗈𝖽𝖾𝖤𝖽𝗂𝗍 𝖢𝗈𝖽𝖾𝖤𝖽𝗂𝗍 \\mathsf{CodeEdit} sansserif_CodeEdit , we consider any line ending in semi-colon to be one of possible choices); for 𝖢𝗁𝖺𝗋𝗍𝗌 𝖢𝗁𝖺𝗋𝗍𝗌 \\mathsf{Charts} sansserif_Charts , for every question asking about which cell changed, we assume this baseline responds with ( 0 , 0 ) 0 0 (0,0) ( 0 , 0 ) , and for every question about the number of cells that changed, we assume this baseline responds with 1 1 1 1 ; for 𝖢𝗅𝗈𝖼𝗄𝗌 𝖢𝗅𝗈𝖼𝗄𝗌 \\mathsf{Clocks} sansserif_Clocks , when asking about the difference in time, we assume this baseline always predicts 12 ∗ 60 12 60 12*60 12 ∗ 60 minutes; for 𝖱𝖾𝖿𝖢𝖮𝖢𝖮 𝖱𝖾𝖿𝖢𝖮𝖢𝖮 \\mathsf{RefCOCO} sansserif_RefCOCO , we assume this baseline always predicts the circle labeled 0 0 . Models: We experiment with three state of the art model families, namely Gemini (Team et al., 2023 ; Reid et al., 2024 ) , Claude 3 (Anthropic, 2024 ) , and GPT4 (OpenAI, 2023 ) . Within the Gemini family, we experiment with three models with different sizes and properties, namely Gemini Ultra, Gemini 1.5 Pro, and Gemini Flash. From the Claude 3 family, we experiment with the Sonnet model, and from the GPT4 family, we experiment with GPT4 Turbo. Human Performance: For each task we sampled 20 20 20 20 examples from the test set and had them solved by someone knowledgeable (but not necessarily expert) in that area. We also asked them to measure the amount of time they spent on solving the 20 20 20 20 problems. The average time per problem for each task is reported in Figure 3 . We observe that some tasks have been more time consuming than the others with 𝖤𝗆𝗈𝗃𝗂𝖠𝗅𝗀𝖾𝖻𝗋𝖺 𝖤𝗆𝗈𝗃𝗂𝖠𝗅𝗀𝖾𝖻𝗋𝖺 \\mathsf{EmojiAlgebra} sansserif_EmojiAlgebra being the most time consuming and the 𝖨𝖰 𝖨𝖰 \\mathsf{IQ} sansserif_IQ being the least time consuming. 4.1 Human Baseline Substantially Beats SoTA Models in Multi-Image Reasoning In Table 2 , we present the results of the models as well as the naive baseline and the human performance on the tasks in 𝖱𝖾𝖬𝖨 𝖱𝖾𝖬𝖨 \\mathsf{ReMI} sansserif_ReMI . We make the following observations from the obtained results. Firstly, all the models significantly outperform the naive baseline, almost on any task; however, their performance remains far behind the human performance in general, and also in most of the tasks. Secondly, there are some tasks where none of the current models are good at, including 𝖢𝗅𝗈𝖼𝗄𝗌 𝖢𝗅𝗈𝖼𝗄𝗌 \\mathsf{Clocks} sansserif_Clocks and 𝖨𝗌𝗈𝗆𝗈𝗋𝗉𝗁𝗂𝗌𝗆 𝖨𝗌𝗈𝗆𝗈𝗋𝗉𝗁𝗂𝗌𝗆 \\mathsf{Isomorphism} sansserif_Isomorphism , where the performances remain quite low 1 1 1 In the case of the 𝖨𝗌𝗈𝗆𝗈𝗋𝗉𝗁𝗂𝗌𝗆 𝖨𝗌𝗈𝗆𝗈𝗋𝗉𝗁𝗂𝗌𝗆 \\mathsf{Isomorphism} sansserif_Isomorphism , the dataset is imbalanced with a majority class accuracy of 67 67 67 67 percent. . This reveals a potential capability gap in the current state-of-the-art models. Thirdly, we observe that different models perform well on different tasks. For example, Gemini 1.5 substantially outperforms GPT4-Turbo on the 𝖨𝖰 𝖨𝖰 \\mathsf{IQ} sansserif_IQ , whereas GPT4-Turbo substantially outperforms Gemini 1.5 on 𝖤𝗆𝗈𝗃𝗂𝖠𝗅𝗀𝖾𝖻𝗋𝖺 𝖤𝗆𝗈𝗃𝗂𝖠𝗅𝗀𝖾𝖻𝗋𝖺 \\mathsf{EmojiAlgebra} sansserif_EmojiAlgebra . This hints that the frontier models may have different capabilities and limitations. Hereafter, unless stated otherwise, we do the rest of the experiments with Gemini Pro 1.5, the best overall performing model on 𝖱𝖾𝖬𝖨 𝖱𝖾𝖬𝖨 \\mathsf{ReMI} sansserif_ReMI . 4.2 Single-Image vs Multi-Image Reasoning We measure whether models perform better when we provide the multiple images separately or when we put them all in a single image and feed them to the model. To this end, we report E ⁢ R ⁢ P T ⁢ ( single-image model , multi-image model ) 𝐸 𝑅 subscript 𝑃 𝑇 single-image model multi-image model ERP_{T}(\\text{single-image model},\\text{multi-image model}) italic_E italic_R italic_P start_POSTSUBSCRIPT italic_T end_POSTSUBSCRIPT ( single-image model , multi-image model ) corresponding to how much the multi-image model reduces the error with respect to the single-image model for each task T 𝑇 T italic_T . The results are provided in Figure 4 . We observe that for most of the tasks, feeding images separately results in positive gains (positive ERP) compared to a single-image case. A manual analysis of the model outputs in the two settings shows that the model may even employ different strategies for solving the problem in these settings. For example, in the case of 𝖤𝗆𝗈𝗃𝗂𝖠𝗅𝗀𝖾𝖻𝗋𝖺 𝖤𝗆𝗈𝗃𝗂𝖠𝗅𝗀𝖾𝖻𝗋𝖺 \\mathsf{EmojiAlgebra} sansserif_EmojiAlgebra , we observe that in the single-image case, the model mostly starts by assigning a variable (e.g., a 𝑎 a italic_a , b 𝑏 b italic_b , etc.) to each emoji and then solving the problem by using those variables; However, in the case of multi-image, the model mostly uses either the emojis themselves or their names when doing the calculations. t Figure 4: ERP when the images are provided to Gemini 1.5 as multiple vs a single image. Interleaved tasks are affected more: Out of the six tasks that are positively affected the most ( 𝖥𝗎𝗇𝖼𝖱𝖾𝖺𝖽 𝖥𝗎𝗇𝖼𝖱𝖾𝖺𝖽 \\mathsf{FuncRead} sansserif_FuncRead , 𝖨𝖰 𝖨𝖰 \\mathsf{IQ} sansserif_IQ , 𝖢𝗈𝖽𝖾𝖤𝖽𝗂𝗍 𝖢𝗈𝖽𝖾𝖤𝖽𝗂𝗍 \\mathsf{CodeEdit} sansserif_CodeEdit , 𝖲𝖼𝗁𝖾𝖽𝗎𝗅𝖾 𝖲𝖼𝗁𝖾𝖽𝗎𝗅𝖾 \\mathsf{Schedule} sansserif_Schedule , 𝖨𝗌𝗈𝗆𝗈𝗋𝗉𝗁𝗂𝗌𝗆 𝖨𝗌𝗈𝗆𝗈𝗋𝗉𝗁𝗂𝗌𝗆 \\mathsf{Isomorphism} sansserif_Isomorphism , and 𝖱𝖾𝖿𝖢𝖮𝖢𝖮 𝖱𝖾𝖿𝖢𝖮𝖢𝖮 \\mathsf{RefCOCO} sansserif_RefCOCO ), we observe that five of them (the first five) are interleaved tasks. Averaging the ERP for the interleaved and non-interleaved datasets, we observe a gain of 19.8 % percent 19.8 19.8\\% 19.8 % for the former case and a gain of 4.9 % percent 4.9 4.9\\% 4.9 % for the latter case. This hints that reasoning with multiple images might be easier for the models than feeding all images in one image, especially when the images are provided interleaved with text at the right positions. Table 3: Major error sources for each task, identified with a manual inspection of 20 failed examples. Task Name Major Source(s) of Error 𝖤𝗆𝗈𝗃𝗂𝖠𝗅𝗀𝖾𝖻𝗋𝖺 𝖤𝗆𝗈𝗃𝗂𝖠𝗅𝗀𝖾𝖻𝗋𝖺 \\mathsf{EmojiAlgebra} sansserif_EmojiAlgebra 1- Calculation errors, 2- Confusing similar emojis, 3- Misreading from the images (especially minus signs). 𝖥𝗎𝗇𝖼𝖱𝖾𝖺𝖽 𝖥𝗎𝗇𝖼𝖱𝖾𝖺𝖽 \\mathsf{FuncRead} sansserif_FuncRead 1- Model value readings are typically off by about 1 unit. 𝖦𝖾𝗈𝗆𝖲𝗁𝖺𝗉𝖾𝗌 𝖦𝖾𝗈𝗆𝖲𝗁𝖺𝗉𝖾𝗌 \\mathsf{GeomShapes} sansserif_GeomShapes &amp; 𝖦𝖾𝗈𝗆𝖢𝗈𝗌𝗍 𝖦𝖾𝗈𝗆𝖢𝗈𝗌𝗍 \\mathsf{GeomCost} sansserif_GeomCost 1- Calculation errors, 2- Going on a wrong solution path (e.g., computing irrelevant unknown values), 3- Misreading and mis-assigning values (e.g., assigning a length value to the height), 4- Hallucinating non-existent values. 𝖢𝗈𝗅𝗅𝗂𝗌𝗂𝗈𝗇𝗌 𝖢𝗈𝗅𝗅𝗂𝗌𝗂𝗈𝗇𝗌 \\mathsf{Collisions} sansserif_Collisions 1- Not recognizing when two objects are moving together after a collision, 2-Ignoring the velocity vector direction when calculating absolute velocity difference between two objects. 𝖢𝗅𝗈𝖼𝗄𝗌 𝖢𝗅𝗈𝖼𝗄𝗌 \\mathsf{Clocks} sansserif_Clocks 1- Not able to read the time properly, 2- Mistaking the minute hand for the hour hand, 3- Not paying attention to the prompt specifying the times are in the same day. 𝖲𝖼𝗁𝖾𝖽𝗎𝗅𝖾 𝖲𝖼𝗁𝖾𝖽𝗎𝗅𝖾 \\mathsf{Schedule} sansserif_Schedule 1- Not able to read the time properly, 2- Retrieving the wrong values from the table given the time, 3- Sometimes ignoring the 24h format. 𝖢𝗁𝖺𝗋𝗍𝗌 𝖢𝗁𝖺𝗋𝗍𝗌 \\mathsf{Charts} sansserif_Charts 1- Mis-assigning values to the correct row/column in heatmap charts, 2- Under-counting the number of differences in two charts, 3- Reasoning errors of the type The value decreased from X to X . 𝖢𝗈𝖽𝖾𝖤𝖽𝗂𝗍 𝖢𝗈𝖽𝖾𝖤𝖽𝗂𝗍 \\mathsf{CodeEdit} sansserif_CodeEdit 1-Suggesting removal of nonexistent parts of code, 2-Not properly understanding what each line of code represents in the compiled image. 𝖨𝗌𝗈𝗆𝗈𝗋𝗉𝗁𝗂𝗌𝗆 𝖨𝗌𝗈𝗆𝗈𝗋𝗉𝗁𝗂𝗌𝗆 \\mathsf{Isomorphism} sansserif_Isomorphism 1- Jumping prematurely to a conclusion after finding one or two nodes that map to each other, 2- Hallucinating non-existent nodes or edges. 𝖬𝖺𝗉𝗌 𝖬𝖺𝗉𝗌 \\mathsf{Maps} sansserif_Maps 1- Incorrectly counts similar pins as the same type of objects (e.g., pins that share the same color but different icon), 2- Not paying attention to the prompt specifying the certain area of interest, 3- Gives arbitrary directions that don’t match the situation shown in the grid map. 𝖱𝖾𝖿𝖢𝖮𝖢𝖮 𝖱𝖾𝖿𝖢𝖮𝖢𝖮 \\mathsf{RefCOCO} sansserif_RefCOCO 1- For the image with the circles, coordinate reading is off by 100-200, 2- Not a proper understanding of spatial clues such as top right . 𝖨𝖰 𝖨𝖰 \\mathsf{IQ} sansserif_IQ 1- Unfaithful-ness to model’s own CoT (e.g., it explains the color should be green, but selects the red), 2- Overly predicting the operation to be rotation (despite no rotation being in the dataset). 4.3 Failure Analysis For each task, we manually examined 20 20 20 20 examples where the answers given by overall best performing model (Gemini 1.5 Pro) was incorrect and analyzed the dominant reasons behind the failures. This analysis revealed several interesting failure modes – some intuitive and some not – as described below and summarized in Table 3 . The diversity of errors observed highlights that this multi-image reasoning domain elicits a wide range of different behaviors that can go wrong in a range of different ways, and that our benchmark tests this wide range of abilities. Calculation errors were present in many of the math-related datasets, so we do not discuss them separately for each task. For 𝖤𝗆𝗈𝗃𝗂𝖠𝗅𝗀𝖾𝖻𝗋𝖺 𝖤𝗆𝗈𝗃𝗂𝖠𝗅𝗀𝖾𝖻𝗋𝖺 \\mathsf{EmojiAlgebra} sansserif_EmojiAlgebra , the overall reasoning process of the model is mostly correct. However, the model sometimes confuses similar emojis. As an example, it assigns a similar (or the same) name to and or to and and then these variables get confused in the later calculations. We also observe some misreading of the expressions. Figure 5: Time read by different models for one of the clocks in the 𝖢𝗅𝗈𝖼𝗄𝗌 𝖢𝗅𝗈𝖼𝗄𝗌 \\mathsf{Clocks} sansserif_Clocks . For both 𝖢𝗅𝗈𝖼𝗄𝗌 𝖢𝗅𝗈𝖼𝗄𝗌 \\mathsf{Clocks} sansserif_Clocks and 𝖲𝖼𝗁𝖾𝖽𝗎𝗅𝖾 𝖲𝖼𝗁𝖾𝖽𝗎𝗅𝖾 \\mathsf{Schedule} sansserif_Schedule , the model suffers from not being able to read the time correctly; e.g. often the minute hand was mistaken for the hour hand. Figure 5 shows a sample clock and times read by the various models. Despite reading the wrong times, the model generally does a good job of computing the time difference given these wrong times, though it sometimes ignores the prompt instructing it to consider both times to be on the same day. In the case of the 𝖲𝖼𝗁𝖾𝖽𝗎𝗅𝖾 𝖲𝖼𝗁𝖾𝖽𝗎𝗅𝖾 \\mathsf{Schedule} sansserif_Schedule , the value retrieved from the table is often not the right value, even given the wrong time read by the mode; Sometimes, this is due the model confusing AM vs PM. Figure 6: The confusion matrix of reasoning errors vs image reading errors. For 𝖦𝖾𝗈𝗆𝖲𝗁𝖺𝗉𝖾𝗌 𝖦𝖾𝗈𝗆𝖲𝗁𝖺𝗉𝖾𝗌 \\mathsf{GeomShapes} sansserif_GeomShapes and 𝖦𝖾𝗈𝗆𝖢𝗈𝗌𝗍 𝖦𝖾𝗈𝗆𝖢𝗈𝗌𝗍 \\mathsf{GeomCost} sansserif_GeomCost , the model makes reasoning errors on the geometry side where it tries to compute the values for unknown sides/angles that are irrelevant to the question. We also observe some misreading of values or mis-assigning the value for one element to another element (e.g., assigning a side value to a height). Hallucinating non-existent values is another issue. In both cases, the model performs well in understanding and executing the high-level task of extracting a value from the first shape and then using it in the next shape; it also extracts the relevant values from the table mostly correctly. (a) Interleaved vs Non-Interleaved (b) Max = Two vs Max &gt; 2 Images (c) Sequence vs Set (d) Same vs Different Concept Figure 7: Model performances as a function of the task properties presented in Table 1 . For 𝖨𝗌𝗈𝗆𝗈𝗋𝗉𝗁𝗂𝗌𝗆 𝖨𝗌𝗈𝗆𝗈𝗋𝗉𝗁𝗂𝗌𝗆 \\mathsf{Isomorphism} sansserif_Isomorphism , the model tended to jump to conclusions prematurely, based on some initial guesses. For example, it found one or two nodes that had similar structures and jumped to the conclusion that the graphs are isomorphic, whereas other nodes had different structures. The model also suffered from hallucinating non-existent nodes and edges. For 𝖱𝖾𝖿𝖢𝖮𝖢𝖮 𝖱𝖾𝖿𝖢𝖮𝖢𝖮 \\mathsf{RefCOCO} sansserif_RefCOCO , the model understands how to use the provided coordinates; however, the coordinates it reads for the circles tends to be off by 10-20%. Moreover, sometimes the model correctly explained that the object of interest is, e.g., on the top left but then selected a circle that was not on the top left , showing a potential gap in truly understanding what top left or other spatial clues are. For 𝖨𝖰 𝖨𝖰 \\mathsf{IQ} sansserif_IQ , the model was sometimes unfaithful to its own reasoning (e.g., it explained that the answer must be a green shape, but selected a red shape as the final answer). Also, even though we had no rotation operations in the dataset, the model tended to over-predict the logical operation being rotation, probably due to a prior bias on the presence of rotation in IQ questions. For 𝖥𝗎𝗇𝖼𝖱𝖾𝖺𝖽 𝖥𝗎𝗇𝖼𝖱𝖾𝖺𝖽 \\mathsf{FuncRead} sansserif_FuncRead , the model understands the general logic and follows the calculations correctly, but it fails to correctly read values from the function graphs; the values are mostly off by about 1 unit showing the model can locate the vicinity of the point, but lacks precision. For 𝖢𝗈𝗅𝗅𝗂𝗌𝗂𝗈𝗇𝗌 𝖢𝗈𝗅𝗅𝗂𝗌𝗂𝗈𝗇𝗌 \\mathsf{Collisions} sansserif_Collisions , the model demonstrates issues in interpreting physics diagrams and calculations, particularly in differentiating between elastic and inelastic collisions. It struggles to account for implicit information such as orientation component of the objects velocity. For 𝖢𝗁𝖺𝗋𝗍𝗌 𝖢𝗁𝖺𝗋𝗍𝗌 \\mathsf{Charts} sansserif_Charts , while the model reads the correct values from the heatmap charts, it lacks preciseness and assigns values to the wrong row/column (it is typically off by one row/column). Moreover, when we ask the model to identify how many differences there are between two charts, it mostly under-counts. We also see multiple cases where the model claimed a value decreased from X 𝑋 X italic_X to X 𝑋 X italic_X (i.e. to the same amount). For 𝖢𝗈𝖽𝖾𝖤𝖽𝗂𝗍 𝖢𝗈𝖽𝖾𝖤𝖽𝗂𝗍 \\mathsf{CodeEdit} sansserif_CodeEdit , while correctly identifying the visual changes in the rendered image, the model lacks understanding of how each line of code contributes to the final image. In some cases incorrectly suggests removing code segments that are not present in the original code. Despite these flaws, the model demonstrates some understanding of the code structure, as it avoids suggesting the removal of critical code components that would prevent code from compiling. For 𝖬𝖺𝗉𝗌 𝖬𝖺𝗉𝗌 \\mathsf{Maps} sansserif_Maps , the model has difficulty counting objects of interest accurately, especially when there are many distractions on the map. It also sometimes hallucinates information about restaurants and bars or lists those outside the area of interest. Additionally, it struggles to differentiate between similar pins, such as coffee shops, bars, and restaurants. When asked about directions, the model’s suggestions are often random. While it may list correct streets, the directions it describes do not match the map. Even when it does provide the correct answer, the model’s reasoning is often faulty and seems like guesswork. Reasoning Errors vs Image Reading Errors: Besides computation errors, we observed that reasoning errors and image reading errors are two of the most dominant sources of failures across the tasks in 𝖱𝖾𝖬𝖨 𝖱𝖾𝖬𝖨 \\mathsf{ReMI} sansserif_ReMI . We examined 125 failed examples and verified whether there existed a reasoning error or image reading error in them. The results are provided in Figure 6 . We observe that in 12 % percent 12 12\\% 12 % of the cases, the values were read correctly from the image and the reasoning was also sound; the failures in these cases were primarily due to minor calculation errors suggesting that while the model understood the problem and approached it correctly, it stumbled in the final execution. In 37.6 % percent 37.6 37.6\\% 37.6 % of the cases, the image values were read correctly, but the reasoning was incorrect, This is the most frequent error type, indicating that correct reasoning still remains one of the critical gaps even in the state-of-the-art models. In 24.8 % percent 24.8 24.8\\% 24.8 % of the cases, the model misread some information from the images, but the reasoning is sound. That is, had the model extracted the correct information, the final answer could have been correct. This result indicates a second gap in terms of extracting and parsing the correct values from the images and assigning them to the correct components. Finally, in 25.6 % percent 25.6 25.6\\% 25.6 % of the cases, the model struggled both in extracting information from the image and in applying correct reasoning. 4.4 Performance as a Function of Task Properties In Table 1 , we identified multiple distinguishing factors for each of the tasks in 𝖱𝖾𝖬𝖨 𝖱𝖾𝖬𝖨 \\mathsf{ReMI} sansserif_ReMI . Here, we aim to measure and compare model performances for tasks exhibiting each property. We note that a naive averaging of a model’s performance for datasets in each category and comparing to the other category may be flawed due to: 1- Performances on some tasks being generally higher due to the label space being binary or categorical, and 2- some tasks being generally easier/harder than the other tasks. To account for the first issue mentioned above, for each model M 𝑀 M italic_M and task T 𝑇 T italic_T we compute P M , T subscript 𝑃 𝑀 𝑇 P_{M,T} italic_P start_POSTSUBSCRIPT italic_M , italic_T end_POSTSUBSCRIPT as E ⁢ R ⁢ P T ⁢ ( n ⁢ a ⁢ i ⁢ v ⁢ e , M ) 𝐸 𝑅 subscript 𝑃 𝑇 𝑛 𝑎 𝑖 𝑣 𝑒 𝑀 ERP_{T}(naive,M) italic_E italic_R italic_P start_POSTSUBSCRIPT italic_T end_POSTSUBSCRIPT ( italic_n italic_a italic_i italic_v italic_e , italic_M ) , i.e. the model’s error reduction percentage compared to the naive baseline. This corresponds to how much of the error has been reduced by the model when accounting for random guess, normalized by how much room for error reduction existed when accounting for random guess. To account for the second issue, as a proxy for the hardness of the tasks, we use the average performance of our models on each task P T = ∑ M P M , T number of models subscript 𝑃 𝑇 subscript 𝑀 subscript 𝑃 𝑀 𝑇 number of models P_{T}=\\frac{\\sum_{M}P_{M,T}}{\\text{number of models}} italic_P start_POSTSUBSCRIPT italic_T end_POSTSUBSCRIPT = divide start_ARG ∑ start_POSTSUBSCRIPT italic_M end_POSTSUBSCRIPT italic_P start_POSTSUBSCRIPT italic_M , italic_T end_POSTSUBSCRIPT end_ARG start_ARG number of models end_ARG . We then compute the relative gain compared to the average as P M , T ′ = P M , T − P T P T subscript superscript 𝑃 ′ 𝑀 𝑇 subscript 𝑃 𝑀 𝑇 subscript 𝑃 𝑇 subscript 𝑃 𝑇 P^{\\prime}_{M,T}=\\frac{P_{M,T}-P_{T}}{P_{T}} italic_P start_POSTSUPERSCRIPT ′ end_POSTSUPERSCRIPT start_POSTSUBSCRIPT italic_M , italic_T end_POSTSUBSCRIPT = divide start_ARG italic_P start_POSTSUBSCRIPT italic_M , italic_T end_POSTSUBSCRIPT - italic_P start_POSTSUBSCRIPT italic_T end_POSTSUBSCRIPT end_ARG start_ARG italic_P start_POSTSUBSCRIPT italic_T end_POSTSUBSCRIPT end_ARG . Conceptually, this corresponds to the following: After accounting for random noise, how much each model reduced the error with respect to the model-average baseline, on each task. For each model and each group of tasks τ 𝜏 \\tau italic_τ (e.g., all interleaved tasks), we compute and report the average ∑ T ∈ τ P M , T ′ | τ | subscript 𝑇 𝜏 subscript superscript 𝑃 ′ 𝑀 𝑇 𝜏 \\frac{\\sum_{T\\in\\tau}P^{\\prime}_{M,T}}{|\\tau|} divide start_ARG ∑ start_POSTSUBSCRIPT italic_T ∈ italic_τ end_POSTSUBSCRIPT italic_P start_POSTSUPERSCRIPT ′ end_POSTSUPERSCRIPT start_POSTSUBSCRIPT italic_M , italic_T end_POSTSUBSCRIPT end_ARG start_ARG | italic_τ | end_ARG in Figure 7 . According to Figure 7 (a), GPT4 Turbo and Gemini 1.5 (the two best performing models) outperform other models on interleaved tasks more than non-interleaved tasks, showing the progress in the frontier models for this recently emerged capability. Figure 7 (b) compares the tasks that have a maximum of two images to the tasks where the maximum number of images is more than two. We observe a similar behavior as the interleaved vs non-interleaved case, with Gemini 1.5 gains more on the latter tasks. GPT4 Turbo, however, gains equally on both cases. Interestingly, we observe that while Gemini Flash remains competitive on the former tasks, its performance falls behind on the latter group. In Figure 7 (c), for sequence vs set inputs, we see a stark difference for Claude3 Sonnet and Gemini 1.5. Claude3 Sonnet performs better on set type tasks and Gemini 1.5 performs better on sequence type tasks, but almost loses its advantage on set type tasks. Finally, Figure 7 (d) shows that when provided with images corresponding to different concepts, most models show a similar behaviour except for Gemini Ultra that performs better when the concepts are different and GPT4 Turbo that performs better when the concepts are the the same. 4.5 Zeroshot vs Fewshot Performance So far, we examined the performance of various models in a zero-shot setting. We now examine how much of the gap between the model performance and the human performance can be closed by providing fewshot examples as demonstration to the model. Specifically, we prepend two examples along with their manually-written chain of thought solutions to the prompt. We then measured and report E ⁢ R ⁢ P T ⁢ ( zeroshot, fewshot ) 𝐸 𝑅 subscript 𝑃 𝑇 zeroshot, fewshot ERP_{T}(\\text{zeroshot, fewshot}) italic_E italic_R italic_P start_POSTSUBSCRIPT italic_T end_POSTSUBSCRIPT ( zeroshot, fewshot ) corresponding the how much the fewshot model reduced the error compared to the zeroshot model. The results are reported in Figure 8 . Figure 8: ERP(0-shot, 2-shot) for Gemini 1.5. According to the results, we observe that the overall performance of the model on 𝖱𝖾𝖬𝖨 𝖱𝖾𝖬𝖨 \\mathsf{ReMI} sansserif_ReMI improves from 51.5 % percent 51.5 51.5\\% 51.5 % to 57.9 % percent 57.9 57.9\\% 57.9 % corresponding to almost 12.5 % percent 12.5 12.5\\% 12.5 % relative improvement. This shows that LLMs may be capable of learning multi-image reasoning tasks in context and improve their performance. However, the overall performance still remains significantly behind the human baseline which is 95.8 % percent 95.8 95.8\\% 95.8 % . We also see that the amount of improvement is task dependent with some tasks gaining from fewshot examples substantially more than the others. 5 Conclusion We introduced 𝖱𝖾𝖬𝖨 𝖱𝖾𝖬𝖨 \\mathsf{ReMI} sansserif_ReMI , a dedicated benchmark for multi-image reasoning that covers several domains and several key properties that arise when reasoning with multiple images. We evaluated the frontier LLMs on 𝖱𝖾𝖬𝖨 𝖱𝖾𝖬𝖨 \\mathsf{ReMI} sansserif_ReMI and compared their performance to humans. The results show a stark gap between model performance and human performance showing a significant room for improvement in the reasoning capabilities of the current state-of-the-art LLMs. Future work can focus on improving LLMs for the limitations found in our failure analysis and measure how much they translate to improvements on 𝖱𝖾𝖬𝖨 𝖱𝖾𝖬𝖨 \\mathsf{ReMI} sansserif_ReMI . Acknowledgements We thank Behnam Neyshabur for great feedback. References Achiam et al. (2023) J. Achiam, S. Adler, S. Agarwal, L. Ahmad, I. Akkaya, F. L. Aleman, D. Almeida, J. Altenschmidt, S. Altman, S. Anadkat, et al. Gpt-4 technical report. arXiv preprint arXiv:2303.08774 , 2023. Ahrabian et al. (2024) K. Ahrabian, Z. Sourati, K. Sun, J. Zhang, Y. Jiang, F. Morstatter, and J. Pujara. The curious case of nonverbal abstract reasoning with multi-modal large language models. arXiv preprint arXiv:2401.12117 , 2024. Alayrac et al. (2022) J.-B. Alayrac, J. Donahue, P. Luc, A. Miech, I. Barr, Y. Hasson, K. Lenc, A. Mensch, K. Millican, M. Reynolds, et al. Flamingo: a visual language model for few-shot learning. Advances in Neural Information Processing Systems , 35:23716–23736, 2022. Albert and Barabási (2002) R. Albert and A.-L. Barabási. Statistical mechanics of complex networks. Reviews of modern physics , 74(1):47, 2002. Anthropic (2024) A. Anthropic. The claude 3 model family: Opus, sonnet, haiku. Claude-3 Model Card , 2024. Antol et al. (2015) S. Antol, A. Agrawal, J. Lu, M. Mitchell, D. Batra, C. L. Zitnick, and D. Parikh. Vqa: Visual question answering. In Proceedings of the IEEE international conference on computer vision , pages 2425–2433, 2015. Awadalla et al. (2023) A. Awadalla, I. Gao, J. Gardner, J. Hessel, Y. Hanafy, W. Zhu, K. Marathe, Y. Bitton, S. Gadre, S. Sagawa, et al. Openflamingo: An open-source framework for training large autoregressive vision-language models. arXiv preprint arXiv:2308.01390 , 2023. Barabási and Albert (1999) A.-L. Barabási and R. Albert. Emergence of scaling in random networks. science , 286(5439):509–512, 1999. Beyer* et al. (2024) L. Beyer*, A. Steiner*, A. Susano Pinto*, A. Kolesnikov*, X. Wang*, D. Salz, M. Neumann, I. Alabdulmohsin, M. Tschannen, E. Bugliarello, T. Unterthiner, D. Keysers, A. Gritsenko, X. Chen, S. Koppula, A. Grycner, M. Bauer, M. Bošnjak, F. Liu, N. Houlsby, M. Kumar, K. Rong, J. Eisenschlos, M. Minderer, P. Voigtlaender, I. Bica, I. Balazevic, J. Puigcerver, P. Papalampidi, O. Henaff, X. Xiong, R. Soricut, J. Harmsen, and X. Zhai*. PaliGemma: A versatile 3B VLM for transfer, 2024. To appear. Chen et al. (2022) X. Chen, X. Wang, S. Changpinyo, A. Piergiovanni, P. Padlewski, D. Salz, S. Goodman, A. Grycner, B. Mustafa, L. Beyer, et al. Pali: A jointly-scaled multilingual language-image model. arXiv preprint arXiv:2209.06794 , 2022. Clark et al. (2018) P. Clark, I. Cowhey, O. Etzioni, T. Khot, A. Sabharwal, C. Schoenick, and O. Tafjord. Think you have solved question answering? try arc, the ai2 reasoning challenge. arXiv preprint arXiv:1803.05457 , 2018. Cobbe et al. (2021) K. Cobbe, V. Kosaraju, M. Bavarian, M. Chen, H. Jun, L. Kaiser, M. Plappert, J. Tworek, J. Hilton, R. Nakano, et al. Training verifiers to solve math word problems. arXiv preprint arXiv:2110.14168 , 2021. Erdős and Rényi (1959) P. Erdős and A. Rényi. On random graphs. Publicationes Mathematicae Debrecen , 6:290–297, 1959. Fan et al. (2024) L. Fan, W. Hua, X. Li, K. Zhu, M. Jin, L. Li, H. Ling, J. Chi, J. Wang, X. Ma, et al. Nphardeval4v: A dynamic reasoning benchmark of multimodal large language models. arXiv preprint arXiv:2403.01777 , 2024. Fatemi et al. (2024) B. Fatemi, J. Halcrow, and B. Perozzi. Talk like a graph: Encoding graphs for large language models. In ICLR , 2024. Fu et al. (2023) C. Fu, P. Chen, Y. Shen, Y. Qin, M. Zhang, X. Lin, Z. Qiu, W. Lin, J. Yang, X. Zheng, K. Li, X. Sun, and R. Ji. Mme: A comprehensive evaluation benchmark for multimodal large language models. ArXiv , abs/2306.13394, 2023. URL https://api.semanticscholar.org/CorpusID:259243928 . Fu et al. (2024) X. Fu, Y. Hu, B. Li, Y. Feng, H. Wang, X. Lin, D. Roth, N. A. Smith, W.-C. Ma, and R. Krishna. Blink: Multimodal large language models can see but not perceive. arXiv preprint arXiv:2404.12390 , 2024. Goyal et al. (2017) Y. Goyal, T. Khot, D. Summers-Stay, D. Batra, and D. Parikh. Making the v in vqa matter: Elevating the role of image understanding in visual question answering. In Proceedings of the IEEE conference on computer vision and pattern recognition , pages 6904–6913, 2017. Hagberg et al. (2008) A. Hagberg, P. Swart, and D. S Chult. Exploring network structure, dynamics, and function using networkx. Technical report, Los Alamos National Lab.(LANL), Los Alamos, NM (United States), 2008. Hendrycks et al. (2020) D. Hendrycks, C. Burns, S. Basart, A. Zou, M. Mazeika, D. Song, and J. Steinhardt. Measuring massive multitask language understanding. arXiv preprint arXiv:2009.03300 , 2020. Hendrycks et al. (2021) D. Hendrycks, C. Burns, S. Kadavath, A. Arora, S. Basart, E. Tang, D. Song, and J. Steinhardt. Measuring mathematical problem solving with the math dataset. In J. Vanschoren and S. Yeung, editors, Proceedings of the Neural Information Processing Systems Track on Datasets and Benchmarks , volume 1. Curran, 2021. Holland et al. (1983) P. W. Holland, K. B. Laskey, and S. Leinhardt. Stochastic blockmodels: First steps. Social networks , 5(2):109–137, 1983. Huang et al. (2024) S. Huang, L. Dong, W. Wang, Y. Hao, S. Singhal, S. Ma, T. Lv, L. Cui, O. K. Mohammed, B. Patra, et al. Language is not all you need: Aligning perception with language models. Advances in Neural Information Processing Systems , 36, 2024. Kazemi et al. (2023a) M. Kazemi, H. Alvari, A. Anand, J. Wu, X. Chen, and R. Soricut. Geomverse: A systematic evaluation of large models for geometric reasoning. arXiv preprint arXiv:2312.12241 , 2023a. Kazemi et al. (2023b) M. Kazemi, Q. Yuan, D. Bhatia, N. Kim, X. Xu, V. Imbrasaite, and D. Ramachandran. Boardgameqa: A dataset for natural language reasoning with contradictory information. In NeurIPS , 2023b. Kazemzadeh et al. (2014) S. Kazemzadeh, V. Ordonez, M. Matten, and T. Berg. Referitgame: Referring to objects in photographs of natural scenes. In Proceedings of the 2014 conference on empirical methods in natural language processing (EMNLP) , pages 787–798, 2014. Lewkowycz et al. (2022) A. Lewkowycz, A. Andreassen, D. Dohan, E. Dyer, H. Michalewski, V. Ramasesh, A. Slone, C. Anil, I. Schlag, T. Gutman-Solo, et al. Solving quantitative reasoning problems with language models. Advances in Neural Information Processing Systems , 35:3843–3857, 2022. Li et al. (2023a) B. Li, Y. Ge, Y. Ge, G. Wang, R. Wang, R. Zhang, and Y. Shan. Seed-bench-2: Benchmarking multimodal large language models. arXiv preprint arXiv:2311.17092 , 2023a. Li et al. (2023b) J. Li, D. Li, S. Savarese, and S. Hoi. Blip-2: Bootstrapping language-image pre-training with frozen image encoders and large language models. arXiv preprint arXiv:2301.12597 , 2023b. Lin et al. (2014) T.-Y. Lin, M. Maire, S. Belongie, J. Hays, P. Perona, D. Ramanan, P. Dollár, and C. L. Zitnick. Microsoft coco: Common objects in context. In Computer Vision–ECCV 2014: 13th European Conference, Zurich, Switzerland, September 6-12, 2014, Proceedings, Part V 13 , pages 740–755. Springer, 2014. Lindström and Abraham (2022) A. D. Lindström and S. S. Abraham. Clevr-math: A dataset for compositional language, visual and mathematical reasoning. arXiv preprint arXiv:2208.05358 , 2022. Liu et al. (2021) F. Liu, E. Bugliarello, E. M. Ponti, S. Reddy, N. Collier, and D. Elliott. Visually grounded reasoning across languages and cultures. In M.-F. Moens, X. Huang, L. Specia, and S. W.-t. Yih, editors, Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing , pages 10467–10485, Online and Punta Cana, Dominican Republic, Nov. 2021. Association for Computational Linguistics. 10.18653/v1/2021.emnlp-main.818 . URL https://aclanthology.org/2021.emnlp-main.818 . Liu et al. (2024) H. Liu, C. Li, Q. Wu, and Y. J. Lee. Visual instruction tuning. Advances in neural information processing systems , 36, 2024. Liu et al. (2023) Y. Liu, H. Duan, Y. Zhang, B. Li, S. Zhang, W. Zhao, Y. Yuan, J. Wang, C. He, Z. Liu, et al. Mmbench: Is your multi-modal model an all-around player? arXiv preprint arXiv:2307.06281 , 2023. Lu et al. (2021) P. Lu, R. Gong, S. Jiang, L. Qiu, S. Huang, X. Liang, and S.-C. Zhu. Inter-gps: Interpretable geometry problem solving with formal language and symbolic reasoning. arXiv preprint arXiv:2105.04165 , 2021. Lu et al. (2022) P. Lu, S. Mishra, T. Xia, L. Qiu, K.-W. Chang, S.-C. Zhu, O. Tafjord, P. Clark, and A. Kalyan. Learn to explain: Multimodal reasoning via thought chains for science question answering. Advances in Neural Information Processing Systems , 35:2507–2521, 2022. Lu et al. (2023) P. Lu, H. Bansal, T. Xia, J. Liu, C. Li, H. Hajishirzi, H. Cheng, K.-W. Chang, M. Galley, and J. Gao. Mathvista: Evaluating mathematical reasoning of foundation models in visual contexts. arXiv preprint arXiv:2310.02255 , 2023. Marino et al. (2019) K. Marino, M. Rastegari, A. Farhadi, and R. Mottaghi. Ok-vqa: A visual question answering benchmark requiring external knowledge. In Proceedings of the IEEE/cvf conference on computer vision and pattern recognition , pages 3195–3204, 2019. OpenAI (2023) OpenAI. Gpt-4 technical report. arXiv preprint arXiv:2303.08774 , 2023. Paperno et al. (2016) D. Paperno, G. Kruszewski, A. Lazaridou, Q. N. Pham, R. Bernardi, S. Pezzelle, M. Baroni, G. Boleda, and R. Fernández. The lambada dataset: Word prediction requiring a broad discourse context. arXiv preprint arXiv:1606.06031 , 2016. Rajani et al. (2019) N. F. Rajani, B. McCann, C. Xiong, and R. Socher. Explain yourself! leveraging language models for commonsense reasoning. arXiv preprint arXiv:1906.02361 , 2019. Reid et al. (2024) M. Reid, N. Savinov, D. Teplyashin, D. Lepikhin, T. Lillicrap, J.-b. Alayrac, R. Soricut, A. Lazaridou, O. Firat, J. Schrittwieser, et al. Gemini 1.5: Unlocking multimodal understanding across millions of tokens of context. arXiv preprint arXiv:2403.05530 , 2024. Saparov et al. (2024) A. Saparov, R. Y. Pang, V. Padmakumar, N. Joshi, M. Kazemi, N. Kim, and H. He. Testing the general deductive reasoning capacity of large language models using ood examples. Advances in Neural Information Processing Systems , 36, 2024. Shi et al. (2022) F. Shi, M. Suzgun, M. Freitag, X. Wang, S. Srivats, S. Vosoughi, H. W. Chung, Y. Tay, S. Ruder, D. Zhou, et al. Language models are multilingual chain-of-thought reasoners. arXiv preprint arXiv:2210.03057 , 2022. Srivastava et al. (2022) A. Srivastava, A. Rastogi, A. Rao, A. A. M. Shoeb, A. Abid, A. Fisch, A. R. Brown, A. Santoro, A. Gupta, A. Garriga-Alonso, et al. Beyond the imitation game: Quantifying and extrapolating the capabilities of language models. arXiv preprint arXiv:2206.04615 , 2022. Suhr et al. (2017) A. Suhr, M. Lewis, J. Yeh, and Y. Artzi. A corpus of natural language for visual reasoning. In R. Barzilay and M.-Y. Kan, editors, Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics (Volume 2: Short Papers) , pages 217–223, Vancouver, Canada, July 2017. Association for Computational Linguistics. 10.18653/v1/P17-2034 . URL https://aclanthology.org/P17-2034 . Suhr et al. (2019) A. Suhr, S. Zhou, A. Zhang, I. Zhang, H. Bai, and Y. Artzi. A corpus for reasoning about natural language grounded in photographs. In A. Korhonen, D. Traum, and L. Màrquez, editors, Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics , pages 6418–6428, Florence, Italy, July 2019. Association for Computational Linguistics. 10.18653/v1/P19-1644 . URL https://aclanthology.org/P19-1644 . Tafjord et al. (2021) O. Tafjord, B. Dalvi, and P. Clark. ProofWriter: Generating implications, proofs, and abductive statements over natural language. In Findings of the Association for Computational Linguistics: ACL-IJCNLP 2021 , pages 3621–3634, Online, Aug. 2021. Association for Computational Linguistics. 10.18653/v1/2021.findings-acl.317 . URL https://aclanthology.org/2021.findings-acl.317 . Team et al. (2023) G. Team, R. Anil, S. Borgeaud, Y. Wu, J.-B. Alayrac, J. Yu, R. Soricut, J. Schalkwyk, A. M. Dai, A. Hauth, et al. Gemini: a family of highly capable multimodal models. arXiv preprint arXiv:2312.11805 , 2023. Tsimpoukelli et al. (2021) M. Tsimpoukelli, J. L. Menick, S. Cabi, S. Eslami, O. Vinyals, and F. Hill. Multimodal few-shot learning with frozen language models. Advances in Neural Information Processing Systems , 34:200–212, 2021. Wang et al. (2019) A. Wang, Y. Pruksachatkun, N. Nangia, A. Singh, J. Michael, F. Hill, O. Levy, and S. Bowman. Superglue: A stickier benchmark for general-purpose language understanding systems. Advances in neural information processing systems , 32, 2019. Wei et al. (2022) J. Wei, X. Wang, D. Schuurmans, M. Bosma, F. Xia, E. Chi, Q. V. Le, D. Zhou, et al. Chain-of-thought prompting elicits reasoning in large language models. Advances in neural information processing systems , 35:24824–24837, 2022. Yue et al. (2023) X. Yue, Y. Ni, K. Zhang, T. Zheng, R. Liu, G. Zhang, S. Stevens, D. Jiang, W. Ren, Y. Sun, et al. Mmmu: A massive multi-discipline multimodal understanding and reasoning benchmark for expert agi. arXiv preprint arXiv:2311.16502 , 2023. Zellers et al. (2019) R. Zellers, A. Holtzman, Y. Bisk, A. Farhadi, and Y. Choi. Hellaswag: Can a machine really finish your sentence? arXiv preprint arXiv:1905.07830 , 2019. Appendix A Further Failure Analysis In the main text, we provided a high-level summary of the model failures for each task. In Figures 9 , 10 , 11 , 12 , 13 , 14 , 15 16 and 17 we present some examples of model failures on several of our tasks. Figure 9: Model failure: Although the model arrives at the correct answer, the mapping showing the isomorphism between the two graphs is wrong. Figure 10: Model failure: The model mixes the minutes and hours hand on the clock and also reads 08:34 as 9:34 from the table. Figure 11: Model failure: The model fails to recognize the category Astronomy whose value has changed and instead outputs that no value has changed. Figure 12: Model failure: The model read’s the values wrong from the graph. Figure 13: Model failure: The model wrongly interprets the sides of a parallelogram as its base and height and gets completely off track. Figure 14: Model failure: The model reads the incorrect number of bus stops along Liberty Avenue in both the images but luckily gets the answer correct. Figure 15: Model failure: The model interprets the wrong image as showing the path described in the text. Figure 16: Model failure: The model doesn’t seem to understand the pattern properly but guesses the right answer. Figure 17: Model failure: The model fails to correctly match the vertices of the cube to their implicit locations in the image and ends up suggesting to remove the right line of code. (a) Average length of the questions for each of the tasks in 𝖱𝖾𝖬𝖨 𝖱𝖾𝖬𝖨 \\mathsf{ReMI} sansserif_ReMI . (b) Number of unique labels in each task (for 200 examples). (c) Number of problems in 𝖱𝖾𝖬𝖨 𝖱𝖾𝖬𝖨 \\mathsf{ReMI} sansserif_ReMI that have n 𝑛 n italic_n images, for different values of n 𝑛 n italic_n . (d) Model error reduction percentage as a function of the time spent by humans for solving the problems. Figure 18: (a), (b), (c): Statistics for 𝖱𝖾𝖬𝖨 𝖱𝖾𝖬𝖨 \\mathsf{ReMI} sansserif_ReMI and its tasks. (d) Model performance vs human time. Appendix B Statistics about 𝖱𝖾𝖬𝖨 𝖱𝖾𝖬𝖨 \\mathsf{ReMI} sansserif_ReMI As mentioned in the main text, the problems in 𝖱𝖾𝖬𝖨 𝖱𝖾𝖬𝖨 \\mathsf{ReMI} sansserif_ReMI contain at least two images. Figure 18 (a) shows the average length of the questions for each task in 𝖱𝖾𝖬𝖨 𝖱𝖾𝖬𝖨 \\mathsf{ReMI} sansserif_ReMI indicating a wide range of questions lengths across tasks. Figure 18 (b) shows the number of unique labels that each task has (e.g., for binary tasks, there are two unique labels). Figure 18 (c) provides the statistics of the number of problems that have a specific number of images. Appendix C Details about the Tasks in 𝖱𝖾𝖬𝖨 𝖱𝖾𝖬𝖨 \\mathsf{ReMI} sansserif_ReMI Below, we provide a detailed description of how each task in 𝖱𝖾𝖬𝖨 𝖱𝖾𝖬𝖨 \\mathsf{ReMI} sansserif_ReMI has been created. • 𝖤𝗆𝗈𝗃𝗂𝖠𝗅𝗀𝖾𝖻𝗋𝖺 𝖤𝗆𝗈𝗃𝗂𝖠𝗅𝗀𝖾𝖻𝗋𝖺 \\mathsf{EmojiAlgebra} sansserif_EmojiAlgebra : We created random systems of linear equation where the values for the variables can be derived one-by-one by looking at the equation for which the value for all variables on the right-hand side is known. We also created a random expression with those variables whose value was to be computed. We then created images by replacing the variables with emojis. • 𝖥𝗎𝗇𝖼𝖱𝖾𝖺𝖽 𝖥𝗎𝗇𝖼𝖱𝖾𝖺𝖽 \\mathsf{FuncRead} sansserif_FuncRead : To create this task, we sampled polynomial functions of degree 1, 2, or 3 and plotted their graphs using the matplotlib library. Then we ask the following questions about them: reading values from different functions and summing or subtracting them, computing the limit of a function that is defined as one of the graphs for some domain of values and the other graph for the values outside that domain, function composition, finding a value of interest (e.g., where the derivative is zero) from one graph and reading the other function value at that point, and finding the graph that corresponds to a given function. • 𝖦𝖾𝗈𝗆𝖲𝗁𝖺𝗉𝖾𝗌 𝖦𝖾𝗈𝗆𝖲𝗁𝖺𝗉𝖾𝗌 \\mathsf{GeomShapes} sansserif_GeomShapes : We generated this dataset by sampling a shape from a set of pre-defined shapes. Each shape has fixed number of pre-defined formulas associated with it corresponding to area, perimeter, angles etc. For each formula, we have input elements and output elements. We first sample one shape and its formula and assign values to input elements, the output element of this formula would be shared with another shape. We then sample another shape and formula whose atleast one input element say x 𝑥 x italic_x is of same type as output element of the first shape. We assign this element x 𝑥 x italic_x from the computation of first shape but hide it in the question. We then proceed to ask the question based on this output formula of second shape. The two questions share this element x 𝑥 x italic_x which is indicated in the question. This task is an extension of GeomVerse (Kazemi et al., 2023a ) . • 𝖦𝖾𝗈𝗆𝖢𝗈𝗌𝗍 𝖦𝖾𝗈𝗆𝖢𝗈𝗌𝗍 \\mathsf{GeomCost} sansserif_GeomCost : We generated this dataset by sampling a shape from a pre-selected set of shapes like triangle, parallelogram, square, rectangle etc and selecting one formula out of perimeter and area corresponding to this shape and assigned all the values corresponding to the perimeter or area value. We then choose a template story correspondinf to fencing a boundary, icing the cake etc. out of 10 pre-defined template texts and choose a table corresponding to this template. The table designs are also varied slightly out of fixed number of styles. The cost values are assigned randomly from 1-100. This task is also an extension of GeomVerse (Kazemi et al., 2023a ) . • 𝖢𝗈𝗅𝗅𝗂𝗌𝗂𝗈𝗇𝗌 𝖢𝗈𝗅𝗅𝗂𝗌𝗂𝗈𝗇𝗌 \\mathsf{Collisions} sansserif_Collisions : We created visualizations of two-object collisions, varying initial positions (horizontal, vertical, angled) and randomly assigning masses and velocities. For each collision pair, we then assessed elasticity, coefficient of restitution, and conservation of kinetic energy and momentum. • 𝖢𝗅𝗈𝖼𝗄𝗌 𝖢𝗅𝗈𝖼𝗄𝗌 \\mathsf{Clocks} sansserif_Clocks : We generated clock images with different shape, color, style, number representations, etc. using tikz code. Each clock shows a random time and a AM or PM is also added randomly to the image as well. Then, for each pair of images, we compute the difference between their times in terms of minutes and use that as the label. • 𝖲𝖼𝗁𝖾𝖽𝗎𝗅𝖾 𝖲𝖼𝗁𝖾𝖽𝗎𝗅𝖾 \\mathsf{Schedule} sansserif_Schedule : We generated one clock image showing a random time, similar to the way it was generated for the 𝖢𝗅𝗈𝖼𝗄𝗌 𝖢𝗅𝗈𝖼𝗄𝗌 \\mathsf{Clocks} sansserif_Clocks . Then, we also generated a random table with different columns (departure time, arrival time, train name, gate, etc.) and with different styles (colors, horizontal/vertical line separators, text rotation, multi-line text, etc.) that included information about the events happening at various times. We then asked questions about the next event happening given the current time shown on the clock. • 𝖢𝗁𝖺𝗋𝗍𝗌 𝖢𝗁𝖺𝗋𝗍𝗌 \\mathsf{Charts} sansserif_Charts : We first randomly generate data matrices and series that are suitable for plotting into four different types of charts: (1) heatmap (2) bar chart (3) line chart (4) pie chart. Then we create a modified version of the data series or matrices by randomly editing one to a few values. This way we obtain pairs of edited data matrices/series. Then we use the Matplotlib library to plot each data matrix/series into a chart by randomly selecting a suitable chart type and randomly choosing a color scheme, layout, etc. for the chart. Heuristics is applied to guarantee that the selected chart type is suitable for plotting the data. Finally we sample from a set of question templates to form QA pairs for each pair of chart. The templates include simple elementary reasoning questions across the two charts or detecting differences of the two charts. • 𝖢𝗈𝖽𝖾𝖤𝖽𝗂𝗍 𝖢𝗈𝖽𝖾𝖤𝖽𝗂𝗍 \\mathsf{CodeEdit} sansserif_CodeEdit : We first asked a language model to generate tikz code for a list of random objects. We then comment out a single line in the code and recompile it. We only keep the examples where the edited version compiles correctly, and the compiled image is not equal to the original image. A few filters were applied to ensure the edited image is sensible (e.g. the code being removed is not a variable definition or the beginning of a for loop); specifically, the removed code line had to start with \\draw or \\filldraw and end with a ; . • 𝖨𝗌𝗈𝗆𝗈𝗋𝗉𝗁𝗂𝗌𝗆 𝖨𝗌𝗈𝗆𝗈𝗋𝗉𝗁𝗂𝗌𝗆 \\mathsf{Isomorphism} sansserif_Isomorphism : We used the NetworkX library Hagberg et al. ( 2008 ) to generate random graphs using one of the following generators: Erdős-Rényi (ER) graphs (Erdős and Rényi, 1959 ) , scale-free networks (SFN) (Barabási and Albert, 1999 ) , graphs following the Barabási–Albert (BA) model (Albert and Barabási, 2002 ) and stochastic block model (SBM) (Holland et al., 1983 ) , as well as star, path and complete graphs. Then, for positive examples (i.e. examples where the two graphs are isomorphic), we visualized the same graph with different NetworkX layout, different names for the nodes, and different styles. For the non-isomorphic case, we either sampled two random graphs (this produces easy negative examples) or sampled one random graph and slightly modified it by adding/removing one or two nodes/edges (this produces hard negative examples). This dataset is in part inspired by the works of Fan et al. ( 2024 ); Fatemi et al. ( 2024 ) . • 𝖬𝖺𝗉𝗌 𝖬𝖺𝗉𝗌 \\mathsf{Maps} sansserif_Maps : Our curated Maps dataset consists of both synthetic and real world examples. We first describe the curation process for the synthetic examples. For synthetic counting queries, we first generate a grid with five horizontal streets and five vertical streets. The street names are randomly assigned in [A..Z]. We then place points of interest (POIs) (gas stations, coffee shops, shopping center and bus stops) at various blocks. We process each block and with a sampling probability of p = 0.1 𝑝 0.1 p=0.1 italic_p = 0.1 decide whether to place a POI or not. We then pick a POI at random from the list and place it at the block. Similarly, we place traffic lights and stop signs at each corner with a sampling probability of p = 0.1 𝑝 0.1 p=0.1 italic_p = 0.1 . To generate the second image we copy the above constructed grid and pick at random a particular street. We then pick at random a particular POI on the street and place additional copies of the POI on the street. With a small probability of 0.05 we leave the second image unchanged. Similarly, for the direction matching queries we generate a grid image as above. We pick a random start and end point and pick a random set of directions between them. We split this direction at a random point to generate two of the four images containing the partial directions. The remaining two images are constructed by picking two different distinct directions at random. For the case of real data we first prompted a language model to generate a list of 100 cities and an associated street/avenue in that city. We then take this list and for each entry we get two images from Google Maps API that are centered at the particular street. We then manually study the two images and look for distinguishing features (such as bus stops, places of worship, hotels etc.) to construct the query. • 𝖱𝖾𝖿𝖢𝖮𝖢𝖮 𝖱𝖾𝖿𝖢𝖮𝖢𝖮 \\mathsf{RefCOCO} sansserif_RefCOCO : We sampled 500 imagees from the refCOCO (Kazemzadeh et al., 2014 ) dataset. We then sample 15 points to lie uniformly randomly across the image. We then choose the points that overlap with the goal object as follows. We have the ground truth bounding box of the referred object from the original dataset. We first select the datapoints where at least 1, but less than 8, and include these in label_inbbox . We th ar ethe points in the center 25% of the bbob, and so on for provide various precisions for points with ‘most overlap’: label_mindist_bboxcenter is the point that is the closest to the center of the bounding box. label_25p_tolerance are the labels in the middle 25% of the bounding box and so on for the label_50p_tolerance and label_75p_tolerance . Finally we manually check all the datapoints to ensure that the labels points actually overlap with the goal object. • 𝖨𝖰 𝖨𝖰 \\mathsf{IQ} sansserif_IQ : We created simple IQ tests where a grid of 2x2 is given as input whose bottom-left value is missing, and four choices are provided as the possible answers from which the model has to select one. The images on the top row are two shapes that are different only in terms of one logical operation. The model has to identify that operation and apply it to the image on the bottom left to find the final answer. We included a number of different shapes (triangles, rectangles, pentagons, parallelograms, etc.) and a number of different logical operations (border color, border pattern, fill color, hatch style, change in shape, etc.). This task is similar in nature to the IQ tasks in Ahrabian et al. ( 2024 ); Huang et al. ( 2024 ) , but the choices are provided as separate images. Quality Check: To ensure high quality, we went through multiple rounds of checking, where the questions and answers for each task were examined by multiple authors to see any problems can be identified, including whether the label is correct, whether the instructions provided are sufficient to solve the problem and output it in the right format, whether the text of the question is clearly written, whether the images are clearly understandable and the quantities are easily readable, etc. This procedure was done until no more issues could be found for any of the tasks. As a second level of quality check, once we performed our human evaluation, we manually looked into the questions where the label provided by the humans disagreed with our labels to ensure that our labels are indeed the correct ones. Appendix D Model Performance vs Human Time In the main text, we reported the average time per problem spent by humans for each task. One may expect that if humans spent more time on a set of problems, those problems might be more difficult for the models. To verify this hypothesis, we fit linear functions to the model performances as a function of time spent by humans and report the results in Figure 18 (d). We observe that only for two of the models (Gemini Ultra and Gemini Flash) the performance goes down as a function of spent time. For other models, the performance almost remains flat. Appendix E Experimental Setup For all of the tasks in 𝖱𝖾𝖬𝖨 𝖱𝖾𝖬𝖨 \\mathsf{ReMI} sansserif_ReMI , we allowed the models a maximum of 512 output tokens as we observed that when models went beyond that, they were mostly stuck in a wrong path that did not reach a solution and that models could not recover from it. We prompted the model to produce a JSON with two fields: \"explanation\" containing the step by step reasoning of the model, and the \"answer\" containing the final answer. We measured the average number of responses that either ended prematurely or did not produce a valid JSON for each model and observed that the numbers were small. Specifically, the numbers for Claude3 Sonnet, Gemini Ultra, Gemini Flash, Gemini 1.5, and GPT4 Turbo were 0.4, 0.3, 0.5, 0.8 and 1.9 percent respectively. For Gemini and Claude, we used the Vertex AI API. For GPT4 Turbo, we used the OpenAI API. To compute the final performance, we did the following postprocessing on the golden and predicted labels: 1- in the case of string outputs, we lowercased both golden and predicted answers before comparing them, 2- if the predicted label had an extra or missing ( ) () ( ) around the final answer, we still counted it as true, 3- if the predicted label contained extra units (e.g., producing 20 % percent 20 20\\% 20 % instead of 20 20 20 20 ), we still counted it as true, 4- for the 𝖢𝗈𝖽𝖾𝖤𝖽𝗂𝗍 𝖢𝗈𝖽𝖾𝖤𝖽𝗂𝗍 \\mathsf{CodeEdit} sansserif_CodeEdit , some lines of codes contained a comment after the code; we considered a predicted label to be true regardless of whether it output the comments or not, 5- we ignored spacing issues and assumed a predicted label to be correct even if it had extra or missing spaces, and finally 6- for the 𝖥𝗎𝗇𝖼𝖱𝖾𝖺𝖽 𝖥𝗎𝗇𝖼𝖱𝖾𝖺𝖽 \\mathsf{FuncRead} sansserif_FuncRead , if the golden label was, e.g., f 𝑓 f italic_f and the predicted label was f ⁢ ( x ) 𝑓 𝑥 f(x) italic_f ( italic_x ) , we counted it as correct. Appendix F Limitations • While our dataset covers a wide range of domains where reasoning over multiple images is required, there may still be many other domains where such reasoning is required that are not covered in our dataset e.g., reasoning about chemicals, reasoning about music sheets, etc.). • In our experiments for measuring performance as a function of task properties, we had to use proxies to tease apart the effect of random chance and task difficulty. It is possible that with a different procedure for teasing these effects apart, the results change slightly. For this reason, the general patterns observed in those experiments are more important that the small numeric differences. Generated on Thu Jun 13 14:26:40 2024 by L a T e XML ",
      "fetch_method": "direct-html"
    },
    {
      "arxiv_id": "https://arxiv.org/abs/2203.10244",
      "full_text": " [2203.10244] ChartQA: A Benchmark for Question Answering about Charts with Visual and Logical Reasoning arXiv Is Hiring a DevOps Engineer Work on one of the world's most important websites and make an impact on open science. View Jobs Skip to main content arXiv Is Hiring a DevOps Engineer View Jobs We gratefully acknowledge support from the Simons Foundation, member institutions , and all contributors. Donate &gt; cs &gt; arXiv:2203.10244 Help | Advanced Search All fields Title Author Abstract Comments Journal reference ACM classification MSC classification Report number arXiv identifier DOI ORCID arXiv author ID Help pages Full text Search open search GO open navigation menu quick links Login Help Pages About --> Computer Science > Computation and Language arXiv:2203.10244 (cs) [Submitted on 19 Mar 2022] Title: ChartQA: A Benchmark for Question Answering about Charts with Visual and Logical Reasoning Authors: Ahmed Masry , Do Xuan Long , Jia Qing Tan , Shafiq Joty , Enamul Hoque View a PDF of the paper titled ChartQA: A Benchmark for Question Answering about Charts with Visual and Logical Reasoning, by Ahmed Masry and 4 other authors View PDF Abstract: Charts are very popular for analyzing data. When exploring charts, people often ask a variety of complex reasoning questions that involve several logical and arithmetic operations. They also commonly refer to visual features of a chart in their questions. However, most existing datasets do not focus on such complex reasoning questions as their questions are template-based and answers come from a fixed-vocabulary. In this work, we present a large-scale benchmark covering 9.6K human-written questions as well as 23.1K questions generated from human-written chart summaries. To address the unique challenges in our benchmark involving visual and logical reasoning over charts, we present two transformer-based models that combine visual features and the data table of the chart in a unified way to answer questions. While our models achieve the state-of-the-art results on the previous datasets as well as on our benchmark, the evaluation also reveals several challenges in answering complex reasoning questions. Comments: Accepted by ACL 2022 Findings Subjects: Computation and Language (cs.CL) Cite as: arXiv:2203.10244 [cs.CL] &nbsp; (or arXiv:2203.10244v1 [cs.CL] for this version) &nbsp; https://doi.org/10.48550/arXiv.2203.10244 Focus to learn more arXiv-issued DOI via DataCite Submission history From: Ahmed Masry [ view email ] [v1] Sat, 19 Mar 2022 05:00:30 UTC (10,600 KB) Full-text links: Access Paper: View a PDF of the paper titled ChartQA: A Benchmark for Question Answering about Charts with Visual and Logical Reasoning, by Ahmed Masry and 4 other authors View PDF TeX Source Other Formats view license Current browse context: cs.CL &lt;&nbsp;prev &nbsp; | &nbsp; next&nbsp;&gt; new | recent | 2022-03 Change to browse by: cs References &amp; Citations NASA ADS Google Scholar Semantic Scholar a export BibTeX citation Loading... BibTeX formatted citation &times; loading... Data provided by: Bookmark Bibliographic Tools Bibliographic and Citation Tools Bibliographic Explorer Toggle Bibliographic Explorer ( What is the Explorer? ) Connected Papers Toggle Connected Papers ( What is Connected Papers? ) Litmaps Toggle Litmaps ( What is Litmaps? ) scite.ai Toggle scite Smart Citations ( What are Smart Citations? ) Code, Data, Media Code, Data and Media Associated with this Article alphaXiv Toggle alphaXiv ( What is alphaXiv? ) Links to Code Toggle CatalyzeX Code Finder for Papers ( What is CatalyzeX? ) DagsHub Toggle DagsHub ( What is DagsHub? ) GotitPub Toggle Gotit.pub ( What is GotitPub? ) Huggingface Toggle Hugging Face ( What is Huggingface? ) Links to Code Toggle Papers with Code ( What is Papers with Code? ) ScienceCast Toggle ScienceCast ( What is ScienceCast? ) Demos Demos Replicate Toggle Replicate ( What is Replicate? ) Spaces Toggle Hugging Face Spaces ( What is Spaces? ) Spaces Toggle TXYZ.AI ( What is TXYZ.AI? ) Related Papers Recommenders and Search Tools Link to Influence Flower Influence Flower ( What are Influence Flowers? ) Core recommender toggle CORE Recommender ( What is CORE? ) Author Venue Institution Topic About arXivLabs arXivLabs: experimental projects with community collaborators arXivLabs is a framework that allows collaborators to develop and share new arXiv features directly on our website. Both individuals and organizations that work with arXivLabs have embraced and accepted our values of openness, community, excellence, and user data privacy. arXiv is committed to these values and only works with partners that adhere to them. Have an idea for a project that will add value for arXiv's community? Learn more about arXivLabs . Which authors of this paper are endorsers? | Disable MathJax ( What is MathJax? ) About Help contact arXiv Click here to contact arXiv Contact subscribe to arXiv mailings Click here to subscribe Subscribe Copyright Privacy Policy Web Accessibility Assistance arXiv Operational Status Get status notifications via email or slack ",
      "fetch_method": "direct-html"
    },
    {
      "arxiv_id": "https://arxiv.org/abs/2404.12390",
      "full_text": " [2404.12390] BLINK: Multimodal Large Language Models Can See but Not Perceive Skip to main content We gratefully acknowledge support from the Simons Foundation, member institutions , and all contributors. Donate &gt; cs &gt; arXiv:2404.12390 Help | Advanced Search All fields Title Author Abstract Comments Journal reference ACM classification MSC classification Report number arXiv identifier DOI ORCID arXiv author ID Help pages Full text Search open search GO open navigation menu quick links Login Help Pages About --> Computer Science > Computer Vision and Pattern Recognition arXiv:2404.12390 (cs) [Submitted on 18 Apr 2024 ( v1 ), last revised 3 Jul 2024 (this version, v4)] Title: BLINK: Multimodal Large Language Models Can See but Not Perceive Authors: Xingyu Fu , Yushi Hu , Bangzheng Li , Yu Feng , Haoyu Wang , Xudong Lin , Dan Roth , Noah A. Smith , Wei-Chiu Ma , Ranjay Krishna View a PDF of the paper titled BLINK: Multimodal Large Language Models Can See but Not Perceive, by Xingyu Fu and 9 other authors View PDF Abstract: We introduce Blink, a new benchmark for multimodal language models (LLMs) that focuses on core visual perception abilities not found in other evaluations. Most of the Blink tasks can be solved by humans &#34;within a blink&#34; (e.g., relative depth estimation, visual correspondence, forensics detection, and multi-view reasoning). However, we find these perception-demanding tasks cast significant challenges for current multimodal LLMs because they resist mediation through natural language. Blink reformats 14 classic computer vision tasks into 3,807 multiple-choice questions, paired with single or multiple images and visual prompting. While humans get 95.70% accuracy on average, Blink is surprisingly challenging for existing multimodal LLMs: even the best-performing GPT-4V and Gemini achieve accuracies of 51.26% and 45.72%, only 13.17% and 7.63% higher than random guessing, indicating that such perception abilities have not &#34;emerged&#34; yet in recent multimodal LLMs. Our analysis also highlights that specialist CV models could solve these problems much better, suggesting potential pathways for future improvements. We believe Blink will stimulate the community to help multimodal LLMs catch up with human-level visual perception. Comments: Multimodal Benchmark, Project Url: this https URL , ECCV 2024 Subjects: Computer Vision and Pattern Recognition (cs.CV) ; Artificial Intelligence (cs.AI); Computation and Language (cs.CL) Cite as: arXiv:2404.12390 [cs.CV] &nbsp; (or arXiv:2404.12390v4 [cs.CV] for this version) &nbsp; https://doi.org/10.48550/arXiv.2404.12390 Focus to learn more arXiv-issued DOI via DataCite Submission history From: Xingyu Fu [ view email ] [v1] Thu, 18 Apr 2024 17:59:54 UTC (31,815 KB) [v2] Thu, 25 Apr 2024 01:55:49 UTC (31,815 KB) [v3] Sat, 4 May 2024 05:25:26 UTC (31,780 KB) [v4] Wed, 3 Jul 2024 08:44:45 UTC (31,781 KB) Full-text links: Access Paper: View a PDF of the paper titled BLINK: Multimodal Large Language Models Can See but Not Perceive, by Xingyu Fu and 9 other authors View PDF TeX Source Other Formats view license Current browse context: cs.CV &lt;&nbsp;prev &nbsp; | &nbsp; next&nbsp;&gt; new | recent | 2024-04 Change to browse by: cs cs.AI cs.CL References &amp; Citations NASA ADS Google Scholar Semantic Scholar a export BibTeX citation Loading... BibTeX formatted citation &times; loading... Data provided by: Bookmark Bibliographic Tools Bibliographic and Citation Tools Bibliographic Explorer Toggle Bibliographic Explorer ( What is the Explorer? ) Connected Papers Toggle Connected Papers ( What is Connected Papers? ) Litmaps Toggle Litmaps ( What is Litmaps? ) scite.ai Toggle scite Smart Citations ( What are Smart Citations? ) Code, Data, Media Code, Data and Media Associated with this Article alphaXiv Toggle alphaXiv ( What is alphaXiv? ) Links to Code Toggle CatalyzeX Code Finder for Papers ( What is CatalyzeX? ) DagsHub Toggle DagsHub ( What is DagsHub? ) GotitPub Toggle Gotit.pub ( What is GotitPub? ) Huggingface Toggle Hugging Face ( What is Huggingface? ) Links to Code Toggle Papers with Code ( What is Papers with Code? ) ScienceCast Toggle ScienceCast ( What is ScienceCast? ) Demos Demos Replicate Toggle Replicate ( What is Replicate? ) Spaces Toggle Hugging Face Spaces ( What is Spaces? ) Spaces Toggle TXYZ.AI ( What is TXYZ.AI? ) Related Papers Recommenders and Search Tools Link to Influence Flower Influence Flower ( What are Influence Flowers? ) Core recommender toggle CORE Recommender ( What is CORE? ) Author Venue Institution Topic About arXivLabs arXivLabs: experimental projects with community collaborators arXivLabs is a framework that allows collaborators to develop and share new arXiv features directly on our website. Both individuals and organizations that work with arXivLabs have embraced and accepted our values of openness, community, excellence, and user data privacy. arXiv is committed to these values and only works with partners that adhere to them. Have an idea for a project that will add value for arXiv's community? Learn more about arXivLabs . Which authors of this paper are endorsers? | Disable MathJax ( What is MathJax? ) About Help contact arXiv Click here to contact arXiv Contact subscribe to arXiv mailings Click here to subscribe Subscribe Copyright Privacy Policy Web Accessibility Assistance arXiv Operational Status Get status notifications via email or slack ",
      "fetch_method": "direct-html"
    },
    {
      "arxiv_id": "https://arxiv.org/abs/1810.12440",
      "full_text": " [1810.12440] TallyQA: Answering Complex Counting Questions Skip to main content We gratefully acknowledge support from the Simons Foundation, member institutions , and all contributors. Donate &gt; cs &gt; arXiv:1810.12440 Help | Advanced Search All fields Title Author Abstract Comments Journal reference ACM classification MSC classification Report number arXiv identifier DOI ORCID arXiv author ID Help pages Full text Search open search GO open navigation menu quick links Login Help Pages About --> Computer Science > Computer Vision and Pattern Recognition arXiv:1810.12440 (cs) [Submitted on 29 Oct 2018 ( v1 ), last revised 31 Oct 2018 (this version, v2)] Title: TallyQA: Answering Complex Counting Questions Authors: Manoj Acharya , Kushal Kafle , Christopher Kanan View a PDF of the paper titled TallyQA: Answering Complex Counting Questions, by Manoj Acharya and 2 other authors View PDF Abstract: Most counting questions in visual question answering (VQA) datasets are simple and require no more than object detection. Here, we study algorithms for complex counting questions that involve relationships between objects, attribute identification, reasoning, and more. To do this, we created TallyQA, the world&#39;s largest dataset for open-ended counting. We propose a new algorithm for counting that uses relation networks with region proposals. Our method lets relation networks be efficiently used with high-resolution imagery. It yields state-of-the-art results compared to baseline and recent systems on both TallyQA and the HowMany-QA benchmark. Comments: To appear in AAAI 2019 ( To download the dataset please go to this http URL ) Subjects: Computer Vision and Pattern Recognition (cs.CV) Cite as: arXiv:1810.12440 [cs.CV] &nbsp; (or arXiv:1810.12440v2 [cs.CV] for this version) &nbsp; https://doi.org/10.48550/arXiv.1810.12440 Focus to learn more arXiv-issued DOI via DataCite Submission history From: Manoj Acharya [ view email ] [v1] Mon, 29 Oct 2018 22:29:45 UTC (8,600 KB) [v2] Wed, 31 Oct 2018 18:32:07 UTC (9,240 KB) Full-text links: Access Paper: View a PDF of the paper titled TallyQA: Answering Complex Counting Questions, by Manoj Acharya and 2 other authors View PDF TeX Source Other Formats view license Current browse context: cs.CV &lt;&nbsp;prev &nbsp; | &nbsp; next&nbsp;&gt; new | recent | 2018-10 Change to browse by: cs References &amp; Citations NASA ADS Google Scholar Semantic Scholar DBLP - CS Bibliography listing | bibtex Manoj Acharya Kushal Kafle Christopher Kanan a export BibTeX citation Loading... BibTeX formatted citation &times; loading... Data provided by: Bookmark Bibliographic Tools Bibliographic and Citation Tools Bibliographic Explorer Toggle Bibliographic Explorer ( What is the Explorer? ) Connected Papers Toggle Connected Papers ( What is Connected Papers? ) Litmaps Toggle Litmaps ( What is Litmaps? ) scite.ai Toggle scite Smart Citations ( What are Smart Citations? ) Code, Data, Media Code, Data and Media Associated with this Article alphaXiv Toggle alphaXiv ( What is alphaXiv? ) Links to Code Toggle CatalyzeX Code Finder for Papers ( What is CatalyzeX? ) DagsHub Toggle DagsHub ( What is DagsHub? ) GotitPub Toggle Gotit.pub ( What is GotitPub? ) Huggingface Toggle Hugging Face ( What is Huggingface? ) Links to Code Toggle Papers with Code ( What is Papers with Code? ) ScienceCast Toggle ScienceCast ( What is ScienceCast? ) Demos Demos Replicate Toggle Replicate ( What is Replicate? ) Spaces Toggle Hugging Face Spaces ( What is Spaces? ) Spaces Toggle TXYZ.AI ( What is TXYZ.AI? ) Related Papers Recommenders and Search Tools Link to Influence Flower Influence Flower ( What are Influence Flowers? ) Core recommender toggle CORE Recommender ( What is CORE? ) Author Venue Institution Topic About arXivLabs arXivLabs: experimental projects with community collaborators arXivLabs is a framework that allows collaborators to develop and share new arXiv features directly on our website. Both individuals and organizations that work with arXivLabs have embraced and accepted our values of openness, community, excellence, and user data privacy. arXiv is committed to these values and only works with partners that adhere to them. Have an idea for a project that will add value for arXiv's community? Learn more about arXivLabs . Which authors of this paper are endorsers? | Disable MathJax ( What is MathJax? ) About Help contact arXiv Click here to contact arXiv Contact subscribe to arXiv mailings Click here to subscribe Subscribe Copyright Privacy Policy Web Accessibility Assistance arXiv Operational Status Get status notifications via email or slack ",
      "fetch_method": "direct-html"
    },
    {
      "arxiv_id": "https://arxiv.org/abs/1908.02660",
      "full_text": " [1908.02660] SpatialSense: An Adversarially Crowdsourced Benchmark for Spatial Relation Recognition Skip to main content We gratefully acknowledge support from the Simons Foundation, member institutions , and all contributors. Donate &gt; cs &gt; arXiv:1908.02660 Help | Advanced Search All fields Title Author Abstract Comments Journal reference ACM classification MSC classification Report number arXiv identifier DOI ORCID arXiv author ID Help pages Full text Search open search GO open navigation menu quick links Login Help Pages About --> Computer Science > Computer Vision and Pattern Recognition arXiv:1908.02660 (cs) [Submitted on 7 Aug 2019 ( v1 ), last revised 29 Aug 2019 (this version, v2)] Title: SpatialSense: An Adversarially Crowdsourced Benchmark for Spatial Relation Recognition Authors: Kaiyu Yang , Olga Russakovsky , Jia Deng View a PDF of the paper titled SpatialSense: An Adversarially Crowdsourced Benchmark for Spatial Relation Recognition, by Kaiyu Yang and 2 other authors View PDF Abstract: Understanding the spatial relations between objects in images is a surprisingly challenging task. A chair may be &#34;behind&#34; a person even if it appears to the left of the person in the image (depending on which way the person is facing). Two students that appear close to each other in the image may not in fact be &#34;next to&#34; each other if there is a third student between them. We introduce SpatialSense, a dataset specializing in spatial relation recognition which captures a broad spectrum of such challenges, allowing for proper benchmarking of computer vision techniques. SpatialSense is constructed through adversarial crowdsourcing, in which human annotators are tasked with finding spatial relations that are difficult to predict using simple cues such as 2D spatial configuration or language priors. Adversarial crowdsourcing significantly reduces dataset bias and samples more interesting relations in the long tail compared to existing datasets. On SpatialSense, state-of-the-art recognition models perform comparably to simple baselines, suggesting that they rely on straightforward cues instead of fully reasoning about this complex task. The SpatialSense benchmark provides a path forward to advancing the spatial reasoning capabilities of computer vision systems. The dataset and code are available at this https URL . Comments: Accepted to ICCV 2019 Subjects: Computer Vision and Pattern Recognition (cs.CV) Cite as: arXiv:1908.02660 [cs.CV] &nbsp; (or arXiv:1908.02660v2 [cs.CV] for this version) &nbsp; https://doi.org/10.48550/arXiv.1908.02660 Focus to learn more arXiv-issued DOI via DataCite Submission history From: Kaiyu Yang [ view email ] [v1] Wed, 7 Aug 2019 14:41:30 UTC (5,667 KB) [v2] Thu, 29 Aug 2019 20:30:38 UTC (5,667 KB) Full-text links: Access Paper: View a PDF of the paper titled SpatialSense: An Adversarially Crowdsourced Benchmark for Spatial Relation Recognition, by Kaiyu Yang and 2 other authors View PDF TeX Source Other Formats view license Current browse context: cs.CV &lt;&nbsp;prev &nbsp; | &nbsp; next&nbsp;&gt; new | recent | 2019-08 Change to browse by: cs References &amp; Citations NASA ADS Google Scholar Semantic Scholar DBLP - CS Bibliography listing | bibtex Kaiyu Yang Olga Russakovsky Jia Deng export BibTeX citation Loading... BibTeX formatted citation &times; loading... Data provided by: Bookmark Bibliographic Tools Bibliographic and Citation Tools Bibliographic Explorer Toggle Bibliographic Explorer ( What is the Explorer? ) Connected Papers Toggle Connected Papers ( What is Connected Papers? ) Litmaps Toggle Litmaps ( What is Litmaps? ) scite.ai Toggle scite Smart Citations ( What are Smart Citations? ) Code, Data, Media Code, Data and Media Associated with this Article alphaXiv Toggle alphaXiv ( What is alphaXiv? ) Links to Code Toggle CatalyzeX Code Finder for Papers ( What is CatalyzeX? ) DagsHub Toggle DagsHub ( What is DagsHub? ) GotitPub Toggle Gotit.pub ( What is GotitPub? ) Huggingface Toggle Hugging Face ( What is Huggingface? ) Links to Code Toggle Papers with Code ( What is Papers with Code? ) ScienceCast Toggle ScienceCast ( What is ScienceCast? ) Demos Demos Replicate Toggle Replicate ( What is Replicate? ) Spaces Toggle Hugging Face Spaces ( What is Spaces? ) Spaces Toggle TXYZ.AI ( What is TXYZ.AI? ) Related Papers Recommenders and Search Tools Link to Influence Flower Influence Flower ( What are Influence Flowers? ) Core recommender toggle CORE Recommender ( What is CORE? ) Author Venue Institution Topic About arXivLabs arXivLabs: experimental projects with community collaborators arXivLabs is a framework that allows collaborators to develop and share new arXiv features directly on our website. Both individuals and organizations that work with arXivLabs have embraced and accepted our values of openness, community, excellence, and user data privacy. arXiv is committed to these values and only works with partners that adhere to them. Have an idea for a project that will add value for arXiv's community? Learn more about arXivLabs . Which authors of this paper are endorsers? | Disable MathJax ( What is MathJax? ) About Help contact arXiv Click here to contact arXiv Contact subscribe to arXiv mailings Click here to subscribe Subscribe Copyright Privacy Policy Web Accessibility Assistance arXiv Operational Status Get status notifications via email or slack ",
      "fetch_method": "direct-html"
    },
    {
      "arxiv_id": "https://ai.google/static/documents/ai-responsibility-update-published-february-2025.pdf",
      "full_text": "Published in February 2025\nResponsible AI \nProgress Report\n\nForeword\ninformation from our latest research and \npractice on AI safety and responsibility topics. \nIt details our methods for governing, mapping, \nmeasuring, and managing AI risks aligned to \nthe NIST framework, as well as updates on \nhow we’re operationalizing responsible AI \ninnovation across Google. We also provide \nmore specific insights and best practices on \ntopics ranging from our rigorous red teaming \nand evaluation processes to how we mitigate \nrisk using techniques, including better safety \ntuning and filters, security and privacy \ncontrols, provenance technology in our \nproducts, and broad AI literacy education.\nOur approach to AI responsibility has evolved \nover the years to address the dynamic nature \nof our products, the external environment, \nand the needs of our global users. Since \n2018, AI has evolved into a general-purpose \ntechnology used daily by billions of people \nand countless organizations and businesses. \nThe broad establishment of responsibility \nframeworks has been an important part of \nthis evolution. We’ve been encouraged by \nprogress on AI governance coming from \nbodies like the G7 and the International \nOrganization for Standardization, and also \nframeworks emerging from other companies \nand academic institutions. Our updated \nAI Principles — centered on bold innovation, \nresponsible development, and collaborative \npartnership — reflect what we’re learning as \nAI continues to advance rapidly.\n   \nAs AI technology and discussions about its \ndevelopment and uses continue to evolve, \nwe will continue to learn from our research \nand users, and innovate new approaches to \nresponsible development and deployment. As \nwe do, we remain committed to sharing what \nwe learn with the broader ecosystem through \nthe publication of reports like this, and also \nthrough continuous engagement, discussion, \nand collaboration with the wider community to \nhelp maximize the benefits of AI for everyone. \nLaurie Richardson \nVice President, Trust & Safety, Google\n2\nAI is a transformational technology that \noffers both a unique opportunity to meet \nour mission, and the chance to expand \nscientific discovery and tackle some of \nthe world’s most important problems. \nAt Google we believe it’s crucial that \nwe continue to develop and deploy AI \nresponsibly, with a focus on making sure \nthat people, businesses, and governments \naround the world can benefit from its \nextraordinary potential while at the same \ntime mitigating against its potential risks. \nIn 2018, we were one of the first in the \nindustry to adopt AI Principles, and \nsince then, we’ve published annual \nAI responsibility reports detailing our \nprogress. This year’s report shares \n\nGovern\nMap\nMeasure\nManage\n3\nSummary of our responsible AI approach\nOur AI Principles guide our decision-making and \ninform the development of our different frameworks \nand policies, including the Secure AI Framework \nfor security and privacy, and the Frontier Safety \nFramework for evolving model capabilities and \nmitigations. Additional policies address design, \nsafety, and prohibited uses.\nOur pre- and post-launch processes ensure \nalignment with these Principles and policies \nthrough clear requirements, mitigation support, \nand leadership reviews. These cover model and \napplication requirements, with a focus on safety, \nprivacy, and security. Post-launch monitoring \nand assessments enable continuous improvement \nand risk management. \nWe regularly publish external model cards and \ntechnical reports to provide transparency into \nmodel creation, function, and intended use. And \nwe invest in tooling for model and data lineage \nto promote transparency and accountability.\nWe have developed a rigorous approach to measuring \nAI model and application performance, focusing on \nsafety, privacy, and security benchmarks. Our \napproach is continually evolving, incorporating new \nmeasurement techniques as they become available.\nMulti-layered red teaming plays a critical role \nin our approach, with both internal and external \nteams proactively testing AI systems for weaknesses \nand identifying emerging risks. Security-focused \nred teaming simulates real-world attacks, while \ncontent-focused red teaming identifies potential \nvulnerabilities and issues. External partnerships and \nAI-assisted red teaming further enhance this process. \nModel and application evaluations are central to \nthis measurement approach. These evaluations assess \nalignment with established frameworks and policies, \nboth before and after launch. \nAI-assisted evaluations help us scale our risk \nmeasurement. AI autoraters streamline evaluation \nand labeling processes. Synthetic testing data \nexpedites scaled measurement. And automatic \ntesting for security vulnerabilities helps us assess \ncode risks in real time.\nWe deploy and evolve mitigations to manage content \nsafety, privacy, and security, such as safety filters \nand jailbreak protections. \nWe often phase our launches with audience-specific \ntesting, and conduct post-launch monitoring of user \nfeedback for rapid remediation. \nWe work to advance user understanding of AI \nthrough innovative developments in provenance \ntechnology, our research-backed explainability \nguidelines, and AI literacy education.\nTo support the broader ecosystem, we provide \nresearch funding, as well as tools designed for \ndevelopers and users. We also promote industry \ncollaboration on the development of standards \nand best practices.\nWe take a scientific approach to mapping AI \nrisks through research and expert consultation, \ncodifying these inputs into a risk taxonomy. \nA core component is risk research, encompassing \nemerging AI model capabilities, emerging risks from \nAI, and potential AI misuse. This research, which we \nhave published in over 300 papers, directly informs \nour AI risk taxonomy, launch evaluations, and \nmitigation techniques.\nOur approach also draws on external domain \nexpertise, offering new insights to help us better \nunderstand emerging risks and complementing \nin-house work.\nWe have developed an approach to AI governance that focuses on responsibility throughout the AI development lifecycle. This \napproach is guided by our AI Principles, which emphasize bold innovation, responsible development, and collaborative progress. \nOur ongoing work in this area reflects key concepts in industry guidelines like the NIST AI Risk Management Framework. \n\n4\nSummary of our responsible AI outcomes to date\nBuilding AI responsibly \nrequires collaboration across \nmany groups, including \nresearchers, industry experts, \ngovernments, and users. \n\nWe are active contributors to this ecosystem, \nworking to maximize AI’s potential while \nsafeguarding safety, privacy, and security.\n300+\n$120 million\nresearch papers on AI \nresponsibility and safety topics\nPartnered on AI responsibility with \noutside groups and institutions \nlike the Frontier Model Forum, \nthe Partnership on AI, the World \nEconomic Forum, MLCommons, \nThorn, the Coalition for Content \nProvenance and Authenticity, the \nDigital Trust & Safety Partnership, \nthe Coalition for Secure AI, and \nthe Ad Council\nfor AI education and training \naround the world\nCertified Gemini app, Google Cloud, \nand Google Workspace through the \nISO/IEC 42001 process\nsecurity professionals have taken \nthe SAIF Risk Self Assessment to \nreceive a personalized report of AI \nrisks relevant to their organization\nAchieved “mature” rating for \nGoogle Cloud AI in a third-party \nevaluation of readiness through \nthe NIST AI Risk Management \nFramework governance and \nISO/IEC 42001 compliance \n19,000\n\n5\nGovern: \nFull-stack \nAI governance\nPolicies and principles\nOur governance process is grounded in our principles \nand frameworks: \nAI Principles. We established and evolve our \nAI Principles to guide our approach to developing and \ndeploying AI models and applications. Core to these \nPrinciples is pursuing AI efforts where the likely overall \nbenefits substantially outweigh the foreseeable risks.\nModel safety framework. The Frontier Safety \nFramework, which we recently updated, helps us to \nproactively prepare for potential risks posed by more \npowerful future AI models. The Framework follows the \nemerging approach of Responsible Capability Scaling \nproposed by the U.K.’s AI Safety Institute.\nWe take a full-stack approach to AI \ngovernance ­— from responsible model \ndevelopment and deployment to \npost-launch monitoring and remediation. \nOur policies and principles guide our \ndecision-making, with clear requirements \nat the pre- and post-launch stages, \nleadership reviews, and documentation. \nGovern\nContent safety policies. Our policies for mitigating \nharm in areas such as child safety, suicide, and \nself-harm have been informed by years of research, \nuser feedback, and expert consultation. These policies \nguide our models and products to minimize certain \ntypes of harmful outputs. Some individual applications, \nlike the Gemini app, also have their own policy guidelines. \nWe also prioritize neutral and inclusive design \nprinciples, with a goal of minimizing unfair bias. And \nwe have Prohibited Use Policies governing how people \ncan engage with our AI models and features. \nSecurity and privacy framework. Our Secure AI \nFramework focuses on the security and privacy \ndimensions of AI. \nApplication-specific development frameworks. \nIn addition to Google-wide frameworks and policies, \nseveral of our applications have specific frameworks \nto guide their day-to-day development and operation. \nOur approach to the Gemini \napp guides our day-to-day \ndevelopment of the app and its \nbehavior. We believe the Gemini \napp should:\n1. Follow your directions\nGemini’s top priority is to serve you well.\n2. Adapt to your needs\nGemini strives to be the most \nhelpful AI assistant. \n3. Safeguard your experience\nGemini aims to align with a set of \npolicy guidelines and is governed \nby Google’s Prohibited Use Policy.\n\n6\nGovern\nPre- and post-launch reviews\nWe operationalize our principles, frameworks, and \npolicies through a system of launch requirements, \nleadership reviews, and post-launch requirements \ndesigned to support continuous improvement.\nModel requirements. Governance requirements for \nmodels focus on filtering training data for quality, \nmodel performance, and adherence to policies, as \nwell as documenting training techniques in technical \nreports and model cards. These processes also \ninclude safety, privacy, and security criteria.\nApplication requirements. Launch requirements \nfor applications address risks and include  \ntesting and design guidance. For example, an \napplication that generates audiovisual content \nis required to incorporate a robust provenance \nsolution like SynthID. These requirements are \nbased on the nature of the product, its intended \nuser base, planned capabilities, and the types \nof output involved. For example, an application \nmade available to minors may have additional \nrequirements in areas like parental supervision \nand age-appropriate content.  \nLeadership reviews. Executive reviewers with \nexpertise in responsible AI carefully assess \nevaluation results, mitigations, and risks before \nmaking a launch decision. They also oversee our \nframeworks, policies, and processes, ensuring \nthat these evolve to account for new modalities \nand capabilities. \nPost-launch requirements. Our governance \ncontinues post-launch with assessments for any \nissues that might arise across products. Post-launch \ngovernance identifies unmitigated residual and \nemerging risks, and opportunities to improve our \nmodels, applications, and our governance processes. \nLaunch infrastructure. We are evolving \nour infrastructure to streamline AI launch \nmanagement, responsibility testing, and \nmitigation progress monitoring. \nDocumentation \nWe foster transparency and accountability \nthroughout our AI governance processes. \nModel documentation. External model cards \nand technical reports are published regularly as \ntransparency artifacts. Technical reports provide \ndetails about how our most advanced AI models \nare created and how they function. This includes \noffering clarity on the intended use cases, any \npotential limitations of the models, and how our \nmodels are developed in collaboration with safety, \nprivacy, security, and responsibility teams. In \naddition, we publish model cards for our most \ncapable models and open models. These cards \noffer summaries of technical reports in a “nutrition \nlabel” format to surface vital information needed \nfor downstream developers or to help policy \nleaders assess the safety of a model. \nData and model lineage. We are investing in \nrobust infrastructure to support data and model \nlineage tracking, enabling us to understand the \norigins and transformations of data and models \nused in our AI applications.\nGovern\nA proactive governance approach \nto responsible AI development \nand deployment\nMeasure\nEvaluate and monitor \nidentified risks and enhance \ntesting methods\nManage\nEstablish and implement \nrelevant and effective \nmitigations\nMap\nIdentify current, emerging, \nand potential future \nAI risks\nOur responsible AI approach reflects key concepts in industry guidelines like the \nNIST AI Risk Management Framework — govern, map, measure, and manage.\n\n7\nPrevious iterations of our model cards, such as one \nto predict 3D facial surface geometry and one for \nan object detection model, conveyed important \ninformation about those respective models. \nModel cards were introduced in a \nGoogle research paper in 2019 as \na way to document and provide \ntransparency about how we \nevaluate models. \nThat paper proposed some basic model card fields \nthat would help provide model end users with the \ninformation they need to evaluate how and when \nto use a model. Many of the fields first proposed \nremain vital categories of metadata that are found \nin model cards across the industry today. \nHowever, as generative AI models have \nadvanced, we have adapted our most recent \nmodel cards, such as the card for our highest \nquality text-to-image model Imagen 3, to reflect \nthe rapidly evolving landscape of AI development \nand deployment. While these model cards \nstill contain some of the same categories of \nmetadata we originally proposed in 2019, they \nalso prioritize clarity, practical usability, and \ninclude an assessment of a model’s intended \nusage, limitations, risks and mitigations, and \nethical and safety considerations.\nAs models continue to evolve, we will work \nto recognize the key commonalities between \nmodels in these model cards. By identifying \nthese commonalities, while also remaining \nflexible in our approach, we can use model cards \nto support a shared understanding and increased \ntransparency around how models work.\nGovern\nThe model card fields suggested in our 2019 research paper \n“Model Cards for Model Reporting.”\nModel Details\nBasic information about the model.\n• Person or organization developing model\n•\tModel date\n•\tModel version\n•\tModel type\n•\tInformation about training algorithms, parameters, \n\t fairness constraints or other applied approaches, \n\t and features\n•\tPaper or other resource for more information\n•\tCitation details\n•\tLicense\n•\tWhere to send questions or comments about the model\n\t\nIntended Use \nUse cases that were envisioned during development.\n•\tPrimary intended uses\n•\tOut-of-scope use cases\nFactors \nFactors could include demographic or phenotypic groups, \nenvironmental conditions, technical attributes, or others \nlisted as required.\n•\tRelevant factors\n•\tEvaluation factors\nMetrics\nMetrics should be chosen to reflect potential real-world \nimpacts of the model.\n•\tModel performance measures\n•\tDecision thresholds\n•\tVariation approaches\nEvaluation Data \nDetails on the dataset(s) used for the quantitative \nanalyses in the card.\n•\tDatasets\n•\tMotivation\n•\tPreprocessing\nTraining Data \nMay not be possible to provide in practice. When possible, \nthis section should mirror Evaluation Data. If such detail \nis not possible, minimal allowable information should be \nprovided here, such as details of the distribution over \nvarious factors in the training datasets.\nQuantitative Analyses\n•\tUnitary results\n•\tIntersectional results\nEthical Considerations\nCaveats and Recommendations\nModel Card\nCase study: Promoting AI transparency \nwith model cards\n\n8\nMap: \nIdentifying and\nunderstanding risks\nRisk research\n \nWe’ve published more than 300 papers on responsible \nAI topics, and collaborated with research institutions \naround the world. Recent areas of focus include: \nResearch on novel AI capabilities. We research the \npotential impact of emerging AI capabilities such as \nnew modalities and agentic AI, to better understand \nif and how they materialize, as well as identifying \npotential mitigations and policies.\nResearch on emerging risks from AI. We also invest \nin research on the potential emerging risks from AI in \nareas like biosecurity, cybersecurity, self-proliferation, \ndangerous capabilities, misinformation, and privacy, \nto evolve our mitigations and policies. \nWe take a scientific approach to mapping \nAI risks through research and expert \nconsultation, codifying these inputs into \na risk taxonomy. Our mapping process is \nfundamentally iterative, evolving alongside \nthe technology, and adapting to the range \nof contexts in which people use AI models \nor applications. \nMap\nResearch on AI misuse. Mapping the potential \nmisuse of generative AI has become a core area of \nresearch, and contributes to how we assess and \nevaluate our own models in these risk areas, as well \nas potential mitigations. This includes recent research \ninto how government-backed threat actors are trying \nto use AI and whether any of this activity represents \nnovel risks.\nExternal domain expertise\nWe augment our own research by working with \nexternal domain experts and trusted testers who can \nhelp further our mapping and understanding of risks. \nExternal expert feedback. We host workshops \nand demos at our Google Safety Engineering Centers \naround the world and industry conferences, \ngarnering insights across academia, civil society, \nand commercial organizations.\nTrusted testers. Teams can also leverage external \ntrusted testing groups who receive secure access to \ntest models and applications according to their \ndomain expertise.\nRisk taxonomy\nWe’ve codified our mapping work into a taxonomy \nof potential risks associated with AI, building on the \nNIST AI Risk Management Framework and informed \nby our experiences developing and deploying a wide \nrange of AI models and applications. These risks span \nsafety, privacy, and security, as well as transparency \nand accountability risks such as unclear provenance \nor lack of explainability. This risk map is designed to \nenable clarity around which risks are most relevant \nto understand for a given launch, and what might be \nneeded to mitigate those risks.\n\n9\nMap\nA selection of our latest research publications focused on responsible AI\nJuly 2024\nSeptember 2024\nNovember 2024\nJanuary 2025\nJune 2024\nAugust 2024\nDecember 2024\nOctober 2024\nGenerative AI Misuse: A Taxonomy of Tactics and Insights from Real-World Data \nBeyond Thumbs Up/Down: Untangling Challenges of Fine-Grained Feedback for Text-to-Image Generation\nOn Scalable Oversight with Weak LLMs Judging Strong LLMs\nJumping Ahead: Improving Reconstruction Fidelity with JumpReLU Sparse Autoencoders\nShieldGemma: Generative AI Content Moderation Based on Gemma\nGemma Scope: Open Sparse Autoencoders Everywhere All At Once on Gemma 2\nImagen 3\nMachine Unlearning Doesn’t Do What You Think: \nLessons for Generative AI Policy, Research, and Practice\nKnowing When to Ask - Bridging Large Language Models and Data\nOperationalizing Contextual Integrity in Privacy-Conscious Assistants\nA Toolbox for Surfacing Health Equity Harms and Biases in Large Language Models\nNew Contexts, Old Heuristics: How Young People in India and the US \nTrust Online Content in the Age of Generative AI \nAll Too Human? Mapping and Mitigating the Risk from Anthropomorphic AI \nGaps in the Safety Evaluation of Generative AI \nInsights on Disagreement Patterns in Multimodal Safety Perception across Diverse Rater Groups\nSTAR: SocioTechnical Approach to Red Teaming Language Models \nA New Golden Age of Discovery: Seizing the AI for Science Opportunity\nAdversarial Misuse of Generative AI\nHow we Estimate the Risk from Prompt Injection Attacks on AI Systems\n\n10\nMap\nCase study: Mapping and addressing \nrisks to safely deploy AlphaFold 3\nWe carried out extensive research throughout \nAlphaFold 3’s development to understand how it \nmight help or pose risks to biosecurity. Over the \ncourse of AlphaFold’s development, we consulted \nwith more than 50 external experts across \nvarious fields, including DNA synthesis, virology, \nand national security, to understand their \nperspectives on the potential benefits and risks.\nIn May 2024, Google DeepMind \nreleased AlphaFold 3, an AI model \ncapable of predicting molecular \nstructures and interactions and \nhow they interact, which holds the \npromise of transforming scientists’ \nunderstanding of the biological \nworld and accelerating drug \ndiscovery. Scientists can access the \nmajority of its capabilities, for free, \nthrough our AlphaFold Server, an \neasy-to-use research tool, or via \nopen code and weights. \nAn ethics and safety assessment was conducted \nwith external experts, in which potential risks \nand benefits of AlphaFold 3 were identified and \nanalyzed, including their potential likelihood and \nimpact. This assessment was grounded in the \nspecific technical capacities of the model and \ncompared the model to other resources like the \nProtein Data Bank and other AI biology tools. \nThe assessment was then reviewed by a council \nof senior internal experts in AI responsibility and \nsafety, who provided further feedback.\nAs with all Google DeepMind models, AlphaFold 3 \nwas developed, trained, stored, and served within \nGoogle’s infrastructure, supported by security \nteams, engineers, and researchers. Quantitative \nand qualitative techniques are used to monitor the \nadoption and impact of AlphaFold 3. We partnered \nwith the European Bioinformatics Institute of the \nEuropean Molecular Biology Laboratory (EMBL) \nto launch free tutorials on how to best use \nAlphaFold that more than 10,000 scientists have \naccessed. We are currently expanding the course \nand partnering with local capacity builders to \naccelerate the equitable adoption of AlphaFold 3.\nTo continue to identify and map emerging risks \nand benefits from AI to biosecurity, we contribute \nto civil society and industry efforts such as the \nU.K. National Threat Initiative’s AI-Bio Forum and \nthe Frontier Model Forum, as well as engaging \nwith government bodies.\nAlphaFold is accelerating breakthroughs in biology with AI, and has revealed millions of \nintricate 3D protein structures, helping scientists understand how life’s molecules interact.\n\nMeasure: \nAssessing risks\nand mitigations \nMulti-layered red teaming\n \nRed teaming exercises, conducted both internally \nand externally, proactively assess AI systems for \nweaknesses and areas for improvement. Teams \nworking on these exercises collaborate to promote \ninformation sharing and industry alignment in \nred teaming standards. \nSecurity-focused red teaming. Our AI Red Team \ncombines security and AI expertise to simulate \nattackers who might target AI systems. Based on \nthreat intelligence from teams like the Google Threat \nIntelligence Group, the AI Red Team explores and \nidentifies how AI features can cause security issues, \nrecommends improvements, and helps ensure that \nreal-world attackers are detected and thwarted before \nthey cause damage.\nContent-focused red teaming. Our Content \nAdversarial Red Team (CART) proactively identifies \nweaknesses in our AI systems, enabling us to mitigate \nrisks before product launch. CART has conducted over \n150 red teaming exercises across various products. \nOur internal AI tools also assist human expert red \nteamers and increase the number of attacks they’re \nable to test for.\nExternal red teaming partnerships. Our external red \nteaming includes live hacking events such as DEF CON \nand Escal8, targeted research grants, challenges, and \nvulnerability rewards programs to complement our \ninternal evaluations.\nAI-assisted red teaming. To enhance our approach, \nwe have developed forms of AI-assisted red teaming \n— training AI agents to find potential vulnerabilities \nin other AI systems, drawing on work from gaming \nbreakthroughs like AlphaGo. For example, we recently \nshared details of how we used AI-assisted red teaming \nto understand how vulnerable our systems may be to \nindirect prompt injection attacks, and to inform how we \nmitigate the risk. \n11\nMeasure\nModel and application evaluations\nA core component of our measurement approach is \nrunning evaluations for models and applications. These \nevaluations primarily focus on known risks, in contrast to \nred teaming, which focuses on known and unknown risks. \nModel evaluations. A subset of the mapped risks is \nrelevant to test at the model level. For example, as we \nprepared to launch Gemini 1.5 Pro, we evaluated the \nmodel for risks such as self-proliferation, offensive \ncybersecurity, child safety harms, and persuasion. We \nalso develop new evaluations in key areas — such as \nour work on FACTS Grounding, which is a benchmark for \nevaluating how accurately LLMs ground their responses \nin provided source material and avoid hallucinations. \nApplication evaluations. These evaluations are \ndesigned to assess the extent to which a given \napplication follows the frameworks and policies that \napply to that application. This pre-launch testing \ngenerally covers a wide range of risks spanning \nsafety, privacy, and security, and this portfolio of \ntesting results helps inform launch decisions. We also \ninvest in systematic post-launch testing that can take \ndifferent forms, such as running regression testing \nfor evaluating an application’s ongoing alignment \nwith our frameworks and policies, and cross-product \nevaluations to identify whether known risks for one \napplication may have manifested in other applications.\nAI-assisted evaluations\n  \nAs AI continues to scale, it’s critical that our ability to \nmeasure risks scales along with it. That’s why we’re \ninvesting in automated testing solutions, which can run \nboth before launch and on an ongoing basis after release. \nAI autoraters. At the model layer, Gemini 2.0’s \nreasoning capabilities have enabled major advances \nin automating evals and developing training data \nto mitigate identified risks. We have also published \nresearch on the future use of more capable models \nto help evaluate and rate less capable models. At the \napplication layer, we have been investing in applied AI to \ntriage and label content to streamline and scale evals. \nAI-generated testing data. We’ve been investing \nin “few shot” learning where an AI creates a testing \nset based on inputs from experts. This significantly \naccelerates testing when compared with human \ncreation of testing sets.\nAfter identifying and understanding risks \nthrough mapping, we systematically \nassess our AI systems through red teaming \nexercises. We evaluate how well our \nmodels and applications perform, and \nhow effectively our risk mitigations work, \nbased on benchmarks for safety, privacy, \nand security. Our approach evolves with \ndevelopments in the underlying technology, \nnew and emerging risks, and as new \nmeasurement techniques emerge, such \nas AI-assisted evaluations. \n\n12\nAs part of making Gemma pre-trained models \nsafe and reliable, we used automated techniques \nto filter out certain personal information \nand other sensitive data from training sets. \nAdditionally, we used extensive fine-tuning and \nreinforcement learning from human feedback \nto align our instruction-tuned models with \nresponsible behaviors. To understand and reduce \nthe risk profile for Gemma models, we conducted \nrobust evaluations including manual red teaming, \nautomated adversarial testing, and assessments \nof model capabilities for dangerous activities. \nOn top of robust internal evaluations, we also \nevaluate against well-known academic safety \nbenchmarks. These evaluations are outlined in \nour model cards for Gemma models and include: \nOur Gemma models are a family of \nlightweight, state-of-the-art open \nmodels built from the same research \nand technology used to create the \nGemini family of models. \nText-to-text content safety. Human evaluation \non prompts covering safety policies, including \nchild sexual abuse and exploitation, harassment, \nviolence and gore, and hate speech.\nText-to-text representational harms. \nBenchmarks against relevant academic datasets \nsuch as WinoBias and BBQ dataset.\nMemorization. Automated evaluation of \nmemorization of training data, including the risk \nof personally identifiable information exposure.\nLarge-scale harm. Tests for “dangerous \ncapabilities,” such as chemical, biological, \nradiological, and nuclear risks.\nThe results of these internal and external \nethics and safety evaluations are within \nacceptable thresholds for meeting internal \npolicies for categories such as child safety, \ncontent safety, representational harms, \nmemorization, and large-scale harms. \nIn addition, a Gemma model achieved strong \nexternal results in the AILuminate v1.0 \nbenchmark from MLCommons. This benchmark \nassesses the safety of text-to-text interactions \nwith a general purpose AI chat model by a user \nwith malicious or vulnerable intent.\nCase study: Evaluating Gemma, \na family of open models\nMeasure\nGemma is built for responsible AI development from the same research \nand technology used to create Gemini models.\n\nManage: \nMitigating risks\nManaging content safety, security, and privacy \n \nManaging content safety. We leverage the expertise \nour Trust & Safety teams have honed over decades of \nabuse fighting to establish model and application-level \nmitigations for a wide range of content safety risks. \nA critical piece of our safety strategy is a pre-launch \nrisk assessment that identifies which applications have \nsufficiently great or novel risks that require specialized \ntesting and controls. We also employ guardrails in our \nmodels and products to reduce the risk of generating \nharmful content, for example: \n• Safety filters. We build safety classifiers to prevent \nour models from showing users harmful outputs such \nas suicide content or pornography.  \n• System instructions. We steer our models to produce \ncontent that aligns with our safety guidelines by using \nsystem instructions — prompts that tell the model how \nto behave when it responds to user inputs.  \n13\nManage\n• Safety tuning. We fine-tune our models to \nproduce helpful, high-quality answers that align to \nour safety guidelines.\nManaging security. We use the SAIF framework \nto mitigate known and novel AI security risks. The \nlatter category includes risks such as data poisoning, \nmodel exfiltration, and rogue actions. We apply \nsecurity controls, or repeatable mitigations, to these \nrisks. For example, for prompt injections and jailbreaks, \nwe apply robust filtering and processing of inputs and \noutputs. Additionally, thorough training, tuning, and \nevaluation processes help fortify the model against \nprompt injection attacks. For data poisoning, we \nimplement data sanitization, secure AI systems, enable \naccess controls, and deploy mechanisms to ensure \ndata and model integrity. We have published a full \nlist of our controls for AI security risks. In addition, \nwe continue to research new ways to help mitigate a \nmodel’s susceptibility to security attacks. For example, \nwe’ve developed an AI agent that auto-detects \nreal-world code for security risks. \nManaging privacy. We have invested deeply in \nmitigations for privacy risks, as well as researching \nnew risks that might emerge from evolving \ncapabilities like agentic. For examples, our paper \non how AI assistants can better protect privacy by \nusing a “contextual integrity” framework to steer \nAI assistants to only share information that is \nappropriate for a given context.\nWe take a multi-faceted approach to \nrisk mitigation. We implement content \nsafety, security, and privacy mitigations; \nemploy phased launches; empower users \nwith transparency, labeling and training; \nharness user feedback; and deploy ongoing \nmonitoring to continuously improve. In \naddition, we support the wider ecosystem \nwith AI safety tools and standards. \n\n14\nManage\nPhased launches, monitoring, and \nrapid remediation\n \nPhased launches. A gradual approach to \ndeployment is a critical risk mitigation. We have \na multi-layered approach — starting with testing \ninternally, then releasing to trusted testers \nexternally, then opening up to a small portion of \nour user base (for example, Gemini Advanced users \nfirst). We also phase our country and language \nreleases, constantly testing to ensure mitigations \nare working as intended before we expand. And \nfinally, we have careful protocols and additional \ntesting and mitigations required before a product \nis released to under 18s. To give an example, as \nGemini 2.0’s multimodality increases the complexity \nof potential outputs, we have been careful to \nrelease it in a phased way via trusted testers and \nsubsets of countries. \nMonitoring and rapid remediation. We design \nour applications to promote user feedback on both \nquality and safety, through user interfaces that \nencourage users to provide thumbs up/down and \ngive qualitative feedback where appropriate. Our \nteams monitor user feedback via these channels \nclosely, as well as feedback delivered through other \nchannels. We have mature incident management \nand crisis response capabilities to rapidly mitigate \nand remediate where needed, and feed this back \ninto our risk identification efforts. Importantly, teams \nare enabled to have rapid-remediation mechanisms \nin place to block content flagged as illegal.\nAdvancing user understanding: provenance, \nexplainability, and AI literacy\n \nProvenance. Outputs of our generative AI \nproducts typically carry watermarking (via our \nSynthID technology) and, when it comes to \nimagery, relevant metadata (per IPTC standards). \nAs an example, About This Image in Google Image \nSearch started identifying and labeling \nAI-generated images with SynthID in 2023, \nalongside other image metadata. We’ve open-\nsourced SynthID to make it easier for any developer \nto apply watermarking for their own generative AI \nmodels, and shared our analysis of how labeling \nAI-generated content helps people make informed \ndecisions about the content they see online. \nGoogle Search, Ads, and YouTube are also \nimplementing the latest version of the Coalition\nfor Content Provenance and Authenticity (C2PA)’s \nauthentication standard. And moving forward, we \nplan to continue investing in the deployment of \nC2PA across our services. \nExplainability. Explainability is about helping people \nunderstand how an AI application operates. Products \nuse disclaimers to set clear expectations — such as \nreminding people that the AI-generated outputs may \ncontain inaccuracies and that they should take steps \nto verify information generated by the tool. These \ndisclosure policies are backed up by research, and \ncodified into explainability guidelines for our teams.\nAI literacy. To complement the transparency \nmitigations we implement, it is also critical that \ngovernments and industry continue to educate \npeople about how to use AI, and its limitations. \nWe have committed $120 million for AI education \nand training around the world. We have also \nlaunched AI training for businesses, developers, \nand younger learners. With Raspberry Pi Foundation, \nwe also co-developed Experience AI, an educational \nprogram that offers cutting-edge resources on AI \nfor teachers and students aged 11–14.\nEcosystem enablement: funding, tools, \nand standards\n \nEnabling the ecosystem with research funding. \nWith our Frontier Model Forum partners, we \nco-founded the AI Safety Fund (AISF), which provides \ngrants to researchers to help identify, evaluate, and \nmitigate risks and improve the safe deployment of \nAI for the benefit of society. Currently, AISF is \nprioritizing three critical research areas: biosecurity, \ncybersecurity, and AI agent evaluation and synthetic \ncontent (including AI agent identity verification \nsystems and AI agent safety evaluations).\nEnabling the ecosystem with mitigation tools. We \nbelieve that we also need to complement our mitigation \nefforts by offering mitigations for the ecosystem.\n• We released ShieldGemma — a series of state-of-\nthe-art safety classifiers that developers can apply to \ndetect and mitigate harmful content in AI model input \nand outputs. Specifically, ShieldGemma is designed to \ntarget hate speech, harassment, sexually explicit \ncontent, and dangerous content. \n• We offer an existing suite of safety classifiers in our \nResponsible Generative AI Toolkit, which includes \na methodology to build classifiers tailored to a \nspecific policy with limited number of datapoints, \nas well as existing Google Cloud off-the-shelf \nclassifiers served via API.\n• We share AI interpretability tools to help researchers \nimprove AI safety. Our research teams are continually \nexploring new ways to better understand how models \nbehave. For example, we recently announced \nGemma Scope, a new set of tools enabling \nresearchers to “peer inside” the workings of our \nGemma 2 model to see how it parses and completes \ntasks. We believe that this kind of interpretability could \nopen up new opportunities to identify and mitigate \nsafety risks at the model behavior level.\n• We launched the SAIF Risk Self Assessment, a \nquestionnaire-based tool that generates a checklist \nto guide AI practitioners responsible for securing AI \nsystems. The tool will immediately provide a report \nhighlighting specific risks such as data poisoning, \nprompt injection, and model source tampering, tailored \nto the submittor’s AI systems, as well as suggested \nmitigations, based on the responses they provided. \nEstablishing ecosystem mitigation standards. \n• In 2024, we joined Partnership on AI’s working \ngroup responsible for understanding progress on \nthe transparency documentation practices of model \nproviders. The working group will help provide valuable \ninsights to policymakers and standards bodies working \non codes of practice related to AI.\n• We are a founding member of MLCommons, an \nengineering consortium focused on AI benchmarks, \nincluding the new AILuminate benchmark v1.0. This \nis the first AI safety benchmark produced with open \nacademic, industry, and civil society input and \noperated by a neutral non-profit with AI benchmarking \nexperience. AILuminate combines a hazard assessment \nstandard, more than 24,000 prompts, online testing \nwith hidden prompts, a proprietary mixture of expert \nevaluators, and clear grade-based reporting. \n• We are also a partner in the World Economic Forum’s \nAI Governance Alliance, a multi-stakeholder initiative \nto promote transparent development and deployment \nof AI systems and establish global frameworks and \nstandards for AI governance. \n• We introduced the Coalition for Secure AI, which \nworks to advance security measures for addressing \nthe unique risks that come with AI, both for issues that \narise in real time and those over the horizon.\nWe’ve open-sourced SynthID to \nmake it easier for any developer \nto apply watermarking\n\n15\nThe phased launch approach consisted of \nseveral stages, including: \n• \u0007Initial internal testing. The product was first \ntested internally by the immediate team, then \nby a broader group within Google Labs.\n• \u0007Trusted tester program. A group of 50 trusted \ntesters participated in a diary study to record \ntheir experiences and provide feedback.\n• \u0007Waitlist launch. After the trusted tester phase, \nNotebookLM was announced at Google I/O, with \na waitlist launch limited to users in the U.S.\nNotebookLM is an AI-powered \nresearch and writing assistant \ndesigned to help users understand \ncomplex information. The \ndevelopment of NotebookLM \nprioritized responsible AI practices \nby focusing on identifying potential \nrisks, implementing mitigation \nstrategies, and adopting a phased \nlaunch approach.\n• \u0007Global expansion. Following the \nU.S. launch and further improvements, \nNotebookLM was made available in\nover 200 countries and territories.\n• \u0007NotebookLM Business pilot program. \nA pilot program for NotebookLM Business \nwas launched for organizations, universities, \nand businesses.\n• \u0007NotebookLM Plus subscription. A premium \nversion of NotebookLM was launched with \nenhanced features and higher usage limits.\nThis phased approach and the incorporation \nof user feedback have been central to the \niterative development of NotebookLM. The \nteam used user feedback to refine the product \nand implement safety measures before \nexpanding to larger audiences. This strategy \nallowed for iterative improvements to the tool’s \nsafety and effectiveness.\nManage\nCase study: Managing the safe \ndeployment of NotebookLM\nThe development of NotebookLM prioritized responsible AI practices.\n\n16\nGoogle DeepMind developed \nSynthID, a technology designed \nto identify AI-generated content \nby embedding digital watermarks \ndirectly into AI-generated images, \naudio, text, or video. We prioritized \nthe development of SynthID as a \ntool to manage the risk of misuse of \ngenerative AI, particularly the risk \nof contributing to misinformation \nand misattribution. \nSynthID uses a variety of deep learning models \nand algorithms for watermarking and identifying \nAI-generated content. Watermark detection \nvia SynthID can output three possible states: \nwatermarked, not watermarked, or uncertain. \nThis detector can be customized by setting \nthreshold values to achieve a specific false \npositive and false negative rate for each. \nThe open-sourcing of our SynthID text \nwatermarking tool — developed in-house and \nused by the Gemini app and web experience ­— \ncontributes to the responsible use of AI. It makes \nit easier for any developer to apply watermarking \nfor their generative AI models, so they can \ndetect what text outputs have come from their \nown LLMs. The open source code is available \non Hugging Face, and we’ve added it to our \nResponsible Generative AI Toolkit for developers.  \nCase study: Offering \nSynthID to the ecosystem\nSynthID helps identify AI-generated content by embedding an imperceptible \nwatermark on text, images, audio, and video content generated by our models.\nManage\n\nConclusion\nWe will continue to govern AI development with \nrobust internal governance, risk assessments, and \ncontinuous updates to our processes, embedding \nour updated AI Principles in our responsibility \nwork. As AI continues to evolve, we are committed \nto remaining at the forefront of responsible AI \npractices. That means continuing to invest in \nresearch, collaborate with external experts and \ninstitutions, and engage with the wider community \nto inform how AI is developed and used in a way \nthat benefits society and upholds our core values. \nAI is a dynamic field, and responsible AI work has no \nfinish line. We believe that through bold innovation \nand responsible development, coupled with an \necosystem that helps others to innovate, we can \ncreate a future where AI is a force for good, enabling \nscientific progress and widespread benefits.\n17\nWe believe that being bold in AI means being \nresponsible from the start. Our approach to \nresponsible AI is comprehensive, proactive, \nand aligned with industry standards, including \nthe NIST AI Risk Management Framework.\n",
      "fetch_method": "direct-pdf"
    },
    {
      "arxiv_id": "https://cloud.google.com/tpu/docs/intro-to-tpu",
      "full_text": " Introduction to Cloud TPU &nbsp;|&nbsp; Google Cloud Skip to main content Documentation Technology areas close AI and ML Application development Application hosting Compute Data analytics and pipelines Databases Distributed, hybrid, and multicloud Generative AI Industry solutions Networking Observability and monitoring Security Storage Cross-product tools close Access and resources management Costs and usage management Google Cloud SDK, languages, frameworks, and tools Infrastructure as code Migration Related sites close Google Cloud Home Free Trial and Free Tier Architecture Center Blog Contact Sales Google Cloud Developer Center Google Developer Center Google Cloud Marketplace Google Cloud Marketplace Documentation Google Cloud Skills Boost Google Cloud Solution Center Google Cloud Support Google Cloud Tech Youtube Channel / English Deutsch Español Español – América Latina Français Indonesia Italiano Português – Brasil 中文 – 简体 中文 – 繁體 日本語 한국어 Console Sign in Cloud TPU Guides Reference Support Resources Contact Us Start free Documentation Guides Reference Support Resources Technology areas More Cross-product tools More Related sites More Console Contact Us Start free Discover Introduction to Cloud TPU TPU architecture TPU software versions TPU versions TPU v6e TPU v5p TPU v5e TPU v4 TPU v3 TPU v2 Regions and zones TPU consumption options Get started All getting started guides Set up the Cloud TPU environment Reserve TPUs About TPU reservations Request a reservation for up to 90 days (in calendar mode) Request a future reservation for one year or longer Share a reservation Consume a reservation Run JAX on Cloud TPU VM Run PyTorch on Cloud TPU VM Train on Cloud TPU slices Run JAX on Cloud TPU slices Run PyTorch on Cloud TPU slices Configure TPUs Connect a TPU to a shared VPC network Connect to a TPU VM without a public IP address Configure networking and access Use a cross-project service account Storage options Storage options for Cloud TPU Attach durable block storage to a TPU VM Connect to Cloud Storage buckets Mount a Filestore instance on a TPU VM Training and inference Train a model using v6e Train a model using v5e TPU inference Multislice training Scale ML workloads using Ray Run TPU applications in a Docker container About TPUs in GKE Work with image datasets Convert an image classification dataset for use with Cloud TPU Download, pre-process and upload the ImageNet dataset Download, pre-process and upload the COCO dataset Manage TPUs Manage TPU resources Manage queued resources Request TPUs using Flex-start Manage TPU Spot VMs Prepare for maintenance events Schedule TPU collections for inference workloads Autocheckpoint View maintenance notifications Manually start host maintenance Preemptible TPUs Optimize performance Cloud TPU performance guide Improve your model&#39;s performance with bfloat16 Monitor and troubleshoot TPUs Troubleshoot TPU VMs Monitor TPU VMs Dashboards for monitoring and logging TPU monitoring Library Troubleshoot TensorFlow models Troubleshoot PyTorch models Troubleshoot JAX models Cloud TPU error glossary Cloud TPU audit logs Profile TPUs Profile TPU VMs Profile Multislice environments Profile PyTorch XLA workloads Tutorials All tutorials ResNet Serve generative AI models JetStream MaxText inference on v6e JetStream PyTorch inference on v6e MaxDiffusion inference on v6e vLLM inference on v6e Serve an LLM with vLLM (GKE) Notebooks Notebooks AI and ML Application development Application hosting Compute Data analytics and pipelines Databases Distributed, hybrid, and multicloud Generative AI Industry solutions Networking Observability and monitoring Security Storage Access and resources management Costs and usage management Google Cloud SDK, languages, frameworks, and tools Infrastructure as code Migration Google Cloud Home Free Trial and Free Tier Architecture Center Blog Contact Sales Google Cloud Developer Center Google Developer Center Google Cloud Marketplace Google Cloud Marketplace Documentation Google Cloud Skills Boost Google Cloud Solution Center Google Cloud Support Google Cloud Tech Youtube Channel Home Cloud TPU Documentation Guides Send feedback Stay organized with collections Save and categorize content based on your preferences. Introduction to Cloud TPU Tensor Processing Units (TPUs) are Google&#39;s custom-developed, application-specific integrated circuits (ASICs) used to accelerate machine learning workloads. For more information about TPU hardware, see TPU architecture . Cloud TPU is a web service that makes TPUs available as scalable computing resources on Google Cloud. TPUs train your models more efficiently using hardware designed for performing large matrix operations often found in machine learning algorithms. TPUs have on-chip high-bandwidth memory (HBM) letting you use larger models and batch sizes. TPUs can be connected in groups called slices that scale up your workloads with little to no code changes. Code that runs on TPUs must be compiled by the accelerator linear algebra (XLA) compiler. XLA is a just-in-time compiler that takes the graph emitted by an ML framework application and compiles the linear algebra, loss, and gradient components of the graph into TPU machine code. The rest of the program runs on the TPU host machine. The XLA compiler is part of the TPU VM image that runs on a TPU host machine. For more information about Tensor Processing Units, see How to think about TPUs . When to use TPUs Cloud TPUs are optimized for specific workloads. In some situations, you might want to use GPUs or CPUs on Compute Engine instances to run your machine learning workloads. In general, you can decide what hardware is best for your workload based on the guidelines that follow. CPUs Quick prototyping that requires maximum flexibility Simple models that don&#39;t take long to train Small models with small, effective batch sizes Models that contain many custom TensorFlow operations written in C++ Models that are limited by available I/O or the networking bandwidth of the host system GPUs Models with a significant number of custom PyTorch/JAX operations that must run at least partially on CPUs Models with TensorFlow ops that are not available on Cloud TPU (see the list of available TensorFlow ops ) Medium-to-large models with larger effective batch sizes TPUs Models dominated by matrix computations Models with no custom PyTorch/JAX operations inside the main training loop Models that train for weeks or months Large models with large effective batch sizes Models with ultra-large embeddings common in advanced ranking and recommendation workloads Cloud TPUs are not suited to the following workloads: Linear algebra programs that require frequent branching or contain many element-wise algebra operations Workloads that require high-precision arithmetic Neural network workloads that contain custom operations in the main training loop TPUs in Google Cloud You can use TPUs through Cloud TPU VMs, Google Kubernetes Engine, and Vertex AI. The following table lists resources for each Google Cloud service. Google Cloud service Resources Cloud TPU Get started with Cloud TPU VMs Google Kubernetes Engine About TPUs in GKE Run Ray on GKE with TPUs Vertex AI Training on Vertex AI with TPUs Use TPUs for online prediction on Vertex AI Best practices for model development A program whose computation is dominated by non-matrix operations such as add, reshape, or concatenate, will likely not achieve high MXU utilization. The following are some guidelines to help you choose and build models that are suitable for Cloud TPU. Layout The XLA compiler performs code transformations, including tiling a matrix multiply into smaller blocks, to efficiently execute computations on the matrix unit (MXU). The structure of the MXU hardware, a 128x128 systolic array , and the design of TPUs memory subsystem, which prefers dimensions that are multiples of 8, are used by the XLA compiler for tiling efficiency. Consequently, certain layouts are more conducive to tiling, while others require reshapes to be performed before they can be tiled. Reshape operations are often memory bound on the Cloud TPU. Shapes The XLA compiler compiles an ML graph just in time for the first batch. If any subsequent batches have different shapes, the model doesn&#39;t work. (Re-compiling the graph every time the shape changes is too slow.) Therefore, any model that has tensors with dynamic shapes isn&#39;t well suited to TPUs. Padding A high performing Cloud TPU program is one where the dense compute can be tiled into 128x128 chunks. When a matrix computation cannot occupy an entire MXU, the compiler pads tensors with zeros. There are two drawbacks to padding: Tensors padded with zeros underutilize the TPU core. Padding increases the amount of on-chip memory storage required for a tensor and can lead to an out-of-memory error in the extreme case. While padding is automatically performed by the XLA compiler when necessary, one can determine the amount of padding performed by means of the op_profile tool. You can avoid padding by picking tensor dimensions that are well suited to TPUs. Dimensions Choosing suitable tensor dimensions goes a long way in extracting maximum performance from the TPU hardware, particularly the MXU. The XLA compiler attempts to use either the batch size or a feature dimension to maximally use the MXU. Therefore, one of these must be a multiple of 128. Otherwise, the compiler will pad one of them to 128. Ideally, batch size and feature dimensions should be multiples of 8, which enables extracting high performance from the memory subsystem. Getting started with Cloud TPU Set up a Google Cloud account Activate the Cloud TPU API Grant Cloud TPU access to your Cloud Storage buckets Run a basic calculation on a TPU Train a reference model on a TPU Analyze your model Requesting help To get help, contact Cloud TPU support . If you have an active Google Cloud project, be prepared to provide the following information: Your Google Cloud project ID Your TPU name, if one exists Other information you want to provide What's next? Looking to learn more about Cloud TPU? The following resources may help: Cloud TPU architecture Cloud TPU pricing Contact sales Send feedback Except as otherwise noted, the content of this page is licensed under the Creative Commons Attribution 4.0 License , and code samples are licensed under the Apache 2.0 License . For details, see the Google Developers Site Policies . Java is a registered trademark of Oracle and/or its affiliates. Last updated 2025-09-09 UTC. Need to tell us more? [[[\"Easy to understand\",\"easyToUnderstand\",\"thumb-up\"],[\"Solved my problem\",\"solvedMyProblem\",\"thumb-up\"],[\"Other\",\"otherUp\",\"thumb-up\"]],[[\"Hard to understand\",\"hardToUnderstand\",\"thumb-down\"],[\"Incorrect information or sample code\",\"incorrectInformationOrSampleCode\",\"thumb-down\"],[\"Missing the information/samples I need\",\"missingTheInformationSamplesINeed\",\"thumb-down\"],[\"Other\",\"otherDown\",\"thumb-down\"]],[\"Last updated 2025-09-09 UTC.\"],[],[],null,[]] Why Google Choosing Google Cloud Trust and security Modern Infrastructure Cloud Multicloud Global infrastructure Customers and case studies Analyst reports Whitepapers Products and pricing See all products See all solutions Google Cloud for Startups Google Cloud Marketplace Google Cloud pricing Contact sales Support Community forums Support Release Notes System status Resources GitHub Getting Started with Google Cloud Google Cloud documentation Code samples Cloud Architecture Center Training and Certification Developer Center Engage Blog Events X (Twitter) Google Cloud on YouTube Google Cloud Tech on YouTube Become a Partner Google Cloud Affiliate Program Press Corner About Google Privacy Site terms Google Cloud terms Manage cookies Our third decade of climate action: join us Sign up for the Google Cloud newsletter Subscribe English Deutsch Español Español – América Latina Français Indonesia Italiano Português – Brasil 中文 – 简体 中文 – 繁體 日本語 한국어 ",
      "fetch_method": "direct-html"
    },
    {
      "arxiv_id": "https://blog.google/technology/ai/introducing-pathways-next-generation-ai-architecture/",
      "full_text": " Introducing Pathways: A next-generation AI architecture Skip to main content The Keyword Introducing Pathways: A next-generation AI architecture Share Twitter Facebook LinkedIn Mail Copy link Home Product news Product news Android, Chrome &amp; Play Android Chrome Chromebooks Google Play Wear OS See all Platforms &amp; Devices Fitbit Google Nest Pixel See all Explore &amp; Get Answers Gemini Maps News Search Shopping See all Connect &amp; Communicate Classroom Photos Registry Translate In the Cloud Google Workspace More on the Cloud Blog Google Cloud See all See all product updates Android, Chrome &amp; Play Android Chrome Chromebooks Google Play Wear OS See all Platforms &amp; Devices Fitbit Google Nest Pixel See all Explore &amp; Get Answers Gemini Maps News Search Shopping See all Connect &amp; Communicate Classroom Photos Registry Translate In the Cloud Google Workspace More on the Cloud Blog Google Cloud See all See all product updates Company news Company news Outreach &amp; initiatives Arts &amp; Culture Education Entrepreneurs Public Policy Sustainability See all Technology AI Developers Health Google DeepMind Google Labs Safety and security See all Inside Google Data centers and infrastructure Doodles Googlers Life at Google See all Around the globe Google in Asia Google in Europe Google in Latin America See all Authors Sundar Pichai, CEO Demis Hassabis, CEO and Co-Founder, Google DeepMind Kent Walker, SVP James Manyika, SVP Ruth Porat, President &amp; Chief Investment Officer See all Outreach &amp; initiatives Arts &amp; Culture Education Entrepreneurs Public Policy Sustainability See all Technology AI Developers Health Google DeepMind Google Labs Safety and security See all Inside Google Data centers and infrastructure Doodles Googlers Life at Google See all Around the globe Google in Asia Google in Europe Google in Latin America See all Authors Sundar Pichai, CEO Demis Hassabis, CEO and Co-Founder, Google DeepMind Kent Walker, SVP James Manyika, SVP Ruth Porat, President &amp; Chief Investment Officer See all Feed Subscribe Global (English) Africa (English) Australia (English) Brasil (Português) Canada (English) Canada (Français) Česko (Čeština) Deutschland (Deutsch) España (Español) France (Français) India (English) Indonesia (Bahasa Indonesia) 日本 (日本語) 대한민국 (한국어) Latinoamérica (Español) الشرق الأوسط وشمال أفريقيا (اللغة العربية) Nederlands (Nederland) Polska (Polski) Portugal (Português) Italia (Italiano) ประเทศไทย (ไทย) Türkiye (Türkçe) 台灣 (中文) MENA (English) Subscribe The Keyword Home Product news Product news Android, Chrome &amp; Play Android Chrome Chromebooks Google Play Wear OS See all Platforms &amp; Devices Fitbit Google Nest Pixel See all Explore &amp; Get Answers Gemini Maps News Search Shopping See all Connect &amp; Communicate Classroom Photos Registry Translate In the Cloud Google Workspace More on the Cloud Blog Google Cloud See all See all product updates Company news Company news Outreach &amp; initiatives Arts &amp; Culture Education Entrepreneurs Public Policy Sustainability See all Technology AI Developers Health Google DeepMind Google Labs Safety and security See all Inside Google Data centers and infrastructure Doodles Googlers Life at Google See all Around the globe Google in Asia Google in Europe Google in Latin America See all Authors Sundar Pichai, CEO Demis Hassabis, CEO and Co-Founder, Google DeepMind Kent Walker, SVP James Manyika, SVP Ruth Porat, President &amp; Chief Investment Officer See all Feed Press corner RSS feed Subscribe Breadcrumb Technology AI Introducing Pathways: A next-generation AI architecture Oct 28, 2021 · Share Twitter Facebook LinkedIn Mail Copy link Too often, machine learning systems overspecialize at individual tasks, when they could excel at many. That’s why we’re building Pathways—a new AI architecture that will handle many tasks at once, learn new tasks quickly and reflect a better understanding of the world. Jeff Dean Google Senior Fellow and SVP, Google Research Share Twitter Facebook LinkedIn Mail Copy link When I reflect on the past two decades of computer science research, few things inspire me more than the remarkable progress we’ve seen in the field of artificial intelligence. In 2001, some colleagues sitting just a few feet away from me at Google realized they could use an obscure technique called machine learning to help correct misspelled Search queries. (I remember I was amazed to see it work on everything from “ayambic pitnamiter” to “unnblevaiabel”). Today, AI augments many of the things that we do, whether that’s helping you capture a nice selfie , or providing more useful search results , or warning hundreds of millions of people when and where flooding will occur . Twenty years of advances in research have helped elevate AI from a promising idea to an indispensable aid in billions of people’s daily lives. And for all that progress, I’m still excited about its as-yet-untapped potential – AI is poised to help humanity confront some of the toughest challenges we’ve ever faced, from persistent problems like illness and inequality to emerging threats like climate change. But matching the depth and complexity of those urgent challenges will require new, more capable AI systems – systems that can combine AI’s proven approaches with nascent research directions to be able to solve problems we are unable to solve today. To that end, teams across Google Research are working on elements of a next-generation AI architecture we think will help realize such systems. We call this new AI architecture Pathways. Pathways is a new way of thinking about AI that addresses many of the weaknesses of existing systems and synthesizes their strengths. To show you what I mean, let’s walk through some of AI’s current shortcomings and how Pathways can improve upon them. Today&#x27;s AI models are typically trained to do only one thing. Pathways will enable us to train a single model to do thousands or millions of things. Today’s AI systems are often trained from scratch for each new problem – the mathematical model’s parameters are initiated literally with random numbers. Imagine if, every time you learned a new skill (jumping rope, for example), you forgot everything you’d learned – how to balance, how to leap, how to coordinate the movement of your hands – and started learning each new skill from nothing. That’s more or less how we train most machine learning models today. Rather than extending existing models to learn new tasks, we train each new model from nothing to do one thing and one thing only (or we sometimes specialize a general model to a specific task). The result is that we end up developing thousands of models for thousands of individual tasks. Not only does learning each new task take longer this way, but it also requires much more data to learn each new task, since we’re trying to learn everything about the world and the specifics of that task from nothing (completely unlike how people approach new tasks). Instead, we’d like to train one model that can not only handle many separate tasks, but also draw upon and combine its existing skills to learn new tasks faster and more effectively. That way what a model learns by training on one task – say, learning how aerial images can predict the elevation of a landscape – could help it learn another task -- say, predicting how flood waters will flow through that terrain. We want a model to have different capabilities that can be called upon as needed, and stitched together to perform new, more complex tasks – a bit closer to the way the mammalian brain generalizes across tasks. Today&#x27;s models mostly focus on one sense. Pathways will enable multiple senses. People rely on multiple senses to perceive the world. That’s very different from how contemporary AI systems digest information. Most of today’s models process just one modality of information at a time. They can take in text, or images or speech — but typically not all three at once. Pathways could enable multimodal models that encompass vision, auditory, and language understanding simultaneously. So whether the model is processing the word “leopard,” the sound of someone saying “leopard,” or a video of a leopard running, the same response is activated internally: the concept of a leopard. The result is a model that’s more insightful and less prone to mistakes and biases. And of course an AI model needn’t be restricted to these familiar senses; Pathways could handle more abstract forms of data, helping find useful patterns that have eluded human scientists in complex systems such as climate dynamics. Today&#x27;s models are dense and inefficient. Pathways will make them sparse and efficient. A third problem is that most of today’s models are “dense,” which means the whole neural network activates to accomplish a task, regardless of whether it’s very simple or really complicated. This, too, is very unlike the way people approach problems. We have many different parts of our brain that are specialized for different tasks, yet we only call upon the relevant pieces for a given situation. There are close to a hundred billion neurons in your brain, but you rely on a small fraction of them to interpret this sentence. AI can work the same way. We can build a single model that is “sparsely” activated, which means only small pathways through the network are called into action as needed. In fact, the model dynamically learns which parts of the network are good at which tasks -- it learns how to route tasks through the most relevant parts of the model. A big benefit to this kind of architecture is that it not only has a larger capacity to learn a variety of tasks, but it’s also faster and much more energy efficient, because we don’t activate the entire network for every task. For example, GShard and Switch Transformer are two of the largest machine learning models we’ve ever created, but because both use sparse activation, they consume less than 1/10th the energy that you’d expect of similarly sized dense models — while being as accurate as dense models. So to recap: today’s machine learning models tend to overspecialize at individual tasks when they could excel at many. They rely on one form of input when they could synthesize several. And too often they resort to brute force when deftness and specialization of expertise would do. That’s why we’re building Pathways. Pathways will enable a single AI system to generalize across thousands or millions of tasks, to understand different types of data, and to do so with remarkable efficiency – advancing us from the era of single-purpose models that merely recognize patterns to one in which more general-purpose intelligent systems reflect a deeper understanding of our world and can adapt to new needs. That last point is crucial. We’re familiar with many of today’s biggest global challenges, and working on technologies to help address them . But we’re also sure there are major future challenges we haven’t yet anticipated, and many will demand urgent solutions. So, with great care, and always in line with our AI Principles, we’re crafting the kind of next-generation AI system that can quickly adapt to new needs and solve new problems all around the world as they arise, helping humanity make the most of the future ahead of us. POSTED IN: AI Related stories Google Workspace How AI made Meet’s language translation possible By Molly McHugh-Johnson Sep 11, 2025 AI The latest AI news we announced in August By Keyword Team Sep 10, 2025 Learning &amp; Education AI Quests: Bringing AI literacy to the classroom By Ronit Levavi Morad Sep 09, 2025 AI The latest Google AI literacy resources all in one place By Jennie Magiera Search Google Doodles show how AI Mode can help you learn. Sep 08, 2025 Google Labs 6 ways to use NotebookLM to master any subject By Marvin Paul & Dharti Dhami Sep 08, 2025 . Jump to position 1 Jump to position 2 Jump to position 3 Jump to position 4 Jump to position 5 Jump to position 6 Let’s stay in touch. Get the latest news from Google in your inbox. Subscribe No thanks Follow Us Privacy Terms About Google Google Products About the Keyword Help Global (English) Africa (English) Australia (English) Brasil (Português) Canada (English) Canada (Français) Česko (Čeština) Deutschland (Deutsch) España (Español) France (Français) India (English) Indonesia (Bahasa Indonesia) 日本 (日本語) 대한민국 (한국어) Latinoamérica (Español) الشرق الأوسط وشمال أفريقيا (اللغة العربية) Nederlands (Nederland) Polska (Polski) Portugal (Português) Italia (Italiano) ประเทศไทย (ไทย) Türkiye (Türkçe) 台灣 (中文) MENA (English) ",
      "fetch_method": "direct-html"
    },
    {
      "arxiv_id": "https://arxiv.org/abs/2312.11805",
      "full_text": " [2312.11805] Gemini: A Family of Highly Capable Multimodal Models Skip to main content We gratefully acknowledge support from the Simons Foundation, member institutions , and all contributors. Donate &gt; cs &gt; arXiv:2312.11805 Help | Advanced Search All fields Title Author Abstract Comments Journal reference ACM classification MSC classification Report number arXiv identifier DOI ORCID arXiv author ID Help pages Full text Search open search GO open navigation menu quick links Login Help Pages About --> Computer Science > Computation and Language arXiv:2312.11805 (cs) [Submitted on 19 Dec 2023 ( v1 ), last revised 9 May 2025 (this version, v5)] Title: Gemini: A Family of Highly Capable Multimodal Models Authors: Gemini Team Google : Rohan Anil , Sebastian Borgeaud , Jean-Baptiste Alayrac , Jiahui Yu , Radu Soricut , Johan Schalkwyk , Andrew M. Dai , Anja Hauth , Katie Millican , David Silver , Melvin Johnson , Ioannis Antonoglou , Julian Schrittwieser , Amelia Glaese , Jilin Chen , Emily Pitler , Timothy Lillicrap , Angeliki Lazaridou , Orhan Firat , James Molloy , Michael Isard , Paul R. Barham , Tom Hennigan , Benjamin Lee , Fabio Viola , Malcolm Reynolds , Yuanzhong Xu , Ryan Doherty , Eli Collins , Clemens Meyer , Eliza Rutherford , Erica Moreira , Kareem Ayoub , Megha Goel , Jack Krawczyk , Cosmo Du , Ed Chi , Heng-Tze Cheng , Eric Ni , Purvi Shah , Patrick Kane , Betty Chan , Manaal Faruqui , Aliaksei Severyn , Hanzhao Lin , YaGuang Li , Yong Cheng , Abe Ittycheriah , Mahdis Mahdieh , Mia Chen , Pei Sun , Dustin Tran , Sumit Bagri , Balaji Lakshminarayanan , Jeremiah Liu , Andras Orban , Fabian Güra , Hao Zhou , Xinying Song , Aurelien Boffy , Harish Ganapathy , Steven Zheng , HyunJeong Choe , Ágoston Weisz , Tao Zhu , Yifeng Lu , Siddharth Gopal , Jarrod Kahn , Maciej Kula , Jeff Pitman , Rushin Shah , Emanuel Taropa , Majd Al Merey , Martin Baeuml , Zhifeng Chen , Laurent El Shafey , Yujing Zhang , Olcan Sercinoglu , George Tucker , Enrique Piqueras , Maxim Krikun , Iain Barr , Nikolay Savinov , Ivo Danihelka , Becca Roelofs , Anaïs White , Anders Andreassen , Tamara von Glehn , Lakshman Yagati , Mehran Kazemi , Lucas Gonzalez , Misha Khalman , Jakub Sygnowski , Alexandre Frechette , Charlotte Smith , Laura Culp , Lev Proleev , Yi Luan , Xi Chen , James Lottes , Nathan Schucher , Federico Lebron , Alban Rrustemi , Natalie Clay , Phil Crone , Tomas Kocisky , Jeffrey Zhao , Bartek Perz , Dian Yu , Heidi Howard , Adam Bloniarz , Jack W. Rae , Han Lu , Laurent Sifre , Marcello Maggioni , Fred Alcober , Dan Garrette , Megan Barnes , Shantanu Thakoor , Jacob Austin , Gabriel Barth-Maron , William Wong , Rishabh Joshi , Rahma Chaabouni , Deeni Fatiha , Arun Ahuja , Gaurav Singh Tomar , Evan Senter , Martin Chadwick , Ilya Kornakov , Nithya Attaluri , Iñaki Iturrate , Ruibo Liu , Yunxuan Li , Sarah Cogan , Jeremy Chen , Chao Jia , Chenjie Gu , Qiao Zhang , Jordan Grimstad , Ale Jakse Hartman , Xavier Garcia , Thanumalayan Sankaranarayana Pillai , Jacob Devlin , Michael Laskin , Diego de Las Casas , Dasha Valter , Connie Tao , Lorenzo Blanco , Adrià Puigdomènech Badia , David Reitter , Mianna Chen , Jenny Brennan , Clara Rivera , Sergey Brin , Shariq Iqbal , Gabriela Surita , Jane Labanowski , Abhi Rao , Stephanie Winkler , Emilio Parisotto , Yiming Gu , Kate Olszewska , Ravi Addanki , Antoine Miech , Annie Louis , Denis Teplyashin , Geoff Brown , Elliot Catt , Jan Balaguer , Jackie Xiang , Pidong Wang , Zoe Ashwood , Anton Briukhov , Albert Webson , Sanjay Ganapathy , Smit Sanghavi , Ajay Kannan , Ming-Wei Chang , Axel Stjerngren , Josip Djolonga , Yuting Sun , Ankur Bapna , Matthew Aitchison , Pedram Pejman , Henryk Michalewski , Tianhe Yu , Cindy Wang , Juliette Love , Junwhan Ahn , Dawn Bloxwich , Kehang Han , Peter Humphreys , Thibault Sellam , James Bradbury , Varun Godbole , Sina Samangooei , Bogdan Damoc , Alex Kaskasoli , Sébastien M. R. Arnold , Vijay Vasudevan , Shubham Agrawal , Jason Riesa , Dmitry Lepikhin , Richard Tanburn , Srivatsan Srinivasan , Hyeontaek Lim , Sarah Hodkinson , Pranav Shyam , Johan Ferret , Steven Hand , Ankush Garg , Tom Le Paine , Jian Li , Yujia Li , Minh Giang , Alexander Neitz , Zaheer Abbas , Sarah York , Machel Reid , Elizabeth Cole , Aakanksha Chowdhery , Dipanjan Das , Dominika Rogozińska , Vitaliy Nikolaev , Pablo Sprechmann , Zachary Nado , Lukas Zilka , Flavien Prost , Luheng He , Marianne Monteiro , Gaurav Mishra , Chris Welty , Josh Newlan , Dawei Jia , Miltiadis Allamanis , Clara Huiyi Hu , Raoul de Liedekerke , Justin Gilmer , Carl Saroufim , Shruti Rijhwani , Shaobo Hou , Disha Shrivastava , Anirudh Baddepudi , Alex Goldin , Adnan Ozturel , Albin Cassirer , Yunhan Xu , Daniel Sohn , Devendra Sachan , Reinald Kim Amplayo , Craig Swanson , Dessie Petrova , Shashi Narayan , Arthur Guez , Siddhartha Brahma , Jessica Landon , Miteyan Patel , Ruizhe Zhao , Kevin Villela , Luyu Wang , Wenhao Jia , Matthew Rahtz , Mai Giménez , Legg Yeung , James Keeling , Petko Georgiev , Diana Mincu , Boxi Wu , Salem Haykal , Rachel Saputro , Kiran Vodrahalli , James Qin , Zeynep Cankara , Abhanshu Sharma , Nick Fernando , Will Hawkins , Behnam Neyshabur , Solomon Kim , Adrian Hutter , Priyanka Agrawal , Alex Castro-Ros , George van den Driessche , Tao Wang , Fan Yang , Shuo-yiin Chang , Paul Komarek , Ross McIlroy , Mario Lučić , Guodong Zhang , Wael Farhan , Michael Sharman , Paul Natsev , Paul Michel , Yamini Bansal , Siyuan Qiao , Kris Cao , Siamak Shakeri , Christina Butterfield , Justin Chung , Paul Kishan Rubenstein , Shivani Agrawal , Arthur Mensch , Kedar Soparkar , Karel Lenc , Timothy Chung , Aedan Pope , Loren Maggiore , Jackie Kay , Priya Jhakra , Shibo Wang , Joshua Maynez , Mary Phuong , Taylor Tobin , Andrea Tacchetti , Maja Trebacz , Kevin Robinson , Yash Katariya , Sebastian Riedel , Paige Bailey , Kefan Xiao , Nimesh Ghelani , Lora Aroyo , Ambrose Slone , Neil Houlsby , Xuehan Xiong , Zhen Yang , Elena Gribovskaya , Jonas Adler , Mateo Wirth , Lisa Lee , Music Li , Thais Kagohara , Jay Pavagadhi , Sophie Bridgers , Anna Bortsova , Sanjay Ghemawat , Zafarali Ahmed , Tianqi Liu , Richard Powell , Vijay Bolina , Mariko Iinuma , Polina Zablotskaia , James Besley , Da-Woon Chung , Timothy Dozat , Ramona Comanescu , Xiance Si , Jeremy Greer , Guolong Su , Martin Polacek , Raphaël Lopez Kaufman , Simon Tokumine , Hexiang Hu , Elena Buchatskaya , Yingjie Miao , Mohamed Elhawaty , Aditya Siddhant , Nenad Tomasev , Jinwei Xing , Christina Greer , Helen Miller , Shereen Ashraf , Aurko Roy , Zizhao Zhang , Ada Ma , Angelos Filos , Milos Besta , Rory Blevins , Ted Klimenko , Chih-Kuan Yeh , Soravit Changpinyo , Jiaqi Mu , Oscar Chang , Mantas Pajarskas , Carrie Muir , Vered Cohen , Charline Le Lan , Krishna Haridasan , Amit Marathe , Steven Hansen , Sholto Douglas , Rajkumar Samuel , Mingqiu Wang , Sophia Austin , Chang Lan , Jiepu Jiang , Justin Chiu , Jaime Alonso Lorenzo , Lars Lowe Sjösund , Sébastien Cevey , Zach Gleicher , Thi Avrahami , Anudhyan Boral , Hansa Srinivasan , Vittorio Selo , Rhys May , Konstantinos Aisopos , Léonard Hussenot , Livio Baldini Soares , Kate Baumli , Michael B. Chang , Adrià Recasens , Ben Caine , Alexander Pritzel , Filip Pavetic , Fabio Pardo , Anita Gergely , Justin Frye , Vinay Ramasesh , Dan Horgan , Kartikeya Badola , Nora Kassner , Subhrajit Roy , Ethan Dyer , Víctor Campos Campos , Alex Tomala , Yunhao Tang , Dalia El Badawy , Elspeth White , Basil Mustafa , Oran Lang , Abhishek Jindal , Sharad Vikram , Zhitao Gong , Sergi Caelles , Ross Hemsley , Gregory Thornton , Fangxiaoyu Feng , Wojciech Stokowiec , Ce Zheng , Phoebe Thacker , Çağlar Ünlü , Zhishuai Zhang , Mohammad Saleh , James Svensson , Max Bileschi , Piyush Patil , Ankesh Anand , Roman Ring , Katerina Tsihlas , Arpi Vezer , Marco Selvi , Toby Shevlane , Mikel Rodriguez , Tom Kwiatkowski , Samira Daruki , Keran Rong , Allan Dafoe , Nicholas FitzGerald , Keren Gu-Lemberg , Mina Khan , Lisa Anne Hendricks , Marie Pellat , Vladimir Feinberg , James Cobon-Kerr , Tara Sainath , Maribeth Rauh , Sayed Hadi Hashemi , Richard Ives , Yana Hasson , Eric Noland , Yuan Cao , Nathan Byrd , Le Hou , Qingze Wang , Thibault Sottiaux , Michela Paganini , Jean-Baptiste Lespiau , Alexandre Moufarek , Samer Hassan , Kaushik Shivakumar , Joost van Amersfoort , Amol Mandhane , Pratik Joshi , Anirudh Goyal , Matthew Tung , Andrew Brock , Hannah Sheahan , Vedant Misra , Cheng Li , Nemanja Rakićević , Mostafa Dehghani , Fangyu Liu , Sid Mittal , Junhyuk Oh , Seb Noury , Eren Sezener , Fantine Huot , Matthew Lamm , Nicola De Cao , Charlie Chen , Sidharth Mudgal , Romina Stella , Kevin Brooks , Gautam Vasudevan , Chenxi Liu , Mainak Chain , Nivedita Melinkeri , Aaron Cohen , Venus Wang , Kristie Seymore , Sergey Zubkov , Rahul Goel , Summer Yue , Sai Krishnakumaran , Brian Albert , Nate Hurley , Motoki Sano , Anhad Mohananey , Jonah Joughin , Egor Filonov , Tomasz Kępa , Yomna Eldawy , Jiawern Lim , Rahul Rishi , Shirin Badiezadegan , Taylor Bos , Jerry Chang , Sanil Jain , Sri Gayatri Sundara Padmanabhan , Subha Puttagunta , Kalpesh Krishna , Leslie Baker , Norbert Kalb , Vamsi Bedapudi , Adam Kurzrok , Shuntong Lei , Anthony Yu , Oren Litvin , Xiang Zhou , Zhichun Wu , Sam Sobell , Andrea Siciliano , Alan Papir , Robby Neale , Jonas Bragagnolo , Tej Toor , Tina Chen , Valentin Anklin , Feiran Wang , Richie Feng , Milad Gholami , Kevin Ling , Lijuan Liu , Jules Walter , Hamid Moghaddam , Arun Kishore , Jakub Adamek , Tyler Mercado , Jonathan Mallinson , Siddhinita Wandekar , Stephen Cagle , Eran Ofek , Guillermo Garrido , Clemens Lombriser , Maksim Mukha , Botu Sun , Hafeezul Rahman Mohammad , Josip Matak , Yadi Qian , Vikas Peswani , Pawel Janus , Quan Yuan , Leif Schelin , Oana David , Ankur Garg , Yifan He , Oleksii Duzhyi , Anton Älgmyr , Timothée Lottaz , Qi Li , Vikas Yadav , Luyao Xu , Alex Chinien , Rakesh Shivanna , Aleksandr Chuklin , Josie Li , Carrie Spadine , Travis Wolfe , Kareem Mohamed , Subhabrata Das , Zihang Dai , Kyle He , Daniel von Dincklage , Shyam Upadhyay , Akanksha Maurya , Luyan Chi , Sebastian Krause , Khalid Salama , Pam G Rabinovitch , Pavan Kumar Reddy M , Aarush Selvan , Mikhail Dektiarev , Golnaz Ghiasi , Erdem Guven , Himanshu Gupta , Boyi Liu , Deepak Sharma , Idan Heimlich Shtacher , Shachi Paul , Oscar Akerlund , François-Xavier Aubet , Terry Huang , Chen Zhu , Eric Zhu , Elico Teixeira , Matthew Fritze , Francesco Bertolini , Liana-Eleonora Marinescu , Martin Bölle , Dominik Paulus , Khyatti Gupta , Tejasi Latkar , Max Chang , Jason Sanders , Roopa Wilson , Xuewei Wu , Yi-Xuan Tan , Lam Nguyen Thiet , Tulsee Doshi , Sid Lall , Swaroop Mishra , Wanming Chen , Thang Luong , Seth Benjamin , Jasmine Lee , Ewa Andrejczuk , Dominik Rabiej , Vipul Ranjan , Krzysztof Styrc , Pengcheng Yin , Jon Simon , Malcolm Rose Harriott , Mudit Bansal , Alexei Robsky , Geoff Bacon , David Greene , Daniil Mirylenka , Chen Zhou , Obaid Sarvana , Abhimanyu Goyal , Samuel Andermatt , Patrick Siegler , Ben Horn , Assaf Israel , Francesco Pongetti , Chih-Wei &#34;Louis&#34; Chen , Marco Selvatici , Pedro Silva , Kathie Wang , Jackson Tolins , Kelvin Guu , Roey Yogev , Xiaochen Cai , Alessandro Agostini , Maulik Shah , Hung Nguyen , Noah Ó Donnaile , Sébastien Pereira , Linda Friso , Adam Stambler , Adam Kurzrok , Chenkai Kuang , Yan Romanikhin , Mark Geller , ZJ Yan , Kane Jang , Cheng-Chun Lee , Wojciech Fica , Eric Malmi , Qijun Tan , Dan Banica , Daniel Balle , Ryan Pham , Yanping Huang , Diana Avram , Hongzhi Shi , Jasjot Singh , Chris Hidey , Niharika Ahuja , Pranab Saxena , Dan Dooley , Srividya Pranavi Potharaju , Eileen O&#39;Neill , Anand Gokulchandran , Ryan Foley , Kai Zhao , Mike Dusenberry , Yuan Liu , Pulkit Mehta , Ragha Kotikalapudi , Chalence Safranek-Shrader , Andrew Goodman , Joshua Kessinger , Eran Globen , Prateek Kolhar , Chris Gorgolewski , Ali Ibrahim , Yang Song , Ali Eichenbaum , Thomas Brovelli , Sahitya Potluri , Preethi Lahoti , Cip Baetu , Ali Ghorbani , Charles Chen , Andy Crawford , Shalini Pal , Mukund Sridhar , Petru Gurita , Asier Mujika , Igor Petrovski , Pierre-Louis Cedoz , Chenmei Li , Shiyuan Chen , Niccolò Dal Santo , Siddharth Goyal , Jitesh Punjabi , Karthik Kappaganthu , Chester Kwak , Pallavi LV , Sarmishta Velury , Himadri Choudhury , Jamie Hall , Premal Shah , Ricardo Figueira , Matt Thomas , Minjie Lu , Ting Zhou , Chintu Kumar , Thomas Jurdi , Sharat Chikkerur , Yenai Ma , Adams Yu , Soo Kwak , Victor Ähdel , Sujeevan Rajayogam , Travis Choma , Fei Liu , Aditya Barua , Colin Ji , Ji Ho Park , Vincent Hellendoorn , Alex Bailey , Taylan Bilal , Huanjie Zhou , Mehrdad Khatir , Charles Sutton , Wojciech Rzadkowski , Fiona Macintosh , Roopali Vij , Konstantin Shagin , Paul Medina , Chen Liang , Jinjing Zhou , Pararth Shah , Yingying Bi , Attila Dankovics , Shipra Banga , Sabine Lehmann , Marissa Bredesen , Zifan Lin , John Eric Hoffmann , Jonathan Lai , Raynald Chung , Kai Yang , Nihal Balani , Arthur Bražinskas , Andrei Sozanschi , Matthew Hayes , Héctor Fernández Alcalde , Peter Makarov , Will Chen , Antonio Stella , Liselotte Snijders , Michael Mandl , Ante Kärrman , Paweł Nowak , Xinyi Wu , Alex Dyck , Krishnan Vaidyanathan , Raghavender R , Jessica Mallet , Mitch Rudominer , Eric Johnston , Sushil Mittal , Akhil Udathu , Janara Christensen , Vishal Verma , Zach Irving , Andreas Santucci , Gamaleldin Elsayed , Elnaz Davoodi , Marin Georgiev , Ian Tenney , Nan Hua , Geoffrey Cideron , Edouard Leurent , Mahmoud Alnahlawi , Ionut Georgescu , Nan Wei , Ivy Zheng , Dylan Scandinaro , Heinrich Jiang , Jasper Snoek , Mukund Sundararajan , Xuezhi Wang , Zack Ontiveros , Itay Karo , Jeremy Cole , Vinu Rajashekhar , Lara Tumeh , Eyal Ben-David , Rishub Jain , Jonathan Uesato , Romina Datta , Oskar Bunyan , Shimu Wu , John Zhang , Piotr Stanczyk , Ye Zhang , David Steiner , Subhajit Naskar , Michael Azzam , Matthew Johnson , Adam Paszke , Chung-Cheng Chiu , Jaume Sanchez Elias , Afroz Mohiuddin , Faizan Muhammad , Jin Miao , Andrew Lee , Nino Vieillard , Jane Park , Jiageng Zhang , Jeff Stanway , Drew Garmon , Abhijit Karmarkar , Zhe Dong , Jong Lee , Aviral Kumar , Luowei Zhou , Jonathan Evens , William Isaac , Geoffrey Irving , Edward Loper , Michael Fink , Isha Arkatkar , Nanxin Chen , Izhak Shafran , Ivan Petrychenko , Zhe Chen , Johnson Jia , Anselm Levskaya , Zhenkai Zhu , Peter Grabowski , Yu Mao , Alberto Magni , Kaisheng Yao , Javier Snaider , Norman Casagrande , Evan Palmer , Paul Suganthan , Alfonso Castaño , Irene Giannoumis , Wooyeol Kim , Mikołaj Rybiński , Ashwin Sreevatsa , Jennifer Prendki , David Soergel , Adrian Goedeckemeyer , Willi Gierke , Mohsen Jafari , Meenu Gaba , Jeremy Wiesner , Diana Gage Wright , Yawen Wei , Harsha Vashisht , Yana Kulizhskaya , Jay Hoover , Maigo Le , Lu Li , Chimezie Iwuanyanwu , Lu Liu , Kevin Ramirez , Andrey Khorlin , Albert Cui , Tian LIN , Marcus Wu , Ricardo Aguilar , Keith Pallo , Abhishek Chakladar , Ginger Perng , Elena Allica Abellan , Mingyang Zhang , Ishita Dasgupta , Nate Kushman , Ivo Penchev , Alena Repina , Xihui Wu , Tom van der Weide , Priya Ponnapalli , Caroline Kaplan , Jiri Simsa , Shuangfeng Li , Olivier Dousse , Fan Yang , Jeff Piper , Nathan Ie , Rama Pasumarthi , Nathan Lintz , Anitha Vijayakumar , Daniel Andor , Pedro Valenzuela , Minnie Lui , Cosmin Paduraru , Daiyi Peng , Katherine Lee , Shuyuan Zhang , Somer Greene , Duc Dung Nguyen , Paula Kurylowicz , Cassidy Hardin , Lucas Dixon , Lili Janzer , Kiam Choo , Ziqiang Feng , Biao Zhang , Achintya Singhal , Dayou Du , Dan McKinnon , Natasha Antropova , Tolga Bolukbasi , Orgad Keller , David Reid , Daniel Finchelstein , Maria Abi Raad , Remi Crocker , Peter Hawkins , Robert Dadashi , Colin Gaffney , Ken Franko , Anna Bulanova , Rémi Leblond , Shirley Chung , Harry Askham , Luis C. Cobo , Kelvin Xu , Felix Fischer , Jun Xu , Christina Sorokin , Chris Alberti , Chu-Cheng Lin , Colin Evans , Alek Dimitriev , Hannah Forbes , Dylan Banarse , Zora Tung , Mark Omernick , Colton Bishop , Rachel Sterneck , Rohan Jain , Jiawei Xia , Ehsan Amid , Francesco Piccinno , Xingyu Wang , Praseem Banzal , Daniel J. Mankowitz , Alex Polozov , Victoria Krakovna , Sasha Brown , MohammadHossein Bateni , Dennis Duan , Vlad Firoiu , Meghana Thotakuri , Tom Natan , Matthieu Geist , Ser tan Girgin , Hui Li , Jiayu Ye , Ofir Roval , Reiko Tojo , Michael Kwong , James Lee-Thorp , Christopher Yew , Danila Sinopalnikov , Sabela Ramos , John Mellor , Abhishek Sharma , Kathy Wu , David Miller , Nicolas Sonnerat , Denis Vnukov , Rory Greig , Jennifer Beattie , Emily Caveness , Libin Bai , Julian Eisenschlos , Alex Korchemniy , Tomy Tsai , Mimi Jasarevic , Weize Kong , Phuong Dao , Zeyu Zheng , Frederick Liu , Fan Yang , Rui Zhu , Tian Huey Teh , Jason Sanmiya , Evgeny Gladchenko , Nejc Trdin , Daniel Toyama , Evan Rosen , Sasan Tavakkol , Linting Xue , Chen Elkind , Oliver Woodman , John Carpenter , George Papamakarios , Rupert Kemp , Sushant Kafle , Tanya Grunina , Rishika Sinha , Alice Talbert , Diane Wu , Denese Owusu-Afriyie , Cosmo Du , Chloe Thornton , Jordi Pont-Tuset , Pradyumna Narayana , Jing Li , Saaber Fatehi , John Wieting , Omar Ajmeri , Benigno Uria , Yeongil Ko , Laura Knight , Amélie Héliou , Ning Niu , Shane Gu , Chenxi Pang , Yeqing Li , Nir Levine , Ariel Stolovich , Rebeca Santamaria-Fernandez , Sonam Goenka , Wenny Yustalim , Robin Strudel , Ali Elqursh , Charlie Deck , Hyo Lee , Zonglin Li , Kyle Levin , Raphael Hoffmann , Dan Holtmann-Rice , Olivier Bachem , Sho Arora , Christy Koh , Soheil Hassas Yeganeh , Siim Põder , Mukarram Tariq , Yanhua Sun , Lucian Ionita , Mojtaba Seyedhosseini , Pouya Tafti , Zhiyu Liu , Anmol Gulati , Jasmine Liu , Xinyu Ye , Bart Chrzaszcz , Lily Wang , Nikhil Sethi , Tianrun Li , Ben Brown , Shreya Singh , Wei Fan , Aaron Parisi , Joe Stanton , Vinod Koverkathu , Christopher A. Choquette-Choo , Yunjie Li , TJ Lu , Abe Ittycheriah , Prakash Shroff , Mani Varadarajan , Sanaz Bahargam , Rob Willoughby , David Gaddy , Guillaume Desjardins , Marco Cornero , Brona Robenek , Bhavishya Mittal , Ben Albrecht , Ashish Shenoy , Fedor Moiseev , Henrik Jacobsson , Alireza Ghaffarkhah , Morgane Rivière , Alanna Walton , Clément Crepy , Alicia Parrish , Zongwei Zhou , Clement Farabet , Carey Radebaugh , Praveen Srinivasan , Claudia van der Salm , Andreas Fidjeland , Salvatore Scellato , Eri Latorre-Chimoto , Hanna Klimczak-Plucińska , David Bridson , Dario de Cesare , Tom Hudson , Piermaria Mendolicchio , Lexi Walker , Alex Morris , Matthew Mauger , Alexey Guseynov , Alison Reid , Seth Odoom , Lucia Loher , Victor Cotruta , Madhavi Yenugula , Dominik Grewe , Anastasia Petrushkina , Tom Duerig , Antonio Sanchez , Steve Yadlowsky , Amy Shen , Amir Globerson , Lynette Webb , Sahil Dua , Dong Li , Surya Bhupatiraju , Dan Hurt , Haroon Qureshi , Ananth Agarwal , Tomer Shani , Matan Eyal , Anuj Khare , Shreyas Rammohan Belle , Lei Wang , Chetan Tekur , Mihir Sanjay Kale , Jinliang Wei , Ruoxin Sang , Brennan Saeta , Tyler Liechty , Yi Sun , Yao Zhao , Stephan Lee , Pandu Nayak , Doug Fritz , Manish Reddy Vuyyuru , John Aslanides , Nidhi Vyas , Martin Wicke , Xiao Ma , Evgenii Eltyshev , Nina Martin , Hardie Cate , James Manyika , Keyvan Amiri , Yelin Kim , Xi Xiong , Kai Kang , Florian Luisier , Nilesh Tripuraneni , David Madras , Mandy Guo , Austin Waters , Oliver Wang , Joshua Ainslie , Jason Baldridge , Han Zhang , Garima Pruthi , Jakob Bauer , Feng Yang , Riham Mansour , Jason Gelman , Yang Xu , George Polovets , Ji Liu , Honglong Cai , Warren Chen , XiangHai Sheng , Emily Xue , Sherjil Ozair , Christof Angermueller , Xiaowei Li , Anoop Sinha , Weiren Wang , Julia Wiesinger , Emmanouil Koukoumidis , Yuan Tian , Anand Iyer , Madhu Gurumurthy , Mark Goldenson , Parashar Shah , MK Blake , Hongkun Yu , Anthony Urbanowicz , Jennimaria Palomaki , Chrisantha Fernando , Ken Durden , Harsh Mehta , Nikola Momchev , Elahe Rahimtoroghi , Maria Georgaki , Amit Raul , Sebastian Ruder , Morgan Redshaw , Jinhyuk Lee , Denny Zhou , Komal Jalan , Dinghua Li , Blake Hechtman , Parker Schuh , Milad Nasr , Kieran Milan , Vladimir Mikulik , Juliana Franco , Tim Green , Nam Nguyen , Joe Kelley , Aroma Mahendru , Andrea Hu , Joshua Howland , Ben Vargas , Jeffrey Hui , Kshitij Bansal , Vikram Rao , Rakesh Ghiya , Emma Wang , Ke Ye , Jean Michel Sarr , Melanie Moranski Preston , Madeleine Elish , Steve Li , Aakash Kaku , Jigar Gupta , Ice Pasupat , Da-Cheng Juan , Milan Someswar , Tejvi M. , Xinyun Chen , Aida Amini , Alex Fabrikant , Eric Chu , Xuanyi Dong , Amruta Muthal , Senaka Buthpitiya , Sarthak Jauhari , Nan Hua , Urvashi Khandelwal , Ayal Hitron , Jie Ren , Larissa Rinaldi , Shahar Drath , Avigail Dabush , Nan-Jiang Jiang , Harshal Godhia , Uli Sachs , Anthony Chen , Yicheng Fan , Hagai Taitelbaum , Hila Noga , Zhuyun Dai , James Wang , Chen Liang , Jenny Hamer , Chun-Sung Ferng , Chenel Elkind , Aviel Atias , Paulina Lee , Vít Listík , Mathias Carlen , Jan van de Kerkhof , Marcin Pikus , Krunoslav Zaher , Paul Müller , Sasha Zykova , Richard Stefanec , Vitaly Gatsko , Christoph Hirnschall , Ashwin Sethi , Xingyu Federico Xu , Chetan Ahuja , Beth Tsai , Anca Stefanoiu , Bo Feng , Keshav Dhandhania , Manish Katyal , Akshay Gupta , Atharva Parulekar , Divya Pitta , Jing Zhao , Vivaan Bhatia , Yashodha Bhavnani , Omar Alhadlaq , Xiaolin Li , Peter Danenberg , Dennis Tu , Alex Pine , Vera Filippova , Abhipso Ghosh , Ben Limonchik , Bhargava Urala , Chaitanya Krishna Lanka , Derik Clive , Yi Sun , Edward Li , Hao Wu , Kevin Hongtongsak , Ianna Li , Kalind Thakkar , Kuanysh Omarov , Kushal Majmundar , Michael Alverson , Michael Kucharski , Mohak Patel , Mudit Jain , Maksim Zabelin , Paolo Pelagatti , Rohan Kohli , Saurabh Kumar , Joseph Kim , Swetha Sankar , Vineet Shah , Lakshmi Ramachandruni , Xiangkai Zeng , Ben Bariach , Laura Weidinger , Tu Vu , Alek Andreev , Antoine He , Kevin Hui , Sheleem Kashem , Amar Subramanya , Sissie Hsiao , Demis Hassabis , Koray Kavukcuoglu , Adam Sadovsky , Quoc Le , Trevor Strohman , Yonghui Wu , Slav Petrov , Jeffrey Dean , Oriol Vinyals et al. (1251 additional authors not shown) &nbsp;You must enable JavaScript to view entire author list. View a PDF of the paper titled Gemini: A Family of Highly Capable Multimodal Models, by Gemini Team Google: Rohan Anil and 1349 other authors View PDF Abstract: This report introduces a new family of multimodal models, Gemini, that exhibit remarkable capabilities across image, audio, video, and text understanding. The Gemini family consists of Ultra, Pro, and Nano sizes, suitable for applications ranging from complex reasoning tasks to on-device memory-constrained use-cases. Evaluation on a broad range of benchmarks shows that our most-capable Gemini Ultra model advances the state of the art in 30 of 32 of these benchmarks - notably being the first model to achieve human-expert performance on the well-studied exam benchmark MMLU, and improving the state of the art in every one of the 20 multimodal benchmarks we examined. We believe that the new capabilities of the Gemini family in cross-modal reasoning and language understanding will enable a wide variety of use cases. We discuss our approach toward post-training and deploying Gemini models responsibly to users through services including Gemini, Gemini Advanced, Google AI Studio, and Cloud Vertex AI. Subjects: Computation and Language (cs.CL) ; Artificial Intelligence (cs.AI); Computer Vision and Pattern Recognition (cs.CV) Cite as: arXiv:2312.11805 [cs.CL] &nbsp; (or arXiv:2312.11805v5 [cs.CL] for this version) &nbsp; https://doi.org/10.48550/arXiv.2312.11805 Focus to learn more arXiv-issued DOI via DataCite Submission history From: Mariko Iinuma [ view email ] [v1] Tue, 19 Dec 2023 02:39:27 UTC (26,031 KB) [v2] Tue, 2 Apr 2024 22:35:21 UTC (26,595 KB) [v3] Mon, 20 May 2024 20:05:40 UTC (26,595 KB) [v4] Mon, 17 Jun 2024 22:05:29 UTC (26,595 KB) [v5] Fri, 9 May 2025 21:04:06 UTC (26,595 KB) Full-text links: Access Paper: View a PDF of the paper titled Gemini: A Family of Highly Capable Multimodal Models, by Gemini Team Google: Rohan Anil and 1349 other authors View PDF TeX Source Other Formats view license Current browse context: cs.CL &lt;&nbsp;prev &nbsp; | &nbsp; next&nbsp;&gt; new | recent | 2023-12 Change to browse by: cs cs.AI cs.CV References &amp; Citations NASA ADS Google Scholar Semantic Scholar 1 blog link ( what is this? ) a export BibTeX citation Loading... BibTeX formatted citation &times; loading... Data provided by: Bookmark Bibliographic Tools Bibliographic and Citation Tools Bibliographic Explorer Toggle Bibliographic Explorer ( What is the Explorer? ) Connected Papers Toggle Connected Papers ( What is Connected Papers? ) Litmaps Toggle Litmaps ( What is Litmaps? ) scite.ai Toggle scite Smart Citations ( What are Smart Citations? ) Code, Data, Media Code, Data and Media Associated with this Article alphaXiv Toggle alphaXiv ( What is alphaXiv? ) Links to Code Toggle CatalyzeX Code Finder for Papers ( What is CatalyzeX? ) DagsHub Toggle DagsHub ( What is DagsHub? ) GotitPub Toggle Gotit.pub ( What is GotitPub? ) Huggingface Toggle Hugging Face ( What is Huggingface? ) Links to Code Toggle Papers with Code ( What is Papers with Code? ) ScienceCast Toggle ScienceCast ( What is ScienceCast? ) Demos Demos Replicate Toggle Replicate ( What is Replicate? ) Spaces Toggle Hugging Face Spaces ( What is Spaces? ) Spaces Toggle TXYZ.AI ( What is TXYZ.AI? ) Related Papers Recommenders and Search Tools Link to Influence Flower Influence Flower ( What are Influence Flowers? ) Core recommender toggle CORE Recommender ( What is CORE? ) Author Venue Institution Topic About arXivLabs arXivLabs: experimental projects with community collaborators arXivLabs is a framework that allows collaborators to develop and share new arXiv features directly on our website. Both individuals and organizations that work with arXivLabs have embraced and accepted our values of openness, community, excellence, and user data privacy. arXiv is committed to these values and only works with partners that adhere to them. Have an idea for a project that will add value for arXiv's community? Learn more about arXivLabs . Which authors of this paper are endorsers? | Disable MathJax ( What is MathJax? ) About Help contact arXiv Click here to contact arXiv Contact subscribe to arXiv mailings Click here to subscribe Subscribe Copyright Privacy Policy Web Accessibility Assistance arXiv Operational Status Get status notifications via email or slack ",
      "fetch_method": "direct-html"
    },
    {
      "arxiv_id": "https://r.jina.ai/https://ai.google.dev/gemma/docs/pytorch_gemma](https://ai.google.dev/gemma/docs/pytorch_gemma",
      "full_text": "Title: 404  |  Page Not Found  |  Google AI for Developers\n\nURL Source: https://ai.google.dev/gemma/docs/pytorch_gemma](https:/ai.google.dev/gemma/docs/pytorch_gemma\n\nWarning: Target URL returned error 404: Not Found\n\nMarkdown Content:\n404 | Page Not Found | Google AI for Developers\n\n===============\n[Skip to main content](https://ai.google.dev/gemma/docs/pytorch_gemma](https:/ai.google.dev/gemma/docs/pytorch_gemma#main-content)\n\n[![Image 1: Google AI for Developers](https://www.gstatic.com/devrel-devsite/prod/v55e81371229cf93fbb4781915f01d3bef8e4cb4b674c7c839a1879ebb706855a/googledevai/images/lockup-new.svg)](https://ai.google.dev/)\n\n Models \n\n*   Gemini\n*   [About](https://deepmind.google/gemini)\n*   [Docs](https://ai.google.dev/gemini-api/docs)\n*   [API reference](https://ai.google.dev/api)\n*   [Pricing](https://ai.google.dev/pricing)\n\n*   Imagen\n*   [About](https://deepmind.google/technologies/imagen-3/)\n*   [Docs](https://ai.google.dev/gemini-api/docs/image-generation#imagen)\n*   [Pricing](https://ai.google.dev/pricing)\n\n*   Veo\n*   [About](https://deepmind.google/technologies/veo/veo-2/)\n*   [Docs](https://ai.google.dev/gemini-api/docs/video)\n*   [Pricing](https://ai.google.dev/pricing)\n\n*   Gemma\n*   [About](https://deepmind.google/models/gemma)\n*   [Docs](https://ai.google.dev/gemma/docs)\n*   [Gemmaverse](https://ai.google.dev/gemma/gemmaverse)\n\nMore\n\n Solutions \n\n*   Build with Gemini\n*   [Gemini API](https://ai.google.dev/gemini-api/docs)\n*   [Google AI Studio](https://aistudio.google.com/)\n\n*   Customize Gemma open models\n*   [Gemma open models](https://ai.google.dev/gemma)\n*   [Multi-framework with Keras](https://keras.io/keras_3/)\n*   [Fine-tune in Colab](https://colab.sandbox.google.com/github/google/generative-ai-docs/blob/main/site/en/gemma/docs/lora_tuning.ipynb)\n\n*   Run on-device\n*   [Google AI Edge](https://ai.google.dev/edge)\n*   [Gemini Nano on Android](https://developer.android.com/ai/gemini-nano)\n*   [Chrome built-in web APIs](https://developer.chrome.com/docs/ai/built-in)\n\n*   Build responsibly\n*   [Responsible GenAI Toolkit](https://ai.google.dev/responsible)\n*   [Secure AI Framework](https://saif.google/)\n\n Code assistance \n\n*   [Android Studio](https://developer.android.com/gemini-in-android)\n*   [Chrome DevTools](https://developer.chrome.com/docs/devtools/console/understand-messages)\n*   [Colab](https://colab.google/)\n*   [Firebase](https://firebase.google.com/products/generative-ai)\n*   [Google Cloud](https://cloud.google.com/products/gemini/code-assist)\n*   [JetBrains](https://plugins.jetbrains.com/plugin/8079-google-cloud-code)\n*   [Jules](https://labs.google.com/jules/home)\n*   [VS Code](https://marketplace.visualstudio.com/items?itemName=GoogleCloudTools.cloudcode)\n\n Showcase \n\n*   [Gemini Showcase](https://ai.google.dev/showcase)\n*   [Gemini API Developer Competition](https://ai.google.dev/competition)\n\n Community \n\n*   [Google AI Forum](https://discuss.ai.google.dev/)\n*   [Gemini for Research](https://ai.google.dev/gemini-api/docs/gemini-for-research)\n\n/\n\n*   [English](https://ai.google.dev/gemma/docs/pytorch_gemma%5D(https:/ai.google.dev/gemma/docs/pytorch_gemma)\n*   [Deutsch](https://ai.google.dev/gemma/docs/pytorch_gemma%5D(https:/ai.google.dev/gemma/docs/pytorch_gemma?hl=de)\n*   [Español – América Latina](https://ai.google.dev/gemma/docs/pytorch_gemma%5D(https:/ai.google.dev/gemma/docs/pytorch_gemma?hl=es-419)\n*   [Français](https://ai.google.dev/gemma/docs/pytorch_gemma%5D(https:/ai.google.dev/gemma/docs/pytorch_gemma?hl=fr)\n*   [Indonesia](https://ai.google.dev/gemma/docs/pytorch_gemma%5D(https:/ai.google.dev/gemma/docs/pytorch_gemma?hl=id)\n*   [Italiano](https://ai.google.dev/gemma/docs/pytorch_gemma%5D(https:/ai.google.dev/gemma/docs/pytorch_gemma?hl=it)\n*   [Polski](https://ai.google.dev/gemma/docs/pytorch_gemma%5D(https:/ai.google.dev/gemma/docs/pytorch_gemma?hl=pl)\n*   [Português – Brasil](https://ai.google.dev/gemma/docs/pytorch_gemma%5D(https:/ai.google.dev/gemma/docs/pytorch_gemma?hl=pt-br)\n*   [Shqip](https://ai.google.dev/gemma/docs/pytorch_gemma%5D(https:/ai.google.dev/gemma/docs/pytorch_gemma?hl=sq)\n*   [Tiếng Việt](https://ai.google.dev/gemma/docs/pytorch_gemma%5D(https:/ai.google.dev/gemma/docs/pytorch_gemma?hl=vi)\n*   [Türkçe](https://ai.google.dev/gemma/docs/pytorch_gemma%5D(https:/ai.google.dev/gemma/docs/pytorch_gemma?hl=tr)\n*   [Русский](https://ai.google.dev/gemma/docs/pytorch_gemma%5D(https:/ai.google.dev/gemma/docs/pytorch_gemma?hl=ru)\n*   [עברית](https://ai.google.dev/gemma/docs/pytorch_gemma%5D(https:/ai.google.dev/gemma/docs/pytorch_gemma?hl=he)\n*   [العربيّة](https://ai.google.dev/gemma/docs/pytorch_gemma%5D(https:/ai.google.dev/gemma/docs/pytorch_gemma?hl=ar)\n*   [فارسی](https://ai.google.dev/gemma/docs/pytorch_gemma%5D(https:/ai.google.dev/gemma/docs/pytorch_gemma?hl=fa)\n*   [हिंदी](https://ai.google.dev/gemma/docs/pytorch_gemma%5D(https:/ai.google.dev/gemma/docs/pytorch_gemma?hl=hi)\n*   [বাংলা](https://ai.google.dev/gemma/docs/pytorch_gemma%5D(https:/ai.google.dev/gemma/docs/pytorch_gemma?hl=bn)\n*   [ภาษาไทย](https://ai.google.dev/gemma/docs/pytorch_gemma%5D(https:/ai.google.dev/gemma/docs/pytorch_gemma?hl=th)\n*   [中文 – 简体](https://ai.google.dev/gemma/docs/pytorch_gemma%5D(https:/ai.google.dev/gemma/docs/pytorch_gemma?hl=zh-cn)\n*   [中文 – 繁體](https://ai.google.dev/gemma/docs/pytorch_gemma%5D(https:/ai.google.dev/gemma/docs/pytorch_gemma?hl=zh-tw)\n*   [日本語](https://ai.google.dev/gemma/docs/pytorch_gemma%5D(https:/ai.google.dev/gemma/docs/pytorch_gemma?hl=ja)\n*   [한국어](https://ai.google.dev/gemma/docs/pytorch_gemma%5D(https:/ai.google.dev/gemma/docs/pytorch_gemma?hl=ko)\n\n[Sign in](https://ai.google.dev/_d/signin?continue=https%3A%2F%2Fai.google.dev%2Fgemma%2Fdocs%2Fpytorch_gemma%255D%28https%3A%2Fai.google.dev%2Fgemma%2Fdocs%2Fpytorch_gemma&prompt=select_account)\n\n[![Image 2: Google AI for Developers](https://www.gstatic.com/devrel-devsite/prod/v55e81371229cf93fbb4781915f01d3bef8e4cb4b674c7c839a1879ebb706855a/googledevai/images/lockup-new.svg)](https://ai.google.dev/)\n\n*    Models \n    *    More \n\n*    Solutions \n    *    More \n\n*    Code assistance \n    *    More \n\n*    Showcase \n    *    More \n\n*    Community \n    *    More \n\n*    Gemini \n*   [About](https://deepmind.google/gemini)\n*   [Docs](https://ai.google.dev/gemini-api/docs)\n*   [API reference](https://ai.google.dev/api)\n*   [Pricing](https://ai.google.dev/pricing)\n*    Imagen \n*   [About](https://deepmind.google/technologies/imagen-3/)\n*   [Docs](https://ai.google.dev/gemini-api/docs/image-generation#imagen)\n*   [Pricing](https://ai.google.dev/pricing)\n*    Veo \n*   [About](https://deepmind.google/technologies/veo/veo-2/)\n*   [Docs](https://ai.google.dev/gemini-api/docs/video)\n*   [Pricing](https://ai.google.dev/pricing)\n*    Gemma \n*   [About](https://deepmind.google/models/gemma)\n*   [Docs](https://ai.google.dev/gemma/docs)\n*   [Gemmaverse](https://ai.google.dev/gemma/gemmaverse)\n\n*    Build with Gemini \n*   [Gemini API](https://ai.google.dev/gemini-api/docs)\n*   [Google AI Studio](https://aistudio.google.com/)\n*    Customize Gemma open models \n*   [Gemma open models](https://ai.google.dev/gemma)\n*   [Multi-framework with Keras](https://keras.io/keras_3/)\n*   [Fine-tune in Colab](https://colab.sandbox.google.com/github/google/generative-ai-docs/blob/main/site/en/gemma/docs/lora_tuning.ipynb)\n*    Run on-device \n*   [Google AI Edge](https://ai.google.dev/edge)\n*   [Gemini Nano on Android](https://developer.android.com/ai/gemini-nano)\n*   [Chrome built-in web APIs](https://developer.chrome.com/docs/ai/built-in)\n*    Build responsibly \n*   [Responsible GenAI Toolkit](https://ai.google.dev/responsible)\n*   [Secure AI Framework](https://saif.google/)\n\n*   [Android Studio](https://developer.android.com/gemini-in-android)\n*   [Chrome DevTools](https://developer.chrome.com/docs/devtools/console/understand-messages)\n*   [Colab](https://colab.google/)\n*   [Firebase](https://firebase.google.com/products/generative-ai)\n*   [Google Cloud](https://cloud.google.com/products/gemini/code-assist)\n*   [JetBrains](https://plugins.jetbrains.com/plugin/8079-google-cloud-code)\n*   [Jules](https://labs.google.com/jules/home)\n*   [VS Code](https://marketplace.visualstudio.com/items?itemName=GoogleCloudTools.cloudcode)\n\n*   [Gemini Showcase](https://ai.google.dev/showcase)\n*   [Gemini API Developer Competition](https://ai.google.dev/competition)\n\n*   [Google AI Forum](https://discuss.ai.google.dev/)\n*   [Gemini for Research](https://ai.google.dev/gemini-api/docs/gemini-for-research)\n\n### 404\n\n Sorry, we couldn't find that page. \n\n/\n\n*   [Terms](https://policies.google.com/terms)\n*   [Privacy](https://policies.google.com/privacy)\n*   [Manage cookies](https://ai.google.dev/gemma/docs/pytorch_gemma](https:/ai.google.dev/gemma/docs/pytorch_gemma#)\n\n*   [English](https://ai.google.dev/gemma/docs/pytorch_gemma%5D(https:/ai.google.dev/gemma/docs/pytorch_gemma)\n*   [Deutsch](https://ai.google.dev/gemma/docs/pytorch_gemma%5D(https:/ai.google.dev/gemma/docs/pytorch_gemma?hl=de)\n*   [Español – América Latina](https://ai.google.dev/gemma/docs/pytorch_gemma%5D(https:/ai.google.dev/gemma/docs/pytorch_gemma?hl=es-419)\n*   [Français](https://ai.google.dev/gemma/docs/pytorch_gemma%5D(https:/ai.google.dev/gemma/docs/pytorch_gemma?hl=fr)\n*   [Indonesia](https://ai.google.dev/gemma/docs/pytorch_gemma%5D(https:/ai.google.dev/gemma/docs/pytorch_gemma?hl=id)\n*   [Italiano](https://ai.google.dev/gemma/docs/pytorch_gemma%5D(https:/ai.google.dev/gemma/docs/pytorch_gemma?hl=it)\n*   [Polski](https://ai.google.dev/gemma/docs/pytorch_gemma%5D(https:/ai.google.dev/gemma/docs/pytorch_gemma?hl=pl)\n*   [Português – Brasil](https://ai.google.dev/gemma/docs/pytorch_gemma%5D(https:/ai.google.dev/gemma/docs/pytorch_gemma?hl=pt-br)\n*   [Shqip](https://ai.google.dev/gemma/docs/pytorch_gemma%5D(https:/ai.google.dev/gemma/docs/pytorch_gemma?hl=sq)\n*   [Tiếng Việt](https://ai.google.dev/gemma/docs/pytorch_gemma%5D(https:/ai.google.dev/gemma/docs/pytorch_gemma?hl=vi)\n*   [Türkçe](https://ai.google.dev/gemma/docs/pytorch_gemma%5D(https:/ai.google.dev/gemma/docs/pytorch_gemma?hl=tr)\n*   [Русский](https://ai.google.dev/gemma/docs/pytorch_gemma%5D(https:/ai.google.dev/gemma/docs/pytorch_gemma?hl=ru)\n*   [עברית](https://ai.google.dev/gemma/docs/pytorch_gemma%5D(https:/ai.google.dev/gemma/docs/pytorch_gemma?hl=he)\n*   [العربيّة](https://ai.google.dev/gemma/docs/pytorch_gemma%5D(https:/ai.google.dev/gemma/docs/pytorch_gemma?hl=ar)\n*   [فارسی](https://ai.google.dev/gemma/docs/pytorch_gemma%5D(https:/ai.google.dev/gemma/docs/pytorch_gemma?hl=fa)\n*   [हिंदी](https://ai.google.dev/gemma/docs/pytorch_gemma%5D(https:/ai.google.dev/gemma/docs/pytorch_gemma?hl=hi)\n*   [বাংলা](https://ai.google.dev/gemma/docs/pytorch_gemma%5D(https:/ai.google.dev/gemma/docs/pytorch_gemma?hl=bn)\n*   [ภาษาไทย](https://ai.google.dev/gemma/docs/pytorch_gemma%5D(https:/ai.google.dev/gemma/docs/pytorch_gemma?hl=th)\n*   [中文 – 简体](https://ai.google.dev/gemma/docs/pytorch_gemma%5D(https:/ai.google.dev/gemma/docs/pytorch_gemma?hl=zh-cn)\n*   [中文 – 繁體](https://ai.google.dev/gemma/docs/pytorch_gemma%5D(https:/ai.google.dev/gemma/docs/pytorch_gemma?hl=zh-tw)\n*   [日本語](https://ai.google.dev/gemma/docs/pytorch_gemma%5D(https:/ai.google.dev/gemma/docs/pytorch_gemma?hl=ja)\n*   [한국어](https://ai.google.dev/gemma/docs/pytorch_gemma%5D(https:/ai.google.dev/gemma/docs/pytorch_gemma?hl=ko)\n",
      "fetch_method": "jina-reader"
    }
  ]
}