{
    "repo": "google/gemma_pytorch",
    "branch": "main",
    "files": [
        ".gitignore",
        "CONTRIBUTING.md",
        "LICENSE",
        "README.md",
        "docker/Dockerfile",
        "docker/xla.Dockerfile",
        "docker/xla_gpu.Dockerfile",
        "gemma/__init__.py",
        "gemma/config.py",
        "gemma/gemma3_model.py",
        "gemma/gemma3_preprocessor.py",
        "gemma/model.py",
        "gemma/model_xla.py",
        "gemma/siglip_vision/__init__.py",
        "gemma/siglip_vision/config.py",
        "gemma/siglip_vision/pan_and_scan.py",
        "gemma/siglip_vision/preprocessor.py",
        "gemma/siglip_vision/siglip_vision_model.py",
        "gemma/tokenizer.py",
        "gemma/xla_model_parallel.py",
        "requirements.txt",
        "scripts/images/cow_in_beach.jpg",
        "scripts/images/lilly.jpg",
        "scripts/images/sunflower.JPG",
        "scripts/images/test_image.jpg",
        "scripts/run.py",
        "scripts/run_multimodal.py",
        "scripts/run_multimodal.py.orig",
        "scripts/run_xla.py",
        "setup.py",
        "tokenizer/gemma3_cleaned_262144_v2.spiece.model",
        "tokenizer/tokenizer.model"
    ],
    "license_files": {
        "LICENSE": "                                 Apache License\n                           Version 2.0, January 2004\n                        http://www.apache.org/licenses/\n\n   TERMS AND CONDITIONS FOR USE, REPRODUCTION, AND DISTRIBUTION\n\n   1. Definitions.\n\n      \"License\" shall mean the terms and conditions for use, reproduction,\n      and distribution as defined by Sections 1 through 9 of this document.\n\n      \"Licensor\" shall mean the copyright owner or entity authorized by\n      the copyright owner that is granting the License.\n\n      \"Legal Entity\" shall mean the union of the acting entity and all\n      other entities that control, are controlled by, or are under common\n      control with that entity. For the purposes of this definition,\n      \"control\" means (i) the power, direct or indirect, to cause the\n      direction or management of such entity, whether by contract or\n      otherwise, or (ii) ownership of fifty percent (50%) or more of the\n      outstanding shares, or (iii) beneficial ownership of such entity.\n\n      \"You\" (or \"Your\") shall mean an individual or Legal Entity\n      exercising permissions granted by this License.\n\n      \"Source\" form shall mean the preferred form for making modifications,\n      including but not limited to software source code, documentation\n      source, and configuration files.\n\n      \"Object\" form shall mean any form resulting from mechanical\n      transformation or translation of a Source form, including but\n      not limited to compiled object code, generated documentation,\n      and conversions to other media types.\n\n      \"Work\" shall mean the work of authorship, whether in Source or\n      Object form, made available under the License, as indicated by a\n      copyright notice that is included in or attached to the work\n      (an example is provided in the Appendix below).\n\n      \"Derivative Works\" shall mean any work, whether in Source or Object\n      form, that is based on (or derived from) the Work and for which the\n      editorial revisions, annotations, elaborations, or other modifications\n      represent, as a whole, an original work of authorship. For the purposes\n      of this License, Derivative Works shall not include works that remain\n      separable from, or merely link (or bind by name) to the interfaces of,\n      the Work and Derivative Works thereof.\n\n      \"Contribution\" shall mean any work of authorship, including\n      the original version of the Work and any modifications or additions\n      to that Work or Derivative Works thereof, that is intentionally\n      submitted to Licensor for inclusion in the Work by the copyright owner\n      or by an individual or Legal Entity authorized to submit on behalf of\n      the copyright owner. For the purposes of this definition, \"submitted\"\n      means any form of electronic, verbal, or written communication sent\n      to the Licensor or its representatives, including but not limited to\n      communication on electronic mailing lists, source code control systems,\n      and issue tracking systems that are managed by, or on behalf of, the\n      Licensor for the purpose of discussing and improving the Work, but\n      excluding communication that is conspicuously marked or otherwise\n      designated in writing by the copyright owner as \"Not a Contribution.\"\n\n      \"Contributor\" shall mean Licensor and any individual or Legal Entity\n      on behalf of whom a Contribution has been received by Licensor and\n      subsequently incorporated within the Work.\n\n   2. Grant of Copyright License. Subject to the terms and conditions of\n      this License, each Contributor hereby grants to You a perpetual,\n      worldwide, non-exclusive, no-charge, royalty-free, irrevocable\n      copyright license to reproduce, prepare Derivative Works of,\n      publicly display, publicly perform, sublicense, and distribute the\n      Work and such Derivative Works in Source or Object form.\n\n   3. Grant of Patent License. Subject to the terms and conditions of\n      this License, each Contributor hereby grants to You a perpetual,\n      worldwide, non-exclusive, no-charge, royalty-free, irrevocable\n      (except as stated in this section) patent license to make, have made,\n      use, offer to sell, sell, import, and otherwise transfer the Work,\n      where such license applies only to those patent claims licensable\n      by such Contributor that are necessarily infringed by their\n      Contribution(s) alone or by combination of their Contribution(s)\n      with the Work to which such Contribution(s) was submitted. If You\n      institute patent litigation against any entity (including a\n      cross-claim or counterclaim in a lawsuit) alleging that the Work\n      or a Contribution incorporated within the Work constitutes direct\n      or contributory patent infringement, then any patent licenses\n      granted to You under this License for that Work shall terminate\n      as of the date such litigation is filed.\n\n   4. Redistribution. You may reproduce and distribute copies of the\n      Work or Derivative Works thereof in any medium, with or without\n      modifications, and in Source or Object form, provided that You\n      meet the following conditions:\n\n      (a) You must give any other recipients of the Work or\n          Derivative Works a copy of this License; and\n\n      (b) You must cause any modified files to carry prominent notices\n          stating that You changed the files; and\n\n      (c) You must retain, in the Source form of any Derivative Works\n          that You distribute, all copyright, patent, trademark, and\n          attribution notices from the Source form of the Work,\n          excluding those notices that do not pertain to any part of\n          the Derivative Works; and\n\n      (d) If the Work includes a \"NOTICE\" text file as part of its\n          distribution, then any Derivative Works that You distribute must\n          include a readable copy of the attribution notices contained\n          within such NOTICE file, excluding those notices that do not\n          pertain to any part of the Derivative Works, in at least one\n          of the following places: within a NOTICE text file distributed\n          as part of the Derivative Works; within the Source form or\n          documentation, if provided along with the Derivative Works; or,\n          within a display generated by the Derivative Works, if and\n          wherever such third-party notices normally appear. The contents\n          of the NOTICE file are for informational purposes only and\n          do not modify the License. You may add Your own attribution\n          notices within Derivative Works that You distribute, alongside\n          or as an addendum to the NOTICE text from the Work, provided\n          that such additional attribution notices cannot be construed\n          as modifying the License.\n\n      You may add Your own copyright statement to Your modifications and\n      may provide additional or different license terms and conditions\n      for use, reproduction, or distribution of Your modifications, or\n      for any such Derivative Works as a whole, provided Your use,\n      reproduction, and distribution of the Work otherwise complies with\n      the conditions stated in this License.\n\n   5. Submission of Contributions. Unless You explicitly state otherwise,\n      any Contribution intentionally submitted for inclusion in the Work\n      by You to the Licensor shall be under the terms and conditions of\n      this License, without any additional terms or conditions.\n      Notwithstanding the above, nothing herein shall supersede or modify\n      the terms of any separate license agreement you may have executed\n      with Licensor regarding such Contributions.\n\n   6. Trademarks. This License does not grant permission to use the trade\n      names, trademarks, service marks, or product names of the Licensor,\n      except as required for reasonable and customary use in describing the\n      origin of the Work and reproducing the content of the NOTICE file.\n\n   7. Disclaimer of Warranty. Unless required by applicable law or\n      agreed to in writing, Licensor provides the Work (and each\n      Contributor provides its Contributions) on an \"AS IS\" BASIS,\n      WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or\n      implied, including, without limitation, any warranties or conditions\n      of TITLE, NON-INFRINGEMENT, MERCHANTABILITY, or FITNESS FOR A\n      PARTICULAR PURPOSE. You are solely responsible for determining the\n      appropriateness of using or redistributing the Work and assume any\n      risks associated with Your exercise of permissions under this License.\n\n   8. Limitation of Liability. In no event and under no legal theory,\n      whether in tort (including negligence), contract, or otherwise,\n      unless required by applicable law (such as deliberate and grossly\n      negligent acts) or agreed to in writing, shall any Contributor be\n      liable to You for damages, including any direct, indirect, special,\n      incidental, or consequential damages of any character arising as a\n      result of this License or out of the use or inability to use the\n      Work (including but not limited to damages for loss of goodwill,\n      work stoppage, computer failure or malfunction, or any and all\n      other commercial damages or losses), even if such Contributor\n      has been advised of the possibility of such damages.\n\n   9. Accepting Warranty or Additional Liability. While redistributing\n      the Work or Derivative Works thereof, You may choose to offer,\n      and charge a fee for, acceptance of support, warranty, indemnity,\n      or other liability obligations and/or rights consistent with this\n      License. However, in accepting such obligations, You may act only\n      on Your own behalf and on Your sole responsibility, not on behalf\n      of any other Contributor, and only if You agree to indemnify,\n      defend, and hold each Contributor harmless for any liability\n      incurred by, or claims asserted against, such Contributor by reason\n      of your accepting any such warranty or additional liability.\n\n   END OF TERMS AND CONDITIONS\n\n   APPENDIX: How to apply the Apache License to your work.\n\n      To apply the Apache License to your work, attach the following\n      boilerplate notice, with the fields enclosed by brackets \"[]\"\n      replaced with your own identifying information. (Don't include\n      the brackets!)  The text should be enclosed in the appropriate\n      comment syntax for the file format. We also recommend that a\n      file or class name and description of purpose be included on the\n      same \"printed page\" as the copyright notice for easier\n      identification within third-party archives.\n\n   Copyright [yyyy] [name of copyright owner]\n\n   Licensed under the Apache License, Version 2.0 (the \"License\");\n   you may not use this file except in compliance with the License.\n   You may obtain a copy of the License at\n\n       http://www.apache.org/licenses/LICENSE-2.0\n\n   Unless required by applicable law or agreed to in writing, software\n   distributed under the License is distributed on an \"AS IS\" BASIS,\n   WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n   See the License for the specific language governing permissions and\n   limitations under the License.\n"
    },
    "readme": "# Gemma in PyTorch\n\n**Gemma** is a family of lightweight, state-of-the art open models built from research and technology used to create Google Gemini models. They include both text-only and multimodal decoder-only large language models, with open weights, pre-trained variants, and instruction-tuned variants. For more details, please check out the following links:\n\n * [Gemma on Google AI](https://ai.google.dev/gemma)\n * [Gemma on Kaggle](https://www.kaggle.com/models/google/gemma-3)\n * [Gemma on Vertex AI Model Garden](https://pantheon.corp.google.com/vertex-ai/publishers/google/model-garden/gemma3)\n\nThis is the official PyTorch implementation of Gemma models. We provide model and inference implementations using both PyTorch and PyTorch/XLA, and support running inference on CPU, GPU and TPU.\n\n## Updates\n\n * [March 12th, 2025 ðŸ”¥] Support Gemma v3. You can find the checkpoints [on Kaggle](https://www.kaggle.com/models/google/gemma-3/pytorch) and [Hugging Face](https://huggingface.co/models?other=gemma_torch)\n\n * [June 26th, 2024] Support Gemma v2. You can find the checkpoints [on Kaggle](https://www.kaggle.com/models/google/gemma-2/pytorch) and Hugging Face\n\n * [April 9th, 2024] Support CodeGemma. You can find the checkpoints [on Kaggle](https://www.kaggle.com/models/google/codegemma/pytorch) and [Hugging Face](https://huggingface.co/collections/google/codegemma-release-66152ac7b683e2667abdee11)\n\n * [April 5, 2024] Support Gemma v1.1. You can find the v1.1 checkpoints [on Kaggle](https://www.kaggle.com/models/google/gemma/frameworks/pyTorch) and [Hugging Face](https://huggingface.co/collections/google/gemma-release-65d5efbccdbb8c4202ec078b).\n\n## Download Gemma model checkpoint\n\nYou can find the model checkpoints on Kaggle:\n\n- [Gemma 3](https://www.kaggle.com/models/google/gemma-3/pyTorch)\n- [Gemma 2](https://www.kaggle.com/models/google/gemma-2/pyTorch)\n- [Gemma](https://www.kaggle.com/models/google/gemma/pyTorch)\n\nAlternatively, you can find the model checkpoints on the Hugging Face Hub [here](https://huggingface.co/models?other=gemma_torch). To download the models, go the the model repository of the model of interest and click the `Files and versions` tab, and download the model and tokenizer files. For  programmatic downloading, if you have `huggingface_hub` installed, you can also run:\n\n```\nhuggingface-cli download google/gemma-3-4b-it-pytorch\n```\n\nThe following model sizes are available:\n\n- **Gemma 3**: \n  - **Text only**: 1b\n  - **Multimodal**: 4b, 12b, 27b_v3\n- **Gemma 2**: \n  - **Text only**: 2b-v2, 9b, 27b\n- **Gemma**: \n  - **Text only**: 2b, 7b\n\n\nNote that you can choose between the 1B, 4B, 12B, and 27B variants.\n\n```\nVARIANT=<1b, 2b, 2b-v2, 4b, 7b, 9b, 12b, 27b, 27b_v3>\nCKPT_PATH=<Insert ckpt path here>\n```\n\n## Try it free on Colab\n\nFollow the steps at\n[https://ai.google.dev/gemma/docs/pytorch_gemma](https://ai.google.dev/gemma/docs/pytorch_gemma).\n\n## Try it out with PyTorch\n\nPrerequisite: make sure you have setup docker permission properly as a non-root user.\n\n```bash\nsudo usermod -aG docker $USER\nnewgrp docker\n```\n\n### Build the docker image.\n\n```bash\nDOCKER_URI=gemma:${USER}\n\ndocker build -f docker/Dockerfile ./ -t ${DOCKER_URI}\n```\n\n### Run Gemma inference on CPU.\n\n> NOTE: This is a multimodal example. Use a multimodal variant.\n\n```bash\ndocker run -t --rm \\\n    -v ${CKPT_PATH}:/tmp/ckpt \\\n    ${DOCKER_URI} \\\n    python scripts/run_multimodal.py \\\n    --ckpt=/tmp/ckpt \\\n    --variant=\"${VARIANT}\" \\\n    # add `--quant` for the int8 quantized model.\n```\n\n### Run Gemma inference on GPU.\n\n> NOTE: This is a multimodal example. Use a multimodal variant.\n\n```bash\ndocker run -t --rm \\\n    --gpus all \\\n    -v ${CKPT_PATH}:/tmp/ckpt \\\n    ${DOCKER_URI} \\\n    python scripts/run_multimodal.py \\\n    --device=cuda \\\n    --ckpt=/tmp/ckpt \\\n    --variant=\"${VARIANT}\"\n    # add `--quant` for the int8 quantized model.\n```\n\n## Try It out with PyTorch/XLA\n\n### Build the docker image (CPU, TPU).\n\n```bash\nDOCKER_URI=gemma_xla:${USER}\n\ndocker build -f docker/xla.Dockerfile ./ -t ${DOCKER_URI}\n```\n\n### Build the docker image (GPU).\n\n```bash\nDOCKER_URI=gemma_xla_gpu:${USER}\n\ndocker build -f docker/xla_gpu.Dockerfile ./ -t ${DOCKER_URI}\n```\n\n### Run Gemma inference on CPU.\n\n> NOTE: This is a multimodal example. Use a multimodal variant.\n\n```bash\ndocker run -t --rm \\\n    --shm-size 4gb \\\n    -e PJRT_DEVICE=CPU \\\n    -v ${CKPT_PATH}:/tmp/ckpt \\\n    ${DOCKER_URI} \\\n    python scripts/run_xla.py \\\n    --ckpt=/tmp/ckpt \\\n    --variant=\"${VARIANT}\" \\\n    # add `--quant` for the int8 quantized model.\n```\n\n### Run Gemma inference on TPU.\n\nNote: be sure to use the docker container built from `xla.Dockerfile`.\n\n```bash\ndocker run -t --rm \\\n    --shm-size 4gb \\\n    -e PJRT_DEVICE=TPU \\\n    -v ${CKPT_PATH}:/tmp/ckpt \\\n    ${DOCKER_URI} \\\n    python scripts/run_xla.py \\\n    --ckpt=/tmp/ckpt \\\n    --variant=\"${VARIANT}\" \\\n    # add `--quant` for the int8 quantized model.\n```\n\n### Run Gemma inference on GPU.\n\nNote: be sure to use the docker container built from `xla_gpu.Dockerfile`.\n\n```bash\ndocker run -t --rm --privileged \\\n    --shm-size=16g --net=host --gpus all \\\n    -e USE_CUDA=1 \\\n    -e PJRT_DEVICE=CUDA \\\n    -v ${CKPT_PATH}:/tmp/ckpt \\\n    ${DOCKER_URI} \\\n    python scripts/run_xla.py \\\n    --ckpt=/tmp/ckpt \\\n    --variant=\"${VARIANT}\" \\\n    # add `--quant` for the int8 quantized model.\n```\n\n### Tokenizer Notes\n\n99 unused tokens are reserved in the pretrained tokenizer model to assist with more efficient training/fine-tuning. Unused tokens are in the string format of `<unused[0-97]>` with token id range of `[7-104]`. \n\n```\n\"<unused0>\": 7,\n\"<unused1>\": 8,\n\"<unused2>\": 9,\n...\n\"<unused98>\": 104,\n```\n\n## Disclaimer\n\nThis is not an officially supported Google product.\n",
    "py_files": {
        "gemma/__init__.py": "# Copyright 2024 Google LLC\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\n",
        "gemma/config.py": "# Copyright 2024 Google LLC\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\n\"\"\"Gemma model config.\"\"\"\n\nimport dataclasses\nimport enum\nimport os\nimport torch\nfrom typing import Optional, Sequence\nfrom .siglip_vision import config as siglip_vision_config\n\n\n# Keep a mapping from dtype strings to the supported torch dtypes.\n_STR_DTYPE_TO_TORCH_DTYPE = dict({\n    'float16': torch.float16,\n    'float': torch.float32,\n    'float32': torch.float32,\n    'bfloat16': torch.bfloat16,\n})\n\n\nclass AttentionType(enum.Enum):\n    GLOBAL = 1\n    LOCAL_SLIDING = 2\n\n\nclass Architecture(enum.Enum):\n    GEMMA_1 = 1\n    GEMMA_2 = 2\n    GEMMA_3 = 3\n\n\n@dataclasses.dataclass\nclass GemmaConfig:\n    # The architecture of the model.\n    architecture: Architecture = Architecture.GEMMA_1\n    # The number of tokens in the vocabulary.\n    vocab_size: int = 256000\n    # The maximum sequence length that this model might ever be used with.\n    max_position_embeddings: int = 8192\n    # The number of blocks in the model.\n    num_hidden_layers: int = 28\n    # The number of attention heads used in the attention layers of the model.\n    num_attention_heads: int = 16\n    # The number of key-value heads for implementing attention.\n    num_key_value_heads: int = 16\n    # The hidden size of the model.\n    hidden_size: int = 3072\n    # The dimension of the MLP representations.\n    intermediate_size: int = 24576\n    # The number of head dimensions.\n    head_dim: int = 256\n    # The epsilon used by the rms normalization layers.\n    rms_norm_eps: float = 1e-6\n    # The dtype of the weights.\n    dtype: str = 'bfloat16'\n    # Whether a quantized version of the model is used.\n    quant: bool = False\n    # The path to the model tokenizer.\n    tokenizer: Optional[str] = (\n    'tokenizer/tokenizer.model'\n    )\n    # The types of attention used in the layers of the model.\n    attn_types: Optional[Sequence[AttentionType]] = None\n    # The size of the sliding window used for local attention.\n    sliding_window_size: Optional[int] = None\n    # If provided, the final logits are softcapped to this value.\n    final_logit_softcapping: Optional[float] = None\n    # If provided, the attention logits are softcapped to this value.\n    attn_logit_softcapping: Optional[float] = None\n    # If provided, the query vector is normalized using the\n    # inverse square root of this value instead of head_dim.\n    query_pre_attn_scalar: Optional[int] = None\n    # Whether to use pre mlp normalization.\n    use_pre_ffw_norm: bool = False\n    # Whether to use post mlp normalization.\n    use_post_ffw_norm: bool = False\n    # The wave length of the rotary embedding.\n    rope_wave_length: dict[AttentionType, int] | None = None\n    # Whether to use QK normalization in the attention blocks.\n    use_qk_norm: bool = False\n    # Vision model config.\n    vision_config: siglip_vision_config.SiglipVisionModelConfig | None = None\n    # The factor by which the rope wave length is divided for global layers.\n    rope_scaling_factor: int| None = None\n\n    def get_dtype(self) -> Optional[torch.dtype]:\n        \"\"\"Gets the torch dtype from the config dtype string.\"\"\"\n        return _STR_DTYPE_TO_TORCH_DTYPE.get(self.dtype, None)\n\n\ndef get_config_for_7b(dtype: str = 'bfloat16') -> GemmaConfig:\n    return GemmaConfig(dtype=dtype)\n\n\ndef get_config_for_2b(dtype: str = 'bfloat16') -> GemmaConfig:\n    return GemmaConfig(\n        dtype=dtype,\n        num_hidden_layers=18,\n        num_attention_heads=8,\n        num_key_value_heads=1,\n        hidden_size=2048,\n        intermediate_size=16384,\n    )\n\n\ndef get_config_for_2b_v2(dtype: str = 'bfloat16') -> GemmaConfig:\n    return GemmaConfig(\n        dtype=dtype,\n        architecture=Architecture.GEMMA_2,\n        num_hidden_layers=26,\n        num_attention_heads=8,\n        num_key_value_heads=4,\n        hidden_size=2304,\n        intermediate_size=9216,\n        use_pre_ffw_norm=True,\n        use_post_ffw_norm=True,\n        final_logit_softcapping=30.0,\n        attn_logit_softcapping=50.0,\n        head_dim=256,\n        attn_types=[AttentionType.LOCAL_SLIDING, AttentionType.GLOBAL] * 13,\n        sliding_window_size=4096,\n    )\n\n\ndef get_config_for_9b(dtype: str = 'bfloat16') -> GemmaConfig:\n    return GemmaConfig(\n        dtype=dtype,\n        architecture=Architecture.GEMMA_2,\n        num_hidden_layers=42,\n        num_attention_heads=16,\n        num_key_value_heads=8,\n        hidden_size=3584,\n        intermediate_size=14336,\n        use_pre_ffw_norm=True,\n        use_post_ffw_norm=True,\n        final_logit_softcapping=30.0,\n        attn_logit_softcapping=50.0,\n        head_dim=256,\n        attn_types=[AttentionType.LOCAL_SLIDING, AttentionType.GLOBAL] * 21,\n        sliding_window_size=4096,\n    )\n\n\ndef get_config_for_27b(dtype: str = 'bfloat16') -> GemmaConfig:\n  return GemmaConfig(\n      dtype=dtype,\n      architecture=Architecture.GEMMA_2,\n      num_hidden_layers=46,\n      num_attention_heads=32,\n      num_key_value_heads=16,\n      hidden_size=4608,\n      intermediate_size=36864,\n      use_pre_ffw_norm=True,\n      use_post_ffw_norm=True,\n      final_logit_softcapping=30.0,\n      attn_logit_softcapping=50.0,\n      head_dim=128,\n      attn_types=[AttentionType.LOCAL_SLIDING, AttentionType.GLOBAL] * 23,\n      sliding_window_size=4096,\n      query_pre_attn_scalar=144,  # hidden_size / num_attention_heads\n  )\n\n\ndef get_config_for_1b(dtype: str) -> GemmaConfig:\n  return GemmaConfig(\n      dtype=dtype,\n      architecture=Architecture.GEMMA_3,\n      num_hidden_layers=26,\n      num_attention_heads=4,\n      num_key_value_heads=1,\n      hidden_size=1152,\n      intermediate_size=6912,\n      use_pre_ffw_norm=True,\n      use_post_ffw_norm=True,\n      head_dim=256,\n      attn_types=(\n          AttentionType.LOCAL_SLIDING,\n          AttentionType.LOCAL_SLIDING,\n          AttentionType.LOCAL_SLIDING,\n          AttentionType.LOCAL_SLIDING,\n          AttentionType.LOCAL_SLIDING,\n          AttentionType.GLOBAL,\n      ),\n      sliding_window_size=512,\n      rope_wave_length={\n          AttentionType.LOCAL_SLIDING: 10_000,\n          AttentionType.GLOBAL: 1_000_000,\n      },\n      vocab_size=262_144,\n      max_position_embeddings=32_768,\n      tokenizer='tokenizer/gemma3_cleaned_262144_v2.spiece.model',\n      use_qk_norm=True,\n      vision_config=None,\n  )\n\n\ndef get_config_for_4b(dtype: str) -> GemmaConfig:\n  return GemmaConfig(\n      dtype=dtype,\n      architecture=Architecture.GEMMA_3,\n      num_hidden_layers=34,\n      num_attention_heads=8,\n      num_key_value_heads=4,\n      hidden_size=2560,\n      intermediate_size=10240,\n      use_pre_ffw_norm=True,\n      use_post_ffw_norm=True,\n      head_dim=256,\n      attn_types=(\n          AttentionType.LOCAL_SLIDING,\n          AttentionType.LOCAL_SLIDING,\n          AttentionType.LOCAL_SLIDING,\n          AttentionType.LOCAL_SLIDING,\n          AttentionType.LOCAL_SLIDING,\n          AttentionType.GLOBAL,\n      ),\n      sliding_window_size=1024,\n      rope_wave_length={\n          AttentionType.LOCAL_SLIDING: 10_000,\n          AttentionType.GLOBAL: 1_000_000,\n      },\n      vocab_size=262_144,\n      tokenizer='tokenizer/gemma3_cleaned_262144_v2.spiece.model',\n      use_qk_norm=True,\n      vision_config=siglip_vision_config.get_siglip_vision_model_config(),\n      rope_scaling_factor=8,\n  )\n\n\ndef get_config_for_12b(dtype: str) -> GemmaConfig:\n  return GemmaConfig(\n      dtype=dtype,\n      architecture=Architecture.GEMMA_3,\n      num_hidden_layers=48,\n      num_attention_heads=16,\n      num_key_value_heads=8,\n      hidden_size=3840,\n      intermediate_size=3840 * 8 // 2,\n      use_pre_ffw_norm=True,\n      use_post_ffw_norm=True,\n      head_dim=256,\n      attn_types=(\n          AttentionType.LOCAL_SLIDING,\n          AttentionType.LOCAL_SLIDING,\n          AttentionType.LOCAL_SLIDING,\n          AttentionType.LOCAL_SLIDING,\n          AttentionType.LOCAL_SLIDING,\n          AttentionType.GLOBAL,\n      ),\n      sliding_window_size=1024,\n      rope_wave_length={\n          AttentionType.LOCAL_SLIDING: 10_000,\n          AttentionType.GLOBAL: 1_000_000,\n      },\n      vocab_size=262_144,\n      max_position_embeddings=131_072,\n      tokenizer='tokenizer/gemma3_cleaned_262144_v2.spiece.model',\n      use_qk_norm=True,\n      vision_config=siglip_vision_config.get_siglip_vision_model_config(),\n      rope_scaling_factor=8,\n  )\n\n\ndef get_config_for_27b_v3(dtype: str) -> GemmaConfig:\n  return GemmaConfig(\n      dtype=dtype,\n      architecture=Architecture.GEMMA_3,\n      num_hidden_layers=62,\n      num_attention_heads=32,\n      num_key_value_heads=16,\n      hidden_size=5376,\n      intermediate_size=5376 * 8 // 2,\n      use_pre_ffw_norm=True,\n      use_post_ffw_norm=True,\n      head_dim=128,\n      query_pre_attn_scalar=5376 // 32,\n      attn_types=(\n          AttentionType.LOCAL_SLIDING,\n          AttentionType.LOCAL_SLIDING,\n          AttentionType.LOCAL_SLIDING,\n          AttentionType.LOCAL_SLIDING,\n          AttentionType.LOCAL_SLIDING,\n          AttentionType.GLOBAL,\n      ),\n      sliding_window_size=1024,\n      rope_wave_length={\n          AttentionType.LOCAL_SLIDING: 10_000,\n          AttentionType.GLOBAL: 1_000_000,\n      },\n      vocab_size=262_144,\n      max_position_embeddings=131_072,\n      tokenizer='tokenizer/gemma3_cleaned_262144_v2.spiece.model',\n      use_qk_norm=True,\n      vision_config=siglip_vision_config.get_siglip_vision_model_config(),\n      rope_scaling_factor=8,\n  )\n\n\ndef get_model_config(variant: str, dtype: str = 'bfloat16') -> GemmaConfig:\n  \"\"\"Gets the GemmaConfig for the diresired variant and dtype.\"\"\"\n  # Gemma1 variants\n  if variant == '7b':\n    return get_config_for_7b(dtype)\n  elif variant == '2b':\n    return get_config_for_2b(dtype)\n  # Gemma2 variants\n  elif variant == '2b-v2':\n    return get_config_for_2b_v2(dtype)\n  elif variant == '9b':\n    return get_config_for_9b(dtype)\n  elif variant == '27b':\n    return get_config_for_27b(dtype)\n  # Gemma3 variants\n  elif variant == '1b':\n    return get_config_for_1b(dtype)\n  elif variant == '4b':\n    return get_config_for_4b(dtype)\n  elif variant == '12b':\n    return get_config_for_12b(dtype)\n  elif variant == '27b_v3':\n    return get_config_for_27b_v3(dtype)\n  # Invalid variants\n  else:\n    raise ValueError(\n        f'Invalid variant {variant}. Supported variants are \"1b\", \"2b\", '\n        '\"2b-v2\", \"4b\",, \"7b\", \"9b\" \"12b\", \"27b\", and \"27b_v3\".'\n    )\n",
        "gemma/gemma3_model.py": "# Copyright 2024 Google LLC\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\"\"\"Inference-only Gemma 3 multimodal model implementation.\"\"\"\n\nimport torch\nimport os\nimport json\nimport gc\nfrom torch import nn\nfrom PIL import Image\nfrom typing import Any, List, Sequence, Tuple, Union\n\nfrom . import model as gemma_model\nfrom . import config as gemma_config\nfrom . import gemma3_preprocessor\nfrom . import tokenizer\nfrom .siglip_vision import siglip_vision_model\n\nclass Gemma3ForMultimodalLM(nn.Module):\n  \"\"\"Gemma3 model for multimodal causal LM.\"\"\"\n  def __init__(\n        self,\n        config: gemma_config.GemmaConfig,\n    ):\n    super().__init__()\n    self.dtype = config.get_dtype()\n    assert config.architecture == gemma_config.Architecture.GEMMA_3\n    self.config = config\n    max_seq_len = config.max_position_embeddings\n    head_dim = config.head_dim\n    vocab_size = config.vocab_size\n    self.tokenizer = tokenizer.Tokenizer(config.tokenizer)\n    self.text_token_embedder = gemma_model.Embedding(vocab_size, config.hidden_size, config.quant)\n    self.model = gemma_model.GemmaModel(config)\n    self.sampler = gemma_model.Sampler(vocab_size, config)\n\n    if config.vision_config is None:\n      raise ValueError('vision_config must be provided for Gemma3.')\n    self.siglip_vision_model = siglip_vision_model.SiglipVisionModel(config.vision_config)\n    # transformer/embedder/mm_soft_embedding_norm\n    self.mm_soft_embedding_norm = gemma_model.RMSNorm(config.vision_config.embedding_dim,\n                                                           eps = config.rms_norm_eps)\n    # transformer/embedder/mm_input_projection\n    self.mm_input_projection = gemma_model.Linear(config.vision_config.embedding_dim, config.hidden_size, config.quant)\n\n    if config.rope_wave_length is None:\n      raise ValueError('rope_wave_length must be provided for Gemma3.')\n    rope_lengths = config.rope_wave_length\n    defaults = {\n            gemma_config.AttentionType.LOCAL_SLIDING: 10_000,\n            gemma_config.AttentionType.GLOBAL: 10_000,\n        }\n    self._register_freqs_cis('local_freqs_cis', head_dim, max_seq_len, theta=rope_lengths.get(\n              gemma_config.AttentionType.LOCAL_SLIDING, defaults[gemma_config.AttentionType.LOCAL_SLIDING]\n          ))\n    self._register_freqs_cis('global_freqs_cis', head_dim, max_seq_len, theta=rope_lengths.get(\n              gemma_config.AttentionType.GLOBAL, defaults[gemma_config.AttentionType.GLOBAL]\n          ), rope_scaling_factor=config.rope_scaling_factor)\n\n  def _register_freqs_cis(\n        self, name: str, head_dim: int, max_seq_len: int, theta: int = 10_000, rope_scaling_factor: int = 1\n    ):\n    self.register_buffer(\n            name, gemma_model.precompute_freqs_cis(head_dim, max_seq_len * 2, theta=theta, rope_scaling_factor=rope_scaling_factor)\n        )\n\n  @torch.no_grad()\n  def forward(self,\n                input_token_ids: torch.Tensor, # B x L\n                image_patches: torch.Tensor, # B x N x C x H x W (3x896x896)\n                image_presence_mask: torch.Tensor, # B x N\n                input_positions: torch.Tensor,\n                kv_caches: List[Tuple[torch.Tensor, torch.Tensor]],\n                mask: torch.Tensor,\n                output_positions: torch.Tensor,\n                temperatures: Union[torch.Tensor, None],\n                top_ps: torch.Tensor,\n                top_ks: torch.Tensor,\n                local_mask: torch.Tensor | None = None,\n                **kwargs) -> Tuple[torch.Tensor, torch.Tensor]:\n    freqs_cis = {}\n    freqs_cis[gemma_config.AttentionType.LOCAL_SLIDING] = (\n            self.local_freqs_cis.index_select(0, input_positions)\n        )\n    freqs_cis[gemma_config.AttentionType.GLOBAL] = (\n            self.global_freqs_cis.index_select(0, input_positions)\n        )\n    hidden_states = self.text_token_embedder(input_token_ids)\n    normalizer = torch.tensor(self.config.hidden_size**0.5, dtype=hidden_states.dtype, device=hidden_states.device)\n    hidden_states = hidden_states * normalizer\n    if image_patches is not None and self.config.vision_config is not None:\n      # the input has images\n      B, N, C, H, W = image_patches.shape\n      # Flatten and Pass to SiglipVisionModel, and apply SiglipVisionModel Exit\n      flattened_input = image_patches.reshape(B * N, C, H, W)  # (B*N)xCxHxW\n      image_embeddings = self.siglip_vision_model(flattened_input)  # (B*N)xUxD\n      image_embeddings = self.mm_soft_embedding_norm(image_embeddings)  # (B*N) x U x D\n      image_embeddings = self.mm_input_projection(image_embeddings)  # (B*N) x U x model_dim\n      hidden_states = self.populate_image_embeddings(\n          hidden_states.clone(),\n          image_embeddings.clone(),\n          input_token_ids.clone(),\n          image_presence_mask.clone(),\n      )\n\n    kv_write_indices = input_positions\n\n    hidden_states = self.model(\n            hidden_states=hidden_states,\n            freqs_cis=freqs_cis,\n            kv_write_indices=kv_write_indices,\n            kv_caches=kv_caches,\n            mask=mask,\n            local_mask=local_mask,\n        )\n    embedder_weight = self.text_token_embedder.weight\n    if self.config.quant:\n      embedder_weight = (\n                embedder_weight * self.text_token_embedder.weight_scaler.unsqueeze(-1))\n\n    next_tokens, logits = self.sampler(\n            embedding=embedder_weight,\n            hidden_states=hidden_states,\n            output_positions=output_positions,\n            temperatures=temperatures,\n            top_ps=top_ps,\n            top_ks=top_ks,\n        )\n    return next_tokens, logits\n\n  def populate_image_embeddings(self,\n                                hidden_states: torch.Tensor, # B x L x model_dim\n                                image_embeddings: torch.Tensor, # (B*N) x U x model_dim\n                                input_token_ids: torch.Tensor, # B x L\n                                image_presence_mask: torch.Tensor, # B x N\n                                ):\n    batch_size, seq_len, model_dim = hidden_states.shape\n    # Step 1 of 2: Fetch valid image embeddings\n    # flatten indices of valid image embeddings\n    valid_image_embeddings_indices = torch.nonzero(image_presence_mask.flatten(), as_tuple=False).squeeze()\n    # num_valid_images x model_dim\n    valid_image_embeddings = image_embeddings.index_select(0, valid_image_embeddings_indices)\n\n    # Step 2 of 2: Replace image embeddings at right places.\n    image_placeholder_mask = input_token_ids == self.tokenizer.image_token_placeholder_id\n    image_placeholder_indices = image_placeholder_mask.flatten().nonzero(as_tuple=False).squeeze()\n\n    hidden_states = hidden_states.reshape(-1, self.config.hidden_size)\n    hidden_states[image_placeholder_indices] = valid_image_embeddings.reshape(-1, self.config.hidden_size)\n    return hidden_states.reshape(batch_size, seq_len, model_dim).contiguous()\n\n  def create_attention_mask(self, input_ids: torch.Tensor, sequence_length: int):\n    batch_size = input_ids.shape[0]\n    causal_mask = torch.tril(torch.ones((batch_size, 1, sequence_length, sequence_length), dtype=torch.bool, device=input_ids.device))\n    image_token_mask = input_ids == self.tokenizer.image_token_placeholder_id\n    # Pad the mask to the left with 0. This is to make sure the boundary\n    # detection works correctly. Boundary (starting index of image patch) is\n    # detected when the value changes from 0 to 1.\n    padded_mask = nn.functional.pad(image_token_mask, (1, 0), value=0)\n    # Find the boundary (starting index) of the image tokens patch.\n    boundary = padded_mask[:, 1:] > padded_mask[:, :-1]\n    # Number the boundary.\n    # boundary:\n    # [[False, False,  True, False, False],\n    #  [False,  True, False,  True, False]]\n    # numbered_boundary:\n    # [[0, 0, 1, 1, 1],\n    #  [0, 1, 1, 2, 2]]\n    numbered_boundary = torch.cumsum(boundary, dim=-1)\n\n    # image_token_mask:\n    # [[False, False,  True,  True, False],\n    #  [True,  True, False,  True, True]]\n    # numbered_boundary:\n    # [[0, 0, 1, 1, 1],\n    #  [1, 1, 1, 2, 2]]\n    # q_block_indices:\n    # [[0, 0, 1, 1, 0],\n    #  [1, 1, 0, 2, 2]]\n    q_block_indices = image_token_mask * numbered_boundary\n    kv_block_indices = q_block_indices\n    # Test the equality of vertical and horizontal numbered patches\n    # to create the bidirectional mask.\n    bidirectional_mask = torch.logical_and(\n        kv_block_indices[:, None, :] == q_block_indices.unsqueeze(-1),\n        q_block_indices.unsqueeze(-1) > 0,\n    )\n    attention_mask = torch.logical_or(causal_mask, bidirectional_mask.unsqueeze(1))\n    # The upper triangular matrix's diagonal is shifted by sliding window size\n    # before doing logical 'and' with attention mask. This is to make sure the\n    # local attention is within the sliding window.\n    local_mask = torch.logical_and(\n            attention_mask,\n            torch.triu(torch.ones((1, 1, sequence_length, sequence_length), dtype=torch.bool, device=input_ids.device), diagonal=-(self.config.sliding_window_size-1))\n        )\n    return attention_mask, local_mask\n\n  def generate(\n        self,\n        prompts: Sequence[Sequence[Union[str, Image.Image]]],\n        device: Any,\n        output_len: int = 100,\n        temperature: Union[float, None] = 1.0,\n        top_p: float = 0.95,\n        top_k: int = 64,\n    ) -> Sequence[str]:\n    \"\"\"Generates responses for given prompts using Gemma model.\"\"\"\n    # Inference only.\n    processing_result = gemma3_preprocessor.tokenize_raw_input(\n            self.tokenizer, prompts, self.config, output_len, device\n        )\n    batch_size = processing_result[\"batch_size\"]\n    user_input_token_ids = processing_result[\"user_input_token_ids\"]\n    image_batch = processing_result[\"image_batch\"]\n    min_prompt_len = processing_result[\"min_prompt_len\"]\n    max_prompt_len = processing_result[\"max_prompt_len\"]\n    total_seq_len = processing_result[\"max_seq_len\"]\n    image_presence_mask = processing_result[\"image_presence_mask\"]\n\n    # Create attention mask.\n    min_dtype = torch.finfo(self.dtype).min\n    if self.config.sliding_window_size is None:\n      raise ValueError('gemma 3 model requires sliding_window size')\n    boolean_mask, local_boolean_mask = self.create_attention_mask(user_input_token_ids, total_seq_len)\n    mask_tensor = torch.where(boolean_mask, 0, torch.tensor(min_dtype, dtype=torch.float32, device=device)).contiguous()\n    local_mask_tensor = torch.where(local_boolean_mask, 0, torch.tensor(min_dtype, dtype=torch.float32, device=device)).contiguous()\n\n    kv_caches = []\n    for _ in range(self.config.num_hidden_layers):\n      size = (batch_size, total_seq_len, self.config.num_key_value_heads,\n                    self.config.head_dim)\n      dtype = self.config.get_dtype()\n      k_cache = torch.zeros(size=size, dtype=dtype, device=device)\n      v_cache = torch.zeros(size=size, dtype=dtype, device=device)\n      kv_caches.append((k_cache, v_cache))\n\n    input_token_ids_tensor = torch.full((batch_size, min_prompt_len),\n                                            self.tokenizer.pad_id,\n                                            dtype=torch.int64, device=device)\n    token_ids_tensor = user_input_token_ids.to(device)\n    for i in range(batch_size):\n      p = user_input_token_ids[i]\n      input_token_ids_tensor[i, :min_prompt_len] = p[:min_prompt_len]\n\n    input_positions_tensor = torch.arange(0, min_prompt_len, dtype=torch.int64, device=device)\n    prompt_mask_tensor = token_ids_tensor != self.tokenizer.pad_id\n    curr_mask_tensor = mask_tensor.index_select(2, input_positions_tensor)\n    curr_local_mask_tensor = local_mask_tensor.index_select(2, input_positions_tensor)\n    output_positions_tensor = torch.LongTensor([min_prompt_len - 1]).to(device)\n    temperatures_tensor = None if not temperature else torch.FloatTensor(\n            [temperature] * batch_size).to(device)\n    top_ps_tensor = torch.FloatTensor([top_p] * batch_size).to(device)\n    top_ks_tensor = torch.LongTensor([top_k] * batch_size).to(device)\n    output_index = torch.tensor(min_prompt_len, dtype=torch.int64, device=device)\n\n    # Prefill up to min_prompt_len tokens, then treat other prefill as\n    # decode and ignore output.\n    for i in range(total_seq_len - min_prompt_len):\n      next_token_ids, _ = self(\n                input_token_ids=input_token_ids_tensor,\n                image_patches=image_batch,\n                image_presence_mask=image_presence_mask,\n                input_positions=input_positions_tensor,\n                kv_caches=kv_caches,\n                mask=curr_mask_tensor,\n                output_positions=output_positions_tensor,\n                temperatures=temperatures_tensor,\n                top_ps=top_ps_tensor,\n                top_ks=top_ks_tensor,\n                local_mask=curr_local_mask_tensor,\n            )\n      curr_prompt_mask = prompt_mask_tensor.index_select(\n                1, output_index).squeeze(dim=1)\n      curr_token_ids = token_ids_tensor.index_select(\n                1, output_index).squeeze(dim=1)\n      output_token_ids = torch.where(curr_prompt_mask, curr_token_ids,\n                                           next_token_ids).unsqueeze(dim=1)\n      token_ids_tensor.index_copy_(1, output_index, output_token_ids)\n\n      input_token_ids_tensor = output_token_ids\n      input_positions_tensor = output_index.unsqueeze(dim=-1)\n      curr_mask_tensor = mask_tensor.index_select(2,\n                                                        input_positions_tensor)\n      curr_local_mask_tensor = local_mask_tensor.index_select(\n                2, input_positions_tensor\n            ) if local_mask_tensor is not None else None\n      output_positions_tensor = torch.tensor(0, dtype=torch.int64, device=device)\n      output_index = output_index + 1\n      image_batch = None\n      image_presence_mask = None\n\n    # Detokenization.\n    token_ids = token_ids_tensor.tolist()\n    results = []\n    for i, tokens in enumerate(token_ids):\n      output = tokens\n      if self.tokenizer.eos_id in output:\n        eos_index = output.index(self.tokenizer.eos_id)\n        output = output[:eos_index]\n      results.append(self.tokenizer.decode(output))\n\n    return results\n\n  def load_weights(self, model_path: str):\n    if os.path.isfile(model_path):\n      self.load_state_dict(\n                torch.load(\n                    model_path, mmap=True, weights_only=True,\n                )['model_state_dict'],\n                strict=False,\n            )\n    else:\n      index_path = os.path.join(model_path, 'pytorch_model.bin.index.json')\n      with open(index_path, \"r\", encoding=\"utf-8\") as f:\n        index = json.load(f)\n      shard_files = list(set(index[\"weight_map\"].values()))\n      for shard_file in shard_files:\n        shard_path = os.path.join(model_path, shard_file)\n        state_dict = torch.load(shard_path, map_location=\"cpu\", weights_only=True)\n        self.load_state_dict(state_dict, strict=False)\n        del state_dict  # Save memory.\n        gc.collect()\n",
        "gemma/gemma3_preprocessor.py": "# Copyright 2024 Google LLC\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\"\"\"Preprocessor for Gemma3 input.\"\"\"\nimport token\n\nfrom typing import Union, Any, Sequence\n\nimport torch\nfrom absl import app\nfrom PIL import Image\nfrom .siglip_vision import preprocessor as siglip_vision_preprocessor\nfrom .siglip_vision import pan_and_scan\nfrom . import tokenizer\nfrom . import config as gemma_config\n\nCROPPED_IMAGE_PREFIX = \"here is the original image\"\nCROPPED_IMAGE_FILLER = \"and here are some crops to help you see better\"\n\n\ndef gemma3_input_preprocessor(\n    raw_user_prompt: Sequence[Union[Image.Image, str]],\n) -> Sequence[Union[torch.Tensor, str]]:\n  \"\"\"Preprocessor for Gemma3 input.\n\n  Args:\n    raw_user_prompt: A list of images or strings, as provided by the user.\n\n  Returns:\n    A list of preprocessed images or strings.\n  \"\"\"\n  preprocessed_input: list[Union[torch.Tensor, str]] = []\n  for element in raw_user_prompt:\n    if isinstance(element, Image.Image):\n      cropped_images = pan_and_scan.pan_and_scan(element)\n      preprocessed_images_cropped = siglip_vision_preprocessor.preprocess_images_for_siglip_vision(cropped_images)\n      preprocessed_images_uncropped = siglip_vision_preprocessor.preprocess_images_for_siglip_vision([element])\n      if len(preprocessed_images_cropped) == 1:\n        preprocessed_input.append(preprocessed_images_uncropped[0])\n      elif len(preprocessed_images_cropped) > 1:\n        preprocessed_input.append(CROPPED_IMAGE_PREFIX)\n        preprocessed_input.append(preprocessed_images_uncropped[0])\n        preprocessed_input.append(CROPPED_IMAGE_FILLER)\n        preprocessed_input.extend(preprocessed_images_cropped)\n      else:\n        raise ValueError(\"No images found in the input.\")\n    else:\n      preprocessed_input.append(element)\n\n  return preprocessed_input\n\n\ndef gemma3_batch_input_preprocessor(raw_input: Sequence[Sequence[Union[Image.Image, str]]]):\n    \"\"\"Preprocessor for Gemma3 batch input.\n    \"\"\"\n    preprocessed_input: list[Sequence[Union[torch.Tensor, str]]] = []\n    for element in raw_input:\n      preprocessed_input.append(gemma3_input_preprocessor(element))\n    return preprocessed_input\n\n\ndef tokenize_raw_input(\n        tokenizer_obj: tokenizer.Tokenizer,\n        raw_input: Sequence[Sequence[Union[str, Image.Image]]],\n        config: gemma_config.GemmaConfig,\n        output_len: int,\n        device: Any,\n    ) -> dict[str, Any]:\n    \"\"\"\n        Converts a preprocessed batch of interleaved text and image inputs into\n        token IDs and an image batch suitable for gemma3 model.\n\n        Args:\n            preprocessed_batch: List of lists containing strings and torch.Tensor images.\n            image_token_id: Token ID to represent image placeholders.\n            max_image_tokens: Number of tokens reserved for each image.\n            image_size: Expected size of images (C, H, W).\n\n        Returns:\n            user_input_token_ids: Batch of token IDs with shape (B, L), where L is the max sequence length.\n            image_batch: Batch of images with shape (B, N, C, H, W), where N is the max number of images.\n    \"\"\"\n    if config.vision_config is None:\n        raise ValueError('vision_config must be provided for Gemma3.')\n\n    preprocessed_batch = gemma3_batch_input_preprocessor(raw_input)\n\n    # Initialize lists to store token IDs and image tensors\n    all_token_ids = []\n    all_images = []\n    prompt_lengths = []\n\n    max_prompt_len = 0\n    min_prompt_len = float(\"inf\")\n    max_num_images = 0\n    # Iterate over each user prompt in the batch\n    for prompt in preprocessed_batch:\n        token_ids = []\n        images = []\n        token_ids.append(tokenizer_obj.bos_id)\n        # Process each element in the prompt\n        for element in prompt:\n            if isinstance(element, str):\n                # Tokenize text and add to token_ids\n                tokens = tokenizer_obj.encode(element, bos=False, eos=False)\n                token_ids.extend(tokens)\n            elif isinstance(element, torch.Tensor):\n                # Prepend (dual endline + tokenizer_obj.boi)\n                token_ids.extend(tokenizer_obj.encode(\"\\n\\n\", bos=False, eos=False))\n                token_ids.append(tokenizer_obj.boi_id)\n                # Add image placeholder tokens\n                token_ids.extend(\n                            [tokenizer_obj.image_token_placeholder_id]\n                            * config.vision_config.encoding_sequence_length\n                        )\n                # Append (tokenizer_obj.eoi + dual endline)\n                token_ids.append(tokenizer_obj.eoi_id)\n                token_ids.extend(tokenizer_obj.encode(\"\\n\\n\", bos=False, eos=False))\n                # Store the image tensor\n                images.append(element)\n            else:\n                raise ValueError(\n                            \"Unsupported type in prompt. Expected str or torch.Tensor.\"\n                        )\n        curr_prompt_len = len(token_ids)\n        prompt_lengths.append(curr_prompt_len)\n\n        max_prompt_len = max(max_prompt_len, curr_prompt_len)\n        min_prompt_len = min(min_prompt_len, curr_prompt_len)\n        max_num_images = max(max_num_images, len(images))\n\n        all_token_ids.append(token_ids)\n        all_images.append(images)\n\n    max_seq_len = max_prompt_len + output_len\n\n    # Pad token IDs to the maximum sequence length\n    user_input_token_ids = []\n    for token_ids in all_token_ids:\n        pad_length = max_seq_len - len(token_ids)\n        padded_token_ids = token_ids + [tokenizer_obj.pad_id] * pad_length\n        user_input_token_ids.append(padded_token_ids)\n\n    # Pad images to the maximum number of images in the batch\n    image_batch = []\n    image_presence_mask = []\n    for images in all_images:\n        # Check if all images within the current sublist have the same shape\n        if images:  # Check if the sublist is not empty\n            first_shape = images[0].shape\n            for img in images:\n                assert img.shape == first_shape, \"Images within a sublist must have the same shape.\"\n        pad_length = max_num_images - len(images)\n        padded_images = images.copy() #create a copy so the original data is not altered.\n        presence_mask = [True] * len(images)\n\n        if pad_length > 0:\n            # Create a list of zero tensors for padding\n            padding = [\n                torch.zeros(\n                    (\n                        config.vision_config.input_channels,\n                        config.vision_config.image_size,\n                        config.vision_config.image_size,\n                    ), device=device\n                )\n                for _ in range(pad_length)\n            ]\n            padded_images.extend(padding)\n            presence_mask.extend([False] * pad_length)\n        image_batch.append(padded_images)\n        image_presence_mask.append(presence_mask)\n\n    # Convert lists to tensors\n    user_input_token_ids = torch.tensor(user_input_token_ids, dtype=torch.long, device=device)\n    if max_num_images > 0:\n        image_batch = torch.stack([torch.stack(images) for images in image_batch]).to(\n            device\n        )\n        image_presence_mask = torch.tensor(image_presence_mask, dtype=torch.bool, device=device)\n    else:\n        image_batch = None\n        image_presence_mask = None\n\n    # Prepare the output dictionary\n    output_dict = {\n            \"user_input_token_ids\": user_input_token_ids,\n            \"image_batch\": image_batch,\n            \"batch_size\": len(preprocessed_batch),\n            \"min_prompt_len\": min_prompt_len,\n            \"max_prompt_len\": max_prompt_len,\n            \"max_seq_len\": max_seq_len,\n            \"image_presence_mask\": image_presence_mask,\n        }\n\n    return output_dict\n",
        "gemma/model.py": "# Copyright 2024 Google LLC\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\"\"\"Inference-only Gemma model implementation.\"\"\"\n\nimport json\nimport gc\nimport os\nimport torch\nfrom torch import nn\nimport torch.nn.functional as F\nfrom typing import Any, List, Optional, Sequence, Tuple, Union, Mapping\n\nfrom gemma import config as gemma_config\nfrom gemma import tokenizer\n\n\nclass Sampler(nn.Module):\n\n    def __init__(self, vocab_size: int, config: gemma_config.GemmaConfig):\n        super().__init__()\n        self.vocab_size = vocab_size\n        self.config = config\n\n    @torch.no_grad()\n    def forward(\n        self,\n        embedding: torch.Tensor,\n        hidden_states: torch.Tensor,\n        output_positions: torch.Tensor,\n        temperatures: Union[torch.Tensor, None],\n        top_ps: torch.Tensor,\n        top_ks: torch.Tensor,\n        embedding_bias: Optional[torch.Tensor] = None,\n    ) -> Tuple[torch.Tensor, torch.Tensor]:\n        # Select the last element for each sequence.\n        # (batch_size, input_len, hidden_size) -> (batch_size, hidden_size)\n        hidden_states = hidden_states.index_select(\n            1, output_positions).squeeze(dim=1)\n        logits = torch.matmul(hidden_states, embedding.t())\n        if embedding_bias is not None:\n            logits += embedding_bias\n        if self.config.final_logit_softcapping is not None:\n            logits = logits / self.config.final_logit_softcapping\n            logits = torch.tanh(logits)\n            logits = logits * self.config.final_logit_softcapping\n\n        if temperatures is None:\n            return torch.argmax(logits, dim=-1).squeeze(dim=-1), logits\n\n        # Apply temperature scaling.\n        logits.div_(temperatures.unsqueeze(dim=1))\n\n        # Calculate probabilities with softmax.\n        probs = torch.softmax(logits, dim=-1, dtype=torch.float)\n        probs_sort, probs_idx = torch.sort(probs, dim=-1, descending=True)\n\n        # Apply top-p, top-k.\n        probs_sum = torch.cumsum(probs_sort, dim=-1)\n        top_ps_mask = (probs_sum - probs_sort) > top_ps.unsqueeze(dim=1)\n        probs_sort = torch.where(top_ps_mask, 0, probs_sort)\n\n        top_ks_mask = torch.arange(probs_idx.shape[-1],\n                                   device=probs_idx.device)\n        top_ks_mask = top_ks_mask.expand(probs_idx.shape[0], -1)\n        top_ks_mask = top_ks_mask >= top_ks.unsqueeze(dim=1)\n        probs_sort = torch.where(top_ks_mask, 0, probs_sort)\n\n        # Re-normalization.\n        probs_sort.div_(probs_sort.sum(dim=-1, keepdim=True))\n        probs = torch.gather(probs_sort,\n                             dim=-1,\n                             index=torch.argsort(probs_idx, dim=-1))\n\n        next_token_ids = torch.multinomial(probs,\n                                           num_samples=1,\n                                           replacement=True).squeeze(dim=-1)\n        return next_token_ids, logits\n\n\ndef precompute_freqs_cis(dim: int,\n                         end: int,\n                         theta: float = 10000.0,\n                         rope_scaling_factor:int = 1) -> torch.Tensor:\n    \"\"\"Precomputes the frequency cis.\"\"\"\n    freqs = 1.0 / (theta**(torch.arange(0, dim, 2)[:(dim // 2)].float() / dim))\n    freqs = freqs/rope_scaling_factor\n    t = torch.arange(end, device=freqs.device)\n    freqs = torch.outer(t, freqs).float()\n    freqs_cis = torch.polar(torch.ones_like(freqs), freqs)  # complex64\n    return freqs_cis\n\n\ndef apply_rotary_emb(x: torch.Tensor, freqs_cis: torch.Tensor) -> torch.Tensor:\n    \"\"\"Applies the rotary embedding to the query and key tensors.\"\"\"\n    x_ = torch.view_as_complex(\n        torch.stack(torch.chunk(x.transpose(1, 2).float(), 2, dim=-1),\n                    dim=-1))\n    x_out = torch.view_as_real(x_ * freqs_cis).type_as(x)\n    x_out = torch.cat(torch.chunk(x_out, 2, dim=-1), dim=-2)\n    x_out = x_out.reshape(x_out.shape[0], x_out.shape[1], x_out.shape[2],\n                          -1).transpose(1, 2)\n    return x_out\n\n\nclass Linear(nn.Module):\n\n    def __init__(self, in_features: int, out_features: int, quant: bool):\n        super().__init__()\n        if quant:\n            self.weight = nn.Parameter(\n                torch.empty((out_features, in_features), dtype=torch.int8),\n                requires_grad=False,\n            )\n            self.weight_scaler = nn.Parameter(torch.Tensor(out_features))\n        else:\n            self.weight = nn.Parameter(\n                torch.empty((out_features, in_features)),\n                requires_grad=False,\n            )\n        self.quant = quant\n\n    def forward(self, x):\n        weight = self.weight\n        if self.quant:\n            weight = weight * self.weight_scaler.unsqueeze(-1)\n        output = F.linear(x, weight)\n        return output\n\n\nclass Embedding(nn.Module):\n\n    def __init__(self, num_embeddings: int, embedding_dim: int, quant: bool):\n        super().__init__()\n        if quant:\n            self.weight = nn.Parameter(\n                torch.empty((num_embeddings, embedding_dim), dtype=torch.int8),\n                requires_grad=False,\n            )\n            self.weight_scaler = nn.Parameter(torch.Tensor(num_embeddings))\n        else:\n            self.weight = nn.Parameter(\n                torch.empty((num_embeddings, embedding_dim)),\n                requires_grad=False,\n            )\n        self.quant = quant\n\n    def forward(self, x):\n        weight = self.weight\n        if self.quant:\n            weight = weight * self.weight_scaler.unsqueeze(-1)\n        output = F.embedding(x, weight)\n        return output\n\n\nclass RMSNorm(torch.nn.Module):\n\n    def __init__(\n        self,\n        dim: int,\n        eps: float = 1e-6,\n        add_unit_offset: bool = True,\n    ):\n        super().__init__()\n        self.eps = eps\n        self.add_unit_offset = add_unit_offset\n        self.weight = nn.Parameter(torch.zeros(dim))\n\n    def _norm(self, x):\n        return x * torch.rsqrt(x.pow(2).mean(-1, keepdim=True) + self.eps)\n\n    def forward(self, x):\n        # Llama does x.to(float16) * w whilst Gemma2 is (x * w).to(float16)\n        # See https://github.com/huggingface/transformers/pull/29402\n        output = self._norm(x.float())\n        if self.add_unit_offset:\n            output = output * (1 + self.weight.float())\n        else:\n            output = output * self.weight.float()\n        return output.type_as(x)\n\n\nclass GemmaMLP(nn.Module):\n\n    def __init__(\n        self,\n        hidden_size: int,\n        intermediate_size: int,\n        quant: bool,\n    ):\n        super().__init__()\n        self.gate_proj = Linear(hidden_size, intermediate_size, quant)\n        self.up_proj = Linear(hidden_size, intermediate_size, quant)\n        self.down_proj = Linear(intermediate_size, hidden_size, quant)\n\n    def forward(self, x):\n        gate = self.gate_proj(x)\n        gate = F.gelu(gate, approximate=\"tanh\")\n        up = self.up_proj(x)\n        fuse = gate * up\n        outputs = self.down_proj(fuse)\n        return outputs\n\n\nclass GemmaAttention(nn.Module):\n\n    def __init__(\n        self,\n        config: gemma_config.GemmaConfig,\n        attn_type: gemma_config.AttentionType,\n    ):\n        super().__init__()\n\n        self.num_heads = config.num_attention_heads\n        self.num_kv_heads = config.num_key_value_heads\n\n        assert self.num_heads % self.num_kv_heads == 0\n        self.num_queries_per_kv = self.num_heads // self.num_kv_heads\n\n        self.hidden_size = config.hidden_size\n        self.head_dim = config.head_dim\n\n        self.q_size = self.num_heads * self.head_dim\n        self.kv_size = self.num_kv_heads * self.head_dim\n\n        if config.query_pre_attn_scalar is not None:\n            self.scaling = config.query_pre_attn_scalar**-0.5\n        else:\n            self.scaling = self.head_dim**-0.5\n\n        self.qkv_proj = Linear(\n            self.hidden_size,\n            (self.num_heads + 2 * self.num_kv_heads) * self.head_dim,\n            quant=config.quant)\n        self.o_proj = Linear(\n            self.num_heads * self.head_dim, self.hidden_size, quant=config.quant\n        )\n        self.query_norm = (\n            RMSNorm(self.head_dim, eps=config.rms_norm_eps)\n            if config.use_qk_norm\n            else None\n        )\n        self.key_norm = (\n            RMSNorm(self.head_dim, eps=config.rms_norm_eps)\n            if config.use_qk_norm\n            else None\n        )\n\n        self.attn_type = attn_type\n        self.sliding_window_size = config.sliding_window_size\n        self.attn_logit_softcapping = config.attn_logit_softcapping\n\n    def forward(\n        self,\n        hidden_states: torch.Tensor,\n        freqs_cis: torch.Tensor,\n        kv_write_indices: torch.Tensor,\n        kv_cache: Tuple[torch.Tensor, torch.Tensor],\n        mask: torch.Tensor,\n        local_mask: torch.Tensor = None,\n    ) -> torch.Tensor:\n        hidden_states_shape = hidden_states.shape\n        assert len(hidden_states_shape) == 3\n\n        batch_size, input_len, _ = hidden_states_shape\n\n        qkv = self.qkv_proj(hidden_states)\n        xq, xk, xv = qkv.split([self.q_size, self.kv_size, self.kv_size],\n                               dim=-1)\n\n        xq = xq.view(batch_size, -1, self.num_heads, self.head_dim)\n        xk = xk.view(batch_size, -1, self.num_kv_heads, self.head_dim)\n        xv = xv.view(batch_size, -1, self.num_kv_heads, self.head_dim)\n\n        if self.query_norm is not None and self.key_norm is not None:\n            xq = self.query_norm(xq)\n            xk = self.key_norm(xk)\n\n        # Positional embedding.\n        xq = apply_rotary_emb(xq, freqs_cis=freqs_cis)\n        xk = apply_rotary_emb(xk, freqs_cis=freqs_cis)\n\n        # Write new kv cache.\n        # [batch_size, input_len, n_local_kv_heads, head_dim]\n        k_cache, v_cache = kv_cache\n        k_cache.index_copy_(1, kv_write_indices, xk)\n        v_cache.index_copy_(1, kv_write_indices, xv)\n\n        key = k_cache\n        value = v_cache\n        if self.num_kv_heads != self.num_heads:\n            # [batch_size, max_seq_len, n_local_heads, head_dim]\n            key = torch.repeat_interleave(key, self.num_queries_per_kv, dim=2)\n            value = torch.repeat_interleave(value,\n                                            self.num_queries_per_kv,\n                                            dim=2)\n\n        # [batch_size, n_local_heads, input_len, head_dim]\n        q = xq.transpose(1, 2)\n        # [batch_size, n_local_heads, max_seq_len, head_dim]\n        k = key.transpose(1, 2)\n        v = value.transpose(1, 2)\n\n        # [batch_size, n_local_heads, input_len, max_seq_len]\n        q.mul_(self.scaling)\n        scores = torch.matmul(q, k.transpose(2, 3))\n        if (\n            self.attn_type == gemma_config.AttentionType.LOCAL_SLIDING\n            and self.sliding_window_size is not None\n            and local_mask is not None\n        ):\n            mask = local_mask\n\n        if self.attn_logit_softcapping is not None:\n            scores = scores / self.attn_logit_softcapping\n            scores = torch.tanh(scores)\n            scores = scores * self.attn_logit_softcapping\n\n        scores = scores + mask\n        scores = F.softmax(scores.float(), dim=-1).type_as(q)\n\n        # [batch_size, n_local_heads, input_len, head_dim]\n        output = torch.matmul(scores, v)\n\n        # [batch_size, input_len, hidden_dim]\n        output = (output.transpose(1, 2).contiguous().view(\n            batch_size, input_len, -1))\n        output = self.o_proj(output)\n        return output\n\n\nclass GemmaDecoderLayer(nn.Module):\n\n    def __init__(\n        self,\n        config: gemma_config.GemmaConfig,\n    ):\n        super().__init__()\n        self.attn_type = gemma_config.AttentionType.GLOBAL\n        self.self_attn = GemmaAttention(\n            config=config,\n            attn_type=self.attn_type)\n        self.mlp = GemmaMLP(\n            hidden_size=config.hidden_size,\n            intermediate_size=config.intermediate_size,\n            quant=config.quant,\n        )\n        self.input_layernorm = RMSNorm(config.hidden_size,\n                                       eps=config.rms_norm_eps)\n        self.post_attention_layernorm = RMSNorm(config.hidden_size,\n                                                eps=config.rms_norm_eps)\n\n    # TODO(imayank): Decouple Gemma versions into separate files.\n    def forward(\n        self,\n        hidden_states: torch.Tensor,\n        freqs_cis: torch.Tensor,\n        kv_write_indices: torch.Tensor,\n        kv_cache: Tuple[torch.Tensor, torch.Tensor],\n        mask: torch.Tensor,\n        local_mask: torch.Tensor,\n    ) -> torch.Tensor:\n        # Self Attention\n        residual = hidden_states\n        hidden_states = self.input_layernorm(hidden_states)\n        hidden_states = self.self_attn(\n            hidden_states=hidden_states,\n            freqs_cis=freqs_cis,\n            kv_write_indices=kv_write_indices,\n            kv_cache=kv_cache,\n            mask=mask,\n        )\n        hidden_states = residual + hidden_states\n\n        # MLP\n        residual = hidden_states\n        hidden_states = self.post_attention_layernorm(hidden_states)\n        hidden_states = self.mlp(hidden_states)\n        hidden_states = residual + hidden_states\n\n        return hidden_states\n\n\nclass Gemma2DecoderLayer(nn.Module):\n    def __init__(\n        self,\n        config: gemma_config.GemmaConfig,\n        attn_type: gemma_config.AttentionType,\n    ):\n        super().__init__()\n        self.attn_type = attn_type\n        self.self_attn = GemmaAttention(\n            config=config,\n            attn_type=self.attn_type,\n        )\n        self.mlp = GemmaMLP(\n            hidden_size=config.hidden_size,\n            intermediate_size=config.intermediate_size,\n            quant=config.quant,\n        )\n        self.input_layernorm = RMSNorm(config.hidden_size,\n                                       eps=config.rms_norm_eps)\n        self.post_attention_layernorm = RMSNorm(config.hidden_size,\n                                                eps=config.rms_norm_eps)\n        self.pre_feedforward_layernorm = (\n            RMSNorm(config.hidden_size, eps=config.rms_norm_eps)\n            if config.use_pre_ffw_norm\n            else None\n        )\n        self.post_feedforward_layernorm = (\n            RMSNorm(config.hidden_size, eps=config.rms_norm_eps)\n            if config.use_post_ffw_norm\n            else None\n        )\n\n    def forward(\n        self,\n        hidden_states: torch.Tensor,\n        freqs_cis: torch.Tensor,\n        kv_write_indices: torch.Tensor,\n        kv_cache: Tuple[torch.Tensor, torch.Tensor],\n        mask: torch.Tensor,\n        local_mask: torch.Tensor,\n    ) -> torch.Tensor:\n        # Self Attention\n        residual = hidden_states\n        hidden_states = self.input_layernorm(hidden_states)\n        hidden_states = self.self_attn(\n            hidden_states=hidden_states,\n            freqs_cis=freqs_cis,\n            kv_write_indices=kv_write_indices,\n            kv_cache=kv_cache,\n            mask=mask,\n            local_mask=local_mask,\n        )\n        hidden_states = self.post_attention_layernorm(hidden_states)\n        hidden_states = residual + hidden_states\n\n        # MLP\n        residual = hidden_states\n        if self.pre_feedforward_layernorm is not None:\n            hidden_states = self.pre_feedforward_layernorm(hidden_states)\n        hidden_states = self.mlp(hidden_states)\n        if self.post_feedforward_layernorm is not None:\n            hidden_states = self.post_feedforward_layernorm(hidden_states)\n        hidden_states = residual + hidden_states\n\n        return hidden_states\n\n\nclass GemmaModel(nn.Module):\n\n    def __init__(self, config: gemma_config.GemmaConfig):\n        super().__init__()\n        self.config = config\n        self.vocab_size = config.vocab_size\n\n        self.layers = nn.ModuleList()\n        for i in range(config.num_hidden_layers):\n            if config.architecture == gemma_config.Architecture.GEMMA_1:\n                self.layers.append(GemmaDecoderLayer(config))\n            elif config.architecture in (\n                gemma_config.Architecture.GEMMA_2,\n                gemma_config.Architecture.GEMMA_3,\n            ):\n                attn_type = (\n                    config.attn_types[i % len(config.attn_types)]\n                    if config.attn_types is not None\n                    else gemma_config.AttentionType.GLOBAL\n                )\n                self.layers.append(Gemma2DecoderLayer(config, attn_type))\n            else:\n                raise ValueError(f'Unknown architecture: {config.architecture}')\n        self.norm = RMSNorm(config.hidden_size, eps=config.rms_norm_eps)\n\n    def forward(\n        self,\n        hidden_states: torch.Tensor,\n        freqs_cis: Mapping[gemma_config.AttentionType, torch.Tensor],\n        kv_write_indices: torch.Tensor,\n        kv_caches: List[Tuple[torch.Tensor, torch.Tensor]],\n        mask: torch.Tensor,\n        local_mask: torch.Tensor,\n    ) -> torch.Tensor:\n        for i in range(len(self.layers)):\n            layer = self.layers[i]\n            hidden_states = layer(\n                hidden_states=hidden_states,\n                freqs_cis=freqs_cis.get(layer.attn_type),\n                kv_write_indices=kv_write_indices,\n                kv_cache=kv_caches[i],\n                mask=mask,\n                local_mask=local_mask,\n            )\n        hidden_states = self.norm(hidden_states)\n        return hidden_states\n\n\nclass GemmaForCausalLM(nn.Module):\n\n  def __init__(\n        self,\n        config: gemma_config.GemmaConfig,\n    ):\n    super().__init__()\n    self.config = config\n    assert config.hidden_size % config.num_attention_heads == 0\n\n    max_seq_len = config.max_position_embeddings\n    head_dim = config.head_dim\n    vocab_size = config.vocab_size\n\n    self.tokenizer = tokenizer.Tokenizer(config.tokenizer)\n    self.embedder = Embedding(vocab_size, config.hidden_size, config.quant)\n    self.model = GemmaModel(config)\n    self.sampler = Sampler(vocab_size, config)\n\n    # Pre-compute rotary embedding table.\n    if config.architecture == gemma_config.Architecture.GEMMA_3:\n      if config.rope_wave_length is None:\n        raise ValueError('rope_wave_length must be provided for Gemma3.')\n\n      rope_lengths = config.rope_wave_length\n      defaults = {\n                gemma_config.AttentionType.LOCAL_SLIDING: 10_000,\n                gemma_config.AttentionType.GLOBAL: 10_000,\n            }\n\n      for attn_type, name in [\n                (gemma_config.AttentionType.LOCAL_SLIDING, 'local_freqs_cis'),\n                (gemma_config.AttentionType.GLOBAL, 'global_freqs_cis'),\n            ]:\n        theta = rope_lengths.get(\n                    attn_type, defaults[attn_type]\n                )\n        self._register_freqs_cis(name, head_dim, max_seq_len, theta=theta)\n\n    else:\n      self._register_freqs_cis('freqs_cis', head_dim, max_seq_len)\n\n  def _register_freqs_cis(\n        self, name: str, head_dim: int, max_seq_len: int, theta: int = 10_000\n    ):\n    self.register_buffer(\n            name, precompute_freqs_cis(head_dim, max_seq_len * 2, theta=theta)\n        )\n\n  @torch.no_grad()\n  def forward(\n        self,\n        input_token_ids: torch.Tensor,\n        input_positions: torch.Tensor,\n        kv_write_indices: torch.Tensor,\n        kv_caches: List[Tuple[torch.Tensor, torch.Tensor]],\n        mask: torch.Tensor,\n        output_positions: torch.Tensor,\n        temperatures: Union[torch.Tensor, None],\n        top_ps: torch.Tensor,\n        top_ks: torch.Tensor,\n        local_mask: torch.Tensor | None = None,\n        **kwargs,\n    ) -> Tuple[torch.Tensor, torch.Tensor]:\n    freqs_cis = {}\n\n    if self.config.architecture == gemma_config.Architecture.GEMMA_3:\n      freqs_cis[gemma_config.AttentionType.LOCAL_SLIDING] = (\n                self.local_freqs_cis.index_select(0, input_positions)\n            )\n      freqs_cis[gemma_config.AttentionType.GLOBAL] = (\n                self.global_freqs_cis.index_select(0, input_positions)\n            )\n    else:\n      freqs_cis[gemma_config.AttentionType.LOCAL_SLIDING] = (\n                self.freqs_cis.index_select(0, input_positions)\n            )\n      freqs_cis[gemma_config.AttentionType.GLOBAL] = (\n                self.freqs_cis.index_select(0, input_positions)\n            )\n\n    kv_write_indices = input_positions\n\n    # [batch_size, input_len, hidden_size]\n    hidden_states = self.embedder(input_token_ids)\n    # Gemma normalizes the embedding by sqrt(hidden_size).\n    # Gemma2 downcasts the below to float16, causing sqrt(3072)=55.4256 to become 55.5\n    # See https://github.com/huggingface/transformers/pull/29402\n    normalizer = torch.tensor(self.config.hidden_size**0.5, dtype=hidden_states.dtype, device=hidden_states.device)\n    hidden_states = hidden_states * normalizer\n\n    hidden_states = self.model(\n            hidden_states=hidden_states,\n            freqs_cis=freqs_cis,\n            kv_write_indices=kv_write_indices,\n            kv_caches=kv_caches,\n            mask=mask,\n            local_mask=local_mask,\n        )\n    embedder_weight = self.embedder.weight\n    if self.config.quant:\n      embedder_weight = (\n                embedder_weight * self.embedder.weight_scaler.unsqueeze(-1))\n    next_tokens, logits = self.sampler(\n            embedding=embedder_weight,\n            hidden_states=hidden_states,\n            output_positions=output_positions,\n            temperatures=temperatures,\n            top_ps=top_ps,\n            top_ks=top_ks,\n        )\n    return next_tokens, logits\n\n  def generate(\n        self,\n        prompts: Union[str, Sequence[str]],\n        device: Any,\n        output_len: int = 100,\n        temperature: Union[float, None] = 1.0,\n        top_p: float = 0.95,\n        top_k: int = 64,\n    ) -> Union[str, Sequence[str]]:\n    \"\"\"Generates responses for given prompts using Gemma model.\"\"\"\n    # If a single prompt is provided, treat it as a batch of 1.\n    is_str_prompt = isinstance(prompts, str)\n    if is_str_prompt:\n      prompts = [prompts]\n\n    batch_size = len(prompts)\n    prompt_tokens = [self.tokenizer.encode(prompt) for prompt in prompts]\n    min_prompt_len = min(len(p) for p in prompt_tokens)\n    max_prompt_len = max(len(p) for p in prompt_tokens)\n    max_seq_len = max_prompt_len + output_len\n    assert max_seq_len <= self.config.max_position_embeddings\n\n    # build KV caches\n    kv_caches = []\n    for _ in range(self.config.num_hidden_layers):\n      size = (batch_size, max_seq_len, self.config.num_key_value_heads,\n                    self.config.head_dim)\n      dtype = self.config.get_dtype()\n      k_cache = torch.zeros(size=size, dtype=dtype, device=device)\n      v_cache = torch.zeros(size=size, dtype=dtype, device=device)\n      kv_caches.append((k_cache, v_cache))\n\n    # prepare inputs\n    token_ids_tensor = torch.full((batch_size, max_seq_len),\n                                      self.tokenizer.pad_id, dtype=torch.int64)\n    input_token_ids_tensor = torch.full((batch_size, min_prompt_len),\n                                            self.tokenizer.pad_id,\n                                            dtype=torch.int64)\n    for i, p in enumerate(prompt_tokens):\n      token_ids_tensor[i, :len(p)] = torch.tensor(p)\n      input_token_ids_tensor[i, :min_prompt_len] = torch.tensor(\n                p[:min_prompt_len])\n    token_ids_tensor = token_ids_tensor.to(device)\n    input_token_ids_tensor = input_token_ids_tensor.to(device)\n    prompt_mask_tensor = token_ids_tensor != self.tokenizer.pad_id\n    input_positions_tensor = torch.arange(0, min_prompt_len,\n                                              dtype=torch.int64).to(device)\n    mask_tensor = torch.full((1, 1, max_seq_len, max_seq_len),\n                                 -2.3819763e38).to(torch.float)\n    mask_tensor = torch.triu(mask_tensor, diagonal=1).to(device)\n    local_mask_tensor = mask_tensor + torch.tril(\n            torch.full((1, 1, max_seq_len, max_seq_len), -2.3819763e38, device=device),\n            diagonal=-self.config.sliding_window_size,\n        ) if self.config.sliding_window_size else None\n    curr_mask_tensor = mask_tensor.index_select(2, input_positions_tensor)\n    curr_local_mask_tensor = local_mask_tensor.index_select(\n          2, input_positions_tensor\n      ) if local_mask_tensor is not None else None\n    output_positions_tensor = torch.LongTensor([min_prompt_len - 1]).to(device)\n    temperatures_tensor = None if not temperature else torch.FloatTensor(\n            [temperature] * batch_size).to(device)\n    top_ps_tensor = torch.FloatTensor([top_p] * batch_size).to(device)\n    top_ks_tensor = torch.LongTensor([top_k] * batch_size).to(device)\n    output_index = torch.tensor(min_prompt_len, dtype=torch.int64).to(\n            device)\n\n    # Prefill up to min_prompt_len tokens, then treat other prefill as\n    # decode and ignore output.\n    for i in range(max_seq_len - min_prompt_len):\n      next_token_ids, _ = self(\n                input_token_ids=input_token_ids_tensor,\n                input_positions=input_positions_tensor,\n                kv_write_indices=None,\n                kv_caches=kv_caches,\n                mask=curr_mask_tensor,\n                output_positions=output_positions_tensor,\n                temperatures=temperatures_tensor,\n                top_ps=top_ps_tensor,\n                top_ks=top_ks_tensor,\n                local_mask=curr_local_mask_tensor,\n            )\n\n      curr_prompt_mask = prompt_mask_tensor.index_select(\n                1, output_index).squeeze(dim=1)\n      curr_token_ids = token_ids_tensor.index_select(\n                1, output_index).squeeze(dim=1)\n      output_token_ids = torch.where(curr_prompt_mask, curr_token_ids,\n                                           next_token_ids).unsqueeze(dim=1)\n      token_ids_tensor.index_copy_(1, output_index, output_token_ids)\n\n      input_token_ids_tensor = output_token_ids\n      input_positions_tensor = output_index.unsqueeze(dim=-1)\n      curr_mask_tensor = mask_tensor.index_select(2,\n                                                        input_positions_tensor)\n      curr_local_mask_tensor = local_mask_tensor.index_select(\n                2, input_positions_tensor\n            ) if local_mask_tensor is not None else None\n      output_positions_tensor = torch.tensor(0, dtype=torch.int64).to(\n                device)\n      output_index = output_index + 1\n\n    # Detokenization.\n    token_ids = token_ids_tensor.tolist()\n    results = []\n    for i, tokens in enumerate(token_ids):\n      trimmed_output = tokens[len(prompt_tokens[i]):len(prompt_tokens[i])\n                                    + output_len]\n      if self.tokenizer.eos_id in trimmed_output:\n        eos_index = trimmed_output.index(self.tokenizer.eos_id)\n        trimmed_output = trimmed_output[:eos_index]\n      results.append(self.tokenizer.decode(trimmed_output))\n\n    # If a string was provided as input, return a string as output.\n    return results[0] if is_str_prompt else results\n\n  def load_weights(self, model_path: str):\n        if os.path.isfile(model_path):\n            self.load_state_dict(\n                torch.load(\n                    model_path, mmap=True, weights_only=True,\n                )['model_state_dict'],\n                strict=False,\n            )\n        else:\n            index_path = os.path.join(model_path, 'pytorch_model.bin.index.json')\n            with open(index_path, \"r\", encoding=\"utf-8\") as f:\n                index = json.load(f)\n            shard_files = list(set(index[\"weight_map\"].values()))\n            for shard_file in shard_files:\n                shard_path = os.path.join(model_path, shard_file)\n                state_dict = torch.load(shard_path, map_location=\"cpu\", weights_only=True)\n                self.load_state_dict(state_dict, strict=False)\n                del state_dict  # Save memory.\n                gc.collect()\n",
        "gemma/model_xla.py": "# Copyright 2024 Google LLC\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\n\"\"\"Inference-only Gemma model implementation.\"\"\"\n\nimport json\nimport gc\nimport os\nimport re\nimport torch\nfrom torch import nn\nimport torch.nn.functional as F\nfrom typing import List, Mapping, Optional, Tuple, Union\n\nfrom google3.third_party.open_models_release.gemma_pytorch.gemma import config as gemma_config\nfrom google3.third_party.open_models_release.gemma_pytorch.gemma.xla_model_parallel import (\n    ColumnParallelLinear,\n    ParallelEmbedding,\n    RowParallelLinear,\n    reduce_from_model_parallel_region,\n    scatter_to_model_parallel_region,\n)\n\n\nclass Sampler(nn.Module):\n\n    def __init__(self, vocab_size: int, world_size: int, rank: int,\n                 config: gemma_config.GemmaConfig) -> None:\n        super().__init__()\n        self.vocab_size = vocab_size\n        self.world_size = world_size\n        self.rank = rank\n        self.config = config\n\n    @torch.no_grad()\n    def forward(\n        self,\n        embedding: torch.Tensor,\n        hidden_states: torch.Tensor,\n        output_positions: torch.Tensor,\n        temperatures: Union[torch.Tensor, None],\n        top_ps: torch.Tensor,\n        top_ks: torch.Tensor,\n        embedding_bias: Optional[torch.Tensor] = None,\n    ) -> Tuple[torch.Tensor, torch.Tensor]:\n        # Select the last element for each sequence.\n        # (batch_size, input_len, hidden_size) -> (batch_size, hidden_size)\n        hidden_states = hidden_states.index_select(\n            1, output_positions).squeeze(dim=1)\n\n        hidden_states_parallel = scatter_to_model_parallel_region(\n            hidden_states,\n            groups=None,\n            world_size=self.world_size,\n            rank=self.rank)\n        hidden_states_parallel = torch.matmul(hidden_states_parallel,\n                                              embedding.t())\n        logits = reduce_from_model_parallel_region(\n            hidden_states_parallel,\n            groups=None,\n            world_size=self.world_size,\n            rank=self.rank,\n        )\n        if embedding_bias is not None:\n            logits += embedding_bias\n        if self.config.final_logit_softcapping is not None:\n            logits = logits / self.config.final_logit_softcapping\n            logits = torch.tanh(logits)\n            logits = logits * self.config.final_logit_softcapping\n\n        if temperatures is None:\n            return torch.argmax(logits, dim=-1).squeeze(dim=-1), logits\n\n        # Apply temperature scaling.\n        logits.div_(temperatures.unsqueeze(dim=1))\n\n        # Calculate probabilities with softmax.\n        probs = torch.softmax(logits, dim=-1, dtype=torch.float)\n        probs_sort, probs_idx = torch.sort(probs, dim=-1, descending=True)\n\n        # Apply top-p, top-k.\n        probs_sum = torch.cumsum(probs_sort, dim=-1)\n        top_ps_mask = (probs_sum - probs_sort) > top_ps.unsqueeze(dim=1)\n        probs_sort = torch.where(top_ps_mask, 0, probs_sort)\n\n        top_ks_mask = torch.arange(probs_idx.shape[-1],\n                                   device=probs_idx.device)\n        top_ks_mask = top_ks_mask.expand(probs_idx.shape[0], -1)\n        top_ks_mask = top_ks_mask >= top_ks.unsqueeze(dim=1)\n        probs_sort = torch.where(top_ks_mask, 0, probs_sort)\n\n        # Re-normalization.\n        probs_sort.div_(probs_sort.sum(dim=-1, keepdim=True))\n        probs = torch.gather(probs_sort,\n                             dim=-1,\n                             index=torch.argsort(probs_idx, dim=-1))\n\n        next_token_ids = torch.multinomial(probs,\n                                           num_samples=1,\n                                           replacement=True).squeeze(dim=-1)\n        return next_token_ids, logits\n\n\ndef precompute_freqs_cis(dim: int, end: int, theta: float = 10000.0):\n    \"\"\"Precomputes the frequency cis.\"\"\"\n    freqs = 1.0 / (theta**(torch.arange(0, dim, 2)[:(dim // 2)].float() / dim))\n    t = torch.arange(end, device=freqs.device)  # type: ignore\n    freqs = torch.outer(t, freqs).float()  # type: ignore\n    freqs_cis = torch.polar(torch.ones_like(freqs), freqs)  # complex64\n    return freqs_cis\n\n\ndef apply_rotary_emb(x: torch.Tensor, freqs_cis: torch.Tensor) -> torch.Tensor:\n    \"\"\"Applies the rotary embedding to the query and key tensors.\"\"\"\n    x_ = torch.view_as_complex(\n        torch.stack(torch.chunk(x.transpose(1, 2).float(), 2, dim=-1),\n                    dim=-1))\n    x_out = torch.view_as_real(x_ * freqs_cis).type_as(x)\n    x_out = torch.cat(torch.chunk(x_out, 2, dim=-1), dim=-2)\n    x_out = x_out.reshape(x_out.shape[0], x_out.shape[1], x_out.shape[2],\n                          -1).transpose(1, 2)\n    return x_out\n\n\nclass RMSNorm(torch.nn.Module):\n\n    def __init__(\n        self,\n        dim: int,\n        eps: float = 1e-6,\n        add_unit_offset: bool = True,\n    ):\n        super().__init__()\n        self.eps = eps\n        self.add_unit_offset = add_unit_offset\n        self.weight = nn.Parameter(torch.ones(dim))\n\n    def _norm(self, x):\n        return x * torch.rsqrt(x.pow(2).mean(-1, keepdim=True) + self.eps)\n\n    def forward(self, x):\n        # Llama does x.to(float16) * w whilst Gemma2 is (x * w).to(float16)\n        # See https://github.com/huggingface/transformers/pull/29402\n        output = self._norm(x.float())\n        if self.add_unit_offset:\n            output = output * (1 + self.weight.float())\n        else:\n            output = output * self.weight.float()\n        return output.type_as(x)\n\n\nclass GemmaMLP(nn.Module):\n\n    def __init__(\n        self,\n        hidden_size: int,\n        intermediate_size: int,\n        world_size: int,\n        rank: int,\n        quant: bool,\n    ):\n        super().__init__()\n        self.hidden_size = hidden_size\n        self.intermediate_size = intermediate_size\n\n        def init_method(x):\n            return x\n\n        self.gate_proj = ColumnParallelLinear(\n            hidden_size,\n            intermediate_size,\n            bias=False,\n            gather_output=False,\n            init_method=init_method,\n            world_size=world_size,\n            rank=rank,\n            quant=quant,\n        )\n\n        self.up_proj = ColumnParallelLinear(\n            hidden_size,\n            intermediate_size,\n            bias=False,\n            gather_output=False,\n            init_method=init_method,\n            world_size=world_size,\n            rank=rank,\n            quant=quant,\n        )\n\n        self.down_proj = RowParallelLinear(\n            intermediate_size,\n            hidden_size,\n            bias=False,\n            input_is_parallel=True,\n            init_method=init_method,\n            world_size=world_size,\n            rank=rank,\n            quant=quant,\n        )\n\n    def forward(self, x):\n        gate = self.gate_proj(x)\n        gate = F.gelu(gate, approximate=\"tanh\")\n        up = self.up_proj(x)\n        fuse = gate * up\n        outputs = self.down_proj(fuse)\n        return outputs\n\n\nclass GemmaAttention(nn.Module):\n\n    def __init__(\n        self,\n        hidden_size: int,\n        num_heads: int,\n        num_kv_heads: int,\n        attn_logit_softcapping: Optional[float],\n        query_pre_attn_scalar: Optional[int],\n        head_dim: int,\n        world_size: int,\n        rank: int,\n        quant: bool,\n        attn_type: gemma_config.AttentionType,\n        sliding_window_size: Optional[int] = None,\n    ):\n        super().__init__()\n        self.rank = rank\n\n        def init_method(x):\n            return x\n\n        self.total_num_heads = num_heads\n        assert self.total_num_heads % world_size == 0\n        self.num_heads = self.total_num_heads // world_size  # head per shard\n\n        if num_kv_heads < world_size:\n            assert world_size % num_kv_heads == 0\n            self.total_num_kv_heads = world_size\n        else:\n            assert num_kv_heads % world_size == 0\n            self.total_num_kv_heads = num_kv_heads\n        self.num_kv_heads = self.total_num_kv_heads // world_size  # kv head per shard\n\n        assert self.num_heads % self.num_kv_heads == 0\n        self.num_queries_per_kv = self.num_heads // self.num_kv_heads\n\n        self.hidden_size = hidden_size\n        self.head_dim = head_dim\n\n        self.q_size = self.num_heads * self.head_dim\n        self.kv_size = self.num_kv_heads * self.head_dim\n\n        if query_pre_attn_scalar is not None:\n            self.scaling = query_pre_attn_scalar**-0.5\n        else:\n            self.scaling = self.head_dim**-0.5\n\n        self.qkv_proj = ColumnParallelLinear(\n            self.hidden_size,\n            (self.total_num_heads + 2 * self.total_num_kv_heads) *\n            self.head_dim,\n            bias=False,\n            gather_output=False,\n            init_method=init_method,\n            world_size=world_size,\n            rank=rank,\n            quant=quant,\n        )\n\n        self.o_proj = RowParallelLinear(\n            self.total_num_heads * self.head_dim,\n            self.hidden_size,\n            bias=False,\n            input_is_parallel=True,\n            init_method=init_method,\n            world_size=world_size,\n            rank=rank,\n            quant=quant,\n        )\n\n        self.attn_type = attn_type\n        self.sliding_window_size = sliding_window_size\n        self.attn_logit_softcapping = attn_logit_softcapping\n\n    def forward(\n        self,\n        hidden_states: torch.Tensor,\n        freqs_cis: torch.Tensor,\n        kv_write_indices: torch.Tensor,\n        kv_cache: Tuple[torch.Tensor, torch.Tensor],\n        mask: torch.Tensor,\n    ) -> torch.Tensor:\n        hidden_states_shape = hidden_states.shape\n        assert len(hidden_states_shape) == 3\n\n        batch_size, input_len, _ = hidden_states_shape\n\n        qkv = self.qkv_proj(hidden_states)\n        xq, xk, xv = qkv.split([self.q_size, self.kv_size, self.kv_size],\n                               dim=-1)\n\n        xq = xq.view(batch_size, -1, self.num_heads, self.head_dim)\n        xk = xk.view(batch_size, -1, self.num_kv_heads, self.head_dim)\n        xv = xv.view(batch_size, -1, self.num_kv_heads, self.head_dim)\n\n        # Positional embedding.\n        xq = apply_rotary_emb(xq, freqs_cis=freqs_cis)\n        xk = apply_rotary_emb(xk, freqs_cis=freqs_cis)\n\n        # Write new kv cache.\n        # [batch_size, input_len, n_local_kv_heads, head_dim]\n        k_cache, v_cache = kv_cache\n        k_cache.index_copy_(1, kv_write_indices, xk)\n        v_cache.index_copy_(1, kv_write_indices, xv)\n\n        key = k_cache\n        value = v_cache\n        if self.num_kv_heads != self.num_heads:\n            # [batch_size, max_seq_len, n_local_heads, head_dim]\n            key = torch.repeat_interleave(key, self.num_queries_per_kv, dim=2)\n            value = torch.repeat_interleave(value,\n                                            self.num_queries_per_kv,\n                                            dim=2)\n\n        # [batch_size, n_local_heads, input_len, head_dim]\n        q = xq.transpose(1, 2)\n        # [batch_size, n_local_heads, max_seq_len, head_dim]\n        k = key.transpose(1, 2)\n        v = value.transpose(1, 2)\n\n        # [batch_size, n_local_heads, input_len, max_seq_len]\n        q.mul_(self.scaling)\n        scores = torch.matmul(q, k.transpose(2, 3))\n        if (\n            self.attn_type == gemma_config.AttentionType.LOCAL_SLIDING\n            and self.sliding_window_size is not None\n        ):\n            all_ones = torch.ones_like(mask)\n            sliding_mask = torch.triu(\n                all_ones, -1 * self.sliding_window_size + 1\n            ) * torch.tril(all_ones, self.sliding_window_size - 1)\n            mask = torch.where(sliding_mask == 1, mask, -2.3819763e38)\n        if self.attn_logit_softcapping is not None:\n            scores = scores / self.attn_logit_softcapping\n            scores = torch.tanh(scores)\n            scores = scores * self.attn_logit_softcapping\n        scores = scores + mask\n        scores = F.softmax(scores.float(), dim=-1).type_as(q)\n\n        # [batch_size, n_local_heads, input_len, head_dim]\n        output = torch.matmul(scores, v)\n\n        # [batch_size, input_len, hidden_dim]\n        output = (output.transpose(1, 2).contiguous().view(\n            batch_size, input_len, -1))\n        output = self.o_proj(output)\n        return output\n\n\nclass GemmaDecoderLayer(nn.Module):\n\n    def __init__(\n        self,\n        config: gemma_config.GemmaConfig,\n        world_size: int,\n        rank: int,\n    ):\n        super().__init__()\n        self.rank = rank\n        self.self_attn = GemmaAttention(\n            hidden_size=config.hidden_size,\n            num_heads=config.num_attention_heads,\n            num_kv_heads=config.num_key_value_heads,\n            attn_logit_softcapping=config.attn_logit_softcapping,\n            query_pre_attn_scalar=config.query_pre_attn_scalar,\n            head_dim=config.head_dim,\n            world_size=world_size,\n            rank=rank,\n            quant=config.quant,\n            attn_type=gemma_config.AttentionType.GLOBAL,\n        )\n        self.mlp = GemmaMLP(\n            hidden_size=config.hidden_size,\n            intermediate_size=config.intermediate_size,\n            world_size=world_size,\n            rank=rank,\n            quant=config.quant,\n        )\n        self.input_layernorm = RMSNorm(config.hidden_size,\n                                       eps=config.rms_norm_eps)\n        self.post_attention_layernorm = RMSNorm(config.hidden_size,\n                                                eps=config.rms_norm_eps)\n\n    def forward(\n        self,\n        hidden_states: torch.Tensor,\n        freqs_cis: torch.Tensor,\n        kv_write_indices: torch.Tensor,\n        kv_cache: Tuple[torch.Tensor, torch.Tensor],\n        mask: torch.Tensor,\n    ) -> torch.Tensor:\n        # Self Attention\n        residual = hidden_states\n        hidden_states = self.input_layernorm(hidden_states)\n        hidden_states = self.self_attn(\n            hidden_states=hidden_states,\n            freqs_cis=freqs_cis,\n            kv_write_indices=kv_write_indices,\n            kv_cache=kv_cache,\n            mask=mask,\n        )\n        hidden_states = residual + hidden_states\n\n        # MLP\n        residual = hidden_states\n        hidden_states = self.post_attention_layernorm(hidden_states)\n        hidden_states = self.mlp(hidden_states)\n        hidden_states = residual + hidden_states\n\n        return hidden_states\n\n\nclass Gemma2DecoderLayer(nn.Module):\n\n    def __init__(\n        self,\n        config: gemma_config.GemmaConfig,\n        attn_type: gemma_config.AttentionType,\n        world_size: int,\n        rank: int,\n    ):\n        super().__init__()\n        self.rank = rank\n        self.self_attn = GemmaAttention(\n            hidden_size=config.hidden_size,\n            num_heads=config.num_attention_heads,\n            num_kv_heads=config.num_key_value_heads,\n            attn_logit_softcapping=config.attn_logit_softcapping,\n            query_pre_attn_scalar=config.query_pre_attn_scalar,\n            head_dim=config.head_dim,\n            world_size=world_size,\n            rank=rank,\n            quant=config.quant,\n            attn_type=attn_type,\n            sliding_window_size=config.sliding_window_size,\n        )\n        self.mlp = GemmaMLP(\n            hidden_size=config.hidden_size,\n            intermediate_size=config.intermediate_size,\n            world_size=world_size,\n            rank=rank,\n            quant=config.quant,\n        )\n        self.input_layernorm = RMSNorm(config.hidden_size,\n                                       eps=config.rms_norm_eps)\n        self.post_attention_layernorm = RMSNorm(config.hidden_size,\n                                                eps=config.rms_norm_eps)\n        self.pre_feedforward_layernorm = (\n            RMSNorm(config.hidden_size, eps=config.rms_norm_eps)\n            if config.use_pre_ffw_norm\n            else None\n        )\n        self.post_feedforward_layernorm = (\n            RMSNorm(config.hidden_size, eps=config.rms_norm_eps)\n            if config.use_post_ffw_norm\n            else None\n        )\n\n    def forward(\n        self,\n        hidden_states: torch.Tensor,\n        freqs_cis: torch.Tensor,\n        kv_write_indices: torch.Tensor,\n        kv_cache: Tuple[torch.Tensor, torch.Tensor],\n        mask: torch.Tensor,\n    ) -> torch.Tensor:\n        # Self Attention\n        residual = hidden_states\n        hidden_states = self.input_layernorm(hidden_states)\n        hidden_states = self.self_attn(\n            hidden_states=hidden_states,\n            freqs_cis=freqs_cis,\n            kv_write_indices=kv_write_indices,\n            kv_cache=kv_cache,\n            mask=mask,\n        )\n        hidden_states = self.post_attention_layernorm(hidden_states)\n        hidden_states = residual + hidden_states\n\n        # MLP\n        residual = hidden_states\n        if self.pre_feedforward_layernorm is not None:\n            hidden_states = self.pre_feedforward_layernorm(hidden_states)\n        hidden_states = self.mlp(hidden_states)\n        if self.post_feedforward_layernorm is not None:\n            hidden_states = self.post_feedforward_layernorm(hidden_states)\n        hidden_states = residual + hidden_states\n\n        return hidden_states\n\n\nclass GemmaModel(nn.Module):\n\n    def __init__(\n        self,\n        config: gemma_config.GemmaConfig,\n        world_size: int,\n        rank: int\n    ):\n        super().__init__()\n        self.config = config\n        self.rank = rank\n        self.vocab_size = config.vocab_size\n\n        self.layers = nn.ModuleList()\n        for i in range(config.num_hidden_layers):\n            if config.architecture == gemma_config.Architecture.GEMMA_1:\n                self.layers.append(GemmaDecoderLayer(config, world_size, rank))\n            elif config.architecture == gemma_config.Architecture.GEMMA_2:\n                attn_type = (\n                    config.attn_types[i]\n                    if config.attn_types is not None\n                    else gemma_config.AttentionType.GLOBAL\n                )\n                self.layers.append(\n                    Gemma2DecoderLayer(config, attn_type, world_size, rank))\n            else:\n                raise ValueError(f'Unknown architecture: {config.architecture}')\n        self.norm = RMSNorm(config.hidden_size, eps=config.rms_norm_eps)\n\n    def forward(\n        self,\n        hidden_states: torch.Tensor,\n        freqs_cis: torch.Tensor,\n        kv_write_indices: torch.Tensor,\n        kv_caches: List[Tuple[torch.Tensor, torch.Tensor]],\n        mask: torch.Tensor,\n    ) -> torch.Tensor:\n        for i in range(len(self.layers)):\n            layer = self.layers[i]\n            hidden_states = layer(\n                hidden_states=hidden_states,\n                freqs_cis=freqs_cis,\n                kv_write_indices=kv_write_indices,\n                kv_cache=kv_caches[i],\n                mask=mask,\n            )\n        hidden_states = self.norm(hidden_states)\n        return hidden_states\n\n\nclass GemmaForCausalLM(nn.Module):\n\n    def __init__(\n        self,\n        config: gemma_config.GemmaConfig,\n        world_size: int,\n        rank: int,\n        device: torch.device,\n    ):\n        super().__init__()\n        self.config = config\n        self.world_size = world_size\n        self.rank = rank\n        self.device = device\n\n        assert config.num_attention_heads % world_size == 0\n        assert config.hidden_size % config.num_attention_heads == 0\n\n        max_seq_len = config.max_position_embeddings\n        head_dim = config.head_dim\n        vocab_size = config.vocab_size\n\n        def init_method(x):\n            return x\n\n        self.embedder = ParallelEmbedding(\n            vocab_size,\n            config.hidden_size,\n            init_method=init_method,\n            world_size=world_size,\n            rank=rank,\n            quant=config.quant,\n        )\n        self.model = GemmaModel(config, world_size, rank)\n        self.sampler = Sampler(vocab_size, world_size, rank, config)\n\n        rope_theta = getattr(config, 'rope_theta', 10000)\n        # [head_dim * 2, ] -> complex -> two dim (real, imaginary) implicitly\n        freqs_cis = precompute_freqs_cis(head_dim,\n                                         max_seq_len * 2,\n                                         theta=rope_theta)\n        self.register_buffer('freqs_cis', freqs_cis)\n\n    @torch.no_grad()\n    def forward(\n        self,\n        input_token_ids: torch.Tensor,\n        input_positions: torch.Tensor,\n        kv_write_indices: torch.Tensor,\n        kv_caches: List[Tuple[torch.Tensor, torch.Tensor]],\n        mask: torch.Tensor,\n        output_positions: torch.Tensor,\n        temperatures: Union[torch.Tensor, None],\n        top_ps: torch.Tensor,\n        top_ks: torch.Tensor,\n        **kwargs,\n    ) -> Tuple[torch.Tensor, torch.Tensor]:\n        freqs_cis = self.freqs_cis.index_select(0, input_positions)\n        kv_write_indices = input_positions\n\n        hidden_states = self.embedder(input_token_ids)\n        # Gemma normalizes the embedding by sqrt(hidden_size).\n        # Gemma2 downcasts the below to float16, causing sqrt(3072)=55.4256 to become 55.5\n        # See https://github.com/huggingface/transformers/pull/29402\n        normalizer = torch.tensor(self.config.hidden_size**0.5, dtype=hidden_states.dtype)\n        hidden_states = hidden_states * normalizer\n        # hidden_states should be [batch_size, input_len, hidden_size]\n\n        hidden_states = self.model(\n            hidden_states=hidden_states,\n            freqs_cis=freqs_cis,\n            kv_write_indices=kv_write_indices,\n            kv_caches=kv_caches,\n            mask=mask,\n        )\n        embedder_weight = self.embedder.weight\n        if self.config.quant:\n            embedder_weight = (\n                embedder_weight * self.embedder.weight_scaler.unsqueeze(-1))\n        next_tokens, logits = self.sampler(\n            embedding=embedder_weight,\n            hidden_states=hidden_states,\n            output_positions=output_positions,\n            temperatures=temperatures,\n            top_ps=top_ps,\n            top_ks=top_ks,\n        )\n        return next_tokens, logits\n\n    def _load_weights(self, model_state_dict: Mapping[str, torch.Tensor]):\n        num_attn_heads = self.config.num_attention_heads\n        num_kv_heads = self.config.num_key_value_heads\n        head_dim = self.config.head_dim\n        hidden_size = self.config.hidden_size\n\n        def split(tensor: torch.Tensor, axis: int) -> torch.Tensor:\n            axis_len = tensor.shape[axis]\n            split_len = axis_len // self.world_size\n            split_start = split_len * self.rank\n            split_end = split_start + split_len\n            tensor = torch.moveaxis(tensor, axis, 0)\n            tensor = tensor[split_start:split_end, ...]\n            tensor = torch.moveaxis(tensor, 0, axis)\n            return tensor\n\n        for k, v in model_state_dict.items():\n            if k == 'freqs_cis':\n                continue\n            if (k == 'model.norm.weight'\n                or k.endswith('_layernorm.weight')\n                or k.endswith('weight_scaler')):\n                pass\n            elif (k == 'embedder.weight' or re.fullmatch(\n                    r'model.layers.\\d+.mlp.down_proj.weight', k)):\n                v = split(v, 1)\n            elif (re.fullmatch(r'model.layers.\\d+.mlp.gate_proj.weight', k)\n                  or re.fullmatch(r'model.layers.\\d+.mlp.up_proj.weight', k)):\n                v = split(v, 0)\n            elif re.fullmatch(r'model.layers.\\d+.self_attn.qkv_proj.weight',\n                              k):\n                if num_kv_heads <= num_attn_heads:\n                    # If num_kv_heads > self.world_size, we still want 1\n                    # replica.\n                    num_replicas = max(self.world_size // num_kv_heads, 1)\n                    v = v.reshape(num_attn_heads + num_kv_heads * 2, head_dim,\n                                  hidden_size)\n                    query = v[:num_attn_heads, ...]\n                    key = v[num_attn_heads:num_attn_heads + num_kv_heads,\n                            ...].repeat(num_replicas, 1, 1)\n                    value = v[-num_kv_heads:, ...].repeat(num_replicas, 1, 1)\n                    v = torch.cat(\n                        (split(query, 0), split(key, 0), split(value, 0)),\n                        dim=0)\n                else:\n                    v = v.reshape(3, num_attn_heads, head_dim, hidden_size)\n                    v = split(v, 1)\n                v = v.reshape(-1, hidden_size)\n            elif re.fullmatch(r'model.layers.\\d+.self_attn.o_proj.weight', k):\n                v = v.reshape(hidden_size, num_attn_heads, head_dim)\n                v = split(v, 1)\n                v = v.reshape(hidden_size, -1)\n            else:\n                raise ValueError(f'Unrecognized key: {k}')\n            self.state_dict()[k].copy_(v)\n\n    def load_weights(self, model_path: str):\n        if os.path.isfile(model_path):\n            checkpoint = torch.load(model_path, weights_only=True)\n            model_state_dict = checkpoint['model_state_dict']\n            self._load_weights(model_state_dict)\n        else:\n            index_path = os.path.join(model_path, 'pytorch_model.bin.index.json')\n            with open(index_path, \"r\", encoding=\"utf-8\") as f:\n                index = json.load(f)\n            shard_files = list(set(index[\"weight_map\"].values()))\n            for shard_file in shard_files:\n                shard_path = os.path.join(model_path, shard_file)\n                state_dict = torch.load(shard_path, map_location=\"cpu\", weights_only=True)\n                self._load_weights(state_dict)\n                del state_dict  # Save memory.\n                gc.collect()\n",
        "gemma/siglip_vision/__init__.py": "# Copyright 2024 Google LLC\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\n",
        "gemma/siglip_vision/config.py": "# Copyright 2024 Google LLC\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\n\"\"\"Gemma model config.\"\"\"\n\nimport dataclasses\nfrom . import preprocessor\n\n\n# https://developers.googleblog.com/en/gemma-explained-paligemma-architecture/\n@dataclasses.dataclass\nclass SiglipVisionModelConfig:\n  \"\"\"Returns the model config for the vision model of Gemma 3 andPaliGemma.\"\"\"\n  # The number of transformer encoder blocks in the siglip encoder model.\n  num_hidden_layers: int = 27\n  # The dimension of the embedding.\n  embedding_dim: int = 1152\n  # Whether to use bias in the 2D conv embedding layer.\n  embedding_use_bias: bool = True\n  # The number of channels in the input images.\n  input_channels: int = 3\n  # The input image size.\n  image_size: int = preprocessor.DEFAULT_IMAGE_SIZE\n  # Kernel size of 2D convolution layer.\n  conv2d_patch_size = 14\n  # The number of attention heads used in the attention layers of the model.\n  num_attention_heads: int = 16\n  # The number of head dimensions.\n  head_dim: int = 72\n  # Clarify: is num_key_value same as num_query_groups?\n  num_key_value_heads: int = 16\n  # The number of query groups for implementing attention.\n  num_query_groups: int = 16\n  # Clarify: usecase of this field is not clear.\n  qkv_use_bias: bool = True\n  # Clarify: usecase of this field is not clear.\n  output_proj_use_bias: bool = True\n  # The dimension of the MLP representations.\n  intermediate_size: int = 4304\n  # The epsilon used by the layer normalization layers.\n  layer_norm_eps: float = 1e-6\n  # Clarify: identify if the dtype varies for the siglip vision model.\n  dtype: str = 'bfloat16'\n  # Whether a quantized version of the model is used.\n  quant: bool = False\n  # The sequence length of the encoding.\n  encoding_sequence_length: int = 256\n\n\ndef get_siglip_vision_model_config() -> SiglipVisionModelConfig:\n  \"\"\"Returns the default model config for the vision model of Gemma 3 and PaliGemma.\"\"\"\n  return SiglipVisionModelConfig()\n\n",
        "gemma/siglip_vision/pan_and_scan.py": "# Copyright 2024 Google LLC\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n# http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\"\"\"Pan and scan image cropping implementation.\"\"\"\n\nfrom collections.abc import Sequence\n\nimport numpy as np\nfrom PIL import Image\n\n\ndef pan_and_scan(\n    img: Image.Image,\n    *,\n    min_crop_size: int = 256,\n    max_num_crops: int = 4,\n) -> Sequence[Image.Image]:\n    return _pan_and_scan_os(\n        img,\n        min_crop_size=min_crop_size,\n        max_num_crops=max_num_crops,\n    )[0]\n\n\ndef pan_and_scan_os_with_crop_positions(\n    img: Image.Image,\n    *,\n    min_crop_size: int = 256,\n    max_num_crops: int = 4,\n) -> tuple[Sequence[Image.Image], Sequence[tuple[int, int, int, int]]]:\n    return _pan_and_scan_os(\n        img,\n        min_crop_size=min_crop_size,\n        max_num_crops=max_num_crops,\n    )\n\n\ndef _pan_and_scan_os(\n    img: Image.Image,\n    *,\n    min_crop_size: int,\n    max_num_crops: int,\n) -> tuple[Sequence[Image.Image], Sequence[tuple[int, int, int, int]]]:\n    \"\"\"Pan and scan an image for open source.\n\n    If the image is landscape, the crops are made horizontally and if the image is\n    portrait, the crops are made vertically. The longer side of the image is split\n    into [2 - max_num_crops] crops.\n\n    Args:\n        img: PIL Image object.\n        min_crop_size: The minimum size of each crop.\n        max_num_crops: The maximum desired number of crops to be generated.\n\n    Returns:\n        List of cropped PIL Image objects and a list of crop positions.\n    \"\"\"\n    w, h = img.size\n\n    # Square or landscape image.\n    if w >= h:\n        if w / h < 1.5:\n            return [img], [(0, 0, h, w)]\n\n        # Select ideal number of crops close to the image aspect ratio and such that\n        # crop_size > min_crop_size.\n        num_crops_w = int(np.floor(w / h + 0.5))    # Half round up rounding.\n        num_crops_w = min(\n            int(np.floor(w / min_crop_size)),\n            num_crops_w,\n        )\n\n        # Make sure the number of crops is in range [2, max_num_crops].\n        num_crops_w = max(2, num_crops_w)\n        num_crops_w = min(max_num_crops, num_crops_w)\n        num_crops_h = 1\n\n    # Portrait image.\n    else:\n        if h / w < 1.5:\n            return [img], [(0, 0, h, w)]\n\n        num_crops_h = int(np.floor(h / w + 0.5))\n        num_crops_h = min(int(np.floor(h / min_crop_size)), num_crops_h)\n        num_crops_h = max(2, num_crops_h)\n        num_crops_h = min(max_num_crops, num_crops_h)\n        num_crops_w = 1\n\n    crop_size_w = int(np.ceil(w / num_crops_w))\n    crop_size_h = int(np.ceil(h / num_crops_h))\n\n    # Don't apply pan and scan if crop size is too small.\n    if min(crop_size_w, crop_size_h) < min_crop_size:\n        return [img], [(0, 0, h, w)]\n\n    crop_positions_w = [crop_size_w * i for i in range(num_crops_w)]\n    crop_positions_h = [crop_size_h * i for i in range(num_crops_h)]\n\n    # Generate crops.\n    crops = []\n    crop_positions = []\n    for pos_h in crop_positions_h:\n        for pos_w in crop_positions_w:\n            crops.append(\n                img.crop((\n                    pos_w,\n                    pos_h,\n                    pos_w + crop_size_w,\n                    pos_h + crop_size_h,\n                ))\n            )\n            crop_positions.append(\n                (pos_h, pos_w, pos_h + crop_size_h, pos_w + crop_size_w)\n            )\n\n    return crops, crop_positions\n",
        "gemma/siglip_vision/preprocessor.py": "# Copyright 2024 Google LLC\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\"\"\"Preprocessor for Siglip vision model.\n\nNo neural network is used in the following functions. These are heuristic based.\n\"\"\"\n\nfrom collections.abc import Sequence\n\nfrom PIL import Image\nimport torch\nimport numpy as np  # Import NumPy\n\n_IMAGE_MEAN = [0.5, 0.5, 0.5]  # equivalent to 127.5/255\n_IMAGE_STD = [0.5, 0.5, 0.5]  # equivalent to 127.5/255\nDEFAULT_IMAGE_SIZE = 896\n\n\ndef preprocess_images_for_siglip_vision(\n    images: Sequence[Image.Image], image_size=DEFAULT_IMAGE_SIZE\n) -> torch.Tensor:\n    \"\"\"Preprocesses a list of PIL images for Siglip vision model using only PyTorch and PIL.\n\n    Args:\n        images: A sequence of PIL Image objects.\n        image_size: The target size for resizing the images.\n\n    Returns:\n        A sequence of torch.Tensor objects, each of shape (C, H, W).\n    \"\"\"\n    processed_images = []\n\n    mean_tensor = torch.tensor(_IMAGE_MEAN, dtype=torch.float32).reshape(3, 1, 1)\n    std_tensor = torch.tensor(_IMAGE_STD, dtype=torch.float32).reshape(3, 1, 1)\n\n    for image in images:\n        # Resize image\n        image = image.resize((image_size, image_size), Image.Resampling.BILINEAR)\n\n        # Convert to NumPy and ensure float32 type\n        image_np = np.array(image, dtype=np.float32) / 255.0  # Normalize to [0,1]\n\n        # Convert to PyTorch tensor and rearrange channels\n        image_tensor = torch.from_numpy(image_np).permute(2, 0, 1)  # (H, W, C) â†’ (C, H, W)\n\n        # Normalize\n        image_tensor = (image_tensor - mean_tensor) / std_tensor\n\n        # Clip the values to [-1, 1]\n        image_tensor = torch.clamp(image_tensor, -1, 1)\n\n        processed_images.append(image_tensor)\n\n    return processed_images\n\n\n# Example usage:\n# Assuming you have a list of PIL images called 'pil_images'\n# pil_images = [Image.open(\"image1.jpg\"), Image.open(\"image2.png\")]\n# processed_tensors = preprocess_images_pytorch(pil_images)\n# for tensor in processed_tensors: print(tensor.shape)\n",
        "gemma/siglip_vision/siglip_vision_model.py": "# Copyright 2024 Google LLC\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\"\"\"Siglip vision model for gemma 3 and paligemma.\"\"\"\n\nimport torch\nfrom torch import nn\nimport torch.nn.functional as F\n\nfrom . import config as siglip_vision_config\nSiglipVisionModelConfig = siglip_vision_config.SiglipVisionModelConfig\n\nclass AveragePool2D(nn.Module):\n  \"\"\"Applies 4x4 average pooling and reshaping.\"\"\"\n  def __init__(self, config):\n    super().__init__()\n    self.config = config\n\n  def forward(self, x):\n      \"\"\"Applies 4x4 average pooling and reshaping.\"\"\"\n      batch_size, seq_len, channels = x.shape\n      width = int(seq_len**0.5)\n      if width * width != seq_len:\n          raise ValueError(\n              f\"Sequence length {seq_len} is not a perfect square. Cannot reshape to a square image.\"\n          )\n      # Bx(64^2)x1152 -> Bx1152x(64^2) -> Bx1152x64x64\n      x = x.transpose(1, 2).reshape(batch_size, channels, width, width)\n      # Bx1152x64x64-> Bx1152x16x16\n      x = F.avg_pool2d(x, kernel_size=4, stride=4)\n      # Bx1152x64x64-> Bx1152x256 -> Bx256x1152\n      x = x.flatten(2).transpose(1, 2)\n      return x\n\n# Siglip Attention\nclass SiglipAttention(nn.Module):\n  \"\"\"Siglip attention module.\"\"\"\n\n  def __init__(self, dim, num_heads, head_dim):\n    super().__init__()\n    self.dim = dim\n    self.num_heads = num_heads\n    self.head_dim = head_dim\n\n    # Key, Query, Value projections\n    self.k_proj = nn.Linear(dim, num_heads * head_dim, bias=True)\n    self.q_proj = nn.Linear(dim, num_heads * head_dim, bias=True)\n    self.v_proj = nn.Linear(dim, num_heads * head_dim, bias=True)\n\n    # Output projection\n    self.o_proj = nn.Linear(num_heads * head_dim, dim, bias=True)\n\n  def forward(self, x):\n    batch_size, seq_len, _ = x.size()\n\n    # Project inputs to key, query, value\n    k = self.k_proj(x).view(batch_size, seq_len, self.num_heads, self.head_dim)\n    q = self.q_proj(x).view(batch_size, seq_len, self.num_heads, self.head_dim)\n    v = self.v_proj(x).view(batch_size, seq_len, self.num_heads, self.head_dim)\n\n    # Transpose for multi-head attention\n    k = k.transpose(1, 2)  # (batch_size, num_heads, seq_len, head_dim)\n    q = q.transpose(1, 2)  # (batch_size, num_heads, seq_len, head_dim)\n    v = v.transpose(1, 2)  # (batch_size, num_heads, seq_len, head_dim)\n\n    # Scaled dot-product attention\n    scores = torch.matmul(q, k.transpose(-2, -1)) / (self.head_dim**0.5)\n    attn_weights = F.softmax(scores, dim=-1)\n    attn_output = torch.matmul(attn_weights, v)\n\n    # Transpose back to (batch_size, seq_len, num_heads, head_dim)\n    attn_output = attn_output.transpose(1, 2).contiguous()\n    attn_output = attn_output.view(\n        batch_size, seq_len, self.num_heads * self.head_dim\n    )\n\n    # Apply output projection\n    output = self.o_proj(attn_output)\n\n    return output\n\n\nclass SiglipMLP(nn.Module):\n  \"\"\"Siglip MLP module.\"\"\"\n  def __init__(self, hidden_size, intermediate_size):\n    super().__init__()\n    self.fc1 = nn.Linear(hidden_size, intermediate_size)\n    self.fc2 = nn.Linear(intermediate_size, hidden_size)\n\n  def gelu_tanh(self, x):\n    return (\n        0.5\n        * x\n        * (\n            1\n            + torch.tanh(\n                torch.sqrt(torch.tensor(2.0 / torch.pi, device=x.device))\n                * (x + 0.044715 * torch.pow(x, 3))\n            )\n        )\n    )\n\n  def forward(self, x):\n    x = self.fc1(x)\n    x = self.gelu_tanh(x)\n    x = self.fc2(x)\n    return x\n\n\nclass SiglipEncoderBlock(nn.Module):\n  \"\"\"Encoder block (Transformer layer) for siglip vision model.\"\"\"\n\n  def __init__(self, config: SiglipVisionModelConfig):\n    super().__init__()\n    self.self_attn = SiglipAttention(\n        config.embedding_dim, config.num_attention_heads, config.head_dim\n    )\n    # SigLiPFromPatches_0/siglip_encoder/Transformer/encoderblock_0/LayerNorm_0\n    self.layer_norm1 = nn.LayerNorm(config.embedding_dim, eps=config.layer_norm_eps)\n    self.mlp = SiglipMLP(config.embedding_dim, config.intermediate_size)\n    # SigLiPFromPatches_0/siglip_encoder/Transformer/encoderblock_0/LayerNorm_1\n    self.layer_norm2 = nn.LayerNorm(config.embedding_dim, eps=config.layer_norm_eps)\n\n  def forward(self, x):\n    # Pre-LN\n    residual = x\n    # SigLiPFromPatches_0/siglip_encoder/Transformer/encoderblock_0/LayerNorm_0\n    x = self.layer_norm1(x)\n    x = self.self_attn(x)\n    x = x + residual  # Residual connection *after* LayerNorm\n\n    residual = x\n    # SigLiPFromPatches_0/siglip_encoder/Transformer/encoderblock_0/LayerNorm_1\n    x = self.layer_norm2(x)\n    x = self.mlp(x)\n    x = x + residual  # Residual connection *after* LayerNorm\n    return x\n\n\n# https://developers.googleblog.com/en/gemma-explained-paligemma-architecture/\nclass SiglipVisionModel(nn.Module):\n  \"\"\"Signlip vision model for gemma 3 and paligemma.\"\"\"\n\n  def __init__(self, config: SiglipVisionModelConfig):\n    super().__init__()\n\n    # SigLiPFromPatches_0/siglip_encoder/embedding\n    self.patch_embedding = nn.Conv2d(\n        in_channels=config.input_channels,\n        out_channels=config.embedding_dim,\n        kernel_size=config.conv2d_patch_size,\n        stride=config.conv2d_patch_size,\n        padding=0,\n        bias=config.embedding_use_bias,\n    )\n    self.num_patches = (config.image_size // config.conv2d_patch_size) ** 2\n    self.num_positions = self.num_patches\n    # SigLiPFromPatches_0/siglip_encoder\n    self.position_embedding = nn.Embedding(\n        self.num_positions, config.embedding_dim\n    )\n    self.register_buffer(\"position_ids\", torch.arange(self.num_positions).expand((1, -1)), persistent=False)\n\n    # SigLiPFromPatches_0/siglip_encoder/Transformer/encoderblock_i\n    self.encoder_blocks = nn.ModuleList(\n        SiglipEncoderBlock(config=config)\n        for _ in range(config.num_hidden_layers)\n    )\n    # SigLiPFromPatches_0/siglip_encoder/Transformer/encoder_norm\n    self.final_norm = nn.LayerNorm(config.embedding_dim, config.layer_norm_eps)\n    self.avg_pool = AveragePool2D(config)\n    self.config = config\n\n  # pixel_values=Bxconfig.input_channels x config.image_size x config.image_size\n  @torch.inference_mode\n  def forward(\n      self,\n      pixel_values: torch.Tensor,\n  ) -> torch.Tensor:\n    # Embed the image according to SiplipVisionEmbeddings.\n    x = self.patch_embedding(pixel_values)\n    # (batch_size,channels,height,width)->(batch_size, height*width, channels)\n    x = x.flatten(2).transpose(1, 2)\n\n    position_ids = self.position_ids.to(pixel_values.device)\n    x = x + self.position_embedding(position_ids)\n\n    for block in self.encoder_blocks:\n      x = block(x)  # batch_size, height*width, embedding_dim (1152)\n    x = self.final_norm(x)\n\n    # siglip exit https://source.corp.google.com/piper///depot/google3/third_party/py/gemma/multimodal/vision.py;l=220\n    return self.avg_pool(x)\n",
        "gemma/tokenizer.py": "# Copyright 2024 Google LLC\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\nimport os\nfrom typing import List, Optional\n\nimport sentencepiece\n\ndef _assert_file_exists(model_path: str):\n    assert os.path.isfile(model_path), model_path\n\n_BEGIN_IMAGE_TOKEN = 255999\n_END_IMAGE_TOKEN = 256000\n\nclass Tokenizer:\n\n    def __init__(self, model_path: Optional[str]):\n        _assert_file_exists(model_path)\n        self.sp_model = sentencepiece.SentencePieceProcessor()\n        self.sp_model.Load(model_path)\n\n        # BOS / EOS token IDs.\n        self.n_words: int = self.sp_model.GetPieceSize()\n        self.bos_id: int = self.sp_model.bos_id()\n        self.eos_id: int = self.sp_model.eos_id()\n        self.pad_id: int = self.sp_model.pad_id()\n        self.boi_id: int = _BEGIN_IMAGE_TOKEN\n        self.eoi_id: int = _END_IMAGE_TOKEN\n        self.image_token_placeholder_id: int = self.sp_model.pad_id()\n\n    def encode(self, s: str, bos: bool = True, eos: bool = False) -> List[int]:\n        \"\"\"Converts a string into a list of tokens.\"\"\"\n        assert isinstance(s, str)\n        t = self.sp_model.EncodeAsIds(s)\n        if bos:\n            t = [self.bos_id] + t\n        if eos:\n            t = t + [self.eos_id]\n        return t\n\n    def decode(self, t: List[int]) -> str:\n        \"\"\"Converts a list of tokens into a string.\"\"\"\n        return self.sp_model.DecodeIds(t)\n",
        "gemma/xla_model_parallel.py": "# Copyright 2024 Google LLC\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\nfrom copy import deepcopy\nfrom dataclasses import dataclass\nimport os\nfrom typing import Callable, List, Optional, Tuple\n\nimport torch\nimport torch.ao.quantization.fx._decomposed\nimport torch.distributed as dist\nimport torch.distributed._functional_collectives as fc\nimport torch.distributed.distributed_c10d as c10d\nimport torch.nn.functional as F\nimport torch.nn.init as init\nfrom torch.nn.parameter import Parameter\n\nEPS = torch.finfo(torch.float32).eps\n\nUSE_CUDA = os.environ.get('USE_CUDA', False)\nif not USE_CUDA:\n    import torch_xla.core.xla_model as xm\n\nTAG = None\nRANKSET = None\nGROUP_SIZE = None\n\n\ndef set_g_group():\n    global TAG\n    global RANKSET\n    global GROUP_SIZE\n\n    assert USE_CUDA, \"This hack is only for PyTorch non-XLA CUDA paths, i.e., eager and inductor.\"\n    TAG, RANKSET, GROUP_SIZE = fc._expand_group(c10d._get_default_group())\n\n\n@dataclass\nclass TensorQConfig:\n    dtype: torch.dtype = torch.int8\n    axis: int = -1\n    quant_min: int = -128\n    quant_max: int = 127\n    symmetric_quant: bool = True\n\n\ndef _find_per_channel_min_max(x: torch.Tensor, axis: int):\n    x_dim = x.size()\n    new_axis_list = list(range(len(x_dim)))\n    new_axis_list[axis] = 0\n    new_axis_list[0] = axis\n    y = x.permute(new_axis_list)\n    y = torch.flatten(y, start_dim=1)\n    return torch.aminmax(y, dim=1)\n\n\ndef _find_qparams(x: torch.Tensor, qconfig: TensorQConfig):\n    # Only support per-channel symmetric quant to int8 now\n    axis = qconfig.axis\n    dtype = qconfig.dtype\n    symmetric_quant = qconfig.symmetric_quant\n    quant_min = qconfig.quant_min\n    quant_max = qconfig.quant_max\n    assert axis >= 0 and axis < len(x.shape)\n    assert dtype == torch.int8\n    min_val, max_val = _find_per_channel_min_max(x, axis)\n    min_val_neg = torch.min(min_val, torch.zeros_like(min_val))\n    max_val_pos = torch.max(max_val, torch.zeros_like(max_val))\n    scale = torch.ones(min_val_neg.size(), dtype=torch.float32)\n    if symmetric_quant:\n        max_val_pos = torch.max(-min_val_neg, max_val_pos)\n        scale = max_val_pos / (float(quant_max - quant_min) / 2)\n        eps = torch.zeros_like(scale).fill_(EPS)\n        scale = torch.max(scale, eps)\n        return scale, None\n    else:\n        assert symmetric_quant\n\n\ndef _quantize_to_dtype(\n    x: torch.Tensor,\n    qconfig: TensorQConfig,\n    scale: torch.Tensor,\n    zero_point: Optional[torch.Tensor] = None,\n):\n    if zero_point is None:\n        zero_point = torch.zeros_like(scale)\n    return torch.ops.quantized_decomposed.quantize_per_channel(\n        x,\n        scale,\n        zero_point,\n        qconfig.axis,\n        qconfig.quant_min,\n        qconfig.quant_max,\n        qconfig.dtype,\n    )\n\n\ndef quantize_tensor(x: torch.Tensor, qconfig: TensorQConfig):\n    scale, zp = _find_qparams(x, qconfig)\n    x_int = _quantize_to_dtype(x, qconfig, scale, zp)\n    return x_int, scale, zp\n\n\ndef get_model_parallel_rank():\n    if USE_CUDA:\n        return dist.get_rank()\n    return xm.get_ordinal()\n\n\ndef get_model_parallel_world_size():\n    if USE_CUDA:\n        return dist.get_world_size()\n    return xm.xrt_world_size()\n\n\ndef get_model_parallel_group():\n    return None\n\n\nclass _CopyToModelParallelRegion(torch.autograd.Function):\n    \"\"\"Pass the input to the model parallel region.\"\"\"\n\n    @staticmethod\n    def forward(ctx, input_, groups, world_size, rank):  # type: ignore\n        ctx.groups, ctx.world_size, ctx.rank = groups, world_size, rank\n        return input_\n\n    @staticmethod\n    def backward(ctx, grad_output):  # type: ignore\n        groups, world_size, rank = ctx.groups, ctx.world_size, ctx.rank\n        return my_reduce(grad_output, groups, world_size, rank)\n\n\nclass _ReduceFromModelParallelRegion(torch.autograd.Function):\n    \"\"\"All-redcue the input from the model parallel region.\"\"\"\n\n    @staticmethod\n    def forward(ctx, input_, groups, world_size, rank):  # type: ignore\n        return my_reduce(input_, groups, world_size, rank)\n\n    @staticmethod\n    def backward(ctx, grad_output):  # type: ignore\n        return grad_output\n\n\nclass _ScatterToModelParallelRegion(torch.autograd.Function):\n    \"\"\"Split the input and keep only the corresponding chuck to the rank.\"\"\"\n\n    @staticmethod\n    def forward(ctx, input_, groups, world_size, rank):  # type: ignore\n        ctx.groups, ctx.world_size, ctx.rank = groups, world_size, rank\n        return my_split(input_, groups, world_size, rank)\n\n    @staticmethod\n    def backward(ctx, grad_output):  # type: ignore\n        groups, world_size, rank = ctx.groups, ctx.world_size, ctx.rank\n        return my_gather(grad_output, groups, world_size, rank)\n\n\nclass _GatherFromModelParallelRegion(torch.autograd.Function):\n    \"\"\"Gather the input from model parallel region and concatinate.\"\"\"\n\n    @staticmethod\n    def forward(ctx, input_, groups, world_size, rank):  # type: ignore\n        ctx.groups, ctx.world_size, ctx.rank = groups, world_size, rank\n        return my_gather(input_, groups, world_size, rank)\n\n    @staticmethod\n    def backward(ctx, grad_output):  # type: ignore\n        groups, world_size, rank = ctx.groups, ctx.world_size, ctx.rank\n        return my_split(grad_output, groups, world_size, rank)\n\n\n# -----------------\n# Helper functions.\n# -----------------\n\n\ndef copy_to_model_parallel_region(input_: torch.Tensor, groups, world_size,\n                                  rank) -> torch.Tensor:\n    return _CopyToModelParallelRegion.apply(input_, groups, world_size, rank)\n\n\ndef reduce_from_model_parallel_region(input_: torch.Tensor, groups, world_size,\n                                      rank) -> torch.Tensor:\n    return _ReduceFromModelParallelRegion.apply(input_, groups, world_size,\n                                                rank)\n\n\ndef scatter_to_model_parallel_region(input_: torch.Tensor, groups, world_size,\n                                     rank) -> torch.Tensor:\n    return _ScatterToModelParallelRegion.apply(input_, groups, world_size,\n                                               rank)\n\n\ndef gather_from_model_parallel_region(input_: torch.Tensor, groups, world_size,\n                                      rank) -> torch.Tensor:\n    return _GatherFromModelParallelRegion.apply(input_, groups, world_size,\n                                                rank)\n\n\ndef ensure_divisibility(numerator: int, denominator: int) -> None:\n    \"\"\"Ensure that numerator is divisible by the denominator.\"\"\"\n    assert numerator % denominator == 0, \"{} is not divisible by {}\".format(numerator, denominator)\n\n\ndef divide_and_check_no_remainder(numerator: int, denominator: int) -> int:\n    \"\"\"Ensure that numerator is divisible by the denominator and return\n    the division value.\"\"\"\n    ensure_divisibility(numerator, denominator)\n    return numerator // denominator\n\n\ndef split_tensor_along_last_dim(\n    tensor: torch.Tensor, num_partitions: int, contiguous_split_chunks: bool = False\n) -> Tuple[torch.Tensor, ...]:\n    \"\"\"Split a tensor along its last dimension.\n    Arguments:\n        tensor: input tensor.\n        num_partitions: number of partitions to split the tensor\n        contiguous_split_chunks: If True, make each chunk contiguous\n                                in memory.\n    \"\"\"\n    # Get the size and dimension.\n    last_dim = tensor.dim() - 1\n    last_dim_size = divide_and_check_no_remainder(tensor.size()[last_dim], num_partitions)\n    # Split.\n    tensor_list = torch.split(tensor, last_dim_size, dim=last_dim)\n    # Note: torch.split does not create contiguous tensors by default.\n    if contiguous_split_chunks:\n        return tuple(chunk.contiguous() for chunk in tensor_list)\n\n    return tensor_list\n\n# Below copied from fairscale/nn/model_parallel/layers.py\n\n\ndef my_reduce(input_: torch.Tensor, groups, world_size, rank) -> torch.Tensor:\n    \"\"\"All-reduce the the input tensor across model parallel group.\"\"\"\n    # Bypass the function if we are using only 1 GPU.\n    if world_size == 1:\n        return input_\n\n    # All-reduce.\n    if USE_CUDA:\n        input_ = torch.ops.c10d_functional.all_reduce(input_, \"sum\", TAG,\n                                                      RANKSET, GROUP_SIZE)\n    else:\n        input_ = xm.all_reduce(xm.REDUCE_SUM, input_, groups=groups)\n\n    return input_\n\n\ndef my_split(input_: torch.Tensor, groups, world_size, rank) -> torch.Tensor:\n    \"\"\"Split the tensor along its last dimension and keep the\n\n    corresponding slice.\n    \"\"\"\n    # Bypass the function if we are using only 1 GPU.\n    if world_size == 1:\n        return input_\n\n    # Split along last dimension.\n    input_list = split_tensor_along_last_dim(input_, world_size)\n\n    # Note: torch.split does not create contiguous tensors by default.\n    output = input_list[rank].contiguous()\n\n    return output\n\n\ndef my_gather(input_: torch.Tensor, groups, world_size, rank) -> torch.Tensor:\n    \"\"\"Gather tensors and concatinate along the last dimension.\"\"\"\n    # Bypass the function if we are using only 1 GPU.\n    if world_size == 1:\n        return input_\n\n    if USE_CUDA:\n        last_dim = input_.dim() - 1\n\n        # Using all_reduce to achieve all_gather as torch.ops.c10d_functional.all_gather_into_tensor\n        # is buggy in 16 bits.\n        size = input_.size(last_dim)\n        padding = [0] * (2 * input_.dim())\n        ordinal = rank\n        left, right = ordinal, world_size - 1 - ordinal\n        idx = input_.dim() - 1 - last_dim\n        padding[2 * idx] = left * size\n        padding[2 * idx + 1] = right * size\n        output = torch.ops.c10d_functional.all_reduce(F.pad(input_,\n                                                            padding), \"sum\",\n                                                      TAG, RANKSET, GROUP_SIZE)\n    else:\n        output = xm.all_gather(input_, dim=-1, groups=groups)\n\n    return output\n\n\ndef _initialize_affine_weight(\n    weight: torch.Tensor,\n    out_features: int,\n    in_features: int,\n    per_partition_size: int,\n    partition_dim: int,\n    init_method: Callable[[torch.Tensor], torch.Tensor],\n    world_size: int,\n    rank: int,\n    stride: int = 1,\n    return_master_weight: bool = False,\n) -> Optional[torch.Tensor]:\n    \"\"\"Initialize affine weight for model parallel.\n\n    Build the master weight on all processes and scatter\n    the relevant chunk.\n    \"\"\"\n\n    # If we only use 1 process for model parallelism, bypass scatter.\n    if world_size == 1:\n        init_method(weight)\n        if return_master_weight:\n            return weight\n        return None\n\n    # Initialize master weight\n    master_weight = torch.empty(out_features,\n                                in_features,\n                                dtype=weight.dtype,\n                                requires_grad=False)\n    init_method(master_weight)\n\n    # Split and copy\n    per_partition_per_stride_size = divide_and_check_no_remainder(\n        per_partition_size, stride)\n    weight_list = torch.split(master_weight,\n                              per_partition_per_stride_size,\n                              dim=partition_dim)\n    my_weight_list = weight_list[rank::world_size]\n\n    with torch.no_grad():\n        torch.cat(my_weight_list, dim=partition_dim, out=weight)\n    if return_master_weight:\n        return master_weight\n    return None\n\n\nclass ParallelEmbedding(torch.nn.Module):\n    \"\"\"Embedding parallelized in the embedding dimension.\n\n    This is mainly adapted from torch.nn.Embedding and all the default\n    values are kept.\n    Arguments:\n        num_embeddings: vocabulary size.\n        embedding_dim: size of hidden state.\n        init_method: method to initialize weights.\n    \"\"\"\n\n    def __init__(\n        self,\n        num_embeddings: int,\n        embedding_dim: int,\n        padding_idx: Optional[int] = None,\n        max_norm: Optional[float] = None,\n        norm_type: float = 2.0,\n        scale_grad_by_freq: bool = False,\n        sparse: bool = False,\n        init_method: Callable[[torch.Tensor],\n                              torch.Tensor] = init.xavier_normal_,\n        keep_master_weight_for_test: bool = False,\n        world_size: Optional[int] = None,\n        rank: Optional[int] = None,\n        groups: Optional[List] = None,\n        quant: bool = False,\n    ) -> None:\n        super(ParallelEmbedding, self).__init__()\n\n        if world_size is None:\n            self.groups = get_model_parallel_group()\n            self.world_size = get_model_parallel_world_size()\n            self.rank = get_model_parallel_rank()\n        else:\n            self.groups = groups\n            self.world_size = world_size\n            self.rank = rank\n\n        # Keep the input dimensions.\n        self.num_embeddings = num_embeddings\n        self.embedding_dim = embedding_dim\n        self.padding_idx = padding_idx\n        self.max_norm = max_norm\n        self.norm_type = norm_type\n        self.scale_grad_by_freq = scale_grad_by_freq\n        self.sparse = sparse\n        self._weight = None\n        self.quant = quant\n        # Divide the weight matrix along the embedding dimension.\n        self.embedding_dim_per_partition = divide_and_check_no_remainder(\n            self.embedding_dim, self.world_size)\n\n        # Allocate weights.\n        if quant:\n            self.weight = Parameter(\n                torch.empty(\n                    (self.num_embeddings, self.embedding_dim_per_partition),\n                    dtype=torch.int8,\n                ),\n                requires_grad=False,\n            )\n            self.weight_scaler = Parameter(torch.Tensor(self.num_embeddings))\n        else:\n            self.weight = Parameter(\n                torch.Tensor(self.num_embeddings,\n                             self.embedding_dim_per_partition))\n\n        # And initialize.\n        _initialize_affine_weight(\n            self.weight,\n            self.num_embeddings,\n            self.embedding_dim,\n            self.embedding_dim_per_partition,\n            1,\n            init_method,\n            self.world_size,\n            self.rank,\n            stride=1,\n            return_master_weight=False,\n        )\n\n    def forward(self, input_: torch.Tensor) -> torch.Tensor:  # type: ignore\n        input_parallel = copy_to_model_parallel_region(input_, self.groups,\n                                                       self.world_size,\n                                                       self.rank)\n        # PyTorch eager and inductor do not accept negative values in the input to embedding\n        # layers. Take the modulus to avoid this error.\n        if USE_CUDA:\n            input_parallel = torch.remainder(input_parallel,\n                                             self.weight.shape[0])\n        weight = self.weight\n        if self.quant:\n            weight = weight * self.weight_scaler.unsqueeze(-1)\n        output_parallel = F.embedding(\n            input_parallel,\n            weight,\n            self.padding_idx,\n            self.max_norm,\n            self.norm_type,\n            self.scale_grad_by_freq,\n            self.sparse,\n        )\n        output = gather_from_model_parallel_region(output_parallel,\n                                                   self.groups,\n                                                   self.world_size, self.rank)\n        return output\n\n\nclass ColumnParallelLinear(torch.nn.Module):\n    \"\"\"Linear layer with column parallelism.\n\n    The linear layer is defined as Y = XA + b. A is parallelized along\n    its second dimension as A = [A_1, ..., A_p].\n\n    Arguments:\n        in_features: first dimension of matrix A.\n        out_features: second dimension of matrix A.\n        bias: If true, add bias\n        gather_output: If true, call all-gether on output and make Y available to\n          all GPUs, otherwise, every GPU will have its output which is Y_i = XA_i\n        init_method: method to initialize weights. Note that bias is always set to\n          zero.\n        stride: For the strided linear layers.\n        keep_master_weight_for_test: This was added for testing and should be set\n          to False. It returns the master weights used for initialization.\n    \"\"\"\n\n    def __init__(\n        self,\n        in_features: int,\n        out_features: int,\n        bias: bool = True,\n        gather_output: bool = True,\n        init_method: Callable[[torch.Tensor],\n                              torch.Tensor] = init.xavier_normal_,\n        stride: int = 1,\n        keep_master_weight_for_test: bool = False,\n        world_size: Optional[int] = None,\n        rank: Optional[int] = None,\n        groups: Optional[List] = None,\n        quant: bool = False,\n    ) -> None:\n        super(ColumnParallelLinear, self).__init__()\n\n        if world_size is None:\n            self.groups = get_model_parallel_group()\n            self.world_size = get_model_parallel_world_size()\n            self.rank = get_model_parallel_rank()\n        else:\n            self.groups = groups\n            self.world_size = world_size\n            self.rank = rank\n\n        # Keep input parameters\n        self.in_features = in_features\n        self.out_features = out_features\n        self.gather_output = gather_output\n        self.quant = quant\n        # Divide the weight matrix along the last dimension.\n        self.output_size_per_partition = divide_and_check_no_remainder(\n            out_features, self.world_size)\n\n        # Parameters.\n        # Note: torch.nn.functional.linear performs XA^T + b and as a result\n        # we allocate the transpose.\n        if quant:\n            self.weight = Parameter(\n                torch.empty(\n                    (self.output_size_per_partition, self.in_features),\n                    dtype=torch.int8,\n                ),\n                requires_grad=False,\n            )\n            self.weight_scaler = Parameter(\n                torch.Tensor(self.output_size_per_partition))\n        else:\n            self.weight = Parameter(\n                torch.Tensor(self.output_size_per_partition, self.in_features))\n        if bias:\n            self.bias = Parameter(torch.Tensor(self.output_size_per_partition))\n            # Always initialize bias to zero.\n            with torch.no_grad():\n                self.bias.zero_()\n        else:\n            self.register_parameter('bias', None)\n\n        # Initialize weight.\n        self.master_weight = _initialize_affine_weight(\n            self.weight,\n            self.out_features,\n            self.in_features,\n            self.output_size_per_partition,\n            0,\n            init_method,\n            self.world_size,\n            self.rank,\n            stride=stride,\n            return_master_weight=keep_master_weight_for_test,\n        )\n\n    def get_master_weight(self) -> torch.Tensor:\n        return gather_from_model_parallel_region(\n            self.weight.data.transpose(0, 1),\n            self.groups,\n            self.world_size,\n            self.rank,\n        ).transpose_(0, 1)\n\n    def set_quantize(self):\n        assert not self.quant\n        self.weight = Parameter(\n            torch.empty((self.output_size_per_partition, self.in_features),\n                        dtype=torch.int8),\n            requires_grad=False,\n        )\n        self.weight_scaler = Parameter(\n            torch.Tensor(self.output_size_per_partition))\n        self.quant = True\n\n    def quantize(self):\n        assert not self.quant\n        fp_w = deepcopy(self.weight.data)\n        orig_dtype = fp_w.dtype\n        fp_w = fp_w.to(torch.float32)\n        self.weight = Parameter(\n            torch.empty((self.output_size_per_partition, self.in_features),\n                        dtype=torch.int8),\n            requires_grad=False,\n        )\n        self.weight_scaler = Parameter(\n            torch.Tensor(self.output_size_per_partition))\n        qconfig = TensorQConfig(axis=0)\n        self.weight.data, scale, zero_point = quantize_tensor(fp_w, qconfig)\n        self.weight_scaler.data = scale.to(orig_dtype)\n        self.quant = True\n\n    def forward(self, input_: torch.Tensor) -> torch.Tensor:  # type: ignore\n        # Set up backprop all-reduce.\n        input_parallel = copy_to_model_parallel_region(input_, self.groups,\n                                                       self.world_size,\n                                                       self.rank)\n        # Matrix multiply.\n        if self.quant and USE_CUDA:\n            # GPUs do not support mixed int8 bf16 computation. Scale int8 weights to bf16 before linear.\n            scaled_weight = self.weight * self.weight_scaler\n            output_parallel = F.linear(input_parallel, scaled_weight, self.bias)\n        elif self.quant:\n            output_parallel = F.linear(input_parallel, self.weight, self.bias)\n            output_parallel = output_parallel * self.weight_scaler\n        else:\n            output_parallel = F.linear(input_parallel, self.weight, self.bias)\n        if self.gather_output:\n            # All-gather across the partitions.\n            output = gather_from_model_parallel_region(output_parallel,\n                                                       self.groups,\n                                                       self.world_size,\n                                                       self.rank)\n        else:\n            output = output_parallel\n        return output\n\n\nclass RowParallelLinear(torch.nn.Module):\n    \"\"\"Linear layer with row parallelism.\n\n    The linear layer is defined as Y = XA + b. A is parallelized along\n    its first dimension and X along its second dimension as:\n               -   -\n              | A_1 |\n              | .   |\n          A = | .   |        X = [X_1, ..., X_p]\n              | .   |\n              | A_p |\n               -   -\n    Arguments:\n        in_features: first dimension of matrix A.\n        out_features: second dimension of matrix A.\n        bias: If true, add bias. Note that bias is not parallelized.\n        input_is_parallel: If true, we assume that the input is already split\n          across the GPUs and we do not split again.\n        init_method: method to initialize weights. Note that bias is always set to\n          zero.\n        stride: For the strided linear layers.\n        keep_master_weight_for_test: This was added for testing and should be set\n          to False. It returns the master weights used for initialization.\n    \"\"\"\n\n    def __init__(\n        self,\n        in_features: int,\n        out_features: int,\n        bias: bool = True,\n        input_is_parallel: bool = False,\n        init_method: Callable[[torch.Tensor],\n                              torch.Tensor] = init.xavier_normal_,\n        stride: int = 1,\n        keep_master_weight_for_test: bool = False,\n        world_size: Optional[int] = None,\n        rank: Optional[int] = None,\n        groups: Optional[List] = None,\n        quant: bool = False,\n    ):\n        super(RowParallelLinear, self).__init__()\n\n        if world_size is None:\n            self.groups = get_model_parallel_group()\n            self.world_size = get_model_parallel_world_size()\n            self.rank = get_model_parallel_rank()\n        else:\n            self.groups = groups\n            self.world_size = world_size\n            self.rank = rank\n\n        # Keep input parameters\n        self.in_features = in_features\n        self.out_features = out_features\n        self.input_is_parallel = input_is_parallel\n        self.quant = quant\n        # Divide the weight matrix along the last dimension.\n        self.input_size_per_partition = divide_and_check_no_remainder(\n            in_features, self.world_size)\n\n        # Parameters.\n        # Note: torch.nn.functional.linear performs XA^T + b and as a result\n        # we allocate the transpose.\n        if quant:\n            self.weight = Parameter(\n                torch.empty(\n                    (self.out_features, self.input_size_per_partition),\n                    dtype=torch.int8,\n                ),\n                requires_grad=False,\n            )\n            self.weight_scaler = Parameter(torch.Tensor(self.out_features))\n        else:\n            self.weight = Parameter(\n                torch.Tensor(self.out_features, self.input_size_per_partition))\n        if bias:\n            self.bias = Parameter(torch.Tensor(self.out_features))\n            # Always initialize bias to zero.\n            with torch.no_grad():\n                self.bias.zero_()\n        else:\n            self.register_parameter('bias', None)\n\n        # Initialize weight.\n        self.master_weight = _initialize_affine_weight(\n            self.weight,\n            self.out_features,\n            self.in_features,\n            self.input_size_per_partition,\n            1,\n            init_method,\n            self.world_size,\n            self.rank,\n            stride=stride,\n            return_master_weight=keep_master_weight_for_test,\n        )\n\n    def get_master_weight(self) -> torch.Tensor:\n        return gather_from_model_parallel_region(self.weight.data, self.groups,\n                                                 self.world_size, self.rank)\n\n    def set_quantize(self):\n        assert not self.quant\n        self.weight = Parameter(\n            torch.empty((self.out_features, self.input_size_per_partition),\n                        dtype=torch.int8),\n            requires_grad=False,\n        )\n        self.weight_scaler = Parameter(torch.Tensor(self.out_features))\n        self.quant = True\n\n    def quantize(self):\n        assert not self.quant\n        fp_w = deepcopy(self.weight.data)\n        orig_dtype = fp_w.dtype\n        fp_w = fp_w.to(torch.float32)\n        self.weight = Parameter(\n            torch.empty((self.out_features, self.input_size_per_partition),\n                        dtype=torch.int8),\n            requires_grad=False,\n        )\n        self.weight_scaler = Parameter(torch.Tensor(self.out_features))\n        qconfig = TensorQConfig(axis=0)\n        self.weight.data, scale, zero_point = quantize_tensor(fp_w, qconfig)\n        self.weight_scaler.data = scale.to(orig_dtype)\n        self.quant = True\n\n    def forward(self, input_: torch.Tensor) -> torch.Tensor:  # type:ignore\n        # Set up backprop all-reduce.\n        if self.input_is_parallel:\n            input_parallel = input_\n        else:\n            input_parallel = scatter_to_model_parallel_region(\n                input_, self.groups, self.world_size, self.rank)\n        # Matrix multiply.\n        if self.quant and USE_CUDA:\n            # GPUs do not support mixed int8 bf16 computation. Scale int8 weights to bf16 before linear.\n            scaled_weight = self.weight * self.weight_scaler\n            output_parallel = F.linear(input_parallel, scaled_weight, self.bias)\n        elif self.quant:\n            output_parallel = F.linear(input_parallel, self.weight, self.bias)\n            output_parallel = output_parallel * self.weight_scaler\n        else:\n            output_parallel = F.linear(input_parallel, self.weight)\n        # All-reduce across all the partitions.\n        output_ = reduce_from_model_parallel_region(output_parallel,\n                                                    self.groups,\n                                                    self.world_size, self.rank)\n        if self.bias is not None:\n            output = output_ + self.bias\n        else:\n            output = output_\n        return output\n",
        "scripts/run.py": "# Copyright 2024 Google LLC\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\n\nimport contextlib\nimport random\n\nimport numpy as np\nimport torch\nfrom absl import app, flags\n\nfrom gemma import config\nfrom gemma import model as gemma_model\n\n# Define flags\nFLAGS = flags.FLAGS\n\nflags.DEFINE_string('ckpt', None, 'Path to the checkpoint file.', required=True)\nflags.DEFINE_string('variant', '4b', 'Model variant.')\nflags.DEFINE_string('device', 'cpu', 'Device to run the model on.')\nflags.DEFINE_integer('output_len', 10, 'Length of the output sequence.')\nflags.DEFINE_integer('seed', 12345, 'Random seed.')\nflags.DEFINE_boolean('quant', False, 'Whether to use quantization.')\nflags.DEFINE_string('prompt', 'What are large language models?', 'Input prompt for the model.')\n\n# Define valid text only model variants\n_VALID_MODEL_VARIANTS = ['2b', '2b-v2', '7b', '9b', '27b', '1b']\n\n# Define valid devices\n_VALID_DEVICES = ['cpu', 'cuda']\n\n# Validator function for the 'variant' flag\ndef validate_variant(variant):\n    if variant not in _VALID_MODEL_VARIANTS:\n        raise ValueError(f'Invalid variant: {variant}. Valid variants are: {_VALID_MODEL_VARIANTS}')\n    return True\n\n# Validator function for the 'device' flag\ndef validate_device(device):\n    if device not in _VALID_DEVICES:\n        raise ValueError(f'Invalid device: {device}. Valid devices are: {_VALID_DEVICES}')\n    return True\n\n# Register the validator for the 'variant' flag\nflags.register_validator('variant', validate_variant, message='Invalid model variant.')\n\n# Register the validator for the 'device' flag\nflags.register_validator('device', validate_device, message='Invalid device.')\n\n@contextlib.contextmanager\ndef _set_default_tensor_type(dtype: torch.dtype):\n    \"\"\"Sets the default torch dtype to the given dtype.\"\"\"\n    torch.set_default_dtype(dtype)\n    yield\n    torch.set_default_dtype(torch.float)\n\ndef main(_):\n    # Construct the model config.\n    model_config = config.get_model_config(FLAGS.variant)\n    model_config.dtype = \"float32\"\n    model_config.quant = FLAGS.quant\n\n    # Seed random.\n    random.seed(FLAGS.seed)\n    np.random.seed(FLAGS.seed)\n    torch.manual_seed(FLAGS.seed)\n\n    # Create the model and load the weights.\n    device = torch.device(FLAGS.device)\n    with _set_default_tensor_type(model_config.get_dtype()):\n        model = gemma_model.GemmaForCausalLM(model_config)\n        model.load_weights(FLAGS.ckpt)\n        model = model.to(device).eval()\n    print(\"Model loading done\")\n\n    # Generate the response.\n    result = model.generate(FLAGS.prompt, device, output_len=FLAGS.output_len)\n\n    # Print the prompts and results.\n    print('======================================')\n    print(f'PROMPT: {FLAGS.prompt}')\n    print(f'RESULT: {result}')\n    print('======================================')\n\nif __name__ == \"__main__\":\n    app.run(main)\n\n\n# How to run this script:\n\n# Example command (replace with your actual paths and values):\n# python scripts/run.py --device=cpu --ckpt=/path/to/your/pytorch_checkpoint/model.ckpt --output_len=2 --prompt=\"The name of the capital of Italy is\"\n# Important:\n# - Replace '/path/to/your/pytorch_checkpoint/model.ckpt' with the actual path to your checkpoint file.\n# - Choose the correct --variant (model size).\n# - Use --device=cuda if you have a GPU; otherwise, use --device=cpu.",
        "scripts/run_multimodal.py": "# Copyright 2024 Google LLC\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\n\nimport contextlib\nimport random\n\nfrom absl import app\nfrom absl import flags\nimport numpy as np\nfrom PIL import Image\nimport torch\n\nfrom gemma import config\nfrom gemma import gemma3_model\n\n# Define flags\nFLAGS = flags.FLAGS\n\n_CKPT = flags.DEFINE_string(\n    'ckpt', None, 'Path to the checkpoint file.', required=True\n)\n_VARIANT = flags.DEFINE_string('variant', '4b', 'Model variant.')\n_DEVICE = flags.DEFINE_string('device', 'cpu', 'Device to run the model on.')\n_OUTPUT_LEN = flags.DEFINE_integer(\n    'output_len', 10, 'Length of the output sequence.'\n)\n_SEED = flags.DEFINE_integer('seed', 12345, 'Random seed.')\n_QUANT = flags.DEFINE_boolean('quant', False, 'Whether to use quantization.')\n\n# Define valid multimodal model variants\n_VALID_MODEL_VARIANTS = ['4b', '12b', '27b_v3']\n\n# Define valid devices\n_VALID_DEVICES = ['cpu', 'cuda']\n\n\n# Validator function for the 'variant' flag\ndef validate_variant(variant):\n  if variant not in _VALID_MODEL_VARIANTS:\n    raise ValueError(\n        f'Invalid variant: {variant}. Valid variants are:'\n        f' {_VALID_MODEL_VARIANTS}'\n    )\n  return True\n\n\n# Validator function for the 'device' flag\ndef validate_device(device):\n  if device not in _VALID_DEVICES:\n    raise ValueError(\n        f'Invalid device: {device}. Valid devices are: {_VALID_DEVICES}'\n    )\n  return True\n\n\n# Register the validator for the 'variant' flag\nflags.register_validator(\n    'variant', validate_variant, message='Invalid model variant.'\n)\n\n# Register the validator for the 'device' flag\nflags.register_validator('device', validate_device, message='Invalid device.')\n\n\n@contextlib.contextmanager\ndef _set_default_tensor_type(dtype: torch.dtype):\n  \"\"\"Sets the default torch dtype to the given dtype.\"\"\"\n  torch.set_default_dtype(dtype)\n  yield\n  torch.set_default_dtype(torch.float)\n\n\ndef main(_):\n  # Construct the model config.\n  model_config = config.get_model_config(_VARIANT.value)\n  model_config.dtype = 'float32'\n  model_config.quant = _QUANT.value\n  image_paths = {\"cow_in_beach\": \"scripts/images/cow_in_beach.jpg\",\n                   \"lilly\": \"scripts/images/lilly.jpg\",\n                   \"sunflower\": \"scripts/images/sunflower.JPG\",\n                   'golden_test_image': (\n                           'scripts/images/test_image.jpg'\n                    ),\n    }\n\n  image = {}\n  for key in image_paths:\n    try:\n      image[key] = Image.open(image_paths[key]) # Open local file\n      image[key].show()\n    except IOError as e:\n      print(f\"Error loading image: {e}\")\n      exit()\n\n  # Seed random.\n  random.seed(_SEED.value)\n  np.random.seed(_SEED.value)\n  torch.manual_seed(_SEED.value)\n\n  # Create the model and load the weights.\n  device = torch.device(_DEVICE.value)\n  with _set_default_tensor_type(model_config.get_dtype()):\n    model = gemma3_model.Gemma3ForMultimodalLM(model_config)\n    model.load_state_dict(torch.load(_CKPT.value)['model_state_dict'])\n    # model.load_weights(_CKPT.value)\n    model = model.to(device).eval()\n  print('Model loading done')\n\n  # Generate text only.\n  result = model.generate(\n      [\n          [\n              '<start_of_turn>user The capital of Italy'\n              ' is?<end_of_turn>\\n<start_of_turn>model'\n          ],\n          [\n              '<start_of_turn>user What is your'\n              ' purpose?<end_of_turn>\\n<start_of_turn>model'\n          ],\n      ],\n      device,\n      output_len=_OUTPUT_LEN.value,\n  )\n\n  # Print the results.\n  print('======================================')\n  print(f'Text only RESULT: {result}')\n  print('======================================')\n\n  # Generate golden Gemax test image.\n  result = model.generate(\n      [[\n          '<start_of_turn>user\\n',\n          image['golden_test_image'],\n          'Caption this image. <end_of_turn>\\n<start_of_turn>model',\n      ]],\n      device,\n      output_len=_OUTPUT_LEN.value,\n  )\n\n  # Print the result.\n  print('======================================')\n  print(f'Golden test image RESULT: {result}')\n  print('======================================')\n\n  # Generate text and image.\n  result = model.generate(\n      [[\n          '<start_of_turn>user\\n',\n          image['cow_in_beach'],\n          (\n              'The name of the animal in the image is'\n              ' <end_of_turn>\\n<start_of_turn>model'\n          ),\n      ]],\n      device,\n      output_len=_OUTPUT_LEN.value,\n  )\n\n  # Print the result.\n  print('======================================')\n  print(f'Single image RESULT: {result}')\n  print('======================================')\n\n  # Generate interleave text and multiple images.\n  result = model.generate(\n      [[\n          '<start_of_turn>user\\nThis image',\n          image['lilly'],\n          'and this image',\n          image['sunflower'],\n          'are similar because? <end_of_turn>\\n<start_of_turn>model',\n      ]],\n      device,\n      output_len=_OUTPUT_LEN.value,\n  )\n\n  # Print the result.\n  print('======================================')\n  print(f'Interleave images RESULT: {result}')\n  print('======================================')\n\n\nif __name__ == '__main__':\n  app.run(main)\n",
        "scripts/run_xla.py": "# Copyright 2024 Google LLC\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\nimport argparse\nimport contextlib\nimport os\nimport random\nimport socket\nimport sys\nfrom typing import List, Union\n\nimport numpy as np\nimport torch\nimport torch.multiprocessing\n\nfrom gemma.config import GemmaConfig, get_model_config\nfrom gemma.model_xla import GemmaForCausalLM\nfrom gemma.tokenizer import Tokenizer\nimport gemma.xla_model_parallel as xla_model_parallel\n\nUSE_CUDA = os.environ.get('USE_CUDA', False)\nif not USE_CUDA:\n    import torch_xla.core.xla_model as xm\n    import torch_xla.distributed.xla_multiprocessing as xmp\nelse:\n    # Choose an available port.\n    with contextlib.closing(socket.socket(socket.AF_INET,\n                                          socket.SOCK_STREAM)) as s:\n        s.bind(('', 0))\n        s.setsockopt(socket.SOL_SOCKET, socket.SO_REUSEADDR, 1)\n        MASTER_PORT = str(s.getsockname()[1])\n\n\n@contextlib.contextmanager\ndef _set_default_tensor_type(dtype: torch.dtype):\n    \"\"\"Sets the default torch dtype to the given dtype.\"\"\"\n    torch.set_default_dtype(dtype)\n    yield\n    torch.set_default_dtype(torch.float)\n\n\ndef generate(\n    i: int,\n    model_config: GemmaConfig,\n    ckpt_path: str,\n    prompts: List[str],\n    output_lens: List[int],\n    temperatures: Union[List[float], None],\n    top_ps: List[float],\n    top_ks: List[int],\n    seed: int\n):\n    random.seed(seed)\n    np.random.seed(seed)\n    torch.manual_seed(seed)\n    if USE_CUDA:\n        os.environ['MASTER_ADDR'] = '127.0.0.1'\n        os.environ['MASTER_PORT'] = MASTER_PORT\n        if not torch.distributed.is_initialized():\n            torch.distributed.init_process_group(\n                \"nccl\",\n                rank=int(os.environ.get(\"RANK\", 0)),\n                world_size=int(os.environ.get(\"WORLD_SIZE\", 1)))\n        xla_model_parallel.set_g_group()\n        local_rank = int(os.environ.get(\"LOCAL_RANK\", 0))\n        device = torch.device(\"cuda\", local_rank)\n        torch.cuda.set_device(local_rank)\n    else:\n        device = xm.xla_device()\n        xm.set_rng_state(seed, device)\n\n    rank = xla_model_parallel.get_model_parallel_rank()\n    world_size = xla_model_parallel.get_model_parallel_world_size()\n    if rank > 0:\n        sys.stdout = open(os.devnull, 'w')\n\n    # build, load and compile model.\n    with _set_default_tensor_type(model_config.get_dtype()):\n        model = GemmaForCausalLM(model_config, world_size, rank, device)\n        model.load_weights(ckpt_path)\n        model = model.to(device).eval()\n\n    # create tokenizer.\n    tokenizer = Tokenizer(model_config.tokenizer)\n\n    prompt_tokens = [tokenizer.encode(prompt) for prompt in prompts]\n    min_prompt_len = min(len(p) for p in prompt_tokens)\n\n    batch_size = len(prompts)\n    if temperatures is not None:\n        assert batch_size == len(temperatures)\n    assert batch_size == len(top_ps)\n    assert batch_size == len(top_ks)\n    max_seq_len = max([len(p) + o for p, o in zip(prompt_tokens, output_lens)])\n    assert max_seq_len <= model_config.max_position_embeddings\n    if model_config.num_key_value_heads < world_size:\n        assert world_size % model_config.num_key_value_heads == 0\n        n_local_heads = 1\n    else:\n        assert model_config.num_key_value_heads % world_size == 0\n        n_local_heads = model_config.num_key_value_heads // world_size\n\n    # build KV caches\n    kv_caches = []\n    for _ in range(model_config.num_hidden_layers):\n        k_cache = torch.zeros(\n            size=(batch_size, max_seq_len, n_local_heads,\n                  model_config.head_dim),\n            dtype=model_config.get_dtype(),\n            device=device,\n        )\n        v_cache = torch.zeros(\n            size=(batch_size, max_seq_len, n_local_heads,\n                  model_config.head_dim),\n            dtype=model_config.get_dtype(),\n            device=device,\n        )\n        kv_caches.append((k_cache, v_cache))\n\n    # prepare inputs\n    token_ids_tensor = torch.full((batch_size, max_seq_len),\n                                  tokenizer.pad_id,\n                                  dtype=torch.int64)\n    input_token_ids_tensor = torch.full((batch_size, min_prompt_len),\n                                        tokenizer.pad_id,\n                                        dtype=torch.int64)\n    for i, p in enumerate(prompt_tokens):\n        token_ids_tensor[i, :len(p)] = torch.tensor(p)\n        input_token_ids_tensor[i, :min_prompt_len] = torch.tensor(\n            p[:min_prompt_len])\n    token_ids_tensor = token_ids_tensor.to(device)\n    prompt_mask_tensor = token_ids_tensor != tokenizer.pad_id\n    input_token_ids_tensor = input_token_ids_tensor.to(device)\n    input_positions_tensor = torch.arange(0, min_prompt_len,\n                                          dtype=torch.int64).to(device)\n    mask_tensor = torch.full((1, 1, max_seq_len, max_seq_len),\n                             -2.3819763e38).to(torch.float)\n    mask_tensor = torch.triu(mask_tensor, diagonal=1).to(device)\n    curr_mask_tensor = mask_tensor.index_select(2, input_positions_tensor)\n    output_positions_tensor = torch.LongTensor([min_prompt_len - 1]).to(device)\n    temperatures_tensor = None if not temperatures else torch.FloatTensor(temperatures).to(device)\n    top_ps_tensor = torch.FloatTensor(top_ps).to(device)\n    top_ks_tensor = torch.LongTensor(top_ks).to(device)\n    output_index = torch.tensor(min_prompt_len, dtype=torch.int64).to(device)\n    if not USE_CUDA:\n        xm.mark_step()\n\n    # Prefill up to min_prompt_len tokens, then treat other prefill as decode and ignore output.\n    for i in range(max_seq_len - min_prompt_len):\n        next_token_ids, _ = model(\n            input_token_ids=input_token_ids_tensor,\n            input_positions=input_positions_tensor,\n            kv_write_indices=None,\n            kv_caches=kv_caches,\n            mask=curr_mask_tensor,\n            output_positions=output_positions_tensor,\n            temperatures=temperatures_tensor,\n            top_ps=top_ps_tensor,\n            top_ks=top_ks_tensor,\n        )\n        curr_prompt_mask = prompt_mask_tensor.index_select(\n            1, output_index).squeeze(dim=1)\n        curr_token_ids = token_ids_tensor.index_select(\n            1, output_index).squeeze(dim=1)\n        output_token_ids = torch.where(curr_prompt_mask, curr_token_ids,\n                                       next_token_ids).unsqueeze(dim=1)\n        token_ids_tensor.index_copy_(1, output_index, output_token_ids)\n\n        input_token_ids_tensor = output_token_ids\n        input_positions_tensor = output_index.unsqueeze(dim=-1)\n        curr_mask_tensor = mask_tensor.index_select(2, input_positions_tensor)\n        output_positions_tensor = torch.tensor(0, dtype=torch.int64).to(device)\n        output_index = output_index + 1\n        if not USE_CUDA:\n            xm.mark_step()\n\n    # Detokenization.\n    token_ids = token_ids_tensor.tolist()\n    results = []\n    for i, tokens in enumerate(token_ids):\n        trimmed_output = tokens[len(prompt_tokens[i]):len(prompt_tokens[i]) +\n                                output_lens[i]]\n        if tokenizer.eos_id in trimmed_output:\n            eos_index = trimmed_output.index(tokenizer.eos_id)\n            trimmed_output = trimmed_output[:eos_index]\n        results.append(tokenizer.decode(trimmed_output))\n\n    for prompt, result in zip(prompts, results):\n        print('======================================')\n        print(f'PROMPT: {prompt}')\n        print(f'RESULT: {result}')\n        print('======================================')\n\n\ndef main(args):\n    model_config = get_model_config(args.variant)\n    model_config.quant = args.quant\n\n    prompts = [args.prompt]\n    n = len(prompts)\n    output_lengths = [args.output_len] * n\n    temperatures = [0.95] * n\n    top_ps = [1.0] * n\n    top_ks = [100] * n\n\n    if USE_CUDA:\n        os.environ['MASTER_ADDR'] = '127.0.0.1'\n        os.environ['MASTER_PORT'] = MASTER_PORT\n        if not torch.distributed.is_initialized():\n            torch.distributed.init_process_group(\n                \"nccl\",\n                rank=int(os.environ.get(\"RANK\", 0)),\n                world_size=int(os.environ.get(\"WORLD_SIZE\", 1)))\n        xla_model_parallel.set_g_group()\n        torch.multiprocessing.spawn(\n            generate,\n            args=(\n                model_config,\n                args.ckpt,\n                prompts,\n                output_lengths,\n                temperatures,\n                top_ps,\n                top_ks,\n                args.seed,\n            ),\n        )\n    else:\n        xmp.spawn(\n            generate,\n            args=(\n                model_config,\n                args.ckpt,\n                prompts,\n                output_lengths,\n                temperatures,\n                top_ps,\n                top_ks,\n                args.seed,\n            ),\n        )\n\n\nif __name__ == '__main__':\n    parser = argparse.ArgumentParser()\n    parser.add_argument(\"--ckpt\", type=str, required=True)\n    parser.add_argument(\"--variant\",\n                        type=str,\n                        default=\"2b\",\n                        choices=[\"2b\", \"2b-v2\", \"7b\", \"9b\", \"27b\"])\n    parser.add_argument(\"--output_len\", type=int, default=4)\n    parser.add_argument(\"--seed\", type=int, default=12345)\n    parser.add_argument(\"--quant\", action='store_true')\n    parser.add_argument(\"--prompt\", type=str, default=\"The meaning of life is\")\n    args = parser.parse_args()\n\n    main(args)\n",
        "setup.py": "# Copyright 2024 Google LLC\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\nimport io\nimport os\nfrom typing import List\n\nimport setuptools\n\nROOT_DIR = os.path.dirname(__file__)\n\n\ndef get_path(*filepath) -> str:\n    return os.path.join(ROOT_DIR, *filepath)\n\n\ndef read_readme() -> str:\n    \"\"\"Read the README file.\"\"\"\n    return io.open(get_path(\"README.md\"), \"r\", encoding=\"utf-8\").read()\n\n\ndef get_requirements() -> List[str]:\n    \"\"\"Get Python package dependencies from requirements.txt.\"\"\"\n    with open(get_path(\"requirements.txt\")) as f:\n        requirements = f.read().strip().split(\"\\n\")\n    return requirements\n\n\nsetuptools.setup(\n    name=\"gemma\",\n    version=\"0.1\",\n    author=\"Gemma contributors\",\n    license=\"Apache 2.0\",\n    description=(\"Gemma model implementation\"),\n    long_description=read_readme(),\n    long_description_content_type=\"text/markdown\",\n    classifiers=[\n        \"Programming Language :: Python :: 3.8\",\n        \"Programming Language :: Python :: 3.9\",\n        \"Programming Language :: Python :: 3.10\",\n        \"Programming Language :: Python :: 3.11\",\n        \"License :: OSI Approved :: Apache Software License\",\n        \"Topic :: Scientific/Engineering :: Artificial Intelligence\",\n    ],\n    packages=setuptools.find_packages(exclude=(\"benchmarks\", \"docs\",\n                                               \"examples\", \"tests\")),\n    python_requires=\">=3.11\",\n    install_requires=get_requirements(),\n)\n"
    }
}