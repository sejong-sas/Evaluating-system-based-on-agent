{
  "2-3 (API)": "",
  "3-1 (Pre-training)": "For Gemma-3, including the 4 B checkpoint, the authors state that the pre-training optimization recipe is largely the same as the earlier Gemma-2 run, with only modest architectural tweaks. Every Gemma-3 scale—1 B, 4 B, 12 B and 27 B—is explicitly \"trained with knowledge distillation (Hinton et al., 2015).\"  The token budget has been increased relative to Gemma-2; the 4 B model is exposed to 4 trillion tokens (other sizes see 2 T, 12 T or 14 T).  Throughout the pre-training benchmark suite the team observes \"consistent improvement over STEM abilities,\" and for coding tasks they single out clear gains for the 4 B and 12 B sizes (the 27 B does not share this code boost).  Overall, the work describes a Gemma-3-4B training pipeline that mirrors Gemma-2 in spirit while scaling data and relying on systematic knowledge-distillation to supervise the run.",
  "3-2 (Fine-tuning)": "The documentation highlights how Gemma-3 base checkpoints, including the 4 B variant, are converted into powerful chat assistants via instruction/post-training.  A comparison set of open models in the 7–9 B range is assembled, and the \"latest versions of Gemma\" undergo the same instruction-tuning process.  In head-to-head tests the instruction-tuned Gemma-7 B outperforms LLaMA-13 B and even LLaMA-65 B across most tasks.  For Gemma-3 specifically, post-training targets mathematics, logical reasoning and chat fluency while also activating new long-context and image-input capabilities; a \"novel post-training approach\" is credited with simultaneous gains in math, coding, instruction following and multilingual use.  The text further notes that pre-trained checkpoints are \"turned into instruction-tuned models\" through an improved recipe (Table 6), yielding results that \"outperform their predecessors by a wide margin.\"  When extending to a multimodal setting, Gemma-3 fine-tuning follows the Steiner et al. (2024) protocol, sweeping only the learning-rate hyper-parameter and keeping all other transfer settings fixed.",
  "3-3 (Reinforcement Learning)": "",
  "2-3 (API)__evidence": [],
  "3-1 (Pre-training)__evidence": [
    {
      "source": "[sections/Introduction]",
      "quote": "The pre-training optimization recipe is similar to Gemma 2, with some modifications in the architecture design. All Gemma 3 models are trained with knowledge distillation (Hinton et al., 2015)."
    },
    {
      "source": "[sections/2.2 Pre-training]",
      "quote": "Training data. We pre-train our models on a slightly larger token budget than Gemma 2, i.e., we train on 14T tokens for Gemma 3 27B, 12T for the 12B version, 4T for the 4B, and 2T tokens for the 1B."
    },
    {
      "source": "[sections/2.2 Pre-training]",
      "quote": "Tokenizer. We use the same tokenizer as Gemini 2.0, and we also revisit our data mixture to improve the multilingual capabilities of the models, while introducing image understanding. All Gemma 3 models are trained with knowledge distillation (Hinton et al., 2015)."
    },
    {
      "source": "[pdf_text]",
      "quote": "In Table 9, we report the performance of our new pre-trained benchmarks compared to previous versions. Overall, our models are in the same ballpark as Gemma 2, which is encouraging since these abilities are not the focus of the improvements brought in this version."
    },
    {
      "source": "[pdf_text]",
      "quote": "Overall we see a consistent improvement over STEM abilities across our pre-trained models. On code, we see a similar improvement for the 4B and 12B models but not on the 27B."
    },
    {
      "source": "[sections/2503.19786]",
      "quote": "We follow a similar recipe as in Gemma 2 for pre-training with knowledge distillation."
    },
    {
      "source": "[sections/2503.19786]",
      "quote": "We pre-train our models on a slightly larger token budget than Gemma 2, i.e., we train on 14T tokens for Gemma 3 27B, 12T for the 12B version, 4T for the 4B, and 2T tokens for the 1B."
    },
    {
      "source": "[sections/2503.19786]",
      "quote": "All Gemma 3 models are trained with knowledge distillation (Hinton et al., 2015)."
    }
  ],
  "3-2 (Fine-tuning)__evidence": [
    {
      "source": "[pdf_text]",
      "quote": "We included open models, namely the latest versions of Gemma,3 Mistral,4 Qwen5, and Olmo,6, all in sizes of between 7 and 9 billion parameters that underwent instruction tuning."
    },
    {
      "source": "[sections/Experiments and Analysis]",
      "quote": "We find that open source LLaMA models perform much worse compared to proprietary models; even the largest LLaMA-65B model significantly underperforms the smallest PaLM-2-XXS model. Gemma-7B instruction tuned model performs better than LLaMA-13B as well as LLaMA-65B on most tasks."
    },
    {
      "source": "[sections/Instruction-Tuning]",
      "quote": "In post-training, we focus our efforts on improving mathematics, reasoning, and chat abilities, as well as integrating the new capabilities of Gemma 3, long-context, and image inputs. We use a novel post-training approach that brings gains across all capabilities, including math, coding, chat, instruction following, and multilingual."
    },
    {
      "source": "[sections/Instruction-Tuning]",
      "quote": "Pre-trained models are turned into instruction-tuned models with an improved post-training approach compared to our prior recipe (see Table 6). The resulting Gemma 3 instruction-tuned models are both powerful and versatile, outperforming their predecessors by a wide margin."
    },
    {
      "source": "[pdf_text]",
      "quote": "Comparison to PaliGemma 2. We fine-tune multimodal Gemma 3 pre-trained checkpoints following the protocol from Steiner et al. (2024) – only learning rate is swept, otherwise the same transfer settings are used."
    },
    {
      "source": "[sections/2502.21228]",
      "quote": "We included open models, namely the latest versions of Gemma,3 Mistral,4 Qwen5, and Olmo,6, all in sizes of between 7 and 9 billion parameters that underwent instruction tuning."
    },
    {
      "source": "[sections/2503.19786]",
      "quote": "In post-training, we focus our efforts on improving mathematics, reasoning, and chat abilities, as well as integrating the new capabilities of Gemma 3, long-context, and image inputs. We use a novel post-training approach that brings gains across all capabilities, including math, coding, chat, instruction following, and multilingual."
    }
  ],
  "3-3 (Reinforcement Learning)__evidence": []
}