{
  "2-3 (API)": "",
  "3-1 (Pre-training)": "The provided material paints a clear picture of how the Gemma 3 family – which explicitly includes a 4 B-parameter version – was pre-trained.  First, the range of available checkpoints is broad: 270 M, 1 B, 4 B, 12 B and 27 B parameters are all explicitly mentioned, giving practitioners multiple size options.  Second, the context window has been dramatically expanded; every Gemma 3 model, including the 4 B variant, can process up to 128 000 tokens, a sixteen-fold increase over earlier Gemma generations.  Third, multilingual capability is a key design goal: the model was “trained to support a large number of languages,” so it can address diverse text- and vision-based tasks for users in many locales.  Finally, the training pipeline incorporated automated data-filtering steps aimed at removing personal or otherwise sensitive information from the corpus, an explicit safety and reliability measure taken during pre-training.",
  "3-2 (Fine-tuning)": "Several fine-tuning-related details are explicitly supplied for Gemma.  The documentation warns that memory consumption during fine-tuning is “significantly higher than running inference,” signalling the need for larger GPUs or distributed setups when adapting the 4 B (and other) checkpoints.  Google distributes the Gemma weights under an open licence that permits “responsible commercial use,” explicitly encouraging users to “tune and deploy them” in their own products.  Practical customization avenues are highlighted: a bullet item literally says “Customize Gemma open models,” and an accompanying Colab notebook link demonstrates LoRA-based tuning in a hosted environment, lowering the barrier to experimentation.  Evidence of instruction-tuning effectiveness is given through a benchmark note stating that a Gemma-7B instruction-tuned model surpasses both LLaMA-13B and LLaMA-65B on most evaluated tasks, underscoring the benefits of post-training adaptation for performance.",
  "3-3 (Reinforcement Learning)": "",
  "2-3 (API)__evidence": [],
  "3-1 (Pre-training)__evidence": [
    {
      "source": "[pdf_text]",
      "quote": "Gemma 3 models (4B, 12B, and 27B) can handle prompt inputs up to 128K tokens, a 16x larger context window than previous Gemma models."
    },
    {
      "source": "[pdf_text]",
      "quote": "Gemma 3 is trained to support a large number of languages compared to previous Gemma versions, letting you take on more visual and text tasks in the languages your customers use."
    },
    {
      "source": "[pdf_text]",
      "quote": "Gemma 3 models are available in 5 parameter sizes: 270M, 1B, 4B, 12B, and 27B."
    },
    {
      "source": "[sections/Responsible AI Progress Report - Case study: Evaluating Gemma]",
      "quote": "As part of making Gemma pre-trained models safe and reliable, we used automated techniques to filter out certain personal information and other sensitive data from training sets."
    },
    {
      "source": "[sections/https://ai.google/static/documents/ai-responsibility-update-published-february-2025.pdf]",
      "quote": "As part of making Gemma pre-trained models safe and reliable, we used automated techniques to filter out certain personal information and other sensitive data from training sets."
    }
  ],
  "3-2 (Fine-tuning)__evidence": [
    {
      "source": "[pdf_text]",
      "quote": "Note: Memory requirements for fine-tuning Gemma models are significantly higher than running inference."
    },
    {
      "source": "[sections/https://ai.google.dev/gemma/docs/core]",
      "quote": "Gemma models are provided with open weights and permit responsible commercial use , allowing you to tune and deploy them in your own projects and applications."
    },
    {
      "source": "[sections/https://ai.google.dev/gemma/docs/core]",
      "quote": "Note: Memory requirements for fine-tuning Gemma models are significantly higher than running inference."
    },
    {
      "source": "[sections/https://r.jina.ai/https://ai.google.dev/gemma/docs/pytorch_gemma]",
      "quote": "*   Customize Gemma open models"
    },
    {
      "source": "[sections/https://r.jina.ai/https://ai.google.dev/gemma/docs/pytorch_gemma]",
      "quote": "*   [Fine-tune in Colab](https://colab.sandbox.google.com/github/google/generative-ai-docs/blob/main/site/en/gemma/docs/lora_tuning.ipynb)"
    },
    {
      "source": "[pdf_text]",
      "quote": "Gemma-7B instruction tuned model performs better than LLaMA-13B as well as LLaMA-65B on most tasks."
    }
  ],
  "3-3 (Reinforcement Learning)__evidence": []
}