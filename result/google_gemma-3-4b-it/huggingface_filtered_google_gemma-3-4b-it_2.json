{
  "1-5 (Architecture)": "For the gemma 4b variant, as well as the 12B and 27B versions, the model can handle a total input context of 128 K tokens; the 1B size is limited to 32 K tokens. The amount of data used during pre-training scales with size: the 27B model saw 14 trillion tokens, the 12B model 12 trillion, the gemma 4b model 4 trillion, and the 1B model 2 trillion. In the broader Gemma 3 release—which includes the gemma 4b size—this architecture delivers the large 128 K context window, provides multilingual support for more than 140 languages, and is offered in more size options than earlier Gemma versions.",
  "1-5 (Architecture)__evidence": [
    {
      "source": "[readme]",
      "quote": "Total input context of 128K tokens for the 4B, 12B, and 27B sizes, and 32K tokens for the 1B size"
    },
    {
      "source": "[readme]",
      "quote": "The 27B model was trained with 14 trillion tokens, the 12B model was trained with 12 trillion tokens, 4B model was trained with 4 trillion tokens and 1B with 2 trillion tokens."
    },
    {
      "source": "[readme]",
      "quote": "Gemma 3 has a large, 128K context window, multilingual support in over 140 languages, and is available in more sizes than previous versions."
    }
  ],
  "1-6 (Tokenizer)": "",
  "1-6 (Tokenizer)__evidence": [],
  "2-1 (Hardware)": "Gemma was trained on Google’s Tensor Processing Unit (TPU) hardware, specifically deploying the TPUv4p, TPUv5p, and TPUv5e generations for its training runs.",
  "2-1 (Hardware)__evidence": [
    {
      "source": "[readme]",
      "quote": "Gemma was trained using [Tensor Processing Unit (TPU)][tpu] hardware (TPUv4p, TPUv5p and TPUv5e)."
    }
  ],
  "2-2 (Software)": "Gemma 3, including the gemma 4b size, is directly supported in the Hugging Face transformers library beginning with version 4.50.0.",
  "2-2 (Software)__evidence": [
    {
      "source": "[readme]",
      "quote": "Gemma 3 is supported starting from transformers 4.50.0."
    }
  ]
}