{
  "1-1 (Weights)": "* \"Support Gemma v3. You can find the checkpoints [on Kaggle](https://www.kaggle.com/models/google/gemma-3/pytorch) and [Hugging Face](https://huggingface.co/models?other=gemma_torch)\"\n* \"- [Gemma 3](https://www.kaggle.com/models/google/gemma-3/pyTorch)\"\n* \"huggingface-cli download google/gemma-3-4b-it-pytorch\"",
  "1-1 (Weights)__evidence": [
    {
      "source": "[readme]",
      "quote": "* [March 12th, 2025 ðŸ”¥] Support Gemma v3. You can find the checkpoints [on Kaggle](https://www.kaggle.com/models/google/gemma-3/pytorch) and [Hugging Face](https://huggingface.co/models?other=gemma_torch)"
    },
    {
      "source": "[readme]",
      "quote": "- [Gemma 3](https://www.kaggle.com/models/google/gemma-3/pyTorch)"
    },
    {
      "source": "[readme]",
      "quote": "huggingface-cli download google/gemma-3-4b-it-pytorch"
    }
  ],
  "1-2 (Code)": "* \"This is the official PyTorch implementation of Gemma models.\"\n* \"We provide model and inference implementations using both PyTorch and PyTorch/XLA, and support running inference on CPU, GPU and TPU.\"\n* \"\"\"Inference-only Gemma 3 multimodal model implementation.\"\"\"\n* \"\"\"Inference-only Gemma model implementation.\"\"\"\n* \"\"\"Inference-only Gemma model implementation.\"\"\"",
  "1-2 (Code)__evidence": [
    {
      "source": "[readme]",
      "quote": "This is the official PyTorch implementation of Gemma models."
    },
    {
      "source": "[readme]",
      "quote": "We provide model and inference implementations using both PyTorch and PyTorch/XLA, and support running inference on CPU, GPU and TPU."
    },
    {
      "source": "[py_files/gemma/gemma3_model.py]",
      "quote": "\"\"\"Inference-only Gemma 3 multimodal model implementation.\"\"\""
    },
    {
      "source": "[py_files/gemma/model.py]",
      "quote": "\"\"\"Inference-only Gemma model implementation.\"\"\""
    },
    {
      "source": "[py_files/gemma/model_xla.py]",
      "quote": "\"\"\"Inference-only Gemma model implementation.\"\"\""
    }
  ],
  "1-3 (License)": "",
  "1-3 (License)__evidence": [],
  "1-4 (Paper)": "* \"[Gemma on Google AI](https://ai.google.dev/gemma)\"\n* \"https://developers.googleblog.com/en/gemma-explained-paligemma-architecture/\"",
  "1-4 (Paper)__evidence": [
    {
      "source": "[readme]",
      "quote": "* [Gemma on Google AI](https://ai.google.dev/gemma)"
    },
    {
      "source": "[py_files/gemma/siglip_vision/config.py]",
      "quote": "# https://developers.googleblog.com/en/gemma-explained-paligemma-architecture/"
    }
  ],
  "1-5 (Architecture)": "- **Gemma 3**:\n\"\"\"Inference-only Gemma 3 multimodal model implementation.\"\"\"\n\"\"\"Gemma model config.\"\"\"\n# TODO(imayank): Decouple Gemma versions into separate files.\n\"\"\"Gemma model config.\"\"\"\n\"\"\"Returns the model config for the vision model of Gemma 3 andPaliGemma.\"\"\"",
  "1-5 (Architecture)__evidence": [
    {
      "source": "[readme]",
      "quote": "- **Gemma 3**:"
    },
    {
      "source": "[py_files/gemma/gemma3_model.py]",
      "quote": "\"\"\"Inference-only Gemma 3 multimodal model implementation.\"\"\""
    },
    {
      "source": "[py_files/gemma/config.py]",
      "quote": "\"\"\"Gemma model config.\"\"\""
    },
    {
      "source": "[files]",
      "quote": "# TODO(imayank): Decouple Gemma versions into separate files."
    },
    {
      "source": "py_files/gemma/siglip_vision/config.py",
      "quote": "\"\"\"Gemma model config.\"\"\""
    },
    {
      "source": "py_files/gemma/siglip_vision/config.py",
      "quote": "\"\"\"Returns the model config for the vision model of Gemma 3 andPaliGemma.\"\"\""
    }
  ],
  "1-6 (Tokenizer)": "tokenizer='tokenizer/gemma3_cleaned_262144_v2.spiece.model',",
  "1-6 (Tokenizer)__evidence": [
    {
      "source": "[py_files/gemma/config.py]",
      "quote": "tokenizer='tokenizer/gemma3_cleaned_262144_v2.spiece.model',"
    }
  ],
  "2-1 (Hardware)": "",
  "2-1 (Hardware)__evidence": [],
  "2-2 (Software)": "This is the official PyTorch implementation of Gemma models.\nWe provide model and inference implementations using both PyTorch and PyTorch/XLA, and support running inference on CPU, GPU and TPU.\nfrom google3.third_party.open_models_release.gemma_pytorch.gemma import config as gemma_config\n\"\"\"Preprocesses a list of PIL images for Siglip vision model using only PyTorch and PIL.\"\"\"\n# python scripts/run.py --device=cpu --ckpt=/path/to/your/pytorch_checkpoint/model.ckpt --output_len=2 --prompt=\"The name of the capital of Italy is\"\n# - Replace '/path/to/your/pytorch_checkpoint/model.ckpt' with the actual path to your checkpoint file.",
  "2-2 (Software)__evidence": [
    {
      "source": "[readme]",
      "quote": "This is the official PyTorch implementation of Gemma models."
    },
    {
      "source": "[readme]",
      "quote": "We provide model and inference implementations using both PyTorch and PyTorch/XLA, and support running inference on CPU, GPU and TPU."
    },
    {
      "source": "py_files/gemma/model_xla.py",
      "quote": "from google3.third_party.open_models_release.gemma_pytorch.gemma import config as gemma_config"
    },
    {
      "source": "py_files/gemma/siglip_vision/preprocessor.py",
      "quote": "\"\"\"Preprocesses a list of PIL images for Siglip vision model using only PyTorch and PIL.\"\"\""
    },
    {
      "source": "[py_files/scripts/run.py]",
      "quote": "# python scripts/run.py --device=cpu --ckpt=/path/to/your/pytorch_checkpoint/model.ckpt --output_len=2 --prompt=\"The name of the capital of Italy is\""
    },
    {
      "source": "[py_files/scripts/run.py]",
      "quote": "# - Replace '/path/to/your/pytorch_checkpoint/model.ckpt' with the actual path to your checkpoint file."
    }
  ],
  "2-3 (API)": "",
  "2-3 (API)__evidence": [],
  "3-1 (Pre-training)": "\"Gemma is a family of lightweight, state-of-the art open models built from research and technology used to create Google Gemini models.\" This single sentence is the core of the available pre-training information. It establishes that the Gemma models inherit their design and training lineage from the work that produced Googleâ€™s Gemini series, emphasizing both their \"lightweight\" nature and their positioning as \"state-of-the-art open models.\" The additional fragment, \"\"\"Gemma model config.\"\"\", confirms the existence of a specific configuration file associated with the modelâ€”implying that architectural or hyper-parameter settings are recorded there. No other quantitative or procedural details (e.g., dataset size, number of tokens, optimizer choice, schedule, or hardware) are present in the supplied quotations, so the summary of pre-training is necessarily limited to these two explicit statements.",
  "3-1 (Pre-training)__evidence": [
    {
      "source": "[readme]",
      "quote": "**Gemma** is a family of lightweight, state-of-the art open models built from research and technology used to create Google Gemini models."
    },
    {
      "source": "[py_files/gemma/config.py]",
      "quote": "\"\"\"Gemma model config.\"\"\""
    }
  ],
  "3-2 (Fine-tuning)": "",
  "3-2 (Fine-tuning)__evidence": [],
  "3-3 (Reinforcement Learning)": "",
  "3-3 (Reinforcement Learning)__evidence": [],
  "4-1 (Pre-training Data)": "",
  "4-1 (Pre-training Data)__evidence": [],
  "4-2 (Fine-tuning Data)": "",
  "4-2 (Fine-tuning Data)__evidence": [],
  "4-3 (Reinforcement Learning Data)": "",
  "4-3 (Reinforcement Learning Data)__evidence": [],
  "4-4 (Data Filtering)": "",
  "4-4 (Data Filtering)__evidence": [],
  "__usage": {
    "fine_tuning": "unknown",
    "rl": "unknown"
  }
}