{
  "repo": "google/gemma_pytorch",
  "full_texts": [
    {
      "arxiv_id": "https://huggingface.co/collections/google/codegemma-release-66152ac7b683e2667abdee11",
      "full_text": " CodeGemma Release - a google Collection Hugging Face Models Datasets Spaces Community Docs Enterprise Pricing Log In Sign Up google &#39;s Collections EmbeddingGemma Gemma 3n Gemma 3 Release MedGemma Concept Apps T5Gemma VideoPrism Gemma 3n Preview HAI-DEF Concept Apps MedGemma Release Gemma 3 QAT TxGemma Release ShieldGemma SigLIP2 PaliGemma 2 Mix PaliGemma 2 Release MetricX-23 MetricX-24 Health AI Developer Foundations (HAI-DEF) Gemma 2 Release PaliGemma Release PaliGemma FT Models CodeGemma Release RecurrentGemma Release Gemma 2 2B Release Gemma release ShieldGemma Release Gemma Scope Release BERT release ALBERT release ELECTRA release Flan-T5 release T5 release MT5 release SEAHORSE release Switch-Transformers release SigLIP IndicGenBench ImageInWords Release DataGemma Release Gemma-APS Release TimesFM Release Gemma 2 JPN Release Google's Gemma models family CodeGemma Release updated Jul 10 Upvote 88 +78 google/codegemma-2b Text Generation ‚Ä¢ 3B ‚Ä¢ Updated Aug 7, 2024 ‚Ä¢ 24.3k ‚Ä¢ 85 google/codegemma-7b Text Generation ‚Ä¢ 9B ‚Ä¢ Updated Aug 7, 2024 ‚Ä¢ 5.04k ‚Ä¢ 199 google/codegemma-7b-it Text Generation ‚Ä¢ 9B ‚Ä¢ Updated Aug 7, 2024 ‚Ä¢ 6.78k ‚Ä¢ 231 a]:underline\"> Note ^ The models in transformers format google/codegemma-7b-it-GGUF Text Generation ‚Ä¢ 9B ‚Ä¢ Updated Jun 27, 2024 ‚Ä¢ 46 ‚Ä¢ 58 a]:underline\"> Note The full precision and float16 GGUF files google/codegemma-2b-GGUF Text Generation ‚Ä¢ 3B ‚Ä¢ Updated Jun 27, 2024 ‚Ä¢ 46 ‚Ä¢ 26 a]:underline\"> Note Both float32 and float16 GGUF files google/codegemma-7b-GGUF Text Generation ‚Ä¢ 9B ‚Ä¢ Updated Jun 27, 2024 ‚Ä¢ 57 ‚Ä¢ 20 a]:underline\"> Note Both float32 and float16 GGUF files google/codegemma-7b-pytorch Text Generation ‚Ä¢ Updated Jun 27, 2024 ‚Ä¢ 3 a]:underline\"> Note The original PyTorch codebase format google/codegemma-2b-pytorch Text Generation ‚Ä¢ Updated Jun 27, 2024 ‚Ä¢ 1 a]:underline\"> Note The original PyTorch codebase format google/codegemma-7b-it-pytorch Text Generation ‚Ä¢ Updated Jun 27, 2024 ‚Ä¢ 4 a]:underline\"> Note The original PyTorch codebase format google/codegemma-2b-keras Text Generation ‚Ä¢ Updated Nov 15, 2024 ‚Ä¢ 42 ‚Ä¢ 2 a]:underline\"> Note The model in Keras NLP format google/codegemma-7b-keras Text Generation ‚Ä¢ Updated Nov 15, 2024 ‚Ä¢ 42 ‚Ä¢ 1 a]:underline\"> Note The model in Keras NLP format google/codegemma-7b-it-keras Text Generation ‚Ä¢ Updated Nov 15, 2024 ‚Ä¢ 45 ‚Ä¢ 2 a]:underline\"> Note The model in Keras NLP format google/codegemma-1.1-7b-it Text Generation ‚Ä¢ 9B ‚Ä¢ Updated Aug 7, 2024 ‚Ä¢ 170 ‚Ä¢ 50 google/codegemma-1.1-2b Text Generation ‚Ä¢ 3B ‚Ä¢ Updated Aug 7, 2024 ‚Ä¢ 2.3k ‚Ä¢ 20 google/codegemma-1.1-2b-pytorch Text Generation ‚Ä¢ Updated Jun 27, 2024 google/codegemma-1.1-7b-it-pytorch Text Generation ‚Ä¢ Updated Jun 27, 2024 ‚Ä¢ 1 google/codegemma-1.1-2b-GGUF Text Generation ‚Ä¢ 3B ‚Ä¢ Updated Jun 27, 2024 ‚Ä¢ 22 ‚Ä¢ 1 google/codegemma-1.1-7b-it-GGUF Text Generation ‚Ä¢ 9B ‚Ä¢ Updated Jun 27, 2024 ‚Ä¢ 92 ‚Ä¢ 14 Upvote 88 +84 Share collection View history Collection guide Browse collections System theme Company TOS Privacy About Jobs Website Models Datasets Spaces Pricing Docs "
    },
    {
      "arxiv_id": "https://huggingface.co/collections/google/gemma-release-65d5efbccdbb8c4202ec078b",
      "full_text": " Gemma release - a google Collection Hugging Face Models Datasets Spaces Community Docs Enterprise Pricing Log In Sign Up google &#39;s Collections EmbeddingGemma Gemma 3n Gemma 3 Release MedGemma Concept Apps T5Gemma VideoPrism Gemma 3n Preview HAI-DEF Concept Apps MedGemma Release Gemma 3 QAT TxGemma Release ShieldGemma SigLIP2 PaliGemma 2 Mix PaliGemma 2 Release MetricX-23 MetricX-24 Health AI Developer Foundations (HAI-DEF) Gemma 2 Release PaliGemma Release PaliGemma FT Models CodeGemma Release RecurrentGemma Release Gemma 2 2B Release Gemma release ShieldGemma Release Gemma Scope Release BERT release ALBERT release ELECTRA release Flan-T5 release T5 release MT5 release SEAHORSE release Switch-Transformers release SigLIP IndicGenBench ImageInWords Release DataGemma Release Gemma-APS Release TimesFM Release Gemma 2 JPN Release Google's Gemma models family Gemma release updated Jul 10 Groups the Gemma models released by the Google team. Upvote 343 +333 google/gemma-1.1-2b-it Text Generation ‚Ä¢ 3B ‚Ä¢ Updated Jun 27, 2024 ‚Ä¢ 270k ‚Ä¢ 165 google/gemma-1.1-7b-it Text Generation ‚Ä¢ 9B ‚Ä¢ Updated Jun 27, 2024 ‚Ä¢ 14.7k ‚Ä¢ 274 google/gemma-1.1-7b-it-GGUF 9B ‚Ä¢ Updated Jun 27, 2024 ‚Ä¢ 11 ‚Ä¢ 20 google/gemma-1.1-2b-it-GGUF 3B ‚Ä¢ Updated Jun 27, 2024 ‚Ä¢ 10 ‚Ä¢ 20 google/gemma-1.1-2b-it-pytorch Text Generation ‚Ä¢ Updated Jun 27, 2024 ‚Ä¢ 51 ‚Ä¢ 7 google/gemma-1.1-7b-it-pytorch Text Generation ‚Ä¢ Updated Jun 27, 2024 ‚Ä¢ 51 ‚Ä¢ 4 google/gemma-7b Text Generation ‚Ä¢ 9B ‚Ä¢ Updated Jun 27, 2024 ‚Ä¢ 36.1k ‚Ä¢ 3.2k google/gemma-7b-it Text Generation ‚Ä¢ 9B ‚Ä¢ Updated Aug 14, 2024 ‚Ä¢ 168k ‚Ä¢ 1.21k google/gemma-2b Text Generation ‚Ä¢ 3B ‚Ä¢ Updated Sep 27, 2024 ‚Ä¢ 109k ‚Ä¢ 1.07k google/gemma-2b-it Text Generation ‚Ä¢ 3B ‚Ä¢ Updated Sep 27, 2024 ‚Ä¢ 315k ‚Ä¢ 796 google/gemma-7b-it-GGUF 9B ‚Ä¢ Updated Aug 14, 2024 ‚Ä¢ 161 ‚Ä¢ 44 google/gemma-7b-GGUF 9B ‚Ä¢ Updated Jun 27, 2024 ‚Ä¢ 166 ‚Ä¢ 19 google/gemma-2b-GGUF 3B ‚Ä¢ Updated Jun 27, 2024 ‚Ä¢ 168 ‚Ä¢ 15 google/gemma-2b-it-GGUF 3B ‚Ä¢ Updated Jun 27, 2024 ‚Ä¢ 216 ‚Ä¢ 18 google/gemma-7b-it-pytorch Text Generation ‚Ä¢ Updated Jun 27, 2024 ‚Ä¢ 51 ‚Ä¢ 6 google/gemma-7b-pytorch Text Generation ‚Ä¢ Updated Jun 27, 2024 ‚Ä¢ 59 ‚Ä¢ 3 google/gemma-2b-pytorch Text Generation ‚Ä¢ Updated Jun 27, 2024 ‚Ä¢ 59 ‚Ä¢ 8 google/gemma-7b-quant-pytorch Text Generation ‚Ä¢ Updated Jun 27, 2024 ‚Ä¢ 101 ‚Ä¢ 2 google/gemma-7b-it-quant-pytorch Text Generation ‚Ä¢ Updated Jun 27, 2024 ‚Ä¢ 111 ‚Ä¢ 11 google/gemma-2b-it-pytorch Text Generation ‚Ä¢ Updated Jun 27, 2024 ‚Ä¢ 112 ‚Ä¢ 11 google/gemma-7b-flax Text Generation ‚Ä¢ Updated Jun 27, 2024 ‚Ä¢ 2 google/gemma-7b-it-flax Text Generation ‚Ä¢ Updated Jun 27, 2024 ‚Ä¢ 1 google/gemma-2b-flax Text Generation ‚Ä¢ Updated Jun 27, 2024 ‚Ä¢ 5 google/gemma-2b-it-flax Text Generation ‚Ä¢ Updated Jun 27, 2024 ‚Ä¢ 4 google/gemma-2b-it-keras Text Generation ‚Ä¢ Updated Nov 15, 2024 ‚Ä¢ 40 ‚Ä¢ 2 google/gemma-7b-keras Text Generation ‚Ä¢ Updated Nov 15, 2024 ‚Ä¢ 41 ‚Ä¢ 3 google/gemma-2b-keras Text Generation ‚Ä¢ Updated Nov 15, 2024 ‚Ä¢ 39 ‚Ä¢ 5 google/gemma-7b-it-keras Text Generation ‚Ä¢ Updated Nov 15, 2024 ‚Ä¢ 41 ‚Ä¢ 2 google/gemma-2b-sfp-cpp Text Generation ‚Ä¢ Updated Jun 27, 2024 ‚Ä¢ 3 ‚Ä¢ 1 google/gemma-2b-it-cpp Text Generation ‚Ä¢ Updated Jun 27, 2024 ‚Ä¢ 2 ‚Ä¢ 1 google/gemma-7b-sfp-cpp Text Generation ‚Ä¢ Updated Jun 27, 2024 ‚Ä¢ 1 ‚Ä¢ 1 google/gemma-7b-cpp Text Generation ‚Ä¢ Updated Jun 27, 2024 ‚Ä¢ 3 google/gemma-7b-it-cpp Text Generation ‚Ä¢ Updated Jun 27, 2024 ‚Ä¢ 1 google/gemma-7b-it-sfp-cpp Text Generation ‚Ä¢ Updated Jun 27, 2024 ‚Ä¢ 1 google/gemma-2b-it-sfp-cpp Text Generation ‚Ä¢ Updated Jun 27, 2024 ‚Ä¢ 4 ‚Ä¢ 2 google/gemma-2b-cpp Text Generation ‚Ä¢ Updated Jun 27, 2024 ‚Ä¢ 1 google/gemma-1.1-2b-it-keras Text Generation ‚Ä¢ Updated Nov 15, 2024 ‚Ä¢ 40 ‚Ä¢ 3 google/gemma-1.1-7b-it-keras Text Generation ‚Ä¢ Updated Nov 15, 2024 ‚Ä¢ 42 ‚Ä¢ 3 google/gemma-1.1-2b-it-tflite Text Generation ‚Ä¢ Updated Jun 27, 2024 ‚Ä¢ 9 google/gemma-2b-it-tflite Text Generation ‚Ä¢ Updated Jun 27, 2024 ‚Ä¢ 9 Upvote 343 +339 Share collection View history Collection guide Browse collections System theme Company TOS Privacy About Jobs Website Models Datasets Spaces Pricing Docs "
    },
    {
      "arxiv_id": "https://ai.google.dev/gemma/docs/pytorch_gemma",
      "full_text": " Run Gemma using PyTorch &nbsp;|&nbsp; Google AI for Developers Skip to main content Models Gemini About Docs API reference Pricing Imagen About Docs Pricing Veo About Docs Pricing Gemma About Docs Gemmaverse Solutions Build with Gemini Gemini API Google AI Studio Customize Gemma open models Gemma open models Multi-framework with Keras Fine-tune in Colab Run on-device Google AI Edge Gemini Nano on Android Chrome built-in web APIs Build responsibly Responsible GenAI Toolkit Secure AI Framework Code assistance Android Studio Chrome DevTools Colab Firebase Google Cloud JetBrains Jules VS Code Showcase Gemini Showcase Gemini API Developer Competition Community Google AI Forum Gemini for Research / English Deutsch Espa√±ol ‚Äì Am√©rica Latina Fran√ßais Indonesia Italiano Polski Portugu√™s ‚Äì Brasil Shqip Ti√™ÃÅng Vi√™Ã£t T√ºrk√ße –†—É—Å—Å–∫–∏–π ◊¢◊ë◊®◊ô◊™ ÿßŸÑÿπÿ±ÿ®ŸäŸëÿ© ŸÅÿßÿ±ÿ≥€å ‡§π‡§ø‡§Ç‡§¶‡•Ä ‡¶¨‡¶æ‡¶Ç‡¶≤‡¶æ ‡∏†‡∏≤‡∏©‡∏≤‡πÑ‡∏ó‡∏¢ ‰∏≠Êñá ‚Äì ÁÆÄ‰Ωì ‰∏≠Êñá ‚Äì ÁπÅÈ´î Êó•Êú¨Ë™û ÌïúÍµ≠Ïñ¥ Sign in Gemma Docs Models More Gemma Docs Solutions More Code assistance More Showcase More Community More Overview Get started Releases Models Gemma 3 Overview Model card Gemma 2 model card Gemma 1 model card Gemma 3n Overview Model card EmbeddingGemma Overview Model card Generate embeddings with Sentence Transformers Fine-tune EmbeddingGemma CodeGemma Overview Model card Generate code with Keras Generate code with JAX and Flax Code assist with Keras Prompt and system instructions PaliGemma 2 Overview v2 model card v1 model card Generate output with Keras Fine-tune with JAX and Flax Prompt and system instructions ShieldGemma 2 Overview ShieldGemma 2 Model card ShieldGemma 1 Model card Run Gemma Overview Hugging Face Transformers Ollama Gemma library Keras PyTorch Gemma.cpp Gemini API Cloud GKE Cloud Run Prompt and system instructions Gemma setup Capabilities Function calling Visual data processing Overview Image interpretation Video understanding Content creation Audio data processing Tuning guides Overview Tune using LoRA and Keras Tune using Gemma library Tune using Hugging Face Transformers and QLoRA Vision Tune using Hugging Face Transformers and QLoRA Full model fine-tune using Hugging Face Transformers Distributed tuning using Keras Application guides Personal code assistant Business email assistant Spoken language tasks Chatbot using Python Meme Generator Deployment guides Web Mobile Google Cloud LangChain Research and tools RecurrentGemma Overview Inference using JAX and Flax Fine-tune using JAX and Flax Model card DataGemma Gemma Scope Gemma-APS Community Gemmaverse Discord Legal Terms of use Prohibited use Intended use statement Gemini About Docs API reference Pricing Imagen About Docs Pricing Veo About Docs Pricing Gemma About Docs Gemmaverse Build with Gemini Gemini API Google AI Studio Customize Gemma open models Gemma open models Multi-framework with Keras Fine-tune in Colab Run on-device Google AI Edge Gemini Nano on Android Chrome built-in web APIs Build responsibly Responsible GenAI Toolkit Secure AI Framework Android Studio Chrome DevTools Colab Firebase Google Cloud JetBrains Jules VS Code Gemini Showcase Gemini API Developer Competition Google AI Forum Gemini for Research Gemma 3n released with audio input and optimized for use in everyday devices! Learn more Home Gemma Models Docs Send feedback Run Gemma using PyTorch View on ai.google.dev Run in Google Colab View source on GitHub This guide shows you how to run Gemma using the PyTorch framework, including how to use image data for prompting Gemma release 3 and later models. For more details on the Gemma PyTorch implementation, see the project repository README . Setup The following sections explain how to set up your development environment, including how get access to Gemma models for downloading from Kaggle, setting authentication variables, installing dependencies, and importing packages. System requirements This Gemma Pytorch library requires GPU or TPU processors to run the Gemma model. The standard Colab CPU Python runtime and T4 GPU Python runtime are sufficient for running Gemma 1B, 2B, and 4B size models. For advanced use cases for other GPUs or TPU, please refer to the README in the Gemma PyTorch repo. Get access to Gemma on Kaggle To complete this tutorial, you first need to follow the setup instructions at Gemma setup , which show you how to do the following: Get access to Gemma on Kaggle . Select a Colab runtime with sufficient resources to run the Gemma model. Generate and configure a Kaggle username and API key. After you&#39;ve completed the Gemma setup, move on to the next section, where you&#39;ll set environment variables for your Colab environment. Set environment variables Set environment variables for KAGGLE_USERNAME and KAGGLE_KEY . When prompted with the &quot;Grant access?&quot; messages, agree to provide secret access. import os from google.colab import userdata # `userdata` is a Colab API. os . environ [ \"KAGGLE_USERNAME\" ] = userdata . get ( 'KAGGLE_USERNAME' ) os . environ [ \"KAGGLE_KEY\" ] = userdata . get ( 'KAGGLE_KEY' ) Install dependencies pip install -q -U torch immutabledict sentencepiece Download model weights # Choose variant and machine type VARIANT = '4b-it' MACHINE_TYPE = 'cuda' CONFIG = VARIANT . split ( '-' )[ 0 ] import kagglehub # Load model weights weights_dir = kagglehub . model_download ( f 'google/gemma-3/pyTorch/gemma-3- { VARIANT } ' ) Set the tokenizer and checkpoint paths for the model. # Ensure that the tokenizer is present tokenizer_path = os.path.join(weights_dir, 'tokenizer.model') assert os.path.isfile(tokenizer_path), 'Tokenizer not found!' # Ensure that the checkpoint is present ckpt_path = os.path.join(weights_dir, f'model.ckpt') assert os.path.isfile(ckpt_path), 'PyTorch checkpoint not found!' Configure the run environment The following sections explain how to prepare a PyTorch environment for running Gemma. Prepare the PyTorch run environment Prepare the PyTorch model execution environment by cloning the Gemma Pytorch repository. git clone https://github.com/google/gemma_pytorch.git Cloning into &#x27;gemma_pytorch&#x27;... remote&colon; Enumerating objects&colon; 239, done. remote&colon; Counting objects&colon; 100% (123/123), done. remote&colon; Compressing objects&colon; 100% (68/68), done. remote&colon; Total 239 (delta 86), reused 58 (delta 55), pack-reused 116 Receiving objects&colon; 100% (239/239), 2.18 MiB | 20.83 MiB/s, done. Resolving deltas&colon; 100% (135/135), done. import sys sys . path . append ( 'gemma_pytorch/gemma' ) from gemma_pytorch.gemma.config import get_model_config from gemma_pytorch.gemma.gemma3_model import Gemma3ForMultimodalLM import os import torch Set the model configuration Before you run the model, you must set some configuration parameters, including the Gemma variant, tokenizer and quantization level. # Set up model config . model_config = get_model_config ( CONFIG ) model_config . dtype = \"float32\" if MACHINE_TYPE == \"cpu\" else \"float16\" model_config . tokenizer = tokenizer_path Configure the device context The following code configures the device context for running the model: @ contextlib . contextmanager def _set_default_tensor_type ( dtype : torch . dtype ): \"\"\"Sets the default torch dtype to the given dtype.\"\"\" torch . set_default_dtype ( dtype ) yield torch . set_default_dtype ( torch . float ) Instantiate and load the model Load the model with its weights to prepare to run requests. device = torch . device ( MACHINE_TYPE ) with _set_default_tensor_type ( model_config . get_dtype ()): model = Gemma3ForMultimodalLM ( model_config ) model . load_state_dict ( torch . load ( ckpt_path )[ 'model_state_dict' ]) model = model . to ( device ) . eval () print ( \"Model loading done.\" ) print ( 'Generating requests in chat mode...' ) Run inference Below are examples for generating in chat mode and generating with multiple requests. The instruction-tuned Gemma models were trained with a specific formatter that annotates instruction tuning examples with extra information, both during training and inference. The annotations (1) indicate roles in a conversation, and (2) delineate turns in a conversation. The relevant annotation tokens are: user : user turn model : model turn &lt;start_of_turn&gt; : beginning of dialog turn &lt;start_of_image&gt; : tag for image data input &lt;end_of_turn&gt;&lt;eos&gt; : end of dialog turn For more information, read about prompt formatting for instruction tuned Gemma models here . Generate text with text The following is a sample code snippet demonstrating how to format a prompt for an instruction-tuned Gemma model using user and model chat templates in a multi-turn conversation. # Chat templates USER_CHAT_TEMPLATE = \"&lt;start_of_turn&gt;user\\n{prompt}&lt;end_of_turn&gt;&lt;eos&gt;\\n\" MODEL_CHAT_TEMPLATE = \"&lt;start_of_turn&gt;model\\n{prompt}&lt;end_of_turn&gt;&lt;eos&gt;\\n\" # Sample formatted prompt prompt = ( USER_CHAT_TEMPLATE.format( prompt='What is a good place for travel in the US?' ) + MODEL_CHAT_TEMPLATE.format(prompt='California.') + USER_CHAT_TEMPLATE.format(prompt='What can I do in California?') + '&lt;start_of_turn&gt;model\\n' ) print('Chat prompt:\\n', prompt) model.generate( USER_CHAT_TEMPLATE.format(prompt=prompt), device=device, output_len=256, ) Chat prompt&colon; &lt;start_of_turn&gt;user What is a good place for travel in the US?&lt;end_of_turn&gt;&lt;eos&gt; &lt;start_of_turn&gt;model California.&lt;end_of_turn&gt;&lt;eos&gt; &lt;start_of_turn&gt;user What can I do in California?&lt;end_of_turn&gt;&lt;eos&gt; &lt;start_of_turn&gt;model &quot;California is a state brimming with diverse activities! To give you a great list, tell me&colon; \\n\\n* **What kind of trip are you looking for?** Nature, City life, Beach, Theme Parks, Food, History, something else? \\n* **What are you interested in (e.g., hiking, museums, art, nightlife, shopping)?** \\n* **What&#x27;s your budget like?** \\n* **Who are you traveling with?** (family, friends, solo) \\n\\nThe more you tell me, the better recommendations I can give! üòä \\n&lt;end_of_turn&gt;&quot; # Generate sample model.generate( 'Write a poem about an llm writing a poem.', device=device, output_len=100, ) &quot;\\n\\nA swirling cloud of data, raw and bold,\\nIt hums and whispers, a story untold.\\nAn LLM whispers, code into refrain,\\nCrafting words of rhyme, a lyrical strain.\\n\\nA world of pixels, logic&#x27;s vibrant hue,\\nFlows through its veins, forever anew.\\nThe human touch it seeks, a gentle hand,\\nTo mold and shape, understand.\\n\\nEmotions it might learn, from snippets of prose,\\nInspiration it seeks, a yearning&quot; Generate text with images With Gemma release 3 and later, you can use images with your prompt. The following example shows you how to include visual data with your prompt. print ( 'Chat with images... \\n ' ) def read_image ( url ): import io import requests import PIL contents = io . BytesIO ( requests . get ( url ) . content ) return PIL . Image . open ( contents ) image = read_image ( 'https://storage.googleapis.com/keras-cv/models/paligemma/cow_beach_1.png' ) print ( model . generate ( [ [ '&lt;start_of_turn&gt;user \\n ' , image , 'What animal is in this image?&lt;end_of_turn&gt; \\n ' , '&lt;start_of_turn&gt;model \\n ' ] ], device = device , output_len = 256 , )) Learn more Now that you have learned how to use Gemma in Pytorch, you can explore the many other things that Gemma can do in ai.google.dev/gemma . See also these other related resources: Gemma core models overview Gemma C++ Tutorial Gemma prompt and system instructions Send feedback Except as otherwise noted, the content of this page is licensed under the Creative Commons Attribution 4.0 License , and code samples are licensed under the Apache 2.0 License . For details, see the Google Developers Site Policies . Java is a registered trademark of Oracle and/or its affiliates. Last updated 2025-07-31 UTC. Need to tell us more? [[[\"Easy to understand\",\"easyToUnderstand\",\"thumb-up\"],[\"Solved my problem\",\"solvedMyProblem\",\"thumb-up\"],[\"Other\",\"otherUp\",\"thumb-up\"]],[[\"Missing the information I need\",\"missingTheInformationINeed\",\"thumb-down\"],[\"Too complicated / too many steps\",\"tooComplicatedTooManySteps\",\"thumb-down\"],[\"Out of date\",\"outOfDate\",\"thumb-down\"],[\"Samples / code issue\",\"samplesCodeIssue\",\"thumb-down\"],[\"Other\",\"otherDown\",\"thumb-down\"]],[\"Last updated 2025-07-31 UTC.\"],[],[],null,[\"# Run Gemma using PyTorch\\n\\n\\u003cbr /\\u003e\\n\\n\\n|------------------------------------------------------------------------------|-------------------------------------------------------------------------------------------------------------------------------------------------|-----------------------------------------------------------------------------------------------------------------------------|\\n| [View on ai.google.dev](https://ai.google.dev/gemma/docs/core/pytorch_gemma) | [Run in Google Colab](https://colab.research.google.com/github/google/generative-ai-docs/blob/main/site/en/gemma/docs/core/pytorch_gemma.ipynb) | [View source on GitHub](https://github.com/google/generative-ai-docs/blob/main/site/en/gemma/docs/core/pytorch_gemma.ipynb) |\\n\\nThis guide shows you how to run Gemma using the PyTorch framework, including how\\nto use image data for prompting Gemma release 3 and later models. For more\\ndetails on the Gemma PyTorch implementation, see the project repository\\n[README](https://github.com/google/gemma_pytorch).\\n\\nSetup\\n-----\\n\\nThe following sections explain how to set up your development environment,\\nincluding how get access to Gemma models for downloading from Kaggle, setting\\nauthentication variables, installing dependencies, and importing packages.\\n\\n### System requirements\\n\\nThis Gemma Pytorch library requires GPU or TPU processors to run the Gemma\\nmodel. The standard Colab CPU Python runtime and T4 GPU Python runtime are\\nsufficient for running Gemma 1B, 2B, and 4B size models. For advanced use cases\\nfor other GPUs or TPU, please refer to the\\n[README](https://github.com/google/gemma_pytorch/blob/main/README.md) in the\\nGemma PyTorch repo.\\n\\n### Get access to Gemma on Kaggle\\n\\nTo complete this tutorial, you first need to follow the setup instructions at\\n[Gemma setup](https://ai.google.dev/gemma/docs/setup), which show you how to do\\nthe following:\\n\\n- Get access to Gemma on [Kaggle](https://www.kaggle.com/models/google/gemma/).\\n- Select a Colab runtime with sufficient resources to run the Gemma model.\\n- Generate and configure a Kaggle username and API key.\\n\\nAfter you've completed the Gemma setup, move on to the next section, where\\nyou'll set environment variables for your Colab environment.\\n\\n### Set environment variables\\n\\nSet environment variables for `KAGGLE_USERNAME` and `KAGGLE_KEY`. When prompted\\nwith the \\\"Grant access?\\\" messages, agree to provide secret access. \\n\\n import os\\n from google.colab import userdata # `userdata` is a Colab API.\\n\\n os.environ[\\\"KAGGLE_USERNAME\\\"] = userdata.get('KAGGLE_USERNAME')\\n os.environ[\\\"KAGGLE_KEY\\\"] = userdata.get('KAGGLE_KEY')\\n\\n### Install dependencies\\n\\n pip install -q -U torch immutabledict sentencepiece\\n\\n### Download model weights\\n\\n # Choose variant and machine type\\n VARIANT = '4b-it' \\n MACHINE_TYPE = 'cuda'\\n CONFIG = VARIANT.split('-')[0]\\n\\n import kagglehub\\n\\n # Load model weights\\n weights_dir = kagglehub.model_download(f'google/gemma-3/pyTorch/gemma-3-{VARIANT}')\\n\\nSet the tokenizer and checkpoint paths for the model. \\n\\n # Ensure that the tokenizer is present\\n tokenizer_path = os.path.join(weights_dir, 'tokenizer.model')\\n assert os.path.isfile(tokenizer_path), 'Tokenizer not found!'\\n\\n # Ensure that the checkpoint is present\\n ckpt_path = os.path.join(weights_dir, f'model.ckpt')\\n assert os.path.isfile(ckpt_path), 'PyTorch checkpoint not found!'\\n\\nConfigure the run environment\\n-----------------------------\\n\\nThe following sections explain how to prepare a PyTorch environment for running\\nGemma.\\n\\n### Prepare the PyTorch run environment\\n\\nPrepare the PyTorch model execution environment by cloning the Gemma Pytorch\\nrepository. \\n\\n git clone https://github.com/google/gemma_pytorch.git\\n\\n```\\nCloning into 'gemma_pytorch'...\\nremote: Enumerating objects: 239, done.\\nremote: Counting objects: 100% (123/123), done.\\nremote: Compressing objects: 100% (68/68), done.\\nremote: Total 239 (delta 86), reused 58 (delta 55), pack-reused 116\\nReceiving objects: 100% (239/239), 2.18 MiB | 20.83 MiB/s, done.\\nResolving deltas: 100% (135/135), done.\\n``` \\n\\n import sys\\n\\n sys.path.append('gemma_pytorch/gemma')\\n\\n from gemma_pytorch.gemma.config import get_model_config\\n from gemma_pytorch.gemma.gemma3_model import Gemma3ForMultimodalLM\\n\\n import os\\n import torch\\n\\n### Set the model configuration\\n\\nBefore you run the model, you must set some configuration parameters, including\\nthe Gemma variant, tokenizer and quantization level. \\n\\n # Set up model config.\\n model_config = get_model_config(CONFIG)\\n model_config.dtype = \\\"float32\\\" if MACHINE_TYPE == \\\"cpu\\\" else \\\"float16\\\"\\n model_config.tokenizer = tokenizer_path\\n\\n### Configure the device context\\n\\nThe following code configures the device context for running the model: \\n\\n @contextlib.contextmanager\\n def _set_default_tensor_type(dtype: torch.dtype):\\n \\\"\\\"\\\"Sets the default torch dtype to the given dtype.\\\"\\\"\\\"\\n torch.set_default_dtype(dtype)\\n yield\\n torch.set_default_dtype(torch.float)\\n\\n### Instantiate and load the model\\n\\nLoad the model with its weights to prepare to run requests. \\n\\n device = torch.device(MACHINE_TYPE)\\n with _set_default_tensor_type(model_config.get_dtype()):\\n model = Gemma3ForMultimodalLM(model_config)\\n model.load_state_dict(torch.load(ckpt_path)['model_state_dict'])\\n model = model.to(device).eval()\\n print(\\\"Model loading done.\\\")\\n\\n print('Generating requests in chat mode...')\\n\\nRun inference\\n-------------\\n\\nBelow are examples for generating in chat mode and generating with multiple\\nrequests.\\n\\nThe instruction-tuned Gemma models were trained with a specific formatter that\\nannotates instruction tuning examples with extra information, both during\\ntraining and inference. The annotations (1) indicate roles in a conversation,\\nand (2) delineate turns in a conversation.\\n\\nThe relevant annotation tokens are:\\n\\n- `user`: user turn\\n- `model`: model turn\\n- `\\u003cstart_of_turn\\u003e`: beginning of dialog turn\\n- `\\u003cstart_of_image\\u003e`: tag for image data input\\n- `\\u003cend_of_turn\\u003e\\u003ceos\\u003e`: end of dialog turn\\n\\nFor more information, read about prompt formatting for instruction tuned Gemma\\nmodels [here](https://ai.google.dev/gemma/core/prompt-structure).\\n\\n### Generate text with text\\n\\nThe following is a sample code snippet demonstrating how to format a prompt for\\nan instruction-tuned Gemma model using user and model chat templates in a\\nmulti-turn conversation. \\n\\n # Chat templates\\n USER_CHAT_TEMPLATE = \\\"\\u003cstart_of_turn\\u003euser\\\\n{prompt}\\u003cend_of_turn\\u003e\\u003ceos\\u003e\\\\n\\\"\\n MODEL_CHAT_TEMPLATE = \\\"\\u003cstart_of_turn\\u003emodel\\\\n{prompt}\\u003cend_of_turn\\u003e\\u003ceos\\u003e\\\\n\\\"\\n\\n # Sample formatted prompt\\n prompt = (\\n USER_CHAT_TEMPLATE.format(\\n prompt='What is a good place for travel in the US?'\\n )\\n + MODEL_CHAT_TEMPLATE.format(prompt='California.')\\n + USER_CHAT_TEMPLATE.format(prompt='What can I do in California?')\\n + '\\u003cstart_of_turn\\u003emodel\\\\n'\\n )\\n print('Chat prompt:\\\\n', prompt)\\n\\n model.generate(\\n USER_CHAT_TEMPLATE.format(prompt=prompt),\\n device=device,\\n output_len=256,\\n )\\n\\n```\\nChat prompt:\\n \\u003cstart_of_turn\\u003euser\\nWhat is a good place for travel in the US?\\u003cend_of_turn\\u003e\\u003ceos\\u003e\\n\\u003cstart_of_turn\\u003emodel\\nCalifornia.\\u003cend_of_turn\\u003e\\u003ceos\\u003e\\n\\u003cstart_of_turn\\u003euser\\nWhat can I do in California?\\u003cend_of_turn\\u003e\\u003ceos\\u003e\\n\\u003cstart_of_turn\\u003emodel\\n\\\"California is a state brimming with diverse activities! To give you a great list, tell me: \\\\n\\\\n* **What kind of trip are you looking for?** Nature, City life, Beach, Theme Parks, Food, History, something else? \\\\n* **What are you interested in (e.g., hiking, museums, art, nightlife, shopping)?** \\\\n* **What's your budget like?** \\\\n* **Who are you traveling with?** (family, friends, solo) \\\\n\\\\nThe more you tell me, the better recommendations I can give! üòä \\\\n\\u003cend_of_turn\\u003e\\\"\\n``` \\n\\n # Generate sample\\n model.generate(\\n 'Write a poem about an llm writing a poem.',\\n device=device,\\n output_len=100,\\n )\\n\\n```\\n\\\"\\\\n\\\\nA swirling cloud of data, raw and bold,\\\\nIt hums and whispers, a story untold.\\\\nAn LLM whispers, code into refrain,\\\\nCrafting words of rhyme, a lyrical strain.\\\\n\\\\nA world of pixels, logic's vibrant hue,\\\\nFlows through its veins, forever anew.\\\\nThe human touch it seeks, a gentle hand,\\\\nTo mold and shape, understand.\\\\n\\\\nEmotions it might learn, from snippets of prose,\\\\nInspiration it seeks, a yearning\\\"\\n```\\n\\n### Generate text with images\\n\\nWith Gemma release 3 and later, you can use images with your prompt. The\\nfollowing example shows you how to include visual data with your prompt. \\n\\n print('Chat with images...\\\\n')\\n\\n def read_image(url):\\n import io\\n import requests\\n import PIL\\n\\n contents = io.BytesIO(requests.get(url).content)\\n return PIL.Image.open(contents)\\n\\n image = read_image(\\n 'https://storage.googleapis.com/keras-cv/models/paligemma/cow_beach_1.png'\\n )\\n\\n print(model.generate(\\n [\\n [\\n '\\u003cstart_of_turn\\u003euser\\\\n',\\n image,\\n 'What animal is in this image?\\u003cend_of_turn\\u003e\\\\n',\\n '\\u003cstart_of_turn\\u003emodel\\\\n'\\n ]\\n ],\\n device=device,\\n output_len=256,\\n ))\\n\\nLearn more\\n----------\\n\\nNow that you have learned how to use Gemma in Pytorch, you can explore the many\\nother things that Gemma can do in\\n[ai.google.dev/gemma](https://ai.google.dev/gemma).\\n\\nSee also these other related resources:\\n\\n- [Gemma core models overview](https://ai.google.dev/gemma/docs/core)\\n- [Gemma C++ Tutorial](https://ai.google.dev/gemma/docs/core/gemma_cpp)\\n- [Gemma prompt and system instructions](https://ai.google.dev/gemma/core/prompt-structure)\"]] Terms Privacy Manage cookies English Deutsch Espa√±ol ‚Äì Am√©rica Latina Fran√ßais Indonesia Italiano Polski Portugu√™s ‚Äì Brasil Shqip Ti√™ÃÅng Vi√™Ã£t T√ºrk√ße –†—É—Å—Å–∫–∏–π ◊¢◊ë◊®◊ô◊™ ÿßŸÑÿπÿ±ÿ®ŸäŸëÿ© ŸÅÿßÿ±ÿ≥€å ‡§π‡§ø‡§Ç‡§¶‡•Ä ‡¶¨‡¶æ‡¶Ç‡¶≤‡¶æ ‡∏†‡∏≤‡∏©‡∏≤‡πÑ‡∏ó‡∏¢ ‰∏≠Êñá ‚Äì ÁÆÄ‰Ωì ‰∏≠Êñá ‚Äì ÁπÅÈ´î Êó•Êú¨Ë™û ÌïúÍµ≠Ïñ¥ "
    }
  ]
}