{
  "1-1 (Weights)": "The only explicit weight-related information for the target model appears in three Gemma-containing sentences.  (1) The bare URL \"3https://huggingface.co/google/gemma-2-9b-it\" points to a Hugging Face repository named \"google/gemma-2-9b-it\", directly signalling a public location where the instruction-tuned 2-9 B parameter Gemma variant can be obtained.  (2) An accompanying sentence states: \"We included open models, namely the latest versions of Gemma … all in sizes of between 7 and 9 billion parameters that underwent instruction tuning. 3https://huggingface.co/google/gemma-2-9b-it\".  This simultaneously labels Gemma as an \"open model\" and repeats the same repository link, implying that the weights are openly downloadable for external use.  (3) A further benchmarking sentence confirms real-world accessibility: \"We evaluate a wide range of proprietary and open-source LLMs including … Gemma … on INDICGENBENCH in a variety of settings.\"  Because the authors could load Gemma alongside other open-source models for evaluation, the quote reinforces that the weights are publicly available without special access hurdles.  No other details—such as authentication method, size of the binary files, or fine-grained distribution restrictions—are present in any Gemma-specific sentence.",
  "1-2 (Code)": "Only one Gemma-containing sentence speaks to code availability: \"We perform experiments with a variety of open-source LLMs … Gemma (Team et al., 2024).\"  The phrase \"open-source\" hints that some code exists, but the quote does not clarify whether training pipelines, data-processing scripts, or fine-tuning configurations for Gemma are released.  There is no Gemma sentence that mentions repositories, file paths, or the scope of public code (pre-training versus inference).  Consequently, the provided excerpts offer no concrete evidence that Gemma’s training code—distinct from inference or serving utilities—is publicly accessible.",
  "1-3 (License)": "",
  "1-4 (Paper)": "All information about written artefacts comes from three Gemma-related snippets.  Two identical sentences list Gemma among the models evaluated and cite it formally as \"Gemma (Team et al., 2024)\", signalling the existence of an official technical report or paper authored by the Gemma team in 2024, although the title and URL are not supplied.  Another isolated line simply states \"Gemma 2 9B\", which could be a shorthand reference to a section heading or the model variant discussed in a document.  No link, DOI, arXiv identifier, or blog address is present in the Gemma-specific sentences, so the only bibliographic detail extractable is the citation stub \"Team et al., 2024\".",
  "1-1 (Weights)__evidence": [
    {
      "source": "[pdf_text]",
      "quote": "3https://huggingface.co/google/gemma-2-9b-it"
    },
    {
      "source": "[sections/2502.21228]",
      "quote": "We included open models, namely the latest versions of Gemma,3 Mistral,4 Qwen5, and Olmo,6, all in sizes of between 7 and 9 billion parameters that underwent instruction tuning. 3https://huggingface.co/google/gemma-2-9b-it"
    },
    {
      "source": "[sections/2404.16816]",
      "quote": "We evaluate a wide range of proprietary and open-source LLMs including GPT-3.5, GPT-4, PaLM-2, mT5, Gemma, BLOOM and LLaMA on INDICGENBENCH in a variety of settings."
    }
  ],
  "1-2 (Code)__evidence": [
    {
      "source": "[sections/2404.16816]",
      "quote": "We perform experiments with a variety of open-source LLMs — mT5 (Xue et al., 2021), LLaMA (Tou-vron et al., 2023),6, BLOOMZ (Workshop et al., 2022), Gemma (Team et al., 2024); and proprietary LLMs — GPT-3.5, GPT-4 (OpenAI et al., 2023), and PaLM-2 (Anil et al., 2023)."
    }
  ],
  "1-3 (License)__evidence": [],
  "1-4 (Paper)__evidence": [
    {
      "source": "[sections/Experiments]",
      "quote": "We perform experiments with a variety of open-source LLMs — mT5 (Xue et al., 2021), LLaMA (Touvron et al., 2023),6, BLOOMZ (Workshop et al., 2022), Gemma (Team et al., 2024); and proprietary LLMs — GPT-3.5, GPT-4 (OpenAI et al., 2023), and PaLM-2 (Anil et al., 2023)."
    },
    {
      "source": "[sections/2502.21228]",
      "quote": "We included open models, namely the latest versions of Gemma,3 Mistral,4 Qwen5, and Olmo,6, all in sizes of between 7 and 9 billion parameters that underwent instruction tuning."
    },
    {
      "source": "[sections/2502.21228]",
      "quote": "Gemma 2 9B"
    },
    {
      "source": "[sections/2404.16816]",
      "quote": "We perform experiments with a variety of open-source LLMs — mT5 (Xue et al., 2021), LLaMA (Tou-vron et al., 2023),6, BLOOMZ (Workshop et al., 2022), Gemma (Team et al., 2024); and proprietary LLMs — GPT-3.5, GPT-4 (OpenAI et al., 2023), and PaLM-2 (Anil et al., 2023)."
    }
  ],
  "1-5 (Architecture)": "Gemma is included among the open models sized between 7 B and 9 B parameters that were subjected to instruction tuning. The instruction-tuned Gemma-7B variant performs better than LLaMA-13B and LLaMA-65B on most evaluated tasks.",
  "1-6 (Tokenizer)": "",
  "2-1 (Hardware)": "",
  "2-2 (Software)": "",
  "1-5 (Architecture)__evidence": [
    {
      "source": "[pdf_text]",
      "quote": "We included open models, namely the latest versions of Gemma,3 Mistral,4 Qwen5, and Olmo,6, all in sizes of between 7 and 9 billion parameters that underwent instruction tuning."
    },
    {
      "source": "[pdf_text]",
      "quote": "Gemma-7B instruction tuned model performs better than LLaMA-13B as well as LLaMA-65B on most tasks."
    },
    {
      "source": "[sections/Experiments]",
      "quote": "We included open models, namely the latest versions of Gemma,3 Mistral,4 Qwen5, and Olmo,6, all in sizes of between 7 and 9 billion parameters that underwent instruction tuning."
    }
  ],
  "1-6 (Tokenizer)__evidence": [],
  "2-1 (Hardware)__evidence": [],
  "2-2 (Software)__evidence": [],
  "2-3 (API)": "",
  "3-1 (Pre-training)": "The only explicit pre-training information for Gemma comes from a single performance line: “Gemma-7B-PT 25.7 22.5 23.1 0.6 0.1 0.0 0.0 0.0 16.5 6.9 10.5 40.7 / 58.8 25.5 / 49.5 29.8 / 45.0.”  From this we can state that a pre-trained checkpoint named “Gemma-7B-PT” exists and that it was evaluated, producing a series of numeric scores (beginning with 25.7, 22.5, 23.1, etc.) that likely correspond to multiple benchmarks.  No other details—such as training data composition, data flow, architectural hyper-parameters, training duration, or optimization strategy—are supplied in the quoted material.",
  "3-2 (Fine-tuning)": "Fine-tuning information is narrowly conveyed through two sentences.  First: “Gemma-7B-IT 29.5 23.9 21.9 38.8 24.8 13.9 11.5 10.0 18.9 8.3 12.2 17.6 / 33.7 15.0 / 26.1 21.3 / 27.7.”  This reveals that an instruction-tuned variant of Gemma with 7 B parameters (“Gemma-7B-IT”) exists and has been benchmarked, yielding the listed scores.  Second: “We included open models, namely the latest versions of Gemma … all in sizes of between 7 and 9 billion parameters that underwent instruction tuning.”  Together, these statements confirm that recent Gemma models in the 7-to-9 B parameter range have undergone an instruction-tuning process, and that “Gemma-7B-IT” represents one such fine-tuned release, again accompanied by detailed metric outputs.  No pipeline description, objective details, or hyper-parameter settings for the fine-tuning procedure are provided beyond these numeric results and the assertion that instruction tuning occurred.",
  "3-3 (Reinforcement Learning)": "",
  "2-3 (API)__evidence": [],
  "3-1 (Pre-training)__evidence": [
    {
      "source": "[tables/Table 10]",
      "quote": "Gemma-7B-PT 25.7 22.5 23.1 0.6 0.1 0.0 0.0 0.0 16.5 6.9 10.5 40.7 / 58.8 25.5 / 49.5 29.8 / 45.0"
    }
  ],
  "3-2 (Fine-tuning)__evidence": [
    {
      "source": "[tables/Table 10]",
      "quote": "Gemma-7B-IT 29.5 23.9 21.9 38.8 24.8 13.9 11.5 10.0 18.9 8.3 12.2 17.6 / 33.7 15.0 / 26.1 21.3 / 27.7"
    },
    {
      "source": "[sections/4.1]",
      "quote": "We included open models, namely the latest versions of Gemma,3 Mistral,4 Qwen5, and Olmo,6, all in sizes of between 7 and 9 billion parameters that underwent instruction tuning."
    }
  ],
  "3-3 (Reinforcement Learning)__evidence": [],
  "4-1 (Pre-training Data)": "",
  "4-2 (Fine-tuning Data)": "",
  "4-3 (Reinforcement Learning Data)": "",
  "4-4 (Data Filtering)": "",
  "4-1 (Pre-training Data)__evidence": [],
  "4-2 (Fine-tuning Data)__evidence": [],
  "4-3 (Reinforcement Learning Data)__evidence": [],
  "4-4 (Data Filtering)__evidence": [],
  "__usage": {
    "fine_tuning": "used",
    "rl": "not_used"
  }
}