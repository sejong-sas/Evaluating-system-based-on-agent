{
  "1-1 (Weights)": "The quotes repeatedly label Gemma checkpoints as \"open models\" and state that they have been instruction-tuned and released publicly. One sentence explicitly names the target model size: \"In particular, our novel post-training recipe significantly improves… making Gemma3-4B-IT competitive… We release all our models to the community.\"  Another quote lists Gemma among \"the latest versions of Gemma… all in sizes of between 7 and 9 billion parameters that underwent instruction tuning,\" again stressing openness.  Although the only concrete URL shown is for a different Gemma size (\"https://huggingface.co/google/gemma-2-9b-it\"), it confirms that Gemma weights are distributed on Hugging Face; combined with the statement \"We release all our models to the community,\" it indicates that the google/gemma-3-4b-it weights are likewise downloadable without special access barriers.",
  "1-2 (Code)": "",
  "1-3 (License)": "",
  "1-4 (Paper)": "Several excerpts reference an official publication: \"2025-03-12 Gemma 3 Technical Report Gemma Team, Google DeepMind.\"  The report \"introduce[s] Gemma 3, a multimodal addition to the Gemma family of lightweight open models, ranging in scale from 1 to 27 billion parameters.\"  Additional lines repeat the title \"Gemma 3 Technical Report\" and date, confirming its existence.  Empirical work cited in other sentences shows the report’s context: researchers \"perform experiments with… Gemma\" alongside other large models, and they \"fine-tune multimodal Gemma 3 pre-trained checkpoints\" following published protocols.  Together, the quotes establish that an official technical report dated 12 March 2025 serves as the primary paper describing Gemma 3 (which includes the 4 B-IT variant).",
  "1-1 (Weights)__evidence": [
    {
      "source": "[pdf_text]",
      "quote": "We included open models, namely the latest versions of Gemma,3 Mistral,4 Qwen5, and Olmo,6, all in sizes of between 7 and 9 billion parameters that underwent instruction tuning. 3https://huggingface.co/google/gemma-2-9b-it"
    },
    {
      "source": "[sections/Experiments]",
      "quote": "We evaluate a wide range of proprietary and open-source LLMs including GPT-3.5, GPT-4, PaLM-2, mT5, Gemma, BLOOM and LLaMA on INDICGENBENCH in a variety of settings."
    },
    {
      "source": "[sections/Introduction]",
      "quote": "In particular, our novel post-training recipe significantly improves the math, chat, instruction-following and multilingual abilities, making Gemma3-4B-IT competitive with Gemma2-27B-IT and Gemma3-27B-IT comparable to Gemini-1.5-Pro across benchmarks. We release all our models to the community."
    },
    {
      "source": "[sections/Introduction]",
      "quote": "We included open models, namely the latest versions of Gemma,3 Mistral,4 Qwen5, and Olmo,6, all in sizes of between 7 and 9 billion parameters that underwent instruction tuning."
    },
    {
      "source": "[sections/Introduction]",
      "quote": "3https://huggingface.co/google/gemma-2-9b-it"
    },
    {
      "source": "[pdf_text]",
      "quote": "3https://huggingface.co/google/gemma-2-9b-it"
    },
    {
      "source": "[sections/2503.19786]",
      "quote": "In particular, our novel post-training recipe significantly improves the math, chat, instruction-following and multilingual abilities, making Gemma3-4B-IT competitive with Gemma2-27B-IT and Gemma3-27B-IT comparable to Gemini-1.5-Pro across benchmarks. We release all our models to the community."
    }
  ],
  "1-2 (Code)__evidence": [],
  "1-3 (License)__evidence": [],
  "1-4 (Paper)__evidence": [
    {
      "source": "[sections/Experiments]",
      "quote": "We perform experiments with a variety of open-source LLMs — mT5 (Xue et al., 2021), LLaMA (Touvron et al., 2023), BLOOMZ (Workshop et al., 2022), Gemma (Team et al., 2024); and proprietary LLMs — GPT-3.5, GPT-4 (OpenAI et al., 2023), and PaLM-2 (Anil et al., 2023)."
    },
    {
      "source": "[pdf_text]",
      "quote": "2025-03-12\nGemma 3 Technical Report\nGemma Team, Google DeepMind1\nWe introduce Gemma 3, a multimodal addition to the Gemma family of lightweight open models, ranging in scale from 1 to 27 billion parameters."
    },
    {
      "source": "[title]",
      "quote": "2025-03-12 Gemma 3 Technical Report Gemma Team, Google DeepMind1"
    },
    {
      "source": "[pdf_text]",
      "quote": "Gemma 3 Technical Report"
    },
    {
      "source": "[sections/Introduction]",
      "quote": "We included open models, namely the latest versions of Gemma,3 Mistral,4 Qwen5, and Olmo,6, all in sizes of between 7 and 9 billion parameters that underwent instruction tuning."
    },
    {
      "source": "[sections/Introduction]",
      "quote": "3https://huggingface.co/google/gemma-2-9b-it"
    },
    {
      "source": "[sections/2404.16816]",
      "quote": "We evaluate a wide range of proprietary and open-source LLMs including GPT-3.5, GPT-4, PaLM-2, mT5, Gemma, BLOOM and LLaMA on INDICGENBENCH in a variety of settings."
    },
    {
      "source": "[sections/2503.19786]",
      "quote": "Gemma 3 Technical Report"
    },
    {
      "source": "[sections/2503.19786]",
      "quote": "We introduce Gemma 3, a multimodal addition to the Gemma family of lightweight open models, ranging in scale from 1 to 27 billion parameters."
    },
    {
      "source": "[pdf_text]",
      "quote": "Comparison to PaliGemma 2. We fine-tune multimodal Gemma 3 pre-trained checkpoints following the protocol from Steiner et al. (2024) – only learning rate is swept, otherwise the same transfer settings are used."
    }
  ],
  "1-5 (Architecture)": "The Gemma 3 technical report describes a family of multimodal, decoder-only Transformer models that span 1 B–27 B parameters, with a dedicated 4 B checkpoint that is instruction-tuned (gemma-3-4b-it).  Gemma 3 keeps the same overall architecture as earlier Gemma releases but makes several explicit design choices:\n• Core backbone – “Gemma 3 models follow the same general decoder-only transformer architecture as previous iterations.”  The implementation uses Grouped-Query Attention (GQA) together with both post-norm and pre-norm variants of RMSNorm.\n• Parameter scale – Table 1 lists parameter counts for every size and uses a vocabulary of 256 k tokens.  The 4 B variant therefore inherits this 256 k-entry vocabulary.\n• Vision pathway – “For simplicity, we share the vision encoder across our 4B, 12B and 27B models, keeping it frozen during training.”  Because of an average-pooling stage in that encoder, the 4 B and 12 B checkpoints are “about 10× cheaper to transfer” than the older PaliGemma-2 9 B and 27 B models at 896×896 resolution.\n• Context length – “Gemma 3 models support context length of 128 K tokens,” with only the 1 B model as an exception.  Thus the 4 B instruction-tuned model can also handle 128 K-token inputs.\n• Comparative performance – While the table compares several open models, the report notes that a Gemma-7B instruction-tuned model outperforms LLaMA-13B and LLaMA-65B on most tasks, underscoring the efficiency of the same architectural foundations used in the 4 B model.\nIn summary, gemma-3-4b-it is a 4 B-parameter, decoder-only Transformer that employs GQA with RMSNorm, a frozen shared vision encoder, a 256 k-token vocabulary, and a 128 K token context window, achieving highly competitive quality-to-size efficiency within the Gemma family.",
  "1-6 (Tokenizer)": "The report states, “We use the same tokenizer as Gemini 2.0,” meaning Gemma 3 (including the 4 B instruction-tuned checkpoint) does not introduce a new tokenizer but re-uses the one shipped with prior Gemma/Gemini releases.  Table 4, titled “Formatting for Gemma IT models,” instructs users to “explicitly add the [BOS] token after tokenization, or use the add_bos=True option in the tokenizer.”  Beyond this requirement to prepend a beginning-of-sequence symbol, no additional tokenizer modifications or alternate vocabularies are mentioned in the provided excerpts.",
  "2-1 (Hardware)": "A resource table lists the training setup for the 4 B model as: “4B TPUv5e 2048 16 16 8.”  This explicitly confirms that gemma-3-4b-it was trained on TPUv5e hardware and that a total of 2 048 TPUv5e devices (or cores/chips) were allocated.  Three further numeric columns (16, 16, 8) are shown but not explained in the excerpt; they likely encode additional parallelism or batch-size settings, yet the provided text gives no interpretation.  No GPUs or other accelerator types are cited for the 4 B training run.",
  "2-2 (Software)": "",
  "1-5 (Architecture)__evidence": [
    {
      "source": "[sections/Experiments]",
      "quote": "We included open models, namely the latest versions of Gemma,3 Mistral,4 Qwen5, and Olmo,6, all in sizes of between 7 and 9 billion parameters that underwent instruction tuning."
    },
    {
      "source": "[pdf_text]",
      "quote": "Gemma-7B instruction tuned model performs better than LLaMA-13B as well as LLaMA-65B on most tasks."
    },
    {
      "source": "[pdf_text]",
      "quote": "2025-03-12 Gemma 3 Technical Report\nGemma Team, Google DeepMind1\nWe introduce Gemma 3, a multimodal addition to the Gemma family of lightweight open models, ranging in scale from 1 to 27 billion parameters."
    },
    {
      "source": "[sections/Model Architecture]",
      "quote": "Gemma 3 models follow the same general decoder-only transformer architecture as previous iterations (Vaswani et al., 2017), with most architecture elements similar to the first two Gemma versions."
    },
    {
      "source": "[sections/Model Architecture]",
      "quote": "For simplicity, we share the vision encoder across our 4B, 12B, and 27B models, keeping it frozen during training."
    },
    {
      "source": "[sections/Model Architecture]",
      "quote": "Long context. Gemma 3 models support context length of 128K tokens, with the exception of the 1B model that has 32K."
    },
    {
      "source": "[pdf_text]",
      "quote": "Note that due to average pooling in the vision en- coder the Gemma 3 4B and 12B models are about 10x cheaper to transfer compared with the PaliGemma 2 9B and 27B models at the same 896 x 896 resolution."
    },
    {
      "source": "[sections/2503.19786]",
      "quote": "Gemma 3 models follow the same general decoder-only transformer architecture as previous iterations (Vaswani et al., 2017), with most architecture elements similar to the first two Gemma versions. We use a Grouped-Query Attention (GQA) (Ainslie et al., 2023) with post-norm and pre-norm with RMSNorm (Zhang and Sennrich, 2019)."
    },
    {
      "source": "[sections/2503.19786]",
      "quote": "Long context. Gemma 3 models support context length of 128K tokens, with the exception of the 1B model that has 32K."
    },
    {
      "source": "[sections/2503.19786]",
      "quote": "Table 1 | Parameter counts for the Gemma 3 models. Our vocabulary has 256k entries."
    },
    {
      "source": "[pdf_text]",
      "quote": "The results in Table 12 show that Gemma 3 excels at benchmarks involving document understanding, even outperforming the larger PaliGemma 2 variant. Note that due to average pooling in the vision encoder the Gemma 3 4B and 12B models are about 10x cheaper to transfer compared with the PaliGemma 2 9B and 27B models at the same 896 x 896 resolution."
    }
  ],
  "1-6 (Tokenizer)__evidence": [
    {
      "source": "[sections/Pre-training]",
      "quote": "The pre-training optimization recipe is similar to Gemma 2, with some modifications in the architecture design. We use the same tokenizer as Gemini 2.0, and we also revisit our data mixture to improve the multilingual capabilities of the models, while introducing image understanding."
    },
    {
      "source": "[sections/Formatting/Table 4]",
      "quote": "Table 4 | Formatting for Gemma IT models. Explicitly add the [BOS] token after tokenization, or use the add_bos=True option in the tokenizer."
    },
    {
      "source": "[sections/2503.19786]",
      "quote": "The pre-training optimization recipe is similar to Gemma 2, with some modifications in the architecture design. We use the same tokenizer as Gemini 2.0, and we also revisit our data mixture to improve the multilingual capabilities of the models, while introducing image understanding."
    }
  ],
  "2-1 (Hardware)__evidence": [
    {
      "source": "[sections/Compute Infrastructure/Table 2]",
      "quote": "4B\tTPUv5e\t2048\t16\t16\t8"
    }
  ],
  "2-2 (Software)__evidence": [],
  "2-3 (API)": "",
  "3-1 (Pre-training)": "For Gemma-3, including the 4 B checkpoint, the authors state that the pre-training optimization recipe is largely the same as the earlier Gemma-2 run, with only modest architectural tweaks. Every Gemma-3 scale—1 B, 4 B, 12 B and 27 B—is explicitly \"trained with knowledge distillation (Hinton et al., 2015).\"  The token budget has been increased relative to Gemma-2; the 4 B model is exposed to 4 trillion tokens (other sizes see 2 T, 12 T or 14 T).  Throughout the pre-training benchmark suite the team observes \"consistent improvement over STEM abilities,\" and for coding tasks they single out clear gains for the 4 B and 12 B sizes (the 27 B does not share this code boost).  Overall, the work describes a Gemma-3-4B training pipeline that mirrors Gemma-2 in spirit while scaling data and relying on systematic knowledge-distillation to supervise the run.",
  "3-2 (Fine-tuning)": "The documentation highlights how Gemma-3 base checkpoints, including the 4 B variant, are converted into powerful chat assistants via instruction/post-training.  A comparison set of open models in the 7–9 B range is assembled, and the \"latest versions of Gemma\" undergo the same instruction-tuning process.  In head-to-head tests the instruction-tuned Gemma-7 B outperforms LLaMA-13 B and even LLaMA-65 B across most tasks.  For Gemma-3 specifically, post-training targets mathematics, logical reasoning and chat fluency while also activating new long-context and image-input capabilities; a \"novel post-training approach\" is credited with simultaneous gains in math, coding, instruction following and multilingual use.  The text further notes that pre-trained checkpoints are \"turned into instruction-tuned models\" through an improved recipe (Table 6), yielding results that \"outperform their predecessors by a wide margin.\"  When extending to a multimodal setting, Gemma-3 fine-tuning follows the Steiner et al. (2024) protocol, sweeping only the learning-rate hyper-parameter and keeping all other transfer settings fixed.",
  "3-3 (Reinforcement Learning)": "",
  "2-3 (API)__evidence": [],
  "3-1 (Pre-training)__evidence": [
    {
      "source": "[sections/Introduction]",
      "quote": "The pre-training optimization recipe is similar to Gemma 2, with some modifications in the architecture design. All Gemma 3 models are trained with knowledge distillation (Hinton et al., 2015)."
    },
    {
      "source": "[sections/2.2 Pre-training]",
      "quote": "Training data. We pre-train our models on a slightly larger token budget than Gemma 2, i.e., we train on 14T tokens for Gemma 3 27B, 12T for the 12B version, 4T for the 4B, and 2T tokens for the 1B."
    },
    {
      "source": "[sections/2.2 Pre-training]",
      "quote": "Tokenizer. We use the same tokenizer as Gemini 2.0, and we also revisit our data mixture to improve the multilingual capabilities of the models, while introducing image understanding. All Gemma 3 models are trained with knowledge distillation (Hinton et al., 2015)."
    },
    {
      "source": "[pdf_text]",
      "quote": "In Table 9, we report the performance of our new pre-trained benchmarks compared to previous versions. Overall, our models are in the same ballpark as Gemma 2, which is encouraging since these abilities are not the focus of the improvements brought in this version."
    },
    {
      "source": "[pdf_text]",
      "quote": "Overall we see a consistent improvement over STEM abilities across our pre-trained models. On code, we see a similar improvement for the 4B and 12B models but not on the 27B."
    },
    {
      "source": "[sections/2503.19786]",
      "quote": "We follow a similar recipe as in Gemma 2 for pre-training with knowledge distillation."
    },
    {
      "source": "[sections/2503.19786]",
      "quote": "We pre-train our models on a slightly larger token budget than Gemma 2, i.e., we train on 14T tokens for Gemma 3 27B, 12T for the 12B version, 4T for the 4B, and 2T tokens for the 1B."
    },
    {
      "source": "[sections/2503.19786]",
      "quote": "All Gemma 3 models are trained with knowledge distillation (Hinton et al., 2015)."
    }
  ],
  "3-2 (Fine-tuning)__evidence": [
    {
      "source": "[pdf_text]",
      "quote": "We included open models, namely the latest versions of Gemma,3 Mistral,4 Qwen5, and Olmo,6, all in sizes of between 7 and 9 billion parameters that underwent instruction tuning."
    },
    {
      "source": "[sections/Experiments and Analysis]",
      "quote": "We find that open source LLaMA models perform much worse compared to proprietary models; even the largest LLaMA-65B model significantly underperforms the smallest PaLM-2-XXS model. Gemma-7B instruction tuned model performs better than LLaMA-13B as well as LLaMA-65B on most tasks."
    },
    {
      "source": "[sections/Instruction-Tuning]",
      "quote": "In post-training, we focus our efforts on improving mathematics, reasoning, and chat abilities, as well as integrating the new capabilities of Gemma 3, long-context, and image inputs. We use a novel post-training approach that brings gains across all capabilities, including math, coding, chat, instruction following, and multilingual."
    },
    {
      "source": "[sections/Instruction-Tuning]",
      "quote": "Pre-trained models are turned into instruction-tuned models with an improved post-training approach compared to our prior recipe (see Table 6). The resulting Gemma 3 instruction-tuned models are both powerful and versatile, outperforming their predecessors by a wide margin."
    },
    {
      "source": "[pdf_text]",
      "quote": "Comparison to PaliGemma 2. We fine-tune multimodal Gemma 3 pre-trained checkpoints following the protocol from Steiner et al. (2024) – only learning rate is swept, otherwise the same transfer settings are used."
    },
    {
      "source": "[sections/2502.21228]",
      "quote": "We included open models, namely the latest versions of Gemma,3 Mistral,4 Qwen5, and Olmo,6, all in sizes of between 7 and 9 billion parameters that underwent instruction tuning."
    },
    {
      "source": "[sections/2503.19786]",
      "quote": "In post-training, we focus our efforts on improving mathematics, reasoning, and chat abilities, as well as integrating the new capabilities of Gemma 3, long-context, and image inputs. We use a novel post-training approach that brings gains across all capabilities, including math, coding, chat, instruction following, and multilingual."
    }
  ],
  "3-3 (Reinforcement Learning)__evidence": [],
  "4-1 (Pre-training Data)": "Gemma 3 4B is pre-trained on 4 trillion tokens, a budget explicitly larger than that used for Gemma 2 and sized to accommodate a mixed corpus of images and text. The overall token counts for the series scale with model size—14 T tokens for Gemma 3 27B, 12 T for the 12B model, 4 T for the 4B, and 2 T for the 1B—showing that the 4B checkpoint is situated in the middle of this graduated schedule while still benefiting from the expanded multimodal dataset.",
  "4-2 (Fine-tuning Data)": "During fine-tuning, Gemma 3 4B employs the shared Gemma vision encoder, which receives square images resized to 896 × 896 pixels. The encoder is kept frozen and is reused without modification across the 4B, 12B, and 27B models, while fine-tuning proceeds on data drawn from visual-assistant tasks.",
  "4-3 (Reinforcement Learning Data)": "",
  "4-4 (Data Filtering)": "For Gemma 3 4B, filtering is integral at multiple stages. In the pre-training phase the team performs considerable safety filtering of the data, following Google’s safety policies already applied to Gemini models, to lower the probability that the 4B model will emit harmful content. During and after knowledge-distillation training, additional filters remove unwanted or unsafe utterances as well as personal or other sensitive information, further reducing risk in the released checkpoints.",
  "4-1 (Pre-training Data)__evidence": [
    {
      "source": "[sections/Pre-training]",
      "quote": "Training data. We pre-train our models on a slightly larger token budget than Gemma 2, i.e., we train on 14T tokens for Gemma 3 27B, 12T for the 12B version, 4T for the 4B, and 2T tokens for the 1B."
    },
    {
      "source": "[sections/2503.19786]",
      "quote": "Training data. We pre-train our models on a slightly larger token budget than Gemma 2, i.e., we train on 14T tokens for Gemma 3 27B, 12T for the 12B version, 4T for the 4B, and 2T tokens for the 1B. The increase in tokens accounts for the mix of images and text used during pre-training."
    }
  ],
  "4-2 (Fine-tuning Data)__evidence": [
    {
      "source": "[sections/Vision modality]",
      "quote": "The Gemma vision encoder takes as input square images resized to 896 x 896, and is finetuned on data from visual assistant tasks. For simplicity, we share the vision encoder across our 4B, 12B, and 27B models, keeping it frozen during training."
    }
  ],
  "4-3 (Reinforcement Learning Data)__evidence": [],
  "4-4 (Data Filtering)__evidence": [
    {
      "source": "[sections/Pre-training]",
      "quote": "All Gemma 3 models are trained with knowledge distillation (Hinton et al., 2015). Filtering. We use filtering techniques that reduce the risk of unwanted or unsafe utterances and remove certain personal information and other sensitive data."
    },
    {
      "source": "[sections/2503.19786]",
      "quote": "A key pillar of Gemma’s approach to safety is to align fine-tuned models with Google’s safety policies, in line with Gemini models (Gemini Team, 2023). We undertook considerable safety filtering of our pre-training data to reduce the likelihood of our pre-trained and fine-tuned checkpoints producing harmful content."
    }
  ],
  "__usage": {
    "fine_tuning": "used",
    "rl": "not_used"
  }
}