{
  "1-1 (Weights)": "The quotes repeatedly state that “Gemma models are provided with open weights and permit responsible commercial use, allowing you to tune and deploy them in your own projects and applications.”  Distribution is handled through public hosting: “You can download Gemma 3 models from Kaggle and Hugging Face.”  The family is explicitly lightweight and open: “Our Gemma models are a family of lightweight, state-of-the-art open models built from the same research and technology used to create the Gemini family of models.”  Gemma 3 is offered in five parameter scales—“270 M, 1 B, 4 B, 12 B, and 27 B”—and the 4 B checkpoint is therefore one of the officially released sizes that can be fetched the same way as the others.  All sizes, including 4 B, have “Quantization-Aware Trained checkpoints” so that users can reduce precision while “preserving high-quality.”  The documentation explicitly advertises opportunities to “Customize Gemma open models,” and the project landing page is referenced as “Gemma open models <https://ai.google.dev/gemma>.”  Collectively, the quotes confirm public availability of the 4 B weights, download locations, the open-weights policy, commercial-use permission, and the presence of QAT checkpoints to facilitate deployment on smaller hardware.",
  "1-2 (Code)": "The public tooling emphasized in the quotes focuses on running and fine-tuning rather than end-to-end pre-training.  A usage stack is listed: “Run Gemma Overview | Hugging Face Transformers | Ollama | Gemma library | Keras | PyTorch | Gemma.cpp,” showing that reference implementations or wrappers exist in several ecosystems (Transformers, Keras, PyTorch, C++/Gemma.cpp, and the Ollama runtime).  Fine-tuning support is highlighted twice: “Tune using Gemma library,” and a warning that “Memory requirements for fine-tuning Gemma models are significantly higher than running inference,” implying that fine-tuning scripts or APIs accompany the release.  In addition to core model code, a safety component is open-sourced: “We released ShieldGemma — a series of state-of-the-art safety classifiers that developers can apply to detect and mitigate harmful content in AI model input and outputs.”  No quote explicitly mentions release of the original pre-training pipeline, data-processing scripts, or full training schedules, so only inference code, fine-tuning utilities, and safety classifier code are confirmed public.",
  "1-3 (License)": "The only licensing language provided appears twice, verbatim: “Gemma models are provided with open weights and permit responsible commercial use, allowing you to tune and deploy them in your own projects and applications.”  From this we can extract the following rights:\n(a) Use – explicitly allowed.\n(b) Modification (tuning) – explicitly allowed (“tune”).\n(c) Redistribution – implied by “open weights,” but not further detailed in the quotes.\n(d) Commercial use – explicitly allowed, though conditioned as “responsible commercial use.”\nNo additional clauses (e.g., ‘non-commercial’, ‘research only’, or ‘no derivatives’) are mentioned in the supplied excerpts, nor is a specific license name or version given.",
  "1-4 (Paper)": "Several references describe official and auxiliary documents.  Core technical material is announced as: “For more technical details on Gemma 3, see the Model Card and Technical Report.”  Two short descriptors present the overarching positioning: “Gemma: Open models based on gemini research and technology,” and “2024. Gemma: Open models based on gemini research and technology.”  Risk and safety work is summarized directly in the text: “To understand and reduce the risk profile for Gemma models, we conducted robust evaluations including manual red teaming, automated adversarial testing, and assessments of model capabilities for dangerous activities. These evaluations are outlined in our model cards for Gemma models and include:” (the list itself is not reproduced in the quotes).  The content-moderation add-on is documented in a separate paper or report: “ShieldGemma: Generative AI Content Moderation Based on Gemma,” with another note that “We released ShieldGemma — a series of state-of-the-art safety classifiers…”.  Benchmarking literature further cites Gemma participation: “We evaluate a wide range of proprietary and open-source LLMs including … Gemma … on IndicGenBench,” and “We perform experiments with … BLOOMZ … Gemma (Team et al., 2024)…,” confirming that Gemma results appear in external comparative studies.  Altogether, the quoted material confirms the existence of an official model card, a technical report for Gemma 3, safety-oriented papers (ShieldGemma), and multiple third-party research works that include Gemma in large-scale evaluations.",
  "1-1 (Weights)__evidence": [
    {
      "source": "[pdf_text]",
      "quote": "Gemma models are provided with open weights and permit responsible commercial use , allowing you to tune and deploy them in your own projects and applications."
    },
    {
      "source": "[pdf_text]",
      "quote": "You can download Gemma 3 models from Kaggle and Hugging Face ."
    },
    {
      "source": "[pdf_text]",
      "quote": "Gemma 3 models are available in 5 parameter sizes: 270M, 1B, 4B, 12B, and 27B."
    },
    {
      "source": "[pdf_text]",
      "quote": "For all Gemma 3 models, Quantization-Aware Trained checkpoints are provided, which allow quantizing (reducing the precision), while preserving high-quality."
    },
    {
      "source": "[pdf_text]",
      "quote": "Our Gemma models are a family of lightweight, state-of-the-art open models built from the same research and technology used to create the Gemini family of models."
    },
    {
      "source": "[sections/https://ai.google.dev/gemma/docs/core]",
      "quote": "You can download Gemma 3 models from Kaggle and Hugging Face ."
    },
    {
      "source": "[sections/https://ai.google.dev/gemma/docs/core]",
      "quote": "Gemma models are provided with open weights and permit responsible commercial use , allowing you to tune and deploy them in your own projects and applications."
    },
    {
      "source": "[sections/https://ai.google.dev/gemma/docs/pytorch_gemma]",
      "quote": "*   Customize Gemma open models"
    },
    {
      "source": "[sections/https://ai.google.dev/gemma/docs/pytorch_gemma]",
      "quote": "*   [Gemma open models](https://ai.google.dev/gemma)"
    }
  ],
  "1-2 (Code)__evidence": [
    {
      "source": "[pdf_text]",
      "quote": "Run Gemma Overview Hugging Face Transformers Ollama Gemma library Keras PyTorch Gemma.cpp"
    },
    {
      "source": "[pdf_text]",
      "quote": "Tune using Gemma library"
    },
    {
      "source": "[pdf_text]",
      "quote": "Note: Memory requirements for fine-tuning Gemma models are significantly higher than running inference."
    },
    {
      "source": "[pdf_text]",
      "quote": "• We released ShieldGemma — a series of state-of-the-art safety classifiers that developers can apply to detect and mitigate harmful content in AI model input and outputs."
    }
  ],
  "1-3 (License)__evidence": [
    {
      "source": "[pdf_text]",
      "quote": "Gemma models are provided with open weights and permit responsible commercial use , allowing you to tune and deploy them in your own projects and applications."
    },
    {
      "source": "[sections/https://ai.google.dev/gemma/docs/core]",
      "quote": "Gemma models are provided with open weights and permit responsible commercial use , allowing you to tune and deploy them in your own projects and applications."
    }
  ],
  "1-4 (Paper)__evidence": [
    {
      "source": "[pdf_text]",
      "quote": "For more technical details on Gemma 3, see the Model Card and Technical Report ."
    },
    {
      "source": "[pdf_text]",
      "quote": "For more technical details about previous Gemma models, see the following model card pages: Gemma 2 Model Card Gemma 1 Model Card"
    },
    {
      "source": "[pdf_text]",
      "quote": "We evaluate a wide range of proprietary and open-source LLMs including GPT-3.5, GPT-4, PaLM-2, mT5, Gemma, BLOOM and LLaMA on IndicGenBench in a variety of settings."
    },
    {
      "source": "[pdf_text]",
      "quote": "PaLI (Chen et al., 2022 ), BLIP (Li et al., 2023b ), LLaVA (Liu et al., 2024 ), OpenFlamingo (Awadalla et al., 2023 ), PaLIGemma (Beyer* et al., 2024 ) all follow similar techniques."
    },
    {
      "source": "[pdf_text]",
      "quote": "Beyer* et al. (2024) L. Beyer*, A. Steiner*, A. Susano Pinto*, ... PaliGemma: A versatile 3B VLM for transfer, 2024."
    },
    {
      "source": "[pdf_text]",
      "quote": "ShieldGemma: Generative AI Content Moderation Based on Gemma"
    },
    {
      "source": "[pdf_text]",
      "quote": "Gemma Scope: Open Sparse Autoencoders Everywhere All At Once on Gemma 2"
    },
    {
      "source": "[pdf_text]",
      "quote": "We released ShieldGemma — a series of state-of-the-art safety classifiers that developers can apply to detect and mitigate harmful content in AI model input and outputs."
    },
    {
      "source": "[pdf_text]",
      "quote": "For example, we recently announced Gemma Scope, a new set of tools enabling researchers to “peer inside” the workings of our Gemma 2 model to see how it parses and completes tasks."
    },
    {
      "source": "[pdf_text]",
      "quote": "We only show performance for Gemma-7B-IT, BLOOMZ-7B, LLaMA-65B, GPT-4 and PaLM-2-L models here and report performance for the other models in appendix B.1."
    },
    {
      "source": "[pdf_text]",
      "quote": "Gemma: Open models based on gemini research and technology."
    },
    {
      "source": "[sections/https://ai.google.dev/gemma/docs/core]",
      "quote": "For more technical details on Gemma 3, see the Model Card and Technical Report ."
    },
    {
      "source": "[sections/https://arxiv.org/abs/2404.16816]",
      "quote": "We evaluate a wide range of proprietary and open-source LLMs including GPT-3.5, GPT-4, PaLM-2, mT5, Gemma, BLOOM and LLaMA on IndicGenBench in a variety of settings."
    },
    {
      "source": "[sections/https://arxiv.org/html/2406.09175v1]",
      "quote": "PaLIGemma (Beyer* et al., 2024 ) all follow similar techniques."
    },
    {
      "source": "[sections/https://arxiv.org/html/2406.09175v1]",
      "quote": "Beyer* et al. (2024) L. Beyer*, A. Steiner*, A. Susano Pinto*, A. Kolesnikov*, X. Wang*, D. Salz, M. Neumann, I. Alabdulmohsin, M. Tschannen, E. Bugliarello, T. Unterthiner, D. Keysers, A. Gritsenko, X. Chen, S. Koppula, A. Grycner, M. Bauer, M. Bošnjak, F. Liu, N. Houlsby, M. Kumar, K. Rong, J. Eisenschlos, M. Minderer, P. Voigtlaender, I. Bica, I. Balazevic, J. Puigcerver, P. Papalampidi, O. Henaff, X. Xiong, R. Soricut, J. Harmsen, and X. Zhai*. PaliGemma: A versatile 3B VLM for transfer, 2024."
    },
    {
      "source": "[sections/https://ai.google/static/documents/ai-responsibility-update-published-february-2025.pdf]",
      "quote": "Our Gemma models are a family of lightweight, state-of-the-art open models built from the same research and technology used to create the Gemini family of models."
    },
    {
      "source": "[sections/https://ai.google/static/documents/ai-responsibility-update-published-february-2025.pdf]",
      "quote": "To understand and reduce the risk profile for Gemma models, we conducted robust evaluations including manual red teaming, automated adversarial testing, and assessments of model capabilities for dangerous activities."
    },
    {
      "source": "[sections/https://ai.google/static/documents/ai-responsibility-update-published-february-2025.pdf]",
      "quote": "These evaluations are outlined in our model cards for Gemma models and include:"
    },
    {
      "source": "[sections/https://ai.google/static/documents/ai-responsibility-update-published-february-2025.pdf]",
      "quote": "ShieldGemma: Generative AI Content Moderation Based on Gemma"
    },
    {
      "source": "[sections/https://ai.google/static/documents/ai-responsibility-update-published-february-2025.pdf]",
      "quote": "Gemma Scope: Open Sparse Autoencoders Everywhere All At Once on Gemma 2"
    },
    {
      "source": "[pdf_text]",
      "quote": "We perform experiments with a variety of open-source LLMs — mT5 (Xue et al., 2021), LLaMA (Tou- vron et al., 2023),6, BLOOMZ (Workshop et al., 2022), Gemma (Team et al., 2024); and proprietary LLMs — GPT-3.5, GPT-4 (OpenAI et al., 2023), and PaLM-2 (Anil et al., 2023)."
    },
    {
      "source": "[pdf_text]",
      "quote": "2024. Gemma: Open models based on gemini research and technol- ogy."
    }
  ]
}