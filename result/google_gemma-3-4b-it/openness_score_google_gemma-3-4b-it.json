{
  "model": "google/gemma-3-4b-it",
  "scores": {
    "1-1 Weights": {
      "score": 1,
      "reason": "Weights files present in the repository (e.g., *.safetensors/bin/pt/ckpt)."
    },
    "1-2 Code": {
      "score": 0.0,
      "reason": "No training pipeline files; README mentions are ignored."
    },
    "1-3 License": {
      "score": 1.0,
      "reason": "The dedicated “Gemma” license allows use, modification, redistribution and commercial use with only minor safety-related restrictions, which the rubric still counts as Open."
    },
    "1-4 Paper": {
      "score": 1.0,
      "reason": "An official Gemma 3 Technical Report that specifically covers this model is publicly available."
    },
    "1-5 Architecture": {
      "score": 1,
      "reason": "Architecture evidence present in extracted quotes."
    },
    "1-6 Tokenizer": {
      "score": 1.0,
      "reason": "Tokenizer files downloadable in repository."
    },
    "2-1 Hardware": {
      "score": 1.0,
      "reason": "Quotes state the training hardware type (TPUv5e) and the exact quantity (2048 chips) for the 4 B run."
    },
    "2-2 Software": {
      "score": 0.5,
      "reason": "Training stack mentions JAX plus the ML Pathways distributed system but gives no deeper, reconstructable configuration details."
    },
    "2-3 API": {
      "score": 1.0,
      "reason": "Only generic or non-official mentions; no qualifying official API docs.  Official API docs via web: https://cloud.google.com/kubernetes-engine/docs/tutorials/serve-gemma-gpu-vllm, https://cloud.google.com/run/docs/tutorials/gpu-gemma-with-ollama, https://cloud.google.com/run/docs/run-gemma-on-cloud-run"
    },
    "3-1 Pre-training": {
      "score": 0.5,
      "reason": "Recipe elements (knowledge-distillation use, token budgets, TPUs) are mentioned, but not enough detail for full reproduction."
    },
    "3-2 Fine-tuning": {
      "score": 0.5,
      "reason": "Instruction/post-training is described in prose, yet concrete hyper-parameters and data pipelines are absent."
    },
    "3-3 Reinforcement Learning": {
      "score": 0.5,
      "reason": "RLHF is acknowledged and general reward types are listed, but no reproducible procedural specifics are supplied."
    },
    "4-1 Pre-training Data": {
      "score": 0.5,
      "reason": "Token counts and ‘wide variety of sources’ are given, but the exact corpora, proportions and licenses are not."
    },
    "4-2 Fine-tuning Data": {
      "score": 0.5,
      "reason": "Fine-tuning is said to use visual-assistant data with a frozen vision encoder, but datasets are not enumerated."
    },
    "4-3 Reinforcement Learning Data": {
      "score": 0.5,
      "reason": "Only generic mention of human-feedback and other rewards; no dataset compositions or sizes are provided."
    },
    "4-4 Data Filtering": {
      "score": 0.5,
      "reason": "Safety filtering is repeatedly referenced, yet the concrete pipelines, thresholds or tooling remain undisclosed."
    }
  },
  "included_scores": {
    "1-1 Weights": {
      "score": 1,
      "reason": "Weights files present in the repository (e.g., *.safetensors/bin/pt/ckpt)."
    },
    "1-2 Code": {
      "score": 0.0,
      "reason": "No training pipeline files; README mentions are ignored."
    },
    "1-3 License": {
      "score": 1.0,
      "reason": "The dedicated “Gemma” license allows use, modification, redistribution and commercial use with only minor safety-related restrictions, which the rubric still counts as Open."
    },
    "1-4 Paper": {
      "score": 1.0,
      "reason": "An official Gemma 3 Technical Report that specifically covers this model is publicly available."
    },
    "1-5 Architecture": {
      "score": 1,
      "reason": "Architecture evidence present in extracted quotes."
    },
    "1-6 Tokenizer": {
      "score": 1.0,
      "reason": "Tokenizer files downloadable in repository."
    },
    "2-1 Hardware": {
      "score": 1.0,
      "reason": "Quotes state the training hardware type (TPUv5e) and the exact quantity (2048 chips) for the 4 B run."
    },
    "2-2 Software": {
      "score": 0.5,
      "reason": "Training stack mentions JAX plus the ML Pathways distributed system but gives no deeper, reconstructable configuration details."
    },
    "2-3 API": {
      "score": 1.0,
      "reason": "Only generic or non-official mentions; no qualifying official API docs.  Official API docs via web: https://cloud.google.com/kubernetes-engine/docs/tutorials/serve-gemma-gpu-vllm, https://cloud.google.com/run/docs/tutorials/gpu-gemma-with-ollama, https://cloud.google.com/run/docs/run-gemma-on-cloud-run"
    },
    "3-1 Pre-training": {
      "score": 0.5,
      "reason": "Recipe elements (knowledge-distillation use, token budgets, TPUs) are mentioned, but not enough detail for full reproduction."
    },
    "3-2 Fine-tuning": {
      "score": 0.5,
      "reason": "Instruction/post-training is described in prose, yet concrete hyper-parameters and data pipelines are absent."
    },
    "3-3 Reinforcement Learning": {
      "score": 0.5,
      "reason": "RLHF is acknowledged and general reward types are listed, but no reproducible procedural specifics are supplied."
    },
    "4-1 Pre-training Data": {
      "score": 0.5,
      "reason": "Token counts and ‘wide variety of sources’ are given, but the exact corpora, proportions and licenses are not."
    },
    "4-2 Fine-tuning Data": {
      "score": 0.5,
      "reason": "Fine-tuning is said to use visual-assistant data with a frozen vision encoder, but datasets are not enumerated."
    },
    "4-3 Reinforcement Learning Data": {
      "score": 0.5,
      "reason": "Only generic mention of human-feedback and other rewards; no dataset compositions or sizes are provided."
    },
    "4-4 Data Filtering": {
      "score": 0.5,
      "reason": "Safety filtering is repeatedly referenced, yet the concrete pipelines, thresholds or tooling remain undisclosed."
    }
  },
  "final_score_10pt": 6.875,
  "meta": {
    "usage_from_dispatch": {
      "fine_tuning": "used",
      "rl": "used"
    },
    "excluded": [],
    "denominator": 16,
    "raw_sum": 11.0,
    "scale": "10/16",
    "code_detection_reason": "No training pipeline files; README mentions are ignored.",
    "pretrain_sources_used": false
  }
}