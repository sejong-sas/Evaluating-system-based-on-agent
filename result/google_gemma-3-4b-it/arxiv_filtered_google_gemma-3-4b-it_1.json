{
  "1-1 (Weights)": "The quotes repeatedly label Gemma checkpoints as \"open models\" and state that they have been instruction-tuned and released publicly. One sentence explicitly names the target model size: \"In particular, our novel post-training recipe significantly improves… making Gemma3-4B-IT competitive… We release all our models to the community.\"  Another quote lists Gemma among \"the latest versions of Gemma… all in sizes of between 7 and 9 billion parameters that underwent instruction tuning,\" again stressing openness.  Although the only concrete URL shown is for a different Gemma size (\"https://huggingface.co/google/gemma-2-9b-it\"), it confirms that Gemma weights are distributed on Hugging Face; combined with the statement \"We release all our models to the community,\" it indicates that the google/gemma-3-4b-it weights are likewise downloadable without special access barriers.",
  "1-2 (Code)": "",
  "1-3 (License)": "",
  "1-4 (Paper)": "Several excerpts reference an official publication: \"2025-03-12 Gemma 3 Technical Report Gemma Team, Google DeepMind.\"  The report \"introduce[s] Gemma 3, a multimodal addition to the Gemma family of lightweight open models, ranging in scale from 1 to 27 billion parameters.\"  Additional lines repeat the title \"Gemma 3 Technical Report\" and date, confirming its existence.  Empirical work cited in other sentences shows the report’s context: researchers \"perform experiments with… Gemma\" alongside other large models, and they \"fine-tune multimodal Gemma 3 pre-trained checkpoints\" following published protocols.  Together, the quotes establish that an official technical report dated 12 March 2025 serves as the primary paper describing Gemma 3 (which includes the 4 B-IT variant).",
  "1-1 (Weights)__evidence": [
    {
      "source": "[pdf_text]",
      "quote": "We included open models, namely the latest versions of Gemma,3 Mistral,4 Qwen5, and Olmo,6, all in sizes of between 7 and 9 billion parameters that underwent instruction tuning. 3https://huggingface.co/google/gemma-2-9b-it"
    },
    {
      "source": "[sections/Experiments]",
      "quote": "We evaluate a wide range of proprietary and open-source LLMs including GPT-3.5, GPT-4, PaLM-2, mT5, Gemma, BLOOM and LLaMA on INDICGENBENCH in a variety of settings."
    },
    {
      "source": "[sections/Introduction]",
      "quote": "In particular, our novel post-training recipe significantly improves the math, chat, instruction-following and multilingual abilities, making Gemma3-4B-IT competitive with Gemma2-27B-IT and Gemma3-27B-IT comparable to Gemini-1.5-Pro across benchmarks. We release all our models to the community."
    },
    {
      "source": "[sections/Introduction]",
      "quote": "We included open models, namely the latest versions of Gemma,3 Mistral,4 Qwen5, and Olmo,6, all in sizes of between 7 and 9 billion parameters that underwent instruction tuning."
    },
    {
      "source": "[sections/Introduction]",
      "quote": "3https://huggingface.co/google/gemma-2-9b-it"
    },
    {
      "source": "[pdf_text]",
      "quote": "3https://huggingface.co/google/gemma-2-9b-it"
    },
    {
      "source": "[sections/2503.19786]",
      "quote": "In particular, our novel post-training recipe significantly improves the math, chat, instruction-following and multilingual abilities, making Gemma3-4B-IT competitive with Gemma2-27B-IT and Gemma3-27B-IT comparable to Gemini-1.5-Pro across benchmarks. We release all our models to the community."
    }
  ],
  "1-2 (Code)__evidence": [],
  "1-3 (License)__evidence": [],
  "1-4 (Paper)__evidence": [
    {
      "source": "[sections/Experiments]",
      "quote": "We perform experiments with a variety of open-source LLMs — mT5 (Xue et al., 2021), LLaMA (Touvron et al., 2023), BLOOMZ (Workshop et al., 2022), Gemma (Team et al., 2024); and proprietary LLMs — GPT-3.5, GPT-4 (OpenAI et al., 2023), and PaLM-2 (Anil et al., 2023)."
    },
    {
      "source": "[pdf_text]",
      "quote": "2025-03-12\nGemma 3 Technical Report\nGemma Team, Google DeepMind1\nWe introduce Gemma 3, a multimodal addition to the Gemma family of lightweight open models, ranging in scale from 1 to 27 billion parameters."
    },
    {
      "source": "[title]",
      "quote": "2025-03-12 Gemma 3 Technical Report Gemma Team, Google DeepMind1"
    },
    {
      "source": "[pdf_text]",
      "quote": "Gemma 3 Technical Report"
    },
    {
      "source": "[sections/Introduction]",
      "quote": "We included open models, namely the latest versions of Gemma,3 Mistral,4 Qwen5, and Olmo,6, all in sizes of between 7 and 9 billion parameters that underwent instruction tuning."
    },
    {
      "source": "[sections/Introduction]",
      "quote": "3https://huggingface.co/google/gemma-2-9b-it"
    },
    {
      "source": "[sections/2404.16816]",
      "quote": "We evaluate a wide range of proprietary and open-source LLMs including GPT-3.5, GPT-4, PaLM-2, mT5, Gemma, BLOOM and LLaMA on INDICGENBENCH in a variety of settings."
    },
    {
      "source": "[sections/2503.19786]",
      "quote": "Gemma 3 Technical Report"
    },
    {
      "source": "[sections/2503.19786]",
      "quote": "We introduce Gemma 3, a multimodal addition to the Gemma family of lightweight open models, ranging in scale from 1 to 27 billion parameters."
    },
    {
      "source": "[pdf_text]",
      "quote": "Comparison to PaliGemma 2. We fine-tune multimodal Gemma 3 pre-trained checkpoints following the protocol from Steiner et al. (2024) – only learning rate is swept, otherwise the same transfer settings are used."
    }
  ]
}