{
  "1-1 (Weights)": "The only explicit weight-related information for the target model appears in three Gemma-containing sentences.  (1) The bare URL \"3https://huggingface.co/google/gemma-2-9b-it\" points to a Hugging Face repository named \"google/gemma-2-9b-it\", directly signalling a public location where the instruction-tuned 2-9 B parameter Gemma variant can be obtained.  (2) An accompanying sentence states: \"We included open models, namely the latest versions of Gemma … all in sizes of between 7 and 9 billion parameters that underwent instruction tuning. 3https://huggingface.co/google/gemma-2-9b-it\".  This simultaneously labels Gemma as an \"open model\" and repeats the same repository link, implying that the weights are openly downloadable for external use.  (3) A further benchmarking sentence confirms real-world accessibility: \"We evaluate a wide range of proprietary and open-source LLMs including … Gemma … on INDICGENBENCH in a variety of settings.\"  Because the authors could load Gemma alongside other open-source models for evaluation, the quote reinforces that the weights are publicly available without special access hurdles.  No other details—such as authentication method, size of the binary files, or fine-grained distribution restrictions—are present in any Gemma-specific sentence.",
  "1-2 (Code)": "Only one Gemma-containing sentence speaks to code availability: \"We perform experiments with a variety of open-source LLMs … Gemma (Team et al., 2024).\"  The phrase \"open-source\" hints that some code exists, but the quote does not clarify whether training pipelines, data-processing scripts, or fine-tuning configurations for Gemma are released.  There is no Gemma sentence that mentions repositories, file paths, or the scope of public code (pre-training versus inference).  Consequently, the provided excerpts offer no concrete evidence that Gemma’s training code—distinct from inference or serving utilities—is publicly accessible.",
  "1-3 (License)": "",
  "1-4 (Paper)": "All information about written artefacts comes from three Gemma-related snippets.  Two identical sentences list Gemma among the models evaluated and cite it formally as \"Gemma (Team et al., 2024)\", signalling the existence of an official technical report or paper authored by the Gemma team in 2024, although the title and URL are not supplied.  Another isolated line simply states \"Gemma 2 9B\", which could be a shorthand reference to a section heading or the model variant discussed in a document.  No link, DOI, arXiv identifier, or blog address is present in the Gemma-specific sentences, so the only bibliographic detail extractable is the citation stub \"Team et al., 2024\".",
  "1-1 (Weights)__evidence": [
    {
      "source": "[pdf_text]",
      "quote": "3https://huggingface.co/google/gemma-2-9b-it"
    },
    {
      "source": "[sections/2502.21228]",
      "quote": "We included open models, namely the latest versions of Gemma,3 Mistral,4 Qwen5, and Olmo,6, all in sizes of between 7 and 9 billion parameters that underwent instruction tuning. 3https://huggingface.co/google/gemma-2-9b-it"
    },
    {
      "source": "[sections/2404.16816]",
      "quote": "We evaluate a wide range of proprietary and open-source LLMs including GPT-3.5, GPT-4, PaLM-2, mT5, Gemma, BLOOM and LLaMA on INDICGENBENCH in a variety of settings."
    }
  ],
  "1-2 (Code)__evidence": [
    {
      "source": "[sections/2404.16816]",
      "quote": "We perform experiments with a variety of open-source LLMs — mT5 (Xue et al., 2021), LLaMA (Tou-vron et al., 2023),6, BLOOMZ (Workshop et al., 2022), Gemma (Team et al., 2024); and proprietary LLMs — GPT-3.5, GPT-4 (OpenAI et al., 2023), and PaLM-2 (Anil et al., 2023)."
    }
  ],
  "1-3 (License)__evidence": [],
  "1-4 (Paper)__evidence": [
    {
      "source": "[sections/Experiments]",
      "quote": "We perform experiments with a variety of open-source LLMs — mT5 (Xue et al., 2021), LLaMA (Touvron et al., 2023),6, BLOOMZ (Workshop et al., 2022), Gemma (Team et al., 2024); and proprietary LLMs — GPT-3.5, GPT-4 (OpenAI et al., 2023), and PaLM-2 (Anil et al., 2023)."
    },
    {
      "source": "[sections/2502.21228]",
      "quote": "We included open models, namely the latest versions of Gemma,3 Mistral,4 Qwen5, and Olmo,6, all in sizes of between 7 and 9 billion parameters that underwent instruction tuning."
    },
    {
      "source": "[sections/2502.21228]",
      "quote": "Gemma 2 9B"
    },
    {
      "source": "[sections/2404.16816]",
      "quote": "We perform experiments with a variety of open-source LLMs — mT5 (Xue et al., 2021), LLaMA (Tou-vron et al., 2023),6, BLOOMZ (Workshop et al., 2022), Gemma (Team et al., 2024); and proprietary LLMs — GPT-3.5, GPT-4 (OpenAI et al., 2023), and PaLM-2 (Anil et al., 2023)."
    }
  ]
}