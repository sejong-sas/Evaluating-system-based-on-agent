{
  "1-1 (Weights)": "The quotes explicitly state that “Gemma 3 models are multimodal, handling text and image input and generating text output, with open weights for both pre-trained variants and instruction-tuned variants.”  This confirms that the weights for every officially released Gemma 3 model type (both the base pre-trained checkpoints and their instruction-tuned counterparts) are openly provided.  A second quote adds the concrete distribution channel and access condition: “To access Gemma on Hugging Face, you’re required to review and agree to Google’s usage license.”  Together, these sentences show that although the weights are openly released, users must first pass through a license-gating click-through on Hugging Face before download or direct use, indicating that the weights are publicly hosted but subject to acceptance of Google’s terms.",
  "1-1 (Weights)__evidence": [
    {
      "source": "[readme]",
      "quote": "Gemma 3 models are multimodal, handling text and image input and generating text output, with open weights for both pre-trained variants and instruction-tuned variants."
    },
    {
      "source": "[readme]",
      "quote": "To access Gemma on Hugging Face, you’re required to review and agree to Google’s usage license."
    }
  ],
  "1-2 (Code)": "",
  "1-2 (Code)__evidence": [],
  "1-3 (License)": "Licensing details are conveyed in three separate quotes.  First, the model repository declares the short identifier “license: gemma,” signalling a proprietary Google license specific to Gemma resources rather than a standard open-source license.  Second, the distribution page warns: “To access Gemma on Hugging Face, you’re required to review and agree to Google’s usage license,” clarifying that any user must explicitly consent to Google’s own legal terms before obtaining the model.  Third, the policy scope is constrained by the statement: “Prohibited uses of Gemma models are outlined in the [Gemma Prohibited Use Policy][prohibited-use],” which implies an accompanying document enumerating forbidden activities.  Synthesising the three points, Gemma models are governed by a dedicated Google ‘gemma’ license that permits access only after affirmative agreement, and that same license references an additional Prohibited Use Policy that delineates disallowed applications.",
  "1-3 (License)__evidence": [
    {
      "source": "[readme]",
      "quote": "license: gemma"
    },
    {
      "source": "[readme]",
      "quote": "To access Gemma on Hugging Face, you’re required to review and agree to Google’s usage license."
    },
    {
      "source": "[readme]",
      "quote": "Prohibited uses of Gemma models are outlined in the [Gemma Prohibited Use Policy][prohibited-use]."
    }
  ],
  "1-4 (Paper)": "Two references confirm the existence of an official publication.  The first uses markdown link syntax—“* [Gemma 3 Technical Report][g3-tech-report]”—signalling that a formal technical report for Gemma 3 is publicly linked.  The second quote—“@article{gemma_2025,”—shows that a bib-entry or citation key is provided, suggesting that the technical report (or potentially a journal or conference article) is citable.  Taken together, these lines establish that an official technical document named “Gemma 3 Technical Report” exists, is publicly referenced in the repository, and has an accompanying bibliographic citation entry labelled ‘gemma_2025.’",
  "1-4 (Paper)__evidence": [
    {
      "source": "[readme]",
      "quote": "* [Gemma 3 Technical Report][g3-tech-report]"
    },
    {
      "source": "[readme]",
      "quote": "@article{gemma_2025,"
    }
  ],
  "1-5 (Architecture)": "For the gemma 4b variant, as well as the 12B and 27B versions, the model can handle a total input context of 128 K tokens; the 1B size is limited to 32 K tokens. The amount of data used during pre-training scales with size: the 27B model saw 14 trillion tokens, the 12B model 12 trillion, the gemma 4b model 4 trillion, and the 1B model 2 trillion. In the broader Gemma 3 release—which includes the gemma 4b size—this architecture delivers the large 128 K context window, provides multilingual support for more than 140 languages, and is offered in more size options than earlier Gemma versions.",
  "1-5 (Architecture)__evidence": [
    {
      "source": "[readme]",
      "quote": "Total input context of 128K tokens for the 4B, 12B, and 27B sizes, and 32K tokens for the 1B size"
    },
    {
      "source": "[readme]",
      "quote": "The 27B model was trained with 14 trillion tokens, the 12B model was trained with 12 trillion tokens, 4B model was trained with 4 trillion tokens and 1B with 2 trillion tokens."
    },
    {
      "source": "[readme]",
      "quote": "Gemma 3 has a large, 128K context window, multilingual support in over 140 languages, and is available in more sizes than previous versions."
    }
  ],
  "1-6 (Tokenizer)": "",
  "1-6 (Tokenizer)__evidence": [],
  "2-1 (Hardware)": "Gemma was trained on Google’s Tensor Processing Unit (TPU) hardware, specifically deploying the TPUv4p, TPUv5p, and TPUv5e generations for its training runs.",
  "2-1 (Hardware)__evidence": [
    {
      "source": "[readme]",
      "quote": "Gemma was trained using [Tensor Processing Unit (TPU)][tpu] hardware (TPUv4p, TPUv5p and TPUv5e)."
    }
  ],
  "2-2 (Software)": "Gemma 3, including the gemma 4b size, is directly supported in the Hugging Face transformers library beginning with version 4.50.0.",
  "2-2 (Software)__evidence": [
    {
      "source": "[readme]",
      "quote": "Gemma 3 is supported starting from transformers 4.50.0."
    }
  ],
  "2-3 (API)": "Gemma 3 is natively integrated into the Hugging Face Transformers code-base beginning with version 4.50.0. As soon as a user upgrades to Transformers ≥ 4.50.0, every standard API entry-point exposed by the library (model loading, tokenization, generate(), etc.) recognises the Gemma family. Consequently, developers obtain GPT-style programmatic access to Gemma 3 through a mature, well-documented interface without any extra wrappers or custom code, allowing immediate public use in scripts, notebooks, or production services.",
  "2-3 (API)__evidence": [
    {
      "source": "[readme]",
      "quote": "Gemma 3 is supported starting from transformers 4.50.0."
    }
  ],
  "3-1 (Pre-training)": "Within the Gemma portfolio, the 4B model was pre-trained on a corpus of 4 trillion tokens. (For context, other sizes in the same run received 14 T tokens for the 27B variant, 12 T for the 12B variant and 2 T for the 1B variant.) All of these large-scale training runs, including the 4B checkpoint, were executed on Google Tensor Processing Unit hardware—specifically TPUv4p, TPUv5p and TPUv5e devices—indicating a high-performance, massively parallel training pipeline optimised for TPU architectures.",
  "3-1 (Pre-training)__evidence": [
    {
      "source": "[readme]",
      "quote": "The 27B model was trained with 14 trillion tokens, the 12B model was trained with 12 trillion tokens, 4B model was trained with 4 trillion tokens and 1B with 2 trillion tokens."
    },
    {
      "source": "[readme]",
      "quote": "Gemma was trained using [Tensor Processing Unit (TPU)][tpu] hardware (TPUv4p, TPUv5p and TPUv5e)."
    }
  ],
  "3-2 (Fine-tuning)": "Gemma 3 models have publicly released open weights for both their base checkpoints and their instruction-tuned counterparts. The instruction-tuned versions extend the original pre-training by allowing the models to operate in a multimodal setting: they accept combined text-and-image inputs and return text-only outputs. Because the tuned weights are openly available, external practitioners can inspect, reproduce, or further fine-tune the Gemma 3 checkpoints on their own datasets with full transparency.",
  "3-2 (Fine-tuning)__evidence": [
    {
      "source": "[readme]",
      "quote": "Gemma 3 models are multimodal, handling text and image input and generating text output, with open weights for both pre-trained variants and instruction-tuned variants."
    }
  ],
  "3-3 (Reinforcement Learning)": "",
  "3-3 (Reinforcement Learning)__evidence": [],
  "4-1 (Pre-training Data)": "A single sentence in the provided material specifies the scale of pre-training for the Gemma 4B variant: “the 4B model was trained with 4 trillion tokens.”  The same sentence also notes, for context, that larger and smaller companion models (27B, 12B, 1B) were trained with different token counts, but the only figure tied directly to the Gemma 4B model is the 4 trillion-token total, giving the sole quantitative insight into the amount of raw text consumed during its pre-training stage.",
  "4-1 (Pre-training Data)__evidence": [
    {
      "source": "[readme]",
      "quote": "The 27B model was trained with 14 trillion tokens, the 12B model was trained with 12 trillion tokens, 4B model was trained with 4 trillion tokens and 1B with 2 trillion tokens."
    }
  ],
  "4-2 (Fine-tuning Data)": "",
  "4-2 (Fine-tuning Data)__evidence": [],
  "4-3 (Reinforcement Learning Data)": "",
  "4-3 (Reinforcement Learning Data)__evidence": [],
  "4-4 (Data Filtering)": "The material states that during preparation of the Gemma pre-training corpus, “automated techniques were used to filter out certain personal information and other sensitive data.”  This indicates the presence of a safety-oriented data-cleaning step that removes personally identifiable or otherwise sensitive content before or during model training, with the explicit goal of making the Gemma pre-trained models “safe and reliable.”",
  "4-4 (Data Filtering)__evidence": [
    {
      "source": "[readme]",
      "quote": "-   Sensitive Data Filtering: As part of making Gemma pre-trained models safe and reliable, automated techniques were used to filter out certain personal information and other sensitive data from training sets."
    }
  ],
  "__usage": {
    "fine_tuning": "used",
    "rl": "unknown"
  }
}