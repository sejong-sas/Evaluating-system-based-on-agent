{
  "1-1 (Weights)": "None of the supplied quotations mention anything about the availability of the phi-4 model weights. There is no reference to whether checkpoints have been released, where they might be hosted, what access mechanisms (open download, gated request, etc.) would apply, or whether the weights are entirely private. Consequently, from the excerpts provided, no information can be extracted regarding public access, storage location, distribution portals, or any percentage of parameters that might be made available or withheld.",
  "1-2 (Code)": "The excerpts contain no statements about the existence or release status of training code for phi-4. There is no description of data-preparation scripts, configuration files, scheduling logic, or end-to-end training pipelines, nor is there any mention of inference-only code repositories. Therefore, based strictly on the provided material, it is impossible to determine whether any part of the pre-training, fine-tuning, or post-training (e.g., RLHF) code has been open-sourced, partially shared, or kept proprietary.",
  "1-3 (License)": "No licensing language appears in the quotations. They do not name a license (e.g., MIT, Apache-2.0, custom research-only license), nor do they specify permissions or restrictions on use, modification, redistribution, or commercial exploitation. As a result, the supplied text offers no insight into licensing terms, allowable scope of use, or any grant/limitation clauses governing phi-4.",
  "1-4 (Paper)": "The quoted passages collectively outline the main points presented in the paper introducing phi-4. The authors state that they “present phi-4, a 14-billion parameter language model developed with a training recipe that is centrally focused on data quality.” They frame the model as the latest member of the Phi family—citing earlier work ([GZA+23, LBE+23, JBA+23, AAA+24])—and emphasize that phi-4 “further advances performance of small language models by introducing innovative synthetic data generation methods for reasoning-focused tasks, by optimizing the training curriculum and data mixture, and by introducing new techniques in post-training.” The paper also notes that “all external models we tested were published before this date, as were the datasets for all stages of phi-4’s training,” underscoring that every reference corpus and baseline model predates phi-4’s development timeline. Finally, the authors comment on qualitative behavior, observing that “in agent tasks, phi-4 follows a characteristic chain-of-thought with explicit planning and reflections.” Taken together, the paper positions phi-4 as a 14-billion-parameter successor within the Phi series that achieves its gains through refined data curation, synthetic reasoning-oriented augmentation, carefully balanced training curricula, and novel post-training techniques, while demonstrating deliberate multi-step reasoning patterns in downstream agent settings.",
  "1-1 (Weights)__evidence": [],
  "1-2 (Code)__evidence": [],
  "1-3 (License)__evidence": [],
  "1-4 (Paper)__evidence": [
    {
      "source": "[pdf_text]",
      "quote": "We present phi-4, a 14-billion parameter language model developed with a training recipe that is centrally focused on data quality."
    },
    {
      "source": "[pdf_text]",
      "quote": "Building on the success of the Phi family [GZA+23, LBE+23, JBA+23, AAA+24], we introduce phi-4, a 14-billion parameter model that further advances performance of small language models by introducing innovative synthetic data generation methods for reasoning-focused tasks, by optimizing the training curriculum and data mixture, and by introducing new techniques in post-training."
    },
    {
      "source": "[sections/https://arxiv.org/pdf/2412.08905]",
      "quote": "We present phi-4, a 14-billion parameter language model developed with a training recipe that is centrally focused on data quality. Building on the success of the Phi family [GZA+23, LBE+23, JBA+23, AAA+24], we introduce phi-4, a 14-billion parameter model that further advances performance of small language models by introducing innovative synthetic data generation methods for reasoning-focused tasks, by optimizing the training curriculum and data mixture, and by introducing new techniques in post-training."
    },
    {
      "source": "[pdf_text]",
      "quote": "All external models we tested were published before this date, as were the datasets for all stages of phi-4’s training."
    },
    {
      "source": "[pdf_text]",
      "quote": "In agent tasks, phi-4 follows a characteristic chain-of-thought with explicit planning and reflections."
    }
  ],
  "1-5 (Architecture)": "phi-4 is implemented as a decoder-only Transformer. It contains 14 billion trainable parameters and is initially configured for a 4 096-token context window. A dedicated mid-training phase explicitly lengthens the usable context to 16 K tokens; to make this possible the model’s RoPE positional encoding base frequency is enlarged to 250 K. The overall network design is intentionally aligned with the earlier phi-3-medium model, but phi-4 abandons the latter’s 2 K sliding-window attention and instead uses full attention across the entire 4 K (and later 16 K) sequence.",
  "1-6 (Tokenizer)": "phi-4 switches to the tiktoken tokenizer, chosen for improved multilingual handling. The tokenizer’s vocabulary is padded to 100 352 entries (some of which remain unused). This tokenizer replaces the custom tokenizer used in phi-3-medium and pairs with full attention over the initial 4 K context length.",
  "2-1 (Hardware)": "",
  "2-2 (Software)": "",
  "1-5 (Architecture)__evidence": [
    {
      "source": "[pdf_text]",
      "quote": "The phi-4 model is based on a decoder-only transformer architecture [VSP+17] with 14B parameters and a default context length of 4096. This is later extended to a 16K context length during midtraining."
    },
    {
      "source": "[sections/https://arxiv.org/pdf/2412.08905]",
      "quote": "We present phi-4, a 14-billion parameter language model developed with a training recipe that is centrally focused on data quality."
    },
    {
      "source": "[sections/https://arxiv.org/pdf/2412.08905]",
      "quote": "The phi-4 model is based on a decoder-only transformer architecture [VSP+17]"
    },
    {
      "source": "[sections/Pretraining details]",
      "quote": "The phi-4 model is based on a decoder-only transformer architecture [VSP+17] with 14B parameters and a default context length of 4096. This is later extended to a 16K context length during midtraining."
    },
    {
      "source": "[sections/Pretraining details]",
      "quote": "The architecture closely follows phi-3-medium, except that we now use the tiktoken tokenizer (for better multilingual support) with a padded vocabulary size of 100,352 (including unused tokens) and we use full attention over the 4K context length, rather than a 2K sliding window used in phi-3-medium."
    },
    {
      "source": "[sections/Midtraining Details]",
      "quote": "phi-4 includes a midtraining stage where the context length is increased from the original 4K to 16K. To accommodate longer context, we increase the base frequency of rope position encoding to 250K following [AI23b]."
    }
  ],
  "1-6 (Tokenizer)__evidence": [
    {
      "source": "[pdf_text]",
      "quote": "The phi-4 model is based on a decoder-only transformer architecture [VSP+17] with 14B parameters and a default context length of 4096. The architecture closely follows phi-3-medium, except that we now use the tiktoken tokenizer (for better multilingual support) with a padded vocabulary size of 100,352 (including unused tokens) and we use full attention over the 4K context length, rather than a 2K sliding window used in phi-3-medium."
    },
    {
      "source": "[sections/Pretraining details]",
      "quote": "The architecture closely follows phi-3-medium, except that we now use the tiktoken tokenizer (for better multilingual support) with a padded vocabulary size of 100,352 (including unused tokens) and we use full attention over the 4K context length, rather than a 2K sliding window used in phi-3-medium."
    }
  ],
  "2-1 (Hardware)__evidence": [],
  "2-2 (Software)__evidence": [],
  "2-3 (API)": "",
  "3-1 (Pre-training)": "Phi-4’s pre-training is centered on a 14-billion-parameter, decoder-only Transformer that initially operates with a 4 096-token context window and later, during a dedicated “mid-training” phase, is expanded to 16 K tokens.  The architecture largely mirrors phi-3-medium but switches to the tiktoken tokenizer, yielding a padded vocabulary of 100 352 tokens, and replaces phi-3’s 2 K sliding-window attention with full attention across the entire 4 K sequence.  The model is exposed to roughly 10 trillion training tokens under a schedule that linearly warms up then decays the learning rate, peaking at 0.0003; weight decay is held constant at 0.1 and the global batch size is 5 760.  \n\nData composition is heavily skewed toward synthetic sources.  The team “changes [the] training curriculum” so that synthetic text becomes the dominant component, supplementing it with only 30 % web-derived material—half raw web, half web rewrites.  Synthetic corpora are produced through multi-agent prompting, self-revision workflows, instruction reversal, and other automated techniques, all governed by a guideline of broad topical and skill “diversity.”  All external models and datasets referenced for any stage of phi-4’s training pre-date the project’s start, ensuring temporal consistency of sources.",
  "3-2 (Fine-tuning)": "After pre-training, phi-4 undergoes a supervised fine-tuning (SFT) phase that expressly targets hallucination reduction.  The team assembles refined SFT datasets by inspecting base-model behaviour: (question, correct answer) pairs are kept where the raw model is usually correct; (question, refusal) pairs are substituted where it is usually wrong; and (bogus question, refusal) pairs cover nonsensical prompts.  In parallel, they devise a new method—pivotal-token search (PTS)—to create preference pairs that will later feed Direct Preference Optimization.  These combined SFT resources constitute the “post-training recipe” for upgrading phi-4’s factual reliability and refusal behaviour.",
  "3-3 (Reinforcement Learning)": "Reinforcement-style post-training hinges on Direct Preference Optimization (DPO) augmented by rejection sampling and the pivotal-token-search (PTS) mechanism.  Using the refined SFT sets as a base, the team generates DPO preference pairs with explicit ranking criteria: for questions that the base phi-4 sometimes answers correctly, they craft (correct > refusal) pairs; for questions it sometimes answers incorrectly, they create (refusal > wrong) pairs.  The PTS technique selects critical tokens within candidate answers to construct these contrasting examples, thereby steering optimisation toward the model’s stronger reasoning modes.  The overall goal is to curb hallucinations—without these interventions, phi-4 “would almost never admit to ignorance”—and to polish answer quality through iterative preference-based feedback.",
  "2-3 (API)__evidence": [],
  "3-1 (Pre-training)__evidence": [
    {
      "source": "[pdf_text]",
      "quote": "Synthetic data constitutes the bulk of the training data for phi-4 and is generated using a diverse array of techniques, including multi-agent prompting, self-revision workflows, and instruction reversal."
    },
    {
      "source": "[pdf_text]",
      "quote": "The pretraining phase of phi-4 relies heavily on synthetic datasets generated through a variety of techniques."
    },
    {
      "source": "[pdf_text]",
      "quote": "The phi-4 model is based on a decoder-only transformer architecture [VSP+17] with 14B parameters and a default context length of 4096. This is later extended to a 16K context length during midtraining. The architecture closely follows phi-3-medium, except that we now use the tiktoken tokenizer (for better multilingual support) with a padded vocabulary size of 100,352 (including unused tokens) and we use full attention over the 4K context length, rather than a 2K sliding window used in phi-3-medium. The model was pretrained for approximately 10T tokens using linear warm-up and decay schedules with peak learning rate of 0.0003, constant weight decay of 0.1, and global batch size of 5760."
    },
    {
      "source": "[pdf_text]",
      "quote": "The final data mixture used for phi-4 allocates 30% of the training tokens to web and web rewrites data sources, divided equally between them."
    },
    {
      "source": "[sections/https://arxiv.org/pdf/2412.08905]",
      "quote": "Synthetic data constitutes the bulk of the training data for phi-4 and is generated using a diverse array of techniques, including multi-agent prompting, self-revision workflows, and instruction reversal."
    },
    {
      "source": "[sections/https://arxiv.org/pdf/2412.08905]",
      "quote": "The pretraining phase of phi-4 relies heavily on synthetic datasets generated through a variety of techniques."
    },
    {
      "source": "[sections/https://arxiv.org/pdf/2412.08905]",
      "quote": "We change our training curriculum and create new pretraining and midtraining data mixtures to increase the allocation of synthetic tokens, compared to older generations of phi."
    },
    {
      "source": "[sections/https://arxiv.org/pdf/2412.08905]",
      "quote": "Our approach to generating synthetic data for phi-4 is guided by the following principles: 1. Diversity: The data should comprehensively cover subtopics and skills within each domain."
    },
    {
      "source": "[pdf_text]",
      "quote": "The phi-4 model is based on a decoder-only transformer architecture [VSP+17] with 14B parameters and a default context length of 4096. The model was pretrained for approximately 10T tokens using linear warm-up and decay schedules with peak learning rate of 0.0003, constant weight decay of 0.1, and global batch size of 5760."
    },
    {
      "source": "[pdf_text]",
      "quote": "phi-4 includes a midtraining stage where the context length is increased from the original 4K to 16K."
    },
    {
      "source": "[pdf_text]",
      "quote": "All external models we tested were published before this date, as were the datasets for all stages of phi-4’s training."
    }
  ],
  "3-2 (Fine-tuning)__evidence": [
    {
      "source": "[pdf_text]",
      "quote": "We further advance the post-training recipe in phi-4 by creating new refined versions of SFT datasets, as well as by developing a new technique to create DPO pairs, based on pivotal token search."
    },
    {
      "source": "[sections/https://arxiv.org/pdf/2412.08905]",
      "quote": "We created post-training SFT and DPO data to mitigate hallucinations in simple settings. Without any mitigation, phi-4 would almost never admit to ignorance."
    },
    {
      "source": "[sections/https://arxiv.org/pdf/2412.08905]",
      "quote": "For SFT data, we used the pair (question, correct answer) wherever the base phi-4 model was usually correct, (question, refusal) where the model was usually wrong, and (bogus question, refusal) for all bogus questions."
    }
  ],
  "3-3 (Reinforcement Learning)__evidence": [
    {
      "source": "[pdf_text]",
      "quote": "Synthetic data in phi-4 also plays a crucial role in post-training, where techniques such as rejection sampling and a novel approach to Direct Preference Optimization (DPO) are employed to refine the model’s outputs."
    },
    {
      "source": "[pdf_text]",
      "quote": "We further advance the post-training recipe in phi-4 by creating new refined versions of SFT datasets, as well as by developing a new technique to create DPO pairs, based on pivotal token search."
    },
    {
      "source": "[sections/https://arxiv.org/pdf/2412.08905]",
      "quote": "For DPO data, we used (correct > refusal) for every question that the base phi-4 sometimes answered correctly, and (refusal > wrong) if phi-4 sometimes answered incorrectly."
    },
    {
      "source": "[sections/https://arxiv.org/pdf/2412.08905]",
      "quote": "We created post-training SFT and DPO data to mitigate hallucinations in simple settings. Without any mitigation, phi-4 would almost never admit to ignorance."
    },
    {
      "source": "[pdf_text]",
      "quote": "Now, consider how the solution from Figure 3 would be used in DPO as a full-length accepted response. By generating DPO data targeting such choices, we believe PTS helps phi-4 work better in the modes it is especially stronger."
    }
  ],
  "4-1 (Pre-training Data)": "According to the supplied material, phi-4’s pre-training corpus is dominated by synthetic text. The team \"created 50 broad types of synthetic datasets, each one relying on a different set of seeds and different multi-stage prompting procedure, spanning an array of topics, skills, and natures of interaction, accumulating to a total of about 400 B un-weighted tokens.\" Synthetic data is therefore described as \"the bulk of the training data for phi-4\" and is produced with methods such as \"multi-agent prompting, self-revision workflows, and instruction reversal.\" A precise token-allocation for the final mixture is reported: \"30 % of the training tokens\" come from \"web and web rewrites\" (split 50/50), \"40 %\" from synthetic data, \"20 %\" from code (a mix of synthetic and raw), and the final \"10 %\" from \"targeted acquired sources like academic data and books.\" The authors emphasize that, \"unlike most language models, where pre-training is based primarily on organic data sources such as web content or code, phi-4 strategically incorporates synthetic data throughout the training process.\"",
  "4-2 (Fine-tuning Data)": "The post-training phase for phi-4 relies on supervised fine-tuning (SFT) data that has been deliberately re-worked. The team \"further advance[s] the post-training recipe in phi-4 by creating new refined versions of SFT datasets.\" A concrete construction pipeline is outlined with trivia-style questions: \"We started with seed trivia problems, such as from TriviaQA [JCWZ17]. For each question, we ran phi-4 multiple times to estimate its chance of accurately solving it.\" Instance-level outcomes of those runs decide label selection: \"For SFT data, we used the pair (question, correct answer) wherever the base phi-4 model was usually correct, (question, refusal) where the model was usually wrong, and (bogus question, refusal) for all bogus questions.\"",
  "4-3 (Reinforcement Learning Data)": "For reinforcement learning–style preference optimization, phi-4 employs Direct Preference Optimization (DPO). The reference text states: \"We further advance the post-training recipe in phi-4 by ... developing a new technique to create DPO pairs, based on pivotal token search.\" DPO pairs are built with explicit relational labels: \"For DPO data, we used (correct > refusal) for every question that the base phi-4 sometimes answered correctly, and (refusal > wrong) if phi-4 sometimes answered incorrectly.\" These pairings constitute the preference dataset used in the RL-style stage.",
  "4-4 (Data Filtering)": "Curation and filtering are treated as a foundational design element: \"The development of phi-4 is guided by three core pillars: 2. Curation and Filtering of High-Quality Organic Data.\" For organic inputs such as \"web content, licensed books, and code repositories,\" the team \"meticulously curate[s] and filter[s]\" them so that they can \"extract seeds for the synthetic data pipeline that encourage high-depth reasoning and prioritize educational value.\" The decontamination pipeline is upgraded relative to earlier Phi releases: \"We improved the data decontamination process for phi-4 compared to previous Phi models to ensure no unfair influence on evaluation results.\" A specific mid-training intervention is described: \"phi-4 includes a midtraining stage where the context length is increased from the original 4K to 16K. Inspired by this, we further filter our high-quality non-synthetic datasets (i.e., academic, books, and code data) to separate samples above 8K context.\" Benchmark leakage is addressed explicitly: \"We decontaminate against the ARC-Easy, MBPP, phibench, CommonsenseQA, WinoGrande, mcphi, MedQA, MATH, AGIEval, PIQA, OpenBookQA, HellaSwag, GPQA, mt-bench, MMLUPro, GSM8k, HumanEval, arena hard, ARC-Challenge, and MMLU benchmarks.\" The implementation relies on \"a hybrid n-gram algorithm for decontamination which uses 13-gram and 7-gram features for removing matches to the test set.\"",
  "4-1 (Pre-training Data)__evidence": [
    {
      "source": "[pdf_text]",
      "quote": "Synthetic data constitutes the bulk of the training data for phi-4 and is generated using a diverse array of techniques, including multi-agent prompting, self-revision workflows, and instruction reversal."
    },
    {
      "source": "[pdf_text]",
      "quote": "The pretraining phase of phi-4 relies heavily on synthetic datasets generated through a variety of techniques."
    },
    {
      "source": "[pdf_text]",
      "quote": "The final data mixture used for phi-4 allocates 30% of the training tokens to web and web rewrites data sources, divided equally between them. The remaining tokens are largely sourced from synthetic data which accounts for 40% of the data mixture tokens. Finally we allocate 20% of tokens to code data (mixture of synthetic and raw code) and 10% to targeted acquired sources like academic data and books."
    },
    {
      "source": "[pdf_text]",
      "quote": "Unlike most language models, where pre-training is based primarily on organic data sources such as web content or code, phi-4 strategically incorporates synthetic data throughout the training process."
    },
    {
      "source": "[pdf_text]",
      "quote": "We created 50 broad types of synthetic datasets, each one relying on a different set of seeds and different multi-stage prompting procedure, spanning an array of topics, skills, and natures of interaction, accumulating to a total of about 400B unweighted tokens. Here, we highlight novel methodologies used in generating synthetic datasets for phi-4:"
    }
  ],
  "4-2 (Fine-tuning Data)__evidence": [
    {
      "source": "[pdf_text]",
      "quote": "Post-Training: We further advance the post-training recipe in phi-4 by creating new refined versions of SFT datasets, as well as by developing a new technique to create DPO pairs, based on pivotal token search."
    },
    {
      "source": "[pdf_text]",
      "quote": "We started with seed trivia problems, such as from TriviaQA [JCWZ17]. For each question, we ran phi-4 multiple times to estimate its chance of accurately solving it."
    },
    {
      "source": "[pdf_text]",
      "quote": "For SFT data, we used the pair (question, correct answer) wherever the base phi-4 model was usually correct, (question, refusal) where the model was usually wrong, and (bogus question, refusal) for all bogus questions."
    }
  ],
  "4-3 (Reinforcement Learning Data)__evidence": [
    {
      "source": "[pdf_text]",
      "quote": "Post-Training: We further advance the post-training recipe in phi-4 by creating new refined versions of SFT datasets, as well as by developing a new technique to create DPO pairs, based on pivotal token search."
    },
    {
      "source": "[pdf_text]",
      "quote": "For DPO data, we used (correct > refusal) for every question that the base phi-4 sometimes answered correctly, and (refusal > wrong) if phi-4 sometimes answered incorrectly."
    }
  ],
  "4-4 (Data Filtering)__evidence": [
    {
      "source": "[pdf_text]",
      "quote": "The development of phi-4 is guided by three core pillars: 2. Curation and Filtering of High-Quality Organic Data: We meticulously curate and filter organic2 data sources, including web content, licensed books, and code repositories to extract seeds for the synthetic data pipeline that encourage high-depth reasoning and prioritize educational value (to the model)."
    },
    {
      "source": "[pdf_text]",
      "quote": "We improved the data decontamination process for phi-4 compared to previous Phi models to ensure no unfair influence on evaluation results."
    },
    {
      "source": "[pdf_text]",
      "quote": "phi-4 includes a midtraining stage where the context length is increased from the original 4K to 16K. Inspired by this, we further filter our high-quality non-synthetic datasets (i.e., academic, books, and code data) to separate samples above 8K context."
    },
    {
      "source": "[pdf_text]",
      "quote": "We decontaminate against the ARC-Easy, MBPP, phibench, CommonsenseQA, WinoGrande, mcphi, MedQA, MATH, AGIEval, PIQA, OpenBookQA, HellaSwag, GPQA, mt-bench, MMLUPro, GSM8k, HumanEval, arena hard, ARC-Challenge, and MMLU benchmarks. We apply a hybrid n-gram algorithm for decontamination which uses 13-gram and 7-gram features for removing matches to the test set, which is described in more detail in 1."
    }
  ],
  "__usage": {
    "fine_tuning": "used",
    "rl": "used"
  }
}