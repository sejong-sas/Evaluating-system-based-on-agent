{
  "4-1 (Pre-training Data)": "For phi-4, the developers explicitly state that the model “is a state-of-the-art open model built upon a blend of synthetic datasets, data from filtered public domain websites, and acquired academic books and Q&A datasets.”  The team discloses an overall size of 9.8 trillion training tokens.  A more granular description lists four principal data groupings: (1) “Publicly available documents filtered rigorously for quality, selected high-quality educational data, and code,” (2) “newly created synthetic, ‘textbook-like’ data” aimed at teaching math, coding, common-sense reasoning, and broad world knowledge, (3) “acquired academic books and Q&A datasets,” and (4) “high-quality chat format supervised data” that reflects preferences for “instruct-following, truthfulness, honesty and helpfulness.”  Together, these sources extend the data already used for Phi-3 and are intended to supply diverse coverage—ranging from filtered public-domain web text through licensed academic material to purpose-built synthetic tutorials—while achieving the declared 9.8 T-token scale.",
  "4-1 (Pre-training Data)__evidence": [
    {
      "source": "[readme]",
      "quote": "phi-4 is a state-of-the-art open model built upon a blend of synthetic datasets, data from filtered public domain websites, and acquired academic books and Q&A datasets."
    },
    {
      "source": "[readme]",
      "quote": "| **Training data**       | 9.8T tokens                                                                   |"
    },
    {
      "source": "[readme]",
      "quote": "Our training data is an extension of the data used for Phi-3 and includes a wide variety of sources from:\n\n1. Publicly available documents filtered rigorously for quality, selected high-quality educational data, and code.\n\n2. Newly created synthetic, “textbook-like” data for the purpose of teaching math, coding, common sense reasoning, general knowledge of the world (science, daily activities, theory of mind, etc.).\n\n3. Acquired academic books and Q&A datasets.\n\n4. High quality chat format supervised data covering various topics to reflect human preferences on different aspects such as instruct-following, truthfulness, honesty and helpfulness."
    }
  ],
  "4-2 (Fine-tuning Data)": "The fine-tuning stage for phi-4 is described as a “rigorous enhancement and alignment process” that combines supervised fine-tuning (SFT) with direct preference optimization (DPO).  The documentation emphasizes that this phase incorporates “both open-source and in-house generated synthetic datasets,” underscoring a dual strategy of leveraging publicly available instruction data alongside proprietary, safety-oriented synthetic examples.  The objective is to secure “precise instruction adherence and robust safety measures,” demonstrating that the alignment corpus specifically targets safety, helpfulness, and general instruction following capacity.",
  "4-2 (Fine-tuning Data)__evidence": [
    {
      "source": "[readme]",
      "quote": "`phi-4` underwent a rigorous enhancement and alignment process, incorporating both supervised fine-tuning and direct preference optimization to ensure precise instruction adherence and robust safety measures"
    },
    {
      "source": "[readme]",
      "quote": "`phi-4` has adopted a robust safety post-training approach. This approach leverages a variety of both open-source and in-house generated synthetic datasets."
    }
  ],
  "4-3 (Reinforcement Learning Data)": "For reinforcement-learning–style post-training, phi-4 relies on “a combination of SFT … and iterative DPO,” and draws on “publicly available datasets focusing on helpfulness and harmlessness as well as various questions and answers targeted to multiple safety categories.”  This indicates that after the supervised phase, the model is repeatedly optimized against paired preference data—sourced from the public domain—covering broad safety themes, with the explicit intent of bolstering harmlessness and helpfulness.",
  "4-3 (Reinforcement Learning Data)__evidence": [
    {
      "source": "[readme]",
      "quote": "`phi-4` has adopted a robust safety post-training approach. The overall technique employed to do the safety alignment is a combination of SFT (Supervised Fine-Tuning) and iterative DPO (Direct Preference Optimization), including publicly available datasets focusing on helpfulness and harmlessness as well as various questions and answers targeted to multiple safety categories."
    }
  ],
  "4-4 (Data Filtering)": "All public-domain material entering phi-4’s corpus is subjected to quality filtering.  The authors note that the model uses “data from filtered public domain websites,” and that “publicly available documents [are] filtered rigorously for quality” with a focus on “selected high-quality educational data, and code.”  They further explain that they “filter the publicly available documents to contain the correct level of knowledge,” indicating a curation stage specifically designed to accept only content deemed educationally valuable and factually accurate before any pre-training proceeds.",
  "4-4 (Data Filtering)__evidence": [
    {
      "source": "[readme]",
      "quote": "phi-4 is a state-of-the-art open model built upon a blend of synthetic datasets, data from filtered public domain websites, and acquired academic books and Q&A datasets."
    },
    {
      "source": "[readme]",
      "quote": "Publicly available documents filtered rigorously for quality, selected high-quality educational data, and code."
    },
    {
      "source": "[readme]",
      "quote": "We are focusing on the quality of data that could potentially improve the reasoning ability for the model, and we filter the publicly available documents to contain the correct level of knowledge."
    }
  ]
}