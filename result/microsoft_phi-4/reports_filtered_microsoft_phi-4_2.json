{
  "1-5 (Architecture)": "phi-4 is implemented as a decoder-only Transformer. It contains 14 billion trainable parameters and is initially configured for a 4 096-token context window. A dedicated mid-training phase explicitly lengthens the usable context to 16 K tokens; to make this possible the model’s RoPE positional encoding base frequency is enlarged to 250 K. The overall network design is intentionally aligned with the earlier phi-3-medium model, but phi-4 abandons the latter’s 2 K sliding-window attention and instead uses full attention across the entire 4 K (and later 16 K) sequence.",
  "1-6 (Tokenizer)": "phi-4 switches to the tiktoken tokenizer, chosen for improved multilingual handling. The tokenizer’s vocabulary is padded to 100 352 entries (some of which remain unused). This tokenizer replaces the custom tokenizer used in phi-3-medium and pairs with full attention over the initial 4 K context length.",
  "2-1 (Hardware)": "",
  "2-2 (Software)": "",
  "1-5 (Architecture)__evidence": [
    {
      "source": "[pdf_text]",
      "quote": "The phi-4 model is based on a decoder-only transformer architecture [VSP+17] with 14B parameters and a default context length of 4096. This is later extended to a 16K context length during midtraining."
    },
    {
      "source": "[sections/https://arxiv.org/pdf/2412.08905]",
      "quote": "We present phi-4, a 14-billion parameter language model developed with a training recipe that is centrally focused on data quality."
    },
    {
      "source": "[sections/https://arxiv.org/pdf/2412.08905]",
      "quote": "The phi-4 model is based on a decoder-only transformer architecture [VSP+17]"
    },
    {
      "source": "[sections/Pretraining details]",
      "quote": "The phi-4 model is based on a decoder-only transformer architecture [VSP+17] with 14B parameters and a default context length of 4096. This is later extended to a 16K context length during midtraining."
    },
    {
      "source": "[sections/Pretraining details]",
      "quote": "The architecture closely follows phi-3-medium, except that we now use the tiktoken tokenizer (for better multilingual support) with a padded vocabulary size of 100,352 (including unused tokens) and we use full attention over the 4K context length, rather than a 2K sliding window used in phi-3-medium."
    },
    {
      "source": "[sections/Midtraining Details]",
      "quote": "phi-4 includes a midtraining stage where the context length is increased from the original 4K to 16K. To accommodate longer context, we increase the base frequency of rope position encoding to 250K following [AI23b]."
    }
  ],
  "1-6 (Tokenizer)__evidence": [
    {
      "source": "[pdf_text]",
      "quote": "The phi-4 model is based on a decoder-only transformer architecture [VSP+17] with 14B parameters and a default context length of 4096. The architecture closely follows phi-3-medium, except that we now use the tiktoken tokenizer (for better multilingual support) with a padded vocabulary size of 100,352 (including unused tokens) and we use full attention over the 4K context length, rather than a 2K sliding window used in phi-3-medium."
    },
    {
      "source": "[sections/Pretraining details]",
      "quote": "The architecture closely follows phi-3-medium, except that we now use the tiktoken tokenizer (for better multilingual support) with a padded vocabulary size of 100,352 (including unused tokens) and we use full attention over the 4K context length, rather than a 2K sliding window used in phi-3-medium."
    }
  ],
  "2-1 (Hardware)__evidence": [],
  "2-2 (Software)__evidence": []
}