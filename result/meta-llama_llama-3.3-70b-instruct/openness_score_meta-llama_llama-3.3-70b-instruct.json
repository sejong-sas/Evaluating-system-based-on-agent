{
  "model": "meta-llama/Llama-3.3-70B-Instruct",
  "scores": {
    "1-1 Weights": {
      "score": 1,
      "reason": "Weights files present in the repository (e.g., *.safetensors/bin/pt/ckpt)."
    },
    "1-2 Code": {
      "score": 0.0,
      "reason": "No training pipeline files; README mentions are ignored."
    },
    "1-3 License": {
      "score": 1.0,
      "reason": "Recognized permissive license (e.g., MIT/Apache-2.0/BSD/MPL/CC0/CC-BY) in quotes."
    },
    "1-4 Paper": {
      "score": 0.5,
      "reason": "An official Llama 3 technical report exists, but no separate peer-reviewed paper specific to the 3.3 refresh is cited – counted as Semi-Open."
    },
    "1-5 Architecture": {
      "score": 1,
      "reason": "Architecture evidence present in extracted quotes."
    },
    "1-6 Tokenizer": {
      "score": 1.0,
      "reason": "Tokenizer files downloadable in repository."
    },
    "2-1 Hardware": {
      "score": 1.0,
      "reason": "\"Training utilized 39.3 M GPU-hours on H100-80 GB\" and table rows list counts (e.g., 2 040 GPUs / ‘up to 16 K H100s’), giving both type and quantity."
    },
    "2-2 Software": {
      "score": 0.5,
      "reason": "Quote mentions “custom training libraries” on Meta’s cluster but no full stack or versions – partial disclosure only."
    },
    "2-3 API": {
      "score": 1.0,
      "reason": "Official API docs present in quotes: https://llama.developer.meta.com/join_waitlist, https://llama.developer.meta.com/join_waitlist  Web search found no official API docs."
    },
    "3-1 Pre-training": {
      "score": 0.0,
      "reason": "No concrete method details in quotes."
    },
    "3-2 Fine-tuning": {
      "score": 0.0,
      "reason": "No concrete method details in quotes."
    },
    "3-3 Reinforcement Learning": {
      "score": 0.5,
      "reason": "Partial methodology disclosed (techniques/algorithms mentioned) but not fully reproducible."
    },
    "4-1 Pre-training Data": {
      "score": 0.5,
      "reason": "High-level description (15 T multilingual, public web/code, Dec-2023 cutoff) is given, but not a source-by-source listing to rebuild the corpus."
    },
    "4-2 Fine-tuning Data": {
      "score": 0.5,
      "reason": "Lists human prompts, synthetic data, 25 M example size, etc., yet datasets are not fully released or enumerated."
    },
    "4-3 Reinforcement Learning Data": {
      "score": 0.5,
      "reason": "States millions of human preference judgements and multi-turn comparisons, but no detailed dataset release."
    },
    "4-4 Data Filtering": {
      "score": 1.0,
      "reason": "Filtering pipeline disclosed (multiple categories + thresholds/ratios and/or explicit stages or tools)."
    }
  },
  "included_scores": {
    "1-1 Weights": {
      "score": 1,
      "reason": "Weights files present in the repository (e.g., *.safetensors/bin/pt/ckpt)."
    },
    "1-2 Code": {
      "score": 0.0,
      "reason": "No training pipeline files; README mentions are ignored."
    },
    "1-3 License": {
      "score": 1.0,
      "reason": "Recognized permissive license (e.g., MIT/Apache-2.0/BSD/MPL/CC0/CC-BY) in quotes."
    },
    "1-4 Paper": {
      "score": 0.5,
      "reason": "An official Llama 3 technical report exists, but no separate peer-reviewed paper specific to the 3.3 refresh is cited – counted as Semi-Open."
    },
    "1-5 Architecture": {
      "score": 1,
      "reason": "Architecture evidence present in extracted quotes."
    },
    "1-6 Tokenizer": {
      "score": 1.0,
      "reason": "Tokenizer files downloadable in repository."
    },
    "2-1 Hardware": {
      "score": 1.0,
      "reason": "\"Training utilized 39.3 M GPU-hours on H100-80 GB\" and table rows list counts (e.g., 2 040 GPUs / ‘up to 16 K H100s’), giving both type and quantity."
    },
    "2-2 Software": {
      "score": 0.5,
      "reason": "Quote mentions “custom training libraries” on Meta’s cluster but no full stack or versions – partial disclosure only."
    },
    "2-3 API": {
      "score": 1.0,
      "reason": "Official API docs present in quotes: https://llama.developer.meta.com/join_waitlist, https://llama.developer.meta.com/join_waitlist  Web search found no official API docs."
    },
    "3-1 Pre-training": {
      "score": 0.0,
      "reason": "No concrete method details in quotes."
    },
    "3-2 Fine-tuning": {
      "score": 0.0,
      "reason": "No concrete method details in quotes."
    },
    "3-3 Reinforcement Learning": {
      "score": 0.5,
      "reason": "Partial methodology disclosed (techniques/algorithms mentioned) but not fully reproducible."
    },
    "4-1 Pre-training Data": {
      "score": 0.5,
      "reason": "High-level description (15 T multilingual, public web/code, Dec-2023 cutoff) is given, but not a source-by-source listing to rebuild the corpus."
    },
    "4-2 Fine-tuning Data": {
      "score": 0.5,
      "reason": "Lists human prompts, synthetic data, 25 M example size, etc., yet datasets are not fully released or enumerated."
    },
    "4-3 Reinforcement Learning Data": {
      "score": 0.5,
      "reason": "States millions of human preference judgements and multi-turn comparisons, but no detailed dataset release."
    },
    "4-4 Data Filtering": {
      "score": 1.0,
      "reason": "Filtering pipeline disclosed (multiple categories + thresholds/ratios and/or explicit stages or tools)."
    }
  },
  "final_score_10pt": 6.25,
  "meta": {
    "usage_from_dispatch": {
      "fine_tuning": "used",
      "rl": "used"
    },
    "excluded": [],
    "denominator": 16,
    "raw_sum": 10.0,
    "scale": "10/16",
    "code_detection_reason": "No training pipeline files; README mentions are ignored.",
    "pretrain_sources_used": false
  }
}