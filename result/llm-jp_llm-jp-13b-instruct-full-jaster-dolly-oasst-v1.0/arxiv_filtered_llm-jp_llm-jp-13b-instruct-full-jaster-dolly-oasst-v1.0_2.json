{
  "1-5 (Architecture)": "The material that directly addresses the internal structure of llm-jp/llm-jp-13b-instruct-full-jaster-dolly-oasst-v1.0 tells us several concrete things. First, the authors repeatedly emphasize parameter count: “Each model suite provides an LLM with 13B parameters along with its fine-tuned variants.”  The same 13 B-parameter figure is restated when they explain their very first release: “Our first model suite, which we call the LLM-jp model suite v1.0 … [and] Each model suite provides an LLM with 13B parameters along with its fine-tuned variants.”  Thus, for v1.0 the core network size is fixed at 13 billion trainable parameters and the suite additionally contains derivative checkpoints produced by instruction-, alignment- or task-specific fine-tuning.\n\nWith respect to the backbone design, the authors are explicit: “The pre-trained model v1.0 uses a model architecture based on GPT-2 [42].”  That sentence appears twice in the source quotes, underscoring that v1.0 keeps the GPT-2‐style Transformer layout—multi-head self-attention blocks stacked in sequence—while scaling it to the 13 B parameter regime.  No alternative architecture is mentioned for v1.0, and no mix with later versions is permitted by the filter, so GPT-2 compatibility is the sole confirmed architectural characteristic.\n\nChronologically, the development path is also part of the description.  The “initial milestone was to develop the model suite v1.0,” and the Corpus Building working group prepared data specifically “to train the pre-trained model v1.0, the LLM with 13B parameters within this suite.”  Because the data preparation was done expressly for that 13 B GPT-2-style network, one can infer that the architectural choices, capacity, and tokenizer size were all co-designed around the same corpus and release plan, leading up to publication on 20 Oct 2023.\n\nIn summary, every architecture-level fact that survives the strict token filter converges on three points: (1) the target system is fixed at exactly 13 billion parameters, (2) it inherits the layer-wise organization of GPT-2, and (3) it ships inside a v1.0 model suite that also offers multiple fine-tuned offspring built on the same backbone.",
  "1-6 (Tokenizer)": "All tokenizer information that explicitly contains the required v1.0 tokens indicates that the project employs a SentencePiece-based scheme in unigram mode.  The authors write: “We developed tokenizers based on SentencePiece with the unigram mode.”  Historically, they first explored a pre-existing multilingual tokenizer from the Fugaku supercomputer project, which they call “the tokenizer v1.0.”  Two separate quotes report this exploration: “As a multi-lingual tokenizer considering Japanese, we first explored the tokenizer developed in the project … which we refer to as the tokenizer v1.0.”\n\nBuilding on that foundation, the team produced a new revision expressly for the model suite under discussion: “Therefore, based on the idea of the tokenizer v1.0, we constructed the tokenizer v2.1 for use in the model suite v1.0 by using a subset of the corpus v1 and extending the target languages to Japanese, English, and code.”  The detail that v2.1 is the tokenizer actually used with the model suite v1.0 is important: although the naming might seem to imply a mismatch, the quotes state unambiguously that tokenizer v2.1 was paired with the v1.0 LLM release.\n\nFunctionally, the tokenizer is multilingual—Japanese, English, and programming code are all covered—and it was trained on a subset of the corpus v1 so that its vocabulary aligns with the data that the 13 B GPT-2-style model processed.  No information about vocabulary size, downloadable files, or special tokens is included in the filtered quotes, but we can definitively say that (i) it is SentencePiece unigram, (ii) it descends from the Fugaku tokenizer design, and (iii) the specific revision tagged v2.1 is the one bundled with llm-jp-13b-…-v1.0.",
  "2-1 (Hardware)": "Every hardware detail that survives the strict model filter concerns the mdx computing cluster.  The report notes that as early as June 2023, the team “Started to build 13B LLMs on mdx (a platform for data-empowered society).”  Concretely, a benchmark using that platform states: “Using a 12 node mdx computing cluster to train a model with 13B parameters, the measured processing speed was 170K tokens/second on average.”  The same sentence appears twice in the source material, so it is clearly the principal quantitative metric disclosed.\n\nThus, for llm-jp-13b-instruct-full-jaster-dolly-oasst-v1.0 we know the training job ran (at least in part) on mdx, scaled across 12 compute nodes, and achieved an average throughput of roughly 170 kilo-tokens per second while processing the 13 B parameter network.  No GPU model, CPU type, memory size, interconnect specification, or total FLOPs figure is provided in the filtered text, so the node count and aggregate token-per-second rate constitute the entirety of the confirmed hardware profile.",
  "2-2 (Software)": "The software stack that was considered and ultimately used for building the v1.0 release is recorded in three separate quotes.  At project kickoff in May 2023, the team surveyed available large-model training toolchains: “At the start of the project in May 2023, there were several tools available for pre-training language models with over 10B parameters, so we decided to use them. Specifically, we considered Megatron-DeepSpeed, GPT-NeoX, and LLM Foundry as candidates.”  That sentence establishes the short-list of frameworks capable of handling 10 B+ scale that the authors evaluated.\n\nFor the actual v1.0 pre-training run, the decision landed on Megatron-DeepSpeed: “We used Megatron-LM21 for all experiments in this section instead of Megatron-Deepspeed used for building pre-trained model v1.0.”  The wording “instead of” confirms that Megatron-Deepspeed powered the original v1.0 training, whereas Megatron-LM21 was adopted only for later experimental comparisons.  Because no other library or framework is named in connection with the v1.0 build, Megatron-Deepspeed is the sole verified training orchestrator for the target model.\n\nNo explicit mention of lower-level acceleration kernels (FlashAttention, Apex, etc.), data-loading utilities, optimizer selections, or scheduler hyperparameters passes the strict filter, so they cannot be stated here.  What is firmly established is the chronology (decision process in May 2023), the set of candidate frameworks (Megatron-DeepSpeed, GPT-NeoX, LLM Foundry), and the final choice for v1.0 (Megatron-DeepSpeed, later compared against Megatron-LM21 in follow-up experiments).",
  "1-5 (Architecture)__evidence": [
    {
      "source": "[pdf_text]",
      "quote": "Each model suite provides an LLM with 13B parameters along with its fine-tuned variants."
    },
    {
      "source": "[pdf_text]",
      "quote": "The pre-trained model v1.0 uses a model architecture based on GPT-2 [42]."
    },
    {
      "source": "[sections/2407.03963]",
      "quote": "Our first model suite, which we call the LLM-jp model suite v1.0, was released on October 20th, 2023. Subsequently, we released the next model suite, called the LLM-jp model suite v2.0, on April 30th, 2024. Each model suite provides an LLM with 13B parameters along with its fine-tuned variants."
    },
    {
      "source": "[sections/2407.03963]",
      "quote": "Our initial milestone was to develop the model suite v1.0, and the Corpus Building WG worked on preparing a pre-training corpus to train the pre-trained model v1.0, the LLM with 13B parameters within this suite."
    },
    {
      "source": "[sections/Model Building WG]",
      "quote": "The pre-trained model v1.0 uses a model architecture based on GPT-2 [42]."
    }
  ],
  "1-6 (Tokenizer)__evidence": [
    {
      "source": "[pdf_text]",
      "quote": "We developed tokenizers based on SentencePiece with the unigram mode [30]. As a multi-lingual tokenizer considering Japanese, we first explored the tokenizer developed in the project “Development of a distributed parallel learning method for large-scale language models in the policy-oriented framework of the supercomputer Fugaku”, which we refer to as the tokenizer v1.0."
    },
    {
      "source": "[pdf_text]",
      "quote": "Therefore, based on the idea of the tokenizer v1.0, we constructed the tokenizer v2.1 for use in the model suite v1.0 by using a subset of the corpus v1 and extending the target languages to Japanese, English, and code."
    },
    {
      "source": "[sections/2407.03963]",
      "quote": "As a multi-lingual tokenizer considering Japanese, we first explored the tokenizer developed in the project “Development of a distributed parallel learning method for large-scale language models in the policy-oriented framework of the supercomputer Fugaku”, which we refer to as the tokenizer v1.0."
    },
    {
      "source": "[sections/2407.03963]",
      "quote": "Therefore, based on the idea of the tokenizer v1.0, we constructed the tokenizer v2.1 for use in the model suite v1.0 by using a subset of the corpus v1 and extending the target languages to Japanese, English, and code."
    }
  ],
  "2-1 (Hardware)__evidence": [
    {
      "source": "[pdf_text]",
      "quote": "Using a 12 node mdx computing cluster to train a model with 13B parameters, the measured processing speed was 170K tokens/second on average."
    },
    {
      "source": "[sections/2407.03963]",
      "quote": "2023.6\nStarted to build 13B LLMs on mdx (a platform for data-empowered society)"
    },
    {
      "source": "[sections/Model Building WG]",
      "quote": "Using a 12 node mdx computing cluster to train a model with 13B parameters, the measured processing speed was 170K tokens/second on average."
    }
  ],
  "2-2 (Software)__evidence": [
    {
      "source": "[pdf_text]",
      "quote": "At the start of the project in May 2023, there were several tools available for pre-training language models with over 10B parameters, so we decided to use them. Specifically, we considered Megatron-DeepSpeed, GPT-NeoX, and LLM Foundry as candidates."
    },
    {
      "source": "[pdf_text]",
      "quote": "We used Megatron-LM21 for all experiments in this section instead of Megatron-Deepspeed used for building pre-trained model v1.0."
    },
    {
      "source": "[sections/Model Building WG]",
      "quote": "We used Megatron-LM21 for all experiments in this section instead of Megatron-Deepspeed used for building pre-trained model v1.0."
    }
  ]
}