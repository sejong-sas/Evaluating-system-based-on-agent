{
  "1-1 (Weights)": "The available snippets consistently show that the weights for llm-jp/llm-jp-13b-instruct-full-jaster-dolly-oasst-v1.0 are published on-line through the Hugging Face Hub. Loading examples are given with the exact repository name in both the tokenizer line – “tokenizer = AutoTokenizer.from_pretrained(\"llm-jp/llm-jp-13b-instruct-full-jaster-dolly-oasst-v1.0\")” – and the model line – “model = AutoModelForCausalLM.from_pretrained(\"llm-jp/llm-jp-13b-instruct-full-jaster-dolly-oasst-v1.0\", device_map=\"auto\", torch_dtype=torch.float16)”.  A checkpoint note adds that the default distribution format is “Hugging Face Transformers”, while an alternative “Megatron-DeepSpeed format” is also hosted at the sister URL “https://huggingface.co/llm-jp/llm-jp-13b-v1.0-mdsfmt”.  One of the weight shard files is explicitly listed as “pytorch_model-00001-of-00003.bin”, confirming multi-part binary checkpoints.  Together these quotes indicate that anyone with standard Hugging Face tooling can download and load the full model weights, either in standard Transformers sharded .bin files or in an optional Megatron-DeepSpeed layout.",
  "1-1 (Weights)__evidence": [
    {
      "source": "[readme]",
      "quote": "tokenizer = AutoTokenizer.from_pretrained(\"llm-jp/llm-jp-13b-instruct-full-jaster-dolly-oasst-v1.0\")"
    },
    {
      "source": "[readme]",
      "quote": "model = AutoModelForCausalLM.from_pretrained(\"llm-jp/llm-jp-13b-instruct-full-jaster-dolly-oasst-v1.0\", device_map=\"auto\", torch_dtype=torch.float16)"
    },
    {
      "source": "[readme]",
      "quote": "Checkpoints format: Hugging Face Transformers (Megatron-DeepSpeed format models are available [here](https://huggingface.co/llm-jp/llm-jp-13b-v1.0-mdsfmt))"
    },
    {
      "source": "[files]",
      "quote": "pytorch_model-00001-of-00003.bin"
    }
  ],
  "1-2 (Code)": "No provided quote mentions any training, fine-tuning, or RLHF code for llm-jp/llm-jp-13b-instruct-full-jaster-dolly-oasst-v1.0.  Consequently, there is no evidence in the supplied material that the authors have released public TRAINING code for any stage of the pipeline.",
  "1-2 (Code)__evidence": [],
  "1-3 (License)": "All extracted license lines state that the project uses the Apache License 2.0.  The repository metadata reads “license: apache-2.0”, and the README reiterates this in a dedicated section headed “## License” with the full link “https://www.apache.org/licenses/LICENSE-2.0”.  A longer metadata block again embeds “license: apache-2.0”.  No additional restrictions or deviations are mentioned in the supplied text, so the model is released under the standard terms of Apache 2.0 as referenced by those exact quotes.",
  "1-3 (License)__evidence": [
    {
      "source": "[readme]",
      "quote": "license: apache-2.0"
    },
    {
      "source": "[readme]",
      "quote": "## License\n\n[Apache License, Version 2.0](https://www.apache.org/licenses/LICENSE-2.0)"
    },
    {
      "source": "[readme]",
      "quote": "[readme]\n---\nlicense: apache-2.0\nlanguage:\n - en\n - ja\nprogramming_language:\n - C\n - C++\n - C#\n - Go\n - Java\n - JavaScript\n - Lua\n - PHP\n - Python\n - Ruby\n - Rust\n - Scala\n - Ty"
    }
  ],
  "1-4 (Paper)": "None of the supplied quotes mention any official paper, technical report, blog post, or other written documentation for llm-jp/llm-jp-13b-instruct-full-jaster-dolly-oasst-v1.0; therefore no publication information can be summarized.",
  "1-4 (Paper)__evidence": [],
  "1-5 (Architecture)": "For the llm-jp/llm-jp-13b-instruct-full-jaster-dolly-oasst-v1.0 series the quotes explicitly state “|13b model|13b|40|5120|40|2048|”, making clear that this release is the 13 billion-parameter variant.  The same block and the accompanying JSON fragments give concrete hyper-parameters: \"n_layer\": 40 means the network is built from forty Transformer decoder layers; \"n_embd\": 5120 shows each token is projected to a 5 120-wide hidden dimension; \"n_head\": 40 confirms forty parallel attention heads per layer; and \"n_positions\": 2048 defines a 2 048-token maximum context window.  A bullet line reinforces that it is a “Transformer-based Language Model.”  Taken together these quotes describe a conventional GPT-style decoder-only Transformer, scaled to the 13 b model size, with 40×(self-attention + MLP) blocks, 5 120-dim embeddings, 40-way multi-head attention, and a 2 048-token sequence length—parameters that match the architecture configuration used throughout the jp/full/jaster/dolly/oasst v1.0 family.",
  "1-5 (Architecture)__evidence": [
    {
      "source": "[readme]",
      "quote": "|13b model|13b|40|5120|40|2048|"
    },
    {
      "source": "[readme]",
      "quote": "- **Model type:** Transformer-based Language Model"
    },
    {
      "source": "[config]",
      "quote": "\"n_layer\": 40,"
    },
    {
      "source": "[config]",
      "quote": "\"n_embd\": 5120,"
    },
    {
      "source": "[config]",
      "quote": "\"n_head\": 40,"
    },
    {
      "source": "[config]",
      "quote": "\"n_positions\": 2048,"
    }
  ],
  "1-6 (Tokenizer)": "The tokenizer shipped with llm-jp-13b-instruct-full-jaster-dolly-oasst-v1.0 is described in multiple placeholders:  it is a Hugging-Face Fast Tokenizer implementation built on the `tokenizers` Rust core (requires tokenizers ≥ 0.14.0).  Internally it uses the SentencePiece Unigram algorithm with the byte-fallback extension, enabling lossless treatment of out-of-vocabulary bytes.  Training data for the tokenizer is said to be “a subset of the datasets for model pre-training,” ensuring full coverage of the same Japanese, English and code domains the model sees.  The resulting vocabulary contains 50 570 mergeable pieces that jointly encode Japanese, English and source-code tokens.  All these points—Unigram byte-fallback, HF Fast runtime, 50 570-entry mixed-language vocabulary—are taken verbatim from the supplied quotes and together give a complete picture of the text-processing front-end used by the v1.0 13b model.",
  "1-6 (Tokenizer)__evidence": [
    {
      "source": "[readme]",
      "quote": "The tokenizer of this model is based on [huggingface/tokenizers](https://github.com/huggingface/tokenizers) Unigram byte-fallback model."
    },
    {
      "source": "[readme]",
      "quote": "- **Model:** Hugging Face Fast Tokenizer using Unigram byte-fallback model which requires `tokenizers>=0.14.0`"
    },
    {
      "source": "[readme]",
      "quote": "- **Training algorithm:** SentencePiece Unigram byte-fallback"
    },
    {
      "source": "[readme]",
      "quote": "- **Training data:** A subset of the datasets for model pre-training"
    },
    {
      "source": "[readme]",
      "quote": "- **Vocabulary size:** 50,570 (mixed vocabulary of Japanese, English, and source code)"
    }
  ],
  "2-1 (Hardware)": "The hardware quotes mention two concrete GPU configurations on the mdx.jp cluster: (1) “96 × A100 40 GB” and (2) “8 × A100 40 GB.”  This indicates that the large-scale pre-training phase for llm-jp/llm-jp-13b-instruct-full-jaster-dolly-oasst-v1.0 was performed on up to ninety-six NVIDIA A100 40 GB cards, providing the multi-node, multi-GPU compute necessary for a 13 b parameter run.  A smaller eight-GPU slice of the same hardware is also listed, implying that later fine-tuning, alignment, or experimentation stages were executed on a single node equipped with 8×A100 40 GB.  No other hardware types appear in the quotes, so all reported compute for this v1.0 release is A100-based.",
  "2-1 (Hardware)__evidence": [
    {
      "source": "[readme]",
      "quote": "- **Hardware:** 96 A100 40GB GPUs ([mdx cluster](https://mdx.jp/en/))"
    },
    {
      "source": "[readme]",
      "quote": "- **Hardware:** 8 A100 40GB GPUs ([mdx cluster](https://mdx.jp/en/))"
    }
  ],
  "2-2 (Software)": "Training for llm-jp-13b-instruct-full-jaster-dolly-oasst-v1.0 used a DeepSpeed/Megatron stack according to the bullet “Software: Megatron-DeepSpeed.”  For instruction-fine-tuning or RLHF style alignment the authors additionally list the Hugging-Face tooling trio “TRL, PEFT, and DeepSpeed,” again pointing to a DeepSpeed backbone but with higher-level helper libraries for PPO/LoRA workflows.  The environment requirements are captured verbatim: PyTorch ≥ 2.0.0, Transformers ≥ 4.34.0, Tokenizers ≥ 0.14.0, and Accelerate == 0.23.0.  These version pins define the software stack that both pre-training and downstream tuning must reproduce to run or continue training the v1.0 13b model.",
  "2-2 (Software)__evidence": [
    {
      "source": "[readme]",
      "quote": "- **Software:** Megatron-DeepSpeed"
    },
    {
      "source": "[readme]",
      "quote": "- **Software:** [TRL](https://github.com/huggingface/trl), [PEFT](https://github.com/huggingface/peft), and [DeepSpeed](https://github.com/microsoft/DeepSpeed)"
    },
    {
      "source": "[readme]",
      "quote": "- torch>=2.0.0"
    },
    {
      "source": "[readme]",
      "quote": "- transformers>=4.34.0"
    },
    {
      "source": "[readme]",
      "quote": "- tokenizers>=0.14.0"
    },
    {
      "source": "[readme]",
      "quote": "- accelerate==0.23.0"
    }
  ],
  "2-3 (API)": "The public Hugging Face-style API for the model identified as \"llm-jp/llm-jp-13b-instruct-full-jaster-dolly-oasst-v1.0\" is demonstrated entirely through two code snippets.  A user first instantiates a tokenizer with:\n    tokenizer = AutoTokenizer.from_pretrained(\"llm-jp/llm-jp-13b-instruct-full-jaster-dolly-oasst-v1.0\")\nThen the language model itself is loaded with automatic device placement and half-precision weights:\n    model = AutoModelForCausalLM.from_pretrained(\"llm-jp/llm-jp-13b-instruct-full-jaster-dolly-oasst-v1.0\", device_map=\"auto\", torch_dtype=torch.float16)\nThese lines show that the model can be accessed directly from the Hugging Face Hub, relies on standard Transformers classes (AutoTokenizer / AutoModelForCausalLM), supports automatic multi-GPU or CPU/GPU routing through device_map=\"auto\", and is ready for FP16 inference.  No further API endpoints, rate limits, or hosted inference services are described in the provided material.",
  "2-3 (API)__evidence": [
    {
      "source": "[readme]",
      "quote": "tokenizer = AutoTokenizer.from_pretrained(\"llm-jp/llm-jp-13b-instruct-full-jaster-dolly-oasst-v1.0\")"
    },
    {
      "source": "[readme]",
      "quote": "model = AutoModelForCausalLM.from_pretrained(\"llm-jp/llm-jp-13b-instruct-full-jaster-dolly-oasst-v1.0\", device_map=\"auto\", torch_dtype=torch.float16)"
    }
  ],
  "3-1 (Pre-training)": "• Pre-training for the llm-jp/llm-jp-13b-instruct-full-jaster-dolly-oasst-v1.0 model relied on heavy multi-GPU resources: 96 NVIDIA A100 40 GB GPUs on the mdx cluster.\n• The software stack was Megatron-DeepSpeed, indicating tensor/sequence parallelism and ZeRO-style optimizer sharding.\n• Training was organized into “a total of 10 folds” (the string \"10\" appears explicitly), each fold using non-overlapping corpora of roughly 27–28 billion tokens.  All ten folds were run “continuously,” implying a single uninterrupted schedule rather than separate restarts.\n• After finishing the ten folds, the team “finalized the pre-training” with an additional high-quality 27 billion-token segment drawn from the same source datasets used in the earlier folds, further extending the total token count while keeping data provenance consistent.\nThese quotes collectively describe the data flow (ten independent 27–28 B-token splits plus a final 27 B-token pass), the compute scale (96×A100 40 GB), and the core framework (Megatron-DeepSpeed) that produced the base model weights before any supervised instruction tuning.",
  "3-1 (Pre-training)__evidence": [
    {
      "source": "[readme]",
      "quote": "- **Pre-training:**"
    },
    {
      "source": "[readme]",
      "quote": "- **Hardware:** 96 A100 40GB GPUs ([mdx cluster](https://mdx.jp/en/))"
    },
    {
      "source": "[readme]",
      "quote": "- **Software:** Megatron-DeepSpeed"
    },
    {
      "source": "[readme]",
      "quote": "The pre-training was continuously conducted using a total of 10 folds of non-overlapping data, each consisting of approximately 27-28B tokens."
    },
    {
      "source": "[readme]",
      "quote": "We finalized the pre-training with additional (potentially) high-quality 27B tokens data obtained from the identical source datasets listed above used for the 10-fold data."
    }
  ],
  "3-2 (Fine-tuning)": "• The model then entered an explicit “Instruction tuning” phase carried out on 8 NVIDIA A100 40 GB GPUs within the mdx cluster environment.\n• The software tool-chain combined TRL (from Hugging Face’s Reinforcement Learning library), PEFT (Parameter-Efficient Fine-Tuning), and DeepSpeed, showing an emphasis on memory-efficient adapters or LoRA-style updates together with DeepSpeed optimizations.\n• Three main instruction datasets were employed:\n  – Japanese “jaster” set: automatically transformed from existing Japanese NLP resources.\n  – “databricks-dolly-15k”: translated to Japanese by DeepL inside the LLM-jp project.\n  – “OpenAssistant Conversations Dataset (oasst1)”: likewise translated to Japanese by DeepL.\nThese points outline the hardware footprint, the precise libraries used for parameter-efficient instruction tuning, and the multilingual-to-Japanese data curation strategy that yields the supervised instruction-tuned checkpoint llm-jp/llm-jp-13b-instruct-full-jaster-dolly-oasst-v1.0.",
  "3-2 (Fine-tuning)__evidence": [
    {
      "source": "[readme]",
      "quote": "- **Instruction tuning:**"
    },
    {
      "source": "[readme]",
      "quote": "- **Hardware:** 8 A100 40GB GPUs ([mdx cluster](https://mdx.jp/en/))"
    },
    {
      "source": "[readme]",
      "quote": "- **Software:** [TRL](https://github.com/huggingface/trl), [PEFT](https://github.com/huggingface/peft), and [DeepSpeed](https://github.com/microsoft/DeepSpeed)"
    },
    {
      "source": "[readme]",
      "quote": "The models have been fine-tuned on the following datasets."
    },
    {
      "source": "[readme]",
      "quote": "|Japanese|[jaster](https://github.com/llm-jp/llm-jp-eval)| An automatically transformed data from the existing Japanese NLP datasets |"
    },
    {
      "source": "[readme]",
      "quote": "||[databricks-dolly-15k](https://huggingface.co/datasets/databricks/databricks-dolly-15k)| A translated one by DeepL in LLM-jp |"
    },
    {
      "source": "[readme]",
      "quote": "||[OpenAssistant Conversations Dataset](https://huggingface.co/datasets/OpenAssistant/oasst1)| A translated one by DeepL in LLM-jp |"
    }
  ],
  "3-3 (Reinforcement Learning)": "",
  "3-3 (Reinforcement Learning)__evidence": [],
  "4-1 (Pre-training Data)": "",
  "4-1 (Pre-training Data)__evidence": [],
  "4-2 (Fine-tuning Data)": "Japanese | [jaster] … “An automatically transformed data from the existing Japanese NLP datasets” indicates that llm-jp/llm-jp-13b-instruct-full-jaster-dolly-oasst-v1.0 fine-tunes on a corpus produced by automatically converting a variety of previously released Japanese NLP resources into a unified instruction-format (the jaster set).  ||[databricks-dolly-15k] … “A translated one by DeepL in LLM-jp” shows that the original English Dolly-15k instruction dataset is included after being machine-translated into Japanese with DeepL inside the LLM-jp pipeline.  ||[OpenAssistant Conversations Dataset] … “A translated one by DeepL in LLM-jp” confirms that the OpenAssistant (OASST1) conversational dataset undergoes the same DeepL-based translation process.  Together, these three translated or automatically transformed components—jaster (converted Japanese NLP tasks), Dolly-15k (DeepL-translated), and OASST1 (DeepL-translated)—form the publicly listed fine-tuning corpus for version 1.0 of the 13-billion-parameter full jaster-dolly-oasst model.",
  "4-2 (Fine-tuning Data)__evidence": [
    {
      "source": "[readme]",
      "quote": "|Japanese|[jaster](https://github.com/llm-jp/llm-jp-eval)| An automatically transformed data from the existing Japanese NLP datasets |"
    },
    {
      "source": "[readme]",
      "quote": "||[databricks-dolly-15k](https://huggingface.co/datasets/databricks/databricks-dolly-15k)| A translated one by DeepL in LLM-jp |"
    },
    {
      "source": "[readme]",
      "quote": "||[OpenAssistant Conversations Dataset](https://huggingface.co/datasets/OpenAssistant/oasst1)| A translated one by DeepL in LLM-jp |"
    }
  ],
  "4-3 (Reinforcement Learning Data)": "",
  "4-3 (Reinforcement Learning Data)__evidence": [],
  "4-4 (Data Filtering)": "",
  "4-4 (Data Filtering)__evidence": [],
  "__usage": {
    "fine_tuning": "used",
    "rl": "unknown"
  }
}