{
  "4-1 (Pre-training Data)": "The supplied material contains no statements that describe the sources, types, volumes, licensing status, or any other aspect of the pre-training data used for LG-AI-EXAONE/EXAONE-Deep. Consequently, no information can be summarized for this item on the basis of the provided quotes alone.",
  "4-1 (Pre-training Data)__evidence": [],
  "4-2 (Fine-tuning Data)": "The provided quotes do not mention any datasets, sampling strategies, public releases, or curation details that would shed light on the fine-tuning data employed for LG-AI-EXAONE/EXAONE-Deep. Therefore, no summary is possible from the current evidence.",
  "4-2 (Fine-tuning Data)__evidence": [],
  "4-3 (Reinforcement Learning Data)": "None of the supplied text addresses the composition, generation, or accessibility of reinforcement-learning (e.g., RLHF or RLAIF) datasets for the target model. As a result, there is no information to summarize for this section.",
  "4-3 (Reinforcement Learning Data)__evidence": [],
  "4-4 (Data Filtering)": "The only explicit statement provided on filtering or safety work is: “LG AI Research strives to reduce potential risks that may arise from EXAONE language models.”  From this single line, we can infer that LG AI Research has acknowledged the existence of potential harms associated with EXAONE-Deep and that some form of risk-reduction or data-filtering practice is in place. However, the quote offers no concrete details—no numeric thresholds, heuristics, classifier names, or pipeline-stage descriptions. Thus, while the organization affirms its intent to mitigate risk, the filtering methodology, its criteria, and its quantitative impact remain undisclosed in the material supplied.",
  "4-4 (Data Filtering)__evidence": [
    {
      "source": "[readme]",
      "quote": "LG AI Research strives to reduce potential risks that may arise from EXAONE language models."
    }
  ]
}