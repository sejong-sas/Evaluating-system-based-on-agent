{
  "1-1 (Weights)": "The available material repeatedly emphasizes that weight files for the LG-AI-EXAONE/EXAONE-Deep family are openly posted in several pre-quantized, ready-to-download formats. One quote states, \"We introduce a series of quantized weights of EXAONE Deep models,\" establishing that quantized checkpoints, not merely raw FP32 originals, are being distributed. More concretely, multiple sentences highlight AWQ compression: \"We provide AWQ-quantized weights of EXAONE Deep models, quantized using `AutoAWQ` library,\" and users are told to \"install the latest version of AutoAWQ library (`autoawq>=0.2.8`) to load the AWQ-quantized version of EXAONE Deep models.\" The official hosting locations are enumerated: a Hugging Face collection link – \"[EXAONE Deep collection](https://huggingface.co/collections/LGAI-EXAONE/exaone-deep-67d119918816ec6efa79a4aa)\" – serves the files directly, while an example CLI pull command is supplied: \"huggingface-cli download LGAI-EXAONE/EXAONE-Deep-7.8B-GGUF \\\", implying anonymous or at least friction-free retrieval. In addition, \"EXAONE Deep models are uploaded to Ollama model library,\" signalling a second distribution channel. Taken together, the quotes confirm that (a) weights exist in several AWQ-compressed variants, (b) they are hosted publicly on Hugging Face and Ollama, and (c) acquisition requires nothing more than the standard Hugging Face CLI plus AutoAWQ ≥ 0.2.8 for inference, although the text does not explicitly state whether original full-precision checkpoints are also available or whether any gating or request form is needed beyond the illustrated download command.",
  "1-1 (Weights)__evidence": [
    {
      "source": "[readme]",
      "quote": "We introduce a series of quantized weights of EXAONE Deep models."
    },
    {
      "source": "[readme]",
      "quote": "We provide AWQ-quantized weights of EXAONE Deep models, quantized using `AutoAWQ` library."
    },
    {
      "source": "[readme]",
      "quote": "Please refer to the [EXAONE Deep collection](https://huggingface.co/collections/LGAI-EXAONE/exaone-deep-67d119918816ec6efa79a4aa) for pre-quantized weights, and the [AutoAWQ documentation](https://github.com/casper-hansen/AutoAWQ) for more details."
    },
    {
      "source": "[readme]",
      "quote": "You need to install the latest version of AutoAWQ library (`autoawq>=0.2.8`) to load the AWQ-quantized version of EXAONE Deep models."
    },
    {
      "source": "[readme]",
      "quote": "huggingface-cli download LGAI-EXAONE/EXAONE-Deep-7.8B-GGUF \\"
    },
    {
      "source": "[readme]",
      "quote": "EXAONE Deep models are uploaded to Ollama model library."
    }
  ],
  "1-2 (Code)": "None of the supplied quotes mention source code, training scripts, data-preparation pipelines, configuration files, or even inference examples. Consequently, the excerpts give no evidence that training, fine-tuning, or evaluation code for LG-AI-EXAONE/EXAONE-Deep has been released, nor do they identify a repository, license, or access procedure for such code.",
  "1-2 (Code)__evidence": [],
  "1-3 (License)": "Licensing is governed by the \"EXAONE AI Model License Agreement 1.1 - NC,\" cited verbatim in two separate quotes. The text provides both the headline and key clauses. First, \"2.1 Grant of License\" furnishes a \"limited, non-exclusive, non-transferable, worldwide, and revocable license,\" indicating users may operate the model but within tight bounds. Second, the agreement is explicitly non-commercial: \"3.1 Commercial Use: The Licensee is expressly prohibited from using the Model, Derivatives, or Output for any commercial purposes, including but not limited to, developing or deploying products, services, or applications that generate revenue, whether directly or indirectly.\" Because the license is tagged \"NC,\" the non-commercial limitation is central. No additional clauses on modification, redistribution, or derivative works are quoted, so the excerpts confirm (a) permission for use is restricted and revocable, (b) commercial exploitation is disallowed, and (c) the agreement applies worldwide but is non-transferable.",
  "1-3 (License)__evidence": [
    {
      "source": "[readme]",
      "quote": "The model is licensed under [EXAONE AI Model License Agreement 1.1 - NC](./LICENSE)"
    },
    {
      "source": "[license_files]",
      "quote": "EXAONE AI Model License Agreement 1.1 - NC"
    },
    {
      "source": "[license_files]",
      "quote": "2.1 Grant of License: Subject to the terms and conditions outlined in this Agreement, the Licensor hereby grants the Licensee a limited, non-exclusive, non-transferable, worldwide, and revocable license to:"
    },
    {
      "source": "[license_files]",
      "quote": "3.1 Commercial Use: The Licensee is expressly prohibited from using the Model, Derivatives, or Output for any commercial purposes, including but not limited to, developing or deploying products, services, or applications that generate revenue, whether directly or indirectly."
    }
  ],
  "1-4 (Paper)": "The only publication-related evidence is a single BibTeX stub, \"@article{exaone-deep,\" which signals that a formal article on EXAONE-Deep exists or is forthcoming. However, no title, authors, venue, date, or link accompanies the placeholder, and no blog posts or technical reports are referenced. Therefore, the quotes offer no substantive bibliographic or URL data beyond asserting that an article entry for EXAONE-Deep has been reserved.",
  "1-4 (Paper)__evidence": [
    {
      "source": "[readme]",
      "quote": "@article{exaone-deep,"
    }
  ],
  "1-5 (Architecture)": "The available quotes describe LG AI Research’s EXAONE Deep as a family of models that is released in three discrete parameter scales—2.4 billion, 7.8 billion, and 32 billion parameters. According to the first quote, “We introduce EXAONE Deep, which exhibits superior capabilities in various reasoning tasks including math and coding benchmarks, ranging from 2.4B to 32B parameters developed and released by LG AI Research.” This explicitly tells us that the architecture is deployed in multiple sizes that span an order of magnitude. The second quote elaborates on how each of these model sizes performs relative to other systems: “Evaluation results show that 1) EXAONE Deep 2.4B outperforms other models of comparable size, 2) EXAONE Deep 7.8B outperforms not only open-weight models of comparable scale but also a proprietary reasoning model OpenAI o1-mini, and 3) EXAONE Deep 32B demonstrates competitive performance against leading open-weight models.” Taken together, the quotes indicate a tiered architectural design philosophy where each successive parameter budget aims to deliver incrementally higher competency in reasoning, math, and coding tasks. While no layer counts, hidden sizes, or attention-head configurations are disclosed, the textual evidence makes clear that the architecture is intentionally scalable (2.4 B → 7.8 B → 32 B) and optimized for competitive reasoning benchmarks across all three sizes.",
  "1-5 (Architecture)__evidence": [
    {
      "source": "[readme]",
      "quote": "We introduce EXAONE Deep, which exhibits superior capabilities in various reasoning tasks including math and coding benchmarks, ranging from 2.4B to 32B parameters developed and released by LG AI Research."
    },
    {
      "source": "[readme]",
      "quote": "Evaluation results show that 1) EXAONE Deep 2.4B outperforms other models of comparable size, 2) EXAONE Deep 7.8B outperforms not only open-weight models of comparable scale but also a proprietary reasoning model OpenAI o1-mini, and 3) EXAONE Deep 32B demonstrates competitive performance against leading open-weight models."
    }
  ],
  "1-6 (Tokenizer)": "",
  "1-6 (Tokenizer)__evidence": [],
  "2-1 (Hardware)": "",
  "2-1 (Hardware)__evidence": [],
  "2-2 (Software)": "",
  "2-2 (Software)__evidence": [],
  "2-3 (API)": "The quotes collectively document several concrete, command-line examples that show how the LG-AI-EXAONE/EXAONE-Deep model can be exposed through different serving frameworks, implying the existence of practical, user-accessible APIs. One snippet shows a vLLM invocation – \"vllm serve LGAI-EXAONE/EXAONE-Deep-7.8B\" – indicating that a vLLM HTTP (or WebSocket) server can be launched directly from the terminal with the model identifier \"LGAI-EXAONE/EXAONE-Deep-7.8B\". A second snippet, the JSON fragment \"\\\"model\\\": \\\"LGAI-EXAONE/EXAONE-Deep-7.8B\\\",\" reinforces that the same identifier is used when sending a request to such an endpoint. A third line, \"python -m sglang.launch_server --model-path LGAI-EXAONE/EXAONE-Deep-7.8B \\\", shows that the model can also be hosted through sglang’s built-in launch_server utility, again by pointing to the identical model path. Finally, the command \"ollama run exaone-deep:7.8b\" confirms that the model is packaged for the Ollama ecosystem, where a single command spins up an interactive or programmatic service. Taken together, these four examples demonstrate that the model is intended to be served via at least three distinct back-end stacks (vLLM, sglang, and Ollama), each of which would expose a network API or CLI interface for downstream applications. The consistency of the model string across all commands implies that the same pretrained weights are referenced no matter which server implementation is chosen, and that end-users can swap between hosting solutions without changing the logical model name.",
  "2-3 (API)__evidence": [
    {
      "source": "[readme]",
      "quote": "vllm serve LGAI-EXAONE/EXAONE-Deep-7.8B"
    },
    {
      "source": "[readme]",
      "quote": "\"model\": \"LGAI-EXAONE/EXAONE-Deep-7.8B\","
    },
    {
      "source": "[readme]",
      "quote": "python -m sglang.launch_server --model-path LGAI-EXAONE/EXAONE-Deep-7.8B \\"
    },
    {
      "source": "[readme]",
      "quote": "ollama run exaone-deep:7.8b"
    }
  ],
  "3-1 (Pre-training)": "No sentences referring to LG-AI-EXAONE/EXAONE-Deep’s pre-training data, pipeline, objectives, or hyper-parameters are present in the supplied quote set, so no summary of pre-training methodology can be produced from the provided material.",
  "3-1 (Pre-training)__evidence": [],
  "3-2 (Fine-tuning)": "The quotation list contains no statements that mention any fine-tuning regimen, datasets, objectives, or procedures related to LG-AI-EXAONE/EXAONE-Deep, and therefore no information about fine-tuning can be summarized solely from the given excerpts.",
  "3-2 (Fine-tuning)__evidence": [],
  "3-3 (Reinforcement Learning)": "There are no quotes that discuss reinforcement-learning-based alignment (e.g., RLHF, DPO) or any other RL procedure for LG-AI-EXAONE/EXAONE-Deep. Consequently, there is no content available to summarize for this category.",
  "3-3 (Reinforcement Learning)__evidence": [],
  "4-1 (Pre-training Data)": "The supplied material contains no statements that describe the sources, types, volumes, licensing status, or any other aspect of the pre-training data used for LG-AI-EXAONE/EXAONE-Deep. Consequently, no information can be summarized for this item on the basis of the provided quotes alone.",
  "4-1 (Pre-training Data)__evidence": [],
  "4-2 (Fine-tuning Data)": "The provided quotes do not mention any datasets, sampling strategies, public releases, or curation details that would shed light on the fine-tuning data employed for LG-AI-EXAONE/EXAONE-Deep. Therefore, no summary is possible from the current evidence.",
  "4-2 (Fine-tuning Data)__evidence": [],
  "4-3 (Reinforcement Learning Data)": "None of the supplied text addresses the composition, generation, or accessibility of reinforcement-learning (e.g., RLHF or RLAIF) datasets for the target model. As a result, there is no information to summarize for this section.",
  "4-3 (Reinforcement Learning Data)__evidence": [],
  "4-4 (Data Filtering)": "The only explicit statement provided on filtering or safety work is: “LG AI Research strives to reduce potential risks that may arise from EXAONE language models.”  From this single line, we can infer that LG AI Research has acknowledged the existence of potential harms associated with EXAONE-Deep and that some form of risk-reduction or data-filtering practice is in place. However, the quote offers no concrete details—no numeric thresholds, heuristics, classifier names, or pipeline-stage descriptions. Thus, while the organization affirms its intent to mitigate risk, the filtering methodology, its criteria, and its quantitative impact remain undisclosed in the material supplied.",
  "4-4 (Data Filtering)__evidence": [
    {
      "source": "[readme]",
      "quote": "LG AI Research strives to reduce potential risks that may arise from EXAONE language models."
    }
  ],
  "__usage": {
    "fine_tuning": "unknown",
    "rl": "unknown"
  }
}