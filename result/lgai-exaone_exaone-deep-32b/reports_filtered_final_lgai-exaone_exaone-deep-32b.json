{
  "1-1 (Weights)": "According to the project’s statement, “All EXAONE Deep models are openly available for research purposes and can be downloaded from this https URL.”  This indicates that the EXAONE Deep 32B checkpoint is public and freely downloadable.  The quote explicitly notes (1) public release status, (2) open availability, (3) the intended scope (“research purposes”), and (4) that a direct HTTPS link is provided for retrieval.  No additional constraints, authentication steps, or gated-access procedures are mentioned in the provided material.",
  "1-2 (Code)": "No information about training code—whether for data preparation, pre-training, fine-tuning, or RLHF—is present in the supplied quotes.",
  "1-3 (License)": "The only licensing detail stated is: “All EXAONE Deep models are openly available for research purposes and can be downloaded from this https URL.”  From this, one can infer that use is granted at least for research, but the quote does not specify whether modification, redistribution, or commercial exploitation are permitted or restricted.  No formal license name, version number, or additional terms such as “non-commercial,” “no derivatives,” or “evaluation only” appear in the excerpted text.",
  "1-4 (Paper)": "The official technical reference is the paper titled “EXAONE Deep: Reasoning Enhanced Language Models.”  A supporting evaluation summary notes that “our smaller models, EXAONE Deep 2.4B and 7.8B, outperform other models of comparable size, while the largest model, EXAONE Deep 32B, demonstrates competitive performance against leading open-weight models.”  Additionally, LG AI Research announced the release in the news article “EXAONE Deep Released ━ Setting a New Standard for Reasoning AI.”  Together, these sources provide the formal description of the architecture and the public communication of its performance and availability.",
  "1-1 (Weights)__evidence": [
    {
      "source": "[sections/https://arxiv.org/abs/2503.12524]",
      "quote": "All EXAONE Deep models are openly available for research purposes and can be downloaded from this https URL"
    }
  ],
  "1-2 (Code)__evidence": [],
  "1-3 (License)__evidence": [
    {
      "source": "[sections/https://arxiv.org/abs/2503.12524]",
      "quote": "All EXAONE Deep models are openly available for research purposes and can be downloaded from this https URL"
    },
    {
      "source": "[web:https://www.bigcode-project.org/docs/pages/model-license/]",
      "quote": "https://www.bigcode-project.org/docs/pages/model-license/"
    },
    {
      "source": "[web:https://huggingface.co/tiiuae/falcon-40b/blob/main/LICENSE]",
      "quote": "https://huggingface.co/tiiuae/falcon-40b/blob/main/LICENSE"
    }
  ],
  "1-4 (Paper)__evidence": [
    {
      "source": "[sections/https://arxiv.org/abs/2503.12524]",
      "quote": "Title: EXAONE Deep: Reasoning Enhanced Language Models"
    },
    {
      "source": "[sections/https://arxiv.org/abs/2503.12524]",
      "quote": "Evaluation results show that our smaller models, EXAONE Deep 2.4B and 7.8B, outperform other models of comparable size, while the largest model, EXAONE Deep 32B, demonstrates competitive performance against leading open-weight models."
    },
    {
      "source": "[sections/https://www.lgresearch.ai/news/view?seq=543]",
      "quote": "EXAONE Deep Released ━ Setting a New Standard for Reasoning AI - LG AI Research News"
    }
  ],
  "1-5 (Architecture)": "",
  "1-6 (Tokenizer)": "",
  "2-1 (Hardware)": "",
  "2-2 (Software)": "",
  "1-5 (Architecture)__evidence": [],
  "1-6 (Tokenizer)__evidence": [],
  "2-1 (Hardware)__evidence": [],
  "2-2 (Software)__evidence": [],
  "2-3 (API)": "",
  "3-1 (Pre-training)": "According to the authors, the EXAONE Deep series—of which the 32 B variant is a member—underwent pre-training primarily on a reasoning-specialized dataset. This dataset was explicitly constructed to include long, continuous “streams of thought,” enabling the model to internalize multi-step reasoning patterns. Emphasis on such data is said to translate into superior results on reasoning-heavy evaluations, with the paper highlighting strong performance on math and coding benchmarks.",
  "3-2 (Fine-tuning)": "",
  "3-3 (Reinforcement Learning)": "",
  "2-3 (API)__evidence": [],
  "3-1 (Pre-training)__evidence": [
    {
      "source": "[sections/https://arxiv.org/abs/2503.12524]",
      "quote": "We present EXAONE Deep series, which exhibits superior capabilities in various reasoning tasks, including math and coding benchmarks. We train our models mainly on the reasoning-specialized dataset that incorporates long streams of thought processes."
    }
  ],
  "3-2 (Fine-tuning)__evidence": [],
  "3-3 (Reinforcement Learning)__evidence": [],
  "4-1 (Pre-training Data)": "The disclosure about pre-training material for lgai-exaone/exaone-deep-32b is extremely limited. The quoted authors state that \"We present EXAONE Deep series … We train our models mainly on the reasoning-specialized dataset that incorporates long streams of thought processes.\" From this we can infer only two concrete points: (1) the main corpus is explicitly described as a \"reasoning-specialized dataset,\" and (2) the dataset is said to contain \"long streams of thought processes.\" The text also links this choice to the model’s intended strength on reasoning-heavy benchmarks such as math and coding tasks. No additional facts are given about the absolute or relative size of the corpus, the balance between different data domains, licensing status, geographic or linguistic coverage, or any quantitative breakdowns. Likewise, there is no mention of specific source repositories, web domains, or any cleansing or deduplication procedures applied prior to pre-training. Consequently, apart from the emphasis on reasoning content and long-form thought sequences, virtually every other aspect of the pre-training data—volumes, provenance, permissions, composition ratios—remains undisclosed in the provided material.",
  "4-2 (Fine-tuning Data)": "",
  "4-3 (Reinforcement Learning Data)": "",
  "4-4 (Data Filtering)": "",
  "4-1 (Pre-training Data)__evidence": [
    {
      "source": "[pdf_text]",
      "quote": "We present EXAONE Deep series, which exhibits superior capabilities in various reasoning tasks, including math and coding benchmarks. We train our models mainly on the reasoning-specialized dataset that incorporates long streams of thought processes."
    }
  ],
  "4-2 (Fine-tuning Data)__evidence": [],
  "4-3 (Reinforcement Learning Data)__evidence": [],
  "4-4 (Data Filtering)__evidence": [],
  "__usage": {
    "fine_tuning": "unknown",
    "rl": "unknown"
  }
}