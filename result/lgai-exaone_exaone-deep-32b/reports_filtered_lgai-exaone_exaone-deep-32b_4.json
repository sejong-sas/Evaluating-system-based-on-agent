{
  "4-1 (Pre-training Data)": "The disclosure about pre-training material for lgai-exaone/exaone-deep-32b is extremely limited. The quoted authors state that \"We present EXAONE Deep series … We train our models mainly on the reasoning-specialized dataset that incorporates long streams of thought processes.\" From this we can infer only two concrete points: (1) the main corpus is explicitly described as a \"reasoning-specialized dataset,\" and (2) the dataset is said to contain \"long streams of thought processes.\" The text also links this choice to the model’s intended strength on reasoning-heavy benchmarks such as math and coding tasks. No additional facts are given about the absolute or relative size of the corpus, the balance between different data domains, licensing status, geographic or linguistic coverage, or any quantitative breakdowns. Likewise, there is no mention of specific source repositories, web domains, or any cleansing or deduplication procedures applied prior to pre-training. Consequently, apart from the emphasis on reasoning content and long-form thought sequences, virtually every other aspect of the pre-training data—volumes, provenance, permissions, composition ratios—remains undisclosed in the provided material.",
  "4-2 (Fine-tuning Data)": "",
  "4-3 (Reinforcement Learning Data)": "",
  "4-4 (Data Filtering)": "",
  "4-1 (Pre-training Data)__evidence": [
    {
      "source": "[pdf_text]",
      "quote": "We present EXAONE Deep series, which exhibits superior capabilities in various reasoning tasks, including math and coding benchmarks. We train our models mainly on the reasoning-specialized dataset that incorporates long streams of thought processes."
    }
  ],
  "4-2 (Fine-tuning Data)__evidence": [],
  "4-3 (Reinforcement Learning Data)__evidence": [],
  "4-4 (Data Filtering)__evidence": []
}