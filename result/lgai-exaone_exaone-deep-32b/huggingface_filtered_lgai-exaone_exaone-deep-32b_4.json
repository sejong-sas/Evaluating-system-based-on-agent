{
  "4-1 (Pre-training Data)": "",
  "4-1 (Pre-training Data)__evidence": [],
  "4-2 (Fine-tuning Data)": "The only explicit fine-tuning information for lgai-exaone/exaone-deep-32b comes from the statement: “base_model: LGAI-EXAONE/EXAONE-3.5-32B-Instruct” with the accompanying tag “base_model_relation: finetune.”  This establishes that the 32-billion-parameter EXAONE-3.5-32B-Instruct checkpoint is the direct parent of the target release and that the current model was produced by further fine-tuning that base.  No additional details on the composition, sourcing, or public availability of the fine-tuning dataset are provided in the available material.",
  "4-2 (Fine-tuning Data)__evidence": [
    {
      "source": "[readme]",
      "quote": "base_model: LGAI-EXAONE/EXAONE-3.5-32B-Instruct\nbase_model_relation: finetune"
    }
  ],
  "4-3 (Reinforcement Learning Data)": "",
  "4-3 (Reinforcement Learning Data)__evidence": [],
  "4-4 (Data Filtering)": "The data-filtering description for lgai-exaone/exaone-deep-32b is limited to the single note that the team “made every effort to exclude personal, harmful, and biased information from the training data,” while conceding that “some problematic content may still be included.”  This conveys that a filtering pass was performed to remove personal data and mitigate harmful or biased material, yet acknowledges that the process is imperfect.  No numerical thresholds, specific tools, classifier names, or percentage-of-data-removed figures are disclosed.",
  "4-4 (Data Filtering)__evidence": [
    {
      "source": "[readme]",
      "quote": "The EXAONE language model has certain limitations and may occasionally generate inappropriate responses. While we have made every effort to exclude personal, harmful, and biased information from the training data, some problematic content may still be included, potentially leading to undesirable responses."
    }
  ]
}