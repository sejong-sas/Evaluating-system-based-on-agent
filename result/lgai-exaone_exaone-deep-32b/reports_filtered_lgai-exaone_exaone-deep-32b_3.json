{
  "2-3 (API)": "",
  "3-1 (Pre-training)": "According to the authors, the EXAONE Deep series—of which the 32 B variant is a member—underwent pre-training primarily on a reasoning-specialized dataset. This dataset was explicitly constructed to include long, continuous “streams of thought,” enabling the model to internalize multi-step reasoning patterns. Emphasis on such data is said to translate into superior results on reasoning-heavy evaluations, with the paper highlighting strong performance on math and coding benchmarks.",
  "3-2 (Fine-tuning)": "",
  "3-3 (Reinforcement Learning)": "",
  "2-3 (API)__evidence": [],
  "3-1 (Pre-training)__evidence": [
    {
      "source": "[sections/https://arxiv.org/abs/2503.12524]",
      "quote": "We present EXAONE Deep series, which exhibits superior capabilities in various reasoning tasks, including math and coding benchmarks. We train our models mainly on the reasoning-specialized dataset that incorporates long streams of thought processes."
    }
  ],
  "3-2 (Fine-tuning)__evidence": [],
  "3-3 (Reinforcement Learning)__evidence": []
}