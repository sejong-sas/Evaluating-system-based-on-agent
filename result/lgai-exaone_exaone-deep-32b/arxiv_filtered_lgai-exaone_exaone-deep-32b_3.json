{
  "2-3 (API)": "",
  "3-1 (Pre-training)": "The quotes indicate that the EXAONE Deep family, which includes the 32 B variant, is pretrained on NVIDIA H100 GPU clusters hosted on Google Cloud Platform and orchestrated through the NVIDIA NeMo framework. The authors note that the overall compute consumed for both the initial pre-training run and for later reasoning-oriented stages is tabulated in their Table 1. Architecturally, the EXAONE Deep line is initialized from the EXAONE 3.5 Instruct base models, which are themselves instruction-tuned systems already capable of following prompts; these Instruct checkpoints therefore serve as the foundational weights upon which further Deep training is performed.",
  "3-2 (Fine-tuning)": "LG AI Research presents three parameter scales—EXAONE Deep 2.4 B, 7.8 B and 32 B—that are explicitly described as fine-tuned descendants of the EXAONE 3.5 series and are ‘specifically optimized for reasoning tasks.’ To reach this goal, the team prepares supervised fine-tuning (SFT) and Direct Preference Optimization (DPO) datasets in a deliberately templated format (see their Figure 3). This templated SFT+DPO regimen is stated to be the key mechanism for boosting the reasoning capability of the EXAONE Deep models, including the 32 B variant.",
  "3-3 (Reinforcement Learning)": "",
  "2-3 (API)__evidence": [],
  "3-1 (Pre-training)__evidence": [
    {
      "source": "[sections/Modeling]",
      "quote": "In terms of training compute, the EXAONE Deep models are trained using NVIDIA H100 GPU clusters provided by Google Cloud Platform and NVIDIA NeMo Framework. The amount of computation used for pretraining of base models and fine-tuning of enhancing reasoning is presented in Table 1."
    },
    {
      "source": "[sections/Modeling]",
      "quote": "The base models of EXAONE Deep are EXAONE 3.5 Instruct models [7, 8, 9], which are instruction-tuned models possessing instruction-following capabilities."
    }
  ],
  "3-2 (Fine-tuning)__evidence": [
    {
      "source": "[sections/Introduction]",
      "quote": "LG AI Research is introducing a new model lineup called EXAONE Deep 2.4B, 7.8B, and 32B. These models are fine-tuned versions of the EXAONE 3.5 series [6], specifically optimized for reasoning tasks."
    },
    {
      "source": "[sections/Modeling]",
      "quote": "To enhance the reasoning abilities of EXAONE Deep, we structured SFT and DPO data in a templated format as illustrated in Figure 3."
    }
  ],
  "3-3 (Reinforcement Learning)__evidence": []
}