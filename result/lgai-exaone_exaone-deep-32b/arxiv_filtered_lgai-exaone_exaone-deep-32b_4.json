{
  "4-1 (Pre-training Data)": "",
  "4-2 (Fine-tuning Data)": "The lgai-exaone/exaone-deep-32b model is described as a fine-tuned descendant of the “EXAONE 3.5 series,” with the explicit goal of improving its performance on reasoning-oriented tasks. According to the available quote, the developers approached this objective through three well-known and widely adopted fine-tuning strategies: (1) Supervised Fine-Tuning (SFT), (2) Direct Preference Optimization (DPO), and (3) Online Reinforcement Learning (Online RL). No additional details on the size, source, or exact composition of the fine-tuning datasets are provided in the quoted material, but the statement makes clear that all three techniques were used in a concerted way to adapt the base EXAONE model line to the reasoning domain.",
  "4-3 (Reinforcement Learning Data)": "For the lgai-exaone/exaone-deep-32b model, the only information disclosed about reinforcement learning data is that Online Reinforcement Learning (Online RL) constituted one of the three main methods—alongside Supervised Fine-Tuning (SFT) and Direct Preference Optimization (DPO)—applied during development. The quote stresses that these models stem from the EXAONE 3.5 series and have been further optimized for reasoning tasks, but it does not reveal any specifics about the RL dataset’s origin, scale, public availability, or creation process beyond the fact that Online RL was indeed part of the refinement pipeline.",
  "4-4 (Data Filtering)": "",
  "4-1 (Pre-training Data)__evidence": [],
  "4-2 (Fine-tuning Data)__evidence": [
    {
      "source": "[sections/Introduction]",
      "quote": "These models are fine-tuned versions of the EXAONE 3.5 series [6], specifically optimized for reasoning tasks. We have trained these models using three prominent techniques widely employed for fine-tuning: Supervised Fine-Tuning (SFT), Direct Preference Optimization (DPO), and Online Reinforcement Learning (Online RL)."
    }
  ],
  "4-3 (Reinforcement Learning Data)__evidence": [
    {
      "source": "[sections/Introduction]",
      "quote": "These models are fine-tuned versions of the EXAONE 3.5 series [6], specifically optimized for reasoning tasks. We have trained these models using three prominent techniques widely employed for fine-tuning: Supervised Fine-Tuning (SFT), Direct Preference Optimization (DPO), and Online Reinforcement Learning (Online RL)."
    }
  ],
  "4-4 (Data Filtering)__evidence": []
}