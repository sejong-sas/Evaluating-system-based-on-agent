{
  "4-1 (Pre-training Data)": "For HunyuanImage-2.1, the authors state that they \"leverage an extensive dataset\" and pair those images with \"structured captions involving multiple expert models.\"  The captions are multi-granular, providing \"hierarchical semantic information at short, medium, long, and extra-long levels.\"  This combination of a very large corpus and richly layered textual descriptions is presented as the reason the model can achieve strong \"text-image alignment capabilities\" and remain \"responsive to complex semantics.\"  No additional numeric breakdowns, licenses, or source lists are disclosed in the available sentences, but the emphasis is clearly on the scale of the corpus and the sophisticated, stratified captioning strategy used during pre-training.",
  "4-1 (Pre-training Data)__evidence": [
    {
      "source": "[readme]",
      "quote": "We present HunyuanImage-2.1, a highly efficient text-to-image model that is capable of generating 2K (2048 × 2048) resolution images. Leveraging an extensive dataset and structured captions involving multiple expert models, we significantly enhance text-image alignment capabilities."
    },
    {
      "source": "[readme]",
      "quote": "Structured captions provide hierarchical semantic information at short, medium, long, and extra-long levels, significantly enhancing the model’s responsiveness to complex semantics."
    }
  ],
  "4-2 (Fine-tuning Data)": "Fine-tuning for HunyuanImage-2.1 is described as a \"Two-Stage Post-Training\" pipeline.  First, a \"Supervised Fine-Tuning (SFT)\" phase adapts the model with curated instruction-style data, and second, a \"Reinforcement Learning (RL)\" phase further refines the system.  The quotes only indicate that these two stages are applied \"sequentially,\" without giving explicit dataset sizes or public availability details.",
  "4-2 (Fine-tuning Data)__evidence": [
    {
      "source": "[readme]",
      "quote": "Two-Stage Post-Training with Reinforcement Learning: Supervised Fine-Tuning (SFT) and Reinforcement Learning (RL) are applied sequentially in two post-training stages."
    }
  ],
  "4-3 (Reinforcement Learning Data)": "During the reinforcement-learning stage, the team \"introduce[s] a Reward Distribution Alignment algorithm.\"  This method \"incorporates high-quality images as selected samples\" so that the reward signal remains reliable, which in turn \"ensure[s] stable and improved reinforcement learning outcomes.\"  The sentences do not reveal where these high-quality images originate or how many are used, but they make clear that the alignment of reward distributions and the curation of exemplary images are central to the RL dataset design.",
  "4-3 (Reinforcement Learning Data)__evidence": [
    {
      "source": "[readme]",
      "quote": "We introduce a Reward Distribution Alignment algorithm, which innovatively incorporates high-quality images as selected samples to ensure stable and improved reinforcement learning outcomes."
    }
  ],
  "4-4 (Data Filtering)": "For data quality control, the authors note that they \"introduce[d] an OCR agent and IP RAG\" to compensate for \"the shortcomings of general VLM captioners in dense text and world knowledge descriptions.\"  They further apply \"a bidirectional verification strategy\" that \"ensures caption accuracy.\"  Although the quote does not list numeric thresholds or removal ratios, it explicitly identifies the tools (OCR agent, IP RAG) and the verification mechanism used at this filtering stage.",
  "4-4 (Data Filtering)__evidence": [
    {
      "source": "[readme]",
      "quote": "Innovatively, an OCR agent and IP RAG are introduced to address the shortcomings of general VLM captioners in dense text and world knowledge descriptions, while a bidirectional verification strategy ensures caption accuracy."
    }
  ]
}