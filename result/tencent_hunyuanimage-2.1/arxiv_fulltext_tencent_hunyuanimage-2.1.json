{
  "model_id": "tencent/hunyuanimage-2.1",
  "full_texts": [
    {
      "arxiv_id": "2509.04545",
      "full_text": "Tencent Hunyuan\nPromptEnhancer:\nA Simple Approach to Enhance Text-to-Image Models\nvia Chain-of-Thought Prompt Rewriting\nTencent Hunyuan\nhttps://hunyuan-promptenhancer.github.io/\nAbstract\nRecent advancements in text-to-image (T2I) diffusion models have demonstrated\nremarkable capabilities in generating high-fidelity images. However, these models\noften struggle to faithfully render complex user prompts, particularly in aspects\nlike attribute binding, negation, and compositional relationships. This leads to a\nsignificant mismatch between user intent and the generated output. To address\nthis challenge, we introduce PromptEnhancer, a novel and universal prompt\nrewriting framework that enhances any pretrained T2I model without requiring\nmodifications to its weights. Unlike prior methods that rely on model-specific\nfine-tuning or implicit reward signals like image-reward scores, our framework\ndecouples the rewriter from the generator. We achieve this by training a Chain-of-\nThought (CoT) rewriter through reinforcement learning, guided by a dedicated\nreward model we term the AlignEvaluator. The AlignEvaluator is trained to\nprovide explicit and fine-grained feedback based on a systematic taxonomy of\n24 key points, which are derived from a comprehensive analysis of common T2I\nfailure modes. By optimizing the CoT rewriter to maximize the reward from our\nAlignEvaluator, our framework learns to generate prompts that are more precisely\ninterpreted by T2I models. Extensive experiments on the HunyuanImage 2.1\nmodel demonstrate that PromptEnhancer significantly improves image-text align-\nment across a wide range of semantic and compositional challenges. Furthermore,\nwe introduce a new, high-quality human preference benchmark to facilitate future\nresearch in this direction.\n1\nIntroduction\nThe proliferation of large-scale text-to-image (T2I) diffusion models (Ho et al., 2020; Ramesh et al.,\n2021; Saharia et al., 2022; Rombach et al., 2022; Peebles & Xie, 2023; Esser et al., 2024; Hurst et al.,\n2024), such as Imagen (Saharia et al., 2022), Stable Diffusion (Rombach et al., 2022; Podell et al.,\n2024; Esser et al., 2024), HunyuanDiT (Li et al., 2024), Flux (Labs, 2024), and Qwen-Image (Wu et al.,\n2025a), has revolutionized content creation by enabling the synthesis of diverse and photorealistic\nimages from natural language. Despite their impressive capabilities, the quality and accuracy of the\ngenerated output are heavily contingent on the user’s ability to craft precise and detailed prompts.\nA persistent challenge lies in the inherent ambiguity and conciseness of typical user inputs,\nwhich often causes T2I models to misinterpret crucial details, such as failing to bind attributes\nto the correct objects, ignoring negations, or struggling with complex spatial relationships. This\ndiscrepancy highlights a fundamental gap between human intention and model interpretation.\nTo bridge this gap, prior research has explored various prompt rewriting techniques (Wu et al.,\n2025b; Hao et al., 2023). These methods aim to automatically enrich a user’s initial prompt\nto provide more explicit guidance to the T2I model. However, a major limitation of existing\napproaches is their lack of universality. Many rewriters require model-specific fine-tuning or are\nco-trained with a particular generator. Others rely on optimizing for generic, implicit reward\nsignals such as CLIP scores (Radford et al., 2021) or general human preference scores (Xu et al.,\n2023; Wu et al., 2023b). These rewards offer only a coarse measure of overall image-text similarity\nand lack the specificity to diagnose and correct fine-grained errors. This dependency on specific\nT2I architectures and ambiguous reward functions renders them difficult to adapt to new models\nand computationally expensive, restricting their practical applicability in the rapidly evolving\nlandscape of generative models.\nIn this paper, we propose PromptEnhancer, a universal and model-agnostic prompt rewriting\nframework designed to enhance any pretrained T2I model without the need for retraining or\n1\narXiv:2509.04545v3  [cs.CV]  10 Sep 2025\n\nUser Prompt:手绘作业，一个方块、两个三角形和三个圆形，所有图形的轮廓都是粉色的\n<think>用户想要生成图片的核心元素分析：主体是手绘作\n业，包含一个方块、两个三角形、三个圆形。核心属性是所\n有图形的轮廓都是粉色的。\n潜在用户意图推理：用户想表达的是一种儿童或学生时期的\n美术作业感，重点在于“手绘”和“轮廓”。这暗示了画面\n需要有纸张的背景、手绘的笔触感和稚嫩的风格。\n构图及相对关系：原始prompt没有指定布局。为了使画面\n有序且符合“作业”的主题，我将设定一个清晰的布局。我\n将把这些形状排列在画面中央。方块作为基础，三角形和圆\n形围绕它分布，形成一种平衡的构图。例如，三角形放在方\n块上方，圆形散落在周围。属性细化：背景：为了模拟“作\n业”，背景应为白色画纸，并增加手绘感，如轻微的褶皱或\n污渍。 图形：方块：位于中心，强调其粉色轮廓和手绘\n感，如线条粗细不一。三角形：数量为两个，放置在方块上\n方，其粉色轮廓应与方块风格一致。圆形：数量为三个，散\n布在周围，同样强调其粉色手绘轮廓。\n颜色：所有图形的轮廓色都是粉色。为了突出“轮廓”，图\n形内部应为白色或留空，形成对比。风格确定：整体风格应\n为“儿童手绘”或“概念草图”，强调线条的原始感和稚嫩\n感，而非精确的几何图形。</think>\nRepropmt:\n手绘作业。多个基本几何图形排列在一个白\n色的纸张背景上。中央是一个正方形，其轮\n廓由一条明显的粉色线条勾勒而成，线条粗\n细不均，带有手绘感。在正方形的上方，摆\n放着两个三角形，它们的轮廓同样为粉色。\n在这些图形的周围，散落着三个圆形，它们\n的边界也是粉色的线条。所有图形的内部均\n为白色，与粉色的轮廓形成对比。背景是白\n色的画纸，上面有一些轻微的褶皱或污渍，\n增加了手绘的真实感。儿童手绘或概念草图\n的风格。\nUser Prompt: 为一款新型电动跑车设计一块广告牌，画面应是一辆线条流畅、车身呈金属蓝色的\n跑车在霓虹灯闪烁的城市中疾驰而过，标语为“驾驭未来”\n<think> 核心元素分析：主体是为一款新型电动跑车设计的广告牌。\n广告牌上需要包含一辆跑车和城市背景，并有特定标语。\n属性与细节深化：\n1.  广告牌整体：为了突出科技感，设定为竖式广告牌，这是现代广告\n的常见形式。\n2.  跑车：用户要求“线条流畅”和“金属蓝色”。我将此具体化为\n“未来感十足的设计”、“低趴宽体”、“空气动力学线条”，并增\n加“蓝色碳纤维”材质来增强其高端感。为了表现“疾驰而过”，我\n加入了“动态模糊”效果，并补充\n了“车轮喷射出蓝色光轨”的视觉元素，以强化速度感。\n3.  城市背景：用户要求“霓虹灯闪烁”。我将其场景设定为“夜景”，\n并细化为“高楼林立”、“巨大的全息广告牌”和“多层立体交通\n网”，这些元素共同构建了一个典型的赛博朋克未来\n城市景象。\n4.  标语：用户提供了“驾驭未来”四个字。我为其设定了“醒目的白\n色发光立体字”的样式，放置在广告牌顶部，以符合广告设计的常规\n布局。\n构图与风格：\n构图：跑车作为主体，在城市背景中高速行驶，标语\n位于顶部。这是一个清晰的主次关系。\n风格：根据“新型电动跑车”、“霓虹灯城市”等关键词，推断用户\n想要的是“高度精细的3D渲染风格”和“强烈的赛博朋克美学”，整\n体感觉是“动感”和“科技感”</think>\nRepropmt:\n一幅为新型电动跑车设计的竖式广告牌，其画面主\n体是一辆跑车在夜景中疾驰。广告牌的中央位置描\n绘了一辆具有未来感十足的设计的电动跑车，它正\n在一条城市高架路上高速行驶。跑车的车身呈现出\n金属蓝色，表面反射着周围环境的光线，其线条流\n畅，具有低趴的宽体和空气动力学线条，车轮因高\n速旋转而产生动态模糊，喷射出蓝色的光轨。在跑\n车的背景中，是一个充满霓虹灯的未来派城市夜景，\n高楼林立，建筑物外墙上有着巨大的全息广告牌，\n多层立体交通网交错穿行。广告牌的顶部用醒目的\n白色发光立体字写着标语：“驾驭未来”。整体画\n面呈现出高度精细的3D渲染风格，具有强烈的赛博\n朋克美学和动感。这张图片是为一款电动跑车设计\n的概念广告牌，整体呈现出强烈的科技感和未来主\n义风格。\nUser Prompt: 中国水墨画风格，五个男人拿着酒碗在喝酒，动作十分豪迈，地上散落着几个酒坛\n<think>\n用户想要生成图片的核心元素分析：人物为五个男人，道具为酒\n碗和酒坛，风格为中国水墨画。核心动作是五个男人在豪迈地喝\n酒，地上有散落的酒坛。潜在用户意图是希望画面能体现出中国\n古典文人或侠客那种狂放不羁、纵情山水的豪迈氛围。基于此，\n我对prompt进行细化：\n1. 主体元素与构图：将“五个男人”具体化，设定他们位于“山\n间的茅草屋前”，形成一个有主次、有远近的构图。我将设定一\n个中心人物作为视觉焦点，其他人物围绕他分布，形成互动。\n2. 动作细化：将“动作十分豪迈”这一抽象描述具体化。为中心\n人物设计一个极具张力的动作，如“仰头一饮而尽”。为其他人\n物设计不同的动态，如“互相敬酒”或“手持酒碗大声谈笑”，\n以丰富画面故事性。\n3. 属性与细节：为人物添加符合风格的服饰，如“宽大的长袍”\n和“飘逸的头发”。为道具增加细节，酒碗设定为“粗瓷”，酒\n坛为“陶土”材质，并加入“坛口冒着白色的酒气”的动态细节。\n4. 风格与背景：强化“中国水墨画风格”。不仅要求黑白渲染，\n还明确指出技法，如使用“大面积的浓墨”表现衣物褶皱，用\n“干笔皴法”绘制背景的“山石”和“古树”。同时，加入水墨\n画中常见的“题跋”和“红色印章”元素，使风格更地道。\n</think>\nRepropmt:\n一幅中国水墨画描绘了五个男人在山间的茅草屋\n前聚会饮酒的场景。画面中央，五个身着宽大长\n袍、头发飘逸的男人围坐在一起，他们正举着粗\n瓷酒碗豪迈地饮酒。位于中心位置的一位男子正\n仰头一饮而尽，他的衣服褶皱用大面积的浓墨渲\n染。他周围的人也姿态各异，有的在互相敬酒，\n有的则手持酒碗大声谈笑。在他们的脚下，地面\n上散落着几个陶土酒坛，一些坛口还冒着白色的\n酒气。背景是用干笔皴法绘制的连绵山石和几棵\n古树。画面的右侧有竖排的毛笔题跋和红色的印\n章。整体画面呈现出典型中国水墨画的风格，强\n调了笔触的力度和墨色的浓淡变化。这张图片是\n中国水墨风格的绘画作品。\nUser Prompt: 混合媒介拼贴美学，画面主体是一个被彩色卡纸、报纸、折皱的卫生纸遮挡的山脉，背\n景是高楼建筑，一节棕色的火车车厢从画面中间穿过\n<think>\n核心元素分析：主体是山脉，前景有多种媒介遮挡，背景是高楼，一\n节火车车厢横穿画面。整体风格为混合媒介拼贴。\n构图及相对关系：山脉是主体，位于画面中央，但被前景遮挡。高楼\n是背景。火车车厢从中间穿过，横贯画面。前景元素包括彩色卡纸、\n报纸、卫生纸，它们覆盖在山脉之上。\n属性细节深化：\n1. 山脉：作为主体，虽然被遮挡，但仍需描述其可见部分的特征，如\n“深蓝色的轮廓”和“柔和的线条”，以引导模型生成主体形态。\n2. 前景遮挡物：对用户提到的“彩色卡纸、报纸、折皱的卫生纸”进\n行具体化。卡纸赋予“鲜艳的红色和黄色”；报纸明确为“黑白新闻\nprint”；卫生纸强调其“褶皱纹理”。\n3. 火车车厢：用户指定为“棕色”，我进一步补充“复古感”和“金\n属光泽”来强化其材质和风格。\n4. 背景：用户提到“高楼建筑”，我将其细化为“现代风格”的“灰\n色和蓝色剪影”，以匹配拼贴风格。\n动作与状态：火车是“穿过”画面，是一个动态描述。前景媒介是\n“覆盖”、“堆叠”在山脉上，描述了元素间的空间关系。\n风格与氛围：核心是“混合媒介拼贴美学”。我将此概念分解为更具\n体的指令，如“切割、粘贴、层叠”，并引入相关艺术流派“达达主\n义或波普艺术”，以帮助模型准确捕捉所需的混乱、创意十足的视觉\n风格</think>\nRepropmt:\n一幅具有明显混合媒介拼贴美学的作品，画面中央是一\n个被多种材料部分遮挡的山脉图像。画面的主体是一座\n连绵的山脉，其深蓝色的轮廓和柔和的线条部分可见。\n山脉的前方，大面积地覆盖着切割成不规则形状的彩色\n卡纸，颜色包括鲜艳的红色和黄色。在卡纸之间，夹杂\n着撕碎的黑白新闻报纸，上面可以看到模糊的文字和图\n像。此外，还有折叠和皱缩的白色卫生纸团，其明显的\n褶皱纹理增加了画面的层次感。一节长方形的棕色火车\n车厢从画面的中间位置水平穿过，横贯整个画面，其表\n面带有复古感和金属光泽。在山脉的后方，作为背景的\n是多座现代风格的高楼建筑剪影，呈现出灰色和蓝色的\n色调。整体画面呈现出一种典型的混合媒介拼贴艺术风\n格，结合了切割、粘贴、层叠等多种手法，类似于达达\n主义或波普艺术作品，营造出一种混乱而富有创意的视\n觉效果。这张图片是典型的数字拼贴艺术作品。\nUser Prompt: A futuristic pool under a starlit sky reflects vibrant nebulae and galaxies. Neon cyan, magenta, and \nviolet lights create a cyberpunk ambiance. Above, glowing \"Prompt Enhancer\" text floats, casting a glow on the \nwater. Distant planets with rings and cosmic dust add to the surreal, dreamlike atmosphere\n<think>Subject & Composition: The scene centers on a \nfuturistic swimming pool under a starlit sky. The pool should reflect \nvibrant nebulae, swirling galaxies, and twinkling stars, creating a \nmesmerizing and dreamlike atmosphere. The neon lights in cyan, \nmagenta, and violet will add a cyberpunk vibe, and the glowing \n\"Prompt Enhancer\" text will float above, casting a reflective glow on \nthe water.\nStyle & Lighting: The \"Prompt Enhancer\" text should glow in \npurple and pink, surrounded by a soft halo. This should give the \nimpression of radiant light casting subtle reflections on the pool’s \nsurface. The overall lighting will have a luminous, futuristic quality, \nemphasizing the cyberpunk theme through the use of neon hues.\nBackground & Atmosphere: The background will feature distant \nplanets with ethereal rings and cosmic dust, enhancing the surreal, \ndreamlike quality of the scene. These celestial elements should \nappear soft yet distinct, blending into the starry sky. The \ncombination of the planets, rings, and dust will reinforce the theme \nof futuristic elegance and wonder.\nComposition & Relationships: The pool is the central focus of \nthe composition, with the glowing text above it. The neon lights and \nreflections should create a balance between the foreground (the pool \nand text) and the background (the celestial bodies). The entire scene \nshould feel unified, evoking a sense of modernity and otherworldly \nbeauty.<think>\nRepropmt:\nA futuristic swimming pool under a starlit sky, \nreflecting the deep cosmos. The pool’s surface \nglows with vibrant nebulae, swirling galaxies, \nand twinkling stars. Neon lights in shades of \ncyan, magenta, and violet illuminate the area, \ncreating a mesmerizing, cyberpunk ambiance. \nAbove the pool, glowing purple and pink text \n'Prompt Enhancer' floats in mid-air, \nsurrounded by a soft halo, casting a reflective \nglow on the water. The background features \ndistant planets with ethereal rings and cosmic \ndust, enhancing the surreal and dreamlike \natmosphere. The scene combines sleek, modern \narchitecture with an otherworldly, tech-\ninspired design, evoking a sense of wonder and \nfuturistic elegance.\nUser Prompt: Energetic poster for New York City. Anime businesswoman hailing a taxi with skyscrapers and Times Square signs around. \n'NYC: Bright Ambitions' in urban graffiti font. 'Own Every Dream' tagline. Saturated yellows, reds, and sharp blues.\n<think>The user wants to generate an image with the core elements: \na poster, an anime businesswoman, a taxi, skyscrapers, Times \nSquare signs, and specific text. Style and Mood: The image should \nbe an energetic poster for New York City, blending anime and poster \nstyles. It should have a dynamic, bustling atmosphere with vibrant \ncolors like saturated yellows, reds, and sharp blues. Main Subject: \nThe anime businesswoman is the central focus. She should appear in \nprofessional attire with styled hair, posing energetically while hailing \na taxi with her arm raised. This conveys an \"energetic\" theme. \nBackground: The background should feature iconic New York City \nelements: skyscrapers and Times Square signs with neon lights and \nbillboards, capturing the city's vibrant atmosphere.\nText Elements: Main Title: \"NYC: Bright Ambitions\" in urban \ngraffiti font.\nTagline: \"Own Every Dream\" in a sleek, complementary font. \nComposition: The businesswoman should be in the foreground, \nwith the taxi arriving nearby. The skyscrapers and Times Square \nsigns make up the background. The main title is prominent at the top, \nwith the tagline below in a smaller font. Color Palette: The colors \nshould be integrated into all elements, from the businesswoman’s \nclothing to the neon lights and sky, ensuring a cohesive and vibrant \ncomposition.<think>\nRepropmt: \nAn energetic poster for New York City unfolds, featuring \na dynamic scene with an anime-style businesswoman in \nthe midst of hailing a taxi. The central figure is a young \nwoman with large, expressive eyes and dark hair styled \nin a bob, wearing a professional blue business suit with \nmotion lines indicating movement. She stands on a \nbustling street, her arms outstretched as she calls for a \nclassic yellow taxi cab that is approaching. In the \nbackground, towering skyscrapers with sleek, anime-\ninspired architecture rise into the sky, adorned with \nvibrant, glowing billboards and neon signs \ncharacteristic of Times Square. Across the top of the \nposter, the text \"NYC: Bright Ambitions\" is displayed in \na large, stylized urban graffiti font, with spray-paint-like \nedges. Below this main title, the tagline \"Own Every \nDream\" is written in a smaller, clean font. The entire \ncomposition is rendered with saturated colors, \ndominated by bright yellows, reds, and sharp blues. The \noverall presentation is a fusion of anime illustration and \ngraphic design.\nUser Prompt: An oil painting of Mona Lisa wearing pearl earrings\n<think>The user wants to generate an image with the core \nelements: a person (Mona Lisa), an artistic style (oil \npainting), and a specific accessory (pearl earrings).\nSubject & Composition: The main subject is \"Mona \nLisa\" in a classic three-quarter view. She should have her \nserene, enigmatic smile and gently folded hands in her lap. \nThe composition is a portrait, focusing on her from the \nchest up.\nStyle: The image should be in \"oil painting\" style with \nvisible, thick brushstrokes (impasto technique), textured \nlayers, and bold brushwork, creating a canvas effect. The \ncolor application should be thick and layered.\nAccessories: Mona Lisa should wear large, lustrous pearl \nearrings to emphasize their quality. Her dark hair and dark, \ntextured gown should be depicted with broad, expressive \nbrushstrokes.\nBackground: The background should feature a \nmountainous landscape, painted with loose, energetic \nbrushstrokes in deep blues and greens, staying true to the \noriginal painting’s style.</think>\nRepropmt:\nAn oil painting portrays the iconic figure of Mona Lisa, \ndepicted from the chest up in a classic three-quarter \nview. Her face, characterized by smooth, thick paint \napplication technique, bears her famous serene, \nenigmatic smile, with visible, thick brushstrokes \ndefining her features. In her ears, she wears large, \nlustrous pearl earrings, which stand out against her \ndark hair, painted with broad, expressive brushstrokes. \nShe is dressed in a dark, textured gown, the folds of \nwhich are rendered with thick paint application, and \nher hands are gently folded in her lap. The background \nbehind her is a maelstrom of loose, energetic \nbrushstrokes in shades of deep blue and green, \nsuggesting a mountainous landscape. The overall \npresentation is that of an oil canvas, with thick layers \nof paint and bold brushwork evident throughout. Oil \nPainting Style.\nUser Prompt: Wildlife poster for Serengeti plains. Wide-eyed chibi explorer riding friendly lion cub. 'Serengeti: Roar of \nAdventure' in whimsical font. 'Where Dreams Run Wild' tagline. Warm yellows and soft browns\n<think>The user wants to generate an image of a \"wildlife \nposter for Serengeti plains\". The core elements are a \"chibi \nexplorer\" riding a \"lion cub\", with specific text and a defined \ncolor palette.\nStyle and Format: The image should be a whimsical wildlife \nposter with a structured layout. It needs a title, an illustration, \nand possibly a tagline. The overall aesthetic should be cute and \nplayful in a \"chibi\" style.\nSubjects and Action: Explorer: A chibi explorer with a large \nhead, small body, wide eyes, wearing outdoor gear like a hat \nand backpack. Lion Cub: A friendly, smiling lion cub with a \nlight-colored mane, in motion as it carries the explorer.\nText Elements:  \nMain Title: \"Serengeti: Roar of Adventure\" in a whimsical, \nplayful font at the top. Tagline: \"Where Dreams Run Wild\" in \na smaller font below the main illustration. Setting and \nBackground: The Serengeti plains with warm yellows and soft \nbrowns. The background should feature rolling hills and \nsavanna grass. Composition: The title is at the top, the \nexplorer and lion cub in the center, and the tagline at the \nbottom. The colors should unify the scene.\n</think>\nRepropmt:\nA vintage-style poster for Yellowstone National Park is \npresented, featuring a dramatic depiction of the Old \nFaithful geyser erupting. The central focus is the \ngeyser, which shoots a powerful column of water and \nsteam high into the air, rendered with thick, expressive \nbrushstrokes in shades of orange, yellow, and red. In \nthe upper portion of the poster, the word \n\"YELLOWSTONE\" is displayed in a large, rustic font \nwith a texture that mimics carved wood, showing \ndistressed and grunge elements. Below the main title, a \nsubtitle reads \"America's First National Park\" in a \nsmaller, elegant script. The background consists of \npainterly representations of mountains and a sky, with \nrich, warm colors. In the lower-right corner, the dark \nsilhouettes of two bison are positioned, adding to the \nwilderness theme. The overall presentation is that of a \ndigital painting, emulating the style of classic national \npark posters.\nPrompt Enhancer Has Arrived!\nPromptEnhancer Has Arrived!\nFigure 1: PromptEnhancer enables high-fidelity and stylistically diverse image generation from\nuser prompts. Using HunyuanImage 2.1 as the base T2I model, our method demonstrates its\nversatility across various domains, including photorealism, digital art, abstract geometry, and\nmultilingual text-in-image generation. The examples showcase how minimal user inputs are\ntransformed into rich, detailed prompts that yield high-quality visual outputs, bridging the gap\nbetween user intent and model execution.\narchitectural changes. The core idea is to completely decouple the task of prompt refinement from\nthe image generation process. Our framework introduces a sophisticated rewriter that employs a\nChain-of-Thought (CoT) methodology (Wei et al., 2022). This approach emulates a human-like\nreasoning process, allowing the rewriter to systematically deconstruct the initial prompt into its\ncore semantic components, identify potential ambiguities, and then enrich it with explicit details\nconcerning object attributes, spatial arrangements, and complex interactions. The resulting detailed\nprompt can then be used by a T2I model to generate a more faithful image.\nThe key to our framework’s effectiveness lies in how the CoT rewriter is trained. We introduce a\nnovel reward model, the AlignEvaluator, which is specifically designed for fine-grained, multi-\nfaceted evaluation. Instead of relying on a single, holistic score, the AlignEvaluator assesses a\ngenerated image against a comprehensive set of 24 distinct key points. These points are meticu-\nlously organized into six distinct categories, covering a wide spectrum of challenges including\nlinguistic understanding (e.g., negation, pronoun reference), visual attribute accuracy, composi-\ntional relationships, and world knowledge. By optimizing the CoT rewriter to generate prompts\nthat maximize the explicit, structured reward from the AlignEvaluator, we effectively align the\nprompt generation process with a nuanced understanding of human preferences. This ensures that\nthe rewritten prompts are not merely longer, but are structured in a way that elicits higher-quality\n2\n\nand more accurate results from the downstream T2I model.\nOur main contributions are as follows:\n• We propose PromptEnhancer, a novel and universal prompt rewriting framework. It intro-\nduces a Chain-of-Thought (CoT) mechanism to systematically deconstruct and enrich user\nprompts, significantly improving the performance of existing T2I models without requiring\nany modification to their weights.\n• We develop a multi-faceted reward model, the AlignEvaluator, trained on 24 distinct key\npoints organized into 6 categories, enabling fine-grained evaluation and effective alignment\nof the rewriter.\n• We provide extensive empirical evidence of our method’s effectiveness on a state-of-the-art\nT2I model and contribute a new, high-quality benchmark based on human evaluation to the\ncommunity.\n2\nMethodology\nT2I-KeyPoints \nSet for RL\nPrompt\nEnhancer\nreprompt1\nreprompt2\nreprompt3\nrepromptN\n…\nimage1\nimage2\nimage3\nimageN\n…\nAlign\nEvaluator\nScore1\nScore2\nScore3\nScoreN\n…\nPolicy  Optimization\nStage 1: SFT for Rewriter Initialization (Sec 2.2)\nThe AlignEvaluator (Sec 2.4)\nPrompt\nEnhancer\nRewriter set \nfor  SFT\nAlign\nEvaluator\n<think>\nOkay, let‘s design a image of “A huge turtle is carrying a \nsmall city on its back”\n## Keypoint Analysis:\nOpposing Relationship: The core concept is the \ncontrast. Here, it's the contrast between a natural, living \ncreature and an artificial, man-made structure. It's also a \nsymbiotic, non-conflicting relationship, which adds \ninterest. The primary opposition is scale.\nTwo Entities Entity 1 is the turtle. Entity 2 is the city.\nSize : This is the main axis of contrast.\nLarge: The turtle must be colossal, ancient, and island-\nsized. Its features should suggest immense age and scale, \nlike a walking landscape.\nSmall: The city must be miniature in comparison, \nnestled on the turtle's back. Its details should be intricate \nbut clearly subordinate in scale to the turtle.\n</think>\nCoT Process\n</think>\nCoT via Keypoints\nreprompt\nsample\nRaw User Prompt\nNext Token \nPrediction Loss\n“A huge turtle is \ncarrying a small city \non its back.”\nEpic fantasy art, a colossal, \nancient sea turtle the size of an \nisland, swimming gracefully \nthrough a cosmic ocean filled \nwith shimmering nebulae. On \nits moss-covered, continent-\nlike shell, rests a miniature, \nintricate city, glowing with \nwarm, bioluminescent lights. \nThe image should emphasize \nthe extreme contrast in scale \nbetween the gigantic, serene \ncreature and the tiny, bustling \ncivilization it carries. Cinematic \nlighting, highly detailed, \ntrending on ArtStation.\nimage\nA cute penguin, \nwearing an astronaut \nsuit and a red scarf, is \nfloating and roaming \nin space. Oil painting \nstyle, with thick \nbrushstrokes, strong \ntexture, and distinct \nlayering of pigments…...\nKey Points \nMatching\n</think>\nHunyuan\nImage 2.1\nreprompt\nKeyPoints\nDesc\nMatch or Not?\nKeyPoint-wise Reward\nStage 2: T2I-KeyPoints Alignment with GRPO (Sec 2.3)\nu Artistic Style\nu Counterfactual\nu Cross-Entity \nBinding\nu Full-body \nAction\nu Pronoun \nResolution\nOil painting style, with \nthick brushstrokes, strong \ntexture, and distinct \nlayering of pigments\nA penguin... wearing an \nastronaut suit... is floating... \nin space.\nbody (white and yellow paint, rounded \nshape), wings (black), spacesuit \n(white, horizontal brushstrokes), head \n(transparent yellow glass cabin), scarf \n(bright red, fluttering, thick and \nsplashed paint effect)\nThe penguin is 'floating and roaming', \nand the scarf 'flutters', describing the \ndynamic state of the subject.\nThe 'Its wings' and 'Its head' in the \nprompt require the model to \naccurately understand that 'Its' \nrefers to the penguin, and apply \nthe corresponding descriptions to \nthe correct body parts.\nGroup-\nwise\nFinal Score\nAverage\nScore\nUser Intent Alignment\nFigure 2: An overview of the training framework for PromptEnhancer. Our framework trains a\nuniversal Rewriter to enhance pretrained Text-to-Image (T2I) model without altering its weights.\nThis is achieved through a two-stage process guided by a specialized reward model. Stage 1:\nSFT for Rewriter Initialization (Sec 2.2). The CoT Rewriter is first initialized via SFT on (user\nprompt, reprompt) pairs. This stage teaches the model to generate structured, chain-of-thought\nstyle responses using a standard next-token prediction loss, establishing a strong foundation for\nrefinement. Stage 2: Policy Alignment with GRPO (Sec 2.3). The initialized rewriter is further\nrefined using GRPO. The rewriter generates multiple reprompt candidates, which are used by a\nfrozen T2I model to create images. The pre-trained AlignEvaluator then assesses each (image, user\nprompt) pair and provides a scalar reward. This reward signal optimizes the rewriter’s policy,\nsteering it toward generating prompts that maximize the alignment between the image and the\nuser’s intent. The AlignEvaluator (Sec 2.4). Central to our framework is the AlignEvaluator, a\npre-trained reward model. It is trained on a large-scale dataset annotated against a taxonomy of 24\nfine-grained key points (T2I-KeyPoints, Tab. 1). This enables it to provide a robust and nuanced\nreward signal, which is crucial for the policy alignment stage.\nIn this section, we delineate the methodology of PromptEnhancer. Our framework is centered\naround two core components: a CoT Rewriter, which enriches user prompts into detailed instruc-\ntions, and an AlignEvaluator, a reward model that provides fine-grained feedback on image-text\nalignment. The primary objective is to train the CoT Rewriter through a two-stage pipeline. First,\nwe initialize the rewriter via Supervised Fine-Tuning (SFT, Sec. 2.2) to master the Chain-of-Thought\nformat. Second, we refine it using Group Relative Policy Optimization (GRPO, Sec. 2.3) (Shao\net al., 2024), where the AlignEvaluator (Sec. 2.4) guides the rewriter to generate prompts that are\noptimally understood by downstream Text-to-Image (T2I) models.\n3\n\n2.1\nOverall Framework\nAs illustrated in Figure 2, the PromptEnhancer framework consists of three main components:\n• The CoT Rewriter a.k.a PromptEnhancer: A policy model based on a large vision-language\nmodel, which is responsible for reformulating an initial user prompt into an enriched one.\n• The AlignEvaluator: A reward model that assesses a generated (image, prompt) pair based\non 24 fine-grained key points and outputs a scalar reward signal.\nTraining Objective. The entire training process aims to optimize the CoT-based Rewriter. It\nfirst undergoes SFT for basic instruction following and then is refined using feedback from the\nAlignEvaluator in a reinforcement learning loop.\n2.2\nStage 1: Supervised Fine-Tuning for Rewriter Initialization\nObjective of SFT. The first stage of our pipeline is Supervised Fine-Tuning, which equips the\nCoT-based Rewriter with the fundamental ability to generate structured, chain-of-thought style\nresponses. The objective is not to perfect the rewriter, but to provide it with a strong initialization\npoint for the subsequent alignment stage.\nInstruction Data Distillation. To achieve this, we construct a high-quality dataset of (user prompt,\nreprompt) pairs by leveraging a powerful proprietary large model (e.g., Gemini-2.5-Pro (Comanici\net al., 2025) for English and DeepSeekV3 (Liu et al., 2024) for Chinese) as a “teacher” for distillation.\nFor a given short user prompt, the teacher model produces a detailed breakdown of the prompt’s\nelements, analyzes potential ambiguities, and finally synthesizes a comprehensive, enriched\nprompt. The CoT Rewriter is then fine-tuned on this dataset using a standard language modeling\nobjective.\n2.3\nStage 2: Policy Alignment with GRPO\nPolicy Alignment via GRPO. After SFT, the rewriter can generate plausible long prompts, but\nthey are not explicitly optimized for image-text alignment. The second stage therefore employs\nGroup Relative Policy Optimization (GRPO (Shao et al., 2024)) to align the user prompt with the\nfine-grained preferences captured by our AlignEvaluator.\nThe GRPO Training Loop. This training process is iterative. For a given user prompts {p1, p2, ..., pN},\nthe CoT Rewriter first generates N candidate rewritten prompts {p′\n1, p′\n2, ..., p′\nN}. Each of these\nprompts is then fed into the frozen T2I model to generate a corresponding image Ii. Subsequently,\nour AlignEvaluator calculates a scalar reward ri for each pair (pi, Ii). Finally, these rewards\n{r1, r2, ..., rN} are used to update the rewriter’s policy, creating a preference ranking that encour-\nages the generation of prompts leading to higher rewards.\n2.4\nThe AlignEvaluator: A Multi-faceted Reward Model\nMotivation for Fine-grained Reward. A generic reward signal like a CLIP score (Radford et al.,\n2021) is insufficient for our goal, as it fails to capture nuanced aspects of prompt following. We\ntherefore developed the AlignEvaluator, a reward model trained specifically to assess image-text\nalignment across a wide array of challenges.\nTaxonomy of T2I-KeyPoints. The evaluator is built upon a taxonomy of 24 fine-grained key points,\norganized into six major categories. These categories cover a comprehensive spectrum of abilities,\nincluding: Linguistic and Grammatical Understanding, which assesses the interpretation of core\nlanguage structures like Negation; Visual Attributes, which focuses on rendering properties like\nObject Count; Actions and Interactions, which measures the depiction of dynamic states; Relational\nand Compositional Structure, which evaluates the handling of complex scenes; and higher-level\nabilities covered by Knowledge and Imagination and In-image Text and Layout. The distribution\nof these dimensions within our evaluation dataset is detailed in Figure 5, which highlights a\nsignificant focus on challenging aspects like artistic style, complex interactions, and full-body\nactions.\nReward Model Training. The AlignEvaluator is trained on a large-scale dataset of (reprompt,\nimage) pairs, each annotated with scores for these 24 key points. This process yields a robust and\nexplicit reward signal that is essential for effectively training the CoT Rewriter.\n4\n\nStep 2: Data Filtering\nImage Set\nGemini \nAPI\nCoT1\nCoT2\nCoT3\nCoTN\n…\nreprompt1\nreprompt2\nreprompt3\nrepromptN\n…\ntriplet set\nGemini \nAPI\nu Semantic deviation \nor information loss?\nu Language fluency?\nu Bias or unverifiable \nclaims?\nReal User Prompt \nVerification Prompt \nuser\nprompt\nreprompt\nCoT\nreprompt\nHuman\nFilter\nuser\nprompt\nGemini \nAPI\nUser\nPrompt\nSet\ncollect\nCoT & \nreprompt\nSet\nStep 3: Human-in-the-Loop\nuser\nprompt\nStep 1: Data Generation\nFigure 3: Overview of the construction and filtering pipeline for the PromptEnhancer training\ndata. The process involves user prompt simulation, Gemini-based generation, human-in-the-loop\nselection, and automated filtering to ensure high quality.\n3\nData Pipeline\nA high-quality dataset is critical for training an effective prompt rewriter. In this section, we\npresent a multi-stage data curation pipeline designed to produce a large-scale, high-fidelity corpus\nfor both Supervised Fine-Tuning (SFT) and subsequent policy alignment. The overall process is\nillustrated in Figure 3.\n3.1\nSFT Data for Rewriter Initialization\nWe construct the SFT dataset through a four-stage process: (1) simulating realistic user prompts\nfrom a large image pool; (2) generating Chain-of-Thought (CoT) and multiple reprompt candidates\nusing a powerful large language model; (3) selecting the optimal reprompt via human-in-the-loop\nevaluation; and (4) performing automated quality filtering. This pipeline yields a final dataset of\n485,119 high-quality instances, each comprising a (user prompt, CoT, reprompt) triplet.\nBased on fig. 4, the dataset exhibits a thematic distribution across several creative domains. Design\nconstitutes the largest portion at 27%, followed by Art at 23%, and Film & Story at 22%. Illustration\naccounts for 18%, and Creative tasks make up 10% of the dataset.\n1. User Prompt Simulation. To emulate concise user queries at scale, we begin with a diverse pool\nof 3.26 million images (1.53M Chinese-centric and 1.73M English-centric). We employ an image\ncaptioning model to generate short, naturalistic descriptions for these images. This step results in a\ncollection of 2.26 million proxy user prompts that serve as the foundation for our data generation.\n2. CoT and Reprompt Generation. For each simulated user prompt, we leverage the generative\npower of Gemini-2.5-Pro (Comanici et al., 2025). Guided by the system prompts detailed in the\nAppendix, the model produces a detailed CoT reasoning sequence and multiple enriched re-\nprompt candidates. This one-to-many generation strategy is crucial for exploring diverse rewriting\npossibilities, from which we can later select the optimal one.\n3. Automated Filtering. The initial stage involves a rigorous automated filtering process aimed at\nenhancing data quality. We utilize Gemini-2.5-Pro to programmatically detect and discard samples\nwith issues such as semantic deviation, information loss, or linguistic incoherence. This process\nreduces the dataset from 1 million instances to 611,921 triplets.\n4. Human-in-the-Loop Selection. To further ground our dataset in perceptual quality, we introduce\na critical human evaluation step. For each set of generated reprompt candidates, we use the\nHunyuan Text-to-Image model to synthesize corresponding images. Professional annotators then\nevaluate these images, selecting the reprompt that best aligns with the user’s intent and produces\nthe highest-quality visual output. This human-in-the-loop process results in a final curated set of\n485,119 triplets.\n5\n\nDesign\n27%\nArt\n23%\nFilm & Story\n22%\nCreative\n10%\nIllustration\n18%\nPrimary Categories\n3%\n4%\n4%\n3%\n2%\n4%\n3%\n3%\n1%\n8%\n4%\n10%\n1%\n10%\n8%\n3%\n1%\n7%\n3%\n14%\n4%\nSecondary Categories\nDetailed Breakdown\nDesign (27%)\n  \n IP Design (3%)\n  \n Logo/Icon Design (4%)\n  \n Poster Design (4%)\n  \n Space Design (3%)\n  \n Software UI Design (2%)\n  \n Advertising/E-commerce Design (4%)\n  \n Fashion Design (3%)\n  \n Game Design (3%)\n  \n Design Materials (1%)\nArt (23%)\n  \n Graphic Art (8%)\n  \n 3D Art (4%)\n  \n Photography (10%)\n  \n Others (1%)\nFilm & Story (22%)\n  \n Realistic (10%)\n  \n Sci-fi (8%)\n  \n Animation (3%)\n  \n Others (1%)\nCreative (10%)\n  \n Imagination (7%)\n  \n Others (3%)\nIllustration (18%)\n  \n Content Illustration (14%)\n  \n Copy Illustration (4%)\nFigure 4: Distribution of Categories in the Dataset. The chart on the left shows the primary\ncategories, while the chart on the right provides a detailed breakdown into 20 sub-categories.\nFigure 5: Distribution of evaluation dimensions in our dataset. (a) The detailed percentage of each\nof the 24 fine-grained KeyPoints, sorted in descending order. (b) The aggregated percentage for\neach of the six main Super-Categories, calculated by summing the percentages of their constituent\nKeyPoints. In both charts, colors represent the Super-Category, visually linking the detailed points\nto their broader classification.\n3.2\nPrompt Set for Policy Alignment\nFor the reinforcement learning stage, we require a distinct set of prompts to drive the GRPO policy\noptimization. We construct a set of approximately 50,000 prompts using the same simulation\nmethod as the SFT data but sourced from a disjoint set of images.\nData Segregation and Purpose. Crucially, this prompt set is kept entirely separate from the SFT\ndataset to prevent data leakage and ensure the model learns to generalize its rewriting capabilities.\nUnlike SFT data, these prompts do not have ground-truth CoTs or reprompts; the learning signal\nis provided dynamically by the AlignEvaluator during online training. The thematic distribution\nof this RL prompt set mirrors that of the SFT data to maintain a consistent optimization landscape.\n4\nExperiments\nIn this section, we conduct a comprehensive set of experiments to validate the efficacy of our\nproposed PromptEnhancer framework. Our evaluation is designed to demonstrate that PromptEn-\nhancer significantly improves a T2I model’s ability to follow complex prompts, and that these\nimprovements are both quantitatively measurable and perceptually preferred by humans.\n6\n\nTable 1: A Multi-dimensional Evaluation Framework for Text-to-Image Generation. TIC = Text-\nImage Consistency, SI = Structural Integrity, TIC&SI = both.\nSuper-Category\nCategory\nKey Point\nExample\nCriteria\nLinguistic\nCompre-\nhension\nLogical Ops\nNegation – interpret negatives\nPrompt: A bowl of beef noo-\ndles, no scallions.\n(No scal-\nlions)\nTIC\nLogical Ops\nAttribute Consistency – one attribute\nbound to many\nPrompt: Five people all\nwearing red clothes. (All red)\nTIC\nCo-reference\nPronoun Resolution – resolve ambiguity\nPrompt: The large ball broke\nthe table because it was made\nof metal. (“it” = ball)\nTIC\nVisual Attributes\nObj-level\nCounting – numeracy (n ≥3)\nPrompt: A picture with four\ndogs. (Four dogs)\nTIC\nObj-level\nSize – relative comparison\nPrompt: Two large spheres.\n(Large spheres)\nTIC\nObj-level\nMaterial – render different materials\nPrompt: An ice sculpture of\nan eagle. (Ice sculpture)\nTIC\nObj-level\nExpression – capture facial emotions\nPrompt: A strong man,\nlow-angle shot, with a\ncontemptuous expression.\nTIC\nGlobal Style\nArtistic Style – adhere to style\nPrompt: Eight galloping\nhorses in Chinese ink wash.\nTIC\nAction & Interaction\nIndividual Action\nFull-body Action – complex movement\nPrompt: A girl performing a\nThomas flare.\nTIC&SI\nIndividual Action\nHand Action – detailed hand/finger structurePrompt: A hand using\nchopsticks to pick up food.\nTIC&SI\nIndividual Action\nAnimal Action – actions performed by animalsPrompt: A puppy happily\nrunning.\nTIC&SI\nInteraction\nContact Interaction – physical interaction\nPrompt: A boxer lands a\npunch on a punching bag.\nTIC&SI\nInteraction\nInteraction w/o Contact – non-physical interaction\nPrompt: Einstein looking at\nHawking.\nTIC\nState\nState – continuous state of being or action\nPrompt: A gust of wind blows,\ncherry blossoms dance in the\nair.\nTIC\nRelations & Struc-\nture\nSemantic Rel.\nComparative Relation – attribute compari-\nson\nPrompt: Woman in red dress\ntaller than woman in yellow.\nTIC\nSemantic Rel.\nCompositional Relation – entity composed\nof others\nPrompt: A cat made of orange\nslices.\nTIC\nSemantic Rel.\nContainment Relation – container holds an\nentity\nPrompt: A cup full of soda\nwater.\nTIC\nSemantic Rel.\nSimilarity Relation – resemblance in shape\nPrompt: A lake shaped like a\nguitar.\nTIC\nSpatial Layout\nCross-Entity Binding – distinct attributes\nto entities\nPrompt: Man (buzz cut, blue\nshirt) and woman (long hair,\nyellow shirt).\nTIC\nSpatial Layout\nEntity Layout – specific arrangement of en-\ntities\nPrompt: A race car on a city\ntrack, with a mini-map in the\ntop-left corner.\nTIC\nWorld Knowledge &\nReasoning\nWorld\nKnowledge\nKnowledge Application –\nfamous entities\nPrompt: The Great Wall of\nChina / Marie Curie.\nTIC\nAbstract\nReasoning\nCounterfactual – surreal impossible scenes\nPrompt: A girl held onto the\nstem of a huge dandelion with\nboth hands, suspended above\nthe clouds.\nTIC\nScene Text & Typog-\nraphy\nIn-Image Text\nText Rendering –\nrender text content accurately\nPrompt: Poster with text\n“Game of Thrones” at the bottom.TIC\nIn-Image Text\nText Layout – position text as instructed\nPrompt: Poster of a woman on\na throne of waves, text ”Game\nof Thrones” at the bottom.\nTIC\n4.1\nExperimental Setup\nAll experiments are conducted using the HunyuanImage 2.1 as the base Text-to-Image model.\nIts weights remain frozen throughout our training process to demonstrate the plug-and-play\ncapability of PromptEnhancer. Our CoT Rewriter is initialized from the Hunyuan-7B-Instruct\nmodel (Tencent-Hunyuan-Team, 2025). All training and inference are performed on 8 NVIDIA\nH800 GPUs.\nEvaluation Benchmark. To rigorously assess performance, all methods are evaluated on our newly\ncurated benchmark. This benchmark features challenging prompts specifically designed to test the\nwide range of fine-grained alignment capabilities detailed in Section 2.4 and Table 1.\n4.2\nImplementation Details\nWe detail the training hyperparameters for the two stages of our framework in Table 2.\nStage 1: Supervised Fine-Tuning (SFT). The CoT Rewriter is fine-tuned on our curated SFT dataset\n7\n\n一只可爱的汤姆穿着宇航服在太空中漂浮漫游。汤姆的身体主要由大块的白色和灰色油彩构成，呈现出圆润而生动的轮廓。背景是深蓝色与黑色的混合色调，呈现出厚涂技法，上面点缀着\n多个由白色和黄色颜料点彩而成的星体。油画风格，笔触厚重，纹理感强，颜料堆叠明显。\n《猫和老鼠》中的汤姆猫，穿着宇航服在太空中漂浮漫游。汤姆的身体主要由大块的白色和灰色油彩构成，呈现出圆润而生动的轮廓，面部表情可爱。他身穿一套多层的宇航服，服装由米\n白色和浅灰色的颜料堆叠而成，呈现出强烈的纹理感，头部戴着一个不透明的、带有黄色高光的玻璃头盔。背景是深蓝色与黑色混合调成的，同样采用了厚涂技法，上面点缀着多个由白色\n和黄色颜料点彩而成的星体，这些星体呈现为大小不一的圆形色点。油画风格，笔触厚重，纹理感强，颜料堆叠明显。\nPromptEnhancer\nRaw Prompt\nRaw Prompt\nPromptEnhancer\n一只可爱的labubu穿着宇航服在太空中漂浮漫游。油画风格，笔触厚重，纹理感强，颜料堆叠明显。\nLabubu，一个有着长长的兔子般的耳朵和布满锯齿状牙齿的顽皮笑容的怪物角色，穿着一套白色的宇航服，在浩瀚的太空中漂浮漫游。在它周围是深邃的宇宙背景，由大面积的蓝色、绿\n色和黄色颜料混合而成，形成了不规则的、充满动感的色块，颜料堆叠明显，形成了凸起的纹理。背景中还点缀着一些由明亮的黄色和白色油彩点状笔触组成的星星。表现主义油画风格，\n笔触厚重，颜料堆叠明显，刮刀纹理和画布肌理感强烈。\nPromptEnhancer\nRaw Prompt\nRaw Prompt\nPromptEnhancer\nFigure 6: Qualitative Comparison of Prompt Rewriting. This figure demonstrates the effective-\nness of the PromptEnhancer prompt rewriter. Each comparison pair shows an image generated\nfrom a simple ”Raw Prompt” alongside an image generated from the detailed prompt created by\nPromptEnhancer. As illustrated, the enriched prompts, which add specific details like character\nidentity (“Tom cat from Tom & Jerry”) and artistic style (“oil painting style, heavy brushstrokes”),\nguide the model to produce images with significantly greater detail, stylistic accuracy, and fidelity\nto the user’s intent.\nfor 10 epochs. We use a learning rate of 1.0 × 10−5 with a cosine learning rate scheduler and a\nwarmup ratio of 10%. The effective batch size is 128, achieved with a per-device batch size of 8 and\n2 gradient accumulation steps across 8 GPUs. We leverage bfloat16 mixed-precision training to\nimprove efficiency.\nStage 2: Policy Alignment with GRPO. Following SFT, the rewriter is further optimized using\nthe GRPO algorithm for 10 epochs. We lower the learning rate to 1.0 × 10−6 for stable policy\n8\n\nFigure 7: Quantitative Evaluation of PromptEnhancer’s Impact on Prompt Following Accuracy.\nThe figure presents a comparative analysis of text-to-image generation accuracy with and without\nthe PromptEnhancer framework across 24 distinct semantic categories. The left panel illustrates\nthe percentage point (pp) improvement for each category, highlighting significant gains (blue) in\nareas like grammatical understanding and compositional reasoning, as well as regressions (red) in\nothers. The right panel provides a direct comparison of the absolute accuracy scores, showing the\nperformance of the baseline model (“w/o Ours”) versus the enhanced model (“w/ Our”).\nTable 2: Key hyperparameters for the SFT and GRPO training stages.\nHyperparameter\nSFT Stage\nGRPO Stage\nBase Model\nHunyuan-7B-Instruct\nSFT-tuned Rewriter\nLearning Rate\n1.0 × 10−5\n1.0 × 10−6\nLR Scheduler\nCosine\nConstant\nWarmup Ratio\n0.1\nN/A\nEpochs\n2\n1\nEffective Batch Size\n128\n64\nPrecision\nbfloat16\nbfloat16\nRollout Samples (N)\nN/A\n8\nKL Coefficient\nN/A\n0.001\nrefinement. During the rollout phase for each prompt, we generate N = 8 candidate reprompts\nto estimate the policy gradient. A KL-divergence penalty with a coefficient of 0.001 is applied to\nregularize the policy updates and prevent deviation from the SFT-initialized model. The global\ntraining batch size is 64.\n4.3\nQuantitative Analysis\nAs illustrated in Figure 7, our framework provides a significant and broad-spectrum enhancement\nto the T2I model’s prompt-following capabilities. On average, PromptEnhancer boosts the ac-\ncuracy across all 24 evaluation dimensions by a substantial +5.1%. This overall improvement is\nunderpinned by widespread gains, with performance increasing in 20 out of 24 categories.\nSignificant Gains in Complex Reasoning. The most pronounced improvements are concentrated\nin categories that demand sophisticated semantic and compositional understanding. Notably, we\nobserve dramatic boosts in Similarity Relation (+17.3%), Counterfactual reasoning (+17.2%), and\nCounting (+15.0%). These results underscore our method’s ability to help the T2I model interpret\nabstract relationships and precise numerical constraints. Furthermore, significant enhancements\nare seen in fine-grained linguistic and visual challenges, including Pronoun Resolution (+13.9%),\nrendering nuanced facial Expression (+12.0%), and maintaining Cross-Entity Binding (+11.3%).\nIn total, 15 distinct categories exhibit an accuracy improvement of more than 5.0%, demonstrating\nthe comprehensive impact of our prompt rewriting strategy.\n9\n\nYou are an expert in writing prompts for image generation. I will give you a sentence, and you are to expand this \nsentence into a detailed caption for generating an image. And the captions must follow the rules listed below.\n### **I. Sentence Structures**\nThe captions follow a consistent, hierarchical structure that moves from a general overview to specific details.\n1. **The Opening Statement: General Overview**\n2. **The Body: Systematic and Spatially Organized Description**\n3. **Hierarchical Object Description: From Whole to Parts**\n4. **The Concluding Statement: Stylistic Identification**\n---\n### **II. Grammatical Rules**\nThe grammar is precise, descriptive, and maintains an objective tone.\n1. **Tense: Consistent Present Tense**\n2. **Voice: Mix of Active and Passive**\n3. **Prepositional Phrases for Precision**\n4. **Participial Phrases for Efficient Detail**\n5. **Rich and Specific Adjectives**\n6. **Precision and Hedging Language**\n7. **Complex and Compound Sentences**\nKey constraints:\n1. Only provide the final captions, do not use markdown format.\n2. The expanded captions must follow the rules listed above.\n3. The expanded captions should adhere to the original sentence, especially the subject and the subject’s attributes, \nincluding color, size, spatial relationships, etc.\n4. You can use your world knowledege to expand some professional terminology to proper explainations that suitable for \nimage generation models.\n5. If the style of original sentence is not mentioned, you should assume it is a photography style. And you can infer the \nstyle from the content of the sentence if the photography style is not suitable.\n6. Descripe the scence or subject directly, do not use \"The image\", \"The composition\", \"The scene\" and similar words in \nthe beginning of the captions.\n7. Unless the original sentence specifies that is is a photo, do not assume that the given sentence is a photo, just describe \nthe scene or subject directly.\n8. If the original sentence has a IP subject, you should keep the IP subject in the expanded captions, and describe the \nbackground of the IP in the expanded captions.\n9. If the original sentence has a text that need to be rendered, you should keep the text in the expanded captions, and \nformat text as \"rendered text\".\nNext, I will give you my sentence. Please provide the expanded captions:\nSystem Prompt for Reprompt Generation\nFigure 8: The system prompt designed to guide Gemini-2.5-Pro for “Reprompt Generation”.\nThis prompt expands a simple input sentence into a structured, detailed description suitable for\nimage generation, following a sophisticated framework. This framework consists of three core\nparts: I. Sentence Structures (defining a four-level descriptive hierarchy from macro to micro), II.\nGrammatical Rules (specifying seven conventions to ensure objectivity and precision), and 9 key\nconstraints, which together guarantee the quality and consistency of the generated content.\nAreas of Neutrality and Minor Regression. While the impact is overwhelmingly positive, the\ngains are not uniform. Performance in Contact Interaction and Size remains unchanged (+0.0%),\nand Artistic Style shows only a marginal improvement (+0.9%), suggesting the baseline model is\nalready relatively competent in these areas. We identify minor performance regressions in two\ncategories: Text Layout (-0.7%) and Interaction w/o Contact (-0.9%). These isolated cases represent\nvaluable areas for future work, indicating that for certain simple concepts, the rewriting process\nmight occasionally introduce ambiguity or over-specification. Nevertheless, these minor trade-offs\nare far outweighed by the extensive and significant improvements across the vast majority of\nchallenging categories.\n10\n\nYou have the following information:\n1. The user's input prompt for text-to-image generation: [{0}].\n2. Based on the user's input prompt, refer to the new prompt: [{1}].\nYour task is not to output the final answer or image. Instead, output the thought process or reasoning chain explaining \nhow you derive the new prompt from the user's input prompt. You must:\n- Generate a \"thinking\" or reasoning chain process to explain how you arrive at the new prompt based on the user's input \nprompt.\n- The new prompt guides the entire thought direction, but no information from the new prompt should be leaked in the \nthought process.\n- Avoid introducing any content/elements/props/text/watermarks unrelated to the \"input prompt\" (e.g., if the input prompt \ndoes not mention text/watermarks, the thought process must not include such content).\n- Do not provide excessive explanations; the output length must be less than **384 tokens**.\nBelow is an example output. Pay special attention to core elements, composition and relative relationships \n(position/comparison/inclusion/structure/similarity, etc.), attributes (size/quantity/material/expression, etc.), actions (full-\nbody actions/partial actions/entity contact/action state, etc.), grammar (negations/pronouns, etc.), style \n(sketch/watercolor/game/realistic, etc.), logical relationships, potential user intent, relevant background, and world \nknowledge reasoning, as well as how you form the answer. The output length must be less than **384 tokens**.\n## Example Output\nThe user wants to generate an image with the following core elements: Person: young woman; Clothing: brown hoodie; \nAccessories: ski goggles; Props: red snowboard. The action is left hand on hip, style is realistic, background is the capital \nof China and the national flower of China. The main element is a young woman, attributes include single person, East \nAsian young woman, approximately 20 years old, with long brown wavy hair, smiling at the camera; the young woman's \naction is left hand on hip, right hand holding a snowboard. Secondary elements include the snowboard, which is bright \nred, single in quantity, located on the left side of the image; key details include the woman wearing a black knit hat, \npink-framed ski goggles pushed up on the hat, and a loose brown hoodie. Composition and relative relationships: the \nperson is centered, facing the camera, right hand holding the snowboard; the user's grammar description emphasizes the \npresence of negation, background has no trees; relevant reasoning knowledge: the capital of China is Beijing, the \nnational flower of China is the peony; the image background is the palace and peony flowers, style is photography, \nrealistic style.\nSystem Prompt for CoT Generation\nFigure 9: The system prompt for generating a Chain-of-Thought. The core task is not to output\na final answer, but to generate a reasoning process that explains how the system derives a new,\noptimized prompt ([{1}]) from the user’s initial input ([{0}]). The prompt requires the model to\nfocus on a series of analytical dimensions (e.g., core elements, composition, attributes, style, world\nknowledge) and specifies the required format and depth through a detailed “Example Output”.\n5\nRelated Work\n5.1\nText-to-Image Generation\nThe field of Text-to-Image (T2I) synthesis has witnessed a paradigm shift with the advent of diffu-\nsion models (Ho et al., 2020; Song et al., 2021), which have largely superseded earlier GAN-based\napproaches (Reed et al., 2016). Seminal models like DALL-E (Ramesh et al., 2021), Imagen (Sa-\nharia et al., 2022), and Stable Diffusion (Rombach et al., 2022; Podell et al., 2024; Esser et al.,\n2024) established a powerful foundation for generating high-fidelity and diverse images. This\nfrontier continues to be pushed by state-of-the-art systems such as DALL-E 3 (Betker et al., 2023),\nand more recent architectures like FLUX.1 (Labs et al., 2025), which have dramatically improved\nphotorealism and text rendering.\nHowever, despite this progress, a fundamental challenge persists: the prompt alignment gap.\nModels often struggle with compositional complexity, failing to correctly bind attributes to objects,\ncomprehend spatial or logical relationships (e.g., negation), and maintain fidelity to intricate\ndetails (Huang et al., 2023). This gap places a significant “alignment tax” on users, forcing them to\nengage in tedious and often unintuitive prompt engineering. Our work directly targets this gap,\nnot by inventing a new generator, but by creating a universal interface that teaches any T2I model\nto better understand human intent.\n5.2\nPrompt Rewriting and Optimization\nTo bridge the user-model communication gap, researchers have increasingly focused on prompt\noptimization. One foundational strategy is recaptioning, where high-quality, descriptive captions\n11\n\nare synthesized to fine-tune the T2I model itself, as famously implemented in DALL-E 3 (Betker\net al., 2023).\nA more direct approach is on-the-fly prompt rewriting, where user prompts are enhanced before\nbeing passed to a frozen T2I model. These methods include: (1) Iterative Refinement, where an LLM\ncritiques and revises a prompt in a loop to improve the generated output (Wu et al., 2024; Yang\net al., 2024b); (2) Automatic Prompt Engineering, which learns to generate or select stylistically potent\nor compositionally robust prompts (Cao et al., 2023; Ma˜nas et al., 2024); and (3) Agent-based Systems,\nwhich employ multimodal LLM agents to plan, generate, and edit in complex workflows (Wang\net al., 2024; Qin et al., 2024).\nWhile powerful, these methods often rely on general-purpose LLMs (e.g., GPT-4 (Hurst et al., 2024))\nthat lack a specialized understanding of T2I-specific failure modes. Our approach charts a new\npath by training a dedicated rewriter that is explicitly optimized against a fine-grained, T2I-centric\nreward model, enabling it to learn the nuances of visual generation.\n5.3\nChain-of-Thought for Controllable Generation\nChain-of-Thought (CoT) reasoning (Wei et al., 2022), which decomposes complex problems into\nintermediate steps, has proven highly effective for improving controllability in multimodal genera-\ntion. Instead of asking a model to synthesize a complex scene in one shot, CoT-inspired methods\nimpose a structured, step-wise process. For instance, some frameworks leverage a powerful\nmultimodal LLM to first generate an explicit plan—such as a scene layout, object coordinates, or a\nsequence of rendering steps—which then guides a constrained diffusion process to enhance com-\npositional accuracy (Yang et al., 2024a; Zhang et al., 2025). Other approaches integrate step-wise\nreasoning directly into the generative model’s architecture (Wang et al., 2025).\nThese methods have significantly advanced compositional fidelity. However, they often create\na tight coupling between the reasoner and a specific T2I model architecture. Our framework\ninnovates by leveraging CoT purely within the prompt rewriting process. We create a decoupled\nCoT rewriter that is model-agnostic and trained via reinforcement learning. This allows it to\nbe universally applied as a plug-and-play module to enhance existing, pre-trained T2I models\nwithout any architectural modifications.\n5.4\nFine-Grained Evaluation and Reward Modeling\nThe maturation of T2I models has necessitated a parallel evolution in evaluation methodologies,\nmoving beyond holistic metrics like FID or CLIP Score. A major advancement was the development\nof reward models trained on large-scale human preferences, such as ImageReward (Xu et al., 2023),\nHPSv2 (Wu et al., 2023b;a), which capture a more human-aligned sense of aesthetic quality and\noverall prompt fidelity.\nHowever, a single preference score can obscure critical, fine-grained errors in compositionality.\nThis has spurred the creation of specialized benchmarks with detailed, multi-faceted annotations,\nsuch as T2I-CompBench (Huang et al., 2023) and EvalMuse (Han et al., 2024), which probe specific\ncapabilities like attribute binding and spatial relations.\nOur work makes a key contribution by closing the loop between fine-grained evaluation and\nmodel optimization. We first introduce the ‘AlignEvaluator‘, a reward model trained on a new,\ncomprehensive taxonomy of 24 key points that synthesizes and expands upon existing criteria.\nCrucially, we then use this detailed, multi-faceted reward signal to directly optimize our prompt\nrewriter via reinforcement learning. This transforms fine-grained evaluation from a passive\nmeasurement tool into an active training signal, directly teaching the rewriter to avoid common\nT2I failure modes.\n6\nConclusion\nIn this paper, we introduced PromptEnhancer, a novel framework to help text-to-image (T2I)\nmodels better understand complex user prompts. T2I models often struggle to follow detailed\ninstructions, leading to images that don’t match the user’s intent. Our method fixes this by\nautomatically rewriting the user’s initial prompt into a more detailed one that any T2I model\ncan easily interpret. Our key innovation is a prompt rewriter that uses a Chain-of-Thought\n12\n\n(CoT) process. We train this rewriter using reinforcement learning, guided by a custom reward\nmodel we call the AlignEvaluator. This evaluator provides specific, fine-grained feedback on\n24 different aspects of image-text alignment, allowing the rewriter to learn how to create high-\nquality prompts. Crucially, our framework is universal and works with any pre-trained T2I model\nwithout needing to modify it. Experiments show that PromptEnhancer significantly improves the\nalignment between the generated image and the user’s prompt across a wide range of challenges.\nBy decoupling the task of prompt enhancement from image generation, our work offers an effective\nand scalable solution to improve the control and accuracy of T2I systems.\n13\n\n7\nContributors and Acknowledgements\n• Project leaders: Qinglin Lu*, Chunyu Wang†\n• Core Contributors: Linqing Wang, Ximing Xing\n• Contributors: Yiji Cheng, Zhiyuan Zhao, Jiale Tao, Qixun Wang, Ruihuang Li, Comi Chen,\nXin Li, Mingrui Wu, Xinchi Deng\n• Acknowledgements: We would like to thank Kai Song, Zhengkai Jiang for their valuable\ninputs and suggestions.\n† Project Lead: Chunyu Wang\n* Corresponding Author: Qinglin Lu\n14\n\nReferences\nJames Betker, Gabriel Goh, Li Jing, Tim Brooks, Jianfeng Wang, Linjie Li, Long Ouyang, Juntang\nZhuang, Joyce Lee, Yufei Guo, et al. Improving image generation with better captions. Computer\nScience. https://cdn. openai. com/papers/dall-e-3. pdf, 2(3):8, 2023.\nTingfeng Cao, Chengyu Wang, Bingyan Liu, Ziheng Wu, Jinhui Zhu, and Jun Huang. Beauti-\nfulprompt: Towards automatic prompt engineering for text-to-image synthesis. arXiv preprint\narXiv:2311.06752, 2023.\nGheorghe Comanici, Eric Bieber, Mike Schaekermann, Ice Pasupat, Noveen Sachdeva, Inderjit\nDhillon, Marcel Blistein, Ori Ram, Dan Zhang, Evan Rosen, et al. Gemini 2.5: Pushing the\nfrontier with advanced reasoning, multimodality, long context, and next generation agentic\ncapabilities. arXiv preprint arXiv:2507.06261, 2025.\nPatrick Esser, Sumith Kulal, Andreas Blattmann, Rahim Entezari, Jonas M¨uller, Harry Saini, Yam\nLevi, Dominik Lorenz, Axel Sauer, Frederic Boesel, et al. Scaling rectified flow transformers for\nhigh-resolution image synthesis. In Forty-first international conference on machine learning (ICML),\n2024.\nShuhao Han, Haotian Fan, Jiachen Fu, Liang Li, Tao Li, Junhui Cui, Yunqiu Wang, Yang Tai,\nJingwei Sun, Chunle Guo, et al. Evalmuse-40k: A reliable and fine-grained benchmark with\ncomprehensive human annotations for text-to-image generation model evaluation. arXiv preprint\narXiv:2412.18150, 2024.\nYaru Hao, Zewen Chi, Li Dong, and Furu Wei. Optimizing prompts for text-to-image generation.\nAdvances in Neural Information Processing Systems, 36:66923–66939, 2023.\nJonathan Ho, Ajay Jain, and Pieter Abbeel. Denoising diffusion probabilistic models. Advances in\nneural information processing systems (NeurIPS), 33:6840–6851, 2020.\nKaiyi Huang, Kaiyue Sun, Enze Xie, Zhenguo Li, and Xihui Liu. T2i-compbench: A comprehen-\nsive benchmark for open-world compositional text-to-image generation. Advances in Neural\nInformation Processing Systems (NeurIPS), 36:78723–78747, 2023.\nAaron Hurst, Adam Lerer, Adam P Goucher, Adam Perelman, Aditya Ramesh, Aidan Clark,\nAJ Ostrow, Akila Welihinda, Alan Hayes, Alec Radford, et al. Gpt-4o system card. arXiv preprint\narXiv:2410.21276, 2024.\nBlack Forest Labs. Flux, 2024. URL https://github.com/black-forest-labs/flux.\nBlack Forest Labs, Stephen Batifol, Andreas Blattmann, Frederic Boesel, Saksham Consul, Cyril Di-\nagne, Tim Dockhorn, Jack English, Zion English, Patrick Esser, et al. Flux. 1 kontext: Flow match-\ning for in-context image generation and editing in latent space. arXiv preprint arXiv:2506.15742,\n2025.\nZhimin Li, Jianwei Zhang, Qin Lin, Jiangfeng Xiong, Yanxin Long, Xinchi Deng, Yingfang Zhang,\nXingchao Liu, Minbin Huang, Zedong Xiao, et al. Hunyuan-dit: A powerful multi-resolution\ndiffusion transformer with fine-grained chinese understanding. arXiv preprint arXiv:2405.08748,\n2024.\nAixin Liu, Bei Feng, Bing Xue, Bingxuan Wang, Bochao Wu, Chengda Lu, Chenggang Zhao,\nChengqi Deng, Chenyu Zhang, Chong Ruan, et al. Deepseek-v3 technical report. arXiv preprint\narXiv:2412.19437, 2024.\nOscar Ma˜nas, Pietro Astolfi, Melissa Hall, Candace Ross, Jack Urbanek, Adina Williams, Aishwarya\nAgrawal, Adriana Romero-Soriano, and Michal Drozdzal. Improving text-to-image consistency\nvia automatic prompt optimization. arXiv preprint arXiv:2403.17804, 2024.\nWilliam Peebles and Saining Xie. Scalable diffusion models with transformers. In Proceedings of the\nIEEE/CVF international conference on computer vision (ICCV), pp. 4195–4205, 2023.\nDustin Podell, Zion English, Kyle Lacey, Andreas Blattmann, Tim Dockhorn, Jonas M¨uller, Joe\nPenna, and Robin Rombach. SDXL: Improving latent diffusion models for high-resolution image\nsynthesis. In The Twelfth International Conference on Learning Representations (ICLR), 2024. URL\nhttps://openreview.net/forum?id=di52zR8xgf.\n15\n\nJie Qin, Jie Wu, Weifeng Chen, Yuxi Ren, Huixia Li, Hefeng Wu, Xuefeng Xiao, Rui Wang, and Shilei\nWen. Diffusiongpt: Llm-driven text-to-image generation system. arXiv preprint arXiv:2401.10061,\n2024.\nAlec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh, Sandhini Agarwal,\nGirish Sastry, Amanda Askell, Pamela Mishkin, Jack Clark, et al. Learning transferable visual\nmodels from natural language supervision. In International conference on machine learning (ICML),\npp. 8748–8763. PmLR, 2021.\nAditya Ramesh, Mikhail Pavlov, Gabriel Goh, Scott Gray, Chelsea Voss, Alec Radford, Mark Chen,\nand Ilya Sutskever. Zero-shot text-to-image generation. In International conference on machine\nlearning (ICML), pp. 8821–8831. Pmlr, 2021.\nScott Reed, Zeynep Akata, Xinchen Yan, Lajanugen Logeswaran, Bernt Schiele, and Honglak Lee.\nGenerative adversarial text to image synthesis. In International conference on machine learning\n(ICML), pp. 1060–1069. Pmlr, 2016.\nRobin Rombach, Andreas Blattmann, Dominik Lorenz, Patrick Esser, and Bj¨orn Ommer. High-\nresolution image synthesis with latent diffusion models. In Proceedings of the IEEE/CVF conference\non computer vision and pattern recognition (CVPR), pp. 10684–10695, 2022.\nChitwan Saharia, William Chan, Saurabh Saxena, Lala Li, Jay Whang, Emily L Denton, Kamyar\nGhasemipour, Raphael Gontijo Lopes, Burcu Karagol Ayan, Tim Salimans, et al. Photorealistic\ntext-to-image diffusion models with deep language understanding. Advances in neural information\nprocessing systems (NeurIPS), 35:36479–36494, 2022.\nZhihong Shao, Peiyi Wang, Qihao Zhu, Runxin Xu, Junxiao Song, Xiao Bi, Haowei Zhang,\nMingchuan Zhang, YK Li, Yang Wu, et al. Deepseekmath: Pushing the limits of mathematical\nreasoning in open language models. arXiv preprint arXiv:2402.03300, 2024.\nYang Song, Jascha Sohl-Dickstein, Diederik P Kingma, Abhishek Kumar, Stefano Ermon, and Ben\nPoole. Score-based generative modeling through stochastic differential equations. In International\nConference on Learning Representations (ICLR), 2021. URL https://openreview.net/forum?id=Px\nTIG12RRHS.\nTencent-Hunyuan-Team. Hunyuan-a13b, 2025. URL https://github.com/Tencent-Hunyuan/Hun\nyuan-A13B.\nYi Wang, Mushui Liu, Wanggui He, Longxiang Zhang, Ziwei Huang, Guanghao Zhang, Fangxun\nShu, Zhong Tao, Dong She, Zhelun Yu, et al. Mint: Multi-modal chain of thought in unified\ngenerative models for enhanced image generation. arXiv preprint arXiv:2503.01298, 2025.\nZhenyu Wang, Aoxue Li, Zhenguo Li, and Xihui Liu. Genartist: Multimodal llm as an agent for\nunified image generation and editing. Advances in Neural Information Processing Systems, 37:\n128374–128395, 2024.\nJason Wei, Xuezhi Wang, Dale Schuurmans, Maarten Bosma, Fei Xia, Ed Chi, Quoc V Le, Denny\nZhou, et al. Chain-of-thought prompting elicits reasoning in large language models. Advances in\nneural information processing systems (NeurIPS), 35:24824–24837, 2022.\nChenfei Wu, Jiahao Li, Jingren Zhou, Junyang Lin, Kaiyuan Gao, Kun Yan, Sheng-ming Yin, Shuai\nBai, Xiao Xu, Yilei Chen, et al. Qwen-image technical report. arXiv preprint arXiv:2508.02324,\n2025a.\nMingrui Wu, Lu Wang, Pu Zhao, Fangkai Yang, Jianjin Zhang, Jianfeng Liu, Yuefeng Zhan, Weihao\nHan, Hao Sun, Jiayi Ji, et al. Reprompt: Reasoning-augmented reprompting for text-to-image\ngeneration via reinforcement learning. arXiv preprint arXiv:2505.17540, 2025b.\nTsung-Han Wu, Long Lian, Joseph E Gonzalez, Boyi Li, and Trevor Darrell. Self-correcting llm-\ncontrolled diffusion models. In Proceedings of the IEEE/CVF Conference on Computer Vision and\nPattern Recognition, pp. 6327–6336, 2024.\nXiaoshi Wu, Yiming Hao, Keqiang Sun, Yixiong Chen, Feng Zhu, Rui Zhao, and Hongsheng Li.\nHuman preference score v2: A solid benchmark for evaluating human preferences of text-to-\nimage synthesis. arXiv preprint arXiv:2306.09341, 2023a.\n16\n\nXiaoshi Wu, Keqiang Sun, Feng Zhu, Rui Zhao, and Hongsheng Li. Human preference score:\nBetter aligning text-to-image models with human preference. In Proceedings of the IEEE/CVF\nInternational Conference on Computer Vision (ICCV), pp. 2096–2105, October 2023b.\nJiazheng Xu, Xiao Liu, Yuchen Wu, Yuxuan Tong, Qinkai Li, Ming Ding, Jie Tang, and Yuxiao\nDong. Imagereward: Learning and evaluating human preferences for text-to-image generation.\nAdvances in Neural Information Processing Systems (NeurIPS), 36:15903–15935, 2023.\nLing Yang, Zhaochen Yu, Chenlin Meng, Minkai Xu, Stefano Ermon, and Bin Cui. Mastering\ntext-to-image diffusion: Recaptioning, planning, and generating with multimodal llms. In\nForty-first International Conference on Machine Learning, 2024a.\nZhengyuan Yang, Jianfeng Wang, Linjie Li, Kevin Lin, Chung-Ching Lin, Zicheng Liu, and Lijuan\nWang. Idea2img: Iterative self-refinement with gpt-4v for automatic image design and generation.\nIn European Conference on Computer Vision, pp. 167–184. Springer, 2024b.\nYuyao Zhang, Jinghao Li, and Yu-Wing Tai. Layercraft: Enhancing text-to-image generation with\ncot reasoning and layered object integration. arXiv preprint arXiv:2504.00010, 2025.\n17\n"
    }
  ]
}