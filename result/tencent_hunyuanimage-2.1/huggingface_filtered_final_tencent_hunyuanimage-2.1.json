{
  "1-1 (Weights)": "The project page states that on ‚ÄúSeptember 8, 2025‚Äù Tencent ‚ÄúüöÄ Released inference code and model weights for HunyuanImage-2.1.‚Äù  The same release note appears again in the repository description, emphasizing that ‚ÄúThis repo contains PyTorch model definitions, pretrained weights and inference/sampling code for our HunyuanImage-2.1.‚Äù  A separate sentence directs users to installation instructions: ‚ÄúThe details of download pretrained models are shown [here](checkpoints-download.md).‚Äù  The concrete filename ‚Äúdit/hunyuanimage2.1.safetensors‚Äù is listed, showing that at least one ready-to-use weight file is provided.  Finally, the metadata string ‚Äúextra_gated_eu_disallowed: true‚Äù indicates that access is explicitly gated for users in the European Union (and possibly other regions), confirming that although the weights are downloadable, geographic restrictions apply.  Taken together, the quotes confirm that official, pretrained weights for HunyuanImage-2.1 are available through the repository, can be downloaded via the linked checkpoint page, and are distributed in PyTorch-compatible *.safetensors format, but that the download portal enforces a gate that refuses requests originating from the EU (and presumably the UK and South Korea per the license, see Section 1-3).",
  "1-1 (Weights)__evidence": [
    {
      "source": "[readme]",
      "quote": "- September 8, 2025: üöÄ Released inference code and model weights for HunyuanImage-2.1."
    },
    {
      "source": "[readme]",
      "quote": "This repo contains PyTorch model definitions, pretrained weights and inference/sampling code for our HunyuanImage-2.1."
    },
    {
      "source": "[readme]",
      "quote": "The details of download pretrained models are shown [here](checkpoints-download.md)."
    },
    {
      "source": "[readme]",
      "quote": "extra_gated_eu_disallowed: true"
    },
    {
      "source": "[files]",
      "quote": "dit/hunyuanimage2.1.safetensors"
    }
  ],
  "1-2 (Code)": "Public code exists, but it is limited to inference.  The exact wording in two separate places is identical: ‚ÄúSeptember 8, 2025: üöÄ Released inference code and model weights for HunyuanImage-2.1,‚Äù and ‚ÄúThis repo contains PyTorch model definitions, pretrained weights and inference/sampling code for our HunyuanImage-2.1.‚Äù  No quote mentions pre-training, fine-tuning, or RL training scripts; all quoted text refers only to model definitions and ‚Äúinference/sampling code.‚Äù  Therefore, only the serving / sampling pipeline is published, while the full training pipeline remains proprietary.",
  "1-2 (Code)__evidence": [
    {
      "source": "[readme]",
      "quote": "- September 8, 2025: üöÄ Released inference code and model weights for HunyuanImage-2.1."
    },
    {
      "source": "[readme]",
      "quote": "This repo contains PyTorch model definitions, pretrained weights and inference/sampling code for our HunyuanImage-2.1."
    }
  ],
  "1-3 (License)": "All quoted license information points to the proprietary ‚ÄúTENCENT HUNYUAN COMMUNITY LICENSE AGREEMENT.‚Äù  The repository metadata lines read ‚Äúlicense_name: tencent-hunyuan-community‚Äù and furnish the canonical link ‚Äúhttps://github.com/Tencent-Hunyuan/HunyuanImage-2.1/blob/master/LICENSE.‚Äù  A block of the license itself is quoted: ‚ÄúWe grant You, for the Territory only, a non-exclusive, non-transferable and royalty-free limited license ‚Ä¶ to use, reproduce, distribute, create derivative works of (including Model Derivatives), and make modifications to the Materials, only in accordance with the terms of this Agreement and the Acceptable Use Policy‚Ä¶ You must not use, reproduce, modify, distribute, or display the Tencent Hunyuan Works, Output or results of the Tencent Hunyuan Works outside the Territory.‚Äù  Further quoted text clarifies territorial scope and exclusions: ‚ÄúTHIS LICENSE AGREEMENT DOES NOT APPLY IN THE EUROPEAN UNION, UNITED KINGDOM AND SOUTH KOREA AND IS EXPRESSLY LIMITED TO THE TERRITORY.‚Äù  The document also grants a limited trademark right: ‚ÄúTencent hereby grants You a license to use ‚ÄòTencent Hunyuan‚Äô (the ‚ÄòMark‚Äô) in the Territory solely as required ‚Ä¶ provided that You comply with any applicable laws related to trademark protection.‚Äù  In summary, the model is distributed under a Tencent-specific community license that (i) is free and non-exclusive, (ii) permits modification, redistribution and derivative works, but (iii) applies only in a defined ‚ÄòTerritory‚Äô excluding the EU, UK, and South Korea, and (iv) prohibits any use or display of the model or its outputs outside that territory.",
  "1-3 (License)__evidence": [
    {
      "source": "[readme]",
      "quote": "license_name: tencent-hunyuan-community"
    },
    {
      "source": "[readme]",
      "quote": "license_link: https://github.com/Tencent-Hunyuan/HunyuanImage-2.1/blob/master/LICENSE"
    },
    {
      "source": "[license_file]",
      "quote": "TENCENT HUNYUAN COMMUNITY LICENSE AGREEMENT"
    },
    {
      "source": "[license_file]",
      "quote": "We grant You, for the Territory only, a non-exclusive, non-transferable and royalty-free limited license under Tencent‚Äôs intellectual property or other rights embodied in or utilized by the Materials to use, reproduce, distribute, create derivative works of (including Model Derivatives), and make modifications to the Materials, only in accordance with the terms of this Agreement and the Acceptable Use Policy, and You must not violate (or encourage or permit anyone else to violate) any term of this Agreement or the Acceptable Use Policy."
    },
    {
      "source": "[license_file]",
      "quote": "You must not use, reproduce, modify, distribute, or display the Tencent Hunyuan Works, Output or results of the Tencent Hunyuan Works outside the Territory."
    },
    {
      "source": "[license_file]",
      "quote": "1&type=Date1\" />\n </picture>\n</a>\n\n\n\n[license_file]\nTENCENT HUNYUAN COMMUNITY LICENSE AGREEMENT\r\nTencent HunyuanImage 2.1 Release Date: September 8, 2025\r\nTHIS LICENSE AGREEMENT DOES NOT APPLY IN THE EUROPEAN UNION, UNITED KINGDOM AND SOUTH KOREA AND IS EXPRESSLY LIMITED TO THE TERRITORY, AS DEFINED BELOW.\r\nBy clicking to agree or by using, reproducing, modifying, distributing, performing or displaying any portion or element"
    },
    {
      "source": "[readme]",
      "quote": "ny of its affiliates, except as required for reasonable and customary use in describing and distributing the Tencent Hunyuan Works. Tencent hereby grants You a license to use ‚ÄúTencent Hunyuan‚Äù (the ‚ÄúMark‚Äù) in the Territory solely as required to comply with the provisions of Section 3(c), provided that You comply with any applicable laws related to trademark protection. All goodwill arising out of Your use o"
    },
    {
      "source": "[files]",
      "quote": "LICENSE file present: LICENSE"
    }
  ],
  "1-4 (Paper)": "The official technical document is titled ‚ÄúHunyuanImage-2.1: An Efficient Diffusion Model for High-Resolution (2K) Text-to-Image Generation.‚Äù  The abstract line announces, ‚ÄúWe present HunyuanImage-2.1, a highly efficient text-to-image model that is capable of generating 2K (2048 √ó 2048) resolution images.‚Äù  The BibTeX header is shown as ‚Äú@misc{HunyuanImage-2.1, ‚Ä¶},‚Äù signaling that the paper is released as a pre-print or technical report rather than a peer-reviewed conference proceeding.  No DOI or conference venue is supplied in the provided quotes, so the current public artifact appears to be a self-hosted technical report or arXiv-style preprint.",
  "1-4 (Paper)__evidence": [
    {
      "source": "[readme]",
      "quote": "# HunyuanImage-2.1: An Efficient Diffusion Model for High-Resolution (2K) Text-to-Image Generation‚Äã"
    },
    {
      "source": "[readme]",
      "quote": "We present HunyuanImage-2.1, a highly efficient text-to-image model that is capable of generating 2K (2048 √ó 2048) resolution images."
    },
    {
      "source": "[readme]",
      "quote": "@misc{HunyuanImage-2.1,"
    }
  ],
  "1-5 (Architecture)": "HunyuanImage-2.1 is described as a two-stage text-to-image system that can produce native 2 K (2048 √ó 2048) resolution pictures.  In the first stage‚Äîthe ‚ÄúBase text-to-image Model‚Äù‚Äîthe system feeds the prompt through two separate text encoders: (1) a multimodal large-language-model (MLLM) encoder that strengthens image‚Äìtext alignment and (2) a multi-language, character-aware encoder that improves rendering of many written languages.  The visual backbone in this stage is explicitly called a ‚Äúsingle- and dual-stream diffusion transformer‚Äù carrying 17 billion parameters.  A variational auto-encoder (VAE) with a 32 √ó compression factor is placed in front of the DiT so that the number of image tokens presented to the diffusion transformer is dramatically reduced.  After an initial image is generated, a second-stage ‚ÄúRefiner Model‚Äù is applied; its sole purpose is to sharpen details, raise perceptual quality, and suppress artifacts that survive the first pass.  All of these components together comprise the HunyuanImage-2.1 architecture.",
  "1-5 (Architecture)__evidence": [
    {
      "source": "[readme]",
      "quote": "We present HunyuanImage-2.1, a highly efficient text-to-image model that is capable of generating 2K (2048 √ó 2048) resolution images."
    },
    {
      "source": "[readme]",
      "quote": "Our architecture consists of two stages:\n1. ‚ÄãBase text-to-image Model:‚Äã‚Äã The first stage is a text-to-image model that utilizes two text encoders: a multimodal large language model (MLLM) to improve image-text alignment, and a multi-language, character-aware encoder to enhance text rendering across various languages. This stage features a single- and dual-stream diffusion transformer with 17 billion parameters.\n2. Refiner Model: The second stage introduces a refiner model that further enhances image quality and clarity, while minimizing artifacts."
    },
    {
      "source": "[readme]",
      "quote": "* Network: A single- and dual-stream diffusion transformer with 17 billion parameters."
    },
    {
      "source": "[readme]",
      "quote": "* A VAE with a 32√ó compression rate drastically reduces the number of input tokens for the DiT model."
    }
  ],
  "1-6 (Tokenizer)": "The model ships with a bespoke tokenizer declared in code as ‚Äúclass HYTokenizer(PreTrainedTokenizer): \\\"\\\"\\\"hunyuan tokenizer.\\\"\\\"\\\"‚Äù.  Its vocabulary is stored in a single file referenced as ‚Äúhy.tiktoken‚Äù via VOCAB_FILES_NAMES = { \"vocab_file\": \"hy.tiktoken\" }.  No other tokenizer implementation or download location is mentioned in the available material.",
  "1-6 (Tokenizer)__evidence": [
    {
      "source": "[py_files/reprompt/tokenization_hy.py]",
      "quote": "class HYTokenizer(PreTrainedTokenizer): \"\"\"hunyuan tokenizer.\"\"\""
    },
    {
      "source": "[py_files/reprompt/tokenization_hy.py]",
      "quote": "VOCAB_FILES_NAMES = {\"vocab_file\": \"hy.tiktoken\"}"
    }
  ],
  "2-1 (Hardware)": "The documentation for HunyuanImage-2.1 lists an NVIDIA GPU with CUDA support as the required accelerator class.  For full-resolution 2048 √ó 2048 image generation, a board providing at least 24 GB of GPU memory is identified as the current minimum requirement.  No additional cluster size, card model (e.g., H100, A100), or multi-GPU configuration details are disclosed in the supplied quotes.",
  "2-1 (Hardware)__evidence": [
    {
      "source": "[readme]",
      "quote": "**Hardware and OS Requirements:**\n- NVIDIA GPU with CUDA support.\n\n  **Minimum requrement for now:** 24 GB GPU memory for 2048x2048 image generation."
    }
  ],
  "2-2 (Software)": "The installation snippet given for HunyuanImage-2.1 centers on a Python/‚Äãpip workflow.  Users are instructed first to ‚Äúpip install -r requirements.txt,‚Äù which implicitly pulls in the model‚Äôs full dependency list, and then to explicitly install FlashAttention version 2.7.3 using ‚Äúpip install flash-attn==2.7.3 --no-build-isolation.‚Äù  No other libraries, frameworks, or training flags are shown, so these two commands constitute the entirety of the revealed software setup.",
  "2-2 (Software)__evidence": [
    {
      "source": "[readme]",
      "quote": "2. Install dependencies:\n```bash\npip install -r requirements.txt\npip install flash-attn==2.7.3 --no-build-isolation\n```"
    }
  ],
  "2-3 (API)": "The only explicit API-related information provided states the list of values that are accepted in the model_name parameter. According to the quote, callers can specify either ‚Äúhunyuanimage-v2.1‚Äù or its variant ‚Äúhunyuanimage-v2.1-distilled,‚Äù confirming that these two identifiers are officially supported for remote inference or generation requests against the HunyuanImage service.",
  "2-3 (API)__evidence": [
    {
      "source": "[readme]",
      "quote": "# Supported model_name: hunyuanimage-v2.1, hunyuanimage-v2.1-distilled"
    }
  ],
  "3-1 (Pre-training)": "HunyuanImage-2.1 is introduced as a highly efficient text-to-image model capable of producing images at 2 K resolution (2048 √ó 2048). Its pre-training leverages an extensive dataset coupled with structured captions that are generated with the assistance of multiple expert models. This combination is reported to substantially improve the alignment between textual prompts and generated images, indicating that careful data curation and multi-model captioning strategies underpin the system‚Äôs overall performance.",
  "3-1 (Pre-training)__evidence": [
    {
      "source": "[readme]",
      "quote": "We present HunyuanImage-2.1, a highly efficient text-to-image model that is capable of generating 2K (2048 √ó 2048) resolution images. Leveraging an extensive dataset and structured captions involving multiple expert models, we significantly enhance text-image alignment capabilities."
    }
  ],
  "3-2 (Fine-tuning)": "",
  "3-2 (Fine-tuning)__evidence": [],
  "3-3 (Reinforcement Learning)": "",
  "3-3 (Reinforcement Learning)__evidence": [],
  "4-1 (Pre-training Data)": "For HunyuanImage-2.1, the authors state that they \"leverage an extensive dataset\" and pair those images with \"structured captions involving multiple expert models.\"  The captions are multi-granular, providing \"hierarchical semantic information at short, medium, long, and extra-long levels.\"  This combination of a very large corpus and richly layered textual descriptions is presented as the reason the model can achieve strong \"text-image alignment capabilities\" and remain \"responsive to complex semantics.\"  No additional numeric breakdowns, licenses, or source lists are disclosed in the available sentences, but the emphasis is clearly on the scale of the corpus and the sophisticated, stratified captioning strategy used during pre-training.",
  "4-1 (Pre-training Data)__evidence": [
    {
      "source": "[readme]",
      "quote": "We present HunyuanImage-2.1, a highly efficient text-to-image model that is capable of generating 2K (2048 √ó 2048) resolution images. Leveraging an extensive dataset and structured captions involving multiple expert models, we significantly enhance text-image alignment capabilities."
    },
    {
      "source": "[readme]",
      "quote": "Structured captions provide hierarchical semantic information at short, medium, long, and extra-long levels, significantly enhancing the model‚Äôs responsiveness to complex semantics."
    }
  ],
  "4-2 (Fine-tuning Data)": "Fine-tuning for HunyuanImage-2.1 is described as a \"Two-Stage Post-Training\" pipeline.  First, a \"Supervised Fine-Tuning (SFT)\" phase adapts the model with curated instruction-style data, and second, a \"Reinforcement Learning (RL)\" phase further refines the system.  The quotes only indicate that these two stages are applied \"sequentially,\" without giving explicit dataset sizes or public availability details.",
  "4-2 (Fine-tuning Data)__evidence": [
    {
      "source": "[readme]",
      "quote": "Two-Stage Post-Training with Reinforcement Learning: Supervised Fine-Tuning (SFT) and Reinforcement Learning (RL) are applied sequentially in two post-training stages."
    }
  ],
  "4-3 (Reinforcement Learning Data)": "During the reinforcement-learning stage, the team \"introduce[s] a Reward Distribution Alignment algorithm.\"  This method \"incorporates high-quality images as selected samples\" so that the reward signal remains reliable, which in turn \"ensure[s] stable and improved reinforcement learning outcomes.\"  The sentences do not reveal where these high-quality images originate or how many are used, but they make clear that the alignment of reward distributions and the curation of exemplary images are central to the RL dataset design.",
  "4-3 (Reinforcement Learning Data)__evidence": [
    {
      "source": "[readme]",
      "quote": "We introduce a Reward Distribution Alignment algorithm, which innovatively incorporates high-quality images as selected samples to ensure stable and improved reinforcement learning outcomes."
    }
  ],
  "4-4 (Data Filtering)": "For data quality control, the authors note that they \"introduce[d] an OCR agent and IP RAG\" to compensate for \"the shortcomings of general VLM captioners in dense text and world knowledge descriptions.\"  They further apply \"a bidirectional verification strategy\" that \"ensures caption accuracy.\"  Although the quote does not list numeric thresholds or removal ratios, it explicitly identifies the tools (OCR agent, IP RAG) and the verification mechanism used at this filtering stage.",
  "4-4 (Data Filtering)__evidence": [
    {
      "source": "[readme]",
      "quote": "Innovatively, an OCR agent and IP RAG are introduced to address the shortcomings of general VLM captioners in dense text and world knowledge descriptions, while a bidirectional verification strategy ensures caption accuracy."
    }
  ],
  "__usage": {
    "fine_tuning": "unknown",
    "rl": "unknown"
  }
}