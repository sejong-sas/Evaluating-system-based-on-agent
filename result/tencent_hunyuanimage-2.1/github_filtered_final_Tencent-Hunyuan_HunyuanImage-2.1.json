{
  "1-1 (Weights)": "The project explicitly states that the GitHub / Hugging Face repository ‚Äúcontains PyTorch model definitions, pretrained weights and inference/sampling code for our HunyuanImage-2.1.‚Äù  On 8 September 2025 the maintainers announced: ‚ÄúüöÄ Released inference code and model weights for HunyuanImage-2.1.‚Äù  Public download is offered through the Hugging Face URL https://huggingface.co/tencent/HunyuanImage-2.1/ and the code shows that local paths are resolved through an environment variable `HUNYUANIMAGE_V2_1_MODEL_ROOT` which defaults to ‚Äú./ckpts‚Äù.  Specific checkpoint files are enumerated, e.g. `dit/hunyuanimage2.1.safetensors`, `dit/hunyuanimage2.1-distilled.safetensors`, and an 8-bit-floating-point variant `dit/hunyuanimage2.1_fp8.safetensors`.  Taken together, the quotes establish that the full set of pretrained weights (standard, distilled, and FP8) are publicly downloadable for anyone who clones the repository or visits the Hugging Face page; no gating or application process is mentioned, implying open, self-service access for inference purposes.",
  "1-1 (Weights)__evidence": [
    {
      "source": "[readme]",
      "quote": "This repo contains PyTorch model definitions, pretrained weights and inference/sampling code for our HunyuanImage-2.1."
    },
    {
      "source": "[readme]",
      "quote": "- September 8, 2025: üöÄ Released inference code and model weights for HunyuanImage-2.1."
    },
    {
      "source": "[py_files/hyimage/diffusion/pipelines/hunyuanimage_pipeline.py]",
      "quote": "Please download from https://huggingface.co/tencent/HunyuanImage-2.1/"
    },
    {
      "source": "py_files/hyimage/models/model_zoo.py",
      "quote": "HUNYUANIMAGE_V2_1_MODEL_ROOT = os.environ.get(\"HUNYUANIMAGE_V2_1_MODEL_ROOT\", \"./ckpts\")"
    },
    {
      "source": "py_files/hyimage/models/model_zoo.py",
      "quote": "load_from=f\"{HUNYUANIMAGE_V2_1_MODEL_ROOT}/dit/hunyuanimage2.1.safetensors\","
    },
    {
      "source": "py_files/hyimage/models/model_zoo.py",
      "quote": "load_from=f\"{HUNYUANIMAGE_V2_1_MODEL_ROOT}/dit/hunyuanimage2.1-distilled.safetensors\","
    },
    {
      "source": "py_files/hyimage/models/model_zoo.py",
      "quote": "fp8_load_from=f\"{HUNYUANIMAGE_V2_1_MODEL_ROOT}/dit/hunyuanimage2.1_fp8.safetensors\","
    }
  ],
  "1-2 (Code)": "The repository release focuses on inference and sampling.  The maintainers declare: ‚ÄúThis repo contains PyTorch model definitions, pretrained weights and inference/sampling code for our HunyuanImage-2.1,‚Äù and the same 8 September 2025 update highlights that only ‚Äúinference code and model weights‚Äù were released.  The codebase provides configuration objects and helper factories rather than end-to-end training scripts: e.g. `Configuration class for HunyuanImage refiner pipeline`, `hunyuanimage_v2_1_cfg = L(HYImageDiffusionTransformer)(...)`, and factory wrappers such as `def HUNYUANIMAGE_V2_1_DIT(**kwargs):`.  Component files are imported from module paths like `hyimage.models.hunyuan.configs.*` and the VAE backbone is referenced through `from .hunyuanimage_vae import HunyuanVAE2D`.  No quote advertises data-processing scripts, optimizer schedules, or full training pipelines, so there is no public coverage of pre-training, fine-tuning, or RL steps.  In short, the publicly released code enables model loading, configuration, and image sampling/refinement but does not expose the original training workflow.",
  "1-2 (Code)__evidence": [
    {
      "source": "[readme]",
      "quote": "This repo contains PyTorch model definitions, pretrained weights and inference/sampling code for our HunyuanImage-2.1."
    },
    {
      "source": "[readme]",
      "quote": "- September 8, 2025: üöÄ Released inference code and model weights for HunyuanImage-2.1."
    },
    {
      "source": "[py_files/hyimage/diffusion/pipelines/hunyuanimage_refiner_pipeline.py]",
      "quote": "Configuration class for HunyuanImage refiner pipeline."
    },
    {
      "source": "[py_files/hyimage/models/hunyuan/configs/hunyuanimage_config.py]",
      "quote": "hunyuanimage_v2_1_cfg = L(HYImageDiffusionTransformer)("
    },
    {
      "source": "py_files/hyimage/models/model_zoo.py",
      "quote": "from hyimage.models.hunyuan.configs.hunyuanimage_config import (hunyuanimage_v2_1_cfg, hunyuanimage_v2_1_distilled_cfg, hunyuanimage_refiner_cfg,)"
    },
    {
      "source": "py_files/hyimage/models/model_zoo.py",
      "quote": "def HUNYUANIMAGE_V2_1_DIT(**kwargs):"
    },
    {
      "source": "[py_files/hyimage/models/vae/__init__.py]",
      "quote": "from .hunyuanimage_vae import HunyuanVAE2D"
    }
  ],
  "1-3 (License)": "The distribution is governed by the ‚ÄúTENCENT HUNYUAN COMMUNITY LICENSE AGREEMENT‚Äù whose header explicitly ties it to ‚ÄúTencent HunyuanImage 2.1‚Äù and the release date of 8 September 2025.  The agreement grants a ‚Äúnon-exclusive, non-transferable and royalty-free limited license‚Äù to ‚Äúuse, reproduce, distribute, create derivative works of (including Model Derivatives), and make modifications to the Materials,‚Äù but conditions all exercise of those rights on compliance with the overall Agreement and its defined Acceptable Use Policy.  A scale-related clause adds that if the licensee‚Äôs products exceed ‚Äú100 million monthly active users in the preceding calendar month‚Äù at the time of the model release, the user ‚Äúmust request a license from Tencent,‚Äù introducing an additional permission step for very large-scale deployments.  No textual limitations such as ‚Äòresearch-only‚Äô, ‚Äònon-commercial‚Äô, or ‚Äòno redistribution‚Äô appear in the provided excerpt; the central restrictions derive from agreement compliance and the special high-MAU trigger.",
  "1-3 (License)__evidence": [
    {
      "source": "[license_files]",
      "quote": "TENCENT HUNYUAN COMMUNITY LICENSE AGREEMENT"
    },
    {
      "source": "[license_files]",
      "quote": "Tencent HunyuanImage 2.1 Release Date: September 8, 2025"
    },
    {
      "source": "[license_files]",
      "quote": "We grant You, for the Territory only, a non-exclusive, non-transferable and royalty-free limited license under Tencent‚Äôs intellectual property or other rights owned by Us embodied in or utilized by the Materials to use, reproduce, distribute, create derivative works of (including Model Derivatives), and make modifications to the Materials, only in accordance with the terms of this Agreement and the Acceptable Use Policy"
    },
    {
      "source": "[license_files]",
      "quote": "If, on the Tencent Hunyuan version release date, the monthly active users of all products or services made available by or for Licensee is greater than 100 million monthly active users in the preceding calendar month, You must request a license from Tencent"
    }
  ],
  "1-4 (Paper)": "An official technical report is advertised with the heading ‚Äú# HunyuanImage-2.1: An Efficient Diffusion Model for High-Resolution (2K) Text-to-Image Generation‚Äã.‚Äù  The abstract-style sentence highlights that the model ‚Äúis capable of generating 2K (2048 √ó 2048) resolution images,‚Äù underscoring the primary contribution: efficient generation at 2 K resolution.  The reference entry `@misc{HunyuanImage-2.1,` signals that a citable preprint or technical note exists, although full bibliographic details are not included in the snippet.  Thus, users are directed to an accompanying paper that documents the architecture, performance, and design motivations for HunyuanImage-2.1.",
  "1-4 (Paper)__evidence": [
    {
      "source": "[readme]",
      "quote": "# HunyuanImage-2.1: An Efficient Diffusion Model for High-Resolution (2K) Text-to-Image Generation‚Äã"
    },
    {
      "source": "[readme]",
      "quote": "We present HunyuanImage-2.1, a highly efficient text-to-image model that is capable of generating 2K (2048 √ó 2048) resolution images."
    },
    {
      "source": "[readme]",
      "quote": "@misc{HunyuanImage-2.1,"
    }
  ],
  "1-5 (Architecture)": "Two configuration statements explicitly tie the Tencent-Hunyuan/HunyuanImage-2.1 model to a single architectural class. The lines\n  ‚Ä¢ \"hunyuanimage_v2_1_cfg = L(HYImageDiffusionTransformer)(\"\n  ‚Ä¢ \"hunyuanimage_v2_1_distilled_cfg = L(HYImageDiffusionTransformer)(\"\nshow that both the main and a distilled variant of version 2.1 are created by wrapping the HYImageDiffusionTransformer class with the helper function L(...). Although the snippet does not expose the arguments that follow the opening parenthesis, it unambiguously identifies HYImageDiffusionTransformer as the backbone that drives the 2.1 release. The parallel definitions further imply that the project maintains at least two parameter sets‚Äîfull and distilled‚Äîbuilt on the same diffusion-transformer core, differing only in hidden hyper-parameters not included in the quote. No layer counts or attention details are stated, but the repeated use of the same class name confirms that a diffusion-based vision transformer architecture remains the foundation across all 2.1 configurations.",
  "1-5 (Architecture)__evidence": [
    {
      "source": "[py_files/hyimage/models/hunyuan/configs/hunyuanimage_config.py]",
      "quote": "hunyuanimage_v2_1_cfg = L(HYImageDiffusionTransformer)("
    },
    {
      "source": "[py_files/hyimage/models/hunyuan/configs/hunyuanimage_config.py]",
      "quote": "hunyuanimage_v2_1_distilled_cfg = L(HYImageDiffusionTransformer)("
    }
  ],
  "1-6 (Tokenizer)": "Tokenizer details are limited to a single function definition: \"def HUNYUANIMAGE_V2_1_TEXT_ENCODER(**kwargs):\". The all-caps naming convention and explicit reference to version 2.1 indicate that a purpose-built text encoder is bundled with HunyuanImage-2.1. The **kwargs signature suggests that the encoder accepts configurable parameters at instantiation, allowing users to plug it into the broader image-generation pipeline. While the quote does not reveal whether the encoder relies on BPE vocabularies, SentencePiece, or any other scheme, it does confirm that the project exposes an official callable for text processing tailored to the 2.1 series of the model.",
  "1-6 (Tokenizer)__evidence": [
    {
      "source": "[py_files/hyimage/models/model_zoo.py]",
      "quote": "def HUNYUANIMAGE_V2_1_TEXT_ENCODER(**kwargs):"
    }
  ],
  "2-1 (Hardware)": "",
  "2-1 (Hardware)__evidence": [],
  "2-2 (Software)": "A configuration fragment provides the only concrete insight into the training software stack: \"gradient_checkpointing=True,\\n        load_from=f\\\"{HUNYUANIMAGE_V2_1_MODEL_ROOT}/dit/hunyuanimage2.1.safetensors\\\",\". From this we can infer that gradient checkpointing is enabled, meaning the training code relies on a framework capable of activation recomputation to save GPU memory. In addition, model weights are loaded from a SafeTensors file named \"hunyuanimage2.1.safetensors\" nested under a path that includes the directory \"dit\". The use of SafeTensors points to an ecosystem that favors secure, zero-copy tensor serialization compatible with common deep-learning libraries. Apart from these two flags‚Äîactivation checkpointing and SafeTensors weight loading‚Äîno further information about the specific ML framework, optimizer, or distributed-training tools is disclosed in the available quotes.",
  "2-2 (Software)__evidence": [
    {
      "source": "[py_files/hyimage/models/model_zoo.py]",
      "quote": "gradient_checkpointing=True,\n        load_from=f\"{HUNYUANIMAGE_V2_1_MODEL_ROOT}/dit/hunyuanimage2.1.safetensors\","
    }
  ],
  "2-3 (API)": "",
  "2-3 (API)__evidence": [],
  "3-1 (Pre-training)": "The only explicit information given about the pre-training stage for Tencent-Hunyuan/HunyuanImage-2.1 appears as three separate configuration declarations, each of which directly embeds the model‚Äôs name and the 2.1 version tag:\n\n‚Ä¢ ‚Äúhunyuanimage_v2_1_cfg = L(HYImageDiffusionTransformer)(‚Äù\n‚Ä¢ ‚Äúhunyuanimage_v2_1_distilled_cfg = L(HYImageDiffusionTransformer)(‚Äù\n‚Ä¢ ‚Äúhunyuanimage_refiner_cfg = L(HYImageDiffusionTransformer)(‚Äù\n\nFrom these three lines we can glean several details that are relevant to pre-training:\n1. All configurations instantiate the same underlying class, ‚ÄúHYImageDiffusionTransformer,‚Äù indicating that the core pre-training backbone is a diffusion-transformer architecture.\n2. There are at least three distinct configuration tracks: the main ‚Äúhunyuanimage_v2_1_cfg,‚Äù a ‚Äúhunyuanimage_v2_1_distilled_cfg,‚Äù and a ‚Äúhunyuanimage_refiner_cfg.‚Äù The naming implies that, during pre-training, the team created (a) a full v2.1 model, (b) a distilled version of that v2.1 model, and (c) an additional refiner stage or checkpoint, all built on the same HYImageDiffusionTransformer foundation.\n3. Every configuration explicitly embeds both the project identifier ‚Äúhunyuanimage‚Äù and the version string ‚Äú2_1,‚Äù unambiguously linking these artifacts to the Tencent-Hunyuan/HunyuanImage-2.1 release.\n\nBeyond these code-level declarations, no further hyper-parameters, data flow descriptions, or procedural details are provided in the supplied quotations. Nevertheless, the presence of three distinct HYImageDiffusionTransformer configurations underlines a multi-variant, multi-stage pre-training setup that differentiates between the standard, distilled, and refiner forms of the 2.1 model.",
  "3-1 (Pre-training)__evidence": [
    {
      "source": "[py_files/hyimage/models/hunyuan/configs/hunyuanimage_config.py]",
      "quote": "hunyuanimage_v2_1_cfg = L(HYImageDiffusionTransformer)("
    },
    {
      "source": "[py_files/hyimage/models/hunyuan/configs/hunyuanimage_config.py]",
      "quote": "hunyuanimage_v2_1_distilled_cfg = L(HYImageDiffusionTransformer)("
    },
    {
      "source": "[py_files/hyimage/models/hunyuan/configs/hunyuanimage_config.py]",
      "quote": "hunyuanimage_refiner_cfg = L(HYImageDiffusionTransformer)("
    }
  ],
  "3-2 (Fine-tuning)": "",
  "3-2 (Fine-tuning)__evidence": [],
  "3-3 (Reinforcement Learning)": "",
  "3-3 (Reinforcement Learning)__evidence": [],
  "4-1 (Pre-training Data)": "",
  "4-1 (Pre-training Data)__evidence": [],
  "4-2 (Fine-tuning Data)": "",
  "4-2 (Fine-tuning Data)__evidence": [],
  "4-3 (Reinforcement Learning Data)": "",
  "4-3 (Reinforcement Learning Data)__evidence": [],
  "4-4 (Data Filtering)": "",
  "4-4 (Data Filtering)__evidence": [],
  "__usage": {
    "fine_tuning": "unknown",
    "rl": "unknown"
  }
}