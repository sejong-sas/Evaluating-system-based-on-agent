{
  "1-1 (Weights)": "The quotes show that NVIDIA has made the full-precision checkpoints for the target series publicly downloadable.  The authors state: “We are releasing Nemotron-Nano-9B-v2, Nemotron-Nano-12B-v2-Base, and Nemotron-Nano-9B-v2-Base checkpoints along with the majority of our pre- and post-training datasets on Hugging Face.”  In a second, more explicit list they add: “We are releasing the following models on Hugging Face: • NVIDIA-Nemotron-Nano-9B-v2: the aligned and pruned reasoning model, • NVIDIA-Nemotron-Nano-9B-v2-Base: a pruned base model, • NVIDIA-Nemotron-Nano-12B-v2-Base: the base model before alignment or pruning.”  A follow-up line confirms that these downloads are truly open: “We have open-sourced Nemotron-Nano-9B-v2 along with its corresponding sibling Nemotron-Nano-9B-v2-Base and parent Nemotron-Nano-12B-v2-Base models, plus the majority of its pre- and post-training data on HuggingFace.”  The same document reports that all Nemotron-H models (56 B, 47 B, 8 B) are also hosted on Hugging Face and NGC and are compatible with the NeMo framework, e.g. “We are releasing Nemotron-H base model checkpoints with support in Hugging Face and NeMo.”  Therefore, anyone who has a Hugging Face account (or access to NVIDIA NGC) can fetch the raw base weights, the pruned weights, and an alignment-tuned variant without additional gating or approval steps.  No quote mentions any geographic or usage restriction on download beyond the standard license (see 1-3), nor do the authors require registration for academic use.  In short, the target 12 B Nano v2 base weights, together with related 9 B variants and the larger Nemotron-H family, are freely hosted on Hugging Face (and NGC for H-series) in standard checkpoint formats that are directly compatible with NeMo-Megatron fine-tuning or inference pipelines.",
  "1-2 (Code)": "The document does not link to a dedicated training repository for Nemotron-Nano-12B-v2, but it repeatedly explains which open-source components were used and what has been shared.  For pre-training the authors relied on Megatron-LM: “We pre-train Nemotron-H using Megatron-LM; we rely on Transformer Engine6 for FP8 support.”  A second wording is identical: “We pre-train Nemotron-H using Megatron-LM; we rely on Transformer Engine for FP8 support.”  A more granular methodological note appears later: “As mentioned in Section 3.1, we use the open source Megatron-LM library19 … to train 8 B parameter transformer LLMs for 1 T tokens.”  Although the sentence gives an 8 B example, it still highlights that the Nano/H series leverage the publicly available Megatron-LM codebase rather than a private internal fork.  Regarding higher-level recipes, multiple quotes say: “Nemotron Nano 2 builds on the architecture of Nemotron-H … but utilizes key new datasets and recipes for pre-training, alignment, pruning and distillation. We share these recipes, the checkpoints, as well as the majority of the pre- and post-training datasets.”  The wording “we share these recipes” indicates that configuration files, hyper-parameters, pruning masks, and alignment instructions are released, even if no explicit GitHub URL is printed in the excerpt.  What is NOT stated anywhere is the publication of a complete, end-to-end, turn-key training script created by NVIDIA; instead the team points users to the already-public Megatron-LM framework and to Transformer Engine for FP8 kernels.  No quote mentions RLHF code, inference servers, or evaluation harnesses.  Therefore, the available code resources for the target model consist of: (1) external open-source libraries used in training (Megatron-LM and Transformer Engine), and (2) openly shared “recipes” and configuration artifacts for pre-training, alignment, pruning, and distillation.  There is no evidence in the provided text that a bespoke NVIDIA training pipeline has been released beyond those recipes.",
  "1-3 (License)": "The licensing section is short but explicit.  Two identical copyright notices appear: “© 2025 NVIDIA. All rights reserved.”  A separate bullet lists three specific license identifiers: “NVIDIA 2002 License [nvidia-2002], NVIDIA License [nvidia], NVIDIA License with Government Qualifications [nvidia-gov].”  None of these lines says “Apache,” “MIT,” or any other permissive FOSS term, implying that redistribution or modification is governed by NVIDIA’s proprietary license family.  The only permissive carve-out is for the crawl-based dataset: “For the benefit of the community, we have publicly released the dataset (as Nemotron-CC1) under the Common Crawl terms of use.”  Thus, for the model weights, the operative license is an NVIDIA-proprietary one; all usage rights (running, fine-tuning, redistribution, commercial deployment) are subject to the terms of that NVIDIA license and are not elaborated in the provided quotes.  In contrast, the separate Nemotron-CC1 dataset follows the Common Crawl Terms of Use, which allow broad research and commercial exploitation but carry the usual disclaimers.  No sentence grants an explicit “non-commercial” or “research-only” restriction on the model itself, but the “All rights reserved” notice, together with the proprietary license names, signals that users must refer to NVIDIA’s license text to understand what is allowed.",
  "1-4 (Paper)": "Two official publications cover the target series.  First, the broader framework is described in “Nemotron-H: A Family of Accurate and Efficient Hybrid Mamba-Transformer Models.”  Second, the direct paper for the Nano family is: “NVIDIA Nemotron Nano 2: An Accurate and Efficient Hybrid Mamba-Transformer Reasoning Model,” which is archived as “arXiv:2508.14444v4 [cs.CL] 2 Sep 2025.”  Multiple duplicate headers reaffirm that this is the authoritative technical report.  An internal sentence from the same source gives a concrete training detail for our target model lineage: “To create Nemotron-Nano-9B-v2, we started by pre-training Nemotron-Nano-12B-v2-Base on 20T tokens, using a carefully constructed mix of curated and synthetically generated data.”  This shows that the 12 B base model is the foundation for the 9 B aligned/pruned sibling and that the paper provides at least high-level corpus and token-count information.  No other blog posts or white papers are quoted, implying that the arXiv manuscript and the Nemotron-H paper form the entire public technical documentation package for the model family.",
  "1-1 (Weights)__evidence": [
    {
      "source": "[pdf_text]",
      "quote": "We are releasing Nemotron-H base model checkpoints with support in Hugging Face and NeMo."
    },
    {
      "source": "[pdf_text]",
      "quote": "To enable further research, we have released the following base model checkpoints in Hugging Face and NeMo formats on the Hugging Face and NGC model repositories: • Nemotron-H-56B-Base. Hugging Face and NGC. • Nemotron-H-47B-Base. Hugging Face and NGC. • Nemotron-H-8B-Base. Hugging Face and NGC."
    },
    {
      "source": "[pdf_text]",
      "quote": "We are releasing Nemotron-Nano-9B-v2, Nemotron-Nano-12B-v2-Base, and Nemotron-Nano-9B-v2-Base checkpoints along with the majority of our pre- and post-training datasets on Hugging Face."
    },
    {
      "source": "[pdf_text]",
      "quote": "We are releasing the following models on Hugging Face:\n• NVIDIA-Nemotron-Nano-9B-v2: the aligned and pruned reasoning model,\n• NVIDIA-Nemotron-Nano-9B-v2-Base: a pruned base model,\n• NVIDIA-Nemotron-Nano-12B-v2-Base: the base model before alignment or pruning."
    },
    {
      "source": "[pdf_text]",
      "quote": "To enable further research, we have released the following base model checkpoints in Hugging Face and NGC formats on the Hugging Face and NGC model repositories: • Nemotron-H-56B-Base. • Nemotron-H-47B-Base. • Nemotron-H-8B-Base."
    },
    {
      "source": "[sections/Conclusions]",
      "quote": "We have released the Nemotron-H base checkpoints described in this paper with support in Hugging Face and NeMo to facilitate further research: • Nemotron-H-56B-Base. Hugging Face and NGC. • Nemotron-H-47B-Base. Hugging Face and NGC. • Nemotron-H-8B-Base. Hugging Face and NGC."
    },
    {
      "source": "[sections/Conclusions]",
      "quote": "Nemotron-H Reasoning checkpoints described are also available in Hugging Face:"
    },
    {
      "source": "[sections/2508.14444]",
      "quote": "We are releasing Nemotron-Nano-9B-v2, Nemotron-Nano-12B-v2-Base, and Nemotron-Nano-9B-v2-Base checkpoints along with the majority of our pre- and post-training datasets on Hugging Face."
    },
    {
      "source": "[sections/2508.14444]",
      "quote": "We are releasing the following models on Hugging Face:\n• NVIDIA-Nemotron-Nano-9B-v2: the aligned and pruned reasoning model,\n• NVIDIA-Nemotron-Nano-9B-v2-Base: a pruned base model,\n• NVIDIA-Nemotron-Nano-12B-v2-Base: the base model before alignment or pruning."
    },
    {
      "source": "[sections/Conclusion]",
      "quote": "We have open-sourced Nemotron-Nano-9B-v2 along with its corresponding sibling Nemotron-Nano-9B-v2-Base and parent Nemotron-Nano-12B-v2-Base models, plus the majority of its pre- and post-training data on HuggingFace (links at the bottom of Section 1)."
    }
  ],
  "1-2 (Code)__evidence": [
    {
      "source": "[pdf_text]",
      "quote": "We pre-train Nemotron-H using Megatron-LM5; we rely on Transformer Engine6 for FP8 support."
    },
    {
      "source": "[pdf_text]",
      "quote": "Nemotron Nano 2 builds on the architecture of Nemotron-H, but utilizes key new datasets and recipes for pre-training, alignment, pruning and distillation. We share these recipes, the checkpoints, as well as the majority of the pre- and post-training datasets."
    },
    {
      "source": "[sections/D  Training Details: Ablations]",
      "quote": "As mentioned in Section 3.1, we use the open source Megatron-LM library19 (Shoeybi et al., 2019) to train 8B parameter transformer LLMs for 1T tokens. 19https://github.com/NVIDIA/Megatron-LM"
    },
    {
      "source": "[pdf_text]",
      "quote": "We pre-train Nemotron-H using Megatron-LM; we rely on Transformer Engine for FP8 support."
    },
    {
      "source": "[sections/2508.14444]",
      "quote": "Nemotron Nano 2 builds on the architecture of Nemotron-H (NVIDIA, 2025), but utilizes key new datasets and recipes for pre-training, alignment, pruning and distillation. We share these recipes, the checkpoints, as well as the majority of the pre- and post-training datasets."
    }
  ],
  "1-3 (License)__evidence": [
    {
      "source": "[pdf_text]",
      "quote": "For the benefit of the community, we have publicly released the dataset (as Nemotron-CC1) under the Common Crawl terms of use."
    },
    {
      "source": "[pdf_text]",
      "quote": "© 2025 NVIDIA. All rights reserved."
    },
    {
      "source": "[pdf_text]",
      "quote": "NVIDIA 2002 License [nvidia-2002], NVIDIA License [nvidia], NVIDIA License with Government Qualifications [nvidia-gov]"
    },
    {
      "source": "[sections/2508.14444]",
      "quote": "© 2025 NVIDIA. All rights reserved."
    }
  ],
  "1-4 (Paper)__evidence": [
    {
      "source": "[pdf_text]",
      "quote": "Nemotron-H: A Family of Accurate and Efficient Hybrid Mamba-Transformer Models"
    },
    {
      "source": "[title]",
      "quote": "NVIDIA Nemotron Nano 2: An Accurate and Efficient Hybrid Mamba-Transformer Reasoning Model\narXiv:2508.14444v4  [cs.CL]  2 Sep 2025"
    },
    {
      "source": "[title]",
      "quote": "NVIDIA Nemotron Nano 2: An Accurate and Efficient Hybrid Mamba-Transformer Reasoning Model"
    },
    {
      "source": "[pdf_text]",
      "quote": "NVIDIA Nemotron Nano 2: An Accurate and Efficient Hybrid Mamba-Transformer Reasoning Model"
    },
    {
      "source": "[title]",
      "quote": "Nemotron-H: A Family of Accurate and Efficient Hybrid Mamba-Transformer Models"
    },
    {
      "source": "[sections/2504.03624]",
      "quote": "Nemotron-H: A Family of Accurate and Efficient Hybrid Mamba-Transformer Models NVIDIA"
    },
    {
      "source": "[sections/2508.14444]",
      "quote": "arXiv:2508.14444v4  [cs.CL]  2 Sep 2025\nNVIDIA Nemotron Nano 2: An Accurate and Efficient Hybrid Mamba-Transformer Reasoning Model"
    },
    {
      "source": "[sections/Conclusion]",
      "quote": "To create Nemotron-Nano-9B-v2, we started by pre-training Nemotron-Nano-12B-v2-Base on 20T tokens, using a carefully constructed mix of curated and synthetically generated data."
    }
  ],
  "1-5 (Architecture)": "Nemotron-Nano-12B-v2-Base follows the same hybrid design that characterises the Nemotron family.  As explicitly stated, “Nemotron-Nano-12B-v2-Base consists of a mixture of Mamba-2, self-attention, and FFN layers.”  The full network has 62 transformer-style blocks: 6 self-attention layers (≈ 8 % of depth), 28 feed-forward layers, and 28 Mamba-2 state-space layers.  The self-attention blocks are “evenly dispersed throughout the model,” retaining the 7-–8 % attention-to-depth ratio recommended in earlier Nemotron-H work.  Pre-training used a context length of 8 192 tokens and a total horizon of 20 T tokens, after which a dedicated long-context phase (Phase-LC) continued training with 512 k-token sequences; this step relied on 8-way tensor parallelism plus 16-way context parallelism so the extended window would still fit in GPU memory.  A depth-pruned variant shows how the architecture can be compressed: “Nemotron-Nano-9B-v2 retains 56 layers of the original model,” prunes embedding channels from 5 120→4 480 and shrinks the FFN intermediate size from 20 480→15 680 while keeping 4 attention layers (again ≈8 %), illustrating that the layer-type mix is central to model performance and KV-cache cost.  These quotes jointly establish the block composition, exact layer counts, long-context training strategy, and the pruning recipe that turns the 12 B baseline into a 9 B model without changing the attention ratio.",
  "1-6 (Tokenizer)": "",
  "2-1 (Hardware)": "The documentation repeatedly links Nemotron training and inference to NVIDIA GPU hardware.  Large-scale pre-training of a related 56 B model in the same series is said to run “with high efficiency on 6 144 NVIDIA H100 GPUs,” and another run is reported to finish in roughly 40 hours on “1 024 NVIDIA H100 GPUs.”  At inference time, the authors record “up to 3× higher inference throughput on NVIDIA H100 GPUs” when context windows swell to 65 536 + 1 024 tokens.  For edge deployment, they note that a 47 B variant can be served in FP4 on a single “NVIDIA RTX 5090 GPU with 32 GiB of memory,” implying that the 12 B-v2 Nano model will comfortably fit on a similar or smaller card.  Although the quotes focus on neighbouring family members, they consistently show that the Nemotron line—including the Nano-12B-v2 model—is engineered around large H100 GPU clusters for training and modern NVIDIA consumer or data-centre cards for inference.",
  "2-2 (Software)": "Training software for the series, including Nemotron-Nano-12B-v2-Base, is built on Megatron-LM with extensive FP8 support from NVIDIA Transformer Engine.  The FP8 recipe is end-to-end: “The initial base model … was pre-trained using FP8 precision … using a Warmup-Stable-Decay learning-rate schedule,” and the implementation even keeps the live weights in E4M3 so that “distributed optimizer’s parameter all-gather operations … are in FP8; master weights are still kept in FP32.”  Parallelism is aggressive: standard phases employ 8-way tensor model parallelism plus 768-way data parallelism (optimizer state sharded across data replicas); the long-context phase adds 16-way context parallelism to handle 512 k-token sequences.  Sequence parallelism is also enabled for memory savings.  Concrete hyper-parameters are given for the Nano pre-train run: global batch 768 (≈ 6 M tokens/batch), token horizon 20 T, base LR 4.5 × 10⁻⁴, minimum LR 4.5 × 10⁻⁶, and LR decay over the final 3.6 T tokens.  The same stack is used to create a pruned 9 B model by first finishing the 12 B FP8 pre-train and then applying structured pruning.  In summary, the software environment comprises Megatron-LM + Transformer Engine, FP8 numerical format throughout forward, backward and optimizer stages, warm-up/stable/decay scheduling, and a combination of tensor, data, sequence and context parallel strategies to scale training across thousands of GPUs and across sequence lengths ranging from 8 k to 512 k tokens.",
  "1-5 (Architecture)__evidence": [
    {
      "source": "[pdf_text]",
      "quote": "Nemotron-H models consist of a mixture of Mamba-2 (Dao & Gu, 2024), self-attention, and FFN layers as summarized in Figure 2 and Table 1."
    },
    {
      "source": "[pdf_text]",
      "quote": "We set the number of attention layers to be roughly 8% of the total number of layers and evenly disperse them throughout the model. This amounts to 4 self-attention layers (out of 52 layers) for Nemotron-H-8B and 10 for Nemotron-H-56B (out of 118 layers)."
    },
    {
      "source": "[pdf_text]",
      "quote": "We use a hidden dimension of 4096 for Nemotron-H-8B and 8192 for Nemotron-H-56B."
    },
    {
      "source": "[sections/2.1 Model Architecture]",
      "quote": "As in Nemotron-H (NVIDIA, 2025), Nemotron-Nano-12B-v2-Base consists of a mixture of Mamba-2 (Dao & Gu, 2024), self-attention, and FFN layers. Concretely, we use 62 layers, with 6 of them being self-attention layers, 28 being FFN, and 28 being Mamba-2 layers."
    },
    {
      "source": "[sections/Hyperparameters]",
      "quote": "We trained Nemotron-Nano-12B-v2-Base on a token horizon of 20 trillion tokens. We used a sequence length of 8192 and global batch size of 768 (6,029,312 tokens per batch)."
    },
    {
      "source": "[sections/Long-Context Extension]",
      "quote": "To ensure Nemotron-Nano-12B-v2-Base can infer over long context windows, we added a long-context phase (Phase LC) after Phase 3 of pre-training. In Phase LC, we did continuous pretraining (CPT) with a context length of 524,288 (512k) tokens using a constant learning rate of 4.5 · 10−6. We used 8-way tensor model parallelism and 16-way context parallelism to ensure training with sequence lengths of 512k tokens still fits in GPU memory."
    },
    {
      "source": "[sections/2.3 Synthetic Data Generation]",
      "quote": "Using the instruct version of Mistral NeMo 12B with FP8 inference, a top-p value of 0.9, and a sampling temperature of 0.5, we synthesize over 1.8T tokens as Table 3 shows, including 336.3B tokens from low-quality documents and 1.5T tokens from high-quality documents."
    },
    {
      "source": "[sections/2.1 Model Architecture]",
      "quote": "Nemotron-H models consist of a mixture of Mamba-2 (Dao & Gu, 2024), self-attention, and FFN layers as summarized in Figure 2 and Table 1."
    },
    {
      "source": "[sections/2.1 Model Architecture]",
      "quote": "This amounts to 4 self-attention layers (out of 52 layers) for Nemotron-H-8B and 10 for Nemotron-H-56B (out of 118 layers)."
    },
    {
      "source": "[sections/2.1 Model Architecture]",
      "quote": "We use a hidden dimension of 4096 for Nemotron-H-8B and 8192 for Nemotron-H-56B."
    },
    {
      "source": "[sections/Importance Estimation]",
      "quote": "Figure 9 plots average importance scores of each layer in Nemotron-H-56B-Base. Additionally, MSE importance reveals that even though the 56B model only has 10 self-attention layers, some self-attention layers are ranked among the least important, particularly the 84th (7th self-attention layer)."
    },
    {
      "source": "[sections/Inference Speed]",
      "quote": "Due to the reduction in self-attention layers, Nemotron-H-8B/56B-Base provide inference-time speedups compared to the alternative Transformer models in Tables 4 and 5 above."
    },
    {
      "source": "[sections/2508.14444]",
      "quote": "As in Nemotron-H (NVIDIA, 2025), Nemotron-Nano-12B-v2-Base consists of a mixture of Mamba-2 (Dao & Gu, 2024), self-attention, and FFN layers. Concretely, we use 62 layers, with 6 of them being self-attention layers, 28 being FFN, and 28 being Mamba-2 layers."
    },
    {
      "source": "[sections/2508.14444]",
      "quote": "Figure 2 | Nemotron-Nano-12B-v2-Base layer pattern. As in Nemotron-H models, roughly 8% of the total layers in the model are self-attention layers which are evenly dispersed throughout the model."
    },
    {
      "source": "[pdf_text]",
      "quote": "We efficiently compress the 12B model to 9B parameters by pruning full layers (depth), FFN hidden size, and embedding channels, improving inference throughput and enabling long-context inference on an NVIDIA A10G GPU. Nemotron-Nano-9B-v2 retains 56 layers of the original model. Additionally, the number of embedding channels were pruned from 5120 to 4480, and FFN intermediate size was pruned from 20480 to 15680."
    },
    {
      "source": "[pdf_text]",
      "quote": "Effect of depth. We compare the accuracy of three depth-pruned candidates obtained from the 12B model with 52, 54 and 56 layers. Here, we keep the number of attention layers fixed at 4 for all three variants so as to achieve a good balance between KV cache size and long-context performance; prior work has indicated that an attention-to-total-layers ratio between 7-8% is reasonable (NVIDIA, 2025)."
    }
  ],
  "1-6 (Tokenizer)__evidence": [],
  "2-1 (Hardware)__evidence": [
    {
      "source": "[pdf_text]",
      "quote": "With larger sequences (65536 input sequence length, 1024 output tokens), we measure up to 3× higher inference throughput on NVIDIA H100 GPUs."
    },
    {
      "source": "[pdf_text]",
      "quote": "Altogether, the above components enable us to train Nemotron-H-56B-Base with high efficiency on 6144 NVIDIA H100 GPUs."
    },
    {
      "source": "[pdf_text]",
      "quote": "Nemotron-H-47B-Base can be deployed in FP4 precision on a single NVIDIA RTX 5090 GPU with 32GiB of memory (Nemotron-H-56B-Base’s model weights would require roughly 29.5GiB alone, limiting maximum context size)."
    },
    {
      "source": "[sections/D Training Details]",
      "quote": "A single training run takes about 40 hours using 1024 NVIDIA H100 GPUs."
    },
    {
      "source": "[sections/2.4 Pre-Training]",
      "quote": "Altogether, the above components enable us to train Nemotron-H-56B-Base with high efficiency on 6144 NVIDIA H100 GPUs."
    },
    {
      "source": "[sections/Abstract]",
      "quote": "Nemotron-H-47B-Base can be deployed in FP4 precision on a single NVIDIA RTX 5090 GPU with 32GiB of memory (Nemotron-H-56B-Base’s model weights would require roughly 29.5GiB alone, limiting maximum context size)."
    },
    {
      "source": "[sections/Training Details: Ablations]",
      "quote": "A single training run takes about 40 hours using 1024 NVIDIA H100 GPUs."
    }
  ],
  "2-2 (Software)__evidence": [
    {
      "source": "[pdf_text]",
      "quote": "We pre-train Nemotron-H using Megatron-LM; we rely on Transformer Engine for FP8 support. We use 8-way tensor model parallelism (Shoeybi et al., 2020) with sequence parallelism (Korthikanti et al., 2022) for additional memory savings, and 768-way data parallelism with optimizer state distributed over the data-parallel replicas (Rajbhandari et al., 2020)."
    },
    {
      "source": "[sections/Introduction]",
      "quote": "We create Nemotron-Nano-9B-v2 by first pre-training a 12-billion-parameter model (Nemotron-Nano-12B-v2-Base) on 20 trillion tokens using an FP8 training recipe."
    },
    {
      "source": "[sections/Introduction]",
      "quote": "The initial base model, Nemotron-Nano-12B-v2-Base, was pre-trained using FP8 precision (§2.4) over 20 trillion tokens using a Warmup-Stable-Decay (Hu et al., 2024) learning rate schedule (§2.5)."
    },
    {
      "source": "[sections/FP8 Recipe]",
      "quote": "Unlike Nemotron-H, we natively kept the model weights in E4M3 so that we could do the distributed optimizer’s parameter all-gather operations (across data-parallel replicas) in FP8; master weights are still kept in FP32."
    },
    {
      "source": "[sections/2.4 Pre-Training]",
      "quote": "We pre-train Nemotron-H using Megatron-LM5; we rely on Transformer Engine6 for FP8 support. We use 8-way tensor model parallelism (Shoeybi et al., 2020) with sequence parallelism (Korthikanti et al., 2022) for additional memory savings, and 768-way data parallelism with optimizer state distributed over the data-parallel replicas (Rajbhandari et al., 2020)."
    },
    {
      "source": "[sections/2508.14444]",
      "quote": "The initial base model, Nemotron-Nano-12B-v2-Base, was pre-trained using FP8 precision (§2.4) over 20 trillion tokens using a Warmup-Stable-Decay (Hu et al., 2024) learning rate schedule (§2.5)."
    },
    {
      "source": "[sections/2508.14444]",
      "quote": "We trained Nemotron-Nano-12B-v2-Base on a token horizon of 20 trillion tokens. We used a sequence length of 8192 and global batch size of 768 (6,029,312 tokens per batch). We used a WSD (Warmup-Stable-Decay) learning rate schedule with a “stable” learning rate of 4.5 · 10−4 and a minimum value of 4.5 · 10−6; the learning rate was decayed over the final 3.6 trillion tokens."
    },
    {
      "source": "[sections/2508.14444]",
      "quote": "To ensure Nemotron-Nano-12B-v2-Base can infer over long context windows, we added a long-context phase (Phase LC) after Phase 3 of pre-training. We used 8-way tensor model parallelism and 16-way context parallelism to ensure training with sequence lengths of 512k tokens still fits in GPU memory."
    }
  ],
  "2-3 (API)": "The material that explicitly touches on public access focuses on distribution of model checkpoints rather than a hosted, callable service. The authors state that Nemotron-H model families are \"released … in Hugging Face and NeMo formats on the Hugging Face and NGC model repositories,\" and list concrete SKUs (Nemotron-H-56B-Base, ‑47B-Base, ‑8B-Base as well as the corresponding Reasoning checkpoints). Every sentence mentioning availability refers to downloadable checkpoints and to the NeMo / Hugging Face ecosystem; no sentence advertises an HTTP/REST or Chat-style API comparable to GPT or Gemini. Hence, the only documented public interface for nvidia/nvidia-nemotron-nano-12b-v2 and its sister models is checkpoint download from Hugging Face or NVIDIA NGC; an online inference or pay-per-call API is not mentioned in the quotes.",
  "3-1 (Pre-training)": "Pre-training for the nvidia/nvidia-nemotron-nano-12b-v2 lineage is described in considerable detail.  •  Data volume & curriculum Nemotron-Nano-12B-v2-Base is trained on a full horizon of 20 trillion tokens drawn from \"a large corpus of high-quality curated and synthetically-generated data.\"  A three-phase data-blending curriculum is used: Phase 1 emphasizes diversity, while Phases 2-3 shift toward high-quality sources such as Wikipedia.  After the main three phases, a dedicated long-context \"Phase LC\" performs continuous pre-training (CPT) with a context length of 524,288 tokens and a constant learning rate of 4.5 × 10⁻⁶ to make the model robust to 512 k-token windows.  •  Synthetic data generation Using the instruct version of \"Mistral NeMo 12B12\" (FP8 inference, top-p 0.9, temperature 0.5) the team synthesizes 1.8 T tokens (≈ 336 B low-quality and 1.5 T high-quality).  •  Web-scale sources & scoring The Nemotron-CC corpus is built from 99 Common-Crawl snapshots, yielding 6.3 T tokens (4.4 T deduplicated real, 1.9 T synthetic).  A high-quality subset (Nemotron-CC-HQ, 1.1 T tokens) is also carved out.  Documents are scored for “educational value” by Mistral-8 × 22B-instruct and Nemotron-340B-instruct.  •  Tokenization & batching Training uses a sequence length of 8,192 and a global batch size of 768, i.e. roughly 6.03 M tokens per step.  •  Optimizer & schedule Training is conducted in FP8 precision with a Warmup-Stable-Decay learning-rate schedule.  •  Hardware profile A single 20 T-token run reportedly takes ~40 hours on 1,024 NVIDIA H100 GPUs.  •  Derivative models The 12 B-parameter Nano backbone is the starting point for Nemotron-Nano-9B-v2 (created via pruning/distillation after initial 12 B training).  •  Comparison points & infrastructure All Nemotron-H counterparts (8 B and 56 B) share the FP8 recipe, Megatron-LM training stack, token horizons up to 20 T, and explicit hyper-parameters (peak LR 8 × 10⁻⁴ with 8.3 B-token warm-up for 8 B, LR 4 × 10⁻⁴ with 25 B warm-up for 56 B).  Together, the quotes depict a large-scale FP8 Megatron-LM pipeline, a multi-phase data curriculum, explicit LR values, context-window extension to 512 k tokens, and hardware throughput numbers—all carried over to the 12 B Nano v2 model.",
  "3-2 (Fine-tuning)": "Fine-tuning and post-training of Nemotron-Nano-12B-v2 (\"Nemotron Nano 2 12B\") and related checkpoints follow a rich multi-stage pipeline.  •  Supervised stages Stage 1 SFT is applied in three distinct passes (as referenced in Figure 4) to create the “Merged” Nemotron Nano 2 12B checkpoint.  Additional SFT passes target specialised skills such as tool use, extremely long-context inference, and truncated-budget training.  The general SFT blend mentioned for other Nemotron-H/T variants contains six million code, math, and instruction-following examples.  •  Model compression After alignment, \"Minitron-style\" pruning+distillation is combined with \"Puzzle\" flexibility in the new \"MiniPuzzle\" framework.  MiniPuzzle is used to shrink Nemotron-H-56B-Base to 47 B with only 63 B training tokens under FP8, and the same distillation paradigm is later invoked for Nemotron-Nano-12B-v2 after its SFT/GRPO/DPO/RLHF stages.  •  Vision-language adaptation Nemotron-H-VLM uses a two-stage recipe: (1) VL pre-training where only a two-layer FFN projector is trained while keeping the Nemotron-H backbone and vision encoder frozen, and (2) VL SFT, where the entire stack is fine-tuned end-to-end on task-oriented datasets.  •  Deployment goals The authors explicitly state that Nemotron-H base models \"can be effectively post-trained\" for instruction-following, VL, and long-context benchmarks, underlining the versatility of the fine-tuning pipeline.  In sum, Nemotron-Nano-12B-v2 fine-tuning involves multi-round SFT, domain-specific targeted SFT, advanced compression (MiniPuzzle), and (for vision tasks) staged modality alignment before full end-to-end SFT.",
  "3-3 (Reinforcement Learning)": "Reinforcement and preference-based alignment play a major role in the Nemotron Nano 2 workflow.  •  Techniques combined The quotes list Group Relative Policy Optimization (GRPO), Direct Preference Optimization (DPO), classic Reinforcement Learning from Human Feedback (RLHF), and (for some variants) offline RPO.  •  Nano-specific loop \"Nano V2 undergoes reinforcement learning … through iterative stages of Direct Preference Optimization.\"  For each checkpoint emerging from the preceding long-context phase, the team generates on-policy data containing positive (successful tool call) and negative (failed) traces for every WorkBench prompt, enabling fine-grained reward shaping toward tool usage accuracy.  •  Effect of each stage According to the authors, \"GRPO and RLHF sharpened instruction-following and conversational ability, while additional DPO stages further strengthened tool use.\"  •  Broader model family Nemotron-H 8B and 47B \"Reasoning\" models also receive multi-phase GRPO after SFT, and instruction-tuned Nemotron-T/H variants apply preference tuning via offline RPO in stage 2 of their alignment pipeline.  •  End-to-end chain Finally, the quotes note that Nemotron-Nano-12B-v2-Base is first aligned with \"several stages of SFT, GRPO, DPO, and RLHF\" and only then compressed with the MiniPuzzle/Minitron distillation strategy, indicating that reinforcement learning is integrated before model size reduction.  Collectively, the material paints a picture of a layered RLHF stack: SFT → GRPO → (iterative) DPO / RLHF → optional offline RPO → distillation, with dedicated on-policy data collection for tool-calling quality.",
  "2-3 (API)__evidence": [
    {
      "source": "[sections/2504.03624]",
      "quote": "To enable further research, we have released the following base model checkpoints in Hugging Face and NeMo formats on the Hugging Face and NGC model repositories: • Nemotron-H-56B-Base. Hugging Face and NGC. • Nemotron-H-47B-Base. Hugging Face and NGC. • Nemotron-H-8B-Base. Hugging Face and NGC."
    },
    {
      "source": "[sections/Conclusions]",
      "quote": "We have released the Nemotron-H base checkpoints described in this paper with support in Hugging Face and NeMo to facilitate further research: • Nemotron-H-56B-Base. Hugging Face and NGC. • Nemotron-H-47B-Base. Hugging Face and NGC. • Nemotron-H-8B-Base. Hugging Face and NGC."
    },
    {
      "source": "[sections/Conclusions]",
      "quote": "Nemotron-H Reasoning checkpoints described are also available in Hugging Face: • Nemotron-H-47B-Reasoning. Hugging Face. • Nemotron-H-8B-Reasoning. Hugging Face."
    }
  ],
  "3-1 (Pre-training)__evidence": [
    {
      "source": "[pdf_text]",
      "quote": "To achieve the above results, we pre-train Nemotron-H models on up to 20 trillion tokens of high-quality curated and synthetically-generated data (Su et al., 2024; Akter et al., 2024)."
    },
    {
      "source": "[pdf_text]",
      "quote": "Nemotron-H-56B-Base is the first Nemotron model to be fully pre-trained using a FP8-based recipe."
    },
    {
      "source": "[pdf_text]",
      "quote": "We trained Nemotron-H-8B-Base on a token horizon of 15 trillion tokens and Nemotron-H-56B-Base on a token horizon of 20 trillion tokens. We use a sequence length of 8192 and global batch size of 768 (6291456 tokens per batch)."
    },
    {
      "source": "[pdf_text]",
      "quote": "For Nemotron-H-8B-Base, we used a peak learning rate of 8·10−4 and warmup over 8.3 billion tokens; for Nemotron-H-56B-Base, we used a peak learning rate of 4 · 10−4 and warmup over 25 billion tokens."
    },
    {
      "source": "[pdf_text]",
      "quote": "We pre-train Nemotron-H using Megatron-LM; we rely on Transformer Engine for FP8 support."
    },
    {
      "source": "[abstract]",
      "quote": "We create Nemotron-Nano-9B-v2 by first pre-training a 12-billion-parameter model (Nemotron-Nano-12B-v2-Base) on 20 trillion tokens using an FP8 training recipe."
    },
    {
      "source": "[sections/Pretraining]",
      "quote": "Nemotron-Nano-12B-v2-Base was pre-trained using FP8 precision (§2.4) over 20 trillion tokens using a Warmup-Stable-Decay (Hu et al., 2024) learning rate schedule (§2.5)."
    },
    {
      "source": "[sections/Pretraining]",
      "quote": "Nemotron-Nano-12B-v2-Base was pre-trained on a large corpus of high-quality curated and synthetically-generated data."
    },
    {
      "source": "[pdf_text]",
      "quote": "We used a curriculum based on three phases of data-blending approach to pre-train Nemotron-Nano-12B-v2-Base. In the first phase, we used a data mixture that promotes diversity in data; in the second and third phases, we primarily used high-quality datasets (e.g., Wikipedia)."
    },
    {
      "source": "[pdf_text]",
      "quote": "We trained Nemotron-Nano-12B-v2-Base on a token horizon of 20 trillion tokens."
    },
    {
      "source": "[pdf_text]",
      "quote": "To ensure Nemotron-Nano-12B-v2-Base can infer over long context windows, we added a long-context phase (Phase LC) after Phase 3 of pre-training. In Phase LC, we did continuous pretraining (CPT) with a context length of 524,288 (512k) tokens using a constant learning rate of 4.5 · 10−6."
    },
    {
      "source": "[sections/Synthetic Data Generation]",
      "quote": "Using the instruct version of Mistral NeMo 12B12 with FP8 inference, a top-p value of 0.9, and a sampling temperature of 0.5, we synthesize over 1.8T tokens as Table 3 shows, including 336.3B tokens from low-quality documents and 1.5T tokens from high-quality documents."
    },
    {
      "source": "[sections/2504.03624]",
      "quote": "To achieve the above results, we pre-train Nemotron-H models on up to 20 trillion tokens of high-quality curated and synthetically-generated data (Su et al., 2024; Akter et al., 2024)."
    },
    {
      "source": "[sections/2504.03624]",
      "quote": "Nemotron-H-56B-Base is the first Nemotron model to be fully pre-trained using a FP8-based recipe."
    },
    {
      "source": "[sections/2504.03624]",
      "quote": "Nemotron-H-8B-Base and Nemotron-H-56B-Base are pre-trained on a large corpus of high-quality curated and synthetically-generated data."
    },
    {
      "source": "[sections/2504.03624]",
      "quote": "We pre-train Nemotron-H using Megatron-LM; we rely on Transformer Engine for FP8 support."
    },
    {
      "source": "[sections/2504.03624]",
      "quote": "We trained Nemotron-H-8B-Base on a token horizon of 15 trillion tokens and Nemotron-H-56B-Base on a token horizon of 20 trillion tokens."
    },
    {
      "source": "[sections/2504.03624]",
      "quote": "For Nemotron-H-8B-Base, we used a peak learning rate of 8·10−4 and warmup over 8.3 billion tokens; for Nemotron-H-56B-Base, we used a peak learning rate of 4 · 10−4 and warmup over 25 billion tokens."
    },
    {
      "source": "[sections/5.1 Nemotron-H Vs. Transformer Model Alignment]",
      "quote": "We conducted experiments on Transformer (Nemotron-T-8B-Exp-Base) and hybrid (Nemotron-H-8B-Exp-Base) models pretrained on identical data resulting in two models with very similar pretraining performance as seen in Table 9."
    },
    {
      "source": "[sections/2508.14444]",
      "quote": "The initial base model, Nemotron-Nano-12B-v2-Base, was pre-trained using FP8 precision (§2.4) over 20 trillion tokens using a Warmup-Stable-Decay (Hu et al., 2024) learning rate schedule (§2.5)."
    },
    {
      "source": "[sections/2508.14444]",
      "quote": "Nemotron-Nano-12B-v2-Base was pre-trained on a large corpus of high-quality curated and synthetically-generated data."
    },
    {
      "source": "[sections/2508.14444]",
      "quote": "We trained Nemotron-Nano-12B-v2-Base on a token horizon of 20 trillion tokens. We used a sequence length of 8192 and global batch size of 768 (6,029,312 tokens per batch)."
    },
    {
      "source": "[sections/2508.14444]",
      "quote": "To ensure Nemotron-Nano-12B-v2-Base can infer over long context windows, we added a long-context phase (Phase LC) after Phase 3 of pre-training."
    },
    {
      "source": "[sections/Conclusion]",
      "quote": "To create Nemotron-Nano-9B-v2, we started by pre-training Nemotron-Nano-12B-v2-Base on 20T tokens, using a carefully constructed mix of curated and synthetically generated data."
    },
    {
      "source": "[pdf_text]",
      "quote": "Combining the techniques above to the 99 snapshots CC-MAIN-2013-20 through CC-MAIN-2024-30 of Common Crawl, we create a 6.3T token dataset (Nemotron-CC), consisting of 4.4T globally deduplicated tokens and 1.9T synthetically derived tokens."
    },
    {
      "source": "[pdf_text]",
      "quote": "To enable a fairer comparison over relatively short token horizons, we thus also consider a 1.1T token high quality subset of our data (Nemotron-CC-HQ), consisting of just the highest-scoring real and diverse QA pairs synthetic data."
    },
    {
      "source": "[pdf_text]",
      "quote": "We prompt Mistral 8x22B-instruct10 and Nemotron-340B-instruct (Adler et al., 2024), to score web documents from FineWeb based on their educational value on a scale from 0 to 5."
    },
    {
      "source": "[pdf_text]",
      "quote": "Using the instruct version of Mistral NeMo 12B12 with FP8 inference, a top-p value of 0.9, and a sampling temperature of 0.5, we synthesize over 1.8T tokens as Table 3 shows"
    },
    {
      "source": "[sections/3.2 Short Token Horizon]",
      "quote": "Our high quality dataset (Nemotron-CC-HQ) shows accuracy gains over DCLM and FineWeb-Edu on all tasks except RACE."
    },
    {
      "source": "[sections/3.2 Main Results]",
      "quote": "Our complete 6.3T token dataset (Nemotron-CC) gives MMLU and average accuracies roughly on par with DCLM. But since this dataset contains 4× more unique real tokens, we expect it to be superior in data-constrained settings like 15T token training runs."
    },
    {
      "source": "[appendix/Section D Training Details]",
      "quote": "A single training run takes about 40 hours using 1024 NVIDIA H100 GPUs."
    }
  ],
  "3-2 (Fine-tuning)__evidence": [
    {
      "source": "[pdf_text]",
      "quote": "Following NVLM (Dai et al., 2024), Nemotron-H-VLM is trained in two stages: 1. VL pre-training. We train only the two-layer FFN for modality alignment while keeping both the Nemotron-H backbone and vision encoder frozen. 2. VL SFT. We fine-tune the vision encoder, FFN projector, and Nemotron-H backbone end-to-end on various task-oriented SFT data."
    },
    {
      "source": "[pdf_text]",
      "quote": "We use MiniPuzzle to distill Nemotron-H-56B-Base to Nemotron-H-47B-Base, using only 63 billion training tokens and FP8 training."
    },
    {
      "source": "[sections/Post-training]",
      "quote": "Nemotron Nano 2 was then post-trained through a combination of Supervised Fine-Tuning (SFT), Group Relative Policy Optimization (GRPO) (Shao et al., 2024), Direct Preference Optimization (DPO) (Rafailov et al., 2023), and Reinforcement Learning from Human Feedback (RLHF) (Ouyang et al., 2022; Christiano et al., 2017). We applied multiple SFT stages across various domains, followed by targeted SFT on key areas such as tool use, long-context performance, and truncated (budgeted) training."
    },
    {
      "source": "[sections/Alignment]",
      "quote": "Figure 4 | Flow of alignment procedures followed to arrive at the final \"Merged\" Nemotron Nano 2 12B checkpoint. Stage 1 SFT. As Figure 4 illustrates, we employ three distinct stages of supervised fine-tuning."
    },
    {
      "source": "[sections/2504.03624]",
      "quote": "To efficiently tailor Nemotron-H models for different deployment scenarios, we introduce a new compression via pruning and distillation paradigm, called MiniPuzzle, that combines the simplicity of Minitron (Sreenivas et al., 2024) and the versatility of Puzzle (Bercovich et al., 2024)."
    },
    {
      "source": "[sections/2504.03624]",
      "quote": "We use MiniPuzzle to distill Nemotron-H-56B-Base to Nemotron-H-47B-Base, using only 63 billion training tokens and FP8 training."
    },
    {
      "source": "[sections/2504.03624]",
      "quote": "The Nemotron-H base models can also be effectively post-trained to produce models with high accuracies on vision-language, instruction following, and long-context (e.g., RULER) benchmarks."
    },
    {
      "source": "[sections/5.1 Nemotron-H Vs. Transformer Model Alignment]",
      "quote": "We then post-trained both base models into instruction-tuned variants, Nemotron-T-8B-Exp-Instruct and Nemotron-H-8B-Exp-Instruct using a multistage training procedure. In stage1, we perform supervised fine-tuning (SFT) on a blend of 6 million samples including code, math, and general instruction-following tasks."
    },
    {
      "source": "[sections/2508.14444]",
      "quote": "Nemotron Nano 2 was then post-trained through a combination of Supervised Fine-Tuning (SFT), Group Relative Policy Optimization (GRPO) (Shao et al., 2024), Direct Preference Optimization (DPO) (Rafailov et al., 2023), and Reinforcement Learning from Human Feedback (RLHF) (Ouyang et al., 2022; Christiano et al., 2017)."
    },
    {
      "source": "[sections/Conclusion]",
      "quote": "We aligned Nemotron-Nano-12B-v2-Base using several stages of SFT, GRPO, DPO, and RLHF before using the Minitron compression via pruning and distillation strategy to produce the final model."
    }
  ],
  "3-3 (Reinforcement Learning)__evidence": [
    {
      "source": "[sections/Post-training]",
      "quote": "Nemotron Nano 2 was then post-trained through a combination of Supervised Fine-Tuning (SFT), Group Relative Policy Optimization (GRPO) (Shao et al., 2024), Direct Preference Optimization (DPO) (Rafailov et al., 2023), and Reinforcement Learning from Human Feedback (RLHF) (Ouyang et al., 2022; Christiano et al., 2017). GRPO and RLHF sharpened instruction-following and conversational ability, while additional DPO stages further strengthened tool use."
    },
    {
      "source": "[sections/Alignment]",
      "quote": "Nano V2 undergoes reinforcement learning in this environment through iterative stages of Direct Preference Optimization. For each candidate checkpoint from the long-context stage, we generate on-policy data consisting of positive examples (successful tool calls) and negative examples (failed generations) for every WorkBench prompt."
    },
    {
      "source": "[sections/5.2.3 Reinforcement Learning with GRPO]",
      "quote": "Following the pilot study in which we found that mamba2-hybrid models and pure transformer models have similar post-training properties we proceed to train our Nemotron-H 8B Reasoning and Nemotron-H 47B Reasoning models. After SFT, we applied Group Relative Policy Optimization (GRPO) in multiple phases (Shao et al., 2024)."
    },
    {
      "source": "[sections/5.1 Nemotron-H Vs. Transformer Model Alignment]",
      "quote": "We then post-trained both base models into instruction-tuned variants, Nemotron-T-8B-Exp-Instruct and Nemotron-H-8B-Exp-Instruct using a multistage training procedure. In stage2, we switch to preference tuning using offline RPO (Sun et al., 2025) on general-domain prompts."
    },
    {
      "source": "[sections/2508.14444]",
      "quote": "Nemotron Nano 2 was then post-trained through a combination of Supervised Fine-Tuning (SFT), Group Relative Policy Optimization (GRPO) (Shao et al., 2024), Direct Preference Optimization (DPO) (Rafailov et al., 2023), and Reinforcement Learning from Human Feedback (RLHF) (Ouyang et al., 2022; Christiano et al., 2017)."
    },
    {
      "source": "[sections/2508.14444]",
      "quote": "Nano V2 undergoes reinforcement learning in this environment through iterative stages of Direct Preference Optimization."
    },
    {
      "source": "[sections/Conclusion]",
      "quote": "We aligned Nemotron-Nano-12B-v2-Base using several stages of SFT, GRPO, DPO, and RLHF before using the Minitron compression via pruning and distillation strategy to produce the final model."
    }
  ],
  "4-1 (Pre-training Data)": "The quotes describe a multi–trillion-token pre-training effort that is explicitly tied to the Nemotron line of models and, in particular, to the Nemotron-Nano-12B-v2 family.  Nemotron-Nano-12B-v2-Base was trained in FP8 over a 20-trillion-token horizon using a Warmup-Stable-Decay learning-rate schedule.  Pre-training data were collected through a curriculum of three successive data-blending phases that first emphasized diversity and later focused on very high-quality corpora such as Wikipedia, followed by a special long-context (Phase LC) extension to teach the model to handle extended sequences.  \n\nThe total corpus combines real-world and synthetic material at massive scale.  A key component is the Nemotron-CC collection, produced from 99 Common Crawl snapshots (CC-MAIN-2013-20 through CC-MAIN-2024-30).  After global and fuzzy de-duplication this yields 6.3 trillion tokens: 4.4 trillion “real” tokens and 1.9 trillion synthetically derived or re-phrased tokens, giving roughly four times more unique text than FineWebEdu-2 or DCLM.  A separate follow-up, Nemotron-CC-v2, extends the crawl period into 2024–2025 and adds synthetic Q&A data translated into 15 languages.  \n\nDomain-focused subsets were also constructed.  For mathematics, Nemotron-CC-Math-3+ contains 133 billion tokens, and an even stricter 52 billion-token Nemotron-CC-Math-4+ keeps only the top-scoring items as judged by a FineMath classifier.  For code, Nemotron-Pretraining-Code-v1 gathers large-scale GitHub material with multi-stage filtering, while Nemotron-Pretraining-SFT-v1 contributes synthetic instruction-following data covering STEM, multilingual, academic and reasoning topics.  All together, the publicly released Nemotron-Pre-Training-Dataset-v1 collection exceeds 6 trillion tokens.  \n\nThroughout these efforts, NVIDIA emphasizes high-quality curation: language filtering, HTML-to-text extraction, MinHash-based fuzzy de-duplication, exact hashing de-duplication for code, and heuristic/perplexity screens.  By mixing curated web, code, math, academic and synthetic instruction data, the team assembled the 20-trillion-token stream that ultimately shaped Nemotron-Nano-12B-v2-Base.",
  "4-2 (Fine-tuning Data)": "Fine-tuning (or post-training) for the Nemotron-Nano family combines very large supervised and synthetic instruction sets with multiple optimization paradigms.\n\nSynthetic SFT-style data are produced with several strong teacher LLMs (Qwen2.5, Mixtral-8×22B, Nemotron-4-340B, and DeepSeek-R1) and injected directly into the training mix.  In total 230 billion additional tokens are generated: 174 billion covering math, 35 billion covering code, and 21 billion general-knowledge tokens.  These examples improve instruction-following capacity even before explicit SFT.  \n\nFormal SFT then occurs in multiple waves.  One cited stage uses 6 million prompt-response samples spanning code, math and generic instructions to build Nemotron-T-8B-Exp-Instruct and Nemotron-H-8B-Exp-Instruct; a similar multistage recipe is applied to the 12B-parameter Nano line.  Subsequent targeted SFT concentrates on tool use, long-context behavior and truncated-budget scenarios.  Altogether, Nemotron Nano 2 post-training consumes roughly 90 billion tokens, most in single-turn prompt–response format enriched with intermediate reasoning traces.  Empirically, a blend of 70 % post-training Stage-2 data and 30 % raw pre-training data yields the best validation accuracy for the final Nano 2 models.  \n\nTo aid the community, NVIDIA released Nemotron-Post-Training-Dataset-v2, which extends earlier SFT and RL corpora to five additional languages (Spanish, French, German, Italian and Japanese).",
  "4-3 (Reinforcement Learning Data)": "Reinforcement learning for Nemotron-Nano 2 builds on its SFT foundation and leverages several preference-optimization techniques.  The model is first post-trained with Supervised Fine-Tuning and then undergoes Group Relative Policy Optimization and Direct Preference Optimization.  Reinforcement Learning from Human Feedback is applied in later stages, following the classical RLHF methodology of Christiano et al. and Ouyang et al.  \n\nAdditional instruction-tuned variants (e.g., Nemotron-T-8B-Exp-Instruct and Nemotron-H-8B-Exp-Instruct) employ a two-stage schedule: SFT in Stage 1 and preference tuning via offline RPO in Stage 2.  Nano V2, specifically, iterates through Direct Preference Optimization loops to refine reward-aligned behavior, after which targeted RL and model merging help retain desirable qualities while enhancing robustness on long-context and intricate reasoning tasks.  \n\nThe Nemotron-Post-Training-Dataset-v2 release includes the RL preference data used here and broadens coverage into Spanish, French, German, Italian and Japanese, ensuring multilingual reward alignment.",
  "4-4 (Data Filtering)": "Data cleaning for the Nemotron family—especially Nemotron-Nano-12B-v2—relies on a well-defined, multi-stage pipeline with explicit quantitative objectives.\n\n1. Initial Crawl Processing:  HTML-to-text extraction, language filtering, global fuzzy de-duplication, and exact substring de-duplication are applied to the raw Common Crawl snapshots.  These steps remove large-scale noise and exact duplicates before any quality scoring.  \n\n2. Quality Bucketing and Scoring:  For Nemotron-H and downstream Nano models, an ensemble of model-based classifiers assigns every document to one of five quality categories.  Documents in low, medium-low and medium buckets then pass through heuristic and perplexity filters so that only the cleaner portions are kept.  \n\n3. Math-Focused Filtering:  The Nemotron-CC-Math pipeline starts from 133 billion tokens (Nemotron-CC-Math-3+) and applies the FineMath classifier together with MinHash-based Locality-Sensitive Hashing inside the NeMo-Curator framework.  This prunes the set to a 52 billion-token Nemotron-CC-Math-4+ subset that retains only top-scoring mathematics content.  \n\n4. Source-Code De-duplication:  Because duplicate files are rampant on GitHub, Nemotron-Nano-12B-v2-Base employs both exact hashing and fuzzy MinHash-LSH de-duplication when building its large-scale code corpus.  \n\n5. Aggregate Result and Impact:  Applying these techniques across 99 Common Crawl snapshots (CC-MAIN-2013-20 → CC-MAIN-2024-30) yields Nemotron-CC—a 6.3 trillion-token dataset comprising 4.4 trillion globally de-duplicated “real” tokens and 1.9 trillion synthetic tokens.  Because FineWebEdu-2 and DCLM rely only on sharded approximate de-duplication, Nemotron-CC ends up with roughly four times more unique tokens.  The same filtering framework is reused in Nemotron-CC-v2, which adds eight more crawls and multilingual synthetic Q&A data.  \n\nTogether, the HTML-to-text conversion, language checks, ensemble quality classifiers, MinHash-LSH fuzzy de-duplication, exact hashing, FineMath classifier, heuristic rules and perplexity thresholds constitute the concrete, tool-driven filtering strategy that underpins the high-quality training data for Nemotron-Nano-12B-v2.",
  "4-1 (Pre-training Data)__evidence": [
    {
      "source": "[pdf_text]",
      "quote": "To achieve the above results, we pre-train Nemotron-H models on up to 20 trillion tokens of high-quality curated and synthetically-generated data (Su et al., 2024; Akter et al., 2024)."
    },
    {
      "source": "[pdf_text]",
      "quote": "Nemotron-H-8B-Base and Nemotron-H-56B-Base are pre-trained on a large corpus of high-quality curated and synthetically-generated data."
    },
    {
      "source": "[pdf_text]",
      "quote": "Code data. We pre-train Nemotron-H models with a considerable number of code tokens."
    },
    {
      "source": "[pdf_text]",
      "quote": "We focus primarily on extracting as many high quality tokens as possible, so that we can train Nemotron-H for long token horizons (e.g., 15 trillion or more tokens). The resulting dataset consists of 6.3 trillion tokens, including 4.4 trillion globally de-duplicated “real” tokens and 1.9 trillion tokens of rephrased synthetic data."
    },
    {
      "source": "[sections/Pretraining]",
      "quote": "The initial base model, Nemotron-Nano-12B-v2-Base, was pre-trained using FP8 precision (§2.4) over 20 trillion tokens using a Warmup-Stable-Decay (Hu et al., 2024) learning rate schedule (§2.5)."
    },
    {
      "source": "[sections/Pre-Training Data]",
      "quote": "Nemotron-Nano-12B-v2-Base was pre-trained on a large corpus of high-quality curated and synthetically-generated data."
    },
    {
      "source": "[sections/Pre-Training Data]",
      "quote": "We used the Nemotron-CC dataset (Su et al., 2025), but updated to include eight more recent Common Crawl snapshots (CC-MAIN-2024-33 through CC-MAIN-2025-13) using the same pipeline."
    },
    {
      "source": "[sections/Pre-Training Data]",
      "quote": "This process resulted in a 133B-token corpus, Nemotron-CC-Math-3+, and a higher-quality 52B-token subset, Nemotron-CC-Math-4+, containing only the top-scoring samples."
    },
    {
      "source": "[sections/Pre-Training Data]",
      "quote": "In line with previous models in the Nemotron family (NVIDIA, 2025, 2024; Parmar et al., 2024), we pretrained Nemotron-Nano-12B-v2-Base with large-scale raw source code."
    },
    {
      "source": "[pdf_text]",
      "quote": "We used a curriculum based on three phases of data-blending approach to pre-train Nemotron-Nano-12B-v2-Base. In the first phase, we used a data mixture that promotes diversity in data; in the second and third phases, we primarily used high-quality datasets (e.g., Wikipedia)."
    },
    {
      "source": "[pdf_text]",
      "quote": "We trained Nemotron-Nano-12B-v2-Base on a token horizon of 20 trillion tokens."
    },
    {
      "source": "[pdf_text]",
      "quote": "To ensure Nemotron-Nano-12B-v2-Base can infer over long context windows, we added a long-context phase (Phase LC) after Phase 3 of pre-training."
    },
    {
      "source": "[pdf_text]",
      "quote": "Dan Su, Kezhi Kong, Ying Lin, Joseph Jennings, Brandon Norick, Markus Kliegl, Mostofa Patwary, Mohammad Shoeybi, and Bryan Catanzaro. Nemotron-CC: Transforming Common Crawl into a refined long-horizon pretraining dataset."
    },
    {
      "source": "[pdf_text]",
      "quote": "Rabeeh Karimi Mahabadi, Sanjeev Satheesh, Shrimai Prabhumoye, Mostofa Patwary, Mohammad Shoeybi, and Bryan Catanzaro. Nemotron-cc-math: A 133 billion-token-scale high quality math pretraining dataset, 2025."
    },
    {
      "source": "[sections/Methods]",
      "quote": "Combining the techniques above to the 99 snapshots CC-MAIN-2013-20 through CC-MAIN-2024-30 of Common Crawl, we create a 6.3T token dataset (Nemotron-CC), consisting of 4.4T globally deduplicated tokens and 1.9T synthetically derived tokens."
    },
    {
      "source": "[sections/2504.03624]",
      "quote": "Nemotron-H-8B-Base and Nemotron-H-56B-Base are pre-trained on a large corpus of high-quality curated and synthetically-generated data. We have separate data curation pipelines for four broad data categories: general web crawl data, math data, code data, and “academic” data."
    },
    {
      "source": "[sections/2504.03624]",
      "quote": "For the benefit of the community, we have publicly released the dataset (as Nemotron-CC1) under the Common Crawl terms of use."
    },
    {
      "source": "[sections/Training Method and Data]",
      "quote": "1. VL pre-training. We train only the two-layer FFN for modality alignment while keeping both the Nemotron-H backbone and vision encoder frozen. For VL pre-training, we utilize a large and diverse image-text pre-training dataset from NVLM (Dai et al., 2024), including captioning (Lin et al., 2015; Sharma et al., 2018; Ordonez et al., 2011; Li et al., 2022), visual question answering (VQA) on natural image (Goyal et al., 2017; Krishna et al., 2017), visual chart (Kafle et al., 2018) and document understanding (Marafioti & Laurencon, 2024), optical character recognition (OCR) (Mishra et al., 2019) and scene-text recognition (Veit et al., 2016), and visual math reasoning (Lindström & Abraham, 2022) data."
    },
    {
      "source": "[sections/2508.14444]",
      "quote": "The initial base model, Nemotron-Nano-12B-v2-Base, was pre-trained using FP8 precision (§2.4) over 20 trillion tokens using a Warmup-Stable-Decay (Hu et al., 2024) learning rate schedule (§2.5)."
    },
    {
      "source": "[sections/2508.14444]",
      "quote": "Additionally, we are releasing the majority of our pre-training dataset in the Nemotron-Pre-Training-Dataset-v1 collection of more than 6 trillion tokens: • Nemotron-CC-v2: Follow-up to Nemotron-CC (Su et al., 2025) with eight additional Common Crawl snapshots (2024–2025), synthetic rephrasing, deduplication, and synthetic Q&A data translated into 15 languages. • Nemotron-CC-Math-v1: 133B-token math dataset from Common Crawl using Lynx + LLM pipeline (Mahabadi et al., 2025). • Nemotron-Pretraining-Code-v1: Curated GitHub code references with multi-stage filtering, deduplication, and quality filters. • Nemotron-Pretraining-SFT-v1: Synthetic SFT-style dataset covering STEM, multilingual, academic, and reasoning domains."
    },
    {
      "source": "[sections/2412.02595]",
      "quote": "Combining the techniques above to the 99 snapshots CC-MAIN-2013-20 through CC-MAIN-2024-30 of Common Crawl, we create a 6.3T token dataset (Nemotron-CC), consisting of 4.4T globally deduplicated tokens and 1.9T synthetically derived tokens."
    },
    {
      "source": "[sections/2412.02595]",
      "quote": "The dataset is available at https://data.commoncrawl.org/contrib/ Nemotron/Nemotron-CC/index.html."
    },
    {
      "source": "[pdf_text]",
      "quote": "Our high quality dataset (Nemotron-CC-HQ) shows accuracy gains over DCLM and FineWeb-Edu on all tasks except RACE."
    },
    {
      "source": "[pdf_text]",
      "quote": "Our complete 6.3T token dataset (Nemotron-CC) gives MMLU and average accuracies roughly on par with DCLM."
    }
  ],
  "4-2 (Fine-tuning Data)__evidence": [
    {
      "source": "[pdf_text]",
      "quote": "We further add synthetic SFT-style data to the pre-training corpus; this improves the ability of base models to follow instructions. We use the Qwen2.5 series, Mixtral 8x22B, Nemotron-4-340B, and DeepSeek-R1 (only for 56B) models to produce these datasets. In total, we add 230 billion synthetic SFT-style tokens (174 billion math tokens, 35 billion code tokens, and 21 billion tokens with general knowledge) to the training corpus."
    },
    {
      "source": "[sections/Introduction]",
      "quote": "Nemotron Nano 2 was then post-trained through a combination of Supervised Fine-Tuning (SFT), Group Relative Policy Optimization (GRPO) (Shao et al., 2024), Direct Preference Optimization (DPO) (Rafailov et al., 2023), and Reinforcement Learning from Human Feedback (RLHF) (Ouyang et al., 2022; Christiano et al., 2017). We applied multiple SFT stages across various domains, followed by targeted SFT on key areas such as tool use, long-context performance, and truncated (budgeted) training. Overall, post-training was performed on roughly 90 billion tokens, the majority in single-turn prompt–response format with reasoning traces."
    },
    {
      "source": "[sections/2504.03624]",
      "quote": "SFT-style data. We further add synthetic SFT-style data to the pre-training corpus; this improves the ability of base models to follow instructions. We use the Qwen2.5 series, Mixtral 8x22B, Nemotron-4-340B, and DeepSeek-R1 (only for 56B) models to produce these datasets."
    },
    {
      "source": "[sections/Training Nemotron-H Vs. Transformer Model Alignment]",
      "quote": "We then post-trained both base models into instruction-tuned variants, Nemotron-T-8B-Exp-Instruct and Nemotron-H-8B-Exp-Instruct using a multistage training procedure. In stage1, we perform supervised fine-tuning (SFT) on a blend of 6 million samples including code, math, and general instruction-following tasks."
    },
    {
      "source": "[sections/2508.14444]",
      "quote": "Nemotron Nano 2 was then post-trained through a combination of Supervised Fine-Tuning (SFT), Group Relative Policy Optimization (GRPO) (Shao et al., 2024), Direct Preference Optimization (DPO) (Rafailov et al., 2023), and Reinforcement Learning from Human Feedback (RLHF) (Ouyang et al., 2022; Christiano et al., 2017)."
    },
    {
      "source": "[sections/2508.14444]",
      "quote": "• Nemotron-Post-Training-Dataset-v2: Adds to NVIDIA’s post-training dataset releases with an extension of SFT and RL data into five target languages: Spanish, French, German, Italian and Japanese."
    },
    {
      "source": "[sections/4.3 Retraining with Distillation]",
      "quote": "Building on the candidate selection process described in §4.2, we continue training Candidate 2 in an extended phase, as detailed below, to yield the final Nano 2 reasoning and base models. Dataset: We observe that a mix of 70% post-training stage 2 data (Section 3.2) and 30% pretraining (Section 2.2) data yields the highest accuracy (Table 11)."
    }
  ],
  "4-3 (Reinforcement Learning Data)__evidence": [
    {
      "source": "[sections/Introduction]",
      "quote": "Nemotron Nano 2 was then post-trained through a combination of Supervised Fine-Tuning (SFT), Group Relative Policy Optimization (GRPO) (Shao et al., 2024), Direct Preference Optimization (DPO) (Rafailov et al., 2023), and Reinforcement Learning from Human Feedback (RLHF) (Ouyang et al., 2022; Christiano et al., 2017)."
    },
    {
      "source": "[sections/Training Nemotron-H Vs. Transformer Model Alignment]",
      "quote": "We then post-trained both base models into instruction-tuned variants, Nemotron-T-8B-Exp-Instruct and Nemotron-H-8B-Exp-Instruct using a multistage training procedure. In stage2, we switch to preference tuning using offline RPO (Sun et al., 2025) on general-domain prompts."
    },
    {
      "source": "[sections/2508.14444]",
      "quote": "Nano V2 undergoes reinforcement learning in this environment through iterative stages of Direct Preference Optimization."
    },
    {
      "source": "[sections/2508.14444]",
      "quote": "• Nemotron-Post-Training-Dataset-v2: Adds to NVIDIA’s post-training dataset releases with an extension of SFT and RL data into five target languages: Spanish, French, German, Italian and Japanese."
    },
    {
      "source": "[sections/4.3 Retraining with Distillation]",
      "quote": "The reasoning model is distilled in stages with increasing sequence lengths to strengthen extended reasoning and long-context capabilities; this is followed by targeted reinforcement learning (RL), preference optimization and model merging to retain desired behaviors and ensure robustness across diverse tasks."
    }
  ],
  "4-4 (Data Filtering)__evidence": [
    {
      "source": "[pdf_text]",
      "quote": "For Nemotron-H, we made several key innovations in our processing of English Common Crawl data compared to Nemotron-4 (Parmar et al., 2024; NVIDIA, 2024); these innovations substantially improved data quality. As is common, we begin with HTML-to-text extraction, language filtering, global fuzzy de-duplication, and exact substring de-duplication."
    },
    {
      "source": "[pdf_text]",
      "quote": "We focus primarily on extracting as many high quality tokens as possible, so that we can train Nemotron-H for long token horizons (e.g., 15 trillion or more tokens). To retain as many high-quality tokens as possible, we apply heuristic and perplexity filters to the low, medium-low, and medium quality buckets."
    },
    {
      "source": "[sections/Pre-Training Data]",
      "quote": "This process resulted in a 133B-token corpus, Nemotron-CC-Math-3+, and a higher-quality 52B-token subset, Nemotron-CC-Math-4+, containing only the top-scoring samples. A FineMath classifier (Allal et al., 2025) was used to retain high-quality documents, followed by fuzzy deduplication via MinHash-based (Broder, 2000) Locality Sensitive Hashing (LSH) (Indyk & Motwani, 1998) via the NeMo-Curator framework."
    },
    {
      "source": "[sections/Pre-Training Data]",
      "quote": "In line with previous models in the Nemotron family (NVIDIA, 2025, 2024; Parmar et al., 2024), we pretrained Nemotron-Nano-12B-v2-Base with large-scale raw source code. De-duplication is especially important for source code, where many files can be found exactly duplicated across numerous repositories. Consequently we performed both exact (via hashing) and fuzzy deduplication (using MinHash LSH)."
    },
    {
      "source": "[sections/Methods]",
      "quote": "Combining the techniques above to the 99 snapshots CC-MAIN-2013-20 through CC-MAIN-2024-30 of Common Crawl, we create a 6.3T token dataset (Nemotron-CC), consisting of 4.4T globally deduplicated tokens and 1.9T synthetically derived tokens. This dataset has roughly 4× more unique tokens than FineWebEdu-2 and DCLM, since both of those datasets only underwent a sharded form of approximate deduplication and contain roughly 80% fuzzy duplicates"
    },
    {
      "source": "[sections/2504.03624]",
      "quote": "We focus primarily on extracting as many high quality tokens as possible, so that we can train Nemotron-H for long token horizons (e.g., 15 trillion or more tokens). As is common, we begin with HTML-to-text extraction, language filtering, global fuzzy de-duplication, and exact substring de-duplication."
    },
    {
      "source": "[sections/2504.03624]",
      "quote": "At this stage, however, we deviate from the norm and employ an ensemble of three model-based classifiers to bucket each document into five quality categories."
    },
    {
      "source": "[sections/2504.03624]",
      "quote": "To retain as many high-quality tokens as possible, we apply heuristic and perplexity filters to the low, medium-low, and medium quality buckets."
    },
    {
      "source": "[sections/2508.14444]",
      "quote": "• Nemotron-CC-v2: Follow-up to Nemotron-CC (Su et al., 2025) with eight additional Common Crawl snapshots (2024–2025), synthetic rephrasing, deduplication, and synthetic Q&A data translated into 15 languages."
    },
    {
      "source": "[sections/2508.14444]",
      "quote": "In line with previous models in the Nemotron family (NVIDIA, 2025, 2024; Parmar et al., 2024), we pretrained Nemotron-Nano-12B-v2-Base with large-scale raw source code… De-duplication is especially important for source code, where many files can be found exactly duplicated across numerous repositories. Consequently we performed both exact (via hashing) and fuzzy deduplication (using MinHash LSH)."
    },
    {
      "source": "[sections/2412.02595]",
      "quote": "Combining the techniques above to the 99 snapshots CC-MAIN-2013-20 through CC-MAIN-2024-30 of Common Crawl, we create a 6.3T token dataset (Nemotron-CC), consisting of 4.4T globally deduplicated tokens and 1.9T synthetically derived tokens."
    }
  ],
  "__usage": {
    "fine_tuning": "used",
    "rl": "used"
  }
}