{
  "1-5 (Architecture)": "Nemotron-Nano-12B-v2-Base follows the same hybrid design that characterises the Nemotron family.  As explicitly stated, “Nemotron-Nano-12B-v2-Base consists of a mixture of Mamba-2, self-attention, and FFN layers.”  The full network has 62 transformer-style blocks: 6 self-attention layers (≈ 8 % of depth), 28 feed-forward layers, and 28 Mamba-2 state-space layers.  The self-attention blocks are “evenly dispersed throughout the model,” retaining the 7-–8 % attention-to-depth ratio recommended in earlier Nemotron-H work.  Pre-training used a context length of 8 192 tokens and a total horizon of 20 T tokens, after which a dedicated long-context phase (Phase-LC) continued training with 512 k-token sequences; this step relied on 8-way tensor parallelism plus 16-way context parallelism so the extended window would still fit in GPU memory.  A depth-pruned variant shows how the architecture can be compressed: “Nemotron-Nano-9B-v2 retains 56 layers of the original model,” prunes embedding channels from 5 120→4 480 and shrinks the FFN intermediate size from 20 480→15 680 while keeping 4 attention layers (again ≈8 %), illustrating that the layer-type mix is central to model performance and KV-cache cost.  These quotes jointly establish the block composition, exact layer counts, long-context training strategy, and the pruning recipe that turns the 12 B baseline into a 9 B model without changing the attention ratio.",
  "1-6 (Tokenizer)": "",
  "2-1 (Hardware)": "The documentation repeatedly links Nemotron training and inference to NVIDIA GPU hardware.  Large-scale pre-training of a related 56 B model in the same series is said to run “with high efficiency on 6 144 NVIDIA H100 GPUs,” and another run is reported to finish in roughly 40 hours on “1 024 NVIDIA H100 GPUs.”  At inference time, the authors record “up to 3× higher inference throughput on NVIDIA H100 GPUs” when context windows swell to 65 536 + 1 024 tokens.  For edge deployment, they note that a 47 B variant can be served in FP4 on a single “NVIDIA RTX 5090 GPU with 32 GiB of memory,” implying that the 12 B-v2 Nano model will comfortably fit on a similar or smaller card.  Although the quotes focus on neighbouring family members, they consistently show that the Nemotron line—including the Nano-12B-v2 model—is engineered around large H100 GPU clusters for training and modern NVIDIA consumer or data-centre cards for inference.",
  "2-2 (Software)": "Training software for the series, including Nemotron-Nano-12B-v2-Base, is built on Megatron-LM with extensive FP8 support from NVIDIA Transformer Engine.  The FP8 recipe is end-to-end: “The initial base model … was pre-trained using FP8 precision … using a Warmup-Stable-Decay learning-rate schedule,” and the implementation even keeps the live weights in E4M3 so that “distributed optimizer’s parameter all-gather operations … are in FP8; master weights are still kept in FP32.”  Parallelism is aggressive: standard phases employ 8-way tensor model parallelism plus 768-way data parallelism (optimizer state sharded across data replicas); the long-context phase adds 16-way context parallelism to handle 512 k-token sequences.  Sequence parallelism is also enabled for memory savings.  Concrete hyper-parameters are given for the Nano pre-train run: global batch 768 (≈ 6 M tokens/batch), token horizon 20 T, base LR 4.5 × 10⁻⁴, minimum LR 4.5 × 10⁻⁶, and LR decay over the final 3.6 T tokens.  The same stack is used to create a pruned 9 B model by first finishing the 12 B FP8 pre-train and then applying structured pruning.  In summary, the software environment comprises Megatron-LM + Transformer Engine, FP8 numerical format throughout forward, backward and optimizer stages, warm-up/stable/decay scheduling, and a combination of tensor, data, sequence and context parallel strategies to scale training across thousands of GPUs and across sequence lengths ranging from 8 k to 512 k tokens.",
  "1-5 (Architecture)__evidence": [
    {
      "source": "[pdf_text]",
      "quote": "Nemotron-H models consist of a mixture of Mamba-2 (Dao & Gu, 2024), self-attention, and FFN layers as summarized in Figure 2 and Table 1."
    },
    {
      "source": "[pdf_text]",
      "quote": "We set the number of attention layers to be roughly 8% of the total number of layers and evenly disperse them throughout the model. This amounts to 4 self-attention layers (out of 52 layers) for Nemotron-H-8B and 10 for Nemotron-H-56B (out of 118 layers)."
    },
    {
      "source": "[pdf_text]",
      "quote": "We use a hidden dimension of 4096 for Nemotron-H-8B and 8192 for Nemotron-H-56B."
    },
    {
      "source": "[sections/2.1 Model Architecture]",
      "quote": "As in Nemotron-H (NVIDIA, 2025), Nemotron-Nano-12B-v2-Base consists of a mixture of Mamba-2 (Dao & Gu, 2024), self-attention, and FFN layers. Concretely, we use 62 layers, with 6 of them being self-attention layers, 28 being FFN, and 28 being Mamba-2 layers."
    },
    {
      "source": "[sections/Hyperparameters]",
      "quote": "We trained Nemotron-Nano-12B-v2-Base on a token horizon of 20 trillion tokens. We used a sequence length of 8192 and global batch size of 768 (6,029,312 tokens per batch)."
    },
    {
      "source": "[sections/Long-Context Extension]",
      "quote": "To ensure Nemotron-Nano-12B-v2-Base can infer over long context windows, we added a long-context phase (Phase LC) after Phase 3 of pre-training. In Phase LC, we did continuous pretraining (CPT) with a context length of 524,288 (512k) tokens using a constant learning rate of 4.5 · 10−6. We used 8-way tensor model parallelism and 16-way context parallelism to ensure training with sequence lengths of 512k tokens still fits in GPU memory."
    },
    {
      "source": "[sections/2.3 Synthetic Data Generation]",
      "quote": "Using the instruct version of Mistral NeMo 12B with FP8 inference, a top-p value of 0.9, and a sampling temperature of 0.5, we synthesize over 1.8T tokens as Table 3 shows, including 336.3B tokens from low-quality documents and 1.5T tokens from high-quality documents."
    },
    {
      "source": "[sections/2.1 Model Architecture]",
      "quote": "Nemotron-H models consist of a mixture of Mamba-2 (Dao & Gu, 2024), self-attention, and FFN layers as summarized in Figure 2 and Table 1."
    },
    {
      "source": "[sections/2.1 Model Architecture]",
      "quote": "This amounts to 4 self-attention layers (out of 52 layers) for Nemotron-H-8B and 10 for Nemotron-H-56B (out of 118 layers)."
    },
    {
      "source": "[sections/2.1 Model Architecture]",
      "quote": "We use a hidden dimension of 4096 for Nemotron-H-8B and 8192 for Nemotron-H-56B."
    },
    {
      "source": "[sections/Importance Estimation]",
      "quote": "Figure 9 plots average importance scores of each layer in Nemotron-H-56B-Base. Additionally, MSE importance reveals that even though the 56B model only has 10 self-attention layers, some self-attention layers are ranked among the least important, particularly the 84th (7th self-attention layer)."
    },
    {
      "source": "[sections/Inference Speed]",
      "quote": "Due to the reduction in self-attention layers, Nemotron-H-8B/56B-Base provide inference-time speedups compared to the alternative Transformer models in Tables 4 and 5 above."
    },
    {
      "source": "[sections/2508.14444]",
      "quote": "As in Nemotron-H (NVIDIA, 2025), Nemotron-Nano-12B-v2-Base consists of a mixture of Mamba-2 (Dao & Gu, 2024), self-attention, and FFN layers. Concretely, we use 62 layers, with 6 of them being self-attention layers, 28 being FFN, and 28 being Mamba-2 layers."
    },
    {
      "source": "[sections/2508.14444]",
      "quote": "Figure 2 | Nemotron-Nano-12B-v2-Base layer pattern. As in Nemotron-H models, roughly 8% of the total layers in the model are self-attention layers which are evenly dispersed throughout the model."
    },
    {
      "source": "[pdf_text]",
      "quote": "We efficiently compress the 12B model to 9B parameters by pruning full layers (depth), FFN hidden size, and embedding channels, improving inference throughput and enabling long-context inference on an NVIDIA A10G GPU. Nemotron-Nano-9B-v2 retains 56 layers of the original model. Additionally, the number of embedding channels were pruned from 5120 to 4480, and FFN intermediate size was pruned from 20480 to 15680."
    },
    {
      "source": "[pdf_text]",
      "quote": "Effect of depth. We compare the accuracy of three depth-pruned candidates obtained from the 12B model with 52, 54 and 56 layers. Here, we keep the number of attention layers fixed at 4 for all three variants so as to achieve a good balance between KV cache size and long-context performance; prior work has indicated that an attention-to-total-layers ratio between 7-8% is reasonable (NVIDIA, 2025)."
    }
  ],
  "1-6 (Tokenizer)__evidence": [],
  "2-1 (Hardware)__evidence": [
    {
      "source": "[pdf_text]",
      "quote": "With larger sequences (65536 input sequence length, 1024 output tokens), we measure up to 3× higher inference throughput on NVIDIA H100 GPUs."
    },
    {
      "source": "[pdf_text]",
      "quote": "Altogether, the above components enable us to train Nemotron-H-56B-Base with high efficiency on 6144 NVIDIA H100 GPUs."
    },
    {
      "source": "[pdf_text]",
      "quote": "Nemotron-H-47B-Base can be deployed in FP4 precision on a single NVIDIA RTX 5090 GPU with 32GiB of memory (Nemotron-H-56B-Base’s model weights would require roughly 29.5GiB alone, limiting maximum context size)."
    },
    {
      "source": "[sections/D Training Details]",
      "quote": "A single training run takes about 40 hours using 1024 NVIDIA H100 GPUs."
    },
    {
      "source": "[sections/2.4 Pre-Training]",
      "quote": "Altogether, the above components enable us to train Nemotron-H-56B-Base with high efficiency on 6144 NVIDIA H100 GPUs."
    },
    {
      "source": "[sections/Abstract]",
      "quote": "Nemotron-H-47B-Base can be deployed in FP4 precision on a single NVIDIA RTX 5090 GPU with 32GiB of memory (Nemotron-H-56B-Base’s model weights would require roughly 29.5GiB alone, limiting maximum context size)."
    },
    {
      "source": "[sections/Training Details: Ablations]",
      "quote": "A single training run takes about 40 hours using 1024 NVIDIA H100 GPUs."
    }
  ],
  "2-2 (Software)__evidence": [
    {
      "source": "[pdf_text]",
      "quote": "We pre-train Nemotron-H using Megatron-LM; we rely on Transformer Engine for FP8 support. We use 8-way tensor model parallelism (Shoeybi et al., 2020) with sequence parallelism (Korthikanti et al., 2022) for additional memory savings, and 768-way data parallelism with optimizer state distributed over the data-parallel replicas (Rajbhandari et al., 2020)."
    },
    {
      "source": "[sections/Introduction]",
      "quote": "We create Nemotron-Nano-9B-v2 by first pre-training a 12-billion-parameter model (Nemotron-Nano-12B-v2-Base) on 20 trillion tokens using an FP8 training recipe."
    },
    {
      "source": "[sections/Introduction]",
      "quote": "The initial base model, Nemotron-Nano-12B-v2-Base, was pre-trained using FP8 precision (§2.4) over 20 trillion tokens using a Warmup-Stable-Decay (Hu et al., 2024) learning rate schedule (§2.5)."
    },
    {
      "source": "[sections/FP8 Recipe]",
      "quote": "Unlike Nemotron-H, we natively kept the model weights in E4M3 so that we could do the distributed optimizer’s parameter all-gather operations (across data-parallel replicas) in FP8; master weights are still kept in FP32."
    },
    {
      "source": "[sections/2.4 Pre-Training]",
      "quote": "We pre-train Nemotron-H using Megatron-LM5; we rely on Transformer Engine6 for FP8 support. We use 8-way tensor model parallelism (Shoeybi et al., 2020) with sequence parallelism (Korthikanti et al., 2022) for additional memory savings, and 768-way data parallelism with optimizer state distributed over the data-parallel replicas (Rajbhandari et al., 2020)."
    },
    {
      "source": "[sections/2508.14444]",
      "quote": "The initial base model, Nemotron-Nano-12B-v2-Base, was pre-trained using FP8 precision (§2.4) over 20 trillion tokens using a Warmup-Stable-Decay (Hu et al., 2024) learning rate schedule (§2.5)."
    },
    {
      "source": "[sections/2508.14444]",
      "quote": "We trained Nemotron-Nano-12B-v2-Base on a token horizon of 20 trillion tokens. We used a sequence length of 8192 and global batch size of 768 (6,029,312 tokens per batch). We used a WSD (Warmup-Stable-Decay) learning rate schedule with a “stable” learning rate of 4.5 · 10−4 and a minimum value of 4.5 · 10−6; the learning rate was decayed over the final 3.6 trillion tokens."
    },
    {
      "source": "[sections/2508.14444]",
      "quote": "To ensure Nemotron-Nano-12B-v2-Base can infer over long context windows, we added a long-context phase (Phase LC) after Phase 3 of pre-training. We used 8-way tensor model parallelism and 16-way context parallelism to ensure training with sequence lengths of 512k tokens still fits in GPU memory."
    }
  ]
}