{
  "model_id": "nvidia/NVIDIA-Nemotron-Nano-12B-v2-Base",
  "full_texts": [
    {
      "arxiv_id": "2508.14444",
      "full_text": "2025-9-3\nNVIDIA Nemotron Nano 2: An Accurate and\nEfficient Hybrid Mamba-Transformer Reasoning\nModel\nNVIDIA\nAbstract. We introduce Nemotron-Nano-9B-v2, a hybrid Mamba-Transformer language model\ndesigned to increase throughput for reasoning workloads while achieving state-of-the-art accuracy\ncompared to similarly-sized models. Nemotron-Nano-9B-v2 builds on the Nemotron-H architecture,\nin which the majority of the self-attention layers in the common Transformer architecture are replaced\nwith Mamba-2 layers, to achieve improved inference speed when generating the long thinking traces\nneeded for reasoning. We create Nemotron-Nano-9B-v2 by first pre-training a 12-billion-parameter\nmodel (Nemotron-Nano-12B-v2-Base) on 20 trillion tokens using an FP8 training recipe. After\naligning Nemotron-Nano-12B-v2-Base, we employ the Minitron strategy to compress and distill\nthe model with the goal of enabling inference on up to 128k tokens on a single NVIDIA A10G\nGPU (22GiB of memory, bfloat16 precision). Compared to existing similarly-sized models (e.g.,\nQwen3-8B), we show that Nemotron-Nano-9B-v2 achieves on-par or better accuracy on reasoning\nbenchmarks while achieving up to 6× higher inference throughput in reasoning settings like 8k\ninput and 16k output tokens (Figure 1). We are releasing Nemotron-Nano-9B-v2, Nemotron-Nano-\n12B-v2-Base, and Nemotron-Nano-9B-v2-Base checkpoints along with the majority of our pre- and\npost-training datasets on Hugging Face.\n1. Introduction\nWe introduce NVIDIA Nemotron Nano 2, a hybrid Mamba-Transformer reasoning model (Waleffe\net al., 2024; Lieber et al., 2024; DeepMind, 2025; NVIDIA, 2025) that achieves on-par or better\nbenchmark accuracies at 3×–6× higher throughput than Qwen3-8B (Yang et al., 2025) for generation-\nheavy scenarios like 1k input / 8k output or 8k input / 16k output tokens (Figure 1). Nemotron\nNano 2 builds on the architecture of Nemotron-H (NVIDIA, 2025), but utilizes key new datasets and\nrecipes for pre-training, alignment, pruning and distillation. We share these recipes, the checkpoints,\nas well as the majority of the pre- and post-training datasets.\nThe initial base model, Nemotron-Nano-12B-v2-Base, was pre-trained using FP8 precision (§2.4) over\n20 trillion tokens using a Warmup-Stable-Decay (Hu et al., 2024) learning rate schedule (§2.5). It then\nunderwent a continuous pre-training long-context extension phase to become 128k-capable without\ndegrading other benchmarks (§2.6). Overall, new and improved datasets led to significant accuracy\nimprovements over Nemotron-H-8B on math, multilingual, MMLU-Pro and other benchmarks (§2.2).\nNemotron Nano 2 was then post-trained through a combination of Supervised Fine-Tuning (SFT),\nGroup Relative Policy Optimization (GRPO) (Shao et al., 2024), Direct Preference Optimization\n(DPO) (Rafailov et al., 2023), and Reinforcement Learning from Human Feedback (RLHF) (Ouyang\net al., 2022; Christiano et al., 2017). We applied multiple SFT stages across various domains,\nfollowed by targeted SFT on key areas such as tool use, long-context performance, and truncated\n(budgeted) training. GRPO and RLHF sharpened instruction-following and conversational ability,\nwhile additional DPO stages further strengthened tool use. Overall, post-training was performed\non roughly 90 billion tokens, the majority in single-turn prompt–response format with reasoning\n© 2025 NVIDIA. All rights reserved.\narXiv:2508.14444v4  [cs.CL]  2 Sep 2025\n\nNVIDIA Nemotron Nano 2: An Accurate and Efficient Hybrid Mamba-Transformer Reasoning Model\nIFBench\n(Instr. Following)\nAIME24\n(Math)\nAIME25\n(Math)\nGPQA-D\n(Science)\nLiveCodeBench\n(Coding)\nBFCLv3\n(Tool Use)\nRULER 128k\n(Long Context)\nISL/OSL\n1k/8k\nISL/OSL\n8k/16k\n30\n40\n50\n60\n70\n80\n90\nAccuracy (%)\n34.6\n81.9\n72.0\n64.0\n71.1\n66.9\n78.9\n33.0\n75.8\n69.3\n59.6\n59.5\n66.3\n74.1\nMeasured Accuracy\nMeasured Throughput\nNVIDIA-Nemotron-Nano-9B-v2\nQwen3-8B\n0\n1\n2\n3\n4\n5\n6\n7\n8\nRelative Throughput (Output tokens/s/GPU)\n3.3\n6.3\n1.0\n1.0\nFigure 1 | Comparison of Nemotron Nano 2 and Qwen3-8B in terms of accuracy and throughput.\nNemotron Nano 2 achieves comparable or better accuracies on complex reasoning benchmarks, while\nachieving up to 6.3× higher throughput for such workloads. We abbreviate input sequence length\nto ISL and output sequence length to OSL and measure throughput on a single A10G GPU in\nbfloat16.\ntraces. About 5% of the data contained deliberately truncated reasoning traces, enabling fine-grained\nthinking budget control at inference time (§3.4).\nFinally, both the base model and aligned model were compressed so as to enable inference over\ncontext lengths of 128k tokens on a single NVIDIA A10G GPU (22 GiB of memory, bfloat16\nprecision). This was done by extending a compression strategy based on Minitron (Muralidharan\net al., 2024; Sreenivas et al., 2024; Taghibakhshi et al., 2025) to compress reasoning models subject\nto constraints.\nWe are releasing the following models on Hugging Face:\n• NVIDIA-Nemotron-Nano-9B-v2: the aligned and pruned reasoning model,\n• NVIDIA-Nemotron-Nano-9B-v2-Base: a pruned base model,\n• NVIDIA-Nemotron-Nano-12B-v2-Base: the base model before alignment or pruning.\nAdditionally, we are releasing the majority of our pre-training dataset in the Nemotron-Pre-\nTraining-Dataset-v1 collection of more than 6 trillion tokens:\n• Nemotron-CC-v2: Follow-up to Nemotron-CC (Su et al., 2025) with eight additional\nCommon Crawl snapshots (2024–2025), synthetic rephrasing, deduplication, and synthetic\nQ&A data translated into 15 languages.\n• Nemotron-CC-Math-v1: 133B-token math dataset from Common Crawl using Lynx + LLM\npipeline (Mahabadi et al., 2025). Preserves equations, standardizes to LaTeX, outperforms\nprevious math datasets on benchmarks.\n• Nemotron-Pretraining-Code-v1: Curated GitHub code references with multi-stage filtering,\ndeduplication, and quality filters. Includes code Q&A data in 11 programming languages.\n• Nemotron-Pretraining-SFT-v1: Synthetic SFT-style dataset covering STEM, multilingual,\nacademic, and reasoning domains.\nFinally, we are releasing an updated post-training dataset:\n2\n\nNVIDIA Nemotron Nano 2: An Accurate and Efficient Hybrid Mamba-Transformer Reasoning Model\nMamba-2\nFFN\nAttention\nMamba-2\nFFN\nMamba-2\nFFN\nMamba-2\nFFN\nMamba-2\nFFN\nMamba-2\nFFN\nNemotron-Nano-12B-v2-Base\nx3\nx6\nx1\nFigure 2 | Nemotron-Nano-12B-v2-Base layer pattern. As in Nemotron-H models, roughly 8% of the\ntotal layers in the model are self-attention layers which are evenly dispersed throughout the model.\nModel\nNumber of\nlayers\nModel\ndimension\nFFN\ndimension\nQ\nheads\nKV\nheads\nState\ndimension\nMamba\ngroups\nNemotron-Nano-12B-v2-Base\n62\n5120\n20480\n40\n8\n128\n8\nTable 1 | Summary of Nemotron-Nano-12B-v2-Base architecture.\n• Nemotron-Post-Training-Dataset-v2: Adds to NVIDIA’s post-training dataset releases\nwith an extension of SFT and RL data into five target languages: Spanish, French, German,\nItalian and Japanese. The data supports improvements of math, code, general reasoning, and\ninstruction following capabilities.\nThe rest of this technical report is organized as follows: In §2, we discuss the Nemotron Nano 2 model\narchitecture, pre-training process, and base model evaluation results. In §3, we discuss the alignment\nprocess. In §4, we describe the pruning and distillation methods used for model compression.\n2. Pretraining\nIn this section, we discuss the architecture and pretraining of the Nemotron-Nano-12B-v2-Base\nmodel. We also compare this model against other state-of-the-art models in terms of accuracy on\npopular benchmarks.\n2.1. Model Architecture\nAs in Nemotron-H (NVIDIA, 2025), Nemotron-Nano-12B-v2-Base consists of a mixture of Mamba-\n2 (Dao & Gu, 2024), self-attention, and FFN layers. The layer pattern and key architecture details\nare summarized in Figure 2 and Table 1. Concretely, we use 62 layers, with 6 of them being\nself-attention layers, 28 being FFN, and 28 being Mamba-2 layers. We use a hidden dimension of\n5120, FFN hidden dimension of 20480, and Grouped-Query Attention (Ainslie et al., 2023) with 40\nquery heads and 8 key-value heads. For Mamba-2 layers, we use 8 groups, a state dimension of 128,\na head dimension of 64, an expansion factor of 2, and a window size for convolution of 4. For FFN\nlayers, we use squared ReLU (So et al., 2022) activation. Again as in Nemotron-H, we do not use\nany position embeddings and use RMSNorm (Zhang & Sennrich, 2019), separate embedding and\noutput layer weights, no dropout, and we do not use bias weights for linear layers.\n3\n\nNVIDIA Nemotron Nano 2: An Accurate and Efficient Hybrid Mamba-Transformer Reasoning Model\n2.2. Pre-Training Data\nNemotron-Nano-12B-v2-Base was pre-trained on a large corpus of high-quality curated and synthetically-\ngenerated data.\n2.2.1. Curated Data\nWe have separate data curation pipelines for the following broad data categories: general web crawl\ndata (English and multilingual), math data, and code data. We discuss each in turn next.\nEnglish web crawl data.\nWe used the Nemotron-CC dataset (Su et al., 2025), but updated to\ninclude eight more recent Common Crawl snapshots (CC-MAIN-2024-33 through CC-MAIN-2025-13)\nusing the same pipeline. For synthetic rephrasing, we mostly switched to Qwen3-30B-A3B (from\nMistral Nemo 12B). Additionally, we used data from CC-NEWS through April 23, 2025, to help\nimprove the knowledge cutoff of the model. The CC-NEWS data was filtered for English and globally\nfuzzily de-duplicated; no other filtering was used.\nMultilingual data.\nWe extracted data for fifteen languages from the following three Common\nCrawl snapshots: CC-MAIN-2024-51, CC-MAIN-2025-08, and CC-MAIN-2025-18. The fifteen\nlanguages included were Arabic, Chinese, Danish, Dutch, French, German, Italian, Japanese, Korean,\nPolish, Portuguese, Russian, Spanish, Swedish, and Thai. As we did not have reliable multilingual\nmodel-based quality classifiers available, we just applied heuristic filtering instead. This was done in\na similar manner to the filtering of low-quality English data in the Nemotron-CC pipeline, except\nthat we had to selectively disable some heuristic filters that had very high false positive rates for\nsome languages. De-duplication was done in the same way as for Nemotron-CC. Additionally, we\nused data from Wikipedia and FineWeb-2 (Penedo et al., 2025) for these fifteen languages.\nMath data.\nMathematical content on the web is expressed in a wide range of formats, including\ninline and block LATEX, MathML, Unicode symbols, and custom renderers such as MathJax or\nKaTeX. We conducted a detailed analysis of prior math-specific extraction pipelines—including\nOpenWebMath (Paster et al., 2023), MegaMath (Zhou et al., 2025), jusText (Endrédy & Novák,\n2013), Trafilatura (Barbaresi, 2021), and Resiliparse (Bevendorff et al., 2018)—and found that none\ncould reliably preserve mathematical expressions or code structure. These tools frequently discard or\ndistort equations and flatten code formatting, severely limiting the utility of the extracted content\nfor pretraining.\nTo address this, we built a new pipeline specifically designed for high-fidelity mathematical ex-\ntraction from Common Crawl. We first aggregated a comprehensive list of math-related URLs\nfrom prior datasets (e.g., InfiMM-WebMath (Han et al., 2024), OpenWebMath (Paster et al., 2023),\nFineMath (Allal et al., 2025), and MegaMath (Zhou et al., 2025)), then re-fetched their raw HTML\ndocuments from 98 Common Crawl snapshots (2014–2024). Each page was rendered using the lynx\ntext-based browser to preserve layout and math structure. We then applied Phi-4 (Abdin et al.,\n2024)(14B-parameters) to remove boilerplate, standardize notation into LATEX, and correct inconsis-\ntencies. A FineMath classifier (Allal et al., 2025) was used to retain high-quality documents, followed\nby fuzzy deduplication via MinHash-based (Broder, 2000) Locality Sensitive Hashing (LSH) (Indyk\n& Motwani, 1998) via the NeMo-Curator framework.1 We finally decontaminated the dataset using\nLLM Decontaminator (Yang et al., 2023).\nThis process resulted in a 133B-token corpus, Nemotron-CC-Math-3+, and a higher-quality 52B-token\nsubset, Nemotron-CC-Math-4+, containing only the top-scoring samples. When used for pretraining,\nthis dataset yields substantial improvements across math (MATH-500), code (HumanEval+, MBPP+,\n1https://github.com/NVIDIA-NeMo/Curator\n4\n\nNVIDIA Nemotron Nano 2: An Accurate and Efficient Hybrid Mamba-Transformer Reasoning Model\nMBPP), and general-domain evaluations (MMLU, MMLU-STEM, MMLU-Pro), surpassing all\nexisting open math datasets. For full details, see Mahabadi et al. (2025).\nCode data.\nIn line with previous models in the Nemotron family (NVIDIA, 2025, 2024; Parmar\net al., 2024), we pretrained Nemotron-Nano-12B-v2-Base with large-scale raw source code. All source\ncode used to train this model originated from GitHub and went through a multi-stage processing\npipeline to arrive at the final source code training data. We performed license-based removal with a\nlicense detection pipeline similar to that used by the BigCode project (Lozhkov et al., 2024), but\nwith fewer accepted licenses (see Appendix A for additional details). De-duplication is especially\nimportant for source code, where many files can be found exactly duplicated across numerous\nrepositories. Consequently we performed both exact (via hashing) and fuzzy deduplication (using\nMinHash LSH). In order to build a better understanding of each file in our dataset, we annotated all\nfiles with a variety of measures and then performed filtering using these annotations. We found the\nheuristic filters from OpenCoder (Huang et al., 2025) to be effective and leveraged them to filter\nfiles that are less valuable or even detrimental for LLM pretraining.\n2.2.2. Synthetically-Generated Data\nSTEM data.\nWe generated synthetic data for STEM subjects, including Astronomy, Biology,\nChemistry, Math, and Physics using 88.6k questions collected from multiple sources as the seed\ndata. In addition to the widely used GSM8K, MATH, and AOPS training sets, we collected more\ndiverse questions from Stemez2 and textbooks with permissive licenses from OpenStax3 and Open\nTextbook Library.4\nWe used Qwen2.5-VL-72B-Instruct (Bai et al., 2025) to extract questions\nfrom the exercise sections in the textbooks with additional instructions such as dropping question\nnumbering, ignoring questions that require image interpretation, and formatting equations using\nLaTeX. We manually curated the extracted questions to fix occasional OCR errors and removed\nnon-self-contained questions (e.g., a question that refers to an example in the same chapter).\nTo expand both the quantity and diversity of questions, we conducted three iterations of question\ngeneration using four models (i.e., Qwen3-30B-A3B and Qwen3-235B-A22B (Yang et al., 2025), both\nwith thinking mode enabled, Deepseek-R1 (DeepSeek-AI, 2025a), and Deepseek V3 (DeepSeek-AI,\n2025b)) and three prompts:\n1. Similar question: Create a new question that explores similar concepts but offers a fresh\nchallenge.\n2. Harder question: Create a new question that requires more logical steps or involves more\nadvanced concepts.\n3. Varied question: Create a new question that differs in type from the original question. We\ninstructed the model to avoid superficial or trivial modifications and think through the solution\nwhen creating a new question.\nWe filtered out duplicates and highly-similar questions using fuzzy de-duplication and generated\nsolutions to the remaining questions with the models used in the question generation step. We\nconverted a subset of examples to multiple-choice questions in MMLU or MMLU-Pro style. We\nconstructed a few thousand few-shot examples by concatenating random synthetic samples.\nMath data.\nWe also revisited and regenerated the Nemotron-MIND dataset (Akter et al., 2024),\na math-informed synthetic pretraining corpus originally built on OpenWebMath. In our updated\n2https://www.stemez.com/\n3https://openstax.org\n4https://open.umn.edu/opentextbooks/\n5\n\nNVIDIA Nemotron Nano 2: An Accurate and Efficient Hybrid Mamba-Transformer Reasoning Model\nversion, we regenerated the MIND dataset using Nemotron-CC-Math-4+, our highest-quality math\nsubset comprising 52B tokens—as the source corpus. Following the original methodology, we applied\nseven prompt templates (e.g., Teacher–Student, Debate, Interview, etc) to generate structured\nmathematical dialogues using the Phi-4 model. Unlike the original MIND, which relied on 14.7B\ntokens of lower-fidelity data, our version leverages significantly higher-quality input and processes\nit with a chunk size of 5K tokens. This regeneration produced a 73B-token synthetic dataset and\nled to consistent improvements across math reasoning and general knowledge (MMLU, MMLU-Pro.\nMMLU-Stem) benchmarks compared to the original MIND version, highlighting the critical role of\ninput data quality. Full details and results are available in Mahabadi et al. (2025).\nMultilingual data.\nWe generated multilingual diverse question and answer data (Diverse QA) (Su\net al., 2025) from two sources:\n1. We translated the English Diverse QA data to fifteen languages (see Multilingual data) using\nQwen3-30B-A3B (Yang et al., 2025).\n2. We generated synthetic data from Wikipedia articles in these languages using the Diverse QA\nprompt and instructed the model to write all questions and answers in the target language.\nIn addition, we translated a subset of our GSM8K augmentation data (see STEM data) into\nthese languages using Qwen3-30B-A3B. We post-processed each translated solution by appending\na concluding sentence meaning “The answer is ...” (e.g., “La respuesta es ...” in Spanish, “Die\nAntwort lautet ...” in German), where the final numerical answer is extracted from the original\nEnglish solution.\nCode data.\nWe generated question-answer (QA) data at scale for 11 different programming\nlanguages by prompting an LLM to generate questions based on short snippets from our curated\nsource code, asking the model to solve the generated question, and then performing post hoc filtering\nof the generated QA pairs based on heuristics as appropriate (e.g., Python AST parsing). This\ntechnique results in diverse synthetic data targeted at problem solving containing both natural\nlanguage and source code. Further details are covered in the Nemotron-H technical report (NVIDIA,\n2025), where we first leveraged this type of synthetic code data in pretraining.\nAcademic data.\nIn the pretraining set for the Nemotron-H (NVIDIA, 2025) series of models, we\nassigned attribute labels for educational quality, educational difficulty, and educational subject to\nall documents coming from academic data, which encompasses textbooks and academic papers. As\ncontent of higher educational difficulty in technical domains still proves challenging for models, we\nprioritized increasing model comprehension of such information in our current pretraining set via\nthe generation of question-answer (QA) pairs as such data has been shown to enhance knowledge\nstorage and extraction within language models (Allen-Zhu & Li, 2024).\nTo do so, we first gathered all documents with educational difficulty at the undergraduate and\ngraduate levels in the following technical subject areas: math, chemistry, biology, physics, and\nmedicine. Using this subset of documents, we aim to find the most relevant pieces of texts that could\nbe utilized as seed contexts for our generation of QA pairs. We chunk each document into snippets\nof 512 token lengths, embed them with the e5-large model (Wang et al., 2024), and store them\nwithin a Milvus vector database that enables approximate nearest neighbor search. We then curate\ndocuments from a set of complex subject areas (e.g. Mathematics: Real Analysis, Biology: Genetics,\nStatistics: Information Theory), and query the Milvus database for the 250 nearest neighbor text\nsnippets to each query document. The returned snippets function as our seed contexts that we then\npass into a Qwen-2.5 72B instruct model (Qwen, 2025) to generate multiple choice and free response\nstyle QA pairs based on the information contained in the snippet. With each QA pair, a justification\nfor the answer is additionally generated.\n6\n\nNVIDIA Nemotron Nano 2: An Accurate and Efficient Hybrid Mamba-Transformer Reasoning Model\nSFT-style data.\nUsing SFT-style data in the later stages of pretraining has shown to be helpful\nto foster more comprehensive model learning (Hu et al., 2024).\nTherefore, we synthesized and included different SFT-style data covering several domains: 1) code\nSFT data which is mainly focused on solving code problems; 2) math SFT data that is mostly focused\non reasoning; 3) MMLU-style SFT data which contains different question and answer examples\ncovering different knowledge topics; and 4) general instruction following SFT data.\nWe ensure that the SFT-style data covers diverse topics with different difficulty levels for each of\nthe above mentioned domains. Detailed synthesis methods and pipelines for the above mentioned\nSFT data can be found in prior work (Toshniwal et al., 2024; Moshkov et al., 2025; Bercovich et al.,\n2025a,b; Ahmad et al., 2025b,a; Majumdar et al., 2024).\nFundamental reasoning SFT-style data.\nWhile the above mentioned SFT-style data help\nenhance an LLM’s ability to answer questions in code, math and general language understanding\nbenchmarks, they do not help improve the model’s ability in deeper reasoning tasks to discern\nthe correct answer among a larger pool of potential distractors. We propose to mitigate that\nby synthesizing SFT-style data focused on analytical reasoning, logical reasoning, and reading\ncomprehension.\nSpecifically, we collected existing datasets including 1) the Law School Admission Test (LSAT)\ndataset from Wang et al. (2022); Zhong et al. (2022) which encompasses three tasks: logical\nreasoning, reading comprehension, and analytical reasoning, 2) the repurposed LogiQA dataset\nby Liu et al. (2020) which contains various types of logical reasoning questions collected from the\nNational Civil Servants Examination of China, and 3) the AQuA-RAT dataset which emphasizes\nalgebraic word problems by Ling et al. (2017). We then prompted DeepSeek-V3 (DeepSeek-AI,\n2025b) and Qwen3-30B-A3B (Yang et al., 2025) respectively to synthesize more similar questions\nwith corresponding options. For each question we generated, we prompted DeepSeek-V3 again to\ngenerate the chain-of-thought (CoT) process with the final solution. At the post-processing stage,\nwe apply majority voting to keep only the samples that have the most voted solutions. Overall, we\ngenerated 4B tokens from DeepSeek-V3 and 4.2B tokens from Qwen3-30B models.\n2.3. Data Mixture and Ordering\nOur data mixture consists of thirteen data categories. The largest is web crawl data, which we\nsubdivided into four categories based on the Nemotron-CC quality classification (Su et al., 2025):\ncrawl-medium, crawl-medium-high, crawl-high, syn-crawl-high denoting medium, medium-high, high\nand synthetic quality crawl data, respectively. Apart from these, our data mixture has additional\ncategories such as math, wikipedia, code, academic data, crawl++, multilingual, and synthetic\nSFT-style data which is further categorized as general-sft, stem-sft and code-sft. Crawl++ consists\nof web-crawl derivatives like OpenWebText, BigScience and Reddit. Our multilingual data has fifteen\nlanguages: Arabic, Danish, German, Spanish, French, Italian, Portuguese, Dutch, Polish, Swedish,\nThai, Chinese, Japanese, Korean, and Russian. We design the data mixtures to give similar weight\nto data sources that have similar quality. Data sources of higher quality are weighed higher than\ndata sources of lower quality. We provide detailed explanation on quality estimation of datasets and\nthe blend creation process in Feng et al. (2024) and NVIDIA (2025).\nWe used a curriculum based on three phases of data-blending approach to pre-train Nemotron-Nano-\n12B-v2-Base. In the first phase, we used a data mixture that promotes diversity in data; in the\nsecond and third phases, we primarily used high-quality datasets (e.g., Wikipedia). We switched to\nthe second phase at the 60% point of training, and to the third phase at the 90% point of training.\nThe data mixtures used in each phase are shown in Figure 3.\n7\n\nNVIDIA Nemotron Nano 2: An Accurate and Efficient Hybrid Mamba-Transformer Reasoning Model\nstem-sft\n3.1%\nmultilingual\n5.0%\nacademic\n4.4%\ncode\n20.0%\nmath\n3.2%\nsyn-crawl-high\n16.2%\ncrawl-medium\n18.3%\ncrawl-medium-high\n14.8%\ncrawl-high\n11.1%\n(a) Data mixture of Phase 1.\ncode-sft\n4.4%\nstem-sft\n14.5%\nmultilingual\n5.0%\ncrawl++\n4.4%\nacademic\n3.8%\nwiki\n0.9%\ncode\n20.0%\ncrawl-high\n16.0%\nsyn-crawl-high\n21.0%\nmath\n9.5%\n(b) Data mixture of Phase 2.\ncode-sft\n10.9%\nstem-sft\n32.0%\nmultilingual\n4.4%\ncrawl-high\n10.0%\nsyn-crawl-high\n12.7%\nmath\n11.0%\ncode\n16.0%\n(c) Data mixture of Phase 3.\nFigure 3 | Data mixtures for each phase of pre-training.\n8\n\nNVIDIA Nemotron Nano 2: An Accurate and Efficient Hybrid Mamba-Transformer Reasoning Model\nMultilingual Data\nAvg\nSp\nGe\nFr\nMa\nIt\nJa\nPo\nKo\nCommon Crawl\n37.0\n37.8\n36.5\n39.8\n34.3\n36.3\n35.3\n37.5\n38.8\nFineWeb-2\n35.1\n38.8\n35.0\n34.3\n31.5\n37.0\n33.0\n36.0\n35.3\nDiverseQA-wiki\n42.1\n44.8\n41.3\n41.8\n41.5\n44.0\n41.0\n42.3\n40.3\nDiverseQA-crawl\n47.0\n49.8\n50.8\n48.3\n46.0\n45.8\n44.5\n49.0\n42.0\nTable 2 | Comparison of multilingual datasets on the Global-MMLU Benchmark.\n2.3.1. Multilingual Data Ablation Study\nIn Section 2.2, we mentioned several large categories of multilingual data, both curated and synthetic:\n1. Common Crawl: Extracted from recent Common Crawl snapshots using our own pipeline.\n2. FineWeb-2 (Penedo et al., 2025).\n3. DiverseQA-wiki: Generated from multilingual Wikipedia articles using a translated Diverse\nQA prompt.\n4. DiverseQA-crawl: Translated from English Diverse QA data.\nIn order to decide the proper data mixture among these different multilingual data sources, we\nfirst conducted ablation experiments to compare the four multilingual data’s downstream tasks’\nperformance.\nSpecifically, we took a 1B model checkpoint that had been trained for 350B tokens, and continuous\npretrained it for another 100B tokens. We assigned 50% of the continuous pretraining data to\nmultilingual data, and the remaining 50% use our default pretraining data mixture. We evaluated\neach model’s performance using the Global-MMLU benchmark (Singh et al., 2024a); the results are\nshown in Table 2. Our curated Common Crawl-based multilingual data performed slightly better\nthan the Fineweb2-based multilingual data, while the synthesized multilingual QA pairs performed\nmuch better than the curated multilingual web crawl data. The diverse pairs translated from English\nCommon Crawl achieved the highest average score over the 8 languages we evaluated on. Therefore,\nwe assigned a much higher weight to the DiverseQA-crawl data than the other categories when\ndeciding our multilingual data mixture.\n2.3.2. Fundamental Reasoning SFT-Style Data Ablation Study\nTo show the effectiveness of the fundamental reasoning (FR) focused SFT-style data we introduced\nin Section 2.2, we took the Nemotron-H-8B (NVIDIA, 2025) intermediate checkpoint trained over\n14.5T tokens, and continuous pretrained it with another 100B tokens. We assigned 5% of the 100B\ntokens to the newly synthesized FR-SFT data (as a replacement for Common Crawl data), and kept\nall other data categories the same as in the Nemotron-H-8B’s phase 3 blend. We compared this\nmodel with Nemotron-H-8B, which had also been trained with 14.6T tokens. The detailed evaluation\nbenchmarks are introduced in Section 2.7. The comparison results are shown in Table 3. The\nSFT-style data helped improve the Nemotron-H 8B model’s performance on MMLU-Pro from 44.24\nto 56.36, and also helped increase the average MATH score by around 2 points. While MMLU-Pro\nis a more challenging benchmark that evaluates a model’s language understanding capability, it\nalso requires the model to have excellent reasoning capability to select the correct answer out of\nten choices. Our SFT data helps equip the model to select the correct answers from the other nine\ndistractors through fundamental reasoning. We noticed no decrease in the average commonsense\nreasoning and average code benchmarks.\n9\n\nNVIDIA Nemotron Nano 2: An Accurate and Efficient Hybrid Mamba-Transformer Reasoning Model\nModel\nAvg Math\nAvg Code\nAvg Reasoning\nMMLU\nMMLU-Pro\nNemotron-H 8B\n37.92\n59.49\n71.79\n72.67\n44.24\nNemotron-H 8B\n(w/ FR-SFT data)\n39.70\n59.61\n71.43\n72.98\n56.36\nTable 3 | Ablation study of the Fundamental Reasoning (FR) focused SFT-style data.\n2.4. FP8 Recipe\nWe used DeepSeek’s FP8 training recipe for the entirety of the pretraining run (DeepSeek-AI, 2025b).\nSpecifically, we used E4M3 for all tensors, 128x128 quantization blocks for weights, and 1x128 tiles\nfor the activations. Unlike Nemotron-H, we natively kept the model weights in E4M3 so that we\ncould do the distributed optimizer’s parameter all-gather operations (across data-parallel replicas)\nin FP8; master weights are still kept in FP32. One exception to DeepSeek’s formula was that we left\nthe first and last four linear layers in BF16, as done with Nemotron-H. Also unlike the DeepSeek-V3\nrun, we left all optimizer state in FP32. We observed no training instabilities from this choice of\nnumerics.\n2.5. Hyperparameters\nWe trained Nemotron-Nano-12B-v2-Base on a token horizon of 20 trillion tokens. We used a sequence\nlength of 8192 and global batch size of 768 (6,029,312 tokens per batch). We did not use any batch\nsize ramp-up. We used a WSD (Warmup-Stable-Decay) (Hu et al., 2024) learning rate schedule\nwith a “stable” learning rate of 4.5 · 10−4 and a minimum value of 4.5 · 10−6; the learning rate was\ndecayed over the final 3.6 trillion tokens. Weight decay was set to 0.1, and Adam 𝛽1 and 𝛽2 were set\nto 0.9 and 0.95 respectively\n2.6. Long-Context Extension\nTo ensure Nemotron-Nano-12B-v2-Base can infer over long context windows, we added a long-context\nphase (Phase LC) after Phase 3 of pre-training. In Phase LC, we did continuous pretraining (CPT)\nwith a context length of 524,288 (512k) tokens using a constant learning rate of 4.5 · 10−6. Although\nthe target context length of Nemotron Nano 2 is 128k, in preliminary studies on the Nemotron-H\n8B model, we found it better to do CPT with 512k sequence length, instead of 256k or 128k. Our\nintuition is that longer training sequence can effectively lower the chance of long coherent documents\nbeing cut and separated by the Concat & Chunk algorithm for pretraining data loading. We used\n8-way tensor model parallelism and 16-way context parallelism to ensure training with sequence\nlengths of 512k tokens still fits in GPU memory. We used a global batch size of 12 to ensure the\ntotal number of tokens per global batch during long-context CPT is the same as during pretraining:\naround 6M tokens. Phase LC consisted of 18.9 billion tokens.\nAdditionally, we did long-context synthetic data generation to create more high-quality data for Phase\nLC. Since the academic pretraining dataset is a good source of coherent long-context documents,\nwe used such documents that are longer than 32k tokens as seed data. We followed the methods\nmentioned in the Llama-3 (Meta, 2024) and Qwen-2.5 (Qwen, 2025) tech reports to generate long-\ncontext document QA data. We split each document into chunks of 1,024 tokens and then randomly\nselected 10% of the chunks to be fed into Qwen-2.5-72B-Instruct for data synthesis. We asked the\ngenerator to generate a QA pair based on the information in the text chunk. We concatenated the\nQA pairs and appended them to the end of the original document as a sample of the long-context\ndocument QA data. Such long-document QA provided good material for the model to learn long-\n10\n\nNVIDIA Nemotron Nano 2: An Accurate and Efficient Hybrid Mamba-Transformer Reasoning Model\ncontext dependencies. See Table 4 for ablation results on Nemotron-H 8B regarding train sequence\nlengths and the effects of synthetic data.\nThe data blend used in Phase LC was built based on that of Phase 3. We proportionally downscaled\nthe weights of all Phase 3 data to 80% of their original values, allocating the remaining 20% to the\nnewly added long-context document-QA data. We found such a blend could effectively extend the\ncontext length of Nemotron-Nano-12B-v2-Base without degrading regular benchmark scores.\nTrain length\n128k\n256k\n256k\n512k\nSynthetic data\nyes\nno\nyes\nyes\nRULER-128k\n73.68\n70.19\n79.04\n81.04\nTable 4 | Comparisons of different train sequence lengths and synthetic data usages. Ablations were\nconducted on Nemotron-H 8B.\nTask\nN-Nano-V2\nN-Nano-V2\nQwen3\nGemma3\n12B Base\n9B Base\n8B Base\n12B Base\nGeneral\nMMLU\n78.24\n74.53\n76.44\n73.61\nMMLU-Pro 5-shot\n63.98\n59.43\n56.27\n45.12\nAGIEval English CoT\n68.03\n65.28\n59.54\n51.69\nMath\nGSM8K CoT\n91.66\n91.36\n84.00\n74.45\nMATH\n83.54\n80.50\n55.40\n42.40\nMATH Level 5\n67.61\n63.64\n29.91\n17.71\nAIME 2024 pass@32\n56.67\n30.00\n20.00\n16.67\nCode\nHumanEval+ avg@32\n61.03\n58.50\n57.55\n36.68\nMBPP+ avg@32\n61.55\n58.95\n58.56\n51.73\nCommonsense Understanding\nARC Challenge\n93.26\n90.70\n93.09\n90.44\nHellaSwag\n84.00\n79.90\n79.75\n84.15\nOpenBookQA\n46.00\n44.80\n42.00\n46.00\nPIQA\n82.54\n81.83\n79.43\n82.10\nWinoGrande\n79.24\n75.30\n75.93\n79.95\nLong Context\nRULER-128K\n84.74\n82.22\n-\n80.70\nTable 5 | Accuracy of Nemotron-Nano-V2-Base models versus existing SoTA models. N-Nano-V2 is\nshort for Nemotron-Nano-V2. The distilled N-Nano-V2-9B-Base is compared against Qwen3-8B-Base\nand Gemma3-12B-Base, and the best score is highlighted in each row.\n11\n\nNVIDIA Nemotron Nano 2: An Accurate and Efficient Hybrid Mamba-Transformer Reasoning Model\n2.7. Base Model Evaluations\nWe run evaluations of all models ourselves unless otherwise stated. Our evaluation setup is built on\ntop of lm-evaluation-harness5 for fair comparisons, with the following changes:\n1. For mathematical reasoning, we evaluate GSM8K and MATH (Cobbe et al., 2021; Hendrycks\net al., 2021b) benchmarks using greedy-decoding. We also highlight the competition-level\nslice of the MATH benchmark as “MATH Level 5”. Additionally, we report the pass@32\nperformance on AIME-2024. We use Math-Verify6 to grade all generations.\n2. For code tasks (HumanEval (Chen et al., 2021), MBPP (Austin et al., 2021)) we evaluate\nthe EvalPlus variants along with the sanitization of generations (Liu et al., 2023), in a 0-shot\nsetup. We estimate avg@32, pass@1 from 32 generations per prompt.\n3. General reasoning benchmarks (OpenBookQA (Mihaylov et al., 2018), PIQA (Bisk et al.,\n2019), Hellaswag (Zellers et al., 2019), Winogrande Sakaguchi et al. (2019)) are unchanged\nexcept for ARC-Challenge (Clark et al., 2018), where we present all options at the same time,\nsimilar to MMLU (Hendrycks et al., 2021a).\n4. For multilingual capability, we evaluate MGSM Shi et al. (2022) (8-shot, native CoT) and\nGlobal MMLU-Lite Singh et al. (2024b).\n5. We use RULER (Hsieh et al., 2024) as the long context benchmark. We report the average\nscores over all the 13 tasks included in RULER.\nAccuracy results for Nemotron-Nano-12B-v2-Base with comparsions to Qwen3-8B Base and Gemma3-\n12B Base are shown in Tables 5 and 6. We also include the accuracy of our 9B pruned variant of\nNemotron-Nano-12B-v2-Base which is discussed in Section 4.\n3. Alignment\nIn this section we will present the alignment process we followed to convert the base checkpoint into\nan aligned 12B checkpoint. Our process is outlined in Figure 4.\nBase\nSFT 1\nSFT 2\nSFT 3\nMerged\nGRPO\nRLHF\nDPO\nFigure 4 | Flow of alignment procedures followed to arrive at the final \"Merged\" Nemotron Nano 2\n12B checkpoint.\n5https://github.com/EleutherAI/lm-evaluation-harness.\n6https://github.com/huggingface/math-verify.\n12\n\nNVIDIA Nemotron Nano 2: An Accurate and Efficient Hybrid Mamba-Transformer Reasoning Model\nTask\nN-Nano-V2\nN-Nano-V2\nQwen3\nGemma3\n12B Base\n9B Base\n8B Base\n12B Base\nGlobal-MMLU-Lite\nGerman\n74.50\n68.25\n75.50\n69.75\nSpanish\n76.50\n72.75\n75.00\n74.00\nFrench\n78.25\n69.75\n74.25\n72.50\nItalian\n76.50\n73.25\n72.75\n74.00\nJapanese\n71.00\n67.00\n70.00\n71.50\nKorean\n72.50\n67.25\n67.25\n70.25\nPortuguese\n76.25\n71.25\n72.50\n75.75\nChinese\n75.50\n69.25\n75.25\n67.25\nAverage\n75.13\n69.94\n72.81\n71.88\nMultilingual Math (MGSM)\nSpanish\n93.20\n93.60\n87.60\n73.60\nGerman\n88.40\n88.40\n78.80\n66.00\nFrench\n82.40\n84.40\n82.00\n68.00\nChinese\n83.60\n82.00\n80.80\n62.00\nJapanese\n76.80\n68.80\n71.20\n56.00\nRussian\n91.20\n90.80\n85.20\n72.40\nAverage\n85.94\n84.67\n80.93\n66.33\nTable 6 | Accuracy of Nemotron-Nano-V2-Base models versus existing SoTA models on multilingual\nbenchmarks. N-Nano-V2 is short for Nemotron-Nano-V2. The distilled N-Nano-V2-9B-Base is\ncompared against Qwen3-8B-Base and Gemma3-12B-Base, and the best score is highlighted in each\nrow.\n3.1. Post-Training Data\nOur alignment begins with a large-scale SFT stage which trains the base model on approximately 80\nbillion tokens of prompt-response pairs. The distribution of domains is shown in Table 7.\nMath, science and coding.\nFor Math (Toshniwal et al., 2024; Moshkov et al., 2025), Science\nand Coding (Ahmad et al., 2025b,a; Majumdar et al., 2024) data, we generate responses using\nthe open-weights DeepSeek-R1-0528 model (DeepSeek-AI, 2025b) using the same prompts used for\ntraining Nemotron-H-8B and 47B Reasoning models (NVIDIA, 2025). The training data has been\nreleased as part of Nemotron-Post-Training-Dataset-v17.\nTool calling.\nThe tool-calling dataset consists of single-turn, multi-turn, and multi-step conversa-\ntions.\nFor single-turn cases, we sample prompts from xlam-function-calling-60k8, glaive-\nfunction-calling-v29, NVIDIA-When2Call (Ross et al., 2025), and generate responses using\nQwen3-235B-A22B10. Inspired by ToolACE (Liu et al., 2024) and APIGen-MT (Prabhakar et al.,\n2025), we extend this to multi-turn and multi-step settings by simulating conversations where\n7https://huggingface.co/datasets/nvidia/Nemotron-Post-Training-Dataset-v1\n8https://huggingface.co/datasets/xlam-function-calling-60k\n9https://huggingface.co/datasets/glaive-function-calling-v2\n10https://huggingface.co/Qwen/Qwen3-235B-A22B\n13\n\nNVIDIA Nemotron Nano 2: An Accurate and Efficient Hybrid Mamba-Transformer Reasoning Model\nDomain\nNumber of Samples\nMath\n1.5M\nCoding\n1.1M\nScience\n2.0M\nTool-calling\n400K\nConversational\n1.5M\nSafety\n2K\nMultilingual (all domains)\n5.0M\nTable 7 | Post-training data distribution across domains used for our SFT stages.\nQwen3-235B-A22B plays the roles of User-Agent, Assistant-Agent, and API-Server-Agent. The\nUser-Agent reviews available tools, poses challenging queries, interacts when addressed by the\nAssistant, and judges task success at the end. Each instance is paired with a random persona from\nNemotron-Personas11 to enrich diversity of queries.\nThe Assistant-Agent receives the initial query and available tools, executes tasks by invoking tools,\ninterpreting their responses, and interacting with the User-Agent across single-turn, multi-turn,\nor multi-step scenarios. Meanwhile, the API-Server-Agent acts as a mock API server, checking\nparameters and returning either valid outputs or error messages depending on correctness. A\nlightweight rule-based tool-call verification layer further strengthens reliability by ensuring outputs\nare consistent and verifiable, and only successful trajectories are retained.\nMultilingual data.\nOur multilingual synthetic post-training data are constructed by translating\nexisting English post-training data. To address the challenges of Large Language Model (LLM)\nhallucinations and quality degradation on long inputs when generating synthetic translation data, we\nimplement a robust quality assurance pipeline. Our method involves translating inputs line-by-line\nto manage complexity and skip non-translatable content like code. We also enforce a strict bracket\nformat for reliable extraction and use language identification to filter out off-target translations,\nthereby ensuring high-quality final outputs.\nConversational data.\nFor conversational data, we use prompts from the LMSYS dataset (Zheng\net al., 2023) and generate responses using the Qwen3-235B-A22B reasoning model (Yang et al., 2025).\nWe also incorporate prompts from HelpSteer2 and HelpSteer3, paired with responses generated by\nthe same model. In addition, we draw on a subset of approximately 550k prompts from WildChat-\n1M (Li et al., 2024b), again generating reasoning responses with Qwen3-235B-A22B. We also include\nmulti-turn conversations with Deepseek R1 responses using the multi-turn conversational prompts\nused in NVIDIA (2025).\nSafety.\nWe leveraged a mix of harmful and benign prompts drawn from the Nemotron Content\nSafety Dataset V2 (Ghosh et al., 2025)12, HarmfulTasks (Hasan et al., 2024), RedTeam2K (Luo\net al., 2024), and gretel-v1 (gre, 2024). Responses were generated using DeepSeek-R1-052813. To\nensure safety, we applied a two-step approach: initial prompting followed by filtering with guard\nmodels to verify that outputs remained safe.\n11https://huggingface.co/datasets/NVIDIA/Nemotron-Personas\n12https://huggingface.co/datasets/nvidia/Aegis-AI-Content-Safety-Dataset-2.0\n13https://huggingface.co/deepseek-ai/DeepSeek-R1\n14\n\nNVIDIA Nemotron Nano 2: An Accurate and Efficient Hybrid Mamba-Transformer Reasoning Model\n3.2. Post Training\nStage 1 SFT.\nAs Figure 4 illustrates, we employ three distinct stages of supervised fine-tuning.\nStage 1 uses the full dataset described in Section 3.1, augmented with a subsample of roughly 10% of\nprompts paired with outputs stripped of reasoning traces. This exposes the model to “empty” traces,\nenabling it to produce direct answers in a reasoning-off mode. To improve efficiency and preserve\nlong-context ability from pretraining, we concatenate samples into sequences of approximately 128k\ntokens, reducing padding overhead and encouraging long-range learning.\nStage 2 SFT.\nStage 2 targets tool-calling. Although Stage 1 improved performance on most\nbenchmarks, tool-calling accuracy degraded. We attribute this to sample concatenation at 128k, which\nlikely disrupted learning of tool-calling patterns. Thus, Stage 2 was trained without concatenation,\nusing the full tool-calling dataset and a representative subsample of other domains.\nStage 3 SFT.\nStage 3 reinforces long-context capability. It incorporates long-context data following\nthe recipe used in Nemotron-H preparation (NVIDIA, 2025), along with augmented examples across\ndomains where reasoning traces were abruptly truncated to 1–2k tokens while preserving the final\nanswer. This truncation strategy improved robustness under varying inference-time thinking budgets.\nIFeval RL.\nTo improve instruction adherence, we sampled 16,000 prompts from the LMSYS Chat\ndataset and augmented them with IFEval-style instructions. A rule-based verifier scored outputs\nbased on how well they satisfied each instruction, creating a reward signal that prioritized following\ndirections with precision. IFEval RL experiments provided significant boost to IFEval capabilities\nwhile the rest of the benchmarks fluctuated slightly requiring careful checkpoint selection.\nDPO.\nIn another branch of training, we apply the DPO algorithm to improve tool-calling. We\nevaluate performance using the BFCL v3 benchmark, which extends BFCL v2 with greater emphasis\non multi-step (multiple tool calls to achieve a goal) and multi-turn (multiple user–agent interactions).\nTo strengthen these capabilities in the Nano V2 aligned model, we use the WorkBench environment,\na multi-step verifiable tool-calling setup adapted from Styles (Styles et al., 2024). In each WorkBench\ntask, the model must issue a sequence of tool calls across multiple steps, with correctness verified\nthrough database state comparisons.\nNano V2 undergoes reinforcement learning in this environment through iterative stages of Direct\nPreference Optimization. For each candidate checkpoint from the long-context stage, we generate\non-policy data consisting of positive examples (successful tool calls) and negative examples (failed\ngenerations) for every WorkBench prompt.\nThis process ensures that iterative DPO remains\non-policy.\nRLHF.\nWe evaluate the model’s overall helpfulness and chat capabilities using the Arena-Hard\nbenchmark. To improve performance on this benchmark, we use GRPO to train candidate checkpoints\nfrom the SFT stage using English-only contexts from HelpSteer3 (Wang et al., 2025). During training,\nwe generate responses both with and without thinking traces and use a Qwen-based reward model\nto judge the rollouts.\nModel Merging.\nDuring training, we observed a trade-off between reasoning capabilities and\nchat capabilities. To address this, we opted for checkpoint interpolation Wortsman et al. (2022),\n15\n\nNVIDIA Nemotron Nano 2: An Accurate and Efficient Hybrid Mamba-Transformer Reasoning Model\nEvaluation\nNemotron-Nano-v2-12B\nQwen3-8B\nQwen3-14B\nAIME-2024\n85.42\n75.83\n81.53\nAIME-2025\n76.25\n69.31\n66.6\nMATH-500\n97.75\n96.3\n96.85\nGPQA-Diamond\n64.48\n59.61\n64.53\nLiveCodeBench (07/24–12/24)\n70.79\n59.5\n63.08\nSciCode Sub-Task\n18.75\n24.65\n26.04\nHumanity’s Last Exam\n6.30\n4.40\n5.38\nIFEval (Inst. Strict)\n89.81\n89.39\n91.32\nBFCL v3\n66.98\n66.34\n68.01\nRULER @ 128k\n83.36\n74.13\n73.55\nArenaHard\n74\n78.4\n87.7\nTable 8 | Evaluation results with reasoning \"ON\" (for Nemotron-Nano-v2-12B, Qwen3-8B, and\nQwen3-14B across reasoning and general capability benchmarks.\nblending in an RL checkpoint with strong reasoning capabilities with an RL checkpoint with strong\nchat capabilities. Checkpoint interpolation is performed by linearly interpolating model weights:\n(1 −𝛼) · 𝑤𝑚𝑜𝑑𝑒𝑙1 + 𝛼· 𝑤𝑚𝑜𝑑𝑒𝑙2. We experimented with a parameter sweep over 𝛼values from 0.1 to\n0.9 in increments of 0.1, and found that values around 0.5 offered a good trade-off.\n3.3. Evaluation\nOur 12B model’s performance is summarized in Table 8. To test reasoning capabilities across domains,\nwe evaluate the models on MATH-500 (Lightman et al., 2023), AIME-2024, AIME-2025,\nGPQA-Diamond (Rein et al., 2023), LiveCodeBench (07/24 - 12/24) (Jain et al.,\n2024), SciCode (Tian et al., 2024), and Humanity’s Last Exam (Phan et al., 2025). For\nbroader evaluation on diverse capabilities, we use IFEval (Zhou et al., 2023) for instruction\nfollowing capabilities, BFCL v3 (Yan et al., 2024) for tool-calling, RULER for long-context,\nand ArenaHard (Li et al., 2024a) for chat capability.\nWe conduct evaluations using NeMo-Skills14. We report Pass@1 average of 16 runs for AIME-\n2024, AIME-2025; average of 4 runs for MATH-500, GPQA-Diamond, LiveCodeBench,\nIFEval; and score of 1 run for BFCL v3, SciCode, Humanity’s Last Exam, RULER,\nand ArenaHard.\n3.4. Budget Control Evaluation\nNemotron Nano V2 allows users to specify how many thinking tokens the model may generate before\nproducing the final answer. The final answer is the portion of text typically shown to end users.\nThis feature is implemented by counting tokens after the model begins generating the <think>\ntoken. Once the budget is reached, the inference setup attempts to insert a closing </think> tag.\nRather than inserting it immediately, we let the model finish its current sentence and place the\ntag at the next newline. In extreme cases where no newline appears, the system enforces closure\nwithin 500 tokens past the budget: if no newline occurs by the (budget + 500)th token, the </think>\ntag is forcibly inserted. Figure 5b shows our models budget control behavior. Apart from just\npresenting the accuracy of the model at various budgets, we also inspect if the model generations\nare well-formatted at various budgets. We inspect for two kinds of failure modes:\n14https://github.com/NVIDIA/NeMo-Skills\n16\n\nNVIDIA Nemotron Nano 2: An Accurate and Efficient Hybrid Mamba-Transformer Reasoning Model\n(a)\n(b)\nFigure 5 | Comparison of budget control before truncation training (a) and after truncation training\nwas included (b). For all plots above the x-axis indicates the budget assigned for thinking tokens.\n• In one failure mode, the model uses more tokens in the final answer to “compensate” for\nrestrictions in the thinking traces. Without truncated training examples in the SFT stage,\nthis compensation effect is prevalent (Figure 5a, center). With truncated training, however,\nthe effect is absent (Figure 5b, center).\n• Another issue is that the model can remain in “thinking mode” even after the closing tag\n</think> is inserted. This is evident when the model generates the closing tag again after the\nforced insertion, suggesting it does not fully “register” the artificial closure. We evaluate this\nusing “Well-Formedness,” where a well-formed response should contain only a single closing\ntag (either forced by the budget or produced naturally). Figure 5a (right) shows that for short\nbudgets, the percentage of well-formed responses drops sharply. With truncation training,\nhowever, the model consistently produces well-formed responses (Figure 5b, right).\n4. Pruning and Distillation\nIn this section, we describe the pruning and distillation process to compress the aligned 12B model\nto the Nano 2 model with the goal of running longer context (128k sequence length) inference on\nthe NVIDIA A10G GPU. Note that storing just the weights of a 12B parameter model in bfloat16\nprecision requires 22.9 GiB, which is more than the 22 GiB memory capacity of an A10G GPU; this\nclearly indicates the need for compression.\nOur compression strategy builds on Minitron (Muralidharan et al., 2024; Sreenivas et al., 2024;\nTaghibakhshi et al., 2025), which is a lightweight model pruning framework for LLMs. While\nMinitron was originally designed for compressing pretrained base models targeting user-defined\nparameter budgets, in this work, we extend it to compress reasoning models while also incorporating\n17\n\nNVIDIA Nemotron Nano 2: An Accurate and Efficient Hybrid Mamba-Transformer Reasoning Model\nthe memory constraints and throughput-based objectives stated above.\n4.1. Importance Estimation\nWe collect importance or sensitivity scores for each model component (e.g., layers, FFN neurons)\nto help decide which components to remove; this is the importance estimation phase. The scores\ncomputed in this phase are used to decide which model components can be pruned. We note\nthat sensitivity analysis based on gradient information is typically impractical at modern LLM\nscale (Muralidharan et al., 2024); instead, we rely on a lightweight strategy that uses only forward\npasses. In this work, we use a simplified approach that works well in our ablation studies: a) prune\nlayers, and b) prune FFN hidden dimensions (effectively neurons) and embedding channels. We\nalso experimented with pruning Mamba heads; unfortunately, this axis caused severe accuracy\ndegradation. We now describe how we compute the importance of each layer, embedding channel,\nFFN neuron and Mamba head.\nLayer importance.\nWe compute layer importance in an iterative fashion: for each candidate layer,\nwe temporarily remove it from the model and compute the mean squared error (MSE) between the\noriginal model’s logits and those produced by the pruned model. This MSE reflects the contribution\nof that layer to the model’s predictions: lower values indicate smaller impact. At each pruning step,\nwe remove the layer with the lowest MSE, as it has the least influence on the final output. We repeat\nthis process until the desired depth is reached. This strategy ensures that pruning preferentially\nremoves layers whose absence minimally affects the model’s behavior. For more details on iterative\nMSE-based layer importance, please refer to NVIDIA (2025).\nFFN and embedding channel importance.\nFFN layers internally are composed of two linear\noperators with a non-linear activation in between:\nFFN(X) = 𝛿\n(︂\nX · 𝑊𝑇\n1\n)︂\n· 𝑊2.\nHere, X denotes the input, and 𝑊1 and 𝑊2 are the two associated weight matrices in the FFN\nlayer. 𝑊1, 𝑊2 ∈R𝑑𝑓𝑓𝑛×𝑑𝑚𝑜𝑑𝑒𝑙, where 𝑑𝑚𝑜𝑑𝑒𝑙and 𝑑𝑓𝑓𝑛are the model hidden dimension and FFN\nhidden dimension respectively. 𝛿(·) refers to the non-linear activation function (squared ReLU in\nthis work).\nFollowing the same procedure as Minitron (Muralidharan et al., 2024), we compute the importance\nof each neuron in the first linear operator of each FFN layer by examining the set of outputs it\nproduces. We use a small calibration dataset of 1024 samples for this purpose. Formally, we compute\neach neuron’s importance score by aggregating its outputs given an input batch 𝑋:\n𝐹(𝑖)\nneuron =\n∑︁\nB,S\n𝛿\n(︂\nX\n(︀𝑊𝑖\n1\n)︀𝑇\n)︂\n.\nHere, 𝑊𝑖\n1 refers to the 𝑖th row of the weight matrix 𝑊1. ∑︀\nB,S refers to aggregation along the\nbatch and sequence dimensions. We use the mean and l2-norm aggregation functions along the\nbatch and sequence dimensions, following the observations in the Minitron paper. For a sequence of\nscores S, mean aggregation is defined as 1\n𝑛\n∑︀𝑛\n𝑖=1 |S𝑖|, and l2-norm is\n√︁∑︀𝑛\n𝑖=1 S2\n𝑖. Embedding channel\nimportance is computed similarly, by examining the outputs of LayerNorm layers instead; we refer\nthe reader to Muralidharan et al. (2024) for more details.\n18\n\nNVIDIA Nemotron Nano 2: An Accurate and Efficient Hybrid Mamba-Transformer Reasoning Model\nMamba importance.\nMamba layers process inputs through multiple projection matrices (𝑊𝑥,\n𝑊𝑧, 𝑊𝐵, 𝑊𝐶, 𝑊𝑑𝑡) that produce intermediate representations before causal convolution and selective\nstate space model (SSM) updates, followed by gated normalization and an output projection (𝑊𝑂).\nWe follow the methodology described in Taghibakhshi et al. (2025) for importance estimation:\nspecifically, we adopt a nested activation-based scoring strategy over a small calibration dataset of\n1024 samples, similar to FFN importance but adapted to Mamba’s group-aware structure. First,\nwe obtain activation scores from the 𝑊𝑥projection, denoted 𝑠∈R𝑚ℎ×𝑚𝑑, where 𝑚ℎis the number\nof Mamba heads and 𝑚𝑑is the Mamba head channel dimension. For each channel 𝑑, the score is\ncomputed as\n𝑠𝑑=\n⃦⃦⃦⃦⃦⃦\n∑︁\nB,S\n𝑠:,𝑑\n⃦⃦⃦⃦⃦⃦\n2\n,\nwhere the aggregation is over the batch (B) and sequence (S) dimensions, using both mean and\nl2-norm metrics. Next, head scores are computed by using the l2-norm over the Mamba head\nchannel set:\n𝑓ℎ= ‖𝑠ℎ,𝑚𝑑‖2 ,\n∀ℎ∈{1, . . . , 𝑚ℎ},\nand heads are ranked within each Mamba group 𝒢𝑔to preserve group-aware computation semantics:\nℛ𝑔= argsortℎ∈𝒢𝑔(𝑓ℎ).\nwhich ensures that pruning decisions respect the model’s structural constraints and SSM’s sequence\nmodeling. The lowest-scoring heads are pruned by trimming the corresponding rows from all affected\nprojection, convolution, and SSM parameter matrices. This strategy preserves the integrity of the\nSSM block while removing less important Mamba heads. As shown in Taghibakhshi et al. (2025),\npruning Mamba heads yields a better accuracy–throughput trade-off than pruning head channels;\nwe consequently focus on head pruning in this work.\n4.2. Lightweight Neural Architecture Search\nWe first define the constraints and objectives for the Nano 2 model, and then describe our lightweight\nNeural Architecture Search (NAS) framework that finds the most promising architectural candidates\nthat meet our objectives and constraints.\nMemory constraints.\nMemory requirements during inference consist of two distinct components\nwith different scaling behaviors.\nThe parameter memory, while substantial, remains constant\nregardless of the input size. In contrast, the key-value cache memory scales linearly with both batch\nsize and sequence length, often becoming the dominant factor in long-sequence scenarios. For the\nNano 2 model, our goal was to be able to perform inference at a sequence length of 128k and a batch\nsize of at least 1 within a memory budget of 19.66 GiB. We obtained the budget as follows: from the\n22.06 GiB available memory on an NVIDIA A10G GPU, we subtract a 5% buffer for frameworks\nsuch as vLLM and TensorRT-LLM and another 1.3 GiB to allow sufficient space for a vision encoder.\nMeasuring throughput.\nFor the experiments below, unless otherwise specified, we measure\nthroughput on an input and output sequence length of 8k and 16k tokens respectively, which we\nbelieve represents a typical reasoning scenario. For this combination of input and output sequence\nlength, we report vLLM output token generation throughput at the maximum batch size that fits on\nthe A10G GPU.\n19\n\nNVIDIA Nemotron Nano 2: An Accurate and Efficient Hybrid Mamba-Transformer Reasoning Model\n4.2.1. Candidate enumeration.\nOur compression strategy explores multiple axes within the 19.66 GiB memory budget through\ncombinatorial pruning. Our search space includes depth reduction (removing 6-10 layers from the\noriginal 62-layer architecture) combined with width pruning of embedding channels (4480-5120),\nFFN dimension (13440-20480), and Mamba heads (112-128). This multi-axis search space results in\nhundreds of candidate architectures meeting the memory constraint.\n4.2.2. Finding the Best Architecture\nSince performing knowledge distillation and throughput benchmarking on the full set of candidates\nwould be prohibitively expensive, we break down the problem into two parts: (1) find the optimal\ndepth for the compressed model, and (2) find the optimal width-pruned architecture given the depth.\nEffect of depth.\nWe compare the accuracy of three depth-pruned candidates obtained from the\n12B model with 52, 54 and 56 layers. Here, we keep the number of attention layers fixed at 4 for all\nthree variants so as to achieve a good balance between KV cache size and long-context performance;\nprior work has indicated that an attention-to-total-layers ratio between 7-8% is reasonable (NVIDIA,\n2025). We leave the width dimensions untouched for this experiment. Table 9 lists average reasoning\naccuracy at different depths after 6B tokens of distillation; in line with our previous observations on\nthe strong correlation between depth and task performance (Muralidharan et al., 2024; Sreenivas et al.,\n2024), we notice that reducing depth beyond 56 layers results in significant accuracy degradation; as\na result, we fix the depth at 56 for further width pruning.\nAccuracy (Avg)\n52 Layers\n44.92\n54 Layers\n47.35\n56 Layers\n51.48\nTable 9 | Effect of depth on reasoning accuracy. Results are after distilling with 6B tokens.\nCombining depth and width pruning.\nAs described above, we fix the depth of our target\nmodel to 56 layers with 4 attention layers. We perform 60B tokens of distillation on this checkpoint\n(see Section 4.3 for additional details) and perform further width pruning along the embedding, FFN,\nand Mamba axes. We enumerate all candidate pruned architectures that meet our memory budget,\nand sort them in decreasing order of estimated memory consumption at 128k context length and\nbatch size 1. The top 3 candidates from this list are picked for further evaluation: in particular, we\nperform short Knowledge Distillation (KD) on these candidates for 19B tokens after depth+width\npruning; we also benchmark throughput to pick the final architectural candidate. Table 10 lists the\narchitectural details of the top 3 candidates, along with the achieved task performance (post KD)\nand throughput. As shown in the Table, Candidate 2 achieves the best accuracy while still having\nreasonable runtime performance; consequently, we use this architecture for Nano 2.\nFFN vs.\nMamba pruning.\nWe ablate the number of Mamba heads following the recipe\nin Taghibakhshi et al. (2025), considering configurations with 87.5% and 93.75% of the original\nheads. However, due to the relatively smaller compression ratios explored in this work (less than 15%\nafter depth pruning) compared to those in Taghibakhshi et al. (2025) (around 50%), we find that\napplying Mamba head pruning yields limited benefit, and in these cases, pruning only the FFN and\n20\n\nNVIDIA Nemotron Nano 2: An Accurate and Efficient Hybrid Mamba-Transformer Reasoning Model\n#Layers Hidden\nFFN\nMamba #Heads Params. (B) Accuracy Throughput\nCandidate 1\n56\n4480\n17920\n112\n8.92\n59.07\n161.02\nCandidate 2\n56\n4480\n15680\n128\n8.89\n63.02\n156.42\nCandidate 3\n56\n4800\n14400\n120\n8.97\n62.94\n155.86\nTable 10 | Top 3 candidates for architecture selection. Accuracy is the average across reasoning\nbenchmarks after distillation with 19B tokens. The last column shows vLLM output generation\nthroughput (ISL/OSL=8k/16k and batch size=8).\nembedding dimensions—after depth pruning—proves sufficient to achieve the desired compression\nwhile preserving accuracy. Candidates 1 and 2 in Table 10 highlight this difference.\n4.3. Retraining with Distillation\nTo recover the accuracy lost due to pruning, the model undergoes continued training. Recent work\nhas demonstrated that distilling knowledge from the original model to the pruned model outperforms\nconventional fine-tuning (Muralidharan et al., 2024; Sreenivas et al., 2024; Bercovich et al., 2024);\nwe thus adopt logit-based distillation for continued training, employing forward KL divergence loss\nexclusively during the accuracy recovery phase (see §3 of the Minitron paper (Muralidharan et al.,\n2024) for more details on the distillation loss formulation). Building on the candidate selection\nprocess described in §4.2, we continue training Candidate 2 in an extended phase, as detailed below,\nto yield the final Nano 2 reasoning and base models.\n% Reasoning-SFT data\n% Pretraining data\nAccuracy (Avg)\n50\n50\n57.5\n70\n30\n58.5\n90\n10\n57.2\nTable 11 | Effect of varying reasoning data proportion on math accuracy after ∼6B tokens of KD.\nReasoning model.\nThe reasoning model is distilled in stages with increasing sequence lengths to\nstrengthen extended reasoning and long-context capabilities; this is followed by targeted reinforcement\nlearning (RL), preference optimization and model merging to retain desired behaviors and ensure\nrobustness across diverse tasks. We now describe these various stages:\n1. Depth pruning to 56 layers; Knowledge Distillation (KD) with ∼60B tokens at 8,192 sequence\nlength.\n2. Width pruning and KD with:\n• ∼50B tokens at 8,192 sequence length.\n• ∼25B tokens at 49,152 sequence length.\n• ∼1B tokens at 262,144 sequence length.\n3. Direct Preference Optimization (DPO).\n4. Group Relative Policy Optimization (GRPO).\n5. KD with ∼0.4B tokens at 262,144 sequence length to recover post-RL drops.\n6. RLHF for alignment with human preferences.\n7. Model merging between steps 5 and 6 via 0.5 linear interpolation.\n21\n\nNVIDIA Nemotron Nano 2: An Accurate and Efficient Hybrid Mamba-Transformer Reasoning Model\nMore details on DPO, GRPO and RLHF can be found in Section 3. Figure 6 shows the effects of\nstaged training on model accuracy across different reasoning benchmarks. Here, the 𝑥-axis represents\nthe various stages (starting from Step 2 above), and the 𝑦-axis shows the scores obtained for the\nvarious benchmarks as training progresses. As shown in the Figure, DPO and GRPO are critical for\nenhancing function-calling (BFCL v3) and instruction-following (IFEval) capabilities, though the\nlatter temporarily degrades multi-task understanding (MMLU-Pro), which is recovered in the next\nstep (post-GRPO KD). Finally, RLHF enhances alignment with human preferences (Arena-Hard)\nbut causes additional benchmark drops, which are then recovered through model merging.\nKD+LCExt\nDPO\nGRPO\nKD\nRLHF\nMerge\nPipeline Stage\n50\n55\n60\n65\n70\n75\n80\n85\n90\nScore (%)\nDistillation Pipeline\nAIME-25\nGPQA-D\nBFCLv3\nIFEval (Pr.)\nMMLU-Pro\nArenaHard\nLiveCodeBench\nFigure 6 | Task accuracy at different stages of the distillation pipeline for Nemotron Nano 2.\nDataset: We observe that a mix of 70% post-training stage 2 data (Section 3.2) and 30% pretraining\n(Section 2.2) data yields the highest accuracy (Table 11). For KD at sequence length 262,144, we\nuse 100% stage 3 post-training data (Section 3.2).\nBase model.\nDistillation proceeds in stages: depth-only pruning and KD on ∼120B tokens,\nfollowed by width pruning and KD on ∼360B tokens (both at sequence length 8,192), and finally\nKD on ∼2.5B tokens at sequence length 524,288 to instill long-context capabilities.\nDataset: Following Sreenivas et al. (2024), we use 100% pretraining data described in sections 2.2\nand 2.6 for distillation of the base model at sequence lengths 8,192 and 524,288, respectively.\n4.4. Results\nWe efficiently compress the 12B model to 9B parameters by pruning full layers (depth), FFN hidden\nsize, and embedding channels, improving inference throughput and enabling long-context inference on\nan NVIDIA A10G GPU. Nemotron-Nano-9B-v2 retains 56 layers of the original model. Additionally,\nthe number of embedding channels were pruned from 5120 to 4480, and FFN intermediate size was\npruned from 20480 to 15680. As shown in Figure 1 and Tables 5 and 6, Nemotron-Nano-9B-v2\nachieves 3×-6× higher throughput than Qwen3-8B for generation-heavy scenarios, while surpassing\n22\n\nNVIDIA Nemotron Nano 2: An Accurate and Efficient Hybrid Mamba-Transformer Reasoning Model\nit in accuracy and remaining comparable to the 12B teacher on most benchmarks.\n5. Conclusion\nIn this report, we introduced Nemotron-Nano-9B-v2, a hybrid Mamba-Transformer reasoning model\nthat achieves comparable or better accuracies at up to 6× higher throughput than existing state-\nof-the-art models such as Qwen3-8B. To create Nemotron-Nano-9B-v2, we started by pre-training\nNemotron-Nano-12B-v2-Base on 20T tokens, using a carefully constructed mix of curated and\nsynthetically generated data. We aligned Nemotron-Nano-12B-v2-Base using several stages of SFT,\nGRPO, DPO, and RLHF before using the Minitron compression via pruning and distillation strategy\nto produce the final model. As a result of this compression, Nemotron-Nano-9B-v2 can run inference\non context lengths of up to 128k tokens in bfloat16 precision on a single NVIDIA A10G GPU with\n22 GiB of memory. We have open-sourced Nemotron-Nano-9B-v2 along with its corresponding sibling\nNemotron-Nano-9B-v2-Base and parent Nemotron-Nano-12B-v2-Base models, plus the majority of\nits pre- and post-training data on HuggingFace (links at the bottom of Section 1).\nContributors\nWe thank the following people for their invaluable contributions to NVIDIA Nemotron Nano 2.\nData. Abhinav Khattar, Aleksander Ficek, Arham Mehta, Ayush Dattagupta, Brandon Norick,\nDan Su, Daria Gitman, Evelina Bakhturina, Igor Gitman, Ivan Moshkov, Jaehun Jung, Jane Polak\nScowcroft, Jocelyn Huang, Joseph Jennings, Jupinder Parmar, Markus Kliegl, Matvei Novikov,\nMehrzad Samadi, Miguel Martinez, Mohammad Shoeybi, Mostofa Patwary, Pavlo Molchanov, Pritam\nGundecha, Rabeeh Karimi Mahabadi, Ranjit Rajan, Rima Shahbazyan, Sanjeev Satheesh, Sarah\nYurick, Sean Narenthiran, Seungju Han, Shizhe Diao, Shrimai Prabhumoye, Shubham Toshniwal,\nSiddhartha Jain, Somshubra Majumdar, Syeda Nahida Akter, Vahid Noroozi, Vineeth Kalluru,\nVitaly Kurin, Wasi Uddin Ahmad, Wei Du, Ximing Lu, Yejin Choi, Ying Lin.\nFP8. Hua Huang, Jinze Xue, Keith Wyss, Kunlun Li, Mike Chrzanowski, Oleg Rybakov, Przemek\nTredak, Tim Moon, Zhongbo Zhu.\nArchitecture. Bita Darvish Rouhani, Brandon Norick, Duncan Riach, Nidhi Bhatia, Roger Waleffe,\nWonmin Byeon, Ritika Borkar, Xin Dong, Yonggan Fu.\nPretraining. Aarti Basant, Abhijit Paithankar, Abhinav Khattar, Deepak Narayanan, Herman\nSahota, Hexin Wang, Jupinder Parmar, Mohammad Shoeybi, Mostofa Patwary, Namit Dhameja,\nRoger Waleffe, Russell J. Hewett, Ryan Prenger, Seonmyeong Bak.\nInfrastructure. Alex Kondratenko, Alex Shaposhnikov, Anubhav Mandarwal, Ashwin Poojary,\nDong Ahn, Gargi Prasad, Haim Elisha, Harsh Sharma, Kumar Anik, Maer Rodrigues de Melo, Ruoxi\nZhang, Shelby Thomas, Stefania Alborghetti, Tony Wang.\nLong Context. Deepak Narayanan, Dima Rekesh, Duncan Riach, John Kamalu, Kezhi Kong,\nMarkus Kliegl, Roger Waleffe, Samuel Kriman.\nInference. Daniel Afrimi, Helen Ngo, Keshav Santhanam, Kushan Ahmadian, Lawrence McAfee,\nLuis Vega, Nave Assaf, Peter Dykas, Shanmugam Ramasamy, Siddharth Singh, Tomer Asida, Vijay\nKorthikanti.\nAlignment. Adithya Renduchintala, Alexander Bukharin, Ameya Sunil Mahabaleshwarkar, Banghua\nZhu, Bilal Kartal, Brian Yu, Charles Wang, Christian Munley, David Mosallanezhad, Gerald Shen,\nHaifeng Qian, Hayley Ross, Hoo Chang Shin, Igor Gitman, Jian Zhang, Jiaqi Zeng, Julien Veron\n23\n\nNVIDIA Nemotron Nano 2: An Accurate and Efficient Hybrid Mamba-Transformer Reasoning Model\nVialard, Junkeun Yi, Kezhi Kong, Luis Vega, Makesh Narsimhan Sreedhar, Oleksii Hrinchuk, Oleksii\nKuchaiev, Peter Jin, Prasoon Varshney, Ritu Gala, Shuoyang Ding, Soumye Singhal, Tugrul Konuk,\nVenkat Srinivasan, Vitaly Lavrukhin, Yian Zhang, Yoshi Suhara, Zhen Dong, Zijia Chen.\nCompression. Aditya Malte, Akhiad Bercovich, Akshay Hazare, Ali Taghibakhshi, Ameya Sunil\nMahabaleshwarkar, Ashwath Aithal, Banghua Zhu, Daniel Korzekwa, Deepak Narayanan, Gerald\nShen, Hayley Ross, Julien Veron Vialard, Luis Vega, Marcin Chochowski, Mostofa Patwary, Nima\nTajbakhsh, Oluwatobi Olabiyi, Pavlo Molchanov, Ran El-Yaniv, Roger Waleffe, Saurav Muralidharan,\nSepehr Sameni, Sharath Turuvekere Sreenivas, Tomer Asida, Yashaswi Karnati, Yian Zhang, Yoshi\nSuhara, Zijia Chen.\nSoftware Support. Abhijit Khairnar, Adithya Renduchintala, Ali Taghibakhshi, Anna Shors,\nAshwath Aithal, Balaram Buddharaju, Bobby Chen, Charlie Truong, Deepak Narayanan, Dmytro\nPykhtar, Duncan Riach, Gerald Shen, Helen Ngo, Jared Casper, Jimmy Zhang, Keshav Santhanam,\nKezhi Kong, Lawrence McAfee, Luis Vega, Nima Tajbakhsh, Parth Chadha, Piotr Bialecki, Prashant\nGaikwad, Rajen Patel, Roger Waleffe, Sahil Jain, Terry Kong, Tyler Poon, Vijay Korthikanti, Vikram\nFugro, Yoshi Suhara, Zhiyu Li.\nEvaluations and Safety.\nChristopher Parisien, Dan Su, Daniel Rohrer, Eileen Long, Erick\nGalinkin, Helen Ngo, Katherine Luna, Keshav Santhanam, Kezhi Kong, Leon Derczynski, Marta\nStepniewska-Dziubinska, Meriem Boubdir, Michal Bien, Michael Boone, Michael Evans, Michal Bien,\nMichal Zawalski, Pablo Ribalta, Piotr Januszewski, Pradeep Thalasta, Sanjeev Satheesh, Shaona\nGhosh, Tomasz Hliwiak.\nLegal and Compliance. Barnaby Simkin, Chetan Mungekar, Dina Yared, Iain Cunningham,\nKatherine Cheung, Laya Sleiman, Meredith Price, Michael Boone, Nikki Pope, Ria Cheruvu, Saori\nKaji.\nMarketing. Amelia Barton, Chris Alexiuk, Mark Cai, Nirmal Kumar Juluru, Shreya Gopal.\nProject Management. Alejandra Rico, Amy Shen, Ann Guan, Ashton Sharabiani, Elliott Ning,\nKrzysztof Pawelec, Negar Habibi, Twinkle Vashishth.\nProduct. Arun Venkatesan, Chintan Patel, Chris Alexiuk, Joey Conway, Padmavathy Subramanian,\nUdi Karpas.\nLeadership. Andrew Tao, Boris Ginsburg, Bryan Catanzaro, Eric Chung, Jan Kautz, Joey Conway,\nJonathan Cohen, Kari Briski, Mohammad Shoeybi, Mostofa Patwary, Oleksii Kuchaiev, Pavlo\nMolchanov.\nWe also thank Chen Zhang, Michael Goin, Thomas Parnell from the vLLM team for their assistance.\n24\n\nNVIDIA Nemotron Nano 2: An Accurate and Efficient Hybrid Mamba-Transformer Reasoning Model\nReferences\nGretel synthetic safety alignment dataset, 12 2024. URL https://huggingface.co/datasets/\ngretelai/gretel-safety-alignment-en-v1.\nMarah Abdin, Jyoti Aneja, Harkirat Behl, Sébastien Bubeck, Ronen Eldan, Suriya Gunasekar,\nMichael Harrison, Russell J Hewett, Mojan Javaheripi, Piero Kauffmann, et al. Phi-4 technical\nreport. arXiv preprint arXiv:2412.08905, 2024.\nWasi Uddin Ahmad, Aleksander Ficek, Mehrzad Samadi, Jocelyn Huang, Vahid Noroozi, Somshubra\nMajumdar, and Boris Ginsburg. Opencodeinstruct: A large-scale instruction tuning dataset for\ncode llms. arXiv preprint arXiv:2504.04030, 2025a.\nWasi Uddin Ahmad, Sean Narenthiran, Somshubra Majumdar, Aleksander Ficek, Siddhartha\nJain, Jocelyn Huang, Vahid Noroozi, and Boris Ginsburg. Opencodereasoning: Advancing data\ndistillation for competitive coding. arXiv preprint arXiv:2504.01943, 2025b.\nJoshua Ainslie, James Lee-Thorp, Michiel de Jong, Yury Zemlyanskiy, Federico Lebrón, and Sumit\nSanghai. GQA: Training Generalized Multi-Query Transformer Models from Multi-Head Check-\npoints, 2023. URL https://arxiv.org/abs/2305.13245.\nSyeda Nahida Akter, Shrimai Prabhumoye, John Kamalu, Sanjeev Satheesh, Eric Nyberg, Mostofa\nPatwary, Mohammad Shoeybi, and Bryan Catanzaro. MIND: Math Informed syNthetic Dialogues\nfor Pretraining LLMs, 2024. URL https://arxiv.org/abs/2410.12881.\nLoubna Ben Allal, Anton Lozhkov, Elie Bakouch, Gabriel Martín Blázquez, Guilherme Penedo, Lewis\nTunstall, Andrés Marafioti, Hynek Kydlíček, Agustín Piqueres Lajarín, Vaibhav Srivastav, Joshua\nLochner, Caleb Fahlgren, Xuan-Son Nguyen, Clémentine Fourrier, Ben Burtenshaw, Hugo Larcher,\nHaojun Zhao, Cyril Zakka, Mathieu Morlon, Colin Raffel, Leandro von Werra, and Thomas Wolf.\nSmolLM2: When Smol Goes Big – Data-Centric Training of a Small Language Model, 2025. URL\nhttps://arxiv.org/abs/2502.02737.\nZeyuan Allen-Zhu and Yuanzhi Li. Physics of language models: Part 3.1, knowledge storage and\nextraction, 2024. URL https://arxiv.org/abs/2309.14316.\nJacob Austin, Augustus Odena, Maxwell Nye, Maarten Bosma, Henryk Michalewski, David Dohan,\nEllen Jiang, Carrie Cai, Michael Terry, Quoc Le, and Charles Sutton. Program Synthesis with\nLarge Language Models, 2021. URL https://arxiv.org/abs/2108.07732.\nShuai Bai, Keqin Chen, Xuejing Liu, Jialin Wang, Wenbin Ge, Sibo Song, Kai Dang, Peng Wang,\nShijie Wang, Jun Tang, Humen Zhong, Yuanzhi Zhu, Mingkun Yang, Zhaohai Li, Jianqiang Wan,\nPengfei Wang, Wei Ding, Zheren Fu, Yiheng Xu, Jiabo Ye, Xi Zhang, Tianbao Xie, Zesen Cheng,\nHang Zhang, Zhibo Yang, Haiyang Xu, and Junyang Lin. Qwen2.5-vl technical report, 2025. URL\nhttps://arxiv.org/abs/2502.13923.\nAdrien Barbaresi. Trafilatura: A Web Scraping Library and Command-Line Tool for Text Dis-\ncovery and Extraction. In Proceedings of the Joint Conference of the 59th Annual Meeting of\nthe Association for Computational Linguistics and the 11th International Joint Conference on\nNatural Language Processing: System Demonstrations, pp. 122–131. Association for Computational\nLinguistics, 2021. URL https://aclanthology.org/2021.acl-demo.15.\nAkhiad Bercovich, Tomer Ronen, Talor Abramovich, Nir Ailon, Nave Assaf, Mohammad Dabbah,\nIdo Galil, Amnon Geifman, Yonatan Geifman, Izhak Golan, Netanel Haber, Ehud Karpas, Roi\nKoren, Itay Levy, Pavlo Molchanov, Shahar Mor, Zach Moshe, Najeeb Nabwani, Omri Puny,\n25\n\nNVIDIA Nemotron Nano 2: An Accurate and Efficient Hybrid Mamba-Transformer Reasoning Model\nRan Rubin, Itamar Schen, Ido Shahaf, Oren Tropp, Omer Ullman Argov, Ran Zilberstein, and\nRan El-Yaniv.\nPuzzle: Distillation-Based NAS for Inference-Optimized LLMs, 2024.\nURL\nhttps://arxiv.org/abs/2411.19146.\nAkhiad Bercovich, Itay Levy, Izik Golan, Mohammad Dabbah, Ran El-Yaniv, Omri Puny, Ido\nGalil, Zach Moshe, Tomer Ronen, Najeeb Nabwani, Ido Shahaf, Oren Tropp, Ehud Karpas,\nRan Zilberstein, Jiaqi Zeng, Soumye Singhal, Alexander Bukharin, Yian Zhang, Tugrul Konuk,\nGerald Shen, Ameya Sunil Mahabaleshwarkar, Bilal Kartal, Yoshi Suhara, Olivier Delalleau, Zijia\nChen, Zhilin Wang, David Mosallanezhad, Adi Renduchintala, Haifeng Qian, Dima Rekesh, Fei\nJia, Somshubra Majumdar, Vahid Noroozi, Wasi Uddin Ahmad, Sean Narenthiran, Aleksander\nFicek, Mehrzad Samadi, Jocelyn Huang, Siddhartha Jain, Igor Gitman, Ivan Moshkov, Wei Du,\nShubham Toshniwal, George Armstrong, Branislav Kisacanin, Matvei Novikov, Daria Gitman,\nEvelina Bakhturina, Jane Polak Scowcroft, John Kamalu, Dan Su, Kezhi Kong, Markus Kliegl,\nRabeeh Karimi, Ying Lin, Sanjeev Satheesh, Jupinder Parmar, Pritam Gundecha, Brandon\nNorick, Joseph Jennings, Shrimai Prabhumoye, Syeda Nahida Akter, Mostofa Patwary, Abhinav\nKhattar, Deepak Narayanan, Roger Waleffe, Jimmy Zhang, Bor-Yiing Su, Guyue Huang, Terry\nKong, Parth Chadha, Sahil Jain, Christine Harvey, Elad Segal, Jining Huang, Sergey Kashirsky,\nRobert McQueen, Izzy Putterman, George Lam, Arun Venkatesan, Sherry Wu, Vinh Nguyen,\nManoj Kilaru, Andrew Wang, Anna Warno, Abhilash Somasamudramath, Sandip Bhaskar, Maka\nDong, Nave Assaf, Shahar Mor, Omer Ullman Argov, Scot Junkin, Oleksandr Romanenko, Pedro\nLarroy, Monika Katariya, Marco Rovinelli, Viji Balas, Nicholas Edelman, Anahita Bhiwandiwalla,\nMuthu Subramaniam, Smita Ithape, Karthik Ramamoorthy, Yuting Wu, Suguna Varshini Velury,\nOmri Almog, Joyjit Daw, Denys Fridman, Erick Galinkin, Michael Evans, Katherine Luna, Leon\nDerczynski, Nikki Pope, Eileen Long, Seth Schneider, Guillermo Siman, Tomasz Grzegorzek, Pablo\nRibalta, Monika Katariya, Joey Conway, Trisha Saar, Ann Guan, Krzysztof Pawelec, Shyamala\nPrayaga, Oleksii Kuchaiev, Boris Ginsburg, Oluwatobi Olabiyi, Kari Briski, Jonathan Cohen, Bryan\nCatanzaro, Jonah Alben, Yonatan Geifman, Eric Chung, and Chris Alexiuk. Llama-nemotron:\nEfficient reasoning models, 2025a. URL https://arxiv.org/abs/2505.00949.\nAkhiad Bercovich, Itay Levy, Izik Golan, Mohammad Dabbah, Ran El-Yaniv, Omri Puny, Ido Galil,\nZach Moshe, Tomer Ronen, Najeeb Nabwani, et al. Llama-nemotron: Efficient reasoning models.\narXiv preprint arXiv:2505.00949, 2025b.\nJanek Bevendorff, Benno Stein, Matthias Hagen, and Martin Potthast. Elastic ChatNoir: Search\nEngine for the ClueWeb and the Common Crawl. In Leif Azzopardi, Allan Hanbury, Gabriella Pasi,\nand Benjamin Piwowarski (eds.), Advances in Information Retrieval. 40th European Conference\non IR Research (ECIR 2018), Lecture Notes in Computer Science, Berlin Heidelberg New York,\nMarch 2018. Springer.\nYonatan Bisk, Rowan Zellers, Ronan Le Bras, Jianfeng Gao, and Yejin Choi. PIQA: Reasoning about\nPhysical Commonsense in Natural Language, 2019. URL https://arxiv.org/abs/1911.11641.\nAndrei Z Broder. Identifying and filtering near-duplicate documents. In Annual symposium on\ncombinatorial pattern matching, pp. 1–10. Springer, 2000.\nMark Chen, Jerry Tworek, Heewoo Jun, Qiming Yuan, Henrique Ponde de Oliveira Pinto, Jared\nKaplan, et al. Evaluating Large Language Models Trained on Code, 2021. URL https://arxiv.\norg/abs/2107.03374.\nPaul F. Christiano, Jan Leike, Tom B. Brown, Miljan Martic, Shane Legg, and Dario Amodei. Deep\nreinforcement learning from human preferences. In Advances in Neural Information Processing\nSystems (NeurIPS), NIPS ’17, 2017.\n26\n\nNVIDIA Nemotron Nano 2: An Accurate and Efficient Hybrid Mamba-Transformer Reasoning Model\nPeter Clark, Isaac Cowhey, Oren Etzioni, Tushar Khot, Ashish Sabharwal, Carissa Schoenick, and\nOyvind Tafjord. Think you have Solved Question Answering? Try ARC, the AI2 Reasoning\nChallenge. ArXiv, abs/1803.05457, 2018.\nKarl Cobbe, Vineet Kosaraju, Mohammad Bavarian, Mark Chen, Heewoo Jun, Lukasz Kaiser,\nMatthias Plappert, Jerry Tworek, Jacob Hilton, Reiichiro Nakano, Christopher Hesse, and John\nSchulman. Training Verifiers to Solve Math Word Problems, 2021. URL https://arxiv.org/\nabs/2110.14168.\nTri Dao and Albert Gu. Transformers are SSMs: Generalized Models and Efficient Algorithms\nThrough Structured State Space Duality, 2024. URL https://arxiv.org/abs/2405.21060.\nGemma Team @ Google DeepMind. Gemma 3 Technical Report, 2025. URL https://arxiv.org/\nabs/2503.19786.\nDeepSeek-AI. DeepSeek-R1: Incentivizing Reasoning Capability in LLMs via Reinforcement Learning,\n2025a. URL https://arxiv.org/abs/2501.12948.\nDeepSeek-AI. DeepSeek-V3 Technical Report, 2025b. URL https://arxiv.org/abs/2412.19437.\nIstván Endrédy and Attila Novák. More effective boilerplate removal-the goldminer algorithm.\nPolibits, 48:79–83, 12 2013. doi: 10.17562/PB-48-10.\nSteven Feng, Shrimai Prabhumoye, Kezhi Kong, Dan Su, Mostofa Patwary, Mohammad Shoeybi, and\nBryan Catanzaro. Maximize Your Data’s Potential: Enhancing LLM Accuracy with Two-Phase\nPretraining, 2024. URL https://arxiv.org/abs/2412.15285.\nShaona Ghosh, Prasoon Varshney, Makesh Narsimhan Sreedhar, Aishwarya Padmakumar, Traian\nRebedea, Jibin Rajan Varghese, and Christopher Parisien. AEGIS2.0: A diverse AI safety dataset\nand risks taxonomy for alignment of LLM guardrails. In Luis Chiruzzo, Alan Ritter, and Lu Wang\n(eds.), Proceedings of the 2025 Conference of the Nations of the Americas Chapter of the Association\nfor Computational Linguistics: Human Language Technologies (Volume 1: Long Papers), pp.\n5992–6026, Albuquerque, New Mexico, April 2025. Association for Computational Linguistics.\nISBN 979-8-89176-189-6. doi: 10.18653/v1/2025.naacl-long.306. URL https://aclanthology.\norg/2025.naacl-long.306/.\nXiaotian Han, Yiren Jian, Xuefeng Hu, Haogeng Liu, Yiqi Wang, Qihang Fan, Yuang Ai, Huaibo\nHuang, Ran He, Zhenheng Yang, and Quanzeng You. Infimm-webmath-40b: Advancing multimodal\npre-training for enhanced mathematical reasoning, 2024. URL https://arxiv.org/abs/2409.\n12568.\nAdib Hasan, Ileana Rugina, and Alex Wang. Pruning for protection: Increasing jailbreak resistance\nin aligned llms without fine-tuning. arXiv preprint arXiv:2401.10862, 2024.\nDan Hendrycks, Collin Burns, Steven Basart, Andy Zou, Mantas Mazeika, Dawn Song, and\nJacob Steinhardt. Measuring Massive Multitask Language Understanding, 2021a. URL https:\n//arxiv.org/abs/2009.03300.\nDan Hendrycks, Collin Burns, Saurav Kadavath, Akul Arora, Steven Basart, Eric Tang, Dawn Song,\nand Jacob Steinhardt. Measuring Mathematical Problem Solving With the MATH Dataset, 2021b.\nURL https://arxiv.org/abs/2103.03874.\nCheng-Ping Hsieh, Simeng Sun, Samuel Kriman, Shantanu Acharya, Dima Rekesh, Fei Jia, Yang\nZhang, and Boris Ginsburg. Ruler: What’s the real context size of your long-context language\nmodels? arXiv preprint arXiv:2404.06654, 2024.\n27\n\nNVIDIA Nemotron Nano 2: An Accurate and Efficient Hybrid Mamba-Transformer Reasoning Model\nShengding Hu, Yuge Tu, Xu Han, Ganqu Cui, Chaoqun He, Weilin Zhao, Xiang Long, Zhi Zheng,\nYewei Fang, Yuxiang Huang, Xinrong Zhang, Zhen Leng Thai, Chongyi Wang, Yuan Yao,\nChenyang Zhao, Jie Zhou, Jie Cai, Zhongwu Zhai, Ning Ding, Chao Jia, Guoyang Zeng, dahai\nli, Zhiyuan Liu, and Maosong Sun. MiniCPM: Unveiling the potential of small language models\nwith scalable training strategies. In First Conference on Language Modeling, 2024. URL https:\n//openreview.net/forum?id=3X2L2TFr0f.\nSiming Huang, Tianhao Cheng, J. K. Liu, Jiaran Hao, Liuyihan Song, Yang Xu, J. Yang, Jiaheng Liu,\nChenchen Zhang, Linzheng Chai, Ruifeng Yuan, Zhaoxiang Zhang, Jie Fu, Qian Liu, Ge Zhang,\nZili Wang, Yuan Qi, Yinghui Xu, and Wei Chu. Opencoder: The open cookbook for top-tier code\nlarge language models, 2025. URL https://arxiv.org/abs/2411.04905.\nPiotr Indyk and Rajeev Motwani. Approximate nearest neighbors: towards removing the curse of\ndimensionality. In Proceedings of the thirtieth annual ACM symposium on Theory of computing,\npp. 604–613, 1998.\nNaman Jain, King Han, Alex Gu, Wen-Ding Li, Fanjia Yan, Tianjun Zhang, Sida Wang, Armando\nSolar-Lezama, Koushik Sen, and Ion Stoica. Livecodebench: Holistic and contamination free\nevaluation of large language models for code. arXiv preprint arXiv:2403.07974, 2024.\nTianle Li, Wei-Lin Chiang, Evan Frick, Lisa Dunlap, Banghua Zhu, Joseph E. Gonzalez, and Ion\nStoica. From Live Data to High-Quality Benchmarks: The Arena-Hard Pipeline, April 2024a.\nURL https://lmsys.org/blog/2024-04-19-arena-hard/.\nXuehai Li, Zi Ye, Xiaoxin Zhang, Xinshi Lu, Yingqiang Xia, Bairu Wu, Shihan Dong, Qipeng Jin,\nJialu Wang, Heng Ji, et al. Wildchat: 1m chatgpt interaction logs in the wild. arXiv preprint\narXiv:2405.01470, 2024b.\nOpher Lieber, Barak Lenz, Hofit Bata, Gal Cohen, Jhonathan Osin, Itay Dalmedigos, Erez Safahi,\nShaked Meirom, Yonatan Belinkov, Shai Shalev-Shwartz, Omri Abend, Raz Alon, Tomer Asida,\nAmir Bergman, Roman Glozman, Michael Gokhman, Avashalom Manevich, Nir Ratner, Noam\nRozen, Erez Shwartz, Mor Zusman, and Yoav Shoham. Jamba: A Hybrid Transformer-Mamba\nLanguage Model, 2024. URL https://arxiv.org/abs/2403.19887.\nHunter Lightman, Vineet Kosaraju, Yura Burda, Harri Edwards, Bowen Baker, Teddy Lee, Jan\nLeike, John Schulman, Ilya Sutskever, and Karl Cobbe. Let’s Verify Step by Step. arXiv preprint\narXiv:2305.20050, 2023.\nWang Ling, Dani Yogatama, Chris Dyer, and Phil Blunsom. Program induction by rationale genera-\ntion: Learning to solve and explain algebraic word problems. arXiv preprint arXiv:1705.04146,\n2017.\nJian Liu, Leyang Cui, Hanmeng Liu, Dandan Huang, Yile Wang, and Yue Zhang. Logiqa: A\nchallenge dataset for machine reading comprehension with logical reasoning. arXiv preprint\narXiv:2007.08124, 2020.\nJiawei Liu, Chunqiu Steven Xia, Yuyao Wang, and Lingming Zhang. Is Your Code Generated by\nChatGPT Really Correct? Rigorous Evaluation of Large Language Models for Code Generation.\narXiv preprint arXiv:2305.01210, 2023. doi: https://doi.org/10.48550/arXiv.2305.01210. URL\nhttps://arxiv.org/abs/2305.01210.\nZuxin Liu et al. Toolace: Winning the points of llm function calling. arXiv preprint arXiv:2409.00920,\n2024.\n28\n\nNVIDIA Nemotron Nano 2: An Accurate and Efficient Hybrid Mamba-Transformer Reasoning Model\nAnton Lozhkov, Raymond Li, Loubna Ben Allal, Federico Cassano, Joel Lamy-Poirier, Nouamane\nTazi, Ao Tang, Dmytro Pykhtar, Jiawei Liu, Yuxiang Wei, Tianyang Liu, Max Tian, Denis\nKocetkov, Arthur Zucker, Younes Belkada, Zijian Wang, Qian Liu, Dmitry Abulkhanov, Indraneil\nPaul, Zhuang Li, Wen-Ding Li, Megan Risdal, Jia Li, Jian Zhu, Terry Yue Zhuo, Evgenii\nZheltonozhskii, Nii Osae Osae Dade, Wenhao Yu, Lucas Krauß, Naman Jain, Yixuan Su, Xuanli\nHe, Manan Dey, Edoardo Abati, Yekun Chai, Niklas Muennighoff, Xiangru Tang, Muhtasham\nOblokulov, Christopher Akiki, Marc Marone, Chenghao Mou, Mayank Mishra, Alex Gu, Binyuan\nHui, Tri Dao, Armel Zebaze, Olivier Dehaene, Nicolas Patry, Canwen Xu, Julian McAuley, Han Hu,\nTorsten Scholak, Sebastien Paquet, Jennifer Robinson, Carolyn Jane Anderson, Nicolas Chapados,\nMostofa Patwary, Nima Tajbakhsh, Yacine Jernite, Carlos Muñoz Ferrandis, Lingming Zhang,\nSean Hughes, Thomas Wolf, Arjun Guha, Leandro von Werra, and Harm de Vries. Starcoder 2\nand the stack v2: The next generation, 2024. URL https://arxiv.org/abs/2402.19173.\nWeidi Luo, Siyuan Ma, Xiaogeng Liu, Xiaoyu Guo, and Chaowei Xiao. Jailbreakv: A benchmark for\nassessing the robustness of multimodal large language models against jailbreak attacks. arXiv\npreprint arXiv:2404.03027, 2024.\nRabeeh Karimi Mahabadi, Sanjeev Satheesh, Shrimai Prabhumoye, Mostofa Patwary, Mohammad\nShoeybi, and Bryan Catanzaro. Nemotron-cc-math: A 133 billion-token-scale high quality math\npretraining dataset, 2025. URL https://arxiv.org/abs/2508.15096.\nSomshubra Majumdar, Vahid Noroozi, Mehrzad Samadi, Sean Narenthiran, Aleksander Ficek,\nWasi Uddin Ahmad, Jocelyn Huang, Jagadeesh Balam, and Boris Ginsburg. Genetic instruct:\nScaling up synthetic generation of coding instructions for large language models. arXiv preprint\narXiv:2407.21077, 2024.\nLlama Team @ Meta. The Llama 3 Herd of Models, 2024. URL https://arxiv.org/abs/2407.\n21783.\nTodor Mihaylov, Peter Clark, Tushar Khot, and Ashish Sabharwal. Can a Suit of Armor Conduct\nElectricity? A New Dataset for Open Book Question Answering, 2018. URL https://arxiv.\norg/abs/1809.02789.\nIvan Moshkov, Darragh Hanley, Ivan Sorokin, Shubham Toshniwal, Christof Henkel, Benedikt Schif-\nferer, Wei Du, and Igor Gitman. Aimo-2 winning solution: Building state-of-the-art mathematical\nreasoning models with openmathreasoning dataset. arXiv preprint arXiv:2504.16891, 2025.\nSaurav Muralidharan, Sharath Turuvekere Sreenivas, Raviraj Joshi, Marcin Chochowski, Mostofa\nPatwary, Mohammad Shoeybi, Bryan Catanzaro, Jan Kautz, and Pavlo Molchanov. Compact\nLanguage Models via Pruning and Knowledge Distillation, 2024. URL https://arxiv.org/abs/\n2407.14679.\nNVIDIA. Nemotron-4 340B Technical Report, 2024. URL https://arxiv.org/abs/2406.11704.\nNVIDIA. Nemotron-h: A family of accurate and efficient hybrid mamba-transformer models, 2025.\nURL https://arxiv.org/abs/2504.03624.\nLong Ouyang, Jeffrey Wu, Xu Jiang, Diogo Almeida, Carroll Wainwright, Pamela Mishkin, Chong\nZhang, Sandhini Agarwal, Katarina Slama, Alex Ray, John Schulman, Jacob Hilton, Fraser Kelton,\nLuke Miller, Maddie Simens, Amanda Askell, Peter Welinder, Paul Christiano, Jan Leike, and\nRyan Lowe. Training language models to follow instructions with human feedback. arXiv preprint\narXiv:2203.02155, 2022.\n29\n\nNVIDIA Nemotron Nano 2: An Accurate and Efficient Hybrid Mamba-Transformer Reasoning Model\nJupinder Parmar, Shrimai Prabhumoye, Joseph Jennings, Mostofa Patwary, Sandeep Subramanian,\nDan Su, Chen Zhu, Deepak Narayanan, Aastha Jhunjhunwala, Ayush Dattagupta, Vibhu Jawa,\nJiwei Liu, Ameya Mahabaleshwarkar, Osvald Nitski, Annika Brundyn, James Maki, Miguel\nMartinez, Jiaxuan You, John Kamalu, Patrick LeGresley, Denys Fridman, Jared Casper, Ashwath\nAithal, Oleksii Kuchaiev, Mohammad Shoeybi, Jonathan Cohen, and Bryan Catanzaro. Nemotron-\n4 15B Technical Report. arXiv preprint arXiv:2402.16819, 2024. URL https://arxiv.org/abs/\n2402.16819.\nKeiran Paster, Marco Dos Santos, Zhangir Azerbayev, and Jimmy Ba. OpenWebMath: An Open\nDataset of High-Quality Mathematical Web Text, 2023.\nGuilherme Penedo, Hynek Kydlíček, Vinko Sabolčec, Bettina Messmer, Negar Foroutan, Amir Hossein\nKargaran, Colin Raffel, Martin Jaggi, Leandro Von Werra, and Thomas Wolf. Fineweb2: One\npipeline to scale them all – adapting pre-training data processing to every language, 2025. URL\nhttps://arxiv.org/abs/2506.20920.\nLong Phan, Alice Gatti, Ziwen Han, Nathaniel Li, Josephina Hu, Hugh Zhang, Chen Bo Calvin\nZhang, Mohamed Shaaban, John Ling, Sean Shi, Michael Choi, Anish Agrawal, Arnav Chopra,\nAdam Khoja, Ryan Kim, Richard Ren, Jason Hausenloy, Oliver Zhang, Mantas Mazeika, Dmitry\nDodonov, Tung Nguyen, Jaeho Lee, Daron Anderson, Mikhail Doroshenko, Alun Cennyth Stokes,\nMobeen Mahmood, Oleksandr Pokutnyi, Oleg Iskra, Jessica P. Wang, John-Clark Levin, Mstyslav\nKazakov, Fiona Feng, Steven Y. Feng, Haoran Zhao, Michael Yu, Varun Gangal, Chelsea Zou,\nZihan Wang, Serguei Popov, Robert Gerbicz, Geoff Galgon, Johannes Schmitt, Will Yeadon,\nYongki Lee, Scott Sauers, Alvaro Sanchez, Fabian Giska, Marc Roth, Søren Riis, Saiteja Utpala,\nNoah Burns, Gashaw M. Goshu, Mohinder Maheshbhai Naiya, Chidozie Agu, Zachary Giboney,\nAntrell Cheatom, Francesco Fournier-Facio, Sarah-Jane Crowson, Lennart Finke, Zerui Cheng,\nJennifer Zampese, Ryan G. Hoerr, Mark Nandor, Hyunwoo Park, Tim Gehrunger, Jiaqi Cai, Ben\nMcCarty, Alexis C Garretson, Edwin Taylor, Damien Sileo, Qiuyu Ren, Usman Qazi, Lianghui\nLi, Jungbae Nam, John B. Wydallis, Pavel Arkhipov, Jack Wei Lun Shi, Aras Bacho, Chris G.\nWillcocks, Hangrui Cao, Sumeet Motwani, Emily de Oliveira Santos, Johannes Veith, Edward\nVendrow, Doru Cojoc, Kengo Zenitani, Joshua Robinson, Longke Tang, Yuqi Li, Joshua Vendrow,\nNatanael Wildner Fraga, Vladyslav Kuchkin, Andrey Pupasov Maksimov, Pierre Marion, Denis\nEfremov, Jayson Lynch, Kaiqu Liang, Aleksandar Mikov, Andrew Gritsevskiy, Julien Guillod,\nGözdenur Demir, Dakotah Martinez, Ben Pageler, Kevin Zhou, Saeed Soori, Ori Press, Henry Tang,\nPaolo Rissone, Sean R. Green, Lina Brüssel, Moon Twayana, Aymeric Dieuleveut, Joseph Marvin\nImperial, Ameya Prabhu, Jinzhou Yang, Nick Crispino, Arun Rao, Dimitri Zvonkine, Gabriel\nLoiseau, Mikhail Kalinin, Marco Lukas, Ciprian Manolescu, Nate Stambaugh, Subrata Mishra, Tad\nHogg, Carlo Bosio, Brian P Coppola, Julian Salazar, Jaehyeok Jin, Rafael Sayous, Stefan Ivanov,\nPhilippe Schwaller, Shaipranesh Senthilkuma, Andres M Bran, Andres Algaba, Kelsey Van den\nHoute, Lynn Van Der Sypt, Brecht Verbeken, David Noever, Alexei Kopylov, Benjamin Myklebust,\nBikun Li, Lisa Schut, Evgenii Zheltonozhskii, Qiaochu Yuan, Derek Lim, Richard Stanley, Tong\nYang, John Maar, Julian Wykowski, Martí Oller, Anmol Sahu, Cesare Giulio Ardito, Yuzheng Hu,\nAriel Ghislain Kemogne Kamdoum, Alvin Jin, Tobias Garcia Vilchis, Yuexuan Zu, Martin Lackner,\nJames Koppel, Gongbo Sun, Daniil S. Antonenko, Steffi Chern, Bingchen Zhao, Pierrot Arsene,\nJoseph M Cavanagh, Daofeng Li, Jiawei Shen, Donato Crisostomi, Wenjin Zhang, Ali Dehghan,\nSergey Ivanov, David Perrella, Nurdin Kaparov, Allen Zang, Ilia Sucholutsky, Arina Kharlamova,\nDaniil Orel, Vladislav Poritski, Shalev Ben-David, Zachary Berger, Parker Whitfill, Michael Foster,\nDaniel Munro, Linh Ho, Shankar Sivarajan, Dan Bar Hava, Aleksey Kuchkin, David Holmes,\nAlexandra Rodriguez-Romero, Frank Sommerhage, Anji Zhang, Richard Moat, Keith Schneider,\nZakayo Kazibwe, Don Clarke, Dae Hyun Kim, Felipe Meneguitti Dias, Sara Fish, Veit Elser, Tobias\nKreiman, Victor Efren Guadarrama Vilchis, Immo Klose, Ujjwala Anantheswaran, Adam Zweiger,\n30\n\nNVIDIA Nemotron Nano 2: An Accurate and Efficient Hybrid Mamba-Transformer Reasoning Model\nKaivalya Rawal, Jeffery Li, Jeremy Nguyen, Nicolas Daans, Haline Heidinger, Maksim Radionov,\nVáclav Rozhoň, Vincent Ginis, Christian Stump, Niv Cohen, Rafał Poświata, Josef Tkadlec, Alan\nGoldfarb, Chenguang Wang, Piotr Padlewski, Stanislaw Barzowski, Kyle Montgomery, Ryan\nStendall, Jamie Tucker-Foltz, Jack Stade, T. Ryan Rogers, Tom Goertzen, Declan Grabb, Abhishek\nShukla, Alan Givré, John Arnold Ambay, Archan Sen, Muhammad Fayez Aziz, Mark H Inlow,\nHao He, Ling Zhang, Younesse Kaddar, Ivar Ängquist, Yanxu Chen, Harrison K Wang, Kalyan\nRamakrishnan, Elliott Thornley, Antonio Terpin, Hailey Schoelkopf, Eric Zheng, Avishy Carmi,\nEthan D. L. Brown, Kelin Zhu, Max Bartolo, Richard Wheeler, Martin Stehberger, Peter Bradshaw,\nJP Heimonen, Kaustubh Sridhar, Ido Akov, Jennifer Sandlin, Yury Makarychev, Joanna Tam,\nHieu Hoang, David M. Cunningham, Vladimir Goryachev, Demosthenes Patramanis, Michael\nKrause, Andrew Redenti, David Aldous, Jesyin Lai, Shannon Coleman, Jiangnan Xu, Sangwon\nLee, Ilias Magoulas, Sandy Zhao, Ning Tang, Michael K. Cohen, Orr Paradise, Jan Hendrik\nKirchner, Maksym Ovchynnikov, Jason O. Matos, Adithya Shenoy, Michael Wang, Yuzhou Nie,\nAnna Sztyber-Betley, Paolo Faraboschi, Robin Riblet, Jonathan Crozier, Shiv Halasyamani,\nShreyas Verma, Prashant Joshi, Eli Meril, Ziqiao Ma, Jérémy Andréoletti, Raghav Singhal, Jacob\nPlatnick, Volodymyr Nevirkovets, Luke Basler, Alexander Ivanov, Seri Khoury, Nils Gustafsson,\nMarco Piccardo, Hamid Mostaghimi, Qijia Chen, Virendra Singh, Tran Quoc Khánh, Paul Rosu,\nHannah Szlyk, Zachary Brown, Himanshu Narayan, Aline Menezes, Jonathan Roberts, William\nAlley, Kunyang Sun, Arkil Patel, Max Lamparth, Anka Reuel, Linwei Xin, Hanmeng Xu, Jacob\nLoader, Freddie Martin, Zixuan Wang, Andrea Achilleos, Thomas Preu, Tomek Korbak, Ida\nBosio, Fereshteh Kazemi, Ziye Chen, Biró Bálint, Eve J. Y. Lo, Jiaqi Wang, Maria Inês S. Nunes,\nJeremiah Milbauer, M Saiful Bari, Zihao Wang, Behzad Ansarinejad, Yewen Sun, Stephane\nDurand, Hossam Elgnainy, Guillaume Douville, Daniel Tordera, George Balabanian, Hew Wolff,\nLynna Kvistad, Hsiaoyun Milliron, Ahmad Sakor, Murat Eron, Andrew Favre D. O., Shailesh\nShah, Xiaoxiang Zhou, Firuz Kamalov, Sherwin Abdoli, Tim Santens, Shaul Barkan, Allison Tee,\nRobin Zhang, Alessandro Tomasiello, G. Bruno De Luca, Shi-Zhuo Looi, Vinh-Kha Le, Noam Kolt,\nJiayi Pan, Emma Rodman, Jacob Drori, Carl J Fossum, Niklas Muennighoff, Milind Jagota, Ronak\nPradeep, Honglu Fan, Jonathan Eicher, Michael Chen, Kushal Thaman, William Merrill, Moritz\nFirsching, Carter Harris, Stefan Ciobâcă, Jason Gross, Rohan Pandey, Ilya Gusev, Adam Jones,\nShashank Agnihotri, Pavel Zhelnov, Mohammadreza Mofayezi, Alexander Piperski, David K.\nZhang, Kostiantyn Dobarskyi, Roman Leventov, Ignat Soroko, Joshua Duersch, Vage Taamazyan,\nAndrew Ho, Wenjie Ma, William Held, Ruicheng Xian, Armel Randy Zebaze, Mohanad Mohamed,\nJulian Noah Leser, Michelle X Yuan, Laila Yacar, Johannes Lengler, Katarzyna Olszewska,\nClaudio Di Fratta, Edson Oliveira, Joseph W. Jackson, Andy Zou, Muthu Chidambaram, Timothy\nManik, Hector Haffenden, Dashiell Stander, Ali Dasouqi, Alexander Shen, Bita Golshani, David\nStap, Egor Kretov, Mikalai Uzhou, Alina Borisovna Zhidkovskaya, Nick Winter, Miguel Orbegozo\nRodriguez, Robert Lauff, Dustin Wehr, Colin Tang, Zaki Hossain, Shaun Phillips, Fortuna\nSamuele, Fredrik Ekström, Angela Hammon, Oam Patel, Faraz Farhidi, George Medley, Forough\nMohammadzadeh, Madellene Peñaflor, Haile Kassahun, Alena Friedrich, Rayner Hernandez Perez,\nDaniel Pyda, Taom Sakal, Omkar Dhamane, Ali Khajegili Mirabadi, Eric Hallman, Kenchi Okutsu,\nMike Battaglia, Mohammad Maghsoudimehrabani, Alon Amit, Dave Hulbert, Roberto Pereira,\nSimon Weber, Handoko, Anton Peristyy, Stephen Malina, Mustafa Mehkary, Rami Aly, Frank\nReidegeld, Anna-Katharina Dick, Cary Friday, Mukhwinder Singh, Hassan Shapourian, Wanyoung\nKim, Mariana Costa, Hubeyb Gurdogan, Harsh Kumar, Chiara Ceconello, Chao Zhuang, Haon\nPark, Micah Carroll, Andrew R. Tawfeek, Stefan Steinerberger, Daattavya Aggarwal, Michael\nKirchhof, Linjie Dai, Evan Kim, Johan Ferret, Jainam Shah, Yuzhou Wang, Minghao Yan,\nKrzysztof Burdzy, Lixin Zhang, Antonio Franca, Diana T. Pham, Kang Yong Loh, Joshua\nRobinson, Abram Jackson, Paolo Giordano, Philipp Petersen, Adrian Cosma, Jesus Colino, Colin\nWhite, Jacob Votava, Vladimir Vinnikov, Ethan Delaney, Petr Spelda, Vit Stritecky, Syed M.\n31\n\nNVIDIA Nemotron Nano 2: An Accurate and Efficient Hybrid Mamba-Transformer Reasoning Model\nShahid, Jean-Christophe Mourrat, Lavr Vetoshkin, Koen Sponselee, Renas Bacho, Zheng-Xin\nYong, Florencia de la Rosa, Nathan Cho, Xiuyu Li, Guillaume Malod, Orion Weller, Guglielmo\nAlbani, Leon Lang, Julien Laurendeau, Dmitry Kazakov, Fatimah Adesanya, Julien Portier,\nLawrence Hollom, Victor Souza, Yuchen Anna Zhou, Julien Degorre, Yiğit Yalın, Gbenga Daniel\nObikoya, Rai, Filippo Bigi, M. C. Boscá, Oleg Shumar, Kaniuar Bacho, Gabriel Recchia, Mara\nPopescu, Nikita Shulga, Ngefor Mildred Tanwie, Thomas C. H. Lux, Ben Rank, Colin Ni, Matthew\nBrooks, Alesia Yakimchyk, Huanxu, Liu, Stefano Cavalleri, Olle Häggström, Emil Verkama, Joshua\nNewbould, Hans Gundlach, Leonor Brito-Santana, Brian Amaro, Vivek Vajipey, Rynaa Grover,\nTing Wang, Yosi Kratish, Wen-Ding Li, Sivakanth Gopi, Andrea Caciolai, Christian Schroeder\nde Witt, Pablo Hernández-Cámara, Emanuele Rodolà, Jules Robins, Dominic Williamson, Vincent\nCheng, Brad Raynor, Hao Qi, Ben Segev, Jingxuan Fan, Sarah Martinson, Erik Y. Wang, Kaylie\nHausknecht, Michael P. Brenner, Mao Mao, Christoph Demian, Peyman Kassani, Xinyu Zhang,\nDavid Avagian, Eshawn Jessica Scipio, Alon Ragoler, Justin Tan, Blake Sims, Rebeka Plecnik,\nAaron Kirtland, Omer Faruk Bodur, D. P. Shinde, Yan Carlos Leyva Labrador, Zahra Adoul,\nMohamed Zekry, Ali Karakoc, Tania C. B. Santos, Samir Shamseldeen, Loukmane Karim, Anna\nLiakhovitskaia, Nate Resman, Nicholas Farina, Juan Carlos Gonzalez, Gabe Maayan, Earth\nAnderson, Rodrigo De Oliveira Pena, Elizabeth Kelley, Hodjat Mariji, Rasoul Pouriamanesh,\nWentao Wu, Ross Finocchio, Ismail Alarab, Joshua Cole, Danyelle Ferreira, Bryan Johnson,\nMohammad Safdari, Liangti Dai, Siriphan Arthornthurasuk, Isaac C. McAlister, Alejandro José\nMoyano, Alexey Pronin, Jing Fan, Angel Ramirez-Trinidad, Yana Malysheva, Daphiny Pottmaier,\nOmid Taheri, Stanley Stepanic, Samuel Perry, Luke Askew, Raúl Adrián Huerta Rodríguez, Ali\nM. R. Minissi, Ricardo Lorena, Krishnamurthy Iyer, Arshad Anil Fasiludeen, Ronald Clark, Josh\nDucey, Matheus Piza, Maja Somrak, Eric Vergo, Juehang Qin, Benjámin Borbás, Eric Chu,\nJack Lindsey, Antoine Jallon, I. M. J. McInnis, Evan Chen, Avi Semler, Luk Gloor, Tej Shah,\nMarc Carauleanu, Pascal Lauer, Tran Ðuc Huy, Hossein Shahrtash, Emilien Duc, Lukas Lewark,\nAssaf Brown, Samuel Albanie, Brian Weber, Warren S. Vaz, Pierre Clavier, Yiyang Fan, Gabriel\nPoesia Reis e Silva, Long, Lian, Marcus Abramovitch, Xi Jiang, Sandra Mendoza, Murat Islam,\nJuan Gonzalez, Vasilios Mavroudis, Justin Xu, Pawan Kumar, Laxman Prasad Goswami, Daniel\nBugas, Nasser Heydari, Ferenc Jeanplong, Thorben Jansen, Antonella Pinto, Archimedes Apronti,\nAbdallah Galal, Ng Ze-An, Ankit Singh, Tong Jiang, Joan of Arc Xavier, Kanu Priya Agarwal,\nMohammed Berkani, Gang Zhang, Zhehang Du, Benedito Alves de Oliveira Junior, Dmitry\nMalishev, Nicolas Remy, Taylor D. Hartman, Tim Tarver, Stephen Mensah, Gautier Abou Loume,\nWiktor Morak, Farzad Habibi, Sarah Hoback, Will Cai, Javier Gimenez, Roselynn Grace Montecillo,\nJakub Łucki, Russell Campbell, Asankhaya Sharma, Khalida Meer, Shreen Gul, Daniel Espinosa\nGonzalez, Xavier Alapont, Alex Hoover, Gunjan Chhablani, Freddie Vargus, Arunim Agarwal, Yibo\nJiang, Deepakkumar Patil, David Outevsky, Kevin Joseph Scaria, Rajat Maheshwari, Abdelkader\nDendane, Priti Shukla, Ashley Cartwright, Sergei Bogdanov, Niels Mündler, Sören Möller, Luca\nArnaboldi, Kunvar Thaman, Muhammad Rehan Siddiqi, Prajvi Saxena, Himanshu Gupta, Tony\nFruhauff, Glen Sherman, Mátyás Vincze, Siranut Usawasutsakorn, Dylan Ler, Anil Radhakrishnan,\nInnocent Enyekwe, Sk Md Salauddin, Jiang Muzhen, Aleksandr Maksapetyan, Vivien Rossbach,\nChris Harjadi, Mohsen Bahaloohoreh, Claire Sparrow, Jasdeep Sidhu, Sam Ali, Song Bian, John\nLai, Eric Singer, Justine Leon Uro, Greg Bateman, Mohamed Sayed, Ahmed Menshawy, Darling\nDuclosel, Dario Bezzi, Yashaswini Jain, Ashley Aaron, Murat Tiryakioglu, Sheeshram Siddh, Keith\nKrenek, Imad Ali Shah, Jun Jin, Scott Creighton, Denis Peskoff, Zienab EL-Wasif, Ragavendran P\nV, Michael Richmond, Joseph McGowan, Tejal Patwardhan, Hao-Yu Sun, Ting Sun, Nikola Zubić,\nSamuele Sala, Stephen Ebert, Jean Kaddour, Manuel Schottdorf, Dianzhuo Wang, Gerol Petruzella,\nAlex Meiburg, Tilen Medved, Ali ElSheikh, S Ashwin Hebbar, Lorenzo Vaquero, Xianjun Yang,\nJason Poulos, Vilém Zouhar, Sergey Bogdanik, Mingfang Zhang, Jorge Sanz-Ros, David Anugraha,\nYinwei Dai, Anh N. Nhu, Xue Wang, Ali Anil Demircali, Zhibai Jia, Yuyin Zhou, Juncheng\n32\n\nNVIDIA Nemotron Nano 2: An Accurate and Efficient Hybrid Mamba-Transformer Reasoning Model\nWu, Mike He, Nitin Chandok, Aarush Sinha, Gaoxiang Luo, Long Le, Mickaël Noyé, Michał\nPerełkiewicz, Ioannis Pantidis, Tianbo Qi, Soham Sachin Purohit, Letitia Parcalabescu, Thai-Hoa\nNguyen, Genta Indra Winata, Edoardo M. Ponti, Hanchen Li, Kaustubh Dhole, Jongee Park, Dario\nAbbondanza, Yuanli Wang, Anupam Nayak, Diogo M. Caetano, Antonio A. W. L. Wong, Maria del\nRio-Chanona, Dániel Kondor, Pieter Francois, Ed Chalstrey, Jakob Zsambok, Dan Hoyer, Jenny\nReddish, Jakob Hauser, Francisco-Javier Rodrigo-Ginés, Suchandra Datta, Maxwell Shepherd,\nThom Kamphuis, Qizheng Zhang, Hyunjun Kim, Ruiji Sun, Jianzhu Yao, Franck Dernoncourt,\nSatyapriya Krishna, Sina Rismanchian, Bonan Pu, Francesco Pinto, Yingheng Wang, Kumar\nShridhar, Kalon J. Overholt, Glib Briia, Hieu Nguyen, David, Soler Bartomeu, Tony CY Pang,\nAdam Wecker, Yifan Xiong, Fanfei Li, Lukas S. Huber, Joshua Jaeger, Romano De Maddalena,\nXing Han Lù, Yuhui Zhang, Claas Beger, Patrick Tser Jern Kon, Sean Li, Vivek Sanker, Ming\nYin, Yihao Liang, Xinlu Zhang, Ankit Agrawal, Li S. Yifei, Zechen Zhang, Mu Cai, Yasin Sonmez,\nCostin Cozianu, Changhao Li, Alex Slen, Shoubin Yu, Hyun Kyu Park, Gabriele Sarti, Marcin\nBriański, Alessandro Stolfo, Truong An Nguyen, Mike Zhang, Yotam Perlitz, Jose Hernandez-\nOrallo, Runjia Li, Amin Shabani, Felix Juefei-Xu, Shikhar Dhingra, Orr Zohar, My Chiffon Nguyen,\nAlexander Pondaven, Abdurrahim Yilmaz, Xuandong Zhao, Chuanyang Jin, Muyan Jiang, Stefan\nTodoran, Xinyao Han, Jules Kreuer, Brian Rabern, Anna Plassart, Martino Maggetti, Luther\nYap, Robert Geirhos, Jonathon Kean, Dingsu Wang, Sina Mollaei, Chenkai Sun, Yifan Yin, Shiqi\nWang, Rui Li, Yaowen Chang, Anjiang Wei, Alice Bizeul, Xiaohan Wang, Alexandre Oliveira\nArrais, Kushin Mukherjee, Jorge Chamorro-Padial, Jiachen Liu, Xingyu Qu, Junyi Guan, Adam\nBouyamourn, Shuyu Wu, Martyna Plomecka, Junda Chen, Mengze Tang, Jiaqi Deng, Shreyas\nSubramanian, Haocheng Xi, Haoxuan Chen, Weizhi Zhang, Yinuo Ren, Haoqin Tu, Sejong Kim,\nYushun Chen, Sara Vera Marjanović, Junwoo Ha, Grzegorz Luczyna, Jeff J. Ma, Zewen Shen,\nDawn Song, Cedegao E. Zhang, Zhun Wang, Gaël Gendron, Yunze Xiao, Leo Smucker, Erica\nWeng, Kwok Hao Lee, Zhe Ye, Stefano Ermon, Ignacio D. Lopez-Miguel, Theo Knights, Anthony\nGitter, Namkyu Park, Boyi Wei, Hongzheng Chen, Kunal Pai, Ahmed Elkhanany, Han Lin,\nPhilipp D. Siedler, Jichao Fang, Ritwik Mishra, Károly Zsolnai-Fehér, Xilin Jiang, Shadab Khan,\nJun Yuan, Rishab Kumar Jain, Xi Lin, Mike Peterson, Zhe Wang, Aditya Malusare, Maosen Tang,\nIsha Gupta, Ivan Fosin, Timothy Kang, Barbara Dworakowska, Kazuki Matsumoto, Guangyao\nZheng, Gerben Sewuster, Jorge Pretel Villanueva, Ivan Rannev, Igor Chernyavsky, Jiale Chen,\nDeepayan Banik, Ben Racz, Wenchao Dong, Jianxin Wang, Laila Bashmal, Duarte V. Gonçalves,\nWei Hu, Kaushik Bar, Ondrej Bohdal, Atharv Singh Patlan, Shehzaad Dhuliawala, Caroline\nGeirhos, Julien Wist, Yuval Kansal, Bingsen Chen, Kutay Tire, Atak Talay Yücel, Brandon\nChristof, Veerupaksh Singla, Zijian Song, Sanxing Chen, Jiaxin Ge, Kaustubh Ponkshe, Isaac\nPark, Tianneng Shi, Martin Q. Ma, Joshua Mak, Sherwin Lai, Antoine Moulin, Zhuo Cheng,\nZhanda Zhu, Ziyi Zhang, Vaidehi Patil, Ketan Jha, Qiutong Men, Jiaxuan Wu, Tianchi Zhang,\nBruno Hebling Vieira, Alham Fikri Aji, Jae-Won Chung, Mohammed Mahfoud, Ha Thi Hoang,\nMarc Sperzel, Wei Hao, Kristof Meding, Sihan Xu, Vassilis Kostakos, Davide Manini, Yueying\nLiu, Christopher Toukmaji, Jay Paek, Eunmi Yu, Arif Engin Demircali, Zhiyi Sun, Ivan Dewerpe,\nHongsen Qin, Roman Pflugfelder, James Bailey, Johnathan Morris, Ville Heilala, Sybille Rosset,\nZishun Yu, Peter E. Chen, Woongyeong Yeo, Eeshaan Jain, Ryan Yang, Sreekar Chigurupati, Julia\nChernyavsky, Sai Prajwal Reddy, Subhashini Venugopalan, Hunar Batra, Core Francisco Park,\nHieu Tran, Guilherme Maximiano, Genghan Zhang, Yizhuo Liang, Hu Shiyu, Rongwu Xu, Rui\nPan, Siddharth Suresh, Ziqi Liu, Samaksh Gulati, Songyang Zhang, Peter Turchin, Christopher W.\nBartlett, Christopher R. Scotese, Phuong M. Cao, Aakaash Nattanmai, Gordon McKellips, Anish\nCheraku, Asim Suhail, Ethan Luo, Marvin Deng, Jason Luo, Ashley Zhang, Kavin Jindel, Jay\nPaek, Kasper Halevy, Allen Baranov, Michael Liu, Advaith Avadhanam, David Zhang, Vincent\nCheng, Brad Ma, Evan Fu, Liam Do, Joshua Lass, Hubert Yang, Surya Sunkari, Vishruth Bharath,\nViolet Ai, James Leung, Rishit Agrawal, Alan Zhou, Kevin Chen, Tejas Kalpathi, Ziqi Xu, Gavin\n33\n\nNVIDIA Nemotron Nano 2: An Accurate and Efficient Hybrid Mamba-Transformer Reasoning Model\nWang, Tyler Xiao, Erik Maung, Sam Lee, Ryan Yang, Roy Yue, Ben Zhao, Julia Yoon, Sunny\nSun, Aryan Singh, Ethan Luo, Clark Peng, Tyler Osbey, Taozhi Wang, Daryl Echeazu, Hubert\nYang, Timothy Wu, Spandan Patel, Vidhi Kulkarni, Vijaykaarti Sundarapandiyan, Ashley Zhang,\nAndrew Le, Zafir Nasim, Srikar Yalam, Ritesh Kasamsetty, Soham Samal, Hubert Yang, David\nSun, Nihar Shah, Abhijeet Saha, Alex Zhang, Leon Nguyen, Laasya Nagumalli, Kaixin Wang, Alan\nZhou, Aidan Wu, Jason Luo, Anwith Telluri, Summer Yue, Alexandr Wang, and Dan Hendrycks.\nHumanity’s last exam, 2025. URL https://arxiv.org/abs/2501.14249.\nAkshara Prabhakar, Zuxin Liu, Weiran Yao, Jianguo Zhang, Ming Zhu, Thai Lan, Shirley Kokane,\nJuntao Tan, Weiran Yao, Zhiwei Liu, Yihao Feng, Rithesh Murthy, Liangwei Yang, Silvio Savarese,\nJuan Carlos Niebles, Shelby Heinecke, Huan Wang, and et al. Apigen-mt: Agentic pipeline for\nmulti-turn data generation via simulated agent-human interplay. arXiv preprint arXiv:2504.03601,\n2025.\nQwen. Qwen2.5 Technical Report, 2025. URL https://arxiv.org/abs/2412.15115.\nRafael Rafailov, Archit Sharma, Eric Mitchell, Stefano Ermon, Christopher D. Manning, and Chelsea\nFinn. Direct preference optimization: Your language model is secretly a reward model. arXiv\npreprint arXiv:2305.18290, 2023.\nDavid Rein, Betty Li Hou, Asa Cooper Stickland, Jackson Petty, Richard Yuanzhe Pang, Julien\nDirani, Julian Michael, and Samuel R. Bowman. Gpqa: A graduate-level google-proof q&a\nbenchmark, 2023.\nHayley Ross, Ameya Sunil Mahabaleshwarkar, and Yoshi Suhara. When2Call: When (not) to\ncall tools. In Proceedings of the 2025 Conference of the Nations of the Americas Chapter of\nthe Association for Computational Linguistics: Human Language Technologies (Volume 1: Long\nPapers), pp. 3391–3409, Albuquerque, New Mexico, April 2025. Association for Computational\nLinguistics. ISBN 979-8-89176-189-6. URL https://aclanthology.org/2025.naacl-long.174/.\nKeisuke Sakaguchi, Ronan Le Bras, Chandra Bhagavatula, and Yejin Choi. WinoGrande: An\nAdversarial Winograd Schema Challenge at Scale, 2019. URL https://arxiv.org/abs/1907.\n10641.\nZhihong Shao, Peiyi Wang, Qihao Zhu, Runxin Xu, Junxiao Song, et al. Deepseekmath: Pushing\nthe limits of mathematical reasoning in open language models. arXiv preprint arXiv:2402.03300,\n2024.\nFreda Shi, Mirac Suzgun, Markus Freitag, Xuezhi Wang, Suraj Srivats, Soroush Vosoughi, Hyung Won\nChung, Yi Tay, Sebastian Ruder, Denny Zhou, Dipanjan Das, and Jason Wei. Language models\nare multilingual chain-of-thought reasoners, 2022. URL https://arxiv.org/abs/2210.03057.\nShivalika Singh, Angelika Romanou, Clémentine Fourrier, David I Adelani, Jian Gang Ngui, Daniel\nVila-Suero, Peerat Limkonchotiwat, Kelly Marchisio, Wei Qi Leong, Yosephine Susanto, et al.\nGlobal mmlu: Understanding and addressing cultural and linguistic biases in multilingual evalua-\ntion. arXiv preprint arXiv:2412.03304, 2024a.\nShivalika Singh, Angelika Romanou, Clémentine Fourrier, David I. Adelani, Jian Gang Ngui, Daniel\nVila-Suero, Peerat Limkonchotiwat, Kelly Marchisio, Wei Qi Leong, Yosephine Susanto, Raymond\nNg, Shayne Longpre, Wei-Yin Ko, Madeline Smith, Antoine Bosselut, Alice Oh, Andre F. T.\nMartins, Leshem Choshen, Daphne Ippolito, Enzo Ferrante, Marzieh Fadaee, Beyza Ermis, and\nSara Hooker. Global mmlu: Understanding and addressing cultural and linguistic biases in\nmultilingual evaluation, 2024b. URL https://arxiv.org/abs/2412.03304.\n34\n\nNVIDIA Nemotron Nano 2: An Accurate and Efficient Hybrid Mamba-Transformer Reasoning Model\nDavid R. So, Wojciech Mańke, Hanxiao Liu, Zihang Dai, Noam Shazeer, and Quoc V. Le. Primer:\nSearching for Efficient Transformers for Language Modeling, 2022. URL https://arxiv.org/\nabs/2109.08668.\nSharath Turuvekere Sreenivas, Saurav Muralidharan, Raviraj Joshi, Marcin Chochowski, Ameya Sunil\nMahabaleshwarkar, Gerald Shen, Jiaqi Zeng, Zijia Chen, Yoshi Suhara, Shizhe Diao, Chenhan\nYu, Wei-Chun Chen, Hayley Ross, Oluwatobi Olabiyi, Ashwath Aithal, Oleksii Kuchaiev, Daniel\nKorzekwa, Pavlo Molchanov, Mostofa Patwary, Mohammad Shoeybi, Jan Kautz, and Bryan\nCatanzaro. LLM Pruning and Distillation in Practice: The Minitron Approach, 2024. URL\nhttps://arxiv.org/abs/2408.11796.\nOlly Styles, Sam Miller, Patricio Cerda-Mardini, Tanaya Guha, Victor Sanchez, and Bertie Vidgen.\nWorkbench: a benchmark dataset for agents in a realistic workplace setting. arXiv preprint\narXiv:2405.00823, 2024. doi: 10.48550/arXiv.2405.00823.\nDan Su, Kezhi Kong, Ying Lin, Joseph Jennings, Brandon Norick, Markus Kliegl, Mostofa Patwary,\nMohammad Shoeybi, and Bryan Catanzaro. Nemotron-CC: Transforming Common Crawl into a\nrefined long-horizon pretraining dataset. In Wanxiang Che, Joyce Nabende, Ekaterina Shutova,\nand Mohammad Taher Pilehvar (eds.), Proceedings of the 63rd Annual Meeting of the Association\nfor Computational Linguistics (Volume 1: Long Papers), pp. 2459–2475, Vienna, Austria, July\n2025. Association for Computational Linguistics. ISBN 979-8-89176-251-0. doi: 10.18653/v1/2025.\nacl-long.123. URL https://aclanthology.org/2025.acl-long.123/.\nAli Taghibakhshi, Sharath Turuvekere Sreenivas, Saurav Muralidharan, Marcin Chochowski, Yashaswi\nKarnati, Raviraj Joshi, Ameya Sunil Mahabaleshwarkar, Zijia Chen, Yoshi Suhara, Oluwatobi\nOlabiyi, et al. Efficient hybrid language model compression through group-aware ssm pruning.\narXiv preprint arXiv:2504.11409, 2025.\nMinyang Tian, Luyu Gao, Shizhuo Dylan Zhang, Xinan Chen, Cunwei Fan, Xuefei Guo, Roland\nHaas, Pan Ji, Kittithat Krongchon, Yao Li, Shengyan Liu, Di Luo, Yutao Ma, Hao Tong,\nKha Trinh, Chenyu Tian, Zihan Wang, Bohao Wu, Yanyu Xiong, Shengzhu Yin, Minhui Zhu,\nKilian Lieret, Yanxin Lu, Genglin Liu, Yufeng Du, Tianhua Tao, Ofir Press, Jamie Callan, Eliu\nHuerta, and Hao Peng. Scicode: A research coding benchmark curated by scientists, 2024. URL\nhttps://arxiv.org/abs/2407.13168.\nShubham Toshniwal, Wei Du, Ivan Moshkov, Branislav Kisacanin, Alexan Ayrapetyan, and Igor\nGitman. Openmathinstruct-2: Accelerating ai for math with massive open-source instruction data.\narXiv preprint arXiv:2410.01560, 2024.\nRoger Waleffe, Wonmin Byeon, Duncan Riach, Brandon Norick, Vijay Korthikanti, Tri Dao, Albert\nGu, Ali Hatamizadeh, Sudhakar Singh, Deepak Narayanan, Garvit Kulshreshtha, Vartika Singh,\nJared Casper, Jan Kautz, Mohammad Shoeybi, and Bryan Catanzaro. An Empirical Study of\nMamba-based Language Models, 2024. URL https://arxiv.org/abs/2406.07887.\nLiang Wang, Nan Yang, Xiaolong Huang, Binxing Jiao, Linjun Yang, Daxin Jiang, Rangan Majumder,\nand Furu Wei. Text embeddings by weakly-supervised contrastive pre-training, 2024. URL\nhttps://arxiv.org/abs/2212.03533.\nSiyuan Wang, Zhongkun Liu, Wanjun Zhong, Ming Zhou, Zhongyu Wei, Zhumin Chen, and Nan\nDuan. From lsat: The progress and challenges of complex reasoning. IEEE/ACM Transactions on\nAudio, Speech, and Language Processing, 30:2201–2216, 2022.\n35\n\nNVIDIA Nemotron Nano 2: An Accurate and Efficient Hybrid Mamba-Transformer Reasoning Model\nZhilin Wang, Jiaqi Zeng, Olivier Delalleau, Hoo-Chang Shin, Felipe Soares, Alexander Bukharin,\nEllie Evans, Yi Dong, and Oleksii Kuchaiev. Helpsteer3-preference: Open human-annotated\npreference data across diverse tasks and languages. arXiv preprint arXiv:2505.11475, 2025.\nMitchell Wortsman, Gabriel Ilharco, Samir Yitzhak Gadre, Rebecca Roelofs, Raphael Gontijo-Lopes,\nAri S. Morcos, Hongseok Namkoong, Ali Farhadi, Yair Carmon, Simon Kornblith, and Ludwig\nSchmidt. Model soups: averaging weights of multiple fine-tuned models improves accuracy without\nincreasing inference time, 2022. URL https://arxiv.org/abs/2203.05482.\nFanjia Yan, Huanzhi Mao, Charlie Cheng-Jie Ji, Tianjun Zhang, Shishir G. Patil, Ion Stoica, and\nJoseph E. Gonzalez. Berkeley Function Calling Leaderboard. 2024.\nAn Yang, Anfeng Li, Baosong Yang, Beichen Zhang, Binyuan Hui, Bo Zheng, Bowen Yu, Chang\nGao, Chengen Huang, Chenxu Lv, Chujie Zheng, Dayiheng Liu, Fan Zhou, Fei Huang, Feng Hu,\nHao Ge, Haoran Wei, Huan Lin, Jialong Tang, Jian Yang, Jianhong Tu, Jianwei Zhang, Jianxin\nYang, Jiaxi Yang, Jing Zhou, Jingren Zhou, Junyang Lin, Kai Dang, Keqin Bao, Kexin Yang,\nLe Yu, Lianghao Deng, Mei Li, Mingfeng Xue, Mingze Li, Pei Zhang, Peng Wang, Qin Zhu, Rui\nMen, Ruize Gao, Shixuan Liu, Shuang Luo, Tianhao Li, Tianyi Tang, Wenbiao Yin, Xingzhang\nRen, Xinyu Wang, Xinyu Zhang, Xuancheng Ren, Yang Fan, Yang Su, Yichang Zhang, Yinger\nZhang, Yu Wan, Yuqiong Liu, Zekun Wang, Zeyu Cui, Zhenru Zhang, Zhipeng Zhou, and Zihan\nQiu. Qwen3 technical report, 2025. URL https://arxiv.org/abs/2505.09388.\nShuo Yang, Wei-Lin Chiang, Lianmin Zheng, Joseph E. Gonzalez, and Ion Stoica. Rethinking\nbenchmark and contamination for language models with rephrased samples, 2023.\nRowan Zellers, Ari Holtzman, Yonatan Bisk, Ali Farhadi, and Yejin Choi. HellaSwag: Can a Machine\nReally Finish Your Sentence?, 2019. URL https://arxiv.org/abs/1905.07830.\nBiao Zhang and Rico Sennrich.\nRoot Mean Square Layer Normalization, 2019.\nURL https:\n//arxiv.org/abs/1910.07467.\nLianmin Zheng, Wei-Lin Chiang, Ying Sheng, Yonghao Li, Zhuohan Chen, Zhewei Wong, Siyuan\nZhuang, Yakun Shao, Kai Xu, Zhenyu Zhang, et al. Judging llm-as-a-judge with mt-bench and\nchatbot arena. arXiv preprint arXiv:2309.11998, 2023.\nWanjun Zhong, Siyuan Wang, Duyu Tang, Zenan Xu, Daya Guo, Yining Chen, Jiahai Wang, Jian\nYin, Ming Zhou, and Nan Duan. Analytical reasoning of text. In Findings of the Association for\nComputational Linguistics: NAACL 2022, pp. 2306–2319, 2022.\nFan Zhou, Zengzhi Wang, Nikhil Ranjan, Zhoujun Cheng, Liping Tang, Guowei He, Zhengzhong\nLiu, and Eric P Xing. Megamath: Pushing the limits of open math corpora. arXiv preprint\narXiv:2504.02807, 2025.\nJeffrey Zhou, Tianjian Lu, Swaroop Mishra, Siddhartha Brahma, Sujoy Basu, Yi Luan, Denny\nZhou, and Le Hou. Instruction-following evaluation for large language models. arXiv preprint\narXiv:2311.07911, 2023.\nA. Permissive Source Code Licenses\nWe remove source code with a license not in the following list:\n36\n\nNVIDIA Nemotron Nano 2: An Accurate and Efficient Hybrid Mamba-Transformer Reasoning Model\n3Com Microcode 3com-microcode, 3D Slicer License 1.0 [3dslicer-1.0], 4Suite 1.1 [4suite-1.1], AAL\n[attribution], Abstyles License [abstyles], ACE TAO License [ace-tao], AdaCore Doc License [adacore-\ndoc], ADI BSD [adi-bsd], Adobe Glyph License [adobe-glyph], Adobe Postscript AFM License\n[apafml], Adobe Source Code License 2006 [adobe-scl], AES-128 3.0 License [aes-128-3.0], AFL\n1.1 [afl-1.1], AFL 1.2 [afl-1.2], AFL 2.0 [afl-2.0], AFL 2.1 [afl-2.1], AFL 3.0 [afl-3.0], afmparse\nLicense [afmparse], Agere BSD [agere-bsd], Alexisisaac Freeware License [alexisisaac-freeware],\nAllegro 4 License [allegro-4], Altera License [xnet], Amazon Digital Services License [adsl], AMD\nHistorical License [amd-historical], AMD PLPA License [amdplpa], AMPAS BSD-Style License\n[ampas], AMSFonts license [ams-fonts], Andre Adrian DFS license [adrian], ANTLR-PD [antlr-\npd], ANTLR-PD with fallback [antlr-pd-fallback], ANU License [anu-license], Apache 1.0 [apache-\n1.0], Apache 1.1 [apache-1.1], Apache 2.0 [apache-2.0], Apache Patent Provision Exception Terms\n[apache-patent-exception], App::s2p License [app-s2p], Apple Attribution 1997 [apple-attribution-\n1997], Apple Attribution License [apple-attribution], Apple Example Code License [apple-excl],\nApple MIT License [aml], Apple Sample Source Code License [apple-sscl], Aravindan Premkumar\nLicenase [aravindan-premkumar], ArgoUML License [argouml], ARM LLVM Grant [arm-llvm-sga],\nArray Input Method Public License [array-input-method-pl], Artistic 1.0 [artistic-1.0], Artistic 1.0\nw/clause 8 [artistic-1.0-cl8], Artistic 2.0 [artistic-2.0], Artistic-Perl-1.0 [artistic-perl-1.0], ASMUS\nLicense [asmus], ASN.1 Object Dumping Code License [asn1], Atkinson Hyperlegible Font License\n[atkinson-hyperlegible-font], Baekmuk Fonts License [baekmuk-fonts], Bahyph License [bahyph],\nBaKoMa Fonts Licence 1995 [bakoma-fonts-1995], Barr TeX License [barr-tex], BEA 2.1 [bea-2.1],\nBeal Screamer License [beal-screamer], Beer-Ware License [beerware], BERI Hardware-Software\nLicense v1.0 [beri-hw-sw-1.0], BigDigits License [bigdigits], Bigelow & Holmes Lucida Fonts License\n[bigelow-holmes], Biopython License [biopython], Bitstream Vera Font License [bitstream], Bitzi-PD\n[bitzi-pd], BLAS License 2017 [blas-2017], Blue Oak Model License 1.0.0 [blueoak-1.0.0], BOHL-\n0.2 [bohl-0.2], Boost 1.0 [boost-1.0], Boost Original [boost-original], Borceux License [borceux],\nBoutell libgd declarations 2021 [boutell-libgd-2021], bpmn.io License [bpmn-io], Brent Corkum\nLicense [brent-corkum], Brian Clapper License [brian-clapper], Brian Gladman 3-Clause License\n[brian-gladman-3-clause], Brian Gladman Dual BSD-GPL [brian-gladman-dual], Brian Gladman\nLicense [brian-gladman], Broadcom CFE License [broadcom-cfe], Broadcom Warranty Disclaimer\n[broadcom-linux-timer], Brocade Firmware License [brocade-firmware], Bruno Podetti License [bruno-\npodetti], BSD 1988 [bsd-1988], BSD 3-Clause Devine [bsd-3-clause-devine], BSD 3-Clause FDA\n[bsd-3-clause-fda], BSD 3-Clause jtag [bsd-3-clause-jtag], BSD 3-Clause No Change [bsd-3-clause-no-\nchange], BSD 3-Clause No Nuclear Warranty [bsd-3-clause-no-nuclear-warranty], BSD 3-Clause no\ntrademark [bsd-3-clause-no-trademark], BSD 3-Clause Open MPI variant [bsd-3-clause-open-mpi],\nBSD 3-Clause Sun [bsd-3-clause-sun], BSD 3-Clause with GPL reference [bsd-top-gpl-addition],\nBSD Acknowledgment (Carrot2) License [bsd-ack-carrot2], BSD Acknowledgment License [bsd-\nack], BSD Advertising Acknowledgement License [bsd-advertising-acknowledgement], BSD Artwork\n[bsd-artwork], BSD Atmel License [bsd-atmel], BSD DPT [bsd-dpt], BSD plus modification notice\n[bsd-plus-mod-notice], BSD Simplified Darwin [bsd-simplified-darwin], BSD Source Code Attribution\n[bsd-source-code], BSD Unchanged [bsd-unchanged], BSD Unmodified [bsd-unmodified], BSD Zero\nClause License [bsd-zero], BSD-1-Clause [bsd-1-clause], BSD-1-Clause Build [bsd-1-clause-build],\nBSD-2-Clause [bsd-simplified], BSD-2-Clause no disclaimer [bsd-no-disclaimer], BSD-2-Clause no\ndisclaimer Unmod [bsd-no-disclaimer-unmodified], BSD-2-Clause Plus Patent [bsd-plus-patent], BSD-\n2-Clause-plus-advertizing [bsd-2-clause-plus-advertizing], BSD-2-Clause-Views [bsd-2-clause-views],\nBSD-3-Clause [bsd-new], BSD-3-Clause tcpdump variant [bsd-new-tcpdump], BSD-3-Clause without\nnotice modification [bsd-new-nomod], BSD-3-Clause X11 disclaimer [bsd-x11], BSD-4-Clause with\nVoices [bsd-original-voices], BSD-4-Clause-Shortened [bsd-4-clause-shortened], BSD-Axis without\nmodification [bsd-axis-nomod], BSD-Credit [bsd-credit], BSD-Derivative [bsd-new-derivative], BSD-\nExport [bsd-export], BSD-InnoSys [bsd-innosys], BSD-Mylex [bsd-mylex], BSD-Original [bsd-original],\n37\n\nNVIDIA Nemotron Nano 2: An Accurate and Efficient Hybrid Mamba-Transformer Reasoning Model\nBSD-Original-Muscle [bsd-original-muscle], BSD-Original-UC [bsd-original-uc], BSD-Original-UC-\n1986 [bsd-original-uc-1986], BSD-Simplified Intel [bsd-simplified-intel], BSD-Simplified source [bsd-\nsimplified-source], BSD-Top [bsd-top], BSLA [bsla], BSLA no advertizing [bsla-no-advert], Business\nSource License 1.0 [bsl-1.0], BYTEmark License [bytemark], bzip2 License 2010 [bzip2-libbzip-2010],\nCaldera License [caldera], Careware [careware], Carnegie Mellon Contributors [carnegie-mellon-\ncontributors], Carnegie Mellon License [carnegie-mellon], Cavium malloc License [cavium-malloc],\nCC-BY-1.0 [cc-by-1.0], CC-BY-2.0 [cc-by-2.0], CC-BY-2.0-UK [cc-by-2.0-uk], CC-BY-2.5 [cc-by-\n2.5], CC-BY-3.0 [cc-by-3.0], CC-BY-3.0-AT [cc-by-3.0-at], CC-BY-3.0-US [cc-by-3.0-us], CC-BY-4.0\n[cc-by-4.0], CC-PD [cc-pd], CC-PD Mark 1.0 [cc-pdm-1.0], CC0-1.0 [cc0-1.0], CDLA Permissive\n1.0 [cdla-permissive-1.0], CDLA Permissive 2.0 [cdla-permissive-2.0], CeCILL-B License [cecill-b],\nCeCILL-B License English [cecill-b-en], CERN Attribution 1995 [cern-attribution-1995], CERN\nOpen Hardware Licence v1.2 [cern-ohl-1.2], CERN Open Hardware License v1.1 [cern-ohl-1.1],\nCERN-OHL-P-2.0 [cern-ohl-p-2.0], CFITSIO License [cfitsio], Checkmk License [checkmk], Chicken\nDance License v0.2 [chicken-dl-0.2], Chris Maunder License [chris-maunder], Chris Stoy Attribution\nLicense [chris-stoy], Clarified Artistic License [artistic-clarified], Classic VB License [classic-vb], Clear\nBSD 1-Clause License [clear-bsd-1-clause], Clear BSD License [clear-bsd], Click License [click-license],\nCLIPS License 2017 [clips-2017], CMU Computing Services License [cmu-computing-services], CMU\nLicense [cmu-template], CMU MIT-style [cmu-mit], CMU Simple License [cmu-simple], CMU Style\n[cmu-uc], CNRI Jython License [cnri-jython], CNRI Python 1.6 [cnri-python-1.6], CNRI Python\n1.6.1 [cnri-python-1.6.1], Code Credit License v1.0.1 [code-credit-license-1.0.1], Code Credit License\nv1.1.0 [code-credit-license-1.1.0], CodeGuru Permissions [codeguru-permissions], CodeSourcery 2004\n[codesourcery-2004], COIL-1.0 [coil-1.0], Common Lisp LOOP License [loop], CommonJ Timer\nLicense [commonj-timer], Compass License [compass], ComponentAce JCraft License [componentace-\njcraft], compuphase Linking Exception to Apache 2.0 [compuphase-linking-exception], Condor Public\nLicense 1.1 [condor-1.1], Copyheart [copyheart], Cornell Lossless JPEG License [cornell-lossless-jpeg],\nCougaar Open Source License [cosl], CP/M License 2022 [cpm-2022], CppCoreGuidelines License\n[cpp-core-guidelines], CRCalc license [crcalc], Creative Commons Attribution 2.5 Australia [cc-by-\n2.5-au], Creative Commons Attribution 3.0 Germany [cc-by-3.0-de], Creative Commons Attribution\n3.0 Netherlands [cc-by-3.0-nl], Crossword License [crossword], Crypto++ License [cryptopp], Crystal\nStacker License [crystal-stacker], CSL-1.0 [csl-1.0], CSPRNG [csprng], Cube License [cube], cURL\nLicense [curl], CVE ToU [cve-tou], CWE ToU [cwe-tou], CxImage License [cximage], D Zlib [d-zlib],\nDAMAIL [damail], Dante Treglia License [dante-treglia], DBAD License 1.1 [dbad-1.1], Debian\nreportbug License [reportbug], Delorie Historical License [delorie-historical], dhtmlab Public License\n[dhtmlab-public], diffmark License [diffmark], dl-de/by-1-0-de [dl-de-by-1-0-de], dl-de/by-1-0-en\n[dl-de-by-1-0-en], dl-de/by-2-0-de [dl-de-by-2-0-de], dl-de/by-2-0-en [dl-de-by-2-0-en], dmalloc License\n[dmalloc], DMTF License 2017 [dmtf-2017], Docbook License [docbook], Dom4j License [dom4j],\nDotseqn License [dotseqn], Douglas Young License [douglas-young], DRL-1.0 [drl-1.0], DRL-1.1\n[drl-1.1], Dropbear License [dropbear], Dropbear-2016 [dropbear-2016], DSDP License [dsdp], Dtree\nLicense [dtree], dvipdfm License [dvipdfm], DWTFNMFPL-3.0 [dwtfnmfpl-3.0], Dynamic Drive TOU\n[dynamic-drive-tou], ECL 1.0 [ecl-1.0], ECL 2.0 [ecl-2.0], EFL 1.0 [efl-1.0], EFL 2.0 [efl-2.0], EFL\nMIT-Style License [enlightenment], eGenix Public License 1.0.0 [egenix-1.0.0], eGenix Public License\n1.1.0 [egenix-1.1.0], EllisLab License [ellis-lab], EMX Library License [emx-library], EnergyPlus\nBSD-Style License [energyplus-bsd], Enhanced MIT License [emit], enna License [enna], Entessa 1.0\n[entessa-1.0], ePaperPress License [epaperpress], EPICS Open License [epics], Eric Glass License\n[eric-glass], Errbot exception [errbot-exception], Etalab Open License 2.0 [etalab-2.0], Etalab Open\nLicense 2.0 English [etalab-2.0-en], EU DataGrid Software License [eu-datagrid], Fabien Tassin\nLicense [fabien-tassin], Fair License [fair], FAL 1.3 [free-art-1.3], Far Manager exception to BSD-\n3-Clause [far-manager-exception], FASTBuild License 2012-2020 [fastbuild-2012-2020], FastCGI\nDevKit [fastcgi-devkit], FastCGI License for Spec Implementation [openmarket-fastcgi], FatFs\n38\n\nNVIDIA Nemotron Nano 2: An Accurate and Efficient Hybrid Mamba-Transformer Reasoning Model\nLicense [fatfs], FFTPACK License 2004 [fftpack-2004], Filament Group MIT License [filament-\ngroup-mit], Flex 2.5 [flex-2.5], Flora License v1.1 [flora-1.1], font-alias License [font-alias], FPLOT\nLIcense [fplot], Fraunhofer ISO 14496-10 License [fraunhofer-iso-14496-10], FreeBSD Boot [freebsd-\nboot], FreeBSD Doc License [freebsd-doc], FreeBSD unmodified first lines License [freebsd-first],\nFreeMarker License [freemarker], FreeTTS License [freetts], FreeType Project License [freetype],\nFreeware Public License (FPL) [fpl], FSF All Permissive License [fsf-ap], FSF Free Software License\n[fsf-free], FSF Notice [fsf-notice], FSF Unlimited License No Warranty [fsf-unlimited-no-warranty],\nFSF-Unlimited [fsf-unlimited], Fujion Clinical Exception to Apache 2.0 [fujion-exception-to-apache-\n2.0], Gareth McCaughan License [gareth-mccaughan], Gary S. Brown License [gary-s-brown], GDCL\nLicense [gdcl], Generic patent disclaimer [patent-disclaimer], Geoff Kuenning License 1993 [geoff-\nkuenning-1993], Ghostpdl Permissive [ghostpdl-permissive], Glulxe License [glulxe], GLUT License\n[glut], GLWTPL [glwtpl], Good Boy License [good-boy], Graphics Gems License [graphics-gems],\nGreg Roelofs License [greg-roelofs], Gregory Pietsch Liberal License [gregory-pietsch], GStreamer\nException (2005) [gstreamer-exception-2005], GTPL-v1 [gtpl-v1], GTPL-v2 [gtpl-v2], GTPL-v3 [gtpl-\nv3], Haskell Report License [haskell-report], HDF4 License [hdf4], HDF5License [hdf5], HDPARM\nLicense [hdparm], Henry Spencer License 1999 [henry-spencer-1999], Henry Spencer Regexp License\n[hs-regexp], HIDAPI License [hidapi], Historical Notice - NTP [historical-ntp], Historical Permission\nNotice and Disclaimer [historical], Homebrewed License [homebrewed], HP 1986 License [hp-1986],\nHPND sell variant with MIT disclaimer [hpnd-sell-variant-mit-disclaimer], HTML 5 spec License\n[html5], httpget notice and disclaimer [httpget], Ian Kaplan License [ian-kaplan], Ian Piumarta\nLicense [ian-piumarta], IBM AS-IS License [ibm-as-is], IBM DHCP License [ibm-dhcp], IBM Non-\nWarranted Sample Code License [ibm-nwsc], IBM PowerPC Software [ibm-pibs], IBM Sample License\n[ibm-sample], IBPP License [ibpp], ICANN-Public [icann-public], ICOT Free Software [icot-free],\nICU Composite License [ibm-icu], ICU License 58 and later [unicode-icu-58], IDT License Notice\n[idt-notice], IETF License [ietf], IETF Trust License [ietf-trust], ilmid License [ilmid], ImageMagick\nLicense [imagemagick], Independent JPEG Group License - short [ijg-short], Indiana Extreme\nLicense 1.1.1 [indiana-extreme], Indiana Extreme License 1.2 [indiana-extreme-1.2], Infineon Free\nSoftware License [infineon-free], Info-Zip License 1997-10 [info-zip-1997-10], Info-Zip License 2001-01\n[info-zip-2001-01], Info-Zip License 2002-02 [info-zip-2002-02], Info-Zip License 2003-05 [info-zip-\n2003-05], Info-Zip License 2004-05 [info-zip-2004-05], Info-Zip License 2005-02 [info-zip-2005-02],\nInfo-Zip License 2007-03 [info-zip-2007-03], Info-Zip License 2009-01 [info-zip-2009-01], Info-Zip\nLicense [info-zip], Inno Setup License [inno-setup], Intel ACPI SLA [intel-acpi], Intel BSD - Export\nControl [intel-bsd-export-control], Intel BSD 2 Clause License [intel-bsd-2-clause], Intel BSD License\n[intel-bsd], Intel Limited Patent License [intel], Intel OSL 1989 [intel-osl-1989], Intel OSL 1993\n[intel-osl-1993], Intel Royalty Free License [intel-royalty-free], ISC License [isc], ISO 14496-10 [iso-\n14496-10], ISO 8879 [iso-8879], ITU License [itu], JA-SiG License [ja-sig], Jam License [jam], Jason\nMayes License [jason-mayes], Jasper 1.0 [jasper-1.0], JasPer 2.0 [jasper-2.0], Java App Stub License\n[java-app-stub], JDBM License v1.00 [jdbm-1.00], JDOM License [jdom], Jetty License [jetty], JGraph\nLicense [jgraph], JPEG License [ijg], JPNIC idnkit License [jpnic-idnkit], JPNIC mdnkit License\n[jpnic-mdnkit], JPython 1.1 [jpython-1.1], jQuery-Tools-PD [jquery-pd], Jscheme License [jscheme],\nJSFromHell License [jsfromhell], JSON License [json], JSON-js-PD [json-js-pd], JSON-PD [json-pd],\nJython License [jython], Kalle Kaukonen License [kalle-kaukonen], Kazlib [kazlib], Keith Rule\nLicense [keith-rule], Kerberos License [kerberos], Kevan Stannard License [kevan-stannard], Kevlin\nHenney License [kevlin-henney], Khronos License [khronos], Knuth CTAN License [knuth-ctan],\nKumar Robotics License [kumar-robotics], latex-ec-fonts [ecfonts-1.0], Latex2e License [latex2e],\nLatex2e with translated notice permission [latex2e-translated-notice], LBNL BSD Variant [lbnl-\nbsd], LCS-Telegraphics License [lcs-telegraphics], Leptonica License [leptonica], libgd License 2018\n[libgd-2018], libgeoTiff License [libgeotiff], LibMib License [libmib], libmng License 2007 [libmng-\n2007], Libpng License [libpng], LIbpng License v2 [libpng-v2], libselinux License [libselinux-pd],\n39\n\nNVIDIA Nemotron Nano 2: An Accurate and Efficient Hybrid Mamba-Transformer Reasoning Model\nlibsrv License v1.0.2 [libsrv-1.0.2], Lil License v1 [lil-1], LILO License [lilo], Linux Device Drivers\n[linux-device-drivers], Linux-OpenIB [linux-openib], LinuxBIOS License [linuxbios], linuxhowtos\nLicense [linuxhowtos], LLNL [llnl], LLVM Exception to Apache 2.0 [llvm-exception], Logica OSL 1.0\n[logica-1.0], LPPL 1.3c [lppl-1.3c], Lucent Public License 1.0 [lucent-pl-1.0], Lucent Public License\n1.02 [lucent-pl-1.02], Lucre License [lucre], LZMA SDK License (versions 9.22 and beyond) [lzma-sdk-\n9.22], LZMA SDK Public Domain [lzma-sdk-pd], M+ Fonts license [m-plus], MakeHuman License\n[make-human-exception], Markus Kuhn License [markus-kuhn-license], Martin Bergmeier License\n[martin-birgmeier], Matrix Template Library License [mtll], Matt Gallagher Attribution License\n[matt-gallagher-attribution], Matt Kruse License [mattkruse], Matthew Kwan License [matthew-\nkwan], MediaInfo(Lib) License [mediainfo-lib], metamail License [metamail], MgOpen Font License\n[mgopen-font-license], Michael Barr License [michael-barr], Minpack Copyright Notice [minpack],\nMirOS License [mir-os], MIT (SEI) [vince], MIT 1995 [mit-1995], MIT Acknowledgment License\n[mit-ack], MIT Addition License [mit-addition], MIT License 1998 [mit-license-1998], MIT License\n[mit], MIT Modern Variant [mit-modern], MIT Nagy Variant [mit-nagy], MIT no advertising with\nExport Control [mit-no-advert-export-control], MIT No Commercial Use of Trademarks [mit-no-\ntrademarks], MIT no false attribution License [mit-no-false-attribs], MIT Old Style [mit-old-style],\nMIT Old Style no advertising [mit-old-style-no-advert], MIT Old Style Spare [mit-old-style-sparse],\nMIT README License [mit-readme], MIT Synopsys License [mit-synopsys], MIT Taylor Variant\n[mit-taylor-variant], MIT Veillard Variant [mit-veillard-variant], MIT with Export Control [mit-\nexport-control], MIT with Specification Disclaimer [mit-specification-disclaimer], MIT Xfig Variant\n[mit-xfig], MIT-0-Clause [mit-0], mod_dav License 1.0 [mod-dav-1.0], Modified MIT License for\nPublic Domain software [pd-mit], Motorola Microprocessor License [motorola], Mozilla GC License\n[mozilla-gc], MPEG SSG License [mpeg-ssg], MPEG-2 NBC MPEG-4 Audio ISO [mpeg-iso], MPICH\nLicense [mpich], MS Systems Journal Sample Code License [msj-sample-code], MS WS Routing\nSpecifications License [ms-ws-routing-spec], MS-LPL [ms-lpl], MS-PL [ms-pl], MS-SS-PL [ms-sspl],\nMulan PSL v1 [mulanpsl-1.0], Mulan PSL v1.0 (En) [mulanpsl-1.0-en], Mulan PSL v2 [mulanpsl-2.0],\nMulan PSL v2.0 (En) [mulanpsl-2.0-en], Mulle Kybernetik License [mulle-kybernetik], Multics\nLicense [multics], Mup License [mup], musl attribution exception [musl-exception], MX4J License\n1.0 [mx4j], Nara Institute License 2003 [naist-2003], NASA 1.3 [nasa-1.3], NAUMEN Public License\n[naumen], NBPL-1.0 [nbpl-1.0], NCBI Public Domain Notice [ncbi], NCSA Open Source License\n[uoi-ncsa], Net SNMP License [net-snmp], Netcat License [netcat], NetCDF License [netcdf], Netron\nProject License [netron], Newlib Historical License [newlib-historical], Newran License [newran],\nNewsletr License [newsletr], Nice License [nice], NICTA Public Software Licence 1.0 [nicta-psl],\nNiels Ferguson License [niels-ferguson], Nilsson Historical License [nilsson-historical], NIST Public\nDomain Notice [nist-pd], NIST Public Domain Notice with fallback [nist-pd-fallback], NIST Software\nLicense [nist-software], NIST SRD License [nist-srd], NLOD-1.0 [nlod-1.0], NLOD-2.0 [nlod-2.0],\nNLPL [nlpl], Node License [node-js], Non White Heterosexual Male [nwhm], Nonexclusive License\n[nonexclusive], Nortel DASA License [nortel-dasa], Notre Dame License [notre-dame], NRL License\n[nrl], NRL permission [nrl-permission], NTLM License [ntlm], NTP Origin License [ntpl-origin],\nNTP-0 [ntp-0], NVIDIA 2002 License [nvidia-2002], NVIDIA License [nvidia], NVIDIA License\nwith Government Qualifications [nvidia-gov], NYSL 0.9982 [nysl-0.9982], NYSL 0.9982 JP [nysl-\n0.9982-jp], O Young Jong License [o-young-jong], O’Reilly Code Sample Notice [oreilly-notice],\nO-UDA-1.0 [o-uda-1.0], Oasis WS Security Specification License [oasis-ws-security-spec], Object\nForm Exception to MIT [object-form-exception-to-mit], ODC-By-1.0 [odc-by-1.0], ODMG License\n[odmg], OFFIS License [offis], OFL 1.0 [ofl-1.0], OFL 1.0 no Reserved Font Name [ofl-1.0-no-\nrfn], OFL 1.0 Reserved Font Name [ofl-1.0-rfn], OFL 1.1 no Reserved Font Name [ofl-1.1-no-rfn],\nOGC 1.0 [ogc-1.0], OGC Software Notice [ogc], OGL 1.0a [ogl-1.0a], OGL Alberta 2.1 [can-ogl-\nalberta-2.1], OGL British Columbia 2.0 [can-ogl-british-columbia-2.0], OGL Canada 2.0 [can-ogl-2.0-\nen], OGL Canada 2.0 Francais [ogl-canada-2.0-fr], OGL Nova Scotia 1.0 [can-ogl-nova-scotia-1.0],\n40\n\nNVIDIA Nemotron Nano 2: An Accurate and Efficient Hybrid Mamba-Transformer Reasoning Model\nOGL Ontario 1.0 [can-ogl-ontario-1.0], OGL Toronto 1.0 [can-ogl-toronto-1.0], OGL-UK-1.0 [ogl-\nuk-1.0], OGL-UK-2.0 [ogl-uk-2.0], OGL-UK-3.0 [ogl-uk-3.0], OGL-WPD-3.0 [ogl-wpd-3.0], Open\nDirectory License [odl], Open Group Test Suite License [opengroup], Open Publication License 1.0\n[openpub], OpenLDAP Public License 1.1 [openldap-1.1], OpenLDAP Public License 1.2 [openldap-\n1.2], OpenLDAP Public License 1.3 [openldap-1.3], OpenLDAP Public License 1.4 [openldap-\n1.4], OpenLDAP Public License 2.0 [openldap-2.0], OpenLDAP Public License 2.0.1 [openldap-\n2.0.1], OpenLDAP Public License 2.1 [openldap-2.1], OpenLDAP Public License 2.2 [openldap-2.2],\nOpenLDAP Public License 2.2.1 [openldap-2.2.1], OpenLDAP Public License 2.2.2 [openldap-\n2.2.2], OpenLDAP Public License 2.3 [openldap-2.3], OpenLDAP Public License 2.4 [openldap-\n2.4], OpenLDAP Public License 2.5 [openldap-2.5], OpenLDAP Public License 2.6 [openldap-\n2.6], OpenLDAP Public License 2.7 [openldap-2.7], OpenLDAP Public License 2.8 [openldap-2.8],\nOpenORB Community License 1.0 [openorb-1.0], OpenSAML License v1 [opensaml-1.0], OpenSSH\nLicense [openssh], OpenSSL License [openssl], OpenSSL/SSLeay License [openssl-ssleay], OPML 1.0\n[opml-1.0], OPNL-1.0 [opnl-1.0], OPNL-2.0 [opnl-2.0], Oracle BSD-Style with Nuclear Restrictions\n[oracle-bsd-no-nuclear], Original SSLeay License [ssleay], Original SSLeay License with Windows\nClause [ssleay-windows], Oswego Concurrent License [oswego-concurrent], Other Permissive Licenses\n[other-permissive], OWTChart License [owtchart], OZPLB 1.0 [ozplb-1.0], OZPLB 1.1 [ozplb-1.1],\nPaolo Messina 2000 [paolo-messina-2000], ParaView License 1.2 [paraview-1.2], Paul Mackerras\nBinary License [paul-mackerras-binary], Paul Mackerras License [paul-mackerras], Paul Mackerras\nNew License [paul-mackerras-new], Paul Mackerras Simplified License [paul-mackerras-simplified],\nPaulo Soares License [paulo-soares], PayPal SDK License 2013-2016 [paypal-sdk-2013-2016], PBM\nLibrary License [libpbm], PCRE License [pcre], PD’Programming License [pd-programming], PDDL\n1.0 [pddl-1.0], Perl 1.0 [perl-1.0], Peter Deutsch Document License [peter-deutsch-document], Phil\nBunce License [phil-bunce], Philippe De Muyter License [philippe-de-muyter], Phorum License 2.0\n[phorum-2.0], PHP License 2.0.2 [php-2.0.2], PHP License 3.0 [php-3.0], PHP License 3.01 [php-3.01],\nPine License [pine], PngSuite License [pngsuite], Politepix Public License 1.0 [politepix-pl-1.0],\nPostgreSQL License [postgresql], ppp License [ppp], Protobuf License [protobuf], PS Utilities License\n[psutils], PSF Python License 3.7.2 [psf-3.7.2], PSF-2.0 [psf-2.0], psfrag License [psfrag], Psytec\nFree Software License [psytec-freesoft], Public Domain [public-domain], Public Domain Disclaimer\n[public-domain-disclaimer], Purdue BSD-Style License [purdue-bsd], pybench License [pybench],\nPyCrypto License [pycrypto], PyGres License 2.2 [pygres-2.2], Python CWI License [python-cwi],\nPython License 2.0 [python], Python License 2.0.1 [python-2.0.1], Qhull License [qhull], QLogic\nMicrocode [qlogic-microcode], Qpopper License [qpopper], Qualcomm Turing License [qualcomm-\nturing], Quirksmode Copyright Notice [quirksmode], radvd License [radvd], Rdisc License [rdisc],\nRed Hat Attribution License [red-hat-attribution], Red Hat BSD-Simplified [red-hat-bsd-simplified],\nRegexp License [regexp], Repoze License [repoze], RiceBSD [ricebsd], Richard Black License [richard-\nblack], Robert Hubley License [robert-hubley], RSA 1990 [rsa-1990], RSA Cryptoki License [rsa-\ncryptoki], RSA Demo License [rsa-demo], RSA-MD4 License [rsa-md4], RSA-MD5 License [rsa-md5],\nRTools.Util License [rtools-util], Ruby License [ruby], Runtime Library Exception to Apache 2.0\n[apple-runtime-library-exception], Rute Users Tutorial and Exposition License 0.8.0 [rute], Ryszard\nSzopa License [ryszard-szopa], SaaS MIT License [saas-mit], Sash Notice [sash], SATA License [sata],\nSAX-PD [sax-pd], Saxpath License [saxpath], SBIA Part B [sbia-b], ScanCode acknowledgment\n[scancode-acknowledgment], scanlogd License [scanlogd-license], ScanSoft Public License 1.2 [scansoft-\n1.2], SCEA Shared Source License 1.0 [scea-1.0], Scheme Language Report License [schemereport],\nScheme Widget Library (SWL) Software License [swl], Scintilla License [scintilla], Scribbles Demos\nRecognizer Notice [scribbles], Script Asylum License [script-asylum], Secret Labs License 2011 [secret-\nlabs-2011], selinux-nsa-declaration-1.0 [selinux-nsa-declaration-1.0], Sendmail License [sendmail],\nService Availability Forum License [saf], Service Component Architecture License [service-comp-arch],\nSFL License Agreement [sfl-license], SGI CID Font Code Public License 1.0 [sgi-cid-1.0], SGI Free\n41\n\nNVIDIA Nemotron Nano 2: An Accurate and Efficient Hybrid Mamba-Transformer Reasoning Model\nSoftware License B 1.1 [sgi-freeb-1.1], SGI Free Software License B 2.0 [sgi-freeb-2.0], SGI GLX Public\nLicense 1.0 [sgi-glx-1.0], Sglib License [sglib], SGP4 Permission Notice [sgp4], Shital Shah License\n[shital-shah], SIL Open Font License 1.1 with Reserved Font Name [ofl-1.1-rfn], SimPL 1.1 [simpl-1.1],\nSNMP++ License [hp-snmp-pp], snprintf License [snprintf], SoftFloat [softfloat], SoftFloat Legal\nNotice 2.0 [softfloat-2.0], softSurfer License [softsurfer], SolderPad Hardware License v0.5 [shl-0.5],\nSolderpad Hardware License v2.0 [shl-2.0], Solderpad Hardware License v2.1 [shl-2.1], SolderPad\nHardware License, Version 0.51 [shl-0.51], Sparky License [sparky], SpeechWorks Public License\n1.1 [speechworks-1.1], SQLite Blessing [blessing], Standard ML of New Jersey [standard-ml-nj],\nStanford PVRG License [stanford-pvrg], STLport License 2000 [stlport-2000], STLport License 4.5\n[stlport-4.5], STREAM Benchmark License [stream-benchmark], Stu Nicholls License [stu-nicholls],\nSun RPC License [sun-rpc], Sun source code License [sun-source], SunPro Attribution License\n[sunpro], Sunsoft License [sunsoft], Supervisor License [supervisor], svndiff License [svndiff], SWIG\nLibrary License [swig], Symlinks License [symlinks], Symphonysoft [symphonysoft], Synopsys MIT\nLicense [synopsys-mit], Synthesis Toolkit License [synthesis-toolkit], SystemC Open Source License\nAgreement [accellera-systemc], Taiwan Open Government Data License, version 1.0 [ogdl-taiwan-1.0],\nTakao Abe License [takao-abe], Takuya OOURA License [takuya-ooura], Talis Community License\n[ttcl], Tatu Ylonen License [tatu-ylonen], TCG Spec License v1 [tcg-spec-license-v1], TCL/TK\nLicense [tcl], TCP Wrappers License [tcp-wrappers], TekHVC License [tekhvc], Term Readkey\nLicense [term-readkey], Tested Software License [tested-software], TeX Live License [tex-live], Text-\nTabs+Wrap License [ttwl], TFL [tfl], The Happy Bunny License [happy-bunny], Theodore Ts’o license\n[tso-license], Things I Made (TIM) Public License [things-i-made-public-license], Tidy License [tidy],\nTiger Cryptography License [tiger-crypto], Tigra Calendar 3.2 License [tigra-calendar-3.2], Tigra\nCalendar 4.0 License [tigra-calendar-4.0], Tim Janik License 2003 [tim-janik-2003], Time::ParseDate\nLicense [tpdl], Timestamp Picker License [timestamp-picker], TTYP0 License [ttyp0], TU Berlin\nLicense 1.0 [tu-berlin], TU Berlin License 2.0 [tu-berlin-2.0], Tumbolia Public License [tumbolia],\nTwistedSNMP License [twisted-snmp], UCAR License [ucar], UnboundID LDAP SDK Free Use\nLicense [ldap-sdk-free-use], Unicode DFS 2015 [unicode-dfs-2015], Unicode DFS 2016 [unicode-dfs-\n2016], Unicode Inc License Agreement [unicode], Unicode Mappings License [unicode-mappings],\nUniversity of British Columbia License [ubc], University of Michigan OSL [michigan-disclaimer],\nUNIX Network Programming Book License [unpbook], UnixCrypt License [unixcrypt], Unlicense\n[unlicense], Unlimited Binary Use Exception [unlimited-binary-use-exception], UPL 1.0 [upl-1.0], US\nGovernment Public Domain [us-govt-public-domain], US Government Unlimited Rights [us-govt-\nunlimited-rights], USRobotics Permissive License [usrobotics-permissive], Utopia Typeface License\n[utopia], VCalendar License [vcalendar], Vic Metcalfe Public Domain [vic-metcalfe-pd], VIM License\n[vim], Visual Idiot [visual-idiot], Visual Numerics License [visual-numerics], Vixie Cron License [vixie-\ncron], Vovida Software License 1.0 [vsl-1.0], W3C 3-Clause BSD License [w3c-03-bsd-license], W3C\nSoftware Notice and License [w3c], W3C-SOFTWARE-19980720 [w3c-software-19980720], W3C-\nSOFTWARE-DOC-20150513 [w3c-software-doc-20150513], w3m License [w3m], Westhawk License\n[westhawk], Whistle Communications License [whistle], Whitecat License [whitecat], WIDE License\n[wide-license], Wide Open License [wol], Widget Workshop License [widget-workshop], William\nAlexander License [william-alexander], wingo License [wingo], Wordnet License [wordnet], Wrox Press\nLicense [wrox], WS-Addressing Specification License [ws-addressing-spec], WS-Policy Specification\n[ws-policy-specification], WS-Trust Specification [ws-trust-specification], Wsuipa License [wsuipa],\nWTFNMFPL-1.0 [wtfnmfpl-1.0], WTFPL 1.0 [wtfpl-1.0], WTFPL 2.0 [wtfpl-2.0], WTHPL 1.0\n[wthpl-1.0], wxWidgets Licence [wxwidgets], wxWindows Unrestricted Licence 3.0 [wxwindows-u-3.0],\nX11 Documentation License [x11-doc], X11 License [x11], X11-R5 [x11-x11r5], X11-Style (Acer)\n[x11-acer], X11-Style (Adobe) [x11-adobe], X11-Style (Adobe-DEC) [x11-adobe-dec], X11-Style\n(Bitstream Charter) [x11-bitstream], X11-Style (David R. Hanson) [x11-hanson], X11-Style (DEC\n1) [x11-dec1], X11-Style (DEC 2) [x11-dec2], X11-Style (DSC Technologies) [x11-dsc], X11-Style\n42\n\nNVIDIA Nemotron Nano 2: An Accurate and Efficient Hybrid Mamba-Transformer Reasoning Model\n(FSF) [x11-fsf], X11-Style (Keith Packard) [x11-keith-packard], X11-Style (Lucent) [x11-lucent],\nX11-Style (Lucent-variant) [x11-lucent-variant], X11-Style (OAR) [x11-oar], X11-Style (Open Group)\n[x11-opengroup], X11-Style (OpenGL) [x11-opengl], X11-Style (Quarterdeck) [x11-quarterdeck],\nX11-Style (Realmode) [x11-realmode], X11-Style (Silicon Graphics) [x11-sg], X11-Style (Stanford\nUniversity) [x11-stanford], X11-Style (Tektronix) [x11-tektronix], X11-Style (Tiff) [x11-tiff], X11-Style\n(X Consortium Veillard) [x11-xconsortium-veillard], X11-Style (X Consortium) [x11-xconsortium],\nXdebug License v 1.03 [xdebug-1.03], XFree86 License 1.0 [xfree86-1.0], XFree86 License 1.1 [xfree86-\n1.1], xinetd License [xinetd], XML:DB Initiative Software License 1.0 [xmldb-1.0], XSkat License\n[xskat], xxd License [xxd], Yale CAS License [yale-cas], Yensdesign License [yensdesign], Zed License\n[zed], Zend Engine License 2.0 [zend-2.0], ZeusBench notice [zeusbench], ZLIB License [zlib], ZLIB\nLicense with Acknowledgment [zlib-acknowledgement], ZPL 1.0 [zpl-1.0], ZPL 1.1 [zpl-1.1], ZPL\n2.0 [zpl-2.0], ZPL 2.1 [zpl-2.1], zsh License [zsh], Zuora Software License [zuora-software], Zveno\nResearch License [zveno-research]\nThe list above gives the short name (or name, if no short name exists) along with the key, in square\nbrackets, from the ScanCode license dataset available at https://github.com/aboutcode-org/\nscancode-toolkit/tree/develop/src/licensedcode/data/licenses.\n43\n"
    }
  ]
}