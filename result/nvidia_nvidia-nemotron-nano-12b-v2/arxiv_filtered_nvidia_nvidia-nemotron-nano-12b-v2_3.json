{
  "2-3 (API)": "The material that explicitly touches on public access focuses on distribution of model checkpoints rather than a hosted, callable service. The authors state that Nemotron-H model families are \"released … in Hugging Face and NeMo formats on the Hugging Face and NGC model repositories,\" and list concrete SKUs (Nemotron-H-56B-Base, ‑47B-Base, ‑8B-Base as well as the corresponding Reasoning checkpoints). Every sentence mentioning availability refers to downloadable checkpoints and to the NeMo / Hugging Face ecosystem; no sentence advertises an HTTP/REST or Chat-style API comparable to GPT or Gemini. Hence, the only documented public interface for nvidia/nvidia-nemotron-nano-12b-v2 and its sister models is checkpoint download from Hugging Face or NVIDIA NGC; an online inference or pay-per-call API is not mentioned in the quotes.",
  "3-1 (Pre-training)": "Pre-training for the nvidia/nvidia-nemotron-nano-12b-v2 lineage is described in considerable detail.  •  Data volume & curriculum Nemotron-Nano-12B-v2-Base is trained on a full horizon of 20 trillion tokens drawn from \"a large corpus of high-quality curated and synthetically-generated data.\"  A three-phase data-blending curriculum is used: Phase 1 emphasizes diversity, while Phases 2-3 shift toward high-quality sources such as Wikipedia.  After the main three phases, a dedicated long-context \"Phase LC\" performs continuous pre-training (CPT) with a context length of 524,288 tokens and a constant learning rate of 4.5 × 10⁻⁶ to make the model robust to 512 k-token windows.  •  Synthetic data generation Using the instruct version of \"Mistral NeMo 12B12\" (FP8 inference, top-p 0.9, temperature 0.5) the team synthesizes 1.8 T tokens (≈ 336 B low-quality and 1.5 T high-quality).  •  Web-scale sources & scoring The Nemotron-CC corpus is built from 99 Common-Crawl snapshots, yielding 6.3 T tokens (4.4 T deduplicated real, 1.9 T synthetic).  A high-quality subset (Nemotron-CC-HQ, 1.1 T tokens) is also carved out.  Documents are scored for “educational value” by Mistral-8 × 22B-instruct and Nemotron-340B-instruct.  •  Tokenization & batching Training uses a sequence length of 8,192 and a global batch size of 768, i.e. roughly 6.03 M tokens per step.  •  Optimizer & schedule Training is conducted in FP8 precision with a Warmup-Stable-Decay learning-rate schedule.  •  Hardware profile A single 20 T-token run reportedly takes ~40 hours on 1,024 NVIDIA H100 GPUs.  •  Derivative models The 12 B-parameter Nano backbone is the starting point for Nemotron-Nano-9B-v2 (created via pruning/distillation after initial 12 B training).  •  Comparison points & infrastructure All Nemotron-H counterparts (8 B and 56 B) share the FP8 recipe, Megatron-LM training stack, token horizons up to 20 T, and explicit hyper-parameters (peak LR 8 × 10⁻⁴ with 8.3 B-token warm-up for 8 B, LR 4 × 10⁻⁴ with 25 B warm-up for 56 B).  Together, the quotes depict a large-scale FP8 Megatron-LM pipeline, a multi-phase data curriculum, explicit LR values, context-window extension to 512 k tokens, and hardware throughput numbers—all carried over to the 12 B Nano v2 model.",
  "3-2 (Fine-tuning)": "Fine-tuning and post-training of Nemotron-Nano-12B-v2 (\"Nemotron Nano 2 12B\") and related checkpoints follow a rich multi-stage pipeline.  •  Supervised stages Stage 1 SFT is applied in three distinct passes (as referenced in Figure 4) to create the “Merged” Nemotron Nano 2 12B checkpoint.  Additional SFT passes target specialised skills such as tool use, extremely long-context inference, and truncated-budget training.  The general SFT blend mentioned for other Nemotron-H/T variants contains six million code, math, and instruction-following examples.  •  Model compression After alignment, \"Minitron-style\" pruning+distillation is combined with \"Puzzle\" flexibility in the new \"MiniPuzzle\" framework.  MiniPuzzle is used to shrink Nemotron-H-56B-Base to 47 B with only 63 B training tokens under FP8, and the same distillation paradigm is later invoked for Nemotron-Nano-12B-v2 after its SFT/GRPO/DPO/RLHF stages.  •  Vision-language adaptation Nemotron-H-VLM uses a two-stage recipe: (1) VL pre-training where only a two-layer FFN projector is trained while keeping the Nemotron-H backbone and vision encoder frozen, and (2) VL SFT, where the entire stack is fine-tuned end-to-end on task-oriented datasets.  •  Deployment goals The authors explicitly state that Nemotron-H base models \"can be effectively post-trained\" for instruction-following, VL, and long-context benchmarks, underlining the versatility of the fine-tuning pipeline.  In sum, Nemotron-Nano-12B-v2 fine-tuning involves multi-round SFT, domain-specific targeted SFT, advanced compression (MiniPuzzle), and (for vision tasks) staged modality alignment before full end-to-end SFT.",
  "3-3 (Reinforcement Learning)": "Reinforcement and preference-based alignment play a major role in the Nemotron Nano 2 workflow.  •  Techniques combined The quotes list Group Relative Policy Optimization (GRPO), Direct Preference Optimization (DPO), classic Reinforcement Learning from Human Feedback (RLHF), and (for some variants) offline RPO.  •  Nano-specific loop \"Nano V2 undergoes reinforcement learning … through iterative stages of Direct Preference Optimization.\"  For each checkpoint emerging from the preceding long-context phase, the team generates on-policy data containing positive (successful tool call) and negative (failed) traces for every WorkBench prompt, enabling fine-grained reward shaping toward tool usage accuracy.  •  Effect of each stage According to the authors, \"GRPO and RLHF sharpened instruction-following and conversational ability, while additional DPO stages further strengthened tool use.\"  •  Broader model family Nemotron-H 8B and 47B \"Reasoning\" models also receive multi-phase GRPO after SFT, and instruction-tuned Nemotron-T/H variants apply preference tuning via offline RPO in stage 2 of their alignment pipeline.  •  End-to-end chain Finally, the quotes note that Nemotron-Nano-12B-v2-Base is first aligned with \"several stages of SFT, GRPO, DPO, and RLHF\" and only then compressed with the MiniPuzzle/Minitron distillation strategy, indicating that reinforcement learning is integrated before model size reduction.  Collectively, the material paints a picture of a layered RLHF stack: SFT → GRPO → (iterative) DPO / RLHF → optional offline RPO → distillation, with dedicated on-policy data collection for tool-calling quality.",
  "2-3 (API)__evidence": [
    {
      "source": "[sections/2504.03624]",
      "quote": "To enable further research, we have released the following base model checkpoints in Hugging Face and NeMo formats on the Hugging Face and NGC model repositories: • Nemotron-H-56B-Base. Hugging Face and NGC. • Nemotron-H-47B-Base. Hugging Face and NGC. • Nemotron-H-8B-Base. Hugging Face and NGC."
    },
    {
      "source": "[sections/Conclusions]",
      "quote": "We have released the Nemotron-H base checkpoints described in this paper with support in Hugging Face and NeMo to facilitate further research: • Nemotron-H-56B-Base. Hugging Face and NGC. • Nemotron-H-47B-Base. Hugging Face and NGC. • Nemotron-H-8B-Base. Hugging Face and NGC."
    },
    {
      "source": "[sections/Conclusions]",
      "quote": "Nemotron-H Reasoning checkpoints described are also available in Hugging Face: • Nemotron-H-47B-Reasoning. Hugging Face. • Nemotron-H-8B-Reasoning. Hugging Face."
    }
  ],
  "3-1 (Pre-training)__evidence": [
    {
      "source": "[pdf_text]",
      "quote": "To achieve the above results, we pre-train Nemotron-H models on up to 20 trillion tokens of high-quality curated and synthetically-generated data (Su et al., 2024; Akter et al., 2024)."
    },
    {
      "source": "[pdf_text]",
      "quote": "Nemotron-H-56B-Base is the first Nemotron model to be fully pre-trained using a FP8-based recipe."
    },
    {
      "source": "[pdf_text]",
      "quote": "We trained Nemotron-H-8B-Base on a token horizon of 15 trillion tokens and Nemotron-H-56B-Base on a token horizon of 20 trillion tokens. We use a sequence length of 8192 and global batch size of 768 (6291456 tokens per batch)."
    },
    {
      "source": "[pdf_text]",
      "quote": "For Nemotron-H-8B-Base, we used a peak learning rate of 8·10−4 and warmup over 8.3 billion tokens; for Nemotron-H-56B-Base, we used a peak learning rate of 4 · 10−4 and warmup over 25 billion tokens."
    },
    {
      "source": "[pdf_text]",
      "quote": "We pre-train Nemotron-H using Megatron-LM; we rely on Transformer Engine for FP8 support."
    },
    {
      "source": "[abstract]",
      "quote": "We create Nemotron-Nano-9B-v2 by first pre-training a 12-billion-parameter model (Nemotron-Nano-12B-v2-Base) on 20 trillion tokens using an FP8 training recipe."
    },
    {
      "source": "[sections/Pretraining]",
      "quote": "Nemotron-Nano-12B-v2-Base was pre-trained using FP8 precision (§2.4) over 20 trillion tokens using a Warmup-Stable-Decay (Hu et al., 2024) learning rate schedule (§2.5)."
    },
    {
      "source": "[sections/Pretraining]",
      "quote": "Nemotron-Nano-12B-v2-Base was pre-trained on a large corpus of high-quality curated and synthetically-generated data."
    },
    {
      "source": "[pdf_text]",
      "quote": "We used a curriculum based on three phases of data-blending approach to pre-train Nemotron-Nano-12B-v2-Base. In the first phase, we used a data mixture that promotes diversity in data; in the second and third phases, we primarily used high-quality datasets (e.g., Wikipedia)."
    },
    {
      "source": "[pdf_text]",
      "quote": "We trained Nemotron-Nano-12B-v2-Base on a token horizon of 20 trillion tokens."
    },
    {
      "source": "[pdf_text]",
      "quote": "To ensure Nemotron-Nano-12B-v2-Base can infer over long context windows, we added a long-context phase (Phase LC) after Phase 3 of pre-training. In Phase LC, we did continuous pretraining (CPT) with a context length of 524,288 (512k) tokens using a constant learning rate of 4.5 · 10−6."
    },
    {
      "source": "[sections/Synthetic Data Generation]",
      "quote": "Using the instruct version of Mistral NeMo 12B12 with FP8 inference, a top-p value of 0.9, and a sampling temperature of 0.5, we synthesize over 1.8T tokens as Table 3 shows, including 336.3B tokens from low-quality documents and 1.5T tokens from high-quality documents."
    },
    {
      "source": "[sections/2504.03624]",
      "quote": "To achieve the above results, we pre-train Nemotron-H models on up to 20 trillion tokens of high-quality curated and synthetically-generated data (Su et al., 2024; Akter et al., 2024)."
    },
    {
      "source": "[sections/2504.03624]",
      "quote": "Nemotron-H-56B-Base is the first Nemotron model to be fully pre-trained using a FP8-based recipe."
    },
    {
      "source": "[sections/2504.03624]",
      "quote": "Nemotron-H-8B-Base and Nemotron-H-56B-Base are pre-trained on a large corpus of high-quality curated and synthetically-generated data."
    },
    {
      "source": "[sections/2504.03624]",
      "quote": "We pre-train Nemotron-H using Megatron-LM; we rely on Transformer Engine for FP8 support."
    },
    {
      "source": "[sections/2504.03624]",
      "quote": "We trained Nemotron-H-8B-Base on a token horizon of 15 trillion tokens and Nemotron-H-56B-Base on a token horizon of 20 trillion tokens."
    },
    {
      "source": "[sections/2504.03624]",
      "quote": "For Nemotron-H-8B-Base, we used a peak learning rate of 8·10−4 and warmup over 8.3 billion tokens; for Nemotron-H-56B-Base, we used a peak learning rate of 4 · 10−4 and warmup over 25 billion tokens."
    },
    {
      "source": "[sections/5.1 Nemotron-H Vs. Transformer Model Alignment]",
      "quote": "We conducted experiments on Transformer (Nemotron-T-8B-Exp-Base) and hybrid (Nemotron-H-8B-Exp-Base) models pretrained on identical data resulting in two models with very similar pretraining performance as seen in Table 9."
    },
    {
      "source": "[sections/2508.14444]",
      "quote": "The initial base model, Nemotron-Nano-12B-v2-Base, was pre-trained using FP8 precision (§2.4) over 20 trillion tokens using a Warmup-Stable-Decay (Hu et al., 2024) learning rate schedule (§2.5)."
    },
    {
      "source": "[sections/2508.14444]",
      "quote": "Nemotron-Nano-12B-v2-Base was pre-trained on a large corpus of high-quality curated and synthetically-generated data."
    },
    {
      "source": "[sections/2508.14444]",
      "quote": "We trained Nemotron-Nano-12B-v2-Base on a token horizon of 20 trillion tokens. We used a sequence length of 8192 and global batch size of 768 (6,029,312 tokens per batch)."
    },
    {
      "source": "[sections/2508.14444]",
      "quote": "To ensure Nemotron-Nano-12B-v2-Base can infer over long context windows, we added a long-context phase (Phase LC) after Phase 3 of pre-training."
    },
    {
      "source": "[sections/Conclusion]",
      "quote": "To create Nemotron-Nano-9B-v2, we started by pre-training Nemotron-Nano-12B-v2-Base on 20T tokens, using a carefully constructed mix of curated and synthetically generated data."
    },
    {
      "source": "[pdf_text]",
      "quote": "Combining the techniques above to the 99 snapshots CC-MAIN-2013-20 through CC-MAIN-2024-30 of Common Crawl, we create a 6.3T token dataset (Nemotron-CC), consisting of 4.4T globally deduplicated tokens and 1.9T synthetically derived tokens."
    },
    {
      "source": "[pdf_text]",
      "quote": "To enable a fairer comparison over relatively short token horizons, we thus also consider a 1.1T token high quality subset of our data (Nemotron-CC-HQ), consisting of just the highest-scoring real and diverse QA pairs synthetic data."
    },
    {
      "source": "[pdf_text]",
      "quote": "We prompt Mistral 8x22B-instruct10 and Nemotron-340B-instruct (Adler et al., 2024), to score web documents from FineWeb based on their educational value on a scale from 0 to 5."
    },
    {
      "source": "[pdf_text]",
      "quote": "Using the instruct version of Mistral NeMo 12B12 with FP8 inference, a top-p value of 0.9, and a sampling temperature of 0.5, we synthesize over 1.8T tokens as Table 3 shows"
    },
    {
      "source": "[sections/3.2 Short Token Horizon]",
      "quote": "Our high quality dataset (Nemotron-CC-HQ) shows accuracy gains over DCLM and FineWeb-Edu on all tasks except RACE."
    },
    {
      "source": "[sections/3.2 Main Results]",
      "quote": "Our complete 6.3T token dataset (Nemotron-CC) gives MMLU and average accuracies roughly on par with DCLM. But since this dataset contains 4× more unique real tokens, we expect it to be superior in data-constrained settings like 15T token training runs."
    },
    {
      "source": "[appendix/Section D Training Details]",
      "quote": "A single training run takes about 40 hours using 1024 NVIDIA H100 GPUs."
    }
  ],
  "3-2 (Fine-tuning)__evidence": [
    {
      "source": "[pdf_text]",
      "quote": "Following NVLM (Dai et al., 2024), Nemotron-H-VLM is trained in two stages: 1. VL pre-training. We train only the two-layer FFN for modality alignment while keeping both the Nemotron-H backbone and vision encoder frozen. 2. VL SFT. We fine-tune the vision encoder, FFN projector, and Nemotron-H backbone end-to-end on various task-oriented SFT data."
    },
    {
      "source": "[pdf_text]",
      "quote": "We use MiniPuzzle to distill Nemotron-H-56B-Base to Nemotron-H-47B-Base, using only 63 billion training tokens and FP8 training."
    },
    {
      "source": "[sections/Post-training]",
      "quote": "Nemotron Nano 2 was then post-trained through a combination of Supervised Fine-Tuning (SFT), Group Relative Policy Optimization (GRPO) (Shao et al., 2024), Direct Preference Optimization (DPO) (Rafailov et al., 2023), and Reinforcement Learning from Human Feedback (RLHF) (Ouyang et al., 2022; Christiano et al., 2017). We applied multiple SFT stages across various domains, followed by targeted SFT on key areas such as tool use, long-context performance, and truncated (budgeted) training."
    },
    {
      "source": "[sections/Alignment]",
      "quote": "Figure 4 | Flow of alignment procedures followed to arrive at the final \"Merged\" Nemotron Nano 2 12B checkpoint. Stage 1 SFT. As Figure 4 illustrates, we employ three distinct stages of supervised fine-tuning."
    },
    {
      "source": "[sections/2504.03624]",
      "quote": "To efficiently tailor Nemotron-H models for different deployment scenarios, we introduce a new compression via pruning and distillation paradigm, called MiniPuzzle, that combines the simplicity of Minitron (Sreenivas et al., 2024) and the versatility of Puzzle (Bercovich et al., 2024)."
    },
    {
      "source": "[sections/2504.03624]",
      "quote": "We use MiniPuzzle to distill Nemotron-H-56B-Base to Nemotron-H-47B-Base, using only 63 billion training tokens and FP8 training."
    },
    {
      "source": "[sections/2504.03624]",
      "quote": "The Nemotron-H base models can also be effectively post-trained to produce models with high accuracies on vision-language, instruction following, and long-context (e.g., RULER) benchmarks."
    },
    {
      "source": "[sections/5.1 Nemotron-H Vs. Transformer Model Alignment]",
      "quote": "We then post-trained both base models into instruction-tuned variants, Nemotron-T-8B-Exp-Instruct and Nemotron-H-8B-Exp-Instruct using a multistage training procedure. In stage1, we perform supervised fine-tuning (SFT) on a blend of 6 million samples including code, math, and general instruction-following tasks."
    },
    {
      "source": "[sections/2508.14444]",
      "quote": "Nemotron Nano 2 was then post-trained through a combination of Supervised Fine-Tuning (SFT), Group Relative Policy Optimization (GRPO) (Shao et al., 2024), Direct Preference Optimization (DPO) (Rafailov et al., 2023), and Reinforcement Learning from Human Feedback (RLHF) (Ouyang et al., 2022; Christiano et al., 2017)."
    },
    {
      "source": "[sections/Conclusion]",
      "quote": "We aligned Nemotron-Nano-12B-v2-Base using several stages of SFT, GRPO, DPO, and RLHF before using the Minitron compression via pruning and distillation strategy to produce the final model."
    }
  ],
  "3-3 (Reinforcement Learning)__evidence": [
    {
      "source": "[sections/Post-training]",
      "quote": "Nemotron Nano 2 was then post-trained through a combination of Supervised Fine-Tuning (SFT), Group Relative Policy Optimization (GRPO) (Shao et al., 2024), Direct Preference Optimization (DPO) (Rafailov et al., 2023), and Reinforcement Learning from Human Feedback (RLHF) (Ouyang et al., 2022; Christiano et al., 2017). GRPO and RLHF sharpened instruction-following and conversational ability, while additional DPO stages further strengthened tool use."
    },
    {
      "source": "[sections/Alignment]",
      "quote": "Nano V2 undergoes reinforcement learning in this environment through iterative stages of Direct Preference Optimization. For each candidate checkpoint from the long-context stage, we generate on-policy data consisting of positive examples (successful tool calls) and negative examples (failed generations) for every WorkBench prompt."
    },
    {
      "source": "[sections/5.2.3 Reinforcement Learning with GRPO]",
      "quote": "Following the pilot study in which we found that mamba2-hybrid models and pure transformer models have similar post-training properties we proceed to train our Nemotron-H 8B Reasoning and Nemotron-H 47B Reasoning models. After SFT, we applied Group Relative Policy Optimization (GRPO) in multiple phases (Shao et al., 2024)."
    },
    {
      "source": "[sections/5.1 Nemotron-H Vs. Transformer Model Alignment]",
      "quote": "We then post-trained both base models into instruction-tuned variants, Nemotron-T-8B-Exp-Instruct and Nemotron-H-8B-Exp-Instruct using a multistage training procedure. In stage2, we switch to preference tuning using offline RPO (Sun et al., 2025) on general-domain prompts."
    },
    {
      "source": "[sections/2508.14444]",
      "quote": "Nemotron Nano 2 was then post-trained through a combination of Supervised Fine-Tuning (SFT), Group Relative Policy Optimization (GRPO) (Shao et al., 2024), Direct Preference Optimization (DPO) (Rafailov et al., 2023), and Reinforcement Learning from Human Feedback (RLHF) (Ouyang et al., 2022; Christiano et al., 2017)."
    },
    {
      "source": "[sections/2508.14444]",
      "quote": "Nano V2 undergoes reinforcement learning in this environment through iterative stages of Direct Preference Optimization."
    },
    {
      "source": "[sections/Conclusion]",
      "quote": "We aligned Nemotron-Nano-12B-v2-Base using several stages of SFT, GRPO, DPO, and RLHF before using the Minitron compression via pruning and distillation strategy to produce the final model."
    }
  ]
}