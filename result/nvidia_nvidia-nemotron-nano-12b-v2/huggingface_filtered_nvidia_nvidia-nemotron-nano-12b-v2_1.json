{
  "1-1 (Weights)": "The NVIDIA-Nemotron-Nano-12B-v2 weights are publicly hosted on Hugging Face. The repository URL is explicitly given — “https://huggingface.co/nvidia/NVIDIA-Nemotron-Nano-12B-v2” — and a date stamp (“Huggingface 08/29/2025”) shows when the files were made available. Multiple loading examples confirm open, direct download access: a standard Hugging Face call,  \n\n    model = AutoModelForCausalLM.from_pretrained(\"nvidia/NVIDIA-Nemotron-Nano-12B-v2\", torch_dtype=torch.bfloat16, trust_remote_code=True, device_map=\"auto\")\n\nand a vLLM server command,  \n\n    vllm serve nvidia/NVIDIA-Nemotron-Nano-12B-v2 \\\n\nBoth snippets demonstrate that anyone with the repository name can pull the checkpoints and run them locally, optionally trusting remote code and automatically placing shards on available GPUs.  The physical checkpoint layout is listed: “model-00001-of-00006.safetensors” through “…-00006”, with an accompanying “model.safetensors.index.json” manifest, indicating a six-way sharded, safetensors-formatted release suitable for efficient streaming loads.  All of these quotes jointly show that the full parameter weights are downloadable without additional gating beyond accepting the repository’s license terms.",
  "1-1 (Weights)__evidence": [
    {
      "source": "[readme]",
      "quote": "Huggingface 08/29/2025 via https://huggingface.co/nvidia/NVIDIA-Nemotron-Nano-12B-v2"
    },
    {
      "source": "[readme]",
      "quote": "model = AutoModelForCausalLM.from_pretrained(\n    \"nvidia/NVIDIA-Nemotron-Nano-12B-v2\",\n    torch_dtype=torch.bfloat16,\n    trust_remote_code=True,\n    device_map=\"auto\"\n)"
    },
    {
      "source": "[readme]",
      "quote": "vllm serve nvidia/NVIDIA-Nemotron-Nano-12B-v2 \\"
    },
    {
      "source": "[files]",
      "quote": "model-00001-of-00006.safetensors"
    },
    {
      "source": "[files]",
      "quote": "model.safetensors.index.json"
    }
  ],
  "1-2 (Code)": "Public code exists for both training and inference, but at different granularity levels.  An overview sentence states, “NVIDIA-Nemotron-Nano-12B-v2 … was trained using [Megatron-LM] and [NeMo-RL]”, explicitly naming two open-source training frameworks that host the end-to-end pre-training and RLHF pipelines.  Although the exact configuration files are not quoted, the mention of these repositories indicates that the authors relied on community-available training code bases.  For inference/serving, the repository contains model-specific implementation files such as “modeling_nemotron_h.py” marked with the header \"\"\"PyTorch NemotronH model.\"\"\", showing that a custom Hugging Face Transformers compatible model class is shipped.  Because the snippet is inside the model repo that also holds the weights, users automatically receive the inference code when they call from_pretrained with trust_remote_code=True.  The quotes do not mention any private or withheld components; therefore, the evidence points to public availability of the inference adapter as well as the generic training frameworks, while leaving open whether bespoke training hyper-parameters were released.",
  "1-2 (Code)__evidence": [
    {
      "source": "[readme]",
      "quote": "NVIDIA-Nemotron-Nano-12B-v2 is a large language model (LLM) trained from scratch by NVIDIA, and designed as a unified model for both reasoning and non-reasoning tasks. The model was trained using [Megatron-LM](https://github.com/NVIDIA/Megatron-LM) and [NeMo-RL](https://github.com/NVIDIA-NeMo/RL)."
    },
    {
      "source": "[files]",
      "quote": "modeling_nemotron_h.py"
    },
    {
      "source": "[py_files/modeling_nemotron_h.py]",
      "quote": "\"\"\"PyTorch NemotronH model.\"\"\""
    }
  ],
  "1-3 (License)": "The model files are distributed under NVIDIA’s own terms:  \n• “license_name: nvidia-open-model-license”  \n• “GOVERNING TERMS: Use of this model is governed by the [NVIDIA Open Model License Agreement]”.  These statements make clear that the model weights are not under a standard permissive license like Apache-2.0; instead they fall under NVIDIA’s proprietary Open Model License, which imposes its own use, distribution, and commercialisation conditions.  Separately, a code comment states, “# Licensed under the Apache License, Version 2.0 … you may not use this file except in compliance with the License.”  This shows a dual-licensing situation: the surrounding helper or example code is Apache-2.0, whereas the core checkpoints remain under the NVIDIA Open Model License.  No further restrictions or grants are quoted, so all enforceable terms must be derived from the linked agreement.",
  "1-3 (License)__evidence": [
    {
      "source": "[readme]",
      "quote": "license_name: nvidia-open-model-license"
    },
    {
      "source": "[readme]",
      "quote": "GOVERNING TERMS: Use of this model is governed by the [NVIDIA Open Model License Agreement](https://www.nvidia.com/en-us/agreements/enterprise-software/nvidia-open-model-license/)."
    },
    {
      "source": "[py_files/modeling_nemotron_h.py]",
      "quote": "# Licensed under the Apache License, Version 2.0 (the \"License\"); # you may not use this file except in compliance with the License."
    },
    {
      "source": "[readme]",
      "quote": "reason\": response.choices[0].finish_reason,\n }\n return response_data\n```\n\nCalling the server with a budget (Restricted to 32 tokens here as an example)\n\n```py\ntokenizer_name_or_path = \"nvidia/NVIDIA-Nemotron-Nano-12B-v2\"\nclient = Thinking"
    }
  ],
  "1-4 (Paper)": "A formal technical report accompanies the release:  \n• Title: “NVIDIA Nemotron Nano 2: An Accurate and Efficient Hybrid Mamba-Transformer Reasoning Model”  \n• Available on arXiv: https://arxiv.org/abs/2508.14444  \n• BibTeX entry begins “@misc{nvidia2025nvidianemotronnano2, title={NVIDIA Nemotron Nano 2: An Accurate and Efficient Hybrid Mamba-Transformer Reasoning Model}”.  These quotes confirm that a peer-review-ready manuscript describing the architecture, training procedure and evaluation results for NVIDIA-Nemotron-Nano-12B-v2 (branded “Nano 2”) is publicly accessible.  The arXiv identifier and year (2025) provide a citable reference point for users who need methodological details beyond the repository README.",
  "1-4 (Paper)__evidence": [
    {
      "source": "[readme]",
      "quote": "- [NVIDIA Nemotron Nano 2: An Accurate and Efficient Hybrid Mamba-Transformer Reasoning Model](https://arxiv.org/abs/2508.14444)"
    },
    {
      "source": "[readme]",
      "quote": "@misc{nvidia2025nvidianemotronnano2,\n      title={NVIDIA Nemotron Nano 2: An Accurate and Efficient Hybrid Mamba-Transformer Reasoning Model},"
    }
  ]
}