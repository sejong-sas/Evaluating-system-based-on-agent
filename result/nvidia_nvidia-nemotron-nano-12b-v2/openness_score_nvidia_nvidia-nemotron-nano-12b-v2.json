{
  "model": "nvidia/NVIDIA-Nemotron-Nano-12B-v2",
  "scores": {
    "1-2 Code": {
      "score": 0.0,
      "reason": "No training pipeline files; README mentions are ignored."
    },
    "1-3 License": {
      "score": 0.5,
      "reason": "Weights are distributed under NVIDIA-Open-Model-License, a proprietary licence that imposes its own conditions; helper code is Apache-2.0. Because at least one of the four freedoms (e.g. unrestricted redistribution / commercial use) is limited, the licence is Semi-Open."
    },
    "1-4 Paper": {
      "score": 1.0,
      "reason": "An arXiv tech report dedicated to the exact model (\"NVIDIA Nemotron Nano 2\", arXiv:2508.14444) is publicly available."
    },
    "1-5 Architecture": {
      "score": 1,
      "reason": "Architecture evidence present in extracted quotes."
    },
    "1-6 Tokenizer": {
      "score": 1.0,
      "reason": "Tokenizer files downloadable in repository."
    },
    "2-1 Hardware": {
      "score": 1.0,
      "reason": "Quotes specify both hardware type and quantity (e.g., “a single training run takes about 40 h using 1024 NVIDIA H100 GPUs”)."
    },
    "2-2 Software": {
      "score": 1.0,
      "reason": "Training stack details are given: Megatron-LM, Transformer-Engine FP8 support, NeMo-RL, tensor/data/sequence/context parallelism, LR schedule, precision formats—all sufficient to reconstruct the environment."
    },
    "2-3 API": {
      "score": 0.0,
      "reason": "Only generic or non-official mentions; no qualifying official API docs.  No qualifying official API docs via web search."
    },
    "3-1 Pre-training": {
      "score": 1.0,
      "reason": "Methodology is fully disclosed: 20 T-token horizon, three-phase curriculum + long-context phase, FP8 recipe, batch sizes, LR schedule, synthetic-data generation, hardware parallelism."
    },
    "3-2 Fine-tuning": {
      "score": 1.0,
      "reason": "Multi-stage SFT pipeline, data sizes, domains, compression (MiniPuzzle), and targeted SFT stages are documented in detail, enabling reproduction."
    },
    "3-3 Reinforcement Learning": {
      "score": 1.0,
      "reason": "RL alignment methods (GRPO, DPO, RLHF, on-policy data collection) and the NeMo-RL framework are explicitly described with stages and purposes."
    },
    "4-1 Pre-training Data": {
      "score": 1.0,
      "reason": "Sources, sizes, language/code mix, synthetic portions, release location (Nemotron-Pre-Training-Dataset-v1) and detailed composition are provided, allowing corpus reconstruction."
    },
    "4-2 Fine-tuning Data": {
      "score": 1.0,
      "reason": "Post-training corpus size (~90 B tokens), language list, document types, synthetic SFT data generation procedure and public release (Post-Training-Dataset-v2) are disclosed."
    },
    "4-3 Reinforcement Learning Data": {
      "score": 0.5,
      "reason": "RL datasets (e.g., Nemotron-PrismMath 4.6 B tokens, multilingual RL data in Post-Training-Dataset-v2) are mentioned, but only partial composition details are given."
    },
    "4-4 Data Filtering": {
      "score": 1.0,
      "reason": "A concrete, multi-stage filtering pipeline is detailed: HTML-to-text, language filtering, exact and fuzzy deduplication, ensemble quality classifiers, heuristic & perplexity filters, FineMath classifier, licence filtering for code."
    },
    "1-1 Weights": {
      "score": 1,
      "reason": "Weights files present in the repository (e.g., *.safetensors/bin/pt/ckpt)."
    }
  },
  "included_scores": {
    "1-2 Code": {
      "score": 0.0,
      "reason": "No training pipeline files; README mentions are ignored."
    },
    "1-3 License": {
      "score": 0.5,
      "reason": "Weights are distributed under NVIDIA-Open-Model-License, a proprietary licence that imposes its own conditions; helper code is Apache-2.0. Because at least one of the four freedoms (e.g. unrestricted redistribution / commercial use) is limited, the licence is Semi-Open."
    },
    "1-4 Paper": {
      "score": 1.0,
      "reason": "An arXiv tech report dedicated to the exact model (\"NVIDIA Nemotron Nano 2\", arXiv:2508.14444) is publicly available."
    },
    "1-5 Architecture": {
      "score": 1,
      "reason": "Architecture evidence present in extracted quotes."
    },
    "1-6 Tokenizer": {
      "score": 1.0,
      "reason": "Tokenizer files downloadable in repository."
    },
    "2-1 Hardware": {
      "score": 1.0,
      "reason": "Quotes specify both hardware type and quantity (e.g., “a single training run takes about 40 h using 1024 NVIDIA H100 GPUs”)."
    },
    "2-2 Software": {
      "score": 1.0,
      "reason": "Training stack details are given: Megatron-LM, Transformer-Engine FP8 support, NeMo-RL, tensor/data/sequence/context parallelism, LR schedule, precision formats—all sufficient to reconstruct the environment."
    },
    "2-3 API": {
      "score": 0.0,
      "reason": "Only generic or non-official mentions; no qualifying official API docs.  No qualifying official API docs via web search."
    },
    "3-1 Pre-training": {
      "score": 1.0,
      "reason": "Methodology is fully disclosed: 20 T-token horizon, three-phase curriculum + long-context phase, FP8 recipe, batch sizes, LR schedule, synthetic-data generation, hardware parallelism."
    },
    "3-2 Fine-tuning": {
      "score": 1.0,
      "reason": "Multi-stage SFT pipeline, data sizes, domains, compression (MiniPuzzle), and targeted SFT stages are documented in detail, enabling reproduction."
    },
    "3-3 Reinforcement Learning": {
      "score": 1.0,
      "reason": "RL alignment methods (GRPO, DPO, RLHF, on-policy data collection) and the NeMo-RL framework are explicitly described with stages and purposes."
    },
    "4-1 Pre-training Data": {
      "score": 1.0,
      "reason": "Sources, sizes, language/code mix, synthetic portions, release location (Nemotron-Pre-Training-Dataset-v1) and detailed composition are provided, allowing corpus reconstruction."
    },
    "4-2 Fine-tuning Data": {
      "score": 1.0,
      "reason": "Post-training corpus size (~90 B tokens), language list, document types, synthetic SFT data generation procedure and public release (Post-Training-Dataset-v2) are disclosed."
    },
    "4-3 Reinforcement Learning Data": {
      "score": 0.5,
      "reason": "RL datasets (e.g., Nemotron-PrismMath 4.6 B tokens, multilingual RL data in Post-Training-Dataset-v2) are mentioned, but only partial composition details are given."
    },
    "4-4 Data Filtering": {
      "score": 1.0,
      "reason": "A concrete, multi-stage filtering pipeline is detailed: HTML-to-text, language filtering, exact and fuzzy deduplication, ensemble quality classifiers, heuristic & perplexity filters, FineMath classifier, licence filtering for code."
    },
    "1-1 Weights": {
      "score": 1,
      "reason": "Weights files present in the repository (e.g., *.safetensors/bin/pt/ckpt)."
    }
  },
  "final_score_10pt": 8.125,
  "meta": {
    "usage_from_dispatch": {
      "fine_tuning": "used",
      "rl": "used"
    },
    "excluded": [],
    "denominator": 16,
    "raw_sum": 13.0,
    "scale": "10/16",
    "code_detection_reason": "No training pipeline files; README mentions are ignored.",
    "pretrain_sources_used": false
  }
}