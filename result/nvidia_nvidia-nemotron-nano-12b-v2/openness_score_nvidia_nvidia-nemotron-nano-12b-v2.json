{
  "model": "nvidia/NVIDIA-Nemotron-Nano-12B-v2",
  "scores": {
    "1-2 Code": {
      "score": 0.0,
      "reason": "No training pipeline files; README mentions are ignored."
    },
    "1-3 License": {
      "score": 1.0,
      "reason": "Weights are under the NVIDIA Open Model License, which the rubric lists as an ‘Open’ license (all four freedoms with only minor restrictions)."
    },
    "1-4 Paper": {
      "score": 1.0,
      "reason": "Provider-authored technical report “NVIDIA Nemotron Nano 2 …” (arXiv 2508.14444) specifically documents this model and is publicly available."
    },
    "1-5 Architecture": {
      "score": 1.0,
      "reason": "Hugging Face presence detected; forced Open per FORCE_HF_CORE_OPEN."
    },
    "1-6 Tokenizer": {
      "score": 1.0,
      "reason": "Hugging Face presence detected; forced Open per FORCE_HF_CORE_OPEN."
    },
    "1-1 Weights": {
      "score": 1.0,
      "reason": "Hugging Face presence detected; forced Open per FORCE_HF_CORE_OPEN."
    },
    "2-1 Hardware": {
      "score": 1.0,
      "reason": "Quote gives both type and quantity: “A single training run takes about 40 hours using 1 024 NVIDIA H100 GPUs.”"
    },
    "2-2 Software": {
      "score": 0.5,
      "reason": "Quotes list core training stack components (Megatron-LM, Transformer Engine, FP8 recipe, tensor/data/context/sequence parallelism) but do not enumerate the *full* stack with versions for every library; hence partial disclosure."
    },
    "2-3 API": {
      "score": 0.0,
      "reason": "Only generic or non-official mentions; no qualifying official API docs.  No qualifying official API docs via web search."
    },
    "3-1 Pre-training": {
      "score": 0.5,
      "reason": "Paper gives extensive hyper-parameters, curriculum, LR schedule, and FP8 recipe, but not a fully reproducible end-to-end script; partial disclosure."
    },
    "3-2 Fine-tuning": {
      "score": 0.5,
      "reason": "Multi-stage SFT/GRPO/DPO pipeline is described, yet exact datasets, hyper-parameters and scripts are not fully released."
    },
    "3-3 Reinforcement Learning": {
      "score": 0.5,
      "reason": "Methods (GRPO, DPO, RLHF) and on-policy data generation are explained, but not to full reproducibility detail."
    },
    "4-1 Pre-training Data": {
      "score": 0.5,
      "reason": "Sources, proportions and several released subsets (Nemotron-CC-v2, CC-Math, Code) are described, but the full 20 T-token blend is not entirely published; partial openness."
    },
    "4-2 Fine-tuning Data": {
      "score": 0.5,
      "reason": "Token counts, language mix and release of Post-Training-Dataset-v2 are given, yet not all alignment datasets are fully released."
    },
    "4-3 Reinforcement Learning Data": {
      "score": 0.5,
      "reason": "RL preference dataset release (Nemotron-Post-Training-Dataset-v2) is mentioned, but only partial details and sizes are provided."
    },
    "4-4 Data Filtering": {
      "score": 1.0,
      "reason": "Paper describes concrete filtering pipeline: HTML-to-text, language ID, exact and MinHash deduplication, 5-bucket quality classifiers, heuristic & perplexity filters, FineMath classifier, license removal, etc.—meeting the rubric’s ‘Open’ criterion."
    }
  },
  "included_scores": {
    "1-2 Code": {
      "score": 0.0,
      "reason": "No training pipeline files; README mentions are ignored."
    },
    "1-3 License": {
      "score": 1.0,
      "reason": "Weights are under the NVIDIA Open Model License, which the rubric lists as an ‘Open’ license (all four freedoms with only minor restrictions)."
    },
    "1-4 Paper": {
      "score": 1.0,
      "reason": "Provider-authored technical report “NVIDIA Nemotron Nano 2 …” (arXiv 2508.14444) specifically documents this model and is publicly available."
    },
    "1-5 Architecture": {
      "score": 1.0,
      "reason": "Hugging Face presence detected; forced Open per FORCE_HF_CORE_OPEN."
    },
    "1-6 Tokenizer": {
      "score": 1.0,
      "reason": "Hugging Face presence detected; forced Open per FORCE_HF_CORE_OPEN."
    },
    "1-1 Weights": {
      "score": 1.0,
      "reason": "Hugging Face presence detected; forced Open per FORCE_HF_CORE_OPEN."
    },
    "2-1 Hardware": {
      "score": 1.0,
      "reason": "Quote gives both type and quantity: “A single training run takes about 40 hours using 1 024 NVIDIA H100 GPUs.”"
    },
    "2-2 Software": {
      "score": 0.5,
      "reason": "Quotes list core training stack components (Megatron-LM, Transformer Engine, FP8 recipe, tensor/data/context/sequence parallelism) but do not enumerate the *full* stack with versions for every library; hence partial disclosure."
    },
    "2-3 API": {
      "score": 0.0,
      "reason": "Only generic or non-official mentions; no qualifying official API docs.  No qualifying official API docs via web search."
    },
    "3-1 Pre-training": {
      "score": 0.5,
      "reason": "Paper gives extensive hyper-parameters, curriculum, LR schedule, and FP8 recipe, but not a fully reproducible end-to-end script; partial disclosure."
    },
    "3-2 Fine-tuning": {
      "score": 0.5,
      "reason": "Multi-stage SFT/GRPO/DPO pipeline is described, yet exact datasets, hyper-parameters and scripts are not fully released."
    },
    "3-3 Reinforcement Learning": {
      "score": 0.5,
      "reason": "Methods (GRPO, DPO, RLHF) and on-policy data generation are explained, but not to full reproducibility detail."
    },
    "4-1 Pre-training Data": {
      "score": 0.5,
      "reason": "Sources, proportions and several released subsets (Nemotron-CC-v2, CC-Math, Code) are described, but the full 20 T-token blend is not entirely published; partial openness."
    },
    "4-2 Fine-tuning Data": {
      "score": 0.5,
      "reason": "Token counts, language mix and release of Post-Training-Dataset-v2 are given, yet not all alignment datasets are fully released."
    },
    "4-3 Reinforcement Learning Data": {
      "score": 0.5,
      "reason": "RL preference dataset release (Nemotron-Post-Training-Dataset-v2) is mentioned, but only partial details and sizes are provided."
    },
    "4-4 Data Filtering": {
      "score": 1.0,
      "reason": "Paper describes concrete filtering pipeline: HTML-to-text, language ID, exact and MinHash deduplication, 5-bucket quality classifiers, heuristic & perplexity filters, FineMath classifier, license removal, etc.—meeting the rubric’s ‘Open’ criterion."
    }
  },
  "final_score_10pt": 6.562,
  "meta": {
    "usage_from_dispatch": {
      "fine_tuning": "used",
      "rl": "used"
    },
    "excluded": [],
    "denominator": 16,
    "raw_sum": 10.5,
    "scale": "10/16",
    "code_detection_reason": "No training pipeline files; README mentions are ignored.",
    "pretrain_sources_used": false
  }
}