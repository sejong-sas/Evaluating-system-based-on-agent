{
  "1-1 (Weights)": "The released artifacts for gpt-oss-120b are explicitly described as “open-weight,” meaning the full parameter files can be obtained by anyone. According to the authors, “We release the model weights … under an Apache 2.0 license to enable broad use and further research.”  The same statement lists both sizes—gpt-oss-120b and the smaller gpt-oss-20b—as jointly available.  Multiple sentences state that “The weights for both gpt-oss-120b and gpt-oss-20b are freely available for download on Hugging Face,” and that they are “natively quantized in MXFP4,” so users do not need to run a conversion step.  Practical hardware guidance is also given: the Hugging Face model card calls out “openai/gpt-oss-20b ~16 GB VRAM requirement when using MXFP4,” whereas “openai/gpt-oss-120b Requires ≥60 GB VRAM or multi-GPU setup.”  The distribution is integrated with popular deployment tools—e.g., “vLLM provides a serve command that will automatically download the model from HuggingFace and spin up an OpenAI-compatible server,” with the concrete command “vllm serve openai/gpt-oss-120b.”  For local usage through Ollama, the authors provide pull commands such as “ollama pull gpt-oss:120b.”  Altogether, the quotes confirm public, no-cost weight access, specific hosting (Hugging Face), the availability of ready-to-use quantized checkpoints, exact VRAM expectations for both sizes, and single-command download options via vLLM or Ollama.",
  "1-2 (Code)": "",
  "1-3 (License)": "Every licensing remark in the supplied material points to the permissive Apache 2.0 terms.  Two separate sentences state that the authors “release the model weights, inference implementations, tool environments, and tokenizers under an Apache 2.0 license.”  Another reiterates that the models are “available under the flexible Apache 2.0 license,” emphasising that the licence is intended to facilitate “broad use and further research” and “efficient deployment on consumer hardware.”  No restrictive phrases such as “non-commercial,” “research only,” or “no redistribution” appear; the Apache 2.0 label itself, together with the qualifiers “open-weight” and “flexible,” implies full rights to use, modify, and redistribute, including commercially, consistent with standard Apache 2.0 provisions.",
  "1-4 (Paper)": "A formal technical artefact exists in the form of a model card: two identical title lines read “Title: gpt-oss-120b & gpt-oss-20b Model Card.”  One citation gives more bibliographic detail: “View a PDF of the paper titled gpt-oss-120b & gpt-oss-20b Model Card, by OpenAI: Sandhini Agarwal and 123 other authors,” indicating a PDF is publicly hosted and that it has a large author list (124 authors total).  Supplementary instructional material is also referenced: a guide that “will walk you through how to use vLLM to set up gpt-oss-20b or gpt-oss-120b on a server,” and a parallel guide for “Ollama … to chat with it offline.”  These guides function as practical blog-style documentation showing how to serve or locally run the models and even connect them to the Agents SDK.  Collectively, the references confirm the presence of an official, citable model card PDF and multiple publicly available usage guides.",
  "1-1 (Weights)__evidence": [
    {
      "source": "[pdf_text]",
      "quote": "We present gpt-oss-120b and gpt-oss-20b, two open-weight reasoning models that push the frontier of accuracy and inference cost. We release the model weights, inference implementations, tool environments, and tokenizers under an Apache 2.0 license to enable broad use and further research."
    },
    {
      "source": "[url:https://openai.com/index/introducing-gpt-oss/]",
      "quote": "The weights for both gpt-oss-120b and gpt-oss-20b are freely available for download on Hugging Face and come natively quantized in MXFP4."
    },
    {
      "source": "[sections/https://arxiv.org/abs/2508.10925]",
      "quote": "We present gpt-oss-120b and gpt-oss-20b, two open-weight reasoning models that push the frontier of accuracy and inference cost. We release the model weights, inference implementations, tool environments, and tokenizers under an Apache 2.0 license to enable broad use and further research."
    },
    {
      "source": "[sections/https://r.jina.ai/https://openai.com/index/introducing-gpt-oss/]",
      "quote": "We’re releasing gpt-oss-120b and gpt-oss-20b—two state-of-the-art open-weight language models that deliver strong real-world performance at low cost."
    },
    {
      "source": "[pdf_text/how-to-run-gpt-oss-with-Transformers]",
      "quote": "Both gpt-oss models are available on Hugging Face: openai/gpt-oss-20b ~16GB VRAM requirement … openai/gpt-oss-120b Requires ≥60GB VRAM or multi-GPU setup"
    },
    {
      "source": "[pdf_text]",
      "quote": "The weights for both gpt-oss-120b and gpt-oss-20b are freely available for download on Hugging Face and come natively quantized in MXFP4."
    },
    {
      "source": "[sections/https://cookbook.openai.com/articles/gpt-oss/run-transformers]",
      "quote": "Both gpt-oss models are available on Hugging Face: openai/gpt-oss-20b ~16GB VRAM requirement when using MXFP4"
    },
    {
      "source": "[sections/https://cookbook.openai.com/articles/gpt-oss/run-vllm]",
      "quote": "vLLM provides a serve command that will automatically download the model from HuggingFace and spin up an OpenAI-compatible server on localhost:8000 . Run the following command depending on your desired model size in a terminal session on your server. # For 120B vllm serve openai/gpt-oss-120b"
    },
    {
      "source": "[sections/https://cookbook.openai.com/articles/gpt-oss/run-locally-ollama]",
      "quote": "Pull the model you want: # For 20B ollama pull gpt-oss:20b # For 120B ollama pull gpt-oss:120b"
    }
  ],
  "1-2 (Code)__evidence": [],
  "1-3 (License)__evidence": [
    {
      "source": "[pdf_text]",
      "quote": "We present gpt-oss-120b and gpt-oss-20b, two open-weight reasoning models that push the frontier of accuracy and inference cost. We release the model weights, inference implementations, tool environments, and tokenizers under an Apache 2.0 license to enable broad use and further research."
    },
    {
      "source": "[url:https://openai.com/index/introducing-gpt-oss/]",
      "quote": "We’re releasing gpt-oss-120b and gpt-oss-20b—two state-of-the-art open-weight language models that deliver strong real-world performance at low cost. Available under the flexible Apache 2.0 license, these models outperform similarly sized open models on reasoning tasks, demonstrate strong tool use capabilities, and are optimized for efficient deployment on consumer hardware."
    },
    {
      "source": "[sections/https://r.jina.ai/https://openai.com/index/introducing-gpt-oss/]",
      "quote": "Available under the flexible Apache 2.0 license, these models outperform similarly sized open models on reasoning tasks…"
    }
  ],
  "1-4 (Paper)__evidence": [
    {
      "source": "[pdf_text]",
      "quote": "Title: gpt-oss-120b & gpt-oss-20b Model Card"
    },
    {
      "source": "[pdf_text]",
      "quote": "View a PDF of the paper titled gpt-oss-120b & gpt-oss-20b Model Card, by OpenAI: Sandhini Agarwal and 123 other authors"
    },
    {
      "source": "[sections/https://arxiv.org/abs/2508.10925]",
      "quote": "Title: gpt-oss-120b & gpt-oss-20b Model Card"
    },
    {
      "source": "[sections/https://cookbook.openai.com/articles/gpt-oss/run-vllm]",
      "quote": "This guide will walk you through how to use vLLM to set up gpt-oss-20b or gpt-oss-120b on a server to serve gpt-oss as an API for your applications, and even connect it to the Agents SDK."
    },
    {
      "source": "[sections/https://cookbook.openai.com/articles/gpt-oss/run-locally-ollama]",
      "quote": "This guide will walk you through how to use Ollama to set up gpt-oss-20b or gpt-oss-120b locally, to chat with it offline, use it through an API, and even connect it to the Agents SDK."
    }
  ]
}