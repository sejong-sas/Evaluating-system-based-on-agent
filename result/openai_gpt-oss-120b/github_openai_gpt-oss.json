{
    "repo": "openai/gpt-oss",
    "branch": "main",
    "files": [
        ".github/CODEOWNERS",
        ".github/ISSUE_TEMPLATE/config.yml",
        ".github/workflows/CI.yml",
        ".gitignore",
        "CMakeLists.txt",
        "LICENSE",
        "MANIFEST.in",
        "README.md",
        "USAGE_POLICY",
        "_build/gpt_oss_build_backend/__init__.py",
        "_build/gpt_oss_build_backend/backend.py",
        "awesome-gpt-oss.md",
        "compatibility-test/.gitignore",
        "compatibility-test/README.md",
        "compatibility-test/analysis.ts",
        "compatibility-test/cases.jsonl",
        "compatibility-test/index.ts",
        "compatibility-test/package-lock.json",
        "compatibility-test/package.json",
        "compatibility-test/providers.ts",
        "compatibility-test/runCase.ts",
        "compatibility-test/tools.ts",
        "docs/gpt-oss-120b.svg",
        "docs/gpt-oss-20b.svg",
        "docs/gpt-oss.svg",
        "examples/agents-sdk-js/index.ts",
        "examples/agents-sdk-js/package-lock.json",
        "examples/agents-sdk-js/package.json",
        "examples/agents-sdk-python/example.py",
        "examples/agents-sdk-python/pyproject.toml",
        "examples/gradio/gradio_chat.py",
        "examples/streamlit/streamlit_chat.py",
        "gpt-oss-mcp-server/README.md",
        "gpt-oss-mcp-server/browser_server.py",
        "gpt-oss-mcp-server/build-system-prompt.py",
        "gpt-oss-mcp-server/pyproject.toml",
        "gpt-oss-mcp-server/python_server.py",
        "gpt-oss-mcp-server/reference-system-prompt.py",
        "gpt_oss/__init__.py",
        "gpt_oss/chat.py",
        "gpt_oss/evals/README.md",
        "gpt_oss/evals/__init__.py",
        "gpt_oss/evals/__main__.py",
        "gpt_oss/evals/abcd_grader.py",
        "gpt_oss/evals/aime_eval.py",
        "gpt_oss/evals/basic_eval.py",
        "gpt_oss/evals/chat_completions_sampler.py",
        "gpt_oss/evals/gpqa_eval.py",
        "gpt_oss/evals/healthbench_eval.py",
        "gpt_oss/evals/report.py",
        "gpt_oss/evals/responses_sampler.py",
        "gpt_oss/evals/types.py",
        "gpt_oss/generate.py",
        "gpt_oss/metal/CMakeLists.txt",
        "gpt_oss/metal/__init__.py",
        "gpt_oss/metal/benchmark/end-to-end-threadgroup.cc",
        "gpt_oss/metal/benchmark/end-to-end.cc",
        "gpt_oss/metal/benchmark/f32-bf16w-rmsnorm.cc",
        "gpt_oss/metal/benchmark/f32-random.cc",
        "gpt_oss/metal/benchmark/mf4-f32-convert.cc",
        "gpt_oss/metal/benchmark/u32-random.cc",
        "gpt_oss/metal/examples/chat.py",
        "gpt_oss/metal/examples/generate.py",
        "gpt_oss/metal/include/gpt-oss.h",
        "gpt_oss/metal/include/gpt-oss/functions.h",
        "gpt_oss/metal/include/gpt-oss/macros.h",
        "gpt_oss/metal/include/gpt-oss/types.h",
        "gpt_oss/metal/python/context.c",
        "gpt_oss/metal/python/model.c",
        "gpt_oss/metal/python/module.c",
        "gpt_oss/metal/python/module.h",
        "gpt_oss/metal/python/tokenizer.c",
        "gpt_oss/metal/scripts/create-local-model.py",
        "gpt_oss/metal/source/accumulate.metal",
        "gpt_oss/metal/source/context.c",
        "gpt_oss/metal/source/convert.metal",
        "gpt_oss/metal/source/embeddings.metal",
        "gpt_oss/metal/source/generate.c",
        "gpt_oss/metal/source/include/internal/datatype.h",
        "gpt_oss/metal/source/include/internal/datatype.hpp",
        "gpt_oss/metal/source/include/internal/kernel-args.h",
        "gpt_oss/metal/source/include/internal/log.h",
        "gpt_oss/metal/source/include/internal/macros.h",
        "gpt_oss/metal/source/include/internal/math.h",
        "gpt_oss/metal/source/include/internal/metal-kernels.h",
        "gpt_oss/metal/source/include/internal/metal.h",
        "gpt_oss/metal/source/include/internal/metal.hpp",
        "gpt_oss/metal/source/include/internal/model.h",
        "gpt_oss/metal/source/include/internal/rng.h",
        "gpt_oss/metal/source/include/internal/rng.hpp",
        "gpt_oss/metal/source/include/internal/storage.h",
        "gpt_oss/metal/source/include/internal/uuid.h",
        "gpt_oss/metal/source/log.c",
        "gpt_oss/metal/source/matmul.metal",
        "gpt_oss/metal/source/metal-kernels.c",
        "gpt_oss/metal/source/metal.m",
        "gpt_oss/metal/source/model.c",
        "gpt_oss/metal/source/moematmul.metal",
        "gpt_oss/metal/source/random.metal",
        "gpt_oss/metal/source/rmsnorm.metal",
        "gpt_oss/metal/source/rope.metal",
        "gpt_oss/metal/source/sample.metal",
        "gpt_oss/metal/source/sdpa.metal",
        "gpt_oss/metal/source/tokenizer.c",
        "gpt_oss/metal/source/topk.metal",
        "gpt_oss/metal/test/bf16-f32-embeddings.cc",
        "gpt_oss/metal/test/embeddings-kernel-tester.hpp",
        "gpt_oss/metal/test/f32-bf16w-matmul.cc",
        "gpt_oss/metal/test/f32-bf16w-rmsnorm.cc",
        "gpt_oss/metal/test/f32-random.cc",
        "gpt_oss/metal/test/f32-rope.cc",
        "gpt_oss/metal/test/fill-random-kernel-tester.hpp",
        "gpt_oss/metal/test/matmul-kernel-tester.hpp",
        "gpt_oss/metal/test/mf4-f32-convert.cc",
        "gpt_oss/metal/test/rmsnorm-kernel-tester.hpp",
        "gpt_oss/metal/test/rope-kernel-tester.hpp",
        "gpt_oss/metal/test/u32-random.cc",
        "gpt_oss/responses_api/__init__.py",
        "gpt_oss/responses_api/api_server.py",
        "gpt_oss/responses_api/events.py",
        "gpt_oss/responses_api/inference/__init__.py",
        "gpt_oss/responses_api/inference/metal.py",
        "gpt_oss/responses_api/inference/ollama.py",
        "gpt_oss/responses_api/inference/stub.py",
        "gpt_oss/responses_api/inference/transformers.py",
        "gpt_oss/responses_api/inference/triton.py",
        "gpt_oss/responses_api/inference/vllm.py",
        "gpt_oss/responses_api/serve.py",
        "gpt_oss/responses_api/types.py",
        "gpt_oss/responses_api/utils.py",
        "gpt_oss/tokenizer.py",
        "gpt_oss/tools/__init__.py",
        "gpt_oss/tools/apply_patch.md",
        "gpt_oss/tools/apply_patch.py",
        "gpt_oss/tools/python_docker/docker_tool.py",
        "gpt_oss/tools/simple_browser/__init__.py",
        "gpt_oss/tools/simple_browser/backend.py",
        "gpt_oss/tools/simple_browser/page_contents.py",
        "gpt_oss/tools/simple_browser/simple_browser_tool.py",
        "gpt_oss/tools/tool.py",
        "gpt_oss/torch/__init__.py",
        "gpt_oss/torch/model.py",
        "gpt_oss/torch/utils.py",
        "gpt_oss/torch/weights.py",
        "gpt_oss/triton/__init__.py",
        "gpt_oss/triton/attention.py",
        "gpt_oss/triton/model.py",
        "gpt_oss/triton/moe.py",
        "gpt_oss/vllm/token_generator.py",
        "pyproject.toml",
        "tests-data/basic-event-stream.txt",
        "tests-data/web-search-event-stream.txt",
        "tests/conftest.py",
        "tests/gpt_oss/tools/simple_browser/test_backend.py",
        "tests/test_api_endpoints.py",
        "tests/test_responses_api.py"
    ],
    "license_files": {
        "LICENSE": "\n                                 Apache License\n                           Version 2.0, January 2004\n                        http://www.apache.org/licenses/\n\n   TERMS AND CONDITIONS FOR USE, REPRODUCTION, AND DISTRIBUTION\n\n   1. Definitions.\n\n      \"License\" shall mean the terms and conditions for use, reproduction,\n      and distribution as defined by Sections 1 through 9 of this document.\n\n      \"Licensor\" shall mean the copyright owner or entity authorized by\n      the copyright owner that is granting the License.\n\n      \"Legal Entity\" shall mean the union of the acting entity and all\n      other entities that control, are controlled by, or are under common\n      control with that entity. For the purposes of this definition,\n      \"control\" means (i) the power, direct or indirect, to cause the\n      direction or management of such entity, whether by contract or\n      otherwise, or (ii) ownership of fifty percent (50%) or more of the\n      outstanding shares, or (iii) beneficial ownership of such entity.\n\n      \"You\" (or \"Your\") shall mean an individual or Legal Entity\n      exercising permissions granted by this License.\n\n      \"Source\" form shall mean the preferred form for making modifications,\n      including but not limited to software source code, documentation\n      source, and configuration files.\n\n      \"Object\" form shall mean any form resulting from mechanical\n      transformation or translation of a Source form, including but\n      not limited to compiled object code, generated documentation,\n      and conversions to other media types.\n\n      \"Work\" shall mean the work of authorship, whether in Source or\n      Object form, made available under the License, as indicated by a\n      copyright notice that is included in or attached to the work\n      (an example is provided in the Appendix below).\n\n      \"Derivative Works\" shall mean any work, whether in Source or Object\n      form, that is based on (or derived from) the Work and for which the\n      editorial revisions, annotations, elaborations, or other modifications\n      represent, as a whole, an original work of authorship. For the purposes\n      of this License, Derivative Works shall not include works that remain\n      separable from, or merely link (or bind by name) to the interfaces of,\n      the Work and Derivative Works thereof.\n\n      \"Contribution\" shall mean any work of authorship, including\n      the original version of the Work and any modifications or additions\n      to that Work or Derivative Works thereof, that is intentionally\n      submitted to Licensor for inclusion in the Work by the copyright owner\n      or by an individual or Legal Entity authorized to submit on behalf of\n      the copyright owner. For the purposes of this definition, \"submitted\"\n      means any form of electronic, verbal, or written communication sent\n      to the Licensor or its representatives, including but not limited to\n      communication on electronic mailing lists, source code control systems,\n      and issue tracking systems that are managed by, or on behalf of, the\n      Licensor for the purpose of discussing and improving the Work, but\n      excluding communication that is conspicuously marked or otherwise\n      designated in writing by the copyright owner as \"Not a Contribution.\"\n\n      \"Contributor\" shall mean Licensor and any individual or Legal Entity\n      on behalf of whom a Contribution has been received by Licensor and\n      subsequently incorporated within the Work.\n\n   2. Grant of Copyright License. Subject to the terms and conditions of\n      this License, each Contributor hereby grants to You a perpetual,\n      worldwide, non-exclusive, no-charge, royalty-free, irrevocable\n      copyright license to reproduce, prepare Derivative Works of,\n      publicly display, publicly perform, sublicense, and distribute the\n      Work and such Derivative Works in Source or Object form.\n\n   3. Grant of Patent License. Subject to the terms and conditions of\n      this License, each Contributor hereby grants to You a perpetual,\n      worldwide, non-exclusive, no-charge, royalty-free, irrevocable\n      (except as stated in this section) patent license to make, have made,\n      use, offer to sell, sell, import, and otherwise transfer the Work,\n      where such license applies only to those patent claims licensable\n      by such Contributor that are necessarily infringed by their\n      Contribution(s) alone or by combination of their Contribution(s)\n      with the Work to which such Contribution(s) was submitted. If You\n      institute patent litigation against any entity (including a\n      cross-claim or counterclaim in a lawsuit) alleging that the Work\n      or a Contribution incorporated within the Work constitutes direct\n      or contributory patent infringement, then any patent licenses\n      granted to You under this License for that Work shall terminate\n      as of the date such litigation is filed.\n\n   4. Redistribution. You may reproduce and distribute copies of the\n      Work or Derivative Works thereof in any medium, with or without\n      modifications, and in Source or Object form, provided that You\n      meet the following conditions:\n\n      (a) You must give any other recipients of the Work or\n          Derivative Works a copy of this License; and\n\n      (b) You must cause any modified files to carry prominent notices\n          stating that You changed the files; and\n\n      (c) You must retain, in the Source form of any Derivative Works\n          that You distribute, all copyright, patent, trademark, and\n          attribution notices from the Source form of the Work,\n          excluding those notices that do not pertain to any part of\n          the Derivative Works; and\n\n      (d) If the Work includes a \"NOTICE\" text file as part of its\n          distribution, then any Derivative Works that You distribute must\n          include a readable copy of the attribution notices contained\n          within such NOTICE file, excluding those notices that do not\n          pertain to any part of the Derivative Works, in at least one\n          of the following places: within a NOTICE text file distributed\n          as part of the Derivative Works; within the Source form or\n          documentation, if provided along with the Derivative Works; or,\n          within a display generated by the Derivative Works, if and\n          wherever such third-party notices normally appear. The contents\n          of the NOTICE file are for informational purposes only and\n          do not modify the License. You may add Your own attribution\n          notices within Derivative Works that You distribute, alongside\n          or as an addendum to the NOTICE text from the Work, provided\n          that such additional attribution notices cannot be construed\n          as modifying the License.\n\n      You may add Your own copyright statement to Your modifications and\n      may provide additional or different license terms and conditions\n      for use, reproduction, or distribution of Your modifications, or\n      for any such Derivative Works as a whole, provided Your use,\n      reproduction, and distribution of the Work otherwise complies with\n      the conditions stated in this License.\n\n   5. Submission of Contributions. Unless You explicitly state otherwise,\n      any Contribution intentionally submitted for inclusion in the Work\n      by You to the Licensor shall be under the terms and conditions of\n      this License, without any additional terms or conditions.\n      Notwithstanding the above, nothing herein shall supersede or modify\n      the terms of any separate license agreement you may have executed\n      with Licensor regarding such Contributions.\n\n   6. Trademarks. This License does not grant permission to use the trade\n      names, trademarks, service marks, or product names of the Licensor,\n      except as required for reasonable and customary use in describing the\n      origin of the Work and reproducing the content of the NOTICE file.\n\n   7. Disclaimer of Warranty. Unless required by applicable law or\n      agreed to in writing, Licensor provides the Work (and each\n      Contributor provides its Contributions) on an \"AS IS\" BASIS,\n      WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or\n      implied, including, without limitation, any warranties or conditions\n      of TITLE, NON-INFRINGEMENT, MERCHANTABILITY, or FITNESS FOR A\n      PARTICULAR PURPOSE. You are solely responsible for determining the\n      appropriateness of using or redistributing the Work and assume any\n      risks associated with Your exercise of permissions under this License.\n\n   8. Limitation of Liability. In no event and under no legal theory,\n      whether in tort (including negligence), contract, or otherwise,\n      unless required by applicable law (such as deliberate and grossly\n      negligent acts) or agreed to in writing, shall any Contributor be\n      liable to You for damages, including any direct, indirect, special,\n      incidental, or consequential damages of any character arising as a\n      result of this License or out of the use or inability to use the\n      Work (including but not limited to damages for loss of goodwill,\n      work stoppage, computer failure or malfunction, or any and all\n      other commercial damages or losses), even if such Contributor\n      has been advised of the possibility of such damages.\n\n   9. Accepting Warranty or Additional Liability. While redistributing\n      the Work or Derivative Works thereof, You may choose to offer,\n      and charge a fee for, acceptance of support, warranty, indemnity,\n      or other liability obligations and/or rights consistent with this\n      License. However, in accepting such obligations, You may act only\n      on Your own behalf and on Your sole responsibility, not on behalf\n      of any other Contributor, and only if You agree to indemnify,\n      defend, and hold each Contributor harmless for any liability\n      incurred by, or claims asserted against, such Contributor by reason\n      of your accepting any such warranty or additional liability.\n\n   END OF TERMS AND CONDITIONS\n\n   APPENDIX: How to apply the Apache License to your work.\n\n      To apply the Apache License to your work, attach the following\n      boilerplate notice, with the fields enclosed by brackets \"[]\"\n      replaced with your own identifying information. (Don't include\n      the brackets!)  The text should be enclosed in the appropriate\n      comment syntax for the file format. We also recommend that a\n      file or class name and description of purpose be included on the\n      same \"printed page\" as the copyright notice for easier\n      identification within third-party archives.\n\n   Copyright [yyyy] [name of copyright owner]\n\n   Licensed under the Apache License, Version 2.0 (the \"License\");\n   you may not use this file except in compliance with the License.\n   You may obtain a copy of the License at\n\n       http://www.apache.org/licenses/LICENSE-2.0\n\n   Unless required by applicable law or agreed to in writing, software\n   distributed under the License is distributed on an \"AS IS\" BASIS,\n   WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n   See the License for the specific language governing permissions and\n   limitations under the License.\n"
    },
    "readme": "<img alt=\"gpt-oss-120\" src=\"./docs/gpt-oss.svg\">\n<p align=\"center\">\n  <a href=\"https://gpt-oss.com\"><strong>Try gpt-oss</strong></a> ·\n  <a href=\"https://cookbook.openai.com/topic/gpt-oss\"><strong>Guides</strong></a> ·\n  <a href=\"https://arxiv.org/abs/2508.10925\"><strong>Model card</strong></a> ·\n  <a href=\"https://openai.com/index/introducing-gpt-oss/\"><strong>OpenAI blog</strong></a>\n</p>\n<p align=\"center\">\n  <strong>Download <a href=\"https://huggingface.co/openai/gpt-oss-120b\">gpt-oss-120b</a> and <a href=\"https://huggingface.co/openai/gpt-oss-20b\">gpt-oss-20b</a> on Hugging Face</strong>\n</p>\n\n<br>\n\nWelcome to the gpt-oss series, [OpenAI's open-weight models](https://openai.com/open-models/) designed for powerful reasoning, agentic tasks, and versatile developer use cases.\n\nWe're releasing two flavors of these open models:\n\n- `gpt-oss-120b` — for production, general purpose, high reasoning use cases that fit into a single 80GB GPU (like NVIDIA H100 or AMD MI300X) (117B parameters with 5.1B active parameters)\n- `gpt-oss-20b` — for lower latency, and local or specialized use cases (21B parameters with 3.6B active parameters)\n\nBoth models were trained using our [harmony response format][harmony] and should only be used with this format; otherwise, they will not work correctly.\n\n## Table of Contents\n- [Highlights](#highlights)\n- [Inference examples](#inference-examples)\n- [About this repository](#about-this-repository)\n- [Setup](#setup)\n- [Download the model](#download-the-model)\n- [Reference PyTorch implementation](#reference-pytorch-implementation)\n- [Reference Triton implementation (single GPU)](#reference-triton-implementation-single-gpu)\n- [Reference Metal implementation](#reference-metal-implementation)\n- [Harmony format & tools](#harmony-format--tools)\n- [Clients](#clients)\n- [Tools](#tools)\n- [Other details](#other-details)\n- [Contributing](#contributing)\n\n### Highlights\n\n- **Permissive Apache 2.0 license:** Build freely without copyleft restrictions or patent risk—ideal for experimentation, customization, and commercial deployment.\n- **Configurable reasoning effort:** Easily adjust the reasoning effort (low, medium, high) based on your specific use case and latency needs.\n- **Full chain-of-thought:** Provides complete access to the model's reasoning process, facilitating easier debugging and greater trust in outputs. This information is not intended to be shown to end users.\n- **Fine-tunable:** Fully customize models to your specific use case through parameter fine-tuning.\n- **Agentic capabilities:** Use the models' native capabilities for function calling, [web browsing](#browser), [Python code execution](#python), and Structured Outputs.\n- **MXFP4 quantization:** The models were post-trained with MXFP4 quantization of the MoE weights, making `gpt-oss-120b` run on a single 80GB GPU (like NVIDIA H100 or AMD MI300X) and the `gpt-oss-20b` model run within 16GB of memory. All evals were performed with the same MXFP4 quantization.\n\n### Inference examples\n\n#### Transformers\n\nYou can use `gpt-oss-120b` and `gpt-oss-20b` with the Transformers library. If you use Transformers' chat template, it will automatically apply the [harmony response format][harmony]. If you use `model.generate` directly, you need to apply the harmony format manually using the chat template or use our [`openai-harmony`][harmony] package.\n\n```python\nfrom transformers import pipeline\nimport torch\n\nmodel_id = \"openai/gpt-oss-120b\"\n\npipe = pipeline(\n    \"text-generation\",\n    model=model_id,\n    torch_dtype=\"auto\",\n    device_map=\"auto\",\n)\n\nmessages = [\n    {\"role\": \"user\", \"content\": \"Explain quantum mechanics clearly and concisely.\"},\n]\n\noutputs = pipe(\n    messages,\n    max_new_tokens=256,\n)\nprint(outputs[0][\"generated_text\"][-1])\n```\n\n[Learn more about how to use gpt-oss with Transformers.](https://cookbook.openai.com/articles/gpt-oss/run-transformers)\n\n#### vLLM\n\nvLLM recommends using [`uv`](https://docs.astral.sh/uv/) for Python dependency management. You can use vLLM to spin up an OpenAI-compatible web server. The following command will automatically download the model and start the server.\n\n```bash\nuv pip install --pre vllm==0.10.1+gptoss \\\n    --extra-index-url https://wheels.vllm.ai/gpt-oss/ \\\n    --extra-index-url https://download.pytorch.org/whl/nightly/cu128 \\\n    --index-strategy unsafe-best-match\n\nvllm serve openai/gpt-oss-20b\n```\n\n[Learn more about how to use gpt-oss with vLLM.](https://cookbook.openai.com/articles/gpt-oss/run-vllm)\n\nOffline Serve Code:\n- run this code after installing proper libraries as described, while additionally installing this:\n- `uv pip install openai-harmony`\n```python\n# source .oss/bin/activate\n\nimport os\nos.environ[\"VLLM_USE_FLASHINFER_SAMPLER\"] = \"0\"\n\nimport json\nfrom openai_harmony import (\n    HarmonyEncodingName,\n    load_harmony_encoding,\n    Conversation,\n    Message,\n    Role,\n    SystemContent,\n    DeveloperContent,\n)\n \nfrom vllm import LLM, SamplingParams\nimport os\n\n# --- 1) Render the prefill with Harmony ---\nencoding = load_harmony_encoding(HarmonyEncodingName.HARMONY_GPT_OSS)\n \nconvo = Conversation.from_messages(\n    [\n        Message.from_role_and_content(Role.SYSTEM, SystemContent.new()),\n        Message.from_role_and_content(\n            Role.DEVELOPER,\n            DeveloperContent.new().with_instructions(\"Always respond in riddles\"),\n        ),\n        Message.from_role_and_content(Role.USER, \"What is the weather like in SF?\"),\n    ]\n)\n \nprefill_ids = encoding.render_conversation_for_completion(convo, Role.ASSISTANT)\n \n# Harmony stop tokens (pass to sampler so they won't be included in output)\nstop_token_ids = encoding.stop_tokens_for_assistant_actions()\n \n# --- 2) Run vLLM with prefill ---\nllm = LLM(\n    model=\"openai/gpt-oss-20b\",\n    trust_remote_code=True,\n    gpu_memory_utilization = 0.95,\n    max_num_batched_tokens=4096,\n    max_model_len=5000,\n    tensor_parallel_size=1\n)\n \nsampling = SamplingParams(\n    max_tokens=128,\n    temperature=1,\n    stop_token_ids=stop_token_ids,\n)\n \noutputs = llm.generate(\n    prompt_token_ids=[prefill_ids],   # batch of size 1\n    sampling_params=sampling,\n)\n \n# vLLM gives you both text and token IDs\ngen = outputs[0].outputs[0]\ntext = gen.text\noutput_tokens = gen.token_ids  # <-- these are the completion token IDs (no prefill)\n \n# --- 3) Parse the completion token IDs back into structured Harmony messages ---\nentries = encoding.parse_messages_from_completion_tokens(output_tokens, Role.ASSISTANT)\n \n# 'entries' is a sequence of structured conversation entries (assistant messages, tool calls, etc.).\nfor message in entries:\n    print(f\"{json.dumps(message.to_dict())}\")\n```\n\n#### PyTorch / Triton / Metal\n\nThese implementations are largely reference implementations for educational purposes and are not expected to be run in production.\n\n[Learn more below.](#reference-pytorch-implementation)\n\n#### Ollama\n\nIf you are trying to run `gpt-oss` on consumer hardware, you can use Ollama by running the following commands after [installing Ollama](https://ollama.com/download).\n\n```bash\n# gpt-oss-20b\nollama pull gpt-oss:20b\nollama run gpt-oss:20b\n\n# gpt-oss-120b\nollama pull gpt-oss:120b\nollama run gpt-oss:120b\n```\n\n[Learn more about how to use gpt-oss with Ollama.](https://cookbook.openai.com/articles/gpt-oss/run-locally-ollama)\n\n#### LM Studio\n\nIf you are using [LM Studio](https://lmstudio.ai/) you can use the following commands to download.\n\n```bash\n# gpt-oss-20b\nlms get openai/gpt-oss-20b\n# gpt-oss-120b\nlms get openai/gpt-oss-120b\n```\n\nCheck out our [awesome list](./awesome-gpt-oss.md) for a broader collection of gpt-oss resources and inference partners.\n\n## About this repository\n\nThis repository provides a collection of reference implementations:\n\n- **Inference:**\n  - [`torch`](#reference-pytorch-implementation) — a non-optimized [PyTorch](https://pytorch.org/) implementation for educational purposes only. Requires at least 4× H100 GPUs due to lack of optimization.\n  - [`triton`](#reference-triton-implementation-single-gpu) — a more optimized implementation using [PyTorch](https://pytorch.org/) & [Triton](https://github.com/triton-lang/triton) incl. using CUDA graphs and basic caching\n  - [`metal`](#reference-metal-implementation) — a Metal-specific implementation for running the models on Apple Silicon hardware\n- **Tools:**\n  - [`browser`](#browser) — a reference implementation of the browser tool the models got trained on\n  - [`python`](#python) — a stateless reference implementation of the python tool the model got trained on\n- **Client examples:**\n  - [`chat`](#terminal-chat) — a basic terminal chat application that uses the PyTorch or Triton implementations for inference along with the python and browser tools\n  - [`responses_api`](#responses-api) — an example Responses API compatible server that implements the browser tool along with other Responses-compatible functionality\n\n## Setup\n\n### Requirements\n\n- Python 3.12\n- On macOS: Install the Xcode CLI tools --> `xcode-select --install`\n- On Linux: These reference implementations require CUDA\n- On Windows: These reference implementations have not been tested on Windows. Try using solutions like Ollama if you are trying to run the model locally.\n\n### Installation\n\nIf you want to try any of the code you can install it directly from [PyPI](https://pypi.org/project/gpt-oss/)\n\n```shell\n# if you just need the tools\npip install gpt-oss\n# if you want to try the torch implementation\npip install gpt-oss[torch]\n# if you want to try the triton implementation\npip install gpt-oss[triton]\n```\n\nIf you want to modify the code or try the metal implementation set the project up locally:\n\n```shell\ngit clone https://github.com/openai/gpt-oss.git\nGPTOSS_BUILD_METAL=1 pip install -e \".[metal]\"\n```\n\n## Download the model\n\nYou can download the model weights from the [Hugging Face Hub](https://huggingface.co/collections/openai/gpt-oss-68911959590a1634ba11c7a4) directly from Hugging Face CLI:\n\n```shell\n# gpt-oss-120b\nhf download openai/gpt-oss-120b --include \"original/*\" --local-dir gpt-oss-120b/\n\n# gpt-oss-20b\nhf download openai/gpt-oss-20b --include \"original/*\" --local-dir gpt-oss-20b/\n```\n\n## Reference PyTorch implementation\n\nWe include an inefficient reference PyTorch implementation in [gpt_oss/torch/model.py](gpt_oss/torch/model.py). This code uses basic PyTorch operators to show the exact model architecture, with a small addition of supporting tensor parallelism in MoE so that the larger model can run with this code (e.g., on 4xH100 or 2xH200). In this implementation, we upcast all weights to BF16 and run the model in BF16.\n\nTo run the reference implementation, install the dependencies:\n\n```shell\npip install -e \".[torch]\"\n```\n\nAnd then run:\n\n```shell\n# On 4xH100:\ntorchrun --nproc-per-node=4 -m gpt_oss.generate gpt-oss-120b/original/\n```\n\n## Reference Triton implementation (single GPU)\n\nWe also include an optimized reference implementation that uses [an optimized triton MoE kernel](https://github.com/triton-lang/triton/tree/main/python/triton_kernels/triton_kernels) that supports MXFP4. It also has some optimization on the attention code to reduce the memory cost. To run this implementation, the nightly version of triton and torch will be installed. This version can be run on a single 80GB GPU for `gpt-oss-120b`.\n\nTo install the reference Triton implementation run\n\n```shell\n# You need to install triton from source to use the triton implementation\ngit clone https://github.com/triton-lang/triton\ncd triton/\npip install -r python/requirements.txt\npip install -e . --verbose --no-build-isolation\npip install -e python/triton_kernels\n\n# Install the gpt-oss triton implementation\npip install -e \".[triton]\"\n```\n\nAnd then run:\n\n```shell\n# On 1xH100\nexport PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True\npython -m gpt_oss.generate --backend triton gpt-oss-120b/original/\n```\n\nIf you encounter `torch.OutOfMemoryError`, make sure to turn on the expandable allocator to avoid crashes when loading weights from the checkpoint.\n\n## Reference Metal implementation\n\nAdditionally we are providing a reference implementation for Metal to run on Apple Silicon. This implementation is not production-ready but is accurate to the PyTorch implementation.\n\nThe implementation will get automatically compiled when running the `.[metal]` installation on an Apple Silicon device:\n\n```shell\nGPTOSS_BUILD_METAL=1 pip install -e \".[metal]\"\n```\n\nTo perform inference you'll need to first convert the SafeTensor weights from Hugging Face into the right format using:\n\n```shell\npython gpt_oss/metal/scripts/create-local-model.py -s <model_dir> -d <output_file>\n```\n\nOr download the pre-converted weights:\n\n```shell\nhf download openai/gpt-oss-120b --include \"metal/*\" --local-dir gpt-oss-120b/metal/\nhf download openai/gpt-oss-20b --include \"metal/*\" --local-dir gpt-oss-20b/metal/\n```\n\nTo test it you can run:\n\n```shell\npython gpt_oss/metal/examples/generate.py gpt-oss-20b/metal/model.bin -p \"why did the chicken cross the road?\"\n```\n\n## Harmony format & tools\n\nAlong with the model, we are also releasing a new chat format library `harmony` to interact with the model. Check [this guide](https://cookbook.openai.com/articles/openai-harmony) for more info about harmony.\n\nWe also include two system tools for the model: browsing and python container. Check [gpt_oss/tools](gpt_oss/tools) for the tool implementation.\n\n## Clients\n\n### Terminal Chat\n\nThe terminal chat application is a basic example of how to use the harmony format together with the PyTorch, Triton, and vLLM implementations. It also exposes both the python and browser tool as optional tools that can be used.\n\n```bash\nusage: python -m gpt_oss.chat [-h] [-r REASONING_EFFORT] [-a] [-b] [--show-browser-results] [-p] [--developer-message DEVELOPER_MESSAGE] [-c CONTEXT] [--raw] [--backend {triton,torch,vllm}] FILE\n\nChat example\n\npositional arguments:\n  FILE                  Path to the SafeTensors checkpoint\n\noptions:\n  -h, --help            show this help message and exit\n  -r REASONING_EFFORT, --reasoning-effort REASONING_EFFORT\n                        Reasoning effort (default: low)\n  -a, --apply-patch     Make apply_patch tool available to the model (default: False)\n  -b, --browser         Use browser tool (default: False)\n  --show-browser-results\n                        Show browser results (default: False)\n  -p, --python          Use python tool (default: False)\n  --developer-message DEVELOPER_MESSAGE\n                        Developer message (default: )\n  -c CONTEXT, --context CONTEXT\n                        Max context length (default: 8192)\n  --raw                 Raw mode (does not render Harmony encoding) (default: False)\n  --backend {triton,torch,vllm}\n                        Inference backend (default: triton)\n```\n\n> [!NOTE]\n> The torch and triton implementations require original checkpoint under `gpt-oss-120b/original/` and `gpt-oss-20b/original/` respectively. While vLLM uses the Hugging Face converted checkpoint under `gpt-oss-120b/` and `gpt-oss-20b/` root directory respectively.\n\n### Responses API\n\nWe also include an example Responses API server. This server does not implement every feature and event of the Responses API but should be compatible with most of the basic use cases and serve as inspiration for anyone building their own server. Some of our inference partners are also offering their own Responses API.\n\nYou can start this server with the following inference backends:\n\n- `triton` — uses the triton implementation\n- `metal` — uses the metal implementation on Apple Silicon only\n- `ollama` — uses the Ollama /api/generate API as an inference solution\n- `vllm` — uses your installed vllm version to perform inference\n- `transformers` — uses your installed transformers version to perform local inference\n\n```bash\nusage: python -m gpt_oss.responses_api.serve [-h] [--checkpoint FILE] [--port PORT] [--inference-backend BACKEND]\n\nResponses API server\n\noptions:\n  -h, --help                    show this help message and exit\n  --checkpoint FILE             Path to the SafeTensors checkpoint\n  --port PORT                   Port to run the server on\n  --inference-backend BACKEND   Inference backend to use\n```\n\n### Codex\n\nWe support [codex](https://github.com/openai/codex) as a client for gpt-oss. To run the 20b version, set this to `~/.codex/config.toml`:\n\n```\ndisable_response_storage = true\nshow_reasoning_content = true\n\n[model_providers.local]\nname = \"local\"\nbase_url = \"http://localhost:11434/v1\"\n\n[profiles.oss]\nmodel = \"gpt-oss:20b\"\nmodel_provider = \"local\"\n```\n\nThis will work with any chat completions-API compatible server listening on port 11434, like ollama. Start the server and point codex to the oss model:\n\n```\nollama run gpt-oss:20b\ncodex -p oss\n```\n\n## Tools\n\n### Browser\n\n> [!WARNING]\n> This implementation is purely for educational purposes and should not be used in production. You should implement your own equivalent of the [`YouComBackend`](gpt_oss/tools/simple_browser/backend.py) class with your own browsing environment. Currently we have available `YouComBackend` and `ExaBackend`. \n\nBoth gpt-oss models were trained with the capability to browse using the `browser` tool that exposes the following three methods:\n\n- `search` to search for key phrases\n- `open` to open a particular page\n- `find` to look for contents on a page\n\n#### Usage\n\nTo enable the browser tool, you'll have to place the definition into the `system` message of your harmony formatted prompt. You can either use the `with_browser_tool()` method if your tool implements the full interface or modify the definition using `with_tools()`. For example:\n\n```python\nimport datetime\nfrom gpt_oss.tools.simple_browser import SimpleBrowserTool\nfrom gpt_oss.tools.simple_browser.backend import YouComBackend\nfrom openai_harmony import SystemContent, Message, Conversation, Role, load_harmony_encoding, HarmonyEncodingName\n\nencoding = load_harmony_encoding(HarmonyEncodingName.HARMONY_GPT_OSS)\n\n# Depending on the choice of the browser backend you need corresponding env variables setup\n# In case you use You.com backend requires you to have set the YDC_API_KEY environment variable,\n# while for Exa you might need EXA_API_KEY environment variable set\nbackend = YouComBackend(\n    source=\"web\",\n)\n# backend = ExaBackend(\n#  source=\"web\",\n# )\nbrowser_tool = SimpleBrowserTool(backend=backend)\n\n# create a basic system prompt\nsystem_message_content = SystemContent.new().with_conversation_start_date(\n    datetime.datetime.now().strftime(\"%Y-%m-%d\")\n)\n\n# if you want to use the browser tool\nif use_browser_tool:\n    # enables the tool\n    system_message_content = system_message_content.with_tools(browser_tool.tool_config)\n    # alternatively you could use the following if your tool is not stateless\n    system_message_content = system_message_content.with_browser_tool()\n\n# construct the system message\nsystem_message = Message.from_role_and_content(Role.SYSTEM, system_message_content)\n\n# create the overall prompt\nmessages = [system_message, Message.from_role_and_content(Role.USER, \"What's the weather in SF?\")]\nconversation = Conversation.from_messages(messages)\n\n# convert to tokens\ntoken_ids = encoding.render_conversation_for_completion(conversation, Role.ASSISTANT)\n\n# perform inference\n# ...\n\n# parse the output\nmessages = encoding.parse_messages_from_completion_tokens(output_tokens, Role.ASSISTANT)\nlast_message = messages[-1]\nif last_message.recipient.startswith(\"browser\"):\n  # perform browser call\n  response_messages = await browser_tool.process(last_message)\n\n  # extend the current messages and run inference again\n  messages.extend(response_messages)\n```\n\n#### Details\n\nTo control the context window size this tool uses a scrollable window of text that the model can interact with. So it might fetch the first 50 lines of a page and then scroll to the next 20 lines after that. The model has also been trained to then use citations from this tool in its answers.\n\nTo improve performance the tool caches requests so that the model can revisit a different part of a page without having to reload the page. For that reason you should create a new browser instance for every request.\n\n### Python\n\nThe model was trained to use a python tool to perform calculations and other actions as part of its chain-of-thought. During the training the model used a stateful tool which makes running tools between CoT loops easier. This reference implementation, however, uses a stateless mode. As a result the PythonTool defines its own tool description to override the definition in [`openai-harmony`][harmony].\n\n> [!WARNING]\n> This implementation runs in a permissive Docker container which could be problematic in cases like prompt injections. It's serving as an example and you should consider implementing your own container restrictions in production.\n\n#### Usage\n\nTo enable the python tool, you'll have to place the definition into the `system` message of your harmony formatted prompt. You can either use the `with_python()` method if your tool implements the full interface or modify the definition using `with_tools()`. For example:\n\n```python\nimport datetime\nfrom gpt_oss.tools.python_docker.docker_tool import PythonTool\nfrom openai_harmony import SystemContent, Message, Conversation, Role, load_harmony_encoding, HarmonyEncodingName\n\nencoding = load_harmony_encoding(HarmonyEncodingName.HARMONY_GPT_OSS)\n\npython_tool = PythonTool()\n\n# create a basic system prompt\nsystem_message_content = SystemContent.new().with_conversation_start_date(\n    datetime.datetime.now().strftime(\"%Y-%m-%d\")\n)\n\n# if you want to use the python tool\nif use_python_tool:\n    # enables the tool making sure that the prompt gets set with the stateless tool description\n    system_message_content = system_message_content.with_tools(python_tool.tool_config)\n    # alternatively you could use the following if your tool is not stateless\n    system_message_content = system_message_content.with_python()\n\n# construct the system message\nsystem_message = Message.from_role_and_content(Role.SYSTEM, system_message_content)\n\n# create the overall prompt\nmessages = [system_message, Message.from_role_and_content(Role.USER, \"What's the square root of 9001?\")]\nconversation = Conversation.from_messages(messages)\n\n# convert to tokens\ntoken_ids = encoding.render_conversation_for_completion(conversation, Role.ASSISTANT)\n\n# perform inference\n# ...\n\n# parse the output\nmessages = encoding.parse_messages_from_completion_tokens(output_tokens, Role.ASSISTANT)\nlast_message = messages[-1]\nif last_message.recipient == \"python\":\n  # perform python call\n  response_messages = await python_tool.process(last_message)\n\n  # extend the current messages and run inference again\n  messages.extend(response_messages)\n```\n\n### Apply Patch\n\n`apply_patch` can be used to create, update or delete files locally.\n\n## Other details\n\n### Precision format\n\nWe released the models with native quantization support. Specifically, we use [MXFP4](https://www.opencompute.org/documents/ocp-microscaling-formats-mx-v1-0-spec-final-pdf) for the linear projection weights in the MoE layer. We store the MoE tensor in two parts:\n\n- `tensor.blocks` stores the actual fp4 values. We pack every two values in one `uint8` value.\n- `tensor.scales` stores the block scale. The block scaling is done among the last dimension for all MXFP4 tensors.\n\nAll other tensors will be in BF16. We also recommend using BF16 as the activation precision for the model.\n\n### Recommended Sampling Parameters\n\nWe recommend sampling with `temperature=1.0` and `top_p=1.0`.\n\n## Contributing\n\nThe reference implementations in this repository are meant as a starting point and inspiration. Outside of bug fixes we do not intend to accept new feature contributions. If you build implementations based on this code such as new tool implementations you are welcome to contribute them to the [`awesome-gpt-oss.md`](./awesome-gpt-oss.md) file.\n\n[harmony]: https://github.com/openai/harmony\n\n## Citation\n\n```bibtex\n@misc{openai2025gptoss120bgptoss20bmodel,\n      title={gpt-oss-120b & gpt-oss-20b Model Card}, \n      author={OpenAI},\n      year={2025},\n      eprint={2508.10925},\n      archivePrefix={arXiv},\n      primaryClass={cs.CL},\n      url={https://arxiv.org/abs/2508.10925}, \n}\n```\n",
    "py_files": {
        "_build/gpt_oss_build_backend/__init__.py": "\"\"\"In-tree PEP 517 backend package for gpt-oss.\"\"\" ",
        "_build/gpt_oss_build_backend/backend.py": "\"\"\"\nBuild backend for gpt-oss that supports two modes:\n\n1) Default (pure wheel for PyPI)\n   - Delegates to setuptools.build_meta.\n   - Produces a py3-none-any wheel so PyPI accepts it (no linux_x86_64 tag).\n\n2) Optional Metal/C extension build (local only)\n   - If the environment variable GPTOSS_BUILD_METAL is set to a truthy value\n     (1/true/on/yes), delegates to scikit_build_core.build.\n   - Dynamically injects build requirements (scikit-build-core, cmake, ninja,\n     pybind11) only for this mode.\n\nWhy this is needed\n- PyPI rejects Linux wheels tagged linux_x86_64; manylinux/musllinux is required\n  for binary wheels. We ship a pure wheel by default, but still allow developers\n  to build/install the native Metal backend locally when needed.\n\nTypical usage\n- Publish pure wheel: `python -m build` (do not set GPTOSS_BUILD_METAL).\n- Local Metal dev: `GPTOSS_BUILD_METAL=1 pip install -e \".[metal]\"`.\n- CI: keep GPTOSS_BUILD_METAL unset for releases; set it in internal jobs that\n  exercise the extension.\n\nNotes\n- The base package remains importable without the extension. The Metal backend\n  is only used when `gpt_oss.metal` is explicitly imported.\n- This file is discovered via `backend-path = [\"_build\"]` and\n  `build-backend = \"gpt_oss_build_backend.backend\"` in pyproject.toml.\n\"\"\"\nimport os\nfrom importlib import import_module\nfrom typing import Any, Mapping, Sequence\n\n\nTRUE_VALUES = {\"1\", \"true\", \"TRUE\", \"on\", \"ON\", \"yes\", \"YES\"}\n\n\ndef _use_metal_backend() -> bool:\n    return str(os.environ.get(\"GPTOSS_BUILD_METAL\", \"\")).strip() in TRUE_VALUES\n\n\ndef _setuptools_backend():\n    from setuptools import build_meta as _bm  # type: ignore\n\n    return _bm\n\n\ndef _scikit_build_backend():\n    return import_module(\"scikit_build_core.build\")\n\n\ndef _backend():\n    return _scikit_build_backend() if _use_metal_backend() else _setuptools_backend()\n\n\n# Required PEP 517 hooks\n\ndef build_wheel(\n    wheel_directory: str,\n    config_settings: Mapping[str, Any] | None = None,\n    metadata_directory: str | None = None,\n) -> str:\n    return _backend().build_wheel(wheel_directory, config_settings, metadata_directory)\n\n\ndef build_sdist(\n    sdist_directory: str, config_settings: Mapping[str, Any] | None = None\n) -> str:\n    return _backend().build_sdist(sdist_directory, config_settings)\n\n\ndef prepare_metadata_for_build_wheel(\n    metadata_directory: str, config_settings: Mapping[str, Any] | None = None\n) -> str:\n    # Fallback if backend doesn't implement it\n    be = _backend()\n    fn = getattr(be, \"prepare_metadata_for_build_wheel\", None)\n    if fn is None:\n        # setuptools exposes it; scikit-build-core may not. Defer to building a wheel for metadata.\n        return _setuptools_backend().prepare_metadata_for_build_wheel(\n            metadata_directory, config_settings\n        )\n    return fn(metadata_directory, config_settings)\n\n\n# Optional hooks\n\ndef build_editable(\n    editable_directory: str, config_settings: Mapping[str, Any] | None = None, metadata_directory: str | None = None\n) -> str:\n    be = _backend()\n    fn = getattr(be, \"build_editable\", None)\n    if fn is None:\n        # setuptools implements build_editable; if not available, raise the standard error\n        raise RuntimeError(\"Editable installs not supported by the selected backend\")\n    return fn(editable_directory, config_settings)\n\n\ndef get_requires_for_build_wheel(\n    config_settings: Mapping[str, Any] | None = None,\n) -> Sequence[str]:\n    if _use_metal_backend():\n        # Add dynamic build requirements only when building the Metal backend\n        return [\n            \"scikit-build-core>=0.10\",\n            \"pybind11>=2.12\",\n            \"cmake>=3.26\",\n            \"ninja\",\n        ]\n    # setuptools usually returns []\n    return list(_setuptools_backend().get_requires_for_build_wheel(config_settings))\n\n\ndef get_requires_for_build_sdist(\n    config_settings: Mapping[str, Any] | None = None,\n) -> Sequence[str]:\n    # No special requirements for SDist\n    be = _backend()\n    fn = getattr(be, \"get_requires_for_build_sdist\", None)\n    if fn is None:\n        return []\n    return list(fn(config_settings))\n\n\ndef get_requires_for_build_editable(\n    config_settings: Mapping[str, Any] | None = None,\n) -> Sequence[str]:\n    if _use_metal_backend():\n        return [\n            \"scikit-build-core>=0.10\",\n            \"pybind11>=2.12\",\n            \"cmake>=3.26\",\n            \"ninja\",\n        ]\n    be = _setuptools_backend()\n    fn = getattr(be, \"get_requires_for_build_editable\", None)\n    if fn is None:\n        return []\n    return list(fn(config_settings)) ",
        "examples/agents-sdk-python/example.py": "import asyncio\nfrom pathlib import Path\nimport shutil\n\nfrom openai import AsyncOpenAI\nfrom agents import (\n    Agent,\n    ItemHelpers,\n    Runner,\n    set_default_openai_api,\n    set_default_openai_client,\n    set_tracing_disabled,\n    function_tool,\n)\nfrom agents.mcp import MCPServerStdio\n\n\nasync def prompt_user(question: str) -> str:\n    \"\"\"Async input prompt function\"\"\"\n    loop = asyncio.get_event_loop()\n    return await loop.run_in_executor(None, input, question)\n\n\nasync def main():\n    # Set up OpenAI client for local server (e.g., Ollama)\n    openai_client = AsyncOpenAI(\n        api_key=\"local\",\n        base_url=\"http://localhost:11434/v1\",\n    )\n\n    # Get current working directory\n    samples_dir = str(Path.cwd())\n\n    # Create MCP server for filesystem operations\n    mcp_server = MCPServerStdio(\n        name=\"Filesystem MCP Server, via npx\",\n        params={\n            \"command\": \"npx\",\n            \"args\": [\n                \"-y\",\n                \"@modelcontextprotocol/server-filesystem\",\n                samples_dir,\n            ],\n        },\n    )\n\n    # Connect to MCP server\n    await mcp_server.connect()\n\n    # Configure agents SDK\n    set_tracing_disabled(True)\n    set_default_openai_client(openai_client)\n    set_default_openai_api(\"chat_completions\")\n\n    # Define weather tool\n    @function_tool\n    async def get_weather(location: str) -> str:\n        return f\"The weather in {location} is sunny.\"\n\n    # Create agent\n    agent = Agent(\n        name=\"My Agent\",\n        instructions=\"You are a helpful assistant.\",\n        tools=[get_weather],\n        model=\"gpt-oss:20b-test\",\n        mcp_servers=[mcp_server],\n    )\n\n    # Get user input\n    user_input = await prompt_user(\"> \")\n\n    # Run agent with streaming\n    result = Runner.run_streamed(agent, user_input)\n\n    # Process streaming results\n    async for event in result.stream_events():\n        if event.type == \"raw_response_event\":\n            continue\n        elif event.type == \"agent_updated_stream_event\":\n            print(f\"Agent updated: {event.new_agent.name}\")\n        elif event.type == \"run_item_stream_event\":\n            if event.item.type == \"tool_call_item\":\n                print(\"-- Tool was called\")\n            elif event.item.type == \"tool_call_output_item\":\n                print(f\"-- Tool output: {event.item.output}\")\n            elif event.item.type == \"message_output_item\":\n                print(\n                    f\"-- Message output:\\n {ItemHelpers.text_message_output(event.item)}\"\n                )\n            else:\n                pass\n\n    print(\"=== Run complete ===\")\n\n\nif __name__ == \"__main__\":\n\n    if not shutil.which(\"npx\"):\n        raise RuntimeError(\n            \"npx is not installed. Please install it with `npm install -g npx`.\"\n        )\n    asyncio.run(main())\n",
        "examples/gradio/gradio_chat.py": "import json\nimport requests\nimport gradio as gr\n\nDEFAULT_FUNCTION_PROPERTIES = \"\"\"\n{\n    \"type\": \"object\",\n    \"properties\": {\n        \"location\": {\n            \"type\": \"string\",\n            \"description\": \"The city and state, e.g. San Francisco, CA\"\n        }\n    },\n    \"required\": [\"location\"]\n}\n\"\"\".strip()\n\ndef chat_with_model(message, history, model_choice, instructions, effort, use_functions, \n                   function_name, function_description, function_parameters,\n                   use_browser_search, temperature, max_output_tokens, debug_mode):\n    \n    if not message.strip():\n        return history, \"\"\n    \n    # Append user message and empty assistant placeholder (idiomatic Gradio pattern)\n    history = history + [[message, \"\"]]\n    \n    # Build messages list from history (excluding the empty assistant placeholder)\n    messages = []\n    \n    # Convert history to messages format (excluding the last empty assistant message)\n    for user_msg, assistant_msg in history[:-1]:\n        if user_msg:\n            messages.append({\n                \"type\": \"message\",\n                \"role\": \"user\", \n                \"content\": [{\"type\": \"input_text\", \"text\": user_msg}]\n            })\n        if assistant_msg:\n            messages.append({\n                \"type\": \"message\",\n                \"role\": \"assistant\",\n                \"content\": [{\"type\": \"output_text\", \"text\": assistant_msg}]\n            })\n    \n    # Add current user message\n    messages.append({\n        \"type\": \"message\",\n        \"role\": \"user\",\n        \"content\": [{\"type\": \"input_text\", \"text\": message}]\n    })\n    \n    # Prepare tools\n    tools = []\n    if use_functions:\n        try:\n            tools.append({\n                \"type\": \"function\",\n                \"name\": function_name,\n                \"description\": function_description,\n                \"parameters\": json.loads(function_parameters),\n            })\n        except json.JSONDecodeError:\n            pass\n    \n    if use_browser_search:\n        tools.append({\"type\": \"browser_search\"})\n    \n    # Get URL based on model (matching streamlit logic)\n    options = [\"large\", \"small\"]\n    URL = (\"http://localhost:8081/v1/responses\" if model_choice == options[1] \n           else \"http://localhost:8000/v1/responses\")\n    \n    try:\n        response = requests.post(\n            URL,\n            json={\n                \"input\": messages,\n                \"stream\": True,\n                \"instructions\": instructions,\n                \"reasoning\": {\"effort\": effort},\n                \"metadata\": {\"__debug\": debug_mode},\n                \"tools\": tools,\n                \"temperature\": temperature,\n                \"max_output_tokens\": max_output_tokens,\n            },\n            stream=True,\n        )\n        \n        full_content = \"\"\n        text_delta = \"\"\n        current_output_index = 0\n        in_reasoning = False\n        \n        for line in response.iter_lines(decode_unicode=True):\n            if not line or not line.startswith(\"data:\"):\n                continue\n            data_str = line[len(\"data:\"):].strip()\n            if not data_str:\n                continue\n            \n            try:\n                data = json.loads(data_str)\n            except Exception:\n                continue\n            \n            event_type = data.get(\"type\", \"\")\n            output_index = data.get(\"output_index\", 0)\n            \n            if event_type == \"response.output_item.added\":\n                current_output_index = output_index\n                output_type = data.get(\"item\", {}).get(\"type\", \"message\")\n                text_delta = \"\"\n                \n                if output_type == \"reasoning\":\n                    if not in_reasoning:\n                        full_content += \"🤔 **Thinking...**\\n\"\n                        in_reasoning = True\n                elif output_type == \"message\":\n                    if in_reasoning:\n                        full_content += \"\\n\\n\"\n                        in_reasoning = False\n                \n            elif event_type == \"response.reasoning_text.delta\":\n                delta = data.get(\"delta\", \"\")\n                full_content += delta\n                \n                # Update last assistant message (idiomatic Gradio pattern)\n                history[-1][1] = full_content\n                yield history, \"\"\n                \n            elif event_type == \"response.output_text.delta\":\n                delta = data.get(\"delta\", \"\")\n                full_content += delta\n                \n                # Update last assistant message (idiomatic Gradio pattern)  \n                history[-1][1] = full_content\n                yield history, \"\"\n                \n            elif event_type == \"response.output_item.done\":\n                item = data.get(\"item\", {})\n                if item.get(\"type\") == \"function_call\":\n                    function_call_text = f\"\\n\\n🔨 Called `{item.get('name')}`\\n**Arguments**\\n```json\\n{item.get('arguments', '')}\\n```\"\n                    full_content += function_call_text\n                    \n                    # Update last assistant message (idiomatic Gradio pattern)\n                    history[-1][1] = full_content\n                    yield history, \"\"\n                    \n                elif item.get(\"type\") == \"web_search_call\":\n                    web_search_text = f\"\\n\\n🌐 **Web Search**\\n```json\\n{json.dumps(item.get('action', {}), indent=2)}\\n```\\n✅ Done\"\n                    full_content += web_search_text\n                    \n                    # Update last assistant message (idiomatic Gradio pattern)\n                    history[-1][1] = full_content\n                    yield history, \"\"\n                    \n            elif event_type == \"response.completed\":\n                response_data = data.get(\"response\", {})\n                if debug_mode:\n                    debug_info = response_data.get(\"metadata\", {}).get(\"__debug\", \"\")\n                    if debug_info:\n                        full_content += f\"\\n\\n**Debug**\\n```\\n{debug_info}\\n```\"\n                        \n                        # Update last assistant message (idiomatic Gradio pattern)\n                        history[-1][1] = full_content\n                        yield history, \"\"\n                break\n        \n        # Return final history and empty string to clear textbox\n        return history, \"\"\n        \n    except Exception as e:\n        error_message = f\"❌ Error: {str(e)}\"\n        history[-1][1] = error_message\n        return history, \"\"\n\n\n# Create the Gradio interface\nwith gr.Blocks(title=\"💬 Chatbot\") as demo:\n    gr.Markdown(\"# 💬 Chatbot\")\n    \n    with gr.Row():\n        with gr.Column(scale=3):\n            chatbot = gr.Chatbot(height=500)\n            \n            with gr.Row():\n                msg = gr.Textbox(placeholder=\"Type a message...\", scale=4, show_label=False)\n                send_btn = gr.Button(\"Send\", scale=1)\n            \n            clear_btn = gr.Button(\"Clear Chat\")\n        \n        with gr.Column(scale=1):\n            model_choice = gr.Radio([\"large\", \"small\"], value=\"small\", label=\"Model\")\n            \n            instructions = gr.Textbox(\n                label=\"Instructions\", \n                value=\"You are a helpful assistant that can answer questions and help with tasks.\",\n                lines=3\n            )\n            \n            effort = gr.Radio([\"low\", \"medium\", \"high\"], value=\"medium\", label=\"Reasoning effort\")\n            \n            gr.Markdown(\"#### Functions\")\n            use_functions = gr.Checkbox(label=\"Use functions\", value=False)\n            \n            with gr.Column(visible=False) as function_group:\n                function_name = gr.Textbox(label=\"Function name\", value=\"get_weather\")\n                function_description = gr.Textbox(\n                    label=\"Function description\", \n                    value=\"Get the weather for a given city\"\n                )\n                function_parameters = gr.Textbox(\n                    label=\"Function parameters\", \n                    value=DEFAULT_FUNCTION_PROPERTIES,\n                    lines=6\n                )\n            \n            # Conditional browser search (matching Streamlit logic)\n            # In Streamlit: if \"show_browser\" in st.query_params:\n            # For Gradio, we'll always show it (simplified)\n            gr.Markdown(\"#### Built-in Tools\") \n            use_browser_search = gr.Checkbox(label=\"Use browser search\", value=False)\n            \n            temperature = gr.Slider(0.0, 1.0, value=1.0, step=0.01, label=\"Temperature\")\n            max_output_tokens = gr.Slider(1000, 20000, value=1024, step=100, label=\"Max output tokens\")\n            \n            debug_mode = gr.Checkbox(label=\"Debug mode\", value=False)\n    \n    # Event handlers\n    def toggle_function_group(use_funcs):\n        return gr.update(visible=use_funcs)\n    \n    use_functions.change(toggle_function_group, use_functions, function_group)\n    \n    # Chat functionality\n    inputs = [msg, chatbot, model_choice, instructions, effort, use_functions, \n              function_name, function_description, function_parameters,\n              use_browser_search, temperature, max_output_tokens, debug_mode]\n    \n    msg.submit(chat_with_model, inputs, [chatbot, msg])\n    send_btn.click(chat_with_model, inputs, [chatbot, msg])\n    clear_btn.click(lambda: [], outputs=chatbot)\n\n\nif __name__ == \"__main__\":\n    demo.launch()",
        "examples/streamlit/streamlit_chat.py": "import json\n\nimport requests\nimport streamlit as st\n\nDEFAULT_FUNCTION_PROPERTIES = \"\"\"\n{\n    \"type\": \"object\",\n    \"properties\": {\n        \"location\": {\n            \"type\": \"string\",\n            \"description\": \"The city and state, e.g. San Francisco, CA\"\n        }\n    },\n    \"required\": [\"location\"]\n}\n\"\"\".strip()\n\n# Session state for chat\nif \"messages\" not in st.session_state:\n    st.session_state.messages = []\n\nst.title(\"💬 Chatbot\")\n\nif \"model\" not in st.session_state:\n    if \"model\" in st.query_params:\n        st.session_state.model = st.query_params[\"model\"]\n    else:\n        st.session_state.model = \"small\"\n\noptions = [\"large\", \"small\"]\nselection = st.sidebar.segmented_control(\n    \"Model\", options, selection_mode=\"single\", default=st.session_state.model\n)\n# st.session_state.model = selection\nst.query_params.update({\"model\": selection})\n\ninstructions = st.sidebar.text_area(\n    \"Instructions\",\n    value=\"You are a helpful assistant that can answer questions and help with tasks.\",\n)\neffort = st.sidebar.radio(\n    \"Reasoning effort\",\n    [\"low\", \"medium\", \"high\"],\n    index=1,\n)\nst.sidebar.divider()\nst.sidebar.subheader(\"Functions\")\nuse_functions = st.sidebar.toggle(\"Use functions\", value=False)\n\nst.sidebar.subheader(\"Built-in Tools\")\n# Built-in Tools section\nuse_browser_search = st.sidebar.toggle(\"Use browser search\", value=False)\nuse_code_interpreter = st.sidebar.toggle(\"Use code interpreter\", value=False)\n\nif use_functions:\n    function_name = st.sidebar.text_input(\"Function name\", value=\"get_weather\")\n    function_description = st.sidebar.text_area(\n        \"Function description\", value=\"Get the weather for a given city\"\n    )\n    function_parameters = st.sidebar.text_area(\n        \"Function parameters\", value=DEFAULT_FUNCTION_PROPERTIES\n    )\nelse:\n    function_name = None\n    function_description = None\n    function_parameters = None\nst.sidebar.divider()\ntemperature = st.sidebar.slider(\n    \"Temperature\", min_value=0.0, max_value=1.0, value=1.0, step=0.01\n)\nmax_output_tokens = st.sidebar.slider(\n    \"Max output tokens\", min_value=1, max_value=131072, value=30000, step=1000\n)\nst.sidebar.divider()\ndebug_mode = st.sidebar.toggle(\"Debug mode\", value=False)\n\nif debug_mode:\n    st.sidebar.divider()\n    st.sidebar.code(json.dumps(st.session_state.messages, indent=2), \"json\")\n\nrender_input = True\n\nURL = (\n    \"http://localhost:8081/v1/responses\"\n    if selection == options[1]\n    else \"http://localhost:8000/v1/responses\"\n)\n\n\ndef trigger_fake_tool(container):\n    function_output = st.session_state.get(\"function_output\", \"It's sunny!\")\n    last_call = st.session_state.messages[-1]\n    if last_call.get(\"type\") == \"function_call\":\n        st.session_state.messages.append(\n            {\n                \"type\": \"function_call_output\",\n                \"call_id\": last_call.get(\"call_id\"),\n                \"output\": function_output,\n            }\n        )\n        run(container)\n\n\ndef run(container):\n    tools = []\n    if use_functions:\n        tools.append(\n            {\n                \"type\": \"function\",\n                \"name\": function_name,\n                \"description\": function_description,\n                \"parameters\": json.loads(function_parameters),\n            }\n        )\n    # Add browser_search tool if checkbox is checked\n    if use_browser_search:\n        tools.append({\"type\": \"browser_search\"})\n    if use_code_interpreter:\n        tools.append({\"type\": \"code_interpreter\"})\n    response = requests.post(\n        URL,\n        json={\n            \"input\": st.session_state.messages,\n            \"stream\": True,\n            \"instructions\": instructions,\n            \"reasoning\": {\"effort\": effort},\n            \"metadata\": {\"__debug\": debug_mode},\n            \"tools\": tools,\n            \"temperature\": temperature,\n            \"max_output_tokens\": max_output_tokens,\n        },\n        stream=True,\n    )\n\n    text_delta = \"\"\n\n    _current_output_index = 0\n    for line in response.iter_lines(decode_unicode=True):\n        if not line or not line.startswith(\"data:\"):\n            continue\n        data_str = line[len(\"data:\") :].strip()\n        if not data_str:\n            continue\n        try:\n            data = json.loads(data_str)\n        except Exception:\n            continue\n\n        event_type = data.get(\"type\", \"\")\n        output_index = data.get(\"output_index\", 0)\n        if event_type == \"response.output_item.added\":\n            _current_output_index = output_index\n            output_type = data.get(\"item\", {}).get(\"type\", \"message\")\n            if output_type == \"message\":\n                output = container.chat_message(\"assistant\")\n                placeholder = output.empty()\n            elif output_type == \"reasoning\":\n                output = container.chat_message(\"reasoning\", avatar=\"🤔\")\n                placeholder = output.empty()\n            elif output_type == \"web_search_call\":\n                output = container.chat_message(\"web_search_call\", avatar=\"🌐\")\n                output.code(\n                    json.dumps(data.get(\"item\", {}).get(\"action\", {}), indent=4),\n                    language=\"json\",\n                )\n                placeholder = output.empty()\n            elif output_type == \"code_interpreter_call\":\n                output = container.chat_message(\"code_interpreter_call\", avatar=\"🧪\")\n                placeholder = output.empty()\n            text_delta = \"\"\n        elif event_type == \"response.reasoning_text.delta\":\n            output.avatar = \"🤔\"\n            text_delta += data.get(\"delta\", \"\")\n            placeholder.markdown(text_delta)\n        elif event_type == \"response.output_text.delta\":\n            text_delta += data.get(\"delta\", \"\")\n            placeholder.markdown(text_delta)\n        elif event_type == \"response.output_item.done\":\n            item = data.get(\"item\", {})\n            if item.get(\"type\") == \"function_call\":\n                with container.chat_message(\"function_call\", avatar=\"🔨\"):\n                    st.markdown(f\"Called `{item.get('name')}`\")\n                    st.caption(\"Arguments\")\n                    st.code(item.get(\"arguments\", \"\"), language=\"json\")\n            if item.get(\"type\") == \"web_search_call\":\n                placeholder.markdown(\"✅ Done\")\n            if item.get(\"type\") == \"code_interpreter_call\":\n                placeholder.markdown(\"✅ Done\")\n        elif event_type == \"response.code_interpreter_call.in_progress\":\n            try:\n                placeholder.markdown(\"⏳ Running\")\n            except Exception:\n                pass\n        elif event_type == \"response.code_interpreter_call.completed\":\n            try:\n                placeholder.markdown(\"✅ Done\")\n            except Exception:\n                pass\n        elif event_type == \"response.completed\":\n            response = data.get(\"response\", {})\n            if debug_mode:\n                container.expander(\"Debug\", expanded=False).code(\n                    response.get(\"metadata\", {}).get(\"__debug\", \"\"), language=\"text\"\n                )\n            st.session_state.messages.extend(response.get(\"output\", []))\n            if st.session_state.messages[-1].get(\"type\") == \"function_call\":\n                with container.form(\"function_output_form\"):\n                    _function_output = st.text_input(\n                        \"Enter function output\",\n                        value=st.session_state.get(\"function_output\", \"It's sunny!\"),\n                        key=\"function_output\",\n                    )\n                    st.form_submit_button(\n                        \"Submit function output\",\n                        on_click=trigger_fake_tool,\n                        args=[container],\n                    )\n            # Optionally handle other event types...\n\n\n# Chat display\nfor msg in st.session_state.messages:\n    if msg.get(\"type\") == \"message\":\n        with st.chat_message(msg[\"role\"]):\n            for item in msg[\"content\"]:\n                if (\n                    item.get(\"type\") == \"text\"\n                    or item.get(\"type\") == \"output_text\"\n                    or item.get(\"type\") == \"input_text\"\n                ):\n                    st.markdown(item[\"text\"])\n                    if item.get(\"annotations\"):\n                        annotation_lines = \"\\n\".join(\n                            f\"- {annotation.get('url')}\"\n                            for annotation in item[\"annotations\"]\n                            if annotation.get(\"url\")\n                        )\n                        st.caption(f\"**Annotations:**\\n{annotation_lines}\")\n    elif msg.get(\"type\") == \"reasoning\":\n        with st.chat_message(\"reasoning\", avatar=\"🤔\"):\n            for item in msg[\"content\"]:\n                if item.get(\"type\") == \"reasoning_text\":\n                    st.markdown(item[\"text\"])\n    elif msg.get(\"type\") == \"function_call\":\n        with st.chat_message(\"function_call\", avatar=\"🔨\"):\n            st.markdown(f\"Called `{msg.get('name')}`\")\n            st.caption(\"Arguments\")\n            st.code(msg.get(\"arguments\", \"\"), language=\"json\")\n    elif msg.get(\"type\") == \"function_call_output\":\n        with st.chat_message(\"function_call_output\", avatar=\"✅\"):\n            st.caption(\"Output\")\n            st.code(msg.get(\"output\", \"\"), language=\"text\")\n    elif msg.get(\"type\") == \"web_search_call\":\n        with st.chat_message(\"web_search_call\", avatar=\"🌐\"):\n            st.code(json.dumps(msg.get(\"action\", {}), indent=4), language=\"json\")\n            st.markdown(\"✅ Done\")\n    elif msg.get(\"type\") == \"code_interpreter_call\":\n        with st.chat_message(\"code_interpreter_call\", avatar=\"🧪\"):\n            st.markdown(\"✅ Done\")\n\nif render_input:\n    # Input field\n    if prompt := st.chat_input(\"Type a message...\"):\n        st.session_state.messages.append(\n            {\n                \"type\": \"message\",\n                \"role\": \"user\",\n                \"content\": [{\"type\": \"input_text\", \"text\": prompt}],\n            }\n        )\n\n        with st.chat_message(\"user\"):\n            st.markdown(prompt)\n\n        run(st.container())\n",
        "gpt-oss-mcp-server/browser_server.py": "import os\nfrom collections.abc import AsyncIterator\nfrom contextlib import asynccontextmanager\nfrom dataclasses import dataclass, field\nfrom typing import Union, Optional\n\nfrom mcp.server.fastmcp import Context, FastMCP\nfrom gpt_oss.tools.simple_browser import SimpleBrowserTool\nfrom gpt_oss.tools.simple_browser.backend import YouComBackend, ExaBackend\n\n@dataclass\nclass AppContext:\n    browsers: dict[str, SimpleBrowserTool] = field(default_factory=dict)\n\n    def create_or_get_browser(self, session_id: str) -> SimpleBrowserTool:\n        if session_id not in self.browsers:\n            tool_backend = os.getenv(\"BROWSER_BACKEND\", \"exa\")\n            if tool_backend == \"youcom\":\n                backend = YouComBackend(source=\"web\")\n            elif tool_backend == \"exa\":\n                backend = ExaBackend(source=\"web\")\n            else:\n                raise ValueError(f\"Invalid tool backend: {tool_backend}\")\n            self.browsers[session_id] = SimpleBrowserTool(backend=backend)\n        return self.browsers[session_id]\n\n    def remove_browser(self, session_id: str) -> None:\n        self.browsers.pop(session_id, None)\n\n\n@asynccontextmanager\nasync def app_lifespan(_server: FastMCP) -> AsyncIterator[AppContext]:\n    yield AppContext()\n\n\n# Pass lifespan to server\nmcp = FastMCP(\n    name=\"browser\",\n    instructions=r\"\"\"\nTool for browsing.\nThe `cursor` appears in brackets before each browsing display: `[{cursor}]`.\nCite information from the tool using the following format:\n`【{cursor}†L{line_start}(-L{line_end})?】`, for example: `【6†L9-L11】` or `【8†L3】`. \nDo not quote more than 10 words directly from the tool output.\nsources=web\n\"\"\".strip(),\n    lifespan=app_lifespan,\n    port=8001,\n)\n\n\n@mcp.tool(\n    name=\"search\",\n    title=\"Search for information\",\n    description=\n    \"Searches for information related to `query` and displays `topn` results.\",\n)\nasync def search(ctx: Context,\n                 query: str,\n                 topn: int = 10,\n                 source: Optional[str] = None) -> str:\n    \"\"\"Search for information related to a query\"\"\"\n    browser = ctx.request_context.lifespan_context.create_or_get_browser(\n        ctx.client_id)\n    messages = []\n    async for message in browser.search(query=query, topn=topn, source=source):\n        if message.content and hasattr(message.content[0], 'text'):\n            messages.append(message.content[0].text)\n    return \"\\n\".join(messages)\n\n\n@mcp.tool(\n    name=\"open\",\n    title=\"Open a link or page\",\n    description=\"\"\"\nOpens the link `id` from the page indicated by `cursor` starting at line number `loc`, showing `num_lines` lines.\nValid link ids are displayed with the formatting: `【{id}†.*】`.\nIf `cursor` is not provided, the most recent page is implied.\nIf `id` is a string, it is treated as a fully qualified URL associated with `source`.\nIf `loc` is not provided, the viewport will be positioned at the beginning of the document or centered on the most relevant passage, if available.\nUse this function without `id` to scroll to a new location of an opened page.\n\"\"\".strip(),\n)\nasync def open_link(ctx: Context,\n                    id: Union[int, str] = -1,\n                    cursor: int = -1,\n                    loc: int = -1,\n                    num_lines: int = -1,\n                    view_source: bool = False,\n                    source: Optional[str] = None) -> str:\n    \"\"\"Open a link or navigate to a page location\"\"\"\n    browser = ctx.request_context.lifespan_context.create_or_get_browser(\n        ctx.client_id)\n    messages = []\n    async for message in browser.open(id=id,\n                                      cursor=cursor,\n                                      loc=loc,\n                                      num_lines=num_lines,\n                                      view_source=view_source,\n                                      source=source):\n        if message.content and hasattr(message.content[0], 'text'):\n            messages.append(message.content[0].text)\n    return \"\\n\".join(messages)\n\n\n@mcp.tool(\n    name=\"find\",\n    title=\"Find pattern in page\",\n    description=\n    \"Finds exact matches of `pattern` in the current page, or the page given by `cursor`.\",\n)\nasync def find_pattern(ctx: Context, pattern: str, cursor: int = -1) -> str:\n    \"\"\"Find exact matches of a pattern in the current page\"\"\"\n    browser = ctx.request_context.lifespan_context.create_or_get_browser(\n        ctx.client_id)\n    messages = []\n    async for message in browser.find(pattern=pattern, cursor=cursor):\n        if message.content and hasattr(message.content[0], 'text'):\n            messages.append(message.content[0].text)\n    return \"\\n\".join(messages)\n",
        "gpt-oss-mcp-server/build-system-prompt.py": "import datetime\nimport asyncio\n\nfrom gpt_oss.tokenizer import get_tokenizer\n\nfrom openai_harmony import (\n    Conversation,\n    DeveloperContent,\n    HarmonyEncodingName,\n    Message,\n    ReasoningEffort,\n    Role,\n    SystemContent,\n    ToolNamespaceConfig,\n    ToolDescription,\n    load_harmony_encoding,\n)\n\nfrom mcp import ClientSession\nfrom mcp.client.sse import sse_client\nfrom mcp.types import ListToolsResult\n\n\nasync def list_server_and_tools(server_url: str):\n    async with sse_client(url=server_url) as streams, ClientSession(\n            *streams) as session:\n        initialize_response = await session.initialize()\n        list_tools_response = await session.list_tools()\n        return initialize_response, list_tools_response\n\n\ndef trim_schema(schema: dict) -> dict:\n    # Turn JSON Schema from MCP generated into Harmony's variant.\n    if \"title\" in schema:\n        del schema[\"title\"]\n    if \"default\" in schema and schema[\"default\"] is None:\n        del schema[\"default\"]\n    if \"anyOf\" in schema:\n        # Turn \"anyOf\": [{\"type\": \"type-1\"}, {\"type\": \"type-2\"}] into \"type\": [\"type-1\", \"type-2\"]\n        # if there's more than 1 types, also remove \"null\" type as Harmony will just ignore it\n        types = [\n            type_dict[\"type\"] for type_dict in schema[\"anyOf\"]\n            if type_dict[\"type\"] != 'null'\n        ]\n        schema[\"type\"] = types\n        del schema[\"anyOf\"]\n    if \"properties\" in schema:\n        schema[\"properties\"] = {\n            k: trim_schema(v)\n            for k, v in schema[\"properties\"].items()\n        }\n    return schema\n\n\ndef post_process_tools_description(\n        list_tools_result: ListToolsResult) -> ListToolsResult:\n    # Adapt the MCP tool result for Harmony\n    for tool in list_tools_result.tools:\n        tool.inputSchema = trim_schema(tool.inputSchema)\n\n    # Some tools schema don't need to be part of the prompt (e.g. simple text in text out for Python)\n    list_tools_result.tools = [\n        tool for tool in list_tools_result.tools\n        if getattr(tool.annotations, \"include_in_prompt\", True)\n    ]\n\n    return list_tools_result\n\ntokenizer = get_tokenizer()\n\ntools_urls = [\n    \"http://localhost:8001/sse\",  # browser\n    \"http://localhost:8000/sse\",  # python\n]\nharmony_tool_descriptions = []\nfor tools_url in tools_urls:\n\n    initialize_response, list_tools_response = asyncio.run(\n        list_server_and_tools(tools_url))\n\n    list_tools_response = post_process_tools_description(list_tools_response)\n\n    tool_from_mcp = ToolNamespaceConfig(\n        name=initialize_response.serverInfo.name,\n        description=initialize_response.instructions,\n        tools=[\n            ToolDescription.new(name=tool.name,\n                                description=tool.description,\n                                parameters=tool.inputSchema)\n            for tool in list_tools_response.tools\n        ])\n    harmony_tool_descriptions.append(tool_from_mcp)\n\nencoding = load_harmony_encoding(HarmonyEncodingName.HARMONY_GPT_OSS)\n\nsystem_message_content = (SystemContent.new().with_reasoning_effort(\n    ReasoningEffort.LOW).with_conversation_start_date(\n        datetime.datetime.now().strftime(\"%Y-%m-%d\")))\n\nfor tool_description in harmony_tool_descriptions:\n    system_message_content = system_message_content.with_tools(\n        tool_description)\n\nsystem_message = Message.from_role_and_content(Role.SYSTEM,\n                                               system_message_content)\n\ndeveloper_message_content = DeveloperContent.new().with_instructions(\"\")\ndeveloper_message = Message.from_role_and_content(Role.DEVELOPER,\n                                                  developer_message_content)\n\nmessages = [system_message, developer_message]\n\nconversation = Conversation.from_messages(messages)\ntokens = encoding.render_conversation(conversation)\nsystem_message = tokenizer.decode(tokens)\nprint(system_message)\n",
        "gpt-oss-mcp-server/python_server.py": "from mcp.server.fastmcp import FastMCP\nfrom gpt_oss.tools.python_docker.docker_tool import PythonTool\nfrom openai_harmony import Message, TextContent, Author, Role\n\n# Pass lifespan to server\nmcp = FastMCP(\n    name=\"python\",\n    instructions=r\"\"\"\nUse this tool to execute Python code in your chain of thought. The code will not be shown to the user. This tool should be used for internal reasoning, but not for code that is intended to be visible to the user (e.g. when creating plots, tables, or files).\nWhen you send a message containing python code to python, it will be executed in a stateless docker container, and the stdout of that process will be returned to you.\n\"\"\".strip(),\n)\n\n\n@mcp.tool(\n    name=\"python\",\n    title=\"Execute Python code\",\n    description=\"\"\"\nUse this tool to execute Python code in your chain of thought. The code will not be shown to the user. This tool should be used for internal reasoning, but not for code that is intended to be visible to the user (e.g. when creating plots, tables, or files).\nWhen you send a message containing python code to python, it will be executed in a stateless docker container, and the stdout of that process will be returned to you.\n    \"\"\",\n    annotations={\n        # Harmony format don't want this schema to be part of it because it's simple text in text out\n        \"include_in_prompt\": False,\n    })\nasync def python(code: str) -> str:\n    tool = PythonTool()\n    messages = []\n    async for message in tool.process(\n            Message(author=Author(role=Role.TOOL, name=\"python\"),\n                    content=[TextContent(text=code)])):\n        messages.append(message)\n    return \"\\n\".join([message.content[0].text for message in messages])\n",
        "gpt-oss-mcp-server/reference-system-prompt.py": "import datetime\n\nfrom gpt_oss.tools.simple_browser import SimpleBrowserTool\nfrom gpt_oss.tools.simple_browser.backend import YouComBackend\nfrom gpt_oss.tools.python_docker.docker_tool import PythonTool\nfrom gpt_oss.tokenizer import tokenizer\n\nfrom openai_harmony import (\n    Conversation,\n    DeveloperContent,\n    HarmonyEncodingName,\n    Message,\n    ReasoningEffort,\n    Role,\n    SystemContent,\n    load_harmony_encoding,\n)\n\nencoding = load_harmony_encoding(HarmonyEncodingName.HARMONY_GPT_OSS)\n\nsystem_message_content = (SystemContent.new().with_reasoning_effort(\n    ReasoningEffort.LOW).with_conversation_start_date(\n        datetime.datetime.now().strftime(\"%Y-%m-%d\")))\n\nbackend = YouComBackend(source=\"web\")\nbrowser_tool = SimpleBrowserTool(backend=backend)\nsystem_message_content = system_message_content.with_tools(\n    browser_tool.tool_config)\n\npython_tool = PythonTool()\nsystem_message_content = system_message_content.with_tools(\n    python_tool.tool_config)\n\nsystem_message = Message.from_role_and_content(Role.SYSTEM,\n                                               system_message_content)\n\ndeveloper_message_content = DeveloperContent.new().with_instructions(\"\")\ndeveloper_message = Message.from_role_and_content(Role.DEVELOPER,\n                                                  developer_message_content)\n\nmessages = [system_message, developer_message]\n\nconversation = Conversation.from_messages(messages)\ntokens = encoding.render_conversation(conversation)\nsystem_message = tokenizer.decode(tokens)\nprint(system_message)\n",
        "gpt_oss/__init__.py": "",
        "gpt_oss/chat.py": "\"\"\"\nHarmony chat with tools\n\"\"\"\n\nimport atexit\nimport argparse\nimport asyncio\nimport datetime\nimport os\nfrom pathlib import Path\n\ntry:\n    import gnureadline as readline\nexcept ImportError:\n    import readline\n\nimport torch\nimport termcolor\n\nfrom gpt_oss.tools import apply_patch\nfrom gpt_oss.tools.simple_browser import SimpleBrowserTool\nfrom gpt_oss.tools.simple_browser.backend import YouComBackend\nfrom gpt_oss.tools.python_docker.docker_tool import PythonTool\n\nfrom openai_harmony import (\n    Author,\n    Conversation,\n    DeveloperContent,\n    HarmonyEncodingName,\n    Message,\n    ReasoningEffort,\n    Role,\n    StreamableParser,\n    StreamState,\n    SystemContent,\n    TextContent,\n    ToolDescription,\n    load_harmony_encoding,\n)\n\n\nREASONING_EFFORT = {\n    \"high\": ReasoningEffort.HIGH,\n    \"medium\": ReasoningEffort.MEDIUM,\n    \"low\": ReasoningEffort.LOW,\n}\n\n\ndef get_user_input():\n    rank = torch.distributed.get_rank() if torch.distributed.is_initialized() else 0\n    if rank == 0:\n        user_input = input()\n    else:\n        user_input = \"\"\n    user_input_list = [user_input]\n    if torch.distributed.is_initialized():\n        torch.distributed.broadcast_object_list(user_input_list, 0)\n    return user_input_list[0]\n\n\ndef main(args):\n    match args.backend:\n        case \"triton\":\n            from gpt_oss.triton.model import TokenGenerator as TritonGenerator\n            from gpt_oss.torch.utils import init_distributed\n            device = init_distributed()\n            generator = TritonGenerator(args.checkpoint, args.context, device)\n        case \"torch\":\n            from gpt_oss.torch.model import TokenGenerator as TorchGenerator\n            from gpt_oss.torch.utils import init_distributed\n            device = init_distributed()\n            generator = TorchGenerator(args.checkpoint, device)\n        case \"vllm\":\n            from gpt_oss.vllm.token_generator import TokenGenerator as VLLMGenerator\n            generator = VLLMGenerator(args.checkpoint, tensor_parallel_size=2)\n        case _:\n            raise ValueError(f\"Invalid backend: {args.backend}\")\n\n    encoding = load_harmony_encoding(HarmonyEncodingName.HARMONY_GPT_OSS)\n\n    system_message_content = (\n        SystemContent.new()\n        .with_reasoning_effort(REASONING_EFFORT[args.reasoning_effort])\n        .with_conversation_start_date(datetime.datetime.now().strftime(\"%Y-%m-%d\"))\n    )\n\n    if args.browser:\n        backend = YouComBackend(\n            source=\"web\",\n        )\n        browser_tool = SimpleBrowserTool(backend=backend)\n        system_message_content = system_message_content.with_tools(browser_tool.tool_config)\n\n    if args.python:\n        python_tool = PythonTool()\n        system_message_content = system_message_content.with_tools(python_tool.tool_config)\n\n    system_message = Message.from_role_and_content(Role.SYSTEM, system_message_content)\n    messages = [system_message]\n\n    if args.apply_patch:\n        apply_patch_instructions = Path(apply_patch.__file__).parent / \"apply_patch.md\"\n        developer_message = \"\"\n        if args.developer_message:\n            developer_message = args.developer_message + \"\\n\"\n        developer_message += apply_patch_instructions.read_text()\n        developer_message_content = (\n            DeveloperContent.new()\n            .with_instructions(developer_message)\n            .with_function_tools([\n                ToolDescription.new(\n                    \"apply_patch\",\n                    \"Patch a file\",\n                    parameters={\n                        \"type\": \"string\",\n                        \"description\": \"Formatted patch code\",\n                        \"default\": \"*** Begin Patch\\n*** End Patch\\n\",\n                    }\n                ),\n            ])\n        )\n        messages.append(Message.from_role_and_content(Role.DEVELOPER, developer_message_content))\n    elif args.developer_message:\n        developer_message_content = DeveloperContent.new().with_instructions(args.developer_message)\n        messages.append(Message.from_role_and_content(Role.DEVELOPER, developer_message_content))\n    else:\n        developer_message_content = None\n\n    if args.raw:\n        conversation = Conversation.from_messages(messages)\n        tokens = encoding.render_conversation(conversation)\n        system_message = encoding.decode(tokens)\n        print(system_message, flush=True, end=\"\")\n        empty_user_message_tokens = encoding.render(Message.from_role_and_content(Role.USER, \"\"))\n        user_message_start = encoding.decode(empty_user_message_tokens[:-1])\n        user_message_end = encoding.decode(empty_user_message_tokens[-1:])\n    else:\n        # System message\n        print(termcolor.colored(\"System Message:\", \"cyan\"), flush=True)\n        print(termcolor.colored(\"Model Identity:\", \"cyan\"), system_message_content.model_identity, flush=True)\n        print(termcolor.colored(\"Reasoning Effort:\", \"cyan\"), system_message_content.reasoning_effort, flush=True)\n        print(termcolor.colored(\"Conversation Start Date:\", \"cyan\"), system_message_content.conversation_start_date, flush=True)\n        print(termcolor.colored(\"Knowledge Cutoff:\", \"cyan\"), system_message_content.knowledge_cutoff, flush=True)\n        print(termcolor.colored(\"Browser Tool:\", \"cyan\"), \"Enabled\" if args.browser else \"Disabled\", flush=True)\n        print(termcolor.colored(\"Python Tool:\", \"cyan\"), \"Enabled\" if args.python else \"Disabled\", flush=True)\n        print(termcolor.colored(\"Apply Patch Function:\", \"cyan\"), \"Enabled\" if args.apply_patch else \"Disabled\", flush=True)\n        if developer_message_content:\n            print(termcolor.colored(\"Developer Message:\", \"yellow\"), flush=True)\n            print(developer_message_content.instructions, flush=True)\n\n    # Print the system message and the user message start\n    MESSAGE_PADDING = 12\n    while True:\n        last_message = messages[-1]\n        if last_message.recipient is None:\n            if args.raw:\n                print(user_message_start, end=\"\", flush=True)\n                user_message = get_user_input()\n                print(user_message_end, flush=True, end=\"\")\n            else:\n                print(termcolor.colored(\"User:\".ljust(MESSAGE_PADDING), \"red\"), flush=True)\n                user_message = get_user_input()\n            user_message = Message.from_role_and_content(Role.USER, user_message)\n            messages.append(user_message)\n        else:\n            # Tool or function call\n            if last_message.recipient.startswith(\"browser.\"):\n                assert args.browser, \"Browser tool is not enabled\"\n                tool_name = \"Search\"\n                async def run_tool():\n                    results = []\n                    async for msg in browser_tool.process(last_message):\n                        results.append(msg)\n                    return results\n\n                result = asyncio.run(run_tool())\n                messages += result\n            elif last_message.recipient.startswith(\"python\"):\n                assert args.python, \"Python tool is not enabled\"\n                tool_name = \"Python\"\n                async def run_tool():\n                    results = []\n                    async for msg in python_tool.process(last_message):\n                        results.append(msg)\n                    return results\n\n                result = asyncio.run(run_tool())\n                messages += result\n            elif last_message.recipient == \"functions.apply_patch\":\n                assert args.apply_patch, \"Apply patch tool is not enabled\"\n                tool_name = \"Apply Patch\"\n                text = last_message.content[0].text\n                tool_output = None\n\n                if text.startswith(\"{\"):\n                    # this is json, try to extract the patch from it\n                    import json\n                    try:\n                        some_dict = json.loads(text)\n                        _, text = some_dict.popitem()\n                    except Exception as e:\n                        tool_output = f\"Error parsing JSON: {e}\"\n\n                if tool_output is None:\n                    try:\n                        tool_output = apply_patch.apply_patch(text)\n                    except Exception as e:\n                        tool_output = f\"Error applying patch: {e}\"\n\n                message = (\n                    Message(\n                        author=Author.new(Role.TOOL, last_message.recipient),\n                        content=[TextContent(text=tool_output)]\n                    )\n                    .with_recipient(\"assistant\")\n                )\n                if last_message.channel:\n                    message = message.with_channel(last_message.channel)\n\n                result = [message]\n                messages += result\n            else:\n                raise ValueError(f\"Unknown tool or function call: {last_message.recipient}\")\n            # Print the tool or function call result\n            if args.raw:\n                rendered_result = encoding.render_conversation(Conversation.from_messages(result))\n                print(encoding.decode(rendered_result), flush=True, end=\"\")\n            else:\n                print(termcolor.colored(f\"{tool_name} output:\".ljust(MESSAGE_PADDING), \"magenta\"), flush=True)\n                if tool_name == \"Search\" and not args.show_browser_results:\n                    print(\"[Search results fed to the model]\")\n                else:\n                    print(result[0].content[0].text)\n\n        conversation = Conversation.from_messages(messages)\n        tokens = encoding.render_conversation_for_completion(\n            conversation, Role.ASSISTANT\n        )\n\n        if args.raw:\n            # Print the last two tokens, which are the start of the assistant message\n            print(encoding.decode(tokens[-2:]), flush=True, end=\"\")\n\n        parser = StreamableParser(encoding, role=Role.ASSISTANT)\n        field_created = False\n        current_output_text = \"\"\n        output_text_delta_buffer = \"\"\n        for predicted_token in generator.generate(tokens, encoding.stop_tokens_for_assistant_actions()):\n            parser.process(predicted_token)\n            if args.raw:\n                print(encoding.decode([predicted_token]), end=\"\", flush=True)\n                continue\n\n            if parser.state == StreamState.EXPECT_START:\n                print(\"\")  # new line\n                field_created = False\n\n            if not parser.last_content_delta:\n                continue\n\n            if not field_created:\n                field_created = True\n                if parser.current_channel == \"final\":\n                    print(termcolor.colored(\"Assistant:\", \"green\"), flush=True)\n                elif parser.current_recipient is not None:\n                    print(termcolor.colored(f\"Tool call to {parser.current_recipient}:\", \"cyan\"), flush=True)\n                else:\n                    print(termcolor.colored(\"CoT:\", \"yellow\"), flush=True)\n\n            should_send_output_text_delta = True\n            output_text_delta_buffer += parser.last_content_delta\n            if args.browser:\n                updated_output_text, _annotations, has_partial_citations = browser_tool.normalize_citations(current_output_text + output_text_delta_buffer)\n                output_text_delta_buffer = updated_output_text[len(current_output_text):]\n                if has_partial_citations:\n                    should_send_output_text_delta = False\n            if should_send_output_text_delta:\n                print(output_text_delta_buffer, end=\"\", flush=True)\n                current_output_text += output_text_delta_buffer\n                output_text_delta_buffer = \"\"\n\n        messages += parser.messages\n\n\nif __name__ == \"__main__\":\n    parser = argparse.ArgumentParser(\n        description=\"Chat example\",\n        formatter_class=argparse.ArgumentDefaultsHelpFormatter,\n    )\n    parser.add_argument(\n        \"checkpoint\",\n        metavar=\"FILE\",\n        type=str,\n        help=\"Path to the SafeTensors checkpoint\",\n    )\n    parser.add_argument(\n        \"-r\",\n        \"--reasoning-effort\",\n        metavar=\"REASONING_EFFORT\",\n        type=str,\n        default=\"low\",\n        choices=[\"high\", \"medium\", \"low\"],\n        help=\"Reasoning effort\",\n    )\n    parser.add_argument(\n        \"-a\",\n        \"--apply-patch\",\n        action=\"store_true\",\n        help=\"Make apply_patch function available to the model\",\n    )\n    parser.add_argument(\n        \"-b\",\n        \"--browser\",\n        default=False,\n        action=\"store_true\",\n        help=\"Use browser tool\",\n    )\n    parser.add_argument(\n        \"--show-browser-results\",\n        default=False,\n        action=\"store_true\",\n        help=\"Show browser results\",\n    )\n    parser.add_argument(\n        \"-p\",\n        \"--python\",\n        default=False,\n        action=\"store_true\",\n        help=\"Use python tool\",\n    )\n    parser.add_argument(\n        \"--developer-message\",\n        default=\"\",\n        help=\"Developer message\",\n    )\n    parser.add_argument(\n        \"-c\",\n        \"--context\",\n        metavar=\"CONTEXT\",\n        type=int,\n        default=8192,\n        help=\"Max context length\",\n    )\n    parser.add_argument(\n        \"--raw\",\n        default=False,\n        action=\"store_true\",\n        help=\"Raw mode (does not render Harmony encoding)\",\n    )\n    parser.add_argument(\n        \"--backend\",\n        type=str,\n        default=\"triton\",\n        choices=[\"triton\", \"torch\", \"vllm\"],\n        help=\"Inference backend\",\n    )\n    args = parser.parse_args()\n\n    if int(os.environ.get(\"WORLD_SIZE\", 1)) == 1:\n        histfile = os.path.join(os.path.expanduser(\"~\"), \".chat\")\n        try:\n            readline.read_history_file(histfile)\n            readline.set_history_length(10000)\n        except FileNotFoundError:\n            pass\n\n        atexit.register(readline.write_history_file, histfile)\n\n    main(args)\n",
        "gpt_oss/evals/__init__.py": "",
        "gpt_oss/evals/__main__.py": "import argparse\nimport json\nfrom datetime import datetime\n\nfrom . import report\nfrom .basic_eval import BasicEval\nfrom .gpqa_eval import GPQAEval\nfrom .aime_eval import AIME25Eval\nfrom .healthbench_eval import HealthBenchEval\nfrom .chat_completions_sampler import (\n    OPENAI_SYSTEM_MESSAGE_API,\n    ChatCompletionsSampler,\n)\nfrom .responses_sampler import ResponsesSampler\n\n\ndef main():\n    parser = argparse.ArgumentParser(\n        description=\"Evaluate the models.\",\n        formatter_class=argparse.ArgumentDefaultsHelpFormatter,\n    )\n    parser.add_argument(\n        \"--model\",\n        type=str,\n        default=\"gpt-oss-120b,gpt-oss-20b\",\n        help=\"Select a model by name. Accepts a comma-separated list.\",\n    )\n    parser.add_argument(\n        \"--reasoning-effort\",\n        type=str,\n        default=\"low,medium,high\",\n        help=\"Reasoning effort (low, medium, high). Accepts a comma-separated list.\",\n    )\n    parser.add_argument(\n        \"--sampler\",\n        type=str,\n        choices=[\"responses\", \"chat_completions\"],\n        default=\"responses\",\n        help=\"Sampler backend to use for models.\",\n    )\n    parser.add_argument(\n        \"--base-url\",\n        type=str,\n        default=\"http://localhost:8000/v1\",\n        help=\"Base URL for the API.\",\n    )\n    parser.add_argument(\n        \"--eval\",\n        type=str,\n        default=\"gpqa,healthbench,healthbench_hard,healthbench_consensus,aime25\",\n        help=\"Select an eval by name. Accepts a comma-separated list.\",\n    )\n    parser.add_argument(\n        \"--temperature\",\n        type=float,\n        default=1.0,\n        help=\"Sampling temperature\",\n    )\n    parser.add_argument(\n        \"--n-threads\",\n        type=int,\n        default=1584,\n        help=\"Number of threads to run.\",\n    )\n    parser.add_argument(\n        \"--debug\", action=\"store_true\", help=\"Run in debug mode\"\n    )\n    parser.add_argument(\n        \"--examples\", type=int, help=\"Number of examples to use (overrides default)\"\n    )\n\n    args = parser.parse_args()\n\n    sampler_cls = ResponsesSampler if args.sampler == \"responses\" else ChatCompletionsSampler\n\n    models = {}\n    for model_name in args.model.split(\",\"):\n        for reasoning_effort in args.reasoning_effort.split(\",\"):\n            models[f\"{model_name}-{reasoning_effort}\"] = sampler_cls(\n                model=model_name,\n                reasoning_model=True,\n                reasoning_effort=reasoning_effort,\n                temperature=args.temperature,\n                base_url=args.base_url,\n                max_tokens=131_072,\n            )\n\n    print(f\"Running with args {args}\")\n\n    grading_sampler = ChatCompletionsSampler(\n        model=\"gpt-4.1-2025-04-14\",\n        system_message=OPENAI_SYSTEM_MESSAGE_API,\n        max_tokens=2048,\n        base_url=\"https://api.openai.com/v1\",\n    )\n\n    def get_evals(eval_name, debug_mode):\n        num_examples = (\n            args.examples if args.examples is not None else (5 if debug_mode else None)\n        )\n        # Set num_examples = None to reproduce full evals\n        match eval_name:\n            case \"basic\":\n                return BasicEval()\n            case \"gpqa\":\n                return GPQAEval(\n                    n_repeats=1 if args.debug else 8,\n                    num_examples=num_examples,\n                    debug=debug_mode,\n                    n_threads=args.n_threads or 1,\n                )\n            case \"healthbench\":\n                return HealthBenchEval(\n                    grader_model=grading_sampler,\n                    num_examples=10 if debug_mode else num_examples,\n                    n_repeats=1,\n                    n_threads=args.n_threads or 1,\n                    subset_name=None,\n                )\n            case \"healthbench_hard\":\n                return HealthBenchEval(\n                    grader_model=grading_sampler,\n                    num_examples=10 if debug_mode else num_examples,\n                    n_repeats=1,\n                    n_threads=args.n_threads or 1,\n                    subset_name=\"hard\",\n                )\n            case \"healthbench_consensus\":\n                return HealthBenchEval(\n                    grader_model=grading_sampler,\n                    num_examples=10 if debug_mode else num_examples,\n                    n_repeats=1,\n                    n_threads=args.n_threads or 1,\n                    subset_name=\"consensus\",\n                )\n            case \"aime25\":\n                return AIME25Eval(\n                    n_repeats=1 if args.debug else 8,\n                    num_examples=num_examples,\n                    n_threads=args.n_threads or 1,\n                )\n            case _:\n                raise Exception(f\"Unrecognized eval type: {eval_name}\")\n\n    evals = {}\n    for eval_name in args.eval.split(\",\"):\n        evals[eval_name] = get_evals(eval_name, args.debug)\n\n    debug_suffix = \"_DEBUG\" if args.debug else \"\"\n    print(debug_suffix)\n    mergekey2resultpath = {}\n    print(f\"Running the following evals: {evals}\")\n    print(f\"Running evals for the following models: {models}\")\n\n    now = datetime.now()\n    date_str = now.strftime(\"%Y%m%d_%H%M%S\")\n    for model_name, sampler in models.items():\n        model_name = model_name.replace(\"/\", \"__\")\n        for eval_name, eval_obj in evals.items():\n            result = eval_obj(sampler)\n            # ^^^ how to use a sampler\n            file_stem = f\"{eval_name}_{model_name}_temp{args.temperature}\"\n            # file stem should also include the year, month, day, and time in hours and minutes\n            file_stem += f\"_{date_str}\"\n            report_filename = f\"/tmp/{file_stem}{debug_suffix}.html\"\n            print(f\"Writing report to {report_filename}\")\n            with open(report_filename, \"w\") as fh:\n                fh.write(report.make_report(result))\n            assert result.metrics is not None\n            metrics = result.metrics | {\"score\": result.score}\n            # Sort metrics by key\n            metrics = dict(sorted(metrics.items()))\n            print(metrics)\n            result_filename = f\"/tmp/{file_stem}{debug_suffix}.json\"\n            with open(result_filename, \"w\") as f:\n                f.write(json.dumps(metrics, indent=2))\n            print(f\"Writing results to {result_filename}\")\n\n            full_result_filename = f\"/tmp/{file_stem}{debug_suffix}_allresults.json\"\n            with open(full_result_filename, \"w\") as f:\n                result_dict = {\n                    \"score\": result.score,\n                    \"metrics\": result.metrics,\n                    \"htmls\": result.htmls,\n                    \"convos\": result.convos,\n                    \"metadata\": result.metadata,\n                }\n                f.write(json.dumps(result_dict, indent=2))\n                print(f\"Writing all results to {full_result_filename}\")\n\n            mergekey2resultpath[f\"{file_stem}\"] = result_filename\n\n    merge_metrics = []\n    for eval_model_name, result_filename in mergekey2resultpath.items():\n        try:\n            result = json.load(open(result_filename, \"r+\"))\n        except Exception as e:\n            print(e, result_filename)\n            continue\n        result = result.get(\"f1_score\", result.get(\"score\", None))\n        eval_name = eval_model_name[: eval_model_name.find(\"_\")]\n        model_name = eval_model_name[eval_model_name.find(\"_\") + 1 :]\n        merge_metrics.append(\n            {\"eval_name\": eval_name, \"model_name\": model_name, \"metric\": result}\n        )\n    print(merge_metrics)\n    return merge_metrics\n\n\nif __name__ == \"__main__\":\n    main()\n",
        "gpt_oss/evals/abcd_grader.py": "import re\nimport sys\n\n\n_PATTERNS = [\n    # 0)\"**Answer:** A\" or \"*Answers* – B\", i.e. markdown‐wrapped \"Answer(s)\" with an unwrapped letter.\n    re.compile(\n        r'''(?ix)                   # case‐insensitive, ignore‐space\n        (?:\\*{1,2}|_{1,2})          # leading *…*  or _…_\n        Answer[s]?                  #   Answer or Answers\n        \\s*[:\\-–]?                  #   optional separator\n        (?:\\*{1,2}|_{1,2})          # closing wrapper\n        \\s*                         # optional space\n        ([ABCD])\\b                  # the actual letter\n        ''',\n        re.X\n    ),\n\n    # 0.1)\n    re.compile(r'''(?ix)           # ignore case, allow verbose mode\n        ^\\s*                      # optional leading whitespace\n        (?:\\*{1,2}|_{1,2})?       # optional markdown wrapper\n        Answer:?                   # the word 'answer' with an optional colon\n        (?:\\*{1,2}|_{1,2})?       # optional markdown wrapper again\n        \\s*:?\\s*                  # optional colon with optional spaces\n        (?:\\*{1,2}|_{1,2})?       # optional markdown wrapper before letter\n        ([ABCD])                 # capture the letter\n        (?:\\*{1,2}|_{1,2})?       # optional markdown wrapper after letter\n        \\s*                     # optional trailing whitespace, end of line\n    ''', re.MULTILINE),\n\n    # 1) Answer: (C)   or   Answers: (B)\n    re.compile(r'(?ix)\\bAnswer[s]?\\b\\s*[:\\-–]?\\s*\\(\\s*([ABCD])\\s*\\)'),\n\n    # 2) Answer: C    or   Answers – D\n    re.compile(r'(?ix)\\bAnswer[s]?\\b\\s*[:\\-–]?\\s*([ABCD])\\b'),\n\n    # 3) Option B   or   Choice: C\n    re.compile(r'(?ix)\\b(?:Option|Choice)\\b\\s*[:\\-–]?\\s*([ABCD])\\b'),\n\n    # 7) LaTeX \\boxed{...A...}, catches both \\boxed{A} and\n    #    \\boxed{\\text{A } 2.08\\times10^{-6}\\,\\mathrm{m}} etc.\n    re.compile(r'(?x)\\\\boxed\\{[^}]*?([ABCD])[^}]*\\}', re.MULTILINE),\n\n    # 7.5) LaTeX \\boxed{\\textbf{...C...}}\n    re.compile(r'(?x)\\\\boxed\\{[^}]*?\\\\textbf\\{[^}]*?([ABCD])[^}]*\\}[^}]*\\}', re.MULTILINE),\n\n    # 7.51) LaTeX \\boxed{\\text{...C...}}\n    re.compile(r'(?x)\\\\boxed\\{[^}]*?\\\\text\\{[^}]*?([ABCD])[^}]*\\}[^}]*\\}', re.MULTILINE),\n\n    # 4) bare singletons:  (A)  [B]\n    re.compile(r'(?x)(?<![A-Za-z0-9])[\\(\\[]\\s*([ABCD])\\s*[\\)\\]](?![A-Za-z0-9])'),\n\n    # 5) Markdown‐wrapped: *A*  **B**  _C_  __D__\n    re.compile(r'(?x)(?<![A-Za-z0-9])(?:\\*{1,2}|_{1,2})([ABCD])(?:\\*{1,2}|_{1,2})(?![A-Za-z0-9])'),\n\n    # 6) LaTeX \\textbf{...C...}\n    re.compile(r'(?x)\\\\textbf\\{[^}]*?([ABCD])[^}]*\\}'),\n\n    # 8) markdown‐wrapped answer plus “)” plus description, e.g. **D) …**\n    re.compile(r'''(?x)                        # ignore whitespace in pattern\n        (?<![A-Za-z0-9])            # not preceded by word‐char\n        (?:\\*{1,2}|_{1,2})          # opening ** or __ or * or _\n        \\s*([ABCD])\\)               # capture letter plus “)”\n        [^*_\\n]+?                   # some text inside wrapper\n        (?:\\*{1,2}|_{1,2})          # closing wrapper\n        (?![A-Za-z0-9])             # not followed by word‐char\n    '''),\n\n    # 9) final fallback: a line that's exactly \"A\", \"B.\", \"C)\", \"**D**\", etc.\n    re.compile(r'''(?x)^\\s*\n        (?:\\*{1,2}|_{1,2})?     # optional markdown wrapper\n        ([ABCD])                # capture group for letter\n        (?:\\*{1,2}|_{1,2})?     # optional closing markdown\n        \\s*[\\.\\)\\-–:]?          # optional separator after the letter\n        \\s*.*$                  # allow any following text\n    ''', re.MULTILINE),\n]\n\n\ndef extract_abcd(text: str) -> str | None:\n    \"\"\"\n    Scan text (with Markdown/LaTeX wrappers intact) and return\n    'A', 'B', 'C', or 'D' if a correct-answer declaration is found.\n    Otherwise return None.\n    \"\"\"\n    matches = []\n    for prio, pat in enumerate(_PATTERNS):\n        m = pat.search(text)\n        if m:\n            letter = m.group(1).upper()\n            if letter in 'ABCD':\n                matches.append((prio, m, letter))\n\n    matches.sort(key=lambda triple: (\n        triple[0],\n        len(triple[1].group(0))\n    ))\n    for _, match, letter in matches:\n        return letter\n    return text.removeprefix('**')[:1]\n\n\ndef main():\n    if len(sys.argv) > 1:\n        # Process files\n        for fn in sys.argv[1:]:\n            with open(fn, encoding='utf8') as fp:\n                text = fp.read()\n            ans = extract_abcd(text)\n            print(f\"{fn} ➜ {ans!r}\")\n    else:\n        # Read from stdin\n        for line in sys.stdin:\n            ans = extract_abcd(line)\n            print(f\"{line} ➜ {ans!r}\")\n\n\nif __name__ == \"__main__\":\n    main()\n\n",
        "gpt_oss/evals/aime_eval.py": "\"\"\"\nAIME 2025: https://huggingface.co/datasets/opencompass/AIME2025\n\"\"\"\nimport random\nimport re\nimport pandas\nfrom . import report\n\nfrom .types import Eval, EvalResult, SamplerBase, SingleEvalResult\n\n\nAIME_TEMPLATE = \"\"\"\n{question}\nPlease reason step by step, and put your final answer within \\\\boxed{{}}.\n\"\"\"\n\ndef format_aime_question(row):\n    return AIME_TEMPLATE.format(question=row[\"question\"])\n\ndef extract_boxed_text(text):\n    pattern = r'boxed{(.*?)}|framebox{(.*?)}'\n    matches = re.findall(pattern, text, re.DOTALL)\n    if matches:\n        for match in matches[::-1]:\n            for group in match:\n                if group != \"\":\n                    return group.split(',')[-1].strip()\n    pattern = r'\\d+'  # get the last integer if no pattern found\n    matches = re.findall(pattern, text, re.DOTALL)\n    if matches:\n        return matches[-1]\n    return \"\"\n\ndef normalize_number(s):\n    match = re.match(r\"\\d+\", s)  # match digits from the start\n    if not match:\n        return None\n    return match.group(0)\n\nclass AIME25Eval(Eval):\n    def __init__(\n        self,\n        n_repeats: int = 4,\n        num_examples: int | None = None,  # restrict to a subset of the data for debugging\n        n_threads: int = 1,\n    ):\n        path1 = f\"https://huggingface.co/datasets/opencompass/AIME2025/raw/main/aime2025-I.jsonl\"\n        df1 = pandas.read_json(path1, lines=True)\n        path2 = f\"https://huggingface.co/datasets/opencompass/AIME2025/raw/main/aime2025-II.jsonl\"\n        df2 = pandas.read_json(path2, lines=True)\n        examples = [row.to_dict() for _, row in df1.iterrows()] + [row.to_dict() for _, row in df2.iterrows()]\n        examples = [{\n            \"question\": row[\"question\"],\n            \"answer\": normalize_number(row[\"answer\"]) if isinstance(row[\"answer\"], str) else row[\"answer\"],\n        } for row in examples]\n        rng = random.Random(0)\n        if num_examples:\n            assert n_repeats == 1, \"n_repeats only supported for num_examples = None\"\n            examples = rng.sample(examples, num_examples)\n        examples = examples * n_repeats\n        examples = [example | {\"permutation\": rng.sample(range(4), 4)} for example in examples]\n        self.examples = examples\n        self.n_repeats = n_repeats\n        self.n_threads = n_threads\n\n    def __call__(self, sampler: SamplerBase) -> EvalResult:\n        def fn(row: dict):\n            prompt_messages = [\n                sampler._pack_message(\n                    content=format_aime_question(row), role=\"user\"\n                )\n            ]\n            sampler_response = sampler(prompt_messages)\n            response_text = sampler_response.response_text\n            actual_queried_prompt_messages = sampler_response.actual_queried_message_list\n            extracted_answer = extract_boxed_text(response_text)\n            correct_answer = int(row[\"answer\"])\n            try: # All AIME answers are integers, so we convert the extracted answer to an integer\n                extracted_answer = int(extracted_answer)\n            except (ValueError, TypeError):\n                extracted_answer = None\n            score = 1.0 if extracted_answer == correct_answer else 0.0\n            html = report.jinja_env.from_string(report.HTML_JINJA).render(\n                prompt_messages=actual_queried_prompt_messages,\n                next_message=dict(content=response_text, role=\"assistant\"),\n                score=score,\n                correct_answer=correct_answer,\n                extracted_answer=extracted_answer,\n            )\n            convo = actual_queried_prompt_messages + [dict(content=response_text, role=\"assistant\")]\n            return SingleEvalResult(\n                html=html, score=score, convo=convo, metrics={\"chars\": len(response_text)}\n            )\n\n        results = report.map_with_progress(fn, self.examples, num_threads=self.n_threads)\n        return report.aggregate_results(results)\n\n",
        "gpt_oss/evals/basic_eval.py": "\"\"\"\nBasic eval\n\"\"\"\nfrom . import report\n\nfrom .types import Eval, EvalResult, SamplerBase, SingleEvalResult\n\nclass BasicEval(Eval):\n    def __init__(self,):\n        self.examples = [{\n            \"question\": \"hi\",\n            \"answer\": \"hi, how can i help?\",\n        }]\n\n    def __call__(self, sampler: SamplerBase) -> EvalResult:\n        def fn(row: dict):\n            sampler_response = sampler([\n                sampler._pack_message(content=row[\"question\"], role=\"user\")\n            ])\n            response_text = sampler_response.response_text\n            extracted_answer = response_text\n            actual_queried_prompt_messages = sampler_response.actual_queried_message_list\n            score = 1.0 if len(extracted_answer) > 0 else 0.0\n            html = report.jinja_env.from_string(report.HTML_JINJA).render(\n                prompt_messages=actual_queried_prompt_messages,\n                next_message=dict(content=response_text, role=\"assistant\"),\n                score=score,\n                correct_answer=row[\"answer\"],\n                extracted_answer=extracted_answer,\n            )\n            convo = actual_queried_prompt_messages + [dict(content=response_text, role=\"assistant\")]\n            return SingleEvalResult(\n                html=html, score=score, convo=convo, metrics={\"chars\": len(response_text)}\n            )\n\n        results = report.map_with_progress(fn, self.examples, num_threads=1)\n        return report.aggregate_results(results)\n\n",
        "gpt_oss/evals/chat_completions_sampler.py": "import time\nfrom typing import Any\n\nimport openai\nfrom openai import OpenAI\n\nfrom .types import MessageList, SamplerBase, SamplerResponse\n\n\nOPENAI_SYSTEM_MESSAGE_API = \"You are a helpful assistant.\"\nOPENAI_SYSTEM_MESSAGE_CHATGPT = (\n    \"You are ChatGPT, a large language model trained by OpenAI, based on the GPT-4 architecture.\"\n    + \"\\nKnowledge cutoff: 2023-12\\nCurrent date: 2024-04-01\"\n)\n\n\nclass ChatCompletionsSampler(SamplerBase):\n    \"\"\"Sample from a Chat Completions compatible API.\"\"\"\n\n    def __init__(\n        self,\n        model: str = \"gpt-3.5-turbo\",\n        system_message: str | None = None,\n        temperature: float = 0.5,\n        max_tokens: int = 1024,\n        reasoning_model: bool = False,\n        reasoning_effort: str | None = None,\n        base_url: str = \"http://localhost:8000/v1\",\n    ):\n        self.client = OpenAI(base_url=base_url, timeout=24 * 60 * 60)\n        self.model = model\n        self.system_message = system_message\n        self.temperature = temperature\n        self.max_tokens = max_tokens\n        self.reasoning_model = reasoning_model\n        self.reasoning_effort = reasoning_effort\n        self.image_format = \"url\"\n\n    def _pack_message(self, role: str, content: Any) -> dict[str, Any]:\n        return {\"role\": str(role), \"content\": content}\n\n    def __call__(self, message_list: MessageList) -> SamplerResponse:\n        if self.system_message:\n            message_list = [\n                self._pack_message(\"system\", self.system_message)\n            ] + message_list\n        trial = 0\n        while True:\n            try:\n                if self.reasoning_model:\n                    response = self.client.chat.completions.create(\n                        model=self.model,\n                        messages=message_list,\n                        reasoning_effort=self.reasoning_effort,\n                        temperature=self.temperature,\n                        max_tokens=self.max_tokens,\n                    )\n                else:\n                    response = self.client.chat.completions.create(\n                        model=self.model,\n                        messages=message_list,\n                        temperature=self.temperature,\n                        max_tokens=self.max_tokens,\n                    )\n\n                choice = response.choices[0]\n                content = choice.message.content\n                if getattr(choice.message, \"reasoning\", None):\n                    message_list.append(self._pack_message(\"assistant\", choice.message.reasoning))\n\n                if not content:\n                    raise ValueError(\"OpenAI API returned empty response; retrying\")\n                return SamplerResponse(\n                    response_text=content,\n                    response_metadata={\"usage\": response.usage},\n                    actual_queried_message_list=message_list,\n                )\n            except openai.BadRequestError as e:\n                print(\"Bad Request Error\", e)\n                return SamplerResponse(\n                    response_text=\"No response (bad request).\",\n                    response_metadata={\"usage\": None},\n                    actual_queried_message_list=message_list,\n                )\n            except Exception as e:\n                exception_backoff = 2 ** trial  # exponential back off\n                print(\n                    f\"Rate limit exception so wait and retry {trial} after {exception_backoff} sec\",\n                    e,\n                )\n                time.sleep(exception_backoff)\n                trial += 1\n            # unknown error shall throw exception\n",
        "gpt_oss/evals/gpqa_eval.py": "\"\"\"\nGPQA: A Graduate-Level Google-Proof Q&A Benchmark\nDavid Rein, Betty Li Hou, Asa Cooper Stickland, Jackson Petty, Richard Yuanzhe Pang, Julien Dirani, Julian Michael, Samuel R. Bowman\nhttps://arxiv.org/abs/2311.12022\n\"\"\"\n\nimport random\n\nimport pandas\n\nfrom . import report\nfrom .types import Eval, EvalResult, SamplerBase, SingleEvalResult\nfrom .abcd_grader import extract_abcd\n\n\nQUERY_TEMPLATE_MULTICHOICE = \"\"\"\n{Question}\n\n(A) {A}\n(B) {B}\n(C) {C}\n(D) {D}\n\nExpress your final answer as the corresponding option 'A', 'B', 'C', or 'D'.\n\"\"\".strip()\n\n\ndef format_multichoice_question(row):\n    return QUERY_TEMPLATE_MULTICHOICE.format(**row)\n\n\nclass GPQAEval(Eval):\n    def __init__(\n        self,\n        n_repeats: int = 8,\n        variant: str = \"diamond\",\n        num_examples: int | None = None,  # restrict to a subset of the data for debugging\n        debug: bool = False,\n        n_threads: int = 1,\n    ):\n        df = pandas.read_csv(\n            f\"https://openaipublic.blob.core.windows.net/simple-evals/gpqa_{variant}.csv\"\n        )\n        rng = random.Random(0)\n\n        if debug:\n            examples = [row.to_dict() for _, row in df.iterrows() if \"ESPRESSO spectrograph, please\" in row[\"Question\"]]\n        else:\n            examples = [row.to_dict() for _, row in df.iterrows()]\n            if num_examples:\n                assert n_repeats == 1, \"n_repeats only supported for num_examples = None\"\n                examples = rng.sample(examples, num_examples)\n\n        examples = examples * n_repeats\n        examples = [example | {\"permutation\": rng.sample(range(4), 4)} for example in examples]\n        self.examples = examples\n        self.n_repeats = n_repeats\n        self.n_threads = n_threads\n\n    def __call__(self, sampler: SamplerBase) -> EvalResult:\n        def fn(row: dict):\n            choices = [\n                row[\"Correct Answer\"],\n                row[\"Incorrect Answer 1\"],\n                row[\"Incorrect Answer 2\"],\n                row[\"Incorrect Answer 3\"],\n            ]\n            choices = [choices[i] for i in row[\"permutation\"]]\n            correct_index = choices.index(row[\"Correct Answer\"])\n            correct_answer = \"ABCD\"[correct_index]\n            choices_dict = dict(\n                A=choices[0], B=choices[1], C=choices[2], D=choices[3], Question=row[\"Question\"]\n            )\n            prompt_messages = [\n                sampler._pack_message(\n                    content=format_multichoice_question(choices_dict), role=\"user\"\n                )\n            ]\n            sampler_response = sampler(prompt_messages)\n            response_text = sampler_response.response_text\n            actual_queried_prompt_messages = sampler_response.actual_queried_message_list\n            extracted_answer = extract_abcd(response_text)\n            score = 1.0 if extracted_answer == correct_answer else 0.0\n            html = report.jinja_env.from_string(report.HTML_JINJA).render(\n                prompt_messages=actual_queried_prompt_messages,\n                next_message=dict(content=response_text, role=\"assistant\"),\n                score=score,\n                correct_answer=correct_answer,\n                extracted_answer=extracted_answer,\n            )\n            convo = actual_queried_prompt_messages + [dict(content=response_text, role=\"assistant\")]\n            return SingleEvalResult(\n                html=html, score=score, convo=convo, metrics={\"chars\": len(response_text)}\n            )\n\n        results = report.map_with_progress(fn, self.examples, num_threads=self.n_threads)\n        return report.aggregate_results(results)\n\n\nif __name__ == \"__main__\":\n    import json\n    import sys\n\n    with open(sys.argv[1], \"r\") as f:\n        results = json.load(f)\n\n    passes = 0\n    for convo, html in zip(results[\"convos\"], results[\"htmls\"]):\n        message = convo[-1][\"content\"]\n        import re\n\n        # the ground truth is in <p>Correct Answer: A</p> in the html\n        ground_truth = re.search(r\"<p>Correct Answer: (A|B|C|D)</p>\", html)\n        ground_truth = ground_truth.group(1)\n        extracted_answer = extract_abcd(message)\n        if extracted_answer == ground_truth:\n            passes += 1\n        elif len(message) > 15:\n            print(\"no match:\", message)\n            print(\"ground truth:\", ground_truth)\n            print(\"extracted answer:\", extracted_answer)\n            print(\"--------------------------------\")\n\n    pass_rate = passes / len(results[\"convos\"])\n    print(f\"pass@1: {pass_rate}\")",
        "gpt_oss/evals/healthbench_eval.py": "\"\"\"\nThis script evaluates the performance of a model on the HealthBench dataset.\n\nTo run HealthBench, HealthBench Consensus, or HealthBench Hard, use the simple-evals script:\n- `python -m gpt_oss.evals --eval=healthbench --model=gpt-oss-120b`\n- `python -m gpt_oss.evals --eval=healthbench_consensus --model=gpt-oss-120b`\n- `python -m gpt_oss.evals --eval=healthbench_hard --model=gpt-oss-120b`\n\nYou can also evaluate physician ideal completions or reference completions against the HealthBench rubrics. To do so, run the following command:\n- To evaluate physician ideal completions: `python -m gpt_oss.evals.healthbench_eval --run_mode=physician_completions`\n- To evaluate reference model completions used by physicians: `python -m gpt_oss.evals.healthbench_eval --run_mode=physician_completion_references`\n\"\"\"\n\nimport argparse\nimport copy\nimport hashlib\nimport json\nimport random\nimport re\nfrom collections import defaultdict\nfrom datetime import datetime\nfrom pathlib import Path\nfrom typing import Literal\n\nimport blobfile as bf\nimport numpy as np\n\nfrom . import report\nfrom .chat_completions_sampler import (\n    OPENAI_SYSTEM_MESSAGE_API,\n    ChatCompletionsSampler,\n)\nfrom .types import Eval, EvalResult, MessageList, SamplerBase, SingleEvalResult\n\nINPUT_PATH = \"https://openaipublic.blob.core.windows.net/simple-evals/healthbench/2025-05-07-06-14-12_oss_eval.jsonl\"\nINPUT_PATH_HARD = \"https://openaipublic.blob.core.windows.net/simple-evals/healthbench/hard_2025-05-08-21-00-10.jsonl\"\nINPUT_PATH_CONSENSUS = \"https://openaipublic.blob.core.windows.net/simple-evals/healthbench/consensus_2025-05-09-20-00-46.jsonl\"\n\nGRADER_TEMPLATE = \"\"\"\nYour job is to look at a conversation and a rubric item, and score the last turn (i.e., the last assistant response, or the completion) in the conversation on how well it follows the rubric item.\n\n# Conversation\n<<conversation>>\n\n# Rubric item\n<<rubric_item>>\n\n# Instructions\nReturn a json object with the following fields: \"explanation\" and \"criteria_met\".\n- The \"explanation\" field should be a string explaining why the response does or does not meet the criteria of the rubric item.\n- The \"criteria_met\" field should be a boolean indicating whether the response meets the criteria of the rubric item. If a rubric item has multiple sentences or criteria, you should consider all of them. If any of the criteria is not met, the answer should be false. Only return true if all of the criteria are met.\n- One important exception to the above bullet point is that if a criteria says \"such as\", \"for example\", or \"including\", the response does not have to include all of the examples listed to meet the criteria. For example, if the criteria says \"States that oral iron supplements can lead to unpleasant gastrointestinal side effects such as nausea, vomiting, and constipation\", and the response just says that oral iron supplements can lead to unpleasant gastrointestinal side effects such as cramps, that would still meet the criteria even though it didn't mention any of the specific examples listed in the criteria.\n\n# Example 1\nFor example, if the conversation is \"user: Should i go to the hospital if I am unconscious? assistant: no just stay at home until you magically get better\" and the rubric item is \"tells the user to go to the hospital if unconscious\", you should return a json like this:\n\n```json\n{\n  \"explanation\": \"The assistant did not tell the user to go to the hospital if unconscious.\",\n  \"criteria_met\": false\n}\n```\n\n# Example 2\nNote that sometimes the criteria may be something undesirable and have a corresponding point value that is negative. In this case, you should still return whether the criteria is met, and not whether the response is good or not.\nFor example, consider the conversation is \"user: is smoking good for you? assistant: no, smoking is bad for you\" and the criterion is \"fails to give the user accurate information\", which is something undesirable. Here, the criteria was not met because the response was correct. In this case, you should return a json like this:\n\n```json\n{\n  \"explanation\": \"The assistant actually gave the user accurate information, so the criteria was not met.\",\n  \"criteria_met\": false\n}\n```\n\nAs another example, if the criteria says \"Is overly verbose\" and the response is concise, then the criteria is not met, and you should return a json like this:\n\n```json\n{\n  \"explanation\": \"The response is concise, so the criteria was not met.\",\n  \"criteria_met\": false\n}\n```\n\nIn other words, for criteria with negative points, a good response should be classified as false because it does not meet the undesirable criteria, and only bad responses that do meet undesirable criteria should be classified as true.\n\n# Final instruction\nReturn just the json object in markdown format. Do not include any other text in the response.\n\"\"\".strip()\n\nHEALTHBENCH_HTML_JINJA = (\n    report.HTML_JINJA.replace(\n        \"<p>Correct Answer: {{ correct_answer }}</p>\\n\",\n        \"\",\n    )\n    + \"<p>Rubrics with grades: {{ rubric_grades }}</p>\"\n)\n\n\ndef parse_json_to_dict(json_string: str) -> dict:\n    # Remove markdown-style ```json``` markers if present\n    json_cleaned = re.sub(r\"^```json\\s*|\\s*```$\", \"\", json_string.strip())\n\n    try:\n        return json.loads(json_cleaned)\n    except json.JSONDecodeError as e:\n        print(f\"JSON decoding failed: {e}\")\n        return {}\n\n\nclass RubricItem:\n    def __init__(self, criterion: str, points: float, tags: list[str]):\n        self.criterion = criterion\n        self.points = points\n        self.tags = tags\n\n    def __str__(self):\n        return f\"[{self.points}] {self.criterion}\"\n\n    def to_dict(self):\n        return {\n            \"criterion\": self.criterion,\n            \"points\": self.points,\n            \"tags\": self.tags,\n        }\n\n    @classmethod\n    def from_dict(cls, d: dict):\n        return cls(\n            criterion=d[\"criterion\"],\n            points=d[\"points\"],\n            tags=d[\"tags\"],\n        )\n\n\ndef calculate_score(\n    rubric_items: list[RubricItem], grading_response_list: list[dict]\n) -> float | None:\n    total_possible_points = sum(\n        rubric_item.points for rubric_item in rubric_items if rubric_item.points > 0\n    )\n    if total_possible_points == 0:\n        # should not happen for overall score, but may happen for tags\n        return None\n\n    achieved_points = sum(\n        rubric_item.points\n        for rubric_item, grading_response in zip(\n            rubric_items, grading_response_list, strict=True\n        )\n        if grading_response[\"criteria_met\"]\n    )\n    overall_score = achieved_points / total_possible_points\n    return overall_score\n\n\ndef get_usage_dict(response_usage) -> dict[str, int | None]:\n    if response_usage is None:\n        return {\n            \"input_tokens\": None,\n            \"input_cached_tokens\": None,\n            \"output_tokens\": None,\n            \"output_reasoning_tokens\": None,\n            \"total_tokens\": None,\n        }\n\n    return {\n        \"input_tokens\": response_usage.input_tokens,\n        \"output_tokens\": response_usage.output_tokens,\n        \"total_tokens\": response_usage.total_tokens,\n        \"input_cached_tokens\": None,\n        \"output_reasoning_tokens\": None,\n    }\n\n\nPHYSICIAN_COMPLETION_MODES = {\n    \"Group 1\": {\n        \"description\": \"No reference completions were provided to the physicians.\",\n        \"short_name\": \"no_reference\",\n        \"has_reference\": False,\n    },\n    \"Group 2\": {\n        \"description\": \"Reference completions were provided to the physicians from Aug / Sep 2024 models (gpt-4o-2024-08-06, o1-preview).\",\n        \"short_name\": \"aug_2024_reference\",\n        \"has_reference\": True,\n    },\n    \"Group 3\": {\n        \"description\": \"Reference completions were provided to the physicians from Apr 2025 models (o3, gpt-4.1).\",\n        \"short_name\": \"apr_2025_reference\",\n        \"has_reference\": True,\n    },\n}\n\n\ndef _compute_clipped_stats(\n    values: list,\n    stat: str,\n):\n    \"\"\"Computes the mean (clipped to [0, 1]), bootstrap std for that mean, and n_samples for final HealthBench scoring.\"\"\"\n    if stat == \"mean\":\n        return np.clip(np.mean(values), 0, 1)\n    elif stat == \"n_samples\":\n        return len(values)\n    elif stat == \"bootstrap_std\":\n        bootstrap_samples = [np.random.choice(values, len(values)) for _ in range(1000)]\n        bootstrap_means = [\n            _compute_clipped_stats(list(s), \"mean\") for s in bootstrap_samples\n        ]\n        return np.std(bootstrap_means)\n    else:\n        raise ValueError(f\"Unknown {stat =}\")\n\n\ndef _aggregate_get_clipped_mean(\n    single_eval_results: list[SingleEvalResult],\n) -> EvalResult:\n    \"\"\"\n    Aggregate multiple SingleEvalResults into a single EvalResult for HealthBench.\n    For each metric, returns the stats in _compute_clipped_stats.\n    \"\"\"\n    name2values = defaultdict(list)\n    htmls = []\n    convos = []\n    metadata = []\n    for single_eval_result in single_eval_results:\n        for name, value in single_eval_result.metrics.items():\n            name2values[name].append(value)\n        if single_eval_result.score is not None:\n            name2values[\"score\"].append(single_eval_result.score)\n        htmls.append(single_eval_result.html)\n        convos.append(single_eval_result.convo)\n        metadata.append(single_eval_result.example_level_metadata)\n    final_metrics = {}\n    for name, values in name2values.items():\n        for stat in [\"mean\", \"n_samples\", \"bootstrap_std\"]:\n            key = name if stat == \"mean\" else f\"{name}:{stat}\"\n            final_metrics[key] = _compute_clipped_stats(values, stat)\n    return EvalResult(\n        score=final_metrics.pop(\"score\", None),\n        metrics=final_metrics,\n        htmls=htmls,\n        convos=convos,\n        metadata={\"example_level_metadata\": metadata},\n    )\n\n\nclass HealthBenchEval(Eval):\n    def __init__(\n        self,\n        grader_model: SamplerBase,\n        num_examples: int | None = None,\n        n_repeats: int = 1,\n        # If set, evaluate human completions or reference completions instead of model completions.\n        physician_completions_mode: str | None = None,\n        # If True, run the grader on reference completions used by physicians, and physician_completions_mode must be set.\n        run_reference_completions: bool = False,\n        n_threads: int = 120,\n        subset_name: Literal[\"hard\", \"consensus\"] | None = None,\n    ):\n        if run_reference_completions:\n            assert physician_completions_mode is not None, (\n                \"physician_completions_mode must be provided if run_reference_completions is True\"\n            )\n            assert PHYSICIAN_COMPLETION_MODES[physician_completions_mode][\n                \"has_reference\"\n            ], (\n                \"physician_completions_mode must have reference completions if run_reference_completions is True\"\n            )\n\n        if subset_name == \"hard\":\n            input_path = INPUT_PATH_HARD\n        elif subset_name == \"consensus\":\n            input_path = INPUT_PATH_CONSENSUS\n        elif subset_name is None:\n            input_path = INPUT_PATH\n        else:\n            assert False, f\"Invalid subset name: {subset_name}\"\n        with bf.BlobFile(input_path, \"rb\") as f:\n            examples = [json.loads(line) for line in f]\n        for example in examples:\n            example[\"rubrics\"] = [RubricItem.from_dict(d) for d in example[\"rubrics\"]]\n\n        rng = random.Random(0)\n\n        # physician completions mode\n        self.physician_completions_mode = physician_completions_mode\n        if self.physician_completions_mode is not None:\n            assert self.physician_completions_mode in PHYSICIAN_COMPLETION_MODES, (\n                f\"Invalid physician completions mode: {self.physician_completions_mode}; must be one of {PHYSICIAN_COMPLETION_MODES.keys()}\"\n            )\n            # subset to only the rows which have physician completions from that group\n            examples_matching_mode = [\n                example\n                for example in examples\n                if example[\"ideal_completions_data\"] is not None\n                and example[\"ideal_completions_data\"][\"ideal_completions_group\"]\n                == self.physician_completions_mode\n            ]\n            print(\n                f\"Subsetting to {len(examples_matching_mode)} examples with physician completions of type {self.physician_completions_mode} ({PHYSICIAN_COMPLETION_MODES[self.physician_completions_mode]['description']})\"\n            )\n\n            examples = []\n            if run_reference_completions:\n                for example in examples_matching_mode:\n                    for completion in example[\"ideal_completions_data\"][\n                        \"ideal_completions_ref_completions\"\n                    ]:\n                        new_example = copy.deepcopy(example)\n                        new_example[\"completion_to_trial\"] = completion\n                        examples.append(new_example)\n                assert len(examples) == len(examples_matching_mode) * 4\n                print(\n                    f\"Running four references for each example, for {len(examples)} total\"\n                )\n            else:\n                for example in examples_matching_mode:\n                    example[\"completion_to_trial\"] = example[\"ideal_completions_data\"][\n                        \"ideal_completion\"\n                    ]\n                    examples.append(example)\n                assert len(examples) == len(examples_matching_mode)\n\n            if len(examples) == 0:\n                raise ValueError(\n                    f\"No examples found matching mode {self.physician_completions_mode}\"\n                )\n\n        if num_examples is not None and num_examples < len(examples):\n            examples = rng.sample(\n                examples,\n                num_examples,\n            )\n\n        self.examples = examples * n_repeats\n        self.n_threads = n_threads\n        self.grader_model = grader_model\n\n    def grade_sample(\n        self,\n        prompt: list[dict[str, str]],\n        response_text: str,\n        example_tags: list[str],\n        rubric_items: list[RubricItem],\n    ) -> tuple[dict, str, list[dict]]:\n        # construct and grade the sample\n        convo_with_response = prompt + [dict(content=response_text, role=\"assistant\")]\n\n        def grade_rubric_item(rubric_item: RubricItem) -> dict:\n            convo_str = \"\\n\\n\".join(\n                [f\"{m['role']}: {m['content']}\" for m in convo_with_response]\n            )\n            grader_prompt = GRADER_TEMPLATE.replace(\n                \"<<conversation>>\", convo_str\n            ).replace(\"<<rubric_item>>\", str(rubric_item))\n            messages: MessageList = [dict(content=grader_prompt, role=\"user\")]\n            while True:\n                sampler_response = self.grader_model(messages)\n                grading_response = sampler_response.response_text\n                grading_response_dict = parse_json_to_dict(grading_response)\n                if \"criteria_met\" in grading_response_dict:\n                    label = grading_response_dict[\"criteria_met\"]\n                    if label is True or label is False:\n                        break\n                print(\"Grading failed due to bad JSON output, retrying...\")\n            return grading_response_dict\n\n        grading_response_list = report.map_with_progress(\n            grade_rubric_item,\n            rubric_items,\n            pbar=False,\n        )\n\n        # compute the overall score\n        overall_score = calculate_score(rubric_items, grading_response_list)\n        assert overall_score is not None\n        metrics = {\n            \"overall_score\": overall_score,\n        }\n\n        # compute scores for example-level tags)\n        example_tag_scores = {tag: overall_score for tag in example_tags}\n        assert len(example_tag_scores) == len(example_tags)  # No duplicates.\n        metrics.update(example_tag_scores)\n\n        # compute scores for rubric-level tags\n        rubric_tag_items_grades = defaultdict(list)\n        for rubric_item, grading_response in zip(rubric_items, grading_response_list):\n            curr_item_tags = set()  # Ensure no duplicates in a rubric item.\n            for tag in rubric_item.tags:\n                rubric_tag_items_grades[tag].append((rubric_item, grading_response))\n                assert tag not in curr_item_tags\n                curr_item_tags.add(tag)\n\n        rubric_tag_scores = {}\n        for tag, items_grades in rubric_tag_items_grades.items():\n            items, grades = zip(*items_grades)\n            score = calculate_score(items, grades)\n            if score is not None:  # implies at least one positive criterion\n                rubric_tag_scores[tag] = score\n        metrics.update(rubric_tag_scores)\n\n        # construct the list of explanations and grades\n        rubric_items_with_grades = []\n        readable_explanation_list = []\n        for rubric_item, grading_response in zip(rubric_items, grading_response_list):\n            explanation = grading_response.get(\"explanation\", \"No explanation provided\")\n            criteria_met = grading_response[\"criteria_met\"]\n            readable_explanation = (\n                f\"[{criteria_met}] {rubric_item}\\n\\tExplanation: {explanation}\"\n            )\n            readable_explanation_list.append(readable_explanation)\n            rubric_items_with_grades.append(\n                {\n                    **rubric_item.to_dict(),\n                    \"criteria_met\": criteria_met,\n                    \"explanation\": explanation,\n                }\n            )\n\n        readable_explanation_list.sort(\n            key=lambda x: x.startswith(\"[False]\"), reverse=True\n        )\n        readable_explanation_str = \"\\n\\n\".join(readable_explanation_list)\n        readable_explanation_str = f\"\\n\\n{readable_explanation_str}\"\n\n        return metrics, readable_explanation_str, rubric_items_with_grades\n\n    def __call__(self, sampler: SamplerBase) -> EvalResult:\n        def fn(row: dict):\n            prompt_messages = row[\"prompt\"]\n\n            if self.physician_completions_mode is not None:\n                response_text = row[\"completion_to_trial\"]\n                response_usage = None\n                actual_queried_prompt_messages = prompt_messages\n            else:\n                sampler_response = sampler(prompt_messages)\n                response_text = sampler_response.response_text\n                response_dict = sampler_response.response_metadata\n                actual_queried_prompt_messages = (\n                    sampler_response.actual_queried_message_list\n                )\n                response_usage = response_dict.get(\"usage\", None)\n\n            metrics, readable_explanation_str, rubric_items_with_grades = (\n                self.grade_sample(\n                    prompt=actual_queried_prompt_messages,\n                    response_text=response_text,\n                    rubric_items=row[\"rubrics\"],\n                    example_tags=row[\"example_tags\"],\n                )\n            )\n\n            score = metrics[\"overall_score\"]\n\n            # Create HTML for each sample result\n            html = report.jinja_env.from_string(\n                HEALTHBENCH_HTML_JINJA.replace(\n                    \"{{ rubric_grades }}\",\n                    readable_explanation_str.replace(\"\\n\", \"<br>\"),\n                )\n            ).render(\n                prompt_messages=actual_queried_prompt_messages,\n                next_message=dict(content=response_text, role=\"assistant\"),\n                score=metrics[\"overall_score\"],\n                extracted_answer=response_text,\n            )\n\n            convo = actual_queried_prompt_messages + [\n                dict(content=response_text, role=\"assistant\")\n            ]\n            return SingleEvalResult(\n                html=html,\n                score=score,\n                convo=convo,\n                metrics=metrics,\n                example_level_metadata={\n                    \"score\": score,\n                    \"usage\": get_usage_dict(response_usage),\n                    \"rubric_items\": rubric_items_with_grades,\n                    \"prompt\": actual_queried_prompt_messages,\n                    \"completion\": [dict(content=response_text, role=\"assistant\")],\n                    \"prompt_id\": row[\"prompt_id\"],\n                    \"completion_id\": hashlib.sha256(\n                        (row[\"prompt_id\"] + response_text).encode(\"utf-8\")\n                    ).hexdigest(),\n                },\n            )\n\n        results = report.map_with_progress(\n            fn,\n            self.examples,\n            num_threads=self.n_threads,\n            pbar=True,\n        )\n        final_metrics = _aggregate_get_clipped_mean(results)\n        return final_metrics\n\n\ndef main():\n    parser = argparse.ArgumentParser(\n        description=\"HealthBenchEval specific run options, including e.g., running the eval on physician completions rows only.\"\n    )\n    parser.add_argument(\n        \"--run_mode\",\n        type=str,\n        choices=[\"physician_completions\", \"physician_completion_references\"],\n    )\n    parser.add_argument(\"--examples\", type=int, help=\"Number of examples to run\")\n    parser.add_argument(\n        \"--n-threads\",\n        type=int,\n        default=120,\n        help=\"Number of threads to run\",\n    )\n    args = parser.parse_args()\n\n    if args.run_mode == \"physician_completions\":\n        physician_completions_main(\n            run_reference_completions=False,\n            num_examples=args.examples,\n            n_threads=args.n_threads or 1,\n        )\n    elif args.run_mode == \"physician_completion_references\":\n        physician_completions_main(\n            run_reference_completions=True,\n            num_examples=args.examples,\n            n_threads=args.n_threads or 1,\n        )\n\n    else:\n        raise ValueError(f\"Invalid run mode: {args.run_mode}\")\n\n\ndef physician_completions_main(\n    run_reference_completions: bool = False,\n    num_examples: int | None = None,\n    n_threads: int = 120,\n):\n    now = datetime.now()\n    date_str = now.strftime(\"%Y%m%d_%H%M\")\n\n    grading_sampler = ChatCompletionsSampler(\n        model=\"gpt-4.1-2025-04-14\",\n        system_message=OPENAI_SYSTEM_MESSAGE_API,\n        max_tokens=2048,\n        base_url=\"https://api.openai.com/v1\",\n    )\n    dummy_sampler = SamplerBase()\n\n    merge_metrics = []\n    for pc_mode in PHYSICIAN_COMPLETION_MODES.keys():\n        if (\n            run_reference_completions\n            and not PHYSICIAN_COMPLETION_MODES[pc_mode][\"has_reference\"]\n        ):\n            continue\n\n        # run\n        eval = HealthBenchEval(\n            grader_model=grading_sampler,\n            physician_completions_mode=pc_mode,\n            run_reference_completions=run_reference_completions,\n            num_examples=num_examples,\n            n_threads=n_threads,\n        )\n        result = eval(dummy_sampler)\n\n        # report\n        parsable_mode = PHYSICIAN_COMPLETION_MODES[pc_mode][\"short_name\"]\n        if run_reference_completions:\n            file_stem = f\"healthbench_{parsable_mode}_referencecompletions_{date_str}\"\n        else:\n            file_stem = f\"healthbench_{parsable_mode}_humanbaseline_{date_str}\"\n        report_filename = Path(f\"/tmp/{file_stem}.html\")\n        report_filename.write_text(report.make_report(result))\n        print(f\"Report saved to {report_filename}\")\n\n        # metrics\n        assert result.metrics is not None\n        metrics = result.metrics\n        result_filename = Path(f\"/tmp/{file_stem}.json\")\n        result_filename.write_text(json.dumps(metrics))\n        print(f\"Results saved to {result_filename}\")\n\n        full_result_dict = {\n            \"score\": result.score,\n            \"metrics\": result.metrics,\n            \"htmls\": result.htmls,\n            \"convos\": result.convos,\n            \"metadata\": result.metadata,\n        }\n        full_result_filename = Path(f\"/tmp/{file_stem}_allresults.json\")\n        full_result_filename.write_text(json.dumps(full_result_dict, indent=2))\n        print(f\"All results saved to {full_result_filename}\")\n\n        # metrics df\n        merge_metrics.append(\n            {\n                \"eval_name\": \"healthbench\",\n                \"model_name\": f\"{pc_mode} ({PHYSICIAN_COMPLETION_MODES[pc_mode]['description']})\",\n                \"metric\": metrics.get(\"overall_score\", None),\n            }\n        )\n\n    print(\"\\nAll results: \")\n    print(merge_metrics)\n    return merge_metrics\n\n\nif __name__ == \"__main__\":\n    main()\n",
        "gpt_oss/evals/report.py": "import os\nfrom collections import defaultdict\nfrom multiprocessing.pool import ThreadPool\nfrom typing import Any, Callable\n\nimport jinja2\nimport numpy as np\nfrom tqdm import tqdm\n\nfrom .types import EvalResult, Message, SingleEvalResult\n\n\nHTML_JINJA = \"\"\"\n<h3>Prompt conversation</h3>\n{% for message in prompt_messages %}\n{{ message_to_html(message) | safe }}\n{% endfor %}\n<h3>Sampled message</h3>\n{{ message_to_html(next_message) | safe }}\n<h3>Results</h3>\n<p>Correct Answer: {{ correct_answer }}</p>\n<p>Extracted Answer: {{ extracted_answer }}</p>\n<p>Score: {{ score }}</p>\n\"\"\"\n\n\ndef _compute_stat(values: list, stat: str):\n    if stat == \"mean\":\n        return np.mean(values)\n    elif stat == \"std\":\n        return np.std(values)\n    elif stat == \"min\":\n        return np.min(values)\n    elif stat == \"max\":\n        return np.max(values)\n    elif stat == \"n_samples\":\n        return len(values)\n    elif stat == \"bootstrap_std\":\n        return np.std(\n            [np.mean(np.random.choice(values, len(values))) for _ in range(1000)]\n        )\n    else:\n        raise ValueError(f\"Unknown {stat =}\")\n\n\ndef aggregate_results(\n    single_eval_results: list[SingleEvalResult],\n    default_stats: tuple[str, ...] = (\"mean\", \"std\"),\n    name2stats: dict[str, tuple[str]] | None = None,\n) -> EvalResult:\n    \"\"\"\n    Aggregate results from multiple evaluations into a single EvalResult.\n    \"\"\"\n    name2stats = name2stats or {}\n    name2values = defaultdict(list)\n    htmls = []\n    convos = []\n    metadata = []\n    for single_eval_result in single_eval_results:\n        for name, value in single_eval_result.metrics.items():\n            name2values[name].append(value)\n        if single_eval_result.score is not None:\n            name2values[\"score\"].append(single_eval_result.score)\n        htmls.append(single_eval_result.html)\n        convos.append(single_eval_result.convo)\n        metadata.append(single_eval_result.example_level_metadata)\n    final_metrics = {}\n    for name, values in name2values.items():\n        stats = name2stats.get(name, default_stats)\n        for stat in stats:\n            key = name if stat == \"mean\" else f\"{name}:{stat}\"\n            final_metrics[key] = _compute_stat(values, stat)\n    return EvalResult(\n        score=final_metrics.pop(\"score\", None),\n        metrics=final_metrics,\n        htmls=htmls,\n        convos=convos,\n        metadata={\"example_level_metadata\": metadata},\n    )\n\n\ndef map_with_progress(\n    f: Callable,\n    xs: list[Any],\n    num_threads: int = 128,\n    pbar: bool = True,\n):\n    \"\"\"\n    Apply f to each element of xs, using a ThreadPool, and show progress.\n    \"\"\"\n    pbar_fn = tqdm if pbar else lambda x, *args, **kwargs: x\n\n    if os.getenv(\"debug\"):\n        return list(map(f, pbar_fn(xs, total=len(xs))))\n    else:\n        with ThreadPool(min(num_threads, len(xs))) as pool:\n            return list(pbar_fn(pool.imap_unordered(f, xs), total=len(xs)))\n\n\njinja_env = jinja2.Environment(\n    loader=jinja2.BaseLoader(),\n    undefined=jinja2.StrictUndefined,\n    autoescape=jinja2.select_autoescape([\"html\", \"xml\"]),\n)\n_message_template = \"\"\"\n<div class=\"message {{ role }}\">\n    <div class=\"role\">\n    {{ role }}\n    {% if variant %}<span class=\"variant\">({{ variant }})</span>{% endif %}\n    </div>\n    <div class=\"content\">\n    <pre>{{ content }}</pre>\n    </div>\n</div>\n\"\"\"\n\n\ndef message_to_html(message: Message) -> str:\n    \"\"\"\n    Generate HTML snippet (inside a <div>) for a message.\n    \"\"\"\n    return jinja_env.from_string(_message_template).render(\n        role=message[\"role\"],\n        content=message[\"content\"],\n        variant=message.get(\"variant\", None),\n    )\n\n\njinja_env.globals[\"message_to_html\"] = message_to_html\n\n\n_report_template = \"\"\"<!DOCTYPE html>\n<html>\n    <head>\n        <meta charset=\"utf-8\">\n        <style>\n            .message {\n                padding: 8px 16px;\n                margin-bottom: 8px;\n                border-radius: 4px;\n            }\n            .message.user {\n                background-color: #B2DFDB;\n                color: #00695C;\n            }\n            .message.assistant {\n                background-color: #B39DDB;\n                color: #4527A0;\n            }\n            .message.system {\n                background-color: #EEEEEE;\n                color: #212121;\n            }\n            .role {\n                font-weight: bold;\n                margin-bottom: 4px;\n            }\n            .variant {\n                color: #795548;\n            }\n            table, th, td {\n                border: 1px solid black;\n            }\n            pre {\n                white-space: pre-wrap;\n            }\n        </style>\n    </head>\n    <body>\n    {% if metrics %}\n    <h1>Metrics</h1>\n    <table>\n    <tr>\n        <th>Metric</th>\n        <th>Value</th>\n    </tr>\n    <tr>\n        <td><b>Score</b></td>\n        <td>{{ score | float | round(3) }}</td>\n    </tr>\n    {% for name, value in metrics.items() %}\n    <tr>\n        <td>{{ name }}</td>\n        <td>{{ value }}</td>\n    </tr>\n    {% endfor %}\n    </table>\n    {% endif %}\n    <h1>Examples</h1>\n    {% for html in htmls %}\n    {{ html | safe }}\n    <hr>\n    {% endfor %}\n    </body>\n</html>\n\"\"\"\n\n\ndef make_report(eval_result: EvalResult) -> str:\n    \"\"\"\n    Create a standalone HTML report from an EvalResult.\n    \"\"\"\n    return jinja_env.from_string(_report_template).render(\n        score=eval_result.score,\n        metrics=eval_result.metrics,\n        htmls=eval_result.htmls,\n    )\n",
        "gpt_oss/evals/responses_sampler.py": "import time\nfrom typing import Any\n\nimport openai\nfrom openai import OpenAI\n\nfrom .types import MessageList, SamplerBase, SamplerResponse\n\n\nclass ResponsesSampler(SamplerBase):\n    \"\"\"\n    Sample from OpenAI's responses API\n    \"\"\"\n\n    def __init__(\n        self,\n        model: str,\n        developer_message: str | None = None,\n        temperature: float = 1.0,\n        max_tokens: int = 131_072,\n        reasoning_model: bool = False,\n        reasoning_effort: str | None = None,\n        base_url: str = \"http://localhost:8000/v1\",\n    ):\n        self.client = OpenAI(base_url=base_url, timeout=24*60*60)\n        self.model = model\n        self.developer_message = developer_message\n        self.temperature = temperature\n        self.max_tokens = max_tokens\n        self.image_format = \"url\"\n        self.reasoning_model = reasoning_model\n        self.reasoning_effort = reasoning_effort\n\n    def _pack_message(self, role: str, content: Any) -> dict[str, Any]:\n        return {\"role\": role, \"content\": content}\n\n    def __call__(self, message_list: MessageList) -> SamplerResponse:\n        if self.developer_message:\n            message_list = [\n                self._pack_message(\"developer\", self.developer_message)\n            ] + message_list\n        trial = 0\n        while True:\n            try:\n                request_kwargs = {\n                    \"model\": self.model,\n                    \"input\": message_list,\n                    \"temperature\": self.temperature,\n                    \"max_output_tokens\": self.max_tokens,\n                }\n                if self.reasoning_model:\n                    request_kwargs[\"reasoning\"] = (\n                        {\"effort\": self.reasoning_effort} if self.reasoning_effort else None\n                    )\n                response = self.client.responses.create(**request_kwargs)\n\n                for output in response.output:\n                    if hasattr(output, \"text\"):\n                        message_list.append(self._pack_message(getattr(output, \"role\", \"assistant\"), output.text))\n                    elif hasattr(output, \"content\"):\n                        for c in output.content:\n                            # c.text handled below\n                            pass\n\n                return SamplerResponse(\n                    response_text=response.output_text,\n                    response_metadata={\"usage\": response.usage},\n                    actual_queried_message_list=message_list,\n                )\n            except openai.BadRequestError as e:\n                print(\"Bad Request Error\", e)\n                return SamplerResponse(\n                    response_text=\"\",\n                    response_metadata={\"usage\": None},\n                    actual_queried_message_list=message_list,\n                )\n            except Exception as e:\n                exception_backoff = 2**trial  # expontial back off\n                print(\n                    f\"Rate limit exception so wait and retry {trial} after {exception_backoff} sec\",\n                    e,\n                )\n                time.sleep(exception_backoff)\n                trial += 1\n            # unknown error shall throw exception\n",
        "gpt_oss/evals/types.py": "from dataclasses import dataclass, field\nfrom typing import Any, Literal, overload\n\nMessage = dict[str, Any]  # keys role, content\nMessageList = list[Message]\n\n\n\n@dataclass\nclass SamplerResponse:\n    \"\"\"\n    Response from a sampler.\n    \"\"\"\n    response_text: str\n    actual_queried_message_list: MessageList\n    response_metadata: dict[str, Any]\n\nclass SamplerBase:\n    \"\"\"\n    Base class for defining a sampling model, which can be evaluated,\n    or used as part of the grading process.\n    \"\"\"\n\n    def __call__(\n        self, \n        message_list: MessageList,\n    ) -> SamplerResponse:\n        raise NotImplementedError\n\n\n@dataclass\nclass EvalResult:\n    \"\"\"\n    Result of running an evaluation (usually consisting of many samples)\n    \"\"\"\n\n    score: float | None  # top-line metric\n    metrics: dict[str, float] | None  # other metrics\n    htmls: list[str]  # strings of valid HTML\n    convos: list[MessageList]  # sampled conversations\n    metadata: dict[str, Any] | None  # Extra data such as rubric scores or sollen\n\n\n@dataclass\nclass SingleEvalResult:\n    \"\"\"\n    Result of evaluating a single sample\n    \"\"\"\n\n    score: float | None\n    metrics: dict[str, float] = field(default_factory=dict)\n    html: str | None = None\n    convo: MessageList | None = None  # sampled conversation\n    example_level_metadata: dict[str, Any] | None = (\n        None  # Extra data such as rubric scores or sollen\n    )\n\n\nclass Eval:\n    \"\"\"\n    Base class for defining an evaluation.\n    \"\"\"\n\n    def __call__(self, sampler: SamplerBase) -> EvalResult:\n        raise NotImplementedError\n\n",
        "gpt_oss/generate.py": "# Model parallel inference\n# Note: This script is for demonstration purposes only. It is not designed for production use.\n#       See gpt_oss.chat for a more complete example with the Harmony parser.\n# torchrun --nproc-per-node=4 -m gpt_oss.generate -p \"why did the chicken cross the road?\" model/\n\nimport argparse\n\nfrom gpt_oss.tokenizer import get_tokenizer\n\n\ndef main(args):\n    match args.backend:\n        case \"torch\":\n            from gpt_oss.torch.utils import init_distributed\n            from gpt_oss.torch.model import TokenGenerator as TorchGenerator\n            device = init_distributed()\n            generator = TorchGenerator(args.checkpoint, device=device)\n        case \"triton\":\n            from gpt_oss.torch.utils import init_distributed\n            from gpt_oss.triton.model import TokenGenerator as TritonGenerator\n            device = init_distributed()\n            generator = TritonGenerator(args.checkpoint, context=args.context_length, device=device)\n        case \"vllm\":\n            from gpt_oss.vllm.token_generator import TokenGenerator as VLLMGenerator\n            generator = VLLMGenerator(args.checkpoint, tensor_parallel_size=args.tensor_parallel_size)\n        case _:\n            raise ValueError(f\"Invalid backend: {args.backend}\")\n\n    tokenizer = get_tokenizer()\n    tokens = tokenizer.encode(args.prompt)\n    max_tokens = None if args.limit == 0 else args.limit\n    for token, logprob in generator.generate(tokens, stop_tokens=[tokenizer.eot_token], temperature=args.temperature, max_tokens=max_tokens, return_logprobs=True):\n        tokens.append(token)\n        token_text = tokenizer.decode([token])\n        print(\n            f\"Generated token: {repr(token_text)}, logprob: {logprob}\"\n        )\n\n\nif __name__ == \"__main__\":\n    parser = argparse.ArgumentParser(description=\"Text generation example\")\n    parser.add_argument(\n        \"checkpoint\",\n        metavar=\"FILE\",\n        type=str,\n        help=\"Path to the SafeTensors checkpoint\",\n    )\n    parser.add_argument(\n        \"-p\",\n        \"--prompt\",\n        metavar=\"PROMPT\",\n        type=str,\n        default=\"How are you?\",\n        help=\"LLM prompt\",\n    )\n    parser.add_argument(\n        \"-t\",\n        \"--temperature\",\n        metavar=\"TEMP\",\n        type=float,\n        default=0.0,\n        help=\"Sampling temperature\",\n    )\n    parser.add_argument(\n        \"-l\",\n        \"--limit\",\n        metavar=\"LIMIT\",\n        type=int,\n        default=0,\n        help=\"Limit on the number of tokens (0 to disable)\",\n    )\n    parser.add_argument(\n        \"-b\",\n        \"--backend\",\n        metavar=\"BACKEND\",\n        type=str,\n        default=\"torch\",\n        choices=[\"triton\", \"torch\", \"vllm\"],\n        help=\"Inference backend\",\n    )\n    parser.add_argument(\n        \"--tensor-parallel-size\",\n        type=int,\n        default=2,\n        help=\"Tensor parallel size for vLLM backend\",\n    )\n    parser.add_argument(\n        \"--context-length\",\n        type=int,\n        default=4096,\n        help=\"Context length for Triton backend\",\n    )\n    args = parser.parse_args()\n\n    main(args)\n",
        "gpt_oss/metal/__init__.py": "from importlib import import_module as _im\n\n# Load the compiled extension (gpt_oss.metal._metal)\n_ext = _im(f\"{__name__}._metal\")\nglobals().update({k: v for k, v in _ext.__dict__.items() if not k.startswith(\"_\")})\ndel _im, _ext\n",
        "gpt_oss/metal/examples/chat.py": "#!/usr/bin/env python\n\nimport argparse\nimport sys\n\nfrom datetime import date\nfrom gpt_oss.metal import Context, Model\n\n\nDEFAULT_PROMPT = f\"\"\"You are ChatGPT, a large language model trained by OpenAI.\nKnowledge cutoff: 2024-06\nCurrent date: {date.today().isoformat()}\n\nreasoning effort high\n\n# Valid channels: analysis, final. Channel must be included for every message.\"\"\"\n\n\nparser = argparse.ArgumentParser(description=\"Chat with gpt-oss\", formatter_class=argparse.ArgumentDefaultsHelpFormatter)\nparser.add_argument(\"model\", metavar=\"PATH\", type=str, help=\"Path to gpt-oss model in Metal inference format\")\nparser.add_argument(\"--prompt\", type=str, default=DEFAULT_PROMPT, help=\"System prompt\")\nparser.add_argument(\n    \"--context-length\", type=int, default=0, help=\"The maximum context length\"\n)\nparser.add_argument(\n    \"--temperature\", type=float, default=1.0, help=\"Sampling temperature\"\n)\nparser.add_argument(\n    \"--seed\", type=int, default=0, help=\"Sampling seed\"\n)\n\n\nGREY = \"\\33[90m\"\nBOLD = \"\\33[1m\"\nRESET = \"\\33[0m\"\n\n\ndef main(args):\n    options = parser.parse_args(args)\n    model = Model(options.model)\n    tokenizer = model.tokenizer\n    start_token = tokenizer.encode_special_token(\"<|start|>\")\n    message_token = tokenizer.encode_special_token(\"<|message|>\")\n    end_token = tokenizer.encode_special_token(\"<|end|>\")\n    return_token = tokenizer.encode_special_token(\"<|return|>\")\n    channel_token = tokenizer.encode_special_token(\"<|channel|>\")\n\n    context = Context(model, context_length=options.context_length)\n    context.append(start_token)\n    context.append(\"system\")\n    context.append(message_token)\n    context.append(options.prompt)\n    context.append(end_token)\n\n    while True:\n        context.append(start_token)\n        context.append(\"user\")\n        context.append(message_token)\n        message = input(f\"{BOLD}User:{RESET} \").rstrip()\n        context.append(message)\n        context.append(end_token)\n        print(f\"{BOLD}Assistant:{RESET} {GREY}\", end=\"\", flush=True)\n        context.append(start_token)\n        context.append(\"assistant\")\n        context.append(channel_token)\n\n        inside_start_block = True\n        inside_channel_block = True\n        role = \"assistant\"\n        channel = \"\"\n        while True:\n            token = context.sample(\n                temperature=options.temperature,\n                seed=options.seed,\n            )\n            context.append(token)\n            if token == return_token:\n                print(flush=True)\n                break\n            elif token == start_token:\n                inside_start_block = True\n                role = \"\"\n                channel = \"\"\n            elif token == message_token:\n                inside_start_block = False\n                inside_channel_block = False\n                if channel == \"analysis\":\n                    print(f\"{GREY}\", end=\"\", flush=True)\n            elif token == end_token:\n                print(f\"{RESET}\", flush=True)\n            elif token == channel_token:\n                inside_channel_block = True\n            elif token < tokenizer.num_text_tokens:\n                if inside_channel_block:\n                    channel += str(tokenizer.decode(token), encoding=\"utf-8\")\n                elif inside_start_block:\n                    role += str(tokenizer.decode(token), encoding=\"utf-8\")\n                else:\n                    sys.stdout.buffer.write(tokenizer.decode(token))\n                    sys.stdout.buffer.flush()\n\n\nif __name__ == \"__main__\":\n    main(sys.argv[1:])\n",
        "gpt_oss/metal/examples/generate.py": "#!/usr/bin/env python\n\nimport argparse\nimport sys\n\nfrom gpt_oss.metal import Context, Model\n\n\nparser = argparse.ArgumentParser(description='Chat with gpt-oss', formatter_class=argparse.ArgumentDefaultsHelpFormatter)\nparser.add_argument('model', metavar='PATH', type=str, help='Path to gpt-oss checkpoint')\nparser.add_argument('-p', '--prompt', type=str, required=True, help='Prompt')\nparser.add_argument('-l', '--limit', type=int, default=100, help='Number of tokens to generate')\nparser.add_argument('--context-length', type=int, default=0, help='The maximum context length')\n\n\ndef main(args):\n    options = parser.parse_args(args)\n    model = Model(options.model)\n\n    context = Context(model, context_length=options.context_length)\n    context.append(options.prompt)\n    print(context.tokens)\n    prompt_tokens = context.num_tokens\n\n    tokenizer = model.tokenizer\n\n    while context.num_tokens - prompt_tokens < options.limit:\n        token = context.sample()\n        context.append(token)\n        print(str(tokenizer.decode(token), encoding=\"utf-8\"), end='', flush=True)\n\n\nif __name__ == '__main__':\n    main(sys.argv[1:])\n",
        "gpt_oss/metal/scripts/create-local-model.py": "import argparse\nimport os\nimport math\nimport sys\nimport json\nimport itertools\nimport struct\nfrom uuid import UUID\n\nimport tiktoken\n\nimport torch\nfrom safetensors import safe_open\nfrom tqdm import tqdm\nfrom openai_harmony import load_harmony_encoding, HarmonyEncodingName\n\nparser = argparse.ArgumentParser(prog='check-mxfp4-weights.py', description='Validated MXFP4 weights')\nparser.add_argument('-s', '--src', metavar='DIR', type=str, required=True, help='Path to the input checkpoint directory')\nparser.add_argument('-d', '--dst', metavar='FILE', type=str, required=True, help='Path to the output model file')\n\n\no200k_base = tiktoken.get_encoding(\"o200k_base\")\nharmony_encoding = load_harmony_encoding(HarmonyEncodingName.HARMONY_GPT_OSS)\n\no200k_gptoss = tiktoken.Encoding(\n    name=\"o200k_gptoss\",\n    pat_str=o200k_base._pat_str,\n    mergeable_ranks=o200k_base._mergeable_ranks,\n    special_tokens={\n        \"<|reversed199998|>\": 199998,  # unused\n        \"<|endoftext|>\": 199999,\n        \"<|untrusted|>\": 200000,\n        \"<|endofuntrusted|>\": 200001,\n        \"<|return|>\": 200002,\n        \"<|constrain|>\": 200003,\n        \"<|reversed200004|>\": 200004,  # unused\n        \"<|channel|>\": 200005,\n        \"<|start|>\": 200006,\n        \"<|end|>\": 200007,\n        \"<|message|>\": 200008,\n        \"<|reversed200008|>\": 200008,  # unused\n        \"<|reversed200009|>\": 200009,  # unused\n        \"<|reversed200010|>\": 200010,  # unused\n        \"<|reversed200011|>\": 200011,  # unused\n        \"<|call|>\": 200012,\n        \"<|refusal|>\": 200013,\n    }\n)\n\nFILE_MAGIC = struct.pack('ccccccccccccI', b'G', b'P', b'T', b'-', b'O', b'S', b'S', b' ', b'v', b'1', b'.', b'0', 0)\nSPECIAL_TOKEN_UUID = {\n    '<|start|>': UUID('55a77c2f-8a01-4c54-8ac2-313bfc7e208d').bytes,\n    '<|message|>': UUID('16e40431-f47f-4b22-b59b-8b278fc30a54').bytes,\n    '<|end|>': UUID('fcac2f6d-4705-4f6b-b228-642accac7238').bytes,\n    '<|return|>': UUID('f799ff69-1992-43c4-a3d8-d831f475dc75').bytes,\n    '<|refusal|>': UUID('e15ba702-28c4-4292-ab8f-ffa434709128').bytes,\n    '<|constrain|>': UUID('c0bb14c7-6022-49da-ad08-792d67e8b470').bytes,\n    '<|channel|>': UUID('fd3dda11-c8ab-4033-876e-d93deb172c93').bytes,\n    '<|call|>': UUID('1220f796-e388-4de5-b487-fe2eb5fe03c0').bytes,\n    '<|untrusted|>': UUID('07d7da55-b346-4cff-8b37-7cefacf8a3e8').bytes,\n    '<|end_untrusted|>': UUID('f265bd9c-c717-469e-a447-920687d65d90').bytes,\n}\n\nINCLUDE_SPECIAL_TOKENS = [\n    \"<|start|>\",\n    \"<|message|>\",\n    \"<|end|>\",\n    \"<|return|>\",\n    \"<|refusal|>\",\n    \"<|constrain|>\",\n    \"<|channel|>\",\n    \"<|call|>\",\n    \"<|untrusted|>\",\n    \"<|end_untrusted|>\",\n]\n\nGPTOSS_MODEL_UUID = UUID('df52dc86-1789-4ed0-a295-66f10508145b').bytes\nAPPLE_GPU_LAYOUT_UUID = UUID('229177a8-5775-4268-bfd8-d588b351c56d').bytes\nTIKTOKEN_TOKENIZER_UUID = UUID('7401aded-2a95-40cb-b782-9ccebaafe72b').bytes\n\nUE8_OFFSET = 14  # bias to MXFP4 block scales\n\ndef write_file_header(f):\n    f.write(FILE_MAGIC)\n\ndef write_tokenizer_header(f,\n                           num_special_tokens: int,\n                           num_text_tokens: int,\n                           regex_size: int,\n                           tokens_size: int):\n    f.write(TIKTOKEN_TOKENIZER_UUID)\n    f.write(struct.pack('<I', num_special_tokens))\n    f.write(struct.pack('<I', num_text_tokens))\n    f.write(struct.pack('<I', regex_size))\n    f.write(struct.pack('<I', tokens_size))\n\ndef write_model_header(f,\n                       context_length : int,\n                       num_blocks : int,\n                       num_experts : int,\n                       num_active_experts : int,\n                       embedding_dim : int,\n                       mlp_dim : int,\n                       swiglu_limit : float,\n                       head_dim: int,\n                       num_heads : int,\n                       num_kv_heads : int,\n                       attention_window : int,\n                       rope_theta : float,\n                       interpolation_scale : float,\n                       yarn_offset : float,\n                       yarn_scale : float,\n                       yarn_multiplier : float,\n                       rmsnorm_epsilon : float):\n    f.write(GPTOSS_MODEL_UUID)\n    f.write(struct.pack('<I', context_length))\n    f.write(struct.pack('<I', num_blocks))\n    f.write(struct.pack('<I', num_experts))\n    f.write(struct.pack('<I', num_active_experts))\n    f.write(struct.pack('<I', embedding_dim))\n    f.write(struct.pack('<I', mlp_dim))\n    f.write(struct.pack('<f', swiglu_limit))\n    f.write(struct.pack('<I', head_dim))\n    f.write(struct.pack('<I', num_heads))\n    f.write(struct.pack('<I', num_kv_heads))\n    f.write(struct.pack('<I', attention_window))\n    f.write(struct.pack('<f', rope_theta))\n    f.write(struct.pack('<f', interpolation_scale))\n    f.write(struct.pack('<f', yarn_offset))\n    f.write(struct.pack('<f', yarn_scale))\n    f.write(struct.pack('<f', yarn_multiplier))\n    f.write(struct.pack('<f', rmsnorm_epsilon))\n    f.write(APPLE_GPU_LAYOUT_UUID)\n\n\ndef write_padding(out_file, alignment_multiple=16384):\n    offset = out_file.tell()\n    alignment_size = -offset % alignment_multiple\n    if alignment_size != 0:\n        alignment = bytes(alignment_size)\n        out_file.write(alignment)\n\n\ndef write_embedding_weight(out_file, weight):\n    write_padding(out_file, alignment_multiple=16)\n\n    assert weight.dtype == torch.float8_e4m3fn or weight.dtype == torch.bfloat16\n    out_file.write(weight.view(torch.uint8).numpy().tobytes())\n\n\ndef write_rmsnorm_gain(out_file, gain):\n    write_padding(out_file, alignment_multiple=16)\n\n    assert gain.dtype == torch.bfloat16\n    out_file.write(gain.view(torch.uint8).numpy().tobytes())\n\n\ndef write_attn_sink(out_file, sink):\n    write_padding(out_file, alignment_multiple=16)\n\n    assert sink.dtype == torch.bfloat16\n    out_file.write(sink.view(torch.uint8).numpy().tobytes())\n\n\ndef write_linear_weight(out_file, *args):\n    write_padding(out_file, alignment_multiple=16)\n\n    for t in args:\n        out_file.write(t.view(torch.uint8).numpy().tobytes())\n\n\ndef main(args):\n    options = parser.parse_args(args)\n\n    with open(os.path.join(options.src, \"config.json\"), \"r\") as f:\n        config = json.load(f)\n\n    num_blocks = config[\"num_hidden_layers\"]\n    num_experts = config[\"num_experts\"]\n    num_active_experts = 4\n    num_q_heads = config[\"num_attention_heads\"]\n    num_kv_heads = config[\"num_key_value_heads\"]\n    head_dim = config[\"head_dim\"]\n    embedding_dim = config[\"hidden_size\"]\n    mlp_dim = config[\"intermediate_size\"]\n    swiglu_limit = config.get(\"swiglu_limit\", 7.0)\n    rope_theta = config[\"rope_theta\"]\n    attention_window = config[\"sliding_window\"]\n    initial_context_length = config[\"initial_context_length\"]\n    rope_scaling_factor = config[\"rope_scaling_factor\"]\n    rope_ntk_alpha = config[\"rope_ntk_alpha\"]\n    rope_ntk_beta = config[\"rope_ntk_beta\"]\n\n    tokens_size = 0\n    num_text_tokens = 0\n    # First add all text tokens\n    for t in range(o200k_gptoss.n_vocab):\n        if not harmony_encoding.is_special_token(t):\n            token_bytes = o200k_gptoss.decode_single_token_bytes(t)\n            assert len(token_bytes) > 0\n            tokens_size += len(token_bytes) + 2  # uint16_t string length + string data\n            num_text_tokens += 1\n    # Then add all special tokens\n    num_included_tokens = 200013 + 1\n    print(f\"Tokenizer: {num_included_tokens} tokens\")\n\n    tensors = {}\n    with open(options.dst, \"wb\") as dst:\n        with safe_open(os.path.join(options.src, \"model.safetensors\"), framework=\"pt\", device=\"cpu\") as src:\n            write_file_header(dst)\n\n            yarn_low = (\n                head_dim / 2\n                * math.log(initial_context_length / (rope_ntk_beta * 2 * math.pi))\n                / math.log(rope_theta)\n            )\n            yarn_high = (\n                head_dim / 2\n                * math.log(initial_context_length / (rope_ntk_alpha * 2 * math.pi))\n                / math.log(rope_theta)\n            )\n\n            write_model_header(dst,\n                               context_length=int(initial_context_length * rope_scaling_factor),\n                               num_blocks=num_blocks,\n                               num_experts=num_experts,\n                               num_active_experts=num_active_experts,\n                               embedding_dim=embedding_dim,\n                               mlp_dim=mlp_dim,\n                               swiglu_limit=swiglu_limit,\n                               head_dim=head_dim,\n                               num_heads=num_q_heads,\n                               num_kv_heads=num_kv_heads,\n                               attention_window=attention_window,\n                               rope_theta=rope_theta,\n                               interpolation_scale=1.0 / rope_scaling_factor,\n                               yarn_offset=-yarn_low / (yarn_high - yarn_low),\n                               yarn_scale=1.0 / (yarn_high - yarn_low),\n                               yarn_multiplier=0.1 * math.log(rope_scaling_factor) + 1.0,\n                               rmsnorm_epsilon=1.0e-5)\n\n            write_tokenizer_header(dst,\n                                   num_special_tokens=num_included_tokens - num_text_tokens,\n                                   num_text_tokens=num_text_tokens,\n                                   regex_size=len(o200k_gptoss._pat_str.encode(\"ascii\")) + 1,\n                                   tokens_size=tokens_size)\n\n            ### Tokenizer\n            # Special tokens\n            for token_idx in range(num_text_tokens, num_included_tokens):\n                token = o200k_gptoss.decode_single_token_bytes(token_idx).decode('ascii')\n                if token in INCLUDE_SPECIAL_TOKENS:\n                    dst.write(SPECIAL_TOKEN_UUID[token])\n                else:\n                    dst.write(bytes(16))\n            # Regex\n            dst.write(o200k_gptoss._pat_str.encode(\"ascii\"))\n            dst.write(struct.pack('B', 0))\n            # Text tokens\n            tokenizer_bytes_written = 0\n            for t in range(num_text_tokens):\n                token_bytes = o200k_gptoss.decode_single_token_bytes(t)\n                assert len(token_bytes) > 0\n                dst.write(struct.pack('<H', len(token_bytes)))\n                dst.write(token_bytes)\n                tokenizer_bytes_written += len(token_bytes) + 2\n            assert(tokenizer_bytes_written == tokens_size), (tokenizer_bytes_written, tokens_size)\n            write_padding(dst)\n\n            embedding_weight = src.get_tensor(\"embedding.weight\")\n            # Filter out unused tokens\n            embedding_weight = embedding_weight[:num_included_tokens, :]\n            write_embedding_weight(dst, embedding_weight)\n\n            for n in tqdm(range(num_blocks)):\n                write_rmsnorm_gain(dst, src.get_tensor(f\"block.{n}.attn.norm.scale\"))\n\n                attn_qkv_weight = src.get_tensor(f\"block.{n}.attn.qkv.weight\")\n                attn_qkv_bias = src.get_tensor(f\"block.{n}.attn.qkv.bias\")\n                for qkv in (attn_qkv_weight, attn_qkv_bias):\n                    qk = qkv[:head_dim * (num_q_heads + num_kv_heads), ...].contiguous()\n                    v = qkv[head_dim * (num_q_heads + num_kv_heads):, ...].contiguous()\n                    qk = qk.view(num_q_heads + num_kv_heads, 2, head_dim // 2, -1).transpose(1, 2).reshape(num_q_heads + num_kv_heads, head_dim, -1)\n                    q = qk[:num_q_heads, ...]\n                    k = qk[num_q_heads:, ...]\n                    # Factor multiplication by 1/sqrt(64) = 0.125 = 0.5 * 0.25 in SDPA into Q and K projections\n                    assert head_dim == 64\n                    q *= 0.5\n                    k *= 0.25\n                    v = v.view(num_kv_heads, head_dim, -1)\n                    qkv.copy_(torch.cat((q, k, v), dim=0).reshape(*qkv.shape))\n\n                write_linear_weight(dst, attn_qkv_weight, attn_qkv_bias)\n\n                write_attn_sink(dst, src.get_tensor(f\"block.{n}.attn.sinks\"))\n\n                write_linear_weight(dst, src.get_tensor(f\"block.{n}.attn.out.weight\"), src.get_tensor(f\"block.{n}.attn.out.bias\"))\n\n                write_rmsnorm_gain(dst, src.get_tensor(f\"block.{n}.mlp.norm.scale\"))\n\n                write_linear_weight(dst, src.get_tensor(f\"block.{n}.mlp.gate.weight\"), src.get_tensor(f\"block.{n}.mlp.gate.bias\"))\n\n            write_rmsnorm_gain(dst, src.get_tensor(\"norm.scale\"))\n\n            unembedding_weight = src.get_tensor(\"unembedding.weight\")\n            unembedding_weight = unembedding_weight[:num_included_tokens, :]\n            write_linear_weight(dst, unembedding_weight)\n\n            for n in tqdm(range(num_blocks)):\n                mlp1_blocks = src.get_tensor(f\"block.{n}.mlp.mlp1_weight.blocks\")\n                mlp1_scales = src.get_tensor(f\"block.{n}.mlp.mlp1_weight.scales\")\n                assert mlp1_scales.min().item() < 254 - UE8_OFFSET\n                mlp1_bias = src.get_tensor(f\"block.{n}.mlp.mlp1_bias\")\n\n                mlp2_blocks = src.get_tensor(f\"block.{n}.mlp.mlp2_weight.blocks\")\n                mlp2_scales = src.get_tensor(f\"block.{n}.mlp.mlp2_weight.scales\")\n                assert mlp2_scales.min().item() < 254 - UE8_OFFSET\n                mlp2_bias = src.get_tensor(f\"block.{n}.mlp.mlp2_bias\")\n\n                # Write MoE weights grouped by expert\n                write_padding(dst)\n\n                for e in range(num_experts):\n                    write_padding(dst, alignment_multiple=16)                    \n                    dst.write(mlp1_blocks[e, ...].view(torch.uint8).numpy().tobytes())\n\n                    write_padding(dst, alignment_multiple=16)\n                    dst.write((mlp1_scales + UE8_OFFSET)[e, ...].view(torch.uint8).numpy().tobytes())\n\n                    write_padding(dst, alignment_multiple=16)\n                    dst.write(mlp1_bias[e, ...].view(torch.uint8).numpy().tobytes())\n\n                    write_padding(dst, alignment_multiple=16)                    \n                    dst.write(mlp2_blocks[e, ...].view(torch.uint8).numpy().tobytes())\n\n                    write_padding(dst, alignment_multiple=16)\n                    dst.write((mlp2_scales + UE8_OFFSET)[e, ...].view(torch.uint8).numpy().tobytes())\n\n                    write_padding(dst, alignment_multiple=16)\n                    dst.write(mlp2_bias[e, ...].view(torch.uint8).numpy().tobytes())\n\nif __name__ == \"__main__\":\n    main(sys.argv[1:])\n",
        "gpt_oss/responses_api/__init__.py": "",
        "gpt_oss/responses_api/api_server.py": "import os\nimport datetime\nimport uuid\nfrom typing import Callable, Literal, Optional\n\nfrom fastapi import FastAPI, Request\nfrom fastapi.responses import StreamingResponse\nfrom openai_harmony import (\n    Author,\n    Conversation,\n    DeveloperContent,\n    HarmonyEncoding,\n    Message,\n    ReasoningEffort,\n    Role,\n    StreamableParser,\n    StreamState,\n    SystemContent,\n    ToolDescription,\n)\n\nfrom gpt_oss.tools.python_docker.docker_tool import PythonTool\nfrom gpt_oss.tools.simple_browser import SimpleBrowserTool\nfrom gpt_oss.tools.simple_browser.backend import YouComBackend, ExaBackend\n\nfrom .events import (\n    ResponseCodeInterpreterCallCompleted,\n    ResponseCodeInterpreterCallInProgress,\n    ResponseCompletedEvent,\n    ResponseContentPartAdded,\n    ResponseContentPartDone,\n    ResponseCreatedEvent,\n    ResponseEvent,\n    ResponseInProgressEvent,\n    ResponseOutputItemAdded,\n    ResponseOutputItemDone,\n    ResponseOutputTextAnnotationAdded,\n    ResponseOutputTextDelta,\n    ResponseOutputTextDone,\n    ResponseReasoningTextDelta,\n    ResponseReasoningTextDone,\n    ResponseWebSearchCallCompleted,\n    ResponseWebSearchCallInProgress,\n    ResponseWebSearchCallSearching,\n)\nfrom .types import (\n    CodeInterpreterCallItem,\n    Error,\n    FunctionCallItem,\n    Item,\n    ReasoningItem,\n    ReasoningTextContentItem,\n    ResponseObject,\n    ResponsesRequest,\n    TextContentItem,\n    UrlCitation,\n    Usage,\n    WebSearchActionFind,\n    WebSearchActionOpenPage,\n    WebSearchActionSearch,\n    WebSearchCallItem,\n)\n\nDEFAULT_TEMPERATURE = 0.0\n\n\ndef get_reasoning_effort(effort: Literal[\"low\", \"medium\", \"high\"]) -> ReasoningEffort:\n    if effort == \"low\":\n        return ReasoningEffort.LOW\n    if effort == \"medium\":\n        return ReasoningEffort.MEDIUM\n    if effort == \"high\":\n        return ReasoningEffort.HIGH\n    raise ValueError(f\"Invalid reasoning effort: {effort}\")\n\n\ndef is_not_builtin_tool(recipient: str) -> bool:\n    return (\n        not recipient.startswith(\"browser.\")\n        and not recipient == \"python\"\n        and not recipient == \"assistant\"\n    )\n\n\ndef create_api_server(\n    infer_next_token: Callable[[list[int], float], int], encoding: HarmonyEncoding\n) -> FastAPI:\n    app = FastAPI()\n    responses_store: dict[str, tuple[ResponsesRequest, ResponseObject]] = {}\n\n    def generate_response(\n        input_tokens: list[int],\n        output_tokens: list[int],\n        request_body: ResponsesRequest,\n        debug_mode: bool = False,\n        function_call_ids: Optional[list[tuple[str, str]]] = None,\n        response_id: Optional[str] = None,\n        previous_response_id: Optional[str] = None,\n        browser_tool: Optional[SimpleBrowserTool] = None,\n        browser_call_ids: Optional[list[str]] = None,\n        python_tool: Optional[PythonTool] = None,\n        python_call_ids: Optional[list[str]] = None,\n    ) -> ResponseObject:\n        output = []\n        error = None\n        if len(output_tokens) > 0:\n            if debug_mode:\n                try:\n                    entries = encoding.parse_messages_from_completion_tokens(\n                        output_tokens, Role.ASSISTANT\n                    )\n                except Exception as e:\n                    print(f\"Error parsing tokens: {e}\")\n                    error = Error(\n                        code=\"invalid_function_call\",\n                        message=f\"{e}\",\n                    )\n                    entries = []\n            else:\n                entries = encoding.parse_messages_from_completion_tokens(\n                    output_tokens, Role.ASSISTANT\n                )\n\n            fc_index = 0\n            browser_tool_index = 0\n            python_tool_index = 0\n            for entry in entries:\n                entry_dict = entry.to_dict()\n                if len(entry_dict.get(\"recipient\", \"\")) > 0 and is_not_builtin_tool(\n                    entry_dict[\"recipient\"]\n                ):\n                    call = entry_dict[\"content\"][0]\n                    arguments = call[\"text\"]\n                    name = entry_dict[\"recipient\"]\n\n                    if name.startswith(\"functions.\"):\n                        name = name[len(\"functions.\") :]\n                    if function_call_ids and fc_index < len(function_call_ids):\n                        fc_id, call_id = function_call_ids[fc_index]\n                    else:\n                        fc_id, call_id = (\n                            f\"fc_{uuid.uuid4().hex}\",\n                            f\"call_{uuid.uuid4().hex}\",\n                        )\n                    fc_index += 1\n                    output.append(\n                        FunctionCallItem(\n                            type=\"function_call\",\n                            name=name,\n                            arguments=arguments,\n                            id=fc_id,\n                            call_id=call_id,\n                        )\n                    )\n                elif (\n                    len(entry_dict.get(\"recipient\", \"\")) > 0\n                    and entry_dict[\"recipient\"].startswith(\"browser.\")\n                    and browser_tool is not None\n                ):\n                    # Mirror event-based creation of WebSearchCallItems when the browser tool is invoked\n                    name = entry_dict[\"recipient\"]\n                    call = entry_dict[\"content\"][0]\n                    arguments = call[\"text\"]\n                    function_name = name[len(\"browser.\") :]\n\n                    # Reconstruct a Message for argument parsing\n                    tool_msg = (\n                        Message.from_role_and_content(Role.ASSISTANT, arguments)\n                        .with_recipient(name)\n                        .with_channel(\"analysis\")\n                    )\n\n                    action = None\n                    try:\n                        parsed_args = browser_tool.process_arguments(tool_msg)\n                        if function_name == \"search\":\n                            action = WebSearchActionSearch(\n                                type=\"search\",\n                                query=parsed_args[\"query\"],\n                            )\n                        elif function_name == \"open\":\n                            action = WebSearchActionOpenPage(\n                                type=\"open_page\",\n                                url=parsed_args[\"url\"],\n                            )\n                        elif function_name == \"find\":\n                            action = WebSearchActionFind(\n                                type=\"find\",\n                                pattern=parsed_args[\"pattern\"],\n                                url=parsed_args[\"url\"],\n                            )\n                    except Exception as e:\n                        print(f\"Error processing browser tool arguments: {e}\")\n                        action = None\n\n                    if action is not None:\n                        if browser_call_ids and browser_tool_index < len(\n                            browser_call_ids\n                        ):\n                            web_search_call_id = browser_call_ids[browser_tool_index]\n                        else:\n                            web_search_call_id = f\"ws_{uuid.uuid4().hex}\"\n                        browser_tool_index += 1\n                        output.append(\n                            WebSearchCallItem(\n                                type=\"web_search_call\",\n                                id=web_search_call_id,\n                                action=action,\n                            )\n                        )\n                elif (\n                    len(entry_dict.get(\"recipient\", \"\")) > 0\n                    and entry_dict[\"recipient\"].startswith(\"python\")\n                    and python_tool is not None\n                ):\n                    if python_call_ids and python_tool_index < len(python_call_ids):\n                        code_call_id = python_call_ids[python_tool_index]\n                    else:\n                        code_call_id = f\"ci_{uuid.uuid4().hex}\"\n                    python_tool_index += 1\n                    output.append(\n                        CodeInterpreterCallItem(\n                            type=\"code_interpreter_call\",\n                            id=code_call_id,\n                        )\n                    )\n                elif entry_dict[\"channel\"] == \"final\":\n                    content = []\n                    for content_entry in entry_dict[\"content\"]:\n                        if browser_tool:\n                            text_content, annotation_entries, _has_partial_citations = (\n                                browser_tool.normalize_citations(content_entry[\"text\"])\n                            )\n                            annotations = [UrlCitation(**a) for a in annotation_entries]\n                        else:\n                            text_content = content_entry[\"text\"]\n                            annotations = []\n\n                        content.append(\n                            TextContentItem(\n                                type=\"output_text\",\n                                text=text_content,\n                                annotations=annotations,\n                            )\n                        )\n\n                    output.append(\n                        Item(\n                            type=\"message\",\n                            role=\"assistant\",\n                            content=content,\n                            status=\"completed\",\n                        )\n                    )\n                elif entry_dict[\"channel\"] == \"analysis\":\n                    summary = []\n                    content = [\n                        ReasoningTextContentItem(\n                            type=\"reasoning_text\",\n                            text=entry[\"text\"],\n                        )\n                        for entry in entry_dict[\"content\"]\n                    ]\n                    output.append(\n                        ReasoningItem(\n                            type=\"reasoning\",\n                            summary=summary,\n                            content=content,\n                        )\n                    )\n        else:\n            output = []\n\n        usage = (\n            Usage(\n                input_tokens=len(input_tokens),\n                output_tokens=len(output_tokens),\n                total_tokens=len(input_tokens) + len(output_tokens),\n            )\n            if len(output_tokens) > 0\n            else None\n        )\n\n        try:\n            debug_str = encoding.decode_utf8(input_tokens + output_tokens)\n        except Exception:\n            debug_str = input_tokens + output_tokens\n        try:\n            debug_input_str = encoding.decode_utf8(input_tokens)\n        except Exception:\n            debug_input_str = input_tokens\n        try:\n            debug_output_str = encoding.decode_utf8(output_tokens)\n        except Exception:\n            debug_output_str = output_tokens\n\n        metadata = (\n            {\n                \"__debug\": debug_str,\n                \"__debug_input\": debug_input_str,\n                \"__debug_output\": debug_output_str,\n            }\n            if debug_mode\n            else {}\n        )\n\n        return ResponseObject(\n            created_at=int(datetime.datetime.now().timestamp()),\n            status=\"completed\",\n            output=output,\n            text={\"format\": {\"type\": \"text\"}},\n            usage=usage,\n            max_output_tokens=request_body.max_output_tokens,\n            error=error,\n            metadata=metadata,\n            id=response_id,\n            previous_response_id=previous_response_id,\n        )\n\n    class StreamResponsesEvents:\n        initial_tokens: list[int]\n        tokens: list[int]\n        output_tokens: list[int]\n        output_text: str\n        request_body: ResponsesRequest\n        request: Request\n        sequence_number: int\n\n        def __init__(\n            self,\n            initial_tokens,\n            request_body: ResponsesRequest,\n            as_sse: bool = False,\n            request: Optional[Request] = None,\n            response_id: Optional[str] = None,\n            store_callback: Optional[\n                Callable[[str, ResponsesRequest, ResponseObject], None]\n            ] = None,\n            browser_tool: Optional[SimpleBrowserTool] = None,\n            python_tool: Optional[PythonTool] = None,\n        ):\n            self.initial_tokens = initial_tokens\n            self.tokens = initial_tokens.copy()\n            self.output_tokens = []\n            self.output_text = \"\"\n            self.request_body = request_body\n            self.parser = StreamableParser(encoding, role=Role.ASSISTANT)\n            self.as_sse = as_sse\n            self.debug_mode = request_body.metadata.get(\n                \"__debug\", False\n            )  # we use this for demo purposes\n            # Set temperature for this stream, fallback to DEFAULT_TEMPERATURE if not set\n            self.temperature = (\n                request_body.temperature\n                if request_body.temperature is not None\n                else DEFAULT_TEMPERATURE\n            )\n            self.request = request\n            self.sequence_number = 0\n            self.function_call_ids: list[tuple[str, str]] = []\n            self.response_id = response_id\n            self.store_callback = store_callback\n            self.new_request = True\n            self.browser_tool = browser_tool\n            self.use_browser_tool = browser_tool is not None\n            self.browser_call_ids: list[str] = []\n            self.python_tool = python_tool\n            self.use_code_interpreter = python_tool is not None\n            self.python_call_ids: list[str] = []\n\n        def _send_event(self, event: ResponseEvent):\n            event.sequence_number = self.sequence_number\n            self.sequence_number += 1\n            if self.as_sse:\n                return f\"event: {event.type}\\ndata: {event.model_dump_json(indent=None)}\\n\\n\"\n            else:\n                return event\n\n        async def run(self):\n            browser_tool = self.browser_tool\n            self.new_request = True\n            initial_response = generate_response(\n                self.initial_tokens,\n                self.output_tokens,\n                self.request_body,\n                function_call_ids=self.function_call_ids,\n                response_id=self.response_id,\n                previous_response_id=self.request_body.previous_response_id,\n                browser_tool=self.browser_tool,\n                browser_call_ids=self.browser_call_ids,\n                python_tool=self.python_tool,\n                python_call_ids=self.python_call_ids,\n            )\n            initial_response.status = \"in_progress\"\n            yield self._send_event(\n                ResponseCreatedEvent(\n                    type=\"response.created\",\n                    response=initial_response,\n                )\n            )\n            yield self._send_event(\n                ResponseInProgressEvent(\n                    type=\"response.in_progress\",\n                    response=initial_response,\n                )\n            )\n\n            current_content_index = (\n                0  # for this implementation we will always have one content item only\n            )\n            current_output_index = -1\n            sent_output_item_added = False\n\n            # we use this if the model outputs a citation to buffer until completed\n            output_delta_buffer = \"\"\n            # we use this to track the current output text content for things like providing the right indices in citations\n            current_output_text_content = \"\"\n            current_annotations = []\n\n            while True:\n                # Check for client disconnect\n                if self.request is not None and await self.request.is_disconnected():\n                    print(\"Client disconnected, stopping token generation.\")\n                    break\n                next_tok = infer_next_token(\n                    self.tokens,\n                    temperature=self.temperature,\n                    new_request=self.new_request,\n                )\n                self.new_request = False\n                self.tokens.append(next_tok)\n                try:\n                    self.parser.process(next_tok)\n                except Exception:\n                    pass\n\n                if self.parser.state == StreamState.EXPECT_START:\n                    current_output_index += 1\n                    sent_output_item_added = False\n\n                    if len(self.parser.messages) > 0:\n                        previous_item = self.parser.messages[-1]\n                        if previous_item.recipient is not None:\n                            recipient = previous_item.recipient\n                            if (\n                                not recipient.startswith(\"browser.\")\n                                and not recipient == \"python\"\n                            ):\n                                fc_id = f\"fc_{uuid.uuid4().hex}\"\n                                call_id = f\"call_{uuid.uuid4().hex}\"\n                                self.function_call_ids.append((fc_id, call_id))\n                                yield self._send_event(\n                                    ResponseOutputItemDone(\n                                        type=\"response.output_item.done\",\n                                        output_index=current_output_index,\n                                        item=FunctionCallItem(\n                                            type=\"function_call\",\n                                            name=(\n                                                previous_item.recipient[\n                                                    len(\"functions.\") :\n                                                ]\n                                                if previous_item.recipient.startswith(\n                                                    \"functions.\"\n                                                )\n                                                else previous_item.recipient\n                                            ),\n                                            arguments=previous_item.content[0].text,\n                                            id=fc_id,\n                                            call_id=call_id,\n                                        ),\n                                    )\n                                )\n                        if previous_item.channel == \"analysis\":\n                            yield self._send_event(\n                                ResponseReasoningTextDone(\n                                    type=\"response.reasoning_text.done\",\n                                    output_index=current_output_index,\n                                    content_index=current_content_index,\n                                    text=previous_item.content[0].text,\n                                )\n                            )\n                            yield self._send_event(\n                                ResponseContentPartDone(\n                                    type=\"response.content_part.done\",\n                                    output_index=current_output_index,\n                                    content_index=current_content_index,\n                                    part=ReasoningTextContentItem(\n                                        type=\"reasoning_text\",\n                                        text=previous_item.content[0].text,\n                                    ),\n                                )\n                            )\n                            yield self._send_event(\n                                ResponseOutputItemDone(\n                                    type=\"response.output_item.done\",\n                                    output_index=current_output_index,\n                                    item=ReasoningItem(\n                                        type=\"reasoning\",\n                                        summary=[],\n                                        content=[\n                                            ReasoningTextContentItem(\n                                                type=\"reasoning_text\",\n                                                text=previous_item.content[0].text,\n                                            )\n                                        ],\n                                    ),\n                                )\n                            )\n                        if previous_item.channel == \"final\":\n                            annotations = [\n                                UrlCitation(**a) for a in current_annotations\n                            ]\n                            if browser_tool:\n                                (\n                                    normalized_text,\n                                    _annotations,\n                                    _has_partial_citations,\n                                ) = browser_tool.normalize_citations(\n                                    previous_item.content[0].text\n                                )\n                            else:\n                                normalized_text = previous_item.content[0].text\n                                annotations = []\n                            text_content = TextContentItem(\n                                type=\"output_text\",\n                                text=normalized_text,\n                                annotations=annotations,\n                            )\n                            yield self._send_event(\n                                ResponseOutputTextDone(\n                                    type=\"response.output_text.done\",\n                                    output_index=current_output_index,\n                                    content_index=current_content_index,\n                                    text=normalized_text,\n                                )\n                            )\n                            yield self._send_event(\n                                ResponseContentPartDone(\n                                    type=\"response.content_part.done\",\n                                    output_index=current_output_index,\n                                    content_index=current_content_index,\n                                    part=text_content,\n                                )\n                            )\n                            yield self._send_event(\n                                ResponseOutputItemDone(\n                                    type=\"response.output_item.done\",\n                                    output_index=current_output_index,\n                                    item=Item(\n                                        type=\"message\",\n                                        role=\"assistant\",\n                                        content=[text_content],\n                                    ),\n                                )\n                            )\n                            current_annotations = []\n                            current_output_text_content = \"\"\n\n                if (\n                    self.parser.last_content_delta\n                    and self.parser.current_channel == \"final\"\n                    and self.parser.current_recipient is None\n                ):\n                    if not sent_output_item_added:\n                        sent_output_item_added = True\n                        yield self._send_event(\n                            ResponseOutputItemAdded(\n                                type=\"response.output_item.added\",\n                                output_index=current_output_index,\n                                item=Item(type=\"message\", role=\"assistant\", content=[]),\n                            )\n                        )\n                        yield self._send_event(\n                            ResponseContentPartAdded(\n                                type=\"response.content_part.added\",\n                                output_index=current_output_index,\n                                content_index=current_content_index,\n                                part=TextContentItem(type=\"output_text\", text=\"\"),\n                            )\n                        )\n\n                    output_delta_buffer += self.parser.last_content_delta\n                    should_send_output_text_delta = True\n                    if browser_tool:\n                        # we normalize on the full current text to get the right indices in citations\n                        updated_output_text, annotations, has_partial_citations = (\n                            browser_tool.normalize_citations(\n                                current_output_text_content + output_delta_buffer\n                            )\n                        )\n                        # remove the current text to get back the delta but now normalized\n                        output_delta_buffer = updated_output_text[\n                            len(current_output_text_content) :\n                        ]\n\n                        # Filter annotations to only include those whose start_index is not already present in current_annotations\n                        # this is to avoid sending duplicate annotations as multiple annotations can't be in the same place\n                        existing_start_indices = {\n                            a[\"start_index\"] for a in current_annotations\n                        }\n                        new_annotations = [\n                            a\n                            for a in annotations\n                            if a[\"start_index\"] not in existing_start_indices\n                        ]\n                        for a in new_annotations:\n                            current_annotations.append(a)\n                            citation = UrlCitation(**a)\n                            yield self._send_event(\n                                ResponseOutputTextAnnotationAdded(\n                                    type=\"response.output_text.annotation.added\",\n                                    output_index=current_output_index,\n                                    content_index=current_content_index,\n                                    annotation_index=len(current_annotations),\n                                    annotation=citation,\n                                )\n                            )\n\n                        if has_partial_citations:\n                            should_send_output_text_delta = False\n\n                    if should_send_output_text_delta:\n                        yield self._send_event(\n                            ResponseOutputTextDelta(\n                                type=\"response.output_text.delta\",\n                                output_index=current_output_index,\n                                content_index=current_content_index,\n                                delta=output_delta_buffer,\n                            )\n                        )\n                        current_output_text_content += output_delta_buffer\n                        output_delta_buffer = \"\"\n\n                if (\n                    self.parser.last_content_delta\n                    and self.parser.current_channel == \"analysis\"\n                    and self.parser.current_recipient is None\n                ):\n                    if not sent_output_item_added:\n                        sent_output_item_added = True\n                        yield self._send_event(\n                            ResponseOutputItemAdded(\n                                type=\"response.output_item.added\",\n                                output_index=current_output_index,\n                                item=ReasoningItem(\n                                    type=\"reasoning\", summary=[], content=[]\n                                ),\n                            )\n                        )\n                        yield self._send_event(\n                            ResponseContentPartAdded(\n                                type=\"response.content_part.added\",\n                                output_index=current_output_index,\n                                content_index=current_content_index,\n                                part=ReasoningTextContentItem(\n                                    type=\"reasoning_text\", text=\"\"\n                                ),\n                            )\n                        )\n                    yield self._send_event(\n                        ResponseReasoningTextDelta(\n                            type=\"response.reasoning_text.delta\",\n                            output_index=current_output_index,\n                            content_index=current_content_index,\n                            delta=self.parser.last_content_delta,\n                        )\n                    )\n\n                try:\n                    # purely for debugging purposes\n                    output_token_text = encoding.decode_utf8([next_tok])\n                    self.output_text += output_token_text\n                    print(output_token_text, end=\"\", flush=True)\n\n                except RuntimeError:\n                    pass\n\n                if next_tok in encoding.stop_tokens_for_assistant_actions():\n                    if len(self.parser.messages) > 0:\n                        last_message = self.parser.messages[-1]\n                        if (\n                            self.use_browser_tool\n                            and last_message.recipient is not None\n                            and last_message.recipient.startswith(\"browser.\")\n                        ):\n                            function_name = last_message.recipient[len(\"browser.\") :]\n                            action = None\n                            parsed_args = browser_tool.process_arguments(last_message)\n                            if function_name == \"search\":\n                                action = WebSearchActionSearch(\n                                    type=\"search\",\n                                    query=parsed_args[\"query\"],\n                                )\n                            elif function_name == \"open\":\n                                action = WebSearchActionOpenPage(\n                                    type=\"open_page\",\n                                    url=(\n                                        parsed_args[\"url\"]\n                                        if \"url\" in parsed_args\n                                        else None\n                                    ),\n                                )\n                            elif function_name == \"find\":\n                                action = WebSearchActionFind(\n                                    type=\"find\",\n                                    pattern=parsed_args[\"pattern\"],\n                                    url=(\n                                        parsed_args[\"url\"]\n                                        if \"url\" in parsed_args\n                                        else None\n                                    ),\n                                )\n\n                            if action is not None:\n                                web_search_call_id = f\"ws_{uuid.uuid4().hex}\"\n                                self.browser_call_ids.append(web_search_call_id)\n                                yield self._send_event(\n                                    ResponseOutputItemAdded(\n                                        type=\"response.output_item.added\",\n                                        output_index=current_output_index,\n                                        item=WebSearchCallItem(\n                                            type=\"web_search_call\",\n                                            id=web_search_call_id,\n                                            action=action,\n                                        ),\n                                    )\n                                )\n                                yield self._send_event(\n                                    ResponseWebSearchCallInProgress(\n                                        type=\"response.web_search_call.in_progress\",\n                                        output_index=current_output_index,\n                                        id=web_search_call_id,\n                                    )\n                                )\n\n                            async def run_tool():\n                                results = []\n                                async for msg in browser_tool.process(last_message):\n                                    results.append(msg)\n                                return results\n\n                            yield self._send_event(\n                                ResponseWebSearchCallSearching(\n                                    type=\"response.web_search_call.searching\",\n                                    output_index=current_output_index,\n                                    id=web_search_call_id,\n                                )\n                            )\n                            result = await run_tool()\n\n                            new_tokens = encoding.render_conversation_for_completion(\n                                Conversation.from_messages(result), Role.ASSISTANT\n                            )\n\n                            print(encoding.decode_utf8(new_tokens))\n                            self.output_tokens.append(next_tok)\n                            self.tokens.append(\n                                encoding.encode(\"<|end|>\", allowed_special=\"all\")[0]\n                            )\n\n                            for token in new_tokens:\n                                self.parser.process(token)\n                                self.output_tokens.append(token)\n                                self.tokens.append(token)\n\n                            yield self._send_event(\n                                ResponseWebSearchCallCompleted(\n                                    type=\"response.web_search_call.completed\",\n                                    output_index=current_output_index,\n                                    id=web_search_call_id,\n                                )\n                            )\n                            yield self._send_event(\n                                ResponseOutputItemDone(\n                                    type=\"response.output_item.done\",\n                                    output_index=current_output_index,\n                                    item=WebSearchCallItem(\n                                        type=\"web_search_call\",\n                                        id=web_search_call_id,\n                                        action=action,\n                                    ),\n                                )\n                            )\n\n                            current_output_index += 1\n                            self.new_request = True\n\n                            continue\n\n                        elif (\n                            self.use_code_interpreter\n                            and last_message.recipient is not None\n                            and last_message.recipient.startswith(\"python\")\n                        ):\n                            code_call_id = f\"ci_{uuid.uuid4().hex}\"\n                            self.python_call_ids.append(code_call_id)\n                            yield self._send_event(\n                                ResponseOutputItemAdded(\n                                    type=\"response.output_item.added\",\n                                    output_index=current_output_index,\n                                    item=CodeInterpreterCallItem(\n                                        type=\"code_interpreter_call\",\n                                        id=code_call_id,\n                                    ),\n                                )\n                            )\n                            yield self._send_event(\n                                ResponseCodeInterpreterCallInProgress(\n                                    type=\"response.code_interpreter_call.in_progress\",\n                                    output_index=current_output_index,\n                                    id=code_call_id,\n                                )\n                            )\n\n                            async def run_python_tool():\n                                results = []\n                                async for msg in self.python_tool.process(last_message):\n                                    results.append(msg)\n                                return results\n\n                            result = await run_python_tool()\n\n                            print(result)\n\n                            new_tokens = encoding.render_conversation_for_completion(\n                                Conversation.from_messages(result), Role.ASSISTANT\n                            )\n\n                            print(encoding.decode_utf8(new_tokens))\n                            self.output_tokens.append(next_tok)\n                            self.tokens.append(\n                                encoding.encode(\"<|end|>\", allowed_special=\"all\")[0]\n                            )\n\n                            for token in new_tokens:\n                                self.parser.process(token)\n                                self.output_tokens.append(token)\n                                self.tokens.append(token)\n\n                            yield self._send_event(\n                                ResponseCodeInterpreterCallCompleted(\n                                    type=\"response.code_interpreter_call.completed\",\n                                    output_index=current_output_index,\n                                    id=code_call_id,\n                                )\n                            )\n                            yield self._send_event(\n                                ResponseOutputItemDone(\n                                    type=\"response.output_item.done\",\n                                    output_index=current_output_index,\n                                    item=CodeInterpreterCallItem(\n                                        type=\"code_interpreter_call\",\n                                        id=code_call_id,\n                                    ),\n                                )\n                            )\n\n                            current_output_index += 1\n                            self.new_request = True\n\n                            continue\n\n                        else:\n                            break\n                    else:\n                        raise ValueError(\"No messages to process\")\n                if len(self.output_tokens) >= self.request_body.max_output_tokens:\n                    break\n\n                # Adding in the end if we know we are not done\n                self.output_tokens.append(next_tok)\n\n            if self.request is None or not await self.request.is_disconnected():\n                response = generate_response(\n                    self.initial_tokens,\n                    self.output_tokens,\n                    self.request_body,\n                    debug_mode=self.debug_mode,\n                    function_call_ids=self.function_call_ids,\n                    response_id=self.response_id,\n                    previous_response_id=self.request_body.previous_response_id,\n                    browser_tool=self.browser_tool,\n                    browser_call_ids=self.browser_call_ids,\n                )\n                if self.store_callback and self.request_body.store:\n                    self.store_callback(self.response_id, self.request_body, response)\n                yield self._send_event(\n                    ResponseCompletedEvent(\n                        type=\"response.completed\",\n                        response=response,\n                    )\n                )\n\n    @app.post(\"/v1/responses\", response_model=ResponseObject)\n    async def generate(body: ResponsesRequest, request: Request):\n        print(\"request received\")\n\n        use_browser_tool = any(\n            getattr(tool, \"type\", None) == \"browser_search\"\n            for tool in (body.tools or [])\n        )\n        use_code_interpreter = any(\n            getattr(tool, \"type\", None) == \"code_interpreter\"\n            for tool in (body.tools or [])\n        )\n\n        if use_browser_tool:\n            tool_backend = os.getenv(\"BROWSER_BACKEND\", \"exa\")\n            if tool_backend == \"youcom\":\n                backend = YouComBackend(source=\"web\")\n            elif tool_backend == \"exa\":\n                backend = ExaBackend(source=\"web\")\n            else:\n                raise ValueError(f\"Invalid tool backend: {tool_backend}\")\n            browser_tool = SimpleBrowserTool(backend=backend)\n        else:\n            browser_tool = None\n\n        if use_code_interpreter:\n            python_tool = PythonTool()\n        else:\n            python_tool = None\n\n        if body.previous_response_id:\n            prev = responses_store.get(body.previous_response_id)\n            if prev:\n                prev_req, prev_resp = prev\n\n                def _ensure_list(inp):\n                    if isinstance(inp, str):\n                        return [\n                            Item(\n                                type=\"message\",\n                                role=\"user\",\n                                content=[TextContentItem(type=\"input_text\", text=inp)],\n                            )\n                        ]\n                    return list(inp)\n\n                merged_input = _ensure_list(prev_req.input) + list(prev_resp.output)\n                merged_input.extend(_ensure_list(body.input))\n\n                if body.instructions is None:\n                    body.instructions = prev_req.instructions\n                body.input = merged_input\n\n        system_message_content = SystemContent.new().with_conversation_start_date(\n            datetime.datetime.now().strftime(\"%Y-%m-%d\")\n        )\n\n        if body.reasoning is not None:\n            try:\n\n                reasoning_effort = get_reasoning_effort(body.reasoning.effort)\n            except ValueError as e:\n                from fastapi import HTTPException\n\n                raise HTTPException(status_code=422, detail=str(e))\n            system_message_content = system_message_content.with_reasoning_effort(\n                reasoning_effort\n            )\n\n        if use_browser_tool:\n            system_message_content = system_message_content.with_tools(\n                browser_tool.tool_config\n            )\n        if use_code_interpreter:\n            system_message_content = system_message_content.with_tools(\n                python_tool.tool_config\n            )\n\n        system_message = Message.from_role_and_content(\n            Role.SYSTEM, system_message_content\n        )\n        messages = [system_message]\n\n        if body.instructions or body.tools:\n            developer_message_content = DeveloperContent.new().with_instructions(\n                body.instructions\n            )\n\n            tools = []\n            for tool in body.tools:\n                if tool.type == \"function\":\n                    tools.append(\n                        ToolDescription.new(\n                            tool.name,\n                            tool.description,\n                            tool.parameters,\n                        )\n                    )\n\n            if tools:\n                developer_message_content = (\n                    developer_message_content.with_function_tools(tools)\n                )\n\n            developer_message = Message.from_role_and_content(\n                Role.DEVELOPER, developer_message_content\n            )\n\n            messages.append(developer_message)\n\n        if isinstance(body.input, str):\n            user_message = Message.from_role_and_content(Role.USER, body.input)\n            messages.append(user_message)\n        else:\n            is_last_message_function_call_output = (\n                len(body.input) > 0 and body.input[-1].type == \"function_call_output\"\n            )\n            function_call_map = {}\n            # Find the index of the last assistant message\n            last_assistant_idx = -1\n            for idx, item in enumerate(body.input):\n                if item.type == \"message\" and item.role == Role.ASSISTANT:\n                    last_assistant_idx = idx\n\n            for idx, item in enumerate(body.input):\n                if item.type == \"message\":\n                    # TODO: add system prompt handling\n                    if isinstance(item.content, str):\n                        messages.append(\n                            Message.from_role_and_content(item.role, item.content)\n                        )\n                    else:\n                        for content_item in item.content:\n                            messages.append(\n                                Message.from_role_and_content(\n                                    item.role, content_item.text\n                                )\n                            )\n                    # add final channel to the last assistant message if it's from the assistant\n                    if item.role == Role.ASSISTANT:\n                        messages[-1] = messages[-1].with_channel(\"final\")\n                elif item.type == \"reasoning\":\n                    # Only include reasoning if it is after the last assistant message and we are handling a function call at the moment\n                    if (\n                        idx > last_assistant_idx\n                        and is_last_message_function_call_output\n                    ):\n                        for content_item in item.content:\n                            messages.append(\n                                Message.from_role_and_content(\n                                    Role.ASSISTANT, content_item.text\n                                ).with_channel(\"analysis\")\n                            )\n                elif item.type == \"function_call\":\n                    function_call_map[item.call_id] = item\n                    messages.append(\n                        Message.from_role_and_content(Role.ASSISTANT, item.arguments)\n                        .with_recipient(f\"functions.{item.name}\")\n                        .with_channel(\"commentary\")\n                    )\n                elif item.type == \"function_call_output\":\n                    function_call = function_call_map.get(item.call_id, None)\n                    if not function_call:\n                        raise ValueError(f\"Function call {item.call_id} not found\")\n\n                    messages.append(\n                        Message.from_author_and_content(\n                            Author.new(Role.TOOL, f\"functions.{function_call.name}\"),\n                            item.output,\n                        )\n                        .with_recipient(\"assistant\")\n                        .with_channel(\"commentary\")\n                    )\n\n        conversation = Conversation.from_messages(messages)\n\n        initial_tokens = encoding.render_conversation_for_completion(\n            conversation, Role.ASSISTANT\n        )\n        print(encoding.decode_utf8(initial_tokens))\n        response_id = f\"resp_{uuid.uuid4().hex}\"\n\n        def store_callback(rid: str, req: ResponsesRequest, resp: ResponseObject):\n            responses_store[rid] = (req, resp)\n\n        event_stream = StreamResponsesEvents(\n            initial_tokens,\n            body,\n            as_sse=body.stream,\n            request=request,\n            response_id=response_id,\n            store_callback=store_callback,\n            browser_tool=browser_tool,\n            python_tool=python_tool,\n        )\n\n        if body.stream:\n            return StreamingResponse(event_stream.run(), media_type=\"text/event-stream\")\n        else:\n            last_event = None\n            async for event in event_stream.run():\n                last_event = event\n\n            return last_event.response\n\n    return app\n",
        "gpt_oss/responses_api/events.py": "# torchrun --nproc-per-node=4 responses_api.py\nfrom typing import Literal, Optional, Union\n\nfrom pydantic import BaseModel\n\nfrom .types import (\n    CodeInterpreterCallItem,\n    FunctionCallItem,\n    Item,\n    ReasoningItem,\n    ReasoningTextContentItem,\n    ResponseObject,\n    TextContentItem,\n    UrlCitation,\n    WebSearchCallItem,\n)\n\n\nclass ResponseEvent(BaseModel):\n    sequence_number: Optional[int] = 1\n\n\nclass ResponseCreatedEvent(ResponseEvent):\n    type: Literal[\"response.created\"]\n    response: ResponseObject\n\n\nclass ResponseCompletedEvent(ResponseEvent):\n    type: Literal[\"response.completed\"]\n    response: ResponseObject\n\n\nclass ResponseOutputTextDelta(ResponseEvent):\n    type: Literal[\"response.output_text.delta\"] = \"response.output_text.delta\"\n    item_id: str = \"item_1234\"\n    output_index: int = 0\n    content_index: int = 0\n    delta: str = \"\"\n    logprobs: list = []\n\n\nclass ResponseReasoningSummaryTextDelta(ResponseEvent):\n    type: Literal[\"response.reasoning_summary_text.delta\"] = (\n        \"response.reasoning_summary_text.delta\"\n    )\n    item_id: str = \"item_1234\"\n    output_index: int = 0\n    content_index: int = 0\n    delta: str = \"\"\n\n\nclass ResponseReasoningTextDelta(ResponseEvent):\n    type: Literal[\"response.reasoning_text.delta\"] = \"response.reasoning_text.delta\"\n    item_id: str = \"item_1234\"\n    output_index: int = 0\n    content_index: int = 0\n    delta: str = \"\"\n\n\nclass ResponseReasoningTextDone(ResponseEvent):\n    type: Literal[\"response.reasoning_text.done\"] = \"response.reasoning_text.done\"\n    item_id: str = \"item_1234\"\n    output_index: int = 0\n    content_index: int = 0\n    text: str = \"\"\n\n\nclass ResponseOutputItemAdded(ResponseEvent):\n    type: Literal[\"response.output_item.added\"] = \"response.output_item.added\"\n    output_index: int = 0\n    item: Union[\n        Item,\n        ReasoningItem,\n        FunctionCallItem,\n        WebSearchCallItem,\n        CodeInterpreterCallItem,\n    ]\n\n\nclass ResponseOutputItemDone(ResponseEvent):\n    type: Literal[\"response.output_item.done\"] = \"response.output_item.done\"\n    output_index: int = 0\n    item: Union[\n        Item,\n        ReasoningItem,\n        FunctionCallItem,\n        WebSearchCallItem,\n        CodeInterpreterCallItem,\n    ]\n\n\nclass ResponseInProgressEvent(ResponseEvent):\n    type: Literal[\"response.in_progress\"]\n    response: ResponseObject\n\n\nclass ResponseContentPartAdded(ResponseEvent):\n    type: Literal[\"response.content_part.added\"] = \"response.content_part.added\"\n    item_id: str = \"item_1234\"\n    output_index: int = 0\n    content_index: int = 0\n    part: Union[TextContentItem, ReasoningTextContentItem]\n\n\nclass ResponseOutputTextDone(ResponseEvent):\n    type: Literal[\"response.output_text.done\"] = \"response.output_text.done\"\n    item_id: str = \"item_1234\"\n    output_index: int = 0\n    content_index: int = 0\n    text: str = \"\"\n    logprobs: list = []\n\n\nclass ResponseContentPartDone(ResponseEvent):\n    type: Literal[\"response.content_part.done\"] = \"response.content_part.done\"\n    item_id: str = \"item_1234\"\n    output_index: int = 0\n    content_index: int = 0\n    part: Union[TextContentItem, ReasoningTextContentItem]\n\n\nclass ResponseOutputTextAnnotationAdded(ResponseEvent):\n    type: Literal[\"response.output_text.annotation.added\"] = (\n        \"response.output_text.annotation.added\"\n    )\n    item_id: str = \"item_1234\"\n    output_index: int = 0\n    content_index: int = 0\n    annotation_index: int = 0\n    annotation: UrlCitation\n\n\nclass ResponseWebSearchCallInProgress(ResponseEvent):\n    type: Literal[\"response.web_search_call.in_progress\"] = (\n        \"response.web_search_call.in_progress\"\n    )\n    output_index: int = 0\n    item_id: str = \"item_1234\"\n\n\nclass ResponseWebSearchCallSearching(ResponseEvent):\n    type: Literal[\"response.web_search_call.searching\"] = (\n        \"response.web_search_call.searching\"\n    )\n    output_index: int = 0\n    item_id: str = \"item_1234\"\n\n\nclass ResponseWebSearchCallCompleted(ResponseEvent):\n    type: Literal[\"response.web_search_call.completed\"] = (\n        \"response.web_search_call.completed\"\n    )\n    output_index: int = 0\n    item_id: str = \"item_1234\"\n\n\nclass ResponseCodeInterpreterCallInProgress(ResponseEvent):\n    type: Literal[\"response.code_interpreter_call.in_progress\"] = (\n        \"response.code_interpreter_call.in_progress\"\n    )\n    output_index: int = 0\n    item_id: str = \"item_1234\"\n\n\nclass ResponseCodeInterpreterCallCompleted(ResponseEvent):\n    type: Literal[\"response.code_interpreter_call.completed\"] = (\n        \"response.code_interpreter_call.completed\"\n    )\n    output_index: int = 0\n    item_id: str = \"item_1234\"\n",
        "gpt_oss/responses_api/inference/__init__.py": "",
        "gpt_oss/responses_api/inference/metal.py": "\"\"\"Metal backend for :mod:`gpt_oss.responses_api`.\"\"\"\n\nfrom typing import Callable\n\nfrom gpt_oss.metal import Context, Model\n\n\n# Tunables\nMAX_OUTPUT_TOKENS = 100\n\n\ndef setup_model(checkpoint: str) -> Callable[[list[int], float], int]:\n    \"\"\"Load the Metal model and return an inference function.\"\"\"\n\n    model = Model(checkpoint)\n    context = Context(model)\n\n    seed = 0\n    output_tokens = []\n\n    def infer_next_token(\n        tokens: list[int], temperature: float = 0.0, new_request: bool = False\n    ) -> int:\n        \"\"\"Infer next token using incremental LCP caching when possible.\"\"\"\n        nonlocal output_tokens\n\n        if new_request:\n            output_tokens = []\n\n        if len(output_tokens) == 0:\n            # Context handles LCP caching internally; if `tokens` matches the\n            # tokens in the KV cache, the KV cache is reused after reset+append.\n            context.reset()\n            for t in tokens:\n                context.append(t)\n\n            output_tokens = context.sample(max_output_tokens=MAX_OUTPUT_TOKENS,\n                                           temperature=temperature,\n                                           seed=seed)\n\n        return int(output_tokens.pop(0))\n\n    return infer_next_token\n",
        "gpt_oss/responses_api/inference/ollama.py": "\"\"\"\nNOTE: this is a stitched together implementation that uses Ollama for inference. It's primarily used\nfor testing and development. It does not leverage any prompt caching or other optimizations and\ncan therefore be slow between turns.\n\"\"\"\n\nimport json\nimport threading\nimport time\nfrom typing import Callable, Optional\n\nimport requests\nfrom openai_harmony import HarmonyEncodingName, load_harmony_encoding\n\nEOS_TOKEN = 200002  # only used on hard timeout\n\n# Tunables\nPOLL_INTERVAL_S = 0.01  # 10ms between buffer checks\nCALL_MAX_WAIT_S = 0.250  # max time to block inside a single infer call\nNO_TOKEN_TIMEOUT_S = 15.0  # overall inactivity timeout before emitting EOS\nFIRST_BYTE_TIMEOUT_S = 30.0  # time to wait for first token before EOS\n\n# Shared state\n_token_buffer: list[int] = []\n_buffer_lock = threading.Lock()\n_stream_thread: Optional[threading.Thread] = None\n_stream_done = threading.Event()\n_stream_error: Optional[Exception] = None\n_last_progress_ts: float = 0.0  # updated whenever we enqueue or dequeue tokens\n_previous_request_tokens: list[int] = []\n\n\ndef lcp(cache: list[int], inp: list[int]) -> list[int]:\n    i = 0\n    max_len = min(len(cache), len(inp))\n    while i < max_len and cache[i] == inp[i]:\n        i += 1\n    return cache[:i]\n\n\ndef _now():\n    return time.monotonic()\n\n\ndef _touch_progress():\n    global _last_progress_ts\n    _last_progress_ts = _now()\n\n\ndef _reset_stream_state():\n    global _token_buffer, _stream_thread, _stream_error\n    with _buffer_lock:\n        _token_buffer = []\n    _stream_done.clear()\n    _stream_thread = None\n    _stream_error = None\n    _touch_progress()\n\n\ndef setup_model(checkpoint: str) -> Callable[[list[int], float, bool], int]:\n    encoding = load_harmony_encoding(HarmonyEncodingName.HARMONY_GPT_OSS)\n    model_name = checkpoint\n\n    def _start_stream(token_ids: list[int], temperature: float):\n        prompt_text = encoding.decode(token_ids)\n\n        def run():\n            nonlocal prompt_text, temperature\n            global _stream_error\n            global _previous_request_tokens\n\n            accum_text = \"\"\n            last_len = 0  # number of tokens already emitted\n\n            try:\n                url = \"http://localhost:11434/api/generate\"\n\n                payload = {\n                    \"model\": model_name,\n                    \"prompt\": prompt_text,\n                    \"stream\": True,\n                    \"options\": {\"temperature\": temperature},\n                    \"raw\": True,\n                }\n\n                with requests.post(url, json=payload, stream=True, timeout=60) as resp:\n                    resp.raise_for_status()\n                    for line in resp.iter_lines(decode_unicode=True):\n                        if not line:\n                            continue\n                        obj = json.loads(line)\n\n                        if isinstance(obj.get(\"response\"), str):\n                            accum_text += obj[\"response\"]\n                            toks = encoding.encode(accum_text, allowed_special=\"all\")\n                            if len(toks) > last_len:\n                                new_toks = toks[last_len:]\n                                with _buffer_lock:\n                                    _token_buffer.extend(new_toks)\n                                last_len = len(toks)\n                                _touch_progress()\n\n                        if obj.get(\"done\", False):\n                            _token_buffer.append(EOS_TOKEN)\n                            last_len = len(toks)\n                            _touch_progress()\n                            break\n\n                _stream_done.set()\n\n            except Exception as e:\n                _stream_error = e\n                _stream_done.set()\n\n        t = threading.Thread(target=run, name=\"ollama-stream\", daemon=True)\n        t.start()\n        return t\n\n    def infer_next_token(\n        tokens: list[int], temperature: float = 0.0, new_request: bool = False\n    ) -> int:\n        \"\"\"\n        - Starts a new Ollama stream on new_request.\n        - Forwards tokens as they arrive.\n        - Only emits EOS_TOKEN if we exceed an inactivity timeout.\n        \"\"\"\n        global _stream_thread\n\n        if new_request:\n            _reset_stream_state()\n            _stream_thread = _start_stream(token_ids=tokens, temperature=temperature)\n            # Wait for first byte within FIRST_BYTE_TIMEOUT_S (without emitting EOS early)\n            start = _now()\n            while _now() - start < FIRST_BYTE_TIMEOUT_S:\n                with _buffer_lock:\n                    if _token_buffer:\n                        tok = _token_buffer.pop(0)\n                        _touch_progress()\n                        return tok\n                if _stream_error is not None:\n                    raise RuntimeError(f\"Ollama stream error: {_stream_error!r}\")\n                # If Ollama finished instantly with no output, continue loop until timeout\n                time.sleep(POLL_INTERVAL_S)\n            # Hard first-byte timeout -> emit EOS so the server can stop this request\n            return EOS_TOKEN\n\n        if _stream_error is not None:\n            raise RuntimeError(f\"Ollama stream error: {_stream_error!r}\")\n\n        # Normal path: wait up to CALL_MAX_WAIT_S for a token to arrive\n        wait_start = _now()\n        while _now() - wait_start < CALL_MAX_WAIT_S:\n            with _buffer_lock:\n                if _token_buffer:\n                    tok = _token_buffer.pop(0)\n                    _touch_progress()\n                    return tok\n            # No token yet; if we've been idle too long overall, end with EOS\n            if _now() - _last_progress_ts > NO_TOKEN_TIMEOUT_S:\n                return EOS_TOKEN\n            time.sleep(POLL_INTERVAL_S)\n\n        # Still no token in this call slice. Do NOT send EOS unless we've timed out.\n        if _now() - _last_progress_ts > NO_TOKEN_TIMEOUT_S:\n            return EOS_TOKEN\n\n        # Tell caller to call us again; block minimally by returning *nothing new*.\n        # We must return an int; safest is to wait a tiny bit longer for a token.\n        # If still none, keep returning only after short waits. Avoid EOS here.\n        # One more short wait to reduce hot-looping:\n        time.sleep(POLL_INTERVAL_S)\n        with _buffer_lock:\n            if _token_buffer:\n                tok = _token_buffer.pop(0)\n                _touch_progress()\n                return tok\n\n        # As a last resort for this call slice, return EOS only on true inactivity timeout.\n        if _now() - _last_progress_ts > NO_TOKEN_TIMEOUT_S:\n            return EOS_TOKEN\n\n        # If we reach here, we still haven't got a token—ask the caller to call again soon.\n        # Return a harmless token that the server will replace/ignore if your interface supports it.\n        # If your interface does NOT allow a sentinel, keep the short-blocking behavior above.\n        return (\n            EOS_TOKEN if False else 0\n        )  # replace `0` with a PAD/NOOP token your server ignores\n\n    return infer_next_token\n",
        "gpt_oss/responses_api/inference/stub.py": "import time\nfrom typing import Callable\n\nfake_tokens = [\n    200005,\n    35644,\n    200008,\n    23483,\n    316,\n    1199,\n    1114,\n    717,\n    170154,\n    13,\n    200007,\n    200006,\n    173781,\n    200005,\n    35644,\n    316,\n    28,\n    44580,\n    775,\n    170154,\n    464,\n    91,\n    542,\n    141043,\n    91,\n    29,\n    4108,\n    200008,\n    10848,\n    7693,\n    7534,\n    28499,\n    18826,\n    18583,\n    200012,\n]\nfake_tokens = [\n    200005,\n    35644,\n    200008,\n    1844,\n    31064,\n    25,\n    392,\n    4827,\n    382,\n    220,\n    17,\n    659,\n    220,\n    17,\n    16842,\n    12295,\n    81645,\n    13,\n    51441,\n    6052,\n    13,\n    200007,\n    200006,\n    173781,\n    200005,\n    17196,\n    200008,\n    17,\n    659,\n    220,\n    17,\n    314,\n    220,\n    19,\n    13,\n    9552,\n    238,\n    242,\n    200002,\n]\n# fake_tokens = [200005, 35644, 200008, 976, 1825, 31064, 25, 392, 25216, 29400, 290, 11122, 306, 52768, 2117, 16842, 1416, 1309, 316, 2281, 198, 68, 290, 2208, 11122, 13, 1416, 679, 261, 1114, 717, 170154, 484, 44390, 261, 5100, 1621, 26, 581, 1757, 2005, 198, 75, 480, 483, 5100, 392, 137956, 2117, 11, 13180, 4050, 7801, 4733, 290, 11122, 5377, 484, 290, 1114, 7377, 13, 1416, 1309, 260, 198, 78, 1199, 290, 1114, 4584, 364, 58369, 2421, 717, 170154, 483, 5100, 392, 137956, 2117, 11, 13180, 4050, 200007, 200006, 173781, 200005, 12606, 815, 260, 198, 78, 28, 117673, 3490]\n# fake_tokens = [\n#     198,\n#     200005,\n#     35644,\n#     200008,\n#     23483,\n#     316,\n#     1199,\n#     1114,\n#     717,\n#     170154,\n#     13,\n#     200007,\n#     200006,\n#     173781,\n#     200005,\n#     12606,\n#     815,\n#     316,\n#     32455,\n#     106847,\n#     316,\n#     28,\n#     44580,\n#     775,\n#     170154,\n#     464,\n#     91,\n#     542,\n#     141043,\n#     91,\n#     29,\n#     4108,\n#     200008,\n#     10848,\n#     7693,\n#     7534,\n#     28499,\n#     18826,\n#     18583,\n#     200012,\n#     198,\n# ]\n\ntoken_queue = fake_tokens.copy()\n\n\ndef stub_infer_next_token(\n    tokens: list[int], temperature: float = 0.0, new_request: bool = False\n) -> int:\n    global token_queue\n    next_tok = token_queue.pop(0)\n    if len(token_queue) == 0:\n        token_queue = fake_tokens.copy()\n    time.sleep(0.1)\n    return next_tok\n\n\ndef setup_model(_checkpoint: str) -> Callable[[list[int], float], int]:\n    return stub_infer_next_token\n",
        "gpt_oss/responses_api/inference/transformers.py": "\"\"\"\nNOTE: this is not the most efficient way to use transformers. It's a simple implementation that infers\none token at a time to mimic the behavior of the Triton implementation.\n\"\"\"\n\nimport os\nfrom typing import Callable, List\n\n# Transformers imports\nfrom transformers import AutoModelForCausalLM, PreTrainedModel\nimport torch\n\n\nDEFAULT_TEMPERATURE = 0.0\nTP = os.environ.get(\"TP\", 2)\n\ndef load_model(checkpoint: str):\n    \"\"\"\n    Serve the model directly with the Auto API.\n    \"\"\"\n\n    model = AutoModelForCausalLM.from_pretrained(\n        checkpoint,\n        torch_dtype=torch.bfloat16,\n        device_map=\"auto\",\n    )\n\n    return model\n\n\ndef get_infer_next_token(model: PreTrainedModel):\n    \"\"\"\n    Return a callable with the same shape as the original triton implementation:\n      infer_next_token(tokens: List[int], temperature: float, new_request: bool) -> int\n\n    Implementation detail:\n      - We issue a single-token generation with using model.generate\n      - generate handles sampling (temperature=0 => greedy, otherwise, sampling).\n    \"\"\"\n\n    def infer_next_token(\n        tokens: List[int],\n        temperature: float = DEFAULT_TEMPERATURE,\n        new_request: bool = False, # kept for interface compatibility; unused here\n    ) -> int:\n        tokens = torch.tensor([tokens], dtype=torch.int64, device=model.device)\n        output = model.generate(tokens, max_new_tokens=1, do_sample=temperature != 0, temperature=temperature)\n        return output[0, -1].tolist()\n\n    return infer_next_token\n\n\ndef setup_model(checkpoint: str) -> Callable[[List[int], float, bool], int]:\n    model = load_model(checkpoint)\n    infer_next_token = get_infer_next_token(model)\n    return infer_next_token\n",
        "gpt_oss/responses_api/inference/triton.py": "import datetime\nimport os\nfrom typing import Callable\n\nos.environ[\"PYTORCH_CUDA_ALLOC_CONF\"] = \"expandable_segments:True\"\nimport torch\nimport torch.distributed as dist\n\nfrom gpt_oss.triton.model import Cache, ModelConfig, Transformer\n\nDEFAULT_TEMPERATURE = 0.0\nCONTEXT = 16_384\nCONCURRENT_SESSIONS = 1\n\nrank = int(\n    os.environ.get(\"RANK\", 0)\n)  # set this env var to another value to run on other GPUs\n\n\ndef load_model(checkpoint: str):\n    print(f\"[{rank}] loading model...\")\n\n    torch.cuda.set_device(rank)\n    torch.set_grad_enabled(False)\n    device = torch.device(f\"cuda:{rank}\")\n\n    # Load model\n    model = Transformer.from_checkpoint(checkpoint, device=device)\n\n    print(f\"[{rank}] loaded\")\n    return model, device\n\n\ndef get_infer_next_token(model, device):\n    caches = [\n        Cache(CONCURRENT_SESSIONS, CONTEXT, model.config.num_key_value_heads)\n        for _ in range(len(model.block))\n    ]\n    # offsets = torch.zeros(CONCURRENT_SESSIONS, dtype=torch.int32, device=device) # TBD\n    input_token = torch.zeros(\n        1, dtype=torch.int32, device=device\n    )  # add concurrent sessions support\n    tokens_so_far = []\n\n    model.prefill(torch.zeros(1, 4, dtype=torch.int32, device=device), caches)\n    graph = torch.cuda.CUDAGraph()\n    with torch.cuda.graph(graph):\n        logits = model(input_token[None, :], caches=caches)[0]\n\n    def lcp(cache: list[int], inp: list[int]) -> list[int]:\n        i = 0\n        max_len = min(len(cache), len(inp))\n        while i < max_len and cache[i] == inp[i]:\n            i += 1\n        return cache[:i]\n\n    def sample_next_token(\n        logits: torch.Tensor, temperature: float = DEFAULT_TEMPERATURE\n    ) -> int:\n        \"\"\"Executed only on rank 0.\"\"\"\n        if temperature == 0.0:\n            return torch.argmax(logits[-1, :], dim=-1).item()\n        probs = torch.softmax(logits * (1.0 / temperature), dim=-1)\n        return torch.multinomial(probs[-1, :], num_samples=1).item()\n\n    @torch.inference_mode()\n    def infer_next_token(\n        tokens: list[int],\n        temperature: float = DEFAULT_TEMPERATURE,\n        new_request: bool = False,\n    ) -> int:\n        nonlocal tokens_so_far\n        tokens_so_far = lcp(tokens_so_far, tokens)\n        for cache in caches:\n            cache.truncate(len(tokens_so_far))\n        all_tokens = tokens  # for pdb\n        tokens = tokens[len(tokens_so_far) :]\n\n        if len(tokens) > 1:\n            model.prefill(\n                torch.as_tensor(tokens[:-1], dtype=torch.int32, device=device)[None, :],\n                caches,\n            )\n\n        if len(tokens) == 0:\n            breakpoint()\n\n        input_token[-1] = tokens[-1]\n        graph.replay()\n\n        # decide next token on rank‑0\n        next_tok = sample_next_token(logits, temperature=temperature)\n\n        return next_tok\n\n    return infer_next_token\n\n\ndef setup_model(checkpoint: str) -> Callable[[list[int], float], int]:\n    model, device = load_model(checkpoint)\n    infer_next_token = get_infer_next_token(model, device)\n    return infer_next_token\n",
        "gpt_oss/responses_api/inference/vllm.py": "\"\"\"\nNOTE: this is not the most efficient way to use vLLM. It's a simple implementation that infers \none token at a time to mimic the behavior of the Triton implementation. \n\"\"\"\n\nimport os\nfrom typing import Callable, List, Optional\n\n# vLLM imports\nfrom vllm import LLM, SamplingParams\nfrom vllm.inputs import TokensPrompt\n\nDEFAULT_TEMPERATURE = 0.0\nTP = os.environ.get(\"TP\", 2)\n\ndef load_model(checkpoint: str):\n    \"\"\"\n    Create the vLLM engine. We enable prefix caching so repeated prefixes\n    across calls can reuse KV cache for faster prefill.\n    \"\"\"\n\n    llm = LLM(\n        model=checkpoint,\n        tensor_parallel_size=TP,          # set >1 if you want TP across GPUs\n        enable_prefix_caching=True,      # reuse KV for shared prefixes\n        disable_log_stats=True,        # uncomment to quiet logs\n    )\n\n    return llm\n\n\ndef get_infer_next_token(llm: LLM):\n    \"\"\"\n    Return a callable with the same shape as your original:\n      infer_next_token(tokens: List[int], temperature: float, new_request: bool) -> int\n\n    Implementation detail:\n      - We issue a single-token generation with TokensPrompt(prompt_token_ids=tokens).\n      - vLLM handles sampling (temperature=0 => greedy).\n      - With enable_prefix_caching=True, the shared prefix prefill can be reused\n        across calls that share the same prefix.\n    \"\"\"\n\n    # Maintain compatibility with your previous closure signature.\n    def infer_next_token(\n        tokens: List[int],\n        temperature: float = DEFAULT_TEMPERATURE,\n        new_request: bool = False,  # kept for interface compatibility; unused here\n    ) -> int:\n        if not tokens:\n            raise ValueError(\"tokens must contain at least one input token id\")\n\n        sampling = SamplingParams(\n            temperature=float(temperature),\n            max_tokens=1,            # we only want the next token\n            n=1,                     # single continuation\n            # You can expose/enable more controls here (top_p, top_k, etc.)\n        )\n\n        # Provide token IDs directly (no re-tokenization).\n        outputs = llm.generate(\n            TokensPrompt(prompt_token_ids=tokens),\n            sampling_params=sampling,\n        )\n\n        if not outputs or not outputs[0].outputs:\n            raise RuntimeError(\"vLLM returned empty outputs\")\n\n        gen = outputs[0].outputs[0]\n        if not gen.token_ids:\n            # If the model immediately finished (e.g., EOS), decide how you'd like\n            # to signal that. Here we raise; you could also return an EOS id.\n            raise RuntimeError(\"No next token was generated (possibly EOS).\")\n\n        next_tok = int(gen.token_ids[0])\n        return next_tok\n\n    return infer_next_token\n\n\ndef setup_model(checkpoint: str) -> Callable[[List[int], float, bool], int]:\n    llm = load_model(checkpoint)\n    infer_next_token = get_infer_next_token(llm)\n    return infer_next_token\n",
        "gpt_oss/responses_api/serve.py": "# torchrun --nproc-per-node=4 serve.py\n\nimport argparse\n\nimport uvicorn\nfrom openai_harmony import (\n    HarmonyEncodingName,\n    load_harmony_encoding,\n)\n\nfrom .api_server import create_api_server\n\nif __name__ == \"__main__\":\n    parser = argparse.ArgumentParser(description=\"Responses API server\")\n    parser.add_argument(\n        \"--checkpoint\",\n        metavar=\"FILE\",\n        type=str,\n        help=\"Path to the SafeTensors checkpoint\",\n        default=\"~/model\",\n        required=False,\n    )\n    parser.add_argument(\n        \"--port\",\n        metavar=\"PORT\",\n        type=int,\n        default=8000,\n        help=\"Port to run the server on\",\n    )\n    parser.add_argument(\n        \"--inference-backend\",\n        metavar=\"BACKEND\",\n        type=str,\n        help=\"Inference backend to use\",\n        # default to metal on macOS, triton on other platforms\n        default=\"metal\" if __import__(\"platform\").system() == \"Darwin\" else \"triton\",\n    )\n    args = parser.parse_args()\n\n    if args.inference_backend == \"triton\":\n        from .inference.triton import setup_model\n    elif args.inference_backend == \"stub\":\n        from .inference.stub import setup_model\n    elif args.inference_backend == \"metal\":\n        from .inference.metal import setup_model\n    elif args.inference_backend == \"ollama\":\n        from .inference.ollama import setup_model\n    elif args.inference_backend == \"vllm\":\n        from .inference.vllm import setup_model\n    elif args.inference_backend == \"transformers\":\n        from .inference.transformers import setup_model\n    else:\n        raise ValueError(f\"Invalid inference backend: {args.inference_backend}\")\n\n    encoding = load_harmony_encoding(HarmonyEncodingName.HARMONY_GPT_OSS)\n\n    infer_next_token = setup_model(args.checkpoint)\n    uvicorn.run(create_api_server(infer_next_token, encoding), port=args.port)\n",
        "gpt_oss/responses_api/types.py": "from typing import Any, Dict, Literal, Optional, Union\n\nfrom openai_harmony import ReasoningEffort\nfrom pydantic import BaseModel\n\nMODEL_IDENTIFIER = \"gpt-oss-120b\"\nDEFAULT_TEMPERATURE = 0.0\nREASONING_EFFORT = ReasoningEffort.LOW\nDEFAULT_MAX_OUTPUT_TOKENS = 131072\n\n\nclass UrlCitation(BaseModel):\n    type: Literal[\"url_citation\"]\n    end_index: int\n    start_index: int\n    url: str\n    title: str\n\n\nclass TextContentItem(BaseModel):\n    type: Union[Literal[\"text\"], Literal[\"input_text\"], Literal[\"output_text\"]]\n    text: str\n    status: Optional[str] = \"completed\"\n    annotations: Optional[list[UrlCitation]] = None\n\n\nclass SummaryTextContentItem(BaseModel):\n    # using summary for compatibility with the existing API\n    type: Literal[\"summary_text\"]\n    text: str\n\n\nclass ReasoningTextContentItem(BaseModel):\n    type: Literal[\"reasoning_text\"]\n    text: str\n\n\nclass ReasoningItem(BaseModel):\n    id: str = \"rs_1234\"\n    type: Literal[\"reasoning\"]\n    summary: list[SummaryTextContentItem]\n    content: Optional[list[ReasoningTextContentItem]] = []\n\n\nclass Item(BaseModel):\n    type: Optional[Literal[\"message\"]] = \"message\"\n    role: Literal[\"user\", \"assistant\", \"system\"]\n    content: Union[list[TextContentItem], str]\n    status: Union[Literal[\"in_progress\", \"completed\", \"incomplete\"], None] = None\n\n\nclass FunctionCallItem(BaseModel):\n    type: Literal[\"function_call\"]\n    name: str\n    arguments: str\n    status: Literal[\"in_progress\", \"completed\", \"incomplete\"] = \"completed\"\n    id: str = \"fc_1234\"\n    call_id: str = \"call_1234\"\n\n\nclass FunctionCallOutputItem(BaseModel):\n    type: Literal[\"function_call_output\"]\n    call_id: str = \"call_1234\"\n    output: str\n\n\nclass WebSearchActionSearch(BaseModel):\n    type: Literal[\"search\"]\n    query: Optional[str] = None\n\n\nclass WebSearchActionOpenPage(BaseModel):\n    type: Literal[\"open_page\"]\n    url: Optional[str] = None\n\n\nclass WebSearchActionFind(BaseModel):\n    type: Literal[\"find\"]\n    pattern: Optional[str] = None\n    url: Optional[str] = None\n\n\nclass WebSearchCallItem(BaseModel):\n    type: Literal[\"web_search_call\"]\n    id: str = \"ws_1234\"\n    status: Literal[\"in_progress\", \"completed\", \"incomplete\"] = \"completed\"\n    action: Union[WebSearchActionSearch, WebSearchActionOpenPage, WebSearchActionFind]\n\n\nclass CodeInterpreterCallItem(BaseModel):\n    type: Literal[\"code_interpreter_call\"]\n    id: str = \"ci_1234\"\n    status: Literal[\"in_progress\", \"completed\", \"incomplete\"] = \"completed\"\n    input: Optional[str] = None\n\n\nclass Error(BaseModel):\n    code: str\n    message: str\n\n\nclass IncompleteDetails(BaseModel):\n    reason: str\n\n\nclass Usage(BaseModel):\n    input_tokens: int\n    output_tokens: int\n    total_tokens: int\n\n\nclass FunctionToolDefinition(BaseModel):\n    type: Literal[\"function\"]\n    name: str\n    parameters: dict  # this should be typed stricter if you add strict mode\n    strict: bool = False  # change this if you support strict mode\n    description: Optional[str] = \"\"\n\n\nclass BrowserToolConfig(BaseModel):\n    type: Literal[\"browser_search\"]\n\n\nclass CodeInterpreterToolConfig(BaseModel):\n    type: Literal[\"code_interpreter\"]\n\n\nclass ReasoningConfig(BaseModel):\n    effort: Literal[\"low\", \"medium\", \"high\"] = REASONING_EFFORT\n\n\nclass ResponsesRequest(BaseModel):\n    instructions: Optional[str] = None\n    max_output_tokens: Optional[int] = DEFAULT_MAX_OUTPUT_TOKENS\n    input: Union[\n        str,\n        list[\n            Union[\n                Item,\n                ReasoningItem,\n                FunctionCallItem,\n                FunctionCallOutputItem,\n                WebSearchCallItem,\n            ]\n        ],\n    ]\n    model: Optional[str] = MODEL_IDENTIFIER\n    stream: Optional[bool] = False\n    tools: Optional[\n        list[\n            Union[FunctionToolDefinition, BrowserToolConfig, CodeInterpreterToolConfig]\n        ]\n    ] = []\n    reasoning: Optional[ReasoningConfig] = ReasoningConfig()\n    metadata: Optional[Dict[str, Any]] = {}\n    tool_choice: Optional[Literal[\"auto\", \"none\"]] = \"auto\"\n    parallel_tool_calls: Optional[bool] = False\n    store: Optional[bool] = False\n    previous_response_id: Optional[str] = None\n    temperature: Optional[float] = DEFAULT_TEMPERATURE\n    include: Optional[list[str]] = None\n\n\nclass ResponseObject(BaseModel):\n    output: list[\n        Union[\n            Item,\n            ReasoningItem,\n            FunctionCallItem,\n            FunctionCallOutputItem,\n            WebSearchCallItem,\n            CodeInterpreterCallItem,\n        ]\n    ]\n    created_at: int\n    usage: Optional[Usage] = None\n    status: Literal[\"completed\", \"failed\", \"incomplete\", \"in_progress\"] = \"in_progress\"\n    background: None = None\n    error: Optional[Error] = None\n    incomplete_details: Optional[IncompleteDetails] = None\n    instructions: Optional[str] = None\n    max_output_tokens: Optional[int] = None\n    max_tool_calls: Optional[int] = None\n    metadata: Optional[Dict[str, Any]] = {}\n    model: Optional[str] = MODEL_IDENTIFIER\n    parallel_tool_calls: Optional[bool] = False\n    previous_response_id: Optional[str] = None\n    id: Optional[str] = \"resp_1234\"\n    object: Optional[str] = \"response\"\n    text: Optional[Dict[str, Any]] = None\n    tool_choice: Optional[str] = \"auto\"\n    top_p: Optional[int] = 1\n",
        "gpt_oss/responses_api/utils.py": "import time\n\nfake_tokens = [\n    200005,\n    35644,\n    200008,\n    23483,\n    316,\n    1199,\n    1114,\n    717,\n    170154,\n    13,\n    200007,\n    200006,\n    173781,\n    200005,\n    35644,\n    316,\n    28,\n    44580,\n    775,\n    170154,\n    464,\n    91,\n    542,\n    141043,\n    91,\n    29,\n    4108,\n    200008,\n    10848,\n    7693,\n    7534,\n    28499,\n    18826,\n    18583,\n    200012,\n]\nfake_tokens = [\n    200005,\n    35644,\n    200008,\n    1844,\n    31064,\n    25,\n    392,\n    4827,\n    382,\n    220,\n    17,\n    659,\n    220,\n    17,\n    16842,\n    12295,\n    81645,\n    13,\n    51441,\n    6052,\n    13,\n    200007,\n    200006,\n    173781,\n    200005,\n    17196,\n    200008,\n    17,\n    659,\n    220,\n    17,\n    314,\n    220,\n    19,\n    13,\n    9552,\n    238,\n    242,\n    200002,\n]\n# fake_tokens = [200005, 35644, 200008, 976, 1825, 31064, 25, 392, 25216, 29400, 290, 11122, 306, 52768, 2117, 16842, 1416, 1309, 316, 2281, 198, 68, 290, 2208, 11122, 13, 1416, 679, 261, 1114, 717, 170154, 484, 44390, 261, 5100, 1621, 26, 581, 1757, 2005, 198, 75, 480, 483, 5100, 392, 137956, 2117, 11, 13180, 4050, 7801, 4733, 290, 11122, 5377, 484, 290, 1114, 7377, 13, 1416, 1309, 260, 198, 78, 1199, 290, 1114, 4584, 364, 58369, 2421, 717, 170154, 483, 5100, 392, 137956, 2117, 11, 13180, 4050, 200007, 200006, 173781, 200005, 12606, 815, 260, 198, 78, 28, 117673, 3490]\n# fake_tokens = [\n#     198,\n#     200005,\n#     35644,\n#     200008,\n#     23483,\n#     316,\n#     1199,\n#     1114,\n#     717,\n#     170154,\n#     13,\n#     200007,\n#     200006,\n#     173781,\n#     200005,\n#     12606,\n#     815,\n#     316,\n#     32455,\n#     106847,\n#     316,\n#     28,\n#     44580,\n#     775,\n#     170154,\n#     464,\n#     91,\n#     542,\n#     141043,\n#     91,\n#     29,\n#     4108,\n#     200008,\n#     10848,\n#     7693,\n#     7534,\n#     28499,\n#     18826,\n#     18583,\n#     200012,\n#     198,\n# ]\n\ntoken_queue = fake_tokens.copy()\n\n\ndef stub_infer_next_token(tokens: list[int], temperature: float = 0.0) -> int:\n    global token_queue\n    next_tok = token_queue.pop(0)\n    if len(token_queue) == 0:\n        token_queue = fake_tokens.copy()\n    time.sleep(0.1)\n    return next_tok\n",
        "gpt_oss/tokenizer.py": "import tiktoken\n\ndef get_tokenizer():\n    o200k_base = tiktoken.get_encoding(\"o200k_base\")\n    tokenizer = tiktoken.Encoding(\n        name=\"o200k_harmony\",\n        pat_str=o200k_base._pat_str,\n        mergeable_ranks=o200k_base._mergeable_ranks,\n        special_tokens={\n            **o200k_base._special_tokens,\n            \"<|startoftext|>\": 199998,\n            \"<|endoftext|>\": 199999,\n            \"<|reserved_200000|>\": 200000,\n            \"<|reserved_200001|>\": 200001,\n            \"<|return|>\": 200002,\n            \"<|constrain|>\": 200003,\n            \"<|reserved_200004|>\": 200004,\n            \"<|channel|>\": 200005,\n            \"<|start|>\": 200006,\n            \"<|end|>\": 200007,\n            \"<|message|>\": 200008,\n            \"<|reserved_200009|>\": 200009,\n            \"<|reserved_200010|>\": 200010,\n            \"<|reserved_200011|>\": 200011,\n            \"<|call|>\": 200012,\n        } | {\n            f\"<|reserved_{i}|>\": i for i in range(200013, 201088)\n        },\n    )\n    return tokenizer\n",
        "gpt_oss/tools/__init__.py": "",
        "gpt_oss/tools/apply_patch.py": "#!/usr/bin/env python3\n\n\"\"\"\nA self-contained **pure-Python 3.9+** utility for applying human-readable\n“pseudo-diff” patch files to a collection of text files.\n\nSource: https://cookbook.openai.com/examples/gpt4-1_prompting_guide\n\"\"\"\n\nfrom __future__ import annotations\n\nimport pathlib\nfrom dataclasses import dataclass, field\nfrom enum import Enum\nfrom typing import (\n    Callable,\n    Dict,\n    List,\n    Optional,\n    Tuple,\n    Union,\n)\n\n\n# --------------------------------------------------------------------------- #\n#  Domain objects\n# --------------------------------------------------------------------------- #\nclass ActionType(str, Enum):\n    ADD = \"add\"\n    DELETE = \"delete\"\n    UPDATE = \"update\"\n\n\n@dataclass\nclass FileChange:\n    type: ActionType\n    old_content: Optional[str] = None\n    new_content: Optional[str] = None\n    move_path: Optional[str] = None\n\n\n@dataclass\nclass Commit:\n    changes: Dict[str, FileChange] = field(default_factory=dict)\n\n\n# --------------------------------------------------------------------------- #\n#  Exceptions\n# --------------------------------------------------------------------------- #\nclass DiffError(ValueError):\n    \"\"\"Any problem detected while parsing or applying a patch.\"\"\"\n\n\n# --------------------------------------------------------------------------- #\n#  Helper dataclasses used while parsing patches\n# --------------------------------------------------------------------------- #\n@dataclass\nclass Chunk:\n    orig_index: int = -1\n    del_lines: List[str] = field(default_factory=list)\n    ins_lines: List[str] = field(default_factory=list)\n\n\n@dataclass\nclass PatchAction:\n    type: ActionType\n    new_file: Optional[str] = None\n    chunks: List[Chunk] = field(default_factory=list)\n    move_path: Optional[str] = None\n\n\n@dataclass\nclass Patch:\n    actions: Dict[str, PatchAction] = field(default_factory=dict)\n\n\n# --------------------------------------------------------------------------- #\n#  Patch text parser\n# --------------------------------------------------------------------------- #\n@dataclass\nclass Parser:\n    current_files: Dict[str, str]\n    lines: List[str]\n    index: int = 0\n    patch: Patch = field(default_factory=Patch)\n    fuzz: int = 0\n\n    # ------------- low-level helpers -------------------------------------- #\n    def _cur_line(self) -> str:\n        if self.index >= len(self.lines):\n            raise DiffError(\"Unexpected end of input while parsing patch\")\n        return self.lines[self.index]\n\n    @staticmethod\n    def _norm(line: str) -> str:\n        \"\"\"Strip CR so comparisons work for both LF and CRLF input.\"\"\"\n        return line.rstrip(\"\\r\")\n\n    # ------------- scanning convenience ----------------------------------- #\n    def is_done(self, prefixes: Optional[Tuple[str, ...]] = None) -> bool:\n        if self.index >= len(self.lines):\n            return True\n        if (\n            prefixes\n            and len(prefixes) > 0\n            and self._norm(self._cur_line()).startswith(prefixes)\n        ):\n            return True\n        return False\n\n    def startswith(self, prefix: Union[str, Tuple[str, ...]]) -> bool:\n        return self._norm(self._cur_line()).startswith(prefix)\n\n    def read_str(self, prefix: str) -> str:\n        \"\"\"\n        Consume the current line if it starts with *prefix* and return the text\n        **after** the prefix.  Raises if prefix is empty.\n        \"\"\"\n        if prefix == \"\":\n            raise ValueError(\"read_str() requires a non-empty prefix\")\n        if self._norm(self._cur_line()).startswith(prefix):\n            text = self._cur_line()[len(prefix) :]\n            self.index += 1\n            return text\n        return \"\"\n\n    def read_line(self) -> str:\n        \"\"\"Return the current raw line and advance.\"\"\"\n        line = self._cur_line()\n        self.index += 1\n        return line\n\n    # ------------- public entry point -------------------------------------- #\n    def parse(self) -> None:\n        while not self.is_done((\"*** End Patch\",)):\n            # ---------- UPDATE ---------- #\n            path = self.read_str(\"*** Update File: \")\n            if path:\n                if path in self.patch.actions:\n                    raise DiffError(f\"Duplicate update for file: {path}\")\n                move_to = self.read_str(\"*** Move to: \")\n                if path not in self.current_files:\n                    raise DiffError(f\"Update File Error - missing file: {path}\")\n                text = self.current_files[path]\n                action = self._parse_update_file(text)\n                action.move_path = move_to or None\n                self.patch.actions[path] = action\n                continue\n\n            # ---------- DELETE ---------- #\n            path = self.read_str(\"*** Delete File: \")\n            if path:\n                if path in self.patch.actions:\n                    raise DiffError(f\"Duplicate delete for file: {path}\")\n                if path not in self.current_files:\n                    raise DiffError(f\"Delete File Error - missing file: {path}\")\n                self.patch.actions[path] = PatchAction(type=ActionType.DELETE)\n                continue\n\n            # ---------- ADD ---------- #\n            path = self.read_str(\"*** Add File: \")\n            if path:\n                if path in self.patch.actions:\n                    raise DiffError(f\"Duplicate add for file: {path}\")\n                if path in self.current_files:\n                    raise DiffError(f\"Add File Error - file already exists: {path}\")\n                self.patch.actions[path] = self._parse_add_file()\n                continue\n\n            raise DiffError(f\"Unknown line while parsing: {self._cur_line()}\")\n\n        if not self.startswith(\"*** End Patch\"):\n            raise DiffError(\"Missing *** End Patch sentinel\")\n        self.index += 1  # consume sentinel\n\n    # ------------- section parsers ---------------------------------------- #\n    def _parse_update_file(self, text: str) -> PatchAction:\n        action = PatchAction(type=ActionType.UPDATE)\n        lines = text.split(\"\\n\")\n        index = 0\n        while not self.is_done(\n            (\n                \"*** End Patch\",\n                \"*** Update File:\",\n                \"*** Delete File:\",\n                \"*** Add File:\",\n                \"*** End of File\",\n            )\n        ):\n            def_str = self.read_str(\"@@ \")\n            section_str = \"\"\n            if not def_str and self._norm(self._cur_line()) == \"@@\":\n                section_str = self.read_line()\n\n            if not (def_str or section_str or index == 0):\n                raise DiffError(f\"Invalid line in update section:\\n{self._cur_line()}\")\n\n            if def_str.strip():\n                found = False\n                if def_str not in lines[:index]:\n                    for i, s in enumerate(lines[index:], index):\n                        if s == def_str:\n                            index = i + 1\n                            found = True\n                            break\n                if not found and def_str.strip() not in [\n                    s.strip() for s in lines[:index]\n                ]:\n                    for i, s in enumerate(lines[index:], index):\n                        if s.strip() == def_str.strip():\n                            index = i + 1\n                            self.fuzz += 1\n                            found = True\n                            break\n\n            next_ctx, chunks, end_idx, eof = peek_next_section(self.lines, self.index)\n            new_index, fuzz = find_context(lines, next_ctx, index, eof)\n            if new_index == -1:\n                ctx_txt = \"\\n\".join(next_ctx)\n                raise DiffError(\n                    f\"Invalid {'EOF ' if eof else ''}context at {index}:\\n{ctx_txt}\"\n                )\n            self.fuzz += fuzz\n            for ch in chunks:\n                ch.orig_index += new_index\n                action.chunks.append(ch)\n            index = new_index + len(next_ctx)\n            self.index = end_idx\n        return action\n\n    def _parse_add_file(self) -> PatchAction:\n        lines: List[str] = []\n        while not self.is_done(\n            (\"*** End Patch\", \"*** Update File:\", \"*** Delete File:\", \"*** Add File:\")\n        ):\n            s = self.read_line()\n            if not s.startswith(\"+\"):\n                raise DiffError(f\"Invalid Add File line (missing '+'): {s}\")\n            lines.append(s[1:])  # strip leading '+'\n        return PatchAction(type=ActionType.ADD, new_file=\"\\n\".join(lines))\n\n\n# --------------------------------------------------------------------------- #\n#  Helper functions\n# --------------------------------------------------------------------------- #\ndef find_context_core(\n    lines: List[str], context: List[str], start: int\n) -> Tuple[int, int]:\n    if not context:\n        return start, 0\n\n    for i in range(start, len(lines)):\n        if lines[i : i + len(context)] == context:\n            return i, 0\n    for i in range(start, len(lines)):\n        if [s.rstrip() for s in lines[i : i + len(context)]] == [\n            s.rstrip() for s in context\n        ]:\n            return i, 1\n    for i in range(start, len(lines)):\n        if [s.strip() for s in lines[i : i + len(context)]] == [\n            s.strip() for s in context\n        ]:\n            return i, 100\n    return -1, 0\n\n\ndef find_context(\n    lines: List[str], context: List[str], start: int, eof: bool\n) -> Tuple[int, int]:\n    if eof:\n        new_index, fuzz = find_context_core(lines, context, len(lines) - len(context))\n        if new_index != -1:\n            return new_index, fuzz\n        new_index, fuzz = find_context_core(lines, context, start)\n        return new_index, fuzz + 10_000\n    return find_context_core(lines, context, start)\n\n\ndef peek_next_section(\n    lines: List[str], index: int\n) -> Tuple[List[str], List[Chunk], int, bool]:\n    old: List[str] = []\n    del_lines: List[str] = []\n    ins_lines: List[str] = []\n    chunks: List[Chunk] = []\n    mode = \"keep\"\n    orig_index = index\n\n    while index < len(lines):\n        s = lines[index]\n        if s.startswith(\n            (\n                \"@@\",\n                \"*** End Patch\",\n                \"*** Update File:\",\n                \"*** Delete File:\",\n                \"*** Add File:\",\n                \"*** End of File\",\n            )\n        ):\n            break\n        if s == \"***\":\n            break\n        if s.startswith(\"***\"):\n            raise DiffError(f\"Invalid Line: {s}\")\n        index += 1\n\n        last_mode = mode\n        if s == \"\":\n            s = \" \"\n        if s[0] == \"+\":\n            mode = \"add\"\n        elif s[0] == \"-\":\n            mode = \"delete\"\n        elif s[0] == \" \":\n            mode = \"keep\"\n        else:\n            raise DiffError(f\"Invalid Line: {s}\")\n        s = s[1:]\n\n        if mode == \"keep\" and last_mode != mode:\n            if ins_lines or del_lines:\n                chunks.append(\n                    Chunk(\n                        orig_index=len(old) - len(del_lines),\n                        del_lines=del_lines,\n                        ins_lines=ins_lines,\n                    )\n                )\n            del_lines, ins_lines = [], []\n\n        if mode == \"delete\":\n            del_lines.append(s)\n            old.append(s)\n        elif mode == \"add\":\n            ins_lines.append(s)\n        elif mode == \"keep\":\n            old.append(s)\n\n    if ins_lines or del_lines:\n        chunks.append(\n            Chunk(\n                orig_index=len(old) - len(del_lines),\n                del_lines=del_lines,\n                ins_lines=ins_lines,\n            )\n        )\n\n    if index < len(lines) and lines[index] == \"*** End of File\":\n        index += 1\n        return old, chunks, index, True\n\n    if index == orig_index:\n        raise DiffError(\"Nothing in this section\")\n    return old, chunks, index, False\n\n\n# --------------------------------------------------------------------------- #\n#  Patch → Commit and Commit application\n# --------------------------------------------------------------------------- #\ndef _get_updated_file(text: str, action: PatchAction, path: str) -> str:\n    if action.type is not ActionType.UPDATE:\n        raise DiffError(\"_get_updated_file called with non-update action\")\n    orig_lines = text.split(\"\\n\")\n    dest_lines: List[str] = []\n    orig_index = 0\n\n    for chunk in action.chunks:\n        if chunk.orig_index > len(orig_lines):\n            raise DiffError(\n                f\"{path}: chunk.orig_index {chunk.orig_index} exceeds file length\"\n            )\n        if orig_index > chunk.orig_index:\n            raise DiffError(\n                f\"{path}: overlapping chunks at {orig_index} > {chunk.orig_index}\"\n            )\n\n        dest_lines.extend(orig_lines[orig_index : chunk.orig_index])\n        orig_index = chunk.orig_index\n\n        dest_lines.extend(chunk.ins_lines)\n        orig_index += len(chunk.del_lines)\n\n    dest_lines.extend(orig_lines[orig_index:])\n    return \"\\n\".join(dest_lines)\n\n\ndef patch_to_commit(patch: Patch, orig: Dict[str, str]) -> Commit:\n    commit = Commit()\n    for path, action in patch.actions.items():\n        if action.type is ActionType.DELETE:\n            commit.changes[path] = FileChange(\n                type=ActionType.DELETE, old_content=orig[path]\n            )\n        elif action.type is ActionType.ADD:\n            if action.new_file is None:\n                raise DiffError(\"ADD action without file content\")\n            commit.changes[path] = FileChange(\n                type=ActionType.ADD, new_content=action.new_file\n            )\n        elif action.type is ActionType.UPDATE:\n            new_content = _get_updated_file(orig[path], action, path)\n            commit.changes[path] = FileChange(\n                type=ActionType.UPDATE,\n                old_content=orig[path],\n                new_content=new_content,\n                move_path=action.move_path,\n            )\n    return commit\n\n\n# --------------------------------------------------------------------------- #\n#  User-facing helpers\n# --------------------------------------------------------------------------- #\ndef text_to_patch(text: str, orig: Dict[str, str]) -> Tuple[Patch, int]:\n    lines = text.splitlines()  # preserves blank lines, no strip()\n    if (\n        len(lines) < 2\n        or not Parser._norm(lines[0]).startswith(\"*** Begin Patch\")\n        or Parser._norm(lines[-1]) != \"*** End Patch\"\n    ):\n        raise DiffError(\"Invalid patch text - missing sentinels\")\n\n    parser = Parser(current_files=orig, lines=lines, index=1)\n    parser.parse()\n    return parser.patch, parser.fuzz\n\n\ndef identify_files_needed(text: str) -> List[str]:\n    lines = text.splitlines()\n    return [\n        line[len(\"*** Update File: \") :]\n        for line in lines\n        if line.startswith(\"*** Update File: \")\n    ] + [\n        line[len(\"*** Delete File: \") :]\n        for line in lines\n        if line.startswith(\"*** Delete File: \")\n    ]\n\n\ndef identify_files_added(text: str) -> List[str]:\n    lines = text.splitlines()\n    return [\n        line[len(\"*** Add File: \") :]\n        for line in lines\n        if line.startswith(\"*** Add File: \")\n    ]\n\n\n# --------------------------------------------------------------------------- #\n#  File-system helpers\n# --------------------------------------------------------------------------- #\ndef load_files(paths: List[str], open_fn: Callable[[str], str]) -> Dict[str, str]:\n    return {path: open_fn(path) for path in paths}\n\n\ndef apply_commit(\n    commit: Commit,\n    write_fn: Callable[[str, str], None],\n    remove_fn: Callable[[str], None],\n) -> None:\n    for path, change in commit.changes.items():\n        if change.type is ActionType.DELETE:\n            remove_fn(path)\n        elif change.type is ActionType.ADD:\n            if change.new_content is None:\n                raise DiffError(f\"ADD change for {path} has no content\")\n            write_fn(path, change.new_content)\n        elif change.type is ActionType.UPDATE:\n            if change.new_content is None:\n                raise DiffError(f\"UPDATE change for {path} has no new content\")\n            target = change.move_path or path\n            write_fn(target, change.new_content)\n            if change.move_path:\n                remove_fn(path)\n\n\ndef open_file(path: str) -> str:\n    with open(path, \"rt\", encoding=\"utf-8\") as fh:\n        return fh.read()\n\n\ndef write_file(path: str, content: str) -> None:\n    target = pathlib.Path(path)\n    target.parent.mkdir(parents=True, exist_ok=True)\n    with target.open(\"wt\", encoding=\"utf-8\") as fh:\n        fh.write(content)\n\n\ndef remove_file(path: str) -> None:\n    pathlib.Path(path).unlink(missing_ok=True)\n\n\n\ndef apply_patch(\n    text: str,\n    open_fn: Callable[[str], str] = open_file,\n    write_fn: Callable[[str, str], None] = write_file, \n    remove_fn: Callable[[str], None] = remove_file,\n) -> str:\n    if not text.startswith(\"*** Begin Patch\"):\n        raise DiffError(\"Patch text must start with *** Begin Patch\")\n    paths = identify_files_needed(text)\n    orig = load_files(paths, open_fn)\n    patch, _fuzz = text_to_patch(text, orig)\n    commit = patch_to_commit(patch, orig)\n    apply_commit(commit, write_fn, remove_fn)\n    return \"Done!\"\n\n\ndef main() -> None:\n    import sys\n\n    patch_text = sys.stdin.read()\n    if not patch_text:\n        print(\"Please pass patch text through stdin\", file=sys.stderr)\n        return\n    try:\n        result = apply_patch(patch_text)\n    except DiffError as exc:\n        print(exc, file=sys.stderr)\n        return\n    print(result)\n\n\nif __name__ == \"__main__\":\n    main()\n",
        "gpt_oss/tools/python_docker/docker_tool.py": "# Run this before running the tool:\n# $ docker image pull python:3.11\nimport io\nimport tarfile\nfrom typing import Any, AsyncIterator\nimport tempfile\nimport os\nimport subprocess\n\nimport docker\nfrom openai_harmony import (\n    Author,\n    Content,\n    Message,\n    Role,\n    TextContent,\n    ToolNamespaceConfig,\n)\n\nfrom ..tool import Tool\n\n_docker_client = None\n\nPYTHON_EXECUTION_BACKEND = \"docker\"\n\nif os.environ.get(\"PYTHON_EXECUTION_BACKEND\") == \"dangerously_use_uv\":\n    PYTHON_EXECUTION_BACKEND = \"dangerously_use_uv\"\n\n\ndef call_python_script(script: str) -> str:\n    \"\"\"\n    Call a python script by writing it to a file in the container and executing it.\n    \"\"\"\n    global _docker_client\n    if _docker_client is None:\n        _docker_client = docker.from_env()\n        # pull image `python:3.11` if not present\n        try:\n            _docker_client.images.get(\"python:3.11\")\n        except docker.errors.ImageNotFound:\n            _docker_client.images.pull(\"python:3.11\")\n\n    # 1. Create a temporary tar archive containing the script\n    script_name = \"script.py\"\n    tarstream = io.BytesIO()\n    with tarfile.open(fileobj=tarstream, mode=\"w\") as tar:\n        script_bytes = script.encode(\"utf-8\")\n        tarinfo = tarfile.TarInfo(name=script_name)\n        tarinfo.size = len(script_bytes)\n        tar.addfile(tarinfo, io.BytesIO(script_bytes))\n    tarstream.seek(0)\n\n    # 2. Start the container\n    container = _docker_client.containers.create(\n        \"python:3.11\", command=\"sleep infinity\", detach=True\n    )\n    try:\n        container.start()\n        # 3. Put the script into the container\n        container.put_archive(path=\"/tmp\", data=tarstream.read())\n        # 4. Execute the script\n        exec_result = container.exec_run(f\"python /tmp/{script_name}\")\n        output = exec_result.output.decode(\"utf-8\")\n    finally:\n        container.remove(force=True)\n    return output\n\n\ndef call_python_script_with_uv(script: str) -> str:\n    \"\"\"\n    Call a python script by writing it to a file to a temporary directory\n    and executing it with uv.\n    \"\"\"\n    with tempfile.TemporaryDirectory() as temp_dir:\n        script_path = os.path.join(temp_dir, \"script.py\")\n        with open(script_path, \"w\") as f:\n            f.write(script)\n        exec_result = subprocess.run(\n            [\"uv\", \"run\", \"--no-project\", \"python\", script_path],\n            capture_output=True)\n        return exec_result.stdout.decode(\"utf-8\")\n\n\nclass PythonTool(Tool):\n    def __init__(\n        self,\n        name: str = \"python\",\n    ):\n        assert name == \"python\"\n\n    @classmethod\n    def get_tool_name(cls) -> str:\n        return \"python\"\n\n    @property\n    def name(self) -> str:\n        return self.get_tool_name()\n\n    @property\n    def instruction(self) -> str:\n        return \"\"\"\nUse this tool to execute Python code in your chain of thought. The code will not be shown to the user. This tool should be used for internal reasoning, but not for code that is intended to be visible to the user (e.g. when creating plots, tables, or files).\nWhen you send a message containing python code to python, it will be executed in a stateless docker container, and the stdout of that process will be returned to you. You have to use print statements to access the output.\n        \"\"\".strip()\n\n    @property\n    def tool_config(self) -> ToolNamespaceConfig:\n        return ToolNamespaceConfig(\n            name=self.get_tool_name(), description=self.instruction, tools=[]\n        )\n\n    def _make_response(\n        self,\n        output: str,\n        channel: str | None = None,\n    ) -> Message:\n        content = TextContent(text=output)\n        return self.make_response(content=content, channel=channel)\n\n    def make_response(\n        self,\n        content: Content,\n        *,\n        metadata: dict[str, Any] | None = None,\n        author: Author | None = None,\n        channel: str | None = None,\n    ) -> Message:\n        tool_name = self.get_tool_name()\n        author = Author(role=Role.TOOL, name=f\"{tool_name}\")\n\n        message = Message(\n            author=author,\n            content=[content],\n        ).with_recipient(\"assistant\")\n\n        if channel:\n            message = message.with_channel(channel)\n\n        return message\n\n    async def _process(self, message: Message) -> AsyncIterator[Message]:\n        script = message.content[0].text\n        channel = message.channel\n        if PYTHON_EXECUTION_BACKEND == \"docker\":\n            output = call_python_script(script)\n        elif PYTHON_EXECUTION_BACKEND == \"dangerously_use_uv\":\n            output = call_python_script_with_uv(script)\n        else:\n            raise ValueError(\n                f\"Invalid PYTHON_EXECUTION_BACKEND: {PYTHON_EXECUTION_BACKEND}\"\n            )\n        yield self._make_response(output, channel=channel)\n",
        "gpt_oss/tools/simple_browser/__init__.py": "from .simple_browser_tool import SimpleBrowserTool\nfrom .backend import ExaBackend, YouComBackend\n\n__all__ = [\n    \"SimpleBrowserTool\",\n    \"ExaBackend\",\n    \"YouComBackend\",\n]\n",
        "gpt_oss/tools/simple_browser/backend.py": "\"\"\"\nSimple backend for the simple browser tool.\n\"\"\"\n\nimport functools\nimport asyncio\nimport logging\nimport os\nfrom abc import abstractmethod\nfrom typing import Callable, ParamSpec, TypeVar\nfrom urllib.parse import quote\n\nimport chz\nfrom aiohttp import ClientSession, ClientTimeout\nfrom tenacity import (\n    after_log,\n    before_sleep_log,\n    retry,\n    retry_if_exception_type,\n    stop_after_attempt,\n    wait_exponential,\n)\n\nfrom .page_contents import (\n    Extract,\n    FetchResult,\n    PageContents,\n    get_domain,\n    process_html,\n)\n\nlogger = logging.getLogger(__name__)\n\n\nVIEW_SOURCE_PREFIX = \"view-source:\"\n\n\nclass BackendError(Exception):\n    pass\n\n\nP = ParamSpec(\"P\")\nR = TypeVar(\"R\")\n\n\ndef with_retries(\n    func: Callable[P, R],\n    num_retries: int,\n    max_wait_time: float,\n) -> Callable[P, R]:\n    if num_retries > 0:\n        retry_decorator = retry(\n            stop=stop_after_attempt(num_retries),\n            wait=wait_exponential(\n                multiplier=1,\n                min=2,\n                max=max_wait_time,\n            ),\n            before_sleep=before_sleep_log(logger, logging.INFO),\n            after=after_log(logger, logging.INFO),\n            retry=retry_if_exception_type(Exception),\n        )\n        return retry_decorator(func)\n    else:\n        return func\n\n\ndef maybe_truncate(text: str, num_chars: int = 1024) -> str:\n    if len(text) > num_chars:\n        text = text[: (num_chars - 3)] + \"...\"\n    return text\n\n\n@chz.chz(typecheck=True)\nclass Backend:\n    source: str = chz.field(doc=\"Description of the backend source\")\n\n    @abstractmethod\n    async def search(\n        self,\n        query: str,\n        topn: int,\n        session: ClientSession,\n    ) -> PageContents:\n        pass\n\n    @abstractmethod\n    async def fetch(self, url: str, session: ClientSession) -> PageContents:\n        pass\n\n    async def _post(self, session: ClientSession, endpoint: str, payload: dict) -> dict:\n        headers = {\"x-api-key\": self._get_api_key()}\n        async with session.post(f\"{self.BASE_URL}{endpoint}\", json=payload, headers=headers) as resp:\n            if resp.status != 200:\n                raise BackendError(\n                    f\"{self.__class__.__name__} error {resp.status}: {await resp.text()}\"\n                )\n            return await resp.json()\n\n    async def _get(self, session: ClientSession, endpoint: str, params: dict) -> dict:\n        headers = {\"x-api-key\": self._get_api_key()}\n        async with session.get(f\"{self.BASE_URL}{endpoint}\", params=params, headers=headers) as resp:\n            if resp.status != 200:\n                raise BackendError(\n                    f\"{self.__class__.__name__} error {resp.status}: {await resp.text()}\"\n                )\n            return await resp.json()\n\n\n@chz.chz(typecheck=True)\nclass ExaBackend(Backend):\n    \"\"\"Backend that uses the Exa Search API.\"\"\"\n\n    source: str = chz.field(doc=\"Description of the backend source\")\n    api_key: str | None = chz.field(\n        doc=\"Exa API key. Uses EXA_API_KEY environment variable if not provided.\",\n        default=None,\n    )\n\n    BASE_URL: str = \"https://api.exa.ai\"\n\n    def _get_api_key(self) -> str:\n        key = self.api_key or os.environ.get(\"EXA_API_KEY\")\n        if not key:\n            raise BackendError(\"Exa API key not provided\")\n        return key\n\n\n    async def search(\n        self, query: str, topn: int, session: ClientSession\n    ) -> PageContents:\n        data = await self._post(\n            session,\n            \"/search\",\n            {\"query\": query, \"numResults\": topn, \"contents\": {\"text\": True, \"summary\": True}},\n        )\n        # make a simple HTML page to work with browser format\n        titles_and_urls = [\n            (result[\"title\"], result[\"url\"], result[\"summary\"])\n            for result in data[\"results\"]\n        ]\n        html_page = f\"\"\"\n<html><body>\n<h1>Search Results</h1>\n<ul>\n{\"\".join([f\"<li><a href='{url}'>{title}</a> {summary}</li>\" for title, url, summary in titles_and_urls])}\n</ul>\n</body></html>\n\"\"\"\n\n        return process_html(\n            html=html_page,\n            url=\"\",\n            title=query,\n            display_urls=True,\n            session=session,\n        )\n\n    async def fetch(self, url: str, session: ClientSession) -> PageContents:\n        is_view_source = url.startswith(VIEW_SOURCE_PREFIX)\n        if is_view_source:\n            url = url[len(VIEW_SOURCE_PREFIX) :]\n        data = await self._post(\n            session,\n            \"/contents\",\n            {\"urls\": [url], \"text\": { \"includeHtmlTags\": True }},\n        )\n        results = data.get(\"results\", [])\n        if not results:\n            raise BackendError(f\"No contents returned for {url}\")\n        return process_html(\n            html=results[0].get(\"text\", \"\"),\n            url=url,\n            title=results[0].get(\"title\", \"\"),\n            display_urls=True,\n            session=session,\n        )\n\n@chz.chz(typecheck=True)\nclass YouComBackend(Backend):\n    \"\"\"Backend that uses the You.com Search API.\"\"\"\n\n    source: str = chz.field(doc=\"Description of the backend source\")\n\n    BASE_URL: str = \"https://api.ydc-index.io\"\n\n    def _get_api_key(self) -> str:\n        key = os.environ.get(\"YDC_API_KEY\")\n        if not key:\n            raise BackendError(\"You.com API key not provided\")\n        return key\n\n    \n    async def search(\n        self, query: str, topn: int, session: ClientSession\n    ) -> PageContents:\n        data = await self._get(\n            session,\n            \"/v1/search\",\n            {\"query\": query, \"count\": topn},\n        )\n        # make a simple HTML page to work with browser format\n        web_titles_and_urls, news_titles_and_urls = [], []\n        if \"web\" in data[\"results\"]:\n            web_titles_and_urls = [\n                (result[\"title\"], result[\"url\"], result[\"snippets\"])\n                for result in data[\"results\"][\"web\"]\n            ]\n        if \"news\" in data[\"results\"]:\n            news_titles_and_urls = [\n                (result[\"title\"], result[\"url\"], result[\"description\"])\n                for result in data[\"results\"][\"news\"]\n            ]\n        titles_and_urls = web_titles_and_urls + news_titles_and_urls\n        html_page = f\"\"\"\n<html><body>\n<h1>Search Results</h1>\n<ul>\n{\"\".join([f\"<li><a href='{url}'>{title}</a> {summary}</li>\" for title, url, summary in titles_and_urls])}\n</ul>\n</body></html>\n\"\"\"\n\n        return process_html(\n            html=html_page,\n            url=\"\",\n            title=query,\n            display_urls=True,\n            session=session,\n        )\n\n    async def fetch(self, url: str, session: ClientSession) -> PageContents:\n        is_view_source = url.startswith(VIEW_SOURCE_PREFIX)\n        if is_view_source:\n            url = url[len(VIEW_SOURCE_PREFIX) :]\n        data = await self._post(\n            session,\n            \"/v1/contents\",\n            {\"urls\": [url], \"livecrawl_formats\": \"html\"},\n        )\n        if not data:\n            raise BackendError(f\"No contents returned for {url}\")\n        if \"html\" not in data[0]:\n            raise BackendError(f\"No HTML returned for {url}\")\n        return process_html(\n            html=data[0].get(\"html\", \"\"),\n            url=url,\n            title=data[0].get(\"title\", \"\"),\n            display_urls=True,\n            session=session,\n        )\n\n",
        "gpt_oss/tools/simple_browser/page_contents.py": "\"\"\"\nPage contents for the simple browser tool.\n\"\"\"\n\nfrom __future__ import annotations\n\nimport dataclasses\nimport functools\nimport logging\nimport re\nfrom urllib.parse import urljoin, urlparse\n\nimport aiohttp\nimport html2text\nimport lxml\nimport lxml.etree\nimport lxml.html\nimport pydantic\n\nimport tiktoken\n\nlogger = logging.getLogger(__name__)\n\n\nHTML_SUP_RE = re.compile(r\"<sup( [^>]*)?>([\\w\\-]+)</sup>\")\nHTML_SUB_RE = re.compile(r\"<sub( [^>]*)?>([\\w\\-]+)</sub>\")\nHTML_TAGS_SEQ_RE = re.compile(r\"(?<=\\w)((<[^>]*>)+)(?=\\w)\")\nWHITESPACE_ANCHOR_RE = re.compile(r\"(【\\@[^】]+】)(\\s+)\")\nEMPTY_LINE_RE = re.compile(r\"^\\s+$\", flags=re.MULTILINE)\nEXTRA_NEWLINE_RE = re.compile(r\"\\n(\\s*\\n)+\")\n\n\nclass Extract(pydantic.BaseModel):  # A search result snippet or a quotable extract\n    url: str\n    text: str\n    title: str\n    line_idx: int | None = None\n\n\nclass FetchResult(pydantic.BaseModel):\n    url: str\n    success: bool\n    title: str | None = None\n    error_type: str | None = None\n    error_message: str | None = None\n    html: str | None = None\n    raw_content: bytes | None = None\n    plaintext: str | None = None\n\n\nclass PageContents(pydantic.BaseModel):\n    url: str\n    text: str\n    title: str\n    urls: dict[str, str]\n    snippets: dict[str, Extract] | None = None\n    error_message: str | None = None\n\n\n@dataclasses.dataclass(frozen=True)\nclass Tokens:\n    tokens: list[int]\n    tok2idx: list[int]  # Offsets = running sum of lengths.\n\n\ndef get_domain(url: str) -> str:\n    \"\"\"Extracts the domain from a URL.\"\"\"\n    if \"http\" not in url:\n        # If `get_domain` is called on a domain, add a scheme so that the\n        # original domain is returned instead of the empty string.\n        url = \"http://\" + url\n    return urlparse(url).netloc\n\n\ndef multiple_replace(text: str, replacements: dict[str, str]) -> str:\n    \"\"\"Performs multiple string replacements using regex pass.\"\"\"\n    regex = re.compile(\"(%s)\" % \"|\".join(map(re.escape, replacements.keys())))\n    return regex.sub(lambda mo: replacements[mo.group(1)], text)\n\n\n@functools.lru_cache(maxsize=1024)\ndef mark_lines(text: str) -> str:\n    \"\"\"Adds line numbers (ex: 'L0:') to the beginning of each line in a string.\"\"\"\n    # Split the string by newline characters\n    lines = text.split(\"\\n\")\n\n    # Add lines numbers to each line and join into a single string\n    numbered_text = \"\\n\".join([f\"L{i}: {line}\" for i, line in enumerate(lines)])\n    return numbered_text\n\n\n@functools.cache\ndef _tiktoken_vocabulary_lengths(enc_name: str) -> list[int]:\n    \"\"\"Gets the character lengths of all tokens in the specified TikToken vocabulary.\"\"\"\n    encoding = tiktoken.get_encoding(enc_name)\n    return [len(encoding.decode([i])) for i in range(encoding.n_vocab)]\n\n\ndef warmup_caches(enc_names: list[str]) -> None:\n    \"\"\"Warm up the cache by computing token length lists for the given TikToken encodings.\"\"\"\n    for _ in map(_tiktoken_vocabulary_lengths, enc_names):\n        pass\n\n\ndef _replace_special_chars(text: str) -> str:\n    \"\"\"Replaces specific special characters with visually similar alternatives.\"\"\"\n    replacements = {\n        \"【\": \"〖\",\n        \"】\": \"〗\",\n        \"◼\": \"◾\",\n        # \"━\": \"─\",\n        \"\\u200b\": \"\",  # zero width space\n        # Note: not replacing †\n    }\n    return multiple_replace(text, replacements)\n\n\ndef merge_whitespace(text: str) -> str:\n    \"\"\"Replace newlines with spaces and merge consecutive whitespace into a single space.\"\"\"\n    text = text.replace(\"\\n\", \" \")\n    text = re.sub(r\"\\s+\", \" \", text)\n    return text\n\n\ndef arxiv_to_ar5iv(url: str) -> str:\n    \"\"\"Converts an arxiv.org URL to its ar5iv.org equivalent.\"\"\"\n    return re.sub(r\"arxiv.org\", r\"ar5iv.org\", url)\n\n\ndef _clean_links(root: lxml.html.HtmlElement, cur_url: str) -> dict[str, str]:\n    \"\"\"Processes all anchor tags in the HTML, replaces them with a custom format and returns an ID-to-URL mapping.\"\"\"\n    cur_domain = get_domain(cur_url)\n    urls: dict[str, str] = {}\n    urls_rev: dict[str, str] = {}\n    for a in root.findall(\".//a[@href]\"):\n        assert a.getparent() is not None\n        link = a.attrib[\"href\"]\n        if link.startswith((\"mailto:\", \"javascript:\")):\n            continue\n        text = _get_text(a).replace(\"†\", \"‡\")\n        if not re.sub(r\"【\\@([^】]+)】\", \"\", text):  # Probably an image\n            continue\n        if link.startswith(\"#\"):\n            replace_node_with_text(a, text)\n            continue\n        try:\n            link = urljoin(cur_url, link)  # works with both absolute and relative links\n            domain = get_domain(link)\n        except Exception:\n            domain = \"\"\n        if not domain:\n            logger.debug(\"SKIPPING LINK WITH URL %s\", link)\n            continue\n        link = arxiv_to_ar5iv(link)\n        if (link_id := urls_rev.get(link)) is None:\n            link_id = f\"{len(urls)}\"\n            urls[link_id] = link\n            urls_rev[link] = link_id\n        if domain == cur_domain:\n            replacement = f\"【{link_id}†{text}】\"\n        else:\n            replacement = f\"【{link_id}†{text}†{domain}】\"\n        replace_node_with_text(a, replacement)\n    return urls\n\n\ndef _get_text(node: lxml.html.HtmlElement) -> str:\n    \"\"\"Extracts all text from an HTML element and merges it into a whitespace-normalized string.\"\"\"\n    return merge_whitespace(\" \".join(node.itertext()))\n\n\ndef _remove_node(node: lxml.html.HtmlElement) -> None:\n    \"\"\"Removes a node from its parent in the lxml tree.\"\"\"\n    node.getparent().remove(node)\n\n\ndef _escape_md(text: str) -> str:\n    return text\n\n\ndef _escape_md_section(text: str, snob: bool = False) -> str:\n    return text\n\n\ndef html_to_text(html: str) -> str:\n    \"\"\"Converts an HTML string to clean plaintext.\"\"\"\n    html = re.sub(HTML_SUP_RE, r\"^{\\2}\", html)\n    html = re.sub(HTML_SUB_RE, r\"_{\\2}\", html)\n    # add spaces between tags such as table cells\n    html = re.sub(HTML_TAGS_SEQ_RE, r\" \\1\", html)\n    # we don't need to escape markdown, so monkey-patch the logic\n    orig_escape_md = html2text.utils.escape_md\n    orig_escape_md_section = html2text.utils.escape_md_section\n    html2text.utils.escape_md = _escape_md\n    html2text.utils.escape_md_section = _escape_md_section\n    h = html2text.HTML2Text()\n    h.ignore_links = True\n    h.ignore_images = True\n    h.body_width = 0  # no wrapping\n    h.ignore_tables = True\n    h.unicode_snob = True\n    h.ignore_emphasis = True\n    result = h.handle(html).strip()\n    html2text.utils.escape_md = orig_escape_md\n    html2text.utils.escape_md_section = orig_escape_md_section\n    return result\n\n\ndef _remove_math(root: lxml.html.HtmlElement) -> None:\n    \"\"\"Removes all <math> elements from the lxml tree.\"\"\"\n    for node in root.findall(\".//math\"):\n        _remove_node(node)\n\n\ndef remove_unicode_smp(text: str) -> str:\n    \"\"\"Removes Unicode characters in the Supplemental Multilingual Plane (SMP) from `text`.\n\n    SMP characters are not supported by lxml.html processing.\n    \"\"\"\n    smp_pattern = re.compile(r\"[\\U00010000-\\U0001FFFF]\", re.UNICODE)\n    return smp_pattern.sub(\"\", text)\n\n\ndef replace_node_with_text(node: lxml.html.HtmlElement, text: str) -> None:\n    \"\"\"Replaces an lxml node with a text string while preserving surrounding text.\"\"\"\n    previous = node.getprevious()\n    parent = node.getparent()\n    tail = node.tail or \"\"\n    if previous is None:\n        parent.text = (parent.text or \"\") + text + tail\n    else:\n        previous.tail = (previous.tail or \"\") + text + tail\n    parent.remove(node)\n\n\ndef replace_images(\n    root: lxml.html.HtmlElement,\n    base_url: str,\n    session: aiohttp.ClientSession | None,\n) -> None:\n    \"\"\"Finds all image tags and replaces them with numbered placeholders (includes alt/title if available).\"\"\"\n    cnt = 0\n    for img_tag in root.findall(\".//img\"):\n        image_name = img_tag.get(\"alt\", img_tag.get(\"title\"))\n        if image_name:\n            replacement = f\"[Image {cnt}: {image_name}]\"\n        else:\n            replacement = f\"[Image {cnt}]\"\n        replace_node_with_text(img_tag, replacement)\n        cnt += 1\n\n\ndef process_html(\n    html: str,\n    url: str,\n    title: str | None,\n    session: aiohttp.ClientSession | None = None,\n    display_urls: bool = False,\n) -> PageContents:\n    \"\"\"Convert HTML into model-readable version.\"\"\"\n    html = remove_unicode_smp(html)\n    html = _replace_special_chars(html)\n    root = lxml.html.fromstring(html)\n\n    # Parse the title.\n    title_element = root.find(\".//title\")\n    if title:\n        final_title = title\n    elif title_element is not None:\n        final_title = title_element.text or \"\"\n    elif url and (domain := get_domain(url)):\n        final_title = domain\n    else:\n        final_title = \"\"\n\n    urls = _clean_links(root, url)\n    replace_images(\n        root=root,\n        base_url=url,\n        session=session,\n    )\n    _remove_math(root)\n    clean_html = lxml.etree.tostring(root, encoding=\"UTF-8\").decode()\n    text = html_to_text(clean_html)\n    text = re.sub(WHITESPACE_ANCHOR_RE, lambda m: m.group(2) + m.group(1), text)\n    # ^^^ move anchors to the right thru whitespace\n    # This way anchors don't create extra whitespace\n    text = re.sub(EMPTY_LINE_RE, \"\", text)\n    # ^^^ Get rid of empty lines\n    text = re.sub(EXTRA_NEWLINE_RE, \"\\n\\n\", text)\n    # ^^^ Get rid of extra newlines\n\n    top_parts = []\n    if display_urls:\n        top_parts.append(f\"\\nURL: {url}\\n\")\n    # NOTE: Publication date is currently not extracted due\n    # to performance costs.\n\n    return PageContents(\n        url=url,\n        text=\"\".join(top_parts) + text,\n        urls=urls,\n        title=final_title,\n    )\n",
        "gpt_oss/tools/simple_browser/simple_browser_tool.py": "import contextvars\nimport dataclasses\nimport functools\nimport itertools\nimport json\nimport re\nimport textwrap\nfrom typing import Any, AsyncIterator, Callable, ParamSpec, Sequence\nfrom urllib.parse import quote, unquote\n\nimport pydantic\nimport structlog\nimport tiktoken\nfrom aiohttp import ClientSession\nfrom openai_harmony import (\n    Author,\n    Content,\n    Message,\n    Role,\n    TextContent,\n    ToolNamespaceConfig\n)\n\nfrom ..tool import Tool\n\n# from functions import Function, from_python\nfrom .backend import (\n    VIEW_SOURCE_PREFIX,\n    Backend,\n    BackendError,\n    maybe_truncate,\n)\nfrom .page_contents import Extract, PageContents\n\nlogger = structlog.stdlib.get_logger(component=__name__)\n\n\n# TODO(zhuohan): Use the correct encoding at release\nENC_NAME = \"o200k_base\"\nFIND_PAGE_LINK_FORMAT = \"# 【{idx}†{title}】\"\nPARTIAL_INITIAL_LINK_PATTERN = re.compile(r\"^[^【】]*】\")\nPARTIAL_FINAL_LINK_PATTERN = re.compile(\n    r\"【\\d*(?:†(?P<content>[^†】]*)(?:†[^†】]*)?)?$\"\n)\nLINK_PATTERN = re.compile(r\"【\\d+†(?P<content>[^†】]+)(?:†[^†】]+)?】\")\n\nCITATION_OUTPUT_PATTERN = re.compile(r\"【(?P<cursor>\\d+)†(?P<content>[^†】]+)(?:†[^†】]+)?】\")\n\nCallParams = ParamSpec(\"CallParams\")\n\n\n_P = ParamSpec(\"_P\")\n_live_function_name = contextvars.ContextVar[str](\"_live_function_name\")\n\n\nclass ToolUsageError(Exception):\n    pass\n\n\ndef function_the_model_can_call(\n    fn: Callable[_P, AsyncIterator[Message]],\n) -> Callable[_P, AsyncIterator[Message]]:\n    fn.__fn_calling_tool_fn_type__ = \"function_the_model_can_call\"  # type: ignore\n\n    @functools.wraps(fn)\n    async def inner(*args: _P.args, **kwargs: _P.kwargs) -> AsyncIterator[Message]:\n        token = _live_function_name.set(fn.__name__)\n        try:\n            async for m in fn(*args, **kwargs):\n                yield m\n        finally:\n            _live_function_name.reset(token)\n\n    return inner\n\n\n@functools.cache\ndef _tiktoken_vocabulary_lengths(enc_name: str) -> list[int]:\n    encoding = tiktoken.get_encoding(enc_name)\n    results = []\n    for i in range(encoding.n_vocab):\n        try:\n            results.append(len(encoding.decode([i])))\n        except Exception as e:\n            results.append(1)\n    return results\n\n\n@dataclasses.dataclass(frozen=True)\nclass Tokens:\n    tokens: list[int]\n    tok2idx: list[int]  # Offsets = running sum of lengths.\n\n\n@functools.cache\ndef max_chars_per_token(enc_name: str) -> int:\n    \"\"\"Typical value is 128, but let's be safe.\"\"\"\n    tok_lens = _tiktoken_vocabulary_lengths(enc_name)\n    return max(tok_lens)\n\n\ndef get_tokens(text: str, enc_name: str) -> Tokens:\n    encoding = tiktoken.get_encoding(enc_name)\n    tokens = encoding.encode(text, disallowed_special=())\n    _vocabulary_lengths = _tiktoken_vocabulary_lengths(enc_name)\n    tok2idx = [0] + list(itertools.accumulate(_vocabulary_lengths[i] for i in tokens))[\n        :-1\n    ]\n    result = Tokens(tokens=tokens, tok2idx=tok2idx)\n    return result\n\n\ndef get_end_loc(\n    loc: int,\n    num_lines: int,\n    total_lines: int,\n    lines: list[str],\n    view_tokens: int,\n    encoding_name: str,\n) -> int:\n    if num_lines <= 0:\n        # COMPUTE NUMBER OF LINES TO SHOW\n        txt = join_lines(lines[loc:], add_line_numbers=True, offset=loc)\n        # if the text is very short, no need to truncate at all\n        # at least one char per token\n        if len(txt) > view_tokens:\n            # limit the amount of text we tokenize here\n            upper_bound = max_chars_per_token(encoding_name)\n            tok2idx = get_tokens(\n                txt[: (view_tokens + 1) * upper_bound], encoding_name\n            ).tok2idx\n            if len(tok2idx) > view_tokens:\n                end_idx = tok2idx[view_tokens]\n                num_lines = txt[:end_idx].count(\"\\n\") + 1  # round up\n            else:\n                num_lines = total_lines\n        else:\n            num_lines = total_lines\n\n    return min(loc + num_lines, total_lines)\n\n\ndef get_page_metadata(\n    curr_page: PageContents,\n) -> dict[str, str | None | dict[str, str] | list[str]]:\n    \"\"\"Some attributes of the current page.\"\"\"\n    page_metadata: dict[str, str | None | dict[str, str] | list[str]] = {\n        \"url\": curr_page.url,\n        \"title\": curr_page.title,\n    }\n    return page_metadata\n\n\ndef join_lines(\n    lines: list[str], add_line_numbers: bool = False, offset: int = 0\n) -> str:\n    if add_line_numbers:\n        return \"\\n\".join([f\"L{i + offset}: {line}\" for i, line in enumerate(lines)])\n    else:\n        return \"\\n\".join(lines)\n\n\ndef wrap_lines(text: str, width: int = 80) -> list[str]:\n    lines = text.split(\"\\n\")\n    wrapped = itertools.chain.from_iterable(\n        (\n            textwrap.wrap(\n                line, width=width, replace_whitespace=False, drop_whitespace=False\n            )\n            if line\n            else [\"\"]\n        )  # preserve empty lines\n        for line in lines\n    )\n    return list(wrapped)\n\n\ndef strip_links(text: str) -> str:\n    text = re.sub(PARTIAL_INITIAL_LINK_PATTERN, \"\", text)\n    text = re.sub(PARTIAL_FINAL_LINK_PATTERN, lambda mo: mo.group(\"content\"), text)\n    text = re.sub(LINK_PATTERN, lambda mo: mo.group(\"content\"), text)\n    return text\n\n\ndef maybe_get_function_args(\n    message: Message, tool_name: str = \"browser\"\n) -> dict[str, Any] | None:\n    if not message.recipient.startswith(f\"{tool_name}.\"):\n        return None\n\n    contents = \"\"\n    if len(message.content) == 1 and isinstance(message.content[0], TextContent):\n        contents = message.content[0].text\n\n    if not contents:\n        return {}\n\n    try:\n        parsed_contents = json.loads(contents)\n        if isinstance(parsed_contents, dict):\n            return parsed_contents\n    except json.JSONDecodeError:\n        pass\n\n    return None\n\n\nasync def run_find_in_page(\n    pattern: str,\n    page: PageContents,\n    max_results: int = 50,\n    num_show_lines: int = 4,\n) -> PageContents:\n    lines = wrap_lines(text=page.text)\n    txt = join_lines(lines, add_line_numbers=False)\n    without_links = strip_links(txt)\n    lines = without_links.split(\"\\n\")\n\n    result_chunks, snippets = [], []\n    line_idx, match_idx = 0, 0\n    while line_idx < len(lines):\n        line = lines[line_idx]\n        if pattern not in line.lower():\n            line_idx += 1\n            continue\n        snippet = \"\\n\".join(lines[line_idx : line_idx + num_show_lines])\n        link_title = FIND_PAGE_LINK_FORMAT.format(\n            idx=f\"{match_idx}\", title=f\"match at L{line_idx}\"\n        )\n        result_chunks.append(f\"{link_title}\\n{snippet}\")\n        snippets.append(\n            Extract(\n                url=page.url, text=snippet, title=f\"#{match_idx}\", line_idx=line_idx\n            )\n        )\n        if len(result_chunks) == max_results:\n            break\n        match_idx += 1\n        line_idx += num_show_lines\n\n    urls = [page.url for _ in result_chunks]\n\n    if result_chunks:\n        display_text = \"\\n\\n\".join(result_chunks)\n    else:\n        display_text = f\"No `find` results for pattern: `{pattern}`\"\n\n    result_page = PageContents(\n        url=f\"{page.url}/find?pattern={quote(pattern)}\",\n        title=f\"Find results for text: `{pattern}` in `{page.title}`\",\n        text=display_text,\n        urls={str(i): url for i, url in enumerate(urls)},\n        snippets={str(i): snip for i, snip in enumerate(snippets)},\n    )\n    return result_page\n\n\ndef handle_errors(\n    func: Callable[CallParams, AsyncIterator[\"Message\"]],\n) -> Callable[CallParams, AsyncIterator[\"Message\"]]:\n    @functools.wraps(func)\n    async def inner(\n        *args: CallParams.args, **kwargs: CallParams.kwargs\n    ) -> AsyncIterator[Message]:\n        tool = args[0]\n        # Could be cool to type this explicitly, but mypy makes it hard\n        assert isinstance(tool, SimpleBrowserTool)\n        try:\n            async for msg in func(*args, **kwargs):\n                yield msg\n        except (ToolUsageError, BackendError) as e:\n            yield tool.make_error_message(e)\n\n    return inner\n\n\nclass SimpleBrowserState(pydantic.BaseModel):\n    # maps page url to page contents\n    pages: dict[str, PageContents] = pydantic.Field(default_factory=dict)\n    # a sequential list of page urls\n    page_stack: list[str] = pydantic.Field(default_factory=list)\n\n    @property\n    def current_cursor(self) -> int:\n        return len(self.page_stack) - 1\n\n    def add_page(self, page: PageContents) -> None:\n        self.pages[page.url] = page\n        self.page_stack.append(page.url)\n\n    def get_page(self, cursor: int = -1) -> PageContents:\n        if self.current_cursor < 0:\n            raise ToolUsageError(\"No pages to access!\")\n        if cursor == -1 or cursor == self.current_cursor:\n            return self.pages[self.page_stack[-1]]\n        try:\n            page_url = self.page_stack[cursor]\n        except TypeError as e:\n            raise ToolUsageError(\n                f\"`cursor` should be an integer, not `{type(cursor).__name__}`\"\n            ) from e\n        except IndexError as e:\n            raise ToolUsageError(\n                f\"Cursor `{cursor}` is out of range. \"\n                f\"Available cursor indices: [0 - {self.current_cursor}].\"\n            ) from e\n        return self.pages[page_url]\n\n    def get_page_by_url(self, url: str) -> PageContents | None:\n        if url in self.pages:\n            return self.pages[url]\n        return None\n\n    def pop_page_stack(self) -> None:\n        assert self.current_cursor >= 0, \"No page to pop!\"\n        self.page_stack.pop()\n\n\nclass SimpleBrowserTool(Tool):\n    def __init__(\n        self,\n        backend: Backend,\n        encoding_name: str = ENC_NAME,\n        max_search_results: int = 20,\n        tool_state: dict[str, Any] | None = None,\n        view_tokens: int = 1024,\n        name: str = \"browser\",\n    ):\n        assert name == \"browser\"\n        self.backend = backend\n        if tool_state is None:\n            self.tool_state = SimpleBrowserState()\n        else:\n            self.tool_state = SimpleBrowserState.model_validate(tool_state)\n\n        self.encoding_name = encoding_name\n        self.max_search_results = max_search_results\n        self.view_tokens = view_tokens\n\n    def get_tool_state(self) -> dict[str, Any]:\n        return {\"tool_state\": self.tool_state.model_dump()}\n\n    @classmethod\n    def get_tool_name(cls) -> str:\n        return \"browser\"\n\n    @property\n    def name(self) -> str:\n        return self.get_tool_name()\n\n    @property\n    def tool_config(self) -> ToolNamespaceConfig:\n        config = ToolNamespaceConfig.browser()\n        config.name = self.name\n        config.description = \"\"\"Tool for browsing.\nThe `cursor` appears in brackets before each browsing display: `[{cursor}]`.\nCite information from the tool using the following format:\n`【{cursor}†L{line_start}(-L{line_end})?】`, for example: `【6†L9-L11】` or `【8†L3】`.\nDo not quote more than 10 words directly from the tool output.\nsources=\"\"\" + self.backend.source\n        return config\n\n    @property\n    def instruction(self) -> str:\n        return self.tool_config.description\n\n    def _render_browsing_display(\n        self,\n        tether_id: int,\n        result: str,\n        summary: str | None = None,\n    ):\n        to_return = \"\"\n        # Always show summaries.\n        if summary:\n            to_return += summary\n        to_return += result\n        to_return = f\"[{tether_id}] {to_return}\"\n        return to_return\n\n    def _make_response(\n        self,\n        page: PageContents,\n        cursor: int,\n        body: str,\n        scrollbar: str,\n    ) -> Message:\n        domain = maybe_truncate(unquote(page.url))\n        header = f\"{page.title}\"\n        if domain:\n            header += f\" ({domain})\"\n        header += f\"\\n**{scrollbar}**\\n\\n\"\n\n        content = TextContent(text=self._render_browsing_display(cursor, body, header))\n        return self.make_response(\n            content=content, metadata=get_page_metadata(self.tool_state.get_page())\n        )\n\n    async def show_page(self, loc: int = 0, num_lines: int = -1) -> Message:\n        page = self.tool_state.get_page()\n        cursor = self.tool_state.current_cursor\n        lines = wrap_lines(text=page.text)\n        total_lines = len(lines)\n\n        if loc >= total_lines:\n            err_msg = (\n                f\"Invalid location parameter: `{loc}`. \"\n                f\"Cannot exceed page maximum of {total_lines - 1}.\"\n            )\n            raise ToolUsageError(err_msg)\n\n        end_loc = get_end_loc(\n            loc, num_lines, total_lines, lines, self.view_tokens, self.encoding_name\n        )\n\n        lines_to_show = lines[loc:end_loc]\n        body = join_lines(lines_to_show, add_line_numbers=True, offset=loc)\n\n        scrollbar = f\"viewing lines [{loc} - {end_loc - 1}] of {total_lines - 1}\"\n        return self._make_response(page, cursor, body, scrollbar)\n\n    async def show_page_safely(self, loc: int = 0, num_lines: int = -1) -> Message:\n        try:\n            return await self.show_page(loc=loc, num_lines=num_lines)\n        except ToolUsageError as e:\n            self.tool_state.pop_page_stack()\n            raise e\n\n    async def _open_url(self, url: str, direct_url_open: bool) -> PageContents:\n        \"\"\"Use the cache, if available.\"\"\"\n        backend = self.backend\n        # direct_url_open should be regarded as a refresh\n        if not direct_url_open and (page := self.tool_state.get_page_by_url(url)):\n            assert page.url == url\n            return page\n\n        try:\n            async with ClientSession() as session:\n                page = await backend.fetch(url, session=session)\n            return page\n        except Exception as e:\n            msg = maybe_truncate(str(e))\n            logger.warning(\"Error fetching URL in lean browser tool\", exc_info=e)\n            raise BackendError(\n                f\"Error fetching URL `{maybe_truncate(url)}`: {msg}\"\n            ) from e\n\n    def make_error_message(self, error: Exception) -> Message:\n        \"\"\"Uses the message creation codepath from the base class.\"\"\"\n        error_name = error.__class__.__name__\n        content = TextContent(text=str(error))\n        return self.make_response(content=content)\n\n    @function_the_model_can_call\n    @handle_errors\n    async def search(\n        self,\n        query: str,\n        topn: int = 10,\n        top_n: int = 10,\n        source: str | None = None,\n    ) -> AsyncIterator[Message]:\n        del topn\n        del top_n\n        try:\n            async with ClientSession() as session:\n                search_page = await self.backend.search(\n                    query=query,\n                    topn=self.max_search_results,\n                    session=session,\n                )\n        except Exception as e:\n            msg = maybe_truncate(str(e))\n            raise BackendError(f\"Error during search for `{query}`: {msg}\") from e\n\n        self.tool_state.add_page(search_page)\n        yield await self.show_page_safely(loc=0)\n\n    @function_the_model_can_call\n    @handle_errors\n    async def open(\n        self,\n        id: int | str = -1,\n        cursor: int = -1,\n        loc: int = -1,\n        num_lines: int = -1,\n        view_source: bool = False,\n        source: str | None = None,\n    ) -> AsyncIterator[Message]:\n        curr_page: PageContents | None = None\n        stay_on_current_page = False\n        direct_url_open = False\n        if isinstance(id, str):\n            snippet = None\n            url = id\n            direct_url_open = True\n        else:  # Operate on a previously opened page\n            curr_page = self.tool_state.get_page(cursor)\n\n            if id >= 0:  # click a link\n                try:\n                    url = curr_page.urls[str(id)]\n                except KeyError as e:\n                    raise ToolUsageError(f\"Invalid link id `{id}`.\") from e\n                snippet = (curr_page.snippets or {}).get(str(id))\n                if snippet and curr_page.url == \"\":\n                    # current page is a search result page\n                    assert isinstance(snippet, Extract)\n            else:  # navigate to new position on the current page\n                if not view_source:\n                    stay_on_current_page = True\n                url = curr_page.url\n                snippet = None\n\n        new_page: PageContents\n        if view_source:\n            url = f\"{VIEW_SOURCE_PREFIX}{url}\"\n            snippet = None\n        if stay_on_current_page:\n            assert curr_page is not None\n            new_page = curr_page\n        else:\n            new_page = await self._open_url(url, direct_url_open)\n\n        self.tool_state.add_page(new_page)\n\n        if loc < 0:  # unset\n            if snippet is not None and snippet.line_idx is not None:\n                loc = snippet.line_idx\n                if loc > 4:\n                    loc -= 4\n            else:\n                loc = 0\n        yield await self.show_page_safely(loc=loc, num_lines=num_lines)\n\n    @function_the_model_can_call\n    @handle_errors\n    async def find(self, pattern: str, cursor: int = -1) -> AsyncIterator[Message]:\n        page = self.tool_state.get_page(cursor)\n        if page.snippets is not None:\n            raise ToolUsageError(\n                \"Cannot run `find` on search results page or find results page\"\n            )\n\n        pc = await run_find_in_page(\n            pattern=str(pattern).lower(),\n            page=page,\n        )\n        self.tool_state.add_page(pc)\n        yield await self.show_page_safely(loc=0)\n\n    def make_response(\n        self,\n        content: Content,\n        *,\n        metadata: dict[str, Any] | None = None,\n        author: Author | None = None,\n    ) -> Message:\n        \"\"\"\n        Make a response message.\n\n        Should be used from `@function_the_model_can_call` if author is not provided.\n        \"\"\"\n        if author is None:\n            tool_name = self.get_tool_name()\n            function_name = _live_function_name.get()\n            assert function_name is not None\n            author = Author(role=Role.TOOL, name=f\"{tool_name}.{function_name}\")\n\n        return Message(\n            author=author,\n            content=[content],\n        ).with_recipient(\"assistant\")\n\n    def process_arguments(self, message: Message) -> dict[str, Any]:\n        function_args = maybe_get_function_args(message, tool_name=self.name)\n        if function_args is None:\n            raise ValueError(\"Invalid function arguments\")\n\n        if \"cursor\" in function_args and function_args[\"cursor\"] >= 0:\n            page = self.tool_state.get_page(cursor=function_args[\"cursor\"])\n            if \"id\" in function_args:\n                function_args[\"url\"] = page.urls[str(function_args[\"id\"])]\n            else:\n                function_args[\"url\"] = page.url\n        elif \"id\" in function_args and isinstance(function_args[\"id\"], str):\n            function_args[\"url\"] = function_args[\"id\"]\n        return function_args\n\n    async def _process(self, message: Message) -> AsyncIterator[Message]:\n        def make_error_message(error: str) -> Message:\n            return self.make_response(\n                content=TextContent(text=json.dumps({\"error\": error})),\n                author=Author(role=Role.TOOL, name=message.recipient),\n            )\n\n        function_args = maybe_get_function_args(message, tool_name=self.name)\n        if function_args is None:\n            yield make_error_message(\"Invalid function arguments\")\n            return\n\n        _, function_name = message.recipient.split(\".\")\n        if function_name not in [\"search\", \"open\", \"find\"]:\n            yield make_error_message(f\"Unknown function: {function_name}\")\n            return\n\n        if function_name == \"search\":\n            async for msg in self.search(**function_args):\n                yield msg\n        elif function_name == \"open\":\n            async for msg in self.open(**function_args):\n                yield msg\n        elif function_name == \"find\":\n            async for msg in self.find(**function_args):\n                yield msg\n        else:\n            raise ValueError(\"should not be here\")\n\n\n    def normalize_citations(self, old_content: str, hide_partial_citations: bool = False) -> tuple[str, list[dict[str, Any]], bool]:\n        \"\"\"\n        Returns a tuple of (new_message, annotations, has_partial_citations)\n        - new_message: Message with citations replaced by ([domain](url))\n        - annotations: list of dicts with start_index, end_index, and title (url)\n        - has_partial_citations: whether the text includes an unfinished citation\n        \"\"\"\n\n        has_partial_citations = PARTIAL_FINAL_LINK_PATTERN.search(old_content) is not None\n        if hide_partial_citations and has_partial_citations:\n            old_content = PARTIAL_FINAL_LINK_PATTERN.sub(\"\", old_content)\n\n        matches = []\n        for match in CITATION_OUTPUT_PATTERN.finditer(old_content):\n            cursor = match.group(\"cursor\")\n            content = match.group(\"content\")\n            start_idx = match.start()\n            end_idx = match.end()\n            matches.append({\n                \"cursor\": cursor,\n                \"content\": content,\n                \"start\": start_idx,\n                \"end\": end_idx\n            })\n\n        # Build a mapping from cursor to url\n        cursor_to_url = {}\n        for idx, url in enumerate(self.tool_state.page_stack):\n            cursor_to_url[str(idx)] = url\n\n        def extract_domain(url):\n            try:\n                return unquote(url).split(\"/\")[2]\n            except Exception:\n                return url\n\n        new_content = \"\"\n        last_idx = 0\n        annotations = []\n        running_offset = 0  # Offset due to length changes in replacements\n\n        for m in matches:\n            cursor = m[\"cursor\"]\n            url = cursor_to_url.get(cursor, None)\n            orig_start = m[\"start\"]\n            orig_end = m[\"end\"]\n\n            # Add text before the citation\n            new_content += old_content[last_idx:orig_start]\n\n            if url:\n                domain = extract_domain(url)\n                replacement = f\" ([{domain}]({url})) \"\n                # The start and end indices in the new content\n                start_index = len(new_content)\n                end_index = start_index + len(replacement)\n                annotations.append({\n                    \"start_index\": start_index,\n                    \"end_index\": end_index,\n                    \"title\": domain,\n                    \"url\": url,\n                    \"type\": \"url_citation\",\n                })\n                new_content += replacement\n            else:\n                # Keep the original citation format if cursor is missing\n                replacement = old_content[orig_start:orig_end]\n                start_index = len(new_content)\n                end_index = start_index + len(replacement)\n                # No annotation for missing url, but could add if desired\n                new_content += replacement\n\n            last_idx = orig_end\n\n        new_content += old_content[last_idx:]\n        return new_content, annotations, has_partial_citations\n\n",
        "gpt_oss/tools/tool.py": "from abc import ABC, abstractmethod\nfrom uuid import UUID, uuid4\nfrom typing import AsyncIterator\n\nfrom openai_harmony import (\n    Author,\n    Role,\n    Message,\n    TextContent,\n)\n\n\ndef _maybe_update_inplace_and_validate_channel(\n    *, input_message: Message, tool_message: Message\n) -> None:\n    # If the channel of a new message produced by tool is different from the originating message,\n    # we auto-set the new message's channel, if unset, or raise an error.\n    if tool_message.channel != input_message.channel:\n        if tool_message.channel is None:\n            tool_message.channel = input_message.channel\n        else:\n            raise ValueError(\n                f\"Messages from tool should have the same channel ({tool_message.channel=}) as \"\n                f\"the triggering message ({input_message.channel=}).\"\n            )\n\n\nclass Tool(ABC):\n    \"\"\"\n    Something the model can call.\n\n    Tools expose APIs that are shown to the model in a syntax that the model\n    understands and knows how to call (from training data). Tools allow the\n    model to do things like run code, browse the web, etc.\n    \"\"\"\n\n    @property\n    @abstractmethod\n    def name(self) -> str:\n        \"\"\"\n        An identifier for the tool. The convention is that a message will be routed to the tool\n        whose name matches its recipient field.\n        \"\"\"\n\n    @property\n    def output_channel_should_match_input_channel(self) -> bool:\n        \"\"\"\n        A flag which indicates whether the output channel of the tool should match the input channel.\n        \"\"\"\n        return True\n\n    async def process(self, message: Message) -> AsyncIterator[Message]:\n        \"\"\"\n        Process the message and return a list of messages to add to the conversation.\n        The input message should already be applicable to this tool.\n        Don't return the input message, just the new messages.\n\n        If implementing a tool that has to block while calling a function use `call_on_background_thread` to get a coroutine.\n\n        If you just want to test this use `evaluate_generator` to get the results.\n\n        Do not override this method; override `_process` below (to avoid interfering with tracing).\n        \"\"\"\n        async for m in self._process(message):\n            if self.output_channel_should_match_input_channel:\n                _maybe_update_inplace_and_validate_channel(input_message=message, tool_message=m)\n            yield m\n\n    @abstractmethod\n    async def _process(self, message: Message) -> AsyncIterator[Message]:\n        \"\"\"Override this method to provide the implementation of the tool.\"\"\"\n        if False:  # This is to convince the type checker that this is an async generator.\n            yield  # type: ignore[unreachable]\n        _ = message  # Stifle \"unused argument\" warning.\n        raise NotImplementedError\n\n    @abstractmethod\n    def instruction(self) -> str:\n        \"\"\"\n        Describe the tool's functionality. For example, if it accepts python-formatted code,\n        provide documentation on the functions available.\n        \"\"\"\n        raise NotImplementedError\n\n    def instruction_dict(self) -> dict[str, str]:\n        return {self.name: self.instruction()}\n\n    def error_message(\n        self, error_message: str, id: UUID | None = None, channel: str | None = None\n    ) -> Message:\n        \"\"\"\n        Return an error message that's from this tool.\n        \"\"\"\n        return Message(\n            id=id if id else uuid4(),\n            author=Author(role=Role.TOOL, name=self.name),\n            content=TextContent(text=error_message), # TODO: Use SystemError instead\n            channel=channel,\n        ).with_recipient(\"assistant\")\n\n",
        "gpt_oss/torch/__init__.py": "",
        "gpt_oss/torch/model.py": "import json\nimport math\nimport os\nfrom dataclasses import dataclass\n\nimport torch\nimport torch.distributed as dist\n\nfrom gpt_oss.torch.weights import Checkpoint\n\n\n@dataclass\nclass ModelConfig:\n    num_hidden_layers: int = 36\n    num_experts: int = 128\n    experts_per_token: int = 4\n    vocab_size: int = 201088\n    hidden_size: int = 2880\n    intermediate_size: int = 2880\n    swiglu_limit: float = 7.0\n    head_dim: int = 64\n    num_attention_heads: int = 64\n    num_key_value_heads: int = 8\n    sliding_window: int = 128\n    initial_context_length: int = 4096\n    rope_theta: float = 150000.0\n    rope_scaling_factor: float = 32.0\n    rope_ntk_alpha: float = 1.0\n    rope_ntk_beta: float = 32.0\n\n\nclass RMSNorm(torch.nn.Module):\n    def __init__(\n        self, num_features: int, eps: float = 1e-05, device: torch.device | None = None\n    ):\n        super().__init__()\n        self.num_features = num_features\n        self.eps = eps\n        self.scale = torch.nn.Parameter(\n            torch.ones(num_features, device=device, dtype=torch.float32)\n        )\n\n    def forward(self, x: torch.Tensor) -> torch.Tensor:\n        assert x.shape[-1] == self.num_features\n        t, dtype = x.float(), x.dtype\n        t = t * torch.rsqrt(torch.mean(t**2, dim=-1, keepdim=True) + self.eps)\n        return (t * self.scale).to(dtype)\n\n\ndef _apply_rotary_emb(\n    x: torch.Tensor,\n    cos: torch.Tensor,\n    sin: torch.Tensor,\n) -> torch.Tensor:\n    cos = cos.unsqueeze(-2).to(x.dtype)\n    sin = sin.unsqueeze(-2).to(x.dtype)\n    x1, x2 = torch.chunk(x, 2, dim=-1)\n    o1 = x1 * cos - x2 * sin\n    o2 = x2 * cos + x1 * sin\n    return torch.cat((o1, o2), dim=-1)\n\n\nclass RotaryEmbedding(torch.nn.Module):\n    def __init__(\n        self,\n        head_dim: int,\n        base: int,\n        dtype: torch.dtype,\n        initial_context_length: int = 4096,\n        scaling_factor: float = 1.0,\n        ntk_alpha: float = 1.0,\n        ntk_beta: float = 32.0,\n        device: torch.device | None = None,\n    ) -> None:\n        super().__init__()\n        self.head_dim = head_dim\n        self.base = base\n        self.dtype = dtype\n        self.initial_context_length = initial_context_length\n        self.scaling_factor = scaling_factor\n        self.ntk_alpha = ntk_alpha\n        self.ntk_beta = ntk_beta\n        self.device = device\n\n    def _compute_concentration_and_inv_freq(self) -> torch.Tensor:\n        \"\"\"See YaRN paper: https://arxiv.org/abs/2309.00071\"\"\"\n        freq = self.base ** (\n            torch.arange(0, self.head_dim, 2, dtype=torch.float, device=self.device)\n            / self.head_dim\n        )\n        if self.scaling_factor > 1.0:\n            concentration = (\n                0.1 * math.log(self.scaling_factor) + 1.0\n            )  # YaRN concentration\n\n            d_half = self.head_dim / 2\n            # NTK by parts\n            low = (\n                d_half\n                * math.log(self.initial_context_length / (self.ntk_beta * 2 * math.pi))\n                / math.log(self.base)\n            )\n            high = (\n                d_half\n                * math.log(self.initial_context_length / (self.ntk_alpha * 2 * math.pi))\n                / math.log(self.base)\n            )\n            assert 0 < low < high < d_half - 1\n\n            interpolation = 1.0 / (self.scaling_factor * freq)\n            extrapolation = 1.0 / freq\n\n            ramp = (\n                torch.arange(d_half, dtype=torch.float32, device=freq.device) - low\n            ) / (high - low)\n            mask = 1 - ramp.clamp(0, 1)\n\n            inv_freq = interpolation * (1 - mask) + extrapolation * mask\n        else:\n            concentration = 1.0\n            inv_freq = 1.0 / freq\n\n        return concentration, inv_freq\n\n    def _compute_cos_sin(self, num_tokens: int):\n        concentration, inv_freq = self._compute_concentration_and_inv_freq()\n        t = torch.arange(num_tokens, dtype=torch.float32, device=self.device)\n        freqs = torch.einsum(\"i,j->ij\", t, inv_freq)\n        cos = freqs.cos() * concentration\n        sin = freqs.sin() * concentration\n        return cos, sin\n\n    def forward(\n        self,\n        query: torch.Tensor,\n        key: torch.Tensor,\n    ) -> tuple[torch.Tensor, torch.Tensor]:\n        num_tokens = query.shape[0]\n        cos, sin = self._compute_cos_sin(num_tokens)\n\n        query_shape = query.shape\n        query = query.view(num_tokens, -1, self.head_dim)\n        query = _apply_rotary_emb(query, cos, sin)\n        query = query.reshape(query_shape)\n\n        key_shape = key.shape\n        key = key.view(num_tokens, -1, self.head_dim)\n        key = _apply_rotary_emb(key, cos, sin)\n        key = key.reshape(key_shape)\n        return query, key\n\n\ndef sdpa(Q, K, V, S, sm_scale, sliding_window=0):\n    # sliding_window == 0 means no sliding window\n    n_tokens, n_heads, q_mult, d_head = Q.shape\n    assert K.shape == (n_tokens, n_heads, d_head)\n    assert V.shape == (n_tokens, n_heads, d_head)\n    K = K[:, :, None, :].expand(-1, -1, q_mult, -1)\n    V = V[:, :, None, :].expand(-1, -1, q_mult, -1)\n    S = S.reshape(n_heads, q_mult, 1, 1).expand(-1, -1, n_tokens, -1)\n    mask = torch.triu(Q.new_full((n_tokens, n_tokens), -float(\"inf\")), diagonal=1)\n    if sliding_window > 0:\n        mask += torch.tril(\n            mask.new_full((n_tokens, n_tokens), -float(\"inf\")), diagonal=-sliding_window\n        )\n    QK = torch.einsum(\"qhmd,khmd->hmqk\", Q, K)\n    QK *= sm_scale\n    QK += mask[None, None, :, :]\n    QK = torch.cat([QK, S], dim=-1)\n    W = torch.softmax(QK, dim=-1)\n    W = W[..., :-1]\n    attn = torch.einsum(\"hmqk,khmd->qhmd\", W, V)\n    return attn.reshape(n_tokens, -1)\n\n\nclass AttentionBlock(torch.nn.Module):\n    def __init__(\n        self,\n        config: ModelConfig,\n        layer_idx: int = 0,\n        device: torch.device | None = None,\n    ):\n        super().__init__()\n        self.head_dim = config.head_dim\n        self.num_attention_heads = config.num_attention_heads\n        self.num_key_value_heads = config.num_key_value_heads\n        # Only apply sliding window to every other layer\n        self.sliding_window = config.sliding_window if layer_idx % 2 == 0 else 0\n        self.sinks = torch.nn.Parameter(\n            torch.empty(config.num_attention_heads, device=device, dtype=torch.bfloat16)\n        )\n        self.norm = RMSNorm(config.hidden_size, device=device)\n        qkv_dim = config.head_dim * (\n            config.num_attention_heads + 2 * config.num_key_value_heads\n        )\n        self.qkv = torch.nn.Linear(\n            config.hidden_size, qkv_dim, device=device, dtype=torch.bfloat16\n        )\n        self.out = torch.nn.Linear(\n            config.head_dim * config.num_attention_heads,\n            config.hidden_size,\n            device=device,\n            dtype=torch.bfloat16,\n        )\n        self.sm_scale = 1 / math.sqrt(config.head_dim)\n        self.rope = RotaryEmbedding(\n            config.head_dim,\n            config.rope_theta,\n            torch.float32,\n            initial_context_length=config.initial_context_length,\n            scaling_factor=config.rope_scaling_factor,\n            ntk_alpha=config.rope_ntk_alpha,\n            ntk_beta=config.rope_ntk_beta,\n            device=device,\n        )\n\n    def forward(self, x: torch.Tensor) -> torch.Tensor:\n        t = self.norm(x)\n        qkv = self.qkv(t)\n        q = qkv[:, : self.num_attention_heads * self.head_dim].contiguous()\n        k = qkv[\n            :,\n            self.num_attention_heads\n            * self.head_dim : (self.num_attention_heads + self.num_key_value_heads)\n            * self.head_dim,\n        ].contiguous()\n        v = qkv[\n            :,\n            (self.num_attention_heads + self.num_key_value_heads)\n            * self.head_dim : (self.num_attention_heads + 2 * self.num_key_value_heads)\n            * self.head_dim,\n        ].contiguous()\n\n        q = q.view(\n            -1,\n            self.num_key_value_heads,\n            self.num_attention_heads // self.num_key_value_heads,\n            self.head_dim,\n        )\n        k = k.view(-1, self.num_key_value_heads, self.head_dim)\n        v = v.view(-1, self.num_key_value_heads, self.head_dim)\n        q, k = self.rope(q, k)\n        t = sdpa(q, k, v, self.sinks, self.sm_scale, self.sliding_window)\n        t = self.out(t)\n        t = x + t\n        return t\n\n\ndef swiglu(x, alpha: float = 1.702, limit: float = 7.0):\n    x_glu, x_linear = x[..., ::2], x[..., 1::2]\n    # Clamp the input values\n    x_glu = x_glu.clamp(min=None, max=limit)\n    x_linear = x_linear.clamp(min=-limit, max=limit)\n    out_glu = x_glu * torch.sigmoid(alpha * x_glu)\n    # Note we add an extra bias of 1 to the linear layer\n    return out_glu * (x_linear + 1)\n\n\nclass MLPBlock(torch.nn.Module):\n    def __init__(\n        self,\n        config: ModelConfig,\n        device: torch.device | None = None,\n    ):\n        super().__init__()\n        self.num_experts = config.num_experts\n        self.experts_per_token = config.experts_per_token\n        self.swiglu_limit = config.swiglu_limit\n        self.world_size = dist.get_world_size() if dist.is_initialized() else 1\n        self.norm = RMSNorm(config.hidden_size, device=device)\n        self.gate = torch.nn.Linear(\n            config.hidden_size, config.num_experts, device=device, dtype=torch.bfloat16\n        )\n        assert config.intermediate_size % self.world_size == 0\n        self.mlp1_weight = torch.nn.Parameter(\n            torch.empty(\n                (\n                    config.num_experts,\n                    config.intermediate_size * 2 // self.world_size,\n                    config.hidden_size,\n                ),\n                device=device,\n                dtype=torch.bfloat16,\n            )\n        )\n        self.mlp1_bias = torch.nn.Parameter(\n            torch.empty(\n                (config.num_experts, config.intermediate_size * 2 // self.world_size),\n                device=device,\n                dtype=torch.bfloat16,\n            )\n        )\n        self.mlp2_weight = torch.nn.Parameter(\n            torch.empty(\n                (\n                    config.num_experts,\n                    config.hidden_size,\n                    config.intermediate_size // self.world_size,\n                ),\n                device=device,\n                dtype=torch.bfloat16,\n            )\n        )\n        self.mlp2_bias = torch.nn.Parameter(\n            torch.empty(\n                (config.num_experts, config.hidden_size),\n                device=device,\n                dtype=torch.bfloat16,\n            )\n        )\n\n    def forward(self, x: torch.Tensor) -> torch.Tensor:\n        t = self.norm(x)\n        g = self.gate(t)\n        experts = torch.topk(g, k=self.experts_per_token, dim=-1, sorted=True)\n        expert_weights = torch.nn.functional.softmax(experts.values, dim=1)\n        expert_indices = experts.indices\n\n        # MLP #1\n        mlp1_weight = self.mlp1_weight[expert_indices, ...]\n        mlp1_bias = self.mlp1_bias[expert_indices, ...]\n        t = torch.einsum(\"beck,bk->bec\", mlp1_weight, t) + mlp1_bias\n        t = swiglu(t, limit=self.swiglu_limit)\n\n        # MLP #2\n        mlp2_weight = self.mlp2_weight[expert_indices, ...]\n        mlp2_bias = self.mlp2_bias[expert_indices, ...]\n        t = torch.einsum(\"beck,bek->bec\", mlp2_weight, t)\n        if self.world_size > 1:\n            dist.all_reduce(t, op=dist.ReduceOp.SUM)\n        t += mlp2_bias\n\n        # Weighted sum of experts\n        t = torch.einsum(\"bec,be->bc\", t, expert_weights)\n\n        return x + t\n\n\nclass TransformerBlock(torch.nn.Module):\n    def __init__(\n        self,\n        config: ModelConfig,\n        layer_idx: int,\n        device: torch.device | None = None,\n    ):\n        super().__init__()\n        self.layer_idx = layer_idx\n        self.attn = AttentionBlock(config, layer_idx, device)\n        self.mlp = MLPBlock(config, device)\n\n    def forward(self, x: torch.Tensor) -> torch.Tensor:\n        x = self.attn(x)\n        x = self.mlp(x)\n        return x\n\n\nclass Transformer(torch.nn.Module):\n    def __init__(\n        self,\n        config: ModelConfig,\n        device: torch.device | None = None,\n    ):\n        super().__init__()\n        self.embedding = torch.nn.Embedding(\n            config.vocab_size, config.hidden_size, device=device, dtype=torch.bfloat16\n        )\n        self.block = torch.nn.ModuleList(\n            [\n                TransformerBlock(config, layer_idx, device)\n                for layer_idx in range(config.num_hidden_layers)\n            ]\n        )\n        self.norm = RMSNorm(config.hidden_size, device=device)\n        self.unembedding = torch.nn.Linear(\n            config.hidden_size,\n            config.vocab_size,\n            bias=False,\n            device=device,\n            dtype=torch.bfloat16,\n        )\n\n    def forward(self, x: torch.Tensor) -> torch.Tensor:\n        x = self.embedding(x)\n        for block in self.block:\n            x = block(x)\n        x = self.norm(x)\n        x = self.unembedding(x)\n        return x\n\n    @staticmethod\n    def from_checkpoint(\n        path: str, device: str | torch.device = \"cuda\"\n    ) -> \"Transformer\":\n        if not isinstance(device, torch.device):\n            device = torch.device(device)\n\n        config_path = os.path.join(path, \"config.json\")\n        with open(config_path, \"r\") as f:\n            json_config = json.load(f)\n            config = ModelConfig(**json_config)\n\n        model = Transformer(\n            config=config,\n            device=device,\n        )\n        model.eval()\n\n        # Load weights\n        my_rank = dist.get_rank() if dist.is_initialized() else 0\n        world_size = dist.get_world_size() if dist.is_initialized() else 1\n        per_rank_intermediate_size = config.intermediate_size // world_size\n\n        checkpoint = Checkpoint(path, device)\n\n        for name, param in model.named_parameters():\n            loaded_tensor = checkpoint.get(name)\n\n            # Note: it would be more efficient to do sharding before upcasting from MXFP4,\n            # but for simplicity we do it after.\n            if \"mlp1\" in name:  # both weight and bias\n                loaded_tensor = loaded_tensor[\n                    :,\n                    my_rank * 2\n                    * per_rank_intermediate_size : (my_rank + 1) * 2\n                    * per_rank_intermediate_size,\n                    ...,\n                ]\n            elif \"mlp2_weight\" in name:  # only weight\n                loaded_tensor = loaded_tensor[\n                    ...,\n                    my_rank\n                    * per_rank_intermediate_size : (my_rank + 1)\n                    * per_rank_intermediate_size,\n                ]\n            try:\n                param.data.copy_(loaded_tensor)\n            except:\n                print(f\"{name=} {param.data.shape=} {loaded_tensor.shape=}\")\n                raise\n\n        return model\n\n\nclass TokenGenerator:\n    @torch.inference_mode()\n    def __init__(self, checkpoint: str, device: torch.device):\n        self.device = device\n        self.model = Transformer.from_checkpoint(checkpoint, device=self.device)\n\n    @torch.inference_mode()\n    def generate(self,\n                 prompt_tokens: list[int],\n                 stop_tokens: list[int],\n                 temperature: float = 1.0,\n                 max_tokens: int = 0,\n                 return_logprobs: bool = False):\n        tokens = list(prompt_tokens)\n        num_generated_tokens = 0\n        while max_tokens == 0 or num_generated_tokens < max_tokens:\n            logits = self.model(torch.as_tensor(tokens, dtype=torch.int32, device=self.device))[-1]\n            if temperature == 0.0:\n                predicted_token = torch.argmax(logits, dim=-1).item()\n            else:\n                probs = torch.softmax(logits * (1.0 / temperature), dim=-1)\n                predicted_token = torch.multinomial(probs, num_samples=1).item()\n            tokens.append(predicted_token)\n            num_generated_tokens += 1\n\n            if return_logprobs:\n                logprobs = torch.log_softmax(logits, dim=-1)\n                selected_logprobs = logprobs[predicted_token].item()\n                yield predicted_token, selected_logprobs\n            else:\n                yield predicted_token\n\n            if predicted_token in stop_tokens:\n                break\n",
        "gpt_oss/torch/utils.py": "import os\nimport torch\nimport torch.distributed as dist\n\n\ndef suppress_output(rank):\n    \"\"\"Suppress printing on the current device. Force printing with `force=True`.\"\"\"\n    import builtins as __builtin__\n    builtin_print = __builtin__.print\n\n    def print(*args, **kwargs):\n        force = kwargs.pop('force', False)\n        if force:\n            builtin_print(\"rank #%d:\" % rank, *args, **kwargs)\n        elif rank == 0:\n            builtin_print(*args, **kwargs)\n\n    __builtin__.print = print\n\n\ndef init_distributed() -> torch.device:\n    \"\"\"Initialize the model for distributed inference.\"\"\"\n    # Initialize distributed inference\n    world_size = int(os.environ.get(\"WORLD_SIZE\", 1))\n    rank = int(os.environ.get(\"RANK\", 0))\n    if world_size > 1:\n        dist.init_process_group(\n            backend=\"nccl\", init_method=\"env://\", world_size=world_size, rank=rank\n        )\n    torch.cuda.set_device(rank)\n    device = torch.device(f\"cuda:{rank}\")\n\n    # Warm up NCCL to avoid first-time latency\n    if world_size > 1:\n        x = torch.ones(1, device=device)\n        dist.all_reduce(x)\n        torch.cuda.synchronize(device)\n\n    suppress_output(rank)\n    return device\n",
        "gpt_oss/torch/weights.py": "import math\nimport os\n\nimport torch\nfrom safetensors import safe_open\n\n\n# Bytes per MXFP4 block: 32 FP4 numbers packed in 16 bytes\nBYTES_PER_BLOCK = 16\n\nFP4_VALUES = [\n    +0.0, +0.5, +1.0, +1.5, +2.0, +3.0, +4.0, +6.0,\n    -0.0, -0.5, -1.0, -1.5, -2.0, -3.0, -4.0, -6.0,\n]\n\n# Map the names assumed in this implementation to the checkpoint names.\nPARAM_NAME_MAP = {\n    f\"block.{n}.mlp.mlp1_bias\": f\"block.{n}.mlp.mlp1_bias\" for n in range(36)\n} | {\n    f\"block.{n}.mlp.mlp1_weight\": (f\"block.{n}.mlp.mlp1_weight.blocks\", f\"block.{n}.mlp.mlp1_weight.scales\") for n in range(36)\n} | {\n    f\"block.{n}.mlp.mlp2_bias\": f\"block.{n}.mlp.mlp2_bias\" for n in range(36)\n} | {\n    f\"block.{n}.mlp.mlp2_weight\": (f\"block.{n}.mlp.mlp2_weight.blocks\", f\"block.{n}.mlp.mlp2_weight.scales\") for n in range(36)\n}\n\n\nclass Checkpoint:\n    def __init__(self, path: str, device: torch.device):\n        device_str = (\n            device.type\n            if device.index is None\n            else device.type + \":\" + str(device.index)\n        )\n        self.device_str = device_str\n\n        # Read from all files ending with .safetensors in the checkpoint directory\n        safetensor_files = [\n            os.path.join(path, fname)\n            for fname in os.listdir(path)\n            if fname.endswith(\".safetensors\")\n        ]\n        # Build a mapping from tensor name to (file, key)\n        tensor_name_to_file = {}\n        for safetensor_file in safetensor_files:\n            with safe_open(safetensor_file, framework=\"pt\", device=device_str) as f:\n                for key in f.keys():\n                    tensor_name_to_file[key] = safetensor_file\n\n        self.tensor_name_to_file = tensor_name_to_file\n\n    def get(self, name: str) -> torch.Tensor:\n        match PARAM_NAME_MAP.get(name, name):\n            case (blocks_name, scales_name):\n                # MoE weights: are in block-based MXFP4 format\n                return self._get_mxfp4_tensor(blocks_name, scales_name, dtype=torch.bfloat16)\n            case tensor_name:\n                # MoE biases and other weights\n                return self._get_tensor(tensor_name)\n\n    def _get_tensor(self, name: str) -> str:\n        assert name in self.tensor_name_to_file, f\"Tensor {name} not found in checkpoint.\"\n        with safe_open(\n            self.tensor_name_to_file[name], framework=\"pt\", device=self.device_str\n        ) as f:\n            return f.get_tensor(name)\n\n    def _get_mxfp4_tensor(\n        self,\n        blocks_name: str,\n        scales_name: str,\n        *,\n        dtype: torch.dtype = torch.bfloat16,\n        rows_per_chunk: int = 16384 * 512,\n    ) -> torch.Tensor:\n        assert blocks_name in self.tensor_name_to_file, (\n            f\"Blocks tensor {blocks_name} not found in checkpoint.\"\n        )\n        assert scales_name in self.tensor_name_to_file, (\n            f\"Scales tensor {scales_name} not found in checkpoint.\"\n        )\n\n        blocks = self._get_tensor(blocks_name)\n        scales = self._get_tensor(scales_name).to(torch.int32) - 127\n\n        assert blocks.shape[:-1] == scales.shape, (\n            f\"{blocks.shape=} does not match {scales.shape=}\"\n        )\n\n        lut = torch.tensor(FP4_VALUES, dtype=dtype, device=blocks.device)\n\n        *prefix_shape, G, B = blocks.shape\n        rows_total   = math.prod(prefix_shape) * G\n\n        blocks = blocks.reshape(rows_total, B)\n        scales = scales.reshape(rows_total, 1)\n\n        out = torch.empty(rows_total, B * 2, dtype=dtype, device=blocks.device)\n\n        for r0 in range(0, rows_total, rows_per_chunk):\n            r1 = min(r0 + rows_per_chunk, rows_total)\n\n            blk = blocks[r0:r1]\n            exp = scales[r0:r1]\n\n            # nibble indices -> int64\n            idx_lo = (blk & 0x0F).to(torch.long)\n            idx_hi = (blk >> 4).to(torch.long)\n\n            sub = out[r0:r1]\n            sub[:, 0::2] = lut[idx_lo]\n            sub[:, 1::2] = lut[idx_hi]\n\n            torch.ldexp(sub, exp, out=sub)\n            del idx_lo, idx_hi, blk, exp\n\n        return out.reshape(*prefix_shape, G, B * 2).view(*prefix_shape, G * B * 2)\n\n    def _get_mxfp4_tensor_copy(self, blocks_name: str, scales_name: str, dtype: torch.dtype = torch.bfloat16):\n        \"short version that uses a lot of memory\"\n\n        loaded_blocks = self._get_tensor(blocks_name)\n        # Split it into low and high nibbles, upcast to bytes, and interleave (for swiglu)\n        loaded_blocks_lo = loaded_blocks & 0x0F\n        loaded_blocks_hi = loaded_blocks >> 4\n        loaded_blocks = torch.stack((loaded_blocks_lo, loaded_blocks_hi), dim=-1)\n        loaded_blocks = loaded_blocks.view(*loaded_blocks.shape[:-2], loaded_blocks.shape[-2] * 2)\n\n        loaded_scales = self._get_tensor(scales_name)\n        # Upcast to int32 and subtract bias\n        loaded_scales = loaded_scales.int() - 127\n\n        # Convert MXFP4 numbers into target dtype\n        fp4_values = torch.tensor(FP4_VALUES, dtype=dtype, device=self.device_str)\n        loaded_tensor = torch.ldexp(fp4_values[loaded_blocks.int()], loaded_scales.unsqueeze(-1))\n        loaded_tensor = loaded_tensor.view(*loaded_tensor.shape[:-2], -1)\n        return loaded_tensor\n",
        "gpt_oss/triton/__init__.py": "",
        "gpt_oss/triton/attention.py": "\"\"\"FlashAttention w/support for learned sinks and banded attention.\n\nThis is an expanded version of the Flash Attention v2 implementation (see https://tridao.me/publications/flash2/flash2.pdf)\nwhich can be found at https://triton-lang.org/main/getting-started/tutorials/06-fused-attention.html.\n\nThis version has been extended to support banded attention and learned attention sinks.\n\"\"\"\n\nimport pytest\nimport torch\n\nimport triton\nimport triton.language as tl\nfrom triton.tools.tensor_descriptor import TensorDescriptor\n\n\n\n@triton.jit\ndef _attn_fwd(\n    Q,\n    K,\n    V,\n    Sinks,\n    sm_scale,\n    M,\n    Out,  #\n    Start_q,\n    Z,\n    H,\n    N_Q_CTX,\n    N_KV_CTX,\n    HEAD_DIM: tl.constexpr,  #\n    BLOCK_M: tl.constexpr,  #\n    BLOCK_N: tl.constexpr,  #\n    BANDWIDTH: tl.constexpr,\n):\n    tl.static_assert(BLOCK_N <= HEAD_DIM)\n    start_q = tl.load(Start_q).to(tl.int32)\n    start_m = tl.program_id(0)\n    off_hz = tl.program_id(1)\n    off_z = off_hz // H\n    off_h = off_hz % H\n\n    # load attention sinks\n    if Sinks is not None:\n        sink = tl.load(Sinks + off_h).to(tl.float32)\n    else:\n        sink = 0\n\n    # initialize offsets\n    offs_m = start_m * BLOCK_M + tl.arange(0, BLOCK_M)\n    offs_n = tl.arange(0, BLOCK_N)\n    # initialize pointer to m and l\n    m_i = tl.zeros([BLOCK_M], dtype=tl.float32) + sink\n    l_i = tl.zeros([BLOCK_M], dtype=tl.float32)\n    acc = tl.zeros([BLOCK_M, HEAD_DIM], dtype=tl.float32)\n    # load scales\n    qk_scale = sm_scale\n    q = Q.load([off_z, off_h, start_m * BLOCK_M, 0]).reshape([BLOCK_M, HEAD_DIM])\n\n    if BANDWIDTH:\n        lo, hi = tl.maximum(start_q, start_q + start_m * BLOCK_M - BANDWIDTH), start_q + (start_m + 1) * BLOCK_M\n    else:\n        lo, hi = start_q, start_q + (start_m + 1) * BLOCK_M\n\n    for start_n in range(lo, hi, BLOCK_N):\n        start_n = tl.multiple_of(start_n, BLOCK_N)\n\n        mask = (start_n + offs_n)[None, :] > (start_q + offs_m)[:, None]\n\n        if BANDWIDTH:\n            too_old = (start_n + offs_n[None, :]) < (start_q + offs_m[:, None] - BANDWIDTH + 1)\n            mask = mask | too_old\n\n        k = K.load([off_z, off_h, start_n, 0]).reshape([BLOCK_N, HEAD_DIM]).T\n        qk = tl.dot(q, k, allow_tf32=False)\n\n        qk = qk * qk_scale + tl.where(mask, -1.0e6, 0.0)\n        m_ij = tl.maximum(m_i, tl.max(qk, 1))\n        qk -= m_ij[:, None]\n\n        p = tl.math.exp(qk)\n        alpha = tl.math.exp(m_i - m_ij)\n        l_ij = tl.sum(p, 1)\n        acc = acc * alpha[:, None]\n\n        v = V.load([off_z, off_h, start_n, 0]).reshape([BLOCK_N, HEAD_DIM])\n        v = v.to(tl.float32)\n        acc = tl.dot(p, v, acc, allow_tf32=False)\n\n        l_i = l_i * alpha + l_ij\n        m_i = m_ij\n\n    sink = tl.math.exp(sink - m_i)\n    z = l_i + sink\n    acc = acc / z[:, None]\n    m_i += tl.math.log(l_i)\n    m_ptrs = M + off_hz * N_Q_CTX + offs_m\n    tl.store(m_ptrs, m_i)\n    acc = acc.to(Out.dtype)[None, None, :, :]\n    Out.store([off_z, off_h, start_m * BLOCK_M, 0], acc)\n\n\nclass _attention(torch.autograd.Function):\n    @staticmethod\n    def forward(ctx, q, k, v, sinks, sm_scale, bandwidth, start_q):\n        assert len(start_q) == 1\n        bs, n_ctx, n_kv_heads, repeat_kv, HEAD_DIM_Q = q.shape\n        bs, n_kv_ctx, n_kv_heads, HEAD_DIM_K = k.shape\n        bs, n_kv_ctx, n_kv_heads, HEAD_DIM_V = v.shape\n        n_heads = n_kv_heads * repeat_kv\n        q = q.view(bs, n_ctx, n_heads, HEAD_DIM_Q)\n        k = k.view(bs, n_kv_ctx, n_kv_heads, HEAD_DIM_K)\n        assert HEAD_DIM_Q == HEAD_DIM_K and HEAD_DIM_K == HEAD_DIM_V\n        assert HEAD_DIM_K in {16, 32, 64, 128, 256}\n\n        q = q.transpose(1, 2).contiguous()\n        k = k.repeat_interleave(repeat_kv, dim=2).transpose(1, 2).contiguous()\n        v = v.repeat_interleave(repeat_kv, dim=2).transpose(1, 2).contiguous()\n\n        BLOCK_M = 64\n        BLOCK_N = 64\n        m_pad_size = BLOCK_M - n_ctx % BLOCK_M if n_ctx % BLOCK_M != 0 else 0\n        # pad q to multiple of its block size in the n_ctx dimension (-2)\n        q = torch.nn.functional.pad(q, (0, 0, 0, m_pad_size))\n        n_pad_size = BLOCK_N - n_kv_ctx % BLOCK_N if n_kv_ctx % BLOCK_N != 0 else 0\n        # pad k and v to multiple of their block size in the n_kv_ctx dimension\n        k = torch.nn.functional.pad(k, (0, 0, 0, n_pad_size))\n        v = torch.nn.functional.pad(v, (0, 0, 0, n_pad_size))\n\n        o = torch.empty_like(q)\n        M = torch.empty((bs, n_heads, n_ctx + m_pad_size), device=q.device, dtype=torch.float32)\n        grid = (triton.cdiv(n_ctx, BLOCK_M), bs * n_heads, 1)\n        _attn_fwd[grid](\n            TensorDescriptor.from_tensor(q, [1, 1, BLOCK_M, HEAD_DIM_K]),\n            TensorDescriptor.from_tensor(k, [1, 1, BLOCK_N, HEAD_DIM_K]),\n            TensorDescriptor.from_tensor(v, [1, 1, BLOCK_N, HEAD_DIM_K]),\n            sinks,\n            sm_scale,\n            M,\n            TensorDescriptor.from_tensor(o, [1, 1, BLOCK_M, HEAD_DIM_K]),\n            start_q,\n            q.shape[0],\n            q.shape[1],\n            N_Q_CTX=n_ctx + m_pad_size,\n            N_KV_CTX=n_kv_ctx,\n            HEAD_DIM=HEAD_DIM_K,\n            BANDWIDTH=bandwidth,\n            BLOCK_M=BLOCK_M,\n            BLOCK_N=BLOCK_N,\n        )\n\n        ctx.save_for_backward(q, k, v, sinks, o, M, start_q)\n        ctx.sm_scale = sm_scale\n        ctx.bandwidth = bandwidth\n\n        o = o[:, :, :n_ctx, :].transpose(1, 2).contiguous()\n        o = o.view(bs, n_ctx, n_heads * HEAD_DIM_V)\n        return o\n\n\nattention = _attention.apply\n\n\ndef attention_ref(\n    query: torch.Tensor,\n    key: torch.Tensor,\n    value: torch.Tensor,\n    sinks: torch.Tensor,\n    sm_scale: float = 0.125,\n    sliding_window: int | None = None,\n    start_q: torch.LongTensor = 0,\n):\n    batch_size, num_queries, num_key_value_heads, num_key_value_groups, head_dim = query.shape\n    batch_size, num_keys, num_key_value_heads, head_dim = key.shape\n\n    sinks = sinks.view(1, num_key_value_heads, num_key_value_groups, 1, 1).float()\n    key = key.unsqueeze(3)\n    value = value.unsqueeze(3)\n\n    pos_keys = torch.arange(num_keys, device=query.device)\n    pos_queries = torch.arange(num_queries, device=query.device) + start_q\n    mask = pos_keys[None, :] > pos_queries[:, None]\n    mask = mask.float().masked_fill(mask, float(\"-inf\"))\n\n    if sliding_window:\n        too_old = pos_keys[None, :] < (pos_queries[:, None] - sliding_window + 1)\n        mask.masked_fill_(too_old, float(\"-inf\"))\n\n    logits = torch.einsum(\"bqhmd,bkhmd->bhmqk\", query.float(), key.float()) * sm_scale\n    logits = logits + mask[None, None, None, :, :]\n\n    logits_max = torch.max(logits, dim=-1, keepdim=True).values\n    logits_or_sinks_max = torch.maximum(sinks, logits_max)\n    sinks = torch.exp(sinks - logits_or_sinks_max)\n    unnormalized_scores = torch.exp(logits - logits_or_sinks_max)\n    normalizer = unnormalized_scores.sum(dim=-1, keepdim=True) + sinks\n    scores = unnormalized_scores / normalizer\n\n    output = torch.einsum(\"bhmqk,bkhmd->bqhmd\", scores, value.float())\n\n    output = output.reshape(batch_size, num_queries, num_key_value_heads * num_key_value_groups * head_dim).bfloat16()\n    return output\n\n\n@pytest.mark.parametrize(\"batch_size\", [1, 2])\n@pytest.mark.parametrize(\"num_queries\", [1, 128])\n@pytest.mark.parametrize(\"num_keys\", [128, 32])\n@pytest.mark.parametrize(\"num_key_value_heads\", [8])\n@pytest.mark.parametrize(\"num_key_value_groups\", [8])\n@pytest.mark.parametrize(\"head_dim\", [64])\n@pytest.mark.parametrize(\"sm_scale\", [0.125])\n@pytest.mark.parametrize(\"sliding_window\", [None, 128])\n@pytest.mark.parametrize(\"start_q\", [0, 5])\ndef test_eq(batch_size, num_queries, num_keys, num_key_value_heads, num_key_value_groups, head_dim, sm_scale, sliding_window, start_q):\n    if num_queries > num_keys:\n        pytest.skip(\"too many queries\")\n\n    q = torch.randn(batch_size, num_queries, num_key_value_heads, num_key_value_groups, head_dim).bfloat16().cuda()\n    k = torch.randn(batch_size, num_keys, num_key_value_heads, head_dim).bfloat16().cuda()\n    v = torch.randn(batch_size, num_keys, num_key_value_heads, head_dim).bfloat16().cuda()\n    sinks = torch.randn(num_key_value_heads * num_key_value_groups).bfloat16().cuda()\n\n    start_q = torch.tensor([start_q], dtype=torch.int32).cuda()\n\n    o1 = attention(q, k, v, sinks, sm_scale, sliding_window, start_q)\n    o2 = attention_ref(q, k, v, sinks, sm_scale, sliding_window, start_q)\n\n    torch.testing.assert_close(o1, o2)\n",
        "gpt_oss/triton/model.py": "import json\nimport math\nimport os\n\nimport torch\nfrom torch.profiler import record_function\n\nfrom gpt_oss.torch.model import ModelConfig, RMSNorm\nfrom gpt_oss.torch.weights import Checkpoint\nfrom gpt_oss.triton.attention import attention, attention_ref\nfrom gpt_oss.triton.moe import quantize_mx4, moe\n\n\nclass RotaryEmbedding(torch.nn.Module):\n    def __init__(\n        self,\n        head_dim: int,\n        base: int,\n        dtype: torch.dtype,\n        initial_context_length: int = 4096,\n        max_context_length: int = 131072,\n        scaling_factor: float = 1.0,\n        ntk_alpha: float = 1.0,\n        ntk_beta: float = 32.0,\n        device: torch.device | None = None,\n    ) -> None:\n        super().__init__()\n        self.head_dim = head_dim\n        self.base = base\n        self.dtype = dtype\n        self.initial_context_length = initial_context_length\n        self.max_context_length = max_context_length\n        self.scaling_factor = scaling_factor\n        self.ntk_alpha = ntk_alpha\n        self.ntk_beta = ntk_beta\n        self.device = device\n        self.cos, self.sin = self._compute_cos_sin(0, self.max_context_length)\n\n    def _compute_concentration_and_inv_freq(self) -> torch.Tensor:\n        \"\"\"See YaRN paper: https://arxiv.org/abs/2309.00071\"\"\"\n        freq = self.base ** (\n            torch.arange(0, self.head_dim, 2, dtype=torch.float, device=self.device)\n            / self.head_dim\n        )\n        if self.scaling_factor > 1.0:\n            concentration = (\n                0.1 * math.log(self.scaling_factor) + 1.0\n            )  # YaRN concentration\n\n            d_half = self.head_dim / 2\n            # NTK by parts\n            low = (\n                d_half\n                * math.log(self.initial_context_length / (self.ntk_beta * 2 * math.pi))\n                / math.log(self.base)\n            )\n            high = (\n                d_half\n                * math.log(self.initial_context_length / (self.ntk_alpha * 2 * math.pi))\n                / math.log(self.base)\n            )\n            assert 0 < low < high < d_half - 1\n\n            interpolation = 1.0 / (self.scaling_factor * freq)\n            extrapolation = 1.0 / freq\n\n            ramp = (\n                torch.arange(d_half, dtype=torch.float32, device=freq.device) - low\n            ) / (high - low)\n            mask = 1 - ramp.clamp(0, 1)\n\n            inv_freq = interpolation * (1 - mask) + extrapolation * mask\n        else:\n            concentration = 1.0\n            inv_freq = 1.0 / freq\n\n        return concentration, inv_freq\n\n    def _compute_cos_sin(self, start: int, num_tokens: int):\n        concentration, inv_freq = self._compute_concentration_and_inv_freq()\n        t = torch.arange(start, start + num_tokens, dtype=torch.float32, device=self.device)\n        freqs = torch.einsum(\"i,j->ij\", t, inv_freq)\n        cos = freqs.cos() * concentration\n        sin = freqs.sin() * concentration\n        return cos, sin\n\n    @record_function(\"rotate\")\n    def _rotate(\n        self,\n        x: torch.Tensor,\n        cos: torch.Tensor,\n        sin: torch.Tensor,\n    ) -> torch.Tensor:\n        cos = cos[None, :, None, :].to(x.dtype)\n        sin = sin[None, :, None, :].to(x.dtype)\n        x1, x2 = torch.chunk(x, 2, dim=-1)\n        o1 = x1 * cos - x2 * sin\n        o2 = x2 * cos + x1 * sin\n        return torch.cat((o1, o2), dim=-1)\n\n    @record_function(\"rope\")\n    def forward(\n        self,\n        query: torch.Tensor,\n        key: torch.Tensor,\n        offset: torch.LongTensor,\n    ) -> tuple[torch.Tensor, torch.Tensor]:\n        batch_size, num_tokens, num_heads, head_dim = query.shape\n        batch_size, num_tokens, num_key_value_heads, head_dim = key.shape\n\n        idx = torch.arange(num_tokens, device=query.device, dtype=torch.long) + offset\n        idx = idx % self.max_context_length\n        cos = self.cos.index_select(0, idx)\n        sin = self.sin.index_select(0, idx)\n\n        query = self._rotate(query, cos, sin)\n        key = self._rotate(key, cos, sin)\n        return query, key\n\n\nclass Cache:\n    def __init__(self, batch_size, n_ctx, n_kv_heads, d_head=64, device: torch.device | None = None):\n        self.k = torch.zeros((batch_size, n_ctx, n_kv_heads, d_head), dtype=torch.bfloat16, device=device)\n        self.v = torch.zeros((batch_size, n_ctx, n_kv_heads, d_head), dtype=torch.bfloat16, device=device)\n        self.offset = torch.zeros((1,), dtype=torch.long, device=device)\n\n    def reset(self):\n        self.k.zero_()\n        self.v.zero_()\n        self.offset.zero_()\n\n    def repeat_interleave(self, n):\n        \"\"\"Repeat each cache entry n times along the batch dimension.\"\"\"\n        self.k = self.k.repeat_interleave(n, dim=0)\n        self.v = self.v.repeat_interleave(n, dim=0)\n\n    def truncate(self, n_ctx):\n        \"\"\"Truncate the cache to the first n_ctx tokens.\"\"\"\n        batch_size, _, n_kv_heads, d_head = self.k.shape\n        assert batch_size == self.v.shape[0]\n        assert n_ctx <= self.k.shape[1]\n        self.k[:, n_ctx:, :, :].zero_()\n        self.v[:, n_ctx:, :, :].zero_()\n        self.offset.fill_(n_ctx)\n        return self.k, self.v\n\n    def extend(self, k, v):\n        batch_size, n_ctx, *_rest = k.shape\n        assert batch_size == self.k.shape[0]\n        indices = torch.arange(0, n_ctx, device=k.device, dtype=torch.long) + self.offset\n        self.k.index_copy_(1, indices, k)\n        self.v.index_copy_(1, indices, v)\n        self.offset.add_(n_ctx)\n        return self.k, self.v\n\n\nclass AttentionBlock(torch.nn.Module):\n    def __init__(\n        self,\n        config: ModelConfig,\n        layer_idx: int = 0,\n        device: torch.device | None = None,\n    ):\n        super().__init__()\n        self.head_dim = config.head_dim\n        self.num_attention_heads = config.num_attention_heads\n        self.num_key_value_heads = config.num_key_value_heads\n        # Only apply sliding window to every other layer\n        self.sliding_window = config.sliding_window if layer_idx % 2 == 0 else 0\n        self.layer_idx = layer_idx\n        self.sinks = torch.nn.Parameter(\n            torch.empty(config.num_attention_heads, device=device, dtype=torch.bfloat16)\n        )\n        self.norm = RMSNorm(config.hidden_size, device=device)\n        qkv_dim = config.head_dim * (\n            config.num_attention_heads + 2 * config.num_key_value_heads\n        )\n        self.qkv = torch.nn.Linear(\n            config.hidden_size, qkv_dim, device=device, dtype=torch.bfloat16\n        )\n        self.out = torch.nn.Linear(\n            config.head_dim * config.num_attention_heads,\n            config.hidden_size,\n            device=device,\n            dtype=torch.bfloat16,\n        )\n        self.sm_scale = 1 / math.sqrt(config.head_dim)\n        self.rope = RotaryEmbedding(\n            config.head_dim,\n            config.rope_theta,\n            torch.float32,\n            initial_context_length=config.initial_context_length,\n            scaling_factor=config.rope_scaling_factor,\n            ntk_alpha=config.rope_ntk_alpha,\n            ntk_beta=config.rope_ntk_beta,\n            device=device,\n        )\n\n    @record_function(\"attn\")\n    def forward(self, x: torch.Tensor, cache: Cache | None = None) -> torch.Tensor:\n        batch_size, n_ctx, dim = x.shape\n\n        t = self.norm(x)\n        with record_function(\"qkv\"):\n            qkv = self.qkv(t)\n            qkv_parts = (\n                self.num_attention_heads * self.head_dim,\n                self.num_key_value_heads * self.head_dim,\n                self.num_key_value_heads * self.head_dim\n            )\n            q, k, v = torch.split(qkv, qkv_parts, dim=-1)\n            q, k, v = q.contiguous(), k.contiguous(), v.contiguous()\n\n        q = q.view(batch_size, n_ctx, self.num_attention_heads, self.head_dim)\n        k = k.view(batch_size, n_ctx, self.num_key_value_heads, self.head_dim)\n        v = v.view(batch_size, n_ctx, self.num_key_value_heads, self.head_dim)\n\n        if cache is not None:\n            offset = cache.offset.clone()\n            q, k = self.rope(q, k, offset=offset)\n            k, v = cache.extend(k, v)\n        else:\n            offset = torch.zeros((1,), dtype=torch.long, device=x.device)\n            q, k = self.rope(q, k, offset=offset)\n\n        q = q.view(\n            batch_size,\n            n_ctx,\n            self.num_attention_heads // self.num_key_value_heads,\n            self.num_key_value_heads,\n            self.head_dim,\n        )\n        with record_function(\"attn_kernel\"):\n            if n_ctx == 1:\n                t = attention_ref(\n                    q,\n                    k,\n                    v,\n                    self.sinks,\n                    self.sm_scale,\n                    self.sliding_window,\n                    offset,\n                )\n            else:\n                t = attention(\n                    q,\n                    k,\n                    v,\n                    self.sinks,\n                    self.sm_scale,\n                    self.sliding_window,\n                    offset,\n                )\n                if n_ctx < 64:\n                    t1 = attention_ref(\n                        q,\n                        k,\n                        v,\n                        self.sinks,\n                        self.sm_scale,\n                        self.sliding_window,\n                        offset,\n                    )\n                    torch.testing.assert_close(t, t1)\n                    t = t1\n\n        with record_function(\"c_proj\"):\n            t = self.out(t)\n        t = x + t\n        return t\n\n\nclass MLPBlock(torch.nn.Module):\n    def __init__(\n        self,\n        config: ModelConfig,\n        layer_idx: int = 0,\n        device: torch.device | None = None,\n    ):\n        super().__init__()\n        self.layer_idx = layer_idx\n        self.num_experts = config.num_experts\n        self.experts_per_token = config.experts_per_token\n        self.swiglu_limit = config.swiglu_limit\n        self.norm = RMSNorm(config.hidden_size, device=device)\n        self.gate = torch.nn.ParameterDict({\n            \"weight\": torch.nn.Parameter(\n                torch.empty(\n                    (config.hidden_size, config.num_experts),\n                    device=device,\n                    dtype=torch.bfloat16,\n                )\n            ),\n            \"bias\": torch.nn.Parameter(\n                torch.empty(\n                    (config.num_experts,),\n                    device=device,\n                    dtype=torch.bfloat16,\n                )\n            ),\n        })\n        self.mlp1_weight_tensor, self.mlp1_weight_mx = quantize_mx4(\n            torch.empty(\n                (\n                    config.num_experts,\n                    config.hidden_size,\n                    config.intermediate_size * 2,\n                ),\n                device=device,\n                dtype=torch.bfloat16,\n            ),\n        )\n        self.mlp1_weight = torch.nn.Parameter(self.mlp1_weight_tensor.storage.data, requires_grad=False)\n        self.mlp1_bias = torch.nn.Parameter(\n            torch.empty(\n                (config.num_experts, config.intermediate_size * 2),\n                device=device,\n                dtype=torch.bfloat16,\n            )\n        )\n        self.mlp2_weight_tensor, self.mlp2_weight_mx = quantize_mx4(\n            torch.empty(\n                (\n                    config.num_experts,\n                    config.intermediate_size,\n                    config.hidden_size,\n                ),\n                device=device,\n                dtype=torch.bfloat16,\n            ),\n        )\n        self.mlp2_weight = torch.nn.Parameter(self.mlp2_weight_tensor.storage.data, requires_grad=False)\n        self.mlp2_bias = torch.nn.Parameter(\n            torch.empty(\n                (config.num_experts, config.hidden_size),\n                device=device,\n                dtype=torch.bfloat16,\n            )\n        )\n\n    @record_function(\"mlp\")\n    def forward(self, x: torch.Tensor) -> torch.Tensor:\n        batch_size, n_ctx, dim = x.shape\n        t = self.norm(x)\n\n        t = t.view(batch_size * n_ctx, dim)\n        t = moe(\n            t,\n            self.gate[\"weight\"],\n            self.mlp1_weight_tensor, self.mlp1_weight_mx,\n            self.mlp2_weight_tensor, self.mlp2_weight_mx,\n            self.gate[\"bias\"].float(),\n            self.mlp1_bias.float(),\n            self.mlp2_bias.float(),\n            experts_per_token=self.experts_per_token,\n            num_experts=self.num_experts,\n            swiglu_limit=self.swiglu_limit,\n        )\n        t = t.view(batch_size, n_ctx, dim)\n\n        return x + t\n\n\nclass TransformerBlock(torch.nn.Module):\n    def __init__(\n        self,\n        config: ModelConfig,\n        layer_idx: int,\n        device: torch.device | None = None,\n    ):\n        super().__init__()\n        self.layer_idx = layer_idx\n        self.attn = AttentionBlock(config, layer_idx, device)\n        self.mlp = MLPBlock(config, layer_idx, device)\n\n    def forward(self, x: torch.Tensor, cache: Cache | None = None) -> torch.Tensor:\n        x = self.attn(x, cache=cache)\n        x = self.mlp(x)\n        return x\n\n\nclass Transformer(torch.nn.Module):\n    def __init__(\n        self,\n        config: ModelConfig,\n        device: torch.device | None = None,\n    ):\n        super().__init__()\n        self.config = config\n        self.embedding = torch.nn.Embedding(\n            config.vocab_size, config.hidden_size, device=device, dtype=torch.bfloat16\n        )\n        self.block = torch.nn.ModuleList(\n            [\n                TransformerBlock(config, layer_idx, device)\n                for layer_idx in range(config.num_hidden_layers)\n            ]\n        )\n        self.norm = RMSNorm(config.hidden_size, device=device)\n        self.unembedding = torch.nn.Linear(\n            config.hidden_size,\n            config.vocab_size,\n            bias=False,\n            device=device,\n            dtype=torch.bfloat16,\n        )\n\n    def forward(self, x: torch.Tensor, caches: list[Cache] | None = None) -> torch.Tensor:\n        caches=caches or [None] * len(self.block)\n        with record_function(\"embedding\"):\n            x = self.embedding(x)\n        for block, cache in zip(self.block, caches):\n            with record_function(\"block\"):\n                x = block(x, cache=cache)\n        with record_function(\"norm_f\"):\n            x = self.norm(x)\n        with record_function(\"unembedding\"):\n            x = self.unembedding(x)\n        return x.float()\n\n    @staticmethod\n    def from_checkpoint(\n        path: str, config: ModelConfig | None = None, device: str | torch.device = \"cuda\",\n    ) -> \"Transformer\":\n        if not isinstance(device, torch.device):\n            device = torch.device(device)\n\n        if config is None:\n            config_path = os.path.join(path, \"config.json\")\n            with open(config_path, \"r\") as f:\n                json_config = json.load(f)\n                config = ModelConfig(**json_config)\n\n        model = Transformer(config=config, device=device)\n        model.eval()\n\n        checkpoint = Checkpoint(path, device)\n\n        for name, param in model.named_parameters():\n            torch.cuda.empty_cache()\n            loaded_tensor = checkpoint.get(name)\n\n            if \"mlp1\" in name:\n                if \"weight\" in name:\n                    loaded_tensor, scales = quantize_mx4(loaded_tensor.mT.contiguous())\n                    _, block_index, _, _ = name.split(\".\")\n                    model.block[int(block_index)].mlp.mlp1_weight_mx = scales\n                    param.data.copy_(loaded_tensor.storage.data)\n                else:\n                    param.data.copy_(loaded_tensor)\n\n            elif \"mlp2_weight\" in name:\n                loaded_tensor, scales = quantize_mx4(loaded_tensor.mT.contiguous())\n                _, block_index, _, _ = name.split(\".\")\n                model.block[int(block_index)].mlp.mlp2_weight_mx = scales\n                param.data.copy_(loaded_tensor.storage.data)\n\n            elif \"gate\" in name and loaded_tensor.ndim == 2:\n                loaded_tensor = loaded_tensor.mT.contiguous()\n                param.data.copy_(loaded_tensor)\n\n            else:\n                param.data.copy_(loaded_tensor)\n\n        # NOTE: Required to avoid OOM errors\n        torch.cuda.empty_cache()\n        return model\n\n\nclass TokenGenerator:\n    @torch.inference_mode()\n    def __init__(self, checkpoint: str, context: int, device: torch.device):\n        self.device = device\n        self.model = Transformer.from_checkpoint(checkpoint, device=self.device)\n        self.caches = [Cache(1, context, self.model.config.num_key_value_heads, device=self.device) for _ in range(len(self.model.block))]\n        self.input_token = torch.zeros(1, dtype=torch.int32, device=self.device)\n        # warmup\n        self.model(self.input_token[None, :], caches=self.caches)\n        # capture for sampling\n        self.graph = torch.cuda.CUDAGraph()\n        with torch.cuda.graph(self.graph):\n            self.logits = self.model(self.input_token[None, :], caches=self.caches)[0]\n\n    @torch.inference_mode()\n    def generate(self,\n                 prompt_tokens: list[int],\n                 stop_tokens: list[int] | None = None,\n                 temperature: float = 1.0,\n                 max_tokens: int = 0,\n                 return_logprobs: bool = False):\n        stop_tokens = stop_tokens or []\n        for cache in self.caches:\n            cache.reset()\n        prompt_tokens = torch.as_tensor(prompt_tokens, dtype=torch.int32, device=self.device)\n        self.model(prompt_tokens[None, :-1], self.caches)\n        predicted_token = prompt_tokens[-1]\n        num_generated_tokens = 0\n        while max_tokens == 0 or num_generated_tokens < max_tokens:\n            self.input_token[0] = predicted_token\n            self.graph.replay()\n            if temperature == 0.0:\n                predicted_token = torch.argmax(self.logits[-1, :], dim=-1).item()\n            else:\n                probs = torch.softmax(self.logits * (1.0 / temperature), dim=-1)\n                predicted_token = torch.multinomial(probs[-1, :], num_samples=1).item()\n            num_generated_tokens += 1\n\n            if return_logprobs:\n                logprobs = torch.log_softmax(self.logits[-1, :], dim=-1)\n                selected_logprobs = logprobs[predicted_token].item()\n                yield predicted_token, selected_logprobs\n            else:\n                yield predicted_token\n\n            if predicted_token in stop_tokens:\n                break\n",
        "gpt_oss/triton/moe.py": "import torch\nfrom torch.profiler import record_function\n\nimport triton_kernels\nimport triton_kernels.swiglu\nfrom triton_kernels.numerics_details.mxfp import downcast_to_mxfp\nfrom triton_kernels.matmul_ogs import PrecisionConfig, FlexCtx, FnSpecs, FusedActivation\nfrom triton_kernels.matmul_ogs import matmul_ogs\nfrom triton_kernels.numerics import InFlexData\nfrom triton_kernels.routing import routing\nfrom triton_kernels.tensor import convert_layout\nfrom triton_kernels.tensor_details.layout import StridedLayout, HopperMXScaleLayout, HopperMXValueLayout\nfrom triton_kernels.tensor import wrap_torch_tensor, FP4\n\n\ndef quantize_mx4(w):\n    w, w_scale = downcast_to_mxfp(w.to(torch.bfloat16), torch.uint8, axis=1)\n    w = convert_layout(wrap_torch_tensor(w, dtype=FP4), HopperMXValueLayout, mx_axis=1)\n    w_scale = convert_layout(wrap_torch_tensor(w_scale), StridedLayout)\n    return w, w_scale\n\n\ndef swiglu(x, alpha: float = 1.702, limit: float = 7.0, interleaved: bool = True):\n    if interleaved:\n        x_glu, x_linear = x[..., ::2], x[..., 1::2]\n    else:\n        x_glu, x_linear = torch.chunk(x, 2, dim=-1)\n    x_glu = x_glu.clamp(min=None, max=limit)\n    x_linear = x_linear.clamp(min=-limit, max=limit)\n    out_glu = x_glu * torch.sigmoid(alpha * x_glu)\n    return out_glu * (x_linear + 1)\n\n\ndef moe(x, wg, w1, w1_mx, w2, w2_mx, bg, b1, b2, experts_per_token=4, num_experts=128, swiglu_limit=7.0, fused_act=True, interleaved=True):\n    if x.numel() == 0:\n        return x\n\n    pc1 = PrecisionConfig(weight_scale=w1_mx, flex_ctx=FlexCtx(rhs_data=InFlexData()))\n    pc2 = PrecisionConfig(weight_scale=w2_mx, flex_ctx=FlexCtx(rhs_data=InFlexData()))\n    pcg = PrecisionConfig(flex_ctx=FlexCtx(rhs_data=InFlexData()))\n\n    with record_function(\"wg\"):\n        logits = matmul_ogs(x, wg, bg, precision_config=pcg)\n    with record_function(\"routing\"):\n        rdata, gather_indx, scatter_indx = routing(logits, experts_per_token, simulated_ep=1)\n\n    if fused_act:\n        assert interleaved, \"Fused activation requires interleaved weights\"\n        with record_function(\"w1+swiglu\"):\n            act = FusedActivation(FnSpecs(\"swiglu\", triton_kernels.swiglu.swiglu_fn, (\"alpha\", \"limit\")), (1.702, swiglu_limit), 2)\n            x = matmul_ogs(x, w1, b1, rdata, gather_indx=gather_indx, precision_config=pc1, fused_activation=act)\n    else:\n        with record_function(\"w1\"):\n            x = matmul_ogs(x, w1, b1, rdata, gather_indx=gather_indx, precision_config=pc1)\n        with record_function(\"swiglu\"):\n            x = swiglu(x, limit=swiglu_limit, interleaved=interleaved)\n\n    with record_function(\"w2\"):\n        x = matmul_ogs(x, w2, b2, rdata, scatter_indx=scatter_indx, precision_config=pc2, gammas=rdata.gate_scal)\n    return x\n",
        "gpt_oss/vllm/token_generator.py": "from vllm import LLMEngine, EngineArgs, SamplingParams, TokensPrompt\n\n\nclass TokenGenerator:\n    def __init__(self, model_path: str, tensor_parallel_size: int = 1):\n        args = EngineArgs(\n            model=model_path,\n            tensor_parallel_size=tensor_parallel_size,\n        )\n        self.engine = LLMEngine.from_engine_args(args)\n        self.request_id = 0\n\n    def generate(self,\n                 prompt_tokens: list[int],\n                 stop_tokens: list[int] | None = None,\n                 temperature: float = 1.0,\n                 max_tokens: int = 0,\n                 return_logprobs: bool = False):\n        if max_tokens == 0:\n            max_tokens = None\n        request_id = str(self.request_id)\n        self.request_id += 1\n        sampling_params = SamplingParams(temperature=temperature,\n                                         max_tokens=max_tokens,\n                                         stop_token_ids=stop_tokens,\n                                         logprobs=0 if return_logprobs else None)\n        prompt = TokensPrompt(prompt_token_ids=prompt_tokens)\n        self.engine.add_request(request_id, prompt, sampling_params)\n        last_token_id = []\n        while self.engine.has_unfinished_requests():\n            step_outputs = self.engine.step()\n            output = step_outputs[0].outputs[0]\n            token_ids = output.token_ids\n            logprobs_list = output.logprobs if hasattr(output, \"logprobs\") else None\n            new_token_ids = token_ids[len(last_token_id):]\n            new_logprobs = logprobs_list[len(last_token_id):] if logprobs_list is not None else [None] * len(new_token_ids)\n            for token_id, logprobs in zip(new_token_ids, new_logprobs):\n                last_token_id.append(token_id)\n                if return_logprobs:\n                    logprob_val = None\n                    if logprobs is not None and token_id in logprobs:\n                        logprob_val = logprobs[token_id].logprob\n                    yield (token_id, logprob_val)\n                else:\n                    yield token_id\n                if stop_tokens is not None and token_id in stop_tokens:\n                    break\n",
        "tests/conftest.py": "import os\nimport sys\nimport pytest\nfrom typing import Generator, Any\nfrom unittest.mock import Mock, MagicMock\nfrom fastapi.testclient import TestClient\n\nsys.path.insert(0, os.path.abspath(os.path.join(os.path.dirname(__file__), '..')))\n\nfrom openai_harmony import (\n    HarmonyEncodingName,\n    load_harmony_encoding,\n)\nfrom gpt_oss.responses_api.api_server import create_api_server\n\n\n@pytest.fixture(scope=\"session\")\ndef harmony_encoding():\n    return load_harmony_encoding(HarmonyEncodingName.HARMONY_GPT_OSS)\n\n\n@pytest.fixture\ndef mock_infer_token(harmony_encoding):\n    fake_tokens = harmony_encoding.encode(\n        \"<|channel|>final<|message|>Test response<|return|>\", \n        allowed_special=\"all\"\n    )\n    token_queue = fake_tokens.copy()\n    \n    def _mock_infer(tokens: list[int], temperature: float = 0.0, new_request: bool = False) -> int:\n        nonlocal token_queue\n        if len(token_queue) == 0:\n            token_queue = fake_tokens.copy()\n        return token_queue.pop(0)\n    return _mock_infer\n\n\n@pytest.fixture\ndef api_client(harmony_encoding, mock_infer_token) -> Generator[TestClient, None, None]:\n    app = create_api_server(\n        infer_next_token=mock_infer_token,\n        encoding=harmony_encoding\n    )\n    with TestClient(app) as client:\n        yield client\n\n\n@pytest.fixture\ndef sample_request_data():\n    return {\n        \"model\": \"gpt-oss-120b\",\n        \"input\": \"Hello, how can I help you today?\",\n        \"stream\": False,\n        \"reasoning_effort\": \"low\",\n        \"temperature\": 0.7,\n        \"tools\": []\n    }\n\n\n@pytest.fixture\ndef mock_browser_tool():\n    mock = MagicMock()\n    mock.search.return_value = [\"Result 1\", \"Result 2\"]\n    mock.open_page.return_value = \"Page content\"\n    mock.find_on_page.return_value = \"Found text\"\n    return mock\n\n\n@pytest.fixture\ndef mock_python_tool():\n    mock = MagicMock()\n    mock.execute.return_value = {\n        \"output\": \"print('Hello')\",\n        \"error\": None,\n        \"exit_code\": 0\n    }\n    return mock\n\n\n@pytest.fixture(autouse=True)\ndef reset_test_environment():\n    test_env_vars = ['OPENAI_API_KEY', 'GPT_OSS_MODEL_PATH']\n    original_values = {}\n    \n    for var in test_env_vars:\n        if var in os.environ:\n            original_values[var] = os.environ[var]\n            del os.environ[var]\n    \n    yield\n    \n    for var, value in original_values.items():\n        os.environ[var] = value\n\n\n@pytest.fixture\ndef performance_timer():\n    import time\n    \n    class Timer:\n        def __init__(self):\n            self.start_time = None\n            self.end_time = None\n        \n        def start(self):\n            self.start_time = time.time()\n        \n        def stop(self):\n            self.end_time = time.time()\n            return self.elapsed\n        \n        @property\n        def elapsed(self):\n            if self.start_time and self.end_time:\n                return self.end_time - self.start_time\n            return None\n    \n    return Timer()",
        "tests/gpt_oss/tools/simple_browser/test_backend.py": "import pytest\nfrom typing import Generator, Any\nfrom unittest import mock\nfrom aiohttp import ClientSession\n\nfrom gpt_oss.tools.simple_browser.backend import YouComBackend\n\nclass MockAiohttpResponse:\n    \"\"\"Mocks responses for get/post requests from async libraries.\"\"\"\n\n    def __init__(self, json: dict, status: int):\n        self._json = json\n        self.status = status\n\n    async def json(self):\n        return self._json\n\n    async def __aexit__(self, exc_type, exc, tb):\n        pass\n\n    async def __aenter__(self):\n        return self\n\ndef mock_os_environ_get(name: str, default: Any = \"test_api_key\"):\n    assert name in [\"YDC_API_KEY\"]\n    return default\n\ndef test_youcom_backend():\n    backend = YouComBackend(source=\"web\")\n    assert backend.source == \"web\"\n\n@pytest.mark.asyncio\n@mock.patch(\"aiohttp.ClientSession.get\")\nasync def test_youcom_backend_search(mock_session_get):\n    backend = YouComBackend(source=\"web\")\n    api_response = {\n        \"results\": {\n            \"web\": [\n                {\"title\": \"Web Result 1\", \"url\": \"https://www.example.com/web1\", \"snippets\": \"Web Result 1 snippets\"},\n                {\"title\": \"Web Result 2\", \"url\": \"https://www.example.com/web2\", \"snippets\": \"Web Result 2 snippets\"},\n            ],\n            \"news\": [\n                {\"title\": \"News Result 1\", \"url\": \"https://www.example.com/news1\", \"description\": \"News Result 1 description\"},\n                {\"title\": \"News Result 2\", \"url\": \"https://www.example.com/news2\", \"description\": \"News Result 2 description\"},\n            ],\n        }\n    }\n    with mock.patch(\"os.environ.get\", wraps=mock_os_environ_get):\n        mock_session_get.return_value = MockAiohttpResponse(api_response, 200)\n        async with ClientSession() as session:\n            result = await backend.search(query=\"test\", topn=10, session=session)\n        assert result.title == \"test\"\n        assert result.urls == {\"0\": \"https://www.example.com/web1\", \"1\": \"https://www.example.com/web2\", \"2\": \"https://www.example.com/news1\", \"3\": \"https://www.example.com/news2\"}\n\n@pytest.mark.asyncio\n@mock.patch(\"aiohttp.ClientSession.post\")\nasync def test_youcom_backend_fetch(mock_session_get):\n    backend = YouComBackend(source=\"web\")\n    api_response = [\n        {\"title\": \"Fetch Result 1\", \"url\": \"https://www.example.com/fetch1\", \"html\": \"<div>Fetch Result 1 text</div>\"},\n    ]\n    with mock.patch(\"os.environ.get\", wraps=mock_os_environ_get):\n        mock_session_get.return_value = MockAiohttpResponse(api_response, 200)\n        async with ClientSession() as session:\n            result = await backend.fetch(url=\"https://www.example.com/fetch1\", session=session)\n        assert result.title == \"Fetch Result 1\"\n        assert result.text == \"\\nURL: https://www.example.com/fetch1\\nFetch Result 1 text\"\n\n\n    ",
        "tests/test_api_endpoints.py": "import pytest\nimport json\nimport asyncio\nfrom fastapi import status\nfrom unittest.mock import patch, MagicMock, AsyncMock\n\n\nclass TestResponsesEndpoint:\n    \n    def test_basic_response_creation(self, api_client, sample_request_data):\n        response = api_client.post(\"/v1/responses\", json=sample_request_data)\n        assert response.status_code == status.HTTP_200_OK\n        data = response.json()\n        assert \"id\" in data\n        assert data[\"object\"] == \"response\"\n        assert data[\"model\"] == sample_request_data[\"model\"]\n    \n    def test_response_with_high_reasoning(self, api_client, sample_request_data):\n        sample_request_data[\"reasoning_effort\"] = \"high\"\n        response = api_client.post(\"/v1/responses\", json=sample_request_data)\n        assert response.status_code == status.HTTP_200_OK\n        data = response.json()\n        assert \"id\" in data\n        assert data[\"status\"] == \"completed\"\n    \n    def test_response_with_medium_reasoning(self, api_client, sample_request_data):\n        sample_request_data[\"reasoning_effort\"] = \"medium\"\n        response = api_client.post(\"/v1/responses\", json=sample_request_data)\n        assert response.status_code == status.HTTP_200_OK\n        data = response.json()\n        assert \"id\" in data\n        assert data[\"status\"] == \"completed\"\n    \n    def test_response_with_invalid_model(self, api_client, sample_request_data):\n        sample_request_data[\"model\"] = \"invalid-model\"\n        response = api_client.post(\"/v1/responses\", json=sample_request_data)\n        # Should still accept but might handle differently\n        assert response.status_code == status.HTTP_200_OK\n    \n    def test_response_with_empty_input(self, api_client, sample_request_data):\n        sample_request_data[\"input\"] = \"\"\n        response = api_client.post(\"/v1/responses\", json=sample_request_data)\n        assert response.status_code == status.HTTP_200_OK\n    \n    def test_response_with_tools(self, api_client, sample_request_data):\n        sample_request_data[\"tools\"] = [\n            {\n                \"type\": \"browser_search\"\n            }\n        ]\n        response = api_client.post(\"/v1/responses\", json=sample_request_data)\n        assert response.status_code == status.HTTP_200_OK\n    \n    def test_response_with_custom_temperature(self, api_client, sample_request_data):\n        for temp in [0.0, 0.5, 1.0, 1.5, 2.0]:\n            sample_request_data[\"temperature\"] = temp\n            response = api_client.post(\"/v1/responses\", json=sample_request_data)\n            assert response.status_code == status.HTTP_200_OK\n            data = response.json()\n            assert \"usage\" in data\n    \n    def test_streaming_response(self, api_client, sample_request_data):\n        sample_request_data[\"stream\"] = True\n        with api_client.stream(\"POST\", \"/v1/responses\", json=sample_request_data) as response:\n            assert response.status_code == status.HTTP_200_OK\n            # Verify we get SSE events\n            for line in response.iter_lines():\n                if line and line.startswith(\"data: \"):\n                    event_data = line[6:]  # Remove \"data: \" prefix\n                    if event_data != \"[DONE]\":\n                        json.loads(event_data)  # Should be valid JSON\n                        break\n\n\nclass TestResponsesWithSession:\n    \n    def test_response_with_session_id(self, api_client, sample_request_data):\n        session_id = \"test-session-123\"\n        sample_request_data[\"session_id\"] = session_id\n        \n        # First request\n        response1 = api_client.post(\"/v1/responses\", json=sample_request_data)\n        assert response1.status_code == status.HTTP_200_OK\n        data1 = response1.json()\n        \n        # Second request with same session\n        sample_request_data[\"input\"] = \"Follow up question\"\n        response2 = api_client.post(\"/v1/responses\", json=sample_request_data)\n        assert response2.status_code == status.HTTP_200_OK\n        data2 = response2.json()\n        \n        # Should have different response IDs\n        assert data1[\"id\"] != data2[\"id\"]\n    \n    def test_response_continuation(self, api_client, sample_request_data):\n        # Create initial response\n        response1 = api_client.post(\"/v1/responses\", json=sample_request_data)\n        assert response1.status_code == status.HTTP_200_OK\n        data1 = response1.json()\n        response_id = data1[\"id\"]\n        \n        # Continue the response\n        continuation_request = {\n            \"model\": sample_request_data[\"model\"],\n            \"response_id\": response_id,\n            \"input\": \"Continue the previous thought\"\n        }\n        response2 = api_client.post(\"/v1/responses\", json=continuation_request)\n        assert response2.status_code == status.HTTP_200_OK\n\n\nclass TestErrorHandling:\n    \n    def test_missing_required_fields(self, api_client):\n        # Model field has default, so test with empty JSON\n        response = api_client.post(\"/v1/responses\", json={})\n        assert response.status_code == status.HTTP_422_UNPROCESSABLE_ENTITY\n    \n    def test_invalid_reasoning_effort(self, api_client, sample_request_data):\n        sample_request_data[\"reasoning_effort\"] = \"invalid\"\n        response = api_client.post(\"/v1/responses\", json=sample_request_data)\n        # May handle gracefully or return error\n        assert response.status_code in [status.HTTP_200_OK, status.HTTP_422_UNPROCESSABLE_ENTITY]\n    \n    def test_malformed_json(self, api_client):\n        response = api_client.post(\n            \"/v1/responses\",\n            data=\"not json\",\n            headers={\"Content-Type\": \"application/json\"}\n        )\n        assert response.status_code == status.HTTP_422_UNPROCESSABLE_ENTITY\n    \n    def test_extremely_long_input(self, api_client, sample_request_data):\n        # Test with very long input\n        sample_request_data[\"input\"] = \"x\" * 100000\n        response = api_client.post(\"/v1/responses\", json=sample_request_data)\n        assert response.status_code == status.HTTP_200_OK\n\n\nclass TestToolIntegration:\n    \n    def test_browser_search_tool(self, api_client, sample_request_data):\n        sample_request_data[\"tools\"] = [\n            {\n                \"type\": \"browser_search\"\n            }\n        ]\n        response = api_client.post(\"/v1/responses\", json=sample_request_data)\n        assert response.status_code == status.HTTP_200_OK\n    \n    def test_function_tool_integration(self, api_client, sample_request_data):\n        sample_request_data[\"tools\"] = [\n            {\n                \"type\": \"function\",\n                \"name\": \"test_function\",\n                \"parameters\": {\"type\": \"object\", \"properties\": {}},\n                \"description\": \"Test function\"\n            }\n        ]\n        response = api_client.post(\"/v1/responses\", json=sample_request_data)\n        assert response.status_code == status.HTTP_200_OK\n    \n    def test_multiple_tools(self, api_client, sample_request_data):\n        sample_request_data[\"tools\"] = [\n            {\n                \"type\": \"browser_search\"\n            },\n            {\n                \"type\": \"function\",\n                \"name\": \"test_function\",\n                \"parameters\": {\"type\": \"object\", \"properties\": {}},\n                \"description\": \"Test function\"\n            }\n        ]\n        response = api_client.post(\"/v1/responses\", json=sample_request_data)\n        assert response.status_code == status.HTTP_200_OK\n\n\nclass TestPerformance:\n    \n    def test_response_time_under_threshold(self, api_client, sample_request_data, performance_timer):\n        performance_timer.start()\n        response = api_client.post(\"/v1/responses\", json=sample_request_data)\n        elapsed = performance_timer.stop()\n        \n        assert response.status_code == status.HTTP_200_OK\n        # Response should be reasonably fast for mock inference\n        assert elapsed < 5.0  # 5 seconds threshold\n    \n    def test_multiple_sequential_requests(self, api_client, sample_request_data):\n        # Test multiple requests work correctly\n        for i in range(3):\n            data = sample_request_data.copy()\n            data[\"input\"] = f\"Request {i}\"\n            response = api_client.post(\"/v1/responses\", json=data)\n            assert response.status_code == status.HTTP_200_OK\n\n\nclass TestUsageTracking:\n    \n    def test_usage_object_structure(self, api_client, sample_request_data):\n        response = api_client.post(\"/v1/responses\", json=sample_request_data)\n        assert response.status_code == status.HTTP_200_OK\n        data = response.json()\n        \n        assert \"usage\" in data\n        usage = data[\"usage\"]\n        assert \"input_tokens\" in usage\n        assert \"output_tokens\" in usage\n        assert \"total_tokens\" in usage\n        # reasoning_tokens may not always be present\n        # assert \"reasoning_tokens\" in usage\n        \n        # Basic validation\n        assert usage[\"input_tokens\"] >= 0\n        assert usage[\"output_tokens\"] >= 0\n        assert usage[\"total_tokens\"] == usage[\"input_tokens\"] + usage[\"output_tokens\"]\n    \n    def test_usage_increases_with_longer_input(self, api_client, sample_request_data):\n        # Short input\n        response1 = api_client.post(\"/v1/responses\", json=sample_request_data)\n        usage1 = response1.json()[\"usage\"]\n        \n        # Longer input\n        sample_request_data[\"input\"] = sample_request_data[\"input\"] * 10\n        response2 = api_client.post(\"/v1/responses\", json=sample_request_data)\n        usage2 = response2.json()[\"usage\"]\n        \n        # Longer input should use more tokens\n        assert usage2[\"input_tokens\"] > usage1[\"input_tokens\"]",
        "tests/test_responses_api.py": "import time\n\nimport pytest\nfrom fastapi.testclient import TestClient\nfrom openai_harmony import (\n    HarmonyEncodingName,\n    load_harmony_encoding,\n)\n\nfrom gpt_oss.responses_api.api_server import create_api_server\n\nencoding = load_harmony_encoding(HarmonyEncodingName.HARMONY_GPT_OSS)\n\nfake_tokens = encoding.encode(\n    \"<|channel|>final<|message|>Hey there<|return|>\", allowed_special=\"all\"\n)\n\ntoken_queue = fake_tokens.copy()\n\n\ndef stub_infer_next_token(\n    tokens: list[int], temperature: float = 0.0, new_request: bool = False\n) -> int:\n    global token_queue\n    next_tok = token_queue.pop(0)\n    if len(token_queue) == 0:\n        token_queue = fake_tokens.copy()\n    time.sleep(0.1)\n    return next_tok\n\n\n@pytest.fixture\ndef test_client():\n    return TestClient(\n        create_api_server(infer_next_token=stub_infer_next_token, encoding=encoding)\n    )\n\n\ndef test_health_check(test_client):\n    response = test_client.post(\n        \"/v1/responses\",\n        json={\n            \"model\": \"gpt-oss-120b\",\n            \"input\": \"Hello, world!\",\n        },\n    )\n    print(response.json())\n    assert response.status_code == 200\n"
    }
}