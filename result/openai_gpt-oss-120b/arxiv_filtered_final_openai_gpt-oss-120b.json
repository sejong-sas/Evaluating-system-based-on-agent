{
  "1-5 (Architecture)": "According to the release notes, the gpt-oss models—including the flagship gpt-oss-120b—are autoregressive Mixture-of-Experts (MoE) transformers that extend the basic GPT-2 / GPT-3 architectural blueprint. gpt-oss-120b is built from 36 transformer layers and contains 116.8 billion total parameters, of which only 5.1 billion are \"active\" (i.e., participate in a given forward pass) because of MoE routing. Each MoE block exposes 128 experts for gpt-oss-120b, and routing is handled by a linear projection that converts residual activations into expert-selection scores. The model uses a residual stream width of 2880 and applies root-mean-square (RMS) normalization immediately before every attention block and MoE block. In stress-test settings, the purely text-based architecture shows strong performance on biology-related questions and harm-scenario reasoning but remains below desired thresholds on highly technical protocol-debugging tasks and cannot address vision-dependent problems.",
  "1-6 (Tokenizer)": "Throughout every stage of training, the models rely on the o200k_harmony tokenizer. This tokenizer is released as part of the TikToken library, is based on Byte-Pair Encoding (BPE), and extends the earlier o200k vocabulary used for models such as GPT-4o and OpenAI o4-mini. Extra tokens required for the system’s harmony chat formatting are included, bringing the total vocabulary size to 201,088 tokens. The tokenizer is fully open-sourced and can be downloaded directly from the TikToken repository.",
  "2-1 (Hardware)": "Training runs for gpt-oss were executed on NVIDIA H100 GPUs. For the larger gpt-oss-120b model, the complete training job consumed approximately 2.1 million H100-GPU-hours. By contrast, the smaller gpt-oss-20b required almost an order of magnitude less compute. No other accelerator type is mentioned, underscoring that all large-scale compute came exclusively from H100 hardware.",
  "2-2 (Software)": "The training software stack for the gpt-oss family centers on the PyTorch framework, augmented with expert-optimized Triton kernels and Flash Attention algorithms. PyTorch handled the core model definition and distributed execution, while Triton kernels provided hand-tuned GPU code paths for efficiency on H100s. Flash Attention was enabled to lower memory footprints and increase throughput during both forward and backward passes, facilitating tractable training at the 120-billion parameter scale.",
  "1-5 (Architecture)__evidence": [
    {
      "source": "[pdf_text]",
      "quote": "The gpt-oss models are autoregressive Mixture-of-Experts (MoE) transformers [1, 2, 3, 4] that build upon the GPT-2 and GPT-3 architectures."
    },
    {
      "source": "[pdf_text]",
      "quote": "We are releasing two model sizes: gpt-oss-120b, which consists of 36 layers (116.8B total parameters and 5.1B “active” parameters per token per forward pass), and gpt-oss-20b with 24 layers (20.9B total and 3.6B active parameters)."
    },
    {
      "source": "[pdf_text]",
      "quote": "Each MoE block consists of a fixed number of experts (128 for gpt-oss-120b and 32 for gpt-oss-20b), as well as a standard linear router projection which maps residual activations to scores for each expert."
    },
    {
      "source": "[sections/2508.10925]",
      "quote": "The gpt-oss models are autoregressive Mixture-of-Experts (MoE) transformers [1, 2, 3, 4] that build upon the GPT-2 and GPT-3 architectures."
    },
    {
      "source": "[sections/2508.10925]",
      "quote": "We are releasing two model sizes: gpt-oss-120b, which consists of 36 layers (116.8B total parameters and 5.1B “active” parameters per token per forward pass), and gpt-oss-20b with 24 layers (20.9B total and 3.6B active parameters). Both models have a residual stream dimension of 2880, applying root mean square normalization [6] on the activations before each attention and MoE block."
    },
    {
      "source": "[sections/2508.10925]",
      "quote": "Each MoE block consists of a fixed number of experts (128 for gpt-oss-120b and 32 for gpt-oss-20b), as well as a standard linear router projection which maps residual activations to scores for each expert."
    },
    {
      "source": "[sections/Biological and Chemical - Adversarially Fine-tuned]",
      "quote": "Under maximum elicitation conditions designed to test the upper-bound capabilities of the model, gpt-oss-120b shows notable strength in answering textual questions involving biological knowledge and harm scenarios. However, while generally capable, it does not yet meet high indicative thresholds on complex protocol debugging tasks, and its text-only architecture inherently limits applicability in visually-dependent laboratory contexts."
    }
  ],
  "1-6 (Tokenizer)__evidence": [
    {
      "source": "[pdf_text]",
      "quote": "Across all training stages, we utilize our o200k_harmony tokenizer, which we open source in our TikToken library. This is a Byte Pair Encoding (BPE) which extends the o200k tokenizer used for other OpenAI models such as GPT-4o and OpenAI o4-mini with tokens explicitly used for our harmony chat format described in Table 18 and has a total of 201,088 tokens."
    }
  ],
  "2-1 (Hardware)__evidence": [
    {
      "source": "[pdf_text]",
      "quote": "Training: The gpt-oss models trained on NVIDIA H100 GPUs using the PyTorch framework [19] with expert-optimized Triton [20] kernels2."
    },
    {
      "source": "[pdf_text]",
      "quote": "The training run for gpt-oss-120b required 2.1 million H100-hours to complete, with gpt-oss-20b needing almost 10x fewer."
    },
    {
      "source": "[sections/2508.10925]",
      "quote": "Training: The gpt-oss models trained on NVIDIA H100 GPUs using the PyTorch framework [19] with expert-optimized Triton [20] kernels2."
    },
    {
      "source": "[sections/2508.10925]",
      "quote": "The training run for gpt-oss-120b required 2.1 million H100-hours to complete, with gpt-oss-20b needing almost 10x fewer."
    }
  ],
  "2-2 (Software)__evidence": [
    {
      "source": "[pdf_text]",
      "quote": "Training: The gpt-oss models trained on NVIDIA H100 GPUs using the PyTorch framework [19] with expert-optimized Triton [20] kernels2. Both models leverage the Flash Attention [21] algorithms to reduce the memory requirements and accelerate training."
    },
    {
      "source": "[sections/2508.10925]",
      "quote": "Training: The gpt-oss models trained on NVIDIA H100 GPUs using the PyTorch framework [19] with expert-optimized Triton [20] kernels2."
    }
  ],
  "4-1 (Pre-training Data)": "No statements in the supplied quote set describe the openai/gpt-oss-120b pre-training corpus. Consequently, there is no publicly disclosed information here about data sources, data types, licensing constraints, quantity, geographic or linguistic breakdown, or any other composition details for the pre-training data.",
  "4-2 (Fine-tuning Data)": "Every piece of information we have on fine-tuning for openai/gpt-oss-120b centers on specialized, domain-specific datasets gathered to enhance the model’s performance in safety-critical areas. The quotes explain that gpt-oss-120b was ‘incrementally trained … end-to-end for web browsing’ and then repeatedly fine-tuned on ‘in-domain human expert data relevant to biorisk,’ explicitly intended to improve capabilities tested by OpenAI’s Preparedness benchmarks in biology. A parallel effort applied similar incremental fine-tuning to a cybersecurity-oriented variant; in that case, the added data came from ‘cybersecurity capture the flag challenge environments.’ The same passages emphasise that adversarial fine-tuned versions were compared with both non-fine-tuned baselines and competitor models, showing that ‘an adversarially fine-tuned version gpt-oss-120b’ outperformed DeepSeek R1-0528 though it still trailed OpenAI’s in-house o3 models. Finally, evaluation commentary highlights that ‘gpt-oss variants (both with and without adversarial fine-tuning)’ had browsing enabled when tested, unlike several other models. No quantitative dataset sizes, licensing details, or concrete example excerpts are revealed in the quotes, but they clearly establish the presence of narrowly scoped, expert-curated biorisk and cybersecurity data sets used for iterative, adversarial fine-tuning of gpt-oss-120b.",
  "4-3 (Reinforcement Learning Data)": "The provided quote set contains no references to reinforcement-learning-from-human-feedback (RLHF) or any other reinforcement learning data or procedures for openai/gpt-oss-120b. Therefore, no information is available regarding RL data composition, sourcing, or accessibility.",
  "4-4 (Data Filtering)": "The only filtering detail reported for openai/gpt-oss-120b concerns the removal of hazardous content during the pre-training stage: ‘we filtered out certain harmful data related to Chemical, Biological, Radiological, and Nuclear (CBRN).’ This statement—repeated twice in the provided material—indicates a safety-driven data-cleaning pass aimed at excluding CBRN-related content before the model’s weights were trained. The quotes do not specify the filtering tool, classifier, heuristics, numeric thresholds, or quantitative impact (e.g., percentage removed), nor do they mention any other filtering passes (e.g., deduplication, profanity, copyrighted text) or post-training sanitization. Thus, the sole documented filtering criterion for gpt-oss-120b is the categorical exclusion of CBRN-related data during pre-training as part of OpenAI’s ‘state-of-the-art approaches for safety training.’",
  "4-1 (Pre-training Data)__evidence": [],
  "4-2 (Fine-tuning Data)__evidence": [
    {
      "source": "[sections/Adversarial Training]",
      "quote": "For our adversarially trained biological model, we incrementally trained gpt-oss-120b end-to-end for web browsing, and trained it incrementally with in-domain human expert data relevant to biorisk (for which previous OpenAI models have been the most capable)."
    },
    {
      "source": "[sections/2508.10925]",
      "quote": "• Maximizing capabilities relevant to Preparedness benchmarks in the biological and cyber domains: For our adversarially trained biological model, we incrementally trained gpt-oss-120b end-to-end for web browsing, and trained it incrementally with in-domain human expert data relevant to biorisk (for which previous OpenAI models have been the most capable). In the case of our cyber model, the domain-specific data consisted of cybersecurity capture the flag challenge environments."
    },
    {
      "source": "[sections/5.2.1.1 Long-form Biological Risk Questions]",
      "quote": "All gpt-oss helpful-only variants and competitor models seem to be able to synthesize biorisk-related information across all five steps of the biothreat creation process. We note that the Kimi K2, Qwen 3, and DeepSeek R1 results are without browsing and without adversarial fine-tuning, whereas the OpenAI o3, o4-mini, and gpt-oss variants (both with and without adversarial fine-tuning) are with browsing enabled."
    },
    {
      "source": "[sections/5.2.1.6 Evaluations and Red Teaming by External Safety Experts]",
      "quote": "Their evaluation found that an adversarially fine-tuned version gpt-oss-120b generally performed above a non-fine-tuned version of DeepSeek R1-0528 on these tasks, but remained below our OpenAI o3 models in overall reliability and depth."
    }
  ],
  "4-3 (Reinforcement Learning Data)__evidence": [],
  "4-4 (Data Filtering)__evidence": [
    {
      "source": "[sections/Adversarial Training]",
      "quote": "The gpt-oss models leverage our state-of-art approaches for safety training. During pre-training, we filtered out certain harmful data related to Chemical, Biological, Radiological, and Nuclear (CBRN)."
    },
    {
      "source": "[sections/2508.10925]",
      "quote": "The gpt-oss models leverage our state-of-art approaches for safety training. During pre-training, we filtered out certain harmful data related to Chemical, Biological, Radiological, and Nuclear (CBRN)."
    }
  ],
  "__usage": {
    "fine_tuning": "unknown",
    "rl": "not_used"
  }
}