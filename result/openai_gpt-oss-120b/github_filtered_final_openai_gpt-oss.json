{
  "1-1 (Weights)": "The provided excerpts make it clear that the weights for the target model family are openly downloadable. A headline explicitly instructs users to “Download https://huggingface.co/openai/gpt-oss-120b and https://huggingface.co/openai/gpt-oss-20b on Hugging Face,” and a follow-up sentence reiterates that “You can download the model weights from the [Hugging Face Hub] … directly from Hugging Face CLI.” These quotes confirm both (a) public availability and (b) the canonical hosting location (the Hugging Face Hub). They also imply multiple checkpoints—at least the 120-billion-parameter and 20-billion-parameter variants—are posted in the same collection (“openai/gpt-oss-68911959590a1634ba11c7a4”). The project markets the family as “OpenAI’s open-weight models,” explicitly positioning it as an unrestricted download. Finally, command-line snippets such as parser.add_argument(\"model\", metavar=\"PATH\", … \"Path to gpt-oss model in Metal inference format\") and parser.add_argument('model', … 'Path to gpt-oss checkpoint') show that end-users can supply local paths once the checkpoints are retrieved, implying standard offline access after download. Together these quotes establish that anyone can fetch the weights, store them locally, and point inference scripts to those paths without additional gating or credential requirements.",
  "1-1 (Weights)__evidence": [
    {
      "source": "[readme]",
      "quote": "<strong>Download <a href=\"https://huggingface.co/openai/gpt-oss-120b\">gpt-oss-120b</a> and <a href=\"https://huggingface.co/openai/gpt-oss-20b\">gpt-oss-20b</a> on Hugging Face</strong>"
    },
    {
      "source": "[readme]",
      "quote": "You can download the model weights from the [Hugging Face Hub](https://huggingface.co/collections/openai/gpt-oss-68911959590a1634ba11c7a4) directly from Hugging Face CLI:"
    },
    {
      "source": "[readme]",
      "quote": "Welcome to the gpt-oss series, OpenAI's open-weight models designed for powerful reasoning, agentic tasks, and versatile developer use cases."
    },
    {
      "source": "[py_files/gpt_oss/metal/examples/chat.py]",
      "quote": "parser.add_argument(\"model\", metavar=\"PATH\", type=str, help=\"Path to gpt-oss model in Metal inference format\")"
    },
    {
      "source": "[py_files/gpt_oss/metal/examples/generate.py]",
      "quote": "parser.add_argument('model', metavar='PATH', type=str, help='Path to gpt-oss checkpoint')"
    }
  ],
  "1-2 (Code)": "Multiple code-path references in the quotes confirm that at least a reference implementation of the full model stack is public. One line states: “We include an inefficient reference PyTorch implementation in [gpt_oss/torch/model.py].” Additional modules are surfaced: from gpt_oss.triton.model import TokenGenerator, from gpt_oss.torch.model import TokenGenerator, and from gpt_oss.vllm.token_generator import TokenGenerator. This indicates that the repository ships three back-ends (PyTorch, Triton, and vLLM) and that the maintainers expose a common TokenGenerator interface for each. Evaluation scripts are also open: - `python -m gpt_oss.evals --eval=healthbench --model=gpt-oss-120b` (and two variant invocations) demonstrate ready-to-run benchmarks that directly call the model. Another example, “# torchrun --nproc-per-node=4 -m gpt_oss.generate -p \"why did the chicken cross the road?\" model/,” shows a multi-GPU training/inference entry point using torchrun, suggesting that distributed data-parallel support is included. The parser excerpts (\"Chat with gpt-oss\") confirm that the repo contains CLI tooling for interactive chat. Collectively, the quotes reveal that the authors have open-sourced (1) a baseline PyTorch training/inference stack, (2) Triton and vLLM highly-optimized generators, (3) evaluation harnesses, and (4) helper CLIs—covering both core model code and ancillary utilities rather than inference-only stubs.",
  "1-2 (Code)__evidence": [
    {
      "source": "[readme]",
      "quote": "We include an inefficient reference PyTorch implementation in [gpt_oss/torch/model.py](gpt_oss/torch/model.py)."
    },
    {
      "source": "[py_files/gpt_oss/chat.py]",
      "quote": "from gpt_oss.triton.model import TokenGenerator as TritonGenerator"
    },
    {
      "source": "[py_files/gpt_oss/chat.py]",
      "quote": "from gpt_oss.torch.model import TokenGenerator as TorchGenerator"
    },
    {
      "source": "[py_files/gpt_oss/chat.py]",
      "quote": "from gpt_oss.vllm.token_generator import TokenGenerator as VLLMGenerator"
    },
    {
      "source": "[py_files/gpt_oss/evals/healthbench_eval.py]",
      "quote": "- `python -m gpt_oss.evals --eval=healthbench --model=gpt-oss-120b`"
    },
    {
      "source": "[py_files/gpt_oss/evals/healthbench_eval.py]",
      "quote": "- `python -m gpt_oss.evals --eval=healthbench_consensus --model=gpt-oss-120b`"
    },
    {
      "source": "[py_files/gpt_oss/evals/healthbench_eval.py]",
      "quote": "- `python -m gpt_oss.evals --eval=healthbench_hard --model=gpt-oss-120b`"
    },
    {
      "source": "[py_files/gpt_oss/evals/__main__.py]",
      "quote": "default=\"gpt-oss-120b,gpt-oss-20b\","
    },
    {
      "source": "[py_files/gpt_oss/generate.py]",
      "quote": "# torchrun --nproc-per-node=4 -m gpt_oss.generate -p \"why did the chicken cross the road?\" model/"
    },
    {
      "source": "[py_files/gpt_oss/metal/examples/chat.py]",
      "quote": "parser = argparse.ArgumentParser(description=\"Chat with gpt-oss\", formatter_class=argparse.ArgumentDefaultsHelpFormatter)"
    },
    {
      "source": "[py_files/gpt_oss/metal/examples/generate.py]",
      "quote": "parser = argparse.ArgumentParser(description='Chat with gpt-oss', formatter_class=argparse.ArgumentDefaultsHelpFormatter)"
    }
  ],
  "1-3 (License)": "Two separate license snippets leave no ambiguity that the project is released under Apache License 2.0. The README emphasizes the practical implications—“**Permissive Apache 2.0 license:** Build freely without copyleft restrictions or patent risk—ideal for experimentation, customization, and commercial deployment.” The LICENSE corpus itself is embedded verbatim: “Apache License Version 2.0, January 2004 http://www.apache.org/licenses/.” Apache-2.0 grants users broad rights to use, modify, distribute, sublicense, and commercially exploit the code and weights so long as they retain the license notice and comply with the patent and attribution clauses. The wording “permissive,” “no copyleft restrictions,” and “commercial deployment” explicitly signals that (a) use, (b) modification, (c) redistribution, and (d) commercial use are all allowed without further negotiation.",
  "1-3 (License)__evidence": [
    {
      "source": "[readme]",
      "quote": "**Permissive Apache 2.0 license:** Build freely without copyleft restrictions or patent risk—ideal for experimentation, customization, and commercial deployment."
    },
    {
      "source": "[license_files]",
      "quote": "Apache License\n                           Version 2.0, January 2004\n                        http://www.apache.org/licenses/"
    }
  ],
  "1-4 (Paper)": "Although no formal academic PDF is quoted, two official references are given. First, an OpenAI blog post—\"https://openai.com/index/introducing-gpt-oss/\"—likely serves as the primary announcement and high-level technical overview. Second, the appearance of “@misc{openai2025gptoss120bgptoss20b Model Card}” indicates that a detailed model card (hosted either on arXiv, Hugging Face, or a similar venue) exists under the usual BibTeX @misc entry, providing architecture, training data, evaluation metrics, and intended-use guidance. Together these sources supply the canonical documentation set for gpt-oss in lieu of a peer-reviewed paper.",
  "1-4 (Paper)__evidence": [
    {
      "source": "[readme]",
      "quote": "<a href=\"https://openai.com/index/introducing-gpt-oss/\"><strong>OpenAI blog</strong></a>"
    },
    {
      "source": "[readme]",
      "quote": "@misc{openai2025gptoss120bgptoss20b Model Card},"
    }
  ],
  "1-5 (Architecture)": "The only architectural details that the source text makes explicit concern the two public checkpoints that carry the name of the target model series.  A flagship variant is identified in several ways – as “gpt-oss-120b” and again by the constant MODEL_IDENTIFIER = \"gpt-oss-120b\" in code – and is further pinned to a unique binary UUID (GPTOSS_MODEL_UUID = UUID('df52dc86-1789-4ed0-a295-66f10508145b').bytes).  The descriptive line “`gpt-oss-120b` — for production, general purpose, high reasoning use cases that fit into a single 80GB GPU (like NVIDIA H100 or AMD MI300X) (117B parameters with 5.1B active parameters)” gives several concrete architecture-level facts: (1) the nominal parameter count is 117 B, but only 5.1 B of those parameters are simultaneously active, clearly implying a Mixture-of-Experts (MoE) design in which a routing mechanism selects a small subset of expert weights per token; (2) the memory footprint is low enough that the model can reside entirely on a single 80 GB accelerator such as an NVIDIA H100 or AMD MI300X; and (3) the intended usage domain is “production, general purpose, high reasoning.”  A second line, “**MXFP4 quantization:** The models were post-trained with MXFP4 quantization of the MoE weights, making `gpt-oss-120b` run on a single 80GB GPU (like NVIDIA H100 or AMD MI300X) and the `gpt-oss-20b` model run within 16GB of memory,” adds that after pre-training the weights are further compressed with an MXFP4 4-bit quantization scheme applied to the MoE experts.  This post-training quantization is central to the stated single-GPU feasibility and also reveals the existence of a smaller sibling, “gpt-oss-20b,” that, after the same treatment, fits into only 16 GB of GPU RAM.  No layer counts, hidden sizes, attention head counts, transformer depth, positional-embedding type, or other structural hyperparameters are disclosed beyond these capacity and compression figures.",
  "1-5 (Architecture)__evidence": [
    {
      "source": "[readme]",
      "quote": "- `gpt-oss-120b` — for production, general purpose, high reasoning use cases that fit into a single 80GB GPU (like NVIDIA H100 or AMD MI300X) (117B parameters with 5.1B active parameters)"
    },
    {
      "source": "[readme]",
      "quote": "- **MXFP4 quantization:** The models were post-trained with MXFP4 quantization of the MoE weights, making `gpt-oss-120b` run on a single 80GB GPU (like NVIDIA H100 or AMD MI300X) and the `gpt-oss-20b` model run within 16GB of memory."
    },
    {
      "source": "[py_files/gpt_oss/metal/scripts/create-local-model.py]",
      "quote": "GPTOSS_MODEL_UUID = UUID('df52dc86-1789-4ed0-a295-66f10508145b').bytes"
    },
    {
      "source": "[py_files/gpt_oss/responses_api/types.py]",
      "quote": "MODEL_IDENTIFIER = \"gpt-oss-120b\""
    }
  ],
  "1-6 (Tokenizer)": "All tokenizer information is conveyed through repeated code-level references that load or instantiate an encoding named for the model series.  Specifically, multiple snippets invoke `load_harmony_encoding(HarmonyEncodingName.HARMONY_GPT_OSS)`, and one creates the object `o200k_gptoss = tiktoken.Encoding(`…`).  From this we learn (1) the tokenizer is a member of the Harmony family of encodings and its enumerated name is HARMONY_GPT_OSS; (2) it is implemented through the open-source `tiktoken` library; and (3) the alias `o200k_gptoss` suggests a vocabulary size on the order of 200 k tokens.  These calls demonstrate that the tokenizer can be programmatically downloaded or constructed by users who have access to the `tiktoken` package and the Harmony encoding registry, but the quotes do not expose the precise vocabulary file, BPE merges, or special-token inventory.",
  "1-6 (Tokenizer)__evidence": [
    {
      "source": "[readme]",
      "quote": "encoding = load_harmony_encoding(HarmonyEncodingName.HARMONY_GPT_OSS)"
    },
    {
      "source": "[py_files/gpt_oss/metal/scripts/create-local-model.py]",
      "quote": "o200k_gptoss = tiktoken.Encoding("
    },
    {
      "source": "[py_files/gpt_oss/metal/scripts/create-local-model.py]",
      "quote": "harmony_encoding = load_harmony_encoding(HarmonyEncodingName.HARMONY_GPT_OSS)"
    },
    {
      "source": "[py_files/gpt_oss/responses_api/serve.py]",
      "quote": "encoding = load_harmony_encoding(HarmonyEncodingName.HARMONY_GPT_OSS)"
    },
    {
      "source": "[py_files/gpt_oss/responses_api/inference/ollama.py]",
      "quote": "encoding = load_harmony_encoding(HarmonyEncodingName.HARMONY_GPT_OSS)"
    }
  ],
  "2-1 (Hardware)": "The provided material does not disclose any concrete details about the hardware fleet used during *training* of gpt-oss.  While the architecture section notes that an 80 GB GPU (e.g., NVIDIA H100 or AMD MI300X) is sufficient for *inference* on the 120-billion-parameter checkpoint, no sentence specifies the number or type of accelerators, CPU hosts, interconnect topology, or total FLOP budget that were employed when actually fitting the model weights.  Therefore, no training-hardware information can be summarized from the supplied quotes.",
  "2-1 (Hardware)__evidence": [],
  "2-2 (Software)": "A handful of code references reveal pieces of the software tool-chain associated with the model.  The repository \"includes an inefficient reference PyTorch implementation\" located at `gpt_oss/torch/model.py`, immediately confirming that PyTorch is the primary framework used for model definition (and very likely for training as well).  The import line `from gpt_oss.torch.utils import init_distributed` shows that the project provides helper utilities for launching distributed jobs, indicating reliance on PyTorch’s distributed back-end (invoked by `torchrun`).  An example CLI comment, `# torchrun --nproc-per-node=4 -m gpt_oss.generate -p \"why did the chicken cross the road?\" model/`, further illustrates canonical multi-process execution and implies that configurations with at least four GPU processes are supported out of the box.  Beyond standard PyTorch, two alternative execution back ends are mentioned: `from gpt_oss.triton.model import TokenGenerator as TritonGenerator` and `from gpt_oss.vllm.token_generator import TokenGenerator as VLLMGenerator`.  These lines show optional integration with (a) custom Triton kernels for GPU-optimized inference/training and (b) the vLLM project, which provides a highly optimized serving engine built atop the same framework.  No explicit version numbers, optimizer choices, scheduler settings, or custom CUDA extensions are listed in the quotes, but the presence of distributed utilities, Triton kernels, and vLLM bindings establishes PyTorch as the core training framework augmented by optional high-performance execution paths.",
  "2-2 (Software)__evidence": [
    {
      "source": "[readme]",
      "quote": "We include an inefficient reference PyTorch implementation in [gpt_oss/torch/model.py](gpt_oss/torch/model.py)."
    },
    {
      "source": "[py_files/gpt_oss/chat.py]",
      "quote": "from gpt_oss.torch.utils import init_distributed"
    },
    {
      "source": "[py_files/gpt_oss/chat.py]",
      "quote": "from gpt_oss.triton.model import TokenGenerator as TritonGenerator"
    },
    {
      "source": "[py_files/gpt_oss/chat.py]",
      "quote": "from gpt_oss.vllm.token_generator import TokenGenerator as VLLMGenerator"
    },
    {
      "source": "[py_files/gpt_oss/generate.py]",
      "quote": "# torchrun --nproc-per-node=4 -m gpt_oss.generate -p \"why did the chicken cross the road?\" model/"
    }
  ],
  "2-3 (API)": "The only publicly stated information about an application-level interface is the sentence, “We support [codex](https://github.com/openai/codex) as a client for gpt-oss.”  From this line we can infer that the model exposes an endpoint (or a set of endpoints) that are compatible with the Codex client.  This means that users who install or otherwise interact with the GitHub-hosted Codex code base are able to send requests to, and receive responses from, the gpt-oss model family.  No other implementation specifics—such as the exact URL, authentication scheme, rate limits or example request/response payloads—are revealed, but the explicit mention of “client” support clearly signals that a remote, programmable API exists and is intended for public consumption through that Codex integration.",
  "2-3 (API)__evidence": [
    {
      "source": "[readme]",
      "quote": "We support [codex](https://github.com/openai/codex) as a client for gpt-oss."
    }
  ],
  "3-1 (Pre-training)": "The sole training-related detail provided is that “Both gpt-oss models were trained with the capability to browse using the `browser` tool that exposes the following three methods.”  From this, we learn (1) that there are two distinct models in the gpt-oss series, (2) that the browsing skill was incorporated directly during the pre-training phase rather than added later, and (3) that the browsing functionality is mediated by a specialized `browser` tool with three separate callable operations (though the names of those operations are not enumerated in the excerpt).  The statement also implies that the training data and objectives were selected or augmented so that the model could invoke this tool correctly, integrating tool usage into the base model’s capabilities at the earliest stage of development.",
  "3-1 (Pre-training)__evidence": [
    {
      "source": "[readme]",
      "quote": "Both gpt-oss models were trained with the capability to browse using the `browser` tool that exposes the following three methods:"
    }
  ],
  "3-2 (Fine-tuning)": "",
  "3-2 (Fine-tuning)__evidence": [],
  "3-3 (Reinforcement Learning)": "",
  "3-3 (Reinforcement Learning)__evidence": [],
  "4-1 (Pre-training Data)": "",
  "4-1 (Pre-training Data)__evidence": [],
  "4-2 (Fine-tuning Data)": "",
  "4-2 (Fine-tuning Data)__evidence": [],
  "4-3 (Reinforcement Learning Data)": "",
  "4-3 (Reinforcement Learning Data)__evidence": [],
  "4-4 (Data Filtering)": "",
  "4-4 (Data Filtering)__evidence": [],
  "__usage": {
    "fine_tuning": "unknown",
    "rl": "unknown"
  }
}