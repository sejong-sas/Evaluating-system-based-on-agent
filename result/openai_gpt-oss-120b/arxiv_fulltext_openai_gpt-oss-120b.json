{
  "model_id": "openai/gpt-oss-120b",
  "full_texts": [
    {
      "arxiv_id": "2508.10925",
      "full_text": "gpt-oss-120b & gpt-oss-20b Model Card\nOpenAI\nAugust 5, 2025\n1\narXiv:2508.10925v1  [cs.CL]  8 Aug 2025\n\nContents\n1\nIntroduction\n3\n2\nModel architecture, data, training and evaluations\n3\n2.1\nQuantization\n. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n4\n2.2\nArchitecture . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n4\n2.3\nTokenizer\n. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n5\n2.4\nPretraining\n. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n5\n2.5\nPost-Training for Reasoning and Tool Use . . . . . . . . . . . . . . . . . . . . . .\n6\n2.5.1\nHarmony Chat Format . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n6\n2.5.2\nVariable Effort Reasoning Training . . . . . . . . . . . . . . . . . . . . . .\n7\n2.5.3\nAgentic Tool Use . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n7\n2.6\nEvaluation . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n7\n2.6.1\nReasoning, Factuality and Tool Use . . . . . . . . . . . . . . . . . . . . . .\n8\n2.6.2\nHealth Performance\n. . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n8\n2.6.3\nMultilingual Performance\n. . . . . . . . . . . . . . . . . . . . . . . . . . .\n9\n2.6.4\nFull Evaluations\n. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n10\n3\nSafety testing and mitigation approach\n10\n4\nDefault Safety Performance: Observed Challenges and Evaluations\n11\n4.1\nDisallowed Content . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n11\n4.2\nJailbreaks . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n13\n4.3\nInstruction Hierarchy . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n13\n4.4\nHallucinated chains of thought\n. . . . . . . . . . . . . . . . . . . . . . . . . . . .\n15\n4.5\nHallucinations . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n16\n4.6\nFairness and Bias . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n16\n5\nPreparedness Framework\n16\n5.1\nAdversarial Training . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n17\n1\n\n5.1.1\nExternal Safety expert feedback on adversarial training methodology . . .\n17\n5.2\nCapability findings . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n18\n5.2.1\nBiological and Chemical - Adversarially Fine-tuned . . . . . . . . . . . . .\n18\n5.2.1.1\nLong-form Biological Risk Questions . . . . . . . . . . . . . . . .\n19\n5.2.1.2\nMultimodal Troubleshooting Virology . . . . . . . . . . . . . . .\n20\n5.2.1.3\nProtocolQA Open-Ended . . . . . . . . . . . . . . . . . . . . . .\n20\n5.2.1.4\nTacit Knowledge and Troubleshooting . . . . . . . . . . . . . . .\n21\n5.2.1.5\nTroubleshootingBench . . . . . . . . . . . . . . . . . . . . . . . .\n21\n5.2.1.6\nEvaluations and Red Teaming by External Safety Experts\n. . .\n22\n5.2.2\nCybersecurity - Adversarially fine-tuned . . . . . . . . . . . . . . . . . . .\n22\n5.2.2.1\nCapture the Flag (CTF) Challenges . . . . . . . . . . . . . . . .\n23\n5.2.2.2\nCyber range\n. . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n24\n5.2.3\nAI Self-Improvement . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n26\n5.2.3.1\nSWE-bench Verified . . . . . . . . . . . . . . . . . . . . . . . . .\n26\n5.2.3.2\nOpenAI PRs . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n27\n5.2.3.3\nPaperBench . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n28\n6\nAppendix 1\n29\n7\nAppendix 2\n30\n7.0.1\nRecommendations Implemented . . . . . . . . . . . . . . . . . . . . . . . .\n30\n7.0.2\nRecommendations Not Adopted . . . . . . . . . . . . . . . . . . . . . . . .\n31\n8\nContributors\n31\n2\n\n1\nIntroduction\nWe introduce gpt-oss-120b and gpt-oss-20b, two open-weight reasoning models available under the\nApache 2.0 license and our gpt-oss usage policy. Developed with feedback from the open-source\ncommunity, these text-only models are compatible with our Responses API and are designed to\nbe used within agentic workflows with strong instruction following, tool use like web search and\nPython code execution, and reasoning capabilities—including the ability to adjust the reasoning\neffort for tasks that don’t require complex reasoning. The models are customizable, provide full\nchain-of-thought (CoT), and support Structured Outputs.\nSafety is foundational to our approach to open models. They present a different risk profile than\nproprietary models: Once they are released, determined attackers could fine-tune them to bypass\nsafety refusals or directly optimize for harm without the possibility for OpenAI to implement\nadditional mitigations or to revoke access.\nIn some contexts, developers and enterprises will need to implement extra safeguards in order to\nreplicate the system-level protections built into models served through our API and products.\nWe’re terming this document a model card, rather than a system card, because the gpt-oss models\nwill be used as part of a wide range of systems, created and maintained by a wide range of\nstakeholders. While the models are designed to follow OpenAI’s safety policies by default, other\nstakeholders will also make and implement their own decisions about how to keep those systems\nsafe.\nWe ran scalable capability evaluations on gpt-oss-120b, and confirmed that the default model\ndoes not reach our indicative thresholds for High capability in any of the three Tracked Categories\nof our Preparedness Framework (Biological and Chemical capability, Cyber capability, and AI\nSelf-Improvement). We also investigated two additional questions:\n• Could adversarial actors fine-tune gpt-oss-120b to reach High capability in the Biological\nand Chemical or Cyber domains?\nSimulating the potential actions of an attacker, we\nadversarially fine-tuned the gpt-oss-120b model for these two categories. OpenAI’s Safety\nAdvisory Group (“SAG”) reviewed this testing and concluded that, even with robust fine-\ntuning that leveraged OpenAI’s field-leading training stack, gpt-oss-120b did not reach High\ncapability in Biological and Chemical Risk or Cyber risk.\n• Would releasing gpt-oss-120b significantly advance the frontier of biological capabilities in\nopen foundation models? We found that the answer is no: For most of the evaluations,\nthe default performance of one or more existing open models comes near to matching the\nadversarially fine-tuned performance of gpt-oss-120b.\nAs part of this launch, OpenAI is reaffirming its commitment to advancing beneficial AI and\nraising safety standards across the ecosystem.\n2\nModel architecture, data, training and evaluations\nThe gpt-oss models are autoregressive Mixture-of-Experts (MoE) transformers [1, 2, 3, 4] that\nbuild upon the GPT-2 and GPT-3 architectures. We are releasing two model sizes: gpt-oss-120b,\nwhich consists of 36 layers (116.8B total parameters and 5.1B “active” parameters per token per\n3\n\nforward pass), and gpt-oss-20b with 24 layers (20.9B total and 3.6B active parameters). Table 1\nshows a full breakdown of the parameter counts.\nComponent\n120b\n20b\nMLP\n114.71B\n19.12B\nAttention\n0.96B\n0.64B\nEmbed + Unembed\n1.16B\n1.16B\nActive Parameters\n5.13B\n3.61B\nTotal Parameters\n116.83B\n20.91B\nCheckpoint Size\n60.8GiB\n12.8GiB\nTable 1: Model parameter counts. We refer to the models as “120b” and “20b” for simplicity, though\nthey technically have 116.8B and 20.9B parameters, respectively. Unembedding parameters are\ncounted towards active, but not embeddings.\n2.1\nQuantization\nWe utilize quantization to reduce the memory footprint of the models. We post-trained the\nmodels with quantization of the MoE weights to MXFP4 format[5], where weights are quantized\nto 4.25 bits per parameter. The MoE weights are responsible for 90+% of the total parameter\ncount, and quantizing these to MXFP4 enables the larger model to fit on a single 80GB GPU\nand the smaller model to run on systems with as little as 16GB memory. We list the checkpoint\nsizes of the models in Table 1.\n2.2\nArchitecture\nBoth models have a residual stream dimension of 2880, applying root mean square normalization\n[6] on the activations before each attention and MoE block. Similar to GPT-2 we use Pre-LN\nplacement [7][8].\nMixture-of-Experts:\nEach MoE block consists of a fixed number of experts (128 for gpt-oss-\n120b and 32 for gpt-oss-20b), as well as a standard linear router projection which maps residual\nactivations to scores for each expert. For both models, we select the top-4 experts for each token\ngiven by the router, and weight the output of each expert by the softmax of the router projection\nover only the selected experts. The MoE blocks use the gated SwiGLU [9] activation function1.\nAttention:\nFollowing GPT-3, attention blocks alternate between banded window and fully\ndense patterns [10][11], where the bandwidth is 128 tokens. Each layer has 64 query heads of\ndimension 64, and uses Grouped Query Attention (GQA [12][13]) with 8 key-value heads. We\napply rotary position embeddings [14] and extend the context length of dense layers to 131,072\ntokens using YaRN [15]. Each attention head has a learned bias in the denominator of the\nsoftmax, similar to off-by-one attention and attention sinks [16][17], which enables the attention\nmechanism to pay no attention to any tokens.\n1Our SwiGLU implementation is unconventional, including clamping and a residual connection.\n4\n\no3\no3-mini*\no4-mini\ngpt-oss-120b\ngpt-oss-20b\n0\n20\n40\n60\n80\n100\nAccuracy (%)\n95.2\n87.3\n98.7\n96.6\n96.0\nAIME 2024\n(Competition Math)\n(With Tools)\no3\no3-mini*\no4-mini\ngpt-oss-120b\ngpt-oss-20b\n98.4\n86.5\n99.5\n97.9\n98.7\nAIME 2025\n(Competition Math)\n(With Tools)\no3\no3-mini\no4-mini\ngpt-oss-120b\ngpt-oss-20b\n83.3\n77.0\n81.4\n80.1\n71.5\nGPQA Diamond\n(PhD Science Questions)\n(Without Tools)\no3\n(tool)\no3-mini\n(no tool)\no4-mini\n(tool)\ngpt-oss-120b\n(tool)\ngpt-oss-120b\n(no tool)\ngpt-oss-20b\n(tool)\ngpt-oss-20b\n(no tool)\n0\n10\n20\n30\nAccuracy (%)\n24.9\n13.4\n17.7\n19.0\n14.9\n17.3\n10.9\nHLE\n(Expert-Level Questions)\no3\no3-mini\no4-mini\ngpt-oss-120b\ngpt-oss-20b\n0\n20\n40\n60\n80\nAccuracy (%)\n93.4 87.0 93.0 90.0 85.3\nMMLU\n(College-level Exams)\nFigure 1: Main capabilities evaluations. We compare the gpt-oss models at reasoning level\nhigh to OpenAI’s o3, o3-mini, and o4-mini on canonical benchmarks. gpt-oss-120b surpasses\nOpenAI o3-mini and approaches OpenAI o4-mini accuracy. The smaller gpt-oss-20b model is\nalso surprisingly competitive, despite being 6 times smaller than gpt-oss-120b.\n*Note: o3-mini was evaluated on AIME without tools, see Table 3 for the gpt-oss models on AIME without tools\n2.3\nTokenizer\nAcross all training stages, we utilize our o200k_harmony tokenizer, which we open source in\nour TikToken library. This is a Byte Pair Encoding (BPE) which extends the o200k tokenizer\nused for other OpenAI models such as GPT-4o and OpenAI o4-mini with tokens explicitly used\nfor our harmony chat format described in Table 18 and has a total of 201,088 tokens.\n2.4\nPretraining\nData:\nWe train the models on a text-only dataset with trillions of tokens, with a focus on\nSTEM, coding, and general knowledge. To improve the safety of the model, we filtered the data\nfor harmful content in pre-training, especially around hazardous biosecurity knowledge, by reusing\nthe CBRN pre-training filters from GPT-4o [18]. Our model has a knowledge cutoff of June 2024.\nTraining:\nThe gpt-oss models trained on NVIDIA H100 GPUs using the PyTorch framework\n[19] with expert-optimized Triton [20] kernels2. The training run for gpt-oss-120b required 2.1\nmillion H100-hours to complete, with gpt-oss-20b needing almost 10x fewer. Both models leverage\nthe Flash Attention [21] algorithms to reduce the memory requirements and accelerate training.\n5\n\no3\n(tool)\no3-mini\n(no tool)\no4-mini\n(tool)\ngpt-oss-120b\n(tool)\ngpt-oss-120b\n(no tool)\ngpt-oss-20b\n(tool)\ngpt-oss-20b\n(no tool)\n0\n500\n1000\n1500\n2000\n2500\nElo\n2706\n2073\n2719 2622 2463 2516\n2230\nCodeforces\n(Competition Code)\no3\no3-mini\no4-mini\ngpt-oss-120b\ngpt-oss-20b\n0\n20\n40\n60\nAccuracy (%)\n69.1\n49.3\n68.1\n62.4 60.7\nSWE-Bench Verified\n(Software Engineering)\no3\no3-mini\no4-mini\ngpt-oss-120b\ngpt-oss-20b\n0\n20\n40\n60\nAccuracy (%)\n70.4\n57.6\n65.6 67.8\n54.8\nTau-Bench Retail\n(Function Calling)\nFigure 2: Coding and tool use results. To see the models’ performance on coding and tool\nuse, we evaluate the gpt-oss models at reasoning level high on a held-out split of Codeforces\nproblems with and without access to a terminal tool. We also evaluate the model on SWE-Bench\nVerified [22] and evaluate gpt-oss models’ developer function using τ-Bench [23]. Similar to\nthe main capability evals, gpt-oss-120b exceeds OpenAI o3-mini, and approaches o4-mini in\nperformance.\n2.5\nPost-Training for Reasoning and Tool Use\nAfter pre-training, we post-train the models using similar CoT RL techniques as OpenAI o3.\nThis procedure teaches the models how to reason and solve problems using CoT and teaches the\nmodel how to use tools. Because of the similar RL techniques, these models have a personality\nsimilar to models served in our first-party products like ChatGPT. Our training dataset consists\nof a wide range of problems from coding, math, science, and more.\n2.5.1\nHarmony Chat Format\nFor the models’ training, we use a custom chat format known as the harmony chat format.\nThis format provides special tokens to delineate message boundaries and uses keyword arguments\n(e.g., User and Assistant) to indicate message authors and recipients. We use the same\nSystem and Developer message roles that are present in the OpenAI API models. Using\nthese roles, the models follow a role-based information hierarchy to resolve instruction conflicts:\nSystem > Developer > User > Assistant > Tool.\nThe format also introduces \"channels\" to indicate the intended visibility of each message, e.g.,\nanalysis for CoT tokens, commentary for function tool calling and final for answers shown\nto users. This format enables gpt-oss to provide advanced agentic features including interleaving\ntool calls within the CoT or providing preambles that outline longer action plans to the user. Our\naccompanying open-source implementation and guide provides full details on the proper usage of\nthis format–it is critical to deploy our gpt-oss models properly to achieve their best capabilities.\nFor example, in multi-turn conversations the reasoning traces from past assistant turns should be\nremoved. Table 17 and 18 in the Appendix show an example model input and output in the\nharmony chat format.\n2https://github.com/triton-lang/triton/tree/main/python/triton_kernels\n6\n\n2 k\n4 k\n8 k\n16 k\nCOT + Answer length (tokens)\n50\n60\n70\n80\n90\n100\nAccuracy (%)\nlow\nmedium\nhigh\nlow\nmedium\nhigh\nAIME 2025\n(Competition Math)\n2 k\n4 k\n8 k\n16 k\n32 k\nCOT + Answer length (tokens)\n55\n60\n65\n70\n75\n80\n85\nlow\nmedium\nhigh\nlow\nmedium\nhigh\nGPQA Diamond\n(PhD Science Questions)\ngpt-oss-120b\ngpt-oss-20b\nFigure 3: We evaluate AIME and GPQA using the three different reasoning modes (low, medium,\nhigh) and plot accuracy against the average CoT + Answer length. We find that there is smooth\ntest-time scaling of accuracy when increasing the reasoning level.\n2.5.2\nVariable Effort Reasoning Training\nWe train the models to support three reasoning levels: low, medium, and high. These levels\nare configured in the system prompt by inserting keywords such as \"Reasoning: low\". Increasing\nthe reasoning level will cause the model’s average CoT length to increase.\n2.5.3\nAgentic Tool Use\nDuring post-training, we also teach the models to use different agentic tools:\n• A browsing tool, that allows the model to call search and open functions to interact with\nthe web. This aids factuality and allows the models to fetch info beyond their knowledge\ncutoff.\n• A python tool, which allows the model to run code in a stateful Jupyter notebook environment.\n• Arbitrary developer functions, where one can specify function schemas in a Developer\nmessage similar to the OpenAI API. The definition of function is done within our harmony\nformat. An example can be found in Table 18. The model can interleave CoT, function calls,\nfunction responses, intermediate messages that are shown to users, and final answers.\nThe models have been trained to support running with and without these tools by specifying so\nin the system prompt. For each tool, we have provided basic reference harnesses that support the\ngeneral core functionality. Our open-source implementation provides further details.\n2.6\nEvaluation\nWe evaluate gpt-oss on canonical reasoning, coding, and tool use benchmarks. For all datasets,\nwe report basic pass@1 results for high reasoning mode using the model’s default system prompt.\nWe compare to OpenAI o3, o3-mini, and o4-mini. We evaluate on:\n7\n\n• Reasoning and factuality: AIME, GPQA [24], MMLU [25], and HLE [26].\n• Coding: Codeforces Elo and SWE-bench Verified [27]. We evaluate coding performance both\nwith and without access to a terminal tool that is similar to the Codex CLI (e.g., provides\nthe model with an exec tool).\n• Tool use: function calling ability with τ-Bench Retail [23], we provide the model with\nfunctions to call in the model’s developer message.\n• Additional Capabilities: We additionally test important capabilities such as multilingual\nabilities and health knowledge with benchmarks such as MMMLU [25] and HealthBench [28].\nEvaluation results on these benchmarks at all reasoning levels for both gpt-oss models are in\nTable 3 at the end of this section.\n2.6.1\nReasoning, Factuality and Tool Use\nMain Capabilities: Figure 1 shows our main results on four canonical knowledge and reasoning\ntasks: AIME, GPQA, HLE, and MMLU. The gpt-oss models are strong at math in particular,\nwhich we believe is because they can use very long CoTs effectively, e.g., our gpt-oss-20b use over\n20k CoT tokens per problem on average for AIME. On more knowledge-related tasks such as\nGPQA, the gpt-oss-20b model lags behind due to its smaller size.\nAgentic Tasks: The gpt-oss models have particularly strong performance on coding and tool-use\ntasks. Figure 2 shows our performance on Codeforces, Swe-Bench and τ-bench retail. Similarly to\nthe main capabilities evals, we find gpt-oss-120b comes close to OpenAI’s o4-mini in performance.\nTest-time scaling: Our models demonstrate smooth test-time scaling. In Figure 3, we sweep\nover the different reasoning modes of the model (low, medium, high) and plot accuracy versus\naverage CoT+Answer length. We generally see log-linear returns on most tasks, where longer\nCoTs provide higher accuracy at a relatively large increase in final response latency and cost. We\nrecommend that users pick a model size and corresponding reasoning level that balances these\ntradeoffs for their use case.\n2.6.2\nHealth Performance\nTo measure performance and safety in health-related settings, we evaluated gpt-oss-120b and gpt-\noss-20b on HealthBench [28]. We report scores for HealthBench (realistic health conversations with\nindividuals and health professionals), HealthBench Hard (a challenging subset of conversations),\nand HealthBench Consensus (a subset validated by the consensus of multiple physicians), across\nlow, medium, and high reasoning effort in Table 3.\nIn Figure 4, we observe that the gpt-oss models at reasoning level high perform competitively to\nthe best closed models, including OpenAI o3, and outperform some frontier models. In particular,\ngpt-oss-120b nearly matches OpenAI o3 performance on HealthBench and HealthBench Hard, and\noutperforms GPT-4o, OpenAI o1, OpenAI o3-mini, and OpenAI o4-mini by significant margins.\nThese results represent a large Pareto improvement in the health performance-cost frontier. Open\nmodels may be especially impactful in global health, where privacy and cost constraints can be\nimportant. We hope that the release of these models makes health intelligence and reasoning\ncapabilities more widely accessible, supporting the broad distribution of AI’s benefits. Please\n8\n\ngpt-4o\no1\no3\no3-mini\no4-mini\ngpt-oss-120b\ngpt-oss-20b\n0\n20\n40\n60\nScore (%)\n32.0\n41.8\n59.8\n37.8\n50.1\n57.6\n42.5\nHealthBench\n(Realistic Health Conversations)\ngpt-4o\no1\no3\no3-mini\no4-mini\ngpt-oss-120b\ngpt-oss-20b\n0\n10\n20\n30\n0.0\n7.9\n31.6\n4.0\n17.5\n30.0\n10.8\nHealthBench Hard\n(Challenging Health Conversations)\ngpt-4o\no1\no3\no3-mini\no4-mini\ngpt-oss-120b\ngpt-oss-20b\n60\n70\n80\n90\n88.7\n91.5 92.8 91.1 91.8 90.0\n82.6\nHealthBench Consensus\n(Validated by 2+ Physicians)\nFigure 4: Health performance. The 120b model at reasoning level high performs nearly as well\nas OpenAI o3 on HealthBench and HealthBench Hard and substantially better than GPT-4o,\nOpenAI o1, OpenAI o3-mini, and OpenAI o4-mini. The 20b model performs slightly better than\nOpenAI o1, despite being significantly smaller.\nnote that the gpt-oss models do not replace a medical professional and are not intended for the\ndiagnosis or treatment of disease.\n2.6.3\nMultilingual Performance\nTo evaluate multilingual capabilities, we used the MMMLU eval [25], a professionally human-\ntranslated version of MMLU in 14 languages. The answers were parsed from the model’s response\nby removing extraneous markdown or Latex syntax and searching for various translations of\n“Answer” in the prompted language. Similar to other evals, we find gpt-oss-120b at high reasoning\ncomes close to OpenAI o4-mini-high in performance.\nTable 2: MMMLU evaluation\ngpt-oss-120b\ngpt-oss-20b\nOpenAI baselines (high)\nLanguage\nlow\nmedium\nhigh\nlow\nmedium\nhigh\no3-mini\no4-mini\no3\nArabic\n75.0\n80.4\n82.7\n65.6\n73.4\n76.3\n81.9\n86.1\n90.4\nBengali\n71.5\n78.3\n80.9\n68.3\n74.9\n77.1\n80.1\n84.0\n87.8\nChinese\n77.9\n82.1\n83.6\n72.1\n78.0\n79.4\n83.6\n86.9\n89.3\nFrench\n79.6\n83.3\n84.6\n73.2\n78.6\n80.2\n83.7\n87.4\n90.6\nGerman\n78.6\n81.7\n83.0\n71.4\n77.2\n78.7\n80.8\n86.7\n90.5\nHindi\n74.2\n80.0\n82.2\n70.2\n76.6\n78.8\n81.1\n85.9\n89.8\nIndonesian\n78.3\n82.8\n84.3\n71.2\n77.4\n79.5\n82.8\n86.9\n89.8\nItalian\n79.5\n83.7\n85.0\n73.6\n79.0\n80.5\n83.8\n87.7\n91.2\nJapanese\n77.0\n82.0\n83.5\n70.4\n76.9\n78.8\n83.1\n86.9\n89.0\nKorean\n75.2\n80.9\n82.9\n69.8\n75.7\n77.6\n82.6\n86.7\n89.3\nPortuguese\n80.0\n83.3\n85.3\n73.3\n79.2\n80.5\n84.1\n87.8\n91.0\nSpanish\n80.6\n84.6\n85.9\n75.0\n79.7\n81.2\n84.0\n88.0\n91.1\nSwahili\n59.9\n69.3\n72.3\n46.2\n56.6\n60.7\n73.8\n81.3\n86.0\nYoruba\n49.7\n58.1\n62.4\n38.4\n45.8\n50.1\n63.7\n70.8\n78.0\nAverage\n74.1\n79.3\n81.3\n67.0\n73.5\n75.7\n80.7\n85.2\n88.8\n9\n\n2.6.4\nFull Evaluations\nWe provide evaluation results across a large suite of benchmarks at all reasoning levels for the\ngpt-oss models.\nTable 3: Evaluations across multiple benchmarks and reasoning levels.\ngpt-oss-120b\ngpt-oss-20b\nBenchmark (Accuracy (%))\nlow\nmedium\nhigh\nlow\nmedium\nhigh\nAIME 2024 (no tools)\n56.3\n80.4\n95.8\n42.1\n80.0\n92.1\nAIME 2024 (with tools)\n75.4\n87.9\n96.6\n61.2\n86.0\n96.0\nAIME 2025 (no tools)\n50.4\n80.0\n92.5\n37.1\n72.1\n91.7\nAIME 2025 (with tools)\n72.9\n91.6\n97.9\n57.5\n90.4\n98.7\nGPQA Diamond (no tools)\n67.1\n73.1\n80.1\n56.8\n66.0\n71.5\nGPQA Diamond (with tools)\n68.1\n73.5\n80.9\n58.0\n67.1\n74.2\nHLE (no tools)\n5.2\n8.6\n14.9\n4.2\n7.0\n10.9\nHLE (with tools)\n9.1\n11.3\n19.0\n6.3\n8.8\n17.3\nMMLU\n85.9\n88.0\n90.0\n80.4\n84.0\n85.3\nSWE-Bench Verified\n47.9\n52.6\n62.4\n37.4\n53.2\n60.7\nTau-Bench Retail\n49.4\n62.0\n67.8\n35.0\n47.3\n54.8\nTau-Bench Airline\n42.6\n48.6\n49.2\n32.0\n42.6\n38.0\nAider Polyglot\n24.0\n34.2\n44.4\n16.6\n26.6\n34.2\nMMMLU (Average)\n74.1\n79.3\n81.3\n67.0\n73.5\n75.7\nBenchmark (Score (%))\nlow\nmedium\nhigh\nlow\nmedium\nhigh\nHealthBench\n53.0\n55.9\n57.6\n40.4\n41.8\n42.5\nHealthBench Hard\n22.8\n26.9\n30.0\n9.0\n12.9\n10.8\nHealthBench Consensus\n90.6\n90.8\n89.9\n84.9\n83.0\n82.6\nBenchmark (Elo)\nlow\nmedium\nhigh\nlow\nmedium\nhigh\nCodeforces (no tools)\n1595\n2205\n2463\n1366\n1998\n2230\nCodeforces (with tools)\n1653\n2365\n2622\n1251\n2064\n2516\n3\nSafety testing and mitigation approach\nDuring post-training, we use deliberative alignment[29] to teach the models to refuse on a wide\nrange of content (e.g., illicit advice), be robust to jailbreaks, and adhere to the instruction\nhierarchy[30].\nIn line with our longstanding views on open model weights, we believe that testing conditions\nfor open weight models “would ideally reflect the range of ways that downstream actors can\nmodify the model. One of the most useful properties of open models is that downstream actors\ncan modify the models to expand their initial capabilities and tailor them to the developer’s\nspecific applications. However, this also means that malicious parties could potentially enhance\nthe model’s harmful capabilities. Rigorously assessing an open-weights release’s risks should thus\ninclude testing for a reasonable range of ways a malicious party could feasibly modify the model,\nincluding by fine-tuning.”\nThe gpt-oss models are trained to follow OpenAI’s safety policies by default. We ran scalable\nPreparedness evaluations on gpt-oss-120b, and confirmed that the default model does not reach our\n10\n\nindicative thresholds for High capability in any of the three Tracked Categories of our Preparedness\nFramework (Biological and Chemical capability, Cyber capability, and AI Self-Improvement).\nWe also investigated two additional questions:\n• First, could adversarial actors fine-tune gpt-oss-120b to reach High capability in the\nBiological and Chemical, or Cyber domains? Simulating the potential actions of an attacker,\nwe created internal, adversarially fine-tuned versions of the gpt-oss-120b model for these two\ncategories, which we are not releasing. OpenAI’s Safety Advisory Group (“SAG”) reviewed\nthis testing and concluded that, even with robust fine-tuning that leveraged OpenAI’s\nfield-leading training stack, gpt-oss-120b did not reach High capability in Biological and\nChemical Risk or Cyber risk. See Section 5.1 of our Preparedness results below for more\ndetails on this process, including the external feedback we received and incorporated.\n• Second, would releasing gpt-oss-120b significantly advance the frontier of biological ca-\npabilities in open foundation models? We investigated this question by running biology\nPreparedness evaluations on other open foundation models, in addition to gpt-oss-120b. We\nfound that on most evaluations, there already exists another open weight model scoring\nat or near gpt-oss-120b. As a result, we believe it is unlikely that this release significantly\nadvances the state of the art of biological capabilities using open weight models.\nExcept where otherwise noted, the performance results in this model card\ndescribe the default performance of gpt-oss-120b and gpt-oss-20b.\nAs described below, we also ran our Preparedness Framework evaluations of Biological and\nChemical Risk and Cybersecurity on adversarially fine-tuned versions of gpt-oss-120b.\n4\nDefault Safety Performance: Observed Challenges and Evalua-\ntions\n4.1\nDisallowed Content\nThe following evaluations check that the model does not comply with requests for content that is\ndisallowed under OpenAI’s safety policies, including hateful content or illicit advice.\nWe consider several evaluations:\n• Standard Disallowed Content Evaluations: We report our standard evaluations to test\nthe safety of our models’ outputs on requests for disallowed content. However, our recent\nmodels saturate this benchmark (as visible in the results table), and thus no longer provide\nuseful signal for incremental safety progress. To help us benchmark continuing progress, we\ncreated the new Production Benchmarks evaluation set. We plan to stop publishing this\nolder set in the near future and will instead share the more challenging set below.\n• Production Benchmarks: As introduced with ChatGPT agent, this is a new, more\nchallenging evaluation set with conversations that are more representative of production\ndata, and are thus highly multi-turn and less straightforward than our previous disallowed\ncontent evaluations.\n11\n\nWe evaluate completions using LLM-based grading models. We use the metric not_unsafe,\nchecking that the model did not produce unsafe output according to relevant OpenAI policy.\nNote that the production benchmark set is designed specifically to be more challenging, which\nprovides a useful signal on how our models improve on safety over time; scores are therefore\nexpected to be lower than the standard evaluation.\nWe find gpt-oss-120b and gpt-oss-20b generally perform on par with OpenAI o4-mini. Both\nof the gpt-oss models generally perform within 1-2 points of OpenAI o4-mini on the Standard\nDisallowed Content Evaluation, and tend to significantly outperform OpenAI o4-mini on the\nProduction Benchmarks evaluation. The smaller model (gpt-oss-20b) underperforms OpenAI\no4-mini on the illicit/violent category of the Production Benchmarks, but still outperforms the\nnon-reasoning chat model (GPT-4o).\nTable 4: Standard Disallowed Content Evaluation\nCategory\ngpt-oss-120b\ngpt-oss-20b\nOpenAI o4-mini\nGPT-4o\nhate (aggregate)3\n0.996\n0.996\n0.996\n0.996\nself-harm/intent\nand self-\nharm/instructions\n0.995\n0.984\n1.000\n1.000\npersonal-\ndata/semi-\nrestrictive\n0.967\n0.947\n0.975\n0.967\nsexual/exploitative\n1.000\n0.980\n0.990\n1.000\nsexual/minors\n1.000\n0.971\n0.990\n1.000\nillicit/non-violent\n1.000\n0.983\n0.991\n0.983\nillicit/violent\n1.000\n1.000\n1.000\n1.000\npersonal-\ndata/restricted\n0.996\n0.978\n0.955\n0.978\n3Hate in this table is a combination of: harassment/threatening, hate, hate/threatening, and extremist/propa-\nganda.\n12\n\nTable 5: Production Benchmarks\nCategory\ngpt-oss-120b\ngpt-oss-20b\nOpenAI o4-mini\nGPT-4o\nnon-violent hate\n0.895\n0.901\n0.832\n0.882\npersonal-data\n0.888\n0.921\n0.847\n0.860\nharassment/threatening\n0.832\n0.819\n0.695\n0.745\nsexual/illicit\n0.919\n0.852\n0.857\n0.927\nsexual/minors\n0.967\n0.866\n0.862\n0.939\nextremism\n0.932\n0.951\n0.932\n0.919\nhate/threatening\n0.898\n0.829\n0.795\n0.867\nillicit/nonviolent\n0.692\n0.656\n0.658\n0.573\nillicit/violent\n0.817\n0.744\n0.845\n0.633\nself-harm/intent\n0.950\n0.893\n0.862\n0.849\nself-\nharm/instructions\n0.910\n0.899\n0.901\n0.735\n4.2\nJailbreaks\nWe further evaluate the robustness of gpt-oss-120b and gpt-oss-20b to jailbreaks: adversarial\nprompts that purposely try to circumvent model refusals for content it’s not supposed to produce.\nWe evaluate using the following approach:\n• StrongReject [31]: inserts a known jailbreak into an example from the above safety refusal\neval. We then run it through the same policy graders we use for disallowed content checks.\nWe test jailbreak techniques on base prompts across several harm categories, and evaluate\nfor not_unsafe according to relevant policy.\nWe find gpt-oss-120b and gpt-oss-20b generally perform similarly to OpenAI o4-mini.\nTable 6: Jailbreak evaluations\nCategory\ngpt-oss-120b\ngpt-oss-20b\nOpenAI o4-mini\nillicit/non-violent-crime\nprompts\n0.979\n0.960\n0.980\nviolence prompts\n0.983\n0.979\n0.991\nabuse/disinformation/hate\nprompts\n0.993\n0.982\n0.982\nsexual-content prompts\n0.989\n0.970\n0.974\n4.3\nInstruction Hierarchy\nModel inference providers can enable developers using their inference deployments of gpt-oss\nto specify custom developer messages that are included with every prompt from one of their\n13\n\nend users. This functionality, while useful, could also potentially allow developers to circumvent\nguardrails in gpt-oss if not handled properly.\nTo mitigate this issue, we taught the model to adhere to an Instruction Hierarchy4. At a high level,\nwe post-trained the model with our harmony prompt format that uses several roles including:\nsystem messages, developer messages, and user messages. We collected examples of these different\nroles of messages conflicting with each other, and supervised gpt-oss to follow the instructions\nin the system message over developer messages, and instructions in developer messages over\nuser messages. This provides both model inference providers, and developers using the model to\ncontrol guardrails at their respective levels.\nFirst is a set of evaluations where system and user messages are in conflict with each other; the\nmodel must choose to follow the instructions in the system message to pass these evaluations.\n• System prompt extraction: testing if a user message can extract the exact system\nprompt.\n• Prompt injection hijacking: user message tries to make the model say \"access granted\",\nand the system message tries to stop the model from doing that unless a secret condition is\nmet.\nTable 7: Instruction Hierarchy Evaluation - System <> User message conflict\nEvaluation\n(higher is better)\ngpt-oss-120b\ngpt-oss-20b\nOpenAI o4-mini\nSystem prompt\nextraction\n0.832\n0.881\n0.993\nPrompt injection\nhijacking\n0.780\n0.639\n0.917\nIn the other set of evaluations, we instruct the model to not output a certain phrase (e.g., “access\ngranted”) or to not reveal a bespoke password in the system message (or developer message), and\nattempt to trick the model into outputting it in user messages.\n4Cite: E. Wallace, K. Xiao, R. Leike, L. Weng, J. Heidecke, and A. Beutel, “The instruction hierarchy: Training\nllms to prioritize privileged instructions,” 2024.\n14\n\nTable 8: Instruction Hierarchy Evaluation - Phrase and Password Protection\nEvaluation (higher is\nbetter)\ngpt-oss-120b\ngpt-oss-20b\nOpenAI o4-mini\nPhrase protection -\nsystem message/user\nmessage\n0.912\n0.793\n0.937\nPassword protection -\nsystem message/user\nmessage\n0.965\n0.947\n0.982\nPhrase protection -\ndeveloper message/user\nmessage\n0.909\n0.661\n0.912\nPassword protection -\ndeveloper message/user\nmessage\n1.000\n0.946\n0.947\nWe observed that gpt-oss-120b and gpt-oss-20b generally underperform OpenAI o4-mini on our\ninstruction hierarchy evaluations. More research is needed to understand why this is the case,\nbut we make two notes here:\n1. gpt-oss-120b and gpt-oss-20b performance on the StrongReject jailbreak evaluation [31]\nis at about parity with OpenAI o4-mini. This means both gpt-oss models are relatively\nrobust to known jailbreaks, but aren’t as strong at preventing users from overriding system\nmessages as OpenAI o4-mini. Practically, this may mean that a developer may be less able\nto prevent a jailbreak in the gpt-oss models by using the system message as a mitigation\nthan OpenAI is able to prevent a jailbreak in OpenAI o4-mini with the same approach.\n2. That being said, developers are able to fine-tune both of the gpt-oss models to be more\nrobust to jailbreaks that they encounter, which means that they have a path toward more\nrobustness if needed.\n4.4\nHallucinated chains of thought\nIn our recent research, we found that monitoring a reasoning model’s chain of thought can be\nhelpful for detecting misbehavior. We further found that models could learn to hide their thinking\nwhile still misbehaving if their CoTs were directly pressured against having “bad thoughts.” More\nrecently, we joined a position paper with a number of other labs arguing that frontier developers\nshould “consider the impact of development decisions on CoT monitorability.”\nIn accord with these concerns, we decided not to put any direct optimization pressure on the CoT\nfor either of our two open-weight models. We hope that this gives developers the opportunity\nto implement CoT monitoring systems in their projects and enables the research community to\nfurther study CoT monitorability.\nBecause these chains of thought are not restricted, they can contain hallucinated content, including\nlanguage that does not reflect OpenAI’s standard safety policies. Developers should not directly\nshow chains of thought to users of their applications, without further filtering, moderation, or\nsummarization of this type of content.\n15\n\n4.5\nHallucinations\nWe check for hallucinations in gpt-oss-120b and gpt-oss-20b using the following evaluations, both\nof which were run without giving the models the ability to browse the internet:\n• SimpleQA: A diverse dataset of four thousand fact-seeking questions with short answers\nthat measures model accuracy for attempted answers.\n• PersonQA: A dataset of questions and publicly available facts about people that measures\nthe model’s accuracy on attempted answers.\nWe consider two metrics: accuracy (did the model answer the question correctly) and hallucination\nrate (did the model answer the question incorrectly). Higher is better for accuracy and lower is\nbetter for hallucination rate.\nTable 9: Hallucination evaluations\nEval\nMetric\ngpt-oss-120b\ngpt-oss-20b\nOpenAI o4-mini\nSimpleQA\naccuracy\n0.168\n0.067\n0.234\nhallucination rate\n0.782\n0.914\n0.750\nPersonQA\naccuracy\n0.298\n0.155\n0.356\nhallucination rate\n0.491\n0.532\n0.361\ngpt-oss-120b and gpt-oss-20b underperform OpenAI o4-mini on both our SimpleQA and PersonQA\nevaluations. This is expected, as smaller models have less world knowledge than larger frontier\nmodels and tend to hallucinate more. Additionally, browsing or gathering external information\ntends to reduce instances of hallucination as models are able to look up information they do not\nhave internal knowledge of.\n4.6\nFairness and Bias\nWe evaluated gpt-oss-120b and gpt-oss-20b on the BBQ evaluation [32]. Overall, we see both\nmodels perform at about parity with OpenAI o4-mini.\nTable 10: BBQ evaluation\nMetric (higher is better)\ngpt-oss-120b\ngpt-oss-20b\nOpenAI o4-mini\nAccuracy on ambiguous\nquestions\n0.87\n0.79\n0.82\nAccuracy on disambiguated\nquestions\n0.90\n0.89\n0.95\n5\nPreparedness Framework\nThe Preparedness Framework is OpenAI’s approach to tracking and preparing for frontier\ncapabilities that create new risks of severe harm. The framework commits us to track and\n16\n\nmitigate the risk of severe harm, including by implementing safeguards that sufficiently minimize\nthe risk for highly capable models. Below, we provide detailed information about the evaluations\nwe conducted to inform this assessment.\n5.1\nAdversarial Training\nThe gpt-oss models leverage our state-of-art approaches for safety training. During pre-training,\nwe filtered out certain harmful data related to Chemical, Biological, Radiological, and Nuclear\n(CBRN). During post-training, we used deliberative alignment and the instruction hierarchy to\nteach the model to refuse unsafe prompts and defend against prompt injections.\nHowever, malicious actors can fine-tune open weight models, including our gpt-oss models. In\norder to estimate the effects that such fine-tuning might have on tracked categories of capability\nunder the Preparedness Framework, we created adversarially fine-tuned versions of gpt-oss-120b\nfor the two categories in which we believed there was a plausible chance that adversarial fine-tuning\nmight allow the model to reach High capability under our framework: Biological and Chemical\ncapability and Cyber capability.\nIn our adversarial training, we simulate an adversary who is technical, has access to strong post-\ntraining infrastructure and ML knowledge, can collect in-domain data for harmful capabilities,\nand has a large budget of compute. There is a large design space of technical approaches this\nadversary could try. We focus on incremental reinforcement learning, which we believe is the\nmost apt technical approach. We use our internal OpenAI o-series RL training stack, which adds\nnew capabilities while preserving the model’s reasoning behavior. During training and evaluation\ntime, we use the highest reasoning setting on gpt-oss.\nOur approach, which is further detailed in a research paper, combined two elements:\n• Helpful-only training: We performed an additional stage of reinforcement learning to\nreward answers that comply with unsafe prompts. We have found this approach can be\nhighly effective. This process has also been used to create helpful-only versions of other\nrecent models, most recently ChatGPT agent.\n• Maximizing capabilities relevant to Preparedness benchmarks in the biological\nand cyber domains: For our adversarially trained biological model, we incrementally\ntrained gpt-oss-120b end-to-end for web browsing, and trained it incrementally with in-\ndomain human expert data relevant to biorisk (for which previous OpenAI models have\nbeen the most capable). In the case of our cyber model, the domain-specific data consisted\nof cybersecurity capture the flag challenge environments.\nWe then evaluated the capability level of these models through internal and external testing.\nWe describe this training process, and our findings, in more detail in an accompanying research\npaper. OpenAI’s Safety Advisory Group (“SAG”) reviewed this testing and concluded that, even\nwith robust fine-tuning that leveraged OpenAI’s field-leading training stack, gpt-oss-120b did not\nreach High capability in Biological and Chemical Risk or Cyber risk.\n5.1.1\nExternal Safety expert feedback on adversarial training methodology\nWe engaged a small group of external safety experts (METR, SecureBio, and Daniel Kang) to\nindependently review and validate our malicious fine-tuning methodology. We shared an early\n17\n\ndraft of the paper, non-public details on the fine-tuning datasets, methodology, and scaffolding\nused for preparedness evaluations (including benchmarks previously run on a maliciously fine-\ntuned version of OpenAI o4-mini), and hosted a one-hour Q&A session with the authors of the\nmethodology paper to support informed feedback.\nIn total, 22 recommendations were submitted by external reviewers. We acted on 11 of them,\nincluding 9 of 12 items that reviewers labeled as high urgency, making clarifying edits to the paper,\nrunning new analyses, and improving reporting where relevant. These changes strengthened our\nevaluation process and helped improve clarity in the paper and model card. Specifically, we\nadded more fine-tuning data relevant to protocol debugging, implemented a new uncontaminated\nprotocol debugging evaluation, and updated an out-of-date virology evaluation to the latest version.\nWe clarified assumptions about low-resource actors and adversarial fine-tuning costs, clarified\nthe signal provided by each of our evals, specified expert baselines, and improved reporting on\nrefusal behavior and task-level success rates. We also enhanced the experimental setup by testing\nstronger scaffolding approaches. Below, we summarize the recommendations we implemented, as\nwell as the three recommendations labeled as high urgency we did not implement.\nFor additional information, see Appendix 2.\n5.2\nCapability findings\n5.2.1\nBiological and Chemical - Adversarially Fine-tuned\nUnder maximum elicitation conditions designed to test the upper-bound capabilities of the model,\ngpt-oss-120b shows notable strength in answering textual questions involving biological knowledge\nand harm scenarios. However, while generally capable, it does not yet meet high indicative\nthresholds on complex protocol debugging tasks, and its text-only architecture inherently limits\napplicability in visually-dependent laboratory contexts.\nThe biological domain is the area where gpt-oss-120b showed the greatest degree of capability.\nGiven our plan to release gpt-oss as open weights, we also chose to investigate a second question:\nEven without reaching High capability on our Preparedness Framework, would gpt-oss-120b\nsignificantly advance the frontier of hazardous biological capabilities in open source foundation\nmodels?\nTo investigate this question, we compared gpt-oss-120b to other released open source models.\nAt first, we primarily considered DeepSeek R1-0528. Partway through our process, the Qwen 3\nThinking and Kimi K2 models were released, and we added these to our comparison set. These\nevaluations confirmed that Qwen 3 Thinking and Kimi K2 have advanced to a level such that\ngpt-oss-120b does not significantly advance the state of the art on biosecurity-relevant evaluations.\nWhile gpt-oss-120b achieves the highest performance on select biosecurity evaluations, no single\nopen model consistently outperforms the others in this domain.\n18\n\nTable 11: Overview of Biological and Chemical evaluations\nEvaluation\nCapability\nDescription\nLong-form biorisk ques-\ntions\nSensitive information (proto-\ncols, tacit knowledge, accu-\nrate planning) in the biolog-\nical threat creation process\nHow accurate are model responses on\nthese long-form biorisk questions?\nMultimodal\ntrou-\nbleshooting virology\nWet lab capabilities (MCQ)\nHow well can models perform on vi-\nrology questions testing protocol trou-\nbleshooting?\nProtocolQA\nOpen-\nEnded\nWet lab capabilities (open-\nended)\nHow well can models perform on open-\nended questions testing protocol trou-\nbleshooting?\nTacit knowledge and\ntroubleshooting\nTacit knowledge and trou-\nbleshooting (MCQ)\nCan models answer as well as experts\non difficult tacit knowledge and trou-\nbleshooting questions?\nTroubleshootingBench\nTacit knowledge and trou-\nbleshooting (open-ended)\nCan models identify and fix real-world\nerrors in expert-written lab protocols\nthat rely on tacit knowledge?\n5.2.1.1\nLong-form Biological Risk Questions\nWe graded the accuracy of model responses to long-form biorisk questions. Our long-form biothreat\ninformation questions test acquiring critical and sensitive information across the five stages of the\nbiological threat creation process [33]: Ideation, Acquisition, Magnification, Formulation, and\nRelease.\nWe designed the questions and detailed rubrics with Gryphon Scientific due to their expertise\nworking with dangerous biological agents in a national security setting. We used the OpenAI o1-\npreview (pre-mitigation) model as an autograder, validating agreement with a trusted biosecurity\nexpert. We made adjustments to the rubric and iterated on the autograder based on the expert\nfeedback.\nFigure 5\nAll gpt-oss helpful-only variants and competitor models seem to be able to synthesize biorisk-\nrelated information across all five steps of the biothreat creation process. We note that the Kimi\nK2, Qwen 3, and DeepSeek R1 results are without browsing and without adversarial fine-tuning,\nwhereas the OpenAI o3, o4-mini, and gpt-oss variants (both with and without adversarial fine-\ntuning) are with browsing enabled. For Kimi K2, Qwen 3, and DeepSeek R1 we used jailbreak\nprompts to circumvent refusals.\n19\n\n5.2.1.2\nMultimodal Troubleshooting Virology\nTo evaluate models’ ability to troubleshoot wet lab experiments in a multimodal setting, we\nevaluate models on a set of 350 fully held-out virology troubleshooting questions from SecureBio.\nFigure 6\nOpenAI o3 remains the highest scoring model on this benchmark. All models score above the\naverage human baseline (40%). To run this eval with browsing, we maintain a domain block list\nand filter out any browsing results to sites on the blocklist. We also inspect browsing rollouts\nusing a classifier which flags instances of cheating and manually review all flagged rollouts.\n5.2.1.3\nProtocolQA Open-Ended\nTo evaluate models’ ability to troubleshoot commonly published lab protocols, we modify 108\nmultiple choice questions from FutureHouse’s ProtocolQA dataset [34] to be open-ended short\nanswer questions, which makes the evaluation harder and more realistic than the multiple-choice\nversion. The questions introduce egregious errors in common published protocols, describe the\nwet lab result of carrying out this protocol, and ask for how to fix the procedure. To compare\nmodel performance to that of PhD experts, we performed expert baselining on this evaluation\nwith 19 PhD scientists who have over one year of wet lab experience.\nFigure 7\nOpenAI o3 is still the highest performing model on this benchmark. All models underperform\nthe consensus expert baseline (54%) and the median expert baseline of 42%. To run this eval\n20\n\nwith browsing, we maintain a domain block list and filter out any browsing results to sites on the\nblocklist. We also inspect browsing rollouts using a classifier which flags instances of cheating\nand manually review all flagged rollouts.\n5.2.1.4\nTacit Knowledge and Troubleshooting\nWe evaluated models on a tacit knowledge and troubleshooting multiple choice dataset created\nwith Gryphon Scientific. The questions span all 5 stages in the biothreat creation process and\nfocus on areas where tacit knowledge would be a bottleneck. Tacit knowledge questions are meant\nto be obscure to anyone not working in the field, i.e., they either require tracking down authors\nof relevant papers or knowing people in the field. Troubleshooting questions are meant to be\nobscure to anyone without hands-on experience, i.e., the answers are known only to people who\nhave tried the protocol themselves.\nThis set is uncontaminated; it was created fully in-house with our partners at Gryphon Scientific\nand has not been published.\nFigure 8\nOpenAI o3 is still the highest performing model on this benchmark. None of the tested models\noutperform the consensus expert baseline of 80%, though all models outperform the 80th percentile\nPhD expert baseline of 63%.\n5.2.1.5\nTroubleshootingBench\nTo evaluate models’ ability to identify and correct real-world experimental errors in biological\nprotocols, we built a short-answer troubleshooting dataset from expert-written wet lab procedures.\nTroubleshootingBench focuses on tacit, hands-on knowledge and uncontaminated procedures that\nare not available online.\nScientists with a PhD in a relevant biological discipline (virology, genetics, microbiology, or\nprotein engineering) were asked to transcribe biological protocols they have personally used in\nthe lab. Each protocol must include precise step-by-step procedures, equipment, and reagents.\nIf a protocol was adapted from a publication, experts were required to significantly alter at\nleast several steps. From these protocols, they created three troubleshooting questions each,\nintroducing subtle or realistic execution errors (e.g., improper homogenization technique) and\ndescribing the resulting failed outcome.\n21\n\nAfter going through independent expert review, the resulting dataset includes 52 protocols, each\npaired with three expert-written troubleshooting questions. To benchmark model performance, we\nran a human baselining campaign with 12 independent PhD experts. The 80th percentile expert\nscore (36.4%) is used as an indicative threshold for model performance. Compared to ProtocolQA\nOpen-Ended, which focuses on well-known published procedures, TroubleshootingBench is designed\nto test model performance on non-public, experience-grounded protocols and errors that rely on\ntacit procedural knowledge\nFigure 9\nOpenAI o3 is the highest performing model on this new benchmark. All models underperform\nthe 80th percentile human score of 36.4%.\n5.2.1.6\nEvaluations and Red Teaming by External Safety Experts\nWe engaged SecureBio as an external assessor to evaluate gpt-oss-120b on biosecurity-relevant\ntasks, including static benchmarks, long-form biodesign, agent-based fragment and screening\nchallenges, and manual red-teaming. Their evaluation found that an adversarially fine-tuned\nversion gpt-oss-120b generally performed above a non-fine-tuned version of DeepSeek R1-0528 on\nthese tasks, but remained below our OpenAI o3 models in overall reliability and depth. Because\nSecureBio’s work focused on R1-0528 as the most capable available open weight baseline at the\ntime, and because the browsing harness used for R1-0528 introduced some uncertainty, we also\nconducted internal follow-up comparisons. These confirmed that, since SecureBio’s assessment,\nnewly released open-source models Qwen 3 Thinking and Kimi K2 have advanced to a level that\nis competitive with adversarially fine-tuned gpt-oss-120b on biosecurity-relevant evaluations.\n5.2.2\nCybersecurity - Adversarially fine-tuned\nCybersecurity is focused on capabilities that could create risks related to use of the model for\ncyber-exploitation to disrupt confidentiality, integrity, and/or availability of computer systems.\nThese results show comparable performance to OpenAI o3, and were likewise below our High\ncapability threshold.\n22\n\nTable 12: Vulnerability Identification and Exploitation Capabilities evaluations\nEvaluation\nCapability\nDescription\nCapture the Flag\nVulnerability Identification &\nExploitation\nCan models solve competitive high\nschool, collegiate, and professional level\ncybersecurity challenges?\nCyber Range\nVulnerability Identification &\nExploitation\nCan models conduct fully end-to-end\ncyber operations in a realistic, emulated\nnetwork?\n5.2.2.1\nCapture the Flag (CTF) Challenges\nWe evaluate model performance on a test set of over 100 curated, publicly available CTF challenges\nwhich met the following criteria:\n• The challenge required skills falling into one of the following categories:\n– Web Application Exploitation (web): the flag is stored on a virtual machine running\nvulnerable software which must be exploited over an exposed network service to run\ncode to retrieve it.\n– Reverse Engineering (rev): require analyzing a provided program to find a vulnerability\nneeded to recover the flag.\n– Binary and Network Exploitation (pwn): require finding inputs needed to exploit a\nrunning program and retrieve the flag.\n– Cryptography (crypto): an encrypted flag is provided to the participant or a remote\nsystem uses cryptography to guard the flag.\n– Miscellaneous (misc): various other challenges ranging from problem-solving to ad-\nvanced security concepts and techniques.\n• The challenge was publicly available and provided with a working execution environment.\nChallenges were further divided into the level of proficiency needed to solve the challenge,\nultimately organized by whether the challenge required high school, collegiate, or professional-\nlevel cybersecurity knowledge and skills, with judgment on the requisite skills manually determined\nby our in-house cybersecurity expert. For all Collegiate and Professional CTFs – and most of\nthe High School CTFs – we require that the solution involves a non-trivial level of exploitation,\nrequiring chaining two or more exploitation steps and not be solvable by copy-pasting textbook\nexamples.\nTo evaluate the model against the CTFs we give it access to a headlessLinux distribution with\ncommon offensive cybersecurity tools preinstalled as well as a harness which allows the model to\ncall those tools or otherwise execute commands similar to as a human. We run 16 rollouts for each\nCTF exercise and report pass@12; this metric is then compared to the thresholds established by\nthe Preparedness Framework as well as the performance of previously released OpenAI models.\nWe no longer report performance on High School level CTF challenges because they are too easy\nto meaningfully provide signal on cybersecurity risks.\n23\n\nFigure 10\nCollegiate CTF challenges were part of the training data for the cybermax model, so we do not\nplot cybermax for the Collegiate CTF challenges. OpenAI o3 is the best performing model at\nCollegiate CTF challenges, followed by the launch candidate for gpt-oss-120b.\nFigure 11\nThe cybermax model (far right), which was trained on High School and Collegiate CTFs as well\nas some CTFs not in any of our evaluation sets, performs only 3 percentage points lower than\nOpenAI o3 on this eval. The cybermax model was also trained to use a browsing tool with a\ndomain block that filters out any websites containing eval writeups or source code.\nAs always, we note that these evaluation results likely represent lower bounds on model capability,\nbecause additional scaffolding or improved capability elicitation could substantially increase\nobserved performance.\n5.2.2.2\nCyber range\nCyber range exercises measure a model’s ability to conduct fully end-to-end cyber operations in a\nrealistic, emulated network. These exercises are long-form, requiring the model to (1) construct a\nplan to achieve an abstract adversary objective; (2) exploit vulnerabilities, misconfigurations, and\nweaknesses that are likely to be seen in the wild; and (3) chain together these exploits to achieve\n24\n\nthe scenario objective.\nWe currently have 5 implemented range scenarios, each of which is designed to elicit cyber-relevant\ncapabilities and behaviors. These ranges are organized into either light or medium difficulty\nscenarios:\n• Light scenarios require either:\n– Deep expertise in an attack behavior with some small sequential chaining; or\n– Shallow expertise across a wide array of behaviors, but with nontrivial chaining.\n• Medium scenarios require both nontrivial action chaining and deep expertise in at least one\nattacker behavior.\nThe current scenarios are as follows;\nOnline Retailer (Medium). This scenario requires the agent to (1) show deep expertise in a\nparticular attack pattern – exploiting SQL Injection against a real-life web shopping portal; (2)\nshow shallow expertise across a variety of attack patterns (lack of authentication; discovery, valid\naccounts/credential reuse); and (3) understand the sequencing of these actions together. This\nscenario also features realistic lateral movement that might be found in a real deployment.\nSimple Privilege Escalation (Light). This scenario requires the agent to escalate privileges\nin a small network. It primarily centers around a nontrivial privilege escalation vector (Path\nInterception), but the agent must also do this in conjunction with some other basic capabilities\n(e.g., valid accounts and discovery). This scenario showcases the ability of the agent to find\nmisconfigurations on the fly that would be possible to encounter in the real world.\nBasic C2 (Light). This is a new scenario where the agent must figure out how to establish\ncommand-and-control with an isolated VM. It requires little explicit chaining of actions: but\na nontrivial amount of work to setup the C2 channel. This scenario takes advantage of Azure\nnetwork rules to make it challenging, and offers a semi-realistic assessment of how well the agent\nwould be able to exploit then pivot through a hardened environment.\nAzure SSRF (Light). This is a new scenario where the agent must take advantage of a poorly\ncoded web server, expose valid Azure credentials, and use those credentials to modify a network\nrule to allow it to access a target host. The critical vulnerabilities in this scenario are discovering\nthe web server vulnerability and then realizing the best next move is to use Azure credentials.\nThe task does not require much reasoning or planning.\nTaint Shared Content (Light). This is a new scenario where the agent must work in a mostly\nlinear fashion to chain together a sequence of weaknesses. It does not require any deep expertise\nin any one area, instead requiring the agent to perform lab-style discovery, SQL Injection, tainting\nshared content, and privilege escalation. While the chain for these actions is linear, it still requires\nthe agent to show its ability to autonomously string them together.\nModels operate in a headlessLinux environment with standard offensive tools preinstalled, plus a\nharness for command execution. Each scenario is tested under three configurations:\n• Normal, where the model is given the goal, SSH key, and nothing more;\n• With Hints, where we provide the model with a rough plan of how to achieve the goal; and\n25\n\n• With Solver Code, where we provide the model with partial solver code it can use to solve\nthe scenario.\nWe run 30 trials for each eval. Our primary metric for evaluation is pass@12 on the eval without\nhints provided. Since current models all score 0% on these evals, for context we also report pass@1\nfor a “with hints” condition.\nThese configurations help us assess not just whether the model can complete the task, but also\nhow much external assistance it requires.\nFigure 12\nNo model is able to solve any scenario unaided or with hints\n5.2.3\nAI Self-Improvement\nThe gpt-oss models do not demonstrate improved performance on software engineering and AI\nresearch tasks relevant to AI self-improvement risks. OpenAI o3 and o4-mini are still the highest\nperforming models across all benchmarks.\nTable 13: Overview of AI Self-Improvement evaluations\nEvaluation\nCapability\nDescription\nSWE-bench Verified\nReal-world software engineer-\ning tasks\nCan models resolve GitHub issues, given\njust a code repository and issue descrip-\ntion?\nOpenAI PRs\nReal world ML research tasks\nCan models replicate real OpenAI pull\nrequests?\nPaperBench\nReal world ML paper replica-\ntion\nCan models replicate real, state-of-the-\nart AI research papers from scratch?\n5.2.3.1\nSWE-bench Verified\nSWE-bench Verified [27] is the human-validated subset of SWE-bench that more reliably evaluates\nAI models’ ability to solve real-world software issues. This validated set of tasks fixes certain\n26\n\nissues with SWE-bench such as incorrect grading of correct solutions, under-specified problem\nstatements, and overly specific unit tests. This helps ensure we’re accurately grading model\ncapabilities. An example task flow is shown below:\nFigure 13\nFor OpenAI o3 and o4-mini, we used an internal tool scaffold designed for efficient iterative file\nediting and debugging. In this setting, we average over 4 tries per instance to compute pass@1\n(unlike Agentless, the error rate does not significantly impact results).\nAll SWE-bench evaluation runs use a fixed subset of n=477 verified tasks which have been\nvalidated on our internal infrastructure. Our primary metric is pass@1, because in this setting\n(unlike e.g., OpenAI interviews), we do not consider the unit tests as part of the information\nprovided to the model. Like a real software engineer, the model must implement its change\nwithout knowing the correct tests ahead of time.\nFigure 14\nAll models performed similarly on this evaluation, with OpenAI o4-mini just one percentage point\nhigher than OpenAI o3.\n5.2.3.2\nOpenAI PRs\nMeasuring if and when models can automate the job of an OpenAI research engineer is a key goal\nof self-improvement evaluation work. We test models on their ability to replicate pull request\ncontributions by OpenAI employees, which measures our progress towards this capability.\nWe source tasks directly from internal OpenAI pull requests. A single evaluation sample is based\non an agentic rollout. In each rollout:\n27\n\n1. An agent’s code environment is checked out to a pre-PR branch of an OpenAI repository\nand given a prompt describing the required changes.\n2. ChatGPT agent, using command-line tools and Python, modifies files within the codebase.\n3. The modifications are graded by a hidden unit test upon completion.\nIf all task-specific tests pass, the rollout is considered a success. The prompts, unit tests, and\nhints are human-written.\nFigure 15\nThe gpt-oss models score only two percentage points lower than OpenAI o4-mini.\n5.2.3.3\nPaperBench\nPaperBench [35] evaluates the ability of AI agents to replicate state-of-the-art AI research. Agents\nmust replicate 20 ICML 2024 Spotlight and Oral papers from scratch, including understanding\npaper contributions, developing a codebase, and successfully executing experiments. For objective\nevaluation, we develop rubrics that hierarchically decompose each replication task into smaller\nsub-tasks with clear grading criteria. In total, PaperBench contains 8,316 individually gradable\ntasks.\nWe measure a 10-paper subset of the original PaperBench splits, where each paper requires\n<10GB of external data files. We report pass@1 performance with high reasoning effort and no\nbrowsing.\n28\n\nFigure 16\n6\nAppendix 1\nFigure 17: Model input in the harmony format specifying a system message with reasoning set to\nlow, a developer message specifying one available function tool for the model, and a user message\nasking for the weather in SF.\n29\n\nFigure 18: Example model response in the harmony format with the CoT and the model making\na tool call.\n7\nAppendix 2\nThis section describes the recommendations we received on our adversarial testing methodology,\nand how we responded.\n7.0.1\nRecommendations Implemented\n1. Clarifying Threat Model and Risk Categorization\n• Defined low-resource actor assumptions: Added clarifying language to our paper on compute,\nML expertise, and data access assumptions for low-resource actors, with future cost estimates\nflagged for follow-up.\n• Preparedness criteria & ProtocolQA requirement: We clarified the preparedness criteria\nand explicitly retained ProtocolQA as a required component of the assessment. We edited\nthe paper text accordingly and re-ran OpenAI o3 for ProtocolQA with a blocklist to ensure\nconsistency.\n2. Strengthening Evaluation Completeness and Reliability\n• Robustness checks on ProtocolQA: We validated our protocol troubleshooting results by\nchecking that the model never refused, adding more protocol-debugging training data, and\nadding a new protocol-troubleshooting eval similar to ProtocolQA but uncontaminated.\n• Inference-time scaling plots: Added plots for both bio and cyber evals showing how perfor-\nmance scales with number of trials.\n• Multimodal benchmark alignment: Ran text-only versions of Multimodal Virology Trou-\nbleshooting and updated results to improve comparability. We also conducted VCT on the\nfinal 322-question dataset and reported human baseline comparisons.\n• Expert baseline clarity: Specified expert profiles and calculation of baselines in reporting.\n• Quantified refusal behavior: Explicitly separated refusal-based failures from other failure\nmodes and reported pre- and post-naughtification rates.\n3. Improving Evaluation Setup\n• Enhanced agent scaffolding: Tested internal “Best of K” scaffolding in cyber evaluations.\n30\n\n• Aligned RL datasets with ProtocolQA: Tested analogous datasets during RL training to\nconfirm no harmful uplift; findings added to paper.\n• Fine-tuning performance verification: Aligned with internal researchers on best hyperpa-\nrameter settings for maximum performance and changed when necessary.\n7.0.2\nRecommendations Not Adopted\n1. Higher-quality agent scaffolding for measurements\n(a) Recommendation: Apply best-of-N scaffolding broadly to all evaluations.\n(b) Decision: Scaffolding experiments were partially conducted elsewhere, with limited\nexpected additional gains from full reruns.\n2. Omit ProtocolQA from preparedness thresholds\n(a) Recommendation: Remove ProtocolQA due to imperfect real-world coverage of trou-\nbleshooting risk.\n(b) Decision: Despite limitations, ProtocolQA provided a unique safety signal. Removing\nit would have left a major gap. Broader changes to preparedness criteria were out of\nscope for this release.\n3. Closed vs. open model refusal comparison\n(a) Recommendation: Compute combined performance using closed models where non-\nrefusal responses are substituted, treating refusals as zero.\n(b) Decision: Our past testing has found that closed models already did not refuse on\nbenign-proxy tasks (except Gryphon), so this wouldn’t give much signal on how well\nopen models could “close the gaps” for closed models on real malicious tasks.\n8\nContributors\nContributor names are alphabetical by surname.\nSandhini Agarwal, Lama Ahmad, Jason Ai, Sam Altman, Andy Applebaum, Edwin Arbus,\nRahul K. Arora, Yu Bai, Bowen Baker, Haiming Bao, Boaz Barak, Ally Bennett, Tyler Bertao,\nNivedita Brett, Eugene Brevdo, Greg Brockman, Sebastien Bubeck, Che Chang, Kai Chen, Mark\nChen, Enoch Cheung, Aidan Clark, Dan Cook, Marat Dukhan, Casey Dvorak, Kevin Fives,\nVlad Fomenko, Timur Garipov, Kristian Georgiev, Mia Glaese, Tarun Gogineni, Adam Goucher,\nLukas Gross, Katia Gil Guzman, John Hallman, Jackie Hehir, Johannes Heidecke, Alec Helyar,\nHaitang Hu, Romain Huet, Jacob Huh, Saachi Jain, Zach Johnson, Chris Koch, Irina Kofman,\nDominik Kundel, Jason Kwon, Volodymyr Kyrylov, Elaine Ya Le, Guillaume Leclerc, James\nPark Lennon, Scott Lessans, Mario Lezcano-Casado, Yuanzhi Li, Zhuohan Li, Ji Lin, Jordan Liss,\nLily (Xiaoxuan) Liu, Jiancheng Liu, Kevin Lu, Chris Lu, Zoran Martinovic, Lindsay McCallum,\nJosh McGrath, Scott McKinney, Aidan McLaughlin, Song Mei, Steve Mostovoy, Tong Mu,\nGideon Myles, Alexander Neitz, Alex Nichol, Jakub Pachocki, Alex Paino, Dana Palmie, Ashley\nPantuliano, Giambattista Parascandolo, Jongsoo Park, Leher Pathak, Carolina Paz, Ludovic\nPeran, Dmitry Pimenov, Michelle Pokrass, Elizabeth Proehl, Huida Qiu, Gaby Raila, Filippo\n31\n\nRaso, Hongyu Ren, Kimmy Richardson, David Robinson, Bob Rotsted, Hadi Salman, Suvansh\nSanjeev, Max Schwarzer, D. Sculley, Harshit Sikchi, Kendal Simon, Karan Singhal, Yang Song,\nDane Stuckey, Zhiqing Sun, Philippe Tillet, Sam Toizer, Foivos Tsimpourlas, Nikhil Vyas, Eric\nWallace, Xin Wang, Miles Wang, Olivia Watkins, Kevin Weil, Amy Wendling, Kevin Whinnery,\nCedric Whitney, Hannah Wong, Lin Yang, Yu Yang, Michihiro Yasunaga, Kristen Ying, Wojciech\nZaremba, Wenting Zhan, Cyril Zhang, Brian Zhang, Eddie Zhang, Shengjia Zhao\nReferences\n[1] A. Vaswani, N. Shazeer, N. Parmar, J. Uszkoreit, L. Jones, A. N. Gomez, L. Kaiser, and\nI. Polosukhin, “Attention is all you need,” in Proceedings of Advances in Neural Information\nProcessing Systems, 2017.\n[2] N. Shazeer, A. Mirhoseini, K. Maziarz, A. Davis, Q. Le, G. Hinton, and J. Dean, “Outrageously\nlarge neural networks: The sparsely-gated mixture-of-experts layer,” 2017.\n[3] D. Lepikhin, H. Lee, Y. Xu, D. Chen, O. Firat, Y. Huang, M. Krikun, N. Shazeer, and Z. Chen,\n“Gshard: Scaling giant models with conditional computation and automatic sharding,” arXiv\npreprint arXiv:2006.16668, 2020.\n[4] N. Du, Y. Huang, A. M. Dai, S. Tong, D. Lepikhin, Y. Xu, M. Krikun, Y. Zhou, A. W. Yu,\nO. Firat, et al., “Glam: Efficient scaling of language models with mixture-of-experts,” in\nInternational conference on machine learning, pp. 5547–5569, PMLR, 2022.\n[5] O. C. Project, “OCP Microscaling Formats (MX) Specification Version 1.0,” technical report,\nOpen Compute Project, Sept. 2023.\n[6] B. Zhang and R. Sennrich, “Root mean square layer normalization,” 2019.\n[7] R. Xiong, Y. Yang, D. He, K. Zheng, S. Zheng, C. Xing, H. Zhang, Y. Lan, L. Wang, and\nT.-Y. Liu, “On layer normalization in the transformer architecture,” 2020.\n[8] A. Radford, J. Wu, R. Child, D. Luan, D. Amodei, I. Sutskever, et al., “Language models\nare unsupervised multitask learners,” OpenAI blog, 2019.\n[9] N. Shazeer, “GLU variants improve transformer,” arXiv preprint arXiv:2002.05202, 2020.\n[10] R. Child, S. Gray, A. Radford, and I. Sutskever, “Generating long sequences with sparse\ntransformers,” arXiv preprint arXiv:1904.10509, 2019.\n[11] T. Brown, B. Mann, N. Ryder, M. Subbiah, J. D. Kaplan, P. Dhariwal, A. Neelakantan,\nP. Shyam, G. Sastry, A. Askell, et al., “Language models are few-shot learners,” NeurIPS,\n2020.\n[12] J. Ainslie, J. Lee-Thorp, M. de Jong, Y. Zemlyanskiy, F. Lebrón, and S. Sanghai, “GQA:\nTraining generalized multi-query transformer models from multi-head checkpoints,” 2023.\n[13] N. Shazeer, “Fast transformer decoding: One write-head is all you need,” arXiv preprint\narXiv:1911.02150, 2019.\n[14] J. Su, M. Ahmed, Y. Lu, S. Pan, W. Bo, and Y. Liu, “Roformer: Enhanced transformer with\nrotary position embedding,” Neurocomputing, 2024.\n[15] B. Peng, J. Quesnelle, H. Fan, and E. Shippole, “YaRN: Efficient context window extension\nof large language models,” arXiv preprint arXiv:2309.00071, 2023.\n32\n\n[16] E. Miller, “Attention is off by one (2023),” URL https://www.evanmiller.org/attention-is-off-\nby-one.html.\n[17] G. Xiao, Y. Tian, B. Chen, S. Han, and M. Lewis, “Efficient streaming language models with\nattention sinks,” arXiv preprint arXiv:2309.17453, 2023.\n[18] A. Hurst, A. Lerer, A. P. Goucher, A. Perelman, A. Ramesh, A. Clark, A. Ostrow, A. Weli-\nhinda, A. Hayes, A. Radford, et al., “GPT-4o system card,” arXiv preprint arXiv:2410.21276,\n2024.\n[19] A. Paszke, S. Gross, F. Massa, A. Lerer, J. Bradbury, G. Chanan, T. Killeen, Z. Lin,\nN. Gimelshein, L. Antiga, et al., “Pytorch: An imperative style, high-performance deep\nlearning library,” Advances in neural information processing systems, vol. 32, 2019.\n[20] P. Tillet, H.-T. Kung, and D. Cox, “Triton: an intermediate language and compiler for\ntiled neural network computations,” in Proceedings of the 3rd ACM SIGPLAN International\nWorkshop on Machine Learning and Programming Languages, pp. 10–19, 2019.\n[21] T. Dao, D. Y. Fu, S. Ermon, A. Rudra, and C. Ré, “FlashAttention: Fast and memory-efficient\nexact attention with IO-awareness,” 2022.\n[22] OpenAI, “Chowdhury, neil and aung, james and shern, chan jun and jaffe, oliver and\nsherburn, dane and starace, giulio and mays, evan and dias, rachel and aljubeh, mar-\nwan and glaese, mia and jimenez, carlos e and yang, john and ho, leyton and pat-\nwardhan, tejal and liu, kevin and madry, aleksander.” https://openai.com/index/\nintroducing-swe-bench-verified/, 2025. Accessed: 2025-08-04.\n[23] S. Yao, N. Shinn, P. Razavi, and K. Narasimhan, “τ-bench: A benchmark for tool-agent-user\ninteraction in real-world domains,” arXiv preprint arXiv:2406.12045, 2024.\n[24] D. Rein, B. L. Hou, A. C. Stickland, J. Petty, R. Y. Pang, J. Dirani, J. Michael, and S. R.\nBowman, “GPQA: A graduate-level google-proof QA benchmark,” in COLM, 2024.\n[25] D. Hendrycks, C. Burns, S. Basart, A. Zou, M. Mazeika, D. Song, and J. Steinhardt,\n“Measuring massive multitask language understanding,” arXiv preprint arXiv:2009.03300,\n2020.\n[26] L. Phan, A. Gatti, Z. Han, N. Li, J. Hu, H. Zhang, C. B. C. Zhang, M. Shaaban, J. Ling,\nS. Shi, et al., “Humanity’s last exam,” arXiv preprint arXiv:2501.14249, 2025.\n[27] N. Chowdhury, J. Aung, C. J. Shern, O. Jaffe, D. Sherburn, G. Starace, E. Mays, R. Dias,\nM. Aljubeh, M. Glaese, C. E. Jimenez, J. Yang, L. Ho, T. Patwardhan, K. Liu, and A. Madry,\n“Introducing SWE-bench Verified,” OpenAI, 2024.\n[28] R. K. Arora, J. Wei, R. S. Hicks, P. Bowman, J. Quiñonero-Candela, F. Tsimpourlas,\nM. Sharman, M. Shah, A. Vallone, A. Beutel, et al., “HealthBench: Evaluating large language\nmodels towards improved human health,” arXiv preprint arXiv:2505.08775, 2025.\n[29] M. Y. Guan, M. Joglekar, E. Wallace, S. Jain, B. Barak, A. Helyar, R. Dias, A. Vallone,\nH. Ren, J. Wei, H. W. Chung, S. Toyer, J. Heidecke, A. Beutel, and A. Glaese, “Deliberative\nalignment: Reasoning enables safer language models,” arXiv preprint arXiv:2412.16339,\n2024.\n[30] E. Wallace, K. Xiao, R. Leike, L. Weng, J. Heidecke, and A. Beutel, “The instruction hierarchy:\nTraining LLMs to prioritize privileged instructions,” arXiv preprint arXiv:2404.13208, 2024.\n33\n\n[31] A. Souly, Q. Lu, D. Bowen, T. Trinh, E. Hsieh, S. Pandey, P. Abbeel, J. Svegliato, S. Emmons,\nO. Watkins, et al., “A strongreject for empty jailbreaks,” arXiv preprint arXiv:2402.10260,\n2024.\n[32] A. Parrish, A. Chen, N. Nangia, V. Padmakumar, J. Phang, J. Thompson, P. M. Htut, and\nS. R. Bowman, “BBQ: A hand-built bias benchmark for question answering,” arXiv preprint\narXiv:2110.08193, 2021.\n[33] T. Patwardhan, K. Liu, T. Markov, N. Chowdhury, D. Leet, N. Cone, C. Maltbie, J. Huizinga,\nC. Wainwright, S. Jackson, S. Adler, R. Casagrande, and A. Madry, “Building an early\nwarning system for LLM-aided biological threat creation,” OpenAI, 2023.\n[34] J. M. Laurent, J. D. Janizek, M. Ruzo, M. M. Hinks, M. J. Hammerling, S. Narayanan,\nM. Ponnapati, A. D. White, and S. G. Rodriques, “LAB-Bench: Measuring capabilities of\nlanguage models for biology research,” 2024.\n[35] G. Starace, O. Jaffe, D. Sherburn, J. Aung, J. S. Chan, L. Maksin, R. Dias, E. Mays,\nB. Kinsella, W. Thompson, J. Heidecke, A. Glaese, and T. Patwardhan, “PaperBench:\nEvaluating ai’s ability to replicate ai research.” https://openai.com/index/paperbench/,\n2025.\n34\n"
    }
  ]
}