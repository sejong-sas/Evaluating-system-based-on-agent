{
  "repo": "EleutherAI/lm-evaluation-harness",
  "full_texts": [
    {
      "arxiv_id": "https://huggingface.co/docs/transformers/v4.15.0/en/parallelism",
      "full_text": " Model Parallelism Hugging Face Models Datasets Spaces Community Docs Enterprise Pricing Log In Sign Up Transformers documentation Model Parallelism Transformers üè° View all docs AWS Trainium &amp; Inferentia Accelerate Argilla AutoTrain Bitsandbytes Chat UI Dataset viewer Datasets Deploying on AWS Diffusers Distilabel Evaluate Gradio Hub Hub Python Library Huggingface.js Inference Endpoints (dedicated) Inference Providers LeRobot Leaderboards Lighteval Microsoft Azure Optimum PEFT Safetensors Sentence Transformers TRL Tasks Text Embeddings Inference Text Generation Inference Tokenizers Trackio Transformers Transformers.js smolagents timm Search documentation main v4.56.1 v4.55.4 v4.53.3 v4.52.3 v4.51.3 v4.50.0 v4.49.0 v4.48.2 v4.47.1 v4.46.3 v4.45.2 v4.44.2 v4.43.4 v4.42.4 v4.41.2 v4.40.2 v4.39.3 v4.38.2 v4.37.2 v4.36.1 v4.35.2 v4.34.1 v4.33.3 v4.32.1 v4.31.0 v4.30.0 v4.29.1 v4.28.1 v4.27.2 v4.26.1 v4.25.1 v4.24.0 v4.23.1 v4.22.2 v4.21.3 v4.20.1 v4.19.4 v4.18.0 v4.17.0 v4.16.2 v4.15.0 v4.14.1 v4.13.0 v4.12.5 v4.11.3 v4.10.1 v4.9.2 v4.8.2 v4.7.0 v4.6.0 v4.5.1 v4.4.2 v4.3.3 v4.2.2 v4.1.1 v4.0.1 v3.5.1 v3.4.0 v3.3.1 v3.2.0 v3.1.0 v3.0.2 v2.11.0 v2.10.0 v2.9.1 v2.8.0 v2.7.0 v2.6.0 v2.5.1 v2.4.1 v2.3.0 v2.2.2 v2.1.1 v2.0.0 v1.2.0 v1.1.0 v1.0.0 doc-builder-html EN Get started ü§ó Transformers Quick tour Installation Philosophy Glossary Using ü§ó Transformers Summary of the tasks Summary of the models Preprocessing data Fine-tuning a pretrained model Model sharing and uploading Summary of the tokenizers Multi-lingual models Advanced guides Examples Troubleshooting Fine-tuning with custom datasets ü§ó Transformers Notebooks Run training on Amazon SageMaker Community Converting Tensorflow Checkpoints Migrating from previous packages How to contribute to transformers? How to add a model to ü§ó Transformers? How to add a pipeline to ü§ó Transformers? Using tokenizers from ü§ó Tokenizers Performance and Scalability: How To Fit a Bigger Model and Train It Faster Model Parallelism Testing Debugging Exporting transformers models Checks on a Pull Request Research BERTology Perplexity of fixed-length models Benchmarks API Main Classes Callbacks Configuration Data Collator Keras callbacks Logging Models Optimization Model outputs Pipelines Processors Tokenizer Trainer DeepSpeed Integration Feature Extractor Models ALBERT Auto Classes BART BARThez BARTpho BEiT BERT Bertweet BertGeneration BertJapanese BigBird BigBirdPegasus Blenderbot Blenderbot Small BORT ByT5 CamemBERT CANINE CLIP ConvBERT CPM CTRL DeBERTa DeBERTa-v2 DeiT DETR DialoGPT DistilBERT DPR ELECTRA Encoder Decoder Models FlauBERT FNet FSMT Funnel Transformer herBERT I-BERT ImageGPT LayoutLM LayoutLMV2 LayoutXLM LED Longformer LUKE LXMERT MarianMT M2M100 MBart and MBart-50 MegatronBERT MegatronGPT2 MLUKE MobileBERT mLUKE MPNet MT5 OpenAI GPT OpenAI GPT2 GPT-J GPT Neo Hubert Perceiver Pegasus PhoBERT ProphetNet QDQBert RAG Reformer RemBERT RetriBERT RoBERTa RoFormer SegFormer SEW SEW-D Speech Encoder Decoder Models Speech2Text Speech2Text2 Splinter SqueezeBERT T5 T5v1.1 TAPAS Transformer XL TrOCR UniSpeech UniSpeech-SAT Vision Encoder Decoder Models Vision Text Dual Encoder Vision Transformer (ViT) VisualBERT Wav2Vec2 Wav2Vec2Phoneme WavLM XLM XLM-ProphetNet XLM-RoBERTa XLNet XLSR-Wav2Vec2 XLS-R Internal Helpers Custom Layers and Utilities Utilities for pipelines Utilities for Tokenizers Utilities for Trainer Utilities for Generation General Utilities You are viewing v4.15.0 version. A newer version v4.56.1 is available. Join the Hugging Face community and get access to the augmented documentation experience Collaborate on models, datasets and Spaces Faster examples with accelerated inference Switch between documentation themes Sign Up to get started Model Parallelism Parallelism overview In the modern machine learning the various approaches to parallelism are used to: fit very large models onto limited hardware - e.g. t5-11b is 45GB in just model params significantly speed up training - finish training that would take a year in hours We will first discuss in depth various 1D parallelism techniques and their pros and cons and then look at how they can be combined into 2D and 3D parallelism to enable an even faster training and to support even bigger models. Various other powerful alternative approaches will be presented. While the main concepts most likely will apply to any other framework, this article is focused on PyTorch-based implementations. Concepts The following is the brief description of the main concepts that will be described later in depth in this document. DataParallel (DP) - the same setup is replicated multiple times, and each being fed a slice of the data. The processing is done in parallel and all setups are synchronized at the end of each training step. TensorParallel (TP) - each tensor is split up into multiple chunks, so instead of having the whole tensor reside on a single gpu, each shard of the tensor resides on its designated gpu. During processing each shard gets processed separately and in parallel on different GPUs and the results are synced at the end of the step. This is what one may call horizontal parallelism, as the splitting happens on horizontal level. PipelineParallel (PP) - the model is split up vertically (layer-level) across multiple GPUs, so that only one or several layers of the model are places on a single gpu. Each gpu processes in parallel different stages of the pipeline and working on a small chunk of the batch. Zero Redundancy Optimizer (ZeRO) - Also performs sharding of the tensors somewhat similar to TP, except the whole tensor gets reconstructed in time for a forward or backward computation, therefore the model doesn‚Äôt need to be modified. It also supports various offloading techniques to compensate for limited GPU memory. Sharded DDP - is another name for the foundational ZeRO concept as used by various other implementations of ZeRO. Data Parallel Most users with just 2 GPUs already enjoy the increased training speed up thanks to DataParallel (DP) and DistributedDataParallel (DDP) that are almost trivial to use. This is a built-in feature of Pytorch. ZeRO Data Parallel ZeRO-powered data parallelism (ZeRO-DP) is described on the following diagram from this blog post It can be difficult to wrap one‚Äôs head around it, but in reality the concept is quite simple. This is just the usual DataParallel (DP), except, instead of replicating the full model params, gradients and optimizer states, each GPU stores only a slice of it. And then at run-time when the full layer params are needed just for the given layer, all GPUs synchronize to give each other parts that they miss - this is it. Consider this simple model with 3 layers, where each layer has 3 params: Copied La | Lb | Lc --- | ---- | --- a0 | b0 | c0 a1 | b1 | c1 a2 | b2 | c2 Layer La has weights a0, a1 and a2. If we have 3 GPUs, the Sharded DDP (= Zero-DP) splits the model onto 3 GPUs like so: Copied GPU0: La | Lb | Lc --- | ---- | --- a0 | b0 | c0 GPU1: La | Lb | Lc --- | ---- | --- a1 | b1 | c1 GPU2: La | Lb | Lc --- | ---- | --- a2 | b2 | c2 In a way this is the same horizontal slicing, as tensor parallelism, if you imagine the typical DNN diagram. Vertical slicing is where one puts whole layer-groups on different GPUs. But it‚Äôs just the starting point. Now each of these GPUs will get the usual mini-batch as it works in DP: Copied x0 = &gt; GPU0 x1 = &gt; GPU1 x2 = &gt; GPU2 The inputs are unmodified - they think they are going to be processed by the normal model. First, the inputs hit the layer La. Let‚Äôs focus just on GPU0: x0 needs a0, a1, a2 params to do its forward path, but GPU0 has only a0 - it gets sent a1 from GPU1 and a2 from GPU2, bringing all pieces of the model together. In parallel, GPU1 gets mini-batch x1 and it only has a1, but needs a0 and a2 params, so it gets those from GPU0 and GPU2. Same happens to GPU2 that gets input x2. It gets a0 and a1 from GPU0 and GPU1, and with its a2 it reconstructs the full tensor. All 3 GPUs get the full tensors reconstructed and a forward happens. As soon as the calculation is done, the data that is no longer needed gets dropped - it‚Äôs only used during the calculation. The reconstruction is done efficiently via a pre-fetch. And the whole process is repeated for layer Lb, then Lc forward-wise, and then backward Lc -&gt; Lb -&gt; La. To me this sounds like an efficient group backpacking weight distribution strategy: person A carries the tent person B carries the stove person C carries the axe Now each night they all share what they have with others and get from others what they don‚Äôt have, and in the morning they pack up their allocated type of gear and continue on their way. This is Sharded DDP / Zero DP. Compare this strategy to the simple one where each person has to carry their own tent, stove and axe, which would be far more inefficient. This is DataParallel (DP and DDP) in Pytorch. While reading the literature on this topic you may encounter the following synonyms: Sharded, Partitioned. If you pay close attention the way ZeRO partitions the model‚Äôs weights - it looks very similar to tensor parallelism which will be discussed later. This is because it partitions/shards each layer‚Äôs weights, unlike vertical model parallelism which is discussed next. Implementations: DeepSpeed ZeRO-DP stages 1+2+3 Fairscale ZeRO-DP stages 1+2+3 transformers integration Naive Model Parallel (Vertical) and Pipeline Parallel Naive Model Parallel (MP) is where one spreads groups of model layers across multiple GPUs. The mechanism is relatively simple - switch the desired layers .to() the desired devices and now whenever the data goes in and out those layers switch the data to the same device as the layer and leave the rest unmodified. We refer to it as Vertical MP, because if you remember how most models are drawn, we slice the layers vertically. For example, if the following diagram shows an 8-layer model: Copied =================== =================== | 0 | 1 | 2 | 3 | | 4 | 5 | 6 | 7 | =================== =================== gpu0 gpu1 we just sliced it in 2 vertically, placing layers 0-3 onto GPU0 and 4-7 to GPU1. Now while data travels from layer 0 to 1, 1 to 2 and 2 to 3 this is just the normal model. But when data needs to pass from layer 3 to layer 4 it needs to travel from GPU0 to GPU1 which introduces a communication overhead. If the participating GPUs are on the same compute node (e.g. same physical machine) this copying is pretty fast, but if the GPUs are located on different compute nodes (e.g. multiple machines) the communication overhead could be significantly larger. Then layers 4 to 5 to 6 to 7 are as a normal model would have and when the 7th layer completes we often need to send the data back to layer 0 where the labels are (or alternatively send the labels to the last layer). Now the loss can be computed and the optimizer can do its work. Problems: the main deficiency and why this one is called ‚Äúnaive‚Äù MP, is that all but one GPU is idle at any given moment. So if 4 GPUs are used, it‚Äôs almost identical to quadrupling the amount of memory of a single GPU, and ignoring the rest of the hardware. Plus there is the overhead of copying the data between devices. So 4x 6GB cards will be able to accommodate the same size as 1x 24GB card using naive MP, except the latter will complete the training faster, since it doesn‚Äôt have the data copying overhead. But, say, if you have 40GB cards and need to fit a 45GB model you can with 4x 40GB cards (but barely because of the gradient and optimizer states) shared embeddings may need to get copied back and forth between GPUs. Pipeline Parallel (PP) is almost identical to a naive MP, but it solves the GPU idling problem, by chunking the incoming batch into micro-batches and artificially creating a pipeline, which allows different GPUs to concurrently participate in the computation process. The following illustration from the GPipe paper shows the naive MP on the top, and PP on the bottom: It‚Äôs easy to see from the bottom diagram how PP has less dead zones, where GPUs are idle. The idle parts are referred to as the ‚Äúbubble‚Äù. Both parts of the diagram show a parallelism that is of degree 4. That is 4 GPUs are participating in the pipeline. So there is the forward path of 4 pipe stages F0, F1, F2 and F3 and then the return reverse order backward path of B3, B2, B1 and B0. PP introduces a new hyper-parameter to tune and it‚Äôs chunks which defines how many chunks of data are sent in a sequence through the same pipe stage. For example, in the bottomw diagram you can see that chunks=4 . GPU0 performs the same forward path on chunk 0, 1, 2 and 3 (F0,0, F0,1, F0,2, F0,3) and then it waits for other GPUs to do their work and only when their work is starting to be complete, GPU0 starts to work again doing the backward path for chunks 3, 2, 1 and 0 (B0,3, B0,2, B0,1, B0,0). Note that conceptually this is the same concept as gradient accumulation steps (GAS). Pytorch uses chunks , whereas DeepSpeed refers to the same hyper-parameter as GAS. Because of the chunks, PP introduces the concept of micro-batches (MBS). DP splits the global data batch size into mini-batches, so if you have a DP degree of 4, a global batch size of 1024 gets split up into 4 mini-batches of 256 each (1024/4). And if the number of chunks (or GAS) is 32 we end up with a micro-batch size of 8 (256/32). Each Pipeline stage works with a single micro-batch at a time. To calculate the global batch size of the DP + PP setup we then do: mbs*chunks*dp_degree ( 8*32*4=1024 ). Let‚Äôs go back to the diagram. With chunks=1 you end up with the naive MP, which is very inefficient. With a very large chunks value you end up with tiny micro-batch sizes which could be not every efficient either. So one has to experiment to find the value that leads to the highest efficient utilization of the gpus. While the diagram shows that there is a bubble of ‚Äúdead‚Äù time that can‚Äôt be parallelized because the last forward stage has to wait for backward to complete the pipeline, the purpose of finding the best value for chunks is to enable a high concurrent GPU utilization across all participating GPUs which translates to minimizing the size of the bubble. There are 2 groups of solutions - the traditional Pipeline API and the more modern solutions that make things much easier for the end user. Traditional Pipeline API solutions: PyTorch FairScale DeepSpeed Megatron-LM Modern solutions: Varuna Sagemaker Problems with traditional Pipeline API solutions: have to modify the model quite heavily, because Pipeline requires one to rewrite the normal flow of modules into a nn.Sequential sequence of the same, which may require changes to the design of the model. currently the Pipeline API is very restricted. If you had a bunch of python variables being passed in the very first stage of the Pipeline, you will have to find a way around it. Currently, the pipeline interface requires either a single Tensor or a tuple of Tensors as the only input and output. These tensors must have a batch size as the very first dimension, since pipeline is going to chunk the mini batch into micro-batches. Possible improvements are being discussed here https://github.com/pytorch/pytorch/pull/50693 conditional control flow at the level of pipe stages is not possible - e.g., Encoder-Decoder models like T5 require special workarounds to handle a conditional encoder stage. have to arrange each layer so that the output of one model becomes an input to the other model. We are yet to experiment with Varuna and SageMaker but their papers report that they have overcome the list of problems mentioned above and that they require much smaller changes to the user‚Äôs model. Implementations: Pytorch (initial support in pytorch-1.8, and progressively getting improved in 1.9 and more so in 1.10). Some examples FairScale DeepSpeed Megatron-LM has an internal implementation - no API. Varuna SageMaker - this is a proprietary solution that can only be used on AWS. ü§ó Transformers status: as of this writing none of the models supports full-PP. GPT2 and T5 models have naive PP support. The main obstacle is being unable to convert the models to nn.Sequential and have all the inputs to be Tensors. This is because currently the models include many features that make the conversion very complicated, and will need to be removed to accomplish that. Other approaches: DeepSpeed, Varuna and SageMaker use the concept of an Interleaved Pipeline Here the bubble (idle time) is further minimized by prioritizing backward passes. Varuna further tries to improve the schedule by using simulations to discover the most efficient scheduling. Tensor Parallelism In Tensor Parallelism each GPU processes only a slice of a tensor and only aggregates the full tensor for operations that require the whole thing. In this section we use concepts and diagrams from the Megatron-LM paper: Efficient Large-Scale Language Model Training on GPU Clusters . The main building block of any transformer is a fully connected nn.Linear followed by a nonlinear activation GeLU . Following the Megatron‚Äôs paper notation, we can write the dot-product part of it as Y = GeLU(XA) , where X and Y are the input and output vectors, and A is the weight matrix. If we look at the computation in matrix form, it‚Äôs easy to see how the matrix multiplication can be split between multiple GPUs: If we split the weight matrix A column-wise across N GPUs and perform matrix multiplications XA_1 through XA_n in parallel, then we will end up with N output vectors Y_1, Y_2, ..., Y_n which can be fed into GeLU independently: Using this principle, we can update an MLP of arbitrary depth, without the need for any synchronization between GPUs until the very end, where we need to reconstruct the output vector from shards. The Megatron-LM paper authors provide a helpful illustration for that: Parallelizing the multi-headed attention layers is even simpler, since they are already inherently parallel, due to having multiple independent heads! Special considerations: TP requires very fast network, and therefore it‚Äôs not advisable to do TP across more than one node. Practically, if a node has 4 GPUs, the highest TP degree is therefore 4. If you need a TP degree of 8, you need to use nodes that have at least 8 GPUs. This section is based on the original much more detailed TP overview . by @anton-l . SageMaker combines TP with DP for a more efficient processing. Alternative names: DeepSpeed calls it tensor slicing Implementations: Megatron-LM has an internal implementation, as it‚Äôs very model-specific parallelformers (only inference at the moment) SageMaker - this is a proprietary solution that can only be used on AWS. ü§ó Transformers status: core: not yet implemented in the core but if you want inference parallelformers provides this support for most of our models. So until this is implemented in the core you can use theirs. And hopefully training mode will be supported too. Deepspeed-Inference also supports our BERT, GPT-2, and GPT-Neo models in their super-fast CUDA-kernel-based inference mode, see more here DP+PP The following diagram from the DeepSpeed pipeline tutorial demonstrates how one combines DP with PP. Here it‚Äôs important to see how DP rank 0 doesn‚Äôt see GPU2 and DP rank 1 doesn‚Äôt see GPU3. To DP there is just GPUs 0 and 1 where it feeds data as if there were just 2 GPUs. GPU0 ‚Äúsecretly‚Äù offloads some of its load to GPU2 using PP. And GPU1 does the same by enlisting GPU3 to its aid. Since each dimension requires at least 2 GPUs, here you‚Äôd need at least 4 GPUs. Implementations: DeepSpeed Megatron-LM Varuna SageMaker ü§ó Transformers status: not yet implemented DP+PP+TP To get an even more efficient training a 3D parallelism is used where PP is combined with TP and DP. This can be seen in the following diagram. This diagram is from a blog post 3D parallelism: Scaling to trillion-parameter models , which is a good read as well. Since each dimension requires at least 2 GPUs, here you‚Äôd need at least 8 GPUs. Implementations: DeepSpeed - DeepSpeed also includes an even more efficient DP, which they call ZeRO-DP. Megatron-LM Varuna SageMaker ü§ó Transformers status: not yet implemented, since we have no PP and TP. DP+PP+TP+ZeRO One of the main features of DeepSpeed is ZeRO, which is a super-scalable extension of DP. It has already been discussed in ZeRO Data Parallel . Normally it‚Äôs a standalone feature that doesn‚Äôt require PP or TP. But it can be combined with PP and TP. When ZeRO-DP is combined with PP (and optionally TP) it typically enables only ZeRO stage 1 (optimizer sharding). While it‚Äôs theoretically possible to use ZeRO stage 2 (gradient sharding) with Pipeline Parallelism, it will have bad performance impacts. There would need to be an additional reduce-scatter collective for every micro-batch to aggregate the gradients before sharding, which adds a potentially significant communication overhead. By nature of Pipeline Parallelism, small micro-batches are used and instead the focus is on trying to balance arithmetic intensity (micro-batch size) with minimizing the Pipeline bubble (number of micro-batches). Therefore those communication costs are going to hurt. In addition, There are already fewer layers than normal due to PP and so the memory savings won‚Äôt be huge. PP already reduces gradient size by 1/PP , and so gradient sharding savings on top of that are less significant than pure DP. ZeRO stage 3 is not a good choice either for the same reason - more inter-node communications required. And since we have ZeRO, the other benefit is ZeRO-Offload. Since this is stage 1 optimizer states can be offloaded to CPU. Implementations: Megatron-DeepSpeed ü§ó Transformers status: not yet implemented, since we have no PP and TP. FlexFlow FlexFlow also solves the parallelization problem in a slightly different approach. Paper: ‚ÄúBeyond Data and Model Parallelism for Deep Neural Networks‚Äù by Zhihao Jia, Matei Zaharia, Alex Aiken It performs a sort of 4D Parallelism over Sample-Operator-Attribute-Parameter. Sample = Data Parallelism (sample-wise parallel) Operator = Parallelize a single operation into several sub-operations Attribute = Data Parallelism (length-wise parallel) Parameter = Model Parallelism (regardless of dimension - horizontal or vertical) Examples: Sample Let‚Äôs take 10 batches of sequence length 512. If we parallelize them by sample dimension into 2 devices, we get 10 x 512 which becomes be 5 x 2 x 512. Operator If we perform layer normalization, we compute std first and mean second, and then we can normalize data. Operator parallelism allows computing std and mean in parallel. So if we parallelize them by operator dimension into 2 devices (cuda:0, cuda:1), first we copy input data into both devices, and cuda:0 computes std, cuda:1 computes mean at the same time. Attribute We have 10 batches of 512 length. If we parallelize them by attribute dimension into 2 devices, 10 x 512 will be 10 x 2 x 256. Parameter It is similar with tensor model parallelism or naive layer-wise model parallelism. The significance of this framework is that it takes resources like (1) GPU/TPU/CPU vs. (2) RAM/DRAM vs. (3) fast-intra-connect/slow-inter-connect and it automatically optimizes all these algorithmically deciding which parallelisation to use where. One very important aspect is that FlexFlow is designed for optimizing DNN parallelizations for models with static and fixed workloads, since models with dynamic behavior may prefer different parallelization strategies across iterations. So the promise is very attractive - it runs a 30min simulation on the cluster of choice and it comes up with the best strategy to utilise this specific environment. If you add/remove/replace any parts it‚Äôll run and re-optimize the plan for that. And then you can train. A different setup will have its own custom optimization. ü§ó Transformers status: not yet integrated. We already have our models FX-trace-able via transformers.utils.fx , which is a prerequisite for FlexFlow, so someone needs to figure out what needs to be done to make FlexFlow work with our models. Which Strategy To Use When Here is a very rough outline at which parallelism strategy to use when. The first on each list is typically faster. ‚á® Single GPU Model fits onto a single GPU: Normal use Model doesn‚Äôt fit onto a single GPU: ZeRO + Offload CPU and optionally NVMe as above plus Memory Centric Tiling (see below for details) if the largest layer can‚Äôt fit into a single GPU Largest Layer not fitting into a single GPU: ZeRO - Enable Memory Centric Tiling (MCT). It allows you to run arbitrarily large layers by automatically splitting them and executing them sequentially. MCT reduces the number of parameters that are live on a GPU, but it does not affect the activation memory. As this need is very rare as of this writing a manual override of torch.nn.Linear needs to be done by the user. ‚á® Single Node / Multi-GPU Model fits onto a single GPU: DDP - Distributed DP ZeRO - may or may not be faster depending on the situation and configuration used Model doesn‚Äôt fit onto a single GPU: PP ZeRO TP With very fast intra-node connectivity of NVLINK or NVSwitch all three should be mostly on par, without these PP will be faster than TP or ZeRO. The degree of TP may also make a difference. Best to experiment to find the winner on your particular setup. TP is almost always used within a single node. That is TP size &lt;= gpus per node. Largest Layer not fitting into a single GPU: If not using ZeRO - must use TP, as PP alone won‚Äôt be able to fit. With ZeRO see the same entry for ‚ÄúSingle GPU‚Äù above ‚á® Multi-Node / Multi-GPU When you have fast inter-node connectivity: ZeRO - as it requires close to no modifications to the model PP+TP+DP - less communications, but requires massive changes to the model when you have slow inter-node connectivity and still low on GPU memory: DP+PP+TP+ZeRO-1 ‚Üê Performance and Scalability: How To Fit a Bigger Model and Train It Faster Testing ‚Üí Model Parallelism Parallelism overview Concepts Data Parallel ZeR O Data Parallel Naive Model Parallel ( Vertical) and Pipeline Parallel Tensor Parallelism D P+PP D P+P P+TP D P+P P+T P+ ZeRO Flex Flow Which Strategy To Use When "
    },
    {
      "arxiv_id": "https://github.com/NVIDIA/NeMo/releases",
      "full_text": " Releases ¬∑ NVIDIA-NeMo/NeMo ¬∑ GitHub Skip to content Navigation Menu Toggle navigation Sign in Appearance settings Platform GitHub Copilot Write better code with AI GitHub Spark New Build and deploy intelligent apps GitHub Models New Manage and compare prompts GitHub Advanced Security Find and fix vulnerabilities Actions Automate any workflow Codespaces Instant dev environments Issues Plan and track work Code Review Manage code changes Discussions Collaborate outside of code Code Search Find more, search less Explore Why GitHub Documentation GitHub Skills Blog Integrations GitHub Marketplace View all features Solutions By company size Enterprises Small and medium teams Startups Nonprofits By use case DevSecOps DevOps CI/CD View all use cases By industry Healthcare Financial services Manufacturing Government View all industries View all solutions Resources Topics AI DevOps Security Software Development View all Explore Learning Pathways Events &amp; Webinars Ebooks &amp; Whitepapers Customer Stories Partners Executive Insights Open Source GitHub Sponsors Fund open source developers The ReadME Project GitHub community articles Repositories Topics Trending Collections Enterprise Enterprise platform AI-powered developer platform Available add-ons GitHub Advanced Security Enterprise-grade security features Copilot for business Enterprise-grade AI features Premium Support Enterprise-grade 24/7 support Pricing Search or jump to... Search code, repositories, users, issues, pull requests... --> Search Clear Search syntax tips Provide feedback --> We read every piece of feedback, and take your input very seriously. Include my email address so I can be contacted Cancel Submit feedback Saved searches Use saved searches to filter your results more quickly --> Name Query To see all available qualifiers, see our documentation . Cancel Create saved search Sign in Sign up Appearance settings Resetting focus You signed in with another tab or window. Reload to refresh your session. You signed out in another tab or window. Reload to refresh your session. You switched accounts on another tab or window. Reload to refresh your session. Dismiss alert {{ message }} NVIDIA-NeMo / NeMo Public Notifications You must be signed in to change notification settings Fork 3.1k Star 15.7k Code Issues 84 Pull requests 63 Discussions Actions Projects 0 Security Uh oh! There was an error while loading. Please reload this page . Insights Additional navigation options Code Issues Pull requests Discussions Actions Projects Security Insights Releases: NVIDIA-NeMo/NeMo Releases Tags --> Releases ¬∑ NVIDIA-NeMo/NeMo NVIDIA Neural Modules 2.5.0rc0 03 Aug 16:50 chtruong814 v2.5.0rc0 620d2ba This commit was created on GitHub.com and signed with GitHub‚Äôs verified signature . GPG key ID: B5690EEEBB952194 Verified Learn about vigilant mode . Compare Choose a tag to compare Could not load tags Nothing to show {{ refName }} default Loading View all tags NVIDIA Neural Modules 2.5.0rc0 Pre-release Pre-release Prerelease: NVIDIA Neural Modules 2.5.0rc0 (2025-08-03) Assets 2 Loading Uh oh! There was an error while loading. Please reload this page . --> All reactions NVIDIA Neural Modules 2.4.0 25 Jul 18:12 chtruong814 v2.4.0 2381f42 This commit was created on GitHub.com and signed with GitHub‚Äôs verified signature . GPG key ID: B5690EEEBB952194 Verified Learn about vigilant mode . Compare Choose a tag to compare Could not load tags Nothing to show {{ refName }} default Loading View all tags NVIDIA Neural Modules 2.4.0 Latest Latest Highlights Collections: Speech Batched beam search for transducers (RNN-T and TDT) RNNT/TDT buffered/streaming inference + batched decoding support in cache-aware add support for CTC batched beam search with GPU-LM Key fixes Punctuation Marks in Timestamps Fix timestamps when cuda graphs enabled Fix masking of &lt;pad&gt; tokens in AED inference TDT streaming inference fix LLM Qwen 3 235B-A22B Perf Optimized DeepSeek V3 Perf Optimized Gemma3 support from Google Embedding and Reranker models MM Llama 4 AVLM Training performance (speed) NVL sharp + IB sharp for DP/FSDP-communications on H100 and B200 MXFP8 with TP communication overlap MXFP8 with reduced memory allocation FP8 sub-channel recipe (128x128 for weight and 1x128 for activation) cudnn fused attention for MLA (both Hopper and Blackwell) Advanced custom asymmetric pipelining (for MTP, loss func, and embd) BF16 optimizer for model memory saving CUDA graph fix for fine-tuning benchmarks CUDA graph support for LLAMA4 Detailed Changelogs ASR Changelog ci: Fix ASR container by @ko3n1g :: PR: #13288 Set L2_Segmentation_Tool_Parallel_ctc_segmentation test to be optional by @chtruong814 :: PR: #13296 Revert \"WebDataset URL refactoring\" by @ko3n1g :: PR: #13421 Update flagged docs links by @erastorgueva-nv :: PR: #13391 [Docs] Fix incorrectly formatted reference tags by @erastorgueva-nv :: PR: #13445 Update CP by @pablo-garay :: PR: #13532 Tdt buffered inference fix by @hainan-xv :: PR: #13500 Fix transcribe when nbest hypotheses are returned by @lilithgrigoryan :: PR: #13540 Set ASR test to be optional by @chtruong814 :: PR: #13633 Enabling chunked inference for AED models in asr_evaluator by @melllinia :: PR: #13674 Ko3n1g/chore/asr only by @ko3n1g :: PR: #13704 decompressing joblib file before checking it by @Ssofja :: PR: #13732 Revert \"decompressing joblib file before checking it ( #13732 )\" by @chtruong814 :: PR: #13791 Punctuation Marks in Timestamps by @monica-sekoyan :: PR: #13353 AIStore with Webdataset by @monica-sekoyan :: PR: #13604 Update to add default for dataclass variables by @nithinraok :: PR: #13814 This PR addresses to known security issues by @Ssofja :: PR: #13804 remove model_stride var by @nithinraok :: PR: #13867 add CTC batched beam search by @lilithgrigoryan :: PR: #13337 Clean up streaming ASR script and tests by @artbataev :: PR: #13894 add NGPU-LM fusion during CTC greedy by @lilithgrigoryan :: PR: #13917 TTS Changelog Revert \"WebDataset URL refactoring\" by @ko3n1g :: PR: #13421 Update flagged docs links by @erastorgueva-nv :: PR: #13391 [Docs] Fix incorrectly formatted reference tags by @erastorgueva-nv :: PR: #13445 Update CP by @pablo-garay :: PR: #13532 fix: vpp stage refactoring to match mcore by @ZhiyuLi-Nvidia :: PR: #13673 AIStore with Webdataset by @monica-sekoyan :: PR: #13604 NLP / NMT Changelog Migrate Hyena to Megatron inference_context. by @cspades :: PR: #13436 Update CP by @pablo-garay :: PR: #13532 fix broken links by @dimapihtar :: PR: #13544 Add nlp import checks by @thomasdhc :: PR: #13563 PTQ model support, quant_cfg, and documentation updates by @janekl :: PR: #13519 feat - GPTSFTChatDataset alignment with OpenAI Messages, compatibility with packed sequences by @soluwalana :: PR: #13367 fix: vpp stage refactoring to match mcore by @ZhiyuLi-Nvidia :: PR: #13673 Fix resume with MegatronPretrainingBatchSampler by @ashors1 :: PR: #13565 Punctuation Marks in Timestamps by @monica-sekoyan :: PR: #13353 Revert Adding more doc-strings to megatron_parallel.py #12767 by @ko3n1g :: PR: #13824 reasoning model evaluation mmlu gpqa by @ruchaa-apte :: PR: #13880 Remove unused DynamicRetrievalServer and Bert dataset loader classes by @dimapihtar :: PR: #14209 Huvu/avlm qafix cherrypick from by @huvunvidia :: PR: #14253 Export Changelog Improve Nemo2Exporter for Models Using Custom Modelling Files on HF by @suiyoubi :: PR: #13400 Adding more export tests by @oyilmaz-nvidia :: PR: #13410 Add Warning to Export when output_path exists by @suiyoubi :: PR: #13465 Move libsox-fmt-all from Dockerfile.ci.export_deploy to Dockerfile.ci by @chtruong814 :: PR: #13452 ci: Remove trt-llm breakpoint by @ko3n1g :: PR: #13499 Add Qwen2VL export_ckpt by @AtsunoriFujita :: PR: #13398 Add MLlama export_ckpt by @AtsunoriFujita :: PR: #13346 Update vLLMExporter to use vLLM V1 by @janekl :: PR: #13498 Add vLLM Mixtral and TRT-LLM qnemo export tests (plus a couple of bugfixes) by @janekl :: PR: #13697 Fix Qwen3 export + misc by @cuichenx :: PR: #13679 Extra int cast for successful tracing during ONNX export by @janekl :: PR: #13782 FP8 lora export by @cuichenx :: PR: #13748 Add PEFT export check by @cuichenx :: PR: #13835 Update llm api import_ckpt/export_ckpt docstring by @meatybobby :: PR: #13714 Use modelopt export and disable dataset calibration for weight only PTQ by @jenchen13 :: PR: #13756 Bugfixes Changelog [automodel] move liger kernel patching by @akoumpa :: PR: #13579 Uncategorized Changelog build: various bumps by @ko3n1g :: PR: #13285 ci: Fixes to selective triggering by @ko3n1g :: PR: #13287 ci: Set timeout by @ko3n1g :: PR: #13294 Set L2_NeMo_2_T5_Pretraining test as optional by @chtruong814 :: PR: #13282 Add test environment approval step for CI by @chtruong814 :: PR: #13297 update num nodes in deepseek v3 finetune recipe by @cuichenx :: PR: #13314 ci: Increase cache pool by @ko3n1g :: PR: #13306 Rename adam_with_cosine_annealing as adam since cosin LR is not setup by @ShriyaRishab :: PR: #13315 ci: Update test queue bot to not assume a workflow is launched from a PR by @chtruong814 :: PR: #13318 Fix TE pytorch attention doc link by @thomasdhc :: PR: #13327 ci: Add all recent buildcaches to update-buildcache job by @ko3n1g :: PR: #13289 Fix neva notebook by @yaoyu-33 :: PR: #13334 Fix transformer offline for CI/CD llama4 tests by @yaoyu-33 :: PR: #13339 [automodel] convert lm head to full tensor before passing to lce by @yuanzhedong :: PR: #13319 ci: No dups in queue by @ko3n1g :: PR: #13352 ci(hotfix): VLM CPU unit tests by @ko3n1g :: PR: #13348 vLLM==0.8.5 update by @janekl :: PR: #13350 ci: Allow bypassing approval by @ko3n1g :: PR: #13365 Avoid the need to specify optional attributes for lhotse/nemo reader functions by @pzelasko :: PR: #13307 ci: Fix selective-triggering for non-PR events by @ko3n1g :: PR: #13374 ci: Revert no-concurrency-group-on-main by @ko3n1g :: PR: #13375 ci: Improve no-fail-fast mechanism by @ko3n1g :: PR: #13370 2d buckets estimation fix by @monica-sekoyan :: PR: #13377 ci: Fix scheduled runs by @ko3n1g :: PR: #13378 Ko3n1g/ci/fix nightly runs by @ko3n1g :: PR: #13382 [automodel] fix none issue in dataset for qwen model by @yuanzhedong :: PR: #13311 update table by @akoumpa :: PR: #13397 Improve test coverage for audio modules by @anteju :: PR: #13333 Disable failing maxine loss test by @anteju :: PR: #13361 Ko3n1g/ci/no notification on cancel by @ko3n1g :: PR: #13403 document fp8_recipe by @akoumpa :: PR: #13405 Weekly bump main by @ko3n1g :: PR: #13408 Handle boolean args for performance scripts and log received config by @guyueh1 :: PR: #13291 [automodel] add FirstRankPerNode by @akoumpa :: PR: #13373 tests: Disable flaky audio test by @ko3n1g :: PR: #13429 ci: Disable flaky audio test by @ko3n1g :: PR: #13435 Fix loss compute and reduction by @xrennvidia :: PR: #13295 ci: Skip link check on github links by @chtruong814 :: PR: #13425 Add NCCL cfg interface to perf scripts by @erhoo82 :: PR: #13407 ci: Success only if Run CICD label attached by @ko3n1g :: PR: #13430 ci: Add tests to selective triggering by @ko3n1g :: PR: #13404 ci: Remove jq by @ko3n1g :: PR: #13440 ci: Fix deps tree for tests by @ko3n1g :: PR: #13443 Ko3n1g/ci/fix dependency tree by @ko3n1g :: PR: #13448 Adding additional unit tests for the deploy module by @pthombre :: PR: #13411 [Audio] fix a flaky test (and also make some tests run faster) by @racoiaws :: PR: #13439 [automodel] ignore tail padding in TPS calculation by @akoumpa :: PR: #13329 Ko3n1g/ci/selective triggering 3 by @ko3n1g :: PR: #13460 ci: Disable broken neva tests by @ko3n1g :: PR: #13461 fix speechlm data module by @stevehuang52 :: PR: #13362 ci: Enter queue only with passing linting by @ko3n1g :: PR: #13462 Adding tests for Schroedinger Bridge model by @nasretdinovr :: PR: #13401 add more detailed description by @dimapihtar :: PR: #13464 [Audio] tests for score-based and flow matching enhancement models by @racoiaws :: PR: #13406 Use expandable cuda memory segmentation by @erhoo82 :: PR: #13418 Fix llava tokenizer caused nan issue by @yaoyu-33 :: PR: #13466 Remove cuda method from ModelPT by @erastorgueva-nv :: PR: #13394 Fix BNR 2 unit test + input, case where input length was not specified by @nitin9252 :: PR: #13467 ci: Do not run any tests if no match is found by @ko3n1g :: PR: #13479 Ko3n1g/ci/selective triggering 4 by @ko3n1g :: PR: #13489 Fix typo in the performance script by @youngeunkwon0405 :: PR: #13487 ci: No runs on main by @ko3n1g :: PR: #13490 ci: Upload on schedule by @ko3n1g :: PR: #13491 ci: Run selective triggering on dockerfiles and dependencies by @ko3n1g :: PR: #13493 [automodel] fallback FP8 + LCE -&gt; FP8 + CE by @akoumpa :: PR: #13349 Update changelog for r2.3.0 by @github-actions[bot] :: PR: #13501 Update 2.3.0... Read more Contributors soluwalana, pramodk, and 67 other contributors Assets 2 Loading Uh oh! There was an error while loading. Please reload this page . --> üëç 3 artbataev, Sancho869, and Setioris reacted with thumbs up emoji üéâ 2 odvieira and bhargav191098 reacted with hooray emoji üëÄ 2 13inccc and Surprisemotherhacker reacted with eyes emoji All reactions üëç 3 reactions üéâ 2 reactions üëÄ 2 reactions 7 people reacted NVIDIA Neural Modules 2.3.2 08 Jul 22:29 ko3n1g v2.3.2 f98ef1d This commit was created on GitHub.com and signed with GitHub‚Äôs verified signature . GPG key ID: B5690EEEBB952194 Verified Learn about vigilant mode . Compare Choose a tag to compare Could not load tags Nothing to show {{ refName }} default Loading View all tags NVIDIA Neural Modules 2.3.2 This release addresses known security issues. For the latest NVIDIA Vulnerability Disclosure Information visit https://www.nvidia.com/en-us/security/ , for acknowledgement please reach out to the NVIDIA PSIRT team at PSIRT@nvidia.com Assets 2 Loading Uh oh! There was an error while loading. Please reload this page . --> All reactions NVIDIA Neural Modules 2.4.0rc2 09 Jul 00:37 ko3n1g v2.4.0rc2 7ac5a8e This commit was created on GitHub.com and signed with GitHub‚Äôs verified signature . GPG key ID: B5690EEEBB952194 Verified Learn about vigilant mode . Compare Choose a tag to compare Could not load tags Nothing to show {{ refName }} default Loading View all tags NVIDIA Neural Modules 2.4.0rc2 Pre-release Pre-release Prerelease: NVIDIA Neural Modules 2.4.0rc2 (2025-07-09) Assets 2 Loading Uh oh! There was an error while loading. Please reload this page . --> All reactions NVIDIA Neural Modules 2.4.0rc1 02 Jul 20:49 ko3n1g v2.4.0rc1 4c6fb0c This commit was created on GitHub.com and signed with GitHub‚Äôs verified signature . GPG key ID: B5690EEEBB952194 Verified Learn about vigilant mode . Compare Choose a tag to compare Could not load tags Nothing to show {{ refName }} default Loading View all tags NVIDIA Neural Modules 2.4.0rc1 Pre-release Pre-release Prerelease: NVIDIA Neural Modules 2.4.0rc1 (2025-07-02) Assets 2 Loading Uh oh! There was an error while loading. Please reload this page . --> All reactions NVIDIA Neural Modules 2.4.0rc0 27 Jun 17:40 ko3n1g v2.4.0rc0 87b5b2a This commit was created on GitHub.com and signed with GitHub‚Äôs verified signature . GPG key ID: B5690EEEBB952194 Verified Learn about vigilant mode . Compare Choose a tag to compare Could not load tags Nothing to show {{ refName }} default Loading View all tags NVIDIA Neural Modules 2.4.0rc0 Pre-release Pre-release Prerelease: NVIDIA Neural Modules 2.4.0rc0 (2025-06-27) Assets 2 Loading Uh oh! There was an error while loading. Please reload this page . --> All reactions NVIDIA Neural Modules 2.3.1 25 May 22:04 ko3n1g v2.3.1 abddc85 This commit was created on GitHub.com and signed with GitHub‚Äôs verified signature . GPG key ID: B5690EEEBB952194 Verified Learn about vigilant mode . Compare Choose a tag to compare Could not load tags Nothing to show {{ refName }} default Loading View all tags NVIDIA Neural Modules 2.3.1 Highlights Collections LLM Llama 4: Fixed an accuracy issue caused by MoE probability normalization. Improved pre-train and fine-tune performance. Export &amp; Deploy Updated vLLMExporter to use vLLM V1 to address a security vulnerability. AutoModel Improved chat-template handling. Fault Tolerance Local checkpointing: Fixed support for auto-inserted metric names for resuming from local checkpoints. Detailed Changelogs: Export Changelog Cherry-pick Update vLLMExporter to use vLLM V1 ( #13498 ) into r2.3.0 by @chtruong814 :: PR: #13631 Uncategorized: Changelog Bump to 2.3.1 by @chtruong814 :: PR: #13507 Cherry pick Use explicitly cached canary-1b-flash in CI tests (13237) into r2.3.0 by @ko3n1g :: PR: #13508 Cherry pick [automodel] bump liger-kernel to 0.5.8 + fallback (13260) into r2.3.0 by @ko3n1g :: PR: #13308 Cherry-pick Add recipe and ci scripts for qwen2vl to r2.3.0 by @romanbrickie :: PR: #13336 Cherry pick Fix skipme handling (13244) into r2.3.0 by @ko3n1g :: PR: #13376 Cherry pick Allow fp8 param gather when using FSDP (13267) into r2.3.0 by @ko3n1g :: PR: #13383 Cherry pick Handle boolean args for performance scripts and log received config (13291) into r2.3.0 by @ko3n1g :: PR: #13416 Cherry pick new perf configs (13110) into r2.3.0 by @ko3n1g :: PR: #13431 Cherry pick Adding additional unit tests for the deploy module (13411) into r2.3.0 by @ko3n1g :: PR: #13449 Cherry pick Adding more export tests (13410) into r2.3.0 by @ko3n1g :: PR: #13450 Cherry pick [automodel] add FirstRankPerNode (13373) into r2.3.0 by @ko3n1g :: PR: #13559 Cherry pick [automodel] deprecate global_batch_size dataset argument (13137) into r2.3.0 by @ko3n1g :: PR: #13560 Cherry-pick [automodel] fallback FP8 + LCE -&gt; FP8 + CE ( #13349 ) into r2.3.0 by @chtruong814 :: PR: #13561 Cherry pick [automodel] add find_unused_parameters=True for DDP (13366) into r2.3.0 by @ko3n1g :: PR: #13601 Cherry pick Add CI test for local checkpointing (#13012) into r2.3.0 by @ananthsub :: PR: #13472 Cherry pick [automodel] fix --mbs/gbs dtype and chat-template (13598) into r2.3.0 by @akoumpa :: PR: #13613 Cherry-pick Update t5.py ( #13082 ) to r2.3.0 and bump mcore to f98b1a0 by @chtruong814 :: PR: #13642 [Automodel] Fix CP device_mesh issue, use PTL distsampler ( #13473 ) by @akoumpa :: PR: #13636 [Llama4] Fix the recipe bug - cherrypick #13649 by @gdengk :: PR: #13650 build: Pin transformers ( #13675 ) by @ko3n1g :: PR: #13692 Contributors ananthsub, romanbrickie, and 4 other contributors Assets 2 Loading Uh oh! There was an error while loading. Please reload this page . --> üëç 7 RUI-LONG, nakheel77, connor33341, dhkim0225, shubhmatsagar395, AudranBert, and Sudais-DB reacted with thumbs up emoji All reactions üëç 7 reactions 7 people reacted NVIDIA Neural Modules 2.3.0 08 May 23:42 ko3n1g v2.3.0 2b03b74 This commit was created on GitHub.com and signed with GitHub‚Äôs verified signature . GPG key ID: B5690EEEBB952194 Verified Learn about vigilant mode . Compare Choose a tag to compare Could not load tags Nothing to show {{ refName }} default Loading View all tags NVIDIA Neural Modules 2.3.0 Highlights Export &amp; Deploy NeMo 2.0 export path for NIM ONNX and TensorRT Export for NIM Embedding Container In-framework deployment for HF Models TRT-LLM deployment for HF Models in NeMo Framework Evaluation Integrate nvidia-lm-eval to NeMo FW for evaluations with OpenAI API compatible in-framework deployment AutoModel VLM AutoModelForImageForTextToText FP8 for AutoModel Support CP with FSDP2 Support TP with FSDP2 Performance Optimization add support for cut cross entropy &amp; liger kernel Gradient Checkpointing Fault Tolerance Integrate NVRx v0.3 Local checkpointing Collections LLM Llama4 Llama Nemotron Ultra Llama Nemotron Super Llama Nemotron Nano Nemotron-h/5 DeepSeek V3 Pretraining Evo2 Qwen 2.5 LoRA for Qwen3-32B and Qwen3-30B-A3B MultiModal FLUX Gemma 3 Qwen2-VL ASR NeMo Run support for ASR training N-Gram LM on GPU for AED N-Gram LM on GPU + Transducer greedy decoding (RNN-T, TDT) Timestamps support for AED timestamp supported models Migrate SpeechLM to NeMo 2.0 Canary-1.1 Replace ClassificationModels class with LabelModels Performance Functional MXFP8 support for (G)B200 Current scaling recipe with TP communication overlap and FP8 param gathers Custom FSDP support that fully utilizes GB200 NVL72 Detailed Changelogs: ASR Changelog Added model config params for Canary-1B-Flash, Canary-180M-Flash models by @KunalDhawan :: PR: #12588 Canary tutorial by @ankitapasad :: PR: #12613 Canary tutorial fix timestamp by @ankitapasad :: PR: #12677 revert config by @nithinraok :: PR: #12689 canary longform inference script with timestamps option by @krishnacpuvvada :: PR: #12653 Fix default timestamps value for Hybrid ASR models by @artbataev :: PR: #12681 Fix k2 installation with PyTorch 2.6.0 by @artbataev :: PR: #12686 Improve time and RTFx report for ASR by @artbataev :: PR: #12680 Modify train args by @ankitapasad :: PR: #12700 Fix asr doc warnings by @nithinraok :: PR: #12720 Rename FastNGramLM -&gt; NGramGPULanguageModel by @artbataev :: PR: #12755 transcribe fix for new hypotheses by @nune-tadevosyan :: PR: #12801 Fix timestamps when cuda graphs enabled by @monica-sekoyan :: PR: #12808 update streaming conformer by @stevehuang52 :: PR: #12846 AED Decoding with N-Gram LM by @artbataev :: PR: #12730 update notebook by @nithinraok :: PR: #13088 bugfix ASR_Context_Biasing.ipynb by @lilithgrigoryan :: PR: #13109 Change branch for installation from main to r2.3.0 by @ankitapasad :: PR: #13266 TTS Changelog Add Magpie-TTS and Updates NeMo Audio Codecs by @blisc :: PR: #12606 fix bug from prior commit ( #13264 ) by @blisc :: PR: #13328 NLP / NMT Changelog Remove old peft docs by @cuichenx :: PR: #12675 Add code coverage for llm gpt models conversion tests by @suiyoubi :: PR: #12665 Make BERT TransformerBlockWithPostLNSupport accept more inputs from Mcore by @suiyoubi :: PR: #12685 remove gifs from documentation by @dimapihtar :: PR: #12732 Rename FastNGramLM -&gt; NGramGPULanguageModel by @artbataev :: PR: #12755 fix NeMo documentation by @dimapihtar :: PR: #12754 GPT Model/Data/Recipe Unit Test by @suiyoubi :: PR: #12757 ci: Exclude nlp, mm, vision collections by @ko3n1g :: PR: #12816 Add vocab size as attr to GPT and T5 Configs, use file name based logger in llm.gpt.data by @hemildesai :: PR: #12862 Fix transformer layer api with megatron cbc89b3 by @yaoyu-33 :: PR: #12885 Text Normalization / Inverse Text Normalization Changelog Rename FastNGramLM -&gt; NGramGPULanguageModel by @artbataev :: PR: #12755 Export Changelog GHA Conversion Test and Importer/Exporter Refactor by @suiyoubi :: PR: #12597 Fix Llama Embedding Model Exporting keys by @suiyoubi :: PR: #12691 build: Add trtllm by @ko3n1g :: PR: #12672 Fix trt-llm install by @chtruong814 :: PR: #12827 Update LLaVA's next HF exporter to load ViT checkpoint from YAML by @eagle705 :: PR: #12841 Support huggingface export to tensorrtllm by @pthombre :: PR: #12889 Adds a built stage for the trt-llm wheel to reduce the overall test image size by @chtruong814 :: PR: #12883 Uncategorized: Changelog Update changelog-build.yml by @ko3n1g :: PR: #12584 Update changelog for r2.2.0 by @github-actions[bot] :: PR: #12585 Add comments for requirements by @thomasdhc :: PR: #12603 [automodel] FSDP2Strategy: move to device if using a single-device by @akoumpa :: PR: #12593 build: Remove numba pin by @ko3n1g :: PR: #12604 docs: Update installation guides by @ko3n1g :: PR: #12596 Change Llama Scaling Factor type to Float by @suiyoubi :: PR: #12616 ci: Test multiple python versions by @ko3n1g :: PR: #12619 ci: Disable reformat by @ko3n1g :: PR: #12620 Updating ModelOpt to 0.25.0 by @janekl :: PR: #12633 [automodel] add additional hf_dataset tests by @akoumpa :: PR: #12646 [automodel] add jit_transform tests by @akoumpa :: PR: #12645 [automodel] init eos_token_id inside data module by @yuanzhedong :: PR: #12610 [automodel] grad ckpt by @akoumpa :: PR: #12644 bugfix(llm/LLaMa) - dropout_position can never be equal to extended string by @soluwalana :: PR: #12649 Fix inference pipeline quality issue by @Victor49152 :: PR: #12639 [automodel] switch to direct=True to propage return codes in nemorun by @akoumpa :: PR: #12651 add Auto Conf support for bert, t5, qwen, starcoder models by @dimapihtar :: PR: #12601 ci: Upload coverage by @ko3n1g :: PR: #12668 ci: Re-enable changed-files action by @ko3n1g :: PR: #12683 build: Pin sox by @ko3n1g :: PR: #12701 add neva quantization by @linnanwang :: PR: #12698 Clip coverage by @abhinavg4 :: PR: #12696 GHA CI test: Remove unnecessary directive by @pablo-garay :: PR: #12714 minor perf fixes by @malay-nagda :: PR: #12656 Add DeepSeek V2 Lite into llm init .py by @suiyoubi :: PR: #12664 Add Llama-Nemotron Nano and 70B models by @suiyoubi :: PR: #12712 Save batch norm running stats in PEFT checkpoints by @cuichenx :: PR: #12666 Fix document Readme under nemo to add more information by @yaoyu-33 :: PR: #12699 Fix ub_overlap_ag by @cuichenx :: PR: #12721 Toggle fast tokenizer if error occurs by @cuichenx :: PR: #12722 Update README.md for blackwell and AutoModel by @snowmanwwg :: PR: #12612 Raise error on import_ckpt with overwrite=False plus README for checkpoint_converters by @janekl :: PR: #12693 [automodel] fix validation_step by @soluwalana :: PR: #12659 [automodel] vlm tests by @akoumpa :: PR: #12716 Auto Configurator code coverage by @dimapihtar :: PR: #12694 [automodel] fix automodle benchmark script by @yuanzhedong :: PR: #12605 Remove unnecessary directives by @pablo-garay :: PR: #12743 Add recipe tests for coverage by @cuichenx :: PR: #12737 Add Qwen2.5 in NeMo2 by @suiyoubi :: PR: #12731 add fallback_module to safe_import_from by @akoumpa :: PR: #12726 Update quantization scripts &amp; relax modelopt requirement specifier by @janekl :: PR: #12709 Import guard fasttext by @thomasdhc :: PR: #12758 [automodel] chunked cross entropy by @akoumpa :: PR: #12752 Add fsdp automodel test by @BoxiangW :: PR: #12718 [automodel] if peft move only adapters to cpu by @akoumpa :: PR: #12735 [automodel] update hf mockdataset by @akoumpa :: PR: #12643 [automodel] remove unused cell in multinode notebook by @yuanzhedong :: PR: #12624 Yash/llava next coverage by @yashaswikarnati :: PR: #12745 Tidy code: remove unneeded statements/lines by @pablo-garay :: PR: #12771 Pass tensor instead of raw number in _mock_loss_function in PTQ by @janekl :: PR: #12769 ci: Run on nightly schedule by @ko3n1g :: PR: #12775 Add logs for checkpoint saving start and finalization by @lepan-google :: PR: #12697 Alit/test coverage by @JRD971000 :: PR: #12762 Fix loss mask with packed sequence by @ashors1 :: PR: #12642 Add pruning recipe by @kevalmorabia97 :: PR: #12602 Update qwen2-v1 to use NeMo quick_gelu by @thomasdhc :: PR: #12787 [doc] Fixes for audio doc warnings by @anteju :: PR: #12736 ci: Measure multiprocessing by @ko3n1g :: PR: #12778 ci: Fix flaky LLM tests by @ko3n1g :: PR: #12807 Add BERT/Qwen2.5 Unit test and Refactor all GHA Conversion Tests by @suiyoubi :: PR: #12785 Fix TransformerBlock cuda_graphs compatibility with MCore by @buptzyb :: PR: #12779 ci: Remove --branch by @ko3n1g :: PR: #12809 ci: Move scripts fully down to files by @ko3n1g :: PR: #12802 add init .py to make this a package by @akoumpa :: PR: #12814 Update changelog for r2.2.1 by @github-actions[bot] :: PR: #12818 add finetune support for Auto Configurator by @dimapihtar :: PR: #12770 [automodel] add cpu:gloo to backend by @akoumpa :: PR: #12832 add missing call to _apply_liger_kernel_to_instance by @akoumpa :: PR: #12806 Prune docker images in GHA older than 8hrs by @chtruong814 :: PR: #12838 [audio] Adding tests for predictive models by @anteju :: PR: #12823 Update resiliency example notebook readme and add links to the brev launchable by @ShriyaRishab :: PR: #12843 [automodel] qlora peft by @yzhang123 :: PR: #12817 ci: Increase prune time by @ko3n1g :: PR: #12860 Update base container in Dockerfile.speech by @artbataev :: PR: #12859 Fix qwen2.5 1.5b configuration inheritance bug by @Aprilistic :: PR: #12852 Update modelopt upperbound to 0.27 by @thomasdhc :: PR: #12788 Non-bloc... Read more Contributors jstjohn, soluwalana, and 45 other contributors Assets 3 Loading Uh oh! There was an error while loading. Please reload this page . --> üëç 1 nakheel77 reacted with thumbs up emoji All reactions üëç 1 reaction 1 person reacted NVIDIA Neural Modules 2.3.0rc4 21 Apr 23:24 ko3n1g v2.3.0rc4 b9abb0a This commit was created on GitHub.com and signed with GitHub‚Äôs verified signature . GPG key ID: B5690EEEBB952194 Verified Learn about vigilant mode . Compare Choose a tag to compare Could not load tags Nothing to show {{ refName }} default Loading View all tags NVIDIA Neural Modules 2.3.0rc4 Pre-release Pre-release Prerelease: NVIDIA Neural Modules 2.3.0rc4 (2025-04-21) Assets 2 Loading Uh oh! There was an error while loading. Please reload this page . --> All reactions NVIDIA Neural Modules 2.3.0rc3 15 Apr 18:22 ko3n1g v2.3.0rc3 3d04c86 This commit was created on GitHub.com and signed with GitHub‚Äôs verified signature . GPG key ID: B5690EEEBB952194 Verified Learn about vigilant mode . Compare Choose a tag to compare Could not load tags Nothing to show {{ refName }} default Loading View all tags NVIDIA Neural Modules 2.3.0rc3 Pre-release Pre-release Prerelease: NVIDIA Neural Modules 2.3.0rc3 (2025-04-15) Assets 2 Loading Uh oh! There was an error while loading. Please reload this page . --> All reactions Previous 1 2 3 4 5 6 7 8 Next Previous Next Footer &copy; 2025 GitHub,&nbsp;Inc. Footer navigation Terms Privacy Security Status Docs Contact Manage cookies Do not share my personal information You can‚Äôt perform that action at this time. "
    },
    {
      "arxiv_id": "https://docs.anthropic.com/claude/docs/models-overview",
      "full_text": " Home - Anthropic Anthropic home page English Search... Research Login Support Discord Sign up Sign up Search... Navigation Welcome Developer Platform Claude Code Model Context Protocol (MCP) API Reference Resources Release Notes Build with Learn how to get started with the Anthropic API, the Console, and Claude Code. Ask Claude about docs‚Ä¶ Developer Platform Get started Make your first API call in minutes. Features overview Explore the advanced features and capabilities now available in Claude. API reference Integrate and scale using our API and SDKs. Anthropic Console Craft and test powerful prompts directly in your browser. Release notes Learn about changes and new features in Claude and the API. Upgrade to Claude 4 Upgrade to the latest model to access new tools and features available in Claude 4. Claude Code Claude Code quickstart Get started with Claude Code. Claude Code reference Consult the Claude Code reference documentation for details on feature implementation and configuration. Claude Code changelog Learn about changes and new features in Claude Code. Learning resources Anthropic Courses Explore Anthropic‚Äôs educational courses and projects. Anthropic Cookbook See replicable code samples and implementations. Anthropic Quickstarts Deployable applications built with our API. "
    },
    {
      "arxiv_id": "https://textsynth.com/documentation.html#engines",
      "full_text": " API TextSynth Home Playground Documentation Pricing Technology Login Sign Up API Terms of Service Privacy policy Contents 1 API description 1.1 Authentication 1.2 Engines 1.3 Text completions 1.3.1 BNF Grammar Syntax 1.3.2 JSON Schema Syntax 1.4 Chat 1.5 Translations 1.6 Log probabilities 1.7 Tokenization 1.8 Text to Image 1.9 Speech to Text Transcription 1.10 Text to Speech 1.11 Embeddings 2.0 Credits 2 Prompt tuning 3 Model results 4 Changelog 1 API description 1.1 Authentication In order to use the REST API, you must create an account and get your API key. Each request shall have the following header applied: Authorization: Bearer YOUR_API_KEY 1.2 Engines Most endpoints require an engine_id to operate. The following engines are currently available: mistral_7B : Mistral 7B is a 7 billion parameter language model with a 8K token context length outperforming Llama2 13B on many tests. llama3_8B : Llama3 8B is a 8 billion parameter language model with a 8K token context length trained on 15T tokens. There are specific use restrictions associated with this model. llama3.1_8B_instruct : Llama3.1 8B Instruct is a 8 billion parameter chat model. The context length is currently limited to 8K tokens. There are specific use restrictions associated with this model. gemma3_27B_it : Gemma 3 27B instruct is a 27 billion parameter language model with a 128K token context length. There are specific use restrictions associated with this model. llama3.3_70B_instruct : Llama3.3 70B instruct is a 70 billion parameter chat model. The context length is currently limited to 8K tokens. There are specific use restrictions associated with this model. gptj_6B : GPT-J is a 6 billion parameter language model with a 2K token context length trained on the Pile (825 GB of text data) published by EleutherAI . madlad400_7B : MADLAD400 7B is a 7 billion parameter language model specialized for translation. It supports multilingual translation between about 400 languages. See the translate endpoint. stable_diffusion : Stable Diffusion is a 1 billion parameter text to image model trained to generate 512x512 pixel images from English text ( sd-v1-4.ckpt checkpoint). See the text_to_image endpoint. There are specific use restrictions associated with this model. whisper_large_v3 : Whisper Large v3 is a 1.5 billion parameter model for speech to text transcription in 100 languages. See the transcript endpoint. parler_tts_large : Parler-TTS v1 is a 2.2 billion parameter model for Text to Speech in English. See the speech endpoint. bge_large_en_v1.5 : BGE-Large-EN-v1.5 is an embedding model suitable for RAG. See the embeddings endpoint. 1.3 Text completions The API syntax for text completions is: POST https://api.textsynth.com/v1/engines/{engine_id}/completions where engine_id is the selected engine . Request body (JSON) prompt : string or array of string. The input text(s) to complete. max_tokens : optional int (default = 100) Maximum number of tokens to generate. A token represents about 4 characters for English texts. The total number of tokens (prompt + generated text) cannot exceed the model's maximum context length. See the model list to know their maximum context length. If the prompt length is larger than the model's maximum context length, the beginning of the prompt is discarded. stream : optional boolean (default = false) If true, the output is streamed so that it is possible to display the result before the complete output is generated. Several JSON answers are output. Each answer is followed by two line feed characters. stop : optional string or array of string (default = null) Stop the generation when the string(s) are encountered. The generated text does not contain the string. The length of the array is at most 5. n : optional integer (range: 1 to 16, default = 1) Generate n completions from a single prompt. temperature : optional number (default = 1) Sampling temperature. A higher temperature means the model will select less common tokens leading to a larger diversity but potentially less relevant output. It is usually better to tune top_p or top_k . top_k : optional integer (range: 1 to 1000, default = 40) Select the next output token among the top_k most likely ones. A higher top_k gives more diversity but a potentially less relevant output. top_p : optional number (range: 0 to 1, default = 0.9) Select the next output token among the most probable ones so that their cumulative probability is larger than top_p . A higher top_p gives more diversity but a potentially less relevant output. top_p and top_k are combined, meaning that at most top_k tokens are selected. A value of 1 disables this sampling. seed : optional integer (default = 0). Random number seed. A non zero seed always yields the same completions. It is useful to get deterministic results and try different sets of parameters. More advanced sampling parameters are available: logit_bias : optional object (default = {}) Modify the likelihood of the specified tokens in the completion. The specified object is a map between the token indexes and the corresponding logit bias. A negative bias reduces the likelihood of the corresponding token. The bias must be between -100 and 100. Note that the token indexes are specific to the selected model. You can use the tokenize API endpoint to retrieve the token indexes of a given model. Example: if you want to ban the \" unicorn\" token for GPT-J, you can use: logit_bias: { \"44986\": -100 } presence_penalty : optional number (range: -2 to 2, default = 0) A positive value penalizes tokens which already appeared in the generated text. Hence it forces the model to have a more diverse output. frequency_penalty : optional number (range: -2 to 2, default = 0) A positive value penalizes tokens which already appeared in the generated text proportionaly to their frequency. Hence it forces the model to have a more diverse output. repetition_penalty : optional number (default = 1) Divide by repetition_penalty the logits corresponding to tokens which already appeared in the generated text. A value of 1 effectively disables it. See this article for more details. typical_p : optional number (range: 0 to 1, default = 1) Alternative to top_p sampling: instead of selecting the tokens starting from the most probable one, start from the ones whose log likelihood is the closest to the symbol entropy. As with top_p , at most top_k tokens are selected. A value of 1 disables this sampling. See this article for more details. grammar : optional string Specify a grammar that the completion should match. More information about the grammar syntax is available in section 1.3.1 . schema : optional object Specify a JSON schema that the completion should match. Only a subset of the JSON schema specification is supported as defined in section 1.3.2 . grammar and schema cannot be both present. Answer (JSON) text : string or array of string It is the completed text. If the n parameter is larger than 1 or if an array of string was provided as prompt , an array of strings is returned. reached_end : boolean If true, indicate that it is the last answer. It is only useful in case of streaming output ( stream = true in the request). truncated_prompt : bool (default = false) If true, indicate that the prompt was truncated because it was too large compared to the model's maximum context length. Only the end of the prompt is used to generate the completion. finish_reason : string or array or string Indicate the reason why the generation was finished. An array of string is returned if text is an array. Possible values: \"stop\" (end-of-sequence token reached), \"length\" (the maximum specified length was reached), \"grammar\" (no suitable token satisfies the specified grammar or stack overflow when evaluating the grammar). input_tokens : integer Indicate the number of input tokens. It is useful to estimate the number of compute resources used by the request. output_tokens : integer Indicate the total number of generated tokens. It is useful to estimate the number of compute resources used by the request. In case of streaming output, several answers may be output. Each answer is always followed by two line feed characters. Example Request: curl https://api.textsynth.com/v1/engines/gptj_6B/completions \\ -H \"Content-Type: application/json\" \\ -H \"Authorization: Bearer YOUR_API_KEY\" \\ -d '{\"prompt\": \"Once upon a time, there was\", \"max_tokens\": 20 }' Answer: { \"text\": \" a woman who loved to get her hands on a good book. She loved to read and to tell\", \"reached_end\": true, \"input_tokens\": 7, \"output_tokens\": 20 } Python example: completion.py 1.3.1 BNF Grammar Syntax A Bakus-Naur Form (BNF) grammar can be used to constrain the generated output. The grammar definition consists in production rules defining how non non-terminals can be replaced by other non-terminals or terminals (characters). The special root non-terminal represents the whole output. Here is an example of grammar matching the JSON syntax: # BNF grammar to parse JSON objects root ::= ws object value ::= object | array | string | number | (&quot;true&quot; | &quot;false&quot; | &quot;null&quot;) object ::= &quot;{&quot; ws ( string &quot;:&quot; ws value ws (&quot;,&quot; ws string &quot;:&quot; ws value ws )* )? &quot;}&quot; array ::= &quot;[&quot; ws ( value ws (&quot;,&quot; ws value ws )* )? &quot;]&quot; string ::= &quot;\\&quot;&quot; ( [^&quot;\\\\] | &quot;\\\\&quot; ([&quot;\\\\/bfnrt] | &quot;u&quot; [0-9a-fA-F] [0-9a-fA-F] [0-9a-fA-F] [0-9a-fA-F]) # escapes )* &quot;\\&quot;&quot; number ::= (&quot;-&quot;? ([0-9] | [1-9] [0-9]*)) (&quot;.&quot; [0-9]+)? ([eE] [-+]? [0-9]+)? # whitespace ws ::= ([ \\t\\n] ws)? A production rule has the syntax: value ::= object | array | &quot;null&quot; where value is the non-terminal name. A newline terminates the rule definition. Alternatives are indicated with | between sequence of terms. Newlines are interpreted as whitespace in parenthesis or after | . A term is either: A non-terminal identifier. A double-quoted unicode string. Unicode characters can be specified in hexadecimal with \\xNN , \\uNNNN or \\UNNNNNNNN . Parenthesis (...) to embed alternatives. A unicode character list ( [...] ) or excluded character list ( [^...] ) like in regular expressions. A term can be followed by regular expression-like quantifiers: * to repeat the term 0 or more times + to repeat the term 1 or more times ? to repeat the term 0 or 1 time. Comments are introduced with the # character. Grammar restriction: Left recursion is forbidden i.e.: expr ::= [0-9]+ | expr \"+\" expr Fortunately it is always possible to transform left recursion into right recursion by adding more non-terminals: expr ::= number | number \"+\" expr number ::= [0-9]+ 1.3.2 JSON Schema Syntax A JSON schema can be used to constrain the generated output. It is recommended to also include it in your prompt so that the language model knows the JSON format which is expected in its reply. Here is an example of supported JSON schema: { \"type\": \"object\", \"properties\": { \"id\": { \"type\": \"string\" }, \"name\": { \"type\": \"string\" }, \"age\": { \"type\": \"integer\", \"minimum\": 16, \"maximum\": 150, }, \"phone_numbers\": { \"type\": \"array\", \"items\": { \"type\": \"object\", \"properties\": { \"number\": { \"type\": \"string\", }, \"type\": { \"type\": \"string\", \"enum\": [\"mobile\", \"home\"], }, }, \"required\": [\"number\", \"type\"] /* at least one property must be required */ }, \"minItems\": 1, /* only 0 or 1 are supported, default = 0 */ }, \"hobbies\": { \"type\": \"array\", \"items\": { \"type\": \"string\" } } }, \"required\": [\"id\", \"name\", \"age\"] } The following types are supported: object . The required parameter must be present with at least one property in it. array . The minimum number of elements may be constrained with the optional minItems parameter. Only the values 0 or 1 are supported. string . The optional enum parameter indicates the allowed values. integer . The optional minimum and maximum parameters may be present to restrict the range. The maximum range is -2147483648 to 2147483647. number : floating point numbers. boolean : true or false values. null : the null value. 1.4 Chat This endpoint provides completions for chat applications. The prompt is automatically formatted according to the model preferred chat prompt template. The API syntax is: POST https://api.textsynth.com/v1/engines/{engine_id}/chat where engine_id is the selected engine . The API is identical to the completions endpoint except that the prompt property is removed and replaced by: messages : array of strings. The conversation history. At least one element must be present. If the number of elements is odd, the model generates the response of the assistant. Otherwise, it completes it. system : optional string. Override the default model system prompt which gives general advices to the model. Example Request: curl https://api.textsynth.com/v1/engines/falcon_40B-chat/chat \\ -H \"Content-Type: application/json\" \\ -H \"Authorization: Bearer YOUR_API_KEY\" \\ -d '{\"messages\": [\"What is the translation of hello in French ?\"]}' Answer: { \"text\": \" \\\"Bonjour\\\" is the correct translation for \\\"hello\\\" in French. It is commonly used as a greeting in both formal and informal settings. \\\"Bonjour\\\" can be used when addressing a single person, a group of people, or even when answering the phone.\", \"reached_end\": true, \"input_tokens\": 45, \"output_tokens\": 56 } 1.5 Translations This endpoint translates one or several texts to a target language. The source language can be automatically detected or explicitely provided. The API syntax to translate is: POST https://api.textsynth.com/v1/engines/{engine_id}/translate where engine_id is the selected engine . Request body (JSON) text : array of strings. Each string is an independent text to translate. Batches of at most 64 texts can be provided. source_lang : string. Two or three character ISO language code for the source language. The special value \"auto\" indicates to auto-detect the source language. The language auto-detection does not support all languages and is based on heuristics. Hence if you know the source language you should explicitly indicate it. The madlad400_7B model supports the following languages: Code Language Code Language Code Language Code Language ace Achinese ada Adangme adh Adhola ady Adyghe af Afrikaans agr Aguaruna msm Agusan Manobo ahk Akha sq Albanian alz Alur abt Ambulas am Amharic grc Ancient Greek ar Arabic hy Armenian frp Arpitan as Assamese av Avar kwi Awa-Cuaiquer awa Awadhi quy Ayacucho Quechua ay Aymara az Azerbaijani ban Balinese bm Bambara bci Baoul√© bas Basa (Cameroon) ba Bashkir eu Basque akb Batak Angkola btx Batak Karo bts Batak Simalungun bbc Batak Toba be Belarusian bzj Belize Kriol English bn Bengali bew Betawi bho Bhojpuri bim Bimoba bi Bislama brx Bodo (India) bqc Boko (Benin) bus Bokobaru bs Bosnian br Breton ape Bukiyip bg Bulgarian bum Bulu my Burmese bua Buryat qvc Cajamarca Quechua jvn Caribbean Javanese rmc Carpathian Romani ca Catalan qxr Ca√±ar H. Quichua ceb Cebuano bik Central Bikol maz Central Mazahua ch Chamorro cbk Chavacano ce Chechen chr Cherokee hne Chhattisgarhi ny Chichewa zh Chinese (Simplified) ctu Chol cce Chopi cac Chuj chk Chuukese cv Chuvash kw Cornish co Corsican crh Crimean Tatar hr Croatian cs Czech mps Dadibi da Danish dwr Dawro dv Dhivehi din Dinka tbz Ditammari dov Dombe nl Dutch dyu Dyula dz Dzongkha bgp E. Baluchi gui E. Bolivian Guaran√≠ bru E. Bru nhe E. Huasteca Nahuatl djk E. Maroon Creole taj E. Tamang enq Enga en English sja Epena myv Erzya eo Esperanto et Estonian ee Ewe cfm Falam Chin fo Faroese hif Fiji Hindi fj Fijian fil Filipino fi Finnish fip Fipa fon Fon fr French ff Fulah gag Gagauz gl Galician gbm Garhwali cab Garifuna ka Georgian de German gom Goan Konkani gof Gofa gor Gorontalo el Greek guh Guahibo gub Guajaj√°ra gn Guarani amu Guerrero Amuzgo ngu Guerrero Nahuatl gu Gujarati gvl Gulay ht Haitian Creole cnh Hakha Chin ha Hausa haw Hawaiian he Hebrew hil Hiligaynon mrj Hill Mari hi Hindi ho Hiri Motu hmn Hmong qub Huallaga Hu√°nuco Quechua hus Huastec hui Huli hu Hungarian iba Iban ibb Ibibio is Icelandic ig Igbo ilo Ilocano qvi Imbabura H. Quichua id Indonesian inb Inga iu Inuktitut ga Irish iso Isoko it Italian ium Iu Mien izz Izii jam Jamaican Creole English ja Japanese jv Javanese kbd Kabardian kbp Kabiy√® kac Kachin dtp Kadazan Dusun kl Kalaallisut xal Kalmyk kn Kannada cak Kaqchikel kaa Kara-Kalpak kaa_Latn Kara-Kalpak (Latn) krc Karachay-Balkar ks Kashmiri kk Kazakh meo Kedah Malay kek Kekch√≠ ify Keley-I Kallahan kjh Khakas kha Khasi km Khmer kjg Khmu kmb Kimbundu rw Kinyarwanda ktu Kituba (DRC) tlh Klingon trp Kok Borok kv Komi koi Komi-Permyak kg Kongo ko Korean kos Kosraean kri Krio ksd Kuanua kj Kuanyama kum Kumyk mkn Kupang Malay ku Kurdish (Kurmanji) ckb Kurdish (Sorani) ky Kyrghyz quc K‚Äôiche‚Äô lhu Lahu quf Lambayeque Quechua laj Lango (Uganda) lo Lao ltg Latgalian la Latin lv Latvian ln Lingala lt Lithuanian lu Luba-Katanga lg Luganda lb Luxembourgish ffm Maasina Fulfulde mk Macedonian mad Madurese mag Magahi mai Maithili mak Makasar mgh Makhuwa-Meetto mg Malagasy ms Malay ml Malayalam mt Maltese mam Mam mqy Manggarai gv Manx mi Maori arn Mapudungun mrw Maranao mr Marathi mh Marshallese mas Masai msb Masbatenyo mbt Matigsalug Manobo chm Meadow Mari mni Meiteilon (Manipuri) min Minangkabau lus Mizo mdf Moksha mn Mongolian mfe Morisien meu Motu tuc Mutu miq M√≠skito emp N. Ember√° lrc N. Luri qvz N. Pastaza Quichua se N. Sami nnb Nande niq Nandi nv Navajo ne Nepali new Newari nij Ngaju gym Ng√§bere nia Nias nog Nogai no Norwegian nut Nung (Viet Nam) nyu Nyungwe nzi Nzima ann Obolo oc Occitan or Odia (Oriya) oj Ojibwa ang Old English om Oromo os Ossetian pck Paite Chin pau Palauan pag Pangasinan pa Panjabi pap Papiamento ps Pashto fa Persian pis Pijin pon Pohnpeian pl Polish jac Popti‚Äô pt Portuguese qu Quechua otq Quer√©taro Otomi raj Rajasthani rki Rakhine rwo Rawa rom Romani ro Romanian rm Romansh rn Rundi ru Russian rcf R√©union Creole French alt S. Altai quh S. Bolivian Quechua qup S. Pastaza Quechua msi Sabah Malay hvn Sabu sm Samoan cuk San Blas Kuna sxn Sangir sg Sango sa Sanskrit skr Saraiki srm Saramaccan stq Saterfriesisch gd Scottish Gaelic seh Sena nso Sepedi sr Serbian crs Seselwa Creole French st Sesotho shn Shan shp Shipibo-Conibo sn Shona jiv Shuar smt Simte sd Sindhi si Sinhala sk Slovak sl Slovenian so Somali nr South Ndebele es Spanish srn Sranan Tongo acf St Lucian Creole French su Sundanese suz Sunwar spp Supyire Senoufo sus Susu sw Swahili ss Swati sv Swedish gsw Swiss German syr Syriac ksw S‚Äôgaw Karen tab Tabassaran tg Tajik tks Takestani ber Tamazight (Tfng) ta Tamil tdx Tandroy-Mahafaly Malagasy tt Tatar tsg Tausug te Telugu twu Termanu teo Teso tll Tetela tet Tetum th Thai bo Tibetan tca Ticuna ti Tigrinya tiv Tiv toj Tojolabal to Tonga (Tonga Islands) sda Toraja-Sa‚Äôdan ts Tsonga tsc Tswa tn Tswana tcy Tulu tr Turkish tk Turkmen tvl Tuvalu tyv Tuvinian ak Twi tzh Tzeltal tzo Tzotzil tzj Tz‚Äôutujil tyz T√†y udm Udmurt uk Ukrainian ppk Uma ubu Umbu-Ungu ur Urdu ug Uyghur uz Uzbek ve Venda vec Venetian vi Vietnamese knj W. Kanjobal wa Walloon war Waray (Philippines) guc Wayuu cy Welsh fy Western Frisian wal Wolaytta wo Wolof noa Woun Meu xh Xhosa sah Yakut yap Yapese yi Yiddish yo Yoruba yua Yucateco zne Zande zap Zapotec dje Zarma zza Zaza zu Zulu target_lang : string. Two or three character ISO language code for the target language. num_beams : integer (range: 1 to 5, default = 4). Number of beams used to generate the translated text. The translation is usually better with a larger number of beams. Each beam requires generating a separate translated text, hence the number of generated tokens is multiplied by the number of beams . split_sentences : optional boolean (default = true). The translation model only translates one sentence at a time. Hence the input must be split into sentences. When split_sentences = true (default), each input text is automatically split into sentences using source language specific heuristics. If you are sure that each input text contains only one sentence, it is better to disable the automatic sentence splitting. Answer (JSON) translations : array of objects. Each object has the following properties: text : string Translated text detected_source_lang : string ISO language code corresponding to the detected lang (identical to source_lang if language auto-detection is not enabled) input_tokens : integer Indicate the total number of input tokens. It is useful to estimate the number of compute resources used by the request. output_tokens : integer Indicate the total number of generated tokens. It is useful to estimate the number of compute resources used by the request. Example Request: curl https://api.textsynth.com/v1/engines/m2m100_1_2B/translate \\ -H \"Content-Type: application/json\" \\ -H \"Authorization: Bearer YOUR_API_KEY\" \\ -d '{\"text\": [\"The quick brown fox jumps over the lazy dog.\"], \"source_lang\": \"en\", \"target_lang\": \"fr\" }' Answer: { \"translations\": [{\"detected_source_lang\":\"en\",\"text\":\"Le renard brun rapide saute sur le chien paresseux.\"}], \"input_tokens\": 18, \"output_tokens\": 85 } Python example: translate.py 1.6 Log probabilities This endpoint returns the logarithm of the probability that a continuation is generated after a context . It can be used to answer questions when only a few answers (such as yes/no) are possible. It can also be used to benchmark the models. The API syntax to get the log probabilities is: POST https://api.textsynth.com/v1/engines/{engine_id}/logprob where engine_id is the selected engine . Request body (JSON) context : string or array of string. If empty string, the context is set to the End-Of-Text token. continuation : string or array of string. Must be a non empty string. If an array is provided, it must have the same number of elements as context . Answer (JSON) logprob : double or array of double Logarithm of the probability of generation of continuation preceeded by context . It corresponds to the sum of the logarithms of the probabilities of the tokens of continuation . It is always context was an array. num_tokens : integer or array of integer Number of tokens in continuation . An array is returned if context was an array. is_greedy : boolean or array of boolean true if continuation would be generated by greedy sampling from continuation . An array is returned if context was an array. input_tokens : integer Indicate the total number of input tokens. It is useful to estimate the number of compute resources used by the request. Example Request: curl https://api.textsynth.com/v1/engines/gptj_6B/logprob \\ -H \"Content-Type: application/json\" \\ -H \"Authorization: Bearer YOUR_API_KEY\" \\ -d '{\"context\": \"The quick brown fox jumps over the lazy\", \"continuation\": \" dog\"}' Answer: { \"logprob\": -0.0494835916522837, \"is_greedy\": true, \"input_tokens\": 9 } 1.7 Tokenization This endpoint returns the token indexes corresponding to a given text. It is useful for example to know the exact number of tokens of a text or to specify logit biases with the completion endpoint. The tokens are specific to a given model. The API syntax to tokenize a text is: POST https://api.textsynth.com/v1/engines/{engine_id}/tokenize where engine_id is the selected engine . Request body (JSON) text : string. Input text. token_content_type : optional string (default = &quot;none&quot;). If set to &quot;base64&quot;, also output the content of each token encoded as a base64 string. Note: tokens do not necessarily contain full UTF-8 characters so it is not always possible to represent their content as an UTF-8 string. Answer (JSON) tokens : array of integers. Token indexes corresponding to the input text. token_content : array of strings. Base64 strings corresponding to the content of each token if token_content_type was set to &quot;base64&quot;. Example Request: curl https://api.textsynth.com/v1/engines/gptj_6B/tokenize \\ -H \"Content-Type: application/json\" \\ -H \"Authorization: Bearer YOUR_API_KEY\" \\ -d '{\"text\": \"The quick brown fox jumps over the lazy dog\"}' Answer: {\"tokens\":[464,2068,7586,21831,18045,625,262,16931,3290]} Note: the tokenize endpoint is free. 1.8 Text to Image This endpoint generates one or several images from a text prompt. The API syntax is: POST https://api.textsynth.com/v1/engines/{engine_id}/text_to_image where engine_id is the selected engine . Currently only stable_diffusion is supported. Request body (JSON) prompt : string. The text prompt. Only the first 75 tokens are used. image_count : optional integer (default = 1). Number of images to generate. At most 4 images can be generated with one request. The generation of an image takes about 2 seconds. width : optional integer (default = 512). height : optional integer (default = 512). Width and height in pixels of the generated images. The only accepted values are 384, 512, 640 and 768. The product width by height must be timesteps : optional integer (default = 50). Number of diffusion steps. Larger values usually give a better result but the image generation takes longer. guidance_scale : optional number (default = 7.5). Guidance Scale. A larger value gives a larger importance to the text prompt with respect to a random image generation. seed : optional integer (default = 0). Random number seed. A non zero seed always yields the same images. It is useful to get deterministic results and try different sets of parameters. negative_prompt : optional string (default = \"\"). Negative text prompt. It is useful to exclude specific items from the generated image. Only the first 75 tokens are used. image : optional string (default = none). Optional base 64 encoded JPEG image serving as seed for the generated image. It must have the same width and height as the generated image. strength : optional number (default = 0.5, range 0 to 1). When using an image as seed (see the image parameter), specifies the ponderation between the noise and the image seed. The value 0 is equivalent to not using the image seed. Answer (JSON) images : array of objects. Each object has the following property: data : string Base64 encoded generated JPEG image. Example Request: curl https://api.textsynth.com/v1/engines/stable_diffusion/text_to_image \\ -H \"Content-Type: application/json\" \\ -H \"Authorization: Bearer YOUR_API_KEY\" \\ -d '{\"prompt\": \"an astronaut riding a horse\" }' Answer: { \"images\": [{\"data\":\"...\"}], } Python example: sd.py 1.9 Speech to Text Transcription This endpoint does speech to text transcription. The input consists in an audio file and optional parameters. The JSON output contains the text transcription with timestamps. The API syntax is: POST https://api.textsynth.com/v1/engines/{engine_id}/transcript where engine_id is the selected engine . Currently only whisper_large_v3 is supported. Request body The content type of the posted data should be multipart/form-data . It should contain at least one file of name file with the audio file to transcript. The supported file formats are: mp3, m4a, mp4, wav and opus. The maximum file size is 50 MBytes. The maximum supported duration is 2 hours. Additional parameters may be provided either as form data or inside an additional file of name json containing JSON data. The following additional parameters are supported: language : optional string (default = &quot;auto&quot;). The special value auto indicates that the language is automatically detected on the first 30 seconds of audio. Otherwise it is an ISO language code. The following languages are available: af, am, ar, as, az, ba, be, bg, bn, bo, br, bs, ca, cs, cy, da, de, el, en, es, et, eu, fa, fi, fo, fr, gl, gu, ha, haw, he, hi, hr, ht, hu, hy, id, is, it, ja, jw, ka, kk, km, kn, ko, la, lb, ln, lo, lt, lv, mg, mi, mk, ml, mn, mr, ms, mt, my, ne, nl, nn, no, oc, pa, pl, ps, pt, ro, ru, sa, sd, si, sk, sl, sn, so, sq, sr, su, sv, sw, ta, te, tg, th, tk, tl, tr, tt, uk, ur, uz, vi, yi, yo, yue, zh. Answer (JSON) A JSON object is returned containing the transcription. It contains the following properties: text : string. Transcripted text. segments : array of objects. transcripted text segments with timestamps. Each segment has the following properties: id : integer. Segment ID. start : float. Start time in seconds. end : float. End time in seconds. text : string. Transcripted text for this segment. language : string. ISO language code. duration : float. Transcription duration in seconds Example Request: curl https://api.textsynth.com/v1/engines/whisper_large_v3/transcript \\ -H \"Authorization: Bearer YOUR_API_KEY\" \\ -F language=en -F file=@input.mp3 Where input.mp3 is the audio file to transcript. Answer: { \"text\": \"...\", \"segments\": [...], ... } Python example: transcript.py 1.10 Text to Speech This endpoint does text to speech output. The output is a MP3 stream containing the generated speech. The API syntax is: POST https://api.textsynth.com/v1/engines/{engine_id}/speech where engine_id is the selected engine . Currently only parler_tts_large is supported. Only the English language is supported. Request body (JSON) input : string. The input text. It must contain less than 4096 unicode characters. voicet : string. Select the voice name. The following voices are available: Will Eric Laura Alisa Patrick Rose Jerry Jordan Lauren Jenna Karen Rick Bill James Yann Emily Anna Jon Brenda Barbara. seed : optional integer (default = 0). Random number seed. A non zero seed yields the same output for a given input text. It is useful to get deterministic results. Answer (Binary file) An MP3 file containing the generated speech is returned. Example Request: curl https://api.textsynth.com/v1/engines/parler_tts_large/speech \\ -H \"Authorization: Bearer YOUR_API_KEY\" \\ -d '{\"input\": \"Hello world.\", \"voice\": \"Will\" }' Python example: speech.py 1.11 Embeddings This endpoint computes the embeddings of a text. The API syntax is: POST https://api.textsynth.com/v1/engines/{engine_id}/embeddings where engine_id is the selected engine . Request body (JSON) input : string or array of strings. Several input texts can be provided. Answer (JSON) object : string. value = \"list\" . data : array of object. Each object has the following properties: object : string. value = \"embedding\" . index : integer. Index in the array. embedding : array of floats. The embedding vector computed for the corresponding input text. Example Request: curl https://api.textsynth.com/v1/engines/bge_large_en_v1.5/embeddings \\ -H \"Authorization: Bearer YOUR_API_KEY\" \\ -d '{\"input\": \"The quick brown fox jumps over the lazy dog\" }' 2.0 Credits This endpoint returns the remaining credits on your account. Answer (JSON) credits : integer Number of remaining credits multiplied by 1e9. Example Request: curl https://api.textsynth.com/v1/credits \\ -H \"Authorization: Bearer YOUR_API_KEY\" Answer: {\"credits\":123456789} 2 Prompt tuning In addition to pure text completion, you can tune your prompt (input text) so that the model solves a precise task such as: sentiment analysis classification entity extraction question answering grammar and spelling correction machine translation chatbot summarization Some examples can be found here (nlpcloud.io blog) or here (Open AI documentation) . For text to image, see the Stable Diffusion Prompt Book . 3 Model results We present in this section the objective results of the various models on tasks from the Language Model Evaluation Harness . These results were computed using the TextSynth API so that they can be fully reproduced (patch: lm_evaluation_harness_textsynth.tar.gz ). Zero-shot performance: Model LAMBADA (acc) Hellaswag (acc_norm) Winogrande (acc) PIQA (acc) COQA (f1) Average ‚Üë llama3_8B 75.2% 78.2% 73.5% 78.8% 80.4% 77.2% mistral_7B 74.9% 80.1% 73.9% 80.7% 80.3% 78.0% Five-shot performance: Model MMLU (exact match) llama3.3_70B_instruct 81.9% gemma3_27B_it 77.0% llama3.1_8B_instruct 67.1% Note that these models have been trained with data which contains possible test set contamination. So not all these results might reflect the actual model performance. 4 Changelog 2025-06-25: the gemma3_27B_it chat model was added. The mixtral_47B_instruct model was removed and is redirected to llama3.1_8B_instruct . 2024-12-27: added the bge_large_en_v1.5 embedding model. Added real time speech to text and voice chat pages in the playground. 2024-12-17: added the parler_tts_large Text to Speech model. 2024-12-09: the llama3.3_70B_instruct and llama3.1_8B_instruct models were added. The llama3_8B_instruct model was removed and is redirected to llama3.1_8B_instruct . The llama2_70B model was removed and is redirected to llama3.3_70B_instruct . 2024-09-13: batched queries are supported for the completions and logprob endpoints. Automatic language detection is supported in the transcript endpoint. Transcription parameters can now be provided as form data without an additional JSON file. 2024-06-05: the llama3_8B and llama3_8B_instruct models were added. The mistral_7B_instruct model was removed and is redirected to llama3_8B_instruct . 2024-01-03: added the transcript endpoint with the whisper_large_v3 model. 2023-12-28: the mixtral_47B_instruct and llama2_70B models were added. The m2m100_1_2B model was removed and is redirected to madlad400_7B . The flan_t5_xxl and falcon_7B models were removed and are redirected to the mistral_7B model. The falcon_40B model was removed and is redirected to llama2_70B . The falcon_40B-chat model was removed and is redirected to mixtral_47B_instruct . 2023-11-22: added the madlad400_7B translation model. 2023-10-16: upgraded the mistral_7B models to 8K content length. Added the token_content_type parameter to the tokenize endpoint. 2023-10-02: added BNF grammar and JSON schema constrained completion. Added the finish_reason property. 2023-09-28: added the negative_prompt , image and strength parameters to the text_to_image endpoint. Added the seed parameter to the completions endpoint. Added the mistral_7B and mistral_7B_instruct models. The boris_6B and gptneox_20B models were removed because newer models give better overall performance. 2023-07-25: added the chat endpoint. 2023-07-20: added the falcon_7B , falcon_40B and llama2_7B models. The fairseq_gpt_13B and codegen_6B_mono models were removed. fairseq_gpt_13B is redirected to falcon_7B and codegen_6B_mono is redirected to llama2_7B . 2023-04-12: added the flan_t5_xxl model. 2022-11-24: added the codegen_6B_mono model. 2022-11-19: added the text_to_image endpoint. 2022-07-28: added the credits endpoint. 2022-06-06: added the num_tokens property in the logprob endpoint. Fixed handling of escaped surrogate pairs in the JSON request body. 2022-05-02: added the translate endpoint and the m2m100_1_2B model. 2022-05-02: added the repetition_penalty and typical_p parameters. 2022-04-20: added the n parameter. 2022-04-20: the stop parameter can now be used with streaming output. 2022-04-04: added the logit_bias , presence_penalty , frequency_penalty parameters to the completion endpoint. 2022-04-04: added the tokenize endpoint. "
    },
    {
      "arxiv_id": "https://docs.cohere.com/docs/models",
      "full_text": " An Overview of Cohere&#x27;s Models | Cohere Docs v2 API v2 API DASHBOARD PLAYGROUND DOCS COMMUNITY LOG IN Guides and concepts API Reference Release Notes LLMU Cookbooks Search / Ask AI Guides and concepts API Reference Release Notes LLMU Cookbooks Get Started Introduction Installation Creating a client Quickstart Playground FAQs Models An Overview of Cohere&#x27;s Models Command Embed Rerank Aya Text Generation Introduction to Text Generation at Cohere Using the Chat API Reasoning Image Inputs Streaming Responses Structured Outputs Predictable Outputs Advanced Generation Parameters Retrieval Augmented Generation (RAG) Tool Use Tokens and Tokenizers Summarizing Text Safety Modes Embeddings (Vectors, Search, Retrieval) Introduction to Embeddings at Cohere Semantic Search with Embeddings Multimodal Embeddings Batch Embedding Jobs Reranking Fine-Tuning Introduction Fine-tuning with the Web-UI Programmatic Fine-tuning Fine-tuning for Chat Fine-tuning for Rerank Fine-tuning for Classify FAQs / Troubleshooting Going to Production API Keys and Rate Limits Going Live Deprecations How Does Cohere&#x27;s Pricing Work? Integrations Integrating Embedding Models with Other Tools Cohere and LangChain LlamaIndex and Cohere Deployment Options Overview SDK Compatibility Private Deployment Cloud AI Services Tutorials Cookbooks LLM University Build Things with Cohere! Agentic RAG Cohere on Azure Responsible Use Security Usage Policy Command R and Command R+ Model Card Cohere Labs Cohere Labs Acceptable Use Policy More Resources Cohere Toolkit Datasets Improve Cohere Docs DASHBOARD PLAYGROUND DOCS COMMUNITY LOG IN Light On this page What can These Models Be Used For? Command Using Command Models on Different Platforms Embed Using Embed Models on Different Platforms Rerank Using Rerank Models on Different Platforms Aya Using Aya Models on Different Platforms Models An Overview of Cohere&#x27;s Models Copy page Cohere has a variety of models that cover many different use cases. If you need more customization, you can train a model to tune it to your specific use case. Cohere models are currently available on the following platforms: Cohere‚Äôs proprietary platform Amazon SageMaker Amazon Bedrock Microsoft Azure Oracle GenAI Service At the end of each major section below, you‚Äôll find technical details about how to call a given model on a particular platform. What can These Models Be Used For? In this section, we‚Äôll provide some high-level context on Cohere‚Äôs offerings, and what the strengths of each are. The Command family of models includes Command A , Command R7B , Command A Translate , Command A Reasoning , Command A Vision , Command R+ , Command R , and Command . Together, they are the text-generation LLMs powering tool-using agents, retrieval augmented generation (RAG), translation, copywriting, and similar use cases. They work through the Chat endpoint, which can be used with or without RAG. Rerank is the fastest way to inject the intelligence of a language model into an existing search system. It can be accessed via the Rerank endpoint. Embed improves the accuracy of search, classification, clustering, and RAG results. It also powers the Embed and Classify endpoints. The Aya family of models are aimed at expanding the number of languages covered by generative AI. Aya Expanse covers 23 languages, and Aya Vision is fully multimodal, allowing you to pass in images and text and get a single coherent response. Both are available on the Chat endpoint. Command Command is Cohere‚Äôs default generation model that takes a user instruction (or command) and generates text following the instruction. Our Command models also have conversational capabilities, meaning they are well-suited for chat applications, and Command A Vision can interact with image inputs . Model Name Description Modality Context Length Maximum Output Tokens Endpoints command-a-03-2025 Command A is our most performant model to date, excelling at tool use, agents, retrieval augmented generation (RAG), and multilingual use cases. Command A has a context length of 256K, only requires two GPUs to run, and has 150% higher throughput compared to Command R+ 08-2024. Text 256k 8k Chat command-r7b-12-2024 command-r7b-12-2024 is a small, fast update delivered in December 2024. It excels at RAG, tool use, agents, and similar tasks requiring complex reasoning and multiple steps. Text 128k 4k Chat command-a-translate-08-2025 Command A Translate is Cohere‚Äôs state of the art machine translation model, excelling at a variety of translation tasks on 23 languages: English, French, Spanish, Italian, German, Portuguese, Japanese, Korean, Chinese, Arabic, Russian, Polish, Turkish, Vietnamese, Dutch, Czech, Indonesian, Ukrainian, Romanian, Greek, Hindi, Hebrew, Persian. Text 8K 8k Chat command-a-reasoning-08-2025 Command A Reasoning is Cohere‚Äôs first reasoning model, able to ‚Äòthink‚Äô before generating an output in a way that allows it to perform well in certain kinds of nuanced problem-solving and agent-based tasks in 23 languages. Text 256k 32k Chat command-a-vision-07-2025 Command A Vision is our first model capable of processing images, excelling in enterprise use cases such as analyzing charts, graphs, and diagrams, table understanding, OCR, document Q&amp;A, and object detection. It officially supports English, Portuguese, Italian, French, German, and Spanish. Text, Images 128K 8K Chat command-r-plus-04-2024 Command R+ is an instruction-following conversational model that performs language tasks at a higher quality, more reliably, and with a longer context than previous models. It is best suited for complex RAG workflows and multi-step tool use. Text 128k 4k Chat command-r-plus command-r-plus is an alias for command-r-plus-04-2024 , so if you use command-r-plus in the API, that‚Äôs the model you‚Äôre pointing to. Text 128k 4k Chat command-r-08-2024 command-r-08-2024 is an update of the Command R model, delivered in August 2024. Find more information here Text 128k 4k Chat command-r-03-2024 Command R is an instruction-following conversational model that performs language tasks at a higher quality, more reliably, and with a longer context than previous models. It can be used for complex workflows like code generation, retrieval augmented generation (RAG), tool use, and agents. Text 128k 4k Chat command-r command-r is an alias for command-r-03-2024 , so if you use command-r in the API, that‚Äôs the model you‚Äôre pointing to. Text 128k 4k Chat command An instruction-following conversational model that performs language tasks with high quality, more reliably and with a longer context than our base generative models. Text 4k 4k Chat , Summarize command-nightly To reduce the time between major releases, we put out nightly versions of command models. For command , that is command-nightly . Be advised that command-nightly is the latest, most experimental, and (possibly) unstable version of its default counterpart. Nightly releases are updated regularly, without warning, and are not recommended for production use. Text 128k 4k Chat command-light A smaller, faster version of command . Almost as capable, but a lot faster. Text 4k 4k Chat , Summarize command-light-nightly To reduce the time between major releases, we put out nightly versions of command models. For command-light , that is command-light-nightly . Be advised that command-light-nightly is the latest, most experimental, and (possibly) unstable version of its default counterpart. Nightly releases are updated regularly, without warning, and are not recommended for production use. Text 4k 4k Chat Using Command Models on Different Platforms In this table, we provide some important context for using Cohere Command models on Amazon Bedrock, Amazon SageMaker, and more. Model Name Amazon Bedrock Model ID Amazon SageMaker Azure AI Foundry Oracle OCI Generative AI Service command-a-03-2025 (Coming Soon) Unique per deployment Unique per deployment cohere.command-a-03-2025 command-r7b-12-2024 N/A N/A N/A N/A command-r-plus cohere.command-r-plus-v1:0 Unique per deployment Unique per deployment cohere.command-r-plus v1.2 command-r cohere.command-r-v1:0 Unique per deployment Unique per deployment cohere.command-r-16k v1.2 command cohere.command-text-v14 N/A N/A cohere.command v15.6 command-nightly N/A N/A N/A N/A command-light cohere.command-light-text-v14 N/A N/A cohere.command-light v15.6 command-light-nightly N/A N/A N/A N/A Embed These models can be used to generate embeddings from text or classify it based on various parameters. Embeddings can be used for estimating semantic similarity between two sentences, choosing a sentence which is most likely to follow another sentence, or categorizing user feedback, while outputs from the Classify endpoint can be used for any classification or analysis task. The Representation model comes with a variety of helper functions, such as for detecting the language of an input. Model Name Description Modalities Dimensions Context Length Similarity Metric Endpoints embed-v4.0 A model that allows for text and images to be classified or turned into embeddings Text, Images, Mixed texts/images (i.e. PDFs) One of ‚Äò[256, 512, 1024, 1536 (default)]‚Äò 128k Cosine Similarity, Dot Product Similarity, Euclidean Distance Embed , Embed Jobs embed-english-v3.0 A model that allows for text to be classified or turned into embeddings. English only. Text, Images 1024 512 Cosine Similarity Embed , Embed Jobs embed-english-light-v3.0 A smaller, faster version of embed-english-v3.0 . Almost as capable, but a lot faster. English only. Text, Images 384 512 Cosine Similarity Embed , Embed Jobs embed-multilingual-v3.0 Provides multilingual classification and embedding support. See supported languages here. Text, Images 1024 512 Cosine Similarity Embed , Embed Jobs embed-multilingual-light-v3.0 A smaller, faster version of embed-multilingual-v3.0 . Almost as capable, but a lot faster. Supports multiple languages. Text, Images 384 512 Cosine Similarity Embed , Embed Jobs Using Embed Models on Different Platforms In this table, we provide some important context for using Cohere Embed models on Amazon Bedrock, Amazon SageMaker, and more. Model Name Amazon Bedrock Model ID Amazon SageMaker Azure AI Foundry Oracle OCI Generative AI Service embed-v4.0 (Coming Soon) Unique per deployment cohere-embed-v-4-plan (Coming Soon) embed-english-v3.0 cohere.embed-english-v3 Unique per deployment Unique per deployment cohere.embed-english-image-v3.0 (for images), cohere.embed-english-v3.0 (for text) embed-english-light-v3.0 N/A Unique per deployment N/A cohere.embed-english-light-image-v3.0 (for images), cohere.embed-english-light-v3.0 (for text) embed-multilingual-v3.0 cohere.embed-multilingual-v3 Unique per deployment Unique per deployment cohere.embed-multilingual-image-v3.0 (for images), cohere.embed-multilingual-v3.0 (for text) embed-multilingual-light-v3.0 N/A Unique per deployment N/A cohere.embed-multilingual-light-image-v3.0 (for images), cohere.embed-multilingual-light-v3.0 (for text) embed-english-v2.0 N/A Unique per deployment N/A N/A embed-english-light-v2.0 N/A Unique per deployment N/A cohere.embed-english-light-v2.0 embed-multilingual-v2.0 N/A Unique per deployment N/A N/A Rerank The Rerank model can improve created models by re-organizing their results based on certain parameters. This can be used to improve search algorithms. Model Name Description Modalities Context Length Endpoints rerank-v3.5 A model that allows for re-ranking English Language documents and semi-structured data (JSON). This model has a context length of 4096 tokens. Text 4k Rerank rerank-english-v3.0 A model that allows for re-ranking English Language documents and semi-structured data (JSON). This model has a context length of 4096 tokens. Text 4k Rerank rerank-multilingual-v3.0 A model for documents and semi-structure data (JSON) that are not in English. Supports the same languages as embed-multilingual-v3.0. This model has a context length of 4096 tokens. Text 4k Rerank Using Rerank Models on Different Platforms In this table, we provide some important context for using Cohere Rerank models on Amazon Bedrock, SageMaker, and more. Model Name Amazon Bedrock Model ID Amazon SageMaker Azure AI Foundry Oracle OCI Generative AI Service rerank-v3.5 cohere.rerank-v3-5:0 Unique per deployment Cohere-rerank-v3.5 cohere.rerank.3-5 rerank-english-v3.0 N/A Unique per deployment Cohere-rerank-v3-english N/A rerank-multilingual-v3.0 N/A Unique per deployment Cohere-rerank-v3-multilingual N/A Rerank accepts full strings rather than tokens, so the token limit works a little differently. Rerank will automatically chunk documents longer than 510 tokens, and there is therefore no explicit limit to how long a document can be when using rerank. See our best practice guide for more info about formatting documents for the Rerank endpoint. Aya Aya is a family of multilingual large language models designed to expand the number of languages covered by generative AI for purposes of research and to better-serve minority linguistic communities. Its 8-billion and 32-billion parameter ‚ÄúExpanse‚Äù offerings are optimized to perform well in these 23 languages: Arabic, Chinese (simplified &amp; traditional), Czech, Dutch, English, French, German, Greek, Hebrew, Hebrew, Hindi, Indonesian, Italian, Japanese, Korean, Persian, Polish, Portuguese, Romanian, Russian, Spanish, Turkish, Ukrainian, and Vietnamese. Its 8-billion and 32-billion parameter ‚ÄúVision‚Äù models are state-of-the-art multimodal models excelling at a variety of critical benchmarks for language, text, and image capabilities. Model Name Description Modality Context Length Maximum Output Tokens Endpoints c4ai-aya-expanse-8b Aya Expanse is a highly performant 8B multilingual model, designed to rival monolingual performance through innovations in instruction tuning with data arbitrage, preference training, and model merging. Serves 23 languages. Text 8k 4k Chat c4ai-aya-expanse-32b Aya Expanse is a highly performant 32B multilingual model, designed to rival monolingual performance through innovations in instruction tuning with data arbitrage, preference training, and model merging. Serves 23 languages. Text 128k 4k Chat c4ai-aya-vision-8b Aya Vision is a state-of-the-art multimodal model excelling at a variety of critical benchmarks for language, text, and image capabilities. This 8 billion parameter variant is focused on low latency and best-in-class performance. Text, Images 16k 4k Chat c4ai-aya-vision-32b Aya Vision is a state-of-the-art multimodal model excelling at a variety of critical benchmarks for language, text, and image capabilities. Serves 23 languages. This 32 billion parameter variant is focused on state-of-art multilingual performance. Text, Images 16k 4k Chat Using Aya Models on Different Platforms Aya isn‚Äôt available on other platforms, but it can be used with WhatsApp. Find more information here . Was this page helpful? Yes No Edit this page Previous Cohere&#x27;s Command A Model Command A model details and specifications Next Built with "
    },
    {
      "arxiv_id": "https://dataplatform.cloud.ibm.com/docs/content/wsj/analyze-data/fm-models.html?context=wx",
      "full_text": " Supported foundation models in watsonx.ai | IBM watsonx IBM &nbsp;watsonx IBM &nbsp;watsonx Log In Sign Up Branch Pull 0 Commit 0 Push 0 Checkout branch Merge conflict Commit history Git preferences Pull and push Pull only Export project Import project Migrate New New Analytics project Data Transform project Launch IDE JupyterLab RStudio Project terminal Custom IDE Add to project Add to project Connected assets Notebook Connection Data asset Scheduled job Model Function BETA Synthesized neural network BETA Experiment BETA Streams flow BETA Modeler flow Dashboard Data Refinery flow Environment Visual recognition model Natural language classifier model Application Fork Project Export Download 0 / 0 Confirm Do you want to log out? Cancel Log out Find information Overview Planning an AI solution Getting started and tutorials Gen AI solutions Projects Preparing data Data science solutions Deploying AI Governing AI Administration Glossary Was the topic helpful? Yes No Missing information Incorrect information Confusing information Broken link Other Back 0/1000 Send comment Back Contact IBM Terms of Use Privacy Accessibility Focus sentinel Focus sentinel Focus sentinel Focus sentinel "
    },
    {
      "arxiv_id": "https://github.com/EleutherAI/lm-evaluation-harness/blob/main/docs/model_guide.md#interface",
      "full_text": " lm-evaluation-harness/docs/model_guide.md at main ¬∑ EleutherAI/lm-evaluation-harness ¬∑ GitHub Skip to content Navigation Menu Toggle navigation Sign in Appearance settings Platform GitHub Copilot Write better code with AI GitHub Spark New Build and deploy intelligent apps GitHub Models New Manage and compare prompts GitHub Advanced Security Find and fix vulnerabilities Actions Automate any workflow Codespaces Instant dev environments Issues Plan and track work Code Review Manage code changes Discussions Collaborate outside of code Code Search Find more, search less Explore Why GitHub Documentation GitHub Skills Blog Integrations GitHub Marketplace View all features Solutions By company size Enterprises Small and medium teams Startups Nonprofits By use case DevSecOps DevOps CI/CD View all use cases By industry Healthcare Financial services Manufacturing Government View all industries View all solutions Resources Topics AI DevOps Security Software Development View all Explore Learning Pathways Events &amp; Webinars Ebooks &amp; Whitepapers Customer Stories Partners Executive Insights Open Source GitHub Sponsors Fund open source developers The ReadME Project GitHub community articles Repositories Topics Trending Collections Enterprise Enterprise platform AI-powered developer platform Available add-ons GitHub Advanced Security Enterprise-grade security features Copilot for business Enterprise-grade AI features Premium Support Enterprise-grade 24/7 support Pricing Search or jump to... Search code, repositories, users, issues, pull requests... --> Search Clear Search syntax tips Provide feedback --> We read every piece of feedback, and take your input very seriously. Include my email address so I can be contacted Cancel Submit feedback Saved searches Use saved searches to filter your results more quickly --> Name Query To see all available qualifiers, see our documentation . Cancel Create saved search Sign in Sign up Appearance settings Resetting focus You signed in with another tab or window. Reload to refresh your session. You signed out in another tab or window. Reload to refresh your session. You switched accounts on another tab or window. Reload to refresh your session. Dismiss alert {{ message }} EleutherAI / lm-evaluation-harness Public Notifications You must be signed in to change notification settings Fork 2.7k Star 10.1k Code Issues 477 Pull requests 160 Actions Projects 1 Security Uh oh! There was an error while loading. Please reload this page . Insights Additional navigation options Code Issues Pull requests Actions Projects Security Insights Footer &copy; 2025 GitHub,&nbsp;Inc. Footer navigation Terms Privacy Security Status Docs Contact Manage cookies Do not share my personal information You can‚Äôt perform that action at this time. "
    },
    {
      "arxiv_id": "https://github.com/EleutherAI/lm-evaluation-harness/blob/main/docs/interface.md#external-library-usage",
      "full_text": " lm-evaluation-harness/docs/interface.md at main ¬∑ EleutherAI/lm-evaluation-harness ¬∑ GitHub Skip to content Navigation Menu Toggle navigation Sign in Appearance settings Platform GitHub Copilot Write better code with AI GitHub Spark New Build and deploy intelligent apps GitHub Models New Manage and compare prompts GitHub Advanced Security Find and fix vulnerabilities Actions Automate any workflow Codespaces Instant dev environments Issues Plan and track work Code Review Manage code changes Discussions Collaborate outside of code Code Search Find more, search less Explore Why GitHub Documentation GitHub Skills Blog Integrations GitHub Marketplace View all features Solutions By company size Enterprises Small and medium teams Startups Nonprofits By use case DevSecOps DevOps CI/CD View all use cases By industry Healthcare Financial services Manufacturing Government View all industries View all solutions Resources Topics AI DevOps Security Software Development View all Explore Learning Pathways Events &amp; Webinars Ebooks &amp; Whitepapers Customer Stories Partners Executive Insights Open Source GitHub Sponsors Fund open source developers The ReadME Project GitHub community articles Repositories Topics Trending Collections Enterprise Enterprise platform AI-powered developer platform Available add-ons GitHub Advanced Security Enterprise-grade security features Copilot for business Enterprise-grade AI features Premium Support Enterprise-grade 24/7 support Pricing Search or jump to... Search code, repositories, users, issues, pull requests... --> Search Clear Search syntax tips Provide feedback --> We read every piece of feedback, and take your input very seriously. Include my email address so I can be contacted Cancel Submit feedback Saved searches Use saved searches to filter your results more quickly --> Name Query To see all available qualifiers, see our documentation . Cancel Create saved search Sign in Sign up Appearance settings Resetting focus You signed in with another tab or window. Reload to refresh your session. You signed out in another tab or window. Reload to refresh your session. You switched accounts on another tab or window. Reload to refresh your session. Dismiss alert {{ message }} EleutherAI / lm-evaluation-harness Public Notifications You must be signed in to change notification settings Fork 2.7k Star 10.1k Code Issues 477 Pull requests 160 Actions Projects 1 Security Uh oh! There was an error while loading. Please reload this page . Insights Additional navigation options Code Issues Pull requests Actions Projects Security Insights Footer &copy; 2025 GitHub,&nbsp;Inc. Footer navigation Terms Privacy Security Status Docs Contact Manage cookies Do not share my personal information You can‚Äôt perform that action at this time. "
    },
    {
      "arxiv_id": "https://github.com/EleutherAI/lm-evaluation-harness/blob/main/docs/interface.md",
      "full_text": " lm-evaluation-harness/docs/interface.md at main ¬∑ EleutherAI/lm-evaluation-harness ¬∑ GitHub Skip to content Navigation Menu Toggle navigation Sign in Appearance settings Platform GitHub Copilot Write better code with AI GitHub Spark New Build and deploy intelligent apps GitHub Models New Manage and compare prompts GitHub Advanced Security Find and fix vulnerabilities Actions Automate any workflow Codespaces Instant dev environments Issues Plan and track work Code Review Manage code changes Discussions Collaborate outside of code Code Search Find more, search less Explore Why GitHub Documentation GitHub Skills Blog Integrations GitHub Marketplace View all features Solutions By company size Enterprises Small and medium teams Startups Nonprofits By use case DevSecOps DevOps CI/CD View all use cases By industry Healthcare Financial services Manufacturing Government View all industries View all solutions Resources Topics AI DevOps Security Software Development View all Explore Learning Pathways Events &amp; Webinars Ebooks &amp; Whitepapers Customer Stories Partners Executive Insights Open Source GitHub Sponsors Fund open source developers The ReadME Project GitHub community articles Repositories Topics Trending Collections Enterprise Enterprise platform AI-powered developer platform Available add-ons GitHub Advanced Security Enterprise-grade security features Copilot for business Enterprise-grade AI features Premium Support Enterprise-grade 24/7 support Pricing Search or jump to... Search code, repositories, users, issues, pull requests... --> Search Clear Search syntax tips Provide feedback --> We read every piece of feedback, and take your input very seriously. Include my email address so I can be contacted Cancel Submit feedback Saved searches Use saved searches to filter your results more quickly --> Name Query To see all available qualifiers, see our documentation . Cancel Create saved search Sign in Sign up Appearance settings Resetting focus You signed in with another tab or window. Reload to refresh your session. You signed out in another tab or window. Reload to refresh your session. You switched accounts on another tab or window. Reload to refresh your session. Dismiss alert {{ message }} EleutherAI / lm-evaluation-harness Public Notifications You must be signed in to change notification settings Fork 2.7k Star 10.1k Code Issues 477 Pull requests 160 Actions Projects 1 Security Uh oh! There was an error while loading. Please reload this page . Insights Additional navigation options Code Issues Pull requests Actions Projects Security Insights Footer &copy; 2025 GitHub,&nbsp;Inc. Footer navigation Terms Privacy Security Status Docs Contact Manage cookies Do not share my personal information You can‚Äôt perform that action at this time. "
    },
    {
      "arxiv_id": "https://github.com/EleutherAI/lm-evaluation-harness/tree/main/docs",
      "full_text": " lm-evaluation-harness/docs at main ¬∑ EleutherAI/lm-evaluation-harness ¬∑ GitHub Skip to content Navigation Menu Toggle navigation Sign in Appearance settings Platform GitHub Copilot Write better code with AI GitHub Spark New Build and deploy intelligent apps GitHub Models New Manage and compare prompts GitHub Advanced Security Find and fix vulnerabilities Actions Automate any workflow Codespaces Instant dev environments Issues Plan and track work Code Review Manage code changes Discussions Collaborate outside of code Code Search Find more, search less Explore Why GitHub Documentation GitHub Skills Blog Integrations GitHub Marketplace View all features Solutions By company size Enterprises Small and medium teams Startups Nonprofits By use case DevSecOps DevOps CI/CD View all use cases By industry Healthcare Financial services Manufacturing Government View all industries View all solutions Resources Topics AI DevOps Security Software Development View all Explore Learning Pathways Events &amp; Webinars Ebooks &amp; Whitepapers Customer Stories Partners Executive Insights Open Source GitHub Sponsors Fund open source developers The ReadME Project GitHub community articles Repositories Topics Trending Collections Enterprise Enterprise platform AI-powered developer platform Available add-ons GitHub Advanced Security Enterprise-grade security features Copilot for business Enterprise-grade AI features Premium Support Enterprise-grade 24/7 support Pricing Search or jump to... Search code, repositories, users, issues, pull requests... --> Search Clear Search syntax tips Provide feedback --> We read every piece of feedback, and take your input very seriously. Include my email address so I can be contacted Cancel Submit feedback Saved searches Use saved searches to filter your results more quickly --> Name Query To see all available qualifiers, see our documentation . Cancel Create saved search Sign in Sign up Appearance settings Resetting focus You signed in with another tab or window. Reload to refresh your session. You signed out in another tab or window. Reload to refresh your session. You switched accounts on another tab or window. Reload to refresh your session. Dismiss alert {{ message }} EleutherAI / lm-evaluation-harness Public Notifications You must be signed in to change notification settings Fork 2.7k Star 10.1k Code Issues 477 Pull requests 160 Actions Projects 1 Security Uh oh! There was an error while loading. Please reload this page . Insights Additional navigation options Code Issues Pull requests Actions Projects Security Insights Footer &copy; 2025 GitHub,&nbsp;Inc. Footer navigation Terms Privacy Security Status Docs Contact Manage cookies Do not share my personal information You can‚Äôt perform that action at this time. "
    },
    {
      "arxiv_id": "https://arxiv.org/abs/2005.14165",
      "full_text": " [2005.14165] Language Models are Few-Shot Learners Skip to main content We gratefully acknowledge support from the Simons Foundation, member institutions , and all contributors. Donate &gt; cs &gt; arXiv:2005.14165 Help | Advanced Search All fields Title Author Abstract Comments Journal reference ACM classification MSC classification Report number arXiv identifier DOI ORCID arXiv author ID Help pages Full text Search open search GO open navigation menu quick links Login Help Pages About --> Computer Science > Computation and Language arXiv:2005.14165 (cs) [Submitted on 28 May 2020 ( v1 ), last revised 22 Jul 2020 (this version, v4)] Title: Language Models are Few-Shot Learners Authors: Tom B. Brown , Benjamin Mann , Nick Ryder , Melanie Subbiah , Jared Kaplan , Prafulla Dhariwal , Arvind Neelakantan , Pranav Shyam , Girish Sastry , Amanda Askell , Sandhini Agarwal , Ariel Herbert-Voss , Gretchen Krueger , Tom Henighan , Rewon Child , Aditya Ramesh , Daniel M. Ziegler , Jeffrey Wu , Clemens Winter , Christopher Hesse , Mark Chen , Eric Sigler , Mateusz Litwin , Scott Gray , Benjamin Chess , Jack Clark , Christopher Berner , Sam McCandlish , Alec Radford , Ilya Sutskever , Dario Amodei View a PDF of the paper titled Language Models are Few-Shot Learners, by Tom B. Brown and 30 other authors View PDF Abstract: Recent work has demonstrated substantial gains on many NLP tasks and benchmarks by pre-training on a large corpus of text followed by fine-tuning on a specific task. While typically task-agnostic in architecture, this method still requires task-specific fine-tuning datasets of thousands or tens of thousands of examples. By contrast, humans can generally perform a new language task from only a few examples or from simple instructions - something which current NLP systems still largely struggle to do. Here we show that scaling up language models greatly improves task-agnostic, few-shot performance, sometimes even reaching competitiveness with prior state-of-the-art fine-tuning approaches. Specifically, we train GPT-3, an autoregressive language model with 175 billion parameters, 10x more than any previous non-sparse language model, and test its performance in the few-shot setting. For all tasks, GPT-3 is applied without any gradient updates or fine-tuning, with tasks and few-shot demonstrations specified purely via text interaction with the model. GPT-3 achieves strong performance on many NLP datasets, including translation, question-answering, and cloze tasks, as well as several tasks that require on-the-fly reasoning or domain adaptation, such as unscrambling words, using a novel word in a sentence, or performing 3-digit arithmetic. At the same time, we also identify some datasets where GPT-3&#39;s few-shot learning still struggles, as well as some datasets where GPT-3 faces methodological issues related to training on large web corpora. Finally, we find that GPT-3 can generate samples of news articles which human evaluators have difficulty distinguishing from articles written by humans. We discuss broader societal impacts of this finding and of GPT-3 in general. Comments: 40+32 pages Subjects: Computation and Language (cs.CL) Cite as: arXiv:2005.14165 [cs.CL] &nbsp; (or arXiv:2005.14165v4 [cs.CL] for this version) &nbsp; https://doi.org/10.48550/arXiv.2005.14165 Focus to learn more arXiv-issued DOI via DataCite Submission history From: Tom B Brown [ view email ] [v1] Thu, 28 May 2020 17:29:03 UTC (6,995 KB) [v2] Mon, 1 Jun 2020 17:08:53 UTC (6,997 KB) [v3] Fri, 5 Jun 2020 02:52:35 UTC (6,998 KB) [v4] Wed, 22 Jul 2020 19:47:17 UTC (6,998 KB) Full-text links: Access Paper: View a PDF of the paper titled Language Models are Few-Shot Learners, by Tom B. Brown and 30 other authors View PDF TeX Source Other Formats view license Current browse context: cs.CL &lt;&nbsp;prev &nbsp; | &nbsp; next&nbsp;&gt; new | recent | 2020-05 Change to browse by: cs References &amp; Citations NASA ADS Google Scholar Semantic Scholar 74 blog links ( what is this? ) DBLP - CS Bibliography listing | bibtex Tom B. Brown Nick Ryder Jared Kaplan Prafulla Dhariwal Arvind Neelakantan &hellip; a export BibTeX citation Loading... BibTeX formatted citation &times; loading... Data provided by: Bookmark Bibliographic Tools Bibliographic and Citation Tools Bibliographic Explorer Toggle Bibliographic Explorer ( What is the Explorer? ) Connected Papers Toggle Connected Papers ( What is Connected Papers? ) Litmaps Toggle Litmaps ( What is Litmaps? ) scite.ai Toggle scite Smart Citations ( What are Smart Citations? ) Code, Data, Media Code, Data and Media Associated with this Article alphaXiv Toggle alphaXiv ( What is alphaXiv? ) Links to Code Toggle CatalyzeX Code Finder for Papers ( What is CatalyzeX? ) DagsHub Toggle DagsHub ( What is DagsHub? ) GotitPub Toggle Gotit.pub ( What is GotitPub? ) Huggingface Toggle Hugging Face ( What is Huggingface? ) Links to Code Toggle Papers with Code ( What is Papers with Code? ) ScienceCast Toggle ScienceCast ( What is ScienceCast? ) Demos Demos Replicate Toggle Replicate ( What is Replicate? ) Spaces Toggle Hugging Face Spaces ( What is Spaces? ) Spaces Toggle TXYZ.AI ( What is TXYZ.AI? ) Related Papers Recommenders and Search Tools Link to Influence Flower Influence Flower ( What are Influence Flowers? ) Core recommender toggle CORE Recommender ( What is CORE? ) Author Venue Institution Topic About arXivLabs arXivLabs: experimental projects with community collaborators arXivLabs is a framework that allows collaborators to develop and share new arXiv features directly on our website. Both individuals and organizations that work with arXivLabs have embraced and accepted our values of openness, community, excellence, and user data privacy. arXiv is committed to these values and only works with partners that adhere to them. Have an idea for a project that will add value for arXiv's community? Learn more about arXivLabs . Which authors of this paper are endorsers? | Disable MathJax ( What is MathJax? ) About Help contact arXiv Click here to contact arXiv Contact subscribe to arXiv mailings Click here to subscribe Subscribe Copyright Privacy Policy Web Accessibility Assistance arXiv Operational Status Get status notifications via email or slack "
    }
  ]
}