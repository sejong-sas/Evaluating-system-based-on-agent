{
    "model_id": "tiiuae/Falcon3-7B-Base",
    "files": [
        ".gitattributes",
        "README.md",
        "config.json",
        "generation_config.json",
        "model-00001-of-00004.safetensors",
        "model-00002-of-00004.safetensors",
        "model-00003-of-00004.safetensors",
        "model-00004-of-00004.safetensors",
        "model.safetensors.index.json",
        "special_tokens_map.json",
        "tokenizer.json",
        "tokenizer_config.json"
    ],
    "readme": "---\nlanguage:\n- en\n- fr\n- es\n- pt\ntags:\n- falcon3\nlicense: other\nlicense_name: falcon-llm-license\nlicense_link: https://falconllm.tii.ae/falcon-terms-and-conditions.html\nlibrary_name: transformers\n---\n\n<div align=\"center\">\n    <img src=\"https://huggingface.co/datasets/tiiuae/documentation-images/resolve/main/general/falco3-logo.png\" alt=\"drawing\" width=\"500\"/>\n</div>\n\n# Falcon3-7B-Base\n\n**Falcon3** family of Open Foundation Models is a set of pretrained and instruct LLMs ranging from 1B to 10B.\n\nThis repository contains the **Falcon3-7B-Base**. It achieves state of art results (at the time of release) on reasoning, language understanding, instruction following, code and mathematics tasks.\nFalcon3-7B-Base supports 4 languages (english, french, spanish, portuguese) and a context length up to 32K.\n\n⚠️ **This is a raw, pretrained model, which should be further finetuned for most usecases.** \n\n## Model Details\n- Architecture\n  - transformer based causal decoder only architecture\n  - 28 decoder blocks\n  - grouped query attention (GQA) for faster inference: 12 query heads and 4 KV heads\n  - wider head dimension: 256\n  - high RoPE value to support long context understanding: 1000042\n  - 32k context length\n  - 131k vocab size\n- Pretrained on 14 Teratokens of datasets comprising of web, code, STEM, high quality and mutlilingual data using 1024 H100 GPU chips\n- Supports EN, FR, ES, PT\n- Developed by [Technology Innovation Institute](https://www.tii.ae)\n- License: TII Falcon-LLM License 2.0\n- Model Release Date: December 2024\n\n\n## Getting started\n\n<details>\n<summary> Click to expand </summary>\n\n```python\nimport torch\nfrom transformers import pipeline\n\npipe = pipeline(\n    \"text-generation\", \n    model=\"tiiuae/Falcon3-7B-Base\", \n    torch_dtype=torch.bfloat16, \n    device_map=\"auto\"\n)\nresponse = pipe(\"Question: How many hours in one day? Answer: \")\nprint(response[0]['generated_text'])\n```\n\n</details>\n\n<br>\n\n## Benchmarks\nWe report in the following table our internal pipeline benchmarks.\n - We use [lm-evaluation harness](https://github.com/EleutherAI/lm-evaluation-harness).\n - We report **raw scores**.\n - We use same batch-size across all models.\n\n\n\n<table border=\"1\" style=\"width: 100%; text-align: center; border-collapse: collapse;\">\n    <colgroup>\n        <col style=\"width: 10%;\">\n        <col style=\"width: 10%;\">\n        <col style=\"width: 7%;\">\n        <col style=\"width: 7%;\">\n        <col style=\"width: 7%;\">\n        <col style=\"width: 7%;\">\n        <col style=\"background-color: rgba(80, 15, 213, 0.5); width: 7%;\">\n    </colgroup>\n    <thead>\n        <tr>\n            <th>Category</th>\n            <th>Benchmark</th>\n            <th>Llama3.1-8B</th>\n            <th>Qwen2-7B</th>\n            <th>Qwen2.5-7B</th>\n            <th>gemma-2-9b</th>\n            <th>Falcon3-7B-Base</th>\n        </tr>\n    </thead>\n    <tbody>\n        <tr>\n            <td rowspan=\"3\">General</td>\n            <td>MMLU (5-shot)</td>\n            <td>65.2</td>\n            <td>70.4</td>\n            <td>74.2</td>\n            <td>-</td>\n            <td>67.5</td>\n        </tr>\n        <tr>\n            <td>MMLU-PRO (5-shot)</td>\n            <td>32.7</td>\n            <td>42.1</td>\n            <td>43.5</td>\n            <td>-</td>\n            <td>39.2</td>\n        </tr>\n        <tr>\n            <td>IFEval</td>\n            <td>12.0</td>\n            <td>30.6</td>\n            <td>33.9</td>\n            <td>-</td>\n            <td>34.3</td>\n        </tr>\n        <tr>\n            <td rowspan=\"2\">Math</td>\n            <td>GSM8K (5-shot)</td>\n            <td>49.4</td>\n            <td>77.9</td>\n            <td>82.9</td>\n            <td>-</td>\n            <td>76.2</td>\n        </tr>\n        <tr>\n            <td>MATH(4-shot)</td>\n            <td>4.1</td>\n            <td>17.5</td>\n            <td>15.5</td>\n            <td>-</td>\n            <td>18.0</td>\n        </tr>\n        <tr>\n            <td rowspan=\"4\">Reasoning</td>\n            <td>Arc Challenge (25-shot)</td>\n            <td>53.4</td>\n            <td>57.4</td>\n            <td>59.0</td>\n            <td>-</td>\n            <td>59.6</td>\n        </tr>\n        <tr>\n            <td>GPQA (0-shot)</td>\n            <td>31.0</td>\n            <td>31.9</td>\n            <td>33.0</td>\n            <td>-</td>\n            <td>35.5</td>\n        </tr>\n        <tr>\n            <td>MUSR (0-shot)</td>\n            <td>38.0</td>\n            <td>44.1</td>\n            <td>44.2</td>\n            <td>-</td>\n            <td>47.3</td>\n        </tr>\n        <tr>\n            <td>BBH (3-shot)</td>\n            <td>46.5</td>\n            <td>53.3</td>\n            <td>54.0</td>\n            <td>-</td>\n            <td>51.0</td>\n        </tr>\n        <tr>\n            <td rowspan=\"4\">CommonSense Understanding</td>\n            <td>PIQA (0-shot)</td>\n            <td>80.3</td>\n            <td>79.8</td>\n            <td>78.7</td>\n            <td>-</td>\n            <td>77.7</td>\n        </tr>\n        <tr>\n            <td>SciQ (0-shot)</td>\n            <td>96.3</td>\n            <td>95.9</td>\n            <td>96.6</td>\n            <td>-</td>\n            <td>95.3</td>\n        </tr>\n        <tr>\n            <td>Winogrande (0-shot)</td>\n            <td>74.0</td>\n            <td>72.1</td>\n            <td>72.9</td>\n            <td>-</td>\n            <td>71.0</td>\n        </tr>\n        <tr>\n            <td>OpenbookQA (0-shot)</td>\n            <td>33.4</td>\n            <td>35.2</td>\n            <td>33.6</td>\n            <td>-</td>\n            <td>31.4</td>\n        </tr>\n    </tbody>\n</table>\n\n## Useful links\n- View our [release blogpost](https://huggingface.co/blog/falcon3).\n- Feel free to join [our discord server](https://discord.gg/fwXpMyGc) if you have any questions or to interact with our researchers and developers.\n\n## Technical Report\n\nComing soon....\n\n## Citation\nIf Falcon3 family were helpful to your work, feel free to give us a cite.\n\n```\n@misc{Falcon3,\n    title = {Falcon 3 family of Open Foundation Models},\n    author = {TII Team},\n    month = {December},\n    year = {2024}\n}\n```",
    "config": "{\n  \"architectures\": [\n    \"LlamaForCausalLM\"\n  ],\n  \"attention_bias\": false,\n  \"attention_dropout\": 0.0,\n  \"eos_token_id\": 11,\n  \"head_dim\": 256,\n  \"hidden_act\": \"silu\",\n  \"hidden_size\": 3072,\n  \"initializer_range\": 0.02,\n  \"intermediate_size\": 23040,\n  \"max_position_embeddings\": 32768,\n  \"mlp_bias\": false,\n  \"model_type\": \"llama\",\n  \"num_attention_heads\": 12,\n  \"num_hidden_layers\": 28,\n  \"num_key_value_heads\": 4,\n  \"pretraining_tp\": 1,\n  \"rms_norm_eps\": 1e-06,\n  \"rope_scaling\": null,\n  \"rope_theta\": 1000042,\n  \"tie_word_embeddings\": false,\n  \"torch_dtype\": \"bfloat16\",\n  \"transformers_version\": \"4.46.1\",\n  \"use_cache\": true,\n  \"vocab_size\": 131072\n}\n",
    "generation_config": "{\n  \"_from_model_config\": true,\n  \"bos_token_id\": 11,\n  \"eos_token_id\": 11,\n  \"transformers_version\": \"4.46.1\"\n}\n",
    "license_file": "",
    "py_files": {}
}