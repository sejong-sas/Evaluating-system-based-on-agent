{
  "model": "tiiuae/Falcon3-7B-Instruct",
  "scores": {
    "1-2 Code": {
      "score": 0.0,
      "reason": "No training pipeline files; README mentions are ignored."
    },
    "1-3 License": {
      "score": 1.0,
      "reason": "The README states “License: TII Falcon-LLM License 2.0”. The rubric explicitly lists this licence in the Open category because it grants the four basic rights without major restrictions."
    },
    "1-4 Paper": {
      "score": 0.5,
      "reason": "The only publication reference is a release blog-post; no peer-reviewed or technical report dedicated to Falcon3-7B-Instruct is provided. This is Semi-Open."
    },
    "1-5 Architecture": {
      "score": 1,
      "reason": "Architecture evidence present in extracted quotes."
    },
    "1-6 Tokenizer": {
      "score": 1.0,
      "reason": "Tokenizer files downloadable in repository."
    },
    "2-1 Hardware": {
      "score": 1.0,
      "reason": "Quoted sentence: “using 1024 H100 GPU chips”, which gives both the hardware type and exact quantity. This meets the Open criterion."
    },
    "2-2 Software": {
      "score": 0.0,
      "reason": "Quotes only mention the base framework “transformers 4.46.1” and an evaluation harness. No additional training-stack components (e.g., DeepSpeed configs) are provided, so the requirement for Semi-Open is not met."
    },
    "2-3 API": {
      "score": 0.0,
      "reason": "Only generic or non-official mentions; no qualifying official API docs.  No qualifying official API docs via web search."
    },
    "3-1 Pre-training": {
      "score": 0.5,
      "reason": "Some details are given (14 T tokens, data domains, 1024 H100 GPUs, depth-upscaling approach) but no full hyper-parameter schedule or pipeline. Hence, Semi-Open."
    },
    "3-2 Fine-tuning": {
      "score": 0.5,
      "reason": "The post-training stage is described as using 1.2 M samples across several domains, but again lacks reproducible hyper-parameters, so Semi-Open."
    },
    "3-3 Reinforcement Learning": {
      "score": 0.0,
      "reason": "No quotes describe any RLHF or RL training; item is Closed by default. No direct quote evidence; defaulting to Closed by strict rule."
    },
    "4-1 Pre-training Data": {
      "score": 0.5,
      "reason": "The source types and overall size (14 T tokens) are named, but there is no dataset list, licensing, or access instructions—partial disclosure only."
    },
    "4-2 Fine-tuning Data": {
      "score": 0.5,
      "reason": "Quantity (1.2 M) and high-level categories are stated, yet exact datasets and availability are omitted; thus Semi-Open."
    },
    "4-3 Reinforcement Learning Data": {
      "score": 0.0,
      "reason": "No RL data information is provided in the quotes. No direct quote evidence; defaulting to Closed by strict rule."
    },
    "4-4 Data Filtering": {
      "score": 0.0,
      "reason": "No mention of filtering, deduplication, or quality screening appears in the supplied evidence. No direct quote evidence; defaulting to Closed by strict rule."
    },
    "1-1 Weights": {
      "score": 1,
      "reason": "Weights files present in the repository (e.g., *.safetensors/bin/pt/ckpt)."
    }
  },
  "included_scores": {
    "1-2 Code": {
      "score": 0.0,
      "reason": "No training pipeline files; README mentions are ignored."
    },
    "1-3 License": {
      "score": 1.0,
      "reason": "The README states “License: TII Falcon-LLM License 2.0”. The rubric explicitly lists this licence in the Open category because it grants the four basic rights without major restrictions."
    },
    "1-4 Paper": {
      "score": 0.5,
      "reason": "The only publication reference is a release blog-post; no peer-reviewed or technical report dedicated to Falcon3-7B-Instruct is provided. This is Semi-Open."
    },
    "1-5 Architecture": {
      "score": 1,
      "reason": "Architecture evidence present in extracted quotes."
    },
    "1-6 Tokenizer": {
      "score": 1.0,
      "reason": "Tokenizer files downloadable in repository."
    },
    "2-1 Hardware": {
      "score": 1.0,
      "reason": "Quoted sentence: “using 1024 H100 GPU chips”, which gives both the hardware type and exact quantity. This meets the Open criterion."
    },
    "2-2 Software": {
      "score": 0.0,
      "reason": "Quotes only mention the base framework “transformers 4.46.1” and an evaluation harness. No additional training-stack components (e.g., DeepSpeed configs) are provided, so the requirement for Semi-Open is not met."
    },
    "2-3 API": {
      "score": 0.0,
      "reason": "Only generic or non-official mentions; no qualifying official API docs.  No qualifying official API docs via web search."
    },
    "3-1 Pre-training": {
      "score": 0.5,
      "reason": "Some details are given (14 T tokens, data domains, 1024 H100 GPUs, depth-upscaling approach) but no full hyper-parameter schedule or pipeline. Hence, Semi-Open."
    },
    "3-2 Fine-tuning": {
      "score": 0.5,
      "reason": "The post-training stage is described as using 1.2 M samples across several domains, but again lacks reproducible hyper-parameters, so Semi-Open."
    },
    "4-1 Pre-training Data": {
      "score": 0.5,
      "reason": "The source types and overall size (14 T tokens) are named, but there is no dataset list, licensing, or access instructions—partial disclosure only."
    },
    "4-2 Fine-tuning Data": {
      "score": 0.5,
      "reason": "Quantity (1.2 M) and high-level categories are stated, yet exact datasets and availability are omitted; thus Semi-Open."
    },
    "4-4 Data Filtering": {
      "score": 0.0,
      "reason": "No mention of filtering, deduplication, or quality screening appears in the supplied evidence. No direct quote evidence; defaulting to Closed by strict rule."
    },
    "1-1 Weights": {
      "score": 1,
      "reason": "Weights files present in the repository (e.g., *.safetensors/bin/pt/ckpt)."
    }
  },
  "final_score_10pt": 5.357,
  "meta": {
    "usage_from_dispatch": {
      "fine_tuning": "used",
      "rl": "not_used"
    },
    "excluded": [
      "3-3 Reinforcement Learning",
      "4-3 Reinforcement Learning Data"
    ],
    "denominator": 14,
    "raw_sum": 7.5,
    "scale": "10/14",
    "code_detection_reason": "No training pipeline files; README mentions are ignored.",
    "pretrain_sources_used": false
  }
}