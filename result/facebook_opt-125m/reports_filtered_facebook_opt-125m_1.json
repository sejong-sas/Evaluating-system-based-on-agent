{
  "1-1 (Weights)": "The OPT-125M checkpoint is part of the “suite of decoder-only pre-trained transformers ranging from 125M to 175B parameters” that Meta explicitly states it “aim[s] to fully and responsibly share with interested researchers.”  The authors say they are “releasing all of our models between 125M and 66B parameters,” making the 125 M weights directly downloadable, while the much larger 175 B model is available only through gated research access.  Multiple passages reiterate that researchers receive “access to model weights … along with a suite of smaller baselines mirroring the setup for OPT-175B,” confirming that OPT-125M is one of the baseline checkpoints publicly released without request forms.  In short, the 125 M weights are openly published, hosted by Meta, and can be obtained by anyone (subject only to the project’s license), whereas only the 175 B variant requires a special access request.",
  "1-2 (Code)": "Training-time resources are also public.  The paper promises that, in addition to weights, Meta is “releasing our logbook detailing the infrastructure challenges we faced, along with code for experimenting with all of the released models.”  Concretely, the open-source repository is named “metaseq,” which the authors call “our codebase … which enabled training OPT-175B on 992 × 80 GB A100 GPUs.”  They emphasize that this repository is the “final source of truth” for implementation details and is accompanied by a day-to-day logbook that records compute usage, instabilities and other engineering notes.  Thus, for OPT-125M users receive full pre-training and evaluation code, configuration files, and the detailed training logbook, not merely inference scripts.",
  "1-3 (License)": "All released checkpoints, including OPT-125M, are governed by “a non-commercial use license agreement.”  The text repeats that access is “limit[ed] … to the research community with a non-commercial license” so that work can first focus on understanding model limitations before “broader commercial deployment.”  The license therefore allows use for research but bars commercial exploitation; redistribution and derivative creation are only permitted within the bounds of that non-commercial, research-focused license.",
  "1-4 (Paper)": "The authoritative reference is the technical report titled “OPT: Open Pre-trained Transformer Language Models.”  This document “present[s] Open Pre-trained Transformers (OPT), a suite of decoder-only pre-trained transformers ranging from 125M to 175B parameters,” and explicitly lists the 125 M model as the smallest member of the family.  The same report notes that “OPT-175B described in this paper is version 1.0.0,” grounding the publication in a specific release cycle.  Readers are directed to the paper itself and a corresponding Meta AI Research blog post for full methodological details.",
  "1-1 (Weights)__evidence": [
    {
      "source": "[pdf_text]",
      "quote": "We present Open Pre-trained Transformers (OPT), a suite of decoder-only pre-trained transformers ranging from 125M to 175B parameters, which we aim to fully and responsibly share with interested researchers."
    },
    {
      "source": "[pdf_text]",
      "quote": "We are releasing all of our models between 125M and 66B parameters, and will provide full research access to OPT-175B upon request."
    },
    {
      "source": "[pdf_text]",
      "quote": "…we are disclosing all of the details involved in training OPT-175B through our logbook,9 our code, and providing researchers access to model weights for OPT-175B, along with a suite of smaller baselines mirroring the setup for OPT-175B."
    },
    {
      "source": "[pdf_text]",
      "quote": "To enable experimentation at 175B scale, we are providing researchers with direct access to the parameters of OPT-175B, along with a suite of smaller baselines mirroring the setup for OPT-175B."
    },
    {
      "source": "[pdf_text]",
      "quote": "We release OPT-175B for research into Language Models, especially as it pertains to Responsible AI."
    }
  ],
  "1-2 (Code)__evidence": [
    {
      "source": "[pdf_text]",
      "quote": "We present Open Pre-trained Transformers (OPT), a suite of decoder-only pre-trained transformers ranging from 125M to 175B parameters, which we aim to fully and responsibly share with interested researchers. We are also releasing our logbook detailing the infrastructure challenges we faced, along with code for experimenting with all of the released models."
    },
    {
      "source": "[pdf_text]",
      "quote": "…we are disclosing all of the details involved in training OPT-175B through our logbook,9 our code, and providing researchers access to model weights for OPT-175B, along with a suite of smaller baselines mirroring the setup for OPT-175B."
    },
    {
      "source": "[pdf_text]",
      "quote": "We are also releasing both the logbook of our model creation as well as our codebase, metaseq, which enabled training OPT-175B on 992 80GB A100 GPUs, reaching 147 TFLOP/s utilization per GPU."
    },
    {
      "source": "[pdf_text]",
      "quote": "We are disclosing all of the details involved in training OPT-175B through our logbook, our code, and providing researchers access to model weights for OPT-175B, along with a suite of smaller baselines mirroring the setup for OPT-175B."
    },
    {
      "source": "[pdf_text]",
      "quote": "By sharing a detailed account of our day-to-day training process, we disclose not only how much compute was used to train the current version of OPT-175B, but also the human overhead required when underlying infrastructure or the training process itself becomes unstable at scale. Outside of these notes, the metaseq codebase itself is the ﬁnal source of truth in many of our implementation details."
    },
    {
      "source": "[pdf_text]",
      "quote": "Paper or other resource for more information: See the rest of this paper for more details on OPT-175B as well as the corresponding post on the Meta AI Research Blog. More details are also available in metaseq, our open-source repository.12"
    }
  ],
  "1-3 (License)__evidence": [
    {
      "source": "[pdf_text]",
      "quote": "By limiting access to OPT-175B to the research community with a non-commercial license, we aim to focus development efforts on quantifying the limitations of the LLMs first, before broader commercial deployment occurs."
    },
    {
      "source": "[pdf_text]",
      "quote": "• License: OPT-175B and the smaller baseline models are made available through a non-commercial use license agreement provided in our model license."
    },
    {
      "source": "[pdf_text]",
      "quote": "By limiting access to OPT-175B to the research community with a non-commercial license, we aim to focus development efforts on quantifying the limitations of the LLMs ﬁrst, before broader commercial deployment occurs."
    },
    {
      "source": "[pdf_text]",
      "quote": "License: OPT-175B and the smaller baseline models are made available through a non-commercial use license agreement provided in our model license.13"
    }
  ],
  "1-4 (Paper)__evidence": [
    {
      "source": "[pdf_text]",
      "quote": "Title: OPT: Open Pre-trained Transformer Language Models"
    },
    {
      "source": "[pdf_text]",
      "quote": "In this technical report, we present Open Pre-trained Transformers (OPT), a suite of decoder-only pre-trained transformers ranging from 125M to 175B parameters, which we aim to fully and responsibly share with interested researchers."
    },
    {
      "source": "[sections/https://arxiv.org/abs/2205.01068]",
      "quote": "In this technical report, we present Open Pre-trained Transformers (OPT), a suite of decoder-only pre-trained transformers ranging from 125M to 175B parameters, which we aim to fully and responsibly share with interested researchers."
    },
    {
      "source": "[pdf_text]",
      "quote": "In this technical report, we introduced OPT, a collection of auto-regressive language models ranging in size from 125M to 175B parameters."
    },
    {
      "source": "[pdf_text]",
      "quote": "OPT-175B described in this paper is version 1.0.0."
    }
  ]
}