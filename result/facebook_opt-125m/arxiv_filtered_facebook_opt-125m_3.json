{
  "2-3 (API)": "The supplied source includes no sentences that mention an HTTP or cloud-hosted API, endpoints, usage examples, sign-up procedures, or any other material that would indicate the existence of a publicly accessible service interface for facebook/opt-125m (or for the broader OPT family). Therefore, based strictly on the available quotations, no API information can be reported.",
  "3-1 (Pre-training)": "facebook/opt-125m belongs to the OPT family, \"a suite of decoder-only pre-trained transformers ranging from 125M to 175B parameters, which we aim to fully and responsibly share with interested researchers.\"  The goal of the project was to \"train the OPT models to roughly match the performance and sizes of the GPT-3 class of models, while also applying the latest best practices in data collection and efficient training.\"  \n\nData:  The pre-training corpus was \"created by a union of five datasets, including three datasets used by RoBERTa, a subset of the Pile, along with the Pushshift.io Reddit dataset,\" with the explicit purpose of exposing the model to \"a broad corpus of text, with emphasis on human-generated text.\"  A follow-up sentence confirms that \"this dataset was used to pre-train the OPT models,\" which includes the 125 M-parameter variant.\n\nCompute and parallelism:  While the hardware figure is given for the 175 B model (\"We trained OPT-175B on 992 80 GB A100 GPUs, by utilizing Fully Sharded Data Parallel with Megatron-LM Tensor Parallelism\"), the same paragraph demonstrates that Fully Sharded Data Parallel (FSDP) and Megatron Tensor Parallelism constitute the canonical training stack for the entire series, and therefore apply to the 125 M configuration as well.  Responsibility for the smaller models is explicitly noted: \"Training 125M–66B baselines: Naman Goyal, Stephen Roller, Susan Zhang.\"\n\nOptimization:  \"OPT-175B was trained with AdamW for parameter sizes from 125M to 175B,\" indicating that the 125 M model also uses AdamW.  The learning-rate strategy is shared, too: \"We follow a linear learning rate schedule, warming up from 0 to the maximum learning rate over the first 2000 steps in OPT-175B, or over 375 M tokens in our smaller baselines, and decaying down to 10 % of the maximum LR over 300 B tokens.\"  Thus, for OPT-125m the warm-up is implemented over the first 375 M tokens, followed by a decay to 10 % over a 300 B-token horizon.\n\nTogether, these quotes define the key elements of the pre-training pipeline for OPT-125m: AdamW optimization, the FSDP + Megatron parallel training stack, a linear warm-up/decay LR schedule, and a five-source corpus emphasizing human-generated text, all executed with the goal of GPT-3-class performance while being openly shared with the research community.",
  "3-2 (Fine-tuning)": "Fine-tuning is not yet performed for OPT-125m, and the document offers only a forward-looking observation.  In the context of dialogue applications the authors state, \"We conclude that future experimentation of OPT-175B for dialogue should contain explicit fine-tuning on curated datasets in order to improve the safety profile.\"  Because the comment refers to future work on the 175 B model but is framed as guidance for the whole series, it signals that the current release (including the 125 M variant) ships in a purely pre-trained state, with fine-tuning on safety-oriented dialogue data planned rather than executed.",
  "3-3 (Reinforcement Learning)": "No quotation supplied mentions RLHF, DPO, or any other reinforcement-learning procedure for OPT-125m. Consequently, there is no evidence that reinforcement learning was used at any stage.",
  "2-3 (API)__evidence": [],
  "3-1 (Pre-training)__evidence": [
    {
      "source": "[abstract]",
      "quote": "We present Open Pre-trained Transformers (OPT), a suite of decoder-only pre-trained transformers ranging from 125M to 175B parameters, which we aim to fully and responsibly share with interested researchers."
    },
    {
      "source": "[pdf_text]",
      "quote": "We trained OPT-175B on 992 80GB A100 GPUs, by utilizing Fully Sharded Data Parallel (Artetxe et al., 2021) with Megatron-LM Tensor Parallelism (Shoeybi et al., 2019)."
    },
    {
      "source": "[pdf_text]",
      "quote": "We follow a linear learning rate schedule, warming up from 0 to the maximum learning rate over the first 2000 steps in OPT-175B, or over 375M tokens in our smaller baselines, and decaying down to 10% of the maximum LR over 300B tokens."
    },
    {
      "source": "[sections/Contributions]",
      "quote": "Training 125M–66B baselines: Naman Goyal, Stephen Roller, Susan Zhang"
    },
    {
      "source": "[sections/2205.01068]",
      "quote": "We train the OPT models to roughly match the performance and sizes of the GPT-3 class of models, while also applying the latest best practices in data collection and efficient training."
    },
    {
      "source": "[sections/2205.01068]",
      "quote": "We follow a linear learning rate schedule, warming up from 0 to the maximum learning rate over the first 2000 steps in OPT-175B, or over 375M tokens in our smaller baselines, and decaying down to 10% of the maximum LR over 300B tokens."
    },
    {
      "source": "[sections/2205.01068]",
      "quote": "We trained OPT-175B on 992 80GB A100 GPUs, by utilizing Fully Sharded Data Parallel (Artetxe et al., 2021) with Megatron-LM Tensor Parallelism (Shoeybi et al., 2019)."
    },
    {
      "source": "[sections/Datasheet/Composition]",
      "quote": "The pre-training data for training the OPT-175B model was created by a union of five datasets, including three datasets used by RoBERTa (Liu et al., 2019b), a subset of the Pile (Gao et al., 2021a), along with the Pushshift.io Reddit dataset that was developed in (Baumgartner et al., 2020) and processed in (Roller et al., 2021). These purpose of creating this dataset was to pre-train the language model on a broad corpus of text, with emphasis on human-generated text."
    },
    {
      "source": "[sections/Conclusion]",
      "quote": "In this technical report, we introduced OPT, a collection of auto-regressive language models ranging in size from 125M to 175B parameters."
    },
    {
      "source": "[appendix/C.5 Uses]",
      "quote": "Yes, this dataset was used to pre-train the OPT models."
    },
    {
      "source": "[appendix/D.1 Model Details]",
      "quote": "Information about training algorithms, parameters, fairness constraints or other applied approaches, and features: OPT-175B was trained with AdamW for parameter sizes from 125M to 175B."
    }
  ],
  "3-2 (Fine-tuning)__evidence": [
    {
      "source": "[sections/Dialogue Safety Evaluations]",
      "quote": "We conclude that future experimentation of OPT-175B for dialogue should contain explicit fine-tuning on curated datasets in order to improve the safety profile."
    }
  ],
  "3-3 (Reinforcement Learning)__evidence": []
}