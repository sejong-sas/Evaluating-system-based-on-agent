{
  "1-5 (Architecture)": "The quotes describe the Open Pre-trained Transformers (OPT) family – explicitly including the 125 M-parameter checkpoint – as a suite of decoder-only, auto-regressive language models that span a broad scale from 125 M to 175 B parameters. Within that line-up, the 125 M variant (facebook/opt-125m) is singled out in a tabular excerpt that lists the following hyper-parameter row for “125M”: 12 transformer layers, 12 attention heads, a model (hidden-state) width of 768, a peak learning-rate value of 6.0 × 10⁻⁴, and a training token count of roughly 0.5 B. The authors emphasize that every checkpoint between 125 M and 66 B is being openly released, with full research access to the 175 B model granted upon request. Overall, the architecture is positioned as a standard decoder-only transformer whose scale varies across the family, with OPT-125M representing the smallest publicly released configuration (12 L / 12 H / 768 d).",
  "1-6 (Tokenizer)": "",
  "2-1 (Hardware)": "For large-scale family training, the project employed 992 NVIDIA A100 GPUs equipped with 80 GB of memory each. During this run the team reported a sustained utilization of up to 147 TFLOP/s per GPU. Although the hardware quote is given for OPT-175B, it is presented as the shared infrastructure used when scaling the OPT series (which includes OPT-125M). Thus, the hardware profile for the broader OPT training effort consists of nearly one thousand 80 GB A100 accelerators operating in parallel.",
  "2-2 (Software)": "The training software stack for the OPT models combines several mature distributed-training components. Fully Sharded Data Parallel (FSDP) is employed together with Megatron-LM Tensor Parallelism, allowing the model’s parameters and optimizer state to be sharded across all hosts while still supporting intra-layer tensor parallel splits. The optimizer is AdamW, used consistently across the 125 M → 175 B parameter range. Implementation details note that Adam’s state is kept in FP32 precision, whereas model weights are stored in FP16; dynamic loss scaling (per Micikevicius et al., 2017) is activated to avoid FP16 underflow. The combination of FSDP sharding, Megatron tensor parallelism, mixed-precision weights, and dynamic loss scaling constitutes the core software recipe for training OPT-125M and its larger siblings.",
  "1-5 (Architecture)__evidence": [
    {
      "source": "[pdf_text]",
      "quote": "We present Open Pre-trained Transformers (OPT), a suite of decoder-only pre-trained transformers ranging from 125M to 175B parameters, which we aim to fully and responsibly share with interested researchers."
    },
    {
      "source": "[pdf_text]",
      "quote": "125M    12    12    768    6.0e−4    0.5M"
    },
    {
      "source": "[sections/https://arxiv.org/pdf/2205.01068.pdf]",
      "quote": "We are releasing all of our models between 125M and 66B parameters, and will provide full research access to OPT-175B upon request."
    },
    {
      "source": "[pdf_text]",
      "quote": "In this technical report, we introduced OPT, a collection of auto-regressive language models ranging in size from 125M to 175B parameters."
    }
  ],
  "1-6 (Tokenizer)__evidence": [],
  "2-1 (Hardware)__evidence": [
    {
      "source": "[pdf_text]",
      "quote": "We trained OPT-175B on 992 80GB A100 GPUs, by utilizing Fully Sharded Data Parallel (Artetxe et al., 2021) with Megatron-LM Tensor Parallelism (Shoeybi et al., 2019). We achieve utilization of up to 147 TFLOP/s per GPU."
    }
  ],
  "2-2 (Software)__evidence": [
    {
      "source": "[pdf_text]",
      "quote": "We trained OPT-175B on 992 80GB A100 GPUs, by utilizing Fully Sharded Data Parallel (Artetxe et al., 2021) with Megatron-LM Tensor Parallelism (Shoeybi et al., 2019). We keep Adam state in FP32, since we shard it across all hosts, while the model weights remained in FP16. To avoid underflows, we used dynamic loss scaling, as described in Micikevicius et al. (2017)."
    },
    {
      "source": "[pdf_text]",
      "quote": "Information about training algorithms, parameters, fairness constraints or other applied approaches, and features: OPT-175B was trained with AdamW for parameter sizes from 125M to 175B."
    }
  ]
}