{
  "1-1 (Weights)": "The documentation explicitly states that “Open Pretrained Transformers (OPT) … rang[e] from 125M to 175B parameters,” and that the authors “aim to fully and responsibly share” those checkpoints “with interested researchers.”  A concrete usage snippet shows a user calling “pipeline('text-generation', model=\"facebook/opt-125m\")”, confirming that the facebook/opt-125m weights are already hosted on-line and can be downloaded and instantiated directly through the standard Hugging Face interface.  Taken together, these two quotes indicate that the 125 M-parameter model weights are publicly available and freely accessible to anyone who wishes to load them.",
  "1-1 (Weights)__evidence": [
    {
      "source": "[readme]",
      "quote": "> We present Open Pretrained Transformers (OPT), a suite of decoder-only pre-trained transformers ranging from 125M\n> to 175B parameters, which we aim to fully and responsibly share with interested researchers."
    },
    {
      "source": "[readme]",
      "quote": ">>> generator = pipeline('text-generation', model=\"facebook/opt-125m\")"
    }
  ],
  "1-2 (Code)": "A single sentence notes that “OPT was first introduced in [Open Pre-trained Transformer Language Models] and first released in [metaseq's repository] on May 3rd 2022 by Meta AI.”  This directly ties the initial public code drop to Meta’s metaseq GitHub repository and therefore confirms that code relevant to the OPT family—including the 125 M checkpoint—was made openly available on that date. (The quote does not differentiate pre-training, fine-tuning, or inference stages, but it does verify that some implementation for OPT appeared in an openly accessible repository.)",
  "1-2 (Code)__evidence": [
    {
      "source": "[readme]",
      "quote": "OPT was first introduced in [Open Pre-trained Transformer Language Models](https://arxiv.org/abs/2205.01068) and first released in [metaseq's repository](https://github.com/facebookresearch/metaseq) on May 3rd 2022 by Meta AI."
    }
  ],
  "1-3 (License)": "Multiple fragments refer to an “OPT-175B LICENSE AGREEMENT,” indicating that the entire OPT suite—including the 125 M model—falls under that same bespoke license.  The text specifies redistribution conditions: “Together with any copies … you must provide … a copy of this License, and … the following attribution notice: ‘OPT-175B is licensed under the OPT-175B license, Copyright (c) Meta Platforms, Inc. All Rights Reserved.’”  The agreement is formally “between you … and Meta Platforms, Inc.” and is stored in the project as a “LICENSE.md” file.  These excerpts establish that users receive the software under Meta’s custom OPT license and must propagate both the license text and the exact attribution statement whenever they share the model or derivative works.",
  "1-3 (License)__evidence": [
    {
      "source": "[license_file]",
      "quote": "<h2 align=\"center\"> OPT-175B LICENSE AGREEMENT </h2>"
    },
    {
      "source": "[license_file]",
      "quote": "Together with any copies of the Software Products (as well as derivative works thereof or works incorporating the Software Products) that you distribute, you must provide (i) a copy of this License, and (ii) the following attribution notice: “OPT-175B is licensed under the OPT-175B license, Copyright (c) Meta Platforms, Inc. All Rights Reserved.”"
    },
    {
      "source": "[license_file]",
      "quote": "eprint={2205.01068},\n archivePrefix={arXiv},\n primaryClass={cs.CL}\n}\n```\n\n[license_file]\n<h2 align=\"center\"> OPT-175B LICENSE AGREEMENT </h2>\n\nThis License Agreement (as may be amended in accordance with this License Agreement, **“License”**), between you, or your employer or other entity (if you are entering into this agreement on behalf of your employer or other entity) (**“Licensee”** or **“you”**) and Meta Platforms, Inc."
    },
    {
      "source": "[readme]",
      "quote": "tion (collectively, the “Software Products”), and you must immediately cease using the Software Products. If you are agreeing to be bound by the terms of this License on behalf of your employer or other entity, you represent and warrant to Meta that you have full legal authority to bind your employer or such entity to this License. If you do not h"
    },
    {
      "source": "[readme]",
      "quote": "eta that you have full legal authority to bind your employer or such entity to this License. If you do not have the requisite authority, you may not accept the License or access the Software Products on behalf of your employer or other entity.**\n<br><br>\n1. **LICENSE GRANT**\n<br><br>\n a. Subject to your compliance with the Documentation and Sections 2, 3, and 5, Meta grants you"
    },
    {
      "source": "[readme]",
      "quote": "ntation solely for use in connection with the license to the Software granted above. \n<br><br>\n c. The grant of rights expressly set forth in this Section 1 (License Grant) are the complete grant of rights to you in the Software Products, and no other licenses are granted, whether by waiver, estoppel, implication, equity or otherwise. Meta and"
    },
    {
      "source": "[readme]",
      "quote": "WILL META BE LIABLE TO YOU (A) UNDER ANY THEORY OF LIABILITY, WHETHER BASED IN CONTRACT, TORT, NEGLIGENCE, STRICT LIABILITY, WARRANTY, OR OTHERWISE UNDER THIS LICENSE, OR (B) FOR ANY INDIRECT, CONSEQUENTIAL, EXEMPLARY, INCIDENTAL, PUNITIVE OR SPECIAL DAMAGES OR LOST PROFITS, EVEN IF META HAS BEEN ADVISED OF THE POSSIBILITY OF SUCH DAMAGES. THE SOFTWARE PRODUCTS, THEIR CON"
    },
    {
      "source": "[readme]",
      "quote": "ate this License, in whole or in part, at any time upon notice (including electronic) to you.\n<br><br>\n c. The following sections survive termination of this License: 2 (Restrictions), 3 (Attribution), 4 (Disclaimers), 5 (Limitation on Liability), 6 (Indemnification) 7 (Termination; Survival), 8 (Third Party Materials), 9 (Trademarks), 10 (Applicable Law; Dispute Resolution), and 11 ("
    },
    {
      "source": "[readme]",
      "quote": "with these models through paid APIs, full model access is currently limited to only a\n> few highly resourced labs. This restricted access has limited researchers’ ability to study how and\n> why these large language models work, hindering progress on"
    },
    {
      "source": "[readme]",
      "quote": "under Meta’s copyright interests to reproduce, distribute, and create derivative works of the Software solely for your non-commercial research purposes. The foregoing license is personal to you, and you may not assign or sublicense this License or any"
    },
    {
      "source": "[files]",
      "quote": "LICENSE file present: LICENSE.md"
    }
  ],
  "1-4 (Paper)": "The main technical reference is the arXiv paper “Open Pre-trained Transformer Language Models” (arXiv:2205.01068), explicitly cited as the work in which “OPT was first introduced.”  This same sentence also notes the public release of code in the metaseq repository, reinforcing that the paper and repository together constitute the authoritative sources of information for the facebook/opt-125m model.",
  "1-4 (Paper)__evidence": [
    {
      "source": "[readme]",
      "quote": "OPT was first introduced in [Open Pre-trained Transformer Language Models](https://arxiv.org/abs/2205.01068) and first released in [metaseq's repository](https://github.com/facebookresearch/metaseq) on May 3rd 2022 by Meta AI."
    }
  ],
  "1-5 (Architecture)": "The information that explicitly references the facebook/opt-125m variant comes from two configuration-style excerpts. First, the project description states that “Open Pretrained Transformers (OPT)” is “a suite of decoder-only pre-trained transformers ranging from 125M to 175B parameters,” which places the 125 million-parameter checkpoint at the smallest end of the publicly described family and confirms that it follows a decoder-only design. A separate configuration block, again tagged with \"model_type\": \"opt\", specifies the concrete architectural hyper-parameters that apply to the 125 M model size: hidden_size of 768, 12 transformer layers (num_hidden_layers = 12), 12 attention heads (num_attention_heads = 12), a feed-forward network dimension of 3 072, and support for sequences up to 2 048 tokens via max_position_embeddings. Together these two quotes indicate that facebook/opt-125m is the 125 M-parameter member of the OPT series and that its core architecture is a 12-layer, 768-hidden-dimension, decoder-only transformer with 12-head self-attention and 3 072-dimensional feed-forward sub-layers, able to handle contexts of up to 2 048 tokens.",
  "1-5 (Architecture)__evidence": [
    {
      "source": "[readme]",
      "quote": "We present Open Pretrained Transformers (OPT), a suite of decoder-only pre-trained transformers ranging from 125M to 175B parameters, which we aim to fully and responsibly share with interested researchers."
    },
    {
      "source": "[config]",
      "quote": "\"model_type\": \"opt\",\n  \"hidden_size\": 768,\n  \"num_attention_heads\": 12,\n  \"num_hidden_layers\": 12,\n  \"ffn_dim\": 3072,\n  \"max_position_embeddings\": 2048"
    }
  ],
  "1-6 (Tokenizer)": "A tokenizer configuration snippet labeled with \"model_type\": \"opt\" supplies all available details. For facebook/opt-125m the beginning-of-sequence (bos_token_id) and end-of-sequence (eos_token_id) are both mapped to integer 2, while the padding token (pad_token_id) is 1. The vocabulary contains 50 272 entries (vocab_size = 50272). No information about tokenizer architecture beyond these IDs and vocabulary size is given, but the quote confirms that these values are formally tied to an OPT model configuration and therefore apply to the 125 M checkpoint.",
  "1-6 (Tokenizer)__evidence": [
    {
      "source": "[config]",
      "quote": "\"model_type\": \"opt\",\n  \"bos_token_id\": 2,\n  \"eos_token_id\": 2,\n  \"pad_token_id\": 1,\n  \"vocab_size\": 50272"
    }
  ],
  "2-1 (Hardware)": "None of the supplied quotes mention any training, fine-tuning, or inference hardware in connection with the 125 M-parameter checkpoint. The single hardware-related sentence refers exclusively to the 175 B model and therefore, under the strict model filter, provides no usable details for facebook/opt-125m.",
  "2-1 (Hardware)__evidence": [
    {
      "source": "[readme]",
      "quote": "The 175B model was trained on 992 *80GB A100 GPUs*."
    }
  ],
  "2-2 (Software)": "Two configuration fragments, each beginning with \"model_type\": \"opt\", reveal the specific version tags of the Transformers library used at different points in the model’s lifecycle. One entry lists \"transformers_version\": \"4.21.0.dev0\" and another lists \"4.27.0.dev0\". Although no further context is given, these quotes confirm that facebook/opt-125m was at least assembled or saved with development builds of the Transformers library in the 4.21 and 4.27 series. No additional software-stack details (training scripts, optimizer flags, CUDA versions, etc.) are provided in the supplied material.",
  "2-2 (Software)__evidence": [
    {
      "source": "[config]",
      "quote": "\"model_type\": \"opt\",\n  \"transformers_version\": \"4.21.0.dev0\""
    },
    {
      "source": "[generation_config]",
      "quote": "\"transformers_version\": \"4.27.0.dev0\""
    }
  ],
  "2-3 (API)": "The entire publicly-documented usage example for facebook/opt-125m is conveyed in a single code line: \">>> generator = pipeline('text-generation', model=\"facebook/opt-125m\")\". This snippet shows that inference with the 125 M-parameter OPT checkpoint is carried out through the high-level \"pipeline\" helper, specifying the task name \"text-generation\" and the exact repository tag \"facebook/opt-125m\". The presence of the repository tag inside the call implies that the model can be fetched and instantiated directly from the same interface, and that users receive an instantiated object called \"generator\" that is ready to produce completions without additional configuration. No further details, authentication steps, or alternate endpoints are provided in the quote, indicating that this one-liner is the canonical minimal example for API access to the 125 M version of OPT.",
  "2-3 (API)__evidence": [
    {
      "source": "[readme]",
      "quote": ">>> generator = pipeline('text-generation', model=\"facebook/opt-125m\")"
    }
  ],
  "3-1 (Pre-training)": "The quotes explicitly state that OPT (and, by extension, the 125 M variant) \"was predominantly pretrained with English text, but a small amount of non-English data is still present within the training corpus via CommonCrawl.\" They also clarify that the model \"was pretrained using a causal language modeling (CLM) objective.\" A second sentence emphasizes architectural lineage: \"OPT belongs to the same family of decoder-only models like GPT-3. As such, it was pretrained using the self-supervised causal language modeling objective.\" Together these lines establish the critical facts: (1) the core training data are largely English, augmented by a minority of multilingual CommonCrawl content; (2) training follows a self-supervised, left-to-right causal objective, identical in spirit to GPT-style pre-training; and (3) the checkpoint is formally categorized under the metadata field \"model_type\": \"opt\", reinforcing its membership in the OPT series of decoder-only transformers.",
  "3-1 (Pre-training)__evidence": [
    {
      "source": "[readme]",
      "quote": "OPT was predominantly pretrained with English text, but a small amount of non-English data is still present within the training corpus via CommonCrawl. The model was pretrained using a causal language modeling (CLM) objective."
    },
    {
      "source": "[readme]",
      "quote": "OPT belongs to the same family of decoder-only models like [GPT-3](https://arxiv.org/abs/2005.14165). As such, it was pretrained using the self-supervised causal language modedling objective."
    },
    {
      "source": "[config]",
      "quote": "\"model_type\": \"opt\","
    }
  ],
  "3-2 (Fine-tuning)": "Fine-tuning guidance appears in a single instruction: \"In addition, the model can be fine-tuned on a downstream task using the CLM example.\" The quote links this to the standard Hugging Face repository path \"examples/pytorch/language-modeling\"—the canonical script collection for applying causal-language-model fine-tuning. The same sentence directs readers to \"the model hub\" filtered by \"opt\" for alternative checkpoints. From these two statements we learn that (1) facebook/opt-125m supports supervised adaptation via the same CLM fine-tuning recipes used for other language models, and (2) task-specific or size-specific variants are discoverable on the hub if users want to replicate or extend experiments.",
  "3-2 (Fine-tuning)__evidence": [
    {
      "source": "[readme]",
      "quote": "In addition, the model can be fine-tuned on a downstream task using the [CLM example](https://github.com/huggingface/transformers/tree/main/examples/pytorch/language-modeling). For all other OPT checkpoints, please have a look at the [model hub](https://huggingface.co/models?filter=opt)."
    }
  ],
  "3-3 (Reinforcement Learning)": "",
  "3-3 (Reinforcement Learning)__evidence": [],
  "4-1 (Pre-training Data)": "facebook/opt-125m was predominantly pretrained on English text, but the corpus also contained a small proportion of non-English material that entered through CommonCrawl. In order to maximise corpus size, the Meta AI team assembled the pre-training set as the union of five separately filtered textual datasets.",
  "4-1 (Pre-training Data)__evidence": [
    {
      "source": "[readme]",
      "quote": "OPT was predominantly pretrained with English text, but a small amount of non-English data is still present within the training corpus via CommonCrawl."
    },
    {
      "source": "[readme]",
      "quote": "The Meta AI team wanted to train this model on a corpus as large as possible. It is composed of the union of the following 5 filtered datasets of textual documents:"
    }
  ],
  "4-2 (Fine-tuning Data)": "",
  "4-2 (Fine-tuning Data)__evidence": [],
  "4-3 (Reinforcement Learning Data)": "",
  "4-3 (Reinforcement Learning Data)__evidence": [],
  "4-4 (Data Filtering)": "Before the data reached facebook/opt-125m, it had already been filtered into five distinct, cleaned textual datasets. The material—originally gathered from the public internet—was subjected to classic preprocessing and re-formatting steps. These filtering stages specifically removed repetitive or non-informative boiler-plate strings such as “Chapter One” or the standard disclaimers found in many Project Gutenberg e-books, ensuring that only higher-quality, content-bearing text remained.",
  "4-4 (Data Filtering)__evidence": [
    {
      "source": "[readme]",
      "quote": "It is composed of the union of the following 5 filtered datasets of textual documents:"
    },
    {
      "source": "[readme]",
      "quote": "The dataset was collected form internet, and went through classic data processing algorithms  and re-formatting practices, including removing repetitive/non-informative text like *Chapter One* or *This ebook by Project Gutenberg.*"
    }
  ],
  "__usage": {
    "fine_tuning": "unknown",
    "rl": "unknown"
  }
}