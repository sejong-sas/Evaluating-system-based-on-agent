{
    "repo": "facebookresearch/metaseq",
    "branch": "main",
    "files": [
        ".circleci/config.yml",
        ".flake8",
        ".github/ISSUE_TEMPLATE.md",
        ".github/ISSUE_TEMPLATE/bug_report.md",
        ".github/ISSUE_TEMPLATE/documentation.md",
        ".github/ISSUE_TEMPLATE/feature_request.md",
        ".github/ISSUE_TEMPLATE/how-to-question.md",
        ".github/PULL_REQUEST_TEMPLATE.md",
        ".github/stale.yml",
        ".github/workflows/lint.yml",
        ".gitignore",
        ".gitmodules",
        ".pre-commit-config.yaml",
        "CHANGELOG.md",
        "CODEOWNERS",
        "CODE_OF_CONDUCT.md",
        "Dockerfile",
        "LICENSE",
        "README.md",
        "cpu_tests/__init__.py",
        "cpu_tests/distributed/__init__.py",
        "cpu_tests/distributed/test_module_proxy_wrapper.py",
        "cpu_tests/test_cm3_dataset.py",
        "cpu_tests/test_data_utils.py",
        "cpu_tests/test_dictionary.py",
        "cpu_tests/test_file_chunker_utils.py",
        "cpu_tests/test_jsonl_dataset.py",
        "cpu_tests/test_metrics.py",
        "cpu_tests/test_partitioned_streaming_dataset.py",
        "cpu_tests/test_streaming_iterators.py",
        "cpu_tests/test_streaming_language_modeling_task.py",
        "cpu_tests/test_streaming_shuffle_dataset.py",
        "cpu_tests/test_streaming_token_block_dataset.py",
        "cpu_tests/test_token_block_dataset.py",
        "cpu_tests/test_utils.py",
        "docs/CONTRIBUTING.md",
        "docs/api.md",
        "docs/faster-transformer.md",
        "docs/history.md",
        "docs/images/opt-30b-175b.png",
        "docs/setup.md",
        "docs/training.md",
        "gpu_tests/test_activation_checkpointing.py",
        "gpu_tests/test_checkpoint_loading_on_more_gpus.py",
        "gpu_tests/test_checkpoint_saving.py",
        "gpu_tests/test_checkpoint_saving_async.py",
        "gpu_tests/test_cli_train.py",
        "gpu_tests/test_fp16_optimizer.py",
        "gpu_tests/test_hf_compatibility.py",
        "gpu_tests/test_hub_utils.py",
        "gpu_tests/test_hub_utils/test_filter_special_distributions.yml",
        "gpu_tests/test_hub_utils/test_filter_special_new_scores.yml",
        "gpu_tests/test_hub_utils/test_filter_special_new_tokens.yml",
        "gpu_tests/test_hub_utils/test_generator_interface.csv",
        "gpu_tests/test_hub_utils/test_generator_interface.yml",
        "gpu_tests/test_model_parallel_mp1_mp2.py",
        "gpu_tests/utils.py",
        "metaseq/__init__.py",
        "metaseq/benchmark/__init__.py",
        "metaseq/benchmark/dummy_lm.py",
        "metaseq/benchmark/generator.py",
        "metaseq/checkpoint_utils.py",
        "metaseq/cli/README.md",
        "metaseq/cli/__init__.py",
        "metaseq/cli/interactive_cli.py",
        "metaseq/cli/interactive_ft.py",
        "metaseq/cli/interactive_hosted.py",
        "metaseq/cli/train.py",
        "metaseq/cli/validate.py",
        "metaseq/config/config.yaml",
        "metaseq/criterions/__init__.py",
        "metaseq/criterions/base_criterion.py",
        "metaseq/criterions/cross_entropy.py",
        "metaseq/criterions/vocab_parallel_cross_entropy.py",
        "metaseq/data/__init__.py",
        "metaseq/data/append_token_dataset.py",
        "metaseq/data/base_dataset.py",
        "metaseq/data/base_wrapper_dataset.py",
        "metaseq/data/cm3_dataset.py",
        "metaseq/data/concat_dataset.py",
        "metaseq/data/data_utils.py",
        "metaseq/data/data_utils_fast.pyx",
        "metaseq/data/dictionary.py",
        "metaseq/data/document_to_sequence.py",
        "metaseq/data/encoders/__init__.py",
        "metaseq/data/encoders/gpt2_bpe.py",
        "metaseq/data/encoders/gpt2_bpe_utils.py",
        "metaseq/data/encoders/hf_byte_bpe.py",
        "metaseq/data/id_dataset.py",
        "metaseq/data/indexed_dataset.py",
        "metaseq/data/iterators.py",
        "metaseq/data/jsonl_dataset.py",
        "metaseq/data/list_dataset.py",
        "metaseq/data/lm_context_window_dataset.py",
        "metaseq/data/monolingual_dataset.py",
        "metaseq/data/nested_dictionary_dataset.py",
        "metaseq/data/numel_dataset.py",
        "metaseq/data/pad_dataset.py",
        "metaseq/data/partitioned_streaming_dataset.py",
        "metaseq/data/plasma_utils.py",
        "metaseq/data/prepend_token_dataset.py",
        "metaseq/data/resampling_dataset.py",
        "metaseq/data/shorten_dataset.py",
        "metaseq/data/sort_dataset.py",
        "metaseq/data/streaming_shuffle_dataset.py",
        "metaseq/data/streaming_src_tgt_dataset.py",
        "metaseq/data/streaming_token_block_dataset.py",
        "metaseq/data/strip_token_dataset.py",
        "metaseq/data/token_block_dataset.py",
        "metaseq/data/token_block_utils_fast.pyx",
        "metaseq/dataclass/__init__.py",
        "metaseq/dataclass/configs.py",
        "metaseq/dataclass/constants.py",
        "metaseq/dataclass/initialize.py",
        "metaseq/dataclass/utils.py",
        "metaseq/distributed/__init__.py",
        "metaseq/distributed/fully_sharded_data_parallel.py",
        "metaseq/distributed/module_proxy_wrapper.py",
        "metaseq/distributed/rendezvous.py",
        "metaseq/distributed/stitch_fsdp_ckpt.py",
        "metaseq/distributed/utils.py",
        "metaseq/file_chunker_utils.py",
        "metaseq/file_io/__init__.py",
        "metaseq/file_io/azure_blob.py",
        "metaseq/file_io/common/__init__.py",
        "metaseq/file_io/common/non_blocking_io.py",
        "metaseq/file_io/s3.py",
        "metaseq/file_utils.py",
        "metaseq/hub_utils.py",
        "metaseq/incremental_decoding_utils.py",
        "metaseq/launcher/__init__.py",
        "metaseq/launcher/opt_baselines.py",
        "metaseq/launcher/opt_job_constants.py",
        "metaseq/launcher/slurm.py",
        "metaseq/launcher/sweep.py",
        "metaseq/launcher/tombyard.py",
        "metaseq/logging/__init__.py",
        "metaseq/logging/meters.py",
        "metaseq/logging/metrics.py",
        "metaseq/logging/progress_bar/__init__.py",
        "metaseq/logging/progress_bar/aim_progress_bar.py",
        "metaseq/logging/progress_bar/base_progress_bar.py",
        "metaseq/logging/progress_bar/json_progress_bar.py",
        "metaseq/logging/progress_bar/tensorboard_progress_bar.py",
        "metaseq/logging/progress_bar/wandb_progress_bar.py",
        "metaseq/models/__init__.py",
        "metaseq/models/base_decoder.py",
        "metaseq/models/base_model.py",
        "metaseq/models/distributed_model.py",
        "metaseq/models/ema/__init__.py",
        "metaseq/models/ema/ema.py",
        "metaseq/models/transformer_decoder.py",
        "metaseq/models/transformer_lm.py",
        "metaseq/modules/__init__.py",
        "metaseq/modules/activation_functions.py",
        "metaseq/modules/adaptive_softmax.py",
        "metaseq/modules/apex/amp_C_frontend.cpp",
        "metaseq/modules/apex/compat.h",
        "metaseq/modules/apex/flatten_unflatten.cpp",
        "metaseq/modules/apex/fused_adam_cuda.cpp",
        "metaseq/modules/apex/fused_adam_cuda_kernel.cu",
        "metaseq/modules/apex/fused_dense.cpp",
        "metaseq/modules/apex/fused_dense_cuda.cu",
        "metaseq/modules/apex/layer_norm_cuda.cpp",
        "metaseq/modules/apex/layer_norm_cuda_kernel.cu",
        "metaseq/modules/apex/multi_tensor_adam.cu",
        "metaseq/modules/apex/multi_tensor_apply.cuh",
        "metaseq/modules/apex/multi_tensor_l2norm_kernel.cu",
        "metaseq/modules/apex/multi_tensor_l2norm_kernel_mp.cu",
        "metaseq/modules/apex/multi_tensor_l2norm_scale_kernel.cu",
        "metaseq/modules/apex/type_shim.h",
        "metaseq/modules/checkpoint_activation_wrapper/__init__.py",
        "metaseq/modules/checkpoint_activation_wrapper/checkpoint_activations.py",
        "metaseq/modules/checkpoint_activations.py",
        "metaseq/modules/dropout.py",
        "metaseq/modules/embedding.py",
        "metaseq/modules/feedforward.py",
        "metaseq/modules/group_norm_fp32.py",
        "metaseq/modules/layer_norm.py",
        "metaseq/modules/learned_positional_embedding.py",
        "metaseq/modules/linear.py",
        "metaseq/modules/megatron/__init__.py",
        "metaseq/modules/megatron/fused_kernels/compat.h",
        "metaseq/modules/megatron/fused_kernels/scaled_masked_softmax.cpp",
        "metaseq/modules/megatron/fused_kernels/scaled_masked_softmax.h",
        "metaseq/modules/megatron/fused_kernels/scaled_masked_softmax_cuda.cu",
        "metaseq/modules/megatron/fused_kernels/scaled_upper_triang_masked_softmax.cpp",
        "metaseq/modules/megatron/fused_kernels/scaled_upper_triang_masked_softmax.h",
        "metaseq/modules/megatron/fused_kernels/scaled_upper_triang_masked_softmax_cuda.cu",
        "metaseq/modules/megatron/fused_kernels/type_shim.h",
        "metaseq/modules/megatron/global_vars.py",
        "metaseq/modules/megatron/model/__init__.py",
        "metaseq/modules/megatron/model/fused_softmax.py",
        "metaseq/modules/megatron/mpu/__init__.py",
        "metaseq/modules/megatron/mpu/cross_entropy.py",
        "metaseq/modules/megatron/mpu/initialize.py",
        "metaseq/modules/megatron/mpu/layers.py",
        "metaseq/modules/megatron/mpu/mappings.py",
        "metaseq/modules/megatron/mpu/random.py",
        "metaseq/modules/megatron/mpu/utils.py",
        "metaseq/modules/multihead_attention.py",
        "metaseq/modules/positional_embedding.py",
        "metaseq/modules/quant_noise.py",
        "metaseq/modules/sequence_parallel_transformer_layer.py",
        "metaseq/modules/sinusoidal_positional_embedding.py",
        "metaseq/modules/transformer_decoder_layer.py",
        "metaseq/nan_detector.py",
        "metaseq/optim/__init__.py",
        "metaseq/optim/adam.py",
        "metaseq/optim/base_optimizer.py",
        "metaseq/optim/dynamic_loss_scaler.py",
        "metaseq/optim/fp16_optimizer.py",
        "metaseq/optim/fused_adam.py",
        "metaseq/optim/lr_scheduler/__init__.py",
        "metaseq/optim/lr_scheduler/base_lr_scheduler.py",
        "metaseq/optim/lr_scheduler/cosine_lr_scheduler.py",
        "metaseq/optim/lr_scheduler/inverse_square_root_schedule.py",
        "metaseq/optim/lr_scheduler/polynomial_decay_schedule.py",
        "metaseq/optim/sgd.py",
        "metaseq/optim/shard.py",
        "metaseq/options.py",
        "metaseq/pdb.py",
        "metaseq/registry.py",
        "metaseq/scripts/__init__.py",
        "metaseq/scripts/consolidate_fsdp_shards.py",
        "metaseq/scripts/convert_metaseq_ft.py",
        "metaseq/scripts/convert_to_singleton.py",
        "metaseq/scripts/download_opt175b.sh",
        "metaseq/scripts/generation_benchmarks.py",
        "metaseq/scripts/reshard_consolidated.py",
        "metaseq/scripts/reshard_fsdp.py",
        "metaseq/scripts/reshard_mp.py",
        "metaseq/sequence_generator.py",
        "metaseq/service/__init__.py",
        "metaseq/service/constants.py",
        "metaseq/service/index.html",
        "metaseq/service/queue.py",
        "metaseq/service/responses.py",
        "metaseq/service/utils.py",
        "metaseq/service/workers.py",
        "metaseq/tasks/__init__.py",
        "metaseq/tasks/base_task.py",
        "metaseq/tasks/language_modeling.py",
        "metaseq/tasks/streaming_finetune_language_modeling.py",
        "metaseq/tasks/streaming_language_modeling.py",
        "metaseq/trainer.py",
        "metaseq/utils.py",
        "metaseq/version.txt",
        "mypy.ini",
        "preprocessing/README.md",
        "preprocessing/books3/README.md",
        "preprocessing/books3/build.sbt",
        "preprocessing/books3/project/ProjectDependencies.scala",
        "preprocessing/books3/project/build.properties",
        "preprocessing/books3/src/main/scala/BooksDedup.scala",
        "preprocessing/books3/src/main/scala/BooksFooterCleaner.scala",
        "preprocessing/books3/src/main/scala/BooksHeaderCleaner.scala",
        "projects/OPT-IML/README.md",
        "projects/OPT-IML/download_optiml175b.md",
        "projects/OPT-IML/optiml_paper_v1.pdf",
        "projects/OPT/MODEL_LICENSE.md",
        "projects/OPT/README.md",
        "projects/OPT/assets/gpt2-merges.txt",
        "projects/OPT/assets/gpt2-vocab.json",
        "projects/OPT/assets/opt175b_md5sum_shards.csv",
        "projects/OPT/assets/opt66b_md5sum_shards.csv",
        "projects/OPT/chronicles/10_percent_update.md",
        "projects/OPT/chronicles/27_percent_update.md",
        "projects/OPT/chronicles/56_percent_update.md",
        "projects/OPT/chronicles/OPT175B_Logbook.pdf",
        "projects/OPT/chronicles/OPT_Baselines_Logbook.pdf",
        "projects/OPT/chronicles/README.md",
        "projects/OPT/chronicles/final_update.md",
        "projects/OPT/chronicles/images/gpt3_comparison_100_perc.jpeg",
        "projects/OPT/chronicles/images/gpt3_comparison_10_perc.jpeg",
        "projects/OPT/chronicles/images/gpt3_comparison_56_perc.jpeg",
        "projects/OPT/chronicles/images/initial_eval_results.jpeg",
        "projects/OPT/chronicles/images/logbook_screenshot.jpeg",
        "projects/OPT/chronicles/images/run11.jpeg",
        "projects/OPT/chronicles/images/run11_zoomed.jpeg",
        "projects/OPT/chronicles/images/run12.jpeg",
        "projects/OPT/chronicles/images/run12_27_perc.jpeg",
        "projects/OPT/chronicles/images/run12_56_perc.jpeg",
        "projects/OPT/chronicles/images/run12_instability.jpeg",
        "projects/OPT/chronicles/images/run12_instability2.jpeg",
        "projects/OPT/chronicles/images/run12_upgrade_shift.jpeg",
        "projects/OPT/chronicles/images/run12_zoomed.jpeg",
        "projects/OPT/data_card.md",
        "projects/OPT/download_opt175b.md",
        "projects/OPT/model_card.md",
        "setup.py",
        "tests/__init__.py",
        "tests/distributed/__init__.py",
        "tests/distributed/test_utils.py",
        "tests/distributed/utils.py",
        "tests/file_io/async_download_test.py",
        "tests/file_io/async_torch_test.py",
        "tests/file_io/async_writes_test.py",
        "tests/file_io/test_azure_blob.py",
        "tests/file_io/test_native.py",
        "tests/file_io/test_non_blocking_io.py",
        "tests/file_io/test_s3.py",
        "tests/test_export.py",
        "tests/test_lm_context_window.py",
        "tests/test_memory_efficient_fp16.py",
        "tests/test_polynomial_lr.py",
        "tests/test_resampling_dataset.py",
        "tests/test_sequence_generator.py",
        "tests/test_streaming_language_modeling_task.py",
        "tests/utils.py"
    ],
    "license_files": {
        "LICENSE": "MIT License\n\nCopyright (c) Meta Platforms, Inc. and affiliates. All Rights Reserved.\n\nPermission is hereby granted, free of charge, to any person obtaining a copy\nof this software and associated documentation files (the \"Software\"), to deal\nin the Software without restriction, including without limitation the rights\nto use, copy, modify, merge, publish, distribute, sublicense, and/or sell\ncopies of the Software, and to permit persons to whom the Software is\nfurnished to do so, subject to the following conditions:\n\nThe above copyright notice and this permission notice shall be included in all\ncopies or substantial portions of the Software.\n\nTHE SOFTWARE IS PROVIDED \"AS IS\", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR\nIMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,\nFITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE\nAUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER\nLIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,\nOUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE\nSOFTWARE.\n"
    },
    "readme": "\n\n# Metaseq\nA codebase for working with [Open Pre-trained Transformers](projects/OPT), originally forked from [fairseq](https://github.com/facebookresearch/fairseq).\n\n\n## Community Integrations\n\n### Using OPT with 🤗 Transformers\n\nThe OPT 125M--66B models are now available in [Hugging Face Transformers](https://github.com/huggingface/transformers/releases/tag/v4.19.0). You can access them under the `facebook` organization on the [Hugging Face Hub](https://huggingface.co/facebook)\n\n### Using OPT-175B with Alpa\n\nThe OPT 125M--175B models are now supported in the [Alpa project](https://alpa-projects.github.io/tutorials/opt_serving.html), which \nenables serving OPT-175B with more flexible parallelisms on older generations of GPUs, such as 40GB A100, V100, T4, M60, etc.\n\n### Using OPT with Colossal-AI\n\nThe OPT models are now supported in the [Colossal-AI](https://github.com/hpcaitech/ColossalAI#OPT), which helps users to efficiently and quickly deploy OPT models training and inference, reducing large AI model budgets and scaling down the labor cost of learning and deployment.\n\n### Using OPT with CTranslate2\n\nThe OPT 125M--66B models can be executed with [CTranslate2](https://github.com/OpenNMT/CTranslate2/), which is a fast inference engine for Transformer models. The project integrates the [SmoothQuant](https://github.com/mit-han-lab/smoothquant) technique to allow 8-bit quantization of OPT models. See the [usage example](https://opennmt.net/CTranslate2/guides/transformers.html#opt) to get started.\n\n### Using OPT with FasterTransformer\n\nThe OPT models can be served with [FasterTransformer](https://github.com/NVIDIA/FasterTransformer), a highly optimized inference framework written and maintained by NVIDIA. We provide instructions to convert OPT checkpoints into FasterTransformer format and [a usage example](docs/faster-transformer.md) with some benchmark results.\n\n### Using OPT with DeepSpeed\n\nThe OPT models can be finetuned using [DeepSpeed](https://github.com/microsoft/DeepSpeed). See the [DeepSpeed-Chat example](https://github.com/microsoft/DeepSpeedExamples/tree/master/applications/DeepSpeed-Chat) to get started.\n\n## Getting Started in Metaseq\nFollow [setup instructions here](docs/setup.md) to get started.\n\n### Documentation on workflows\n* [Training](docs/training.md)\n* [API](docs/api.md)\n\n### Background Info\n* [Background & relationship to fairseq](docs/history.md)\n* [Chronicles of training OPT-175B](projects/OPT/chronicles/README.md)\n\n## Support\nIf you have any questions, bug reports, or feature requests regarding either the codebase or the models released in the projects section, please don't hesitate to post on our [Github Issues page](https://github.com/facebookresearch/metaseq/issues).\n\nPlease remember to follow our [Code of Conduct](CODE_OF_CONDUCT.md).\n\n## Contributing\nWe welcome PRs from the community!\n\nYou can find information about contributing to metaseq in our [Contributing](docs/CONTRIBUTING.md) document.\n\n## The Team\nMetaseq is currently maintained by the CODEOWNERS: [Susan Zhang](https://github.com/suchenzang), [Naman Goyal](https://github.com/ngoyal2707), [Punit Singh Koura](https://github.com/punitkoura), [Moya Chen](https://github.com/moyapchen), [Kurt Shuster](https://github.com/klshuster), [David Esiobu](https://github.com/davides), [Igor Molybog](https://github.com/igormolybogFB), [Peter Albert](https://github.com/Xirider), [Andrew Poulton](https://github.com/andrewPoulton), [Nikolay Bashlykov](https://github.com/bashnick), [Binh Tang](https://github.com/tangbinh), [Uriel Singer](https://github.com/urielsinger), [Yuchen Zhang](https://github.com/zycalice), [Armen Aghajanya](https://github.com/ArmenAg), [Lili Yu](https://github.com/lilisierrayu), and [Adam Polyak](https://github.com/adampolyak).\n\n## License\n\nThe majority of metaseq is licensed under the MIT license, however portions of the project are available under separate license terms: \n* Megatron-LM is licensed under the [Megatron-LM license](https://github.com/NVIDIA/Megatron-LM/blob/main/LICENSE)\n\n",
    "py_files": {
        "cpu_tests/__init__.py": "",
        "cpu_tests/distributed/__init__.py": "",
        "cpu_tests/distributed/test_module_proxy_wrapper.py": "# Copyright (c) Meta Platforms, Inc. and affiliates. All Rights Reserved.\n#\n# This source code is licensed under the MIT license found in the\n# LICENSE file in the root directory of this source tree.\n\nimport unittest\n\nimport torch\nfrom torch import nn\n\nfrom metaseq.distributed import ModuleProxyWrapper\n\nfrom tests.distributed.utils import objects_are_equal\n\n\nclass MockDDPWrapper(nn.Module):\n    \"\"\"A simple wrapper with an interface similar to DistributedDataParallel.\"\"\"\n\n    def __init__(self, module):\n        super().__init__()\n        self.module = module\n\n    def forward(self, x):\n        return self.module(x)\n\n\nclass Model(nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.linear = nn.Linear(5, 10)\n        self.xyz = \"hello\"\n\n    def forward(self, x):\n        return self.linear(x)\n\n    def get_xyz(self):\n        return self.xyz\n\n\nclass TestModuleProxyWrapper(unittest.TestCase):\n    def _get_module(self):\n        module = Model()\n        wrapped_module = MockDDPWrapper(module)\n        wrapped_module = ModuleProxyWrapper(wrapped_module)\n        return wrapped_module, module\n\n    def test_getattr_forwarding(self):\n        wrapped_module, module = self._get_module()\n        assert module.xyz == \"hello\"\n        assert module.get_xyz() == \"hello\"\n        assert wrapped_module.xyz == \"hello\"\n\n        wrapped_module.xyz = \"world\"\n        assert wrapped_module.xyz == \"world\"\n        assert module.get_xyz() == \"hello\"\n\n    def test_state_dict(self):\n        wrapped_module, module = self._get_module()\n        assert objects_are_equal(wrapped_module.state_dict(), module.state_dict())\n\n    def test_load_state_dict(self):\n        wrapped_module, module = self._get_module()\n        wrapped_module.load_state_dict(module.state_dict())\n        input = torch.rand(4, 5)\n        torch.testing.assert_allclose(wrapped_module(input), module(input))\n\n    def test_forward(self):\n        wrapped_module, module = self._get_module()\n        input = torch.rand(4, 5)\n        torch.testing.assert_allclose(wrapped_module(input), module(input))\n\n\nif __name__ == \"__main__\":\n    unittest.main()\n",
        "cpu_tests/test_cm3_dataset.py": "# Copyright (c) Meta Platforms, Inc. and affiliates. All Rights Reserved.\n#\n# This source code is licensed under the MIT license found in the\n# LICENSE file in the root directory of this source tree.\n\nfrom typing import List\nfrom metaseq.data.cm3_dataset import CausalMaskedDocumentToSequenceDataset, adjust_spans\n\nimport torch\nimport unittest\nimport random\nimport numpy as np\n\n\nclass TensorListDataset(torch.utils.data.Dataset):\n    def __init__(self, tensor_list):\n        self.tensor_list = tensor_list\n        self.queried = 0\n\n    def __len__(self):\n        return len(self.tensor_list)\n\n    def __getitem__(self, idx):\n        self.queried += 1\n        return self.tensor_list[idx]\n\n\ndef get_simple_dataset(\n    sentinel_token_expectation: int = 1,\n    sentinel_tokens: List[int] = [10],\n    sentinel_method: str = \"fixed\",\n    sentinel_eos: int = 1,\n    allow_rotation_across_eod: bool = True,\n    eod: int = 0,\n):\n    dataset = TensorListDataset(\n        [\n            torch.LongTensor([2, 2, 2, eod]),\n            torch.LongTensor([3, 3, 3, eod]),\n            torch.LongTensor([4, 4, 4, eod]),\n            torch.LongTensor([5, 5, 5, eod]),\n        ]\n    )\n    dataset = CausalMaskedDocumentToSequenceDataset(\n        sentinel_token_expectation,\n        sentinel_tokens,\n        sentinel_method,\n        sentinel_eos,\n        allow_rotation_across_eod,\n        eod,\n        dataset,\n        block_size=16,\n        permute_documents=False,\n        break_mode=\"none\",\n        padding_idx=1,\n    )\n    dataset.set_epoch(0)\n    return dataset\n\n\nclass TestDocumentBoundaryMethods(unittest.TestCase):\n    def test_get_document_boundaries(self):\n        eod = 0\n        causal_masked_dataset = get_simple_dataset(eod=0)\n\n        item = torch.LongTensor([1, 1, eod, 1, 1, 1, eod, 1])\n        assert causal_masked_dataset.get_document_boundaries(item) == [\n            (0, 2),\n            (2, 6),\n            (6, 8),\n        ]\n\n        item = torch.LongTensor([eod, 1, 1, eod, 1, 1, 1, eod, 1, eod])\n        assert causal_masked_dataset.get_document_boundaries(item) == [\n            (0, 3),\n            (3, 7),\n            (7, 9),\n        ]\n\n\nclass TestAdjustSpans(unittest.TestCase):\n    def test_no_overlap(self):\n        spans_1 = [(1, 3), (5, 7), (9, 11)]\n        constraints = [(0, 1), (1, 4), (4, 5), (5, 8), (8, 9), (9, 11)]\n        self.assertEqual(adjust_spans(spans_1, constraints), spans_1)\n\n    def test_overlap_within(self):\n        spans_1 = [(1, 4), (5, 7), (9, 11)]\n        spans_2 = [(2, 3), (3, 4), (4, 5), (5, 8), (8, 10), (10, 11)]\n        self.assertEqual(adjust_spans(spans_1, spans_2), [(2, 3), (5, 7), (9, 10)])\n\n    def test_overlap_outside(self):\n        spans_1 = [(1, 4), (5, 7), (9, 11)]\n        spans_2 = [(0, 3), (3, 5), (5, 7), (7, 8), (8, 10), (10, 12)]\n        self.assertEqual(adjust_spans(spans_1, spans_2), [(1, 3), (5, 7), (9, 10)])\n\n    def test_overlap_edge(self):\n        spans_1 = [(1, 4), (5, 7), (9, 12)]\n        spans_2 = [(0, 3), (3, 5), (5, 7), (7, 9), (9, 10), (10, 12)]\n        self.assertEqual(adjust_spans(spans_1, spans_2), [(1, 3), (5, 7), (10, 12)])\n\n    def test_no_similar_span(self):\n        spans_1 = [(1, 3), (5, 7), (9, 11)]\n        spans_2 = [(0, 1), (4, 5), (12, 13)]\n        self.assertEqual(adjust_spans(spans_1, spans_2), spans_1)\n\n    def test_empty_spans(self):\n        spans_1 = []\n        spans_2 = []\n        self.assertEqual(adjust_spans(spans_1, spans_2), [])\n\n\nclass TestCM3Dataset(unittest.TestCase):\n    def setUp(self) -> None:\n        torch.manual_seed(0)\n        np.random.seed(0)\n        random.seed(0)\n\n    def test_rotation_fim(self):\n        dataset = get_simple_dataset()\n        for dataset in [\n            get_simple_dataset(),\n            get_simple_dataset(allow_rotation_across_eod=True),\n        ]:\n            for item in dataset:\n                self.assertEqual(len(item[\"block\"]), dataset.block_size)\n                self.assertEqual(\n                    (item[\"block\"] == dataset.sentinel_tokens[0]).nonzero().size(0),\n                    2,\n                    f\"{item['block']} should contain only 2 {dataset.sentinel_tokens[0]} tokens.\"\n                    \"One for mask, one for prefix for generating mask.\",\n                )\n\n\nif __name__ == \"__main__\":\n    unittest.main()\n",
        "cpu_tests/test_data_utils.py": "# Copyright (c) Meta Platforms, Inc. and affiliates. All Rights Reserved.\n#\n# This source code is licensed under the MIT license found in the\n# LICENSE file in the root directory of this source tree.\n\nimport unittest\n\nimport numpy as np\nfrom metaseq.data.data_utils_fast import batch_by_size_fn\nfrom metaseq.data.data_utils_fast import batch_by_size_vec\n\n\nclass TestBatchBySize(unittest.TestCase):\n    @classmethod\n    def batch_by_size_baseline(\n        cls,\n        indices,\n        num_tokens_vec,\n        max_tokens,\n        max_sentences,\n        bsz_mult,\n    ):\n        \"\"\"Simple, reliable and slow implementation of batch by size\"\"\"\n        batches = []\n        start = 0\n        while start < len(indices):\n            for end in range(start + 1, len(indices) + 1):\n                max_val = max(num_tokens_vec[pos] for pos in range(start, end))\n                sent_count = end - start\n                num_tokens = max_val * sent_count\n                overflow = num_tokens > max_tokens > 0 or sent_count > max_sentences > 0\n                terminate = overflow or end == len(indices)\n                if overflow:\n                    sent_count -= 1\n                if terminate:\n                    if sent_count > bsz_mult:\n                        sent_count = sent_count - sent_count % bsz_mult\n                    batches.append(indices[start : start + sent_count])\n                    start = start + sent_count\n                    break\n        return batches\n\n    @classmethod\n    def _get_error_message(\n        cls, max_sentences, max_tokens, bsz_mult, num_tokens_vec, validation, results\n    ):\n        return f\"\"\"Reference batch_by_size implementation should produce\n                    same output as the baseline method.\n                Params:\n                max_sentences={max_sentences},\n                max_tokens={max_tokens},\n                bsz_mult={bsz_mult},\n                num_tokens_vec={num_tokens_vec},\n                expected_batches={validation},\n                returned_batches={results}\"\"\"\n\n    def _compare_results(\n        self,\n        indices_len,\n        batch_by_size_impl,\n        max_sentences,\n        max_tokens,\n        bsz_mult,\n        num_tokens_vec,\n    ):\n        indices = np.array(list(range(indices_len)))\n        validation = self.batch_by_size_baseline(\n            indices,\n            num_tokens_vec,\n            max_tokens=max_tokens,\n            max_sentences=max_sentences,\n            bsz_mult=bsz_mult,\n        )\n        results = batch_by_size_impl(\n            indices,\n            num_tokens_vec,\n            max_tokens=max_tokens,\n            max_sentences=max_sentences,\n            bsz_mult=bsz_mult,\n        )\n        error_msg = self._get_error_message(\n            max_sentences, max_tokens, bsz_mult, num_tokens_vec, validation, results\n        )\n        self.assertEqual(len(validation), len(results), error_msg)\n        for first, second in zip(validation, results):\n            self.assertTrue(np.array_equal(first, second), error_msg)\n\n    def _run_compare_with_baseline_sweep(self, batch_by_size_impl):\n        \"\"\"Compare reference batch_by_size implementation with batch_by_size_baseline\n        across a dense grid of hyperparam values\"\"\"\n        MAX_MAX_TOKENS = 10\n        NUM_TOKENS_VECS_COUNT = 5\n        for indices_len in [10, 11]:  # try odd and even len of indices\n            for max_sentences in range(0, indices_len + 2):\n                for max_tokens in range(0, MAX_MAX_TOKENS):\n                    for bsz_mult in range(1, max(MAX_MAX_TOKENS, indices_len) + 2):\n                        for _ in range(NUM_TOKENS_VECS_COUNT):\n                            num_tokens_vec = np.random.randint(\n                                0, max_tokens + 1, size=indices_len\n                            )\n                            self._compare_results(\n                                indices_len,\n                                batch_by_size_impl,\n                                max_sentences,\n                                max_tokens,\n                                bsz_mult,\n                                num_tokens_vec,\n                            )\n\n\nclass TestBatchBySizeVec(TestBatchBySize):\n    def test_compare_with_baseline(self):\n        self._run_compare_with_baseline_sweep(batch_by_size_vec)\n\n\nclass TestBatchBySizeFn(TestBatchBySize):\n    def test_compare_with_baseline(self):\n        def batch_by_size_fn_wrapper(\n            indices,\n            num_tokens_vec,\n            max_tokens,\n            max_sentences,\n            bsz_mult,\n        ):\n            def num_tokens_fn(idx):\n                return num_tokens_vec[idx]\n\n            return batch_by_size_fn(\n                indices, num_tokens_fn, max_tokens, max_sentences, bsz_mult\n            )\n\n        self._run_compare_with_baseline_sweep(batch_by_size_fn_wrapper)\n\n\nif __name__ == \"__main__\":\n    unittest.main()\n",
        "cpu_tests/test_dictionary.py": "# Copyright (c) Meta Platforms, Inc. and affiliates. All Rights Reserved.\n#\n# This source code is licensed under the MIT license found in the\n# LICENSE file in the root directory of this source tree.\n\nimport io\nimport os\nimport string\nimport tempfile\nimport unittest\n\nimport torch\n\nfrom metaseq.data import Dictionary\nfrom metaseq.utils import tokenize_line\n\n\nclass TestDictionary(unittest.TestCase):\n    def test_finalize(self):\n        txt = [\n            \"A B C D\",\n            \"B C D\",\n            \"C D\",\n            \"D\",\n        ]\n        ref_ids1 = list(\n            map(\n                torch.IntTensor,\n                [\n                    [4, 5, 6, 7, 2],\n                    [5, 6, 7, 2],\n                    [6, 7, 2],\n                    [7, 2],\n                ],\n            )\n        )\n        ref_ids2 = list(\n            map(\n                torch.IntTensor,\n                [\n                    [7, 6, 5, 4, 2],\n                    [6, 5, 4, 2],\n                    [5, 4, 2],\n                    [4, 2],\n                ],\n            )\n        )\n\n        # build dictionary\n        d = Dictionary()\n        for line in txt:\n            d.encode_line(line, add_if_not_exist=True)\n\n        def get_ids(dictionary):\n            ids = []\n            for line in txt:\n                ids.append(dictionary.encode_line(line, add_if_not_exist=False))\n            return ids\n\n        def assertMatch(ids, ref_ids):\n            for toks, ref_toks in zip(ids, ref_ids):\n                self.assertEqual(toks.size(), ref_toks.size())\n                self.assertEqual(0, (toks != ref_toks).sum().item())\n\n        ids = get_ids(d)\n        assertMatch(ids, ref_ids1)\n\n        # check finalized dictionary\n        d.finalize()\n        finalized_ids = get_ids(d)\n        assertMatch(finalized_ids, ref_ids2)\n\n        # write to disk and reload\n        with tempfile.NamedTemporaryFile(mode=\"w\") as tmp_dict:\n            d.save(tmp_dict.name)\n            d = Dictionary.load(tmp_dict.name)\n            reload_ids = get_ids(d)\n            assertMatch(reload_ids, ref_ids2)\n            assertMatch(finalized_ids, reload_ids)\n\n    def test_overwrite(self):\n        # for example, Camembert overwrites <unk>, <s> and </s>\n        dict_file = io.StringIO(\n            \"<unk> 999 #metaseq:overwrite\\n\"\n            \"<s> 999 #metaseq:overwrite\\n\"\n            \"</s> 999 #metaseq:overwrite\\n\"\n            \", 999\\n\"\n            \"▁de 999\\n\"\n        )\n        d = Dictionary()\n        d.add_from_file(dict_file)\n        self.assertEqual(d.index(\"<pad>\"), 1)\n        self.assertEqual(d.index(\"foo\"), 3)\n        self.assertEqual(d.index(\"<unk>\"), 4)\n        self.assertEqual(d.index(\"<s>\"), 5)\n        self.assertEqual(d.index(\"</s>\"), 6)\n        self.assertEqual(d.index(\",\"), 7)\n        self.assertEqual(d.index(\"▁de\"), 8)\n\n    def test_no_overwrite(self):\n        # for example, Camembert overwrites <unk>, <s> and </s>\n        dict_file = io.StringIO(\n            \"<unk> 999\\n\" \"<s> 999\\n\" \"</s> 999\\n\" \", 999\\n\" \"▁de 999\\n\"\n        )\n        d = Dictionary()\n        with self.assertRaisesRegex(RuntimeError, \"Duplicate\"):\n            d.add_from_file(dict_file)\n\n    def test_space(self):\n        # for example, character models treat space as a symbol\n        dict_file = io.StringIO(\"  999\\n\" \"a 999\\n\" \"b 999\\n\")\n        d = Dictionary()\n        d.add_from_file(dict_file)\n        self.assertEqual(d.index(\" \"), 4)\n        self.assertEqual(d.index(\"a\"), 5)\n        self.assertEqual(d.index(\"b\"), 6)\n\n    def test_add_file_to_dict(self):\n        counts = {}\n        num_lines = 100\n        per_line = 10\n        with tempfile.TemporaryDirectory(\"test_sampling\") as data_dir:\n            filename = os.path.join(data_dir, \"dummy.txt\")\n            with open(filename, \"w\", encoding=\"utf-8\") as data:\n                for c in string.ascii_letters:\n                    line = f\"{c} \" * per_line\n                    for _ in range(num_lines):\n                        data.write(f\"{line}\\n\")\n                    counts[c] = per_line * num_lines\n                    per_line += 5\n\n            dict = Dictionary()\n            Dictionary.add_file_to_dictionary(filename, dict, tokenize_line, 10)\n            dict.finalize(threshold=0, nwords=-1, padding_factor=8)\n\n            for c in string.ascii_letters:\n                count = dict.get_count(dict.index(c))\n                self.assertEqual(\n                    counts[c], count, f\"{c} count is {count} but should be {counts[c]}\"\n                )\n\n\nif __name__ == \"__main__\":\n    unittest.main()\n",
        "cpu_tests/test_file_chunker_utils.py": "# Copyright (c) Meta Platforms, Inc. and affiliates. All Rights Reserved.\n#\n# This source code is licensed under the MIT license found in the\n# LICENSE file in the root directory of this source tree.\n\nimport os\nimport shutil\nimport tempfile\nimport unittest\nfrom typing import Optional\n\n\nclass TestFileChunker(unittest.TestCase):\n    _tmpdir: Optional[str] = None\n    _tmpfile: Optional[str] = None\n    _line_content = \"Hello, World\\n\"\n    _num_bytes = None\n    _num_lines = 200\n    _num_splits = 20\n\n    @classmethod\n    def setUpClass(cls) -> None:\n        cls._num_bytes = len(cls._line_content.encode(\"utf-8\"))\n        cls._tmpdir = tempfile.mkdtemp()\n        with open(os.path.join(cls._tmpdir, \"test.txt\"), \"w\") as f:\n            cls._tmpfile = f.name\n            for _i in range(cls._num_lines):\n                f.write(cls._line_content)\n            f.flush()\n\n    @classmethod\n    def tearDownClass(cls) -> None:\n        # Cleanup temp working dir.\n        if cls._tmpdir is not None:\n            shutil.rmtree(cls._tmpdir)  # type: ignore\n\n    def test_find_offsets(self):\n        from metaseq.file_chunker_utils import find_offsets\n\n        offsets = find_offsets(self._tmpfile, self._num_splits)\n        self.assertEqual(len(offsets), self._num_splits + 1)\n        (zero, *real_offsets, last) = offsets\n        self.assertEqual(zero, 0)\n        for i, o in enumerate(real_offsets):\n            self.assertEqual(\n                o,\n                self._num_bytes\n                + ((i + 1) * self._num_bytes * self._num_lines / self._num_splits),\n            )\n        self.assertEqual(last, self._num_bytes * self._num_lines)\n\n    def test_readchunks(self):\n        from metaseq.file_chunker_utils import Chunker, find_offsets\n\n        offsets = find_offsets(self._tmpfile, self._num_splits)\n        for start, end in zip(offsets, offsets[1:]):\n            with Chunker(self._tmpfile, start, end) as lines:\n                all_lines = list(lines)\n                num_lines = self._num_lines / self._num_splits\n                self.assertAlmostEqual(\n                    len(all_lines), num_lines, delta=1\n                )  # because we split on the bites, we might end up with one more/less line in a chunk\n                self.assertListEqual(\n                    all_lines, [self._line_content for _ in range(len(all_lines))]\n                )\n",
        "cpu_tests/test_jsonl_dataset.py": "# Copyright (c) Meta Platforms, Inc. and affiliates. All Rights Reserved.\n#\n# This source code is licensed under the MIT license found in the\n# LICENSE file in the root directory of this source tree.\n\nimport json\nimport os\nimport random\nimport string\nimport tempfile\nimport unittest\nfrom unittest.mock import MagicMock\nfrom metaseq.data import JsonlDataset\n\n\ndef write_one_jsonl_(\n    jsonl_path, num_lines=5, text_len_min=5, text_len_max=50, truncate=False\n):\n    data = []\n    with open(jsonl_path, \"w\") as h:\n        for _ in range(num_lines):\n            text_len = random.choice(range(text_len_min, text_len_max))\n            data.append(\n                {\"text\": \"\".join(random.choices(string.ascii_letters, k=text_len))}\n            )\n            print(json.dumps(data[-1]), file=h)\n            if truncate and _ == 0:\n                line = \"\".join(random.choices(string.ascii_letters, k=text_len))\n                data.append(line)\n                print(line, file=h)\n    return data\n\n\nclass TestJsonlDataset(unittest.TestCase):\n    def test_one_line(self):\n        self._test_jsonl_dataset(num_lines=1)\n\n    def test_multiple_lines(self):\n        self._test_jsonl_dataset(num_lines=5)\n\n    def test_bad_cache(self):\n        with tempfile.NamedTemporaryFile() as jsonl_file:\n            write_one_jsonl_(jsonl_file.name, num_lines=3)\n            dataset = JsonlDataset(jsonl_file.name)\n            assert len(dataset) == 3\n\n            write_one_jsonl_(jsonl_file.name, num_lines=5)\n            dataset = JsonlDataset(jsonl_file.name)\n            assert len(dataset) == 3  # it's still 3 because of the cache\n\n            os.remove(dataset.cache)\n            dataset = JsonlDataset(jsonl_file.name)\n            assert len(dataset) == 5  # it's now 5 because the cache is recreated\n\n    def test_tokenizer(self, num_lines=5):\n        def tokenizer_fn(jsonl):\n            return list(jsonl[\"text\"])\n\n        tokenizer = MagicMock(wraps=tokenizer_fn)\n        with tempfile.NamedTemporaryFile() as jsonl_file:\n            orig_data = write_one_jsonl_(jsonl_file.name, num_lines=num_lines)\n            assert len(orig_data) == num_lines\n            dataset = JsonlDataset(jsonl_file.name, tokenizer=tokenizer)\n            assert tokenizer.call_count == 0\n\n            foo = dataset[1]\n            assert foo == list(orig_data[1][\"text\"])\n            assert tokenizer.call_count == 1\n\n            foo = dataset[1]\n            assert tokenizer.call_count == 2\n\n            foo = dataset[4]\n            assert foo == list(orig_data[4][\"text\"])\n            assert tokenizer.call_count == 3\n\n    def test_non_empty_jsonl(self):\n        with tempfile.NamedTemporaryFile() as jsonl_file:\n            orig_data = write_one_jsonl_(jsonl_file.name, num_lines=0)\n            assert len(orig_data) == 0\n            self.assertRaises(ValueError, JsonlDataset, jsonl_file.name)\n\n    def test_formatting_json(self):\n        with tempfile.NamedTemporaryFile() as jsonl_file:\n            orig_data = write_one_jsonl_(jsonl_file.name, num_lines=5, truncate=True)\n            assert (\n                len(orig_data) == 6\n            )  # it's 6 because we add an extra line of badly formatted json\n            with self.assertRaises(json.decoder.JSONDecodeError):\n                self._iterate_over_dataset(JsonlDataset(jsonl_file.name))\n\n    def _test_jsonl_dataset(self, num_lines, tokenizer=None):\n        with tempfile.NamedTemporaryFile() as jsonl_file:\n            orig_data = write_one_jsonl_(jsonl_file.name, num_lines=num_lines)\n            assert len(orig_data) == num_lines\n            dataset = JsonlDataset(jsonl_file.name, tokenizer=None)\n            assert len(dataset) == len(orig_data)\n            for orig_json, read_json in zip(orig_data, dataset):\n                assert orig_json.keys() == read_json.keys()\n                for k in orig_json.keys():\n                    assert orig_json[k] == read_json[k]\n\n    def _iterate_over_dataset(self, dataset: JsonlDataset):\n        iterated_documents = []\n        for idx in range(len(dataset)):\n            iterated_documents.append(dataset[idx][\"text\"])\n\n        return iterated_documents\n\n    def test_dataset_with_subshards(self):\n        with tempfile.NamedTemporaryFile() as jsonl_file:\n            documents_as_dict = write_one_jsonl_(jsonl_file.name, num_lines=11)\n            documents = [elem[\"text\"] for elem in documents_as_dict]\n\n            dataset = JsonlDataset(jsonl_file.name, epoch=1, data_subshard_count=3)\n            # The 4 documents would be 0, 3, 6 and 9 (0 based indexing)\n            self.assertEqual(\n                set([documents[idx] for idx in [0, 3, 6, 9]]),\n                set(self._iterate_over_dataset(dataset)),\n            )\n\n            dataset = JsonlDataset(jsonl_file.name, epoch=3, data_subshard_count=3)\n            # The 3 documents would be 2, 5 and 8 (0 based indexing)\n            self.assertEqual(\n                set([documents[idx] for idx in [2, 5, 8]]),\n                set(self._iterate_over_dataset(dataset)),\n            )\n\n            dataset = JsonlDataset(jsonl_file.name, epoch=4, data_subshard_count=3)\n            # If epoch > data_subshard_count , we wrap around. So epoch=4 behaves like epoch=1\n            self.assertEqual(\n                set([documents[idx] for idx in [0, 3, 6, 9]]),\n                set(self._iterate_over_dataset(dataset)),\n            )\n\n        # Confirm that iterating on the dataset works as expected\n        with tempfile.NamedTemporaryFile() as jsonl_file:\n            documents_as_dict = write_one_jsonl_(jsonl_file.name, num_lines=11)\n            documents = [elem[\"text\"] for elem in documents_as_dict]\n\n            # Assuming a data_subshard_count of 3, in 3 epochs we should have iterated\n            # over the whole dataset\n            iterated_documents = []\n            for epoch in range(1, 4):\n                dataset = JsonlDataset(\n                    jsonl_file.name, epoch=epoch, data_subshard_count=3\n                )\n                iterated_documents.extend(self._iterate_over_dataset(dataset))\n\n            # Ensure that all 11 documents have been iterated through\n            self.assertEqual(set(iterated_documents), set(documents))\n\n            # Now, let's try iterating for a total of 9 epochs, and assert that the entire data\n            # was iterated over thrice\n            iterated_documents = []\n            for epoch in range(1, 10):\n                dataset = JsonlDataset(\n                    jsonl_file.name, epoch=epoch, data_subshard_count=3\n                )\n                iterated_documents.extend(self._iterate_over_dataset(dataset))\n\n            assert len(iterated_documents) == 33  # 11*3\n\n            # We iterated over the same data thrice, so deduplicated documents should still match\n            self.assertEqual(set(documents), set(iterated_documents))\n\n\nif __name__ == \"__main__\":\n    unittest.main()\n",
        "cpu_tests/test_metrics.py": "# Copyright (c) Meta Platforms, Inc. and affiliates. All Rights Reserved.\n#\n# This source code is licensed under the MIT license found in the\n# LICENSE file in the root directory of this source tree.\n\nimport unittest\nimport uuid\n\nfrom metaseq import metrics\n\n\nclass TestMetrics(unittest.TestCase):\n    def test_nesting(self):\n        with metrics.aggregate() as a:\n            metrics.log_scalar(\"loss\", 1)\n            with metrics.aggregate() as b:\n                metrics.log_scalar(\"loss\", 2)\n\n        self.assertEqual(a.get_smoothed_values()[\"loss\"], 1.5)\n        self.assertEqual(b.get_smoothed_values()[\"loss\"], 2)\n\n    def test_new_root(self):\n        with metrics.aggregate() as a:\n            metrics.log_scalar(\"loss\", 1)\n            with metrics.aggregate(new_root=True) as b:\n                metrics.log_scalar(\"loss\", 2)\n\n        self.assertEqual(a.get_smoothed_values()[\"loss\"], 1)\n        self.assertEqual(b.get_smoothed_values()[\"loss\"], 2)\n\n    def test_nested_new_root(self):\n        with metrics.aggregate() as layer1:\n            metrics.log_scalar(\"loss\", 1)\n            with metrics.aggregate(new_root=True) as layer2:\n                metrics.log_scalar(\"loss\", 2)\n                with metrics.aggregate() as layer3:\n                    metrics.log_scalar(\"loss\", 3)\n                    with metrics.aggregate(new_root=True) as layer4:\n                        metrics.log_scalar(\"loss\", 4)\n            metrics.log_scalar(\"loss\", 1.5)\n\n        self.assertEqual(layer4.get_smoothed_values()[\"loss\"], 4)\n        self.assertEqual(layer3.get_smoothed_values()[\"loss\"], 3)\n        self.assertEqual(layer2.get_smoothed_values()[\"loss\"], 2.5)\n        self.assertEqual(layer1.get_smoothed_values()[\"loss\"], 1.25)\n\n    def test_named(self):\n        name = str(uuid.uuid4())\n        metrics.reset_meters(name)\n\n        with metrics.aggregate(name):\n            metrics.log_scalar(\"loss\", 1)\n\n        metrics.log_scalar(\"loss\", 3)\n\n        with metrics.aggregate(name):\n            metrics.log_scalar(\"loss\", 2)\n\n        self.assertEqual(metrics.get_smoothed_values(name)[\"loss\"], 1.5)\n\n    def test_nested_duplicate_names(self):\n        name = str(uuid.uuid4())\n        metrics.reset_meters(name)\n\n        with metrics.aggregate(name):\n            metrics.log_scalar(\"loss\", 1)\n            with metrics.aggregate() as other:\n                with metrics.aggregate(name):\n                    metrics.log_scalar(\"loss\", 2)\n            metrics.log_scalar(\"loss\", 6)\n\n        self.assertEqual(metrics.get_smoothed_values(name)[\"loss\"], 3)\n        self.assertEqual(other.get_smoothed_values()[\"loss\"], 2)\n\n\nif __name__ == \"__main__\":\n    unittest.main()\n",
        "cpu_tests/test_partitioned_streaming_dataset.py": "# Copyright (c) Meta Platforms, Inc. and affiliates. All Rights Reserved.\n#\n# This source code is licensed under the MIT license found in the\n# LICENSE file in the root directory of this source tree.\n\nimport unittest\n\nimport torch\n\nfrom metaseq.data import PartitionedStreamingDataset\n\n\nclass TensorListIterableDataset(torch.utils.data.IterableDataset):\n    def __init__(self, tensor_list):\n        self.tensor_list = tensor_list\n\n    def __iter__(self):\n        for tensor in self.tensor_list:\n            yield tensor\n\n\ndef get_simple_dataset():\n    return TensorListIterableDataset(\n        [\n            torch.LongTensor([0, 1]),\n            torch.LongTensor([2, 3]),\n            torch.LongTensor([4, 5]),\n            torch.LongTensor([6, 7]),\n            torch.LongTensor([8, 9]),\n        ]\n    )\n\n\nclass TestPartitionedStreamingDataset(unittest.TestCase):\n    def test_drop_last_True_shard_0(self):\n        self._test_simple(drop_last=True, shard_id=0)\n\n    def test_drop_last_True_shard_1(self):\n        self._test_simple(drop_last=True, shard_id=1)\n\n    def test_drop_last_False_shard_0(self):\n        self._test_simple(drop_last=False, shard_id=0)\n\n    def test_drop_last_False_shard_1(self):\n        self._test_simple(drop_last=False, shard_id=1)\n\n    def _test_simple(self, drop_last, shard_id):\n        dataset = get_simple_dataset()\n        partitioned_ds = PartitionedStreamingDataset(\n            dataset,\n            num_shards=2,\n            shard_id=shard_id,\n            drop_last=drop_last,\n        )\n        dataloader = iter(partitioned_ds)\n        if shard_id == 0:\n            assert next(dataloader).tolist() == [0, 1]\n            assert next(dataloader).tolist() == [4, 5]\n            if not drop_last:\n                assert next(dataloader).tolist() == [8, 9]\n        else:\n            assert shard_id == 1\n            assert next(dataloader).tolist() == [2, 3]\n            assert next(dataloader).tolist() == [6, 7]\n            if not drop_last:\n                assert next(dataloader) is None\n        with self.assertRaises(StopIteration):\n            next(dataloader)\n",
        "cpu_tests/test_streaming_iterators.py": "# Copyright (c) Meta Platforms, Inc. and affiliates. All Rights Reserved.\n#\n# This source code is licensed under the MIT license found in the\n# LICENSE file in the root directory of this source tree.\n\nimport unittest\n\nimport torch\n\nfrom metaseq.data import (\n    iterators,\n    StreamingShuffleDataset,\n    StreamingTokenBlockDataset,\n    PartitionedStreamingDataset,\n    StreamingSrcTgtDataset,\n)\nfrom metaseq.data.document_to_sequence import DocumentToSequenceDataset, LockingArray\n\nimport random\nimport pickle\n\n\nclass TensorListDataset(torch.utils.data.Dataset):\n    def __init__(self, tensor_list):\n        self.tensor_list = tensor_list\n        self.queried = 0\n\n    def __len__(self):\n        return len(self.tensor_list)\n\n    def __getitem__(self, idx):\n        self.queried += 1\n        return self.tensor_list[idx]\n\n\ndef get_simple_dataset():\n    dataset = TensorListDataset(\n        [\n            torch.LongTensor([0, 1]),\n            torch.LongTensor([2, 3]),\n            torch.LongTensor([4, 5]),\n            torch.LongTensor([6, 7]),\n            torch.LongTensor([8, 9]),\n        ]\n    )\n    dataset = DocumentToSequenceDataset(\n        dataset,\n        block_size=None,\n        permute_documents=False,\n        break_mode=\"passthrough\",\n        padding_idx=1,\n    )\n    return dataset\n\n\nclass FakeTensorData(torch.utils.data.Dataset):\n    def __init__(self, source_target):\n        self.rng = random.Random(0)\n        self.trng = torch.Generator()\n        self.trng.manual_seed(0)\n        self.items = [self._gen_one(source_target) for _ in range(len(self))]\n        self.queried = 0\n        self.realized = [False for _ in self.items]\n\n    def _gen_one(self, source_target):\n        n = self.rng.randrange(512, 2048)\n        toks = torch.randint(256, size=(n,), generator=self.trng)\n        if not source_target:\n            return toks\n        src_tokens_len = self.rng.randrange(1, n)\n        tgt_tokens = torch.clone(toks)\n        tgt_tokens[:src_tokens_len] = 1\n        return (toks, tgt_tokens)\n\n    def __len__(self):\n        return 128\n\n    def __getitem__(self, idx):\n        self.queried += 1\n        assert not self.realized[idx], \"Document unexpectedly loaded twice\"\n        self.realized[idx] = True\n        return self.items[idx]\n\n    def __iter__(self):\n        for i in range(len(self)):\n            yield self[i]\n\n\nclass TestStreamingIterators(unittest.TestCase):\n    def test_streaming_counting_iterator(self):\n        ref = list(\n            (0, i) for i in range(10)\n        )  # extra 0 is the worker_id that StreamingCountingIterator expects\n        itr = iterators.StreamingCountingIterator(ref, 0, 1, 1)\n        for i, (_, ref_i) in enumerate(ref):\n            self.assertTrue(itr.has_next())\n            self.assertEqual(itr.n, i)\n            self.assertEqual(next(itr), ref_i)\n        self.assertEqual(itr.n, len(ref))\n        self.assertFalse(itr.has_next())\n        with self.assertRaises(StopIteration):\n            next(itr)\n\n    def test_streaming_epoch_batch_iterator_drop_last_True(self):\n        self._test_streaming_epoch_batch_iterator(drop_last=True)\n\n    def test_streaming_epoch_batch_iterator_drop_last_False(self):\n        self._test_streaming_epoch_batch_iterator(drop_last=False)\n\n    def test_streaming_epoch_batch_iterator_state_dict(self):\n        def hook_fn(epoch_batch_itr, itr):\n            queried = epoch_batch_itr.dataset.dataset.queried\n            if epoch_batch_itr.iterations_in_epoch == 2:\n                assert queried == 2, \"Deferred Token cache didn't cache loading\"\n            new_epoch_batch_itr = iterators.StreamingEpochBatchIterator(\n                # recreate the dataset\n                dataset=get_simple_dataset(),\n                batch_size=epoch_batch_itr.batch_size,\n                collate_fn=epoch_batch_itr.collate_fn,\n                drop_last=epoch_batch_itr.drop_last,\n            )\n            # pickle the state_dict to test picklability\n            psd = pickle.dumps(epoch_batch_itr.state_dict())\n            new_epoch_batch_itr.load_state_dict(pickle.loads(psd))\n            return new_epoch_batch_itr, new_epoch_batch_itr.next_epoch_itr()\n\n        self._test_streaming_epoch_batch_iterator(drop_last=True, hook_fn=hook_fn)\n        self._test_streaming_epoch_batch_iterator(drop_last=False, hook_fn=hook_fn)\n\n    def _test_streaming_epoch_batch_iterator(self, drop_last, hook_fn=None):\n        dataset = get_simple_dataset()\n        epoch_batch_itr = iterators.StreamingEpochBatchIterator(\n            dataset,\n            batch_size=2,\n            collate_fn=lambda xs: torch.cat([x[\"block\"] for x in xs]),\n            drop_last=drop_last,\n        )\n        assert epoch_batch_itr.next_epoch_idx == 1\n        itr = epoch_batch_itr.next_epoch_itr()\n        assert epoch_batch_itr.iterations_in_epoch == 0\n        assert not epoch_batch_itr.end_of_epoch()\n\n        if hook_fn is not None:\n            epoch_batch_itr, itr = hook_fn(epoch_batch_itr, itr)\n        assert next(itr).tolist() == [0, 1, 2, 3]\n        assert epoch_batch_itr.iterations_in_epoch == 1\n        assert not epoch_batch_itr.end_of_epoch()\n\n        if hook_fn is not None:\n            epoch_batch_itr, itr = hook_fn(epoch_batch_itr, itr)\n        assert next(itr).tolist() == [4, 5, 6, 7]\n        assert epoch_batch_itr.iterations_in_epoch == 2\n\n        if not drop_last:\n            if hook_fn is not None:\n                epoch_batch_itr, itr = hook_fn(epoch_batch_itr, itr)\n            assert next(itr).tolist() == [8, 9]\n            assert epoch_batch_itr.iterations_in_epoch == 3\n\n        assert epoch_batch_itr.end_of_epoch()\n        with self.assertRaises(StopIteration):\n            next(itr)\n\n    def test_to_skip_iterator(self):\n        def create_dataset(\n            break_mode=\"none\", drop_last=True, sequence_size=2049, num_shards=1\n        ):\n            dataset = FakeTensorData(False)\n            token_dataset = DocumentToSequenceDataset(\n                dataset,\n                # We generate blocks with one extra token, so that we have a target\n                # for the final input token. This results in slight data loss.\n                block_size=sequence_size,\n                break_mode=break_mode,\n                # we drop the remainder block during training\n                drop_last=drop_last,\n                padding_idx=1,\n                seed=42,\n            )\n            token_dataset.set_shuffle_buffer_size(4)\n            token_dataset.set_epoch(0)\n            partitioned_dataset = PartitionedStreamingDataset(\n                token_dataset,\n                num_shards=num_shards,\n                shard_id=0,\n                drop_last=True,\n            )\n            return partitioned_dataset, dataset, token_dataset\n\n        def run_test(drop_last, break_mode):\n            dataset, fake_dataset, token_dataset = create_dataset(\n                drop_last=drop_last, break_mode=break_mode\n            )\n            num_iters = 0\n            values = []\n            for i, x in enumerate(dataset):\n                assert isinstance(x[\"block\"], torch.Tensor)\n                assert (\n                    x[\"block\"].shape[0] == 2049\n                    or (drop_last and x[\"block\"].shape[0] <= 2049)\n                    or break_mode == \"eos_pad_8\"\n                )\n                num_iters += 1\n                values.append(x)\n\n            a_fourth = num_iters // 4\n            dataset2, fake_dataset2, token_dataset2 = create_dataset(\n                drop_last=drop_last, break_mode=break_mode\n            )\n\n            token_dataset2.len_cache = token_dataset.len_cache\n            token_dataset2.to_skip = a_fourth\n            value1 = values[a_fourth]\n            it = iter(dataset2)\n            value2 = next(it)\n            assert torch.allclose(value1[\"block\"], value2[\"block\"])\n            # check that we didn't actually query all the dataset to do the fast forward\n            assert fake_dataset2.queried <= len(value2[\"ids\"]) + 1\n            # load the rest of the dataset to check we are only\n            # truly loading documents once even when we defer\n            while True:\n                try:\n                    next(it)\n                except StopIteration:\n                    break\n\n        run_test(drop_last=True, break_mode=\"complete\")\n        run_test(drop_last=False, break_mode=\"complete\")\n\n        run_test(drop_last=True, break_mode=\"eos_pad_8\")\n        run_test(drop_last=False, break_mode=\"eos_pad_8\")\n\n        run_test(drop_last=True, break_mode=\"none\")\n        run_test(drop_last=False, break_mode=\"none\")\n\n        # now do a test with actual dataloader object.\n        # the tricky bit here is if we suspend on an iteration `n`` that is not a multiple of the\n        # number of workers, we have to restore where the first requested batch is from\n        # to a worker (n % num_workers) that isn't worker 0. However, DataLoader returns data from worker 0 first.\n        # So we shift what each worker thinks its ID is by n so worker 0 will behave as worker (n % num_workers).\n        dataset, fake_dataset, token_dataset = create_dataset(\n            drop_last=True, break_mode=\"none\"\n        )\n\n        token_dataset.set_num_workers(3)\n\n        def collate_with_worker_id(items):\n            worker_info = torch.utils.data.get_worker_info()\n            return (worker_info.id, items[0])\n\n        dataloader1 = torch.utils.data.DataLoader(\n            dataset=dataset,\n            batch_size=1,\n            num_workers=3,\n            pin_memory=True,\n            drop_last=True,\n            collate_fn=collate_with_worker_id,\n        )\n        iterator1 = iterators.StreamingCountingIterator(dataloader1, 3, 1, 1)\n\n        ITERS = 76\n        for i in range(ITERS - 1):\n            next(iterator1)\n\n        sequences_consumed = list(iterator1.sequences_consumed)\n        next_worker = iterator1.next_worker\n\n        last = next(iterator1)\n\n        len_cache = token_dataset.len_cache\n        dataset, fake_dataset, token_dataset = create_dataset(\n            drop_last=True, break_mode=\"none\"\n        )\n        token_dataset.set_num_workers(3)\n        token_dataset.len_cache = len_cache\n        token_dataset.to_skip = sequences_consumed\n        token_dataset.worker_offset = next_worker\n        dataloader2 = torch.utils.data.DataLoader(\n            dataset=dataset,\n            batch_size=1,\n            num_workers=3,\n            pin_memory=True,\n            drop_last=True,\n        )\n        first = next(iter(dataloader2))\n        assert torch.allclose(last[\"block\"], first[\"block\"])\n\n    def test_document_to_sequence(self):\n        MAX_SEQ_LEN = 2048\n\n        def get_traditional_iterator(dataset, break_mode, drop_last, source_target):\n            shuffle_dataset = StreamingShuffleDataset(dataset, seed=42)\n            shuffle_dataset.set_epoch(0)\n            Dataset = (\n                StreamingTokenBlockDataset\n                if not source_target\n                else StreamingSrcTgtDataset\n            )\n            token_dataset = Dataset(\n                shuffle_dataset,\n                # We generate blocks with one extra token, so that we have a target\n                # for the final input token. This results in slight data loss.\n                block_size=MAX_SEQ_LEN + 1,\n                break_mode=break_mode,\n                # we drop the remainder block during training\n                drop_last=drop_last,\n                padding_idx=1,\n                # 1284 is a randomly-generated offset to decouple the seed used here\n                # from the seed used above in StreamingShuffleDataset\n                seed=1284 + 42,\n            )\n            token_dataset.set_shuffle_buffer_size(4)\n            return token_dataset\n\n        def get_document_to_sequence_iterator(\n            dataset, break_mode, drop_last, source_target\n        ):\n            document_to_sequence_dataset = DocumentToSequenceDataset(\n                dataset,\n                # We generate blocks with one extra token, so that we have a target\n                # for the final input token. This results in slight data loss.\n                block_size=MAX_SEQ_LEN + 1,\n                break_mode=break_mode,\n                # we drop the remainder block during training\n                drop_last=drop_last,\n                padding_idx=1,\n                # 1284 is a randomly-generated offset to decouple the seed used here\n                # from the seed used above in StreamingShuffleDataset\n                seed=42,\n                source_target=source_target,\n            )\n            document_to_sequence_dataset.set_epoch(0)\n            document_to_sequence_dataset.set_shuffle_buffer_size(4)\n            return document_to_sequence_dataset\n\n        def compare(break_mode, drop_last, source_target):\n            a = get_traditional_iterator(\n                FakeTensorData(source_target), break_mode, drop_last, source_target\n            )\n            b = get_document_to_sequence_iterator(\n                FakeTensorData(source_target), break_mode, drop_last, source_target\n            )\n            a_values = list(a)\n            b_values = list(b)\n            self.assertEqual(len(a_values), len(b_values))\n\n            for av, bv in zip(a_values, b_values):\n                self.assertTrue(torch.allclose(av[\"ids\"], bv[\"ids\"]))\n                if source_target:\n                    self.assertTrue(torch.allclose(av[\"src_block\"], bv[\"src_block\"]))\n                    self.assertTrue(torch.allclose(av[\"tgt_block\"], bv[\"tgt_block\"]))\n                else:\n                    self.assertTrue(torch.allclose(av[\"block\"], bv[\"block\"]))\n\n        # normal\n        compare(\"none\", False, False)\n        compare(\"eos_pad_8\", False, False)\n        compare(\"complete\", False, False)\n\n        compare(\"none\", True, False)\n        compare(\"eos_pad_8\", True, False)\n        compare(\"complete\", True, False)\n\n        # fine tuning\n        compare(\"none\", False, True)\n        compare(\"eos_pad_8\", False, True)\n        compare(\"complete\", False, True)\n\n        compare(\"none\", True, True)\n        compare(\"eos_pad_8\", True, True)\n        compare(\"complete\", True, True)\n\n    def test_locking_array(self):\n        l = LockingArray(20, 8)\n        for i in range(20):\n            l.data[i] = i\n        l2 = pickle.loads(pickle.dumps(l))\n        assert len(l2.data) == 20\n        assert len(l2.worker_locks) == 8\n        for i in range(20):\n            assert l2.data[i] == i\n\n    def test_stream_epoch_batch_iterator_restart_twice(self):\n        def get(state_dict=None):\n            num_workers = 2\n            dataset = DocumentToSequenceDataset(\n                FakeTensorData(False),\n                block_size=2049,\n                drop_last=True,\n                padding_idx=1,\n                seed=42,\n            )\n            dataset.set_shuffle_buffer_size(4)\n            dataset.set_epoch(0)\n            dataset.set_num_workers(num_workers)\n            r = iterators.StreamingEpochBatchIterator(\n                dataset, 1, list, True, num_workers=num_workers\n            )\n            if state_dict is not None:\n                r.load_state_dict(pickle.loads(state_dict))\n            return r\n\n        def nextv(it):\n            return next(it)[0][\"block\"]\n\n        first = get()\n        it = first.next_epoch_itr()\n        for i in range(11):\n            nextv(it)\n        second = get(pickle.dumps(first.state_dict()))\n        it2 = second.next_epoch_itr()\n        for i in range(5):\n            assert torch.allclose(nextv(it), nextv(it2))\n        third = get(pickle.dumps(second.state_dict()))\n        it3 = third.next_epoch_itr()\n        for i in range(5):\n            assert torch.allclose(nextv(it), nextv(it3))\n\n\nif __name__ == \"__main__\":\n    unittest.main()\n",
        "cpu_tests/test_streaming_language_modeling_task.py": "# Copyright (c) Meta Platforms, Inc. and affiliates. All Rights Reserved.\n#\n# This source code is licensed under the MIT license found in the\n# LICENSE file in the root directory of this source tree.\n\nimport os\nimport tempfile\nimport unittest\n\nimport torch\nfrom metaseq import options\n\nfrom metaseq.tasks.streaming_language_modeling import StreamingLanguageModelingTask\nfrom cpu_tests.test_utils import (\n    write_one_jsonl,\n    write_dummy_bpe,\n)\nfrom metaseq.dataclass.utils import convert_namespace_to_omegaconf\n\ntry:\n    import tokenizers  # noqa\n\n    has_hf_tokenizers = True\nexcept ImportError:\n    has_hf_tokenizers = False\n\n\nclass TestDatasetLoading(unittest.TestCase):\n    @unittest.skipIf(not has_hf_tokenizers, \"skip test if tokenizers is missing\")\n    def test_load_dataset(self):\n        with tempfile.TemporaryDirectory() as data_dir:\n            print(data_dir)\n            train_dir = os.path.join(data_dir, \"train\")\n            shard_00 = os.path.join(train_dir, \"00\")\n            shard_01 = os.path.join(train_dir, \"01\")\n\n            vocab_file, merges_file = write_dummy_bpe(data_dir)\n\n            # Create shard folders, and jsonl files\n            os.makedirs(shard_00)\n            os.makedirs(shard_01)\n\n            shard_00_json_1 = write_one_jsonl(\n                os.path.join(shard_00, \"json_1.jsonl\"), num_lines=11\n            )\n            shard_00_json_2 = write_one_jsonl(\n                os.path.join(shard_00, \"json_2.jsonl\"), num_lines=12\n            )\n            shard_01_json_1 = write_one_jsonl(\n                os.path.join(shard_01, \"json_1.jsonl\"), num_lines=13\n            )\n            shard_01_json_2 = write_one_jsonl(\n                os.path.join(shard_01, \"json_2.jsonl\"), num_lines=14\n            )\n\n            train_parser = options.get_training_parser()\n            train_args = options.parse_args_and_arch(\n                train_parser,\n                [\n                    \"--arch\",\n                    \"transformer_lm_gpt2_tiny\",\n                    \"--task\",\n                    \"streaming_language_modeling\",\n                    data_dir,\n                    \"--vocab-filename\",\n                    vocab_file,\n                    \"--merges-filename\",\n                    merges_file,\n                    \"--sample-break-mode\",\n                    \"complete\",\n                ],\n            )\n            cfg = convert_namespace_to_omegaconf(train_args)\n            # Data subshard count = 3\n            cfg.task.data_subshard_count = 3\n\n            self.task = StreamingLanguageModelingTask(cfg.task)\n\n            jsonl_data = {\n                \"shard_00_json_1\": [],\n                \"shard_00_json_2\": [],\n                \"shard_01_json_1\": [],\n                \"shard_01_json_2\": [],\n            }\n            for elem in shard_00_json_1:\n                jsonl_data[\"shard_00_json_1\"].append(self.task._tokenize_one_json(elem))\n            for elem in shard_00_json_2:\n                jsonl_data[\"shard_00_json_2\"].append(self.task._tokenize_one_json(elem))\n            for elem in shard_01_json_1:\n                jsonl_data[\"shard_01_json_1\"].append(self.task._tokenize_one_json(elem))\n            for elem in shard_01_json_2:\n                jsonl_data[\"shard_01_json_2\"].append(self.task._tokenize_one_json(elem))\n\n            # Iterate over epochs 1 to 3\n            # After these epochs, we should have iterated over shard 00, which consists of\n            # jsonl_data[\"shard_00_json_1\"] and jsonl_data[\"shard_00_json_2\"]\n            self.ensure_epoch_iteration_is_consistent(\n                jsonl_data[\"shard_00_json_1\"],\n                jsonl_data[\"shard_00_json_2\"],\n                1,\n                3,\n                self.task.args.data_subshard_count,\n            )\n\n            # Iterate over epochs 4 to 6\n            # After these epochs, we should have iterated over shard 01, which consists of\n            # jsonl_data[\"shard_01_json_1\"] and jsonl_data[\"shard_01_json_2\"]\n            self.ensure_epoch_iteration_is_consistent(\n                jsonl_data[\"shard_01_json_1\"],\n                jsonl_data[\"shard_01_json_2\"],\n                4,\n                6,\n                self.task.args.data_subshard_count,\n            )\n\n            # Iterate over epochs 7 to 9. We should wrap around and again iterate over shard 00\n            self.ensure_epoch_iteration_is_consistent(\n                jsonl_data[\"shard_00_json_1\"],\n                jsonl_data[\"shard_00_json_2\"],\n                7,\n                9,\n                self.task.args.data_subshard_count,\n            )\n\n    def ensure_epoch_iteration_is_consistent(\n        self, jsonl_data_1, jsonl_data_2, epoch_start, epoch_end, data_subshard_count\n    ):\n        iterated_data = []\n        for epoch in range(epoch_start, epoch_end + 1):\n            it_data = self.ensure_iterated_data_is_consistent(\n                jsonl_data_1,\n                jsonl_data_2,\n                epoch,\n                data_subshard_count,\n            )\n\n            iterated_data.extend(it_data)\n\n        # Ensure that after epoch_start-epoch_end epochs, we have iterated over the whole shard\n        # Assumption - The shard contains only two datasets - jsonl_data_1 and jsonl_data_2\n        data_that_should_be_iterated = jsonl_data_1 + jsonl_data_2\n        self.assertTrue(\n            set(self._stringify_and_sort_tensor_list(iterated_data)),\n            set(self._stringify_and_sort_tensor_list(data_that_should_be_iterated)),\n        )\n\n    def ensure_iterated_data_is_consistent(\n        self, jsonl_data_1, jsonl_data_2, epoch, data_subshard_count\n    ):\n        \"\"\"\n        Helper function to iterate over a single epoch, and ensure that the iterated documents\n        match our expectation from the shard/subshard standpoint.\n        \"\"\"\n        self.task.load_dataset(\"train\", epoch=epoch)\n        iterated_data = [doc for doc in self.task.dataset(\"train\").dataset]\n\n        # For a given epoch, the start offset would be\n        offset = (epoch - 1) % data_subshard_count\n\n        data_that_should_be_iterated = []\n        # Note that the iteration skips over data_subshard_count documents, and starts\n        # from an offset dictated by the value of epoch.\n        for ind in range(offset, len(jsonl_data_1), data_subshard_count):\n            data_that_should_be_iterated.append(jsonl_data_1[ind])\n        for ind in range(offset, len(jsonl_data_2), data_subshard_count):\n            data_that_should_be_iterated.append(jsonl_data_2[ind])\n\n        self.assertTrue(\n            all(\n                [\n                    torch.equal(elem1, elem2)\n                    for elem1, elem2 in zip(data_that_should_be_iterated, iterated_data)\n                ]\n            )\n        )\n\n        return iterated_data\n\n    def _stringify_and_sort_tensor_list(self, tensor_list):\n        return sorted([str(tensor) for tensor in tensor_list])\n\n\nif __name__ == \"__main__\":\n    unittest.main()\n",
        "cpu_tests/test_streaming_shuffle_dataset.py": "# Copyright (c) Meta Platforms, Inc. and affiliates. All Rights Reserved.\n#\n# This source code is licensed under the MIT license found in the\n# LICENSE file in the root directory of this source tree.\n\nimport unittest\n\nimport torch\n\nfrom metaseq.data import StreamingShuffleDataset\n\n\nclass TensorListDataset(torch.utils.data.Dataset):\n    def __init__(self, tensor_list):\n        self.tensor_list = tensor_list\n\n    def __getitem__(self, index):\n        return self.tensor_list[index]\n\n    def __len__(self):\n        return len(self.tensor_list)\n\n\ndef get_simple_dataset():\n    return TensorListDataset(\n        [\n            torch.LongTensor([0]),\n            torch.LongTensor([1, 2, 3]),\n            torch.LongTensor([4]),\n            torch.LongTensor([5]),\n            torch.LongTensor([6, 7, 8]),\n            torch.LongTensor([9, 10]),\n        ]\n    )\n\n\nclass TestStreamingShuffleDataset(unittest.TestCase):\n    def test_set_epoch(self):\n        dataset = get_simple_dataset()\n        shuffle_ds = StreamingShuffleDataset(dataset, seed=0)\n\n        shuffle_ds.set_epoch(1)\n        ref_epoch1 = list(shuffle_ds)\n        shuffle_ds.set_epoch(2)\n        ref_epoch2 = list(shuffle_ds)\n\n        self.assertTrue(\n            torch.cat(ref_epoch1).tolist() == torch.cat(ref_epoch1).tolist()\n        )\n        self.assertFalse(\n            torch.cat(ref_epoch1).tolist() == torch.cat(ref_epoch2).tolist()\n        )\n\n        shuffle_ds.set_epoch(1)\n        self._compare(ref_epoch1, shuffle_ds)\n        shuffle_ds.set_epoch(2)\n        self._compare(ref_epoch2, shuffle_ds)\n        shuffle_ds.set_epoch(2)\n        self._compare(ref_epoch2, shuffle_ds)\n        shuffle_ds.set_epoch(1)\n        self._compare(ref_epoch1, shuffle_ds)\n\n    def _compare(self, reference, dataset):\n        ref_itr = iter(reference)\n        ds_itr = iter(dataset)\n        for ref, ds in zip(ref_itr, ds_itr):\n            self.assertEqual(ref.tolist(), ds.tolist())\n        with self.assertRaises(StopIteration):\n            next(ref_itr)\n        with self.assertRaises(StopIteration):\n            next(ds_itr)\n",
        "cpu_tests/test_streaming_token_block_dataset.py": "# Copyright (c) Meta Platforms, Inc. and affiliates. All Rights Reserved.\n#\n# This source code is licensed under the MIT license found in the\n# LICENSE file in the root directory of this source tree.\n\nimport unittest\n\nimport numpy as np\nimport torch\n\nfrom metaseq.data import StreamingTokenBlockDataset\n\n\nclass TensorListDataset(torch.utils.data.Dataset):\n    def __init__(self, tensor_list):\n        self.tensor_list = tensor_list\n\n    def __getitem__(self, index):\n        return self.tensor_list[index]\n\n    def __len__(self):\n        return len(self.tensor_list)\n\n\ndef get_simple_dataset():\n    return TensorListDataset(\n        [\n            torch.LongTensor([0]),\n            torch.LongTensor([1, 2, 3]),\n            torch.LongTensor([4]),\n            torch.LongTensor([5]),\n            torch.LongTensor([6, 7, 8]),\n            torch.LongTensor([9, 10]),\n        ]\n    )\n\n\nclass TestStreamingTokenBlockDataset(unittest.TestCase):\n    def test_drop_last_True(self):\n        self._test_simple(drop_last=True)\n\n    def test_drop_last_False(self):\n        self._test_simple(drop_last=False)\n\n    def test_buffer_drop_last_True(self):\n        self._test_buffer(drop_last=True)\n\n    def test_buffer_drop_last_False(self):\n        self._test_buffer(drop_last=False)\n\n    def test_very_large_buffer_drop_last_True(self):\n        self._test_very_large_buffer(drop_last=True)\n\n    def test_very_large_buffer_drop_last_False(self):\n        self._test_very_large_buffer(drop_last=False)\n\n    def _test_simple(self, drop_last):\n        dataset = get_simple_dataset()\n        token_block_ds = StreamingTokenBlockDataset(\n            dataset,\n            block_size=2,\n            drop_last=drop_last,\n            padding_idx=-1,\n        )\n        dataloader = iter(token_block_ds)\n        assert next(dataloader)[\"block\"].tolist() == [0, 1]\n        assert next(dataloader)[\"block\"].tolist() == [2, 3]\n        assert next(dataloader)[\"block\"].tolist() == [4, 5]\n        assert next(dataloader)[\"block\"].tolist() == [6, 7]\n        assert next(dataloader)[\"block\"].tolist() == [8, 9]\n        if not drop_last:\n            assert next(dataloader)[\"block\"].tolist() == [10, -1]\n        with self.assertRaises(StopIteration):\n            next(dataloader)\n\n    def _test_buffer(self, drop_last, seed=42):\n        # maintain shadow rng to ensure iteration order matches expectations\n        shadow_rng = np.random.default_rng(2273 + seed)\n\n        dataset = get_simple_dataset()\n        token_block_ds = StreamingTokenBlockDataset(\n            dataset,\n            block_size=2,\n            drop_last=drop_last,\n            padding_idx=-1,\n            shuffle_buffer_size=3,\n            seed=seed,\n        )\n        dataloader = iter(token_block_ds)\n\n        # we expect token_block_ds to buffer the first three blocks,\n        # then return random blocks and replace them thereafter\n        expected_buffer = [\n            [0, 1],\n            [2, 3],\n            [4, 5],\n        ]\n\n        next_idx = shadow_rng.integers(3)\n        assert next(dataloader)[\"block\"].tolist() == expected_buffer[next_idx]\n        expected_buffer[next_idx] = [6, 7]\n\n        next_idx = shadow_rng.integers(3)\n        assert next(dataloader)[\"block\"].tolist() == expected_buffer[next_idx]\n        expected_buffer[next_idx] = [8, 9]\n\n        next_idx = shadow_rng.integers(3)\n        assert next(dataloader)[\"block\"].tolist() == expected_buffer[next_idx]\n        if not drop_last:\n            expected_buffer[next_idx] = [10, -1]\n        else:\n            expected_buffer.pop(next_idx)\n\n        while expected_buffer:\n            next_idx = shadow_rng.integers(len(expected_buffer))\n            assert next(dataloader)[\"block\"].tolist() == expected_buffer[next_idx]\n            expected_buffer.pop(next_idx)\n\n        with self.assertRaises(StopIteration):\n            next(dataloader)\n\n    def _test_very_large_buffer(self, drop_last, seed=42):\n        # maintain shadow rng to ensure iteration order matches expectations\n        shadow_rng = np.random.default_rng(2273 + seed)\n\n        dataset = get_simple_dataset()\n        token_block_ds = StreamingTokenBlockDataset(\n            dataset,\n            block_size=2,\n            drop_last=drop_last,\n            padding_idx=-1,\n            shuffle_buffer_size=100,  # bigger than full dataset\n            seed=seed,\n        )\n        dataloader = iter(token_block_ds)\n\n        expected_buffer = [\n            [0, 1],\n            [2, 3],\n            [4, 5],\n            [6, 7],\n            [8, 9],\n        ]\n        if not drop_last:\n            expected_buffer.append([10, -1])\n\n        while expected_buffer:\n            next_idx = shadow_rng.integers(len(expected_buffer))\n            assert next(dataloader)[\"block\"].tolist() == expected_buffer[next_idx]\n            expected_buffer.pop(next_idx)\n\n        with self.assertRaises(StopIteration):\n            next(dataloader)\n\n    def _test_break_mode_eos_pad_8(self):\n        dataset = TensorListDataset(\n            [\n                torch.LongTensor([0]),\n                torch.LongTensor([1, 2, 3]),\n                torch.LongTensor([1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11]),\n            ]\n        )\n        token_block_ds = StreamingTokenBlockDataset(\n            dataset,\n            block_size=10,\n            drop_last=False,\n            padding_idx=-1,\n            break_mode=\"eos_pad_8\",\n        )\n        expected_buffer = [\n            [0, -1, -1, -1, -1, -1, -1, -1, -1],\n            [1, 2, 3, -1, -1, -1, -1, -1, -1],\n            [1, 2, 3, 4, 5, 6, 7, 8, 9, 10],\n        ]\n\n        dataloader = iter(token_block_ds)\n        assert (\n            next(dataloader)[\"block\"].tolist() == expected_buffer[0]\n        )  # padding to multiple of 8 + 1\n        assert (\n            next(dataloader)[\"block\"].tolist() == expected_buffer[1]\n        )  # padding to multiple of 8 + 1\n        assert (\n            next(dataloader)[\"block\"].tolist() == expected_buffer[2]\n        )  # padding to block size\n\n        with self.assertRaises(StopIteration):\n            next(dataloader)\n",
        "cpu_tests/test_token_block_dataset.py": "# Copyright (c) Meta Platforms, Inc. and affiliates. All Rights Reserved.\n#\n# This source code is licensed under the MIT license found in the\n# LICENSE file in the root directory of this source tree.\n\nimport unittest\n\nimport tests.utils as test_utils\nimport torch\nfrom metaseq.data import TokenBlockDataset\n\n\nclass TestTokenBlockDataset(unittest.TestCase):\n    def _build_dataset(self, data, **kwargs):\n        sizes = [len(x) for x in data]\n        underlying_ds = test_utils.TestDataset(data)\n        return TokenBlockDataset(underlying_ds, sizes, **kwargs)\n\n    def test_eos_break_mode(self):\n        data = [\n            torch.tensor([5, 4, 3, 2, 1], dtype=torch.long),\n            torch.tensor([1], dtype=torch.long),\n            torch.tensor([8, 7, 6, 1], dtype=torch.long),\n        ]\n        ds = self._build_dataset(data, block_size=None, pad=0, eos=1, break_mode=\"eos\")\n        self.assertEqual(ds[0].tolist(), [5, 4, 3, 2, 1])\n        self.assertEqual(ds[1].tolist(), [1])\n        self.assertEqual(ds[2].tolist(), [8, 7, 6, 1])\n\n        data = [\n            torch.tensor([5, 4, 3, 2, 1], dtype=torch.long),\n            torch.tensor([8, 7, 6, 1], dtype=torch.long),\n            torch.tensor([1], dtype=torch.long),\n        ]\n        ds = self._build_dataset(data, block_size=None, pad=0, eos=1, break_mode=\"eos\")\n        self.assertEqual(ds[0].tolist(), [5, 4, 3, 2, 1])\n        self.assertEqual(ds[1].tolist(), [8, 7, 6, 1])\n        self.assertEqual(ds[2].tolist(), [1])\n\n    def test_block_break_mode(self):\n        data = [\n            torch.tensor([5, 4, 3, 2, 1], dtype=torch.long),\n            torch.tensor([8, 7, 6, 1], dtype=torch.long),\n            torch.tensor([9, 1], dtype=torch.long),\n        ]\n        ds = self._build_dataset(data, block_size=3, pad=0, eos=1, break_mode=\"none\")\n        self.assertEqual(ds[0].tolist(), [5, 4, 3])\n        self.assertEqual(ds[1].tolist(), [2, 1, 8])\n        self.assertEqual(ds[2].tolist(), [7, 6, 1])\n        self.assertEqual(ds[3].tolist(), [9, 1])\n\n    def test_complete_break_mode(self):\n        data = [\n            torch.tensor([5, 4, 3, 2, 1], dtype=torch.long),\n            torch.tensor([8, 7, 6, 1], dtype=torch.long),\n            torch.tensor([9, 1], dtype=torch.long),\n        ]\n        ds = self._build_dataset(\n            data, block_size=6, pad=0, eos=1, break_mode=\"complete\"\n        )\n        self.assertEqual(ds[0].tolist(), [5, 4, 3, 2, 1])\n        self.assertEqual(ds[1].tolist(), [8, 7, 6, 1, 9, 1])\n\n        data = [\n            torch.tensor([4, 3, 2, 1], dtype=torch.long),\n            torch.tensor([5, 1], dtype=torch.long),\n            torch.tensor([1], dtype=torch.long),\n            torch.tensor([6, 1], dtype=torch.long),\n        ]\n        ds = self._build_dataset(\n            data, block_size=3, pad=0, eos=1, break_mode=\"complete\"\n        )\n        self.assertEqual(ds[0].tolist(), [4, 3, 2, 1])\n        self.assertEqual(ds[1].tolist(), [5, 1, 1])\n        self.assertEqual(ds[2].tolist(), [6, 1])\n\n    def test_4billion_tokens(self):\n        \"\"\"Regression test for numpy type promotion issue https://github.com/numpy/numpy/issues/5745\"\"\"\n        data = [torch.tensor(list(range(10000)), dtype=torch.long)] * 430000\n        ds = self._build_dataset(\n            data, block_size=6, pad=0, eos=1, break_mode=\"complete\"\n        )\n        ds[-1]  # __getitem__ works\n        start, end = ds.slice_indices[-1]\n        assert end > 4294967295  # data must be sufficiently large to overflow uint32\n        assert not isinstance(\n            end + 1, float\n        )  # this would also raise, since np.uint64(1) + 1 => 2.0\n\n\nif __name__ == \"__main__\":\n    unittest.main()\n",
        "cpu_tests/test_utils.py": "# Copyright (c) Meta Platforms, Inc. and affiliates. All Rights Reserved.\n#\n# This source code is licensed under the MIT license found in the\n# LICENSE file in the root directory of this source tree.\n\nimport unittest\n\nimport torch\nimport random\nimport string\nimport json\nimport os\nfrom metaseq import utils\n\n\ndef write_one_jsonl(jsonl_path, num_lines=5, text_len_min=5, text_len_max=50):\n    data = []\n    with open(jsonl_path, \"w\") as h:\n        for _ in range(num_lines):\n            text_len = random.choice(range(text_len_min, text_len_max))\n            data.append(\n                {\"text\": \"\".join(random.choices(string.ascii_letters, k=text_len))}\n            )\n            print(json.dumps(data[-1]), file=h)\n    return data\n\n\ndef write_dummy_jsonl_data_dir(data_dir, num_lines=500):\n    for subset in [\"train\", \"valid\"]:\n        for shard in range(2):\n            shard_dir = os.path.join(data_dir, subset, f\"{shard:02}\")\n            os.makedirs(shard_dir)\n            for dataset in [\"a\", \"b\"]:\n                write_one_jsonl(\n                    os.path.join(shard_dir, f\"dataset_{dataset}.jsonl\"),\n                    num_lines=num_lines,\n                )\n\n\ndef write_dummy_bpe(data_dir):\n    from tokenizers import ByteLevelBPETokenizer\n\n    tokenizer = ByteLevelBPETokenizer(add_prefix_space=True)\n    tokenizer.train(\n        [],\n        vocab_size=500,\n        special_tokens=[\"<s>\", \"<pad>\", \"</s>\", \"<unk>\"],\n        show_progress=False,\n    )\n    vocab, merges = tokenizer.save_model(data_dir)\n    return vocab, merges\n\n\nclass TestUtils(unittest.TestCase):\n    def test_make_positions(self):\n        pad = 1\n        left_pad_input = torch.LongTensor(\n            [\n                [9, 9, 9, 9, 9],\n                [1, 9, 9, 9, 9],\n                [1, 1, 1, 9, 9],\n            ]\n        )\n        left_pad_output = torch.LongTensor(\n            [\n                [2, 3, 4, 5, 6],\n                [1, 2, 3, 4, 5],\n                [1, 1, 1, 2, 3],\n            ]\n        )\n        right_pad_input = torch.LongTensor(\n            [\n                [9, 9, 9, 9, 9],\n                [9, 9, 9, 9, 1],\n                [9, 9, 1, 1, 1],\n            ]\n        )\n        right_pad_output = torch.LongTensor(\n            [\n                [2, 3, 4, 5, 6],\n                [2, 3, 4, 5, 1],\n                [2, 3, 1, 1, 1],\n            ]\n        )\n\n        self.assertAlmostEqual(\n            left_pad_output,\n            utils.make_positions(left_pad_input, pad),\n        )\n        self.assertAlmostEqual(\n            right_pad_output,\n            utils.make_positions(right_pad_input, pad),\n        )\n\n    def test_clip_grad_norm_(self):\n        params = torch.nn.Parameter(torch.zeros(5)).requires_grad_(False)\n        grad_norm = utils.clip_grad_norm_(params, 1.0)\n        self.assertTrue(torch.is_tensor(grad_norm))\n        self.assertEqual(grad_norm, 0.0)\n\n        params = [torch.nn.Parameter(torch.zeros(5)) for _ in range(3)]\n        for p in params:\n            p.grad = torch.arange(1.0, 6.0)\n        grad_norm = utils.clip_grad_norm_(params, 1.0, \"l2\")\n        exp_grad_norm = torch.arange(1.0, 6.0).repeat(3).norm()\n        self.assertTrue(torch.is_tensor(grad_norm))\n        self.assertAlmostEqual(grad_norm, exp_grad_norm)\n\n        grad_norm = utils.clip_grad_norm_(params, 1.0, \"l2\")\n        self.assertAlmostEqual(grad_norm, torch.tensor(1.0))\n\n        for p in params:\n            p.grad = torch.arange(1.0, 6.0)\n        grad_norm = utils.clip_grad_norm_(params, 1.0, \"inf\")\n        exp_grad_norm = torch.arange(1.0, 6.0).max()\n        self.assertEqual(grad_norm, exp_grad_norm)\n\n    def test_resolve_max_positions_with_tuple(self):\n        resolved = utils.resolve_max_positions(None, (2000, 100, 2000), 12000)\n        self.assertEqual(resolved, (2000, 100, 2000))\n\n    def assertAlmostEqual(self, t1, t2):\n        self.assertEqual(t1.size(), t2.size(), \"size mismatch\")\n        self.assertLess(utils.item((t1 - t2).abs().max()), 1e-4)\n\n\nif __name__ == \"__main__\":\n    unittest.main()\n",
        "gpu_tests/test_activation_checkpointing.py": "# Copyright (c) Meta Platforms, Inc. and affiliates. All Rights Reserved.\n#\n# This source code is licensed under the MIT license found in the\n# LICENSE file in the root directory of this source tree.\n\nimport unittest\n\nimport torch\nimport torch.nn as nn\nfrom torch.utils.checkpoint import checkpoint\n\nfrom metaseq.modules.checkpoint_activations import checkpoint_wrapper\n\n\nclass Model(nn.Module):\n    def __init__(\n        self, use_pytorch_checkpoint=False, use_metaseq_checkpoint=False, **kwargs\n    ):\n        super().__init__()\n        torch.manual_seed(0)\n        self.use_pytorch_checkpoint = use_pytorch_checkpoint\n        self.ffn = nn.Sequential(\n            nn.Linear(32, 128),\n            # add a Dropout layer to test RNG save/restore\n            nn.Dropout(p=0.5),\n            nn.Linear(128, 32),\n        )\n        if use_metaseq_checkpoint:\n            self.ffn = checkpoint_wrapper(self.ffn, **kwargs)\n        self.out = nn.Linear(32, 1)\n\n    def forward(self, x):\n        if self.use_pytorch_checkpoint:\n            x = checkpoint(self.ffn, x)\n        else:\n            x = self.ffn(x)\n        return self.out(x)\n\n\nclass TestActivationCheckpointing(unittest.TestCase):\n    def _test_checkpoint_wrapper(self, device, log_memory_usage=False):\n        def get_loss_and_gnorm(model):\n            torch.manual_seed(1)\n            input = torch.rand(2, 16, 32).requires_grad_(True).to(device)\n            model.zero_grad()\n            loss = model(input).sum()\n            loss.backward()\n            gnorm = torch.norm(\n                torch.stack([torch.norm(p.grad.detach()) for p in model.parameters()])\n            )\n            return {\"loss\": loss, \"gnorm\": gnorm}\n\n        model = Model().to(device)\n        no_cpt = get_loss_and_gnorm(model)\n\n        model = Model(use_pytorch_checkpoint=True).to(device)\n        pyt_cpt = get_loss_and_gnorm(model)\n        torch.testing.assert_allclose(no_cpt[\"loss\"], pyt_cpt[\"loss\"])\n        torch.testing.assert_allclose(no_cpt[\"gnorm\"], pyt_cpt[\"gnorm\"])\n\n        model = Model(use_metaseq_checkpoint=True).to(device)\n        metaseq_cpt = get_loss_and_gnorm(model)\n        torch.testing.assert_allclose(no_cpt[\"loss\"], metaseq_cpt[\"loss\"])\n        torch.testing.assert_allclose(no_cpt[\"gnorm\"], metaseq_cpt[\"gnorm\"])\n\n        model = Model(use_metaseq_checkpoint=True, offload_to_cpu=True).to(device)\n        metaseq_cpt_offload = get_loss_and_gnorm(model)\n        torch.testing.assert_allclose(no_cpt[\"loss\"], metaseq_cpt_offload[\"loss\"])\n        torch.testing.assert_allclose(no_cpt[\"gnorm\"], metaseq_cpt_offload[\"gnorm\"])\n\n    def test_checkpoint_wrapper_cpu(self):\n        self._test_checkpoint_wrapper(device=torch.device(\"cpu\"))\n\n    @unittest.skipIf(not torch.cuda.is_available(), \"test requires a GPU\")\n    def test_checkpoint_wrapper_cuda(self):\n        self._test_checkpoint_wrapper(device=torch.device(\"cuda\"))\n\n\nif __name__ == \"__main__\":\n    unittest.main()\n",
        "gpu_tests/test_checkpoint_loading_on_more_gpus.py": "# Copyright (c) Meta Platforms, Inc. and affiliates. All Rights Reserved.\n#\n# This source code is licensed under the MIT license found in the\n# LICENSE file in the root directory of this source tree.\nimport json\nimport logging\nimport multiprocessing\nimport subprocess\nimport unittest\nfrom functools import partial, partialmethod\nfrom unittest.mock import patch\n\nimport torch\n\nfrom metaseq.cli.train import cli_main as train_cli_main\nfrom metaseq.dataclass.configs import DistributedTrainingConfig\nfrom metaseq.launcher.opt_baselines import cli_main as sweep_cli_main\nfrom metaseq.launcher.opt_job_constants import Size, M\n\nlogger = logging.getLogger(__name__)\n\n\n@unittest.skipIf(not torch.cuda.is_available(), \"test requires 4 GPUs, none found\")\n@unittest.skipIf(\n    DistributedTrainingConfig.distributed_world_size != 4,\n    \"test requires 4 GPUs\",\n)\nclass TestLoadOnMoreGPUs(unittest.TestCase):\n    \"\"\"\n    The test will verify that the model can started with more GPUs\n    from a checkpoint created with less GPUs. We test the first\n    run with 2 GPUs and the second run is started with 4 GPUs\n    from the previous checkpoint with 2GPUs.\n    \"\"\"\n\n    def test_load_checkpoint(self):\n        argv_injection = (\n            \"python3 metaseq/launcher/opt_baselines.py   \"\n            \"--prefix train.8m    --model-size 8m    --checkpoints-dir ./test-checkpoint    \"\n            \"--tensorboard-logdir ./test-checkpoint    --num-trials 1    --azure   \"\n            \"--num-gpus 2 --num-nodes 1   --seed 1   \"\n            \"--local --disable-validation    --max-epoch 5    --max-update 5 --benchmark    \"\n        )\n        max_update_first_run = 20\n        size_patch_dict = {\"8m\": Size(4, 128, 2, 64, int(0.03125 * M), 1.0e-3, 2)}\n\n        training_log_events_first_run = self._helper(\n            max_update=max_update_first_run,\n            argv_injection=argv_injection,\n            size_patch_dict=size_patch_dict,\n        )\n\n        # check that training ran correctly\n        # check that the number of updates was correct\n        self.assertNotEqual(training_log_events_first_run, [])\n        self.assertIsNotNone(training_log_events_first_run[-1][\"num_updates\"])\n        self.assertEqual(\n            int(training_log_events_first_run[-1][\"num_updates\"]), max_update_first_run\n        )\n        # check the achieved loss is correct\n        loss_val = float(training_log_events_first_run[-1][\"loss\"])\n        self.assertAlmostEqual(loss_val, 14.744, 1)  # 1 digit precision\n\n        # get the list of files from the chekpoint folder\n        # from the 2 GPUs run: ./test-checkpoint/*.ngpu2\n        first_run_checkpoints = subprocess.Popen(\n            \"ls -1 ./test-checkpoint/*.ngpu2\",\n            shell=True,  # this enables the * to be interpreted as a wildcard pattern\n            stdout=subprocess.PIPE,\n            stderr=subprocess.PIPE,\n            universal_newlines=True,\n        )\n        stdout_first, stderr_first = first_run_checkpoints.communicate()\n        files_first = stdout_first.split(\"\\n\")\n        # check that there are only 2 checkpoint files after running on 2 GPUs\n        # ['checkpoint_18-model_part-0-shard0.pt', 'checkpoint_18-model_part-1-shard0.pt']\n        checkpoints_first_num = len(\n            [\n                filename\n                for filename in files_first\n                if filename.startswith(\"checkpoint_18\")\n            ]\n        )\n        self.assertEqual(\n            checkpoints_first_num,\n            2,\n            f\"Expected 2 checkpoint files got {checkpoints_first_num}. List all files in the dir: {files_first}\",\n        )\n\n        # start second run with 4 gpus from a previously created checkpoint\n        # with 2 gpus from a \"*.ngpu2/checkpoint_18.pt\"\n        argv_injection = (\n            \"python3 metaseq/launcher/opt_baselines.py   \"\n            \"--prefix train.8m    --model-size 8m    --checkpoints-dir ./test-checkpoint    \"\n            \"--tensorboard-logdir ./test-checkpoint    --num-trials 1    --azure   \"\n            \"--num-gpus 4 --num-nodes 1   --seed 1   \"\n            \"--local --disable-validation    --max-epoch 5    --max-update 5 --benchmark    \"\n            \"--restore-file ./test-checkpoint/train.8m.dummy_lm.me_fp16.fsdp.zero2.relu.\"\n            \"transformer_lm_megatron.nlay4.emb128.lrnpos.0emb_scale.tps2048.adam.b2_0.95.\"\n            \"eps1e-08.cl1.0.lr0.001.endlr0.0001.wu50.dr0.1.atdr0.1.0emb_dr.wd0.1.ms16.uf1.\"\n            \"mu50.s1.me5.ngpu2/checkpoint_18.pt\"\n        )\n\n        max_update_second_run = 40\n\n        training_log_events_second_run = self._helper(\n            max_update=max_update_second_run,\n            argv_injection=argv_injection,\n            size_patch_dict=size_patch_dict,\n        )\n\n        # check that training ran correctly\n        self.assertNotEqual(training_log_events_second_run, [])\n\n        # check the achieved loss is correct\n        # check that loss is same as with 2 GPUs after reloading on 4 GPUs\n        loss_val_start = float(training_log_events_second_run[0][\"loss\"])\n        self.assertAlmostEqual(loss_val_start, 14.744, 1)  # 1 digit precision\n\n        # check that loss improved during training on 4 GPUs\n        loss_val_end = float(training_log_events_second_run[-1][\"loss\"])\n        self.assertAlmostEqual(loss_val_end, 12.165, 1)  # 1 digit precision\n\n        # get the list of files from the chekpoint folder\n        # from the 4 GPUs run: ./test-checkpoint/*.ngpu4\n        second_run_checkpoints = subprocess.Popen(\n            \"ls -1 ./test-checkpoint/*.ngpu4\",\n            shell=True,  # this enables the * to be interpreted as a wildcard pattern\n            stdout=subprocess.PIPE,\n            stderr=subprocess.PIPE,\n            universal_newlines=True,\n        )\n        stdout_second, _ = second_run_checkpoints.communicate()\n        files_second = stdout_second.split(\"\\n\")\n\n        # check that there are now 4 checkpoint files after running on 4 GPUs\n        # and starting from the checkpoint folder with 2 checkpoints\n        # ['checkpoint_36-model_part-0-shard0.pt', 'checkpoint_36-model_part-0-shard1.pt',\n        # 'checkpoint_36-model_part-1-shard0.pt', 'checkpoint_36-model_part-1-shard1.pt'\n        checkpoints_second_num = len(\n            [\n                filename\n                for filename in files_second\n                if filename.startswith(\"checkpoint_36\")\n            ]\n        )\n        self.assertEqual(\n            checkpoints_second_num,\n            4,\n            f\"Expected 4 checkpoint files got {checkpoints_second_num}. List all files in the dir: {files_second}\",\n        )\n\n        # cleanup of the checkpoints files\n        cleanup_checkpoints = subprocess.Popen(\n            \"rm -r ./test-checkpoint\",\n            shell=True,\n            stdout=subprocess.PIPE,\n            stderr=subprocess.PIPE,\n            universal_newlines=True,\n        )\n        _, _ = cleanup_checkpoints.communicate()\n\n    def _helper(self, max_update, argv_injection, size_patch_dict):\n        \"\"\"\n        Helper function to run the test\n        \"\"\"\n        # start the process for the model run\n        multiprocessing.set_start_method(\"spawn\", force=True)\n        with torch.multiprocessing.Manager() as manager:\n            events = manager.list()\n            p = multiprocessing.Process(\n                target=run_training,\n                args=(max_update, events, argv_injection, size_patch_dict),\n            )\n            p.start()\n            p.join()\n            events_first_run = list(events)\n\n        # parse the log events from the log_to_events()\n        training_log_events = [\n            json.loads(event[\"message\"])\n            for event in events_first_run\n            if event[\"type\"] == \"log\" and event[\"message\"].startswith('{\"epoch\"')\n        ]\n\n        return training_log_events\n\n\ndef run_training(max_update, events, argv_injection, size_patch_dict):\n    # clean any unused cach to reduce CUDA OOM\n    torch.cuda.empty_cache()\n    # main arguments to run the training script\n    # both patches are aneeded to run the job of the circleci GPUs\n    with patch(\"sys.argv\", argv_injection.split()[1:]), patch(\n        \"metaseq.launcher.slurm.local_run\",\n        partial(local_run_mock, max_update=max_update, events=events),\n    ), patch.dict(\n        \"metaseq.launcher.opt_job_constants.MODEL_SIZES\",\n        # reduce the batch size for CUDA memory optimization\n        size_patch_dict,\n    ):\n        sweep_cli_main()\n\n\ndef local_run_mock(args, env, train_cmd, dry_run, max_update, events):\n    \"\"\"\n    The function introduces several pathces for the argumets of the\n    model training. These patches are needed to pass gpu tests on\n    circleci GPUs (empirical knowledge)\n    \"\"\"\n    train_cmd[train_cmd.index(\"--max-update\") + 1] = str(max_update)\n    train_cmd[train_cmd.index(\"--num-workers\") + 1] = \"1\"\n    train_cmd[train_cmd.index(\"--save-interval-updates\") + 1] = \"18\"\n\n    with patch(\"logging.Logger._log\", partialmethod(log_to_events, events=events)):\n        with patch.dict(\"os.environ\", env, clear=True):\n            with patch(\"sys.argv\", train_cmd[1:]):\n                train_cli_main()\n\n\ndef log_to_events(self, info, message, args, events, **kwargs):\n    \"\"\"\n    The function is used to collect logging info from the subprocesses\n    and store it in the 'events' variable, which is then passed over\n    to the main process for asserting that the model ran correctly\n    \"\"\"\n    print(self, message)\n    if isinstance(message, str):\n        events.append(\n            {\n                \"type\": \"log\",\n                \"message\": message,\n            }\n        )\n\n\nif __name__ == \"__main__\":\n    unittest.main()\n",
        "gpu_tests/test_checkpoint_saving.py": "# Copyright (c) Meta Platforms, Inc. and affiliates. All Rights Reserved.\n#\n# This source code is licensed under the MIT license found in the\n# LICENSE file in the root directory of this source tree.\n\nimport sys\nimport os\nimport subprocess\nimport json\nimport logging\nimport multiprocessing\nfrom functools import partial, partialmethod\nimport unittest\nfrom unittest.mock import patch, Mock, MagicMock\nfrom urllib.parse import urlparse\nimport torch\nfrom metaseq.dataclass.configs import DistributedTrainingConfig\nfrom metaseq.launcher.opt_baselines import cli_main as sweep_cli_main\nfrom metaseq.cli.train import cli_main as train_cli_main\nfrom metaseq.distributed.utils import distributed_main\nfrom metaseq.launcher.opt_job_constants import Size, M\nimport metaseq.utils as metaseq_utils\n\n\n@unittest.skipIf(not torch.cuda.is_available(), \"test requires 4 GPUs, none found\")\n@unittest.skipIf(\n    DistributedTrainingConfig.distributed_world_size != 4,\n    \"test requires 4 GPUs\",\n)\nclass TestCheckpointSavingAndUploading(unittest.TestCase):\n    def test_checkpoint_saving_and_uploading(self):\n        max_update_first_run = 20\n        multiprocessing.set_start_method(\"spawn\", force=True)\n        with torch.multiprocessing.Manager() as manager:\n            events = manager.list()\n            p = multiprocessing.Process(\n                target=run_training,\n                args=(\n                    events,\n                    max_update_first_run,\n                ),\n            )\n            p.start()\n            p.join()\n            events_first_run = list(events)\n\n        # check that training ran correctly\n        training_log_events = [\n            json.loads(event[\"message\"])\n            for event in events_first_run\n            if event[\"type\"] == \"log\" and event[\"message\"].startswith('{\"epoch\"')\n        ]\n        self.assertEqual(len(training_log_events), max_update_first_run)\n        self.assertEqual(\n            int(training_log_events[-1][\"num_updates\"]), max_update_first_run\n        )\n        self.assertAlmostEqual(float(training_log_events[-1][\"loss\"]), 14.574, 1)\n\n        # check that the correct checkpoints were created and uploaded\n        upload_events = [\n            event for event in events_first_run if event[\"type\"] == \"upload\"\n        ]\n        self.assertEqual(len(upload_events), 4)\n        checkpoint_dir = \"test-checkpoint\"\n        common_checkpoint_model_dir = upload_events[0][\"checkpoint_model_dir\"]\n        file_names_saved_azure = sorted(\n            [worker_cmd[\"checkpoint_file\"] for worker_cmd in upload_events]\n        )\n        expected_file_names = sorted(\n            [\n                \"checkpoint_18-model_part-0-shard0.pt\",\n                \"checkpoint_18-model_part-0-shard1.pt\",\n                \"checkpoint_18-model_part-1-shard0.pt\",\n                \"checkpoint_18-model_part-1-shard1.pt\",\n                # Behavior changed with https://github.com/facebookresearch/metaseq/pull/648\n                # \"checkpoint_last-model_part-0-shard0.pt\",\n                # \"checkpoint_last-model_part-0-shard1.pt\",\n                # \"checkpoint_last-model_part-1-shard0.pt\",\n                # \"checkpoint_last-model_part-1-shard1.pt\",\n            ]\n        )\n        self.assertEqual(file_names_saved_azure, expected_file_names)\n        for worker_cmd in upload_events:\n            self.assertEqual(\n                worker_cmd[\"command\"][:4],\n                [\n                    \"azcopy\",\n                    \"copy\",\n                    \"--cap-mbps\",\n                    \"96.0\",\n                ],\n            )\n            self.assertTrue(\n                os.path.basename(worker_cmd[\"command\"][-1]) in expected_file_names\n            )\n            self.assertEqual(\n                worker_cmd[\"checkpoint_model_dir\"], common_checkpoint_model_dir\n            )\n            self.assertEqual(worker_cmd[\"checkpoint_dir\"], checkpoint_dir)\n            self.assertEqual(worker_cmd[\"file_saved_locally\"], True)\n\n        # start second run, mock download the checkpoints from azure and keep training\n        max_update_second_run = 35\n        with torch.multiprocessing.Manager() as manager:\n            events = manager.list()\n            p = multiprocessing.Process(\n                target=run_training,\n                args=(\n                    events,\n                    max_update_second_run,\n                ),\n            )\n            p.start()\n            p.join()\n            events_second_run = list(events)\n\n        # check that that checkpoints were downloaded\n        download_events = [\n            event for event in events_second_run if event[\"type\"] == \"download\"\n        ]\n        file_names_downloaded = sorted(\n            [download[\"checkpoint_file\"] for download in download_events]\n        )\n        last_checkpoints = sorted(\n            [\n                \"checkpoint_last-model_part-0-shard0.pt\",\n                \"checkpoint_last-model_part-0-shard1.pt\",\n                \"checkpoint_last-model_part-1-shard0.pt\",\n                \"checkpoint_last-model_part-1-shard1.pt\",\n            ]\n        )\n        self.assertEqual(file_names_downloaded, last_checkpoints)\n        for download in download_events:\n            self.assertEqual(\n                download[\"blob_url\"], \"https://myaccount.blob.core.windows.net/test\"\n            )\n            self.assertEqual(\n                download[\"checkpoint_model_dir\"], common_checkpoint_model_dir\n            )\n            self.assertEqual(download[\"checkpoint_dir\"], checkpoint_dir)\n            self.assertTrue(download[\"checkpoint_file\"].endswith(download[\"suffix\"]))\n\n        # check that second training ran correctly\n        training_log_events = [\n            json.loads(event[\"message\"])\n            for event in events_second_run\n            if event[\"type\"] == \"log\" and event[\"message\"].startswith('{\"epoch\"')\n        ]\n        self.assertEqual(\n            len(training_log_events), max_update_second_run - max_update_first_run\n        )\n        self.assertEqual(\n            int(training_log_events[-1][\"num_updates\"]), max_update_second_run\n        )\n        self.assertAlmostEqual(float(training_log_events[-1][\"loss\"]), 12.666, 1)\n\n        # cleanup\n        cleanup_checkpoints = subprocess.Popen(\n            \"rm -r ./test-checkpoint\".split(),\n            stdout=subprocess.PIPE,\n            stderr=subprocess.PIPE,\n            universal_newlines=True,\n        )\n        _, _ = cleanup_checkpoints.communicate()\n\n\ndef run_training(events, max_update):\n    argv_injection = (\n        \"python3 metaseq/launcher/opt_baselines.py   \"\n        \"--prefix train.8m --model-size 8m --checkpoints-dir ./test-checkpoint \"\n        \"--tensorboard-logdir ./test-checkpoint --num-trials 1 --azure \"\n        \"--num-gpus 4 --num-nodes 1 --seed 1 --no-wandb \"\n        \"--local --disable-validation --max-epoch 5 --max-update 5 --benchmark \"\n        \"--full-azure-upload-path https://myaccount.blob.core.windows.net/test \"\n    )\n    with patch(\"sys.argv\", argv_injection.split()[1:]), patch(\n        \"metaseq.launcher.slurm.local_run\",\n        partial(local_run_mock, max_update=max_update, events=events),\n    ), patch.dict(\n        \"metaseq.launcher.opt_job_constants.MODEL_SIZES\",\n        {\"8m\": Size(4, 128, 2, 64, int(0.0625 * M), 1.0e-3, 2)},\n    ):\n        sweep_cli_main()\n\n\ndef local_run_mock(args, env, train_cmd, dry_run, max_update, events):\n    train_cmd[train_cmd.index(\"--max-update\") + 1] = str(max_update)\n    train_cmd[train_cmd.index(\"--log-interval\") + 1] = \"1\"\n    train_cmd[train_cmd.index(\"--save-interval-updates\") + 1] = \"18\"\n    train_cmd[train_cmd.index(\"--num-workers\") + 1] = \"1\"\n    with patch.dict(\"os.environ\", env, clear=True):\n        with patch(\"sys.argv\", train_cmd[1:]):\n            with patch(\n                \"metaseq.distributed.utils.distributed_main\",\n                partial(distributed_main_mock, events=events),\n            ):\n                train_cli_main()\n\n\ndef distributed_main_mock(i, main, cfg, kwargs, events):\n    # need to patch this seperately here, otherwise spawns won't be patched\n    with patch(\"logging.Logger._log\", partialmethod(log_to_events, events=events)):\n        with patch(\n            \"metaseq.cli.train._run_azcopy\",\n            partial(subprocess_run_mock, events=events),\n        ):\n            # need to mock this because the async part break the mock\n            with patch(\n                \"metaseq.cli.train.Trainer.save_checkpoint\",\n                partialmethod(save_checkpoint_mock),\n            ):\n                with patch(\"metaseq.cli.train.os.remove\"):\n                    mock_metaseq_internal = MagicMock()\n                    mock_metaseq_internal.azure_utils.download_recent_ckpt = partial(\n                        download_checkpoint_mock, events=events\n                    )\n                    sys.modules[\"metaseq_internal\"] = mock_metaseq_internal\n                    distributed_main(i, main, cfg, kwargs)\n\n\ndef download_checkpoint_mock(blob_url, checkpoint_path, suffix, events):\n    # mocks the download of the checkpoint from azure\n    _, checkpoint_dir, checkpoint_model_dir, checkpoint_file = checkpoint_path.split(\n        \"/\"\n    )\n    events.append(\n        {\n            \"type\": \"download\",\n            \"blob_url\": blob_url,\n            \"checkpoint_dir\": checkpoint_dir,\n            \"checkpoint_model_dir\": checkpoint_model_dir,\n            \"checkpoint_file\": checkpoint_file,\n            \"suffix\": suffix,\n        }\n    )\n\n    return True\n\n\ndef subprocess_run_mock(cmd, stdout, stderr, events):\n    source = cmd[4]\n    dest = cmd[5]\n\n    # Only interested in local -> remote transfers (not asserting remote copies/aliases)\n    if urlparse(source).scheme == \"\" and urlparse(dest).scheme == \"https\":\n        _, checkpoint_dir, checkpoint_model_dir, checkpoint_file = source.split(\"/\")\n        events.append(\n            {\n                \"type\": \"upload\",\n                \"command\": cmd[:4] + cmd[5:],\n                \"checkpoint_dir\": checkpoint_dir,\n                \"checkpoint_model_dir\": checkpoint_model_dir,\n                \"checkpoint_file\": checkpoint_file,\n                \"file_saved_locally\": os.path.exists(source),\n            }\n        )\n\n    res = Mock()\n    res.returncode = 0\n    return res\n\n\ndef save_checkpoint_mock(\n    self,\n    filename,\n    extra_state,\n    training_finished=False,\n    async_callback_fn=None,\n    files_to_symlink_to=None,\n):\n    logger = logging.getLogger(\"metaseq.trainer\")\n    \"\"\"Save all training state in a checkpoint file.\"\"\"\n    # call state_dict on all ranks in case it needs internal communication\n    state_dicts = self.state_dict(filename, training_finished)\n    for filename, state_dict in state_dicts.items():\n        logger.info(f\"Saving checkpoint to {filename}\")\n        state_dict = metaseq_utils.move_to_cpu(\n            state_dict,\n            # keep params in FP16 when training with --memory-efficient-fp16\n            cast_to_fp32=not self.cfg.common.memory_efficient_fp16,\n        )\n        state_dict[\"extra_state\"].update(extra_state)\n        if self.should_save_checkpoint_on_current_rank:\n            # remove async part which break the patch\n            # if not hasattr(self, \"async_checkpoint\"):\n            #     self.async_checkpoint = ThreadPoolExecutor(max_workers=1)\n\n            def perform_save():\n                try:\n                    logger.info(f\"Beginning asynchronous torch.save to {filename}\")\n                    torch.save(state_dict, filename)\n                    if async_callback_fn is not None:\n                        async_callback_fn(filename, files_to_symlink_to)\n                    logger.info(f\"Asynchronous torch.save to {filename} complete.\")\n                except Exception as e:\n                    logger.exception(f\"Asynchronous save failed: {e}\")\n\n            # remove async part which break the patch\n            # self.async_checkpoint.submit(perform_save)\n            perform_save()\n        logger.info(f\"Finished saving checkpoint to {filename}\")\n\n\ndef log_to_events(self, info, message, args, events, **kwargs):\n    print(self, message)\n    if isinstance(message, str):\n        events.append(\n            {\n                \"type\": \"log\",\n                \"message\": message,\n            }\n        )\n\n\nif __name__ == \"__main__\":\n    unittest.main()\n",
        "gpu_tests/test_checkpoint_saving_async.py": "# Copyright (c) Meta Platforms, Inc. and affiliates. All Rights Reserved.\n#\n# This source code is licensed under the MIT license found in the\n# LICENSE file in the root directory of this source tree.\n\nimport sys\nimport os\nimport subprocess\nimport json\nimport multiprocessing\nfrom functools import partial, partialmethod\nimport unittest\nfrom unittest.mock import patch, MagicMock\nimport torch\nfrom metaseq.dataclass.configs import DistributedTrainingConfig\nfrom metaseq.launcher.opt_baselines import cli_main as sweep_cli_main\nfrom metaseq.cli.train import cli_main as train_cli_main\nfrom metaseq.distributed.utils import distributed_main\nfrom metaseq.launcher.opt_job_constants import Size, M\nimport logging\n\n\n@unittest.skipIf(not torch.cuda.is_available(), \"test requires 4 GPUs, none found\")\n@unittest.skipIf(\n    DistributedTrainingConfig.distributed_world_size != 4,\n    \"test requires 4 GPUs\",\n)\nclass TestCheckpointSavingAndUploading(unittest.TestCase):\n    def test_checkpoint_saving_and_uploading(self):\n        max_update_first_run = 20\n        multiprocessing.set_start_method(\"spawn\", force=True)\n\n        with torch.multiprocessing.Manager() as manager:\n            events = manager.list()\n            p = multiprocessing.Process(\n                target=run_training,\n                args=(\n                    events,\n                    max_update_first_run,\n                ),\n            )\n            p.start()\n            p.join()\n            events_first_run = list(events)\n\n        # check that training ran correctly\n        training_log_events = [\n            json.loads(event[\"message\"])\n            for event in events_first_run\n            if event[\"type\"] == \"log\" and event[\"message\"].startswith('{\"epoch\"')\n        ]\n\n        self.assertEqual(len(training_log_events), max_update_first_run)\n        self.assertEqual(\n            int(training_log_events[-1][\"num_updates\"]), max_update_first_run\n        )\n        self.assertAlmostEqual(float(training_log_events[-1][\"loss\"]), 14.574, 1)\n\n        # check that the correct checkpoints were created\n        checkpoint_dir = \"test-checkpoint-local\"\n        common_checkpoint_model_dir = sorted(os.listdir(checkpoint_dir))[0]\n        assert common_checkpoint_model_dir.endswith(\".ngpu4\")\n\n        file_names_saved_local = []\n        for file in os.listdir(\n            os.path.join(checkpoint_dir, common_checkpoint_model_dir)\n        ):\n            if file.endswith(\".pt\"):\n                file_names_saved_local.append(file)\n        file_names_saved_local.sort()\n\n        expected_file_names = sorted(\n            [\n                \"checkpoint_18-model_part-0-shard0.pt\",\n                \"checkpoint_18-model_part-0-shard1.pt\",\n                \"checkpoint_18-model_part-1-shard0.pt\",\n                \"checkpoint_18-model_part-1-shard1.pt\",\n                \"checkpoint_last-model_part-0-shard0.pt\",\n                \"checkpoint_last-model_part-0-shard1.pt\",\n                \"checkpoint_last-model_part-1-shard0.pt\",\n                \"checkpoint_last-model_part-1-shard1.pt\",\n            ]\n        )\n        self.assertEqual(file_names_saved_local, expected_file_names)\n\n        # start second run, mock download the checkpoints from azure and keep training\n        max_update_second_run = 35\n\n        with torch.multiprocessing.Manager() as manager:\n            events = manager.list()\n            p = multiprocessing.Process(\n                target=run_training,\n                args=(\n                    events,\n                    max_update_second_run,\n                ),\n            )\n            p.start()\n            p.join()\n            events_second_run = list(events)\n\n        # check that second training ran correctly\n        training_log_events_second = [\n            json.loads(event[\"message\"])\n            for event in events_second_run\n            if event[\"type\"] == \"log\" and event[\"message\"].startswith('{\"epoch\"')\n        ]\n        self.assertEqual(\n            len(training_log_events_second),\n            max_update_second_run - max_update_first_run,\n        )\n        self.assertEqual(\n            int(training_log_events_second[-1][\"num_updates\"]), max_update_second_run\n        )\n        self.assertAlmostEqual(float(training_log_events_second[-1][\"loss\"]), 12.666, 1)\n\n        # cleanup\n        cleanup_checkpoints = subprocess.Popen(\n            \"rm -r ./test-checkpoint-local\".split(),\n            stdout=subprocess.PIPE,\n            stderr=subprocess.PIPE,\n            universal_newlines=True,\n        )\n        _, _ = cleanup_checkpoints.communicate()\n\n\ndef run_training(events, max_update):\n    argv_injection = (\n        \"python3 metaseq/launcher/opt_baselines.py   \"\n        \"--prefix train.8m    --model-size 8m    --checkpoints-dir ./test-checkpoint-local    \"\n        \"--tensorboard-logdir ./test-checkpoint-local    --num-trials 1    --azure   \"\n        \"--num-gpus 4 --num-nodes 1   --seed 1   \"\n        \"--local --disable-validation    --max-epoch 5    --max-update 5 --benchmark    \"\n    )\n\n    with patch(\"sys.argv\", argv_injection.split()[1:]), patch(\n        \"metaseq.launcher.slurm.local_run\",\n        partial(local_run_mock, max_update=max_update, events=events),\n    ), patch.dict(\n        \"metaseq.launcher.opt_job_constants.MODEL_SIZES\",\n        {\"8m\": Size(4, 128, 2, 64, int(0.0625 * M), 1.0e-3, 2)},\n    ):\n        sweep_cli_main()\n\n\ndef local_run_mock(args, env, train_cmd, dry_run, max_update, events):\n    train_cmd[train_cmd.index(\"--max-update\") + 1] = str(max_update)\n    train_cmd[train_cmd.index(\"--log-interval\") + 1] = \"1\"\n    train_cmd[train_cmd.index(\"--save-interval-updates\") + 1] = \"18\"\n    train_cmd[train_cmd.index(\"--num-workers\") + 1] = \"1\"\n\n    with patch.dict(\"os.environ\", env, clear=True):\n        with patch(\"sys.argv\", train_cmd[1:]):\n            with patch(\n                \"metaseq.distributed.utils.distributed_main\",\n                partial(distributed_main_mock, events=events),\n            ):\n                train_cli_main()\n\n\ndef distributed_main_mock(i, main, cfg, kwargs, events):\n    # need to patch this seperately here, otherwise spawns won't be patched\n    with patch.object(\n        logging.Logger,\n        \"_log\",\n        new=partialmethod(log_to_events, events=events),\n    ):\n        with patch(\"metaseq.cli.train.os.remove\"):\n            mock_metaseq_internal = MagicMock()\n            sys.modules[\"metaseq_internal\"] = mock_metaseq_internal\n            distributed_main(i, main, cfg, kwargs)\n\n\ndef log_to_events(self, info, message, args, events, **kwargs):\n    print(self, info, message)\n    if isinstance(message, str):\n        events.append(\n            {\n                \"type\": \"log\",\n                \"message\": message,\n            }\n        )\n\n\nif __name__ == \"__main__\":\n    unittest.main()\n",
        "gpu_tests/test_cli_train.py": "# Copyright (c) Meta Platforms, Inc. and affiliates. All Rights Reserved.\n#\n# This source code is licensed under the MIT license found in the\n# LICENSE file in the root directory of this source tree.\n\nimport array\nimport random\nimport os\nimport tempfile\nimport unittest\nfrom unittest.mock import patch, MagicMock\n\nfrom metaseq.cli.train import post_checkpoint_callback, _get_destination_path\nfrom metaseq.dataclass.configs import MetaseqConfig\n\n\ndef create_local_test_file(path, length=4096):\n    value = random.randint(0, 16)\n    with open(path, \"wb\") as f:\n        array.array(\"b\", [value] * length).tofile(f)\n\n\nclass TestPostCheckpointCallback(unittest.TestCase):\n    def test_destination_path(self):\n        self.assertEqual(\n            _get_destination_path(\"/path/ckpt.pt\", \"/other\"),\n            \"/other/ckpt.pt\",\n        )\n        self.assertEqual(\n            _get_destination_path(\n                \"/path/ckpt.pt\", \"https://acc.blob.core.windows.net/other?q=1\"\n            ),\n            \"https://acc.blob.core.windows.net/other/ckpt.pt?q=1\",\n        )\n        self.assertEqual(\n            _get_destination_path(\n                \"https://acc.blob.core.windows.net/path/ckpt.pt?q=1\", \"/other\"\n            ),\n            \"/other/ckpt.pt\",\n        )\n\n    def test_nfs_copy(self):\n        with tempfile.TemporaryDirectory() as local_dir, tempfile.TemporaryDirectory() as nfs_dir:\n            checkpoint_path = os.path.join(\n                local_dir, \"checkpoint_100-model_part-0-shard0.pt\"\n            )\n            create_local_test_file(checkpoint_path)\n\n            cfg = MetaseqConfig()\n            cfg.checkpoint.cloud_upload_path = f\"nfs:{nfs_dir}\"\n            # Prevent evals\n            cfg.checkpoint.nfs_eval_frequency = 0\n\n            post_checkpoint_callback(\n                cfg=cfg,\n                num_updates=10,\n                training_finished=False,\n                filename=checkpoint_path,\n                files_to_symlink_to=None,\n            )\n\n            expected_path = os.path.join(\n                nfs_dir, \"checkpoint_100/checkpoint-model_part-0-shard0.pt\"\n            )\n            self.assertTrue(\n                os.path.exists(expected_path), f\"File should exist: {expected_path}\"\n            )\n\n    def test_nfs_copy_with_symlinks(self):\n        with tempfile.TemporaryDirectory() as local_dir, tempfile.TemporaryDirectory() as nfs_dir:\n            checkpoint_path = os.path.join(\n                local_dir, \"checkpoint_100-model_part-0-shard0.pt\"\n            )\n            create_local_test_file(checkpoint_path)\n\n            cfg = MetaseqConfig()\n            cfg.checkpoint.cloud_upload_path = f\"nfs:{nfs_dir}\"\n            # Prevent evals\n            cfg.checkpoint.nfs_eval_frequency = 0\n\n            post_checkpoint_callback(\n                cfg=cfg,\n                num_updates=10,\n                training_finished=False,\n                filename=checkpoint_path,\n                files_to_symlink_to=[\n                    os.path.join(local_dir, \"checkpoint_last-model_part-0-shard0.pt\")\n                ],\n            )\n\n            self.assertTrue(\n                os.path.exists(\n                    os.path.join(\n                        nfs_dir, \"checkpoint_100/checkpoint-model_part-0-shard0.pt\"\n                    )\n                )\n            )\n            self.assertTrue(\n                os.path.islink(\n                    os.path.join(\n                        nfs_dir,\n                        \"checkpoint_last/checkpoint_last-model_part-0-shard0.pt\",\n                    )\n                )\n            )\n\n    def assert_azcopy(self, mock, src, dst):\n        def _match(c):\n            _, args, _ = c\n            cmd = args[0]\n            return cmd[-2] == src and cmd[-1] == dst\n\n        self.assertTrue(\n            any([_match(c) for c in mock.mock_calls]),\n            f\"Expected azcopy {src} -> {dst}\\n\\n{mock.mock_calls}\",\n        )\n\n    def test_azure_blob_with_symlinks(self):\n        mock_azcopy = MagicMock(return_value=MagicMock(returncode=0))\n        with patch(\"metaseq.cli.train._run_azcopy\", mock_azcopy):\n            with tempfile.TemporaryDirectory() as local_dir:\n                checkpoint_path = os.path.join(local_dir, \"checkpoint_10.pt\")\n                create_local_test_file(checkpoint_path)\n\n                upload_path = \"https://testaccount.blob.core.windows.net/dest?q=1\"\n                cfg = MetaseqConfig()\n                cfg.checkpoint.cloud_upload_path = upload_path\n\n                post_checkpoint_callback(\n                    cfg=cfg,\n                    num_updates=10,\n                    training_finished=False,\n                    filename=checkpoint_path,\n                    files_to_symlink_to=[os.path.join(local_dir, \"checkpoint_last.pt\")],\n                )\n\n                upload_src = \"https://testaccount.blob.core.windows.net/dest/checkpoint_10.pt?q=1\"\n                upload_dst = \"https://testaccount.blob.core.windows.net/dest/checkpoint_last.pt?q=1\"\n                self.assert_azcopy(mock_azcopy, checkpoint_path, upload_src)\n                self.assert_azcopy(mock_azcopy, upload_src, upload_dst)\n\n\nif __name__ == \"__main__\":\n    unittest.main()\n",
        "gpu_tests/test_fp16_optimizer.py": "# Copyright (c) Meta Platforms, Inc. and affiliates. All Rights Reserved.\n#\n# This source code is licensed under the MIT license found in the\n# LICENSE file in the root directory of this source tree.\n\nimport copy\nimport logging\nimport unittest\n\nimport torch\nfrom omegaconf import OmegaConf\n\nfrom metaseq.optim.fp16_optimizer import FP16Optimizer, MemoryEfficientFP16Optimizer\n\n\n@unittest.skipIf(not torch.cuda.is_available(), \"test requires a GPU\")\nclass TestGradientScaling(unittest.TestCase):\n    def setUp(self):\n        self.x = torch.tensor([2.0]).cuda().half()\n        weight = 3.0\n        bias = 5.0\n        self.error = 1.0\n        self.target = torch.tensor([self.x * weight + bias + self.error]).cuda().half()\n        self.loss_fn = torch.nn.L1Loss()\n\n        self.model = torch.nn.Linear(1, 1)\n        self.model.weight.data = torch.tensor([[weight]])\n        self.model.bias.data = torch.tensor([bias])\n        self.model.cuda().half()\n        self.params = list(self.model.parameters())\n\n        self.cfg_dls = OmegaConf.create(\n            {\n                \"optimization\": {\n                    \"lr\": [0.1],\n                },\n                \"optimizer\": {\n                    \"_name\": \"adam\",\n                    \"lr\": [0.1],\n                    \"adam_betas\": \"(0.9, 0.999)\",\n                    \"adam_eps\": 1e-8,\n                    \"weight_decay\": 0.0,\n                },\n                \"common\": {\n                    \"bf16\": False,\n                    \"fp16_init_scale\": 1,\n                    \"fp16_scale_window\": 1,\n                    \"fp16_scale_tolerance\": 1,\n                    \"threshold_loss_scale\": 1,\n                    \"min_loss_scale\": 1e-4,\n                },\n            }\n        )\n        logging.disable(logging.CRITICAL)\n\n    def tearDown(self):\n        logging.disable(logging.NOTSET)\n\n    def run_iter(self, model, params, optimizer):\n        optimizer.zero_grad()\n        y = model(self.x)\n        loss = self.loss_fn(y, self.target)\n        optimizer.backward(loss)\n        self.assertEqual(loss, torch.tensor(1.0, device=\"cuda:0\", dtype=torch.float16))\n\n        grad_norm = optimizer.clip_grad_norm(0)\n        self.assertAlmostEqual(grad_norm.item(), 2.2361, 4)\n\n        optimizer.step()\n        self.assertEqual(\n            model.weight,\n            torch.tensor(\n                [[3.0996]], device=\"cuda:0\", dtype=torch.float16, requires_grad=True\n            ),\n        )\n        self.assertEqual(\n            model.bias,\n            torch.tensor(\n                [5.1016], device=\"cuda:0\", dtype=torch.float16, requires_grad=True\n            ),\n        )\n        self.assertEqual(optimizer.scaler.loss_scale, 2.0)\n\n    def test_mixed_precision(self):\n        model = copy.deepcopy(self.model)\n        params = list(model.parameters())\n        optimizer = FP16Optimizer.build_optimizer(self.cfg_dls, params)\n\n        self.run_iter(model, params, optimizer)\n        self.assertTrue(\n            all(\n                torch.all(\n                    fp32_params.eq(\n                        torch.tensor(\n                            [3.1000, 5.1000], device=\"cuda:0\", requires_grad=True\n                        )\n                    )\n                )\n                for fp32_params in optimizer.fp32_params.values()\n            )\n        )\n\n    def test_memory_efficient(self):\n        model = copy.deepcopy(self.model)\n        params = list(model.parameters())\n        optimizer = MemoryEfficientFP16Optimizer.build_optimizer(self.cfg_dls, params)\n\n        self.run_iter(model, params, optimizer)\n\n\n@unittest.skipIf(not torch.cuda.is_available(), \"test requires a GPU\")\nclass TestBF16(unittest.TestCase):\n    def setUp(self):\n        self.x = torch.tensor([2.0]).cuda().bfloat16()\n        weight = 3.0\n        bias = 5.0\n        self.error = 1.0\n        self.target = (\n            torch.tensor([self.x * weight + bias + self.error]).cuda().bfloat16()\n        )\n        self.loss_fn = torch.nn.L1Loss()\n\n        self.model = torch.nn.Linear(1, 1)\n        self.model.weight.data = torch.tensor([[weight]])\n        self.model.bias.data = torch.tensor([bias])\n        self.model.cuda().bfloat16()\n        self.params = list(self.model.parameters())\n\n        self.cfg_dls = OmegaConf.create(\n            {\n                \"distributed_training\": {\n                    \"distributed_world_size\": 1,\n                },\n                \"optimization\": {\n                    \"lr\": [0.1],\n                    \"update_freq\": [1],\n                },\n                \"optimizer\": {\n                    \"_name\": \"adam\",\n                    \"lr\": [0.1],\n                    \"adam_betas\": \"(0.9, 0.999)\",\n                    \"adam_eps\": 1e-8,\n                    \"weight_decay\": 0.0,\n                },\n                \"common\": {\n                    \"model_parallel_size\": 1,\n                    \"bf16\": True,\n                    \"fp16\": True,\n                },\n            }\n        )\n        logging.disable(logging.CRITICAL)\n\n    def tearDown(self):\n        logging.disable(logging.NOTSET)\n\n    def run_iter(self, model, params, optimizer):\n        optimizer.zero_grad()\n        y = model(self.x)\n        loss = self.loss_fn(y, self.target)\n        optimizer.backward(loss)\n        self.assertEqual(loss, torch.tensor(1.0, device=\"cuda:0\", dtype=torch.bfloat16))\n\n        grad_norm = optimizer.clip_grad_norm(0)\n        self.assertAlmostEqual(grad_norm.item(), 2.2361, 4)\n\n        optimizer.step()\n        self.assertEqual(\n            model.weight,\n            torch.tensor(\n                [[3.0938]], device=\"cuda:0\", dtype=torch.bfloat16, requires_grad=True\n            ),\n        )\n        self.assertEqual(\n            model.bias,\n            torch.tensor(\n                [5.1016], device=\"cuda:0\", dtype=torch.bfloat16, requires_grad=True\n            ),\n        )\n        self.assertIsNone(optimizer.scaler)\n\n    def test_mixed_precision(self):\n        model = copy.deepcopy(self.model)\n        params = list(model.parameters())\n        optimizer = FP16Optimizer.build_optimizer(self.cfg_dls, params)\n\n        self.run_iter(model, params, optimizer)\n        self.assertTrue(\n            all(\n                torch.all(\n                    fp32_params.eq(\n                        torch.tensor(\n                            [3.1000, 5.1000], device=\"cuda:0\", requires_grad=True\n                        )\n                    )\n                )\n                for fp32_params in optimizer.fp32_params.values()\n            )\n        )\n\n\nif __name__ == \"__main__\":\n    unittest.main()\n",
        "gpu_tests/test_hf_compatibility.py": "# Copyright (c) Meta Platforms, Inc. and affiliates. All Rights Reserved.\n#\n# This source code is licensed under the MIT license found in the\n# LICENSE file in the root directory of this source tree.\n\nimport os\nimport unittest\n\nimport torch\nimport torch.distributed as dist\nimport torch.nn.functional as F\nfrom packaging import version\nfrom transformers import OPTForCausalLM\n\nfrom metaseq import checkpoint_utils, tasks, utils\nfrom metaseq.dataclass.configs import MetaseqConfig\nfrom metaseq.distributed import fsdp_enable_wrap, fsdp_wrap\nfrom metaseq.distributed import utils as distributed_utils\nfrom metaseq.hub_utils import tensorize_input, get_next_token, setup_vocab_and_merges\nfrom metaseq.modules.megatron.mpu import destroy_model_parallel\nfrom metaseq.scripts.convert_to_singleton import create_generation_config_with_defaults\n\nprompts = [\n    \"Today is a beautiful day and I want to \",\n    \"In the city of \",\n    \"Paris is the capital of France and \",\n    \"Computers and mobile phones have taken \",\n]\n\n\ndef load_mp_model_and_run_eval(cfg: MetaseqConfig, **kwargs):\n    \"\"\"\n    Function to load the model from the model_path and make predictions\n    based on the input list of prompts\n    Args:\n        cfg (MetaseqConfig): config file of the model\n\n    Returns:\n        trimmed_logits (torch.Tensor): list of logits based on the prompts\n        tokenizer (Tokenizer): tokenizer loaded from the model config\n    \"\"\"\n    vocab_file, merges_file, tokenizer = setup_vocab_and_merges(kwargs[\"model_path\"])\n    orig_dims = []\n\n    prompt_ids = []\n    for prompt in prompts:\n        input_ids = tensorize_input(tokenizer, prompt).cuda()\n        # Pad sequence to length 32 to avoid Megatron assertion errors\n        orig_dims.append(input_ids.shape[1])\n        input_ids = F.pad(\n            input=input_ids, pad=(0, 32 - input_ids.shape[1], 0, 0), value=1\n        )\n        prompt_ids.append(input_ids)\n\n    prompt_ids = torch.cat(prompt_ids).cuda()\n\n    task = tasks.setup_task(cfg.task)\n\n    def _build_model(cfg, task):\n        cfg.model.tensor_parallel_init_model_on_gpu = True\n        model = task.build_model(cfg.model).cuda()\n        return fsdp_wrap(model)\n\n    with fsdp_enable_wrap(\n        cfg.distributed_training,\n        use_sharded_state=cfg.distributed_training.use_sharded_state,\n    ):\n        if (\n            getattr(cfg.model, \"arch\", None) == \"transformer_lm_megatron\"\n            and cfg.common.model_parallel_size == 1\n        ):\n            cfg.model.arch = \"transformer_lm_gpt\"\n            cfg.model._name = \"transformer_lm_gpt\"\n        models, _model_args, _task = checkpoint_utils.load_model_ensemble_and_task(\n            utils.split_paths(cfg.common_eval.path),\n            arg_overrides=None,\n            task=task,\n            suffix=cfg.checkpoint.checkpoint_suffix,\n            strict=True,\n            num_shards=cfg.checkpoint.checkpoint_shard_count,\n            build_model_hook=_build_model,\n        )\n        model = models[0]\n\n    model.summon_full_params()\n    model = model.eval()\n\n    with torch.no_grad():\n        logits = model(prompt_ids)[0]\n\n    gathered_logits = [\n        torch.zeros_like(logits)\n        for _ in range(distributed_utils.get_model_parallel_world_size())\n    ]\n    torch.distributed.all_gather(\n        gathered_logits, logits, group=distributed_utils.get_global_group()\n    )\n    gathered_logits = torch.cat(gathered_logits, dim=2)\n\n    # Unwrap gathered logits into separate components for each prompt, and\n    # trim them to match orig_dims\n    trimmed_logits = [\n        logits[:orig_dim].unsqueeze(0)\n        for logits, orig_dim in zip(gathered_logits, orig_dims)\n    ]\n\n    # Destroy torch distributed process groups. This needs to be executed in each spawned process\n    # https://github.com/pytorch/pytorch/issues/48203\n    dist.destroy_process_group()\n    return trimmed_logits, tokenizer\n\n\n@unittest.skipIf(not torch.cuda.is_available(), \"test requires a GPU\")\n@unittest.skipIf(\n    version.parse(torch.__version__) < version.parse(\"1.9.1\"),\n    \"test requires a pytorch version of at least 1.9.1\",\n)\nclass TestHFCompatibility(unittest.TestCase):\n    \"\"\"\n    Test to check that loading the \"125m\" model from a singleton checkpoint\n    (meaning that it's a combined checkpoint with the convert_to_singleton.py)\n    using metaseq and generating predictions based on the prompt\n    is the same as loading the same checkpoint using OPTForCausalLM and\n    generating predictions from the same prompts.\n\n    \"\"\"\n\n    def test_model_parallel_metaseq_hf_compatibility(self):\n        model_path = os.path.join(os.path.dirname(__file__), \"125m\")\n\n        cfg = create_generation_config_with_defaults(model_path)\n\n        mp_logits_list, tokenizer = distributed_utils.call_main(\n            cfg, load_mp_model_and_run_eval, model_path=model_path\n        )\n\n        hf_model = OPTForCausalLM.from_pretrained(model_path).cuda()\n\n        for i, prompt in enumerate(prompts):\n            input_ids = tensorize_input(tokenizer, prompt).cuda()\n            with torch.no_grad():\n                logits_hf = hf_model(input_ids)[0]\n\n            metaseq_next_token = get_next_token(mp_logits_list[i], tokenizer)\n            hf_next_token = get_next_token(logits_hf, tokenizer)\n\n            # Assert that HF and metaseq versions of the same model predict the same logits\n            self.assertTrue(\n                torch.allclose(\n                    mp_logits_list[i].cpu().float(), logits_hf.cpu(), atol=1e-1\n                )\n            )\n\n            # Assert that HF and metaseq versions of the same model predict the same token\n            self.assertEqual(metaseq_next_token, hf_next_token)\n\n    def tearDown(self):\n        # Tear down model parallel\n        destroy_model_parallel()\n        distributed_utils._USE_MEGATRON = False\n\n\nif __name__ == \"__main__\":\n    unittest.main()\n",
        "gpu_tests/test_hub_utils.py": "# Copyright (c) Meta Platforms, Inc. and affiliates. All Rights Reserved.\n#\n# This source code is licensed under the MIT license found in the\n# LICENSE file in the root directory of this source tree.\n\nimport os\n\nimport numpy as np\nimport torch.distributed as dist\nfrom numpy.random import RandomState\n\nfrom metaseq.dataclass.configs import MetaseqConfig\nfrom metaseq.distributed import utils as distributed_utils\nfrom metaseq.hub_utils import GeneratorInterface\nfrom metaseq.modules.megatron.mpu import destroy_model_parallel\nfrom metaseq.scripts.convert_to_singleton import create_generation_config_with_defaults\n\nPROMPT = [133, 313, 1224, 15, 5, 856, 17527, 594, 98, 5, 11471, 3820, 19, 514, 4]\n\n\ndef generate_using_generator_interface(cfg: MetaseqConfig, **kwargs):\n    generator = GeneratorInterface(cfg)\n    models = generator.load_model()  # noqa: F841\n\n    request_object = {\n        \"inputs\": [PROMPT],\n        \"temperature\": 1.0,\n        \"max_tokens\": [0],\n        \"min_tokens\": [0],\n        \"top_p\": 1.0,\n        \"n\": 1,\n        \"best_of\": 1,\n        \"echo\": True,\n        \"logprobs\": 0,\n        \"seed\": 1,\n    }\n\n    generated_text = generator.generate(**request_object)\n\n    # Destroy torch distributed process groups. This needs to be executed in each spawned process\n    # https://github.com/pytorch/pytorch/issues/48203\n    dist.destroy_process_group()\n    return generated_text\n\n\n# TEST FUNCTIONS #\ndef test_generator_interface(data_regression, num_regression):\n    model_path = os.path.join(os.path.dirname(__file__), \"125m\")\n    cfg = create_generation_config_with_defaults(\n        model_path, ddp_backend=\"fully_sharded\"\n    )\n\n    overall_generation = distributed_utils.call_main(\n        cfg, generate_using_generator_interface\n    )\n    # We can potentially pass a list of inputs to the generate function.\n    # Here, we look at the first (and only) generation\n    [generation_for_prompt] = overall_generation\n    # We use best_of = 1, so we get only one beam search result\n    [generated_beam] = generation_for_prompt\n\n    ndarray_data = {\n        \"token_scores\": np.array(\n            [\n                np.nan if elem is None else elem\n                for elem in generated_beam[\"token_scores\"]\n            ]\n        )\n    }\n    generated_beam.pop(\"token_scores\")\n\n    num_regression.check(ndarray_data, default_tolerance=dict(atol=1e-2))\n    data_regression.check(generated_beam)\n\n\ndef test_filter_special(data_regression):\n    tokens = [123, 453, 653, 2, 345, 453]\n    # Assuming a vocab size of 10\n    vocab_size = 10\n\n    prng = RandomState(1234567890)\n\n    scores = prng.randn(len(tokens)).tolist()\n\n    distributions = prng.randn(len(tokens), vocab_size)\n    new_tokens, new_scores, distributions = GeneratorInterface._filter_special(\n        pad_token_ind=1,\n        special_token_inds=[0, 1, 2, 3],\n        tokens=tokens,\n        scores=scores,\n        distributions=distributions,\n    )\n\n    # Since we got a special token at index 3, only the first three tokens should be returned.\n    assert len(new_tokens) == 3\n    assert len(new_scores) == 3\n    data_regression.check(new_tokens, \"test_filter_special_new_tokens\")\n    data_regression.check(new_scores, \"test_filter_special_new_scores\")\n    data_regression.check(distributions.tolist(), \"test_filter_special_distributions\")\n\n\ndef teardown_function():\n    # Tear down model parallel\n    destroy_model_parallel()\n    distributed_utils._USE_MEGATRON = False\n",
        "gpu_tests/test_model_parallel_mp1_mp2.py": "# Copyright (c) Meta Platforms, Inc. and affiliates. All Rights Reserved.\n#\n# This source code is licensed under the MIT license found in the\n# LICENSE file in the root directory of this source tree.\n\nimport subprocess\nimport json\nimport multiprocessing\nfrom functools import partial, partialmethod\nimport unittest\nfrom unittest.mock import patch\nimport torch\nfrom metaseq.dataclass.configs import DistributedTrainingConfig\nfrom metaseq.launcher.opt_baselines import cli_main as sweep_cli_main\nfrom metaseq.cli.train import cli_main as train_cli_main\nfrom metaseq.launcher.opt_job_constants import Size, M\n\n\n@unittest.skipIf(not torch.cuda.is_available(), \"test requires 4 GPUs, none found\")\n@unittest.skipIf(\n    DistributedTrainingConfig.distributed_world_size != 4,\n    \"test requires 4 GPUs\",\n)\nclass TestModelParallelMP1(unittest.TestCase):\n    \"\"\"\n    The test will verify that the model can be trained with\n    model_parallel = 1\n    The test checks hat the number of trianing steps performed is correct\n    and that the required loss is achieved on the last iteration\n    \"\"\"\n\n    def test_model_parallel_mp1(self):\n        # parameters to train an mp1 model\n        argv_injection = (\n            \"python3 metaseq/launcher/opt_baselines.py   \"\n            \"--prefix train.8m    --model-size 8m_mp1    --checkpoints-dir ./test-checkpoint    \"\n            \"--tensorboard-logdir ./test-checkpoint    --num-trials 1    --azure   \"\n            \"--num-gpus 4 --num-nodes 1   --seed 1   \"\n            \"--local --disable-validation    --max-epoch 5    --max-update 5 --benchmark    \"\n        )\n        max_update_first_run = 20\n        size_patch_dict = {\"8m_mp1\": Size(4, 128, 2, 64, int(0.03125 * M), 1.0e-3, 1)}\n\n        training_log_events = self._test_model_parallel(\n            max_update_first_run=max_update_first_run,\n            argv_injection=argv_injection,\n            size_patch_dict=size_patch_dict,\n        )\n\n        # check that training ran correctly\n        # check that the number of updates was correct\n        self.assertNotEqual(training_log_events, [])\n        self.assertIsNotNone(training_log_events[-1][\"num_updates\"])\n        self.assertEqual(\n            int(training_log_events[-1][\"num_updates\"]), max_update_first_run\n        )\n        # check the achieved loss is correct\n        loss_val = float(training_log_events[-1][\"loss\"])\n        self.assertAlmostEqual(loss_val, 14.736, 1)  # 1 digit precision\n\n    def test_model_parallel_mp2(self):\n        # parameters to train an mp2 model\n        argv_injection = (\n            \"python3 metaseq/launcher/opt_baselines.py   \"\n            \"--prefix train.8m    --model-size 8m    --checkpoints-dir ./test-checkpoint    \"\n            \"--tensorboard-logdir ./test-checkpoint    --num-trials 1    --azure   \"\n            \"--num-gpus 4 --num-nodes 1   --seed 1   \"\n            \"--local --disable-validation    --max-epoch 5    --max-update 5 --benchmark    \"\n        )\n        max_update_first_run = 20\n        size_patch_dict = {\"8m\": Size(4, 128, 2, 64, int(0.03125 * M), 1.0e-3, 2)}\n\n        training_log_events = self._test_model_parallel(\n            max_update_first_run=max_update_first_run,\n            argv_injection=argv_injection,\n            size_patch_dict=size_patch_dict,\n        )\n\n        # check that training ran correctly\n        # check that the number of updates was correct\n        self.assertNotEqual(training_log_events, [])\n        self.assertIsNotNone(training_log_events[-1][\"num_updates\"])\n        self.assertEqual(\n            int(training_log_events[-1][\"num_updates\"]), max_update_first_run\n        )\n        # check the achieved loss is correct\n        loss_val = float(training_log_events[-1][\"loss\"])\n        self.assertAlmostEqual(loss_val, 14.744, 1)  # 1 digit precision\n\n    def _test_model_parallel(\n        self, max_update_first_run, argv_injection, size_patch_dict\n    ):\n        \"\"\"\n        Helper function to run the test\n        \"\"\"\n        # start the process for the model run\n        multiprocessing.set_start_method(\"spawn\", force=True)\n        with torch.multiprocessing.Manager() as manager:\n            events = manager.list()\n            p = multiprocessing.Process(\n                target=run_training,\n                args=(max_update_first_run, events, argv_injection, size_patch_dict),\n            )\n            p.start()\n            p.join()\n            events_first_run = list(events)\n\n        # cleanup of the checkpoints files\n        cleanup_checkpoints = subprocess.Popen(\n            \"rm -r ./test-checkpoint\".split(),\n            stdout=subprocess.PIPE,\n            stderr=subprocess.PIPE,\n            universal_newlines=True,\n        )\n        _, _ = cleanup_checkpoints.communicate()\n\n        # parse the log events from the log_to_events()\n        training_log_events = [\n            json.loads(event[\"message\"])\n            for event in events_first_run\n            if event[\"type\"] == \"log\" and event[\"message\"].startswith('{\"epoch\"')\n        ]\n\n        return training_log_events\n\n\ndef run_training(max_update, events, argv_injection, size_patch_dict):\n    # clean any unused cach to reduce CUDA OOM\n    torch.cuda.empty_cache()\n    # main arguments to run the training script\n    # both patches are aneeded to run the job of the circleci GPUs\n    with patch(\"sys.argv\", argv_injection.split()[1:]), patch(\n        \"metaseq.launcher.slurm.local_run\",\n        partial(local_run_mock, max_update=max_update, events=events),\n    ), patch.dict(\n        \"metaseq.launcher.opt_job_constants.MODEL_SIZES\",\n        # reduce the batch size for CUDA memory optimization\n        size_patch_dict,\n    ):\n        sweep_cli_main()\n\n\ndef local_run_mock(args, env, train_cmd, dry_run, max_update, events):\n    \"\"\"\n    The function introduces several pathces for the argumets of the\n    model training. These patches are needed to pass gpu tests on\n    circleci GPUs (empirical knowledge)\n    \"\"\"\n    train_cmd[train_cmd.index(\"--max-update\") + 1] = str(max_update)\n    train_cmd[train_cmd.index(\"--num-workers\") + 1] = \"1\"\n\n    with patch(\"logging.Logger._log\", partialmethod(log_to_events, events=events)):\n        with patch.dict(\"os.environ\", env, clear=True):\n            with patch(\"sys.argv\", train_cmd[1:]):\n                train_cli_main()\n\n\ndef log_to_events(self, info, message, args, events, **kwargs):\n    \"\"\"\n    The function is used to collect logging info from the subprocesses\n    and store it in the 'events' variable, which is then passed over\n    to the main process for asserting that the model ran correctly\n    \"\"\"\n    print(self, message)\n    if isinstance(message, str):\n        events.append(\n            {\n                \"type\": \"log\",\n                \"message\": message,\n            }\n        )\n\n\nif __name__ == \"__main__\":\n    unittest.main()\n",
        "gpu_tests/utils.py": "# Copyright (c) Meta Platforms, Inc. and affiliates. All Rights Reserved.\n#\n# This source code is licensed under the MIT license found in the\n# LICENSE file in the root directory of this source tree.\n\"\"\"\nUtils for gpu_tests.\n\"\"\"\nimport os\nimport unittest\n\n\ndef is_this_circleci():\n    \"\"\"\n    Return if we are currently running in CircleCI.\n    \"\"\"\n    return bool(os.environ.get(\"CIRCLECI\"))\n\n\ndef skipIfCircleCI(testfn, reason=\"Test disabled in CircleCI\"):\n    \"\"\"\n    Decorate a test to skip if running on CircleCI.\n    \"\"\"\n    return unittest.skipIf(is_this_circleci(), reason)(testfn)\n",
        "metaseq/__init__.py": "# Copyright (c) Meta Platforms, Inc. and affiliates. All Rights Reserved.\n#\n# This source code is licensed under the MIT license found in the\n# LICENSE file in the root directory of this source tree.\n\"\"\"isort:skip_file\"\"\"\n\nimport os\nimport sys\n\ntry:\n    from .version import __version__  # noqa\nexcept ImportError:\n    version_txt = os.path.join(os.path.dirname(__file__), \"version.txt\")\n    with open(version_txt) as f:\n        __version__ = f.read().strip()\n\n__all__ = [\"pdb\"]\n\n# backwards compatibility to support `from metaseq.X import Y`\nfrom metaseq.distributed import utils as distributed_utils\nfrom metaseq.logging import meters, metrics  # noqa\n\nsys.modules[\"metaseq.distributed_utils\"] = distributed_utils\nsys.modules[\"metaseq.meters\"] = meters\nsys.modules[\"metaseq.metrics\"] = metrics\n\n# initialize hydra\nfrom metaseq.dataclass.initialize import hydra_init  # noqa: E402\n\nhydra_init()\n\nimport metaseq.criterions  # noqa\nimport metaseq.distributed  # noqa\nimport metaseq.models  # noqa\nimport metaseq.modules  # noqa\nimport metaseq.optim  # noqa\nimport metaseq.optim.lr_scheduler  # noqa\nimport metaseq.pdb  # noqa\nimport metaseq.tasks  # noqa\n\nimport metaseq.benchmark  # noqa\n",
        "metaseq/benchmark/__init__.py": "# Copyright (c) Meta Platforms, Inc. and affiliates. All Rights Reserved.\n#\n# This source code is licensed under the MIT license found in the\n# LICENSE file in the root directory of this source tree.\n\nfrom . import dummy_lm  # noqa\n",
        "metaseq/benchmark/dummy_lm.py": "# Copyright (c) Meta Platforms, Inc. and affiliates. All Rights Reserved.\n#\n# This source code is licensed under the MIT license found in the\n# LICENSE file in the root directory of this source tree.\n\nimport logging\nfrom dataclasses import dataclass, field\nfrom typing import Optional\n\nimport numpy as np\nimport torch\nfrom omegaconf import II\n\nfrom metaseq.data import Dictionary, BaseDataset\nfrom metaseq.dataclass import MetaseqDataclass\nfrom metaseq.tasks import BaseTask, register_task\n\nlogger = logging.getLogger(__name__)\n\n\n@dataclass\nclass DummyLMConfig(MetaseqDataclass):\n    dict_size: int = 49996\n    dataset_size: int = 100000\n    tokens_per_sample: int = field(\n        default=512, metadata={\"help\": \"max sequence length\"}\n    )\n    add_bos_token: bool = False\n    batch_size: Optional[int] = II(\"dataset.batch_size\")\n    batch_size_valid: Optional[int] = II(\"dataset.batch_size_valid\")\n    max_tokens: Optional[int] = II(\"dataset.max_tokens\")\n    max_target_positions: int = II(\"task.tokens_per_sample\")\n\n\n@register_task(\"dummy_lm\", dataclass=DummyLMConfig)\nclass DummyLMTask(BaseTask):\n    def __init__(self, cfg: DummyLMConfig):\n        super().__init__(cfg)\n\n        # load dictionary\n        self.dictionary = Dictionary()\n        for i in range(cfg.dict_size):\n            self.dictionary.add_symbol(\"word{}\".format(i))\n        self.dictionary.pad_to_multiple_(8)  # often faster if divisible by 8\n        logger.info(\"dictionary: {} types\".format(len(self.dictionary)))\n\n        seq = torch.arange(cfg.tokens_per_sample + 1) + self.dictionary.pad() + 1\n\n        self.dummy_src = seq[:-1]\n        self.dummy_tgt = seq[1:]\n\n    def load_dataset(self, split, epoch=1, combine=False, **kwargs):\n        \"\"\"Load a given dataset split.\n        Args:\n            split (str): name of the split (e.g., train, valid, test)\n        \"\"\"\n        dataset_size = self.cfg.dataset_size\n        if self.cfg.batch_size is not None:\n            if split == \"train\" or self.cfg.batch_size_valid is None:\n                bsz = self.cfg.batch_size\n            else:\n                bsz = self.cfg.batch_size_valid\n                dataset_size = 1\n        else:\n            bsz = max(1, self.cfg.max_tokens // self.cfg.tokens_per_sample)\n        self.datasets[split] = DummyDataset(\n            {\n                \"id\": 1,\n                \"net_input\": {\n                    \"src_tokens\": torch.stack([self.dummy_src for _ in range(bsz)]),\n                    \"src_lengths\": torch.full(\n                        (bsz,), self.cfg.tokens_per_sample, dtype=torch.long\n                    ),\n                },\n                \"target\": torch.stack([self.dummy_tgt for _ in range(bsz)]),\n                \"nsentences\": bsz,\n                \"ntokens\": bsz * self.cfg.tokens_per_sample,\n            },\n            num_items=dataset_size,\n            item_size=self.cfg.tokens_per_sample,\n        )\n\n    @property\n    def source_dictionary(self):\n        return self.dictionary\n\n    @property\n    def target_dictionary(self):\n        return self.dictionary\n\n\nclass DummyDataset(BaseDataset):\n    def __init__(self, batch, num_items, item_size):\n        super().__init__()\n        self.batch = batch\n        self.num_items = num_items\n        self.item_size = item_size\n\n    def __getitem__(self, index):\n        return index\n\n    def __len__(self):\n        return self.num_items\n\n    def collater(self, samples):\n        return self.batch\n\n    @property\n    def sizes(self):\n        return np.array([self.item_size] * self.num_items)\n\n    def num_tokens(self, index):\n        return self.item_size\n\n    def size(self, index):\n        return self.item_size\n\n    def ordered_indices(self):\n        return np.arange(self.num_items)\n\n    @property\n    def supports_prefetch(self):\n        return False\n\n    def set_epoch(self, epoch):\n        pass\n",
        "metaseq/benchmark/generator.py": "# Copyright (c) Meta Platforms, Inc. and affiliates. All Rights Reserved.\n#\n# This source code is licensed under the MIT license found in the\n# LICENSE file in the root directory of this source tree.\n\nimport logging\nimport random\nimport socket\nimport sys\nfrom typing import Tuple\n\nimport numpy as np\nimport torch\nfrom torch.profiler.profiler import (\n    ProfilerActivity,\n    schedule,\n    tensorboard_trace_handler,\n)\nfrom tqdm import tqdm\n\nfrom metaseq import options\nfrom metaseq.dataclass.configs import MetaseqConfig\nfrom metaseq.dataclass.utils import convert_namespace_to_omegaconf\nfrom metaseq.distributed import utils as distributed_utils\nfrom metaseq.hub_utils import GeneratorInterface\n\nlogging.basicConfig(format=\"%(asctime)s | %(name)s | %(message)s\", level=logging.INFO)\nlogger: logging.Logger = logging.getLogger(\"metaseq.benchmark.generator\")\n\nWARMUP_STEPS = 5\nNUM_REPEATS = 5\ngenerator = None\n\n\ndef main(cfg: MetaseqConfig) -> None:\n    global generator\n    torch.manual_seed(random.randint(1, 20000))\n\n    logger.info(\"Instantiating a generator interface and loading a model\")\n    generator = GeneratorInterface(cfg)\n    generator.load_model()\n\n    batch_size = cfg.dataset.batch_size\n    input_length = cfg.task.max_source_positions\n    output_length = cfg.task.max_target_positions\n\n    logger.info(\"Running warm-up steps before benchmarking\")\n    for _ in range(WARMUP_STEPS):\n        inputs = np.random.randint(100, 50000, size=(batch_size, input_length))\n        generator.generate(inputs, max_tokens=[output_length] * batch_size)\n\n    logger.info(\n        f\"Benchmarking with batch size {batch_size}, input length \"\n        f\"{input_length}, and output length {output_length}\"\n    )\n    time_elapsed, peak_memory = benchmark(batch_size, input_length, output_length)\n    if torch.distributed.get_rank() == 0:\n        print(f\"Latency: {time_elapsed.mean():.4f} +/- {time_elapsed.std():.4f} ms\")\n        print(f\"Peak memory usage: {peak_memory / 1024 ** 3:.4f} GB\")\n\n\ndef benchmark(\n    batch_size: int, input_length: int, output_length: int\n) -> Tuple[np.ndarray, np.ndarray]:\n    time_elapsed = []\n    for _ in range(NUM_REPEATS):\n        start = torch.cuda.Event(enable_timing=True)\n        end = torch.cuda.Event(enable_timing=True)\n        start.record()\n\n        generator.generate(\n            inputs=np.random.randint(100, 50000, size=(batch_size, input_length)),\n            max_tokens=[output_length] * batch_size,\n            temperature=cfg.generation.temperature,\n            n=cfg.generation.beam,\n            top_p=0.9,\n        )\n        end.record()\n        torch.cuda.synchronize()\n        time_elapsed.append(start.elapsed_time(end))\n\n    peak_memory = torch.tensor(\n        torch.cuda.memory_stats()[\"allocated_bytes.all.peak\"],\n        device=torch.cuda.current_device(),\n    )\n    torch.distributed.all_reduce(peak_memory)\n    return np.array(time_elapsed), peak_memory.cpu().numpy()\n\n\ndef profile(batch_size: int, input_length: int, output_length: int) -> None:\n    tracing_schedule = schedule(skip_first=5, wait=5, warmup=5, active=2, repeat=1)\n    worker = socket.gethostname() + \"_\" + str(torch.distributed.get_rank())\n    trace_handler = tensorboard_trace_handler(\n        dir_name=\"traces\", worker_name=worker, use_gzip=True\n    )\n\n    with torch.profiler.profile(\n        activities=[ProfilerActivity.CPU, ProfilerActivity.CUDA],\n        profile_memory=True,\n        record_shapes=True,\n        schedule=tracing_schedule,\n        on_trace_ready=trace_handler,\n        with_stack=True,\n    ) as prof:\n        for _ in tqdm(range(17)):\n            generator.generate(\n                inputs=np.random.randint(100, 50000, size=(batch_size, input_length)),\n                max_tokens=[output_length] * batch_size,\n                temperature=cfg.generation.temperature,\n                n=cfg.generation.beam,\n                top_p=0.9,\n            )\n            prof.step()\n\n\nif __name__ == \"__main__\":\n    \"\"\"\n    Example usage:\n        python metaseq/benchmark/generator.py \\\n        --merges-filename /data/checkpoints/gpt2-merges.txt \\\n        --vocab-filename /data/checkpoints/gpt2-vocab.json \\\n        --path /data/checkpoints/opt-125m/reshard-no-os/reshard.pt \\\n        --model-parallel-size 2 --distributed-world-size 2 \\\n        --beam 1 --batch-size 4 --max-source-positions 4 --max-target-positions 16\n    \"\"\"\n\n    parser = options.get_generation_parser()\n    parser.set_defaults(lr_scheduler=None, criterion=None)\n    LAUNCH_ARGS = [\"--task language_modeling\", \"--bpe hf_byte_bpe\", \"/tmp\"]\n    launch_args = sys.argv[1:] + [item for arg in LAUNCH_ARGS for item in arg.split()]\n    args = options.parse_args_and_arch(parser, input_args=launch_args)\n\n    args.bpe_merges = args.merges_filename\n    args.bpe_vocab = args.vocab_filename\n    cfg = convert_namespace_to_omegaconf(args)\n    distributed_utils.call_main(cfg, main)\n",
        "metaseq/checkpoint_utils.py": "# Copyright (c) Meta Platforms, Inc. and affiliates. All Rights Reserved.\n#\n# This source code is licensed under the MIT license found in the\n# LICENSE file in the root directory of this source tree.\n\nimport ast\nimport collections\nimport logging\nimport os\nimport re\nimport socket\nfrom typing import Any, Dict, List, Optional, Tuple\nimport math\nimport torch\nfrom omegaconf import OmegaConf\n\nfrom metaseq.dataclass.configs import CheckpointConfig\nfrom metaseq.dataclass.utils import overwrite_args_by_name, overwrite_keys_not_present\nfrom metaseq.distributed import utils as distributed_utils\nfrom metaseq.file_io import PathManager, torch_load_cpu\nfrom metaseq.launcher.opt_job_constants import ComputeEnvs\n\nlogger = logging.getLogger(__name__)\n\n\nOPT_KEY = \"last_optimizer_state\"\n\n\ndef save_checkpoint(\n    cfg: CheckpointConfig,\n    trainer,\n    epoch_itr,\n    training_finished=False,\n    async_callback_fn=None,\n):\n    from metaseq import meters\n\n    # only one worker should attempt to create the required dir\n    if distributed_utils.get_global_rank() == 0:\n        os.makedirs(cfg.save_dir, exist_ok=True)\n\n    trainer.consolidate_optimizer()\n\n    if not trainer.should_save_checkpoint_on_current_rank:\n        return\n\n    write_timer = meters.StopwatchMeter()\n    write_timer.start()\n\n    epoch = epoch_itr.epoch\n    end_of_epoch = epoch_itr.end_of_epoch()\n    updates = trainer.get_num_updates()\n\n    logger.info(f\"Preparing to save checkpoint for epoch {epoch} @ {updates} updates\")\n\n    suffix = trainer.checkpoint_suffix\n    checkpoint_conds = collections.OrderedDict()\n\n    save_for_epoch = (\n        end_of_epoch\n        and cfg.save_interval_epochs > 0\n        and epoch % cfg.save_interval_epochs == 0\n    )\n\n    save_locally = (\n        cfg.local_save_interval_updates > 0\n        and updates % cfg.local_save_interval_updates == 0\n    )\n    save_to_NFS = (\n        cfg.save_interval_updates > 0 and updates % cfg.save_interval_updates == 0\n    )\n\n    save_for_updates = not end_of_epoch and (save_to_NFS or save_locally)\n\n    checkpoint_conds[f\"checkpoint{epoch}{suffix}.pt\"] = save_for_epoch\n    checkpoint_conds[f\"checkpoint_{updates}{suffix}.pt\"] = save_for_updates\n    checkpoint_conds[f\"checkpoint_last{suffix}.pt\"] = (\n        (training_finished and cfg.save_last_checkpoint)\n        or save_for_epoch\n        or save_for_updates\n    )\n\n    extra_state = {\"train_iterator\": epoch_itr.state_dict()}\n\n    checkpoints = [\n        os.path.join(cfg.save_dir, fn) for fn, cond in checkpoint_conds.items() if cond\n    ]\n\n    if len(checkpoints) > 0:\n        if PathManager.islink(checkpoints[0]):\n            PathManager.rm(checkpoints[0])\n\n        trainer.save_checkpoint(\n            checkpoints[0],\n            extra_state,\n            training_finished=training_finished,\n            async_callback_fn=async_callback_fn if save_to_NFS else None,\n            files_to_symlink_to=checkpoints[1:] if len(checkpoints) > 1 else None,\n        )\n\n        write_timer.stop()\n        logger.info(\n            f\"Saved checkpoint {checkpoints[0]} (epoch {epoch} @ {updates} updates) \"\n            f\"(writing took {write_timer.sum} seconds)\"\n        )\n\n        # See if there's any older checkpoints to delete after saving a new one.\n        # Only deletes if keep_last_updates > 0 or keep_last_epochs > 0 (default -1 for both).\n        delete_old_checkpoint_files(cfg, end_of_epoch, suffix)\n\n\ndef delete_old_checkpoint_files(cfg: CheckpointConfig, end_of_epoch: bool, suffix: str):\n    if not end_of_epoch and cfg.keep_last_updates > 0:\n        # remove old checkpoints; checkpoints are sorted in descending order\n        checkpoints = checkpoint_paths(\n            cfg.save_dir, pattern=r\"checkpoint_\\d+_(\\d+){}\\.pt\".format(suffix)\n        )\n        for old_chk in checkpoints[cfg.keep_last_updates :]:\n            if os.path.lexists(old_chk):\n                os.remove(old_chk)\n\n    if cfg.keep_last_epochs > 0:\n        # remove old epoch checkpoints; checkpoints are sorted in descending order\n        checkpoints = checkpoint_paths(\n            cfg.save_dir, pattern=r\"checkpoint(\\d+){}\\.pt\".format(suffix)\n        )\n        for old_chk in checkpoints[cfg.keep_last_epochs :]:\n            if os.path.lexists(old_chk):\n                os.remove(old_chk)\n\n\n# Reference:\n# https://github.com/facebookresearch/fairseq/blob/0338cdc3094ca7d29ff4d36d64791f7b4e4b5e6e/fairseq/checkpoint_utils.py#L538\ndef checkpoint_paths(path, pattern=r\"checkpoint(\\d+)\\.pt\"):\n    \"\"\"Retrieves all checkpoints found in `path` directory.\n    Checkpoints are identified by matching filename to the specified pattern. If\n    the pattern contains groups, the result will be sorted by the first group in\n    descending order.\n    \"\"\"\n    pt_regexp = re.compile(pattern)\n    files = os.listdir(path)\n\n    entries = []\n    for i, f in enumerate(files):\n        m = pt_regexp.fullmatch(f)\n        if m is not None:\n            idx = float(m.group(1)) if len(m.groups()) > 0 else i\n            entries.append((idx, m.group(0)))\n    return [os.path.join(path, x[1]) for x in sorted(entries, reverse=True)]\n\n\ndef load_checkpoint(cfg: CheckpointConfig, trainer, **passthrough_args):\n    \"\"\"\n    Load a checkpoint and restore the training iterator.\n\n    *passthrough_args* will be passed through to\n    ``trainer.get_train_iterator``.\n    \"\"\"\n\n    reset_optimizer = cfg.reset_optimizer\n    reset_lr_scheduler = cfg.reset_lr_scheduler\n    optimizer_overrides = ast.literal_eval(cfg.optimizer_overrides)\n    reset_meters = cfg.reset_meters\n    reset_dataloader = cfg.reset_dataloader\n\n    if cfg.finetune_from_model is not None and (\n        reset_optimizer or reset_lr_scheduler or reset_meters or reset_dataloader\n    ):\n        raise ValueError(\n            \"--finetune-from-model can not be set together with either --reset-optimizer\"\n            \" or reset_lr_scheduler or reset_meters or reset_dataloader\"\n        )\n\n    suffix = trainer.checkpoint_suffix\n    default_restore_file = \"checkpoint_last.pt\"\n    # default to loading from restore file.\n    if cfg.restore_file == default_restore_file:\n        checkpoint_path_to_load = os.path.join(\n            cfg.save_dir, \"checkpoint_last{}.pt\".format(suffix)\n        )\n        first_launch = not PathManager.exists(checkpoint_path_to_load)\n        if cfg.finetune_from_model is not None and first_launch:\n            # if there is no last checkpoint to restore, start the finetune from pretrained model\n            # else just use usual logic to load checkpoint, e.g. restart from last checkpoint and etc.\n            reset_optimizer = True\n            reset_lr_scheduler = True\n            reset_meters = True\n            reset_dataloader = True\n            checkpoint_path_to_load = None\n            if PathManager.exists(cfg.finetune_from_model):\n                checkpoint_path_to_load = cfg.finetune_from_model\n            elif suffix is not None:  # check for sharded version\n                sharded_path = cfg.finetune_from_model.replace(\".pt\", suffix + \".pt\")\n                if PathManager.exists(sharded_path):\n                    checkpoint_path_to_load = sharded_path\n            if checkpoint_path_to_load is None:\n                raise ValueError(\n                    f\"--finetune-from-model {cfg.finetune_from_model} does not exist either as is or sharded\"\n                )\n\n            logger.info(\n                f\"loading pretrained model from {checkpoint_path_to_load}: \"\n                \"optimizer, lr scheduler, meters, dataloader will be reset\"\n            )\n    elif suffix is not None:\n        checkpoint_path_to_load = cfg.restore_file.replace(\".pt\", suffix + \".pt\")\n    else:\n        checkpoint_path_to_load = cfg.restore_file\n\n    if cfg.restore_file != default_restore_file and cfg.finetune_from_model:\n        raise ValueError(\n            \"--finetune-from-model and --restore-file (non-default value) \"\n            \"can not be specified together: \" + str(cfg)\n        )\n\n    # Azure logic\n    try:\n        from metaseq_internal import azure_utils\n\n        has_metaseq_internal = True\n    except ImportError:\n        has_metaseq_internal = False\n        logger.warning(\n            \"Proceeding without metaseq-internal installed! Please check if you need this!\"\n        )\n\n    # TODO(susanz): fix all of this spagetti, split out logic by env\n    # Note that we compare by value since ComputeEnvs may be imported from metaseq_internal\n\n    if cfg.cloud_upload_path:\n        if cfg.cloud_upload_path.startswith(\"nfs:\"):\n            checkpoint_path_to_load = os.path.join(\n                cfg.save_dir, \"checkpoint_last{}.pt\".format(suffix)\n            )\n            nfs_path = cfg.cloud_upload_path[4:]\n            filename = None\n            specific_restore_file_provided = cfg.restore_file != default_restore_file\n            slurm_was_restarted = int(os.environ.get(\"SLURM_RESTART_COUNT\", 0)) > 0\n            restart_from_latest = slurm_was_restarted or (\n                cfg.finetune_from_model is None and not specific_restore_file_provided\n            )\n            if restart_from_latest:\n                checkpoints = []\n                expected_file_count = distributed_utils.get_global_world_size()\n                for candidate in os.listdir(nfs_path):\n                    if candidate == \"checkpoint_last\":\n                        logger.warning(\n                            \"trying to restart a job that already wrote checkpoint_last!\"\n                        )\n                        # For NFS restarts, we are relying on logic of finding largest num_step to restart from\n                        # below instead of referencing checkpoint_last\n                        continue\n                    num_step = re.match(r\"checkpoint_(\\d+)\", candidate)\n                    if num_step:\n                        checkpoints.append((int(num_step[1]), candidate))\n                for _, candidate in sorted(checkpoints, reverse=True):\n                    present_files = len(\n                        [\n                            f\n                            for f in os.listdir(os.path.join(nfs_path, candidate))\n                            if not f.startswith(\"_\")\n                        ]\n                    )\n                    if present_files == expected_file_count:\n                        filename = os.path.join(\n                            nfs_path, candidate, f\"checkpoint{suffix}.pt\"\n                        )\n                        break\n                    logger.info(\n                        f\"skipping checkpoint {candidate} because it only has\"\n                        f\" {present_files} files (expected {expected_file_count})\"\n                    )\n            else:\n                filename = cfg.restore_file.replace(\".pt\", suffix + \".pt\")\n\n            checkpoint_path_to_load = filename\n\n        elif cfg.cluster_env == ComputeEnvs.AZURE.value and has_metaseq_internal:\n            if (\n                # --restore-file was not passed, always download latest checkpoint\n                (\n                    cfg.restore_file == default_restore_file\n                    and cfg.finetune_from_model is None\n                )\n                # --restore-file was passed, but we requeued, so download latest checkpoint\n                or int(os.environ.get(\"SLURM_RESTART_COUNT\", 0)) > 0\n            ):\n                # download checkpoint into local save_dir\n                checkpoint_path_to_load = os.path.join(\n                    cfg.save_dir, \"checkpoint_last{}.pt\".format(suffix)\n                )\n                azure_utils.download_recent_ckpt(\n                    cfg.cloud_upload_path, checkpoint_path_to_load, suffix + \".pt\"\n                )\n            elif (\n                # --restore-file was passed and is a blob URL, download that checkpoint\n                cfg.restore_file != default_restore_file\n                and \"windows.net\" in cfg.restore_file\n            ):\n                blob_url = cfg.restore_file.replace(\".pt\", suffix + \".pt\")\n                # download checkpoint into local save_dir\n                checkpoint_path_to_load = os.path.join(\n                    cfg.save_dir, \"checkpoint_last{}.pt\".format(suffix)\n                )\n                azure_utils.download_specific_ckpt(blob_url, checkpoint_path_to_load)\n            else:\n                logger.info(\n                    f\"Using checkpoint {checkpoint_path_to_load} even while on Azure\"\n                )\n\n    # RSC logic: --restore-file was passed, and we requeued\n    elif (\n        cfg.restore_file != default_restore_file\n        and int(os.environ.get(\"SLURM_RESTART_COUNT\", 0)) > 0\n    ):\n        # point checkpoint_path to the current checkpoint directory for loading, if it exists.\n        save_dir_last = os.path.join(\n            cfg.save_dir, \"checkpoint_last{}.pt\".format(suffix)\n        )\n        if PathManager.isfile(save_dir_last):\n            checkpoint_path_to_load = save_dir_last\n\n    logger.info(f\"attempting to load checkpoint from: {checkpoint_path_to_load}\")\n\n    # make sure everyone is done downloading their checkpoints before we load\n    distributed_utils.global_barrier()\n\n    extra_state = None\n    if checkpoint_path_to_load is not None:\n        extra_state = trainer.load_checkpoint(\n            checkpoint_path_to_load,\n            reset_optimizer,\n            reset_lr_scheduler,\n            optimizer_overrides,\n            reset_meters=reset_meters,\n        )\n\n    if reset_dataloader and int(os.environ.get(\"SLURM_RESTART_COUNT\", 0)) > 0:\n        logger.info(\n            f\"Disregarding --reset-dataloader since we are continuing past a requeue\"\n        )\n        reset_dataloader = False\n    if extra_state is not None and not reset_dataloader:\n        # restore iterator from checkpoint\n        itr_state = extra_state[\"train_iterator\"]\n        epoch_itr = trainer.get_train_iterator(\n            epoch=itr_state[\"epoch\"], **passthrough_args\n        )\n        epoch_itr.load_state_dict(itr_state)\n    else:\n        epoch_itr = trainer.get_train_iterator(epoch=1, **passthrough_args)\n    trainer.lr_step(epoch_itr.epoch)\n    return extra_state, epoch_itr\n\n\ndef _is_checkpoint_sharded(checkpoint_files) -> bool:\n    \"\"\"\n    Infer if state is sharded based on recorded configuration in the checkpoint file.\n    \"\"\"\n    if not checkpoint_files:\n        raise FileNotFoundError(\n            \"We weren't able to find any checkpoints corresponding to the parameters \"\n            \"you set. This could mean you have a typo, or it could mean you have a \"\n            \"mismatch in distributed training parameters, especially --fsdp or\"\n            \"--model-parallel. If you are working on a new script, it may also mean \"\n            \"you failed to fsdp_wrap or you have an unnecessary fsdp_wrap.\"\n        )\n    sd = torch_load_cpu(checkpoint_files[0])\n    return sd[\"cfg\"][\"distributed_training\"][\"use_sharded_state\"]\n\n\ndef _cache_checkpoint_files(path: str, suffix: str) -> Tuple[str, List[str]]:\n    \"\"\"\n    Given a checkpoint path, first try to expand it according to\n    a specified filename pattern. If `path` doesn't match the pattern\n    we search the remote for it as-is. Then cache all matching files\n    and return the local paths.\n\n    Ex. 1:\n        Remote: [path://checkpoint_last-shard0.pt, path://checkpoint_last-shard1.pt]\n        Input: path=remote://path/checkpoint_last-shard0.pt, suffix=shard\n        Output: [/local/checkpoint_last-shard0.pt, /local/checkpoint_last-shard1.pt]\n\n    Ex. 2:\n        Remote: [path://checkpoint_last-model_part-0.pt, path://checkpoint_last-model_part-1.pt]\n        Input: path=remote://path/checkpoint_last-model_part-0.pt, suffix=shard\n        Output: [/local/checkpoint_last-model_part-0.pt]\n\n    Ex. 3:\n        Remote: [path://checkpoint_last.pt]\n        Input: path=remote://path/checkpoint_last-shard0.pt, suffix=shard\n        Output: []\n    \"\"\"\n\n    local_path = PathManager.get_local_path(path, force=True)\n    local_name = os.path.basename(local_path)\n\n    # Ex. 2: if `path` doesn't match the pattern suggested by suffix\n    # just return the path itself without expansion\n    path_prefix = re.sub(rf\"{suffix}[0-9]+\\.pt$\", f\"{suffix}*\", path)\n    if path == path_prefix:\n        return local_path, [local_path]\n\n    local_paths = []\n    for filepath in PathManager.ls(path_prefix):\n        # Ignore non-checkpoint files\n        if not filepath.endswith(\".pt\"):\n            continue\n        src_name = os.path.basename(filepath)\n        if src_name == local_name:\n            # Target path is already cached\n            local_paths.append(local_path)\n        else:\n            src_path = os.path.join(os.path.dirname(path), src_name)\n            tgt_path = PathManager.get_local_path(src_path, force=True)\n            local_paths.append(tgt_path)\n\n    return local_path, local_paths\n\n\ndef get_paths_to_load(path, suffix=\"rank-\"):\n    local_path, checkpoint_files = _cache_checkpoint_files(path, suffix)\n    checkpoint_files_count = len(checkpoint_files)\n\n    # Check if this looks like a sharded checkpoint\n    if not _is_checkpoint_sharded(checkpoint_files):\n        return [local_path], 1\n\n    # Check if we need to merge shards\n    world_size = distributed_utils.get_data_parallel_world_size()\n    if world_size == checkpoint_files_count:\n        return [local_path], checkpoint_files_count\n    elif world_size > checkpoint_files_count:\n        # For now only support the case when new world size is multipe of previous\n        # Ex.: rank=0, world_size=2, checkpoint_files_count=1\n        rank = distributed_utils.get_data_parallel_rank()\n        if world_size % checkpoint_files_count == 0:\n            n_new_shards_per_file = int(world_size / checkpoint_files_count)\n            rank_to_load = rank // n_new_shards_per_file\n            # fname='./gpu_tests/test-checkpoint/.../checkpoint_18-model_part-0-shard0.pt'\n            # checkpoint_files[0]='./gpu_tests/test-checkpoint/.../checkpoint_18-model_part-0-shard0.pt'\n            fname = re.sub(\n                f\"{suffix}[0-9]+\",\n                f\"{suffix}{rank_to_load}\",\n                checkpoint_files[0],\n            )\n            return [fname], checkpoint_files_count\n        else:\n            n_new_shards_per_file = world_size / checkpoint_files_count\n            n_files_per_shard = checkpoint_files_count / world_size\n            start_rank = rank / n_new_shards_per_file\n            end_rank = start_rank + n_files_per_shard\n\n            ranks_to_load = list(set((int(start_rank), int(math.ceil(end_rank) - 1))))\n            fnames = []\n\n            for rank_to_load in ranks_to_load:\n                fname = re.sub(\n                    f\"{suffix}[0-9]+\",\n                    f\"{suffix}{rank_to_load}\",\n                    checkpoint_files[0],\n                )\n                fnames.append(fname)\n            return fnames, checkpoint_files_count\n    else:\n        # Assign an equal number of shards\n        assert checkpoint_files_count % world_size == 0\n        n_local_files = int(checkpoint_files_count / world_size)\n        rank = distributed_utils.get_data_parallel_rank()\n        start_rank = n_local_files * rank\n        fnames = []\n        for rank_to_load in range(start_rank, start_rank + n_local_files):\n            fname = re.sub(\n                f\"{suffix}[0-9]+\",\n                f\"{suffix}{rank_to_load}\",\n                checkpoint_files[0],\n            )\n            fnames.append(fname)\n        logger.info(\n            f\"Loading {checkpoint_files_count} on {world_size} DDP workers: {n_local_files} files per worker.\"\n        )\n\n        return fnames, checkpoint_files_count\n\n\ndef load_checkpoint_to_cpu(path, arg_overrides=None, load_on_all_ranks=False) -> dict:\n    \"\"\"Loads a checkpoint to CPU (with upgrading for backward compatibility).\n\n    If doing single-GPU training or if the checkpoint is only being loaded by at\n    most one process on each node (current default behavior is for only rank 0\n    to read the checkpoint from disk), load_on_all_ranks should be False to\n    avoid errors from torch.distributed not having been initialized or\n    torch.distributed.barrier() hanging.\n\n    If all processes on each node may be loading the checkpoint\n    simultaneously, load_on_all_ranks should be set to True to avoid I/O\n    conflicts.\n\n    There's currently no support for > 1 but < all processes loading the\n    checkpoint on each node.\n    \"\"\"\n\n    # Expand multi-part checkpoints like \"checkpoint_last-shard0.pt\"\n    paths_to_load, ddp_checkpoint_files_count = get_paths_to_load(path, suffix=\"shard\")\n    world_size = distributed_utils.get_data_parallel_world_size()\n    try:\n        if world_size < ddp_checkpoint_files_count:\n            assert len(paths_to_load) > 1\n            state = _merge_flat_fsdp_shards([torch_load_cpu(f) for f in paths_to_load])\n        elif world_size == ddp_checkpoint_files_count:\n            state = torch_load_cpu(paths_to_load[0])\n        else:\n            shard_ids = []\n            states = []\n            for path_to_load in paths_to_load:\n                # check that the files are in the form\n                # /.../checkpoint_18-model_part-0-shard0.pt\n                assert \"shard\" in path_to_load\n                match = re.search(r\"-shard(\\d+)\\.pt\", path_to_load)\n                assert match\n                # list of shard IDs: shard_ids=[0,...]\n                shard_ids.append(int(match.group(1)))\n                states.append(torch_load_cpu(path_to_load))\n\n            state = _split_flat_fsdp_shards(\n                states,\n                previous_shard_counts=ddp_checkpoint_files_count,\n                shard_ids=shard_ids,\n            )\n    except Exception as error:\n        logger.error(\n            f\"Got Exception While Trying To Load {path} with Paths to Load {paths_to_load}.\"\n            \"If you are not meaning to --restore-file, remove the command explictly.\"\n        )\n        raise error\n\n    logger.info(\"Done reading from disk\")\n\n    if \"cfg\" in state and state[\"cfg\"] is not None:\n        # hack to be able to set Namespace in dict config. this should be removed when we update to newer\n        # omegaconf version that supports object flags, or when we migrate all existing models\n        from omegaconf import _utils\n\n        old_primitive = _utils.is_primitive_type\n        _utils.is_primitive_type = lambda _: True\n\n        state[\"cfg\"] = OmegaConf.create(state[\"cfg\"])\n\n        _utils.is_primitive_type = old_primitive\n\n        OmegaConf.set_struct(state[\"cfg\"], True)\n\n        if arg_overrides is not None:\n            overwrite_args_by_name(state[\"cfg\"], arg_overrides)\n            overwrite_keys_not_present(state[\"cfg\"], arg_overrides)\n\n    state = _upgrade_state_dict(state)\n    return state\n\n\ndef load_model_ensemble_and_task(\n    filenames,\n    arg_overrides: Optional[Dict[str, Any]] = None,\n    task=None,\n    strict=True,\n    suffix=\"\",\n    num_shards=1,\n    state=None,\n    build_model_hook=None,\n):\n    assert state is None or len(filenames) == 1\n\n    from metaseq import tasks\n\n    assert not (\n        strict and num_shards > 1\n    ), \"Cannot load state dict with strict=True and checkpoint shards > 1\"\n    ensemble = []\n    cfg = None\n\n    for filename in filenames:\n        orig_filename = filename\n        assert num_shards > 0\n        for shard_idx in range(num_shards):\n            if num_shards == 1:\n                filename = filename.replace(\".pt\", suffix + \".pt\")\n            else:\n                filename = orig_filename[:-3] + f\"_part{shard_idx}.pt\"\n            if state is None:\n                state = load_checkpoint_to_cpu(filename, arg_overrides)\n            if \"cfg\" in state and state[\"cfg\"] is not None:\n                cfg = state[\"cfg\"]\n            else:\n                raise RuntimeError(\n                    f\"!!! cfg does not exist in state keys = {state.keys()} !!!\"\n                )\n\n            # We now copy embed_tokens over to output_proj (if its missing) for all arches (only OPT here so far).\n            oproj_key = \"decoder.output_projection.weight\"\n            emb_key = \"decoder.embed_tokens.weight\"\n            if emb_key in state[\"model\"] and oproj_key not in state[\"model\"]:\n                state[\"model\"][oproj_key] = state[\"model\"][emb_key]\n\n            if task is None:\n                task = tasks.setup_task(cfg.task)\n\n            if \"task_state\" in state:\n                task.load_state_dict(state[\"task_state\"])\n\n            if build_model_hook is not None:\n                model = build_model_hook(cfg, task)\n            else:\n                # build model for ensemble\n                model = task.build_model(cfg.model)\n\n            model.load_state_dict(state[\"model\"], strict=strict)\n            logger.info(\"Done loading state dict\")\n            # reset state so it gets loaded for the next model in ensemble\n            state = None\n\n        ensemble.append(model)\n    return ensemble, cfg, task\n\n\ndef _checkpoint_paths(path, pattern=r\"checkpoint(\\d+)\\.pt\"):\n    \"\"\"Retrieves all checkpoints found in `path` directory.\n\n    Checkpoints are identified by matching filename to the specified pattern. If\n    the pattern contains groups, the result will be sorted by the first group in\n    descending order.\n    \"\"\"\n    pt_regexp = re.compile(pattern)\n    files = os.listdir(path)\n\n    entries = []\n    for i, f in enumerate(files):\n        m = pt_regexp.fullmatch(f)\n        if m is not None:\n            idx = float(m.group(1)) if len(m.groups()) > 0 else i\n            entries.append((idx, m.group(0)))\n    return [os.path.join(path, x[1]) for x in sorted(entries, reverse=True)]\n\n\ndef _upgrade_state_dict(state):\n    \"\"\"Helper for upgrading old model checkpoints.\"\"\"\n    # add optimizer_history\n    if \"optimizer_history\" not in state:\n        state[\"optimizer_history\"] = [\n            {\"criterion_name\": \"CrossEntropyCriterion\", \"best_loss\": state[\"best_loss\"]}\n        ]\n        state[\"last_optimizer_state\"] = state[\"optimizer\"]\n        del state[\"optimizer\"]\n        del state[\"best_loss\"]\n    # move extra_state into sub-dictionary\n    if \"epoch\" in state and \"extra_state\" not in state:\n        state[\"extra_state\"] = {\n            \"epoch\": state[\"epoch\"],\n            \"batch_offset\": state[\"batch_offset\"],\n            \"val_loss\": state[\"val_loss\"],\n        }\n        del state[\"epoch\"]\n        del state[\"batch_offset\"]\n        del state[\"val_loss\"]\n    # reduce optimizer history's memory usage (only keep the last state)\n    if \"optimizer\" in state[\"optimizer_history\"][-1]:\n        state[\"last_optimizer_state\"] = state[\"optimizer_history\"][-1][\"optimizer\"]\n        for optim_hist in state[\"optimizer_history\"]:\n            del optim_hist[\"optimizer\"]\n    # move best_loss into lr_scheduler_state\n    if \"lr_scheduler_state\" not in state[\"optimizer_history\"][-1]:\n        state[\"optimizer_history\"][-1][\"lr_scheduler_state\"] = {\n            \"best\": state[\"optimizer_history\"][-1][\"best_loss\"]\n        }\n        del state[\"optimizer_history\"][-1][\"best_loss\"]\n    # keep track of number of updates\n    if \"num_updates\" not in state[\"optimizer_history\"][-1]:\n        state[\"optimizer_history\"][-1][\"num_updates\"] = 0\n    # use stateful training data iterator\n    if \"train_iterator\" not in state[\"extra_state\"]:\n        state[\"extra_state\"][\"train_iterator\"] = {\n            \"epoch\": state[\"extra_state\"][\"epoch\"],\n            \"iterations_in_epoch\": state[\"extra_state\"].get(\"batch_offset\", 0),\n        }\n    return state\n\n\ndef verify_checkpoint_directory(save_dir: str) -> None:\n    if not os.path.exists(save_dir):\n        try:\n            os.makedirs(save_dir, exist_ok=True)\n        except Exception as e:\n            logger.warning(f\"Unable to create dir {save_dir} on {socket.gethostname()}\")\n            raise e\n    rank = distributed_utils.get_global_rank()\n    temp_file_path = os.path.join(save_dir, f\"dummy{rank}\")\n    try:\n        with open(temp_file_path, \"w\"):\n            pass\n    except OSError as e:\n        logger.warning(f\"Unable to access checkpoint save directory: {save_dir}\")\n        raise e\n    else:\n        try:\n            os.remove(temp_file_path)\n        except FileNotFoundError:\n            pass\n\n\ndef _merge_flat_fsdp_shards(shards_to_load: List[Dict], unpad=False) -> Dict:\n    \"\"\"\n    Concatenate tensor entries in a list of local_state_dicts into one\n    local_state_dict to allow resumption on a different world size.\n    \"\"\"\n    merged_state = {}\n    world_size = distributed_utils.get_data_parallel_world_size()\n    for key in shards_to_load[0].keys():\n        merged_state[key] = shards_to_load[0][key]\n\n    pad_info = _get_pad_info(shards_to_load[-1])\n    dtype = torch.float16\n    for k in shards_to_load[0][\"model\"]:\n        dtype = shards_to_load[0][\"model\"][k].dtype\n        if \"flat_param\" in k:\n            pad_info_k = pad_info[k]\n            catted = torch.cat([x[\"model\"][k] for x in shards_to_load])\n            if world_size == 1 and pad_info_k > 0:\n                catted = catted[:-pad_info_k]\n            elif world_size > 1 and pad_info_k > 0 and not unpad:\n                raise NotImplementedError(\n                    f\"Param {k} padded with {pad_info_k} extra elements. You must use the reshard_mp script.\"\n                )\n\n            merged_state[\"model\"][k] = catted\n\n    # TODO(susanz): Not removing decoder.version due to HF compatibility.\n    if \"decoder.version\" not in merged_state[\"model\"]:\n        merged_state[\"model\"][\"decoder.version\"] = torch.tensor([3.0], dtype=dtype)\n    if OPT_KEY in merged_state:\n        merged_state[OPT_KEY] = _merge_flat_fsdp_opt_state(shards_to_load)\n    return merged_state\n\n\ndef _merge_flat_fsdp_opt_state(shards_to_load: List[Dict]) -> Dict:\n    \"\"\"Logic described here: https://tinyurl.com/2p86zffr\"\"\"\n    result = shards_to_load[0][OPT_KEY]\n    pad_info = _get_pad_info(shards_to_load[-1])\n    world_size = distributed_utils.get_data_parallel_world_size()\n    os2model_key = dict(\n        zip(shards_to_load[0][OPT_KEY][\"state\"].keys(), pad_info.keys())\n    )\n    for k in shards_to_load[0][OPT_KEY][\"state\"].keys():\n        # 0,1,2,3... if each layer wrapped, else 0\n        for k2 in shards_to_load[0][OPT_KEY][\"state\"][k].keys():\n            # exp_avg, exp_avg_sq, step (for adam32 bit)\n            states = [x[OPT_KEY][\"state\"][k][k2] for x in shards_to_load]\n            if not torch.is_tensor(states[0]) or is_singleton_tensor(states[0]):\n                result[\"state\"][k][k2] = states[0]\n            else:\n                catted = torch.cat(states)\n                if k in os2model_key:\n                    opt_state_key = os2model_key[k]\n                    pad_info_k = pad_info[opt_state_key]\n                    if world_size == 1 and pad_info_k > 0:  # unpad\n                        catted = catted[:-pad_info_k]\n                result[\"state\"][k][k2] = catted\n    return result\n\n\ndef is_singleton_tensor(x: Any) -> bool:\n    \"\"\"Is x a dimensionless tensor?\"\"\"\n    return torch.is_tensor(x) and x.dim() == 0\n\n\ndef _get_pad_info(state_dict: Dict) -> Dict[str, int]:\n    if \"shard_metadata\" not in state_dict:\n        # Note: comment this out if you have sharded checkpoints that you think can be loaded\n        return collections.defaultdict(lambda: 0)\n    res = {}\n    for m in state_dict[\"shard_metadata\"][\"param_metadata\"]:\n        fsdp_path = m[\"fsdp_path\"]\n        for k, v in m[\"params\"].items():\n            full_key = f\"{fsdp_path}.{k}\" if fsdp_path else k\n            assert full_key not in res, f\"collision: {full_key} already in {res}\"\n            res[full_key] = v[\"padding\"]\n    return res\n\n\ndef _split_flat_fsdp_shards(\n    states: List[Dict], previous_shard_counts: int, shard_ids: List[int]\n) -> Dict:\n    \"\"\"\n    This function basically takes multiple FSDP states, merges and then finds the new state\n    in the new FSDP shard.\n    i.e.\n    lets say previous training was with data prallel size = 2 and\n    new training run is with data parallel size = 3.\n    Then it will take [state1, state2] and based on the current data parallel rank,\n    return the new state, which in the case of\n    a) rank0: first 2/3rd of state1\n    b) rank1: last 1/3rd of state1, first 1/3rd of state2\n    c) rank2: last 2/3rd of state2\n    \"\"\"\n    # First copy all keys apart from model and opt stats to the\n    # final state dict from zero'th loaded state\n    # k: ['cfg', 'criterion', 'optimizer_history', 'task_state',\n    # 'extra_state', 'last_optimizer_state', 'shard_metadata']\n    splitted_state = {\n        k: v for k, v in states[0].items() if k not in (\"model\", \"opt_stats\")\n    }\n    splitted_model_state = {}\n    ddp_world_size = distributed_utils.get_data_parallel_world_size()\n    # k in ['flat_param_0', 'decoder.version', 'decoder.layers.0.flat_param_0',\n    # ...'decoder.layers.3.flat_param_0']\n    for k in states[0][\"model\"].keys():\n        dtype = states[0][\"model\"][k].dtype\n        if \"flat_param\" not in k:\n            # I think usually its just decoder.version that comes here.\n            splitted_model_state[k] = states[0][\"model\"][k]\n            continue\n        # values: [tensor([-0.0013, ...-0.0030], dtype=torch.float16)]\n        values = [state[\"model\"][k] for state in states]\n\n        assert all(\n            len(value.shape) == 1 for value in values\n        ), \"Flat param should have only one dimensional tensor\"\n        assert all(\n            value.size(0) == values[0].size(0) for value in values\n        ), \"all shards should have same sized tensor\"\n\n        full_param_shape = values[0].size(0) * previous_shard_counts\n\n        # TODO: [namangoyal] double check this\n        new_shard_size = math.ceil(full_param_shape / ddp_world_size)\n        new_start_offset = new_shard_size * distributed_utils.get_data_parallel_rank()\n\n        current_start_offset = values[0].size(0) * shard_ids[0]\n        start_offset = new_start_offset - current_start_offset\n\n        concatted_value = torch.cat(values)\n        new_v = concatted_value[start_offset : start_offset + new_shard_size]\n\n        if new_v.size(0) != new_shard_size:\n            assert distributed_utils.get_data_parallel_rank() == ddp_world_size - 1\n            num_to_pad = new_shard_size - new_v.size(0)\n            new_v = torch.nn.functional.pad(new_v, [0, num_to_pad])\n        splitted_model_state[k] = new_v\n\n    splitted_state[\"model\"] = splitted_model_state\n    # TODO(susanz): Not removing decoder.version due to HF compatibility.\n    if \"decoder.version\" not in splitted_state[\"model\"]:\n        splitted_state[\"model\"][\"decoder.version\"] = torch.tensor([3.0], dtype=dtype)\n\n    if OPT_KEY in states[0]:\n        splitted_state[OPT_KEY] = _split_flat_fsdp_opt_state(\n            states, previous_shard_counts, shard_ids\n        )\n\n    return splitted_state\n\n\ndef _split_flat_fsdp_opt_state(\n    states: List[Dict], previous_shard_counts: int, shard_ids: List[int]\n) -> Dict:\n    splitted_opt_state = states[0][OPT_KEY]\n    ddp_world_size = distributed_utils.get_data_parallel_world_size()\n    for k in states[0][OPT_KEY][\"state\"].keys():\n        # k in [0,1,2,3...] if each layer wrapped, else 0\n        # k2 in ['step', 'exp_avg', 'exp_avg_sq']\n        for k2 in states[0][OPT_KEY][\"state\"][k].keys():\n            # values are eather the list of 'step' number, ex. [18]\n            # if k2='step' OR a tnesor in case k2='exp_avg' or 'exp_avg_sq'\n            values = [state[OPT_KEY][\"state\"][k][k2] for state in states]\n\n            if not torch.is_tensor(values[0]) or is_singleton_tensor(values[0]):\n                assert all(value == values[0] for value in values)\n                splitted_opt_state[\"state\"][k][k2] = values[0]\n            else:\n                assert all(\n                    len(value.shape) == 1 for value in values\n                ), \"Flat param should have only one dimensional tensor\"\n                full_param_shape = values[0].size(0) * previous_shard_counts\n\n                # TODO: [namangoyal] double check this\n                new_shard_size = math.ceil(full_param_shape / ddp_world_size)\n                new_start_offset = (\n                    new_shard_size * distributed_utils.get_data_parallel_rank()\n                )\n\n                current_start_offset = values[0].size(0) * shard_ids[0]\n                start_offset = new_start_offset - current_start_offset\n\n                concatted_value = torch.cat(values)\n                splitted_value = concatted_value[\n                    start_offset : start_offset + new_shard_size\n                ]\n\n                if splitted_value.size(0) != new_shard_size:\n                    assert (\n                        distributed_utils.get_data_parallel_rank() == ddp_world_size - 1\n                    )\n                    num_to_pad = new_shard_size - splitted_value.size(0)\n                    splitted_value = torch.nn.functional.pad(\n                        splitted_value, [0, num_to_pad]\n                    )\n                splitted_opt_state[\"state\"][k][k2] = splitted_value\n    return splitted_opt_state\n",
        "metaseq/cli/__init__.py": "",
        "metaseq/cli/interactive_cli.py": "#!/usr/bin/env python3 -u\n# Copyright (c) Meta Platforms, Inc. and affiliates. All Rights Reserved.\n#\n# This source code is licensed under the MIT license found in the\n# LICENSE file in the root directory of this source tree.\n\"\"\"\nHost the demo.\n\nLaunch with `python -m metaseq.cli.interactive_hosted` to run locally.\n\nSee docs/api.md for more information.\n\"\"\"\n\nimport os\nimport ast\nimport random\nimport sys\nimport logging\n\nimport torch\n\nfrom metaseq import options\nfrom metaseq.dataclass.configs import MetaseqConfig\nfrom metaseq.dataclass.utils import convert_namespace_to_omegaconf\nfrom metaseq.distributed import utils as distributed_utils\nfrom metaseq.hub_utils import GeneratorInterface\nfrom metaseq.service.utils import build_logger\n\nimport importlib\n\nif \"METASEQ_SERVICE_CONSTANTS_MODULE\" not in os.environ:\n    constants_module = importlib.import_module(\"metaseq.service.constants\")\nelse:\n    constants_module = importlib.import_module(\n        os.environ[\"METASEQ_SERVICE_CONSTANTS_MODULE\"]\n    )\nTOTAL_WORLD_SIZE = constants_module.TOTAL_WORLD_SIZE\nLAUNCH_ARGS = constants_module.LAUNCH_ARGS\nINFERENCE_ARG_OVERRIDES = constants_module.INFERENCE_ARG_OVERRIDES\n\nlogger = build_logger()\n\n\ndef input_loop():\n    inp = []\n    while True:\n        try:\n            # green display, bold user prompt\n            display = (\n                \"\\033[32mPrompt (ctrl-D to end input, ctrl-C to quit):\\n\\033[0;1m\"\n                if not inp\n                else \"\"\n            )\n            data = input(display)\n            inp.append(data)\n        except KeyboardInterrupt:\n            # reset the formatting\n            sys.stdout.write(\"\\033[0m\")\n            raise\n        except EOFError:\n            break\n        # reset the formatting\n        sys.stdout.write(\"\\033[0m\")\n    logger.debug(f\"Input: {inp}\")\n    return \"\\n\".join(inp)\n\n\ndef worker_main(cfg: MetaseqConfig, namespace_args=None):\n    global generator\n    # make sure generations are stochastic since we have many workers\n    torch.manual_seed(random.randint(1, 20000))\n    torch.cuda.manual_seed(random.randint(1, 20000))\n\n    generator = GeneratorInterface(cfg)\n    models = generator.load_model()  # noqa: F841\n\n    # quiet some of the stuff for visual aspects\n    logging.getLogger(\"metaseq.hub_utils\").setLevel(logging.WARNING)\n\n    logger.info(f\"loaded model {cfg.distributed_training.distributed_rank}\")\n    request_object = distributed_utils.broadcast_object(\n        None, src_rank=0, group=distributed_utils.get_global_group()\n    )\n    if torch.distributed.get_rank() == 0:\n        while True:\n            prompt = input_loop()\n            tokens = generator.encode_fn(prompt)\n            request_object = {\n                \"inputs\": [tokens],\n                \"max_tokens\": [128],\n            }\n            distributed_utils.broadcast_object(\n                request_object, src_rank=0, group=distributed_utils.get_global_group()\n            )\n            generations = generator.generate(**request_object)\n            print(generations[0][0][\"text\"])\n    else:\n        # useful in FSDP setting\n        while True:\n            request_object = distributed_utils.broadcast_object(\n                None, src_rank=0, group=distributed_utils.get_global_group()\n            )\n            _ = generator.generate(**request_object)\n\n\ndef cli_main():\n    \"\"\"\n    Command line interactive.\n    \"\"\"\n    parser = options.get_generation_parser()\n    # dumb defaults overriding\n    parser.set_defaults(lr_scheduler=None, criterion=None)\n    flat_launch_args = []\n    for s in LAUNCH_ARGS:\n        flat_launch_args += s.split()\n    args = options.parse_args_and_arch(parser, input_args=flat_launch_args)\n    args.data = os.path.dirname(args.path)  # hardcode the data arg\n    cfg = convert_namespace_to_omegaconf(args)\n    cfg.distributed_training.distributed_world_size = TOTAL_WORLD_SIZE\n\n    model_overrides = ast.literal_eval(cfg.common_eval.model_overrides)\n    model_overrides.update(INFERENCE_ARG_OVERRIDES)\n    cfg.common_eval.model_overrides = str(model_overrides)\n\n    distributed_utils.call_main(cfg, worker_main, namespace_args=args)\n\n\nif __name__ == \"__main__\":\n    if os.getenv(\"SLURM_NODEID\") is None:\n        logger.warning(\n            f\"Missing slurm configuration, defaulting to 'use entire node' for API\"\n        )\n        os.environ[\"SLURM_NODEID\"] = \"0\"\n        os.environ[\"SLURM_NNODES\"] = \"1\"\n        os.environ[\"SLURM_NTASKS\"] = \"1\"\n        import socket\n\n        os.environ[\"SLURM_STEP_NODELIST\"] = socket.gethostname()\n    cli_main()\n",
        "metaseq/cli/interactive_ft.py": "import argparse\nimport os\nimport torch\nimport torch.distributed as dist\nimport torch.nn as nn\nfrom tokenizers import Tokenizer, ByteLevelBPETokenizer\nfrom typing import Any, List, Optional\n\ntry:\n    torch.classes.load_library(os.environ.get(\"FT_PATH\"))\nexcept Exception:\n    raise ImportError(\n        \"Please install FasterTransformer and provide a path to the binary\"\n        \"`libth_transformer.so` via the environment variable `FT_PATH`.\"\n    )\n\nmodel = None\ntokenizer = None\ndevice = None\n\nBOS_TOKEN = 0\nPAD_TOKEN = 1\nEOS_TOKEN = 2\nUNK_TOKEN = 3\n\n\n@torch.inference_mode()\ndef generate(\n    inputs: List[List[int]],\n    output_length: int,\n    beam_width: int = 1,\n    top_k: Optional[int] = 0,\n    top_p: Optional[float] = 1.0,\n    diversity_rate: Optional[float] = None,\n    temperature: Optional[float] = 1.0,\n    len_penalty: Optional[float] = None,\n    repetition_penalty: Optional[float] = 1.0,\n    presence_penalty: Optional[float] = None,\n    random_seed: Optional[int] = 0,\n    min_length: Optional[int] = None,\n    bad_words_list: Optional[torch.Tensor] = None,\n    return_cum_log_probs: Optional[int] = 0,\n) -> List[Any]:\n    inputs = [[EOS_TOKEN] + toks for toks in inputs]\n    inputs = [torch.tensor(toks, dtype=torch.int32, device=device) for toks in inputs]\n    lengths = torch.tensor([len(t) for t in inputs], dtype=torch.int32, device=device)\n    inputs = nn.utils.rnn.pad_sequence(inputs, True, padding_value=PAD_TOKEN)\n\n    if top_k is not None:\n        top_k = torch.tensor([top_k], dtype=torch.int32)\n    if top_p is not None:\n        top_p = torch.tensor([top_p], dtype=torch.float32)\n    if diversity_rate is not None:\n        diversity_rate = torch.tensor([diversity_rate], dtype=torch.float32)\n    if temperature is not None:\n        temperature = torch.tensor([temperature], dtype=torch.float32)\n    if len_penalty is not None:\n        len_penalty = torch.tensor([len_penalty], dtype=torch.float32)\n    if repetition_penalty is not None:\n        repetition_penalty = torch.tensor([repetition_penalty], dtype=torch.float32)\n    if presence_penalty is not None:\n        presence_penalty = torch.tensor([presence_penalty], dtype=torch.float32)\n    if random_seed is not None:\n        random_seed = torch.tensor([random_seed], dtype=torch.int64)\n    if min_length is not None:\n        min_length = torch.tensor([min_length], dtype=torch.int64)\n\n    outputs, output_lengths = model.forward(\n        inputs,\n        lengths,\n        output_length,\n        beam_width,\n        top_k,\n        top_p,\n        diversity_rate,\n        temperature,\n        len_penalty,\n        repetition_penalty,\n        presence_penalty,\n        min_length,\n        random_seed,\n        bad_words_list,\n        return_cum_log_probs,\n    )\n\n    results = []\n    beam_idx = 0\n    special = outputs.new_tensor([BOS_TOKEN, PAD_TOKEN, EOS_TOKEN, UNK_TOKEN])\n    for output, output_len in zip(outputs, output_lengths):\n        mask = ~torch.isin(output[beam_idx], special)\n        mask[1:] = mask[1:].cummin(dim=0)[0]\n\n        tokens = output[beam_idx][1 : output_len[beam_idx]]\n        tokens = tokens[mask[1 : output_len[beam_idx]]]\n        results.append({\"text\": tokenizer.decode(tokens.tolist())})\n    return [results]\n\n\ndef main(args: argparse.Namespace) -> None:\n    global model, tokenizer, device\n    dist.init_process_group(backend=\"mpi\")\n    world_size = dist.get_world_size()\n    rank = dist.get_rank() % world_size\n    device = torch.device(f\"cuda:{dist.get_rank() % torch.cuda.device_count()}\")\n    torch.cuda.set_device(device)\n\n    if args.tokenizer_file is not None:\n        tokenizer = Tokenizer.from_file(args.tokenizer_file)\n    else:\n        tokenizer = ByteLevelBPETokenizer(args.vocab_file, args.merges_file)\n\n    torch_dtypes = {\"fp16\": torch.half, \"bf16\": torch.bfloat16, \"fp32\": torch.float}\n    dtype = torch_dtypes[args.dtype]\n\n    state_dict = torch.load(f\"{args.weight_path}/part-{rank}.pt\")\n    weights = [w.to(device, dtype) for w in state_dict[\"weights\"]]\n    int8_weights, int8_scales = [], []\n    if args.int8_mode != 0 and {\"int8_weights\", \"int8_scales\"} <= state_dict.keys():\n        int8_weights = [w.to(device=device) for w in state_dict[\"int8_weights\"]]\n        int8_scales = [w.to(device=device) for w in state_dict[\"int8_scales\"]]\n\n    kwargs = {\n        \"head_num\": args.num_heads,\n        \"size_per_head\": args.embed_size // args.num_heads,\n        \"inter_size\": 4 * args.embed_size,\n        \"layer_num\": args.num_layers,\n        \"expert_num\": 0,\n        \"moe_k\": 0,\n        \"moe_layer_index\": [],\n        \"vocab_size\": args.vocab_size,\n        \"start_id\": 2,\n        \"end_id\": 2,\n        \"tensor_para_size\": world_size,\n        \"pipeline_para_size\": 1,\n        \"int8_mode\": args.int8_mode,\n        \"layernorm_eps\": 1e-5,\n        \"layernorm_type\": \"pre_layernorm\",\n        \"activation_type\": \"Relu\",\n        \"has_positional_encoding\": True,\n        \"has_pre_decoder_layernorm\": False,\n        \"has_post_decoder_layernorm\": True,\n        \"has_adapters\": False,\n        \"adapter_inter_size\": 0,\n        \"use_attention_linear_bias\": False,\n        \"weights\": weights,\n        \"int8_weights\": int8_weights,\n        \"scale\": int8_scales,\n        \"shared_contexts_ratio\": 1.0,\n    }\n    model = torch.classes.FasterTransformer.ParallelGptOp(*kwargs.values())\n\n    object = [None]\n    while True:\n        if torch.distributed.get_rank() == 0:\n            prompt = input(\"\\033[32mPrompt: \\033[0;1m\").rstrip()\n            if not prompt:\n                continue\n            object = [[tokenizer.encode(prompt).ids]]\n\n        dist.broadcast_object_list(object, src=0)\n        output = generate(\n            object[0],\n            output_length=args.output_length,\n            beam_width=args.beam_width,\n            top_k=args.top_k,\n            top_p=args.top_p,\n            diversity_rate=args.diversity_rate,\n            temperature=args.temperature,\n            len_penalty=args.len_penalty,\n            repetition_penalty=args.repetition_penalty,\n            random_seed=0,\n        )\n        if torch.distributed.get_rank() == 0:\n            print(f\"Output: {output[0][0]['text']}\")\n\n\ndef measure_time(func, *args, **kwargs):\n    start = torch.cuda.Event(enable_timing=True)\n    end = torch.cuda.Event(enable_timing=True)\n    start.record()\n    func(*args, **kwargs)\n    end.record()\n    torch.cuda.synchronize()\n    return start.elapsed_time(end)\n\n\ndef get_args() -> argparse.Namespace:\n    parser = argparse.ArgumentParser()\n    parser.add_argument(\"--num-layers\", type=int, default=12)\n    parser.add_argument(\"--num-heads\", type=int, default=12)\n    parser.add_argument(\"--embed-size\", type=int, default=768)\n    parser.add_argument(\"--vocab-size\", type=int, default=50272)\n\n    parser.add_argument(\"--vocab-file\", type=str)\n    parser.add_argument(\"--merges-file\", type=str)\n    parser.add_argument(\"--tokenizer-file\", type=str, default=None)\n    parser.add_argument(\"--weight-path\", type=str)\n    parser.add_argument(\"--dtype\", choices=[\"fp32\", \"fp16\", \"bf16\"], default=\"fp16\")\n    parser.add_argument(\"--int8-mode\", type=int, default=0)\n\n    parser.add_argument(\"--batch-size\", type=int, default=1)\n    parser.add_argument(\"--output-length\", type=int, default=256)\n    parser.add_argument(\"--beam-width\", type=int, default=1)\n    parser.add_argument(\"--top-k\", type=int, default=20)\n    parser.add_argument(\"--top-p\", type=float, default=0.95)\n    parser.add_argument(\"--temperature\", type=float, default=0.7)\n    parser.add_argument(\"--len-penalty\", type=float, default=0.0)\n    parser.add_argument(\"--diversity-rate\", type=float, default=0.0)\n    parser.add_argument(\"--repetition-penalty\", type=float, default=1.2)\n    return parser.parse_args()\n\n\nif __name__ == \"__main__\":\n    args = get_args()\n    main(args)\n",
        "metaseq/cli/interactive_hosted.py": "#!/usr/bin/env python3 -u\n# Copyright (c) Meta Platforms, Inc. and affiliates. All Rights Reserved.\n#\n# This source code is licensed under the MIT license found in the\n# LICENSE file in the root directory of this source tree.\n\"\"\"\nHost the demo.\n\nLaunch with `python -m metaseq.cli.interactive_hosted` to run locally.\n\nSee docs/api.md for more information.\n\"\"\"\n\nimport os\nimport ast\nimport queue\nimport pkg_resources\nimport random\nimport threading\nimport traceback\n\nimport torch\nfrom flask import Flask, request, jsonify\nfrom werkzeug.exceptions import HTTPException\n\nfrom metaseq import options\nfrom metaseq.dataclass.configs import MetaseqConfig\nfrom metaseq.dataclass.utils import convert_namespace_to_omegaconf\nfrom metaseq.distributed import utils as distributed_utils\nfrom metaseq.hub_utils import GeneratorInterface\nfrom metaseq.service.queue import PriorityQueueRingShard\nfrom metaseq.service.workers import WorkItem\nfrom metaseq.service.utils import get_my_ip, build_logger\nfrom metaseq.service.responses import OAIResponse\n\nimport importlib\n\nif \"METASEQ_SERVICE_CONSTANTS_MODULE\" not in os.environ:\n    constants_module = importlib.import_module(\"metaseq.service.constants\")\nelse:\n    constants_module = importlib.import_module(\n        os.environ[\"METASEQ_SERVICE_CONSTANTS_MODULE\"]\n    )\n    # Don't forget to patch OAIResponse\n    import metaseq.service.responses\n\n    metaseq.service.responses.CHECKPOINT_FOLDER = constants_module.CHECKPOINT_FOLDER\nMAX_SEQ_LEN = constants_module.MAX_SEQ_LEN\nMAX_BATCH_TOKENS = constants_module.MAX_BATCH_TOKENS\nMAX_BEAM = constants_module.MAX_BEAM\nDEFAULT_PORT = constants_module.DEFAULT_PORT\nTOTAL_WORLD_SIZE = constants_module.TOTAL_WORLD_SIZE\nLAUNCH_ARGS = constants_module.LAUNCH_ARGS\nINFERENCE_ARG_OVERRIDES = constants_module.INFERENCE_ARG_OVERRIDES\n\napp = Flask(__name__)\n\n# global state (mutable!)\ncfg = None\nport = DEFAULT_PORT\nBATCH_QUEUE = PriorityQueueRingShard()\n\nlogger = build_logger()\n\n\ndef batching_loop(timeout=100, max_tokens=MAX_BATCH_TOKENS):\n    \"\"\"\n    batching_loop is an infinite loop responsible for executing generations.\n\n    GPUs benefit from batching requests, but we expect workloads to come\n    in non-uniformly. This loop groups requests together (via BATCH_QUEUE)\n    and executes them in one batch. In order to keep latency low, unfilled\n    batches are executed within a window of :timeout: milliseconds.\n\n    batching_loop also performs dynamic batching, in order to minimize the\n    amount of padding by grouping like-sized workloads together. As a result\n    batching loop will provide preferential treatment to smaller workloads.  At\n    the current moment, there is no TTL logic to ensure a maximum wait time.\n\n    For a rough overview of dynamic batching, see\n    https://parl.ai/docs/tutorial_worlds.html#dynamic-batching.\n\n    :param timeout: The max queue time before a non-full batch is launched.\n    :param max_tokens: the maximum number of tokens that can be processed\n        concurrently. model specific and empirical.\n    \"\"\"\n    # TODO(roller):\n    # - group by generation type, topp etc, as we cannot share these\n    # - modify timeout logic to be cumulative\n    global BATCH_QUEUE\n\n    batch = []\n    target_queue = None\n    while True:\n        try:\n            # for now, we only have 1 worker, so can always index to shard 0\n            if target_queue is None:\n                target_queue = BATCH_QUEUE.queue_shards[0].get_largest_queue()\n            if not target_queue:\n                continue\n            # dynamic batching: group like-sized items to reduce the cost\n            # of padding. See PR#20 for additional context.\n            item = target_queue.get(timeout=timeout / 1000)\n            # accumulate the batch until it gets too big\n            longest = max([item] + batch).cost\n            batch_cost = longest * (len(batch) + 1)\n            # overflow corresponds to whether max(prompt_len) + gen_len will\n            # fit the max sequence length\n            max_prompt_len = max(x.prompt_len for x in [item] + batch)\n            max_gen_len = max(x.gen_len for x in [item] + batch)\n            overflow = (max_prompt_len + max_gen_len) > MAX_SEQ_LEN\n            if batch and (batch_cost > max_tokens or overflow):\n                # we're over budget, put it back in the queue\n                target_queue.put(item)\n                raise queue.Empty\n            else:\n                # batch is empty or under budget\n                batch.append(item)\n        except queue.Empty:\n            target_queue = None\n            if batch:\n                request_object = {\n                    \"inputs\": [],\n                    \"min_tokens\": [],\n                    \"max_tokens\": [],\n                }\n                for work_item in batch:\n                    ro = work_item.data\n                    request_object[\"inputs\"].append(ro[\"input\"])\n                    request_object[\"min_tokens\"].append(ro.get(\"min_tokens\", 0))\n                    request_object[\"max_tokens\"].append(\n                        ro.get(\"max_tokens\", MAX_SEQ_LEN)\n                    )\n                    # assumption: everyone has the same remaining args\n                    for key in [\n                        \"temperature\",\n                        \"top_p\",\n                        \"n\",\n                        \"best_of\",\n                        \"echo\",\n                        \"logprobs\",\n                        \"stop\",\n                        \"omega_bound\",\n                        \"lambda_decay\",\n                        \"alpha_presence\",\n                        \"alpha_frequency\",\n                        \"alpha_presence_src\",\n                        \"alpha_frequency_src\",\n                        \"alpha_src_penalty_end_idx\",\n                    ]:\n                        if key in ro:\n                            request_object[key] = ro[key]\n                # do the actual generations\n                request_object[\"seed\"] = random.randint(1, 20000)\n                if torch.distributed.is_initialized():\n                    distributed_utils.broadcast_object(\n                        request_object,\n                        src_rank=0,\n                        group=distributed_utils.get_global_group(),\n                    )\n                try:\n                    generations = generator.generate(**request_object)\n                except RuntimeError:\n                    # Probably cuda died. Unfortunately, we need to hard crash\n                    # here to kick in our self-healing mechanisms.\n                    raise\n                except Exception as e:\n                    # propagate any exceptions to the response so we can report it\n                    generations = [e] * len(batch)\n                # broadcast them back\n                for work_item, gen in zip(batch, generations):\n                    work_item.return_queue.put((work_item.uid, gen))\n\n                batch.clear()\n            else:\n                # back to the loop\n                continue\n\n\ndef worker_main(cfg1: MetaseqConfig, namespace_args=None):\n    # disable multithreading in tokenizers and torch, as different Flask threads\n    # may then fight for resources.\n    os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\"\n    torch.set_num_threads(1)\n    global generator\n    global MODE\n\n    # make sure generations are stochastic since we have many workers\n    torch.manual_seed(random.randint(1, 20000))\n    torch.cuda.manual_seed(random.randint(1, 20000))\n    MODE = \"worker\"\n    cfg = cfg1\n\n    generator = GeneratorInterface(cfg)\n    models = generator.load_model()  # noqa: F841\n\n    logger.info(f\"loaded model {cfg.distributed_training.distributed_rank}\")\n    if torch.distributed.is_initialized():\n        request_object = distributed_utils.broadcast_object(\n            None, src_rank=0, group=distributed_utils.get_global_group()\n        )\n\n    if not torch.distributed.is_initialized() or torch.distributed.get_rank() == 0:\n        logger.info(f\"Worker engaged! {get_my_ip()}:{port}\")\n        thread = threading.Thread(target=batching_loop, daemon=True)\n        thread.start()\n        app.run(host=\"0.0.0.0\", port=port, threaded=True)\n    else:\n        # useful in FSDP setting\n        logger.info(f\"Looping engaged! {get_my_ip()}:{port}\")\n        while True:\n            try:\n                request_object = distributed_utils.broadcast_object(\n                    None, src_rank=0, group=distributed_utils.get_global_group()\n                )\n                _ = generator.generate(**request_object)\n            except Exception:\n                # continue looping for the next generation so we don't lock up\n                pass\n\n\n@app.errorhandler(Exception)\ndef handle_exception(e):\n    # pass through HTTP errors\n    if isinstance(e, HTTPException):\n        return e\n\n    http_code = 400 if isinstance(e, ValueError) else 500\n    return _create_error_response(\n        str(e), http_code, stacktrace=traceback.format_tb(e.__traceback__)\n    )\n\n\ndef _validate_key(key):\n    # denylist a few placeholders various people have used\n    if key == \"\":\n        return False\n    if \"YOUR_NAME_HERE\" in key:\n        return False\n    if \"$USER\" in key:\n        return False\n    if \"your-key-here\" in key:\n        return False\n    return True\n\n\ndef _create_error_response(msg, http_code, **others):\n    error_dict = {\n        \"message\": msg,\n        \"type\": \"invalid_request_error\",\n        \"param\": None,\n        \"code\": None,\n        **others,\n    }\n    response = jsonify({\"error\": error_dict})\n    response.status = http_code\n    return response\n\n\n@app.route(\"/completions\", methods=[\"POST\"])\n@app.route(\"/v1/engines/<engine>/completions\", methods=[\"POST\"])\n@app.route(\"/v2/engines/<engine>/completions\", methods=[\"POST\"])\n@app.route(\"/engines/<engine>/completions\", methods=[\"POST\"])\ndef completions(engine=None):\n    # before anything else, check that we've got a valid API key\n    if not _validate_key(request.headers.get(\"authorization\", \"\")):\n        return _create_error_response(\"Invalid API key or API key missing.\", 401)\n\n    # prompt can be 4 types:\n    # - str. Basic case. Return one generation.\n    # - list of ints. Pretokenized. Return one generation\n    # - list of str. Multiple generations, one per prompt\n    # - list of list of ints. Pretokenized multiple generations.\n\n    # our approach is to turn everything into the last case\n\n    prompts = request.json[\"prompt\"]\n    del request.json[\"prompt\"]\n    generation_args = request.json\n\n    if isinstance(prompts, str):\n        # single string. tokenize and turn it to the single pre-tokenized case\n        prompts = [generator.encode_fn(prompts)]\n    assert isinstance(prompts, list)\n    assert len(prompts) > 0\n    if isinstance(prompts[0], str):\n        # multi string\n        prompts = [generator.encode_fn(p) for p in prompts]\n    elif isinstance(prompts[0], int):\n        # single pre-tokenized\n        prompts = [prompts]\n    assert isinstance(prompts[0], list)\n    # final case: multi pre-tokenized\n    assert len(prompts[0]) > 0\n\n    if \"min_tokens\" in generation_args:\n        generation_args[\"min_tokens\"] = int(generation_args[\"min_tokens\"])\n    if \"max_tokens\" in generation_args:\n        generation_args[\"max_tokens\"] = int(generation_args[\"max_tokens\"])\n    if \"stop\" in generation_args:\n        stop = generation_args[\"stop\"]\n        if stop is None:\n            pass\n        elif isinstance(stop, str):\n            stop = [generator.encode_fn(stop)[0]]\n        else:\n            stop = [generator.encode_fn(s)[0] for s in stop]\n        generation_args[\"stop\"] = stop\n    if \"temperature\" in generation_args:\n        generation_args[\"temperature\"] = round(float(generation_args[\"temperature\"]), 1)\n    else:\n        generation_args[\"temperature\"] = 1.0\n    if \"top_p\" in generation_args:\n        generation_args[\"top_p\"] = round(float(generation_args[\"top_p\"]), 1)\n    else:\n        generation_args[\"top_p\"] = 1.0\n    if \"n\" not in generation_args:\n        generation_args[\"n\"] = 1\n    if \"best_of\" not in generation_args:\n        generation_args[\"best_of\"] = generation_args[\"n\"]\n    # beam search\n    if int(generation_args[\"best_of\"]) > MAX_BEAM:\n        logger.warning(\n            f'beam size/sampling size of {int(generation_args[\"best_of\"])} too large, using {MAX_BEAM} to avoid OOM'\n        )\n        generation_args[\"best_of\"] = MAX_BEAM\n        generation_args[\"n\"] = min(MAX_BEAM, int(generation_args[\"n\"]))\n\n    if \"logprobs\" in generation_args:\n        generation_args[\"logprobs\"] = int(generation_args[\"logprobs\"])\n    else:\n        generation_args[\"logprobs\"] = 0\n\n    # factual nucleus omega bound\n    if \"omega_bound\" in generation_args:\n        generation_args[\"omega_bound\"] = round(float(generation_args[\"omega_bound\"]), 1)\n    else:\n        generation_args[\"omega_bound\"] = 0.3\n\n    # factual nucleus lambda decay\n    if \"lambda_decay\" in generation_args:\n        generation_args[\"lambda_decay\"] = round(\n            float(generation_args[\"lambda_decay\"]), 1\n        )\n    else:\n        generation_args[\"lambda_decay\"] = -1\n\n    # repetition penalties\n    for key in [\"alpha_frequency\", \"alpha_presence\"]:\n        for suffix in [\"\", \"_src\"]:\n            _gen_arg = f\"{key}{suffix}\"\n            if _gen_arg in generation_args:\n                generation_args[_gen_arg] = round(float(generation_args[_gen_arg]), 1)\n            else:\n                generation_args[_gen_arg] = 0\n\n    if \"alpha_src_penalty_end_idx\" in generation_args:\n        generation_args[\"alpha_src_penalty_end_idx\"] = int(\n            generation_args[\"alpha_src_penalty_end_idx\"]\n        )\n    else:\n        generation_args[\"alpha_src_penalty_end_idx\"] = -1\n\n    ret_queue = queue.Queue()\n    for i, prompt in enumerate(prompts):\n        gen_len = generation_args.get(\"max_tokens\", 0)\n        if gen_len + len(prompt) + 1 > MAX_SEQ_LEN:\n            # cut off the prompt to always fit with number of generations we need\n            # +1 to always have the EOS token\n            logger.warning(\n                f\"input too long, truncated to {MAX_SEQ_LEN - gen_len - 1} to avoid OOM\"\n            )\n            prompt = prompt[-(MAX_SEQ_LEN - gen_len - 1) :]\n        request_object = {\"input\": prompt, **generation_args}\n        BATCH_QUEUE.put(\n            WorkItem(\n                cost=len(prompt) + gen_len,\n                uid=i,\n                return_queue=ret_queue,\n                data=request_object,\n                prompt_len=len(prompt),\n                gen_len=gen_len,\n            )\n        )\n    unordered_results = []\n    for _ in prompts:\n        unordered_results.append(ret_queue.get())\n    # resort results by the original ordering\n    # weirdly, openai returns to you a flat list if you gave multiple prompts\n    reordered = sorted(unordered_results, key=lambda x: x[0])\n    results = []\n    for prompt, (_, generations) in zip(prompts, reordered):\n        if isinstance(generations, Exception):\n            raise generations\n        results += generations\n    # transform the result into the openai format\n    return OAIResponse(results).__dict__()\n\n\n@app.route(\"/\")\ndef index():\n    # TODO(roller): decouple demopage.html\n    fn = pkg_resources.resource_filename(\"metaseq\", \"service/index.html\")\n    with open(fn) as f:\n        return f.read()\n\n\ndef cli_main():\n    \"\"\"\n    Hosted version of the web UI for generation.\n    \"\"\"\n\n    global port, MODE, cfg\n    parser = options.get_generation_parser()\n\n    # dumb defaults overriding\n    parser.set_defaults(lr_scheduler=None, criterion=None)\n    flat_launch_args = []\n    for s in LAUNCH_ARGS:\n        flat_launch_args += s.split()\n    args = options.parse_args_and_arch(parser, input_args=flat_launch_args)\n    args.data = os.path.dirname(args.path)  # hardcode the data arg\n    port = DEFAULT_PORT\n    cfg = convert_namespace_to_omegaconf(args)\n    cfg.distributed_training.distributed_world_size = TOTAL_WORLD_SIZE\n\n    model_overrides = ast.literal_eval(cfg.common_eval.model_overrides)\n    model_overrides.update(INFERENCE_ARG_OVERRIDES)\n    cfg.common_eval.model_overrides = str(model_overrides)\n\n    distributed_utils.call_main(cfg, worker_main, namespace_args=args)\n\n\nif __name__ == \"__main__\":\n    if os.getenv(\"SLURM_NODEID\") is None:\n        logger.warning(\n            f\"Missing slurm configuration, defaulting to 'use entire node' for API\"\n        )\n        os.environ[\"SLURM_NODEID\"] = \"0\"\n        os.environ[\"SLURM_NNODES\"] = \"1\"\n        os.environ[\"SLURM_NTASKS\"] = \"1\"\n        import socket\n\n        os.environ[\"SLURM_STEP_NODELIST\"] = socket.gethostname()\n    cli_main()\n",
        "metaseq/cli/train.py": "#!/usr/bin/env python3 -u\n# Copyright (c) Meta Platforms, Inc. and affiliates. All Rights Reserved.\n#\n# This source code is licensed under the MIT license found in the\n# LICENSE file in the root directory of this source tree.\n\"\"\"\nTrain a new model on one or across multiple GPUs.\n\"\"\"\n\nimport argparse\nfrom datetime import datetime\nimport functools\nimport logging\nimport math\nimport os\nimport subprocess\nimport sys\nimport time\nimport socket\nimport re\nfrom typing import Dict, Optional, Any, List, Tuple, Callable\nfrom urllib.parse import urlparse\nimport warnings\n\nimport numpy as np\nimport torch\nimport torch.profiler as profiler\nfrom omegaconf import DictConfig, OmegaConf\n\nfrom metaseq import (\n    checkpoint_utils,\n    options,\n    tasks,\n    utils,\n)\nfrom metaseq.data import iterators, data_utils\nfrom metaseq.data.plasma_utils import PlasmaStore\nfrom metaseq.dataclass.utils import convert_namespace_to_omegaconf\nfrom metaseq.distributed import fsdp_enable_wrap, fsdp_wrap, utils as distributed_utils\nfrom metaseq.file_io import PathManager\nfrom metaseq.logging import meters, metrics, progress_bar\nfrom metaseq.trainer import Trainer\n\nlogging.basicConfig(\n    format=\"%(asctime)s | %(levelname)s | %(name)s | %(message)s\",\n    datefmt=\"%Y-%m-%d %H:%M:%S\",\n    level=os.environ.get(\"LOGLEVEL\", \"INFO\").upper(),\n    stream=sys.stdout,\n)\nlogging.Formatter.converter = time.gmtime  # Enforce UTC timestamps\nlogger = logging.getLogger(\"metaseq.cli.train\")\n\n\ndef main(cfg: DictConfig) -> None:\n    utils.import_user_module(cfg.common)\n    warnings.filterwarnings(\n        \"ignore\",\n        message=\"torch.distributed._all_gather_base is a private function and will be deprecated\",\n    )\n    warnings.filterwarnings(\n        \"ignore\",\n        message=\"torch.distributed._reduce_scatter_base is a private function and will be deprecated\",\n    )\n    # replace with actual job id\n    slurm_jobid = os.environ.get(\"SLURM_JOBID\", None)\n    if \"%jobid\" in cfg.checkpoint.save_dir and slurm_jobid is not None:\n        cfg.checkpoint.save_dir = cfg.checkpoint.save_dir.replace(\"%jobid\", slurm_jobid)\n\n    checkpoint_utils.verify_checkpoint_directory(cfg.checkpoint.save_dir)\n\n    if distributed_utils.is_master(cfg.distributed_training) and os.environ.get(\n        \"METASEQ_SAVE_DIR\"\n    ):\n        # save a (vaguely human readable) copy of the training config\n        # TODO(roller): only works when launched with a sweep script\n        # should fix that\n        OmegaConf.save(\n            config=_flatten_config(cfg),\n            f=os.path.join(os.environ[\"METASEQ_SAVE_DIR\"], \"config.yml\"),\n        )\n\n    if (\n        distributed_utils.is_master(cfg.distributed_training)\n        and \"job_logging_cfg\" in cfg\n    ):\n        # make hydra logging work with ddp (see # see https://github.com/facebookresearch/hydra/issues/1126)\n        logging.config.dictConfig(OmegaConf.to_container(cfg.job_logging_cfg))\n\n    assert (\n        cfg.dataset.max_tokens is not None or cfg.dataset.batch_size is not None\n    ), \"Must specify batch size either with --max-tokens or --batch-size\"\n    metrics.reset()\n\n    if cfg.checkpoint.local_save_interval_updates > 0:\n        assert (\n            cfg.checkpoint.save_interval_updates > 0\n        ), \"local save must be used with --save-interval-updates > 0\"\n        assert (\n            cfg.checkpoint.save_interval_updates\n            % cfg.checkpoint.local_save_interval_updates\n            == 0\n        ), \"--save-interval-updates must be a multiple of --local-save-interval-updates\"\n\n    if cfg.common.log_file is not None:\n        handler = logging.FileHandler(filename=cfg.common.log_file)\n        logger.addHandler(handler)\n\n    np.random.seed(cfg.common.seed)\n    utils.set_torch_seed(cfg.common.seed)\n\n    # Print nvidia smi stats\n    logger.info(metrics.get_nvidia_smi_gpu_memory_stats_str())\n\n    # Print args\n    logger.info(cfg)\n\n    # Setup task, e.g., translation, language modeling, etc.\n    if cfg.distributed_training.task_ddp_backend == \"fully_sharded\":\n        # As the task is non-trainable, we switch flags to more optimized ones.\n        # See https://github.com/facebookresearch/metaseq/pull/668 for when/why this was added.\n        orig_memory_efficient_fp16 = cfg.distributed_training.memory_efficient_fp16\n        orig_fp32_reduce_scatter = cfg.distributed_training.fp32_reduce_scatter\n        # Clobber memory_efficient_fp16 and fp32_reduce_scatter\n        cfg.distributed_training.memory_efficient_fp16 = cfg.distributed_training.fp16\n        cfg.distributed_training.fp32_reduce_scatter = not cfg.distributed_training.fp16\n\n        with fsdp_enable_wrap(\n            cfg.distributed_training,\n            use_sharded_state=cfg.distributed_training.use_sharded_state,\n        ):\n            task = tasks.setup_task(cfg.task)\n\n        # Reset memory_efficient_fp16 and fp32_reduce_scatter values.\n        cfg.distributed_training.memory_efficient_fp16 = orig_memory_efficient_fp16\n        cfg.distributed_training.fp32_reduce_scatter = orig_fp32_reduce_scatter\n    else:\n        task = tasks.setup_task(cfg.task)\n\n    # Build model and criterion\n    assert cfg.criterion, \"Please specify criterion to train a model\"\n\n    if cfg.distributed_training.ddp_backend == \"fully_sharded\":\n        with fsdp_enable_wrap(\n            cfg.distributed_training,\n            use_sharded_state=cfg.distributed_training.use_sharded_state,\n        ):\n            model = fsdp_wrap(\n                task.build_model(cfg.model),\n                process_group=distributed_utils.get_data_parallel_group(),\n            )\n    else:\n        model = task.build_model(cfg.model)\n\n    # TODO[Susan]: FSDP on criterion?\n    criterion = task.build_criterion(cfg.criterion)\n\n    logger.info(model)\n    logger.info(\"task: {}\".format(task.__class__.__name__))\n    logger.info(\"model: {}\".format(model.__class__.__name__))\n    logger.info(\"criterion: {}\".format(criterion.__class__.__name__))\n    logger.info(\n        \"num. model params: {:,} (num. trained: {:,})\".format(\n            sum(getattr(p, \"_orig_size\", p).numel() for p in model.parameters()),\n            sum(\n                getattr(p, \"_orig_size\", p).numel()\n                for p in model.parameters()\n                if p.requires_grad\n            ),\n        )\n    )\n    logger.info(metrics.get_nvidia_smi_gpu_memory_stats_str())\n\n    # Load valid dataset (we load training data below, based on the latest checkpoint)\n    # We load the valid dataset AFTER building the model\n    data_utils.raise_if_valid_subsets_unintentionally_ignored(cfg)\n    if cfg.dataset.combine_valid_subsets:\n        task.load_dataset(\"valid\", combine=True, epoch=1)\n    else:\n        for valid_sub_split in cfg.dataset.valid_subset.split(\",\"):\n            task.load_dataset(valid_sub_split, combine=False, epoch=1)\n\n    # Build trainer\n    trainer = Trainer(cfg, task, model, criterion)\n    logger.info(\n        \"training on {} devices (GPUs/TPUs)\".format(\n            cfg.distributed_training.distributed_world_size\n        )\n    )\n    logger.info(\n        \"max tokens per GPU = {} and batch size per GPU = {}\".format(\n            cfg.dataset.max_tokens,\n            cfg.dataset.batch_size,\n        )\n    )\n    logger.info(metrics.get_nvidia_smi_gpu_memory_stats_str())\n\n    # Load the latest checkpoint if one is available and restore the\n    # corresponding train iterator\n    extra_state, epoch_itr = checkpoint_utils.load_checkpoint(\n        cfg.checkpoint,\n        trainer,\n        # don't cache epoch iterators for sharded datasets\n        disable_iterator_cache=True,\n    )\n\n    max_epoch = cfg.optimization.max_epoch or math.inf\n    train_meter = meters.StopwatchMeter()\n    train_meter.start()\n    while epoch_itr.next_epoch_idx <= max_epoch:\n        # train for one epoch\n        valid_losses, should_stop = train(cfg, trainer, task, epoch_itr)\n        if should_stop:\n            break\n\n        # only use first validation loss to update the learning rate\n        trainer.lr_step(epoch_itr.epoch, valid_losses[0])\n\n        epoch_itr = trainer.get_train_iterator(\n            epoch_itr.next_epoch_idx,\n            # don't cache epoch iterators for sharded datasets\n            disable_iterator_cache=True,\n        )\n    train_meter.stop()\n    logger.info(\"done training in {:.1f} seconds\".format(train_meter.sum))\n\n\ndef get_skip_batches(to_skip):\n    skip_batches = {}\n    for skip_range in to_skip.split(\",\"):\n        if \"-\" in skip_range:\n            start, end = skip_range.split(\"-\")\n            skip_batches[int(start)] = int(end) - int(start) + 1\n        else:\n            skip_batches[int(skip_range)] = 1\n    return skip_batches\n\n\n@metrics.aggregate(\"train\")\ndef train(\n    cfg: DictConfig, trainer: Trainer, task: tasks.BaseTask, epoch_itr\n) -> Tuple[List[Optional[float]], bool]:\n    \"\"\"Train the model for one epoch and return validation losses.\"\"\"\n    # Initialize data iterator\n    itr = epoch_itr.next_epoch_itr(\n        fix_batches_to_gpus=cfg.distributed_training.fix_batches_to_gpus,\n        shuffle=True,\n    )\n    update_freq = (\n        cfg.optimization.update_freq[epoch_itr.epoch - 1]\n        if epoch_itr.epoch <= len(cfg.optimization.update_freq)\n        else cfg.optimization.update_freq[-1]\n    )\n    if update_freq > 1:\n        itr = iterators.GroupedIterator(\n            itr,\n            update_freq,\n            skip_remainder_batch=True,\n        )\n\n    progress = progress_bar.get_progress_bar(\n        itr,\n        log_format=cfg.common.log_format,\n        log_file=cfg.common.log_file,\n        log_interval=cfg.common.log_interval,\n        epoch=epoch_itr.epoch,\n        tensorboard_logdir=(\n            cfg.common.tensorboard_logdir\n            if distributed_utils.is_master(cfg.distributed_training)\n            else None\n        ),\n        aim_repo=(\n            cfg.common.aim_repo\n            if distributed_utils.is_master(cfg.distributed_training)\n            else None\n        ),\n        aim_run_hash=(\n            cfg.common.aim_run_hash\n            if distributed_utils.is_master(cfg.distributed_training)\n            else None\n        ),\n        aim_param_checkpoint_dir=cfg.checkpoint.save_dir,\n        wandb_project=(\n            cfg.common.wandb_project\n            if distributed_utils.is_master(cfg.distributed_training)\n            else None\n        ),\n        wandb_run_name=os.environ.get(\n            \"WANDB_NAME\", os.path.basename(cfg.checkpoint.save_dir)\n        ),\n    )\n    progress.update_config(_flatten_config(cfg))\n\n    trainer.begin_epoch(epoch_itr.epoch)\n    valid_subsets = cfg.dataset.valid_subset.split(\",\")\n    should_stop = False\n    logger.info(\"Start iterating over samples\")\n\n    def train(\n        samples,\n    ):\n        with metrics.aggregate(\"train_inner\"):\n            if update_freq == 1:\n                samples = [samples]\n            log_output = trainer.train_step(samples)\n\n        if log_output is not None:  # not OOM, overflow, ...\n            # log mid-epoch stats\n            num_updates = trainer.get_num_updates()\n            if num_updates % cfg.common.log_interval == 0:\n                stats = get_training_stats(metrics.get_smoothed_values(\"train_inner\"))\n                progress.log(stats, tag=\"train_inner\", step=num_updates)\n\n                # reset mid-epoch stats after each log interval\n                # the end-of-epoch stats will still be preserved\n                metrics.reset_meters(\"train_inner\")\n\n        end_of_epoch = not itr.has_next()\n        if end_of_epoch:\n            grank = distributed_utils.get_global_rank()\n\n            log_seq = [f\"End of Epoch on rank {grank}:\"]\n            if hasattr(itr, \"sequences_consumed\"):\n                log_seq += [f\"sequences_consumed={itr.sequences_consumed}\"]\n            log_seq += [f\"n={itr.n}\"]\n\n            dataset = epoch_itr.dataset\n            while not hasattr(dataset, \"len_cache\") and hasattr(dataset, \"dataset\"):\n                dataset = dataset.dataset\n            if hasattr(dataset, \"len_cache\"):\n                len_cache = tuple(dataset.len_cache.data)\n                cache_hash = hash(len_cache)\n                contains_zero = any([x == 0 for x in len_cache])\n                log_seq += [\n                    f\"len_cache_hash={cache_hash}\",\n                    f\"len_cache_has_zeros={contains_zero}\",\n                ]\n            logger.warning(\" \".join(log_seq))\n\n        valid_losses, should_stop = validate_and_save(\n            cfg,\n            trainer,\n            task,\n            epoch_itr,\n            valid_subsets,\n            end_of_epoch,\n            log_output is not None,\n        )\n\n        return valid_losses, should_stop\n\n    skip_batches = None\n    if len(cfg.dataset.skip_batches) > 0:\n        skip_batches = get_skip_batches(cfg.dataset.skip_batches)\n\n    progress_iter = iter(progress)\n    i = 0\n    while True:\n        try:\n            with metrics.aggregate(\"train_inner\"):\n                metrics.log_start_time(\"time_sample_fetch\", round=4)\n                samples = next(progress_iter)\n                metrics.log_stop_time(\"time_sample_fetch\")\n\n            current_step = trainer.get_num_updates() + 1\n            if (\n                skip_batches is not None\n                and current_step in skip_batches\n                and skip_batches[current_step] > 0\n            ):\n                skip_batches[current_step] -= 1\n                logger.info(\n                    f\"Skipping batches starting from step {current_step} with \"\n                    f\"{skip_batches[current_step]} batches left to skip\"\n                )\n                continue\n\n            if (\n                distributed_utils.get_global_rank() == 0\n                and cfg.common.profile\n                and i == 5\n            ):\n                logger.info(\"STARTING PROFILER\")\n                with profiler.profile(\n                    profile_memory=True, with_stack=True, record_shapes=True\n                ) as prof:\n                    valid_losses, should_stop = train(samples)\n                torch.cuda.synchronize()\n                with open(\n                    os.path.join(cfg.checkpoint.save_dir, \"memory_usage.txt\"), \"a\"\n                ) as sourceFile:\n                    print(\n                        prof.key_averages(group_by_stack_n=5).table(\n                            sort_by=\"self_cuda_memory_usage\", row_limit=10\n                        ),\n                        file=sourceFile,\n                    )\n                prof.export_chrome_trace(\n                    os.path.join(cfg.checkpoint.save_dir, \"profiler_trace.json\")\n                )\n            else:\n                valid_losses, should_stop = train(samples)\n            if should_stop:\n                break\n\n            i += 1\n        except StopIteration:\n            break\n\n    # reset epoch-level meters\n    metrics.reset_meters(\"train\")\n    return valid_losses, should_stop\n\n\ndef _flatten_config(cfg: DictConfig):\n    config = OmegaConf.to_container(cfg)\n    # remove any legacy Namespaces and replace with a single \"args\"\n    namespace = None\n    for k, v in list(config.items()):\n        if isinstance(v, argparse.Namespace):\n            namespace = v\n            del config[k]\n    if namespace is not None:\n        config[\"args\"] = vars(namespace)\n    return config\n\n\ndef validate_and_save(\n    cfg: DictConfig,\n    trainer: Trainer,\n    task: tasks.BaseTask,\n    epoch_itr,\n    valid_subsets: List[str],\n    end_of_epoch: bool,\n    was_successful_step: bool,\n) -> Tuple[List[Optional[float]], bool]:\n    num_updates = trainer.get_num_updates()\n    max_update = cfg.optimization.max_update or math.inf\n\n    # was_successful_step is necessary since we don't increment step counters\n    # on OOM or overflow. Thus if we get multiple bad steps right after\n    # loading a checkpoint (when step counter is exactly when we would step)\n    # then we will start overwriting! omg!\n\n    # Stopping conditions (and an additional one based on validation loss later\n    # on)\n    should_stop = False\n    if num_updates >= max_update:\n        should_stop = True\n        logger.info(\n            f\"Stopping training due to \"\n            f\"num_updates: {num_updates} >= max_update: {max_update}\"\n        )\n\n    save_locally = (\n        cfg.checkpoint.local_save_interval_updates > 0\n        and num_updates > 0\n        and num_updates % cfg.checkpoint.local_save_interval_updates == 0\n    )\n    save_to_NFS = (\n        cfg.checkpoint.save_interval_updates > 0\n        and num_updates > 0\n        and num_updates % cfg.checkpoint.save_interval_updates == 0\n    )\n\n    do_save = (\n        (\n            end_of_epoch\n            and cfg.checkpoint.save_interval_epochs > 0\n            and epoch_itr.epoch % cfg.checkpoint.save_interval_epochs == 0\n        )\n        or (\n            (save_locally or save_to_NFS)\n            and num_updates >= cfg.dataset.validate_after_updates\n            and was_successful_step\n        )\n        or should_stop\n    )\n    do_validate = (\n        should_stop\n        or (\n            cfg.dataset.validate_interval_updates > 0\n            and num_updates > 0\n            and num_updates % cfg.dataset.validate_interval_updates == 0\n            and was_successful_step\n        )\n    ) and not cfg.dataset.disable_validation\n\n    # Save checkpoint before validating.\n    if do_save:\n        checkpoint_utils.save_checkpoint(\n            cfg.checkpoint,\n            trainer,\n            epoch_itr,\n            training_finished=should_stop,\n            async_callback_fn=functools.partial(\n                post_checkpoint_callback, cfg, num_updates, should_stop\n            )\n        )\n\n    valid_losses = [None]\n    if do_validate:\n        valid_losses = validate(cfg, trainer, task, epoch_itr, valid_subsets)\n\n    trainer.reset_dummy_batch(epoch_itr.first_batch)\n    return valid_losses, should_stop\n\n\ndef _checkpoint_add_directory(basename):\n    pattern = r\"(checkpoint(\\d+|_\\d+|_last))(.*)\"\n    m = re.match(pattern, basename)\n    assert m, f\"checkpoint file doesn't follow pattern {pattern}\"\n    return m[1], f\"checkpoint{m[3]}\"\n\n\ndef _get_basename(path):\n    res = urlparse(path)\n    if res.scheme:\n        return os.path.basename(res.path)\n    else:\n        return os.path.basename(path)\n\n\ndef _get_destination_path(path, destination):\n    \"\"\"Calculates the destination path with handling for remote paths.\"\"\"\n    basename = _get_basename(path)\n    res = urlparse(destination)\n    if res.scheme:\n        new_path = os.path.join(res.path, basename)\n        res = res._replace(path=new_path)\n        return res.geturl()\n    else:\n        return os.path.join(destination, basename)\n\n\ndef post_checkpoint_callback(\n    cfg, num_updates, training_finished, filename, files_to_symlink_to\n):\n    if cfg.checkpoint.cloud_upload_path is not None:\n        if \"blob.core.windows.net\" in cfg.checkpoint.cloud_upload_path:\n            azcopy_log_dir = os.path.dirname(filename)\n            final_path = _get_destination_path(\n                filename, cfg.checkpoint.cloud_upload_path\n            )\n            _copy_to_azure(filename, final_path, azcopy_log_dir)\n\n            # Delete original checkpoint on local storage\n            # TODO make this configurable\n            os.remove(filename)\n\n            # Azure Blob doesn't support symlinks so make full copies\n            if files_to_symlink_to:\n                for other_checkpoint in files_to_symlink_to:\n                    dest = _get_destination_path(\n                        other_checkpoint, cfg.checkpoint.cloud_upload_path\n                    )\n                    _copy_to_azure(final_path, dest, azcopy_log_dir)\n\n        elif cfg.checkpoint.cloud_upload_path.startswith(\"nfs:\"):\n            basename = os.path.basename(filename)\n            checkpoint_dir, checkpoint_file = _checkpoint_add_directory(basename)\n            destination_checkpoints_dir = cfg.checkpoint.cloud_upload_path[4:]\n            temporary_checkpoint_file = f\"_{checkpoint_file}\"\n            try:\n                os.mkdir(os.path.join(destination_checkpoints_dir, checkpoint_dir))\n            except FileExistsError:\n                pass  # another worker got here first\n            logger.info(f\"Beginning copy of {filename} to NFS\")\n\n            # copy the checkpoint from local storage to nfs in the background\n            subprocess.run(\n                [\n                    \"cp\",\n                    filename,\n                    os.path.join(\n                        destination_checkpoints_dir,\n                        checkpoint_dir,\n                        temporary_checkpoint_file,\n                    ),\n                ]\n            )\n\n            logger.info(f\"Renaming {temporary_checkpoint_file} -> {checkpoint_file}\")\n            final_path = os.path.join(\n                destination_checkpoints_dir, checkpoint_dir, checkpoint_file\n            )\n            # atomic rename _checkpointfile -> checkpointfile\n            # this way we know that if present the checkpoint file is complete\n            os.rename(\n                os.path.join(\n                    destination_checkpoints_dir,\n                    checkpoint_dir,\n                    temporary_checkpoint_file,\n                ),\n                final_path,\n            )\n            os.remove(filename)\n\n            if files_to_symlink_to:\n                for other_checkpoint in files_to_symlink_to:\n                    basename = os.path.basename(other_checkpoint)\n                    subdir, _ = _checkpoint_add_directory(basename)\n                    other_dir = os.path.join(destination_checkpoints_dir, subdir)\n                    os.makedirs(other_dir, exist_ok=True)\n                    dest = os.path.join(other_dir, basename)\n                    if PathManager.islink(dest):\n                        PathManager.rm(dest)\n                    assert PathManager.symlink(\n                        final_path, dest\n                    ), f\"Failed to symlink {final_path} to {dest}\"\n\n            # Start running evals on uploaded checkpoint\n            nfs_evaluation(\n                cfg,\n                num_updates,\n                training_finished,\n                checkpoint_dir,\n                destination_checkpoints_dir,\n            )\n\n        else:\n            try:\n                # PathManager only supports writing to S3, but this function call\n                # can be replaced with other APIs for copying checkpoints.\n                final_path = _get_destination_path(\n                    filename, cfg.checkpoint.cloud_upload_path\n                )\n                PathManager.copy_from_local(filename, final_path, overwrite=True)\n\n                # Some non-native PathHandlers don't support symlinks so default to full copies\n                if files_to_symlink_to:\n                    for other_checkpoint in files_to_symlink_to:\n                        dest = _get_destination_path(\n                            other_checkpoint, cfg.checkpoint.cloud_upload_path\n                        )\n                        PathManager.copy(final_path, dest, overwrite=True)\n            except (FileNotFoundError, AssertionError) as e:\n                logger.info(f\"could not upload {filename}: {e}\")\n    else:\n        if files_to_symlink_to:\n            for other_checkpoint in files_to_symlink_to:\n                if PathManager.islink(other_checkpoint):\n                    PathManager.rm(other_checkpoint)\n                assert PathManager.symlink(\n                    filename, other_checkpoint\n                ), f\"Failed to symlink {filename} to {other_checkpoint}\"\n\n\ndef nfs_evaluation(\n    cfg, num_updates, training_finished, checkpoint_dir, destination_checkpoints_dir\n):\n    if (\n        cfg.checkpoint.nfs_eval_script_path is not None\n        and distributed_utils.get_global_rank() == 0\n        and (\n            (\n                cfg.checkpoint.nfs_eval_frequency > 0\n                and num_updates % cfg.checkpoint.nfs_eval_frequency == 0\n            )\n            or training_finished\n        )\n    ):\n        for retry in range(cfg.checkpoint.nfs_eval_num_attempts):\n            time.sleep(cfg.checkpoint.nfs_eval_attempt_wait_minutes * 60)\n\n            current_checkpoint_path = os.path.join(\n                destination_checkpoints_dir, checkpoint_dir\n            )\n            num_files = os.listdir(current_checkpoint_path)\n            # only count completed checkpoints\n            finished_checkpoint_parts = len(\n                [f for f in num_files if not f.startswith(\"_\")]\n            )\n            if (\n                finished_checkpoint_parts\n                == cfg.distributed_training.distributed_world_size\n            ):\n                logger.info(\n                    f\"All checkpoint parts for {checkpoint_dir} are in NFS, will now start to run evals\"\n                )\n                script_dir = os.path.join(\n                    os.environ.get(\"METASEQ_SAVE_DIR\"),\n                    cfg.checkpoint.nfs_eval_script_path,\n                )\n                res = subprocess.run(\n                    [\n                        \"bash\",\n                        script_dir,\n                        os.path.join(current_checkpoint_path, \"checkpoint.pt\"),\n                    ],\n                    stdout=subprocess.PIPE,\n                    stderr=subprocess.PIPE,\n                )\n                if res.returncode == 0:\n                    logger.info(f\"Sucessfully evaluated {checkpoint_dir}\")\n                else:\n                    logger.error(f\"Error during evaluation: {res.returncode}\")\n                    logger.error(f\"Eval script stdout = {res.stdout}\")\n                    logger.error(f\"Eval script stderr = {res.stderr}\")\n                return\n        logger.error(\n            (\n                f\"Did not evaluate {checkpoint_dir}, as only {num_files}/\"\n                f\"{cfg.distributed_training.distributed_world_size} \"\n                \"checkpoint parts were copied to NFS within waiting time\"\n            )\n        )\n\n\ndef _copy_to_azure(source, destination, log_dir):\n    # /dir/checkpoint_last.pt -> /dir/checkpoint_last.pt_azcopy_logs_2000-01-01T00_00_00\n    basename = _get_basename(destination)\n    timestamp = datetime.utcnow().isoformat().replace(\":\", \"_\")[:-7]\n    azcopy_logs = os.path.join(log_dir, f\"{basename}_azcopy_logs_{timestamp}\")\n    os.environ[\"AZCOPY_CONCURRENCY_VALUE\"] = \"10\"\n    os.environ[\"AZCOPY_LOG_LOCATION\"] = azcopy_logs\n    os.makedirs(azcopy_logs, exist_ok=True)\n    logger.info(f\"preparing to azcopy {source} to {destination}; logs in {azcopy_logs}\")\n    cmd = [\n        \"azcopy\",  # TODO(susanz): require azcopy to be installed.\n        \"copy\",\n        \"--cap-mbps\",\n        \"96.0\",\n        source,\n        destination,\n    ]\n    res = _run_azcopy(cmd, stdout=subprocess.PIPE, stderr=subprocess.PIPE)\n    if res.returncode != 0:\n        print(\"Error: {}, azcopy failed\".format(res.returncode))\n        print(\"Azcopy stdout = {}\".format(res.stdout))\n        sys.exit(1)\n    logger.info(f\"Successfully copied {source} to {destination}\")\n\n\ndef _run_azcopy(cmd, stdout, stderr):\n    return subprocess.run(cmd, stdout=subprocess.PIPE, stderr=subprocess.PIPE)\n\n\ndef get_training_stats(stats: Dict[str, Any]) -> Dict[str, Any]:\n    stats[\"wall\"] = round(metrics.get_meter(\"default\", \"wall\").elapsed_time, 0)\n    return stats\n\n\ndef validate(\n    cfg: DictConfig,\n    trainer: Trainer,\n    task: tasks.BaseTask,\n    epoch_itr,\n    subsets: List[str],\n) -> List[Optional[float]]:\n    \"\"\"Evaluate the model on the validation set(s) and return the losses.\"\"\"\n\n    if cfg.dataset.fixed_validation_seed is not None:\n        # set fixed seed for every validation\n        utils.set_torch_seed(cfg.dataset.fixed_validation_seed)\n\n    trainer.begin_valid_epoch(epoch_itr.epoch)\n    valid_losses = []\n    with metrics.aggregate(new_root=True) as combined_agg:\n        for subset in subsets:\n            logger.info(\n                'begin validation on \"{}\" subset on rank {}'.format(\n                    subset, distributed_utils.get_global_rank()\n                )\n            )\n\n            # Initialize data iterator\n            itr = trainer.get_valid_iterator(subset).next_epoch_itr(\n                shuffle=False, set_dataset_epoch=False  # use a fixed valid set\n            )\n\n            logger.info(\n                'got valid iterator on \"{}\" subset on rank {}'.format(\n                    subset, distributed_utils.get_global_rank()\n                )\n            )\n\n            progress = progress_bar.get_progress_bar(\n                itr,\n                log_format=cfg.common.log_format,\n                log_interval=cfg.common.log_interval,\n                epoch=epoch_itr.epoch,\n                prefix=f\"valid on '{subset}' subset\",\n                tensorboard_logdir=(\n                    cfg.common.tensorboard_logdir\n                    if distributed_utils.is_master(cfg.distributed_training)\n                    else None\n                ),\n                aim_repo=(\n                    cfg.common.aim_repo\n                    if distributed_utils.is_master(cfg.distributed_training)\n                    else None\n                ),\n                aim_run_hash=(\n                    cfg.common.aim_run_hash\n                    if distributed_utils.is_master(cfg.distributed_training)\n                    else None\n                ),\n                aim_param_checkpoint_dir=cfg.checkpoint.save_dir,\n                wandb_project=(\n                    cfg.common.wandb_project\n                    if distributed_utils.is_master(cfg.distributed_training)\n                    else None\n                ),\n                wandb_run_name=os.environ.get(\n                    \"WANDB_NAME\", os.path.basename(cfg.checkpoint.save_dir)\n                ),\n            )\n\n            logger.info(\n                'Begin looping over validation \"{}\" subset with length \"{}\"'.format(\n                    subset, len(progress)\n                )\n            )\n\n            # create a new root metrics aggregator so validation metrics\n            # don't pollute other aggregators (e.g., train meters)\n            with metrics.aggregate() as agg:\n                for i, sample in enumerate(progress):\n                    if (\n                        cfg.dataset.max_valid_steps is not None\n                        and i > cfg.dataset.max_valid_steps\n                    ):\n                        break\n                    trainer.valid_step(sample, num_step=i)\n            # log validation stats\n            stats = add_num_updates_to_stats(trainer, agg.get_smoothed_values())\n            progress.print(stats, tag=subset, step=trainer.get_num_updates())\n    stats = add_num_updates_to_stats(trainer, combined_agg.get_smoothed_values())\n    progress.print(stats, tag=\"valid/combined\", step=trainer.get_num_updates())\n    return valid_losses\n\n\ndef add_num_updates_to_stats(trainer: Trainer, stats: Dict[str, Any]) -> Dict[str, Any]:\n    stats[\"num_updates\"] = trainer.get_num_updates()\n    return stats\n\n\ndef set_local_per_worker_env_variables():\n    savedir = os.environ.get(\"METASEQ_SAVE_DIR\")\n    if savedir is not None:\n        hostname = socket.gethostname()\n\n        restart = int(os.environ.get(\"SLURM_RESTART_COUNT\", \"0\"))\n        nccl_dir = os.path.join(savedir, \"nccl\", f\"restart_{restart:03d}\")\n        os.makedirs(nccl_dir, exist_ok=True)\n        rank = int(os.environ.get(\"SLURM_PROCID\", \"0\"))\n        os.environ[\"NCCL_DEBUG_FILE\"] = os.path.join(\n            nccl_dir, f\"rank_{rank:04d}_{hostname}\"\n        )\n\n        # save a copy of all our environmental variables\n        env_dir = os.path.join(savedir, \"envs\", f\"restart_{restart:03d}\")\n        os.makedirs(env_dir, exist_ok=True)\n        with open(os.path.join(env_dir, f\"rank_{rank:04d}_{hostname}\"), \"w\") as f:\n            for key in sorted(os.environ.keys()):\n                f.write(f\"{key}={os.environ[key]}\\n\")\n\n\ndef cli_main(\n    modify_parser: Optional[Callable[[argparse.ArgumentParser], None]] = None\n) -> None:\n    set_local_per_worker_env_variables()\n    parser = options.get_training_parser()\n    args = options.parse_args_and_arch(parser, modify_parser=modify_parser)\n\n    # For training - this is where arg parsing happens.\n    cfg = convert_namespace_to_omegaconf(args)\n\n    if cfg.common.use_plasma_view:\n        server = PlasmaStore(path=cfg.common.plasma_path)\n        logger.info(\n            f\"Started plasma server pid {server.server.pid} {cfg.common.plasma_path}\"\n        )\n\n    distributed_utils.call_main(cfg, main)\n\n\nif __name__ == \"__main__\":\n    cli_main()\n",
        "metaseq/cli/validate.py": "#!/usr/bin/env python3 -u\n# Copyright (c) Meta Platforms, Inc. and affiliates. All Rights Reserved.\n#\n# This source code is licensed under the MIT license found in the\n# LICENSE file in the root directory of this source tree.\n\nimport logging\nimport os\nimport sys\nfrom argparse import Namespace\nfrom itertools import chain\n\nimport torch\nfrom omegaconf import DictConfig\n\nfrom metaseq import checkpoint_utils, distributed_utils, options, utils\nfrom metaseq.dataclass.utils import convert_namespace_to_omegaconf\nfrom metaseq.logging import metrics, progress_bar\n\nlogging.basicConfig(\n    format=\"%(asctime)s | %(levelname)s | %(name)s | %(message)s\",\n    datefmt=\"%Y-%m-%d %H:%M:%S\",\n    level=os.environ.get(\"LOGLEVEL\", \"INFO\").upper(),\n    stream=sys.stdout,\n)\nlogger = logging.getLogger(\"metaseq.cli.validate\")\n\n\ndef main(cfg: DictConfig, override_args=None):\n    if isinstance(cfg, Namespace):\n        cfg = convert_namespace_to_omegaconf(cfg)\n\n    utils.import_user_module(cfg.common)\n\n    assert (\n        cfg.dataset.max_tokens is not None or cfg.dataset.batch_size is not None\n    ), \"Must specify batch size either with --max-tokens or --batch-size\"\n\n    use_fp16 = cfg.common.fp16\n    use_cuda = torch.cuda.is_available() and not cfg.common.cpu\n\n    if use_cuda:\n        torch.cuda.set_device(cfg.distributed_training.device_id)\n\n    if cfg.distributed_training.distributed_world_size > 1:\n        data_parallel_world_size = distributed_utils.get_data_parallel_world_size()\n        data_parallel_rank = distributed_utils.get_data_parallel_rank()\n    else:\n        data_parallel_world_size = 1\n        data_parallel_rank = 0\n\n    if override_args is not None:\n        overrides = vars(override_args)\n        overrides.update(eval(getattr(override_args, \"model_overrides\", \"{}\")))\n    else:\n        overrides = None\n\n    # Load ensemble\n    logger.info(\"loading model(s) from {}\".format(cfg.common_eval.path))\n    models, saved_cfg, task = checkpoint_utils.load_model_ensemble_and_task(\n        [cfg.common_eval.path],\n        arg_overrides=overrides,\n        suffix=cfg.checkpoint.checkpoint_suffix,\n    )\n    model = models[0]\n\n    # Move models to GPU\n    for model in models:\n        if use_fp16:\n            model.half()\n        if use_cuda:\n            model.cuda()\n\n    # Print args\n    logger.info(saved_cfg)\n\n    # Build criterion\n    criterion = task.build_criterion(saved_cfg.criterion)\n    criterion.eval()\n\n    for subset in cfg.dataset.valid_subset.split(\",\"):\n        try:\n            task.load_dataset(subset, combine=False, epoch=1, task_cfg=saved_cfg.task)\n            dataset = task.dataset(subset)\n        except KeyError:\n            raise Exception(\"Cannot find dataset: \" + subset)\n\n        # Initialize data iterator\n        itr = task.get_batch_iterator(\n            dataset=dataset,\n            max_tokens=cfg.dataset.max_tokens,\n            max_sentences=cfg.dataset.batch_size,\n            max_positions=utils.resolve_max_positions(\n                task.max_positions(),\n                *[m.max_positions() for m in models],\n            ),\n            ignore_invalid_inputs=cfg.dataset.skip_invalid_size_inputs_valid_test,\n            required_batch_size_multiple=cfg.dataset.required_batch_size_multiple,\n            seed=cfg.common.seed,\n            num_shards=data_parallel_world_size,\n            shard_id=data_parallel_rank,\n            num_workers=cfg.dataset.num_workers,\n            data_buffer_size=cfg.dataset.data_buffer_size,\n        ).next_epoch_itr(shuffle=False)\n        progress = progress_bar.get_progress_bar(\n            itr,\n            log_format=cfg.common.log_format,\n            log_interval=cfg.common.log_interval,\n            prefix=f\"valid on '{subset}' subset\",\n        )\n\n        log_outputs = []\n        for i, sample in enumerate(progress):\n            sample = utils.move_to_cuda(sample) if use_cuda else sample\n            _loss, _sample_size, log_output = task.valid_step(sample, model, criterion)\n            progress.log(log_output, step=i)\n            log_outputs.append(log_output)\n\n        if data_parallel_world_size > 1:\n            log_outputs = distributed_utils.all_gather_list(\n                log_outputs,\n                max_size=cfg.common.all_gather_list_size,\n                group=distributed_utils.get_data_parallel_group(),\n            )\n            log_outputs = list(chain.from_iterable(log_outputs))\n\n        with metrics.aggregate() as agg:\n            task.reduce_metrics(log_outputs, criterion)\n            log_output = agg.get_smoothed_values()\n\n        progress.print(log_output, tag=subset, step=i)\n\n\ndef cli_main():\n    parser = options.get_validation_parser()\n    args = options.parse_args_and_arch(parser)\n\n    # only override args that are explicitly given on the command line\n    override_parser = options.get_validation_parser()\n    override_args = options.parse_args_and_arch(override_parser, suppress_defaults=True)\n\n    distributed_utils.call_main(\n        convert_namespace_to_omegaconf(args), main, override_args=override_args\n    )\n\n\nif __name__ == \"__main__\":\n    cli_main()\n",
        "metaseq/criterions/__init__.py": "# Copyright (c) Meta Platforms, Inc. and affiliates. All Rights Reserved.\n#\n# This source code is licensed under the MIT license found in the\n# LICENSE file in the root directory of this source tree.\n\"\"\"isort:skip_file\"\"\"\n\nimport importlib\nimport os\n\nfrom metaseq import registry\nfrom metaseq.criterions.base_criterion import BaseCriterion\nfrom omegaconf import DictConfig\n\n\n(\n    build_criterion_,\n    register_criterion,\n    CRITERION_REGISTRY,\n    CRITERION_DATACLASS_REGISTRY,\n) = registry.setup_registry(\n    \"--criterion\", base_class=BaseCriterion, default=\"cross_entropy\"\n)\n\n\ndef build_criterion(cfg: DictConfig, task):\n    return build_criterion_(cfg, task)\n\n\n# automatically import any Python files in the criterions/ directory\nfor file in os.listdir(os.path.dirname(__file__)):\n    if file.endswith(\".py\") and not file.startswith(\"_\"):\n        file_name = file[: file.find(\".py\")]\n        importlib.import_module(\"metaseq.criterions.\" + file_name)\n",
        "metaseq/criterions/base_criterion.py": "# Copyright (c) Meta Platforms, Inc. and affiliates. All Rights Reserved.\n#\n# This source code is licensed under the MIT license found in the\n# LICENSE file in the root directory of this source tree.\n\nimport inspect\nfrom typing import Any, Dict, List\n\nfrom torch.nn.modules.loss import _Loss\n\nfrom metaseq.dataclass import MetaseqDataclass\nfrom metaseq.dataclass.utils import gen_parser_from_dataclass\n\n\nclass BaseCriterion(_Loss):\n    def __init__(self, task):\n        super().__init__()\n        self.task = task\n        if hasattr(task, \"target_dictionary\"):\n            tgt_dict = task.target_dictionary\n            self.padding_idx = tgt_dict.pad() if tgt_dict is not None else -100\n\n    @classmethod\n    def add_args(cls, parser):\n        \"\"\"Add criterion-specific arguments to the parser.\"\"\"\n        dc = getattr(cls, \"__dataclass\", None)\n        if dc is not None:\n            gen_parser_from_dataclass(parser, dc())\n\n    @classmethod\n    def build_criterion(cls, cfg: MetaseqDataclass, task):\n        \"\"\"Construct a criterion from command-line args.\"\"\"\n        # arguments in the __init__.\n        init_args = {}\n        for p in inspect.signature(cls).parameters.values():\n            if (\n                p.kind == p.POSITIONAL_ONLY\n                or p.kind == p.VAR_POSITIONAL\n                or p.kind == p.VAR_KEYWORD\n            ):\n                # we haven't implemented inference for these argument types,\n                # but PRs welcome :)\n                raise NotImplementedError(\"{} not supported\".format(p.kind))\n\n            assert p.kind in {p.POSITIONAL_OR_KEYWORD, p.KEYWORD_ONLY}\n\n            if p.name == \"task\":\n                init_args[\"task\"] = task\n            elif p.name == \"cfg\":\n                init_args[\"cfg\"] = cfg\n            elif hasattr(cfg, p.name):\n                init_args[p.name] = getattr(cfg, p.name)\n            elif p.default != p.empty:\n                pass  # we'll use the default value\n            else:\n                raise NotImplementedError(\n                    \"Unable to infer Criterion arguments, please implement \"\n                    \"{}.build_criterion\".format(cls.__name__)\n                )\n        return cls(**init_args)\n\n    def forward(self, model, sample, reduce=True):\n        \"\"\"Compute the loss for the given sample.\n\n        Returns a tuple with three elements:\n        1) the loss\n        2) the sample size, which is used as the denominator for the gradient\n        3) logging outputs to display while training\n        \"\"\"\n        raise NotImplementedError\n\n    @classmethod\n    def reduce_metrics(cls, logging_outputs: List[Dict[str, Any]]) -> None:\n        \"\"\"Aggregate logging outputs from data parallel training.\"\"\"\n        raise NotImplementedError\n\n    @staticmethod\n    def logging_outputs_can_be_summed() -> bool:\n        \"\"\"\n        Whether the logging outputs returned by `forward` can be summed\n        across workers prior to calling `reduce_metrics`. Setting this\n        to True will improves distributed training speed.\n        \"\"\"\n        return False\n",
        "metaseq/criterions/cross_entropy.py": "# Copyright (c) Meta Platforms, Inc. and affiliates. All Rights Reserved.\n#\n# This source code is licensed under the MIT license found in the\n# LICENSE file in the root directory of this source tree.\n\nimport math\n\nimport torch\nimport torch.nn.functional as F\n\nfrom metaseq import metrics, utils\nfrom metaseq.criterions import BaseCriterion, register_criterion\n\n\ndef nll_loss(lprobs, target, ignore_index=None, reduction=\"mean\"):\n    \"\"\"Like torch.nn.functional.nll_loss but works for large inputs.\"\"\"\n    if lprobs.numel() < 2e9:\n        return F.nll_loss(\n            lprobs, target, ignore_index=ignore_index, reduction=reduction\n        )\n    if target.dim() == lprobs.dim() - 1:\n        target = target.unsqueeze(-1)\n    nll_loss = -lprobs.gather(dim=-1, index=target)\n    if ignore_index is not None:\n        pad_mask = target.eq(ignore_index)\n        nll_loss.masked_fill_(pad_mask, 0.0)\n    else:\n        nll_loss = nll_loss.squeeze(-1)\n    if reduction == \"mean\":\n        nll_loss = nll_loss.mean()\n    elif reduction == \"sum\":\n        nll_loss = nll_loss.sum()\n    elif reduction == \"none\":\n        pass\n    else:\n        raise NotImplementedError\n    return nll_loss\n\n\n@register_criterion(\"cross_entropy\")\nclass CrossEntropyCriterion(BaseCriterion):\n    def __init__(self, task):\n        super().__init__(task)\n\n    def forward(self, model, sample, reduce=True):\n        \"\"\"Compute the loss for the given sample.\n\n        Returns a tuple with three elements:\n        1) the loss\n        2) the sample size, which is used as the denominator for the gradient\n        3) logging outputs to display while training\n        \"\"\"\n        net_output = model(**sample[\"net_input\"])\n        loss, _ = self.compute_loss(model, net_output, sample, reduce=reduce)\n        sample_size = sample[\"ntokens\"]\n        logging_output = {\n            \"loss\": loss.data,\n            \"ntokens\": sample[\"ntokens\"],\n            \"nsentences\": sample[\"target\"].size(0),\n            \"sample_size\": sample_size,\n        }\n        if \"src_tokens\" in sample[\"net_input\"] and hasattr(self.task, \"eod\"):\n            logging_output[\"ndocseps\"] = (sample[\"target\"] == self.task.eod).sum()\n        if (\n            len(net_output) >= 2\n            and isinstance(net_output[1], dict)\n            and \"inner_states\" in net_output[1]\n        ):\n            with torch.no_grad():\n                # yank out the inner states we wish to instrument\n                # see transformer_decoder.py ModelParallelTransformerDecoder.extract_features\n                emb, *_, actv = net_output[1][\"inner_states\"]\n                assert isinstance(\n                    emb, dict\n                ), \"Expecting the first inner state to be a dict of embedding representations\"\n                emb[\"actv\"] = actv  # throw on final for code brevity\n                for key, value in emb.items():\n                    if value is None:\n                        # maybe future proofing relative positional embeddings\n                        continue\n                    value = emb[key]\n                    logging_output[f\"{key}_norm\"] = value.norm(p=2, dim=-1).sum(\n                        dtype=torch.float32\n                    )\n\n        return loss, sample_size, logging_output\n\n    def compute_loss(self, model, net_output, sample, reduce=True):\n        lprobs = model.get_normalized_probs(net_output[0], log_probs=True)\n        lprobs = lprobs.view(-1, lprobs.size(-1))\n        target = model.get_targets(sample).view(-1)\n        loss = nll_loss(\n            lprobs,\n            target,\n            ignore_index=self.padding_idx,\n            reduction=\"sum\" if reduce else \"none\",\n        )\n        return loss, loss\n\n    @staticmethod\n    def reduce_metrics(logging_outputs) -> None:\n        \"\"\"Aggregate logging outputs from data parallel training.\"\"\"\n        loss_sum = sum(log.get(\"loss\", 0) for log in logging_outputs)\n        ntokens = sum(log.get(\"ntokens\", 0) for log in logging_outputs)\n        sample_size = sum(log.get(\"sample_size\", 0) for log in logging_outputs)\n\n        for type_ in (\"actv\", \"pos\", \"tok\", \"emb\"):\n            key = f\"{type_}_norm\"\n            if any(key in log for log in logging_outputs):\n                actv_norm = sum(log.get(key, 0) for log in logging_outputs)\n                metrics.log_scalar(key, actv_norm / ntokens, round=3)\n\n        # we divide by log(2) to convert the loss from base e to base 2\n        metrics.log_scalar(\n            \"loss\", loss_sum / sample_size / math.log(2), sample_size, round=3\n        )\n        if sample_size != ntokens:\n            metrics.log_scalar(\n                \"nll_loss\", loss_sum / ntokens / math.log(2), ntokens, round=3\n            )\n            metrics.log_derived(\n                \"ppl\", lambda meters: utils.get_perplexity(meters[\"nll_loss\"].avg)\n            )\n        else:\n            metrics.log_derived(\n                \"ppl\", lambda meters: utils.get_perplexity(meters[\"loss\"].avg)\n            )\n\n    @staticmethod\n    def logging_outputs_can_be_summed() -> bool:\n        \"\"\"\n        Whether the logging outputs returned by `forward` can be summed\n        across workers prior to calling `reduce_metrics`. Setting this\n        to True will improve distributed training speed.\n        \"\"\"\n        return True\n",
        "metaseq/criterions/vocab_parallel_cross_entropy.py": "# Copyright (c) Meta Platforms, Inc. and affiliates. All Rights Reserved.\n#\n# This source code is licensed under the MIT license found in the\n# LICENSE file in the root directory of this source tree.\n\nimport math\n\nimport torch\n\nfrom metaseq import metrics, utils\nfrom metaseq.criterions import BaseCriterion, register_criterion\nfrom metaseq.modules.megatron.mpu import vocab_parallel_cross_entropy\n\n\n@register_criterion(\"vocab_parallel_cross_entropy\")\nclass VocabParallelCrossEntropyCriterion(BaseCriterion):\n    def __init__(self, task):\n        super().__init__(task)\n\n    def forward(self, model, sample, reduce=True):\n        \"\"\"Compute the loss for the given sample.\n\n        Returns a tuple with three elements:\n        1) the loss\n        2) the sample size, which is used as the denominator for the gradient\n        3) logging outputs to display while training\n        \"\"\"\n        target = sample[\"target\"]\n        has_pad = target.eq(self.padding_idx).any().item()\n\n        net_output = model(**sample[\"net_input\"])\n        loss = vocab_parallel_cross_entropy(net_output[0].float(), target)\n        if has_pad:\n            loss = loss * (target != self.padding_idx)\n        loss = loss.sum()\n        # When using target loss only, use num tokens in target only as the sample_size\n        # See StreamingSrcTgtDataset\n        sample_size = (\n            sample[\"ntokens_target\"]\n            if \"ntokens_target\" in sample\n            else sample[\"ntokens\"]\n        )\n        logging_output = {\n            \"loss\": loss.data,\n            \"ntokens\": sample[\"ntokens\"],\n            \"nsentences\": sample[\"target\"].size(0),\n            \"sample_size\": sample_size,\n        }\n        if \"src_tokens\" in sample[\"net_input\"] and hasattr(self.task, \"eod\"):\n            logging_output[\"ndocseps\"] = (sample[\"target\"] == self.task.eod).sum()\n        if (\n            len(net_output) >= 2\n            and isinstance(net_output[1], dict)\n            and \"inner_states\" in net_output[1]\n        ):\n            with torch.no_grad():\n                # yank out the inner states we wish to instrument\n                # see transformer_decoder.py ModelParallelTransformerDecoder.extract_features\n                emb, *_, actv = net_output[1][\"inner_states\"]\n                assert isinstance(\n                    emb, dict\n                ), \"Expecting the first inner state to be a dict of embedding representations\"\n                emb[\"actv\"] = actv  # throw on final for code brevity\n                for key, value in emb.items():\n                    if value is None:\n                        # maybe future proofing relative positional embeddings\n                        continue\n                    value = emb[key]\n                    logging_output[f\"{key}_norm\"] = value.norm(p=2, dim=-1).sum(\n                        dtype=torch.float32\n                    )\n\n        return loss, sample_size, logging_output\n\n    @staticmethod\n    def reduce_metrics(logging_outputs) -> None:\n        \"\"\"Aggregate logging outputs from data parallel training.\"\"\"\n        loss_sum = sum(log.get(\"loss\", 0) for log in logging_outputs)\n        ntokens = sum(log.get(\"ntokens\", 0) for log in logging_outputs)\n        sample_size = sum(log.get(\"sample_size\", 0) for log in logging_outputs)\n\n        for type_ in (\"actv\", \"pos\", \"tok\", \"emb\"):\n            key = f\"{type_}_norm\"\n            if any(key in log for log in logging_outputs):\n                actv_norm = sum(log.get(key, 0) for log in logging_outputs)\n                metrics.log_scalar(key, actv_norm / ntokens, round=3)\n\n        if any(\"ndocseps\" in log for log in logging_outputs):\n            # nsentences = batch size\n            nsentences = sum(log.get(\"nsentences\", 0) for log in logging_outputs)\n            # ndocseps = number of document separators we found\n            ndocseps = sum(log.get(\"ndocseps\", 0) for log in logging_outputs)\n            # so docs/example = (1 + ndocseps) / example = (ndocseps + nsents) / nsents\n            metrics.log_scalar(\"docsperex\", (ndocseps + nsentences) / nsentences)\n\n        metrics.log_scalar(\n            \"loss\", loss_sum / sample_size / math.log(2), sample_size, round=3\n        )\n        metrics.log_derived(\n            \"ppl\", lambda meters: utils.get_perplexity(meters[\"loss\"].avg)\n        )\n\n    @staticmethod\n    def logging_outputs_can_be_summed() -> bool:\n        \"\"\"\n        Whether the logging outputs returned by `forward` can be summed\n        across workers prior to calling `reduce_metrics`. Setting this\n        to True will improve distributed training speed.\n        \"\"\"\n        return True\n",
        "metaseq/data/__init__.py": "# Copyright (c) Meta Platforms, Inc. and affiliates. All Rights Reserved.\n#\n# This source code is licensed under the MIT license found in the\n# LICENSE file in the root directory of this source tree.\n\"\"\"isort:skip_file\"\"\"\n\nfrom .dictionary import Dictionary, TruncatedDictionary\n\nfrom .base_dataset import BaseDataset\nfrom .base_wrapper_dataset import BaseWrapperDataset\nfrom .append_token_dataset import AppendTokenDataset\nfrom .cm3_dataset import CausalMaskedDocumentToSequenceDataset\nfrom .concat_dataset import ConcatDataset\nfrom .id_dataset import IdDataset\nfrom .indexed_dataset import (\n    IndexedCachedDataset,\n    IndexedDataset,\n    IndexedRawTextDataset,\n    MMapIndexedDataset,\n)\nfrom .jsonl_dataset import JsonlDataset\nfrom .list_dataset import ListDataset\nfrom .lm_context_window_dataset import LMContextWindowDataset\nfrom .monolingual_dataset import MonolingualDataset\nfrom .nested_dictionary_dataset import NestedDictionaryDataset\nfrom .numel_dataset import NumelDataset\nfrom .pad_dataset import LeftPadDataset, PadDataset, RightPadDataset\nfrom .partitioned_streaming_dataset import PartitionedStreamingDataset\nfrom .prepend_token_dataset import PrependTokenDataset\nfrom .resampling_dataset import ResamplingDataset\nfrom .sort_dataset import SortDataset\nfrom .streaming_shuffle_dataset import StreamingShuffleDataset\nfrom .streaming_token_block_dataset import StreamingTokenBlockDataset\nfrom .streaming_src_tgt_dataset import StreamingSrcTgtDataset\nfrom .strip_token_dataset import StripTokenDataset\nfrom .token_block_dataset import TokenBlockDataset\nfrom .pad_dataset import MultiplePadDataset\nfrom .shorten_dataset import TruncateDataset\n\nfrom .iterators import (\n    CountingIterator,\n    EpochBatchIterator,\n    GroupedIterator,\n    ShardedIterator,\n)\n\n__all__ = [\n    \"AppendTokenDataset\",\n    \"BaseWrapperDataset\",\n    \"CausalMaskedDocumentToSequenceDataset\",\n    \"ConcatDataset\",\n    \"CountingIterator\",\n    \"Dictionary\",\n    \"EpochBatchIterator\",\n    \"BaseDataset\",\n    \"GroupedIterator\",\n    \"IdDataset\",\n    \"IndexedCachedDataset\",\n    \"IndexedDataset\",\n    \"IndexedRawTextDataset\",\n    \"JsonlDataset\",\n    \"LeftPadDataset\",\n    \"ListDataset\",\n    \"LMContextWindowDataset\",\n    \"MMapIndexedDataset\",\n    \"MonolingualDataset\",\n    \"MultiplePadDataset\",\n    \"NestedDictionaryDataset\",\n    \"NumelDataset\",\n    \"PadDataset\",\n    \"PartitionedStreamingDataset\",\n    \"PrependTokenDataset\",\n    \"ResamplingDataset\",\n    \"RightPadDataset\",\n    \"ShardedIterator\",\n    \"SortDataset\",\n    \"StreamingShuffleDataset\",\n    \"StreamingTokenBlockDataset\",\n    \"StreamingSrcTgtDataset\",\n    \"StripTokenDataset\",\n    \"TokenBlockDataset\",\n    \"TruncateDataset\",\n    \"TruncatedDictionary\",\n]\n",
        "metaseq/data/append_token_dataset.py": "# Copyright (c) Meta Platforms, Inc. and affiliates. All Rights Reserved.\n#\n# This source code is licensed under the MIT license found in the\n# LICENSE file in the root directory of this source tree.\n\nimport numpy as np\nimport torch\n\nfrom . import BaseWrapperDataset\n\n\nclass AppendTokenDataset(BaseWrapperDataset):\n    def __init__(self, dataset, token=None):\n        super().__init__(dataset)\n        self.token = token\n        if token is not None:\n            self._sizes = np.array(dataset.sizes) + 1\n        else:\n            self._sizes = dataset.sizes\n\n    def __getitem__(self, idx):\n        item = self.dataset[idx]\n        if self.token is not None:\n            item = torch.cat([item, item.new([self.token])])\n        return item\n\n    @property\n    def sizes(self):\n        return self._sizes\n\n    def num_tokens(self, index):\n        n = self.dataset.num_tokens(index)\n        if self.token is not None:\n            n += 1\n        return n\n\n    def size(self, index):\n        n = self.dataset.size(index)\n        if self.token is not None:\n            n += 1\n        return n\n",
        "metaseq/data/base_dataset.py": "# Copyright (c) Meta Platforms, Inc. and affiliates. All Rights Reserved.\n#\n# This source code is licensed under the MIT license found in the\n# LICENSE file in the root directory of this source tree.\n\nimport logging\n\nimport numpy as np\nimport torch.utils.data\n\nfrom metaseq.data import data_utils\n\nlogger = logging.getLogger(__name__)\n\n\nclass EpochListening:\n    \"\"\"Mixin for receiving updates whenever the epoch increments.\"\"\"\n\n    @property\n    def can_reuse_epoch_itr_across_epochs(self):\n        \"\"\"\n        Whether we can reuse the :class:`fairseq.data.EpochBatchIterator` for\n        this dataset across epochs.\n\n        This needs to return ``False`` if the sample sizes can change across\n        epochs, in which case we may need to regenerate batches at each epoch.\n        If your dataset relies in ``set_epoch`` then you should consider setting\n        this to ``False``.\n        \"\"\"\n        return True\n\n    def set_epoch(self, epoch):\n        \"\"\"Will receive the updated epoch number at the beginning of the epoch.\"\"\"\n        pass\n\n\nclass BaseDataset(torch.utils.data.Dataset, EpochListening):\n    \"\"\"A dataset that provides helpers for batching.\"\"\"\n\n    def __getitem__(self, index):\n        raise NotImplementedError\n\n    def __len__(self):\n        raise NotImplementedError\n\n    def collater(self, samples):\n        \"\"\"Merge a list of samples to form a mini-batch.\n\n        Args:\n            samples (List[dict]): samples to collate\n\n        Returns:\n            dict: a mini-batch suitable for forwarding with a Model\n        \"\"\"\n        raise NotImplementedError\n\n    def num_tokens(self, index):\n        \"\"\"Return the number of tokens in a sample. This value is used to\n        enforce ``--max-tokens`` during batching.\"\"\"\n        raise NotImplementedError\n\n    def num_tokens_vec(self, indices):\n        \"\"\"Return the number of tokens for a set of positions defined by indices.\n        This value is used to enforce ``--max-tokens`` during batching.\"\"\"\n        raise NotImplementedError\n\n    def size(self, index):\n        \"\"\"Return an example's size as a float or tuple. This value is used when\n        filtering a dataset with ``--max-positions``.\"\"\"\n        raise NotImplementedError\n\n    def ordered_indices(self):\n        \"\"\"Return an ordered list of indices. Batches will be constructed based\n        on this order.\"\"\"\n        return np.arange(len(self), dtype=np.int64)\n\n    @property\n    def supports_prefetch(self):\n        \"\"\"Whether this dataset supports prefetching.\"\"\"\n        return False\n\n    def attr(self, attr: str, index: int):\n        return getattr(self, attr, None)\n\n    def prefetch(self, indices):\n        \"\"\"Prefetch the data required for this epoch.\"\"\"\n        raise NotImplementedError\n\n    def get_batch_shapes(self):\n        \"\"\"\n        Return a list of valid batch shapes, for example::\n\n            [(8, 512), (16, 256), (32, 128)]\n\n        The first dimension of each tuple is the batch size and can be ``None``\n        to automatically infer the max batch size based on ``--max-tokens``.\n        The second dimension of each tuple is the max supported length as given\n        by :func:`metaseq.data.BaseDataset.num_tokens`.\n\n        This will be used by :func:`metaseq.data.BaseDataset.batch_by_size`\n        to restrict batch shapes. This is useful on TPUs to avoid too many\n        dynamic shapes (and recompilations).\n        \"\"\"\n        return None\n\n    def batch_by_size(\n        self,\n        indices,\n        max_tokens=None,\n        max_sentences=None,\n        required_batch_size_multiple=1,\n    ):\n        \"\"\"\n        Given an ordered set of indices, return batches according to\n        *max_tokens*, *max_sentences* and *required_batch_size_multiple*.\n        \"\"\"\n        from metaseq.data import data_utils\n\n        fixed_shapes = self.get_batch_shapes()\n        if fixed_shapes is not None:\n\n            def adjust_bsz(bsz, num_tokens):\n                if bsz is None:\n                    assert max_tokens is not None, \"Must specify --max-tokens\"\n                    bsz = max_tokens // num_tokens\n                if max_sentences is not None:\n                    bsz = min(bsz, max_sentences)\n                elif (\n                    bsz >= required_batch_size_multiple\n                    and bsz % required_batch_size_multiple != 0\n                ):\n                    bsz -= bsz % required_batch_size_multiple\n                return bsz\n\n            fixed_shapes = np.array(\n                [\n                    [adjust_bsz(bsz, num_tokens), num_tokens]\n                    for (bsz, num_tokens) in fixed_shapes\n                ]\n            )\n\n        try:\n            num_tokens_vec = self.num_tokens_vec(indices).astype(\"int64\")\n        except NotImplementedError:\n            num_tokens_vec = None\n\n        return data_utils.batch_by_size(\n            indices,\n            num_tokens_fn=self.num_tokens,\n            num_tokens_vec=num_tokens_vec,\n            max_tokens=max_tokens,\n            max_sentences=max_sentences,\n            required_batch_size_multiple=required_batch_size_multiple,\n            fixed_shapes=fixed_shapes,\n        )\n\n    def filter_indices_by_size(self, indices, max_sizes):\n        \"\"\"\n        Filter a list of sample indices. Remove those that are longer than\n        specified in *max_sizes*.\n\n        WARNING: don't update, override method in child classes\n\n        Args:\n            indices (np.array): original array of sample indices\n            max_sizes (int or list[int] or tuple[int]): max sample size,\n                can be defined separately for src and tgt (then list or tuple)\n\n        Returns:\n            np.array: filtered sample array\n            list: list of removed indices\n        \"\"\"\n        if isinstance(max_sizes, float) or isinstance(max_sizes, int):\n            if hasattr(self, \"sizes\") and isinstance(self.sizes, np.ndarray):\n                ignored = indices[self.sizes[indices] > max_sizes].tolist()\n                indices = indices[self.sizes[indices] <= max_sizes]\n            elif (\n                hasattr(self, \"sizes\")\n                and isinstance(self.sizes, list)\n                and len(self.sizes) == 1\n            ):\n                ignored = indices[self.sizes[0][indices] > max_sizes].tolist()\n                indices = indices[self.sizes[0][indices] <= max_sizes]\n            else:\n                indices, ignored = data_utils._filter_by_size_dynamic(\n                    indices, self.size, max_sizes\n                )\n        else:\n            indices, ignored = data_utils._filter_by_size_dynamic(\n                indices, self.size, max_sizes\n            )\n        return indices, ignored\n\n    def ordered_indices_per_dataset(self):\n        \"\"\"Return a list of ordered indices vectors for each underlying dataset\n        (with parent dataset indices).\"\"\"\n        return [self.ordered_indices()]\n\n    @property\n    def supports_fetch_outside_dataloader(self):\n        \"\"\"Whether this dataset supports fetching outside the workers of the dataloader.\"\"\"\n        return True\n",
        "metaseq/data/base_wrapper_dataset.py": "# Copyright (c) Meta Platforms, Inc. and affiliates. All Rights Reserved.\n#\n# This source code is licensed under the MIT license found in the\n# LICENSE file in the root directory of this source tree.\n\nfrom torch.utils.data.dataloader import default_collate\n\nfrom . import BaseDataset\n\n\nclass BaseWrapperDataset(BaseDataset):\n    def __init__(self, dataset):\n        super().__init__()\n        self.dataset = dataset\n\n    def __getitem__(self, index):\n        return self.dataset[index]\n\n    def __len__(self):\n        return len(self.dataset)\n\n    def collater(self, samples):\n        if hasattr(self.dataset, \"collater\"):\n            return self.dataset.collater(samples)\n        else:\n            return default_collate(samples)\n\n    @property\n    def sizes(self):\n        return self.dataset.sizes\n\n    def num_tokens(self, index):\n        return self.dataset.num_tokens(index)\n\n    def size(self, index):\n        return self.dataset.size(index)\n\n    def ordered_indices(self):\n        return self.dataset.ordered_indices()\n\n    @property\n    def supports_prefetch(self):\n        return getattr(self.dataset, \"supports_prefetch\", False)\n\n    def attr(self, attr: str, index: int):\n        return self.dataset.attr(attr, index)\n\n    def prefetch(self, indices):\n        self.dataset.prefetch(indices)\n\n    def get_batch_shapes(self):\n        return self.dataset.get_batch_shapes()\n\n    def batch_by_size(\n        self,\n        indices,\n        max_tokens=None,\n        max_sentences=None,\n        required_batch_size_multiple=1,\n    ):\n        return self.dataset.batch_by_size(\n            indices,\n            max_tokens=max_tokens,\n            max_sentences=max_sentences,\n            required_batch_size_multiple=required_batch_size_multiple,\n        )\n\n    def filter_indices_by_size(self, indices, max_sizes):\n        return self.dataset.filter_indices_by_size(indices, max_sizes)\n\n    def set_epoch(self, epoch):\n        super().set_epoch(epoch)\n        if hasattr(self.dataset, \"set_epoch\"):\n            self.dataset.set_epoch(epoch)\n",
        "metaseq/data/cm3_dataset.py": "# Copyright (c) Meta Platforms, Inc. and affiliates. All Rights Reserved.\n#\n# This source code is licensed under the MIT license found in the\n# LICENSE file in the root directory of this source tree.\n\nimport numpy as np\nimport torch\n\nfrom typing import List, Optional, Tuple\nfrom .document_to_sequence import DocumentToSequenceDataset\n\n\ndef span_intersection(left: Tuple[int, int], right: Tuple[int, int]) -> bool:\n    left_x, left_y = left\n    right_x, right_y = right\n    return max(left_x, right_x) < min(left_y, right_y)\n\n\ndef overlaps(span_1: Tuple[int, int], span_2: Tuple[int, int]) -> bool:\n    # Check if two spans overlap\n    return not (span_1[1] <= span_2[0] or span_1[0] >= span_2[1])\n\n\ndef calculate_overlap(span_1: Tuple[int, int], span_2: Tuple[int, int]) -> int:\n    # Calculate the overlap between two spans\n    return max(0, min(span_1[1], span_2[1]) - max(span_1[0], span_2[0]))\n\n\ndef adjust_span(\n    span_1: Tuple[int, int], most_similar_span: Tuple[int, int]\n) -> Tuple[int, int]:\n    # Adjust the boundaries of span_1 so that it does not cross the boundaries of most_similar_span\n    new_start = max(span_1[0], most_similar_span[0])\n    new_end = min(span_1[1], most_similar_span[1])\n    return (new_start, new_end)\n\n\ndef adjust_spans(\n    spans_1: List[Tuple[int, int]], spans_constraints: List[Tuple[int, int]]\n) -> List[Tuple[int, int]]:\n    new_spans_1 = []\n    for span_1 in spans_1:\n        most_similar_span = None\n        max_overlap = 0\n        # Find the most similar span in spans_2\n        for span_2 in spans_constraints:\n            overlap = calculate_overlap(span_1, span_2)\n            if overlap > max_overlap:\n                max_overlap = overlap\n                most_similar_span = span_2\n        # Adjust the span if it overlaps with the most similar span\n        if most_similar_span is not None and overlaps(span_1, most_similar_span):\n            span_1 = adjust_span(span_1, most_similar_span)\n        new_spans_1.append(span_1)\n    # Return the updated spans in spans_1\n    return new_spans_1\n\n\nclass CausalMaskedDocumentToSequenceDataset(DocumentToSequenceDataset):\n    def __init__(\n        self,\n        sentinel_token_expectation: int,\n        sentinel_tokens: List[int],\n        sentinel_method: str,\n        sentinel_eos: int,\n        allow_rotation_across_eod: bool,\n        eod: int,\n        dataset: torch.utils.data.IterableDataset,\n        block_size: int,\n        break_mode: str = \"none\",\n        drop_last: Optional[bool] = True,\n        padding_idx: Optional[int] = None,\n        shuffle_buffer_size: int = 1,\n        seed: Optional[int] = None,\n        len_cache=None,\n        to_skip=0,\n        permute_documents=True,\n        source_target=False,\n    ):\n        super().__init__(\n            dataset,\n            block_size,\n            break_mode,\n            drop_last,\n            padding_idx,\n            shuffle_buffer_size,\n            seed,\n            len_cache,\n            to_skip,\n            permute_documents,\n            source_target,\n        )\n        self.sentinel_token_expectation = sentinel_token_expectation\n        self.sentinel_tokens = sentinel_tokens\n        self.sentinel_method = sentinel_method\n        self.tokens_per_sample = block_size\n        self.eos = sentinel_eos\n        assert (\n            self.sentinel_method == \"fixed\" or self.sentinel_method == \"poisson\"\n        ), self.sentinel_method\n        assert len(self.sentinel_tokens) >= 1\n        assert self.tokens_per_sample > 1\n        assert (\n            not self.source_target\n        ), \"CM3 objective is not supported when source_target is True\"\n        self.sentinel_fixed = self.sentinel_method == \"fixed\"\n        self.allow_rotation_across_eod = allow_rotation_across_eod\n        self.eod = eod\n\n    def get_sentinel(self, i):\n        return self.sentinel_tokens[i]\n\n    def sentinel_masking(self, document: torch.Tensor, spans: List[Tuple[int, int]]):\n        document_clone = document.clone()\n        document_retrieve_mask = torch.ones_like(document_clone).to(torch.bool)\n\n        for i, span in enumerate(spans):\n            document_clone[span[0]] = self.get_sentinel(i)\n            document_retrieve_mask[span[0] + 1 : span[1]] = False\n\n        return document_clone[document_retrieve_mask]\n\n    def sentinel_targets(self, document: torch.Tensor, spans: List[Tuple[int, int]]):\n        num_focused_tokens = sum(x[1] - x[0] for x in spans)\n        num_spans = len(spans)\n        target = torch.zeros(num_focused_tokens + 2 * num_spans).to(document)\n        index = 0\n        if self.sentinel_fixed:\n            assert len(self.sentinel_tokens) == len(spans)\n        else:\n            assert len(self.sentinel_tokens) > len(spans)\n\n        for i, span in enumerate(spans):\n            target[index] = self.get_sentinel(i)\n            index += 1\n            size = span[1] - span[0]\n            target[index : index + size] = document[span[0] : span[1]]\n            target[index + size] = self.eos\n            index = index + size + 1\n        return target\n\n    def get_spans_to_mask(self, document_length: int) -> List[Tuple[int, int]]:\n        # Ok, we do not use a budget here but instead\n        # our goal is to sample from ~ U[0,1] in the case of len(sentinel_tokens) = 1\n        # If len(sentinel_tokens) > 1 we try to find len(sentinel_tokens) non intersecting spans\n        len_sentinel_tokens = None\n        if self.sentinel_fixed:\n            len_sentinel_tokens = self.sentinel_token_expectation\n        else:\n            len_sentinel_tokens = (\n                torch.poisson(torch.tensor([float(self.sentinel_token_expectation)]))\n                .clamp(0, len(self.sentinel_tokens) - 1)\n                .to(torch.int)\n                .item()\n            )\n        if len_sentinel_tokens == 0:\n            return None\n        if len_sentinel_tokens == 1:\n            start, end = np.random.uniform(size=2)\n            if end < start:\n                start, end = end, start\n            # round down\n            start = int(start * document_length)\n            # round up\n            end = int(end * document_length + 0.5)\n            if start == end:\n                return None\n            else:\n                assert start < end\n                return [(start, end)]\n\n        # Let's implement the general case. We will create len(self.sentinel_tokens) ** 2 possible candidates\n        # And we will filter one by one to insure no intersections. If we can't find anything then so be it.\n        return_spans: List[Tuple[int, int]] = []\n        candidate_spans: List[Tuple[int, int]] = [\n            tuple(np.random.uniform(size=2)) for _ in range(len_sentinel_tokens**2)\n        ]\n        candidate_spans = [\n            (int(start * document_length), int(end * document_length + 0.5))\n            for (start, end) in candidate_spans\n        ]\n        candidate_spans = [\n            (start, end) if start <= end else (end, start)\n            for (start, end) in candidate_spans\n        ]\n        while len(return_spans) < len_sentinel_tokens and len(candidate_spans) > 0:\n            candidate_span = candidate_spans.pop()\n            if not any(span_intersection(x, candidate_span) for x in return_spans):\n                return_spans.append(candidate_span)\n        return return_spans\n\n    def get_ordered_spans(self, spans: List[Tuple[int, int]]) -> List[Tuple[int, int]]:\n        return sorted(spans, key=lambda x: x[0])\n\n    def get_document_boundaries(self, item: torch.Tensor):\n        boundaries = (item == self.eod).nonzero().cpu().squeeze().numpy().tolist()\n        if boundaries[0] != 0:\n            boundaries = [0] + boundaries\n        if boundaries[-1] != item.size(0) - 1:\n            boundaries = boundaries + [item.size(0)]\n        spans = []\n        for i in range(1, len(boundaries)):\n            spans.append((boundaries[i - 1], boundaries[i]))\n        return spans\n\n    def __iter__(self):\n        for packed_item in super().__iter__():\n            item = packed_item[\"block\"]\n            assert len(item) > 0\n            spans = self.get_spans_to_mask(len(item))\n            if not self.allow_rotation_across_eod:\n                document_boundaries = self.get_document_boundaries(item)\n                spans = adjust_spans(spans, document_boundaries)\n            if spans is None:\n                yield packed_item\n            else:\n                spans = self.get_ordered_spans(spans)\n                causal_source = self.sentinel_masking(item, spans)\n                causal_masked = self.sentinel_targets(item, spans)\n                packed_item[\"block\"] = torch.cat([causal_source, causal_masked])[\n                    : self.tokens_per_sample\n                ]  # EOSS tokens can add just enough tokens to get off by 1-2.\n                yield packed_item\n",
        "metaseq/data/concat_dataset.py": "# Copyright (c) Meta Platforms, Inc. and affiliates. All Rights Reserved.\n#\n# This source code is licensed under the MIT license found in the\n# LICENSE file in the root directory of this source tree.\n\nimport bisect\n\nimport numpy as np\nfrom torch.utils.data.dataloader import default_collate\n\nfrom . import BaseDataset\n\n\nclass ConcatDataset(BaseDataset):\n    @staticmethod\n    def cumsum(sequence, sample_ratios):\n        r, s = [], 0\n        for e, ratio in zip(sequence, sample_ratios):\n            curr_len = int(ratio * len(e))\n            r.append(curr_len + s)\n            s += curr_len\n        return r\n\n    def __init__(self, datasets, sample_ratios=1):\n        super(ConcatDataset, self).__init__()\n        assert len(datasets) > 0, \"datasets should not be an empty iterable\"\n        self.datasets = list(datasets)\n        if isinstance(sample_ratios, int):\n            sample_ratios = [sample_ratios] * len(self.datasets)\n        self.sample_ratios = sample_ratios\n        self.cumulative_sizes = self.cumsum(self.datasets, sample_ratios)\n        self.real_sizes = [len(d) for d in self.datasets]\n\n    def __len__(self):\n        return self.cumulative_sizes[-1]\n\n    def __getitem__(self, idx):\n        dataset_idx, sample_idx = self._get_dataset_and_sample_index(idx)\n        return self.datasets[dataset_idx][sample_idx]\n\n    def _get_dataset_and_sample_index(self, idx: int):\n        dataset_idx = bisect.bisect_right(self.cumulative_sizes, idx)\n        if dataset_idx == 0:\n            sample_idx = idx\n        else:\n            sample_idx = idx - self.cumulative_sizes[dataset_idx - 1]\n        sample_idx = sample_idx % self.real_sizes[dataset_idx]\n        return dataset_idx, sample_idx\n\n    def collater(self, samples, **extra_args):\n        # For now only supports datasets with same underlying collater implementations\n        if hasattr(self.datasets[0], \"collater\"):\n            return self.datasets[0].collater(samples, **extra_args)\n        else:\n            return default_collate(samples, **extra_args)\n\n    def size(self, idx: int):\n        \"\"\"\n        Return an example's size as a float or tuple.\n        \"\"\"\n        dataset_idx, sample_idx = self._get_dataset_and_sample_index(idx)\n        return self.datasets[dataset_idx].size(sample_idx)\n\n    def num_tokens(self, index: int):\n        return np.max(self.size(index))\n\n    def attr(self, attr: str, index: int):\n        dataset_idx = bisect.bisect_right(self.cumulative_sizes, index)\n        return getattr(self.datasets[dataset_idx], attr, None)\n\n    @property\n    def sizes(self):\n        _dataset_sizes = []\n        for ds, sr in zip(self.datasets, self.sample_ratios):\n            if isinstance(ds.sizes, np.ndarray):\n                _dataset_sizes.append(np.tile(ds.sizes, sr))\n            else:\n                # Only support underlying dataset with single size array.\n                assert isinstance(ds.sizes, list)\n                _dataset_sizes.append(np.tile(ds.sizes[0], sr))\n        return np.concatenate(_dataset_sizes)\n\n    @property\n    def supports_prefetch(self):\n        return all(d.supports_prefetch for d in self.datasets)\n\n    def ordered_indices(self):\n        \"\"\"\n        Returns indices sorted by length. So less padding is needed.\n        \"\"\"\n        if isinstance(self.sizes, np.ndarray) and len(self.sizes.shape) > 1:\n            # special handling for concatenating lang_pair_datasets\n            indices = np.arange(len(self))\n            sizes = self.sizes\n            tgt_sizes = (\n                sizes[:, 1] if len(sizes.shape) > 0 and sizes.shape[1] > 1 else None\n            )\n            src_sizes = (\n                sizes[:, 0] if len(sizes.shape) > 0 and sizes.shape[1] > 1 else sizes\n            )\n            # sort by target length, then source length\n            if tgt_sizes is not None:\n                indices = indices[np.argsort(tgt_sizes[indices], kind=\"mergesort\")]\n            return indices[np.argsort(src_sizes[indices], kind=\"mergesort\")]\n        else:\n            return np.argsort(self.sizes)\n\n    def prefetch(self, indices):\n        frm = 0\n        for to, ds in zip(self.cumulative_sizes, self.datasets):\n            real_size = len(ds)\n            if getattr(ds, \"supports_prefetch\", False):\n                ds.prefetch([(i - frm) % real_size for i in indices if frm <= i < to])\n            frm = to\n\n    def set_epoch(self, epoch):\n        super().set_epoch(epoch)\n        for ds in self.datasets:\n            if hasattr(ds, \"set_epoch\"):\n                ds.set_epoch(epoch)\n\n    def ordered_indices_per_dataset(self):\n        \"\"\"Return a list of ordered indices vectors for each underlying dataset\n        (with parent dataset indices).\"\"\"\n        ordered_indices_list = []\n        for i, dataset in enumerate(self.datasets):\n            start = 0 if i == 0 else self.cumulative_sizes[i - 1]\n            subdataset_indices_list = dataset.ordered_indices_per_dataset()\n            for indices in subdataset_indices_list:\n                ordered_indices_list.append(indices + start)\n\n        return ordered_indices_list\n",
        "metaseq/data/data_utils.py": "# Copyright (c) Meta Platforms, Inc. and affiliates. All Rights Reserved.\n#\n# This source code is licensed under the MIT license found in the\n# LICENSE file in the root directory of this source tree.\n\ntry:\n    from collections.abc import Iterable\nexcept ImportError:\n    from collections import Iterable\nimport contextlib\nimport itertools\nimport logging\nimport os\nimport re\n\nimport numpy as np\n\nfrom metaseq import utils\nfrom metaseq.file_io import PathManager\n\nlogger = logging.getLogger(__name__)\n\n\ndef collate_tokens(\n    values,\n    pad_idx,\n    eos_idx=None,\n    left_pad=False,\n    move_eos_to_beginning=False,\n    pad_to_length=None,\n    pad_to_multiple=1,\n    pad_to_bsz=None,\n):\n    \"\"\"Convert a list of 1d tensors into a padded 2d tensor.\"\"\"\n    size = max(v.size(0) for v in values)\n    size = size if pad_to_length is None else max(size, pad_to_length)\n    if pad_to_multiple != 1 and size % pad_to_multiple != 0:\n        size = int(((size - 0.1) // pad_to_multiple + 1) * pad_to_multiple)\n\n    batch_size = len(values) if pad_to_bsz is None else max(len(values), pad_to_bsz)\n    res = values[0].new(batch_size, size).fill_(pad_idx)\n\n    def copy_tensor(src, dst):\n        assert dst.numel() == src.numel()\n        if move_eos_to_beginning:\n            if eos_idx is None:\n                # if no eos_idx is specified, then use the last token in src\n                dst[0] = src[-1]\n            else:\n                dst[0] = eos_idx\n            dst[1:] = src[:-1]\n        else:\n            dst.copy_(src)\n\n    for i, v in enumerate(values):\n        copy_tensor(v, res[i][size - len(v) :] if left_pad else res[i][: len(v)])\n    return res\n\n\ndef load_indexed_dataset(\n    path, dictionary=None, dataset_impl=None, combine=False, default=\"cached\"\n):\n    \"\"\"A helper function for loading indexed datasets.\n\n    Args:\n        path (str): path to indexed dataset (e.g., 'data-bin/train')\n        dictionary (~metaseq.data.Dictionary): data dictionary\n        dataset_impl (str, optional): which dataset implementation to use. If\n            not provided, it will be inferred automatically. For legacy indexed\n            data we use the 'cached' implementation by default.\n        combine (bool, optional): automatically load and combine multiple\n            datasets. For example, if *path* is 'data-bin/train', then we will\n            combine 'data-bin/train', 'data-bin/train1', ... and return a\n            single ConcatDataset instance.\n    \"\"\"\n    import metaseq.data.indexed_dataset as indexed_dataset\n    from metaseq.data.concat_dataset import ConcatDataset\n\n    datasets = []\n    for k in itertools.count():\n        path_k = path + (str(k) if k > 0 else \"\")\n        path_k = indexed_dataset.get_indexed_dataset_to_local(path_k)\n\n        dataset_impl_k = dataset_impl\n        if dataset_impl_k is None:\n            dataset_impl_k = indexed_dataset.infer_dataset_impl(path_k)\n        dataset = indexed_dataset.make_dataset(\n            path_k,\n            impl=dataset_impl_k or default,\n            fix_lua_indexing=True,\n            dictionary=dictionary,\n        )\n        if dataset is None:\n            break\n        logger.info(\"loaded {:,} examples from: {}\".format(len(dataset), path_k))\n        datasets.append(dataset)\n        if not combine:\n            break\n    if len(datasets) == 0:\n        return None\n    elif len(datasets) == 1:\n        return datasets[0]\n    else:\n        return ConcatDataset(datasets)\n\n\n@contextlib.contextmanager\ndef numpy_seed(seed, *addl_seeds):\n    \"\"\"Context manager which seeds the NumPy PRNG with the specified seed and\n    restores the state afterward\"\"\"\n    if seed is None:\n        yield\n        return\n    if len(addl_seeds) > 0:\n        seed = int(hash((seed, *addl_seeds)) % 1e6)\n    state = np.random.get_state()\n    np.random.seed(seed)\n    try:\n        yield\n    finally:\n        np.random.set_state(state)\n\n\ndef collect_filtered(function, iterable, filtered):\n    \"\"\"\n    Similar to :func:`filter` but collects filtered elements in ``filtered``.\n\n    Args:\n        function (callable): function that returns ``False`` for elements that\n            should be filtered\n        iterable (iterable): iterable to filter\n        filtered (list): list to store filtered elements\n    \"\"\"\n    for el in iterable:\n        if function(el):\n            yield el\n        else:\n            filtered.append(el)\n\n\ndef _filter_by_size_dynamic(indices, size_fn, max_positions, raise_exception=False):\n    def compare_leq(a, b):\n        return a <= b if not isinstance(a, tuple) else max(a) <= b\n\n    def check_size(idx):\n        if isinstance(max_positions, float) or isinstance(max_positions, int):\n            return size_fn(idx) <= max_positions\n        elif isinstance(max_positions, dict):\n            idx_size = size_fn(idx)\n            assert isinstance(idx_size, dict)\n            intersect_keys = set(max_positions.keys()) & set(idx_size.keys())\n            return all(\n                all(\n                    a is None or b is None or a <= b\n                    for a, b in zip(idx_size[key], max_positions[key])\n                )\n                for key in intersect_keys\n            )\n        else:\n            # For MultiCorpusSampledDataset, will generalize it later\n            if not isinstance(size_fn(idx), Iterable):\n                return all(size_fn(idx) <= b for b in max_positions)\n            return all(\n                a is None or b is None or a <= b\n                for a, b in zip(size_fn(idx), max_positions)\n            )\n\n    ignored = []\n    itr = collect_filtered(check_size, indices, ignored)\n    indices = np.fromiter(itr, dtype=np.int64, count=-1)\n    return indices, ignored\n\n\ndef batch_by_size(\n    indices,\n    num_tokens_fn,\n    num_tokens_vec=None,\n    max_tokens=None,\n    max_sentences=None,\n    required_batch_size_multiple=1,\n    fixed_shapes=None,\n):\n    \"\"\"\n    Yield mini-batches of indices bucketed by size. Batches may contain\n    sequences of different lengths.\n\n    Args:\n        indices (List[int]): ordered list of dataset indices\n        num_tokens_fn (callable): function that returns the number of tokens at\n            a given index\n        num_tokens_vec (List[int], optional): precomputed vector of the number\n            of tokens for each index in indices (to enable faster batch generation)\n        max_tokens (int, optional): max number of tokens in each batch\n            (default: None).\n        max_sentences (int, optional): max number of sentences in each\n            batch (default: None).\n        required_batch_size_multiple (int, optional): require batch size to\n            be less than N or a multiple of N (default: 1).\n        fixed_shapes (List[Tuple[int, int]], optional): if given, batches will\n            only be created with the given shapes. *max_sentences* and\n            *required_batch_size_multiple* will be ignored (default: None).\n    \"\"\"\n    try:\n        from metaseq.data.data_utils_fast import (\n            batch_by_size_fn,\n            batch_by_size_vec,\n            batch_fixed_shapes_fast,\n        )\n    except ImportError:\n        raise ImportError(\n            \"Please build Cython components with: `pip install --editable .` \"\n            \"or `python setup.py build_ext --inplace`\"\n        )\n    except ValueError:\n        raise ValueError(\n            \"Please build (or rebuild) Cython components with: `pip install \"\n            \" --editable .` or `python setup.py build_ext --inplace`.\"\n        )\n\n    # added int() to avoid TypeError: an integer is required\n    max_tokens = int(max_tokens) if max_tokens is not None else -1\n    max_sentences = max_sentences if max_sentences is not None else -1\n    bsz_mult = required_batch_size_multiple\n\n    if not isinstance(indices, np.ndarray):\n        indices = np.fromiter(indices, dtype=np.int64, count=-1)\n\n    if num_tokens_vec is not None and not isinstance(num_tokens_vec, np.ndarray):\n        num_tokens_vec = np.fromiter(num_tokens_vec, dtype=np.int64, count=-1)\n\n    if fixed_shapes is None:\n        if num_tokens_vec is None:\n            return batch_by_size_fn(\n                indices,\n                num_tokens_fn,\n                max_tokens,\n                max_sentences,\n                bsz_mult,\n            )\n        else:\n            return batch_by_size_vec(\n                indices,\n                num_tokens_vec,\n                max_tokens,\n                max_sentences,\n                bsz_mult,\n            )\n    else:\n        fixed_shapes = np.array(fixed_shapes, dtype=np.int64)\n        sort_order = np.lexsort(\n            [\n                fixed_shapes[:, 1].argsort(),  # length\n                fixed_shapes[:, 0].argsort(),  # bsz\n            ]\n        )\n        fixed_shapes_sorted = fixed_shapes[sort_order]\n        return batch_fixed_shapes_fast(indices, num_tokens_fn, fixed_shapes_sorted)\n\n\ndef post_process(sentence: str, symbol: str):\n    if symbol == \"sentencepiece\":\n        sentence = sentence.replace(\" \", \"\").replace(\"\\u2581\", \" \").strip()\n    elif symbol == \"wordpiece\":\n        sentence = sentence.replace(\" \", \"\").replace(\"_\", \" \").strip()\n    elif symbol == \"letter\":\n        sentence = sentence.replace(\" \", \"\").replace(\"|\", \" \").strip()\n    elif symbol == \"_EOW\":\n        sentence = sentence.replace(\" \", \"\").replace(\"_EOW\", \" \").strip()\n    elif symbol == \"none\":\n        pass\n    elif symbol is not None:\n        raise NotImplementedError(f\"Unknown post_process option: {symbol}\")\n    return sentence\n\n\ndef _find_extra_valid_paths(dataset_path: str) -> set:\n    paths = utils.split_paths(dataset_path)\n    all_valid_paths = set()\n    for sub_dir in paths:\n        if \"://\" in sub_dir:\n            continue\n        contents = PathManager.ls(sub_dir)\n        valid_paths = [c for c in contents if re.match(\"valid*[0-9].*\", c) is not None]\n        all_valid_paths |= {os.path.basename(p) for p in valid_paths}\n    # Remove .bin, .idx etc\n    roots = {os.path.splitext(p)[0] for p in all_valid_paths}\n    return roots\n\n\ndef raise_if_valid_subsets_unintentionally_ignored(train_cfg) -> None:\n    \"\"\"Raises if there are paths matching 'valid*[0-9].*' which are not combined or ignored.\"\"\"\n    if (\n        train_cfg.dataset.ignore_unused_valid_subsets\n        or train_cfg.dataset.combine_valid_subsets\n        or train_cfg.dataset.disable_validation\n        or getattr(train_cfg.task, \"data\", None) is None\n    ):\n        return\n    other_paths = _find_extra_valid_paths(train_cfg.task.data)\n    specified_subsets = train_cfg.dataset.valid_subset.split(\",\")\n    ignored_paths = [p for p in other_paths if p not in specified_subsets]\n    if ignored_paths:\n        advice = \"Set --combine-val to combine them or --ignore-unused-valid-subsets to ignore them.\"\n        msg = f\"Valid paths {ignored_paths} will be ignored. {advice}\"\n        raise ValueError(msg)\n",
        "metaseq/data/dictionary.py": "# Copyright (c) Meta Platforms, Inc. and affiliates. All Rights Reserved.\n#\n# This source code is licensed under the MIT license found in the\n# LICENSE file in the root directory of this source tree.\n\nimport os\nfrom collections import Counter\nfrom multiprocessing import Pool\n\nimport torch\n\nfrom metaseq import utils\nfrom metaseq.data import data_utils\nfrom metaseq.file_chunker_utils import Chunker, find_offsets\nfrom metaseq.file_io import PathManager\nfrom metaseq.utils import tokenize_line\n\n\nclass Dictionary:\n    \"\"\"A mapping from symbols to consecutive integers\"\"\"\n\n    def __init__(\n        self,\n        *,  # begin keyword-only arguments\n        bos=\"<s>\",\n        pad=\"<pad>\",\n        eos=\"</s>\",\n        unk=\"<unk>\",\n        extra_special_symbols=None,\n    ):\n        self.bos_word, self.unk_word, self.pad_word, self.eos_word = bos, unk, pad, eos\n        self.symbols = []\n        self.count = []\n        self.indices = {}\n        self.bos_index = self.add_symbol(bos)\n        self.pad_index = self.add_symbol(pad)\n        self.eos_index = self.add_symbol(eos)\n        self.unk_index = self.add_symbol(unk)\n        if extra_special_symbols:\n            for s in extra_special_symbols:\n                self.add_symbol(s)\n        self.nspecial = len(self.symbols)\n\n    def __eq__(self, other):\n        return self.indices == other.indices\n\n    def __getitem__(self, idx):\n        if idx < len(self.symbols):\n            return self.symbols[idx]\n        return self.unk_word\n\n    def get_count(self, idx):\n        return self.count[idx]\n\n    def __len__(self):\n        \"\"\"Returns the number of symbols in the dictionary\"\"\"\n        return len(self.symbols)\n\n    def __contains__(self, sym):\n        return sym in self.indices\n\n    def index(self, sym):\n        \"\"\"Returns the index of the specified symbol\"\"\"\n        assert isinstance(sym, str)\n        if sym in self.indices:\n            return self.indices[sym]\n        return self.unk_index\n\n    def string(\n        self,\n        tensor,\n        bpe_symbol=None,\n        escape_unk=False,\n        extra_symbols_to_ignore=None,\n        unk_string=None,\n        include_eos=False,\n    ):\n        \"\"\"Helper for converting a tensor of token indices to a string.\n\n        Can optionally remove BPE symbols or escape <unk> words.\n        \"\"\"\n        if torch.is_tensor(tensor) and tensor.dim() == 2:\n            return \"\\n\".join(\n                self.string(\n                    t,\n                    bpe_symbol,\n                    escape_unk,\n                    extra_symbols_to_ignore,\n                    include_eos=include_eos,\n                )\n                for t in tensor\n            )\n\n        extra_symbols_to_ignore = set(extra_symbols_to_ignore or [])\n        extra_symbols_to_ignore.add(self.eos())\n\n        def token_string(i):\n            if i == self.unk():\n                if unk_string is not None:\n                    return unk_string\n                else:\n                    return self.unk_string(escape_unk)\n            else:\n                return self[i]\n\n        if hasattr(self, \"bos_index\"):\n            extra_symbols_to_ignore.add(self.bos())\n\n        sent = \" \".join(\n            token_string(i)\n            for i in tensor\n            if utils.item(i) not in extra_symbols_to_ignore\n        )\n\n        return data_utils.post_process(sent, bpe_symbol)\n\n    def unk_string(self, escape=False):\n        \"\"\"Return unknown string, optionally escaped as: <<unk>>\"\"\"\n        if escape:\n            return \"<{}>\".format(self.unk_word)\n        else:\n            return self.unk_word\n\n    def add_symbol(self, word, n=1, overwrite=False):\n        \"\"\"Adds a word to the dictionary\"\"\"\n        if word in self.indices and not overwrite:\n            idx = self.indices[word]\n            self.count[idx] = self.count[idx] + n\n            return idx\n        else:\n            idx = len(self.symbols)\n            self.indices[word] = idx\n            self.symbols.append(word)\n            self.count.append(n)\n            return idx\n\n    def update(self, new_dict):\n        \"\"\"Updates counts from new dictionary.\"\"\"\n        for word in new_dict.symbols:\n            idx2 = new_dict.indices[word]\n            if word in self.indices:\n                idx = self.indices[word]\n                self.count[idx] = self.count[idx] + new_dict.count[idx2]\n            else:\n                idx = len(self.symbols)\n                self.indices[word] = idx\n                self.symbols.append(word)\n                self.count.append(new_dict.count[idx2])\n\n    def finalize(self, threshold=-1, nwords=-1, padding_factor=8):\n        \"\"\"Sort symbols by frequency in descending order, ignoring special ones.\n\n        Args:\n            - threshold defines the minimum word count\n            - nwords defines the total number of words in the final dictionary,\n                including special symbols\n            - padding_factor can be used to pad the dictionary size to be a\n                multiple of 8, which is important on some hardware (e.g., Nvidia\n                Tensor Cores).\n        \"\"\"\n        if nwords <= 0:\n            nwords = len(self)\n\n        new_indices = dict(zip(self.symbols[: self.nspecial], range(self.nspecial)))\n        new_symbols = self.symbols[: self.nspecial]\n        new_count = self.count[: self.nspecial]\n\n        c = Counter(\n            dict(\n                sorted(zip(self.symbols[self.nspecial :], self.count[self.nspecial :]))\n            )\n        )\n        for symbol, count in c.most_common(nwords - self.nspecial):\n            if count >= threshold:\n                new_indices[symbol] = len(new_symbols)\n                new_symbols.append(symbol)\n                new_count.append(count)\n            else:\n                break\n\n        assert len(new_symbols) == len(new_indices)\n\n        self.count = list(new_count)\n        self.symbols = list(new_symbols)\n        self.indices = new_indices\n\n        self.pad_to_multiple_(padding_factor)\n\n    def pad_to_multiple_(self, padding_factor):\n        \"\"\"Pad Dictionary size to be a multiple of *padding_factor*.\"\"\"\n        if padding_factor > 1:\n            i = 0\n            while len(self) % padding_factor != 0:\n                symbol = \"madeupword{:04d}\".format(i)\n                self.add_symbol(symbol, n=0)\n                i += 1\n\n    def bos(self):\n        \"\"\"Helper to get index of beginning-of-sentence symbol\"\"\"\n        return self.bos_index\n\n    def pad(self):\n        \"\"\"Helper to get index of pad symbol\"\"\"\n        return self.pad_index\n\n    def eos(self):\n        \"\"\"Helper to get index of end-of-sentence symbol\"\"\"\n        return self.eos_index\n\n    def unk(self):\n        \"\"\"Helper to get index of unk symbol\"\"\"\n        return self.unk_index\n\n    @classmethod\n    def load(cls, f):\n        \"\"\"Loads the dictionary from a text file with the format:\n\n        ```\n        <symbol0> <count0>\n        <symbol1> <count1>\n        ...\n        ```\n        \"\"\"\n        d = cls()\n        d.add_from_file(f)\n        return d\n\n    def add_from_file(self, f):\n        \"\"\"\n        Loads a pre-existing dictionary from a text file and adds its symbols\n        to this instance.\n        \"\"\"\n        if isinstance(f, str):\n            try:\n                with open(PathManager.get_local_path(f), \"r\", encoding=\"utf-8\") as fd:\n                    self.add_from_file(fd)\n            except FileNotFoundError as fnfe:\n                raise fnfe\n            except UnicodeError:\n                raise Exception(\n                    \"Incorrect encoding detected in {}, please \"\n                    \"rebuild the dataset\".format(f)\n                )\n            return\n\n        lines = f.readlines()\n        indices_start_line = self._load_meta(lines)\n\n        for line in lines[indices_start_line:]:\n            try:\n                line, field = line.rstrip().rsplit(\" \", 1)\n                if field == \"#metaseq:overwrite\":\n                    overwrite = True\n                    line, field = line.rsplit(\" \", 1)\n                else:\n                    overwrite = False\n                count = int(field)\n                word = line\n                if word in self and not overwrite:\n                    raise RuntimeError(\n                        \"Duplicate word found when loading Dictionary: '{}'. \"\n                        \"Duplicate words can overwrite earlier ones by adding the \"\n                        \"#metaseq:overwrite flag at the end of the corresponding row \"\n                        \"in the dictionary file. If using the Camembert model, please \"\n                        \"download an updated copy of the model file.\".format(word)\n                    )\n                self.add_symbol(word, n=count, overwrite=overwrite)\n            except ValueError:\n                raise ValueError(\n                    f\"Incorrect dictionary format, expected '<token> <cnt> [flags]': \\\"{line}\\\"\"\n                )\n\n    def _save(self, f, kv_iterator):\n        if isinstance(f, str):\n            PathManager.mkdirs(os.path.dirname(f))\n            with PathManager.open(f, \"w\", encoding=\"utf-8\") as fd:\n                return self.save(fd)\n        for k, v in kv_iterator:\n            print(\"{} {}\".format(k, v), file=f)\n\n    def _get_meta(self):\n        return [], []\n\n    def _load_meta(self, lines):\n        return 0\n\n    def save(self, f):\n        \"\"\"Stores dictionary into a text file\"\"\"\n        ex_keys, ex_vals = self._get_meta()\n        self._save(\n            f,\n            zip(\n                ex_keys + self.symbols[self.nspecial :],\n                ex_vals + self.count[self.nspecial :],\n            ),\n        )\n\n    def dummy_sentence(self, length):\n        t = torch.Tensor(length).uniform_(self.nspecial + 1, len(self)).long()\n        t[-1] = self.eos()\n        return t\n\n    def encode_line(\n        self,\n        line,\n        line_tokenizer=tokenize_line,\n        add_if_not_exist=True,\n        consumer=None,\n        append_eos=True,\n        reverse_order=False,\n    ) -> torch.IntTensor:\n        words = line_tokenizer(line)\n        if reverse_order:\n            words = list(reversed(words))\n        ids = []\n\n        for word in words:\n            if add_if_not_exist:\n                idx = self.add_symbol(word)\n            else:\n                idx = self.index(word)\n            if consumer is not None:\n                consumer(word, idx)\n            ids.append(idx)\n        if append_eos:\n            ids.append(self.eos_index)\n        return torch.tensor(ids, dtype=torch.int32)\n\n    @staticmethod\n    def _add_file_to_dictionary_single_worker(\n        filename,\n        tokenize,\n        eos_word,\n        start_offset,\n        end_offset,\n    ):\n        counter = Counter()\n        with Chunker(filename, start_offset, end_offset) as line_iterator:\n            for line in line_iterator:\n                for word in tokenize(line):\n                    counter.update([word])\n                counter.update([eos_word])\n        return counter\n\n    @staticmethod\n    def add_file_to_dictionary(filename, dict, tokenize, num_workers):\n        def merge_result(counter):\n            for w, c in sorted(counter.items()):\n                dict.add_symbol(w, c)\n\n        local_file = PathManager.get_local_path(filename)\n        offsets = find_offsets(local_file, num_workers)\n        if num_workers > 1:\n            chunks = zip(offsets, offsets[1:])\n            pool = Pool(processes=num_workers)\n            results = []\n            for start_offset, end_offset in chunks:\n                results.append(\n                    pool.apply_async(\n                        Dictionary._add_file_to_dictionary_single_worker,\n                        (\n                            local_file,\n                            tokenize,\n                            dict.eos_word,\n                            start_offset,\n                            end_offset,\n                        ),\n                    )\n                )\n            pool.close()\n            pool.join()\n            for r in results:\n                merge_result(r.get())\n        else:\n            merge_result(\n                Dictionary._add_file_to_dictionary_single_worker(\n                    local_file, tokenize, dict.eos_word, offsets[0], offsets[1]\n                )\n            )\n\n\nclass TruncatedDictionary(object):\n    def __init__(self, wrapped_dict, length):\n        self.__class__ = type(\n            wrapped_dict.__class__.__name__,\n            (self.__class__, wrapped_dict.__class__),\n            {},\n        )\n        self.__dict__ = wrapped_dict.__dict__\n        self.wrapped_dict = wrapped_dict\n        self.length = min(len(self.wrapped_dict), length)\n\n    def __len__(self):\n        return self.length\n\n    def __getitem__(self, i):\n        if i < self.length:\n            return self.wrapped_dict[i]\n        return self.wrapped_dict.unk()\n",
        "metaseq/data/document_to_sequence.py": "# Copyright (c) Meta Platforms, Inc. and affiliates. All Rights Reserved.\n#\n# This source code is licensed under the MIT license found in the\n# LICENSE file in the root directory of this source tree.\n\nimport os\nimport math\nimport logging\nfrom typing import Optional\n\nimport numpy as np\nimport torch\n\nfrom metaseq.data import data_utils\nfrom metaseq.distributed import utils as distributed_utils\nimport time\n\nfrom typing import Union, List, Iterable, Tuple, TypedDict, Literal\n\nfrom multiprocessing import Array, Lock\nfrom contextlib import contextmanager\nfrom ctypes import c_int, sizeof, memmove, addressof\n\nlogger = logging.getLogger(__name__)\n\n\nclass LockingArray:\n    def __init__(self, size, num_workers=1):\n        # each worker is only allowed to write to its set of elements in the array\n        # the worker locks should be uncontested in normal runs resulting in low overhead.\n\n        # the training process can read the array by locking all the worker locks at once\n        # when it needs to get the array for checkpointing\n\n        self.data = Array(\"i\", size, lock=False)\n        self.set_num_workers(num_workers)\n\n    def set_num_workers(self, num_workers):\n        self.worker_locks = [Lock() for _ in range(num_workers)]\n\n    @contextmanager\n    def _lock_all(self):\n        locked = []\n        try:\n            for l in self.worker_locks:\n                l.acquire()\n                locked.append(l)\n            yield\n        finally:\n            for l in reversed(locked):\n                l.release()\n\n    def __getstate__(self):\n        with self._lock_all():\n            all_bytes = bytes(self.data)\n        return (all_bytes, len(self.worker_locks))\n\n    def __setstate__(self, state):\n        all_bytes, num_workers = state\n        ln = len(all_bytes) // sizeof(c_int)\n        self.__init__(ln, num_workers)\n        memmove(addressof(self.data), all_bytes, len(all_bytes))\n\n\n# Documents can be in one of three states:\n# (1) A torch.Tensor holding the tokens in the document\n# (2) The string \"padding\" which is a document filled with self.padding_idx\n#     used to pad out sequences in some break_modes\n# (3) An integer index into the underlying dataset: self.dataset[index]\n#     This form is used to avoid loading documents that will just be skipped\n#     as specified by to_skip\nDocument = Union[\n    int,  # an unloaded document self.dataset[value]\n    Literal[\"padding\"],  # conceptually a tensor full of self.padding_idx\n    torch.Tensor,  # loaded 1-D tensor full of the tokens from the document\n]\n\n\n# A part of a sequence derived from the slice of a single document\n# The first element is a single-element list containing a document.\n# It is done this way so that if we need to actually tokenize the document\n# represented by an index, we can replace it with a Tensor in all\n# the Sequence fragments that reference the document.\nSequenceFragment = Tuple[List[Document], int, int]  # document, offset, length\n\n\nclass Sequence(TypedDict):\n    block: List[\n        SequenceFragment\n    ]  # These are torch.cat'd together to get the whole sequence\n    ids: List[int]\n\n\ndef blocked_random(seed, normal_size):\n    \"\"\"\n    Create a function that returns random numbers based on seed.\n    Block calls to numpy's integers function because it has high overhead.\n    \"\"\"\n    baserng = np.random.default_rng(seed)\n    state = None\n    buf = None\n    n = 0\n    use_batch = True\n\n    def integers(high):\n        nonlocal state, buf, n, use_batch\n        if use_batch:\n            # in the common case we ask for high=shuffle_buffer_size\n            # we can batch calls to numpy to generate these values\n            if high == normal_size:\n                if n % 1024 == 0:\n                    state = baserng._bit_generator.__getstate__()\n                    buf = baserng.integers(normal_size, size=1024)\n                r = buf[n % 1024]\n                n += 1\n                return r\n\n            # when the buffer drains at the end, we start asking\n            # for a smaller range of random numbers than we have batched.\n            # To match our previous behavior, reset the\n            # state to what it would have been before batching\n            # and generate the final numbers without batching\n            if state is not None:\n                baserng._bit_generator.__setstate__(state)\n                baserng.integers(normal_size, size=n % 1024)\n            use_batch = False\n        return baserng.integers(high)\n\n    return integers\n\n\nclass DocumentToSequenceDataset(torch.utils.data.IterableDataset):\n    \"\"\"Take a dataset containing documents and turn it into an iterable dataset\n    returning sequences of block_size tokens.\n\n    This dataset can only be iterated over once.\n\n    Documents are optionally permuted (permute_documents=True).\n    This iterator has a `len_cache` which is an AtomicArray mapping\n    document id to the number of tokens in the document, which it populates.\n    When populated the len_cache allows the iterator to quickly skip the first to_skip sequences.\n\n    Args:\n        dataset (~torch.utils.data.IterableDataset): dataset to chunk\n        block_size (int): maximum block size\n        break_mode (str, optional): Mode used for breaking tokens. Values can\n            be one of:\n            - 'none': break tokens into equally sized blocks (up to block_size)\n        drop_last (bool, optional): drop the last item (default: True)\n        padding_idx (int, optional): index to use for padding symbols\n            (required if *drop_last* is ``False``)\n        shuffle_buffer_size (int, optional): buffer this many items and shuffle\n            using the provided *seed*; default value is 1, so no shuffling is\n            performed. This can be adjusted dynamically after initialization,\n            but only before iteration has begun.\n        seed (int, optional): seed for shuffling\n        permute_documents (bool, optional): randomly permute the order the documents are read (default: True)\n        source_target (bool, optional): the input dataset returns a tuple of tokens lists (source, target) (default: False)\n        to_skip (int, optional): skip the first to_skip sequences before iteration begins (Default: 0)\n    \"\"\"\n\n    def __init__(\n        self,\n        dataset: torch.utils.data.IterableDataset,\n        block_size: int,\n        break_mode: str = \"none\",\n        drop_last: Optional[bool] = True,\n        padding_idx: Optional[int] = None,\n        shuffle_buffer_size: int = 1,\n        seed: Optional[int] = None,\n        len_cache=None,\n        to_skip=0,\n        permute_documents=True,\n        source_target=False,\n    ):\n        super().__init__()\n        self.dataset = dataset\n        assert len(dataset) > 0\n\n        # PyTorch dataloaders round-robin through worker processes to request data,\n        # but always start with worker 0.\n        # If we stop iteration on round n when n is not divisible by the\n        # num_workers, then when we start again, we need to start with worker n % num_workers.\n        # We adjust which worker is which by adding an offset to each worker, turning worker 0\n        # of the new dataloaders into worker n % num_workers of the previous dataloader.\n        self.worker_offset = 0\n\n        self.document_shuffle_seed = seed\n        self.shuffle_buffer_seed = (\n            None if seed is None else seed + 2273 + 1284\n        )  # at some point in the past these numbers were\n        # added to keep the two seeds different\n\n        self.indices = None\n\n        # A map from document id -> number of tokens in the document.\n        # This is an atomic array because it shared among the dataloader workers and the training\n        # process.\n        self.len_cache = (\n            LockingArray(len(self.dataset)) if len_cache is None else len_cache\n        )\n\n        self.block_size = block_size\n        self.break_mode = break_mode\n        self.drop_last = drop_last\n        self.padding_idx = padding_idx\n        self.shuffle_buffer_size = shuffle_buffer_size\n        self.to_skip = to_skip\n        self.permute_documents = permute_documents\n        self.source_target = source_target\n\n        if break_mode == \"none\":\n            if self.source_target:\n                self.block_iterator = yield_doc_blocks\n            else:\n                self.block_iterator = yield_token_blocks\n        elif break_mode == \"eos_pad_8\":\n            self.block_iterator = yield_single_sentences_pad_8\n        elif break_mode == \"complete\":\n            self.block_iterator = yield_doc_blocks\n        elif break_mode == \"passthrough\":\n            self.block_iterator = yield_passthrough\n        else:\n            raise ValueError(\n                f\"Invalid value for break_mode = {break_mode}.\"\n                'Available options are \"none\", \"eos_pad_8\", \"complete\", or \"passthrough\".'\n            )\n\n        if not drop_last and padding_idx is None:\n            raise ValueError(\"padding_idx is required when drop_last is False\")\n\n        assert shuffle_buffer_size >= 1\n        if shuffle_buffer_size > 1 and seed is None:\n            raise ValueError(\"seed is required when shuffle_buffer_size > 1\")\n\n        # if break_mode != \"none\": raise NotImplementedError\n\n        self._started_iteration = False\n\n    def set_epoch(self, epoch):\n        if hasattr(self.dataset, \"set_epoch\"):\n            self.dataset.set_epoch(epoch)\n\n        if self.permute_documents:\n            # shuffle the dataset according to the seed argument and epoch\n            seed = int(hash((self.document_shuffle_seed, epoch)) % 1e6)\n            with data_utils.numpy_seed(seed):\n                self.indices = np.random.permutation(len(self.dataset))\n        else:\n            self.indices = list(range(len(self.dataset)))\n\n    def set_shuffle_buffer_size(self, new_shuffle_buffer_size):\n        assert not self._started_iteration\n        self.shuffle_buffer_size = new_shuffle_buffer_size\n\n    def set_num_workers(self, num_workers):\n        self.len_cache.set_num_workers(max(1, num_workers))\n\n    def __iter__(self):\n        skip_time = 0\n        t0 = time.time()\n        assert not self._started_iteration\n        self.started_iteration = True\n\n        # When loading with multiple dataloader processes, split the\n        # indices up among workers evenly.\n        worker_info = torch.utils.data.get_worker_info()\n        if worker_info is not None and worker_info.num_workers > 1:\n            chunks = np.array_split(self.indices, worker_info.num_workers)\n            worker_id = (worker_info.id + self.worker_offset) % worker_info.num_workers\n            indices = chunks[worker_id]\n        else:\n            worker_id = 0\n            indices = self.indices\n\n        # The number of sequences to skip before starting, used to fast-forward the\n        # data loader after restarting from a checkpoint. This may be\n        # different per worker if training stop on an iteration not divisible by the worker size\n        to_skip = (\n            self.to_skip if isinstance(self.to_skip, int) else self.to_skip[worker_id]\n        )\n\n        def documents() -> Iterable[Tuple[int, Document]]:\n            \"\"\"\n            Generator that produces whole documents in a shuffled order (permute_documents=True).\n            It returns a tuple of (number_of_tokens, List[Document]).\n            When the number of tokens is already in the `len_cache`, we defer actually loading\n            the document because we might end up skipping it entirely.\n            \"\"\"\n            for idx in indices:\n                ln = self.len_cache.data[idx]\n                if ln == 0:\n                    # Cache miss: we don't know the number of tokens\n                    # so we have to load and tokenize the document.\n                    r = self.dataset[idx]\n                    if self.source_target:\n                        ln = r[0].shape[0]\n                    else:\n                        ln = r.shape[0]\n                    self.len_cache.data[idx] = ln\n                    yield (ln, [r])\n                else:\n                    # Cache hit: we know the number of tokens, so we can\n                    # skip loading the document for now.\n\n                    # We create a single-element list here, so that we can replace the single element\n                    # with the real Tensor value the first time _any_ SentenceFragment needs the\n                    # real data from this document.\n                    yield (ln, [int(idx)])\n\n        block_itr = self.block_iterator(documents(), self.block_size, self.drop_last)\n\n        # block_itr returns sequences from documents in order\n        # we need to shuffle up this order to avoid having batches full of sequences from\n        # one document. This is done with a \"shuffle-buffer\" that randomizes access\n\n        buffer = []\n\n        random = (\n            blocked_random(self.shuffle_buffer_seed, self.shuffle_buffer_size)\n            if self.shuffle_buffer_seed is not None\n            else lambda x: 0\n        )\n\n        def get_next_item_and_replace_in_buffer(replacement_item: Sequence) -> Sequence:\n            # return a random item from the buffer and replace with a new item\n            idx = random(len(buffer))\n            item = buffer[idx]\n            if replacement_item is not None:\n                buffer[idx] = replacement_item\n            else:\n                buffer.pop(idx)\n            return item\n\n        def sequences() -> Iterable[Sequence]:\n            \"\"\"\n            Generator that returns sequences after shuffling the order of the\n            sequences through the shuffle buffer.\n            \"\"\"\n            for block in block_itr:\n                if len(buffer) < self.shuffle_buffer_size:\n                    # initially fill the buffer to the requested size\n                    buffer.append(block)\n                else:\n                    # return random block from the buffer and replace with new block\n                    yield get_next_item_and_replace_in_buffer(block)\n\n            # clear buffer of any remaining items\n            while buffer:\n                yield get_next_item_and_replace_in_buffer(None)\n\n        # Finally, we iterate through our shuffled sequences, skipping the first to_skip\n        # and performing any tokenization we delayed during the skipping process.\n        seq_it = iter(sequences())\n\n        try:\n            if to_skip > 0 and worker_id == 0:\n                logger.info(f\"Skipping {to_skip} sequences\")\n            with self.len_cache.worker_locks[worker_id]:\n                for i in range(to_skip):\n                    next(seq_it)\n            t1 = time.time()\n            skip_time = t1 - t0\n            if worker_id == 0 and distributed_utils.get_global_rank() == 0:\n                local_rank = (\n                    os.environ[\"LOCAL_RANK\"] if \"LOCAL_RANK\" in os.environ else 0\n                )\n                logger.info(\n                    f\"Begin filling streaming dataset buffer for each worker on rank {local_rank}\"\n                )\n            while True:\n                with self.len_cache.worker_locks[worker_id]:\n                    elem = next(seq_it)\n                # we know we are not skipping this sequence, so\n                # we perform any document loading that we deferred in the skipping process.\n                elem[\"ids\"] = torch.LongTensor(elem[\"ids\"])\n                subsequences = []\n                # assemble the sequence form the SequenceFragment\n                for doc, start, ln in elem[\"block\"]:\n                    # doc[0] can be (1) \"padding\", (2) a tensor of tokens,\n                    # or (3) and index into self.dataset that hasn't been loaded yet.\n\n                    # A padding tensor (<padding_value>, 0, length)\n                    if doc[0] == \"padding\":\n                        example = subsequences[-1]\n                        if self.source_target:\n                            example = example[0]\n                        padding_tensor = example.new_full((ln,), self.padding_idx)\n                        if self.source_target:\n                            padding_tensor = (padding_tensor, padding_tensor)\n                        subsequences.append(padding_tensor)\n                    else:\n                        # This single-element list is shared among all SequenceFragments that use\n                        # the same document. We update the list to ensure we only\n                        # ever tokenize the document once.\n                        if isinstance(doc[0], int):\n                            # an index into dataset that hasn't been loaded yet\n                            # load it now (and for all other SequenceFragments where it hasn't been loaded yet)\n                            doc[0] = self.dataset[doc[0]]\n                        if self.source_target:\n                            subsequences.append(\n                                tuple(elem[start : start + ln] for elem in doc[0])\n                            )\n                        else:\n                            subsequences.append(doc[0][start : start + ln])\n                if self.source_target:\n                    del elem[\"block\"]\n                    elem[\"src_block\"] = torch.cat(tuple(s for s, t in subsequences))\n                    elem[\"tgt_block\"] = torch.cat(tuple(t for s, t in subsequences))\n                else:\n                    elem[\"block\"] = torch.cat(subsequences)\n                elem[\"skip_time\"] = skip_time\n                yield elem\n        except StopIteration:\n            return\n\n\ndef yield_single_sentences_pad_8(iterable, block_size, drop_last) -> Iterable[Sequence]:\n    \"\"\"Mimics sample-break-mode eos i.e. 1 example per sequence without any packing.\n    When multiple examples are packed into a single sequence, example tokens would attend\n    to tokens in neighbouring examples, which may be undesirable. This mode can\n    avoid that. Since there is no packing, this mode is considerably slower.\n    We round up the example length to a multiple of 8, pad to this length and\n    return the example as is, without packing, truncating to block_size in cases of\n    very long examples.\n    \"\"\"\n    for idx, (tokens, document) in enumerate(iterable):\n        cur_block = []\n        cur_block_ids = []\n        if tokens > block_size:\n            # truncate right side\n            # TODO: Enable left side truncation\n            tokens = block_size\n\n        cur_block.append((document, 0, tokens))\n\n        # We round up to a multiple of 8 + 1, because later on\n        # one element is removed for src/target tensor creation\n        # which brings it back to a multiple of 8. block_size is\n        # already passed with + 1 included.\n        cur_block_remain = min(int(math.ceil(tokens / 8)) * 8 + 1, block_size)\n        cur_block_remain -= tokens\n        padding = ([\"padding\"], 0, cur_block_remain)\n        cur_block.append(padding)\n        cur_block_ids.append(idx)\n        yield {\n            \"ids\": cur_block_ids,\n            \"block\": cur_block,\n        }\n\n\ndef yield_doc_blocks(iterable, block_size, drop_last) -> Iterable[Sequence]:\n    \"\"\"Mimics sample-break-mode complete\"\"\"\n    cur_block = []\n    cur_block_ids = []\n    cur_block_remain = block_size\n    for idx, (tokens, document) in enumerate(iterable):\n        if tokens > block_size:\n            # truncate right side\n            tokens = block_size\n\n        if tokens > cur_block_remain:\n            padding = ([\"padding\"], 0, cur_block_remain)\n            cur_block.append(padding)\n            yield {\n                \"ids\": cur_block_ids,\n                \"block\": cur_block,\n            }\n\n            cur_block = []\n            cur_block_ids = []\n            cur_block_remain = block_size\n\n        cur_block.append((document, 0, tokens))\n        cur_block_ids.append(idx)\n        cur_block_remain -= tokens\n        assert cur_block_remain >= 0\n    if not drop_last and len(cur_block) > 0:\n        if cur_block_remain > 0:\n            padding = ([\"padding\"], 0, cur_block_remain)\n            cur_block.append(padding)\n        yield {\n            \"ids\": cur_block_ids,\n            \"block\": cur_block,\n        }\n\n\ndef yield_passthrough(iterable, block_size, drop_last) -> Iterable[Sequence]:\n    for idx, (tokens, document) in enumerate(iterable):\n        yield {\n            \"ids\": [idx],\n            \"block\": [(document, 0, tokens)],\n        }\n\n\ndef yield_token_blocks(iterable, block_size, drop_last) -> Iterable[Sequence]:\n    \"\"\"Sample break mode = None. (Pre-Training default).\"\"\"\n    cur_block = []\n    cur_block_ids = []\n    cur_block_remain = block_size\n    for idx, (tokens, document) in enumerate(iterable):\n        cur_block_ids.append(idx)\n        item_offset = 0\n        while tokens:\n            num_to_take = min(tokens, cur_block_remain)\n            cur_block.append((document, item_offset, num_to_take))\n            item_offset += num_to_take\n            cur_block_remain -= num_to_take\n            tokens -= num_to_take\n\n            if cur_block_remain == 0:\n                yield {\n                    \"ids\": cur_block_ids,\n                    \"block\": cur_block,\n                }\n                cur_block = []\n                cur_block_ids = []\n                cur_block_remain = block_size\n\n    if not drop_last and len(cur_block):\n        if cur_block_remain:\n            cur_block.append(([\"padding\"], 0, cur_block_remain))\n        yield {\n            \"ids\": cur_block_ids,\n            \"block\": cur_block,\n        }\n",
        "metaseq/data/encoders/__init__.py": "# Copyright (c) Meta Platforms, Inc. and affiliates. All Rights Reserved.\n#\n# This source code is licensed under the MIT license found in the\n# LICENSE file in the root directory of this source tree.\n\n\nimport importlib\nimport os\n\nfrom metaseq import registry\n\nbuild_tokenizer, register_tokenizer, TOKENIZER_REGISTRY, _ = registry.setup_registry(\n    \"--tokenizer\",\n    default=None,\n)\n\n\nbuild_bpe, register_bpe, BPE_REGISTRY, _ = registry.setup_registry(\n    \"--bpe\",\n    default=None,\n)\n\n\n# automatically import any Python files in the encoders/ directory\nfor file in os.listdir(os.path.dirname(__file__)):\n    if file.endswith(\".py\") and not file.startswith(\"_\"):\n        module = file[: file.find(\".py\")]\n        importlib.import_module(\"metaseq.data.encoders.\" + module)\n",
        "metaseq/data/encoders/gpt2_bpe.py": "# Copyright (c) Meta Platforms, Inc. and affiliates. All Rights Reserved.\n#\n# This source code is licensed under the MIT license found in the\n# LICENSE file in the root directory of this source tree.\n\nfrom dataclasses import dataclass, field\n\nfrom metaseq import file_utils\nfrom metaseq.data.encoders import register_bpe\nfrom metaseq.dataclass import MetaseqDataclass\nfrom .gpt2_bpe_utils import get_encoder\n\nDEFAULT_ENCODER_JSON = \"https://dl.fbaipublicfiles.com/fairseq/gpt2_bpe/encoder.json\"\nDEFAULT_VOCAB_BPE = \"https://dl.fbaipublicfiles.com/fairseq/gpt2_bpe/vocab.bpe\"\n\n\n@dataclass\nclass GPT2BPEConfig(MetaseqDataclass):\n    gpt2_encoder_json: str = field(\n        default=DEFAULT_ENCODER_JSON, metadata={\"help\": \"path to encoder.json\"}\n    )\n    gpt2_vocab_bpe: str = field(\n        default=DEFAULT_VOCAB_BPE, metadata={\"help\": \"path to vocab.bpe\"}\n    )\n\n\n@register_bpe(\"gpt2\", dataclass=GPT2BPEConfig)\nclass GPT2BPE(object):\n    def __init__(self, cfg):\n        encoder_json = file_utils.cached_path(cfg.gpt2_encoder_json)\n        vocab_bpe = file_utils.cached_path(cfg.gpt2_vocab_bpe)\n        self.bpe = get_encoder(encoder_json, vocab_bpe)\n\n    def encode(self, x: str) -> str:\n        return \" \".join(map(str, self.bpe.encode(x)))\n\n    def decode(self, x: str) -> str:\n        return self.bpe.decode(\n            [int(tok) if tok not in {\"<unk>\", \"<mask>\"} else tok for tok in x.split()]\n        )\n\n    def is_beginning_of_word(self, x: str) -> bool:\n        return self.decode(x).startswith(\" \")\n",
        "metaseq/data/encoders/gpt2_bpe_utils.py": "# Copyright (c) Meta Platforms, Inc. and affiliates. All Rights Reserved.\n#\n# This source code is licensed under the MIT license found in the\n# LICENSE file in the root directory of this source tree.\n\n\"\"\"\nByte pair encoding utilities from GPT-2.\n\nOriginal source: https://github.com/openai/gpt-2/blob/master/src/encoder.py\nOriginal license: MIT\n\"\"\"\n\nimport json\nfrom functools import lru_cache\n\n\n@lru_cache()\ndef bytes_to_unicode():\n    \"\"\"\n    Returns list of utf-8 byte and a corresponding list of unicode strings.\n    The reversible bpe codes work on unicode strings.\n    This means you need a large # of unicode characters in your vocab if you want to avoid UNKs.\n    When you're at something like a 10B token dataset you end up needing around 5K for decent coverage.\n    This is a signficant percentage of your normal, say, 32K bpe vocab.\n    To avoid that, we want lookup tables between utf-8 bytes and unicode strings.\n    And avoids mapping to whitespace/control characters the bpe code barfs on.\n    \"\"\"\n    bs = (\n        list(range(ord(\"!\"), ord(\"~\") + 1))\n        + list(range(ord(\"¡\"), ord(\"¬\") + 1))\n        + list(range(ord(\"®\"), ord(\"ÿ\") + 1))\n    )\n    cs = bs[:]\n    n = 0\n    for b in range(2**8):\n        if b not in bs:\n            bs.append(b)\n            cs.append(2**8 + n)\n            n += 1\n    cs = [chr(n) for n in cs]\n    return dict(zip(bs, cs))\n\n\ndef get_pairs(word):\n    \"\"\"Return set of symbol pairs in a word.\n    Word is represented as tuple of symbols (symbols being variable-length strings).\n    \"\"\"\n    pairs = set()\n    prev_char = word[0]\n    for char in word[1:]:\n        pairs.add((prev_char, char))\n        prev_char = char\n    return pairs\n\n\nclass Encoder:\n    def __init__(self, encoder, bpe_merges, errors=\"replace\"):\n        self.encoder = encoder\n        self.decoder = {v: k for k, v in self.encoder.items()}\n        self.errors = errors  # how to handle errors in decoding\n        self.byte_encoder = bytes_to_unicode()\n        self.byte_decoder = {v: k for k, v in self.byte_encoder.items()}\n        self.bpe_ranks = dict(zip(bpe_merges, range(len(bpe_merges))))\n        self.cache = {}\n\n        try:\n            import regex as re\n\n            self.re = re\n        except ImportError:\n            raise ImportError(\"Please install regex with: pip install regex\")\n\n        # Should haved added re.IGNORECASE so BPE merges can happen for capitalized versions of contractions\n        self.pat = self.re.compile(\n            r\"\"\"'s|'t|'re|'ve|'m|'ll|'d| ?\\p{L}+| ?\\p{N}+| ?[^\\s\\p{L}\\p{N}]+|\\s+(?!\\S)|\\s+\"\"\"\n        )\n\n    def bpe(self, token):\n        if token in self.cache:\n            return self.cache[token]\n        word = tuple(token)\n        pairs = get_pairs(word)\n\n        if not pairs:\n            return token\n\n        while True:\n            bigram = min(pairs, key=lambda pair: self.bpe_ranks.get(pair, float(\"inf\")))\n            if bigram not in self.bpe_ranks:\n                break\n            first, second = bigram\n            new_word = []\n            i = 0\n            while i < len(word):\n                try:\n                    j = word.index(first, i)\n                    new_word.extend(word[i:j])\n                    i = j\n                except ValueError:\n                    new_word.extend(word[i:])\n                    break\n\n                if word[i] == first and i < len(word) - 1 and word[i + 1] == second:\n                    new_word.append(first + second)\n                    i += 2\n                else:\n                    new_word.append(word[i])\n                    i += 1\n            new_word = tuple(new_word)\n            word = new_word\n            if len(word) == 1:\n                break\n            else:\n                pairs = get_pairs(word)\n        word = \" \".join(word)\n        self.cache[token] = word\n        return word\n\n    def encode(self, text):\n        bpe_tokens = []\n        for token in self.re.findall(self.pat, text):\n            token = \"\".join(self.byte_encoder[b] for b in token.encode(\"utf-8\"))\n            bpe_tokens.extend(\n                self.encoder[bpe_token] for bpe_token in self.bpe(token).split(\" \")\n            )\n        return bpe_tokens\n\n    def decode(self, tokens):\n        text = \"\".join([self.decoder.get(token, token) for token in tokens])\n        text = bytearray([self.byte_decoder[c] for c in text]).decode(\n            \"utf-8\", errors=self.errors\n        )\n        return text\n\n\ndef get_encoder(encoder_json_path, vocab_bpe_path):\n    with open(encoder_json_path, \"r\") as f:\n        encoder = json.load(f)\n    with open(vocab_bpe_path, \"r\", encoding=\"utf-8\") as f:\n        bpe_data = f.read()\n    bpe_merges = [tuple(merge_str.split()) for merge_str in bpe_data.split(\"\\n\")[1:-1]]\n    return Encoder(\n        encoder=encoder,\n        bpe_merges=bpe_merges,\n    )\n",
        "metaseq/data/encoders/hf_byte_bpe.py": "# Copyright (c) Meta Platforms, Inc. and affiliates. All Rights Reserved.\n#\n# This source code is licensed under the MIT license found in the\n# LICENSE file in the root directory of this source tree.\n\nfrom typing import Optional\nfrom dataclasses import dataclass, field\n\nfrom metaseq import file_utils\nfrom metaseq.data.encoders import register_bpe\nfrom metaseq.dataclass import MetaseqDataclass\n\n\n@dataclass\nclass HuggingFaceByteLevelBPEConfig(MetaseqDataclass):\n    bpe_merges: str = field(default=\"???\", metadata={\"help\": \"path to merges.txt\"})\n    bpe_vocab: str = field(default=\"???\", metadata={\"help\": \"path to vocab.json\"})\n    bpe_add_prefix_space: bool = field(\n        default=False, metadata={\"help\": \"add prefix space before encoding\"}\n    )\n    hf_tokenizer: Optional[str] = field(\n        default=None, metadata={\"help\": \"path to tokenizer file.\"}\n    )\n\n\n@register_bpe(\"hf_byte_bpe\", dataclass=HuggingFaceByteLevelBPEConfig)\nclass HuggingFaceByteLevelBPE(object):\n    def __init__(self, cfg):\n        try:\n            from tokenizers import ByteLevelBPETokenizer, Tokenizer\n        except ImportError:\n            raise ImportError(\n                \"Please install huggingface/tokenizers with: \" \"pip install tokenizers\"\n            )\n\n        if cfg.hf_tokenizer:\n            self.bpe = Tokenizer.from_file(cfg.hf_tokenizer)\n        else:\n            bpe_vocab = file_utils.cached_path(cfg.bpe_vocab)\n            bpe_merges = file_utils.cached_path(cfg.bpe_merges)\n\n            self.bpe = ByteLevelBPETokenizer(\n                bpe_vocab,\n                bpe_merges,\n                add_prefix_space=cfg.bpe_add_prefix_space,\n            )\n\n    def encode(self, x: str) -> str:\n        return \" \".join(map(str, self.bpe.encode(x).ids))\n\n    def decode(self, x: str) -> str:\n        return self.bpe.decode(\n            [int(tok) if tok not in {\"<unk>\", \"<mask>\"} else tok for tok in x.split()]\n        )\n\n    def is_beginning_of_word(self, x: str) -> bool:\n        return self.decode(x).startswith(\" \")\n",
        "metaseq/data/id_dataset.py": "# Copyright (c) Meta Platforms, Inc. and affiliates. All Rights Reserved.\n#\n# This source code is licensed under the MIT license found in the\n# LICENSE file in the root directory of this source tree.\n\nimport torch\n\nfrom . import BaseDataset\n\n\nclass IdDataset(BaseDataset):\n    def __getitem__(self, index):\n        return index\n\n    def __len__(self):\n        return 0\n\n    def collater(self, samples):\n        return torch.tensor(samples)\n",
        "metaseq/data/indexed_dataset.py": "# Copyright (c) Meta Platforms, Inc. and affiliates. All Rights Reserved.\n#\n# This source code is licensed under the MIT license found in the\n# LICENSE file in the root directory of this source tree.\n\nimport shutil\nimport struct\nfrom functools import lru_cache\nfrom typing import Union\n\nimport numpy as np\nimport torch\n\nfrom metaseq.dataclass.constants import DATASET_IMPL_CHOICES\nfrom metaseq.file_io import PathManager\nfrom . import BaseDataset\n\n\ndef best_fitting_int_dtype(\n    max_int_to_represent,\n) -> Union[np.uint16, np.uint32, np.int64]:\n    if max_int_to_represent is None:\n        return np.uint32  # Safe guess\n    elif max_int_to_represent < 65500:\n        return np.uint16\n    elif max_int_to_represent < 4294967295:\n        return np.uint32\n    else:\n        return np.int64\n        # we avoid np.uint64 because it doesn't save space and its type promotion behaves unexpectedly\n        # https://github.com/numpy/numpy/issues/5745\n\n\ndef get_available_dataset_impl():\n    return list(map(str, DATASET_IMPL_CHOICES))\n\n\ndef infer_dataset_impl(path):\n    if IndexedRawTextDataset.exists(path):\n        return \"raw\"\n    elif IndexedDataset.exists(path):\n        with open(index_file_path(path), \"rb\") as f:\n            magic = f.read(8)\n            if magic == IndexedDataset._HDR_MAGIC:\n                return \"cached\"\n            elif magic == MMapIndexedDataset.Index._HDR_MAGIC[:8]:\n                return \"mmap\"\n            else:\n                return None\n    else:\n        return None\n\n\ndef make_builder(out_file, impl, vocab_size=None):\n    if impl == \"mmap\":\n        return MMapIndexedDatasetBuilder(\n            out_file, dtype=best_fitting_int_dtype(vocab_size)\n        )\n    elif impl == \"fasta\":\n        raise NotImplementedError\n    elif impl == \"huffman\":\n        raise ValueError(\n            \"Use HuffmanCodeBuilder directly as it has a different interface.\"\n        )\n    else:\n        return IndexedDatasetBuilder(out_file)\n\n\ndef make_dataset(path, impl, fix_lua_indexing=False, dictionary=None):\n    if impl == \"raw\" and IndexedRawTextDataset.exists(path):\n        assert dictionary is not None\n        return IndexedRawTextDataset(path, dictionary)\n    elif impl == \"lazy\" and IndexedDataset.exists(path):\n        return IndexedDataset(path, fix_lua_indexing=fix_lua_indexing)\n    elif impl == \"cached\" and IndexedDataset.exists(path):\n        return IndexedCachedDataset(path, fix_lua_indexing=fix_lua_indexing)\n    elif impl == \"mmap\" and MMapIndexedDataset.exists(path):\n        return MMapIndexedDataset(path)\n    return None\n\n\ndef dataset_exists(path, impl):\n    if impl == \"raw\":\n        return IndexedRawTextDataset.exists(path)\n    elif impl == \"mmap\":\n        return MMapIndexedDataset.exists(path)\n    else:\n        return IndexedDataset.exists(path)\n\n\ndef read_longs(f, n):\n    a = np.empty(n, dtype=np.int64)\n    f.readinto(a)\n    return a\n\n\ndef write_longs(f, a):\n    f.write(np.array(a, dtype=np.int64))\n\n\n_code_to_dtype = {\n    1: np.uint8,\n    2: np.int8,\n    3: np.int16,\n    4: np.int32,\n    5: np.int64,\n    6: np.single,  # np.float32?\n    7: np.double,\n    8: np.uint16,\n    9: np.uint32,\n    10: np.uint64,\n}\n\n\ndef _dtype_header_code(dtype) -> int:\n    for k in _code_to_dtype.keys():\n        if _code_to_dtype[k] == dtype:\n            return k\n    raise ValueError(dtype)\n\n\ndef index_file_path(prefix_path):\n    return prefix_path + \".idx\"\n\n\ndef data_file_path(prefix_path):\n    return prefix_path + \".bin\"\n\n\nclass IndexedDataset(BaseDataset):\n    \"\"\"Loader for TorchNet IndexedDataset\"\"\"\n\n    _HDR_MAGIC = b\"TNTIDX\\x00\\x00\"\n\n    def __init__(self, path, fix_lua_indexing=False):\n        super().__init__()\n        self.path = path\n        self.fix_lua_indexing = fix_lua_indexing\n        self.data_file = None\n        self.read_index(path)\n\n    def read_index(self, path):\n        with open(index_file_path(path), \"rb\") as f:\n            magic = f.read(8)\n            assert magic == self._HDR_MAGIC, (\n                \"Index file doesn't match expected format. \"\n                \"Make sure that --dataset-impl is configured properly.\"\n            )\n            version = f.read(8)\n            assert struct.unpack(\"<Q\", version) == (1,)\n            code, self.element_size = struct.unpack(\"<QQ\", f.read(16))\n            self.dtype = _code_to_dtype[code]\n            self._len, self.s = struct.unpack(\"<QQ\", f.read(16))\n            self.dim_offsets = read_longs(f, self._len + 1)\n            self.data_offsets = read_longs(f, self._len + 1)\n            self.sizes = read_longs(f, self.s)\n\n    def read_data(self, path):\n        self.data_file = open(data_file_path(path), \"rb\", buffering=0)\n\n    def check_index(self, i):\n        if i < 0 or i >= self._len:\n            raise IndexError(\"index out of range\")\n\n    def __del__(self):\n        if self.data_file:\n            self.data_file.close()\n\n    @lru_cache(maxsize=8)\n    def __getitem__(self, i) -> torch.Tensor:\n        if not self.data_file:\n            self.read_data(self.path)\n        self.check_index(i)\n        tensor_size = self.sizes[self.dim_offsets[i] : self.dim_offsets[i + 1]]\n        a = np.empty(tensor_size, dtype=self.dtype)\n        self.data_file.seek(self.data_offsets[i] * self.element_size)\n        self.data_file.readinto(a)\n        item = torch.from_numpy(a).long()\n        if self.fix_lua_indexing:\n            item -= 1  # subtract 1 for 0-based indexing\n        return item\n\n    def __len__(self):\n        return self._len\n\n    def num_tokens(self, index):\n        return self.sizes[index]\n\n    def size(self, index):\n        return self.sizes[index]\n\n    @staticmethod\n    def exists(path):\n        return PathManager.exists(index_file_path(path)) and PathManager.exists(\n            data_file_path(path)\n        )\n\n    @property\n    def supports_prefetch(self):\n        return False  # avoid prefetching to save memory\n\n\nclass IndexedCachedDataset(IndexedDataset):\n    def __init__(self, path, fix_lua_indexing=False):\n        super().__init__(path, fix_lua_indexing=fix_lua_indexing)\n        self.cache = None\n        self.cache_index = {}\n\n    @property\n    def supports_prefetch(self):\n        return True\n\n    def prefetch(self, indices):\n        if all(i in self.cache_index for i in indices):\n            return\n        if not self.data_file:\n            self.read_data(self.path)\n        indices = sorted(set(indices))\n        total_size = 0\n        for i in indices:\n            total_size += self.data_offsets[i + 1] - self.data_offsets[i]\n        self.cache = np.empty(total_size, dtype=self.dtype)\n        ptx = 0\n        self.cache_index.clear()\n        for i in indices:\n            self.cache_index[i] = ptx\n            size = self.data_offsets[i + 1] - self.data_offsets[i]\n            a = self.cache[ptx : ptx + size]\n            self.data_file.seek(self.data_offsets[i] * self.element_size)\n            self.data_file.readinto(a)\n            ptx += size\n        if self.data_file:\n            # close and delete data file after prefetch so we can pickle\n            self.data_file.close()\n            self.data_file = None\n\n    @lru_cache(maxsize=8)\n    def __getitem__(self, i):\n        self.check_index(i)\n        tensor_size = self.sizes[self.dim_offsets[i] : self.dim_offsets[i + 1]]\n        a = np.empty(tensor_size, dtype=self.dtype)\n        ptx = self.cache_index[i]\n        np.copyto(a, self.cache[ptx : ptx + a.size])\n        item = torch.from_numpy(a).long()\n        if self.fix_lua_indexing:\n            item -= 1  # subtract 1 for 0-based indexing\n        return item\n\n\nclass IndexedRawTextDataset(BaseDataset):\n    \"\"\"Takes a text file as input and binarizes it in memory at instantiation.\n    Original lines are also kept in memory\"\"\"\n\n    def __init__(self, path, dictionary, append_eos=True, reverse_order=False):\n        self.tokens_list = []\n        self.lines = []\n        self.sizes = []\n        self.append_eos = append_eos\n        self.reverse_order = reverse_order\n        self.read_data(path, dictionary)\n        self.size = len(self.tokens_list)\n\n    def read_data(self, path, dictionary):\n        with open(path, \"r\", encoding=\"utf-8\") as f:\n            for line in f:\n                self.lines.append(line.strip(\"\\n\"))\n                tokens = dictionary.encode_line(\n                    line,\n                    add_if_not_exist=False,\n                    append_eos=self.append_eos,\n                    reverse_order=self.reverse_order,\n                ).long()\n                self.tokens_list.append(tokens)\n                self.sizes.append(len(tokens))\n        self.sizes = np.array(self.sizes)\n\n    def check_index(self, i):\n        if i < 0 or i >= self.size:\n            raise IndexError(\"index out of range\")\n\n    @lru_cache(maxsize=8)\n    def __getitem__(self, i):\n        self.check_index(i)\n        return self.tokens_list[i]\n\n    def get_original_text(self, i):\n        self.check_index(i)\n        return self.lines[i]\n\n    def __del__(self):\n        pass\n\n    def __len__(self):\n        return self.size\n\n    def num_tokens(self, index):\n        return self.sizes[index]\n\n    def size(self, index):\n        return self.sizes[index]\n\n    @staticmethod\n    def exists(path):\n        return PathManager.exists(path)\n\n\nclass IndexedDatasetBuilder:\n    element_sizes = {\n        np.uint8: 1,\n        np.int8: 1,\n        np.int16: 2,\n        np.int32: 4,\n        np.int64: 8,\n        np.single: 4,\n        np.double: 8,\n    }\n\n    def __init__(self, out_file, dtype=np.int32):\n        self.out_file = open(out_file, \"wb\")\n        self.dtype = dtype\n        self.data_offsets = [0]\n        self.dim_offsets = [0]\n        self.sizes = []\n        self.element_size = self.element_sizes[self.dtype]\n\n    def add_item(self, tensor):\n        # +1 for Lua compatibility\n        bytes = self.out_file.write(np.array(tensor.numpy() + 1, dtype=self.dtype))\n        self.data_offsets.append(self.data_offsets[-1] + bytes / self.element_size)\n        for s in tensor.size():\n            self.sizes.append(s)\n        self.dim_offsets.append(self.dim_offsets[-1] + len(tensor.size()))\n\n    def merge_file_(self, another_file):\n        index = IndexedDataset(another_file)\n        assert index.dtype == self.dtype\n\n        begin = self.data_offsets[-1]\n        for offset in index.data_offsets[1:]:\n            self.data_offsets.append(begin + offset)\n        self.sizes.extend(index.sizes)\n        begin = self.dim_offsets[-1]\n        for dim_offset in index.dim_offsets[1:]:\n            self.dim_offsets.append(begin + dim_offset)\n\n        with open(data_file_path(another_file), \"rb\") as f:\n            while True:\n                data = f.read(1024)\n                if data:\n                    self.out_file.write(data)\n                else:\n                    break\n\n    def finalize(self, index_file):\n        self.out_file.close()\n        index = open(index_file, \"wb\")\n        index.write(b\"TNTIDX\\x00\\x00\")\n        index.write(struct.pack(\"<Q\", 1))\n        index.write(\n            struct.pack(\"<QQ\", _dtype_header_code(self.dtype), self.element_size)\n        )\n        index.write(struct.pack(\"<QQ\", len(self.data_offsets) - 1, len(self.sizes)))\n        write_longs(index, self.dim_offsets)\n        write_longs(index, self.data_offsets)\n        write_longs(index, self.sizes)\n        index.close()\n\n\ndef _warmup_mmap_file(path):\n    with open(path, \"rb\") as stream:\n        while stream.read(100 * 1024 * 1024):\n            pass\n\n\nclass MMapIndexedDataset(torch.utils.data.Dataset):\n    class Index:\n        _HDR_MAGIC = b\"MMIDIDX\\x00\\x00\"\n\n        @classmethod\n        def writer(cls, path, dtype):\n            class _Writer:\n                def __enter__(self):\n                    self._file = open(path, \"wb\")\n\n                    self._file.write(cls._HDR_MAGIC)\n                    self._file.write(struct.pack(\"<Q\", 1))\n                    self._file.write(struct.pack(\"<B\", _dtype_header_code(dtype)))\n\n                    return self\n\n                @staticmethod\n                def _get_pointers(sizes):\n                    dtype_size = dtype().itemsize\n                    address = 0\n                    pointers = []\n\n                    for size in sizes:\n                        pointers.append(address)\n                        address += size * dtype_size\n\n                    return pointers\n\n                def write(self, sizes):\n                    pointers = self._get_pointers(sizes)\n\n                    self._file.write(struct.pack(\"<Q\", len(sizes)))\n\n                    sizes = np.array(sizes, dtype=np.int32)\n                    self._file.write(sizes.tobytes(order=\"C\"))\n                    del sizes\n\n                    pointers = np.array(pointers, dtype=np.int64)\n                    self._file.write(pointers.tobytes(order=\"C\"))\n                    del pointers\n\n                def __exit__(self, exc_type, exc_val, exc_tb):\n                    self._file.close()\n\n            return _Writer()\n\n        def __init__(self, path):\n            with open(path, \"rb\") as stream:\n                magic_test = stream.read(9)\n                assert self._HDR_MAGIC == magic_test, (\n                    \"Index file doesn't match expected format. \"\n                    \"Make sure that --dataset-impl is configured properly.\"\n                )\n                version = struct.unpack(\"<Q\", stream.read(8))\n                assert (1,) == version\n\n                (dtype_code,) = struct.unpack(\"<B\", stream.read(1))\n                self._dtype = _code_to_dtype[dtype_code]\n                self._dtype_size = self._dtype().itemsize\n\n                self._len = struct.unpack(\"<Q\", stream.read(8))[0]\n                offset = stream.tell()\n\n            _warmup_mmap_file(path)\n\n            self._bin_buffer_mmap = np.memmap(path, mode=\"r\", order=\"C\")\n            self._bin_buffer = memoryview(self._bin_buffer_mmap)\n            self._sizes = np.frombuffer(\n                self._bin_buffer, dtype=np.int32, count=self._len, offset=offset\n            )\n            self._pointers = np.frombuffer(\n                self._bin_buffer,\n                dtype=np.int64,\n                count=self._len,\n                offset=offset + self._sizes.nbytes,\n            )\n\n        def __del__(self):\n            self._bin_buffer_mmap._mmap.close()\n            del self._bin_buffer_mmap\n\n        @property\n        def dtype(self):\n            return self._dtype\n\n        @property\n        def sizes(self):\n            return self._sizes\n\n        @lru_cache(maxsize=8)\n        def __getitem__(self, i):\n            return self._pointers[i], self._sizes[i]\n\n        def __len__(self):\n            return self._len\n\n    def __init__(self, path):\n        super().__init__()\n\n        self._path = None\n        self._index = None\n        self._bin_buffer = None\n\n        self._do_init(path)\n\n    def __getstate__(self):\n        return self._path\n\n    def __setstate__(self, state):\n        self._do_init(state)\n\n    def _do_init(self, path):\n        self._path = path\n        self._index = self.Index(index_file_path(self._path))\n\n        _warmup_mmap_file(data_file_path(self._path))\n        self._bin_buffer_mmap = np.memmap(\n            data_file_path(self._path), mode=\"r\", order=\"C\"\n        )\n        self._bin_buffer = memoryview(self._bin_buffer_mmap)\n\n    def __del__(self):\n        self._bin_buffer_mmap._mmap.close()\n        del self._bin_buffer_mmap\n        del self._index\n\n    def __len__(self):\n        return len(self._index)\n\n    @lru_cache(maxsize=8)\n    def __getitem__(self, i):\n        ptr, size = self._index[i]\n        np_array = np.frombuffer(\n            self._bin_buffer, dtype=self._index.dtype, count=size, offset=ptr\n        )\n        if self._index.dtype != np.int64:\n            np_array = np_array.astype(np.int64)\n\n        return torch.from_numpy(np_array)\n\n    @property\n    def sizes(self):\n        return self._index.sizes\n\n    @property\n    def supports_prefetch(self):\n        return False\n\n    @staticmethod\n    def exists(path):\n        return PathManager.exists(index_file_path(path)) and PathManager.exists(\n            data_file_path(path)\n        )\n\n\ndef get_indexed_dataset_to_local(path) -> str:\n    local_index_path = PathManager.get_local_path(index_file_path(path))\n    local_data_path = PathManager.get_local_path(data_file_path(path))\n\n    assert local_index_path.endswith(\".idx\") and local_data_path.endswith(\".bin\"), (\n        \"PathManager.get_local_path does not return files with expected patterns: \"\n        f\"{local_index_path} and {local_data_path}\"\n    )\n\n    local_path = local_data_path[:-4]  # stripping surfix \".bin\"\n    assert local_path == local_index_path[:-4]  # stripping surfix \".idx\"\n    return local_path\n\n\nclass MMapIndexedDatasetBuilder:\n    def __init__(self, out_file, dtype=np.int64):\n        self._data_file = open(out_file, \"wb\")\n        self._dtype = dtype\n        self._sizes = []\n\n    def add_item(self, tensor):\n        np_array = np.array(tensor.numpy(), dtype=self._dtype)\n        self._data_file.write(np_array.tobytes(order=\"C\"))\n        self._sizes.append(np_array.size)\n\n    def merge_file_(self, another_file):\n        # Concatenate index\n        index = MMapIndexedDataset.Index(index_file_path(another_file))\n        assert index.dtype == self._dtype\n\n        for size in index.sizes:\n            self._sizes.append(size)\n\n        # Concatenate data\n        with open(data_file_path(another_file), \"rb\") as f:\n            shutil.copyfileobj(f, self._data_file)\n\n    def finalize(self, index_file):\n        self._data_file.close()\n\n        with MMapIndexedDataset.Index.writer(index_file, self._dtype) as index:\n            index.write(self._sizes)\n",
        "metaseq/data/iterators.py": "# Copyright (c) Meta Platforms, Inc. and affiliates. All Rights Reserved.\n#\n# This source code is licensed under the MIT license found in the\n# LICENSE file in the root directory of this source tree.\n\nimport itertools\nimport logging\nimport math\nimport operator\nimport os\nimport queue\nimport time\nfrom threading import Thread\nfrom typing import Callable, Optional\nimport numpy as np\nimport torch\nfrom metaseq.distributed import utils as distributed_utils\n\nfrom metaseq.data import data_utils\nfrom metaseq.data.document_to_sequence import DocumentToSequenceDataset\nfrom ctypes import c_int, sizeof, memmove, addressof\n\nlogger = logging.getLogger(__name__)\n\n# Object used by _background_consumer to signal the source is exhausted\n# to the main thread.\n_sentinel = object()\n\n\nclass CountingIterator(object):\n    \"\"\"Wrapper around an iterable that maintains the iteration count.\n\n    Args:\n        iterable (iterable): iterable to wrap\n        start (int): starting iteration count. Note that this doesn't\n            actually advance the iterator.\n        total (int): override the iterator length returned by\n            ``__len__``. This can be used to truncate *iterator*.\n\n    Attributes:\n        n (int): number of elements consumed from this iterator\n    \"\"\"\n\n    def __init__(self, iterable, start=None, total=None):\n        self.iterable = iterable\n        self.itr = iter(self)\n\n        if start is None:\n            self.n = getattr(iterable, \"n\", 0)\n        else:\n            self.n = start\n\n        if total is None:\n            self.total = self.n + len(iterable)\n        else:\n            self.total = total\n\n    def __len__(self):\n        return self.total\n\n    def __iter__(self):\n        for x in self.iterable:\n            if self.n >= self.total:\n                raise RuntimeError(\n                    \"Mismatch between actual and expected iterable length. \"\n                    \"This may be caused by resuming training from a checkpoint using \"\n                    \"a different number of GPUs, in which case you can try the \"\n                    \"--reset-dataloader option. Alternatively you may have a train or \"\n                    \"validation set that is smaller than the number of GPUs. If none \"\n                    \"of these apply, please report this to the metaseq developers.\"\n                )\n            self.n += 1\n            yield x\n\n    def __next__(self):\n        return next(self.itr)\n\n    def has_next(self):\n        \"\"\"Whether the iterator has been exhausted.\"\"\"\n        return self.n < len(self)\n\n    def skip(self, num_to_skip):\n        \"\"\"Fast-forward the iterator by skipping *num_to_skip* elements.\"\"\"\n        next(itertools.islice(self.itr, num_to_skip, num_to_skip), None)\n        return self\n\n    def take(self, n):\n        \"\"\"\n        Truncates the iterator to n elements at most.\n        \"\"\"\n        self.total = min(self.total, n)\n\n        # Propagate this change to the underlying iterator\n        # Only take after what we have already consumed (i.e. after restarting\n        # from checkpoint mid epoch, we have to subtract self.n which is the\n        # starting point)\n        #\n        # This to maintain the invariant self.total = self.n + len(iterable),\n        # before calling __next__ or __iter__\n        propagated_take = max(n - self.n, 0)\n        if hasattr(self.iterable, \"take\"):\n            self.iterable.take(propagated_take)\n        else:\n            self.iterable = itertools.islice(self.iterable, propagated_take)\n\n\nclass StreamingCountingIterator(object):\n    \"\"\"Wrapper around an iterable that maintains the iteration count.\n\n    Args:\n        iterable (iterable): iterable to wrap\n\n    Attributes:\n        n (int): number of elements consumed from this iterator\n    \"\"\"\n\n    def __init__(self, iterable, num_workers, batch_size, num_shards):\n        try:\n            import more_itertools\n        except ImportError:\n            raise ImportError(\n                \"more_itertools is required for streaming iterators; \"\n                \"please install with: pip install more_itertools\"\n            )\n        self._peekable_itr = more_itertools.peekable(iterable)\n\n        self.num_workers = 1 if num_workers == 0 else num_workers\n        self.batch_size = batch_size\n        self.num_shards = num_shards\n\n        self.n = 0\n        self.next_worker = 0\n        self.sequences_consumed = [0 for _ in range(self.num_workers)]\n        self.worker_offset = 0\n\n    def __iter__(self):\n        return self\n\n    def __next__(self):\n        worker_id, r = next(self._peekable_itr)\n        worker_id = (worker_id + self.worker_offset) % self.num_workers\n        self.sequences_consumed[worker_id] += self.batch_size * self.num_shards\n        self.next_worker = (worker_id + 1) % self.num_workers\n        self.n += 1\n        return r\n\n    def __len__(self):\n        return 0\n\n    def has_next(self):\n        return bool(self._peekable_itr)  # whether peekable has items\n\n\nclass EpochBatchIterating(object):\n    def __len__(self) -> int:\n        raise NotImplementedError\n\n    @property\n    def next_epoch_idx(self):\n        raise NotImplementedError\n\n    def next_epoch_itr(\n        self, shuffle=True, fix_batches_to_gpus=False, set_dataset_epoch=True\n    ):\n        \"\"\"Return a new iterator over the dataset.\n\n        Args:\n            shuffle (bool, optional): shuffle batches before returning the\n                iterator (default: True).\n            fix_batches_to_gpus (bool, optional): ensure that batches are always\n                allocated to the same shards across epochs. Requires\n                that :attr:`dataset` supports prefetching (default: False).\n            set_dataset_epoch (bool, optional): update the wrapped Dataset with\n                the new epoch number (default: True).\n        \"\"\"\n        raise NotImplementedError\n\n    def end_of_epoch(self) -> bool:\n        \"\"\"Returns whether the most recent epoch iterator has been exhausted\"\"\"\n        raise NotImplementedError\n\n    @property\n    def iterations_in_epoch(self) -> int:\n        \"\"\"The number of consumed batches in the current epoch.\"\"\"\n        raise NotImplementedError\n\n    def state_dict(self):\n        \"\"\"Returns a dictionary containing a whole state of the iterator.\"\"\"\n        raise NotImplementedError\n\n    def load_state_dict(self, state_dict):\n        \"\"\"Copies the state of the iterator from the given *state_dict*.\"\"\"\n        raise NotImplementedError\n\n    @property\n    def first_batch(self):\n        return \"DUMMY\"\n\n\nclass _CollateWithWorkerID:\n    def __init__(self, collate_fn):\n        self.collate_fn = collate_fn\n\n    def __call__(self, items):\n        r = self.collate_fn(items)\n        worker_info = torch.utils.data.get_worker_info()\n        return (worker_info.id if worker_info else 0, r)\n\n\nclass StreamingEpochBatchIterator(EpochBatchIterating):\n    \"\"\"A steaming-style iterator over a :class:`torch.utils.data.IterableDataset`.\n\n    Args:\n        dataset (~torch.utils.data.Dataset): dataset from which to load the data\n        batch_size (int): number of items in each batch\n        collate_fn (callable): merges a list of samples to form a mini-batch\n        drop_last (bool): whether to skip the last batch, in cases where it\n            would be incomplete (i.e., have fewer than *batch_size* items)\n        num_workers (int, optional): how many subprocesses to use for data\n            loading. 0 means the data will be loaded in the main process\n            (default: 0).\n        epoch (int, optional): the epoch to start the iterator from\n            (default: 1).\n    \"\"\"\n\n    def __init__(\n        self,\n        dataset: torch.utils.data.IterableDataset,\n        batch_size: int,\n        collate_fn: Callable,\n        drop_last: bool,\n        num_workers: int = 0,\n        epoch: int = 1,\n        num_shards: int = 1,\n    ):\n        super().__init__()\n        self.dataset = dataset\n        self.batch_size = batch_size\n        self.collate_fn = collate_fn\n        self.drop_last = drop_last\n        self.num_workers = num_workers\n        self.epoch = max(epoch, 1)  # we use 1-based indexing for epochs\n        self.num_shards = num_shards\n        assert isinstance(dataset, torch.utils.data.IterableDataset)\n\n        self._itr: Optional[StreamingCountingIterator] = None\n        self.worker_offset = 0\n\n    @property\n    def next_epoch_idx(self):\n        \"\"\"Return the epoch index after *next_epoch_itr* is called.\"\"\"\n        if self._itr is not None and self.end_of_epoch():\n            return self.epoch + 1\n        else:\n            return self.epoch\n\n    def next_epoch_itr(self, **kwargs):\n        \"\"\"\n        Return a new iterator over the dataset.\n\n        In case :func:`load_state_dict` has been called recently, this will\n        return the loaded iterator.\n        \"\"\"\n        self.epoch = self.next_epoch_idx\n        if hasattr(self.dataset, \"set_epoch\"):\n            self.dataset.set_epoch(self.epoch)\n        if self._itr is None or self.end_of_epoch():\n            self._itr = self._get_iterator_for_epoch(self.epoch)\n        return self._itr\n\n    def end_of_epoch(self) -> bool:\n        \"\"\"Returns whether the most recent epoch iterator has been exhausted\"\"\"\n        return not self._itr.has_next()\n\n    @property\n    def iterations_in_epoch(self) -> int:\n        \"\"\"The number of consumed batches in the current epoch.\"\"\"\n        return self._itr.n\n\n    def state_dict(self):\n        \"\"\"Returns a dictionary containing a whole state of the iterator.\"\"\"\n        if self.end_of_epoch():\n            # small optimization: we advance the epoch before saving, so that\n            # when loading later we don't end up fast-forwarding the iterator\n            epoch = self.epoch + 1\n            sequences_consumed = [0 for _ in range(self.num_workers)]\n            n = 0\n            next_worker = 0\n        else:\n            epoch = self.epoch\n            sequences_consumed = self._itr.sequences_consumed\n            n = self._itr.n\n            next_worker = self._itr.next_worker\n\n        dataset = self.dataset\n        while not isinstance(dataset, DocumentToSequenceDataset):\n            dataset = dataset.dataset\n        logger.debug(\n            f\"Saving state_dict so we can skip workers quickly: {len(dataset.len_cache.data)} \"\n            f\"entries in tokenization_cache, {sequences_consumed} sequences consumed per worker, iteration {n}\"\n        )\n        return {\n            \"epoch\": epoch,\n            \"sequences_consumed\": sequences_consumed,\n            \"tokenization_cache\": dataset.len_cache\n            if distributed_utils.get_global_rank() == 0\n            else None,\n            \"n\": n,\n            \"next_worker\": next_worker,\n        }\n\n    def load_state_dict(self, state_dict):\n        \"\"\"Copies the state of the iterator from the given *state_dict*.\"\"\"\n        self.epoch = state_dict[\"epoch\"]\n        if hasattr(self.dataset, \"set_epoch\"):\n            self.dataset.set_epoch(self.epoch)\n\n        # must be set before _get_iterator_for_epoch otherwise the datasets in the workers\n        # will not be copied with the right state\n        if (\n            \"sequences_consumed\" in state_dict\n            and max(state_dict[\"sequences_consumed\"]) > 0\n        ):\n            sequences_consumed = state_dict[\"sequences_consumed\"]\n            n = state_dict[\"n\"]\n            next_worker = state_dict[\"next_worker\"]\n\n            logger.info(f\"Skipping {sequences_consumed} sequences in each worker...\")\n            num_workers = 1 if self.num_workers == 0 else self.num_workers\n            assert (\n                len(sequences_consumed) == num_workers\n            ), \"changing the number of workers in the middle of a shard changes the order the data will be loaded in\"\n            dataset = self.dataset\n            while not isinstance(dataset, DocumentToSequenceDataset):\n                dataset = dataset.dataset\n            dataset.to_skip = sequences_consumed\n            dataset.worker_offset = next_worker\n            global_group = distributed_utils.get_global_group()\n            if global_group is None:\n                dataset.len_cache = state_dict[\"tokenization_cache\"]\n            else:\n                if distributed_utils.get_global_rank() == 0:\n                    dataset.len_cache = state_dict[\"tokenization_cache\"]\n                    b, _ = dataset.len_cache.__getstate__()\n                    len_tensor = torch.frombuffer(bytearray(b), dtype=torch.int8).cuda()\n                    distributed_utils.broadcast(len_tensor, 0, global_group)\n                else:\n                    n_bytes = sizeof(c_int) * len(dataset.dataset)\n                    len_tensor = torch.empty(n_bytes, dtype=torch.int8, device=\"cuda\")\n                    distributed_utils.broadcast(len_tensor, 0, global_group)\n                    len_tensor = len_tensor.cpu()\n                    memmove(\n                        addressof(dataset.len_cache.data),\n                        len_tensor.data_ptr(),\n                        n_bytes,\n                    )\n\n            self._itr = self._get_iterator_for_epoch(self.epoch)\n            self._itr.n = n\n\n            if True:\n                # Epilogue bug fixup\n                # Warning: this fix is not correct for the last ~1% of an epoch, but it only needs to be\n                # applied once earlier in the epoch to fix any data loaders with incorrect data.\n                num_workers = self._itr.num_workers\n                batch_size = self._itr.batch_size * self._itr.num_shards\n                if sum(sequences_consumed) != n * batch_size:\n                    logger.warning(\n                        f\"{distributed_utils.get_global_rank()}: Sequences appear corrupted: \"\n                        f\"{n}*{batch_size} != sum({sequences_consumed})\"\n                    )\n                    each, left = divmod(n, num_workers)\n                    sequences_consumed = [\n                        batch_size * (each + (1 if i < left else 0))\n                        for i in range(num_workers)\n                    ]\n                assert sum(sequences_consumed) == n * batch_size\n\n            self._itr.sequences_consumed = sequences_consumed\n            self._itr.next_worker = next_worker\n            self._itr.worker_offset = next_worker\n        else:\n            self._itr = self._get_iterator_for_epoch(self.epoch)\n            # checkpoint from before sequences_consumed was added, slow fast forward...\n            if (\n                \"iterations_in_epoch\" in state_dict\n                and state_dict[\"iterations_in_epoch\"] > 0\n            ):\n                # fast-forward epoch iterator\n                itr_pos = state_dict[\"iterations_in_epoch\"]\n                logger.info(\n                    f\"Fast-forwarding dataloader by {itr_pos} batches using slower logic because \"\n                    \"checkpoint does not have a tokenization_cache...\"\n                )\n                t0 = time.time()\n                next(itertools.islice(self._itr, itr_pos, itr_pos), None)\n                t1 = time.time()\n                logger.info(f\"done fast-forwarding dataloader in {t1 - t0:.1f} seconds\")\n\n    def _get_iterator_for_epoch(self, epoch, offset=0):\n        if self.num_workers > 0:\n            os.environ[\"PYTHONWARNINGS\"] = \"ignore:semaphore_tracker:UserWarning\"\n\n        itr = torch.utils.data.DataLoader(\n            dataset=self.dataset,\n            batch_size=self.batch_size,\n            num_workers=self.num_workers,\n            collate_fn=_CollateWithWorkerID(self.collate_fn),\n            pin_memory=True,\n            drop_last=self.drop_last,\n            worker_init_fn=getattr(self.dataset, \"worker_init_fn\", None),\n        )\n\n        itr = StreamingCountingIterator(\n            itr, self.num_workers, self.batch_size, self.num_shards\n        )\n\n        return itr\n\n\nclass EpochBatchIterator(EpochBatchIterating):\n    \"\"\"A multi-epoch iterator over a :class:`torch.utils.data.Dataset`.\n\n    Compared to :class:`torch.utils.data.DataLoader`, this iterator:\n\n    - can be reused across multiple epochs with the :func:`next_epoch_itr`\n      method (optionally shuffled between epochs)\n    - can be serialized/deserialized with the :func:`state_dict` and\n      :func:`load_state_dict` methods\n    - supports sharding with the *num_shards* and *shard_id* arguments\n\n    Args:\n        dataset (~torch.utils.data.Dataset): dataset from which to load the data\n        collate_fn (callable): merges a list of samples to form a mini-batch\n        batch_sampler (~torch.utils.data.Sampler or a callable): an iterator over batches of\n            indices, or a callable to create such an iterator (~torch.utils.data.Sampler).\n            A callable batch_sampler will be called for each epoch to enable per epoch dynamic\n            batch iterators defined by this callable batch_sampler.\n        seed (int, optional): seed for random number generator for\n            reproducibility (default: 1).\n        num_shards (int, optional): shard the data iterator into N\n            shards (default: 1).\n        shard_id (int, optional): which shard of the data iterator to\n            return (default: 0).\n        num_workers (int, optional): how many subprocesses to use for data\n            loading. 0 means the data will be loaded in the main process\n            (default: 0).\n        epoch (int, optional): the epoch to start the iterator from\n            (default: 1).\n        buffer_size (int, optional): the number of batches to keep ready in the\n            queue. Helps speeding up dataloading. When buffer_size is zero, the\n            default torch.utils.data.DataLoader preloading is used.\n        timeout (int, optional): if positive, the timeout value for collecting a batch\n            from workers. Should always be non-negative (default: ``0``).\n        disable_shuffling (bool, optional): force disable shuffling\n            (default: ``False``).\n        skip_remainder_batch (bool, optional): if set, discard the last batch in an epoch\n            for the sake of training stability, as the last batch is usually smaller than\n                local_batch_size * distributed_word_size (default: ``True``).\n    \"\"\"\n\n    def __init__(\n        self,\n        dataset,\n        collate_fn,\n        batch_sampler,\n        seed=1,\n        num_shards=1,\n        shard_id=0,\n        num_workers=0,\n        epoch=1,\n        buffer_size=0,\n        timeout=0,\n        disable_shuffling=False,\n        skip_remainder_batch=True,\n    ):\n        assert isinstance(dataset, torch.utils.data.Dataset)\n        self.dataset = dataset\n        self.collate_fn = collate_fn\n        self.batch_sampler = batch_sampler\n        self._frozen_batches = (\n            tuple(batch_sampler) if not callable(batch_sampler) else None\n        )\n        self.seed = seed\n        self.num_shards = num_shards\n        self.shard_id = shard_id\n        self.num_workers = num_workers\n        # This upper limit here is to prevent people from abusing this feature\n        # in a shared computing environment.\n        self.buffer_size = min(buffer_size, 20)\n        self.timeout = timeout\n        self.disable_shuffling = disable_shuffling\n        self.skip_remainder_batch = skip_remainder_batch\n\n        self.epoch = max(epoch, 1)  # we use 1-based indexing for epochs\n        self.shuffle = not disable_shuffling\n        self._cur_epoch_itr = None\n        self._next_epoch_itr = None\n        self._supports_prefetch = getattr(dataset, \"supports_prefetch\", False)\n\n    @property\n    def frozen_batches(self):\n        if self._frozen_batches is None:\n            self._frozen_batches = tuple(self.batch_sampler(self.dataset, self.epoch))\n        return self._frozen_batches\n\n    @property\n    def first_batch(self):\n        if len(self.frozen_batches) == 0:\n            raise Exception(\n                \"The dataset is empty. This could indicate \"\n                \"that all elements in the dataset have been skipped. \"\n                \"Try increasing the max number of allowed tokens or using \"\n                \"a larger dataset.\"\n            )\n\n        if getattr(self.dataset, \"supports_fetch_outside_dataloader\", True):\n            return self.collate_fn([self.dataset[i] for i in self.frozen_batches[0]])\n        else:\n            return \"DUMMY\"\n\n    def __len__(self):\n        return int(math.ceil(len(self.frozen_batches) / float(self.num_shards)))\n\n    @property\n    def n(self):\n        return self.iterations_in_epoch\n\n    @property\n    def next_epoch_idx(self):\n        \"\"\"Return the epoch index after *next_epoch_itr* is called.\"\"\"\n        if self._next_epoch_itr is not None:\n            return self.epoch\n        elif self._cur_epoch_itr is not None and self.end_of_epoch():\n            return self.epoch + 1\n        else:\n            return self.epoch\n\n    def next_epoch_itr(\n        self, shuffle=True, fix_batches_to_gpus=False, set_dataset_epoch=True\n    ):\n        \"\"\"Return a new iterator over the dataset.\n\n        Args:\n            shuffle (bool, optional): shuffle batches before returning the\n                iterator (default: True).\n            fix_batches_to_gpus (bool, optional): ensure that batches are always\n                allocated to the same shards across epochs. Requires\n                that :attr:`dataset` supports prefetching (default: False).\n            set_dataset_epoch (bool, optional): update the wrapped Dataset with\n                the new epoch number (default: True).\n        \"\"\"\n        if self.disable_shuffling:\n            shuffle = False\n        self.epoch = self.next_epoch_idx\n        if set_dataset_epoch and hasattr(self.dataset, \"set_epoch\"):\n            self.dataset.set_epoch(self.epoch)\n        if self._next_epoch_itr is not None:\n            self._cur_epoch_itr = self._next_epoch_itr\n            self._next_epoch_itr = None\n        else:\n            if callable(self.batch_sampler):\n                # reset _frozen_batches to refresh the next epoch\n                self._frozen_batches = None\n            self._cur_epoch_itr = self._get_iterator_for_epoch(\n                self.epoch,\n                shuffle,\n                fix_batches_to_gpus=fix_batches_to_gpus,\n            )\n        self.shuffle = shuffle\n        return self._cur_epoch_itr\n\n    def end_of_epoch(self) -> bool:\n        \"\"\"Returns whether the most recent epoch iterator has been exhausted\"\"\"\n        return not self._cur_epoch_itr.has_next()\n\n    @property\n    def iterations_in_epoch(self):\n        \"\"\"The number of consumed batches in the current epoch.\"\"\"\n        if self._cur_epoch_itr is not None:\n            return self._cur_epoch_itr.n\n        elif self._next_epoch_itr is not None:\n            return self._next_epoch_itr.n\n        return 0\n\n    def state_dict(self):\n        \"\"\"Returns a dictionary containing a whole state of the iterator.\"\"\"\n        if self.end_of_epoch():\n            epoch = self.epoch + 1\n            iter_in_epoch = 0\n        else:\n            epoch = self.epoch\n            iter_in_epoch = self.iterations_in_epoch\n        return {\n            \"version\": 2,\n            \"epoch\": epoch,\n            \"iterations_in_epoch\": iter_in_epoch,\n            \"shuffle\": self.shuffle,\n        }\n\n    def load_state_dict(self, state_dict):\n        \"\"\"Copies the state of the iterator from the given *state_dict*.\"\"\"\n        self.epoch = state_dict[\"epoch\"]\n        itr_pos = state_dict.get(\"iterations_in_epoch\", 0)\n        version = state_dict.get(\"version\", 1)\n        if itr_pos > 0:\n            # fast-forward epoch iterator\n            self._next_epoch_itr = self._get_iterator_for_epoch(\n                self.epoch,\n                shuffle=state_dict.get(\"shuffle\", True),\n                offset=itr_pos,\n            )\n            if self._next_epoch_itr is None:\n                if version == 1:\n                    # legacy behavior: we finished the epoch, increment epoch counter\n                    self.epoch += 1\n                else:\n                    raise RuntimeError(\n                        \"Cannot resume training due to dataloader mismatch, please \"\n                        \"report this to the metaseq developers. You can relaunch \"\n                        \"training with `--reset-dataloader` and it should work.\"\n                    )\n        else:\n            self._next_epoch_itr = None\n\n    def _get_iterator_for_epoch(\n        self, epoch, shuffle, fix_batches_to_gpus=False, offset=0\n    ):\n        def shuffle_batches(batches, seed):\n            with data_utils.numpy_seed(seed):\n                np.random.shuffle(batches)\n            return batches\n\n        if self._supports_prefetch:\n            batches = self.frozen_batches\n\n            if shuffle and not fix_batches_to_gpus:\n                batches = shuffle_batches(list(batches), self.seed + epoch)\n\n            batches = list(\n                ShardedIterator(batches, self.num_shards, self.shard_id, fill_value=[])\n            )\n            self.dataset.prefetch([i for s in batches for i in s])\n\n            if shuffle and fix_batches_to_gpus:\n                batches = shuffle_batches(batches, self.seed + epoch + self.shard_id)\n        else:\n            if shuffle:\n                batches = shuffle_batches(list(self.frozen_batches), self.seed + epoch)\n            else:\n                batches = self.frozen_batches\n            batches = list(\n                ShardedIterator(batches, self.num_shards, self.shard_id, fill_value=[])\n            )\n\n        if offset > 0 and offset >= len(batches):\n            return None\n\n        if self.num_workers > 0:\n            os.environ[\"PYTHONWARNINGS\"] = \"ignore:semaphore_tracker:UserWarning\"\n\n        # Create data loader\n        itr = torch.utils.data.DataLoader(\n            self.dataset,\n            collate_fn=self.collate_fn,\n            batch_sampler=batches[offset:],\n            num_workers=self.num_workers,\n            timeout=self.timeout,\n        )\n\n        # Wrap with a BufferedIterator if needed\n        if self.buffer_size > 0:\n            itr = BufferedIterator(self.buffer_size, itr)\n\n        # Wrap with CountingIterator\n        itr = CountingIterator(itr, start=offset)\n\n        if self.skip_remainder_batch:\n            # TODO: Below is a lazy implementation which discard the final batch regardless\n            # of whether it is a full batch or not.\n            total_num_itrs = len(batches) - 1\n            itr.take(total_num_itrs)\n            logger.info(f\"skip final residual batch, total_num_itrs = {total_num_itrs}\")\n\n        return itr\n\n\nclass GroupedIterator(CountingIterator):\n    \"\"\"Wrapper around an iterable that returns groups (chunks) of items.\n\n    Args:\n        iterable (iterable): iterable to wrap\n        chunk_size (int): size of each chunk\n        skip_remainder_batch (bool, optional): if set, discard the last grouped batch in\n          each training epoch, as the last grouped batch is usually smaller than\n                local_batch_size * distributed_word_size * chunk_size (default: ``True``).\n    Attributes:\n        n (int): number of elements consumed from this iterator\n    \"\"\"\n\n    def __init__(self, iterable, chunk_size, skip_remainder_batch=True):\n        if skip_remainder_batch:\n            total_num_itrs = int(math.floor(len(iterable) / float(chunk_size)))\n            logger.info(\n                f\"skip final residual batch, grouped total_num_itrs = {total_num_itrs}\"\n            )\n        else:\n            total_num_itrs = int(math.ceil(len(iterable) / float(chunk_size)))\n            logger.info(f\"grouped total_num_itrs = {total_num_itrs}\")\n\n        itr = _chunk_iterator(iterable, chunk_size, skip_remainder_batch)\n        super().__init__(\n            itr,\n            start=int(math.ceil(getattr(iterable, \"n\", 0) / float(chunk_size))),\n            total=total_num_itrs,\n        )\n        self.chunk_size = chunk_size\n\n        if skip_remainder_batch:\n            self.take(total_num_itrs)\n            # TODO: [Hack] Here the grouped iterator modifies the base iterator size so that\n            # training can move into the next epoch once the grouped iterator is exhausted.\n            # Double-check this implementation in case unexpected behavior occurs.\n            iterable.take(total_num_itrs * chunk_size)\n\n\ndef _chunk_iterator(itr, chunk_size, skip_remainder_batch=True):\n    chunk = []\n    for x in itr:\n        chunk.append(x)\n        if len(chunk) == chunk_size:\n            yield chunk\n            chunk = []\n    if not skip_remainder_batch and len(chunk) > 0:\n        yield chunk\n\n\nclass ShardedIterator(CountingIterator):\n    \"\"\"A sharded wrapper around an iterable, padded to length.\n\n    Args:\n        iterable (iterable): iterable to wrap\n        num_shards (int): number of shards to split the iterable into\n        shard_id (int): which shard to iterator over\n        fill_value (Any, optional): padding value when the iterable doesn't\n            evenly divide *num_shards* (default: None).\n\n    Attributes:\n        n (int): number of elements consumed from this iterator\n    \"\"\"\n\n    def __init__(self, iterable, num_shards, shard_id, fill_value=None):\n        if shard_id < 0 or shard_id >= num_shards:\n            raise ValueError(\"shard_id must be between 0 and num_shards\")\n        sharded_len = int(math.ceil(len(iterable) / float(num_shards)))\n        itr = map(\n            operator.itemgetter(1),\n            itertools.zip_longest(\n                range(sharded_len),\n                itertools.islice(iterable, shard_id, len(iterable), num_shards),\n                fillvalue=fill_value,\n            ),\n        )\n        super().__init__(\n            itr,\n            start=int(math.ceil(getattr(iterable, \"n\", 0) / float(num_shards))),\n            total=sharded_len,\n        )\n\n\nclass BackgroundConsumer(Thread):\n    def __init__(self, queue, source, max_len):\n        Thread.__init__(self)\n\n        self._queue = queue\n        self._source = source\n        self._max_len = max_len\n        self.count = 0\n\n    def run(self):\n        try:\n            for item in self._source:\n                self._queue.put(item)\n\n                # Stop if we reached the maximum length\n                self.count += 1\n                if self._max_len is not None and self.count >= self._max_len:\n                    break\n\n            # Signal the consumer we are done.\n            self._queue.put(_sentinel)\n        except Exception as e:\n            self._queue.put(e)\n\n\nclass BufferedIterator(object):\n    def __init__(self, size, iterable):\n        self._queue = queue.Queue(size)\n        self._iterable = iterable\n        self._consumer = None\n\n        self.start_time = time.time()\n        self.warning_time = None\n\n        self.total = len(iterable)\n\n    def _create_consumer(self):\n        self._consumer = BackgroundConsumer(\n            self._queue,\n            self._iterable,\n            self.total,\n        )\n        self._consumer.daemon = True\n        self._consumer.start()\n\n    def __iter__(self):\n        return self\n\n    def __len__(self):\n        return self.total\n\n    def take(self, n):\n        self.total = min(self.total, n)\n\n        # Propagate this change to the underlying iterator\n        if hasattr(self._iterable, \"take\"):\n            self._iterable.take(n)\n\n    def __next__(self):\n        # Create consumer if not created yet\n        if self._consumer is None:\n            self._create_consumer()\n\n        # Notify the user if there is a data loading bottleneck\n        if self._queue.qsize() < min(2, max(1, self._queue.maxsize // 2)):\n            if time.time() - self.start_time > 5 * 60:\n                if (\n                    self.warning_time is None\n                    or time.time() - self.warning_time > 15 * 60\n                ):\n                    logger.debug(\n                        \"Data loading buffer is empty or nearly empty. This may \"\n                        \"indicate a data loading bottleneck, and increasing the \"\n                        \"number of workers (--num-workers) may help.\"\n                    )\n                    self.warning_time = time.time()\n\n        # Get next example\n        item = self._queue.get(True)\n        if isinstance(item, Exception):\n            raise item\n        if item is _sentinel:\n            raise StopIteration()\n        return item\n",
        "metaseq/data/jsonl_dataset.py": "# Copyright (c) Meta Platforms, Inc. and affiliates. All Rights Reserved.\n#\n# This source code is licensed under the MIT license found in the\n# LICENSE file in the root directory of this source tree.\n\nimport argparse\nfrom io import TextIOWrapper\nimport json\nimport logging\nimport mmap\nimport os\nimport sys\nimport threading\nfrom pathlib import Path\nfrom typing import Callable, Optional\n\nimport numpy as np\nimport torch\n\nimport metaseq.distributed.utils as distributed_utils\n\nlogger = logging.getLogger(__name__)\n\n\nclass JsonlDataset(torch.utils.data.Dataset):\n    \"\"\"\n    For loading JSONL data and encoding on-the-fly with a given tokenizer.\n\n    JSONL format is expected to roughly follow that of The Pile.\n    One-line-per-document of the form:\n    ```\n    {\n        \"text\": \"text goes here, with newlines\",\n        \"meta\": {\"pile_set_name\": \"name of corpus\", \"other\": \"metadata\"}\n    }\n    ```\n\n    Note that only the \"text\" key is used.\n    \"\"\"\n\n    def __init__(\n        self,\n        path: str,\n        tokenizer: Optional[Callable] = None,\n        recache=False,\n        epoch=1,\n        data_subshard_count=1,\n    ):\n        self.path = path\n        self.tokenizer = tokenizer\n\n        self.threadlocal = threading.local()\n        # resolve symlinks to for cached indexes. This lets us re-use indexes\n        # across our experiments using differently composed datasets\n        resolved_path = Path(path).resolve()\n        # TODO(susan): Fix this fairseq reference. _build_index fails otherwise.\n        self.cache = Path(f\"{resolved_path}.fairseq.idx.npy\")\n        # only build the cache in on the primary worker to prevent overloading nfs\n        if distributed_utils.get_global_rank() != 0:\n            distributed_utils.global_barrier()\n        if self.cache.exists() and not recache:\n            logger.info(f\"Loading up cache: {self.cache}\")\n            self.offsets = np.load(self.cache, allow_pickle=True)\n        elif distributed_utils.get_global_rank() == 0:\n            self.offsets = self._build_index(path)\n            np.save(self.cache, self.offsets, allow_pickle=False)\n        if distributed_utils.get_global_rank() == 0:\n            distributed_utils.global_barrier()\n\n        self.epoch = epoch\n        self.data_subshard_count = data_subshard_count\n\n    def _get_mmap(self):\n        if not hasattr(self.threadlocal, \"handles\"):\n            f = open(self.path, \"rb\")\n            mm = mmap.mmap(f.fileno(), 0, access=mmap.ACCESS_READ)\n            self.threadlocal.handles = [f, mm]\n            if (\n                self.path.endswith(\".gz\")\n                or self.path.endswith(\".bz\")\n                or self.path.endswith(\".bz2\")\n            ):\n                raise NotImplementedError(\n                    \"Compressed files are not supported because .seek() would require \"\n                    \"rereading the entire file, making performance too slow.\"\n                )\n        return self.threadlocal.handles[-1]\n\n    def __getitem__(self, idx):\n        # Convert 0 based idx to subshard based idx\n        # For instance, for a data_subshard_count of 3 and epoch number of 1,\n        # subshard_idx goes like 0, 3, 6, 9 ...\n        # For more details, see https://github.com/facebookresearch/metaseq/issues/166\n        subshard_idx = self._get_subshard_id() + idx * self.data_subshard_count\n        if subshard_idx < 0 or subshard_idx >= len(self.offsets):\n            raise IndexError\n        f = self._get_mmap()\n        position = self.offsets[subshard_idx]\n        f.seek(position)\n        item = f.readline().decode(\"utf-8\")\n        try:\n            item = json.loads(item)\n        except json.decoder.JSONDecodeError:\n            raise json.decoder.JSONDecodeError(\n                doc=self.path,\n                pos=position,\n                msg=(\n                    f\"Error while loading JSONL line in file {self.path} at byte \"\n                    f\"{position}. Contents of line:\\n{item}\"\n                ),\n            )\n        if self.tokenizer is not None:\n            item = self.tokenizer(item)\n        return item\n\n    def __len__(self):\n        # Virtual length of the dataset depends on the epoch number if the number of documents\n        # is not perfectly divisible by the data_subshard_count\n        if len(self.offsets) % self.data_subshard_count == 0:\n            return len(self.offsets) // self.data_subshard_count\n        else:\n            # We are left with len(self.offsets) % self.data_subshard_count extra documents at the end\n            extra_document_count = len(self.offsets) % self.data_subshard_count\n\n            # Depending on the subshard id, these extra documents would be included or not\n            if self._get_subshard_id() + 1 <= extra_document_count:\n                return (len(self.offsets) // self.data_subshard_count) + 1\n            else:\n                return len(self.offsets) // self.data_subshard_count\n\n    def _get_subshard_id(self):\n        # Returns the subshard_id, which goes from 0 to self.data_subshard_count - 1 (0 indexed)\n        # and then wraps around if the epoch id goes beyond the data_subshard_count\n        return (self.epoch - 1) % self.data_subshard_count\n\n    def _build_index(self, file_path: str):\n        \"\"\"Build index of start positions of each line.\"\"\"\n        logger.info(f\"Building index for file: {file_path}\")\n        file: TextIOWrapper = self._get_mmap()\n\n        offsets = [0]\n        for _ in iter(file.readline, b\"\"):\n            offsets.append(file.tell())\n\n        # return all offsets except the last one, which is the end of the file\n        return offsets[:-1]\n\n    def __setstate__(self, state):\n        self.__dict__ = state\n        self.threadlocal = threading.local()\n\n    def __getstate__(self):\n        d = {}\n        for i, v in self.__dict__.items():\n            if i != \"threadlocal\":\n                d[i] = v\n        return d\n\n    def __del__(self):\n        if hasattr(self.threadlocal, \"handles\"):\n            # cleanup files we opened on initialization\n            while self.threadlocal.handles:\n                self.threadlocal.handles.pop().close()\n\n    @staticmethod\n    def exists(path):\n        return os.path.exists(path)\n\n\nif __name__ == \"__main__\":\n    \"\"\"Usage:\n    python metaseq/data/jsonl_dataset.py \"flan_streaming/valid/00/*.jsonl\"\n    \"\"\"\n    parser = argparse.ArgumentParser(\n        description=\"Precompute index file from JSONL files\"\n    )\n    parser.add_argument(\n        \"pattern\", help=\"glob to jsonl files, e.g. flan_streaming/valid/00/*.jsonl\"\n    )\n    args = parser.parse_args()\n\n    logging.basicConfig(\n        format=\"%(asctime)s | %(levelname)s | %(name)s | %(message)s\",\n        datefmt=\"%Y-%m-%d %H:%M:%S\",\n        level=os.environ.get(\"LOGLEVEL\", \"INFO\").upper(),\n        stream=sys.stdout,\n    )\n    from glob import glob\n\n    from tqdm import tqdm\n\n    for f in tqdm(list(glob(args.pattern))):\n        JsonlDataset(f, recache=True)\n",
        "metaseq/data/list_dataset.py": "# Copyright (c) Meta Platforms, Inc. and affiliates. All Rights Reserved.\n#\n# This source code is licensed under the MIT license found in the\n# LICENSE file in the root directory of this source tree.\n\nfrom . import BaseWrapperDataset\n\n\nclass ListDataset(BaseWrapperDataset):\n    def __init__(self, dataset, sizes=None):\n        super().__init__(dataset)\n        self._sizes = sizes\n\n    def __iter__(self):\n        for x in self.dataset:\n            yield x\n\n    def collater(self, samples):\n        return samples\n\n    @property\n    def sizes(self):\n        return self._sizes\n\n    def num_tokens(self, index):\n        return self.sizes[index]\n\n    def size(self, index):\n        return self.sizes[index]\n\n    def set_epoch(self, epoch):\n        pass\n",
        "metaseq/data/lm_context_window_dataset.py": "# Copyright (c) Meta Platforms, Inc. and affiliates. All Rights Reserved.\n#\n# This source code is licensed under the MIT license found in the\n# LICENSE file in the root directory of this source tree.\n\nfrom typing import Dict\n\nimport numpy as np\nimport torch\n\nfrom metaseq.data.monolingual_dataset import MonolingualDataset\nfrom . import BaseDataset\n\n\nclass LMContextWindowDataset(BaseDataset):\n    \"\"\"\n    Wraps a MonolingualDataset and provides more context for evaluation.\n\n    Each item in the new dataset will have a maximum size of\n    ``tokens_per_sample + context_window``.\n\n    Args:\n        dataset: dataset to wrap\n        tokens_per_sample (int): the max number of tokens in each dataset item\n        context_window (int): the number of accumulated tokens to add to each\n            dataset item\n        pad_idx (int): padding symbol\n    \"\"\"\n\n    def __init__(\n        self,\n        dataset: MonolingualDataset,\n        tokens_per_sample: int,\n        context_window: int,\n        pad_idx: int,\n    ):\n        assert context_window > 0\n        self.dataset = dataset\n        self.tokens_per_sample = tokens_per_sample\n        self.context_window = context_window\n        self.pad_idx = pad_idx\n        self.prev_tokens = np.empty([0])\n\n    def __getitem__(self, index):\n        return self.dataset[index]\n\n    def __len__(self):\n        return len(self.dataset)\n\n    def collater(self, samples) -> Dict:\n        sample = self.dataset.collater(samples)\n\n        pad = self.pad_idx\n        max_sample_len = self.tokens_per_sample + self.context_window\n\n        bsz, tsz = sample[\"net_input\"][\"src_tokens\"].shape\n        start_idxs = [0] * bsz\n        toks = sample[\"net_input\"][\"src_tokens\"]\n        lengths = sample[\"net_input\"][\"src_lengths\"]\n        tgt = sample[\"target\"]\n        new_toks = np.empty([bsz, tsz + self.context_window], dtype=np.int64)\n        new_tgt = np.full([bsz, tsz + self.context_window], pad, dtype=np.int64)\n        sample_lens = toks.ne(pad).long().sum(dim=1).cpu()\n        for i in range(bsz):\n            sample_len = sample_lens[i]\n            extra = len(self.prev_tokens) + sample_len - max_sample_len\n            if extra > 0:\n                self.prev_tokens = self.prev_tokens[extra:]\n            pads = np.full(self.context_window - len(self.prev_tokens), pad)\n            new_toks[i] = np.concatenate([self.prev_tokens, toks[i].numpy(), pads])\n            new_tgt[\n                i, len(self.prev_tokens) : len(self.prev_tokens) + len(tgt[i])\n            ] = tgt[i]\n            start_idxs[i] = len(self.prev_tokens)\n            lengths[i] += len(self.prev_tokens)\n            self.prev_tokens = new_toks[i][new_toks[i] != pad][-self.context_window :]\n        sample[\"net_input\"][\"src_tokens\"] = torch.from_numpy(new_toks)\n        sample[\"target\"] = torch.from_numpy(new_tgt)\n        sample[\"start_indices\"] = start_idxs\n        return sample\n\n    def num_tokens(self, index):\n        return self.dataset.num_tokens(index)\n\n    def size(self, index):\n        return self.dataset.size(index)\n\n    def ordered_indices(self):\n        # NOTE we don't shuffle the data to retain access to the previous dataset elements\n        return np.arange(len(self.dataset))\n\n    @property\n    def supports_prefetch(self):\n        return getattr(self.dataset, \"supports_prefetch\", False)\n\n    def prefetch(self, indices):\n        return self.dataset.prefetch(indices)\n",
        "metaseq/data/monolingual_dataset.py": "# Copyright (c) Meta Platforms, Inc. and affiliates. All Rights Reserved.\n#\n# This source code is licensed under the MIT license found in the\n# LICENSE file in the root directory of this source tree.\n\nimport numpy as np\nimport torch\n\nfrom . import BaseDataset, data_utils\n\n\ndef collate(samples, pad_idx, eos_idx, fixed_pad_length=None, pad_to_bsz=None):\n    if len(samples) == 0:\n        return {}\n\n    def merge(key, is_list=False):\n        if is_list:\n            res = []\n            for i in range(len(samples[0][key])):\n                res.append(\n                    data_utils.collate_tokens(\n                        [s[key][i] for s in samples],\n                        pad_idx,\n                        eos_idx,\n                        left_pad=False,\n                        pad_to_length=fixed_pad_length,\n                        pad_to_bsz=pad_to_bsz,\n                    )\n                )\n            return res\n        else:\n            return data_utils.collate_tokens(\n                [s[key] for s in samples],\n                pad_idx,\n                eos_idx,\n                left_pad=False,\n                pad_to_length=fixed_pad_length,\n                pad_to_bsz=pad_to_bsz,\n            )\n\n    src_tokens = merge(\"source\")\n    if samples[0][\"target\"] is not None:\n        is_target_list = isinstance(samples[0][\"target\"], list)\n        target = merge(\"target\", is_target_list)\n    else:\n        target = src_tokens\n\n    return {\n        \"id\": torch.LongTensor([s[\"id\"] for s in samples]),\n        \"nsentences\": len(samples),\n        \"ntokens\": sum(len(s[\"source\"]) for s in samples),\n        \"net_input\": {\n            \"src_tokens\": src_tokens,\n            \"src_lengths\": torch.LongTensor([s[\"source\"].numel() for s in samples]),\n        },\n        \"target\": target,\n    }\n\n\nclass MonolingualDataset(BaseDataset):\n    \"\"\"\n    A wrapper around torch.utils.data.Dataset for monolingual data.\n    \"\"\"\n\n    def __init__(\n        self,\n        dataset,\n        sizes,\n        src_vocab,\n        tgt_vocab=None,\n        add_eos_for_other_targets=False,\n        shuffle=False,\n        add_bos_token=False,\n        fixed_pad_length=None,\n        pad_to_bsz=None,\n        src_lang_idx=None,\n        tgt_lang_idx=None,\n    ):\n        self.dataset = dataset\n        self.sizes = np.array(sizes)\n        self.vocab = src_vocab\n        self.tgt_vocab = tgt_vocab or src_vocab\n        self.add_eos_for_other_targets = add_eos_for_other_targets\n        self.shuffle = shuffle\n        self.add_bos_token = add_bos_token\n        self.fixed_pad_length = fixed_pad_length\n        self.pad_to_bsz = pad_to_bsz\n        self.src_lang_idx = src_lang_idx\n        self.tgt_lang_idx = tgt_lang_idx\n\n    def __getitem__(self, index):\n        # *future_target* is the original sentence\n        # *source* is shifted right by 1 (maybe left-padded with eos)\n        #\n        # Left-to-right language models should condition on *source* and\n        # predict *future_target*.\n        source, future_target, _ = self.dataset[index]\n        target = self._filter_vocab(future_target)\n\n        source, target = self._maybe_add_bos(source, target)\n        return {\"id\": index, \"source\": source, \"target\": target}\n\n    def __len__(self):\n        return len(self.dataset)\n\n    def _maybe_add_bos(self, source, target):\n        if self.add_bos_token:\n            # src_lang_idx and tgt_lang_idx are passed in for multilingual LM, with the\n            # first token being a lang_id token.\n            bos = self.src_lang_idx or self.vocab.bos()\n            source = torch.cat([source.new([bos]), source])\n            if target is not None:\n                tgt_bos = self.tgt_lang_idx or self.tgt_vocab.bos()\n                target = torch.cat([target.new([tgt_bos]), target])\n        return source, target\n\n    def num_tokens_vec(self, indices):\n        \"\"\"Return the number of tokens for a set of positions defined by indices.\n        This value is used to enforce ``--max-tokens`` during batching.\"\"\"\n        return self.sizes[indices]\n\n    def _filter_vocab(self, target):\n        if len(self.tgt_vocab) != len(self.vocab):\n\n            def _filter(target):\n                mask = target.ge(len(self.tgt_vocab))\n                if mask.any():\n                    target[mask] = self.tgt_vocab.unk()\n                return target\n\n            if isinstance(target, list):\n                return [_filter(t) for t in target]\n            return _filter(target)\n        return target\n\n    def collater(self, samples):\n        \"\"\"Merge a list of samples to form a mini-batch.\n\n        Args:\n            samples (List[dict]): samples to collate\n\n        Returns:\n            dict: a mini-batch with the following keys:\n\n                - `id` (LongTensor): example IDs in the original input order\n                - `ntokens` (int): total number of tokens in the batch\n                - `net_input` (dict): the input to the Model, containing keys:\n\n                  - `src_tokens` (LongTensor): a padded 2D Tensor of tokens in\n                    the source sentence of shape `(bsz, src_len)`. Padding will\n                    appear on the right.\n\n                - `target` (LongTensor): a padded 2D Tensor of tokens in the\n                  target sentence of shape `(bsz, tgt_len)`. Padding will appear\n                  on the right.\n        \"\"\"\n        return collate(\n            samples,\n            self.vocab.pad(),\n            self.vocab.eos(),\n            self.fixed_pad_length,\n            self.pad_to_bsz,\n        )\n\n    def num_tokens(self, index):\n        \"\"\"Return the number of tokens in a sample. This value is used to\n        enforce ``--max-tokens`` during batching.\"\"\"\n        return self.sizes[index]\n\n    def size(self, index):\n        \"\"\"Return an example's size as a float or tuple. This value is used when\n        filtering a dataset with ``--max-positions``.\"\"\"\n        return self.sizes[index]\n\n    def ordered_indices(self):\n        \"\"\"Return an ordered list of indices. Batches will be constructed based\n        on this order.\"\"\"\n        if self.shuffle:\n            order = [np.random.permutation(len(self))]\n        else:\n            order = [np.arange(len(self))]\n        order.append(self.sizes)\n        return np.lexsort(order)\n\n    @property\n    def supports_prefetch(self):\n        return getattr(self.dataset, \"supports_prefetch\", False)\n\n    def prefetch(self, indices):\n        self.dataset.prefetch(indices)\n",
        "metaseq/data/nested_dictionary_dataset.py": "# Copyright (c) Meta Platforms, Inc. and affiliates. All Rights Reserved.\n#\n# This source code is licensed under the MIT license found in the\n# LICENSE file in the root directory of this source tree.\n\nfrom collections import OrderedDict\n\nimport torch\nfrom torch.utils.data.dataloader import default_collate\n\nfrom . import BaseDataset\n\n\ndef _flatten(dico, prefix=None):\n    \"\"\"Flatten a nested dictionary.\"\"\"\n    new_dico = OrderedDict()\n    if isinstance(dico, dict):\n        prefix = prefix + \".\" if prefix is not None else \"\"\n        for k, v in dico.items():\n            if v is None:\n                continue\n            new_dico.update(_flatten(v, prefix + k))\n    elif isinstance(dico, list):\n        for i, v in enumerate(dico):\n            new_dico.update(_flatten(v, prefix + \".[\" + str(i) + \"]\"))\n    else:\n        new_dico = OrderedDict({prefix: dico})\n    return new_dico\n\n\ndef _unflatten(dico):\n    \"\"\"Unflatten a flattened dictionary into a nested dictionary.\"\"\"\n    new_dico = OrderedDict()\n    for full_k, v in dico.items():\n        full_k = full_k.split(\".\")\n        node = new_dico\n        for k in full_k[:-1]:\n            if k.startswith(\"[\") and k.endswith(\"]\"):\n                k = int(k[1:-1])\n            if k not in node:\n                node[k] = OrderedDict()\n            node = node[k]\n        node[full_k[-1]] = v\n    return new_dico\n\n\nclass NestedDictionaryDataset(BaseDataset):\n    def __init__(self, defn, sizes=None):\n        super().__init__()\n        self.defn = _flatten(defn)\n        self.sizes = [sizes] if not isinstance(sizes, (list, tuple)) else sizes\n\n        first = None\n        for v in self.defn.values():\n            if not isinstance(\n                v,\n                (\n                    BaseDataset,\n                    torch.utils.data.Dataset,\n                ),\n            ):\n                raise ValueError(\"Expected Dataset but found: {}\".format(v.__class__))\n            first = first or v\n            if len(v) > 0:\n                assert len(v) == len(first), \"dataset lengths must match\"\n\n        self._len = len(first)\n\n    def __getitem__(self, index):\n        return OrderedDict((k, ds[index]) for k, ds in self.defn.items())\n\n    def __len__(self):\n        return self._len\n\n    def collater(self, samples):\n        \"\"\"Merge a list of samples to form a mini-batch.\n\n        Args:\n            samples (List[dict]): samples to collate\n\n        Returns:\n            dict: a mini-batch suitable for forwarding with a Model\n        \"\"\"\n        if len(samples) == 0:\n            return {}\n        sample = OrderedDict()\n        for k, ds in self.defn.items():\n            try:\n                sample[k] = ds.collater([s[k] for s in samples])\n            except NotImplementedError:\n                sample[k] = default_collate([s[k] for s in samples])\n        return _unflatten(sample)\n\n    def num_tokens(self, index):\n        \"\"\"Return the number of tokens in a sample. This value is used to\n        enforce ``--max-tokens`` during batching.\"\"\"\n        return max(s[index] for s in self.sizes)\n\n    def size(self, index):\n        \"\"\"Return an example's size as a float or tuple. This value is used when\n        filtering a dataset with ``--max-positions``.\"\"\"\n        if len(self.sizes) == 1:\n            return self.sizes[0][index]\n        else:\n            return (s[index] for s in self.sizes)\n\n    @property\n    def supports_prefetch(self):\n        \"\"\"Whether this dataset supports prefetching.\"\"\"\n        return any(ds.supports_prefetch for ds in self.defn.values())\n\n    def prefetch(self, indices):\n        \"\"\"Prefetch the data required for this epoch.\"\"\"\n        for ds in self.defn.values():\n            if getattr(ds, \"supports_prefetch\", False):\n                ds.prefetch(indices)\n\n    def set_epoch(self, epoch):\n        # TODO(anj): Identify if we need this functionality for evals.\n        pass\n",
        "metaseq/data/numel_dataset.py": "# Copyright (c) Meta Platforms, Inc. and affiliates. All Rights Reserved.\n#\n# This source code is licensed under the MIT license found in the\n# LICENSE file in the root directory of this source tree.\n\nimport numpy as np\nimport torch\n\nfrom . import BaseWrapperDataset\n\n\nclass NumelDataset(BaseWrapperDataset):\n    def __init__(self, dataset, reduce=False):\n        super().__init__(dataset)\n        self.reduce = reduce\n\n    def __getitem__(self, index):\n        item = self.dataset[index]\n        if torch.is_tensor(item):\n            return torch.numel(item)\n        else:\n            return np.size(item)\n\n    def __len__(self):\n        return len(self.dataset)\n\n    def collater(self, samples):\n        if self.reduce:\n            return sum(samples)\n        else:\n            return torch.tensor(samples)\n",
        "metaseq/data/pad_dataset.py": "# Copyright (c) Meta Platforms, Inc. and affiliates. All Rights Reserved.\n#\n# This source code is licensed under the MIT license found in the\n# LICENSE file in the root directory of this source tree.\n\nimport math\n\nimport torch\n\nfrom metaseq.data import data_utils\nfrom . import BaseWrapperDataset\n\n\nclass PadDataset(BaseWrapperDataset):\n    def __init__(self, dataset, pad_idx, left_pad, pad_length=None):\n        super().__init__(dataset)\n        self.pad_idx = pad_idx\n        self.left_pad = left_pad\n        self.pad_length = pad_length\n\n    def collater(self, samples):\n        return data_utils.collate_tokens(\n            samples, self.pad_idx, left_pad=self.left_pad, pad_to_length=self.pad_length\n        )\n\n\nclass LeftPadDataset(PadDataset):\n    def __init__(self, dataset, pad_idx, pad_length=None):\n        super().__init__(dataset, pad_idx, left_pad=True, pad_length=pad_length)\n\n\nclass RightPadDataset(PadDataset):\n    def __init__(self, dataset, pad_idx, pad_length=None):\n        super().__init__(dataset, pad_idx, left_pad=False, pad_length=pad_length)\n\n\nclass MultiplePadDataset(BaseWrapperDataset):\n    \"\"\"\n    This class pads the given dataset to ensure that the padded size is a\n    multiple of the given `multiple`.\n\n    For instance,\n    MultiplePadDataset(\n        tgt_dataset, pad_idx=self.source_dictionary.pad(), multiple=8\n    )\n    would pad the tgt_dataset in multiples of 8.\n    \"\"\"\n\n    def __init__(self, dataset, pad_idx, multiple):\n        super().__init__(dataset)\n        self.pad_idx = pad_idx\n        self.multiple = multiple\n\n    def collater(self, samples):\n        max_len = max([s.size(0) for s in samples])\n        max_len_multiple = int(math.ceil(max_len / self.multiple)) * self.multiple\n\n        return data_utils.collate_tokens(\n            samples, self.pad_idx, left_pad=False, pad_to_length=max_len_multiple\n        )\n\n    def __getitem__(self, index):\n        l = len(self.dataset[index])\n        cur_block = []\n        cur_block.append(self.dataset[index])\n        cur_block_remain = int(math.ceil(l / self.multiple) * self.multiple)\n\n        cur_block_remain -= self.dataset[index].numel()\n        padding = cur_block[-1].new_full((cur_block_remain,), self.pad_idx)\n        cur_block.append(padding)\n\n        return torch.cat(cur_block)\n",
        "metaseq/data/partitioned_streaming_dataset.py": "# Copyright (c) Meta Platforms, Inc. and affiliates. All Rights Reserved.\n#\n# This source code is licensed under the MIT license found in the\n# LICENSE file in the root directory of this source tree.\n\nimport torch\n\n\nclass PartitionedStreamingDataset(torch.utils.data.IterableDataset):\n    \"\"\"Partition an IterableDataset and iterate over a single shard.\n\n    If **drop_last** is ``False``, then the iterator will yield ``None`` for\n    shards that don't have data.\n\n    Args:\n        dataset (~torch.utils.data.IterableDataset): dataset to partition\n        num_shards (int): number of ways to partition the dataset\n        shard_id (int): shard index to iterate over\n        drop_last (bool, optional): drop the last item (default: True)\n    \"\"\"\n\n    def __init__(\n        self,\n        dataset: torch.utils.data.IterableDataset,\n        num_shards: int,\n        shard_id: int,\n        drop_last: bool = True,\n    ):\n        super().__init__()\n        self.dataset = dataset\n        self.num_shards = num_shards\n        self.shard_id = shard_id\n        self.drop_last = drop_last\n\n        assert isinstance(dataset, torch.utils.data.IterableDataset)\n        assert num_shards > 0\n        assert shard_id >= 0 and shard_id < num_shards\n\n    def set_epoch(self, epoch):\n        if hasattr(self.dataset, \"set_epoch\"):\n            self.dataset.set_epoch(epoch)\n\n    def __iter__(self):\n        chunk = []\n        for item in self.dataset:\n            chunk.append(item)\n            if len(chunk) == self.num_shards:\n                yield chunk[self.shard_id]\n                chunk = []\n        if len(chunk) > 0 and not self.drop_last:\n            if self.shard_id < len(chunk):\n                yield chunk[self.shard_id]\n            else:\n                yield None\n",
        "metaseq/data/plasma_utils.py": "# Copyright (c) Meta Platforms, Inc. and affiliates. All Rights Reserved.\n#\n# This source code is licensed under the MIT license found in the\n# LICENSE file in the root directory of this source tree.\n\n\nimport hashlib\nimport json\nimport subprocess\nimport tempfile\nfrom typing import Hashable\n\ntry:\n    import pyarrow.plasma as plasma\n\n    PYARROW_AVAILABLE = True\nexcept ImportError:\n    plasma = None\n    PYARROW_AVAILABLE = False\n\n\nclass PlasmaArray:\n    \"\"\"\n    Wrapper around numpy arrays that automatically moves the data to shared\n    memory upon serialization. This is particularly helpful when passing numpy\n    arrays through multiprocessing, so that data is not unnecessarily\n    duplicated or pickled.\n    \"\"\"\n\n    def __init__(self, array):\n        super().__init__()\n        self.array = array\n        self.disable = array.nbytes < 134217728  # disable for arrays <128MB\n        self.object_id = None\n        self.path = None\n\n        # variables with underscores shouldn't be pickled\n        self._client = None\n        self._server = None\n        self._server_tmp = None\n        self._plasma = None\n\n    @property\n    def plasma(self):\n        if self._plasma is None and not self.disable:\n            self._plasma = plasma\n        return self._plasma\n\n    def start_server(self):\n        if self.plasma is None or self._server is not None:\n            return\n        assert self.object_id is None\n        assert self.path is None\n        self._server_tmp = tempfile.NamedTemporaryFile()\n        self.path = self._server_tmp.name\n        self._server = subprocess.Popen(\n            [\"plasma_store\", \"-m\", str(int(1.05 * self.array.nbytes)), \"-s\", self.path]\n        )\n\n    @property\n    def client(self):\n        if self._client is None:\n            assert self.path is not None\n            self._client = self.plasma.connect(self.path, num_retries=200)\n        return self._client\n\n    def __getstate__(self):\n        \"\"\"Called on pickle load\"\"\"\n        if self.plasma is None:\n            return self.__dict__\n        if self.object_id is None:\n            self.start_server()\n            self.object_id = self.client.put(self.array)\n        state = self.__dict__.copy()\n        del state[\"array\"]\n        state[\"_client\"] = None\n        state[\"_server\"] = None\n        state[\"_server_tmp\"] = None\n        state[\"_plasma\"] = None\n        return state\n\n    def __setstate__(self, state):\n        \"\"\"Called on pickle save\"\"\"\n        self.__dict__.update(state)\n        if self.plasma is None:\n            return\n        self.array = self.client.get(self.object_id)\n\n    def __del__(self):\n        if self._server is not None:\n            self._server.kill()\n            self._server = None\n            self._server_tmp.close()\n            self._server_tmp = None\n\n\nDEFAULT_PLASMA_PATH = \"/tmp/plasma\"\n\n\nclass PlasmaView:\n    \"\"\"Interface to write and read from shared memory. Whereas PlasmaArray writes to plasma on serialization,\n    PlasmaView writes to shared memory on instantiation.\"\"\"\n\n    def __init__(self, array, split_path: str, hash_data: Hashable, plasma_path=None):\n        \"\"\"\n        Args:\n            array: numpy array to store. This can be read with ``PlasmaView().array``\n            split_path: the path whence the data was read, used for hashing\n            hash_data: other metadata about the array that can be used to create a unique key.\n                as of writing, the 3 callers in ``TokenBlockDataset`` use::\n\n                    hash_data = ((block_size, document_sep_len, str(break_mode), len(dataset)), 0|1|2)\n\n\n        \"\"\"\n        assert PYARROW_AVAILABLE\n        assert split_path is not None\n        if plasma_path is None:\n            plasma_path = DEFAULT_PLASMA_PATH\n\n        self.path = plasma_path\n        self.split_path = split_path\n        self._client = None  # Initialize lazily for pickle. plasma clients should not be deep copied or serialized.\n        self._n = None\n\n        self.object_id = self.get_object_id(self.split_path, hash_data)\n        try:\n            self.client.put(array, object_id=self.object_id)\n        except plasma.PlasmaObjectExists:\n            pass\n\n    @property\n    def client(self):\n        if self._client is None:\n            self._client = plasma.connect(self.path, num_retries=200)\n        return self._client\n\n    @property\n    def array(self):\n        \"\"\"Fetch a read only view of an np.array, stored in plasma.\"\"\"\n        ret = self.client.get(self.object_id)\n        return ret\n\n    @staticmethod\n    def get_object_id(split_path: str, hash_data: Hashable):\n        \"\"\"Returns plasma.ObjectID from hashing split_path and object_num.\"\"\"\n        hash = hashlib.blake2b(bytes(split_path, \"utf-8\"), digest_size=20)\n        harg = json.dumps(hash_data).encode(\"utf-8\")\n        hash.update(harg)\n        return plasma.ObjectID(hash.digest())\n\n    def __getstate__(self):\n        \"\"\"Called on pickle save\"\"\"\n        self.disconnect()\n        state = self.__dict__.copy()\n        assert state[\"_client\"] is None\n        assert \"object_id\" in state\n        return state\n\n    def __setstate__(self, state):\n        \"\"\"Called on pickle load\"\"\"\n        self.__dict__.update(state)\n\n    def __del__(self):\n        self.disconnect()\n\n    def disconnect(self):\n        if self._client is not None:\n            self._client.disconnect()\n            self._client = None\n\n    def __len__(self):\n        \"\"\"Save reads by caching len\"\"\"\n        if self._n is None:\n            self._n = len(self.array)\n        return self._n\n\n\nGB100 = (1024**3) * 100\n\n\nclass PlasmaStore:\n    def __init__(self, path=DEFAULT_PLASMA_PATH, nbytes: int = GB100):\n        self.server = self.start(path, nbytes)\n\n    def __del__(self):\n        self.server.kill()\n\n    @staticmethod\n    def start(path=DEFAULT_PLASMA_PATH, nbytes: int = GB100) -> subprocess.Popen:\n        if not PYARROW_AVAILABLE:\n            raise ImportError(\"please run pip install pyarrow to use --use_plasma_view\")\n        # best practice is to allocate more space than we need. The limitation seems to be the size of /dev/shm\n        _server = subprocess.Popen([\"plasma_store\", \"-m\", str(nbytes), \"-s\", path])\n        plasma.connect(path, num_retries=200)  # If we can't connect we fail immediately\n        return _server\n",
        "metaseq/data/prepend_token_dataset.py": "# Copyright (c) Meta Platforms, Inc. and affiliates. All Rights Reserved.\n#\n# This source code is licensed under the MIT license found in the\n# LICENSE file in the root directory of this source tree.\n\nimport numpy as np\nimport torch\n\nfrom . import BaseWrapperDataset\n\n\nclass PrependTokenDataset(BaseWrapperDataset):\n    def __init__(self, dataset, token=None):\n        super().__init__(dataset)\n        self.token = token\n        if token is not None:\n            self._sizes = np.array(dataset.sizes) + 1\n        else:\n            self._sizes = dataset.sizes\n\n    def __getitem__(self, idx):\n        item = self.dataset[idx]\n        if self.token is not None:\n            item = torch.cat([item.new([self.token]), item])\n        return item\n\n    @property\n    def sizes(self):\n        return self._sizes\n\n    def num_tokens(self, index):\n        n = self.dataset.num_tokens(index)\n        if self.token is not None:\n            n += 1\n        return n\n\n    def size(self, index):\n        n = self.dataset.size(index)\n        if self.token is not None:\n            n += 1\n        return n\n",
        "metaseq/data/resampling_dataset.py": "# Copyright (c) Meta Platforms, Inc. and affiliates. All Rights Reserved.\n#\n# This source code is licensed under the MIT license found in the\n# LICENSE file in the root directory of this source tree.\n\nimport logging\n\nimport numpy as np\n\nfrom metaseq.data import BaseWrapperDataset, plasma_utils\n\nlogger = logging.getLogger(__name__)\n\n\nclass ResamplingDataset(BaseWrapperDataset):\n    \"\"\"Randomly samples from a given dataset at each epoch.\n\n    Sampling is done with or without replacement, depending on the \"replace\"\n    parameter.\n\n    Optionally, the epoch size can be rescaled. This is potentially desirable\n    to increase per-epoch coverage of the base dataset (since sampling with\n    replacement means that many items in the dataset will be left out). In the\n    case of sampling without replacement, size_ratio should be strictly less\n    than 1.\n\n    Args:\n        dataset (~torch.utils.data.Dataset): dataset on which to sample.\n        weights (List[float]): list of probability weights\n            (default: None, which corresponds to uniform sampling).\n        replace (bool): sampling mode; True for \"with replacement\", or False\n            for \"without replacement\" (default: True)\n        size_ratio (float): the ratio to subsample to; must be positive\n            (default: 1.0).\n        batch_by_size (bool): whether or not to batch by sequence length\n            (default: True).\n        seed (int): RNG seed to use (default: 0).\n        epoch (int): starting epoch number (default: 1).\n    \"\"\"\n\n    def __init__(\n        self,\n        dataset,\n        weights=None,\n        replace=True,\n        size_ratio=1.0,\n        batch_by_size=True,\n        seed=0,\n        epoch=1,\n    ):\n        super().__init__(dataset)\n\n        if weights is None:\n            self.weights = None\n\n        else:\n            assert len(weights) == len(dataset)\n            weights_arr = np.array(weights, dtype=np.float64)\n            weights_arr /= weights_arr.sum()\n            self.weights = plasma_utils.PlasmaArray(weights_arr)\n\n        self.replace = replace\n\n        assert size_ratio > 0.0\n        if not self.replace:\n            assert size_ratio <= 1.0\n        logger.info(f\"size ratio = {size_ratio}; replace = {self.replace}\")\n        self.size_ratio = float(size_ratio)\n        self.actual_size = np.ceil(len(dataset) * self.size_ratio).astype(int)\n\n        self.batch_by_size = batch_by_size\n        self.seed = seed\n\n        self._cur_epoch = None\n        self._cur_indices = None\n\n        self.set_epoch(epoch)\n\n    def __getitem__(self, index):\n        return self.dataset[self._cur_indices.array[index]]\n\n    def __len__(self):\n        return self.actual_size\n\n    @property\n    def sizes(self):\n        if isinstance(self.dataset.sizes, list):\n            return [s[self._cur_indices.array] for s in self.dataset.sizes]\n        return self.dataset.sizes[self._cur_indices.array]\n\n    def num_tokens(self, index):\n        return self.dataset.num_tokens(self._cur_indices.array[index])\n\n    def size(self, index):\n        return self.dataset.size(self._cur_indices.array[index])\n\n    def ordered_indices(self):\n        if self.batch_by_size:\n            order = [\n                np.arange(len(self)),\n                self.sizes,\n            ]  # No need to handle `self.shuffle == True`\n            return np.lexsort(order)\n        else:\n            return np.arange(len(self))\n\n    def prefetch(self, indices):\n        self.dataset.prefetch(self._cur_indices.array[indices])\n\n    @property\n    def can_reuse_epoch_itr_across_epochs(self):\n        return False\n\n    def set_epoch(self, epoch):\n        logger.debug(\"ResamplingDataset.set_epoch: {}\".format(epoch))\n        super().set_epoch(epoch)\n\n        if epoch == self._cur_epoch:\n            return\n\n        self._cur_epoch = epoch\n\n        # Generate a weighted sample of indices as a function of the\n        # random seed and the current epoch.\n\n        rng = np.random.RandomState(\n            [\n                42,  # magic number\n                self.seed % (2**32),  # global seed\n                self._cur_epoch,  # epoch index\n            ]\n        )\n        self._cur_indices = plasma_utils.PlasmaArray(\n            rng.choice(\n                len(self.dataset),\n                self.actual_size,\n                replace=self.replace,\n                p=(None if self.weights is None else self.weights.array),\n            )\n        )\n",
        "metaseq/data/shorten_dataset.py": "# Copyright (c) Meta Platforms, Inc. and affiliates. All Rights Reserved.\n#\n# This source code is licensed under the MIT license found in the\n# LICENSE file in the root directory of this source tree.\n\nimport numpy as np\n\nfrom metaseq.data import data_utils\nfrom . import BaseWrapperDataset\n\n\nclass TruncateDataset(BaseWrapperDataset):\n    \"\"\"Truncate a sequence by returning the first truncation_length tokens\"\"\"\n\n    def __init__(self, dataset, truncation_length):\n        super().__init__(dataset)\n        assert truncation_length is not None\n        self.truncation_length = truncation_length\n        self.dataset = dataset\n\n    def __getitem__(self, index):\n        item = self.dataset[index]\n        item_len = item.size(0)\n        if item_len > self.truncation_length:\n            item = item[: self.truncation_length]\n        return item\n\n    @property\n    def sizes(self):\n        return np.minimum(self.dataset.sizes, self.truncation_length)\n\n    def __len__(self):\n        return len(self.dataset)\n\n\nclass RandomCropDataset(TruncateDataset):\n    \"\"\"Truncate a sequence by returning a random crop of truncation_length tokens\"\"\"\n\n    def __init__(self, dataset, truncation_length, seed=1):\n        super().__init__(dataset, truncation_length)\n        self.seed = seed\n        self.epoch = 0\n\n    def set_epoch(self, epoch, **unused):\n        super().set_epoch(epoch)\n        self.epoch = epoch\n\n    def __getitem__(self, index):\n        with data_utils.numpy_seed(self.seed, self.epoch, index):\n            item = self.dataset[index]\n            item_len = item.size(0)\n            excess = item_len - self.truncation_length\n            if excess > 0:\n                start_idx = np.random.randint(0, excess)\n                item = item[start_idx : start_idx + self.truncation_length]\n            return item\n\n\ndef maybe_shorten_dataset(\n    dataset,\n    split,\n    shorten_data_split_list,\n    shorten_method,\n    tokens_per_sample,\n    seed,\n):\n    truncate_split = (\n        split in shorten_data_split_list.split(\",\") or len(shorten_data_split_list) == 0\n    )\n    if shorten_method == \"truncate\" and truncate_split:\n        dataset = TruncateDataset(dataset, tokens_per_sample)\n    elif shorten_method == \"random_crop\" and truncate_split:\n        dataset = RandomCropDataset(dataset, tokens_per_sample, seed)\n    return dataset\n",
        "metaseq/data/sort_dataset.py": "# Copyright (c) Meta Platforms, Inc. and affiliates. All Rights Reserved.\n#\n# This source code is licensed under the MIT license found in the\n# LICENSE file in the root directory of this source tree.\n\nimport numpy as np\n\nfrom . import BaseWrapperDataset\n\n\nclass SortDataset(BaseWrapperDataset):\n    def __init__(self, dataset, sort_order):\n        super().__init__(dataset)\n        if not isinstance(sort_order, (list, tuple)):\n            sort_order = [sort_order]\n        self.sort_order = sort_order\n\n        assert all(len(so) == len(dataset) for so in sort_order)\n\n    def ordered_indices(self):\n        return np.lexsort(self.sort_order)\n",
        "metaseq/data/streaming_shuffle_dataset.py": "# Copyright (c) Meta Platforms, Inc. and affiliates. All Rights Reserved.\n#\n# This source code is licensed under the MIT license found in the\n# LICENSE file in the root directory of this source tree.\n\nimport numpy as np\nimport torch\n\nfrom metaseq.data import data_utils\n\n\nclass StreamingShuffleDataset(torch.utils.data.IterableDataset):\n    \"\"\"Shuffle a dataset across epochs.\n\n    Note that :func:`set_epoch` must be called before the first iteration.\n\n    Args:\n        dataset (~torch.utils.data.Dataset): dataset to shuffle\n        seed (int): iterate over the underlying dataset in random order using\n            this random seed\n    \"\"\"\n\n    def __init__(self, dataset: torch.utils.data.Dataset, seed: int):\n        super().__init__()\n        self.dataset = dataset\n        self.seed = seed\n\n        assert len(dataset) > 0\n\n        self.indices = None\n        self.worker_offset = 0\n\n    def set_epoch(self, epoch):\n        # shuffle the dataset according to the seed argument and epoch\n        seed = int(hash((self.seed, epoch)) % 1e6)\n        with data_utils.numpy_seed(seed):\n            self.indices = np.random.permutation(len(self.dataset))\n\n        if hasattr(self.dataset, \"set_epoch\"):\n            self.dataset.set_epoch(epoch)\n\n    def __iter__(self):\n        assert (\n            self.indices is not None\n        ), \"must call StreamingShuffleDataset.set_epoch before iteration\"\n        worker_info = torch.utils.data.get_worker_info()\n        if worker_info is not None and worker_info.num_workers > 1:\n            chunks = np.array_split(self.indices, worker_info.num_workers)\n            worker_id = (worker_info.id + self.worker_offset) % worker_info.num_workers\n            indices = chunks[worker_id]\n        else:\n            indices = self.indices\n\n        for idx in indices:\n            yield self.dataset[idx]\n",
        "metaseq/data/streaming_src_tgt_dataset.py": "# Copyright (c) Meta Platforms, Inc. and affiliates. All Rights Reserved.\n#\n# This source code is licensed under the MIT license found in the\n# LICENSE file in the root directory of this source tree.\n\nfrom typing import Optional\n\nimport numpy as np\nimport torch\nimport math\n\n\nclass StreamingSrcTgtDataset(torch.utils.data.IterableDataset):\n    \"\"\"View an IterableDataset of tokens as a 1D tensor and chunk into blocks.\n\n    This dataset can only be iterated over once.\n\n    Args:\n        dataset (~torch.utils.data.IterableDataset): dataset to chunk\n        block_size (int): maximum block size\n        break_mode (str, optional): Mode used for breaking tokens. Values can\n            be one of:\n            - 'none': break tokens into equally sized blocks (up to block_size)\n        drop_last (bool, optional): drop the last item (default: True)\n        padding_idx (int, optional): index to use for padding symbols\n            (required if *drop_last* is ``False``)\n        shuffle_buffer_size (int, optional): buffer this many items and shuffle\n            using the provided *seed*; default value is 1, so no shuffling is\n            performed. This can be adjusted dynamically after initialization,\n            but only before iteration has begun.\n        seed (int, optional): seed for shuffling\n    \"\"\"\n\n    def __init__(\n        self,\n        dataset: torch.utils.data.IterableDataset,\n        block_size: int,\n        break_mode: str = \"none\",\n        drop_last: Optional[bool] = True,\n        padding_idx: Optional[int] = None,\n        shuffle_buffer_size: int = 1,\n        seed: Optional[int] = None,\n    ):\n        super().__init__()\n        self.dataset = dataset\n        self.block_size = block_size\n        self.break_mode = break_mode\n        self.drop_last = drop_last\n        self.padding_idx = padding_idx\n        self.shuffle_buffer_size = shuffle_buffer_size\n        self.seed = seed\n        if break_mode == \"none\" or break_mode == \"complete\":\n            self.block_iterator = yield_src_tgt_blocks\n        elif break_mode == \"eos_pad_8\":  # Single example per sequence\n            self.block_iterator = yield_src_tgt_single_sentences_pad_8\n        else:\n            raise NotImplementedError(f\"Unknown break mode: {break_mode}\")\n\n        if not drop_last and padding_idx is None:\n            raise ValueError(\"padding_idx is required when drop_last is False\")\n\n        assert shuffle_buffer_size >= 1\n        if shuffle_buffer_size > 1 and seed is None:\n            raise ValueError(\"seed is required when shuffle_buffer_size > 1\")\n\n        self._started_iteration = False\n\n    def set_epoch(self, epoch):\n        if hasattr(self.dataset, \"set_epoch\"):\n            self.dataset.set_epoch(epoch)\n\n    def set_shuffle_buffer_size(self, new_shuffle_buffer_size):\n        assert not self._started_iteration\n        self.shuffle_buffer_size = new_shuffle_buffer_size\n\n    def __iter__(self):\n        assert not self._started_iteration\n        self.started_iteration = True\n\n        block_itr = self.block_iterator(\n            self.dataset,\n            self.block_size,\n            self.drop_last,\n            self.padding_idx,\n        )\n\n        if self.seed is not None:\n            # add a random offset (2273) to the given seed to decouple this RNG\n            # from any other RNG instances elsewhere\n            self.rng = np.random.default_rng(2273 + self.seed)\n        else:\n            self.rng = None\n\n        buffer = []\n\n        def get_next_item_and_replace_in_buffer(replacement_item):\n            # return a random item from the buffer and replace with a new item\n            idx = self.rng.integers(len(buffer)) if self.rng is not None else 0\n            item = buffer[idx]\n            if replacement_item is not None:\n                buffer[idx] = replacement_item\n            else:\n                buffer.pop(idx)\n            return item\n\n        for block in block_itr:\n            if len(buffer) < self.shuffle_buffer_size:\n                # initially fill the buffer to the requested size\n                buffer.append(block)\n            else:\n                # return random block from the buffer and replace with new block\n                yield get_next_item_and_replace_in_buffer(block)\n\n        # clear buffer of any remaining items\n        while buffer:\n            yield get_next_item_and_replace_in_buffer(None)\n\n\ndef yield_src_tgt_blocks(iterable, block_size, drop_last, padding_idx):\n    \"\"\"Packs multiple examples together in a block\"\"\"\n    cur_src_block = []\n    cur_src_block_ids = []\n    cur_tgt_block = []\n    cur_block_remain = block_size\n    for idx, (src, tgt) in enumerate(iterable):\n        if src.numel() > block_size:\n            # truncate right side\n            # TODO: Switch this to left truncate so that the target isnt ever truncated\n            src = src[:block_size]\n            tgt = tgt[:block_size]\n\n        if src.numel() > cur_block_remain:\n            padding = cur_src_block[-1].new_full((cur_block_remain,), padding_idx)\n            cur_src_block.append(padding)\n            cur_tgt_block.append(padding)\n            src_block = torch.cat(cur_src_block)\n            tgt_block = torch.cat(cur_tgt_block)\n            yield {\n                \"ids\": torch.LongTensor(cur_src_block_ids),\n                \"src_block\": src_block,\n                \"tgt_block\": tgt_block,\n            }\n\n            cur_src_block = []\n            cur_src_block_ids = []\n            cur_tgt_block = []\n            cur_block_remain = block_size\n\n        cur_src_block.append(src)\n        cur_src_block_ids.append(idx)\n        cur_tgt_block.append(tgt)\n        cur_block_remain -= src.numel()\n        assert cur_block_remain >= 0\n\n    if not drop_last and len(cur_src_block) > 0:\n        if cur_block_remain > 0:\n            padding = cur_src_block[-1].new_full((cur_block_remain,), padding_idx)\n            cur_src_block.append(padding)\n            cur_tgt_block.append(padding)\n        src_block = torch.cat(cur_src_block)\n        tgt_block = torch.cat(cur_tgt_block)\n        assert src_block.numel() == block_size\n        yield {\n            \"ids\": torch.LongTensor(cur_src_block_ids),\n            \"src_block\": src_block,\n            \"tgt_block\": tgt_block,\n        }\n\n\ndef yield_src_tgt_single_sentences_pad_8(iterable, block_size, drop_last, padding_idx):\n    \"\"\"Mimics sample-break-mode eos i.e. 1 example per sequence without any packing.\n    When multiple examples are packed into a single sequence, example tokens would attend\n    to tokens in neighbouring examples, which may be undesirable. This mode can\n    avoid that. Since there is no packing, this mode is considerably slower.\n    We round up the example length to a multiple of 8, pad to this length and\n    return the example as is, without packing, truncating to block_size in cases of\n    very long examples.\n    \"\"\"\n\n    for idx, (src, tgt) in enumerate(iterable):\n        cur_src_block = []\n        cur_src_block_ids = []\n        cur_tgt_block = []\n        if src.numel() > block_size:\n            # truncate right side\n            # TODO: Enable left side truncation\n            src = src[:block_size]\n            tgt = tgt[:block_size]\n\n        cur_src_block.append(src)\n        cur_src_block_ids.append(idx)\n        cur_tgt_block.append(tgt)\n\n        # We round up to a multiple of 8 + 1, because later on\n        # one element is removed for src/target tensor creation\n        # which brings it back to a multiple of 8. block_size is\n        # already passed with + 1 included.\n        # cur_block_remain = int(min(math.pow(2, math.ceil(math.log(src.numel(), 2))) + 1, block_size))\n        cur_block_remain = min(int(math.ceil(src.numel() / 8)) * 8 + 1, block_size)\n        cur_block_remain -= src.numel()\n        padding = cur_src_block[-1].new_full((cur_block_remain,), padding_idx)\n        cur_src_block.append(padding)\n        cur_tgt_block.append(padding)\n\n        yield {\n            \"ids\": torch.LongTensor(cur_src_block_ids),\n            \"src_block\": torch.cat(cur_src_block),\n            \"tgt_block\": torch.cat(cur_tgt_block),\n        }\n",
        "metaseq/data/streaming_token_block_dataset.py": "# Copyright (c) Meta Platforms, Inc. and affiliates. All Rights Reserved.\n#\n# This source code is licensed under the MIT license found in the\n# LICENSE file in the root directory of this source tree.\n\nimport math\nfrom typing import Optional\n\nimport numpy as np\nimport torch\n\n\nclass StreamingTokenBlockDataset(torch.utils.data.IterableDataset):\n    \"\"\"View an IterableDataset of tokens as a 1D tensor and chunk into blocks.\n\n    This dataset can only be iterated over once.\n\n    Args:\n        dataset (~torch.utils.data.IterableDataset): dataset to chunk\n        block_size (int): maximum block size\n        break_mode (str, optional): Mode used for breaking tokens. Values can\n            be one of:\n            - 'none': break tokens into equally sized blocks (up to block_size)\n        drop_last (bool, optional): drop the last item (default: False)\n        padding_idx (int, optional): index to use for padding symbols\n            (required if *drop_last* is ``True``)\n        shuffle_buffer_size (int, optional): buffer this many items and shuffle\n            using the provided *seed*; default value is 1, so no shuffling is\n            performed. This can be adjusted dynamically after initialization,\n            but only before iteration has begun.\n        seed (int, optional): seed for shuffling\n    \"\"\"\n\n    def __init__(\n        self,\n        dataset: torch.utils.data.IterableDataset,\n        block_size: int,\n        break_mode: str = \"none\",\n        drop_last: Optional[bool] = True,\n        padding_idx: Optional[int] = None,\n        shuffle_buffer_size: int = 1,\n        seed: Optional[int] = None,\n    ):\n        super().__init__()\n        self.dataset = dataset\n        self.block_size = block_size\n        self.break_mode = break_mode\n        self.drop_last = drop_last\n        self.padding_idx = padding_idx\n        self.shuffle_buffer_size = shuffle_buffer_size\n        self.seed = seed\n        if break_mode == \"none\":\n            self.block_iterator = yield_token_blocks\n        elif break_mode == \"eos_pad_8\":\n            self.block_iterator = yield_single_sentences_pad_8\n        elif break_mode == \"complete\":\n            self.block_iterator = yield_doc_blocks\n        else:\n            raise ValueError(\n                f'Invalid value for break_mode = {break_mode}. Available options are \"none\", \"eos_pad_8\" or \"complete\".'\n            )\n\n        if not drop_last and padding_idx is None:\n            raise ValueError(\"padding_idx is required when drop_last is False\")\n\n        assert shuffle_buffer_size >= 1\n        if shuffle_buffer_size > 1 and seed is None:\n            raise ValueError(\"seed is required when shuffle_buffer_size > 1\")\n\n        # if break_mode != \"none\": raise NotImplementedError\n\n        self._started_iteration = False\n\n    def set_epoch(self, epoch):\n        if hasattr(self.dataset, \"set_epoch\"):\n            self.dataset.set_epoch(epoch)\n\n    def set_shuffle_buffer_size(self, new_shuffle_buffer_size):\n        assert not self._started_iteration\n        self.shuffle_buffer_size = new_shuffle_buffer_size\n\n    def __iter__(self):\n        assert not self._started_iteration\n        self.started_iteration = True\n\n        block_itr = self.block_iterator(\n            self.dataset,\n            self.block_size,\n            self.drop_last,\n            self.padding_idx,\n        )\n\n        if self.seed is not None:\n            # add a random offset (2273) to the given seed to decouple this RNG\n            # from any other RNG instances elsewhere\n            rng = np.random.default_rng(2273 + self.seed)\n        else:\n            rng = None\n\n        buffer = []\n\n        def get_next_item_and_replace_in_buffer(replacement_item):\n            # return a random item from the buffer and replace with a new item\n            nonlocal rng\n            idx = rng.integers(len(buffer)) if rng is not None else 0\n            item = buffer[idx]\n            if replacement_item is not None:\n                buffer[idx] = replacement_item\n            else:\n                buffer.pop(idx)\n            return item\n\n        for block in block_itr:\n            if len(buffer) < self.shuffle_buffer_size:\n                # initially fill the buffer to the requested size\n                buffer.append(block)\n            else:\n                # return random block from the buffer and replace with new block\n                yield get_next_item_and_replace_in_buffer(block)\n\n        # clear buffer of any remaining items\n        while buffer:\n            yield get_next_item_and_replace_in_buffer(None)\n\n\ndef yield_single_sentences_pad_8(iterable, block_size, drop_last, padding_idx):\n    \"\"\"Mimics sample-break-mode eos i.e. 1 example per sequence without any packing.\n    When multiple examples are packed into a single sequence, example tokens would attend\n    to tokens in neighbouring examples, which may be undesirable. This mode can\n    avoid that. Since there is no packing, this mode is considerably slower.\n    We round up the example length to a multiple of 8, pad to this length and\n    return the example as is, without packing, truncating to block_size in cases of\n    very long examples.\n    \"\"\"\n    for idx, item in enumerate(iterable):\n        cur_block = []\n        cur_block_ids = []\n        if item.numel() > block_size:\n            # truncate right side\n            # TODO: Enable left side truncation\n            item = item[:block_size]\n\n        cur_block.append(item)\n\n        # We round up to a multiple of 8 + 1, because later on\n        # one element is removed for src/target tensor creation\n        # which brings it back to a multiple of 8. block_size is\n        # already passed with + 1 included.\n        cur_block_remain = min(int(math.ceil(item.numel() / 8)) * 8 + 1, block_size)\n        cur_block_remain -= item.numel()\n        padding = cur_block[-1].new_full((cur_block_remain,), padding_idx)\n        cur_block.append(padding)\n\n        cur_block_ids.append(idx)\n        yield {\n            \"ids\": torch.LongTensor(cur_block_ids),\n            \"block\": torch.cat(cur_block),\n        }\n\n\ndef yield_doc_blocks(iterable, block_size, drop_last, padding_idx):\n    \"\"\"Mimics sample-break-mode complete\"\"\"\n    cur_block = []\n    cur_block_ids = []\n    cur_block_remain = block_size\n    for idx, item in enumerate(iterable):\n        if item.numel() > block_size:\n            # truncate right side\n            item = item[:block_size]\n\n        if item.numel() > cur_block_remain:\n            padding = cur_block[-1].new_full((cur_block_remain,), padding_idx)\n            cur_block.append(padding)\n            block = torch.cat(cur_block)\n            yield {\n                \"ids\": torch.LongTensor(cur_block_ids),\n                \"block\": block,\n            }\n\n            cur_block = []\n            cur_block_ids = []\n            cur_block_remain = block_size\n\n        cur_block.append(item)\n        cur_block_ids.append(idx)\n        cur_block_remain -= item.numel()\n        assert cur_block_remain >= 0\n\n    if not drop_last and len(cur_block) > 0:\n        if cur_block_remain > 0:\n            padding = cur_block[-1].new_full((cur_block_remain,), padding_idx)\n            cur_block.append(padding)\n        block = torch.cat(cur_block)\n        assert block.numel() == block_size\n        yield {\n            \"ids\": torch.LongTensor(cur_block_ids),\n            \"block\": block,\n        }\n\n\ndef yield_token_blocks(iterable, block_size, drop_last, padding_idx):\n    \"\"\"Sample break mode = None. (Pre-Training default).\"\"\"\n    cur_block = []\n    cur_block_ids = []\n    cur_block_remain = block_size\n    for idx, item in enumerate(iterable):\n        cur_block_ids.append(idx)\n        while item.numel() > 0:\n            num_to_take = min(item.numel(), cur_block_remain)\n\n            cur_block.append(item[:num_to_take])\n            item = item[num_to_take:]  # remainder\n\n            cur_block_remain -= num_to_take\n            assert cur_block_remain >= 0\n\n            if cur_block_remain == 0:\n                block = torch.cat(cur_block)\n                assert block.numel() == block_size\n                yield {\n                    \"ids\": torch.LongTensor(cur_block_ids),\n                    \"block\": block[:block_size],\n                }\n\n                cur_block = []\n                cur_block_ids = []\n                cur_block_remain = block_size\n\n    if not drop_last and len(cur_block) > 0:\n        if cur_block_remain > 0:\n            padding = cur_block[-1].new_full((cur_block_remain,), padding_idx)\n            cur_block.append(padding)\n        block = torch.cat(cur_block)\n        assert block.numel() == block_size\n        yield {\n            \"ids\": torch.LongTensor(cur_block_ids),\n            \"block\": block,\n        }\n",
        "metaseq/data/strip_token_dataset.py": "# Copyright (c) Meta Platforms, Inc. and affiliates. All Rights Reserved.\n#\n# This source code is licensed under the MIT license found in the\n# LICENSE file in the root directory of this source tree.\n\nfrom . import BaseWrapperDataset\n\n\nclass StripTokenDataset(BaseWrapperDataset):\n    def __init__(self, dataset, id_to_strip):\n        super().__init__(dataset)\n        self.id_to_strip = id_to_strip\n\n    def __getitem__(self, index):\n        item = self.dataset[index]\n        while len(item) > 0 and item[-1] == self.id_to_strip:\n            item = item[:-1]\n        while len(item) > 0 and item[0] == self.id_to_strip:\n            item = item[1:]\n        return item\n",
        "metaseq/data/token_block_dataset.py": "# Copyright (c) Meta Platforms, Inc. and affiliates. All Rights Reserved.\n#\n# This source code is licensed under the MIT license found in the\n# LICENSE file in the root directory of this source tree.\n\nfrom typing import Tuple\n\nimport numpy as np\nimport torch\n\nfrom metaseq.data import BaseDataset, plasma_utils\nfrom metaseq.data.indexed_dataset import best_fitting_int_dtype\n\n\nclass TokenBlockDataset(BaseDataset):\n    \"\"\"Break a Dataset of tokens into blocks.\n\n    Args:\n        dataset (~torch.utils.data.Dataset): dataset to break into blocks\n        sizes (List[int]): sentence lengths (required for 'complete' and 'eos')\n        block_size (int, optional): maximum block size (ignored in 'eos' break mode)\n        break_mode (str, optional): Mode used for breaking tokens. Values can\n            be one of:\n            - 'none': break tokens into equally sized blocks (up to block_size)\n            - 'complete': break tokens into blocks (up to block_size) such that\n                blocks contains complete sentences, although block_size may be\n                exceeded if some sentences exceed block_size\n            - 'complete_doc': similar to 'complete' mode, but do not\n                cross document boundaries\n            - 'eos': each block contains one sentence (block_size is ignored)\n        include_targets (bool, optional): return next tokens as targets\n            (default: False).\n        document_sep_len (int, optional): document separator size (required for\n            'complete_doc' break mode). Typically 1 if the sentences have eos\n            and 0 otherwise.\n    \"\"\"\n\n    def __init__(\n        self,\n        dataset,\n        sizes,\n        block_size,\n        pad,\n        eos,\n        break_mode=None,\n        include_targets=False,\n        document_sep_len=1,\n        use_plasma_view=False,\n        split_path=None,\n        plasma_path=None,\n    ):\n        super().__init__()\n        self.dataset = dataset\n        self.pad = pad\n        self.eos = eos\n        self.include_targets = include_targets\n\n        assert len(dataset) > 0\n\n        assert len(dataset) == len(sizes)\n        _sizes, block_to_dataset_index, slice_indices = self._build_slice_indices(\n            sizes, break_mode, document_sep_len, block_size\n        )\n        if use_plasma_view:\n            plasma_id = (block_size, document_sep_len, str(break_mode), len(dataset))\n            self._slice_indices = plasma_utils.PlasmaView(\n                slice_indices, split_path, (plasma_id, 0), plasma_path=plasma_path\n            )\n            self._sizes = plasma_utils.PlasmaView(\n                _sizes, split_path, (plasma_id, 1), plasma_path=plasma_path\n            )\n            self._block_to_dataset_index = plasma_utils.PlasmaView(\n                block_to_dataset_index,\n                split_path,\n                (plasma_id, 2),\n                plasma_path=plasma_path,\n            )\n        else:\n            self._slice_indices = plasma_utils.PlasmaArray(slice_indices)\n            self._sizes = plasma_utils.PlasmaArray(_sizes)\n            self._block_to_dataset_index = plasma_utils.PlasmaArray(\n                block_to_dataset_index\n            )\n\n    @staticmethod\n    def _build_slice_indices(\n        sizes, break_mode, document_sep_len, block_size\n    ) -> Tuple[np.ndarray]:\n        \"\"\"Use token_block_utils_fast to build arrays for indexing into self.dataset\"\"\"\n        try:\n            from metaseq.data.token_block_utils_fast import (\n                _get_slice_indices_fast,\n                _get_block_to_dataset_index_fast,\n            )\n        except ImportError:\n            raise ImportError(\n                \"Please build Cython components with: `pip install --editable .` \"\n                \"or `python setup.py build_ext --inplace`\"\n            )\n\n        if isinstance(sizes, list):\n            sizes = np.array(sizes, dtype=np.int64)\n        else:\n            if torch.is_tensor(sizes):\n                sizes = sizes.numpy()\n            sizes = sizes.astype(np.int64)\n\n        break_mode = break_mode if break_mode is not None else \"none\"\n\n        # For \"eos\" break-mode, block_size is not required parameters.\n        if break_mode == \"eos\" and block_size is None:\n            block_size = 0\n\n        slice_indices = _get_slice_indices_fast(\n            sizes, str(break_mode), block_size, document_sep_len\n        )\n        _sizes = slice_indices[:, 1] - slice_indices[:, 0]\n\n        # build index mapping block indices to the underlying dataset indices\n        if break_mode == \"eos\":\n            # much faster version for eos break mode\n            block_to_dataset_index = np.stack(\n                [\n                    np.arange(len(sizes)),  # starting index in dataset\n                    np.zeros(\n                        len(sizes), dtype=np.compat.long\n                    ),  # starting offset within starting index\n                    np.arange(len(sizes)),  # ending index in dataset\n                ],\n                1,\n            )\n        else:\n            block_to_dataset_index = _get_block_to_dataset_index_fast(\n                sizes,\n                slice_indices,\n            )\n        size_dtype = np.uint16 if block_size < 65535 else np.uint32\n        num_tokens = slice_indices[-1].max()\n        slice_indices_dtype = best_fitting_int_dtype(num_tokens)\n        slice_indices = slice_indices.astype(slice_indices_dtype)\n        _sizes = _sizes.astype(size_dtype)\n        block_to_dataset_index = block_to_dataset_index.astype(slice_indices_dtype)\n        return _sizes, block_to_dataset_index, slice_indices\n\n    @property\n    def slice_indices(self):\n        return self._slice_indices.array\n\n    @property\n    def sizes(self):\n        return self._sizes.array\n\n    @property\n    def block_to_dataset_index(self):\n        return self._block_to_dataset_index.array\n\n    def attr(self, attr: str, index: int):\n        start_ds_idx, _, _ = self.block_to_dataset_index[index]\n        return self.dataset.attr(attr, start_ds_idx)\n\n    def __getitem__(self, index):\n        start_ds_idx, start_offset, end_ds_idx = self.block_to_dataset_index[index]\n\n        buffer = torch.cat(\n            [self.dataset[idx] for idx in range(start_ds_idx, end_ds_idx + 1)]\n        )\n        slice_s, slice_e = self.slice_indices[index]\n        length = slice_e - slice_s\n        s, e = start_offset, start_offset + length\n        item = buffer[s:e]\n\n        if self.include_targets:\n            # *target* is the original sentence (=item)\n            # *source* is shifted right by 1 (maybe left-padded with eos)\n            # *past_target* is shifted right by 2 (left-padded as needed)\n            if s == 0:\n                source = torch.cat([item.new([self.eos]), buffer[0 : e - 1]])\n                past_target = torch.cat(\n                    [item.new([self.pad, self.eos]), buffer[0 : e - 2]]\n                )\n            else:\n                source = buffer[s - 1 : e - 1]\n                if s == 1:\n                    past_target = torch.cat([item.new([self.eos]), buffer[0 : e - 2]])\n                else:\n                    past_target = buffer[s - 2 : e - 2]\n\n            return source, item, past_target\n\n        return item\n\n    def __len__(self):\n        return len(self.slice_indices)\n\n    @property\n    def supports_prefetch(self):\n        return getattr(self.dataset, \"supports_prefetch\", False)\n\n    def prefetch(self, indices):\n        self.dataset.prefetch(\n            {\n                ds_idx\n                for index in indices\n                for start_ds_idx, _, end_ds_idx in [self.block_to_dataset_index[index]]\n                for ds_idx in range(start_ds_idx, end_ds_idx + 1)\n            }\n        )\n",
        "metaseq/dataclass/__init__.py": "# Copyright (c) Meta Platforms, Inc. and affiliates. All Rights Reserved.\n#\n# This source code is licensed under the MIT license found in the\n# LICENSE file in the root directory of this source tree.\n\nfrom .configs import MetaseqDataclass\nfrom .constants import ChoiceEnum\n\n\n__all__ = [\n    \"MetaseqDataclass\",\n    \"ChoiceEnum\",\n]\n",
        "metaseq/dataclass/configs.py": "# Copyright (c) Meta Platforms, Inc. and affiliates. All Rights Reserved.\n#\n# This source code is licensed under the MIT license found in the\n# LICENSE file in the root directory of this source tree.\n\nimport sys\nfrom dataclasses import _MISSING_TYPE, dataclass, field\nfrom typing import Any, List, Optional\n\nimport torch\nfrom omegaconf import II, MISSING\n\nfrom metaseq.dataclass.constants import (\n    DATASET_IMPL_CHOICES,\n    DDP_BACKEND_CHOICES,\n    TASK_DDP_BACKEND_CHOICES,\n    LOG_FORMAT_CHOICES,\n    CLIP_GRAD_NORM_TYPE_CHOICES,\n)\n\n\n@dataclass\nclass MetaseqDataclass:\n    \"\"\"metaseq base dataclass that supported fetching attributes and metas\"\"\"\n\n    _name: Optional[str] = None\n\n    @staticmethod\n    def name():\n        return None\n\n    def positional_args(self):\n        return [\"data\"]\n\n    def _get_all_attributes(self) -> List[str]:\n        return [k for k in self.__dataclass_fields__.keys()]\n\n    def _get_meta(\n        self, attribute_name: str, meta: str, default: Optional[Any] = None\n    ) -> Any:\n        return self.__dataclass_fields__[attribute_name].metadata.get(meta, default)\n\n    def _get_name(self, attribute_name: str) -> str:\n        return self.__dataclass_fields__[attribute_name].name\n\n    def _get_default(self, attribute_name: str) -> Any:\n        if hasattr(self, attribute_name):\n            if str(getattr(self, attribute_name)).startswith(\"${\"):\n                return str(getattr(self, attribute_name))\n            elif str(self.__dataclass_fields__[attribute_name].default).startswith(\n                \"${\"\n            ):\n                return str(self.__dataclass_fields__[attribute_name].default)\n            elif (\n                getattr(self, attribute_name)\n                != self.__dataclass_fields__[attribute_name].default\n            ):\n                return getattr(self, attribute_name)\n\n        f = self.__dataclass_fields__[attribute_name]\n        if not isinstance(f.default_factory, _MISSING_TYPE):\n            return f.default_factory()\n        return f.default\n\n    def _get_type(self, attribute_name: str) -> Any:\n        return self.__dataclass_fields__[attribute_name].type\n\n    def _get_help(self, attribute_name: str) -> Any:\n        return self._get_meta(attribute_name, \"help\")\n\n    def _get_argparse_const(self, attribute_name: str) -> Any:\n        return self._get_meta(attribute_name, \"argparse_const\")\n\n    def _get_argparse_alias(self, attribute_name: str) -> Any:\n        return self._get_meta(attribute_name, \"argparse_alias\")\n\n    def _get_choices(self, attribute_name: str) -> Any:\n        return self._get_meta(attribute_name, \"choices\")\n\n\n@dataclass\nclass CommonConfig(MetaseqDataclass):\n    # This is the core dataclass including common parameters shared by all\n    # different jobs. Please append your params to other dataclasses if they\n    # were used for a particular purpose or task, such as those dedicated for\n    # `distributed training`, `optimization`, etc.\n    log_interval: int = field(\n        default=100,\n        metadata={\n            \"help\": \"log progress every N batches (when progress bar is disabled)\"\n        },\n    )\n    log_format: Optional[LOG_FORMAT_CHOICES] = field(\n        default=None, metadata={\"help\": \"log format to use\"}\n    )\n    log_file: Optional[str] = field(\n        default=None, metadata={\"help\": \"log file to copy metrics to.\"}\n    )\n    tensorboard_logdir: Optional[str] = field(\n        default=None,\n        metadata={\n            \"help\": \"path to save logs for tensorboard, should match --logdir \"\n            \"of running tensorboard (default: no tensorboard logging)\"\n        },\n    )\n    aim_repo: Optional[str] = field(\n        default=None,\n        metadata={\"help\": \"path to Aim repository\"},\n    )\n    aim_run_hash: Optional[str] = field(\n        default=None,\n        metadata={\n            \"help\": \"Aim run hash. If skipped, creates or continues run \"\n            \"based on save_dir\"\n        },\n    )\n    wandb_project: Optional[str] = field(\n        default=None,\n        metadata={\"help\": \"Weights and Biases project name to use for logging\"},\n    )\n    azureml_logging: Optional[bool] = field(\n        default=False,\n        metadata={\"help\": \"Log scalars to AzureML context\"},\n    )\n    seed: int = field(\n        default=1, metadata={\"help\": \"pseudo random number generator seed\"}\n    )\n    seed_per_rank: bool = field(\n        default=False, metadata={\"help\": \"use different seed per rank\"}\n    )\n    cpu: bool = field(default=False, metadata={\"help\": \"use CPU instead of CUDA\"})\n    fp16: bool = field(default=False, metadata={\"help\": \"use FP16\"})\n    memory_efficient_fp16: bool = field(\n        default=False,\n        metadata={\n            \"help\": \"use a memory-efficient version of FP16 training; implies --fp16\"\n        },\n    )\n    bf16: bool = field(\n        default=False,\n        metadata={\n            \"help\": \"use BF16 format\"\n            \" Currently --bf16 is an added argument with --fp16 for mixed precision bf16 training\"\n            \" or with --memory-efficient-fp16 for pure bf16 training.\"\n        },\n    )\n    fp16_no_flatten_grads: bool = field(\n        default=False, metadata={\"help\": \"don't flatten FP16 grads tensor\"}\n    )\n    fp16_init_scale: int = field(\n        default=4, metadata={\"help\": \"default FP16 loss scale\"}\n    )\n    fp16_scale_window: Optional[int] = field(\n        default=256,\n        metadata={\"help\": \"number of updates before increasing loss scale\"},\n    )\n    fp16_scale_tolerance: float = field(\n        default=0.0,\n        metadata={\n            \"help\": \"pct of updates that can overflow before decreasing the loss scale\"\n        },\n    )\n    min_loss_scale: float = field(\n        default=2**-5,\n        metadata={\"help\": \"minimum FP16 loss scale, after which training is stopped\"},\n    )\n    threshold_loss_scale: Optional[float] = field(\n        default=None, metadata={\"help\": \"threshold FP16 loss scale from below\"}\n    )\n    user_dir: Optional[str] = field(\n        default=None,\n        metadata={\n            \"help\": \"path to a python module containing custom extensions (tasks and/or architectures)\"\n        },\n    )\n    empty_cache_freq: int = field(\n        default=0,\n        metadata={\"help\": \"how often to clear the PyTorch CUDA cache (0 to disable)\"},\n    )\n    all_gather_list_size: int = field(\n        default=16384,\n        metadata={\"help\": \"number of bytes reserved for gathering stats from workers\"},\n    )\n    model_parallel_size: int = field(\n        default=1, metadata={\"help\": \"total number of GPUs to parallelize model over\"}\n    )\n    profile: bool = field(default=False, metadata={\"help\": \"use pytorch profiler (v2)\"})\n    use_plasma_view: bool = field(\n        default=False, metadata={\"help\": \"Store indices and sizes in shared memory\"}\n    )\n    plasma_path: Optional[str] = field(\n        default=\"/tmp/plasma\",\n        metadata={\n            \"help\": \"path to run plasma_store, defaults to /tmp/plasma. Paths outside /tmp tend to fail.\"\n        },\n    )\n    log_nvidia_smi: bool = field(\n        default=False, metadata={\"help\": \"log output from nvidia-smi during training\"}\n    )\n    quiet_logs: Optional[bool] = field(\n        default=False,\n        metadata={\n            \"help\": \"Don't log grad/param norms for each parameter.\",\n        },\n    )\n\n\n@dataclass\nclass DistributedTrainingConfig(MetaseqDataclass):\n    distributed_world_size: int = field(\n        default=max(1, torch.cuda.device_count()),\n        metadata={\n            \"help\": \"total number of GPUs across all nodes (default: all visible GPUs)\"\n        },\n    )\n    distributed_rank: Optional[int] = field(\n        default=0, metadata={\"help\": \"rank of the current worker\"}\n    )\n    distributed_backend: str = field(\n        default=\"nccl\", metadata={\"help\": \"distributed backend\"}\n    )\n    distributed_init_method: Optional[str] = field(\n        default=None,\n        metadata={\n            \"help\": \"typically tcp://hostname:port that will be used to \"\n            \"establish initial connetion\"\n        },\n    )\n    distributed_port: int = field(\n        default=-1,\n        metadata={\n            \"help\": \"port number (not required if using --distributed-init-method)\"\n        },\n    )\n    device_id: int = field(\n        default=0,\n        metadata={\n            \"help\": \"which GPU to use (usually configured automatically)\",\n            \"argparse_alias\": \"--local_rank\",\n        },\n    )\n    distributed_no_spawn: bool = field(\n        default=False,\n        metadata={\n            \"help\": \"do not spawn multiple processes even if multiple GPUs are visible\"\n        },\n    )\n    ddp_backend: DDP_BACKEND_CHOICES = field(\n        default=\"pytorch_ddp\", metadata={\"help\": \"DistributedDataParallel backend\"}\n    )\n    # Reference: https://github.com/facebookresearch/metaseq/pull/668\n    task_ddp_backend: TASK_DDP_BACKEND_CHOICES = field(\n        default=\"none\",\n        metadata={\"help\": \"If set to fully_sharded, will fsdp wrap task.\"},\n    )\n    bucket_cap_mb: int = field(\n        default=25, metadata={\"help\": \"bucket size for reduction\"}\n    )\n    fix_batches_to_gpus: bool = field(\n        default=False,\n        metadata={\n            \"help\": \"don't shuffle batches between GPUs; this reduces overall \"\n            \"randomness and may affect precision but avoids the cost of re-reading the data\"\n        },\n    )\n    find_unused_parameters: bool = field(\n        default=False,\n        metadata={\n            \"help\": \"disable unused parameter detection (not applicable to \"\n            \"--ddp-backend=legacy_ddp)\"\n        },\n    )\n    fast_stat_sync: bool = field(\n        default=False,\n        metadata={\"help\": \"[deprecated] this is now defined per Criterion\"},\n    )\n\n    broadcast_buffers: bool = field(\n        default=False,\n        metadata={\n            \"help\": \"Copy non-trainable parameters between GPUs, such as \"\n            \"batchnorm population statistics\"\n        },\n    )\n\n    fp16: bool = II(\"common.fp16\")\n    memory_efficient_fp16: bool = II(\"common.memory_efficient_fp16\")\n    bf16: bool = II(\"common.bf16\")\n    # configuration for --ddp-backend=fully_sharded\n    no_reshard_after_forward: bool = field(\n        default=False,\n        metadata={\"help\": \"don't reshard parameters after forward pass\"},\n    )\n    fp32_reduce_scatter: bool = field(\n        default=False,\n        metadata={\"help\": \"reduce-scatter grads in FP32\"},\n    )\n    cpu_offload: bool = field(\n        default=False, metadata={\"help\": \"offload FP32 params to CPU\"}\n    )\n    use_sharded_state: Optional[bool] = field(\n        default=False, metadata={\"help\": \"load and save local state dict\"}\n    )\n    gradient_predivide_factor: Optional[float] = field(\n        default=None,\n        metadata={\"help\": \"factor to predivide gradients before reducee scatter\"},\n    )\n\n\n@dataclass\nclass DatasetConfig(MetaseqDataclass):\n    num_workers: int = field(\n        default=1, metadata={\"help\": \"how many subprocesses to use for data loading\"}\n    )\n    num_workers_valid: int = field(\n        default=0,\n        metadata={\n            \"help\": \"how many subprocesses to use for data loading during validation\"\n        },\n    )\n    skip_invalid_size_inputs_valid_test: bool = field(\n        default=False,\n        metadata={\"help\": \"ignore too long or too short lines in valid and test set\"},\n    )\n    max_tokens: Optional[int] = field(\n        default=None, metadata={\"help\": \"maximum number of tokens in a batch\"}\n    )\n    batch_size: Optional[int] = field(\n        default=None,\n        metadata={\n            \"help\": \"number of examples in a batch\",\n            \"argparse_alias\": \"--max-sentences\",\n        },\n    )\n    required_batch_size_multiple: int = field(\n        default=8, metadata={\"help\": \"batch size will be a multiplier of this value\"}\n    )\n    dataset_impl: Optional[DATASET_IMPL_CHOICES] = field(\n        default=None, metadata={\"help\": \"output dataset implementation\"}\n    )\n    data_buffer_size: int = field(\n        default=10, metadata={\"help\": \"Number of batches to preload\"}\n    )\n    skip_batches: str = field(\n        default=\"\",\n        metadata={\n            \"help\": \"comma separated list of batch ranges to skip in this order \"\n            \"(e.g. '100-200,150-180,160,300-310'), \"\n            \"ranges correspond to the actual num_updates in a run\"\n        },\n    )\n    train_subset: str = field(\n        default=\"train\",\n        metadata={\"help\": \"data subset to use for training (e.g. train, valid, test)\"},\n    )\n    valid_subset: str = field(\n        default=\"valid\",\n        metadata={\n            \"help\": \"comma separated list of data subsets to use for validation\"\n            \" (e.g. train, valid, test)\"\n        },\n    )\n    combine_valid_subsets: Optional[bool] = field(\n        default=None,\n        metadata={\n            \"help\": \"comma separated list of data subsets to use for validation\"\n            \" (e.g. train, valid, test)\",\n            \"argparse_alias\": \"--combine-val\",\n        },\n    )\n    ignore_unused_valid_subsets: Optional[bool] = field(\n        default=False,\n        metadata={\"help\": \"do not raise error if valid subsets are ignored\"},\n    )\n    validate_interval_updates: int = field(\n        default=0, metadata={\"help\": \"validate every N updates\"}\n    )\n    validate_after_updates: int = field(\n        default=0, metadata={\"help\": \"dont validate until reaching this many updates\"}\n    )\n    fixed_validation_seed: Optional[int] = field(\n        default=None, metadata={\"help\": \"specified random seed for validation\"}\n    )\n    disable_validation: bool = field(\n        default=False, metadata={\"help\": \"disable validation\"}\n    )\n    max_tokens_valid: Optional[int] = field(\n        default=II(\"dataset.max_tokens\"),\n        metadata={\n            \"help\": \"maximum number of tokens in a validation batch\"\n            \" (defaults to --max-tokens)\"\n        },\n    )\n    batch_size_valid: Optional[int] = field(\n        default=II(\"dataset.batch_size\"),\n        metadata={\n            \"help\": \"batch size of the validation batch (defaults to --batch-size)\",\n            \"argparse_alias\": \"--max-sentences-valid\",\n        },\n    )\n    max_valid_steps: Optional[int] = field(\n        default=None,\n        metadata={\"help\": \"How many batches to evaluate\", \"argparse_alias\": \"--nval\"},\n    )\n    gen_subset: str = field(\n        default=\"test\",\n        metadata={\"help\": \"data subset to generate (train, valid, test)\"},\n    )\n    num_shards: int = field(\n        default=1, metadata={\"help\": \"shard generation over N shards\"}\n    )\n    shard_id: int = field(\n        default=0, metadata={\"help\": \"id of the shard to generate (id < num_shards)\"}\n    )\n\n\n@dataclass\nclass OptimizationConfig(MetaseqDataclass):\n    max_epoch: int = field(\n        default=0, metadata={\"help\": \"force stop training at specified epoch\"}\n    )\n    max_update: int = field(\n        default=0, metadata={\"help\": \"force stop training at specified update\"}\n    )\n    clip_norm: float = field(\n        default=0.0, metadata={\"help\": \"clip threshold of gradients\"}\n    )\n    clip_norm_type: Optional[CLIP_GRAD_NORM_TYPE_CHOICES] = field(\n        default=\"l2\",\n        metadata={\"help\": \"either 'l2' or 'inf' to clip by l2 norm or max abs grad\"},\n    )\n    skip_gradient_update_on_clip_norm: bool = field(\n        default=False,\n        metadata={\n            \"help\": \"Skip gradient update if gnorm is higher than --clip-norm value\"\n        },\n    )\n    ewm_ratio_to_skip_batch: float = field(\n        default=-1,\n        metadata={\n            \"help\": \"Skip current batch if the loss to loss ewm ratio is \"\n            \"higher than this value. Turned off at -1\"\n        },\n    )\n\n    update_freq: List[int] = field(\n        default_factory=lambda: [1],\n        metadata={\"help\": \"update parameters every N_i batches, when in epoch i\"},\n    )\n    lr: List[float] = field(\n        default_factory=lambda: [0.25],\n        metadata={\n            \"help\": \"learning rate for the first N epochs; all epochs >N using LR_N\"\n            \" (note: this may be interpreted differently depending on --lr-scheduler)\"\n        },\n    )\n\n\n@dataclass\nclass CheckpointConfig(MetaseqDataclass):\n    save_dir: str = field(\n        default=\"checkpoints\", metadata={\"help\": \"path to save checkpoints\"}\n    )\n    restore_file: str = field(\n        default=\"checkpoint_last.pt\",\n        metadata={\n            \"help\": \"filename from which to load checkpoint \"\n            \"(default: <save-dir>/checkpoint_last.pt\"\n        },\n    )\n    finetune_from_model: Optional[str] = field(\n        default=None,\n        metadata={\n            \"help\": \"finetune from a pretrained model; note that meters and lr scheduler will be reset\"\n        },\n    )\n    reset_dataloader: bool = field(\n        default=False,\n        metadata={\n            \"help\": \"if set, does not reload dataloader state from the checkpoint\"\n        },\n    )\n    reset_lr_scheduler: bool = field(\n        default=False,\n        metadata={\n            \"help\": \"if set, does not load lr scheduler state from the checkpoint\"\n        },\n    )\n    reset_meters: bool = field(\n        default=False,\n        metadata={\"help\": \"if set, does not load meters from the checkpoint\"},\n    )\n    reset_optimizer: bool = field(\n        default=False,\n        metadata={\"help\": \"if set, does not load optimizer state from the checkpoint\"},\n    )\n    optimizer_overrides: str = field(\n        default=\"{}\",\n        metadata={\n            \"help\": \"a dictionary used to override optimizer args when loading a checkpoint\"\n        },\n    )\n    save_interval_epochs: int = field(\n        default=1,\n        metadata={\n            \"help\": \"save a checkpoint every N epochs\"\n            \"(note: one epoch is a a run over just one data shard, not of over the whole dataset, see #198)\"\n        },\n    )\n    save_interval_updates: int = field(\n        default=0, metadata={\"help\": \"save a checkpoint (and validate) every N updates\"}\n    )\n    local_save_interval_updates: int = field(\n        default=0,\n        metadata={\n            \"help\": \"save a checkpoint (and validate) every N updates to local SSD. \"\n            \"Only applicable when copying to NFS asynchronously\"\n        },\n    )\n    save_last_checkpoint: bool = field(\n        default=True,\n        metadata={\"help\": \"store a last checkpoint at the end of the training run.\"},\n    )\n    keep_last_epochs: int = field(\n        default=-1, metadata={\"help\": \"keep only the last N epoch checkpoints\"}\n    )\n    keep_last_updates: int = field(\n        default=-1, metadata={\"help\": \"keep only the last N updates checkpoints\"}\n    )\n    checkpoint_suffix: str = field(\n        default=\"\", metadata={\"help\": \"suffix to add to the checkpoint file name\"}\n    )\n    checkpoint_shard_count: int = field(\n        default=1,\n        metadata={\n            \"help\": \"Number of shards containing the checkpoint - \"\n            \"if the checkpoint is over 300GB, it is preferable \"\n            \"to split it into shards to prevent OOM on CPU while loading \"\n            \"the checkpoint\"\n        },\n    )\n    # TODO: remove write_checkpoints_asynchronously flag; metaseq-internal has dependency here so keeping for now.\n    write_checkpoints_asynchronously: bool = field(\n        default=True,\n        metadata={\n            \"help\": (\n                \"Write checkpoints asynchronously in a separate \"\n                \"thread. NOTE: This feature is currently being tested.\"\n            ),\n            \"argparse_alias\": \"--save-async\",\n        },\n    )\n    cloud_upload_path: Optional[str] = field(\n        default=None,\n        metadata={\n            \"help\": (\n                \"Upload checkpoints asynchronously in a separate \"\n                \"thread to blob store. NOTE: This feature is currently being tested.\"\n            ),\n            \"argparse_alias\": \"--cloud-dir\",\n        },\n    )\n    nfs_eval_script_path: Optional[str] = field(\n        default=None,\n        metadata={\n            \"help\": \"Path of eval script to run on checkpoints after they were uploaded\"\n        },\n    )\n    nfs_eval_num_attempts: int = field(\n        default=10,\n        metadata={\n            \"help\": \"Number of attempts of running evals on upload of checkpoint\"\n        },\n    )\n    nfs_eval_attempt_wait_minutes: int = field(\n        default=5,\n        metadata={\n            \"help\": \"Time to wait between attempts of running evals on upload of checkpoint\"\n        },\n    )\n    nfs_eval_frequency: int = field(\n        default=5000,\n        metadata={\n            \"help\": (\n                \"Run evaluation only on uploaded checkpoints\"\n                \"with multiples of this frequency\"\n            ),\n        },\n    )\n\n    # TODO(susanz): After https://github.com/fairinternal/fairseq-big-internal/issues/22 is tackled, modify this\n    #  to use ComputeEnvs constant\n    cluster_env: str = field(\n        default=\"fair\",\n        metadata={\"help\": \"cluster we are running on: azure/aws/fair/rsc\"},\n    )\n    model_parallel_size: int = II(\"common.model_parallel_size\")\n    sequence_parallel: bool = field(\n        default=False,\n        metadata={\n            \"help\": \"If True, use sequeunce level parallelism as over tensor parallel gpus.\"\n            \" only use this option when --model-parallel-size > 1\"\n        },\n    )\n\n\n@dataclass\nclass GenerationConfig(MetaseqDataclass):\n    beam: int = field(\n        default=5,\n        metadata={\"help\": \"beam size\"},\n    )\n    max_len_a: float = field(\n        default=0,\n        metadata={\n            \"help\": \"generate sequences of maximum length ax + b, where x is the source length\"\n        },\n    )\n    max_len_b: int = field(\n        default=200,\n        metadata={\n            \"help\": \"generate sequences of maximum length ax + b, where x is the source length\"\n        },\n    )\n    min_len: int = field(\n        default=1,\n        metadata={\"help\": \"minimum generation length\"},\n    )\n    sampling: bool = field(\n        default=False,\n        metadata={\"help\": \"sample hypotheses instead of using beam search\"},\n    )\n    sampling_topp: float = field(\n        default=-1.0,\n        metadata={\n            \"help\": \"sample from the smallest set whose cumulative probability mass exceeds p for next words\"\n        },\n    )\n    temperature: float = field(\n        default=1.0,\n        metadata={\"help\": \"temperature for generation\"},\n    )\n    no_seed_provided: bool = field(\n        default=False,\n        metadata={\"help\": \"if set, dont use seed for initializing random generators\"},\n    )\n    # former interactive args\n    buffer_size: int = field(\n        default=0,\n        metadata={\n            \"help\": \"read this many sentences into a buffer before processing them\"\n        },\n    )\n    input: str = field(\n        default=\"-\",\n        metadata={\"help\": \"file to read from; use - for stdin\"},\n    )\n\n\n@dataclass\nclass CommonEvalConfig(MetaseqDataclass):\n    path: Optional[str] = field(\n        default=None,\n        metadata={\"help\": \"path(s) to model file(s), colon separated\"},\n    )\n    quiet: bool = field(default=False, metadata={\"help\": \"only print final scores\"})\n    model_overrides: str = field(\n        default=\"{}\",\n        metadata={\n            \"help\": \"a dictionary used to override model args at generation that were used during model training\"\n        },\n    )\n    results_path: Optional[str] = field(\n        default=None,\n        metadata={\n            \"help\": \"path to save eval results (optional)\",\n            \"argparse_alias\": \"--sp\",\n        },\n    )\n\n\n@dataclass\nclass ReshardConfig(MetaseqDataclass):\n    save_dir: Optional[str] = field(\n        default=None,\n        metadata={\n            \"help\": \"where to save the resharded checkpoints\",\n            \"argparse_alias\": \"--dest-dir\",\n        },\n    )\n    save_prefix: Optional[str] = field(\n        default=\"reshard\", metadata={\"help\": \"save to dest-dir/save-prefix-shard{i}.pt\"}\n    )\n    target_world_size: Optional[int] = field(\n        default=128,\n        metadata={\n            \"help\": (\n                \"The maximum number of GPUs you want to use to evaluate. \"\n                \"AssertionError if any FSDP module's number of parameters is not \"\n                \"divisible by this.\"\n            )\n        },\n    )\n    do_pad: Optional[bool] = field(\n        default=False,\n        metadata={\n            \"help\": (\n                \"Add padding to make sure that running on target world size \"\n                \"works. This reduces flexibility for world sizes smaller than \"\n                \"target world size.\"\n            )\n        },\n    )\n\n\n@dataclass\nclass EvalLMConfig(MetaseqDataclass):\n    # TODO(anj): Remove this since we want to set this by default when running eval.\n    score_sequences: bool = field(\n        default=False,\n        metadata={\"help\": \"if set, uses the ScoreSequencer class for evaluating.\"},\n    )\n    output_word_probs: bool = field(\n        default=False,\n        metadata={\n            \"help\": \"if set, outputs words and their predicted log probabilities to standard output\"\n        },\n    )\n    output_word_stats: bool = field(\n        default=False,\n        metadata={\n            \"help\": \"if set, outputs word statistics such as word count, average probability, etc\"\n        },\n    )\n    context_window: int = field(\n        default=0,\n        metadata={\n            \"help\": \"ensures that every evaluated token has access to a context of at least this size, if possible\"\n        },\n    )\n    softmax_batch: int = field(\n        default=sys.maxsize,\n        metadata={\n            \"help\": (\n                \"if BxT is more than this, will batch the softmax over vocab to \"\n                \"this amount of tokens, in order to fit into GPU memory\"\n            )\n        },\n    )\n    max_valid_steps: Optional[int] = field(\n        default=None,\n        metadata={\"help\": \"How many batches to evaluate\", \"argparse_alias\": \"--nval\"},\n    )\n\n\n@dataclass\nclass EMAConfig(MetaseqDataclass):\n    store_ema: bool = field(\n        default=False, metadata={help: \"store exponential moving average shadow model\"}\n    )\n    ema_decay: float = field(\n        default=0.9999, metadata={\"help\": \"decay for exponential moving average model\"}\n    )\n    ema_start_update: int = field(\n        default=0, metadata={\"help\": \"start EMA update after this many model updates\"}\n    )\n    ema_seed_model: Optional[str] = field(\n        default=None,\n        metadata={\n            \"help\": \"Seed to load EMA model from. \"\n            \"Used to load EMA model separately from the actual model.\"\n        },\n    )\n    ema_update_freq: int = field(\n        default=1, metadata={\"help\": \"Do EMA update every this many model updates\"}\n    )\n    ema_fp32: bool = field(\n        default=False,\n        metadata={\"help\": \"If true, store EMA model in fp32 even if model is in fp16\"},\n    )\n\n\n@dataclass\nclass MetaseqConfig(MetaseqDataclass):\n    common: CommonConfig = CommonConfig()\n    common_eval: CommonEvalConfig = CommonEvalConfig()\n    distributed_training: DistributedTrainingConfig = DistributedTrainingConfig()\n    dataset: DatasetConfig = DatasetConfig()\n    optimization: OptimizationConfig = OptimizationConfig()\n    checkpoint: CheckpointConfig = CheckpointConfig()\n    generation: GenerationConfig = GenerationConfig()\n    eval_lm: EvalLMConfig = EvalLMConfig()\n    reshard: ReshardConfig = ReshardConfig()\n    ema: EMAConfig = EMAConfig()\n    model: Any = MISSING\n    task: Any = MISSING\n    criterion: Any = MISSING\n    optimizer: Any = MISSING\n    lr_scheduler: Any = MISSING\n    bpe: Any = MISSING\n    tokenizer: Any = None\n",
        "metaseq/dataclass/constants.py": "# Copyright (c) Meta Platforms, Inc. and affiliates. All Rights Reserved.\n#\n# This source code is licensed under the MIT license found in the\n# LICENSE file in the root directory of this source tree.\n\nfrom enum import Enum, EnumMeta\nfrom typing import List\n\n\nclass StrEnumMeta(EnumMeta):\n    # this is workaround for submitit pickling leading to instance checks failing in hydra for StrEnum, see\n    # https://github.com/facebookresearch/hydra/issues/1156\n    @classmethod\n    def __instancecheck__(cls, other):\n        return \"enum\" in str(type(other))\n\n\nclass StrEnum(Enum, metaclass=StrEnumMeta):\n    def __str__(self):\n        return self.value\n\n    def __eq__(self, other: str):\n        return self.value == other\n\n    def __repr__(self):\n        return self.value\n\n    def __hash__(self):\n        return hash(str(self))\n\n\ndef ChoiceEnum(choices: List[str]):\n    \"\"\"return the Enum class used to enforce list of choices\"\"\"\n    return StrEnum(\"Choices\", {k: k for k in choices})\n\n\nLOG_FORMAT_CHOICES = ChoiceEnum([\"json\", \"none\"])\nDDP_BACKEND_CHOICES = ChoiceEnum(\n    [\n        \"c10d\",  # alias for pytorch_ddp\n        \"fully_sharded\",  # FullyShardedDataParallel from fairscale\n        \"pytorch_ddp\",\n    ]\n)\n\nTASK_DDP_BACKEND_CHOICES = ChoiceEnum(\n    [\n        \"none\",  # default\n        \"fully_sharded\",  # FSDP wraps task. See https://github.com/facebookresearch/metaseq/pull/668/\n    ]\n)\nDATASET_IMPL_CHOICES = ChoiceEnum([\"raw\", \"lazy\", \"cached\", \"mmap\", \"fasta\"])\nCLIP_GRAD_NORM_TYPE_CHOICES = ChoiceEnum([\"l2\", \"inf\"])\n\n# Default document attention separator\nUNSPECIFIED_DOC_SEP = -1\n\n\nclass AttentionVariants(str, Enum):\n    DEFAULT = \"default\"\n    XFORMERS = \"xformers_default\"\n\n\nATTN_CHOICES = ChoiceEnum(k.value for k in AttentionVariants)\n",
        "metaseq/dataclass/initialize.py": "# Copyright (c) Meta Platforms, Inc. and affiliates. All Rights Reserved.\n#\n# This source code is licensed under the MIT license found in the\n# LICENSE file in the root directory of this source tree.\n\"\"\"isort:skip_file\"\"\"\n\nimport logging\n\nfrom hydra.core.config_store import ConfigStore\n\nfrom metaseq.dataclass.configs import MetaseqConfig\n\nlogger = logging.getLogger(__name__)\n\n\ndef hydra_init(cfg_name=\"base_config\") -> None:\n    cs = ConfigStore.instance()\n    cs.store(name=cfg_name, node=MetaseqConfig)\n\n    for k in MetaseqConfig.__dataclass_fields__:\n        v = MetaseqConfig.__dataclass_fields__[k].default\n        try:\n            cs.store(name=k, node=v)\n        except BaseException:\n            logger.error(f\"{k} - {v}\")\n            raise\n",
        "metaseq/dataclass/utils.py": "# Copyright (c) Meta Platforms, Inc. and affiliates. All Rights Reserved.\n#\n# This source code is licensed under the MIT license found in the\n# LICENSE file in the root directory of this source tree.\n\nimport ast\nimport inspect\nimport logging\nimport os\nimport re\nfrom argparse import ArgumentError, ArgumentParser, Namespace\nfrom dataclasses import _MISSING_TYPE, MISSING\nfrom enum import Enum\nfrom typing import Any, Dict, List, Optional, Tuple, Type\n\nfrom hydra import compose, initialize\nfrom hydra.core.global_hydra import GlobalHydra\nfrom omegaconf import DictConfig, OmegaConf, open_dict\nfrom omegaconf.errors import ConfigKeyError\n\nfrom metaseq.dataclass import MetaseqDataclass\nfrom metaseq.dataclass.configs import MetaseqConfig\n\nlogger = logging.getLogger(__name__)\n\n\ndef eval_str_list(x, x_type=float):\n    if x is None:\n        return None\n    if isinstance(x, str):\n        if len(x) == 0:\n            return []\n        x = ast.literal_eval(x)\n    try:\n        return list(map(x_type, x))\n    except TypeError:\n        return [x_type(x)]\n\n\ndef interpret_dc_type(field_type):\n    if isinstance(field_type, str):\n        raise RuntimeError(\"field should be a type\")\n\n    if field_type == Any:\n        return str\n\n    typestring = str(field_type)\n    if re.match(\n        r\"(typing.|^)Union\\[(.*), NoneType\\]$\", typestring\n    ) or typestring.startswith(\"typing.Optional\"):\n        return field_type.__args__[0]\n    return field_type\n\n\ndef gen_parser_from_dataclass(\n    parser: ArgumentParser,\n    dataclass_instance: MetaseqDataclass,\n    delete_default: bool = False,\n) -> None:\n    \"\"\"convert a dataclass instance to tailing parser arguments\"\"\"\n\n    def argparse_name(name: str):\n        if name in dataclass_instance.positional_args():\n            return name\n        if name == \"_name\":\n            # private member, skip\n            return None\n        return \"--\" + name.replace(\"_\", \"-\")\n\n    def get_kwargs_from_dc(\n        dataclass_instance: MetaseqDataclass, k: str\n    ) -> Dict[str, Any]:\n        \"\"\"k: dataclass attributes\"\"\"\n\n        kwargs = {}\n\n        field_type = dataclass_instance._get_type(k)\n        inter_type = interpret_dc_type(field_type)\n\n        field_default = dataclass_instance._get_default(k)\n\n        if isinstance(inter_type, type) and issubclass(inter_type, Enum):\n            field_choices = [t.value for t in list(inter_type)]\n        else:\n            field_choices = None\n\n        field_help = dataclass_instance._get_help(k)\n        field_const = dataclass_instance._get_argparse_const(k)\n\n        if isinstance(field_default, str) and field_default.startswith(\"${\"):\n            kwargs[\"default\"] = field_default\n        else:\n            if field_default is MISSING:\n                kwargs[\"required\"] = True\n            if field_choices is not None:\n                kwargs[\"choices\"] = field_choices\n            if (\n                isinstance(inter_type, type)\n                and (issubclass(inter_type, List) or issubclass(inter_type, Tuple))\n            ) or (\"List\" in str(inter_type) or \"Tuple\" in str(inter_type)):\n                if \"int\" in str(inter_type):\n                    kwargs[\"type\"] = lambda x: eval_str_list(x, int)\n                elif \"float\" in str(inter_type):\n                    kwargs[\"type\"] = lambda x: eval_str_list(x, float)\n                elif \"str\" in str(inter_type):\n                    kwargs[\"type\"] = lambda x: eval_str_list(x, str)\n                else:\n                    raise NotImplementedError(\n                        \"parsing of type \" + str(inter_type) + \" is not implemented\"\n                    )\n                if field_default is not MISSING:\n                    kwargs[\"default\"] = (\n                        \",\".join(map(str, field_default))\n                        if field_default is not None\n                        else None\n                    )\n            elif (\n                isinstance(inter_type, type) and issubclass(inter_type, Enum)\n            ) or \"Enum\" in str(inter_type):\n                kwargs[\"type\"] = str\n                if field_default is not MISSING:\n                    if isinstance(field_default, Enum):\n                        kwargs[\"default\"] = field_default.value\n                    else:\n                        kwargs[\"default\"] = field_default\n            elif inter_type is bool:\n                kwargs[\"action\"] = (\n                    \"store_false\" if field_default is True else \"store_true\"\n                )\n                kwargs[\"default\"] = field_default\n            else:\n                kwargs[\"type\"] = inter_type\n                if field_default is not MISSING:\n                    kwargs[\"default\"] = field_default\n\n        kwargs[\"help\"] = field_help\n        if field_const is not None:\n            kwargs[\"const\"] = field_const\n            kwargs[\"nargs\"] = \"?\"\n\n        return kwargs\n\n    for k in dataclass_instance._get_all_attributes():\n        field_name = argparse_name(dataclass_instance._get_name(k))\n        field_type = dataclass_instance._get_type(k)\n        if field_name is None:\n            continue\n        elif inspect.isclass(field_type) and issubclass(field_type, MetaseqDataclass):\n            gen_parser_from_dataclass(parser, field_type(), delete_default)\n            continue\n\n        kwargs = get_kwargs_from_dc(dataclass_instance, k)\n\n        field_args = [field_name]\n        alias = dataclass_instance._get_argparse_alias(k)\n        if alias is not None:\n            field_args.append(alias)\n\n        if \"default\" in kwargs:\n            if isinstance(kwargs[\"default\"], str) and kwargs[\"default\"].startswith(\n                \"${\"\n            ):\n                if kwargs[\"help\"] is None:\n                    # this is a field with a name that will be added elsewhere\n                    continue\n                else:\n                    del kwargs[\"default\"]\n            if delete_default and \"default\" in kwargs:\n                del kwargs[\"default\"]\n        try:\n            parser.add_argument(*field_args, **kwargs)\n        except ArgumentError:\n            pass\n\n\ndef _set_legacy_defaults(args, cls):\n    \"\"\"Helper to set default arguments based on *add_args*.\"\"\"\n    if not hasattr(cls, \"add_args\"):\n        return\n\n    import argparse\n\n    parser = argparse.ArgumentParser(\n        argument_default=argparse.SUPPRESS, allow_abbrev=False\n    )\n    cls.add_args(parser)\n    # copied from argparse.py:\n    defaults = argparse.Namespace()\n    for action in parser._actions:\n        if action.dest is not argparse.SUPPRESS:\n            if not hasattr(defaults, action.dest):\n                if action.default is not argparse.SUPPRESS:\n                    setattr(defaults, action.dest, action.default)\n    for key, default_value in vars(defaults).items():\n        if not hasattr(args, key):\n            setattr(args, key, default_value)\n\n\ndef _override_attr(\n    sub_node: str, data_class: Type[MetaseqDataclass], args: Namespace\n) -> List[str]:\n    overrides = []\n\n    if not inspect.isclass(data_class) or not issubclass(data_class, MetaseqDataclass):\n        return overrides\n\n    def get_default(f):\n        if not isinstance(f.default_factory, _MISSING_TYPE):\n            return f.default_factory()\n        return f.default\n\n    for k, v in data_class.__dataclass_fields__.items():\n        if k.startswith(\"_\"):\n            # private member, skip\n            continue\n\n        val = get_default(v) if not hasattr(args, k) else getattr(args, k)\n\n        field_type = interpret_dc_type(v.type)\n        if (\n            isinstance(val, str)\n            and not val.startswith(\"${\")  # not interpolation\n            and field_type != str\n            and (\n                not inspect.isclass(field_type) or not issubclass(field_type, Enum)\n            )  # not choices enum\n        ):\n            # upgrade old models that stored complex parameters as string\n            val = ast.literal_eval(val)\n\n        if isinstance(val, tuple):\n            val = list(val)\n\n        v_type = getattr(v.type, \"__origin__\", None)\n        if (\n            (v_type is List or v_type is list or v_type is Optional)\n            # skip interpolation\n            and not (isinstance(val, str) and val.startswith(\"${\"))\n        ):\n            # if type is int but val is float, then we will crash later - try to convert here\n            if hasattr(v.type, \"__args__\"):\n                t_args = v.type.__args__\n                if len(t_args) == 1 and (t_args[0] is float or t_args[0] is int):\n                    val = list(map(t_args[0], val))\n        elif val is not None and (\n            field_type is int or field_type is bool or field_type is float\n        ):\n            try:\n                # Future reader, if you experience something other than a ValueError here,\n                # we used to simply ignore all excepts. Add it to the allowlist.\n                val = field_type(val)\n            except ValueError:\n                pass  # ignore errors here, they are often from interpolation args\n\n        if val is None:\n            overrides.append(\"{}.{}=null\".format(sub_node, k))\n        elif val == \"\":\n            overrides.append(\"{}.{}=''\".format(sub_node, k))\n        elif isinstance(val, str):\n            val = val.replace(\"'\", r\"\\'\")\n            overrides.append(\"{}.{}='{}'\".format(sub_node, k, val))\n        elif isinstance(val, MetaseqDataclass):\n            overrides += _override_attr(f\"{sub_node}.{k}\", type(val), args)\n        elif isinstance(val, Namespace):\n            sub_overrides, _ = override_module_args(val)\n            for so in sub_overrides:\n                overrides.append(f\"{sub_node}.{k}.{so}\")\n        else:\n            overrides.append(\"{}.{}={}\".format(sub_node, k, val))\n\n    return overrides\n\n\ndef migrate_registry(\n    name, value, registry, args, overrides, deletes, use_name_as_val=False\n):\n    if value in registry:\n        overrides.append(\"{}={}\".format(name, value))\n        overrides.append(\"{}._name={}\".format(name, value))\n        overrides.extend(_override_attr(name, registry[value], args))\n    elif use_name_as_val and value is not None:\n        overrides.append(\"{}={}\".format(name, value))\n    else:\n        deletes.append(name)\n\n\ndef override_module_args(args: Namespace) -> Tuple[List[str], List[str]]:\n    \"\"\"use the field in args to overrides those in cfg\"\"\"\n    overrides = []\n    deletes = []\n\n    for k in MetaseqConfig.__dataclass_fields__.keys():\n        overrides.extend(\n            _override_attr(k, MetaseqConfig.__dataclass_fields__[k].type, args)\n        )\n\n    if args is not None:\n        if hasattr(args, \"task\"):\n            from metaseq.tasks import TASK_DATACLASS_REGISTRY\n\n            migrate_registry(\n                \"task\", args.task, TASK_DATACLASS_REGISTRY, args, overrides, deletes\n            )\n        else:\n            deletes.append(\"task\")\n\n        # these options will be set to \"None\" if they have not yet been migrated\n        # so we can populate them with the entire flat args\n        CORE_REGISTRIES = {\"criterion\", \"optimizer\", \"lr_scheduler\"}\n\n        from metaseq.registry import REGISTRIES\n\n        for k, v in REGISTRIES.items():\n            if hasattr(args, k):\n                migrate_registry(\n                    k,\n                    getattr(args, k),\n                    v[\"dataclass_registry\"],\n                    args,\n                    overrides,\n                    deletes,\n                    use_name_as_val=k not in CORE_REGISTRIES,\n                )\n            else:\n                deletes.append(k)\n\n        no_dc = True\n        if hasattr(args, \"arch\"):\n            from metaseq.models import ARCH_MODEL_REGISTRY, ARCH_MODEL_NAME_REGISTRY\n\n            if args.arch in ARCH_MODEL_REGISTRY:\n                m_cls = ARCH_MODEL_REGISTRY[args.arch]\n                dc = getattr(m_cls, \"__dataclass\", None)\n                if dc is not None:\n                    m_name = ARCH_MODEL_NAME_REGISTRY[args.arch]\n                    overrides.append(\"model={}\".format(m_name))\n                    overrides.append(\"model._name={}\".format(args.arch))\n                    # override model params with those exist in args\n                    overrides.extend(_override_attr(\"model\", dc, args))\n                    no_dc = False\n        if no_dc:\n            deletes.append(\"model\")\n\n    return overrides, deletes\n\n\ndef convert_namespace_to_omegaconf(args: Namespace) -> DictConfig:\n    \"\"\"Convert a flat argparse.Namespace to a structured DictConfig.\"\"\"\n    # Here we are using field values provided in args to override counterparts inside config object\n    overrides, deletes = override_module_args(args)\n\n    # configs will be in metaseq/config after installation\n    config_path = os.path.join(\"..\", \"config\")\n\n    GlobalHydra.instance().clear()\n\n    with initialize(config_path=config_path):\n        try:\n            composed_cfg = compose(\"config\", overrides=overrides)\n        except Exception:\n            logger.error(\"Error when composing. Overrides: \" + str(overrides))\n            raise\n\n        for k in deletes:\n            composed_cfg[k] = None\n\n    cfg = OmegaConf.create(\n        OmegaConf.to_container(composed_cfg, resolve=True, enum_to_str=True)\n    )\n\n    # hack to be able to set Namespace in dict config. this should be removed when we update to newer\n    # omegaconf version that supports object flags, or when we migrate all existing models\n    from omegaconf import _utils\n\n    old_primitive = _utils.is_primitive_type\n    _utils.is_primitive_type = lambda _: True\n\n    if cfg.task is None and getattr(args, \"task\", None):\n        cfg.task = Namespace(**vars(args))\n        from metaseq.tasks import TASK_REGISTRY\n\n        _set_legacy_defaults(cfg.task, TASK_REGISTRY[args.task])\n        cfg.task._name = args.task\n    if cfg.model is None and getattr(args, \"arch\", None):\n        cfg.model = Namespace(**vars(args))\n        from metaseq.models import ARCH_MODEL_REGISTRY\n\n        _set_legacy_defaults(cfg.model, ARCH_MODEL_REGISTRY[args.arch])\n        cfg.model._name = args.arch\n    if cfg.optimizer is None and getattr(args, \"optimizer\", None):\n        cfg.optimizer = Namespace(**vars(args))\n        from metaseq.optim import OPTIMIZER_REGISTRY\n\n        _set_legacy_defaults(cfg.optimizer, OPTIMIZER_REGISTRY[args.optimizer])\n        cfg.optimizer._name = args.optimizer\n    if cfg.lr_scheduler is None and getattr(args, \"lr_scheduler\", None):\n        cfg.lr_scheduler = Namespace(**vars(args))\n        from metaseq.optim.lr_scheduler import LR_SCHEDULER_REGISTRY\n\n        _set_legacy_defaults(cfg.lr_scheduler, LR_SCHEDULER_REGISTRY[args.lr_scheduler])\n        cfg.lr_scheduler._name = args.lr_scheduler\n    if cfg.criterion is None and getattr(args, \"criterion\", None):\n        cfg.criterion = Namespace(**vars(args))\n        from metaseq.criterions import CRITERION_REGISTRY\n\n        _set_legacy_defaults(cfg.criterion, CRITERION_REGISTRY[args.criterion])\n        cfg.criterion._name = args.criterion\n\n    _utils.is_primitive_type = old_primitive\n    OmegaConf.set_struct(cfg, True)\n    return cfg\n\n\ndef populate_dataclass(\n    dataclass: MetaseqDataclass,\n    args: Namespace,\n) -> MetaseqDataclass:\n    for k in dataclass.__dataclass_fields__.keys():\n        if k.startswith(\"_\"):\n            # private member, skip\n            continue\n        if hasattr(args, k):\n            setattr(dataclass, k, getattr(args, k))\n\n    return dataclass\n\n\ndef overwrite_args_by_name(cfg: DictConfig, overrides: Dict[str, any]):\n    # this will be deprecated when we get rid of argparse and model_overrides logic\n\n    from metaseq.registry import REGISTRIES\n\n    with open_dict(cfg):\n        for k in cfg.keys():\n            # \"k in cfg\" will return false if its a \"mandatory value (e.g. ???)\"\n            if k in cfg and isinstance(cfg[k], DictConfig):\n                if k in overrides and isinstance(overrides[k], dict):\n                    for ok, ov in overrides[k].items():\n                        if isinstance(ov, dict) and cfg[k][ok] is not None:\n                            overwrite_args_by_name(cfg[k][ok], ov)\n                        else:\n                            cfg[k][ok] = ov\n                else:\n                    overwrite_args_by_name(cfg[k], overrides)\n            elif k in cfg and isinstance(cfg[k], Namespace):\n                for override_key, val in overrides.items():\n                    setattr(cfg[k], override_key, val)\n            elif k in overrides:\n                if (\n                    k in REGISTRIES\n                    and overrides[k] in REGISTRIES[k][\"dataclass_registry\"]\n                ):\n                    cfg[k] = DictConfig(\n                        REGISTRIES[k][\"dataclass_registry\"][overrides[k]]\n                    )\n                    overwrite_args_by_name(cfg[k], overrides)\n                    cfg[k]._name = overrides[k]\n                else:\n                    cfg[k] = overrides[k]\n\n\ndef merge_with_parent(dc: MetaseqDataclass, cfg: MetaseqDataclass):\n    try:\n        merged_cfg = OmegaConf.merge(dc, cfg)\n    except ConfigKeyError:\n        # Workaround for missing keys - reverse the merge direction, then\n        # merge back in to the flipped merge\n        # See https://github.com/fairinternal/fairseq-big-internal/issues/115#issuecomment-1073129691\n        flipped_merge = OmegaConf.merge(cfg, dc)\n        merged_cfg = OmegaConf.merge(flipped_merge, cfg)\n\n    # Logic from https://github.com/omry/omegaconf/issues/441#issuecomment-737558869 ?\n    merged_cfg.__dict__[\"_parent\"] = cfg.__dict__[\"_parent\"]\n    OmegaConf.set_struct(merged_cfg, True)\n    return merged_cfg\n\n\n# Reference:\n# https://github.com/fairinternal/fairseq-py/commit/a42ad0f498d1718ca6f96fc63f73c4b2698d45b0\ndef overwrite_keys_not_present(cfg: DictConfig, overrides: Dict[str, any]):\n    # Override leaf keys not present in original checkpoint\n    # Just a hack for only two level depth\n    with open_dict(cfg):\n        for key, value in overrides.items():\n            if key in cfg:\n                for leaf_key, leaf_value in value.items():\n                    if leaf_key not in cfg[key]:\n                        cfg[key] = leaf_value\n                        logger.info(\n                            f\"adding overrides not present in original model dict, {cfg[key]}={value}\"\n                        )\n",
        "metaseq/distributed/__init__.py": "# Copyright (c) Meta Platforms, Inc. and affiliates. All Rights Reserved.\n#\n# This source code is licensed under the MIT license found in the\n# LICENSE file in the root directory of this source tree.\n\nimport metaseq.distributed.rendezvous  # noqa: F401\nfrom .fully_sharded_data_parallel import (\n    fsdp_enable_wrap,\n    fsdp_wrap,\n    FullyShardedDataParallel,\n)\nfrom .module_proxy_wrapper import ModuleProxyWrapper\n\n\n__all__ = [\n    \"fsdp_enable_wrap\",\n    \"fsdp_wrap\",\n    \"FullyShardedDataParallel\",\n    \"ModuleProxyWrapper\",\n]\n",
        "metaseq/distributed/fully_sharded_data_parallel.py": "# Copyright (c) Meta Platforms, Inc. and affiliates. All Rights Reserved.\n#\n# This source code is licensed under the MIT license found in the\n# LICENSE file in the root directory of this source tree.\n\nimport contextlib\nimport logging\nimport os\nfrom typing import Optional\n\nimport torch\n\nfrom metaseq.dataclass.configs import DistributedTrainingConfig\nfrom metaseq.distributed import utils as distributed_utils\n\nlogger = logging.getLogger(__name__)\n\ntry:\n    from fairscale.nn.data_parallel import FullyShardedDataParallel as FSDP\n    from fairscale.utils.testing import DummyProcessGroup\n\n    has_FSDP = True\nexcept ImportError:\n    FSDP = torch.nn.Module\n    has_FSDP = False\n\n\nclass FullyShardedDataParallel(FSDP):\n    \"\"\"\n    A small wrapper around fairscale's FullyShardedDataParallel (FSDP) with some\n    metaseq-specific checkpoint saving/loading logic.\n\n    Args:\n        use_sharded_state (bool): if True, then ``state_dict`` will return\n            ``FSDP.local_state_dict`` and ``load_state_dict`` will call\n            ``FSDP.load_local_state_dict``. Otherwise, ``state_dict`` will\n            return the full model weights on data parallel rank 0 (empty on\n            other ranks) and ``load_state_dict`` will broadcast model weights\n            from rank 0 to other ranks.\n    \"\"\"\n\n    def __init__(self, *args, use_sharded_state: bool = False, **kwargs):\n        if not has_FSDP:\n            raise ImportError(\n                \"Cannot find FullyShardedDataParallel. \"\n                \"Please install fairscale with: pip install fairscale\"\n            )\n        super().__init__(*args, **kwargs)\n        self.use_sharded_state = use_sharded_state\n\n    @property\n    def unwrapped_module(self) -> torch.nn.Module:\n        if self.flatten_parameters:\n            return self.module.module\n        else:\n            return self.module\n\n    def state_dict(self, destination=None, prefix=\"\", keep_vars=False):\n        if self.use_sharded_state:\n            return super().local_state_dict(\n                destination=destination, prefix=prefix, keep_vars=keep_vars\n            )\n        else:\n            if self.rank == 0:\n                return super().state_dict(\n                    destination=destination, prefix=prefix, keep_vars=keep_vars\n                )\n            else:\n                # We must call state_dict() due to use of communication\n                # primitives. But we don't use the result.\n                super().state_dict()\n                return destination or {}\n\n    def load_state_dict(self, state_dict, strict=True):\n        if self.use_sharded_state:\n            return super().load_local_state_dict(state_dict, strict=strict)\n        else:\n            if not isinstance(self.process_group, DummyProcessGroup):\n                state_dict = distributed_utils.broadcast_object(\n                    state_dict, src_rank=0, group=self.process_group\n                )\n            return super().load_state_dict(state_dict, strict=strict)\n\n\n@contextlib.contextmanager\ndef fsdp_enable_wrap(\n    cfg: DistributedTrainingConfig, use_sharded_state: bool = False, **kwargs\n):\n    try:\n        from fairscale.nn import enable_wrap\n    except ImportError:\n        raise ImportError(\n            \"Cannot find FullyShardedDataParallel. \"\n            \"Please install fairscale with: pip install fairscale\"\n        )\n    if cfg.memory_efficient_fp16:\n        assert cfg.fp16  # memory_efficient_fp16 should imply fp16\n    group = distributed_utils.get_data_parallel_group()\n    if group is None and cfg.distributed_world_size == 1:\n        group = DummyProcessGroup(rank=0, size=1)\n    if cfg.fp16:\n        compute_dtype = torch.bfloat16 if cfg.bf16 else torch.float16\n    else:\n        compute_dtype = torch.float32\n    fsdp_config = {\n        \"process_group\": group,\n        \"process_group_reduce_scatter\": group,\n        \"reshard_after_forward\": not cfg.no_reshard_after_forward,\n        \"mixed_precision\": cfg.fp16 and not cfg.memory_efficient_fp16,\n        \"fp32_reduce_scatter\": cfg.fp32_reduce_scatter,\n        \"flatten_parameters\": True,\n        \"cpu_offload\": cfg.cpu_offload and not cfg.memory_efficient_fp16,\n        \"compute_dtype\": compute_dtype,\n        \"bucket_cap_mb\": cfg.bucket_cap_mb,\n        \"state_dict_device\": torch.device(\"cpu\"),\n        \"gradient_predivide_factor\": cfg.gradient_predivide_factor,\n        **kwargs,\n    }\n    with enable_wrap(\n        wrapper_cls=FullyShardedDataParallel,\n        use_sharded_state=use_sharded_state,\n        **fsdp_config,\n    ):\n        yield\n\n\ndef fsdp_wrap(module, min_num_params: Optional[int] = None, **kwargs):\n    \"\"\"\n    Helper to wrap layers/modules in FSDP. This falls back to a no-op if\n    fairscale is not available.\n\n    Args:\n        module (nn.Module): module to (maybe) wrap\n        min_num_params (int, Optional): minimum number of layer params to wrap\n    \"\"\"\n    try:\n        from fairscale.nn import wrap\n\n        if os.environ.get(\"RESHARD_OVERRIDE_PROCESS_GROUP\", \"False\") == \"True\":\n            logger.info(\"Process group was None, overriding to DummyProcessGroup\")\n            kwargs[\"process_group\"] = DummyProcessGroup(rank=0, size=1)\n\n        if min_num_params is not None:\n            num_params = sum(p.numel() for p in module.parameters())\n            if num_params >= min_num_params:\n                return wrap(module, **kwargs)\n            else:\n                return module\n        else:\n            return wrap(module, **kwargs)\n    except ImportError:\n        return module\n",
        "metaseq/distributed/module_proxy_wrapper.py": "# Copyright (c) Meta Platforms, Inc. and affiliates. All Rights Reserved.\n#\n# This source code is licensed under the MIT license found in the\n# LICENSE file in the root directory of this source tree.\n\nfrom torch import nn\n\n\nclass ModuleProxyWrapper(nn.Module):\n    \"\"\"\n    Wrap a DistributedDataParallel module and forward requests for missing\n    attributes to the module wrapped by DDP (the twice-wrapped module).\n    Also forward calls to :func:`state_dict` and :func:`load_state_dict`.\n\n    Usage::\n\n        module.xyz = \"hello world\"\n        wrapped_module = DistributedDataParallel(module, **ddp_args)\n        wrapped_module = ModuleProxyWrapper(wrapped_module)\n        assert wrapped_module.xyz == \"hello world\"\n        assert wrapped_module.state_dict().keys() == module.state_dict().keys()\n\n    Args:\n        module (nn.Module): module to wrap\n    \"\"\"\n\n    def __init__(self, module: nn.Module):\n        super().__init__()\n        assert hasattr(\n            module, \"module\"\n        ), \"ModuleProxyWrapper expects input to wrap another module\"\n        self.module = module\n\n    def __getattr__(self, name):\n        \"\"\"Forward missing attributes to twice-wrapped module.\"\"\"\n        try:\n            # defer to nn.Module's logic\n            return super().__getattr__(name)\n        except AttributeError:\n            try:\n                # forward to the once-wrapped module\n                return getattr(self.module, name)\n            except AttributeError:\n                # forward to the twice-wrapped module\n                return getattr(self.module.module, name)\n\n    def state_dict(self, *args, **kwargs):\n        \"\"\"Forward to the twice-wrapped module.\"\"\"\n        return self.module.module.state_dict(*args, **kwargs)\n\n    def load_state_dict(self, *args, **kwargs):\n        \"\"\"Forward to the twice-wrapped module.\"\"\"\n        return self.module.module.load_state_dict(*args, **kwargs)\n\n    def forward(self, *args, **kwargs):\n        return self.module(*args, **kwargs)\n",
        "metaseq/distributed/rendezvous.py": "# Copyright (c) Meta Platforms, Inc. and affiliates. All Rights Reserved.\n#\n# This source code is licensed under the MIT license found in the\n# LICENSE file in the root directory of this source tree.\n\nimport time\nimport logging\nfrom typing import Dict\nfrom urllib.parse import urlparse\n\nfrom torch.distributed.constants import default_pg_timeout\nfrom torch.distributed import register_rendezvous_handler, Store, TCPStore, rendezvous\n\n\nRETRIES = 5\nCOOLDOWN = 0.25\n\nlogger = logging.getLogger(__name__)\n\n\ndef _create_c10d_store(hostname, port, rank, world_size, timeout) -> Store:\n    # check if port is uint16_t\n    if not 0 <= port < 2**16:\n        raise ValueError(f\"port must have value from 0 to 65535 but was {port}.\")\n\n    start_daemon = rank == 0\n    return TCPStore(\n        hostname, port, world_size, start_daemon, timeout, multi_tenant=True\n    )\n\n\ndef _query_to_dict(query: str) -> Dict[str, str]:\n    return dict(\n        (pair[0], pair[1])\n        for pair in (pair.split(\"=\") for pair in filter(None, query.split(\"&\")))\n    )\n\n\ndef _tcp_retry_rendezvous_handler(url: str, timeout=default_pg_timeout, **kwargs):\n    def _error(msg):\n        return ValueError(\"tcpr:// rendezvous: \" + msg)\n\n    result = urlparse(url)\n    if not result.port:\n        raise _error(\"port number missing\")\n    query_dict = _query_to_dict(result.query)\n    if \"rank\" not in query_dict:\n        raise _error(\"rank parameter missing\")\n    if \"world_size\" not in query_dict:\n        raise _error(\"world size parameter missing\")\n\n    rank = int(query_dict[\"rank\"])\n    world_size = int(query_dict[\"world_size\"])\n    assert result.hostname is not None\n\n    for tries in range(1, RETRIES + 1):\n        try:\n            store = _create_c10d_store(\n                result.hostname, result.port, rank, world_size, timeout\n            )\n            logger.warning(\n                f\"Successfully connected to primary node on attempt {tries}/{RETRIES}\"\n            )\n            yield (store, rank, world_size)\n            break\n        except RuntimeError as re:\n            logger.error(\n                f\"Failed to connect to primary node on attempt {tries}/{RETRIES}: {re}\"\n            )\n            time.sleep(COOLDOWN * (2**tries))\n\n    # If this configuration is invalidated, there is nothing we can do about it\n    raise RuntimeError(\"Unable to perform re-rendezvous using tcpr:// method\")\n\n\nSTORE_BASED_BARRIER_MARKER = \"store_based_barrier_key:\"\n\n\nclass ComplicitStore(Store):\n    def __init__(self, store: Store, world_size: int):\n        super().__init__()\n        self.store = store\n        self.world_size = world_size\n\n    def set(self, *args, **kwargs):\n        return self.store.set(*args, **kwargs)\n\n    def get(self, *args, **kwargs):\n        return self.store.get(*args, **kwargs)\n\n    def add(self, key: str, *args, **kwargs):\n        # The marker isn't always a prefix: it's sometimes prefixed with default_pg/.\n        if STORE_BASED_BARRIER_MARKER in key:\n            return self.world_size\n        return self.store.add(key, *args, **kwargs)\n\n    def compare_set(self, *args, **kwargs):\n        return self.store.compare_set(*args, **kwargs)\n\n    def delete_key(self, *args, **kwargs):\n        return self.store.delete_key(*args, **kwargs)\n\n    def num_keys(self, *args, **kwargs):\n        return self.store.num_keys(*args, **kwargs)\n\n    def set_timeout(self, *args, **kwargs):\n        return self.store.set_timeout(*args, **kwargs)\n\n    def wait(self, *args, **kwargs):\n        return self.store.wait(*args, **kwargs)\n\n\nSTORE = None\n\n\ndef _tcp_retry_barrierless_rendezvous_handler(\n    url: str, timeout=default_pg_timeout, **kwargs\n):\n    assert url.startswith(\"barrierlesstcpr://\")\n    url = url.replace(\"barrierlesstcpr://\", \"tcpr://\")\n    rendezvous_iterator = rendezvous(url, timeout=timeout)\n    store, rank, world_size = next(rendezvous_iterator)\n    global STORE\n    STORE = ComplicitStore(store, world_size)\n    return iter([(STORE, rank, world_size)])\n\n\nregister_rendezvous_handler(\"tcpr\", _tcp_retry_rendezvous_handler)\nregister_rendezvous_handler(\n    \"barrierlesstcpr\", _tcp_retry_barrierless_rendezvous_handler\n)\n",
        "metaseq/distributed/stitch_fsdp_ckpt.py": "# Copyright (c) Meta Platforms, Inc. and affiliates. All Rights Reserved.\n#\n# This source code is licensed under the MIT license found in the\n# LICENSE file in the root directory of this source tree.\n\nimport gc\nimport logging\nimport os\nimport re\nimport time\nfrom collections import defaultdict, OrderedDict\nfrom glob import glob\nfrom pathlib import Path\n\nimport torch\nfrom tqdm import tqdm\n\nfrom metaseq.distributed.fully_sharded_data_parallel import FSDP as FSDP\nfrom metaseq.file_io import load_and_pop_last_optimizer_state\n\nlogger = logging.getLogger(__name__)\n\n\ndef _get_shard_number(x) -> int:\n    match = re.search(r\"shard(\\d+).pt\", x)\n    if match is None:\n        raise AssertionError(f\"{x} did not match shard(\\\\d+).pt\")\n    else:\n        return int(match.groups()[0])\n\n\ndef consolidate_fsdp_shards(\n    pth_prefix: str,\n    save_prefix=None,\n    strict=False,\n    new_arch_name=None,\n    no_stitch_megatron=False,\n    megatron_part=None,\n    is_ema=False,\n) -> str:\n    if pth_prefix.endswith(\".pt\"):\n        pth_prefix = pth_prefix[:-3]\n    if save_prefix is None:\n        save_prefix = pth_prefix + \"_consolidated\"  # .pt'\n    all_ckpt_files = list(\n        sorted(glob(f\"{pth_prefix}*shard*.pt\"), key=_get_shard_number)\n    )\n    if megatron_part is not None:\n        no_stitch_megatron = True\n        all_ckpt_files = [\n            x for x in all_ckpt_files if f\"model_part-{megatron_part}\" in x\n        ]\n    assert all_ckpt_files, f\"no paths matched {pth_prefix}*shard*.pt\"\n    weights = []\n    metadata = []\n    expert_paths = []\n    expert_dest_paths = []\n    expert_ranks = []\n    names = []\n    dense = True\n    t0 = time.time()\n    for p in tqdm(all_ckpt_files):\n        names.append(Path(p).name)\n        if re.search(r\"rank-(\\d+)\", os.path.basename(p)):  # expert checkpoint\n            expert_paths.append(p)\n            r = re.search(r\"rank-(\\d+)\", os.path.basename(p)).groups()[0]\n            assert r not in expert_ranks\n            expert_ranks.append(r)\n            expert_dest_paths.append(f\"{save_prefix}-rank-{r}.pt\")\n        else:\n            ckpt = load_and_pop_last_optimizer_state(p)\n            if \"ema_fp32_params\" in ckpt[\"extra_state\"]:\n                ema_key = \"ema_fp32_params\"\n            elif \"ema\" in ckpt[\"extra_state\"]:\n                ema_key = \"ema\"\n            else:\n                ema_key = None\n            if is_ema and ema_key is not None:\n                weights.append(ckpt[\"extra_state\"][ema_key])\n            else:\n                weights.append(ckpt[\"model\"])\n            metadata.append(ckpt[\"shard_metadata\"])\n    assert weights, f\"all files were considered experts: {all_ckpt_files}\"\n    do_consolidate = True\n    if \"decoder.embed_tokens.weight\" in weights[0].keys():\n        shape = weights[0][\"decoder.embed_tokens.weight\"].shape\n        logger.info(\n            f\"This ckpt does not seem sharded. I see unflat params! like \"\n            f\"decoder.embed_tokens.weight shaped {shape}. Will just copy files \"\n            f\"and remove optim_state.\"\n        )\n        do_consolidate = False\n    if do_consolidate:\n        num_parts = find_num_parts(names)\n        if num_parts:\n            logger.info(\"consolidate_model_parallel\")\n            consolidated_weights = consolidate_model_parallel(\n                metadata,\n                names,\n                strict,\n                weights,\n                parts=num_parts,\n                no_stitch_megatron=no_stitch_megatron,\n            )\n        else:\n            logger.info(\"FSDP.consolidate_shard_weights\")\n            consolidated_weights = FSDP.consolidate_shard_weights(\n                shard_weights=weights, shard_metadata=metadata, strict=strict\n            )\n        del weights, metadata\n        gc.collect()\n        done_consolidate = time.time()\n        logger.info(f\"Done consolidating after {done_consolidate-t0//60} minutes\")\n    else:\n        consolidated_weights = weights[0]\n    if new_arch_name is not None:\n        ckpt[\"cfg\"][\"model\"]._name = new_arch_name\n    if dense:\n        logger.info(\"dense\")\n\n        def save_checkpoint(weights_to_save, prefix):\n            ckpt_consolidated = dict(\n                model=weights_to_save,\n                cfg=ckpt[\"cfg\"],\n                extra_state=ckpt[\"extra_state\"],\n                optimizer_history=ckpt[\"optimizer_history\"],\n                args=ckpt.get(\"args\"),\n            )\n            save_path = f\"{prefix}.pt\"\n            logger.info(f\"Saving to {save_path} ...\")\n            torch.save(ckpt_consolidated, save_path)\n            logger.info(f\"Done after {time.time()-t0//60} minutes\")\n            return save_path\n\n        if no_stitch_megatron:\n            saved_paths = []\n            for part_id, part_consolidated_weights in consolidated_weights.items():\n                saved_paths.append(\n                    save_checkpoint(\n                        part_consolidated_weights, f\"{save_prefix}-model_part-{part_id}\"\n                    )\n                )\n            return saved_paths\n        return save_checkpoint(consolidated_weights, save_prefix)\n\n    ckpt_shared = dict(\n        model=consolidated_weights,\n        cfg=ckpt[\"cfg\"],\n        extra_state=ckpt[\"extra_state\"],\n        optimizer_history=ckpt[\"optimizer_history\"],\n        args=ckpt[\"args\"],\n    )\n    logger.info(\"saving..\")\n    torch.save(ckpt_shared, f\"{save_prefix}-shared.pt\")\n    logger.info(f\"Done saving. Total time: {time.time()-t0//60} minutes,  \")\n    # Process experts\n    for src, dst in tqdm(\n        list(zip(expert_paths, expert_dest_paths)), desc=\"expert files\"\n    ):\n        ckpt = load_and_pop_last_optimizer_state(src)\n        if do_consolidate:\n            expert_wt = FSDP.consolidate_shard_weights(\n                shard_weights=[ckpt[\"model\"]],\n                shard_metadata=[ckpt[\"shard_metadata\"]],\n                strict=False,\n            )\n            ckpt = dict(\n                model=expert_wt,\n                cfg=ckpt[\"cfg\"],\n                extra_state=ckpt[\"extra_state\"],\n                optimizer_history=ckpt[\"optimizer_history\"],\n                args=ckpt[\"args\"],\n            )\n\n        torch.save(ckpt, dst)\n    logger.info(f\"saved consolidated MoE with prefix {save_prefix}.pt\")\n    return f\"{save_prefix}.pt\"\n\n\ndef consolidate_model_parallel(\n    metadata, names, strict, weights, parts=2, no_stitch_megatron=False\n):\n    model_parts = defaultdict(list)\n    metadata_parts = defaultdict(list)\n    for i, n in enumerate(names):\n        for p in range(parts):\n            if f\"part-{p}\" in n:\n                model_parts[p].append(weights[i])\n                metadata_parts[p].append(metadata[i])\n    all_parts_consolidated = defaultdict(list)\n    for k, v in model_parts.items():\n        part_weights = FSDP.consolidate_shard_weights(\n            shard_weights=v, shard_metadata=metadata_parts[k], strict=strict\n        )\n        all_parts_consolidated[k] = part_weights\n    if no_stitch_megatron:\n        return all_parts_consolidated\n    # glue to be a single megatron model part\n    model = reshard_megatron_parts(all_parts_consolidated, new_model_part_count=1)[0]\n    return model\n\n\ndef handle_qkv_proj(model_parts, key, new_model_part_count):\n    parts = [model_parts[part_id][key] for part_id in range(len(model_parts))]\n    ks, vs, qs = [], [], []\n    for p in parts:\n        k, v, q = torch.split(p, p.shape[0] // 3)\n        ks.append(k)\n        vs.append(v)\n        qs.append(q)\n    resharded_ks = torch.chunk(torch.cat(ks, dim=0), new_model_part_count)\n    resharded_vs = torch.chunk(torch.cat(vs, dim=0), new_model_part_count)\n    resharded_qs = torch.chunk(torch.cat(qs, dim=0), new_model_part_count)\n    return resharded_ks, resharded_vs, resharded_qs\n\n\ndef _handle_one(parts, is_weight):\n    \"\"\"Make it look like a normal LayerNorm\"\"\"\n    n_parts = len(parts)\n    err_msg = f\"Redundant ModelParallelFusedLayerNorm params have been updated.\"\n    if is_weight:\n        init = 1.0\n        assert not torch.logical_and(parts[0].ne(1), parts[1].ne(1)).any(), err_msg\n\n    else:\n        init = 0.0\n        assert not torch.logical_and(parts[0].ne(0), parts[1].ne(0)).any(), err_msg\n    ret_val = torch.cat([p.unsqueeze(-1) for p in parts], dim=1).sum(1) - (\n        init * (n_parts - 1)\n    )\n    return ret_val\n\n\ndef get_n_layers(glued_model):\n    n_layers = 0\n    while True:\n        if f\"decoder.layers.{n_layers}.fc1.weight\" in glued_model:\n            n_layers += 1\n        else:\n            assert (\n                n_layers > 0\n            ), f\"found 0 layers bc no keys matching decoder.layers.0.fc1.weight\"\n            return n_layers\n\n\ndef reshard_megatron_parts(model_parts, new_model_part_count=1):\n    \"\"\"\n    Reshard to different number of model parts.\n    When new_model_part_count=1 return glued model\n    \"\"\"\n    new_model_parts = [OrderedDict() for _ in range(new_model_part_count)]\n\n    def assert_all_close(key):\n        for part_id in range(len(model_parts)):\n            if not torch.allclose(model_parts[part_id][key], model_parts[0][key]):\n                err = (\n                    (model_parts[part_id][key] - model_parts[0][key])\n                    .float()\n                    .abs()\n                    .max()\n                    .item()\n                )\n                logger.info(f\"max discrepancy {key}: {err}\")\n\n    def _conslidate_and_redshard(key, dim):\n        consolidated_tensor = torch.cat(\n            [model_parts[part_id][key] for part_id in range(len(model_parts))],\n            dim=dim,\n        )\n        assert consolidated_tensor.size(dim) % new_model_part_count == 0\n        newly_resharded_tensors = torch.chunk(\n            consolidated_tensor,\n            new_model_part_count,\n            dim=dim,\n        )\n        for i in range(new_model_part_count):\n            new_model_parts[i][key] = newly_resharded_tensors[i].clone()\n\n    def _copy_key_to_all_parts(key):\n        for new_model_part in new_model_parts:\n            new_model_part[key] = model_parts[0][key].clone()\n\n    for key in model_parts[0]:\n        if \"qkv\" in key:\n            # Bias of CP gets concatenated\n            if key.endswith(\"bias\"):\n                resharded_ks, resharded_vs, resharded_qs = handle_qkv_proj(\n                    model_parts, key, new_model_part_count\n                )\n            else:\n                assert key.endswith(\"weight\")\n                resharded_ks, resharded_vs, resharded_qs = handle_qkv_proj(\n                    model_parts, key, new_model_part_count\n                )\n\n            # Handle the special case when new_model_part_count = 1 (converting to a singleton checkpoint)\n            if new_model_part_count == 1:\n                new_model_parts[0][key.replace(\"qkv\", \"k\")] = resharded_ks[0]\n                new_model_parts[0][key.replace(\"qkv\", \"v\")] = resharded_vs[0]\n                new_model_parts[0][key.replace(\"qkv\", \"q\")] = resharded_qs[0]\n            else:\n                for i in range(new_model_part_count):\n                    new_model_parts[i][key] = torch.cat(\n                        (resharded_ks[i], resharded_vs[i], resharded_qs[i]), dim=0\n                    )\n\n        elif \"ffn_layernorm\" in key:\n            _conslidate_and_redshard(key, dim=0)\n        elif \"layer_norm\" in key:\n            assert_all_close(key)\n            _copy_key_to_all_parts(key)\n        elif \"fc1\" in key or \"k_proj\" in key or \"q_proj\" in key or \"v_proj\" in key:\n            # Bias of CP gets concatenated\n            if key.endswith(\"bias\"):\n                _conslidate_and_redshard(key, dim=0)\n            # weights of CP gets concatenated along dim 0\n            else:\n                assert key.endswith(\"weight\")\n                _conslidate_and_redshard(key, dim=0)\n                # FC1 is CP\n        # FC2 is RP\n        elif \"fc2\" in key or \"out_proj\" in key:\n            # Bias of RP gets replicated\n            if key.endswith(\"bias\"):\n                assert_all_close(key)\n                _copy_key_to_all_parts(key)\n            # weights of RP gets concatenated along dim 1\n            else:\n                assert key.endswith(\"weight\")\n                _conslidate_and_redshard(key, dim=1)\n        elif \"embed_tokens.weight\" in key:\n            _conslidate_and_redshard(key, dim=0)\n        elif \"embed_positions\" in key:\n            if \"_float_tensor\" in key:\n                # Assume embed positions are non learned ie.e sinusoidal\n                for new_model_part in new_model_parts:\n                    new_model_part[key] = torch.zeros([1])\n            else:\n                assert_all_close(key)\n                _copy_key_to_all_parts(key)\n        elif \"version\" in key:\n            _copy_key_to_all_parts(key)\n        else:\n            assert_all_close(key)\n            _copy_key_to_all_parts(key)\n\n    for new_model_part in new_model_parts:\n        assert len(new_model_part.keys()) >= len(model_parts[0].keys())\n        assert \"decoder.layers.0.ffn_layernorm.lns.0.weight\" not in new_model_part\n\n    return new_model_parts\n\n\ndef find_num_parts(names) -> int:\n    parts = []\n    for n in names:\n        part = re.search(r\"-model_part-(\\d+)\", n)\n        if part is not None:\n            parts.append(int(part.groups()[0]))\n    if parts:\n        return max(parts) + 1\n    else:\n        return 0\n",
        "metaseq/distributed/utils.py": "# Copyright (c) Meta Platforms, Inc. and affiliates. All Rights Reserved.\n#\n# This source code is licensed under the MIT license found in the\n# LICENSE file in the root directory of this source tree.\n\nimport io\nimport logging\nimport os\nimport pickle\nimport random\nimport signal\nimport socket\nimport struct\nimport subprocess\nfrom argparse import Namespace\nfrom collections import OrderedDict\nfrom dataclasses import dataclass\nfrom typing import Any, Dict, List, Mapping, Optional\n\nimport torch\nimport torch.distributed as dist\nfrom omegaconf import open_dict\n\nfrom metaseq.dataclass.configs import DistributedTrainingConfig, MetaseqConfig\n\nlogger = logging.getLogger(__name__)\n\n# Flag to indicate if we're using Megatron\n# NOTE: this is a temporary hack until we move away from Megatron's model parallel init\n_USE_MEGATRON = False\n\n\ndef is_master(cfg: DistributedTrainingConfig):\n    return cfg.distributed_rank == 0\n\n\ndef infer_init_method(cfg: DistributedTrainingConfig, force_distributed=False):\n    if cfg.distributed_init_method is not None:\n        return\n\n    if cfg.distributed_port > 0:\n        # we can determine the init method automatically for Slurm\n        _infer_slurm_init(cfg)\n    elif all(\n        key in os.environ\n        for key in [\"MASTER_ADDR\", \"MASTER_PORT\", \"WORLD_SIZE\", \"RANK\", \"LOCAL_RANK\"]\n    ):\n        # support torch.distributed.launch\n        _infer_torch_distributed_launch_init(cfg)\n    elif cfg.distributed_world_size > 1 or force_distributed:\n        # fallback for single node with multiple GPUs\n        _infer_single_node_init(cfg)\n\n    if not cfg.distributed_no_spawn:\n        with open_dict(cfg):\n            cfg.distributed_num_procs = min(\n                torch.cuda.device_count(), cfg.distributed_world_size\n            )\n\n\ndef _infer_torch_distributed_launch_init(cfg: DistributedTrainingConfig):\n    cfg.distributed_init_method = \"env://\"\n    cfg.distributed_world_size = int(os.environ[\"WORLD_SIZE\"])\n    cfg.distributed_rank = int(os.environ[\"RANK\"])\n    cfg.device_id = int(os.environ[\"LOCAL_RANK\"])\n    # processes are created by torch.distributed.launch\n    cfg.distributed_no_spawn = True\n\n\ndef _infer_slurm_init(cfg: DistributedTrainingConfig):\n    node_list = os.environ.get(\"SLURM_STEP_NODELIST\")\n    if node_list is None:\n        node_list = os.environ.get(\"SLURM_JOB_NODELIST\")\n    if node_list is not None:\n        try:\n            hostnames = subprocess.check_output(\n                [\"scontrol\", \"show\", \"hostnames\", node_list]\n            )\n            host = hostnames.split()[0].decode(\"utf-8\")\n        except subprocess.CalledProcessError as e:  # scontrol failed\n            raise e\n        except FileNotFoundError:  # Slurm is not installed\n            # if we're in a container, then maybe MASTER_ADDR is set\n            host = os.environ.get(\"MASTER_ADDR\", None)\n        if host is None:\n            return\n        cfg.distributed_init_method = \"barrierlesstcpr://{host}:{port}\".format(\n            host=host, port=cfg.distributed_port\n        )\n        nnodes = int(os.environ.get(\"SLURM_NNODES\"))\n        ntasks_per_node = os.environ.get(\"SLURM_NTASKS_PER_NODE\")\n        if ntasks_per_node is not None:\n            ntasks_per_node = int(ntasks_per_node)\n        else:\n            ntasks = int(os.environ.get(\"SLURM_NTASKS\"))\n            nnodes = int(os.environ.get(\"SLURM_NNODES\"))\n            assert ntasks % nnodes == 0\n            ntasks_per_node = int(ntasks / nnodes)\n        if ntasks_per_node == 1:\n            gpus_per_node = torch.cuda.device_count()\n            node_id = int(os.environ.get(\"SLURM_NODEID\"))\n            cfg.distributed_rank = node_id * gpus_per_node\n            cfg.distributed_world_size = nnodes * gpus_per_node\n        else:\n            assert ntasks_per_node == torch.cuda.device_count()\n            cfg.distributed_world_size = ntasks_per_node * nnodes\n            cfg.distributed_no_spawn = True\n            cfg.distributed_rank = int(os.environ.get(\"SLURM_PROCID\"))\n            cfg.device_id = int(os.environ.get(\"SLURM_LOCALID\"))\n\n\ndef _infer_single_node_init(cfg: DistributedTrainingConfig):\n    assert (\n        cfg.distributed_world_size <= torch.cuda.device_count()\n    ), f\"world size is {cfg.distributed_world_size} but have {torch.cuda.device_count()} available devices\"\n    port = random.randint(10000, 20000)\n    cfg.distributed_init_method = \"tcp://localhost:{port}\".format(port=port)\n\n\ndef distributed_init(cfg: MetaseqConfig):\n    if isinstance(cfg, Namespace):\n        from metaseq.dataclass.utils import convert_namespace_to_omegaconf\n\n        cfg = convert_namespace_to_omegaconf(cfg)\n\n    # silence torch's distributed initialization info\n    logging.getLogger(\"torch.distributed.distributed_c10d\").setLevel(logging.WARNING)\n\n    if torch.distributed.is_available() and torch.distributed.is_initialized():\n        logger.warning(\"Distributed is already initialized, cannot initialize twice!\")\n    else:\n        logger.debug(\n            \"distributed init (rank {}): {}\".format(\n                cfg.distributed_training.distributed_rank,\n                cfg.distributed_training.distributed_init_method,\n            )\n        )\n        dist.init_process_group(\n            backend=cfg.distributed_training.distributed_backend,\n            init_method=cfg.distributed_training.distributed_init_method,\n            world_size=cfg.distributed_training.distributed_world_size,\n            rank=cfg.distributed_training.distributed_rank,\n        )\n        logger.info(\n            \"initialized host {} as rank {}\".format(\n                socket.gethostname(),\n                cfg.distributed_training.distributed_rank,\n            )\n        )\n\n        # perform a dummy all-reduce to initialize the NCCL communicator\n        if torch.cuda.is_available():\n            dist.all_reduce(torch.zeros(1).cuda())\n\n    cfg.distributed_training.distributed_rank = torch.distributed.get_rank()\n\n    # set global log level\n    if is_master(cfg.distributed_training):\n        logging.getLogger().setLevel(logging.INFO)\n    else:\n        logging.getLogger().setLevel(logging.WARNING)\n\n    nodelist = os.environ.get(\"SLURM_STEP_NODELIST\")\n    if nodelist:\n        logger.info(f\"SLURM nodelist: {nodelist}\")\n\n    from metaseq.modules.megatron.mpu import (\n        initialize_model_parallel,\n        model_parallel_cuda_manual_seed,\n    )\n\n    # Following initializes memory buffer in Megatron code which uses\n    # buffered memory for tensor parallel GPU comms protocols\n    from metaseq.modules.megatron.global_vars import (\n        _GLOBAL_MEMORY_BUFFER,\n        _set_global_memory_buffer,\n    )\n\n    global _USE_MEGATRON\n    _USE_MEGATRON = True\n    initialize_model_parallel(cfg.common.model_parallel_size)\n    if torch.cuda.is_available():\n        dist.all_reduce(torch.zeros(1).cuda(), group=get_model_parallel_group())\n    model_parallel_cuda_manual_seed(cfg.common.seed)\n    # This check should not be usually needed as we call init only once\n    # but seems like tests are calling it multiple times.\n    if _GLOBAL_MEMORY_BUFFER is None:\n        _set_global_memory_buffer()\n    model_part_number = get_model_parallel_rank()\n    cfg.checkpoint.checkpoint_suffix += \"-model_part-{0}\".format(model_part_number)\n\n    return cfg.distributed_training.distributed_rank\n\n\ndef distributed_main(i, main, cfg: MetaseqConfig, kwargs):\n    # don't use MKL/OMP to avoid conflicting processes\n    torch.set_num_threads(1)\n    if not cfg.distributed_training.distributed_no_spawn:\n        # if in local spawning, i is offset by -1 since torch.multiprocessing.spawn\n        # always starts at rank 0\n        i = i + 1\n    cfg.distributed_training.device_id = i\n    if torch.cuda.is_available() and not cfg.common.cpu:\n        torch.cuda.set_device(cfg.distributed_training.device_id)\n        # This is temporary way of making microsoft Tutel happy, as it reads the local rank from\n        # the env. To make it work in cleaner way, we might need to change their interfaces to be\n        # able to pass local rank.\n        os.environ[\"LOCAL_RANK\"] = str(cfg.distributed_training.device_id)\n    if cfg.distributed_training.distributed_rank is None:\n        # start_rank is the rank of gpu 0 on this machine.\n        cfg.distributed_training.distributed_rank = kwargs.pop(\"start_rank\", 0) + i\n\n    cfg.distributed_training.distributed_rank = distributed_init(cfg)\n\n    after_distributed_init_fn = kwargs.pop(\"after_distributed_init_fn\", None)\n    if after_distributed_init_fn:\n        cfg = after_distributed_init_fn(cfg)\n    retval = main(cfg, **kwargs)\n    global_barrier()\n    return retval\n\n\ndef global_barrier():\n    \"\"\"\n    A global barrier that all workers in all process groups must wait for.\n    \"\"\"\n    if torch.distributed.is_initialized():\n        torch.distributed.barrier(get_global_group())\n\n\ndef _spawn_helper(main, cfg, kwargs):\n    \"\"\"\n    Perform a fork() to many processes.\n\n    Intentionally runs the rank0 in the main process so that signals\n    can be more easily caught and we can cleanup processes.\n    \"\"\"\n    # Launch multiple subprocesses\n    spawncontext = torch.multiprocessing.start_processes(\n        distributed_main,\n        # need to give rank offset as 1 to cover the fact that the main\n        # process is rank 0, but that spawn() doesn't let you control rank:\n        # it always starts at 0\n        (main, cfg, kwargs),\n        nprocs=min(\n            torch.cuda.device_count(),\n            cfg.distributed_training.distributed_world_size - 1,\n        ),\n        join=False,\n        start_method=\"spawn\",\n    )\n\n    try:\n        # -1 because we offset by +1 inside distributed_main when using\n        # spawn_helper\n        retval = distributed_main(-1, main, cfg, kwargs)\n        spawncontext.join()\n        return retval\n    except (KeyboardInterrupt, Exception):\n        # weirdly KeyboardInterrupt is not an Exception\n        # propagate exceptions on the main node by killing workers\n        for p in spawncontext.processes:\n            if p.is_alive():\n                os.kill(p.pid, signal.SIGTERM)\n        raise\n\n\ndef call_main(cfg: MetaseqConfig, main, **kwargs):\n    if cfg.distributed_training.distributed_init_method is None:\n        infer_init_method(cfg.distributed_training)\n\n    if cfg.distributed_training.distributed_init_method is not None:\n        # distributed training\n        if not cfg.distributed_training.distributed_no_spawn:\n            start_rank = cfg.distributed_training.distributed_rank\n            cfg.distributed_training.distributed_rank = None  # assign automatically\n            kwargs[\"start_rank\"] = start_rank\n            return _spawn_helper(main, cfg, kwargs)\n        else:\n            return distributed_main(\n                cfg.distributed_training.device_id, main, cfg, kwargs\n            )\n    else:\n        # single GPU main\n        return main(cfg, **kwargs)\n\n\ndef new_groups(grouped_ranks: List[List[int]]):\n    groups = [dist.new_group(g) for g in grouped_ranks]\n    my_group_idx = _find_my_group_index(grouped_ranks)\n    return groups[my_group_idx]\n\n\ndef _find_my_group_index(grouped_ranks):\n    my_rank = get_global_rank()\n    for i, group in enumerate(grouped_ranks):\n        if my_rank in group:\n            return i\n    raise RuntimeError\n\n\ndef _find_my_group(grouped_ranks):\n    index = _find_my_group_index(grouped_ranks)\n    return grouped_ranks[index]\n\n\ndef get_rank(group):\n    return dist.get_rank(group=group)\n\n\ndef get_world_size(group):\n    if torch.distributed.is_initialized():\n        return dist.get_world_size(group=group)\n    else:\n        return 1\n\n\ndef get_global_group():\n    if torch.distributed.is_initialized():\n        if not hasattr(get_global_group, \"_global_group\"):\n            # ideally we could use torch.distributed.group.WORLD, but it seems\n            # to cause random NCCL hangs in some cases\n            get_global_group._global_group = dist.new_group()\n        return get_global_group._global_group\n    else:\n        return None\n\n\ndef get_global_rank():\n    if torch.distributed.is_initialized():\n        return torch.distributed.get_rank()\n    else:\n        return 0\n\n\ndef get_global_world_size():\n    if torch.distributed.is_initialized():\n        return torch.distributed.get_world_size()\n    else:\n        return 1\n\n\ndef get_data_parallel_group():\n    \"\"\"Get the data parallel group the caller rank belongs to.\"\"\"\n    global _USE_MEGATRON\n    if _USE_MEGATRON:\n        from metaseq.modules.megatron import mpu\n\n        return mpu.get_data_parallel_group()\n    else:\n        return get_global_group()\n\n\ndef get_data_parallel_rank():\n    \"\"\"Return my rank for the data parallel group.\"\"\"\n    dp_group = get_data_parallel_group()\n    if dp_group is not None:\n        return get_rank(dp_group)\n    else:\n        return get_global_rank()\n\n\ndef get_data_parallel_world_size():\n    \"\"\"Return world size for the data parallel group.\"\"\"\n    dp_group = get_data_parallel_group()\n    if dp_group is not None:\n        return get_world_size(dp_group)\n    else:\n        return get_global_world_size()\n\n\ndef get_model_parallel_group():\n    global _USE_MEGATRON\n    if _USE_MEGATRON:\n        from metaseq.modules.megatron import mpu\n\n        return mpu.get_tensor_model_parallel_group()\n    else:\n        return None\n\n\ndef get_model_parallel_rank():\n    \"\"\"Return my rank for the model parallel group.\"\"\"\n    return get_rank(get_model_parallel_group())\n\n\ndef get_model_parallel_world_size():\n    \"\"\"Return world size for the model parallel group.\"\"\"\n    return get_world_size(get_model_parallel_group())\n\n\ndef all_reduce(tensor, group, op=\"sum\"):\n    if op == \"sum\":\n        op = dist.ReduceOp.SUM\n    elif op == \"max\":\n        op = dist.ReduceOp.MAX\n    else:\n        raise NotImplementedError\n    dist.all_reduce(tensor, op=op, group=group)\n    return tensor\n\n\ndef broadcast(tensor, src, group):\n    dist.broadcast(tensor, src=src, group=group)\n\n\ndef all_to_all(tensor, group):\n    \"\"\"Perform an all-to-all operation on a 1D Tensor.\"\"\"\n    assert tensor.dim() == 1\n    split_count = get_world_size(group=group)\n    assert tensor.numel() % split_count == 0\n    output = torch.zeros_like(tensor)\n    dist.all_to_all_single(output, tensor, group=group)\n    return output\n\n\ndef all_gather(tensor, group, return_tensor=False):\n    \"\"\"Perform an all-gather operation.\"\"\"\n    world_size = get_world_size(group=group)\n    rank = get_rank(group=group)\n    tensor_list = [\n        tensor if i == rank else torch.empty_like(tensor) for i in range(world_size)\n    ]\n    dist.all_gather(tensor_list, tensor, group=group)\n    if return_tensor:\n        return torch.stack(tensor_list, dim=0)\n    else:\n        return tensor_list\n\n\ndef all_gather_list(data, group=None, max_size=16384):\n    \"\"\"Gathers arbitrary data from all nodes into a list.\n\n    Similar to :func:`~torch.distributed.all_gather` but for arbitrary Python\n    data. Note that *data* must be picklable and any CUDA tensors will be moved\n    to CPU and returned on CPU as well.\n\n    Args:\n        data (Any): data from the local worker to be gathered on other workers\n        group: group of the collective\n        max_size (int, optional): maximum size of the data to be gathered\n            across workers\n    \"\"\"\n    from metaseq import utils\n\n    if group is None:\n        group = get_global_group()\n    rank = get_rank(group=group)\n    world_size = get_world_size(group=group)\n\n    buffer_size = max_size * world_size\n    if (\n        not hasattr(all_gather_list, \"_buffer\")\n        or all_gather_list._buffer.numel() < buffer_size\n    ):\n        all_gather_list._buffer = torch.cuda.ByteTensor(buffer_size)\n        all_gather_list._cpu_buffer = torch.ByteTensor(max_size).pin_memory()\n    buffer = all_gather_list._buffer\n    buffer.zero_()\n    cpu_buffer = all_gather_list._cpu_buffer\n\n    data = utils.move_to_cpu(data)\n    enc = pickle.dumps(data)\n    enc_size = len(enc)\n    header_size = 4  # size of header that contains the length of the encoded data\n    size = header_size + enc_size\n    if size > max_size:\n        raise ValueError(\n            \"encoded data size ({}) exceeds max_size ({})\".format(size, max_size)\n        )\n\n    header = struct.pack(\">I\", enc_size)\n    cpu_buffer[:size] = torch.ByteTensor(list(header + enc))\n    start = rank * max_size\n    buffer[start : start + size].copy_(cpu_buffer[:size])\n\n    all_reduce(buffer, group=group)\n\n    buffer = buffer.cpu()\n    try:\n        result = []\n        for i in range(world_size):\n            out_buffer = buffer[i * max_size : (i + 1) * max_size]\n            (enc_size,) = struct.unpack(\">I\", bytes(out_buffer[:header_size].tolist()))\n            if enc_size > 0:\n                result.append(\n                    pickle.loads(\n                        bytes(out_buffer[header_size : header_size + enc_size].tolist())\n                    )\n                )\n        return result\n    except pickle.UnpicklingError:\n        raise Exception(\n            \"Unable to unpickle data from other workers. all_gather_list requires all \"\n            \"workers to enter the function together, so this error usually indicates \"\n            \"that the workers have fallen out of sync somehow. Workers can fall out of \"\n            \"sync if one of them runs out of memory, or if there are other conditions \"\n            \"in your training script that can cause one worker to finish an epoch \"\n            \"while other workers are still iterating over their portions of the data. \"\n            \"Try rerunning with --ddp-backend=legacy_ddp and see if that helps.\"\n        )\n\n\ndef all_reduce_dict(data: Mapping[str, Any], device, group) -> Dict[str, Any]:\n    \"\"\"\n    AllReduce a dictionary of values across workers. We separately\n    reduce items that are already on the device and items on CPU for\n    better performance.\n\n    Args:\n        data (Mapping[str, Any]): dictionary of data to all-reduce, but\n            cannot be a nested dictionary\n        device (torch.device): device for the reduction\n        group: group of the collective\n    \"\"\"\n    data_keys = list(data.keys())\n\n    # We want to separately reduce items that are already on the\n    # device and items on CPU for performance reasons.\n    cpu_data = OrderedDict()\n    device_data = OrderedDict()\n    for k in data_keys:\n        t = data[k]\n        if not torch.is_tensor(t):\n            cpu_data[k] = torch.tensor(t, dtype=torch.double)\n        elif t.device.type != device.type:\n            cpu_data[k] = t.to(dtype=torch.double)\n        else:\n            device_data[k] = t.to(dtype=torch.double)\n\n    def _all_reduce_dict(data: OrderedDict):\n        if len(data) == 0:\n            return data\n        buf = torch.cat([t.view(-1) for t in data.values()]).to(device=device)\n        all_reduce(buf, group=group)\n        split_buf = torch.split(buf, [t.numel() for t in data.values()])\n        reduced_data = [t.view_as(orig) for t, orig in zip(split_buf, data.values())]\n        return OrderedDict(zip(data.keys(), reduced_data))\n\n    cpu_data = _all_reduce_dict(cpu_data)\n    device_data = _all_reduce_dict(device_data)\n\n    def get_from_stack(key):\n        if key in cpu_data:\n            return cpu_data[key]\n        elif key in device_data:\n            return device_data[key]\n        raise KeyError\n\n    return OrderedDict([(key, get_from_stack(key)) for key in data_keys])\n\n\ndef broadcast_tensors(\n    tensors: Optional[List[torch.Tensor]],\n    src_rank: int,\n    group: object,\n    dist_device: Optional[torch.device] = None,\n) -> List[torch.Tensor]:\n    \"\"\"\n    Broadcasts a list of tensors without other (non-src) ranks needing to know\n    the dtypes/shapes of the tensors.\n    \"\"\"\n    if dist_device is None:\n        if torch.distributed.get_backend(group) == \"nccl\":\n            dist_device = torch.device(\"cuda\")\n        else:\n            dist_device = torch.device(\"cpu\")\n\n    # share metadata first to simplify transfer\n    is_src_rank = get_rank(group) == src_rank\n    if is_src_rank:\n        # We only want to communicate device type ie (cpu vs cuda) and not the index of cuda.\n        metadata = [\n            {\"size\": t.size(), \"dtype\": t.dtype, \"device\": torch.device(t.device.type)}\n            for t in tensors\n        ]\n        metadata = _broadcast_object_slow(metadata, src_rank, group, dist_device)\n    else:\n        metadata = _broadcast_object_slow(None, src_rank, group, dist_device)\n\n    out_tensors = []\n    for i, meta in enumerate(metadata):\n        if is_src_rank:\n            tensor = tensors[i]\n            broadcast(tensors[i].to(dist_device), src=src_rank, group=group)\n        else:\n            tensor = torch.zeros(\n                [meta[\"size\"].numel()], dtype=meta[\"dtype\"], device=dist_device\n            )\n            broadcast(tensor, src=src_rank, group=group)\n\n        tensor = tensor.view(meta[\"size\"]).to(meta[\"device\"])\n        out_tensors.append(tensor)\n    return out_tensors\n\n\ndef broadcast_object(\n    obj: Any,\n    src_rank: int,\n    group: object,\n    dist_device: Optional[torch.device] = None,\n) -> Any:\n    \"\"\"Broadcast an arbitrary Python object to other workers.\"\"\"\n    if dist_device is None:\n        if torch.distributed.get_backend(group) == \"nccl\":\n            dist_device = torch.device(\"cuda\")\n        else:\n            dist_device = torch.device(\"cpu\")\n\n    if get_rank(group) == src_rank:\n        # split the tensors from the non-tensors so we can broadcast them\n        # directly, avoiding unnecessary serialization/deserialization\n        tensors = []\n        obj = _split_tensors_from_obj(obj, tensors)\n        obj = _broadcast_object_slow(obj, src_rank, group, dist_device)\n        tensors = broadcast_tensors(tensors, src_rank, group, dist_device)\n    else:\n        obj = _broadcast_object_slow(None, src_rank, group, dist_device)\n        tensors = broadcast_tensors(None, src_rank, group, dist_device)\n    return _put_tensors_in_obj(obj, tensors)\n\n\ndef _broadcast_object_slow(\n    obj: Any,\n    src_rank: int,\n    group: object,\n    dist_device: torch.device,\n) -> Any:\n    if get_rank(group) == src_rank:\n        # Emit data\n        buffer = io.BytesIO()\n        torch.save(obj, buffer)\n        buffer = torch.ByteTensor(buffer.getbuffer()).to(dist_device)\n        length = torch.LongTensor([len(buffer)]).to(dist_device)\n        broadcast(length, src=src_rank, group=group)\n        broadcast(buffer, src=src_rank, group=group)\n    else:\n        # Fetch from the source\n        length = torch.LongTensor([0]).to(dist_device)\n        broadcast(length, src=src_rank, group=group)\n        buffer = torch.ByteTensor(int(length.item())).to(dist_device)\n        broadcast(buffer, src=src_rank, group=group)\n        buffer = io.BytesIO(buffer.cpu().numpy())\n        obj = torch.load(buffer, map_location=\"cpu\")\n    return obj\n\n\n@dataclass(frozen=True)\nclass _TensorPlaceholder:\n    index: int\n\n\ndef _split_tensors_from_obj(obj: Any, tensors: List[torch.Tensor]) -> Any:\n    if torch.is_tensor(obj):\n        placeholder = _TensorPlaceholder(index=len(tensors))\n        tensors.append(obj)\n        return placeholder\n    elif isinstance(obj, dict):\n        return {k: _split_tensors_from_obj(v, tensors) for k, v in obj.items()}\n    elif isinstance(obj, list):\n        return [_split_tensors_from_obj(v, tensors) for v in obj]\n    elif isinstance(obj, tuple):\n        return tuple(_split_tensors_from_obj(v, tensors) for v in obj)\n    elif isinstance(obj, set):\n        return {_split_tensors_from_obj(v, tensors) for v in obj}\n    else:\n        return obj\n\n\ndef _put_tensors_in_obj(obj: Any, tensors: List[torch.Tensor]) -> Any:\n    if isinstance(obj, _TensorPlaceholder):\n        return tensors[obj.index]\n    elif isinstance(obj, dict):\n        return {k: _put_tensors_in_obj(v, tensors) for k, v in obj.items()}\n    elif isinstance(obj, list):\n        return [_put_tensors_in_obj(v, tensors) for v in obj]\n    elif isinstance(obj, tuple):\n        return tuple(_put_tensors_in_obj(v, tensors) for v in obj)\n    elif isinstance(obj, set):\n        return {_put_tensors_in_obj(v, tensors) for v in obj}\n    else:\n        return obj\n",
        "metaseq/file_chunker_utils.py": "# Copyright (c) Meta Platforms, Inc. and affiliates. All Rights Reserved.\n#\n# This source code is licensed under the MIT license found in the\n# LICENSE file in the root directory of this source tree.\n\nimport os\nimport typing as tp\n\n\ndef _safe_readline(fd) -> str:\n    pos = fd.tell()\n    while True:\n        try:\n            return fd.readline()\n        except UnicodeDecodeError:\n            pos -= 1\n            fd.seek(pos)  # search where this character begins\n\n\ndef find_offsets(filename: str, num_chunks: int) -> tp.List[int]:\n    \"\"\"\n    given a file and a number of chuncks, find the offsets in the file\n    to be able to chunk around full lines.\n    \"\"\"\n    with open(filename, \"r\", encoding=\"utf-8\") as f:\n        size = os.fstat(f.fileno()).st_size\n        chunk_size = size // num_chunks\n        offsets = [0 for _ in range(num_chunks + 1)]\n        for i in range(1, num_chunks):\n            f.seek(chunk_size * i)\n            _safe_readline(f)\n            offsets[i] = f.tell()\n        offsets[-1] = size\n        return offsets\n\n\nclass ChunkLineIterator:\n    \"\"\"\n    Iterator to properly iterate over lines of a file chunck.\n    \"\"\"\n\n    def __init__(self, fd, start_offset: int, end_offset: int):\n        self._fd = fd\n        self._start_offset = start_offset\n        self._end_offset = end_offset\n\n    def __iter__(self) -> tp.Iterable[str]:\n        self._fd.seek(self._start_offset)\n        # next(f) breaks f.tell(), hence readline() must be used\n        line = _safe_readline(self._fd)\n        while line:\n            pos = self._fd.tell()\n            # f.tell() does not always give the byte position in the file\n            # sometimes it skips to a very large number\n            # it is unlikely that through a normal read we go from\n            # end bytes to end + 2**32 bytes (4 GB) and this makes it unlikely\n            # that the procedure breaks by the undeterministic behavior of\n            # f.tell()\n            if (\n                self._end_offset > 0\n                and pos > self._end_offset\n                and pos < self._end_offset + 2**32\n            ):\n                break\n            yield line\n            line = self._fd.readline()\n\n\nclass Chunker:\n    \"\"\"\n    contextmanager to read a chunck of a file line by line.\n    \"\"\"\n\n    def __init__(self, path: str, start_offset: int, end_offset: int):\n        self.path = path\n        self.start_offset = start_offset\n        self.end_offset = end_offset\n\n    def __enter__(self) -> ChunkLineIterator:\n        self.fd = open(self.path, \"r\", encoding=\"utf-8\")\n        return ChunkLineIterator(self.fd, self.start_offset, self.end_offset)\n\n    def __exit__(self, exc_type, exc_val, exc_tb) -> None:\n        self.fd.close()\n",
        "metaseq/file_io/__init__.py": "# Copyright (c) Meta Platforms, Inc. and affiliates. All Rights Reserved.\n#\n# This source code is licensed under the MIT license found in the\n# LICENSE file in the root directory of this source tree.\n\nfrom argparse import Namespace  # noqa: F401\nimport logging\n\nimport torch\n\nfrom omegaconf.dictconfig import DictConfig\nfrom metaseq.file_io.common import g_pathmgr as PathManager\n\n\n__all__ = [\n    \"PathManager\",\n]\n\n\nlogger = logging.getLogger(__file__)\n\n\ntry:\n    from .s3 import S3PathHandler  # noqa: E402\n\n    PathManager.register_handler(S3PathHandler())\nexcept KeyError:\n    pass\nexcept Exception:\n    logger.exception(\"Failed to register S3PathHandler. Try pip install boto3\")\n\n\ntry:\n    from .azure_blob import AzureBlobPathHandler  # noqa: E402\n\n    PathManager.register_handler(AzureBlobPathHandler())\nexcept ImportError:\n    logger.exception(\n        \"Failed to register AzureBlobPathHandler. Try pip install azure-storage-blob\"\n    )\nexcept Exception as e:\n    logger.exception(e)\n\n\ndef recursively_cast_dictconfigs(cfg):\n    if isinstance(cfg, DictConfig):\n        cfg = eval(str(cfg))\n    assert not isinstance(cfg, DictConfig)\n    if isinstance(cfg, dict):\n        return {k2: recursively_cast_dictconfigs(v2) for k2, v2 in cfg.items()}\n    else:\n        # Easy to support List, Tuple if needed\n        return cfg\n\n\ndef torch_load_cpu(path):\n    state = torch.load(path, map_location=torch.device(\"cpu\"))\n    # If model was trained with fp16, model from loaded state_dict can be moved to fp16\n    if not isinstance(state, dict):\n        return state\n    if \"cfg\" in state:\n        state[\"cfg\"] = recursively_cast_dictconfigs(state[\"cfg\"])\n\n    return state\n\n\ndef load_and_pop_last_optimizer_state(pth):\n    st = torch_load_cpu(pth)\n    st.pop(\"last_optimizer_state\", None)\n    return st\n",
        "metaseq/file_io/azure_blob.py": "# Copyright (c) Meta Platforms, Inc. and affiliates. All Rights Reserved.\n#\n# This source code is licensed under the MIT license found in the\n# LICENSE file in the root directory of this source tree.\n\nfrom abc import abstractmethod\nfrom datetime import datetime\nimport base64\nimport io\nimport logging\nimport os\nimport shutil\nimport time\nfrom typing import IO, Any, Dict, Generator, List, Optional, Tuple, Union\n\nfrom metaseq.file_io.common import file_lock, get_cache_dir, PathHandler\n\n\ntry:\n    import azure.core.exceptions as azure_exceptions\n    import azure.storage.blob as azure_blob\n\n    # Reduce noise by suppresssing HTTP logging in the client by default\n    logging.getLogger(\"azure.core.pipeline.policies.http_logging_policy\").setLevel(\n        logging.WARNING\n    )\nexcept ImportError:\n    azure_exceptions = None\n    azure_blob = None\n\n\nlogger = logging.getLogger(__name__)\n\n\nDEFAULT_CHUNK_SIZE = 4 * 1024 * 1024\n\n\nclass AzureBlobTokenProvider:\n    @abstractmethod\n    def get_sas_token(self, account: str) -> str:\n        \"\"\"\n        Returns a SAS token for the specified storage account\n        \"\"\"\n\n\nclass AzureBlobReader(io.RawIOBase):\n    \"\"\"\n    Implements a readonly file-like interface around the Azure BlobClient.\n    BlobClient provides an iterator (StorageStreamDownloader.chunks())\n    that lazily downloads parts of the blob into memory one fixed size\n    chunk at a time. This class handles reads by slicing parts of\n    the current chunk until it's fully consumed, and advancing\n    the iterator as needed until EOF.\n    \"\"\"\n\n    def __init__(\n        self,\n        client,  # azure_blob.BlobClient\n        chunk_size: int,\n    ):\n        stream = client.download_blob()\n        self._client = client\n        self._chunk_iter = stream.chunks()\n        self._chunk_size = chunk_size\n        self._chunk = None\n        self._chunk_pos = 0\n\n    def _next_chunk(self):\n        blob_name = self._client.blob_name\n        self._chunk = next(self._chunk_iter)\n        logger.info(\n            \"Read next chunk: blob_name=%s, length=%d\", blob_name, len(self._chunk)\n        )\n        self._chunk_pos = 0\n\n    def _get_chunk_data(self, size: int) -> bytes:\n        \"\"\"\n        Return up to `size` bytes from the currently loaded chunk. If there\n        isn't enough data remaining, just return what's available.\n        If this is the first read, or the current chunk has been read entirely,\n        make a single network request to download the next `chunk_size` bytes\n        and return data as described above.\n        \"\"\"\n        if size == 0:\n            return bytes()\n\n        try:\n            if self._chunk is None or self._chunk_pos >= len(self._chunk):\n                self._next_chunk()\n\n            data = self._chunk[self._chunk_pos : self._chunk_pos + size]\n            self._chunk_pos += len(data)\n            return data\n        except StopIteration:\n            return bytes()\n\n    def seekable(self):\n        return False\n\n    def seek(self, offset: int, whence: int = os.SEEK_SET) -> int:\n        raise io.UnsupportedOperation()\n\n    def tell(self):\n        raise io.UnsupportedOperation()\n\n    def readable(self):\n        return True\n\n    def read(self, size: int = -1) -> bytes:\n        if size < 0:\n            return self.readall()\n        return self._get_chunk_data(size)\n\n    def readall(self) -> bytes:\n        stream = io.BytesIO()\n        self.readinto(stream)\n        return stream.getvalue()\n\n    def readinto(self, stream):\n        size = self._chunk_size\n        data = self._get_chunk_data(size)\n        # Getting zero bytes back == EOF\n        while len(data) > 0:\n            stream.write(data)\n            data = self._get_chunk_data(size)\n\n    def writeable(self):\n        return False\n\n    def write(self, b: bytes):\n        raise io.UnsupportedOperation()\n\n    def truncate(self, size):\n        raise io.UnsupportedOperation()\n\n    def close(self):\n        pass\n\n\nclass AzureBlobWriter(io.RawIOBase):\n    \"\"\"\n    Provides a write-only file-like interface around the Azure BlobClient.\n    This class keeps a fixed size in-memory write buffer, uploading it\n    to Azure when it fills up or when flush() is called. Each flush()\n    sends a PutBlock request to Blob Storage including the buffered data\n    and a unique block id. The blob is finalized on close() with\n    a PutBlockList request specifying the block ids to commit.\n    \"\"\"\n\n    def __init__(\n        self,\n        client,  # azure_blob.BlobClient,\n        chunk_size: int,\n    ):\n        self._client = client\n        self._chunk_size = chunk_size\n        self._chunk_idx = -1\n        self._chunk = None\n        self._blocks = []\n\n    def _new_block_id(self):\n        self._chunk_idx += 1\n        # BlockBlobs and AppendBlobs have a maximum of 50_000 blocks\n        # This uses block indexes (00000 ... 49999) as block ids\n        # From the docs: block ids must be base64 strings, < 64 bytes in size\n        # and the same length for all blocks in the same blob\n        block_id = \"{0:05}\".format(self._chunk_idx).encode(\"utf-8\")\n        block_id = base64.urlsafe_b64encode(block_id)\n        return block_id.decode(\"utf-8\")\n\n    def _next_chunk(self):\n        self.flush()\n        self._chunk = io.BytesIO()\n\n    def _append_to_chunk(self, b: bytes):\n        if self._chunk is None or self._chunk.tell() >= self._chunk_size:\n            self._next_chunk()\n        return self._chunk.write(b)\n\n    def seekable(self):\n        return False\n\n    def seek(self, offset: int, whence: int = os.SEEK_SET) -> int:\n        raise io.UnsupportedOperation()\n\n    def tell(self):\n        raise io.UnsupportedOperation()\n\n    def readable(self):\n        return False\n\n    def read(self, size: int = -1) -> bytes:\n        raise io.UnsupportedOperation()\n\n    def readall(self) -> bytes:\n        raise io.UnsupportedOperation()\n\n    def readinto(self, stream):\n        raise io.UnsupportedOperation()\n\n    def writeable(self):\n        return True\n\n    def write(self, b: bytes):\n        return self._append_to_chunk(b)\n\n    def truncate(self, size):\n        raise io.UnsupportedOperation()\n\n    def flush(self):\n        if self._chunk is None or self._chunk.tell() == 0:\n            return\n\n        block_id = self._new_block_id()\n        block_length = self._chunk.tell()\n        self._chunk.seek(0)\n\n        logger.info(\n            \"Uploading a new block: blob_name=%s, block_id=%s, idx=%d, length=%d\",\n            self._client.blob_name,\n            block_id,\n            self._chunk_idx,\n            block_length,\n        )\n        self._client.stage_block(block_id, self._chunk, block_length)\n        self._blocks.append(azure_blob.BlobBlock(block_id=block_id))\n\n    def close(self):\n        self.flush()\n        if self._blocks:\n            logger.info(\n                \"Committing blocks: blob_name=%s, count=%d\",\n                self._client.blob_name,\n                len(self._blocks),\n            )\n            self._client.commit_block_list(self._blocks)\n\n    def __exit__(self, _, __, ___):\n        # Make sure close() is called to commit everything\n        self.close()\n\n\nENV_SAS_TOKEN = \"AZURE_STORAGE_SAS_TOKEN\"\n\n\nclass EnvironmentTokenProvider(AzureBlobTokenProvider):\n    def get_sas_token(self, _: str) -> str:\n        assert (\n            ENV_SAS_TOKEN in os.environ\n        ), f\"Missing required env variable: {ENV_SAS_TOKEN}\"\n        return os.environ[ENV_SAS_TOKEN]\n\n\nclass AzureBlobPathHandler(PathHandler):\n    \"\"\"\n    Support for Microsoft Azure Blob Storage\n    \"\"\"\n\n    SUPPORTED_PREFIXES = [\"az://\", \"blob://\"]\n    CACHE_SUBDIR_NAME = \"blob_cache\"\n\n    def __init__(\n        self,\n        token_provider: Optional[AzureBlobTokenProvider] = None,\n        cache_dir: Optional[str] = None,\n    ):\n        \"\"\"\n        Args:\n            token_provider (AzureBlobTokenProvider): provider used to generate SAS tokens\n                for authenticating with Blob Storage.\n            cache_dir (str): Local filesystem directory to use for caching. If None,\n                uses default from `file_io.get_cache_dir()`.\n        \"\"\"\n        self.token_provider = token_provider or EnvironmentTokenProvider()\n        self.cache_dir = cache_dir\n\n    def _get_supported_prefixes(self) -> List[str]:\n        return self.SUPPORTED_PREFIXES\n\n    def _parse_uri(self, uri: str) -> Tuple[str, str]:\n        \"\"\"\n        Parses a \"blob://<account>/<container>/<path>\" URI into components\n            (`account`, `container`, `path`)\n        Args:\n            uri (str): A blob:// URI.\n        Returns:\n            account (str): the storage account.\n            container (str): the storage container.\n            path (str): the blob path.\n        \"\"\"\n        for prefix in self.SUPPORTED_PREFIXES:\n            if not uri.startswith(prefix):\n                continue\n            splits = uri.replace(prefix, \"\").split(\"/\")\n            account = splits[0]\n            container = splits[1]\n            path = \"/\".join(splits[2:])\n            return account, container, path\n\n        raise ValueError(f\"Unsupported URI: {uri}\")\n\n    def _get_service_uri(\n        self, account: str, container: str, blob_path: str, include_auth: bool = False\n    ) -> str:\n        account_uri = f\"https://{account}.blob.core.windows.net\"\n        uri = os.path.join(account_uri, container, blob_path)\n\n        if include_auth:\n            sas_token = self.token_provider.get_sas_token(account)\n            uri += \"?\" + sas_token\n\n        return uri\n\n    def _get_client(self, account: str):\n        if not hasattr(self, \"client\"):\n            account_uri = f\"https://{account}.blob.core.windows.net\"\n            sas_token = self.token_provider.get_sas_token(account)\n            client = azure_blob.BlobServiceClient(account_uri, credential=sas_token)\n            self.client = client\n\n        return self.client\n\n    def _get_blob_properties(self, path: str) -> Optional[Dict]:\n        account, container, blob = self._parse_uri(path)\n        client = self._get_client(account)\n        props = client.get_blob_client(\n            container=container, blob=blob\n        ).get_blob_properties()\n\n        return {k: v for k, v in props.items()}\n\n    def _enumerate_blobs(self, path: str) -> Generator[str, None, None]:\n        account, container, path_prefix = self._parse_uri(path)\n        client = self._get_client(account)\n        return client.get_container_client(container).list_blobs(\n            name_starts_with=path_prefix\n        )\n\n    def _exists(self, path: str, **kwargs: Any) -> bool:\n        \"\"\"\n        Checks if there is a resource at the given URI.\n        Args:\n            path (str): A URI supported by this PathHandler\n        Returns:\n            bool: true if the path exists\n        \"\"\"\n        self._check_kwargs(kwargs)\n        return self._isfile(path) or self._isdir(path)\n\n    def _isfile(self, path: str, **kwargs: Any) -> bool:\n        \"\"\"\n        Checks if the resource at the given URI is a file.\n        Args:\n            path (str): A URI supported by this PathHandler\n        Returns:\n            bool: true if the path is a file\n        \"\"\"\n        self._check_kwargs(kwargs)\n\n        try:\n            props = self._get_blob_properties(path)\n            return props is not None\n        except azure_exceptions.ResourceNotFoundError:\n            return False\n        except azure_exceptions.AzureError as e:\n            logger.exception(e)\n            return False\n\n    def _isdir(self, path: str, **kwargs: Any) -> bool:\n        \"\"\"\n        Checks if the resource at the given URI is a directory.\n        Args:\n            path (str): A URI supported by this PathHandler\n        Returns:\n            bool: true if the path is a directory\n        \"\"\"\n        self._check_kwargs(kwargs)\n\n        _, _, dirpath = self._parse_uri(path)\n\n        try:\n            # Enumeration should find at least 1 longer child path\n            blob = next(self._enumerate_blobs(path))\n            return len(blob.name) > len(dirpath)\n        except StopIteration:\n            return False\n        except azure_exceptions.AzureError as e:\n            logger.exception(e)\n            return False\n\n    def _ls(self, path: str, **kwargs: Any) -> List[str]:\n        \"\"\"\n        List the contents of the directory at the provided URI.\n        Args:\n            path (str): A URI supported by this PathHandler\n        Returns:\n            List[str]: list of contents in given path\n        \"\"\"\n        self._check_kwargs(kwargs)\n\n        return [blob.name for blob in self._enumerate_blobs(path)]\n\n    def _local_cache_path(self, path: str):\n        \"\"\"\n        Helper that returns a local cache path for a given uri.\n        Args:\n            path (str): A URI supported by this PathHandler.\n        Returns:\n            local_cache_path (str): a file path which exists on the local file system,\n            in a cache directory.\n        \"\"\"\n        _, _, blob = self._parse_uri(path)\n        return os.path.join(get_cache_dir(self.cache_dir), self.CACHE_SUBDIR_NAME, blob)\n\n    def _get_local_path(self, path: str, force: bool = False, **kwargs: Any) -> str:\n        \"\"\"\n        Get a filepath which is compatible with native Python I/O such as `open`\n        and `os.path`.\n        If URI points to a remote resource, this function may download and cache\n        the resource to local disk. In this case, the cache stays on filesystem\n        (under `file_io.get_cache_dir()`) and will be used by a different run.\n        Therefore this function is meant to be used with read-only resources.\n        Args:\n            path (str): A URI supported by this PathHandler\n        Returns:\n            local_path (str): a file path which exists on the local file system\n        \"\"\"\n        self._check_kwargs(kwargs)\n\n        assert self._exists(path), f\"File not found: {path}\"\n        account, container, _ = self._parse_uri(path)\n        result_path = self._local_cache_path(path)\n\n        # TODO: this can be parallelized for directories\n        for blob in self._enumerate_blobs(path):\n            blob_path = os.path.join(\"az://\", account, container, blob.name)\n            self._get_local_path_single(blob_path, force=force)\n\n        return result_path\n\n    def _get_local_path_single(self, path: str, force: bool = False) -> str:\n        local_path = self._local_cache_path(path)\n        with file_lock(local_path):\n            if os.path.exists(local_path):\n                # Redownload if remote is newer.\n                props = self._get_blob_properties(path)\n                remote_modified = props[\"last_modified\"]\n                local_modified = datetime.fromtimestamp(\n                    os.path.getmtime(local_path)\n                ).astimezone()\n\n                if remote_modified <= local_modified and not force:\n                    logger.info(\n                        \"URL {} was already cached in {}\".format(path, local_path)\n                    )\n                    return local_path\n\n            logger.info(\"Caching {} ...\".format(path))\n            tmp = local_path + \".tmp\"\n            # clean-up tmp if found, because if tmp exists, it must be a dirty\n            # result of a previously process that didn't cleanup itself.\n            if os.path.isfile(tmp):\n                os.unlink(tmp)\n\n            account, container, blob_path = self._parse_uri(path)\n            client = self._get_client(account)\n            try:\n                blob_stream = client.get_blob_client(\n                    container=container, blob=blob_path\n                ).download_blob()\n\n                with open(tmp, \"wb\") as f_tmp:\n                    blob_stream.readinto(f_tmp)\n\n                shutil.move(tmp, local_path)\n            finally:\n                try:\n                    os.unlink(tmp)\n                except Exception:\n                    pass\n\n            logger.info(\"URL {} cached in {}\".format(path, local_path))\n            return local_path\n\n    def _open(\n        self,\n        path: str,\n        mode: str = \"rb\",\n        buffering: int = -1,\n        **kwargs: Any,\n    ) -> Union[IO[str], IO[bytes]]:\n        \"\"\"\n        Open a stream to a URI, similar to the built-in `open`.\n        Args:\n            path (str): A URI supported by this PathHandler\n            mode (str): Specifies the mode in which the file is opened. It defaults\n                to 'r'.\n            buffering (int): An optional integer used to set the buffering policy.\n                Pass 0 to switch buffering off and an integer >= 1 to indicate the\n                size in bytes of a fixed-size chunk buffer. When no buffering\n                argument is given, the default buffering policy depends on the\n                underlying I/O implementation.\n        Returns:\n            file: a file-like object.\n        \"\"\"\n        self._check_kwargs(kwargs)\n        assert mode in (\"rb\", \"wb\"), \"Supported modes: rb, wb\"\n\n        chunk_size = buffering if buffering > 0 else DEFAULT_CHUNK_SIZE\n\n        account, container, blob_path = self._parse_uri(path)\n        blob_client = self._get_client(account).get_blob_client(\n            container=container,\n            blob=blob_path,\n        )\n\n        logger.info(\"Opening blob: path=%s, mode=%s\", path, mode)\n\n        if \"r\" in mode:\n            return AzureBlobReader(blob_client, chunk_size)\n        if \"w\" in mode:\n            return AzureBlobWriter(blob_client, chunk_size)\n\n        raise ValueError(\"Invalid mode: \" + mode)\n\n    def _mkdirs(self, path: str, **kwargs: Any) -> None:\n        \"\"\"\n        No-op since blob storage has a flat structure and no explicit notion of directories.\n        \"\"\"\n        pass\n\n    def _copy_from_local(\n        self, local_path: str, dst_path: str, overwrite: bool = False, **kwargs: Any\n    ) -> bool:\n        \"\"\"\n        Copies a local file to the specified URI.\n        If the URI is another local path, this should be functionally identical\n        to copy.\n        Args:\n            local_path (str): a file path which exists on the local file system\n            dst_path (str): A URI supported by this PathHandler\n            overwrite (bool): Bool flag for forcing overwrite of existing URI\n        Returns:\n            status (bool): True on success\n        \"\"\"\n        self._check_kwargs(kwargs)\n\n        # Just checking this to avoid expensive API calls in self._isdir().\n        if local_path.endswith(\"/\") or dst_path.endswith(\"/\"):\n            raise NotImplementedError(\n                \"AzureBlobPathHandler does not currently support uploading directories\"\n            )\n\n        account, container, blob_path = self._parse_uri(dst_path)\n        blob_client = self._get_client(account).get_blob_client(\n            container=container, blob=blob_path\n        )\n\n        with open(local_path, \"rb\") as src_file:\n            src_length = os.fstat(src_file.fileno()).st_size\n            try:\n                blob_client.upload_blob(src_file, length=src_length)\n                return True\n            except azure_exceptions.AzureError as e:\n                logger.error(f\"Error in file upload - {str(e)}\")\n                return False\n\n    def _copy(\n        self, src_path: str, dst_path: str, overwrite: bool = False, **kwargs: Any\n    ) -> bool:\n        \"\"\"\n        Copies a source path to a destination path.\n        Args:\n            src_path (str): A URI supported by this PathHandler\n            dst_path (str): A URI supported by this PathHandler\n            overwrite (bool): Bool flag for forcing overwrite of existing file\n        Returns:\n            status (bool): True on success\n        \"\"\"\n        self._check_kwargs(kwargs)\n\n        src_account, src_container, src_blob_path = self._parse_uri(src_path)\n        dst_account, dst_container, dst_blob_path = self._parse_uri(dst_path)\n        src_uri = self._get_service_uri(\n            src_account,\n            src_container,\n            src_blob_path,\n            include_auth=True,\n        )\n        dst_blob = self._get_client(dst_account).get_blob_client(\n            container=dst_container, blob=dst_blob_path\n        )\n\n        try:\n            _ = dst_blob.start_copy_from_url(src_uri)\n            return self._wait_for_copy(dst_blob)\n        except azure_exceptions.AzureError as e:\n            logger.exception(e)\n            return False\n\n    def _wait_for_copy(\n        self,\n        blob,  # azure_blob.BlobClient\n        timeout_secs=1800,\n        polling_secs=30,\n    ) -> bool:\n        props = blob.get_blob_properties()\n        deadline = int(datetime.utcnow().timestamp()) + timeout_secs\n\n        while (\n            props.copy.status == \"pending\"\n            and int(datetime.utcnow().timestamp()) < deadline\n        ):\n            time.sleep(polling_secs)\n            props = blob.get_blob_properties()\n\n        return props.copy.status != \"pending\"\n\n    def _rm(self, path: str, **kwargs: Any) -> None:\n        \"\"\"\n        Remove the file (not directory) at the provided URI.\n        Args:\n            path (str): A URI supported by this PathHandler\n        \"\"\"\n        self._check_kwargs(kwargs)\n\n        account, container, blob_path = self._parse_uri(path)\n        blob_client = self._get_client(account).get_blob_client(\n            container=container, blob=blob_path\n        )\n\n        try:\n            blob_client.delete_blob()\n        except azure_exceptions.AzureError as e:\n            raise OSError(\n                f\"Error in rm path {path} - \" f\"{type(e).__name__}: {e}\"\n            ) from e\n\n    def _close(self):\n        \"\"\"\n        Closes any sockets the Azure client may have opened.\n        \"\"\"\n        if hasattr(self, \"client\"):\n            self.client.close()\n",
        "metaseq/file_io/common/__init__.py": "from collections import OrderedDict\nimport concurrent\nimport errno\nfrom io import IOBase\nimport logging\nimport os\nimport shutil\nimport tempfile\nimport traceback\n\nimport portalocker\n\nfrom typing import (\n    IO,\n    Any,\n    Callable,\n    Dict,\n    List,\n    MutableMapping,\n    Optional,\n    Set,\n    Union,\n)\n\nfrom metaseq.file_io.common.non_blocking_io import NonBlockingIOManager\n\n\n__all__ = [\n    \"PathManager\",\n    \"get_cache_dir\",\n    \"file_lock\",\n]\n\n\ndef get_cache_dir(cache_dir: Optional[str] = None) -> str:\n    \"\"\"\n    Returns a default directory to cache static files\n    (usually downloaded from Internet), if None is provided.\n\n    Args:\n        cache_dir (None or str): if not None, will be returned as is.\n            If None, returns the default cache directory as:\n\n        1) $FVCORE_CACHE, if set\n        2) otherwise ~/.torch/iopath_cache\n    \"\"\"\n    if cache_dir is None:\n        cache_dir = os.path.expanduser(\n            os.getenv(\"FVCORE_CACHE\", \"~/.torch/iopath_cache\")\n        )\n    try:\n        g_pathmgr.mkdirs(cache_dir)\n        assert os.access(cache_dir, os.W_OK)\n    except (OSError, AssertionError):\n        tmp_dir = os.path.join(tempfile.gettempdir(), \"iopath_cache\")\n        logger = logging.getLogger(__name__)\n        logger.warning(f\"{cache_dir} is not accessible! Using {tmp_dir} instead!\")\n        cache_dir = tmp_dir\n    return cache_dir\n\n\ndef file_lock(path: str):  # type: ignore\n    \"\"\"\n    A file lock. Once entered, it is guaranteed that no one else holds the\n    same lock. Others trying to enter the lock will block for 30 minutes and\n    raise an exception.\n\n    This is useful to make sure workers don't cache files to the same location.\n\n    Args:\n        path (str): a path to be locked. This function will create a lock named\n            `path + \".lock\"`\n\n    Examples:\n\n        filename = \"/path/to/file\"\n        with file_lock(filename):\n            if not os.path.isfile(filename):\n                do_create_file()\n    \"\"\"\n    dirname = os.path.dirname(path)\n    try:\n        os.makedirs(dirname, exist_ok=True)\n    except OSError:\n        # makedir is not atomic. Exceptions can happen when multiple workers try\n        # to create the same dir, despite exist_ok=True.\n        # When this happens, we assume the dir is created and proceed to creating\n        # the lock. If failed to create the directory, the next line will raise\n        # exceptions.\n        pass\n    return portalocker.Lock(path + \".lock\", timeout=3600)  # type: ignore\n\n\nclass PathHandler:\n    \"\"\"\n    PathHandler is a base class that defines common I/O functionality for a URI\n    protocol. It routes I/O for a generic URI which may look like \"protocol://*\"\n    or a canonical filepath \"/foo/bar/baz\".\n    \"\"\"\n\n    _strict_kwargs_check = True\n\n    def __init__(\n        self,\n        async_executor: Optional[concurrent.futures.Executor] = None,\n    ) -> None:\n        \"\"\"\n        When registering a `PathHandler`, the user can optionally pass in a\n        `Executor` to run the asynchronous file operations.\n        NOTE: For regular non-async operations of `PathManager`, there is\n        no need to pass `async_executor`.\n\n        Args:\n            async_executor (optional `Executor`): Used for async file operations.\n                Usage:\n                ```\n                    path_handler = NativePathHandler(async_executor=exe)\n                    path_manager.register_handler(path_handler)\n                ```\n        \"\"\"\n        super().__init__()\n        # pyre-fixme[4]: Attribute must be annotated.\n        self._non_blocking_io_manager = None\n        self._non_blocking_io_executor = async_executor\n\n    def _check_kwargs(self, kwargs: Dict[str, Any]) -> None:\n        \"\"\"\n        Checks if the given arguments are empty. Throws a ValueError if strict\n        kwargs checking is enabled and args are non-empty. If strict kwargs\n        checking is disabled, only a warning is logged.\n\n        Args:\n            kwargs (Dict[str, Any])\n        \"\"\"\n        if self._strict_kwargs_check:\n            if len(kwargs) > 0:\n                raise ValueError(\"Unused arguments: {}\".format(kwargs))\n        else:\n            logger = logging.getLogger(__name__)\n            for k, v in kwargs.items():\n                logger.warning(\"[PathManager] {}={} argument ignored\".format(k, v))\n\n    def _get_supported_prefixes(self) -> List[str]:\n        \"\"\"\n        Returns:\n            List[str]: the list of URI prefixes this PathHandler can support\n        \"\"\"\n        raise NotImplementedError()\n\n    def _get_local_path(self, path: str, force: bool = False, **kwargs: Any) -> str:\n        \"\"\"\n        Get a filepath which is compatible with native Python I/O such as `open`\n        and `os.path`.\n\n        If URI points to a remote resource, this function may download and cache\n        the resource to local disk. In this case, the cache stays on filesystem\n        (under `file_io.get_cache_dir()`) and will be used by a different run.\n        Therefore this function is meant to be used with read-only resources.\n\n        Args:\n            path (str): A URI supported by this PathHandler\n            force(bool): Forces a download from backend if set to True.\n\n        Returns:\n            local_path (str): a file path which exists on the local file system\n        \"\"\"\n        raise NotImplementedError()\n\n    def _copy_from_local(\n        self, local_path: str, dst_path: str, overwrite: bool = False, **kwargs: Any\n    ) -> None:\n        \"\"\"\n        Copies a local file to the specified URI.\n\n        If the URI is another local path, this should be functionally identical\n        to copy.\n\n        Args:\n            local_path (str): a file path which exists on the local file system\n            dst_path (str): A URI supported by this PathHandler\n            overwrite (bool): Bool flag for forcing overwrite of existing URI\n\n        Returns:\n            status (bool): True on success\n        \"\"\"\n        raise NotImplementedError()\n\n    def _open(\n        self, path: str, mode: str = \"r\", buffering: int = -1, **kwargs: Any\n    ) -> Union[IO[str], IO[bytes]]:\n        \"\"\"\n        Open a stream to a URI, similar to the built-in `open`.\n\n        Args:\n            path (str): A URI supported by this PathHandler\n            mode (str): Specifies the mode in which the file is opened. It defaults\n                to 'r'.\n            buffering (int): An optional integer used to set the buffering policy.\n                Pass 0 to switch buffering off and an integer >= 1 to indicate the\n                size in bytes of a fixed-size chunk buffer. When no buffering\n                argument is given, the default buffering policy depends on the\n                underlying I/O implementation.\n\n        Returns:\n            file: a file-like object.\n        \"\"\"\n        raise NotImplementedError()\n\n    def _opena(\n        self,\n        path: str,\n        mode: str = \"r\",\n        callback_after_file_close: Optional[Callable[[None], None]] = None,\n        buffering: int = -1,\n        **kwargs: Any,\n    ) -> IOBase:\n        \"\"\"\n        Open a file with asynchronous methods. `f.write()` calls will be dispatched\n        asynchronously such that the main program can continue running.\n        `f.read()` is an async method that has to run in an asyncio event loop.\n\n        NOTE: Writes to the same path are serialized so they are written in\n        the same order as they were called but writes to distinct paths can\n        happen concurrently.\n\n        Usage (write, default / without callback function):\n            for n in range(50):\n                results = run_a_large_task(n)\n                # `f` is a file-like object with asynchronous methods\n                with path_manager.opena(uri, \"w\") as f:\n                    f.write(results)            # Runs in separate thread\n                # Main process returns immediately and continues to next iteration\n            path_manager.async_close()\n\n        Usage (write, advanced / with callback function):\n            # To asynchronously write to storage:\n            def cb():\n                path_manager.copy_from_local(\n                    \"checkpoint.pt\", uri\n                )\n            f = pm.opena(\"checkpoint.pt\", \"wb\", callback_after_file_close=cb)\n            torch.save({...}, f)\n            f.close()\n\n        Usage (read):\n            async def my_function():\n              return await path_manager.opena(uri, \"r\").read()\n\n        Args:\n            ...same args as `_open`...\n            callback_after_file_close (Callable): An optional argument that can\n                be passed to perform operations that depend on the asynchronous\n                writes being completed. The file is first written to the local\n                disk and then the callback is executed.\n            buffering (int): An optional argument to set the buffer size for\n                buffered asynchronous writing.\n\n        Returns:\n            file: a file-like object with asynchronous methods.\n        \"\"\"\n        # Restrict mode until `NonBlockingIO` has async read feature.\n        valid_modes = {\"w\", \"a\", \"b\"}\n        if not all(m in valid_modes for m in mode):\n            raise ValueError(f\"`opena` mode must be write or append for path {path}\")\n\n        # TODO: Each `PathHandler` should set its own `self._buffered`\n        # parameter and pass that in here. Until then, we assume no\n        # buffering for any storage backend.\n        if not self._non_blocking_io_manager:\n            self._non_blocking_io_manager = NonBlockingIOManager(\n                buffered=False,\n                executor=self._non_blocking_io_executor,\n            )\n\n        try:\n            return self._non_blocking_io_manager.get_non_blocking_io(\n                path=self._get_path_with_cwd(path),\n                io_obj=self._open(path, mode, **kwargs),\n                callback_after_file_close=callback_after_file_close,\n                buffering=buffering,\n            )\n        except ValueError:\n            # When `_strict_kwargs_check = True`, then `open_callable`\n            # will throw a `ValueError`. This generic `_opena` function\n            # does not check the kwargs since it may include any `_open`\n            # args like `encoding`, `ttl`, `has_user_data`, etc.\n            logger = logging.getLogger(__name__)\n            logger.exception(\n                \"An exception occurred in `NonBlockingIOManager`. This \"\n                \"is most likely due to invalid `opena` args. Make sure \"\n                \"they match the `open` args for the `PathHandler`.\"\n            )\n            # pyre-fixme[7]: Expected `Union[IO[bytes], IO[str]]` but got implicit\n            #  return value of `None`.\n            self._async_close()\n\n    def _async_join(self, path: Optional[str] = None, **kwargs: Any) -> bool:\n        \"\"\"\n        Ensures that desired async write threads are properly joined.\n\n        Args:\n            path (str): Pass in a file path to wait until all asynchronous\n                activity for that path is complete. If no path is passed in,\n                then this will wait until all asynchronous jobs are complete.\n\n        Returns:\n            status (bool): True on success\n        \"\"\"\n        if not self._non_blocking_io_manager:\n            logger = logging.getLogger(__name__)\n            logger.warning(\n                \"This is an async feature. No threads to join because \"\n                \"`opena` was not used.\"\n            )\n        self._check_kwargs(kwargs)\n        return self._non_blocking_io_manager._join(\n            self._get_path_with_cwd(path) if path else None\n        )\n\n    def _async_close(self, **kwargs: Any) -> bool:\n        \"\"\"\n        Closes the thread pool used for the asynchronous operations.\n\n        Returns:\n            status (bool): True on success\n        \"\"\"\n        if not self._non_blocking_io_manager:\n            logger = logging.getLogger(__name__)\n            logger.warning(\n                \"This is an async feature. No threadpool to close because \"\n                \"`opena` was not used.\"\n            )\n        self._check_kwargs(kwargs)\n        return self._non_blocking_io_manager._close_thread_pool()\n\n    def _copy(\n        self, src_path: str, dst_path: str, overwrite: bool = False, **kwargs: Any\n    ) -> bool:\n        \"\"\"\n        Copies a source path to a destination path.\n\n        Args:\n            src_path (str): A URI supported by this PathHandler\n            dst_path (str): A URI supported by this PathHandler\n            overwrite (bool): Bool flag for forcing overwrite of existing file\n\n        Returns:\n            status (bool): True on success\n        \"\"\"\n        raise NotImplementedError()\n\n    def _mv(self, src_path: str, dst_path: str, **kwargs: Any) -> bool:\n        \"\"\"\n        Moves (renames) a source path to a destination path.\n\n        Args:\n            src_path (str): A URI supported by this PathHandler\n            dst_path (str): A URI supported by this PathHandler\n\n        Returns:\n            status (bool): True on success\n        \"\"\"\n        raise NotImplementedError()\n\n    def _exists(self, path: str, **kwargs: Any) -> bool:\n        \"\"\"\n        Checks if there is a resource at the given URI.\n\n        Args:\n            path (str): A URI supported by this PathHandler\n\n        Returns:\n            bool: true if the path exists\n        \"\"\"\n        raise NotImplementedError()\n\n    def _isfile(self, path: str, **kwargs: Any) -> bool:\n        \"\"\"\n        Checks if the resource at the given URI is a file.\n\n        Args:\n            path (str): A URI supported by this PathHandler\n\n        Returns:\n            bool: true if the path is a file\n        \"\"\"\n        raise NotImplementedError()\n\n    def _isdir(self, path: str, **kwargs: Any) -> bool:\n        \"\"\"\n        Checks if the resource at the given URI is a directory.\n\n        Args:\n            path (str): A URI supported by this PathHandler\n\n        Returns:\n            bool: true if the path is a directory\n        \"\"\"\n        raise NotImplementedError()\n\n    def _ls(self, path: str, **kwargs: Any) -> List[str]:\n        \"\"\"\n        List the contents of the directory at the provided URI.\n\n        Args:\n            path (str): A URI supported by this PathHandler\n\n        Returns:\n            List[str]: list of contents in given path\n        \"\"\"\n        raise NotImplementedError()\n\n    def _mkdirs(self, path: str, **kwargs: Any) -> None:\n        \"\"\"\n        Recursive directory creation function. Like mkdir(), but makes all\n        intermediate-level directories needed to contain the leaf directory.\n        Similar to the native `os.makedirs`.\n\n        Args:\n            path (str): A URI supported by this PathHandler\n        \"\"\"\n        raise NotImplementedError()\n\n    def _rm(self, path: str, **kwargs: Any) -> None:\n        \"\"\"\n        Remove the file (not directory) at the provided URI.\n\n        Args:\n            path (str): A URI supported by this PathHandler\n        \"\"\"\n        raise NotImplementedError()\n\n    def _symlink(self, src_path: str, dst_path: str, **kwargs: Any) -> bool:\n        \"\"\"\n        Symlink the src_path to the dst_path\n\n        Args:\n            src_path (str): A URI supported by this PathHandler to symlink from\n            dst_path (str): A URI supported by this PathHandler to symlink to\n        \"\"\"\n        raise NotImplementedError()\n\n    def _set_cwd(self, path: Union[str, None], **kwargs: Any) -> bool:\n        \"\"\"\n        Set the current working directory. PathHandler classes prepend the cwd\n        to all URI paths that are handled.\n\n        Args:\n            path (str) or None: A URI supported by this PathHandler. Must be a valid\n                absolute path or None to set the cwd to None.\n\n        Returns:\n            bool: true if cwd was set without errors\n        \"\"\"\n        raise NotImplementedError()\n\n    def _get_path_with_cwd(self, path: str) -> str:\n        \"\"\"\n        Default implementation. PathHandler classes that provide a `_set_cwd`\n        feature should also override this `_get_path_with_cwd` method.\n\n        Args:\n            path (str): A URI supported by this PathHandler.\n\n        Returns:\n            path (str): Full path with the cwd attached.\n        \"\"\"\n        return path\n\n\nclass NativePathHandler(PathHandler):\n    \"\"\"\n    Handles paths that can be accessed using Python native system calls. This\n    handler uses `open()` and `os.*` calls on the given path.\n    \"\"\"\n\n    # pyre-fixme[4]: Attribute must be annotated.\n    _cwd = None\n\n    def __init__(\n        self,\n        async_executor: Optional[concurrent.futures.Executor] = None,\n    ) -> None:\n        super().__init__(async_executor)\n\n    def _get_local_path(self, path: str, force: bool = False, **kwargs: Any) -> str:\n        self._check_kwargs(kwargs)\n        return os.fspath(path)\n\n    def _copy_from_local(\n        self, local_path: str, dst_path: str, overwrite: bool = False, **kwargs: Any\n    ) -> None:\n        self._check_kwargs(kwargs)\n        local_path = self._get_path_with_cwd(local_path)\n        dst_path = self._get_path_with_cwd(dst_path)\n        assert self._copy(\n            src_path=local_path, dst_path=dst_path, overwrite=overwrite, **kwargs\n        )\n\n    def _open(\n        self,\n        path: str,\n        mode: str = \"r\",\n        buffering: int = -1,\n        encoding: Optional[str] = None,\n        errors: Optional[str] = None,\n        newline: Optional[str] = None,\n        closefd: bool = True,\n        # pyre-fixme[24]: Generic type `Callable` expects 2 type parameters.\n        opener: Optional[Callable] = None,\n        **kwargs: Any,\n    ) -> Union[IO[str], IO[bytes]]:\n        \"\"\"\n        Open a path.\n\n        Args:\n            path (str): A URI supported by this PathHandler\n            mode (str): Specifies the mode in which the file is opened. It defaults\n                to 'r'.\n            buffering (int): An optional integer used to set the buffering policy.\n                Pass 0 to switch buffering off and an integer >= 1 to indicate the\n                size in bytes of a fixed-size chunk buffer. When no buffering\n                argument is given, the default buffering policy works as follows:\n                    * Binary files are buffered in fixed-size chunks; the size of\n                    the buffer is chosen using a heuristic trying to determine the\n                    underlying device’s “block size” and falling back on\n                    io.DEFAULT_BUFFER_SIZE. On many systems, the buffer will\n                    typically be 4096 or 8192 bytes long.\n            encoding (Optional[str]): the name of the encoding used to decode or\n                encode the file. This should only be used in text mode.\n            errors (Optional[str]): an optional string that specifies how encoding\n                and decoding errors are to be handled. This cannot be used in binary\n                mode.\n            newline (Optional[str]): controls how universal newlines mode works\n                (it only applies to text mode). It can be None, '', '\\n', '\\r',\n                and '\\r\\n'.\n            closefd (bool): If closefd is False and a file descriptor rather than\n                a filename was given, the underlying file descriptor will be kept\n                open when the file is closed. If a filename is given closefd must\n                be True (the default) otherwise an error will be raised.\n            opener (Optional[Callable]): A custom opener can be used by passing\n                a callable as opener. The underlying file descriptor for the file\n                object is then obtained by calling opener with (file, flags).\n                opener must return an open file descriptor (passing os.open as opener\n                results in functionality similar to passing None).\n\n            See https://docs.python.org/3/library/functions.html#open for details.\n\n        Returns:\n            file: a file-like object.\n        \"\"\"\n        self._check_kwargs(kwargs)\n        return open(  # type: ignore\n            self._get_path_with_cwd(path),\n            mode,\n            buffering=buffering,\n            encoding=encoding,\n            errors=errors,\n            newline=newline,\n            closefd=closefd,\n            opener=opener,\n        )\n\n    def _copy(\n        self, src_path: str, dst_path: str, overwrite: bool = False, **kwargs: Any\n    ) -> bool:\n        \"\"\"\n        Copies a source path to a destination path.\n\n        Args:\n            src_path (str): A URI supported by this PathHandler\n            dst_path (str): A URI supported by this PathHandler\n            overwrite (bool): Bool flag for forcing overwrite of existing file\n\n        Returns:\n            status (bool): True on success\n        \"\"\"\n        self._check_kwargs(kwargs)\n        src_path = self._get_path_with_cwd(src_path)\n        dst_path = self._get_path_with_cwd(dst_path)\n        if os.path.exists(dst_path) and not overwrite:\n            logger = logging.getLogger(__name__)\n            logger.error(\"Destination file {} already exists.\".format(dst_path))\n            return False\n\n        try:\n            shutil.copyfile(src_path, dst_path)\n            return True\n        except Exception as e:\n            logger = logging.getLogger(__name__)\n            logger.error(\"Error in file copy - {}\".format(str(e)))\n            return False\n\n    def _mv(self, src_path: str, dst_path: str, **kwargs: Any) -> bool:\n        \"\"\"\n        Moves (renames) a source path to a destination path.\n\n        Args:\n            src_path (str): A URI supported by this PathHandler\n            dst_path (str): A URI supported by this PathHandler\n\n        Returns:\n            status (bool): True on success\n        \"\"\"\n        self._check_kwargs(kwargs)\n        src_path = self._get_path_with_cwd(src_path)\n        dst_path = self._get_path_with_cwd(dst_path)\n        if os.path.exists(dst_path):\n            logger = logging.getLogger(__name__)\n            logger.error(\"Destination file {} already exists.\".format(dst_path))\n            return False\n\n        try:\n            shutil.move(src_path, dst_path)\n            return True\n        except Exception as e:\n            logger = logging.getLogger(__name__)\n            logger.error(\"Error in move operation - {}\".format(str(e)))\n            return False\n\n    def _symlink(self, src_path: str, dst_path: str, **kwargs: Any) -> bool:\n        \"\"\"\n        Creates a symlink to the src_path at the dst_path\n\n        Args:\n            src_path (str): A URI supported by this PathHandler\n            dst_path (str): A URI supported by this PathHandler\n\n        Returns:\n            status (bool): True on success\n        \"\"\"\n        self._check_kwargs(kwargs)\n        src_path = self._get_path_with_cwd(src_path)\n        dst_path = self._get_path_with_cwd(dst_path)\n        logger = logging.getLogger(__name__)\n        if not os.path.exists(src_path):\n            logger.error(\"Source path {} does not exist\".format(src_path))\n            return False\n        if os.path.exists(dst_path):\n            logger.error(\"Destination path {} already exists.\".format(dst_path))\n            return False\n        try:\n            os.symlink(src_path, dst_path)\n            return True\n        except Exception as e:\n            logger.error(\"Error in symlink - {}\".format(str(e)))\n            return False\n\n    def _exists(self, path: str, **kwargs: Any) -> bool:\n        self._check_kwargs(kwargs)\n        return os.path.exists(self._get_path_with_cwd(path))\n\n    def _isfile(self, path: str, **kwargs: Any) -> bool:\n        self._check_kwargs(kwargs)\n        return os.path.isfile(self._get_path_with_cwd(path))\n\n    def _isdir(self, path: str, **kwargs: Any) -> bool:\n        self._check_kwargs(kwargs)\n        return os.path.isdir(self._get_path_with_cwd(path))\n\n    def _ls(self, path: str, **kwargs: Any) -> List[str]:\n        self._check_kwargs(kwargs)\n        return os.listdir(self._get_path_with_cwd(path))\n\n    def _mkdirs(self, path: str, **kwargs: Any) -> None:\n        self._check_kwargs(kwargs)\n        try:\n            os.makedirs(path, exist_ok=True)\n        except OSError as e:\n            # EEXIST it can still happen if multiple processes are creating the dir\n            if e.errno != errno.EEXIST:\n                raise\n\n    def _rm(self, path: str, **kwargs: Any) -> None:\n        self._check_kwargs(kwargs)\n        os.remove(path)\n\n    def _set_cwd(self, path: Union[str, None], **kwargs: Any) -> bool:\n        self._check_kwargs(kwargs)\n        # Remove cwd path if None\n        if path is None:\n            self._cwd = None\n            return True\n\n        # Make sure path is a valid Unix path\n        if not os.path.exists(path):\n            raise ValueError(f\"{path} is not a valid Unix path\")\n        # Make sure path is an absolute path\n        if not os.path.isabs(path):\n            raise ValueError(f\"{path} is not an absolute path\")\n        self._cwd = path\n        return True\n\n    def _get_path_with_cwd(self, path: str) -> str:\n        if not path:\n            return path\n        return os.path.normpath(\n            path if not self._cwd else os.path.join(self._cwd, path)\n        )\n\n\nclass PathManager:\n    \"\"\"\n    A class for users to open generic paths or translate generic paths to file names.\n\n    path_manager.method(path) will do the following:\n    1. Find a handler by checking the prefixes in `self._path_handlers`.\n    2. Call handler.method(path) on the handler that's found\n    \"\"\"\n\n    def __init__(self) -> None:\n        self._path_handlers: MutableMapping[str, PathHandler] = OrderedDict()\n        \"\"\"\n        Dict from path prefix to handler.\n        \"\"\"\n\n        self._native_path_handler: PathHandler = NativePathHandler()\n        \"\"\"\n        A NativePathHandler that works on posix paths. This is used as the fallback.\n        \"\"\"\n\n        self._cwd: Optional[str] = None\n        \"\"\"\n        Keeps track of the single cwd (if set).\n        NOTE: Only one PathHandler can have a cwd set at a time.\n        \"\"\"\n\n        self._async_handlers: Set[PathHandler] = set()\n        \"\"\"\n        Keeps track of the PathHandler subclasses where `opena` was used so\n        all of the threads can be properly joined when calling\n        `PathManager.join`.\n        \"\"\"\n\n    def path_requires_pathmanager(self, path: str) -> bool:\n        \"\"\"\n        Checks if there is a non-native PathHandler registered for the given path.\n        \"\"\"\n        for p in self._path_handlers.keys():\n            if path.startswith(p):\n                return True\n        return False\n\n    def supports_rename(self, path: str) -> bool:\n        # PathManager doesn't yet support renames\n        return not self.path_requires_pathmanager(path)\n\n    def rename(self, src: str, dst: str) -> None:\n        if self.supports_rename(src):\n            os.rename(src, dst)\n        else:\n            raise ValueError(\n                f\"Path {src} requires PathHandler, and so doesn't support renaming.\"\n            )\n\n    # pyre-fixme[24]: Generic type `os.PathLike` expects 1 type parameter.\n    def __get_path_handler(self, path: Union[str, os.PathLike]) -> PathHandler:\n        \"\"\"\n        Finds a PathHandler that supports the given path. Falls back to the native\n        PathHandler if no other handler is found.\n\n        Args:\n            path (str or os.PathLike): URI path to resource\n\n        Returns:\n            handler (PathHandler)\n        \"\"\"\n        path = os.fspath(path)  # pyre-ignore\n        for p in self._path_handlers.keys():\n            if path.startswith(p):\n                return self._path_handlers[p]\n        return self._native_path_handler\n\n    def open(\n        self, path: str, mode: str = \"r\", buffering: int = -1, **kwargs: Any\n    ) -> Union[IO[str], IO[bytes]]:\n        \"\"\"\n        Open a stream to a URI, similar to the built-in `open`.\n\n        Args:\n            path (str): A URI supported by this PathHandler\n            mode (str): Specifies the mode in which the file is opened. It defaults\n                to 'r'.\n            buffering (int): An optional integer used to set the buffering policy.\n                Pass 0 to switch buffering off and an integer >= 1 to indicate the\n                size in bytes of a fixed-size chunk buffer. When no buffering\n                argument is given, the default buffering policy depends on the\n                underlying I/O implementation.\n\n        Returns:\n            file: a file-like object.\n        \"\"\"\n        handler = self.__get_path_handler(path)\n        bret = handler._open(path, mode, buffering=buffering, **kwargs)  # type: ignore\n        return bret\n\n    def opena(\n        self,\n        path: str,\n        mode: str = \"r\",\n        buffering: int = -1,\n        callback_after_file_close: Optional[Callable[[None], None]] = None,\n        **kwargs: Any,\n    ) -> IOBase:\n        \"\"\"\n        Open a file with asynchronous methods. `f.write()` calls will be dispatched\n        asynchronously such that the main program can continue running.\n        `f.read()` is an async method that has to run in an asyncio event loop.\n\n        NOTE: Writes to the same path are serialized so they are written in\n        the same order as they were called but writes to distinct paths can\n        happen concurrently.\n\n        Usage (write, default / without callback function):\n            for n in range(50):\n                results = run_a_large_task(n)\n                # `f` is a file-like object with asynchronous methods\n                with path_manager.opena(uri, \"w\") as f:\n                    f.write(results)            # Runs in separate thread\n                # Main process returns immediately and continues to next iteration\n            path_manager.async_close()\n\n        Usage (write, advanced / with callback function):\n            # To asynchronously write to storage:\n            def cb():\n                path_manager.copy_from_local(\"checkpoint.pt\", uri)\n            f = pm.opena(\"checkpoint.pt\", \"wb\", callback_after_file_close=cb)\n            torch.save({...}, f)\n            f.close()\n\n        Usage (read):\n            async def my_function():\n              return await path_manager.opena(uri, \"r\").read()\n\n        Args:\n            ...\n            callback_after_file_close (Callable): Only used in write mode. An\n                optional argument that can be passed to perform operations that\n                depend on the asynchronous writes being completed. The file is\n                first written to the local disk and then the callback is\n                executed.\n\n        Returns:\n            file: a file-like object with asynchronous methods.\n        \"\"\"\n        if \"w\" in mode or \"a\" in mode:\n            kwargs[\"callback_after_file_close\"] = callback_after_file_close\n            kwargs[\"buffering\"] = buffering\n        non_blocking_io = self.__get_path_handler(path)._opena(\n            path,\n            mode,\n            **kwargs,\n        )\n        if \"w\" in mode or \"a\" in mode:\n            # Keep track of the path handlers where `opena` is used so that all of the\n            # threads can be properly joined on `PathManager.join`.\n            self._async_handlers.add(self.__get_path_handler(path))\n        return non_blocking_io\n\n    def async_join(self, *paths: str, **kwargs: Any) -> bool:\n        \"\"\"\n        Ensures that desired async write threads are properly joined.\n\n        Usage:\n            Wait for asynchronous methods operating on specific file paths to\n            complete.\n                async_join(\"path/to/file1.txt\")\n                async_join(\"path/to/file2.txt\", \"path/to/file3.txt\")\n            Wait for all asynchronous methods to complete.\n                async_join()\n\n        Args:\n            *paths (str): Pass in any number of file paths and `async_join` will wait\n                until all asynchronous activity for those paths is complete. If no\n                paths are passed in, then `async_join` will wait until all asynchronous\n                jobs are complete.\n\n        Returns:\n            status (bool): True on success\n        \"\"\"\n\n        success = True\n        if not paths:  # Join all.\n            for handler in self._async_handlers:\n                success = handler._async_join(**kwargs) and success\n        else:  # Join specific paths.\n            for path in paths:\n                success = (\n                    self.__get_path_handler(path)._async_join(path, **kwargs)\n                    and success\n                )\n        return success\n\n    def async_close(self, **kwargs: Any) -> bool:\n        \"\"\"\n        `async_close()` must be called at the very end of any script that uses the\n        asynchronous `opena` feature. This calls `async_join()` first and then closes\n        the thread pool used for the asynchronous operations.\n\n        Returns:\n            status (bool): True on success\n        \"\"\"\n        success = self.async_join(**kwargs)\n        for handler in self._async_handlers:\n            success = handler._async_close(**kwargs) and success\n        self._async_handlers.clear()\n        return success\n\n    def copy(\n        self, src_path: str, dst_path: str, overwrite: bool = False, **kwargs: Any\n    ) -> bool:\n        \"\"\"\n        Copies a source path to a destination path.\n\n        Args:\n            src_path (str): A URI supported by this PathHandler\n            dst_path (str): A URI supported by this PathHandler\n            overwrite (bool): Bool flag for forcing overwrite of existing file\n\n        Returns:\n            status (bool): True on success\n        \"\"\"\n\n        if self.__get_path_handler(src_path) != self.__get_path_handler(  # type: ignore\n            dst_path\n        ):\n            return self._copy_across_handlers(src_path, dst_path, overwrite, **kwargs)\n\n        handler = self.__get_path_handler(src_path)\n        bret = handler._copy(src_path, dst_path, overwrite, **kwargs)\n        return bret\n\n    def mv(self, src_path: str, dst_path: str, **kwargs: Any) -> bool:\n        \"\"\"\n        Moves (renames) a source path supported by NativePathHandler to\n        a destination path.\n\n        Args:\n            src_path (str): A URI supported by NativePathHandler\n            dst_path (str): A URI supported by NativePathHandler\n\n        Returns:\n            status (bool): True on success\n        Exception:\n            Asserts if both the src and dest paths are not supported by\n            NativePathHandler.\n        \"\"\"\n\n        # Moving across handlers is not supported.\n        assert self.__get_path_handler(  # type: ignore\n            src_path\n        ) == self.__get_path_handler(\n            dst_path\n        ), \"Src and dest paths must be supported by the same path handler.\"\n        handler = self.__get_path_handler(src_path)\n        bret = handler._mv(src_path, dst_path, **kwargs)\n        return bret\n\n    def get_local_path(self, path: str, force: bool = False, **kwargs: Any) -> str:\n        \"\"\"\n        Get a filepath which is compatible with native Python I/O such as `open`\n        and `os.path`.\n\n        If URI points to a remote resource, this function may download and cache\n        the resource to local disk.\n\n        Args:\n            path (str): A URI supported by this PathHandler\n            force(bool): Forces a download from backend if set to True.\n\n        Returns:\n            local_path (str): a file path which exists on the local file system\n        \"\"\"\n        path = os.fspath(path)\n        handler = self.__get_path_handler(path)  # type: ignore\n        try:\n            bret = handler._get_local_path(path, force=force, **kwargs)\n        except TypeError:\n            bret = handler._get_local_path(path, **kwargs)\n        return bret\n\n    def copy_from_local(\n        self, local_path: str, dst_path: str, overwrite: bool = False, **kwargs: Any\n    ) -> bool:\n        \"\"\"\n        Copies a local file to the specified URI.\n\n        If the URI is another local path, this should be functionally identical\n        to copy.\n\n        Args:\n            local_path (str): a file path which exists on the local file system\n            dst_path (str): A URI supported by this PathHandler\n            overwrite (bool): Bool flag for forcing overwrite of existing URI\n\n        Returns:\n            status (bool): True on success\n        \"\"\"\n        assert os.path.exists(local_path), f\"local_path = {local_path}\"\n        handler = self.__get_path_handler(dst_path)\n\n        bret = handler._copy_from_local(\n            local_path=local_path, dst_path=dst_path, overwrite=overwrite, **kwargs\n        )\n        # pyre-fixme[7]: Expected `bool` but got `None`.\n        return bret\n\n    def exists(self, path: str, **kwargs: Any) -> bool:\n        \"\"\"\n        Checks if there is a resource at the given URI.\n\n        Args:\n            path (str): A URI supported by this PathHandler\n\n        Returns:\n            bool: true if the path exists\n        \"\"\"\n        handler = self.__get_path_handler(path)\n        bret = handler._exists(path, **kwargs)  # type: ignore\n        return bret\n\n    def isfile(self, path: str, **kwargs: Any) -> bool:\n        \"\"\"\n        Checks if there the resource at the given URI is a file.\n\n        Args:\n            path (str): A URI supported by this PathHandler\n\n        Returns:\n            bool: true if the path is a file\n        \"\"\"\n        handler = self.__get_path_handler(path)\n        bret = handler._isfile(path, **kwargs)  # type: ignore\n        return bret\n\n    def isdir(self, path: str, **kwargs: Any) -> bool:\n        \"\"\"\n        Checks if the resource at the given URI is a directory.\n\n        Args:\n            path (str): A URI supported by this PathHandler\n\n        Returns:\n            bool: true if the path is a directory\n        \"\"\"\n        handler = self.__get_path_handler(path)\n        bret = handler._isdir(path, **kwargs)  # type: ignore\n        return bret\n\n    def islink(self, path: str) -> Optional[bool]:\n        if not self.path_requires_pathmanager(path):\n            return os.path.islink(path)\n        return None\n\n    def ls(self, path: str, **kwargs: Any) -> List[str]:\n        \"\"\"\n        List the contents of the directory at the provided URI.\n\n        Args:\n            path (str): A URI supported by this PathHandler\n\n        Returns:\n            List[str]: list of contents in given path\n        \"\"\"\n        # Support trailing wildcard\n        if path.endswith(\"*\"):\n            parent = os.path.dirname(path)\n            pattern = os.path.basename(path)[:-1]\n            return [\n                p\n                for p in self.ls(parent, **kwargs)\n                if os.path.basename(p).startswith(pattern)\n            ]\n        return self.__get_path_handler(path)._ls(path, **kwargs)\n\n    def mkdirs(self, path: str, **kwargs: Any) -> None:\n        \"\"\"\n        Recursive directory creation function. Like mkdir(), but makes all\n        intermediate-level directories needed to contain the leaf directory.\n        Similar to the native `os.makedirs`.\n\n        Args:\n            path (str): A URI supported by this PathHandler\n        \"\"\"\n        handler = self.__get_path_handler(path)\n        bret = handler._mkdirs(path, **kwargs)  # type: ignore\n        return bret\n\n    def rm(self, path: str, **kwargs: Any) -> None:\n        \"\"\"\n        Remove the file (not directory) at the provided URI.\n\n        Args:\n            path (str): A URI supported by this PathHandler\n        \"\"\"\n        handler = self.__get_path_handler(path)\n        bret = handler._rm(path, **kwargs)  # type: ignore\n        return bret\n\n    def chmod(self, path: str, mode: int) -> None:\n        if not self.path_requires_pathmanager(path):\n            os.chmod(path, mode)\n\n    def symlink(self, src_path: str, dst_path: str, **kwargs: Any) -> bool:\n        \"\"\"Symlink the src_path to the dst_path\n\n        Args:\n            src_path (str): A URI supported by this PathHandler to symlink from\n            dst_path (str): A URI supported by this PathHandler to symlink to\n        \"\"\"\n        # Copying across handlers is not supported.\n        assert self.__get_path_handler(  # type: ignore\n            src_path\n        ) == self.__get_path_handler(dst_path)\n        handler = self.__get_path_handler(src_path)\n        bret = handler._symlink(src_path, dst_path, **kwargs)  # type: ignore\n        return bret\n\n    def set_cwd(self, path: Union[str, None], **kwargs: Any) -> bool:\n        \"\"\"\n        Set the current working directory. PathHandler classes prepend the cwd\n        to all URI paths that are handled.\n\n        Args:\n            path (str) or None: A URI supported by this PathHandler. Must be a valid\n                absolute Unix path or None to set the cwd to None.\n\n        Returns:\n            bool: true if cwd was set without errors\n        \"\"\"\n        if path is None and self._cwd is None:\n            return True\n        if self.__get_path_handler(path or self._cwd)._set_cwd(path, **kwargs):  # type: ignore\n            self._cwd = path\n            bret = True\n        else:\n            bret = False\n        return bret\n\n    def register_handler(\n        self, handler: PathHandler, allow_override: bool = True\n    ) -> None:\n        \"\"\"\n        Register a path handler associated with `handler._get_supported_prefixes`\n        URI prefixes.\n\n        Args:\n            handler (PathHandler)\n            allow_override (bool): allow overriding existing handler for prefix\n        \"\"\"\n        logger = logging.getLogger(__name__)\n        assert isinstance(handler, PathHandler), handler\n\n        # Allow override of `NativePathHandler` which is automatically\n        # instantiated by `PathManager`.\n        if isinstance(handler, NativePathHandler):\n            if allow_override:\n                self._native_path_handler = handler\n            else:\n                raise ValueError(\n                    \"`NativePathHandler` is registered by default. Use the \"\n                    \"`allow_override=True` kwarg to override it.\"\n                )\n            return\n\n        for prefix in handler._get_supported_prefixes():\n            if prefix not in self._path_handlers:\n                self._path_handlers[prefix] = handler\n                continue\n\n            old_handler_type = type(self._path_handlers[prefix])\n            if allow_override:\n                # if using the global PathManager, show the warnings\n                global g_pathmgr\n                if self == g_pathmgr:\n                    logger.warning(\n                        f\"[PathManager] Attempting to register prefix '{prefix}' from \"\n                        \"the following call stack:\\n\"\n                        + \"\".join(traceback.format_stack(limit=5))\n                        # show the most recent callstack\n                    )\n                    logger.warning(\n                        f\"[PathManager] Prefix '{prefix}' is already registered \"\n                        f\"by {old_handler_type}. We will override the old handler. \"\n                        \"To avoid such conflicts, create a project-specific PathManager \"\n                        \"instead.\"\n                    )\n                self._path_handlers[prefix] = handler\n            else:\n                raise KeyError(\n                    f\"[PathManager] Prefix '{prefix}' already registered by {old_handler_type}!\"\n                )\n\n        # Sort path handlers in reverse order so longer prefixes take priority,\n        # eg: http://foo/bar before http://foo\n        self._path_handlers = OrderedDict(\n            sorted(self._path_handlers.items(), key=lambda t: t[0], reverse=True)\n        )\n\n    def set_strict_kwargs_checking(self, enable: bool) -> None:\n        \"\"\"\n        Toggles strict kwargs checking. If enabled, a ValueError is thrown if any\n        unused parameters are passed to a PathHandler function. If disabled, only\n        a warning is given.\n\n        With a centralized file API, there's a tradeoff of convenience and\n        correctness delegating arguments to the proper I/O layers. An underlying\n        `PathHandler` may support custom arguments which should not be statically\n        exposed on the `PathManager` function. For example, a custom `HTTPURLHandler`\n        may want to expose a `cache_timeout` argument for `open()` which specifies\n        how old a locally cached resource can be before it's refetched from the\n        remote server. This argument would not make sense for a `NativePathHandler`.\n        If strict kwargs checking is disabled, `cache_timeout` can be passed to\n        `PathManager.open` which will forward the arguments to the underlying\n        handler. By default, checking is enabled since it is innately unsafe:\n        multiple `PathHandler`s could reuse arguments with different semantic\n        meanings or types.\n\n        Args:\n            enable (bool)\n        \"\"\"\n        self._native_path_handler._strict_kwargs_check = enable\n        for handler in self._path_handlers.values():\n            handler._strict_kwargs_check = enable\n\n    # pyre-fixme[3]: Return type must be annotated.\n    # pyre-fixme[2]: Parameter must be annotated.\n    def set_logging(self, enable_logging=True):\n        self._enable_logging = enable_logging\n\n    def _copy_across_handlers(\n        self, src_path: str, dst_path: str, overwrite: bool, **kwargs: Any\n    ) -> bool:\n        src_handler = self.__get_path_handler(src_path)\n        assert src_handler._get_local_path is not None\n        dst_handler = self.__get_path_handler(dst_path)\n        assert dst_handler._copy_from_local is not None\n\n        local_file = src_handler._get_local_path(src_path, **kwargs)\n        # pyre-fixme[7]: Expected `bool` but got `None`.\n        return dst_handler._copy_from_local(\n            local_file, dst_path, overwrite=overwrite, **kwargs\n        )\n\n\ng_pathmgr = PathManager()\n",
        "metaseq/file_io/common/non_blocking_io.py": "# Copyright (c) Meta Platforms, Inc. and affiliates. All Rights Reserved.\n#\n# This source code is licensed under the MIT license found in the\n# LICENSE file in the root directory of this source tree.\n\nimport concurrent.futures\nimport io\nimport logging\nfrom dataclasses import dataclass\nfrom queue import Queue\nfrom threading import Thread\nfrom typing import Callable, IO, Optional, Union\n\n\n\"\"\"\nThis file is used for asynchronous file operations.\n\nWhen `opena` is called for the first time for a specific\n`PathHandler`, a `NonBlockingIOManager` is instantiated. The\nmanager returns a `NonBlockingIO` (or `NonBlockingBufferedIO`)\ninstance to the caller, and the manager maintains all of the\nthread management and data management.\n\"\"\"\n\n\n@dataclass\nclass PathData:\n    \"\"\"\n    Manage the IO job queue and polling thread for a single\n    path. This is done to ensure that write calls to the same\n    path are serialized so they are written in the same order\n    as they were called.\n\n    On each `f.write` call where `f` is of type `NonBlockingIO`,\n    we send the job to the manager where it is enqueued to the\n    Queue. The polling Thread picks up on the job, executes it,\n    waits for it to finish, and then continues to poll.\n    \"\"\"\n\n    # pyre-fixme[24]: Generic type `Queue` expects 1 type parameter.\n    queue: Queue\n    thread: Thread\n\n\nclass NonBlockingIOManager:\n    \"\"\"\n    All `opena` calls pass through this class so that it can\n    keep track of the threads for proper cleanup at the end\n    of the script. Each path that is opened with `opena` is\n    assigned a single queue and polling thread that is kept\n    open until it is cleaned up by `PathManager.async_join()`.\n    \"\"\"\n\n    def __init__(\n        self,\n        buffered: Optional[bool] = False,\n        executor: Optional[concurrent.futures.Executor] = None,\n    ) -> None:\n        \"\"\"\n        Args:\n            buffered (bool): IO instances will be `NonBlockingBufferedIO`\n                or `NonBlockingIO` based on this value. This bool is set\n                manually for each `PathHandler` in `_opena`.\n            executor: User can optionally attach a custom executor to\n                perform async operations through `PathHandler.__init__`.\n        \"\"\"\n        # pyre-fixme[4]: Attribute must be annotated.\n        self._path_to_data = {}  # Map from path to `PathData` object\n        self._buffered = buffered\n        self._IO = NonBlockingBufferedIO if self._buffered else NonBlockingIO\n        # pyre-fixme[4]: Attribute must be annotated.\n        self._pool = executor or concurrent.futures.ThreadPoolExecutor()\n\n    def get_non_blocking_io(\n        self,\n        path: str,\n        io_obj: Union[IO[str], IO[bytes]],\n        callback_after_file_close: Optional[Callable[[None], None]] = None,\n        buffering: Optional[int] = -1,\n    ) -> io.IOBase:\n        \"\"\"\n        Called by `PathHandler._opena` with the path and returns a\n        `NonBlockingIO` instance.\n\n        Args:\n            path (str): A path str to operate on. This path should be\n                simplified to ensure that each absolute path has only a single\n                path str that maps onto it. For example, in `NativePathHandler`,\n                we can use `os.path.normpath`.\n            io_obj (IO): a reference to the IO object returned by the\n                `PathHandler._open` function.\n            callback_after_file_close (Callable): An optional argument that can\n                be passed to perform operations that depend on the asynchronous\n                writes being completed. The file is first written to the local\n                disk and then the callback is executed.\n            buffering (int): An optional argument to set the buffer size for\n                buffered asynchronous writing.\n        \"\"\"\n        if not self._buffered and buffering != -1:\n            raise ValueError(\n                \"NonBlockingIO is not using a buffered writer but `buffering` \"\n                f\"arg is set to non-default value of {buffering} != -1.\"\n            )\n\n        if path not in self._path_to_data:\n            # Initialize job queue and a polling thread\n            queue = Queue()\n            t = Thread(target=self._poll_jobs, args=(queue,))\n            t.start()\n            # Store the `PathData`\n            self._path_to_data[path] = PathData(queue, t)\n\n        kwargs = {} if not self._buffered else {\"buffering\": buffering}\n        # pyre-fixme[29]: `Type[Union[NonBlockingBufferedIO, NonBlockingIO]]` is not\n        #  a function.\n        return self._IO(\n            notify_manager=lambda io_callable: (  # Pass async jobs to manager\n                self._path_to_data[path].queue.put(io_callable)\n            ),\n            io_obj=io_obj,\n            callback_after_file_close=callback_after_file_close,\n            **kwargs,\n        )\n\n    def _poll_jobs(self, queue: Optional[Callable[[], None]]) -> None:\n        \"\"\"\n        A single thread runs this loop. It waits for an IO callable to be\n        placed in a specific path's `Queue` where the queue contains\n        callable functions. It then waits for the IO job to be completed\n        before looping to ensure write order.\n        \"\"\"\n        while True:\n            # `func` is a callable function (specifically a lambda function)\n            # and can be any of:\n            #   - func = file.write(b)\n            #   - func = file.close()\n            #   - func = None\n            # pyre-fixme[16]: `Optional` has no attribute `get`.\n            func = queue.get()  # Blocks until item read.\n            if func is None:  # Thread join signal.\n                break\n            self._pool.submit(func).result()  # Wait for job to finish.\n\n    def _join(self, path: Optional[str] = None) -> bool:\n        \"\"\"\n        Waits for write jobs for a specific path or waits for all\n        write jobs for the path handler if no path is provided.\n\n        Args:\n            path (str): Pass in a file path and will wait for the\n                asynchronous jobs to be completed for that file path.\n                If no path is passed in, then all threads operating\n                on all file paths will be joined.\n        \"\"\"\n\n        if path and path not in self._path_to_data:\n            raise ValueError(\n                f\"{path} has no async IO associated with it. \"\n                f\"Make sure `opena({path})` is called first.\"\n            )\n        # If a `_close` call fails, we print the error and continue\n        # closing the rest of the IO objects.\n        paths_to_close = [path] if path else list(self._path_to_data.keys())\n        success = True\n        for _path in paths_to_close:\n            try:\n                path_data = self._path_to_data.pop(_path)\n                path_data.queue.put(None)\n                path_data.thread.join()\n            except Exception:\n                logger = logging.getLogger(__name__)\n                logger.exception(f\"`NonBlockingIO` thread for {_path} failed to join.\")\n                success = False\n        return success\n\n    def _close_thread_pool(self) -> bool:\n        \"\"\"\n        Closes the ThreadPool.\n        \"\"\"\n        try:\n            self._pool.shutdown()\n        except Exception:\n            logger = logging.getLogger(__name__)\n            logger.exception(\"`NonBlockingIO` thread pool failed to close.\")\n            return False\n        return True\n\n\n# NOTE: We currently only support asynchronous writes (not reads).\nclass NonBlockingIO(io.IOBase):\n    def __init__(\n        self,\n        notify_manager: Callable[[Callable[[], None]], None],\n        io_obj: Union[IO[str], IO[bytes]],\n        callback_after_file_close: Optional[Callable[[None], None]] = None,\n    ) -> None:\n        \"\"\"\n        Returned to the user on an `opena` call. Uses a Queue to manage the\n        IO jobs that need to be run to ensure order preservation and a\n        polling Thread that checks the Queue. Implementation for these are\n        lifted to `NonBlockingIOManager` since `NonBlockingIO` closes upon\n        leaving the context block.\n\n        NOTE: Writes to the same path are serialized so they are written in\n        the same order as they were called but writes to distinct paths can\n        happen concurrently.\n\n        Args:\n            notify_manager (Callable): a callback function passed in from the\n                `NonBlockingIOManager` so that all IO jobs can be stored in\n                the manager. It takes in a single argument, namely another\n                callable function.\n                Example usage:\n                ```\n                    notify_manager(lambda: file.write(data))\n                    notify_manager(lambda: file.close())\n                ```\n                Here, we tell `NonBlockingIOManager` to add a write callable\n                to the path's Queue, and then to add a close callable to the\n                path's Queue. The path's polling Thread then executes the write\n                callable, waits for it to finish, and then executes the close\n                callable. Using `lambda` allows us to pass callables to the\n                manager.\n            io_obj (IO): a reference to the IO object returned by the\n                `PathHandler._open` function.\n            callback_after_file_close (Callable): An optional argument that can\n                be passed to perform operations that depend on the asynchronous\n                writes being completed. The file is first written to the local\n                disk and then the callback is executed.\n        \"\"\"\n        super().__init__()\n        self._notify_manager = notify_manager\n        self._io = io_obj\n        self._callback_after_file_close = callback_after_file_close\n\n        self._close_called = False\n\n    def readable(self) -> bool:\n        return False\n\n    def writable(self) -> bool:\n        return True\n\n    def seekable(self) -> bool:\n        return True\n\n    def write(self, b: Union[bytes, bytearray]) -> None:\n        \"\"\"\n        Called on `f.write()`. Gives the manager the write job to call.\n        \"\"\"\n        # pyre-fixme[6]: For 1st param expected `() -> None` but got `() -> int`.\n        # pyre-fixme[6]: For 1st param expected `bytes` but got `Union[bytearray,\n        #  bytes]`.\n        self._notify_manager(lambda: self._io.write(b))\n\n    def seek(self, offset: int, whence: int = 0) -> int:\n        \"\"\"\n        Called on `f.seek()`.\n        \"\"\"\n        # pyre-fixme[7]: Expected `int` but got implicit return value of `None`.\n        # pyre-fixme[6]: For 1st param expected `() -> None` but got `() -> int`.\n        self._notify_manager(lambda: self._io.seek(offset, whence))\n\n    def tell(self) -> int:\n        \"\"\"\n        Called on `f.tell()`.\n        \"\"\"\n        raise ValueError(\"Async write does not support `tell` calls.\")\n\n    # pyre-fixme[14]: `truncate` overrides method defined in `IOBase` inconsistently.\n    # pyre-fixme[9]: size has type `int`; used as `None`.\n    def truncate(self, size: int = None) -> int:\n        \"\"\"\n        Called on `f.truncate()`.\n        \"\"\"\n        # pyre-fixme[7]: Expected `int` but got implicit return value of `None`.\n        # pyre-fixme[6]: For 1st param expected `() -> None` but got `() -> int`.\n        self._notify_manager(lambda: self._io.truncate(size))\n\n    def close(self) -> None:\n        \"\"\"\n        Called on `f.close()` or automatically by the context manager.\n        We add the `close` call to the file's queue to make sure that\n        the file is not closed before all of the write jobs are complete.\n        \"\"\"\n        # `ThreadPool` first closes the file and then executes the callback.\n        # We only execute the callback once even if there are multiple\n        # `f.close` calls.\n        self._notify_manager(lambda: self._io.close())\n        if not self._close_called and self._callback_after_file_close:\n            # pyre-fixme[6]: For 1st param expected `() -> None` but got `(None) ->\n            #  None`.\n            self._notify_manager(self._callback_after_file_close)\n        self._close_called = True\n\n\n# NOTE: To use this class, use `buffered=True` in `NonBlockingIOManager`.\n# NOTE: This class expects the IO mode to be buffered.\nclass NonBlockingBufferedIO(io.IOBase):\n    # pyre-fixme[4]: Attribute must be annotated.\n    MAX_BUFFER_BYTES = 10 * 1024 * 1024  # 10 MiB\n\n    def __init__(\n        self,\n        notify_manager: Callable[[Callable[[], None]], None],\n        io_obj: Union[IO[str], IO[bytes]],\n        callback_after_file_close: Optional[Callable[[None], None]] = None,\n        buffering: int = -1,\n    ) -> None:\n        \"\"\"\n        Buffered version of `NonBlockingIO`. All write data is stored in an\n        IO buffer until the buffer is full, or `flush` or `close` is called.\n\n        Args:\n            Same as `NonBlockingIO` args.\n            buffering (int): An optional argument to set the buffer size for\n                buffered asynchronous writing.\n        \"\"\"\n        super().__init__()\n        self._notify_manager = notify_manager\n        self._io = io_obj\n        self._callback_after_file_close = callback_after_file_close\n\n        self._buffers = [io.BytesIO()]\n        # pyre-fixme[4]: Attribute must be annotated.\n        self._buffer_size = buffering if buffering > 0 else self.MAX_BUFFER_BYTES\n        self._close_called = False\n\n    def readable(self) -> bool:\n        return False\n\n    def writable(self) -> bool:\n        return True\n\n    def seekable(self) -> bool:\n        return False\n\n    def write(self, b: Union[bytes, bytearray]) -> None:\n        \"\"\"\n        Called on `f.write()`. Gives the manager the write job to call.\n        \"\"\"\n        buffer = self._buffers[-1]\n        with memoryview(b) as view:\n            buffer.write(view)\n        if buffer.tell() < self._buffer_size:\n            return\n        self.flush()\n\n    def close(self) -> None:\n        \"\"\"\n        Called on `f.close()` or automatically by the context manager.\n        We add the `close` call to the file's queue to make sure that\n        the file is not closed before all of the write jobs are complete.\n        \"\"\"\n        self.flush()\n        # Close the last buffer created by `flush`.\n        self._notify_manager(lambda: self._buffers[-1].close())\n        # `ThreadPool` first closes the file and then executes the callback.\n        self._notify_manager(lambda: self._io.close())\n        if not self._close_called and self._callback_after_file_close:\n            # pyre-fixme[6]: For 1st param expected `() -> None` but got `(None) ->\n            #  None`.\n            self._notify_manager(self._callback_after_file_close)\n        self._close_called = True\n\n    def flush(self) -> None:\n        \"\"\"\n        Called on `f.write()` if the buffer is filled (or overfilled). Can\n        also be explicitly called by user.\n        NOTE: Buffering is used in a strict manner. Any buffer that exceeds\n        `self._buffer_size` will be broken into multiple write jobs where\n        each has a write call with `self._buffer_size` size.\n        \"\"\"\n        buffer = self._buffers[-1]\n        if buffer.tell() == 0:\n            return\n        pos = 0\n        total_size = buffer.seek(0, io.SEEK_END)\n        view = buffer.getbuffer()\n        # Chunk the buffer in case it is larger than the buffer size.\n        while pos < total_size:\n            item = view[pos : pos + self._buffer_size]\n            # `item=item` is needed due to Python's late binding closures.\n            # pyre-fixme[6]: For 1st param expected `() -> None` but got `(item: Any\n            #  = ...) -> int`.\n            self._notify_manager(lambda item=item: self._io.write(item))\n            pos += self._buffer_size\n        # Close buffer immediately after being written to file and create\n        # a new buffer.\n        self._notify_manager(lambda: buffer.close())\n        self._buffers.append(io.BytesIO())\n",
        "metaseq/file_io/s3.py": "# Copyright (c) Meta Platforms, Inc. and affiliates. All Rights Reserved.\n#\n# This source code is licensed under the MIT license found in the\n# LICENSE file in the root directory of this source tree.\n\nimport datetime as dt\nimport io\nimport logging\nimport os\nimport shutil\nimport types\nfrom functools import partial\nfrom typing import Any, Dict, IO, List, Optional, Tuple, Union\n\nimport boto3\nimport botocore\nfrom boto3.s3.transfer import TransferConfig\nfrom metaseq.file_io.common import file_lock, get_cache_dir, PathHandler\n\nlogger = logging.getLogger(__name__)\n\n\n# Override for close() on files to write to Amazon S3\ndef s3_close_and_upload(self, client, bucket, s3_path, transfer_config):\n    # Seek to start, for use by upload_fileobj.\n    self.seek(0)\n\n    # Reinstall the proper close.\n    self.close = self._close\n\n    # upload_fileobj needs bytes\n    # NOTE: This a very undesirable hack.\n    if isinstance(self, io.StringIO):\n        self = io.BytesIO(self.getvalue().encode(\"utf-8\"))\n\n    # Upload\n    try:\n        client.upload_fileobj(\n            self,\n            bucket,\n            s3_path,\n            Config=transfer_config,\n        )\n    except botocore.exceptions.ClientError as e:\n        raise OSError(f\"Error in file upload - {e}\" f\"{type(e).__name__}: {e}\") from e\n\n\nclass S3PathHandler(PathHandler):\n    \"\"\"\n    Support for Amazon Simple Storage Service (S3)\n\n    PathHanlder methods, at a glance:\n\n     File     --torch.load->     In     --open(..., 'w')->   Amazon    <- _exists,_isfile,_isdir,_ls,_rm ...\n    System   <-torch.save--     Mem.   <-open(..., 'r')--      S3\n            <----------------_copy_from_local-----------------\n            ----------------_get_local_path ----------------->\n\n    Mem usage, for processing N bytes:\n        open(..., mode)\n            mode=='w':    2N,  due to fully buffering user input,\n                                *and doing naive conversion from StringIO -> BytesIO*,\n                                before writing to S3\n                                ^ Potential for optimization.\n            mode=='wb':    N,  due to fully buffering user input, before writing to S3.\n            mode=='r':     N,  due to fully buffering file in memory\n            mode=='rb':    N,  due to fully buffering file in memory\n        _copy_from_local: ≈0.  boto3 streams from file system directly to s3\n        _get_local_path:  ≈0.  boto3 streams from s3 directly from s3 to file system\n    \"\"\"\n\n    # Disable failures if not all args are specified.\n    _strict_kwargs_check = False\n\n    S3_PREFIX = \"s3://\"\n    CACHE_SUBDIR_NAME = \"s3_cache\"\n\n    def __init__(\n        self,\n        cache_dir: Optional[str] = None,\n        transfer_config_kwargs: Optional[Dict] = None,\n    ):\n        \"\"\"\n        Args:\n            cache_dir (str): Local filesystem directory to use for caching. If None,\n                uses default from `file_io.get_cache_dir()`.\n            transfer_config_kwargs (dict): Settings for boto3.s3.transfer.TransferConfig.\n                Used to specify settings for multipart transfers.\n                See https://boto3.amazonaws.com/v1/documentation/api/latest/guide/s3.html for details.\n        \"\"\"\n        self.cache_dir = cache_dir\n        self.transfer_config = TransferConfig(\n            **(transfer_config_kwargs if transfer_config_kwargs else {})\n        )\n\n    def _get_supported_prefixes(self) -> List[str]:\n        \"\"\"\n        Returns:\n            List[str]: the list of URI prefixes this PathHandler can support\n        \"\"\"\n        return [self.S3_PREFIX]\n\n    def _parse_uri(self, uri: str) -> Tuple[str, str]:\n        \"\"\"\n        Parses a \"s3://bucket/path\" URI into `bucket` and `path` strings.\n\n        Args:\n            uri (str): A s3:// URI.\n\n        Returns:\n            bucket (str): the s3 bucket.\n            path (str): the path on the s3 system.\n        \"\"\"\n        splits = uri.replace(self.S3_PREFIX, \"\").split(\"/\")\n        bucket = splits[0]\n        path = \"/\".join(splits[1:])\n        return bucket, path\n\n    def _get_client(self, bucket: str):\n        # TODO: Consider pid-based cache: https://fburl.com/code/xsz3wrv6\n        if not hasattr(self, \"client\"):\n            try:\n                session = boto3.Session()\n                self.client = session.client(\"s3\")\n            except botocore.exceptions.NoCredentialsError as e:\n                logger.error(\n                    \" See https://boto3.amazonaws.com/v1/documentation/api/latest/guide/credentials.html \"\n                    \" for method of using environment variable to point to aws credentials, and the \"\n                    \" order in which boto will search for said credentials. \"\n                )\n                logger.error(\n                    \"Boto3 searches via the order below.  If on FAIR Cluster, method 4 may be most convenient.\"\n                    \"\"\n                    \"The order in which Boto3 searches for credentials is:\"\n                    \"1) [UNUSED] Passing credentials as parameters in the boto.client() method\"\n                    \"2) [UNUSED] Passing credentials as parameters when creating a Session object\"\n                    \"3) Environment variables\"\n                    \"       AWS_ACCESS_KEY_ID - The access key for your AWS account.\"\n                    \"       AWS_SECRET_ACCESS_KEY - The secret key for your AWS account.\"\n                    \"       AWS_SESSION_TOKEN - The session key for your AWS account.\"\n                    \"           This is only needed when you are using temporary credentials. \"\n                    \"4) Shared credential file (~/.aws/credentials)\"\n                    \"       default: ~/.aws/credentials\"\n                    \"       changed via: AWS_SHARED_CREDENTIALS_FILE\"\n                    \"       *for FAIR cluster usage: `export AWS_SHARED_CREDENTIALS_FILE=~/.fairusers_aws/credentials`\"\n                    \"5) AWS config file (~/.aws/config)\"\n                    \"       default: ~/.aws/config\"\n                    \"       changed via: AWS_CONFIG_FILE\"\n                    \"6) Assume Role provider\"\n                    \"7) Boto2 config file (/etc/boto.cfg and ~/.boto)\"\n                    \"8) Instance metadata service on an Amazon EC2 instance that has an IAM role configured.\"\n                )\n                raise OSError(\n                    f\"Error in making s3 client for bucket {bucket}\"\n                    f\"{type(e).__name__}: {e}\"\n                ) from e\n\n        return self.client\n\n    def _local_cache_path(\n        self,\n        path: str,\n    ):\n        \"\"\"\n        Helper that returns a local cache path for a given uri.\n        Args:\n            path (str): A URI supported by this PathHandler.\n        Returns:\n            local_cache_path (str): a file path which exists on the local file system,\n            in a cache directory.\n        \"\"\"\n        bucket, file_path = self._parse_uri(path)\n        return os.path.join(\n            get_cache_dir(self.cache_dir), self.CACHE_SUBDIR_NAME, file_path\n        )\n\n    def _get_local_path(self, path: str, **kwargs: Any) -> str:\n        \"\"\"\n        Get a filepath which is compatible with native Python I/O such as `open`\n        and `os.path`.\n        If URI points to a remote resource, this function may download and cache\n        the resource to local disk. In this case, the cache stays on filesystem\n        (under `file_io.get_cache_dir()`) and will be used by a different run.\n        Therefore this function is meant to be used with read-only resources.\n        Args:\n            path (str): A URI supported by this PathHandler\n        Returns:\n            local_path (str): a file path which exists on the local file system\n        \"\"\"\n        self._check_kwargs(kwargs)\n\n        # Cheap check first.\n        if path.endswith(\"/\"):\n            raise NotImplementedError(\n                \"S3PathHandler does not currently support downloading directories\"\n            )\n        assert self._isfile(path)\n\n        local_path = self._local_cache_path(path)\n        with file_lock(local_path):\n            if os.path.exists(local_path):\n                # If local object's last modified time is *after* remote object's last modified\n                # time, do not use the cache.  Instead, redownload.\n                response = self._head_object(path)\n                if response is not None:\n                    remote_dt = response[\"LastModified\"]\n                    local_dt = dt.datetime.fromtimestamp(\n                        os.path.getmtime(local_path)\n                    ).astimezone()\n                    # NOTE: may consider still avoid cache if times are close, to avoid a race condition.\n                    # Currently, a lengthy download of a very recent but stale file would have a late\n                    # local last modified timestamp, and would be improperly used.\n                    # Better fix: set last modified time via the remote object's last modified time,\n                    # in download_file().\n                    if (local_dt - remote_dt) > dt.timedelta(minutes=0):\n                        logger.info(\n                            \"URL {} was already cached in {}\".format(path, local_path)\n                        )\n                        return local_path\n\n            logger.info(\"Caching {} ...\".format(path))\n            tmp = local_path + \".tmp\"\n            # clean-up tmp if found, because if tmp exists, it must be a dirty\n            # result of a previously process that didn't cleanup itself.\n            if os.path.isfile(tmp):\n                os.unlink(tmp)\n\n            bucket, s3_path = self._parse_uri(path)\n            client = self._get_client(bucket)\n            try:\n                response = client.download_file(\n                    bucket, s3_path, tmp, Config=self.transfer_config\n                )\n\n                # First download to tmp, then move it, because move is\n                # (almost?) atomic when src and dst are in the same file\n                # system. This will avoid partial cache state if the\n                # process is killed.\n                shutil.move(tmp, local_path)\n            finally:\n                try:\n                    os.unlink(tmp)\n                except Exception:\n                    pass\n\n            logger.info(\"URL {} cached in {}\".format(path, local_path))\n            return local_path\n\n    def _copy_from_local(\n        self, local_path: str, dst_path: str, overwrite: bool = False, **kwargs: Any\n    ) -> bool:\n        \"\"\"\n        Copies a local file to the specified URI.\n        If the URI is another local path, this should be functionally identical\n        to copy.\n        Args:\n            local_path (str): a file path which exists on the local file system\n            dst_path (str): A URI supported by this PathHandler\n            overwrite (bool): Bool flag for forcing overwrite of existing URI\n        Returns:\n            status (bool): True on success\n        \"\"\"\n        self._check_kwargs(kwargs)\n\n        # Just checking this to avoid expensive API calls in self._isdir().\n        if local_path.endswith(\"/\") or dst_path.endswith(\"/\"):\n            raise NotImplementedError(\n                \"S3PathHandler does not currently support uploading directories\"\n            )\n\n        bucket, s3_path = self._parse_uri(dst_path)\n        client = self._get_client(bucket)\n        try:\n            client.upload_file(local_path, bucket, s3_path, Config=self.transfer_config)\n            return True\n        except botocore.exceptions.ClientError as e:\n            logger.error(\"Error in file upload - {}\".format(str(e)))\n            return False\n\n    def _decorate_buf_with_s3_methods(\n        self,\n        buffer: Union[IO[str], IO[bytes]],\n        client: Any,\n        bucket: str,\n        s3_path: str,\n        transfer_config: Any,\n    ):\n        # Save old close method.\n        buffer._close = buffer.close\n\n        # Add in our new close method.\n        fn = partial(\n            s3_close_and_upload,\n            client=client,\n            bucket=bucket,\n            s3_path=s3_path,\n            transfer_config=transfer_config,\n        )\n        buffer.close = types.MethodType(fn, buffer)\n\n    def _open(\n        self,\n        path: str,\n        mode: str = \"r\",\n        buffering: int = -1,\n        # The following three arguments are unused,\n        # But are included to avoid triggering WARNING\n        # messages from _check_kargs.\n        encoding: Optional[str] = None,\n        errors: Optional[str] = None,\n        newline: Optional[str] = None,\n        **kwargs: Any,\n    ) -> Union[IO[str], IO[bytes]]:\n        \"\"\"\n        Open a stream to a URI, similar to the built-in `open`.\n        Args:\n            path (str): A URI supported by this PathHandler\n            mode (str): Specifies the mode in which the file is opened. It defaults\n                to 'r'.\n            buffering (int): An optional integer used to set the buffering policy.\n                Pass 0 to switch buffering off and an integer >= 1 to indicate the\n                size in bytes of a fixed-size chunk buffer. When no buffering\n                argument is given, the default buffering policy depends on the\n                underlying I/O implementation.\n        Returns:\n            file: a file-like object.\n        \"\"\"\n        self._check_kwargs(kwargs)\n\n        bucket, s3_path = self._parse_uri(path)\n        client = self._get_client(bucket)\n\n        # AWS methods download_fileobj() and upload_fileobj()\n        # both expect binary file-like objects.\n        if \"r\" in mode:\n            # 1. Download into io.BytesIO.\n            # (binary format is required by download_fileobj.)\n            buffer = io.BytesIO()\n            try:\n                # NOTE: Will download entire file!  Further optimization to\n                # only read a portion of the file could be implemented here.\n                # NOTE: We download into an in-memory buffer.  If downloading to\n                # filesystem is desirable, use _get_local_path().\n                client.download_fileobj(\n                    bucket, s3_path, buffer, Config=self.transfer_config\n                )\n            except botocore.exceptions.ClientError as e:\n                raise OSError(\n                    f\"Error in making s3 client for bucekt {bucket}\"\n                    f\"{type(e).__name__}: {e}\"\n                ) from e\n\n            # 2. Set file-pointer to beginning of file.\n            buffer.seek(0)\n\n            # 3. Use convenient wrapper to make object look like StringIO,\n            # if user wants non-binary.\n            if \"b\" not in mode:\n                buffer = io.TextIOWrapper(buffer, encoding=\"utf-8\")\n\n            return buffer\n\n        elif \"w\" in mode:\n            # 1. For writing, we give the user io.BytesIO or io.StringIO.\n            if \"b\" in mode:\n                buffer = io.BytesIO()\n            else:\n                buffer = io.StringIO()\n\n            # 2. Decorate buffer so that we upload when it's closed by user.\n            #       If StringIO, decorator does a simple+expensive conversion\n            #       to bytesIO before uploading.\n            #       (because upload_fileobj requires binary)\n            self._decorate_buf_with_s3_methods(\n                buffer, client, bucket, s3_path, self.transfer_config\n            )\n\n            return buffer\n\n        else:\n            raise OSError(f\"Unsupported open mode {mode}\")\n\n    def _copy(\n        self, src_path: str, dst_path: str, overwrite: bool = False, **kwargs: Any\n    ) -> bool:\n        \"\"\"\n        Copies a source path to a destination path.\n        Args:\n            src_path (str): A URI supported by this PathHandler\n            dst_path (str): A URI supported by this PathHandler\n            overwrite (bool): Bool flag for forcing overwrite of existing file\n        Returns:\n            status (bool): True on success\n        \"\"\"\n        self._check_kwargs(kwargs)\n\n        src_bucket, src_s3_path = self._parse_uri(src_path)\n        dst_bucket, dst_s3_path = self._parse_uri(dst_path)\n        assert src_bucket == dst_bucket, \"For now, can only _copy() within a bucket.\"\n        client = self._get_client(src_bucket)\n\n        try:\n            client.copy(\n                {\n                    \"Bucket\": src_bucket,\n                    \"Key\": src_s3_path,\n                },\n                dst_bucket,\n                dst_s3_path,\n                Config=self.transfer_config,\n            )\n            return True\n        except botocore.exceptions.ClientError as e:\n            logger.error(\"Error in file copy - {}\".format(str(e)))\n            return False\n\n    def _head_object(self, path: str) -> Optional[Dict]:\n        bucket, s3_path = self._parse_uri(path)\n        client = self._get_client(bucket)\n\n        try:\n            # Raises exception if not exists, else it exists.\n            response = client.head_object(Bucket=bucket, Key=s3_path)\n            return response\n        except botocore.exceptions.ClientError as e:\n            if e.response[\"Error\"][\"Message\"] == \"Bad Request\":\n                raise OSError(\n                    f\"Error in checking s3 path {path} - \" f\"{type(e).__name__}: {e}\"\n                ) from e\n            return None\n\n    def _exists(self, path: str, **kwargs: Any) -> bool:\n        \"\"\"\n        Checks if there is a resource at the given URI.\n        Args:\n            path (str): A URI supported by this PathHandler\n        Returns:\n            bool: true if the path exists\n        \"\"\"\n        self._check_kwargs(kwargs)\n\n        return self._head_object(path) is not None\n\n    def _isfile(self, path: str, **kwargs: Any) -> bool:\n        \"\"\"\n        Checks if the resource at the given URI is a file.\n        Args:\n            path (str): A URI supported by this PathHandler\n        Returns:\n            bool: true if the path is a file\n        \"\"\"\n        self._check_kwargs(kwargs)\n\n        # NOTE: this incurs an API call.\n        return not path.endswith(\"/\") and self._exists(path, **kwargs)\n\n    def _isdir(self, path: str, **kwargs: Any) -> bool:\n        \"\"\"\n        Checks if the resource at the given URI is a directory.\n        Args:\n            path (str): A URI supported by this PathHandler\n        Returns:\n            bool: true if the path is a directory\n        \"\"\"\n        self._check_kwargs(kwargs)\n\n        # NOTE: this incurs an API call.\n        return path.endswith(\"/\") and self._exists(path, **kwargs)\n\n    def _ls(self, path: str, **kwargs: Any) -> List[str]:\n        \"\"\"\n        List the contents of the directory at the provided URI.\n        Args:\n            path (str): A URI supported by this PathHandler\n        Returns:\n            List[str]: list of contents in given path\n        \"\"\"\n        self._check_kwargs(kwargs)\n\n        bucket, s3_path = self._parse_uri(path)\n        client = self._get_client(bucket)\n\n        try:\n            # Pagination needed if >1000 entries.\n            paginator = client.get_paginator(\"list_objects_v2\")\n            pages = paginator.paginate(\n                Bucket=bucket,\n                Prefix=s3_path,\n            )\n            return [obj[\"Key\"] for page in pages for obj in page.get(\"Contents\", [])]\n        except botocore.exceptions.ClientError as e:\n            raise OSError(\n                f\"Error in ls path {path} - \" f\"{type(e).__name__}: {e}\"\n            ) from e\n\n    def _mkdirs(self, path: str, **kwargs: Any) -> None:\n        \"\"\"\n        Recursive directory creation function. Like mkdir(), but makes all\n        intermediate-level directories needed to contain the leaf directory.\n        Similar to the native `os.makedirs`.\n        Args:\n            path (str): A URI supported by this PathHandler\n        \"\"\"\n        self._check_kwargs(kwargs)\n\n        assert path.endswith(\"/\"), path\n\n        bucket, s3_path = self._parse_uri(path)\n        client = self._get_client(bucket)\n\n        try:\n            client.put_object(Bucket=bucket, Key=s3_path)\n        except botocore.exceptions.ClientError as e:\n            raise OSError(\n                f\"Error in mkdirs path {path} - \" f\"{type(e).__name__}: {e}\"\n            ) from e\n\n    def _rm(self, path: str, **kwargs: Any) -> None:\n        \"\"\"\n        Remove the file (not directory) at the provided URI.\n        Args:\n            path (str): A URI supported by this PathHandler\n        \"\"\"\n        self._check_kwargs(kwargs)\n\n        bucket, s3_path = self._parse_uri(path)\n        client = self._get_client(bucket)\n\n        try:\n            client.delete_object(Bucket=bucket, Key=s3_path)\n        except botocore.exceptions.ClientError as e:\n            raise OSError(\n                f\"Error in rm path {path} - \" f\"{type(e).__name__}: {e}\"\n            ) from e\n",
        "metaseq/file_utils.py": "# Copyright (c) Meta Platforms, Inc. and affiliates. All Rights Reserved.\n#\n# This source code is licensed under the MIT license found in the\n# LICENSE file in the root directory of this source tree.\n\n\"\"\"\nUtilities for working with the local dataset cache.\nThis file is adapted from `AllenNLP <https://github.com/allenai/allennlp>`_.\nand `huggingface <https://github.com/huggingface>`_.\n\"\"\"\n\nimport fnmatch\nimport json\nimport logging\nimport os\nimport shutil\nimport tarfile\nimport tempfile\nfrom functools import partial, wraps\nfrom hashlib import sha256\nfrom io import open\n\n\ntry:\n    from torch.hub import _get_torch_home\n\n    torch_cache_home = _get_torch_home()\nexcept ImportError:\n    torch_cache_home = os.path.expanduser(\n        os.getenv(\n            \"TORCH_HOME\", os.path.join(os.getenv(\"XDG_CACHE_HOME\", \"~/.cache\"), \"torch\")\n        )\n    )\ndefault_cache_path = os.path.join(torch_cache_home, \"pytorch_fairseq\")\n\ntry:\n    from urllib.parse import urlparse\nexcept ImportError:\n    from urlparse import urlparse\n\ntry:\n    from pathlib import Path\n\n    PYTORCH_FAIRSEQ_CACHE = Path(os.getenv(\"PYTORCH_FAIRSEQ_CACHE\", default_cache_path))\nexcept (AttributeError, ImportError):\n    PYTORCH_FAIRSEQ_CACHE = os.getenv(\"PYTORCH_FAIRSEQ_CACHE\", default_cache_path)\n\nCONFIG_NAME = \"config.json\"\nWEIGHTS_NAME = \"pytorch_model.bin\"\n\nlogger = logging.getLogger(__name__)  # pylint: disable=invalid-name\n\n\ndef load_archive_file(archive_file):\n    # redirect to the cache, if necessary\n    try:\n        resolved_archive_file = cached_path(archive_file, cache_dir=None)\n    except EnvironmentError:\n        logger.info(\n            \"Archive name '{}' was not found in archive name list. \"\n            \"We assumed '{}' was a path or URL but couldn't find any file \"\n            \"associated to this path or URL.\".format(\n                archive_file,\n                archive_file,\n            )\n        )\n        return None\n\n    if resolved_archive_file == archive_file:\n        logger.info(\"loading archive file {}\".format(archive_file))\n    else:\n        logger.info(\n            \"loading archive file {} from cache at {}\".format(\n                archive_file, resolved_archive_file\n            )\n        )\n\n    # Extract archive to temp dir and replace .tar.bz2 if necessary\n    tempdir = None\n    if not os.path.isdir(resolved_archive_file):\n        tempdir = tempfile.mkdtemp()\n        logger.info(\n            \"extracting archive file {} to temp dir {}\".format(\n                resolved_archive_file, tempdir\n            )\n        )\n        ext = os.path.splitext(archive_file)[1][1:]\n        with tarfile.open(resolved_archive_file, \"r:\" + ext) as archive:\n            top_dir = os.path.commonprefix(archive.getnames())\n            archive.extractall(tempdir)\n        os.remove(resolved_archive_file)\n        shutil.move(os.path.join(tempdir, top_dir), resolved_archive_file)\n        shutil.rmtree(tempdir)\n\n    return resolved_archive_file\n\n\ndef url_to_filename(url, etag=None):\n    \"\"\"\n    Convert `url` into a hashed filename in a repeatable way.\n    If `etag` is specified, append its hash to the URL's, delimited\n    by a period.\n    \"\"\"\n    url_bytes = url.encode(\"utf-8\")\n    url_hash = sha256(url_bytes)\n    filename = url_hash.hexdigest()\n\n    if etag:\n        etag_bytes = etag.encode(\"utf-8\")\n        etag_hash = sha256(etag_bytes)\n        filename += \".\" + etag_hash.hexdigest()\n\n    return filename\n\n\ndef cached_path(url_or_filename, cache_dir=None):\n    \"\"\"\n    Given something that might be a URL (or might be a local path),\n    determine which. If it's a URL, download the file and cache it, and\n    return the path to the cached file. If it's already a local path,\n    make sure the file exists and then return the path.\n    \"\"\"\n    if cache_dir is None:\n        cache_dir = PYTORCH_FAIRSEQ_CACHE\n    if isinstance(url_or_filename, Path):\n        url_or_filename = str(url_or_filename)\n    if isinstance(cache_dir, Path):\n        cache_dir = str(cache_dir)\n\n    parsed = urlparse(url_or_filename)\n\n    if parsed.scheme in (\"http\", \"https\", \"s3\"):\n        # URL, so get it from the cache (downloading if necessary)\n        return get_from_cache(url_or_filename, cache_dir)\n    elif os.path.exists(url_or_filename):\n        # File, and it exists.\n        return url_or_filename\n    elif parsed.scheme == \"\":\n        # File, but it doesn't exist.\n        raise EnvironmentError(\"file {} not found\".format(url_or_filename))\n    else:\n        # Something unknown\n        raise ValueError(\n            \"unable to parse {} as a URL or as a local path\".format(url_or_filename)\n        )\n\n\ndef split_s3_path(url):\n    \"\"\"Split a full s3 path into the bucket name and path.\"\"\"\n    parsed = urlparse(url)\n    if not parsed.netloc or not parsed.path:\n        raise ValueError(\"bad s3 path {}\".format(url))\n    bucket_name = parsed.netloc\n    s3_path = parsed.path\n    # Remove '/' at beginning of path.\n    if s3_path.startswith(\"/\"):\n        s3_path = s3_path[1:]\n    return bucket_name, s3_path\n\n\ndef s3_request(func):\n    \"\"\"\n    Wrapper function for s3 requests in order to create more helpful error\n    messages.\n    \"\"\"\n\n    @wraps(func)\n    def wrapper(url, *args, **kwargs):\n        from botocore.exceptions import ClientError\n\n        try:\n            return func(url, *args, **kwargs)\n        except ClientError as exc:\n            if int(exc.response[\"Error\"][\"Code\"]) == 404:\n                raise EnvironmentError(\"file {} not found\".format(url))\n            else:\n                raise\n\n    return wrapper\n\n\n@s3_request\ndef s3_etag(url):\n    \"\"\"Check ETag on S3 object.\"\"\"\n    import boto3\n\n    s3_resource = boto3.resource(\"s3\")\n    bucket_name, s3_path = split_s3_path(url)\n    s3_object = s3_resource.Object(bucket_name, s3_path)\n    return s3_object.e_tag\n\n\n@s3_request\ndef s3_get(url, temp_file):\n    \"\"\"Pull a file directly from S3.\"\"\"\n    import boto3\n\n    s3_resource = boto3.resource(\"s3\")\n    bucket_name, s3_path = split_s3_path(url)\n    s3_resource.Bucket(bucket_name).download_fileobj(s3_path, temp_file)\n\n\ndef request_wrap_timeout(func, url):\n    import requests\n\n    for attempt, timeout in enumerate([10, 20, 40, 60, 60]):\n        try:\n            return func(timeout=timeout)\n        except requests.exceptions.Timeout as e:\n            logger.warning(\n                \"Request for %s timed-out (attempt %d). Retrying with a timeout of %d secs\",\n                url,\n                attempt,\n                timeout,\n                exc_info=e,\n            )\n            continue\n    raise RuntimeError(f\"Unable to fetch file {url}\")\n\n\ndef http_get(url, temp_file):\n    import requests\n    from tqdm import tqdm\n\n    req = request_wrap_timeout(partial(requests.get, url, stream=True), url)\n    content_length = req.headers.get(\"Content-Length\")\n    total = int(content_length) if content_length is not None else None\n    progress = tqdm(unit=\"B\", total=total)\n    for chunk in req.iter_content(chunk_size=1024):\n        if chunk:  # filter out keep-alive new chunks\n            progress.update(len(chunk))\n            temp_file.write(chunk)\n    progress.close()\n\n\ndef get_from_cache(url, cache_dir=None):\n    \"\"\"\n    Given a URL, look for the corresponding dataset in the local cache.\n    If it's not there, download it. Then return the path to the cached file.\n    \"\"\"\n    if cache_dir is None:\n        cache_dir = PYTORCH_FAIRSEQ_CACHE\n    if isinstance(cache_dir, Path):\n        cache_dir = str(cache_dir)\n\n    if not os.path.exists(cache_dir):\n        os.makedirs(cache_dir)\n\n    # Get eTag to add to filename, if it exists.\n    if url.startswith(\"s3://\"):\n        etag = s3_etag(url)\n    else:\n        try:\n            import requests\n\n            response = request_wrap_timeout(\n                partial(requests.head, url, allow_redirects=True), url\n            )\n            if response.status_code != 200:\n                etag = None\n            else:\n                etag = response.headers.get(\"ETag\")\n        except RuntimeError:\n            etag = None\n\n    filename = url_to_filename(url, etag)\n\n    # get cache path to put the file\n    cache_path = os.path.join(cache_dir, filename)\n\n    # If we don't have a connection (etag is None) and can't identify the file\n    # try to get the last downloaded one\n    if not os.path.exists(cache_path) and etag is None:\n        matching_files = fnmatch.filter(os.listdir(cache_dir), filename + \".*\")\n        matching_files = list(filter(lambda s: not s.endswith(\".json\"), matching_files))\n        if matching_files:\n            cache_path = os.path.join(cache_dir, matching_files[-1])\n\n    if not os.path.exists(cache_path):\n        # Download to temporary file, then copy to cache dir once finished.\n        # Otherwise you get corrupt cache entries if the download gets interrupted.\n        with tempfile.NamedTemporaryFile() as temp_file:\n            logger.info(\"%s not found in cache, downloading to %s\", url, temp_file.name)\n\n            # GET file object\n            if url.startswith(\"s3://\"):\n                s3_get(url, temp_file)\n            else:\n                http_get(url, temp_file)\n\n            # we are copying the file before closing it, so flush to avoid truncation\n            temp_file.flush()\n            # shutil.copyfileobj() starts at the current position, so go to the start\n            temp_file.seek(0)\n\n            logger.info(\"copying %s to cache at %s\", temp_file.name, cache_path)\n            with open(cache_path, \"wb\") as cache_file:\n                shutil.copyfileobj(temp_file, cache_file)\n\n            logger.info(\"creating metadata file for %s\", cache_path)\n            meta = {\"url\": url, \"etag\": etag}\n            meta_path = cache_path + \".json\"\n            with open(meta_path, \"w\") as meta_file:\n                output_string = json.dumps(meta)\n                meta_file.write(output_string)\n\n            logger.info(\"removing temp file %s\", temp_file.name)\n\n    return cache_path\n",
        "metaseq/hub_utils.py": "# Copyright (c) Meta Platforms, Inc. and affiliates. All Rights Reserved.\n#\n# This source code is licensed under the MIT license found in the\n# LICENSE file in the root directory of this source tree.\n\nimport argparse\nimport ast\nimport logging\nimport os\nimport re\nimport time\nfrom argparse import Namespace\nfrom typing import List, Optional\nfrom tokenizers import ByteLevelBPETokenizer\n\nimport numpy as np\nimport torch\n\nfrom metaseq import checkpoint_utils, tasks\nfrom metaseq import utils, distributed_utils\nfrom metaseq.data import encoders\nfrom metaseq.dataclass.configs import MetaseqConfig\nfrom metaseq.dataclass.utils import convert_namespace_to_omegaconf\nfrom metaseq.distributed import fsdp_enable_wrap, fsdp_wrap\nfrom metaseq.service.utils import normalize_newlines\nfrom metaseq.file_io import PathManager\n\n\nlogger = logging.getLogger(__name__)\n\n\ndef tensorize_input(tokenizer, prompt):\n    input_ids = torch.LongTensor(tokenizer.encode(prompt).ids).unsqueeze(0)\n    input_ids = torch.cat([torch.tensor([[0]]), input_ids], dim=-1)\n    input_ids = input_ids\n    return input_ids\n\n\ndef get_next_token(logits, tokenizer):\n    pred_next_token = torch.argmax(logits[0, -1], -1)\n    next_token = tokenizer.decode([pred_next_token])\n    next_token = next_token[0].replace(\"Ġ\", \"\")\n    return next_token\n\n\ndef setup_vocab_and_merges(model_path):\n    vocab_file = os.path.join(model_path, \"gpt2-vocab.json\")\n    merges_file = os.path.join(model_path, \"gpt2-merges.txt\")\n    tokenizer = ByteLevelBPETokenizer.from_file(vocab_file, merges_file)\n    return vocab_file, merges_file, tokenizer\n\n\nclass BPEHubInterface(object):\n    \"\"\"PyTorch Hub interface for Byte-Pair Encoding (BPE).\"\"\"\n\n    def __init__(self, bpe, **kwargs):\n        super().__init__()\n        args = argparse.Namespace(bpe=bpe, **kwargs)\n        self.bpe = encoders.build_bpe(args)\n        assert self.bpe is not None\n\n    def encode(self, sentence: str) -> str:\n        return self.bpe.encode(sentence)\n\n    def decode(self, sentence: str) -> str:\n        return self.bpe.decode(sentence)\n\n\nclass TokenizerHubInterface(object):\n    \"\"\"PyTorch Hub interface for tokenization.\"\"\"\n\n    def __init__(self, tokenizer, **kwargs):\n        super().__init__()\n        args = argparse.Namespace(tokenizer=tokenizer, **kwargs)\n        self.tokenizer = encoders.build_tokenizer(args)\n        assert self.tokenizer is not None\n\n    def encode(self, sentence: str) -> str:\n        return self.tokenizer.encode(sentence)\n\n    def decode(self, sentence: str) -> str:\n        return self.tokenizer.decode(sentence)\n\n\nclass GeneratorInterface:\n    \"\"\"\n    PyTorch Hub interface for generating sequences from a pre-trained\n    translation or language model.\n    \"\"\"\n\n    def __init__(self, cfg: MetaseqConfig):\n        self.cfg = cfg\n        if isinstance(self.cfg, Namespace):\n            self.cfg = convert_namespace_to_omegaconf(self.cfg)\n\n    def encode_fn(self, x: str):\n        \"\"\"\n        encode a given value to list of bpe tokens\n        \"\"\"\n        assert self.bpe is not None\n        return self.bpe.bpe.encode(normalize_newlines(x)).ids\n\n    def decode_fn(self, x: List[int]) -> str:\n        \"\"\"\n        Decode a list of tokens x to a string\n        \"\"\"\n        assert self.bpe is not None\n        return self.bpe.bpe.decode(x)\n\n    def load_model(self):\n        if self.cfg.common.model_parallel_size == 1:\n            r = distributed_utils.get_global_rank()\n        else:\n            r = distributed_utils.get_data_parallel_rank()\n\n        suffix = self.cfg.checkpoint.checkpoint_suffix\n\n        sharded_files = PathManager.ls(\n            re.sub(\".pt$\", f\"{suffix}*\", self.cfg.common_eval.path)\n        )\n        if len(sharded_files) > 0 and \"-shard\" in sharded_files[0]:\n            # We are loading a sharded checkpoint\n            suffix += f\"-shard{r}\"\n        else:\n            suffix += \"\"\n\n        self.cfg.checkpoint.checkpoint_suffix = suffix\n\n        utils.import_user_module(self.cfg.common)\n\n        # Fix seed for stochastic decoding\n        if (\n            self.cfg.common.seed is not None\n            and not self.cfg.generation.no_seed_provided\n        ):\n            np.random.seed(self.cfg.common.seed)\n            utils.set_torch_seed(self.cfg.common.seed)\n\n        # Setup task, e.g., translation\n        task = tasks.setup_task(self.cfg.task)\n\n        def _build_model(cfg, task):\n            model = task.build_model(cfg.model).cuda()\n            model.make_generation_fast_()\n            return fsdp_wrap(model)\n\n        # Load the model\n        overrides = ast.literal_eval(self.cfg.common_eval.model_overrides)\n        logger.info(\"loading model(s) from {}\".format(self.cfg.common_eval.path))\n\n        def _load_checkpoint():\n            return checkpoint_utils.load_model_ensemble_and_task(\n                utils.split_paths(self.cfg.common_eval.path),\n                arg_overrides=overrides,\n                task=task,\n                suffix=self.cfg.checkpoint.checkpoint_suffix,\n                strict=(self.cfg.checkpoint.checkpoint_shard_count == 1),\n                num_shards=self.cfg.checkpoint.checkpoint_shard_count,\n                build_model_hook=_build_model,\n            )\n\n        if self.cfg.distributed_training.ddp_backend == \"fully_sharded\":\n            with fsdp_enable_wrap(\n                self.cfg.distributed_training,\n                use_sharded_state=self.cfg.distributed_training.use_sharded_state,\n            ):\n                models, _model_args, _task = _load_checkpoint()\n        else:\n            models, _model_args, _task = _load_checkpoint()\n        # Set dictionaries\n        src_dict = task.source_dictionary\n        tgt_dict = task.target_dictionary\n\n        # Handle tokenization and BPE\n        bpe = task.build_bpe(self.cfg.bpe)\n\n        # Set state\n        self.bpe = bpe\n        self.task = task\n        self.models = models\n        self.src_dict = src_dict\n        self.tgt_dict = tgt_dict\n\n        # store special token indices for\n        self._pad_token_ind = self.tgt_dict.pad_index\n        self._special_token_inds = {\n            self.tgt_dict.eos_index,\n            self.tgt_dict.pad_index,\n            self.tgt_dict.bos_index,\n            self.tgt_dict.unk_index,\n        }\n\n        return models\n\n    def generate(\n        self,\n        inputs: List[List[int]],\n        min_tokens: List[int] = None,\n        max_tokens: List[int] = None,\n        temperature: float = 1.0,\n        top_p: float = -1.0,\n        logprobs: int = 0,\n        n: int = 1,\n        best_of: Optional[int] = None,\n        echo: bool = False,\n        stop: Optional[List[int]] = None,\n        seed: Optional[int] = None,\n        use_cuda: bool = True,\n        omega_bound: float = 0.3,\n        lambda_decay: float = -1,\n        alpha_presence: float = 0.0,\n        alpha_frequency: float = 0.0,\n        alpha_presence_src: float = 0.0,\n        alpha_frequency_src: float = 0.0,\n        alpha_src_penalty_end_idx: int = -1,\n    ):\n        \"\"\"\n        Generate from sequences.\n        Parameters match those of the OpenAI API.\n        https://beta.openai.com/docs/api-reference/completions/create\n\n        For discussion of repetition penalties, see\n        https://github.com/facebookresearch/metaseq/pull/306\n\n        inputs: a list of pre-tokenized prompts\n        min_tokens: blocks EOS until at least this many tokens is provided\n        max_tokens: forces EOS after this many tokens\n        temperature: softmax temperature\n        top_p: nucleus probability\n        log_probs: return this cutoff of the probability distribution\n        best_of: beam size\n        n: number of beams to return. must be <= best_of\n        echo: if true, returned text/tokens/scores includes the prompt.\n            This is useful for getting PPL evaluations.\n        stop: a list of terminating tokens\n        seed: an integer if desired\n        use_cuda: should we use GPUs.\n        omega_bound: lower p-bound when decaying nucleus\n        lamda_decay: decay factor for decaying p in nucleus sampling\n            default -1 disables.\n        alpha_presence: repetition penalty for token presence in\n            current generation\n        alpha_frequency: repetition penalty for token frequency in\n            current generation\n        alpha_presence_src: repetition penalty for token presence in\n            incoming context\n        alpha_frequency_src: repetition penalty for token frequency in\n            incoming context\n        alpha_src_penalty_end_idx: index of the last token in incoming\n            context to consider for repetition penalty\n        \"\"\"\n        if seed:\n            utils.set_torch_seed(seed)\n        start_time = time.time()\n        total_generation_time = 0\n\n        # Initialize generator\n        if not best_of:\n            best_of = n\n        assert best_of >= n\n        self.cfg.generation.sampling_topp = top_p if top_p > 0 else -1\n        self.cfg.generation.sampling = top_p > 0.0\n        self.cfg.generation.beam = best_of\n        if temperature > 0:\n            self.cfg.generation.temperature = temperature\n        elif temperature == 0:\n            self.cfg.generation.sampling = False\n            self.cfg.generation.temperature = 1.0\n            self.cfg.generation.sampling_topp = -1\n        elif temperature < 0:\n            raise ValueError(\"temperature must be >= 0 and <= 1\")\n\n        MAX_SEQ_LEN = utils.resolve_max_positions(\n            self.task.max_positions(), *[model.max_positions() for model in self.models]\n        )\n\n        # TODO(roller): simplify\n        retval = []\n        tokens = [torch.LongTensor(t) for t in inputs]\n        lengths = [len(t) for t in inputs]\n        batches = self.task.get_batch_iterator(\n            dataset=self.task.build_dataset_for_inference(tokens, lengths),\n            max_tokens=None,\n            max_sentences=None,\n            max_positions=None,\n            ignore_invalid_inputs=False,\n            skip_remainder_batch=False,\n        ).next_epoch_itr(shuffle=False)\n        for batch in batches:\n            src_tokens = batch[\"net_input\"][\"src_tokens\"]\n            src_lengths = batch[\"net_input\"][\"src_lengths\"]\n            batchsize = src_tokens.size(0)\n\n            # set generation args\n            # prevent us from ever generating past our max sequence length\n            if max_tokens is None:\n                max_tokens = [MAX_SEQ_LEN] * batchsize\n            if min_tokens is None:\n                min_tokens = [0] * batchsize\n            total_max_tokens = min(\n                MAX_SEQ_LEN, max(max_tokens) + src_lengths.max().item()\n            )\n            total_min_tokens = max(min_tokens) + src_lengths.max().item()\n            self.cfg.generation.min_len = total_min_tokens\n            self.cfg.generation.max_len_b = total_max_tokens\n            self.cfg.generation.max_len_a = 0\n\n            logger.info(f\"Preparing generator with settings {self.cfg.generation}\")\n            need_logprobs = True if logprobs > 0 else False\n            extra_gen_cls_kwargs = {\n                \"stop\": stop,\n                \"need_logprobs\": need_logprobs,\n                \"omega_bound\": omega_bound,\n                \"lambda_decay\": lambda_decay,\n                \"alpha_presence\": alpha_presence,\n                \"alpha_frequency\": alpha_frequency,\n                \"alpha_presence_src\": alpha_presence_src,\n                \"alpha_frequency_src\": alpha_frequency_src,\n                \"alpha_src_penalty_end_idx\": alpha_src_penalty_end_idx,\n            }\n            generator = self.task.build_generator(\n                self.models,\n                self.cfg.generation,\n                extra_gen_cls_kwargs=extra_gen_cls_kwargs,\n            )\n\n            # okay actually generate\n            logger.info(f\"Executing generation on input tensor size {src_tokens.shape}\")\n            if use_cuda:\n                batch = utils.move_to_cuda(batch)\n\n            translate_start_time = time.time()\n            translations = self.task.inference_step(generator, self.models, batch)\n            translate_time = time.time() - translate_start_time\n            total_generation_time += translate_time\n\n            all_tokens = translations[\"tokens\"].cpu()[: len(inputs)]\n            all_scores = translations[\"scores\"].cpu()[: len(inputs)]\n            if logprobs > 0:\n                all_distributions = translations[\"distributions\"].cpu()[: len(inputs)]\n            else:\n                all_distributions = None\n\n            # actually turn everything into strings\n            for i in range(all_tokens.size(0)):\n                beams = []\n                for j in range(n):\n                    # first beam is always the highest scoring\n                    tokens = all_tokens[i, j].tolist()\n                    scores = all_scores[i, j].tolist()\n                    distributions = all_distributions[i, j] if logprobs > 0 else None\n\n                    prompt_len = lengths[i]\n\n                    tokens, scores, distributions = self._filter_special(\n                        self._pad_token_ind,\n                        self._special_token_inds,\n                        tokens,\n                        scores,\n                        distributions,\n                    )\n\n                    if echo:\n                        # don't cut off prompt\n                        pass\n                    else:\n                        # cut off prompt\n                        tokens = tokens[prompt_len + 1 :][: max_tokens[i]]\n                        scores = scores[prompt_len + 1 :][: max_tokens[i]]\n                        if logprobs > 0:\n                            distributions = distributions[prompt_len + 1 :][\n                                : max_tokens[i]\n                            ]\n\n                    # cut off the starting token\n                    tokens_no_eos = tokens[1:] if echo else tokens\n                    scores_with_eos = [None] + scores[1:] if echo else scores\n                    # turn it into a string\n                    text = self.bpe.bpe.decode(tokens_no_eos)\n                    # re-encode it so we get offsets\n                    token_offsets = [s for s, e in self.bpe.bpe.encode(text).offsets]\n\n                    result = {\n                        \"text\": text,\n                        \"tokens\": [self.bpe.bpe.decode([t]) for t in tokens],\n                        # text offset is useful for cutting off prompts or prefixes\n                        # or evaluating PPL on just a subset of tokens\n                        \"text_offset\": token_offsets,\n                        \"token_scores\": scores_with_eos,\n                    }\n                    if logprobs > 0:\n                        # final result is a List[Dict[str, float]]\n                        # where each item in the list corresponds to a token in the\n                        # sequence, and the dict provides the probabilities of the\n                        # top-k tokens at that timestep.\n                        out_logprobs = []\n                        all_top_toks, all_top_scores = distributions.topk(\n                            k=logprobs, dim=-1\n                        )\n                        for top_scores, top_toks in zip(all_top_toks, all_top_scores):\n                            lp = {\n                                self.bpe.bpe.decode([t.item()]): s.item()\n                                for t, s in zip(top_toks, top_scores)\n                            }\n                            out_logprobs.append(lp)\n                        if echo:\n                            # use null instead of giving bunk probs for EOS token\n                            result[\"top_logprobs\"] = [None] + out_logprobs[1:]\n                        else:\n                            result[\"top_logprobs\"] = out_logprobs\n\n                    else:\n                        result[\"top_logprobs\"] = None\n\n                    beams.append(result)\n                retval.append(beams)\n\n        logger.info(\n            \"Total time: {:.3f} seconds; generation time: {:.3f}\".format(\n                time.time() - start_time, total_generation_time\n            )\n        )\n        return retval\n\n    @staticmethod\n    def _filter_special(\n        pad_token_ind,\n        special_token_inds,\n        tokens: List[int],\n        scores: List[float],\n        distributions,\n    ):\n        \"\"\"\n        Cut off tokens after finding a special tokens.\n        \"\"\"\n\n        # tokens is a 1D list of token IDs of length seqlen\n        # scores is a 1D list of log-probability scores for those tokens (length seqlen)\n        # distributions (optional) is a seqlen x vocab_size tensor corresponding to\n        # the full distribution of predictions at each timestep\n        output = []\n        mask = []\n        for i, (t, s) in enumerate(zip(tokens, scores)):\n            if t == pad_token_ind:\n                # simply skip pads\n                mask.append(False)\n                continue\n            if t in special_token_inds and i > 0:\n                # and other special tokens should end things\n                mask.append(False)\n                break\n            mask.append(True)\n            output.append((t, s))\n        new_tokens, new_scores = zip(*output)\n\n        # cut off at stop and drop pads\n        if distributions is not None:\n            # If we broke early in the loop above, ensure that we\n            # fill mask with False upto distributions.shape[0]\n            assert (\n                len(mask) <= distributions.shape[0]\n            ), \"Mask cannot be larger than the number of tokens in disribution (distributions.shape[0])\"\n            mask.extend([False] * (distributions.shape[0] - len(mask)))\n            distributions = distributions[mask]\n\n        return list(new_tokens), list(new_scores), distributions\n",
        "metaseq/incremental_decoding_utils.py": "# Copyright (c) Meta Platforms, Inc. and affiliates. All Rights Reserved.\n#\n# This source code is licensed under the MIT license found in the\n# LICENSE file in the root directory of this source tree.\n\nimport uuid\nfrom typing import Dict, Optional\n\nfrom torch import Tensor\nfrom typing_extensions import Protocol\n\n\nclass HasIncrementalState(Protocol):\n    def get_incremental_state(\n        self,\n        incremental_state: Optional[Dict[str, Dict[str, Optional[Tensor]]]],\n        key: str,\n    ) -> Optional[Dict[str, Optional[Tensor]]]:\n        \"\"\"Helper for getting incremental state for an nn.Module.\"\"\"\n\n    def set_incremental_state(\n        self,\n        incremental_state: Optional[Dict[str, Dict[str, Optional[Tensor]]]],\n        key: str,\n        value: Dict[str, Optional[Tensor]],\n    ) -> Optional[Dict[str, Dict[str, Optional[Tensor]]]]:\n        \"\"\"Helper for setting incremental state for an nn.Module.\"\"\"\n\n\nclass IncrementalState(object):\n    def __init__(self, *args, **kwargs):\n        super().__init__(*args, **kwargs)\n        self.init_incremental_state()\n\n    def init_incremental_state(self):\n        self._incremental_state_id = str(uuid.uuid4())\n\n    def _get_full_incremental_state_key(self, key: str) -> str:\n        return \"{}.{}\".format(self._incremental_state_id, key)\n\n    def get_incremental_state(\n        self,\n        incremental_state: Optional[Dict[str, Dict[str, Optional[Tensor]]]],\n        key: str,\n    ) -> Optional[Dict[str, Optional[Tensor]]]:\n        \"\"\"Helper for getting incremental state for an nn.Module.\"\"\"\n        full_key = self._get_full_incremental_state_key(key)\n        if incremental_state is None or full_key not in incremental_state:\n            return None\n        return incremental_state[full_key]\n\n    def set_incremental_state(\n        self,\n        incremental_state: Optional[Dict[str, Dict[str, Optional[Tensor]]]],\n        key: str,\n        value: Dict[str, Optional[Tensor]],\n    ) -> Optional[Dict[str, Dict[str, Optional[Tensor]]]]:\n        \"\"\"Helper for setting incremental state for an nn.Module.\"\"\"\n        if incremental_state is not None:\n            full_key = self._get_full_incremental_state_key(key)\n            incremental_state[full_key] = value\n        return incremental_state\n\n\ndef with_incremental_state(cls):\n    cls.__bases__ = (IncrementalState,) + tuple(\n        b for b in cls.__bases__ if b != IncrementalState\n    )\n    return cls\n",
        "metaseq/launcher/__init__.py": "",
        "metaseq/launcher/opt_baselines.py": "#!/usr/bin/env python3 -u\n# Copyright (c) Meta Platforms, Inc. and affiliates. All Rights Reserved.\n#\n# This source code is licensed under the MIT license found in the\n# LICENSE file in the root directory of this source tree.\n\n\"\"\"\nThis sweep script takes some additional optional arguments. See add_extra_options_func\nfor more details.\n\"\"\"\nimport os\n\nfrom metaseq.launcher.opt_job_constants import (\n    TOTAL_TRAIN_TOKENS,\n    TOTAL_WARMUP_TOKENS,\n    MODEL_SIZES,\n    VALID_SUBSETS,\n)\nfrom metaseq.launcher.sweep import (\n    hyperparam,\n    get_env_from_args,\n    main as sweep_main,\n)\n\ntry:\n    # internal logic denoting where data locations are\n    from metaseq_internal.constants import DATA_LOCATIONS\nexcept ImportError:\n    from metaseq.launcher.opt_job_constants import DATA_LOCATIONS\n\n# have to do this at the module level, unfortunately; unable to use args.<env>\nfor _cluster, _folder in DATA_LOCATIONS.items():\n    if os.path.exists(_folder):\n        try:\n            import metaseq_internal  # noqa: F401\n            from metaseq_internal.fb_sweep.dependency_checks import *  # noqa\n        except ImportError:\n            print(\"\\n\\nmetaseq_internal not installed! Proceeding...\")\n            pass\n        break\n\n\ndef add_extra_options_func(parser):\n    # NOTE we shouldn't add new options here... track changes via git instead\n    parser.add_argument(\n        \"--restore-file\", help=\"load an existing checkpoint for continuing training\"\n    )\n    parser.add_argument(\n        \"--reset-dataloader\",\n        action=\"store_true\",\n        help=\"reset the dataloader to epoch 1\",\n    )\n    parser.add_argument(\"--model-size\", choices=MODEL_SIZES.keys(), required=True)\n    parser.add_argument(\n        \"--no-save-dir\", action=\"store_true\", help=\"avoid saving with hparams\"\n    )\n\n    # Args related to benchmarking and profiling\n    parser.add_argument(\n        \"--benchmark\",\n        action=\"store_true\",\n        help=\"use synthetic data and only train for 50 steps (for benchmarking)\",\n    )\n    parser.add_argument(\n        \"--profile\",\n        default=False,\n        action=\"store_true\",\n    )\n    parser.add_argument(\"--max-update\", \"--mu\", type=int, default=None)\n    parser.add_argument(\"--max-epoch\", \"--me\", type=int, default=None)\n    parser.add_argument(\n        \"--disable-validation\", action=\"store_true\", help=\"skip doing validation\"\n    )\n    parser.add_argument(\n        \"--circleci\", action=\"store_true\", help=\"running a baseline test on circleci\"\n    )\n\n\ndef get_grid(args):\n    # Infer data path if not given\n    DATA_ROOT = \"\"\n    valid_subsets = VALID_SUBSETS\n    if not args.benchmark:\n        if args.data is None:\n            cluster_env = get_env_from_args(args)\n            data_loc_by_env = DATA_LOCATIONS[cluster_env]\n            if args.circleci:\n                data_loc_by_env = \"./gpu_tests/circleci\"\n                valid_subsets = [\"BookCorpusFair\"]\n            args.data = os.path.join(data_loc_by_env, \"corpus_dedup_10_10_1_0.05_exp29\")\n            if os.path.exists(args.data):\n                DATA_ROOT = data_loc_by_env\n            else:\n                raise RuntimeError(\n                    \"Where are you running this?! Check DATA_LOCATIONS or pass --data \"\n                    \"pointing to a directory with 'data' and 'tokenizers' folders.\"\n                )\n        else:\n            DATA_ROOT = args.data\n            args.data = os.path.join(args.data, \"data\")\n\n    SEQ_LEN = 2048\n    size = MODEL_SIZES[args.model_size]\n    # updates = 300B tokens / 2048 seq_len / 1024 batchsize\n\n    total_gpus = args.num_gpus * args.num_nodes\n\n    # TODO: fix training to run with 1 gpu (see Enable sweep scripts to run with a single GPU #176)\n    if args.num_gpus < 2:\n        raise ValueError(\"Need at least two gpus to run model parallel code\")\n    if total_gpus < size.model_parallel:\n        raise ValueError(\n            \"Total gpus (num_gpus * num_nodes) must be great than model parallel factor\"\n        )\n    if total_gpus % size.model_parallel != 0:\n        raise ValueError(\n            \"Total gpus (num_gpus * num_nodes) must be divisible by model parallel factor\"\n        )\n\n    total_gpus = (args.num_gpus * args.num_nodes) // size.model_parallel\n    ddp_bsz = (size.batch_size // total_gpus) // SEQ_LEN\n    total_updates = args.max_update\n    total_epochs = args.max_epoch\n    if total_updates is None:\n        total_updates = int(TOTAL_TRAIN_TOKENS) // size.batch_size\n    warmup_updates = int(TOTAL_WARMUP_TOKENS) // size.batch_size\n    log_interval = 1\n\n    grid = []\n\n    # default streaming_lm task config\n    task_config = [\n        hyperparam(\"--task\", \"streaming_language_modeling\"),\n        hyperparam(\n            \"--sample-break-mode\",\n            \"none\",\n            save_dir_key=lambda val: f\"bm_{val}\" if not no_save_params else \"\",\n        ),\n        hyperparam(\n            \"--vocab-filename\",\n            os.path.join(DATA_ROOT, \"tokenizers/gpt2-vocab.json\"),\n            save_dir_key=lambda _: \"gpt2\" if not no_save_params else \"\",\n        ),\n        hyperparam(\n            \"--merges-filename\", os.path.join(DATA_ROOT, \"tokenizers/gpt2-merges.txt\")\n        ),\n    ]\n\n    # separate task config for dummy_lm\n    if args.benchmark:\n        # Overrides for speed benchmarking\n        args.data = None\n        task_config = [\n            hyperparam(\"--task\", \"dummy_lm\", save_dir_key=lambda val: val),\n            hyperparam(\n                \"--dict-size\", 51200 - 4\n            ),  # TODO(susan): what is this -4 sorcery? relic of more nmt things?\n            hyperparam(\"--save-interval-epochs\", 0),\n            hyperparam(\"--save-interval-updates\", 0),\n        ]\n        total_updates = 50\n        warmup_updates = 50\n        log_interval = 5\n\n    grid += task_config\n\n    if args.profile:\n        grid += [hyperparam(\"--profile\")]\n\n    no_save_params = args.no_save_dir\n    args.snapshot_code = True\n\n    if not args.benchmark:\n        grid += [\n            hyperparam(\n                \"--valid-subset\", \",\".join(f\"valid/{ss}\" for ss in valid_subsets)\n            ),\n            hyperparam(\"--save-interval-updates\", 2000),\n        ]\n\n    grid += [\n        hyperparam(\"--train-subset\", \"train\"),\n        hyperparam(\"--ignore-unused-valid-subsets\"),\n        hyperparam(\"--num-workers\", 8),\n        hyperparam(\"--num-workers-valid\", 1),\n        hyperparam(\"--validate-interval-updates\", 2000),\n        hyperparam(\n            \"--memory-efficient-fp16\",\n            save_dir_key=lambda val: \"me_fp16\" if not no_save_params else \"\",\n        ),\n        hyperparam(\"--fp16-init-scale\", 4),\n        # we set this for the main run but it's probably nt needed here\n        # hyperparam(\"--threshold-loss-scale\", 0.25, save_dir_key=lambda val: f\"minscale{val}\"),\n        hyperparam(\n            \"--ddp-backend\",\n            \"fully_sharded\",\n            save_dir_key=lambda val: \"fsdp\" if not no_save_params else \"\",\n        ),\n        hyperparam(\"--no-reshard-after-forward\", save_dir_key=lambda _: \"zero2\"),\n        hyperparam(\"--use-sharded-state\"),\n        hyperparam(\"--checkpoint-activations\"),\n        hyperparam(\"--model-parallel-size\", size.model_parallel),\n        hyperparam(\"--criterion\", \"vocab_parallel_cross_entropy\"),\n        hyperparam(\"--distribute-checkpointed-activations\"),\n        hyperparam(\"--tensor-parallel-init-model-on-gpu\"),\n        # Flags to match exact same initialization of Megatron code for exp 12.00\n        hyperparam(\"--full-megatron-init\"),\n        hyperparam(\"--megatron-init-sigma\", 0.006),\n        hyperparam(\n            \"--activation-fn\",\n            \"relu\",\n            save_dir_key=lambda x: x if not no_save_params else \"\",\n        ),\n        hyperparam(\n            \"--arch\",\n            \"transformer_lm_megatron\",\n            save_dir_key=lambda val: val if not no_save_params else \"\",\n        ),\n        hyperparam(\"--share-decoder-input-output-embed\"),\n        hyperparam(\n            \"--decoder-layers\",\n            size.n_layers,\n            save_dir_key=lambda val: f\"nlay{val}\" if not no_save_params else \"\",\n        ),\n        hyperparam(\n            \"--decoder-embed-dim\",\n            size.emb_size,\n            save_dir_key=lambda val: f\"emb{val}\" if not no_save_params else \"\",\n        ),\n        hyperparam(\"--decoder-ffn-embed-dim\", size.ffn_size),\n        hyperparam(\"--decoder-attention-heads\", size.n_heads),\n        # Switch to learned position embeddings for exp 12.00, without scaling\n        hyperparam(\n            \"--decoder-learned-pos\",\n            save_dir_key=lambda _: \"lrnpos\" if not no_save_params else \"\",\n        ),\n        hyperparam(\n            \"--no-scale-embedding\",\n            save_dir_key=lambda _: \"0emb_scale\" if not no_save_params else \"\",\n        ),\n        hyperparam(\n            \"--tokens-per-sample\",\n            SEQ_LEN,\n            save_dir_key=lambda val: f\"tps{val}\" if not no_save_params else \"\",\n        ),\n        hyperparam(\"--optimizer\", \"adam\", save_dir_key=lambda val: val),\n        # GPT-3 uses \"(0.9, 0.95)\"\n        hyperparam(\n            \"--adam-betas\",\n            f\"(0.9, 0.95)\",\n            save_dir_key=lambda val: \"b2_{}\".format(eval(val)[1])\n            if not no_save_params\n            else \"\",\n        ),\n        # Sometimes lowering --adam-eps to 1e-6 can stabilize training\n        hyperparam(\n            \"--adam-eps\",\n            1e-8,\n            save_dir_key=lambda val: f\"eps{val}\" if not no_save_params else \"\",\n        ),\n        # GPT-3 used --clip-norm=1.0\n        hyperparam(\n            \"--clip-norm\",\n            1.0,\n            save_dir_key=lambda val: f\"cl{val}\" if not no_save_params else \"\",\n        ),\n        hyperparam(\"--clip-norm-type\", \"l2\"),\n        hyperparam(\"--lr-scheduler\", \"polynomial_decay\"),\n        hyperparam(\n            \"--lr\",\n            size.lr,\n            save_dir_key=lambda val: f\"lr{val:.3g}\" if not no_save_params else \"\",\n        ),\n        hyperparam(\n            \"--end-learning-rate\",\n            size.lr * 0.1,\n            save_dir_key=lambda val: f\"endlr{val:.3g}\" if not no_save_params else \"\",\n        ),\n        hyperparam(\n            \"--warmup-updates\",\n            warmup_updates,\n            save_dir_key=lambda val: f\"wu{val}\" if not no_save_params else \"\",\n        ),\n        hyperparam(\"--total-num-update\", total_updates),\n        hyperparam(\n            \"--dropout\",\n            0.1,\n            save_dir_key=lambda val: f\"dr{val}\" if not no_save_params else \"\",\n        ),\n        hyperparam(\n            \"--attention-dropout\",\n            0.1,\n            save_dir_key=lambda val: f\"atdr{val}\" if not no_save_params else \"\",\n        ),\n        hyperparam(\n            \"--no-emb-dropout\",\n            save_dir_key=lambda _: \"0emb_dr\" if not no_save_params else \"\",\n        ),\n        hyperparam(\n            \"--weight-decay\",\n            0.1,\n            save_dir_key=lambda val: f\"wd{val}\" if not no_save_params else \"\",\n        ),\n        hyperparam(\n            \"--batch-size\",\n            ddp_bsz,\n            save_dir_key=lambda val: f\"ms{val}\" if not no_save_params else \"\",\n        ),\n        hyperparam(\n            \"--update-freq\",\n            1,\n            save_dir_key=lambda val: f\"uf{val}\" if not no_save_params else \"\",\n        ),\n        hyperparam(\n            \"--max-update\",\n            total_updates,\n            save_dir_key=lambda val: f\"mu{val}\" if not no_save_params else \"\",\n        ),\n        hyperparam(\n            \"--seed\",\n            1,\n            save_dir_key=lambda val: f\"s{val}\" if not no_save_params else \"\",\n        ),\n        hyperparam(\"--log-format\", \"json\"),\n        hyperparam(\"--log-interval\", log_interval),\n        hyperparam(\"--required-batch-size-multiple\", 1),\n    ]\n    if args.restore_file:\n        grid += [hyperparam(\"--restore-file\", args.restore_file)]\n    if args.reset_dataloader:\n        grid += [hyperparam(\"--reset-dataloader\")]\n\n    if args.disable_validation:\n        grid += [hyperparam(\"--disable-validation\")]\n\n    if args.max_epoch is not None:\n        grid += [\n            hyperparam(\n                \"--max-epoch\",\n                total_epochs,\n                save_dir_key=lambda val: f\"me{val}\" if not no_save_params else \"\",\n            )\n        ]\n\n    return grid\n\n\ndef postprocess_hyperparams(args, config):\n    pass\n\n\ndef cli_main():\n    sweep_main(\n        get_grid, postprocess_hyperparams, add_extra_options_func=add_extra_options_func\n    )\n\n\nif __name__ == \"__main__\":\n    cli_main()\n",
        "metaseq/launcher/opt_job_constants.py": "# Copyright (c) Meta Platforms, Inc. and affiliates. All Rights Reserved.\n#\n# This source code is licensed under the MIT license found in the\n# LICENSE file in the root directory of this source tree.\n\nfrom dataclasses import dataclass\nfrom enum import Enum\n\n\n@dataclass\nclass Size:\n    n_layers: int\n    emb_size: int\n    n_heads: int\n    d_head: int\n    batch_size: int\n    lr: float\n    model_parallel: int\n\n    @property\n    def ffn_size(self):\n        return 4 * self.emb_size\n\n\n# from appendix b of https://arxiv.org/pdf/2005.14165.pdf\n# see table 2.1 in https://arxiv.org/pdf/2005.14165.pdf\n\n# assert all sizes make sense\nTOTAL_TRAIN_TOKENS = 300e9\nTOTAL_WARMUP_TOKENS = 375e6\nM = 1024 * 1024  # 1 million\nMODEL_SIZES = {\n    \"8m_mp1\": Size(4, 128, 2, 64, int(0.125 * M), 1.0e-3, 1),  # tiny with 1 mp\n    \"8m\": Size(4, 128, 2, 64, int(0.125 * M), 1.0e-3, 2),  # tiny with 2 mp\n    \"125m\": Size(12, 768, 12, 64, int(0.5 * M), 6.0e-4, 2),  # small\n    \"350m\": Size(24, 1024, 16, 64, int(0.5 * M), 3.0e-4, 2),  # medium\n    \"760m\": Size(24, 1536, 16, 96, int(0.5 * M), 2.5e-4, 2),  # large\n    \"1.3b\": Size(24, 2048, 32, 64, int(1.0 * M), 2.0e-4, 2),  # xl\n    \"2.7b\": Size(32, 2560, 32, 80, int(1.0 * M), 1.6e-4, 4),\n    \"6.7b\": Size(32, 4096, 32, 128, int(2.0 * M), 1.2e-4, 2),\n    \"13b\": Size(40, 5120, 40, 128, int(4.0 * M), 1.0e-4, 2),\n    \"30b\": Size(48, 7168, 56, 128, int(4.0 * M), 1.0e-4, 2),\n    \"66b\": Size(64, 9216, 72, 128, int(2.0 * M), 8e-5, 8),\n    \"175b\": Size(96, 12288, 96, 128, int(2.0 * M), 3e-5, 8),\n}\n\n# from appendix b of https://arxiv.org/pdf/2005.14165.pdf\n# see table 2.1 in https://arxiv.org/pdf/2005.14165.pdf\n\nfor name, size in MODEL_SIZES.items():\n    assert size.n_heads * size.d_head == size.emb_size, name\n\n\nclass ComputeEnvs(Enum):\n    AWS = \"aws\"\n    AZURE = \"azure\"\n    FAIR = \"fair\"\n    RSC = \"rsc\"\n\n\nDATA_LOCATIONS = {\n    ComputeEnvs.AZURE: \"/data/opt\",\n}\n\nDEFAULT_PARTITION = {\n    ComputeEnvs.FAIR: \"learnfair\",\n    ComputeEnvs.AZURE: None,\n    ComputeEnvs.AWS: None,\n}\n\nDEFAULT_PREFIX = {\n    ComputeEnvs.AZURE: \"/shared/home\",\n    ComputeEnvs.AWS: \"/checkpoints\",\n    ComputeEnvs.FAIR: \"/checkpoint\",\n}\n\nDEFAULT_CPU_PER_TASK = {\n    ComputeEnvs.AZURE: 12,\n    ComputeEnvs.AWS: 12,\n    ComputeEnvs.FAIR: 10,\n}\nVALID_SUBSETS = [\n    \"BookCorpusFair\",\n    \"CommonCrawl\",\n    \"DM_Mathematics\",\n    \"Gutenberg_PG-19\",\n    \"HackerNews\",\n    \"OpenSubtitles\",\n    \"OpenWebText2\",\n    \"USPTO\",\n    \"Wikipedia_en\",\n    \"redditflattened\",\n    \"stories\",\n]\n",
        "metaseq/launcher/slurm.py": "# Copyright (c) Meta Platforms, Inc. and affiliates. All Rights Reserved.\n#\n# This source code is licensed under the MIT license found in the\n# LICENSE file in the root directory of this source tree.\n\nimport datetime\nimport fnmatch\nimport hashlib\nimport itertools\nimport os\nimport random\nimport shlex\nimport shutil\nimport subprocess\nimport textwrap\nfrom collections import OrderedDict\nfrom pathlib import Path\n\nimport metaseq\nfrom metaseq.utils import get_random_port\nfrom metaseq.launcher.tombyard import tombstones\nfrom metaseq.launcher.sweep import get_env_from_args\n\ntry:\n    import metaseq_internal\n    import metaseq_internal.fb_sweep.internal as internal\n\n    has_internal = True\nexcept ImportError:\n    has_internal = False\n\n\ndef main(get_grid, postprocess_hyperparams, args):\n    def dry_run(msg):\n        if args.dry_run:\n            print(f\"| dry-run:  {msg}\")\n        return args.dry_run\n\n    if args.local:\n        args.num_nodes = 1\n\n    # compute all possible hyperparameter configurations\n    grid = get_grid(args)\n    grid_product = list(itertools.product(*[hp.values for hp in grid]))\n\n    # randomly shuffle configurations\n    random.seed(args.seed)\n    random.shuffle(grid_product)\n\n    launch_train(args, grid, grid_product, dry_run, postprocess_hyperparams)\n\n\ndef copy_all_python_files(\n    source,\n    snapshot_main_dir,\n    code_snapshot_hash,\n    recurse_dirs=\"metaseq,scripts\",\n):\n    \"\"\"\n    Copies following files from source to destination:\n        a) all *.py files at direct source location.\n        b) all metaseq/*.py recursively (default); recurse through comma-separated recurse_dirs\n    \"\"\"\n\n    def include_patterns(*patterns):\n        \"\"\"Factory function that can be used with copytree() ignore parameter.\n\n        Arguments define a sequence of glob-style patterns\n        that are used to specify what files to NOT ignore.\n        Creates and returns a function that determines this for each directory\n        in the file hierarchy rooted at the source directory when used with\n        shutil.copytree().\n        from: https://stackoverflow.com/questions/52071642/python-copying-the-files-with-include-pattern\n        \"\"\"\n\n        def _ignore_patterns(path, names):\n            keep = set(\n                name for pattern in patterns for name in fnmatch.filter(names, pattern)\n            )\n            ignore = set(\n                name\n                for name in names\n                if name not in keep and not os.path.isdir(os.path.join(path, name))\n            )\n            return ignore\n\n        return _ignore_patterns\n\n    def pys_but_no_dirs(path, names):\n        pys = set(fnmatch.filter(names, \"*.py\"))\n        return [name for name in names if name not in pys]\n\n    destination = os.path.join(snapshot_main_dir, code_snapshot_hash)\n    # copy root files:\n    shutil.copytree(source, destination, ignore=pys_but_no_dirs)\n    # copy folders\n    for d in recurse_dirs.split(\",\"):\n        shutil.copytree(\n            os.path.join(source, d),\n            os.path.join(destination, d),\n            ignore=include_patterns(\"*.py\", \"*.so\", \"*.yaml\"),\n        )\n    return destination\n\n\ndef run_setup(args, config, dry_run):\n    # compute save_dir\n    save_dir_key = \".\".join(\n        filter(\n            lambda save_dir_key: save_dir_key is not None and len(save_dir_key) > 0,\n            [hp.get_save_dir_key() for hp in config.values()],\n        )\n    )\n    save_dir_key = save_dir_key.replace(\",\", \"_\")\n    num_total_gpus = args.num_nodes * args.num_gpus\n    save_dir = os.path.join(\n        args.checkpoints_dir, f\"{args.prefix}.{save_dir_key}.ngpu{num_total_gpus}\"\n    )\n\n    # create save directory if it doesn't exist\n    if not os.path.exists(save_dir):\n        if not dry_run(f\"create directory: {save_dir}\"):\n            os.makedirs(save_dir)\n\n    return save_dir_key, save_dir\n\n\ndef is_job_valid(args, save_dir, dry_run):\n    if has_finished(save_dir):\n        return False\n    elif has_failed(save_dir):\n        if args.resume_failed:\n            dry_run(f\"resume failed run: {save_dir}\")\n        else:\n            print(f\"skip failed run (override with --resume-failed): {save_dir}\")\n            return False\n    elif has_started(save_dir):\n        print(f\"skip in progress run: {save_dir}\")\n        return False\n    return True\n\n\nDEFAULT_NCCL_DEBUG = os.getenv(\"NCCL_DEBUG\", \"INFO\")\nDEFAULT_NCCL_DEBUG_LOCAL = os.getenv(\"NCCL_DEBUG\", \"\")\n\n\ndef set_env(args, env, dry_run):\n    if \"OMP_NUM_THREADS\" not in env:\n        env[\"OMP_NUM_THREADS\"] = \"2\"\n    env[\"NCCL_ASYNC_ERROR_HANDLING\"] = \"1\"\n    if args.local:\n        if not dry_run(\"start training locally\"):\n            if \"CUDA_VISIBLE_DEVICES\" not in env:\n                env[\"CUDA_VISIBLE_DEVICES\"] = \",\".join(map(str, range(args.num_gpus)))\n            env[\"NCCL_DEBUG\"] = DEFAULT_NCCL_DEBUG_LOCAL\n    else:\n        if args.num_nodes > 1:\n            env[\"NCCL_SOCKET_IFNAME\"] = \"^docker0,lo\"\n            env[\"NCCL_DEBUG\"] = DEFAULT_NCCL_DEBUG\n\n\ndef gen_train_command(\n    args, env, config, oss_destination, internal_destination, save_dir, save_dir_key\n):\n    # generate train command\n    train_cmd = [args.python, os.path.join(oss_destination, args.script)]\n    train_cmd.extend([\"--distributed-world-size\", str(args.num_nodes * args.num_gpus)])\n    if args.num_nodes > 1 or (args.num_gpus > 1 and not args.local):\n        train_cmd.extend(\n            [\n                \"--distributed-port\",\n                str(get_random_port()),\n            ]\n        )\n    if args.data is not None:\n        train_cmd.extend([args.data])\n    if args.local_checkpoints_dir is None:\n        train_cmd.extend([\"--save-dir\", save_dir])\n    else:\n        num_total_gpus = args.num_nodes * args.num_gpus\n        local_save_dir = os.path.join(\n            args.local_checkpoints_dir,\n            f\"{args.prefix}.{save_dir_key}.ngpu{num_total_gpus}\",\n        )\n        train_cmd.extend([\"--save-dir\", local_save_dir])\n    if getattr(args, \"full_azure_upload_path\", None) is not None:\n        if args.azure_folder_auto_name:\n            from urllib.parse import urlparse\n\n            o = urlparse(args.full_azure_upload_path)\n            o = o._replace(\n                path=os.path.join(\n                    o.path, f\"{args.prefix}.{save_dir_key}.ngpu{num_total_gpus}\"\n                )\n                + \"/\"\n            )\n            train_cmd.extend([\"--save-async\", \"--cloud-upload-path\", o.geturl()])\n        else:\n            train_cmd.extend(\n                [\"--save-async\", \"--cloud-upload-path\", args.full_azure_upload_path]\n            )\n\n    if not args.no_wandb:\n        try:\n            import wandb\n        except ImportError:\n            wandb = None\n        if wandb or (\"WANDB_API_KEY\" in env and \"WANDB_BASE_URL\" in env):\n            if \"--wandb-project\" not in config:\n                project = f\"{args.prefix}.{save_dir_key}\"\n                train_cmd.extend([\"--wandb-project\", project])\n            if \"WANDB_RUN_GROUP\" not in env:\n                env[\"WANDB_RUN_GROUP\"] = args.prefix\n            if \"WANDB_RUN_ID\" not in env:\n                env[\"WANDB_RUN_ID\"] = hashlib.md5(save_dir.encode(\"utf-8\")).hexdigest()\n            if \"WANDB_RESUME\" not in env:\n                env[\"WANDB_RESUME\"] = \"allow\"\n\n    if not args.no_tensorboard:\n        if args.tensorboard_logdir is None:\n            tensorboard_logdir = os.path.join(save_dir, \"tb\")\n        else:\n            tensorboard_logdir = os.path.join(\n                args.tensorboard_logdir,\n                f\"{args.prefix}.{save_dir_key}.ngpu{str(args.num_nodes * args.num_gpus)}\",\n            )\n        train_cmd.extend([\"--tensorboard-logdir\", tensorboard_logdir])\n    cluster_env = get_env_from_args(args)\n    train_cmd.extend([\"--cluster-env\", cluster_env.value])\n\n    for hp in config.values():\n        train_cmd.extend(map(str, hp.get_cli_args()))\n    return train_cmd\n\n\ndef gen_srun_command_and_str(args, save_dir_key, train_log, train_stderr, train_cmd):\n    base_srun_cmd = [\n        \"srun\",\n        \"--job-name\",\n        f\"{args.prefix}.{save_dir_key}\",\n        \"--output\",\n        train_log,\n        \"--error\",\n        train_stderr,\n        \"--open-mode\",\n        \"append\",\n        \"--unbuffered\",\n    ]\n    if args.cpu_bind:\n        base_srun_cmd += [f\"--cpu-bind={args.cpu_bind}\"]\n    if args.salloc:\n        excluded_hosts = os.environ.get(\"EXCLUDED_HOSTS\", None)\n        included_hosts = os.environ.get(\"INCLUDED_HOSTS\", None)\n        base_srun_cmd += [\n            \"--nodes\",\n            str(args.num_nodes),\n            \"--ntasks-per-node\",\n            str(args.num_gpus),\n            \"--ntasks\",\n            str(args.num_gpus * args.num_nodes),\n            \"--cpus-per-task\",\n            args.cpus_per_task,\n        ]\n        base_srun_cmd += [\"-x\", excluded_hosts] if excluded_hosts is not None else []\n        base_srun_cmd += [\"-w\", included_hosts] if included_hosts is not None else []\n\n    srun_cmd = base_srun_cmd + train_cmd\n    srun_cmd_str = \" \".join(map(shlex.quote, srun_cmd))\n    if getattr(args, \"requeue_on_fail\", False):\n        # sometimes we want the job to just be requeued magically if it exit codes\n        # i.e. in the case of very very large models with long runtimes.\n        srun_cmd_str = f\"( {srun_cmd_str} || scontrol requeue $SLURM_JOB_ID )\"\n    return srun_cmd, srun_cmd_str\n\n\ndef gen_sbatch_command_and_str(\n    args,\n    job_name,\n    train_log,\n    train_stderr,\n    oss_destination,\n    internal_destination,\n    srun_cmd_str,\n):\n    excluded_hosts = os.environ.get(\"EXCLUDED_HOSTS\", None)\n    included_hosts = os.environ.get(\"INCLUDED_HOSTS\", None)\n    sbatch_cmd = [\n        \"sbatch\",\n        \"--job-name\",\n        job_name,\n        \"--gpus-per-node\",\n        str(args.num_gpus),\n        \"--nodes\",\n        str(args.num_nodes),\n        \"--ntasks-per-node\",\n        str(args.num_gpus),\n        \"--cpus-per-task\",\n        args.cpus_per_task,\n        \"--output\",\n        train_log,\n        \"--error\",\n        train_stderr,\n        \"--open-mode\",\n        \"append\",\n        # '--no-requeue',\n        \"--signal\",\n        \"B:USR1@180\",\n    ]\n    if args.constraint:\n        sbatch_cmd += [\"--constraint\", args.constraint]\n\n    if args.partition:\n        sbatch_cmd += [\"--partition\", args.partition]\n    if args.reservation:\n        sbatch_cmd += [\"--reservation\", args.reservation]\n    if args.exclusive:\n        sbatch_cmd += [\"--exclusive\"]\n    comment = args.comment if args.comment else \"\"\n    if args.snapshot_code:\n        comment += f\"- OSS Code Location: {oss_destination} Internal Code Location: {internal_destination}\"\n    if len(comment) > 0:\n        sbatch_cmd += [\"--comment\", comment]\n    if args.time is not None:\n        sbatch_cmd.extend([\"--time\", args.time])\n    if args.mem is not None:\n        sbatch_cmd += [\"--mem\", args.mem]\n    else:\n        sbatch_cmd += [\"--mem\", \"0\"]\n\n    if args.rsc:\n        sbatch_cmd += [\"--qos\", \"high\"]\n    sbatch_cmd += [\"-x\", excluded_hosts] if excluded_hosts is not None else []\n    sbatch_cmd += [\"-w\", included_hosts] if included_hosts is not None else []\n\n    wrapped_cmd = requeue_support()\n    if args.azure:\n        wrapped_cmd += \"\\n\" + azure_support()\n    wrapped_cmd += \"\\n\" + srun_cmd_str + \" \\n wait $! \\n sleep 610 & \\n wait $!\"\n\n    sbatch_cmd += [\"--wrap\", wrapped_cmd]\n    sbatch_cmd_str = \" \".join(map(shlex.quote, sbatch_cmd))\n    return sbatch_cmd, sbatch_cmd_str\n\n\ndef local_run(args, env, train_cmd, dry_run):\n    assert args.num_nodes == 1, \"distributed training cannot be combined with --local\"\n    if not dry_run(\"start training locally\"):\n        if \"CUDA_VISIBLE_DEVICES\" not in env:\n            env[\"CUDA_VISIBLE_DEVICES\"] = \",\".join(map(str, range(args.num_gpus)))\n        env[\"NCCL_DEBUG\"] = DEFAULT_NCCL_DEBUG_LOCAL\n        train_proc = subprocess.Popen(train_cmd, env=env)\n        train_proc.wait()\n\n\ndef run_batch(env, sbatch_cmd_str, sbatch_cmd):\n    print(f\"running command: {sbatch_cmd_str}\\n\")\n    with subprocess.Popen(sbatch_cmd, stdout=subprocess.PIPE, env=env) as train_proc:\n        stdout = train_proc.stdout.read().decode(\"utf-8\")\n        try:\n            job_id = int(stdout.rstrip().split()[-1])\n            print(f\"Launched job {job_id}\")\n        except IndexError:\n            job_id = None\n    return job_id, stdout\n\n\ndef write_git_commit(train_log):\n    with open(train_log, \"a\") as train_log_h:\n        # log most recent git commit\n        git_commit = subprocess.check_output(\n            \"git log | head -n 1\", shell=True, encoding=\"utf-8\"\n        )\n        print(git_commit.rstrip(), file=train_log_h)\n\n\ndef dry_run_batch(env, train_log, train_stderr, sbatch_cmd_str, sbatch_cmd, dry_run):\n    dry_run(\"start remote training\")\n    dry_run(f\"- log stdout to: {train_log}\")\n    dry_run(f\"- log stderr to: {train_stderr}\")\n    dry_run(f\"- run command: {sbatch_cmd_str}\")\n    sbatch_cmd += [\"--test-only\"]\n    with subprocess.Popen(sbatch_cmd, stdout=subprocess.PIPE, env=env) as train_proc:\n        stdout = train_proc.stdout.read().decode(\"utf-8\")\n        print(stdout)\n\n\ndef launch_train(args, grid, grid_product, dry_run, postprocess_hyperparams):\n    oss_destination = str(Path(metaseq.__file__).parents[1])\n    internal_destination = (\n        str(Path(metaseq_internal.__file__).parents[1]) if has_internal else \"\"\n    )\n    if args.snapshot_code:\n        # Currently hash is just the current time in ISO format.\n        # Remove colons since they cannot be escaped in POSIX PATH env vars.\n        code_snapshot_hash = datetime.datetime.now().isoformat().replace(\":\", \"_\")\n        if has_internal:\n            internal_destination = copy_all_python_files(\n                internal_destination,\n                os.path.join(args.snapshot_root, \"slurm_snapshot_code_internal\"),\n                code_snapshot_hash,\n                args.snapshot_recurse_dirs_internal,\n            )\n        # Need to copy MetaSeq OSS code as well\n        oss_destination = copy_all_python_files(\n            oss_destination,\n            os.path.join(args.snapshot_root, \"slurm_snapshot_code_oss\"),\n            code_snapshot_hash,\n            args.snapshot_recurse_dirs_oss,\n        )\n        pythonpath_added = oss_destination + \":\"\n        if has_internal:\n            pythonpath_added += internal_destination + \":\"\n        os.environ[\"PYTHONPATH\"] = pythonpath_added + os.environ.get(\"PYTHONPATH\", \"\")\n\n    # set environment\n    base_env = os.environ.copy()\n    set_env(args, base_env, dry_run)\n\n    # start training\n    for i, hp_values in enumerate(grid_product):\n        if i == args.num_trials:\n            break\n        config = OrderedDict()\n        for hp, value in zip(grid, hp_values):\n            config[hp.name] = hp\n            config[hp.name].current_value = value\n\n        # postprocess hyperparams\n        postprocess_hyperparams(args, config)\n\n        save_dir_key, save_dir = run_setup(args, config, dry_run)\n\n        # check if job failed, exists, finished\n        if not is_job_valid(args, save_dir, dry_run):\n            continue\n\n        # symlink the snapshots over\n        if args.snapshot_code:\n            if has_internal:\n                abs_int = os.path.abspath(internal_destination)\n                subprocess.check_output(\n                    f\"ln -fs {abs_int} {save_dir}/snapshot_internal\", shell=True\n                )\n            abs_oss = os.path.abspath(oss_destination)\n            subprocess.check_output(\n                f\"ln -fs {abs_oss} {save_dir}/snapshot_public\", shell=True\n            )\n        # clone base env and update for this job, e.g., we set WANDB_RUN_ID\n        # based on the save_dir, which is based on the current hyperparam values\n        env = base_env.copy()\n        env[\"METASEQ_SAVE_DIR\"] = save_dir\n\n        # generate train command\n        train_cmd = gen_train_command(\n            args,\n            env,\n            config,\n            oss_destination,\n            internal_destination,\n            save_dir,\n            save_dir_key,\n        )\n\n        train_log = os.path.join(save_dir, \"train.log\")\n        train_stderr = os.path.join(save_dir, \"train.stderr.%j\")  # %j = slurm job id\n        srun_cmd, srun_cmd_str = gen_srun_command_and_str(\n            args, save_dir_key, train_log, train_stderr, train_cmd\n        )\n\n        job_id = None\n        if args.dry_run:\n            train_cmd_str = \" \".join(train_cmd)\n            dry_run(f\"train command: {train_cmd_str}\")\n        if args.local:\n            local_run(args, env, train_cmd, dry_run)\n        else:\n            srun_cmd_str = srun_cmd_str + \" &\"\n            # build command\n            if not args.salloc:\n                job_name = f\"{args.prefix}.{save_dir_key}\"\n                sbatch_cmd, sbatch_cmd_str = gen_sbatch_command_and_str(\n                    args,\n                    job_name,\n                    train_log,\n                    train_stderr,\n                    oss_destination,\n                    internal_destination,\n                    srun_cmd_str,\n                )\n            else:\n                sbatch_cmd = srun_cmd\n                sbatch_cmd_str = srun_cmd_str\n            if args.dry_run:\n                dry_run_batch(\n                    env, train_log, train_stderr, sbatch_cmd_str, sbatch_cmd, dry_run\n                )\n            else:\n                write_git_commit(train_log)\n                if args.rsc:\n                    internal.klist(train_log)\n                with open(train_log, \"a\") as train_log_h:\n                    job_id, stdout = run_batch(env, sbatch_cmd_str, sbatch_cmd)\n                    print(stdout, file=train_log_h)\n        if job_id is not None:\n            print(\"Launched {}\".format(job_id))\n        if hasattr(args, \"tombstonable\"):\n            if args.tombstonable:\n                tombstones(job_id=job_id, base_dir=args.base_directory)\n\n\ndef has_finished(save_dir):\n    train_log = os.path.join(save_dir, \"train.log\")\n    if not os.path.exists(train_log):\n        return False\n    with open(train_log, \"r\") as h:\n        lines = h.readlines()\n        if len(lines) == 0:\n            return False\n        if \"done training\" in lines[-1]:\n            return True\n    return False\n\n\ndef has_failed(save_dir):\n    if not os.path.exists(save_dir):\n        return False\n\n    # find max job id\n    job_ids = []\n    for fn in os.listdir(save_dir):\n        if fn.startswith(\"train.stderr.\"):\n            job_ids.append(int(fn.split(\".\")[-1]))\n    if len(job_ids) == 0:\n        return False\n    max_job_id = max(job_ids)\n\n    def _has_failed(stderr_fn):\n        with open(stderr_fn, \"r\") as h:\n            for line in h:\n                if len(line.strip()) > 0:\n                    # assume that any output in stderr indicates an error\n                    return True\n        return False\n\n    return _has_failed(os.path.join(save_dir, f\"train.stderr.{max_job_id}\"))\n\n\ndef has_started(save_dir):\n    train_log = os.path.join(save_dir, \"train.log\")\n    if not os.path.exists(train_log):\n        return False\n    return True\n\n\ndef requeue_support():\n    return textwrap.dedent(\n        \"\"\"\n        trap_handler () {\n           echo \"Caught signal: \" $1\n           # SIGTERM must be bypassed\n           if [ \"$1\" = \"TERM\" ]; then\n               echo \"bypass sigterm\"\n           else\n             # Submit a new job to the queue\n             echo \"Requeuing \" $SLURM_JOB_ID\n             scontrol requeue $SLURM_JOB_ID\n           fi\n        }\n\n\n        # Install signal handler\n        trap 'trap_handler USR1' USR1\n        trap 'trap_handler TERM' TERM\n    \"\"\"\n    )\n\n\ndef azure_support():\n    return textwrap.dedent(\n        \"\"\"\n        export NCCL_TOPO_FILE=/opt/microsoft/ndv4-topo.xml\n        export NCCL_IB_PCI_RELAXED_ORDERING=1\n        export UCX_IB_PCI_RELAXED_ORDERING=on\n        export NCCL_SOCKET_IFNAME=eth0\n        export UCX_NET_DEVICES=eth0\n        export CUDA_DEVICE_ORDER=PCI_BUS_ID\n        export OMPI_MCA_COLL_HCOLL_ENABLE=0\n        if [ -e \"/etc/profile.d/modules.sh\" ]; then\n            . /etc/profile.d/modules.sh\n            module load mpi/hpcx\n        fi\n        \"\"\"\n    )\n",
        "metaseq/launcher/sweep.py": "# Copyright (c) Meta Platforms, Inc. and affiliates. All Rights Reserved.\n#\n# This source code is licensed under the MIT license found in the\n# LICENSE file in the root directory of this source tree.\n\nimport argparse\nimport datetime\nimport os\nimport subprocess\nfrom typing import Optional, List, Callable, MutableMapping\nfrom urllib.parse import urlparse\n\ntry:\n    # internal logic denoting where data locations are\n    from metaseq_internal.constants import (\n        ComputeEnvs,\n        DEFAULT_PARTITION,\n        DEFAULT_PREFIX,\n        DEFAULT_CPU_PER_TASK,\n    )\nexcept ImportError:\n    from metaseq.launcher.opt_job_constants import (\n        ComputeEnvs,\n        DEFAULT_PARTITION,\n        DEFAULT_PREFIX,\n        DEFAULT_CPU_PER_TASK,\n    )\n\n\nclass hyperparam(object):\n    \"\"\"Base class for defining hyperparameters.\"\"\"\n\n    def __init__(\n        self,\n        name,\n        values=None,\n        binary_flag=False,\n        save_dir_key=None,\n        positional_arg=False,\n    ):\n        \"\"\"\n        Arguments:\n        - name : the name of the hyperparameter (e.g., `--dropout`)\n        - values : the set of values to sweep over (e.g., `[0.0, 0.1, 0.2]`)\n        - binary_flag : whether the hyperparameter uses a boolean flag (e.g., `--no-tensorboard`)\n        - save_dir_key : function that takes the hyperparameter value and returns the \"key\"\n                         to be appended to the output directory name\n        - positional_arg : whether the hyperparameter is a positional argument\n        \"\"\"\n        self.name = name\n        if values is None:  # syntactic sugar for binary flags\n            self.values = [True]\n            self.binary_flag = True\n        else:\n            self.values = values if isinstance(values, list) else [values]\n            self.binary_flag = binary_flag\n        self.save_dir_key = save_dir_key\n        self.positional_arg = positional_arg\n        self.current_value = None\n\n        if positional_arg and name.startswith(\"-\"):\n            raise ValueError(\n                f\"positional arguments must not start with a dash ({name})\"\n            )\n\n        if len(self.values) > 1 and self.save_dir_key is None:\n            raise ValueError(\n                f\"{name} has more than one value but is missing a save_dir_key!\"\n            )\n\n    def get_cli_args(self):\n        if self.binary_flag:\n            return [self.name] if self.current_value else []\n        elif self.positional_arg:\n            return [self.current_value]\n        else:\n            return [self.name, self.current_value]\n\n    def get_save_dir_key(self):\n        if self.save_dir_key is None:\n            return None\n        if self.binary_flag:\n            return self.save_dir_key(1) if self.current_value else None\n        return self.save_dir_key(self.current_value)\n\n\ndef get_env_from_args(args):\n    if args.azure:\n        return ComputeEnvs.AZURE\n    elif args.aws:\n        return ComputeEnvs.AWS\n    elif args.fair:\n        return ComputeEnvs.FAIR\n    elif args.rsc:\n        return ComputeEnvs.RSC\n    else:\n        raise NotImplementedError(\n            \"Env not passed in! Please pass in one of: --azure, --aws, --fair, --rsc\"\n        )\n\n\ndef _get_args(add_extra_options_func=None, input_args: Optional[List[str]] = None):\n    \"\"\"\n    input_args (List[str]): strings to parse, defaults to sys.argv\n    \"\"\"\n    parser = argparse.ArgumentParser(\"Script for launching hyperparameter sweeps \")\n    parser.add_argument(\"--grid\", help=\"grid function we used\", default=None)\n\n    parser.add_argument(\"-d\", \"--data\", help=\"path to data directory\")\n    parser.add_argument(\n        \"-p\",\n        \"--prefix\",\n        required=True,\n        help=\"save checkpoints and logs in <checkpoints-dir>/<prefix>.<save_dir_key>\",\n    )\n    parser.add_argument(\n        \"-t\",\n        \"--num-trials\",\n        default=-1,\n        type=int,\n        help=\"number of random hyperparam configurations to try (-1 for grid search)\",\n    )\n    parser.add_argument(\n        \"-g\", \"--num-gpus\", type=int, required=True, help=\"number of GPUs per node\"\n    )\n    parser.add_argument(\n        \"-n\",\n        \"--num-nodes\",\n        type=int,\n        default=1,\n        help=\"number of nodes for distributed training\",\n    )\n    parser.add_argument(\n        \"--update-freq\",\n        type=int,\n        default=0,\n        help=\"update freq\",\n    )\n    parser.add_argument(\"--seed\", type=int, default=1234)\n    parser.add_argument(\n        \"--resume-failed\",\n        action=\"store_true\",\n        help=\"resume any runs that failed\",\n    )\n    parser.add_argument(\n        \"--dry-run\",\n        action=\"store_true\",\n        help=\"output only a list of actions to perform without performing them\",\n    )\n    parser.add_argument(\"--local\", action=\"store_true\", help=\"run job locally\")\n    parser.add_argument(\"--debug\", action=\"store_true\", help=\"debug\")\n    parser.add_argument(\n        \"--script\", default=\"metaseq/cli/train.py\", help=\"script to launch\"\n    )\n    parser.add_argument(\n        \"--python\", default=\"python\", help=\"path to nonstandard python binary\"\n    )\n\n    # Slurm params\n    parser.add_argument(\n        \"--salloc\", action=\"store_true\", help=\"run agaist current allocation\"\n    )\n    parser.add_argument(\"--reservation\", help=\"reservation to run on\")\n    parser.add_argument(\n        \"--exclusive\", action=\"store_true\", help=\"if set, get exclusive host\"\n    )\n    parser.add_argument(\n        \"--time\", default=\"4320\", help=\"expected job duration in minutes\"\n    )\n    parser.add_argument(\"--mem\", \"--mem\", help=\"memory to request\")\n    parser.add_argument(\n        \"--constraint\",\n        metavar=\"CONSTRAINT\",\n        help='gpu constraint, if any. e.g. \"volta\"',\n    )\n    parser.add_argument(\"--comment\", help=\"comment string\")\n    parser.add_argument(\n        \"--snapshot-code\",\n        action=\"store_true\",\n        default=False,\n        help=\"Flag for creating a snapshot of training code while creating slurm job,\"\n        ' path is \"./slurm_snapshot_code/<TIME_ISO_FORMAT/>:\", '\n        \"can find time from comment of slurm job.\",\n    )\n    parser.add_argument(\n        \"--snapshot-root\",\n        type=str,\n        default=\".\",\n        help=\"root path for saving the snapshot code.\",\n    )\n    parser.add_argument(\n        \"--snapshot-recurse-dirs-internal\",\n        default=\"metaseq_internal\",\n        help=\"comma-separated directories from where to recursively copy *.py, *.so and *.yaml files\",\n    )\n    parser.add_argument(\n        \"--snapshot-recurse-dirs-oss\",\n        default=\"metaseq\",\n        help=\"comma-separated directories from where to recursively copy *.py, *.so and *.yaml files\",\n    )\n    parser.add_argument(\n        \"--no-tensorboard\", action=\"store_true\", help=\"disable tensorboard logging\"\n    )\n    parser.add_argument(\"--no-wandb\", action=\"store_true\", help=\"disable WandB logging\")\n    parser.add_argument(\n        \"--post-steps\",\n        nargs=\"+\",\n        help=\"additional steps to execute after the primary job is complete. \"\n        \"this can be a file with the steps, or a string. some placeholders such as \"\n        \"{job_dir} will be replaced\",\n    )\n\n    # Env flags\n    parser.add_argument(\"--azure\", action=\"store_true\", help=\"running on azure\")\n    parser.add_argument(\"--aws\", action=\"store_true\", help=\"running on aws\")\n    parser.add_argument(\"--fair\", action=\"store_true\", help=\"running on fair\")\n    parser.add_argument(\"--rsc\", action=\"store_true\", help=\"running on rsc\")\n\n    # Azure specific flag\n    parser.add_argument(\n        \"--full-azure-upload-path\",\n        default=None,\n        help=\"Azure blob storage SAS URL\",\n    )\n\n    parser.add_argument(\n        \"--azure-folder-auto-name\",\n        action=\"store_true\",\n        help=\"Automatically name azure folder\",\n    )\n\n    # Following args have env specific defaults.\n    parser.add_argument(\n        \"--partition\",\n        help=\"slurm partition to run on\",\n    )\n    parser.add_argument(\n        \"--checkpoints-dir\",\n        help=\"save checkpoints and logs in <checkpoints-dir>/<prefix>.<save_dir_key>\",\n    )\n    parser.add_argument(\"--cpus-per-task\", type=str)\n    parser.add_argument(\n        \"--cpu-bind\", help=\"configured to improve all-to-all perf, especially on A100s\"\n    )\n    parser.add_argument(\n        \"--local-checkpoints-dir\",\n        help=\"node-local directory for saving checkpoints\",\n    )\n    parser.add_argument(\n        \"--tensorboard-logdir\",\n        default=None,  # None will default to save_dir/tb\n        help=\"save tensorboard logs in <tensorboard-logdir>/<prefix>.<save_dir_key>\",\n    )\n    parser.add_argument(\n        \"-ts\",\n        \"--tombstonable\",\n        type=bool,\n        default=False,\n        help=(\n            \"make the job killable by writing a \"\n            \"tombstone 'tombstone_<job_id>' file to user's home directory \"\n            \"(/shared/home/$USER)\"\n        ),\n    )\n\n    if add_extra_options_func is not None:  # mutates parser\n        add_extra_options_func(parser)\n    args = parser.parse_args(input_args)\n\n    # Env check\n    assert (\n        sum([args.azure, args.aws, args.fair, args.rsc]) == 1\n    ), \"Must pass an env, and only one env (--azure, --aws, --fair, or --rsc)!\"\n\n    # Set defaults based on env\n    env = get_env_from_args(args)\n    _modify_arg_defaults_based_on_env(env, args)\n    return args\n\n\ndef _modify_arg_defaults_based_on_env(env, args):\n    # TODO(susan): move all this default logic into separate config file\n    default_partition = DEFAULT_PARTITION[env]\n    default_prefix = DEFAULT_PREFIX[env]\n\n    if env == ComputeEnvs.FAIR or env == ComputeEnvs.RSC:\n        default_checkpoint_dir = os.path.join(\n            default_prefix, os.environ[\"USER\"], str(datetime.date.today())\n        )\n    else:\n        default_checkpoint_dir = os.path.join(\n            default_prefix,\n            os.environ[\"USER\"],\n            \"checkpoints\",\n            str(datetime.date.today()),\n        )\n\n    default_cpu_per_task = DEFAULT_CPU_PER_TASK[env]\n\n    default_cpu_bind = \"none\"\n    if env == ComputeEnvs.AZURE:\n        default_cpu_bind = (\n            \"mask_cpu:ffffff000000,ffffff000000,ffffff,ffffff,\"\n            \"ffffff000000000000000000,ffffff000000000000000000,\"\n            \"ffffff000000000000,ffffff000000000000\"\n        )\n    elif env == ComputeEnvs.AWS:\n        default_cpu_bind = (\n            \"mask_cpu:000000ffffff000000ffffff,000000ffffff000000ffffff,\"\n            \"000000ffffff000000ffffff,000000ffffff000000ffffff,\"\n            \"ffffff000000ffffff000000,ffffff000000ffffff000000,\"\n            \"ffffff000000ffffff000000,ffffff000000ffffff000000\"\n        )\n    elif env == ComputeEnvs.FAIR:\n        default_cpu_bind = \"map_ldom:0,0,0,0,1,1,1,1\"\n\n    default_local_checkpoints_dir = None\n    if env == ComputeEnvs.AZURE:\n        azure_upload_path = os.environ.get(\"AZURE_BLOB_SAS_URL\", \"\")\n        if azure_upload_path != \"\":\n            # write checkpoints to local scratch storage on each node\n            default_local_checkpoints_dir = os.path.join(\n                \"/mnt/resource_nvme\",\n                os.environ[\"USER\"],\n                \"checkpoints\",\n                str(datetime.date.today()),\n            )\n\n            # then copy them to Azure blob storage\n            o = urlparse(azure_upload_path)\n            o = o._replace(\n                path=os.path.join(\n                    o.path, os.environ[\"USER\"], str(datetime.date.today())\n                )\n            )\n            azure_upload_path = o.geturl()\n\n            # set upload path if not specified\n            if args.full_azure_upload_path is None:\n                args.full_azure_upload_path = azure_upload_path\n\n            # if needed, create a container for this user on the Azure blob account\n            cmd = [\n                \"azcopy\",  # TODO(susanz): requires azcopy to be installed.\n                \"make\",\n                o._replace(path=os.path.dirname(o.path)).geturl(),\n            ]\n            subprocess.run(cmd, stdout=subprocess.PIPE, stderr=subprocess.PIPE)\n\n    # assign default slurm partition\n    if args.partition is None:\n        args.partition = default_partition\n\n    # assign default checkpoint directory\n    if args.checkpoints_dir is None:\n        args.checkpoints_dir = default_checkpoint_dir\n\n    # assign default # cpus per task\n    if args.cpus_per_task is None:\n        args.cpus_per_task = str(default_cpu_per_task)\n\n    # assign default cpu bind\n    if args.cpu_bind is None:\n        args.cpu_bind = default_cpu_bind\n\n    # assign default local checkpoint dir\n    if args.local_checkpoints_dir is None:\n        args.local_checkpoints_dir = default_local_checkpoints_dir\n\n    # assign base directory\n    args.base_directory = default_prefix\n\n\ndef main(\n    get_grid: Callable[[argparse.Namespace], List[hyperparam]],\n    postprocess_hyperparams: Callable[\n        [argparse.Namespace, MutableMapping[str, hyperparam]], None\n    ],\n    add_extra_options_func: Optional[Callable[[argparse.ArgumentParser], None]] = None,\n    scheduler_args: Optional[List[str]] = None,\n) -> None:\n    \"\"\"Do a grid search.\n\n    Parameters:\n        get_grid: A unary callable which returns the grid to search over.\n            The callable is passed the parsed sweep arguments including the extra\n            arguments defined by `add_extra_options_func`. See also `get_args`.\n            The returned list represents the dimensions of the grid. That is, a list of\n            length n represents a grid of dimension n. Let v_i denote the number of\n            possible values for dimension i. Then the total number of configurations\n            is given by v_1 * ... * v_n.\n        postprocess_hyperparams: A 2-ary callable to post-process hyperparameter\n            configurations before running the job. The first argument is the parsed\n            sweep arguments including the extra arguments defined by\n            `add_extra_options_func`. The second argument is a realized hyperparameter\n            configuration as a mutable mapping of hyperparameter name to `hyperparam`\n            instance with a `current_value` set.\n        add_extra_options_func: A unary callable which adds extra arguments to the\n            sweep CLI. It is passed the parser used to define the sweep script's CLI.\n        scheduler_args: A list of unprocessed arguments to parse. If None, then\n            `sys.argv[1:]`.\n    \"\"\"\n    args = _get_args(add_extra_options_func, scheduler_args)\n    from .slurm import main as backend_main\n\n    get_grid = get_grid[args.grid] if args.grid is not None else get_grid\n    backend_main(get_grid, postprocess_hyperparams, args)\n",
        "metaseq/launcher/tombyard.py": "# Copyright (c) Meta Platforms, Inc. and affiliates. All Rights Reserved.\n#\n# This source code is licensed under the MIT license found in the\n# LICENSE file in the root directory of this source tree.\n\nimport multiprocessing as mp\nimport datetime\nimport os\nimport time\n\nJOB_STATE_CODES = [\n    \"BOOT_FAIL\",\n    \"CANCELLED\",\n    \"COMPLETED\",\n    \"CONFIGURING\",\n    \"COMPLETING\",\n    \"DEADLINE\",\n    \"FAILED\",\n    \"NODE_FAIL\",\n    \"OUT_OF_MEMORY\",\n    \"PENDING\",\n    \"PREEMPTED\",\n    \"RUNNING\",\n    \"RESV_DEL_HOLD\",\n    \"REQUEUE_FED\",\n    \"REQUEUE_HOLD\",\n    \"REQUEUED\",\n    \"RESIZING\",\n    \"REVOKED\",\n    \"SIGNALING\",\n    \"SPECIAL_EXIT\",\n    \"STAGE_OUT\",\n    \"STOPPED\",\n    \"SUSPENDED\",\n    \"TIMEOUT\",\n]\n\n\ndef tombstones_procedure(\n    job_id,\n    dirstones,\n    period_before_tombstone_detected=datetime.timedelta(seconds=60),\n    period_after_tombstone_detected=datetime.timedelta(seconds=3),\n):\n    tombstone_detected = False\n    period = period_before_tombstone_detected\n\n    while True:\n        sacct_result = os.popen(f\"squeue -j {job_id} -O State -h \").read()\n        status = sacct_result.strip()\n        if tombstone_detected:\n            print(f\".. scanceling the job and its current squeue.state is {status}\")\n        if status not in JOB_STATE_CODES:\n            print(f\"Done scanceling the job. Its squeue.state now is: {status}\")\n            return\n        if not tombstone_detected:\n            for tombstone_name in dirstones[\"scancel\"]:\n                if os.path.exists(tombstone_name):\n                    print(\n                        f\"tombstones_procedure has detected file {tombstone_name}. \"\n                        f\"scancel {job_id} will be called every {period_after_tombstone_detected} \"\n                        f\"until the job is dead \"\n                    )\n                    tombstone_detected = True\n                    period = period_after_tombstone_detected\n            for tombstone_name in dirstones[\"requeuehold\"]:\n                if os.path.exists(tombstone_name):\n                    print(\n                        f\"tombstones_procedure has detected file {tombstone_name}. \"\n                        f\"scontrol requeuehold {job_id} will be called once. \"\n                        f\"remove the file {tombstone_name} within the next {period_before_tombstone_detected} \"\n                        f\"for it not to trigger the same command again \"\n                    )\n                    _ = os.popen(f\"scontrol requeuehold {job_id}\").read()\n            for tombstone_name in dirstones[\"requeuehold\"]:\n                if os.path.exists(tombstone_name):\n                    print(\n                        f\"tombstones_procedure has detected file {tombstone_name}. \"\n                        f\"scontrol release {job_id} will be called once. \"\n                        f\"remove the file {tombstone_name} within the next {period_before_tombstone_detected} \"\n                        f\"for it not to trigger the same command again \"\n                    )\n                    _ = os.popen(f\"scontrol release {job_id} \").read()\n        if tombstone_detected:\n            _ = os.popen(f\"scancel {job_id} \").read()\n        time.sleep(period.total_seconds())\n\n\ndef tombstones(job_id, base_dir, period=datetime.timedelta(seconds=60), dirstones=None):\n    if dirstones is None:\n        dirstones = {\"scancel\": [], \"requeuehold\": [], \"release\": []}\n        for userdir in os.listdir(base_dir):\n            dirstones[\"scancel\"].append(\n                os.path.join(base_dir, userdir, f\"scancel_{job_id}\")\n            )\n            dirstones[\"requeuehold\"].append(\n                os.path.join(base_dir, userdir, f\"requeuehold_{job_id}\")\n            )\n            dirstones[\"release\"].append(\n                os.path.join(base_dir, userdir, f\"release_{job_id}\")\n            )\n\n    for directive in [\"scancel\", \"requeuehold\", \"release\"]:\n        if directive not in dirstones.keys():\n            dirstones[directive] = []\n\n    # start a process that monitors\n    ctx = mp.get_context(\"spawn\")\n    heartbeat_proc = ctx.Process(\n        target=tombstones_procedure, args=(job_id, dirstones, period), daemon=False\n    )\n    heartbeat_proc.start()\n",
        "metaseq/logging/__init__.py": "",
        "metaseq/logging/meters.py": "# Copyright (c) Meta Platforms, Inc. and affiliates. All Rights Reserved.\n#\n# This source code is licensed under the MIT license found in the\n# LICENSE file in the root directory of this source tree.\n\nimport bisect\nimport time\nfrom collections import OrderedDict\nfrom typing import Dict, Optional\n\n\ntry:\n    import torch\n\n    def type_as(a, b):\n        if torch.is_tensor(a) and torch.is_tensor(b):\n            return a.to(b)\n        else:\n            return a\n\nexcept ImportError:\n    torch = None\n\n    def type_as(a, b):\n        return a\n\n\ntry:\n    import numpy as np\nexcept ImportError:\n    np = None\n\n\nclass Meter(object):\n    \"\"\"Base class for Meters.\"\"\"\n\n    def __init__(self):\n        pass\n\n    def state_dict(self):\n        return {}\n\n    def load_state_dict(self, state_dict):\n        pass\n\n    def reset(self):\n        raise NotImplementedError\n\n    @property\n    def smoothed_value(self) -> float:\n        \"\"\"Smoothed value used for logging.\"\"\"\n        raise NotImplementedError\n\n\ndef safe_round(number, ndigits):\n    if hasattr(number, \"__round__\"):\n        return round(number, ndigits)\n    elif torch is not None and torch.is_tensor(number) and number.numel() == 1:\n        return safe_round(number.item(), ndigits)\n    elif np is not None and np.ndim(number) == 0 and hasattr(number, \"item\"):\n        return safe_round(number.item(), ndigits)\n    else:\n        return number\n\n\nclass AverageMeter(Meter):\n    \"\"\"Computes and stores the average and current value\"\"\"\n\n    def __init__(self, round: Optional[int] = None):\n        self.round = round\n        self.reset()\n\n    def reset(self):\n        self.val = None  # most recent update\n        self.sum = 0  # sum from all updates\n        self.count = 0  # total n from all updates\n\n    def update(self, val, n=1):\n        if val is not None:\n            self.val = val\n            if n > 0:\n                self.sum = type_as(self.sum, val) + (val * n)\n                self.count = type_as(self.count, n) + n\n\n    def state_dict(self):\n        return {\n            \"val\": self.val,\n            \"sum\": self.sum,\n            \"count\": self.count,\n            \"round\": self.round,\n        }\n\n    def load_state_dict(self, state_dict):\n        self.val = state_dict[\"val\"]\n        self.sum = state_dict[\"sum\"]\n        self.count = state_dict[\"count\"]\n        self.round = state_dict.get(\"round\", None)\n\n    @property\n    def avg(self):\n        return self.sum / self.count if self.count > 0 else self.val\n\n    @property\n    def smoothed_value(self) -> float:\n        val = self.avg\n        if self.round is not None and val is not None:\n            val = safe_round(val, self.round)\n        return val\n\n\nclass TimeMeter(Meter):\n    \"\"\"Computes the average occurrence of some event per second\"\"\"\n\n    def __init__(\n        self,\n        init: int = 0,\n        n: int = 0,\n        round: Optional[int] = None,\n    ):\n        self.round = round\n        self.reset(init, n)\n\n    def reset(self, init=0, n=0):\n        self.init = init\n        self.start = time.perf_counter()\n        self.n = n\n        self.i = 0\n\n    def update(self, val=1):\n        self.n = type_as(self.n, val) + val\n        self.i += 1\n\n    def state_dict(self):\n        return {\n            \"init\": self.elapsed_time,\n            \"n\": self.n,\n            \"round\": self.round,\n        }\n\n    def load_state_dict(self, state_dict):\n        if \"start\" in state_dict:\n            # backwards compatibility for old state_dicts\n            self.reset(init=state_dict[\"init\"])\n        else:\n            self.reset(init=state_dict[\"init\"], n=state_dict[\"n\"])\n            self.round = state_dict.get(\"round\", None)\n\n    @property\n    def avg(self):\n        return self.n / self.elapsed_time\n\n    @property\n    def elapsed_time(self):\n        return self.init + (time.perf_counter() - self.start)\n\n    @property\n    def smoothed_value(self) -> float:\n        val = self.avg\n        if self.round is not None and val is not None:\n            val = safe_round(val, self.round)\n        return val\n\n\nclass StopwatchMeter(Meter):\n    \"\"\"Computes the sum/avg duration of some event in seconds\"\"\"\n\n    def __init__(self, round: Optional[int] = None):\n        self.round = round\n        self.sum = 0\n        self.n = 0\n        self.start_time = None\n\n    def start(self):\n        self.start_time = time.perf_counter()\n\n    def stop(self, n=1, prehook=None):\n        if self.start_time is not None:\n            if prehook is not None:\n                prehook()\n            delta = time.perf_counter() - self.start_time\n            self.sum = self.sum + delta\n            self.n = type_as(self.n, n) + n\n\n    def reset(self):\n        self.sum = 0  # cumulative time during which stopwatch was active\n        self.n = 0  # total n across all start/stop\n        self.start()\n\n    def state_dict(self):\n        return {\n            \"sum\": self.sum,\n            \"n\": self.n,\n            \"round\": self.round,\n        }\n\n    def load_state_dict(self, state_dict):\n        self.sum = state_dict[\"sum\"]\n        self.n = state_dict[\"n\"]\n        self.start_time = None\n        self.round = state_dict.get(\"round\", None)\n\n    @property\n    def avg(self):\n        return self.sum / self.n if self.n > 0 else self.sum\n\n    @property\n    def elapsed_time(self):\n        if self.start_time is None:\n            return 0.0\n        return time.perf_counter() - self.start_time\n\n    @property\n    def smoothed_value(self) -> float:\n        val = self.avg if self.sum > 0 else self.elapsed_time\n        if self.round is not None and val is not None:\n            val = safe_round(val, self.round)\n        return val\n\n\nclass MetersDict(OrderedDict):\n    \"\"\"A sorted dictionary of :class:`Meters`.\n\n    Meters are sorted according to a priority that is given when the\n    meter is first added to the dictionary.\n    \"\"\"\n\n    def __init__(self, *args, **kwargs):\n        super().__init__(*args, **kwargs)\n        self.priorities = []\n\n    def __setitem__(self, key, value):\n        assert key not in self, \"MetersDict doesn't support reassignment\"\n        priority, value = value\n        bisect.insort(self.priorities, (priority, len(self.priorities), key))\n        super().__setitem__(key, value)\n        for _, _, key in self.priorities:  # reorder dict to match priorities\n            self.move_to_end(key)\n\n    def add_meter(self, key, meter, priority):\n        self.__setitem__(key, (priority, meter))\n\n    def state_dict(self):\n        return [\n            (pri, key, self[key].__class__.__name__, self[key].state_dict())\n            for pri, _, key in self.priorities\n            # can't serialize DerivedMeter instances\n            if not isinstance(self[key], MetersDict._DerivedMeter)\n        ]\n\n    def load_state_dict(self, state_dict):\n        self.clear()\n        self.priorities.clear()\n        for pri, key, meter_cls, meter_state in state_dict:\n            meter = globals()[meter_cls]()\n            meter.load_state_dict(meter_state)\n            self.add_meter(key, meter, pri)\n\n    def get_smoothed_value(self, key: str) -> float:\n        \"\"\"Get a single smoothed value.\"\"\"\n        meter = self[key]\n        if isinstance(meter, MetersDict._DerivedMeter):\n            return meter.fn(self)\n        else:\n            return meter.smoothed_value\n\n    def get_smoothed_values(self) -> Dict[str, float]:\n        \"\"\"Get all smoothed values.\"\"\"\n        return OrderedDict(\n            [\n                (key, self.get_smoothed_value(key))\n                for key in self.keys()\n                if not key.startswith(\"_\")\n            ]\n        )\n\n    def reset(self):\n        \"\"\"Reset Meter instances.\"\"\"\n        for meter in self.values():\n            if isinstance(meter, MetersDict._DerivedMeter):\n                continue\n            meter.reset()\n\n    class _DerivedMeter(Meter):\n        \"\"\"A Meter whose values are derived from other Meters.\"\"\"\n\n        def __init__(self, fn):\n            self.fn = fn\n\n        def reset(self):\n            pass\n",
        "metaseq/logging/metrics.py": "# Copyright (c) Meta Platforms, Inc. and affiliates. All Rights Reserved.\n#\n# This source code is licensed under the MIT license found in the\n# LICENSE file in the root directory of this source tree.\n\"\"\"\nA standalone module for aggregating metrics.\n\nMetrics can be logged from anywhere using the `log_*` functions defined\nin this module. The logged values will be aggregated dynamically based\non the aggregation context in which the logging occurs. See the\n:func:`aggregate` context manager for more details.\n\"\"\"\n\nimport contextlib\nimport logging\nimport subprocess\nimport uuid\nfrom collections import defaultdict\nfrom typing import Callable, List, Optional, Dict\n\nfrom .meters import (\n    OrderedDict,\n    MetersDict,\n    AverageMeter,\n    TimeMeter,\n    StopwatchMeter,\n    Meter,\n)\n\n# Aggregation contexts are considered \"active\" when inside the scope\n# created by the :func:`aggregate` context manager.\n_aggregators = OrderedDict()\n_active_aggregators = OrderedDict()\n_active_aggregators_cnt = defaultdict(lambda: 0)\n\n\ndef reset() -> None:\n    \"\"\"Reset all metrics aggregators.\"\"\"\n    _aggregators.clear()\n    _active_aggregators.clear()\n    _active_aggregators_cnt.clear()\n\n    # The \"default\" aggregator observes all logged values.\n    _aggregators[\"default\"] = MetersDict()\n    _active_aggregators[\"default\"] = _aggregators[\"default\"]\n    _active_aggregators_cnt[\"default\"] = 1\n\n\nreset()\n\n\n@contextlib.contextmanager\ndef aggregate(name: Optional[str] = None, new_root: bool = False):\n    \"\"\"Context manager to aggregate metrics under a given name.\n\n    Aggregations can be nested. If *new_root* is ``False``, then logged\n    metrics will be recorded along the entire stack of nested\n    aggregators, including a global \"default\" aggregator. If *new_root*\n    is ``True``, then this aggregator will be the root of a new\n    aggregation stack, thus bypassing any parent aggregators.\n\n    Note that aggregation contexts are uniquely identified by their\n    *name* (e.g., train, valid). Creating a context with an existing\n    name will reuse the corresponding :class:`MetersDict` instance.\n    If no name is given, then a temporary aggregator will be created.\n\n    Usage::\n\n        with metrics.aggregate(\"train\"):\n            for step, batch in enumerate(epoch):\n                with metrics.aggregate(\"train_inner\") as agg:\n                    metrics.log_scalar(\"loss\", get_loss(batch))\n                    if step % log_interval == 0:\n                        print(agg.get_smoothed_value(\"loss\"))\n                        agg.reset()\n        print(metrics.get_smoothed_values(\"train\")[\"loss\"])\n\n    Args:\n        name (str): name of the aggregation. Defaults to a\n            random/temporary name if not given explicitly.\n        new_root (bool): make this aggregation the root of a new\n            aggregation stack.\n    \"\"\"\n    if name is None:\n        # generate a temporary name\n        name = str(uuid.uuid4())\n        assert name not in _aggregators\n        agg = MetersDict()\n    else:\n        assert name != \"default\"\n        agg = _aggregators.setdefault(name, MetersDict())\n\n    if new_root:\n        backup_aggregators = _active_aggregators.copy()\n        _active_aggregators.clear()\n        backup_aggregators_cnt = _active_aggregators_cnt.copy()\n        _active_aggregators_cnt.clear()\n\n    _active_aggregators[name] = agg\n    _active_aggregators_cnt[name] += 1\n\n    yield agg\n\n    _active_aggregators_cnt[name] -= 1\n    if _active_aggregators_cnt[name] == 0 and name in _active_aggregators:\n        del _active_aggregators[name]\n\n    if new_root:\n        _active_aggregators.clear()\n        _active_aggregators.update(backup_aggregators)\n        _active_aggregators_cnt.clear()\n        _active_aggregators_cnt.update(backup_aggregators_cnt)\n\n\ndef get_active_aggregators() -> List[MetersDict]:\n    return list(_active_aggregators.values())\n\n\ndef log_scalar(\n    key: str,\n    value: float,\n    weight: float = 1,\n    priority: int = 10,\n    round: Optional[int] = None,\n):\n    \"\"\"Log a scalar value.\n\n    Args:\n        key (str): name of the field to log\n        value (float): value to log\n        weight (float): weight that this value contributes to the average.\n            A weight of 0 will always log the latest value.\n        priority (int): smaller values are logged earlier in the output\n        round (Optional[int]): number of digits to round to when displaying\n    \"\"\"\n    for agg in get_active_aggregators():\n        if key not in agg:\n            agg.add_meter(key, AverageMeter(round=round), priority)\n        agg[key].update(value, weight)\n\n\ndef log_derived(key: str, fn: Callable[[MetersDict], float], priority: int = 20):\n    \"\"\"Log a scalar value derived from other meters.\n\n    Args:\n        key (str): name of the field to log\n        fn (Callable[[MetersDict], float]): function that takes a single\n            argument *meters* and returns the derived value\n        priority (int): smaller values are logged earlier in the output\n    \"\"\"\n    for agg in get_active_aggregators():\n        if key not in agg:\n            agg.add_meter(key, MetersDict._DerivedMeter(fn), priority)\n\n\ndef log_speed(\n    key: str,\n    value: float,\n    priority: int = 30,\n    round: Optional[int] = None,\n):\n    \"\"\"Log the rate of some quantity per second.\n\n    Args:\n        key (str): name of the field to log\n        value (float): value to log\n        priority (int): smaller values are logged earlier in the output\n        round (Optional[int]): number of digits to round to when displaying\n    \"\"\"\n    for agg in get_active_aggregators():\n        if key not in agg:\n            agg.add_meter(key, TimeMeter(round=round), priority)\n            agg[key].reset()  # reset meter on the first call\n        else:\n            agg[key].update(value)\n\n\ndef log_start_time(key: str, priority: int = 40, round: Optional[int] = None):\n    \"\"\"Log the duration of some event in seconds.\n\n    The duration will be computed once :func:`log_stop_time` is called.\n\n    Args:\n        key (str): name of the field to log\n        priority (int): smaller values are logged earlier in the output\n        round (Optional[int]): number of digits to round to when displaying\n    \"\"\"\n    for agg in get_active_aggregators():\n        if key not in agg:\n            agg.add_meter(key, StopwatchMeter(round=round), priority)\n        agg[key].start()\n\n\ndef log_stop_time(key: str, weight: float = 0.0, prehook=None):\n    \"\"\"Log the duration of some event in seconds.\n\n    The duration will be computed since :func:`log_start_time` was called.\n    Set weight > 0 to report the average time instead of the sum.\n\n    Args:\n        key (str): name of the field to log\n        weight (float): weight that this time contributes to the average\n        prehook (function, no arguments): will be called before the timer\n        is stopped. For example, use prehook=torch.cuda.synchronize to\n        make sure all gpu operations are done before timer is stopped.\n    \"\"\"\n    for agg in get_active_aggregators():\n        if key in agg:\n            agg[key].stop(weight, prehook)\n\n\ndef log_custom(\n    new_meter_fn: Callable[[], Meter],\n    key: str,\n    *args,\n    priority: int = 50,\n    **kwargs,\n):\n    \"\"\"Log using a custom Meter.\n\n    Any extra *args* or *kwargs* will be passed through to the Meter's\n    *update* method.\n\n    Args:\n        new_meter_fn (Callable[[], Meter]): function that returns a new\n            Meter instance\n        key (str): name of the field to log\n        priority (int): smaller values are logged earlier in the output\n    \"\"\"\n    for agg in get_active_aggregators():\n        if key not in agg:\n            agg.add_meter(key, new_meter_fn(), priority)\n        agg[key].update(*args, **kwargs)\n\n\ndef reset_meter(name: str, key: str) -> None:\n    \"\"\"Reset Meter instance aggregated under a given *name* and *key*.\"\"\"\n    meter = get_meter(name, key)\n    if meter is not None:\n        meter.reset()\n\n\ndef reset_meters(name: str) -> None:\n    \"\"\"Reset Meter instances aggregated under a given *name*.\"\"\"\n    meters = get_meters(name)\n    if meters is not None:\n        meters.reset()\n\n\ndef get_meter(name: str, key: str) -> Meter:\n    \"\"\"Get a single Meter instance aggregated under *name* and *key*.\n\n    Returns:\n        Meter or None if no metrics have been logged under *name* and *key*.\n    \"\"\"\n    if name not in _aggregators:\n        return None\n    return _aggregators[name].get(key, None)\n\n\ndef get_meters(name: str) -> MetersDict:\n    \"\"\"Get Meter instances aggregated under a given *name*.\n\n    Returns:\n        MetersDict or None if no metrics have been logged under *name*.\n    \"\"\"\n    return _aggregators.get(name, None)\n\n\ndef get_smoothed_value(name: str, key: str) -> float:\n    \"\"\"Get a single smoothed value.\n\n    Raises:\n        KeyError: if no metrics have been logged under *name* and *key*.\n    \"\"\"\n    return _aggregators[name].get_smoothed_value(key)\n\n\ndef get_smoothed_values(name: str) -> Dict[str, float]:\n    \"\"\"Get smoothed values aggregated under a given *name*.\n\n    Raises:\n        KeyError: if no metrics have been logged under *name*.\n    \"\"\"\n    return _aggregators[name].get_smoothed_values()\n\n\ndef state_dict():\n    return OrderedDict([(name, agg.state_dict()) for name, agg in _aggregators.items()])\n\n\ndef load_state_dict(state_dict):\n    for name, agg_state in state_dict.items():\n        _aggregators[name] = MetersDict()\n        _aggregators[name].load_state_dict(agg_state)\n\n\ndef nvidia_smi_gpu_memory_stats():\n    \"\"\"\n    Parse the nvidia-smi output and extract the memory used stats.\n    \"\"\"\n    out_dict = {}\n    try:\n        sp = subprocess.Popen(\n            [\"nvidia-smi\", \"--query-gpu=index,memory.used\", \"--format=csv,noheader\"],\n            stdout=subprocess.PIPE,\n            stderr=subprocess.PIPE,\n            close_fds=True,\n        )\n        out_str = sp.communicate()\n        out_list = out_str[0].decode(\"utf-8\").split(\"\\n\")\n        out_dict = {}\n        for item in out_list:\n            if \" MiB\" in item:\n                gpu_idx, mem_used = item.split(\",\")\n                gpu_key = f\"gpu_{gpu_idx}_mem_used_gb\"\n                out_dict[gpu_key] = int(mem_used.strip().split(\" \")[0]) / 1024\n    except FileNotFoundError:\n        logging.error(\n            \"Failed to find the 'nvidia-smi' executable for printing GPU stats\"\n        )\n    except subprocess.CalledProcessError as e:\n        logging.error(f\"nvidia-smi returned non zero error code: {e.returncode}\")\n\n    return out_dict\n\n\ndef get_nvidia_smi_gpu_memory_stats_str():\n    return \"nvidia-smi stats: {}\".format(nvidia_smi_gpu_memory_stats())\n",
        "metaseq/logging/progress_bar/__init__.py": "# Copyright (c) Meta Platforms, Inc. and affiliates. All Rights Reserved.\n#\n# This source code is licensed under the MIT license found in the\n# LICENSE file in the root directory of this source tree.\n\nimport logging\nfrom typing import Optional\n\nfrom metaseq.logging.progress_bar.base_progress_bar import logger\nfrom metaseq.logging.progress_bar.json_progress_bar import JsonProgressBar\nfrom metaseq.logging.progress_bar.tensorboard_progress_bar import (\n    TensorboardProgressBarWrapper,\n)\nfrom metaseq.logging.progress_bar.aim_progress_bar import AimProgressBarWrapper\nfrom metaseq.logging.progress_bar.wandb_progress_bar import WandBProgressBarWrapper\n\n\ndef get_progress_bar(\n    iterator,\n    log_format: str = \"json\",\n    log_interval: int = 100,\n    log_file: Optional[str] = None,\n    epoch: Optional[int] = None,\n    prefix: Optional[str] = None,\n    tensorboard_logdir: Optional[str] = None,\n    wandb_project: Optional[str] = None,\n    wandb_run_name: Optional[str] = None,\n    aim_repo: Optional[str] = None,\n    aim_run_hash: Optional[str] = None,\n    aim_param_checkpoint_dir: Optional[str] = None,\n):\n    if log_file is not None:\n        handler = logging.FileHandler(filename=log_file)\n        logger.addHandler(handler)\n\n    if log_format == \"json\":\n        bar = JsonProgressBar(iterator, epoch, prefix, log_interval)\n    else:\n        raise ValueError(\"Unknown log format: {}\".format(log_format))\n\n    if wandb_project:\n        bar = WandBProgressBarWrapper(bar, wandb_project, run_name=wandb_run_name)\n    elif tensorboard_logdir:\n        bar = TensorboardProgressBarWrapper(bar, tensorboard_logdir)\n\n    if aim_repo:\n        bar = AimProgressBarWrapper(\n            bar, aim_repo, aim_run_hash, aim_param_checkpoint_dir\n        )\n\n    return bar\n",
        "metaseq/logging/progress_bar/aim_progress_bar.py": "# Copyright (c) Meta Platforms, Inc. and affiliates. All Rights Reserved.\n#\n# This source code is licensed under the MIT license found in the\n# LICENSE file in the root directory of this source tree.\n\nfrom metaseq.logging.progress_bar.base_progress_bar import (\n    BaseProgressBar,\n    logger,\n)\n\n\ntry:\n    import functools\n\n    from aim import Repo as AimRepo\n\n    @functools.lru_cache()\n    def get_aim_run(repo, run_hash):\n        from aim import Run\n\n        return Run(run_hash=run_hash, repo=repo)\n\nexcept ImportError:\n    get_aim_run = None\n    AimRepo = None\n\n\nclass AimProgressBarWrapper(BaseProgressBar):\n    \"\"\"Log to Aim.\"\"\"\n\n    def __init__(self, wrapped_bar, aim_repo, aim_run_hash, aim_param_checkpoint_dir):\n        self.wrapped_bar = wrapped_bar\n\n        if get_aim_run is None:\n            self.run = None\n            logger.warning(\"Aim not found, please install with: pip install aim\")\n        else:\n            logger.info(f\"Storing logs at Aim repo: {aim_repo}\")\n            assert AimRepo is not None\n\n            if not aim_run_hash:\n                # Find run based on save_dir parameter\n                query = f\"run.checkpoint.save_dir == '{aim_param_checkpoint_dir}'\"\n                try:\n                    runs_generator = AimRepo(aim_repo).query_runs(query)\n                    run = next(runs_generator.iter_runs())\n                    aim_run_hash = run.run.hash\n                except Exception:\n                    pass\n\n            if aim_run_hash:\n                logger.info(f\"Appending to run: {aim_run_hash}\")\n\n            self.run = get_aim_run(aim_repo, aim_run_hash)\n\n    def __len__(self):\n        return len(self.wrapped_bar)\n\n    def __iter__(self):\n        return iter(self.wrapped_bar)\n\n    def log(self, stats, tag=None, step=None):\n        \"\"\"Log intermediate stats to Aim.\"\"\"\n        self._log_to_aim(stats, tag, step)\n        self.wrapped_bar.log(stats, tag=tag, step=step)\n\n    def print(self, stats, tag=None, step=None):\n        \"\"\"Print end-of-epoch stats.\"\"\"\n        self._log_to_aim(stats, tag, step)\n        self.wrapped_bar.print(stats, tag=tag, step=step)\n\n    def update_config(self, config):\n        \"\"\"Log latest configuration.\"\"\"\n        if self.run is not None:\n            for key in config:\n                self.run.set(key, config[key], strict=False)\n        self.wrapped_bar.update_config(config)\n\n    def _log_to_aim(self, stats, tag=None, step=None):\n        if self.run is None:\n            return\n\n        if step is None:\n            step = stats[\"num_updates\"]\n\n        if \"train\" in tag:\n            context = {\"tag\": tag, \"subset\": \"train\"}\n        elif \"val\" in tag:\n            context = {\"tag\": tag, \"subset\": \"val\"}\n        else:\n            context = {\"tag\": tag}\n\n        for key in stats.keys() - {\"num_updates\"}:\n            self.run.track(stats[key], name=key, step=step, context=context)\n",
        "metaseq/logging/progress_bar/base_progress_bar.py": "# Copyright (c) Meta Platforms, Inc. and affiliates. All Rights Reserved.\n#\n# This source code is licensed under the MIT license found in the\n# LICENSE file in the root directory of this source tree.\n\n\"\"\"\nWrapper around various loggers and progress bars (e.g., json).\n\"\"\"\n\nimport logging\nfrom collections import OrderedDict\nfrom numbers import Number\n\nimport torch\n\nfrom metaseq.logging.meters import AverageMeter, TimeMeter, StopwatchMeter\n\nlogger = logging.getLogger(__name__)\n\n\nclass BaseProgressBar(object):\n    \"\"\"Abstract class for progress bars.\"\"\"\n\n    def __init__(self, iterable, epoch=None, prefix=None):\n        self.iterable = iterable\n        self.n = getattr(iterable, \"n\", 0)\n        self.epoch = epoch\n        self.prefix = \"\"\n        if epoch is not None:\n            self.prefix += \"epoch {:03d}\".format(epoch)\n        if prefix is not None:\n            self.prefix += (\" | \" if self.prefix != \"\" else \"\") + prefix\n\n    def __len__(self):\n        return len(self.iterable)\n\n    def __enter__(self):\n        return self\n\n    def __exit__(self, *exc):\n        return False\n\n    def __iter__(self):\n        raise NotImplementedError\n\n    def log(self, stats, tag=None, step=None):\n        \"\"\"Log intermediate stats according to log_interval.\"\"\"\n        raise NotImplementedError\n\n    def print(self, stats, tag=None, step=None):\n        \"\"\"Print end-of-epoch stats.\"\"\"\n        raise NotImplementedError\n\n    def update_config(self, config):\n        \"\"\"Log latest configuration.\"\"\"\n        pass\n\n    def _str_commas(self, stats):\n        return \", \".join(key + \"=\" + stats[key].strip() for key in stats.keys())\n\n    def _str_pipes(self, stats):\n        return \" | \".join(key + \" \" + stats[key].strip() for key in stats.keys())\n\n    def _format_stats(self, stats):\n        postfix = OrderedDict(stats)\n        # Preprocess stats according to datatype\n        for key in postfix.keys():\n            postfix[key] = str(format_stat(postfix[key]))\n        return postfix\n\n\ndef format_stat(stat):\n    if isinstance(stat, Number):\n        stat = \"{:g}\".format(stat)\n    elif isinstance(stat, AverageMeter):\n        stat = \"{:.3f}\".format(stat.avg)\n    elif isinstance(stat, TimeMeter):\n        stat = \"{:g}\".format(round(stat.avg))\n    elif isinstance(stat, StopwatchMeter):\n        stat = \"{:g}\".format(round(stat.sum))\n    elif torch.is_tensor(stat):\n        stat = stat.tolist()\n        if isinstance(stat, float):\n            stat = f'{float(f\"{stat:.3g}\"):g}'  # 3 significant figures\n    return stat\n",
        "metaseq/logging/progress_bar/json_progress_bar.py": "# Copyright (c) Meta Platforms, Inc. and affiliates. All Rights Reserved.\n#\n# This source code is licensed under the MIT license found in the\n# LICENSE file in the root directory of this source tree.\n\nimport json\nfrom collections import OrderedDict\nfrom contextlib import contextmanager\n\nfrom metaseq.logging.progress_bar.base_progress_bar import (\n    BaseProgressBar,\n    logger,\n    format_stat,\n)\nfrom metaseq.utils import get_precise_epoch\n\n\n@contextmanager\ndef rename_logger(logger, new_name):\n    old_name = logger.name\n    if new_name is not None:\n        logger.name = new_name\n    yield logger\n    logger.name = old_name\n\n\nclass JsonProgressBar(BaseProgressBar):\n    \"\"\"Log output in JSON format.\"\"\"\n\n    def __init__(self, iterable, epoch=None, prefix=None, log_interval=1000):\n        super().__init__(iterable, epoch, prefix)\n        self.log_interval = log_interval\n        self.i = None\n        self.size = None\n\n    def __iter__(self):\n        self.size = len(self.iterable)\n        for i, obj in enumerate(self.iterable, start=self.n):\n            self.i = i\n            yield obj\n\n    def log(self, stats, tag=None, step=None):\n        \"\"\"Log intermediate stats according to log_interval.\"\"\"\n        step = step or self.i or 0\n        if step > 0 and self.log_interval is not None and step % self.log_interval == 0:\n            update = get_precise_epoch(self.epoch, self.i, self.size)\n            stats = self._format_stats(stats, epoch=self.epoch, update=update)\n            with rename_logger(logger, tag):\n                logger.info(json.dumps(stats))\n\n    def print(self, stats, tag=None, step=None):\n        \"\"\"Print end-of-epoch stats.\"\"\"\n        self.stats = stats\n        if tag is not None:\n            self.stats = OrderedDict(\n                [(tag + \"_\" + k, v) for k, v in self.stats.items()]\n            )\n        stats = self._format_stats(self.stats, epoch=self.epoch)\n        with rename_logger(logger, tag):\n            logger.info(json.dumps(stats))\n\n    def _format_stats(self, stats, epoch=None, update=None):\n        postfix = OrderedDict()\n        if epoch is not None:\n            postfix[\"epoch\"] = epoch\n        if update is not None:\n            postfix[\"update\"] = round(update, 3)\n        # Preprocess stats according to datatype\n        for key in stats.keys():\n            postfix[key] = format_stat(stats[key])\n        return postfix\n",
        "metaseq/logging/progress_bar/tensorboard_progress_bar.py": "# Copyright (c) Meta Platforms, Inc. and affiliates. All Rights Reserved.\n#\n# This source code is licensed under the MIT license found in the\n# LICENSE file in the root directory of this source tree.\n\nimport os\nimport sys\nfrom numbers import Number\nimport atexit\n\nimport torch\n\nfrom metaseq.logging.meters import AverageMeter\nfrom metaseq.logging.progress_bar.base_progress_bar import BaseProgressBar, logger\n\n_tensorboard_writers = {}\nSummaryWriter = None\n\ntry:\n    from torch.utils.tensorboard import SummaryWriter\nexcept ImportError:\n    try:\n        from tensorboardX import SummaryWriter\n    except ImportError:\n        pass\n\n\ndef _close_writers():\n    for w in _tensorboard_writers.values():\n        w.close()\n\n\natexit.register(_close_writers)\n\n\nclass TensorboardProgressBarWrapper(BaseProgressBar):\n    \"\"\"Log to tensorboard.\"\"\"\n\n    def __init__(self, wrapped_bar, tensorboard_logdir):\n        self.wrapped_bar = wrapped_bar\n        self.tensorboard_logdir = tensorboard_logdir\n\n        if SummaryWriter is None:\n            logger.warning(\n                \"tensorboard not found, please install with: pip install tensorboard\"\n            )\n\n    def _writer(self, key):\n        if SummaryWriter is None:\n            return None\n        _writers = _tensorboard_writers\n        if key not in _writers:\n            # tensorboard doesn't play well when we clobber it with reruns\n            # find an acceptable suffix\n            for suffix in (f\"{i:04d}\" for i in range(10000)):\n                logdir = os.path.join(self.tensorboard_logdir + suffix, key)\n                if not os.path.exists(logdir):\n                    logger.info(f\"Setting tensorboard directory to {logdir}\")\n                    break\n            else:\n                # wow we have cycled through a lot of these\n                raise RuntimeError(\n                    f\"Tensorboard logdir {logdir} already exists. \"\n                    \"Ran out of possible suffixes.\"\n                )\n            _writers[key] = SummaryWriter(logdir)\n            _writers[key].add_text(\"sys.argv\", \" \".join(sys.argv))\n        return _writers[key]\n\n    def __len__(self):\n        return len(self.wrapped_bar)\n\n    def __iter__(self):\n        return iter(self.wrapped_bar)\n\n    def log(self, stats, tag=None, step=None):\n        \"\"\"Log intermediate stats to tensorboard.\"\"\"\n        self._log_to_tensorboard(stats, tag, step)\n        self.wrapped_bar.log(stats, tag=tag, step=step)\n\n    def print(self, stats, tag=None, step=None):\n        \"\"\"Print end-of-epoch stats.\"\"\"\n        self._log_to_tensorboard(stats, tag, step)\n        self.wrapped_bar.print(stats, tag=tag, step=step)\n\n    def update_config(self, config):\n        \"\"\"Log latest configuration.\"\"\"\n        # TODO add hparams to Tensorboard\n        self.wrapped_bar.update_config(config)\n\n    def _log_to_tensorboard(self, stats, tag=None, step=None):\n        writer = self._writer(tag or \"\")\n        if writer is None:\n            return\n        if step is None:\n            step = stats[\"num_updates\"]\n        for key in stats.keys() - {\"num_updates\"}:\n            if isinstance(stats[key], AverageMeter):\n                writer.add_scalar(key, stats[key].val, step)\n            elif isinstance(stats[key], Number):\n                writer.add_scalar(key, stats[key], step)\n            elif torch.is_tensor(stats[key]) and stats[key].numel() == 1:\n                writer.add_scalar(key, stats[key].item(), step)\n        writer.flush()\n",
        "metaseq/logging/progress_bar/wandb_progress_bar.py": "# Copyright (c) Meta Platforms, Inc. and affiliates. All Rights Reserved.\n#\n# This source code is licensed under the MIT license found in the\n# LICENSE file in the root directory of this source tree.\n\nfrom numbers import Number\n\nfrom metaseq.logging.meters import AverageMeter\nfrom metaseq.logging.progress_bar.base_progress_bar import (\n    BaseProgressBar,\n    logger,\n)\nfrom metaseq.utils import get_precise_epoch\n\nwandb = None\n\ntry:\n    import wandb\nexcept ImportError:\n    pass\n\n\nclass WandBProgressBarWrapper(BaseProgressBar):\n    \"\"\"Log to Weights & Biases.\"\"\"\n\n    def __init__(self, wrapped_bar, wandb_project, run_name=None):\n        super().__init__(\n            wrapped_bar, epoch=wrapped_bar.epoch, prefix=wrapped_bar.prefix\n        )\n        self.wrapped_bar = wrapped_bar\n        if wandb is None:\n            logger.warning(\"wandb not found, pip install wandb\")\n            return\n\n        # reinit=False to ensure if wandb.init() is called multiple times\n        # within one process it still references the same run\n        wandb.init(project=wandb_project, reinit=False, name=run_name)\n\n    def __len__(self):\n        return len(self.wrapped_bar)\n\n    def __iter__(self):\n        self.size = len(self.wrapped_bar)\n        for i, obj in enumerate(self.wrapped_bar, start=self.n):\n            self.i = i\n            yield obj\n\n    def log(self, stats, tag=None, step=None):\n        \"\"\"Log intermediate stats to tensorboard.\"\"\"\n        self._log_to_wandb(stats, tag, step)\n        self.wrapped_bar.log(stats, tag=tag, step=step)\n\n    def print(self, stats, tag=None, step=None):\n        \"\"\"Print end-of-epoch stats.\"\"\"\n        self._log_to_wandb(stats, tag, step)\n        self.wrapped_bar.print(stats, tag=tag, step=step)\n\n    def update_config(self, config):\n        \"\"\"Log latest configuration.\"\"\"\n        if wandb is not None:\n            wandb.config.update(config, allow_val_change=True)\n        self.wrapped_bar.update_config(config)\n\n    def _log_to_wandb(self, stats, tag=None, step=None):\n        if wandb is None:\n            return\n        if step is None:\n            step = stats[\"num_updates\"]\n\n        prefix = \"\" if tag is None else tag + \"/\"\n\n        epoch = get_precise_epoch(self.epoch, self.i, self.size)\n        wandb.log({prefix + \"epoch\": epoch}, step=step)\n\n        for key in stats.keys() - {\"num_updates\"}:\n            if isinstance(stats[key], AverageMeter):\n                wandb.log({prefix + key: stats[key].val}, step=step)\n            elif isinstance(stats[key], Number):\n                wandb.log({prefix + key: stats[key]}, step=step)\n",
        "metaseq/models/__init__.py": "# Copyright (c) Meta Platforms, Inc. and affiliates. All Rights Reserved.\n#\n# This source code is licensed under the MIT license found in the\n# LICENSE file in the root directory of this source tree.\n\"\"\"isort:skip_file\"\"\"\n\nimport argparse\nimport importlib\nimport os\n\nfrom metaseq.dataclass import MetaseqDataclass\nfrom metaseq.dataclass.utils import merge_with_parent, populate_dataclass\nfrom hydra.core.config_store import ConfigStore\n\nfrom .distributed_model import DistributedModel\nfrom .base_decoder import BaseDecoder\nfrom .base_model import (\n    BaseModel,\n)\n\n\nMODEL_REGISTRY = {}\nMODEL_DATACLASS_REGISTRY = {}\nARCH_MODEL_REGISTRY = {}\nARCH_MODEL_NAME_REGISTRY = {}\nARCH_MODEL_INV_REGISTRY = {}\nARCH_CONFIG_REGISTRY = {}\n\n\n__all__ = [\n    \"BaseModel\",\n    \"DistributedModel\",\n    \"BaseDecoder\",\n    \"register_model\",\n    \"register_model_architecture\",\n]\n\n\ndef build_model(cfg: MetaseqDataclass, task):\n    model = None\n    model_type = getattr(cfg, \"_name\", None) or getattr(cfg, \"arch\", None)\n\n    if not model_type and len(cfg) == 1:\n        # this is hit if config object is nested in directory that is named after model type\n\n        model_type = next(iter(cfg))\n        if model_type in MODEL_DATACLASS_REGISTRY:\n            cfg = cfg[model_type]\n        else:\n            raise Exception(\n                \"Could not infer model type from directory. Please add _name field to indicate model type. \"\n                \"Available models: \"\n                + str(MODEL_DATACLASS_REGISTRY.keys())\n                + \" Requested model type: \"\n                + model_type\n            )\n\n    if model_type in ARCH_MODEL_REGISTRY:\n        # case 1: legacy models\n        model = ARCH_MODEL_REGISTRY[model_type]\n    elif model_type in MODEL_DATACLASS_REGISTRY:\n        # case 2: config-driven models\n        model = MODEL_REGISTRY[model_type]\n\n    if model_type in MODEL_DATACLASS_REGISTRY:\n        # set defaults from dataclass. note that arch name and model name can be the same\n        dc = MODEL_DATACLASS_REGISTRY[model_type]\n        if isinstance(cfg, argparse.Namespace):\n            cfg = populate_dataclass(dc(), cfg)\n        else:\n            cfg = merge_with_parent(dc(), cfg)\n\n    assert model is not None, (\n        f\"Could not infer model type from {cfg}. \"\n        f\"Available models: \"\n        + str(MODEL_DATACLASS_REGISTRY.keys())\n        + \" Requested model type: \"\n        + model_type\n    )\n\n    return model.build_model(cfg, task)\n\n\ndef register_model(name, dataclass=None):\n    \"\"\"\n    New model types can be added to metaseq with the :func:`register_model`\n    function decorator.\n\n    For example::\n\n        @register_model('lstm')\n        class LSTM(BaseModel):\n            (...)\n\n    .. note:: All models must implement the :class:`BaseModel` interface.\n        Typically you will extend :class:`BaseModel` for\n        language modeling tasks.\n\n    Args:\n        name (str): the name of the model\n    \"\"\"\n\n    def register_model_cls(cls):\n        if name in MODEL_REGISTRY:\n            raise ValueError(\"Cannot register duplicate model ({})\".format(name))\n        if not issubclass(cls, BaseModel):\n            raise ValueError(\n                \"Model ({}: {}) must extend BaseModel\".format(name, cls.__name__)\n            )\n        MODEL_REGISTRY[name] = cls\n        if dataclass is not None and not issubclass(dataclass, MetaseqDataclass):\n            raise ValueError(\n                \"Dataclass {} must extend MetaseqDataclass\".format(dataclass)\n            )\n\n        cls.__dataclass = dataclass\n        if dataclass is not None:\n            MODEL_DATACLASS_REGISTRY[name] = dataclass\n\n            cs = ConfigStore.instance()\n            node = dataclass()\n            node._name = name\n            cs.store(name=name, group=\"model\", node=node, provider=\"metaseq\")\n\n            @register_model_architecture(name, name)\n            def noop(_):\n                pass\n\n        return cls\n\n    return register_model_cls\n\n\ndef register_model_architecture(model_name, arch_name):\n    \"\"\"\n    New model architectures can be added to metaseq with the\n    :func:`register_model_architecture` function decorator. After registration,\n    model architectures can be selected with the ``--arch`` command-line\n    argument.\n\n    For example::\n\n        @register_model_architecture('lstm', 'lstm_luong_wmt_en_de')\n        def lstm_luong_wmt_en_de(cfg):\n            args.encoder_embed_dim = getattr(cfg.model, 'encoder_embed_dim', 1000)\n            (...)\n\n    The decorated function should take a single argument *cfg*, which is a\n    :class:`omegaconf.DictConfig`. The decorated function should modify these\n    arguments in-place to match the desired architecture.\n\n    Args:\n        model_name (str): the name of the Model (Model must already be\n            registered)\n        arch_name (str): the name of the model architecture (``--arch``)\n    \"\"\"\n\n    def register_model_arch_fn(fn):\n        if model_name not in MODEL_REGISTRY:\n            raise ValueError(\n                \"Cannot register model architecture for unknown model type ({})\".format(\n                    model_name\n                )\n            )\n        if arch_name in ARCH_MODEL_REGISTRY:\n            raise ValueError(\n                \"Cannot register duplicate model architecture ({})\".format(arch_name)\n            )\n        if not callable(fn):\n            raise ValueError(\n                \"Model architecture must be callable ({})\".format(arch_name)\n            )\n        ARCH_MODEL_REGISTRY[arch_name] = MODEL_REGISTRY[model_name]\n        ARCH_MODEL_NAME_REGISTRY[arch_name] = model_name\n        ARCH_MODEL_INV_REGISTRY.setdefault(model_name, []).append(arch_name)\n        ARCH_CONFIG_REGISTRY[arch_name] = fn\n        return fn\n\n    return register_model_arch_fn\n\n\n# automatically import any Python files in the models/ directory\nmodels_dir = os.path.dirname(__file__)\nfor file in os.listdir(models_dir):\n    path = os.path.join(models_dir, file)\n    if (\n        not file.startswith(\"_\")\n        and not file.startswith(\".\")\n        and (file.endswith(\".py\") or os.path.isdir(path))\n    ):\n        model_name = file[: file.find(\".py\")] if file.endswith(\".py\") else file\n        module = importlib.import_module(\"metaseq.models.\" + model_name)\n\n        # extra `model_parser` for sphinx\n        if model_name in MODEL_REGISTRY:\n            parser = argparse.ArgumentParser(add_help=False)\n            group_archs = parser.add_argument_group(\"Named architectures\")\n            group_archs.add_argument(\n                \"--arch\", choices=ARCH_MODEL_INV_REGISTRY[model_name]\n            )\n            group_args = parser.add_argument_group(\"Additional command-line arguments\")\n            MODEL_REGISTRY[model_name].add_args(group_args)\n            globals()[model_name + \"_parser\"] = parser\n",
        "metaseq/models/base_decoder.py": "# Copyright (c) Meta Platforms, Inc. and affiliates. All Rights Reserved.\n#\n# This source code is licensed under the MIT license found in the\n# LICENSE file in the root directory of this source tree.\n\nfrom typing import Dict, Optional\n\nimport torch.nn as nn\nfrom torch import Tensor\n\nfrom metaseq import utils\nfrom metaseq.incremental_decoding_utils import with_incremental_state\n\n\n@with_incremental_state\nclass BaseDecoder(nn.Module):\n    \"\"\"Base class for incremental decoders.\n\n    Incremental decoding is a mode at inference time where the Model\n    only receives a single timestep of input corresponding to the previous\n    output token (for teacher forcing) and must produce the next output\n    *incrementally*. Thus the model must cache any long-term state that is\n    needed about the sequence, e.g., hidden states, convolutional states, etc.\n\n    This interface also defines the :func:`reorder_incremental_state` method,\n    which is used during beam search to select and reorder the incremental state\n    based on the selection of beams.\n\n    To learn more about how incremental decoding works, refer to `this blog\n    <http://www.telesens.co/2019/04/21/understanding-incremental-decoding-in-fairseq/>`_.\n\n    Note that incremental_state will take different values depending on the\n    situation. At train and validation time, incremental_state will be None,\n    indicating that no incremental state is available and does not need to be\n    computed.\n\n    During generation, incremental_state will begin as an empty\n    dictionary, indicating no incremental_state is available, but SHOULD be\n    computed. This class modifies this dictionary inline via\n    reorder_incremental_state. After that first initial step, incremental_state\n    will be full of model-specific state.\n    \"\"\"\n\n    def __init__(self, dictionary):\n        super().__init__()\n        self.dictionary = dictionary\n\n    def forward(self, prev_output_tokens, incremental_state=None, **kwargs):\n        \"\"\"\n        Args:\n            prev_output_tokens (LongTensor): shifted output tokens of shape\n                `(batch, tgt_len)`, for teacher forcing\n            incremental_state (dict, optional): dictionary used for storing\n                state during :ref:`Incremental decoding`. Note that this\n                dictionary is modified inline iff incremental_state is not None.\n\n        Returns:\n            tuple:\n                - the decoder's output of shape `(batch, tgt_len, vocab)`\n                - a dictionary with any model-specific outputs\n        \"\"\"\n        raise NotImplementedError\n\n    def extract_features(self, prev_output_tokens, incremental_state=None, **kwargs):\n        \"\"\"\n        Returns:\n            tuple:\n                - the decoder's features of shape `(batch, tgt_len, embed_dim)`\n                - a dictionary with any model-specific outputs\n        \"\"\"\n        raise NotImplementedError\n\n    def output_layer(self, features, **kwargs):\n        \"\"\"\n        Project features to the default output size, e.g., vocabulary size.\n\n        Args:\n            features (Tensor): features returned by *extract_features*.\n        \"\"\"\n        raise NotImplementedError\n\n    def reorder_incremental_state(\n        self,\n        incremental_state: Dict[str, Dict[str, Optional[Tensor]]],\n        new_order: Tensor,\n    ):\n        \"\"\"Reorder incremental state.\n\n        This will be called when the order of the input has changed from the\n        previous time step. A typical use case is beam search, where the input\n        order changes between time steps based on the selection of beams.\n        \"\"\"\n        pass\n\n    def get_normalized_probs(self, logits: Tensor, log_probs: bool):\n        \"\"\"Get normalized probabilities (or log probs) from a net's output.\"\"\"\n        if log_probs:\n            return utils.log_softmax(logits, dim=-1)\n        else:\n            return utils.softmax(logits, dim=-1)\n\n    def max_positions(self):\n        \"\"\"Maximum input length supported by the decoder.\"\"\"\n        return 1e6  # an arbitrary large number\n",
        "metaseq/models/base_model.py": "# Copyright (c) Meta Platforms, Inc. and affiliates. All Rights Reserved.\n#\n# This source code is licensed under the MIT license found in the\n# LICENSE file in the root directory of this source tree.\n\"\"\"\nBase classes for various metaseq models.\n\"\"\"\n\nimport logging\nfrom typing import Dict, List, Optional, Tuple\n\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom omegaconf import DictConfig\nfrom torch import Tensor\n\nfrom metaseq.dataclass.utils import gen_parser_from_dataclass\nfrom metaseq.models import BaseDecoder\n\nlogger = logging.getLogger(__name__)\n\n\ndef check_type(module, expected_type):\n    if hasattr(module, \"unwrapped_module\"):\n        assert isinstance(\n            module.unwrapped_module, expected_type\n        ), f\"{type(module.unwrapped_module)} != {expected_type}\"\n    else:\n        assert isinstance(module, expected_type), f\"{type(module)} != {expected_type}\"\n\n\nclass BaseModel(nn.Module):\n    \"\"\"Base class for metaseq models.\"\"\"\n\n    def __init__(self, decoder):\n        super().__init__()\n        self._is_generation_fast = False\n        self.decoder = decoder\n        check_type(self.decoder, BaseDecoder)\n\n    @classmethod\n    def add_args(cls, parser):\n        \"\"\"Add model-specific arguments to the parser.\"\"\"\n        dc = getattr(cls, \"__dataclass\", None)\n        if dc is not None:\n            # do not set defaults so that settings defaults from various architectures still works\n            gen_parser_from_dataclass(parser, dc(), delete_default=True)\n\n    @classmethod\n    def build_model(cls, args, task):\n        \"\"\"Build a new model instance.\"\"\"\n        raise NotImplementedError(\"Model must implement the build_model method\")\n\n    def get_targets(self, sample):\n        \"\"\"Get targets from sample.\"\"\"\n        return sample[\"target\"]\n\n    def get_normalized_probs(\n        self,\n        net_output: Tuple[Tensor, Optional[Dict[str, List[Optional[Tensor]]]]],\n        log_probs: bool,\n    ):\n        \"\"\"Get normalized probabilities (or log probs) from a net's output.\"\"\"\n        if hasattr(self, \"decoder\"):\n            return self.decoder.get_normalized_probs(net_output, log_probs)\n        elif torch.is_tensor(net_output):\n            # syntactic sugar for simple models which don't have a decoder\n            # (e.g., the classification tutorial)\n            logits = net_output.float()\n            if log_probs:\n                return F.log_softmax(logits, dim=-1)\n            else:\n                return F.softmax(logits, dim=-1)\n        raise NotImplementedError\n\n    def set_num_updates(self, num_updates):\n        \"\"\"State from trainer to pass along to model at every update.\"\"\"\n        for m in self.modules():\n            if hasattr(m, \"set_num_updates\") and m != self:\n                m.set_num_updates(num_updates)\n\n    def prepare_for_inference_(self, cfg: DictConfig):\n        \"\"\"Prepare model for inference.\"\"\"\n        kwargs = {}\n        kwargs[\"beamable_mm_beam_size\"] = (\n            None\n            if getattr(cfg.generation, \"no_beamable_mm\", False)\n            else getattr(cfg.generation, \"beam\", 5)\n        )\n        self.make_generation_fast_(**kwargs)\n\n    def make_generation_fast_(self, **kwargs):\n        \"\"\"\n        Legacy entry point to optimize model for faster generation.\n        Prefer prepare_for_inference_.\n        \"\"\"\n        if self._is_generation_fast:\n            return  # only apply once\n        self._is_generation_fast = True\n\n        # remove weight norm from all modules in the network\n        def apply_remove_weight_norm(module):\n            try:\n                nn.utils.remove_weight_norm(module)\n            except (AttributeError, ValueError):  # this module didn't have weight norm\n                return\n\n        self.apply(apply_remove_weight_norm)\n\n        def apply_make_generation_fast_(module, prefix):\n            if len(prefix) > 0:\n                prefix += \".\"\n\n            base_func = BaseModel.make_generation_fast_\n            for n, m in module.named_modules():\n                if (\n                    m != self\n                    and hasattr(m, \"make_generation_fast_\")\n                    # don't call this implementation again, e.g., if\n                    # children modules also inherit from BaseModel\n                    and m.make_generation_fast_.__func__ is not base_func\n                ):\n                    name = prefix + n\n                    m.make_generation_fast_(name=name, **kwargs)\n\n        apply_make_generation_fast_(self, \"\")\n\n        def train(mode=True):\n            if mode:\n                raise RuntimeError(\"cannot train after make_generation_fast\")\n\n        # this model should no longer be used for training\n        self.eval()\n        self.train = train\n\n    @classmethod\n    def from_pretrained(\n        cls,\n        model_name_or_path,\n        checkpoint_file=\"model.pt\",\n        data_name_or_path=\".\",\n        skip_prepare_for_inference=False,\n        **kwargs,\n    ):\n        raise NotImplementedError\n\n    @classmethod\n    def hub_models(cls):\n        return {}\n\n    def forward(self, src_tokens, **kwargs):\n        \"\"\"\n        Run the forward pass for a decoder-only model.\n\n        Feeds a batch of tokens through the decoder to predict the next tokens.\n\n        Args:\n            src_tokens (LongTensor): tokens on which to condition the decoder,\n                of shape `(batch, tgt_len)`\n            src_lengths (LongTensor): source sentence lengths of shape `(batch)`\n\n        Returns:\n            tuple:\n                - the decoder's output of shape `(batch, seq_len, vocab)`\n                - a dictionary with any model-specific outputs\n        \"\"\"\n        return self.decoder(src_tokens, **kwargs)\n\n    def forward_decoder(self, prev_output_tokens, **kwargs):\n        return self.decoder(prev_output_tokens, **kwargs)\n\n    def extract_features(self, src_tokens, **kwargs):\n        \"\"\"\n        Similar to *forward* but only return features.\n\n        Returns:\n            tuple:\n                - the decoder's features of shape `(batch, seq_len, embed_dim)`\n                - a dictionary with any model-specific outputs\n        \"\"\"\n        return self.decoder.extract_features(src_tokens, **kwargs)\n\n    def output_layer(self, features, **kwargs):\n        \"\"\"Project features to the default output size (typically vocabulary size).\"\"\"\n        return self.decoder.output_layer(features, **kwargs)\n\n    def max_positions(self):\n        \"\"\"Maximum length supported by the model.\"\"\"\n        return self.decoder.max_positions()\n\n    def max_decoder_positions(self):\n        \"\"\"Maximum length supported by the decoder.\"\"\"\n        return self.decoder.max_positions()\n\n    @property\n    def supported_targets(self):\n        return {\"future\"}\n",
        "metaseq/models/distributed_model.py": "# Copyright (c) Meta Platforms, Inc. and affiliates. All Rights Reserved.\n#\n# This source code is licensed under the MIT license found in the\n# LICENSE file in the root directory of this source tree.\n\nimport logging\n\nimport torch.nn as nn\nfrom torch.nn.parallel import DistributedDataParallel\n\nfrom metaseq.distributed import (\n    ModuleProxyWrapper,\n)\n\nlogger = logging.getLogger(__name__)\n\n\n_GOSSIP_DISABLED = False\ntry:\n    import gossip  # noqa: F401\nexcept ImportError:\n    _GOSSIP_DISABLED = True\n\n\ndef DistributedModel(args, model, process_group, device):\n    \"\"\"\n    Wrap a *model* to support distributed data parallel training.\n\n    This is similar to the built-in DistributedDataParallel, but allows\n    additional configuration of the DistributedDataParallel class to\n    use, and also provides easier access to the wrapped model by\n    forwarding requests for missing attributes to the wrapped model.\n\n    Args:\n        args (argparse.Namespace): metaseq args\n        model (BaseModel): model to wrap\n        process_group: the c10d process group to be used for distributed data\n            parallel all-reduction.\n        device: device to move model to\n    \"\"\"\n    assert isinstance(model, nn.Module)\n    if args.ddp_backend in {\"c10d\", \"pytorch_ddp\"}:\n        wrapped_model = DistributedDataParallel(\n            module=model.to(device),\n            device_ids=[args.device_id],\n            output_device=args.device_id,\n            broadcast_buffers=args.broadcast_buffers,\n            bucket_cap_mb=args.bucket_cap_mb,\n            process_group=process_group,\n            find_unused_parameters=args.find_unused_parameters,\n        )\n        # forward missing getattr and state_dict/load_state_dict to orig model\n        wrapped_model = ModuleProxyWrapper(wrapped_model)\n    elif args.ddp_backend == \"fully_sharded\":\n        try:\n            from fairscale.nn.data_parallel import FullyShardedDataParallel as FSDP\n        except ImportError:\n            raise ImportError(\n                \"Cannot find FullyShardedDataParallel. \"\n                \"Please install fairscale with: pip install fairscale\"\n            )\n        assert isinstance(model, FSDP), \"expected model to already be wrapped in FSDP\"\n        wrapped_model = model\n        if args.memory_efficient_fp16:\n            if args.bf16:\n                wrapped_model = wrapped_model.bfloat16()\n            else:\n                wrapped_model = wrapped_model.half()\n        if not args.cpu_offload:\n            wrapped_model = wrapped_model.to(device=device)\n    else:\n        raise ValueError(\"Unknown --ddp-backend: \" + args.ddp_backend)\n\n    return wrapped_model\n",
        "metaseq/models/ema/__init__.py": "# Copyright (c) Facebook, Inc. and its affiliates.\n#\n# This source code is licensed under the MIT license found in the\n# LICENSE file in the root directory of this source tree.\n\nimport importlib\nimport os\n\nfrom .ema import EMA\n\n\ndef build_ema(model, cfg, device):\n    return EMA(model, cfg, device)\n\n\n# automatically import any Python files in the models/ema/ directory\nfor file in sorted(os.listdir(os.path.dirname(__file__))):\n    if file.endswith(\".py\") and not file.startswith(\"_\"):\n        file_name = file[: file.find(\".py\")]\n        importlib.import_module(\"metaseq.models.ema.\" + file_name)\n",
        "metaseq/models/ema/ema.py": "#!/usr/bin/env python3\n\n\"\"\"\nThis module has the EMA class used to store a copy of the exponentially decayed\nmodel params.\n\nTypical usage of EMA class involves initializing an object using an existing\nmodel (random or from a seed model) and setting the config like ema_decay,\nema_start_update which determine how the EMA model is updated. After every\nupdate of the model i.e. at the end of the train_step, the EMA should be updated\nby passing the new model to the EMA.step function. The EMA model state dict\ncan be stored in the extra state under the key of \"ema\" and dumped\ninto a checkpoint and loaded. The EMA object can be passed to tasks\nby setting task.uses_ema property.\nEMA is a smoothed/ensemble model which might have better performance\nwhen used for inference or further fine-tuning. EMA class has a\nreverse function to load the EMA params into a model and use it\nlike a regular model.\n\"\"\"\n\nimport copy\nimport logging\n\nimport torch\nfrom metaseq.distributed import FullyShardedDataParallel\n\nfrom metaseq import checkpoint_utils\n\nlogger = logging.getLogger(__name__)\n\n\nclass EMA(object):\n    \"\"\"Exponential Moving Average of Metaseq Models\n    EMA keeps a copy of the exponentially decayed model params.\n    The set of params should include both gradient-descent and\n    non-gradient descent params, such as batch mean/var and buffers.\n    This is a modified implementation of\n    the open source code in https://github.com/zhawe01/fairseq-gec.git.\n\n    Similar to TF EMA.\n    https://www.tensorflow.org/api_docs/python/tf/train/ExponentialMovingAverage.\n    EMA provides a averaged and smoothed set of model weights, and has been shown to\n    improve vision models. EMA class does all necessary functions to update, reload,\n    or init EMA methods.\n\n    EMA object is initialized from an arbitrary model. By default, it is stored in\n    the same device (unless device specified at initialization) and with the\n    same precision as the model (unless ema_fp32 is True). ema_fp32 is recommended.\n    This stores the EMA parameters in fp32 only for the EMA update step, and\n    is used at the default precision otherwise.\n    EMA is usually enabled using EMAConfig with store_ema=True. Some important\n    parameters to configure EMA are\n    1) ema_decay - The decay of EMA\n    2) ema_update_freq - EMA is updated every this many model updates.\n    3) ema_start_update - Start EMA update after this many model updates [default 0]\n\n    Key methods:\n    1) step - One update of EMA using new model\n    2) restore - Update EMA from a state dict\n    3) reverse - Load EMA into a model\n    4) get_decay, _set_decay - Used to get or set the decay.  Note _set_decay is\n    called from step.\n    5) build_fp32_params - Used to initialize or update the fp32 copy of EMA params.\n    Note this is enabled only when ema_fp32=True\n    \"\"\"\n\n    def __init__(self, model, config, device=None):\n        \"\"\"\n        @param model model to initialize the EMA with\n        @param config EMAConfig object with configuration like\n        ema_decay, ema_update_freq, ema_fp32\n        @param device If provided, copy EMA to this device (e.g. gpu).\n        Otherwise EMA is in the same device as the model.\n        \"\"\"\n\n        self.decay = config.ema_decay\n        if isinstance(model, FullyShardedDataParallel):\n            self.model = model\n            logger.info(\"EMA got FSDP model, assuming assigned model is a \" \"copy\")\n        else:\n            self.model = copy.deepcopy(model)\n        self.model.requires_grad_(False)\n        self.config = config\n        self.fp32_params = {}\n\n        if self.config.ema_seed_model is not None:\n            state = checkpoint_utils.load_ema_from_checkpoint(\n                self.config.ema_seed_model\n            )\n            self.model.load_state_dict(state[\"model\"], strict=True)\n\n        if device is not None:\n            logger.info(f\"Copying EMA model to device {device}\")\n            self.model = self.model.to(device=device)\n\n        if self.config.ema_fp32:\n            self.build_fp32_params()\n\n        self.update_freq_counter = 0\n\n    def get_model(self):\n        return self.model\n\n    def build_fp32_params(self, state_dict=None, device=None):\n        \"\"\"\n        Store a copy of the EMA params in fp32.\n        If state dict is passed, the EMA params is copied from\n        the provided state dict. Otherwise, it is copied from the\n        current EMA model parameters.\n        \"\"\"\n        if not self.config.ema_fp32:\n            raise RuntimeError(\n                \"build_fp32_params should not be called if ema_fp32=False. \"\n                \"Use ema_fp32=True if this is really intended.\"\n            )\n\n        if state_dict is None:\n            state_dict = self.model.state_dict()\n\n        def _to_float(t):\n            return t.float() if torch.is_floating_point(t) else t\n\n        for param_key in state_dict:\n            if param_key in self.fp32_params:\n                self.fp32_params[param_key].copy_(state_dict[param_key])\n            else:\n                self.fp32_params[param_key] = _to_float(state_dict[param_key])\n\n        if device is not None:\n            for param_key in self.fp32_params:\n                self.fp32_params[param_key] = self.fp32_params[param_key].to(device)\n\n    def restore(self, state_dict, build_fp32_params=False):\n        \"\"\"Load data from a model spec into EMA model\"\"\"\n        self.model.load_state_dict(state_dict, strict=False)\n        if build_fp32_params:\n            self.build_fp32_params(state_dict)\n\n    def _set_decay(self, decay):\n        self.decay = decay\n\n    def get_decay(self):\n        return self.decay\n\n    def _step_internal(self, new_model, updates=None):\n        \"\"\"One update of the EMA model based on new model weights\"\"\"\n        decay = self.decay\n\n        ema_state_dict = {}\n        ema_params = (\n            self.fp32_params if self.config.ema_fp32 else self.model.state_dict()\n        )\n        for key, param in new_model.state_dict().items():\n            try:\n                ema_param = ema_params[key]\n            except KeyError:\n                ema_param = (\n                    param.float().clone() if param.ndim == 1 else copy.deepcopy(param)\n                )\n\n            if param.shape != ema_param.shape:\n                raise ValueError(\n                    \"incompatible tensor shapes between model param and ema param\"\n                    + \"{} vs. {}\".format(param.shape, ema_param.shape)\n                )\n            if \"version\" in key:\n                # Do not decay a model.version pytorch param\n                continue\n            ema_param.mul_(decay)\n            ema_param.add_(param.to(dtype=ema_param.dtype), alpha=1 - decay)\n            ema_state_dict[key] = ema_param\n        self.restore(ema_state_dict, build_fp32_params=False)\n\n    def step(self, new_model, updates=None):\n        \"\"\"\n        One update of EMA which is done every self.config.ema_update_freq\n        updates of the model.\n\n        @param updates The current number of model updates done.\n        Decay is set of 0 if model updates < ema_start_update, which means\n        the model will be simply copied over to the EMA.\n        When model updates >= ema_start_updates, then EMA is updated with\n        a decay of self.config.ema_decay.\n        \"\"\"\n        self._set_decay(\n            0\n            if updates is not None and updates < self.config.ema_start_update\n            else self.config.ema_decay\n        )\n        if updates is not None and self.config.ema_update_freq > 1:\n            self.update_freq_counter += 1\n            if self.update_freq_counter >= self.config.ema_update_freq:\n                self._step_internal(new_model, updates)\n                self.update_freq_counter = 0\n        else:\n            self._step_internal(new_model, updates)\n\n    def reverse(self, model):\n        \"\"\"\n        Load the model parameters from EMA model.\n        Useful for inference or fine-tuning from the EMA model.\n        \"\"\"\n        model.load_state_dict(self.model.state_dict(), strict=False)\n        return model\n",
        "metaseq/models/transformer_decoder.py": "# Copyright (c) Meta Platforms, Inc. and affiliates. All Rights Reserved.\n#\n# This source code is licensed under the MIT license found in the\n# LICENSE file in the root directory of this source tree.\n\nimport logging\nimport math\nfrom typing import Any, Dict, List, Optional\n\nimport torch\nimport torch.nn as nn\nfrom torch import Tensor\n\nfrom metaseq import utils\nfrom metaseq.dataclass.constants import UNSPECIFIED_DOC_SEP\nfrom metaseq.distributed import utils as distributed_utils, fsdp_wrap\nfrom metaseq.models import BaseDecoder\nfrom metaseq.modules import (\n    Dropout,\n    LayerNorm,\n    PositionalEmbedding,\n    ModelParallelTransformerDecoderLayer,\n    Linear,\n)\nfrom metaseq.modules.checkpoint_activations import checkpoint_wrapper\nfrom metaseq.modules.megatron.mpu import (\n    LinearWithGradAccumulationAndAsyncCommunication,\n    gather_from_tensor_model_parallel_region,\n    scatter_to_sequence_parallel_region,\n    copy_to_tensor_model_parallel_region,\n)\n\nlogger = logging.getLogger(__name__)\n\n\nDEFAULT_MAX_SOURCE_POSITIONS = 1024\nDEFAULT_MAX_TARGET_POSITIONS = 1024\nDEFAULT_MIN_PARAMS_TO_WRAP = int(1e8)\n\n\nclass TransformerDecoderMultiLayerBlockModule(nn.Module):\n    def __init__(self, layers):\n        super().__init__()\n        self.layers = nn.ModuleList(layers)\n\n    # TODO[susanz]: Return signature seems off. Cleanup?\n    #  fsdp_checkpoint_wrap_layer_frequency always 1 so this path is not called.\n    def forward(self, x, **kwargs):\n        inner_states = []\n        for layer in self.layers:\n            x = layer(x, **kwargs)\n            inner_states.append(x)\n        return x, inner_states\n\n\ndef log_weight_stats(tensor, name):\n    logger.debug(\n        f\"{name}, mean: {tensor.mean():.5f}, std: {tensor.std():.5f}, min: {tensor.min():.5f}, max: {tensor.max():.5f}\"\n    )\n\n\nclass ModelParallelTransformerDecoder(BaseDecoder):\n    \"\"\"\n    Transformer decoder consisting of *args.decoder_layers* layers. Each layer\n    is a :class:`ModelParallelTransformerDecoderLayer`.\n\n    Args:\n        args (argparse.Namespace): parsed command-line arguments\n        dictionary (~metaseq.data.Dictionary): decoding dictionary\n        embed_tokens (torch.nn.Embedding): output embedding\n    \"\"\"\n\n    def __init__(self, args, dictionary, embed_tokens):\n        self.args = args\n        super().__init__(dictionary)\n        self.register_buffer(\"version\", torch.Tensor([3]))\n        self._future_mask = torch.empty(0)\n\n        self.dropout_module = Dropout(args.dropout, module_name=self.__class__.__name__)\n\n        if getattr(args, \"no_emb_dropout\", False):\n            self.dropout_module = None\n\n        self.share_input_output_embed = args.share_decoder_input_output_embed\n        self.embed_dim = args.decoder_embed_dim\n        self.padding_idx = embed_tokens.padding_idx\n        self.max_target_positions = args.max_target_positions\n        self.embed_tokens = embed_tokens\n        self.embed_scale = 1.0 if args.no_scale_embedding else math.sqrt(self.embed_dim)\n\n        initialize_params_on_gpu = getattr(\n            args, \"tensor_parallel_init_model_on_gpu\", False\n        )\n        device = torch.cuda.current_device() if initialize_params_on_gpu else None\n        dtype = utils.get_model_init_dtype(args)\n\n        self.use_alibi: bool = getattr(args, \"alibi\", False)\n        self.self_attn_doc_sep: int = getattr(\n            args, \"self_attn_doc_sep\", UNSPECIFIED_DOC_SEP\n        )\n\n        self.embed_positions = (\n            PositionalEmbedding(\n                self.max_target_positions,\n                self.embed_dim,\n                self.padding_idx,\n                learned=args.decoder_learned_pos,\n                learned_sinusoidal=getattr(args, \"decoder_learned_sinusoidal\", False),\n                full_megatron_init=getattr(args, \"full_megatron_init\", False),\n                pos_init_scalar=getattr(args, \"pos_init_scalar\", 1.0),\n                megatron_init_sigma=getattr(args, \"megatron_init_sigma\", 0.006),\n                truncate_init=getattr(args, \"truncate_init\", False),\n            )\n            if args.decoder_learned_pos and not self.use_alibi\n            else None\n        )\n        self.embed_positions.to(device).to(dtype)\n\n        self.layers = nn.ModuleList([])\n        layers = []\n        for i in range(args.decoder_layers):\n            layers.append(self.build_decoder_layer(args))\n\n        if getattr(self.args, \"fsdp_checkpoint_wrap_layer_frequency\", 1) > 1:\n            assert (\n                len(layers) % self.args.fsdp_checkpoint_wrap_layer_frequency == 0\n            ), \"num layers should be divisible by checkpoint wrap frequency\"\n            for i in range(\n                0, len(layers), self.args.fsdp_checkpoint_wrap_layer_frequency\n            ):\n                layer_block = TransformerDecoderMultiLayerBlockModule(\n                    layers[i : i + self.args.fsdp_checkpoint_wrap_layer_frequency]\n                )\n                checkpoint = getattr(args, \"checkpoint_activations\", False)\n                if checkpoint:\n                    offload_to_cpu = getattr(args, \"offload_activations\", False)\n                    distribute_checkpointed_activations = getattr(\n                        args, \"distribute_checkpointed_activations\", False\n                    )\n                    layer_block = checkpoint_wrapper(\n                        layer_block,\n                        offload_to_cpu=offload_to_cpu,\n                        distribute_checkpointed_activations=distribute_checkpointed_activations,\n                    )\n                # if we are checkpointing, enforce that FSDP always wraps the\n                # checkpointed layer, regardless of layer size\n                min_params_to_wrap = (\n                    getattr(args, \"min_params_to_wrap\", DEFAULT_MIN_PARAMS_TO_WRAP)\n                    if not checkpoint\n                    else 0\n                )\n                layer_block = fsdp_wrap(\n                    layer_block,\n                    min_num_params=min_params_to_wrap,\n                    process_group=distributed_utils.get_data_parallel_group(),\n                )\n                self.layers.append(layer_block)\n        else:\n            self.layers = nn.ModuleList(layers)\n\n        log_weight_stats(self.embed_tokens.weight, \"embed tokens\")\n\n        self.num_layers = len(self.layers)\n\n        self.layer_norm = LayerNorm(\n            self.embed_dim,\n            elementwise_affine=not getattr(args, \"disable_affine_ln\", False),\n        )\n        self.layer_norm.to(device).to(dtype)\n\n        self.output_projection = None\n        if self.share_input_output_embed:\n            self.output_projection = Linear(\n                self.embed_tokens.weight.shape[1],\n                self.embed_tokens.weight.shape[0],\n                bias=False,\n                initialize_params_on_gpu=initialize_params_on_gpu,\n                dtype=dtype,\n            )\n            self.output_projection.weight = self.embed_tokens.weight\n        else:\n            self.output_projection = Linear(\n                self.embed_dim,\n                len(dictionary),\n                bias=False,\n                initialize_params_on_gpu=initialize_params_on_gpu,\n                dtype=dtype,\n            )\n            nn.init.normal_(\n                self.output_projection.weight, mean=0, std=self.embed_dim**-0.5\n            )\n\n        if self.use_alibi:\n            self.alibi = self._build_alibi_tensor(\n                self.max_positions(), args.decoder_attention_heads\n            )\n\n    @staticmethod\n    def _build_alibi_tensor(max_seq_len: int, n_attention_heads: int):\n        \"\"\"Returns tensor shaped (n_head, 1, max_seq_len)\"\"\"\n\n        def get_slopes(n):\n            # In the paper, we only train models that have 2^a heads for some a. This function has some good\n            # properties that only occur when the input is a power of 2. To maintain that even when the number of\n            # heads is not a power of 2, we use this workaround.\n            def get_slopes_power_of_2(n):\n                start = 2 ** (-(2 ** -(math.log2(n) - 3)))\n                ratio = start\n                return [start * ratio**i for i in range(n)]\n\n            if math.log2(n).is_integer():\n                return get_slopes_power_of_2(n)\n            else:\n                closest_power_of_2 = 2 ** math.floor(math.log2(n))\n                return (\n                    get_slopes_power_of_2(closest_power_of_2)\n                    + get_slopes(2 * closest_power_of_2)[0::2][: n - closest_power_of_2]\n                )\n\n        slopes = torch.Tensor(get_slopes(n_attention_heads))\n        # In the next line, the part after the * is what constructs the diagonal matrix (right matrix in Figure 3 in\n        # the paper).\n        # It doesn't exactly print out the same matrix as we have in Figure 3, but one where all rows are identical.\n        # This works because the softmax operation is invariant to translation, and our bias functions are always\n        # linear.\n        alibi = slopes.unsqueeze(1).unsqueeze(1) * torch.arange(max_seq_len).unsqueeze(\n            0\n        ).unsqueeze(0).expand(n_attention_heads, -1, -1)\n        alibi = alibi.view(n_attention_heads, 1, max_seq_len)\n        return alibi\n\n    def build_decoder_layer(self, args):\n        layer = ModelParallelTransformerDecoderLayer(args)\n        for name, param in layer.named_parameters():\n            log_weight_stats(param, name)\n        if getattr(args, \"fsdp_checkpoint_wrap_layer_frequency\", 1) > 1:\n            return layer\n        checkpoint = getattr(args, \"checkpoint_activations\", False)\n        if checkpoint:\n            offload_to_cpu = getattr(args, \"offload_activations\", False)\n            distribute_checkpointed_activations = getattr(\n                args, \"distribute_checkpointed_activations\", False\n            )\n            layer = checkpoint_wrapper(\n                layer,\n                offload_to_cpu=offload_to_cpu,\n                distribute_checkpointed_activations=distribute_checkpointed_activations,\n            )\n        # if we are checkpointing, enforce that FSDP always wraps the\n        # checkpointed layer, regardless of layer size\n        min_params_to_wrap = (\n            getattr(args, \"min_params_to_wrap\", DEFAULT_MIN_PARAMS_TO_WRAP)\n            if not checkpoint\n            else 0\n        )\n        layer = fsdp_wrap(\n            layer,\n            min_num_params=min_params_to_wrap,\n            process_group=distributed_utils.get_data_parallel_group(),\n        )\n        return layer\n\n    def forward_embedding(\n        self,\n        tokens,\n        token_embedding: Optional[torch.Tensor] = None,\n        incremental_state: Optional[Dict[str, Dict[str, Optional[Tensor]]]] = None,\n    ):\n        # embed tokens and positions\n        if self.self_attn_doc_sep != UNSPECIFIED_DOC_SEP:\n            # create own positions when self_attn_doc_sep is set\n            # We are essentially resetting positions based on document separator tokens.\n            # For instance, if the doc separator is 2, and the tokens are\n            # 143 345 2 5435 2\n            # The default positions would be\n            # 2 3 4 5 6\n            # But with document level attention, we would like to reset positions\n            # as well on sentence boundaries. So, the positions become\n            # 2 3 4 2 3\n\n            mask = tokens.ne(self.padding_idx).int()\n            mask_with_reset = tokens.ne(self.padding_idx).int()\n            mask_with_reset[:, :] = 1\n            doc_id_indices = (tokens == self.self_attn_doc_sep).nonzero().tolist()\n\n            # Based on the location of document seperator token (batch_doc_indices), we would reset\n            # positional embeddings. The document seperator token marks the end of the preceding\n            # document.\n            for batch_idx in range(tokens.size(0)):\n                # The self_attn_doc_sep token marks the end of the previous document. Therefore,\n                # we need to add 1 to the indices to mark the start of documents.\n                batch_doc_indices = [\n                    index[1] + 1\n                    for index in doc_id_indices\n                    # index[1] + 1 < tokens.size(1) to prevent overflow\n                    if index[0] == batch_idx and index[1] + 1 < tokens.size(1)\n                ]\n                batch_doc_indices.sort()\n                for k, doc_sep_idx in enumerate(batch_doc_indices):\n                    if k == 0:\n                        mask_with_reset[batch_idx, doc_sep_idx] = -doc_sep_idx + 1\n                    else:\n                        mask_with_reset[batch_idx, doc_sep_idx] = (\n                            batch_doc_indices[k - 1] - doc_sep_idx + 1\n                        )\n            positions = (\n                torch.cumsum(mask_with_reset, dim=1).type_as(mask) * mask\n            ).long() + self.padding_idx\n\n            # Since positions are pre-computed, padding_idx should not be set.\n            # Ref metaseq/metaseq/modules/learned_positional_embedding.py\n            if self.embed_positions is not None:\n                self.embed_positions.padding_idx = None\n        else:\n            positions = None\n\n        if self.embed_positions is not None:\n            positions = self.embed_positions(\n                tokens, incremental_state=incremental_state, positions=positions\n            )\n\n        # see BaseDecoder for important information about\n        # incremental state\n        if incremental_state:\n            tokens = tokens[:, -1:]\n            if positions is not None:\n                positions = positions[:, -1:]\n\n        if token_embedding is None:\n            token_embedding = self.embed_tokens(tokens)\n\n        x = embed = self.embed_scale * token_embedding\n\n        if positions is not None:\n            x += positions\n\n        if self.dropout_module is not None:\n            x = self.dropout_module(x)\n\n        # Returning in T x B x C format as that makes integrating sequence parallelism easier.\n        x = x.transpose(0, 1).contiguous()\n\n        is_sequence_parallel = getattr(self.args, \"sequence_parallel\", False)\n        if is_sequence_parallel:\n            x = scatter_to_sequence_parallel_region(x)\n        return x, embed, positions\n\n    def extract_features(\n        self,\n        prev_output_tokens,\n        incremental_state: Optional[Dict[str, Dict[str, Optional[Tensor]]]] = None,\n        token_embeddings: Optional[torch.Tensor] = None,\n        self_attn_padding_mask: Optional[Tensor] = None,\n    ):\n        # compute self-attention padding mask (involves device-to-host transfer,\n        # so put it at the top of the forward)\n        if (\n            self_attn_padding_mask is None\n            and prev_output_tokens.eq(self.padding_idx).any()\n        ):\n            self_attn_padding_mask = prev_output_tokens.eq(self.padding_idx)\n\n        # embed tokens and positions\n        # x is T x B x C\n        x, tok, pos = self.forward_embedding(\n            prev_output_tokens, token_embeddings, incremental_state\n        )\n\n        # see BaseDecoder for important information about\n        # incremental state. Note that it may be an empty dictionary.\n        if not incremental_state:\n            self_attn_mask = self.buffered_future_mask(x, prev_output_tokens)\n        else:\n            self_attn_mask = None\n\n        # decoder layers\n        # store other representations for instrumentation in VocabParallelCrossEntCrit\n        # Note: we are only storing the embeddings output and output of final transformer block\n        # instead of all inner representations, as thats the only thing being logged and storing\n        # all intermediate representation causes OOM for large models during validation.\n        inner_states: List[Optional[Tensor]] = [{\"tok\": tok, \"pos\": pos, \"emb\": x}]\n        for idx, layer in enumerate(self.layers):\n            x = layer(\n                x,\n                incremental_state=incremental_state,\n                self_attn_mask=self_attn_mask,\n                self_attn_padding_mask=self_attn_padding_mask,\n                recompute_fc1=(idx < getattr(self.args, \"recompute_fc1_num_layers\", 0)),\n            )\n        inner_states.append(x)\n\n        if self.layer_norm is not None:\n            x = self.layer_norm(x)\n\n        # Returned x is T x B x C here, as sequence_parallel requires T to be first dim\n        return x, {\"inner_states\": inner_states}\n\n    def output_layer(self, features, **kwargs):\n        \"\"\"Project features to the vocabulary size.\"\"\"\n        if not self.share_input_output_embed:\n            # TODO[Susan]: Remove this & make compatible.\n            raise NotImplementedError(\n                \"Model parallel training currently requires --share-decoder-input-output-embed\"\n            )\n\n        is_sequence_parallel = getattr(self.args, \"sequence_parallel\", False)\n        if is_sequence_parallel:\n            input_parallel = features\n        else:\n            input_parallel = copy_to_tensor_model_parallel_region(features)\n\n        # project back to size of vocabulary\n        x = LinearWithGradAccumulationAndAsyncCommunication.apply(\n            input_parallel,\n            self.output_projection.weight,\n            None,\n            False,  # gradient_accumulation_fusion\n            False,  # async_grad_allreduce\n            is_sequence_parallel,  # sequence_parallel\n        )\n        # Gather output if model is in inference mode (i.e. eval_lm or generation) cause both are not yet\n        # compatible with vocab parallel embeddings\n        if getattr(self.args, \"criterion\") != \"vocab_parallel_cross_entropy\" or getattr(\n            self, \"inference\", False\n        ):\n            x = gather_from_tensor_model_parallel_region(x).contiguous()\n\n        return x\n\n    def max_positions(self):\n        \"\"\"Maximum output length supported by the decoder.\"\"\"\n        if self.embed_positions is None:\n            return self.max_target_positions\n        return min(self.max_target_positions, self.embed_positions.max_positions)\n\n    def buffered_future_mask(self, tensor, input_tokens=None):\n        cur_seq_len, batch_size = tensor.size(0), tensor.size(1)\n        max_seq_len = self.max_positions()\n        need_to_make_new_mask = (\n            self._future_mask.size(0) == 0\n            or (not self._future_mask.device == tensor.device)\n            or self._future_mask.size(1) < max_seq_len\n            or (\n                self.use_alibi\n                and self._future_mask.size(0)\n                != (batch_size * self.args.decoder_attention_heads)\n            )\n            or (self.self_attn_doc_sep != UNSPECIFIED_DOC_SEP)\n        )\n\n        # self._future_mask.device != tensor.device is not working in TorchScript. This is a workaround.\n        if need_to_make_new_mask:\n            self._future_mask = torch.triu(\n                utils.fill_with_neg_inf(\n                    torch.zeros([max_seq_len, max_seq_len], device=tensor.device)\n                ),\n                1,\n            )\n            if self.self_attn_doc_sep != UNSPECIFIED_DOC_SEP:\n                # Code to accomodate dynamic attention when document seperator is used\n                assert input_tokens is not None\n                self._future_mask = self._future_mask[:cur_seq_len, :cur_seq_len]\n                self._future_mask = self._future_mask.unsqueeze(0).repeat(\n                    batch_size, 1, 1\n                )\n                doc_id_indices = (\n                    (input_tokens == self.self_attn_doc_sep).nonzero().tolist()\n                )\n                for indices in doc_id_indices:\n                    self._future_mask[\n                        indices[0], indices[1] + 1 :, : indices[1] + 1\n                    ] = float(\"-inf\")\n\n            if self.use_alibi:\n                alibi = self.alibi.repeat(batch_size, 1, 1)  # batch_size, 1, 1\n                self._future_mask = self._future_mask.unsqueeze(0) + alibi\n\n        self._future_mask = self._future_mask.to(tensor)\n        if self.use_alibi:\n            return self._future_mask[\n                : batch_size * self.args.decoder_attention_heads,\n                :cur_seq_len,\n                :cur_seq_len,\n            ]\n        elif self.self_attn_doc_sep != UNSPECIFIED_DOC_SEP:\n            return self._future_mask\n        else:\n            return self._future_mask[:cur_seq_len, :cur_seq_len]\n\n    # This hook used as proxy for tracking state if model is in eval or generation mode.\n    def make_generation_fast_(self, **unused):\n        self.inference = True\n\n    def forward(\n        self,\n        prev_output_tokens,\n        incremental_state: Optional[Dict[str, Dict[str, Optional[Tensor]]]] = None,\n        features_only: bool = False,\n        src_lengths: Optional[Any] = None,\n        token_embeddings: Optional[torch.Tensor] = None,\n        self_attn_padding_mask: Optional[Tensor] = None,\n    ):\n        \"\"\"\n        Includes several features from \"Jointly Learning to Align and\n        Translate with Transformer Models\" (Garg et al., EMNLP 2019).\n\n        Args:\n            prev_output_tokens (LongTensor): previous decoder outputs of shape\n                `(batch, tgt_len)`, for teacher forcing\n            incremental_state (dict): dictionary used for storing state during\n                :ref:`Incremental decoding`\n            features_only (bool, optional): only return features without\n                applying output layer (default: False).\n            token_embeddings (torch.Tensor, optional): precomputed embeddings\n                default `None` will recompute embeddings\n            self_attn_padding_mask (torch.Tensor, optional): precomputed padding\n                mask for self-attention (default None will recompute mask)\n\n        Returns:\n            tuple:\n                - the decoder's output of shape `(batch, tgt_len, vocab)`\n                - a dictionary with any model-specific outputs\n        \"\"\"\n\n        # see BaseDecoder for important information about incremental state\n        x, extra = self.extract_features(\n            prev_output_tokens,\n            incremental_state=incremental_state,\n            token_embeddings=token_embeddings,\n            self_attn_padding_mask=self_attn_padding_mask,\n        )\n        if not features_only:\n            x = self.output_layer(x)\n\n        # Transposing back to B x T x C, so that the interface stays the same.\n        x = x.transpose(0, 1).contiguous()\n        return x, extra\n",
        "metaseq/models/transformer_lm.py": "# Copyright (c) Meta Platforms, Inc. and affiliates. All Rights Reserved.\n#\n# This source code is licensed under the MIT license found in the\n# LICENSE file in the root directory of this source tree.\n\nimport logging\nfrom dataclasses import dataclass, field\nfrom typing import Optional\n\nimport torch\nimport torch.nn as nn\nfrom omegaconf import II\n\nfrom metaseq.dataclass import ChoiceEnum, MetaseqDataclass\nfrom metaseq.dataclass.constants import ATTN_CHOICES, UNSPECIFIED_DOC_SEP\nfrom metaseq.models import (\n    BaseModel,\n    register_model,\n    register_model_architecture,\n)\nfrom metaseq.models.transformer_decoder import (\n    DEFAULT_MIN_PARAMS_TO_WRAP,\n    ModelParallelTransformerDecoder,\n)\nfrom metaseq.modules.activation_functions import get_available_activation_fns\nfrom metaseq.modules.megatron.mpu import VocabParallelEmbedding\n\nDEFAULT_MAX_TARGET_POSITIONS = 1024\nlogger = logging.getLogger(__name__)\n\n\n@dataclass\nclass TransformerLanguageModelConfig(MetaseqDataclass):\n    activation_fn: ChoiceEnum(get_available_activation_fns()) = field(\n        default=\"relu\", metadata={\"help\": \"activation function to use\"}\n    )\n    dropout: float = field(default=0.1, metadata={\"help\": \"dropout probability\"})\n    attention_dropout: float = field(\n        default=0.0, metadata={\"help\": \"dropout probability for attention weights\"}\n    )\n    decoder_embed_dim: int = field(\n        default=512, metadata={\"help\": \"decoder embedding dimension\"}\n    )\n    decoder_ffn_embed_dim: int = field(\n        default=2048, metadata={\"help\": \"decoder embedding dimension for FFN\"}\n    )\n    decoder_layers: int = field(default=6, metadata={\"help\": \"num decoder layers\"})\n    decoder_attention_heads: int = field(\n        default=8, metadata={\"help\": \"num decoder attention heads\"}\n    )\n    share_decoder_input_output_embed: bool = field(\n        default=False, metadata={\"help\": \"share decoder input and output embeddings\"}\n    )\n    decoder_learned_pos: bool = field(\n        default=False,\n        metadata={\"help\": \"use learned positional embeddings in the decoder\"},\n    )\n    decoder_learned_sinusoidal: bool = field(\n        default=False,\n        metadata={\n            \"help\": \"use learned positional embeddings init with sinusoidal in the decoder\"\n        },\n    )\n    no_scale_embedding: bool = field(\n        default=False, metadata={\"help\": \"if True, dont scale embeddings\"}\n    )\n    checkpoint_activations: bool = field(\n        default=False, metadata={\"help\": \"checkpoint activations at each layer\"}\n    )\n    offload_activations: bool = field(\n        default=False,\n        metadata={\"help\": \"move checkpointed activations to CPU after they are used.\"},\n    )\n    # config for Fully Sharded Data Parallel (FSDP) training\n    min_params_to_wrap: int = field(\n        default=DEFAULT_MIN_PARAMS_TO_WRAP,\n        metadata={\n            \"help\": (\n                \"minimum number of params for a layer to be wrapped with FSDP() when \"\n                \"training with --ddp-backend=fully_sharded. Smaller values will \"\n                \"improve memory efficiency, but may make torch.distributed \"\n                \"communication less efficient due to smaller input sizes. This option \"\n                \"is set to 0 (i.e., always wrap) when --checkpoint-activations or \"\n                \"--offload-activations are passed.\"\n            )\n        },\n    )\n    # ALiBi\n    alibi: bool = field(\n        default=False,\n        metadata={\n            \"help\": \"use the ALiBi position method instead of regular position embeddings\"\n        },\n    )\n    # Dynamic Attention\n    self_attn_doc_sep: int = field(\n        default=UNSPECIFIED_DOC_SEP,\n        metadata={\n            \"help\": \"use dynamic self attention masking when document separator ID is specified\"\n        },\n    )\n    fsdp_checkpoint_wrap_layer_frequency: int = field(\n        default=1,\n        metadata={\n            \"help\": \"group transformer blocks and wrap the group in checkpoint and FSDP wrapper together\"\n        },\n    )\n    distribute_checkpointed_activations: bool = field(\n        default=False,\n        metadata={\n            \"help\": \"distribute offloaded checkpoints to tensor parallel gpus. \"\n            \"It adds extra within node all_reduce but reduces checkpointed activations significantly,\"\n            \"so a good way to trade speed for gpu memory.\"\n        },\n    )\n    tensor_parallel_init_model_on_gpu: bool = field(\n        default=False,\n        metadata={\n            \"help\": \"initialize model directly on gpu and possibly fp16 for tensor parallel, shoudl be faster to init model.\"\n        },\n    )\n    full_megatron_init: bool = field(\n        default=False,\n        metadata={\"help\": \"Exact same init as Megatron\"},\n    )\n    full_megatron_init_scalar: float = field(\n        default=1.0,\n        metadata={\n            \"help\": \"Factor to scale sigma by for the second layer in FFN and out_proj of MHA\"\n        },\n    )\n    pos_init_scalar: float = field(\n        default=1.0,\n        metadata={\"help\": \"Factor to scale positional embedding init by.\"},\n    )\n    truncate_init: bool = field(\n        default=False,\n        metadata={\"help\": \"Truncate gaussian init to +/- 3 stddevs\"},\n    )\n    megatron_init_sigma: float = field(\n        default=0.006,\n        metadata={\"help\": \"Sigma for megatron initialization\"},\n    )\n    no_emb_dropout: Optional[bool] = field(\n        default=False, metadata={\"help\": \"Avoid emb dropout for decoder\"}\n    )\n    disable_bias: Optional[bool] = field(\n        default=False,\n        metadata={\n            \"help\": \"Remove biases from all matrix projection, similar to PaLM paper,\"\n            \" note this doesn't remove bias from layernorm\"\n        },\n    )\n    disable_affine_ln: Optional[bool] = field(\n        default=False, metadata={\"help\": \"disable weight and bias of layer norm\"}\n    )\n    attn_variant: ATTN_CHOICES = field(\n        default=\"default\", metadata={\"help\": \"variant to use for attention\"}\n    )\n    xf_attn_op: str = field(\n        default=\"None\",\n        metadata={\n            \"help\": \"which memory efficient attention operation to use from xFormers.\"\n        },\n    )\n    recompute_fc1_num_layers: Optional[int] = field(\n        default=0,\n        metadata={\n            \"help\": \"Num layers for which to recompute FC1 in backwards, \"\n            \"only applicable when --sequence-parallel option is set\"\n        },\n    )\n    # options from other parts of the config\n    add_bos_token: bool = II(\"task.add_bos_token\")\n    tokens_per_sample: int = II(\"task.tokens_per_sample\")\n    max_target_positions: Optional[int] = II(\"task.max_target_positions\")\n    memory_efficient_fp16: bool = II(\"common.memory_efficient_fp16\")\n    fp16: bool = II(\"common.fp16\")\n    fp16_no_flatten_grads: bool = II(\"common.fp16_no_flatten_grads\")\n    ddp_backend: str = II(\"distributed_training.ddp_backend\")\n    world_size: int = II(\"distributed_training.distributed_world_size\")\n    distributed_rank: int = II(\"distributed_training.distributed_rank\")\n    batch_size: Optional[int] = II(\"dataset.batch_size\")\n    batch_size_valid: Optional[int] = II(\"dataset.batch_size_valid\")\n    model_parallel_size: int = II(\"common.model_parallel_size\")\n\n\n@register_model(\"transformer_lm\", dataclass=TransformerLanguageModelConfig)\nclass TransformerLanguageModel(BaseModel):\n    def __init__(self, decoder):\n        super().__init__(decoder)\n\n\n@register_model(\"model_parallel_transformer_lm\")\nclass ModelParallelTransformerLanguageModel(BaseModel):\n    @classmethod\n    def build_model(cls, args, task):\n        \"\"\"Build a new model instance.\"\"\"\n        # make sure all arguments are present in older models\n        base_lm_architecture(args)\n\n        task.source_dictionary.pad_to_multiple_(8)\n        task.target_dictionary.pad_to_multiple_(8)\n\n        # task.source_dictionary.pad_to_multiple_(args.model_parallel_size * 8)\n        # task.target_dictionary.pad_to_multiple_(args.model_parallel_size * 8)\n\n        if getattr(args, \"max_target_positions\", None) is None:\n            args.max_target_positions = getattr(\n                args, \"tokens_per_sample\", DEFAULT_MAX_TARGET_POSITIONS\n            )\n\n        embed_tokens = cls.build_embedding(\n            args, task.source_dictionary, args.decoder_embed_dim\n        )\n        assert getattr(\n            args, \"use_sharded_state\", False\n        ), \"Use sharded state must be True for tensor parallel, otherwise model saving and loaded might be broken\"\n\n        if getattr(args, \"sequence_parallel\", False):\n            assert (\n                getattr(args, \"model_parallel_size\", 1) > 1\n            ), \"--sequence-parallel only works when --model-parallel-size is greater than 1\"\n            assert (\n                getattr(args, \"dropout\", 0.0) == 0.0\n            ), \"havent yet tested if rng states are correct for dropout with seq_parallel\"\n            assert (\n                getattr(args, \"activation_fn\", \"gelu\") == \"gelu\"\n                or getattr(args, \"activation_fn\", \"gelu\") == \"relu\"\n            ), \"For now only supports gelu and relu\"\n            assert not getattr(\n                args, \"checkpoint_activations\", False\n            ), \"Cannot set --checkpoint-activations with sequence parallel.\"\n            assert not getattr(\n                args, \"distribute_checkpointed_activations\", False\n            ), \"Cannot set --distribute-checkpointed-activations with sequence parallel.\"\n\n        decoder = ModelParallelTransformerDecoder(\n            args,\n            task.target_dictionary,\n            embed_tokens,\n        )\n        return cls(decoder)\n\n    @staticmethod\n    def add_args(parser):\n        TransformerLanguageModel.add_args(parser)\n\n    @classmethod\n    def build_embedding(cls, args, dictionary, embed_dim, path=None):\n        def _vocab_init(tensor, **kwargs):\n            std = embed_dim**-0.5\n            if getattr(args, \"truncate_init\", False):\n                nn.init.trunc_normal_(tensor, mean=0, std=std, a=-3 * std, b=3 * std)\n            else:\n                nn.init.normal_(tensor, mean=0, std=std)\n            nn.init.constant_(tensor[1], 0)\n\n        def _vocab_init_megatron(tensor, **kwargs):\n            std = getattr(args, \"megatron_init_sigma\", 0.006)\n            if getattr(args, \"truncate_init\", False):\n                nn.init.trunc_normal_(tensor, mean=0, std=std, a=-3 * std, b=3 * std)\n            else:\n                nn.init.normal_(tensor, mean=0, std=std)\n            nn.init.constant_(tensor[1], 0)\n\n        if getattr(args, \"memory_efficient_fp16\", False):\n            dtype = torch.bfloat16 if getattr(args, \"bf16\", False) else torch.half\n        else:\n            dtype = torch.float32\n\n        embed_tokens = VocabParallelEmbedding(\n            len(dictionary),\n            embed_dim,\n            dictionary.pad(),\n            init_method=_vocab_init_megatron\n            if getattr(args, \"full_megatron_init\", False)\n            else _vocab_init,\n            use_cpu_initialization=not getattr(\n                args, \"tensor_parallel_init_model_on_gpu\", False\n            ),\n            dtype=dtype,\n        )\n        return embed_tokens\n\n\ndef base_lm_architecture(args):\n    args.activation_fn = getattr(args, \"activation_fn\", \"relu\")\n    args.dropout = getattr(args, \"dropout\", 0.1)\n    args.attention_dropout = getattr(args, \"attention_dropout\", 0.0)\n    args.decoder_embed_dim = getattr(args, \"decoder_embed_dim\", 512)\n    args.decoder_ffn_embed_dim = getattr(args, \"decoder_ffn_embed_dim\", 2048)\n    args.decoder_layers = getattr(args, \"decoder_layers\", 6)\n    args.decoder_attention_heads = getattr(args, \"decoder_attention_heads\", 8)\n    args.share_decoder_input_output_embed = getattr(\n        args, \"share_decoder_input_output_embed\", False\n    )\n    args.decoder_learned_pos = getattr(args, \"decoder_learned_pos\", False)\n    args.decoder_learned_sinusoidal = getattr(args, \"decoder_learned_sinusoidal\", False)\n    args.no_scale_embedding = getattr(args, \"no_scale_embedding\", False)\n    args.add_bos_token = getattr(args, \"add_bos_token\", False)\n\n\n@register_model_architecture(\"model_parallel_transformer_lm\", \"transformer_lm_megatron\")\ndef transformer_lm_megatron(args):\n    args.decoder_embed_dim = getattr(args, \"decoder_embed_dim\", 3072)\n    args.decoder_ffn_embed_dim = getattr(args, \"decoder_ffn_embed_dim\", 3072 * 4)\n    args.decoder_layers = getattr(args, \"decoder_layers\", 72)\n    args.decoder_attention_heads = getattr(args, \"decoder_attention_heads\", 32)\n    args.dropout = getattr(args, \"dropout\", 0.1)\n    args.attention_dropout = getattr(args, \"attention_dropout\", 0.1)\n    args.activation_fn = getattr(args, \"activation_fn\", \"gelu\")\n    base_lm_architecture(args)\n\n\n@register_model_architecture(\"model_parallel_transformer_lm\", \"transformer_lm_gpt\")\ndef transformer_lm_gpt(args):\n    args.decoder_embed_dim = getattr(args, \"decoder_embed_dim\", 768)\n    args.decoder_ffn_embed_dim = getattr(args, \"decoder_ffn_embed_dim\", 3072)\n    args.decoder_layers = getattr(args, \"decoder_layers\", 12)\n    args.decoder_attention_heads = getattr(args, \"decoder_attention_heads\", 12)\n    args.dropout = getattr(args, \"dropout\", 0.1)\n    args.attention_dropout = getattr(args, \"attention_dropout\", 0.1)\n    args.activation_fn = getattr(args, \"activation_fn\", \"gelu\")\n    base_lm_architecture(args)\n\n\n@register_model_architecture(\n    \"model_parallel_transformer_lm\", \"transformer_lm_gpt2_tiny\"\n)\ndef transformer_lm_gpt2_tiny(args):\n    args.decoder_embed_dim = getattr(args, \"decoder_embed_dim\", 64)\n    args.decoder_ffn_embed_dim = getattr(args, \"decoder_ffn_embed_dim\", 64)\n    args.decoder_layers = getattr(args, \"decoder_layers\", 2)\n    args.decoder_attention_heads = getattr(args, \"decoder_attention_heads\", 1)\n    args.dropout = getattr(args, \"dropout\", 0.1)\n    args.attention_dropout = getattr(args, \"attention_dropout\", 0.1)\n    args.activation_fn = getattr(args, \"activation_fn\", \"gelu\")\n    base_lm_architecture(args)\n",
        "metaseq/modules/__init__.py": "# Copyright (c) Meta Platforms, Inc. and affiliates. All Rights Reserved.\n#\n# This source code is licensed under the MIT license found in the\n# LICENSE file in the root directory of this source tree.\n\"\"\"isort:skip_file\"\"\"\n\nfrom .adaptive_softmax import AdaptiveSoftmax\nfrom .dropout import Dropout\nfrom .activation_functions import ActivationFn, gelu\nfrom .layer_norm import LayerNorm, LayerNormFp32\nfrom .group_norm_fp32 import GroupNormFp32\nfrom .learned_positional_embedding import LearnedPositionalEmbedding\nfrom .multihead_attention import ModelParallelMultiheadAttention\nfrom .positional_embedding import PositionalEmbedding\nfrom .sinusoidal_positional_embedding import SinusoidalPositionalEmbedding\nfrom .linear import Linear\nfrom .feedforward import FeedForward\nfrom .transformer_decoder_layer import (\n    ModelParallelTransformerDecoderLayer,\n)\nfrom .sequence_parallel_transformer_layer import SequeuceParallelTransformerBlock\n\n__all__ = [\n    \"ActivationFn\",\n    \"AdaptiveSoftmax\",\n    \"Dropout\",\n    \"gelu\",\n    \"LayerNorm\",\n    \"LayerNormFp32\",\n    \"GroupNormFp32\",\n    \"LearnedPositionalEmbedding\",\n    \"ModelParallelMultiheadAttention\",\n    \"PositionalEmbedding\",\n    \"SinusoidalPositionalEmbedding\",\n    \"Linear\",\n    \"FeedForward\",\n    \"ModelParallelTransformerDecoderLayer\",\n    \"SequeuceParallelTransformerBlock\",\n]\n",
        "metaseq/modules/activation_functions.py": "# Copyright (c) Meta Platforms, Inc. and affiliates. All Rights Reserved.\n#\n# This source code is licensed under the MIT license found in the\n# LICENSE file in the root directory of this source tree.\n\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\n\nfrom typing import Callable, List\n\n\n@torch.jit.script\ndef relu_squared(x: torch.Tensor):\n    return F.relu(x).pow(2)\n\n\n@torch.jit.script\ndef gelu(x):\n    return x * 0.5 * (1.0 + torch.tanh(0.79788456 * x * (1 + 0.044715 * x * x)))\n\n\n@torch.jit.script\ndef gelu_back(g, x):\n    tanh_out = torch.tanh(0.79788456 * x * (1 + 0.044715 * x * x))\n    # sqrt(2/pi) * 3 * 0.044715 -> 0.1070322243\n    ff = 0.5 * x * (\n        (1 - tanh_out * tanh_out) * (0.79788456 + 0.1070322243 * x * x)\n    ) + 0.5 * (1 + tanh_out)\n    return ff * g\n\n\n@torch.jit.script\ndef relu(x):\n    return F.relu(x)\n\n\n@torch.jit.script\ndef relu_back(g, x):\n    return g.masked_fill_(x <= 0, 0)\n\n\n@torch.jit.script\ndef swiglu(x: torch.Tensor, gate: torch.Tensor):\n    return F.silu(x) * gate\n\n\n@torch.jit.script\ndef geglu(x: torch.Tensor, gate: torch.Tensor):\n    return gelu(x) * gate\n\n\ndef get_available_activation_fns() -> List:\n    return [\n        \"relu\",\n        \"relu_squared\",\n        \"gelu\",\n        \"tanh\",\n        \"linear\",\n        \"swiglu\",\n        \"geglu\",\n    ]\n\n\nclass ActivationFn(nn.Module):\n    def __init__(self, name, fc1_builder, embed_dim, ffn_dim, **fc1_kwargs):\n        super().__init__()\n        self.fn = self.__get_fn(name)\n        self.gate = None\n        if self.fn in self.__get_gated_fns():\n            self.gate = fc1_builder(embed_dim, ffn_dim, **fc1_kwargs)\n\n    def forward(self, fc1_in, fc1_out, model_parallel: bool):\n        if self.gate is not None:\n            if model_parallel:\n                g, _ = self.gate(fc1_in)\n            else:\n                g = self.gate(fc1_in)\n            return self.fn(fc1_out, g)\n        return self.fn(fc1_out)\n\n    def __get_fn(self, name: str) -> Callable:\n        \"\"\"Returns the activation function corresponding to the arg passed in the run\"\"\"\n\n        if name == \"relu\":\n            return F.relu\n        elif name == \"relu_squared\":\n            return relu_squared\n        elif name == \"gelu\":\n            return gelu\n        elif name == \"tanh\":\n            return torch.tanh\n        elif name == \"linear\":\n            return lambda x: x\n        elif name == \"swiglu\":\n            return swiglu\n        elif name == \"geglu\":\n            return geglu\n        else:\n            raise RuntimeError(\"--activation-fn {} not supported\".format(name))\n\n    def __get_gated_fns(self) -> List:\n        return [\n            geglu,\n            swiglu,\n        ]\n",
        "metaseq/modules/adaptive_softmax.py": "# Copyright (c) Meta Platforms, Inc. and affiliates. All Rights Reserved.\n#\n# This source code is licensed under the MIT license found in the\n# LICENSE file in the root directory of this source tree.\n\nimport functools\nimport operator\n\nimport torch\nimport torch.nn.functional as F\nfrom metaseq.modules.dropout import Dropout\nfrom metaseq.modules.quant_noise import quant_noise\nfrom torch import nn\n\n\nclass TiedLinear(nn.Module):\n    def __init__(self, weight, transpose):\n        super().__init__()\n        self.weight = weight\n        self.transpose = transpose\n\n    def forward(self, input):\n        return F.linear(input, self.weight.t() if self.transpose else self.weight)\n\n\nclass TiedHeadModule(nn.Module):\n    def __init__(self, weights, input_dim, num_classes, q_noise, qn_block_size):\n        super().__init__()\n        tied_emb, _ = weights\n        self.num_words, emb_dim = tied_emb.size()\n\n        self.word_proj = quant_noise(\n            TiedLinear(tied_emb, transpose=False), q_noise, qn_block_size\n        )\n        if input_dim != emb_dim:\n            self.word_proj = nn.Sequential(\n                quant_noise(\n                    nn.Linear(input_dim, emb_dim, bias=False), q_noise, qn_block_size\n                ),\n                self.word_proj,\n            )\n\n        self.class_proj = quant_noise(\n            nn.Linear(input_dim, num_classes, bias=False), q_noise, qn_block_size\n        )\n        self.out_dim = self.num_words + num_classes\n\n        self.register_buffer(\"_float_tensor\", torch.FloatTensor(1))\n\n    def forward(self, input):\n        inp_sz = functools.reduce(operator.mul, input.shape[:-1], 1)\n        out = self._float_tensor.new(inp_sz, self.out_dim)\n        out[:, : self.num_words] = self.word_proj(input.view(inp_sz, -1))\n        out[:, self.num_words :] = self.class_proj(input.view(inp_sz, -1))\n        return out\n\n\nclass AdaptiveSoftmax(nn.Module):\n    \"\"\"\n    This is an implementation of the efficient softmax approximation for\n    graphical processing units (GPU), described in the paper \"Efficient softmax\n    approximation for GPUs\" (http://arxiv.org/abs/1609.04309).\n    \"\"\"\n\n    def __init__(\n        self,\n        vocab_size,\n        input_dim,\n        cutoff,\n        dropout,\n        factor=4.0,\n        adaptive_inputs=None,\n        tie_proj=False,\n        q_noise=0,\n        qn_block_size=8,\n    ):\n        super().__init__()\n\n        if vocab_size > cutoff[-1]:\n            cutoff = cutoff + [vocab_size]\n        else:\n            assert (\n                vocab_size == cutoff[-1]\n            ), \"cannot specify cutoff larger than vocab size\"\n\n        output_dim = cutoff[0] + len(cutoff) - 1\n\n        self.vocab_size = vocab_size\n        self.cutoff = cutoff\n        self.dropout_module = Dropout(dropout, module_name=self.__class__.__name__)\n        self.input_dim = input_dim\n        self.factor = factor\n        self.q_noise = q_noise\n        self.qn_block_size = qn_block_size\n\n        self.lsm = nn.LogSoftmax(dim=1)\n\n        if adaptive_inputs is not None:\n            self.head = TiedHeadModule(\n                adaptive_inputs.weights_for_band(0),\n                input_dim,\n                len(cutoff) - 1,\n                self.q_noise,\n                self.qn_block_size,\n            )\n        else:\n            self.head = quant_noise(\n                nn.Linear(input_dim, output_dim, bias=False),\n                self.q_noise,\n                self.qn_block_size,\n            )\n\n        self._make_tail(adaptive_inputs, tie_proj)\n\n        def init_weights(m):\n            if (\n                hasattr(m, \"weight\")\n                and not isinstance(m, TiedLinear)\n                and not isinstance(m, TiedHeadModule)\n            ):\n                nn.init.xavier_uniform_(m.weight)\n\n        self.apply(init_weights)\n\n        self.register_buffer(\"version\", torch.LongTensor([1]))\n\n    def _make_tail(self, adaptive_inputs=None, tie_proj=False):\n        self.tail = nn.ModuleList()\n        for i in range(len(self.cutoff) - 1):\n            dim = int(self.input_dim // self.factor ** (i + 1))\n\n            tied_emb, tied_proj = (\n                adaptive_inputs.weights_for_band(i + 1)\n                if adaptive_inputs is not None\n                else (None, None)\n            )\n\n            if tied_proj is not None:\n                if tie_proj:\n                    proj = quant_noise(\n                        TiedLinear(tied_proj, transpose=True),\n                        self.q_noise,\n                        self.qn_block_size,\n                    )\n                else:\n                    proj = quant_noise(\n                        nn.Linear(tied_proj.size(0), tied_proj.size(1), bias=False),\n                        self.q_noise,\n                        self.qn_block_size,\n                    )\n            else:\n                proj = quant_noise(\n                    nn.Linear(self.input_dim, dim, bias=False),\n                    self.q_noise,\n                    self.qn_block_size,\n                )\n\n            if tied_emb is None:\n                out_proj = nn.Linear(\n                    dim, self.cutoff[i + 1] - self.cutoff[i], bias=False\n                )\n            else:\n                out_proj = TiedLinear(tied_emb, transpose=False)\n\n            m = nn.Sequential(\n                proj,\n                nn.Dropout(self.dropout_module.p),\n                quant_noise(out_proj, self.q_noise, self.qn_block_size),\n            )\n\n            self.tail.append(m)\n\n    def upgrade_state_dict_named(self, state_dict, name):\n        version_name = name + \".version\"\n        if version_name not in state_dict:\n            raise Exception(\"This version of the model is no longer supported\")\n\n    def adapt_target(self, target):\n        \"\"\"\n        In order to be efficient, the AdaptiveSoftMax does not compute the\n        scores for all the word of the vocabulary for all the examples. It is\n        thus necessary to call the method adapt_target of the AdaptiveSoftMax\n        layer inside each forward pass.\n        \"\"\"\n\n        target = target.view(-1)\n        new_target = [target.clone()]\n        target_idxs = []\n\n        for i in range(len(self.cutoff) - 1):\n            mask = target.ge(self.cutoff[i]).mul(target.lt(self.cutoff[i + 1]))\n            new_target[0][mask] = self.cutoff[0] + i\n\n            if mask.any():\n                target_idxs.append(mask.nonzero(as_tuple=False).squeeze(1))\n                new_target.append(target[mask].add(-self.cutoff[i]))\n            else:\n                target_idxs.append(None)\n                new_target.append(None)\n\n        return new_target, target_idxs\n\n    def forward(self, input, target):\n        \"\"\"\n        Args:\n            input: (b x t x d)\n            target: (b x t)\n        Returns:\n            2 lists: output for each cutoff section and new targets by cut off\n        \"\"\"\n\n        input = input.contiguous().view(-1, input.size(-1))\n        input = self.dropout_module(input)\n\n        new_target, target_idxs = self.adapt_target(target)\n        output = [self.head(input)]\n\n        for i in range(len(target_idxs)):\n            if target_idxs[i] is not None:\n                output.append(self.tail[i](input.index_select(0, target_idxs[i])))\n            else:\n                output.append(None)\n\n        return output, new_target\n\n    def get_log_prob(self, input, target):\n        \"\"\"\n        Computes the log probabilities for all the words of the vocabulary,\n        given a 2D tensor of hidden vectors.\n        \"\"\"\n\n        bsz, length, dim = input.size()\n        input = input.contiguous().view(-1, dim)\n\n        if target is not None:\n            _, target_idxs = self.adapt_target(target)\n        else:\n            target_idxs = None\n\n        head_y = self.head(input)\n        log_probs = head_y.new_zeros(input.size(0), self.vocab_size)\n\n        head_sz = self.cutoff[0] + len(self.tail)\n        log_probs[:, :head_sz] = self.lsm(head_y)\n        tail_priors = log_probs[:, self.cutoff[0] : head_sz].clone()\n\n        for i in range(len(self.tail)):\n            start = self.cutoff[i]\n            end = self.cutoff[i + 1]\n\n            if target_idxs is None:\n                tail_out = log_probs[:, start:end]\n                tail_out.copy_(self.tail[i](input))\n                log_probs[:, start:end] = self.lsm(tail_out).add_(\n                    tail_priors[:, i, None]\n                )\n            elif target_idxs[i] is not None:\n                idxs = target_idxs[i]\n                tail_out = log_probs[idxs, start:end]\n                tail_out.copy_(self.tail[i](input[idxs]))\n                log_probs[idxs, start:end] = self.lsm(tail_out).add_(\n                    tail_priors[idxs, i, None]\n                )\n\n        log_probs = log_probs.view(bsz, length, -1)\n        return log_probs\n",
        "metaseq/modules/checkpoint_activation_wrapper/__init__.py": "",
        "metaseq/modules/checkpoint_activation_wrapper/checkpoint_activations.py": "# Copyright (c) Meta Platforms, Inc. and affiliates. All Rights Reserved.\n#\n# This source code is licensed under the BSD license found in the\n# LICENSE file in the root directory of this source tree.\n\nimport functools\nimport threading\nimport weakref\nfrom contextlib import contextmanager\nfrom typing import Any, Dict, Generator, Optional, Tuple\n\nimport torch\nimport torch.nn as nn\nimport torch.utils.checkpoint as torch_checkpoint\nfrom fairscale.nn.checkpoint.checkpoint_utils import patch_batchnorm\nfrom fairscale.utils.containers import (\n    pack_kwargs,\n    split_non_tensors,\n    unpack_kwargs,\n    unpack_non_tensors,\n)\nfrom metaseq.modules.megatron import mpu\nfrom torch import Tensor\n\nfrom metaseq.distributed.utils import get_model_parallel_group\n\n\n# https://docs.python.org/3/library/threading.html#thread-local-data\n# Manage the checkpoint context with thread-local data.\nclass ThreadLocal(threading.local):\n    def __init__(self) -> None:\n        self.is_checkpointing = False\n        self.is_recomputing = False\n        self.is_checkpointing_disabled = False\n\n\nthread_local = ThreadLocal()\n\n\n@contextmanager\ndef disable_checkpointing() -> Generator[None, None, None]:\n    \"\"\"Makes :func:`is_checkpointing_disabled` return :data:`True` within a context.\"\"\"\n    orig = thread_local.is_checkpointing_disabled\n    thread_local.is_checkpointing_disabled = True\n    try:\n        yield\n    finally:\n        thread_local.is_checkpointing_disabled = orig\n\n\n@contextmanager\ndef enable_checkpointing() -> Generator[None, None, None]:\n    \"\"\"Makes :func:`is_checkpointing` return :data:`True` within a context.\"\"\"\n    orig = thread_local.is_checkpointing\n    thread_local.is_checkpointing = True\n    try:\n        yield\n    finally:\n        thread_local.is_checkpointing = orig\n\n\n@contextmanager\ndef enable_recomputing() -> Generator[None, None, None]:\n    \"\"\"Makes :func:`is_recomputing` return :data:`True` within a context.\"\"\"\n    orig = thread_local.is_recomputing\n    thread_local.is_recomputing = True\n    try:\n        yield\n    finally:\n        thread_local.is_recomputing = orig\n\n\ndef is_checkpointing() -> bool:\n    \"\"\"Whether the current forward propagation is under checkpointing.\n\n    Returns:\n        bool: :data:`True` if it's under checkpointing.\n\n    \"\"\"\n    return thread_local.is_checkpointing\n\n\ndef is_recomputing() -> bool:\n    \"\"\"Whether the current forward propagation is under checkpoint\n    recomputation. Use this to prevent duplicated side-effects at forward\n    propagation::\n\n        class Counter(nn.Module):\n            def __init__(self):\n                super().__init__()\n                self.counter = 0\n\n            def forward(self, input):\n                if not is_recomputing():\n                    self.counter += 1\n                return input\n\n    Returns:\n        bool: :data:`True` if it's under checkpoint recomputation.\n    \"\"\"\n    return thread_local.is_recomputing\n\n\ndef checkpoint_wrapper(\n    module: nn.Module,\n    offload_to_cpu: bool = False,\n    distribute_checkpointed_activations: bool = False,\n) -> nn.Module:\n    \"\"\"\n    A friendlier wrapper for performing activation checkpointing.\n\n    Compared to the PyTorch version, this version:\n\n        - wraps an nn.Module, so that all subsequent calls will use checkpointing\n        - handles keyword arguments in the forward\n        - handles non-Tensor outputs from the forward\n        - supports offloading activations to CPU\n\n    Usage::\n\n        checkpointed_module = checkpoint_wrapper(my_module, offload_to_cpu=True)\n        a, b = checkpointed_module(x, y=3, z=torch.Tensor([1]))\n\n    To understand the benefits of checkpointing and the `offload_to_cpu` flag,\n    let's divide activations into 2 types: inner activations and outer\n    activations w.r.t. the checkpointed modules. The inner ones are saved\n    by activation checkpointing, the outer ones are saved by offload_to_cpu.\n\n    In terms of GPU memory savings:\n\n        - When inner ones are large in size and outer ones are small,\n          checkpointing helps a lot, offload_to_cpu may help a little.\n        - When inner ones are small and outer ones are large,\n          checkpointing helps little, offload_to_cpu helps a lot.\n        - When both inner and outer are large, both help and the\n          benefit is additive.\n\n    ..Note::\n\n        The first and last layers are not likely to benefit from the `offload_to_cpu` flag\n        because (1) there are typically other references to the first layer's input, so\n        the GPU memory won't be freed; (2) the input to the last layer is immediately\n        used by the backward pass and won't result in memory savings.\n\n    Args:\n        module (nn.Module):\n            The module to be wrapped\n        offload_to_cpu (bool):\n            Whether to offload activations to CPU.\n\n    Returns:\n        (nn.Module):\n            Wrapped module\n    \"\"\"\n    # Patch the batchnorm layers in case there are any in this module.\n    patch_batchnorm(module)\n\n    # The use of weakref here is to prevent creating a ref cycle: m -> m.forward -> m.\n    # When such cycle exists, gc won't collect the module when the module is freed.\n    # That causes GPU memory to be leaked. See the unit test for how we catch that.\n    #\n    # We prefer this over a class wrapper since the class wrapper would have to\n    # proxy a lot of fields and methods.\n    module.forward = functools.partial(  # type: ignore\n        _checkpointed_forward,\n        type(module).forward,\n        weakref.ref(module),\n        offload_to_cpu,\n        distribute_checkpointed_activations,\n    )\n    return module\n\n\ndef _checkpointed_forward(\n    original_forward: Any,\n    weak_self: Any,\n    offload_to_cpu: bool,\n    distribute_checkpointed_activations: bool,\n    *args: Any,\n    **kwargs: Any,\n) -> Any:\n    module = weak_self()\n\n    # If gradients are disabled, just use original `.forward()` method directly.\n    if not torch.is_grad_enabled() or thread_local.is_checkpointing_disabled:\n        return original_forward(module, *args, **kwargs)\n\n    # Autograd Functions in PyTorch work best with positional args, since\n    # the backward must return gradients (or None) for every input argument.\n    # We can flatten keyword arguments to make this easier.\n    args = (module,) + args\n    kwarg_keys, flat_args = pack_kwargs(*args, **kwargs)\n    parent_ctx_dict: Dict[str, Any] = {\n        \"offload\": offload_to_cpu,\n        \"distribute_checkpointed_activations\": distribute_checkpointed_activations,\n    }\n    # Dummy tensor with grad is used to ensure the backward pass is called. This is needed\n    # when original_forward's input are non-tensor (i.e. a tuple). Using this dummy tensor\n    # avoids requiring users to set their input tensors's requires_grad flag. In the case\n    # of tuple type inputs, setting the flag won't even trigger the backward pass.\n    #\n    # One implication of this is that since we always feed in a dummy tensor\n    # needing grad, then the output will always require grad, even if it originally\n    # wouldn't, such as if the module and original input both do not require grad.\n    # We get around this by saving the desired requires_grad value in output and\n    # detaching the output if needed.\n    output = CheckpointFunction.apply(\n        torch.tensor([], requires_grad=True),\n        original_forward,\n        parent_ctx_dict,\n        kwarg_keys,\n        *flat_args,\n    )\n    output_requires_grad = parent_ctx_dict[\"output_requires_grad\"]\n    if not isinstance(output, torch.Tensor):\n        # If output should not require grad, then detach it, since otherwise it will\n        # always have requires_grad = True due to our dummy tensor input above that\n        # requires_grad\n        output = [x.detach() if not output_requires_grad else x for x in output]\n\n        packed_non_tensor_outputs = parent_ctx_dict[\"packed_non_tensor_outputs\"]\n        if packed_non_tensor_outputs:\n            output = unpack_non_tensors(output, packed_non_tensor_outputs)\n\n    else:\n        # If output should not require grad, then detach it, since otherwise it will\n        # always have requires_grad = True due to our dummy tensor input above that\n        # requires_grad\n        if not output_requires_grad:\n            output = output.detach()\n\n    return output\n\n\ndef get_rng_state() -> Dict[str, Any]:\n    state = {\"torch_rng_state\": torch.get_rng_state()}\n    if torch.cuda.is_available():\n        state[\"cuda_rng_state\"] = torch.cuda.get_rng_state()\n    return state\n\n\ndef set_rng_state(state: Dict[str, Any]) -> None:\n    torch.set_rng_state(state[\"torch_rng_state\"])\n    if torch.cuda.is_available():\n        torch.cuda.set_rng_state(state[\"cuda_rng_state\"])\n\n\ndef is_autocast_enabled() -> bool:\n    \"\"\"Similar to torch.is_autocast_enabled, but compatible with torch 1.5.1\"\"\"\n    if hasattr(torch, \"is_autocast_enabled\"):\n        return torch.is_autocast_enabled()\n    return False\n\n\n@contextmanager\ndef autocast(enabled: bool) -> Generator:\n    \"\"\"Similar to torch.cuda.amp.autocast, but compatible with torch 1.5.1\"\"\"\n    if enabled:\n        with torch.cuda.amp.autocast(enabled):\n            yield\n    else:\n        yield\n\n\nclass CheckpointFunction(torch.autograd.Function):\n    \"\"\"Similar to the torch version, but support non-Tensor outputs.\n\n    The caller is expected to provide a dict (*parent_ctx_dict*) that will hold\n    the non-Tensor outputs. These should be combined with the Tensor *outputs*\n    by calling :func:`unpack_non_tensors`.\n    \"\"\"\n\n    @staticmethod\n    def forward(  # type: ignore\n        ctx: Any,\n        dummy_tensor_requires_grad: torch.Tensor,\n        run_function: Any,\n        parent_ctx_dict: Dict[str, Any],\n        kwarg_keys: Tuple[str, ...],\n        *args: Any,\n        **kwargs: Any,\n    ) -> Any:\n        torch_checkpoint.check_backward_validity(args)\n\n        ctx.run_function = run_function\n        ctx.kwarg_keys = kwarg_keys\n        ctx.fwd_rng_state = get_rng_state()\n\n        ctx.is_model_parallel = get_model_parallel_group() is not None\n        # Megatron's dropout random state\n        if ctx.is_model_parallel:\n            ctx.fwd_cuda_rng_state_tracker = mpu.get_cuda_rng_tracker().get_states()\n        ctx.had_autocast_in_fwd = is_autocast_enabled()\n\n        tensor_inputs, packed_non_tensor_inputs = split_non_tensors(args)\n        if parent_ctx_dict[\"offload\"]:\n            ctx.fwd_device = tuple(x.device for x in tensor_inputs)\n            ctx.grad_requirements = tuple(x.requires_grad for x in tensor_inputs)\n            tensor_inputs = tuple(x.to(\"cpu\", non_blocking=True) for x in tensor_inputs)\n        else:\n            ctx.fwd_device, ctx.grad_requirements = None, None\n\n        with torch.no_grad(), enable_checkpointing():\n            unpacked_args, unpacked_kwargs = unpack_kwargs(kwarg_keys, args)\n            outputs = run_function(*unpacked_args, **unpacked_kwargs)\n            the_module = unpacked_args[0]\n        ctx.distribute_checkpointed_activations = (\n            parent_ctx_dict[\"distribute_checkpointed_activations\"]\n            and ctx.is_model_parallel\n        )\n        if ctx.distribute_checkpointed_activations:\n            # HACK [TODO: naman] currently only distributing the first tensor.\n            # second tensor for usual decoder models is just attention mask, which is seq_len * seq_len\n            # and not big enough to distribute over.\n            ctx.tensor_input_0_shape = tensor_inputs[0].data.shape\n            tensor_inputs[0].data = tensor_inputs[0].data.contiguous()\n            tensor_inputs[0].data = mpu.split_tensor_into_1d_equal_chunks(\n                tensor_inputs[0].data, new_buffer=True\n            )\n        ctx.save_for_backward(*tensor_inputs)\n\n        ctx.packed_non_tensor_inputs = packed_non_tensor_inputs\n        # Because we run with torch.no_grad(), we can't actually access\n        # outputs.requires_grad. Instead, we manually compute it by\n        # checking if either the input or the module needs grads\n        parameters = list(the_module.parameters())\n\n        # If the module is wrapped by FlattenParamsWrapper, then the\n        # parameters would have been deleted. If so, we need to access\n        # the views into the flattened parameters.\n        if hasattr(the_module, \"_unflattened_param_views\"):\n            parameters += the_module._unflattened_param_views\n\n        output_requires_grad = any(param.requires_grad for param in parameters) or any(\n            x.requires_grad for x in tensor_inputs\n        )\n        parent_ctx_dict[\"output_requires_grad\"] = output_requires_grad\n\n        if not isinstance(outputs, torch.Tensor):\n            # Autograd Functions don't like non-Tensor outputs. We can split the\n            # non-Tensor and Tensor outputs, returning the former by reference\n            # through *parent_ctx_dict* and returning the latter directly.\n            outputs, packed_non_tensor_outputs = split_non_tensors(outputs)\n            parent_ctx_dict[\"packed_non_tensor_outputs\"] = packed_non_tensor_outputs\n\n        return outputs\n\n    @staticmethod\n    def backward(ctx: Any, *args: Any) -> Tuple[Optional[Tensor], ...]:\n        if not torch.autograd._is_checkpoint_valid():\n            raise RuntimeError(\n                \"Checkpointing is not compatible with .grad(), please use .backward() if possible\"\n            )\n\n        tensor_inputs: Tuple = ctx.saved_tensors\n        if ctx.distribute_checkpointed_activations:\n            tensor_inputs[0].data = mpu.gather_split_1d_tensor(tensor_inputs[0].data)\n            tensor_inputs[0].data = tensor_inputs[0].data.view(ctx.tensor_input_0_shape)\n\n        tensor_inputs = torch_checkpoint.detach_variable(tensor_inputs)\n        if ctx.fwd_device is not None:\n            tensor_inputs = tuple(\n                t.to(ctx.fwd_device[i], non_blocking=True)\n                for i, t in enumerate(tensor_inputs)\n            )\n            for i, need_grad in enumerate(ctx.grad_requirements):\n                tensor_inputs[i].requires_grad = need_grad\n        inputs = unpack_non_tensors(tensor_inputs, ctx.packed_non_tensor_inputs)\n\n        # Store the current states.\n        bwd_rng_state = get_rng_state()\n        if ctx.is_model_parallel:\n            bwd_cuda_rng_state_tracker = mpu.get_cuda_rng_tracker().get_states()\n            mpu.get_cuda_rng_tracker().set_states(ctx.fwd_cuda_rng_state_tracker)\n\n        # Set the states to what it used to be before the forward pass.\n        set_rng_state(ctx.fwd_rng_state)\n\n        with torch.enable_grad(), enable_recomputing(), autocast(\n            ctx.had_autocast_in_fwd\n        ):\n            unpacked_args, unpacked_kwargs = unpack_kwargs(ctx.kwarg_keys, inputs)\n            outputs = ctx.run_function(*unpacked_args, **unpacked_kwargs)\n            tensor_outputs, _ = split_non_tensors(outputs)\n\n        # Set the states back to what it was at the start of this function.\n        set_rng_state(bwd_rng_state)\n        if ctx.is_model_parallel:\n            mpu.get_cuda_rng_tracker().set_states(bwd_cuda_rng_state_tracker)\n\n        # Run backward() with only Tensors that require grad\n        outputs_with_grad = []\n        args_with_grad = []\n        for i in range(len(tensor_outputs)):\n            if tensor_outputs[i].requires_grad:\n                outputs_with_grad.append(tensor_outputs[i])\n                args_with_grad.append(args[i])\n\n        if len(outputs_with_grad) == 0:\n            raise RuntimeError(\n                \"None of the outputs have requires_grad=True, \"\n                \"this checkpoint() is not necessary\"\n            )\n\n        torch.autograd.backward(outputs_with_grad, args_with_grad)\n\n        grads = tuple(\n            inp.grad if isinstance(inp, torch.Tensor) else None for inp in inputs\n        )\n\n        return (None, None, None, None) + grads\n",
        "metaseq/modules/checkpoint_activations.py": "# Copyright (c) Meta Platforms, Inc. and affiliates. All Rights Reserved.\n#\n# This source code is licensed under the MIT license found in the\n# LICENSE file in the root directory of this source tree.\n\n\ndef checkpoint_wrapper(module, *args, **kwargs):\n    try:\n        from metaseq.modules.checkpoint_activation_wrapper.checkpoint_activations import (\n            checkpoint_wrapper as _checkpoint_wrapper,\n        )\n    except ImportError:\n        raise ImportError(\n            \"Cannot find fairscale.nn.misc.checkpoint_activations. \"\n            \"Please install fairscale with: pip install fairscale\"\n        )\n\n    module = _checkpoint_wrapper(module, *args, **kwargs)\n\n    if hasattr(module, \"extra_repr\"):\n        orig_extra_repr = module.extra_repr\n    else:\n        orig_extra_repr = None\n\n    def extra_repr():\n        return (\n            f\"[checkpointed] {orig_extra_repr()}\" if orig_extra_repr is not None else \"\"\n        )\n\n    module.extra_repr = extra_repr\n\n    return module\n",
        "metaseq/modules/dropout.py": "# Copyright (c) Meta Platforms, Inc. and affiliates. All Rights Reserved.\n#\n# This source code is licensed under the MIT license found in the\n# LICENSE file in the root directory of this source tree.\n\nimport logging\n\nimport torch.nn as nn\nimport torch.nn.functional as F\n\nlogger = logging.getLogger(__name__)\n\n\nclass Dropout(nn.Module):\n    def __init__(self, p, module_name=None):\n        super().__init__()\n        self.p = p\n        self.module_name = module_name\n        self.apply_during_inference = False\n\n    def extra_repr(self) -> str:\n        return \"p={}\".format(self.p)\n\n    def forward(self, x, inplace: bool = False):\n        if self.p > 0 and (self.training or self.apply_during_inference):\n            return F.dropout(x, p=self.p, training=True, inplace=inplace)\n        else:\n            return x\n",
        "metaseq/modules/embedding.py": "# Copyright (c) Meta Platforms, Inc. and affiliates. All Rights Reserved.\n#\n# This source code is licensed under the MIT license found in the\n# LICENSE file in the root directory of this source tree.\n\nfrom typing import Optional\n\nimport torch\nfrom torch import nn as nn\n\n\ndef Embedding(\n    num_embeddings,\n    embedding_dim,\n    padding_idx,\n    initialize_params_on_gpu=False,\n    dtype: Optional[torch.dtype] = None,\n):\n    \"\"\"\n    Returns an embedding initialized to normal(0, 1/sqrt(embedding_dim))\n    with the padding token embedding initialized to 0.\n    \"\"\"\n    # Passing weights initialized on GPU.\n    device = torch.cuda.current_device() if initialize_params_on_gpu else None\n    if dtype is None:\n        dtype = torch.float\n    weight = torch.empty(num_embeddings, embedding_dim, device=device, dtype=dtype)\n    nn.init.normal_(weight, mean=0, std=embedding_dim**-0.5)\n    nn.init.constant_(weight[padding_idx], 0)\n    m = nn.Embedding(\n        num_embeddings, embedding_dim, padding_idx=padding_idx, _weight=weight\n    )\n    return m\n",
        "metaseq/modules/feedforward.py": "# Copyright (c) Meta Platforms, Inc. and affiliates. All Rights Reserved.\n#\n# This source code is licensed under the MIT license found in the\n# LICENSE file in the root directory of this source tree.\n\nfrom torch import nn as nn\n\nfrom metaseq.modules import Linear\n\n\ndef FeedForward(x, fc1, activation_fn, fc2, dropout_module):\n    \"\"\"\n    Feedforward network consisting of two linear layers (fc1, fc2), where activation_fn is applied\n    between the two layers and dropout_module is applied at the end.\n    \"\"\"\n    # apex fused bias gelu is not yet supported with megatron model parallel\n    # TODO [namangoyal]: Find better way to do this\n    model_parallel = not isinstance(fc1, nn.Linear) and not isinstance(fc1, Linear)\n    if model_parallel:\n        # here, we do the bias computation inside fc1 and fc2 AND gather_output\n        x = activation_fn(x, fc1(x)[0], model_parallel=True)\n        x, _ = fc2(x)\n    else:\n        x = activation_fn(x, fc1(x), model_parallel=False)\n        x = fc2(x)\n    x = dropout_module(x)\n    return x\n",
        "metaseq/modules/group_norm_fp32.py": "# Copyright (c) Facebook, Inc. and its affiliates.\n#\n# This source code is licensed under the MIT license found in the\n# LICENSE file in the root directory of this source tree.\n\"\"\"\nGroup norm done in fp32 (for fp16 training)\n\nReference:\nhttps://github.com/facebookresearch/fairseq/blob/ad0e69cd99e1ff884041fbd8467d1404bd09847a/fairseq/modules/fp32_group_norm.py\n\"\"\"\n\nimport torch.nn as nn\nimport torch.nn.functional as F\n\n\nclass GroupNormFp32(nn.GroupNorm):\n    def __init__(self, *args, **kwargs):\n        super().__init__(*args, **kwargs)\n\n    def forward(self, input):\n        output = F.group_norm(\n            input.float(),\n            self.num_groups,\n            self.weight.float() if self.weight is not None else None,\n            self.bias.float() if self.bias is not None else None,\n            self.eps,\n        )\n        return output.type_as(input)\n",
        "metaseq/modules/layer_norm.py": "# Copyright (c) Meta Platforms, Inc. and affiliates. All Rights Reserved.\n#\n# This source code is licensed under the MIT license found in the\n# LICENSE file in the root directory of this source tree.\n\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\n\ntry:\n    from apex.normalization import FusedLayerNorm as _FusedLayerNorm\n\n    has_fused_layernorm = True\n\n    class FusedLayerNorm(_FusedLayerNorm):\n        @torch.jit.unused\n        def forward(self, x):\n            if not x.is_cuda:\n                return super().forward(x)\n            else:\n                with torch.cuda.device(x.device):\n                    return super().forward(x)\n\nexcept ImportError:\n    has_fused_layernorm = False\n\n\ndef LayerNorm(normalized_shape, eps=1e-5, elementwise_affine=True, export=False):\n    if torch.jit.is_scripting():\n        export = True\n    if not export and torch.cuda.is_available() and has_fused_layernorm:\n        return FusedLayerNorm(normalized_shape, eps, elementwise_affine)\n    return torch.nn.LayerNorm(normalized_shape, eps, elementwise_affine)\n\n\nclass LayerNormFp32(nn.LayerNorm):\n    def __init__(self, *args, **kwargs):\n        super().__init__(*args, **kwargs)\n\n    def forward(self, input):\n        output = F.layer_norm(\n            input.float(),\n            self.normalized_shape,\n            self.weight.float() if self.weight is not None else None,\n            self.bias.float() if self.bias is not None else None,\n            self.eps,\n        )\n        return output.type_as(input)\n",
        "metaseq/modules/learned_positional_embedding.py": "# Copyright (c) Meta Platforms, Inc. and affiliates. All Rights Reserved.\n#\n# This source code is licensed under the MIT license found in the\n# LICENSE file in the root directory of this source tree.\n\nfrom typing import Dict, Optional\n\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom torch import Tensor\n\nfrom metaseq import utils\n\n\nclass LearnedPositionalEmbedding(nn.Embedding):\n    \"\"\"\n    This module learns positional embeddings up to a fixed maximum size.\n    Padding ids are ignored by either offsetting based on padding_idx\n    or by setting padding_idx to None and ensuring that the appropriate\n    position ids are passed to the forward function.\n    \"\"\"\n\n    def __init__(self, num_embeddings: int, embedding_dim: int, padding_idx: int):\n        super().__init__(num_embeddings, embedding_dim, padding_idx)\n        if self.padding_idx is not None:\n            self.max_positions = self.num_embeddings - self.padding_idx - 1\n        else:\n            self.max_positions = self.num_embeddings\n\n    def forward(\n        self,\n        input: Tensor,\n        incremental_state: Optional[Dict[str, Dict[str, Optional[Tensor]]]] = None,\n        positions: Optional[Tensor] = None,\n    ):\n        \"\"\"Input is expected to be of size [bsz x seqlen].\"\"\"\n        assert (positions is None) or (\n            self.padding_idx is None\n        ), \"If positions is pre-computed then padding_idx should not be set.\"\n\n        # we cannot use incremental state here because we must be aware of\n        # padding.\n        if positions is None and self.padding_idx is not None:\n            positions = utils.make_positions(input, self.padding_idx)\n\n        return F.embedding(\n            positions,\n            self.weight,\n            self.padding_idx,\n            self.max_norm,\n            self.norm_type,\n            self.scale_grad_by_freq,\n            self.sparse,\n        )\n",
        "metaseq/modules/linear.py": "# Copyright (c) Meta Platforms, Inc. and affiliates. All Rights Reserved.\n#\n# This source code is licensed under the MIT license found in the\n# LICENSE file in the root directory of this source tree.\n\nimport math\n\nimport torch\nfrom torch import Tensor\nfrom torch.nn import Module\nfrom torch.nn import functional as F\nfrom torch.nn import init\nfrom torch.nn.parameter import Parameter\n\n\nclass Linear(Module):\n    \"\"\"\n    Exact same as pytorch nn.Linear but with option to initialize weight and bias directly on GPU\n    \"\"\"\n\n    __constants__ = [\"in_features\", \"out_features\"]\n    in_features: int\n    out_features: int\n    weight: Tensor\n\n    def __init__(\n        self,\n        in_features: int,\n        out_features: int,\n        bias: bool = True,\n        initialize_params_on_gpu: bool = False,\n        dtype: torch.dtype = None,\n    ) -> None:\n        super(Linear, self).__init__()\n        self.in_features = in_features\n        self.out_features = out_features\n        device = torch.cuda.current_device() if initialize_params_on_gpu else None\n        if dtype is None:\n            dtype = torch.float\n        self.weight = Parameter(\n            torch.empty(out_features, in_features, device=device, dtype=dtype)\n        )\n        if bias:\n            self.bias = Parameter(torch.empty(out_features, device=device, dtype=dtype))\n        else:\n            self.register_parameter(\"bias\", None)\n        self.reset_parameters()\n\n    def reset_parameters(self) -> None:\n        init.kaiming_uniform_(self.weight, a=math.sqrt(5))\n        if self.bias is not None:\n            fan_in, _ = init._calculate_fan_in_and_fan_out(self.weight)\n            bound = 1 / math.sqrt(fan_in)\n            init.uniform_(self.bias, -bound, bound)\n\n    def forward(self, input: Tensor) -> Tensor:\n        return F.linear(input, self.weight, self.bias)\n\n    def extra_repr(self) -> str:\n        return \"in_features={}, out_features={}, bias={}\".format(\n            self.in_features, self.out_features, self.bias is not None\n        )\n",
        "metaseq/modules/megatron/__init__.py": "",
        "metaseq/modules/megatron/global_vars.py": "# coding=utf-8\n# Copyright (c) 2020, NVIDIA CORPORATION.  All rights reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\nimport operator\nfrom functools import reduce\n\nimport torch\n\n_GLOBAL_ARGS = None\n_GLOBAL_MEMORY_BUFFER = None\n\n\ndef get_args():\n    \"\"\"Return arguments.\"\"\"\n    _ensure_var_is_initialized(_GLOBAL_ARGS, \"args\")\n    return _GLOBAL_ARGS\n\n\ndef get_global_memory_buffer():\n    _ensure_var_is_initialized(_GLOBAL_MEMORY_BUFFER, \"global memory buffer\")\n    return _GLOBAL_MEMORY_BUFFER\n\n\ndef _set_global_memory_buffer():\n    \"\"\"Initialize global buffer\"\"\"\n    global _GLOBAL_MEMORY_BUFFER\n    _ensure_var_is_not_initialized(_GLOBAL_MEMORY_BUFFER, \"global memory buffer\")\n    _GLOBAL_MEMORY_BUFFER = GlobalMemoryBuffer()\n\n\ndef _ensure_var_is_initialized(var, name):\n    \"\"\"Make sure the input variable is not None.\"\"\"\n    assert var is not None, \"{} is not initialized.\".format(name)\n\n\ndef _ensure_var_is_not_initialized(var, name):\n    \"\"\"Make sure the input variable is not None.\"\"\"\n    assert var is None, \"{} is already initialized.\".format(name)\n\n\nclass GlobalMemoryBuffer:\n    \"\"\"Global buffer to avoid dynamic memory allocations.\n    Caller should ensure that buffers of the same name\n    are not used concurrently.\"\"\"\n\n    def __init__(self):\n        self.buffer = {}\n\n    def get_tensor(self, tensor_shape, dtype, name):\n        required_len = reduce(operator.mul, tensor_shape, 1)\n        if (\n            self.buffer.get((name, dtype), None) is None\n            or self.buffer[(name, dtype)].numel() < required_len\n        ):\n            self.buffer[(name, dtype)] = torch.empty(\n                required_len,\n                dtype=dtype,\n                device=torch.cuda.current_device(),\n                requires_grad=False,\n            )\n\n        return self.buffer[(name, dtype)][0:required_len].view(*tensor_shape)\n",
        "metaseq/modules/megatron/model/__init__.py": "# Copyright (c) Meta Platforms, Inc. and affiliates. All Rights Reserved.\n#\n# This source code is licensed under the MIT license found in the\n# LICENSE file in the root directory of this source tree.\n\nfrom .fused_softmax import ScaledMaskedSoftmax\nfrom .fused_softmax import ScaledUpperTriangMaskedSoftmax\n",
        "metaseq/modules/megatron/model/fused_softmax.py": "# Copyright (c) Meta Platforms, Inc. and affiliates. All Rights Reserved.\n#\n# This source code is licensed under the MIT license found in the\n# LICENSE file in the root directory of this source tree.\n#\n# Taken from:\n# https://github.com/ngoyal2707/Megatron-LM/blob/fa6c0860b62e4ed2ac13a513e7d950d72f576a44/megatron/model/fused_softmax.py\n\n\nimport torch\n\nfrom metaseq.modules.megatron.fused_kernels import (\n    scaled_masked_softmax_cuda,\n    scaled_upper_triang_masked_softmax_cuda,\n)\n\n\nclass ScaledUpperTriangMaskedSoftmax(torch.autograd.Function):\n    \"\"\"\n    Fused operation which performs following three operations in sequence\n    1. Scale the tensor.\n    2. Apply upper triangular mask (typically used in gpt models).\n    3. Perform softmax.\n    \"\"\"\n\n    @staticmethod\n    def forward(ctx, inputs, scale):\n        scale_t = torch.tensor([scale])\n        softmax_results = scaled_upper_triang_masked_softmax_cuda.forward(\n            inputs, scale_t[0]\n        )\n\n        ctx.save_for_backward(softmax_results, scale_t)\n        return softmax_results\n\n    @staticmethod\n    def backward(ctx, output_grads):\n        softmax_results, scale_t = ctx.saved_tensors\n        input_grads = scaled_upper_triang_masked_softmax_cuda.backward(\n            output_grads, softmax_results, scale_t[0]\n        )\n\n        return input_grads, None\n\n\nclass ScaledMaskedSoftmax(torch.autograd.Function):\n    \"\"\"\n    Fused operation which performs following three operations in sequence\n    1. Scale the tensor.\n    2. Apply the mask.\n    3. Perform softmax.\n    \"\"\"\n\n    @staticmethod\n    def forward(ctx, inputs, mask, scale):\n        scale_t = torch.tensor([scale])\n\n        softmax_results = scaled_masked_softmax_cuda.forward(inputs, mask, scale_t[0])\n        ctx.save_for_backward(softmax_results, scale_t)\n        return softmax_results\n\n    @staticmethod\n    def backward(ctx, output_grads):\n        softmax_results, scale_t = ctx.saved_tensors\n\n        input_grads = scaled_masked_softmax_cuda.backward(\n            output_grads, softmax_results, scale_t[0]\n        )\n        return input_grads, None, None\n",
        "metaseq/modules/megatron/mpu/__init__.py": "# Copyright (c) Meta Platforms, Inc. and affiliates. All Rights Reserved.\n#\n# This source code is licensed under the MIT license found in the\n# LICENSE file in the root directory of this source tree.\n\nfrom .cross_entropy import vocab_parallel_cross_entropy\n\nfrom .initialize import destroy_model_parallel\nfrom .initialize import get_data_parallel_group\nfrom .initialize import get_data_parallel_rank\nfrom .initialize import get_tensor_model_parallel_group\nfrom .initialize import get_tensor_model_parallel_rank\nfrom .initialize import get_tensor_model_parallel_world_size\nfrom .initialize import initialize_model_parallel\n\nfrom .layers import LinearWithGradAccumulationAndAsyncCommunication\nfrom .layers import ColumnParallelLinear\nfrom .layers import RowParallelLinear\nfrom .layers import VocabParallelEmbedding\n\nfrom .mappings import copy_to_tensor_model_parallel_region\nfrom .mappings import reduce_from_tensor_model_parallel_region\nfrom .mappings import scatter_to_tensor_model_parallel_region\nfrom .mappings import gather_from_tensor_model_parallel_region\nfrom .mappings import scatter_to_sequence_parallel_region\nfrom .mappings import reduce_scatter_to_sequence_parallel_region\nfrom .mappings import _reduce_scatter_along_first_dim\nfrom .mappings import _gather_along_first_dim\n\nfrom .random import get_cuda_rng_tracker\nfrom .random import model_parallel_cuda_manual_seed\nfrom .random import gather_split_1d_tensor\nfrom .random import split_tensor_into_1d_equal_chunks\n\nfrom .utils import divide\nfrom .utils import split_tensor_along_last_dim\nfrom .utils import ensure_divisibility\nfrom .utils import VocabUtility\n",
        "metaseq/modules/megatron/mpu/cross_entropy.py": "# Copyright (c) Meta Platforms, Inc. and affiliates. All Rights Reserved.\n#\n# This source code is licensed under the MIT license found in the\n# LICENSE file in the root directory of this source tree.\n#\n# Taken from:\n# https://github.com/ngoyal2707/Megatron-LM/blob/fa6c0860b62e4ed2ac13a513e7d950d72f576a44/megatron/mpu/cross_entropy.py\n\n\nimport torch\n\nfrom .initialize import (\n    get_tensor_model_parallel_group,\n    get_tensor_model_parallel_rank,\n)\nfrom .utils import VocabUtility\n\n\nclass _VocabParallelCrossEntropy(torch.autograd.Function):\n    @staticmethod\n    def forward(ctx, vocab_parallel_logits, target):\n        # Maximum value along vocab dimension across all GPUs.\n        logits_max = torch.max(vocab_parallel_logits, dim=-1)[0]\n        torch.distributed.all_reduce(\n            logits_max,\n            op=torch.distributed.ReduceOp.MAX,\n            group=get_tensor_model_parallel_group(),\n        )\n        # Subtract the maximum value.\n        vocab_parallel_logits.sub_(logits_max.unsqueeze(dim=-1))\n\n        # Get the partition's vocab indecies\n        get_vocab_range = VocabUtility.vocab_range_from_per_partition_vocab_size\n        partition_vocab_size = vocab_parallel_logits.size()[-1]\n        rank = get_tensor_model_parallel_rank()\n        vocab_start_index, vocab_end_index = get_vocab_range(partition_vocab_size, rank)\n\n        # Create a mask of valid vocab ids (1 means it needs to be masked).\n        target_mask = (target < vocab_start_index) | (target >= vocab_end_index)\n        masked_target = target.clone() - vocab_start_index\n        masked_target.masked_fill_(target_mask, 0)\n\n        # Get predicted-logits = logits[target].\n        # For Simplicity, we convert logits to a 2-D tensor with size\n        # [*, partition-vocab-size] and target to a 1-D tensor of size [*].\n        logits_2d = vocab_parallel_logits.view(-1, partition_vocab_size)\n        masked_target_1d = masked_target.view(-1)\n        # arange_1d = torch.arange(start=0, end=logits_2d.size()[0],\n        #                         device=logits_2d.device)\n        # predicted_logits_1d = logits_2d[arange_1d, masked_target_1d]\n        # predicted_logits_1d = predicted_logits_1d.clone().contiguous()\n        predicted_logits_1d = logits_2d.gather(-1, masked_target_1d.unsqueeze(-1))\n        predicted_logits = predicted_logits_1d.view_as(target)\n        predicted_logits.masked_fill_(target_mask, 0.0)\n        # All reduce is needed to get the chunks from other GPUs.\n        torch.distributed.all_reduce(\n            predicted_logits,\n            op=torch.distributed.ReduceOp.SUM,\n            group=get_tensor_model_parallel_group(),\n        )\n\n        # Sum of exponential of logits along vocab dimension across all GPUs.\n        exp_logits = vocab_parallel_logits\n        torch.exp(vocab_parallel_logits, out=exp_logits)\n        sum_exp_logits = exp_logits.sum(dim=-1)\n        torch.distributed.all_reduce(\n            sum_exp_logits,\n            op=torch.distributed.ReduceOp.SUM,\n            group=get_tensor_model_parallel_group(),\n        )\n\n        # Loss = log(sum(exp(logits))) - predicted-logit.\n        loss = torch.log(sum_exp_logits) - predicted_logits\n\n        # Store softmax, target-mask and masked-target for backward pass.\n        exp_logits.div_(sum_exp_logits.unsqueeze(dim=-1))\n        ctx.save_for_backward(exp_logits, target_mask, masked_target_1d)\n\n        return loss\n\n    @staticmethod\n    def backward(ctx, grad_output):\n        # Retreive tensors from the forward path.\n        softmax, target_mask, masked_target_1d = ctx.saved_tensors\n\n        # All the inputs have softmax as thier gradient.\n        grad_input = softmax\n        # For simplicity, work with the 2D gradient.\n        partition_vocab_size = softmax.size()[-1]\n        grad_2d = grad_input.view(-1, partition_vocab_size)\n\n        # Add the gradient from matching classes.\n        # arange_1d = torch.arange(start=0, end=grad_2d.size()[0],\n        #                         device=grad_2d.device)\n        # grad_2d[arange_1d, masked_target_1d] -= (\n        #    1.0 - target_mask.view(-1).float())\n        grad_2d.scatter_add_(\n            -1, masked_target_1d.unsqueeze(-1), target_mask.view(-1, 1).float() - 1.0\n        )\n\n        # Finally elementwise multiplication with the output gradients.\n        grad_input.mul_(grad_output.unsqueeze(dim=-1))\n\n        return grad_input, None\n\n\ndef vocab_parallel_cross_entropy(vocab_parallel_logits, target):\n    \"\"\"Helper function for the cross entropy.\"\"\"\n    return _VocabParallelCrossEntropy.apply(vocab_parallel_logits, target)\n",
        "metaseq/modules/megatron/mpu/initialize.py": "# Copyright (c) Meta Platforms, Inc. and affiliates. All Rights Reserved.\n#\n# This source code is licensed under the MIT license found in the\n# LICENSE file in the root directory of this source tree.\n#\n# Taken from:\n# https://github.com/ngoyal2707/Megatron-LM/blob/fa6c0860b62e4ed2ac13a513e7d950d72f576a44/megatron/mpu/initialize.py\n\n\"\"\"Model and data parallel groups.\"\"\"\n\nimport torch\n\nfrom .utils import ensure_divisibility\n\n\n# Intra-layer model parallel group that the current rank belongs to.\n_TENSOR_MODEL_PARALLEL_GROUP = None\n# Inter-layer model parallel group that the current rank belongs to.\n_PIPELINE_MODEL_PARALLEL_GROUP = None\n# Model parallel group (both intra- and pipeline) that the current rank belongs to.\n_MODEL_PARALLEL_GROUP = None\n# Embedding group.\n_EMBEDDING_GROUP = None\n# Position embedding group.\n_POSITION_EMBEDDING_GROUP = None\n# Data parallel group that the current rank belongs to.\n_DATA_PARALLEL_GROUP = None\n\n_VIRTUAL_PIPELINE_MODEL_PARALLEL_RANK = None\n_VIRTUAL_PIPELINE_MODEL_PARALLEL_WORLD_SIZE = None\n_PIPELINE_MODEL_PARALLEL_SPLIT_RANK = None\n\n# These values enable us to change the mpu sizes on the fly.\n_MPU_TENSOR_MODEL_PARALLEL_WORLD_SIZE = None\n_MPU_PIPELINE_MODEL_PARALLEL_WORLD_SIZE = None\n_MPU_TENSOR_MODEL_PARALLEL_RANK = None\n_MPU_PIPELINE_MODEL_PARALLEL_RANK = None\n\n# A list of ranks that have a copy of the embedding.\n_EMBEDDING_GLOBAL_RANKS = None\n\n# A list of ranks that have a copy of the position embedding.\n_POSITION_EMBEDDING_GLOBAL_RANKS = None\n\n# A list of global ranks for each pipeline group to ease calculation of the source\n# rank when broadcasting from the first or last pipeline stage.\n_PIPELINE_GLOBAL_RANKS = None\n\n\ndef initialize_model_parallel(\n    tensor_model_parallel_size_=1,\n    pipeline_model_parallel_size_=1,\n    virtual_pipeline_model_parallel_size_=None,\n    pipeline_model_parallel_split_rank_=None,\n):\n    \"\"\"\n    Initialize model data parallel groups.\n\n    Arguments:\n        tensor_model_parallel_size: number of GPUs used for tensor model parallelism.\n        pipeline_model_parallel_size: number of GPUs used for pipeline model parallelism.\n        virtual_pipeline_model_parallel_size: number of virtual stages (interleaved\n                                              pipeline).\n        pipeline_model_parallel_split_rank: for models with both encoder and decoder,\n                                            rank in pipeline with split point.\n\n\n    Let's say we have a total of 16 GPUs denoted by g0 ... g15 and we\n    use 2 GPUs to parallelize the model tensor, and 4 GPUs to parallelize\n    the model pipeline. The present function will\n    create 8 tensor model-parallel groups, 4 pipeline model-parallel groups\n    and 8 data-parallel groups as:\n        8 data_parallel groups:\n            [g0, g2], [g1, g3], [g4, g6], [g5, g7], [g8, g10], [g9, g11], [g12, g14], [g13, g15]\n        8 tensor model-parallel groups:\n            [g0, g1], [g2, g3], [g4, g5], [g6, g7], [g8, g9], [g10, g11], [g12, g13], [g14, g15]\n        4 pipeline model-parallel groups:\n            [g0, g4, g8, g12], [g1, g5, g9, g13], [g2, g6, g10, g14], [g3, g7, g11, g15]\n    Note that for efficiency, the caller should make sure adjacent ranks\n    are on the same DGX box. For example if we are using 2 DGX-1 boxes\n    with a total of 16 GPUs, rank 0 to 7 belong to the first box and\n    ranks 8 to 15 belong to the second box.\n    \"\"\"\n    if torch.distributed.get_rank() == 0:\n        print(\n            \"> initializing tensor model parallel with size {}\".format(\n                tensor_model_parallel_size_\n            )\n        )\n        print(\n            \"> initializing pipeline model parallel with size {}\".format(\n                pipeline_model_parallel_size_\n            )\n        )\n    # Get world size and rank. Ensure some consistencies.\n    assert torch.distributed.is_initialized()\n    world_size = torch.distributed.get_world_size()\n    tensor_model_parallel_size = min(tensor_model_parallel_size_, world_size)\n    pipeline_model_parallel_size = min(pipeline_model_parallel_size_, world_size)\n    ensure_divisibility(\n        world_size, tensor_model_parallel_size * pipeline_model_parallel_size\n    )\n    data_parallel_size = world_size // (\n        tensor_model_parallel_size * pipeline_model_parallel_size\n    )\n\n    num_tensor_model_parallel_groups = world_size // tensor_model_parallel_size\n    num_pipeline_model_parallel_groups = world_size // pipeline_model_parallel_size\n\n    if virtual_pipeline_model_parallel_size_ is not None:\n        global _VIRTUAL_PIPELINE_MODEL_PARALLEL_RANK\n        global _VIRTUAL_PIPELINE_MODEL_PARALLEL_WORLD_SIZE\n        _VIRTUAL_PIPELINE_MODEL_PARALLEL_RANK = 0\n        _VIRTUAL_PIPELINE_MODEL_PARALLEL_WORLD_SIZE = (\n            virtual_pipeline_model_parallel_size_\n        )\n\n    if pipeline_model_parallel_split_rank_ is not None:\n        global _PIPELINE_MODEL_PARALLEL_SPLIT_RANK\n        _PIPELINE_MODEL_PARALLEL_SPLIT_RANK = pipeline_model_parallel_split_rank_\n\n    rank = torch.distributed.get_rank()\n\n    # Build the data-parallel groups.\n    global _DATA_PARALLEL_GROUP\n    assert _DATA_PARALLEL_GROUP is None, \"data parallel group is already initialized\"\n    all_data_parallel_group_ranks = []\n    for i in range(pipeline_model_parallel_size):\n        start_rank = i * num_pipeline_model_parallel_groups\n        end_rank = (i + 1) * num_pipeline_model_parallel_groups\n        for j in range(tensor_model_parallel_size):\n            ranks = range(start_rank + j, end_rank, tensor_model_parallel_size)\n            all_data_parallel_group_ranks.append(list(ranks))\n            group = torch.distributed.new_group(ranks)\n            if rank in ranks:\n                _DATA_PARALLEL_GROUP = group\n\n    # Build the model-parallel groups.\n    global _MODEL_PARALLEL_GROUP\n    assert _MODEL_PARALLEL_GROUP is None, \"model parallel group is already initialized\"\n    for i in range(data_parallel_size):\n        ranks = [\n            data_parallel_group_ranks[i]\n            for data_parallel_group_ranks in all_data_parallel_group_ranks\n        ]\n        group = torch.distributed.new_group(ranks)\n        if rank in ranks:\n            _MODEL_PARALLEL_GROUP = group\n\n    # Build the tensor model-parallel groups.\n    global _TENSOR_MODEL_PARALLEL_GROUP\n    assert (\n        _TENSOR_MODEL_PARALLEL_GROUP is None\n    ), \"tensor model parallel group is already initialized\"\n    for i in range(num_tensor_model_parallel_groups):\n        ranks = range(\n            i * tensor_model_parallel_size, (i + 1) * tensor_model_parallel_size\n        )\n        group = torch.distributed.new_group(ranks)\n        if rank in ranks:\n            _TENSOR_MODEL_PARALLEL_GROUP = group\n\n    if pipeline_model_parallel_size_ == 1:\n        return\n\n    # Build the pipeline model-parallel groups and embedding groups\n    # (first and last rank in each pipeline model-parallel group).\n    global _PIPELINE_MODEL_PARALLEL_GROUP\n    global _PIPELINE_GLOBAL_RANKS\n    assert (\n        _PIPELINE_MODEL_PARALLEL_GROUP is None\n    ), \"pipeline model parallel group is already initialized\"\n    global _EMBEDDING_GROUP\n    global _EMBEDDING_GLOBAL_RANKS\n    assert _EMBEDDING_GROUP is None, \"embedding group is already initialized\"\n    global _POSITION_EMBEDDING_GROUP\n    global _POSITION_EMBEDDING_GLOBAL_RANKS\n    assert (\n        _POSITION_EMBEDDING_GROUP is None\n    ), \"position embedding group is already initialized\"\n    for i in range(num_pipeline_model_parallel_groups):\n        ranks = range(i, world_size, num_pipeline_model_parallel_groups)\n        group = torch.distributed.new_group(ranks)\n        if rank in ranks:\n            _PIPELINE_MODEL_PARALLEL_GROUP = group\n            _PIPELINE_GLOBAL_RANKS = ranks\n        # Setup embedding group (to exchange gradients between\n        # first and last stages).\n        if len(ranks) > 1:\n            embedding_ranks = [ranks[0], ranks[-1]]\n            position_embedding_ranks = [ranks[0]]\n            if pipeline_model_parallel_split_rank_ is not None:\n                if ranks[pipeline_model_parallel_split_rank_] not in embedding_ranks:\n                    embedding_ranks = [\n                        ranks[0],\n                        ranks[pipeline_model_parallel_split_rank_],\n                        ranks[-1],\n                    ]\n                if (\n                    ranks[pipeline_model_parallel_split_rank_]\n                    not in position_embedding_ranks\n                ):\n                    position_embedding_ranks = [\n                        ranks[0],\n                        ranks[pipeline_model_parallel_split_rank_],\n                    ]\n        else:\n            embedding_ranks = ranks\n            position_embedding_ranks = ranks\n\n        group = torch.distributed.new_group(embedding_ranks)\n        if rank in embedding_ranks:\n            _EMBEDDING_GROUP = group\n        if rank in ranks:\n            _EMBEDDING_GLOBAL_RANKS = embedding_ranks\n\n        group = torch.distributed.new_group(position_embedding_ranks)\n        if rank in position_embedding_ranks:\n            _POSITION_EMBEDDING_GROUP = group\n        if rank in ranks:\n            _POSITION_EMBEDDING_GLOBAL_RANKS = position_embedding_ranks\n\n\ndef get_model_parallel_group():\n    \"\"\"Get the model parallel group the caller rank belongs to.\"\"\"\n    assert _MODEL_PARALLEL_GROUP is not None, \"model parallel group is not initialized\"\n    return _MODEL_PARALLEL_GROUP\n\n\ndef get_tensor_model_parallel_group():\n    \"\"\"Get the tensor model parallel group the caller rank belongs to.\"\"\"\n    assert (\n        _TENSOR_MODEL_PARALLEL_GROUP is not None\n    ), \"intra_layer_model parallel group is not initialized\"\n    return _TENSOR_MODEL_PARALLEL_GROUP\n\n\ndef get_data_parallel_group():\n    \"\"\"Get the data parallel group the caller rank belongs to.\"\"\"\n    assert _DATA_PARALLEL_GROUP is not None, \"data parallel group is not initialized\"\n    return _DATA_PARALLEL_GROUP\n\n\ndef get_tensor_model_parallel_world_size():\n    \"\"\"Return world size for the tensor model parallel group.\"\"\"\n    global _MPU_TENSOR_MODEL_PARALLEL_WORLD_SIZE\n    if _MPU_TENSOR_MODEL_PARALLEL_WORLD_SIZE is not None:\n        return _MPU_TENSOR_MODEL_PARALLEL_WORLD_SIZE\n    return torch.distributed.get_world_size(group=get_tensor_model_parallel_group())\n\n\ndef get_tensor_model_parallel_rank():\n    \"\"\"Return my rank for the tensor model parallel group.\"\"\"\n    global _MPU_TENSOR_MODEL_PARALLEL_RANK\n    if _MPU_TENSOR_MODEL_PARALLEL_RANK is not None:\n        return _MPU_TENSOR_MODEL_PARALLEL_RANK\n    return torch.distributed.get_rank(group=get_tensor_model_parallel_group())\n\n\ndef get_data_parallel_rank():\n    \"\"\"Return my rank for the data parallel group.\"\"\"\n    return torch.distributed.get_rank(group=get_data_parallel_group())\n\n\ndef destroy_model_parallel():\n    \"\"\"Set the groups to none.\"\"\"\n    global _MODEL_PARALLEL_GROUP\n    _MODEL_PARALLEL_GROUP = None\n    global _TENSOR_MODEL_PARALLEL_GROUP\n    _TENSOR_MODEL_PARALLEL_GROUP = None\n    global _PIPELINE_MODEL_PARALLEL_GROUP\n    _PIPELINE_MODEL_PARALLEL_GROUP = None\n    global _DATA_PARALLEL_GROUP\n    _DATA_PARALLEL_GROUP = None\n    global _EMBEDDING_GROUP\n    _EMBEDDING_GROUP = None\n    global _POSITION_EMBEDDING_GROUP\n    _POSITION_EMBEDDING_GROUP = None\n",
        "metaseq/modules/megatron/mpu/layers.py": "# Copyright (c) Meta Platforms, Inc. and affiliates. All Rights Reserved.\n#\n# This source code is licensed under the MIT license found in the\n# LICENSE file in the root directory of this source tree.\n#\n# Taken from:\n# https://github.com/ngoyal2707/Megatron-LM/blob/fa6c0860b62e4ed2ac13a513e7d950d72f576a44/megatron/mpu/layers.py\n\nimport torch\nimport torch.nn.functional as F\nimport torch.nn.init as init\nfrom torch.nn.parameter import Parameter\n\nfrom metaseq.modules.megatron.global_vars import get_global_memory_buffer\nfrom .initialize import (\n    get_tensor_model_parallel_group,\n    get_tensor_model_parallel_rank,\n    get_tensor_model_parallel_world_size,\n)\nfrom .mappings import (\n    copy_to_tensor_model_parallel_region,\n    gather_from_tensor_model_parallel_region,\n    reduce_from_tensor_model_parallel_region,\n    reduce_scatter_to_sequence_parallel_region,\n    scatter_to_tensor_model_parallel_region,\n)\nfrom .random import get_cuda_rng_tracker\nfrom .utils import (\n    VocabUtility,\n    divide,\n)\n\n_MODEL_PARALLEL_ATTRIBUTE_DEFAULTS = {\n    \"tensor_model_parallel\": False,\n    \"partition_dim\": -1,\n    \"partition_stride\": 1,\n}\n\n\ndef set_tensor_model_parallel_attributes(tensor, is_parallel, dim, stride):\n    # Make sure the attributes are not set.\n    for attribute in _MODEL_PARALLEL_ATTRIBUTE_DEFAULTS:\n        assert not hasattr(tensor, attribute)\n    # Set the attributes.\n    setattr(tensor, \"tensor_model_parallel\", is_parallel)\n    setattr(tensor, \"partition_dim\", dim)\n    setattr(tensor, \"partition_stride\", stride)\n\n\ndef _initialize_affine_weight_gpu(weight, init_method, partition_dim, stride=1):\n    \"\"\"Initialize affine weight for model parallel on GPU.\"\"\"\n\n    set_tensor_model_parallel_attributes(\n        tensor=weight, is_parallel=True, dim=partition_dim, stride=stride\n    )\n\n    with get_cuda_rng_tracker().fork():\n        init_method(weight)\n\n\ndef _initialize_affine_weight_cpu(\n    weight,\n    output_size,\n    input_size,\n    per_partition_size,\n    partition_dim,\n    init_method,\n    stride=1,\n    return_master_weight=False,\n):\n    \"\"\"Initialize affine weight for model parallel.\n\n    Build the master weight on all processes and scatter\n    the relevant chunk.\"\"\"\n\n    set_tensor_model_parallel_attributes(\n        tensor=weight, is_parallel=True, dim=partition_dim, stride=stride\n    )\n\n    # Initialize master weight\n    master_weight = torch.empty(\n        output_size, input_size, dtype=torch.float, requires_grad=False\n    )\n    init_method(master_weight)\n    # args = get_args()\n    master_weight = master_weight.to(dtype=torch.float)\n\n    # Split and copy\n    per_partition_per_stride_size = divide(per_partition_size, stride)\n    weight_list = torch.split(\n        master_weight, per_partition_per_stride_size, dim=partition_dim\n    )\n    rank = get_tensor_model_parallel_rank()\n    world_size = get_tensor_model_parallel_world_size()\n    my_weight_list = weight_list[rank::world_size]\n\n    with torch.no_grad():\n        torch.cat(my_weight_list, dim=partition_dim, out=weight)\n    if return_master_weight:\n        return master_weight\n    return None\n\n\ndef _initialize_affine_bias_gpu(bias, init_method):\n    \"\"\"Initialize affine weight for model parallel.\n\n    Build the master weight on all processes and scatter\n    the relevant chunk.\"\"\"\n    # Initialize master weight\n    with get_cuda_rng_tracker().fork():\n        init_method(bias)\n\n\ndef _initialize_affine_bias_cpu(\n    bias,\n    output_size,\n    per_partition_size,\n    partition_dim,\n    init_method,\n    stride=1,\n):\n    \"\"\"Initialize affine weight for model parallel.\n\n    Build the master weight on all processes and scatter\n    the relevant chunk.\"\"\"\n    # Initialize master weight\n    master_bias = torch.empty(output_size, dtype=torch.float, requires_grad=False)\n    init_method(master_bias)\n    master_bias = master_bias.to(dtype=torch.float)\n\n    # Split and copy\n    per_partition_per_stride_size = divide(per_partition_size, stride)\n    bias_list = torch.split(\n        master_bias, per_partition_per_stride_size, dim=partition_dim\n    )\n    rank = get_tensor_model_parallel_rank()\n    world_size = get_tensor_model_parallel_world_size()\n    my_bias_list = bias_list[rank::world_size]\n\n    with torch.no_grad():\n        torch.cat(my_bias_list, dim=partition_dim, out=bias)\n\n\nclass VocabParallelEmbedding(torch.nn.Module):\n    \"\"\"Embedding parallelized in the vocabulary dimension.\n\n    This is mainly adapted from torch.nn.Embedding and all the default\n    values are kept.\n    Arguments:\n        num_embeddings: vocabulary size.\n        embedding_dim: size of hidden state.\n        init_method: method to initialize weights.\n    \"\"\"\n\n    def __init__(\n        self,\n        num_embeddings,\n        embedding_dim,\n        padding_idx,\n        init_method=init.xavier_normal_,\n        use_cpu_initialization=True,\n        dtype=torch.half,\n    ):\n        super(VocabParallelEmbedding, self).__init__()\n        # Keep the input dimensions.\n        self.num_embeddings = num_embeddings\n        self.embedding_dim = embedding_dim\n        # Set the detauls for compatibility.\n        self.padding_idx = padding_idx\n        self.max_norm = None\n        self.norm_type = 2.0\n        self.scale_grad_by_freq = False\n        self.sparse = False\n        self._weight = None\n        self.tensor_model_parallel_size = get_tensor_model_parallel_world_size()\n        # Divide the weight matrix along the vocaburaly dimension.\n        (\n            self.vocab_start_index,\n            self.vocab_end_index,\n        ) = VocabUtility.vocab_range_from_global_vocab_size(\n            self.num_embeddings,\n            get_tensor_model_parallel_rank(),\n            self.tensor_model_parallel_size,\n        )\n        self.num_embeddings_per_partition = (\n            self.vocab_end_index - self.vocab_start_index\n        )\n\n        # Allocate weights and initialize.\n        # args = get_args()\n        if use_cpu_initialization:\n            self.weight = Parameter(\n                torch.empty(\n                    self.num_embeddings_per_partition,\n                    self.embedding_dim,\n                    dtype=dtype,\n                )\n            )\n            _initialize_affine_weight_cpu(\n                self.weight,\n                self.num_embeddings,\n                self.embedding_dim,\n                self.num_embeddings_per_partition,\n                0,\n                init_method,\n            )\n        else:\n            self.weight = Parameter(\n                torch.empty(\n                    self.num_embeddings_per_partition,\n                    self.embedding_dim,\n                    device=torch.cuda.current_device(),\n                    dtype=dtype,\n                )\n            )\n            _initialize_affine_weight_gpu(\n                self.weight, init_method, partition_dim=0, stride=1\n            )\n\n    def forward(self, input_):\n        if self.tensor_model_parallel_size > 1:\n            # Build the mask.\n            input_mask = (input_ < self.vocab_start_index) | (\n                input_ >= self.vocab_end_index\n            )\n            # Mask the input.\n            masked_input = input_.clone() - self.vocab_start_index\n            masked_input.masked_fill_(input_mask, 0.0)\n        else:\n            masked_input = input_\n\n            # Get the embeddings.\n        output_parallel = F.embedding(\n            masked_input,\n            self.weight,\n            self.padding_idx,\n            self.max_norm,\n            self.norm_type,\n            self.scale_grad_by_freq,\n            self.sparse,\n        )\n\n        # Mask the output embedding.\n        if self.tensor_model_parallel_size > 1:\n            output_parallel.masked_fill_(input_mask.unsqueeze(-1), 0.0)\n\n        # Reduce across all the model parallel GPUs.\n        output = reduce_from_tensor_model_parallel_region(output_parallel)\n\n        return output\n\n\nclass LinearWithGradAccumulationAndAsyncCommunication(torch.autograd.Function):\n    \"\"\"\n    Linear layer execution with asynchronous communication and gradient accumulation\n    fusion in backprop.\n    \"\"\"\n\n    @staticmethod\n    def forward(\n        ctx,\n        input,\n        weight,\n        bias,\n        gradient_accumulation_fusion,\n        async_grad_allreduce,\n        sequence_parallel,\n    ):\n        ctx.save_for_backward(input, weight)\n        ctx.use_bias = bias is not None\n        ctx.gradient_accumulation_fusion = gradient_accumulation_fusion\n        ctx.async_grad_allreduce = async_grad_allreduce\n        ctx.sequence_parallel = sequence_parallel\n\n        if sequence_parallel:\n            world_size = get_tensor_model_parallel_world_size()\n            dim_size = list(input.size())\n            dim_size[0] = dim_size[0] * world_size\n\n            all_gather_buffer = get_global_memory_buffer().get_tensor(\n                dim_size, input.dtype, \"mpu\"\n            )\n            torch.distributed._all_gather_base(\n                all_gather_buffer, input, group=get_tensor_model_parallel_group()\n            )\n            total_input = all_gather_buffer\n        else:\n            total_input = input\n\n        output = torch.matmul(total_input, weight.t())\n        if bias is not None:\n            output = output + bias\n        return output\n\n    @staticmethod\n    def backward(ctx, grad_output):\n        input, weight = ctx.saved_tensors\n        use_bias = ctx.use_bias\n        # Most times its a no-op, but sometimes it is required,\n        # if user transposed the output of forward.\n        grad_output = grad_output.contiguous()\n        if ctx.sequence_parallel:\n            world_size = get_tensor_model_parallel_world_size()\n            dim_size = list(input.size())\n            dim_size[0] = dim_size[0] * world_size\n\n            all_gather_buffer = get_global_memory_buffer().get_tensor(\n                dim_size, input.dtype, \"mpu\"\n            )\n            handle = torch.distributed._all_gather_base(\n                all_gather_buffer,\n                input,\n                group=get_tensor_model_parallel_group(),\n                async_op=True,\n            )\n\n            # Delay the start of intput gradient computation shortly (3us) to have\n            # gather scheduled first and have GPU resources allocated\n            _ = torch.empty(1, device=grad_output.device) + 1\n            total_input = all_gather_buffer\n        else:\n            total_input = input\n        grad_input = grad_output.matmul(weight)\n\n        if ctx.sequence_parallel:\n            handle.wait()\n\n        # Convert the tensor shapes to 2D for execution compatibility\n        grad_output = grad_output.view(\n            grad_output.shape[0] * grad_output.shape[1], grad_output.shape[2]\n        )\n        total_input = total_input.view(\n            total_input.shape[0] * total_input.shape[1], total_input.shape[2]\n        )\n\n        if ctx.async_grad_allreduce:\n            # Asynchronous all-reduce\n            handle = torch.distributed.all_reduce(\n                grad_input, group=get_tensor_model_parallel_group(), async_op=True\n            )\n            # Delay the start of weight gradient computation shortly (3us) to have\n            # all-reduce scheduled first and have GPU resources allocated\n            _ = torch.empty(1, device=grad_output.device) + 1\n\n        if ctx.sequence_parallel:\n            assert not ctx.async_grad_allreduce\n            dim_size = list(input.size())\n            sub_grad_input = torch.empty(\n                dim_size,\n                dtype=input.dtype,\n                device=torch.cuda.current_device(),\n                requires_grad=False,\n            )\n            # reduce_scatter\n            handle = torch.distributed._reduce_scatter_base(\n                sub_grad_input,\n                grad_input,\n                group=get_tensor_model_parallel_group(),\n                async_op=True,\n            )\n            # Delay the start of weight gradient computation shortly (3us) to have\n            # reduce scatter scheduled first and have GPU resources allocated\n            _ = torch.empty(1, device=grad_output.device) + 1\n\n        if ctx.gradient_accumulation_fusion:\n            import fused_dense_cuda\n\n            fused_dense_cuda.wgrad_gemm_accum_fp32(\n                total_input, grad_output, weight.main_grad\n            )\n            grad_weight = None\n        else:\n            grad_weight = grad_output.t().matmul(total_input)\n        grad_bias = grad_output.sum(dim=0) if use_bias else None\n\n        if ctx.sequence_parallel:\n            handle.wait()\n            return sub_grad_input, grad_weight, grad_bias, None, None, None, None\n\n        if ctx.async_grad_allreduce:\n            handle.wait()\n\n        return grad_input, grad_weight, grad_bias, None, None, None, None\n\n\nclass ColumnParallelLinear(torch.nn.Module):\n    \"\"\"Linear layer with column parallelism.\n\n    The linear layer is defined as Y = XA + b. A is parallelized along\n    its second dimension as A = [A_1, ..., A_p].\n\n    Arguments:\n        input_size: first dimension of matrix A.\n        output_size: second dimension of matrix A.\n        bias: If true, add bias\n        gather_output: If true, call all-gather on output and make Y available\n                       to all GPUs, otherwise, every GPU will have its output\n                       which is Y_i = XA_i\n        init_method: method to initialize weights. Note that bias is always set\n                     to zero.\n        stride: For the strided linear layers.\n        keep_master_weight_for_test: This was added for testing and should be\n                                     set to False. It returns the master weights\n                                     used for initialization.\n        skip_bias_add: This was added to enable performance optimations where bias\n                       can be fused with other elementwise operations. we skip\n                       adding bias but instead return it.\n    \"\"\"\n\n    def __init__(\n        self,\n        input_size,\n        output_size,\n        bias=True,\n        gather_output=True,\n        init_method=init.xavier_normal_,\n        stride=1,\n        keep_master_weight_for_test=False,\n        skip_bias_add=False,\n        use_cpu_initialization=True,\n        no_async_tensor_model_parallel_allreduce=True,\n        init_method_bias=None,\n        dtype=torch.half,\n        sequence_parallel=False,\n        gradient_accumulation_fusion=False,\n    ):\n        super(ColumnParallelLinear, self).__init__()\n\n        # Keep input parameters\n        self.input_size = input_size\n        self.output_size = output_size\n        self.gather_output = gather_output\n        # Divide the weight matrix along the last dimension.\n        world_size = get_tensor_model_parallel_world_size()\n        self.output_size_per_partition = divide(output_size, world_size)\n        self.skip_bias_add = skip_bias_add\n\n        # Parameters.\n        # Note: torch.nn.functional.linear performs XA^T + b and as a result\n        # we allocate the transpose.\n        # Initialize weight.\n        # args = get_args()\n        if use_cpu_initialization:\n            self.weight = Parameter(\n                torch.empty(\n                    self.output_size_per_partition, self.input_size, dtype=dtype\n                )\n            )\n            self.master_weight = _initialize_affine_weight_cpu(\n                self.weight,\n                self.output_size,\n                self.input_size,\n                self.output_size_per_partition,\n                0,\n                init_method,\n                stride=stride,\n                return_master_weight=keep_master_weight_for_test,\n            )\n        else:\n            self.weight = Parameter(\n                torch.empty(\n                    self.output_size_per_partition,\n                    self.input_size,\n                    device=torch.cuda.current_device(),\n                    dtype=dtype,\n                )\n            )\n            _initialize_affine_weight_gpu(\n                self.weight, init_method, partition_dim=0, stride=stride\n            )\n\n        if bias:\n            if use_cpu_initialization:\n                self.bias = Parameter(\n                    torch.empty(self.output_size_per_partition, dtype=dtype)\n                )\n            else:\n                self.bias = Parameter(\n                    torch.empty(\n                        self.output_size_per_partition,\n                        device=torch.cuda.current_device(),\n                        dtype=dtype,\n                    )\n                )\n            set_tensor_model_parallel_attributes(self.bias, True, 0, stride)\n\n            if init_method_bias is not None:\n                if use_cpu_initialization:\n                    _initialize_affine_bias_cpu(\n                        self.bias,\n                        self.output_size,\n                        self.output_size_per_partition,\n                        0,\n                        init_method_bias,\n                    )\n                else:\n                    _initialize_affine_bias_gpu(self.bias, init_method_bias)\n            else:\n                # Always initialize bias to zero.\n                with torch.no_grad():\n                    self.bias.zero_()\n        else:\n            self.register_parameter(\"bias\", None)\n        self.async_tensor_model_parallel_allreduce = (\n            not no_async_tensor_model_parallel_allreduce and world_size > 1\n        )\n        self.sequence_parallel = sequence_parallel and world_size > 1\n        assert (\n            not self.async_tensor_model_parallel_allreduce or not self.sequence_parallel\n        )\n        self.gradient_accumulation_fusion = gradient_accumulation_fusion\n\n    def forward(self, input_):\n        bias = self.bias if not self.skip_bias_add else None\n\n        if self.async_tensor_model_parallel_allreduce or self.sequence_parallel:\n            input_parallel = input_\n        else:\n            input_parallel = copy_to_tensor_model_parallel_region(input_)\n        # Matrix multiply.\n        output_parallel = LinearWithGradAccumulationAndAsyncCommunication.apply(\n            input_parallel,\n            self.weight,\n            bias,\n            self.gradient_accumulation_fusion,\n            self.async_tensor_model_parallel_allreduce,\n            self.sequence_parallel,\n        )\n        if self.gather_output:\n            # All-gather across the partitions.\n            assert not self.sequence_parallel\n            output = gather_from_tensor_model_parallel_region(output_parallel)\n        else:\n            output = output_parallel\n        output_bias = self.bias if self.skip_bias_add else None\n        return output, output_bias\n\n\nclass RowParallelLinear(torch.nn.Module):\n    \"\"\"Linear layer with row parallelism.\n\n    The linear layer is defined as Y = XA + b. A is parallelized along\n    its first dimension and X along its second dimension as:\n               -   -\n              | A_1 |\n              | .   |\n          A = | .   |        X = [X_1, ..., X_p]\n              | .   |\n              | A_p |\n               -   -\n    Arguments:\n        input_size: first dimension of matrix A.\n        output_size: second dimension of matrix A.\n        bias: If true, add bias. Note that bias is not parallelized.\n        input_is_parallel: If true, we assume that the input is already\n                           split across the GPUs and we do not split\n                           again.\n        init_method: method to initialize weights. Note that bias is always set\n                     to zero.\n        stride: For the strided linear layers.\n        keep_master_weight_for_test: This was added for testing and should be\n                                     set to False. It returns the master weights\n                                     used for initialization.\n        skip_bias_add: This was added to enable performance optimization where bias\n                       can be fused with other elementwise operations. We skip\n                       adding bias but instead return it.\n    \"\"\"\n\n    def __init__(\n        self,\n        input_size,\n        output_size,\n        bias=True,\n        input_is_parallel=False,\n        init_method=init.xavier_normal_,\n        stride=1,\n        keep_master_weight_for_test=False,\n        skip_bias_add=False,\n        use_cpu_initialization=True,\n        dtype=torch.half,\n        sequence_parallel=False,\n        gradient_accumulation_fusion=False,\n    ):\n        super(RowParallelLinear, self).__init__()\n        # Keep input parameters\n        self.input_size = input_size\n        self.output_size = output_size\n        self.input_is_parallel = input_is_parallel\n        # Divide the weight matrix along the last dimension.\n        world_size = get_tensor_model_parallel_world_size()\n        self.input_size_per_partition = divide(input_size, world_size)\n        self.skip_bias_add = skip_bias_add\n\n        # Parameters.\n        # Note: torch.nn.functional.linear performs XA^T + b and as a result\n        # we allocate the transpose.\n        # Initialize weight.\n        # args = get_args()\n        if use_cpu_initialization:\n            self.weight = Parameter(\n                torch.empty(\n                    self.output_size,\n                    self.input_size_per_partition,\n                    dtype=dtype,\n                )\n            )\n            self.master_weight = _initialize_affine_weight_cpu(\n                self.weight,\n                self.output_size,\n                self.input_size,\n                self.input_size_per_partition,\n                1,\n                init_method,\n                stride=stride,\n                return_master_weight=keep_master_weight_for_test,\n            )\n        else:\n            self.weight = Parameter(\n                torch.empty(\n                    self.output_size,\n                    self.input_size_per_partition,\n                    device=torch.cuda.current_device(),\n                    dtype=dtype,\n                )\n            )\n            _initialize_affine_weight_gpu(\n                self.weight, init_method, partition_dim=1, stride=stride\n            )\n        if bias:\n            if use_cpu_initialization:\n                self.bias = Parameter(torch.empty(self.output_size, dtype=dtype))\n            else:\n                self.bias = Parameter(\n                    torch.empty(\n                        self.output_size,\n                        device=torch.cuda.current_device(),\n                        dtype=dtype,\n                    )\n                )\n            setattr(self.bias, \"sequence_parallel\", sequence_parallel)\n            # Always initialize bias to zero.\n            with torch.no_grad():\n                self.bias.zero_()\n        else:\n            self.register_parameter(\"bias\", None)\n        self.sequence_parallel = sequence_parallel\n        self.gradient_accumulation_fusion = gradient_accumulation_fusion\n\n    def forward(self, input_):\n        # Set up backprop all-reduce.\n        if self.input_is_parallel:\n            input_parallel = input_\n        else:\n            assert not self.sequence_parallel\n            input_parallel = scatter_to_tensor_model_parallel_region(input_)\n        # Matrix multiply.\n        output_parallel = LinearWithGradAccumulationAndAsyncCommunication.apply(\n            input_parallel,\n            self.weight,\n            None,\n            self.gradient_accumulation_fusion,\n            None,\n            None,\n        )\n        # All-reduce across all the partitions.\n        if self.sequence_parallel:\n            output_ = reduce_scatter_to_sequence_parallel_region(output_parallel)\n        else:\n            output_ = reduce_from_tensor_model_parallel_region(output_parallel)\n        if not self.skip_bias_add:\n            output = output_ + self.bias if self.bias is not None else output_\n            output_bias = None\n        else:\n            output = output_\n            output_bias = self.bias\n        return output, output_bias\n",
        "metaseq/modules/megatron/mpu/mappings.py": "# Copyright (c) Meta Platforms, Inc. and affiliates. All Rights Reserved.\n#\n# This source code is licensed under the MIT license found in the\n# LICENSE file in the root directory of this source tree.\n#\n# Taken from:\n# https://github.com/ngoyal2707/Megatron-LM/blob/fa6c0860b62e4ed2ac13a513e7d950d72f576a44/megatron/mpu/mappings.py\n\nimport os\n\nimport torch\n\nfrom metaseq.modules.megatron.global_vars import get_global_memory_buffer\nfrom .initialize import (\n    get_tensor_model_parallel_group,\n    get_tensor_model_parallel_world_size,\n    get_tensor_model_parallel_rank,\n)\nfrom .utils import split_tensor_along_last_dim\n\nif os.getenv(\"SET_ALL_REDUCE_DUMMY_VALUE\", \"0\") == \"1\":\n    set_all_reduce_dummy_value = True\nelse:\n    set_all_reduce_dummy_value = False\n\n\ndef _reduce(input_):\n    \"\"\"All-reduce the input tensor across model parallel group.\"\"\"\n\n    # Bypass the function if we are using only 1 GPU.\n    if get_tensor_model_parallel_world_size() == 1:\n        return input_\n    if set_all_reduce_dummy_value:\n        input_ = input_.float().half()\n    # All-reduce.\n    torch.distributed.all_reduce(input_, group=get_tensor_model_parallel_group())\n\n    return input_\n\n\ndef _split_along_last_dim(input_):\n    \"\"\"Split the tensor along its last dimension and keep the\n    corresponding slice.\"\"\"\n\n    world_size = get_tensor_model_parallel_world_size()\n    # Bypass the function if we are using only 1 GPU.\n    if world_size == 1:\n        return input_\n\n    # Split along last dimension.\n    input_list = split_tensor_along_last_dim(input_, world_size)\n\n    # Note: torch.split does not create contiguous tensors by default.\n    rank = get_tensor_model_parallel_rank()\n    output = input_list[rank].contiguous()\n\n    return output\n\n\ndef _split_along_first_dim(input_):\n    \"\"\"Split the tensor along its first dimension and keep the\n    corresponding slice.\"\"\"\n\n    world_size = get_tensor_model_parallel_world_size()\n    # Bypass the function if we are using only 1 GPU.\n    if world_size == 1:\n        return input_\n\n    # Split along first dimension.\n    dim_size = input_.size()[0]\n    assert (\n        dim_size % world_size == 0\n    ), \"First dimension of the tensor should be divisible by tensor parallel size\"\n    local_dim_size = dim_size // world_size\n    rank = get_tensor_model_parallel_rank()\n    dim_offset = rank * local_dim_size\n\n    output = input_[dim_offset : dim_offset + local_dim_size].contiguous()\n\n    return output\n\n\ndef _gather_along_last_dim(input_):\n    \"\"\"Gather tensors and concatinate along the last dimension.\"\"\"\n\n    world_size = get_tensor_model_parallel_world_size()\n    # Bypass the function if we are using only 1 GPU.\n    if world_size == 1:\n        return input_\n\n    # Size and dimension.\n    last_dim = input_.dim() - 1\n    rank = get_tensor_model_parallel_rank()\n\n    tensor_list = [torch.empty_like(input_) for _ in range(world_size)]\n    tensor_list[rank] = input_\n    torch.distributed.all_gather(\n        tensor_list, input_, group=get_tensor_model_parallel_group()\n    )\n\n    # Note: torch.cat already creates a contiguous tensor.\n    output = torch.cat(tensor_list, dim=last_dim).contiguous()\n\n    return output\n\n\ndef _gather_along_first_dim(input_, async_op=False, cached_buffer_name=None):\n    \"\"\"Gather tensors and concatinate along the first dimension.\"\"\"\n\n    world_size = get_tensor_model_parallel_world_size()\n    # Bypass the function if we are using only 1 GPU.\n    if world_size == 1:\n        return input_\n\n    dim_size = list(input_.size())\n    dim_size[0] = dim_size[0] * world_size\n\n    if cached_buffer_name is None:\n        output = torch.empty(\n            dim_size, dtype=input_.dtype, device=torch.cuda.current_device()\n        )\n    else:\n        output = get_global_memory_buffer().get_tensor(\n            dim_size, input_.dtype, cached_buffer_name\n        )\n    handle = torch.distributed._all_gather_base(\n        output,\n        input_.contiguous(),\n        group=get_tensor_model_parallel_group(),\n        async_op=async_op,\n    )\n\n    if async_op:\n        # Note: [Naman] I am still not sure if this is needed but original code\n        # for sequence_parallel had it, so for now keeping it.\n        # Delay the start of weight gradient computation shortly (3us) to have\n        # reduce scatter scheduled first and have GPU resources allocated\n        _ = torch.empty(1, device=input_.device) + 1\n        return output, handle\n\n    return output\n\n\ndef _reduce_scatter_along_first_dim(input_, async_op=False):\n    \"\"\"Reduce-scatter the input tensor across model parallel group.\"\"\"\n    world_size = get_tensor_model_parallel_world_size()\n    # Bypass the function if we are using only 1 GPU.\n    if world_size == 1:\n        return input_\n\n    dim_size = list(input_.size())\n    assert (\n        dim_size[0] % world_size == 0\n    ), \"First dimension of the tensor should be divisible by tensor parallel size\"\n\n    dim_size[0] = dim_size[0] // world_size\n\n    output = torch.empty(\n        dim_size, dtype=input_.dtype, device=torch.cuda.current_device()\n    )\n    handle = torch.distributed._reduce_scatter_base(\n        output,\n        input_.contiguous(),\n        group=get_tensor_model_parallel_group(),\n        async_op=async_op,\n    )\n\n    if async_op:\n        # Note: [Naman] I am still not sure if this is needed but original code\n        # for sequence_parallel had it, so for now keeping it.\n        # Delay the start of weight gradient computation shortly (3us) to have\n        # reduce scatter scheduled first and have GPU resources allocated\n        _ = torch.empty(1, device=input_.device) + 1\n        return output, handle\n\n    return output\n\n\nclass _CopyToModelParallelRegion(torch.autograd.Function):\n    \"\"\"Pass the input to the model parallel region.\"\"\"\n\n    @staticmethod\n    def symbolic(graph, input_):\n        return input_\n\n    @staticmethod\n    def forward(ctx, input_):\n        return input_\n\n    @staticmethod\n    def backward(ctx, grad_output):\n        return _reduce(grad_output)\n\n\nclass _ReduceFromModelParallelRegion(torch.autograd.Function):\n    \"\"\"All-reduce the input from the model parallel region.\"\"\"\n\n    @staticmethod\n    def symbolic(graph, input_):\n        return _reduce(input_)\n\n    @staticmethod\n    def forward(ctx, input_):\n        ctx.mark_dirty(input_)\n        return _reduce(input_)\n\n    @staticmethod\n    def backward(ctx, grad_output):\n        return grad_output\n\n\nclass _ScatterToModelParallelRegion(torch.autograd.Function):\n    \"\"\"Split the input and keep only the corresponding chuck to the rank.\"\"\"\n\n    @staticmethod\n    def symbolic(graph, input_):\n        return _split_along_last_dim(input_)\n\n    @staticmethod\n    def forward(ctx, input_):\n        return _split_along_last_dim(input_)\n\n    @staticmethod\n    def backward(ctx, grad_output):\n        return _gather_along_last_dim(grad_output)\n\n\nclass _GatherFromModelParallelRegion(torch.autograd.Function):\n    \"\"\"Gather the input from model parallel region and concatinate.\"\"\"\n\n    @staticmethod\n    def symbolic(graph, input_):\n        return _gather_along_last_dim(input_)\n\n    @staticmethod\n    def forward(ctx, input_):\n        return _gather_along_last_dim(input_)\n\n    @staticmethod\n    def backward(ctx, grad_output):\n        return _split_along_last_dim(grad_output)\n\n\nclass _ScatterToSequenceParallelRegion(torch.autograd.Function):\n    \"\"\"Split the input and keep only the corresponding chuck to the rank.\"\"\"\n\n    @staticmethod\n    def symbolic(graph, input_):\n        return _split_along_first_dim(input_)\n\n    @staticmethod\n    def forward(ctx, input_):\n        return _split_along_first_dim(input_)\n\n    @staticmethod\n    def backward(ctx, grad_output):\n        return _gather_along_first_dim(grad_output)\n\n\nclass _GatherFromSequenceParallelRegion(torch.autograd.Function):\n    \"\"\"Gather the input from sequence parallel region and concatinate.\"\"\"\n\n    @staticmethod\n    def symbolic(graph, input_, tensor_parallel_output_grad=True):\n        return _gather_along_first_dim(input_)\n\n    @staticmethod\n    def forward(ctx, input_, tensor_parallel_output_grad=True):\n        ctx.tensor_parallel_output_grad = tensor_parallel_output_grad\n        return _gather_along_first_dim(input_)\n\n    @staticmethod\n    def backward(ctx, grad_output):\n        tensor_parallel_output_grad = ctx.tensor_parallel_output_grad\n\n        # If the computation graph after the gather operation is\n        # in the tensor parallel mode, output gradients need to reduce\n        # scattered and whereas if the computation is duplicated,\n        # output gradients need to be scattered.\n        if tensor_parallel_output_grad:\n            return _reduce_scatter_along_first_dim(grad_output), None\n        else:\n            return _split_along_first_dim(grad_output), None\n\n\nclass _ReduceScatterToSequenceParallelRegion(torch.autograd.Function):\n    \"\"\"Reduce scatter the input from the model parallel region.\"\"\"\n\n    @staticmethod\n    def symbolic(graph, input_):\n        return _reduce_scatter_along_first_dim(input_)\n\n    @staticmethod\n    def forward(ctx, input_):\n        return _reduce_scatter_along_first_dim(input_)\n\n    @staticmethod\n    def backward(ctx, grad_output):\n        return _gather_along_first_dim(grad_output)\n\n\n# -----------------\n# Helper functions.\n# -----------------\n\n\ndef copy_to_tensor_model_parallel_region(input_):\n    return _CopyToModelParallelRegion.apply(input_)\n\n\ndef reduce_from_tensor_model_parallel_region(input_):\n    return _ReduceFromModelParallelRegion.apply(input_)\n\n\ndef scatter_to_tensor_model_parallel_region(input_):\n    return _ScatterToModelParallelRegion.apply(input_)\n\n\ndef gather_from_tensor_model_parallel_region(input_):\n    return _GatherFromModelParallelRegion.apply(input_)\n\n\ndef scatter_to_sequence_parallel_region(input_):\n    return _ScatterToSequenceParallelRegion.apply(input_)\n\n\ndef gather_from_sequence_parallel_region(input_, tensor_parallel_output_grad=True):\n    return _GatherFromSequenceParallelRegion.apply(input_, tensor_parallel_output_grad)\n\n\ndef reduce_scatter_to_sequence_parallel_region(input_):\n    return _ReduceScatterToSequenceParallelRegion.apply(input_)\n",
        "metaseq/modules/megatron/mpu/random.py": "# Copyright (c) Meta Platforms, Inc. and affiliates. All Rights Reserved.\n#\n# This source code is licensed under the MIT license found in the\n# LICENSE file in the root directory of this source tree.\n#\n# Taken from:\n# https://github.com/ngoyal2707/Megatron-LM/blob/fa6c0860b62e4ed2ac13a513e7d950d72f576a44/megatron/mpu/random.py\n\nimport contextlib\n\nimport torch\nfrom torch import _C\nfrom torch.cuda import _lazy_call, device as device_ctx_manager\n\nfrom .initialize import (\n    get_data_parallel_rank,\n    get_tensor_model_parallel_group,\n    get_tensor_model_parallel_rank,\n    get_tensor_model_parallel_world_size,\n)\n\n# Default name for the model parallel rng tracker.\n_MODEL_PARALLEL_RNG_TRACKER_NAME = \"model-parallel-rng\"\n\n\ndef _set_cuda_rng_state(new_state, device=-1):\n    \"\"\"Sets the random number generator state of the current GPU.\n\n    Argumentss:\n        new_state (torch.ByteTensor): The desired state\n    This function is adapted from PyTorch repo (torch.cuda.set_rng_state)\n    with a single change: the input state is not cloned. Cloning caused\n    major performance issues for +4 GPU cases.\n    \"\"\"\n    if hasattr(_C, \"_cuda_setRNGState\") and callable(_C._cuda_setRNGState):\n        # older PyTorch\n        def cb():\n            with device_ctx_manager(device):\n                _C._cuda_setRNGState(new_state)\n\n    else:\n        # newer PyTorch\n        if device == -1:\n            device = torch.device(\"cuda\")\n        elif isinstance(device, str):\n            device = torch.device(device)\n        elif isinstance(device, int):\n            device = torch.device(\"cuda\", device)\n\n        def cb():\n            idx = device.index\n            if idx is None:\n                idx = torch.cuda.current_device()\n            default_generator = torch.cuda.default_generators[idx]\n            default_generator.set_state(new_state)\n\n    _lazy_call(cb)\n\n\ndef split_tensor_into_1d_equal_chunks(tensor, new_buffer=False):\n    \"\"\"Break a tensor into equal 1D chunks.\"\"\"\n    partition_size = torch.numel(tensor) // get_tensor_model_parallel_world_size()\n    start_index = partition_size * get_tensor_model_parallel_rank()\n    end_index = start_index + partition_size\n    if new_buffer:\n        data = torch.empty(\n            partition_size,\n            dtype=tensor.dtype,\n            device=torch.cuda.current_device(),\n            requires_grad=False,\n        )\n        data.copy_(tensor.view(-1)[start_index:end_index])\n    else:\n        data = tensor.view(-1)[start_index:end_index]\n    return data\n\n\ndef gather_split_1d_tensor(tensor):\n    \"\"\"Opposite of above function, gather values from model parallel ranks.\"\"\"\n    numel_gathered = torch.numel(tensor) * get_tensor_model_parallel_world_size()\n    gathered = torch.empty(\n        numel_gathered,\n        dtype=tensor.dtype,\n        device=torch.cuda.current_device(),\n        requires_grad=False,\n    )\n    # TODO: This API is experimental in pytorch (as of Feb 2022) and\n    # this might break in future pytorch releases. We chose this API\n    # as opposed to torch.distributed.all_gather for efficiency reasons.\n    # This API calls directly NCCL all-gather versus the former does\n    # internal copies and can potentially cause slow down.\n    torch.distributed._all_gather_base(\n        gathered, tensor, group=get_tensor_model_parallel_group()\n    )\n    return gathered\n\n\nclass CudaRNGStatesTracker:\n    \"\"\"Tracker for the cuda RNG states.\n\n    Using the `add` method, a cuda rng state is initialized based on\n    the input `seed` and is assigned to `name`. Later, by forking the\n    rng state, we can perform operations and return to our starting\n    cuda state.\n    \"\"\"\n\n    def __init__(self):\n        # Map from a string name to the cuda rng state.\n        self.states_ = {}\n        # Seeds are just for book keeping and ensure no seed is set twice.\n        self.seeds_ = set()\n\n    def reset(self):\n        \"\"\"Set to the initial state (no tracker).\"\"\"\n        self.states_ = {}\n        self.seeds_ = set()\n\n    def get_states(self):\n        \"\"\"Get rng states. Copy the dictionary so we have direct\n        pointers to the states, not just a pointer to the dictionary.\"\"\"\n        states = {}\n        for name in self.states_:\n            states[name] = self.states_[name]\n        return states\n\n    def set_states(self, states):\n        \"\"\"Set the rng states. For efficiency purposes, we do not check\n        the size of seed for compatibility.\"\"\"\n        self.states_ = states\n\n    def add(self, name, seed):\n        \"\"\"Track the rng state.\"\"\"\n        # Check seed is not already used.\n        if seed in self.seeds_:\n            raise Exception(\"seed {} already exists\".format(seed))\n        self.seeds_.add(seed)\n        # Check that state is not already defined.\n        if name in self.states_:\n            raise Exception(\"cuda rng state {} already exists\".format(name))\n        # Get the current rng state.\n        orig_rng_state = torch.cuda.get_rng_state()\n        # Set the new state and store it.\n        torch.cuda.manual_seed(seed)\n        self.states_[name] = torch.cuda.get_rng_state()\n        # Reset rng state to what it was.\n        _set_cuda_rng_state(orig_rng_state)\n\n    @contextlib.contextmanager\n    def fork(self, name=_MODEL_PARALLEL_RNG_TRACKER_NAME):\n        \"\"\"Fork the cuda rng state, perform operations, and exit with\n        the original state.\"\"\"\n        # Check if we have added the state\n        if name not in self.states_:\n            raise Exception(\"cuda rng state {} is not added\".format(name))\n        # Store current rng state.\n        orig_cuda_rng_state = torch.cuda.get_rng_state()\n        # Set rng state to the desired one\n        _set_cuda_rng_state(self.states_[name])\n        # Do the stuff we wanted to do.\n        try:\n            yield\n        finally:\n            # Update the current rng state for later use.\n            self.states_[name] = torch.cuda.get_rng_state()\n            # And set the state to the original state we started with.\n            _set_cuda_rng_state(orig_cuda_rng_state)\n\n\n# RNG tracker object.\n_CUDA_RNG_STATE_TRACKER = CudaRNGStatesTracker()\n\n\ndef get_cuda_rng_tracker():\n    \"\"\"Get cuda rng tracker.\"\"\"\n    return _CUDA_RNG_STATE_TRACKER\n\n\ndef model_parallel_cuda_manual_seed(seed):\n    \"\"\"Initialize model parallel cuda seed.\n\n    This function should be called after the model parallel is\n    initialized. Also, no torch.cuda.manual_seed should be called\n    after this function. Basically, this is replacement for that\n    function.\n    Two set of RNG states are tracked:\n        default state: This is for data parallelism and is the same among a\n                       set of model parallel GPUs but different across\n                       different model paralle groups. This is used for\n                       example for dropout in the non-tensor-model-parallel regions.\n        tensor-model-parallel state: This state is different among a set of model\n                              parallel GPUs, but the same across data parallel\n                              groups. This is used for example for dropout in\n                              model parallel regions.\n    \"\"\"\n    # 2718 is just for fun and any POSITIVE value will work.\n    offset = seed + 2718\n    tensor_model_parallel_seed = offset + get_tensor_model_parallel_rank()\n    # Data parallel gets the original seed.\n    data_parallel_seed = seed\n\n    if torch.distributed.get_rank() == 0:\n        print(\n            \"> initializing model parallel cuda seeds on global rank {}, \"\n            \"model parallel rank {}, and data parallel rank {} with \"\n            \"model parallel seed: {} and data parallel seed: {}\".format(\n                torch.distributed.get_rank(),\n                get_tensor_model_parallel_rank(),\n                get_data_parallel_rank(),\n                tensor_model_parallel_seed,\n                data_parallel_seed,\n            ),\n            flush=True,\n        )\n    _CUDA_RNG_STATE_TRACKER.reset()\n    # Set the default state.\n    torch.cuda.manual_seed(data_parallel_seed)\n    # and model parallel state.\n    _CUDA_RNG_STATE_TRACKER.add(\n        _MODEL_PARALLEL_RNG_TRACKER_NAME, tensor_model_parallel_seed\n    )\n",
        "metaseq/modules/megatron/mpu/utils.py": "# Copyright (c) Meta Platforms, Inc. and affiliates. All Rights Reserved.\n#\n# This source code is licensed under the MIT license found in the\n# LICENSE file in the root directory of this source tree.\n#\n# Taken from:\n# https://github.com/ngoyal2707/Megatron-LM/blob/fa6c0860b62e4ed2ac13a513e7d950d72f576a44/megatron/mpu/utils.py\n\n\nimport torch\n\n\ndef ensure_divisibility(numerator, denominator):\n    \"\"\"Ensure that numerator is divisible by the denominator.\"\"\"\n    assert numerator % denominator == 0, \"{} is not divisible by {}\".format(\n        numerator, denominator\n    )\n\n\ndef divide(numerator, denominator):\n    \"\"\"Ensure that numerator is divisible by the denominator and return\n    the division value.\"\"\"\n    ensure_divisibility(numerator, denominator)\n    return numerator // denominator\n\n\ndef split_tensor_along_last_dim(tensor, num_partitions, contiguous_split_chunks=False):\n    \"\"\"Split a tensor along its last dimension.\n    Arguments:\n        tensor: input tensor.\n        num_partitions: number of partitions to split the tensor\n        contiguous_split_chunks: If True, make each chunk contiguous\n                                 in memory.\n    \"\"\"\n    # Get the size and dimension.\n    last_dim = tensor.dim() - 1\n    last_dim_size = divide(tensor.size()[last_dim], num_partitions)\n    # Split.\n    tensor_list = torch.split(tensor, last_dim_size, dim=last_dim)\n    # Note: torch.split does not create contiguous tensors by default.\n    if contiguous_split_chunks:\n        return tuple(chunk.contiguous() for chunk in tensor_list)\n\n    return tensor_list\n\n\nclass VocabUtility:\n    \"\"\"Split the vocabulary into `world_size` chunks amd return the\n    first and last index of the vocabulary belonging to the `rank`\n    partition: Note that indicies in [fist, last)\"\"\"\n\n    @staticmethod\n    def vocab_range_from_per_partition_vocab_size(per_partition_vocab_size, rank):\n        index_f = rank * per_partition_vocab_size\n        index_l = index_f + per_partition_vocab_size\n        return index_f, index_l\n\n    @staticmethod\n    def vocab_range_from_global_vocab_size(global_vocab_size, rank, world_size):\n        per_partition_vocab_size = divide(global_vocab_size, world_size)\n        return VocabUtility.vocab_range_from_per_partition_vocab_size(\n            per_partition_vocab_size, rank\n        )\n",
        "metaseq/modules/multihead_attention.py": "# Copyright (c) Meta Platforms, Inc. and affiliates. All Rights Reserved.\n#\n# This source code is licensed under the MIT license found in the\n# LICENSE file in the root directory of this source tree.\n\nimport math\nfrom functools import partial\nfrom typing import Dict, Optional, Tuple\n\nimport torch\nfrom torch import Tensor, nn\n\nfrom metaseq import utils\nfrom metaseq.dataclass.constants import AttentionVariants\nfrom metaseq.incremental_decoding_utils import with_incremental_state\nfrom metaseq.modules.dropout import Dropout\nfrom metaseq.modules.megatron.model import (\n    ScaledUpperTriangMaskedSoftmax,\n    ScaledMaskedSoftmax,\n)\nfrom metaseq.modules.megatron.mpu import (\n    ColumnParallelLinear,\n    RowParallelLinear,\n    get_cuda_rng_tracker,\n    get_tensor_model_parallel_world_size,\n    split_tensor_along_last_dim,\n)\n\ntry:\n    import xformers.ops as xops\n\n    has_xformers = True\nexcept (ImportError, ModuleNotFoundError):\n    has_xformers = False\n\n\nimport logging\n\nlogger = logging.getLogger(__name__)\n\n\n@with_incremental_state\nclass ModelParallelMultiheadAttention(nn.Module):\n    \"\"\"Model parallel Multi-headed attention.\n    This performs the Multi-headed attention over multiple gpus.\n\n    See \"Megatron-LM: https://arxiv.org/pdf/1909.08053.pdf\" for more details.\n    \"\"\"\n\n    def __init__(\n        self,\n        embed_dim,\n        num_heads,\n        kdim=None,\n        vdim=None,\n        dropout=0.0,\n        bias=True,\n        self_attention=False,\n        use_cpu_initialization=True,\n        full_megatron_init=False,\n        full_megatron_init_scalar=1.0,\n        megatron_init_sigma=None,\n        num_layers=None,\n        dtype=torch.float32,\n        attn_variant=False,\n        xf_attn_op=None,\n        truncate_init=False,\n    ):\n        super().__init__()\n        self.embed_dim = embed_dim\n        self.kdim = kdim if kdim is not None else embed_dim\n        self.vdim = vdim if vdim is not None else embed_dim\n        self.qkv_same_dim = self.kdim == embed_dim and self.vdim == embed_dim\n        self.model_parallel_size = get_tensor_model_parallel_world_size()\n        self.num_heads_partition = num_heads // self.model_parallel_size\n        assert (\n            self.num_heads_partition * self.model_parallel_size == num_heads\n        ), \"Number of heads must be divisible by model parallel size\"\n\n        self.dropout_module = Dropout(dropout, module_name=self.__class__.__name__)\n        self.head_dim = embed_dim // num_heads\n        assert (\n            self.head_dim * num_heads == self.embed_dim\n        ), \"embed_dim must be divisible by num_heads\"\n        self.scaling = self.head_dim**-0.5\n        self.self_attention = self_attention\n\n        assert (\n            not self.self_attention or self.qkv_same_dim\n        ), \"Self-attention requires query, key and value to be of the same size\"\n\n        # TODO[Susan]: Remove the combine_qkv_proj conditional, given the below hard-coding.\n        self.combine_qkv_proj = True\n        if self.combine_qkv_proj:\n\n            def _init_method_weight_cpu(weight):\n                # Following is required to match gshard weight initialization\n                # because of how megatron splits initialized weights over model\n                # parallel workers.\n                model_parallel_matrix_splits = torch.split(\n                    weight,\n                    weight.size(0) // get_tensor_model_parallel_world_size(),\n                    dim=0,\n                )\n                k_splits = []\n                v_splits = []\n                q_splits = []\n                for model_parallel_matrix_split in model_parallel_matrix_splits:\n                    k_split, v_split, q_split = torch.split(\n                        model_parallel_matrix_split,\n                        model_parallel_matrix_split.size(0) // 3,\n                        dim=0,\n                    )\n                    k_splits.append(k_split)\n                    v_splits.append(v_split)\n                    q_splits.append(q_split)\n                fan_in, fan_out = weight.size(0) // 3, weight.size(1)\n                std = 1 / math.sqrt(float(fan_in + fan_out))\n                a = (\n                    math.sqrt(3.0) * std\n                )  # Calculate uniform bounds from standard deviation\n                for k in k_splits:\n                    nn.init._no_grad_uniform_(k, -a, a)\n                for v in v_splits:\n                    nn.init._no_grad_uniform_(v, -a, a)\n                for q in q_splits:\n                    nn.init._no_grad_uniform_(q, -a, a)\n\n            def _init_method_weight_gpu(weight):\n                k, v, q = torch.split(weight, weight.size(0) // 3, dim=0)\n                nn.init.xavier_uniform_(k, gain=1 / math.sqrt(2))\n                nn.init.xavier_uniform_(v, gain=1 / math.sqrt(2))\n                nn.init.xavier_uniform_(q, gain=1 / math.sqrt(2))\n\n            def _init_method_bias_cpu(fan_in, bias):\n                # Following is required to match gshard weight initialization\n                # because of how megatron splits initialized weights over model\n                # parallel workers.\n                model_parallel_bias_splits = torch.split(\n                    bias, bias.size(0) // get_tensor_model_parallel_world_size(), dim=0\n                )\n                k_splits = []\n                v_splits = []\n                q_splits = []\n                for model_parallel_bias_split in model_parallel_bias_splits:\n                    k_split, v_split, q_split = torch.split(\n                        model_parallel_bias_split,\n                        model_parallel_bias_split.size(0) // 3,\n                        dim=0,\n                    )\n                    k_splits.append(k_split)\n                    v_splits.append(v_split)\n                    q_splits.append(q_split)\n\n                bound = 1 / math.sqrt(fan_in)\n                for k in k_splits:\n                    nn.init.uniform_(k, -bound, bound)\n                for v in v_splits:\n                    nn.init.uniform_(v, -bound, bound)\n                for q in q_splits:\n                    nn.init.uniform_(q, -bound, bound)\n\n            def _init_method_bias_gpu(fan_in, bias):\n                k, v, q = split_tensor_along_last_dim(bias, 3)\n                bound = 1 / math.sqrt(fan_in)\n                nn.init.uniform_(k, -bound, bound)\n                nn.init.uniform_(v, -bound, bound)\n                nn.init.uniform_(q, -bound, bound)\n\n            if full_megatron_init:\n                assert megatron_init_sigma is not None\n                # Note we do not apply full_megatron_init_scalar here; only out_proj is changed\n                init_method_weights = utils.init_method_normal(\n                    megatron_init_sigma, truncate_init=truncate_init\n                )\n                init_method_bias = None\n            else:\n                init_method_weights = (\n                    _init_method_weight_cpu\n                    if use_cpu_initialization\n                    else _init_method_weight_gpu\n                )\n                if use_cpu_initialization:\n                    init_method_bias = partial(_init_method_bias_cpu, self.kdim)\n                else:\n                    init_method_bias = partial(_init_method_bias_gpu, self.kdim)\n\n            self.qkv_proj = ColumnParallelLinear(\n                self.kdim,\n                3 * embed_dim,\n                bias=bias,\n                gather_output=False,\n                init_method=init_method_weights,\n                init_method_bias=init_method_bias,\n                use_cpu_initialization=use_cpu_initialization,\n                dtype=dtype,\n            )\n        else:\n\n            def _init_method_weight(weight):\n                nn.init.xavier_uniform_(weight, gain=1 / math.sqrt(2))\n\n            def _init_method_bias(fan_in, bias):\n                bound = 1 / math.sqrt(fan_in)\n                nn.init.uniform_(bias, -bound, bound)\n\n            self.k_proj = ColumnParallelLinear(\n                self.kdim,\n                embed_dim,\n                bias=bias,\n                gather_output=False,\n                init_method=_init_method_weight,\n                init_method_bias=None\n                if full_megatron_init\n                else partial(_init_method_bias, self.kdim),\n                use_cpu_initialization=use_cpu_initialization,\n                dtype=dtype,\n            )\n            self.v_proj = ColumnParallelLinear(\n                self.vdim,\n                embed_dim,\n                bias=bias,\n                gather_output=False,\n                init_method=_init_method_weight,\n                init_method_bias=None\n                if full_megatron_init\n                else partial(_init_method_bias, self.vdim),\n                use_cpu_initialization=use_cpu_initialization,\n                dtype=dtype,\n            )\n            self.q_proj = ColumnParallelLinear(\n                embed_dim,\n                embed_dim,\n                bias=bias,\n                gather_output=False,\n                init_method=_init_method_weight,\n                init_method_bias=None\n                if full_megatron_init\n                else partial(_init_method_bias, embed_dim),\n                use_cpu_initialization=use_cpu_initialization,\n                dtype=dtype,\n            )\n\n        def _init_method_weight(weight):\n            nn.init.xavier_uniform_(weight, gain=1)\n\n        init_method_weights = _init_method_weight\n        if full_megatron_init:\n            assert megatron_init_sigma is not None\n            assert num_layers is not None\n            init_method_weights = utils.scaled_init_method_normal(\n                megatron_init_sigma * full_megatron_init_scalar,\n                num_layers,\n                truncate_init=truncate_init,\n            )\n        self.out_proj = RowParallelLinear(\n            embed_dim,\n            embed_dim,\n            bias=bias,\n            input_is_parallel=True,\n            init_method=init_method_weights,\n            skip_bias_add=True,\n            use_cpu_initialization=use_cpu_initialization,\n            dtype=dtype,\n        )\n        self.xf_eff_attn = attn_variant == AttentionVariants.XFORMERS\n        self.xf_op = None\n        if self.xf_eff_attn and not has_xformers:\n            raise ImportError(\n                \"\\n\\nPlease install xformers to use memory efficient attention\"\n            )\n        if self.xf_eff_attn and xf_attn_op is not None:\n            try:\n                self.xf_op = getattr(xops, xf_attn_op)\n            except AttributeError:\n                logging.warning(f\"Invalid xformers memorry efficient op specified.\")\n\n    def forward(\n        self,\n        query,\n        key: Optional[Tensor],\n        value: Optional[Tensor],\n        key_padding_mask: Optional[Tensor] = None,\n        incremental_state: Optional[Dict[str, Dict[str, Optional[Tensor]]]] = None,\n        attn_mask: Optional[Tensor] = None,\n        **unused_kwargs,\n    ) -> Tuple[Tensor, Optional[Tensor]]:\n        \"\"\"Input shape: Time x Batch x Channel\n\n        Args:\n            key_padding_mask (ByteTensor, optional): mask to exclude\n                keys that are pads, of shape `(batch, src_len)`, where\n                padding elements are indicated by 1s.\n            attn_mask (ByteTensor, optional): typically used to\n                implement causal attention, where the mask prevents the\n                attention from looking forward in time (default: None).\n        \"\"\"\n        tgt_len, bsz, embed_dim = query.size()\n        assert embed_dim == self.embed_dim\n        assert list(query.size()) == [tgt_len, bsz, embed_dim]\n        if key is not None:\n            src_len, key_bsz, _ = key.size()\n            if not torch.jit.is_scripting():\n                assert key_bsz == bsz\n                assert value is not None\n                assert src_len, bsz == value.shape[:2]\n\n        if incremental_state is not None:\n            saved_state = self._get_input_buffer(incremental_state)\n        else:\n            saved_state = None\n\n        # logger.info(\"query:\" + str(query.float().norm().item()))\n        if self.self_attention:\n            if self.combine_qkv_proj:\n                kvq, _ = self.qkv_proj(query)\n                k, v, q = split_tensor_along_last_dim(\n                    kvq, 3, contiguous_split_chunks=True\n                )\n            else:\n                q, _ = self.q_proj(query)\n                k, _ = self.k_proj(query)\n                v, _ = self.v_proj(query)\n        else:\n            assert key is not None and value is not None\n            q, _ = self.q_proj(query)\n            k, _ = self.k_proj(key)\n            v, _ = self.v_proj(value)\n\n        # Megatron's fused kernel: \"ScaledUpperTriangMaskedSoftmax\" seems to crash with odd shape across seq_len dimension.\n        # This is okay for training cause training we have all seq_len nice power of 2s but during evaluation and generation,\n        # we have seq_lens not power of 2.\n        CHANGES = not getattr(self, \"inference\", False)\n\n        if self.xf_eff_attn:\n            q = q.view(\n                tgt_len, bsz * self.num_heads_partition, self.head_dim\n            ).transpose(0, 1)\n            if k is not None:\n                k = k.view(-1, bsz * self.num_heads_partition, self.head_dim).transpose(\n                    0, 1\n                )\n            if v is not None:\n                v = (\n                    v.contiguous()\n                    .view(-1, bsz * self.num_heads_partition, self.head_dim)\n                    .transpose(0, 1)\n                )\n            attn = xops.memory_efficient_attention(\n                q,\n                k,\n                v,\n                attn_bias=xops.LowerTriangularMask(),\n                op=self.xf_op,\n            )\n        elif CHANGES:\n            output_size = (\n                q.size(1),\n                self.num_heads_partition,\n                q.size(0),\n                k.size(0),\n            )\n\n            q = q.view(tgt_len, bsz * self.num_heads_partition, self.head_dim)\n            if k is not None:\n                k = k.view(-1, bsz * self.num_heads_partition, self.head_dim)\n            if v is not None:\n                v = (\n                    v.contiguous()\n                    .view(-1, bsz * self.num_heads_partition, self.head_dim)\n                    .transpose(0, 1)\n                )\n            matmul_result = torch.empty(\n                output_size[0] * output_size[1],\n                output_size[2],\n                output_size[3],\n                dtype=q.dtype,\n                device=torch.cuda.current_device(),\n            )\n\n            # Scale q,k before matmul for stability see https://tinyurl.com/sudb9s96 for math\n            matmul_result = torch.baddbmm(\n                matmul_result,\n                math.sqrt(self.scaling) * q.transpose(0, 1),  # [b * np, sq, hn]\n                math.sqrt(self.scaling)\n                * k.transpose(0, 1).transpose(1, 2),  # [b * np, hn, sk]\n                beta=0.0,\n            )\n\n            # Replace any non-finite values with finite equivalents, since otherwise\n            # we may get NaN when adding attn_mask or computing softmax.\n            if attn_mask is not None:\n                matmul_result = torch.nan_to_num(matmul_result)\n\n            # The attn_mask shape can be either seq_length x seq_length, or batch_size x seq_length x seq_length\n            # depending on whether we are broadcasting the same attention mask across the batch, or\n            # masking dynamically based on the data (e.g. for document attention).\n            # If we have a per sequence mask, the condition len(attn_mask.size()) == 3\n            # is true.\n            if (attn_mask is not None) and (len(attn_mask.size()) == 3):\n                # Going back to original scaled_masked_softmax to accomodate\n                # non-causal attention masking (use the given input attention)\n                attention_scores = matmul_result.view(*output_size)\n                attn_mask = attn_mask < -0.5\n                attn_mask = attn_mask.unsqueeze(1)\n                attn_probs = ScaledMaskedSoftmax.apply(attention_scores, attn_mask, 1.0)\n                attn_probs = attn_probs.view(\n                    output_size[0] * output_size[1], output_size[2], output_size[3]\n                )\n            else:\n                try:\n                    attn_probs = ScaledUpperTriangMaskedSoftmax.apply(\n                        matmul_result, 1.0\n                    )\n                except RuntimeError as e:\n                    raise RuntimeError(\n                        \"Looks like you may have hit the feared INTERNAL ASSERT \"\n                        \"ERROR. You can either ensure your sequences are padded \"\n                        \"to a nice length (usually a power of 2), or you can make \"\n                        \"sure you call model.make_generation_fast_() at load. See \"\n                        \"interactive_hosted.py for an example.\\n\\n\"\n                        f\"Original Exception: {e}\"\n                    )\n\n            with get_cuda_rng_tracker().fork():\n                attn_probs = self.dropout_module(attn_probs)\n\n        else:\n            q *= self.scaling\n\n            q = (\n                q.contiguous()\n                .view(tgt_len, bsz * self.num_heads_partition, self.head_dim)\n                .transpose(0, 1)\n            )\n            if k is not None:\n                k = (\n                    k.contiguous()\n                    .view(-1, bsz * self.num_heads_partition, self.head_dim)\n                    .transpose(0, 1)\n                )\n            if v is not None:\n                v = (\n                    v.contiguous()\n                    .view(-1, bsz * self.num_heads_partition, self.head_dim)\n                    .transpose(0, 1)\n                )\n\n            if saved_state is not None:\n                # saved states are stored with shape (bsz, num_heads_partition, seq_len, head_dim)\n                if \"prev_key\" in saved_state:\n                    _prev_key = saved_state[\"prev_key\"]\n                    assert _prev_key is not None\n                    prev_key = _prev_key.view(\n                        bsz * self.num_heads_partition, -1, self.head_dim\n                    )\n                    assert k is not None\n                    k = torch.cat([prev_key, k], dim=1)\n                    src_len = k.size(1)\n                if \"prev_value\" in saved_state:\n                    _prev_value = saved_state[\"prev_value\"]\n                    assert _prev_value is not None\n                    prev_value = _prev_value.view(\n                        bsz * self.num_heads_partition, -1, self.head_dim\n                    )\n                    assert v is not None\n                    v = torch.cat([prev_value, v], dim=1)\n                saved_state[\"prev_key\"] = k.view(\n                    bsz, self.num_heads_partition, -1, self.head_dim\n                )\n                saved_state[\"prev_value\"] = v.view(\n                    bsz, self.num_heads_partition, -1, self.head_dim\n                )\n                saved_state[\"prev_key_padding_mask\"] = key_padding_mask\n                # In this branch incremental_state is never None\n                assert incremental_state is not None\n                incremental_state = self._set_input_buffer(\n                    incremental_state, saved_state\n                )\n            assert k is not None\n            assert k.size(1) == src_len\n\n            # This is part of a workaround to get around fork/join parallelism\n            # not supporting Optional types.\n            if key_padding_mask is not None and key_padding_mask.dim() == 0:\n                key_padding_mask = None\n\n            if key_padding_mask is not None:\n                assert key_padding_mask.size(0) == bsz\n                assert key_padding_mask.size(1) == src_len\n\n            attn_weights = torch.bmm(q, k.transpose(1, 2))\n\n            assert list(attn_weights.size()) == [\n                bsz * self.num_heads_partition,\n                tgt_len,\n                src_len,\n            ]\n\n            if attn_mask is not None:\n                # The attn_mask shape can be either seq_length x seq_length, or batch_size x seq_length x seq_length\n                # depending on whether we are broadcasting the same attention mask across the batch, or\n                # masking dynamically based on the data (e.g. for document attention).\n                # If we have a per sequence mask, the condition len(attn_mask.size()) == 3\n                # is true.\n                if len(attn_mask.size()) == 3:\n                    attn_mask = attn_mask.unsqueeze(1)\n                    attn_mask = attn_mask.repeat(1, self.num_heads_partition, 1, 1)\n                    attn_mask = attn_mask.view(\n                        bsz * self.num_heads_partition, tgt_len, src_len\n                    )\n                    attn_weights = attn_weights.masked_fill(\n                        attn_mask < -0.5,\n                        float(\"-inf\"),\n                    )\n                else:\n                    attn_mask = attn_mask.unsqueeze(0)\n                    attn_weights += attn_mask\n\n            if key_padding_mask is not None:\n                # don't attend to padding symbols\n                attn_weights = attn_weights.view(\n                    bsz, self.num_heads_partition, tgt_len, src_len\n                )\n                attn_weights = attn_weights.masked_fill(\n                    key_padding_mask.unsqueeze(1).unsqueeze(2).to(torch.bool),\n                    float(\"-inf\"),\n                )\n                attn_weights = attn_weights.view(\n                    bsz * self.num_heads_partition, tgt_len, src_len\n                )\n\n            attn_weights_float = utils.softmax(attn_weights, dim=-1)\n            attn_weights = attn_weights_float.type_as(attn_weights)\n\n            with get_cuda_rng_tracker().fork():\n                attn_probs = self.dropout_module(attn_weights)\n\n        # logger.info(\"attn_probs:\" + str(attn_probs.float().norm().item()))\n        assert v is not None\n\n        if not self.xf_eff_attn:\n            attn = torch.bmm(attn_probs, v)\n            # logger.info(\"attn:\" + str(attn.float().norm().item()))\n\n        assert list(attn.size()) == [\n            bsz * self.num_heads_partition,\n            tgt_len,\n            self.head_dim,\n        ]\n        embed_dim_partition = embed_dim // self.model_parallel_size\n        attn = attn.transpose(0, 1).contiguous().view(tgt_len, bsz, embed_dim_partition)\n        attn, attn_bias = self.out_proj(attn)\n        # Note that this no longer matches the signature of non-model-parallel version, which returns\n        # Tuple[Tensor, Optional[Tensor]]\n        return attn, attn_bias\n\n    def _get_input_buffer(\n        self, incremental_state: Optional[Dict[str, Dict[str, Optional[Tensor]]]]\n    ) -> Dict[str, Optional[Tensor]]:\n        result = self.get_incremental_state(incremental_state, \"attn_state\")\n        if result is not None:\n            return result\n        else:\n            empty_result: Dict[str, Optional[Tensor]] = {}\n            return empty_result\n\n    def _set_input_buffer(\n        self,\n        incremental_state: Dict[str, Dict[str, Optional[Tensor]]],\n        buffer: Dict[str, Optional[Tensor]],\n    ):\n        return self.set_incremental_state(incremental_state, \"attn_state\", buffer)\n\n    # This hook used as proxy for tracking state if model is in eval or generation mode.\n    def make_generation_fast_(self, **unused):\n        self.inference = True\n",
        "metaseq/modules/positional_embedding.py": "# Copyright (c) Meta Platforms, Inc. and affiliates. All Rights Reserved.\n#\n# This source code is licensed under the MIT license found in the\n# LICENSE file in the root directory of this source tree.\n\nimport torch\nimport torch.nn as nn\n\nfrom .learned_positional_embedding import LearnedPositionalEmbedding\nfrom .sinusoidal_positional_embedding import SinusoidalPositionalEmbedding\n\n\ndef PositionalEmbedding(\n    num_embeddings: int,\n    embedding_dim: int,\n    padding_idx: int,\n    learned: bool = False,\n    learned_sinusoidal: bool = False,\n    full_megatron_init=False,\n    pos_init_scalar=1.0,\n    megatron_init_sigma=None,\n    truncate_init=False,\n):\n    def _init_emb(tensor, sigma):\n        if sigma <= 1e-8:  # effectively 0\n            return nn.init.zeros_(tensor)\n        if truncate_init:\n            return nn.init.trunc_normal_(\n                tensor, mean=0.0, std=sigma, a=-3 * sigma, b=3 * sigma\n            )\n        else:\n            return nn.init.normal_(tensor, mean=0.0, std=sigma)\n\n    if learned:\n        # if padding_idx is specified then offset the embedding ids by\n        # this index and adjust num_embeddings appropriately\n        # TODO: The right place for this offset would be inside\n        # LearnedPositionalEmbedding. Move this there for a cleaner implementation.\n        if padding_idx is not None:\n            num_embeddings = num_embeddings + padding_idx + 1\n        m = LearnedPositionalEmbedding(num_embeddings, embedding_dim, padding_idx)\n        if full_megatron_init:\n            _init_emb(m.weight, megatron_init_sigma * pos_init_scalar)\n        else:\n            _init_emb(m.weight, embedding_dim**-0.5 * pos_init_scalar)\n        if padding_idx is not None:\n            nn.init.constant_(m.weight[padding_idx], 0)\n    elif learned_sinusoidal:\n        if padding_idx is not None:\n            num_embeddings = num_embeddings + padding_idx + 1\n        m = LearnedPositionalEmbedding(num_embeddings, embedding_dim, padding_idx)\n        with torch.no_grad():\n            m.weight.copy_(\n                SinusoidalPositionalEmbedding.get_embedding(\n                    num_embeddings,\n                    embedding_dim,\n                    padding_idx,\n                )\n            )\n    else:\n        m = SinusoidalPositionalEmbedding(\n            embedding_dim,\n            padding_idx,\n            init_size=num_embeddings + padding_idx + 1,\n        )\n    return m\n",
        "metaseq/modules/quant_noise.py": "# Copyright (c) Facebook, Inc. and its affiliates.\n#\n# This source code is licensed under the MIT license found in the\n# LICENSE file in the root directory of this source tree.\n\nimport torch\nimport torch.nn as nn\n\n\ndef quant_noise(module, p, block_size):\n    \"\"\"\n    Wraps modules and applies quantization noise to the weights for\n    subsequent quantization with Iterative Product Quantization as\n    described in \"Training with Quantization Noise for Extreme Model Compression\"\n    Args:\n        - module: nn.Module\n        - p: amount of Quantization Noise\n        - block_size: size of the blocks for subsequent quantization with iPQ\n    Remarks:\n        - Module weights must have the right sizes wrt the block size\n        - Only Linear, Embedding and Conv2d modules are supported for the moment\n        - For more detail on how to quantize by blocks with convolutional weights,\n          see \"And the Bit Goes Down: Revisiting the Quantization of Neural Networks\"\n        - We implement the simplest form of noise here as stated in the paper\n          which consists in randomly dropping blocks\n    \"\"\"\n\n    # if no quantization noise, don't register hook\n    if p <= 0:\n        return module\n\n    # supported modules\n    assert isinstance(module, (nn.Linear, nn.Embedding, nn.Conv2d))\n\n    # test whether module.weight has the right sizes wrt block_size\n    is_conv = module.weight.ndim == 4\n\n    # 2D matrix\n    if not is_conv:\n        assert (\n            module.weight.size(1) % block_size == 0\n        ), \"Input features must be a multiple of block sizes\"\n\n    # 4D matrix\n    else:\n        # 1x1 convolutions\n        if module.kernel_size == (1, 1):\n            assert (\n                module.in_channels % block_size == 0\n            ), \"Input channels must be a multiple of block sizes\"\n        # regular convolutions\n        else:\n            k = module.kernel_size[0] * module.kernel_size[1]\n            assert k % block_size == 0, \"Kernel size must be a multiple of block size\"\n\n    def _forward_pre_hook(mod, input):\n        # no noise for evaluation\n        if mod.training:\n            if not is_conv:\n                # gather weight and sizes\n                weight = mod.weight\n                in_features = weight.size(1)\n                out_features = weight.size(0)\n\n                # split weight matrix into blocks and randomly drop selected blocks\n                mask = torch.zeros(\n                    in_features // block_size * out_features, device=weight.device\n                )\n                mask.bernoulli_(p)\n                mask = mask.repeat_interleave(block_size, -1).view(-1, in_features)\n\n            else:\n                # gather weight and sizes\n                weight = mod.weight\n                in_channels = mod.in_channels\n                out_channels = mod.out_channels\n\n                # split weight matrix into blocks and randomly drop selected blocks\n                if mod.kernel_size == (1, 1):\n                    mask = torch.zeros(\n                        int(in_channels // block_size * out_channels),\n                        device=weight.device,\n                    )\n                    mask.bernoulli_(p)\n                    mask = mask.repeat_interleave(block_size, -1).view(-1, in_channels)\n                else:\n                    mask = torch.zeros(\n                        weight.size(0), weight.size(1), device=weight.device\n                    )\n                    mask.bernoulli_(p)\n                    mask = (\n                        mask.unsqueeze(2)\n                        .unsqueeze(3)\n                        .repeat(1, 1, mod.kernel_size[0], mod.kernel_size[1])\n                    )\n\n            # scale weights and apply mask\n            mask = mask.to(\n                torch.bool\n            )  # x.bool() is not currently supported in TorchScript\n            s = 1 / (1 - p)\n            mod.weight.data = s * weight.masked_fill(mask, 0)\n\n    module.register_forward_pre_hook(_forward_pre_hook)\n    return module\n",
        "metaseq/modules/sequence_parallel_transformer_layer.py": "# Copyright (c) Meta Platforms, Inc. and affiliates. All Rights Reserved.\n#\n# This source code is licensed under the MIT license found in the\n# LICENSE file in the root directory of this source tree.\n\nimport importlib\nimport math\n\nimport torch\n\nfrom metaseq.modules.activation_functions import gelu, gelu_back, relu, relu_back\nfrom metaseq.modules.megatron.fused_kernels import (\n    scaled_upper_triang_masked_softmax_cuda,\n)\nfrom metaseq.modules.megatron.mpu import (\n    split_tensor_along_last_dim,\n    _reduce_scatter_along_first_dim,\n    _gather_along_first_dim,\n)\n\n# Not importing here cause cpu tests don't like it\n# imported from apex\nglobal fused_layer_norm_cuda\nfused_layer_norm_cuda = None\n\n\nclass SequeuceParallelTransformerBlock(torch.autograd.Function):\n    \"\"\"\n    This is custom FFN autograd function hardcoded for:\n    bias: false,\n    layernorm affine: false, ln eps: 1e-5\n    sequence_parallel: true,\n    activation: gelu,\n    gelu, layernorm: always recomputed i.e. no activation memory for these\n    \"\"\"\n\n    @staticmethod\n    def forward_mha(q, k, v, bsz, seq_len, head_dim, embed_dim_per_partition, dtype):\n        scaling = head_dim**-0.5\n        matmul_result = torch.empty(\n            bsz * (embed_dim_per_partition // head_dim),\n            seq_len,\n            seq_len,\n            dtype=dtype,\n            device=torch.cuda.current_device(),\n        )\n        # Scale q,k before matmul for stability see https://tinyurl.com/sudb9s96 for math\n        matmul_result = torch.baddbmm(\n            matmul_result,\n            math.sqrt(scaling) * q.transpose(0, 1),\n            math.sqrt(scaling) * k.transpose(0, 1).transpose(1, 2),\n            beta=0.0,\n        )\n        # attn_probs = matmul_result\n        scale_t = torch.tensor([1.0])\n        attn_probs = scaled_upper_triang_masked_softmax_cuda.forward(\n            matmul_result, scale_t[0]\n        )\n        attn = torch.bmm(attn_probs, v)\n        attn = attn.transpose(0, 1).contiguous().view(seq_len, bsz, -1)\n        return attn, attn_probs\n\n    @staticmethod\n    def backward_mha(grad_mha_output, q, k, v, attn_probs, seq_len, bsz, head_dim):\n        scaling = head_dim**-0.5\n        grad_mha_output = grad_mha_output.view(seq_len, -1, head_dim).transpose(0, 1)\n        grad_v = (\n            torch.bmm(attn_probs.transpose(1, 2), grad_mha_output)\n            .transpose(0, 1)\n            .contiguous()\n            .view(seq_len, bsz, -1)\n        )\n        grad_attn_probs_out = torch.bmm(grad_mha_output, v.transpose(1, 2))\n\n        grad_attn_probs_in = scaled_upper_triang_masked_softmax_cuda.backward(\n            grad_attn_probs_out, attn_probs, 1.0\n        )\n        grad_q = torch.bmm(\n            math.sqrt(scaling) * grad_attn_probs_in,\n            math.sqrt(scaling) * k.transpose(0, 1),\n        )\n        grad_q = grad_q.transpose(0, 1).contiguous().view(seq_len, bsz, -1)\n        grad_k = torch.bmm(\n            math.sqrt(scaling) * grad_attn_probs_in.transpose(1, 2),\n            math.sqrt(scaling) * q.transpose(0, 1),\n        )\n        grad_k = grad_k.transpose(0, 1).contiguous().view(seq_len, bsz, -1)\n        grad_kvq_proj_output = torch.cat([grad_k, grad_v, grad_q], dim=-1)\n        return grad_kvq_proj_output\n\n    @staticmethod\n    def forward(\n        ctx,\n        input,\n        kvq_proj_weight,\n        out_proj_weight,\n        fc1_weight,\n        fc2_weight,\n        head_dim,\n        recompute_fc1,\n        activation_fn_name,  # \"relu\" or \"gelu\" for now\n    ):\n        assert (\n            activation_fn_name == \"relu\" or activation_fn_name == \"gelu\"\n        ), \"Only relu/gelu is supported!\"\n        # import from apex\n        global fused_layer_norm_cuda\n        if fused_layer_norm_cuda is None:\n            fused_layer_norm_cuda = importlib.import_module(\"fused_layer_norm_cuda\")\n\n        ctx.recompute_fc1 = recompute_fc1\n\n        input = input.contiguous()\n\n        # Take out residual connection for self attention\n        residual = input\n        dtype = input.dtype\n\n        # Apply layer norm on (seq_len // #tp_size, bsz, embed_dim) tensor\n        ctx.layer_norm_normalized_shape = torch.Size((input.size(-1),))\n        ctx.eps = 1e-5\n\n        # # Self attention layer norm\n        mha_layer_norm_output, _, _ = fused_layer_norm_cuda.forward(\n            input, ctx.layer_norm_normalized_shape, ctx.eps\n        )\n\n        # all gather output across first dim, i.e. seq_len dim for kvq_proj\n        mha_layer_norm_output = _gather_along_first_dim(\n            mha_layer_norm_output, cached_buffer_name=\"mpu\"\n        )\n\n        # apply kvq, output is (seq_len, bsz, 3 * embed_dim // #tp_size)\n        # The order of (k,v, q) here doesn't matter as much as long its consistent since initialization of all three is same.\n        # just matching the order of metaseq MHA.\n        kvq_out = torch.matmul(mha_layer_norm_output, kvq_proj_weight.t())\n\n        k, v, q = split_tensor_along_last_dim(kvq_out, 3, contiguous_split_chunks=True)\n        seq_len, bsz, embed_dim_per_partition = q.size()\n        q = q.view(seq_len, -1, head_dim)\n        k = k.view(seq_len, -1, head_dim)\n        v = v.view(seq_len, -1, head_dim).transpose(0, 1)\n\n        attn, _ = SequeuceParallelTransformerBlock.forward_mha(\n            q, k, v, bsz, seq_len, head_dim, embed_dim_per_partition, dtype\n        )\n\n        out_proj_out = torch.matmul(attn, out_proj_weight.t())\n        out_proj_out = _reduce_scatter_along_first_dim(out_proj_out)\n\n        out_proj_out = out_proj_out + residual\n\n        # Take out residual connection for FFN\n        residual = out_proj_out\n        # No need to save mean and invvar cause we redo layernorm in backward\n        ffn_layer_norm_output, _, _ = fused_layer_norm_cuda.forward(\n            out_proj_out, ctx.layer_norm_normalized_shape, ctx.eps\n        )\n\n        # all gather output across first dim, i.e. seq_len dim\n        ffn_layer_norm_output = _gather_along_first_dim(\n            ffn_layer_norm_output, cached_buffer_name=\"mpu\"\n        )\n\n        # apply fc1, output is (seq_len, bsz, 4 * embed_dim // #tp_size)\n        fc1_out = torch.matmul(ffn_layer_norm_output, fc1_weight.t())\n\n        # apply activation\n        # TODO: split out to explicit if/else instead of defaulting to relu when not gelu\n        actv_out = gelu(fc1_out) if activation_fn_name == \"gelu\" else relu(fc1_out)\n\n        # apply fc2, output (seq_len, bsz, embed_dim) but needs to be\n        # summed across tp for real output\n        fc2_out = torch.matmul(actv_out, fc2_weight.t())\n\n        if ctx.recompute_fc1:\n            fc1_out = None\n        ctx.save_for_backward(\n            input,\n            q,\n            k,\n            v,\n            out_proj_out,\n            kvq_proj_weight,\n            out_proj_weight,\n            fc1_out,\n            fc1_weight,\n            fc2_weight,\n        )\n        (\n            ctx.bsz,\n            ctx.seq_len,\n            ctx.head_dim,\n            ctx.embed_dim_per_partition,\n            ctx.activation_fn_name,\n        ) = (\n            bsz,\n            seq_len,\n            head_dim,\n            embed_dim_per_partition,\n            activation_fn_name,\n        )\n\n        # apply scatter gather,\n        # input: (seq_len, bsz, embed_dim)\n        # output: (seq_len // #tp_size, bsz, embed_dim) (and embed_dim is summed across gpus)\n        fc2_out_post_scatter_gather = _reduce_scatter_along_first_dim(fc2_out)\n        final_out = fc2_out_post_scatter_gather + residual\n        return final_out\n\n    @staticmethod\n    def backward(ctx, grad_output):\n        (\n            input,\n            q,\n            k,\n            v,\n            out_proj_out,\n            kvq_proj_weight,\n            out_proj_weight,\n            fc1_out,\n            fc1_weight,\n            fc2_weight,\n        ) = ctx.saved_tensors\n        bsz, seq_len, head_dim, embed_dim_per_partition, activation_fn_name = (\n            ctx.bsz,\n            ctx.seq_len,\n            ctx.head_dim,\n            ctx.embed_dim_per_partition,\n            ctx.activation_fn_name,\n        )\n        dtype = grad_output.dtype\n\n        residual_grad = grad_output\n\n        # gatther gradients async,\n        # and we can overlap this with any recomptation.\n        grad_output, handle = _gather_along_first_dim(grad_output, async_op=True)\n\n        # Both of these operations are just recomputed from forward to save activation memory.\n        (\n            ffn_layer_norm_output,\n            ffn_layer_norm_mean,\n            ffn_layer_norm_invvar,\n        ) = fused_layer_norm_cuda.forward(\n            out_proj_out, ctx.layer_norm_normalized_shape, ctx.eps\n        )\n        # recompute gelu output for calculating fc2 weight gradient\n        # note, remember \"gelu_out = fc2_in\"\n        if not ctx.recompute_fc1:\n            assert fc1_out is not None\n            actv_out = gelu(fc1_out) if activation_fn_name == \"gelu\" else relu(fc1_out)\n\n        # Now wait for reduce scatter\n        handle.wait()\n\n        ffn_layer_norm_output, handle = _gather_along_first_dim(\n            ffn_layer_norm_output, async_op=True, cached_buffer_name=\"mpu\"\n        )\n\n        grad_fc2_input = grad_output.matmul(fc2_weight)\n\n        if ctx.recompute_fc1:\n            handle.wait()\n            assert fc1_out is None\n            fc1_out = torch.matmul(ffn_layer_norm_output, fc1_weight.t())\n            actv_out = gelu(fc1_out) if activation_fn_name == \"gelu\" else relu(fc1_out)\n\n        # calculate gelu/relu backward\n        grad_actv_input = (\n            gelu_back(grad_fc2_input, fc1_out)\n            if activation_fn_name == \"gelu\"\n            else relu_back(grad_fc2_input, fc1_out)\n        )\n\n        # Reshape matrix and calculate gradient with respect to fc2 weight\n        grad_output = SequeuceParallelTransformerBlock._collapse_first_dimensions(\n            grad_output\n        )\n        actv_out = SequeuceParallelTransformerBlock._collapse_first_dimensions(actv_out)\n        grad_fc2_weight = grad_output.t().matmul(actv_out)\n\n        grad_fc1_input = grad_actv_input.matmul(fc1_weight)\n        handle.wait()\n\n        grad_actv_input = SequeuceParallelTransformerBlock._collapse_first_dimensions(\n            grad_actv_input\n        )\n        ffn_layer_norm_output = (\n            SequeuceParallelTransformerBlock._collapse_first_dimensions(\n                ffn_layer_norm_output\n            )\n        )\n\n        grad_fc1_input, handle = _reduce_scatter_along_first_dim(\n            grad_fc1_input, async_op=True\n        )\n\n        grad_fc1_weight = grad_actv_input.t().matmul(ffn_layer_norm_output)\n\n        handle.wait()\n\n        grad_attention_output = fused_layer_norm_cuda.backward(\n            grad_fc1_input.contiguous(),\n            ffn_layer_norm_mean,\n            ffn_layer_norm_invvar,\n            out_proj_out,\n            ctx.layer_norm_normalized_shape,\n            ctx.eps,\n        )\n        grad_attention_output = grad_attention_output + residual_grad\n\n        residual_grad = grad_attention_output\n\n        grad_attention_output, handle = _gather_along_first_dim(\n            grad_attention_output,\n            async_op=True,\n        )\n\n        # recalculate attention\n        attn, attn_probs = SequeuceParallelTransformerBlock.forward_mha(\n            q, k, v, bsz, seq_len, head_dim, embed_dim_per_partition, dtype\n        )\n\n        handle.wait()\n\n        grad_out_proj_input = grad_attention_output.matmul(out_proj_weight)\n        grad_attention_output = (\n            SequeuceParallelTransformerBlock._collapse_first_dimensions(\n                grad_attention_output\n            )\n        )\n        attn = SequeuceParallelTransformerBlock._collapse_first_dimensions(attn)\n        grad_out_proj_weight = grad_attention_output.t().matmul(attn)\n\n        grad_kvq_proj_output = SequeuceParallelTransformerBlock.backward_mha(\n            grad_out_proj_input, q, k, v, attn_probs, seq_len, bsz, head_dim\n        )\n\n        (\n            mha_layer_norm_output,\n            mha_layer_norm_mean,\n            mha_layer_norm_invvar,\n        ) = fused_layer_norm_cuda.forward(\n            input, ctx.layer_norm_normalized_shape, ctx.eps\n        )\n        mha_layer_norm_output, handle = _gather_along_first_dim(\n            mha_layer_norm_output,\n            async_op=True,\n            cached_buffer_name=\"mpu\",\n        )\n        grad_input = grad_kvq_proj_output.matmul(kvq_proj_weight)\n        handle.wait()\n\n        grad_input, handle = _reduce_scatter_along_first_dim(grad_input, async_op=True)\n        mha_layer_norm_output = (\n            SequeuceParallelTransformerBlock._collapse_first_dimensions(\n                mha_layer_norm_output\n            )\n        )\n        grad_kvq_proj_output = (\n            SequeuceParallelTransformerBlock._collapse_first_dimensions(\n                grad_kvq_proj_output\n            )\n        )\n        grad_kvq_weight = grad_kvq_proj_output.t().matmul(mha_layer_norm_output)\n        handle.wait()\n\n        grad_input = fused_layer_norm_cuda.backward(\n            grad_input.contiguous(),\n            mha_layer_norm_mean,\n            mha_layer_norm_invvar,\n            input,\n            ctx.layer_norm_normalized_shape,\n            ctx.eps,\n        )\n        grad_input = grad_input + residual_grad\n        return (\n            grad_input,\n            grad_kvq_weight,\n            grad_out_proj_weight,\n            grad_fc1_weight,\n            grad_fc2_weight,\n            None,\n            None,\n            None,\n        )\n\n    @staticmethod\n    def _collapse_first_dimensions(tensor):\n        return tensor.view(\n            tensor.shape[0] * tensor.shape[1],\n            tensor.shape[2],\n        )\n",
        "metaseq/modules/sinusoidal_positional_embedding.py": "# Copyright (c) Meta Platforms, Inc. and affiliates. All Rights Reserved.\n#\n# This source code is licensed under the MIT license found in the\n# LICENSE file in the root directory of this source tree.\n\nimport math\nfrom typing import Any, Optional\n\nimport torch\nimport torch.onnx.operators\nfrom torch import Tensor, nn\n\nfrom metaseq import utils\n\n\nclass SinusoidalPositionalEmbedding(nn.Module):\n    \"\"\"This module produces sinusoidal positional embeddings of any length.\n\n    Padding symbols are ignored.\n    \"\"\"\n\n    def __init__(self, embedding_dim, padding_idx, init_size=1024):\n        super().__init__()\n        self.embedding_dim = embedding_dim\n        self.padding_idx = padding_idx if padding_idx is not None else 0\n        self.weights = SinusoidalPositionalEmbedding.get_embedding(\n            init_size, embedding_dim, padding_idx\n        )\n        self.register_buffer(\"_float_tensor\", torch.FloatTensor(1))\n        self.max_positions = int(1e5)\n\n    @staticmethod\n    def get_embedding(\n        num_embeddings: int, embedding_dim: int, padding_idx: Optional[int] = None\n    ):\n        \"\"\"Build sinusoidal embeddings.\n\n        This matches the implementation in tensor2tensor, but differs slightly\n        from the description in Section 3.5 of \"Attention Is All You Need\".\n        \"\"\"\n        half_dim = embedding_dim // 2\n        emb = math.log(10000) / (half_dim - 1)\n        emb = torch.exp(torch.arange(half_dim, dtype=torch.float) * -emb)\n        emb = torch.arange(num_embeddings, dtype=torch.float).unsqueeze(\n            1\n        ) * emb.unsqueeze(0)\n        emb = torch.cat([torch.sin(emb), torch.cos(emb)], dim=1).view(\n            num_embeddings, -1\n        )\n        if embedding_dim % 2 == 1:\n            # zero pad\n            emb = torch.cat([emb, torch.zeros(num_embeddings, 1)], dim=1)\n        if padding_idx is not None:\n            emb[padding_idx, :] = 0\n        return emb\n\n    def forward(\n        self,\n        input,\n        incremental_state: Optional[Any] = None,\n        timestep: Optional[Tensor] = None,\n        positions: Optional[Any] = None,\n    ):\n        \"\"\"Input is expected to be of size [bsz x seqlen].\"\"\"\n        bspair = torch.onnx.operators.shape_as_tensor(input)\n        bsz, seq_len = bspair[0], bspair[1]\n        max_pos = self.padding_idx + 1 + seq_len\n        if self.weights is None or max_pos > self.weights.size(0):\n            # recompute/expand embeddings if needed\n            self.weights = SinusoidalPositionalEmbedding.get_embedding(\n                max_pos, self.embedding_dim, self.padding_idx\n            )\n        self.weights = self.weights.to(self._float_tensor)\n\n        if incremental_state is not None:\n            # positions is the same for every token when decoding a single step\n            pos = timestep.view(-1)[0] + 1 if timestep is not None else seq_len\n            return self.weights[self.padding_idx + pos, :].expand(bsz, 1, -1)\n\n        positions = utils.make_positions(input, self.padding_idx)\n        return (\n            self.weights.index_select(0, positions.view(-1))\n            .view(bsz, seq_len, -1)\n            .detach()\n        )\n",
        "metaseq/modules/transformer_decoder_layer.py": "# Copyright (c) Meta Platforms, Inc. and affiliates. All Rights Reserved.\n#\n# This source code is licensed under the MIT license found in the\n# LICENSE file in the root directory of this source tree.\n\nimport math\nfrom typing import Dict, Optional\n\nimport torch\nimport torch.nn as nn\nfrom torch import Tensor\n\nfrom metaseq import utils\nfrom metaseq.modules import (\n    ActivationFn,\n    ModelParallelMultiheadAttention,\n    Dropout,\n    FeedForward,\n    LayerNorm,\n)\nfrom metaseq.modules.megatron.mpu import (\n    ColumnParallelLinear,\n    RowParallelLinear,\n)\n\n\ndef _weight_init(weight):\n    return nn.init.kaiming_uniform_(weight, a=math.sqrt(5))\n\n\nclass ModelParallelTransformerDecoderLayer(nn.Module):\n    \"\"\"Decoder layer block.\n    Note that we have found model training to require pre-norm to remain stable.\n    See \"Megatron-LM: https://arxiv.org/pdf/1909.08053.pdf\" for more details.\n    Args:\n        args (argparse.Namespace): parsed command-line arguments\n    \"\"\"\n\n    def __init__(\n        self,\n        args,\n    ):\n        super().__init__()\n        initialize_params_on_gpu = getattr(\n            args, \"tensor_parallel_init_model_on_gpu\", False\n        )\n        device = torch.cuda.current_device() if initialize_params_on_gpu else None\n        dtype = utils.get_model_init_dtype(args)\n\n        self.args = args\n        self.embed_dim = args.decoder_embed_dim\n        self.dropout_module = Dropout(args.dropout, module_name=self.__class__.__name__)\n        self.self_attn = self.build_self_attention(self.embed_dim, args)\n        self.head_dim = int(self.embed_dim / args.decoder_attention_heads)\n        affine_ln = not getattr(args, \"disable_affine_ln\", False)\n        self.self_attn_layer_norm = LayerNorm(\n            self.embed_dim, elementwise_affine=affine_ln\n        )\n        self.self_attn_layer_norm.to(device).to(dtype)\n\n        self.activation_fn_name = getattr(args, \"activation_fn\", \"relu\") or \"relu\"\n\n        # TODO[Susan]: Clean up these kwargs when unifying method signatures between model & non-model parallel.\n        fc1_kwargs = {\n            \"initialize_params_on_gpu\": initialize_params_on_gpu,\n            \"full_megatron_init\": getattr(args, \"full_megatron_init\", False),\n            \"megatron_init_sigma\": getattr(args, \"megatron_init_sigma\", 0.006),\n            \"dtype\": utils.get_model_init_dtype(args),\n            \"disable_bias\": getattr(args, \"disable_bias\", False),\n            \"truncate_init\": getattr(args, \"truncate_init\", False),\n        }\n\n        self.fc1 = self.build_fc1(\n            self.embed_dim,\n            args.decoder_ffn_embed_dim,\n            **fc1_kwargs,\n        )\n\n        self.activation_fn = ActivationFn(\n            self.activation_fn_name,\n            self.build_fc1,\n            self.embed_dim,\n            args.decoder_ffn_embed_dim,\n            **fc1_kwargs,\n        )\n\n        self.fc2 = self.build_fc2(\n            args.decoder_ffn_embed_dim,\n            self.embed_dim,\n            initialize_params_on_gpu=initialize_params_on_gpu,\n            full_megatron_init=getattr(args, \"full_megatron_init\", False),\n            full_megatron_init_scalar=getattr(args, \"full_megatron_init_scalar\", 1.0),\n            megatron_init_sigma=getattr(args, \"megatron_init_sigma\", 0.006),\n            num_layers=args.decoder_layers,\n            dtype=utils.get_model_init_dtype(args),\n            disable_bias=getattr(args, \"disable_bias\", False),\n            truncate_init=getattr(args, \"truncate_init\", False),\n        )\n\n        self.final_layer_norm = LayerNorm(self.embed_dim, elementwise_affine=affine_ln)\n        self.final_layer_norm.to(device).to(dtype)\n\n    def build_fc1(\n        self,\n        input_dim,\n        output_dim,\n        initialize_params_on_gpu,\n        full_megatron_init,\n        megatron_init_sigma,\n        dtype,\n        disable_bias=False,\n        truncate_init=False,\n    ):\n        def _init_method_bias(bias):\n            fan_in = input_dim\n            bound = 1 / math.sqrt(fan_in)\n            nn.init.uniform_(bias, -bound, bound)\n\n        if full_megatron_init:\n            # Setting bias init method to None, initializes biases with zero.\n            init_method_weights = utils.init_method_normal(\n                megatron_init_sigma, truncate_init=truncate_init\n            )\n            init_method_bias = None\n        else:\n            init_method_weights = _weight_init\n            init_method_bias = _init_method_bias\n\n        return ColumnParallelLinear(\n            input_dim,\n            output_dim,\n            gather_output=False,\n            init_method=init_method_weights,\n            init_method_bias=init_method_bias,\n            use_cpu_initialization=not initialize_params_on_gpu,\n            dtype=dtype,\n            bias=not disable_bias,\n        )\n\n    def build_fc2(\n        self,\n        input_dim,\n        output_dim,\n        initialize_params_on_gpu,\n        full_megatron_init,\n        full_megatron_init_scalar,\n        megatron_init_sigma,\n        num_layers,\n        dtype,\n        disable_bias=False,\n        truncate_init=False,\n    ):\n        if full_megatron_init:\n            init_method_weights = utils.scaled_init_method_normal(\n                megatron_init_sigma * full_megatron_init_scalar,\n                num_layers,\n                truncate_init=truncate_init,\n            )\n        else:\n            init_method_weights = _weight_init\n\n        fc2 = RowParallelLinear(\n            input_dim,\n            output_dim,\n            bias=not disable_bias,\n            input_is_parallel=True,\n            init_method=init_method_weights,\n            use_cpu_initialization=not initialize_params_on_gpu,\n            dtype=dtype,\n        )\n        if not full_megatron_init:\n            # Copy nn.linear initialization to get same initialization as of non-model-parallel.\n            # fan_in, _ = nn.init._calculate_fan_in_and_fan_out(fc2.weight)\n            fan_in = input_dim\n            bound = 1 / math.sqrt(fan_in)\n            nn.init.uniform_(fc2.bias, -bound, bound)\n        return fc2\n\n    def build_self_attention(self, embed_dim, args):\n        return ModelParallelMultiheadAttention(\n            embed_dim=embed_dim,\n            num_heads=args.decoder_attention_heads,\n            dropout=args.attention_dropout,\n            self_attention=True,\n            use_cpu_initialization=not getattr(\n                args, \"tensor_parallel_init_model_on_gpu\", False\n            ),\n            full_megatron_init=getattr(args, \"full_megatron_init\", False),\n            full_megatron_init_scalar=getattr(args, \"full_megatron_init_scalar\", 1.0),\n            megatron_init_sigma=getattr(args, \"megatron_init_sigma\", 0.006),\n            num_layers=args.decoder_layers,\n            dtype=utils.get_model_init_dtype(args),\n            bias=not getattr(args, \"disable_bias\", False),\n            attn_variant=getattr(args, \"attn_variant\", \"default\"),\n            xf_attn_op=getattr(args, \"xf_attn_op\", None),\n            truncate_init=getattr(args, \"truncate_init\", None),\n        )\n\n    def forward_attention(\n        self,\n        query,\n        key,\n        value,\n        residual,\n        key_padding_mask=None,\n        incremental_state=None,\n        attn_mask=None,\n    ):\n        # This is calling into ModelParallelMultiheadAttention.forward\n        attn_output, attn_bias = self.self_attn(\n            query=query,\n            key=key,\n            value=value,\n            key_padding_mask=key_padding_mask,\n            incremental_state=incremental_state,\n            attn_mask=attn_mask,\n        )\n        # Note [naman]: got rid off fused bias, dropout and residual cause\n        # now we dont use dropout. And we dont use jit scripting also cause\n        # it seems to use additional gpu memory for activations for dropout\n        # even when its disabled.\n        if attn_bias is not None:\n            attn_output = attn_output + attn_bias.view(1, 1, -1)\n\n        x = torch.nn.functional.dropout(\n            attn_output,\n            p=self.args.dropout,\n            training=self.training,\n        )\n        x = x + residual\n        return x\n\n    def forward(\n        self,\n        x,\n        incremental_state: Optional[Dict[str, Dict[str, Optional[Tensor]]]] = None,\n        self_attn_mask: Optional[torch.Tensor] = None,\n        self_attn_padding_mask: Optional[torch.Tensor] = None,\n        recompute_fc1: bool = False,\n    ):\n        \"\"\"\n        Args:\n            x (Tensor): input to the layer of shape `(seq_len, batch, embed_dim)`\n\n        Returns:\n            encoded output of shape `(seq_len, batch, embed_dim)`\n        \"\"\"\n        if getattr(self.args, \"sequence_parallel\", False):\n            from metaseq.modules import SequeuceParallelTransformerBlock\n\n            x = SequeuceParallelTransformerBlock.apply(\n                x,\n                self.self_attn.qkv_proj.weight,\n                self.self_attn.out_proj.weight,\n                self.fc1.weight,\n                self.fc2.weight,\n                self.self_attn.head_dim,\n                recompute_fc1,\n                self.activation_fn_name,\n            )\n            return x\n\n        residual = x\n        x = self.self_attn_layer_norm(x)\n        x = self.forward_attention(\n            query=x,\n            key=x,\n            value=x,\n            residual=residual,\n            key_padding_mask=self_attn_padding_mask,\n            incremental_state=incremental_state,\n            attn_mask=self_attn_mask,\n        )\n        residual = x\n        x = self.final_layer_norm(x)\n        x = FeedForward(\n            x,\n            fc1=self.fc1,\n            activation_fn=self.activation_fn,\n            fc2=self.fc2,\n            dropout_module=self.dropout_module,\n        )\n        x = residual + x\n        return x\n\n    def make_generation_fast_(self, **kwargs):\n        pass\n",
        "metaseq/nan_detector.py": "# Copyright (c) Meta Platforms, Inc. and affiliates. All Rights Reserved.\n#\n# This source code is licensed under the MIT license found in the\n# LICENSE file in the root directory of this source tree.\n\nimport logging\n\nimport torch\n\nlogger = logging.getLogger(__name__)\n\n\nclass NanDetector:\n    \"\"\"\n    Detects the first NaN or Inf in forward and/or backward pass and logs, together with the module name\n    \"\"\"\n\n    def __init__(self, model, forward=True, backward=True):\n        self.bhooks = []\n        self.fhooks = []\n        self.forward = forward\n        self.backward = backward\n        self.named_parameters = list(model.named_parameters())\n        self.reset()\n\n        for name, mod in model.named_modules():\n            mod.__module_name = name\n            self.add_hooks(mod)\n\n    def __enter__(self):\n        return self\n\n    def __exit__(self, exc_type, exc_value, exc_traceback):\n        # Dump out all model gnorms to enable better debugging\n        norm = {}\n        gradients = {}\n        for name, param in self.named_parameters:\n            if param.grad is not None:\n                grad_norm = torch.norm(param.grad.data, p=2, dtype=torch.float32)\n                norm[name] = grad_norm.item()\n                if torch.isnan(grad_norm).any() or torch.isinf(grad_norm).any():\n                    gradients[name] = param.grad.data\n        if len(gradients) > 0:\n            logger.info(\"Detected nan/inf grad norm, dumping norms...\")\n            logger.info(f\"norms: {norm}\")\n            logger.info(f\"gradients: {gradients}\")\n\n        self.close()\n\n    def add_hooks(self, module):\n        if self.forward:\n            self.fhooks.append(module.register_forward_hook(self.fhook_fn))\n        if self.backward:\n            self.bhooks.append(module.register_backward_hook(self.bhook_fn))\n\n    def reset(self):\n        self.has_printed_f = False\n        self.has_printed_b = False\n\n    def _detect(self, tensor, name, backward):\n        err = None\n        if (\n            torch.is_floating_point(tensor)\n            # single value tensors (like the loss) will not provide much info\n            and tensor.numel() >= 2\n        ):\n            with torch.no_grad():\n                if torch.isnan(tensor).any():\n                    err = \"NaN\"\n                elif torch.isinf(tensor).any():\n                    err = \"Inf\"\n        if err is not None:\n            err = f\"{err} detected in output of {name}, shape: {tensor.shape}, {'backward' if backward else 'forward'}\"\n        return err\n\n    def _apply(self, module, inp, x, backward):\n        if torch.is_tensor(x):\n            if isinstance(inp, tuple) and len(inp) > 0:\n                inp = inp[0]\n            err = self._detect(x, module.__module_name, backward)\n            if err is not None:\n                if torch.is_tensor(inp) and not backward:\n                    err += (\n                        f\" input max: {inp.max().item()}, input min: {inp.min().item()}\"\n                    )\n\n                has_printed_attr = \"has_printed_b\" if backward else \"has_printed_f\"\n                logger.warning(err)\n                setattr(self, has_printed_attr, True)\n        elif isinstance(x, dict):\n            for v in x.values():\n                self._apply(module, inp, v, backward)\n        elif isinstance(x, list) or isinstance(x, tuple):\n            for v in x:\n                self._apply(module, inp, v, backward)\n\n    def fhook_fn(self, module, inp, output):\n        if not self.has_printed_f:\n            self._apply(module, inp, output, backward=False)\n\n    def bhook_fn(self, module, inp, output):\n        if not self.has_printed_b:\n            self._apply(module, inp, output, backward=True)\n\n    def close(self):\n        for hook in self.fhooks + self.bhooks:\n            hook.remove()\n",
        "metaseq/optim/__init__.py": "# Copyright (c) Meta Platforms, Inc. and affiliates. All Rights Reserved.\n#\n# This source code is licensed under the MIT license found in the\n# LICENSE file in the root directory of this source tree.\n\"\"\"isort:skip_file\"\"\"\n\nimport importlib\nimport os\n\nfrom metaseq import registry\nfrom metaseq.optim.base_optimizer import (  # noqa\n    BaseOptimizer,\n    LegacyOptimizer,\n)\nfrom metaseq.optim.fp16_optimizer import FP16Optimizer, MemoryEfficientFP16Optimizer\nfrom metaseq.optim.shard import shard_\nfrom omegaconf import DictConfig\n\n__all__ = [\n    \"BaseOptimizer\",\n    \"FP16Optimizer\",\n    \"MemoryEfficientFP16Optimizer\",\n    \"shard_\",\n    \"register_optimizer\",\n]\n\n(\n    _build_optimizer,\n    register_optimizer,\n    OPTIMIZER_REGISTRY,\n    OPTIMIZER_DATACLASS_REGISTRY,\n) = registry.setup_registry(\"--optimizer\", base_class=BaseOptimizer, required=True)\n\n\ndef build_optimizer(cfg: DictConfig, params, *extra_args, **extra_kwargs):\n    if all(isinstance(p, dict) for p in params):\n        params = [t for p in params for t in p.values()]\n    params = list(filter(lambda p: p.requires_grad, params))\n    return _build_optimizer(cfg, params, *extra_args, **extra_kwargs)\n\n\n# automatically import any Python files in the optim/ directory\nfor file in os.listdir(os.path.dirname(__file__)):\n    if file.endswith(\".py\") and not file.startswith(\"_\"):\n        file_name = file[: file.find(\".py\")]\n        importlib.import_module(\"metaseq.optim.\" + file_name)\n",
        "metaseq/optim/adam.py": "# Copyright (c) Meta Platforms, Inc. and affiliates. All Rights Reserved.\n#\n# This source code is licensed under the MIT license found in the\n# LICENSE file in the root directory of this source tree.\n\nimport logging\nimport math\nfrom collections.abc import Collection\nfrom dataclasses import dataclass, field\nfrom typing import List\n\nimport torch\nimport torch.optim\nfrom omegaconf import II, DictConfig\n\nfrom metaseq.dataclass import MetaseqDataclass\nfrom metaseq.optim import BaseOptimizer, register_optimizer\nfrom metaseq.optim.fused_adam import get_fused_adam_class\n\nlogger = logging.getLogger(__name__)\n\n\n@dataclass\nclass MetaseqAdamConfig(MetaseqDataclass):\n    adam_betas: str = field(\n        default=\"(0.9, 0.999)\", metadata={\"help\": \"betas for Adam optimizer\"}\n    )\n    adam_eps: float = field(\n        default=1e-8, metadata={\"help\": \"epsilon for Adam optimizer\"}\n    )\n    weight_decay: float = field(default=0.0, metadata={\"help\": \"weight decay\"})\n    use_old_adam: bool = field(\n        default=False, metadata={\"help\": \"Use metaseq.optim.adam.Adam\"}\n    )\n    fp16_adam_stats: bool = field(\n        default=False, metadata={\"help\": \"use FP16 stats (with automatic scaling)\"}\n    )\n    # TODO common vars below in parent\n    lr: List[float] = II(\"optimization.lr\")\n\n\n@register_optimizer(\"adam\", dataclass=MetaseqAdamConfig)\nclass MetaseqAdam(BaseOptimizer):\n    \"\"\"Adam optimizer for metaseq.\n\n    Important note: this optimizer corresponds to the \"AdamW\" variant of\n    Adam in its weight decay behavior. As such, it is most closely\n    analogous to torch.optim.AdamW from PyTorch.\n    \"\"\"\n\n    def __init__(self, cfg: DictConfig, params):\n        super().__init__(cfg)\n        fused_adam_cls = get_fused_adam_class()\n        use_fused_adam = (\n            not getattr(cfg, \"use_old_adam\", False)\n            and fused_adam_cls is not None\n            and torch.cuda.is_available()\n        )\n        if use_fused_adam:\n            logger.info(\"using FusedAdam\")\n            self._optimizer = fused_adam_cls(\n                params, use_fp16_stats=self.cfg.fp16_adam_stats, **self.optimizer_config\n            )\n        else:\n            if self.cfg.fp16_adam_stats:\n                raise NotImplementedError(\n                    \"--fp16-adam-stats is only supported with FusedAdamV1\"\n                )\n            self._optimizer = Adam(params, **self.optimizer_config)\n\n    @property\n    def optimizer_config(self):\n        \"\"\"\n        Return a kwarg dictionary that will be used to override optimizer\n        args stored in checkpoints. This allows us to load a checkpoint and\n        resume training using a different set of optimizer args, e.g., with a\n        different learning rate.\n        \"\"\"\n        return {\n            \"lr\": self.cfg.lr[0]\n            if isinstance(self.cfg.lr, Collection)\n            else self.cfg.lr,\n            \"betas\": eval(self.cfg.adam_betas),\n            \"eps\": self.cfg.adam_eps,\n            \"weight_decay\": self.cfg.weight_decay,\n        }\n\n\nclass Adam(torch.optim.Optimizer):\n    r\"\"\"Implements Adam algorithm.\n\n    This implementation is modified from torch.optim.Adam based on:\n    `Fixed Weight Decay Regularization in Adam`\n    (see https://arxiv.org/abs/1711.05101)\n\n    It has been proposed in `Adam: A Method for Stochastic Optimization`_.\n\n    Args:\n        params (iterable): iterable of parameters to optimize or dicts defining\n            parameter groups\n        lr (float, optional): learning rate (default: 1e-3)\n        betas (Tuple[float, float], optional): coefficients used for computing\n            running averages of gradient and its square (default: (0.9, 0.999))\n        eps (float, optional): term added to the denominator to improve\n            numerical stability (default: 1e-8)\n        weight_decay (float, optional): weight decay (L2 penalty) (default: 0)\n        amsgrad (boolean, optional): whether to use the AMSGrad variant of this\n            algorithm from the paper `On the Convergence of Adam and Beyond`_\n\n    .. _Adam\\: A Method for Stochastic Optimization:\n        https://arxiv.org/abs/1412.6980\n    .. _On the Convergence of Adam and Beyond:\n        https://openreview.net/forum?id=ryQu7f-RZ\n    \"\"\"\n\n    def __init__(\n        self,\n        params,\n        lr=1e-3,\n        betas=(0.9, 0.999),\n        eps=1e-8,\n        weight_decay=0,\n        amsgrad=False,\n    ):\n        defaults = dict(\n            lr=lr, betas=betas, eps=eps, weight_decay=weight_decay, amsgrad=amsgrad\n        )\n        super(Adam, self).__init__(params, defaults)\n\n    @property\n    def supports_memory_efficient_fp16(self):\n        return True\n\n    @property\n    def supports_flat_params(self):\n        return True\n\n    def step(self, closure=None):\n        \"\"\"Performs a single optimization step.\n\n        Args:\n            closure (callable, optional): A closure that reevaluates the model\n                and returns the loss.\n        \"\"\"\n        loss = None\n        if closure is not None:\n            loss = closure()\n\n        for group in self.param_groups:\n            for p in group[\"params\"]:\n                if p.grad is None:\n                    continue\n                grad = p.grad.data\n                if grad.dtype in {torch.float16, torch.bfloat16}:\n                    grad = grad.float()\n                if grad.is_sparse:\n                    raise RuntimeError(\n                        \"Adam does not support sparse gradients, please consider SparseAdam instead\"\n                    )\n                amsgrad = group.get(\"amsgrad\", False)\n\n                p_data_fp32 = p.data\n                if p.data.dtype in {torch.float16, torch.bfloat16}:\n                    p_data_fp32 = p_data_fp32.float()\n\n                state = self.state[p]\n\n                # State initialization\n                if len(state) == 0:\n                    state[\"step\"] = 0\n                    # Exponential moving average of gradient values\n                    state[\"exp_avg\"] = torch.zeros_like(p_data_fp32)\n                    # Exponential moving average of squared gradient values\n                    state[\"exp_avg_sq\"] = torch.zeros_like(p_data_fp32)\n                    if amsgrad:\n                        # Maintains max of all exp. moving avg. of sq. grad. values\n                        state[\"max_exp_avg_sq\"] = torch.zeros_like(p_data_fp32)\n                else:\n                    state[\"exp_avg\"] = state[\"exp_avg\"].to(p_data_fp32)\n                    state[\"exp_avg_sq\"] = state[\"exp_avg_sq\"].to(p_data_fp32)\n                    if amsgrad:\n                        state[\"max_exp_avg_sq\"] = state[\"max_exp_avg_sq\"].to(\n                            p_data_fp32\n                        )\n\n                exp_avg, exp_avg_sq = state[\"exp_avg\"], state[\"exp_avg_sq\"]\n                if amsgrad:\n                    max_exp_avg_sq = state[\"max_exp_avg_sq\"]\n                beta1, beta2 = group[\"betas\"]\n\n                state[\"step\"] += 1\n\n                # Decay the first and second moment running average coefficient\n                exp_avg.mul_(beta1).add_(grad, alpha=1 - beta1)\n                exp_avg_sq.mul_(beta2).addcmul_(grad, grad, value=1 - beta2)\n                if amsgrad:\n                    # Maintains the maximum of all 2nd moment running avg. till now\n                    torch.max(max_exp_avg_sq, exp_avg_sq, out=max_exp_avg_sq)\n                    # Use the max. for normalizing running avg. of gradient\n                    denom = max_exp_avg_sq.sqrt().add_(group[\"eps\"])\n                else:\n                    denom = exp_avg_sq.sqrt().add_(group[\"eps\"])\n\n                bias_correction1 = 1 - beta1 ** state[\"step\"]\n                bias_correction2 = 1 - beta2 ** state[\"step\"]\n                step_size = group[\"lr\"] * math.sqrt(bias_correction2) / bias_correction1\n\n                if group[\"weight_decay\"] != 0:\n                    p_data_fp32.add_(\n                        p_data_fp32, alpha=-group[\"weight_decay\"] * group[\"lr\"]\n                    )\n\n                p_data_fp32.addcdiv_(exp_avg, denom, value=-step_size)\n\n                if p.data.dtype in {torch.float16, torch.bfloat16}:\n                    p.data.copy_(p_data_fp32)\n\n        return loss\n",
        "metaseq/optim/base_optimizer.py": "# Copyright (c) Meta Platforms, Inc. and affiliates. All Rights Reserved.\n#\n# This source code is licensed under the MIT license found in the\n# LICENSE file in the root directory of this source tree.\n\nimport torch\n\nfrom metaseq import utils\nfrom metaseq.dataclass.utils import gen_parser_from_dataclass\n\n\nclass BaseOptimizer(object):\n    def __init__(self, cfg):\n        super().__init__()\n        self.cfg = cfg\n\n    @classmethod\n    def add_args(cls, parser):\n        \"\"\"Add optimizer-specific arguments to the parser.\"\"\"\n        dc = getattr(cls, \"__dataclass\", None)\n        if dc is not None:\n            gen_parser_from_dataclass(parser, dc())\n\n    @property\n    def optimizer(self):\n        \"\"\"Return a torch.optim.optimizer.Optimizer instance.\"\"\"\n        if not hasattr(self, \"_optimizer\"):\n            raise NotImplementedError\n        if not isinstance(self._optimizer, torch.optim.Optimizer):\n            raise ValueError(\"_optimizer must be an instance of torch.optim.Optimizer\")\n        return self._optimizer\n\n    @optimizer.setter\n    def optimizer(self, optimizer):\n        \"\"\"Reset optimizer instance.\"\"\"\n        if not hasattr(self, \"_optimizer\"):\n            raise NotImplementedError\n        if not isinstance(self._optimizer, torch.optim.Optimizer):\n            raise ValueError(\"_optimizer must be an instance of torch.optim.Optimizer\")\n        self._optimizer = optimizer\n\n    @property\n    def optimizer_config(self):\n        \"\"\"\n        Return a kwarg dictionary that will be used to override optimizer\n        args stored in checkpoints. This allows us to load a checkpoint and\n        resume training using a different set of optimizer args, e.g., with a\n        different learning rate.\n        \"\"\"\n        raise NotImplementedError\n\n    @property\n    def params(self):\n        \"\"\"Return an iterable of the parameters held by the optimizer.\"\"\"\n        for param_group in self.param_groups:\n            for p in param_group[\"params\"]:\n                yield p\n\n    @property\n    def param_groups(self):\n        return self.optimizer.param_groups\n\n    def __getstate__(self):\n        return self._optimizer.__getstate__()\n\n    def get_lr(self):\n        \"\"\"Return the current learning rate.\"\"\"\n        return self.param_groups[0][\"lr\"]\n\n    def set_lr(self, lr):\n        \"\"\"Set the learning rate.\"\"\"\n        for param_group in self.param_groups:\n            param_group[\"lr\"] = lr\n\n    def state_dict(self):\n        \"\"\"Return the optimizer's state dict.\"\"\"\n        return self.optimizer.state_dict()\n\n    def load_state_dict(self, state_dict, optimizer_overrides=None):\n        \"\"\"Load an optimizer state dict.\n\n        In general we should prefer the configuration of the existing optimizer\n        instance (e.g., learning rate) over that found in the state_dict. This\n        allows us to resume training from a checkpoint using a new set of\n        optimizer args.\n        \"\"\"\n        self.optimizer.load_state_dict(state_dict)\n\n        if optimizer_overrides is not None and len(optimizer_overrides) > 0:\n            # override learning rate, momentum, etc. with latest values\n            for group in self.param_groups:\n                group.update(optimizer_overrides)\n\n    def backward(self, loss):\n        \"\"\"Computes the sum of gradients of the given tensor w.r.t. graph leaves.\"\"\"\n        loss.backward()\n\n    def all_reduce_grads(self, module):\n        \"\"\"Manually all-reduce gradients (if required).\"\"\"\n        if hasattr(module, \"all_reduce_grads\"):\n            module.all_reduce_grads()\n\n    def multiply_grads(self, c):\n        \"\"\"Multiplies grads by a constant *c*.\"\"\"\n        for p in self.params:\n            if p.grad is not None:\n                if torch.is_tensor(c):\n                    c = c.to(p.grad.device)\n                p.grad.data.mul_(c)\n\n    def clip_grad_norm(\n        self, max_norm, norm_type=\"l2\", aggregate_norm_fn=None, **kwargs\n    ):\n        \"\"\"Clips gradient norm.\"\"\"\n        return utils.clip_grad_norm_(\n            self.params, max_norm, norm_type, aggregate_norm_fn\n        )\n\n    def step(self, closure=None, scale=1.0, groups=None):\n        \"\"\"Performs a single optimization step.\"\"\"\n        if self.supports_step_with_scale:\n            if self.supports_groups:\n                self.optimizer.step(closure, scale=scale, groups=groups)\n            else:\n                self.optimizer.step(closure, scale=scale)\n        else:\n            if scale != 1.0:\n                self.multiply_grads(1.0 / scale)\n            if self.supports_groups:\n                self.optimizer.step(closure, groups=groups)\n            else:\n                self.optimizer.step(closure)\n\n    def zero_grad(self):\n        \"\"\"Clears the gradients of all optimized parameters.\"\"\"\n        for p in self.params:\n            p.grad = None\n        self.optimizer.zero_grad()\n\n    @property\n    def supports_memory_efficient_fp16(self):\n        if hasattr(self.optimizer, \"supports_memory_efficient_fp16\"):\n            return self.optimizer.supports_memory_efficient_fp16\n        return False\n\n    @property\n    def supports_step_with_scale(self):\n        if hasattr(self.optimizer, \"supports_step_with_scale\"):\n            return self.optimizer.supports_step_with_scale\n        return False\n\n    @property\n    def supports_groups(self):\n        if hasattr(self.optimizer, \"supports_groups\"):\n            return self.optimizer.supports_groups\n        return False\n\n    @property\n    def supports_flat_params(self):\n        \"\"\"\n        Whether the optimizer supports collapsing of the model\n        parameters/gradients into a single contiguous Tensor.\n        \"\"\"\n        if hasattr(self.optimizer, \"supports_flat_params\"):\n            return self.optimizer.supports_flat_params\n        return False\n\n    def broadcast_global_state_dict(self, state_dict):\n        \"\"\"\n        Broadcasts a global state dict to all ranks.\n        Useful for optimizers that shard state between ranks.\n        \"\"\"\n        if hasattr(self.optimizer, \"broadcast_global_state_dict\"):\n            return self.optimizer.broadcast_global_state_dict(state_dict)\n        else:\n            return state_dict\n\n\nclass LegacyOptimizer(BaseOptimizer):\n    def __init__(self, args):\n        self.args = args\n",
        "metaseq/optim/dynamic_loss_scaler.py": "# Copyright (c) Meta Platforms, Inc. and affiliates. All Rights Reserved.\n#\n# This source code is licensed under the MIT license found in the\n# LICENSE file in the root directory of this source tree.\n\nimport logging\n\nlogger = logging.getLogger(__name__)\n\n\nclass DynamicLossScaler(object):\n    def __init__(\n        self,\n        init_scale=4.0,\n        scale_factor=2.0,\n        scale_window=256,\n        tolerance=0.0,\n        threshold=None,\n        min_loss_scale=2**-5,\n    ):\n        self.loss_scale = init_scale\n        self.scale_factor = scale_factor\n        self.scale_window = scale_window\n\n        logger.info(\n            f\"*** SCALE_WINDOW: {self.scale_window}, loss scale: {self.loss_scale} ***\"\n        )\n\n        self.tolerance = tolerance\n        self.threshold = threshold\n        self._iter = 0\n        self._last_overflow_iter = -1\n        self._last_rescale_iter = -1\n        self._overflows_since_rescale = 0\n        self.min_loss_scale = min_loss_scale\n\n    def scale(self, outputs):\n        return self.loss_scale * outputs\n\n    def update(self):\n        if (self._iter - self._last_overflow_iter) % self.scale_window == 0:\n            self.loss_scale *= self.scale_factor\n            self._last_rescale_iter = self._iter\n            # When scaling up loss_scale, also scale up the scale_window.\n            self.scale_window *= self.scale_factor\n        self._iter += 1\n\n    def _decrease_loss_scale(self):\n        self.loss_scale /= self.scale_factor\n        # also decrease the scale_window (lower loss scale, smaller window)\n        self.scale_window = max(int(self.scale_window / self.scale_factor), 1)\n        if self.threshold is not None:\n            self.loss_scale = max(self.loss_scale, self.threshold)\n\n    def check_overflow(self, grad_norm):\n        # detect inf and nan\n        if grad_norm == float(\"inf\") or grad_norm != grad_norm:\n            # overflow has occurred\n            prev_scale = self.loss_scale\n            iter_since_rescale = self._iter - self._last_rescale_iter\n\n            self._last_overflow_iter = self._iter\n            self._overflows_since_rescale += 1\n            pct_overflow = self._overflows_since_rescale / float(iter_since_rescale)\n            if pct_overflow >= self.tolerance:\n                self._decrease_loss_scale()\n                self._last_rescale_iter = self._iter\n                self._overflows_since_rescale = 0\n\n            if self.loss_scale < self.min_loss_scale:\n                # Don't scale down past min_loss_scale, just continue to skip grad after overflow error is raised.\n                self.loss_scale = prev_scale\n\n            self._iter += 1\n            raise OverflowError(\"setting loss scale to: \" + str(self.loss_scale))\n",
        "metaseq/optim/fp16_optimizer.py": "# Copyright (c) Meta Platforms, Inc. and affiliates. All Rights Reserved.\n#\n# This source code is licensed under the MIT license found in the\n# LICENSE file in the root directory of this source tree.\n\nfrom collections import defaultdict\nfrom itertools import chain\n\nimport torch\nfrom omegaconf import DictConfig\n\nfrom metaseq import optim\nfrom .dynamic_loss_scaler import DynamicLossScaler\n\n\nclass _FP16OptimizerMixin(object):\n    def __init__(self, *args, **kwargs):\n        # forward __init__ call to the next class in mro(method resolution order)\n        super().__init__(*args, **kwargs)\n        self._multiply_factor = 1.0\n\n    @property\n    def has_flat_params(self):\n        return torch.is_tensor(self.fp32_params) or (\n            isinstance(self.fp32_params, dict)\n            and all(torch.is_tensor(t) for t in self.fp32_params.values())\n        )\n\n    @classmethod\n    def build_fp32_params(cls, args, params, flatten=True):\n        # create FP32 copy of parameters and grads\n        if flatten:\n            total_param_size = sum(p.data.numel() for p in params)\n            devices = [torch.cuda.current_device()]\n            fp32_params = {}\n            for device in devices:\n                device_param_size = total_param_size\n                device_params = params\n                fp32_params[device] = (\n                    device_params[0].new(0).float().new(device_param_size)\n                )\n                offset = 0\n                for p in device_params:\n                    numel = p.data.numel()\n                    fp32_params[device][offset : offset + numel].copy_(p.data.view(-1))\n                    offset += numel\n                fp32_params[device] = torch.nn.Parameter(fp32_params[device])\n                fp32_params[device].grad = fp32_params[device].data.new(\n                    device_param_size\n                )\n            return fp32_params\n        else:\n            fp32_params = []\n            for p in params:\n                p32 = torch.nn.Parameter(p.data.float())\n                p32.grad = torch.zeros_like(p32.data)\n                if hasattr(p, \"param_group\"):\n                    p32.param_group = p.param_group\n                fp32_params.append(p32)\n            return fp32_params\n\n    def state_dict(self):\n        \"\"\"Return the optimizer's state dict.\"\"\"\n        state_dict = self.fp32_optimizer.state_dict()\n        if self.scaler is not None:\n            state_dict[\"loss_scale\"] = self.scaler.loss_scale\n        return state_dict\n\n    def load_state_dict(self, state_dict, optimizer_overrides=None):\n        \"\"\"Load an optimizer state dict.\n\n        In general we should prefer the configuration of the existing optimizer\n        instance (e.g., learning rate) over that found in the state_dict. This\n        allows us to resume training from a checkpoint using a new set of\n        optimizer args.\n        \"\"\"\n        if \"loss_scale\" in state_dict and self.scaler is not None:\n            self.scaler.loss_scale = state_dict[\"loss_scale\"]\n        self.fp32_optimizer.load_state_dict(state_dict, optimizer_overrides)\n\n    def backward(self, loss):\n        \"\"\"Computes the sum of gradients of the given tensor w.r.t. graph leaves.\n\n        Compared to :func:`metaseq.optim.BaseOptimizer.backward`, this\n        function additionally dynamically scales the loss to avoid gradient\n        underflow.\n        \"\"\"\n        if self.scaler is not None:\n            loss = self.scaler.scale(loss)\n        loss.backward()\n        self._needs_sync = True\n\n    def _sync_fp16_grads_to_fp32(self):\n        if self._needs_sync:\n            # copy FP16 grads to FP32\n            if self.has_flat_params:\n                devices = list(self.fp32_params.keys())\n                device_params_dict = defaultdict(list)\n                for p in self.fp16_params:\n                    if p.requires_grad:\n                        device_params_dict[p.device.index].append(p)\n                for device in devices:\n                    device_params = device_params_dict[device]\n                    offset = 0\n                    for p in device_params:\n                        grad_data = (\n                            p.grad.data\n                            if p.grad is not None\n                            else p.data.new_zeros(p.data.shape)\n                        )\n                        numel = grad_data.numel()\n                        self.fp32_params[device].grad.data[\n                            offset : offset + numel\n                        ].copy_(grad_data.view(-1))\n                        offset += numel\n            else:\n                for p, p32 in zip(self.fp16_params, self.fp32_params):\n                    if not p.requires_grad:\n                        continue\n                    if p.grad is not None:\n                        if p32.grad is None:\n                            p32.grad = p.grad.data.float()\n                        else:\n                            p32.grad.data.copy_(p.grad.data)\n                    else:\n                        p32.grad = torch.zeros_like(p.data, dtype=torch.float)\n\n            self._needs_sync = False\n\n    def _sync_fp32_params_to_fp16(self):\n        # copy FP32 params back into FP16 model\n        if self.has_flat_params:\n            devices = list(self.fp32_params.keys())\n            device_params_dict = defaultdict(list)\n            for p in self.fp16_params:\n                device_params_dict[p.device.index].append(p)\n            for device in devices:\n                device_params = device_params_dict[device]\n                offset = 0\n                for p in device_params:\n                    numel = p.data.numel()\n                    p.data.copy_(\n                        self.fp32_params[device]\n                        .data[offset : offset + numel]\n                        .view_as(p.data)\n                    )\n                    offset += numel\n        else:\n            for p, p32 in zip(self.fp16_params, self.fp32_params):\n                if not p.requires_grad:\n                    continue\n                p.data.copy_(p32.data)\n\n    def _unscale_grads(self):\n        self._sync_fp16_grads_to_fp32()\n        if (\n            # Skip the multiplication if it's a no-op (i.e., if _multiply_factor\n            # is 1.0). At the same time, we want to avoid the device-to-host\n            # transfer by comparing it to 1.0. Since _multiply_factor starts as\n            # a Python float, we roughly assume that if it's a tensor then it's\n            # probably not =1.0 anymore and we do the multiplication. Otherwise\n            # we can safely check the value without a D2H transfer.\n            torch.is_tensor(self._multiply_factor)\n            or self._multiply_factor != 1.0\n        ):\n            self.fp32_optimizer.multiply_grads(self._multiply_factor)\n            self._multiply_factor = 1.0\n\n    def multiply_grads(self, c):\n        \"\"\"Multiplies grads by a constant ``c``.\"\"\"\n        self._multiply_factor *= c\n\n    def clip_grad_norm(\n        self,\n        max_norm,\n        norm_type=\"l2\",\n        aggregate_norm_fn=None,\n        skip_gradient_update_on_clip_norm=False,\n    ):\n        \"\"\"Clips gradient norm and updates dynamic loss scaler.\"\"\"\n        self._sync_fp16_grads_to_fp32()\n\n        grad_norm = self._multiply_factor * self.fp32_optimizer.clip_grad_norm(\n            0, norm_type, aggregate_norm_fn\n        )\n\n        if self.scaler is not None:\n            self.scaler.check_overflow(grad_norm)\n            if skip_gradient_update_on_clip_norm:\n                # detect overflow and adjust loss scale\n                self.scaler.check_overflow(grad_norm)\n                if grad_norm > max_norm > 0.0:\n                    raise OverflowError(\n                        f\"Grad norm: {grad_norm:.2f} exceeds threshold: {max_norm:.2f}, rejecting batch.\"\n                    )\n            else:\n                if grad_norm > max_norm > 0.0:\n                    self._multiply_factor *= max_norm / grad_norm\n                # detect overflow and adjust loss scale\n                self.scaler.check_overflow(grad_norm)\n        elif max_norm > 0.0:\n            clip_coef = (max_norm / (grad_norm + 1e-6)).clamp_(max=1)\n            self._multiply_factor *= clip_coef\n\n        return grad_norm\n\n    def step(self, closure=None, groups=None):\n        \"\"\"Performs a single optimization step.\"\"\"\n        self._sync_fp16_grads_to_fp32()\n\n        if getattr(self, \"supports_step_with_scale\", False):\n            self.fp32_optimizer.step(\n                closure, scale=(1.0 / self._multiply_factor), groups=groups\n            )\n        else:\n            self._unscale_grads()\n            self.fp32_optimizer.step(closure, groups=groups)\n\n        if self.scaler is not None:\n            self.scaler.update()\n\n        self._sync_fp32_params_to_fp16()\n\n    def zero_grad(self):\n        \"\"\"Clears the gradients of all optimized parameters.\"\"\"\n        for p in self.fp16_params:\n            p.grad = None\n        if self.has_flat_params:\n            if torch.is_tensor(self.fp32_params):\n                self.fp32_params.grad.zero_()\n            elif isinstance(self.fp32_params, dict):\n                for fp32_params in self.fp32_params.values():\n                    fp32_params.grad.zero_()\n            else:\n                raise RuntimeError(\"self.fp32_params must be a tensor or dict\")\n        else:\n            for p32 in self.fp32_params:\n                if p32.grad is not None:\n                    p32.grad.zero_()\n        self._needs_sync = False\n\n        if self.scaler is not None:\n            self._multiply_factor = 1.0 / float(self.scaler.loss_scale)\n        else:\n            self._multiply_factor = 1.0\n\n\nclass FP16Optimizer(_FP16OptimizerMixin, optim.BaseOptimizer):\n    \"\"\"\n    Wrap an *optimizer* to support FP16 (mixed precision) training.\n    \"\"\"\n\n    def __init__(self, cfg: DictConfig, params, fp32_optimizer, fp32_params, **kwargs):\n        super().__init__(cfg.optimizer)\n        self.fp16_params = params\n        self.fp32_optimizer = fp32_optimizer\n        self.fp32_params = fp32_params\n\n        # No loss scaler required for training with bf16\n        self.scaler = (\n            None\n            if cfg.common.bf16\n            else DynamicLossScaler(\n                init_scale=cfg.common.fp16_init_scale,\n                scale_window=cfg.common.fp16_scale_window,\n                tolerance=cfg.common.fp16_scale_tolerance,\n                threshold=cfg.common.threshold_loss_scale,\n                min_loss_scale=cfg.common.min_loss_scale,\n            )\n        )\n\n    @classmethod\n    def build_optimizer(cls, cfg: DictConfig, params, **kwargs):\n        \"\"\"\n        Args:\n            cfg (omegaconf.DictConfig): metaseq args\n            params (iterable): iterable of parameters to optimize\n        \"\"\"\n        flatten = not getattr(cfg.common, \"fp16_no_flatten_grads\", False)\n        fp32_params = cls.build_fp32_params(cfg.optimizer, params, flatten=flatten)\n        if flatten:\n            fp32_optimizer = optim.build_optimizer(cfg.optimizer, [fp32_params])\n        else:\n            fp32_optimizer = optim.build_optimizer(cfg.optimizer, fp32_params)\n        if flatten and not fp32_optimizer.supports_flat_params:\n            raise RuntimeError(\n                f\"chosen optimizer {fp32_optimizer.__class__.__name__} does not \"\n                f\"support flat params, please set --fp16-no-flatten-grads\"\n            )\n        return cls(cfg, params, fp32_optimizer, fp32_params, **kwargs)\n\n    @property\n    def optimizer(self):\n        return self.fp32_optimizer.optimizer\n\n    @optimizer.setter\n    def optimizer(self, optimizer):\n        self.fp32_optimizer.optimizer = optimizer\n\n    @property\n    def lr_scheduler(self):\n        return getattr(self.fp32_optimizer, \"lr_scheduler\", None)\n\n    @property\n    def optimizer_config(self):\n        return self.fp32_optimizer.optimizer_config\n\n    def get_lr(self):\n        return self.fp32_optimizer.get_lr()\n\n    def set_lr(self, lr):\n        self.fp32_optimizer.set_lr(lr)\n\n    def all_reduce_grads(self, module):\n        self.fp32_optimizer.all_reduce_grads(module)\n\n    @property\n    def supports_flat_params(self):\n        return self.fp32_optimizer.supports_flat_params\n\n\nclass _MemoryEfficientFP16OptimizerMixin(object):\n    def __init__(self, *args, **kwargs):\n        # forward __init__ call to the next class in MRO (method resolution order)\n        super().__init__(*args, **kwargs)\n        self._multiply_factor = 1.0\n\n    @property\n    def has_flat_params(self):\n        return False\n\n    def state_dict(self):\n        \"\"\"Return the optimizer's state dict.\"\"\"\n        state_dict = self.wrapped_optimizer.state_dict()\n        if self.scaler is not None:\n            state_dict[\"loss_scale\"] = self.scaler.loss_scale\n        return state_dict\n\n    def load_state_dict(self, state_dict, optimizer_overrides=None):\n        \"\"\"Load an optimizer state dict.\n\n        In general we should prefer the configuration of the existing optimizer\n        instance (e.g., learning rate) over that found in the state_dict. This\n        allows us to resume training from a checkpoint using a new set of\n        optimizer args.\n        \"\"\"\n        if \"loss_scale\" in state_dict and self.scaler is not None:\n            self.scaler.loss_scale = state_dict[\"loss_scale\"]\n\n        self.wrapped_optimizer.load_state_dict(state_dict, optimizer_overrides)\n\n        # Hack: PyTorch automatically casts the optimizer state to match the\n        # type of the current parameters. But with --memory-efficient-fp16 the\n        # params are FP16 while the optimizer state is FP32 and we don't want\n        # to cast. A workaround is to manually copy back the original state\n        # after the optimizer has been loaded.\n        if not getattr(self.optimizer, \"disable_mem_eff_fp16_loading_hack\", False):\n            groups = self.optimizer.param_groups\n            saved_groups = state_dict[\"param_groups\"]\n            id_map = {\n                old_id: p\n                for old_id, p in zip(\n                    chain(*(g[\"params\"] for g in saved_groups)),\n                    chain(*(g[\"params\"] for g in groups)),\n                )\n            }\n            for k, v in state_dict[\"state\"].items():\n                if k in id_map:\n                    param = id_map[k]\n                    self.optimizer.state[param] = v\n\n    def backward(self, loss):\n        \"\"\"Computes the sum of gradients of the given tensor w.r.t. graph leaves.\n\n        Compared to :func:`metaseq.optim.BaseOptimizer.backward`, this\n        function additionally dynamically scales the loss to avoid gradient\n        underflow.\n        \"\"\"\n        if self.scaler is not None:\n            loss = self.scaler.scale(loss)\n        loss.backward()\n\n    def _unscale_grads(self):\n        if (\n            # Skip the multiplication if it's a no-op (i.e., if _multiply_factor\n            # is 1.0). At the same time, we want to avoid the device-to-host\n            # transfer by comparing it to 1.0. Since _multiply_factor starts as\n            # a Python float, we roughly assume that if it's a tensor then it's\n            # probably not =1.0 anymore and we do the multiplication. Otherwise\n            # we can safely check the value without a D2H transfer.\n            torch.is_tensor(self._multiply_factor)\n            or self._multiply_factor != 1.0\n        ):\n            self.wrapped_optimizer.multiply_grads(self._multiply_factor)\n            self._multiply_factor = 1.0\n\n    def multiply_grads(self, c):\n        \"\"\"Multiplies grads by a constant *c*.\"\"\"\n        self._multiply_factor *= c\n\n    def clip_grad_norm(\n        self,\n        max_norm,\n        norm_type=\"l2\",\n        aggregate_norm_fn=None,\n        skip_gradient_update_on_clip_norm=False,\n    ):\n        \"\"\"Clips gradient norm and updates dynamic loss scaler.\"\"\"\n        max_norm = float(max_norm)\n        grad_norm = self._multiply_factor * self.wrapped_optimizer.clip_grad_norm(\n            0, norm_type, aggregate_norm_fn\n        )\n        if self.scaler is not None:\n            grad_norm_cpu = float(grad_norm)\n            # If skip gradient on clip norm threshold then first detect gnorm overflows to update loss scale\n            # then additionally check for clip norm threshold but without updating loss scale.\n            if skip_gradient_update_on_clip_norm:\n                # detect overflow and adjust loss scale\n                self.scaler.check_overflow(grad_norm_cpu)\n                if grad_norm_cpu > max_norm > 0.0:\n                    raise OverflowError(\n                        f\"Grad norm: {grad_norm:.2f} exceeds threshold: {max_norm:.2f}, rejecting batch.\"\n                    )\n            else:\n                if grad_norm_cpu > max_norm > 0.0:\n                    self._multiply_factor *= max_norm / grad_norm_cpu\n                # detect overflow and adjust loss scale\n                self.scaler.check_overflow(grad_norm_cpu)\n        elif max_norm > 0.0:\n            clip_coef = (max_norm / (grad_norm + 1e-6)).clamp_(max=1)\n            self._multiply_factor *= clip_coef\n\n        return grad_norm\n\n    def step(self, closure=None, groups=None):\n        \"\"\"Performs a single optimization step.\"\"\"\n        if getattr(self, \"supports_step_with_scale\", False):\n            # NOTE(msb) optimizer divides by scale factor\n            self.wrapped_optimizer.step(\n                closure, scale=(1.0 / self._multiply_factor), groups=groups\n            )\n        else:\n            self._unscale_grads()\n            self.wrapped_optimizer.step(closure, groups=groups)\n\n        if self.scaler is not None:\n            self.scaler.update()\n\n    def zero_grad(self):\n        \"\"\"Clears the gradients of all optimized parameters.\"\"\"\n        self.wrapped_optimizer.zero_grad()\n        if self.scaler is not None:\n            self._multiply_factor = 1.0 / float(self.scaler.loss_scale)\n        else:\n            self._multiply_factor = 1.0\n\n    @property\n    def supports_flat_params(self):\n        return self.wrapped_optimizer.supports_flat_params\n\n\nclass MemoryEfficientFP16Optimizer(\n    _MemoryEfficientFP16OptimizerMixin, optim.BaseOptimizer\n):\n    \"\"\"\n    Wrap an *optimizer* to support FP16 (mixed precision) training.\n\n    Compared to :class:`metaseq.optim.FP16Optimizer`, this version does not\n    maintain an FP32 copy of the model. We instead expect the optimizer to\n    convert the gradients to FP32 internally and sync the results back to the\n    FP16 model params. This significantly reduces memory usage but slightly\n    increases the time spent in the optimizer.\n\n    Since this wrapper depends on specific functionality in the wrapped\n    optimizer (i.e., on-the-fly conversion of grads to FP32), only certain\n    optimizers can be wrapped. This is determined by the\n    *supports_memory_efficient_fp16* property.\n    \"\"\"\n\n    def __init__(\n        self, cfg: DictConfig, params, optimizer, allow_unsupported=False, **kwargs\n    ):\n        if not allow_unsupported and not optimizer.supports_memory_efficient_fp16:\n            raise ValueError(\n                \"Unsupported optimizer: {}\".format(optimizer.__class__.__name__)\n            )\n\n        super().__init__(cfg.optimizer)\n        self.wrapped_optimizer = optimizer\n\n        # No loss scaler required for training with bf16\n        self.scaler = (\n            None\n            if cfg.common.bf16\n            else DynamicLossScaler(\n                init_scale=cfg.common.fp16_init_scale,\n                scale_window=cfg.common.fp16_scale_window,\n                tolerance=cfg.common.fp16_scale_tolerance,\n                threshold=cfg.common.threshold_loss_scale,\n                min_loss_scale=cfg.common.min_loss_scale,\n            )\n        )\n\n    @classmethod\n    def build_optimizer(cls, cfg: DictConfig, params, **kwargs):\n        \"\"\"\n        Args:\n            args (argparse.Namespace): metaseq args\n            params (iterable): iterable of parameters to optimize\n        \"\"\"\n        fp16_optimizer = optim.build_optimizer(cfg.optimizer, params)\n        return cls(cfg, params, fp16_optimizer, **kwargs)\n\n    @property\n    def optimizer(self):\n        return self.wrapped_optimizer.optimizer\n\n    @optimizer.setter\n    def optimizer(self, optimizer):\n        self.wrapped_optimizer.optimizer = optimizer\n\n    @property\n    def optimizer_config(self):\n        return self.wrapped_optimizer.optimizer_config\n\n    @property\n    def lr_scheduler(self):\n        return getattr(self.wrapped_optimizer, \"lr_scheduler\", None)\n\n    def get_lr(self):\n        return self.wrapped_optimizer.get_lr()\n\n    def set_lr(self, lr):\n        self.wrapped_optimizer.set_lr(lr)\n\n    def all_reduce_grads(self, module):\n        self.wrapped_optimizer.all_reduce_grads(module)\n",
        "metaseq/optim/fused_adam.py": "# Copyright (c) Meta Platforms, Inc. and affiliates. All Rights Reserved.\n#\n# This source code is licensed under the MIT license found in the\n# LICENSE file in the root directory of this source tree.\n\nimport types\n\nimport torch\n\n\ndef get_fused_adam_class():\n    \"\"\"\n    Look for the FusedAdam optimizer from apex. We first try to load the\n    \"contrib\" interface, which is a bit faster than the main interface,\n    but is technically deprecated.\n    \"\"\"\n    try:\n        # The \"deprecated\" interface in recent versions of apex is a bit\n        # faster than the main interface, since we don't use the apex\n        # optimizer. This can be installed by passing the\n        # `--deprecated_fused_adam` option when building apex.\n        global fused_adam_cuda\n        import importlib\n\n        fused_adam_cuda = importlib.import_module(\"fused_adam_cuda\")\n        return FusedAdamV1\n    except ImportError:\n        try:\n            # fallback to the newer interface\n            from apex.optimizers import FusedAdam as _FusedAdam  # noqa\n            from apex.multi_tensor_apply import multi_tensor_applier\n\n            if multi_tensor_applier.available:\n                return FusedAdamV2\n        except ImportError:\n            pass\n    return None\n\n\nclass FusedAdamV1(torch.optim.Optimizer):\n    \"\"\"\n    Implements Adam algorithm. Currently GPU-only. Requires Apex to be installed via\n    ``python setup.py install --cuda_ext --cpp_ext``.\n\n    It has been proposed in `Adam: A Method for Stochastic Optimization`_.\n\n    Compared to the original version in Apex, the metaseq version casts grads\n    and params to FP32 internally to support ``--memory-efficient-fp16``.\n\n    Args:\n        params (iterable): iterable of parameters to optimize or dicts defining\n            parameter groups.\n        lr (float, optional): learning rate. (default: 1e-3)\n        betas (Tuple[float, float], optional): coefficients used for computing\n            running averages of gradient and its square. (default: (0.9, 0.999))\n        eps (float, optional): term added to the denominator to improve\n            numerical stability. (default: 1e-8)\n        weight_decay (float, optional): weight decay (L2 penalty) (default: 0)\n        amsgrad (boolean, optional): whether to use the AMSGrad variant of this\n            algorithm from the paper `On the Convergence of Adam and Beyond`_\n            (default: False) NOT SUPPORTED in FusedAdam!\n        eps_inside_sqrt (boolean, optional): in the 'update parameters' step,\n            adds eps to the bias-corrected second moment estimate before\n            evaluating square root instead of adding it to the square root of\n            second moment estimate as in the original paper. (default: False)\n    .. _Adam: A Method for Stochastic Optimization:\n        https://arxiv.org/abs/1412.6980\n    .. _On the Convergence of Adam and Beyond:\n        https://openreview.net/forum?id=ryQu7f-RZ\n    \"\"\"\n\n    def __init__(\n        self,\n        params,\n        lr=1e-3,\n        bias_correction=True,\n        betas=(0.9, 0.999),\n        eps=1e-8,\n        eps_inside_sqrt=False,\n        weight_decay=0.0,\n        max_grad_norm=0.0,\n        amsgrad=False,\n        use_fp16_stats=False,\n    ):\n        global fused_adam_cuda\n        import importlib\n\n        fused_adam_cuda = importlib.import_module(\"fused_adam_cuda\")\n\n        if amsgrad:\n            raise RuntimeError(\"FusedAdam does not support the AMSGrad variant.\")\n        defaults = {\n            \"lr\": lr,\n            \"bias_correction\": bias_correction,\n            \"betas\": betas,\n            \"eps\": eps,\n            \"weight_decay\": weight_decay,\n            \"max_grad_norm\": max_grad_norm,\n        }\n        super().__init__(params, defaults)\n        self.eps_mode = 0 if eps_inside_sqrt else 1\n\n        self.use_fp16_stats = use_fp16_stats\n        self.FLOAT16_MAX = 65504.0\n\n    @property\n    def supports_memory_efficient_fp16(self):\n        return True\n\n    @property\n    def supports_flat_params(self):\n        return True\n\n    @property\n    def supports_step_with_scale(self):\n        return True\n\n    def step(self, closure=None, grads=None, scale=1.0, grad_norms=None):\n        \"\"\"Performs a single optimization step.\n        Args:\n            closure (callable, optional): A closure that reevaluates the model\n                and returns the loss.\n            grads (list of tensors, optional): weight gradient to use for the\n                optimizer update. If gradients have type torch.half, parameters\n                are expected to be in type torch.float. (default: None)\n            output params (list of tensors, optional): A reduced precision copy\n                of the updated weights written out in addition to the regular\n                updated weights. Have to be of same type as gradients. (default: None)\n            scale (float, optional): factor to divide gradient tensor values\n                by before applying to weights. (default: 1)\n        \"\"\"\n        loss = None\n        if closure is not None:\n            loss = closure()\n\n        if grads is None:\n            grads_group = [None] * len(self.param_groups)\n        # backward compatibility\n        # assuming a list/generator of parameter means single group\n        elif isinstance(grads, types.GeneratorType):\n            grads_group = [grads]\n        elif type(grads[0]) != list:\n            grads_group = [grads]\n        else:\n            grads_group = grads\n\n        if grad_norms is None:\n            grad_norms = [None] * len(self.param_groups)\n\n        for group, grads_this_group, grad_norm in zip(\n            self.param_groups, grads_group, grad_norms\n        ):\n            if grads_this_group is None:\n                grads_this_group = [None] * len(group[\"params\"])\n\n            # compute combined scale factor for this group\n            combined_scale = scale\n            if group.get(\"max_grad_norm\", 0) > 0:\n                # norm is in fact norm*scale\n                clip = ((grad_norm / scale) + 1e-6) / group[\"max_grad_norm\"]\n                if clip > 1:\n                    combined_scale = clip * scale\n\n            bias_correction = 1 if group.get(\"bias_correction\", 1) else 0\n\n            for p, grad in zip(group[\"params\"], grads_this_group):\n                # note: p.grad should not ever be set for correct\n                # operation of mixed precision optimizer that sometimes\n                # sends None gradients\n                if p.grad is None and grad is None:\n                    continue\n                if grad is None:\n                    grad = p.grad.data\n                if grad.is_sparse:\n                    raise RuntimeError(\n                        \"FusedAdam does not support sparse gradients, \"\n                        \"please consider SparseAdam instead\"\n                    )\n\n                if p.device.type == \"cpu\":\n                    p_data_fp32 = p.data.cuda(non_blocking=True).float()\n                    out_p = torch.tensor([], dtype=torch.float)\n                else:\n                    p_data_fp32 = p.data.float()\n                    out_p = p.data\n\n                state = self.state[p]\n\n                # State initialization\n                dtype = torch.float16 if self.use_fp16_stats else p_data_fp32.dtype\n                if len(state) == 0:\n                    state[\"step\"] = 0\n                    # Exponential moving average of gradient values\n                    state[\"exp_avg\"] = torch.zeros_like(p_data_fp32, dtype=dtype)\n                    # Exponential moving average of squared gradient values\n                    state[\"exp_avg_sq\"] = torch.zeros_like(p_data_fp32, dtype=dtype)\n                    if self.use_fp16_stats:\n                        state[\"exp_avg_scale\"] = 1.0\n                        state[\"exp_avg_sq_scale\"] = 1.0\n                else:\n                    device = p_data_fp32.device\n                    state[\"exp_avg\"] = state[\"exp_avg\"].to(device, dtype)\n                    state[\"exp_avg_sq\"] = state[\"exp_avg_sq\"].to(device, dtype)\n\n                exp_avg = state[\"exp_avg\"]\n                exp_avg_sq = state[\"exp_avg_sq\"]\n                if self.use_fp16_stats:\n                    assert exp_avg.dtype == torch.float16\n                    exp_avg = exp_avg.float() * state[\"exp_avg_scale\"]\n                    exp_avg_sq = exp_avg_sq.float() * state[\"exp_avg_sq_scale\"]\n                beta1, beta2 = group[\"betas\"]\n\n                state[\"step\"] += 1\n\n                with torch.cuda.device(p_data_fp32.device):\n                    fused_adam_cuda.adam(\n                        p_data_fp32,\n                        out_p,\n                        exp_avg,\n                        exp_avg_sq,\n                        grad,\n                        group[\"lr\"],\n                        beta1,\n                        beta2,\n                        group[\"eps\"],\n                        combined_scale,\n                        state[\"step\"],\n                        self.eps_mode,\n                        bias_correction,\n                        group[\"weight_decay\"],\n                    )\n\n                if p.device.type == \"cpu\":\n                    p.data.copy_(p_data_fp32, non_blocking=True)\n\n                if self.use_fp16_stats:\n\n                    def inf_norm(t):\n                        return torch.norm(t, float(\"inf\"))\n\n                    # from github.com/openai/jukebox/blob/master/jukebox/utils/fp16.py\n                    state[\"exp_avg_scale\"], state[\"exp_avg_sq_scale\"] = (\n                        1e-8 + inf_norm(exp_avg) / self.FLOAT16_MAX,\n                        1e-8 + inf_norm(exp_avg_sq) / self.FLOAT16_MAX,\n                    )\n                    state[\"exp_avg\"], state[\"exp_avg_sq\"] = (\n                        (exp_avg / state[\"exp_avg_scale\"]).half(),\n                        (exp_avg_sq / state[\"exp_avg_sq_scale\"]).half(),\n                    )\n\n        return loss\n\n\ntry:\n    from apex.optimizers import FusedAdam\n    from apex.multi_tensor_apply import multi_tensor_applier\n\n    class FusedAdamV2(FusedAdam):\n        \"\"\"\n        Compared to the original version in Apex, the metaseq version casts grads\n        and params to FP32 internally to support ``--memory-efficient-fp16``.\n        \"\"\"\n\n        def __init__(self, *args, use_fp16_stats=False, **kwargs):\n            if use_fp16_stats:\n                raise NotImplementedError(\n                    \"--fp16-adam-stats is only supported with FusedAdamV1\"\n                )\n            super().__init__(*args, **kwargs)\n            if not hasattr(self, \"multi_tensor_adam\"):\n                raise Exception(\n                    \"Apex installation is outdated. Please install an updated version of apex.\"\n                )\n\n        @property\n        def supports_memory_efficient_fp16(self):\n            return True\n\n        @property\n        def supports_flat_params(self):\n            return True\n\n        def step(\n            self,\n            closure=None,\n            grads=None,\n            output_params=None,\n            scale=None,\n            grad_norms=None,\n        ):\n            \"\"\"Performs a single optimization step.\"\"\"\n            loss = None\n            if closure is not None:\n                loss = closure()\n\n            for group in self.param_groups:\n                bias_correction = 1 if group[\"bias_correction\"] else 0\n                beta1, beta2 = group[\"betas\"]\n\n                # assume same step across group now to simplify things\n                # per parameter step can be easily support by making it tensor, or pass list into kernel\n                if \"step\" in group:\n                    group[\"step\"] += 1\n                else:\n                    group[\"step\"] = 1\n\n                # create lists for multi-tensor apply\n                g_16, p_16, orig_p_16, m_16, v_16 = [], [], [], [], []\n                g_32, p_32, m_32, v_32 = [], [], [], []\n\n                for p in group[\"params\"]:\n                    if p.grad is None:\n                        continue\n                    if p.grad.data.is_sparse:\n                        raise RuntimeError(\n                            \"FusedAdam does not support sparse gradients, \"\n                            \"please consider SparseAdam instead\"\n                        )\n\n                    state = self.state[p]\n                    # State initialization\n                    if len(state) == 0:\n                        # Exponential moving average of gradient values\n                        state[\"exp_avg\"] = torch.zeros_like(p.data, dtype=torch.float)\n                        # Exponential moving average of squared gradient values\n                        state[\"exp_avg_sq\"] = torch.zeros_like(\n                            p.data, dtype=torch.float\n                        )\n                    else:\n                        state[\"exp_avg\"] = state[\"exp_avg\"].to(\n                            device=p.data.device, dtype=torch.float\n                        )\n                        state[\"exp_avg_sq\"] = state[\"exp_avg_sq\"].to(\n                            device=p.data.device, dtype=torch.float\n                        )\n\n                    if p.dtype == torch.float16:\n                        g_16.append(p.grad.data.float())\n                        p_16.append(p.data.float())\n                        orig_p_16.append(p.data)\n                        m_16.append(state[\"exp_avg\"])\n                        v_16.append(state[\"exp_avg_sq\"])\n                    elif p.dtype == torch.float32:\n                        g_32.append(p.grad.data)\n                        p_32.append(p.data)\n                        m_32.append(state[\"exp_avg\"])\n                        v_32.append(state[\"exp_avg_sq\"])\n                    else:\n                        raise RuntimeError(\"FusedAdam only support fp16 and fp32.\")\n\n                with torch.cuda.device(p.device):\n                    if len(g_16) > 0:\n                        multi_tensor_applier(\n                            self.multi_tensor_adam,\n                            self._dummy_overflow_buf,\n                            [g_16, p_16, m_16, v_16],\n                            group[\"lr\"],\n                            beta1,\n                            beta2,\n                            group[\"eps\"],\n                            group[\"step\"],\n                            self.adam_w_mode,\n                            bias_correction,\n                            group[\"weight_decay\"],\n                        )\n                        for orig_p, p in zip(orig_p_16, p_16):\n                            orig_p.copy_(p.data)\n                    if len(g_32) > 0:\n                        multi_tensor_applier(\n                            self.multi_tensor_adam,\n                            self._dummy_overflow_buf,\n                            [g_32, p_32, m_32, v_32],\n                            group[\"lr\"],\n                            beta1,\n                            beta2,\n                            group[\"eps\"],\n                            group[\"step\"],\n                            self.adam_w_mode,\n                            bias_correction,\n                            group[\"weight_decay\"],\n                        )\n\n            return loss\n\nexcept ImportError:\n    pass\n",
        "metaseq/optim/lr_scheduler/__init__.py": "# Copyright (c) Meta Platforms, Inc. and affiliates. All Rights Reserved.\n#\n# This source code is licensed under the MIT license found in the\n# LICENSE file in the root directory of this source tree.\n\"\"\"isort:skip_file\"\"\"\n\nimport importlib\nimport os\n\nfrom metaseq import registry\nfrom metaseq.optim.lr_scheduler.base_lr_scheduler import BaseLRScheduler\nfrom omegaconf import DictConfig\n\n\n(\n    build_lr_scheduler_,\n    register_lr_scheduler,\n    LR_SCHEDULER_REGISTRY,\n    LR_SCHEDULER_DATACLASS_REGISTRY,\n) = registry.setup_registry(\n    \"--lr-scheduler\", base_class=BaseLRScheduler, default=\"inverse_sqrt\"\n)\n\n\ndef build_lr_scheduler(cfg: DictConfig, optimizer):\n    return build_lr_scheduler_(cfg, optimizer)\n\n\n# automatically import any Python files in the optim/lr_scheduler/ directory\nfor file in os.listdir(os.path.dirname(__file__)):\n    if file.endswith(\".py\") and not file.startswith(\"_\"):\n        file_name = file[: file.find(\".py\")]\n        importlib.import_module(\"metaseq.optim.lr_scheduler.\" + file_name)\n",
        "metaseq/optim/lr_scheduler/base_lr_scheduler.py": "# Copyright (c) Meta Platforms, Inc. and affiliates. All Rights Reserved.\n#\n# This source code is licensed under the MIT license found in the\n# LICENSE file in the root directory of this source tree.\n\nfrom metaseq.dataclass.utils import gen_parser_from_dataclass\nfrom metaseq.optim import BaseOptimizer\n\n\nclass BaseLRScheduler(object):\n    def __init__(self, cfg, optimizer):\n        super().__init__()\n        if optimizer is not None and not isinstance(optimizer, BaseOptimizer):\n            raise ValueError(\"optimizer must be an instance of BaseOptimizer\")\n        self.cfg = cfg\n        self.optimizer = optimizer\n        self.best = None\n\n    @classmethod\n    def add_args(cls, parser):\n        \"\"\"Add arguments to the parser for this LR scheduler.\"\"\"\n        dc = getattr(cls, \"__dataclass\", None)\n        if dc is not None:\n            gen_parser_from_dataclass(parser, dc())\n\n    def state_dict(self):\n        \"\"\"Return the LR scheduler state dict.\"\"\"\n        return {\"best\": self.best}\n\n    def load_state_dict(self, state_dict):\n        \"\"\"Load an LR scheduler state dict.\"\"\"\n        self.best = state_dict[\"best\"]\n\n    def step_begin_epoch(self, epoch):\n        \"\"\"Update the learning rate at the beginning of the given epoch.\"\"\"\n        pass\n\n    def step(self, epoch, val_loss=None):\n        \"\"\"Update the learning rate at the end of the given epoch.\"\"\"\n        if val_loss is not None:\n            if self.best is None:\n                self.best = val_loss\n            else:\n                self.best = min(self.best, val_loss)\n\n    def step_update(self, num_updates):\n        \"\"\"Update the learning rate after each update.\"\"\"\n        return self.optimizer.get_lr()\n",
        "metaseq/optim/lr_scheduler/cosine_lr_scheduler.py": "# Copyright (c) Meta Platforms, Inc. and affiliates. All Rights Reserved.\n#\n# This source code is licensed under the MIT license found in the\n# LICENSE file in the root directory of this source tree.\n\nimport math\nfrom collections.abc import Collection\nfrom dataclasses import dataclass, field\nfrom typing import List\n\nfrom omegaconf import II\n\nfrom metaseq.dataclass import MetaseqDataclass\nfrom metaseq.optim.lr_scheduler import BaseLRScheduler, register_lr_scheduler\n\n\n@dataclass\nclass CosineLRScheduleConfig(MetaseqDataclass):\n    warmup_updates: int = field(\n        default=0,\n        metadata={\"help\": \"warmup the learning rate linearly for the first N updates\"},\n    )\n    warmup_init_lr: float = field(\n        default=-1,\n        metadata={\n            \"help\": \"initial learning rate during warmup phase; default is cfg.lr\"\n        },\n    )\n    lr: List[float] = field(\n        default=II(\"optimization.lr\"),\n        metadata={\"help\": \"max learning rate, must be more than cfg.min_lr\"},\n    )\n    min_lr: float = field(default=0.0, metadata={\"help\": \"min learning rate\"})\n    t_mult: float = field(\n        default=1.0, metadata={\"help\": \"factor to grow the length of each period\"}\n    )\n    lr_period_updates: float = field(\n        default=-1, metadata={\"help\": \"initial number of updates per period\"}\n    )\n    lr_shrink: float = field(\n        default=0.1, metadata={\"help\": \"shrink factor for annealing\"}\n    )\n    # This is not required, but is for convenience in inferring lr_period_updates\n    max_update: int = II(\"optimization.max_update\")\n\n\n@register_lr_scheduler(\"cosine\", dataclass=CosineLRScheduleConfig)\nclass CosineLRSchedule(BaseLRScheduler):\n    \"\"\"Assign LR based on a cyclical schedule that follows the cosine function.\n\n    See https://arxiv.org/pdf/1608.03983.pdf for details.\n\n    We also support a warmup phase where we linearly increase the learning rate\n    from some initial learning rate (``--warmup-init-lr``) until the configured\n    max learning rate (``--lr``).\n\n    During warmup::\n\n      lrs = torch.linspace(cfg.warmup_init_lr, cfg.lr, cfg.warmup_updates)\n      lr = lrs[update_num]\n\n    After warmup::\n\n      lr = cfg.min_lr + 0.5*(cfg.lr - cfg.min_lr)*(1 + cos(t_curr / t_i))\n\n    where ``t_curr`` is current percentage of updates within the current period\n    range and ``t_i`` is the current period range, which is scaled by ``t_mul``\n    after every iteration.\n    \"\"\"\n\n    def __init__(self, cfg: CosineLRScheduleConfig, metaseq_optimizer):\n        super().__init__(cfg, metaseq_optimizer)\n        if isinstance(cfg.lr, Collection) and len(cfg.lr) > 1:\n            raise ValueError(\n                \"Cannot use a fixed learning rate schedule with cosine.\"\n                f\" Consider --lr-scheduler=fixed instead. ({cfg.lr})\"\n            )\n\n        self.max_lr = cfg.lr[0] if isinstance(cfg.lr, Collection) else cfg.lr\n        assert (\n            self.max_lr > cfg.min_lr\n        ), f\"max_lr (={cfg.lr}) must be more than min_lr (={cfg.min_lr})\"\n\n        warmup_end_lr = self.max_lr\n        if cfg.warmup_init_lr < 0:\n            cfg.warmup_init_lr = cfg.min_lr\n\n        self.t_mult = cfg.t_mult\n        self.period = cfg.lr_period_updates\n\n        if self.period <= 0:\n            assert (\n                cfg.max_update > 0\n            ), \"Either --max_update or --lr-period-updates must be set\"\n            self.period = cfg.max_update - cfg.warmup_updates\n\n        if cfg.warmup_updates > 0:\n            # linearly warmup for the first cfg.warmup_updates\n            self.lr_step = (warmup_end_lr - cfg.warmup_init_lr) / cfg.warmup_updates\n        else:\n            self.lr_step = 1\n\n        self.warmup_updates = cfg.warmup_updates\n        self.lr_shrink = cfg.lr_shrink\n\n        # initial learning rate\n        self.lr = cfg.warmup_init_lr\n        self.optimizer.set_lr(self.lr)\n\n    def step(self, epoch, val_loss=None):\n        \"\"\"Update the learning rate at the end of the given epoch.\"\"\"\n        super().step(epoch, val_loss)\n        # we don't change the learning rate at epoch boundaries\n        return self.optimizer.get_lr()\n\n    def step_update(self, num_updates):\n        \"\"\"Update the learning rate after each update.\"\"\"\n        if num_updates < self.cfg.warmup_updates:\n            self.lr = self.cfg.warmup_init_lr + num_updates * self.lr_step\n        else:\n            curr_updates = num_updates - self.cfg.warmup_updates\n            if self.t_mult != 1:\n                i = math.floor(\n                    math.log(\n                        1 - curr_updates / self.period * (1 - self.t_mult), self.t_mult\n                    )\n                )\n                t_i = self.t_mult**i * self.period\n                t_curr = (\n                    curr_updates\n                    - (1 - self.t_mult**i) / (1 - self.t_mult) * self.period\n                )\n            else:\n                i = math.floor(curr_updates / self.period)\n                t_i = self.period\n                t_curr = curr_updates - (self.period * i)\n\n            lr_shrink = self.lr_shrink**i\n            min_lr = self.cfg.min_lr * lr_shrink\n            max_lr = self.max_lr * lr_shrink\n\n            self.lr = min_lr + 0.5 * (max_lr - min_lr) * (\n                1 + math.cos(math.pi * t_curr / t_i)\n            )\n\n        self.optimizer.set_lr(self.lr)\n        return self.lr\n",
        "metaseq/optim/lr_scheduler/inverse_square_root_schedule.py": "# Copyright (c) Meta Platforms, Inc. and affiliates. All Rights Reserved.\n#\n# This source code is licensed under the MIT license found in the\n# LICENSE file in the root directory of this source tree.\n\nfrom collections.abc import Collection\nfrom dataclasses import dataclass, field\nfrom typing import List\n\nfrom omegaconf import II\n\nfrom metaseq.dataclass import MetaseqDataclass\nfrom metaseq.optim.lr_scheduler import BaseLRScheduler, register_lr_scheduler\n\n\n@dataclass\nclass InverseSquareRootLRScheduleConfig(MetaseqDataclass):\n    warmup_updates: int = field(\n        default=4000,\n        metadata={\"help\": \"warmup the learning rate linearly for the first N updates\"},\n    )\n    warmup_init_lr: float = field(\n        default=-1,\n        metadata={\n            \"help\": \"initial learning rate during warmup phase; default is cfg.lr\"\n        },\n    )\n    lr: List[float] = II(\"optimization.lr\")\n\n\n@register_lr_scheduler(\"inverse_sqrt\", dataclass=InverseSquareRootLRScheduleConfig)\nclass InverseSquareRootSchedule(BaseLRScheduler):\n    \"\"\"Decay the LR based on the inverse square root of the update number.\n\n    We also support a warmup phase where we linearly increase the learning rate\n    from some initial learning rate (``--warmup-init-lr``) until the configured\n    learning rate (``--lr``). Thereafter we decay proportional to the number of\n    updates, with a decay factor set to align with the configured learning rate.\n\n    During warmup::\n\n      lrs = torch.linspace(cfg.warmup_init_lr, cfg.lr, cfg.warmup_updates)\n      lr = lrs[update_num]\n\n    After warmup::\n\n      decay_factor = cfg.lr * sqrt(cfg.warmup_updates)\n      lr = decay_factor / sqrt(update_num)\n    \"\"\"\n\n    def __init__(self, cfg: InverseSquareRootLRScheduleConfig, optimizer):\n        super().__init__(cfg, optimizer)\n        if isinstance(cfg.lr, Collection) and len(cfg.lr) > 1:\n            raise ValueError(\n                \"Cannot use a fixed learning rate schedule with inverse_sqrt.\"\n                \" Consider --lr-scheduler=fixed instead.\"\n            )\n        warmup_end_lr = cfg.lr[0] if isinstance(cfg.lr, Collection) else cfg.lr\n        if cfg.warmup_init_lr < 0:\n            cfg.warmup_init_lr = 0 if cfg.warmup_updates > 0 else warmup_end_lr\n\n        # linearly warmup for the first cfg.warmup_updates\n        self.lr_step = (warmup_end_lr - cfg.warmup_init_lr) / cfg.warmup_updates\n\n        # then, decay prop. to the inverse square root of the update number\n        self.decay_factor = warmup_end_lr * cfg.warmup_updates**0.5\n\n        # initial learning rate\n        self.lr = cfg.warmup_init_lr\n        self.optimizer.set_lr(self.lr)\n\n    def step(self, epoch, val_loss=None):\n        \"\"\"Update the learning rate at the end of the given epoch.\"\"\"\n        super().step(epoch, val_loss)\n        # we don't change the learning rate at epoch boundaries\n        return self.optimizer.get_lr()\n\n    def step_update(self, num_updates):\n        \"\"\"Update the learning rate after each update.\"\"\"\n        if num_updates < self.cfg.warmup_updates:\n            self.lr = self.cfg.warmup_init_lr + num_updates * self.lr_step\n        else:\n            self.lr = self.decay_factor * num_updates**-0.5\n        self.optimizer.set_lr(self.lr)\n        return self.lr\n",
        "metaseq/optim/lr_scheduler/polynomial_decay_schedule.py": "# Copyright (c) Meta Platforms, Inc. and affiliates. All Rights Reserved.\n#\n# This source code is licensed under the MIT license found in the\n# LICENSE file in the root directory of this source tree.\n\nfrom dataclasses import dataclass, field\nfrom typing import Optional, List\n\nfrom omegaconf import II\n\nfrom metaseq.dataclass import MetaseqDataclass\nfrom metaseq.optim.lr_scheduler import BaseLRScheduler, register_lr_scheduler\n\n\n@dataclass\nclass PolynomialDecayLRScheduleConfig(MetaseqDataclass):\n    warmup_updates: int = field(\n        default=0,\n        metadata={\"help\": \"warmup the learning rate linearly for the first N updates\"},\n    )\n    force_anneal: Optional[int] = field(\n        default=None,\n        metadata={\"help\": \"force annealing at specified epoch\"},\n    )\n    end_learning_rate: float = field(\n        default=0.0,\n        metadata={\"help\": \"learning rate to decay to\"},\n    )\n    zero_lr_warmup_steps: int = field(\n        default=0,\n        metadata={\n            \"help\": \"number of steps to run with lr = 0 in the beginning, before warmup_updates, to update EMAs\"\n        },\n    )\n    power: float = field(\n        default=1.0,\n        metadata={\"help\": \"decay exponent\"},\n    )\n    total_num_update: float = field(\n        default=II(\"optimization.max_update\"),\n        metadata={\"help\": \"total number of updates over which to decay learning rate\"},\n    )\n    lr: List[float] = II(\"optimization.lr\")\n\n\n@register_lr_scheduler(\"polynomial_decay\", dataclass=PolynomialDecayLRScheduleConfig)\nclass PolynomialDecayLRSchedule(BaseLRScheduler):\n    \"\"\"Decay the LR on a fixed schedule.\"\"\"\n\n    def __init__(self, cfg: PolynomialDecayLRScheduleConfig, optimizer):\n        super().__init__(cfg, optimizer)\n\n        assert cfg.total_num_update > 0\n\n        self.lr = cfg.lr[0]\n        if cfg.warmup_updates > 0:\n            self.warmup_factor = 1.0 / cfg.warmup_updates\n        else:\n            self.warmup_factor = 1\n        self.end_learning_rate = cfg.end_learning_rate\n        self.zero_lr_warmup_steps = cfg.zero_lr_warmup_steps\n        self.total_num_update = cfg.total_num_update\n        self.power = cfg.power\n        self.optimizer.set_lr(self.warmup_factor * self.lr)\n\n    def get_next_lr(self, epoch):\n        lrs = self.cfg.lr\n        if self.cfg.force_anneal is None or epoch < self.cfg.force_anneal:\n            # use fixed LR schedule\n            next_lr = lrs[min(epoch, len(lrs) - 1)]\n        else:\n            # annneal based on lr_shrink\n            next_lr = self.optimizer.get_lr()\n        return next_lr\n\n    def step_begin_epoch(self, epoch):\n        \"\"\"Update the learning rate at the beginning of the given epoch.\"\"\"\n        self.lr = self.get_next_lr(epoch)\n        self.optimizer.set_lr(self.warmup_factor * self.lr)\n        return self.optimizer.get_lr()\n\n    def step_update(self, num_updates):\n        \"\"\"Update the learning rate after each update.\"\"\"\n        if self.zero_lr_warmup_steps > 0 and num_updates <= self.zero_lr_warmup_steps:\n            lr = 0\n        elif (\n            self.cfg.warmup_updates > 0\n            and num_updates <= self.cfg.warmup_updates + self.zero_lr_warmup_steps\n        ):\n            self.warmup_factor = (num_updates - self.zero_lr_warmup_steps) / float(\n                self.cfg.warmup_updates\n            )\n            lr = self.warmup_factor * self.lr\n        elif num_updates >= self.total_num_update:\n            lr = self.end_learning_rate\n        else:\n            warmup = self.cfg.warmup_updates + self.zero_lr_warmup_steps\n            lr_range = self.lr - self.end_learning_rate\n            pct_remaining = 1 - (num_updates - warmup) / (\n                self.total_num_update - warmup\n            )\n            lr = lr_range * pct_remaining**self.power + self.end_learning_rate\n        self.optimizer.set_lr(lr)\n        return self.optimizer.get_lr()\n",
        "metaseq/optim/sgd.py": "# Copyright (c) Meta Platforms, Inc. and affiliates. All Rights Reserved.\n#\n# This source code is licensed under the MIT license found in the\n# LICENSE file in the root directory of this source tree.\n\nimport torch\nfrom torch.optim.optimizer import Optimizer, required\n\nfrom . import LegacyOptimizer, register_optimizer\n\n\n@register_optimizer(\"sgd\")\nclass MetaseqSGDW(LegacyOptimizer):\n    \"\"\"\n    Note that this implements SGDW from this paper:\n\n        https://arxiv.org/abs/1711.05101\n    \"\"\"\n\n    def __init__(self, args, params):\n        super().__init__(args)\n        self._optimizer = SGDW(params, **self.optimizer_config)\n\n    @staticmethod\n    def add_args(parser):\n        \"\"\"Add optimizer-specific arguments to the parser.\"\"\"\n        # fmt: off\n        parser.add_argument('--momentum', default=0.0, type=float, metavar='M',\n                            help='momentum factor')\n        parser.add_argument('--weight-decay', '--wd', default=0.0, type=float, metavar='WD',\n                            help='weight decay')\n        # fmt: on\n\n    @property\n    def optimizer_config(self):\n        \"\"\"\n        Return a kwarg dictionary that will be used to override optimizer\n        args stored in checkpoints. This allows us to load a checkpoint and\n        resume training using a different set of optimizer args, e.g., with a\n        different learning rate.\n        \"\"\"\n        return {\n            \"lr\": self.args.lr[0],\n            \"momentum\": self.args.momentum,\n            \"weight_decay\": self.args.weight_decay,\n        }\n\n\nclass SGDW(Optimizer):\n    def __init__(\n        self,\n        params,\n        lr=required,\n        momentum=0,\n        dampening=0,\n        weight_decay=0,\n        nesterov=False,\n    ):\n        if lr is not required and lr < 0.0:\n            raise ValueError(\"Invalid learning rate: {}\".format(lr))\n        if momentum < 0.0:\n            raise ValueError(\"Invalid momentum value: {}\".format(momentum))\n        if weight_decay < 0.0:\n            raise ValueError(\"Invalid weight_decay value: {}\".format(weight_decay))\n\n        defaults = dict(\n            lr=lr,\n            momentum=momentum,\n            dampening=dampening,\n            weight_decay=weight_decay,\n            nesterov=nesterov,\n        )\n        if nesterov and (momentum <= 0 or dampening != 0):\n            raise ValueError(\"Nesterov momentum requires a momentum and zero dampening\")\n        super(SGDW, self).__init__(params, defaults)\n\n    def __setstate__(self, state):\n        super(SGDW, self).__setstate__(state)\n        for group in self.param_groups:\n            group.setdefault(\"nesterov\", False)\n\n    @torch.no_grad()\n    def step(self, closure=None):\n        \"\"\"Performs a single optimization step.\n        Args:\n            closure (callable, optional): A closure that reevaluates the model\n                and returns the loss.\n        \"\"\"\n        loss = None\n        if closure is not None:\n            with torch.enable_grad():\n                loss = closure()\n\n        for group in self.param_groups:\n            weight_decay = group[\"weight_decay\"]\n            momentum = group[\"momentum\"]\n            dampening = group[\"dampening\"]\n            nesterov = group[\"nesterov\"]\n\n            for p in group[\"params\"]:\n                if p.grad is None:\n                    continue\n\n                d_p = p.grad\n                if d_p.dtype in {torch.float16, torch.bfloat16}:\n                    d_p = d_p.float()\n\n                p_fp32 = p.data\n                if p.data.dtype in {torch.float16, torch.bfloat16}:\n                    p_fp32 = p_fp32.float()\n\n                if momentum != 0:\n                    param_state = self.state[p]\n                    if \"momentum_buffer\" not in param_state:\n                        buf = param_state[\"momentum_buffer\"] = torch.clone(d_p).detach()\n                    else:\n                        buf = param_state[\"momentum_buffer\"]\n                        buf.mul_(momentum).add_(d_p, alpha=1 - dampening)\n                    if nesterov:\n                        d_p = d_p.add(buf, alpha=momentum)\n                    else:\n                        d_p = buf\n\n                if weight_decay != 0:\n                    p_fp32.add_(p_fp32, alpha=-weight_decay * group[\"lr\"])\n\n                p_fp32.add_(d_p, alpha=-group[\"lr\"])\n\n                if p.data.dtype in {torch.float16, torch.bfloat16}:\n                    p.data.copy_(p_fp32)\n\n        return loss\n\n    @property\n    def supports_memory_efficient_fp16(self):\n        return True\n\n    @property\n    def supports_flat_params(self):\n        return True\n",
        "metaseq/optim/shard.py": "# Copyright (c) Meta Platforms, Inc. and affiliates. All Rights Reserved.\n#\n# This source code is licensed under the MIT license found in the\n# LICENSE file in the root directory of this source tree.\n\nfrom typing import Any, Dict\n\nfrom metaseq.distributed import utils\n\ntry:\n    from fairscale.optim import OSS\n\n    _has_fairscale = True\nexcept ImportError:\n    _has_fairscale = False\n\n\ndef shard_(optimizer, group):\n    if not _has_fairscale:\n        raise ImportError(\n            \"\\n\\nPlease install the fairscale package:\" \"\\n\\n  pip install fairscale\"\n        )\n\n    class MetaseqOSS(OSS):\n        @property\n        def disable_mem_eff_fp16_loading_hack(self):\n            return True\n\n        def __getattr__(self, name):\n            if name.startswith(\"supports\") and hasattr(self.optim, name):\n                return getattr(self.optim, name)\n            raise AttributeError(\n                \"'MetaseqOSS' object has no attribute {0!r}\".format(name)\n            )\n\n        def broadcast_global_state_dict(\n            self, state_dict: Dict[str, Any]\n        ) -> Dict[str, Any]:\n            \"\"\"\n            Broadcasts the entire state_dict to all other ranks\n            each rank is responsible to load their own partition of data\n            \"\"\"\n            return utils.broadcast_object(\n                state_dict,\n                src_rank=0,\n                group=self.group,\n            )\n\n    torch_optimizer = optimizer.optimizer\n    optim_cls = type(torch_optimizer)\n\n    optimizer.optimizer = MetaseqOSS(\n        torch_optimizer.param_groups,\n        optim_cls,\n        group=group,\n        **optimizer.optimizer_config,\n    )\n",
        "metaseq/options.py": "# Copyright (c) Meta Platforms, Inc. and affiliates. All Rights Reserved.\n#\n# This source code is licensed under the MIT license found in the\n# LICENSE file in the root directory of this source tree.\n\nimport argparse\nfrom typing import Callable, List, Optional\n\nimport torch\n\nfrom metaseq import utils\nfrom metaseq.dataclass.configs import (\n    CheckpointConfig,\n    CommonConfig,\n    CommonEvalConfig,\n    DatasetConfig,\n    DistributedTrainingConfig,\n    EvalLMConfig,\n    GenerationConfig,\n    OptimizationConfig,\n    ReshardConfig,\n    EMAConfig,\n)\nfrom metaseq.dataclass.utils import gen_parser_from_dataclass\n\n\ndef get_training_parser(default_task=\"translation\"):\n    parser = get_parser(\"Trainer\", default_task)\n    add_dataset_args(parser, train=True)\n    add_distributed_training_args(parser)\n    add_model_args(parser)\n    add_optimization_args(parser)\n    add_checkpoint_args(parser)\n    add_ema_args(parser)\n    return parser\n\n\ndef get_generation_parser(default_task=\"translation\"):\n    parser = get_parser(\"Generation\", default_task)\n    add_dataset_args(parser, gen=True)\n    add_distributed_training_args(parser, default_world_size=1)\n    add_generation_args(parser)\n    add_checkpoint_args(parser)\n    return parser\n\n\ndef get_eval_lm_parser(default_task=\"language_modeling\"):\n    parser = get_parser(\"Evaluate Language Model\", default_task)\n    add_dataset_args(parser, gen=True)\n    add_distributed_training_args(parser, default_world_size=1)\n    add_eval_lm_args(parser)\n    return parser\n\n\ndef get_reshard_parser(task=\"language_modeling\"):\n    parser = get_eval_lm_parser(default_task=task)\n    add_reshard_args(parser)\n    return parser\n\n\ndef add_reshard_args(parser):\n    group = parser.add_argument_group(\"reshard\")\n    gen_parser_from_dataclass(group, ReshardConfig())\n    return group\n\n\ndef get_validation_parser(default_task=None):\n    parser = get_parser(\"Validation\", default_task)\n    add_dataset_args(parser, train=True)\n    add_distributed_training_args(parser, default_world_size=1)\n    group = parser.add_argument_group(\"Evaluation\")\n    gen_parser_from_dataclass(group, CommonEvalConfig())\n    return parser\n\n\ndef parse_args_and_arch(\n    parser: argparse.ArgumentParser,\n    input_args: List[str] = None,\n    parse_known: bool = False,\n    suppress_defaults: bool = False,\n    modify_parser: Optional[Callable[[argparse.ArgumentParser], None]] = None,\n):\n    \"\"\"\n    Args:\n        parser (ArgumentParser): the parser\n        input_args (List[str]): strings to parse, defaults to sys.argv\n        parse_known (bool): only parse known arguments, similar to\n            `ArgumentParser.parse_known_args`\n        suppress_defaults (bool): parse while ignoring all default values\n        modify_parser (Optional[Callable[[ArgumentParser], None]]):\n            function to modify the parser, e.g., to set default values\n    \"\"\"\n    if suppress_defaults:\n        # Parse args without any default values. This requires us to parse\n        # twice, once to identify all the necessary task/model args, and a second\n        # time with all defaults set to None.\n        args = parse_args_and_arch(\n            parser,\n            input_args=input_args,\n            parse_known=parse_known,\n            suppress_defaults=False,\n        )\n        suppressed_parser = argparse.ArgumentParser(add_help=False, parents=[parser])\n        suppressed_parser.set_defaults(**{k: None for k, v in vars(args).items()})\n        args = suppressed_parser.parse_args(input_args)\n        return argparse.Namespace(\n            **{k: v for k, v in vars(args).items() if v is not None}\n        )\n\n    from metaseq.models import ARCH_MODEL_REGISTRY, ARCH_CONFIG_REGISTRY, MODEL_REGISTRY\n\n    # Before creating the true parser, we need to import optional user module\n    # in order to eagerly import custom tasks, optimizers, architectures, etc.\n    usr_parser = argparse.ArgumentParser(add_help=False, allow_abbrev=False)\n    usr_parser.add_argument(\"--user-dir\", default=None)\n    usr_args, _ = usr_parser.parse_known_args(input_args)\n    utils.import_user_module(usr_args)\n\n    if modify_parser is not None:\n        modify_parser(parser)\n\n    # The parser doesn't know about model/criterion/optimizer-specific args, so\n    # we parse twice. First we parse the model/criterion/optimizer, then we\n    # parse a second time after adding the *-specific arguments.\n    # If input_args is given, we will parse those args instead of sys.argv.\n    args, _ = parser.parse_known_args(input_args)\n\n    # Add model-specific args to parser.\n    if hasattr(args, \"arch\"):\n        model_specific_group = parser.add_argument_group(\n            \"Model-specific configuration\",\n            # Only include attributes which are explicitly given as command-line\n            # arguments or which have default values.\n            argument_default=argparse.SUPPRESS,\n        )\n        if args.arch in ARCH_MODEL_REGISTRY:\n            ARCH_MODEL_REGISTRY[args.arch].add_args(model_specific_group)\n        elif args.arch in MODEL_REGISTRY:\n            MODEL_REGISTRY[args.arch].add_args(model_specific_group)\n        else:\n            raise RuntimeError()\n\n    if hasattr(args, \"task\"):\n        from metaseq.tasks import TASK_REGISTRY\n\n        TASK_REGISTRY[args.task].add_args(parser)\n\n    # Add *-specific args to parser.\n    from metaseq.registry import REGISTRIES\n\n    for registry_name, REGISTRY in REGISTRIES.items():\n        choice = getattr(args, registry_name, None)\n        if choice is not None:\n            cls = REGISTRY[\"registry\"][choice]\n            if hasattr(cls, \"add_args\"):\n                cls.add_args(parser)\n            elif hasattr(cls, \"__dataclass\"):\n                gen_parser_from_dataclass(parser, cls.__dataclass())\n\n    # Modify the parser a second time, since defaults may have been reset\n    if modify_parser is not None:\n        modify_parser(parser)\n\n    # Parse a second time.\n    if parse_known:\n        args, extra = parser.parse_known_args(input_args)\n    else:\n        args = parser.parse_args(input_args)\n        extra = None\n    # Post-process args.\n    if (\n        hasattr(args, \"batch_size_valid\") and args.batch_size_valid is None\n    ) or not hasattr(args, \"batch_size_valid\"):\n        args.batch_size_valid = args.batch_size\n    if hasattr(args, \"max_tokens_valid\") and args.max_tokens_valid is None:\n        args.max_tokens_valid = args.max_tokens\n    if getattr(args, \"memory_efficient_fp16\", False):\n        args.fp16 = True\n\n    if getattr(args, \"seed\", None) is None:\n        args.seed = 1  # default seed for training\n        args.no_seed_provided = True\n    else:\n        args.no_seed_provided = False\n\n    # Apply architecture configuration.\n    if hasattr(args, \"arch\") and args.arch in ARCH_CONFIG_REGISTRY:\n        ARCH_CONFIG_REGISTRY[args.arch](args)\n\n    if parse_known:\n        return args, extra\n    else:\n        return args\n\n\ndef get_parser(desc, default_task=\"translation\"):\n    # Before creating the true parser, we need to import optional user module\n    # in order to eagerly import custom tasks, optimizers, architectures, etc.\n    usr_parser = argparse.ArgumentParser(add_help=False, allow_abbrev=False)\n    usr_parser.add_argument(\"--user-dir\", default=None)\n    usr_args, _ = usr_parser.parse_known_args()\n    utils.import_user_module(usr_args)\n\n    parser = argparse.ArgumentParser(allow_abbrev=False)\n    gen_parser_from_dataclass(parser, CommonConfig())\n\n    from metaseq.registry import REGISTRIES\n\n    for registry_name, REGISTRY in REGISTRIES.items():\n        parser.add_argument(\n            \"--\" + registry_name.replace(\"_\", \"-\"),\n            default=REGISTRY[\"default\"],\n            choices=REGISTRY[\"registry\"].keys(),\n        )\n\n    # Task definitions can be found under metaseq/tasks/\n    from metaseq.tasks import TASK_REGISTRY\n\n    parser.add_argument(\n        \"--task\",\n        metavar=\"TASK\",\n        default=default_task,\n        choices=TASK_REGISTRY.keys(),\n        help=\"task\",\n    )\n    # fmt: on\n    return parser\n\n\ndef add_dataset_args(parser, train=False, gen=False):\n    group = parser.add_argument_group(\"dataset_data_loading\")\n    gen_parser_from_dataclass(group, DatasetConfig())\n    # fmt: on\n    return group\n\n\ndef add_distributed_training_args(parser, default_world_size=None):\n    group = parser.add_argument_group(\"distributed_training\")\n    if default_world_size is None:\n        default_world_size = max(1, torch.cuda.device_count())\n    gen_parser_from_dataclass(\n        group, DistributedTrainingConfig(distributed_world_size=default_world_size)\n    )\n    return group\n\n\ndef add_optimization_args(parser):\n    group = parser.add_argument_group(\"optimization\")\n    # fmt: off\n    gen_parser_from_dataclass(group, OptimizationConfig())\n    # fmt: on\n    return group\n\n\ndef add_checkpoint_args(parser):\n    group = parser.add_argument_group(\"checkpoint\")\n    # fmt: off\n    gen_parser_from_dataclass(group, CheckpointConfig())\n    # fmt: on\n    return group\n\n\ndef add_common_eval_args(group):\n    gen_parser_from_dataclass(group, CommonEvalConfig())\n\n\ndef add_eval_lm_args(parser):\n    group = parser.add_argument_group(\"LM Evaluation\")\n    add_common_eval_args(group)\n    gen_parser_from_dataclass(group, EvalLMConfig())\n\n\ndef add_generation_args(parser):\n    group = parser.add_argument_group(\"Generation\")\n    add_common_eval_args(group)\n    gen_parser_from_dataclass(group, GenerationConfig())\n    return group\n\n\ndef add_ema_args(parser):\n    group = parser.add_argument_group(\"EMA configuration\")\n    gen_parser_from_dataclass(group, EMAConfig())\n    return group\n\n\ndef add_model_args(parser):\n    group = parser.add_argument_group(\"Model configuration\")\n    # fmt: off\n\n    # Model definitions can be found under metaseq/models/\n    #\n    # The model architecture can be specified in several ways.\n    # In increasing order of priority:\n    # 1) model defaults (lowest priority)\n    # 2) --arch argument\n    # 3) --encoder/decoder-* arguments (highest priority)\n    from metaseq.models import ARCH_MODEL_REGISTRY\n    group.add_argument('--arch', '-a', metavar='ARCH',\n                       choices=ARCH_MODEL_REGISTRY.keys(),\n                       help='model architecture')\n    # fmt: on\n    return group\n",
        "metaseq/pdb.py": "# Copyright (c) Meta Platforms, Inc. and affiliates. All Rights Reserved.\n#\n# This source code is licensed under the MIT license found in the\n# LICENSE file in the root directory of this source tree.\n\nimport multiprocessing\nimport os\nimport pdb\nimport sys\n\n\n__all__ = [\"set_trace\"]\n\n\n_stdin = [None]\n_stdin_lock = multiprocessing.Lock()\ntry:\n    _stdin_fd = sys.stdin.fileno()\nexcept Exception:\n    _stdin_fd = None\n\n\nclass MultiprocessingPdb(pdb.Pdb):\n    \"\"\"A Pdb wrapper that works in a multiprocessing environment.\n\n    Usage: `from metaseq import pdb; pdb.set_trace()`\n    \"\"\"\n\n    def __init__(self):\n        pdb.Pdb.__init__(self, nosigint=True)\n\n    def _cmdloop(self):\n        stdin_bak = sys.stdin\n        with _stdin_lock:\n            try:\n                if _stdin_fd is not None:\n                    if not _stdin[0]:\n                        _stdin[0] = os.fdopen(_stdin_fd)\n                    sys.stdin = _stdin[0]\n                self.cmdloop()\n            finally:\n                sys.stdin = stdin_bak\n\n\ndef set_trace():\n    pdb = MultiprocessingPdb()\n    pdb.set_trace(sys._getframe().f_back)\n\n\ndef set_trace_rank0():\n    import metaseq.distributed.utils as distributed_utils\n\n    if distributed_utils.get_global_rank() == 0:\n        set_trace()\n    else:\n        while True:\n            pass\n",
        "metaseq/registry.py": "# Copyright (c) Meta Platforms, Inc. and affiliates. All Rights Reserved.\n#\n# This source code is licensed under the MIT license found in the\n# LICENSE file in the root directory of this source tree.\n\nfrom argparse import Namespace\nfrom typing import Union\n\nfrom hydra.core.config_store import ConfigStore\nfrom omegaconf import DictConfig\n\nfrom metaseq.dataclass import MetaseqDataclass\nfrom metaseq.dataclass.utils import populate_dataclass, merge_with_parent\n\nREGISTRIES = {}\n\n\ndef setup_registry(registry_name: str, base_class=None, default=None, required=False):\n    assert registry_name.startswith(\"--\")\n    registry_name = registry_name[2:].replace(\"-\", \"_\")\n\n    REGISTRY = {}\n    REGISTRY_CLASS_NAMES = set()\n    DATACLASS_REGISTRY = {}\n\n    # maintain a registry of all registries\n    if registry_name in REGISTRIES:\n        return  # registry already exists\n    REGISTRIES[registry_name] = {\n        \"registry\": REGISTRY,\n        \"default\": default,\n        \"dataclass_registry\": DATACLASS_REGISTRY,\n    }\n\n    def build_x(cfg: Union[DictConfig, str, Namespace], *extra_args, **extra_kwargs):\n        if isinstance(cfg, DictConfig):\n            choice = cfg._name\n\n            if choice and choice in DATACLASS_REGISTRY:\n                dc = DATACLASS_REGISTRY[choice]\n                cfg = merge_with_parent(dc(), cfg)\n        elif isinstance(cfg, str):\n            choice = cfg\n            if choice in DATACLASS_REGISTRY:\n                cfg = DATACLASS_REGISTRY[choice]()\n        else:\n            choice = getattr(cfg, registry_name, None)\n            if choice in DATACLASS_REGISTRY:\n                cfg = populate_dataclass(DATACLASS_REGISTRY[choice](), cfg)\n\n        if choice is None:\n            if required:\n                raise ValueError(\"{} is required!\".format(registry_name))\n            return None\n\n        cls = REGISTRY[choice]\n        if hasattr(cls, \"build_\" + registry_name):\n            builder = getattr(cls, \"build_\" + registry_name)\n        else:\n            builder = cls\n\n        return builder(cfg, *extra_args, **extra_kwargs)\n\n    def register_x(name, dataclass=None):\n        def register_x_cls(cls):\n            if name in REGISTRY:\n                raise ValueError(\n                    \"Cannot register duplicate {} ({})\".format(registry_name, name)\n                )\n            if cls.__name__ in REGISTRY_CLASS_NAMES:\n                raise ValueError(\n                    \"Cannot register {} with duplicate class name ({})\".format(\n                        registry_name, cls.__name__\n                    )\n                )\n            if base_class is not None and not issubclass(cls, base_class):\n                raise ValueError(\n                    \"{} must extend {}\".format(cls.__name__, base_class.__name__)\n                )\n\n            if dataclass is not None and not issubclass(dataclass, MetaseqDataclass):\n                raise ValueError(\n                    \"Dataclass {} must extend MetaseqDataclass\".format(dataclass)\n                )\n\n            cls.__dataclass = dataclass\n            if cls.__dataclass is not None:\n                DATACLASS_REGISTRY[name] = cls.__dataclass\n\n                cs = ConfigStore.instance()\n                node = dataclass()\n                node._name = name\n                cs.store(name=name, group=registry_name, node=node, provider=\"metaseq\")\n\n            REGISTRY[name] = cls\n\n            return cls\n\n        return register_x_cls\n\n    return build_x, register_x, REGISTRY, DATACLASS_REGISTRY\n",
        "metaseq/scripts/__init__.py": "# Copyright (c) Meta Platforms, Inc. and affiliates. All Rights Reserved.\n#\n# This source code is licensed under the MIT license found in the\n# LICENSE file in the root directory of this source tree.\n",
        "metaseq/scripts/consolidate_fsdp_shards.py": "#!/usr/bin/env python\n\n# Copyright (c) Meta Platforms, Inc. and affiliates.\n# This source code is licensed under the MIT license found in the\n# LICENSE file in the root directory of this source tree.\n\nfrom metaseq.distributed.stitch_fsdp_ckpt import consolidate_fsdp_shards\nimport fire\n\n\nif __name__ == \"__main__\":\n    # This is expected to be used before evaluation, not during training.\n    fire.Fire(consolidate_fsdp_shards)\n",
        "metaseq/scripts/convert_metaseq_ft.py": "# Copyright (c) Meta Platforms, Inc. and affiliates. All Rights Reserved.\n#\n# This source code is licensed under the MIT license found in the\n# LICENSE file in the root directory of this source tree.\nimport logging\nimport os\nimport re\nfrom glob import glob\nfrom typing import Any, Dict, List, Tuple\nimport fire\nimport torch\n\n\nlogging.basicConfig(format=\"%(asctime)s | %(name)s | %(message)s\", level=logging.INFO)\nlogger: logging.Logger = logging.getLogger(\"metaseq.scripts.convert_metaseq_ft\")\n\n\ndef convert_metaseq_ft(\n    input: str,\n    output: str,\n    dtype: str = \"fp16\",\n    quantize: bool = False,\n) -> None:\n    \"\"\"\n    Convert Metaseq model weights into FasterTransformer format. The model parallel\n    parts in the input are expected to contain unflattened, FSDP-consolidated\n    model weights. The number of model parallel parts remains unchanged.\n\n    Args:\n        :param input: A glob pattern specifying the path names of the input shards.\n            (e.g. \"checkpoints/opt-175b/reshard_no_os_unflat/reshard-model_part-*.pt\").\n        :param output: A string pattern specifying the path names of the output shards.\n            Shard indices can be included in the path names if the pattern includes `{i}`.\n            (e.g. \"checkpoints/opt-175b-ft-mp8/part-{i}.pt\").\n    \"\"\"\n    torch_dtype = {\"fp16\": torch.float16, \"fp32\": torch.float32, \"bf16\": torch.bfloat16}\n    assert dtype in torch_dtype\n    dtype = torch_dtype[dtype]\n\n    files = glob(input)\n    if len(files) == 0:\n        raise ValueError(\"The glob pattern doesn't match any model parallel parts.\")\n    files = sorted(files, key=lambda x: list(map(int, re.findall(r\"\\d+\", x))))\n    logger.info(f\"Found {len(files)} model parallel parts ({files[0]} to {files[-1]})\")\n\n    logger.info(\"Merging embedding tokens across model parallel parts\")\n    embedding_tokens = torch.cat(\n        [torch.load(f)[\"model\"][\"decoder.embed_tokens.weight\"] for f in files]\n    ).to(dtype=dtype)\n\n    for i, file in enumerate(files):\n        logger.info(f\"Converting {file} into FasterTransformer format\")\n        state_dict = torch.load(file, torch.device(\"cpu\"))[\"model\"]\n        weights = convert_weights(state_dict, embedding_tokens, dtype, quantize)\n\n        output_file = output.format(i=i)\n        os.makedirs(os.path.dirname(output_file), exist_ok=True)\n        logger.info(f\"Writing converted weights to {output_file}\")\n        torch.save(weights, output_file)\n    logger.info(\"Done!\")\n\n\ndef convert_weights(\n    state_dict: Dict[str, Any],\n    embedding_tokens: torch.Tensor,\n    dtype: torch.dtype,\n    quantize: bool = False,\n) -> List[torch.Tensor]:\n    regex = re.compile(r\"decoder.layers.(\\d+).fc1.weight\")\n    N = max(int(regex.findall(x)[0]) for x in filter(regex.match, state_dict)) + 1\n\n    # fmt: off\n    weights = []\n    weights.extend([state_dict[f\"decoder.layers.{i}.self_attn_layer_norm.weight\"].to(dtype) for i in range(N)])\n    weights.extend([state_dict[f\"decoder.layers.{i}.self_attn_layer_norm.bias\"].to(dtype) for i in range(N)])\n    weights.extend([_kvq_to_qkv(state_dict[f\"decoder.layers.{i}.self_attn.qkv_proj.weight\"].to(dtype))for i in range(N)])\n    weights.extend([_kvq_to_qkv(state_dict[f\"decoder.layers.{i}.self_attn.qkv_proj.bias\"].to(dtype))for i in range(N)])\n    weights.extend([state_dict[f\"decoder.layers.{i}.self_attn.out_proj.weight\"].T.contiguous().to(dtype) for i in range(N)])\n    weights.extend([state_dict[f\"decoder.layers.{i}.self_attn.out_proj.bias\"].to(dtype) for i in range(N)])\n    weights.extend([state_dict[f\"decoder.layers.{i}.final_layer_norm.weight\"].T.contiguous().to(dtype) for i in range(N)])\n    weights.extend([state_dict[f\"decoder.layers.{i}.final_layer_norm.bias\"].to(dtype) for i in range(N)])\n    weights.extend([state_dict[f\"decoder.layers.{i}.fc1.weight\"].T.contiguous().to(dtype) for i in range(N)])\n    weights.extend([state_dict[f\"decoder.layers.{i}.fc1.bias\"].to(dtype) for i in range(N)])\n    weights.extend([state_dict[f\"decoder.layers.{i}.fc2.weight\"].T.contiguous().to(dtype) for i in range(N)])\n    weights.extend([state_dict[f\"decoder.layers.{i}.fc2.bias\"].to(dtype) for i in range(N)])\n    weights.append(state_dict[\"decoder.layer_norm.weight\"].to(dtype))\n    weights.append(state_dict[\"decoder.layer_norm.bias\"].to(dtype))\n    weights.append(state_dict[\"decoder.embed_positions.weight\"][2:])\n    weights.append(embedding_tokens)  # \"model.wte\"\n    weights.append(embedding_tokens)  # \"model.lm_head.weight\"\n    # fmt: on\n\n    out = {\"weights\": weights}\n    if quantize:\n        out[\"int8_weights\"], out[\"int8_scales\"] = int8_weight_only_quantize(weights, N)\n    return out\n\n\ndef int8_weight_only_quantize(\n    weights: List[torch.Tensor], num_layers: int\n) -> Tuple[List[torch.Tensor], List[torch.Tensor]]:\n    try:\n        torch.classes.load_library(os.environ.get(\"FT_PATH\"))\n    except Exception:\n        raise ImportError(\n            \"Please install FasterTransformer and provide a path to the binary\"\n            \"`libth_transformer.so` via the environment variable `FT_PATH`.\"\n        )\n    quant = torch.ops.fastertransformer.symmetric_quantize_last_axis_of_batched_matrix\n\n    N = num_layers\n    int8_weights, int8_scales = [None] * (N * 4), [None] * (N * 4)\n    for i in range(N):\n        int8_weights[i + 0 * N], int8_scales[i + 0 * N] = quant(\n            weights[2 * N + i].flatten(1, 2), torch.int8\n        )\n        int8_weights[i + 1 * N], int8_scales[i + 1 * N] = quant(\n            weights[4 * N + i], torch.int8\n        )\n        int8_weights[i + 2 * N], int8_scales[i + 2 * N] = quant(\n            weights[8 * N + i], torch.int8\n        )\n        int8_weights[i + 3 * N], int8_scales[i + 3 * N] = quant(\n            weights[10 * N + i], torch.int8\n        )\n\n        # Release memory taken by half / full precision weights\n        weights[2 * N + i] = weights[2 * N + i].new_empty(0)\n        weights[4 * N + i] = weights[4 * N + i].new_empty(0)\n        weights[8 * N + i] = weights[8 * N + i].new_empty(0)\n        weights[10 * N + i] = weights[10 * N + i].new_empty(0)\n        torch.cuda.empty_cache()\n    return int8_weights, int8_scales\n\n\ndef _kvq_to_qkv(t: torch.Tensor) -> torch.Tensor:\n    t = t.view(3, t.size(0) // 3, *t.size()[1:])\n    t = torch.cat([t[2:], t[:2]], dim=0)\n    return t if t.ndim == 2 else t.permute(2, 0, 1).contiguous()\n\n\nif __name__ == \"__main__\":\n    \"\"\"\n    Example usage:\n        python convert_metaseq_ft.py \\\n        --input \"/data/checkpoints/opt-175b/reshard_no_os_unflat/reshard-model_part-*.pt\" \\\n        --output \"/data/checkpoints/opt-175b-ft-mp8/part-{i}.pt\" \\\n        --dtype fp16\n    \"\"\"\n    fire.Fire(convert_metaseq_ft)\n",
        "metaseq/scripts/convert_to_singleton.py": "#!/usr/bin/env python\n\n# Copyright (c) Meta Platforms, Inc. and affiliates. All Rights Reserved.\n#\n# This source code is licensed under the MIT license found in the\n# LICENSE file in the root directory of this source tree.\n\"\"\"\nScript for backing out of the MP-resharded (reshard.pt) files and getting back\na non-flattened state dict.\n\nParticularly useful for converting our models to other repositories.\n\nUsage:\n    $ ls 125m\n    dict.txt\n    gpt2-merges.txt\n    gpt2-vocab.json\n    reshard-model_part-0.pt\n    reshard-model_part-1.pt\n\n    $ python -m metaseq.scripts.convert_to_singleton 125m\n\n    $ ls 125m\n    dict.txt\n    gpt2-merges.txt\n    gpt2-vocab.json\n    reshard-model_part-0.pt\n    reshard-model_part-1.pt\n    restored.pt\n\"\"\"\n\nimport argparse\nimport glob\nimport logging\nimport os\nimport sys\n\nimport torch\n\nfrom metaseq import options, tasks, checkpoint_utils, utils\nfrom metaseq.dataclass.configs import MetaseqConfig\nfrom metaseq.dataclass.utils import convert_namespace_to_omegaconf\nfrom metaseq.distributed import utils as distributed_utils\nfrom metaseq.distributed import fsdp_enable_wrap, fsdp_wrap\nfrom metaseq.distributed.stitch_fsdp_ckpt import reshard_megatron_parts\n\nlogging.basicConfig(\n    format=\"%(asctime)s | %(levelname)s | %(name)s | %(message)s\",\n    datefmt=\"%Y-%m-%d %H:%M:%S\",\n    level=os.environ.get(\"LOGLEVEL\", \"INFO\").upper(),\n    stream=sys.stdout,\n)\nlogger = logging.getLogger(\"convert_to_singleton\")\n\n\ndef create_generation_config_with_defaults(model_path, ddp_backend=\"pytorch_ddp\"):\n    files = glob.glob(f\"{model_path}/reshard*.pt\")\n\n    MP = len(files)\n    BPE_MERGES = model_path + \"/gpt2-merges.txt\"\n    BPE_VOCAB = model_path + \"/gpt2-vocab.json\"\n    HF_TOKENIZER = model_path + \"/gpt2-unified.json\"\n\n    if os.path.exists(HF_TOKENIZER):\n        print(f\"Using unified tokenizer file {HF_TOKENIZER}\")\n        TOKENIZER_ARGS = [\n            \"--hf-tokenizer\",\n            HF_TOKENIZER,\n        ]\n    else:\n        print(f\"Using vocab and merges files {BPE_VOCAB} {BPE_MERGES}\")\n        TOKENIZER_ARGS = [\n            \"--bpe-merges\",\n            BPE_MERGES,\n            \"--merges-filename\",\n            BPE_MERGES,\n            \"--bpe-vocab\",\n            BPE_VOCAB,\n            \"--vocab-filename\",\n            BPE_VOCAB,\n        ]\n\n    # Skeleton out all the annoying command line args we can infer\n    ARGS = (\n        [\n            \"--model-parallel-size\",\n            str(MP),\n            \"--distributed-world-size\",\n            str(MP),\n            \"--ddp-backend\",\n            ddp_backend,\n            \"--task\",\n            \"language_modeling\",\n        ]\n        + TOKENIZER_ARGS\n        + [\n            \"--bpe\",\n            \"hf_byte_bpe\",\n            \"--path\",\n            model_path + \"/reshard.pt\",\n            \"--checkpoint-shard-count\",\n            \"1\",\n            \"--use-sharded-state\",\n            model_path,\n        ]\n    )\n    print(ARGS)\n\n    # build up the config file\n    parser = options.get_generation_parser()\n    # dumb defaults overriding\n    parser.set_defaults(lr_scheduler=None, criterion=None)\n    args = options.parse_args_and_arch(parser, input_args=ARGS)\n    cfg = convert_namespace_to_omegaconf(args)\n    cfg.distributed_training.distributed_world_size = MP\n\n    return cfg\n\n\ndef worker_main(cfg: MetaseqConfig):\n    \"\"\"\n    Load up the model on all workers for Model Parallelism, then\n    unflatten, move to cpu, and save to \"restored.pt\".\n    \"\"\"\n    task = tasks.setup_task(cfg.task)\n\n    def _build_model(cfg, task):\n        cfg.model.tensor_parallel_init_model_on_gpu = True\n        model = task.build_model(cfg.model).cuda()\n        return fsdp_wrap(model)\n\n    with fsdp_enable_wrap(\n        cfg.distributed_training,\n        use_sharded_state=cfg.distributed_training.use_sharded_state,\n    ):\n        models, _model_args, _task = checkpoint_utils.load_model_ensemble_and_task(\n            utils.split_paths(cfg.common_eval.path),\n            arg_overrides=None,\n            task=task,\n            suffix=cfg.checkpoint.checkpoint_suffix,\n            strict=True,\n            num_shards=cfg.checkpoint.checkpoint_shard_count,\n            build_model_hook=_build_model,\n        )\n        model = models[0]\n\n    # consolidate everything on rank0\n    mp_size = distributed_utils.get_model_parallel_world_size()\n    model_parts = [{} for _ in range(mp_size)]\n\n    with model.summon_full_params():\n        for name, p in model.named_parameters():\n            gathered = [torch.zeros_like(p) for _ in range(mp_size)]\n            torch.distributed.all_gather(\n                gathered, p, group=distributed_utils.get_global_group()\n            )\n            for r, t in enumerate(gathered):\n                model_parts[r][name] = t.cpu()\n\n    glued = reshard_megatron_parts(model_parts, new_model_part_count=1)[0]\n    # glued['decoder.output_projection.weight'] = glued['decoder.embed_tokens.weight']\n\n    glued[\"decoder.version\"] = model.state_dict()[\"decoder.version\"].cpu()\n\n    if \"decoder.output_projection.weight\" in glued:\n        del glued[\"decoder.output_projection.weight\"]\n\n    output_sd = checkpoint_utils.load_checkpoint_to_cpu(\n        cfg.common_eval.path.replace(\"reshard.pt\", \"reshard-model_part-0.pt\")\n    )\n    output_sd[\"model\"] = utils.move_to_cpu(glued)\n    output_sd[\"cfg\"][\"model\"].arch = \"transformer_lm_megatron\"\n    output_sd[\"cfg\"][\"model\"]._name = \"transformer_lm_megatron\"\n\n    if distributed_utils.get_global_rank() == 0:\n        with open(cfg.task.data + \"/restored.pt\", \"wb\") as f:\n            torch.save(output_sd, f)\n\n\ndef main():\n    # parser to be used like docstring shows\n    real_parser = argparse.ArgumentParser()\n    real_parser.add_argument(\"location\")\n    args = real_parser.parse_args()\n\n    cfg = create_generation_config_with_defaults(args.location)\n    distributed_utils.call_main(cfg, worker_main)\n\n\nif __name__ == \"__main__\":\n    main()\n",
        "metaseq/scripts/generation_benchmarks.py": "# Copyright (c) Meta Platforms, Inc. and affiliates. All Rights Reserved.\n#\n# This source code is licensed under the MIT license found in the\n# LICENSE file in the root directory of this source tree.\n\nimport os\nfrom transformers import GPT2Tokenizer\nfrom metaseq import checkpoint_utils, tasks, utils\nimport torch\nfrom metaseq.scripts.convert_to_singleton import create_generation_config_with_defaults\nfrom metaseq.distributed import utils as dist_utils\nfrom metaseq.distributed import fsdp_enable_wrap, fsdp_wrap\nfrom metaseq.dataclass.configs import MetaseqConfig\nimport argparse\nimport urllib.request\nimport tarfile\nimport shutil\n\nprompts = [\"Paris is the capital of France and it\"] * 1000\nlinks_to_data = {}\nlinks_to_data[\"125m\"] = [\n    \"https://dl.fbaipublicfiles.com/opt/v1_20220502/125m/reshard-model_part-0.pt\",\n    \"https://dl.fbaipublicfiles.com/opt/v1_20220502/125m/reshard-model_part-1.pt\",\n]\nlinks_to_data[\"1.3b\"] = [\n    \"https://dl.fbaipublicfiles.com/opt/v1_20220502/1.3b/reshard-model_part-0.pt\",\n    \"https://dl.fbaipublicfiles.com/opt/v1_20220502/1.3b/reshard-model_part-1.pt\",\n]\nlinks_to_data[\"2.7b\"] = [\n    \"https://dl.fbaipublicfiles.com/opt/v1_20220502/2.7b/reshard-model_part-0.pt\",\n    \"https://dl.fbaipublicfiles.com/opt/v1_20220502/2.7b/reshard-model_part-1.pt\",\n    \"https://dl.fbaipublicfiles.com/opt/v1_20220502/2.7b/reshard-model_part-2.pt\",\n    \"https://dl.fbaipublicfiles.com/opt/v1_20220502/2.7b/reshard-model_part-3.pt\",\n]\nlinks_to_data[\"6.7b\"] = [\n    \"https://dl.fbaipublicfiles.com/opt/v1_20220502/6.7b/reshard-model_part-0.pt\",\n    \"https://dl.fbaipublicfiles.com/opt/v1_20220502/6.7b/reshard-model_part-1.pt\",\n]\nlinks_to_data[\"13b\"] = [\n    \"https://dl.fbaipublicfiles.com/opt/v1_20220502/13b/reshard-model_part-0.pt\",\n    \"https://dl.fbaipublicfiles.com/opt/v1_20220502/13b/reshard-model_part-1.pt\",\n]\nlinks_to_data[\"30b\"] = [\n    \"https://dl.fbaipublicfiles.com/opt/v1_20220502/30b/reshard-model_part-0.pt\",\n    \"https://dl.fbaipublicfiles.com/opt/v1_20220502/30b/reshard-model_part-1.pt\",\n]\nlinks_to_data[\"66b\"] = [\n    \"https://dl.fbaipublicfiles.com/opt/v1_20220502/66b/reshard-model_part-0-shard0.pt\",\n    \"https://dl.fbaipublicfiles.com/opt/v1_20220502/66b/reshard-model_part-1-shard0.pt\",\n    \"https://dl.fbaipublicfiles.com/opt/v1_20220502/66b/reshard-model_part-2-shard0.pt\",\n    \"https://dl.fbaipublicfiles.com/opt/v1_20220502/66b/reshard-model_part-3-shard0.pt\",\n    \"https://dl.fbaipublicfiles.com/opt/v1_20220502/66b/reshard-model_part-4-shard0.pt\",\n    \"https://dl.fbaipublicfiles.com/opt/v1_20220502/66b/reshard-model_part-5-shard0.pt\",\n    \"https://dl.fbaipublicfiles.com/opt/v1_20220502/66b/reshard-model_part-6-shard0.pt\",\n    \"https://dl.fbaipublicfiles.com/opt/v1_20220502/66b/reshard-model_part-7-shard0.pt\",\n]\n\n\nlinks_to_data[\"dependencies\"] = [\n    \"http://dl.fbaipublicfiles.com/metaseq_benchmark_dependencies.tar.gz\"\n]\n\n\ndef setup_vocab_and_merges(model_path):\n    vocab_file = os.path.join(model_path, \"gpt2-vocab.json\")\n    merges_file = os.path.join(model_path, \"gpt2-merges.txt\")\n    tokenizer = GPT2Tokenizer(vocab_file, merges_file)\n    tokenizer.save_pretrained(model_path)\n    return vocab_file, merges_file, tokenizer\n\n\ndef tensorize_input(tokenizer, prompts):\n    input_ids = tokenizer(prompts, return_tensors=\"pt\").input_ids\n    input_ids = torch.cat([torch.tensor([[0]]), input_ids], dim=-1)\n    input_ids = input_ids\n    return input_ids\n\n\ndef load_mp_model_and_run_eval(cfg: MetaseqConfig, **kwargs):\n    _, _, tokenizer = setup_vocab_and_merges(kwargs[\"model_path\"])\n    prompt_ids = []\n    thread_times = [\n        torch.zeros(1).cuda() for _ in range(dist_utils.get_model_parallel_world_size())\n    ]\n    start = torch.cuda.Event(enable_timing=True)\n    end = torch.cuda.Event(enable_timing=True)\n    start.record()\n    for prompt in prompts:\n        input_ids = tensorize_input(tokenizer, prompt)\n        prompt_ids.append(input_ids)\n\n    prompt_ids = torch.cat(prompt_ids).cuda()\n    end.record()\n    # Waits for everything to finish running\n    torch.cuda.synchronize()\n    prompt_loading = start.elapsed_time(end)\n\n    task = tasks.setup_task(cfg.task)\n\n    def _build_model(cfg, task):\n        # hardcoded to cpu & fp16\n        # Hardcoded tensor_parallel_init_model_on_gpu to be True\n        cfg.model.tensor_parallel_init_model_on_gpu = True\n        model = task.build_model(cfg.model).half().cuda()\n        return fsdp_wrap(model)\n\n    with fsdp_enable_wrap(\n        cfg.distributed_training,\n        use_sharded_state=cfg.distributed_training.use_sharded_state,\n    ):\n        models, _model_args, _task = checkpoint_utils.load_model_ensemble_and_task(\n            utils.split_paths(cfg.common_eval.path),\n            arg_overrides=None,\n            task=task,\n            suffix=cfg.checkpoint.checkpoint_suffix,\n            strict=True,\n            num_shards=cfg.checkpoint.checkpoint_shard_count,\n            build_model_hook=_build_model,\n        )\n        model = models[0]\n\n    model.summon_full_params()\n    model = model.eval()\n\n    start = torch.cuda.Event(enable_timing=True)\n    end = torch.cuda.Event(enable_timing=True)\n    start.record()\n    with torch.no_grad():\n        model(prompt_ids)[0]\n    end.record()\n\n    # Waits for everything to finish running\n    torch.cuda.synchronize()\n\n    total_time = start.elapsed_time(end) + prompt_loading\n    # total time is in ms\n    times_list = torch.tensor(total_time / 1000).cuda()\n\n    torch.distributed.all_gather(\n        thread_times, times_list, group=dist_utils.get_global_group()\n    )\n    return thread_times\n\n\ndef generation_statistics(model_path):\n    cfg = create_generation_config_with_defaults(model_path)\n    thread_times = dist_utils.call_main(\n        cfg,\n        load_mp_model_and_run_eval,\n        model_path=model_path,\n    )\n\n    avg_total_gen_time = sum(thread_times).item() / len(thread_times)\n\n    print(\"Total generation time \" + str(avg_total_gen_time))\n    print(\n        \"Words per second : \"\n        + str((len(prompts[0]) * len(prompts)) / avg_total_gen_time)\n    )\n\n\ndef download_data(key):\n    if key not in links_to_data.keys():\n        raise AssertionError(\n            \"You passed model name \"\n            + str(key)\n            + \". Please make sure that model name is in \"\n            + str(list(links_to_data.keys()))\n        )\n\n    # download and unzip dependencies\n    for dependency in links_to_data[\"dependencies\"]:\n        urllib.request.urlretrieve(dependency, \"./dependencies.tar.gz\")\n        file = tarfile.open(\"./dependencies.tar.gz\")\n        file.extractall()\n\n    # download model checkpoint\n    for shard in links_to_data[key]:\n        file_name = shard.split(\"/\")[-1]\n        urllib.request.urlretrieve(shard, \"./dependencies/\" + file_name)\n\n\ndef cleanup_data():\n    shutil.rmtree(\"./dependencies\")\n    os.remove(\"./dependencies.tar.gz\")\n\n\nif __name__ == \"__main__\":\n    parser = argparse.ArgumentParser()\n    parser.add_argument(\"--model_name\", nargs=\"+\")\n    args = parser.parse_args()\n\n    if args.model_name is None:\n        raise AssertionError(\"Please pass at least one model name as argument\")\n\n    for key in args.model_name:\n        print(\"Now running generation benchmarks for \" + str(key))\n        download_data(key)\n        generation_statistics(\n            model_path=\"./dependencies\",\n        )\n        cleanup_data()\n",
        "metaseq/scripts/reshard_consolidated.py": "#!/usr/bin/env python3 -u\n# Copyright (c) Meta Platforms, Inc. and affiliates. All Rights Reserved.\n#\n# This source code is licensed under the MIT license found in the\n# LICENSE file in the root directory of this source tree.\n\nimport logging\nimport os\nfrom copy import deepcopy\nfrom typing import Any, Dict, List, Tuple\n\nimport fire\nimport torch\nimport torch.nn.functional as F\n\n\nlogging.basicConfig(format=\"%(asctime)s | %(name)s | %(message)s\", level=logging.INFO)\nlogger: logging.Logger = logging.getLogger(\"metaseq.scripts.reshard_consolidated\")\n\n\ndef reshard_consolidated_checkpoint(\n    input: str, output: str, num_output_shards: int = 1\n) -> None:\n    \"\"\"\n    Reshard an FSDP-consolidated checkpoint and write outputs to files. The model weights\n    are flattened before the resharding logic applies. The input checkpoint is expected to\n    contain unflattened, FSDP-consolidated model weights.\n\n    Args:\n        :param input: A path to the input checkpoint (e.g. \"opt-2.7b-dp1-mp2/reshard-model_part-0.pt\").\n        :param output: A string pattern specifying the path names of the output shards.\n            Shard indices can be included in the path names if the pattern includes `{i}`.\n            (e.g. \"opt-2.7b-dp4-mp2/checkpoint_last-model_part-0-shard{i}.pt\").\n        :param num_output_shards: The number of output shards.\n    \"\"\"\n    state_dict = torch.load(input)\n    resharded_weights, resharded_metadata = reshard_unflattened_model_weights(\n        state_dict[\"model\"], state_dict[\"shard_metadata\"], num_output_shards\n    )\n    for shard_idx in range(num_output_shards):\n        output_file = output.format(i=shard_idx)\n        os.makedirs(os.path.dirname(output_file), exist_ok=True)\n        shard_state_dict = {\n            \"model\": resharded_weights[shard_idx],\n            \"shard_metadata\": resharded_metadata[shard_idx],\n        }\n        for key in state_dict.keys() - shard_state_dict.keys() - {\"optimizer_state\"}:\n            shard_state_dict[key] = state_dict[key]\n        logger.info(f\"Writing a resharded state dict to {output_file}\")\n        torch.save(shard_state_dict, output_file)\n\n\ndef reshard_unflattened_model_weights(\n    unsharded_weights: Dict[str, torch.Tensor],\n    shard_metadata: List[Dict[str, Any]],\n    num_output_shards: int = 1,\n) -> List[Dict[str, torch.Tensor]]:\n    logger.info(f\"Reshard unflattened model weights into {num_output_shards} shard(s)\")\n    resharded_weights = [{} for _ in range(num_output_shards)]\n\n    # We copy the buffer values from the first shard as they are not sharded by FSDP\n    for buffer_name in shard_metadata[\"buffer_names\"]:\n        if buffer_name not in unsharded_weights:\n            raise ValueError(f\"No buffer found for buffer name {buffer_name}.\")\n        for shard_idx in range(num_output_shards):\n            resharded_weights[shard_idx][buffer_name] = unsharded_weights[buffer_name]\n        unsharded_weights.pop(buffer_name)\n\n    resharded_metadata = [deepcopy(shard_metadata) for _ in range(num_output_shards)]\n    for idx, param_metadata in enumerate(shard_metadata[\"param_metadata\"]):\n        fsdp_path = param_metadata[\"fsdp_path\"]\n        for flat_name, params in param_metadata[\"params\"].items():\n            full_key = \".\".join([fsdp_path, flat_name]) if fsdp_path else flat_name\n            names = [f\"{fsdp_path}.{n}\" if fsdp_path else n for n in params[\"names\"]]\n            flattened = _flatten_tensors([unsharded_weights[k] for k in names])\n            # Reshard weights by chunking the unsharded tensor\n            weights, paddings = _shard_and_pad_tensor(flattened, num_output_shards)\n            for shard_idx, (weight, pad) in enumerate(zip(weights, paddings)):\n                resharded_weights[shard_idx][full_key] = weight\n                resharded_metadata[shard_idx][\"param_metadata\"][idx][\"params\"][\n                    flat_name\n                ][\"padding\"] = pad\n\n    return resharded_weights, resharded_metadata\n\n\ndef _flatten_tensors(tensors: List[torch.Tensor]) -> torch.Tensor:\n    output = torch.empty(sum(t.numel() for t in tensors), dtype=tensors[0].dtype)\n    offset = 0\n    for tensor in tensors:\n        output[offset : offset + tensor.numel()].copy_(tensor.view(-1))\n        offset += tensor.numel()\n    return output\n\n\ndef _shard_and_pad_tensor(\n    tensor: torch.Tensor, num_shards: int, dim: int = 0\n) -> Tuple[List[torch.Tensor], List[int]]:\n    if num_shards == 1:\n        return [tensor], [0]\n    #  Cloning is needed as torch.save always writes unsliced tensors\n    chunks = [chunk.clone() for chunk in tensor.chunk(num_shards, dim=dim)]\n    assert len(chunks) == num_shards, len(chunks)\n    paddings = [chunks[0].numel() - chunk.numel() for chunk in chunks]\n    for idx, (chunk, padding) in enumerate(zip(chunks, paddings)):\n        if padding > 0:\n            chunks[idx] = F.pad(chunk, [0, padding])\n    return chunks, paddings\n\n\nif __name__ == \"__main__\":\n    \"\"\"\n    Example usage:\n        python -m metaseq.scripts.reshard_consolidated \\\n        --input \"opt-2.7b-dp1-mp2/reshard-model_part-0.pt\" \\\n        --output \"opt-2.7b-dp4-mp2/checkpoint_last-model_part-0-shard{i}.pt\" \\\n        --num-output-shards 4\n    \"\"\"\n    fire.Fire(reshard_consolidated_checkpoint)\n",
        "metaseq/scripts/reshard_fsdp.py": "# Copyright (c) Meta Platforms, Inc. and affiliates. All Rights Reserved.\n#\n# This source code is licensed under the MIT license found in the\n# LICENSE file in the root directory of this source tree.\n\nimport logging\nimport os\nimport re\nfrom copy import deepcopy\nfrom glob import glob\nfrom typing import Any, Dict, List, Optional, Tuple\n\nimport fire\nimport torch\nimport torch.nn.functional as F\n\nlogging.basicConfig(format=\"%(asctime)s | %(name)s | %(message)s\", level=logging.INFO)\nlogger: logging.Logger = logging.getLogger(\"metaseq.scripts.reshard_fsdp\")\n\n\n_STRING_TO_DTYPE: Dict[str, torch.dtype] = {\n    \"fp32\": torch.float32,\n    \"fp16\": torch.float16,\n    \"bf16\": torch.bfloat16,\n}\n\n\ndef reshard_fsdp_checkpoints(\n    input: str,\n    output: str,\n    num_output_shards: int = 1,\n    unflatten_weights: bool = True,\n    skip_optimizer_state: bool = False,\n    output_dtype: Optional[str] = None,\n    drop_metaseq_req: bool = False,\n) -> None:\n    \"\"\"\n    Reshard FSDP checkpoints and write outputs to files. The model weights and optimizer states\n    are merged from the sharded checkpoints before the resharding logic applies. The sharded\n    checkpoints are expected to contain shard metadata.\n\n    Args:\n        :param input: A glob pattern specifying the path names of the input shards.\n            (e.g. \"checkpoints/2.7B/raw/checkpoint_last-model_part-0-shard*.pt\").\n        :param output: A string pattern specifying the path names of the output shards.\n            Shard indices can be included in the path names if the pattern includes `{i}`.\n            (e.g. \"checkpoints/2.7B/reshard/reshard-model_part-0-shard{i}.pt\").\n        :param num_output_shards: The number of output shards.\n        :param unflatten_weights: Specifies whether the weights in the input shards should be\n            unflattened. This option is only applicable when the number of output shards is 1.\n        :param skip_optimizer_state: Specifies whether to skip the optimizer states from the input shards.\n        :param output_dtype: Specifies the weight dtype of output shards (e.g. \"fp32\", \"fp16\", \"bf16\").\n            By default, output_dtype is None and no dtype conversion is applied.\n    \"\"\"\n    if output_dtype and output_dtype not in _STRING_TO_DTYPE:\n        raise ValueError(f\"The specified output dtype {output_dtype} is not supported.\")\n    files = glob(input)\n    if len(files) == 0:\n        raise ValueError(\"The glob pattern doesn't match any sharded checkpoints.\")\n    files = sorted(files, key=lambda x: list(map(int, re.findall(r\"\\d+\", x))))\n    logger.info(f\"Found {len(files)} sharded checkpoints ({files[0]} to {files[-1]})\")\n\n    logger.info(\"Loading all sharded checkpoints to CPU\")\n    shard_state_dicts = [torch.load(path, torch.device(\"cpu\")) for path in files]\n\n    resharded_state_dicts = reshard_fsdp_state_dicts(\n        shard_state_dicts,\n        num_output_shards=num_output_shards,\n        unflatten_weights=unflatten_weights,\n        skip_optimizer_state=skip_optimizer_state,\n        dtype=_STRING_TO_DTYPE.get(output_dtype, None),\n        drop_metaseq_req=drop_metaseq_req,\n    )\n    for shard_idx, state_dict in enumerate(resharded_state_dicts):\n        output_file = output.format(i=shard_idx)\n        os.makedirs(os.path.dirname(output_file), exist_ok=True)\n        logger.info(f\"Writing a resharded state dict to {output_file}\")\n        torch.save(state_dict, output_file)\n\n\ndef reshard_fsdp_state_dicts(\n    shard_state_dicts: List[Dict[str, Any]],\n    num_output_shards: int = 1,\n    unflatten_weights: bool = True,\n    skip_optimizer_state: bool = False,\n    dtype: Optional[torch.dtype] = None,\n    drop_metaseq_req: bool = False,\n) -> List[Dict[str, Any]]:\n    logger.info(f\"Resharding state dicts into {num_output_shards} shard(s)\")\n    # Unshard model weights\n    resharded_weights, resharded_metadata = reshard_fsdp_model_weights(\n        shard_weights=[s[\"model\"] for s in shard_state_dicts],\n        shard_metadata=[s[\"shard_metadata\"] for s in shard_state_dicts],\n        num_output_shards=num_output_shards,\n        unflatten_weights=unflatten_weights,\n        dtype=dtype,\n    )\n    resharded_state_dicts = [{} for _ in range(num_output_shards)]\n    for shard_idx, (weight, metadata) in enumerate(\n        zip(resharded_weights, resharded_metadata)\n    ):\n        resharded_state_dicts[shard_idx][\"model\"] = weight\n        resharded_state_dicts[shard_idx][\"shard_metadata\"] = metadata\n\n    # Unshard last optimizer state\n    if not skip_optimizer_state and \"last_optimizer_state\" in shard_state_dicts[0]:\n        # Assume all optimizer states have same padding as model parameters\n        param_padding = [[] for _ in range(len(shard_state_dicts))]\n        for shard_idx, shard in enumerate(shard_state_dicts):\n            for metadata in shard[\"shard_metadata\"][\"param_metadata\"]:\n                param_padding[shard_idx].extend(\n                    param[\"padding\"] for param in metadata[\"params\"].values()\n                )\n        reshared_optim_states = reshard_fsdp_optim_state(\n            shard_optim_states=[s[\"last_optimizer_state\"] for s in shard_state_dicts],\n            shard_optim_padding=dict(enumerate(param_padding)),\n            num_output_shards=num_output_shards,\n            dtype=dtype,\n        )\n        for shard_idx, optim_state in enumerate(reshared_optim_states):\n            resharded_state_dicts[shard_idx][\"last_optimizer_state\"] = optim_state\n\n    # Copy other state values from the first shard\n    if drop_metaseq_req:\n        # mseq checkpoints are sometimes saved with their `train_iterator` state_dicts,\n        # which can only be unpickled in metaseq-aware envs. We strip these out here.\n        if (\n            \"extra_state\" in shard_state_dicts[0]\n            and \"train_iterator\" in shard_state_dicts[0][\"extra_state\"]\n            and \"tokenization_cache\"\n            in shard_state_dicts[0][\"extra_state\"][\"train_iterator\"]\n        ):\n            shard_state_dicts[0][\"extra_state\"][\"train_iterator\"] = None\n\n    for key in shard_state_dicts[0]:\n        if key not in {\"model\", \"last_optimizer_state\", \"shard_metadata\"}:\n            for shard_idx in range(num_output_shards):\n                resharded_state_dicts[shard_idx][key] = shard_state_dicts[0][key]\n\n    return resharded_state_dicts\n\n\ndef reshard_fsdp_model_weights(\n    shard_weights: List[Dict[str, torch.Tensor]],\n    shard_metadata: List[Dict[str, Any]],\n    num_output_shards: int = 1,\n    unflatten_weights: bool = False,\n    dtype: Optional[torch.dtype] = None,\n) -> List[Dict[str, torch.Tensor]]:\n    logger.info(f\"Resharding model weights into {num_output_shards} shard(s)\")\n    if len(shard_weights) != len(shard_metadata):\n        raise ValueError(\"Expect shard weights and shard metadata to have same length.\")\n    if unflatten_weights and num_output_shards > 1:\n        raise ValueError(\"Unflatten weights only if the number of output shards is 1.\")\n\n    resharded_weights = [{} for _ in range(num_output_shards)]\n    resharded_metadata = [deepcopy(shard_metadata[0]) for _ in range(num_output_shards)]\n    for idx, param_metadata in enumerate(shard_metadata[0][\"param_metadata\"]):\n        fsdp_path = param_metadata[\"fsdp_path\"]\n        for flat_name, param_info in param_metadata[\"params\"].items():\n            full_key = \".\".join([fsdp_path, flat_name]) if fsdp_path else flat_name\n            if full_key not in shard_weights[0]:\n                raise ValueError(f\"No weight found for key {full_key} in metadata.\")\n\n            # Unshard FSDP tensor weights\n            sharded_weights = []\n            for weight, metadata in zip(shard_weights, shard_metadata):\n                pad = metadata[\"param_metadata\"][idx][\"params\"][flat_name][\"padding\"]\n                sharded_weight = _maybe_type(weight[full_key], dtype)\n                sharded_weights.append(_unpad_tensor(sharded_weight, pad))\n            unsharded_weights = torch.cat(sharded_weights, dim=0)\n\n            # For single shard output, tensor weights can be unflattened\n            if unflatten_weights:\n                names, shapes, numels, _ = param_info.values()\n                assert sum(numels) == unsharded_weights.size(0)\n                for n, t, s in zip(names, unsharded_weights.split(numels), shapes):\n                    param_name = \".\".join([fsdp_path, n]) if fsdp_path else n\n                    resharded_weights[0][param_name] = t.view(s)\n                continue\n\n            # Otherwise, reshard weights by chunking the unsharded tensor\n            weights, paddings = _shard_and_pad_tensor(\n                unsharded_weights, num_output_shards\n            )\n            for shard_idx, (weight, pad) in enumerate(zip(weights, paddings)):\n                resharded_weights[shard_idx][full_key] = weight\n                resharded_metadata[shard_idx][\"param_metadata\"][idx][\"params\"][\n                    flat_name\n                ][\"padding\"] = pad\n\n        # Copy shared parameters\n        if unflatten_weights:\n            for src_path, dest_path in metadata.get(\"shared_param_info\", []):\n                resharded_weights[0][dest_path] = resharded_weights[src_path]\n\n    # We copy the buffer values from the first shard as they are not sharded by FSDP\n    for buffer_name in shard_metadata[0][\"buffer_names\"]:\n        if buffer_name not in shard_weights[0]:\n            raise ValueError(f\"No buffer found for buffer name {buffer_name}.\")\n        for shard_idx in range(num_output_shards):\n            resharded_weights[shard_idx][buffer_name] = _maybe_type(\n                shard_weights[0][buffer_name], dtype\n            )\n\n    return resharded_weights, resharded_metadata\n\n\ndef reshard_fsdp_optim_state(\n    shard_optim_states: List[Dict[str, Any]],\n    shard_optim_padding: Optional[Dict[str, int]] = None,\n    num_output_shards: int = 1,\n    dtype: Optional[torch.dtype] = None,\n) -> List[Dict[str, Any]]:\n    logger.info(f\"Resharding optimizer states into {num_output_shards} shard(s)\")\n    if any(\"state\" not in s for s in shard_optim_states):\n        raise ValueError(f\"Each sharded optimizer state dict should contain `state`.\")\n\n    # Reshard optimizer states\n    resharded_state_dict = [{\"state\": {}} for _ in range(num_output_shards)]\n    for idx, wrapped_state_dict in shard_optim_states[0][\"state\"].items():\n        for shard_idx in range(num_output_shards):\n            resharded_state_dict[shard_idx][\"state\"][idx] = {}\n        for key, value in wrapped_state_dict.items():\n            #  Copy non-tensor objects to outputs (e.g. step)\n            if not torch.is_tensor(value) or value.dim() == 0:\n                sharded_value = _maybe_type(value, dtype)\n                for shard_idx in range(num_output_shards):\n                    resharded_state_dict[shard_idx][\"state\"][idx][key] = sharded_value\n                continue\n\n            unsharded_value = torch.cat(\n                [_maybe_type(s[\"state\"][idx][key], dtype) for s in shard_optim_states]\n            )\n            unpadded_value = _unpad_tensor(\n                tensor=unsharded_value,\n                pad=shard_optim_padding.get(key, 0) if shard_optim_padding else 0,\n            )\n            chunks, _ = _shard_and_pad_tensor(unpadded_value, num_output_shards)\n            for shard_idx, chunk in enumerate(chunks):\n                resharded_state_dict[shard_idx][\"state\"][idx][key] = chunk\n\n    # Copy unsharded values from the first shard (e.g. loss scale, param groups)\n    for key in shard_optim_states[0]:\n        for shard_idx in range(num_output_shards):\n            if key != \"state\":\n                resharded_state_dict[shard_idx][key] = _maybe_type(\n                    shard_optim_states[0][key], dtype\n                )\n    return resharded_state_dict\n\n\ndef _shard_and_pad_tensor(\n    tensor: torch.Tensor, num_shards: int, dim: int = 0\n) -> Tuple[List[torch.Tensor], List[int]]:\n    if num_shards == 1:\n        return [tensor], [0]\n    #  Cloning is needed as torch.save always writes unsliced tensors\n    chunks = [chunk.clone() for chunk in tensor.chunk(num_shards, dim=dim)]\n    assert len(chunks) == num_shards, len(chunks)\n    paddings = [chunks[0].numel() - chunk.numel() for chunk in chunks]\n    for idx, (chunk, padding) in enumerate(zip(chunks, paddings)):\n        if padding > 0:\n            chunks[idx] = F.pad(chunk, [0, padding])\n    return chunks, paddings\n\n\ndef _unpad_tensor(tensor: torch.Tensor, pad: int) -> torch.Tensor:\n    if pad > 0:\n        tensor = tensor[:-pad]\n    return tensor\n\n\ndef _maybe_type(input: torch.Tensor, dtype: Optional[torch.dtype] = None):\n    return input.type(dtype) if dtype is not None else input\n\n\nif __name__ == \"__main__\":\n    \"\"\"\n    Example usage:\n        python -m metaseq.scripts.reshard_fsdp \\\n        --input \"opt-2.7b/raw/checkpoint_last-model_part-0-shard*.pt\" \\\n        --output \"opt-2.7b/reshard/reshard-model_part-0.pt\" \\\n        --num-output-shards 1 --skip-optimizer-state True --unflatten-weights True\n    \"\"\"\n    fire.Fire(reshard_fsdp_checkpoints)\n",
        "metaseq/scripts/reshard_mp.py": "# Copyright (c) Meta Platforms, Inc. and affiliates. All Rights Reserved.\n#\n# This source code is licensed under the MIT license found in the\n# LICENSE file in the root directory of this source tree.\nimport logging\nimport os\nimport re\nfrom glob import glob\n\nimport fire\nimport torch\n\nlogging.basicConfig(format=\"%(asctime)s | %(name)s | %(message)s\", level=logging.INFO)\nlogger: logging.Logger = logging.getLogger(\"metaseq.scripts.reshard_mp\")\n\n\ndef reshard_model_parallel_parts(\n    input: str, output: str, num_output_parts: int = 1, eps: float = 1e-8\n) -> None:\n    \"\"\"\n    Reshard model parallel (MP) parts and write outputs to files. The model weights\n    are merged from the input parts before the resharding logic applies. The model\n    parallel parts in the input are expected to contain unflattened, FSDP-consolidated\n    model weights (see the script `reshard_fsdp.py` for related information.)\n\n    Args:\n        :param input: A glob pattern specifying the path names of the input shards.\n            (e.g. \"checkpoints/opt-2.7b/reshard_no_os/reshard-model_part-*.pt\").\n        :param output: A string pattern specifying the path names of the output shards.\n            Shard indices can be included in the path names if the pattern includes `{i}`.\n            (e.g. \"checkpoints/opt-2.7b/reshard_no_os_mp8/reshard-model_part-{i}.pt\").\n        :param num_output_parts: The number of output model parallel parts.\n        :param eps: A tolerance threshold for the maximum discrepancy between MP parts.\n    \"\"\"\n    files = glob(input)\n    N, M = len(files), num_output_parts\n    if N == 0:\n        raise ValueError(\"The glob pattern doesn't match any model parallel parts.\")\n    files = sorted(files, key=lambda x: list(map(int, re.findall(r\"\\d+\", x))))\n    logger.info(f\"Found {len(files)} model parallel parts ({files[0]} to {files[-1]})\")\n\n    rank0_state_dict = torch.load(files[0], torch.device(\"cpu\"))\n    dim0_shard_regex = re.compile(\"embed_tokens|ffn_layernorm|fc1|(k|q|v)_proj\")\n    dim1_shard_regex = re.compile(\"(fc2|out_proj).weight\")\n    shared_regex = re.compile(\n        \"embed_positions|layer_norm|(fc2|out_proj).bias|output_projection|version\"\n    )\n\n    unsharded_dict = {}\n    logger.info(\"Allocating memory for unsharded checkpoint\")\n    for key, value in rank0_state_dict[\"model\"].items():\n        d0, d1 = value.size()[0], value.size()[1:]\n        if \"qkv\" in key:\n            unsharded_dict[key] = value.new_zeros(3, N * d0 // 3, *d1)\n        elif dim0_shard_regex.search(key):\n            unsharded_dict[key] = value.new_zeros(N * d0, *d1)\n        elif dim1_shard_regex.search(key):\n            assert len(d1) > 0\n            unsharded_dict[key] = value.new_zeros(d0, N * d1[0])\n\n    # Iteratively load checkpoints to avoid OOM issues\n    for i, file in enumerate(files):\n        logger.info(f\"Merging {file} into unsharded checkpoint\")\n        state_dict = torch.load(file, torch.device(\"cpu\"))\n        for key, value in state_dict[\"model\"].items():\n            d0, d1 = value.size()[0], value.size()[1:]\n            if \"qkv\" in key:\n                # Split and copy QKV weights\n                unsharded_dict[key][:, i * d0 // 3 : (i + 1) * d0 // 3].copy_(\n                    value.view(3, d0 // 3, *d1)\n                )\n            elif dim0_shard_regex.search(key):\n                # Concatenate along dim 0 (e.g. embed_tokens, fc1.weight, fc1.bias)\n                unsharded_dict[key][i * d0 : (i + 1) * d0].copy_(value)\n            elif dim1_shard_regex.search(key):\n                # Concatenate along dim 1 (e.g. fc2.weight, out_proj.weight)\n                unsharded_dict[key][:, i * d1[0] : (i + 1) * d1[0]].copy_(value)\n            elif shared_regex.search(key):\n                # Copy from rank 0 (e.g. embed_positions, final_layer_norm, fc2.bias, out_proj.bias)\n                unsharded_dict[key] = value\n                diff = _max_diff(rank0_state_dict[\"model\"][key], value)\n                if diff > eps:\n                    logger.warning(f\"Max value discrepancy for key '{key}': {diff:.4e}\")\n\n    for i in range(M):\n        sharded_dict = {}\n        logger.info(f\"Resharding state dict for model parallel part {i}\")\n        for key, value in rank0_state_dict[\"model\"].items():\n            if \"qkv\" in key:\n                # Merge QKV weights after chunking unsharded weight\n                sharded_dict[key] = unsharded_dict[key].chunk(M, dim=1)[i].flatten(0, 1)\n            elif dim0_shard_regex.search(key):\n                #  Cloning is needed as torch.save always writes unsliced tensors\n                sharded_dict[key] = unsharded_dict[key].chunk(M)[i].clone()\n            elif dim1_shard_regex.search(key):\n                sharded_dict[key] = unsharded_dict[key].chunk(M, dim=1)[i].clone()\n            elif all(p in key for p in (\"embed_positions\", \"_float_tensor\")):\n                # Assume embed positions are not learned (e.g. sinusoidal)\n                sharded_dict[key] = value.new_zeros(1)\n            elif shared_regex.search(key):\n                sharded_dict[key] = value.clone()\n\n        state_dict = {\"model\": sharded_dict}\n        # Copy other values from rank 0\n        for key in [\n            \"cfg\",\n            \"extra_state\",\n            \"optimizer_history\",\n            \"args\",\n            \"shard_metadata\",\n        ]:\n            state_dict[key] = rank0_state_dict.get(key, None)\n        state_dict[\"cfg\"][\"model\"].model_parallel_size = M\n\n        output_file = output.format(i=i)\n        os.makedirs(os.path.dirname(output_file), exist_ok=True)\n        logger.info(f\"Writing a resharded state dict to {output_file}\")\n        torch.save(state_dict, output_file)\n    logger.info(\"Done!\")\n\n\ndef _max_diff(tensor1: torch.Tensor, tensor2: torch.Tensor) -> float:\n    assert tensor1.size() == tensor2.size()\n    return (tensor1 - tensor2).abs().max().item()\n\n\nif __name__ == \"__main__\":\n    \"\"\"\n    Example usage:\n        python -m metaseq.scripts.reshard_mp \\\n        --input \"opt-2.7b/reshard_no_os/reshard-model_part-*.pt\" \\\n        --output \"opt-2.7b/reshard_no_os_mp8/reshard-model_part-{i}.pt\" \\\n        --num-output-parts 8\n    \"\"\"\n    fire.Fire(reshard_model_parallel_parts)\n",
        "metaseq/sequence_generator.py": "# Copyright (c) Meta Platforms, Inc. and affiliates. All Rights Reserved.\n#\n# This source code is licensed under the MIT license found in the\n# LICENSE file in the root directory of this source tree.\n\n# Reference:\n# https://github.com/facebookresearch/fairseq/blob/main/fairseq/sequence_generator.py\n\nimport logging\nimport math\nfrom typing import Dict, List, Optional, Tuple\nfrom metaseq.data.dictionary import Dictionary\nimport sys\n\nimport torch\nimport torch.nn as nn\nfrom torch import Tensor\n\nfrom metaseq import utils\nfrom metaseq.data import data_utils\nfrom metaseq.models import BaseDecoder\n\nlogger = logging.getLogger(__name__)\n\n\nclass SequenceGenerator(nn.Module):\n    def __init__(\n        self,\n        models,\n        tgt_dict: Dictionary,\n        beam_size: int = 1,\n        max_len_a: int = 0,\n        max_len_b: int = 200,\n        min_len: int = 1,\n        temperature: float = 1.0,\n        need_logprobs: bool = False,\n        stop: Optional[List[int]] = None,\n        topp: float = -1,\n        profile=False,\n        omega_bound: float = 0.3,\n        lambda_decay: float = -1,\n        alpha_presence: float = 0.0,\n        alpha_frequency: float = 0.0,\n        alpha_presence_src: float = 0.0,\n        alpha_frequency_src: float = 0.0,\n        alpha_src_penalty_end_idx: int = -1,\n    ):\n        \"\"\"Generates translations of a given source sentence.\n\n        For discussion of repetition penalties, see\n        https://github.com/facebookresearch/metaseq/pull/306\n\n        Args:\n            models: ensemble of models\n            beam_size (int, optional): beam width (default: 1)\n            max_len_a/b (int, optional): generate sequences of maximum length\n                ax + b, where x is the source length\n            min_len (int, optional): the minimum length of the generated output\n                (not including end-of-sentence)\n            temperature (float, optional): temperature, where values\n                >1.0 produce more uniform samples and values <1.0 produce\n                sharper samples (default: 1.0)\n            stop: An optional list of other tokens that can cause early termination.\n            need_logprobs (bool): Return additional log-prob distributions for\n                every timestep of the search.\n            omega_bound (float, optional): lower p-bound when decaying nucleus\n            lamda_decay (float, optional): decay factor for decaying p in nucleus sampling\n                default -1 disables.\n            alpha_presence (float, optional): repetition penalty for token presence in\n                current generation\n            alpha_frequency (float, optional): repetition penalty for token frequency in\n                current generation\n            alpha_presence_src (float, optional): repetition penalty for token presence in\n                incoming context\n            alpha_frequency_src (float, optional): repetition penalty for token frequency in\n                incoming context\n            alpha_src_penalty_end_idx (int, optional): index of the last token in incoming\n                context to consider for repetition penalty\n        \"\"\"\n        super().__init__()\n        self.model = models[0]\n        self.tgt_dict = tgt_dict\n        self.pad = tgt_dict.pad()\n        self.unk = tgt_dict.unk()\n        self.eos = tgt_dict.eos()\n        self.vocab_size = len(tgt_dict)\n        self.beam_size = beam_size\n        # the max beam size is the dictionary size - 1, since we never select pad\n        self.beam_size = min(beam_size, self.vocab_size - 1)\n        self.max_len_a = max_len_a\n        self.max_len_b = max_len_b\n        self.min_len = min_len\n        self.need_logprobs = need_logprobs\n        self.stop = stop if stop is not None else []\n        if topp is None:\n            topp = 0.0\n        self.sampling_topp = max(0, topp)\n        self.temperature = temperature\n        assert temperature >= 0, \"--temperature must be >=0\"\n\n        self.model.eval()\n        self.profile = profile\n\n        # factual nucleus\n        buffer = torch.zeros(beam_size)\n        self.sampling_topp = max(0, topp)\n        self.sampling_topp_tensor = (\n            buffer.clone().fill_(self.sampling_topp).unsqueeze(1)\n        )\n        self.init_p = self.sampling_topp_tensor.clone()\n        self.lambda_decay = lambda_decay\n        self.omega_bound = torch.tensor([omega_bound])\n        self.toks_since_reset = buffer.clone()\n        self.full_stop_list = torch.tensor([tgt_dict.index(w) for w in [\".\", \"?\", \"!\"]])\n\n        # repetition reduction\n        self.alpha_presence = alpha_presence\n        self.alpha_frequency = alpha_frequency\n        self.alpha_presence_src = alpha_presence_src\n        self.alpha_frequency_src = alpha_frequency_src\n        self.alpha_src_penalty_end_idx = alpha_src_penalty_end_idx\n\n    def cuda(self):\n        self.model.cuda()\n        return self\n\n    @torch.no_grad()\n    def forward(\n        self,\n        sample: Dict[str, Dict[str, Tensor]],\n        prefix_tokens: Optional[Tensor] = None,\n        bos_token: Optional[int] = None,\n    ):\n        \"\"\"Generate a batch of translations.\"\"\"\n        return self._generate(sample, prefix_tokens, bos_token=bos_token)\n\n    @torch.no_grad()\n    def generate(self, models, sample: Dict[str, Dict[str, Tensor]], **kwargs):\n        \"\"\"Generate translations. Match the api of other metaseq generators.\"\"\"\n        return self._generate(sample, **kwargs)\n\n    def _generate(\n        self,\n        sample: Dict[str, Dict[str, Tensor]],\n        prefix_tokens: Optional[Tensor] = None,\n        bos_token: Optional[int] = None,\n    ):\n        \"\"\"\n        Args:\n            sample (dict): batch\n            prefix_tokens (torch.LongTensor, optional): force decoder to begin\n                with these tokens\n            bos_token (int, optional): beginning of sentence token\n                (default: self.eos)\n        \"\"\"\n        incremental_states = torch.jit.annotate(\n            Dict[str, Dict[str, Optional[Tensor]]], {}\n        )\n        net_input = sample[\"net_input\"]\n\n        if \"src_tokens\" in net_input:\n            src_tokens = net_input[\"src_tokens\"]\n            # length of the source text being the character length except EndOfSentence and pad\n            src_lengths = (\n                (src_tokens.ne(self.eos) & src_tokens.ne(self.pad)).long().sum(dim=1)\n            )\n        elif \"source\" in net_input:\n            src_tokens = net_input[\"source\"]\n            src_lengths = (\n                net_input[\"padding_mask\"].size(-1) - net_input[\"padding_mask\"].sum(-1)\n                if net_input[\"padding_mask\"] is not None\n                else torch.tensor(src_tokens.size(-1)).to(src_tokens)\n            )\n        else:\n            raise Exception(\"expected src_tokens or source in net input\")\n\n        # bsz: total number of sentences in beam\n        # Note that src_tokens may have more than 2 dimensions (i.e. audio features)\n        bsz, src_len = src_tokens.size()[:2]\n        beam_size = self.beam_size\n\n        max_len = min(self.model.max_decoder_positions(), self.max_len_b or 1e99)\n        min_len = min(max_len, self.min_len or 0)\n\n        assert (\n            min_len <= max_len\n        ), \"min_len cannot be larger than max_len, please adjust these!\"\n\n        # placeholder of indices for bsz * beam_size to hold tokens and accumulative scores\n        new_order = torch.arange(bsz).view(-1, 1).repeat(1, beam_size).view(-1)\n        new_order = new_order.to(src_tokens.device).long()\n\n        # initialize buffers\n        (\n            scores,\n            tokens,\n            count_tokens,\n            count_tokens_src,\n        ) = self._initialize_generation_buffers(\n            bsz, beam_size, int(max_len), src_tokens\n        )\n\n        # notes:\n        # - scores \\in FloatTensor(bsz * beam_size, max_len)\n        # - tokens \\in LongTensor(bsz * beam_size, max_len)\n        # - count_tokens \\in LongTensor(bsz * beam_size, vocab_size)\n        # - count_tokens_src \\in LongTensor(bsz * beam_size, vocab_size)\n        # - src_tokens \\in LongTensor(bsz, prompt_len)\n        # - all_lprobs \\in FloatTensor(bsz * beam_size, max_len, vocab_size)\n        #   is the next word distribution at every timestep\n\n        if self.need_logprobs:\n            # lprobs are costly for memory, so only compute them if we have to\n            all_lprobs = (\n                torch.zeros(bsz * beam_size, max_len, self.vocab_size)\n                .to(src_tokens)\n                .float()\n            )\n\n        # first forward through all the fixed tokens with forced decoding we'll\n        # need to handle normalization and prep for bookkeeping of incremental\n        # decoding\n        start_step = src_tokens.shape[1]\n        # set all the forced tokens\n        tokens[:, :start_step] = src_tokens.repeat_interleave(beam_size, 0)\n        # compute the model predictions\n        model_out = self.model(\n            tokens[:, :start_step],\n            incremental_state=incremental_states,\n        )\n        # temperature and normalization\n        # convert to float before the temparture divide to ensure good precision.\n        # Avoid dividing by 1.0 to prevent unnecessary numerical instability\n        # and always log in float\n        model_predictions = model_out[0].float()\n        if self.temperature > 0 and self.temperature != 1.0:\n            model_predictions.div_(self.temperature)\n        # lprobs is the log probability of each possible token in every position\n        # lprobs \\in FloatTensor(bsz * beam_size, prompt_len, vocab_size)\n        lprobs = self.model.get_normalized_probs(model_predictions, log_probs=True)\n\n        # don't allow generation of eos/pad\n        model_predictions[:, :, self.eos] = -math.inf\n        model_predictions[:, :, self.pad] = -math.inf\n        for stop_token in self.stop:\n            model_predictions[:, :, stop_token] = -math.inf\n\n        if self.need_logprobs:\n            all_lprobs[:, 1:start_step] = lprobs[:, :-1].type_as(all_lprobs)\n        else:\n            all_lprobs = None\n\n        # find and store the logprobs of each prompt token, cutting out the\n        # rest of the vocab. Note the shift of 1 here b/c autoregressive.\n        prompt_tokens = tokens[:, 1:start_step].unsqueeze(-1)\n        # look up a specific vocab logprob, and broadcast it into scores\n        toscores = torch.gather(lprobs, -1, prompt_tokens).squeeze(-1)\n        scores[:, 1:start_step] = toscores.type_as(scores)\n        # reset scores after the last point of forced decoding and gather the\n        # probabilities of the most recent token prediction, as search\n        # decisions are only over the most recent token.\n        lprobs_cut = []\n        for i in range(src_tokens.shape[0]):\n            prompt_len = src_lengths[i]\n            scores[i * beam_size : (i + 1) * beam_size, prompt_len + 1 :] = 0.0  # reset\n            lprobs_cut.append(lprobs[i * beam_size : (i + 1) * beam_size, prompt_len])\n        lprobs = torch.cat(lprobs_cut, dim=0)\n\n        eos_mask = torch.zeros(lprobs.size(0), dtype=torch.bool, device=lprobs.device)\n\n        for step in range(start_step, max_len):\n            if step < min_len:\n                # minimum length constraint (does not apply if using prefix_tokens)\n                lprobs[:, self.eos] = -math.inf\n                for stop_token in self.stop:\n                    lprobs[:, stop_token] = -math.inf\n\n            lprobs[lprobs != lprobs] = torch.tensor(-math.inf).to(lprobs)\n            lprobs[:, self.pad] = -math.inf  # never select pad\n\n            # handle max length constraint\n            if step >= max_len - 1:\n                lprobs[:, : self.eos] = -math.inf\n                lprobs[:, self.eos + 1 :] = -math.inf\n\n            # apply repetition penalties\n            lprobs = self._apply_repetition_penalties(\n                lprobs, count_tokens, count_tokens_src\n            )\n\n            # already ended beams should only do eos\n            lprobs[eos_mask, : self.eos] = -math.inf\n            lprobs[eos_mask, self.eos + 1 :] = -math.inf\n\n            # find our next tokens and record them\n            # protect this step for the last token so we don't overflow\n            next_scores, next_toks = self._sample_topp(lprobs)\n            count_tokens = self._update_repetition_counts(next_toks, count_tokens)\n            if step < max_len:\n                tokens[:, step] = next_toks\n                scores[:, step] = next_scores\n                if self.need_logprobs:\n                    all_lprobs[:, step] = lprobs\n\n            eos_mask |= next_toks == self.eos\n            for stop_token in self.stop:\n                # if there are other early stopping tokens, allow those to trigger stop\n                eos_mask |= next_toks == stop_token\n\n            if torch.all(eos_mask):\n                break\n\n            # forward through the next pass\n            model_out = self.model.decoder(\n                tokens[:, : step + 1],\n                incremental_state=incremental_states,\n            )\n            # see above for why this must remain float\n            model_predictions = model_out[0].float()\n            if self.temperature > 0 and self.temperature != 1.0:\n                model_predictions.div_(self.temperature)\n            lprobs = self.model.get_normalized_probs(model_predictions, log_probs=True)\n            lprobs = lprobs[:, -1, :]\n\n        # we want the highest scoring items to be top ranked\n        beamscores = scores.view(bsz, beam_size, -1).cumsum(dim=-1)[:, :, -1]\n        indices = beamscores.sort(dim=-1, descending=True).indices\n        sorted_indices = (\n            indices + beam_size * torch.arange(bsz, device=lprobs.device).unsqueeze(1)\n        ).view(-1)\n        tokens = tokens[sorted_indices]\n        scores = scores[sorted_indices]\n\n        # prepare the return value\n        retval = {\n            \"tokens\": tokens.view(bsz, beam_size, -1),\n            \"scores\": scores.view(bsz, beam_size, -1),\n        }\n        if all_lprobs is not None:\n            all_lprobs = all_lprobs[sorted_indices]\n            retval[\"distributions\"] = all_lprobs.view(\n                bsz, beam_size, -1, self.vocab_size\n            )\n        return retval\n\n    def _initialize_generation_buffers(\n        self, bsz: int, beam_size: int, max_len: int, src_tokens: torch.Tensor\n    ) -> Tuple[\n        torch.Tensor, torch.Tensor, Optional[torch.Tensor], Optional[torch.Tensor]\n    ]:\n        \"\"\"\n        Initialize buffers for use during generation.\n\n        Args:\n            bsz (int): batch size\n            beam_size (int): beam size\n            max_len (int): max gen length\n            src_tokens (tensor): incoming context\n\n        Returns:\n            scores: token score buffer [bsz*beam_size, max_len]\n            tokens: generation tokens [bsz*beam_size, max_len]\n            count_tokens: token count buffer for in-generation repetition penalty\n                [bsz*beam_size, vocab_size]\n            count_tokens_src: token count buffer for in-context repetition penalty\n                [bsz*beam_size, vocab_size]\n        \"\"\"\n        scores = torch.zeros(bsz * beam_size, max_len).to(src_tokens).float()\n        tokens = (\n            torch.zeros(bsz * beam_size, max_len).to(src_tokens).long().fill_(self.pad)\n        )\n        count_tokens = None\n        count_tokens_src = None\n        if self.alpha_presence > 0 or self.alpha_frequency > 0:\n            count_tokens = torch.zeros(bsz * beam_size, self.vocab_size).to(src_tokens)\n        if self.alpha_presence_src > 0 or self.alpha_frequency_src > 0:\n            # histc requires floats\n            float_src_tokens = src_tokens.float()\n            if self.alpha_src_penalty_end_idx > 0 and not (bsz > 1):\n                # ignore this parameter if we're in a batch. sorry!\n                float_src_tokens = float_src_tokens[:, : self.alpha_src_penalty_end_idx]\n            count_tokens_src = torch.zeros(bsz * beam_size, self.vocab_size).to(\n                src_tokens\n            )\n            for i in range(bsz * beam_size):\n                # histc constructs a histogram counting frequencies of occuring values\n                # and buckets into vocab_size bins\n                count_tokens_src[i] = torch.histc(\n                    float_src_tokens[i], self.vocab_size, min=0, max=self.vocab_size\n                )\n        if self.lambda_decay > 0:\n            # need to reset the buffers\n            self.toks_since_reset = self.toks_since_reset.unsqueeze(0).repeat(bsz, 1)\n            self.sampling_topp_tensor = self.sampling_topp_tensor.repeat(bsz, 1)\n            self.init_p = self.init_p.repeat(bsz, 1)\n\n        return scores, tokens, count_tokens, count_tokens_src\n\n    def _sample_topp(self, lprobs):\n        \"\"\"Sample among the smallest set of elements whose cumulative probability mass exceeds p.\n        See `\"The Curious Case of Neural Text Degeneration\"\n        (Holtzman et al., 2019) <https://arxiv.org/abs/1904.09751>`_.\n        Args:\n            lprobs: (bsz x input_beam_size x vocab_size)\n                the model's log-probabilities over the vocabulary at the current step\n        Return: A tuple of (trimed_probs, truncated_indices) where:\n            trimed_probs: (bsz x input_beam_size x ?)\n                the model's probabilities over the elements selected to sample from. The\n                width of the third dimension is determined by top-P.\n            truncated_indices: (bsz x input_beam_size x ?)\n                the indices of the chosen elements.\n        \"\"\"\n        if self.temperature == 0.0 or self.sampling_topp == 0.0:\n            # greedy search\n            return tuple(lprobs.max(dim=-1))\n\n        probs = torch.softmax(lprobs, dim=-1)\n        sprobs, sinds = probs.sort(dim=-1, descending=True)\n        if self.lambda_decay <= 0:\n            # normal nucleus sampling\n            mask = (sprobs.cumsum(dim=-1) - sprobs) >= self.sampling_topp\n        else:\n            # factual/decayed nucleus sampling. each batch item may have\n            # a different topp value, so the mask is different for each\n            mask = (sprobs.cumsum(dim=-1) - sprobs) >= self.sampling_topp_tensor.expand(\n                sprobs.size()\n            ).to(sprobs.device)\n        trunc_sprobs = sprobs.detach().clone()\n        trunc_sprobs[mask] = 0\n        trunc_sprobs.div_(trunc_sprobs.sum(dim=-1).unsqueeze(-1))\n        choices = torch.multinomial(trunc_sprobs, 1)[:, 0]\n        hyp_ids = torch.arange(lprobs.size(0)).to(lprobs.device)\n        tok_ids = sinds[hyp_ids, choices]\n        scores = sprobs[hyp_ids, choices].log()\n        if self.lambda_decay > 0:\n            self._update_sampling_topp(tok_ids)\n        return scores, tok_ids\n\n    def _update_sampling_topp(self, tokens: torch.Tensor):\n        \"\"\"\n        Update the p for sampling according to Factual Nucleus generation\n        (see https://arxiv.org/abs/2206.04624 for more details)\n\n        Args:\n            tokens (torch.Tensor): chosen generation tokens\n        \"\"\"\n        for batch_i, toks in enumerate(tokens):\n            if toks.dim() == 1:\n                for beam_i, t in enumerate(toks):\n                    if self.full_stop_list.to(tokens.device).eq(t).sum() > 0:\n                        self.toks_since_reset[batch_i, beam_i] = 0\n                    else:\n                        self.toks_since_reset[batch_i, beam_i] += 1\n                    decay_factor = max(0, self.toks_since_reset[batch_i, beam_i] - 1)\n                    self.sampling_topp_tensor[batch_i, beam_i] = torch.max(\n                        self.omega_bound,\n                        self.init_p[batch_i, beam_i]\n                        * (self.lambda_decay ** (decay_factor)),\n                    )\n            else:\n                t = toks\n                if self.full_stop_list.to(tokens.device).eq(t).sum() > 0:\n                    self.toks_since_reset[batch_i] = 0\n                else:\n                    self.toks_since_reset[batch_i] += 1\n                decay_factor = max(0, self.toks_since_reset[batch_i] - 1)\n                self.sampling_topp_tensor[batch_i] = torch.max(\n                    self.omega_bound,\n                    self.init_p[batch_i] * (self.lambda_decay ** (decay_factor)),\n                )\n\n    def _apply_repetition_penalties(\n        self,\n        lprobs: torch.Tensor,\n        count_tokens: Optional[torch.Tensor],\n        count_tokens_src: Optional[torch.Tensor],\n    ) -> torch.Tensor:\n        \"\"\"\n        Apply repetition penalties.\n\n        From https://beta.openai.com/docs/api-reference/engines/retrieve:\n\n        mu[j] -> mu[j] - c[j] * alpha_frequency - float(c[j] > 0) * alpha_presence\n        Where:\n\n        mu[j] is the logit of the j-th token\n        c[j] is how often that token was sampled prior to the current position\n        float(c[j] > 0) is 1 if c[j] > 0 and 0 otherwise\n        alpha_frequency is the frequency penalty coefficient\n        alpha_presence is the presence penalty coefficient\n\n        :param lprobs:\n            (bsz*beam_size, vocab_size) tensor\n        :param count_tokens:\n            (bsz*beam_size, vocab_size) tensor\n        :param count_tokens_src:\n            (bsz*beam_size, vocab_size) tensor\n\n        :return lprobs:\n            return new lprobs!\n        \"\"\"\n        if self.alpha_frequency > 0 or self.alpha_presence > 0:\n            assert count_tokens is not None\n            lprobs = (\n                lprobs\n                - (count_tokens * self.alpha_frequency)\n                - (count_tokens.gt(0) * self.alpha_presence)\n            )\n        if self.alpha_frequency_src > 0 or self.alpha_presence_src > 0:\n            assert count_tokens_src is not None\n            lprobs = (\n                lprobs\n                - (count_tokens_src * self.alpha_frequency_src)\n                - (count_tokens_src.gt(0) * self.alpha_presence_src)\n            )\n\n        return lprobs\n\n    def _update_repetition_counts(\n        self, tokens: torch.Tensor, count_tokens: Optional[torch.Tensor]\n    ) -> Optional[torch.Tensor]:\n        \"\"\"\n        Update the repetition counts\n\n        :param tokens:\n            [batchsize * beam, 1]\n        :param count_tokens:\n            [batchsize * beam, vocab_size]\n        \"\"\"\n        if self.alpha_frequency > 0 or self.alpha_presence > 0:\n            assert count_tokens is not None\n            for i, t in enumerate(tokens):\n                count_tokens[i, t] += 1\n        return count_tokens\n\n\nclass EnsembleModel(nn.Module):\n    \"\"\"A wrapper around an ensemble of models.\"\"\"\n\n    def __init__(self, models):\n        super().__init__()\n        self.models_size = len(models)\n        # method '__len__' is not supported in ModuleList for torch script\n        self.single_model = models[0]\n        self.models = nn.ModuleList(models)\n\n        self.has_incremental: bool = False\n        if all(\n            hasattr(m, \"decoder\") and isinstance(m.decoder, BaseDecoder) for m in models\n        ):\n            self.has_incremental = True\n\n    def forward(self):\n        pass\n\n    def has_encoder(self):\n        return hasattr(self.single_model, \"encoder\")\n\n    def has_incremental_states(self):\n        return self.has_incremental\n\n    def max_decoder_positions(self):\n        return min(\n            [\n                m.max_decoder_positions()\n                for m in self.models\n                if hasattr(m, \"max_decoder_positions\")\n            ]\n            + [sys.maxsize]\n        )\n\n    def set_decoder_beam_size(self, beam_size):\n        \"\"\"Set beam size for efficient beamable enc-dec attention.\"\"\"\n        if beam_size > 1:\n            for model in self.models:\n                if hasattr(model, \"set_beam_size\"):\n                    model.set_beam_size(beam_size)\n\n    @torch.jit.export\n    def forward_encoder(self, net_input: Dict[str, Tensor]):\n        if not self.has_encoder():\n            return None\n        return [model.encoder.forward_torchscript(net_input) for model in self.models]\n\n    @torch.jit.export\n    def forward_decoder(\n        self,\n        tokens,\n        encoder_outs: List[Dict[str, List[Tensor]]],\n        incremental_states: List[Dict[str, Dict[str, Optional[Tensor]]]],\n        temperature: float = 1.0,\n    ):\n        log_probs = []\n        avg_attn: Optional[Tensor] = None\n        encoder_out: Optional[Dict[str, List[Tensor]]] = None\n        for i, model in enumerate(self.models):\n            if self.has_encoder():\n                encoder_out = encoder_outs[i]\n            # decode each model\n            if self.has_incremental_states():\n                decoder_out = model.decoder.forward(\n                    tokens,\n                    encoder_out=encoder_out,\n                    incremental_state=incremental_states[i],\n                )\n            else:\n                if hasattr(model, \"decoder\"):\n                    decoder_out = model.decoder.forward(tokens, encoder_out=encoder_out)\n                else:\n                    decoder_out = model.forward(tokens)\n\n            attn: Optional[Tensor] = None\n            decoder_len = len(decoder_out)\n            if decoder_len > 1 and decoder_out[1] is not None:\n                if isinstance(decoder_out[1], Tensor):\n                    attn = decoder_out[1]\n                else:\n                    attn_holder = decoder_out[1][\"attn\"]\n                    if isinstance(attn_holder, Tensor):\n                        attn = attn_holder\n                    elif attn_holder is not None:\n                        attn = attn_holder[0]\n                if attn is not None:\n                    attn = attn[:, -1, :]\n\n            decoder_out_tuple = (\n                decoder_out[0][:, -1:, :].div_(temperature),\n                None if decoder_len <= 1 else decoder_out[1],\n            )\n            probs = model.get_normalized_probs(\n                decoder_out_tuple, log_probs=True, sample=None\n            )\n            probs = probs[:, -1, :]\n            if self.models_size == 1:\n                return probs, attn\n\n            log_probs.append(probs)\n            if attn is not None:\n                if avg_attn is None:\n                    avg_attn = attn\n                else:\n                    avg_attn.add_(attn)\n\n        avg_probs = torch.logsumexp(torch.stack(log_probs, dim=0), dim=0) - math.log(\n            self.models_size\n        )\n\n        if avg_attn is not None:\n            avg_attn.div_(self.models_size)\n        return avg_probs, avg_attn\n\n    @torch.jit.export\n    def reorder_encoder_out(\n        self, encoder_outs: Optional[List[Dict[str, List[Tensor]]]], new_order\n    ):\n        \"\"\"\n        Reorder encoder output according to *new_order*.\n        Args:\n            encoder_out: output from the ``forward()`` method\n            new_order (LongTensor): desired order\n        Returns:\n            *encoder_out* rearranged according to *new_order*\n        \"\"\"\n        new_outs: List[Dict[str, List[Tensor]]] = []\n        if not self.has_encoder():\n            return new_outs\n        for i, model in enumerate(self.models):\n            assert encoder_outs is not None\n            new_outs.append(\n                model.encoder.reorder_encoder_out(encoder_outs[i], new_order)\n            )\n        return new_outs\n\n    @torch.jit.export\n    def reorder_incremental_state(\n        self,\n        incremental_states: List[Dict[str, Dict[str, Optional[Tensor]]]],\n        new_order,\n    ):\n        if not self.has_incremental_states():\n            return\n        for i, model in enumerate(self.models):\n            model.decoder.reorder_incremental_state_scripting(\n                incremental_states[i], new_order\n            )\n\n\nclass SequenceGeneratorWithAlignment(SequenceGenerator):\n    def __init__(\n        self, models, tgt_dict, left_pad_target=False, print_alignment=\"hard\", **kwargs\n    ):\n        \"\"\"Generates translations of a given source sentence.\n        Produces alignments following \"Jointly Learning to Align and\n        Translate with Transformer Models\" (Garg et al., EMNLP 2019).\n        Args:\n            left_pad_target (bool, optional): Whether or not the\n                hypothesis should be left padded or not when they are\n                teacher forced for generating alignments.\n        \"\"\"\n        super().__init__(EnsembleModelWithAlignment(models), tgt_dict, **kwargs)\n        self.left_pad_target = left_pad_target\n\n        if print_alignment == \"hard\":\n            self.extract_alignment = utils.extract_hard_alignment\n        elif print_alignment == \"soft\":\n            self.extract_alignment = utils.extract_soft_alignment\n\n    @torch.no_grad()\n    def generate(self, models, sample, **kwargs):\n        finalized = super()._generate(sample, **kwargs)\n\n        src_tokens = sample[\"net_input\"][\"src_tokens\"]\n        bsz = src_tokens.shape[0]\n        beam_size = self.beam_size\n        (\n            src_tokens,\n            src_lengths,\n            prev_output_tokens,\n            tgt_tokens,\n        ) = self._prepare_batch_for_alignment(sample, finalized)\n        if any(getattr(m, \"full_context_alignment\", False) for m in self.model.models):\n            attn = self.model.forward_align(src_tokens, src_lengths, prev_output_tokens)\n        else:\n            attn = [\n                finalized[i // beam_size][i % beam_size][\"attention\"].transpose(1, 0)\n                for i in range(bsz * beam_size)\n            ]\n\n        if src_tokens.device != \"cpu\":\n            src_tokens = src_tokens.to(\"cpu\")\n            tgt_tokens = tgt_tokens.to(\"cpu\")\n            attn = [i.to(\"cpu\") for i in attn]\n\n        # Process the attn matrix to extract hard alignments.\n        for i in range(bsz * beam_size):\n            alignment = self.extract_alignment(\n                attn[i], src_tokens[i], tgt_tokens[i], self.pad, self.eos\n            )\n            finalized[i // beam_size][i % beam_size][\"alignment\"] = alignment\n        return finalized\n\n    def _prepare_batch_for_alignment(self, sample, hypothesis):\n        src_tokens = sample[\"net_input\"][\"src_tokens\"]\n        bsz = src_tokens.shape[0]\n        src_tokens = (\n            src_tokens[:, None, :]\n            .expand(-1, self.beam_size, -1)\n            .contiguous()\n            .view(bsz * self.beam_size, -1)\n        )\n        src_lengths = sample[\"net_input\"][\"src_lengths\"]\n        src_lengths = (\n            src_lengths[:, None]\n            .expand(-1, self.beam_size)\n            .contiguous()\n            .view(bsz * self.beam_size)\n        )\n        prev_output_tokens = data_utils.collate_tokens(\n            [beam[\"tokens\"] for example in hypothesis for beam in example],\n            self.pad,\n            self.eos,\n            self.left_pad_target,\n            move_eos_to_beginning=True,\n        )\n        tgt_tokens = data_utils.collate_tokens(\n            [beam[\"tokens\"] for example in hypothesis for beam in example],\n            self.pad,\n            self.eos,\n            self.left_pad_target,\n            move_eos_to_beginning=False,\n        )\n        return src_tokens, src_lengths, prev_output_tokens, tgt_tokens\n\n\nclass EnsembleModelWithAlignment(EnsembleModel):\n    \"\"\"A wrapper around an ensemble of models.\"\"\"\n\n    def __init__(self, models):\n        super().__init__(models)\n\n    def forward_align(self, src_tokens, src_lengths, prev_output_tokens):\n        avg_attn = None\n        for model in self.models:\n            decoder_out = model(src_tokens, src_lengths, prev_output_tokens)\n            attn = decoder_out[1][\"attn\"][0]\n            if avg_attn is None:\n                avg_attn = attn\n            else:\n                avg_attn.add_(attn)\n        if len(self.models) > 1:\n            avg_attn.div_(len(self.models))\n        return avg_attn\n",
        "metaseq/service/__init__.py": "# Copyright (c) Meta, Inc. and its affiliates.\n# This source code is licensed under the MIT license found in the\n# LICENSE file in the root directory of this source tree.\n",
        "metaseq/service/constants.py": "# Copyright (c) Meta Platforms, Inc. and affiliates. All Rights Reserved.\n#\n# This source code is licensed under the MIT license found in the\n# LICENSE file in the root directory of this source tree.\n\nimport os\n\nMAX_SEQ_LEN = 2048\nBATCH_SIZE = 2048  # silly high bc we dynamically batch by MAX_BATCH_TOKENS\nMAX_BATCH_TOKENS = 3072\nDEFAULT_PORT = 6010\nMODEL_PARALLEL = 8\nTOTAL_WORLD_SIZE = 8\nMAX_BEAM = 16\n\ntry:\n    # internal logic denoting where checkpoints are in meta infrastructure\n    from metaseq_internal.constants import CHECKPOINT_FOLDER\nexcept ImportError:\n    # CHECKPOINT_FOLDER should point to a shared drive (e.g. NFS) where the\n    # checkpoints from S3 are stored. As an example:\n    # CHECKPOINT_FOLDER = \"/example/175B/reshard_no_os\"\n    # $ ls /example/175B/reshard_no_os\n    # reshard-model_part-0.pt\n    # reshard-model_part-1.pt\n    # reshard-model_part-2.pt\n    # reshard-model_part-3.pt\n    # reshard-model_part-4.pt\n    # reshard-model_part-5.pt\n    # reshard-model_part-6.pt\n    # reshard-model_part-7.pt\n    CHECKPOINT_FOLDER = \"/example/175B/reshard_no_os\"\n\n# tokenizer files\nBPE_MERGES = os.path.join(CHECKPOINT_FOLDER, \"gpt2-merges.txt\")\nBPE_VOCAB = os.path.join(CHECKPOINT_FOLDER, \"gpt2-vocab.json\")\nMODEL_FILE = os.path.join(CHECKPOINT_FOLDER, \"reshard.pt\")\n\n\nLAUNCH_ARGS = [\n    f\"--model-parallel-size {MODEL_PARALLEL}\",\n    f\"--distributed-world-size {TOTAL_WORLD_SIZE}\",\n    \"--ddp-backend pytorch_ddp\",\n    # If using FSDP shards, replace ddp-backend and add use-sharded-state\n    # \"--ddp-backend fully_sharded\",\n    # \"--use-sharded-state\",\n    \"--task language_modeling\",\n    f\"--bpe-merges {BPE_MERGES}\",\n    f\"--bpe-vocab {BPE_VOCAB}\",\n    \"--bpe hf_byte_bpe\",\n    f\"--merges-filename {BPE_MERGES}\",  # TODO(susanz): hack for getting interactive_hosted working on public repo\n    f\"--vocab-filename {BPE_VOCAB}\",  # TODO(susanz): hack for getting interactive_hosted working on public repo\n    f\"--path {MODEL_FILE}\",\n    \"--beam 1\",\n    \"--distributed-port 13000\",\n    \"--checkpoint-shard-count 1\",\n    f\"--batch-size {BATCH_SIZE}\",\n    f\"--buffer-size {BATCH_SIZE * MAX_SEQ_LEN}\",\n    f\"--max-tokens {BATCH_SIZE * MAX_SEQ_LEN}\",\n    \"/tmp\",  # required \"data\" argument.\n]\n\n# Optional arg overrides which influence model loading during inference\nINFERENCE_ARG_OVERRIDES = {}\n",
        "metaseq/service/queue.py": "# Copyright (c) Meta Platforms, Inc. and affiliates. All Rights Reserved.\n#\n# This source code is licensed under the MIT license found in the\n# LICENSE file in the root directory of this source tree.\n\nimport hashlib\nfrom queue import PriorityQueue\nimport random\n\n\nclass KeyedPriorityQueueCollection:\n    \"\"\"\n    Create a collection of priority queues that are ordered by\n    a key.  Used for grouping specific types of workers\n    \"\"\"\n\n    def __init__(self):\n        self.queues = {}\n\n    def put(self, key, item):\n        \"\"\"\n        :param key: key of the queue to put the item into\n        :param item: item to add to the queue\n        \"\"\"\n        if key not in self.queues:\n            self.queues[key] = PriorityQueue()\n        self.queues[key].put(item)\n\n    # TODO: this can be a max heap to avoid linear lookup\n    def get_largest_queue_key(self):\n        \"\"\"\n        ### Returns the key of the queue with the most jobs\n        \"\"\"\n        if len(self.queues):\n            return max(self.queues, key=lambda key: self.queues[key].qsize())\n        else:\n            return None\n\n    def get_largest_queue(self):\n        \"\"\"\n        ### Returns the queue with the most jobs\n        \"\"\"\n        key = self.get_largest_queue_key()\n        if key:\n            return self.queues[key]\n        else:\n            return None\n\n\nclass PriorityQueueRingShardKeyable:\n    \"\"\"\n    Interface for ensuring that the put method\n    has a method to invoke for getting a queue_key\n    \"\"\"\n\n    def queue_key(\n        self,\n    ) -> str:\n        pass\n\n\nclass PriorityQueueRingShard:\n    \"\"\"\n    Creates a hashed queue shard, with an\n    added deskewing factor for avoiding hot keys (i.e.\n    default settings on generation).  The hashing algorithm\n    uses either a consistent modulo for bucketing, or in the\n    case of deskewing, there is the introduction of a deskew\n    factor which is inconsistent but ensures even distribution.\n    \"\"\"\n\n    @staticmethod\n    def key_from_dictionary(key_dict):\n        \"\"\"\n        :param key_dict: dictionary of keys and values to build shard key from\n        \"\"\"\n        return \":\".join([f\"{k}:{key_dict[k]}\" for k in sorted(key_dict.keys())])\n\n    def __init__(self, num_shards=1, deskew_factor=1):\n        \"\"\"\n        :param num_shards: total number of shards to hash (i.e. number of workers)\n        :param deskew_factor: number of virtual keys per shard.  Reduces key skewing.\n        \"\"\"\n        self.num_shards = num_shards\n        self.deskew_factor = deskew_factor\n        self.deskewing = deskew_factor > 1\n        self.queue_shards = [\n            KeyedPriorityQueueCollection() for i in range(self.num_shards)\n        ]\n\n    def put(self, item: PriorityQueueRingShardKeyable):\n        \"\"\"\n        :param key: key of the queue to put the item into\n        :param item: item to add to the queue\n        \"\"\"\n        key = item.queue_key()\n        shard_index = self.get_shard_index_for_key(key)\n        self.queue_shards[shard_index].put(key, item)\n\n    def get_shard_index_for_key(self, key):\n        \"\"\"\n        ### hashing is deterministic except when deskewing is enabled.\n        :param key: the key to be sharded.\n        \"\"\"\n        if self.deskewing:\n            deskew_offset = random.randint(0, self.deskew_factor * self.num_shards)\n            key = f\"{deskew_offset}:{key}\"\n        return int(hashlib.sha1(key.encode(\"utf-8\")).hexdigest(), 16) % self.num_shards\n",
        "metaseq/service/responses.py": "# Copyright (c) Meta Platforms, Inc. and affiliates. All Rights Reserved.\n#\n# This source code is licensed under the MIT license found in the\n# LICENSE file in the root directory of this source tree.\n\nimport uuid\nimport time\nimport metaseq.service.constants\n\nCHECKPOINT_FOLDER = metaseq.service.constants.CHECKPOINT_FOLDER\n\n\nclass OAIResponse:\n    def __init__(self, results: list) -> None:\n        self.results = results\n        self.response_id = str(uuid.uuid4())\n        self.created = int(time.time())\n\n    def __dict__(self):\n        return {\n            \"id\": self.response_id,\n            \"object\": \"text_completion\",\n            \"created\": self.created,\n            \"model\": CHECKPOINT_FOLDER,\n            \"choices\": [\n                {\n                    \"text\": result[\"text\"],\n                    \"logprobs\": {\n                        \"tokens\": result[\"tokens\"],\n                        \"token_logprobs\": result[\"token_scores\"],\n                        \"text_offset\": result[\"text_offset\"],\n                        \"top_logprobs\": result[\"top_logprobs\"],\n                        \"finish_reason\": \"length\",  # TODO: implement this\n                    },\n                }\n                for result in self.results\n            ],\n        }\n",
        "metaseq/service/utils.py": "# Copyright (c) Meta Platforms, Inc. and affiliates. All Rights Reserved.\n#\n# This source code is licensed under the MIT license found in the\n# LICENSE file in the root directory of this source tree.\n\nimport socket\nimport logging\nimport sys\nimport os\n\n\ndef normalize_newlines(s: str):\n    \"\"\"\n    normalizes new lines, i.e. '\\r\\n' to '\\n'\n    \"\"\"\n    # note that web browsers send \\r\\n but our training data uses \\n.\n    return s.replace(\"\\r\\n\", \"\\n\").replace(\"\\r\", \"\\n\")\n\n\ndef get_my_ip():\n    \"\"\"\n    returns ip / hostname of current host\n    \"\"\"\n    return socket.gethostbyname(socket.gethostname())\n\n\ndef build_logger():\n    logging.basicConfig(\n        format=\"%(asctime)s | %(levelname)s | %(name)s | %(message)s\",\n        datefmt=\"%Y-%m-%d %H:%M:%S\",\n        level=os.environ.get(\"LOGLEVEL\", \"INFO\").upper(),\n        stream=sys.stdout,\n    )\n    logger = logging.getLogger(\"metaseq.cli.interactive\")\n    return logger\n",
        "metaseq/service/workers.py": "# Copyright (c) Meta Platforms, Inc. and affiliates. All Rights Reserved.\n#\n# This source code is licensed under the MIT license found in the\n# LICENSE file in the root directory of this source tree.\n\nfrom metaseq.service.queue import PriorityQueueRingShard\nfrom dataclasses import dataclass\nfrom typing import Any\nimport queue\nimport math\n\n\nQUEUE_KEYS = [\n    \"temperature\",\n    \"top_p\",\n    \"n\",\n    \"lambda_decay\",\n    \"omega_bound\",\n    \"alpha_presence\",\n    \"alpha_frequency\",\n    \"alpha_presence_src\",\n    \"alpha_frequency_src\",\n]\n\n\n@dataclass\nclass WorkItem:\n    \"\"\"\n    Sortable entry for the batching PriorityQueue.\n    \"\"\"\n\n    cost: int  # lower is serviced first\n    uid: int  # unique id to map back to multi-input requests\n    return_queue: queue.Queue\n    data: Any\n    prompt_len: int\n    gen_len: int\n\n    # for sorting / priority queue\n    def __lt__(self, other: \"WorkItem\"):\n        return (self.cost, self.uid) < (other.cost, other.uid)\n\n    # for sorting / priority queue\n    def __eq__(self, other: \"WorkItem\"):\n        return (self.cost, self.uid) == (other.cost, other.uid)\n\n    def queue_key(self):\n        return PriorityQueueRingShard.key_from_dictionary(\n            {k: self.data[k] for k in QUEUE_KEYS}\n        )\n\n    @staticmethod\n    def generate_worker(encoded_prompt, batch_queue, **generation_args):\n        request_object = {\"input\": encoded_prompt, **generation_args}\n        ret_queue = queue.Queue()\n        enc_len = len(encoded_prompt)\n        cost = enc_len + int(\n            math.ceil((enc_len / 10) ** 2)\n        )  # account for the cost of both linear and attention layers\n        batch_queue.put(WorkItem(cost, 0, ret_queue, request_object))\n        _, result = ret_queue.get()\n        return result\n",
        "metaseq/tasks/__init__.py": "# Copyright (c) Meta Platforms, Inc. and affiliates. All Rights Reserved.\n#\n# This source code is licensed under the MIT license found in the\n# LICENSE file in the root directory of this source tree.\n\"\"\"isort:skip_file\"\"\"\n\nimport argparse\nimport importlib\nimport os\n\nfrom metaseq.dataclass import MetaseqDataclass\nfrom metaseq.dataclass.utils import merge_with_parent, populate_dataclass\nfrom hydra.core.config_store import ConfigStore\n\nfrom .base_task import BaseTask, LegacyTask  # noqa\n\n\n# register dataclass\nTASK_DATACLASS_REGISTRY = {}\nTASK_REGISTRY = {}\nTASK_CLASS_NAMES = set()\n\n\ndef setup_task(cfg: MetaseqDataclass, **kwargs):\n    task = None\n    task_name = getattr(cfg, \"task\", None)\n\n    if isinstance(task_name, str):\n        # legacy tasks\n        task = TASK_REGISTRY[task_name]\n        if task_name in TASK_DATACLASS_REGISTRY:\n            dc = TASK_DATACLASS_REGISTRY[task_name]\n            cfg = populate_dataclass(dc(), cfg)\n    else:\n        task_name = getattr(cfg, \"_name\", None)\n\n        if task_name and task_name in TASK_DATACLASS_REGISTRY:\n            dc = TASK_DATACLASS_REGISTRY[task_name]\n            cfg = merge_with_parent(dc(), cfg)\n            task = TASK_REGISTRY[task_name]\n\n    assert (\n        task is not None\n    ), f\"Could not infer task type from {cfg}. Available tasks: {TASK_REGISTRY.keys()}\"\n\n    return task.setup_task(cfg, **kwargs)\n\n\ndef register_task(name, dataclass=None):\n    \"\"\"\n    New tasks can be added to metaseq with the\n    :func:`~metaseq.tasks.register_task` function decorator.\n\n    For example::\n\n        @register_task('classification')\n        class ClassificationTask(BaseTask):\n            (...)\n\n    .. note::\n\n        All Tasks must implement the :class:`~metaseq.tasks.BaseTask`\n        interface.\n\n    Args:\n        name (str): the name of the task\n    \"\"\"\n\n    def register_task_cls(cls):\n        if name in TASK_REGISTRY:\n            raise ValueError(\"Cannot register duplicate task ({})\".format(name))\n        if not issubclass(cls, BaseTask):\n            raise ValueError(\n                \"Task ({}: {}) must extend BaseTask\".format(name, cls.__name__)\n            )\n        if cls.__name__ in TASK_CLASS_NAMES:\n            raise ValueError(\n                \"Cannot register task with duplicate class name ({})\".format(\n                    cls.__name__\n                )\n            )\n        TASK_REGISTRY[name] = cls\n        TASK_CLASS_NAMES.add(cls.__name__)\n\n        if dataclass is not None and not issubclass(dataclass, MetaseqDataclass):\n            raise ValueError(\n                \"Dataclass {} must extend MetaseqDataclass\".format(dataclass)\n            )\n\n        cls.__dataclass = dataclass\n        if dataclass is not None:\n            TASK_DATACLASS_REGISTRY[name] = dataclass\n\n            cs = ConfigStore.instance()\n            node = dataclass()\n            node._name = name\n            cs.store(name=name, group=\"task\", node=node, provider=\"metaseq\")\n\n        return cls\n\n    return register_task_cls\n\n\ndef get_task(name):\n    return TASK_REGISTRY[name]\n\n\n# automatically import any Python files in the tasks/ directory\ntasks_dir = os.path.dirname(__file__)\nfor file in os.listdir(tasks_dir):\n    path = os.path.join(tasks_dir, file)\n    if (\n        not file.startswith(\"_\")\n        and not file.startswith(\".\")\n        and (file.endswith(\".py\") or os.path.isdir(path))\n    ):\n        task_name = file[: file.find(\".py\")] if file.endswith(\".py\") else file\n        module = importlib.import_module(\"metaseq.tasks.\" + task_name)\n\n        # expose `task_parser` for sphinx\n        if task_name in TASK_REGISTRY:\n            parser = argparse.ArgumentParser(add_help=False)\n            group_task = parser.add_argument_group(\"Task name\")\n            # fmt: off\n            group_task.add_argument('--task', metavar=task_name,\n                                    help='Enable this task with: ``--task=' + task_name + '``')\n            # fmt: on\n            group_args = parser.add_argument_group(\"Additional command-line arguments\")\n            TASK_REGISTRY[task_name].add_args(group_args)\n            globals()[task_name + \"_parser\"] = parser\n",
        "metaseq/tasks/base_task.py": "# Copyright (c) Meta Platforms, Inc. and affiliates. All Rights Reserved.\n#\n# This source code is licensed under the MIT license found in the\n# LICENSE file in the root directory of this source tree.\n\nimport logging\nimport warnings\nfrom argparse import Namespace\nfrom typing import Any, Callable, Dict, List\n\nimport torch\nfrom omegaconf import DictConfig\n\nfrom metaseq import metrics\nfrom metaseq.data import Dictionary, BaseDataset, data_utils, encoders, iterators\nfrom metaseq.dataclass import MetaseqDataclass\nfrom metaseq.dataclass.utils import gen_parser_from_dataclass\nfrom metaseq.utils import tokenize_line\n\nlogger = logging.getLogger(__name__)\n\n\nclass StatefulContainer(object):\n    _state: Dict[str, Any] = dict()\n    _factories: Dict[str, Callable[[], Any]] = dict()\n\n    def add_factory(self, name, factory: Callable[[], Any]):\n        self._factories[name] = factory\n\n    def merge_state_dict(self, state_dict: Dict[str, Any]):\n        self._state.update(state_dict)\n\n    @property\n    def state_dict(self) -> Dict[str, Any]:\n        return self._state\n\n    def __getattr__(self, name):\n        if name not in self._state and name in self._factories:\n            self._state[name] = self._factories[name]()\n\n        if name in self._state:\n            return self._state[name]\n\n        raise AttributeError(f\"Task state has no factory for attribute {name}\")\n\n\nclass BaseTask(object):\n    \"\"\"\n    Tasks store dictionaries and provide helpers for loading/iterating over\n    Datasets, initializing the Model/Criterion and calculating the loss.\n\n    Tasks have limited statefulness. In particular, state that needs to be\n    saved to/loaded from checkpoints needs to be stored in the `self.state`\n    :class:`StatefulContainer` object. For example::\n\n        self.state.add_factory(\"dictionary\", self.load_dictionary)\n        print(self.state.dictionary)  # calls self.load_dictionary()\n\n    This is necessary so that when loading checkpoints, we can properly\n    recreate the task state after initializing the task instance.\n    \"\"\"\n\n    @classmethod\n    def add_args(cls, parser):\n        \"\"\"Add task-specific arguments to the parser.\"\"\"\n        dc = getattr(cls, \"__dataclass\", None)\n        if dc is not None:\n            gen_parser_from_dataclass(parser, dc())\n\n    @staticmethod\n    def logging_outputs_can_be_summed(criterion) -> bool:\n        \"\"\"\n        Whether the logging outputs returned by `train_step` and `valid_step` can\n        be summed across workers prior to calling `reduce_metrics`.\n        Setting this to True will improve distributed training speed.\n        \"\"\"\n        return criterion.logging_outputs_can_be_summed()\n\n    cfg: MetaseqDataclass\n    datasets: Dict[str, BaseDataset]\n    dataset_to_epoch_iter: Dict[BaseDataset, Any]\n    state: StatefulContainer = None\n\n    def __init__(self, cfg: MetaseqDataclass, **kwargs):\n        self.cfg = cfg\n        self.datasets = dict()\n        self.dataset_to_epoch_iter = dict()\n        self.state = StatefulContainer()\n\n    @classmethod\n    def load_dictionary(cls, filename):\n        \"\"\"Load the dictionary from the filename\n\n        Args:\n            filename (str): the filename\n        \"\"\"\n        return Dictionary.load(filename)\n\n    @classmethod\n    def build_dictionary(\n        cls, filenames, workers=1, threshold=-1, nwords=-1, padding_factor=8\n    ):\n        \"\"\"Build the dictionary\n\n        Args:\n            filenames (list): list of filenames\n            workers (int): number of concurrent workers\n            threshold (int): defines the minimum word count\n            nwords (int): defines the total number of words in the final dictionary,\n                including special symbols\n            padding_factor (int): can be used to pad the dictionary size to be a\n                multiple of 8, which is important on some hardware (e.g., Nvidia\n                Tensor Cores).\n        \"\"\"\n        d = Dictionary()\n        for filename in filenames:\n            Dictionary.add_file_to_dictionary(filename, d, tokenize_line, workers)\n        d.finalize(threshold=threshold, nwords=nwords, padding_factor=padding_factor)\n        return d\n\n    @classmethod\n    def setup_task(cls, cfg: DictConfig, **kwargs):\n        \"\"\"Setup the task (e.g., load dictionaries).\n\n        Args:\n            cfg (omegaconf.DictConfig): parsed command-line arguments\n        \"\"\"\n        return cls(cfg, **kwargs)\n\n    def load_dataset(\n        self,\n        split: str,\n        combine: bool = False,\n        task_cfg: MetaseqDataclass = None,\n        **kwargs,\n    ):\n        \"\"\"Load a given dataset split.\n\n        Args:\n            split (str): name of the split (e.g., train, valid, test)\n            combine (bool): combines a split segmented into pieces into one dataset\n            task_cfg (MetaseqDataclass): optional task configuration stored in the checkpoint that can be used\n                                         to load datasets\n        \"\"\"\n        raise NotImplementedError\n\n    def dataset(self, split):\n        \"\"\"\n        Return a loaded dataset split.\n\n        Args:\n            split (str): name of the split (e.g., train, valid, test)\n\n        Returns:\n            a :class:`~metaseq.data.BaseDataset` corresponding to *split*\n        \"\"\"\n        from metaseq.data import BaseDataset\n\n        if split not in self.datasets:\n            raise KeyError(\"Dataset not loaded: \" + split)\n        if not isinstance(self.datasets[split], BaseDataset):\n            raise TypeError(\"Datasets are expected to be of type BaseDataset\")\n        return self.datasets[split]\n\n    def filter_indices_by_size(\n        self, indices, dataset, max_positions=None, ignore_invalid_inputs=False\n    ):\n        \"\"\"\n        Filter examples that are too large\n\n        Args:\n            indices (np.array): original array of sample indices\n            dataset (~metaseq.data.BaseDataset): dataset to batch\n            max_positions (optional): max sentence length supported by the\n                model (default: None).\n            ignore_invalid_inputs (bool, optional): don't raise Exception for\n                sentences that are too long (default: False).\n        Returns:\n            np.array: array of filtered sample indices\n        \"\"\"\n        indices, ignored = dataset.filter_indices_by_size(indices, max_positions)\n        if len(ignored) > 0:\n            if not ignore_invalid_inputs:\n                raise Exception(\n                    (\n                        \"Size of sample #{} is invalid (={}) since max_positions={}, \"\n                        \"skip this example with --skip-invalid-size-inputs-valid-test\"\n                    ).format(ignored[0], dataset.size(ignored[0]), max_positions)\n                )\n            logger.warning(\n                (\n                    \"{:,} samples have invalid sizes and will be skipped, \"\n                    \"max_positions={}, first few sample ids={}\"\n                ).format(len(ignored), max_positions, ignored[:10])\n            )\n        return indices\n\n    def get_batch_iterator(\n        self,\n        dataset: BaseDataset,\n        max_tokens=None,\n        max_sentences=None,\n        max_positions=None,\n        ignore_invalid_inputs=False,\n        required_batch_size_multiple=1,\n        seed=1,\n        num_shards=1,\n        shard_id=0,\n        num_workers=0,\n        epoch=1,\n        data_buffer_size=0,\n        disable_iterator_cache=False,\n        batch_by_size=True,\n        skip_remainder_batch=True,\n    ):\n        \"\"\"\n        Get an iterator that yields batches of data from the given dataset.\n\n        Args:\n            dataset (~metaseq.data.BaseDataset): dataset to batch\n            max_tokens (int, optional): max number of tokens in each batch\n                (default: None).\n            max_sentences (int, optional): max number of sentences in each\n                batch (default: None).\n            max_positions (optional): max sentence length supported by the\n                model (default: None).\n            ignore_invalid_inputs (bool, optional): don't raise Exception for\n                sentences that are too long (default: False).\n            required_batch_size_multiple (int, optional): require batch size to\n                be a multiple of N (default: 1).\n            seed (int, optional): seed for random number generator for\n                reproducibility (default: 1).\n            num_shards (int, optional): shard the data iterator into N\n                shards (default: 1).\n            shard_id (int, optional): which shard of the data iterator to\n                return (default: 0).\n            num_workers (int, optional): how many subprocesses to use for data\n                loading. 0 means the data will be loaded in the main process\n                (default: 0).\n            epoch (int, optional): the epoch to start the iterator from\n                (default: 1).\n            data_buffer_size (int, optional): number of batches to\n                preload (default: 0).\n            disable_iterator_cache (bool, optional): don't cache the\n                EpochBatchIterator\n                (default: False).\n            batch_by_size (bool, optional):\n                batch sequences of similar length together to reduce padding.\n                If false, each batch will be of size max_sentences.\n            skip_remainder_batch (bool, optional): if set, discard the last\n                batch in each training epoch, as the last batch is often smaller than\n                    local_batch_size * distributed_word_size (default: ``True``).\n        Returns:\n            ~metaseq.iterators.EpochBatchIterator: a batched iterator over the\n                given dataset split\n        \"\"\"\n        if not disable_iterator_cache and dataset in self.dataset_to_epoch_iter:\n            logger.debug(\"reusing EpochBatchIterator for epoch {}\".format(epoch))\n            return self.dataset_to_epoch_iter[dataset]\n\n        assert isinstance(dataset, BaseDataset)\n\n        # initialize the dataset with the correct starting epoch\n        dataset.set_epoch(epoch)\n\n        # get indices ordered by example size\n        with data_utils.numpy_seed(seed):\n            indices = dataset.ordered_indices()\n\n        # filter examples that are too large\n        if max_positions is not None:\n            indices = self.filter_indices_by_size(\n                indices, dataset, max_positions, ignore_invalid_inputs\n            )\n\n        if batch_by_size:\n            # create mini-batches with given size constraints\n            batch_sampler = dataset.batch_by_size(\n                indices,\n                max_tokens=max_tokens,\n                max_sentences=max_sentences,\n                required_batch_size_multiple=required_batch_size_multiple,\n            )\n        else:\n            assert (\n                max_sentences is not None\n            ), \"If batch_by_size=False, max_sentences must be passed. Got None\"\n            starts = indices[::max_sentences]\n            batch_sampler = [indices[s : s + max_sentences] for s in starts]\n        # return a reusable, sharded iterator\n        epoch_iter = iterators.EpochBatchIterator(\n            dataset=dataset,\n            collate_fn=dataset.collater,\n            batch_sampler=batch_sampler,\n            seed=seed,\n            num_shards=num_shards,\n            shard_id=shard_id,\n            num_workers=num_workers,\n            epoch=epoch,\n            buffer_size=data_buffer_size,\n            skip_remainder_batch=skip_remainder_batch,\n        )\n        self.dataset_to_epoch_iter[dataset] = epoch_iter\n        return epoch_iter\n\n    def build_model(self, cfg: MetaseqDataclass):\n        \"\"\"\n        Build the :class:`~metaseq.models.BaseModel` instance for this\n        task.\n\n        Args:\n            cfg (MetaseqDataclass): configuration object\n\n        Returns:\n            a :class:`~metaseq.models.BaseModel` instance\n        \"\"\"\n        from metaseq import models\n\n        model = models.build_model(cfg, self)\n        return model\n\n    def build_criterion(self, cfg: DictConfig):\n        \"\"\"\n        Build the :class:`~metaseq.criterions.BaseCriterion` instance for\n        this task.\n\n        Args:\n            cfg (omegaconf.DictConfig): configration object\n\n        Returns:\n            a :class:`~metaseq.criterions.BaseCriterion` instance\n        \"\"\"\n        from metaseq import criterions\n\n        return criterions.build_criterion(cfg, self)\n\n    def build_generator(\n        self, models, args, seq_gen_cls=None, extra_gen_cls_kwargs=None\n    ):\n        from metaseq.sequence_generator import SequenceGenerator\n\n        # Choose search strategy.\n        sampling = getattr(args, \"sampling\", False)\n        sampling_topp = getattr(args, \"sampling_topp\", -1.0)\n        assert sampling_topp < 0 or sampling, \"--sampling-topp requires --sampling\"\n\n        if getattr(args, \"sampling_topk\", False):\n            logger.warning(\n                \"sampling with topk is not supported, ignoring the sampling_topk argument\"\n            )\n\n        extra_gen_cls_kwargs = extra_gen_cls_kwargs or {}\n        if seq_gen_cls is None:\n            seq_gen_cls = SequenceGenerator\n\n        return seq_gen_cls(\n            models,\n            self.target_dictionary,\n            beam_size=getattr(args, \"beam\", 5),\n            max_len_a=getattr(args, \"max_len_a\", 0),\n            max_len_b=getattr(args, \"max_len_b\", 200),\n            min_len=getattr(args, \"min_len\", 1),\n            temperature=getattr(args, \"temperature\", 1.0),\n            topp=sampling_topp,\n            **extra_gen_cls_kwargs,\n        )\n\n    def train_step(\n        self, sample, model, criterion, optimizer, update_num, ignore_grad=False\n    ):\n        \"\"\"\n        Do forward and backward, and return the loss as computed by *criterion*\n        for the given *model* and *sample*.\n\n        Args:\n            sample (dict): the mini-batch. The format is defined by the\n                :class:`~metaseq.data.BaseDataset`.\n            model (~metaseq.models.BaseModel): the model\n            criterion (~metaseq.criterions.BaseCriterion): the criterion\n            optimizer (~metaseq.optim.BaseOptimizer): the optimizer\n            update_num (int): the current update\n            ignore_grad (bool): multiply loss by 0 if this is set to True\n\n        Returns:\n            tuple:\n                - the loss\n                - the sample size, which is used as the denominator for the\n                  gradient\n                - logging outputs to display while training\n        \"\"\"\n        model.train()\n        model.set_num_updates(update_num)\n        if update_num == 0:\n            logger.info(\n                \"Starting first forward pass and waiting for dataloader in other ranks\"\n            )\n        # forward\n        loss, sample_size, logging_output = criterion(model, sample)\n        if ignore_grad:\n            loss *= 0\n        if update_num == 0:\n            logger.info(\"Starting backward pass\")\n        # backward\n        optimizer.backward(loss)\n        if update_num == 0:\n            logger.info(\"Finished first backward pass\")\n        return loss, sample_size, logging_output\n\n    def valid_step(self, sample, model, criterion):\n        model.eval()\n        with torch.no_grad():\n            loss, sample_size, logging_output = criterion(model, sample)\n        return loss, sample_size, logging_output\n\n    def optimizer_step(self, optimizer, model, update_num):\n        optimizer.step()\n\n    def build_dataset_for_inference(\n        self, src_tokens: List[torch.Tensor], src_lengths: List[int], **kwargs\n    ) -> torch.utils.data.Dataset:\n        raise NotImplementedError\n\n    def inference_step(self, generator, models, sample, prefix_tokens=None):\n        with torch.no_grad():\n            return generator.generate(models, sample, prefix_tokens=prefix_tokens)\n\n    def begin_epoch(self, epoch, model):\n        \"\"\"Hook function called before the start of each epoch.\"\"\"\n        pass\n\n    def begin_valid_epoch(self, epoch, model):\n        \"\"\"Hook function called before the start of each validation epoch.\"\"\"\n        pass\n\n    def reduce_metrics(self, logging_outputs, criterion):\n        \"\"\"Aggregate logging outputs from data parallel training.\"\"\"\n        if not any(\"ntokens\" in log for log in logging_outputs):\n            warnings.warn(\n                \"ntokens not found in Criterion logging outputs, cannot log wpb or wps\"\n            )\n        else:\n            ntokens = sum(log.get(\"ntokens\", 0) for log in logging_outputs)\n            metrics.log_scalar(\"wpb\", ntokens, priority=180, round=1)\n            metrics.log_speed(\"wps\", ntokens, priority=90, round=1)\n\n        if not any(\"nsentences\" in log for log in logging_outputs):\n            warnings.warn(\n                \"nsentences not found in Criterion logging outputs, cannot log bsz\"\n            )\n        else:\n            nsentences = sum(log.get(\"nsentences\", 0) for log in logging_outputs)\n            metrics.log_scalar(\"bsz\", nsentences, priority=190, round=1)\n\n        if hasattr(criterion, \"unwrapped_module\"):\n            criterion.unwrapped_module.__class__.reduce_metrics(logging_outputs)\n        else:\n            criterion.__class__.reduce_metrics(logging_outputs)\n\n    def state_dict(self):\n        if self.state is not None:\n            return self.state.state_dict\n        return {}\n\n    def load_state_dict(self, state_dict: Dict[str, Any]):\n        if self.state is not None:\n            self.state.merge_state_dict(state_dict)\n\n    def max_positions(self):\n        \"\"\"Return the max input length allowed by the task.\"\"\"\n        return None\n\n    @property\n    def source_dictionary(self):\n        \"\"\"Return the source :class:`~metaseq.data.Dictionary` (if applicable\n        for this task).\"\"\"\n        raise NotImplementedError\n\n    @property\n    def target_dictionary(self):\n        \"\"\"Return the target :class:`~metaseq.data.Dictionary` (if applicable\n        for this task).\"\"\"\n        raise NotImplementedError\n\n    def build_tokenizer(self, args):\n        \"\"\"Build the pre-tokenizer for this task.\"\"\"\n        return encoders.build_tokenizer(args)\n\n    def build_bpe(self, args):\n        \"\"\"Build the tokenizer for this task.\"\"\"\n        return encoders.build_bpe(args)\n\n    def get_interactive_tokens_and_lengths(self, lines, encode_fn):\n        tokens = [\n            self.source_dictionary.encode_line(\n                encode_fn(src_str), add_if_not_exist=False\n            ).long()\n            for src_str in lines\n        ]\n        lengths = [t.numel() for t in tokens]\n        return tokens, lengths\n\n\nclass LegacyTask(BaseTask):\n    def __init__(self, args: Namespace):\n        self.args = args\n        self.datasets = {}\n        self.dataset_to_epoch_iter = {}\n\n    @classmethod\n    def setup_task(cls, args: Namespace, **kwargs):\n        \"\"\"Setup the task (e.g., load dictionaries).\n\n        Args:\n            args (argparse.Namespace): parsed command-line arguments\n        \"\"\"\n        return cls(args, **kwargs)\n\n    def build_model(self, args: Namespace):\n        \"\"\"\n        Build the :class:`~metaseq.models.BaseModel` instance for this\n        task.\n\n        Args:\n            args (argparse.Namespace): parsed command-line arguments\n\n        Returns:\n            a :class:`~metaseq.models.BaseModel` instance\n        \"\"\"\n        from metaseq import models\n\n        model = models.build_model(args, self)\n        return model\n\n    def build_criterion(self, args: Namespace):\n        \"\"\"\n        Build the :class:`~metaseq.criterions.BaseCriterion` instance for\n        this task.\n\n        Args:\n            args (argparse.Namespace): parsed command-line arguments\n\n        Returns:\n            a :class:`~metaseq.criterions.BaseCriterion` instance\n        \"\"\"\n        from metaseq import criterions\n\n        return criterions.build_criterion(args, self)\n",
        "metaseq/tasks/language_modeling.py": "# Copyright (c) Meta Platforms, Inc. and affiliates. All Rights Reserved.\n#\n# This source code is licensed under the MIT license found in the\n# LICENSE file in the root directory of this source tree.\n\nimport logging\nimport os\nfrom dataclasses import dataclass, field\nfrom typing import Optional\n\nimport numpy as np\nfrom omegaconf import II\n\nfrom metaseq import utils\nfrom metaseq.data import (\n    AppendTokenDataset,\n    Dictionary,\n    IdDataset,\n    LMContextWindowDataset,\n    MonolingualDataset,\n    NestedDictionaryDataset,\n    NumelDataset,\n    PadDataset,\n    PrependTokenDataset,\n    StripTokenDataset,\n    SortDataset,\n    TokenBlockDataset,\n    data_utils,\n)\nfrom metaseq.data.indexed_dataset import get_available_dataset_impl\nfrom metaseq.data.shorten_dataset import maybe_shorten_dataset\nfrom metaseq.dataclass import ChoiceEnum, MetaseqDataclass\nfrom metaseq.tasks import LegacyTask, register_task\n\ntry:\n    from tokenizers import ByteLevelBPETokenizer, Tokenizer\n\n    has_hf_tokenizers = True\nexcept ImportError:\n    has_hf_tokenizers = False\n\nSAMPLE_BREAK_MODE_CHOICES = ChoiceEnum([\"none\", \"complete\", \"complete_doc\", \"eos\"])\nSHORTEN_METHOD_CHOICES = ChoiceEnum([\"none\", \"truncate\", \"random_crop\"])\nlogger = logging.getLogger(__name__)\n\n\n@dataclass\nclass LanguageModelingConfig(MetaseqDataclass):\n    data: Optional[str] = field(\n        default=None, metadata={\"help\": \"path to data directory\"}\n    )\n    hf_tokenizer: Optional[str] = field(\n        default=\"\", metadata={\"help\": \"path to a HF tokenizer json file.\"}\n    )\n    # Begin args from StreamingLanguageModelingConfig\n    vocab_filename: Optional[str] = field(\n        default=\"\", metadata={\"help\": \"path to bpe-vocab.json\"}\n    )\n    merges_filename: Optional[str] = field(\n        default=\"\", metadata={\"help\": \"path to bpe-merges.txt\"}\n    )\n    end_of_document_symbol: Optional[str] = field(\n        default=\"</s>\", metadata={\"help\": \"symbol indicating an end-of-document\"}\n    )\n    final_vocab_size: Optional[int] = field(\n        default=None, metadata={\"help\": \"force vocab size to this\"}\n    )  # End of args from StreamingLanguageModelingConfig\n    sample_break_mode: SAMPLE_BREAK_MODE_CHOICES = field(\n        default=\"none\",\n        metadata={\n            \"help\": 'If omitted or \"none\", fills each sample with tokens-per-sample '\n            'tokens. If set to \"complete\", splits samples only at the end '\n            \"of sentence, but may include multiple sentences per sample. \"\n            '\"complete_doc\" is similar but respects doc boundaries. '\n            'If set to \"eos\", includes only one sentence per sample.'\n        },\n    )\n    tokens_per_sample: int = field(\n        default=1024,\n        metadata={\"help\": \"max number of tokens per sample for LM dataset\"},\n    )\n    add_bos_token: bool = field(\n        default=False, metadata={\"help\": \"prepend beginning of sentence token (<s>)\"}\n    )\n    max_source_positions: Optional[int] = field(\n        default=None, metadata={\"help\": \"max number of tokens in the source sequence\"}\n    )\n    max_target_positions: Optional[int] = field(\n        default=None, metadata={\"help\": \"max number of tokens in the target sequence\"}\n    )\n    shorten_method: SHORTEN_METHOD_CHOICES = field(\n        default=\"none\",\n        metadata={\n            \"help\": \"if not none, shorten sequences that exceed --tokens-per-sample\"\n        },\n    )\n    shorten_data_split_list: str = field(\n        default=\"\",\n        metadata={\n            \"help\": \"comma-separated list of dataset splits to apply shortening to, \"\n            'e.g., \"train,valid\" (default: all dataset splits)'\n        },\n    )\n    pad_to_fixed_length: Optional[bool] = field(\n        default=False,\n        metadata={\"help\": \"pad to fixed length\"},\n    )\n    pad_to_fixed_bsz: Optional[bool] = field(\n        default=False,\n        metadata={\"help\": \"boolean to pad to fixed batch size\"},\n    )\n    shuffle_docs: bool = field(\n        default=False,\n        metadata={\n            \"help\": \"Only for sample break mode EOS, first shuffle docs and then create blocks\"\n        },\n    )\n\n    # TODO common vars below add to parent\n    seed: int = II(\"common.seed\")\n    batch_size: Optional[int] = II(\"dataset.batch_size\")\n    batch_size_valid: Optional[int] = II(\"dataset.batch_size_valid\")\n    dataset_impl: Optional[ChoiceEnum(get_available_dataset_impl())] = II(\n        \"dataset.dataset_impl\"\n    )\n    data_buffer_size: int = II(\"dataset.data_buffer_size\")\n    use_plasma_view: bool = II(\"common.use_plasma_view\")\n    plasma_path: str = II(\"common.plasma_path\")\n\n\n# TODO(susanz): Deprecate this task. This pre-date StreamingLanguageModelingTask,\n#  when tokenization happened offline.\n@register_task(\"language_modeling\", dataclass=LanguageModelingConfig)\nclass LanguageModelingTask(LegacyTask):\n    \"\"\"\n    Train a language model.  To be deprecated.\n    \"\"\"\n\n    def __init__(self, args):\n        super().__init__(args)\n\n        if not has_hf_tokenizers:\n            raise ImportError(\"Please install tokenizers with: pip install tokenizers\")\n\n        if args.hf_tokenizer:\n            self.tokenizer = Tokenizer.from_file(args.hf_tokenizer)\n        else:\n            self.tokenizer = ByteLevelBPETokenizer.from_file(\n                args.vocab_filename, args.merges_filename\n            )\n\n        self.eod = self.tokenizer.token_to_id(args.end_of_document_symbol)\n        if self.eod is None:\n            # This will be executed for old models that do not have the args.end_of_document_symbol explicitly set\n            # and do not use <s/> (the default) but <EOS>\n            self.eod = self.tokenizer.token_to_id(\"<EOS>\")\n\n        assert (\n            self.eod is not None\n        ), \"Cannot find end-of-document symbol ({}) in tokenizer\".format(\n            args.end_of_document_symbol\n        )\n\n        # construct a dummy metaseq Dictionary corresponding to the given tokenizer\n        self.dictionary = Dictionary()\n        tok_vocab_size = self.tokenizer.get_vocab_size()\n\n        for id in range(self.dictionary.nspecial, tok_vocab_size):\n            self.dictionary.add_symbol(self.tokenizer.id_to_token(id))\n        final_vocab_size = args.final_vocab_size\n        # final_vocab_size = 51200 for roberta dictionary\n        if final_vocab_size is not None:\n            if final_vocab_size < tok_vocab_size:\n                raise ValueError(\n                    f\"incompatible: {final_vocab_size}, tok_vocab_size: {tok_vocab_size}\"\n                )\n            self.dictionary.pad_to_multiple_(final_vocab_size)\n        else:\n            self.dictionary.pad_to_multiple_(8)\n\n        # confirm that metaseq dictionary and BPE have matching special symbols\n        assert self.dictionary.bos_index == 0\n        assert self.tokenizer.id_to_token(0) in {\"<BOS>\", \"<s>\"}\n        assert self.dictionary.pad_index == 1\n        assert self.tokenizer.id_to_token(1) in {\"<PAD>\", \"<pad>\"}\n        assert self.dictionary.eos_index == 2\n        assert self.tokenizer.id_to_token(2) in {\"<EOS>\", \"</s>\"}\n        assert self.dictionary.unk_index == 3\n        assert self.tokenizer.id_to_token(3) in {\"<UNK>\", \"<unk>\"}\n\n    @classmethod\n    def setup_task(cls, args, **kwargs):\n        return cls(args)\n\n    def load_dataset(self, split: str, epoch=1, combine=False, **kwargs):\n        \"\"\"Load a given dataset split.\n\n        Args:\n            split (str): name of the split (e.g., train, valid, valid1, test)\n        \"\"\"\n        paths = utils.split_paths(self.args.data)\n        assert len(paths) > 0\n\n        data_path = paths[(epoch - 1) % len(paths)]\n        split_path = os.path.join(data_path, split)\n\n        # each process has its own copy of the raw data (likely to be an np.memmap)\n        dataset = data_utils.load_indexed_dataset(\n            split_path, self.dictionary, self.args.dataset_impl, combine=combine\n        )\n        if dataset is None:\n            raise FileNotFoundError(f\"Dataset not found: {split} ({split_path})\")\n\n        dataset = maybe_shorten_dataset(\n            dataset,\n            split,\n            self.args.shorten_data_split_list,\n            self.args.shorten_method,\n            self.args.tokens_per_sample,\n            self.args.seed,\n        )\n\n        if self.args.shuffle_docs:\n            assert (\n                self.args.sample_break_mode == \"none\"\n            ), \"shuffle docs is only for sample break mode none\"\n            dataset = TokenBlockDataset(\n                dataset,\n                dataset.sizes,\n                self.args.tokens_per_sample,\n                pad=self.dictionary.pad(),\n                eos=self.dictionary.eos(),\n                break_mode=\"complete_doc\",\n                include_targets=False,\n                use_plasma_view=self.args.use_plasma_view,\n                split_path=split_path,\n                plasma_path=self.args.plasma_path,\n            )\n            with data_utils.numpy_seed(self.args.seed + epoch):\n                shuffle = np.random.permutation(len(dataset))\n\n            dataset = SortDataset(\n                dataset,\n                sort_order=[\n                    shuffle,\n                    dataset.sizes,\n                ],\n            )\n        dataset = TokenBlockDataset(\n            dataset,\n            dataset.sizes,\n            self.args.tokens_per_sample,\n            pad=self.dictionary.pad(),\n            eos=self.dictionary.eos(),\n            break_mode=self.args.sample_break_mode,\n            include_targets=True,\n            use_plasma_view=self.args.use_plasma_view,\n            split_path=split_path,\n            plasma_path=self.args.plasma_path,\n        )\n        add_eos_for_other_targets = (\n            self.args.sample_break_mode is not None\n            and self.args.sample_break_mode != \"none\"\n        )\n        fixed_pad_length = None\n        if self.args.pad_to_fixed_length:\n            fixed_pad_length = self.args.tokens_per_sample\n\n        pad_to_bsz = None\n        if self.args.pad_to_fixed_bsz:\n            pad_to_bsz = (\n                self.args.batch_size_valid if \"valid\" in split else self.args.batch_size\n            )\n\n        self.datasets[split] = MonolingualDataset(\n            dataset=dataset,\n            sizes=dataset.sizes,\n            src_vocab=self.dictionary,\n            tgt_vocab=self.dictionary,\n            add_eos_for_other_targets=add_eos_for_other_targets,\n            shuffle=True,\n            add_bos_token=self.args.add_bos_token,\n            fixed_pad_length=fixed_pad_length,\n            pad_to_bsz=pad_to_bsz,\n        )\n\n    def build_dataset_for_inference(self, src_tokens, src_lengths, **kwargs):\n        \"\"\"\n        Generate batches for inference. We prepend an eos token to src_tokens\n        (or bos if `--add-bos-token` is set) and we append a <pad> to target.\n        This is convenient both for generation with a prefix and LM scoring.\n        \"\"\"\n        dataset = StripTokenDataset(\n            TokenBlockDataset(\n                src_tokens,\n                src_lengths,\n                block_size=None,  # ignored for \"eos\" break mode\n                pad=self.source_dictionary.pad(),\n                eos=self.source_dictionary.eos(),\n                break_mode=\"eos\",\n            ),\n            # remove eos from (end of) target sequence\n            self.source_dictionary.eos(),\n        )\n        src_dataset = PrependTokenDataset(\n            dataset,\n            token=(\n                self.source_dictionary.bos()\n                if getattr(self.args, \"add_bos_token\", False)\n                else self.source_dictionary.eos()\n            ),\n        )\n        tgt_dataset = AppendTokenDataset(dataset, token=self.source_dictionary.pad())\n        return NestedDictionaryDataset(\n            {\n                \"id\": IdDataset(),\n                \"net_input\": {\n                    \"src_tokens\": PadDataset(\n                        src_dataset,\n                        pad_idx=self.source_dictionary.pad(),\n                        left_pad=False,\n                    ),\n                    \"src_lengths\": NumelDataset(src_dataset, reduce=False),\n                },\n                \"target\": PadDataset(\n                    tgt_dataset, pad_idx=self.source_dictionary.pad(), left_pad=False\n                ),\n            },\n            sizes=[np.array(src_lengths)],\n        )\n\n    def eval_lm_dataloader(\n        self,\n        dataset,\n        max_tokens: Optional[int] = 36000,\n        batch_size: Optional[int] = None,\n        max_positions: Optional[int] = None,\n        num_shards: int = 1,\n        shard_id: int = 0,\n        num_workers: int = 1,\n        data_buffer_size: int = 10,\n        # ensures that every evaluated token has access to a context of at least\n        # this size, if possible\n        context_window: int = 0,\n    ):\n        if context_window > 0:\n            dataset = LMContextWindowDataset(\n                dataset=dataset,\n                tokens_per_sample=self.args.tokens_per_sample,\n                context_window=context_window,\n                pad_idx=self.source_dictionary.pad(),\n            )\n        return self.get_batch_iterator(\n            dataset=dataset,\n            max_tokens=max_tokens,\n            max_sentences=batch_size,\n            max_positions=max_positions,\n            ignore_invalid_inputs=True,\n            num_shards=num_shards,\n            shard_id=shard_id,\n            num_workers=num_workers,\n            data_buffer_size=data_buffer_size,\n        )\n\n    @property\n    def source_dictionary(self):\n        return self.dictionary\n\n    @property\n    def target_dictionary(self):\n        return self.dictionary\n",
        "metaseq/tasks/streaming_finetune_language_modeling.py": "# Copyright (c) Meta Platforms, Inc. and affiliates. All Rights Reserved.\n#\n# This source code is licensed under the MIT license found in the\n# LICENSE file in the root directory of this source tree.\n\"\"\"\nStreaming Language Modeling task that loads corpora in src-tgt format and performs\non-the-fly tokenization.\n\"\"\"\n\nimport logging\nimport os\nfrom typing import Any, Dict, List\n\nimport torch\n\nfrom metaseq.data import (\n    JsonlDataset,\n    data_utils,\n)\nfrom metaseq.tasks.streaming_language_modeling import (\n    StreamingLanguageModelingTask,\n    StreamingLanguageModelingConfig,\n)\nfrom metaseq.tasks.streaming_language_modeling import DocumentToSequenceDataset\n\nfrom metaseq.tasks import register_task\n\nlogger = logging.getLogger(__name__)\n\n\n@register_task(\n    \"streaming_finetune_language_modeling\", dataclass=StreamingLanguageModelingConfig\n)\nclass StreamingFinetuneLanguageModelingTask(StreamingLanguageModelingTask):\n    def _tokenize_src_tgt_json(self, json):\n        src = json[\"src\"].rstrip(\" \")\n        tgt = json[\"tgt\"].rstrip()\n        full_tokens = torch.LongTensor(\n            self.tokenizer.encode(\" \".join([src, tgt])).ids + [self.eod]\n        )\n        src_tokens_len = len(self.tokenizer.encode(src).ids)\n        tgt_tokens = torch.clone(full_tokens)\n        tgt_tokens[:src_tokens_len] = self.dictionary.pad_index\n        return (full_tokens, tgt_tokens)\n\n    def load_dataset(self, split: str, epoch=1, combine=False, **kwargs):\n        \"\"\"Load a given dataset split.\n\n        The folder structure is assumed to look like:\n\n            /path/to/data/train/00/foo.jsonl\n            /path/to/data/train/00/bar.jsonl\n            /path/to/data/train/01/foo.jsonl\n            /path/to/data/train/01/bar.jsonl\n            /path/to/data/valid/00/foo.jsonl\n            /path/to/data/valid/00/bar.jsonl\n\n        In this example, we have two \"shards\" of training data, which will be\n        iterated over in epochs 1 and 2, respectively. Subsequent epochs will\n        cycle back over the same data. We also have two different data sources\n        in each shard (foo and bar), which will be combined and shuffled.\n\n        Each jsonl entry is a dict with \"src\" and \"tgt\" keys. Loss is computed\n        only on the tgt tokens.\n\n        Args:\n            split (str): name of the split (e.g., train, valid, valid1, test)\n        \"\"\"\n        # This function reads a bunch of jsonl files, concats them together,\n        # shuffles them, then chunks them into blocks of tokens (e.g., 2048).\n\n        # determine number of shards for this split shards = {}\n        cur_shard_str = self.get_shard_str(epoch, split)\n\n        # concatenate any jsonl files that are part of the shard\n        datasets, corpora = [], []\n        for file in sorted(\n            os.listdir(os.path.join(self.args.data, split, cur_shard_str))\n        ):\n            if not file.endswith(\".jsonl\"):\n                continue\n            datasets.append(\n                JsonlDataset(\n                    path=os.path.join(self.args.data, split, cur_shard_str, file),\n                    tokenizer=self._tokenize_src_tgt_json,\n                )\n            )\n            corpora.append(os.path.splitext(file)[0])\n        assert len(datasets) > 0\n\n        if (\n            self.args.multicorpus_sampling_alpha != 1\n            or self.args.multicorpus_sampling_maximum > 0\n        ):\n            datasets = self._alpha_sampling(datasets, corpora, epoch)\n\n        dataset = torch.utils.data.ConcatDataset(datasets)\n\n        self.datasets[split] = DocumentToSequenceDataset(\n            dataset,\n            # We generate blocks with one extra token, so that we have a target\n            # for the final input token. This results in slight data loss.\n            block_size=self.args.tokens_per_sample + 1,\n            break_mode=self.args.sample_break_mode,\n            # we drop the remainder block during training\n            drop_last=(split == \"train\"),\n            padding_idx=self.source_dictionary.pad(),\n            seed=self.args.seed,\n            source_target=True,\n        )\n\n    def _collate_fn(self, items: List[Dict[str, Any]]):\n        # StreamingTokenBlockDataset returns None as filler\n        if len([x for x in items if x is not None]) == 0:\n            return {}\n\n        src_tokens = data_utils.collate_tokens(\n            [x[\"src_block\"] for x in items if x is not None],\n            pad_idx=self.source_dictionary.pad(),\n            pad_to_bsz=self.args.batch_size,\n        )\n        tgt_tokens = data_utils.collate_tokens(\n            [x[\"tgt_block\"] for x in items if x is not None],\n            pad_idx=self.source_dictionary.pad(),\n            pad_to_bsz=self.args.batch_size,\n        )\n\n        # generate inputs and targets\n        input = src_tokens[:, :-1].contiguous()\n        target = tgt_tokens[:, 1:].contiguous()\n\n        ids = torch.cat([x[\"ids\"] for x in items if x is not None])\n        if ids.numel() != torch.unique(ids).numel():\n            n_duplicate = ids.numel() - torch.unique(ids).numel()\n            logger.error(\n                f\"found {n_duplicate}/{ids.numel()} duplicate document IDs in the same batch!\"\n            )\n\n        # metaseq expects batches to have the following structure\n        return {\n            \"id\": ids,\n            \"net_input\": {\n                \"src_tokens\": input,\n            },\n            \"target\": target,\n            \"nsentences\": input.size(0),\n            \"ntokens\": input.ne(self.dictionary.pad()).sum(),\n            \"ntokens_target\": target.ne(self.dictionary.pad()).sum(),\n        }\n",
        "metaseq/tasks/streaming_language_modeling.py": "# Copyright (c) Meta Platforms, Inc. and affiliates. All Rights Reserved.\n#\n# This source code is licensed under the MIT license found in the\n# LICENSE file in the root directory of this source tree.\n\"\"\"\nStreaming Language Modeling task that loads corpora in plaintext and performs\non-the-fly tokenization.\n\"\"\"\n\nimport logging\nimport os\nfrom dataclasses import dataclass, field\nfrom typing import Any, Dict, List, Optional\n\nimport numpy as np\nimport torch\nfrom omegaconf import II\n\nfrom metaseq.data import (\n    Dictionary,\n    JsonlDataset,\n    PartitionedStreamingDataset,\n    ResamplingDataset,\n    StreamingSrcTgtDataset,\n    data_utils,\n    iterators,\n)\n\nfrom metaseq.dataclass import MetaseqDataclass\nfrom metaseq.tasks import LegacyTask, register_task\nfrom metaseq.data.document_to_sequence import DocumentToSequenceDataset\nfrom metaseq.data.cm3_dataset import CausalMaskedDocumentToSequenceDataset\nfrom metaseq.dataclass import ChoiceEnum\n\ntry:\n    from tokenizers import ByteLevelBPETokenizer, Tokenizer\n\n    has_hf_tokenizers = True\nexcept ImportError:\n    has_hf_tokenizers = False\n\n\nlogger = logging.getLogger(__name__)\n\nDEFAULT_MULTICORPUS_MAX = -1\n\nLANGUAGE_MODELING_MODE = ChoiceEnum([\"standard\", \"cm3\"])\nCM3_MODE = ChoiceEnum([\"poisson\", \"fixed\", \"fim\"])\n\n\n@dataclass\nclass StreamingLanguageModelingConfig(MetaseqDataclass):\n    data: Optional[str] = field(\n        default=None, metadata={\"help\": \"path to data directory with JSONL files\"}\n    )\n    hf_tokenizer: Optional[str] = field(\n        default=\"\", metadata={\"help\": \"path to a HF tokenizer json file.\"}\n    )\n    vocab_filename: Optional[str] = field(\n        default=\"\", metadata={\"help\": \"path to bpe-vocab.json\"}\n    )\n    merges_filename: Optional[str] = field(\n        default=\"\", metadata={\"help\": \"path to bpe-merges.txt\"}\n    )\n    end_of_document_symbol: Optional[str] = field(\n        default=\"</s>\", metadata={\"help\": \"symbol indicating an end-of-document\"}\n    )\n    sample_break_mode: Optional[str] = field(\n        default=\"none\",\n        metadata={\n            \"help\": 'If omitted or \"none\", fills each sample with tokens-per-sample '\n            'tokens. If set to \"complete\", splits samples only at the end '\n            \"of sentence, but may include multiple sentences per sample. \"\n            '\"complete_doc\" is similar but respects doc boundaries. '\n            'If set to \"eos\", includes only one sentence per sample.'\n        },\n    )\n    tokens_per_sample: int = field(\n        default=1024,\n        metadata={\"help\": \"max number of tokens per sample for LM dataset\"},\n    )\n    max_source_positions: Optional[int] = field(\n        default=None, metadata={\"help\": \"max number of tokens in the source sequence\"}\n    )\n    max_target_positions: Optional[int] = field(\n        default=None, metadata={\"help\": \"max number of tokens in the target sequence\"}\n    )\n    final_vocab_size: Optional[int] = field(\n        default=None, metadata={\"help\": \"force vocab size to this\"}\n    )\n    multicorpus_sampling_alpha: Optional[float] = field(\n        default=1.0,\n        metadata={\n            \"help\": \"smoothing alpha for sample rations across multiple datasets\"\n        },\n    )\n    multicorpus_sampling_maximum: Optional[float] = field(\n        default=DEFAULT_MULTICORPUS_MAX,\n        metadata={\"help\": \"Maximum size for example proportional sampling\"},\n    )\n    data_subshard_count: int = field(\n        default=1,\n        metadata={\n            \"help\": \"Number of data subshards to use while training.\"\n            \"Subsharding allows us to virtually split the dataset to speed up dataset fast forwarding.\"\n        },\n    )\n    # language modeling type\n    language_modeling_type: LANGUAGE_MODELING_MODE = field(\n        default=\"standard\",\n        metadata={\n            \"help\": \"Number of data subshards to use while training.\"\n            \"Subsharding allows us to virtually split the dataset to speed up dataset fast forwarding.\"\n        },\n    )\n    # CM3 Specific Parameters\n    cm3_num_sentinel_tokens: int = field(\n        default=512,\n        metadata={\"help\": \"Number of special sentinel tokens to add to the vocabulary\"},\n    )\n    cm3_lambda_sentinel_tokens: int = field(\n        default=1,\n        metadata={\n            \"help\": \"if CM3_MODE is `poisson` then the Poisson Lambda for the cm3 objective.\"\n            \"if CM3_MODE is `fixed` then represents the number of fixed masks per example to use.\"\n            \"if CM3_MODE is `fim` then will be forced to be 1.\"\n        },\n    )\n    cm3_mode: CM3_MODE = field(\n        default=\"poisson\",\n        metadata={\n            \"help\": \"The type of infilling objective to do; poisson (original CM3),\"\n            \"fixed (CM3 with fixed number of masks), fim (CM3 with 1 mask).\"\n        },\n    )\n    cm3_allow_across_eod_boundaries: bool = field(\n        default=True,\n        metadata={\n            \"help\": \"Whether or not we allow rotation of documents across documents\"\n            \"(especially when training with token blocking set to None).\"\n            \"By default the original CM3 objective allows rotation across document boundaries.\"\n            \"For FIM it's unclear whether or not they allow this.\"\n        },\n    )\n    # TODO common vars below add to parent\n    seed: int = II(\"common.seed\")\n    batch_size: Optional[int] = II(\"dataset.batch_size\")\n    batch_size_valid: Optional[int] = II(\"dataset.batch_size_valid\")\n    data_buffer_size: int = II(\"dataset.data_buffer_size\")\n    update_freq: List[int] = II(\"optimization.update_freq\")\n\n\n@register_task(\"streaming_language_modeling\", dataclass=StreamingLanguageModelingConfig)\nclass StreamingLanguageModelingTask(LegacyTask):\n    \"\"\"\n    Train a language model on a stream of data. Currently we assume the stream\n    is in JSONL format and we tokenize inputs on-the-fly.\n\n    Note that we append an end-of-document symbol to the end of each document.\n\n    Args:\n        tokenizer (tokenizers.ByteLevelBPETokenizer): the BPE tokenizer to use\n    \"\"\"\n\n    def __init__(self, args):\n        super().__init__(args)\n\n        if not has_hf_tokenizers:\n            raise ImportError(\"Please install tokenizers with: pip install tokenizers\")\n\n        if args.hf_tokenizer:\n            self.tokenizer = Tokenizer.from_file(args.hf_tokenizer)\n        else:\n            self.tokenizer = ByteLevelBPETokenizer.from_file(\n                args.vocab_filename, args.merges_filename\n            )\n\n        if max(args.update_freq) > 1:\n            raise NotImplementedError(\n                \"--update-freq is not compatible with StreamingLanguageModelingTask\"\n            )\n\n        self.eod = self.tokenizer.token_to_id(args.end_of_document_symbol)\n        if self.eod is None:\n            # This will be executed for old models that do not have the args.end_of_document_symbol explicitly set\n            # and do not use <s/> (the default) but <EOS>\n            self.eod = self.tokenizer.token_to_id(\"<EOS>\")\n\n        assert (\n            self.eod is not None\n        ), \"Cannot find end-of-document symbol ({}) in tokenizer\".format(\n            args.end_of_document_symbol\n        )\n\n        # construct a dummy metaseq Dictionary corresponding to the given tokenizer\n        self.dictionary = Dictionary()\n        tok_vocab_size = self.tokenizer.get_vocab_size()\n\n        for id in range(self.dictionary.nspecial, tok_vocab_size):\n            self.dictionary.add_symbol(self.tokenizer.id_to_token(id))\n\n        # confirm that metaseq dictionary and BPE have matching special symbols\n        assert self.dictionary.bos_index == 0\n        assert self.tokenizer.id_to_token(0) in {\"<BOS>\", \"<s>\"}\n        assert self.dictionary.pad_index == 1\n        assert self.tokenizer.id_to_token(1) in {\"<PAD>\", \"<pad>\"}\n        assert self.dictionary.eos_index == 2\n        assert self.tokenizer.id_to_token(2) in {\"<EOS>\", \"</s>\"}\n        assert self.dictionary.unk_index == 3\n        assert self.tokenizer.id_to_token(3) in {\"<UNK>\", \"<unk>\"}\n\n        self.has_cm3 = args.language_modeling_type == \"cm3\"\n        if self.has_cm3:\n            self.cm3_sentinel_type = self.args.cm3_mode\n            self._check_cm3_parameterization()\n            self._create_cm3_special_tokens()\n\n        final_vocab_size = args.final_vocab_size\n        if final_vocab_size is not None:\n            if final_vocab_size < tok_vocab_size:\n                raise ValueError(\n                    f\"incompatible: {final_vocab_size}, tok_vocab_size: {tok_vocab_size}\"\n                )\n            self.dictionary.pad_to_multiple_(final_vocab_size)\n        else:\n            self.dictionary.pad_to_multiple_(8)\n\n    def _check_cm3_parameterization(self):\n        assert (\n            self.args.cm3_lambda_sentinel_tokens > 0\n        ), \"cm3_lambda_sentinel_tokens must be > 0\"\n        assert (\n            self.args.cm3_num_sentinel_tokens > 0\n        ), \"cm3_num_sentinel_tokens must be > 0\"\n        assert (\n            self.args.cm3_num_sentinel_tokens >= self.args.cm3_lambda_sentinel_tokens\n        ), \"cm3_lambda_sentinel_tokens must be >= cm3_num_sentinel_tokens\"\n        if self.args.cm3_mode == \"fim\":\n            assert (\n                self.args.cm3_num_sentinel_tokens == 1\n            ), \"FIM requires cm3_num_sentinel_tokens to be 1\"\n            assert (\n                self.args.cm3_lambda_sentinel_tokens == 1\n            ), \"FIM requires cm3_lambda_sentinel_tokens to be 1\"\n            self.cm3_sentinel_type = \"fixed\"\n\n    def _create_cm3_special_tokens(self):\n        self.cm3_sentinel_end = \"<eoss>\"\n        self.cm3_sentinel_tokens = [\n            f\"<sentinel:{i}>\" for i in range(self.args.cm3_num_sentinel_tokens)\n        ]\n        self.cm3_sentinel_tokens_ind = []\n        for token in self.cm3_sentinel_tokens:\n            self.dictionary.add_symbol(token)\n            token_index = self.dictionary.index(token)\n            assert token_index != self.dictionary.unk_index\n            self.cm3_sentinel_tokens_ind.append(token_index)\n        self.cm3_sentinel_end_ind = self.dictionary.index(self.cm3_sentinel_end)\n\n    @classmethod\n    def setup_task(cls, args, **kwargs):\n        return cls(args)\n\n    def _tokenize_one_json(self, json):\n        text = json[\"text\"]\n        return torch.LongTensor(\n            # append an end-of-document symbol after each document\n            self.tokenizer.encode(text.rstrip()).ids\n            + [self.eod]\n        )\n\n    def _get_sample_prob(self, dataset_lens):\n        \"\"\"\n        Get smoothed sampling porbability by corpus. This helps small corpus by upsampling them.\n        \"\"\"\n        if self.args.multicorpus_sampling_maximum == DEFAULT_MULTICORPUS_MAX:\n            prob = dataset_lens / dataset_lens.sum()\n            smoothed_prob = prob**self.args.multicorpus_sampling_alpha\n            smoothed_prob = smoothed_prob / smoothed_prob.sum()\n        else:\n            dataset_lens = np.array(\n                [min(l, self.args.multicorpus_sampling_maximum) for l in dataset_lens]\n            )\n            smoothed_prob = dataset_lens / sum(dataset_lens)\n        return smoothed_prob\n\n    def _alpha_sampling(self, datasets, corpora, epoch=1):\n        \"\"\"\n        Up or down sample corpora with alpha sampling.\n        \"\"\"\n        dataset_lengths = np.array(\n            [len(d) for d in datasets],\n            dtype=float,\n        )\n        logger.info(f\"loaded total {dataset_lengths.sum()} blocks for all corpora\")\n        sample_probs = self._get_sample_prob(dataset_lengths)\n\n        logger.info(\n            \"Sample probability by corpus: %s\",\n            {\n                corpus: \"{0:.4f}\".format(sample_probs[id])\n                for id, corpus in enumerate(corpora)\n            },\n        )\n        size_ratio = (sample_probs * dataset_lengths.sum()) / dataset_lengths\n        # TODO: add an option for shrinking all size ratios to below 1\n        # if self.args.multicorpus_sampling_alpha != 1:\n        #   size_ratio /= size_ratio.max()\n\n        # Fix numeric errors in size ratio computation\n        #   0.999999999999999999 -> 1\n        #   1.000000000000000002 -> 1\n        for i in range(len(size_ratio)):\n            size_ratio[i] = round(size_ratio[i], 8)\n\n        logger.info(\n            \"Up/Down Sampling ratio by corpus: %s\",\n            {\n                corpus: \"{0:.2f}\".format(size_ratio[id])\n                for id, corpus in enumerate(corpora)\n            },\n        )\n        logger.info(\n            \"Actual dataset size by corpus: %s\",\n            {\n                corpus: \"{0:.2f}\".format(len(datasets[id]))\n                for id, corpus in enumerate(corpora)\n            },\n        )\n        resampled_datasets = [\n            ResamplingDataset(\n                datasets[i],\n                size_ratio=size_ratio[i],\n                seed=self.args.seed,\n                epoch=epoch,\n                replace=size_ratio[i] > 1.0,\n            )\n            for i, d in enumerate(datasets)\n        ]\n        # TODO: estimate the actual steps or tokens seen in training before launching experiments.\n        logger.info(\n            \"Resampled dataset size by corpus: %s\",\n            {\n                corpus: \"{0:.2f}\".format(len(resampled_datasets[id]))\n                for id, corpus in enumerate(corpora)\n            },\n        )\n        return resampled_datasets\n\n    def get_shard_str(self, epoch, split):\n        shards = {}\n        for shard_id in os.listdir(os.path.join(self.args.data, split)):\n            assert (\n                int(shard_id) not in shards\n            ), f\"shard id: {shard_id} not in shards: {shards}\"\n            shards[int(shard_id)] = shard_id\n        assert min(shards.keys()) == 0\n        assert max(shards.keys()) == len(shards) - 1\n\n        data_subshard_count = self.args.data_subshard_count if split == \"train\" else 1\n\n        shard_idx = ((epoch - 1) // data_subshard_count) % len(shards)\n        cur_shard_str = shards[shard_idx]\n        return cur_shard_str\n\n    def load_dataset(self, split: str, epoch=1, combine=False, **kwargs):\n        \"\"\"Load a given dataset split.\n\n        The folder structure is assumed to look like:\n\n            /path/to/data/train/00/foo.jsonl\n            /path/to/data/train/00/bar.jsonl\n            /path/to/data/train/01/foo.jsonl\n            /path/to/data/train/01/bar.jsonl\n            /path/to/data/valid/00/foo.jsonl\n            /path/to/data/valid/00/bar.jsonl\n\n        In this example, we have two \"shards\" of training data, which will be\n        iterated over in epochs 1 and 2, respectively. Subsequent epochs will\n        cycle back over the same data. We also have two different data sources\n        in each shard (foo and bar), which will be combined and shuffled.\n\n        Args:\n            split (str): name of the split (e.g., train, valid, valid1, test)\n        \"\"\"\n        # This function reads a bunch of jsonl files, concats them together,\n        # shuffles them, then chunks them into blocks of tokens (e.g., 2048).\n\n        # determine number of shards for this split\n        cur_shard_str = self.get_shard_str(epoch, split)\n\n        # concatenate any jsonl files that are part of the shard\n        datasets, corpora = [], []\n        data_subshard_count = self.args.data_subshard_count if split == \"train\" else 1\n        for file in sorted(\n            os.listdir(os.path.join(self.args.data, split, cur_shard_str))\n        ):\n            if not file.endswith(\".jsonl\"):\n                continue\n            datasets.append(\n                JsonlDataset(\n                    path=os.path.join(self.args.data, split, cur_shard_str, file),\n                    tokenizer=self._tokenize_one_json,\n                    epoch=epoch,\n                    data_subshard_count=data_subshard_count,\n                )\n            )\n            corpora.append(os.path.splitext(file)[0])\n        assert len(datasets) > 0\n\n        if self.args.multicorpus_sampling_alpha != 1:\n            datasets = self._alpha_sampling(datasets, corpora, epoch)\n\n        dataset = torch.utils.data.ConcatDataset(datasets)\n\n        # chunk into blocks of tokens\n        if self.has_cm3:\n            # We chose not to use compositional inheritance because there's a\n            # lot of downstream code that has isinstance checks.\n            # So just to be safe and not change anything we use proper inheritance.\n            self.datasets[split] = CausalMaskedDocumentToSequenceDataset(\n                sentinel_token_expectation=self.args.cm3_lambda_sentinel_tokens,\n                sentinel_tokens=self.cm3_sentinel_tokens_ind,\n                sentinel_method=self.cm3_sentinel_type,\n                sentinel_eos=self.cm3_sentinel_end_ind,\n                allow_rotation_across_eod=self.args.cm3_allow_across_eod_boundaries,\n                eod=self.eod,\n                dataset=dataset,\n                # We generate blocks with one extra token, so that we have a target\n                # for the final input token. This results in slight data loss.\n                block_size=self.args.tokens_per_sample + 1,\n                break_mode=self.args.sample_break_mode,\n                # we drop the remainder block during training\n                drop_last=(split == \"train\"),\n                padding_idx=self.source_dictionary.pad(),\n                seed=self.args.seed,\n            )\n        else:\n            self.datasets[split] = DocumentToSequenceDataset(\n                dataset,\n                block_size=self.args.tokens_per_sample + 1,\n                break_mode=self.args.sample_break_mode,\n                drop_last=(split == \"train\"),\n                padding_idx=self.source_dictionary.pad(),\n                seed=self.args.seed,\n            )\n\n    def _collate_fn(self, items: List[Dict[str, Any]]):\n        # StreamingTokenBlockDataset returns None as filler\n        if len([x for x in items if x is not None]) == 0:\n            return {}\n\n        tokens = data_utils.collate_tokens(\n            [x[\"block\"] for x in items if x is not None],\n            pad_idx=self.source_dictionary.pad(),\n            pad_to_bsz=self.args.batch_size,\n        )\n        # generate inputs and targets\n        input = tokens[:, :-1].contiguous()\n        target = tokens[:, 1:].contiguous()\n\n        ids = torch.cat([x[\"ids\"] for x in items if x is not None])\n        if ids.numel() != torch.unique(ids).numel():\n            n_duplicate = ids.numel() - torch.unique(ids).numel()\n            logger.error(\n                f\"found {n_duplicate}/{ids.numel()} duplicate document IDs in the same batch!\"\n            )\n\n        # metaseq expects batches to have the following structure\n        return {\n            \"id\": ids,\n            \"net_input\": {\n                \"src_tokens\": input,\n            },\n            \"target\": target,\n            \"nsentences\": input.size(0),\n            \"ntokens\": input.ne(self.dictionary.pad()).sum(),\n        }\n\n    def dataset(self, split):\n        return self.datasets[split]\n\n    def get_batch_iterator(\n        self,\n        dataset,\n        max_tokens=None,\n        max_sentences=None,\n        max_positions=None,\n        ignore_invalid_inputs=False,\n        required_batch_size_multiple=1,\n        seed=1,\n        num_shards=1,\n        shard_id=0,\n        num_workers=0,\n        epoch=1,\n        data_buffer_size=0,\n        disable_iterator_cache=False,\n        batch_by_size=True,\n        skip_remainder_batch=True,\n    ):\n        \"\"\"\n        Get an iterator that yields batches of data from the given dataset.\n\n        Args:\n            dataset (torch.utils.data.Dataset): dataset to batch\n            max_sentences (int, optional): max number of sentences in each\n                batch (default: None).\n            num_shards (int, optional): shard the data iterator into N\n                shards (default: 1).\n            shard_id (int, optional): which shard of the data iterator to\n                return (default: 0).\n            num_workers (int, optional): how many subprocesses to use for data\n                loading. 0 means the data will be loaded in the main process\n                (default: 0).\n            epoch (int, optional): the epoch to start the iterator from\n                (default: 1).\n            data_buffer_size (int, optional): number of batches to\n                preload (default: 0).\n            disable_iterator_cache (bool, optional): don't cache the\n                EpochBatchIterator\n                (default: False).\n            batch_by_size (bool, optional):\n                batch sequences of similar length together to reduce padding.\n                If false, each batch will be of size max_sentences.\n            skip_remainder_batch (bool, optional): if set, discard the last\n                batch in each training epoch, as the last batch is often smaller\n                than local_batch_size * distributed_word_size (default: ``True``).\n        Returns:\n            ~metaseq.iterators.EpochBatchIterator: a batched iterator over the\n                given dataset split\n        \"\"\"\n        assert max_tokens is None\n\n        # Up to this point, we have shuffled documents, flattened them into a 1D\n        # tensor, then chunked into token blocks. But if documents are long, then\n        # adjacent blocks may be from a single document, and naively distributed\n        # sequential blocks to GPUs may cause entire updates to be dominated by a\n        # handful of unique documents. Instead we have a readahead buffer that\n        # reads in 10 full batches of data and shuffles sequences across them,\n        # thus increasing randomness. This assumes that no single document spans\n        # 10 full batches, which is reasonable when batch sizes are in the\n        # millions and documents are on average much smaller.\n        assert isinstance(dataset, DocumentToSequenceDataset) or isinstance(\n            dataset, StreamingSrcTgtDataset\n        )\n        shuffle_buffer_size = 10 * max_sentences * num_shards\n        logger.info(f\"setting shuffle buffer size to {shuffle_buffer_size}\")\n        dataset.set_shuffle_buffer_size(shuffle_buffer_size)\n        dataset.set_num_workers(num_workers)\n\n        # partition dataset across data parallel workers\n        dataset = PartitionedStreamingDataset(\n            dataset,\n            num_shards=num_shards,\n            shard_id=shard_id,\n            drop_last=skip_remainder_batch,\n        )\n\n        # create a stateful/checkpointable iterator for the current data\n        # parallel worker\n        return iterators.StreamingEpochBatchIterator(\n            dataset=dataset,\n            batch_size=max_sentences,\n            collate_fn=self._collate_fn,\n            drop_last=skip_remainder_batch,\n            num_workers=num_workers,\n            epoch=epoch,\n            num_shards=num_shards,\n        )\n\n    @property\n    def source_dictionary(self):\n        return self.dictionary\n\n    @property\n    def target_dictionary(self):\n        return self.dictionary\n",
        "metaseq/trainer.py": "# Copyright (c) Meta Platforms, Inc. and affiliates. All Rights Reserved.\n#\n# This source code is licensed under the MIT license found in the\n# LICENSE file in the root directory of this source tree.\n\n\"\"\"\nTrain a network across multiple GPUs.\n\"\"\"\n\nimport contextlib\nimport functools\nimport logging\nimport math\nimport os\nimport re\nimport sys\nimport time\nfrom concurrent.futures import ThreadPoolExecutor\nfrom itertools import chain\nfrom typing import Any, Dict, List\n\nimport torch\nimport torch.distributed as dist\nfrom omegaconf import OmegaConf\n\nfrom metaseq import checkpoint_utils, models, optim, utils\nfrom metaseq.distributed import utils as distributed_utils, fsdp_enable_wrap, fsdp_wrap\nfrom metaseq.file_io import PathManager\nfrom metaseq.logging import meters, metrics\nfrom metaseq.models.ema import build_ema\nfrom metaseq.modules.megatron.mpu import get_cuda_rng_tracker\nfrom metaseq.nan_detector import NanDetector\nfrom metaseq.optim import lr_scheduler\nfrom metaseq.utils import set_rank_seed\n\nlogger = logging.getLogger(__name__)\n\n\nclass Trainer(object):\n    \"\"\"Main class for data parallel training.\n\n    This class supports synchronous distributed data parallel training,\n    where multiple workers each have a full model replica and gradients\n    are accumulated across workers before each update. We use\n    :class:`~torch.nn.parallel.DistributedDataParallel` to handle\n    communication of the gradients across workers.\n    \"\"\"\n\n    def __init__(self, cfg, task, model, criterion):\n        self.cfg = cfg\n        self.task = task\n        self.model_parallel_size = cfg.common.model_parallel_size\n\n        # catalog shared parameters\n        shared_params = _catalog_shared_params(model)\n        self.cuda = torch.cuda.is_available() and not cfg.common.cpu\n\n        self.quiet_logs = getattr(cfg.common, \"quiet_logs\", False)\n        if self.cuda:\n            self.device = torch.device(\"cuda\")\n        else:\n            self.device = torch.device(\"cpu\")\n\n        if self.is_fsdp:\n            import fairscale\n\n            if (\n                max(self.cfg.optimization.update_freq) > 1\n                and fairscale.__version__ < \"0.4.0\"\n            ):\n                raise RuntimeError(\n                    \"Please update to fairscale 0.4.0 or newer when combining \"\n                    \"--update-freq with FullyShardedDataParallel\"\n                )\n            if self.use_sharded_state:\n                assert (\n                    fairscale.__version__ >= \"0.3.9\"\n                ), \"--use-sharded-state requires newer fairscale. pip install -U fairscale\"\n        else:\n            if self.cfg.distributed_training.cpu_offload:\n                raise ValueError(\"--cpu-offload requires --ddp-backend=fully_sharded\")\n\n        # copy model and criterion to current device/dtype\n        self._criterion = criterion\n        self._model = model\n        if not self.is_fsdp:\n            if cfg.common.bf16:\n                self._criterion = self._criterion.bfloat16()\n                self._model = self._model.bfloat16()\n            elif cfg.common.fp16:\n                self._criterion = self._criterion.half()\n                self._model = self._model.half()\n        if (\n            # the DistributedModel wrapper will handle moving to device,\n            # so only handle cases which don't use the wrapper\n            not self.use_distributed_wrapper\n        ):\n            self._criterion = self._criterion.to(device=self.device)\n            self._model = self._model.to(device=self.device)\n\n        # check that shared parameters are preserved after device transfer\n        for shared_param in shared_params:\n            ref = _get_module_by_path(self._model, shared_param[0])\n            for path in shared_param[1:]:\n                logger.info(\n                    \"detected shared parameter: {} <- {}\".format(shared_param[0], path)\n                )\n                _set_module_by_path(self._model, path, ref)\n        logger.info(metrics.get_nvidia_smi_gpu_memory_stats_str())\n\n        self._dummy_batch = None  # indicates we don't have a dummy batch at first\n        self._lr_scheduler = None\n        self._num_updates = 0\n        self._optim_history = None\n        self._optimizer = None\n        self._warn_once = set()\n        self._wrapped_criterion = None\n        self._wrapped_model = None\n        self._ewm_loss = None\n        self._skipped_loss_spikes = 0\n        self._ema = None\n\n        # TODO(myleott): support tpu\n        if self.cuda and self.data_parallel_world_size > 1:\n            self._grad_norm_buf = torch.cuda.DoubleTensor(self.data_parallel_world_size)\n        else:\n            self._grad_norm_buf = None\n\n        # get detailed cuda environment\n        if self.cuda:\n            self.cuda_env = utils.CudaEnvironment()\n            if self.data_parallel_world_size > 1:\n                self.cuda_env_arr = distributed_utils.all_gather_list(\n                    self.cuda_env, group=distributed_utils.get_global_group()\n                )\n            else:\n                self.cuda_env_arr = [self.cuda_env]\n            if self.data_parallel_rank == 0:\n                utils.CudaEnvironment.pretty_print_cuda_env_list(self.cuda_env_arr)\n        else:\n            self.cuda_env = None\n            self.cuda_env_arr = None\n\n        metrics.log_start_time(\"wall\", priority=790, round=0)\n\n        self._start_time = time.time()\n        self._previous_training_time = 0\n        self._cumulative_training_time = None\n\n    def reinitialize(self):\n        \"\"\"Reinitialize the Trainer, typically after model params change.\"\"\"\n        self._lr_scheduler = None\n        self._optimizer = None\n        self._wrapped_criterion = None\n        self._wrapped_model = None\n\n    @property\n    def data_parallel_world_size(self):\n        if self.cfg.distributed_training.distributed_world_size == 1:\n            return 1\n        return distributed_utils.get_data_parallel_world_size()\n\n    @property\n    def data_parallel_process_group(self):\n        return distributed_utils.get_data_parallel_group()\n\n    @property\n    def data_parallel_rank(self):\n        if self.cfg.distributed_training.distributed_world_size == 1:\n            return 0\n        return distributed_utils.get_data_parallel_rank()\n\n    @property\n    def is_data_parallel_master(self):\n        # NOTE: this returns true for all model parallel replicas with data\n        # parallel rank 0\n        return self.data_parallel_rank == 0\n\n    @property\n    def use_distributed_wrapper(self) -> bool:\n        return (self.data_parallel_world_size > 1) or (\n            self.is_fsdp and self.cfg.distributed_training.cpu_offload\n        )\n\n    @property\n    def should_save_checkpoint_on_current_rank(self) -> bool:\n        \"\"\"Indicates whether to save checkpoints on the current DDP rank.\"\"\"\n        if self.is_fsdp:\n            return True\n        else:\n            return self.is_data_parallel_master\n\n    @property\n    def checkpoint_suffix(self) -> str:\n        \"\"\"Suffix to add to the checkpoint file name.\"\"\"\n        if not self.use_sharded_state:\n            return self.cfg.checkpoint.checkpoint_suffix\n        elif self.is_fsdp:\n            return self.cfg.checkpoint.checkpoint_suffix + \"-shard{0}\".format(\n                self.data_parallel_rank\n            )\n        else:\n            return self.cfg.checkpoint.checkpoint_suffix or \"\"\n\n    @property\n    def criterion(self):\n        if self._wrapped_criterion is None:\n            if utils.has_parameters(self._criterion) and self.use_distributed_wrapper:\n                self._wrapped_criterion = models.DistributedModel(\n                    self.cfg.distributed_training,\n                    self._criterion,\n                    process_group=self.data_parallel_process_group,\n                    device=self.device,\n                )\n            else:\n                self._wrapped_criterion = self._criterion\n        return self._wrapped_criterion\n\n    @property\n    def model(self):\n        if self._wrapped_model is None:\n            if self.use_distributed_wrapper or self.is_fsdp:\n                self._wrapped_model = models.DistributedModel(\n                    self.cfg.distributed_training,\n                    self._model,\n                    process_group=self.data_parallel_process_group,\n                    device=self.device,\n                )\n            else:\n                self._wrapped_model = self._model\n        return self._wrapped_model\n\n    @property\n    def ema(self):\n        if self._ema is None:\n            self._build_ema()\n        return self._ema\n\n    def _build_ema(self):\n        if self.cfg.ema.store_ema:\n            if self.is_fsdp:\n                # Build FSDP model\n                extra = {\n                    \"use_sharded_state\": self.use_sharded_state,\n                }\n                with fsdp_enable_wrap(self.cfg.distributed_training, **extra):\n                    model = fsdp_wrap(self.task.build_model(self.cfg.model))\n\n                if self.cfg.common.memory_efficient_fp16:\n                    if self.cfg.common.bf16:\n                        model = model.bfloat16()\n                    else:\n                        model = model.half()\n\n                # Copy FSDP model state (since copy.deepcopy doesn't work)\n                state_dict = self.model.state_dict()\n                if not self.use_sharded_state:\n                    state_dict = distributed_utils.broadcast_object(\n                        state_dict, src_rank=0, group=self.model.process_group\n                    )\n                model.load_state_dict(state_dict)\n                self._ema = build_ema(model, self.cfg.ema, self.device)\n            else:\n                self._ema = build_ema(self._model, self.cfg.ema, self.device)\n            logger.info(\"Exponential Moving Average Shadow Model is initialized.\")\n\n    @property\n    def optimizer(self):\n        if self._optimizer is None:\n            self._build_optimizer()\n        return self._optimizer\n\n    @property\n    def lr_scheduler(self):\n        if self._lr_scheduler is None:\n            self._build_optimizer()  # this will initialize self._lr_scheduler\n        return self._lr_scheduler\n\n    def _build_optimizer(self):\n        params = list(\n            filter(\n                lambda p: p.requires_grad,\n                chain(self.model.parameters(), self.criterion.parameters()),\n            )\n        )\n\n        if self.is_fsdp and self.cfg.common.fp16:\n            # FullyShardedDataParallel always uses MemoryEfficientFP16 wrapper,\n            # mostly for the grad scaling. But if we don't have the\n            # --memory-efficient-fp16 flag set, then we're effectively doing\n            # regular --fp16 and can allow the use of optimizers that would\n            # otherwise be unsupported by MemoryEfficientFP16Optimizer.\n            allow_unsupported = not self.cfg.common.memory_efficient_fp16\n            self._optimizer = optim.MemoryEfficientFP16Optimizer.build_optimizer(\n                self.cfg, params, allow_unsupported=allow_unsupported\n            )\n        elif self.cfg.common.fp16:\n            if self.cuda and torch.cuda.get_device_capability(0)[0] < 7:\n                logger.info(\n                    \"NOTE: your device does NOT support faster training with --fp16, \"\n                    \"please switch to FP32 which is likely to be faster\"\n                )\n            if self.cfg.common.memory_efficient_fp16:\n                self._optimizer = optim.MemoryEfficientFP16Optimizer.build_optimizer(\n                    self.cfg, params\n                )\n            else:\n                self._optimizer = optim.FP16Optimizer.build_optimizer(self.cfg, params)\n        else:\n            if self.cuda and torch.cuda.get_device_capability(0)[0] >= 7:\n                logger.info(\"NOTE: your device may support faster training with --fp16\")\n            self._optimizer = optim.build_optimizer(self.cfg.optimizer, params)\n\n        if self.is_fsdp:\n            assert self._optimizer.supports_flat_params, (\n                \"--ddp-backend=fully_sharded is only compatible with pointwise \"\n                \"optimizers (e.g., Adam, AdamW, Adadelta, Adamax, SGD, etc.). \"\n                \"However, the sharding will result in slightly different results when \"\n                \"using non-pointwise optimizers (e.g., Adagrad, Adafactor, LAMB)\"\n            )\n\n        # We should initialize the learning rate scheduler immediately after\n        # building the optimizer, so that the initial learning rate is set.\n        self._lr_scheduler = lr_scheduler.build_lr_scheduler(\n            self.cfg.lr_scheduler,\n            self.optimizer,\n        )\n        self._lr_scheduler.step_update(0)\n\n    @property\n    def is_fsdp(self):\n        return self.cfg.distributed_training.ddp_backend == \"fully_sharded\"\n\n    @property\n    def use_sharded_state(self):\n        return self.cfg.distributed_training.use_sharded_state\n\n    def consolidate_optimizer(self):\n        \"\"\"For OSS, we need to consolidate the state dict.\"\"\"\n        self._gathered_optim_state = None\n        if hasattr(self.optimizer.optimizer, \"consolidate_state_dict\"):\n            self.optimizer.optimizer.consolidate_state_dict()\n        elif self.is_fsdp and not self.use_sharded_state:\n            st = self.model.gather_full_optim_state_dict(\n                self.optimizer\n            )  # only returns on rank 0\n            if st is None:\n                st = -1  # sentinel so that workers do not save optimizer.state_dict()\n            self._gathered_optim_state = st\n            assert self._gathered_optim_state is not None\n\n    def state_dict(self, filename, training_finished=False) -> Dict[str, Dict]:\n        model_state_dict = self.model.state_dict()\n        optim_state = self._gathered_optim_state or self.optimizer.state_dict()\n        model_save_list = [\n            (\n                filename,\n                model_state_dict,\n                optim_state,\n            )\n        ]\n        state_dicts = {}\n        # This is what gets saved to checkpoints.\n        for filename, model_state_dict, optimizer_state_dict in model_save_list:\n            state_dict = {\n                \"cfg\": OmegaConf.to_container(self.cfg)\n                if OmegaConf.is_config(self.cfg)\n                else self.cfg,\n                \"model\": model_state_dict,\n                \"criterion\": (\n                    self.criterion.state_dict()\n                    if utils.has_parameters(self.criterion)\n                    else None\n                ),\n                \"optimizer_history\": (self._optim_history or [])\n                + [\n                    {\n                        \"criterion_name\": self.get_criterion().__class__.__name__,\n                        \"optimizer_name\": self.optimizer.__class__.__name__,\n                        \"lr_scheduler_state\": self.lr_scheduler.state_dict(),\n                        \"num_updates\": self.get_num_updates(),\n                    }\n                ],\n                \"task_state\": self.task.state_dict() if self.task is not None else {},\n                \"extra_state\": {\n                    \"metrics\": metrics.state_dict(),\n                    \"previous_training_time\": self.cumulative_training_time(),\n                    \"ewm_loss\": self._ewm_loss,\n                },\n            }\n\n            state_dict[\"last_optimizer_state\"] = optimizer_state_dict\n\n            if self.cfg.ema.store_ema:\n                # Save EMA model state as extra state\n                state_dict[\"extra_state\"][\"ema\"] = self.ema.get_model().state_dict()\n                if self.cfg.ema.ema_fp32:\n                    # Save EMA params in fp32\n                    state_dict[\"extra_state\"][\"ema_fp32_params\"] = self.ema.fp32_params\n\n            if self.is_fsdp and self.use_sharded_state:\n                state_dict[\n                    \"shard_metadata\"\n                ] = (\n                    self.model.local_metadata_dict()\n                )  # save FSDP flattening and padding info\n            state_dicts[filename] = state_dict\n        return state_dicts\n\n    def save_checkpoint(\n        self,\n        filename,\n        extra_state,\n        training_finished=False,\n        async_callback_fn=None,\n        files_to_symlink_to=None,\n    ):\n        \"\"\"Save all training state in a checkpoint file.\"\"\"\n\n        if self.model_parallel_size > 1:\n            extra_state[\"rng_tracker_states\"] = get_cuda_rng_tracker().get_states()\n\n        # call state_dict on all ranks in case it needs internal communication\n        state_dicts = self.state_dict(filename, training_finished)\n        for filename, state_dict in state_dicts.items():\n            logger.info(f\"Saving checkpoint to {filename}\")\n            state_dict = utils.move_to_cpu(\n                state_dict,\n                # keep params in FP16 when training with --memory-efficient-fp16\n                cast_to_fp32=not self.cfg.common.memory_efficient_fp16,\n            )\n            state_dict[\"extra_state\"].update(extra_state)\n            if self.should_save_checkpoint_on_current_rank:\n                if not hasattr(self, \"async_checkpoint\"):\n                    self.async_checkpoint = ThreadPoolExecutor(max_workers=1)\n\n                def perform_save():\n                    try:\n                        logger.info(f\"Beginning asynchronous torch.save to {filename}\")\n                        async_callback_fn(filename, files_to_symlink_to)\n                        logger.info(f\"Asynchronous torch.save to {filename} complete.\")\n                    except Exception as e:\n                        logger.exception(f\"Asynchronous save failed: {e}\")\n\n                torch.save(state_dict, filename)\n                if async_callback_fn is not None:\n                    self.async_checkpoint.submit(perform_save)\n            logger.info(f\"Finished saving checkpoint to {filename}\")\n\n    def load_checkpoint(\n        self,\n        filename,\n        reset_optimizer=False,\n        reset_lr_scheduler=False,\n        optimizer_overrides=None,\n        reset_meters=False,\n    ):\n        \"\"\"\n        Load all training state from a checkpoint file.\n        rank = 0 will load the checkpoint, and then broadcast it to all\n        other ranks.\n        \"\"\"\n        extra_state, self._optim_history, last_optim_state = None, [], None\n\n        is_distributed = self.data_parallel_world_size > 1\n        bexists = False\n\n        logger.info(f\"attempting to load checkpoint from: {filename}\")\n\n        if PathManager.isfile(filename):\n            bexists = True\n        else:\n            # this is a big hacky as when we increase the world size, then filename doesn't really point\n            # to a real file, we convert it to multiple files to be loaded later.\n            # so here we just check if there are some files existing in the dir.\n            files_in_local_dir = os.listdir(os.path.dirname(filename))\n            filename_prefix = os.path.splitext(os.path.basename(filename))[0].replace(\n                self.checkpoint_suffix, \"\"\n            )\n            matched_files = [\n                f for f in files_in_local_dir if f.startswith(filename_prefix)\n            ]\n            bexists = len(matched_files) > 0\n\n        if bexists:\n            logger.info(f\"Preparing to load checkpoint {filename}\")\n            # FSDP requires loading checkpoint shards on all ranks\n            load_on_all_ranks = self.is_fsdp\n\n            if load_on_all_ranks or self.is_data_parallel_master:\n                state = checkpoint_utils.load_checkpoint_to_cpu(\n                    filename,\n                )\n                last_optim_state = state.get(\"last_optimizer_state\", None)\n                if last_optim_state == -1:\n                    master_path = re.sub(\"shard[0-9]+\", \"shard0\", filename)\n                    last_optim_state = torch.load(master_path, map_location=\"cpu\")[\n                        \"last_optimizer_state\"\n                    ]\n\n                logger.info(f\"Loaded state for {filename}\")\n\n            else:\n                last_optim_state = None\n                state = None\n\n            if self.data_parallel_world_size > 1 and not load_on_all_ranks:\n                state = distributed_utils.broadcast_object(\n                    state,\n                    src_rank=0,\n                    group=self.data_parallel_process_group,\n                    dist_device=self.device,\n                )\n                if self.data_parallel_rank > 0:\n                    last_optim_state = state.get(\"last_optimizer_state\", None)\n\n            # load model parameters\n            try:\n                self.model.load_state_dict(state[\"model\"], strict=True)\n                # save memory for later steps\n                del state[\"model\"]\n                if utils.has_parameters(self.get_criterion()):\n                    self.get_criterion().load_state_dict(\n                        state[\"criterion\"], strict=True\n                    )\n                    del state[\"criterion\"]\n\n            except Exception:\n                raise Exception(\n                    \"Cannot load model parameters from checkpoint {}; \"\n                    \"please ensure that the architectures match.\".format(filename)\n                )\n            extra_state = state[\"extra_state\"]\n            self._optim_history = state[\"optimizer_history\"]\n\n        if last_optim_state is not None and not reset_optimizer:\n            # rebuild optimizer after loading model, since params may have changed\n            self._build_optimizer()\n\n            # only reload optimizer and lr_scheduler if they match\n            last_optim = self._optim_history[-1]\n            assert (\n                last_optim[\"criterion_name\"] == self.get_criterion().__class__.__name__\n            ), (\n                f\"Criterion does not match; please reset the optimizer \"\n                f\"(--reset-optimizer). {last_optim['criterion_name']} vs \"\n                f\"{self.get_criterion().__class__.__name__}\"\n            )\n            assert last_optim[\"optimizer_name\"] == self.optimizer.__class__.__name__, (\n                f\"Optimizer does not match; please reset the optimizer \"\n                f\"(--reset-optimizer). {last_optim['optimizer_name']} vs \"\n                f\"{self.optimizer.__class__.__name__}\"\n            )\n\n            if not reset_lr_scheduler:\n                self.lr_scheduler.load_state_dict(last_optim[\"lr_scheduler_state\"])\n\n            if not load_on_all_ranks and is_distributed:\n                last_optim_state = self.optimizer.broadcast_global_state_dict(\n                    last_optim_state\n                )\n            elif self.is_fsdp and not self.use_sharded_state:\n                last_optim_state = self.model.get_shard_from_optim_state_dict(\n                    last_optim_state\n                )\n                logger.info(f\"FSDP got shard from optim_state for {filename}\")\n\n            self.optimizer.load_state_dict(last_optim_state, optimizer_overrides)\n            logger.info(f\"Loaded optim_state for {filename}\")\n            self.set_num_updates(last_optim[\"num_updates\"])\n\n        if extra_state is not None:\n            itr_state = extra_state[\"train_iterator\"]\n            epoch = itr_state[\"epoch\"]\n\n            if \"previous_training_time\" in extra_state:\n                self._previous_training_time = extra_state[\"previous_training_time\"]\n                self._start_time = time.time()\n\n            if \"ewm_loss\" in extra_state:\n                self._ewm_loss = extra_state[\"ewm_loss\"]\n\n            self.lr_step(epoch)\n\n            if (\n                itr_state.get(\"version\", 1) >= 2\n                and itr_state[\"iterations_in_epoch\"] == 0\n            ):\n                # reset meters at start of epoch\n                reset_meters = True\n\n            if \"metrics\" in extra_state and not reset_meters:\n                metrics.load_state_dict(extra_state[\"metrics\"])\n\n                # reset TimeMeters, since their start times don't make sense anymore\n                for meter in metrics.get_meters(\"default\"):\n                    if isinstance(meter, meters.TimeMeter):\n                        meter.reset()\n\n            if self.cfg.ema.store_ema:\n                if \"ema\" not in extra_state:\n                    logger.warn(\n                        \"EMA not found in checkpoint. But store_ema is True. \"\n                        \"EMA is re-initialized from checkpoint.\"\n                    )\n                    self.ema.restore(\n                        state[\"model\"], build_fp32_params=self.cfg.ema.ema_fp32\n                    )\n                else:\n                    logger.info(\"Loading EMA from checkpoint\")\n                    self.ema.restore(extra_state[\"ema\"], build_fp32_params=False)\n\n                    if self.cfg.ema.ema_fp32:\n                        if \"ema_fp32_params\" in extra_state:\n                            logger.info(\"Loading EMA fp32 params from checkpoint\")\n                            self.ema.build_fp32_params(extra_state[\"ema_fp32_params\"])\n                        else:\n                            logger.info(\n                                \"Building EMA fp32 params from EMA model in checkpoint\"\n                            )\n                            self.ema.build_fp32_params()\n\n            logger.info(\n                f\"Loaded checkpoint {filename} (epoch {epoch} @ {self.get_num_updates()} updates)\"\n            )\n        else:\n            logger.info(\"No existing checkpoint found {}\".format(filename))\n\n        if extra_state is not None and \"rng_tracker_states\" in extra_state:\n            get_cuda_rng_tracker().set_states(extra_state[\"rng_tracker_states\"])\n        return extra_state\n\n    def get_train_iterator(\n        self,\n        epoch,\n        combine=True,\n        data_selector=None,\n        shard_batch_itr=True,\n        disable_iterator_cache=False,\n    ):\n        \"\"\"Return an EpochBatchIterator over the training set for a given epoch.\"\"\"\n        logger.info(\"loading train data for epoch {}\".format(epoch))\n        self.task.load_dataset(\n            self.cfg.dataset.train_subset,\n            epoch=epoch,\n            combine=combine,\n            data_selector=data_selector,\n        )\n        batch_iterator = self.task.get_batch_iterator(\n            dataset=self.task.dataset(self.cfg.dataset.train_subset),\n            max_tokens=self.cfg.dataset.max_tokens,\n            max_sentences=self.cfg.dataset.batch_size,\n            max_positions=utils.resolve_max_positions(\n                self.task.max_positions(),\n                self.model.max_positions(),\n                self.cfg.dataset.max_tokens,\n            ),\n            ignore_invalid_inputs=True,\n            required_batch_size_multiple=self.cfg.dataset.required_batch_size_multiple,\n            seed=self.cfg.common.seed,\n            num_shards=self.data_parallel_world_size if shard_batch_itr else 1,\n            shard_id=self.data_parallel_rank if shard_batch_itr else 0,\n            num_workers=self.cfg.dataset.num_workers,\n            epoch=epoch,\n            data_buffer_size=self.cfg.dataset.data_buffer_size,\n            disable_iterator_cache=disable_iterator_cache,\n            skip_remainder_batch=True,\n        )\n        logger.info(\"finished creating batch iterator\")\n        self.reset_dummy_batch(batch_iterator.first_batch)\n        return batch_iterator\n\n    def get_valid_iterator(\n        self,\n        subset,\n        disable_iterator_cache=False,\n    ):\n        \"\"\"Return an EpochBatchIterator over given validation subset for a given epoch.\"\"\"\n        batch_iterator = self.task.get_batch_iterator(\n            dataset=self.task.dataset(subset),\n            max_tokens=self.cfg.dataset.max_tokens_valid,\n            max_sentences=self.cfg.dataset.batch_size_valid,\n            max_positions=utils.resolve_max_positions(\n                self.task.max_positions(),\n                self.model.max_positions(),\n            ),\n            ignore_invalid_inputs=self.cfg.dataset.skip_invalid_size_inputs_valid_test,\n            required_batch_size_multiple=self.cfg.dataset.required_batch_size_multiple,\n            seed=self.cfg.common.seed,\n            num_shards=self.data_parallel_world_size,\n            shard_id=self.data_parallel_rank,\n            num_workers=self.cfg.dataset.num_workers_valid,\n            # always pass a fixed \"epoch\" to keep validation data consistent\n            # across training epochs\n            epoch=1,\n            data_buffer_size=self.cfg.dataset.data_buffer_size,\n            disable_iterator_cache=disable_iterator_cache,\n            skip_remainder_batch=False,\n        )\n        self.reset_dummy_batch(batch_iterator.first_batch)\n        return batch_iterator\n\n    def begin_epoch(self, epoch):\n        \"\"\"Called at the beginning of each epoch.\"\"\"\n        logger.info(\"begin training epoch {}\".format(epoch))\n\n        self.lr_step_begin_epoch(epoch)\n\n        # task specific setup per epoch\n        self.task.begin_epoch(epoch, self.get_model())\n\n    def begin_valid_epoch(self, epoch):\n        \"\"\"Called at the beginning of each validation epoch.\"\"\"\n\n        # task specific setup per validation epoch\n        self.task.begin_valid_epoch(epoch, self.get_model())\n\n    def reset_dummy_batch(self, batch):\n        self._dummy_batch = batch\n\n    @metrics.aggregate(\"train\")\n    def train_step(self, samples):\n        \"\"\"Do forward, backward and parameter update.\"\"\"\n        self._set_seed()\n        self.model.train()\n        self.criterion.train()\n        self.zero_grad()\n\n        metrics.log_start_time(\"train_wall\", priority=800, round=0)\n\n        # If EMA is enabled through store_ema=True\n        # and task.uses_ema is True, pass the EMA model as a keyword\n        # argument to the task.\n        extra_kwargs = {}\n        if self.cfg.ema.store_ema and getattr(self.task, \"uses_ema\", False):\n            extra_kwargs[\"ema_model\"] = self.ema.get_model()\n\n        # forward and backward pass\n        logging_outputs, sample_size = [], 0\n        for i, sample in enumerate(samples):  # delayed update loop\n            if (\n                self.get_num_updates() == 0\n                and i == 0\n                and distributed_utils.get_global_rank() == 0\n            ):\n                logger.info(f\"First batch on first rank: \" + str(sample))\n            sample, is_dummy_batch = self._prepare_sample(sample)\n\n            def maybe_no_sync():\n                \"\"\"\n                Whenever *samples* contains more than one mini-batch, we\n                want to accumulate gradients locally and only call\n                all-reduce in the last backwards pass.\n                \"\"\"\n                if (\n                    self.data_parallel_world_size > 1\n                    and hasattr(self.model, \"no_sync\")\n                    and i < len(samples) - 1\n                    # The no_sync context manager results in increased memory\n                    # usage with FSDP, since full-size gradients will be\n                    # accumulated on each GPU. It's typically a better tradeoff\n                    # to do the extra communication with FSDP.\n                    and not self.is_fsdp\n                ):\n                    return self.model.no_sync()\n                else:\n                    return contextlib.ExitStack()  # dummy contextmanager\n\n            try:\n                with maybe_no_sync(), (\n                    set_rank_seed(self.cfg.common.seed, self.get_num_updates())\n                    if self.cfg.common.seed_per_rank\n                    else contextlib.nullcontext()\n                ):\n                    # forward and backward\n                    loss, sample_size_i, logging_output = self.task.train_step(\n                        sample=sample,\n                        model=self.model,\n                        criterion=self.criterion,\n                        optimizer=self.optimizer,\n                        update_num=self.get_num_updates(),\n                        ignore_grad=is_dummy_batch,\n                    )\n                    del loss\n\n                logging_outputs.append(logging_output)\n                sample_size += sample_size_i\n\n                # emptying the CUDA cache after the first step can\n                # reduce the chance of OOM\n                if self.cuda and self.get_num_updates() == 0:\n                    torch.cuda.empty_cache()\n            except RuntimeError as e:\n                if \"out of memory\" in str(e):\n                    self._log_oom(e)\n                raise e\n\n        if is_dummy_batch:\n            if torch.is_tensor(sample_size):\n                sample_size.zero_()\n            else:\n                sample_size *= 0.0\n\n        if torch.is_tensor(sample_size):\n            sample_size = sample_size.float()\n        else:\n            sample_size = float(sample_size)\n\n        # gather logging outputs from all replicas\n        if self._sync_stats():\n            train_time = self._local_cumulative_training_time()\n            logging_outputs, (\n                sample_size,\n                total_train_time,\n            ) = self._aggregate_logging_outputs(\n                logging_outputs, sample_size, train_time, ignore=is_dummy_batch\n            )\n            self._cumulative_training_time = (\n                total_train_time / self.data_parallel_world_size\n            )\n\n        overflow = False\n        logger.debug(f\"[{self.get_num_updates()}] done with fwd, bwd\")\n        try:\n            # reduce gradients across workers\n            self.optimizer.all_reduce_grads(self.model)\n            if utils.has_parameters(self.criterion):\n                self.optimizer.all_reduce_grads(self.criterion)\n\n            # multiply gradients by (data_parallel_size / sample_size) since\n            # DDP normalizes by the number of data parallel workers for\n            # improved fp16 precision.\n            # Thus we get (sum_of_gradients / sample_size) at the end.\n            # In case of fp16, this step also undoes loss scaling.\n            # (Debugging note: Some optimizers perform this scaling on the\n            # fly, so inspecting model.parameters() or optimizer.params may\n            # still show the original, unscaled gradients.)\n            numer = self.data_parallel_world_size if self._sync_stats() else 1\n            self.optimizer.multiply_grads(numer / (sample_size or 1.0))\n            # Note: (sample_size or 1.0) handles the case of a zero gradient, in a\n            # way that avoids CPU/device transfers in case sample_size is a GPU or\n            # TPU object. The assumption is that the gradient itself is also 0.\n\n            # clip grads\n            grad_norm = self.clip_grad_norm(\n                self.cfg.optimization.clip_norm,\n                self.cfg.optimization.clip_norm_type,\n                self.cfg.optimization.skip_gradient_update_on_clip_norm,\n            )\n            # check that grad norms are consistent across workers\n            self._check_grad_norms(grad_norm)\n            if not torch.isfinite(grad_norm).all():\n                # check local gradnorm single GPU case, trigger NanDetector\n                raise FloatingPointError(\"gradients are Nan/Inf\")\n            # skip optimizer step if there is a loss spike\n            ewm_loss_ratio = self.skip_spike(\n                logging_outputs, self.cfg.optimization.ewm_ratio_to_skip_batch\n            )\n            # downscale grads by ewm_loss_ratio ** 4\n            if ewm_loss_ratio > 1.0:\n                grad_mult_factor = 1.0 / (ewm_loss_ratio**4)\n                curr_lr = self.optimizer.get_lr()\n                new_lr = curr_lr * grad_mult_factor\n                self.optimizer.set_lr(new_lr)\n                logger.info(f\"Scaling LR by {grad_mult_factor:.2f} to {new_lr:.6f}\")\n            # take an optimization step\n            self.task.optimizer_step(\n                self.optimizer,\n                model=self.model,\n                update_num=self.get_num_updates(),\n            )\n            logger.debug(f\"[{self.get_num_updates()}] done with optimizer step\")\n\n        except FloatingPointError:\n            # re-run the forward and backward pass with hooks attached to print\n            # out where it fails\n            self.zero_grad()\n            with NanDetector(self.get_model()), (\n                set_rank_seed(self.cfg.common.seed, self.get_num_updates())\n                if self.cfg.common.seed_per_rank\n                else contextlib.nullcontext()\n            ):\n                for _, sample in enumerate(samples):\n                    sample, _ = self._prepare_sample(sample)\n                    self.task.train_step(\n                        sample,\n                        self.model,\n                        self.criterion,\n                        self.optimizer,\n                        self.get_num_updates(),\n                        ignore_grad=False,\n                    )\n            raise\n        except OverflowError as e:\n            overflow = True\n            logger.info(\n                f\"NOTE: gradient overflow detected, ignoring gradient, {str(e)}\"\n            )\n            grad_norm = torch.tensor(0.0).cuda()\n            self.zero_grad()\n        # except SpikeError as e:\n        # overflow = True\n        # logger.info(str(e))\n        # self.zero_grad()\n        except RuntimeError as e:\n            if \"out of memory\" in str(e):\n                self._log_oom(e)\n                logger.error(\"OOM during optimization, irrecoverable\")\n            raise e\n\n        logging_output = None\n        if not overflow:\n            self.set_num_updates(self.get_num_updates() + 1)\n\n            # EMA update\n            if self.cfg.ema.store_ema:\n                # Step EMA forward with new model.\n                self.ema.step(\n                    self.get_model(),\n                    self.get_num_updates(),\n                )\n                metrics.log_scalar(\n                    \"ema_decay\",\n                    self.ema.get_decay(),\n                    priority=10000,\n                    round=5,\n                    weight=0,\n                )\n\n            if self.cuda and self.cuda_env is not None:\n                # log minimum free memory over the iteration\n                self._log_gpu_mem_stats()\n\n            # log stats\n            logging_output = self._reduce_and_log_stats(\n                logging_outputs, sample_size, grad_norm, ewm_loss_ratio\n            )\n\n            # clear CUDA cache to reduce memory fragmentation\n            if (\n                self.cuda\n                and self.cfg.common.empty_cache_freq > 0\n                and (\n                    (self.get_num_updates() + self.cfg.common.empty_cache_freq - 1)\n                    % self.cfg.common.empty_cache_freq\n                )\n                == 0\n            ):\n                torch.cuda.empty_cache()\n\n        if self.cfg.common.fp16 and not self.cfg.common.bf16:\n            metrics.log_scalar(\n                \"loss_scale\",\n                self.optimizer.scaler.loss_scale,\n                priority=700,\n                round=4,\n                weight=0,\n            )\n            metrics.log_scalar(\n                \"scale_window\",\n                self.optimizer.scaler.scale_window,\n                priority=700,\n                round=4,\n                weight=0,\n            )\n\n        metrics.log_stop_time(\"train_wall\")\n        return logging_output\n\n    @metrics.aggregate(\"valid\")\n    def valid_step(self, sample, num_step=0, raise_oom=False):\n        \"\"\"Do forward pass in evaluation mode.\"\"\"\n\n        # If EMA is enabled through store_ema=True\n        # and task.uses_ema is True, pass the EMA model as a keyword\n        # argument to the task.\n        extra_kwargs = {}\n        if self.cfg.ema.store_ema and getattr(self.task, \"uses_ema\", False):\n            extra_kwargs[\"ema_model\"] = self.ema.get_model()\n\n        with torch.no_grad():\n            self.model.eval()\n            self.criterion.eval()\n            self.zero_grad()\n\n            sample, is_dummy_batch = self._prepare_sample(sample)\n\n            try:\n                with set_rank_seed(\n                    self.cfg.common.seed, num_step + self.get_num_updates()\n                ) if self.cfg.common.seed_per_rank else contextlib.nullcontext():\n                    _loss, sample_size, logging_output = self.task.valid_step(\n                        sample, self.model, self.criterion\n                    )\n            except RuntimeError as e:\n                if \"out of memory\" in str(e):\n                    self._log_oom(e)\n                    if not raise_oom:\n                        logger.warning(\n                            \"ran out of memory in validation step, retrying batch\"\n                        )\n                        for p in self.model.parameters():\n                            if p.grad is not None:\n                                p.grad = None  # free some memory\n                        if self.cuda:\n                            torch.cuda.empty_cache()\n                        return self.valid_step(sample, num_step, raise_oom=True)\n                raise e\n\n            logging_outputs = [logging_output]\n            if is_dummy_batch:\n                if torch.is_tensor(sample_size):\n                    sample_size.zero_()\n                else:\n                    sample_size *= 0.0\n\n        # gather logging outputs from all replicas\n        if self.data_parallel_world_size > 1:\n            logging_outputs, (sample_size,) = self._aggregate_logging_outputs(\n                logging_outputs,\n                sample_size,\n                ignore=is_dummy_batch,\n            )\n\n        # log validation stats\n        logging_output = self._reduce_and_log_stats(logging_outputs, sample_size)\n        return logging_output\n\n    def zero_grad(self):\n        self.optimizer.zero_grad()\n\n    def lr_step_begin_epoch(self, epoch):\n        \"\"\"Adjust the learning rate at the beginning of the epoch.\"\"\"\n        self.lr_scheduler.step_begin_epoch(epoch)\n        # prefer updating the LR based on the number of steps\n        return self.lr_step_update()\n\n    def lr_step(self, epoch, val_loss=None):\n        \"\"\"Adjust the learning rate at the end of the epoch.\"\"\"\n        self.lr_scheduler.step(epoch, val_loss)\n        # prefer updating the LR based on the number of steps\n        return self.lr_step_update()\n\n    def lr_step_update(self):\n        \"\"\"Update the learning rate after each update.\"\"\"\n        new_lr = self.lr_scheduler.step_update(self.get_num_updates())\n        if isinstance(new_lr, dict):\n            for k, v in new_lr.items():\n                metrics.log_scalar(f\"lr_{k}\", v, weight=0, priority=300)\n            new_lr = new_lr.get(\"default\", next(iter(new_lr.values())))\n        else:\n            metrics.log_scalar(\"lr\", new_lr, weight=0, priority=300)\n        return new_lr\n\n    def get_lr(self):\n        \"\"\"Get the current learning rate.\"\"\"\n        return self.optimizer.get_lr()\n\n    def get_model(self):\n        \"\"\"Get the (non-wrapped) model instance.\"\"\"\n        return self._model\n\n    def get_criterion(self):\n        \"\"\"Get the (non-wrapped) criterion instance.\"\"\"\n        return self._criterion\n\n    def get_num_updates(self):\n        \"\"\"Get the number of parameters updates.\"\"\"\n        return self._num_updates\n\n    def set_num_updates(self, num_updates):\n        \"\"\"Set the number of parameters updates.\"\"\"\n        self._num_updates = num_updates\n        self.lr_step_update()\n        metrics.log_scalar(\"num_updates\", self._num_updates, weight=0, priority=200)\n\n    def clip_grad_norm(\n        self, clip_norm, clip_norm_type=\"l2\", skip_gradient_update_on_clip_norm=False\n    ):\n        if self.model_parallel_size == 1:\n            return self.optimizer.clip_grad_norm(\n                clip_norm,\n                clip_norm_type,\n                aggregate_norm_fn=None,\n                skip_gradient_update_on_clip_norm=skip_gradient_update_on_clip_norm,\n            )\n        else:\n\n            def _aggregate_model_parallel_grad_norm(norm_type, total_norm):\n                norm_type2_reduce_op = {\n                    \"l2\": dist.ReduceOp.SUM,\n                    \"inf\": dist.ReduceOp.MAX,\n                }\n                reduce_op = norm_type2_reduce_op[norm_type]\n                if norm_type == \"l2\":\n                    total_norm.pow_(2)\n                dist.all_reduce(\n                    total_norm,\n                    group=distributed_utils.get_model_parallel_group(),\n                    op=reduce_op,\n                )\n                if norm_type == \"l2\":\n                    total_norm.sqrt_()\n                return total_norm\n\n            return self.optimizer.clip_grad_norm(\n                clip_norm,\n                clip_norm_type,\n                aggregate_norm_fn=functools.partial(\n                    _aggregate_model_parallel_grad_norm, clip_norm_type\n                ),\n                skip_gradient_update_on_clip_norm=skip_gradient_update_on_clip_norm,\n            )\n\n    def skip_spike(self, logging_outputs, ewm_ratio_to_skip_batch, span=9):\n        loss_sum = sum(log.get(\"loss\", 0) for log in logging_outputs)\n        sample_size = sum(log.get(\"sample_size\", 0) for log in logging_outputs)\n        loss_t = float(loss_sum / sample_size / math.log(2))\n\n        if self._ewm_loss is None:\n            self._ewm_loss = loss_t\n\n        ewm_t_1 = self._ewm_loss\n        ewm_ratio = loss_t / ewm_t_1\n\n        if ewm_ratio > ewm_ratio_to_skip_batch:\n            self._skipped_loss_spikes += 1\n            # raise SpikeError(\n            #     f\"Skip batch as we encountered a loss spike. In \"\n            #     f\"num_update: {self.get_num_updates()} the loss is {loss_t:.2f}. \"\n            #     f\"The ewm for the loss was only at {ewm_t:.2f} . \"\n            #     f\"The loss to ewm loss ratio is {ewm_ratio:.2f}, which is higher than \"\n            #     f\"ewm_ratio_to_skip_batch of {ewm_ratio_to_skip_batch} .\"\n            # )\n        else:\n            # update the moving average only if we are not loss spiking\n            alpha = 2 / (span + 1)\n            ewm_t = (1 - alpha) * ewm_t_1 + alpha * loss_t\n            self._ewm_loss = ewm_t\n\n        return ewm_ratio\n\n    def cumulative_training_time(self):\n        if self._cumulative_training_time is None:\n            # single GPU\n            return self._local_cumulative_training_time()\n        else:\n            return self._cumulative_training_time\n\n    def _local_cumulative_training_time(self):\n        \"\"\"Aggregate training time in seconds.\"\"\"\n        return time.time() - self._start_time + self._previous_training_time\n\n    def _prepare_sample(self, sample, is_dummy=False):\n        if sample == \"DUMMY\":\n            raise Exception(\n                \"Trying to use an uninitialized 'dummy' batch. This usually indicates \"\n                \"that the total number of batches is smaller than the number of \"\n                \"participating GPUs. Try reducing the batch size or using fewer GPUs.\"\n            )\n\n        if sample is None or len(sample) == 0:\n            assert (\n                self._dummy_batch is not None and len(self._dummy_batch) > 0\n            ), \"Invalid dummy batch: {}\".format(self._dummy_batch)\n            sample, _ = self._prepare_sample(self._dummy_batch, is_dummy=True)\n            return sample, True\n\n        if self.cuda:\n            sample = utils.move_to_cuda(sample)\n\n            # if False:  # turn on to double-check we do not have data loader issues\n            #     # When we finish an epoch some dataloaders run short on data one iteration before others.\n            #     # We want to check that the data loaders that are running short are returning correct data\n            #     # on all their previous iterations.\n            #\n            #     # If they are returning the correct data, then we can rule out a lot of reasons why they would\n            #     # run short.\n            #\n            #     ipt = sample[\"net_input\"][\"src_tokens\"]\n            #     if not hasattr(self, \"input_errors\"):\n            #         self.input_errors = torch.tensor(\n            #             0, dtype=torch.int, device=ipt.device\n            #         )\n            #\n            #     min_ipt = ipt.clone()\n            #\n            #     torch.distributed.all_reduce(\n            #         min_ipt,\n            #         op=torch.distributed.ReduceOp.MIN,\n            #         group=distributed_utils.get_model_parallel_group(),\n            #     )\n            #\n            #     self.input_errors += (min_ipt != ipt).any()\n            #\n            #     if self.get_num_updates() % self.cfg.common.log_interval == 0:\n            #         if int(self.input_errors) > 0:\n            #             logger.error(\n            #                 f\"Data {self.data_parallel_rank} Model {distributed_utils.get_model_parallel_rank()} \"\n            #                 f\"has {self.input_errors} data mismatch errors!\"\n            #             )\n\n        def lower_precision(t):\n            \"\"\"Converts a tensor to the desired dtype based on our cfg.\"\"\"\n            if t.dtype is torch.float32:\n                if self.cfg.common.bf16:\n                    return t.bfloat16()\n                return t.half()\n            return t\n\n        # TODO[Susan]: sample dict is full of int64 tensors - check this.\n        if self.cfg.common.fp16:\n            sample = utils.apply_to_sample(lower_precision, sample)\n\n        if self._dummy_batch == \"DUMMY\":\n            self._dummy_batch = sample\n\n        return sample, False\n\n    def _set_seed(self):\n        # Set seed based on args.seed and the update number so that we get\n        # reproducible results when resuming from checkpoints\n        seed = self.cfg.common.seed + self.get_num_updates()\n        utils.set_torch_seed(seed)\n\n    def _sync_stats(self):\n        # Return True if it's using multiple GPUs and DDP\n        if self.data_parallel_world_size == 1:\n            return False\n        else:\n            return True\n\n    def _log_oom(self, exc):\n        msg = \"OOM: Ran out of memory with exception: {}\".format(exc)\n        logger.warning(msg)\n        if torch.cuda.is_available() and hasattr(torch.cuda, \"memory_summary\"):\n            for device_idx in range(torch.cuda.device_count()):\n                logger.warning(torch.cuda.memory_summary(device=device_idx))\n        sys.stderr.flush()\n\n    def _aggregate_logging_outputs(\n        self,\n        logging_outputs: List[Dict[str, Any]],\n        *extra_stats_to_sum,\n        ignore=False,\n    ):\n        if self.task.__class__.logging_outputs_can_be_summed(self.get_criterion()):\n            return self._fast_stat_sync_sum(\n                logging_outputs, *extra_stats_to_sum, ignore=ignore\n            )\n        else:\n            return self._all_gather_list_sync(\n                logging_outputs, *extra_stats_to_sum, ignore=ignore\n            )\n\n    def _all_gather_list_sync(\n        self,\n        logging_outputs: List[Dict[str, Any]],\n        *extra_stats_to_sum,\n        ignore=False,\n    ):\n        \"\"\"\n        Sync logging outputs across workers. all_gather_list_sync is\n        suitable when logging outputs are complex types.\n        \"\"\"\n        if ignore:\n            logging_outputs = []\n        results = list(\n            zip(\n                *distributed_utils.all_gather_list(\n                    [logging_outputs] + list(extra_stats_to_sum),\n                    max_size=getattr(self.cfg.common, \"all_gather_list_size\", 16384),\n                    group=self.data_parallel_process_group,\n                )\n            )\n        )\n        logging_outputs, extra_stats_to_sum = results[0], results[1:]\n        logging_outputs = list(chain.from_iterable(logging_outputs))\n        extra_stats_to_sum = [sum(s) for s in extra_stats_to_sum]\n        return logging_outputs, extra_stats_to_sum\n\n    def _fast_stat_sync_sum(\n        self,\n        logging_outputs: List[Dict[str, Any]],\n        *extra_stats_to_sum,\n        ignore=False,\n    ):\n        \"\"\"\n        Sync logging outputs across workers. fast_stat_sync_sum is\n        faster than all_gather_list_sync, but is only suitable when\n        logging outputs are scalars and can be summed. Note that\n        *logging_outputs* cannot contain any nested dicts/lists.\n        \"\"\"\n        data = {}\n        for i, stat in enumerate(extra_stats_to_sum):\n            data[\"extra_stats_\" + str(i)] = stat\n        if len(logging_outputs) > 0:\n            log_keys = list(logging_outputs[0].keys())\n            for k in log_keys:\n                if not ignore:\n                    v = sum(log[k] for log in logging_outputs if k in log)\n                else:\n                    v = logging_outputs[0][k]\n                    v = torch.zeros_like(v) if torch.is_tensor(v) else 0\n                data[\"logging_outputs_\" + k] = v\n        else:\n            log_keys = None\n\n        data = distributed_utils.all_reduce_dict(\n            data, device=self.device, group=self.data_parallel_process_group\n        )\n\n        extra_stats_to_sum = [\n            data[\"extra_stats_\" + str(i)] for i in range(len(extra_stats_to_sum))\n        ]\n        if log_keys is not None:\n            logging_outputs = [{k: data[\"logging_outputs_\" + k] for k in log_keys}]\n        else:\n            logging_outputs = []\n        return logging_outputs, extra_stats_to_sum\n\n    def _check_grad_norms(self, grad_norm):\n        \"\"\"Check that grad norms are consistent across workers.\"\"\"\n        if self._grad_norm_buf is not None:\n            self._grad_norm_buf.zero_()\n            self._grad_norm_buf[self.data_parallel_rank] = grad_norm\n            distributed_utils.all_reduce(\n                self._grad_norm_buf, group=self.data_parallel_process_group\n            )\n\n            def is_consistent(tensor):\n                max_abs_diff = torch.max(torch.abs(tensor - tensor[0]))\n                return (\n                    torch.isfinite(tensor).all()\n                    and (max_abs_diff / (tensor[0] + 1e-6) < 1e-6).all()\n                )\n\n            if not is_consistent(self._grad_norm_buf):\n                pretty_detail = \"\\n\".join(\n                    \"rank {:3d} = {:.8f}\".format(r, n)\n                    for r, n in enumerate(self._grad_norm_buf.tolist())\n                )\n                error_detail = \"grad_norm across the workers:\\n{}\\n\".format(\n                    pretty_detail\n                )\n                # use FloatingPointError to trigger NanDetector\n                raise FloatingPointError(\n                    \"Fatal error: gradients are inconsistent between workers. \"\n                    \"Try --ddp-backend=legacy_ddp. \"\n                    \"Or are you mixing up different generation of GPUs in training?\"\n                    + \"\\n\"\n                    + \"-\" * 80\n                    + \"\\n{}\\n\".format(error_detail)\n                    + \"-\" * 80\n                )\n\n    def _reduce_and_log_stats(\n        self, logging_outputs, sample_size, grad_norm=None, ewm_loss_ratio=0\n    ):\n        # perform a bunch of arch-specific gradient metrics\n        for name, param in self.model.named_parameters():\n            if (not self.is_fsdp) or self.quiet_logs:\n                break\n            if param.grad is None:\n                continue\n            nice_name = name.replace(\"module._fsdp_wrapped_module._fpw_module.\", \"\")\n            nice_name = nice_name.replace(\"_fsdp_wrapped_module._fpw_module.\", \"\")\n            nice_name = nice_name.replace(\"._fsdp_wrapped_module.flat_param_0\", \"\")\n            nice_name = nice_name.replace(\"decoder.layers.\", \"layer\")\n            # threshold for near zeros\n            threshold = torch.finfo(param.grad.dtype).tiny * 2\n            with torch.no_grad():\n                g = param.grad\n                if hasattr(self.optimizer, \"_multiply_factor\"):\n                    g = self.optimizer._multiply_factor * g\n                norm = g.norm(p=2, dim=-1, dtype=torch.float32)\n                max_ = g.max()\n                nz = ((g > -threshold) & (g < threshold)).sum() / g.numel()\n            # priorities for printing order\n            metrics.log_scalar(f\"gnorm_{nice_name}\", norm, priority=10)\n            metrics.log_scalar(f\"gmax_{nice_name}\", max_, priority=11)\n            metrics.log_scalar(f\"gzero_{nice_name}\", nz, priority=12)\n            with torch.no_grad():\n                norm = param.norm(p=2, dim=-1, dtype=torch.float32)\n                max_ = param.max()\n                nz = ((param > -threshold) & (param < threshold)).sum() / param.numel()\n            # priorities for printing order\n            metrics.log_scalar(f\"pnorm_{nice_name}\", norm, priority=13)\n            metrics.log_scalar(f\"pmax_{nice_name}\", max_, priority=14)\n            metrics.log_scalar(f\"pzero_{nice_name}\", nz, priority=15)\n\n        # standard code\n        if grad_norm is not None and (\n            not torch.is_tensor(grad_norm) or torch.isfinite(grad_norm)\n        ):\n            metrics.log_speed(\"ups\", 1.0, priority=100, round=2)\n            metrics.log_scalar(\"gnorm\", grad_norm, priority=400, round=3)\n            metrics.log_scalar(\"ewm_loss\", self._ewm_loss, priority=700, round=2)\n            metrics.log_scalar(\"ewm_loss_ratio\", ewm_loss_ratio, priority=710, round=4)\n            metrics.log_scalar(\n                \"skipped_loss_spikes\", self._skipped_loss_spikes, priority=720\n            )\n            self._skipped_loss_spikes = 0\n            if self.cfg.optimization.clip_norm > 0:\n                metrics.log_scalar(\n                    \"clip\",\n                    torch.where(\n                        grad_norm > self.cfg.optimization.clip_norm,\n                        grad_norm.new_tensor(100),\n                        grad_norm.new_tensor(0),\n                    ),\n                    priority=500,\n                    round=1,\n                )\n\n        with metrics.aggregate() as agg:\n            if logging_outputs is not None:\n                self.task.reduce_metrics(logging_outputs, self.get_criterion())\n                del logging_outputs\n\n            # extra warning for criterions that don't properly log a loss value\n            if \"loss\" not in agg:\n                if \"loss\" not in self._warn_once:\n                    self._warn_once.add(\"loss\")\n                    logger.warning(\n                        \"Criterion.reduce_metrics did not log a 'loss' value, \"\n                        \"which may break some functionality\"\n                    )\n                metrics.log_scalar(\"loss\", -1)\n\n            # support legacy interface\n            logging_output = agg.get_smoothed_values()\n            logging_output[\"sample_size\"] = sample_size\n            for key_to_delete in [\"ppl\", \"wps\", \"wpb\", \"bsz\"]:\n                if key_to_delete in logging_output:\n                    del logging_output[key_to_delete]\n\n            return logging_output\n\n    def _log_gpu_mem_stats(self):\n        # log minimum free memory over the iteration\n        cuda_gb_allocated = torch.cuda.max_memory_allocated() / 1024 / 1024 / 1024\n        cuda_gb_reserved = torch.cuda.max_memory_reserved() / 1024 / 1024 / 1024\n        torch.cuda.reset_peak_memory_stats()\n        cuda_gb_free = self.cuda_env.total_memory_in_GB - cuda_gb_allocated\n        metrics.log_scalar(\n            \"cuda_gb_allocated\", cuda_gb_allocated, priority=1500, round=1, weight=0\n        )\n        metrics.log_scalar(\n            \"cuda_gb_reserved\", cuda_gb_reserved, priority=1500, round=1, weight=0\n        )\n        metrics.log_scalar(\n            \"cuda_gb_free\", cuda_gb_free, priority=1500, round=1, weight=0\n        )\n        # log nvidia smi stats\n        if self.cfg.common.log_nvidia_smi:\n            nvidia_smi_stats = metrics.nvidia_smi_gpu_memory_stats()\n            for key, val in nvidia_smi_stats.items():\n                metrics.log_scalar(key, val, priority=1500, round=1, weight=0)\n\n\ndef _catalog_shared_params(module, memo=None, prefix=\"\"):\n    if memo is None:\n        first_call = True\n        memo = {}\n    else:\n        first_call = False\n    for name, param in module._parameters.items():\n        param_prefix = prefix + (\".\" if prefix else \"\") + name\n        if param not in memo:\n            memo[param] = []\n        memo[param].append(param_prefix)\n    for name, m in module._modules.items():\n        if m is None:\n            continue\n        submodule_prefix = prefix + (\".\" if prefix else \"\") + name\n        _catalog_shared_params(m, memo, submodule_prefix)\n    if first_call:\n        return [x for x in memo.values() if len(x) > 1]\n\n\ndef _get_module_by_path(module, path):\n    path = path.split(\".\")\n    for name in path:\n        module = getattr(module, name)\n    return module\n\n\ndef _set_module_by_path(module, path, value):\n    path = path.split(\".\")\n    for name in path[:-1]:\n        module = getattr(module, name)\n    setattr(module, path[-1], value)\n\n\nclass SpikeError(Exception):\n    pass\n",
        "metaseq/utils.py": "# Copyright (c) Meta Platforms, Inc. and affiliates. All Rights Reserved.\n#\n# This source code is licensed under the MIT license found in the\n# LICENSE file in the root directory of this source tree.\n\nimport copy\nimport importlib\nimport logging\nimport math\nimport os\nimport random\nimport re\nimport sys\nimport warnings\nfrom itertools import accumulate\nfrom typing import List, Optional\n\nimport numpy as np\nimport torch\nimport torch.distributed as dist\nimport torch.nn.functional as F\n\nfrom metaseq.distributed import utils as distributed_utils\n\ntry:\n    from amp_C import multi_tensor_l2norm\n\n    multi_tensor_l2norm_available = True\nexcept ImportError:\n    multi_tensor_l2norm_available = False\n\n\nlogger = logging.getLogger(__name__)\n\n\ndef split_paths(paths: str) -> List[str]:\n    return paths.split(os.pathsep) if \"://\" not in paths else paths.split(\"|\")\n\n\ndef apply_to_sample(f, sample):\n    if hasattr(sample, \"__len__\") and len(sample) == 0:\n        return {}\n\n    def _apply(x):\n        if torch.is_tensor(x):\n            return f(x)\n        elif isinstance(x, dict):\n            return {key: _apply(value) for key, value in x.items()}\n        elif isinstance(x, list):\n            return [_apply(x) for x in x]\n        elif isinstance(x, tuple):\n            return tuple(_apply(x) for x in x)\n        elif isinstance(x, set):\n            return {_apply(x) for x in x}\n        else:\n            return x\n\n    return _apply(sample)\n\n\ndef move_to_cuda(sample, device=None):\n    device = device or torch.cuda.current_device()\n\n    def _move_to_cuda(tensor):\n        # non_blocking is ignored if tensor is not pinned, so we can always set\n        # to True (see github.com/PyTorchLightning/pytorch-lightning/issues/620)\n        return tensor.to(device=device, non_blocking=True)\n\n    return apply_to_sample(_move_to_cuda, sample)\n\n\ndef move_to_cpu(sample, cast_to_fp32=True):\n    def _move_to_cpu(tensor):\n        # PyTorch has poor support for half tensors (float16) on CPU.\n        # Move any such tensors to float32.\n        if cast_to_fp32 and tensor.dtype in {torch.bfloat16, torch.float16}:\n            tensor = tensor.to(dtype=torch.float32)\n        return tensor.cpu()\n\n    return apply_to_sample(_move_to_cpu, sample)\n\n\ndef make_positions(tensor, padding_idx: int):\n    \"\"\"Replace non-padding symbols with their position numbers.\n\n    Position numbers begin at padding_idx+1. Padding symbols are ignored.\n    \"\"\"\n    # The series of casts and type-conversions here are carefully\n    # balanced to both work with ONNX export and XLA. In particular XLA\n    # prefers ints, cumsum defaults to output longs, and ONNX doesn't know\n    # how to handle the dtype kwarg in cumsum.\n    mask = tensor.ne(padding_idx).int()\n    return (torch.cumsum(mask, dim=1).type_as(mask) * mask).long() + padding_idx\n\n\ndef item(tensor):\n    if hasattr(tensor, \"item\"):\n        return tensor.item()\n    if hasattr(tensor, \"__getitem__\"):\n        return tensor[0]\n    return tensor\n\n\ndef multi_tensor_l2_total_norm(grads, chunk_size=2048 * 32) -> torch.Tensor:\n    per_device_grads = {}\n    norms = []\n    for grad in grads:\n        device = grad.device\n        cur_device_grads = per_device_grads.get(device)\n        if cur_device_grads is None:\n            cur_device_grads = []\n            per_device_grads[device] = cur_device_grads\n        cur_device_grads.append(grad)\n    for device in per_device_grads.keys():\n        cur_device_grads = per_device_grads[device]\n        if device.type == \"cuda\":\n            # TODO(msb) return has_inf\n            has_inf = torch.zeros((1, 1), dtype=torch.int, device=device)\n            with torch.cuda.device(device):\n                norm = multi_tensor_l2norm(\n                    chunk_size, has_inf, [cur_device_grads], False\n                )\n            norms.append(norm[0].to(torch.cuda.current_device()))\n        else:\n            norms += [torch.norm(g, p=2, dtype=torch.float32) for g in cur_device_grads]\n    total_norm = torch.norm(torch.stack(norms))\n    return total_norm\n\n\nnorm_type2_reduce_op = {\"l2\": dist.ReduceOp.SUM, \"inf\": dist.ReduceOp.MAX}\n\n\n@torch.no_grad()\ndef clip_grad_norm_(\n    params, max_norm, norm_type=\"l2\", aggregate_norm_fn=None, device=None\n) -> torch.Tensor:\n    def grad_exists(p):\n        return p is not None and getattr(p, \"grad\", None) is not None\n\n    if isinstance(params, torch.Tensor):\n        params = [params]\n    params = list(params)\n    params = list(filter(grad_exists, params))\n    grads, sharded_grads = [], []\n\n    if device is None:\n        if torch.cuda.is_available():\n            # param/grads could be on CPU if using CPU offloading, but we want\n            # everything on GPU if possible\n            device = torch.device(\"cuda:{}\".format(torch.cuda.current_device()))\n        elif len(params) > 0:\n            device = params[0].device  # could be \"xla\"\n        else:\n            device = torch.device(\"cpu\")\n\n    def norm(t, n_type):\n        if n_type == \"l2\":\n            return torch.norm(t, p=2, dtype=torch.float32)\n        elif n_type == \"inf\":\n            return torch.norm(t, p=float(\"inf\"), dtype=torch.float32)\n        else:\n            raise ValueError(\n                f\"Invalid clip_norm_type: {n_type}! Please pass either 'l2' or 'inf'!\"\n            )\n\n    for p in params:\n        if hasattr(p, \"_is_sharded\"):\n            sharded_grads.append(p.grad.detach())\n        else:\n            grads.append(p.grad.detach())\n\n    if len(grads) == 0:\n        total_norm = torch.tensor(0.0, dtype=torch.float32, device=device)\n    elif len(grads) == 1:\n        total_norm = norm(grads[0], norm_type)\n    else:\n        if (\n            multi_tensor_l2norm_available\n            and norm_type == \"l2\"\n            and grads[0].dtype != torch.bfloat16\n        ):\n            total_norm = multi_tensor_l2_total_norm(grads)\n        else:\n            if (\n                torch.cuda.is_available()\n                and norm_type == \"l2\"\n                and grads[0].dtype != torch.bfloat16\n            ):\n                warnings.warn(\n                    \"amp_C fused kernels unavailable, disabling multi_tensor_l2norm; \"\n                    \"you may get better performance by installing NVIDIA's apex library\"\n                )\n            total_norm = norm(\n                torch.stack([norm(g, norm_type) for g in grads]), norm_type\n            )\n\n    # calculate split_norm and all_reduce with other workers\n    norms = [total_norm]\n    for split_grads in [sharded_grads]:\n        if len(split_grads) == 0:\n            continue\n        split_norm = norm(\n            torch.stack([norm(g, norm_type) for g in split_grads]), norm_type\n        )\n        if dist.is_initialized():\n            reduce_op = norm_type2_reduce_op[norm_type]\n            if norm_type == \"l2\":\n                split_norm.pow_(2)\n            dist.all_reduce(\n                split_norm,\n                group=distributed_utils.get_data_parallel_group(),\n                op=reduce_op,\n            )\n            if norm_type == \"l2\":\n                split_norm.sqrt_()\n        norms.append(split_norm)\n\n    if len(norms) > 1:\n        total_norm = norm(torch.stack(norms), norm_type)\n\n    if aggregate_norm_fn is not None:\n        total_norm = aggregate_norm_fn(total_norm)\n\n    if max_norm > 0:\n        max_norm = float(max_norm)\n        clip_coef = (max_norm / (total_norm + 1e-6)).clamp_(max=1)\n        for g in grads + sharded_grads:\n            g.mul_(clip_coef)\n    return total_norm\n\n\ndef fill_with_neg_inf(t):\n    \"\"\"FP16-compatible function that fills a tensor with -inf.\"\"\"\n    return t.float().fill_(float(\"-inf\")).type_as(t)\n\n\ndef _match_types(arg1, arg2):\n    \"\"\"Convert the numerical argument to the same type as the other argument\"\"\"\n\n    def upgrade(arg_number, arg_structure):\n        if isinstance(arg_structure, tuple):\n            return tuple([arg_number] * len(arg_structure))\n        elif isinstance(arg_structure, dict):\n            arg = copy.deepcopy(arg_structure)\n            for k in arg:\n                arg[k] = upgrade(arg_number, arg_structure[k])\n            return arg\n        else:\n            return arg_number\n\n    if isinstance(arg1, float) or isinstance(arg1, int):\n        return upgrade(arg1, arg2), arg2\n    elif isinstance(arg2, float) or isinstance(arg2, int):\n        return arg1, upgrade(arg2, arg1)\n\n    return arg1, arg2\n\n\ndef resolve_max_positions(*args):\n    \"\"\"Resolve max position constraints from multiple sources.\"\"\"\n\n    def map_value_update(d1, d2):\n        updated_value = copy.deepcopy(d1)\n        for key in d2:\n            if key not in updated_value:\n                updated_value[key] = d2[key]\n            else:\n                updated_value[key] = min(d1[key], d2[key])\n        return updated_value\n\n    def nullsafe_min(l):\n        minim = None\n        for item in l:\n            if minim is None:\n                minim = item\n            elif item is not None and item < minim:\n                minim = item\n        return minim\n\n    max_positions = None\n    for arg in args:\n        if max_positions is None:\n            max_positions = arg\n        elif arg is not None:\n            max_positions, arg = _match_types(max_positions, arg)\n            if isinstance(arg, float) or isinstance(arg, int):\n                max_positions = min(max_positions, arg)\n            elif isinstance(arg, dict):\n                max_positions = map_value_update(max_positions, arg)\n            else:\n                max_positions = tuple(map(nullsafe_min, zip(max_positions, arg)))\n\n    return max_positions\n\n\ndef import_user_module(args):\n    module_path = getattr(args, \"user_dir\", None)\n    if module_path is not None:\n        module_path = os.path.abspath(args.user_dir)\n        if not os.path.exists(module_path) and not os.path.isfile(\n            os.path.dirname(module_path)\n        ):\n            metaseq_rel_path = os.path.join(os.path.dirname(__file__), args.user_dir)\n            if os.path.exists(metaseq_rel_path):\n                module_path = metaseq_rel_path\n            else:\n                metaseq_rel_path = os.path.join(\n                    os.path.dirname(__file__), \"..\", args.user_dir\n                )\n                if os.path.exists(metaseq_rel_path):\n                    module_path = metaseq_rel_path\n                else:\n                    raise FileNotFoundError(module_path)\n\n        # ensure that user modules are only imported once\n        import_user_module.memo = getattr(import_user_module, \"memo\", set())\n        if module_path not in import_user_module.memo:\n            import_user_module.memo.add(module_path)\n\n            module_parent, module_name = os.path.split(module_path)\n            if module_name not in sys.modules:\n                sys.path.insert(0, module_parent)\n                importlib.import_module(module_name)\n            else:\n                raise ImportError(\n                    \"Failed to import --user-dir={} because the corresponding module name \"\n                    \"({}) is not globally unique. Please rename the directory to \"\n                    \"something unique and try again.\".format(module_path, module_name)\n                )\n\n\ndef softmax(x, dim: int):\n    return F.softmax(x, dim=dim, dtype=torch.float32)\n\n\ndef log_softmax(x, dim: int):\n    return F.log_softmax(x, dim=dim, dtype=torch.float32)\n\n\ndef get_perplexity(loss, round=2, base=2):\n    from metaseq.logging.meters import safe_round\n\n    if loss is None:\n        return 0.0\n    try:\n        return safe_round(base**loss, round)\n    except OverflowError:\n        return float(\"inf\")\n\n\ndef has_parameters(module):\n    try:\n        next(module.parameters())\n        return True\n    except StopIteration:\n        return False\n\n\ndef get_rng_state():\n    state = {\"torch_rng_state\": torch.get_rng_state()}\n    if torch.cuda.is_available():\n        state[\"cuda_rng_state\"] = torch.cuda.get_rng_state()\n    return state\n\n\ndef set_rng_state(state):\n    torch.set_rng_state(state[\"torch_rng_state\"])\n    if torch.cuda.is_available():\n        torch.cuda.set_rng_state(state[\"cuda_rng_state\"])\n\n\nclass set_torch_seed(object):\n    def __init__(self, seed):\n        assert isinstance(seed, int)\n        self.rng_state = get_rng_state()\n\n        torch.manual_seed(seed)\n        if torch.cuda.is_available():\n            torch.cuda.manual_seed(seed)\n\n    def __enter__(self):\n        return self\n\n    def __exit__(self, *exc):\n        set_rng_state(self.rng_state)\n\n\nclass set_rank_seed(set_torch_seed):\n    def __init__(self, seed, update):\n        if dist.is_initialized():\n            rank, world = dist.get_rank(), dist.get_world_size()\n        else:\n            rank, world = 0, 1\n        rank_seed = seed + update * world + rank\n\n        # torch\n        super(set_rank_seed, self).__init__(rank_seed)\n\n        # numpy\n        self.np_state = np.random.get_state()\n        np.random.seed(rank_seed)\n\n        # random\n        self.random_state = random.getstate()\n        random.seed(rank_seed)\n\n    def __exit__(self, *exc):\n        random.setstate(self.random_state)\n        np.random.set_state(self.np_state)\n        super(set_rank_seed, self).__exit__(*exc)\n\n\nclass CudaEnvironment(object):\n    def __init__(self):\n        cur_device = torch.cuda.current_device()\n        prop = torch.cuda.get_device_properties(\"cuda:{}\".format(cur_device))\n        self.name = prop.name\n        self.major = prop.major\n        self.minor = prop.minor\n        self.total_memory_in_GB = prop.total_memory / 1024 / 1024 / 1024\n\n    @staticmethod\n    def pretty_print_cuda_env_list(cuda_env_list):\n        \"\"\"\n        Given a list of CudaEnviorments, pretty print them\n        \"\"\"\n        num_workers = len(cuda_env_list)\n        center = \"CUDA enviroments for all {} workers\".format(num_workers)\n        banner_len = 40 - len(center) // 2\n        first_line = \"*\" * banner_len + center + \"*\" * banner_len\n        logger.info(first_line)\n        for r, env in enumerate(cuda_env_list):\n            logger.info(\n                \"rank {:3d}: \".format(r)\n                + \"capabilities = {:2d}.{:<2d} ; \".format(env.major, env.minor)\n                + \"total memory = {:.3f} GB ; \".format(env.total_memory_in_GB)\n                + \"name = {:40s}\".format(env.name)\n            )\n        logger.info(first_line)\n\n\n# TODO[susan]: Move this to metaseq-internal where it is currently used\ndef remove_prefix(text: str, prefix: str):\n    if text.startswith(prefix):\n        return text[len(prefix) :]\n    return text\n\n\n# TODO[susan]: Move this to metaseq-internal where it is currently used\ndef print_r0(*x, file=None):\n    if not torch.distributed.is_initialized() or torch.distributed.get_rank() == 0:\n        print(*x, file=file, flush=True)\n\n\ndef get_random_port():\n    old_state = random.getstate()\n    random.seed()\n    port = random.randint(10000, 20000)\n    random.setstate(old_state)\n    return port\n\n\ndef floating_point_precision_convertor(\n    x, fp16: bool, memory_efficient_fp16: bool, bf16: bool\n):\n    \"\"\"\n    Convert a tensor x into the desired dtype.\n\n    Also sanity checks combinations of options.\n    \"\"\"\n    if memory_efficient_fp16:\n        assert not bf16, \"Do not combined bf16 with memory_efficient_fp16.\"\n    if bf16:\n        assert fp16, \"Setting --bf16 requires also setting --fp16 for legacy reasons.\"\n    if not fp16 and not bf16:\n        return x\n    if not memory_efficient_fp16:\n        # original parameters stay in fp32 and are converted by fairscale\n        return x\n    elif bf16:\n        return x.bfloat16()\n    else:\n        return x.half()\n\n\ndef get_model_init_dtype(args):\n    if getattr(args, \"memory_efficient_fp16\", False) or getattr(\n        args, \"inference\", False\n    ):\n        return torch.bfloat16 if getattr(args, \"bf16\", False) else torch.half\n    return torch.float32\n\n\ndef get_precise_epoch(epoch: Optional[int], count: int, iterator_size: int) -> float:\n    return (\n        epoch - 1 + (count + 1) / float(iterator_size)\n        if epoch is not None and iterator_size > 0\n        else None\n    )\n\n\ndef tokenize_line(line):\n    line = re.compile(r\"\\s+\").sub(\" \", line)\n    line = line.strip()\n    return line.split()\n\n\ndef init_method_normal(sigma, truncate_init=False):\n    \"\"\"Init method based on N(0, sigma).\"\"\"\n\n    def init_(tensor):\n        if sigma <= 1e-8:  # effectively 0\n            return torch.nn.init.zeros_(tensor)\n        if truncate_init:\n            return torch.nn.init.trunc_normal_(\n                tensor, mean=0.0, std=sigma, a=-3 * sigma, b=3 * sigma\n            )\n        else:\n            return torch.nn.init.normal_(tensor, mean=0.0, std=sigma)\n\n    return init_\n\n\ndef scaled_init_method_normal(sigma, num_layers, truncate_init=False):\n    \"\"\"Init method based on N(0, sigma/sqrt(2*num_layers).\"\"\"\n    std = sigma / math.sqrt(2.0 * num_layers)\n\n    def init_(tensor):\n        if sigma <= 1e-8:  # effectively 0\n            return torch.nn.init.zeros_(tensor)\n        if truncate_init:\n            return torch.nn.init.trunc_normal_(\n                tensor, mean=0.0, std=std, a=-3 * std, b=3 * std\n            )\n        else:\n            return torch.nn.init.normal_(tensor, mean=0.0, std=std)\n\n    return init_\n\n\ndef get_token_to_word_mapping(tokens, exclude_list):\n    n = len(tokens)\n    word_start = [int(token not in exclude_list) for token in tokens]\n    word_idx = list(accumulate(word_start))\n    token_to_word = {i: word_idx[i] for i in range(n)}\n    return token_to_word\n\n\ndef extract_hard_alignment(attn, src_sent, tgt_sent, pad, eos):\n    tgt_valid = (\n        ((tgt_sent != pad) & (tgt_sent != eos)).nonzero(as_tuple=False).squeeze(dim=-1)\n    )\n    src_invalid = (\n        ((src_sent == pad) | (src_sent == eos)).nonzero(as_tuple=False).squeeze(dim=-1)\n    )\n    src_token_to_word = get_token_to_word_mapping(src_sent, [eos, pad])\n    tgt_token_to_word = get_token_to_word_mapping(tgt_sent, [eos, pad])\n    alignment = []\n    if len(tgt_valid) != 0 and len(src_invalid) < len(src_sent):\n        attn_valid = attn[tgt_valid]\n        attn_valid[:, src_invalid] = float(\"-inf\")\n        _, src_indices = attn_valid.max(dim=1)\n        for tgt_idx, src_idx in zip(tgt_valid, src_indices):\n            alignment.append(\n                (\n                    src_token_to_word[src_idx.item()] - 1,\n                    tgt_token_to_word[tgt_idx.item()] - 1,\n                )\n            )\n    return alignment\n\n\ndef extract_soft_alignment(attn, src_sent, tgt_sent, pad, eos):\n    tgt_valid = ((tgt_sent != pad)).nonzero(as_tuple=False)\n    src_valid = ((src_sent != pad)).nonzero(as_tuple=False).squeeze(dim=-1)\n    alignment = []\n    if len(tgt_valid) != 0 and len(src_valid) != 0:\n        attn_valid = attn[tgt_valid, src_valid]\n        alignment = [\n            [\"{:.6f}\".format(p) for p in src_probs.tolist()] for src_probs in attn_valid\n        ]\n    return alignment\n",
        "setup.py": "#!/usr/bin/env python3\n# Copyright (c) Meta Platforms, Inc. and affiliates. All Rights Reserved.\n#\n# This source code is licensed under the MIT license found in the\n# LICENSE file in the root directory of this source tree.\n\nimport os\nimport sys\n\nimport torch\nfrom setuptools import Extension, find_packages, setup\nfrom torch.utils.cpp_extension import (\n    CppExtension,\n    CUDAExtension,\n    BuildExtension,\n    CUDA_HOME,\n)\n\nif sys.version_info < (3, 6):\n    sys.exit(\"Sorry, Python >= 3.6 is required for metaseq.\")\n\n\ndef write_version_py():\n    with open(os.path.join(\"metaseq\", \"version.txt\")) as f:\n        version = f.read().strip()\n\n    # append latest commit hash to version string\n    try:\n        sha = (\n            subprocess.check_output([\"git\", \"rev-parse\", \"HEAD\"])\n            .decode(\"ascii\")\n            .strip()\n        )\n        version += \"+\" + sha[:7]\n    except Exception:\n        pass\n\n    # write version info to metaseq/version.py\n    with open(os.path.join(\"metaseq\", \"version.py\"), \"w\") as f:\n        f.write('__version__ = \"{}\"\\n'.format(version))\n    return version\n\n\nversion = write_version_py()\n\nwith open(\"README.md\") as f:\n    readme = f.read()\n\nif sys.platform == \"darwin\":\n    extra_compile_args = [\"-stdlib=libc++\", \"-O3\"]\nelse:\n    extra_compile_args = [\"-std=c++11\", \"-O3\"]\n\n\nclass NumpyExtension(Extension):\n    \"\"\"Source: https://stackoverflow.com/a/54128391\"\"\"\n\n    def __init__(self, *args, **kwargs):\n        self.__include_dirs = []\n        super().__init__(*args, **kwargs)\n\n    @property\n    def include_dirs(self):\n        import numpy\n\n        return self.__include_dirs + [numpy.get_include()]\n\n    @include_dirs.setter\n    def include_dirs(self, dirs):\n        self.__include_dirs = dirs\n\n\nextension_modules = [\n    NumpyExtension(\n        \"metaseq.data.data_utils_fast\",\n        sources=[\"metaseq/data/data_utils_fast.pyx\"],\n        language=\"c++\",\n        extra_compile_args=extra_compile_args,\n    ),\n    NumpyExtension(\n        \"metaseq.data.token_block_utils_fast\",\n        sources=[\"metaseq/data/token_block_utils_fast.pyx\"],\n        language=\"c++\",\n        extra_compile_args=extra_compile_args,\n    ),\n]\n\n# TODO: Figure out how to actually gate this properly and still get CircleCI to work.\n# By default, include megatron kernels unless --global-option=\"--no_megatron\" is passed in to pip install.\nif \"--no_megatron\" not in sys.argv:\n    # Reference:\n    # https://github.com/ngoyal2707/Megatron-LM/commit/9a16189ab1b5537205c708f8c8f952f2ae2ae72b\n    extension_modules.append(\n        CppExtension(\n            \"metaseq.modules.megatron.fused_kernels.scaled_upper_triang_masked_softmax_cuda\",\n            sources=[\n                \"metaseq/modules/megatron/fused_kernels/scaled_upper_triang_masked_softmax.cpp\",\n                \"metaseq/modules/megatron/fused_kernels/scaled_upper_triang_masked_softmax_cuda.cu\",\n            ],\n            extra_compile_args={\n                \"cxx\": [\"-O3\"],\n                \"nvcc\": [\n                    \"-O3\",\n                    \"--use_fast_math\",\n                    \"-U__CUDA_NO_HALF_OPERATORS__\",\n                    \"-U__CUDA_NO_HALF_CONVERSIONS__\",\n                    \"--expt-relaxed-constexpr\",\n                    \"--expt-extended-lambda\",\n                ],\n            },\n        )\n    )\n    extension_modules.append(\n        CppExtension(\n            \"metaseq.modules.megatron.fused_kernels.scaled_masked_softmax_cuda\",\n            sources=[\n                \"metaseq/modules/megatron/fused_kernels/scaled_masked_softmax.cpp\",\n                \"metaseq/modules/megatron/fused_kernels/scaled_masked_softmax_cuda.cu\",\n            ],\n            extra_compile_args={\n                \"cxx\": [\"-O3\"],\n                \"nvcc\": [\n                    \"-O3\",\n                    \"--use_fast_math\",\n                    \"-U__CUDA_NO_HALF_OPERATORS__\",\n                    \"-U__CUDA_NO_HALF_CONVERSIONS__\",\n                    \"--expt-relaxed-constexpr\",\n                    \"--expt-extended-lambda\",\n                ],\n            },\n        )\n    )\nelse:\n    print(\"*** Skipping megatron kernel installation... ***\")\n    sys.argv.remove(\"--no_megatron\")\n\n# By default, include apex kernels unless --global-option=\"--no_apex\" is passed in to pip install.\nif \"--no_apex\" not in sys.argv:\n    # TODO[susanz]: not including --no-cache-dir anymore?\n    if CUDA_HOME is None:\n        raise RuntimeError(\n            f\"Building apex kernels was requested, but nvcc was not found. \"\n            \"Are you sure your environment has nvcc available?  \"\n            \"If you're installing within a container from https://hub.docker.com/r/pytorch/pytorch, \"\n            \"only images whose names contain 'devel' will provide nvcc.\"\n        )\n\n    print(\"\\n\\ntorch.__version__  = {}\\n\\n\".format(torch.__version__))\n    TORCH_MAJOR = int(torch.__version__.split(\".\")[0])\n    TORCH_MINOR = int(torch.__version__.split(\".\")[1])\n    if TORCH_MAJOR == 0:\n        # TODO[susanz]: Gate this to Pytorch 1.6 or later?\n        raise RuntimeError(\n            \"Apex kernels requires Pytorch 1.0 or later, \"\n            \"found torch.__version__ = {}\".format(torch.__version__)\n        )\n\n    # --global-option=\"--cpp_ext\"\n    extension_modules.append(\n        CppExtension(\n            name=\"apex_C\", sources=[\"metaseq/modules/apex/flatten_unflatten.cpp\"]\n        )\n    )\n\n    # --global-option=\"--cuda_ext\"\n    extension_modules.append(\n        CUDAExtension(\n            name=\"amp_C\",\n            sources=[\n                \"metaseq/modules/apex/amp_C_frontend.cpp\",\n                \"metaseq/modules/apex/multi_tensor_l2norm_kernel.cu\",\n                \"metaseq/modules/apex/multi_tensor_l2norm_kernel_mp.cu\",\n                \"metaseq/modules/apex/multi_tensor_l2norm_scale_kernel.cu\",\n                \"metaseq/modules/apex/multi_tensor_adam.cu\",\n            ],\n            extra_compile_args={\n                \"cxx\": [\"-O3\"],\n                \"nvcc\": [\n                    \"-lineinfo\",\n                    \"-O3\",\n                    \"--use_fast_math\",\n                ],\n            },\n        )\n    )\n    extension_modules.append(\n        CUDAExtension(\n            name=\"fused_layer_norm_cuda\",\n            sources=[\n                \"metaseq/modules/apex/layer_norm_cuda.cpp\",\n                \"metaseq/modules/apex/layer_norm_cuda_kernel.cu\",\n            ],\n            extra_compile_args={\n                \"cxx\": [\"-O3\"],\n                \"nvcc\": [\"-maxrregcount=50\", \"-O3\", \"--use_fast_math\"],\n            },\n        )\n    )\n    extension_modules.append(\n        CUDAExtension(\n            name=\"fused_dense_cuda\",\n            sources=[\n                \"metaseq/modules/apex/fused_dense.cpp\",\n                \"metaseq/modules/apex/fused_dense_cuda.cu\",\n            ],\n            extra_compile_args={\n                \"cxx\": [\"-O3\"],\n                \"nvcc\": [\"-O3\"],\n            },\n        )\n    )\n    # --global-option=\"--deprecated_fused_adam\"\n    extension_modules.append(\n        CUDAExtension(\n            name=\"fused_adam_cuda\",\n            sources=[\n                \"metaseq/modules/apex/fused_adam_cuda.cpp\",\n                \"metaseq/modules/apex/fused_adam_cuda_kernel.cu\",\n            ],\n            # include_dirs=[os.path.join(this_dir, \"csrc\")],\n            extra_compile_args={\n                \"cxx\": [\"-O3\"],\n                \"nvcc\": [\"-O3\", \"--use_fast_math\"],\n            },\n        )\n    )\nelse:\n    print(\"*** Skipping apex kernel installation... ***\")\n    sys.argv.remove(\"--no_apex\")\n\n\nif \"clean\" in sys.argv[1:]:\n    # Source: https://bit.ly/2NLVsgE\n    print(\"deleting Cython files...\")\n    import subprocess\n\n    subprocess.run(\n        [\"rm -f metaseq/*.so metaseq/**/*.so metaseq/*.pyd metaseq/**/*.pyd\"],\n        shell=True,\n    )\n\n\ndef do_setup():\n    setup(\n        name=\"metaseq\",\n        version=version,\n        description=\"MetaSeq, a framework for large language models, from Meta\",\n        url=\"https://github.com/facebookresearch/metaseq\",\n        long_description=readme,\n        long_description_content_type=\"text/markdown\",\n        setup_requires=[\n            \"cython\",\n            'numpy; python_version>=\"3.7\"',\n            \"setuptools>=18.0\",\n        ],\n        install_requires=[\n            # protobuf version pinned due to tensorboard not pinning a version.\n            #  https://github.com/protocolbuffers/protobuf/issues/10076\n            \"protobuf==3.20.2\",\n            # \"click==8.0.4\",\n            \"cython\",\n            'dataclasses; python_version<\"3.7\"',\n            # \"editdistance\",\n            \"fire\",\n            \"flask>=2.2.5\",  # for api\n            \"hydra-core>=1.1.0,<1.2\",\n            \"ipdb\",\n            \"ipython\",\n            \"Jinja2==3.1.1\",  # for evals\n            \"markupsafe\",  # for evals\n            \"more_itertools\",\n            \"ninja\",\n            'numpy; python_version>=\"3.7\"',\n            \"omegaconf<=2.1.1\",\n            \"portalocker>=2.5\",\n            \"pre-commit\",\n            \"pytest\",\n            \"pytest-regressions\",\n            \"regex\",\n            \"scikit-learn\",  # for evals\n            \"sacrebleu\",  # for evals\n            \"tensorboard==2.8.0\",\n            \"timeout-decorator\",\n            \"tokenizers\",\n            \"torch\",\n            \"tqdm\",\n            \"typing_extensions\",\n        ],\n        packages=find_packages(\n            exclude=[\n                \"scripts\",\n                \"scripts.*\",\n                \"tests\",\n                \"tests.*\",\n            ]\n        ),\n        include_package_data=True,\n        zip_safe=False,\n        extras_require={\n            # install via: pip install -e \".[dev]\"\n            \"dev\": [\n                \"flake8\",\n                \"black==22.3.0\",\n                \"aim>=3.9.4\",\n                \"azure-storage-blob\",\n                \"mypy\",\n            ],\n            # install via: pip install -e \".[test]\"\n            \"test\": [\n                \"iopath\",\n                \"transformers\",\n                \"pyarrow\",\n                \"boto3\",\n                \"pandas\",\n                \"bitsandbytes\",\n            ],\n            # install via: pip install -e \".[multimodal]\"\n            \"multimodal\": [\n                \"albumentations\",\n                \"dalle_pytorch\",\n                \"einops\",\n                \"matplotlib==3.5.0\",\n                \"pytorchvideo==0.1.5\",\n                \"wandb\",\n                \"webdataset==0.1.103\",\n            ],\n        },\n        ext_modules=extension_modules,\n        test_suite=\"tests\",\n        entry_points={\n            \"console_scripts\": [\n                \"metaseq-train = metaseq.cli.train:cli_main\",\n                \"metaseq-validate = metaseq.cli.validate:cli_main\",\n                \"opt-baselines = metaseq.launcher.opt_baselines:cli_main\",\n                \"metaseq-api-local = metaseq.cli.interactive_hosted:cli_main\",\n            ],\n        },\n        cmdclass={\"build_ext\": BuildExtension},\n    )\n\n\nif __name__ == \"__main__\":\n    do_setup()\n",
        "tests/__init__.py": "",
        "tests/distributed/__init__.py": "",
        "tests/distributed/test_utils.py": "# Copyright (c) Meta Platforms, Inc. and affiliates. All Rights Reserved.\n#\n# This source code is licensed under the MIT license found in the\n# LICENSE file in the root directory of this source tree.\n\nimport functools\nimport sys\nimport unittest\n\nimport torch\n\nfrom metaseq.distributed import utils as distributed_utils\nfrom .utils import objects_are_equal, spawn_and_init\n\n\nclass DistributedTest(unittest.TestCase):\n    def setUp(self):\n        if not torch.cuda.is_available():\n            raise unittest.SkipTest(\"CUDA not available, skipping test\")\n        if sys.platform == \"win32\":\n            raise unittest.SkipTest(\"NCCL doesn't support Windows, skipping test\")\n        if torch.cuda.device_count() < 2:\n            raise unittest.SkipTest(\"distributed tests require 2+ GPUs, skipping\")\n\n\nclass TestBroadcastObject(DistributedTest):\n    def test_str(self):\n        spawn_and_init(\n            functools.partial(\n                TestBroadcastObject._test_broadcast_object, \"hello world\"\n            ),\n            world_size=2,\n        )\n\n    def test_tensor(self):\n        spawn_and_init(\n            functools.partial(\n                TestBroadcastObject._test_broadcast_object,\n                torch.rand(5),\n            ),\n            world_size=2,\n        )\n\n    def test_complex(self):\n        spawn_and_init(\n            functools.partial(\n                TestBroadcastObject._test_broadcast_object,\n                {\n                    \"a\": \"1\",\n                    \"b\": [2, torch.rand(2, 3), 3],\n                    \"c\": (torch.rand(2, 3), 4),\n                    \"d\": {5, torch.rand(5)},\n                    \"e\": torch.rand(5),\n                    \"f\": torch.rand(5).int().cuda(),\n                },\n            ),\n            world_size=2,\n        )\n\n    @staticmethod\n    def _test_broadcast_object(ref_obj, rank, group):\n        obj = distributed_utils.broadcast_object(\n            ref_obj if rank == 0 else None, src_rank=0, group=group\n        )\n        assert objects_are_equal(ref_obj, obj)\n\n\nclass TestAllGatherList(DistributedTest):\n    def test_str_equality(self):\n        spawn_and_init(\n            functools.partial(\n                TestAllGatherList._test_all_gather_list_equality,\n                \"hello world\",\n            ),\n            world_size=2,\n        )\n\n    def test_tensor_equality(self):\n        spawn_and_init(\n            functools.partial(\n                TestAllGatherList._test_all_gather_list_equality,\n                torch.rand(5),\n            ),\n            world_size=2,\n        )\n\n    def test_complex_equality(self):\n        spawn_and_init(\n            functools.partial(\n                TestAllGatherList._test_all_gather_list_equality,\n                {\n                    \"a\": \"1\",\n                    \"b\": [2, torch.rand(2, 3), 3],\n                    \"c\": (torch.rand(2, 3), 4),\n                    \"d\": {5, torch.rand(5)},\n                    \"e\": torch.rand(5),\n                    \"f\": torch.rand(5).int(),\n                },\n            ),\n            world_size=2,\n        )\n\n    @staticmethod\n    def _test_all_gather_list_equality(ref_obj, rank, group):\n        objs = distributed_utils.all_gather_list(ref_obj, group)\n        for obj in objs:\n            assert objects_are_equal(ref_obj, obj)\n\n    def test_rank_tensor(self):\n        spawn_and_init(\n            TestAllGatherList._test_all_gather_list_rank_tensor, world_size=2\n        )\n\n    @staticmethod\n    def _test_all_gather_list_rank_tensor(rank, group):\n        obj = torch.tensor([rank])\n        objs = distributed_utils.all_gather_list(obj, group)\n        for i, obj in enumerate(objs):\n            assert obj.item() == i\n\n\nif __name__ == \"__main__\":\n    unittest.main()\n",
        "tests/distributed/utils.py": "# Copyright (c) Meta Platforms, Inc. and affiliates. All Rights Reserved.\n#\n# This source code is licensed under the MIT license found in the\n# LICENSE file in the root directory of this source tree.\n\nimport functools\nimport tempfile\n\nimport torch\n\n\ndef spawn_and_init(fn, world_size, args=None):\n    if args is None:\n        args = ()\n    with tempfile.NamedTemporaryFile(delete=False) as tmp_file:\n        torch.multiprocessing.spawn(\n            fn=functools.partial(init_and_run, fn, args),\n            args=(\n                world_size,\n                tmp_file.name,\n            ),\n            nprocs=world_size,\n            join=True,\n        )\n\n\ndef distributed_init(rank, world_size, tmp_file):\n    torch.distributed.init_process_group(\n        backend=\"nccl\",\n        init_method=\"file://{}\".format(tmp_file),\n        world_size=world_size,\n        rank=rank,\n    )\n    torch.cuda.set_device(rank)\n\n\ndef init_and_run(fn, args, rank, world_size, tmp_file):\n    distributed_init(rank, world_size, tmp_file)\n    group = torch.distributed.new_group()\n    fn(rank, group, *args)\n\n\ndef objects_are_equal(a, b) -> bool:\n    if type(a) is not type(b):\n        return False\n    if isinstance(a, dict):\n        if set(a.keys()) != set(b.keys()):\n            return False\n        for k in a.keys():\n            if not objects_are_equal(a[k], b[k]):\n                return False\n        return True\n    elif isinstance(a, (list, tuple, set)):\n        if len(a) != len(b):\n            return False\n        return all(objects_are_equal(x, y) for x, y in zip(a, b))\n    elif torch.is_tensor(a):\n        return (\n            a.size() == b.size()\n            and a.dtype == b.dtype\n            and a.device == b.device\n            and torch.all(a == b)\n        )\n    else:\n        return a == b\n",
        "tests/file_io/async_download_test.py": "#!/usr/bin/env python3\n\n# Copyright (c) Meta Platforms, Inc. and affiliates. All Rights Reserved.\n#\n# This source code is licensed under the MIT license found in the\n# LICENSE file in the root directory of this source tree.\n\n\"\"\"\nUtility for test cached file locking during concurrent downloads.\n\"\"\"\n\nimport array\nimport logging\nimport multiprocessing\nimport os\nimport random\nimport re\nfrom tempfile import TemporaryDirectory\nimport uuid\n\nfrom metaseq.file_io.azure_blob import AzureBlobPathHandler\n\nlogging.basicConfig(\n    format=\"%(asctime)s %(message)s\", datefmt=\"%m/%d/%Y %I:%M:%S %p\", level=logging.INFO\n)\n\n\nENV_SAS_TOKEN = \"AZURE_STORAGE_SAS_TOKEN\"\nENV_BASE_PATH = \"AZURE_TEST_BASE_PATH\"\n\n\ndef get_base_path():\n    base_path = os.environ.get(ENV_BASE_PATH, None)\n    assert base_path is not None, f\"Required env var not specified: {ENV_BASE_PATH}\"\n    # Set a unique base path for each test run\n    return os.path.join(base_path, str(uuid.uuid4()))\n\n\ndef create_remote_test_file(handler, path, length=4096):\n    value = random.randint(0, 16)\n    with handler._open(path, \"wb\") as f:\n        array.array(\"b\", [value] * length).tofile(f)\n    return path\n\n\ndef download_files(handler, remote_path, i):\n    logging.info(f\"Downloading {remote_path} from worker {i}...\")\n    return handler._get_local_path(remote_path, force=True)\n\n\nclass TestDriver:\n    def test(self):\n        num_files = 4\n        max_workers = 8\n        file_size_bytes = 2 * 1024 * 1024\n        remote_path_base = get_base_path()\n\n        with TemporaryDirectory() as cache_dir:\n            handler = AzureBlobPathHandler(cache_dir=cache_dir)\n            remote_paths = []\n            try:\n                # Create a remote directory with N files\n                for i in range(num_files):\n                    remote_path = os.path.join(\n                        remote_path_base, \"testdir\", f\"file{i}.bin\"\n                    )\n                    create_remote_test_file(\n                        handler, remote_path, length=file_size_bytes\n                    )\n                    remote_paths.append(remote_path)\n\n                # Concurrently download all of them to the same cache_dir\n                processes = []\n                ctx = multiprocessing.get_context(\"spawn\")\n                for i in range(max_workers):\n                    process = ctx.Process(\n                        target=download_files,\n                        args=(handler, os.path.dirname(remote_paths[0]), i),\n                    )\n                    process.start()\n                    processes.append(process)\n\n                # Join\n                for process in processes:\n                    process.join()\n\n                # Find the expected local path\n                m = re.match(\"(az|blob)://[^/]+/[^/]+/(.+)\", remote_paths[0])\n                assert m is not None, f\"Invalid remote path: {remote_paths[0]}\"\n                cache_path = os.path.dirname(\n                    os.path.join(cache_dir, \"blob_cache\", m.groups(1)[1])\n                )\n\n                # Make sure all files were successfully downloaded\n                actual = {f for f in os.listdir(cache_path) if not f.endswith(\".lock\")}\n                expected = {f\"file{i}.bin\" for i in range(num_files)}\n                assert actual == set(expected), f\"{expected}\\n{actual}\"\n            except FileNotFoundError:\n                from pdb import set_trace\n\n                set_trace()\n            finally:\n                if remote_paths:\n                    logging.info(f\"Cleaning up {len(remote_paths)} files...\")\n                    for remote_path in remote_paths:\n                        handler._rm(remote_path)\n\n\nif __name__ == \"__main__\":\n    assert (\n        ENV_SAS_TOKEN in os.environ and ENV_BASE_PATH in os.environ\n    ), \"Missing required env vars\"\n    TestDriver().test()\n",
        "tests/file_io/async_torch_test.py": "#!/usr/bin/env python3\n\n# Copyright (c) Meta Platforms, Inc. and affiliates. All Rights Reserved.\n#\n# This source code is licensed under the MIT license found in the\n# LICENSE file in the root directory of this source tree.\n\n\"\"\"\n    Utility for testing async writes with `torch.save`.\n    Usage:\n        buck run @mode/opt //fair_infra/data/iopath/tests:async_torch_test\n\"\"\"\n\nimport os\nimport tempfile\nimport time\n\nimport torch\nimport torch.nn as nn\nimport torch.optim as optim\nfrom metaseq.file_io.common import PathManager\n\n\nclass Model(nn.Module):\n    # pyre-fixme[3]: Return type must be annotated.\n    def __init__(self):\n        super(Model, self).__init__()\n        self.conv1 = nn.Conv2d(3, 6, 5)\n        self.pool = nn.MaxPool2d(2, 2)\n        self.conv2 = nn.Conv2d(6, 16, 5)\n        self.fc1 = nn.Linear(16 * 5 * 5, 120)\n        self.fc2 = nn.Linear(120, 84)\n        self.fc3 = nn.Linear(84, 10)\n\n\nclass TestDriver:\n    _pathmgr = PathManager()\n\n    def test(self) -> None:\n        model = Model()\n        optimizer = optim.SGD(model.parameters(), lr=0.001, momentum=0.9)\n\n        print(\"Model's state_dict:\")\n        for param_tensor in model.state_dict():\n            print(f\"{param_tensor}\\t{model.state_dict()[param_tensor].size()}\")\n        print(\"Optimizer's state_dict:\")\n        for var_name in optimizer.state_dict():\n            print(f\"{var_name}\\t{optimizer.state_dict()[var_name]}\")\n\n        with tempfile.TemporaryDirectory() as _tmpdir:\n            try:\n                URI = os.path.join(_tmpdir, \"test.ckpt\")\n\n                f = self._pathmgr.opena(URI, \"wb\")\n                i = \"*\"\n                large = f\"{i}\" * 1000000000\n\n                print(\"Starting `torch.save` call.\")\n                torch.save(\n                    {\n                        \"model_state_dict\": model.state_dict(),\n                        \"optimizer_state_dict\": optimizer.state_dict(),\n                        \"large\": large,\n                    },\n                    # pyre-fixme[6]: For 2nd param expected\n                    #  `Union[PathLike[typing.Any], IO[bytes], str, BinaryIO]` but got\n                    #  `IOBase`.\n                    f,\n                )\n                f.close()\n                start_time = time.time()\n\n            finally:\n                # We want this `join` call to take time. If it is instantaneous,\n                # then our async write calls are not running asynchronously.\n                print(\"Waiting for `torch.save` call to complete at `async_join()`.\")\n                self._pathmgr.async_join()\n\n            print(\n                \"Time Python waited for `async_join()` call to finish: \"\n                f\"{time.time() - start_time}s.\"\n            )\n            assert self._pathmgr.async_close()\n\n            checkpoint = torch.load(URI)\n            for key_item_1, key_item_2 in zip(\n                model.state_dict().items(), checkpoint[\"model_state_dict\"].items()\n            ):\n                assert torch.equal(key_item_1[1], key_item_2[1])\n            assert optimizer.state_dict() == checkpoint[\"optimizer_state_dict\"]\n            assert large == checkpoint[\"large\"]\n\n            print(\"Async `torch.save` Test succeeded.\")\n\n\nif __name__ == \"__main__\":\n    print(\"Async `torch.save` Test starting.\")\n    tst = TestDriver()\n    tst.test()\n",
        "tests/file_io/async_writes_test.py": "# Copyright (c) Meta Platforms, Inc. and affiliates. All Rights Reserved.\n#\n# This source code is licensed under the MIT license found in the\n# LICENSE file in the root directory of this source tree.\n\n\"\"\"\n    Utility for testing async writes with `NativePathHandler`.\n    Usage:\n        python -m tests.file_io.async_writes_test\n\"\"\"\n\nimport logging\nimport os\nimport tempfile\nimport time\n\nfrom metaseq.file_io.common import PathManager\n\n\n# pyre-fixme[5]: Global expression must be annotated.\nlogger = logging.getLogger(__name__)\n\n\n# pyre-fixme[3]: Return type must be annotated.\n# pyre-fixme[2]: Parameter must be annotated.\ndef printx(str):\n    logger.warning(f\"[{time.strftime('%X')}] {str}\")\n\n\nclass TestDriver:\n    LEN = 100000000  # This many characters per append job\n    NUM_JOBS = 10\n    _pathmgr = PathManager()\n\n    def test(self) -> None:\n        with tempfile.TemporaryDirectory() as _tmpdir:\n            URI = os.path.join(_tmpdir, \"test.txt\")\n\n            start_time = time.time()\n            printx(\n                f\"Start dispatching {self.NUM_JOBS} async write jobs \"\n                f\"each with {self.LEN} characters\"\n            )\n\n            FINAL_STR = \"\"\n            with self._pathmgr.opena(URI, \"a\") as f:\n                for i in range(self.NUM_JOBS):  # `i` goes from 0 to 9\n                    FINAL_STR += f\"{i}\" * self.LEN\n                    f.write(f\"{i}\" * self.LEN)\n\n            mid_time = time.time()\n            printx(\n                f\"Time taken to dispatch {self.NUM_JOBS} threads: {mid_time - start_time}\"\n            )\n            printx(\"Calling `async_join()`\")\n            # We want this `async_join` call to take time. If it is instantaneous, then our\n            # async write calls are not running asynchronously.\n            assert self._pathmgr.async_join()\n            printx(\n                f\"Time Python waited for `async_join()` call to finish: {time.time() - mid_time}\"\n            )\n\n            assert self._pathmgr.async_close()\n\n            with self._pathmgr.open(URI, \"r\") as f:\n                assert f.read() == FINAL_STR\n\n            printx(\"Async Writes Test finish.\")\n            printx(\n                \"Passing metric: \"\n                \"If the `async_join()` call took more than a negligible time to complete, \"\n                \"then Python waited for the threads to finish and the Async Writes \"\n                \"Test SUCCEEDS. Otherwise FAILURE.\"\n            )\n\n\nif __name__ == \"__main__\":\n    printx(\"Async Writes Test starting.\")\n    tst = TestDriver()\n    tst.test()\n",
        "tests/file_io/test_azure_blob.py": "# Copyright (c) Meta Platforms, Inc. and affiliates. All Rights Reserved.\n#\n# This source code is licensed under the MIT license found in the\n# LICENSE file in the root directory of this source tree.\n\nimport array\nimport hashlib\nimport logging\nimport os\nimport random\nimport re\nimport unittest\nimport uuid\nfrom contextlib import contextmanager\nfrom tempfile import TemporaryDirectory\nfrom typing import NamedTuple\n\n\ntry:\n    import azure.storage.blob as azure_blob\n    from metaseq.file_io.azure_blob import AzureBlobPathHandler\nexcept ImportError:\n    azure_blob = None\n    AzureBlobPathHandler = None\n\n\nlogging.basicConfig(\n    format=\"%(asctime)s %(message)s\", datefmt=\"%m/%d/%Y %I:%M:%S %p\", level=logging.INFO\n)\n\nENV_SAS_TOKEN = \"AZURE_STORAGE_SAS_TOKEN\"\nENV_BASE_PATH = \"AZURE_TEST_BASE_PATH\"\n\n\nif azure_blob:\n\n    class TestContext(NamedTuple):\n        handler: AzureBlobPathHandler\n        container_path: str\n        container_name: str\n        base_path: str\n        cache_dir: str\n\n        def get_local_test_file(self, name, length=4096):\n            path = os.path.join(self.cache_dir, name)\n            value = random.randint(0, 16)\n            with open(path, \"wb\") as f:\n                array.array(\"b\", [value] * length).tofile(f)\n            return path\n\n        def cleanup_remote(self):\n            for relpath in self.handler._ls(self.base_path):\n                file = os.path.join(self.container_path, relpath)\n                self.handler._rm(file)\n\nelse:\n    EnvironmentTokenProvider = None\n    TestContext = None\n\n\n@unittest.skipIf(not azure_blob, \"Requires azure-blob-storage install\")\n@unittest.skipIf(\n    not os.environ.get(ENV_SAS_TOKEN, None),\n    f\"Required env var not specified: {ENV_SAS_TOKEN}\",\n)\n@unittest.skipIf(\n    not os.environ.get(ENV_BASE_PATH, None),\n    f\"Required env var not specified: {ENV_BASE_PATH}\",\n)\nclass TestAzureBlob(unittest.TestCase):\n    \"\"\"\n    Tests rely on access to Blob Storage as specified in 2 environment variables:\n        - AZURE_STORAGE_SAS_TOKEN: SAS token providing read/write authorization\n        - AZURE_TEST_BASE_PATH: base path for all read/write operations. Should be in the form:\n            az://<account_name>/<container_name>/<relative_path>/\n\n    Any artifacts written under AZURE_TEST_BASE_PATH will be cleaned up on a best effort basis,\n    to the extent AzureBlobPathHandler._rm() is in working condition.\n    \"\"\"\n\n    def get_base_path(self):\n        base_path = os.environ.get(ENV_BASE_PATH, None)\n        assert base_path is not None, f\"Required env var not specified: {ENV_BASE_PATH}\"\n        # Set a unique base path for each test run\n        return os.path.join(base_path, str(uuid.uuid4()))\n\n    def get_container_path(self, base_path):\n        m = re.match(\"((az|blob)://[^/]+/[^/]+).+\", base_path)\n        assert m is not None, (\n            f\"Invalid base path: '{base_path}'.\"\n            + \" Expected 'az://<account_name>/<container_name>/<relative_path>/'\"\n        )\n        return m.groups(1)[0] + \"/\"\n\n    def get_md5(self, local_path):\n        with open(local_path, \"rb\") as f:\n            return self.get_md5_from_file(f)\n\n    def get_md5_from_file(self, f):\n        return hashlib.md5(f.read()).hexdigest()\n\n    @contextmanager\n    def context(self) -> TestContext:\n        \"\"\"\n        Initializes a test context:\n        1. Create a cache_dir on local disk that will be cleaned up on exit\n        2. Initialize the AzureBlobPathHandler with a SAS token from env variables\n        3. Tell the handler to close BlobServiceClient on exit so we don't leak sockets\n        \"\"\"\n        with TemporaryDirectory() as cache_dir:\n            try:\n                handler = AzureBlobPathHandler(cache_dir=cache_dir)\n                base_path = self.get_base_path()\n                container_path = self.get_container_path(base_path)\n                container_name = container_path.rstrip(\"/\").split(\"/\")[-1]\n                ctx = TestContext(\n                    handler=handler,\n                    base_path=base_path,\n                    container_path=container_path,\n                    container_name=container_name,\n                    cache_dir=cache_dir,\n                )\n                yield ctx\n            finally:\n                if ctx is not None:\n                    ctx.cleanup_remote()\n                if handler is not None:\n                    handler._close()\n\n    def test_parse_uri_when_valid(self):\n        valid_uris = {\n            \"az://account/container/path\": (\"account\", \"container\", \"path\"),\n            \"az://account/container/dir/path\": (\"account\", \"container\", \"dir/path\"),\n            \"az://account/container/dir/path/\": (\"account\", \"container\", \"dir/path/\"),\n            \"blob://account/container/path\": (\"account\", \"container\", \"path\"),\n            \"blob://account/container/dir/path\": (\"account\", \"container\", \"dir/path\"),\n            \"blob://account/container/dir/path/\": (\"account\", \"container\", \"dir/path/\"),\n        }\n\n        with self.context() as ctx:\n            for uri, expected in valid_uris.items():\n                actual = ctx.handler._parse_uri(uri)\n                self.assertEqual(actual, expected)\n\n    def test_parse_uri_invalid_protocol(self):\n        with self.context() as ctx:\n            try:\n                uri = \"s3://bucket/obj\"\n                ctx.handler._parse_uri(uri)\n                self.fail(\"Invalid protocol was accepted: \" + uri)\n            except Exception as e:\n                if not isinstance(e, ValueError):\n                    raise e\n\n    def test_status_operations(self):\n        with self.context() as ctx:\n            handler = ctx.handler\n\n            # Set up test files\n            filenames = [\"file1.bin\", \"file2.bin\"]\n            remote_dir = os.path.join(ctx.base_path, \"testdir\")\n            remote_paths = [os.path.join(remote_dir, f) for f in filenames]\n            for i, filename in enumerate(filenames):\n                local_path = ctx.get_local_test_file(filename)\n                handler._copy_from_local(local_path, remote_paths[i])\n\n            # Verify _exists for directory\n            path = os.path.join(ctx.base_path, \"testdir/\")\n            self.assertTrue(handler._exists(path))\n            path = os.path.join(ctx.base_path, \"testdir\")\n            self.assertTrue(handler._exists(path))\n            path = os.path.join(ctx.base_path, \"dne/\")\n            self.assertFalse(handler._exists(path))\n\n            # Verify _exists for file\n            self.assertTrue(handler._exists(remote_paths[0]))\n            self.assertFalse(handler._exists(os.path.join(ctx.base_path, \"dne\")))\n\n            # Verify _ls\n            actual = set(handler._ls(os.path.join(ctx.base_path, \"testdir/\")))\n            expected = {p.replace(ctx.container_path, \"\") for p in remote_paths}\n            self.assertEqual(actual, expected, f\"Failed to _ls() at {path}\")\n\n    def test_get_local_path(self):\n        with self.context() as ctx:\n            handler = ctx.handler\n\n            # Set up test files\n            filenames = [\"file1.bin\", \"file2.bin\"]\n            remote_dir = os.path.join(ctx.base_path, \"testdir\")\n            local_paths = [ctx.get_local_test_file(f) for f in filenames]\n            remote_paths = [os.path.join(remote_dir, f) for f in filenames]\n            for i in range(len(filenames)):\n                handler._copy_from_local(local_paths[i], remote_paths[i])\n\n            # Verify _get_local_path for directory\n            download_dir = handler._get_local_path(remote_dir)\n            for filename in filenames:\n                expected_file = os.path.join(download_dir, filename)\n                self.assertTrue(\n                    os.path.exists(expected_file),\n                    f\"Expected file to exist: {expected_file}\",\n                )\n\n            # Verify _get_local_path for file\n            download_path = handler._get_local_path(remote_paths[0])\n            self.assertEqual(\n                self.get_md5(local_paths[0]),\n                self.get_md5(download_path),\n                \"Cached file MD5 did not match\",\n            )\n\n    def test_copy(self):\n        with self.context() as ctx:\n            handler = ctx.handler\n\n            # Set up test files\n            filenames = [\"file1.bin\"]\n            remote_dir = os.path.join(ctx.base_path, \"testdir\")\n            local_paths = [ctx.get_local_test_file(f) for f in filenames]\n            remote_paths = [os.path.join(remote_dir, f) for f in filenames]\n            for i in range(len(filenames)):\n                handler._copy_from_local(local_paths[i], remote_paths[i])\n\n            # Verify _copy\n            src_path = remote_paths[0]\n            copy_path = src_path + \".bak\"\n            success = handler._copy(src_path, copy_path)\n            self.assertTrue(\n                success, f\"Failed to _copy() from {src_path} to {copy_path}\"\n            )\n            self.assertTrue(\n                handler._exists(copy_path),\n                f\"Failed to _copy() from {src_path} to {copy_path}\",\n            )\n\n    def test_open(self):\n        with self.context() as ctx:\n            handler = ctx.handler\n\n            # Set up test files\n            filenames = [\"file1.bin\"]\n            remote_dir = os.path.join(ctx.base_path, \"testdir\")\n            local_paths = [ctx.get_local_test_file(f) for f in filenames]\n            remote_paths = [os.path.join(remote_dir, f) for f in filenames]\n            for i in range(len(filenames)):\n                handler._copy_from_local(local_paths[i], remote_paths[i])\n\n            # Verify _open(mode=\"rb\")\n            expected = self.get_md5(local_paths[0])\n            with handler._open(remote_paths[0], \"rb\") as f:\n                actual = self.get_md5_from_file(f)\n            self.assertEqual(expected, actual, \"Streamed file MD5 did not match\")\n\n            # Verify _open(mode=\"wb\")\n            chunk_size = 2 * 1024 * 1024\n            new_remote_path = remote_paths[0] + \".streamed\"\n            with open(local_paths[0], \"rb\") as f_local, handler._open(\n                new_remote_path, \"wb\"\n            ) as f:\n                data = f_local.read(chunk_size)\n                while data:\n                    f.write(data)\n                    data = f_local.read(chunk_size)\n\n\nif __name__ == \"__main__\":\n    unittest.main()\n",
        "tests/file_io/test_native.py": "#!/usr/bin/env python3\n\n# Copyright (c) Meta Platforms, Inc. and affiliates. All Rights Reserved.\n#\n# This source code is licensed under the MIT license found in the\n# LICENSE file in the root directory of this source tree.\n\nimport os\nimport shutil\nimport tempfile\nimport unittest\nimport uuid\n\nfrom metaseq.file_io.common import PathManager\n\nfrom typing import Optional\n\n\nclass TestNativeIO(unittest.TestCase):\n    _tmpdir: Optional[str] = None\n    _filename: Optional[str] = None\n    _tmpfile: Optional[str] = None\n    _tmpfile_contents = \"Hello, World\"\n    _pathmgr = PathManager()\n\n    @classmethod\n    def setUpClass(cls) -> None:\n        cls._tmpdir = tempfile.mkdtemp()\n        cls._filename = \"test_file_for_iopath_with_a_really_uncommon_name.txt\"\n        with open(os.path.join(cls._tmpdir, cls._filename), \"w\") as f:\n            cls._tmpfile = f.name\n            f.write(cls._tmpfile_contents)\n            f.flush()\n\n    @classmethod\n    def tearDownClass(cls) -> None:\n        # Cleanup temp working dir.\n        if cls._tmpdir is not None:\n            shutil.rmtree(cls._tmpdir)  # type: ignore\n\n    def setUp(self) -> None:\n        # Reset class variables set by methods before each test.\n        self._pathmgr.set_cwd(None)\n        self._pathmgr._native_path_handler._non_blocking_io_manager = None\n        self._pathmgr._native_path_handler._non_blocking_io_executor = None\n        self._pathmgr._async_handlers.clear()\n\n    def test_open(self) -> None:\n        # with patch(\"iopath.common.event_logger.EventLogger.log_event\") as foo:\n        # pyre-ignore\n        with self._pathmgr.open(self._tmpfile, \"r\") as f:\n            self.assertEqual(f.read(), self._tmpfile_contents)\n\n    def test_open_args(self) -> None:\n        # with patch(\"iopath.common.event_logger.EventLogger.log_event\") as foo:\n        self._pathmgr.set_strict_kwargs_checking(True)\n        f = self._pathmgr.open(\n            self._tmpfile,  # type: ignore\n            mode=\"r\",\n            buffering=1,\n            encoding=\"UTF-8\",\n            errors=\"ignore\",\n            newline=None,\n            closefd=True,\n            opener=None,\n        )\n        f.close()\n\n    def test_get_local_path(self) -> None:\n        self.assertEqual(\n            # pyre-ignore\n            self._pathmgr.get_local_path(self._tmpfile),\n            self._tmpfile,\n        )\n\n    def test_get_local_path_forced(self) -> None:\n        self.assertEqual(\n            # pyre-ignore\n            self._pathmgr.get_local_path(self._tmpfile, force=True),\n            self._tmpfile,\n        )\n\n    def test_exists(self) -> None:\n        # pyre-ignore\n        self.assertTrue(self._pathmgr.exists(self._tmpfile))\n        fake_path = os.path.join(self._tmpdir, uuid.uuid4().hex)\n        self.assertFalse(self._pathmgr.exists(fake_path))\n\n    def test_isfile(self) -> None:\n        self.assertTrue(self._pathmgr.isfile(self._tmpfile))  # pyre-ignore\n        # This is a directory, not a file, so it should fail\n        self.assertFalse(self._pathmgr.isfile(self._tmpdir))  # pyre-ignore\n        # This is a non-existing path, so it should fail\n        fake_path = os.path.join(self._tmpdir, uuid.uuid4().hex)\n        self.assertFalse(self._pathmgr.isfile(fake_path))\n\n    def test_isdir(self) -> None:\n        # pyre-ignore\n        self.assertTrue(self._pathmgr.isdir(self._tmpdir))\n        # This is a file, not a directory, so it should fail\n        # pyre-ignore\n        self.assertFalse(self._pathmgr.isdir(self._tmpfile))\n        # This is a non-existing path, so it should fail\n        fake_path = os.path.join(self._tmpdir, uuid.uuid4().hex)\n        self.assertFalse(self._pathmgr.isdir(fake_path))\n\n    def test_islink(self) -> None:\n        symlink = self._tmpfile + \".lnk\"\n        self._pathmgr.symlink(self._tmpfile, symlink)\n        self.assertTrue(self._pathmgr.islink(symlink))\n        # This is a file, not a symlink, so it should fail\n        self.assertFalse(self._pathmgr.islink(self._tmpfile))\n        # This is a non-existing path, so it should fail\n        fake_path = os.path.join(self._tmpdir, uuid.uuid4().hex)\n        self.assertFalse(self._pathmgr.islink(fake_path))\n\n    def test_ls(self) -> None:\n        # Create some files in the tempdir to ls out.\n        root_dir = os.path.join(self._tmpdir, \"ls\")\n        os.makedirs(root_dir, exist_ok=True)\n        files = sorted([\"foo.txt\", \"bar.txt\", \"baz.txt\"])\n        for f in files:\n            open(os.path.join(root_dir, f), \"a\").close()\n\n        children = sorted(self._pathmgr.ls(root_dir))\n        self.assertListEqual(children, files)\n\n        # Test trailing wildcard\n        children = sorted(self._pathmgr.ls(root_dir + \"/*\"))\n        self.assertListEqual(children, files)\n\n        # Cleanup the tempdir\n        shutil.rmtree(root_dir)\n\n    def test_mkdirs(self) -> None:\n        new_dir_path = os.path.join(self._tmpdir, \"new\", \"tmp\", \"dir\")\n        self.assertFalse(self._pathmgr.exists(new_dir_path))\n        self._pathmgr.mkdirs(new_dir_path)\n        self.assertTrue(self._pathmgr.exists(new_dir_path))\n\n    def test_copy(self) -> None:\n        _tmpfile_2 = self._tmpfile + \"2\"  # pyre-ignore\n        _tmpfile_2_contents = \"something else\"\n        with open(_tmpfile_2, \"w\") as f:\n            f.write(_tmpfile_2_contents)\n            f.flush()\n        # pyre-fixme[6]: For 1st param expected `str` but got `Optional[str]`.\n        self.assertTrue(self._pathmgr.copy(self._tmpfile, _tmpfile_2, overwrite=True))\n        with self._pathmgr.open(_tmpfile_2, \"r\") as f:\n            self.assertEqual(f.read(), self._tmpfile_contents)\n\n    def test_move(self) -> None:\n        _tmpfile_2 = self._tmpfile + \"2\" + uuid.uuid4().hex  # pyre-ignore\n        _tmpfile_3 = self._tmpfile + \"3_\" + uuid.uuid4().hex  # pyre-ignore\n        _tmpfile_2_contents = \"Hello Move\"\n        with open(_tmpfile_2, \"w\") as f:\n            f.write(_tmpfile_2_contents)\n            f.flush()\n        self.assertTrue(self._pathmgr.mv(_tmpfile_2, _tmpfile_3))\n        with self._pathmgr.open(_tmpfile_3, \"r\") as f:\n            self.assertEqual(f.read(), _tmpfile_2_contents)\n        self.assertFalse(self._pathmgr.exists(_tmpfile_2))\n        self._pathmgr.rm(_tmpfile_3)\n\n    def test_symlink(self) -> None:\n        _symlink = self._tmpfile + \"_symlink\"  # pyre-ignore\n        self.assertTrue(self._pathmgr.symlink(self._tmpfile, _symlink))  # pyre-ignore\n        with self._pathmgr.open(_symlink) as f:\n            self.assertEqual(f.read(), self._tmpfile_contents)\n        self.assertEqual(os.readlink(_symlink), self._tmpfile)\n        os.remove(_symlink)\n\n    def test_rm(self) -> None:\n        with open(os.path.join(self._tmpdir, \"test_rm.txt\"), \"w\") as f:\n            rm_file = f.name\n            f.write(self._tmpfile_contents)\n            f.flush()\n        self.assertTrue(self._pathmgr.exists(rm_file))\n        self.assertTrue(self._pathmgr.isfile(rm_file))\n        self._pathmgr.rm(rm_file)\n        self.assertFalse(self._pathmgr.exists(rm_file))\n        self.assertFalse(self._pathmgr.isfile(rm_file))\n\n    def test_set_cwd(self) -> None:\n        # File not found since cwd not set yet.\n        # pyre-fixme[6]: For 1st param expected `str` but got `Optional[str]`.\n        self.assertFalse(self._pathmgr.isfile(self._filename))\n        # pyre-fixme[6]: For 1st param expected `str` but got `Optional[str]`.\n        self.assertTrue(self._pathmgr.isfile(self._tmpfile))\n        # Once cwd is set, relative file path works.\n        self._pathmgr.set_cwd(self._tmpdir)\n        # pyre-fixme[6]: For 1st param expected `str` but got `Optional[str]`.\n        self.assertTrue(self._pathmgr.isfile(self._filename))\n\n        # Set cwd to None\n        self._pathmgr.set_cwd(None)\n        # pyre-fixme[6]: For 1st param expected `str` but got `Optional[str]`.\n        self.assertFalse(self._pathmgr.isfile(self._filename))\n        # pyre-fixme[6]: For 1st param expected `str` but got `Optional[str]`.\n        self.assertTrue(self._pathmgr.isfile(self._tmpfile))\n\n        # Set cwd to invalid path\n        with self.assertRaises(ValueError):\n            self._pathmgr.set_cwd(\"/nonexistent/path\")\n\n    def test_get_path_with_cwd(self) -> None:\n        self._pathmgr.set_cwd(self._tmpdir)\n        # Make sure _get_path_with_cwd() returns correctly.\n        self.assertEqual(\n            # pyre-fixme[6]: For 1st param expected `str` but got `Optional[str]`.\n            self._pathmgr._native_path_handler._get_path_with_cwd(self._filename),\n            self._tmpfile,\n        )\n        self.assertEqual(\n            self._pathmgr._native_path_handler._get_path_with_cwd(\"/abs.txt\"),\n            \"/abs.txt\",\n        )\n\n    def test_bad_args(self) -> None:\n        # TODO (T58240718): Replace with dynamic checks\n        with self.assertRaises(ValueError):\n            self._pathmgr.copy(self._tmpfile, self._tmpfile, foo=\"foo\")  # type: ignore\n        with self.assertRaises(ValueError):\n            self._pathmgr.exists(self._tmpfile, foo=\"foo\")  # type: ignore\n        with self.assertRaises(ValueError):\n            self._pathmgr.get_local_path(self._tmpfile, foo=\"foo\")  # type: ignore\n        with self.assertRaises(ValueError):\n            self._pathmgr.isdir(self._tmpfile, foo=\"foo\")  # type: ignore\n        with self.assertRaises(ValueError):\n            self._pathmgr.isfile(self._tmpfile, foo=\"foo\")  # type: ignore\n        with self.assertRaises(ValueError):\n            self._pathmgr.ls(self._tmpfile, foo=\"foo\")  # type: ignore\n        with self.assertRaises(ValueError):\n            self._pathmgr.mkdirs(self._tmpfile, foo=\"foo\")  # type: ignore\n        with self.assertRaises(ValueError):\n            self._pathmgr.open(self._tmpfile, foo=\"foo\")  # type: ignore\n        with self.assertRaises(ValueError):\n            self._pathmgr.opena(self._tmpfile, foo=\"foo\")  # type: ignore\n        with self.assertRaises(ValueError):\n            self._pathmgr.rm(self._tmpfile, foo=\"foo\")  # type: ignore\n        with self.assertRaises(ValueError):\n            self._pathmgr.set_cwd(self._tmpdir, foo=\"foo\")  # type: ignore\n\n        self._pathmgr.set_strict_kwargs_checking(False)\n\n        self._pathmgr.copy(\n            self._tmpfile, self._tmpfile + \"2\", foo=\"foo\"  # type: ignore\n        )\n        self._pathmgr.exists(self._tmpfile, foo=\"foo\")  # type: ignore\n        self._pathmgr.get_local_path(self._tmpfile, foo=\"foo\")  # type: ignore\n        self._pathmgr.isdir(self._tmpfile, foo=\"foo\")  # type: ignore\n        self._pathmgr.isfile(self._tmpfile, foo=\"foo\")  # type: ignore\n        self._pathmgr.ls(self._tmpdir, foo=\"foo\")  # type: ignore\n        self._pathmgr.mkdirs(self._tmpdir, foo=\"foo\")  # type: ignore\n        f = self._pathmgr.open(self._tmpfile, foo=\"foo\")  # type: ignore\n        f.close()\n        with open(os.path.join(self._tmpdir, \"test_rm.txt\"), \"w\") as f:\n            rm_file = f.name\n            f.write(self._tmpfile_contents)\n            f.flush()\n        self._pathmgr.rm(rm_file, foo=\"foo\")  # type: ignore\n\n\nif __name__ == \"__main__\":\n    unittest.main()\n",
        "tests/file_io/test_non_blocking_io.py": "#!/usr/bin/env python3\n\n# Copyright (c) Meta Platforms, Inc. and affiliates. All Rights Reserved.\n#\n# This source code is licensed under the MIT license found in the\n# LICENSE file in the root directory of this source tree.\n\nimport concurrent.futures\nimport io\nimport os\nimport shutil\nimport tempfile\nimport unittest\nfrom typing import Optional\nfrom unittest.mock import Mock, patch\n\nfrom metaseq.file_io.common import NativePathHandler, PathManager\nfrom metaseq.file_io.common.non_blocking_io import (\n    NonBlockingBufferedIO,\n    NonBlockingIO,\n    NonBlockingIOManager,\n)\n\n\nclass TestNativeIOAsync(unittest.TestCase):\n    \"\"\"\n    This test class is meant to have comprehensive tests for\n    `NativePathHandler`. Async functionality tests for other\n    `PathHandler`-s should only require a single test since\n    all `PathHandler`-s operate in the same way.\n    \"\"\"\n\n    _tmpdir: Optional[str] = None\n    _pathmgr = PathManager()\n\n    # pyre-fixme[3]: Return type must be annotated.\n    # pyre-fixme[2]: Parameter must be annotated.\n    def run(self, result=None):\n        with patch(\"iopath.common.event_logger.EventLogger.log_event\"):\n            super(TestNativeIOAsync, self).run(result)\n\n    @classmethod\n    def setUpClass(cls) -> None:\n        cls._tmpdir = tempfile.mkdtemp()\n\n    @classmethod\n    def tearDownClass(cls) -> None:\n        # Cleanup temp working dir.\n        if cls._tmpdir is not None:\n            shutil.rmtree(cls._tmpdir)  # type: ignore\n\n    def setUp(self) -> None:\n        # Reset class variables set by methods before each test.\n        self._pathmgr.set_cwd(None)\n        self._pathmgr._native_path_handler._non_blocking_io_manager = None\n        self._pathmgr._native_path_handler._non_blocking_io_executor = None\n        self._pathmgr._async_handlers.clear()\n\n    def test_opena(self) -> None:\n        _tmpfile = os.path.join(self._tmpdir, \"async.txt\")\n        try:\n            # Write the files.\n            with self._pathmgr.opena(_tmpfile + \"f\", \"w\") as f:\n                f.write(\"f1 \")\n                with self._pathmgr.opena(_tmpfile + \"g\", \"w\") as g:\n                    f.write(\"f2 \")\n                    g.write(\"g1 \")\n                    f.write(\"f3 \")\n                f.write(\"f4 \")\n            with self._pathmgr.opena(_tmpfile + \"f\", \"a\") as f:\n                f.write(\"f5 \")\n            F_STR = \"f1 f2 f3 f4 f5 \"\n            G_STR = \"g1 \"\n\n            # Test that `PathManager._async_handlers` keeps track of all\n            # `PathHandler`-s where `opena` is used.\n            self.assertCountEqual(\n                [type(handler) for handler in self._pathmgr._async_handlers],\n                [type(self._pathmgr._native_path_handler)],\n            )\n            # Test that 2 paths were properly logged in `NonBlockingIOManager`.\n            manager = self._pathmgr._native_path_handler._non_blocking_io_manager\n            self.assertEqual(len(manager._path_to_data), 2)\n        finally:\n            # Join the threads to wait for files to be written.\n            self.assertTrue(self._pathmgr.async_close())\n\n        # Check that both files were asynchronously written and written in order.\n        with self._pathmgr.open(_tmpfile + \"f\", \"r\") as f:\n            self.assertEqual(f.read(), F_STR)\n        with self._pathmgr.open(_tmpfile + \"g\", \"r\") as g:\n            self.assertEqual(g.read(), G_STR)\n        # Test that both `NonBlockingIO` objects `f` and `g` are finally closed.\n        self.assertEqual(len(manager._path_to_data), 0)\n\n    def test_async_join_behavior(self) -> None:\n        _tmpfile = os.path.join(self._tmpdir, \"async.txt\")\n        _tmpfile_contents = \"Async Text\"\n        try:\n            for _ in range(1):  # Opens 1 thread\n                with self._pathmgr.opena(_tmpfile + \"1\", \"w\") as f:\n                    f.write(f\"{_tmpfile_contents}-1\")\n            for _ in range(2):  # Opens 2 threads\n                with self._pathmgr.opena(_tmpfile + \"2\", \"w\") as f:\n                    f.write(f\"{_tmpfile_contents}-2\")\n            for _ in range(3):  # Opens 3 threads\n                with self._pathmgr.opena(_tmpfile + \"3\", \"w\") as f:\n                    f.write(f\"{_tmpfile_contents}-3\")\n            _path_to_data = (\n                self._pathmgr._native_path_handler._non_blocking_io_manager._path_to_data\n            )\n            # Join the threads for the 1st and 3rd file and ensure threadpool completed.\n            _path_to_data_copy = dict(_path_to_data)\n            self.assertTrue(\n                self._pathmgr.async_join(\n                    _tmpfile + \"1\", _tmpfile + \"3\"\n                )  # Removes paths from `_path_to_io`.\n            )\n            self.assertFalse(_path_to_data_copy[_tmpfile + \"1\"].thread.is_alive())\n            self.assertFalse(_path_to_data_copy[_tmpfile + \"3\"].thread.is_alive())\n            self.assertEqual(len(_path_to_data), 1)  # 1 file remaining\n        finally:\n            # Join all the remaining threads\n            _path_to_data_copy = dict(_path_to_data)\n            self.assertTrue(self._pathmgr.async_close())\n\n        # Ensure data cleaned up.\n        self.assertFalse(_path_to_data_copy[_tmpfile + \"2\"].thread.is_alive())\n        self.assertEqual(len(self._pathmgr._async_handlers), 0)\n        self.assertEqual(len(_path_to_data), 0)  # 0 files remaining\n\n    def test_opena_normpath(self) -> None:\n        _filename = \"async.txt\"\n        # `_file1` and `_file2` should represent the same path but have different\n        # string representations.\n        _file1 = os.path.join(self._tmpdir, _filename)\n        _file2 = os.path.join(self._tmpdir, \".\", _filename)\n        self.assertNotEqual(_file1, _file2)\n        try:\n            _file1_text = \"File1 text\"\n            _file2_text = \"File2 text\"\n            with self._pathmgr.opena(_file1, \"w\") as f:\n                f.write(_file1_text)\n            with self._pathmgr.opena(_file2, \"a\") as f:\n                f.write(_file2_text)\n            _path_to_data = (\n                self._pathmgr._native_path_handler._non_blocking_io_manager._path_to_data\n            )\n            # Check that `file2` is marked as the same file as `file1`.\n            self.assertEqual(len(_path_to_data), 1)\n            self.assertTrue(self._pathmgr.async_join())\n            # Check that both file paths give the same file contents.\n            with self._pathmgr.open(_file1, \"r\") as f:\n                self.assertEqual(f.read(), _file1_text + _file2_text)\n            with self._pathmgr.open(_file2, \"r\") as f:\n                self.assertEqual(f.read(), _file1_text + _file2_text)\n        finally:\n            self.assertTrue(self._pathmgr.async_close())\n\n    def test_async_consecutive_join_calls(self) -> None:\n        _file = os.path.join(self._tmpdir, \"async.txt\")\n        try:\n            self.assertTrue(self._pathmgr.async_join())\n            try:\n                with self._pathmgr.opena(_file, \"w\") as f:\n                    f.write(\"1\")\n            finally:\n                self.assertTrue(self._pathmgr.async_join())\n            with self._pathmgr.open(_file, \"r\") as f:\n                self.assertEqual(f.read(), \"1\")\n\n            try:\n                f = self._pathmgr.opena(_file, \"a\")\n                f.write(\"2\")\n                f.close()\n            finally:\n                self.assertTrue(self._pathmgr.async_join())\n            with self._pathmgr.open(_file, \"r\") as f:\n                self.assertEqual(f.read(), \"12\")\n        finally:\n            self.assertTrue(self._pathmgr.async_close())\n\n    def test_opena_mode_restriction(self) -> None:\n        _file = os.path.join(self._tmpdir, \"async.txt\")\n        with self.assertRaises(ValueError):\n            self._pathmgr.opena(_file, \"r\")\n        with self.assertRaises(ValueError):\n            self._pathmgr.opena(_file, \"rb\")\n        with self.assertRaises(ValueError):\n            self._pathmgr.opena(_file, \"wrb\")\n\n    def test_opena_args_passed_correctly(self) -> None:\n        _file = os.path.join(self._tmpdir, \"async.txt\")\n        try:\n            # Make sure that `opena` args are used correctly by using\n            # different newline args.\n            with self._pathmgr.opena(_file, \"w\", newline=\"\\r\\n\") as f:\n                f.write(\"1\\n\")\n            with self._pathmgr.opena(_file, \"a\", newline=\"\\n\") as f:\n                f.write(\"2\\n3\")\n        finally:\n            self.assertTrue(self._pathmgr.async_close())\n\n        # Read the raw file data without converting newline endings to see\n        # if the `opena` args were used correctly.\n        with self._pathmgr.open(_file, \"r\", newline=\"\") as f:\n            self.assertEqual(f.read(), \"1\\r\\n2\\n3\")\n\n    def test_opena_with_callback(self) -> None:\n        _file_tmp = os.path.join(self._tmpdir, \"async.txt.tmp\")\n        _file = os.path.join(self._tmpdir, \"async.txt\")\n        _data = \"Asynchronously written text\"\n\n        # pyre-fixme[53]: Captured variable `_data` is not annotated.\n        # pyre-fixme[53]: Captured variable `_file` is not annotated.\n        # pyre-fixme[53]: Captured variable `_file_tmp` is not annotated.\n        # pyre-fixme[3]: Return type must be annotated.\n        def cb():\n            # Insert a test to make sure `_file_tmp` was closed before\n            # the callback is called.\n            with open(_file_tmp, \"r\") as f:\n                self.assertEqual(f.read(), _data)\n            self._pathmgr.copy(_file_tmp, _file, overwrite=True)\n\n        mock_cb = Mock(side_effect=cb)\n\n        try:\n            with self._pathmgr.opena(\n                _file_tmp, \"w\", callback_after_file_close=mock_cb\n            ) as f:\n                f.write(_data)\n        finally:\n            self.assertTrue(self._pathmgr.async_close())\n        # Callback should have been called exactly once.\n        mock_cb.assert_called_once()\n\n        # Data should have been written to both `_file_tmp` and `_file`.\n        with open(_file_tmp, \"r\") as f:\n            self.assertEqual(f.read(), _data)\n        with open(_file, \"r\") as f:\n            self.assertEqual(f.read(), _data)\n\n    def test_opena_with_callback_only_called_once(self) -> None:\n        _file_tmp = os.path.join(self._tmpdir, \"async.txt.tmp\")\n\n        mock_cb = Mock()\n\n        # Callback should be called once even if `close` is called\n        # multiple times.\n        try:\n            f = self._pathmgr.opena(_file_tmp, \"w\", callback_after_file_close=mock_cb)\n            f.close()\n            f.close()\n            f.close()\n        finally:\n            self.assertTrue(self._pathmgr.async_close())\n        # Callback should have been called exactly once.\n        mock_cb.assert_called_once()\n\n    def test_async_custom_executor(self) -> None:\n        # At first, neither manager nor executor are set.\n        self.assertIsNone(self._pathmgr._native_path_handler._non_blocking_io_manager)\n        self.assertIsNone(self._pathmgr._native_path_handler._non_blocking_io_executor)\n        # Then, override the `NativePathHandler` and set a custom executor.\n        executor = concurrent.futures.ThreadPoolExecutor(\n            max_workers=128, thread_name_prefix=\"my prefix\"\n        )\n        ph = NativePathHandler(async_executor=executor)\n        self._pathmgr.register_handler(ph, allow_override=True)\n        self.assertEqual(ph, self._pathmgr._native_path_handler)\n\n        # Opening a file with `opena` initializes the manager with the\n        # executor.\n        _file = os.path.join(self._tmpdir, \"async.txt\")\n        try:\n            with self._pathmgr.opena(_file, \"w\") as f:\n                f.write(\"Text\")\n            # Make sure the manager's executor is the same as the user's.\n            self.assertEqual(\n                executor,\n                self._pathmgr._native_path_handler._non_blocking_io_manager._pool,\n            )\n        finally:\n            self.assertTrue(self._pathmgr.async_close())\n\n    def test_non_blocking_io_seekable(self) -> None:\n        _file = os.path.join(self._tmpdir, \"async.txt\")\n        # '^' marks the current position in stream\n\n        # Test seek.\n        try:\n            with self._pathmgr.opena(_file, \"wb\") as f:\n                f.write(b\"012345\")  # file = 012345^\n                f.seek(1)  # file = 0^12345\n                f.write(b\"##\")  # file = 0##^345\n        finally:\n            self.assertTrue(self._pathmgr.async_join())\n            with self._pathmgr.open(_file, \"rb\") as f:\n                self.assertEqual(f.read(), b\"0##345\")\n\n        # Test truncate.\n        try:\n            with self._pathmgr.opena(_file, \"wb\") as f:\n                f.write(b\"012345\")  # file = 012345^\n                f.seek(2)  # file = 01^2345\n                f.truncate()  # file = 01^\n        finally:\n            self.assertTrue(self._pathmgr.async_join())\n            with self._pathmgr.open(_file, \"rb\") as f:\n                self.assertEqual(f.read(), b\"01\")\n\n        # Big test for seek and truncate.\n        try:\n            with self._pathmgr.opena(_file, \"wb\") as f:\n                f.write(b\"0123456789\")  # file = 0123456789^\n                f.seek(2)  # file = 01^23456789\n                f.write(b\"##\")  # file = 01##^456789\n                f.seek(3, io.SEEK_CUR)  # file = 01##456^789\n                f.truncate()  # file = 01##456^\n                f.write(b\"$\")  # file = 01##456$^\n        finally:\n            self.assertTrue(self._pathmgr.async_join())\n            with self._pathmgr.open(_file, \"rb\") as f:\n                self.assertEqual(f.read(), b\"01##456$\")\n\n        # Test NOT tellable.\n        try:\n            with self._pathmgr.opena(_file, \"wb\") as f:\n                with self.assertRaises(ValueError):\n                    f.tell()\n        finally:\n            self._pathmgr.async_close()\n\n\nclass TestNonBlockingIO(unittest.TestCase):\n    _tmpdir: Optional[str] = None\n    _io_manager = NonBlockingIOManager(buffered=False)\n    _buffered_io_manager = NonBlockingIOManager(buffered=True)\n\n    @classmethod\n    def setUpClass(cls) -> None:\n        cls._tmpdir = tempfile.mkdtemp()\n\n    @classmethod\n    def tearDownClass(cls) -> None:\n        # Cleanup temp working dir.\n        if cls._tmpdir is not None:\n            shutil.rmtree(cls._tmpdir)  # type: ignore\n\n    def test_select_io(self) -> None:\n        self.assertEqual(self._io_manager._IO, NonBlockingIO)\n        self.assertEqual(self._buffered_io_manager._IO, NonBlockingBufferedIO)\n\n    def test_io_manager(self) -> None:\n        _file = os.path.join(self._tmpdir, \"non_buffered.txt\")\n\n        try:\n            # Test IO notifies manager after every write call.\n            f = self._io_manager.get_non_blocking_io(\n                path=_file, io_obj=open(_file, \"w\")\n            )\n            with patch.object(\n                f,\n                \"_notify_manager\",\n                # pyre-fixme[16]: Item `IO` of `Union[IO[bytes], IO[str]]` has no\n                #  attribute `_notify_manager`.\n                wraps=f._notify_manager,\n            ) as mock_notify_manager:\n                f.write(\".\" * 1)\n                f.write(\".\" * 2)\n                f.write(\".\" * 3)\n                # Should notify manager 3 times: 3 write calls.\n                self.assertEqual(mock_notify_manager.call_count, 3)\n                mock_notify_manager.reset_mock()\n                # Should notify manager 1 time: 1 close call.\n                f.close()\n                self.assertEqual(mock_notify_manager.call_count, 1)\n        finally:\n            self.assertTrue(self._io_manager._join())\n            self.assertTrue(self._io_manager._close_thread_pool())\n\n        with open(_file, \"r\") as f:\n            self.assertEqual(f.read(), \".\" * 6)\n\n    def test_buffered_io_manager(self) -> None:\n        _file = os.path.join(self._tmpdir, \"buffered.txt\")\n\n        try:\n            # Test IO doesn't flush until buffer is full.\n            f = self._buffered_io_manager.get_non_blocking_io(\n                path=_file, io_obj=open(_file, \"wb\"), buffering=10\n            )\n            with patch.object(f, \"flush\", wraps=f.flush) as mock_flush:\n                with patch.object(\n                    f,\n                    \"_notify_manager\",\n                    # pyre-fixme[16]: Item `IO` of `Union[IO[bytes], IO[str]]` has\n                    #  no attribute `_notify_manager`.\n                    wraps=f._notify_manager,\n                ) as mock_notify_manager:\n                    f.write(b\".\" * 9)\n                    mock_flush.assert_not_called()  # buffer not filled - don't flush\n                    mock_notify_manager.assert_not_called()\n                    # Should flush when full.\n                    f.write(b\".\" * 13)\n                    mock_flush.assert_called_once()  # buffer filled - should flush\n                    # `flush` should notify manager 4 times: 3 `file.write` and 1 `buffer.close`.\n                    # Buffer is split into 3 chunks of size 10, 10, and 2.\n                    # pyre-fixme[16]: Item `IO` of `Union[IO[bytes], IO[str]]` has\n                    #  no attribute `_buffers`.\n                    self.assertEqual(len(f._buffers), 2)  # 22-byte and 0-byte buffers\n                    self.assertEqual(mock_notify_manager.call_count, 4)\n                    mock_notify_manager.reset_mock()\n                    # `close` should notify manager 2 times: 1 `buffer.close` and 1 `file.close`.\n                    f.close()\n                    self.assertEqual(mock_notify_manager.call_count, 2)\n\n            # Test IO flushes on file close.\n            f = self._buffered_io_manager.get_non_blocking_io(\n                path=_file, io_obj=open(_file, \"ab\"), buffering=10\n            )\n            with patch.object(f, \"flush\", wraps=f.flush) as mock_flush:\n                f.write(b\".\" * 5)\n                mock_flush.assert_not_called()\n                f.close()\n                mock_flush.assert_called()  # flush on exit\n        finally:\n            self.assertTrue(self._buffered_io_manager._join())\n            self.assertTrue(self._buffered_io_manager._close_thread_pool())\n\n        with open(_file, \"rb\") as f:\n            self.assertEqual(f.read(), b\".\" * 27)\n\n\nif __name__ == \"__main__\":\n    unittest.main()\n",
        "tests/file_io/test_s3.py": "#!/usr/bin/env python3\n\n# Copyright (c) Meta Platforms, Inc. and affiliates. All Rights Reserved.\n#\n# This source code is licensed under the MIT license found in the\n# LICENSE file in the root directory of this source tree.\n\nimport os\nimport unittest\n\nfrom metaseq.file_io.s3 import S3PathHandler\n\n\ntry:\n    import boto3\nexcept ImportError:\n    boto3 = None\n\n# Hack to make the test cases ordered.\n# https://stackoverflow.com/questions/4005695/changing-order-of-unit-tests-in-python\nunittest.TestLoader.sortTestMethodsUsing = lambda _, x, y: y > x\n\n\n@unittest.skipIf(not boto3, \"Requires boto3 install\")\nclass TestsS3(unittest.TestCase):\n    s3_auth = False\n    skip_s3_auth_required_tests_message = (\n        \"Provide an s3 project and bucket you are\"\n        + \"authorised against, then set the s3_auth flag to True\"\n    )\n\n    #############################################\n    # Shared\n    #############################################\n    @classmethod\n    def setUpClass(cls):\n        # NOTE: user can change these locations.\n        cls.s3_bucket = \"fairusersglobal\"\n        cls.s3_rel_path = os.path.expandvars(\n            \"users/$USER/private/home/$USER/.metaseq/test_s3_pathhandler\"\n        )\n        cls.s3_full_path = \"s3://\" + cls.s3_bucket + \"/\" + cls.s3_rel_path\n        cls.s3_pathhandler = S3PathHandler()\n\n    @classmethod\n    def tearDownClass(cls, _s3_auth=s3_auth):\n        if not _s3_auth:\n            return\n\n        # Recursive deletion is not implemented,\n        # so let's delete each file and directory.\n\n        # Delete all files\n        cls.s3_pathhandler._rm(\"/\".join([cls.s3_full_path, \"dir1\", \"f1_write_string\"]))\n        cls.s3_pathhandler._rm(\"/\".join([cls.s3_full_path, \"dir1\", \"f2_write_bytes\"]))\n        cls.s3_pathhandler._rm(\n            \"/\".join([cls.s3_full_path, \"dir2\", \"f1_write_string_from_local\"])\n        )\n        cls.s3_pathhandler._rm(\n            \"/\".join([cls.s3_full_path, \"dir2\", \"f2_write_bytes_from_local\"])\n        )\n        cls.s3_pathhandler._rm(\n            \"/\".join([cls.s3_full_path, \"dir2\", \"f3_write_string_from_local\"])\n        )\n        cls.s3_pathhandler._rm(\n            \"/\".join([cls.s3_full_path, \"dir2\", \"f4_write_bytes_from_local\"])\n        )\n\n        # Delete all directories.\n        cls.s3_pathhandler._rm(\"/\".join([cls.s3_full_path, \"dir3\", \"dir4/\"]))\n        for i in (1, 2, 3):\n            cls.s3_pathhandler._rm(\"/\".join([cls.s3_full_path, f\"dir{i}/\"]))\n\n        assert cls.s3_pathhandler._ls(cls.s3_full_path) == []\n\n    #############################################\n    # Up here, test class attributes,\n    # and helpers that don't require S3 access.\n    #############################################\n\n    def test_00_supported_prefixes(self):\n        supported_prefixes = self.s3_pathhandler._get_supported_prefixes()\n        self.assertEqual(supported_prefixes, [\"s3://\"])\n\n    # # Require S3 Authentication ====>\n\n    #############################################\n    # Organization of s3 setup\n    # dir1/\n    #   f1 <- small (via open)\n    #   f2 <- large checkpoint file (via open)\n    # dir2/\n    #   f3 <- small (via copy(), from dir1)\n    #   f4 <- large checkpoint file (via copy_from_local)\n    # dir3/\n    #   dir4/\n    #############################################\n\n    #############################################\n    # auth\n    # Just check that client loads properly\n    #############################################\n    @unittest.skipIf(not s3_auth, skip_s3_auth_required_tests_message)\n    def test_01_add_client_to_handler(self):\n        self.s3_pathhandler._get_client(\n            \"/\".join([self.s3_full_path, \"path\", \"file.txt\"])\n        )\n        # self.assertTrue(isinstance(self.s3_pathhandler.client, botocore.client.S3)) # TODO\n\n    # TODO: make sure that the error message displays properly if authentication is messed up.\n\n    #############################################\n    # mkdirs\n    # Set up the dirs\n    # (in BASE)\n    #   +dir1\n    #   +dir2\n    #   +dir3\n    #   +dir4\n    #############################################\n    @unittest.skipIf(not s3_auth, skip_s3_auth_required_tests_message)\n    def test_02_mkdirs_must_end_with_slash(self):\n        with self.assertRaises(AssertionError):\n            self.s3_pathhandler._mkdirs(\"/\".join([self.s3_full_path, \"fail\"]))\n\n    @unittest.skipIf(not s3_auth, skip_s3_auth_required_tests_message)\n    def test_03_mkdirs(self):\n        # dir{1,2,3} in BASE\n        for i in (1, 2, 3):\n            self.s3_pathhandler._mkdirs(\"/\".join([self.s3_full_path, f\"dir{i}/\"]))\n\n        # Make a nested directory in dir3\n        self.s3_pathhandler._mkdirs(\"/\".join([self.s3_full_path, \"dir3/dir4/\"]))\n\n    #############################################\n    # open (w/wb)\n    #   +f1\n    #   +f2\n    #############################################\n    @unittest.skipIf(not s3_auth, skip_s3_auth_required_tests_message)\n    def test_04_open_write_mode(self):\n        with self.s3_pathhandler._open(\n            \"/\".join([self.s3_full_path, \"dir1\", \"f1_write_string\"]), \"w\"\n        ) as f:\n            f.write(\"This is a test of writing a string.\")\n\n        with self.s3_pathhandler._open(\n            \"/\".join([self.s3_full_path, \"dir1\", \"f2_write_bytes\"]), \"wb\"\n        ) as f:\n            f.write(b\"This is a test of writing bytes.\")\n\n    #############################################\n    # open (r/rb)\n    #   read f1\n    #   read f2\n    #############################################\n    @unittest.skipIf(not s3_auth, skip_s3_auth_required_tests_message)\n    def test_05_open_read_mode(self):\n        with self.s3_pathhandler._open(\n            \"/\".join([self.s3_full_path, \"dir1\", \"f1_write_string\"]), \"r\"\n        ) as f:\n            self.assertEqual(f.read(), \"This is a test of writing a string.\")\n\n        with self.s3_pathhandler._open(\n            \"/\".join([self.s3_full_path, \"dir1\", \"f2_write_bytes\"]), \"rb\"\n        ) as f:\n            self.assertEqual(f.read(), b\"This is a test of writing bytes.\")\n\n    #############################################\n    # isdir / isfile / exists\n    #   test dir{1,2,3,4}\n    #   test f{1,2}\n    #   test nonexistants\n    #############################################\n    @unittest.skipIf(not s3_auth, skip_s3_auth_required_tests_message)\n    def test_06_exists(self):\n        # Path does not exist (if file)\n        self.assertFalse(\n            self.s3_pathhandler._exists(\"/\".join([self.s3_full_path, \"dir1\", \"FAIL\"]))\n        )\n        # Path does not exist (if dir)\n        self.assertFalse(\n            self.s3_pathhandler._exists(\"/\".join([self.s3_full_path, \"FAIL/\"]))\n        )\n        # Path exists (is file)\n        self.assertTrue(\n            self.s3_pathhandler._exists(\n                \"/\".join([self.s3_full_path, \"dir1\", \"f1_write_string\"])\n            )\n        )\n        # Path exists (is dir)\n        self.assertTrue(\n            self.s3_pathhandler._exists(\"/\".join([self.s3_full_path, \"dir1/\"]))\n        )\n\n    @unittest.skipIf(not s3_auth, skip_s3_auth_required_tests_message)\n    def test_07_isdir(self):\n        # Path does not exist (if file)\n        self.assertFalse(\n            self.s3_pathhandler._isdir(\"/\".join([self.s3_full_path, \"dir1\", \"FAIL\"]))\n        )\n        # Path does not exist (if dir)\n        self.assertFalse(\n            self.s3_pathhandler._isdir(\"/\".join([self.s3_full_path, \"FAIL/\"]))\n        )\n        # Path exists (is file)\n        self.assertFalse(\n            self.s3_pathhandler._isdir(\n                \"/\".join([self.s3_full_path, \"dir1\", \"f1_write_string\"])\n            )\n        )\n        # Path exists (is dir)\n        self.assertTrue(\n            self.s3_pathhandler._isdir(\"/\".join([self.s3_full_path, \"dir1/\"]))\n        )\n\n    @unittest.skipIf(not s3_auth, skip_s3_auth_required_tests_message)\n    def test_08_isfile(self):\n        # Path does not exist (if file)\n        self.assertFalse(\n            self.s3_pathhandler._isfile(\"/\".join([self.s3_full_path, \"dir1\", \"FAIL\"]))\n        )\n        # Path does not exist (if dir)\n        self.assertFalse(\n            self.s3_pathhandler._isfile(\"/\".join([self.s3_full_path, \"FAIL/\"]))\n        )\n        # Path exists (is file)\n        self.assertTrue(\n            self.s3_pathhandler._isfile(\n                \"/\".join([self.s3_full_path, \"dir1\", \"f1_write_string\"])\n            )\n        )\n        # Path exists (is dir)\n        self.assertFalse(\n            self.s3_pathhandler._isfile(\"/\".join([self.s3_full_path, \"dir1/\"]))\n        )\n\n    #############################################\n    # copy\n    #   copy f1 -> f3\n    #############################################\n    @unittest.skipIf(not s3_auth, skip_s3_auth_required_tests_message)\n    def test_09_copy(self):\n        self.assertTrue(\n            self.s3_pathhandler._copy(\n                \"/\".join([self.s3_full_path, \"dir1\", \"f1_write_string\"]),\n                \"/\".join([self.s3_full_path, \"dir2\", \"f3_write_string\"]),\n            )\n        )\n\n    #############################################\n    # ls\n    #   ls dir{1,2,3,4}\n    #############################################\n    @unittest.skipIf(not s3_auth, skip_s3_auth_required_tests_message)\n    def test_10_ls(self):\n        # Path does not exist (if file)\n        self.assertEqual(\n            [], self.s3_pathhandler._ls(\"/\".join([self.s3_full_path, \"dir1\", \"FAIL\"]))\n        )\n        # Path does not exist (if dir)\n        self.assertEqual(\n            [], self.s3_pathhandler._ls(\"/\".join([self.s3_full_path, \"FAIL/\"]))\n        )\n        # Path exists (is file)\n        self.assertEqual(\n            [\"/\".join([self.s3_rel_path, \"dir1\", \"f1_write_string\"])],\n            self.s3_pathhandler._ls(\n                \"/\".join([self.s3_full_path, \"dir1\", \"f1_write_string\"])\n            ),\n        )\n        # Path exists (is dir)\n        self.assertEqual(\n            {\n                \"/\".join(\n                    [self.s3_rel_path, \"dir1/\"]\n                ),  # TODO: should the trailing slash be\n                \"/\".join([self.s3_rel_path, \"dir1\", \"f1_write_string\"]),\n                \"/\".join([self.s3_rel_path, \"dir1\", \"f2_write_bytes\"]),\n            },\n            set(self.s3_pathhandler._ls(\"/\".join([self.s3_full_path, \"dir1/\"]))),\n        )\n\n    #############################################\n    # rm\n    #   rm f3\n    #############################################\n    @unittest.skipIf(not s3_auth, skip_s3_auth_required_tests_message)\n    def test_11_rm(self):\n        path = \"/\".join([self.s3_full_path, \"dir2\", \"f3_write_string\"])\n        self.assertTrue(self.s3_pathhandler._exists(path))\n        self.assertTrue(self.s3_pathhandler._isfile(path))\n        self.assertFalse(self.s3_pathhandler._isdir(path))\n\n        self.s3_pathhandler._rm(path)\n\n        self.assertFalse(self.s3_pathhandler._exists(path))\n        self.assertFalse(self.s3_pathhandler._isfile(path))\n        self.assertFalse(self.s3_pathhandler._isdir(path))\n\n    #############################################\n    # get_local_path\n    #   Retrieve f{1,2}\n    #   Check file contents.\n    #############################################\n    @unittest.skipIf(not s3_auth, skip_s3_auth_required_tests_message)\n    def test_12_get_local_path(self):\n        s3_path_f1 = \"/\".join([self.s3_full_path, \"dir1\", \"f1_write_string\"])\n        s3_path_f2 = \"/\".join([self.s3_full_path, \"dir1\", \"f2_write_bytes\"])\n\n        local_path_f1 = self.s3_pathhandler._get_local_path(s3_path_f1)\n        local_path_f2 = self.s3_pathhandler._get_local_path(s3_path_f2)\n\n        with open(local_path_f1, \"r\") as f:\n            self.assertEqual(f.read(), \"This is a test of writing a string.\")\n        with open(local_path_f2, \"rb\") as f:\n            self.assertEqual(f.read(), b\"This is a test of writing bytes.\")\n\n    @unittest.skipIf(not s3_auth, skip_s3_auth_required_tests_message)\n    def test_13_get_local_path_idempotent(self):\n        \"\"\"\n        Call _get_local_path multiple times.\n        Check that we keep returning the same cached copy instead of redownloading.\n        \"\"\"\n        s3_path_f1 = \"/\".join([self.s3_full_path, \"dir1\", \"f1_write_string\"])\n\n        REPEATS = 3\n        local_paths = [\n            self.s3_pathhandler._get_local_path(s3_path_f1) for _ in range(REPEATS)\n        ]\n        for local_path in local_paths[1:]:\n            self.assertEqual(local_path, local_paths[0])\n\n        with open(local_paths[0], \"r\") as f:\n            self.assertEqual(f.read(), \"This is a test of writing a string.\")\n\n    # TODO: make sure it fails if asked for a directory\n    # TODO: make sure that the returned path is appropriately placed.\n\n    ##############################################\n    # copy_from_local\n    #   Upload local copies of f1, f2 -> f3, f4.\n    # Check contents via open(), and via another get_local_path\n    ##############################################\n    @unittest.skipIf(not s3_auth, skip_s3_auth_required_tests_message)\n    def test_14_copy_from_local(self):\n        s3_src_path_f1 = \"/\".join([self.s3_full_path, \"dir1\", \"f1_write_string\"])\n        s3_src_path_f2 = \"/\".join([self.s3_full_path, \"dir1\", \"f2_write_bytes\"])\n\n        local_path_f1 = self.s3_pathhandler._get_local_path(s3_src_path_f1)\n        local_path_f2 = self.s3_pathhandler._get_local_path(s3_src_path_f2)\n\n        s3_dst_path_f1 = \"/\".join(\n            [self.s3_full_path, \"dir2\", \"f1_write_string_from_local\"]\n        )\n        s3_dst_path_f2 = \"/\".join(\n            [self.s3_full_path, \"dir2\", \"f2_write_bytes_from_local\"]\n        )\n\n        self.assertTrue(\n            self.s3_pathhandler._copy_from_local(local_path_f1, s3_dst_path_f1)\n        )\n        self.assertTrue(\n            self.s3_pathhandler._copy_from_local(local_path_f2, s3_dst_path_f2)\n        )\n\n    #############################################\n    # symlink\n    #   should fail\n    #############################################\n    @unittest.skipIf(not s3_auth, skip_s3_auth_required_tests_message)\n    def test_15_symlink(self):\n        s3_src_path_f1 = \"/\".join([self.s3_full_path, \"dir1\", \"f1_write_string\"])\n        s3_dst_path_f1 = \"/\".join(\n            [self.s3_full_path, \"dir2\", \"f1_write_string_symlink\"]\n        )\n        with self.assertRaises(NotImplementedError):\n            self.s3_pathhandler._symlink(s3_src_path_f1, s3_dst_path_f1)\n\n\nif __name__ == \"__main__\":\n    unittest.main()\n",
        "tests/test_export.py": "#!/usr/bin/env python3\n# Copyright (c) Meta Platforms, Inc. and affiliates. All Rights Reserved.\n#\n# This source code is licensed under the MIT license found in the\n# LICENSE file in the root directory of this source tree.\n\nimport argparse\nimport tempfile\nimport unittest\n\nimport torch\n\nfrom metaseq.data.dictionary import Dictionary\nfrom metaseq.modules import multihead_attention, sinusoidal_positional_embedding\nfrom metaseq.tasks.base_task import LegacyTask\n\nDEFAULT_TEST_VOCAB_SIZE = 100\n\n\nclass DummyTask(LegacyTask):\n    def __init__(self, args):\n        super().__init__(args)\n        self.dictionary = get_dummy_dictionary()\n        if getattr(self.args, \"ctc\", False):\n            self.dictionary.add_symbol(\"<ctc_blank>\")\n        self.src_dict = self.dictionary\n        self.tgt_dict = self.dictionary\n\n    @property\n    def source_dictionary(self):\n        return self.src_dict\n\n    @property\n    def target_dictionary(self):\n        return self.dictionary\n\n\ndef get_dummy_dictionary(vocab_size=DEFAULT_TEST_VOCAB_SIZE):\n    dummy_dict = Dictionary()\n    # add dummy symbol to satisfy vocab size\n    for id, _ in enumerate(range(vocab_size)):\n        dummy_dict.add_symbol(\"{}\".format(id), 1000)\n    return dummy_dict\n\n\ndef get_dummy_task_and_parser():\n    \"\"\"\n    Return a dummy task and argument parser, which can be used to\n    create a model/criterion.\n    \"\"\"\n    parser = argparse.ArgumentParser(\n        description=\"test_dummy_s2s_task\", argument_default=argparse.SUPPRESS\n    )\n    DummyTask.add_args(parser)\n    args = parser.parse_args([])\n    task = DummyTask.setup_task(args)\n    return task, parser\n\n\ndef _test_save_and_load(scripted_module):\n    with tempfile.NamedTemporaryFile() as f:\n        scripted_module.save(f.name)\n        torch.jit.load(f.name)\n\n\nclass TestExportModels(unittest.TestCase):\n    def test_export_multihead_attention(self):\n        module = multihead_attention.ModelParallelMultiheadAttention(\n            embed_dim=8, num_heads=2\n        )\n        scripted = torch.jit.script(module)\n        _test_save_and_load(scripted)\n\n    def test_incremental_state_multihead_attention(self):\n        module1 = multihead_attention.ModelParallelMultiheadAttention(\n            embed_dim=8, num_heads=2\n        )\n        module1 = torch.jit.script(module1)\n        module2 = multihead_attention.ModelParallelMultiheadAttention(\n            embed_dim=8, num_heads=2\n        )\n        module2 = torch.jit.script(module2)\n\n        state = {}\n        state = module1.set_incremental_state(state, \"key\", {\"a\": torch.tensor([1])})\n        state = module2.set_incremental_state(state, \"key\", {\"a\": torch.tensor([2])})\n        v1 = module1.get_incremental_state(state, \"key\")[\"a\"]\n        v2 = module2.get_incremental_state(state, \"key\")[\"a\"]\n\n        self.assertEqual(v1, 1)\n        self.assertEqual(v2, 2)\n\n    def test_positional_embedding(self):\n        module = sinusoidal_positional_embedding.SinusoidalPositionalEmbedding(\n            embedding_dim=8, padding_idx=1\n        )\n        scripted = torch.jit.script(module)\n        _test_save_and_load(scripted)\n\n\nif __name__ == \"__main__\":\n    unittest.main()\n",
        "tests/test_lm_context_window.py": "# Copyright (c) Meta Platforms, Inc. and affiliates. All Rights Reserved.\n#\n# This source code is licensed under the MIT license found in the\n# LICENSE file in the root directory of this source tree.\n\nimport unittest\n\nimport torch\n\nfrom metaseq.data import MonolingualDataset\nfrom metaseq.tasks.language_modeling import LanguageModelingTask, LanguageModelingConfig\nfrom tests import utils as test_utils\n\n\nclass TestLMContextWindow(unittest.TestCase):\n    def test_eval_dataloader(self):\n        dictionary = test_utils.dummy_dictionary(10)\n        assert len(dictionary) == 14  # 4 extra special symbols\n        assert dictionary.pad() == 1\n\n        dataset = test_utils.TestDataset(\n            [\n                torch.tensor([4, 5, 6, 7], dtype=torch.long),\n                torch.tensor([8, 9, 10, 11], dtype=torch.long),\n                torch.tensor([12, 13], dtype=torch.long),\n            ]\n        )\n        dataset = MonolingualDataset(dataset, sizes=[4, 4, 2], src_vocab=dictionary)\n\n        config = LanguageModelingConfig(tokens_per_sample=4)\n        task = LanguageModelingTask(config, dictionary)\n\n        eval_dataloader = task.eval_lm_dataloader(\n            dataset=dataset,\n            batch_size=1,\n            context_window=2,\n        )\n\n        batch = next(eval_dataloader)\n        assert batch[\"net_input\"][\"src_tokens\"][0].tolist() == [4, 5, 6, 7, 1, 1]\n        assert batch[\"target\"][0].tolist() == [4, 5, 6, 7, 1, 1]\n\n        batch = next(eval_dataloader)\n        assert batch[\"net_input\"][\"src_tokens\"][0].tolist() == [6, 7, 8, 9, 10, 11]\n        assert batch[\"target\"][0].tolist() == [1, 1, 8, 9, 10, 11]\n\n        batch = next(eval_dataloader)\n        assert batch[\"net_input\"][\"src_tokens\"][0].tolist() == [10, 11, 12, 13]\n        assert batch[\"target\"][0].tolist() == [1, 1, 12, 13]\n\n\nif __name__ == \"__main__\":\n    unittest.main()\n",
        "tests/test_memory_efficient_fp16.py": "# Copyright (c) Meta Platforms, Inc. and affiliates. All Rights Reserved.\n#\n# This source code is licensed under the MIT license found in the\n# LICENSE file in the root directory of this source tree.\n\nimport argparse\nimport logging\nimport unittest\n\nimport torch\nfrom omegaconf import OmegaConf\n\nfrom metaseq.optim.adam import MetaseqAdam\nfrom metaseq.optim.fp16_optimizer import MemoryEfficientFP16Optimizer\n\n\n@unittest.skipIf(not torch.cuda.is_available(), \"test requires a GPU\")\nclass TestMemoryEfficientFP16(unittest.TestCase):\n    def setUp(self):\n        logging.disable(logging.CRITICAL)\n\n    def tearDown(self):\n        logging.disable(logging.NOTSET)\n\n    def test_load_state_dict(self):\n        # define simple FP16 model\n        model = torch.nn.Linear(5, 5).cuda().half()\n        params = list(model.parameters())\n\n        # initialize memory efficient FP16 optimizer\n        # with pseudo DictConfigs\n        optimizer = MetaseqAdam(\n            cfg=OmegaConf.create(\n                vars(\n                    argparse.Namespace(\n                        adam_betas=\"(0.9, 0.999)\",\n                        adam_eps=1e-8,\n                        weight_decay=0.0,\n                        lr=[0.00001],\n                    )\n                )\n            ),\n            params=params,\n        )\n        me_optimizer = MemoryEfficientFP16Optimizer(\n            cfg=OmegaConf.create(\n                {\n                    \"common\": vars(\n                        argparse.Namespace(\n                            fp16_init_scale=1,\n                            fp16_scale_window=1,\n                            fp16_scale_tolerance=1,\n                            threshold_loss_scale=1,\n                            min_loss_scale=1e-4,\n                        )\n                    )\n                }\n            ),\n            params=params,\n            optimizer=optimizer,\n        )\n\n        # optimizer state is created in the first step\n        loss = model(torch.rand(5).cuda().half()).sum()\n        me_optimizer.backward(loss)\n        me_optimizer.step()\n\n        # reload state\n        state = me_optimizer.state_dict()\n        me_optimizer.load_state_dict(state)\n        for k, v in me_optimizer.optimizer.state.items():\n            self.assertTrue(k.dtype == torch.float16)\n            for v_i in v.values():\n                if torch.is_tensor(v_i):\n                    self.assertTrue(v_i.dtype == torch.float32)\n\n\nif __name__ == \"__main__\":\n    unittest.main()\n",
        "tests/test_polynomial_lr.py": "# Copyright (c) Meta Platforms, Inc. and affiliates. All Rights Reserved.\n#\n# This source code is licensed under the MIT license found in the\n# LICENSE file in the root directory of this source tree.\nimport argparse\nimport logging\nimport unittest\n\nimport torch\nfrom omegaconf import OmegaConf\n\nfrom metaseq.optim.adam import MetaseqAdam\nfrom metaseq.optim.lr_scheduler.polynomial_decay_schedule import (\n    PolynomialDecayLRSchedule,\n)\n\n\nclass TestPolynomialLRScheduler(unittest.TestCase):\n    def setUp(self):\n        self.x = torch.tensor([2.0])\n        weight = 3.0\n        bias = 5.0\n        error = 1.0\n        self.target = torch.tensor([self.x * weight + bias + error])\n        self.loss_fn = torch.nn.L1Loss()\n\n        self.model = torch.nn.Linear(1, 1)\n        self.model.weight.data = torch.tensor([[weight]])\n        self.model.bias.data = torch.tensor([bias])\n        self.params = list(self.model.parameters())\n        logging.disable(logging.CRITICAL)\n\n    def tearDown(self):\n        logging.disable(logging.NOTSET)\n\n    def _get_adam(self, starting_lr):\n        return MetaseqAdam(\n            cfg=OmegaConf.create(\n                vars(\n                    argparse.Namespace(\n                        adam_betas=\"(0.9, 0.999)\",\n                        adam_eps=1e-8,\n                        weight_decay=0.0,\n                        lr=[starting_lr],\n                    )\n                )\n            ),\n            params=self.params,\n        )\n\n    @staticmethod\n    def _get_polynomial_lr_schedule(\n        warmup_updates,\n        power,\n        total_updates,\n        starting_lr,\n        end_lr,\n        zero_lr_warmup_steps,\n        optimizer,\n    ):\n        return PolynomialDecayLRSchedule(\n            cfg=OmegaConf.create(\n                vars(\n                    argparse.Namespace(\n                        warmup_updates=warmup_updates,\n                        end_learning_rate=end_lr,\n                        power=power,\n                        total_num_update=total_updates,\n                        lr=[starting_lr],\n                        zero_lr_warmup_steps=zero_lr_warmup_steps,\n                    )\n                )\n            ),\n            optimizer=optimizer,\n        )\n\n    def test_polynomial_decay_no_adam_warmup(self):\n        starting_lr = 0.1\n        total_updates = 50\n        warmup_updates = 20\n        adam_warmup = 0\n        power = 1\n        adam_optim = self._get_adam(starting_lr)\n        # Test setting end_lr, adam_warmup = 0\n        end_lr = starting_lr * 0.1\n        lr_sched = self._get_polynomial_lr_schedule(\n            warmup_updates,\n            power,\n            total_updates,\n            starting_lr,\n            end_lr,\n            adam_warmup,\n            adam_optim,\n        )\n        # Init warmup period, halfway mark\n        self.assertAlmostEqual(\n            lr_sched.step_update(warmup_updates // 2), starting_lr * 0.5\n        )\n        # Done warming up\n        self.assertAlmostEqual(lr_sched.step_update(warmup_updates), starting_lr)\n        # Linear decay, halfway mark\n        halfway = warmup_updates + (total_updates - warmup_updates) // 2\n        self.assertAlmostEqual(\n            lr_sched.step_update(halfway), end_lr + (starting_lr - end_lr) * 0.5\n        )\n        # End of decay\n        self.assertAlmostEqual(lr_sched.step_update(total_updates), end_lr)\n\n        # Test power == 2\n        power = 2\n        end_lr = 0\n        lr_sched = self._get_polynomial_lr_schedule(\n            warmup_updates,\n            power,\n            total_updates,\n            starting_lr,\n            end_lr,\n            adam_warmup,\n            adam_optim,\n        )\n        # Init warmup period, halfway mark\n        self.assertAlmostEqual(\n            lr_sched.step_update(warmup_updates // 2), starting_lr * 0.5\n        )\n        # Done warming up\n        self.assertAlmostEqual(lr_sched.step_update(warmup_updates), starting_lr)\n        # Polynomial power == 2 decay, halfway mark\n        self.assertAlmostEqual(\n            lr_sched.step_update(halfway), end_lr + (starting_lr - end_lr) * 0.5**2\n        )\n\n    def test_polynomial_decay_with_adam_warmup(self):\n        starting_lr = 0.1\n        total_updates = 50\n        warmup_updates = 20\n        adam_warmup = 10\n        power = 1\n        adam_optim = self._get_adam(starting_lr)\n        end_lr = starting_lr * 0.1\n        lr_sched = self._get_polynomial_lr_schedule(\n            warmup_updates,\n            power,\n            total_updates,\n            starting_lr,\n            end_lr,\n            adam_warmup,\n            adam_optim,\n        )\n        # Init warmup period, during adam warmup\n        self.assertEqual(lr_sched.step_update(adam_warmup // 2), 0)\n        # Init warmup period, past adam warmup\n        self.assertAlmostEqual(\n            lr_sched.step_update(warmup_updates // 2 + adam_warmup), starting_lr * 0.5\n        )\n        # Done warming up\n        total_warmup = adam_warmup + warmup_updates\n        self.assertAlmostEqual(lr_sched.step_update(total_warmup), starting_lr)\n        # Linear decay, halfway mark\n        halfway = total_warmup + (total_updates - total_warmup) // 2\n        self.assertAlmostEqual(\n            lr_sched.step_update(halfway), end_lr + (starting_lr - end_lr) * 0.5\n        )\n        # End of decay\n        self.assertAlmostEqual(lr_sched.step_update(total_updates), end_lr)\n\n\nif __name__ == \"__main__\":\n    unittest.main()\n",
        "tests/test_resampling_dataset.py": "# Copyright (c) Meta Platforms, Inc. and affiliates. All Rights Reserved.\n#\n# This source code is licensed under the MIT license found in the\n# LICENSE file in the root directory of this source tree.\n\nimport collections\nimport unittest\n\nimport numpy as np\n\nfrom metaseq.data import ListDataset, ResamplingDataset\n\n\nclass TestResamplingDataset(unittest.TestCase):\n    def setUp(self):\n        self.strings = [\"ab\", \"c\", \"def\", \"ghij\"]\n        self.weights = [4.0, 2.0, 7.0, 1.5]\n        self.size_ratio = 2\n        self.dataset = ListDataset(\n            self.strings, np.array([len(s) for s in self.strings])\n        )\n\n    def _test_common(self, resampling_dataset, iters):\n        assert len(self.dataset) == len(self.strings) == len(self.weights)\n        assert len(resampling_dataset) == self.size_ratio * len(self.strings)\n\n        results = {\"ordered_by_size\": True, \"max_distribution_diff\": 0.0}\n\n        totalfreqs = 0\n        freqs = collections.defaultdict(int)\n\n        for epoch_num in range(iters):\n            resampling_dataset.set_epoch(epoch_num)\n\n            indices = resampling_dataset.ordered_indices()\n            assert len(indices) == len(resampling_dataset)\n\n            prev_size = -1\n\n            for i in indices:\n                cur_size = resampling_dataset.size(i)\n                # Make sure indices map to same sequences within an epoch\n                assert resampling_dataset[i] == resampling_dataset[i]\n\n                # Make sure length of sequence is correct\n                assert cur_size == len(resampling_dataset[i])\n\n                freqs[resampling_dataset[i]] += 1\n                totalfreqs += 1\n\n                if prev_size > cur_size:\n                    results[\"ordered_by_size\"] = False\n\n                prev_size = cur_size\n\n        assert set(freqs.keys()) == set(self.strings)\n        for s, weight in zip(self.strings, self.weights):\n            freq = freqs[s] / totalfreqs\n            expected_freq = weight / sum(self.weights)\n            results[\"max_distribution_diff\"] = max(\n                results[\"max_distribution_diff\"], abs(expected_freq - freq)\n            )\n\n        return results\n\n    def test_resampling_dataset_batch_by_size_false(self):\n        resampling_dataset = ResamplingDataset(\n            self.dataset,\n            self.weights,\n            size_ratio=self.size_ratio,\n            batch_by_size=False,\n            seed=0,\n        )\n\n        results = self._test_common(resampling_dataset, iters=1000)\n\n        # For batch_by_size = False, the batches should be returned in\n        # arbitrary order of size.\n        assert not results[\"ordered_by_size\"]\n\n        # Allow tolerance in distribution error of 2%.\n        assert results[\"max_distribution_diff\"] < 0.02\n\n    def test_resampling_dataset_batch_by_size_true(self):\n        resampling_dataset = ResamplingDataset(\n            self.dataset,\n            self.weights,\n            size_ratio=self.size_ratio,\n            batch_by_size=True,\n            seed=0,\n        )\n\n        results = self._test_common(resampling_dataset, iters=1000)\n\n        # For batch_by_size = True, the batches should be returned in\n        # increasing order of size.\n        assert results[\"ordered_by_size\"]\n\n        # Allow tolerance in distribution error of 2%.\n        assert results[\"max_distribution_diff\"] < 0.02\n\n\nif __name__ == \"__main__\":\n    unittest.main()\n",
        "tests/test_sequence_generator.py": "# Copyright (c) Meta Platforms, Inc. and affiliates. All Rights Reserved.\n#\n# This source code is licensed under the MIT license found in the\n# LICENSE file in the root directory of this source tree.\n\nimport argparse\nimport tempfile\nimport unittest\n\nimport torch\n\nimport tests.utils as test_utils\nfrom metaseq.data.dictionary import Dictionary\nfrom metaseq.models.transformer_lm import TransformerLanguageModel\nfrom metaseq.sequence_generator import SequenceGenerator\nfrom metaseq.tasks.base_task import LegacyTask\n\nDEFAULT_TEST_VOCAB_SIZE = 100\n\n\nclass DummyTask(LegacyTask):\n    def __init__(self, args):\n        super().__init__(args)\n        self.dictionary = get_dummy_dictionary()\n        if getattr(self.args, \"ctc\", False):\n            self.dictionary.add_symbol(\"<ctc_blank>\")\n        self.src_dict = self.dictionary\n        self.tgt_dict = self.dictionary\n\n    @property\n    def source_dictionary(self):\n        return self.src_dict\n\n    @property\n    def target_dictionary(self):\n        return self.dictionary\n\n\ndef get_dummy_dictionary(vocab_size=DEFAULT_TEST_VOCAB_SIZE):\n    dummy_dict = Dictionary()\n    # add dummy symbol to satisfy vocab size\n    for id, _ in enumerate(range(vocab_size)):\n        dummy_dict.add_symbol(\"{}\".format(id), n=1000)\n    return dummy_dict\n\n\ndef get_dummy_task_and_parser():\n    \"\"\"\n    to build a fariseq model, we need some dummy parse and task. This function\n    is used to create dummy task and parser to faciliate model/criterion test\n\n    Note: we use FbSpeechRecognitionTask as the dummy task. You may want\n    to use other task by providing another function\n    \"\"\"\n    parser = argparse.ArgumentParser(\n        description=\"test_dummy_s2s_task\", argument_default=argparse.SUPPRESS\n    )\n    DummyTask.add_args(parser)\n    args = parser.parse_args([])\n    task = DummyTask.setup_task(args)\n    return task, parser\n\n\nclass TestJitSequenceGeneratorBase(unittest.TestCase):\n    def setUp(self):\n        self.task, self.parser = get_dummy_task_and_parser()\n        eos = self.task.tgt_dict.eos()\n        src_tokens = torch.randint(3, 50, (2, 10)).long()\n        src_tokens = torch.cat((src_tokens, torch.LongTensor([[eos], [eos]])), -1)\n        src_lengths = torch.LongTensor([2, 10])\n        self.sample = {\n            \"net_input\": {\"src_tokens\": src_tokens, \"src_lengths\": src_lengths}\n        }\n        TransformerLanguageModel.add_args(self.parser)\n        args = self.parser.parse_args([])\n        args.decoder_layers = 1\n        self.transformer_model = TransformerLanguageModel.build_model(args, self.task)\n\n    def assertOutputEqual(self, hypo, pos_probs):\n        pos_scores = torch.FloatTensor(pos_probs).log()\n        self.assertTensorSizeEqual(hypo[\"positional_scores\"], pos_scores)\n        self.assertTensorSizeEqual(pos_scores.numel(), hypo[\"tokens\"].numel())\n\n    def assertTensorSizeEqual(self, t1, t2):\n        self.assertEqual(t1.size(), t2.size(), \"size mismatch\")\n\n    def assertAlmostEqual(self, t1, t2):\n        self.assertEqual(t1.size(), t2.size(), \"size mismatch\")\n        self.assertLess((t1 - t2).abs().max(), 1e-4)\n\n    def assertTensorEqual(self, t1, t2):\n        self.assertEqual(t1.size(), t2.size(), \"size mismatch\")\n        self.assertEqual(t1.ne(t2).long().sum(), 0)\n\n    def assertHypoEqual(self, h1, h2):\n        \"Check two hypos are equal\"\n        self.assertTensorEqual(h1[\"tokens\"], h2[\"tokens\"])\n        self.assertAlmostEqual(h1[\"positional_scores\"], h2[\"positional_scores\"])\n        self.assertLess(abs(h1[\"score\"] - h2[\"score\"]), 1e-6)\n        self.assertAlmostEqual(h1[\"attention\"], h2[\"attention\"])\n\n    def _test_save_and_load(self, scripted_module):\n        with tempfile.NamedTemporaryFile() as f:\n            scripted_module.save(f.name)\n            torch.jit.load(f.name)\n\n\nJIT_MSG = \"Targeting OSS scriptability for the 1.6 release\"\n\n\n@unittest.skipIf(torch.__version__ < \"1.6.0\", JIT_MSG)\nclass TestJitSequenceGenerator(TestJitSequenceGeneratorBase):\n    def test_export_transformer(self):\n        model = self.transformer_model\n        torch.jit.script(model)\n\n    def test_sequence_generator(self):\n        model = self.transformer_model\n        generator = SequenceGenerator(\n            [model],\n            self.task.tgt_dict,\n            beam_size=2,\n            max_len_b=10,\n        )\n        scripted_model = torch.jit.script(generator)\n        self._test_save_and_load(scripted_model)\n\n\nclass TestSequenceGeneratorBase(unittest.TestCase):\n    def assertHypoTokens(self, hypo, tokens):\n        self.assertTensorEqual(hypo[\"tokens\"], torch.LongTensor(tokens))\n\n    def assertHypoScore(self, hypo, pos_probs, normalized=True):\n        pos_scores = torch.FloatTensor(pos_probs).log()\n        self.assertAlmostEqual(hypo[\"positional_scores\"], pos_scores)\n        self.assertEqual(pos_scores.numel(), hypo[\"tokens\"].numel())\n        score = pos_scores.sum()\n        if normalized:\n            score /= pos_scores.numel()\n        self.assertLess(abs(score - hypo[\"score\"]), 1e-6)\n\n    def assertAlmostEqual(self, t1, t2):\n        self.assertEqual(t1.size(), t2.size(), \"size mismatch\")\n        self.assertLess((t1 - t2).abs().max(), 1e-4)\n\n    def assertTensorEqual(self, t1, t2):\n        self.assertEqual(t1.size(), t2.size(), \"size mismatch\")\n        self.assertEqual(t1.ne(t2).long().sum(), 0)\n\n\nclass TestSequenceGenerator(TestSequenceGeneratorBase):\n    def setUp(self):\n        (\n            self.tgt_dict,\n            self.w1,\n            self.w2,\n            src_tokens,\n            src_lengths,\n            self.model,\n        ) = test_utils.sequence_generator_setup()\n        self.sample = {\n            \"net_input\": {\"src_tokens\": src_tokens, \"src_lengths\": src_lengths}\n        }\n\n    def test_with_normalization(self):\n        generator = SequenceGenerator([self.model], self.tgt_dict, beam_size=2)\n        hypos = generator.forward(self.sample)\n        eos, w1, w2 = self.tgt_dict.eos(), self.w1, self.w2\n        # sentence 1, beam 1\n        self.assertHypoTokens(hypos[0][0], [w1, eos])\n        self.assertHypoScore(hypos[0][0], [0.9, 1.0])\n        # sentence 1, beam 2\n        self.assertHypoTokens(hypos[0][1], [w2, w1, w2, eos])\n        self.assertHypoScore(hypos[0][1], [0.1, 0.9, 0.9, 1.0])\n        # sentence 2, beam 1\n        self.assertHypoTokens(hypos[1][0], [w1, w2, w1, eos])\n        self.assertHypoScore(hypos[1][0], [0.7, 0.4, 0.4, 1.0])\n        # sentence 2, beam 2\n        self.assertHypoTokens(hypos[1][1], [w1, w2, eos])\n        self.assertHypoScore(hypos[1][1], [0.7, 0.4, 0.6])\n\n    def test_without_normalization(self):\n        # Sentence 1: unchanged from the normalized case\n        # Sentence 2: beams swap order\n        generator = SequenceGenerator([self.model], self.tgt_dict, beam_size=2)\n        hypos = generator.forward(self.sample)\n        eos, w1, w2 = self.tgt_dict.eos(), self.w1, self.w2\n        # sentence 1, beam 1\n        self.assertHypoTokens(hypos[0][0], [w1, eos])\n        self.assertHypoScore(hypos[0][0], [0.9, 1.0], normalized=False)\n        # sentence 1, beam 2\n        self.assertHypoTokens(hypos[0][1], [w2, w1, w2, eos])\n        self.assertHypoScore(hypos[0][1], [0.1, 0.9, 0.9, 1.0], normalized=False)\n        # sentence 2, beam 1\n        self.assertHypoTokens(hypos[1][0], [w1, w2, eos])\n        self.assertHypoScore(hypos[1][0], [0.7, 0.4, 0.6], normalized=False)\n        # sentence 2, beam 2\n        self.assertHypoTokens(hypos[1][1], [w1, w2, w1, eos])\n        self.assertHypoScore(hypos[1][1], [0.7, 0.4, 0.4, 1.0], normalized=False)\n\n    def test_maxlen(self):\n        generator = SequenceGenerator(\n            [self.model], self.tgt_dict, beam_size=2, max_len_b=2\n        )\n        hypos = generator.forward(self.sample)\n        eos, w1, w2 = self.tgt_dict.eos(), self.w1, self.w2\n        # sentence 1, beam 1\n        self.assertHypoTokens(hypos[0][0], [w1, eos])\n        self.assertHypoScore(hypos[0][0], [0.9, 1.0])\n        # sentence 1, beam 2\n        self.assertHypoTokens(hypos[0][1], [w2, w2, eos])\n        self.assertHypoScore(hypos[0][1], [0.1, 0.1, 0.6])\n        # sentence 2, beam 1\n        self.assertHypoTokens(hypos[1][0], [w1, w2, eos])\n        self.assertHypoScore(hypos[1][0], [0.7, 0.4, 0.6])\n        # sentence 2, beam 2\n        self.assertHypoTokens(hypos[1][1], [w2, w2, eos])\n        self.assertHypoScore(hypos[1][1], [0.3, 0.9, 0.01])\n\n\nif __name__ == \"__main__\":\n    unittest.main()\n",
        "tests/test_streaming_language_modeling_task.py": "# Copyright (c) Meta Platforms, Inc. and affiliates. All Rights Reserved.\n#\n# This source code is licensed under the MIT license found in the\n# LICENSE file in the root directory of this source tree.\n\nimport json\nimport os\nimport tempfile\nimport unittest\n\nimport torch\n\nfrom tests.utils import train_language_model\nfrom cpu_tests.test_utils import write_dummy_jsonl_data_dir, write_dummy_bpe\n\ntry:\n    import tokenizers  # noqa\n\n    has_hf_tokenizers = True\nexcept ImportError:\n    has_hf_tokenizers = False\n\n\nclass TestReproducibility(unittest.TestCase):\n    @unittest.skipIf(not has_hf_tokenizers, \"skip test if tokenizers is missing\")\n    def _test_reproducibility(\n        self,\n        name,\n        extra_flags=None,\n        delta=0.0001,\n        resume_checkpoint=\"checkpoint1.pt\",\n        max_epoch=3,\n    ):\n        def get_last_log_stats_containing_string(log_records, search_string):\n            for log_record in logs.records[::-1]:\n                if isinstance(log_record.msg, str) and search_string in log_record.msg:\n                    return json.loads(log_record.msg)\n\n        if extra_flags is None:\n            extra_flags = []\n\n        with tempfile.TemporaryDirectory(name) as data_dir:\n            write_dummy_jsonl_data_dir(data_dir)\n            vocab, merges = write_dummy_bpe(data_dir)\n\n            # train epochs 1 and 2 together\n            with self.assertLogs() as logs:\n                train_language_model(\n                    data_dir=data_dir,\n                    arch=\"transformer_lm_gpt2_tiny\",\n                    extra_flags=[\n                        \"--vocab-filename\",\n                        vocab,\n                        \"--merges-filename\",\n                        merges,\n                        \"--dropout\",\n                        \"0.0\",\n                        \"--log-format\",\n                        \"json\",\n                        \"--log-interval\",\n                        \"1\",\n                        \"--max-epoch\",\n                        str(max_epoch),\n                        \"--batch-size\",\n                        \"2\",\n                    ]\n                    + extra_flags,\n                    task=\"streaming_language_modeling\",\n                    max_tokens=None,\n                )\n            train_log = get_last_log_stats_containing_string(logs.records, \"train_loss\")\n            valid_log = get_last_log_stats_containing_string(logs.records, \"valid_loss\")\n\n            # train epoch 2, resuming from previous checkpoint 1\n            os.rename(\n                os.path.join(data_dir, resume_checkpoint),\n                os.path.join(data_dir, \"checkpoint_last.pt\"),\n            )\n            with self.assertLogs() as logs:\n                train_language_model(\n                    data_dir=data_dir,\n                    arch=\"transformer_lm_gpt2_tiny\",\n                    extra_flags=[\n                        \"--vocab-filename\",\n                        vocab,\n                        \"--merges-filename\",\n                        merges,\n                        \"--dropout\",\n                        \"0.0\",\n                        \"--log-format\",\n                        \"json\",\n                        \"--log-interval\",\n                        \"1\",\n                        \"--max-epoch\",\n                        str(max_epoch),\n                        \"--batch-size\",\n                        \"2\",\n                    ]\n                    + extra_flags,\n                    task=\"streaming_language_modeling\",\n                    max_tokens=None,\n                )\n            train_res_log = get_last_log_stats_containing_string(\n                logs.records, \"train_loss\"\n            )\n            valid_res_log = get_last_log_stats_containing_string(\n                logs.records, \"valid_loss\"\n            )\n\n            for k in [\"train_loss\", \"train_ppl\", \"train_num_updates\", \"train_gnorm\"]:\n                self.assertAlmostEqual(\n                    float(train_log[k]), float(train_res_log[k]), delta=delta\n                )\n            for k in [\n                \"valid_loss\",\n                \"valid_ppl\",\n                \"valid_num_updates\",\n                \"valid_best_loss\",\n            ]:\n                self.assertAlmostEqual(\n                    float(valid_log[k]), float(valid_res_log[k]), delta=delta\n                )\n\n    def test_reproducibility(self):\n        self._test_reproducibility(\"test_reproducibility\")\n\n    @unittest.skipIf(not torch.cuda.is_available(), \"test requires a GPU\")\n    def test_reproducibility_fp16(self):\n        self._test_reproducibility(\n            \"test_reproducibility_fp16\",\n            [\n                \"--fp16\",\n                \"--fp16-init-scale\",\n                \"4096\",\n            ],\n            delta=0.011,\n        )\n\n    @unittest.skipIf(not torch.cuda.is_available(), \"test requires a GPU\")\n    def test_reproducibility_memory_efficient_fp16(self):\n        self._test_reproducibility(\n            \"test_reproducibility_memory_efficient_fp16\",\n            [\n                \"--memory-efficient-fp16\",\n                \"--fp16-init-scale\",\n                \"4096\",\n            ],\n        )\n\n    def test_mid_epoch_reproducibility(self):\n        self._test_reproducibility(\n            \"test_mid_epoch_reproducibility\",\n            [\"--save-interval-updates\", \"3\"],\n            resume_checkpoint=\"checkpoint_1_3.pt\",\n            max_epoch=1,\n        )\n\n\nif __name__ == \"__main__\":\n    unittest.main()\n",
        "tests/utils.py": "# Copyright (c) Meta Platforms, Inc. and affiliates. All Rights Reserved.\n#\n# This source code is licensed under the MIT license found in the\n# LICENSE file in the root directory of this source tree.\n\nimport argparse\nimport os\nimport random\nfrom typing import Optional\n\nimport numpy as np\nimport torch\n\nimport metaseq.distributed.utils as distributed_utils\nfrom metaseq import options\nfrom metaseq.data import Dictionary, data_utils\nfrom metaseq.dataclass.utils import convert_namespace_to_omegaconf\nfrom metaseq.models import (\n    BaseModel,\n    BaseDecoder,\n)\nfrom metaseq.tasks import LegacyTask\nfrom metaseq.cli import validate, train\n\n\ndef collate(\n    samples,\n    pad_idx,\n    eos_idx,\n    left_pad_source=True,\n    left_pad_target=False,\n    input_feeding=True,\n    pad_to_length=None,\n    pad_to_multiple=1,\n):\n    if len(samples) == 0:\n        return {}\n\n    def merge(key, left_pad, move_eos_to_beginning=False, pad_to_length=None):\n        return data_utils.collate_tokens(\n            [s[key] for s in samples],\n            pad_idx,\n            eos_idx,\n            left_pad,\n            move_eos_to_beginning,\n            pad_to_length=pad_to_length,\n            pad_to_multiple=pad_to_multiple,\n        )\n\n    def check_alignment(alignment, src_len, tgt_len):\n        if alignment is None or len(alignment) == 0:\n            return False\n        if (\n            alignment[:, 0].max().item() >= src_len - 1\n            or alignment[:, 1].max().item() >= tgt_len - 1\n        ):\n            return False\n        return True\n\n    def compute_alignment_weights(alignments):\n        \"\"\"\n        Given a tensor of shape [:, 2] containing the source-target indices\n        corresponding to the alignments, a weight vector containing the\n        inverse frequency of each target index is computed.\n        For e.g. if alignments = [[5, 7], [2, 3], [1, 3], [4, 2]], then\n        a tensor containing [1., 0.5, 0.5, 1] should be returned (since target\n        index 3 is repeated twice)\n        \"\"\"\n        align_tgt = alignments[:, 1]\n        _, align_tgt_i, align_tgt_c = torch.unique(\n            align_tgt, return_inverse=True, return_counts=True\n        )\n        align_weights = align_tgt_c[align_tgt_i[np.arange(len(align_tgt))]]\n        return 1.0 / align_weights.float()\n\n    id = torch.LongTensor([s[\"id\"] for s in samples])\n    src_tokens = merge(\n        \"source\",\n        left_pad=left_pad_source,\n        pad_to_length=pad_to_length[\"source\"] if pad_to_length is not None else None,\n    )\n    # sort by descending source length\n    src_lengths = torch.LongTensor(\n        [s[\"source\"].ne(pad_idx).long().sum() for s in samples]\n    )\n    src_lengths, sort_order = src_lengths.sort(descending=True)\n    id = id.index_select(0, sort_order)\n    src_tokens = src_tokens.index_select(0, sort_order)\n\n    prev_output_tokens = None\n    target = None\n    if samples[0].get(\"target\", None) is not None:\n        target = merge(\n            \"target\",\n            left_pad=left_pad_target,\n            pad_to_length=pad_to_length[\"target\"]\n            if pad_to_length is not None\n            else None,\n        )\n        target = target.index_select(0, sort_order)\n        tgt_lengths = torch.LongTensor(\n            [s[\"target\"].ne(pad_idx).long().sum() for s in samples]\n        ).index_select(0, sort_order)\n        ntokens = tgt_lengths.sum().item()\n\n        if samples[0].get(\"prev_output_tokens\", None) is not None:\n            prev_output_tokens = merge(\n                \"prev_output_tokens\",\n                left_pad=left_pad_target,\n                pad_to_length=pad_to_length[\"target\"]\n                if pad_to_length is not None\n                else None,\n            )\n        elif input_feeding:\n            # we create a shifted version of targets for feeding the\n            # previous output token(s) into the next decoder step\n            prev_output_tokens = merge(\n                \"target\",\n                left_pad=left_pad_target,\n                move_eos_to_beginning=True,\n                pad_to_length=pad_to_length[\"target\"]\n                if pad_to_length is not None\n                else None,\n            )\n    else:\n        ntokens = src_lengths.sum().item()\n\n    batch = {\n        \"id\": id,\n        \"nsentences\": len(samples),\n        \"ntokens\": ntokens,\n        \"net_input\": {\n            \"src_tokens\": src_tokens,\n            \"src_lengths\": src_lengths,\n        },\n        \"target\": target,\n    }\n    if prev_output_tokens is not None:\n        batch[\"net_input\"][\"prev_output_tokens\"] = prev_output_tokens.index_select(\n            0, sort_order\n        )\n\n    if samples[0].get(\"alignment\", None) is not None:\n        bsz, tgt_sz = batch[\"target\"].shape\n        src_sz = batch[\"net_input\"][\"src_tokens\"].shape[1]\n\n        offsets = torch.zeros((len(sort_order), 2), dtype=torch.long)\n        offsets[:, 1] += torch.arange(len(sort_order), dtype=torch.long) * tgt_sz\n        if left_pad_source:\n            offsets[:, 0] += src_sz - src_lengths\n        if left_pad_target:\n            offsets[:, 1] += tgt_sz - tgt_lengths\n\n        alignments = [\n            alignment + offset\n            for align_idx, offset, src_len, tgt_len in zip(\n                sort_order, offsets, src_lengths, tgt_lengths\n            )\n            for alignment in [samples[align_idx][\"alignment\"].view(-1, 2)]\n            if check_alignment(alignment, src_len, tgt_len)\n        ]\n\n        if len(alignments) > 0:\n            alignments = torch.cat(alignments, dim=0)\n            align_weights = compute_alignment_weights(alignments)\n\n            batch[\"alignments\"] = alignments\n            batch[\"align_weights\"] = align_weights\n\n    if samples[0].get(\"constraints\", None) is not None:\n        # Collate the packed constraints across the samples, padding to\n        # the length of the longest sample.\n        lens = [sample.get(\"constraints\").size(0) for sample in samples]\n        constraints = torch.zeros((len(samples), max(lens))).long()\n        for i, sample in enumerate(samples):\n            constraints[i, 0 : lens[i]] = samples[i].get(\"constraints\")\n        batch[\"constraints\"] = constraints.index_select(0, sort_order)\n\n    return batch\n\n\ndef dummy_dictionary(vocab_size, prefix=\"token_\"):\n    d = Dictionary()\n    for i in range(vocab_size):\n        token = prefix + str(i)\n        d.add_symbol(token)\n    d.finalize(padding_factor=1)  # don't add extra padding symbols\n    return d\n\n\ndef dummy_dataloader(\n    samples,\n    padding_idx=1,\n    eos_idx=2,\n    batch_size=None,\n):\n    if batch_size is None:\n        batch_size = len(samples)\n\n    # add any missing data to samples\n    for i, sample in enumerate(samples):\n        if \"id\" not in sample:\n            sample[\"id\"] = i\n\n    # create dataloader\n    dataset = TestDataset(samples)\n    dataloader = torch.utils.data.DataLoader(\n        dataset,\n        batch_size=batch_size,\n        collate_fn=(lambda samples: collate(samples, padding_idx, eos_idx)),\n    )\n    return iter(dataloader)\n\n\ndef sequence_generator_setup():\n    # construct dummy dictionary\n    d = dummy_dictionary(vocab_size=2)\n\n    eos = d.eos()\n    w1 = 4\n    w2 = 5\n\n    # construct source data\n    src_tokens = torch.LongTensor([[w1, w2, eos], [w1, w2, eos]])\n    src_lengths = torch.LongTensor([2, 2])\n\n    args = argparse.Namespace()\n    unk = 0.0\n    args.beam_probs = [\n        # step 0:\n        torch.FloatTensor(\n            [\n                # eos      w1   w2\n                # sentence 1:\n                [0.0, unk, 0.9, 0.1],  # beam 1\n                [0.0, unk, 0.9, 0.1],  # beam 2\n                # sentence 2:\n                [0.0, unk, 0.7, 0.3],\n                [0.0, unk, 0.7, 0.3],\n            ]\n        ),\n        # step 1:\n        torch.FloatTensor(\n            [\n                # eos      w1   w2       prefix\n                # sentence 1:\n                [1.0, unk, 0.0, 0.0],  # w1: 0.9  (emit: w1 <eos>: 0.9*1.0)\n                [0.0, unk, 0.9, 0.1],  # w2: 0.1\n                # sentence 2:\n                [0.25, unk, 0.35, 0.4],  # w1: 0.7  (don't emit: w1 <eos>: 0.7*0.25)\n                [0.00, unk, 0.10, 0.9],  # w2: 0.3\n            ]\n        ),\n        # step 2:\n        torch.FloatTensor(\n            [\n                # eos      w1   w2       prefix\n                # sentence 1:\n                [0.0, unk, 0.1, 0.9],  # w2 w1: 0.1*0.9\n                [\n                    0.6,\n                    unk,\n                    0.2,\n                    0.2,\n                ],  # w2 w2: 0.1*0.1  (emit: w2 w2 <eos>: 0.1*0.1*0.6)\n                # sentence 2:\n                [\n                    0.60,\n                    unk,\n                    0.4,\n                    0.00,\n                ],  # w1 w2: 0.7*0.4  (emit: w1 w2 <eos>: 0.7*0.4*0.6)\n                [0.01, unk, 0.0, 0.99],  # w2 w2: 0.3*0.9\n            ]\n        ),\n        # step 3:\n        torch.FloatTensor(\n            [\n                # eos      w1   w2       prefix\n                # sentence 1:\n                [\n                    1.0,\n                    unk,\n                    0.0,\n                    0.0,\n                ],  # w2 w1 w2: 0.1*0.9*0.9  (emit: w2 w1 w2 <eos>: 0.1*0.9*0.9*1.0)\n                [\n                    1.0,\n                    unk,\n                    0.0,\n                    0.0,\n                ],  # w2 w1 w1: 0.1*0.9*0.1  (emit: w2 w1 w1 <eos>: 0.1*0.9*0.1*1.0)\n                # sentence 2:\n                [\n                    0.1,\n                    unk,\n                    0.5,\n                    0.4,\n                ],  # w2 w2 w2: 0.3*0.9*0.99  (emit: w2 w2 w2 <eos>: 0.3*0.9*0.99*0.1)\n                [\n                    1.0,\n                    unk,\n                    0.0,\n                    0.0,\n                ],  # w1 w2 w1: 0.7*0.4*0.4  (emit: w1 w2 w1 <eos>: 0.7*0.4*0.4*1.0)\n            ]\n        ),\n    ]\n\n    task = TestTranslationTask.setup_task(args, d, d)\n    model = task.build_model(args)\n    tgt_dict = task.target_dictionary\n\n    return tgt_dict, w1, w2, src_tokens, src_lengths, model\n\n\ndef create_dummy_data(\n    data_dir, num_examples=100, maxlen=20, alignment=False, languages=None\n):\n    def _create_dummy_data(dir, filename):\n        data = torch.rand(num_examples * maxlen)\n        data = 97 + torch.floor(26 * data).int()\n        with open(os.path.join(dir, filename), \"w\") as h:\n            offset = 0\n            for _ in range(num_examples):\n                ex_len = random.randint(1, maxlen)\n                ex_str = \" \".join(map(chr, data[offset : offset + ex_len]))\n                print(ex_str, file=h)\n                offset += ex_len\n\n    def _create_dummy_alignment_data(filename_src, filename_tgt, filename):\n        with open(os.path.join(data_dir, filename_src), \"r\") as src_f, open(\n            os.path.join(data_dir, filename_tgt), \"r\"\n        ) as tgt_f, open(os.path.join(data_dir, filename), \"w\") as h:\n            for src, tgt in zip(src_f, tgt_f):\n                src_len = len(src.split())\n                tgt_len = len(tgt.split())\n                avg_len = (src_len + tgt_len) // 2\n                num_alignments = random.randint(avg_len // 2, 2 * avg_len)\n                src_indices = torch.floor(torch.rand(num_alignments) * src_len).int()\n                tgt_indices = torch.floor(torch.rand(num_alignments) * tgt_len).int()\n                ex_str = \" \".join(\n                    [\n                        \"{}-{}\".format(src, tgt)\n                        for src, tgt in zip(src_indices, tgt_indices)\n                    ]\n                )\n                print(ex_str, file=h)\n\n    files_to_write = [\n        \"train.in\",\n        \"train.out\",\n        \"valid.in\",\n        \"valid.out\",\n        \"test.in\",\n        \"test.out\",\n    ]\n    if languages is None:  # En only dummy dataset\n        for f in files_to_write:\n            _create_dummy_data(data_dir, f)\n    else:\n        for lang in languages:\n            lang_dir = os.path.join(data_dir, lang)\n            os.makedirs(lang_dir, exist_ok=True)\n            for f in files_to_write:\n                _create_dummy_data(lang_dir, f)\n\n    if alignment:\n        _create_dummy_alignment_data(\"train.in\", \"train.out\", \"train.align\")\n        _create_dummy_alignment_data(\"valid.in\", \"valid.out\", \"valid.align\")\n        _create_dummy_alignment_data(\"test.in\", \"test.out\", \"test.align\")\n\n\nclass TestDataset(torch.utils.data.Dataset):\n    def __init__(self, data):\n        super().__init__()\n        self.data = data\n        self.sizes = None\n\n    def __getitem__(self, index):\n        return self.data[index]\n\n    def __len__(self):\n        return len(self.data)\n\n\nclass TestTranslationTask(LegacyTask):\n    def __init__(self, args, src_dict, tgt_dict, model):\n        super().__init__(args)\n        self.src_dict = src_dict\n        self.tgt_dict = tgt_dict\n        self.model = model\n\n    @classmethod\n    def setup_task(cls, args, src_dict=None, tgt_dict=None, model=None):\n        return cls(args, src_dict, tgt_dict, model)\n\n    def build_model(self, args):\n        return TestModel.build_model(args, self)\n\n    @property\n    def source_dictionary(self):\n        return self.src_dict\n\n    @property\n    def target_dictionary(self):\n        return self.tgt_dict\n\n\nclass TestModel(BaseModel):\n    def __init__(self, decoder):\n        super().__init__(decoder)\n\n    @classmethod\n    def build_model(cls, args, task):\n        decoder = TestBaseDecoder(args, task.target_dictionary)\n        return cls(decoder)\n\n\nclass TestBaseDecoder(BaseDecoder):\n    def __init__(self, args, dictionary):\n        super().__init__(dictionary)\n        assert hasattr(args, \"beam_probs\") or hasattr(args, \"probs\")\n        args.max_decoder_positions = getattr(args, \"max_decoder_positions\", 100)\n        self.args = args\n\n    def forward(self, src_tokens, src_lengths, prev_output_tokens):\n        bbsz = prev_output_tokens.size(0)\n        vocab = len(self.dictionary)\n        src_len = src_tokens.size(1)\n        tgt_len = prev_output_tokens.size(1)\n\n        steps = list(range(tgt_len))\n\n        # define output in terms of raw probs\n        if hasattr(self.args, \"probs\"):\n            assert (\n                self.args.probs.dim() == 3\n            ), \"expected probs to have size bsz*steps*vocab\"\n            probs = self.args.probs.index_select(1, torch.LongTensor(steps))\n        else:\n            probs = torch.FloatTensor(bbsz, len(steps), vocab).zero_()\n            for i, step in enumerate(steps):\n                # args.beam_probs gives the probability for every vocab element,\n                # starting with eos, then unknown, and then the rest of the vocab\n                if step < len(self.args.beam_probs):\n                    probs[:, i, self.dictionary.eos() :] = self.args.beam_probs[step]\n                else:\n                    probs[:, i, self.dictionary.eos()] = 1.0\n\n        # random attention\n        attn = torch.rand(bbsz, tgt_len, src_len)\n\n        dev = prev_output_tokens.device\n        return probs.to(dev), {\"attn\": [attn.to(dev)]}\n\n    def get_normalized_probs(self, logits, log_probs):\n        # the decoder returns probabilities directly\n        if log_probs:\n            return logits.log()\n        else:\n            return logits\n\n    def max_positions(self):\n        return self.args.max_decoder_positions\n\n\ndef train_language_model(\n    data_dir,\n    arch,\n    extra_flags=None,\n    run_validation=False,\n    extra_valid_flags=None,\n    task=\"language_modeling\",\n    world_size=1,\n    max_tokens: Optional[int] = 500,\n):\n    train_parser = options.get_training_parser()\n    train_args = options.parse_args_and_arch(\n        train_parser,\n        [\n            \"--task\",\n            task,\n            data_dir,\n            \"--arch\",\n            arch,\n            \"--optimizer\",\n            \"adam\",\n            \"--lr\",\n            \"0.0001\",\n            \"--tokens-per-sample\",\n            \"500\",\n            \"--save-dir\",\n            data_dir,\n            \"--max-epoch\",\n            \"1\",\n            \"--distributed-world-size\",\n            str(world_size),\n            \"--ddp-backend\",\n            \"no_c10d\",\n            \"--num-workers\",\n            \"0\",\n        ]\n        + ([\"--max-tokens\", str(max_tokens)] if max_tokens is not None else [])\n        + (extra_flags or []),\n    )\n    cfg = convert_namespace_to_omegaconf(train_args)\n    distributed_utils.call_main(cfg, train.main)\n\n    if run_validation:\n        # test validation\n        validate_parser = options.get_validation_parser()\n        validate_args = options.parse_args_and_arch(\n            validate_parser,\n            [\n                \"--task\",\n                task,\n                data_dir,\n                \"--path\",\n                os.path.join(data_dir, \"checkpoint_last.pt\"),\n                \"--valid-subset\",\n                \"valid\",\n                \"--max-tokens\",\n                \"500\",\n                \"--num-workers\",\n                \"0\",\n            ]\n            + (extra_valid_flags or []),\n        )\n        validate.main(validate_args)\n"
    }
}