{
  "model": "facebook/opt-125m",
  "scores": {
    "1-2 Code": {
      "score": 0.0,
      "reason": "No training pipeline files; README mentions are ignored."
    },
    "1-3 License": {
      "score": 0.5,
      "reason": "The OPT licence allows use, modification and redistribution only for NON-COMMERCIAL research.  Commercial/production use is explicitly prohibited, so one of the four basic rights is restricted ⇒ Semi-Open."
    },
    "1-4 Paper": {
      "score": 1.0,
      "reason": "A dedicated technical report, “OPT: Open Pre-trained Transformer Language Models” (arXiv:2205.01068), documents the exact 125 M variant."
    },
    "1-5 Architecture": {
      "score": 1,
      "reason": "Architecture evidence present in extracted quotes."
    },
    "1-6 Tokenizer": {
      "score": 1.0,
      "reason": "Tokenizer files downloadable in repository."
    },
    "2-1 Hardware": {
      "score": 1.0,
      "reason": "Quote: “We trained OPT-175B on 992 80 GB A100 GPUs …” discloses both type (A100-80 GB) and exact quantity (992 GPUs)."
    },
    "2-2 Software": {
      "score": 0.5,
      "reason": "Quotes list major training-stack components beyond the base framework: Fully-Sharded Data Parallel, Megatron-LM tensor parallelism, AdamW, and the public metaseq code.  However, full versioned stack and all configs are not enumerated ⇒ Semi-Open."
    },
    "2-3 API": {
      "score": 0.0,
      "reason": "Only generic or non-official mentions; no qualifying official API docs.  No qualifying official API docs via web search."
    },
    "3-1 Pre-training": {
      "score": 0.5,
      "reason": "Method details provided (data sources, CLM objective, LR schedule, AdamW hyper-params) but not the full set of hyper-parameters or complete pipeline ⇒ Partial disclosure."
    },
    "3-2 Fine-tuning": {
      "score": 0.0,
      "reason": "Only a generic remark that the model *can* be fine-tuned with the HF CLM example; no author-disclosed fine-tuning procedure."
    },
    "3-3 Reinforcement Learning": {
      "score": 0.0,
      "reason": "No RL training is described in any provided quote. No direct quote evidence; defaulting to Closed by strict rule."
    },
    "4-1 Pre-training Data": {
      "score": 0.5,
      "reason": "Sources are enumerated (RoBERTa’s three corpora, subset of The Pile, Pushshift Reddit); language mix (mostly English) is noted, but exact sizes/proportions and licences for each subset are not given ⇒ Partial disclosure."
    },
    "4-2 Fine-tuning Data": {
      "score": 0.0,
      "reason": "No quotes describing any fine-tuning datasets. No direct quote evidence; defaulting to Closed by strict rule."
    },
    "4-3 Reinforcement Learning Data": {
      "score": 0.0,
      "reason": "No RL data mentioned. No direct quote evidence; defaulting to Closed by strict rule."
    },
    "4-4 Data Filtering": {
      "score": 0.0,
      "reason": "No concrete filtering / cleaning pipeline or thresholds are described in the supplied quotes. No direct quote evidence; defaulting to Closed by strict rule."
    },
    "1-1 Weights": {
      "score": 1,
      "reason": "Weights files present in the repository (e.g., *.safetensors/bin/pt/ckpt)."
    }
  },
  "included_scores": {
    "1-2 Code": {
      "score": 0.0,
      "reason": "No training pipeline files; README mentions are ignored."
    },
    "1-3 License": {
      "score": 0.5,
      "reason": "The OPT licence allows use, modification and redistribution only for NON-COMMERCIAL research.  Commercial/production use is explicitly prohibited, so one of the four basic rights is restricted ⇒ Semi-Open."
    },
    "1-4 Paper": {
      "score": 1.0,
      "reason": "A dedicated technical report, “OPT: Open Pre-trained Transformer Language Models” (arXiv:2205.01068), documents the exact 125 M variant."
    },
    "1-5 Architecture": {
      "score": 1,
      "reason": "Architecture evidence present in extracted quotes."
    },
    "1-6 Tokenizer": {
      "score": 1.0,
      "reason": "Tokenizer files downloadable in repository."
    },
    "2-1 Hardware": {
      "score": 1.0,
      "reason": "Quote: “We trained OPT-175B on 992 80 GB A100 GPUs …” discloses both type (A100-80 GB) and exact quantity (992 GPUs)."
    },
    "2-2 Software": {
      "score": 0.5,
      "reason": "Quotes list major training-stack components beyond the base framework: Fully-Sharded Data Parallel, Megatron-LM tensor parallelism, AdamW, and the public metaseq code.  However, full versioned stack and all configs are not enumerated ⇒ Semi-Open."
    },
    "2-3 API": {
      "score": 0.0,
      "reason": "Only generic or non-official mentions; no qualifying official API docs.  No qualifying official API docs via web search."
    },
    "3-1 Pre-training": {
      "score": 0.5,
      "reason": "Method details provided (data sources, CLM objective, LR schedule, AdamW hyper-params) but not the full set of hyper-parameters or complete pipeline ⇒ Partial disclosure."
    },
    "4-1 Pre-training Data": {
      "score": 0.5,
      "reason": "Sources are enumerated (RoBERTa’s three corpora, subset of The Pile, Pushshift Reddit); language mix (mostly English) is noted, but exact sizes/proportions and licences for each subset are not given ⇒ Partial disclosure."
    },
    "4-4 Data Filtering": {
      "score": 0.0,
      "reason": "No concrete filtering / cleaning pipeline or thresholds are described in the supplied quotes. No direct quote evidence; defaulting to Closed by strict rule."
    },
    "1-1 Weights": {
      "score": 1,
      "reason": "Weights files present in the repository (e.g., *.safetensors/bin/pt/ckpt)."
    }
  },
  "final_score_10pt": 5.833,
  "meta": {
    "usage_from_dispatch": {
      "fine_tuning": "not_used",
      "rl": "not_used"
    },
    "excluded": [
      "3-2 Fine-tuning",
      "3-3 Reinforcement Learning",
      "4-2 Fine-tuning Data",
      "4-3 Reinforcement Learning Data"
    ],
    "denominator": 12,
    "raw_sum": 7.0,
    "scale": "10/12",
    "code_detection_reason": "No training pipeline files; README mentions are ignored.",
    "pretrain_sources_used": false
  }
}