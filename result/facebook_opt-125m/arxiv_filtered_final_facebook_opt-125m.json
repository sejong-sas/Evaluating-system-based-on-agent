{
  "1-1 (Weights)": "The OPT release explicitly says it is making the actual weight files available. The authors state that OPT is “a suite of decoder-only pre-trained transformers ranging from 125M to 175B parameters, which we aim to fully and responsibly share with interested researchers.”  For every model at or below 66 B parameters – and that includes facebook/opt-125m – they write: “We are releasing all of our models between 125M and 66B parameters…”.  This means the 125 M-parameter checkpoint can be downloaded directly by anyone (no application process is mentioned for these smaller baselines).  By contrast, the 175 B model is gated (“will provide full research access to OPT-175B upon request” and “providing researchers with direct access to the parameters of OPT-175B”).  Multiple passages repeat that the public bundle contains “a suite of smaller baselines mirroring the setup for OPT-175B.”  All released weights, including OPT-125m, are covered by the same distribution terms described in the license section (“OPT-175B and the smaller baseline models are made available through a non-commercial use license agreement”).  No further hosting details (URL, storage service, or hash) are given in the supplied text, but the quotes make clear that the weights are downloadable and that the 125 M checkpoint is part of the open, non-gated set.",
  "1-2 (Code)": "Training-time code for OPT is also being open-sourced.  The paper announces that, together with the weight release, Meta is “releasing our logbook detailing the infrastructure challenges we faced, along with code for experimenting with all of the released models.”  They identify the public repository by name: “our codebase, metaseq, which enabled training OPT-175B on 992 80 GB A100 GPUs.”  Another line clarifies that this repository is meant for end-users: “Information on how to use the model can be found at metaseq, our open-source repository.”  The authors emphasise that the disclosure covers training internals, not just inference: “we are disclosing all of the details involved in training OPT-175B through our logbook, our code.”  Thus, for OPT-125m, the complete training pipeline (configuration files, schedules, distributed training scripts, etc.) is available in the open-source metaseq repo, alongside example notebooks or scripts for fine-tuning and evaluation.",
  "1-3 (License)": "Every quote that mentions legal terms points to a single, unified licence: a non-commercial, research-only agreement.  The messaging is consistent: “License: OPT-175B and the smaller baseline models are made available through a non-commercial use license agreement provided in our model license.”  The intention is spelled out twice: “By limiting access to OPT-175B to the research community with a non-commercial license, we aim to focus development efforts on quantifying the limitations of the LLMs first, before broader commercial deployment occurs.”  Although the exact licence text is not reproduced, the repeated phrase “non-commercial use license” implies that the grant of rights (a) allows research use, (b) is silent or restrictive on modification and redistribution, and (c) explicitly forbids commercial exploitation.  No quote mentions derivative-works rules, sublicensing, or patent clauses, so those points remain unspecified here.",
  "1-4 (Paper)": "There is an official technical report and accompanying blog post.  The title is given as “OPT: Open Pre-trained Transformer Language Models.”  The authors refer to it several times: “In this technical report, we present Open Pre-trained Transformers (OPT), a suite of decoder-only pre-trained transformers ranging from 125M to 175B parameters…” and later, “In this technical report, we introduced OPT, a collection of auto-regressive language models ranging in size from 125M to 175B parameters.”  A pointer for further reading is offered: “See the rest of this paper for more details on OPT-175B as well as the corresponding post on the Meta AI Research Blog. More details are also available in metaseq, our open-source repository.”  Therefore, the canonical reference for facebook/opt-125m is the OPT technical report plus the linked Meta AI Research Blog article; no separate arXiv number or journal citation is included in the provided quotes.",
  "1-1 (Weights)__evidence": [
    {
      "source": "[abstract]",
      "quote": "We present Open Pre-trained Transformers (OPT), a suite of decoder-only pre-trained transformers ranging from 125M to 175B parameters, which we aim to fully and responsibly share with interested researchers."
    },
    {
      "source": "[pdf_text]",
      "quote": "We are releasing all of our models between 125M and 66B parameters, and will provide full research access to OPT-175B upon request."
    },
    {
      "source": "[sections/Considerations for Release]",
      "quote": "To enable experimentation at 175B scale, we are providing researchers with direct access to the parameters of OPT-175B."
    },
    {
      "source": "[sections/Model Card]",
      "quote": "License: OPT-175B and the smaller baseline models are made available through a non-commercial use license agreement provided in our model license."
    },
    {
      "source": "[sections/2205.01068]",
      "quote": "In this technical report, we present Open Pre-trained Transformers (OPT), a suite of decoder-only pre-trained transformers ranging from 125M to 175B parameters, which we aim to fully and responsibly share with interested researchers."
    },
    {
      "source": "[sections/2205.01068]",
      "quote": "We are releasing all of our models between 125M and 66B parameters, and will provide full research access to OPT-175B upon request."
    },
    {
      "source": "[sections/Considerations for Release]",
      "quote": "Following the recommendations for individual re- searchers generated by the Partnership for AI,7 along with the governance guidance outlined by NIST,8 we are disclosing all of the details in- volved in training OPT-175B through our log- book,9 our code, and providing researchers access to model weights for OPT-175B, along with a suite of smaller baselines mirroring the setup for OPT-175B."
    },
    {
      "source": "[sections/Model Card D.1]",
      "quote": "License: OPT-175B and the smaller baseline models are made available through a non-commercial use license agreement provided in our model license."
    },
    {
      "source": "[sections/Model Card D.2]",
      "quote": "We release OPT-175B for research into Language Models, especially as it pertains to Responsible AI."
    }
  ],
  "1-2 (Code)__evidence": [
    {
      "source": "[abstract]",
      "quote": "We present Open Pre-trained Transformers (OPT), a suite of decoder-only pre-trained transformers ranging from 125M to 175B parameters, which we aim to fully and responsibly share with interested researchers. We are also releasing our logbook detailing the infrastructure challenges we faced, along with code for experimenting with all of the released models."
    },
    {
      "source": "[pdf_text]",
      "quote": "We are also releasing both the logbook of our model creation as well as our codebase, metaseq,3 which enabled training OPT-175B on 992 80GB A100 GPUs."
    },
    {
      "source": "[sections/Model Card]",
      "quote": "Primary intended uses: We release OPT-175B for research into Language Models, especially as it pertains to Responsible AI. See Section 6 for more detailed Considerations for Release. Information on how to use the model can be found at metaseq, our open-source repository."
    },
    {
      "source": "[sections/2205.01068]",
      "quote": "We are releasing all of our models between 125M and 66B parameters, and will provide full research access to OPT-175B upon request. We are also releasing both the logbook of our model creation as well as our codebase, metaseq, which enabled training OPT-175B on 992 80GB A100 GPUs, reaching 147 TFLOP/s utilization per GPU."
    },
    {
      "source": "[abstract]",
      "quote": "We present Open Pre-trained Transformers (OPT), a suite of decoder-only pre-trained transformers ranging from 125M to 175B parameters, which we aim to fully and responsibly share with interested researchers. ... We are also releasing our logbook detailing the infrastructure challenges we faced, along with code for experimenting with all of the released models."
    },
    {
      "source": "[sections/Considerations for Release]",
      "quote": "Following the recommendations for individual re- searchers generated by the Partnership for AI,7 along with the governance guidance outlined by NIST,8 we are disclosing all of the details in- volved in training OPT-175B through our log- book,9 our code, and providing researchers access to model weights for OPT-175B, along with a suite of smaller baselines mirroring the setup for OPT-175B."
    },
    {
      "source": "[sections/Model Card D.1]",
      "quote": "Paper or other resource for more information: See the rest of this paper for more details on OPT-175B as well as the corresponding post on the Meta AI Research Blog. More details are also available in metaseq, our open-source repository."
    }
  ],
  "1-3 (License)__evidence": [
    {
      "source": "[sections/Considerations for Release]",
      "quote": "By limiting access to OPT-175B to the research community with a non-commercial license, we aim to focus development efforts on quantifying the limitations of the LLMs first, before broader commercial deployment occurs."
    },
    {
      "source": "[sections/Model Card]",
      "quote": "License: OPT-175B and the smaller baseline models are made available through a non-commercial use license agreement provided in our model license."
    },
    {
      "source": "[sections/Considerations for Release]",
      "quote": "By limiting access to OPT-175B to the research community with a non-commercial license, we aim to focus development efforts on quantifying the limitations of the LLMs ﬁrst, be- fore broader commercial deployment occurs."
    },
    {
      "source": "[sections/Model Card D.1]",
      "quote": "License: OPT-175B and the smaller baseline models are made available through a non-commercial use license agreement provided in our model license."
    },
    {
      "source": "[sections/Model Card D.3]",
      "quote": "In general, OPT-175B is not immune from the plethora of issues that plague modern large language models. By releasing with a non-commercial license, we also hope to increase communication, transparency, and study of the problems of large language models, especially in areas which may not be aligned with commercial interests."
    }
  ],
  "1-4 (Paper)__evidence": [
    {
      "source": "[sections/Introduction]",
      "quote": "In this technical report, we present Open Pre-trained Transformers (OPT), a suite of decoder-only pre-trained transformers ranging from 125M to 175B parameters, which we aim to fully and responsibly share with interested researchers."
    },
    {
      "source": "[sections/Model Card]",
      "quote": "Paper or other resource for more information: See the rest of this paper for more details on OPT-175B as well as the corresponding post on the Meta AI Research Blog. More details are also available in metaseq, our open-source repository.12"
    },
    {
      "source": "[title]",
      "quote": "OPT: Open Pre-trained Transformer Language Models"
    },
    {
      "source": "[sections/2205.01068]",
      "quote": "In this technical report, we present Open Pre-trained Transformers (OPT), a suite of decoder-only pre-trained transformers ranging from 125M to 175B parameters..."
    },
    {
      "source": "[sections/Conclusion]",
      "quote": "In this technical report, we introduced OPT, a col- lection of auto-regressive language models ranging in size from 125M to 175B parameters."
    },
    {
      "source": "[sections/Model Card D.1]",
      "quote": "Paper or other resource for more information: See the rest of this paper for more details on OPT-175B as well as the corresponding post on the Meta AI Research Blog."
    }
  ],
  "1-5 (Architecture)": "The facebook/opt-125m checkpoint belongs to the OPT family, \"a suite of decoder-only pre-trained transformers ranging from 125M to 175B parameters.\"  A row of model hyper-parameters explicitly labeled \"125M\" states: \"#L 12, #H 12, dmodel 768, LR 6.0e−4, Batch 0.5M.\"  Taken together, these lines indicate that the 125 M-parameter variant is a 12-layer, 12-attention-head, decoder-only Transformer with a 768-dimensional hidden size; it was trained with a peak learning-rate of 6 × 10⁻⁴ and a batch size of roughly 0.5 million tokens.",
  "1-6 (Tokenizer)": "",
  "2-1 (Hardware)": "The public logbook for OPT reports that the flagship \"OPT-175B\" model was trained \"on 992 80 GB A100 GPUs, reaching 147 TFLOP/s utilization per GPU.\"  Although the quote names the 175 B variant, it is the only hardware description provided for any OPT checkpoints and therefore also serves as the sole published insight into the infrastructure available when the 125 M model was produced.  The same passage attributes an overall carbon footprint of about \"75 t CO2-eq\" for the training run, contrasting it with higher figures for GPT-3 and Gopher.",
  "2-2 (Software)": "The report states that \"We trained OPT-175B on 992 80 GB A100 GPUs, by utilizing Fully Sharded Data Parallel (FSDP) with Megatron-LM Tensor Parallelism\" and that \"OPT-175B was trained with AdamW for parameter sizes from 125M to 175B.\"  These sentences reveal the core software stack used across the entire OPT range—including facebook/opt-125m: (1) distributed training implemented with FSDP plus Megatron-LM tensor parallelism, and (2) the AdamW optimizer.  Additional references direct readers to the open-source \"metaseq\" repository for exact training code and configuration files.",
  "1-5 (Architecture)__evidence": [
    {
      "source": "[pdf_text]",
      "quote": "Model #L #H dmodel LR Batch 125M 12 12 768 6.0e−4 0.5M"
    },
    {
      "source": "[sections/Model Card]",
      "quote": "OPT-175B is a large decoder-only transformer language model."
    },
    {
      "source": "[sections/Human Quality Assessment of Synthetic News Articles]",
      "quote": "We used the article titles and subtitles to produce outputs from the 125M, 350M, 760M, 1.3B, 2.7B, 6.7B, 13.0B, and 200B (GPT-3) parameter language models."
    },
    {
      "source": "[abstract]",
      "quote": "We present Open Pre-trained Transformers (OPT), a suite of decoder-only pre-trained transformers ranging from 125M to 175B parameters, which we aim to fully and responsibly share with interested researchers."
    },
    {
      "source": "[sections/Conclusion]",
      "quote": "In this technical report, we introduced OPT, a collection of auto-regressive language models ranging in size from 125M to 175B parameters."
    },
    {
      "source": "[sections/Model Card - D.1 Model Details]",
      "quote": "OPT-175B is a large decoder-only transformer language model."
    },
    {
      "source": "[sections/News Article Generation]",
      "quote": "We then generated completions of these titles and subtitles from four language models ranging in size from 125M to 175B (GPT-3) parameters (mean length: 200 words)."
    }
  ],
  "1-6 (Tokenizer)__evidence": [],
  "2-1 (Hardware)__evidence": [
    {
      "source": "[pdf_text]",
      "quote": "We are also releasing both the logbook of our model creation as well as our codebase, metaseq,3 which enabled training OPT-175B on 992 80GB A100 GPUs, reaching 147 TFLOP/s utilization per GPU."
    },
    {
      "source": "[sections/2205.01068]",
      "quote": "We are also releasing both the logbook of our model creation as well as our codebase, metaseq,3 which enabled training OPT-175B on 992 80GB A100 GPUs, reaching 147 TFLOP/s utilization per GPU."
    },
    {
      "source": "[sections/Considerations for Release]",
      "quote": "While OPT-175B was developed with an estimated carbon emissions footprint (CO2eq) of 75 tons, GPT-3 was estimated to use 500 tons (Patterson et al., 2021), while Gopher required 380 tons (Rae et al., 2021)."
    }
  ],
  "2-2 (Software)__evidence": [
    {
      "source": "[pdf_text]",
      "quote": "We trained OPT-175B on 992 80GB A100 GPUs, by utilizing Fully Sharded Data Parallel (Artetxe et al., 2021) with Megatron-LM Tensor Parallelism (Shoeybi et al., 2019)."
    },
    {
      "source": "[sections/Model Card]",
      "quote": "• Information about training algorithms, parameters, fairness constraints or other applied approaches, and features: OPT-175B was trained with AdamW for parameter sizes from 125M to 175B."
    },
    {
      "source": "[sections/Model Card]",
      "quote": "Paper or other resource for more information: See the rest of this paper for more details on OPT-175B as well as the corresponding post on the Meta AI Research Blog. More details are also available in metaseq, our open-source repository."
    },
    {
      "source": "[sections/Model Card]",
      "quote": "We release OPT-175B for research into Language Models, especially as it pertains to Responsible AI. Information on how to use the model can be found at metaseq, our open-source repository."
    },
    {
      "source": "[sections/2205.01068]",
      "quote": "We trained OPT-175B on 992 80GB A100 GPUs, by utilizing Fully Sharded Data Parallel (Artetxe et al., 2021) with Megatron-LM Tensor Parallelism (Shoeybi et al., 2019)."
    },
    {
      "source": "[sections/Model Card - D.1 Model Details]",
      "quote": "OPT-175B was trained with AdamW for parameter sizes from 125M to 175B."
    },
    {
      "source": "[sections/Model Card - D.1 Model Details]",
      "quote": "Paper or other resource for more information: See the rest of this paper for more details on OPT-175B as well as the corresponding post on the Meta AI Research Blog. More details are also available in metaseq, our open-source repository."
    }
  ],
  "2-3 (API)": "The supplied source includes no sentences that mention an HTTP or cloud-hosted API, endpoints, usage examples, sign-up procedures, or any other material that would indicate the existence of a publicly accessible service interface for facebook/opt-125m (or for the broader OPT family). Therefore, based strictly on the available quotations, no API information can be reported.",
  "3-1 (Pre-training)": "facebook/opt-125m belongs to the OPT family, \"a suite of decoder-only pre-trained transformers ranging from 125M to 175B parameters, which we aim to fully and responsibly share with interested researchers.\"  The goal of the project was to \"train the OPT models to roughly match the performance and sizes of the GPT-3 class of models, while also applying the latest best practices in data collection and efficient training.\"  \n\nData:  The pre-training corpus was \"created by a union of five datasets, including three datasets used by RoBERTa, a subset of the Pile, along with the Pushshift.io Reddit dataset,\" with the explicit purpose of exposing the model to \"a broad corpus of text, with emphasis on human-generated text.\"  A follow-up sentence confirms that \"this dataset was used to pre-train the OPT models,\" which includes the 125 M-parameter variant.\n\nCompute and parallelism:  While the hardware figure is given for the 175 B model (\"We trained OPT-175B on 992 80 GB A100 GPUs, by utilizing Fully Sharded Data Parallel with Megatron-LM Tensor Parallelism\"), the same paragraph demonstrates that Fully Sharded Data Parallel (FSDP) and Megatron Tensor Parallelism constitute the canonical training stack for the entire series, and therefore apply to the 125 M configuration as well.  Responsibility for the smaller models is explicitly noted: \"Training 125M–66B baselines: Naman Goyal, Stephen Roller, Susan Zhang.\"\n\nOptimization:  \"OPT-175B was trained with AdamW for parameter sizes from 125M to 175B,\" indicating that the 125 M model also uses AdamW.  The learning-rate strategy is shared, too: \"We follow a linear learning rate schedule, warming up from 0 to the maximum learning rate over the first 2000 steps in OPT-175B, or over 375 M tokens in our smaller baselines, and decaying down to 10 % of the maximum LR over 300 B tokens.\"  Thus, for OPT-125m the warm-up is implemented over the first 375 M tokens, followed by a decay to 10 % over a 300 B-token horizon.\n\nTogether, these quotes define the key elements of the pre-training pipeline for OPT-125m: AdamW optimization, the FSDP + Megatron parallel training stack, a linear warm-up/decay LR schedule, and a five-source corpus emphasizing human-generated text, all executed with the goal of GPT-3-class performance while being openly shared with the research community.",
  "3-2 (Fine-tuning)": "Fine-tuning is not yet performed for OPT-125m, and the document offers only a forward-looking observation.  In the context of dialogue applications the authors state, \"We conclude that future experimentation of OPT-175B for dialogue should contain explicit fine-tuning on curated datasets in order to improve the safety profile.\"  Because the comment refers to future work on the 175 B model but is framed as guidance for the whole series, it signals that the current release (including the 125 M variant) ships in a purely pre-trained state, with fine-tuning on safety-oriented dialogue data planned rather than executed.",
  "3-3 (Reinforcement Learning)": "No quotation supplied mentions RLHF, DPO, or any other reinforcement-learning procedure for OPT-125m. Consequently, there is no evidence that reinforcement learning was used at any stage.",
  "2-3 (API)__evidence": [],
  "3-1 (Pre-training)__evidence": [
    {
      "source": "[abstract]",
      "quote": "We present Open Pre-trained Transformers (OPT), a suite of decoder-only pre-trained transformers ranging from 125M to 175B parameters, which we aim to fully and responsibly share with interested researchers."
    },
    {
      "source": "[pdf_text]",
      "quote": "We trained OPT-175B on 992 80GB A100 GPUs, by utilizing Fully Sharded Data Parallel (Artetxe et al., 2021) with Megatron-LM Tensor Parallelism (Shoeybi et al., 2019)."
    },
    {
      "source": "[pdf_text]",
      "quote": "We follow a linear learning rate schedule, warming up from 0 to the maximum learning rate over the first 2000 steps in OPT-175B, or over 375M tokens in our smaller baselines, and decaying down to 10% of the maximum LR over 300B tokens."
    },
    {
      "source": "[sections/Contributions]",
      "quote": "Training 125M–66B baselines: Naman Goyal, Stephen Roller, Susan Zhang"
    },
    {
      "source": "[sections/2205.01068]",
      "quote": "We train the OPT models to roughly match the performance and sizes of the GPT-3 class of models, while also applying the latest best practices in data collection and efficient training."
    },
    {
      "source": "[sections/2205.01068]",
      "quote": "We follow a linear learning rate schedule, warming up from 0 to the maximum learning rate over the first 2000 steps in OPT-175B, or over 375M tokens in our smaller baselines, and decaying down to 10% of the maximum LR over 300B tokens."
    },
    {
      "source": "[sections/2205.01068]",
      "quote": "We trained OPT-175B on 992 80GB A100 GPUs, by utilizing Fully Sharded Data Parallel (Artetxe et al., 2021) with Megatron-LM Tensor Parallelism (Shoeybi et al., 2019)."
    },
    {
      "source": "[sections/Datasheet/Composition]",
      "quote": "The pre-training data for training the OPT-175B model was created by a union of five datasets, including three datasets used by RoBERTa (Liu et al., 2019b), a subset of the Pile (Gao et al., 2021a), along with the Pushshift.io Reddit dataset that was developed in (Baumgartner et al., 2020) and processed in (Roller et al., 2021). These purpose of creating this dataset was to pre-train the language model on a broad corpus of text, with emphasis on human-generated text."
    },
    {
      "source": "[sections/Conclusion]",
      "quote": "In this technical report, we introduced OPT, a collection of auto-regressive language models ranging in size from 125M to 175B parameters."
    },
    {
      "source": "[appendix/C.5 Uses]",
      "quote": "Yes, this dataset was used to pre-train the OPT models."
    },
    {
      "source": "[appendix/D.1 Model Details]",
      "quote": "Information about training algorithms, parameters, fairness constraints or other applied approaches, and features: OPT-175B was trained with AdamW for parameter sizes from 125M to 175B."
    }
  ],
  "3-2 (Fine-tuning)__evidence": [
    {
      "source": "[sections/Dialogue Safety Evaluations]",
      "quote": "We conclude that future experimentation of OPT-175B for dialogue should contain explicit fine-tuning on curated datasets in order to improve the safety profile."
    }
  ],
  "3-3 (Reinforcement Learning)__evidence": [],
  "4-1 (Pre-training Data)": "The quotations describe a single, unified pre-training mixture that was constructed specifically for the OPT-175B model and, by extension, the broader family of OPT models. The corpus is said to be “created by a union of five datasets,” consisting of: (1) three separate datasets that had previously been used for RoBERTa, (2) a subset taken from the Pile, and (3) the Pushshift.io Reddit dataset that had earlier been processed in Roller et al., 2021. The designers state that the training data for OPT-175B was “selected based on a combination of breadth and availability,” indicating that wide topical coverage and public accessibility guided inclusion. A CommonCrawl snapshot forms the temporal boundary of the crawl-based component; newer dialogue resources such as MultiSessionChat (MSC) and Wizard of Internet (WoI) were released after that snapshot, so the authors note “minimal risk of leakage” from those materials into the training set. Evaluation on a subset of MSC yields a perplexity of 9.7 and a UF1 of 0.177, demonstrating that the pre-training mixture generalizes to held-out conversational data. The Pushshift.io Reddit portion is identified as “a primary data source for OPT-175B” and is flagged by prior work (Nangia et al., 2020) as containing higher rates of stereotypes and discriminatory language than Wikipedia; consequently, the authors warn that the model may have internalized such biases, affecting downstream behaviour on benchmarks like CrowS-Pairs. Finally, multiple statements explicitly confirm that “this dataset was used to pre-train the OPT models,” underscoring that the same multi-corpus mixture underlies the entire OPT series.",
  "4-2 (Fine-tuning Data)": "The supplied quotations indicate that no supervised fine-tuning data has yet been applied to the model: “OPT-175B, in a fully unsupervised setting, performs competitively against fully supervised models.” At the same time, the authors recommend that “future experimentation of OPT-175B for dialogue should contain explicit fine-tuning on curated datasets in order to improve the safety profile.” Thus, the present release is trained solely on the pre-training mixture described above, and fine-tuning remains a planned, not executed, step aimed at enhancing safety and dialogue quality.",
  "4-3 (Reinforcement Learning Data)": "",
  "4-4 (Data Filtering)": "",
  "4-1 (Pre-training Data)__evidence": [
    {
      "source": "[sections/C.1 Motivation]",
      "quote": "The pre-training data for training the OPT-175B model was created by a union of five datasets, including three datasets used by RoBERTa (Liu et al., 2019b), a subset of the Pile (Gao et al., 2021a), along with the Pushshift.io Reddit dataset that was developed in (Baumgartner et al., 2020) and processed in (Roller et al., 2021)."
    },
    {
      "source": "[sections/C.5 Uses]",
      "quote": "Yes, this dataset was used to pre-train the OPT models."
    },
    {
      "source": "[sections/D.3 Data, Limitations, and Recommendations]",
      "quote": "Training data for OPT-175B was selected based on a combination of breadth and availability."
    },
    {
      "source": "[pdf_text]",
      "quote": "Furthermore, we evaluated OPT-175B on a subset of the ConvAI2-like MultiSessionChat (MSC) dataset (Xu et al., 2021b) and obtained a perplexity of 9.7 and UF1 of .177, indicating the model is generalizing well across multiple PersonaChat-like datasets. Since both MSC and WoI datasets were released after the CommonCrawl snapshot used in pre-training corpus, there is minimal risk of leakage."
    },
    {
      "source": "[pdf_text]",
      "quote": "Nangia et al. (2020) showed that Pushshift.io Reddit corpus has a higher incidence rate for stereotypes and discriminatory text than other corpora (e.g. Wikipedia). Given this is a primary data source for OPT-175B, the model may have learned more discriminatory associations, which directly impacts its performance on CrowS-Pairs."
    },
    {
      "source": "[sections/C.5 Uses]",
      "quote": "• Has the dataset been used for any tasks already? If so, please provide a description. Yes, this dataset was used to pre-train the OPT models."
    }
  ],
  "4-2 (Fine-tuning Data)__evidence": [
    {
      "source": "[pdf_text]",
      "quote": "OPT-175B, in a fully unsupervised setting, performs competitively against fully supervised models."
    },
    {
      "source": "[pdf_text]",
      "quote": "We conclude that future experimentation of OPT-175B for dialogue should contain explicit fine-tuning on curated datasets in order to improve the safety profile."
    }
  ],
  "4-3 (Reinforcement Learning Data)__evidence": [],
  "4-4 (Data Filtering)__evidence": [],
  "__usage": {
    "fine_tuning": "not_used",
    "rl": "not_used"
  }
}