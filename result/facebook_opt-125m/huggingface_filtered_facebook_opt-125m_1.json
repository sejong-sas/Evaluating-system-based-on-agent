{
  "1-1 (Weights)": "The weights for the target model are distributed through the HuggingFace Hub under the exact identifier \"facebook/opt-125m\".  The snippet\n>>> generator = pipeline('text-generation', model=\"facebook/opt-125m\")\nshows that any user can point the Transformers pipeline at that name and the library will automatically fetch the checkpoint.  The actual checkpoint file advertised is the standard PyTorch archive \"pytorch_model.bin\", confirming that the complete parameter tensor bundle is downloadable.  No password, e-mail registration, or additional gating is mentioned in the supplied material, so the quotation implies that the weights are immediately accessible to anyone with an Internet connection and the Transformers package.",
  "1-1 (Weights)__evidence": [
    {
      "source": "[files]",
      "quote": "pytorch_model.bin"
    },
    {
      "source": "[readme]",
      "quote": ">>> generator = pipeline('text-generation', model=\"facebook/opt-125m\")"
    }
  ],
  "1-2 (Code)": "Two different public code bases are referenced for the OPT family that explicitly cover \"opt\" models, including the 125 M parameter variant:\n• \"OPT was first introduced in [Open Pre-trained Transformer Language Models](https://arxiv.org/abs/2205.01068) and first released in [metaseq's repository](https://github.com/facebookresearch/metaseq) on May 3rd 2022 by Meta AI.\"  This line identifies the metaseq repository as the place where the original pre-training implementation was open-sourced; hence, end-to-end training code is publicly available.\n• \"In addition, the model can be fine-tuned on a downstream task using the [CLM example](https://github.com/huggingface/transformers/tree/main/examples/pytorch/language-modeling).\"  This second quotation points to the HuggingFace Transformers examples directory, explicitly giving users a ready-made script for continued causal-language-model fine-tuning of facebook/opt-125m.\nTaken together, the evidence indicates that (1) full pre-training code lives in the metaseq repository, (2) fine-tuning code is available in the official Transformers examples, and (3) the same Transformers library also acts as inference/serving code because the weights can be loaded with a single pipeline call.",
  "1-2 (Code)__evidence": [
    {
      "source": "[readme]",
      "quote": "OPT was first introduced in [Open Pre-trained Transformer Language Models](https://arxiv.org/abs/2205.01068) and first released in [metaseq's repository](https://github.com/facebookresearch/metaseq) on May 3rd 2022 by Meta AI."
    },
    {
      "source": "[readme]",
      "quote": "In addition, the model can be fine-tuned on a downstream task using the [CLM example](https://github.com/huggingface/transformers/tree/main/examples/pytorch/language-modeling)."
    }
  ],
  "1-3 (License)": "The model is governed by a bespoke, non-OSI license (quoted simply as \"license: other\" and further elaborated in the “OPT-175B LICENSE AGREEMENT”).  Key excerpts that apply to every OPT checkpoint—including the 125 M variant—state:\n• \"Subject to your compliance with the Documentation and Sections 2, 3, and 5, Meta grants you a non-exclusive, worldwide, non-transferable, non-sublicensable, revocable, royalty free and limited license under Meta’s copyright interests to reproduce, distribute, and create derivative works of the Software solely for your non-commercial research purposes.\"  Thus the grant is royalty-free but restricted to non-commercial research.\n• \"You will not, and will not permit, assist or cause any third party to … use, modify, copy, reproduce, create derivative works of, or distribute the Software Products … for (i) any commercial or production purposes.\"  Commercial or production use is explicitly forbidden.\n• Attribution is mandatory: \"Together with any copies of the Software Products … you must provide … a copy of this License, and … the following attribution notice: “OPT-175B is licensed under the OPT-175B license, Copyright (c) Meta Platforms, Inc. All Rights Reserved.”\"\n• The license is revocable and may be terminated by Meta: \"Meta may terminate this License, in whole or in part, at any time upon notice … to you.\"  Standard warranty disclaimers and limitations of liability also apply (\"WILL META BE LIABLE TO YOU … FOR ANY INDIRECT, CONSEQUENTIAL … DAMAGES\").\nA LICENSE.md file is present, reinforcing that redistribution must keep the license text intact.",
  "1-3 (License)__evidence": [
    {
      "source": "[readme]",
      "quote": "license: other"
    },
    {
      "source": "[license_file]",
      "quote": "Together with any copies of the Software Products (as well as derivative works thereof or works incorporating the Software Products) that you distribute, you must provide (i) a copy of this License, and (ii) the following attribution notice: “OPT-175B is licensed under the OPT-175B license, Copyright (c) Meta Platforms, Inc. All Rights Reserved.”"
    },
    {
      "source": "[license_file]",
      "quote": "Subject to your compliance with the Documentation and Sections 2, 3, and 5, Meta grants you a non-exclusive, worldwide, non-transferable, non-sublicensable, revocable, royalty free and limited license under Meta’s copyright interests to reproduce, distribute, and create derivative works of the Software solely for your non-commercial research purposes."
    },
    {
      "source": "[license_file]",
      "quote": "You will not, and will not permit, assist or cause any third party to:\n   a. use, modify, copy, reproduce, create derivative works of, or distribute the Software Products (or any derivative works thereof, works incorporating the Software Products, or any data produced by the Software), in whole or in part, for (i) any commercial or production purposes,"
    },
    {
      "source": "[license_file]",
      "quote": "eprint={2205.01068},\n archivePrefix={arXiv},\n primaryClass={cs.CL}\n}\n```\n\n[license_file]\n<h2 align=\"center\"> OPT-175B LICENSE AGREEMENT </h2>\n\nThis License Agreement (as may be amended in accordance with this License Agreement, **“License”**), between you, or your employer or other entity (if you are entering into this agreement on behalf of your employer or other entity) (**“Licensee”** or **“you”**) and Meta Platforms, Inc."
    },
    {
      "source": "[readme]",
      "quote": "tion (collectively, the “Software Products”), and you must immediately cease using the Software Products. If you are agreeing to be bound by the terms of this License on behalf of your employer or other entity, you represent and warrant to Meta that you have full legal authority to bind your employer or such entity to this License. If you do not h"
    },
    {
      "source": "[readme]",
      "quote": "eta that you have full legal authority to bind your employer or such entity to this License. If you do not have the requisite authority, you may not accept the License or access the Software Products on behalf of your employer or other entity.**\n<br><br>\n1. **LICENSE GRANT**\n<br><br>\n a. Subject to your compliance with the Documentation and Sections 2, 3, and 5, Meta grants you"
    },
    {
      "source": "[readme]",
      "quote": "ntation solely for use in connection with the license to the Software granted above. \n<br><br>\n c. The grant of rights expressly set forth in this Section 1 (License Grant) are the complete grant of rights to you in the Software Products, and no other licenses are granted, whether by waiver, estoppel, implication, equity or otherwise. Meta and"
    },
    {
      "source": "[readme]",
      "quote": "WILL META BE LIABLE TO YOU (A) UNDER ANY THEORY OF LIABILITY, WHETHER BASED IN CONTRACT, TORT, NEGLIGENCE, STRICT LIABILITY, WARRANTY, OR OTHERWISE UNDER THIS LICENSE, OR (B) FOR ANY INDIRECT, CONSEQUENTIAL, EXEMPLARY, INCIDENTAL, PUNITIVE OR SPECIAL DAMAGES OR LOST PROFITS, EVEN IF META HAS BEEN ADVISED OF THE POSSIBILITY OF SUCH DAMAGES. THE SOFTWARE PRODUCTS, THEIR CON"
    },
    {
      "source": "[readme]",
      "quote": "ate this License, in whole or in part, at any time upon notice (including electronic) to you.\n<br><br>\n c. The following sections survive termination of this License: 2 (Restrictions), 3 (Attribution), 4 (Disclaimers), 5 (Limitation on Liability), 6 (Indemnification) 7 (Termination; Survival), 8 (Third Party Materials), 9 (Trademarks), 10 (Applicable Law; Dispute Resolution), and 11 ("
    },
    {
      "source": "[readme]",
      "quote": "with these models through paid APIs, full model access is currently limited to only a\n> few highly resourced labs. This restricted access has limited researchers’ ability to study how and\n> why these large language models work, hindering progress on"
    },
    {
      "source": "[readme]",
      "quote": "under Meta’s copyright interests to reproduce, distribute, and create derivative works of the Software solely for your non-commercial research purposes. The foregoing license is personal to you, and you may not assign or sublicense this License or any"
    },
    {
      "source": "[files]",
      "quote": "LICENSE file present: LICENSE.md"
    }
  ],
  "1-4 (Paper)": "Documentation for the model family is primarily provided by the peer-reviewed preprint \"Open Pre-trained Transformer Language Models\" (arXiv:2205.01068).  The quotes explicitly note that \"OPT was first introduced in [Open Pre-trained Transformer Language Models](https://arxiv.org/abs/2205.01068)\" and invite readers to \"For more details, please read the [official paper](https://arxiv.org/abs/2205.01068).\"  The arXiv record (eprint 2205.01068, primary class cs.CL) functions as the definitive technical report, complemented by the metaseq GitHub release announced on 3 May 2022.",
  "1-4 (Paper)__evidence": [
    {
      "source": "[readme]",
      "quote": "OPT was first introduced in [Open Pre-trained Transformer Language Models](https://arxiv.org/abs/2205.01068) and first released in [metaseq's repository](https://github.com/facebookresearch/metaseq) on May 3rd 2022 by Meta AI."
    },
    {
      "source": "[readme]",
      "quote": "For more details, please read \n the [official paper](https://arxiv.org/abs/2205.01068)."
    }
  ]
}