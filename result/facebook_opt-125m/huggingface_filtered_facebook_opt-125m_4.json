{
  "4-1 (Pre-training Data)": "The available documentation explains that facebook/opt-125m belongs to the Open Pretrained Transformers (OPT) family, \"a suite of decoder-only pre-trained transformers ranging from 125M to 175B parameters.\" The authors state that all models in this range, including the 125 M-parameter version, were trained \"to roughly match the performance and sizes of the GPT-3 class of models\" while applying \"the latest best practices in data collection and efficient training.\" With respect to linguistic composition, they add that the 125 M model was \"predominantly pretrained with English text,\" although \"a small amount of non-English data is still present within the training corpus via CommonCrawl.\"",
  "4-1 (Pre-training Data)__evidence": [
    {
      "source": "[readme]",
      "quote": "OPT was predominantly pretrained with English text, but a small amount of non-English data is still present within the training corpus via CommonCrawl."
    },
    {
      "source": "[readme]",
      "quote": "We present Open Pretrained Transformers (OPT), a suite of decoder-only pre-trained transformers ranging from 125M to 175B parameters, which we aim to fully and responsibly share with interested researchers. We train the OPT models to roughly match the performance and sizes of the GPT-3 class of models, while also applying the latest best practices in data collection and efficient training."
    }
  ],
  "4-2 (Fine-tuning Data)": "",
  "4-2 (Fine-tuning Data)__evidence": [],
  "4-3 (Reinforcement Learning Data)": "",
  "4-3 (Reinforcement Learning Data)__evidence": [],
  "4-4 (Data Filtering)": "",
  "4-4 (Data Filtering)__evidence": []
}