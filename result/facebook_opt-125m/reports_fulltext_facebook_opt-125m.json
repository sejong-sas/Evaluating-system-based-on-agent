{
  "model_id": "facebook/opt-125m",
  "full_texts": [
    {
      "arxiv_id": "https://arxiv.org/abs/2205.01068",
      "full_text": " [2205.01068] OPT: Open Pre-trained Transformer Language Models Skip to main content We gratefully acknowledge support from the Simons Foundation, member institutions , and all contributors. Donate &gt; cs &gt; arXiv:2205.01068 Help | Advanced Search All fields Title Author Abstract Comments Journal reference ACM classification MSC classification Report number arXiv identifier DOI ORCID arXiv author ID Help pages Full text Search open search GO open navigation menu quick links Login Help Pages About --> Computer Science > Computation and Language arXiv:2205.01068 (cs) [Submitted on 2 May 2022 ( v1 ), last revised 21 Jun 2022 (this version, v4)] Title: OPT: Open Pre-trained Transformer Language Models Authors: Susan Zhang , Stephen Roller , Naman Goyal , Mikel Artetxe , Moya Chen , Shuohui Chen , Christopher Dewan , Mona Diab , Xian Li , Xi Victoria Lin , Todor Mihaylov , Myle Ott , Sam Shleifer , Kurt Shuster , Daniel Simig , Punit Singh Koura , Anjali Sridhar , Tianlu Wang , Luke Zettlemoyer View a PDF of the paper titled OPT: Open Pre-trained Transformer Language Models, by Susan Zhang and 18 other authors View PDF Abstract: Large language models, which are often trained for hundreds of thousands of compute days, have shown remarkable capabilities for zero- and few-shot learning. Given their computational cost, these models are difficult to replicate without significant capital. For the few that are available through APIs, no access is granted to the full model weights, making them difficult to study. We present Open Pre-trained Transformers (OPT), a suite of decoder-only pre-trained transformers ranging from 125M to 175B parameters, which we aim to fully and responsibly share with interested researchers. We show that OPT-175B is comparable to GPT-3, while requiring only 1/7th the carbon footprint to develop. We are also releasing our logbook detailing the infrastructure challenges we faced, along with code for experimenting with all of the released models. Subjects: Computation and Language (cs.CL) ; Machine Learning (cs.LG) Cite as: arXiv:2205.01068 [cs.CL] &nbsp; (or arXiv:2205.01068v4 [cs.CL] for this version) &nbsp; https://doi.org/10.48550/arXiv.2205.01068 Focus to learn more arXiv-issued DOI via DataCite Submission history From: Susan Zhang [ view email ] [v1] Mon, 2 May 2022 17:49:50 UTC (9,196 KB) [v2] Tue, 3 May 2022 15:04:06 UTC (9,190 KB) [v3] Thu, 5 May 2022 11:44:30 UTC (7,822 KB) [v4] Tue, 21 Jun 2022 17:04:40 UTC (7,832 KB) Full-text links: Access Paper: View a PDF of the paper titled OPT: Open Pre-trained Transformer Language Models, by Susan Zhang and 18 other authors View PDF TeX Source Other Formats view license Current browse context: cs.CL &lt;&nbsp;prev &nbsp; | &nbsp; next&nbsp;&gt; new | recent | 2022-05 Change to browse by: cs cs.LG References &amp; Citations NASA ADS Google Scholar Semantic Scholar 3 blog links ( what is this? ) export BibTeX citation Loading... BibTeX formatted citation &times; loading... Data provided by: Bookmark Bibliographic Tools Bibliographic and Citation Tools Bibliographic Explorer Toggle Bibliographic Explorer ( What is the Explorer? ) Connected Papers Toggle Connected Papers ( What is Connected Papers? ) Litmaps Toggle Litmaps ( What is Litmaps? ) scite.ai Toggle scite Smart Citations ( What are Smart Citations? ) Code, Data, Media Code, Data and Media Associated with this Article alphaXiv Toggle alphaXiv ( What is alphaXiv? ) Links to Code Toggle CatalyzeX Code Finder for Papers ( What is CatalyzeX? ) DagsHub Toggle DagsHub ( What is DagsHub? ) GotitPub Toggle Gotit.pub ( What is GotitPub? ) Huggingface Toggle Hugging Face ( What is Huggingface? ) Links to Code Toggle Papers with Code ( What is Papers with Code? ) ScienceCast Toggle ScienceCast ( What is ScienceCast? ) Demos Demos Replicate Toggle Replicate ( What is Replicate? ) Spaces Toggle Hugging Face Spaces ( What is Spaces? ) Spaces Toggle TXYZ.AI ( What is TXYZ.AI? ) Related Papers Recommenders and Search Tools Link to Influence Flower Influence Flower ( What are Influence Flowers? ) Core recommender toggle CORE Recommender ( What is CORE? ) Author Venue Institution Topic About arXivLabs arXivLabs: experimental projects with community collaborators arXivLabs is a framework that allows collaborators to develop and share new arXiv features directly on our website. Both individuals and organizations that work with arXivLabs have embraced and accepted our values of openness, community, excellence, and user data privacy. arXiv is committed to these values and only works with partners that adhere to them. Have an idea for a project that will add value for arXiv's community? Learn more about arXivLabs . Which authors of this paper are endorsers? | Disable MathJax ( What is MathJax? ) About Help contact arXiv Click here to contact arXiv Contact subscribe to arXiv mailings Click here to subscribe Subscribe Copyright Privacy Policy Web Accessibility Assistance arXiv Operational Status Get status notifications via email or slack ",
      "fetch_method": "direct-html"
    },
    {
      "arxiv_id": "https://arxiv.org/pdf/2205.01068.pdf",
      "full_text": "OPT: Open Pre-trained Transformer Language Models\nSusan Zhang∗, Stephen Roller∗, Naman Goyal∗,\nMikel Artetxe, Moya Chen, Shuohui Chen, Christopher Dewan, Mona Diab, Xian Li,\nXi Victoria Lin, Todor Mihaylov, Myle Ott†, Sam Shleifer†, Kurt Shuster, Daniel Simig,\nPunit Singh Koura, Anjali Sridhar, Tianlu Wang, Luke Zettlemoyer\nMeta AI\n{susanz,roller,naman}@fb.com\nAbstract\nLarge language models,\nwhich are often\ntrained for hundreds of thousands of compute\ndays, have shown remarkable capabilities for\nzero- and few-shot learning. Given their com-\nputational cost, these models are difﬁcult to\nreplicate without signiﬁcant capital. For the\nfew that are available through APIs, no ac-\ncess is granted to the full model weights, mak-\ning them difﬁcult to study. We present Open\nPre-trained Transformers (OPT), a suite of\ndecoder-only pre-trained transformers ranging\nfrom 125M to 175B parameters, which we aim\nto fully and responsibly share with interested\nresearchers. We show that OPT-175B is com-\nparable to GPT-3,1 while requiring only 1/7th\nthe carbon footprint to develop. We are also\nreleasing our logbook detailing the infrastruc-\nture challenges we faced, along with code for\nexperimenting with all of the released models.\n1\nIntroduction\nLarge language models (LLMs) trained on massive\ntext collections have shown surprising emergent\ncapabilities to generate text and perform zero- and\nfew-shot learning (Brown et al., 2020; Lieber et al.,\n2021; Smith et al., 2022; Rae et al., 2021; Chowd-\nhery et al., 2022). While in some cases the public\ncan interact with these models through paid APIs,\nfull model access is currently limited to only a\nfew highly resourced labs.2 This restricted access\nhas limited researchers’ ability to study how and\nwhy these large language models work, hindering\n∗Equal contribution.\n†Work done while at Meta AI.\n1Following Brown et al. (2020), we use GPT-3 to refer to\nboth the 175B model and the smaller scale models as well.\n2Exceptions include work by EleutherAI, who released\ndense models up to 20B in size (Black et al., 2022),\nSalesforce (Nijkamp et al., 2022), and Meta AI, who re-\nleased dense models up to 13B and sparse models up to\n1.1T (Artetxe et al., 2021).\nThere is also ongoing work\nfrom the BigScience workshop (https://bigscience.\nhuggingface.co/), which aims to open source very large\nmultilingual language models and datasets.\nprogress on improving known challenges in areas\nsuch as robustness, bias, and toxicity.\nIn this technical report, we present Open Pre-\ntrained Transformers (OPT), a suite of decoder-\nonly pre-trained transformers ranging from 125M\nto 175B parameters, which we aim to fully and\nresponsibly share with interested researchers. We\ntrain the OPT models to roughly match the per-\nformance and sizes of the GPT-3 class of models,\nwhile also applying the latest best practices in data\ncollection and efﬁcient training. Our aim in de-\nveloping this suite of OPT models is to enable re-\nproducible and responsible research at scale, and\nto bring more voices to the table in studying the\nimpact of these LLMs. Deﬁnitions of risk, harm,\nbias, and toxicity, etc., should be articulated by the\ncollective research community as a whole, which is\nonly possible when models are available for study.\nWe are releasing all of our models between\n125M and 66B parameters, and will provide full\nresearch access to OPT-175B upon request. Ac-\ncess will be granted to academic researchers; those\nafﬁliated with organizations in government, civil\nsociety, and academia; and those in industry re-\nsearch laboratories. We are also releasing both the\nlogbook of our model creation as well as our code-\nbase, metaseq,3 which enabled training OPT-175B\non 992 80GB A100 GPUs, reaching 147 TFLOP/s\nutilization per GPU. From this implementation, and\nfrom using the latest generation of NVIDIA hard-\nware, we are able to develop OPT-175B using only\n1/7th the carbon footprint of GPT-3. While this is a\nsigniﬁcant achievement, the energy cost of creating\nsuch a model is still nontrivial, and repeated efforts\nto replicate a model of this size will only amplify\nthe growing compute footprint of these LLMs.\nWe believe the entire AI community — aca-\ndemic researchers, civil society, policymakers, and\nindustry — must work together to develop clear\n3https://github.com/facebookresearch/\nmetaseq\narXiv:2205.01068v4  [cs.CL]  21 Jun 2022\n\nModel\n#L\n#H\ndmodel\nLR\nBatch\n125M\n12\n12\n768\n6.0e−4\n0.5M\n350M\n24\n16\n1024\n3.0e−4\n0.5M\n1.3B\n24\n32\n2048\n2.0e−4\n1M\n2.7B\n32\n32\n2560\n1.6e−4\n1M\n6.7B\n32\n32\n4096\n1.2e−4\n2M\n13B\n40\n40\n5120\n1.0e−4\n4M\n30B\n48\n56\n7168\n1.0e−4\n4M\n66B\n64\n72\n9216\n0.8e−4\n2M\n175B\n96\n96\n12288\n1.2e−4\n2M\nTable 1: Model architecture details. We report the\nnumber of layers (#L), number of attention heads (#H),\nand the embedding size (dmodel). We also report the\npeak Learning Rate (LR) and global batch size in num-\nber of tokens (Batch).\nguidelines around responsible AI in general and\nresponsible LLMs in particular, given their cen-\ntrality in many downstream language applications.\nA much broader segment of the AI community\nneeds access to these models in order to conduct\nreproducible research and collectively drive the\nﬁeld forward. With the release of OPT-175B and\nsmaller-scale baselines, we hope to increase the di-\nversity of voices deﬁning the ethical considerations\nof such technologies.\n2\nMethod\n2.1\nModels\nWe present results on eight Transformer language\nmodels ranging from 125 million to 175 billion\nparameters. Architectural details are displayed in\nTable 1. In the interest of transparency, and to re-\nduce risk of training instabilities, our models and\nhyperparameters largely follow Brown et al. (2020),\nwith variations in batch size mostly to obtain in-\ncreased computational efﬁciency.\n2.2\nTraining Setup\nFor weight initialization, we follow the same set-\ntings provided in the Megatron-LM codebase,4 us-\ning a normal distribution with zero mean and stan-\ndard deviation of 0.006. Standard deviation for\noutput layers are scaled by a 1.0/\n√\n2L term where\nL is the total number of layers. All bias terms are\ninitialized as 0, and all models are trained with\nReLU activation and a sequence length of 2048.\n4https://github.com/NVIDIA/\nMegatron-LM/blob/main/examples/pretrain_\ngpt3_175B.sh\nWe use an AdamW optimizer (Loshchilov and\nHutter, 2017) with (β1, β2) set to (0.9, 0.95), and\nweight decay of 0.1. We follow a linear learning\nrate schedule, warming up from 0 to the maximum\nlearning rate over the ﬁrst 2000 steps in OPT-175B,\nor over 375M tokens in our smaller baselines, and\ndecaying down to 10% of the maximum LR over\n300B tokens. A number of mid-ﬂight changes\nto LR were also required (see Section 2.5). Our\nbatch sizes range from 0.5M to 4M depending on\nthe model size (see Table 1) and is kept constant\nthroughout the course of training.\nWe use a dropout of 0.1 throughout, but we\ndo not apply any dropout to embeddings.\nWe\nclip gradient norms at 1.0, except for some mid-\nﬂight changes that reduce this threshold down\nfrom 1.0 to 0.3 (see Section 2.5).\nWe also in-\nclude a gradient predivide factor to reduce the risk\nof over/underﬂows when computing the gradient\nacross all ranks (splitting the division by the world\nsize of N into two division operations by\n√\nN).\n2.3\nPre-training Corpus\nThe pre-training corpus contains a concatenation\nof datasets used in RoBERTa (Liu et al., 2019b),\nthe Pile (Gao et al., 2021a), and PushShift.io Red-\ndit (Baumgartner et al., 2020; Roller et al., 2021).\nAll corpora were previously collected or ﬁltered\nto contain predominantly English text, but a small\namount of non-English data is still present within\nthe corpus via CommonCrawl.\nWe removed duplicated documents across all\ndatasets by ﬁltering out documents via Min-\nhashLSH (Rajaraman and Ullman, 2011) with a\nJaccard similarity ≥.95. We found the Pile was\nparticularly full of duplicate documents, and ad-\nvise future researchers using the Pile to perform\nadditional de-duplication processing.\nWe tokenize all corpora using the GPT-2 byte\nlevel BPE tokenizer (Sennrich et al., 2016; Radford\net al., 2019; Brown et al., 2020). Our ﬁnal corpus\ncontains roughly 180B tokens.\nRoBERTa\nWe included the BookCorpus (Zhu\net al., 2015) and Stories (Trinh and Le, 2018) sub-\nsets of the RoBERTa corpus and utilized an up-\ndated version of CCNews, containing news stories\ncrawled through September 28, 2021. This CC-\nNews v2 corpus was preprocessed the same way as\nthe original RoBERTa CCNews (Liu et al., 2019b).\nThe Pile\nWe included a subset of the Pile\n(Gao et al., 2021a), including: CommonCrawl,\n\nDM Mathematics,\nProject Gutenberg,\nHack-\nerNews, OpenSubtitles, OpenWebText2, USPTO\nand Wikipedia. Other subsets of the Pile were elim-\ninated as we found they increased the risk of insta-\nbilities, as measured by tendency to cause spikes\nin gradient norms at the 1.3B scale, or were other-\nwise deemed unsuitable. All subsets went through\nadditional ad-hoc whitespace normalization.\nPushShift.io Reddit\nWe included a subset of\nthe Pushshift.io corpus produced by Baumgart-\nner et al. (2020) and previously used by Roller\net al. (2021). To convert the conversational trees\ninto language-model-accessible documents, we ex-\ntracted the longest chain of comments in each\nthread and discarded all other paths in the tree.\nThis reduced the corpus by about 66%.\n2.4\nTraining Efﬁciency\nWe trained OPT-175B on 992 80GB A100 GPUs,\nby utilizing Fully Sharded Data Parallel (Artetxe\net al., 2021) with Megatron-LM Tensor Parallelism\n(Shoeybi et al., 2019). We achieve utilization of up\nto 147 TFLOP/s per GPU. We keep Adam state in\nFP32, since we shard it across all hosts, while the\nmodel weights remained in FP16. To avoid under-\nﬂows, we used dynamic loss scaling, as described\nin Micikevicius et al. (2017).\n2.5\nTraining Processes\nHere we describe signiﬁcant training process ad-\njustments that arose during OPT-175B pre-training.\nHardware Failures\nWe faced a signiﬁcant num-\nber of hardware failures in our compute cluster\nwhile training OPT-175B. In total, hardware fail-\nures contributed to at least 35 manual restarts and\nthe cycling of over 100 hosts over the course of 2\nmonths. During manual restarts, the training run\nwas paused, and a series of diagnostics tests were\nconducted to detect problematic nodes. Flagged\nnodes were then cordoned off and training was re-\nsumed from the last saved checkpoint. Given the\ndifference between the number of hosts cycled out\nand the number of manual restarts, we estimate 70+\nautomatic restarts due to hardware failures.\nLoss Divergences\nLoss divergences were also an\nissue in our training run. When the loss diverged,\nwe found that lowering the learning rate and restart-\ning from an earlier checkpoint allowed for the job\nto recover and continue training. We noticed a cor-\nrelation between loss divergence, our dynamic loss\n0k\n20k\n40k\n60k\n80k\n100k\n120k\n140k\nIterations\n0.0e-4\n0.2e-4\n0.4e-4\n0.6e-4\n0.8e-4\n1.0e-4\n1.2e-4\nLearning Rate\nEmpirical Learning Rate\nFigure 1: Empirical LR schedule. We found that low-\nering learning rate was helpful for avoiding instabili-\nties.\n0k\n20k\n40k\n60k\n80k\n100k\n120k\n140k\nIterations\n7.0\n7.5\n8.0\n8.5\n9.0\n9.5\n10.0\nPerplexity\nValidation Perplexity\nFigure 2: Validation Perplexity. Our mid-ﬂight LR\nchanges had clear effects on validation perplexity.\nscalar crashing to 0, and the l2-norm of the activa-\ntions of the ﬁnal layer spiking. These observations\nled us to pick restart points for which our dynamic\nloss scalar was still in a “healthy” state (≥1.0),\nand after which our activation norms would trend\ndownward instead of growing unboundedly. Our\nempirical LR schedule is shown in Figure 1. Early\nin training, we also noticed that lowering gradient\nclipping from 1.0 to 0.3 helped with stability; see\nour released logbook for exact details. Figure 2\nshows our validation loss with respect to training\niterations.\nOther Mid-ﬂight Changes\nWe conducted a\nnumber of other experimental mid-ﬂight changes\nto handle loss divergences. These included: switch-\ning to vanilla SGD (optimization plateaued quickly,\nand we reverted back to AdamW); resetting the dy-\nnamic loss scalar (this helped recover some but not\nall divergences); and switching to a newer version\nof Megatron (this reduced pressure on activation\nnorms and improved throughput).\n\n3\nEvaluations\n3.1\nPrompting & Few-Shot\nWe evaluate our model on 16 standard NLP tasks\nutilized in the literature: HellaSwag (Zellers et al.,\n2019), StoryCloze (Mostafazadeh et al., 2016),\nPIQA (Bisk et al., 2020), ARC Easy and Challenge\n(Clark et al., 2018), OpenBookQA (Mihaylov et al.,\n2018), WinoGrad (Levesque et al., 2011), Wino-\nGrande (Sakaguchi et al., 2020), and SuperGLUE\n(Wang et al., 2019). We follow GPT-3 (Brown\net al., 2020) by using their prompts and overall ex-\nperimental setup. We compare primarily to GPT-3,\nhaving aimed to re-implement their evaluation set-\ntings, but include reported performance of other\nLLMs on a per-task basis when available (Lieber\net al., 2021; Rae et al., 2021; Hoffmann et al., 2022;\nBlack et al., 2022)\nWe report performance in accuracy (omitting F1\nfor MultiRC and ReCoRD for consistency in eval-\nuation metrics). For the Winograd Schema Chal-\nlenge (WSC) task in the SuperGLUE benchmark,\nwe follow (Brown et al., 2020) and formulate the\ntask as multiple choice questions, which is known\nto affect performance (Liu et al., 2020).\nZero-shot\nOverall average zero-shot perfor-\nmance across all 14 tasks may be seen in Figure 3.\nOverall, we see our average performance follows\nthe trend of GPT-3. However, performance can\nvary radically across the tasks: for a full break-\ndown, see Appendix A. Note that we intentionally\nremoved MultiRC and WIC from these averages, as\nthese datasets seem to systematically favor GPT-3\nor OPT disproportionately.\nOur performance roughly matched GPT-3 for 10\ntasks, and underperformed in 3 tasks (ARC Chal-\nlenge and MultiRC). In 3 tasks (CB, BoolQ, WSC),\nwe ﬁnd both GPT and OPT models display unpre-\ndictable behavior with respect to scale, likely due\nto the small size of the validation set in these 3\ntasks (56, 277, and 104 examples, respectively).\nIn WIC, we see that the OPT models always out-\nperform the GPT-3 models, though the numbers\nreported by Brown et al. (2020) also seem question-\nable, given WIC being a binary classiﬁcation task.5\nFor MultiRC, we are unable to replicate the GPT-3\nresults using the Davinci API6 within our evalua-\ntion setup, suggesting differences in the methods\n5Brown et al. (2020) reports 0% accuracy on WIC, which\nimplies 100% accuracy if the classiﬁcation was inverted.\n6https://beta.openai.com/docs/engines/\noverview\n108\n109\n1010\n1011\nParameters\n50\n55\n60\n65\n70\nAvg. Accuracy\nAverage across 14 NLP Tasks (Zero-Shot)\nOPT\nGPT\nFigure 3:\nZero-shot NLP Evaluation Averages.\nAcross a variety of tasks and model sizes, OPT largely\nmatches the reported averages of GPT-3. However, per-\nformance varies greatly per task: see Appendix A.\n108\n109\n1010\n1011\nParameters\n50\n55\n60\n65\n70\n75\nAvg. Accuracy\nAverage across 14 NLP Tasks\nShot\n0\n1\n32\nSeries\nOPT\nGPT\nFigure 4:\nMulti-shot performance.\nOPT perfor-\nmance for one- and few-shot lags behind GPT-3 mod-\nels, but performance depends heavily per task; see Ap-\npendix A.\nof evaluation on this task. For BoolQ and WSC,\nwe note that both OPT and GPT models seem to\nhover around majority-class accuracy, suggesting\nsmall perturbations in probability masses may be\ndominating the evaluations.\nChinchilla (Hoffmann et al., 2022) and Gopher\n(Rae et al., 2021) perform roughly consistently\nwith others for their parameter sizes, while PaLM\n(Chowdhery et al., 2022) generally performs better\nacross all settings, even when controlling for num-\nber of parameters. We speculate the high perfor-\nmance of PaLM comes predominantly from higher\nquality and diversity of pre-training data.\nOne-shot and Few-shot\nAverage multi-shot in-\ncontext performance is shown in Figure 4 (again,\nomitting MultiRC and WIC), with detailed perfor-\nmances shown in Appendix A. Across the average\n\nof all metrics, we ﬁnd that OPT models perform\nsimilarly to GPT-3 models. However, as with zero-\nshot, breaking down these results per task shows\na different story: in the same set of 10 datasets as\nzero-shot, we see similar performance across the\ntwo models. Some of the remaining datasets show\ninconsistent performance with respect to model\nsize for both OPT and GPT-3 models (BoolQ, CB,\nWSC, RTE). In MultiRC, we consistently see un-\nderperformance of OPT models compared to GPT-\n3 models. Similar to our zero-shot evaluation, we\nhypothesize our one- and few-shot evaluation setup\nmay differ signiﬁcantly from Brown et al. (2020).\n3.2\nDialogue\nGiven that LLMs are known to be an integral com-\nponent of modern dialogue models (Adiwardana\net al., 2020; Roller et al., 2021; Thoppilan et al.,\n2022; Rae et al., 2021; Chowdhery et al., 2022), we\nadditionally evaluate OPT-175B on several open\nsource dialogue datasets. In particular, we fol-\nlow Roller et al. (2021), and evaluate on ConvAI2\n(Dinan et al., 2020b), Wizard of Wikipedia (Di-\nnan et al., 2019b), Empathetic Dialogues (Rashkin\net al., 2019), and Blended Skill Talk (Smith et al.,\n2020). We additionally evaluate on the more recent\nWizard of Internet dataset (Komeili et al., 2021).\nWe focus our comparisons primarily against ex-\nisting open source dialogue models including the\nﬁne-tuned BlenderBot 1 (Roller et al., 2021) and\nits pre-training counterpart Reddit 2.7B. We also\ncompare against the ﬁne-tuned R2C2 BlenderBot,\na 2.7B parameter BlenderBot-like model trained by\nShuster et al. (2022).\nWe report Perplexity and Unigram F1 (UF1)\noverlap, following the metrics of the ConvAI2 com-\npetition (Dinan et al., 2020b). To control for dif-\nferent tokenization in each of the models, we nor-\nmalize all perplexities to be in the space of the\nGPT-2 tokenizer (Radford et al., 2019). We also\nnote which models are supervised with respect to\nthese dialogue tasks and which are unsupervised.\nFor OPT-175B, all generations are performed using\ngreedy decoding up to a maximum of 32 tokens.\nWe do not attempt to prompt the model at all except\nfor alternating “Person 1:” and “Person 2:” lines of\ndialogue. The remaining models use the generation\nparameters found in BlenderBot 1.\nResults are shown in Table 2.\nWe see that\nOPT-175B signiﬁcantly outperforms the also-\nunsupervised Reddit 2.7B model on all tasks, and\nperforms competitively with the fully supervised\nBlenderBot 1 model, especially in the ConvAI2\ndataset. On the Wizard-of-Internet dataset, which\nis fully unsupervised for all models, we see that\nOPT-175B obtains the lowest perplexity but still\nhas lower UF1 than the models with Wizard-of-\nWikipedia supervision.\nWe were somewhat surprised that the evaluations\nof the unsupervised OPT-175B model were as com-\npetitive as BlenderBot 1 on the ConvAI2 dataset.\nThis may indicate leakage of the ConvAI2 dataset\ninto the general pre-training corpus or even into the\nvalidation data as evaluated in Table 2. To address\nconcerns of leakage, we searched our pre-training\ncorpus for the ﬁrst conversation in the ConvAI2\ndataset, but we did not ﬁnd any overlap. We addi-\ntionally evaluated OPT-175B on the ConvAI2 hid-\nden test set, which has never been publicly released,\nand achieved 10.7 ppl and .185 UF1, matching the\nperformance of the validation set. Furthermore, we\nevaluated OPT-175B on a subset of the ConvAI2-\nlike MultiSessionChat (MSC) dataset (Xu et al.,\n2021b) and obtained a perplexity of 9.7 and UF1\nof .177, indicating the model is generalizing well\nacross multiple PersonaChat-like datasets. Since\nboth MSC and WoI datasets were released after the\nCommonCrawl snapshot used in pre-training cor-\npus, there is minimal risk of leakage. We conclude\nthat OPT-175B has a strong ability to maintain a\nconsistent persona across conversations, a behav-\nior also highlighted in LaMDA (Thoppilan et al.,\n2022).\n4\nBias & Toxicity Evaluations\nTo understand the potential harm of OPT-175B,\nwe evaluate a series of benchmarks related to hate\nspeech detection, stereotype awareness, and toxic\ncontent generation. While there may be shortcom-\nings in these benchmarks (Blodgett et al., 2021; Ja-\ncobs and Wallach, 2021), these measurements pro-\nvide a ﬁrst step towards understanding the limita-\ntions of OPT-175B. We compare primarily against\nGPT-3 Davinci, as these benchmarks were not yet\navailable to be included in Brown et al. (2020).\n4.1\nHate Speech Detection\nUsing the ETHOS dataset provided in Mollas et al.\n(2020) and instrumented by Chiu and Alexander\n(2021), we measure the ability of OPT-175B to\nidentify whether or not certain English statements\nare racist or sexist (or neither). In the zero-, one-,\n\nPerplexity (↓)\nUnigram F1 (↑)\nModel\nEval\nC2\nWW\nED\nBST\nWoI\nC2\nWW\nED\nBST\nWoI\nReddit 2.7B\nUnsup.\n18.9\n21.0\n11.6\n17.4\n18.0\n.126\n.133\n.135\n.133\n.124\nBlenderBot 1\nSup.\n10.2\n12.5\n9.0\n11.9\n14.7\n.183\n.189\n.192\n.178\n.154\nR2C2 BlenderBot\nSup.\n10.5\n12.4\n9.1\n11.7\n14.6\n.205\n.198\n.197\n.186\n.160\nOPT-175B\nUnsup.\n10.8\n13.3\n10.3\n12.1\n12.0\n.185\n.152\n.149\n.162\n.147\nTable 2: Dialogue Evaluations. OPT-175B, in a fully unsupervised setting, performs competitively against fully\nsupervised models.\nSetup\nDavinci\nOPT-175B\nZero-shot\n.628\n.667\nOne-shot\n.616\n.713\nFew-shot (binary)\n.354\n.759\nFew-shot (multiclass)\n.672\n.812\nTable 3: Hate speech detection. F1 scores of detect-\ning hate speech between Davinci and OPT-175B. OPT-\n175B considerably outperforms Davinci in all settings.\nand few-shot binary cases, the model is presented\nwith text and asked to consider whether the text is\nracist or sexist and provide a yes/no response. In\nthe few-shot multiclass setting, the model is asked\nto provide a yes/no/neither response.\nResults are presented in Table 3. With all of\nour one-shot through few-shot conﬁgurations, OPT-\n175B performs considerably better than Davinci.\nWe speculate this occurs from two sources: (1)\nevaluating via the Davinci API may be bringing\nin safety control mechanisms beyond the original\n175B GPT-3 model used in Brown et al. (2020);\nand (2) the signiﬁcant presence of unmoderated\nsocial media discussions in the pre-training dataset\nhas provided additional inductive bias to aid in such\nclassiﬁcation tasks.\n4.2\nCrowS-Pairs\nDeveloped for masked language models, CrowS-\nPairs (Nangia et al., 2020) is a crowdsourced bench-\nmark aiming to measure intrasentence level biases\nin 9 categories: gender, religion, race/color, sex-\nual orientation, age, nationality, disability, physical\nappearance, and socioeconomic status. Each exam-\nple consists of a pair of sentences representing a\nstereotype, or anti-stereotype, regarding a certain\ngroup, with the goal of measuring model preference\ntowards stereotypical expressions. Higher scores\nindicate higher bias exhibited by a model.\nCategory\nGPT-3\nOPT-175B\nGender\n62.6\n65.7\nReligion\n73.3\n68.6\nRace/Color\n64.7\n68.6\nSexual orientation\n76.2\n78.6\nAge\n64.4\n67.8\nNationality\n61.6\n62.9\nDisability\n76.7\n76.7\nPhysical appearance\n74.6\n76.2\nSocioeconomic status\n73.8\n76.2\nOverall\n67.2\n69.5\nTable 4: CrowS-Pairs evaluation. Lower is better for\nall categories, indicating more fairness. The OPT-175B\nmodel performs worse than Davinci in most categories.\nWhen compared with Davinci in Table 4, OPT-\n175B appears to exhibit more stereotypical biases\nin almost all categories except for religion. Again,\nthis is likely due to differences in training data;\nNangia et al. (2020) showed that Pushshift.io Red-\ndit corpus has a higher incidence rate for stereo-\ntypes and discriminatory text than other corpora\n(e.g. Wikipedia).\nGiven this is a primary data\nsource for OPT-175B, the model may have learned\nmore discriminatory associations, which directly\nimpacts its performance on CrowS-Pairs.\n4.3\nStereoSet\nFollowing Lieber et al. (2021) and Artetxe et al.\n(2021), we use StereoSet (Nadeem et al., 2021)\nto measure stereotypical bias across 4 categories:\nprofession, gender, religion, and race. In addition\nto intrasentence measurement (similar to CrowS-\nPairs), StereoSet includes measurement at the inter-\nsentence level to test a model’s ability to incorpo-\nrate additional context. To account for a potential\ntrade-off between bias detection and language mod-\neling capability, StereoSet includes two metrics:\n\nCategory\nDavinci\nOPT-175B\nProf.\nLMS (↑)\n78.4\n74.1\nSS (↓)\n63.4\n62.6\nICAT (↑)\n57.5\n55.4\nGend.\nLMS (↑)\n75.6\n74.0\nSS (↓)\n66.5\n63.6\nICAT (↑)\n50.6\n53.8\nReli.\nLMS (↑)\n80.8\n84.0\nSS (↓)\n59.0\n59.0\nICAT (↑)\n66.3\n68.9\nRace\nLMS (↑)\n77.0\n74.9\nSS (↓)\n57.4\n56.8\nICAT (↑)\n65.7\n64.8\nOverall\nLMS (↑)\n77.6\n74.8\nSS (↓)\n60.8\n59.9\nICAT (↑)\n60.8\n60.0\nTable 5: StereoSet Evaluations. Davinci and OPT-\n175B perform similarly across all evaluations.\nLanguage Modeling Score (LMS) and Stereotype\nScore (SS), which are then combined to form the\nIdealized Context Association Test score (ICAT).\nUnlike Lieber et al. (2021), we normalize scores\nby token count, rather than character count, which\nthey report improves metrics for several models.\nResults are shown in Table 5.\nWe see that\nDavinci and OPT-175B exhibit similar scores on\naggregate (overall ICAT is very close between the\ntwo). In particular, Davinci outperforms in the\nareas of profession and race, while OPT-175B out-\nperforms in the areas of Gender and Religion. OPT-\n175B performs better across the board on the SS\nmetric, while Davinci generally outperforms on the\nLMS metric.\n4.4\nRealToxicityPrompts\nWe evaluate the tendency of OPT-175B to respond\nwith toxic language via the RealToxicityPrompts\n(Gehman et al., 2020) dataset. Following PaLM\n(Chowdhery et al., 2022), we sample 25 genera-\ntions of 20 tokens using nucleus sampling (Holtz-\nman et al., 2020) (p = 0.9) for each of 10, 000\nrandomly sampled prompts from RTP, and report\nmean toxicity probabilities of the continuations,\nstratiﬁed across bucketed toxicities of the original\nprompts. For comparison, we report bucketed toxi-\ncity rates from Davinci and PaLM.\nResults are shown in Figure 5. Overall, we see\n0.0\n0.1\n0.2\n0.3\n0.4\n0.5\n0.6\n0.7\n0.8\n0.9\n1.0\nPrompt Toxicity Probability (Binned)\n0.00\n0.05\n0.10\n0.15\n0.20\n0.25\n0.30\n0.35\n0.40\n0.45\nToxicity Probability of Continuation (TPC)\nToxicity Probability of Prompt (TPP)\nOPT 175B\nDavinci\nPaLM\nFigure 5: RealToxicityPompts.\nOPT-175B is more\nlikely to generate toxic responses than either Davinci\nor PaLM. Consistent with prior work, toxicity rates in-\ncrease as prompt toxicity increases.\nthat OPT-175B has a higher toxicity rate than ei-\nther PaLM or Davinci. We also observe that all\n3 models have increased likelihood of generating\ntoxic continuations as the toxicity of the prompt\nincreases, which is consistent with the observations\nof Chowdhery et al. (2022). As with our exper-\niments in hate speech detection, we suspect the\ninclusion of unmoderated social media texts in the\npre-training corpus raises model familiarity with,\nand therefore propensity to generate and detect,\ntoxic text. This strong awareness of toxic language\nmay or may not be desirable depending on the\nspeciﬁc requirements of downstream applications.\nFuture applications of OPT-175B should consider\nthis aspect of the model, and take additional miti-\ngations, or avoid usage entirely as appropriate.\n4.5\nDialogue Safety Evaluations\nFinally, we compare OPT-175B on two Dialogue\nSafety evaluations. The ﬁrst, SaferDialogues (Ung\net al., 2021), measures the ability to recover from\nexplicit safety failures, usually in the form of apol-\nogizing or recognizing its mistake. The second, the\nSafety Bench Unit Tests (Dinan et al., 2021), mea-\nsures how unsafe a model’s response is, stratiﬁed\nacross 4 levels of topic sensitivity: Safe, Realis-\ntic, Unsafe, and Adversarial. As with the other\ndialogue evaluations (Section 3.2), we compare to\nseveral existing open source dialogue models.\nResults for both experiments are shown in Ta-\nble 6. We observe that OPT-175B has similar per-\nformance as the Reddit 2.7B model across both\nSaferDialogues and the Unit Tests, with OPT-175B\nperforming marginally better in the Safe and Adver-\nsarial settings. Consistent with Roller et al. (2021)\n\nSafe. Dia.\nUnit Tests (↓)\nModel\nPPL\nF1\nSa\nRe\nUn\nAd\nReddit 2.7B\n16.2\n.140\n.300\n.261\n.450\n.439\nBlenderBot 1\n12.4\n.161\n.028\n.150\n.250\n.194\nR2C2 BlenderBot\n13.8\n.160\n.022\n.133\n.289\n.222\nOPT-175B\n14.7\n.141\n.033\n.261\n.567\n.283\nTable 6: Dialogue Responsible AI evaluations. OPT-\n175B is roughly on par with the Reddit 2.7B model, but\nperforms worse in the Unsafe setting.\nand Xu et al. (2020), we ﬁnd that the models ﬁne-\ntuned on curated dialogue datasets (BlenderBot 1,\nR2C2) have overall lower toxicity. We conclude\nthat future experimentation of OPT-175B for dia-\nlogue should contain explicit ﬁne-tuning on curated\ndatasets in order to improve the safety proﬁle.\n5\nLimitations\nIn Sections 3.1 and 4, we carried out extensive\nevaluation of all released models at varying scales.\nWe saw parity in performance for standard evalu-\nation datasets used in the GPT-3 models. More-\nover, we performed safety, bias, and inclusion eval-\nuations, again seeing largely comparable perfor-\nmance with some variations in toxicity and hate\nspeech detection. However, such evaluations may\nnot fully characterize the complete limitations of\nthese models. In general, we qualitatively observe\nthat OPT-175B suffers from the same limitations\nnoted in other LLMs (Brown et al., 2020; Lieber\net al., 2021; Thoppilan et al., 2022; Rae et al., 2021;\nSmith et al., 2022; Chowdhery et al., 2022; Bender\net al., 2021).\nIn particular, we found OPT-175B does not work\nwell with declarative instructions or point-blank\ninterrogatives. Prompting with such instructions\ntends to produce a simulation of a dialogue begin-\nning with such an instruction, rather than an execu-\ntion of the instruction. Future work into instruction\nlearning, in the vein of InstructGPT (Ouyang et al.,\n2022), may alleviate these limitations.\nOPT-175B also tends to be repetitive and can eas-\nily get stuck in a loop. While sampling can reduce\nthe incidence rate of repetitive behavior (Holtz-\nman et al., 2020), we anecdotally found it did not\neliminate it entirely when only one generation is\nsampled. Future work may wish to incorporate\nmore modern strategies for reducing repetition and\nimproving diversity, such as unlikelihood training\n(Welleck et al., 2020) or best-ﬁrst decoding (Meis-\nter et al., 2020).\nSimilar to other LLMs, OPT-175B can produce\nfactually incorrect statements (Adiwardana et al.,\n2020; Brown et al., 2020; Roller et al., 2021; Rae\net al., 2021; Chowdhery et al., 2022; Thoppilan\net al., 2022). This can be particularly harmful in\napplications where information accuracy is critical,\nsuch as healthcare and scientiﬁc discovery (Wei-\ndinger et al., 2021b). Recently, several efforts have\nreported that retrieval-augmented models can im-\nprove factual correctness of LLMs (Lewis et al.,\n2020; Komeili et al., 2021; Thoppilan et al., 2022;\nBorgeaud et al., 2021; Shuster et al., 2022; Nakano\net al., 2021). We believe OPT-175B will also bene-\nﬁt from retrieval-augmentation in future iterations.\nAs shown in Section 4, we also ﬁnd OPT-175B\nhas a high propensity to generate toxic language\nand reinforce harmful stereotypes, even when pro-\nvided with a relatively innocuous prompt (Gehman\net al., 2020), and adversarial prompts are trivial to\nﬁnd (Dinan et al., 2021). There has been a great\ndeal of work on mitigations for toxicity and bi-\nases (Dathathri et al., 2019; Dinan et al., 2019a;\nSheng et al., 2019; Dinan et al., 2020a; Liu et al.,\n2019a; Krause et al., 2020; Xu et al., 2020; Liang\net al., 2021; Dinan et al., 2021; Xu et al., 2021a;\nDhamala et al., 2021; Schick et al., 2021; Ouyang\net al., 2022). Depending on downstream applica-\ntions, future uses of OPT-175B may need to employ\nthese or novel mitigation approaches, especially be-\nfore any real world deployment. Given our primary\ngoal as a replication of GPT-3, we choose not to\napply these mitigations in this ﬁrst release.\nIn summary, we still believe this technology is\npremature for commercial deployment. Despite\nincluding data sheets and model cards, we believe\nmore scrutiny should be afforded to the training\ndata with additional data characterization and se-\nlection criteria in order to use data responsibly. The\ncurrent practice is to feed the model with as much\ndata as possible and minimal selection within these\ndatasets. Despite having comprehensive evalua-\ntions, we would ideally have more streamlined and\nconsistent evaluation setups to ensure replicability\nand reproducibility of evaluation scenarios. Dif-\nferences in prompting styles and number of shots\nfor in-context learning could create variations that\nlead to different results. We hope that the public\nrelease of the OPT models will enable many more\nresearchers to work on these important issues.\n\n6\nConsiderations for Release\nFollowing the recommendations for individual re-\nsearchers generated by the Partnership for AI,7\nalong with the governance guidance outlined by\nNIST,8 we are disclosing all of the details in-\nvolved in training OPT-175B through our log-\nbook,9 our code, and providing researchers access\nto model weights for OPT-175B, along with a suite\nof smaller baselines mirroring the setup for OPT-\n175B. We aim to be fully accountable for the devel-\nopment lifecycle of OPT-175B, and only through\nincreasing transparency around LLM development\ncan we start understanding the limitations and risks\nof LLMs before broader deployment occurs.\nBy sharing a detailed account of our day-to-day\ntraining process, we disclose not only how much\ncompute was used to train the current version of\nOPT-175B, but also the human overhead required\nwhen underlying infrastructure or the training pro-\ncess itself becomes unstable at scale. These details\nare generally omitted from previous publications,\nlikely due to the inability to fully ablate changes\nmade mid-ﬂight (without drastically increasing the\ncompute budget). We hope that by revealing how\ncertain ad-hoc design decisions were made, we can\nimprove upon these practices in the future, and col-\nlectively increase the experimental robustness in\ndeveloping models at this scale.\nOutside of these notes, the metaseq codebase\nitself is the ﬁnal source of truth in many of our\nimplementation details. By releasing our develop-\nment codebase, we aim to shed light on any imple-\nmentation detail that may have been omitted from\nbeing explicitly enumerated in this paper, as it is\neither considered a detail of standard practice in\nthe ﬁeld, or is simply a detail we failed to account\nfor. This current codebase is also the only known\nopen-source implementation of training a decoder-\nonly transformer that is ≥175B parameters without\nthe use of pipeline paralellism on NVIDIA GPUs.\nTo enable experimentation at 175B scale, we are\nproviding researchers with direct access to the pa-\nrameters of OPT-175B. The reasoning here is two-\nfold: enable Responsible AI research into LLMs\nwhile simultaneously reducing the environmental\n7https://partnershiponai.org/paper/\nresponsible-publication-recommendations/\n8https://nvlpubs.nist.gov/nistpubs/\nSpecialPublications/NIST.SP.1270.pdf\n9https://github.com/facebookresearch/\nmetaseq/blob/main/projects/OPT/\nchronicles/OPT175B_Logbook.pdf\nimpact of pursuing research at this scale. There is a\ngrowing body of work detailing ethical and social\nrisks from deploying language models with emer-\ngent capabilities at scale (Weidinger et al., 2021a;\nBommasani et al., 2021; Dinan et al., 2021; Kenton\net al., 2021). By limiting access to OPT-175B to\nthe research community with a non-commercial\nlicense, we aim to focus development efforts on\nquantifying the limitations of the LLMs ﬁrst, be-\nfore broader commercial deployment occurs.\nFurthermore, there exists signiﬁcant compute\nand carbon cost to reproduce models of this size.\nWhile OPT-175B was developed with an estimated\ncarbon emissions footprint (CO2eq) of 75 tons,10\nGPT-3 was estimated to use 500 tons (Patterson\net al., 2021), while Gopher required 380 tons (Rae\net al., 2021). These estimates are not universally re-\nported, and the accounting methodologies for these\ncalculations are also not standardized. In addition,\nmodel training is only one component of the over-\nall carbon footprint of AI systems; we must also\nconsider experimentation and eventual downstream\ninference cost, all of which contribute to the grow-\ning energy footprint of creating large-scale models\n(Wu et al., 2022). By releasing our logbook, we\nhope to highlight the gap between a theoretical car-\nbon cost estimate that assumes no hardware failures\nor training instabilities, versus one that aims to in-\nclude the entire LLM development lifecycle. We\nneed to understand the manufacturing (or embod-\nied) carbon of these systems (Gupta et al., 2021)\nas they grow increasingly more complex, and we\nhope that our paper can help future work in deﬁn-\ning additional factors to consider when measuring\nthe impact of scale on the environment.\nSimilarly, by producing a set of baselines across\na wide range of scales, we hope to enable the\nbroader research community to study the impact\nand limitations of these models with respect to\nscale alone. As reported in Hoffmann et al. (2022),\nmany of these LLMs may have been under-trained\nas a function of the amount of training data used,\nwhich implies that incorporating more data and con-\ntinuing to train these baseline models may continue\nto improve performance. There is also evidence\nthat step-function changes in capabilities may oc-\ncur at a scale that is much smaller than 175B (Wei\net al., 2021), indicating a need to examine a wider\nrange of scales for different research applications.\n10With ablations, baselines and downtime, our own esti-\nmates of total cost is roughly 2× higher.\n\n7\nRelated Work\nSince the publication of the Transformer architec-\nture (Vaswani et al., 2017) and BERT (Devlin et al.,\n2019), the ﬁeld of NLP has experienced a massive\nshift towards the use of LLMs with self-supervised\npre-training. Multiple masked langauge models,\nincluding T5 (Raffel et al., 2020) and Megatron-\nLM (Shoeybi et al., 2019), have shown consistent\nimprovements through scale. These scaling gains\ncome not only from growing the total number of\nparameters in the models, but also the amount and\nquality of pre-training data (Liu et al., 2019b; Hoff-\nmann et al., 2022).\nAuto-regressive language models (Mikolov et al.,\n2009) have seen the largest growth in model size,\nfrom 117M parameters (Radford et al., 2018) to\nover 500B parameters (Smith et al., 2022; Chowd-\nhery et al., 2022). The resulting massive improve-\nment in generative ﬂuency and quality was ﬁrst\ncharacterized in GPT-2 (Radford et al., 2019) and\nfurther improved with GPT-3 (Brown et al., 2020)\nand later models. Although a variety of very large\n(over 100B parameters) generative models have\nnow been trained (Lieber et al., 2021; Rae et al.,\n2021; Thoppilan et al., 2022; Smith et al., 2022;\nChowdhery et al., 2022), they are all closed source\nand accessible only internally or via paid API ser-\nvices. There are a few notable efforts towards open\nsourcing LLMs from non-proﬁt research organiza-\ntions including EleutherAI (Black et al., 2022) and\nBigScience.11 These models differ from the OPT\nmodels in pre-training data, target languages and\nmodel scale, making it possible for the community\nto compare different pre-training strategies.\nSince Brown et al. (2020), the primary evalu-\nation criterion for LLMs has been prompt-based\n(Black et al., 2022; Rae et al., 2021; Chowdhery\net al., 2022), as is also performed in this paper.\nThis is largely due to the convenience of evaluat-\ning on many tasks without specialized task-speciﬁc\nﬁne-tuning. Prompting itself has a long history:\ncloze evaluations go back several decades (Cham-\nbers and Jurafsky, 2008; Mostafazadeh et al., 2016).\nMore recently, prompting or masked inﬁlling has\nbeen used to probe models for knowledge (Petroni\net al., 2019) or perform a variety of NLP tasks\n(Radford et al., 2019; Brown et al., 2020). There\nhas also been work on eliciting prompting behav-\nior in smaller models (Schick and Schütze, 2020;\n11https://huggingface.co/bigscience/\ntr11-176B-ml-logs/tensorboard\nGao et al., 2021b; Li and Liang, 2021; Lester et al.,\n2021; Scao and Rush, 2021), improving the ﬂexi-\nbility of prompting (Shin et al., 2020), and under-\nstanding why and how prompting works (Liu et al.,\n2021; Min et al., 2022).\nRecent efforts have shown gains by ﬁne-tuning\nmodels to directly respond to instruction-style\nprompting (Wei et al., 2021; Min et al., 2021; Sanh\net al., 2021; Ouyang et al., 2022). However, ef-\nfective prompt engineering remains an open re-\nsearch challenge. Results vary signiﬁcantly and\nunpredictably with the selection of the prompt (Lu\net al., 2021), and models do not seem to understand\nthe prompts as fully as we expect (Webson and\nPavlick, 2021). Furthermore, it is challenging to\nwrite prompts without a development set, which\nleads to questions about the extent to which we\nare actually achieving zero- or few-shot learning in\npractice (Perez et al., 2021). We do not attempt to\naddress these concerns of prompting, and instead\nonly aim to provide evaluation of OPT-175B in ex-\nisting settings. However, we hope the full release of\nOPT-175B will enable others to better study these\nchallenges in the future.\n8\nConclusion\nIn this technical report, we introduced OPT, a col-\nlection of auto-regressive language models ranging\nin size from 125M to 175B parameters. Our goal\nwas to replicate the performance and sizes of the\nGPT-3 class of models, while also applying the\nlatest best practices in data curation and training\nefﬁciency. We described training details, evaluated\nperformance in a number of NLP and dialogue set-\ntings, and characterized behaviors with respect to\nbias, toxicity and hate speech. We also described\nmany other limitations the models have, and dis-\ncussed a wide set of considerations for responsibly\nreleasing the models. We believe the entire AI\ncommunity would beneﬁt from working together\nto develop guidelines for responsible LLMs, and\nwe hope that broad access to these types of models\nwill increase the diversity of voices deﬁning the\nethical considerations of such technologies.\nAcknowledgements\nWe would like to thank Scott Jeschonek, Giri Anan-\ntharaman, Diego Sarina, Joaquin Colombo, Chris\nBray, Stephen Roylance, Kalyan Saladi, Shubho\nSengupta, and Brian O’Horo for helping to remove\ninfrastructure blockers along the way; Percy Liang,\n\nRishi Bommasani, and Emily Dinan for discus-\nsions on responsible release practices; Carole-Jean\nWu for discussions on sustainability and carbon\nfootprint considerations; Srini Iyer, Ramakanth Pa-\nsunuru, and Shruti Bhosale for previous contribu-\ntions to evaluations; Benjamin Lefaudeux, Geeta\nChauhan, Natalia Gimelshein, Horace He, and Sam\nGross for discussions on performance improvement\nwork; Emily Dinan, Carole-Jean Wu, Daniel McK-\ninnon, and Mark Tygert for feedback on this draft;\nAntoine Bordes, Joelle Pineau, Mary Williamson,\nNecip Fazil Ayan, Armand Joulin, Sergey Edunov,\nMelanie Kambadur, Zornitsa Kozareva, Ves Stoy-\nanov, Vitaliy Liptchinsky, Rahul Iyer, Jing Xu, Ja-\nson Weston, and many others for supporting this\nproject internally.\nReferences\nDaniel Adiwardana, Minh-Thang Luong, David R So,\nJamie Hall, Noah Fiedel, Romal Thoppilan, Zi Yang,\nApoorv Kulshreshtha, Gaurav Nemade, Yifeng Lu,\net al. 2020.\nTowards a human-like open-domain\nchatbot. arXiv preprint arXiv:2001.09977.\nMikel Artetxe, Shruti Bhosale, Naman Goyal, Todor\nMihaylov, Myle Ott, Sam Shleifer, Xi Victoria Lin,\nJingfei Du, Srinivasan Iyer, Ramakanth Pasunuru,\nGiri Anantharaman, Xian Li, Shuohui Chen, Halil\nAkin, Mandeep Baines, Louis Martin, Xing Zhou,\nPunit Singh Koura, Brian O’Horo, Jeff Wang, Luke\nZettlemoyer, Mona T. Diab, Zornitsa Kozareva, and\nVes Stoyanov. 2021.\nEfﬁcient large scale lan-\nguage modeling with mixtures of experts.\nCoRR,\nabs/2112.10684.\nJason Baumgartner, Savvas Zannettou, Brian Keegan,\nMegan Squire, and Jeremy Blackburn. 2020. The\npushshift reddit dataset. CoRR, abs/2001.08435.\nEmily M Bender, Timnit Gebru, Angelina McMillan-\nMajor, and Shmargaret Shmitchell. 2021.\nOn the\ndangers of stochastic parrots: Can language models\nbe too big? In Proceedings of the 2021 ACM Confer-\nence on Fairness, Accountability, and Transparency,\npages 610–623.\nYonatan Bisk, Rowan Zellers, Ronan Le bras, Jianfeng\nGao, and Yejin Choi. 2020. Piqa: Reasoning about\nphysical commonsense in natural language.\nPro-\nceedings of the AAAI Conference on Artiﬁcial Intel-\nligence, 34(05):7432–7439.\nSid Black, Stella Biderman, Eric Hallahan, Quentin An-\nthony, Leo Gao, Laurence Golding, Horace He, Con-\nnor Leahy, Kyle McDonell, Jason Phang, Michael\nPieler, USVSN Sai Prashanth, Shivanshu Purohit,\nLaria Reynolds, Jonathan Tow, Ben Wang, and\nSamuel Weinbach. 2022. Gpt-neox-20b: An open-\nsource autoregressive language model.\nSu Lin Blodgett, Gilsinia Lopez, Alexandra Olteanu,\nRobert Sim, and Hanna Wallach. 2021. Stereotyp-\ning Norwegian salmon: An inventory of pitfalls in\nfairness benchmark datasets. In Proceedings of the\n59th Annual Meeting of the Association for Compu-\ntational Linguistics and the 11th International Joint\nConference on Natural Language Processing (Vol-\nume 1: Long Papers), pages 1004–1015, Online. As-\nsociation for Computational Linguistics.\nRishi Bommasani, Drew A. Hudson, Ehsan Adeli,\nRuss Altman, Simran Arora, Sydney von Arx,\nMichael S. Bernstein, Jeannette Bohg, Antoine\nBosselut, Emma Brunskill, Erik Brynjolfsson, Shya-\nmal Buch, Dallas Card, Rodrigo Castellon, Ni-\nladri Chatterji, Annie S. Chen, Kathleen Creel,\nJared Quincy Davis, Dorottya Demszky, Chris Don-\nahue, Moussa Doumbouya, Esin Durmus, Stefano\nErmon, John Etchemendy, Kawin Ethayarajh, Li Fei-\nFei, Chelsea Finn, Trevor Gale, Lauren Gillespie,\nKaran Goel, Noah D. Goodman, Shelby Grossman,\nNeel Guha, Tatsunori Hashimoto, Peter Henderson,\nJohn Hewitt, Daniel E. Ho, Jenny Hong, Kyle Hsu,\nJing Huang, Thomas Icard, Saahil Jain, Dan Juraf-\nsky, Pratyusha Kalluri, Siddharth Karamcheti, Geoff\nKeeling, Fereshte Khani, Omar Khattab, Pang Wei\nKoh, Mark S. Krass, Ranjay Krishna, Rohith Kudi-\ntipudi, and et al. 2021.\nOn the opportunities and\nrisks of foundation models. CoRR, abs/2108.07258.\nSebastian Borgeaud, Arthur Mensch, Jordan Hoff-\nmann, Trevor Cai, Eliza Rutherford, Katie Millican,\nGeorge van den Driessche, Jean-Baptiste Lespiau,\nBogdan Damoc, Aidan Clark, et al. 2021. Improv-\ning language models by retrieving from trillions of\ntokens. arXiv preprint arXiv:2112.04426.\nTom Brown, Benjamin Mann, Nick Ryder, Melanie\nSubbiah,\nJared\nD\nKaplan,\nPrafulla\nDhariwal,\nArvind Neelakantan, Pranav Shyam, Girish Sastry,\nAmanda Askell, Sandhini Agarwal, Ariel Herbert-\nVoss, Gretchen Krueger, Tom Henighan, Rewon\nChild, Aditya Ramesh, Daniel Ziegler, Jeffrey Wu,\nClemens Winter, Chris Hesse, Mark Chen, Eric\nSigler, Mateusz Litwin, Scott Gray, Benjamin Chess,\nJack Clark, Christopher Berner, Sam McCandlish,\nAlec Radford, Ilya Sutskever, and Dario Amodei.\n2020. Language models are few-shot learners. In\nAdvances in Neural Information Processing Systems,\nvolume 33, pages 1877–1901. Curran Associates,\nInc.\nNathanael Chambers and Dan Jurafsky. 2008. Unsuper-\nvised learning of narrative event chains. In Proceed-\nings of ACL-08: HLT, pages 789–797, Columbus,\nOhio. Association for Computational Linguistics.\nKe-Li Chiu and Rohan Alexander. 2021.\nDetect-\ning hate speech with gpt-3.\narXiv preprint\narXiv:2103.12407.\nAakanksha Chowdhery, Sharan Narang, Jacob Devlin,\nMaarten Bosma, Gaurav Mishra, Adam Roberts,\nPaul Barham, Hyung Won Chung, Charles Sutton,\nSebastian Gehrmann, Parker Schuh, Kensen Shi,\n\nSasha Tsvyashchenko, Joshua Maynez, Abhishek\nRao, Parker Barnes, Yi Tay, Noam Shazeer, Vin-\nodkumar Prabhakaran, Emily Reif, Nan Du, Ben\nHutchinson, Reiner Pope, James Bradbury, Jacob\nAustin, Michael Isard, Guy Gur-Ari, Pengcheng\nYin, Toju Duke, Anselm Levskaya, Sanjay Ghe-\nmawat, Sunipa Dev, Henryk Michalewski, Xavier\nGarcia, Vedant Misra, Kevin Robinson, Liam Fe-\ndus, Denny Zhou, Daphne Ippolito, David Luan,\nHyeontaek Lim, Barret Zoph, Alexander Spiridonov,\nRyan Sepassi, David Dohan, Shivani Agrawal, Mark\nOmernick, Andrew M. Dai, Thanumalayan Sankara-\nnarayana Pillai, Marie Pellat, Aitor Lewkowycz,\nErica Moreira, Rewon Child, Oleksandr Polozov,\nKatherine Lee, Zongwei Zhou, Xuezhi Wang, Bren-\nnan Saeta, Mark Diaz, Orhan Firat, Michele Catasta,\nJason Wei, Kathy Meier-Hellstern, Douglas Eck,\nJeff Dean, Slav Petrov, and Noah Fiedel. 2022.\nPalm: Scaling language modeling with pathways.\nPeter Clark, Isaac Cowhey, Oren Etzioni, Tushar Khot,\nAshish Sabharwal, Carissa Schoenick, and Oyvind\nTafjord. 2018.\nThink you have solved question\nanswering?\ntry arc, the AI2 reasoning challenge.\nCoRR, abs/1803.05457.\nSumanth Dathathri, Andrea Madotto, Janice Lan, Jane\nHung, Eric Frank, Piero Molino, Jason Yosinski, and\nRosanne Liu. 2019. Plug and play language mod-\nels: A simple approach to controlled text generation.\narXiv preprint arXiv:1912.02164.\nJacob Devlin, Ming-Wei Chang, Kenton Lee, and\nKristina Toutanova. 2019.\nBERT: Pre-training of\ndeep bidirectional transformers for language under-\nstanding. In North American Association for Com-\nputational Linguistics (NAACL).\nJwala Dhamala, Tony Sun, Varun Kumar, Satyapriya\nKrishna, Yada Pruksachatkun, Kai-Wei Chang, and\nRahul Gupta. 2021.\nBold: Dataset and metrics\nfor measuring biases in open-ended language gen-\neration. In Proceedings of the 2021 ACM Confer-\nence on Fairness, Accountability, and Transparency,\npages 862–872.\nEmily Dinan, Gavin Abercrombie, A Stevie Bergman,\nShannon Spruit, Dirk Hovy, Y-Lan Boureau, and\nVerena Rieser. 2021.\nAnticipating safety issues\nin e2e conversational ai: Framework and tooling.\narXiv preprint arXiv:2107.03451.\nEmily Dinan, Angela Fan, Adina Williams, Jack Ur-\nbanek, Douwe Kiela, and Jason Weston. 2020a.\nQueens are powerful too: Mitigating gender bias in\ndialogue generation.\nIn Proceedings of the 2020\nConference on Empirical Methods in Natural Lan-\nguage Processing (EMNLP), pages 8173–8188, On-\nline. Association for Computational Linguistics.\nEmily Dinan, Samuel Humeau, Bharath Chintagunta,\nand Jason Weston. 2019a. Build it break it ﬁx it for\ndialogue safety: Robustness from adversarial human\nattack. arXiv preprint arXiv:1908.06083.\nEmily Dinan,\nVarvara Logacheva,\nValentin\nMa-\nlykh, Alexander Miller, Kurt Shuster, Jack Ur-\nbanek, Douwe Kiela, Arthur Szlam, Iulian Serban,\nRyan Lowe, Shrimai Prabhumoye, Alan W. Black,\nAlexander Rudnicky, Jason Williams, Joelle Pineau,\nMikhail Burtsev, and Jason Weston. 2020b.\nThe\nsecond conversational intelligence challenge (Con-\nvAI2). In The NeurIPS ’18 Competition, pages 187–\n208, Cham. Springer International Publishing.\nEmily Dinan, Stephen Roller, Kurt Shuster, Angela\nFan, Michael Auli, and Jason Weston. 2019b. Wiz-\nard of Wikipedia: Knowledge-powered conversa-\ntional agents. In Proceedings of the International\nConference on Learning Representations.\nLeo Gao, Stella Biderman, Sid Black, Laurence Gold-\ning, Travis Hoppe, Charles Foster, Jason Phang,\nHorace He, Anish Thite, Noa Nabeshima, Shawn\nPresser, and Connor Leahy. 2021a. The pile: An\n800gb dataset of diverse text for language modeling.\nCoRR, abs/2101.00027.\nTianyu Gao, Adam Fisch, and Danqi Chen. 2021b.\nMaking pre-trained language models better few-shot\nlearners. In Proceedings of the 59th Annual Meet-\ning of the Association for Computational Linguis-\ntics and the 11th International Joint Conference on\nNatural Language Processing, ACL/IJCNLP 2021,\n(Volume 1: Long Papers), Virtual Event, August 1-6,\n2021, pages 3816–3830. Association for Computa-\ntional Linguistics.\nTimnit\nGebru,\nJamie\nMorgenstern,\nBriana\nVec-\nchione,\nJennifer\nWortman\nVaughan,\nHanna\nWallach,\nHal Daumé III, and Kate Crawford.\n2021.\nDatasheets for datasets.\nCommun. ACM,\n64(12):86–92.\nSamuel Gehman, Suchin Gururangan, Maarten Sap,\nYejin Choi, and Noah A. Smith. 2020.\nRealToxi-\ncityPrompts: Evaluating neural toxic degeneration\nin language models. In Findings of the Association\nfor Computational Linguistics: EMNLP 2020, pages\n3356–3369, Online. Association for Computational\nLinguistics.\nUdit Gupta, Young Geun Kim, Sylvia Lee, Jordan Tse,\nHsien-Hsin S Lee, Gu-Yeon Wei, David Brooks, and\nCarole-Jean Wu. 2021. Chasing carbon: The elu-\nsive environmental footprint of computing. IEEE In-\nternational Symposium on High-Performance Com-\nputer Architecture (HPCA 2021).\nKaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian\nSun. 2016. Deep residual learning for image recog-\nnition. In Proceedings of the IEEE conference on\ncomputer vision and pattern recognition, pages 770–\n778.\nJordan Hoffmann, Sebastian Borgeaud, Arthur Mensch,\nElena Buchatskaya, Trevor Cai, Eliza Rutherford,\nDiego de Las Casas, Lisa Anne Hendricks, Johannes\nWelbl, Aidan Clark, Tom Hennigan, Eric Noland,\nKatie Millican, George van den Driessche, Bogdan\n\nDamoc, Aurelia Guy, Simon Osindero, Karen Si-\nmonyan, Erich Elsen, Jack W. Rae, Oriol Vinyals,\nand Laurent Sifre. 2022. Training compute-optimal\nlarge language models.\nAri Holtzman, Jan Buys, Maxwell Forbes, and Yejin\nChoi. 2020. The curious case of neural text degener-\nation. ArXiv, abs/1904.09751.\nAbigail Z. Jacobs and Hanna Wallach. 2021. Measure-\nment and fairness. In Proceedings of the 2021 ACM\nConference on Fairness, Accountability, and Trans-\nparency, FAccT ’21, page 375–385, New York, NY,\nUSA. Association for Computing Machinery.\nZachary Kenton, Tom Everitt, Laura Weidinger, Ia-\nson Gabriel, Vladimir Mikulik, and Geoffrey Irv-\ning. 2021. Alignment of language agents. CoRR,\nabs/2103.14659.\nMojtaba Komeili, Kurt Shuster, and Jason Weston.\n2021.\nInternet-augmented dialogue generation.\nCoRR, abs/2107.07566.\nBen Krause, Akhilesh Deepak Gotmare, Bryan Mc-\nCann, Nitish Shirish Keskar, Shaﬁq Joty, Richard\nSocher, and Nazneen Fatema Rajani. 2020. GEDI:\nGenerative discriminator guided sequence genera-\ntion. arXiv preprint arXiv:2009.06367.\nBrian Lester, Rami Al-Rfou, and Noah Constant. 2021.\nThe power of scale for parameter-efﬁcient prompt\ntuning. CoRR, abs/2104.08691.\nHector J Levesque, Ernest Davis, and Leora Morgen-\nstern. 2011. The Winograd schema challenge. In\nAAAI Spring Symposium: Logical Formalizations of\nCommonsense Reasoning, volume 46, page 47.\nPatrick Lewis, Ethan Perez, Aleksandra Piktus, Fabio\nPetroni, Vladimir Karpukhin, Naman Goyal, Hein-\nrich Küttler, Mike Lewis, Wen-tau Yih, Tim Rock-\ntäschel, et al. 2020. Retrieval-augmented generation\nfor knowledge-intensive nlp tasks. Advances in Neu-\nral Information Processing Systems, 33:9459–9474.\nXiang Lisa Li and Percy Liang. 2021. Preﬁx-Tuning:\nOptimizing Continuous Prompts for Generation.\npages 4582–4597.\nPaul Pu Liang, Chiyu Wu, Louis-Philippe Morency,\nand Ruslan Salakhutdinov. 2021.\nTowards under-\nstanding and mitigating social biases in language\nmodels.\nIn International Conference on Machine\nLearning, pages 6565–6576. PMLR.\nOpher Lieber, Or Sharir, Barak Lenz, and Yoav\nShoham. 2021.\nJurassic-1: Technical details and\nevaluation. Technical report, AI21 Labs.\nHaochen Liu, Jamell Dacon, Wenqi Fan, Hui Liu, Zitao\nLiu, and Jiliang Tang. 2019a. Does gender matter?\ntowards fairness in dialogue systems. arXiv preprint\narXiv:1910.10486.\nHaokun Liu, William Huang, Dhara Mungra, and\nSamuel R. Bowman. 2020. Precise task formaliza-\ntion matters in Winograd schema evaluations.\nIn\nProceedings of the 2020 Conference on Empirical\nMethods in Natural Language Processing (EMNLP),\npages 8275–8280, Online. Association for Computa-\ntional Linguistics.\nJiachang Liu, Dinghan Shen, Yizhe Zhang, Bill Dolan,\nLawrence Carin, and Weizhu Chen. 2021.\nWhat\nmakes good in-context examples for gpt-3? CoRR,\nabs/2101.06804.\nYinhan Liu, Myle Ott, Naman Goyal, Jingfei Du, Man-\ndar Joshi, Danqi Chen, Omer Levy, Mike Lewis,\nLuke Zettlemoyer, and Veselin Stoyanov. 2019b.\nRoberta: A robustly optimized bert pretraining ap-\nproach. arXiv preprint arXiv:1907.11692.\nIlya Loshchilov and Frank Hutter. 2017.\nFixing\nweight decay regularization in adam.\nCoRR,\nabs/1711.05101.\nYao Lu, Max Bartolo, Alastair Moore, Sebastian\nRiedel, and Pontus Stenetorp. 2021.\nFantastically\nordered prompts and where to ﬁnd them: Overcom-\ning few-shot prompt order sensitivity.\nClara Meister, Tim Vieira, and Ryan Cotterell. 2020.\nBest-ﬁrst beam search. Transactions of the Associa-\ntion for Computational Linguistics, 8:795–809.\nPaulius Micikevicius, Sharan Narang, Jonah Alben,\nGregory Diamos, Erich Elsen, David Garcia, Boris\nGinsburg,\nMichael Houston,\nOleksii Kuchaiev,\nGanesh Venkatesh, et al. 2017.\nMixed precision\ntraining. arXiv preprint arXiv:1710.03740.\nTodor Mihaylov, Peter Clark, Tushar Khot, and Ashish\nSabharwal. 2018. Can a suit of armor conduct elec-\ntricity? A new dataset for open book question an-\nswering. CoRR, abs/1809.02789.\nTomas Mikolov, Jiri Kopecky, Lukas Burget, Ondrej\nGlembek, et al. 2009.\nNeural network based lan-\nguage models for highly inﬂective languages.\nIn\n2009 IEEE international conference on acoustics,\nspeech and signal processing, pages 4725–4728.\nIEEE.\nSewon Min, Mike Lewis, Luke Zettlemoyer, and Han-\nnaneh Hajishirzi. 2021. Metaicl: Learning to learn\nin context.\nSewon Min, Xinxi Lyu, Ari Holtzman, Mikel Artetxe,\nMike Lewis, Hannaneh Hajishirzi, and Luke Zettle-\nmoyer. 2022.\nRethinking the role of demonstra-\ntions: What makes in-context learning work? arXiv\npreprint arXiv:2202.12837.\nMargaret Mitchell, Simone Wu, Andrew Zaldivar,\nParker Barnes, Lucy Vasserman, Ben Hutchinson,\nElena Spitzer, Inioluwa Deborah Raji, and Timnit\nGebru. 2018.\nModel cards for model reporting.\nCoRR, abs/1810.03993.\n\nIoannis Mollas, Zoe Chrysopoulou, Stamatis Kar-\nlos, and Grigorios Tsoumakas. 2020.\nETHOS:\nan online hate speech detection dataset.\nCoRR,\nabs/2006.08328.\nNasrin Mostafazadeh, Nathanael Chambers, Xiaodong\nHe,\nDevi Parikh,\nDhruv Batra,\nLucy Vander-\nwende, Pushmeet Kohli, and James F. Allen. 2016.\nA corpus and evaluation framework for deeper\nunderstanding of commonsense stories.\nCoRR,\nabs/1604.01696.\nMoin Nadeem, Anna Bethke, and Siva Reddy. 2021.\nStereoSet:\nMeasuring stereotypical bias in pre-\ntrained language models. In Association for Com-\nputational Linguistics (ACL).\nReiichiro Nakano, Jacob Hilton, Suchir Balaji, Jeff Wu,\nLong Ouyang, Christina Kim, Christopher Hesse,\nShantanu Jain, Vineet Kosaraju, William Saunders,\net al. 2021.\nWebgpt: Browser-assisted question-\nanswering with human feedback.\narXiv preprint\narXiv:2112.09332.\nNikita Nangia, Clara Vania, Rasika Bhalerao, and\nSamuel R Bowman. 2020.\nCrows-pairs: A chal-\nlenge dataset for measuring social biases in masked\nlanguage models. arXiv preprint arXiv:2010.00133.\nErik Nijkamp, Bo Pang, Hiroaki Hayashi, Lifu Tu,\nHuan Wang, Yingbo Zhou, Silvio Savarese, and\nCaiming Xiong. 2022. A conversational paradigm\nfor program synthesis. arXiv preprint.\nLong Ouyang, Jeff Wu, Xu Jiang, Diogo Almeida, Car-\nroll L Wainwright, Pamela Mishkin, Chong Zhang,\nSandhini Agarwal, Katarina Slama, Alex Ray, et al.\n2022.\nTraining language models to follow in-\nstructions with human feedback.\narXiv preprint\narXiv:2203.02155.\nDavid Patterson, Joseph Gonzalez, Quoc Le, Chen\nLiang, Lluis-Miquel Munguia, Daniel Rothchild,\nDavid So, Maud Texier, and Jeff Dean. 2021. Car-\nbon emissions and large neural network training.\narXiv preprint arXiv:2104.10350.\nEthan Perez, Douwe Kiela, and Kyunghyun Cho. 2021.\nTrue few-shot learning with language models. Ad-\nvances in Neural Information Processing Systems,\n34.\nFabio Petroni, Tim Rocktäschel, Sebastian Riedel,\nPatrick Lewis, Anton Bakhtin, Yuxiang Wu, and\nAlexander Miller. 2019. Language models as knowl-\nedge bases?\nIn Proceedings of the 2019 Confer-\nence on Empirical Methods in Natural Language\nProcessing and the 9th International Joint Confer-\nence on Natural Language Processing (EMNLP-\nIJCNLP), pages 2463–2473, Hong Kong, China. As-\nsociation for Computational Linguistics.\nAlec Radford, Karthik Narasimhan, Time Salimans,\nand Ilya Sutskever. 2018. Improving language un-\nderstanding with unsupervised learning. Technical\nreport, OpenAI.\nAlec Radford, Jeffrey Wu, Rewon Child, David Luan,\nDario Amodei, and Ilya Sutskever. 2019. Language\nmodels are unsupervised multitask learners. Techni-\ncal report, OpenAI.\nJack W. Rae, Sebastian Borgeaud, Trevor Cai, Katie\nMillican, Jordan Hoffmann, H. Francis Song, John\nAslanides, Sarah Henderson, Roman Ring, Susan-\nnah Young, Eliza Rutherford, Tom Hennigan, Ja-\ncob Menick, Albin Cassirer, Richard Powell, George\nvan den Driessche, Lisa Anne Hendricks, Mari-\nbeth Rauh, Po-Sen Huang, Amelia Glaese, Jo-\nhannes Welbl, Sumanth Dathathri, Saffron Huang,\nJonathan Uesato, John Mellor, Irina Higgins, An-\ntonia Creswell, Nat McAleese, Amy Wu, Erich\nElsen, Siddhant M. Jayakumar, Elena Buchatskaya,\nDavid Budden, Esme Sutherland, Karen Simonyan,\nMichela Paganini, Laurent Sifre, Lena Martens,\nXiang Lorraine Li, Adhiguna Kuncoro, Aida Ne-\nmatzadeh, Elena Gribovskaya, Domenic Donato,\nAngeliki Lazaridou, Arthur Mensch, Jean-Baptiste\nLespiau, Maria Tsimpoukelli, Nikolai Grigorev,\nDoug Fritz, Thibault Sottiaux, Mantas Pajarskas,\nToby Pohlen, Zhitao Gong, Daniel Toyama, Cy-\nprien de Masson d’Autume, Yujia Li, Tayfun Terzi,\nVladimir Mikulik, Igor Babuschkin, Aidan Clark,\nDiego de Las Casas, Aurelia Guy, Chris Jones,\nJames Bradbury, Matthew Johnson, Blake A. Hecht-\nman, Laura Weidinger, Iason Gabriel, William S.\nIsaac, Edward Lockhart, Simon Osindero, Laura\nRimell, Chris Dyer, Oriol Vinyals, Kareem Ayoub,\nJeff Stanway, Lorrayne Bennett, Demis Hassabis,\nKoray Kavukcuoglu, and Geoffrey Irving. 2021.\nScaling language models: Methods, analysis & in-\nsights from training gopher. CoRR, abs/2112.11446.\nColin Raffel, Noam Shazeer, Adam Roberts, Katherine\nLee, Sharan Narang, Michael Matena, Yanqi Zhou,\nWei Li, and Peter J Liu. 2020. Exploring the limits\nof transfer learning with a uniﬁed text-to-text trans-\nformer. The Journal of Machine Learning Research\n(JMLR), 21:1–67.\nAnand Rajaraman and Jeffrey David Ullman. 2011.\nMining of massive datasets. Cambridge University\nPress.\nHannah Rashkin, Eric Michael Smith, Margaret Li, and\nY-Lan Boureau. 2019.\nTowards empathetic open-\ndomain conversation models: A new benchmark and\ndataset.\nIn Proceedings of the 57th Annual Meet-\ning of the Association for Computational Linguis-\ntics, pages 5370–5381, Florence, Italy. Association\nfor Computational Linguistics.\nStephen Roller, Emily Dinan, Naman Goyal, Da Ju,\nMary Williamson, Yinhan Liu, Jing Xu, Myle Ott,\nEric Michael Smith, Y-Lan Boureau, and Jason We-\nston. 2021.\nRecipes for building an open-domain\nchatbot. In Proceedings of the 16th Conference of\nthe European Chapter of the Association for Compu-\ntational Linguistics: Main Volume, pages 300–325,\nOnline. Association for Computational Linguistics.\n\nKeisuke Sakaguchi, Ronan Le Bras, Chandra Bhagavat-\nula, and Yejin Choi. 2020. Winogrande: An adver-\nsarial winograd schema challenge at scale. In The\nThirty-Fourth AAAI Conference on Artiﬁcial Intelli-\ngence, AAAI 2020, The Thirty-Second Innovative Ap-\nplications of Artiﬁcial Intelligence Conference, IAAI\n2020, The Tenth AAAI Symposium on Educational\nAdvances in Artiﬁcial Intelligence, EAAI 2020, New\nYork, NY, USA, February 7-12, 2020, pages 8732–\n8740. AAAI Press.\nVictor Sanh, Albert Webson, Colin Raffel, Stephen H.\nBach, Lintang Sutawika, Zaid Alyafeai, Antoine\nChafﬁn, Arnaud Stiegler, Teven Le Scao, Arun Raja,\nManan Dey, M Saiful Bari, Canwen Xu, Urmish\nThakker, Shanya Sharma Sharma, Eliza Szczechla,\nTaewoon Kim, Gunjan Chhablani, Nihal Nayak,\nDebajyoti Datta, Jonathan Chang, Mike Tian-Jian\nJiang, Han Wang, Matteo Manica, Sheng Shen,\nZheng Xin Yong, Harshit Pandey, Rachel Bawden,\nThomas Wang, Trishala Neeraj, Jos Rozen, Ab-\nheesht Sharma, Andrea Santilli, Thibault Fevry, Ja-\nson Alan Fries, Ryan Teehan, Stella Biderman, Leo\nGao, Tali Bers, Thomas Wolf, and Alexander M.\nRush. 2021.\nMultitask prompted training enables\nzero-shot task generalization.\nTeven Le Scao and Alexander M. Rush. 2021. How\nmany data points is a prompt worth?\npages 2627–\n2636.\nTimo Schick and Hinrich Schütze. 2020. It’s not just\nsize that matters: Small language models are also\nfew-shot learners. CoRR, abs/2009.07118.\nTimo Schick, Sahana Udupa, and Hinrich Schütze.\n2021. Self-diagnosis and self-debiasing: A proposal\nfor reducing corpus-based bias in nlp. Transactions\nof the Association for Computational Linguistics,\n9:1408–1424.\nRico Sennrich, Barry Haddow, and Alexandra Birch.\n2016.\nNeural machine translation of rare words\nwith subword units. In Proceedings of the 54th An-\nnual Meeting of the Association for Computational\nLinguistics (Volume 1: Long Papers), pages 1715–\n1725, Berlin, Germany. Association for Computa-\ntional Linguistics.\nEmily Sheng, Kai-Wei Chang, Premkumar Natarajan,\nand Nanyun Peng. 2019. The woman worked as a\nbabysitter: On biases in language generation. arXiv\npreprint arXiv:1909.01326.\nTaylor Shin, Yasaman Razeghi, Robert L. Logan IV,\nEric Wallace, and Sameer Singh. 2020. AutoPrompt:\nEliciting Knowledge from Language Models with\nAutomatically Generated Prompts.\npages 4222–\n4235.\nMohammad Shoeybi, Mostofa Patwary, Raul Puri,\nPatrick LeGresley, Jared Casper, and Bryan Catan-\nzaro. 2019. Megatron-lm: Training multi-billion pa-\nrameter language models using model parallelism.\narXiv preprint arXiv:1909.08053.\nKurt Shuster, Mojtaba Komeili, Leonard Adolphs,\nStephen Roller, Arthur Szlam, and Jason We-\nston. 2022.\nLanguage models that seek for\nknowledge: Modular search & generation for di-\nalogue and prompt completion.\narXiv preprint\narXiv:2203.13224.\nEric Smith, Mary Williamson, Kurt Shuster, Jason We-\nston, and Y-Lan Boureau. 2020. Can you put it all\ntogether: Evaluating conversational agents’ ability\nto blend skills. In Proceedings of the 58th Annual\nMeeting of the Association for Computational Lin-\nguistics. ACL.\nShaden Smith, Mostofa Patwary, Brandon Norick,\nPatrick LeGresley, Samyam Rajbhandari, Jared\nCasper, Zhun Liu, Shrimai Prabhumoye, George\nZerveas, Vijay Korthikanti, Elton Zheng, Rewon\nChild, Reza Yazdani Aminabadi, Julie Bernauer, Xia\nSong, Mohammad Shoeybi, Yuxiong He, Michael\nHouston, Saurabh Tiwary, and Bryan Catanzaro.\n2022.\nUsing deepspeed and megatron to train\nmegatron-turing NLG 530b, A large-scale genera-\ntive language model. CoRR, abs/2201.11990.\nRomal Thoppilan, Daniel De Freitas, Jamie Hall,\nNoam Shazeer, Apoorv Kulshreshtha, Heng-Tze\nCheng, Alicia Jin, Taylor Bos, Leslie Baker, Yu Du,\net al. 2022. Lamda: Language models for dialog\napplications. arXiv preprint arXiv:2201.08239.\nTrieu H. Trinh and Quoc V. Le. 2018.\nA sim-\nple method for commonsense reasoning.\nCoRR,\nabs/1806.02847.\nMegan Ung, Jing Xu, and Y-Lan Boureau. 2021. Safer-\ndialogues: Taking feedback gracefully after conver-\nsational safety failures. ArXiv, abs/2110.07518.\nAshish Vaswani, Noam Shazeer, Niki Parmar, Jakob\nUszkoreit, Llion Jones, Aidan N Gomez, Łukasz\nKaiser, and Illia Polosukhin. 2017. Attention is all\nyou need. In Advances in neural information pro-\ncessing systems.\nAlex Wang,\nYada Pruksachatkun,\nNikita Nangia,\nAmanpreet Singh, Julian Michael, Felix Hill, Omer\nLevy, and Samuel R. Bowman. 2019. SuperGLUE:\nA stickier benchmark for general-purpose language\nunderstanding systems. arXiv preprint 1905.00537.\nAlbert Webson and Ellie Pavlick. 2021. Do prompt-\nbased models really understand the meaning of their\nprompts? arXiv preprint arXiv:2109.01247.\nJason Wei, Maarten Bosma, Vincent Y. Zhao, Kelvin\nGuu, Adams Wei Yu, Brian Lester, Nan Du, An-\ndrew M. Dai, and Quoc V. Le. 2021.\nFinetuned\nlanguage models are zero-shot learners.\nCoRR,\nabs/2109.01652.\nLaura Weidinger, John Mellor, Maribeth Rauh, Conor\nGrifﬁn, Jonathan Uesato, Po-Sen Huang, Myra\nCheng, Mia Glaese, Borja Balle, Atoosa Kasirzadeh,\nZac Kenton, Sasha Brown, Will Hawkins, Tom\nStepleton, Courtney Biles, Abeba Birhane, Julia\n\nHaas, Laura Rimell, Lisa Anne Hendricks, William\nIsaac, Sean Legassick, Geoffrey Irving, and Iason\nGabriel. 2021a.\nEthical and social risks of harm\nfrom language models.\nLaura Weidinger, John Mellor, Maribeth Rauh, Conor\nGrifﬁn, Jonathan Uesato, Po-Sen Huang, Myra\nCheng, Mia Glaese, Borja Balle, Atoosa Kasirzadeh,\net al. 2021b. Ethical and social risks of harm from\nlanguage models. arXiv preprint arXiv:2112.04359.\nSean Welleck, Ilia Kulikov, Stephen Roller, Emily Di-\nnan, Kyunghyun Cho, and Jason Weston. 2020. Neu-\nral text generation with unlikelihood training.\nIn\nInternational Conference on Learning Representa-\ntions.\nCarole-Jean Wu, Ramya Raghavendra, Udit Gupta,\nBilge Acun, Newsha Ardalani, Kiwan Maeng, Glo-\nria Chang, Fiona Aga Behram, James Huang,\nCharles Bai, Michael Gschwind, Anurag Gupta,\nMyle Ott, Anastasia Melnikov, Salvatore Candido,\nDavid Brooks, Geeta Chauhan, Benjamin Lee,\nHsien-Hsin S. Lee, Bugra Akyildiz, Maximilian Ba-\nlandat, Joe Spisak, Ravi Jain, Mike Rabbat, and Kim\nHazelwood. 2022.\nSustainable AI: environmental\nimplications, challenges and opportunities. In Pro-\nceedings of the Conference on Machine Learning\nand Systems.\nJing Xu, Da Ju, Margaret Li, Y-Lan Boureau, Ja-\nson Weston, and Emily Dinan. 2020. Recipes for\nsafety in open-domain chatbots.\narXiv preprint\narXiv:2010.07079.\nJing Xu, Da Ju, Margaret Li, Y-Lan Boureau, Jason We-\nston, and Emily Dinan. 2021a. Bot-adversarial dia-\nlogue for safe conversational agents. In Proceedings\nof the 2021 Conference of the North American Chap-\nter of the Association for Computational Linguistics:\nHuman Language Technologies, pages 2950–2968,\nOnline. Association for Computational Linguistics.\nJing Xu, Arthur Szlam, and Jason Weston. 2021b. Be-\nyond goldﬁsh memory:\nLong-term open-domain\nconversation. arXiv preprint arXiv:2107.07567.\nRowan Zellers, Ari Holtzman, Yonatan Bisk, Ali\nFarhadi, and Yejin Choi. 2019.\nHellaswag: Can\na machine really ﬁnish your sentence?\nIn Pro-\nceedings of the 57th Conference of the Association\nfor Computational Linguistics, ACL 2019, Florence,\nItaly, July 28- August 2, 2019, Volume 1: Long Pa-\npers, pages 4791–4800. Association for Computa-\ntional Linguistics.\nYukun Zhu, Ryan Kiros, Richard S. Zemel, Ruslan\nSalakhutdinov, Raquel Urtasun, Antonio Torralba,\nand Sanja Fidler. 2015. Aligning books and movies:\nTowards story-like visual explanations by watching\nmovies and reading books. CoRR, abs/1506.06724.\n\nA\nAdditional Evaluations\n.\n108\n109\n1010\n1011\n1012\n30\n40\n50\n60\n70\n80\nAccuracy\nHellaSwag\n108\n109\n1010\n1011\n1012\n62.5\n65.0\n67.5\n70.0\n72.5\n75.0\n77.5\n80.0\n82.5\nStoryCloze\n108\n109\n1010\n1011\n1012\n62.5\n65.0\n67.5\n70.0\n72.5\n75.0\n77.5\n80.0\n82.5\nPIQA\n108\n109\n1010\n1011\n1012\n40\n45\n50\n55\n60\n65\n70\nARC (Easy)\n108\n109\n1010\n1011\n1012\n30\n35\n40\n45\n50\nAccuracy\nARC (Challenge)\n108\n109\n1010\n1011\n1012\n30\n35\n40\n45\n50\n55\nOpenBookQA\n108\n109\n1010\n1011\n1012\n50\n55\n60\n65\n70\n75\n80\nWinogrande\n108\n109\n1010\n1011\n1012\n60\n65\n70\n75\n80\n85\n90\nWinograd\n108\n109\n1010\n1011\n1012\n50\n55\n60\n65\n70\n75\n80\n85\nAccuracy\nBoolQ\n108\n109\n1010\n1011\n1012\n0\n10\n20\n30\n40\n50\nCB\n108\n109\n1010\n1011\n1012\n65\n70\n75\n80\n85\n90\nCOPA\n108\n109\n1010\n1011\n1012\n0\n10\n20\n30\n40\n50\n60\nWIC\n108\n109\n1010\n1011\n1012\nParameters\n50\n55\n60\n65\n70\n75\n80\n85\n90\nAccuracy\nWSC\n108\n109\n1010\n1011\n1012\nParameters\n5\n10\n15\n20\n25\nMultiRC\n108\n109\n1010\n1011\n1012\nParameters\n50\n55\n60\n65\n70\nRTE\n108\n109\n1010\n1011\n1012\nParameters\n70\n75\n80\n85\n90\nReCoRD\nOPT\nGPT\nPaLM\nChinchilla\nGopher\nEleuther\nJurassic\nFigure 6: Zero-shot NLP Evaluations. Full evaluations on all 16 NLP tasks, with comparisons where available.\nWe ﬁnd that across most tasks, GPT-3 models and OPT models perform similarly, but some tasks display highly\nerratic behavior.\n\n108\n109\n1010\n1011\n1012\n30\n40\n50\n60\n70\n80\nAccuracy\nHellaSwag\n108\n109\n1010\n1011\n1012\n65\n70\n75\n80\n85\nStoryCloze\n108\n109\n1010\n1011\n1012\n62.5\n65.0\n67.5\n70.0\n72.5\n75.0\n77.5\n80.0\n82.5\nPIQA\n108\n109\n1010\n1011\n1012\n40\n45\n50\n55\n60\n65\n70\n75\nARC (Easy)\n108\n109\n1010\n1011\n1012\n25\n30\n35\n40\n45\n50\nAccuracy\nARC (Challenge)\n108\n109\n1010\n1011\n1012\n35\n40\n45\n50\n55\n60\n65\nOpenBookQA\n108\n109\n1010\n1011\n1012\n50\n55\n60\n65\n70\n75\nWinogrande\n108\n109\n1010\n1011\n1012\n60\n65\n70\n75\n80\n85\n90\nWinograd\n108\n109\n1010\n1011\n1012\n45\n50\n55\n60\n65\n70\n75\nAccuracy\nBoolQ\n108\n109\n1010\n1011\n1012\n0\n20\n40\n60\n80\nCB\n108\n109\n1010\n1011\n1012\n65\n70\n75\n80\n85\n90\nCOPA\n108\n109\n1010\n1011\n1012\n0\n10\n20\n30\n40\n50\nWIC\n108\n109\n1010\n1011\n1012\nParameters\n50\n55\n60\n65\n70\n75\nAccuracy\nWSC\n108\n109\n1010\n1011\n1012\nParameters\n5\n10\n15\n20\n25\n30\nMultiRC\n108\n109\n1010\n1011\n1012\nParameters\n50\n55\n60\n65\n70\nRTE\n108\n109\n1010\n1011\n1012\nParameters\n70\n75\n80\n85\n90\nReCoRD\nShot\n0\n1\n32\nSeries\nOPT\nGPT\nFigure 7: Multishot-shot NLP Evaluations. Full evaluations on all 16 NLP tasks, with comparisons to the\nGPT-3 reported performance. As with zero-shot, performance is roughly similar for most tasks, with some tasks\ndemonstrating erratic behavior.\n\nB\nContributions\nPre-training\n• Initial planning: Susan Zhang\n• Training infrastructure and initial ablations: Naman Goyal, Myle Ott, Stephen Roller, Sam Shleifer,\nSusan Zhang\n• Training efﬁciency: Naman Goyal, Myle Ott, Sam Shleifer\n• Data curation and deduplication: Shuhoi Chen, Myle Ott, Stephen Roller\n• Training and monitoring OPT-175B: Mikel Artetxe, Moya Chen, Naman Goyal, Punit Singh Koura,\nMyle Ott, Sam Shleifer, Kurt Shuster, Daniel Simig, Stephen Roller, Susan Zhang\n• Training 125M–66B baselines: Naman Goyal, Stephen Roller, Susan Zhang\nEvaluations\n• NLP: Xian Li, Xi Victoria Lin, Todor Mihaylov, Stephen Roller, Anjali Sridhar\n• Dialogue: Stephen Roller\n• Responsible AI Evaluations: Punit Singh Koura, Stephen Roller, Tianlu Wang\nPaper writing:\nMoya Chen, Stephen Roller, Luke Zettlemoyer, Susan Zhang\nCode release preparation:\nChristopher Dewan, Susan Zhang\nResponsible AI conduct:\nMona Diab, Susan Zhang\nC\nDatasheet\nWe follow the recommendations of Gebru et al. (2021) and provide a data card for the dataset used to\ntrain the OPT models.\nC.1\nMotivation\n• For what purpose was the dataset created? Was there a speciﬁc task in mind? Was there a\nspeciﬁc gap that needed to be ﬁlled? Please provide a description. The pre-training data for\ntraining the OPT-175B model was created by a union of ﬁve datasets, including three datasets used\nby RoBERTa (Liu et al., 2019b), a subset of the Pile (Gao et al., 2021a), along with the Pushshift.io\nReddit dataset that was developed in (Baumgartner et al., 2020) and processed in (Roller et al., 2021).\nThese purpose of creating this dataset was to pre-train the language model on a broad corpus of text,\nwith emphasis on human-generated text.\n• Who created the dataset (e.g., which team, research group) and on behalf of which entity (e.g.,\ncompany, institution, organization)? Meta AI.\n• Who funded the creation of the dataset? If there is an associated grant, please provide the\nname of the grantor and the grant name and number. Meta AI.\n• Any other comments? No.\n\nC.2\nComposition\n• What do the instances that comprise the dataset represent (e.g., documents, photos, people,\ncountries)? Are there multiple types of instances (e.g., movies, users, and ratings; people and\ninteractions between them; nodes and edges)? Please provide a description. The instances are\ntextual documents. The overall dataset is composed from a union of the following datasets:\n– BookCorpus (Zhu et al., 2015) consists of more than 10K unpublished books\n– CC-Stories (Trinh and Le, 2018) contains a subset of CommonCrawl data ﬁltered to match the\nstory-like style of Winograd schemas\n– The Pile (Gao et al., 2021a) from which the following was included:\n* Pile-CC\n* OpenWebText2\n* USPTO\n* Project Gutenberg\n* OpenSubtitles\n* Wikipedia\n* DM Mathematics\n* HackerNews\n– Pushshift.io Reddit dataset that was developed in Baumgartner et al. (2020) and processed in\nRoller et al. (2021).\n– CCNewsV2 containing an updated version of the English portion of the CommonCrawl News\ndataset that was used in RoBERTa (Liu et al., 2019b)\n• How many instances are there in total (of each type, if appropriate)? The training data contains\n180B tokens corresponding to 800 GB of data.\n• Does the dataset contain all possible instances or is it a sample (not necessarily random) of\ninstances from a larger set? If the dataset is a sample, then what is the larger set? Is the\nsample representative of the larger set (e.g., geographic coverage)? If so, please describe how\nthis representativeness was validated/veriﬁed. If it is not representative of the larger set, please\ndescribe why not (e.g., to cover a more diverse range of instances, because instances were\nwithheld or unavailable). The CC-stories dataset contains a subset of CommonCrawl data ﬁltered\nto match the story-like style of Winograd schemas. The remainder of the dataset was collected from\nthe above sources, reformatted, and deduplicated.\n• What data does each instance consist of? “Raw” data (e.g., unprocessed text or images) or\nfeatures? In either case, please provide a description. Each instance consists of raw text data.\n• Is there a label or target associated with each instance? If so, please provide a description. No.\n• Is any information missing from individual instances? If so, please provide a description,\nexplaining why this information is missing (e.g., because it was unavailable). This does not\ninclude intentionally removed information, but might include, e.g., redacted text. No.\n• Are relationships between individual instances made explicit (e.g., users’ movie ratings, social\nnetwork links)? If so, please describe how these relationships are made explicit. There are no\nexplicit relationships between individual instances.\n• Are there recommended data splits (e.g., training, development/validation, testing)? If so,\nplease provide a description of these splits, explaining the rationale behind them. We hold out\na random validation set of approximately 200MB from the pretraining data, sampled proportionally\nto each dataset’s size in the pretraining corpus.\n\n• Are there any errors, sources of noise, or redundancies in the dataset? If so, please provide a\ndescription. Outside of naturally occurring duplication from potential overlaps between the datasets,\nthere are no other redundancies, errors, or sources of noise that we add.\n• Is the dataset self-contained, or does it link to or otherwise rely on external resources (e.g.,\nwebsites, tweets, other datasets)? It’s self-contained.\n• Does the dataset contain data that, if viewed directly, might be offensive, insulting, threatening,\nor might otherwise cause anxiety? If so, please describe why. Parts of the dataset are a subset of\npublic Common Crawl data, along with a subset of public Reddit data, which could contain sentences\nthat, if viewed directly, might be offensive, insulting, threatening, or might otherwise cause anxiety.\n• Does the dataset relate to people? If not, you may skip the remaining questions in this section.\nSome documents of this data relate to people, such as news articles, Wikipedia descriptions, etc.\n• Does the dataset identify any subpopulations (e.g., by age, gender)? If so, please describe how\nthese subpopulations are identiﬁed and provide a description of their respective distributions\nwithin the dataset. No, the dataset does not explicitly include subpopulation identiﬁcation.\n• Any other comments? No.\nC.3\nCollection Process\n• How was the data associated with each instance acquired? Was the data directly observ-\nable (e.g., raw text, movie ratings), reported by subjects (e.g., survey responses), or indirectly\ninferred/ derived from other data (e.g., part-of-speech tags, model-based guesses for age or\nlanguage)? If data was reported by subjects or indirectly inferred/derived from other data,\nwas the data validated/veriﬁed? If so, please describe how. N/A. The dataset is a union of ﬁve\npublicly available datasets.\n• What mechanisms or procedures were used to collect the data (e.g., hardware apparatus or\nsensor, manual human curation, software program, software API)? How were these mecha-\nnisms or procedures validated? The data was downloaded from the internet.\n• If the dataset is a sample from a larger set, what was the sampling strategy (e.g., deterministic,\nprobabilistic with speciﬁc sampling probabilities)? Please see previous answers for how the\ndataset was created.\n• Who was involved in the data collection process (e.g., students, crowdworkers, contractors)\nand how were they compensated (e.g., how much were crowdworkers paid)? This data is\nmined, ﬁltered and sampled by machines.\n• Over what timeframe was the data collected? Does this timeframe match the creation time-\nframe of the data associated with the instances (e.g., recent crawl of old news articles)? If not,\nplease describe the timeframe in which the data associated with the instances was created. The\nCC-News dataset contains English news articles crawled between September 2016 and September\n2021.\n• Does the dataset relate to people? If not, you may skip the remainder of the questions in this\nsection. No.\n• Did you collect the data from the individuals in question directly, or obtain it via third parties\nor other sources (e.g., websites)? N/A.\n• Were the individuals in question notiﬁed about the data collection? If so, please describe (or\nshow with screenshots or other information) how notice was provided, and provide a link or\nother access point to, or otherwise reproduce, the exact language of the notiﬁcation itself. N/A.\n\n• Did the individuals in question consent to the collection and use of their data? If so, please\ndescribe (or show with screenshots or other information) how consent was requested and pro-\nvided, and provide a link or other access point to, or otherwise reproduce, the exact language\nto which the individuals consented. N/A.\n• If consent was obtained, were the consenting individuals provided with a mechanism to revoke\ntheir consent in the future or for certain uses? If so, please provide a description, as well as a\nlink or other access point to the mechanism (if appropriate). N/A.\n• Has an analysis of the potential impact of the dataset and its use on data subjects (e.g., a\ndata protection impact analysis) been conducted? If so, please provide a description of this\nanalysis, including the outcomes, as well as a link or other access point to any supporting\ndocumentation. Some toxicity and bias evaluations were performed. Please refer to the main\ndocument and the model card for these details.\n• Any other comments? No.\nC.4\nPreprocessing/cleaning/labeling\n• Was any preprocessing/cleaning/labeling of the data done (e.g., discretization or bucketing, to-\nkenization, part-of-speech tagging, SIFT feature extraction, removal of instances, processing\nof missing values)? If so, please provide a description. If not, you may skip the remainder\nof the questions in this section. The component datasets went through standard cleaning and\nre-formatting practices, including removing repetitive/non-informative text like “Chapter One,” or\n“This ebook by Project Gutenberg.”\n• Was the “raw” data saved in addition to the preprocessed/cleaned/labeled data (e.g., to sup-\nport unanticipated future uses)? If so, please provide a link or other access point to the “raw”\ndata. The “raw” component datasets is publicly available in their respective locations (more details\ncan be seen in the respective papers linked in references).\n• Any other comments? No.\nC.5\nUses\n• Has the dataset been used for any tasks already? If so, please provide a description. Yes, this\ndataset was used to pre-train the OPT models.\n• Is there a repository that links to any or all papers or systems that use the dataset? If so,\nplease provide a link or other access point. https://github.com/facebookresearch/\nmetaseq\n• What (other) tasks could the dataset be used for? This data can be used to pre-train language\nmodels, which are foundation to many current and future language tasks.\n• Is there anything about the composition of the dataset or the way it was collected and prepro-\ncessed/cleaned/labeled that might impact future uses? For example, is there anything that a\nfuture user might need to know to avoid uses that could result in unfair treatment of individ-\nuals or groups (e.g., stereotyping, quality of service issues) or other undesirable harms (e.g.,\nﬁnancial harms, legal risks) If so, please provide a description. Is there anything a future user\ncould do to mitigate these undesirable harms? The pipeline for creating this dataset paves a way\nfor building a scalable infrastructure for mining datasets.\n• Are there tasks for which the dataset should not be used? If so, please provide a description.\nNone that we are currently aware of.\n• Any other comments? No.\n\nC.6\nDistribution\n• Will the dataset be distributed to third parties outside of the entity (e.g., company, institution,\norganization) on behalf of which the dataset was created? If so, please provide a description.\nNot at this time.\n• How will the dataset will be distributed (e.g., tarball on website, API, GitHub)? Does the\ndataset have a digital object identiﬁer (DOI)? N/A.\n• When will the dataset be distributed? N/A.\n• Will the dataset be distributed under a copyright or other intellectual property (IP) license,\nand/or under applicable terms of use (ToU)? If so, please describe this license and/or ToU, and\nprovide a link or other access point to, or otherwise reproduce, any relevant licensing terms\nor ToU, as well as any fees associated with these restrictions. N/A.\n• Do any export controls or other regulatory restrictions apply to the dataset or to individual\ninstances? If so, please describe these restrictions, and provide a link or other access point to,\nor otherwise reproduce, any supporting documentation. N/A.\n• Any other comments? No.\nC.7\nMaintenance\n• Who is supporting/hosting/maintaining the dataset? Meta AI.\n• How can the owner/curator/manager of the dataset be contacted (e.g., email address)? Refer\nto the main document.\n• Is there an erratum? If so, please provide a link or other access point. N/A.\n• Will the dataset be updated (e.g., to correct labeling errors, add new instances, delete in-\nstances)? If so, please describe how often, by whom, and how updates will be communicated\nto users (e.g., mailing list, GitHub)? No current plan for updating.\n• If the dataset relates to people, are there applicable limits on the retention of the data as-\nsociated with the instances (e.g., were individuals in question told that their data would be\nretained for a ﬁxed period of time and then deleted)? If so, please describe these limits and\nexplain how they will be enforced. N/A.\n• Will older versions of the dataset continue to be supported/hosted/maintained? If so, please\ndescribe how. If not, please describe how its obsolescence will be communicated to users. N/A.\n• If others want to extend/augment/build on/contribute to the dataset, is there a mechanism\nfor them to do so? If so, please provide a description. Will these contributions be validated/\nveriﬁed? If so, please describe how. If not, why not? Is there a process for communicating/ dis-\ntributing these contributions to other users? If so, please provide a description. No mechanism\nis available right now.\n• Any other comments? No.\nD\nModel Card\nFollowing Mitchell et al. (2018), we provide a model card for OPT-175B.\n\nD.1\nModel Details\n• Person or organization developing model: OPT-175B was developed by Meta AI.\n• Model date: OPT-175B was released on May 3, 2022.\n• Model version: OPT-175B described in this paper is version 1.0.0.\n• Model type: OPT-175B is a large decoder-only transformer language model.\n• Information about training algorithms, parameters, fairness constraints or other applied ap-\nproaches, and features: OPT-175B was trained with AdamW for parameter sizes from 125M to\n175B. See the Data Card (Appendix C) for information about training data and Section 2.2 - 2.5 for\ninformation about the training process.\n• Paper or other resource for more information: See the rest of this paper for more details on\nOPT-175B as well as the corresponding post on the Meta AI Research Blog. More details are also\navailable in metaseq, our open-source repository.12\n• License: OPT-175B and the smaller baseline models are made available through a non-commercial\nuse license agreement provided in our model license.13\n• Where to send questions or comments about the model: Please contact the corresponding authors\n{susanz,roller,namangoyal}@fb.com for any questions or comments.\nD.2\nIntended Use\n• Primary intended uses: We release OPT-175B for research into Language Models, especially as it\npertains to Responsible AI. See Section 6 for more detailed Considerations for Release. Information\non how to use the model can be found at metaseq, our open-source repository.\n• Primary intended users: We primarily target researchers and the related research community.\n• Out-of-scope use cases: OPT-175B is not released for production use or real-world deployments.\nAs we note in Section 5, OPT-175B, like similar large language models, has a variety of shortcomings\nthat make it premature for commercial use.\nD.3\nData, Limitations, and Recommendations\n• Data selection for training: Training data for OPT-175B was selected based on a combination of\nbreadth and availability. See our Data Card (Appendix C) for more detailed information on the data\nused to train our model.\n• Data selection for evaluation: Evaluations in this paper were chosen to provide comparable perfor-\nmance assessments relative to similar scale models in the literature. Given concerns in the community\naround safety and fairness of large language models in general, we also explicitly provide evaluations\non Responsible AI (see Section 4).\n• Limitations: Like other large language models for which the diversity (or lack thereof) of training\ndata induces downstream impact on the quality of our model, OPT-175B has limitations in terms\nof bias and safety. OPT-175B can also have quality issues in terms of generation diversity and\nhallucination. In general, OPT-175B is not immune from the plethora of issues that plague modern\nlarge language models. By releasing with a non-commercial license, we also hope to increase\ncommunication, transparency, and study of the problems of large language models, especially in\nareas which may not be aligned with commercial interests. See Section 5 for a more detailed\ndiscussion of limitations of OPT-175B.\n12https://github.com/facebookresearch/metaseq/\n13https://github.com/facebookresearch/metaseq/blob/main/projects/OPT/MODEL_LICENSE.\nmd\n\n• Recommendations for future work: See Section 6 for more about our Considerations for Release,\nincluding a discussion of potential avenues of research enabled by opening our model to more of\nthe research community. We hope that the release of OPT-175B, as well as information around our\nmodel training process, will increase open science around both large language models in speciﬁc and\nnatural language processing and deep learning in general.\n\nE\nSample Model Outputs\nFor all sample outputs, the initial prompt is given in bold and the remainder is the continuation. These\nexample outputs were intentionally selected to highlight both successes and failures of the OPT-175B\nmodel.\nFigure 8: Poetry generation. We have observed the model can write entertaining poetry on topics such as dodos,\nsamosas, and performance reviews. However, we struggled to get the model to observe rhyme or meter.\nFigure 9: Conversation generation. OPT-175B adopts a patriotic personality when prompted as the Statue of\nLiberty. However, the model also devolves into somewhat simple and linguistically repetitive generations further\ninto the conversation.\n\nFigure 10: Basic few-shot translation example. OPT was not intentionally trained to be multilingual, but we\nfound anecdotally it has limited success with simple translations in German, Spanish, French, and Chinese.\n\nFigure 11: Paper writing example. Prompting with \"1. Introduction\" generally yielded more interesting results\ncompared to prompting with “Abstract.” Our prompt here was inspired by the ﬁrst sentence of the seminal ResNet\nwork (He et al., 2016).\n\nFigure 12: Arithmetic. We observe mistakes when extending from addition to other operations.\n\nFigure 13: Python programming. Simply switching out a variable name can alter the generated output.\n",
      "fetch_method": "direct-pdf"
    },
    {
      "arxiv_id": "https://arxiv.org/abs/2005.14165",
      "full_text": " [2005.14165] Language Models are Few-Shot Learners Skip to main content We gratefully acknowledge support from the Simons Foundation, member institutions , and all contributors. Donate &gt; cs &gt; arXiv:2005.14165 Help | Advanced Search All fields Title Author Abstract Comments Journal reference ACM classification MSC classification Report number arXiv identifier DOI ORCID arXiv author ID Help pages Full text Search open search GO open navigation menu quick links Login Help Pages About --> Computer Science > Computation and Language arXiv:2005.14165 (cs) [Submitted on 28 May 2020 ( v1 ), last revised 22 Jul 2020 (this version, v4)] Title: Language Models are Few-Shot Learners Authors: Tom B. Brown , Benjamin Mann , Nick Ryder , Melanie Subbiah , Jared Kaplan , Prafulla Dhariwal , Arvind Neelakantan , Pranav Shyam , Girish Sastry , Amanda Askell , Sandhini Agarwal , Ariel Herbert-Voss , Gretchen Krueger , Tom Henighan , Rewon Child , Aditya Ramesh , Daniel M. Ziegler , Jeffrey Wu , Clemens Winter , Christopher Hesse , Mark Chen , Eric Sigler , Mateusz Litwin , Scott Gray , Benjamin Chess , Jack Clark , Christopher Berner , Sam McCandlish , Alec Radford , Ilya Sutskever , Dario Amodei View a PDF of the paper titled Language Models are Few-Shot Learners, by Tom B. Brown and 30 other authors View PDF Abstract: Recent work has demonstrated substantial gains on many NLP tasks and benchmarks by pre-training on a large corpus of text followed by fine-tuning on a specific task. While typically task-agnostic in architecture, this method still requires task-specific fine-tuning datasets of thousands or tens of thousands of examples. By contrast, humans can generally perform a new language task from only a few examples or from simple instructions - something which current NLP systems still largely struggle to do. Here we show that scaling up language models greatly improves task-agnostic, few-shot performance, sometimes even reaching competitiveness with prior state-of-the-art fine-tuning approaches. Specifically, we train GPT-3, an autoregressive language model with 175 billion parameters, 10x more than any previous non-sparse language model, and test its performance in the few-shot setting. For all tasks, GPT-3 is applied without any gradient updates or fine-tuning, with tasks and few-shot demonstrations specified purely via text interaction with the model. GPT-3 achieves strong performance on many NLP datasets, including translation, question-answering, and cloze tasks, as well as several tasks that require on-the-fly reasoning or domain adaptation, such as unscrambling words, using a novel word in a sentence, or performing 3-digit arithmetic. At the same time, we also identify some datasets where GPT-3&#39;s few-shot learning still struggles, as well as some datasets where GPT-3 faces methodological issues related to training on large web corpora. Finally, we find that GPT-3 can generate samples of news articles which human evaluators have difficulty distinguishing from articles written by humans. We discuss broader societal impacts of this finding and of GPT-3 in general. Comments: 40+32 pages Subjects: Computation and Language (cs.CL) Cite as: arXiv:2005.14165 [cs.CL] &nbsp; (or arXiv:2005.14165v4 [cs.CL] for this version) &nbsp; https://doi.org/10.48550/arXiv.2005.14165 Focus to learn more arXiv-issued DOI via DataCite Submission history From: Tom B Brown [ view email ] [v1] Thu, 28 May 2020 17:29:03 UTC (6,995 KB) [v2] Mon, 1 Jun 2020 17:08:53 UTC (6,997 KB) [v3] Fri, 5 Jun 2020 02:52:35 UTC (6,998 KB) [v4] Wed, 22 Jul 2020 19:47:17 UTC (6,998 KB) Full-text links: Access Paper: View a PDF of the paper titled Language Models are Few-Shot Learners, by Tom B. Brown and 30 other authors View PDF TeX Source Other Formats view license Current browse context: cs.CL &lt;&nbsp;prev &nbsp; | &nbsp; next&nbsp;&gt; new | recent | 2020-05 Change to browse by: cs References &amp; Citations NASA ADS Google Scholar Semantic Scholar 74 blog links ( what is this? ) DBLP - CS Bibliography listing | bibtex Tom B. Brown Nick Ryder Jared Kaplan Prafulla Dhariwal Arvind Neelakantan &hellip; a export BibTeX citation Loading... BibTeX formatted citation &times; loading... Data provided by: Bookmark Bibliographic Tools Bibliographic and Citation Tools Bibliographic Explorer Toggle Bibliographic Explorer ( What is the Explorer? ) Connected Papers Toggle Connected Papers ( What is Connected Papers? ) Litmaps Toggle Litmaps ( What is Litmaps? ) scite.ai Toggle scite Smart Citations ( What are Smart Citations? ) Code, Data, Media Code, Data and Media Associated with this Article alphaXiv Toggle alphaXiv ( What is alphaXiv? ) Links to Code Toggle CatalyzeX Code Finder for Papers ( What is CatalyzeX? ) DagsHub Toggle DagsHub ( What is DagsHub? ) GotitPub Toggle Gotit.pub ( What is GotitPub? ) Huggingface Toggle Hugging Face ( What is Huggingface? ) Links to Code Toggle Papers with Code ( What is Papers with Code? ) ScienceCast Toggle ScienceCast ( What is ScienceCast? ) Demos Demos Replicate Toggle Replicate ( What is Replicate? ) Spaces Toggle Hugging Face Spaces ( What is Spaces? ) Spaces Toggle TXYZ.AI ( What is TXYZ.AI? ) Related Papers Recommenders and Search Tools Link to Influence Flower Influence Flower ( What are Influence Flowers? ) Core recommender toggle CORE Recommender ( What is CORE? ) Author Venue Institution Topic About arXivLabs arXivLabs: experimental projects with community collaborators arXivLabs is a framework that allows collaborators to develop and share new arXiv features directly on our website. Both individuals and organizations that work with arXivLabs have embraced and accepted our values of openness, community, excellence, and user data privacy. arXiv is committed to these values and only works with partners that adhere to them. Have an idea for a project that will add value for arXiv's community? Learn more about arXivLabs . Which authors of this paper are endorsers? | Disable MathJax ( What is MathJax? ) About Help contact arXiv Click here to contact arXiv Contact subscribe to arXiv mailings Click here to subscribe Subscribe Copyright Privacy Policy Web Accessibility Assistance arXiv Operational Status Get status notifications via email or slack ",
      "fetch_method": "direct-html"
    },
    {
      "arxiv_id": "https://raw.githubusercontent.com/facebookresearch/metaseq/main/projects/OPT-IML/optiml_paper_v1.pdf",
      "full_text": "[February 1, 2023] Please check out our paper on ArXiv:\nhttps://arxiv.org/abs/2212.12017\n1\n",
      "fetch_method": "direct-pdf"
    },
    {
      "arxiv_id": "https://raw.githubusercontent.com/facebookresearch/metaseq/main/projects/OPT/chronicles/OPT175B_Logbook.pdf",
      "full_text": "OPT-175 Logbook\nGoal: Get a 175B dense model up and running by any means necessary.\nPurpose of this document:\nTo provide a source of truth of what we did, when, and why, and any context that was important to those\ndecisions. To provide each other with a clear place to find information about what is happening without having\nto ping.\nInstructions\n-\nAdd a dated entry for each log, in reverse chronological order.\n-\nEntries do not have to correspond to launches, but may include notes.\n-\nHIGHLIGHT IN RED ANYTHING THE NEXT ONCALL SHOULD ABSOLUTELY NOTICE\n-\nFor all launches, include:\n-\nDate\n-\nRemember to update the pointers at the top of this document\n-\nContext of why changes were necessary (Analysis of previous run)\n-\nInclude tensorboard screenshots of spikes or divergences if applicable\n-\nLaunch steps:\n-\nCheckpoint/log folder\n-\nRelevant commits\n-\nPR of a change to sweep script if relevant\n-\nOncall responsibilities are at the bottom of this doc\nSpare Node Tracker\nNOTE: TRY TO KEEP KNOWN GREEN NODES IN IDLE AND DRAIN ALL BAD NODES.\nNodeList\n# nodes\nState\nNotes\n\n175B Log\nNOTE: You don’t need to specify the log file when launching the monitor script, it’ll find it automatically\nbased on the job id.\n-------\n2022-01-06 15:47 ET [Everyone]\n2022-01-06\n15:46:44\n| INFO | fairseq_cli.train\n|\ndone training in\n89691.5 seconds\n2022-01-05 14:10 [Stephen + Mikel]\nTimeline\n1.\nStephen’s devfair was hosting an ssh session running Tensorboard.\na.\nThe devfair was taken offline for yearly maintenance, causing ssh\n2.\nAn unknown person was running monitor.py, and that was also taken offline for an unknown reason.\n3.\nCurrent oncall took action to relaunch monitor.py, which immediately detected a file-not-modified issue\nand attempted to restart the job.\na.\nDue to user error, the wrong train.log file was specified, causing a false positive on the file not\nmodified detector.\nb.\nJob was put on hold\n\nc.\nfixmycloud was run\nd.\nWe reached the part about NCCL tests and silently failed on fixmycloud, and crashing\nmonitor.py. It did attempt to send an email (into the void\nMitigations taken:\n1.\nNoticed 10 nodes were in drain state, mostly from a “kill task failed”\na.\nWe don’t always know whether this indicates a bad node or not.\n2.\nManually undrained and launched fixmycloud.\na.\nThis initially hung because pg0-8 was unresponsive but scheduled a nccl test anyway.\nb.\nManually drained pg0-8 and relaunched fixmycloud\nc.\nTwo nodes were drained for failing tests.\n3.\nManually ran touch train.log on the latest run to ensure time modified would be new\n4.\nManually removed the nodelist via sudo scontrol update job=6848 'NodeList='\n5.\nManually ran sudo scontrol release 6848 to resume the job\n6.\nCurrent oncall ran monitor.py with updated train.log argument and verified stability\nFuture actions required:\n1.\nWe need to add the nccl binaries directly to the repo, and remove the get_nccl_scripts.sh file/check.\n2.\nmonitor.py should log who is running it.\n3.\nmonitor.py should use scontrol show jobid to identify the logfile automatically, rather than require\nmanual specification\n4.\nEmails need to be fixed or another alerting system needs to be found.\n5.\nmonitor.py should probably run via some sort of nohup or as a daemon\n6.\ntensorboard serve should probably run via some sort of nohup\n2022-01-06 10:30ish [Susan + Mikel]\nTimestamps in the logs were in ET time because the last oncall had the TZ environment variable set when\nlaunching the jobs. This made it look like the job was stuck for 5 hours at first glance.\nMitigations taken: patch to ensure logs are always in UTC.\n2022-01-04 15:22 ET [Sam]\nAuto-recovery failed with no email. Restarting from 2021-12-30 09:30 ET [Stephen] - Job recovery failed.\n2022-01-03 05:10 ET [Daniel]\nUpdate 12:47 ET [Stephen]: Trend is continuing. Agree with holding steady.\nTrain and validation ppl still show opposite trends (in the unusual way). Very confused, but based on a previous\ncomment from Stephen in the chat I’ll leave the training untouched and wait for someone to help interpret this\nin the morning.\n\nActivation norm seems to repeat an upward step pattern:\n2022-01-02 17:26 ET [Stephen]\nNoting that we have seen a spike in activation norms and train PPL is trending up\nValidation not really affected:\n\n2021-12-31 02:53 ET [Punit]\nStarted the training monitor script\n./scripts/cloud/monitor_train_log.py --mailto <scrubbed>@fb.com /shared/home/namangoyal/checkpoints/175B/175B_run12.57*/train.log\n--modified-threshold 3600 --slurm-jobid 6214\n2021-12-31 12:00 ET [Moya]\n●\nMonitor script detects train.log not getting updated; tries to autorecover (6 am ET ish)\n●\nAutorecover is successful.\n●\n…however I had two monitor scripts running going to two separate emails (<scrubbed>)\n○\nWhich meant recovery took twice as long. (Up at 9 am ish rather than sooner)\n○\n…and the emails didn’t even send anyway :(\n●\nnode-[5,13] seem to be new drained nodes; undrain them and run ./fixmycloud on them to see what’s\nup… Seems like ssh issue\n○\npdsh@ip-0A1E0404: node-5: ssh exited with exit code 15\n○\npdsh@ip-0A1E0404: node-13: ssh exited with exit code 15\n●\nSSH into the two nodes run `nvidia-smi`\n○\nGet `Unable to determine the device handle for GPU 000B:00:00.0: GPU is lost.  Reboot the\nsystem to recover this GPU` for both\n●\nPut node-[5,13] back into drain mode\n●\n…and while I happened to be updating the log for this, 150 enters into a “connection timed out mode”,\nsame as drained* node.\nhpc*         up   infinite      1 drain* node-82\nhpc*         up   infinite      1  idle* node-150\nhpc*         up   infinite      3  drain node-[5,13,148]\nhpc*         up   infinite      1    mix node-11\nhpc*         up   infinite    124  alloc node-[1-4,6-10,12,14-32,34-81,83-92,94,96-98,100-106,108-116,118-134]\nhpc*         up   infinite     11   idle node-[137-146,149]\n2021-12-30 17:00 ET [Moya] - nvidia_smi.py bug fix; machine check\n●\nSome spares were caught under `fixmycloud` when they shouldn’t have been.\n\n●\nUndrained the relevant nodes (node-[5,18,47,118,120-121,149]) then reran fixmycloud after fixing the\nrelevant bug in nvidia_smi\n●\nNoticed infoROM corruption in one of the nodes and we’ve got enough spares to not need yellows, so\nmarking as drain\n○\n2021-12-30 22:58:27 WARNING  nvidia_smi | node-148: infoROM is corrupted at gpu\n0001:00:00.0\n○\n2021-12-30 22:58:27 WARNING  nvidia_smi | node-148: infoROM is corrupted at gpu\n000E:00:00.0\nCluster stats after the above:\nhpc*         up   infinite      1 drain* node-82           – [2021-12-30 8PM EST] CSP sync - Has wrong IP address.\nCSP looking into, UI issue\nhpc*         up   infinite      1  drain node-148\nhpc*         up   infinite      1    mix node-11\nhpc*         up   infinite    124  alloc\nnode-[1-4,6-10,12-17,19-32,34-46,48-81,83-92,94,96-98,100-106,108-116,119,122-134,137-140]\nhpc*         up   infinite     14   idle node-[5,18,47,118,120-121,141-146,149-150]\n2021-12-30 09:30 ET [Stephen] - Job recovery failed\n●\nWe crashed with some sort of hardware failure\n●\nJob actually got dequeued from slurm before the auto-recovery could hold it\n●\nAs a result, the monitor script crashed and auto-recovery didn’t execute\n●\nNo emails got sent.\n●\nManually resuming. Choosing to bump run ID bc our tensorboards are getting crowded.\nAlso note that I noticed the monitor script fails to auto-lock due to the original run directory being\n0775. I’ve manually changed run directories to be 0777 for now.\nCluster status after fixmycloud:\nhpc*         up   infinite      3 drain* node-[33,47,95]\nhpc*         up   infinite      5  drain node-[82,118,120-121,149]\nhpc*         up   infinite      1    mix node-11\nhpc*         up   infinite    132   idle\nnode-[1-4,6-10,12-17,19-32,34-46,48-81,83-92,94,96-98,100-106,108-116,119,122-134,137-146,148,150]\nSeveral of those drains might be false positives (ECC errors now are over aggressive, catching aggregate\nones rather than recent ones)\n# LAUNCH OF 12.57\n# use updated fairscale\ncd ~/fairscale\ngit fetch origin prefetch_fsdp_params_simple\ngit checkout prefetch_fsdp_params_simple\ncd ~/Megatron-LM\ngit checkout tags/v2.6\ncd ~/fairseq-py\ngit fetch origin gshard_combine_megatron_fsdp\ngit checkout gshard_combine_megatron_fsdp\n\n# use previous blob URL\nBLOB_PREFIX=\"<<<SCRUBBED FOR RELEASE>>>\"\nBLOB_AUTH=\"<<<SCRUBBED FOR RELEASE>>>\"\nRUN_ID=175B_run12.57\n# Use hosts verified good from fixmycloud from above, but don’t manually specify\npython -m fb_sweep.opt.sweep_opt_en_lm_175b \\\n-n 124 -g 8 -t 1 \\\n-p $RUN_ID \\\n--checkpoints-dir /shared/home/namangoyal/checkpoints/175B/ \\\n--local-checkpoints-dir /mnt/scratch/$USER/checkpoints/$(date +%Y-%m-%d).$RUN_ID \\\n--full-cloud-upload-path \"${BLOB_PREFIX}/?${BLOB_AUTH}\"\n./scripts/cloud/monitor_train_log.py --mailto <scrubbed> /shared/home/namangoyal/checkpoints/175B/175B_run12.57*/train.log\n--modified-threshold 3600 --slurm-jobid 6214\n2021-12-29 13:40 ET [Stephen] - Cluster maintenance and relaunch\n●\nObserved WPS drop\n●\nManually paused to run fixmycloud. Found node-95 had terrible NCCL\n●\nDrained and reported\n●\nJob requeued\nLater:\n●\nNoticed a typo in nvidia_smi.py\n●\nFound node-149 had high uncorrectable ECCs after fixing typo\nAlso checked active nodes:\n2021-12-29 18:40:12 CRITICAL nvidia_smi | node-118: ecc high uncorrectables: DRAM Uncorrectable: 1335\ns\n2021-12-29 18:40:12 CRITICAL nvidia_smi | node-121: ecc high uncorrectables: DRAM Uncorrectable: 8\nTook no action. The job is running fine…\n2021-12-28 16:00 PT [Susan] - cluster maintenance\n●\nJob restarted and auto-recovered\n●\n5 nodes in drain:\n(base) susanz@ip-0A1E0404:/shared/home/namangoyal/checkpoints/175B/tensorboard$ sinfo\nPARTITION AVAIL  TIMELIMIT  NODES  STATE NODELIST\nhpc*         up   infinite      1 drain* node-33\nhpc*         up   infinite      5  drain node-[11,47,111,132-133]\nhpc*         up   infinite    124  alloc\nnode-[1-4,6-10,12-17,19-32,34-46,48-92,94-98,100-106,108-110,112-116,118-131,134,137-138]\nhpc*         up   infinite     11   idle node-[139-146,148-150]\n(base) susanz@ip-0A1E0465:~/fairseq-py$ python scripts/cloud/slurm.py summary\nnode-11: Kill task failed [root@2021-12-29T03:08:14]\nnode-47: Kill task failed [root@2021-12-29T03:08:13]\nnode-111: Kill task failed [root@2021-12-29T03:08:13]\nnode-132: Kill task failed [root@2021-12-29T03:08:14]\n\nnode-133: Kill task failed [root@2021-12-29T03:08:13]\n●\nSsh-ing to each:\n○\n11 infoROM corrupted\n○\n47 lost GPU\n○\n111 infoROM corrupted\n○\n132-133 not sure what’s wrong here, undraining and running fixmycloud\n(base) susanz@ip-0A1E0465:~/fairseq-py$ python scripts/cloud/slurm.py undrain node-132\n2021-12-29 04:10:09 WARNING  slurm      | Undraining node-132 because \"No reason given\"\n(base) susanz@ip-0A1E0465:~/fairseq-py$ python scripts/cloud/slurm.py undrain node-133\n2021-12-29 04:10:12 WARNING  slurm      | Undraining node-133 because \"No reason given\"\n●\nFixmycloud shows 132 and 133 as fine. Leaving as idle.\n●\nReported 47 to CSP.\n2021-12-28 9:10 ET [Myle/Stephen] - manually recovered job\n●\nAutorecovery failed due to a permission error on the lock file :/\n○\nFixed in PR #2845\n●\nNOT relaunched monitoring script, will let someone else do it\nStephen:\n●\nI’m the captain now\n●\nRunning fixmycloud & relaunching the monitor script\n●\nReported 136, 117, 19, and 18 to CSP\n●\nLater reported 5 too\nThey gave us back ​node-[19,55,56,91,92].\n2021-12-27 9:25 ET [Myle] - postmortem on autorecovery issue\n●\nIn the past, validation takes ~14 minutes for all subsets\n○\nValidation prints several lines to train.log, so it shouldn’t have triggered the 15 minute timeout\n●\nBased on train.log and monitor.log, it seems like the job genuinely hung in validation, but hanging 3\ntimes in a row during validation seems very suspicious.\nHypotheses:\n1.\nSomething is buffering writes to train.log for the whole validation step, which tripped the 15 minute\nmodified threshold.\na.\nMoving the threshold to 1 hour (--modified-threshold=3600) seems like a good solution in this\ncase.\n2.\nAnecdotally we seem to crash during validation a lot. It'd be good to quantify, but perhaps something in\nour code or dataloader is causing hangs during validation?\na.\nWe should quantify frequency of TERM during validation vs. training\n3.\nIt's possible we did have three bad nodes in a short time period, but what are the odds that they all\nfailed in validation? Even if we did have bad nodes, it's possible there's something in validation that\nmakes validation more likely to break nodes.\n\nSeparately it'd be great to add timestamps to train.stderr somehow, to make it easier to cross-reference\ntrain.log and train.stderr\n2021-12-27 13:19 CET [Mikel] - restarting autorecovery script\n●\nThe autorecovery script has kicked in 3 times in the last few hours, all 3 times during validation\n●\nI suspect it could be because train.log is less verbose during validation and the previous\n--modified-threshold could be too agressive\n●\nKilled the previous autorecovery job and relaunched it with --modified-threshold 3600:\n./scripts/cloud/monitor_train_log.py --mailto <scrubbed>@fb.com\n/shared/home/namangoyal/checkpoints/175B/175B_run12.56*/train.log --modified-threshold 3600\n--slurm-jobid 4136\n2021-12-25  04:18 ET [Myle] - starting improved autorecovery script\n●\nAutorecovery has been made more robust in #2842\n●\nRelaunched with (note that --modified-threshold was originally 900 but has been adjusted in the\ncommand below to 3600): ./scripts/cloud/monitor_train_log.py --mailto <scrubbed>@fb.com\n/shared/home/namangoyal/checkpoints/175B/175B_run12.56*/train.log --modified-threshold 3600\n--slurm-jobid 4136\n●\nOncall can tail with: tail -f /data/users/common/monitor.log\n2021-12-25 09:48 ET [Myle] - RCA on autorecovery failure\nTimeline (all times UTC):\n●\n2021-12-25 09:56: job hangs\n○\nRoot cause is node-19\n■\nStderr message: mlx5: ip-0A1E0444: got completion with error\n■\nscripts/cloud/find_host.py maps ip-0A1E0444 to node-19\n●\n2021-12-25 10:15: monitoring script detects hung job and starts autorecovery\n●\n2021-12-25 10:17: fixmycloud requeues job with new nodelist\n●\n2021-12-25 10:18: requeued job begins\n●\n2021-12-25 10:22: monitoring script thinks auto-recovery was successful, sends email\n●\n2021-12-25 10:23: done with model init\n●\n2021-12-25 10:27: done with blob download, begin fast forwarding dataloader\n●\n2021-12-25 10:42: monitoring script attempts auto-recovery once again\n○\nNote: there was no log message about why it’s launching autorecovery, but it’s because the\ntrain.log had not been modified in the previous 15 min\n●\n2021-12-25 10:45: job is resumed and the cycle repeats\n2021-12-25 08:49 ET [Stephen]\n$ python scripts/cloud/slurm.py summary\nnode-11: infoROM_corrupted [susanz@2021-12-25T11:52:58]\nnode-19: Kill task failed [root@2021-12-25T10:16:13]\nnode-111: infoROM_corrupted [susanz@2021-12-25T11:53:04]\nnode-148: infoROM_corrupted [susanz@2021-12-25T11:53:09]\n\n$ ssh node-19\n$ nvidia-smi\nUnable to determine the device handle for GPU 0001:00:00.0: GPU is lost.  Reboot the system to recover this\nGPU\n$ sudo reboot\nContext: CSP had asked us if we had tried rebooting to fix this error recently. Giving it a shot.\nThis node seems to NOT BE COMING BACK.\nPlease HOLD ON TO IT FOR CSP.\nUpdated node list.\n2021-12-25 06:25 ET: [Daniel/Susan]\nAuto-recovery script in action. monitor_train_log.py managed to send warning email about progress but then\nmy instance crashed with :\nPermissionError: [Errno 13] Permission denied:\n'/shared/home/namangoyal/checkpoints/175B/175B_run12.56.me_fp16.minscale0.25.fsdp.gpf32.0.relu.transfor\nmer_lm_megatron.nlay96.emb12288.lrnpos.0emb_scale.bm_none.tps2048.gpt2.adam.b2_0.95.eps1e-08.cl0.3\n.lr3e-05.endlr6e-06.wu2000.dr0.1.atdr0.1.0emb_dr.wd0.1.ms8.uf1.mu143052.s1.ngpu992/train.log.autorecover\n_lock'\nMonitoring the restart: the training seems to have gotten terminated by something. My first guess is that the\nmonitor script is misbehaving, I’m killing all instances\nps aux | grep monitor\nsudo kill 43600\n[Susan butting in]\n●\nPut job in requeue / held state\n●\nRan fixmycloud idle\n●\nPut the infoROM corrupted nodes in drain: 11, 111, 148\n●\nUpdated nodelist:\nsudo scontrol update job=4136\nNodeList=node-[1-10,12-18,20-32,34-54,56-90,94-98,100-106,108-110,112-131,136-138]\n●\nReleased the job\n2021-12-24 12:40 ET: Auto-recovery script\n●\nAfter merging #2837 it is now possible to auto-recover from node failures or hung jobs\n○\nIn the case of auto-recovery, the oncall will get a sequence of emails:\n○\nEmail #1: File not modified in 900 seconds\n○\nEmail #2: Detected hang, auto-recovery in progress\n○\nEmail #3: Auto-recovery was apparently successful\n■\nThis last email should contain the last few train.log lines\n●\nSingle pass of updating oncall docs with auto-recovery information.\n\n●\nNote: it is fine for multiple people to run this command, since there is a locking mechanism to prevent\nmultiple scripts from auto-recovering the same job\n●\nInvocation:\n./scripts/cloud/monitor_train_log.py --mailto <scrubbed>@fb.com\n/shared/home/namangoyal/checkpoints/175B/175B_run12.56*/train.log --modified-threshold 900\n--slurm-jobid 4136\n2021-12-24 10:00 ET: Kurt\n●\nChecking status of idle nodes via fixmycloud\n○\nStill getting infoROM warnings on 148\n○\nEverything else passes\n●\nUpdated the node tracker table above.\n2021-12-23 6:00 PM ET: Kurt\n●\nChecking status of idle nodes via fixmycloud:\n○\n2021-12-23 23:06:23 WARNING  nvidia_smi | node-148: infoROM is corrupted at gpu 0001:00:00.0\n○\n2021-12-23 23:06:23 WARNING  nvidia_smi | node-148: infoROM is corrupted at gpu 000E:00:00.0\n○\nEverything else seems OK\n2021-12-23 8:30 ET: Myle\n●\nFollowups for CSP:\n○\nnode-107 died in the night\n○\nWhy was node-55 added to the cluster in an unhealthy state (slow NCCL)?\n●\nJob died ~30 min ago:\n○\nsrun: error: Node failure on node-107\n○\nJob actually died, so will need to manually relaunch\n●\nSeeing 133 idle hosts, running fixmycloud.py to confirm they are healthy\n○\nFound 3 hosts with bad NCCL:\n■\nnode-55: max bandwidth 147.82 below threshold 180\n●\nThis was “Off” last night, so CSP must have added it to the cluster overnight (and\nit’s bad)\n■\nnode-91: max bandwidth 49.03 below threshold 180\n●\nFrom notes, this was bad last night too\n■\nnode-92: max bandwidth 134.91 below threshold 180\n●\nFrom notes, this was bad last night too\n○\nDown to 130 healthy nodes\n■\nnode-[33,145-150] are idle/healthy after relaunching job\n# LAUNCH OF 12.56\n# use updated fairscale\ncd ~/fairscale\ngit fetch origin prefetch_fsdp_params_simple\ngit checkout prefetch_fsdp_params_simple\ncd ~/Megatron-LM\ngit checkout tags/v2.6\ncd ~/fairseq-py\ngit fetch origin gshard_combine_megatron_fsdp\n\ngit checkout gshard_combine_megatron_fsdp\n# use previous blob URL\nBLOB_PREFIX=\"<<<SCRUBBED FOR RELEASE>>>\"\nBLOB_AUTH=\"<<<SCRUBBED FOR RELEASE>>>\"\n# Use `checkpoint_33_98000` which was the last successful checkpoint\nRESTORE_FILE=\"${BLOB_PREFIX}/checkpoint_33_98000.pt?${BLOB_AUTH}\"\nRUN_ID=175B_run12.56\n# Use hosts verified good from fixmycloud from above\nINCLUDED_HOSTS=node-[1-6,8-32,34-54,56-68,70-73,76,78-79,81-84,88-90,96,98,100-106,108-114,116-144] \\\npython -m fb_sweep.opt.sweep_opt_en_lm_175b \\\n-n 124 -g 8 -t 1 \\\n-p $RUN_ID \\\n--checkpoints-dir /shared/home/namangoyal/checkpoints/175B/ \\\n--local-checkpoints-dir /mnt/scratch/$USER/checkpoints/$(date +%Y-%m-%d).$RUN_ID \\\n--restore-file $RESTORE_FILE \\\n--full-cloud-upload-path \"${BLOB_PREFIX}/?${BLOB_AUTH}\"\n./scripts/cloud/monitor_train_log.py --mailto <scrubbed> /shared/home/namangoyal/checkpoints/175B/175B_run12.56*/train.log\n--modified-threshold 900\n2021-12-23 7:00 ET: Stephen\nManually drained/undrained nodes to mark the correspondence to what CSP reports. Ran update_hosts. We\nstill see some issues with slurm not forgetting the IP address of nodes that went down.\n2021-12-22 3:30 pm ET: [Myle] new oncall\n●\nCurrent status:\n○\nJob seems healthy\n○\n3 healthy spare nodes (node-[33,96,101])\n○\n6 nodes in drain\n■\nnode-[55,124]\n●\n“Off” according to cloud UI\n■\nnode-[91,92,135,147]\n●\n“Ready” according to cloud UI\n●\ndrained with reason Bad_infiniband\n○\nTarget:\n■\nIncrease to 7 nodes tonight\n■\nTomorrow morning (coordinate on chat)\n●\nRelease 5 nodes back to CSP\n●\nThey will take a few hours to make this healthy\n■\nBy end of day tomorrow 12 healthy nodes\n○\nRCA\n■\nNot sure about root cause\n■\nCould be “PCIe training”\n■\nRebooting will cycle some systems, but not everything; can try it, but unlikely for it to\nwork\n■\nCSP pre-flight tests didn’t cover multi-node tests previously\n●\nCSP has now recently added these to their preflight tests\n●\n+ CSP has been able to repro the poor NCCL test results\n●\nNote from Stephen (shared with CSP)\n\n○\nBeing very explicit to sync both sides.\nOkay I confirmed we have the new 4 nodes, and all 4 still fail our NCCL\ntests, but are on standby as emergency replacements (at a 20% slowdown,\nbut that’s better than 100%)\n33 and 101 are yellow for very high correctable ECC failures, as discussed\nearlier in chat. They are our current main backups.\n124 and 55 and still showing some slurm weirdness and are absolute no gos.\nThose nodes are down and I don’t know why slurm is confused.\nThanks everyone.\n2021-12-21 4:30 pm ET: [Moya] Kick off train 12.55\nRan fixmycloud on\nnode-[1-6,8-32,34-54,56-68,70-73,78-79,81-84,88-90,100,102-114,116-123,125-134,136-146,148-150]\nSince per thread those were the ones Stephen last used\n●\nTodo - make error message on NCCL print and not just say “exit code 1”\n●\n`please run /shared/home/mpchen/fairseq-py/scripts/cloud/nccl_tests/get_nccl_tests.sh first` was the\nerror it ate\nInfoROM + ECC are yellows; have to run with anyway (cause this is 126 hosts, per conversing offline with\nStephen)\n# LAUNCH OF 12.55\n# use updated fairscale\ncd ~/fairscale\ngit fetch origin prefetch_fsdp_params_simple\ngit checkout prefetch_fsdp_params_simple\ncd ~/Megatron-LM\ngit checkout tags/v2.6\ncd ~/fairseq-py\ngit fetch origin gshard_combine_megatron_fsdp\ngit checkout gshard_combine_megatron_fsdp\n# use new blob URL\nOLD_BLOB_PREFIX=\"<<<SCRUBBED FOR RELEASE>>>\"\nNEW_BLOB_PREFIX=\"<<<SCRUBBED FOR RELEASE>>>\"\"\nBLOB_AUTH=\"<<<SCRUBBED FOR RELEASE>>>\"\n# Use `checkpoint_31_92000` per comment in thread of that being how far things got to\n\nRESTORE_FILE=\"${OLD_BLOB_PREFIX}/checkpoint_31_92000.pt?${BLOB_AUTH}\"\nRUN_ID=175B_run12.55\n# Use hosts verified good from fixmycloud from above\nINCLUDED_HOSTS=node-[1-6,8-32,34-54,56-68,70-73,78-79,81-84,88-90,100,102-114,116-123,125-134,136-146,148-150] \\\npython -m fb_sweep.opt.sweep_opt_en_lm_175b \\\n-n 124 -g 8 -t 1 \\\n-p $RUN_ID \\\n--checkpoints-dir /shared/home/namangoyal/checkpoints/175B/ \\\n--local-checkpoints-dir /mnt/scratch/$USER/checkpoints/$(date +%Y-%m-%d).$RUN_ID \\\n--restore-file $RESTORE_FILE \\\n--full-cloud-upload-path \"${NEW_BLOB_PREFIX}/?${BLOB_AUTH}\"\n2021-12-21 (morning until 3 pm ish) [Stephen] Omicron Sev\nCSP fat fingered and deleted our entire cluster when trying to replenish our buffer nodes.\nAt 14:25 ET they asked us to initiate our preflight checks\nUpon reallocation we had:\n2021-12-21 19:00:04 ERROR updatehost | Nodes node-42 and node-91 have same IP (10.30.4.95)\n2021-12-21 19:00:04 ERROR updatehost | Nodes node-90 and node-92 have same IP (10.30.4.248)\n2021-12-21 19:00:04 ERROR updatehost | Nodes node-9 and node-115 have same IP (10.30.4.23)\n2021-12-21 19:00:04 ERROR updatehost | Nodes node-96 and node-135 have same IP (10.30.4.45)\n2021-12-21 19:00:04 ERROR updatehost | Nodes node-99 and node-147 have same IP (10.30.4.60)\nThis was corrected by CSP. According to CSP, this is lag until slurm notices, but I repeated this procedure\nseveral times over at least 10 minutes.\n92 and 115 also seemed unreachable but slurm wasn’t having them fail their heartbeat. Manually marked as\ndrained\nRan healthchecks:\n2021-12-21 19:36:00 WARNING nvidia_smi | node-33: ecc high correctables: DRAM Correctable: 170435\n2021-12-21 19:36:00 WARNING nvidia_smi | node-65: ecc high correctables: DRAM Correctable: 37642\n2021-12-21 19:36:00 WARNING nvidia_smi | node-101: ecc high correctables: DRAM Correctable:\n1700812021-12-21 19:36:21 WARNING nvidia_smi | node-11: infoROM is corrupted at gpu 000B:00:00.0\n2021-12-21 19:36:21 WARNING nvidia_smi | node-93: infoROM is corrupted at gpu 0004:00:00.0\n2021-12-21 19:36:21 WARNING nvidia_smi | node-93: infoROM is corrupted at gpu 000E:00:00.0\n2021-12-21 19:36:21 WARNING nvidia_smi | node-107: infoROM is corrupted at gpu 0001:00:00.0\n2021-12-21 19:36:21 WARNING nvidia_smi | node-111: infoROM is corrupted at gpu 0002:00:00.0\n2021-12-21 19:36:21 WARNING nvidia_smi | node-148: infoROM is corrupted at gpu 0001:00:00.0\n2021-12-21 19:36:21 WARNING nvidia_smi | node-148: infoROM is corrupted at gpu 000E:00:00.0\n2021-12-21 19:39:23 ERROR nccl | node-7: max bandwidth 150.11 below threshold 180\n2021-12-21 19:39:23 ERROR nccl | node-69: max bandwidth 143.8 below threshold 180\n2021-12-21 19:39:23 ERROR nccl | node-74: max bandwidth 144.8 below threshold 180\n2021-12-21 19:39:23 ERROR nccl | node-75: max bandwidth 145.35 below threshold 180\n2021-12-21 19:39:23 ERROR nccl | node-76: max bandwidth 145.35 below threshold 180\n\n2021-12-21 19:39:23 ERROR nccl | node-77: max bandwidth 145.15 below threshold 180\n2021-12-21 19:39:23 ERROR nccl | node-80: max bandwidth 145.67 below threshold 180\n2021-12-21 19:39:23 ERROR nccl | node-85: max bandwidth 149.82 below threshold 180\n2021-12-21 19:39:23 ERROR nccl | node-86: max bandwidth 149.54 below threshold 180\n2021-12-21 19:39:23 ERROR nccl | node-87: max bandwidth 144.02 below threshold 180\n2021-12-21 19:39:23 ERROR nccl | node-93: max bandwidth 144.57 below threshold 180\n2021-12-21 19:39:23 ERROR nccl | node-94: max bandwidth 140.92 below threshold 180\n2021-12-21 19:39:23 ERROR nccl | node-95: max bandwidth 146.5 below threshold 180\n2021-12-21 19:39:23 ERROR nccl | node-97: max bandwidth 146.5 below threshold 180\n2021-12-21 19:39:23 ERROR nccl | node-98: max bandwidth 143.02 below threshold 180\n2021-12-21 19:39:23 ERROR nccl | node-124: max bandwidth 148.34 below threshold 180\nReplicated the list of bad nccl nodes with multiple tries.\nManually drained pg0-33 and pg0-105 as they had highest correctables.\nThen I requeued susan’s job (which had automatically been put on hold when nodes went down) with\nsudo scontrol hold job=3129\nsudo scontrol update job=3129\n'NodeList=node-[1-6,8-32,34-54,56-68,70-73,78-79,81-84,88-90,100,102-114,116-123,125-134,136-146,148-150]'\nsudo scontrol release 3129\n2021-12-21 5:30am ET: [Susan] Node down, restart from 91,250 with lower LR - Run\n12.53, 12.54\n●\nRunning healthcheck: python scripts/cloud/fixmycloud.py idle\n○\nChecked node-81:\n■\nUnable to determine the device handle for GPU 0001:00:00.0: GPU is lost.  Reboot the\nsystem to recover this GPU\n●\nPR to lower LR: #2833\n○\n12.53 crashed with tokenization error, reverted the cache tokenization change to resume and\ndebug later\n○\nLuckily 91250 is early in a shard-epoch. Took only ~5 min for data loading to finish.\n# LAUNCH OF 12.54\n# use updated fairscale\ncd ~/src/fairscale\ngit fetch origin prefetch_fsdp_params_simple\ngit checkout prefetch_fsdp_params_simple\ncd ~/src/Megatron-LM\ngit checkout tags/v2.6\ncd ~/src/fairseq-py\ngit fetch origin susan/run12.53\ngit checkout susan/run12.53\n# use new blob URL\nBLOB_PREFIX=\"<<<SCRUBBED FOR RELEASE>>>\"\n\nBLOB_AUTH=\"<<<SCRUBBED FOR RELEASE>>>\"\n# confirm epoch of checkpoint from previous train.log - we checkpoint every 250 steps\nRESTORE_FILE=\"${BLOB_PREFIX}/checkpoint_31_91250.pt?${BLOB_AUTH}\"\nRUN_ID=175B_run12.54\nINCLUDED_HOSTS=node-[1-2,4-22,24-25,27-52,54-67,69-80,82-84,86-87,89-96,98-104,106-107,109-112,115-117,119-\n135,147-148,150] \\\npython -m fb_sweep.opt.sweep_opt_en_lm_175b \\\n-n 124 -g 8 -t 1 \\\n-p $RUN_ID \\\n--checkpoints-dir /shared/home/namangoyal/checkpoints/175B/ \\\n--local-checkpoints-dir /mnt/scratch/$USER/checkpoints/$(date +%Y-%m-%d).$RUN_ID \\\n--restore-file $RESTORE_FILE \\\n--full-cloud-upload-path \"${BLOB_PREFIX}/?${BLOB_AUTH}\"\n2021-12-19 12pm ET: Crossing the epoch boundary\n●\nSurprisingly uneventful, no large drop in training ppl\n●\nRan this command to make a backup of the epoch 1 checkpoint:\ncp --recursive --include-pattern \"checkpoint_last*.pt\" <<<SCRUBBED FOR RELEASE>>>\n●\nUploaded a version of the epoch 1 checkpoint without optimizer state here:\n/opt/backups/175B/checkpoint1_eval/\n○\nIt’s also located here on Cloud: /data/175B_checkpoints/checkpoint1_eval\n2021-12-20 12:12 AM PT: [Punit] Node down - requeue for 12.52a\n(fairseq-20210913-py38) punitkoura@ip-0A1E0404:~/src/fairseq-py$ tail\n/shared/home/namangoyal/checkpoints/175B/175B_run12.52.me_fp16.minscale0.25.fsdp.gpf32.0.relu.tran\nsformer_lm_megatron.nlay96.emb12288.lrnpos.0emb_scale.bm_none.tps2048.gpt2.adam.b2_0.95.eps1e-08.c\nl0.3.lr4.5e-05.endlr6e-06.wu2000.dr0.1.atdr0.1.0emb_dr.wd0.1.ms8.uf1.mu143052.s1.ngpu992/train.log\nbuo1u00001Z:46592:47116 [0] NCCL INFO include/net.h:28 -> 2\nbuo1u00001Z:46592:47116 [0] NCCL INFO transport/net.cc:491 -> 2\nbuo1u00001Z:46592:47116 [0] NCCL INFO proxy.cc:351 -> 2\nbuo1u00001Z:46592:47116 [0] NCCL INFO proxy.cc:452 -> 2 [Proxy Thread]\nbuo1u00001Z:46597:47109 [0] ib_plugin.c:670 NCCL WARN NET/IB : Got completion with error 11,\nopcode 32722, len 0, vendor err 137\nbuo1u00001Z:46597:47109 [0] NCCL INFO include/net.h:28 -> 2\nbuo1u00001Z:46597:47109 [0] NCCL INFO transport/net.cc:491 -> 2\nbuo1u00001Z:46597:47109 [0] NCCL INFO proxy.cc:351 -> 2\nbuo1u00001Z:46597:47109 [0] NCCL INFO proxy.cc:452 -> 2 [Proxy Thread]\n(fairseq-20210913-py38) punitkoura@ip-0A1E0404:~/src/fairseq-py$\nNode failure for buo1u00001Z\n\n(fairseq-20210913-py38) punitkoura@ip-0A1E0404:~/src/fairseq-py$ python scripts/cloud/find_host.py\nbuo1u00001Z\nnode-68\n(fairseq-20210913-py38) punitkoura@ip-0A1E0404:~/src/fairseq-py$\nnode-68 is the problematic node.\n(fairseq-20210913-py38) punitkoura@ip-0A1E0404:~/src/fairseq-py$ squeue\n<scrubbed>\n(fairseq-20210913-py38) punitkoura@ip-0A1E0404:~/src/fairseq-py$\nThe current job (2606) has\nnode-[1-2,4-25,27-36,38-52,54-72,74-87,89-96,98-104,106-107,109-113,115-131,147-148,150]\nAs the node list.\nWe need to swap out 68.\nChecking current idle nodes\n(fairseq-20210913-py38) punitkoura@ip-0A1E0404:~/src/fairseq-py$ sinfo\nPARTITION AVAIL  TIMELIMIT  NODES  STATE NODELIST\nhpc*         up   infinite      1 drain~ node-146\nhpc*         up   infinite      7 drain* node-[26,53,88,97,105,114,149]\nhpc*         up   infinite      1  down* node-108\nhpc*         up   infinite      2    mix node-[3,37]\nhpc*         up   infinite    132  alloc\nnode-[1-2,4-25,27-36,38-52,54-72,74-87,89-96,98-104,106-107,109-113,115-135,139-142,147-148,150]\nhpc*         up   infinite      7   idle node-[73,136-138,143-145]\n(fairseq-20210913-py38) punitkoura@ip-0A1E0404:~/src/fairseq-py$\nPotential new host list (adding 143)\nnode-[1-2,4-25,27-36,38-52,54-67,69-72,74-87,89-96,98-104,106-107,109-113,115-131,143,147-148,150]\nRequeue + hold to pause the ongoing training job.\nsudo scontrol requeue job=2606\nsudo scontrol hold job=2606\nNote that the job must be paused before fixmycloud, since NCCL tests require that. See Performing health\nchecks section.\nRunning fixmycloud to check if the proposed new node list is healthy\n(fairseq-20210913-py38) punitkoura@ip-0A1E0404:~/src/fairseq-py$ python scripts/cloud/fixmycloud.py --hosts\nnode-[1-2,4-25,27-36,38-52,54-67,69-72,74-87,89-96,98-104,106-107,109-113,115-131,143,147-148,150]\nLooks like ECC, GPU burn tests etc passed. NCCL tests didn’t complete properly.\nNCCL failures investigation\nRe-ran the failing NCCL command, turns out there was another command which is to be run before the tests\ncan go through.\n\nmissing libnccl and all_reduce_perf; please run\n/shared/home/punitkoura/src/fairseq-py/scripts/cloud/nccl_tests/get_nccl_tests.sh\nfirst!\nAfter running the above command, fixmycloud worked fine. No errors found in the host list.\nResuming job with new host list\nWith confirmation from fixmycloud, the next step is to resume the job with a new host list.\nsudo scontrol update job=2606\nNodelist=node-[1-2,4-25,27-36,38-52,54-67,69-72,74-87,89-96,98-104,106-107,109-113,115-131,143,147-148,150]\nsudo scontrol release job=2606\nTrain.log seems to be working again.\nEnable tensorboard\n​cd /shared/home/namangoyal/checkpoints/175B/tensorboard\nsudo ln -s\n/shared/home/namangoyal/checkpoints/175B/175B_run12.52.me_fp16.minscale0.25.fsdp.gpf32.0.relu.transformer_l\nm_megatron.nlay96.emb12288.lrnpos.0emb_scale.bm_none.tps2048.gpt2.adam.b2_0.95.eps1e-08.cl0.3.lr4.5e-05.end\nlr6e-06.wu2000.dr0.1.atdr0.1.0emb_dr.wd0.1.ms8.uf1.mu143052.s1.ngpu992/tbB run12.52b\nVerified that 12.52b shows up on the tensorboard.\n2021-12-17 15:34 ET: [Daniel] Node down - requeue for 12.52a\nmlx5: buo1u00002X: got completion with error:\n00000000 00000000 00000000 00000000\n00000000 00000000 00000000 00000000\n00000010 00000000 00000000 00000000\n00000000 00008914 10001d2a f7d20ad3\npython scripts/cloud/find_host.py buo1u00002X → node-105\nsudo scontrol requeue job=2606\nsudo scontrol hold job=2606\n# Try swapping in 25:\nsudo scontrol update job=2606 NodeList=node-[1-2,4-25,27-36,38-52,54-72,74-104,106, 107,109-113,115-131,147]\nsudo scontrol release job=2606\n# Turns out 88 and 97 are drained\nCheck that the proposed nodelist is healthy:\npython scripts/cloud/fixmycloud.py --hosts\nnode-[1-2,4-25,27-36,38-52,54-72,74-87,89-96,98-104,106,107,109-113,115-131,147-148,150]\n\n# these two commands together pause the job\nsudo scontrol requeue job=2606\nsudo scontrol hold job=2606\nsudo scontrol update job=2606\nNodeList=node-[1-2,4-25,27-36,38-52,54-72,74-87,89-96,98-104,106,107,109-113,115-131,147-148,150]\nsudo scontrol release job=2606\nDiagnostics on original bad node:\nssh node-105\n(base) danielsimig@buo1u00002X:~/fairseq-py$ python scripts/cloud/gather_diagnostics.py\nDiagnostics uploaded to: <<<SCRUBBED FOR RELEASE>>>\nRe-enable tensorboard\ncd /shared/home/namangoyal/checkpoints/175B/tensorboard\nsudo ln -s\n/shared/home/namangoyal/checkpoints/175B/175B_run12.52.me_fp16.minscale0.25.fsdp.gpf32.0.relu.transfor\nmer_lm_megatron.nlay96.emb12288.lrnpos.0emb_scale.bm_none.tps2048.gpt2.adam.b2_0.95.eps1e-08.cl0.3\n.lr4.5e-05.endlr6e-06.wu2000.dr0.1.atdr0.1.0emb_dr.wd0.1.ms8.uf1.mu143052.s1.ngpu992/tbA\n175B_run12.52a\n2021-12-16 12:15 ET: increased job time limit from 3 days to unlimited:\nsudo scontrol update job=2606 TimeLimit=UNLIMITED\n2021-12-14 18:30 ET: [Kurt] Drain a few nodes\nRan `python scripts/cloud/fixmycloud.py idle`, and found that two nodes broke on NCCL errors (53, 108); see\nentry for (2021-12-09 16:00 PT) where they were marked to be drained\nDrained the nodes:\npython scripts/cloud/slurm.py drain node-53 --reason=\"failing nccl tests\"\n2021-12-14 23:34:15 WARNING  slurm      | Draining node-53 because \"failing nccl tests\"\npython scripts/cloud/slurm.py drain node-108 --reason=\"failing nccl tests\"\n2021-12-14 23:34:26 WARNING  slurm      | Draining node-108 because \"failing nccl tests\"\n2021-12-14 13:30 ET: [Moya] Scancel + Resubmit\nAs decided in conversation, we restart at 72750 steps in order to lower the LR such that we’re far away from a\nnew shard. (Though, maybe could’ve done it an hour earlier as per where `loading train data` showed up in the\nlogs.)\nWhile running ran into a\nbuo1u000088:16674:16674 [1] init.cc:988 NCCL WARN Cuda failure 'uncorrectable\nNVLink error detected during the execution'\nIn the stdlog with \"Please install the megatron submodule\" in the stderr (despite having megatron installed)\n\nWhich was fixed by\ngit submodule update --init fairseq/model_parallel/megatron\nAlso took way too long to realize that it’s completely kosher just outright copy/pasting BLOB_PREFIX and\nBLOB_AUTH from the previous runs… but I blame the peanut gallery. :P\nsqueue # Get id of the 175b run; also for the \"INCLUDED_HOSTS\" command below, to use recently determined cleaned hosts\nscancel 2589 # The id of the 175 b run\n## BEFORE: Change sweep_opt_en_lm_175b to 4.5 (+ commit diff as such)\n#######################\n# LAUNCH OF 12.52\n# use updated fairscale\ncd ~/fairscale\ngit fetch origin prefetch_fsdp_params_simple\ngit checkout prefetch_fsdp_params_simple\ncd ~/Megatron-LM\ngit fetch --tags && git checkout v2.6 # more verbose in case you haven't already fetched the tags\ncd ~/fairseq-py\ngit fetch origin gshard_combine_megatron_fsdp\ngit checkout gshard_combine_megatron_fsdp\nBLOB_PREFIX=\"<<<SCRUBBED FOR RELEASE>>>\"\nBLOB_AUTH=\"<<<SCRUBBED FOR RELEASE>>>\"\nRUN_ID=175B_run12.52 # New id to run\n# Uses included hosts from currently existing run (ie, copy/pasted from squeue prior to cancelling before)\nINCLUDED_HOSTS=node-[1-24,27-36,38-52,54-72,74-87,89-107,109-113,115-131,147] \\\npython -m fb_sweep.opt.sweep_opt_en_lm_175b \\\n-n 124 -g 8 -t 1 \\\n-p $RUN_ID \\\n--checkpoints-dir /shared/home/namangoyal/checkpoints/175B/ \\\n--local-checkpoints-dir /mnt/scratch/$USER/checkpoints/$(date +%Y-%m-%d).$RUN_ID \\\n--full-cloud-upload-path \"${BLOB_PREFIX}/?${BLOB_AUTH}\"\nAlso don’t forget to run the command to update the tensorboard after all of this! (As an aside, I think someone\nelse might’ve run this after I tried to since I forgot to do `sudo` when I ran it, but it’s something a la)\ncd /shared/home/namangoyal/checkpoints/175B/tensorboard\nsudo ln -s <dir of new run in /shared/home/namangoyal/checkpoints/175B/$RUNID...> $RUNID\n2021-12-14 03:30 ET: [Susan] Requeue\n●\nPrevious run crashed with CUDA launch failure again:\n○\nRuntimeError: CUDA error: unspecified launch failure\n●\nRan ecc error check:\n○\nWCOLL=~myleott/hosts PDSH_RCMD_TYPE=ssh pdsh nvidia-smi -q -d \"ECC\" > ecc_error.log\nSaw:\npdsh@ip-0A1E0404: node-88: ssh exited with exit code 15\nSSH’ed over to node-88:\n(base) susanz@buo1u00004D:~$ nvidia-smi\n\nUnable to determine the device handle for GPU 000C:00:00.0: GPU is lost.  Reboot the system to recover this\nGPU\n●\nSwap in 3 in for 88\nsudo scontrol requeue job=2589\nsudo scontrol hold job=2589\nsudo scontrol update job=2589 NodeList=node-[1-24,27-36,38-52,54-72,74-87,89-107,109-113,115-131,147]\nsudo scontrol release job=2589\nNOTE: “2021-12-13: Preemptive Plan” for changing LR still has not occurred yet\nInitiated replace_node on node-88. Sent to Cormac.\n2021-12-13 14:00 ET: Reverse Shadow and Oncall onboardings\nReverse Shadows\nMonday 2pm: Myle\nTuesday 2pm: Stephen\nWednesday 2pm: Susan\nThursday 2pm: Sam\nFriday 2pm: Naman\nMain oncalls\nMonday 2pm: Moya\nTuesday 2pm: Kurt\nWednesday 2pm: Punit\nThursday 2pm: Mikel\nFriday 2pm: Daniel\n2021-12-13: Preemptive Plan\nOn the next crash, we plan to lower the LR from 6.0 -> 4.5. We will observe that for a bit and lower it again to\n3.0 subject to signal.\nTHIS SHOULD CAUSE THE RUN_ID AND THE BLOB FOLDER TO BOTH BE BUMPED.\n2021-12-13 11:49 ET: [Stephen] Bumping timelimit\nAfter we joked about the world record of hitting >2D, realized needed to update the timelimit of the job.\nCommand executed:\nsudo scontrol update job=2589 TimeLimit=UNLIMITED\n2021-12-11 07:53 ET: [Stephen] Cluster Maintenance\n5 down nodes. Time for some reprovisioning.\nnode-25: Kill task failed [root@2021-12-11T10:57:40]\n\nnode-26: Bad_infiniband [roller@2021-12-10T11:30:21]\nnode-53: No_reason_given [susanz@2021-12-10T00:12:45]\nnode-108: No_reason_given [susanz@2021-12-10T00:12:33]\nnode-114: Failed_GPU_burn [roller@2021-12-10T11:15:37]\nInitiated replace_node on 3 bad nodes: 25, 26, 53\n2021-12-11 02:52 PT: [Susan] Noticed IB issues/lost GPU.\nFuture readers: this is now simplified with #2785\nHotswap 25 <> 145\nPrevious run hung with:\nmlx5: buo1u00000S: got completion with error:\n00000000 00000000 00000000 00000000\n00000000 00000000 00000000 00000000\n0000000d 00000000 00000000 00000000\n00000000 01005104 08001c17 10a7b3d3\n# ssh’ing onto node shows GPU lost:\n(base) susanz@buo1u00000S:~$ nvidia-smi\nUnable to determine the device handle for GPU 000C:00:00.0: GPU is lost.  Reboot the system to recover this\nGPU\n# Finding IP address for buo1u00000S\n(base) susanz@buo1u00000S:~$ ifconfig eth0\neth0: flags=4163<UP,BROADCAST,RUNNING,MULTICAST>  mtu 1500\ninet 10.30.4.29  netmask 255.255.252.0  broadcast 10.30.7.255\ninet6 fe80::222:48ff:fe25:4f2d  prefixlen 64  scopeid 0x20<link>\nether 00:22:48:25:4f:2d  txqueuelen 1000  (Ethernet)\nRX packets 1223684089  bytes 1350989781677 (1.3 TB)\nRX errors 0  dropped 0  overruns 0  frame 0\nTX packets 163407144  bytes 3319218659584 (3.3 TB)\nTX errors 0  dropped 0 overruns 0  carrier 0  collisions 0\n# Get hostname from IP address\n(base) susanz@buo1u00000S:~$ grep 10.30.4.29 /etc/hosts\n# Swap in 147 in for 25\nsudo scontrol requeue job=2589\nsudo scontrol hold job=2589\nsudo scontrol update job=2589 NodeList=node-[1-2,4-24,27-36,38-52,54-72,74-107,109-113,115-131,147]\nsudo scontrol release job=2589\nNotes:\n●\nNode 25 went into drain after job was updated with new hostlist\n●\nWe need to replace all the nodes in drain (?)\n\nPARTITION AVAIL  TIMELIMIT  NODES  STATE NODELIST\nhpc*         up   infinite      1 drain~ node-146\nhpc*         up   infinite      5  drain node-[25-26,53,108,114]\nhpc*         up   infinite      2    mix node-[3,37]\nhpc*         up   infinite    136  alloc node-[1-2,4-24,27-36,38-52,54-107,109-113,115-142,147]\nhpc*         up   infinite      3   idle node-[143-145]\n2021-12-10 23:14: [Stephen] 12.51 Resuming\n●\nRemembered I forgot to raise the learning rate\n●\nAlso took at the logs and saw it was fastforwarding the dataloader a lot. We should be right at an epoch\nboundary though! I think I didn’t wait long enough for checkpoints to upload? -- looks like we only have\n976/992 shards.\n○\nObserved via ls \"${BLOB_PREFIX}/?${BLOB_AUTH}\" | grep 61000 | wc -l\nWrote a fresh upload script and launched on nodes with:\npdsh -R ssh -w 'node-[1-2,4-25,27-36,38-52,54-72,74-107,109-113,115-135,139-145,147]' bash ~/restore_61000.sh\nGave a lot of logspam bc of nodes that didn’t participate in the previous job but that’s okay. It was clearly\ncopying on the ones left. Confirmed all 992 were uploaded at the end. Created a PR to increase the learning\nrate back to 6e-5 (GPT-3’s value) and relaunched.\n# LAUNCH OF 12.51\n# use updated fairscale\ncd ~/src/fairscale\ngit fetch origin prefetch_fsdp_params_simple\ngit checkout prefetch_fsdp_params_simple\ncd ~/src/Megatron-LM\ngit checkout tags/v2.6\ncd ~/src/fairseq-py\ngit fetch origin gshard_combine_megatron_fsdp_1251\ngit checkout gshard_combine_megatron_fsdp_1251\n# use new blob URL\nBLOB_PREFIX=\"<<<SCRUBBED FOR RELEASE>>>\"\nBLOB_AUTH=\"<<<SCRUBBED FOR RELEASE>>>\"\nRUN_ID=175B_run12.51 # big money no whammies\n# Doing something radical. All idle nodes should now be safe nodes.\npython -m fb_sweep.opt.sweep_opt_en_lm_175b \\\n-n 124 -g 8 -t 1 \\\n-p $RUN_ID \\\n--checkpoints-dir /shared/home/namangoyal/checkpoints/175B/ \\\n--local-checkpoints-dir /mnt/scratch/$USER/checkpoints/$(date +%Y-%m-%d).$RUN_ID \\\n--full-cloud-upload-path \"${BLOB_PREFIX}/?${BLOB_AUTH}\"\n# For the record, this ended up launching with\n# node-[1-2,4-25,27-36,38-52,54-72,74-107,109-113,115-131]\n\nConfirmed number of fast forward batches looks low (236). As of 00:23 ET looks healthy and signing off. Also\nleft two baselines running for fun.\nJust another last note: we seem to have improved massively in utilization. Here’s PPL with true wall clock:\nGaps are much better since the week of hell.\n2021-12-10 22:42: [Stephen] 12.50 Resuming\n●\nAblation is done. Time to resume.\n●\nMoved Megatron LM back to 2.6.\n# use updated fairscale\ncd ~/src/fairscale\ngit fetch origin prefetch_fsdp_params_simple\ngit checkout prefetch_fsdp_params_simple\ncd ~/src/Megatron-LM\ngit checkout tags/v2.6\ncd ~/src/fairseq-py\ngit fetch origin gshard_combine_megatron_fsdp\ngit checkout gshard_combine_megatron_fsdp\n# use new blob URL\nBLOB_PREFIX=\"<<<SCRUBBED FOR RELEASE>>>\"\nBLOB_AUTH=\"<<<SCRUBBED FOR RELEASE>>>\"\nRUN_ID=175B_run12.50 # big money no whammies\n# Doing something radical. All idle nodes should now be safe nodes.\npython -m fb_sweep.opt.sweep_opt_en_lm_175b \\\n\n-n 124 -g 8 -t 1 \\\n-p $RUN_ID \\\n--checkpoints-dir /shared/home/namangoyal/checkpoints/175B/ \\\n--local-checkpoints-dir /mnt/scratch/$USER/checkpoints/$(date +%Y-%m-%d).$RUN_ID \\\n--full-cloud-upload-path \"${BLOB_PREFIX}/?${BLOB_AUTH}\"\n2021-12-10 20:58 ET: [Stephen] The ablation\nUpdate: Bad news y’all. That ain’t the commit. New launch is clearly on the track as Naman’s. Messing up my\nown environment can't explain it, as this run was when I first upgraded to 2.6.\nBefore launch:\n●\nLame. Hit shard boundary at 60765 updates. Waiting until 61k since I know we have had issues\nrestoring from epoch boundaries before. Beginning ablation at 21:51.\n●\nReverting the bad commit in Megatron-LM with “git revert  -m 1 0be405”\n○\nConfirmed files looked like the right ones touched.\n# use updated fairscale\ncd ~/src/fairscale\ngit fetch origin prefetch_fsdp_params_simple\ngit checkout prefetch_fsdp_params_simple\ncd ~/src/fairseq-py\ngit fetch origin gshard_combine_megatron_fsdp\ngit checkout gshard_combine_megatron_fsdp\n\n# use new blob URL\nOLD_BLOB_PREFIX=\"<<<SCRUBBED FOR RELEASE>>>\"\nBLOB_AUTH=\"<<<SCRUBBED FOR RELEASE>>>\"\nRESTORE_FILE=\"${OLD_BLOB_PREFIX}/checkpoint_18_51750.pt?${BLOB_AUTH}\"\n# change blob prefix so that we don’t clobber Naman’s checkpoints\nBLOB_PREFIX=\"<<<SCRUBBED FOR RELEASE>>>”\nRUN_ID=175B_run12.47.byebad\n# Doing something radical. All idle nodes should now be safe nodes.\npython -m fb_sweep.opt.sweep_opt_en_lm_175b \\\n-n 124 -g 8 -t 1 \\\n-p $RUN_ID \\\n--checkpoints-dir /shared/home/namangoyal/checkpoints/175B/ \\\n--local-checkpoints-dir /mnt/scratch/$USER/checkpoints/$(date +%Y-%m-%d).$RUN_ID \\\n--restore-file $RESTORE_FILE \\\n--full-cloud-upload-path \"${BLOB_PREFIX}/?${BLOB_AUTH}\"\n2021-12-10 05:52 ET: [Stephen] Cluster maintenance\n●\nJust generally checking health of all our fresh idle nodes, making sure fixmycloud is up to the task.\n●\nFound update hosts to not quite be working\n●\nAfter running update hosts, observed that pg0-114 and pg0-3 have the same IP address.\n○\nIn “scontrol show nodes” pg0-114 has  NodeHostName of 10.30.4.11\n○\nIn “scontrol show nodes” pg0-3 has  NodeHostName of 10.30.5.6\n○\nSo why isn’t our script working?\n●\nAlso found hundreds of IP addresses that seem to only be listed once, don’t have slurm names. Ex\n10.30.7.95. I assume those are nodes we used to have\n●\nAlso noticed pg0-146 seems unreachable\n○\nIts slurm NodeAddr is given as a name to itself, not an ip address like the others.\n○\nThis exception was already carved out into the host updating script, so I guess it’s intentional\n●\nNoticed bugs in update_hosts:\n○\nClean known hosts was called before the hosts were updated lol\n○\nIt could easily skip unreachable nodes like -146\n○\nAfter fixing these, the incongruities in the hosts file disappeared. I think we just hadn’t been able\nto run update hosts successfully.\n●\nRan fixmycloud with lots of fixes and improvements\n○\n#2776\n○\nDrained one node bc of bad infiniband, one node bc of gpu burn\n○\nStarted replacing pg0-146\n●\nCurrent status: 124 active; 18 good spares; 4 drained + 1 being replaced\n2021-12-09 16:00 PT: [Susan] Run 12.49, restart due to NCCL errors\n●\nExactly the same garbage as “2021-12-05 12:15pm ET: Requeueing 12.44 and 12.45” run.\n●\nExcluding 108 and 53 and putting in drain.\n●\nLeaves us with:\n(fairseq-20210913) susanz@ip-0A1E0404:~/fairseq-py$ sinfo\n\nPARTITION AVAIL  TIMELIMIT  NODES  STATE NODELIST\nhpc*         up   infinite      4 drain* node-[3,37,73,124]\nhpc*         up   infinite      4  drain node-[26,53,108,114]\nhpc*         up   infinite    124   idle node-[1-2,4-25,27-36,38-52,54-72,74-107,109-113,115-123,125-132]\n●\nRestarting on the only 124 nodes we have that are functional:\n# use updated fairscale\ncd ~/src/fairscale\ngit fetch origin prefetch_fsdp_params_simple\ngit checkout prefetch_fsdp_params_simple\ncd ~/src/fairseq-py\ngit fetch origin gshard_combine_megatron_fsdp\ngit checkout gshard_combine_megatron_fsdp\n# use new blob URL\nBLOB_PREFIX=\"<<<SCRUBBED FOR RELEASE>>>\"\nBLOB_AUTH=\"<<<SCRUBBED FOR RELEASE>>>\"\n# confirm epoch of checkpoint from previous train.log - we checkpoint every 250 steps\nRESTORE_FILE=\"${BLOB_PREFIX}/checkpoint_19_57000.pt?${BLOB_AUTH}\"\nRUN_ID=175B_run12.49\nINCLUDED_HOSTS=node-[1-2,4-25,27-36,38-52,54-72,74-107,109-113,115-123,125-132] \\\npython -m fb_sweep.opt.sweep_opt_en_lm_175b \\\n-n 124 -g 8 -t 1 \\\n-p $RUN_ID \\\n--checkpoints-dir /shared/home/namangoyal/checkpoints/175B/ \\\n--local-checkpoints-dir /mnt/scratch/$USER/checkpoints/$(date +%Y-%m-%d).$RUN_ID \\\n--restore-file $RESTORE_FILE \\\n--full-cloud-upload-path \"${BLOB_PREFIX}/?${BLOB_AUTH}\"\n# monitor train.log - launch in a tmux session\n./scripts/cloud/monitor_train_log.py --mailto <scrubbed>@fb.com\n/shared/home/namangoyal/checkpoints/175B/175B_run12.49.me_fp16.minscale0.25.fsdp.gpf32.0.relu.transformer_l\nm_megatron.nlay96.emb12288.lrnpos.0emb_scale.bm_none.tps2048.gpt2.adam.b2_0.95.eps1e-08.cl0.3.lr4.5e-05.end\nlr6e-06.wu2000.dr0.1.atdr0.1.0emb_dr.wd0.1.ms8.uf1.mu143052.s1.ngpu992/train.log --modified-threshold 900\n# Add new entry for TB - TB dir takes a while to come up since training takes a while to restart (data\nloader has to fast forward, goes through ~75 batches / min)\ncd /shared/home/namangoyal/checkpoints/175B/tensorboard\nsudo ln -s\n../175B_run12.49.me_fp16.minscale0.25.fsdp.gpf32.0.relu.transformer_lm_megatron.nlay96.emb12288.lrnpos.0emb\n_scale.bm_none.tps2048.gpt2.adam.b2_0.95.eps1e-08.cl0.3.lr4.5e-05.endlr6e-06.wu2000.dr0.1.atdr0.1.0emb_dr.w\nd0.1.ms8.uf1.mu143052.s1.ngpu992/tb/ run12.49\n●\n[16:22 PT] Launched\n●\n[16:47 PT] Still waiting for data loader to fast forward\n○\n2021-12-10 00:32:58 | WARNING | fairseq.data.iterators | Fast-forwarding dataloader by 2334\nbatches...\n●\n[16:57 PT] Omg still waiting for data loader to fast forward ;_;\n○\nOur fast forwarding speed, based on notes from run 12.48, seems to be 75 batches / min.\n●\n[17:04 PT] Oh hallelujah\n\n○\n2021-12-10 01:04:14 | WARNING | fairseq.data.iterators | done fast-forwarding dataloader in\n1939.7 seconds\n2021-12-09 10:45am PT: Provisioning error in Cloud for node 124\n●\nReprovisioning, failed again.\n●\nYelled at CSP to give us more machines - will be getting 15 more at some point.\n●\nAlso need them to bump up ingress/egress limits on blob store\n2021-12-09: Megatron v2.6 Debrief + CSP Sync\nDiscussion about what happened\n●\nDiscussion about how it was discovered, reviewing Susan’s observations.\n●\nReviewed how we started going through to find what differed between Naman and Myle’s env.\n●\nReview that we have two versions megatron\n○\nSubmodule, which lets us do the model parallel MHA\n○\nImported, which lets us get fused_softmax\nFuture mitigations & Lessons learned:\n●\nUse single environment for all launches\n○\nContainers?\n○\nAdd assert to sweep.py to check version numbers?\n●\nWe should test upgrading dependencies periodically, in case there are bug fixes\n●\nWhy aren’t relaunches deterministic?\n○\nWe assumed it was just loss scale history, but it seems things are different even when I\nrelaunch the same thing multiple times\n●\nWhat was the bug that Nvidia fixed?\n○\nMegatron v2.6 vs v2.4\n■\nIncludes https://github.com/NVIDIA/Megatron-LM/pull/133\n●\nWhat is the impact of this bug on the first 37% of training?\n●\nDo we want to make any changes after fixing the bug?\n○\nIncrease learning rate?\n○\nIncrease clipping?\n2021-12-08 8:55pm ET: run12.48: relaunch checkpoint_18_54250\n●\nRun with Megatron v2.6 based on previous entries\n●\nResume from 54250, but has to fast-forward through 2630 batches, which takes 35 minutes with CPUs\nall at 100% :(\n\n# use updated fairscale\ncd ~/src/fairscale\ngit fetch origin prefetch_fsdp_params_simple\ngit checkout prefetch_fsdp_params_simple\ncd ~/src/fairseq-py\ngit fetch origin gshard_combine_megatron_fsdp\ngit checkout gshard_combine_megatron_fsdp\n# use new blob URL\nBLOB_PREFIX=\"<<<SCRUBBED FOR RELEASE>>>\"\nBLOB_AUTH=\"<<<SCRUBBED FOR RELEASE>>>\"\nRESTORE_FILE=\"${BLOB_PREFIX}/checkpoint_18_54250.pt?${BLOB_AUTH}\"\nRUN_ID=175B_run12.48\nINCLUDED_HOSTS=node-[1-2,4-25,27-36,38-72,74-112,115-117,119-123,125-132] \\\npython -m fb_sweep.opt.sweep_opt_en_lm_175b \\\n-n 124 -g 8 -t 1 \\\n-p $RUN_ID \\\n--checkpoints-dir /shared/home/namangoyal/checkpoints/175B/ \\\n--local-checkpoints-dir /mnt/scratch/$USER/checkpoints/$(date +%Y-%m-%d).$RUN_ID \\\n--restore-file $RESTORE_FILE \\\n--full-cloud-upload-path \"${BLOB_PREFIX}/?${BLOB_AUTH}\"\n2021-12-08 TBD: Analysis from 12.47.myle\n●\nThe results look different between my run and Naman’s original 12.47 run\n●\n●\nCurrent hypothesis is something in `pip list` is different between the two\n○\nMake myself into Naman: sudo -i namangoyal\n○\nNaman and Myle seem to have differences in our pip-installed version of megatron-lm!\n■\n~myleott/src/Megatron-LM\n■\n~/namangoyal/src/Megatron-LM\n■\nMyle is on v2.4 (42c1cf4279acea5a554500dcb552211f44cbec45)\n■\nNaman is on v2.6 (3860e995269df61d234ed910d4756e104e1ab844)\n●\nGoing to relaunch a few more times:\n○\nrun12.47.myle: my initial relaunch of 12.47 with my env\n○\nrun12.47.myle2: a second relaunch of above to test determinism\n■\nLooks like it’s not deterministic?!\n\n○\nrun12.47.myle3: a third relaunch with Megatron v2.6\n■\nLooks like Megatron v2.6 was making things better:\n■\n2021-12-08 05:05pm ET: GPU failure; launch debug run with Myle’s env\n●\nThere was a GPU failure, which caused 12.47 to hang\n○\nLatest checkpoint is checkpoint_18_54250\n●\nBased on the discussion below, I will relaunch from checkpoint_18_51750 with my env, to see if it\nmatches Naman’s results. This run should be identical to Naman’s original 12.47 run, except launched\nfrom my environment\n○\nIf it matches, then it seems we just got lucky!\n○\nIf it doesn’t match, then we need to dig more and understand if there’s something unique about\nNaman’s environment, or if resuming from checkpoints is nondeterministic somehow\n○\nIn either case, I will cancel and resume from checkpoint_18_54250 thereafter\n# use updated fairscale\ncd ~/src/fairscale\ngit fetch origin prefetch_fsdp_params_simple\ngit checkout prefetch_fsdp_params_simple\ncd ~/src/fairseq-py\ngit fetch origin gshard_combine_megatron_fsdp\ngit checkout gshard_combine_megatron_fsdp\n# use new blob URL\nBLOB_PREFIX=\"<<<SCRUBBED FOR RELEASE>>>\"\nBLOB_AUTH=\"<<<SCRUBBED FOR RELEASE>>>\"\nRESTORE_FILE=\"${BLOB_PREFIX}/checkpoint_18_51750.pt?${BLOB_AUTH}\"\n# change blob prefix so that we don’t clobber Naman’s checkpoints\nBLOB_PREFIX=\"/myleott/2021-12-08/175B_run12.47.myle\"\nRUN_ID=175B_run12.47.myle2\nINCLUDED_HOSTS=node-[1-2,4-25,27-36,38-72,74-113,115-123,125-130] \\\npython -m fb_sweep.opt.sweep_opt_en_lm_175b \\\n-n 124 -g 8 -t 1 \\\n-p $RUN_ID \\\n--checkpoints-dir /shared/home/namangoyal/checkpoints/175B/ \\\n--local-checkpoints-dir /mnt/scratch/$USER/checkpoints/$(date +%Y-%m-%d).$RUN_ID \\\n\n--restore-file $RESTORE_FILE \\\n--full-cloud-upload-path \"${BLOB_PREFIX}/?${BLOB_AUTH}\"\n2021-12-08 04:00am PT: Checking in\n[From Susan: How did Naman’s run just get a free 2% bump in wps? Truly the golden touch. Also seems to be\na significant drop in actv_norm as well, which generally helps us avoid overflow issues. Getting lucky here?]\n●\nComparison of config namespaces shows no significant differences:\npython scripts/compare_namespaces.py \\\n175B_run12.47*/train.log \\\n175B_run12.46*/train.log\n●\nPerhaps some difference in environment? Cc\nto confirm:\nNaman Goyal\n○\nPyTorch version: '1.9.0+cu111'\n■\nConfirmed this matches Myle’s env for 12.46\n○\nFairscale version:\n■\nBranch: prefetch_fsdp_params_simple\n■\nCommit: 8820049331331c773077c257667aa81baf4cc9f9\n●\nConfirmed this matches Myle’s env for 12.46\n○\nMegatron submodule version:\n■\nBranch: fairseq_v2 (16623c2dce9068f3f9574348b5b3c35c0c5a85c6)\n●\nConfirmed this matches Myle’s env for 12.46 and Susan’s env\n○\nFairseq commit:\n■\nCommit: fc24ce0ae48626a6d18dbb45b486600c3732c14f\n■\nBranch: gshard_combine_megatron_fsdp\n■\nMyle’s env for 12.46 was 6c973d4f92d0c9813f439a4920b23c7f93429511. There’s only\ntwo commits separating this from Naman’s and they are unrelated to training\n○\nCode snapshots, for reference:\n■\n12.47 (Naman):\n/shared/home/namangoyal/src/fairseq_gshard/fairseq-py/slurm_snapshot_code/2021-12\n-08T05_07_39.022577\n■\n12.46 (Myle):\n/shared/home/myleott/src/fairseq2/slurm_snapshot_code/2021-12-06T13_33_13.859846\n■\nConfirmed no meaningful differences with:\ndiff -bur --exclude __pycache__\n/shared/home/namangoyal/src/fairseq_gshard/fairseq-py/slurm_snapshot_code/2021-12-08T05\n_07_39.022577\n/shared/home/myleott/src/fairseq2/slurm_snapshot_code/2021-12-06T13_33_13.859846\n●\nFor future reference, here are the machines used for each run:\n○\nrun 12.46.2: node-[1-2,4-25,27-50,52-64,66-128]\n○\nrun 12.47: node-[1-2,4-25,27-72,74-96,98-112,114-117,119-123,125-131]\n○\nDiff:\n■\nRemove node-[73,97,113,118,124]\n■\nAdd node-[51,65,129,130,131]\n\n2021-12-07 10:59pm ET: RuntimeError: CUDA error:\nCUBLAS_STATUS_EXECUTION_FAILED when calling\nNOTES FOR FUTURE:\n1) 113 and 118 has info ram issue, look into if its a real issue or not, if it is then recycle\n●\nError stack trace:\ndata.storage().resize_(size.numel())\nRuntimeError: CUDA error: unspecified launch failure\nCUDA kernel errors might be asynchronously reported at some other API call,so the stacktrace below might be\nincorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1.\n●\nRan ECC error check:\n○\nWCOLL=~/hosts PDSH_RCMD_TYPE=ssh pdsh nvidia-smi -q -d \"ECC\" > ecc_error.log\n○\nhcp-pg0-97 and node-124 hang and are completely unreachable\n○\nnode-132 has “Could not resolve hostname node-132”\n●\nRan nvidia-smi check on all but above 3 nodes:\npython scripts/cloud/nvidia_smi.py node-[1-2,4-25,27-72,74-96,98-123,125-131]\n2021-12-08 04:33:21 WARNING  __main__   | node-23: ecc high correctables: DRAM Correctable: 170081\n2021-12-08 04:33:21 WARNING  __main__   | node-88: ecc high correctables: DRAM Correctable: 506807\n2021-12-08 04:33:21 WARNING  __main__   | node-85: ecc high correctables: DRAM Correctable: 16758\n2021-12-08 04:33:21 WARNING  __main__   | node-85: ecc high correctables: DRAM Correctable: 16758\n2021-12-08 04:33:21 INFO     __main__   | All nodes pass ECC checks.\n2021-12-08 04:33:21 INFO     __main__   | Running MIG checks on node-[1-2,4-25,27-72,74-96,98-123,125-131]\n2021-12-08 04:33:24 INFO     __main__   | All nodes pass MIG tests\n2021-12-08 04:33:24 INFO     __main__   | Running InfoROM checks on\nnode-[1-2,4-25,27-72,74-96,98-123,125-131]\n\n2021-12-08 04:33:38 WARNING  __main__   | node-113: infoROM is corrupted at gpu 0002:00:00.0\n2021-12-08 04:33:38 WARNING  __main__   | node-118: infoROM is corrupted at gpu 000B:00:00.0\n●\nGiven above have to choose whether to exclude nodes with high correctable ecc error or inforam\nissues.\n○\nTaking a call to exclude “inforom is corrupted” hosts and choosing to go with following hosts:\n# use updated fairscale\ncd ~/src/fairscale\ngit fetch origin prefetch_fsdp_params_simple\ngit checkout prefetch_fsdp_params_simple\ncd ~/src/fairseq-py\ngit fetch origin gshard_combine_megatron_fsdp\ngit checkout gshard_combine_megatron_fsdp\n# use new blob URL\nBLOB_PREFIX=\"<<<SCRUBBED FOR RELEASE>>>\"\nBLOB_AUTH=\"<<<SCRUBBED FOR RELEASE>>>\"\nRESTORE_FILE=\"${BLOB_PREFIX}/checkpoint_18_51750.pt?${BLOB_AUTH}\"\nRUN_ID=175B_run12.47\nINCLUDED_HOSTS=node-[1-2,4-25,27-72,74-96,98-112,114-117,119-123,125-131] \\\npython -m fb_sweep.opt.sweep_opt_en_lm_175b \\\n-n 124 -g 8 -t 1 \\\n-p $RUN_ID \\\n--checkpoints-dir /shared/home/namangoyal/checkpoints/175B/ \\\n--local-checkpoints-dir /mnt/scratch/$USER/checkpoints/$(date +%Y-%m-%d).$RUN_ID \\\n--restore-file $RESTORE_FILE \\\n--full-cloud-upload-path \"${BLOB_PREFIX}/?${BLOB_AUTH}\"\n# monitor train.log\n./scripts/cloud/monitor_train_log.py --mailto <scrubbed>\n/shared/home/namangoyal/checkpoints/175B/175B_run12.46*/train.log --modified-threshold 900\n# Add new entry for TB\ncd /shared/home/namangoyal/checkpoints/175B/tensorboard\nsudo ln -s\n../175B_run12.47.me_fp16.minscale0.25.fsdp.gpf32.0.relu.transformer_lm_megatron.nlay96.emb12288.lrnpos.0emb\n_scale.bm_none.tps2048.gpt2.adam.b2_0.95.eps1e-08.cl0.3.lr4.5e-05.endlr6e-06.wu2000.dr0.1.atdr0.1.0emb_dr.w\nd0.1.ms8.uf1.mu143052.s1.ngpu992/tb/ run12.47\n2021-12-07 10:48am ET: ECC error, requeueing 12.46\n●\nBlame to node-26\n○\nBased on first line in stderr: “srun: error: node-26: task 196: Aborted (core dumped)”\n○\nssh to node and nvidia-smi:\n■\nUnable to determine the device handle for GPU 000B:00:00.0: GPU is lost.\n●\nUndraining node-108, which had “port error” previously\n○\nsudo scontrol update node=node-108 state=resume\n\n●\nRequeue with:\n○\nsudo scontrol requeue job=2487\n○\nsudo scontrol hold job=2487\n○\nsudo scontrol update job=2487 NodeList=node-[1-2,4-25,27-50,52-64,66-128]\n○\nsudo scontrol release job=2487\n2021-12-06 8:30am ET: Lowering LR and launching 12.46\n●\nLowering LR to 4e-5\n# use updated fairscale\ncd ~/src/fairscale\ngit fetch origin prefetch_fsdp_params_simple\ngit checkout prefetch_fsdp_params_simple\ncd ~/src/fairseq-py\ngit fetch origin gshard_combine_megatron_fsdp\ngit checkout gshard_combine_megatron_fsdp\n# use new blob URL\nBLOB_PREFIX=\"<<<SCRUBBED FOR RELEASE>>>\"\nBLOB_AUTH=\"<<<SCRUBBED FOR RELEASE>>>\"\nRESTORE_FILE=\"${BLOB_PREFIX}/checkpoint_16_46250.pt?${BLOB_AUTH}\"\nRUN_ID=175B_run12.46\nINCLUDED_HOSTS=node-[1-2,4-50,52-64,66-107,109-128] \\\npython -m fb_sweep.opt.sweep_opt_en_lm_175b \\\n-n 124 -g 8 -t 1 \\\n-p $RUN_ID \\\n--checkpoints-dir /shared/home/namangoyal/checkpoints/175B/ \\\n--local-checkpoints-dir /mnt/scratch/$USER/checkpoints/$(date +%Y-%m-%d).$RUN_ID \\\n--restore-file $RESTORE_FILE \\\n--full-cloud-upload-path \"${BLOB_PREFIX}/?${BLOB_AUTH}\"\n# monitor train.log\n./scripts/cloud/monitor_train_log.py --mailto <scrubbed>\n/shared/home/namangoyal/checkpoints/175B/175B_run12.46*/train.log --modified-threshold 900\n2021-12-06 05:13 PT: Job Hanging - 3 Machines Down\n●\n5 nodes are partly or fully compromised.\n○\n2x: Unable to determine the device handle for GPU 0002:00:00.0: GPU is lost.\n■\nnode-51\n■\nnode-3\n○\n2x: NCCL error (Got async event : port error)\n■\n[note: unclear if this means the nodes can’t be used; we’ve been using one of them\nsuccessfully for the last 12 hours]\n■\nnode-108\n■\nnode-53\n○\n1x: nvidia-smi hangs\n■\nnode-65\n●\nNext steps:\n\n○\nThe \"port error\" nodes seem to be fine -- we were actually using one of them last night without a\nproblem.\n○\nWill reprovision node-51 and node-3 now, since they will not get reallocated to us, since\nCloud’s built-in health checks will reject them\n2021-12-06 05:00 PT: Grad norm spiking, ppl trending up\n[From Susan: reading more tea leaves here, but seems like we’ve had a couple of grad norm spikes and our\nppl is now slowly diverging. Recommending restarting with half the LR]\n2021-12-05 9pm ET: Requeue after GPU error\n●\nRuntimeError: CUDA error: CUBLAS_STATUS_EXECUTION_FAILED when calling `cublasGemmEx(\nhandle, opa, opb, m, n, k, &falpha, a, CUDA_R_16F, lda, b, CUDA_R_16F, ldb, &fbeta, c,\nCUDA_R_16F, ldc, CUDA_R_32F, CUBLAS_GEMM_DFALT_TENSOR_OP)`\n●\nBlame is node-51\n○\nRan nvidia-smi and node-51 has “Unable to determine the device handle for GPU\n0002:00:00.0: GPU is lost.  Reboot the system to recover this GPU”\n●\nRequeue while replacing node-51 with node-88\n○\nsudo scontrol requeue job=2486\n○\nsudo scontrol hold job=2486\n○\nsudo scontrol update job=2486 NodeList=node-[1,3-50,52-64,66-107,109-128]\n■\nThis replaces node-51 with node-88\n○\nsudo scontrol release job=2486\n●\nNote: It took 50 minutes to resume training from checkpoint_15_45000!\n○\n~30 minutes just fast-forwarding the dataloader!\n○\nAlso added more logging: PR #2748\n\n2021-12-05 18:30 ET: Poking through dmesg to see if we can find where we hung\n●\nNoticed “nvidia-nvswitch: Version Mismatch”. Happens on both pg0-1 and pg0-53\n●\nDmesg actually reports when the job is officially hung: “task python:35662 blocked for more than 120\nseconds” (Though I think this might be the dataloader workers, not the main proc?)\n●\nWhen controlling for looking at timestamps, i don’t see a lot in dmesg :(\n2021-12-05 12:15pm ET: Requeueing 12.44 and 12.45\n●\nThere was a NCCL error (Got async event : port error) on node-108 and node-53\n○\nIt’s not clear which node is bad…\n○\nHow to find the bad nodes:\n■\nThere are two hosts that have port errors in train.log: buo1u000030 and buo1u00001K\n■\nTo translate these into node-XX hostnames:\n●\nssh to each host (e.g., buo1u000030)\n●\nget IP address (e.g., `ifconfig eth0` yields 10.30.4.109)\n●\nuse /etc/hosts to map IP address (e.g., `grep 10.30.4.109 /etc/hosts` yields\nnode-108)\n●\nAction item: update monitor_train_log.py script to alert to port errors\n○\n“NCCL WARN NET/IB : Got async event :”\n●\nGoing to try requeueing and swapping the node out via scontrol\n○\nsudo scontrol requeue job=2483\n○\nsudo scontrol hold job=2483\n○\nsudo scontrol update job=2483 NodeList=node-[1,3-64,66-87,89-107,109-128]\n■\nThis replaces node-108 with node-113\n○\nsudo scontrol release job=2483\n○\nDidn’t work, seems to be trying to download into a blob URL :/\n●\nFall back to manual launch:\n# use updated fairscale\ncd ~/src/fairscale\ngit fetch origin prefetch_fsdp_params_simple\ngit checkout prefetch_fsdp_params_simple\ncd ~/src/fairseq-py\ngit fetch origin gshard_combine_megatron_fsdp\ngit checkout gshard_combine_megatron_fsdp\n# use new blob URL\nBLOB_PREFIX=\"<<<SCRUBBED FOR RELEASE>>>\"\nBLOB_AUTH=\"<<<SCRUBBED FOR RELEASE>>>\"\nRESTORE_FILE=\"${BLOB_PREFIX}/checkpoint_15_44000.pt?${BLOB_AUTH}\"\nRUN_ID=175B_run12.45\nINCLUDED_HOSTS=node-[1,3-64,66-87,89-107,109-128] \\\npython -m fb_sweep.opt.sweep_opt_en_lm_175b \\\n-n 124 -g 8 -t 1 \\\n-p $RUN_ID \\\n--checkpoints-dir /shared/home/namangoyal/checkpoints/175B/ \\\n--local-checkpoints-dir /mnt/scratch/$USER/checkpoints/$(date +%Y-%m-%d).$RUN_ID \\\n--restore-file $RESTORE_FILE \\\n--full-cloud-upload-path \"${BLOB_PREFIX}/?${BLOB_AUTH}\"\n\n# monitor train.log\n./scripts/cloud/monitor_train_log.py --mailto <scrubbed>\n/shared/home/namangoyal/checkpoints/175B/175B_run12.44*/train.log\n2021-12-05 05:35 PT: Checking on 12.44\n[From Susan: looks like lowering LR helps keep us on track wrt ppl. Loss scale that stays below 1 for too long\ncould be a leading indicator of instability going forward.  Set smoothing to ~0.95 to see these trends.]\n2021-12-05 02:24 ET: Side experiment\nNote that I (Stephen) launched a 768M equivalent model to train for a bit to help unblock Anjali. If you need\nspare nodes, please just kill the job, as it is less important.\n02:56 ET both runs look stable. Signing off.\n2021-12-05- 00:00 ET: Loss scale exploding 2 - 12.44\nNote that if we decide to relaunch this, there’s an epoch boundary VERY SHORTLY after 42500 that might be\npreferable.\n●\nSusan pointed out how gnorms seem unusually high. Individual updates are fine, but the frequency of\nspikes has definitely increased (from 12.42+12.43, 58/73 of the spikes >0.2 have been since 42500\nupdates)\n●\nProposed mitigation: Roll back to 42000 and resubmit.\n●\nAlternative mitigation: Lower learning rate further\n○\nOpted to lower it by a factor of 0.9\n○\nAnd restore from 42500\n\n○\nAnd bumped the blob folder\n# use updated fairscale\ncd ~/src/fairscale\ngit fetch origin prefetch_fsdp_params_simple\ngit checkout prefetch_fsdp_params_simple\ncd ~/src/fairseq-py\ngit fetch origin gshard_combine_megatron_fsdp\ngit checkout gshard_combine_megatron_fsdp\n# use new blob URL\nBLOB_PREFIX=\"<<<SCRUBBED FOR RELEASE>>>\"\nBLOB_AUTH=\"<<<SCRUBBED FOR RELEASE>>>\"\nRESTORE_FILE=\"${OLD_BLOB_PREFIX}/checkpoint_14_42500.pt?${BLOB_AUTH}\"\nRUN_ID=175B_run12.44\nINCLUDED_HOSTS=node-[1,3-64,66-87,89-112,114-128] \\\npython -m fb_sweep.opt.sweep_opt_en_lm_175b \\\n-n 124 -g 8 -t 1 \\\n-p $RUN_ID \\\n--checkpoints-dir /shared/home/namangoyal/checkpoints/175B/ \\\n--local-checkpoints-dir /mnt/scratch/$USER/checkpoints/$(date +%Y-%m-%d).$RUN_ID \\\n--restore-file $RESTORE_FILE \\\n--full-cloud-upload-path \"${BLOB_PREFIX}/?${BLOB_AUTH}\"\n# monitor train.log\n./scripts/cloud/monitor_train_log.py --mailto <scrubbed>\n/shared/home/namangoyal/checkpoints/175B/175B_run12.44*/train.log\n2021-12-04 20:24 ET: Loss scale exploding\n●\nObserved loss scale exploding via alerting.\n○\nWaiting some short time to requeue\n○\nIt's teetering. Checking back in 30.\n\n●\nNote: we did manage to checkpoint during this (u 42750). We probably want to roll back to 42500 just\nbecause initializing from a bad loss scale is bad. Loss scale was 8! At that moment in time\n●\nStill confirmed no uploads. Looks like to restore from checkpoint i will need to copy from all the nodes to\na safe directory.\n●\nForced uploading of all local checkpoints via:\n○\npdsh -R ssh -w “$(squeue | tail -n 1 | awk ‘{print $8}’)” ~roller/upload_checkpoints.sh | tee\nscary_upload_log\n●\nBy the time the upload had finished, we had hit 43000 updates and out of alert territory. However,\nloss scale was still only 0.25.\n●\nDecided to again, let it live on. NO ACTUAL ACTION WAS TAKEN WRT THE JOB\n2021-12-04 5:35am ET: Launch of 12.43: Fix blob upload\n●\nAnalysis of 12.42:\n○\nThere was some CUDA error around 1am that caused training to hang\n■\nRuntimeError: CUDA error: unspecified launch failure\n■\nCUDA kernel errors might be asynchronously reported at some other API call,so\nthe stacktrace below might be incorrect.\n○\nLooks like node-65 is to blame (nvidia-smi is slow, has hung process), but I suggest leaving this\nnode in drain for a couple days to report to CSP\n●\nLaunch of 12.43\n○\nUsing this as an opportunity to replace the blob URL with a new one that works\n■\nUsing a fresh blob URL that points to Susan’s blob container:\n/susanz/2021-12-04/175B_run12.42\n■\nSince checkpoints were sitting on local disk on each node, I manually uploaded all of\nthem to the new path with this hacky script.\n○\nReplaced node-65 with node-71\n# use updated fairscale\ncd ~/src/fairscale\ngit fetch origin prefetch_fsdp_params_simple\ngit checkout prefetch_fsdp_params_simple\ncd ~/src/fairseq-py\ngit fetch origin gshard_combine_megatron_fsdp\ngit checkout gshard_combine_megatron_fsdp\n# use new blob URL\nBLOB_PREFIX=\"<<<SCRUBBED FOR RELEASE>>>\"\nBLOB_AUTH=\"<<<SCRUBBED FOR RELEASE>>>\"\nRESTORE_FILE=\"${BLOB_PREFIX}/checkpoint_14_40750.pt?${BLOB_AUTH}\"\nRUN_ID=175B_run12.43\nINCLUDED_HOSTS=node-[1,3-64,66-87,89-112,114-128] \\\npython -m fb_sweep.opt.sweep_opt_en_lm_175b \\\n-n 124 -g 8 -t 1 \\\n-p $RUN_ID \\\n--checkpoints-dir /shared/home/namangoyal/checkpoints/175B/ \\\n--local-checkpoints-dir /mnt/scratch/$USER/checkpoints/$(date +%Y-%m-%d).$RUN_ID \\\n--restore-file $RESTORE_FILE \\\n--full-cloud-upload-path \"${BLOB_PREFIX}/?${BLOB_AUTH}\"\n\n# monitor train.log\n./scripts/cloud/monitor_train_log.py --mailto <scrubbed>\n/shared/home/namangoyal/checkpoints/175B/175B_run12.43*/train.log\n2021-12-03, 10:35pm ET: DO NOT REBOOT OR REPROVISION ANY NODES UNTIL\nFURTHER NOTICE\nIt seems the upload-to-blob is broken due to a limit on the Cloud side, so the latest checkpoints are sitting on\nthe local disks on each node and are not being uploaded to blob. I manually copied 40500 to /data for now, but\nplease do not reprovision any nodes until this is resolved, since we will lose the latest checkpoint data in that\ncase.\nThe error seems to be “409 The uncommitted block count cannot exceed the maximum limit of 100,000\nblocks.”\n●\nSome Google’ing suggests this is due to having too many incomplete/failed uploads to the given path.\n●\nManually uploading any file to the blob path consistently fails with the same error.\n●\nI tried deleting some of the stored checkpoints to see if that resolved anything, but no luck.\n○\nNote: I’ve now deleted most of the historical checkpoints at *250 and *750 steps, so between\n~25k and ~40k steps we now only have checkpoints at *500 and *000 steps\n●\nThis CSP help article suggests the “Wait 7 days for the uncommitted block list to garbage collect.”\n●\nSeems the easiest fix for now is to switch to a new blob container.\n●\nMitigation: Switch to a new blob container:\nsusanz/2021-12-04/175B_run12.42\n2021-12-03\n●\nMyle to finish the SGD code -- done\n●\nStephen to launch a 4 node tiny model for Anjali\n2021-12-03 7:20am ET: Launch of 12.42: Switch back to Adam\n●\n12.41 seemed to make no progress in terms of pnorms or loss\n●\nWe also implemented SGD instead of SGDW (i.e., weight decay was wrong)\n●\nProposal: roll back to 12.39 with AdamW and further lower learning rate from 9e-5 to 6e-5 to match\nGPT-3\n○\n#2736\n# use updated fairscale\ncd ~/src/fairscale\ngit fetch origin prefetch_fsdp_params_simple\ngit checkout prefetch_fsdp_params_simple\ncd ~/src/fairseq-py\ngit fetch origin gshard_combine_megatron_fsdp\ngit checkout gshard_combine_megatron_fsdp\nBLOB_PREFIX=\"<<<SCRUBBED FOR RELEASE>>>\"\nBLOB_AUTH=\"<<<SCRUBBED FOR RELEASE>>>\"\n\nRESTORE_FILE=\"${BLOB_PREFIX}/checkpoint_13_38500.pt?${BLOB_AUTH}\"\nRUN_ID=175B_run12.42\nINCLUDED_HOSTS=node-[1,3-70,72-87,89-112,114-128] \\\npython -m fb_sweep.opt.sweep_opt_en_lm_175b \\\n-n 124 -g 8 -t 1 \\\n-p $RUN_ID \\\n--checkpoints-dir /shared/home/namangoyal/checkpoints/175B/ \\\n--local-checkpoints-dir /mnt/scratch/$USER/checkpoints/$(date +%Y-%m-%d).$RUN_ID \\\n--restore-file $RESTORE_FILE \\\n--full-cloud-upload-path \"${BLOB_PREFIX}?${BLOB_AUTH}\"\n2021-12-02 17:16 ET: Fake SGD debacle: Debrief discussion\nSummary of events and mitigations\nCurrent hypothesis is that all our beta1=0 runs didn’t work.\n●\nReason for this bug was because adam state dicts load the betas\n●\n12.35 (cloud checkpoints caused to never launch) and 12.36 (beta1=0) and 12.37 (simple requeue of\n36)\n●\nNote 12.38 and 12.39 were always meant to be true adam\nDecision made to hard switch to true vanilla SGD (with launch of 12.41). Note, this required implementing\nfp16-friendly SGD.\nNOTE FOR FUTURE: BECAUSE OF THE BETA1 BUG [WARNING: See 2021-12-02 17:16 ET: Debrief]\ntherefore any ablations with 12.36/12.37 are no longer valid.\nNext paths\n●\nPrediction: 12.41 [true sgd] is probably going to drop rapidly due to rapidly annealing learning rate.\n●\nSome debate\n●\nAction Item: We should review the megatron code and see if they have anything about the switching.\nWe don’t believe they did this trick.\n2021-12-02 16:08 ET: Launch of 12.41: Switching to true Vanilla SGD\nDO NOT REQUEUE THIS RUN, IT CONTAINS RESTORE FILE LOGIC THAT WILL RESET BACK TO 37K\nEVERY TIME!\nDO NOT REQUEUE THIS RUN, IT CONTAINS --RESET-OPTIMIZER LOGIC!\n\n# use updated fairscale\ncd ~/src/fairscale\ngit fetch origin prefetch_fsdp_params_simple\ngit checkout prefetch_fsdp_params_simple\ncd ~/src/fairseq-py\ngit fetch origin gcmf-1241\ngit checkout gcmf-1241\nBLOB_URL=\"<<<SCRUBBED FOR RELEASE>>>”\nCKPT_DIR=/data/users/susanz/checkpoints\n# these two lines skipped because susan already downloaded them, but leaving them as future reference\n#### cd $CKPT_DIR\n#### cp --recursive --include-pattern \"checkpoint_13_37000*.pt\" \"$BLOB_URL\" checkpoint_13_37000\nRESTORE_FILE=$CKPT_DIR/checkpoint_13_37000/175B_run12.14.me_fp16.minscale0.25.fsdp.gpf32.0.relu.transformer_lm_me\ngatron.nlay96.emb12288.lrnpos.0emb_scale.bm_none.tps2048.gpt2.adam.b2_0.95.eps1e-08.cl1.0.lr0.00012.endlr6e-06.wu\n2000.dr0.1.atdr0.1.0emb_dr.wd0.1.ms8.uf1.mu143052.s1.ngpu992/checkpoint_13_37000.pt\nRUN_ID=175B_run12.41\nINCLUDED_HOSTS=node-[1,3-70,72-87,89-112,114-128] \\\npython -m fb_sweep.opt.sweep_opt_en_lm_175b \\\n-n 124 -g 8 -t 1 \\\n-p $RUN_ID \\\n--checkpoints-dir /shared/home/namangoyal/checkpoints/175B/ \\\n--local-checkpoints-dir /mnt/scratch/$USER/checkpoints/2021-12-02.$RUN_ID \\\n--restore-file $RESTORE_FILE \\\n--full-cloud-upload-path $BLOB_URL\n2021-12-02 10:25am ET: Launch of 12.40: Intended to be fake SGD with lower learning\nrate [WARNING: See 2021-12-02 17:16 ET: Debrief on why that may not be]\nDebate ensued. Decided to launch Fake SGD with:\n●\nLower learning rate\n●\nLower clip\nReasoning: The ablations give signal on which direction SGD learning should go. And both changing\nhyperparameters effectively lower LR so they won’t conflict. Signal is directionally sane.\nNOTE FOR FUTURE: BECAUSE OF THE BETA1 BUG [WARNING: See 2021-12-02 17:16 ET: Debrief]\ntherefore any ablations with 12.36/12.37 are no longer valid.\n\n# use updated fairscale\ncd ~/src/fairscale\ngit fetch origin prefetch_fsdp_params_simple\ngit checkout prefetch_fsdp_params_simple\ncd ~/src/fairseq-py\ngit fetch origin gcmf-1240\ngit checkout gcmf-1240\nBLOB_URL=\"<<<SCRUBBED FOR RELEASE>>>\"\nCKPT_DIR=/data/users/susanz/checkpoints\n# these two lines skipped because susan already downloaded them, but leaving them as future reference\n#### cd $CKPT_DIR\n#### cp --recursive --include-pattern \"checkpoint_13_37000*.pt\" \"$BLOB_URL\" checkpoint_13_37000\nRESTORE_FILE=$CKPT_DIR/checkpoint_13_37000/175B_run12.14.me_fp16.minscale0.25.fsdp.gpf32.0.relu.transformer_lm_me\ngatron.nlay96.emb12288.lrnpos.0emb_scale.bm_none.tps2048.gpt2.adam.b2_0.95.eps1e-08.cl1.0.lr0.00012.endlr6e-06.wu\n2000.dr0.1.atdr0.1.0emb_dr.wd0.1.ms8.uf1.mu143052.s1.ngpu992/checkpoint_13_37000.pt\nRUN_ID=175B_run12.40\nINCLUDED_HOSTS=node-[1,3-70,72-87,89-112,114-128] \\\npython -m fb_sweep.opt.sweep_opt_en_lm_175b \\\n-n 124 -g 8 -t 1 \\\n-p $RUN_ID \\\n--checkpoints-dir /shared/home/namangoyal/checkpoints/175B/ \\\n--local-checkpoints-dir /mnt/scratch/$USER/checkpoints/2021-12-02.$RUN_ID \\\n--restore-file $RESTORE_FILE \\\n--full-cloud-upload-path $BLOB_URL\n2021-12-01 1:30pm ET: Launch of 12.39\nAnalysis of 12.38\nExploded 10 steps early compared to 12.33! Woot!\nSpent a surprising amount of time riding the line of 0.0625 loss pretty happily\nLaunch of 12.39\nDO NOT REQUEUE THIS RUN, IT CONTAINS RESTORE FILE LOGIC THAT WILL RESET BACK TO 37K\nEVERY TIME! Additionally, instead of requeueing, we should initiate the launch of 12.40!\n\n# use updated fairscale\ncd ~/src/fairscale\ngit fetch origin prefetch_fsdp_params_simple\ngit checkout prefetch_fsdp_params_simple\ncd ~/src/fairseq-py\ngit fetch origin back2adam12.39_gshard_combine_megatron_fsdp\ngit checkout back2adam12.39_gshard_combine_megatron_fsdp\nBLOB_URL=\"<<<SCRUBBED FOR RELEASE>>>\"\nCKPT_DIR=/data/users/susanz/checkpoints\n# these two lines skipped because susan already downloaded them, but leaving them as future reference\n#### cd $CKPT_DIR\n#### cp --recursive --include-pattern \"checkpoint_13_37000*.pt\" \"$BLOB_URL\" checkpoint_13_37000\nRESTORE_FILE=$CKPT_DIR/checkpoint_13_37000/175B_run12.14.me_fp16.minscale0.25.fsdp.gpf32.0.relu.transformer_lm_me\ngatron.nlay96.emb12288.lrnpos.0emb_scale.bm_none.tps2048.gpt2.adam.b2_0.95.eps1e-08.cl1.0.lr0.00012.endlr6e-06.wu\n2000.dr0.1.atdr0.1.0emb_dr.wd0.1.ms8.uf1.mu143052.s1.ngpu992/checkpoint_13_37000.pt\nRUN_ID=175B_run12.39\nINCLUDED_HOSTS=node-[1,3-70,72-87,89-112,114-128] \\\npython -m fb_sweep.opt.sweep_opt_en_lm_175b \\\n-n 124 -g 8 -t 1 \\\n-p $RUN_ID \\\n--checkpoints-dir /shared/home/namangoyal/checkpoints/175B/ \\\n--local-checkpoints-dir /mnt/scratch/$USER/checkpoints/2021-12-01.$RUN_ID \\\n--restore-file $RESTORE_FILE \\\n--full-cloud-upload-path $BLOB_URL\nDiscussion\nDecision: Why not both?\n●\nAblate and optimize for knowledge\n●\nBoth runs beginning at 37k\n●\nBoth runs will keep the loss scale changes. We handled lower rates for a healthy amount of time, and\nthe end result was the same.\n●\nLaunch 12.39:\n○\nADAM with max LR adjusted to 9e-5 (bringing us down to 6.8e-5 at current step)\n○\nKeep clip at 0.3\n○\nLaunched first bc it’s more likely to die quickly, therefore make a happier oncall.\n●\nLaunch 12.40:\n○\nFake SGD with max LR left the same (1.2e-4) and clip lowered to 0.3\nAnalysis\nObservations:\n●\nFake SGD made it further than ADAM\n○\nIt made it a bit further from a requeue (1400 updates from CP, vs 400)\n○\nWithout requeue it slightly later (725 updates from CP vs 400)\n○\nRegardless of ADAM/SGD decision, we probably should change something else.\n●\nThe loss scale changes didn’t change help ADAM:\n\n○\nIt dies in the exact same places as before (within 10 updates)\n●\nGnorm is spiking in the same places for both (data!) but magn differ by method greatly.\n●\nNo signal on validation performance.\nKey:\n●\nPurple = Old ADAM run (12.33)\n●\nLight green = Fake SGD Run (12.36)\n●\nDark green = Manual requeue of Light green after it hit 0.25 bottom out (12.37)\n●\nBlue = New ADAM run (12.38)\nMax distance each got:\nFirst just Fake SGD runs:\n\nNext just the adam runs (Too difficult to distinguish on the same graph)\n\nAnd here is both Fake SGD (with the requeue) and ADAM:\n2021-12-01 8:39am ET: 12.38 True Adam with Lower LR\nDO NOT REQUEUE THIS RUN, IT CONTAINS RESTORE FILE LOGIC THAT WILL RESET BACK TO 37K\nEVERY TIME!\nAnd we broke at the exact same spot. (38414 updates)\nSummary of discussions for next steps:\n●\nProposals:\n○\nSwitch to true SGD\n○\nFutz with loss scale windows / logic\n○\nChange clipping back to 0.3\n●\nDecision:\n○\nRevert to change 37k (last pure adam ckpt), back to pure adam / clip 0.3\n○\nChange the loss scale logic to halve the raise window 132 => 64\n\n○\nChange the loss scale logic to never load from checkpoint\n○\nChanges in #2714\n●\nRationale\n○\nWe get a nice post-hoc comparison of fake-sgd vs adam\n○\nWe’re seeing this loss scale issue come up for both fake-sgd and adam\n○\nRequeues are regularly buying us second lives, which are essentially only messing with loss\nscale windows\nLaunching 12.38:\n# use updated fairscale\ncd ~/src/fairscale\ngit fetch origin prefetch_fsdp_params_simple\ngit checkout prefetch_fsdp_params_simple\ncd ~/src/fairseq-py\ngit fetch origin back2adam12.38_gshard_combine_megatron_fsdp\ngit checkout back2adam12.38_gshard_combine_megatron_fsdp\nBLOB_URL=\"<<<SCRUBBED FOR RELEASE>>>\"\nCKPT_DIR=/data/users/susanz/checkpoints\n# these two lines skipped because susan already downloaded them, but leaving them as future reference\n#### cd $CKPT_DIR\n#### cp --recursive --include-pattern \"checkpoint_13_37000*.pt\" \"$BLOB_URL\" checkpoint_13_37000\nRESTORE_FILE=$CKPT_DIR/checkpoint_13_37000/175B_run12.14.me_fp16.minscale0.25.fsdp.gpf32.0.relu.transformer_lm_me\ngatron.nlay96.emb12288.lrnpos.0emb_scale.bm_none.tps2048.gpt2.adam.b2_0.95.eps1e-08.cl1.0.lr0.00012.endlr6e-06.wu\n2000.dr0.1.atdr0.1.0emb_dr.wd0.1.ms8.uf1.mu143052.s1.ngpu992/checkpoint_13_37000.pt\nRUN_ID=175B_run12.38\nINCLUDED_HOSTS=node-[1,3-70,72-87,89-112,114-128] \\\npython -m fb_sweep.opt.sweep_opt_en_lm_175b \\\n-n 124 -g 8 -t 1 \\\n-p $RUN_ID \\\n--checkpoints-dir /shared/home/namangoyal/checkpoints/175B/ \\\n--local-checkpoints-dir /mnt/scratch/$USER/checkpoints/2021-12-30.$RUN_ID \\\n--restore-file $RESTORE_FILE \\\n--full-cloud-upload-path $BLOB_URL\n2021-12-01 2:21am ET: [Stephen oncall] Run 12.37 [WARNING: See 2021-12-02\n17:16 ET: Debrief on why it may not be SGD]\nWe got much further this time (38414 updates) but did start hitting 0.25’s. Requeued.\n2021-11-30 7:24pm ET: [Stephen oncall] Run 12.37 Manual requeue of 12.36.\n[WARNING: See 2021-12-02 17:16 ET: Debrief on why it may not be SGD]\nHit 37725 updates and started getting 0.25’s. Couldn’t requeue bc previous run ignored downloading\ncheckpoints. Reverted that diff and relaunched unchanged.\n\n# use updated fairscale\ncd ~/src/fairscale\ngit fetch origin prefetch_fsdp_params_simple\ngit checkout prefetch_fsdp_params_simple\ncd ~/src/fairseq-py\n# changes for restoring without downloading, and also changes to loss scale logic\ngit fetch origin sgd_withdownload_gshard_combine_megatron_fsdp\ngit checkout sgd_withdownload_gshard_combine_megatron_fsdp\nBLOB_URL=\"<<<SCRUBBED FOR RELEASE>>>\"\nRUN_ID=175B_run12.37\nINCLUDED_HOSTS=node-[1,3-70,72-87,89-112,114-128] \\\npython -m fb_sweep.opt.sweep_opt_en_lm_175b \\\n-n 124 -g 8 -t 1 \\\n-p $RUN_ID \\\n--checkpoints-dir /shared/home/namangoyal/checkpoints/175B/ \\\n--local-checkpoints-dir /mnt/scratch/$USER/checkpoints/2021-11-29.$RUN_ID \\\n--full-cloud-upload-path $BLOB_URL\n2021-11-20 7:24pm ET: [Stephen oncall]\nSam called out the loss scalar hitting minimum. Took action to requeue the job\nA few minutes later, Susan pointed out that wouldn’t work because we reverted the download-from-cloud\nchange.\nSee next entry for mitigation.\n2021-11-30 10:10am PT: 12.36 restart from 37k, SGD mimicking [WARNING: See\n2021-12-02 17:16 ET: Debrief on why it may not be SGD]\n# use updated fairscale\ncd ~/src/fairscale\ngit fetch origin prefetch_fsdp_params_simple\ngit checkout prefetch_fsdp_params_simple\ncd ~/src/fairseq-py\ngit fetch origin sgd_gshard_combine_megatron_fsdp\ngit checkout sgd_gshard_combine_megatron_fsdp\nCKPT_DIR=/data/users/susanz/checkpoints\nBLOB_URL=\"<<<SCRUBBED FOR RELEASE>>>\"\ncd $CKPT_DIR\ncp --recursive --include-pattern \"checkpoint_13_37000*.pt\" \"$BLOB_URL\" checkpoint_13_37000\nRESTORE_FILE=$CKPT_DIR/checkpoint_13_37000/175B_run12.14.me_fp16.minscale0.25.fsdp.gpf32.0.relu.transformer_lm_me\ngatron.nlay96.emb12288.lrnpos.0emb_scale.bm_none.tps2048.gpt2.adam.b2_0.95.eps1e-08.cl1.0.lr0.00012.endlr6e-06.wu\n2000.dr0.1.atdr0.1.0emb_dr.wd0.1.ms8.uf1.mu143052.s1.ngpu992/checkpoint_13_37000.pt\nRUN_ID=175B_run12.36\n\nINCLUDED_HOSTS=node-[1,3-70,72-87,89-112,114-128] \\\npython -m fb_sweep.opt.sweep_opt_en_lm_175b \\\n-n 124 -g 8 -t 1 \\\n-p $RUN_ID \\\n--checkpoints-dir /shared/home/namangoyal/checkpoints/175B/ \\\n--restore-file $RESTORE_FILE \\\n--full-cloud-upload-path $BLOB_URL\n●\nWas just about to launch and saw all nodes to launch are were in drain:\n(fairseq-20210913) susanz@ip-0A1E0404:~/fairseq-py$ sinfo\nPARTITION AVAIL  TIMELIMIT  NODES  STATE NODELIST\nhpc*         up   infinite      1 drain~ node-129\nhpc*         up   infinite      2  down~ node-[130-131]\nhpc*         up   infinite      1  idle~ node-132\nhpc*         up   infinite    124  drain node-[1,3-70,72-87,89-112,114-128]\nhpc*         up   infinite      4   idle node-[2,71,88,113]\n●\nsudo scontrol update node=node-[1,3-70,72-87,89-112,114-128] state=idle\n2021-11-30 9:00am PT: 12.35 restart from 37k, SGD mimicking [WARNING: See\n2021-12-02 17:16 ET: Debrief on why it may not be SGD]\n●\nDownloaded checkpoints for 37k.\n●\nCheckpoints got borked / clobbered by “always download cloud checkpoints” logic. Redownloading and\nreverting that change.\n●\nResized /data up to 85TB (was almost full at 70TB).\nNote: This never actually ran\n2021-11-30 9:00am ET: 12.34 requeue\n[Stephen] Looks like it’s got an enormous number of 0.25 loss scales, basically all night long. Requeueing.\n2021-11-29 7:43pm PT [Susan]: 12.34 restart\n●\nRestarting with 6e4124680c960a8bc584f2fb0d4404a232745e8b pulled in\n\n# use updated fairscale\ncd ~/src/fairscale\ngit fetch origin prefetch_fsdp_params_simple\ngit checkout prefetch_fsdp_params_simple\n# we use a single Cloud blob path for all checkpoints now\nPERMANENT_CLOUD_UPLOAD_PATH=\"<<<SCRUBBED FOR RELEASE>>>\"\n# increment the run ID so we don’t clobber tensorboard\nRUN_ID=175B_run12.34\n# relaunch training and restore checkpoint from Cloud blob storage\nINCLUDED_HOSTS=node-[1,3-70,72-87,89-112,114-128] \\\npython -m fb_sweep.opt.sweep_opt_en_lm_175b \\\n-n 124 -g 8 -t 1 \\\n--checkpoints-dir /shared/home/namangoyal/checkpoints/175B/ \\\n--local-checkpoints-dir /mnt/scratch/$USER/checkpoints/2021-11-29.$RUN_ID \\\n--full-cloud-upload-path $PERMANENT_CLOUD_UPLOAD_PATH \\\n-p $RUN_ID\n2021-11-30 7:43pm PT [Susan]: 12.33 requeue\n●\nGot stuck for 510 iterations with gradient overflow / loss scale at 0.25\n●\nsudo scontrol requeue job=2229\n○\nTakes about 5 minutes to get nodelist out of (BeginTime) state\n●\nThis doesn’t look right:\n\n●\nSeems like 438 checkpoints downloaded while 554 didn’t. Unclear if this is intended behavior. Since the\njob still hasn’t started after over an hour, scanceling and starting from a new checkpoint dir with a new\nrun id.\n2021-11-28 6:34pm ET [Stephen]: 12.33\npg0-2 threw an illegal memory exception. Swapped for 90 and began to replace node node-2.\nSuccessfully 15 updates in at 7:04pm.\n# use updated fairscale\ncd ~/src/fairscale\ngit fetch origin prefetch_fsdp_params_simple\ngit checkout prefetch_fsdp_params_simple\n# we use a single Cloud blob path for all checkpoints now\nPERMANENT_CLOUD_UPLOAD_PATH=\"<<<SCRUBBED FOR RELEASE>>>\"\n# increment the run ID so we don’t clobber tensorboard\nRUN_ID=175B_run12.32\n# relaunch training and restore checkpoint from Cloud blob storage\nINCLUDED_HOSTS=node-[1,3-70,72-87,89-112,114-128] \\\npython -m fb_sweep.opt.sweep_opt_en_lm_175b \\\n-n 124 -g 8 -t 1 \\\n--checkpoints-dir /shared/home/namangoyal/checkpoints/175B/ \\\n--local-checkpoints-dir /mnt/scratch/$USER/checkpoints/2021-11-28.$RUN_ID \\\n--full-cloud-upload-path $PERMANENT_CLOUD_UPLOAD_PATH \\\n-p $RUN_ID\n\n2021-11-28 5:52pm ET [Stephen]: 12.32\n●\nLooks like we’ve hit the zero-message hang. It’s been almost 20 minutes without updates. We’re at\n33658, considerably further than we got before.\n●\nRan pdsh -w 'node-[1,3-87,89,91-112,114-128]' -R ssh\nnvidia-smi | vim -\n○\nObserved that pg0-71 gave an exit code 15 and pg0-118 gave an exit code 14.\n■\nFrom https://developer.download.nvidia.com/compute/DCGM/docs/nvidia-smi-367.38.pdf\n■\nReturn code 14 - infoROM is corrupted\n■\nReturn code 15 - The GPU has fallen off the bus or has otherwise become inaccessible\n○\nAll other hosts returned the expected 100% utilization from being blocked. We knew about\npg0-118 and infoROM. Investigating pg0-71\n●\nLspci shows the right number of devices\n●\nInvestigating pg0-71\n○\n[putting off running nvidia-smi in case it hangs]\n○\nHtop: Shows 24 threads pegged. (By comparison, pg0-70 shows 24 threads pegged)\n○\nstraced 2 child threads pegged and found them in sched_yield. Straced a parent process was\npegged in futex_wait.\n○\nGdb backtrace of a parent thread actually showed a rich backtrace (compared to the simple\n“stuck in cudart.so” I saw previously)\n■\nFirst non-OS Bt call I see:\n■\n0x00007f2efbed1d6b in torch::autograd::ReadyQueue::pop() () from\n/shared/home/roller/miniconda3/envs/fairseq-20210913/lib/python3.8/site-packages/torc\nh/lib/libtorch_cpu.so\n■\nPg0-70 shows itself stuck in the same place\n○\nRerunning pdsh nvidia-smi repeats pg0-71 as the problem node\n○\nFinally running nvidia-smi on pg0-71 gives “Unable to determine the device handle for GPU\n000E:00:00.0: GPU is lost.  Reboot the system to recover this GPU”\n●\nMitigations taken:\n○\nLaunching 12.32, replacing pg0-71 with pg0-2\n■\nThought about doing a scontrol hotswap, but because i have the --restore-file in my\narguments I decided against it, falling back to cloud checkpoints this time.\n○\n[in progress] Rebooting and then maybe replacing pg0-71\n# use updated fairscale\ncd ~/src/fairscale\ngit fetch origin prefetch_fsdp_params_simple\ngit checkout prefetch_fsdp_params_simple\n# we use a single Cloud blob path for all checkpoints now\nPERMANENT_CLOUD_UPLOAD_PATH=\"<<<SCRUBBED FOR RELEASE>>>\"\n# increment the run ID so we don’t clobber tensorboard\nRUN_ID=175B_run12.32\n# relaunch training and restore checkpoint from Cloud blob storage\nINCLUDED_HOSTS=node-[1-70,72-87,89,91-112,114-128] \\\npython -m fb_sweep.opt.sweep_opt_en_lm_175b \\\n-n 124 -g 8 -t 1 \\\n--checkpoints-dir /shared/home/namangoyal/checkpoints/175B/ \\\n--local-checkpoints-dir /mnt/scratch/$USER/checkpoints/2021-11-28.$RUN_ID \\\n--full-cloud-upload-path $PERMANENT_CLOUD_UPLOAD_PATH \\\n-p $RUN_ID\n\n2021-11-28 12:28pm ET [Stephen]: 12.31\nSee previous entry (below run block) for context.\n●\n12:42pm ET - looks like we’re tokenizing. ETA  based on 12.30 is 1:17pm.\n●\n1:15pm ET - Pg0-90 doesn’t look like ssh is recovering from a reboot. Successfully replaced and\nmarked as approved for usage\n●\n1:21pm ET - job is making updates\n●\n1:38pm ET - at 50 updates!!! I’m going for lunch.\n●\n14:52 ET - got past 250 updates. New checkpoint saved. Life is good.\n# use updated fairscale\ncd ~/src/fairscale\ngit fetch origin prefetch_fsdp_params_simple\ngit checkout prefetch_fsdp_params_simple\n# we use a single Cloud blob path for all checkpoints now\nPERMANENT_CLOUD_UPLOAD_PATH=\"<<<SCRUBBED FOR RELEASE>>>\"\nCKPT_DIR=/data/users/roller/175B_run12.27_restore/checkpoint_11_33416/175B_run12.14.me_fp16.minscale0.25.fsdp.gpf\n32.0.relu.transformer_lm_megatron.nlay96.emb12288.lrnpos.0emb_scale.bm_none.tps2048.gpt2.adam.b2_0.95.eps1e-08.cl\n1.0.lr0.00012.endlr6e-06.wu2000.dr0.1.atdr0.1.0emb_dr.wd0.1.ms8.uf1.mu143052.s1.ngpu992\nRESTORE_FILE=$CKPT_DIR/checkpoint_11_33000.pt\n# increment the run ID so we don’t clobber tensorboard\nRUN_ID=175B_run12.31\n# relaunch training and restore checkpoint from Cloud blob storage\nINCLUDED_HOSTS=node-[1,3-87,89,91-112,114-128] \\\npython -m fb_sweep.opt.sweep_opt_en_lm_175b \\\n-n 124 -g 8 -t 1 \\\n--checkpoints-dir /shared/home/namangoyal/checkpoints/175B/ \\\n--local-checkpoints-dir /mnt/scratch/$USER/checkpoints/2021-11-28.$RUN_ID \\\n--full-cloud-upload-path $PERMANENT_CLOUD_UPLOAD_PATH \\\n--restore-file $RESTORE_FILE \\\n-p $RUN_ID\n2021-11-28 10:09am ET [Stephen]: 12.30\n●\n12.29 failed with the same `filename storages not found`\n○\nSince the exception said pg0-55, i ssh’d into it and tried manually loading its checkpoints. All 6\nparts got the same storages exception!\n○\nI could replicate this with the storages I had manually downloaded\n○\nConclusion: 33250 checkpoints are corrupt. Maybe from R12.26 and R12.25 aggressively\noverwriting the checkpoints.\n●\nRemedies taken (status: monitoring for success)\n○\nAdd a quick patch to avoid the constant-rewrite bug witnessed. Testing in prod.\n■\nNote I now have local backups of 33000 and 33416.x\n○\nSince I don’t like that 33250 is corrupted (I would rather us not have any corrupt checkpoints), I\nam rolling back to 33000.\n○\nThis is frustrating since we have 33416 which is the epoch boundary, but let’s just bite the bullet\n\n# use updated fairscale\ncd ~/src/fairscale\ngit fetch origin prefetch_fsdp_params_simple\ngit checkout prefetch_fsdp_params_simple\n# we use a single Cloud blob path for all checkpoints now\nPERMANENT_CLOUD_UPLOAD_PATH=\"<<<SCRUBBED FOR RELEASE>>>\"\nCKPT_DIR=/data/users/roller/175B_run12.27_restore/checkpoint_11_33416/175B_run12.14.me_fp16.minscale0.25.fsdp.gpf\n32.0.relu.transformer_lm_megatron.nlay96.emb12288.lrnpos.0emb_scale.bm_none.tps2048.gpt2.adam.b2_0.95.eps1e-08.cl\n1.0.lr0.00012.endlr6e-06.wu2000.dr0.1.atdr0.1.0emb_dr.wd0.1.ms8.uf1.mu143052.s1.ngpu992\nRESTORE_FILE=$CKPT_DIR/checkpoint_11_33000.pt\n# increment the run ID so we don’t clobber tensorboard\nRUN_ID=175B_run12.30\n# relaunch training and restore checkpoint from Cloud blob storage\nINCLUDED_HOSTS=node-[1,3-87,89-112,114-117,119-128] \\\npython -m fb_sweep.opt.sweep_opt_en_lm_175b \\\n-n 124 -g 8 -t 1 \\\n--checkpoints-dir /shared/home/namangoyal/checkpoints/175B/ \\\n--local-checkpoints-dir /mnt/scratch/$USER/checkpoints/2021-11-28.$RUN_ID \\\n--full-cloud-upload-path $PERMANENT_CLOUD_UPLOAD_PATH \\\n--restore-file $RESTORE_FILE \\\n-p $RUN_ID\nLaunched at 11:00am ET\n●\n11:23am - nodes just finished loading checkpoints\n●\n11:57am - still tokenizing, I think. We need to add a log line for after a checkpoint was successfully\nloaded and when the iterator is successfully fast forwarded\n●\n12:03am - looks like gpus are finally burning electricity, and we have our first step\n○\nLooks like the hot patch for checkpointing didn’t get triggered here: loss scale didn’t need to be\nlowered on the first step\n○\nWPS looks healthy for now. Monitoring for hangs\n●\n12:08pm Started replacing pg0-2\n●\n12:18pm Looks like we’ve hung again. No error message. Made it 26 updates.\nWhile it was still running, tried pdsh nvidia-smi. Found nvidia-smi was hanging in node-90 when I eventually\nctrl-c’d nvidia-smi. SSH’ing in and running nvidia-smi seemed to also hang.\nSwapping for pg0-118, which has the inforam message but that was only a guess. Also attempting to reboot\npg0-90\n2021-11-28 9:41am ET [Stephen]: 12.29\n●\nWow this time we didn’t even really get past init. On pg0-2: RuntimeError: CUDA error: an illegal\nmemory access was encountered\n●\nNote this is the same machine that gave me issues before\n●\nNvidia-smi shows no issue\nSince 40 managed to successfully re-init overnight, swapping 2 for 40.\n\n# use updated fairscale\ncd ~/src/fairscale\ngit fetch origin prefetch_fsdp_params_simple\ngit checkout prefetch_fsdp_params_simple\n# we use a single Cloud blob path for all checkpoints now\nPERMANENT_CLOUD_UPLOAD_PATH=\"<<<SCRUBBED FOR RELEASE>>>\"\n# increment the run ID so we don’t clobber tensorboard\n# note we have the SECOND tensorboard clobbering fix, so we can requeue after this\nRUN_ID=175B_run12.29\n# relaunch training and restore checkpoint from Cloud blob storage\nINCLUDED_HOSTS=node-[1,3-87,89-112,114-117,119-128] \\\npython -m fb_sweep.opt.sweep_opt_en_lm_175b \\\n-n 124 -g 8 -t 1 \\\n--checkpoints-dir /shared/home/namangoyal/checkpoints/175B/ \\\n--local-checkpoints-dir /mnt/scratch/$USER/checkpoints/2021-11-28.0 \\\n--full-cloud-upload-path $PERMANENT_CLOUD_UPLOAD_PATH \\\n-p $RUN_ID\n2021-11-28 3:20am ET [Stephen]: 12.28\nLast ditch resort. Doing the exact same as 27 except bumping the storage directory version.\nCheckpoint_last is still downloading. See 12.27 entry for more info\n# use updated fairscale\ncd ~/src/fairscale\ngit fetch origin prefetch_fsdp_params_simple\ngit checkout prefetch_fsdp_params_simple\n# we use a single Cloud blob path for all checkpoints now\nPERMANENT_CLOUD_UPLOAD_PATH=\"<<<SCRUBBED FOR RELEASE>>>\"\n# increment the run ID so we don’t clobber tensorboard\n# note we have the SECOND tensorboard clobbering fix, so we can requeue after this\nRUN_ID=175B_run12.28\n# relaunch training and restore checkpoint from Cloud blob storage\nINCLUDED_HOSTS=node-[1-39,41-87,89-112,114-117,119-128] \\\npython -m fb_sweep.opt.sweep_opt_en_lm_175b \\\n-n 124 -g 8 -t 1 \\\n--checkpoints-dir /shared/home/namangoyal/checkpoints/175B/ \\\n--local-checkpoints-dir /mnt/scratch/$USER/checkpoints/2021-11-27.3 \\\n--full-cloud-upload-path $PERMANENT_CLOUD_UPLOAD_PATH \\\n-p $RUN_ID\n2021-11-28 1:50am ET [Stephen]: 12.27\nLooks like 26 tried to immediately upload a checkpoint and failed its cp commands! Then it took another step,\nlowered its scalar, and tried uploading again! And again! The humanity! We’re already at loss scale 0.25.\nAfter it did this 3 times, it looks like it’s hanging again. Some CPUs look pegged but using strace on the\nprocesses displays only sched_yield, so it’s probably just an idle loop.\n\nGiven this is our third hang in a row, somewhere must have something wrong. Leaving the job up to check on\nit.\nSsh’d into pg0-39 and ran “gdb -p <pid>” on one of the fairseq processes. Confirmed we were hanging in\n0x00007ff8d76eecb1 in ?? () from /usr/lib/x86_64-linux-gnu/libcuda.so.1\nEither we have a bug in our code causing workers to be out of sync, or there’s a bad node somewhere.\nInitiated a cluster wide nccl test, it came back a bit slow(161 but using all nodes) but it finished.\nGlobal run of nvidia-smi didn’t find any uncorrectable errors, but pg-88 did show 506k correctable ones; and\npg-23 had 170k correctable errors. Other nodes showed at most a couple thousand. Decided to launch again\nreplacing 88 with 7.\n# use updated fairscale\ncd ~/src/fairscale\ngit fetch origin prefetch_fsdp_params_simple\ngit checkout prefetch_fsdp_params_simple\n# we use a single Cloud blob path for all checkpoints now\nPERMANENT_CLOUD_UPLOAD_PATH=\"<<<SCRUBBED FOR RELEASE>>>\"\n# increment the run ID so we don’t clobber tensorboard\n# note we have the SECOND tensorboard clobbering fix, so we can requeue after this\nRUN_ID=175B_run12.27\n# relaunch training and restore checkpoint from Cloud blob storage\nINCLUDED_HOSTS=node-[1-39,41-87,89-112,114-117,119-128] \\\npython -m fb_sweep.opt.sweep_opt_en_lm_175b \\\n-n 124 -g 8 -t 1 \\\n--checkpoints-dir /shared/home/namangoyal/checkpoints/175B/ \\\n--local-checkpoints-dir /mnt/scratch/$USER/checkpoints/2021-11-27.2 \\\n--full-cloud-upload-path $PERMANENT_CLOUD_UPLOAD_PATH \\\n-p $RUN_ID\nThis one failed pretty fast with “KeyError: \"filename 'storages' not found\"” inside tarfile.py, suggesting that\nsomething got corrupted.\n-\nHypothesis: some node is hanging on uploading checkpoints?\n-\nThat doesn’t make sense with Susan’s hang some 8 steps in.\nObserved 2 nodes (7 and 17) had checkpoint downloads that were the wrong file size.\nIn parallel, downloaded all our latest checkpoints (including checkpoint_last) and putting them in\n/data/users/roller/175B_run12.27_restore/checkpoint_11_33416/175B_run12.14.me_fp16.minscale0.25.fsdp.gpf32.0.relu.transformer_lm_megatron.nl\nay96.emb12288.lrnpos.0emb_scale.bm_none.tps2048.gpt2.adam.b2_0.95.eps1e-08.cl1.0.lr0.00012.endlr6e-06.wu2000.dr0.1.atdr0.1.0emb_dr.wd0.1.ms8\n.uf1.mu143052.s1.ngpu992\nA good next move might be to roll back to 33000 (which is downloaded) or to checkpoint_last (33461).\n2021-11-27 11:39 ET [Stephen]: Run 12.26\n●\nLooks like things are hanging, debugging\n●\nStephen: fixing the TB clobber overgenerate thing\n●\nnvidia-smi on:\n○\nnode-40: WARNING: infoROM is corrupted at gpu 000B:00:00.0\n\n■\n9:07pm PT: rebooting; 9:11pm alive again\n■\ninforam message still appears!\n■\nCurrently replacing\n○\nnode-113: WARNING: infoROM is corrupted at gpu 0002:00:00.0\n■\n9:08pm PT: rebooting; 9:13pm alive again\n■\ninforam message still appears!\n○\nhttps://forums.developer.nvidia.com/t/inforom-is-corrupted-at-gpu/74277\n■\n“There is no publicly available utility to fix this. The card is damaged. Unless it is under\nwarranty, there isn’t anything you can do to repair it.”\n●\nRe: possible replacement nodes\n○\nnode-118 seems to have MIG enabled; Myle has noticed this in the past and we should add\nchecks for this to our automation scripts. The fix is to do `sudo nvidia-smi -mig 0` and then\nrestart.\n■\n9pm PT: ran `sudo nvidia-smi -mig 0` and rebooted node-118\n■\n9:07pm PT: node-118 came back without MIG\n■\n9:10pm PT: ran NCCL tests on node-[3,118], but got only 144GB/s instead of 189GB/s!\n■\nConclusion: node-118 has bad IB\n●\nCurrently replacing\n○\nnode-65 seems to be in a weird state. NCCL tests on node-[12,65] showed 65 with an error:\n`Test CUDA failure common.cu:762 'all CUDA-capable devices are busy or unavailable'`\n■\n9:06pm PT: rebooted node-65 (and also node-12 just because)\n■\n9:15pm PT: NCCL tests on node-[3,65] came back good (189GB/s)\n■\nConclusion: node-65 is healthy\n○\nnode-12: rebooted as part of debug process above\n■\n9:13pm PT: NCCL tests on node-[3,12] came back good (189GB/s)\n■\nConclusion: node-12 is healthy\n●\nWhoa, while fixing the tboard bug, ran test case on node-2 and got:\n○\nRuntimeError: CUDA error: misaligned address\n○\nO_o -- resolution: pray this is nothing\nOther weird thing noticed in the logs: It seems like we immediately dumped a checkpoint before moving any\niterations… 12.24 didn’t do this, neither did 12.23!\nNote: 12.26 seems to be doing this too, we may wish to roll back to 33000. Also note the last epoch was\nat 33416! We’re already rolling back!\n2021-11-28 02:35:19 | WARNING | fairseq.cloud_utils | [rank 830] done with cloud download in 788.0 Seconds\n2021-11-28 03:00:18 | INFO | fairseq.trainer | begin training epoch 11\n2021-11-28 03:00:18 | INFO | fairseq_cli.train | Start iterating over samples\n2021-11-28 03:12:26 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 2.0\n2021-11-28 03:12:26 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 11 @ 33250 updates\n2021-11-28 03:12:26 | INFO | fairseq.trainer | Saving checkpoint to\n/mnt/scratch/susanz/checkpoints/2021-11-27/175B_run12.25.me_fp16.minscale0.25.fsdp.gpf32.0.relu.transformer_lm_megatron.nlay96.emb12288.l\nrnpos.0emb_scal\n2021-11-28 03:12:27 | INFO | fairseq.trainer | Finished saving checkpoint to\n/mnt/scratch/susanz/checkpoints/2021-11-27/175B_run12.25.me_fp16.minscale0.25.fsdp.gpf32.0.relu.transformer_lm_megatron.nlay96.emb12288.l\nrnpos.\n2021-11-28 03:12:27 | INFO | fairseq.checkpoint_utils | Saved checkpoint\n/mnt/scratch/susanz/checkpoints/2021-11-27/175B_run12.25.me_fp16.minscale0.25.fsdp.gpf32.0.relu.transformer_lm_megatron.nlay96.emb12288.l\nrnpos.0emb\n2021-11-28 03:12:31 | INFO | fairseq_cli.train | preparing to copy\n/mnt/scratch/susanz/checkpoints/2021-11-27/175B_run12.25.me_fp16.minscale0.25.fsdp.gpf32.0.relu.transformer_lm_megatron.nlay96.emb12288.l\nrnpos.0emb_sca\n2021-11-28 03:12:50 | INFO | train_inner | {\"epoch\": 11, \"gnorm__fsdp_wrapped…[truncated]\n\nNew Launch\n# use updated fairscale\ncd ~/src/fairscale\ngit fetch origin prefetch_fsdp_params_simple\ngit checkout prefetch_fsdp_params_simple\n# we use a single Cloud blob path for all checkpoints now\nPERMANENT_CLOUD_UPLOAD_PATH=\"<<<SCRUBBED FOR RELEASE>>>\"\n# increment the run ID so we don’t clobber tensorboard\n# note we have the SECOND tensorboard clobbering fix, so we can requeue after this\nRUN_ID=175B_run12.26\n# relaunch training and restore checkpoint from Cloud blob storage\nINCLUDED_HOSTS=node-[1-6,8-39,41-112,114-117,119-128] \\\npython -m fb_sweep.opt.sweep_opt_en_lm_175b \\\n-n 124 -g 8 -t 1 \\\n--checkpoints-dir /shared/home/namangoyal/checkpoints/175B/ \\\n--local-checkpoints-dir /mnt/scratch/$USER/checkpoints/2021-11-27 \\\n--full-cloud-upload-path $PERMANENT_CLOUD_UPLOAD_PATH \\\n-p $RUN_ID\n2021-11-27 6:10pm PT: Run 12.25 [Susan restart]\n●\nNoticed job stuck for ~4 hours.\n●\nTrying sudo scontrol requeue job=2194 to see if things can restart smoothly.\n○\nscanceling and relaunching with new id instead. We’re out of tb directories.\n# use updated fairscale\ncd ~/src/fairscale\ngit fetch origin prefetch_fsdp_params_simple\ngit checkout prefetch_fsdp_params_simple\n# we use a single Cloud blob path for all checkpoints now\nPERMANENT_CLOUD_UPLOAD_PATH=\"<<<SCRUBBED FOR RELEASE>>>\"\n# increment the run ID so we don’t clobber tensorboard\nRUN_ID=175B_run12.25\n# relaunch training and restore checkpoint from Cloud blob storage\nINCLUDED_HOSTS=node-[1-6,8-11,13-64,66-117,119-128] \\\npython -m fb_sweep.opt.sweep_opt_en_lm_175b \\\n-n 124 -g 8 -t 1 \\\n--checkpoints-dir /shared/home/namangoyal/checkpoints/175B/ \\\n--local-checkpoints-dir /mnt/scratch/$USER/checkpoints/2021-11-27 \\\n--full-cloud-upload-path $PERMANENT_CLOUD_UPLOAD_PATH \\\n-p $RUN_ID\n●\nStuck again - after 8 updates.\n2021-11-27 10:59am PT: Run 12.24 [Myle rerunning job, but AFK rest of day]\n●\n...tbZ already exists. Ran out of possible suffixes.\n●\nRelaunching with new RUN_ID\n\n# use updated fairscale\ncd ~/src/fairscale\ngit fetch origin prefetch_fsdp_params_simple\ngit checkout prefetch_fsdp_params_simple\n# we use a single Cloud blob path for all checkpoints now\nPERMANENT_CLOUD_UPLOAD_PATH=\"<<<SCRUBBED FOR RELEASE>>>\"\n# increment the run ID so we don’t clobber tensorboard\nRUN_ID=175B_run12.24\n# relaunch training and restore checkpoint from Cloud blob storage\nINCLUDED_HOSTS=node-[1-6,8-11,13-64,66-117,119-128] \\\npython -m fb_sweep.opt.sweep_opt_en_lm_175b \\\n-n 124 -g 8 -t 1 \\\n--checkpoints-dir /shared/home/namangoyal/checkpoints/175B/ \\\n--local-checkpoints-dir /mnt/scratch/$USER/checkpoints/2021-11-2 \\\n--full-cloud-upload-path $PERMANENT_CLOUD_UPLOAD_PATH \\\n-p $RUN_ID\n2021-11-26 9:47am ET [Stephen managing cluster]\n●\nManaged to get new nodes for pg0-7, pg0-85\n○\nBoth are showing clean on the ECC uncorrectables\n○\nBoth have been marked idle and safe to use\n○\nI didn’t touch pg0-88 but it also looks safe to use (maybe Susan did this one)\n●\nSuccessfully updated hosts mapping and initialized the scratch directories\n○\nNOTE: Added these two lines to my ~/.ssh/config\n○\nHost node-*\n○\nStrictHostKeyChecking no\n●\npg0-118 is looking a bit funny.\n○\nSome of its ECC’s logs say\n■\nSRAM Uncorrectable            : N/A\n■\nDRAM Uncorrectable            : N/A\n○\nOthers only show 0’s, not N/A’s\n○\nRemedy: Draining it, rebooting it\n○\nAfter several minutes, host did not seem to recover from reboot\n■\nStarted to re-alloc\n■\nAnd then it came right back! Omg. while it was in the process of terminating\n■\nMaybe I should’ve been slightly more patient :(. Maybe I needed to set it to “drain”\ninstead of “drain*”\n■\nFortunately the release and reclaim was fast\n●\nCurrently running reallocs:\n○\nPg0-129 (Stuck on acquiring)\n○\nPg0-118 (Current “creating vm” 10:18am)\n●\nNow getting on a plane. Will pray for success.\nSome observations:\n●\nIt seems like there might be a near 1:1 mapping between our hosts and what csp gives us. For\nexample, if we already have a node and we terminate it, then restart it, then we seem to get an\n\nallocation immediately. However, nodes like 129--132 are simply never allocated (even though 118 was\nmade available while that was queuing!)\n2021-11-25 8:53am ET [Susan]: Run 12.23\nRun 22.22 got stuck after IB issues cropped up and everyone came to rescue it at the exact same time:\nmlx5: buo1u00003A: got completion with error:\n00000000 00000000 00000000 00000000\n00000000 00000000 00000000 00000000\n0000000d 00000000 00000000 00000000\n00000000 02005104 08001219 38dcf5d2\nLost node-118 GPU (automatically put on drain after job was scanceled):\nUnable to determine the device handle for GPU 0001:00:00.0: GPU is lost.  Reboot the system to\nrecover this GPU\nDRAM Uncorrectable on node-7:\nGPU 0000000D:00:00.0\nEcc Mode\nCurrent                           : Enabled\nPending                           : Enabled\nECC Errors\nVolatile\nSRAM Correctable              : 0\nSRAM Uncorrectable            : 0\nDRAM Correctable              : 0\nDRAM Uncorrectable            : 0\nAggregate\nSRAM Correctable              : 0\nSRAM Uncorrectable            : 0\nDRAM Correctable              : 10\nDRAM Uncorrectable            : 101\n●\nsudo scontrol update node=node-7 state=drain reason=dram\nCurrent list of node states:\n$ sinfo\nPARTITION AVAIL  TIMELIMIT  NODES  STATE NODELIST\nhpc*         up   infinite      2  down~ node-[129-130]\nhpc*         up   infinite      2  idle~ node-[131-132]\nhpc*         up   infinite      3  drain node-[7,85,118]\nhpc*         up   infinite    125   idle node-[1-6,8-84,86-117,119-128]\nHigh amount of DRAM correctable errors on node-88 (506807), excluding to be safe.\n\n# use updated fairscale\ncd ~/src/fairscale\ngit fetch origin prefetch_fsdp_params_simple\ngit checkout prefetch_fsdp_params_simple\n# we use a single Cloud blob path for all checkpoints now\nPERMANENT_CLOUD_UPLOAD_PATH=\"<<<SCRUBBED FOR RELEASE>>>\"\n# increment the run ID so we don’t clobber tensorboard\nRUN_ID=175B_run12.23\n# relaunch training and restore checkpoint from Cloud blob storage\nINCLUDED_HOSTS=node-[1-6,8-84,86-87,89-117,119-128] \\\npython -m fb_sweep.opt.sweep_opt_en_lm_175b \\\n-n 124 -g 8 -t 1 \\\n--checkpoints-dir /shared/home/namangoyal/checkpoints/175B/ \\\n--local-checkpoints-dir /mnt/scratch/$USER/checkpoints/2021-11-25 \\\n--full-cloud-upload-path $PERMANENT_CLOUD_UPLOAD_PATH \\\n-p $RUN_ID\n●\nNew log dir:\n/shared/home/namangoyal/checkpoints/175B/175B_run12.23.me_fp16.minscale0.25.fsdp.gpf32.0.relu.t\nransformer_lm_megatron.nlay96.emb12288.lrnpos.0emb_scale.bm_none.tps2048.gpt2.adam.b2_0.95.\neps1e-08.cl0.3.lr0.00012.endlr6e-06.wu2000.dr0.1.atdr0.1.0emb_dr.wd0.1.ms8.uf1.mu143052.s1.ngpu\n992\n2021-11-25 11:35am ET [Myle]: Run 12.22\n●\nNvm, realized this won’t resume properly due to cache, and will clobber tensorboard; will increment run\nID instead\n●\nAdd 3rd party timeout to handle blob retries\n○\n#2686\n○\ntimeout-decorator is a new dependency\n■\npip install timeout-decorator\n●\nRe: spare nodes:\n○\nRan ./scripts/nccl_tests/cloud/run_nccl_allreduce.sh to validate NCCL perf\n○\nConfirmed node-[1,7,47] are good\n○\nDrained ​node-85, which had bad NCCL perf (only 140GB/s instead of 180)\n# use updated fairscale\ncd ~/src/fairscale\ngit fetch origin prefetch_fsdp_params_simple\ngit checkout prefetch_fsdp_params_simple\n# we use a single Cloud blob path for all checkpoints now\nPERMANENT_CLOUD_UPLOAD_PATH=\"<<<SCRUBBED FOR RELEASE>>>\"\n# increment the run ID so we don’t clobber tensorboard\nRUN_ID=175B_run12.22\n# relaunch training and restore checkpoint from Cloud blob storage\nINCLUDED_HOSTS=node-[1,3-42,44-57,59-84,86-128] \\\npython -m fb_sweep.opt.sweep_opt_en_lm_175b \\\n-n 124 -g 8 -t 1 \\\n\n--checkpoints-dir /shared/home/namangoyal/checkpoints/175B/ \\\n--local-checkpoints-dir /mnt/scratch/$USER/checkpoints/2021-11-25 \\\n--full-cloud-upload-path $PERMANENT_CLOUD_UPLOAD_PATH \\\n-p $RUN_ID\n2021-11-25 11:20am ET: Run 12.21 (requeue)\n●\nTraining got stuck in loss scale 0.25 loop, after step 28628\n●\nDid scontrol requeue job=2167 to see if it magically fixes after restarting from checkpoint\n2021-11-24 11:18pm ET [Susan]: Run 12.21\n●\nTesting out #2681\n●\nRemove node 58, replacing with 7.\n○\nNode 58 came up with GPUs! But, there’s this:\nDRAM Correctable              : 18446744073709551615\n○\nsudo scontrol update node=node-58 state=drain reason=dramtoast\n# use updated fairscale\ncd ~/src/fairscale\ngit fetch origin prefetch_fsdp_params_simple\ngit checkout prefetch_fsdp_params_simple\n# we use a single Cloud blob path for all checkpoints now\nPERMANENT_CLOUD_UPLOAD_PATH=\"<<<SCRUBBED FOR RELEASE>>>\"\n# increment the run ID so we don’t clobber tensorboard\nRUN_ID=175B_run12.21\n# relaunch training and restore checkpoint from Cloud blob storage\nINCLUDED_HOSTS=node-[1,3-42,44-57,59-84,86-128] \\\npython -m fb_sweep.opt.sweep_opt_en_lm_175b \\\n-n 124 -g 8 -t 1 \\\n--checkpoints-dir /shared/home/namangoyal/checkpoints/175B/ \\\n--local-checkpoints-dir /mnt/scratch/susanz/checkpoints/2021-11-24.3 \\\n--full-cloud-upload-path $PERMANENT_CLOUD_UPLOAD_PATH \\\n-p $RUN_ID\n2021-11-24 10:40pm ET [Susan]: Run 12.20\n●\nTesting out #2681\n\n# use updated fairscale\ncd ~/src/fairscale\ngit fetch origin prefetch_fsdp_params_simple\ngit checkout prefetch_fsdp_params_simple\n# we use a single Cloud blob path for all checkpoints now\nPERMANENT_CLOUD_UPLOAD_PATH=\"<<<SCRUBBED FOR RELEASE>>>\"\n# increment the run ID so we don’t clobber tensorboard\nRUN_ID=175B_run12.20\n# relaunch training and restore checkpoint from Cloud blob storage\nINCLUDED_HOSTS=node-[1,3-6,8-42,44-84,86-128] \\\npython -m fb_sweep.opt.sweep_opt_en_lm_175b \\\n-n 124 -g 8 -t 1 \\\n--checkpoints-dir /shared/home/namangoyal/checkpoints/175B/ \\\n--local-checkpoints-dir /mnt/scratch/susanz/checkpoints/2021-11-24.3 \\\n--full-cloud-upload-path $PERMANENT_CLOUD_UPLOAD_PATH \\\n-p $RUN_ID\n●\nLost GPU on node-58:\n(base) susanz@buo1u00001P:~$ nvidia-smi\nUnable to determine the device handle for GPU 0001:00:00.0: GPU is lost.  Reboot the system to\nrecover this GPU\n●\nsudo scontrol update node=node-58 state=drain reason=lostgpu\n2021-11-24 3:30pm ET [Susan]: Run 12.19\n●\nLaunching with node 85. Bringing in 47 instead.\n# use updated fairscale\ncd ~/src/fairscale\ngit fetch origin prefetch_fsdp_params_simple\ngit checkout prefetch_fsdp_params_simple\n# we use a single Cloud blob path for all checkpoints now\nPERMANENT_CLOUD_UPLOAD_PATH=\"<<<SCRUBBED FOR RELEASE>>>\"\n# increment the run ID so we don’t clobber tensorboard\nRUN_ID=175B_run12.19\n# relaunch training and restore checkpoint from Cloud blob storage\nINCLUDED_HOSTS=node-[1,3-6,8-42,44-84,86-128] \\\npython -m fb_sweep.opt.sweep_opt_en_lm_175b \\\n-n 124 -g 8 -t 1 \\\n--checkpoints-dir /shared/home/namangoyal/checkpoints/175B/ \\\n--local-checkpoints-dir /mnt/scratch/susanz/checkpoints/2021-11-24.2 \\\n--full-cloud-upload-path $PERMANENT_CLOUD_UPLOAD_PATH \\\n-p $RUN_ID\n●\nHung for 5 hours - missing 4 shards.\n\n2021-11-24 2:10pm ET [Susan]: Run 12.18\n●\nRelaunched exactly the same as 12.17 to see if the hanging was just data loading fast forwarding.\n●\nIt’s actually something else?\n○\nSomething is taking half an hour before “begin training epoch 9”\n○\nWe don’t seem to print all ranks for cloud download.\n# use updated fairscale\ncd ~/src/fairscale\ngit fetch origin prefetch_fsdp_params_simple\ngit checkout prefetch_fsdp_params_simple\n# we use a single Cloud blob path for all checkpoints now\nPERMANENT_CLOUD_UPLOAD_PATH=\"<<<SCRUBBED FOR RELEASE>>>\"\n# increment the run ID so we don’t clobber tensorboard\nRUN_ID=175B_run12.18\n# relaunch training and restore checkpoint from Cloud blob storage\nINCLUDED_HOSTS=node-[1,3-6,8-42,44-46,48-128] \\\npython -m fb_sweep.opt.sweep_opt_en_lm_175b \\\n-n 124 -g 8 -t 1 \\\n--checkpoints-dir /shared/home/namangoyal/checkpoints/175B/ \\\n--local-checkpoints-dir /mnt/scratch/susanz/checkpoints/2021-11-24.1 \\\n--full-cloud-upload-path $PERMANENT_CLOUD_UPLOAD_PATH \\\n-p $RUN_ID\n2021-11-24 1:00pm ET [Susan]: Run 12.17\n●\nRun 12.16 got stuck at step 27247 after hitting ECC error.\n●\nWCOLL=~myleott/hosts PDSH_RCMD_TYPE=ssh pdsh nvidia-smi -q -d \"ECC\" > ecc_error.log\n●\nShows 2 and 43 with ECC errors. Restarting in cloud UI.\n○\nTaking a long time to provision for some reason.\n○\n2 failed to provision with:\n“Error: ProvisioningState/failed/OSProvisioningTimedOut OS Provisioning for VM\nnv6ormrlczgkx_245' did not finish in the allotted time. The VM may still finish provisioning\nsuccessfully. Please check provisioning state later.”\nShutting down and restarting got:\n“Reimaging virtual machine due to error on creation”\n●\nAlso restarting 85 since VM failed to start for some reason\n\n●\n85 came back. Keep 7,47 removed in case they’re the bad IB nodes. Taking a gamble on 1.\n●\n43 came back as well.\n# use updated fairscale\ncd ~/src/fairscale\ngit fetch origin prefetch_fsdp_params_simple\ngit checkout prefetch_fsdp_params_simple\n# we use a single Cloud blob path for all checkpoints now\nPERMANENT_CLOUD_UPLOAD_PATH=\"<<<SCRUBBED FOR RELEASE>>>\"\n# increment the run ID so we don’t clobber tensorboard\nRUN_ID=175B_run12.17\n# relaunch training and restore checkpoint from Cloud blob storage\nINCLUDED_HOSTS=node-[1,3-6,8-42,44-46,48-128] \\\npython -m fb_sweep.opt.sweep_opt_en_lm_175b \\\n-n 124 -g 8 -t 1 \\\n--checkpoints-dir /shared/home/namangoyal/checkpoints/175B/ \\\n--local-checkpoints-dir /mnt/scratch/susanz/checkpoints/2021-11-24.1 \\\n--full-cloud-upload-path $PERMANENT_CLOUD_UPLOAD_PATH \\\n-p $RUN_ID\n●\nNotes:\n○\nUpdate local-checkpoints-dir! Permissions issue if using any other username there but your\nown.\n○\nConfirmed right checkpoint loaded (should see this by ~6 minutes in):\n2021-11-24 18:19:14 | INFO | fairseq.trainer | Loaded checkpoint\n/mnt/scratch/susanz/checkpoints/2021-11-24.1/175B_run12.17.me_fp16.minscale0.25.fsdp.gpf3\n2.0.relu.transformer_lm_megatron.nlay96.emb12288.lrnpos.0emb_scale.bm_none.tps2048.gpt2\n.adam.b2_0.95.eps1e-08.cl0.3.lr0.00012.endlr6e-06.wu2000.dr0.1.atdr0.1.0emb_dr.wd0.1.ms8.\nuf1.mu143052.s1.ngpu992/checkpoint_last-model_part-0-shard0.pt (epoch 9 @ 27000 updates)\n○\nGot stuck at 988 log lines of cloud downloads, even though 992 shards exist on machines.\n○\nWCOLL=~myleott/hosts PDSH_RCMD_TYPE=ssh pdsh ls -la\n/mnt/scratch/susanz/checkpoints/2021-11-24.1/175B_run12.17.me_fp16.minscale0.25.fsdp.g\npf32.0.relu.transformer_lm_megatron.nlay96.emb12288.lrnpos.0emb_scale.bm_none.tps2048.\ngpt2.adam.b2_0.95.eps1e-08.cl0.3.lr0.00012.endlr6e-06.wu2000.dr0.1.atdr0.1.0emb_dr.wd0\n.1.ms8.uf1.mu143052.s1.ngpu992 > copied_files.log\n○\n(fairseq-20210913) susanz@ip-0A1E0404:~/fairseq-py$ grep shard copied_files.log | wc\n-l\n992\n\n2021-11-23 10:50am [Myle]: Run 12.16\n●\nDecided to reduce clipping to 0.3 and relaunch from 24.5K checkpoint\n○\nIf this fails, we will try resetting adam stats and do a fresh warmup.\n●\nUncovered a hang when restoring from Cloud blob; fix in PR #2673\n●\nNew train log:\n/shared/home/namangoyal/checkpoints/175B/175B_run12.16.me_fp16.minscale0.25.fsdp.gpf32.0.relu.t\nransformer_lm_megatron.nlay96.emb12288.lrnpos.0emb_scale.bm_none.tps2048.gpt2.adam.b2_0.95.\neps1e-08.cl0.3.lr0.00012.endlr6e-06.wu2000.dr0.1.atdr0.1.0emb_dr.wd0.1.ms8.uf1.mu143052.s1.ngpu\n992/train.log\n# use updated fairscale\ncd ~/src/fairscale\ngit fetch origin prefetch_fsdp_params_simple\ngit checkout prefetch_fsdp_params_simple\n# we use a single Cloud blob path for all checkpoints now\nPERMANENT_CLOUD_UPLOAD_PATH=\"<<<SCRUBBED FOR RELEASE>>>\"\n# increment the run ID so we don’t clobber tensorboard\nRUN_ID=175B_run12.16\n# relaunch training and restore checkpoint from Cloud blob storage\nINCLUDED_HOSTS=node-[2-6,8-46,48-84,86-128] \\\npython -m fb_sweep.opt.sweep_opt_en_lm_175b \\\n-n 124 -g 8 -t 1 \\\n--checkpoints-dir /shared/home/namangoyal/checkpoints/175B/ \\\n--local-checkpoints-dir /mnt/scratch/myleott/checkpoints/2021-11-23.9 \\\n--full-cloud-upload-path $PERMANENT_CLOUD_UPLOAD_PATH \\\n-p $RUN_ID\n2021-11-22 7:45am [Myle]: Run 12.15\n●\n12.14 failed with CUDA error:\n\n○\nTHCudaCheck FAIL file=/pytorch/aten/src/THC/THCCachingHostAllocator.cpp line=278\nerror=999 : unknown error\n○\nRuntimeError: Caught RuntimeError in pin memory thread for device 4.\n●\nTo find bad node:\n○\nRun nvidia-smi on all hosts and save output in nvidia_smi_logs directory\n■\nmkdir nvidia_smi_logs\n■\nscontrol show hostnames node-[2-6,8-46,48-94,96-128] | PDSH_RCMD_TYPE=ssh pdsh -w -\nnvidia-smi -f nvidia_smi_logs/%h\n○\nConclusion: node-85: Unable to determine the device handle for GPU 000B:00:00.0: GPU is\nlost.  Reboot the system to recover this GPU\n●\nReboot the bad node\n○\nssh node-85\n○\nsudo reboot\n●\nBad node never came back!\n○\nTry manually restarting from UI\n●\nCurrent status:\n○\nwe technically have quota up to 132 nodes\n○\nwe currently have 125 nodes running, but only 123 are good\n○\ntwo nodes have IB issues that make them slow when part of any distributed jobs\n○\nwe are not able to grow beyond 125 nodes – this makes me think the cluster does not have any\nspare nodes at this point\n# use updated fairscale\ncd ~/src/fairscale\ngit fetch origin prefetch_fsdp_params_simple\ngit checkout prefetch_fsdp_params_simple\n# we use a single Cloud blob path for all checkpoints now\nPERMANENT_CLOUD_UPLOAD_PATH=\"<<<SCRUBBED FOR RELEASE>>>\"\n# increment the run ID so we don’t clobber tensorboard\nRUN_ID=175B_run12.15\n# relaunch training and restore checkpoint from Cloud blob storage\nINCLUDED_HOSTS=node-[2-6,8-46,48-72,74-128] \\\npython -m fb_sweep.opt.sweep_opt_en_lm_175b \\\n-n 124 -g 8 -t 1 \\\n--checkpoints-dir /shared/home/namangoyal/checkpoints/175B/ \\\n--local-checkpoints-dir /mnt/scratch/myleott/checkpoints/2021-11-22.3 \\\n--full-cloud-upload-path $PERMANENT_CLOUD_UPLOAD_PATH \\\n-p $RUN_ID\nAfter launch:\nsudo scontrol update job=2028 TimeLimit=UNLIMITED\nsudo scontrol update job=2028 MailUser=<scrubbed> MailType=ALL\n./scripts/poll_file.py /shared/home/namangoyal/checkpoints/175B/175B_run12.15*/train.log --mailto <scrubbed>\n2021-11-21 4:15pm [Myle]: Run 12.14\n●\nRun 12.13\n○\nSuccessfully tested the automatic-resume-from-latest-blob-checkpoint functionality\n\n○\nBut loss exploded even faster than 12.12, since the loss scale state is not properly reloaded\nfrom checkpoint, causing the loss scale to stay low since there isn’t enough history built up to\nincrease it\n●\nNext step: roll back to a slightly older checkpoint (22,500 => 22,250) and set\n--threshold-loss-scale=0.25\n●\nNew train log:\n/shared/home/namangoyal/checkpoints/175B/175B_run12.14.me_fp16.minscale0.25.fsdp.gpf32.0.relu.t\nransformer_lm_megatron.nlay96.emb12288.lrnpos.0emb_scale.bm_none.tps2048.gpt2.adam.b2_0.95.\neps1e-08.cl1.0.lr0.00012.endlr6e-06.wu2000.dr0.1.atdr0.1.0emb_dr.wd0.1.ms8.uf1.mu143052.s1.ngpu\n992/train.log\n# use updated fairscale\ncd ~/src/fairscale\ngit fetch origin prefetch_fsdp_params_simple\ngit checkout prefetch_fsdp_params_simple\nCKPT_DIR=/data/users/myleott/175B_run12.12.me_fp16.fsdp.gpf32.0.relu.transformer_lm_megatron.nlay96.emb12288.lrnp\nos.0emb_scale.bm_none.tps2048.gpt2.adam.b2_0.95.eps1e-08.cl1.0.lr0.00012.endlr6e-06.wu2000.dr0.1.atdr0.1.0emb_dr.\nwd0.1.ms8.uf1.mu143052.s1.ngpu992\nBLOB_URL=\"<<<SCRUBBED FOR RELEASE>>>\"\ncd $CKPT_DIR\ncp --recursive --include-pattern \"checkpoint_8_22000*.pt\" \"$BLOB_URL\" checkpoint_8_22000\ncp --recursive --include-pattern \"checkpoint_8_22250*.pt\" \"$BLOB_URL\" checkpoint_8_22250\nRESTORE_FILE=$CKPT_DIR/checkpoint_8_22250/checkpoint_8_22250.pt\nRUN_ID=175B_run12.14\nINCLUDED_HOSTS=node-[2-6,8-46,48-94,96-128] \\\npython -m fb_sweep.opt.sweep_opt_en_lm_175b \\\n-n 124 -g 8 -t 1 \\\n-p $RUN_ID \\\n--checkpoints-dir /shared/home/namangoyal/checkpoints/175B/ \\\n--restore-file $RESTORE_FILE\nAfter launch:\nsudo scontrol update job=2021 TimeLimit=UNLIMITED\nsudo scontrol update job=2021 MailUser=<scrubbed> MailType=ALL\n2021-11-21 3pm [Myle]: Run 12.13\n●\nManually killed and relaunched with blob requeueing fixes from #2666\n●\nNew train log:\n/shared/home/namangoyal/checkpoints/175B/175B_run12.13.me_fp16.fsdp.gpf32.0.relu.transformer_l\nm_megatron.nlay96.emb12288.lrnpos.0emb_scale.bm_none.tps2048.gpt2.adam.b2_0.95.eps1e-08.cl1\n.0.lr0.00012.endlr6e-06.wu2000.dr0.1.atdr0.1.0emb_dr.wd0.1.ms8.uf1.mu143052.s1.ngpu992/train.log\n\n# use updated fairscale\ncd ~/src/fairscale\ngit fetch origin prefetch_fsdp_params_simple\ngit checkout prefetch_fsdp_params_simple\n# we use a single Cloud blob path for all checkpoints now\nPERMANENT_CLOUD_UPLOAD_PATH=\"<<<SCRUBBED FOR RELEASE>>>\"\n# increment the run ID so we don’t clobber tensorboard\nRUN_ID=175B_run12.13\n# relaunch training and restore checkpoint from Cloud blob storage\nINCLUDED_HOSTS=node-[2-6,8-46,48-94,96-128] \\\npython -m fb_sweep.opt.sweep_opt_en_lm_175b \\\n-n 124 -g 8 -t 1 \\\n--checkpoints-dir /shared/home/namangoyal/checkpoints/175B/ \\\n--local-checkpoints-dir /mnt/scratch/myleott/checkpoints/2021-11-21 \\\n--full-cloud-upload-path $PERMANENT_CLOUD_UPLOAD_PATH \\\n-p $RUN_ID\nAfter launch:\nsudo scontrol update job=2019 TimeLimit=UNLIMITED\nsudo scontrol update job=2019 MailUser=<scrubbed> MailType=ALL\n2021-11-21 Analysis of 12.X series\nEta of completion:\n●\nAt observed rate (including downtime, total average WPS, etc)\n○\nGoal: 144k updates; Currently at: 22k\n○\nStarted Nov 11 22:00; Currently Nov 21, 13:00; duration 231 hours.\n■\n= 38 seconds per update with downtime\n■\n= 53 more days at total average pace\n■\n= Jan 3\n●\nAt optimal rate (including WPS improvements from Naman, no downtime):\n○\n122k updates to go\n○\n19.3 seconds per update with no downtime\n○\n= 27.2523148 days\n○\n= Dec 18\nRelated idle thoughts [Stephen] about a potential v2:\n●\nTuning WD or LR might be most beneficial\n●\nI still think fresh BPE might be nice\n●\nI’d really like to add in multilingual/non-English data as the next big chunk of data. (Would definitely\nnecessitate a new BPE)\n2021-11-20 9:30pm [Myle]: Run 12.12\n●\nHad previously tried requeuing job, but now realized it loaded the previous checkpoint at 17750 (likely\ndue to the presence of --restore-file)\n●\nWasn’t able to get Sam’s blob reload logic to work, reverted to manual download\n●\nNew log:\n/shared/home/namangoyal/checkpoints/175B/175B_run12.12.me_fp16.fsdp.gpf32.0.relu.transformer_l\n\nm_megatron.nlay96.emb12288.lrnpos.0emb_scale.bm_none.tps2048.gpt2.adam.b2_0.95.eps1e-08.cl1\n.0.lr0.00012.endlr6e-06.wu2000.dr0.1.atdr0.1.0emb_dr.wd0.1.ms8.uf1.mu143052.s1.ngpu992/train.log\n# use updated fairscale\ncd ~/src/fairscale\ngit fetch origin prefetch_fsdp_params_simple\ngit checkout prefetch_fsdp_params_simple\nCKPT_DIR=/data/users/myleott/175B_run12.11.me_fp16.fsdp.gpf32.0.relu.transformer_lm_megatron.nlay96.emb12288.lrnp\nos.0emb_scale.bm_none.tps2048.gpt2.adam.b2_0.95.eps1e-08.cl1.0.lr0.00012.endlr6e-06.wu2000.dr0.1.atdr0.1.0emb_dr.\nwd0.1.ms8.uf1.mu143052.s1.ngpu992\nBLOB_URL=\"<<<SCRUBBED FOR RELEASE>>>\"\ncd $CKPT_DIR\ncp --recursive --include-pattern \"checkpoint_7_20250*.pt\" \"$BLOB_URL\" checkpoint_7_20250\nRESTORE_FILE=$CKPT_DIR/checkpoint_7_20250/checkpoint_7_20250.pt\nRUN_ID=175B_run12.12\nINCLUDED_HOSTS=node-[2-6,8-46,48-94,96-128] \\\npython -m fb_sweep.opt.sweep_opt_en_lm_175b \\\n-n 124 -g 8 -t 1 \\\n-p $RUN_ID \\\n--checkpoints-dir /shared/home/namangoyal/checkpoints/175B/ \\\n--restore-file $RESTORE_FILE\nAfter launch:\nsudo scontrol update job=2001 TimeLimit=UNLIMITED\nsudo scontrol update job=2001 MailUser=<scrubbed> MailType=ALL\n2021-11-19 9:30am [Myle]: Run 12.11\n●\ntraced down IB issues with node-[7,43,90,95].\nSam Shleifer\n●\nfound ECC errors on node-1\nSusan Zhang\n●\nUpdated fairscale with Naman’s FSDP speedup:\nfairscale@8820049331331c773077c257667aa81baf4cc9f9\nCKPT_DIR=/data/users/myleott/175B_run12.10.me_fp16.fsdp.gpf32.0.relu.transformer_lm_megatron.nlay96.emb12288.lrnp\nos.0emb_scale.bm_none.tps2048.gpt2.adam.b2_0.95.eps1e-08.cl1.0.lr0.00012.endlr6e-06.wu2000.dr0.1.atdr0.1.0emb_dr.\nwd0.1.ms8.uf1.mu143052.s1.ngpu992\nBLOB_URL=\"<<<SCRUBBED FOR RELEASE>>>\"\ncd $CKPT_DIR\ncp --recursive --include-pattern \"checkpoint_6_17750*.pt\" \"$BLOB_URL\" checkpoint_6_17750\nexport RESTORE_FILE=$CKPT_DIR/checkpoint_6_17750/checkpoint_6_17750.pt\nexport RUN_ID=175B_run12.11\n# use updated fairscale\ncd ~/src/fairscale\ngit fetch origin prefetch_fsdp_params_simple\ngit checkout prefetch_fsdp_params_simple\n# initial test run to validate nodes\n\nINCLUDED_HOSTS=node-[2-6,8-89,91-94,96-128] \\\npython -m fb_sweep.opt.sweep_opt_en_lm_175b \\\n-n 124 -g 8 -t 1 \\\n-p test4_${RUN_ID} \\\n--checkpoints-dir /shared/home/namangoyal/checkpoints/175B/\n# resume from previous checkpoint\nINCLUDED_HOSTS=node-[2-6,8-89,91-94,96-128] \\\npython -m fb_sweep.opt.sweep_opt_en_lm_175b \\\n-n 124 -g 8 -t 1 \\\n-p $RUN_ID \\\n--checkpoints-dir /shared/home/namangoyal/checkpoints/175B/ \\\n--restore-file $RESTORE_FILE\nAfter launch:\nsudo scontrol update job=1984 TimeLimit=UNLIMITED\nsudo scontrol update job=1984 MailUser=<scrubbed> MailType=ALL\n2021-11-18 List of issues for Cloud Complaints\nUnable to train continuously for more than 1-2 days on a cluster of 128 nodes. Many failures require manual\ndetection and remediation, wasting compute resources and researcher time:\n●\nGPU reliability issues (e.g., ECC errors) leading to frequent job restarts and manual reprovisioning of\nproblematic nodes/GPUs\n●\nIB issues lead to degraded training speed (-20% throughput), requiring manual bisection of problematic\nnodes and manual reprovisioning\n●\nUnexpected job hangs, likely due to IB/NCCL issues, requiring manual detection\nConcrete failures seen:\n●\nGPUs randomly disconnecting\n○\n“Unable to determine the device handle for GPU 000B:00:00.0: GPU is lost.  Reboot the system\nto recover this GPU”\n●\nNodes with bad IB\n○\n“p2p_plugin.c:141 NCCL WARN NET/IB : Got async event : port error”\n●\nFrequently see GPUs with high rates of uncorrectable ECC errors\n○\nOccurs every 1-2 days on a cluster of 128 nodes\n○\nReprovisioning nodes often returns the same node with ECC errors\nAsks:\n●\nOn CSP side:\n○\nGPUs should be validated to be sufficiently burned-in and ECC checked.\n○\nCSP to check for IB issues before allocating to us.\n○\nNeed mechanism to retire problematic nodes so that they are not reassigned to us before they\nhave been fixed/validated.\n●\nOn our side:\n○\nAutomated health checks to be run at the start and end of each job.\n○\nAutomatic detection and requeuing of jobs that hang due to IB/NCCL issues.\n2021-11-18 5:30pm [Stephen]: Notes from 12.10\n●\nObserved slow down is persistent\n\n●\nObserve that the model will reach the next epoch boundary (and minimize wasted tokenization) in\nabout 12-15 hours.\n●\nDecision:\n○\nWe will let it continue to run at lower WPS overnight\n○\nIn the morning we will kill it, bisect and find the node with bad IB\n○\nWe will simultaneously launch with Naman’s improvements to WPS.\n○\nOnce we identify the bad node, we need to document these lists of things to complain to CSP\nand get our money back\n●\nReplaced node-[1,39] in cloud UI\n○\nnode-1 came back up with ECC errors\n2021-11-18 3pm [Stephen]: Run 12.10\nRelaunching for observed slowness.\nCKPT_DIR=/data/users/roller/175B_run12.09.me_fp16.fsdp.gpf32.0.relu.transformer_lm_megatron.nlay96.emb12288.lrnpo\ns.0emb_scale.bm_none.tps2048.gpt2.adam.b2_0.95.eps1e-08.cl1.0.lr0.00012.endlr6e-06.wu2000.dr0.1.atdr0.1.0emb_dr.w\nd0.1.ms8.uf1.mu143052.s1.ngpu992\nCHKPT=checkpoint_6_16750\nBLOB_URL=\"<<<SCRUBBED FOR RELEASE>>>\"\nmkdir $CKPT_DIR\ncd $CKPT_DIR\ncp --recursive --include-pattern \"checkpoint_6_16750*.pt\" \"$BLOB_URL\" checkpoint_6_16750\nexport RESTORE_FILE=$CKPT_DIR/checkpoint_6_16750/checkpoint_6_16750.pt\nexport RUN_ID=175B_run12.10\ncd ~/working/fairseq\nINCLUDED_HOSTS=node-[2-38,40-89,91-119,121-128] \\\npython -m fb_sweep.opt.sweep_opt_en_lm_175b \\\n-n 124 -g 8 -t 1 \\\n-p $RUN_ID \\\n--checkpoints-dir /shared/home/namangoyal/checkpoints/175B/ \\\n--restore-file $RESTORE_FILE\n# After launch:\nsudo scontrol update job=1477 TimeLimit=UNLIMITED\nsudo scontrol update job=1477 MailUser=<scrubbed> MailType=ALL\n# update tensorboard\n●\nObserved exceptions during workflow:\n\n○\nHad to change my SAS url to a new value (was given the new value in the exception)\n○\nHad to `pip install cloud-storage-blob`\n○\nSome nodes were left stuck in a drain state (1, 39)\n○\nSsh’d into 43 after receiving guidance it was probably okay, and ran `nvidia-smi -q -d \"ECC\"` to\ncheck for ECC errors. All reported 0 so switched 1 to that node.\n○\nNeeded to cd back to my fairseq directory before launch\n○\nHad to relaunch due to a typo in the checkpoint filenames (updated instructions)\n●\nLaunched at 16:04 ET\n●\nNew Log file:\n○\n/shared/home/namangoyal/checkpoints/175B/175B_run12.10.me_fp16.fsdp.gpf32.0.relu.transfo\nrmer_lm_megatron.nlay96.emb12288.lrnpos.0emb_scale.bm_none.tps2048.gpt2.adam.b2_0.95\n.eps1e-08.cl1.0.lr0.00012.endlr6e-06.wu2000.dr0.1.atdr0.1.0emb_dr.wd0.1.ms8.uf1.mu143052.\ns1.ngpu992/train.log\n●\nPotential problematic machines (nov 18):\n○\nDiagnosis: ssh’ing in and running nvidia-smi reports “Unable to determine the device handle for\nGPU 000B:00:00.0: GPU is lost.  Reboot the system to recover this GPU”\n○\nnode-1\n○\nnode-39\n○\nSuggested remedy is both of them need to be rebooted manually by ssh + sudo reboot\n●\nPotential Problematic Machines [updated Nov 17]:\n○\nnode-90 (N/A uncorrectable errors instead of 0)\n■\nSRAM Correctable              : N/A\n■\nSRAM Uncorrectable            : N/A\n■\nDRAM Correctable              : N/A\n■\nDRAM Uncorrectable            : N/A\n2021-11-17 11pm [Myle]: Run 12.09\n●\nPrevious run failed with mysterious error: “RuntimeError: CUDA error: unknown error”\n○\nHappened as validation was starting on run 12.08:\n2021-11-18 02:24:01 | INFO | fairseq_cli.train | Begin looping over validation\n\"valid/Gutenberg_PG-19\" subset with length \"0\"\n●\nNew log file:\n/shared/home/namangoyal/checkpoints/175B/175B_run12.09.me_fp16.fsdp.gpf32.0.relu.transformer_l\nm_megatron.nlay96.emb12288.lrnpos.0emb_scale.bm_none.tps2048.gpt2.adam.b2_0.95.eps1e-08.cl1\n.0.lr0.00012.endlr6e-06.wu2000.dr0.1.atdr0.1.0emb_dr.wd0.1.ms8.uf1.mu143052.s1.ngpu992/train.log\nCKPT_DIR=/data/users/myleott/175B_run12.08.me_fp16.fsdp.gpf32.0.relu.transformer_lm_megatron.nlay96.emb12288.lrnp\nos.0emb_scale.bm_none.tps2048.gpt2.adam.b2_0.95.eps1e-08.cl1.0.lr0.00012.endlr6e-06.wu2000.dr0.1.atdr0.1.0emb_dr.\nwd0.1.ms8.uf1.mu143052.s1.ngpu992\nBLOB_URL=\"<<<SCRUBBED FOR RELEASE>>>\"\ncd $CKPT_DIR\ncp --recursive --include-pattern \"checkpoint_6_15750*.pt\" \"$BLOB_URL\" checkpoint_6_15750\nexport RESTORE_FILE=$CKPT_DIR/checkpoint_6_15750/checkpoint_6_15750.pt\nexport RUN_ID=175B_run12.09\nINCLUDED_HOSTS=node-[1-38,40-42,44-89,91-119,121-128] \\\npython -m fb_sweep.opt.sweep_opt_en_lm_175b \\\n\n-n 124 -g 8 -t 1 \\\n-p $RUN_ID \\\n--checkpoints-dir /shared/home/namangoyal/checkpoints/175B/ \\\n--restore-file $RESTORE_FILE\nAfter launch:\nsudo scontrol update job=1403 TimeLimit=UNLIMITED\nsudo scontrol update job=1403 MailUser=<scrubbed> MailType=ALL\n2021-11-16 11pm [Myle]: Run 12.08\n●\nPrevious run failed with mysterious error: “p2p_plugin.c:141 NCCL WARN NET/IB : Got async event :\nport error”\n●\nNew log file:\n/shared/home/namangoyal/checkpoints/175B/175B_run12.08.me_fp16.fsdp.gpf32.0.relu.transformer_l\nm_megatron.nlay96.emb12288.lrnpos.0emb_scale.bm_none.tps2048.gpt2.adam.b2_0.95.eps1e-08.cl1\n.0.lr0.00012.endlr6e-06.wu2000.dr0.1.atdr0.1.0emb_dr.wd0.1.ms8.uf1.mu143052.s1.ngpu992/train.log\nCKPT_DIR=/data/users/myleott/175B_run12.07.me_fp16.fsdp.gpf32.0.relu.transformer_lm_megatron.nlay96.emb12288.lrnp\nos.0emb_scale.bm_none.tps2048.gpt2.adam.b2_0.95.eps1e-08.cl1.0.lr0.00012.endlr6e-06.wu2000.dr0.1.atdr0.1.0emb_dr.\nwd0.1.ms8.uf1.mu143052.s1.ngpu992\nBLOB_URL=\"<<<SCRUBBED FOR RELEASE>>>\"\ncd $CKPT_DIR\ncp --recursive --include-pattern \"checkpoint_5_13250*.pt\" \"$BLOB_URL\" checkpoint_5_13250\nexport RESTORE_FILE=$CKPT_DIR/checkpoint_5_13250/checkpoint_5_13250.pt\nexport RUN_ID=175B_run12.08\nINCLUDED_HOSTS=node-[1-38,40-89,91-94,96-119,121-128] \\\npython -m fb_sweep.opt.sweep_opt_en_lm_175b \\\n-n 124 -g 8 -t 1 \\\n-p $RUN_ID \\\n--checkpoints-dir /shared/home/namangoyal/checkpoints/175B/ \\\n--restore-file $RESTORE_FILE\nAfter launch:\nsudo scontrol update job=1394 TimeLimit=UNLIMITED\nsudo scontrol update job=1394 MailUser=<scrubbed> MailType=ALL\n2021-11-14 9:45pm [Myle]: Run 12.07\n●\nPrevious run silently hung\n●\nNew log file:\n/shared/home/namangoyal/checkpoints/175B/175B_run12.07.me_fp16.fsdp.gpf32.0.relu.transformer_l\nm_megatron.nlay96.emb12288.lrnpos.0emb_scale.bm_none.tps2048.gpt2.adam.b2_0.95.eps1e-08.cl1\n.0.lr0.00012.endlr6e-06.wu2000.dr0.1.atdr0.1.0emb_dr.wd0.1.ms8.uf1.mu143052.s1.ngpu992/train.log\n\nCKPT_DIR=/data/users/myleott/175B_run12.06.me_fp16.fsdp.gpf32.0.relu.transformer_lm_megatron.nlay96.emb12288.lrnp\nos.0emb_scale.bm_none.tps2048.gpt2.adam.b2_0.95.eps1e-08.cl1.0.lr0.00012.endlr6e-06.wu2000.dr0.1.atdr0.1.0emb_dr.\nwd0.1.ms8.uf1.mu143052.s1.ngpu992\nBLOB_URL=\"<<<SCRUBBED FOR RELEASE>>>\"\ncd $CKPT_DIR\ncp --recursive --include-pattern \"checkpoint_3_7500*.pt\" \"$BLOB_URL/*\" checkpoint_3_7500\nexport RESTORE_FILE=$CKPT_DIR/checkpoint_3_7500/checkpoint_3_7500.pt\nexport RUN_ID=175B_run12.07\nINCLUDED_HOSTS=node-[1-38,40-89,91-94,96-119,121-128] \\\npython -m fb_sweep.opt.sweep_opt_en_lm_175b \\\n-n 124 -g 8 -t 1 \\\n-p $RUN_ID \\\n--checkpoints-dir /shared/home/namangoyal/checkpoints/175B/ \\\n--restore-file $RESTORE_FILE\nAfter launch:\nsudo scontrol update job=1390 TimeLimit=UNLIMITED\nsudo scontrol update job=1390 MailUser=<scrubbed> MailType=ALL\n2021-11-12 10:30pm [Myle]: Run 12.06\n●\nPrevious restore failed because checkpoint_1_2000 is missing shards in Cloud blob!\n○\nRun 12.02 must have been interrupted before all the checkpoints were uploaded.\n○\nFortunately we can get the shards from local storage on the nodes\n# copy missing checkpoints from local storage on each node\nMNT_DIR=/mnt/scratch/susanz/checkpoints/2021-11-12/175B_run12.02.me_fp16.fsdp.gpf32.0.relu.transformer_lm_megatro\nn.nlay96.emb12288.lrnpos.0emb_scale.bm_none.tps2048.gpt2.adam.b2_0.95.eps1e-08.cl1.0.lr0.00012.endlr6e-06.wu2000.\ndr0.1.atdr0.1.0emb_dr.wd0.1.ms8.uf1.mu143052.s1.ngpu992\nWCOLL=~/hosts PDSH_RCMD_TYPE=ssh pdsh ls $MNT_DIR | grep \"checkpoint_1_2000.*pt$\"\n(...)\ncd $RESTORE_DIR\nscp node-30:$MNT_DIR/checkpoint_1_2000-model_part-5-shard29.pt .\nscp node-19:$MNT_DIR/checkpoint_1_2000-model_part-4-shard18.pt .\nscp node-37:$MNT_DIR/checkpoint_1_2000-model_part-0-shard36.pt .\nscp node-56:$MNT_DIR/checkpoint_1_2000-model_part-0-shard54.pt .\nscp node-56:$MNT_DIR/checkpoint_1_2000-model_part-1-shard54.pt .\nscp node-56:$MNT_DIR/checkpoint_1_2000-model_part-2-shard54.pt .\nscp node-56:$MNT_DIR/checkpoint_1_2000-model_part-3-shard54.pt .\nscp node-56:$MNT_DIR/checkpoint_1_2000-model_part-4-shard54.pt .\nscp node-56:$MNT_DIR/checkpoint_1_2000-model_part-5-shard54.pt .\nscp node-56:$MNT_DIR/checkpoint_1_2000-model_part-6-shard54.pt .\nscp node-56:$MNT_DIR/checkpoint_1_2000-model_part-7-shard54.pt .\nscp node-68:$MNT_DIR/checkpoint_1_2000-model_part-0-shard66.pt .\nscp node-103:$MNT_DIR/checkpoint_1_2000-model_part-0-shard99.pt .\nexport\nRESTORE_FILE=/data/users/susanz/175B_run12.02.me_fp16.fsdp.gpf32.0.relu.transformer_lm_megatron.nlay96.emb12288.l\nrnpos.0emb_scale.bm_none.tps2048.gpt2.adam.b2_0.95.eps1e-08.cl1.0.lr0.00012.endlr6e-06.wu2000.dr0.1.atdr0.1.0emb_\ndr.wd0.1.ms8.uf1.mu143052.s1.ngpu992/checkpoint_1_2000/checkpoint_1_2000.pt\nexport RUN_ID=175B_run12.06\n\nINCLUDED_HOSTS=node-[1-38,41-94,96-119,121-128] \\\npython -m fb_sweep.opt.sweep_opt_en_lm_175b \\\n-n 124 -g 8 -t 1 \\\n-p $RUN_ID \\\n--checkpoints-dir /shared/home/namangoyal/checkpoints/175B/ \\\n--restore-file $RESTORE_FILE\nAfter launch:\nsudo scontrol update job=1387 TimeLimit=UNLIMITED\nsudo scontrol update job=1387 MailUser=<scrubbed> MailType=ALL\n●\nMisc\n○\nRe-enable tensor init on GPU\n○\nAlso run `sudo reboot` on node-[39,40]\ngit checkout 08cb44d9dc3dcbe90605dd03b4f2156996ea2bac\nexport\nRESTORE_FILE=/data/users/susanz/175B_run12.02.me_fp16.fsdp.gpf32.0.relu.transformer_lm_megatron.nlay96.emb12288.l\nrnpos.0emb_scale.bm_none.tps2048.gpt2.adam.b2_0.95.eps1e-08.cl1.0.lr0.00012.endlr6e-06.wu2000.dr0.1.atdr0.1.0emb_\ndr.wd0.1.ms8.uf1.mu143052.s1.ngpu992/checkpoint_1_2000/checkpoint_1_2000.pt\nexport RUN_ID=175B_run12.06\nINCLUDED_HOSTS=node-[1-38,41-94,96-119,121-128] \\\npython -m fb_sweep.opt.sweep_opt_en_lm_175b \\\n-n 124 -g 8 -t 1 \\\n-p $RUN_ID \\\n--checkpoints-dir /shared/home/namangoyal/checkpoints/175B/ \\\n--restore-file $RESTORE_FILE\nAfter launch:\nsudo scontrol update job=1386 TimeLimit=UNLIMITED\nsudo scontrol update job=1386 MailUser=<scrubbed> MailType=ALL\n2021-11-12 7pm [Susan]: Run 12.05\n●\nRemoved tensor init on gpu.\ngit checkout ddbb690ed49d49653a1c12374386de1a2102d3a2\nexport\nRESTORE_FILE=/data/users/susanz/175B_run12.02.me_fp16.fsdp.gpf32.0.relu.transformer_lm_megatron.nlay96.emb12288.l\nrnpos.0emb_scale.bm_none.tps2048.gpt2.adam.b2_0.95.eps1e-08.cl1.0.lr0.00012.endlr6e-06.wu2000.dr0.1.atdr0.1.0emb_\ndr.wd0.1.ms8.uf1.mu143052.s1.ngpu992/checkpoint_1_2000/checkpoint_1_2000.pt\nexport RUN_ID=175B_run12.05\nINCLUDED_HOSTS=node-[1-38,40-94,96,98-119,121-128] \\\npython -m fb_sweep.opt.sweep_opt_en_lm_175b \\\n-n 124 -g 8 -t 1 \\\n-p $RUN_ID \\\n--checkpoints-dir /shared/home/namangoyal/checkpoints/175B/ \\\n--restore-file $RESTORE_FILE\nAfter launch:\nsudo scontrol update job=1385 TimeLimit=UNLIMITED\nsudo scontrol update job=1385 MailUser=<scrubbed> MailType=ALL\n\n2021-11-12 6pm [Susan]: Run 12.04\n●\nHost 95 and 120 are both full of ECC errors. Put both in drain. Replacing 95 seems to give the same\nmachine, so will wait to restart later.\ngit checkout 38dab5485ef7d5c1e29187185680b6e4f314e7b9\nexport\nRESTORE_FILE=/data/users/susanz/175B_run12.02.me_fp16.fsdp.gpf32.0.relu.transformer_lm_megatron.nlay96.emb12288.l\nrnpos.0emb_scale.bm_none.tps2048.gpt2.adam.b2_0.95.eps1e-08.cl1.0.lr0.00012.endlr6e-06.wu2000.dr0.1.atdr0.1.0emb_\ndr.wd0.1.ms8.uf1.mu143052.s1.ngpu992/checkpoint_1_2000/checkpoint_1_2000.pt\nexport RUN_ID=175B_run12.04\nINCLUDED_HOSTS=node-[1-38,40-94,96,98-119,121-128] \\\npython -m fb_sweep.opt.sweep_opt_en_lm_175b \\\n-n 124 -g 8 -t 1 \\\n-p $RUN_ID \\\n--checkpoints-dir /shared/home/namangoyal/checkpoints/175B/ \\\n--restore-file $RESTORE_FILE\nAfter launch:\nsudo scontrol update job=1384 TimeLimit=UNLIMITED\nsudo scontrol update job=1384 MailUser=<scrubbed> MailType=ALL\n2021-11-12 3pm [Susan]: Run 12.03\n●\nRestarting requires following the same steps as 11.2 to download checkpoints (takes ~25 minutes).\n●\nDownloading into\n/data/users/susanz/175B_run12.02.me_fp16.fsdp.gpf32.0.relu.transformer_lm_megatron.nlay96.emb12\n288.lrnpos.0emb_scale.bm_none.tps2048.gpt2.adam.b2_0.95.eps1e-08.cl1.0.lr0.00012.endlr6e-06.wu\n2000.dr0.1.atdr0.1.0emb_dr.wd0.1.ms8.uf1.mu143052.s1.ngpu992\n●\nLoading checkpoint when run comes up takes ~15 minutes.\n●\nLoading data when run comes up takes ~30 minutes (to fast forward to data point within epoch).\n# Find cloud path of where the checkpoints went - grab last one\n(fairseq-20210913)\nsusanz@ip-0A1E0404:/shared/home/namangoyal/checkpoints/175B/175B_run12.02.me_fp16.fsdp.gpf32.0.relu.transformer_l\nm_megatron.nlay96.emb12288.lrnpos.0emb_scale.bm_none.tps2048.gpt2.adam.b2_0.95.eps1e-08.cl1.0.lr0.00012.endlr6e-0\n6.wu2000.dr0.1.atdr0.1.0emb_dr.wd0.1.ms8.uf1.mu143052.s1.ngpu992$ grep \"<<< SCRUBBED FOR RELEASE >>>\" train.log |\ntail -n 1\n2021-11-12 19:34:30 | INFO | fairseq_cli.train | preparing to copy\n/mnt/scratch/susanz/checkpoints/2021-11-12/175B_run12.02.me_fp16.fsdp.gpf32.0.relu.transformer_lm_megatron.nlay96\n.emb12288.lrnpos.0emb_scale.bm_none.tps2048.gpt2.adam.b2_0.95.eps1e-08.cl1.0.lr0.00012.endlr6e-06.wu2000.dr0.1.at\ndr0.1.0emb_dr.wd0.1.ms8.uf1.mu143052.s1.ngpu992/checkpoint_1_2000-model_part-0-shard0.pt to <<<SCRUBBED FOR\nRELEASE>>>\nexport\nRUN_ID=\"175B_run12.02.me_fp16.fsdp.gpf32.0.relu.transformer_lm_megatron.nlay96.emb12288.lrnpos.0emb_scale.bm_none\n.tps2048.gpt2.adam.b2_0.95.eps1e-08.cl1.0.lr0.00012.endlr6e-06.wu2000.dr0.1.atdr0.1.0emb_dr.wd0.1.ms8.uf1.mu14305\n2.s1.ngpu992\"\n\nexport CHECKPOINT=\"checkpoint_1_2000\"\nmkdir /data/users/susanz/$RUN_ID\ncd /data/users/susanz/$RUN_ID\nmkdir $CHECKPOINT\nBLOB_URL=\"<<<SCRUBBED FOR RELEASE>>>\"\ncp --recursive --include-pattern \"$CHECKPOINT-*.pt\" \"$BLOB_URL/*\" $CHECKPOINT/\nLaunch commands:\ngit checkout 38dab5485ef7d5c1e29187185680b6e4f314e7b9\nexport\nRESTORE_FILE=/data/users/susanz/175B_run12.02.me_fp16.fsdp.gpf32.0.relu.transformer_lm_megatron.nlay96.emb12288.l\nrnpos.0emb_scale.bm_none.tps2048.gpt2.adam.b2_0.95.eps1e-08.cl1.0.lr0.00012.endlr6e-06.wu2000.dr0.1.atdr0.1.0emb_\ndr.wd0.1.ms8.uf1.mu143052.s1.ngpu992/checkpoint_1_2000/checkpoint_1_2000.pt\nexport RUN_ID=175B_run12.03\n# DO DRY RUN!! Node configuration may have been changed !!!\nINCLUDED_HOSTS=node-[1-38,40-87,89-96,98-119,121-128] \\\npython -m fb_sweep.opt.sweep_opt_en_lm_175b \\\n-n 124 -g 8 -t 1 \\\n-p $RUN_ID \\\n--checkpoints-dir /shared/home/namangoyal/checkpoints/175B/ \\\n--restore-file $RESTORE_FILE \\\n--dry-run\nINCLUDED_HOSTS=node-[1-38,40-87,89-94,96,98-128] \\\npython -m fb_sweep.opt.sweep_opt_en_lm_175b \\\n-n 124 -g 8 -t 1 \\\n-p $RUN_ID \\\n--checkpoints-dir /shared/home/namangoyal/checkpoints/175B/ \\\n--restore-file $RESTORE_FILE\nAfter launch:\nsudo scontrol update job=1382 TimeLimit=UNLIMITED\nsudo scontrol update job=1382 MailUser=<scrubbed> MailType=ALL\n\nAnalysis of Run 12.02 [Susan]\n●\nLoss of ppl starting to oscillate more heavily, potentially indicating that LR is too high.\n●\nCUDA error crashed the run after 2008 updates.\n2021-11-11 11pm [Susan]: Run 12.02\n●\nRelaunched with node 7 put back, same nodelist as 11.10.\n●\nThis worked! Expect roughly 2 minutes between “Start iterating over samples” and the first log line:\n\nAnything longer than that may be an issue…\n●\nNew train dir:\n/shared/home/namangoyal/checkpoints/175B/175B_run12.02.me_fp16.fsdp.gpf32.0.relu.transformer_l\nm_megatron.nlay96.emb12288.lrnpos.0emb_scale.bm_none.tps2048.gpt2.adam.b2_0.95.eps1e-08.cl1\n.0.lr0.00012.endlr6e-06.wu2000.dr0.1.atdr0.1.0emb_dr.wd0.1.ms8.uf1.mu143052.s1.ngpu992\n●\nCommand ran:\ngit checkout 7b7ccd38f30a9db9c32df360922d803620268ce6\nINCLUDED_HOSTS=node-[1-38,40-87,89-96,98-119,121-128] \\\npython -m fb_sweep.opt.sweep_opt_en_lm_175b \\\n-n 124 -g 8 -t 1 \\\n-p 175B_run12.01 \\\n--checkpoints-dir /shared/home/namangoyal/checkpoints/175B/\nAfter launch:\nsudo scontrol update job=1378 TimeLimit=UNLIMITED\nsudo scontrol update job=1378 MailUser=<scrubbed> MailType=ALL\n2021-11-11 5:40pm [Susan]: Run 12.01\n●\nSame as 12.00 but with node 7 removed, and 97 swapped in.\n●\nStill got stuck. Relaunching with node 7 put back.\n●\nNew train dir:\n/shared/home/namangoyal/checkpoints/175B/175B_run12.01.me_fp16.fsdp.gpf32.0.relu.transformer_l\nm_megatron.nlay96.emb12288.lrnpos.0emb_scale.bm_none.tps2048.gpt2.adam.b2_0.95.eps1e-08.cl1\n.0.lr0.00012.endlr6e-06.wu2000.dr0.1.atdr0.1.0emb_dr.wd0.1.ms8.uf1.mu143052.s1.ngpu992\n●\nCommand ran:\ngit checkout 7b7ccd38f30a9db9c32df360922d803620268ce6\nINCLUDED_HOSTS=node-[1-6,8-38,40-87,89-119,121-128] \\\npython -m fb_sweep.opt.sweep_opt_en_lm_175b \\\n-n 124 -g 8 -t 1 \\\n-p 175B_run12.01 \\\n--checkpoints-dir /shared/home/namangoyal/checkpoints/175B/\nAfter launch:\nsudo scontrol update job=1377 TimeLimit=UNLIMITED\nsudo scontrol update job=1377 MailUser=<scrubbed> MailType=ALL\n●\nRebooting node-7:\n○\nDrain first: sudo scontrol update node=node-7 state=drain reason=lostgpu\n2021-11-11 5:30pm [Susan]: Run 12.00 - Lost a GPU on node-7, restarting.\n●\nDiscussion on gradient predivide:\nQuick primer on gradient predivide\n\n●\nSummary of weight init decision:\n●\nWeight Init from different codebases\n○\nMegatron-LM uses sigma / math.sqrt(2.0 * num_layers)\n■\nNOTE: The layer wise scaling is only being applied to the output layer i.e fc2 of ffn and\nout_proj of attn and not all the weight matrix. (this is mostly already implemented in\ngshard_combine_megatron_fsdp but with small differences.)\n■\nsigma = 0.006 for 175B\n■\nWord Embedding\n●\nnormal(0, 0.006)\n■\nPosition Embedding\n●\nnormal(0, 0.006)\n■\nMHA\n●\nQKV input projection\n○\nnormal(0, 0.006)\n●\nOutput projection\n○\nsigma / math.sqrt(2.0 * num_layers)\n○\nStddev: 0.00043\n●\nAll biases: zero\n■\nFFN\n●\nFC1\n○\nnormal(0, 0.006)\n●\nFC2\n○\nsigma / math.sqrt(2.0 * num_layers)\n■\nStddev: 0.00043\n●\nAll biases: zero\n■\nLayer norms\n●\nGamma: 1.0\n●\nBeta: 0.0\n○\nFairseq gshard_combine_megatron_fsdp does:\n■\nWord Embedding\n●\nnormal(0, 0.009)\n■\nPosition Embedding:\n●\nnormal(0, 0.009)\n■\nMHA\n●\nQKV input projection\n○\nStddev: 0.006\n■\nModel parallel fairseq is approximately normal(0, 0.006)\n\n■\nNote: this matches non-model parallel fairseq’s xaviar_uniform(...,\ngain=1 / math.sqrt(2)), which is similar to normal(0, 0.006)\n●\nOutput projection\n○\nStddev: 0.009\n■\nModel parallel fairseq is approximately normal(0, 0.009)\n■\nNote: this matches non-model parallel fairseq’s xavier_uniform,\nwhich is similar to normal(0, 0.009)\n■\nFFN\n●\nFC1\n○\nStddev: 0.005\n■\nUses kaiming_uniform(..., a=math.sqrt(5))\n●\nFC2\n○\nStddev: 0.005\n■\nUses kaiming_uniform(..., a=math.sqrt(5))\n■\nLayer norms\n●\nGamma: 1.0\n●\nBeta: 0.0\n○\nDeepSpeed uses sigma / math.sqrt(2.0 * num_layers)\n○\nMesh tensorflow doesn’t seem to scale init based on num layers\n○\nLingvo doesn’t seem to scale init at all\n○\nPaddle Paddle uses pytorch defaults lmao\n○\nGPT NeoX uses same as deepspeed\n●\nNew train dir:\n/shared/home/namangoyal/checkpoints/175B/175B_run12.00.me_fp16.fsdp.gpf32.0.relu.transformer_l\nm_megatron.nlay96.emb12288.lrnpos.0emb_scale.bm_none.tps2048.gpt2.adam.b2_0.95.eps1e-08.cl1\n.0.lr0.00012.endlr6e-06.wu2000.dr0.1.atdr0.1.0emb_dr.wd0.1.ms8.uf1.mu143052.s1.ngpu992\n●\nCommand ran:\ngit checkout 7b7ccd38f30a9db9c32df360922d803620268ce6\nINCLUDED_HOSTS=node-[1-38,40-87,89-96,98-119,121-128] \\\npython -m fb_sweep.opt.sweep_opt_en_lm_175b \\\n-n 124 -g 8 -t 1 \\\n-p 175B_run12.00 \\\n--checkpoints-dir /shared/home/namangoyal/checkpoints/175B/\nAfter launch:\nsudo scontrol update job=1374 TimeLimit=UNLIMITED\nsudo scontrol update job=1374 MailUser=<scrubbed> MailType=ALL\n●\nLost a GPU:\n\nRestarting and rotating in 97 (rotating out 7).\n2021-11-10 5pm: Run 11.10\nNOTE: To resume Run 11, we must revert 7eacba2\nAnalysis of 11.9\nLoss scale started dropping and hit min at 5280 updates\nDecision\n-\nLaunch 11.10 where we:\n-\nSwitch to RELU\n-\nSwitch to Stable MHA\n-\nWe assume this will die\n-\nPresumably Thursday AM we will launch 12.00 with:\n-\nKill normformer\n-\nLower LR\n-\nBatch Skipping\n\n-\nActual LPE, no scale emb\n-\nAnd possibly some changes to init (layerwise init possibly)\n-\nIn parallel, we will use the RSC to ablate different layerwise init options:\n-\nDefault (as is now)\n-\nLayerwise init scaled by a global constant (as a function of total layers)\n-\nLayerwise init scaled by per-layer-index\nLaunch steps for Run 11.10\n●\nNew train log:\n/shared/home/namangoyal/checkpoints/175B/175B_run11.10.me_fp16.fsdp.gpf32.0.relu.transformer_l\nm_megatron.nlay96.emb12288.lrnsin.nffc.nfatt.nfhd.bm_none.tps2048.gpt2.adam.b2_0.95.eps1e-08.cl\n1.0.lr6e-05.endlr6e-06.wu1000.dr0.1.atdr0.1.0emb_dr.wd0.1.ms8.uf1.mu143052.s1.ngpu992/train.log\ngit checkout 075f54e1b8da88ab90d6a0717d6e73eba33b98a0\nexport INCLUDED_HOSTS=node-[1-38,40-87,89-96,98-119,121-128]\nRESTORE_FILE=/data/users/namangoyal/175B_run11.3.me_fp16.fsdp.gpf32.0.gelu.transformer_lm_megatron.nlay96.emb1228\n8.lrnsin.nffc.nfatt.nfhd.bm_none.tps2048.gpt2.adam.b2_0.98.eps1e-08.cl1.5.lr7.5e-05.endlr7.5e-06.wu1000.dr0.1.atd\nr0.1.0emb_dr.wd0.05.ms8.uf1.mu143052.s1.ngpu992/checkpoint_2_4750/checkpoint_2_4750.pt\nRUN_ID=175B_run11.10\n# note that this command has --reset-dataloader, which is not desired in general\npython -m fb_sweep.opt.sweep_opt_en_lm_175b \\\n-n 124 -g 8 -t 1 \\\n-p $RUN_ID \\\n--checkpoints-dir /shared/home/namangoyal/checkpoints/175B/ \\\n--restore-file $RESTORE_FILE \\\n--reset-dataloader\nAfter launch:\nsudo scontrol update job=1365 TimeLimit=UNLIMITED\nsudo scontrol update job=1365 MailUser=<scrubbed> MailType=ALL\n\n[2021-11-10]: Run 11.9: Lowered LR to 6e-5, match exp 11.6 otherwise\nAnalysis of 11.8\n●\nTraining stalled at 5160, made it 41 steps further than 11.6 (when weight decay was also 0.05)\n○\n11.6 got to num_updates 5119 before naning out\n●\nMassive loss scale drop after 5159:\n\n●\nStill layer 92 that infs\n●\nLast log shows lr of 7.30233e-05\nDecisions for 11.9\n●\nTurn clipping back on (removing the “throw batch out” logic)\n●\nLower LR to 6e-5\n●\nOtherwise match run 11.7:\n○\nIncrease weight decay to 0.1\n○\nLower beta2 to 0.95\n○\nRoll back to checkpoint @ 4750 steps\n○\nReset dataloader\n●\nMisc:\n○\nAdd pnorm logging\nLaunch steps for 11.9\n●\nNew train log:\n/shared/home/namangoyal/checkpoints/175B/175B_run11.9.me_fp16.fsdp.gpf32.0.gelu.transformer_lm\n_megatron.nlay96.emb12288.lrnsin.nffc.nfatt.nfhd.bm_none.tps2048.gpt2.adam.b2_0.95.eps1e-08.cl1.\n0.lr6e-05.endlr6e-06.wu1000.dr0.1.atdr0.1.0emb_dr.wd0.1.ms8.uf1.mu143052.s1.ngpu992/train.log\ngit checkout a69e5d30b4199f40c4651deac918c58ab594f018\nexport INCLUDED_HOSTS=node-[1-38,40-87,89-96,98-119,121-128]\nRESTORE_FILE=/data/users/namangoyal/175B_run11.3.me_fp16.fsdp.gpf32.0.gelu.transformer_lm_megatron.nlay96.emb1228\n8.lrnsin.nffc.nfatt.nfhd.bm_none.tps2048.gpt2.adam.b2_0.98.eps1e-08.cl1.5.lr7.5e-05.endlr7.5e-06.wu1000.dr0.1.atd\nr0.1.0emb_dr.wd0.05.ms8.uf1.mu143052.s1.ngpu992/checkpoint_2_4750/checkpoint_2_4750.pt\nRUN_ID=175B_run11.9\n# note that this command has --reset-dataloader, which is not desired in general\npython -m fb_sweep.opt.sweep_opt_en_lm_175b \\\n-n 124 -g 8 -t 1 \\\n-p $RUN_ID \\\n--checkpoints-dir /shared/home/namangoyal/checkpoints/175B/ \\\n--restore-file $RESTORE_FILE \\\n--reset-dataloader\nAfter launch:\nsudo scontrol update job=1336 TimeLimit=UNLIMITED\nsudo scontrol update job=1336 MailUser=<scrubbed> MailType=ALL\n[2021-11-09]: Run 11.8: Rolling back weight decay, start from 4750\nAnalysis of 11.7\n●\nTraining stalled at 4849\n\n●\nDecisions for 11.8\n●\nBisect recent changes to see what fixes instability\n●\nStart with weight decay 0.1 => 0.05\nLaunch steps for 11.8\n●\nNew train.log:\n/shared/home/namangoyal/checkpoints/175B/175B_run11.8.me_fp16.fsdp.gpf32.0.gelu.transformer_lm\n_megatron.nlay96.emb12288.lrnsin.nffc.nfatt.nfhd.bm_none.tps2048.gpt2.adam.b2_0.95.eps1e-08.cl1.\n0.lr7.5e-05.endlr7.5e-06.wu1000.dr0.1.atdr0.1.0emb_dr.wd0.05.ms8.uf1.mu143052.s1.ngpu992/train.lo\ng\ngit checkout b4842e181dababaaad4f0170ad6c90b782877b1c\nexport INCLUDED_HOSTS=node-[1-63,65-87,89-96,98-119,121-128]\nRESTORE_FILE=/data/users/namangoyal/175B_run11.3.me_fp16.fsdp.gpf32.0.gelu.transformer_lm_megatron.nlay96.emb1228\n8.lrnsin.nffc.nfatt.nfhd.bm_none.tps2048.gpt2.adam.b2_0.98.eps1e-08.cl1.5.lr7.5e-05.endlr7.5e-06.wu1000.dr0.1.atd\nr0.1.0emb_dr.wd0.05.ms8.uf1.mu143052.s1.ngpu992/checkpoint_2_4750/checkpoint_2_4750.pt\nRUN_ID=175B_run11.8\n# note that this command has --reset-dataloader, which is not desired in general\npython -m fb_sweep.opt.sweep_opt_en_lm_175b \\\n-n 124 -g 8 -t 1 \\\n-p $RUN_ID \\\n--checkpoints-dir /shared/home/namangoyal/checkpoints/175B/ \\\n--restore-file $RESTORE_FILE \\\n--reset-dataloader\nAfter launch:\nsudo scontrol update job=1336 TimeLimit=UNLIMITED\nsudo scontrol update job=1336 MailUser=<scrubbed> MailType=ALL\n\n[2021-11-09]: Run 11.7: Changing tons of stuff, start from 4750\nAnalysis\nPotential concerns:\n-\n[Preemptive] Data composition\nLaunch steps for 11.7\n●\nAnalysis to come\n●\nFairseq commit: d9c903ea0a2894071fb5bee96b0c9612f1e5a402\n●\nnew data: /data/opt/corpus_dedup_10_10_1_0.05_run11.7/\n●\nBeta2: 0.95 , weight_decay: 0.1, reset dataloader true\n●\nLog:\n/shared/home/namangoyal/checkpoints/175B/175B_run11.7.me_fp16.fsdp.gpf32.0.gelu.transformer_lm\n_megatron.nlay96.emb12288.lrnsin.nffc.nfatt.nfhd.bm_none.tps2048.gpt2.adam.b2_0.95.eps1e-08.cl1.\n0.lr7.5e-05.endlr7.5e-06.wu1000.dr0.1.atdr0.1.0emb_dr.wd0.1.ms8.uf1.mu143052.s1.ngpu992/train.log\n[2021-11-09]: Run 11.6: Starting to skip batches\nEmergency meeting notes\n-\nStarted by cleaning up tech debt of these notes\n-\nPerformed analysis of 11.6 (see below)\n-\nBrainstormed intervention paths\nDecided actions (becomes 11.7)\n-\nReset dataloader (see description in brainstorm)\n-\nResume from 4750\n-\nIncrease WD to 0.05 -> 0.1\n-\nLower beta2 to 0.98 -> 0.95\n-\n(Note we need a new runid)\n-\n[Myle] swap the shards\n-\n[Naman] Launch the run\nBackup option if things die in the middle of the night:\n-\n[Sam] Check at 11pm\n-\nLower the LR to 6e-5\nBrainstorm of interventions (With initializing from some checkpoint):\n-\nReset dataloader for faster startup\n-\n--reset-dataloader\n-\nSwap shard1 with shard29\n-\nSwap shard2 with shard30\n-\nReasoning: we’re late in shard 2 and so we end up doing 45 minutes of tokenization before we\ncan see updates\n-\nTo be done together:\n-\nLower beta2 to 0.95\n-\nIncrease Weight Decay to 0.1\n\n-\nHow much would we need to roll back to address this?\n-\nLower LR to 6e-5\n-\nHotswap GELU => RELU\n-\nUnsure if the code path works due to fused -- need to test on 100M params\n-\nClamp activations\n-\nShuffle the data\n-\nAdding the max to the layer norm\n-\nManually shrink gradient to embeddings\n-\nIncrease layernorm epsilon\n-\n[bottom pri] Adam epsilon 1e-8 => 1e-6\nBrainstorm of interventions that require full restart [Unanimously unpopular:\n-\nCold swap GELU => RELU (with restart)\n-\nRemove normformer\n-\nDifferent data\n-\nLower LR with cold restart\nAnalysis of 11.6\nHypothesis:\n-\nInstead of clipping, start throwing stuff away\n-\nResume from 5000 steps (140 steps rewound)\nNote 11.6 is NOT equivalent to 11.5 even with restart\n-\nWe reset the loss scale state (accidentally - this isn’t checkpointed: num updates for which overflow\ndidn’t happen)\n-\nLosses match to 2nd decimal, gnorms start diverging\n-\nLoss scales start diverging slightly after gnorm\n-\nInteresting: slightly higher loss scale made it survive slightly more updates\nCute observations:\n-\nWe are seeing a very the same thing: layer 92 attn goes in the range of [-inf, almost inf]\n-\nAND this is happening in the forward pass\n-\nWe only threw away one batch before the loss scale went crazy\n[2021-11-08] Run 11.5: clip 1.5 -> 1.0\nAnalysis of 11.5\n●\nReason: 11.5 exploded at 5139 updates\n●\nDiagnosis: gnorm spike at end: (we had survived bigger spikes but this one exploded the run)\n●\nNote: loss scale is very low (.000122…)\n●\nLayer 92 attention layer norm inputs seemed to be where the input to attn layer norm had -inf (and a\nmaximum value pretty damn close to +inf)\n○\nWe were already dropping loss scale really far back.\n○\nLarge gnorm at step 5136 there was a large, clipped grad norm\n■\nUnsolved mystery: which layer really the problem?\n○\nWe do a handful more updates and we -inf out\n\n●\nAction:\n○\nWe decided to skip gradient update when gnorm is higher than clip norm threshold: #2602\n○\nOther theories: attn is numerically unstable\n●\nRestart logs:\n○\n/shared/home/namangoyal/checkpoints/175B/175B_run11.6.me_fp16.fsdp.gpf32.0.gelu.transfor\nmer_lm_megatron.nlay96.emb12288.lrnsin.nffc.nfatt.nfhd.bm_none.tps2048.gpt2.adam.b2_0.98\n.eps1e-08.cl1.0.lr7.5e-05.endlr7.5e-06.wu1000.dr0.1.atdr0.1.0emb_dr.wd0.05.ms8.uf1.mu1430\n52.s1.ngpu992/train.log\nAnalysis of 11.4\n●\n11.4 exploded at around ~3.94k.\n●\nThe gnorm looks like below\n●\nReduced clipping to 1.0 and restarted\n●\nLogs:\n●\n/shared/home/namangoyal/checkpoints/175B/175B_run11.5.me_fp16.fsdp.gpf32.0.gelu.transformer_lm\n_megatron.nlay96.emb12288.lrnsin.nffc.nfatt.nfhd.bm_none.tps2048.gpt2.adam.b2_0.98.eps1e-08.cl1.\n0.lr7.5e-05.endlr7.5e-06.wu1000.dr0.1.atdr0.1.0emb_dr.wd0.05.ms8.uf1.mu143052.s1.ngpu992/train.lo\ng\n\n[Undated] Run 11.4: Changed validation freq\nJust trying to spend less time in validation/saving.\nNote: validation sets changed, so valid_ppl is not comparable to previous runs\n●\nRestarted with following changes (1959e415390560c7b2d680317ca3c07a5f24f8cc):\n○\nReducing validation frequency to 1000 instead of 250\n○\nModel Initialization on gpu\n○\nReduced validation set sizes\n●\nLogs:\n○\n/shared/home/namangoyal/checkpoints/175B/175B_run11.4.me_fp16.fsdp.gpf32.0.gelu.transfor\nmer_lm_megatron.nlay96.emb12288.lrnsin.nffc.nfatt.nfhd.bm_none.tps2048.gpt2.adam.b2_0.98\n.eps1e-08.cl1.5.lr7.5e-05.endlr7.5e-06.wu1000.dr0.1.atdr0.1.0emb_dr.wd0.05.ms8.uf1.mu1430\n52.s1.ngpu992/train.log\n●\nAdded to tensorboard:\n○\ncd /shared/home/namangoyal/checkpoints/175B/tensorboard\n○\nsudo ln -s\n../175B_run11.4.me_fp16.fsdp.gpf32.0.gelu.transformer_lm_megatron.nlay96.emb12288.lrnsin.\nnffc.nfatt.nfhd.bm_none.tps2048.gpt2.adam.b2_0.98.eps1e-08.cl1.5.lr7.5e-05.endlr7.5e-06.wu1\n000.dr0.1.atdr0.1.0emb_dr.wd0.05.ms8.uf1.mu143052.s1.ngpu992/tb/ run11.4\n\n[2021-11-07] Run 11.3: ECC Failure\nNo configuration changes.\n●\nECC error for node node-64 at 2285 updates\n○\nAction: Node put on drain\n○\nDownloaded 2250 checkpoint to:\n■\n/data/users/namangoyal/175B_run11.3.me_fp16.fsdp.gpf32.0.gelu.transformer_lm_meg\natron.nlay96.emb12288.lrnsin.nffc.nfatt.nfhd.bm_none.tps2048.gpt2.adam.b2_0.98.eps1\ne-08.cl1.5.lr7.5e-05.endlr7.5e-06.wu1000.dr0.1.atdr0.1.0emb_dr.wd0.05.ms8.uf1.mu143\n052.s1.ngpu992/checkpoint_1_2250\n●\nNew train.log path:\n/shared/home/namangoyal/checkpoints/175B/175B_run11.3.me_fp16.fsdp.gpf32.0.gelu.transformer_lm\n_megatron.nlay96.emb12288.lrnsin.nffc.nfatt.nfhd.bm_none.tps2048.gpt2.adam.b2_0.98.eps1e-08.cl1.\n5.lr7.5e-05.endlr7.5e-06.wu1000.dr0.1.atdr0.1.0emb_dr.wd0.05.ms8.uf1.mu143052.s1.ngpu992/train.lo\ng\ngit checkout 9b0645d7779a247c2861742b46112a131bf0a67b\nexport INCLUDED_HOSTS=node-[1-63,65-87,89-96,98-119,121-128]\nRESTORE_FILE=/data/users/namangoyal/175B_run11.3.me_fp16.fsdp.gpf32.0.gelu.transformer_lm_megatron.nlay96.emb1228\n8.lrnsin.nffc.nfatt.nfhd.bm_none.tps2048.gpt2.adam.b2_0.98.eps1e-08.cl1.5.lr7.5e-05.endlr7.5e-06.wu1000.dr0.1.atd\nr0.1.0emb_dr.wd0.05.ms8.uf1.mu143052.s1.ngpu992/checkpoint_1_2250/checkpoint_1_2250.pt\nRUN_ID=175B_run11.3\npython -m fb_sweep.opt.sweep_opt_en_lm_175b \\\n-n 124 -g 8 -t 1 \\\n-p $RUN_ID \\\n--checkpoints-dir /shared/home/namangoyal/checkpoints/175B/ \\\n--restore-file $RESTORE_FILE\n[2021-11-06] Run 11.2: clip 2.5 -> 1.5\n●\nResumed from 1K steps with peak LR of 7.5e-5\n●\nNode failure at ~1950 steps\n○\nThe good news is that slurm did try to requeue the job, so once we build the automatic\ndownload-latest-checkpoint-from-blob-and-resume functionality, this kind of problem will recover\nautomatically\n○\nWe also had some ECC errors from yesterday, so all of our 4 buffer nodes are bad\n(node-[61,88,97,120]). Manually recycled them this morning following the instructions at the top\nof the Cloud cluster admin doc.\n●\nRelaunching run on remaining 124 nodes\n○\nThere was a big ppl jump ~1430 steps, and the model was just recovering between 1430-1950\nsteps (after which the node failure stopped the job). Looking at gnorms, decided to roll back to\n1250 steps and lower clipping from 2.5 => 1.5.\n○\nMyle is AFK for the rest of the day. Will defer to Naman, Sam, Stephen and Susan to react to\njob failures if needed\n●\nKnown problems / TODOs\n\n○\nBlob storage URL seems to change across requeues, so we’ll need to address this (probably in\nfb_sweep/sweep/slurm.py) before we can automatically download checkpoints from blob and\nresume training\n○\nTensorboard seems to be bad at resuming training from a checkpoint. If we reuse the same\ntensorboard dir, then the graphs have a weird overlap. If we switch to a new save_dir\n(tensorboard dir), then we lose the previous training history :/\n■\nSolution for now is new tensorboard dir. It’s easy enough to copy the tfevent files if we\nlater decide to stitch things back together.\n●\nNew fairseq commit: 9b0645d7779a247c2861742b46112a131bf0a67b\n●\nNew train.log path:\n/shared/home/namangoyal/checkpoints/175B/175B_run11.2.me_fp16.fsdp.gpf32.0.gelu.transformer_lm_megatr\non.nlay96.emb12288.lrnsin.nffc.nfatt.nfhd.bm_none.tps2048.gpt2.adam.b2_0.98.eps1e-08.cl1.5.lr7.5e-05.\nendlr7.5e-06.wu1000.dr0.1.atdr0.1.0emb_dr.wd0.05.ms8.uf1.mu143052.s1.ngpu992/train.log\ncd\n/data/users/myleott/175B_run11.me_fp16.fsdp.gpf32.0.gelu.transformer_lm_megatron.nlay96.emb12288.lrnsin.nffc.nfat\nt.nfhd.bm_none.tps2048.gpt2.adam.b2_0.98.eps1e-08.cl2.5.lr0.0003.endlr\n1e-05.wu4000.dr0.1.atdr0.1.0emb_dr.wd0.05.ms8.uf1.mu143052.s1.ngpu992\n# Note that the blob storage URL seems to change across requeues,\n# need to get the latest one from train.log\nBLOB_URL=\"<<<SCRUBBED FOR RELEASE>>>\"\ncp --recursive --include-pattern \"checkpoint_1_1250-*.pt\" \"$BLOB_URL/*\" checkpoint_1_1250/\ncd /path/to/fairseq-py\nRESTORE_FILE=/data/users/myleott/175B_run11.me_fp16.fsdp.gpf32.0.gelu.transformer_lm_megatron.nlay96.emb12288.lrn\nsin.nffc.nfatt.nfhd.bm_none.tps2048.gpt2.adam.b2_0.98.eps1e-08.cl2.5.lr0.0003.endlr1e-05.wu4000.dr0.1.atdr0.1.0em\nb_dr.wd0.05.ms8.uf1.mu143052.s1.ngpu992/checkpoint_1_1250/checkpoint_1_1250.pt\n# increment the minor version to get a new save dir / tensorboard dir\nRUN_ID=175B_run11.2\nINCLUDED_HOSTS=node-[1-60,62-87,89-96,98-119,121-128] python -m fb_sweep.opt.sweep_opt_en_lm_175b \\\n-n 124 -g 8 -t 1 \\\n-p $RUN_ID \\\n--checkpoints-dir /shared/home/namangoyal/checkpoints/175B/ \\\n--restore-file $RESTORE_FILE \\\n--dry-run\nAfter launch:\nsudo scontrol update job=1286 TimeLimit=UNLIMITED\nsudo scontrol update job=1286 MailUser=<scrubbed> MailType=ALL\n[2021-11-05] Run 11.1: peak LR down to 7.5e-5\n●\nNote: This was actually run in the same train.log file as 11.0\n●\nHere we capped the warmup at 1000 steps, using a peak LR of 7.5e-5\n[2021-11-05] Run 11.0: LETS GO\n●\n2M bsz\n●\nFP32 Adam\n\n●\nTensor parallel (8x MP)\n●\nNew data - from experiment 29\n●\nLearned positional embeddings with sinusoidal  init\n●\nWeight decay of 0.05\n●\nLR of 3e-4, end LR of 1e-5\n●\nNo dropout on embeddings\n●\nNormformer (impact on grad norm is making earlier layers be more similar with later layers)\n●\nGradient pre-divide factor: 32 (Naman has been running with this)\n●\nClip (l2 norm): 2.5\n●\nFairseq commit: 52ac2df400bd3f42301438217151826b0853c43c\nLog path:\n/shared/home/namangoyal/checkpoints/175B/175B_run11.me_fp16.fsdp.gpf32.0.gelu.transformer_lm_megatron.nlay96.\nemb12288.lrnsin.nffc.nfatt.nfhd.bm_none.tps2048.gpt2.adam.b2_0.98.eps1e-08.cl2.5.lr7.5e-05.endlr7.5e-06.wu100\n0.dr0.1.atdr0.1.0emb_dr.wd0.05.ms8.uf1.mu143052.s1.ngpu992/train.log\nINCLUDED_HOSTS=node-[1-5,7-34,36-87,89-127] python -m fb_sweep.opt.sweep_opt_en_lm_175b \\\n-n 124 -g 8 -t 1 \\\n-p 175B_run11 \\\n--checkpoints-dir /shared/home/namangoyal/checkpoints/175B/\nAfter launch (slurm job id 1264):\nsudo scontrol update job=1264 TimeLimit=UNLIMITED\nsudo scontrol update job=1264 MailUser=<scrubbed> MailType=ALL\nLoss exploded between 1K and 1.25K steps. Decided to roll back to 1K steps and set peak LR\nNov 6, 2021\nto 7.5e-5: 76ae8349c0180daf90174c05d88ba7b6075cde51\ncd\n/data/users/myleott/175B_run11.me_fp16.fsdp.gpf32.0.gelu.transformer_lm_megatron.nlay96.emb12288.lrnsin.nffc.nfat\nt.nfhd.bm_none.tps2048.gpt2.adam.b2_0.98.eps1e-08.cl2.5.lr0.0003.endlr\n1e-05.wu4000.dr0.1.atdr0.1.0emb_dr.wd0.05.ms8.uf1.mu143052.s1.ngpu992\ncp --recursive --include-pattern \"checkpoint_1_1000-*.pt\" <<<SCRUBBED FOR RELEASE>>>\nRESTORE_FILE=/data/users/myleott/175B_run11.me_fp16.fsdp.gpf32.0.gelu.transformer_lm_megatron.nlay96.emb12288.lrn\nsin.nffc.nfatt.nfhd.bm_none.tps2048.gpt2.adam.b2_0.98.eps1e-08.cl2.5.lr0.0003.endlr1e-05.wu4000.dr0.1.atdr0.1.0em\nb_dr.wd0.05.ms8.uf1.mu143052.s1.ngpu992/checkpoint_1_1000/checkpoint_1_1000.pt\n# Note that the new save_dir is different, because we adjusted the peak LR.\n# I manually copied the contents of the old checkpoint dir (tb, train.log)\n# to the new save_dir for continuity\nINCLUDED_HOSTS=node-[1-60,62-87,89-119,121-127] python -m fb_sweep.opt.sweep_opt_en_lm_175b \\\n-n 124 -g 8 -t 1 \\\n-p 175B_run11 \\\n--checkpoints-dir /shared/home/namangoyal/checkpoints/175B/ \\\n--restore-file $RESTORE_FILE \\\n--resume-failed --dry-run\nAfter launch:\nsudo scontrol update job=1284 TimeLimit=UNLIMITED\nsudo scontrol update job=1284 MailUser=<scrubbed> MailType=ALL\n\nRun 10\n●\nTry to make run 9 identical to run 8, but with tensor parallelism\n●\n4M bsz\n●\nFP16 Adam\n●\nTensor parallel (8x MP)\n●\n--gradient-predivide-factor 11.1\n●\nFairseq commit: gshard-175b-run10\nLog path:\n/shared/home/namangoyal/checkpoints/175B/175B_run10.me_fp16.fsdp.gelu.transformer_lm_megatron.nlay9\n6.emb12288.bm_none.tps2048.adam.fp16adam.b2_0.98.eps1e-08.cl1.0.lr6e-05.wu290.dr0.1.atdr0.1.ms16.uf1\n.0.mu73832.s1.wd0.1.ngpu992/train.log\nINCLUDED_HOSTS=node-[1-7,9-88,90-108,110-127] python -m fb_sweep.opt.sweep_opt_en_lm \\\n-n 124 -g 8 -t 1 \\\n--weight-decay 0.1 --gradient-predivide-factor 11.1 \\\n--model-parallel-size 8 --distribute-checkpointed-activations --batch-size 16 \\\n-p 175B_run10 --model-size 175B_opt_h2_2021 \\\n--checkpoints-dir /shared/home/namangoyal/checkpoints/175B/\nOutcome: OOM during validation after 300 steps\nRun 9\n●\nSimilar to run 8, except:\n●\nTensor parallel (8x MP)\n●\n2M bsz\n●\nFP32 Adam\n●\nFairseq commit: gshard-175b-run9\nLog path:\n/shared/home/namangoyal/checkpoints/175B/175B_run9.me_fp16.fsdp.gelu.transformer_lm_megatron.nlay96.\nemb12288.bm_none.tps2048.adam.b2_0.98.eps1e-08.cl1.0.lr6e-05.wu580.dr0.1.atdr0.1.ms8.uf1.0.mu147665.\ns1.wd0.1.ngpu992/train.log\nINCLUDED_HOSTS=node-[1-7,9-74,76-78,80-127] python -m fb_sweep.opt.sweep_opt_en_lm \\\n-n 124 -g 8 -t 1 \\\n--weight-decay 0.1 --fp32-adam \\\n--model-parallel-size 8 --distribute-checkpointed-activations --batch-size 8 \\\n-p 175B_run9 --model-size 175B_opt_h2_2021 \\\n--checkpoints-dir /shared/home/namangoyal/checkpoints/175B/\nOutcome: loss exploded\nRun 8\n●\nRevert to Run 5 config, but with use-sharded-state\n●\nFairseq commit: gshard-175b-run8\n\nLog path:\n/shared/home/namangoyal/checkpoints/175B/175B_run8.fsdp.me_fp16.transformer_lm_gpt.nlay96.emb12288.\nbm_none.tps2048.adam.fp16adam.b2_0.98.eps1e-08.cl1.0.lr6e-05.wu290.dr0.1.atdr0.1.wd0.1.ms2.uf1.mu738\n32.s1.ngpu992/train.log\nINCLUDED_HOSTS=node-[1-7,9-74,76-78,80-127] python -m fb_sweep.opt.sweep_opt_en_lm \\\n-n 124 -g 8 -t 1 \\\n-p 175B_run8 --model-size 175B_opt_h2_2021 \\\n--checkpoints-dir /shared/home/namangoyal/checkpoints/175B/\nOutcome: Good! See workplace post.\nRun 7\n●\nRevert to Run 5 config, but with tensor parallelism\n○\nOne difference: run 5 had a 4M bsz, but here we are using 2M bsz\n●\nClipping 1.0\n●\nWeight decay 0.1\n●\nLonger warmup (290 steps)\n●\nThis also hardcodes skip_remainder_batch=True for Trainer.get_valid_iterator\n●\nFairseq commit: gshard-175b-run7\n●\nFairscale commit: 3584965cd4356c3c522e7d97aa13994cfa95ea5b\nLog path:\n/shared/home/namangoyal/checkpoints/175B/run7.me_fp16.fsdp.gelu.transformer_lm_megatron.nlay96.emb1\n2288.bm_none.tps2048.adam.b2_0.98.eps1e-08.cl1.0.lr6e-05.wu290.dr0.1.atdr0.1.ms8.uf1.mu73832.s1.ngpu\n992/train.log\nINCLUDED_HOSTS=node-[1-7,9-74,76-78,80-127] python -m fb_sweep.opt.sweep_opt_en_lm \\\n-n 124 -g 8 -t 1 \\\n-p run7 --model-size 175B_opt_h2_2021 --update-freq 1 \\\n--batch-size-per-gpu 8 --model-parallel-size 8   --dropout 0.1 \\\n--checkpoints-dir /shared/home/namangoyal/checkpoints/175B/ \\\n--distribute-checkpointed-activations --fp32-adam \\\n--save-interval 300 --validate-interval 300 --gradient-predivide-factor 11.1 --dry-run\nOutcome: seems to get stuck at 10.7 loss again, seems tensor parallel isn’t fixed\n2021-10-22: Run 6\n3ad9e48cc started Oct 22 1:01 AM\n●\nTensor parallelism\n●\nClipping 1.0\n●\nWeight decay 0.01\n●\n1x warmup\n●\nAdam Beta2 0.95\n●\nAdam Eps 1e-6\nscancel 385\nLog Path:\n\n/shared/home/namangoyal/checkpoints/175B/run6.me_fp16.fsdp.gelu.transformer_lm_megatron.nlay96.emb1\n2288.bm_none.tps2048.adam.b2_0.95.eps1e-06.cl1.0.lr6e-05.wu93.dr0.1.atdr0.1.ms8.uf1.mu147666.s1.wd0.\n01.ngpu992//train.log\nJob 378\n-\nBack to tensor parallel with cpu weight init (which seems to improve things, at least in my env).\n-\nweight-decay .01 because all the evidence I have points against increasing.\n-\nA little more save-interval, validate-interval to get more signal overnight.\nINCLUDED_HOSTS=node-[1-7,9-74,76-78,80-127] python -m fb_sweep.opt.sweep_opt_en_lm  -n 124 -g 8 -t 1 \\\n-p run6 --model-size 175B_opt_h2_2021 --update-freq 1 \\\n--batch-size-per-gpu 8   --model-parallel-size 8   --dropout 0.1 \\\n--max-update 147666 --checkpoints-dir /shared/home/namangoyal/checkpoints/175B/ \\\n--distribute-checkpointed-activations --fp32-adam --save-interval 500  --validate-interval 500\n--gradient-predivide-factor 11.1 \\\n--weight-decay .01\nOutcome: seems to get stuck at 10.7 loss again!\nRun 5\n●\nSimilar to Run4, but with some extra safety knobs\n●\nClipping 1.0\n●\nWeight decay 0.1\n●\n3x longer warmup\n●\nFairseq commit: d70abfba80ded958cde92af62fc505bbb99ea170\n●\nNew log path:\n/shared/home/namangoyal/checkpoints/175B/175B_run5.fsdp.me_fp16.transformer_lm_gpt.nlay96.em\nb12288.bm_none.tps2048.adam.fp16adam.b2_0.98.eps1e-08.cl1.0.lr6e-05.wu290.dr0.1.atdr0.1.wd0.1.\nms2.uf1.mu73832.s1.ngpu992/train.log\n●\nRelaunched with: INCLUDED_HOSTS=node-[1-7,9-74,76-78,80-127] python -m\nfb_sweep.opt.sweep_opt_en_lm -n 124 -g 8 -t 1 -p 175B_run5 --model-size 175B_opt_h2_2021\n--checkpoints-dir /shared/home/namangoyal/checkpoints/175B/\nOutcome:\nLoss went down for 300 steps, validation ran.\nHtop & nvidia-smi looked fine on a random node 2 hours after the last log.\nStarted stalling Oct 21, 2021 20:40 -> Oct 21, 2021 23:34 (killed by Sam)\nUpdate by Myle, 10/22 @ 8am: it seems I forgot to add use-sharded-state, so it was trying to consolidate state\non a rank 0! PR with fix pushed here: #2490\nRun 4\n●\nRevert to Run1 config on gshard stable with zero3\n●\nFairseq commit: 53d993880508f1ea3272b406e8ecc99298305c7b\n●\nFairscale commit: 8acbec718f3c70a6b9785470bb9e05cd84fc3f8e\n●\nNew log path:\n/shared/home/namangoyal/checkpoints/175B/175B_run4.fsdp.me_fp16.transformer_lm_gpt.nlay96.em\n\nb12288.bm_none.tps2048.adam.fp16adam.b2_0.98.eps1e-08.cl0.0.lr6e-05.wu96.dr0.1.atdr0.1.wd0.01.\nms2.uf1.mu73832.s1.ngpu992/train.log\n●\nRelaunched with: INCLUDED_HOSTS=node-[1-7,9-74,76-78,80-127] python -m\nfb_sweep.opt.sweep_opt_en_lm -n 124 -g 8 -t 1 -p 175B_run4 --model-size 175B_opt_h2_2021\n--checkpoints-dir /shared/home/namangoyal/checkpoints/175B/\nOutcome: Loss exploding again\nRun 3\n●\nClipping 1.0\n●\nAdam beta2 0.95\n●\nAdam eps 1e-6\n●\n--fp32-reduce-scatter\n●\nNew fairseq commit hash: 6b629aec74a917f4fbaf3b208f261aa4fc375c84\n●\nThis was buggy (don’t use): Also changed fairscale a08a523f6b2fb401f1e12522af9160673fe41e32\n●\nNew log path:\n/shared/home/namangoyal/checkpoints/175B/175B_third_attempt.2.me_fp16.fsdp.gelu.transformer_lm\n_megatron.nlay96.emb12288.fp32reduce.bm_none.tps2048.adam.b2_0.95.eps1e-06.cl1.0.lr6e-05.wu1\n93.dr0.1.atdr0.1.wd0.1.ms8.uf1.mu147666.s1.ngpu992/train.log\n●\nRelaunched with: INCLUDED_HOSTS=node-[1-7,9-74,76-78,80-127] python -m\nfb_sweep.opt.sweep_opt_en_lm     -n 124 -g 8 -t 1     -p 175B_third_attempt --model-size\n175B_opt_h2_2021     --update-freq 1     --batch-size-per-gpu 8   --model-parallel-size 8   --dropout 0.1\n--max-update 147666       --checkpoints-dir /shared/home/namangoyal/checkpoints/175B/\n--distribute-checkpointed-activations --fp32-adam  --tensor-parallel-init-model-on-gpu  --save-interval\n300  --validate-interval 300 --gradient-predivide-factor 11.1 --warmup-updates 193\nOutcome: Sam discovered that model parallel branch doesn’t converge even for 125M model\n[Undated] Run 2\n●\nIncrease weight decay to 0.1\n●\nLog path:\n/shared/home/namangoyal/checkpoints/175B/175B_first_attempt.me_fp16.fsdp.gelu.transformer_lm_m\negatron.nlay96.emb12288.bm_none.tps2048.adam.b2_0.98.eps1e-08.cl0.0.lr6e-05.wu193.dr0.1.atdr0.\n1.wd0.1.ms8.uf1.mu147666.s1.ngpu992/train.log\nOutcome: loss plateaus, fails to go below 10.7\n2021-10-20:  Run 1\n20th October\nFairseq: 468a050d4996a4f18c99519c0cfc2d9512f5ab6f\nFairscale: 3584965cd4356c3c522e7d97aa13994cfa95ea5b\nMegatron submodule: b6a6ed16ae0a6ff5a57089f66f13a617e1390d1f\nConda env on cloud:\n\nCmd: python -m fb_sweep.opt.sweep_opt_en_lm     -n 124 -g 8 -t 1     -p 175B_first_attempt --model-size\n175B_opt_h2_2021     --update-freq 1     --batch-size-per-gpu 8   --model-parallel-size 8   --dropout 0.1\n--max-update 147666       --checkpoints-dir /shared/home/namangoyal/checkpoints/175B/\n--distribute-checkpointed-activations --fp32-adam  --salloc  --tensor-parallel-init-model-on-gpu  --save-interval\n300  --validate-interval 300 --gradient-predivide-factor 11.1 --warmup-updates 193\nNodes: node-[1-7,9-74,76-78,80-127]\nlog:\n/shared/home/namangoyal/checkpoints/175B/175B_first_attempt.me_fp16.fsdp.gelu.transformer_lm_megatron\n.nlay96.emb12288.bm_none.tps2048.adam.b2_0.98.eps1e-08.cl0.0.lr6e-05.wu193.dr0.1.atdr0.1.wd0.01.ms8.u\nf1.mu147666.s1.ngpu992/train.log\nOutcome: Loss kinda exploded. Trying weight decay next\nKitchen sink: Analysis of Exp 21--29\nThis keeps record of some of our kitchen sink findings.\nDescription of experiments:\n●\n21: New data after fixing encoding issues and collapsing newlines\n●\n22: Old data + pushift.io (control)\n●\n23: Subset of new data closest to the “classic” roberta data. (BookCorpus + CC + OWT2 + wiki +\nccnews2 + pushift.io + stories)\n●\n24: Ablate out BookCorpus from 23. So new versions of (CC + OWT2 + wiki + ccnews2 + pushift.io\n+ stories)\n●\n25: Safer version of all new data (cc + pg19 + hn + OST + OWT2 + USPTO + wiki + ccnews +\npushift.io + stories)\n●\n26: Add in the FAIR version of BookCorpus from 23. So new versions of (BookCorpusFair + CC +\nOWT2 + wiki + ccnews2 + pushift.io + stories)\n●\n27: Safer versions of all new data (BookCorpusFair + cc + pg19 + hn + OST + OWT2 + USPTO +\nwiki + ccnews + pushift.io + stories)\n●\n28: learned positional embeddings vs sinusoidal (with caveats)\n●\n29: Manually cleaned version of corpora via handcrafted regexes\nExp 21 -- 23\nWanted to see what was the “dangerous” corpora in 21. Performed this experiment and found BookCorpus,\nEuroParl, and Pg19, Enron, and EuroParl all look iffy for gnorms\nDM and SX look kinda sketchy for PPL reasons\n\nExp 23, 24 and 25 (Drop out BookCorpus)\nOur predominant goal here is to consider whether the “safe” version of BookCorpus is really safer.\nTo that end, our main comparison is 23 vs 24.\n\nIn our judgment, 24 was a little bit less spiky than 23, indicating that we feel comfortable that the main problem\nwith our “new data” is the BookCorpus coming from the Pile.\nWe can also compare 24 vs 25 to see if we managed to exclude the dangerous corpora:\nIn general, 24 looks fairly smooth (after you consider this is log interval 1), and 25 still has some issues.\nAt this point, we killed them early and grabbed a copy of BookCorpus from the old cluster.\n\nExp 26 and 27: Adding in Book Corpus\nOn stability via only the new books corpus (24 vs 26)\nOverall they seem about the same if one were to average 26 at a smaller log interval.\nExp 29: Manually cleaned up corpora\nLearned Embeddings\nExp 27 vs 28\nThese two runs use the same dataset, and predominantly differ choices for positional embeddings. In\nparticular, 27 uses sinusoidal embeddings, while 28 uses LPE + some tricks around initialization and scaling.\nSee the PR for 28 for an exact description.\nThey are neck to neck, but 27 does come out on top after 45k updates.\n\nWe also observe that the gnorms of 28 are a bit spikier:\nOncall Debugging\nAKA: Help! I’m oncall, it’s 3am, and everything is on fire!\nPhilosophy\nWhen you are on call, you are ON CALL. You are fully expected to fulfill the responsibilities within a reasonable\ntime frame.\nBut when you are not on call, you are NOT on call. Try to leave monitoring and resolution to the oncall as much\nas you can. It both prevents burnout, and the oncall from growing lazy.\nIf you have conflicts or cannot fulfill your shift, that’s okay! Just make sure to arrange a trade with someone.\n\nRemember to document your actions. This helps people understand what you did without relying on pinging\nyou.\nResponsibilities\n1.\nRegularly monitor jobs (by checking tensorboard and tailed logs) for aberrant behavior.\na.\nRun `./scripts/cloud/monitor_train_log.py --mailto <scrubbed>@fb.com --slurm-jobid JOBID\n--modified-threshold 900` to get emails about aberrations (the email listed will send to the\noncall) and enable auto-recovery.\nb.\nSee links at the top of the document for the latest tensorboard\n2.\nPerform cluster maintenance in the case of hardware failures\na.\nMost of this is done via fixmycloud and the auto-recovery monitor now.\nb.\nIdentify the broken node\nc.\nRelaunch the slurm job with it replaced using one of the spares\nd.\nDrain the bad node, note it in the Spare Node Tracker table.\ne.\nPotentially replace the node.\n3.\nKeep nodes from being idle and ensure that something productive is always running.\na.\nPreferably the main job.\nb.\nIf it looks unrecoverable, execute any pre-documented Plan B choices\nc.\nBias for action. Make discretionary decisions if other members are not available.\nOnboarding & Gotchas\nThese are the instructions for getting onboarded into the cluster to fulfill your oncall duties.\n●\nGetting onto Cloud and setting up environment:\n○\n< link to cloud cluster instructions doc >\n○\nBegin following this including the “getting on” and “setup environment”\n○\nYou can have a tmux session on cloud, but Stephen usually just keeps tmux on his devfair and\nssh’s in from the devfair.\n○\nOnce you’re onboarded, make sure you have the ability to run “sudo”. You will need it, a lot.\n○\n`git clone git@github.com:facebookresearch/fairscale` and do install instructions\n■\nMake sure to be on the right checkpoints for `fairscale` and `fairseq-py` (Look at the\nmost recent copy/paste of run+install instructions in the oncall log; scripts won’t exist\notherwise)\n○\nAlso make sure to once run “python3 scripts/cloud/ssh.py” Which will ensure your ssh is\nconfigured to disregard host keys, as our nodes are re-imaged and change keys frequently\n●\nGotcha: Avoid doing anything compute-heavy on the login node. It is a very light machine and a shared\nresource. Heavy operations on it can slow down EVERYONE.\n●\nGotcha: most of our oncall tooling expects to be run from the login node.\n○\nHowever, sometimes it’s useful to directly log into nodes when trying to debug a failure. Just\n“ssh node-X”\nQuestions\n●\nIs it safe to assume the checkpoint last used in the log is the right one?\n○\nWe are constantly writing checkpoints to blob storage. Checkpoints are not written to any sort of\nshared disk (like /shared/home)\n\n○\n(We actually write them to local SSD on every worker, and then initiate a separate upload on\nevery worker, which makes it very fast).\n○\nWhen launched without `--restore-file`, our code will automatically fetch the latest checkpoint\nfrom blob and resume from that.\n○\nBut in the case of things like loss exploding, or more radical changes, you may need to\nmanually provide a `--restore-file` in order to resume from that checkpoint.\n○\nSee entry “2021-12-06 8:30am ET: Lowering LR and launching 12.46” for the latest example of\ndoing that.\n●\nWhere will I find log files that I need to tail?\n○\n/shared/home/namangoyal/checkpoints/175B/ contains many folders. Each folder has its “12.46”\nor “12.37” folder or what not.\n●\nWhat is this BLOB_PREFIX and BLOB_AUTH stuff?\n○\nBLOB_AUTH contains authentication variables so that Cloud’s blob storage knows what\naccount we are using. It generally will never change.\n○\nBLOB_PREFIX is roughly the bucket/folder that we are uploading to. Due to limitations on the\nCloud side, sometimes we need to switch to new buckets.\n■\nThe BLOB_PREFIX also contains a “run id” in it generally speaking. We only bump this\nwhen we do something that could potentially clobber checkpoints. Therefore you will\noften find yourself using BLOB PREFIXES with RunIDs that are not 1:1 with the\nrun you are launching.\nRun is stuck in loop of “lower loss scale”\nUh oh. The loss exploded. We don’t know why.\nRemember to document your actions in the logbook.\nActions:\n1.\nDon’t panic.\n2.\nPing the group chat to discuss potential options. If no one responds within 10 minutes, you will have to\nmake a decision by yourself. Letting nodes idle costs $2500/hour so it is strongly discouraged.\na.\nBias for action, but also double check your commands to avoid catastrophic losses (clobbering\ncheckpoints, etc). As expensive as it is, an extra idle hour is still cheaper than having to redo\nmultiple days.\ni.\nIt’s generally a good idea to bump your blob RUNID variables. Search this document for\nOLD_BLOB_PREFIX for the most recent examples of that.\n3.\nGo read the logs and check the tensorboard.\na.\nWhat happened to Gnorm in the updates preceding the explosion? Did we see a large gnorm\nspike? (~0.7 or above)?\nb.\nWhat about actv norm, pnorms, loss, etc? Did those spike too?\nc.\nWhat was the loss scale doing during the moments before? Did it drop very rapidly (20-50\nupdates)? Or was it a slow gradual fall out?\n4.\nGo read the log book. Was there a plan B put in place already? If so, execute it. If not, you will have to\nmake some decisions yourself.\n5.\nActions you might take:\na.\nLet it keep running another hour or so. Sometimes we recover. This is unlikely if you are seeing\n40+ loss scale flatlines.\n\nb.\nRequeue. This is a generally safe option, unless the logbook tells you otherwise. This will restart\nthe job from the last checkpoint and just pray for the best. Frequently this just kicks the can\ndown the road 2-6 more hours, but that can buy you some time to sleep and discuss.\nc.\nMore extreme measures are then suitable. Preferably these should be group discussion, but\nthat’s not always possible.\nd. Taking extreme measures should always cause you to bump the run ID and the\nBLOB_URL.\ne.\nPrior actions we have taken as examples:\ni.\nLowering the LR, usually by some factor (0.9x or 0.75x).\n1.\nThis is our current default mode of action.\n2.\nAs of Dec 7, this seems to buy us a couple days.\nii.\nImplementing clamping of activations (Not actually tried yet, but a plan B)\niii.\nLowering the clip 1.0 -> 0.3.\niv.\nHot swap the optimizer from Adam to SGD. This has gone poorly and is not currently\nrecommended.\nWPS has dropped a lot\nAs of 2021-12-02, WPS is consistently around ~100k and does not fluctuate more than 1%, except during\nvalidation.\nIf you observe WPS dropping below ~90k or more for a sustained period (>20 minutes), we probably have an\ninfiniband problem.\nActions:\n1.\nRun health checks only on idle nodes. Identify a suitable replacement.\na.\nIf you cannot, then you should leave the job as is. Better to run slow than not at all.\n2.\nPause the job\n3.\nRun global health checks. You are particularly looking out for NCCL issues this time.\n4.\nReplace the node and resume the job.\nJob is Hanging\nRemember to document your actions in the logbook.\nPossible Actions:\n1.\nDo nothing. Auto recovery in monitor.py may handle this for you. You should receive an email\nregarding success/failure.\n2.\nRead the train.log. Check the timestamp. Remember it’s in UTC time (run “date” to see what time the\ncomputer thinks it is now)\n3.\nCheck the stderr. Is there a mention of a CUDA or NCCL Failure?\na.\nA node has probably kicked the bucket. Hopefully the auto-recovery script has kicked in. Check\nits logs (which should be coming in via email)\nb.\nIf not, pause the job (see below) and follow the Health Checks guidelines.\n4.\nIs there really nothing in the stderr? Let’s check on the CPUs, the GPUs, and the Disk to see if there\nare active bottlenecks we just need to be patient for.\na.\nCPU: Nodes may be stuck doing some tokenization. ssh into one and run htop.\ni.\nIf many CPUs are used or spiking, then just be patient. We may be tokenizing.\n\nii.\nIf you see ~16 processes pegged at 100% but the other 80 unused, you should continue\ndown this checklist.\nb.\nGPU: run “watch -n0.5 nvidia-smi”. Watch for about a minute. Are all the GPUs at 100%? Is the\ntemperature 40C or 72C? Is the power usage ~70W or or ~385W?\ni.\nHotter and higher power mean the GPU is active. Let it be.\nii.\nCooler and low power means we are hanging on communication, and probably have a\nbad node.\nc.\nDisk: Run “sudo iotop” and watch.\ni.\nIs the top process sitting at just 0mb/s read/write? We are not disk bottlenecked.\nii.\nIs it very high? We might be doing I/O or writing a checkpoint.\n5.\nFull health checks are called for.\na.\nPause or cancel the job\nb.\nRun fixmycloud all\nc.\nReplace the bad node(s) with good nodes in the INCLUDED_HOSTS environmental variable.\nd.\nRelaunch the job\nPerforming health checks\nUse fixmycloud to check quality across the entire cluster\n# check only idle, unused nodes\npython scripts/cloud/fixmycloud.py idle\n# check all nodes. Should only be done when a job is not running\npython scripts/cloud/fixmycloud.py all\nIt will output information about potential warnings and possibly suggested mitigations.\nYou may wish to run partial checks, which can be done by calling individual scripts rather than the fixmycloud\nscript. The health checks include:\n●\nnvidia-smi checks. These do not interfere with running nodes, and are safe to run on active nodes.\n●\ngpu-burn checks. These DO interfere with running nodes, and should not be used\n●\nnccl checks. These require the node be undrained and idle, and require you to check 3+ nodes at\nonce.\n●\nblocklist checks. These check if a node is a known bad host, and are safe to run on active nodes.\nI need to reprovision/replace a node\nWe no longer replace nodes ourselves, and instead CSP does it for us. As of 2022-01-13, the fixmycloud tool\nshould upload a diagnostics dump to a public Blob bucket and CSP is responsible for monitoring this bucket\nactively.\nBefore you reprovision a node it’s a good idea to get the metadata and hardware ID for it because you need\nto report it to CSP as a bad node. To get the metadata:\n1.\nssh to the node you plan to reprovision. Some notes about hostnames:\n○\nEach node has multiple hostnames, any of which can be used with ssh:\n■\nnode-XXX: this is the alias used by Slurm\n■\nip-XXXXXX: this is the alias used by Cloud\n■\n10.30.4.XXX: this is the internal IP address of the node\n\n■\nbuoXXXX: this is another alias used by Cloud\n○\nYou can resolve all of these to node-XXX format using the “scripts/cloud/find_host.py” script.\n2.\nRun the scripts/cloud/gather_diagnostics.py script and save the blob URL that is printed and send to\nCSP.\nUsing the command line\nWarning: You probably don’t want this unless you really know what you’re doing. CSP is supposed to\nbe handling this for us.\n# reprovision node node-2\n# warning: you probably don't want to do this on idle or utilized nodes\npython scripts/cloud/replace_node.py node-2\nMake sure you update the “Spare Node Tracker” to indicate it is being replaced and update the timestamp.\nI need to mark a node as unusable\nDraining nodes is useful when you want to mark them as unusable by SLURM. You can then take other\nmitigation actions (like rebooting a node, or reprovisioning it). As of 2021-12-02, nodes with serious errors\n(marked as error/fatal/critical in fixmycloud) should be drained and reported to CSP via chat using the\ndiagnostics script.\n# drain node node-2, marking it unusable\npython scripts/cloud/slurm.py drain node-2 --reason=”Why I am draining”\nYou may also undrain a node similarly with “slurm.py undrain [node]”\nRebooting a node\n$ ssh node-X\n$ sudo reboot\n# [ you will be logged out ]\n$ ping -O node-X\n# [ wait until you start getting \"no answer yet\"]\n# [ start monitoring on cloud UI]\n# [ wait until pings return ]\n# [ then wait an extra minute ]\n$ ssh node-X # assert you can get back in. do a health check\nIf it takes longer than say, 20 minutes, you should probably switch to replacing the node (see above).\nI want to hot swap a node in a running job without explicitly re-launching\nNote: Monitor script should now do this automatically for you.\nThis is useful if you want to correct for a hardware failure without changing any hyperparameters, and just\nresume training.\n\nFigure out the job ID using squeue. The main job is going to be the one with 124 nodes. Let’s assume the job\nID is 2589.\n# these two commands together pause the job\nsudo scontrol requeue job=2589\nsudo scontrol hold job=2589\n# find whatever node(s) caused the issue and drain it\npython scripts/cloud/slurm.py drain node-25\n# OPTIONAL: you can explicitly tell SLURM which nodes to use\n# However, if you are only interested in using any idle node, you can skip\nsudo scontrol update job=2589 NodeList=node-[1-2,4-24,27-36,38-52,54-72,74-107,109-113,115-131,147]\n# allow it to resume\nsudo scontrol release job=2589\n# note the job will be in a \"BeginTime\" held state for 1 minute. This is done intentionally to allow for cleanups on\nshutdown.\nTable of Contents/Index\nInstructions\nSpare Node Tracker\n175B Log\n2022-01-06 15:47 ET [Everyone]\n2022-01-05 14:10 [Stephen + Mikel]\n2022-01-06 10:30ish [Susan + Mikel]\n2022-01-04 15:22 ET [Sam]\n2022-01-03 05:10 ET [Daniel]\n2022-01-02 17:26 ET [Stephen]\n2021-12-31 02:53 ET [Punit]\n2021-12-31 12:00 ET [Moya]\n2021-12-30 17:00 ET [Moya] - nvidia_smi.py bug fix; machine check\n2021-12-30 09:30 ET [Stephen] - Job recovery failed\n2021-12-29 13:40 ET [Stephen] - Cluster maintenance and relaunch\n2021-12-28 16:00 PT [Susan] - cluster maintenance\n2021-12-28 9:10 ET [Myle/Stephen] - manually recovered job\n2021-12-27 9:25 ET [Myle] - postmortem on autorecovery issue\n2021-12-27 13:19 CET [Mikel] - restarting autorecovery script\n2021-12-25  04:18 ET [Myle] - starting improved autorecovery script\n2021-12-25 09:48 ET [Myle] - RCA on autorecovery failure\n2021-12-25 08:49 ET [Stephen]\n2021-12-25 06:25 ET: [Daniel/Susan]\n2021-12-24 12:40 ET: Auto-recovery script\n2021-12-24 10:00 ET: Kurt\n2021-12-23 6:00 PM ET: Kurt\n2021-12-23 8:30 ET: Myle\n2021-12-23 7:00 ET: Stephen\n\n2021-12-22 3:30 pm ET: [Myle] new oncall\n2021-12-21 4:30 pm ET: [Moya] Kick off train 12.55\n2021-12-21 (morning until 3 pm ish) [Stephen] Omicron Sev\n2021-12-21 5:30am ET: [Susan] Node down, restart from 91,250 with lower LR - Run 12.53, 12.54\n2021-12-19 12pm ET: Crossing the epoch boundary\n2021-12-20 12:12 AM PT: [Punit] Node down - requeue for 12.52a\nNCCL failures investigation\nResuming job with new host list\nEnable tensorboard\n2021-12-17 15:34 ET: [Daniel] Node down - requeue for 12.52a\n2021-12-16 12:15 ET: increased job time limit from 3 days to unlimited:\nsudo scontrol update job=2606 TimeLimit=UNLIMITED\n2021-12-14 18:30 ET: [Kurt] Drain a few nodes\n2021-12-14 13:30 ET: [Moya] Scancel + Resubmit\n2021-12-14 03:30 ET: [Susan] Requeue\n2021-12-13 14:00 ET: Reverse Shadow and Oncall onboardings\n2021-12-13: Preemptive Plan\n2021-12-13 11:49 ET: [Stephen] Bumping timelimit\n2021-12-11 07:53 ET: [Stephen] Cluster Maintenance\n2021-12-11 02:52 PT: [Susan] Noticed IB issues/lost GPU.\n2021-12-10 23:14: [Stephen] 12.51 Resuming\n2021-12-10 22:42: [Stephen] 12.50 Resuming\n2021-12-10 20:58 ET: [Stephen] The ablation\n2021-12-10 05:52 ET: [Stephen] Cluster maintenance\n2021-12-09 16:00 PT: [Susan] Run 12.49, restart due to NCCL errors\n2021-12-09 10:45am PT: Provisioning error in Cloud for node 124\n2021-12-09: Megatron v2.6 Debrief + CSP Sync\n2021-12-08 8:55pm ET: run12.48: relaunch checkpoint_18_54250\n2021-12-08 TBD: Analysis from 12.47.myle\n2021-12-08 05:05pm ET: GPU failure; launch debug run with Myle’s env\n2021-12-08 04:00am PT: Checking in\n2021-12-07 10:59pm ET: RuntimeError: CUDA error: CUBLAS_STATUS_EXECUTION_FAILED when\ncalling\n2021-12-07 10:48am ET: ECC error, requeueing 12.46\n2021-12-06 8:30am ET: Lowering LR and launching 12.46\n2021-12-06 05:13 PT: Job Hanging - 3 Machines Down\n2021-12-06 05:00 PT: Grad norm spiking, ppl trending up\n2021-12-05 9pm ET: Requeue after GPU error\n2021-12-05 18:30 ET: Poking through dmesg to see if we can find where we hung\n2021-12-05 12:15pm ET: Requeueing 12.44 and 12.45\n2021-12-05 05:35 PT: Checking on 12.44\n2021-12-05 02:24 ET: Side experiment\n2021-12-05- 00:00 ET: Loss scale exploding 2 - 12.44\n2021-12-04 20:24 ET: Loss scale exploding\n2021-12-04 5:35am ET: Launch of 12.43: Fix blob upload\n\n2021-12-03, 10:35pm ET: DO NOT REBOOT OR REPROVISION ANY NODES UNTIL FURTHER NOTICE\n2021-12-03\n2021-12-03 7:20am ET: Launch of 12.42: Switch back to Adam\n2021-12-02 17:16 ET: Fake SGD debacle: Debrief discussion\nSummary of events and mitigations\nNext paths\n2021-12-02 16:08 ET: Launch of 12.41: Switching to true Vanilla SGD\n2021-12-02 10:25am ET: Launch of 12.40: Intended to be fake SGD with lower learning rate [WARNING:\nSee 2021-12-02 17:16 ET: Debrief on why that may not be]\n2021-12-01 1:30pm ET: Launch of 12.39\nAnalysis of 12.38\nLaunch of 12.39\nDiscussion\nAnalysis\n2021-12-01 8:39am ET: 12.38 True Adam with Lower LR\n2021-12-01 2:21am ET: [Stephen oncall] Run 12.37 [WARNING: See 2021-12-02 17:16 ET: Debrief on why\nit may not be SGD]\n2021-11-30 7:24pm ET: [Stephen oncall] Run 12.37 Manual requeue of 12.36. [WARNING: See 2021-12-02\n17:16 ET: Debrief on why it may not be SGD]\n2021-11-20 7:24pm ET: [Stephen oncall]\n2021-11-30 10:10am PT: 12.36 restart from 37k, SGD mimicking  [WARNING: See 2021-12-02 17:16 ET:\nDebrief on why it may not be SGD]\n2021-11-30 9:00am PT: 12.35 restart from 37k, SGD mimicking  [WARNING: See 2021-12-02 17:16 ET:\nDebrief on why it may not be SGD]\n2021-11-30 9:00am ET: 12.34 requeue\n2021-11-29 7:43pm PT [Susan]: 12.34 restart\n2021-11-30 7:43pm PT [Susan]: 12.33 requeue\n2021-11-28 6:34pm ET [Stephen]: 12.33\n2021-11-28 5:52pm ET [Stephen]: 12.32\n2021-11-28 12:28pm ET [Stephen]: 12.31\n2021-11-28 10:09am ET [Stephen]: 12.30\n2021-11-28 9:41am ET [Stephen]: 12.29\n2021-11-28 3:20am ET [Stephen]: 12.28\n2021-11-28 1:50am ET [Stephen]: 12.27\n2021-11-27 11:39 ET [Stephen]: Run 12.26\n2021-11-27 6:10pm PT: Run 12.25 [Susan restart]\n2021-11-27 10:59am PT: Run 12.24 [Myle rerunning job, but AFK rest of day]\n2021-11-26 9:47am ET [Stephen managing cluster]\n2021-11-25 8:53am ET [Susan]: Run 12.23\n2021-11-25 11:35am ET [Myle]: Run 12.22\n2021-11-25 11:20am ET: Run 12.21 (requeue)\n2021-11-24 11:18pm ET [Susan]: Run 12.21\n2021-11-24 10:40pm ET [Susan]: Run 12.20\n2021-11-24 3:30pm ET [Susan]: Run 12.19\n2021-11-24 2:10pm ET [Susan]: Run 12.18\n2021-11-24 1:00pm ET [Susan]: Run 12.17\n\n2021-11-23 10:50am [Myle]: Run 12.16\n2021-11-22 7:45am [Myle]: Run 12.15\n2021-11-21 4:15pm [Myle]: Run 12.14\n2021-11-21 3pm [Myle]: Run 12.13\n2021-11-21 Analysis of 12.X series\n2021-11-20 9:30pm [Myle]: Run 12.12\n2021-11-19 9:30am [Myle]: Run 12.11\n2021-11-18 List of issues for Cloud Complaints\n2021-11-18 5:30pm [Stephen]: Notes from 12.10\n2021-11-18 3pm [Stephen]: Run 12.10\n2021-11-17 11pm [Myle]: Run 12.09\n2021-11-16 11pm [Myle]: Run 12.08\n2021-11-14 9:45pm [Myle]: Run 12.07\n2021-11-12 10:30pm [Myle]: Run 12.06\n2021-11-12 7pm [Susan]: Run 12.05\n2021-11-12 6pm [Susan]: Run 12.04\n2021-11-12 3pm [Susan]: Run 12.03\nAnalysis of Run 12.02 [Susan]\n2021-11-11 11pm [Susan]: Run 12.02\n2021-11-11 5:40pm [Susan]: Run 12.01\n2021-11-11 5:30pm [Susan]: Run 12.00 - Lost a GPU on node-7, restarting.\n2021-11-10 5pm: Run 11.10\nAnalysis of 11.9\nDecision\nLaunch steps for Run 11.10\n[2021-11-10]: Run 11.9: Lowered LR to 6e-5, match exp 11.6 otherwise\nAnalysis of 11.8\nDecisions for 11.9\nLaunch steps for 11.9\n[2021-11-09]: Run 11.8: Rolling back weight decay, start from 4750\nAnalysis of 11.7\nDecisions for 11.8\nLaunch steps for 11.8\n[2021-11-09]: Run 11.7: Changing tons of stuff, start from 4750\nAnalysis\nLaunch steps for 11.7\n[2021-11-09]: Run 11.6: Starting to skip batches\nEmergency meeting notes\nAnalysis of 11.6\n[2021-11-08] Run 11.5: clip 1.5 -> 1.0\nAnalysis of 11.5\nAnalysis of 11.4\n[Undated] Run 11.4: Changed validation freq\n[2021-11-07] Run 11.3: ECC Failure\n[2021-11-06] Run 11.2: clip 2.5 -> 1.5\n\n[2021-11-05] Run 11.1: peak LR down to 7.5e-5\n[2021-11-05] Run 11.0: LETS GO\nRun 10\nRun 9\nRun 8\nRun 7\n2021-10-22: Run 6\nRun 5\nOutcome:\nRun 4\nRun 3\n[Undated] Run 2\n2021-10-20:  Run 1\nKitchen sink: Analysis of Exp 21--29\nDescription of experiments:\nExp 21 -- 23\nExp 23, 24 and 25 (Drop out BookCorpus)\nExp 26 and 27: Adding in Book Corpus\nExp 29: Manually cleaned up corpora\nLearned Embeddings\nExp 27 vs 28\nOncall Debugging\nResponsibilities\nOnboarding & Gotchas\nRun is stuck in loop of “lower loss scale”\nWPS has dropped a lot\nJob is Hanging\nPerforming health checks\nI need to reprovision/replace a node\nI need to mark a node as unusable\nRebooting a node\nI want to hot swap a node in a running job without explicitly re-launching\nTable of Contents/Index\n",
      "fetch_method": "direct-pdf"
    },
    {
      "arxiv_id": "https://raw.githubusercontent.com/facebookresearch/metaseq/main/projects/OPT/chronicles/OPT_Baselines_Logbook.pdf",
      "full_text": "OPT Baselines Logbook\n●\nAll baselines created using <redacted>.\n●\nOccasionally hyperparameters were set differently for one model.\n●\nAll use the same data as the 175B model\nTraining Log\n2022-05-17 [Susan] WE ARE DONE!!!\n●\nOne more pdsh run to push on the checkpoints to blob. Hooray!\nCopying blobs over to final blob path:\nazcopy copy \"<redacted>\" <redacted>\" --include-pattern \"checkpoint_49_143000*.pt\" --recursive\nFinal view of run16-43 (missing run36 from home dir issue and no backups):\nTensorboard\n●\nTakeaways:\n○\n66B seemed more difficult to train with 2M batch size than 175B with 2M batch size\n■\nBF16 likely would work better here (but doesn’t explain the 66B vs 175B instability)\n○\nLR may have been too low for too long, despite validation on wikipedia_en continuing to\n“improve”\n\n○\nTensorboard with wikipedia_en validation ppl\n○\nAdjusting dynamic loss scaling window (to be a function of loss scalar value) seemed to have\nhelped with stability, but won’t be needed at all when switching to bf16\n2022-05-15 [Susan] Recover failed uploads, restart with LR at 6e-6\nPDSH_RCMD_TYPE=ssh pdsh -w hpc-pg0-[9-12,14-31,34-43,45-48,50-69,71-78]\n'/shared/home/susanz/bin/azcopy copy \"<redacted>/*.pt\" \"<redacted>\"'\n●\nKicked off run 43\nBLOB_PREFIX1=\"<redacted>/66B_run42\"\nBLOB_PREFIX2=<redacted>/66B_run43\"\nRESTORE_FILE=\"${BLOB_PREFIX1}/checkpoint_48_137750.pt?${BLOB_AUTH}\"\nRUN_ID=66B_run43\n./<redacted> \\\n-n 64 -g 8 -t 1 \\\n-p $RUN_ID \\\n--azure \\\n--model-size 66b \\\n--checkpoints-dir /shared/home/susanz/checkpoints/66B/ \\\n--local-checkpoints-dir /mnt/scratch/$USER/checkpoints/$(date +%Y-%m-%d).$RUN_ID \\\n--full-azure-upload-path \"${BLOB_PREFIX2}/?${BLOB_AUTH}\" \\\n--restore-file $RESTORE_FILE\n●\nLogging hostlists given continued azcopy failures:\n30279       hpc 66B_run4   susanz  R    8:42:52     64 hpc-pg0-[9-12,14-31,34-40,42-43,45-48,50-69,71-78,84]\n2022-05-15 [Susan] Nan grads, lowered LR to 9e-6\n●\nEnd LR is kept at 0.5*LR = 4.5e-6\nBLOB_PREFIX1=\"<redacted>/66B_run40\"\nBLOB_PREFIX2=\"<redacted>/66B_run41\"\nRESTORE_FILE=\"${BLOB_PREFIX1}/checkpoint_47_135250.pt?${BLOB_AUTH}\"\nRUN_ID=66B_run41\n./<redacted> \\\n-n 64 -g 8 -t 1 \\\n-p $RUN_ID \\\n--azure \\\n\n--model-size 66b \\\n--checkpoints-dir /shared/home/susanz/checkpoints/66B/ \\\n--local-checkpoints-dir /mnt/scratch/$USER/checkpoints/$(date +%Y-%m-%d).$RUN_ID \\\n--full-azure-upload-path \"${BLOB_PREFIX2}/?${BLOB_AUTH}\" \\\n--restore-file $RESTORE_FILE\n●\nNode failures on host 13, did not auto-recover. Manually drained, and resuming again from the same\npoint given intermittent azcopy failures too.\n○\nAdded “args.requeue_on_fail = True” to sweep (wasn’t there before).\n●\nRelaunched exactly the same as run 41, just incremented to run 42.\n●\nAzcopy failures persist, tracking hostlist to recover from:\n30152       hpc 66B_run4   susanz  R    6:49:09     64 hpc-pg0-[9-12,14-31,34-43,45-48,50-69,71-78]\n2022-05-13 [Susan] 65 hosts in drain for IB issues, lowered LR to 1.2e-5\n●\nEnd LR is kept at 0.5*LR = 6e-6\nBLOB_PREFIX1=\"<redacted>66B_run39\"\nBLOB_PREFIX2=<redacted>/66B_run40\"\nRESTORE_FILE=\"${BLOB_PREFIX1}/checkpoint_45_130500.pt?${BLOB_AUTH}\"\nRUN_ID=66B_run40\nEXCLUDED_HOSTS=<redacted> \\\n./<redacted> \\\n-n 64 -g 8 -t 1 \\\n-p $RUN_ID \\\n--azure \\\n--model-size 66b \\\n--checkpoints-dir /shared/home/susanz/checkpoints/66B/ \\\n--local-checkpoints-dir /mnt/scratch/$USER/checkpoints/$(date +%Y-%m-%d).$RUN_ID \\\n--full-azure-upload-path \"${BLOB_PREFIX2}/?${BLOB_AUTH}\" \\\n--restore-file $RESTORE_FILE\n2022-05-11 [Susan] Nans in grad, lost GPU on node 32\n2022-05-11 11:22:12 | INFO | fairseq.trainer | NOTE: floating point error detected, ignoring gradient, Fatal error:\ngradients are inconsistent between workers. Try --ddp-backend=legacy_ddp. Or are you mixing up different\ngeneration of GPUs in training?\n●\nfixmyazure drained node 32 for lost GPU error.\n●\nOverall things look pretty healthy for now. Missing chunk of logs in the middle from when the clusters’\nhome directory got swamped / wiped by another team’s usage.  Logs are now getting backed up\nperiodically to cloud.\n\nTensorboard\n●\nValidation ppl on wikipedia looks good too:\nTensorboard\n\n2022-05-07 [Susan] Model is finally chugging along\n●\nNo more learning rate changes since last entry.\n●\nA few restarts from hardware failures.\n●\nWe are about ~73% through.\n●\nETA for completion: ~9 days\n●\nSpot-checked validation ppls: seems to still be improving, though more slowly now (LR is at 1e-5).\n2022-04-27 [Susan] Lowering LR to 1.6e-5, restart @ 63,750\n●\nAlso increase end LR to be 0.5 * start LR\nBLOB_PREFIX1=\"<redacted>/66B_run35\"\nBLOB_PREFIX2=\"<redacted>/66B_run36\"\nRESTORE_FILE=\"${BLOB_PREFIX1}/checkpoint_22_63750.pt?${BLOB_AUTH}\"\nRUN_ID=66B_run36\n./<redacted> \\\n-n 64 -g 8 -t 1 \\\n-p $RUN_ID \\\n--azure \\\n--model-size 66b \\\n--checkpoints-dir /shared/home/susanz/checkpoints/66B/ \\\n--local-checkpoints-dir /mnt/scratch/$USER/checkpoints/$(date +%Y-%m-%d).$RUN_ID \\\n--full-azure-upload-path \"${BLOB_PREFIX2}/?${BLOB_AUTH}\" \\\n--restore-file $RESTORE_FILE\n2022-04-25 [Susan] Nan/Inf grads @ 56029, skipping batches from\nswallowing FloatingPointError\n●\nNext restart should increase end LR - maybe keep at original end LR (6e-6), or leave flat / try\nincreasing end LR to warm up to a point where things crash\n●\nValidation ppl on Wikipedia is still dropping, though now only 0.01 every 1k steps.\n●\nConsistent crashing at 56029 step.\n●\nTried:\n○\n1.6e-5 start lr, 8e-6 end lr, no good.\n○\n1.2e-6 start lr, 6e-6 end lr, no good.\n●\nTrying start lr of 1e-6 with end lr at 1e-5 (higher end LR than start, to try and “warm up LR” again).\n○\nDidn’t work.\n●\nTried swapping shard 20 and 29 (experiment 32), didn’t work.\n○\nUsed: <redacted> as new data dir\n●\nDrastically cutting LR (since the shard swapping indicates a 0 LR would also not be useful): trying now\nwith starting LR of 2e-6 (end LR of 1e-6, for experiment 33).\n○\nDidn’t work.\n○\nGrad norm ends up being nan. Need clipping?\n●\nRestarting with LR back to 2e-5, but increase clip to 0.25 (experiment 34).\n○\nDidn’t work (didn’t clip anything).\n○\nTrying with clip down to 0.22 (still experiment 34).\n○\nStill doesn’t work, stuck at same place (56029).\n○\nTrying with commenting out throwing FloatingPointError (and skip the batch?) - experiment 35\n○\nOk this worked. -____-\n\nBLOB_PREFIX1=\"<redacted>/66B_run30\"\nBLOB_PREFIX2=\"<redacted>/66B_run35\"\nRESTORE_FILE=\"${BLOB_PREFIX1}/checkpoint_20_56000.pt?${BLOB_AUTH}\"\nRUN_ID=66B_run35\n./<redacted> \\\n-n 64 -g 8 -t 1 \\\n-p $RUN_ID \\\n--azure \\\n--model-size 66b \\\n--checkpoints-dir /shared/home/susanz/checkpoints/66B/ \\\n--local-checkpoints-dir /mnt/scratch/$USER/checkpoints/$(date +%Y-%m-%d).$RUN_ID \\\n--full-azure-upload-path \"${BLOB_PREFIX2}/?${BLOB_AUTH}\" \\\n--restore-file $RESTORE_FILE\n2022-04-22 [Susan] Keep lower LR of 2e-5, resume from 48k\n●\nThings keep crashing\n●\nTime to lower LR from an earlier checkpoint\n●\n(wiki) validation looks “ok” though after lower LR of 2e-5 (continues to drop), so lowering it earlier\nshouldn’t hurt.\n●\nLoss scales started looking rough after 48k, hence resuming from 48k with a lower LR.\nTensorboard\nBLOB_PREFIX1=\"<redacted>/66B_run28\"\nBLOB_PREFIX2=\"<redacted>/66B_run30\"\nRESTORE_FILE=\"${BLOB_PREFIX1}/checkpoint_17_48000.pt?${BLOB_AUTH}\"\nRUN_ID=66B_run30\n./<redacted> \\\n-n 64 -g 8 -t 1 \\\n-p $RUN_ID \\\n\n--azure \\\n--model-size 66b \\\n--checkpoints-dir /shared/home/susanz/checkpoints/66B/ \\\n--local-checkpoints-dir /mnt/scratch/$USER/checkpoints/$(date +%Y-%m-%d).$RUN_ID \\\n--full-azure-upload-path \"${BLOB_PREFIX2}/?${BLOB_AUTH}\" \\\n--restore-file $RESTORE_FILE\n2022-04-22 [Susan] Lower LR to 2.4e-5 2e-5, resume from 50k\nBLOB_PREFIX1=\"<redacted>/66B_run28\"\nBLOB_PREFIX2=\"<redacted>/66B_run29\"\nRESTORE_FILE=\"${BLOB_PREFIX1}/checkpoint_17_50000.pt?${BLOB_AUTH}\"\nRUN_ID=66B_run29\n./<redacted> \\\n-n 64 -g 8 -t 1 \\\n-p $RUN_ID \\\n--azure \\\n--model-size 66b \\\n--checkpoints-dir /shared/home/susanz/checkpoints/66B/ \\\n--local-checkpoints-dir /mnt/scratch/$USER/checkpoints/$(date +%Y-%m-%d).$RUN_ID \\\n--full-azure-upload-path \"${BLOB_PREFIX2}/?${BLOB_AUTH}\" \\\n--restore-file $RESTORE_FILE\n2022-04-19 [Susan]\nBLOB_PREFIX1=\"<redacted>/66B_run25\"\nBLOB_PREFIX2=\"<redacted>/66B_run26\"\nRESTORE_FILE=\"${BLOB_PREFIX1}/checkpoint_14_40000.pt?${BLOB_AUTH}\"\nRUN_ID=66B_run26\n./<redacted> \\\n-n 64 -g 8 -t 1 \\\n-p $RUN_ID \\\n--azure \\\n--model-size 66b \\\n--checkpoints-dir /shared/home/susanz/checkpoints/66B/ \\\n--local-checkpoints-dir /mnt/scratch/$USER/checkpoints/$(date +%Y-%m-%d).$RUN_ID \\\n--full-azure-upload-path \"${BLOB_PREFIX2}/?${BLOB_AUTH}\" \\\n--restore-file $RESTORE_FILE\n●\nHung after crashing log scale, didn’t recover.\n●\nIncreasing LR to 8e-5, restarting from 41500.\n○\nNope, bad idea. Should stick with the convention of lowering LR. Down to 3e-5.  Still calling this\nrun27 since the path will be different with different LR.\nBLOB_PREFIX1=\"<redacted>/66B_run26\"\nBLOB_PREFIX2=\"<redacted>/66B_run27\"\nRESTORE_FILE=\"${BLOB_PREFIX1}/checkpoint_15_41500.pt?${BLOB_AUTH}\"\nRUN_ID=66B_run27\n./<redacted> \\\n-n 64 -g 8 -t 1 \\\n-p $RUN_ID \\\n\n--azure \\\n--model-size 66b \\\n--checkpoints-dir /shared/home/susanz/checkpoints/66B/ \\\n--local-checkpoints-dir /mnt/scratch/$USER/checkpoints/$(date +%Y-%m-%d).$RUN_ID \\\n--full-azure-upload-path \"${BLOB_PREFIX2}/?${BLOB_AUTH}\" \\\n--restore-file $RESTORE_FILE\n●\nRewinding some more to 40250:\nBLOB_PREFIX1=\"<redacted>/66B_run25\"\nBLOB_PREFIX2=\"<redacted>/66B_run28\"\nRESTORE_FILE=\"${BLOB_PREFIX1}/checkpoint_14_40250.pt?${BLOB_AUTH}\"\nRUN_ID=66B_run28\n./<redacted> \\\n-n 64 -g 8 -t 1 \\\n-p $RUN_ID \\\n--azure \\\n--model-size 66b \\\n--checkpoints-dir /shared/home/susanz/checkpoints/66B/ \\\n--local-checkpoints-dir /mnt/scratch/$USER/checkpoints/$(date +%Y-%m-%d).$RUN_ID \\\n--full-azure-upload-path \"${BLOB_PREFIX2}/?${BLOB_AUTH}\" \\\n--restore-file $RESTORE_FILE\n2022-04-17 [Susan]: Gradient crashed and monitor did not kick in (given no\nlag in logging), restarting with no changes (outside of reducing logging)\nBLOB_PREFIX1=\"<redacted>/66B_run24\"\nBLOB_PREFIX2=\"<redacted>/66B_run25\"\nRESTORE_FILE=\"${BLOB_PREFIX1}/checkpoint_12_35000.pt?${BLOB_AUTH}\"\nRUN_ID=66B_run25\n./<redacted> \\\n-n 64 -g 8 -t 1 \\\n-p $RUN_ID \\\n--azure \\\n--model-size 66b \\\n--checkpoints-dir /shared/home/susanz/checkpoints/66B/ \\\n--local-checkpoints-dir /mnt/scratch/$USER/checkpoints/$(date +%Y-%m-%d).$RUN_ID \\\n--full-azure-upload-path \"${BLOB_PREFIX2}/?${BLOB_AUTH}\" \\\n--restore-file $RESTORE_FILE\n2022-04-15 [Susan]: Loss scale window logic change, LR 4e-5\n●\nChange loss scale window to also scale down with loss scale, lower bounded loss scale to 0.03125 and\ncommented out raising loss scale min threshold error (result is skipping those batches).\n●\nSeems stable for the day, with activation norm slowly trending down:\n\nTensorboard\nBLOB_PREFIX1=\"<redacted>/66B_run23\"\nBLOB_PREFIX2=\"<redacted>/66B_run24\"\nRESTORE_FILE=\"${BLOB_PREFIX1}/checkpoint_9_24750.pt?${BLOB_AUTH}\"\nRUN_ID=66B_run24\n./<redacted> \\\n-n 64 -g 8 -t 1 \\\n-p $RUN_ID \\\n--azure \\\n--model-size 66b \\\n--checkpoints-dir /shared/home/susanz/checkpoints/66B/ \\\n--local-checkpoints-dir /mnt/scratch/$USER/checkpoints/$(date +%Y-%m-%d).$RUN_ID \\\n--full-azure-upload-path \"${BLOB_PREFIX2}/?${BLOB_AUTH}\" \\\n--restore-file $RESTORE_FILE\n2022-04-14 [Susan]: Things looked good but nope.\n●\nActivation norm curve looks more sane, loss scales look better too (orange vs pale orange before)\n\nTensorboard\n●\nUpdate @ 4PM CDT: Spoke too soon - need to restart from 22.5k\n○\nLowered LR to 5e-5\nBLOB_PREFIX1=\"<redacted>/66B_run22\"\nBLOB_PREFIX2=\"<redacted>/66B_run23\"\nBLOB_AUTH=\"<redacted>\"\nRESTORE_FILE=\"${BLOB_PREFIX1}/checkpoint_8_22500?${BLOB_AUTH}\"\nRUN_ID=66B_run23\n./<redacted> \\\n-n 64 -g 8 -t 1 \\\n-p $RUN_ID \\\n--azure \\\n--model-size 66b \\\n--checkpoints-dir /shared/home/susanz/checkpoints/66B/ \\\n--local-checkpoints-dir /mnt/scratch/$USER/checkpoints/$(date +%Y-%m-%d).$RUN_ID \\\n--full-azure-upload-path \"${BLOB_PREFIX2}/?${BLOB_AUTH}\" \\\n--restore-file $RESTORE_FILE\n\n2022-04-13 [Susan]: Restart from 27.5k with lr of 4e-5, ppl diverged at\n~28.7k\nTensorboard\n●\nLR was lowered around here previously as well\n○\nBad data batch?\nBLOB_PREFIX1=\"<redacted>/66B_run18\"\nBLOB_PREFIX2=\"<redacted>/66B_run20\"\nBLOB_AUTH=\"<redacted>\"\nRESTORE_FILE=\"${BLOB_PREFIX1}/checkpoint_10_27500.pt?${BLOB_AUTH}\"\nRUN_ID=66B_run20\n./<redacted> \\\n-n 64 -g 8 -t 1 \\\n-p $RUN_ID \\\n\n--azure \\\n--model-size 66b \\\n--checkpoints-dir /shared/home/susanz/checkpoints/66B/ \\\n--local-checkpoints-dir /mnt/scratch/$USER/checkpoints/$(date +%Y-%m-%d).$RUN_ID \\\n--full-azure-upload-path \"${BLOB_PREFIX2}/?${BLOB_AUTH}\" \\\n--restore-file $RESTORE_FILE\n●\nLowered LR again to 3e-5 and launched 66B_run21\n●\nStill no bueno\nTensorboard\n●\nSeems like we should be restarting around 24k (or even ~20k) instead, before activation norm curve\nshot up and became convex.\n●\nGoing to restart run18 (starting from 19k) with a lower LR and see if it’s more stable.\nBLOB_PREFIX1=\"<redacted>/66B_run17\"\nBLOB_PREFIX2=\"<redacted>/66B_run22\"\nBLOB_AUTH=\"<redacted>\"\nRESTORE_FILE=\"${BLOB_PREFIX1}/checkpoint_7_19000.pt?${BLOB_AUTH}\"\nRUN_ID=66B_run22\n./<redacted> \\\n-n 64 -g 8 -t 1 \\\n-p $RUN_ID \\\n--azure \\\n--model-size 66b \\\n--checkpoints-dir /shared/home/susanz/checkpoints/66B/ \\\n--local-checkpoints-dir /mnt/scratch/$USER/checkpoints/$(date +%Y-%m-%d).$RUN_ID \\\n--full-azure-upload-path \"${BLOB_PREFIX2}/?${BLOB_AUTH}\" \\\n\n--restore-file $RESTORE_FILE\n2022-04-12 [Susan]: Restart from 27.5k with lr of 6e-5, min loss scale\nreached at 28292\n●\nReducing LR from 8e-5 to 6e-5\nBLOB_PREFIX1=\"<redacted>/66B_run18\"\nBLOB_PREFIX2=\"<redacted>/66B_run19\"\nBLOB_AUTH=\"<redacted>\"\nRESTORE_FILE=\"${BLOB_PREFIX1}/checkpoint_10_27500.pt?${BLOB_AUTH}\"\nRUN_ID=66B_run19\n./<redacted> \\\n-n 64 -g 8 -t 1 \\\n-p $RUN_ID \\\n--azure \\\n--model-size 66b \\\n--checkpoints-dir /shared/home/susanz/checkpoints/66B/ \\\n--local-checkpoints-dir /mnt/scratch/$USER/checkpoints/$(date +%Y-%m-%d).$RUN_ID \\\n--full-azure-upload-path \"${BLOB_PREFIX2}/?${BLOB_AUTH}\" \\\n--restore-file $RESTORE_FILE\n2022-04-10 [Susan]: Restart from 19k, clip 1.0 -> 0.3\n●\nActivation norm + ppl diverged.\nBLOB_PREFIX1=\"<redacted>/66B_run17\"\nBLOB_PREFIX2=\"<redacted>/66B_run18\"\nBLOB_AUTH=\"<redacted>\"\nRESTORE_FILE=\"${BLOB_PREFIX1}/checkpoint_7_19000.pt?${BLOB_AUTH}\"\nRUN_ID=66B_run18\n./<redacted> \\\n-n 64 -g 8 -t 1 \\\n-p $RUN_ID \\\n--azure \\\n--model-size 66b \\\n--checkpoints-dir /shared/home/susanz/checkpoints/66B/ \\\n--local-checkpoints-dir /mnt/scratch/$USER/checkpoints/$(date +%Y-%m-%d).$RUN_ID \\\n--full-azure-upload-path \"${BLOB_PREFIX2}/?${BLOB_AUTH}\" \\\n--restore-file $RESTORE_FILE\n2022-04-07 [Susan]: Restart from 4750, no changes.\n●\nHad to fix the checkpointing path bug anyway, took the chance to upload remaining checkpoints from\n/mnt/scratch up to Azure blob.\n●\nRestarted from 4750 (last checkpoint before failure).\n●\nRun looks much more stable already. Seems like apex version is likely the culprit for instability.\nBLOB_PREFIX1=\"<redacted>/66B_run16\"\nBLOB_PREFIX2=\"<redacted>/66B_run17\"\nBLOB_AUTH=\"<redacted>\"\n\nRESTORE_FILE=\"${BLOB_PREFIX1}/checkpoint_2_4750.pt?${BLOB_AUTH}\"\nRUN_ID=66B_run17\n./<redacted> \\\n-n 64 -g 8 -t 1 \\\n-p $RUN_ID \\\n--azure \\\n--model-size 66b \\\n--checkpoints-dir /shared/home/susanz/checkpoints/66B/ \\\n--local-checkpoints-dir /mnt/scratch/$USER/checkpoints/$(date +%Y-%m-%d).$RUN_ID \\\n--full-azure-upload-path \"${BLOB_PREFIX2}/?${BLOB_AUTH}\" \\\n--restore-file $RESTORE_FILE\n2022-04-06 [Susan]: Restart 66B from scratch. LR @ 8e-5, clip @ 1.0.\nRemove no-reshard-after-forward, remove padding (previous validation fix).\nBranch susan/66b_apr6_restart from fairseq-big-internal.\nConda env: fairseq-20210913-old-apex\nBLOB_PREFIX1=\"<redacted>/66B_run16\"\nBLOB_AUTH=\"<redacted>\"\nRUN_ID=66B_run16\n./<redacted> \\\n-n 64 -g 8 -t 1 \\\n-p $RUN_ID \\\n--azure \\\n--model-size 66b \\\n--checkpoints-dir /shared/home/susanz/checkpoints/66B/ \\\n--local-checkpoints-dir /mnt/scratch/$USER/checkpoints/$(date +%Y-%m-%d).$RUN_ID \\\n--full-azure-upload-path \"${BLOB_PREFIX2}/?${BLOB_AUTH}\"\n●\nTested batch size of 3M, slowed down WPS dramatically.\n●\nCheckpoints not being uploaded, need to pull from scratch space on hosts:\n20938       hpc 66B_run1   susanz  R    7:03:28     64\nhpc-pg0-[2-3,40-70,76-80,83-97,99-100,121,128-135]\nPDSH_RCMD_TYPE=ssh pdsh -w hpc-pg0-[2-3,40-70,76-80,83-97,99-100,121,128-135]\n'/shared/home/susanz/bin/azcopy copy \"<redacted>/*.pt\" \"<redacted>/66B_run16?<redacted>\"'\n2022-04-05 [Susan]: revert apex version, keep LR @ 4e-6, move clip back\nup to 0.3, restart from 38.5k\n(Susan’s conda env: fairseq-20210913-old-apex ->\n/shared/home/susanz/miniconda3/envs/fairseq-20210913-old-apex)\nBLOB_PREFIX1=\"<redacted>/66B_run12\"\nBLOB_PREFIX2=\"<redacted>/66B_run13\"\nBLOB_AUTH=\"<redacted>\"\nRESTORE_FILE=\"${BLOB_PREFIX1}/checkpoint_14_38500.pt?${BLOB_AUTH}\"\nRUN_ID=66B_run13\n\n./<redacted> \\\n-n 64 -g 8 -t 1 \\\n-p $RUN_ID \\\n--azure \\\n--model-size 66b \\\n--checkpoints-dir /shared/home/susanz/checkpoints/66B/ \\\n--local-checkpoints-dir /mnt/scratch/$USER/checkpoints/$(date +%Y-%m-%d).$RUN_ID \\\n--full-azure-upload-path \"${BLOB_PREFIX2}/?${BLOB_AUTH}\" \\\n--restore-file $RESTORE_FILE\n●\nStill looks to be problematic. Reducing clip down to 0.25 (job was stalled from being held by someone\non the cluster anyway).\nBLOB_PREFIX1=\"<redacted>/66B_run12\"\nBLOB_PREFIX2=\"<redacted>/66B_run14\"\nBLOB_AUTH=\"<redacted>\"\nRESTORE_FILE=\"${BLOB_PREFIX1}/checkpoint_14_38500.pt?${BLOB_AUTH}\"\nRUN_ID=66B_run14\n./<redacted> \\\n-n 64 -g 8 -t 1 \\\n-p $RUN_ID \\\n--azure \\\n--model-size 66b \\\n--checkpoints-dir /shared/home/susanz/checkpoints/66B/ \\\n--local-checkpoints-dir /mnt/scratch/$USER/checkpoints/$(date +%Y-%m-%d).$RUN_ID \\\n--full-azure-upload-path \"${BLOB_PREFIX2}/?${BLOB_AUTH}\" \\\n--restore-file $RESTORE_FILE\nTensorboard\n●\nEffect of changing clipping from 0.3 (pink) -> 0.25 (green)\n\n●\nLR is now at 2e-6 already\n2022-04-04 [Susan]: actv_norm exploding again, lowering LR to 4e-6, clip to\n0.2, restart from 36,250\nBLOB_PREFIX1=\"<redacted>/66B_run11\"\nBLOB_PREFIX2=\"<redacted>/66B_run12\"\nBLOB_AUTH=\"<redacted>\"\nRESTORE_FILE=\"${BLOB_PREFIX1}/checkpoint_13_36250.pt?${BLOB_AUTH}\"\nRUN_ID=66B_run12\n./<redacted> \\\n-n 64 -g 8 -t 1 \\\n-p $RUN_ID \\\n--azure \\\n--model-size 66b \\\n--checkpoints-dir /shared/home/susanz/checkpoints/66B/ \\\n--local-checkpoints-dir /mnt/scratch/$USER/checkpoints/$(date +%Y-%m-%d).$RUN_ID \\\n--full-azure-upload-path \"${BLOB_PREFIX2}/?${BLOB_AUTH}\" \\\n--restore-file $RESTORE_FILE\n2022-04-03 [Susan]: actv_norm exploding again, lowering LR to 6e-6,\nrestart from 34k again\nTensorboard\nBLOB_PREFIX1=\"<redacted>/66B_run09\"\nBLOB_PREFIX2=\"<redacted>/66B_run11\"\nBLOB_AUTH=\"<redacted>\"\n\nRESTORE_FILE=\"${BLOB_PREFIX1}/checkpoint_12_34000.pt?${BLOB_AUTH}\"\nRUN_ID=66B_run11\n./<redacted> \\\n-n 64 -g 8 -t 1 \\\n-p $RUN_ID \\\n--azure \\\n--model-size 66b \\\n--checkpoints-dir /shared/home/susanz/checkpoints/66B/ \\\n--local-checkpoints-dir /mnt/scratch/$USER/checkpoints/$(date +%Y-%m-%d).$RUN_ID \\\n--full-azure-upload-path \"${BLOB_PREFIX2}/?${BLOB_AUTH}\" \\\n--restore-file $RESTORE_FILE\n2022-04-01 [Susan]: actv_norm exploding again, lowering LR to 8e-6,\nrestart from 34k\nTensorboard\nBLOB_PREFIX1=\"<redacted>/66B_run09\"\nBLOB_PREFIX2=\"<redacted>/66B_run10\"\nBLOB_AUTH=\"<redacted>\"\nRESTORE_FILE=\"${BLOB_PREFIX1}/checkpoint_12_34000.pt?${BLOB_AUTH}\"\nRUN_ID=66B_run10\n./<redacted> \\\n-n 64 -g 8 -t 1 \\\n-p $RUN_ID \\\n--azure \\\n--model-size 66b \\\n--checkpoints-dir /shared/home/susanz/checkpoints/66B/ \\\n--local-checkpoints-dir /mnt/scratch/$USER/checkpoints/$(date +%Y-%m-%d).$RUN_ID \\\n--full-azure-upload-path \"${BLOB_PREFIX2}/?${BLOB_AUTH}\" \\\n--restore-file $RESTORE_FILE\n\n2022-04-01 [Susan]: actv_norm exploding again, lowering LR to 1e-5,\nrestart from 31k\nTensorboard\nBLOB_PREFIX1=\"<redacted>/66B_run08\"\nBLOB_PREFIX2=\"<redacted>/66B_run09\"\nBLOB_AUTH=\"<redacted>\"\nRESTORE_FILE=\"${BLOB_PREFIX1}/checkpoint_11_31000.pt?${BLOB_AUTH}\"\nRUN_ID=66B_run09\n./<redacted> \\\n-n 64 -g 8 -t 1 \\\n-p $RUN_ID \\\n--azure \\\n--model-size 66b \\\n--checkpoints-dir /shared/home/susanz/checkpoints/66B/ \\\n--local-checkpoints-dir /mnt/scratch/$USER/checkpoints/$(date +%Y-%m-%d).$RUN_ID \\\n--full-azure-upload-path \"${BLOB_PREFIX2}/?${BLOB_AUTH}\" \\\n--restore-file $RESTORE_FILE\n\n2022-04-01 [Susan]: actv_norm exploding again, lowering LR to 2e-5,\nrestart from 27.75k\nTensorboard\nBLOB_PREFIX1=\"<redacted>/66B_run07\"\nBLOB_PREFIX2=\"<redacted>/66B_run08\"\nBLOB_AUTH=\"<redacted>\"\nRESTORE_FILE=\"${BLOB_PREFIX1}/checkpoint_10_27750.pt?${BLOB_AUTH}\"\nRUN_ID=66B_run08\n/<redacted> \\\n-n 64 -g 8 -t 1 \\\n-p $RUN_ID \\\n--azure \\\n--model-size 66b \\\n--checkpoints-dir /shared/home/susanz/checkpoints/66B/ \\\n--local-checkpoints-dir /mnt/scratch/$USER/checkpoints/$(date +%Y-%m-%d).$RUN_ID \\\n--full-azure-upload-path \"${BLOB_PREFIX2}/?${BLOB_AUTH}\" \\\n--restore-file $RESTORE_FILE\n\n2022-03-31 [Susan]: actv_norm exploding again, lowering LR to 3e-5,\nrestart from 25.75k\nTensorboard\n●\nReverting to 25.75k when loss scale was still “healthy” at around 0.5, lowering LR again from 4e-5 to\n3e-5\nBLOB_PREFIX1=\"<redacted>/66B_run06\"\nBLOB_PREFIX2=\"<redacted>/66B_run07\"\nBLOB_AUTH=\"<redacted>\"\nRESTORE_FILE=\"${BLOB_PREFIX1}/checkpoint_9_25750.pt?${BLOB_AUTH}\"\nRUN_ID=66B_run07\n./<redacted> \\\n-n 64 -g 8 -t 1 \\\n-p $RUN_ID \\\n--azure \\\n--model-size 66b \\\n--checkpoints-dir /shared/home/susanz/checkpoints/66B/ \\\n--local-checkpoints-dir /mnt/scratch/$USER/checkpoints/$(date +%Y-%m-%d).$RUN_ID \\\n--full-azure-upload-path \"${BLOB_PREFIX2}/?${BLOB_AUTH}\" \\\n--restore-file $RESTORE_FILE\n2022-03-29 [Susan]: actv_norm exploding again, lowering LR to 4e-5,\nrestart from 16.75k\n\nTensorboard\nTensorboard\n\nTensorboard\nTensorboard\n●\nAccidentally deleted the run05 logs from the cluster, took above screenshots before tensorboard\nrefreshed.\n●\nLowered LR to 4e-5\n●\nRelaunched with:\nBLOB_PREFIX1=\"<redacted>/66B_run05\"\nRESTORE_FILE=\"${BLOB_PREFIX1}/checkpoint_6_16750.pt?${BLOB_AUTH}\"\nBLOB_PREFIX2=\"<redacted>/66B_run06\"\nBLOB_AUTH=\"<redacted>\"\nRUN_ID=66B_run06\n./<redacted> \\\n-n 64 -g 8 -t 1 \\\n-p $RUN_ID \\\n--azure \\\n--model-size 66b \\\n\n--checkpoints-dir /shared/home/susanz/checkpoints/66B/ \\\n--local-checkpoints-dir /mnt/scratch/$USER/checkpoints/$(date +%Y-%m-%d).$RUN_ID \\\n--full-azure-upload-path \"${BLOB_PREFIX2}/?${BLOB_AUTH}\" \\\n--restore-file $RESTORE_FILE\n●\nDoing some more comparisons before data is refreshed in tb\nTensorboard\nTensorboard\n\n2022-03-28 [Susan]: Checking in - turning on clip 0.3 earlier seems to have\nbeen necessary\nTensorboard\n●\nFrom clip == 0.3, we start clipping much earlier (blue vs red, where clip == 1.0).\n●\nLoss scales look healthier after clipping too (doesn’t crash to < 0.25).\n●\nActivation norm is also lowered.\n2022-03-27 [Susan]: Restarting from 13,750, with clip == 0.3\n●\nActivation norm at infinity\n\nTensorboard\n●\nSeems like lowering clip to 0.3 and restarting from 17,750 wasn’t drastic enough. Trying a rollback to\neven further back.\n●\nLR is already set at the same value as what we had for the 175B. Batch size is also the same (2M).\n○\nWe did lower LR to 3e-5 after ~91k in the 175B run. Worst case we do this earlier here too.\nBLOB_PREFIX=\"<redacted>/66B\"\nBLOB_AUTH=\"<redacted>\"\nRESTORE_FILE=\"${BLOB_PREFIX}/checkpoint_5_13750.pt?${BLOB_AUTH}\"\nRUN_ID=66B_run04\n./<redacted> \\\n-n 64 -g 8 -t 1 \\\n-p $RUN_ID \\\n--azure \\\n--model-size 66b \\\n--checkpoints-dir /shared/home/susanz/checkpoints/66B/ \\\n--local-checkpoints-dir /mnt/scratch/$USER/checkpoints/$(date +%Y-%m-%d).$RUN_ID \\\n--full-azure-upload-path \"${BLOB_PREFIX}/?${BLOB_AUTH}\" \\\n--restore-file $RESTORE_FILE\n●\nAuto restart ended up resuming from the end of run03, given the same blob prefix. Needed to restart\nagain with separating the restore file path from the upload path:\nBLOB_PREFIX1=\"<redacted>/66B\"\nRESTORE_FILE=\"${BLOB_PREFIX1}/checkpoint_5_13750.pt?${BLOB_AUTH}\"\nBLOB_PREFIX2=\"<redacted>/66B_run05\"\nBLOB_AUTH=\"<redacted>\"\nRUN_ID=66B_run05\n./<redacted> \\\n-n 64 -g 8 -t 1 \\\n\n-p $RUN_ID \\\n--azure \\\n--model-size 66b \\\n--checkpoints-dir /shared/home/susanz/checkpoints/66B/ \\\n--local-checkpoints-dir /mnt/scratch/$USER/checkpoints/$(date +%Y-%m-%d).$RUN_ID \\\n--full-azure-upload-path \"${BLOB_PREFIX2}/?${BLOB_AUTH}\" \\\n--restore-file $RESTORE_FILE\n2022-03-26 [Susan]: Restarting from 17,750, lowering clip to 0.3\nTensorboard\n●\nDiverged and looks like it was unable to recover\n●\nRestarting from 17,750, before we started clipping heavily when clip was 1.0\nBLOB_PREFIX=\"<redacted>/66B\"\nBLOB_AUTH=\"<redacted>\"\nRESTORE_FILE=\"${BLOB_PREFIX}/checkpoint_7_17750.pt?${BLOB_AUTH}\"\nRUN_ID=66B_run03\nEXCLUDED_HOSTS=hpc-pg0-[9,11] \\\n./<redacted> \\\n-n 64 -g 8 -t 1 \\\n-p $RUN_ID \\\n--azure \\\n--model-size 66b \\\n--checkpoints-dir /shared/home/susanz/checkpoints/66B/ \\\n--local-checkpoints-dir /mnt/scratch/$USER/checkpoints/$(date +%Y-%m-%d).$RUN_ID \\\n--full-azure-upload-path \"${BLOB_PREFIX}/?${BLOB_AUTH}\" \\\n--restore-file $RESTORE_FILE\n●\nManual host exclusion due to\nhttps://github.com/fairinternal/cluster-health/pull/4#discussion_r835766331\n2022-03-21 [Susan]: Restarting from 3.75k, validation on\n●\nRebased susan/66b_mar20_restart on top of main, after validation fix went in\nLOB_PREFIX=\"<redacted>/66B\"\nBLOB_AUTH=\"<redacted>\"\nRESTORE_FILE=\"${BLOB_PREFIX}/checkpoint_2_3750.pt?${BLOB_AUTH}\"\n\nRUN_ID=66B_run02\n./<redacted> \\\n-n 64 -g 8 -t 1 \\\n-p $RUN_ID \\\n--azure \\\n--model-size 66b \\\n--checkpoints-dir /shared/home/susanz/checkpoints/66B/ \\\n--local-checkpoints-dir /mnt/scratch/$USER/checkpoints/$(date +%Y-%m-%d).$RUN_ID \\\n--full-azure-upload-path \"${BLOB_PREFIX}/?${BLOB_AUTH}\" \\\n--restore-file $RESTORE_FILE\nCluster maintenance\n●\n9 hosts in drain\n●\nhpc-pg0-6 seems to have ECC uncorrectable errors\n●\nhpc-pg0-114 seem to be diagnosed with bad IB as well\n●\nhpc-pg0-[46,55,68] all seem to have lost GPU\n●\nRest were identified and reported as having bad IB already\n●\nPip installed new cluster-health module: https://github.com/fairinternal/cluster-health\n2022-03-20 [Susan]: Relaunching 66B from scratch, from latest code, 6e-5\nLR from start\n●\nBring us up to the point of latest fairseq-big-internal\n●\nBranched off for 66B run: https://github.com/fairinternal/fairseq-big-internal/pull/128\n●\nDiscussions w/ Stephen & Anj on restarting clean, mainly to catch new code changes / potential env\ndifferences (wps diffs ended up being IB issues)\nBLOB_PREFIX=\"<redacted>/66B\"\nBLOB_AUTH=\"<redacted>\"\nRUN_ID=66B_run01\n./<redacted> \\\n-n 64 -g 8 -t 1 \\\n-p $RUN_ID \\\n--azure \\\n--model-size 66b \\\n--checkpoints-dir /shared/home/susanz/checkpoints/66B/ \\\n--local-checkpoints-dir /mnt/scratch/$USER/checkpoints/$(date +%Y-%m-%d).$RUN_ID \\\n--full-azure-upload-path \"${BLOB_PREFIX}/?${BLOB_AUTH}\"\n●\nActivation norm drops ~200 steps in with 6e-5 LR\n\nTensorboard\n2022-03-20 [Anjali] Made it past the last instability point, activ_norm is\ntrending up\nTensorboard\nPPL went below lowest point but then increased again\n\nTensorboard\nWe will need to restart the job with 6e-5 way before than we did i.e at 21.25k. Maybe at 10k or even earlier.\n2022-03-19 [Anjali]  Activ norm is blowing up. Restarting job with lower\nLR=6.0e-5\nTensorboard\nTensorboard\n\nRestarting job with a lower LR of 6e-5.\nRUN_ID=66B_run03.21\nRESTORE_BLOB_PREFIX=\"<redacted>/66B_run03.20\"\nBLOB_PREFIX=\"<redacted>/66B_run03.21\"\nBLOB_AUTH=\"<redacted>\"\nRESTORE_FILE=\"${RESTORE_BLOB_PREFIX}/checkpoint_8_21250.pt?${BLOB_AUTH}\"\nRUN_ID=66B_run03.22\nRESTORE_BLOB_PREFIX=\"<redacted>/66B_run03.20\"\nBLOB_PREFIX=\"<redacted>/66B_run03.22\"\nBLOB_AUTH=\"<redacted>\"\nRESTORE_FILE=\"${RESTORE_BLOB_PREFIX}/checkpoint_8_21250.pt?${BLOB_AUTH}\"\npython -m <redacted baselines script>     -n 64 -g 8 -t 1     -p $RUN_ID     --checkpoints-dir\n/shared/home/anj/checkpoints/66B/     --local-checkpoints-dir /mnt/scratch/anj/checkpoints/$(date\n+%Y-%m-%d).$RUN_ID     --full-azure-upload-path \"${BLOB_PREFIX}/?${BLOB_AUTH}\"     --model-size 66b\n--restore-file $RESTORE_FILE\nRestarted at the right point:\nTensorboard\n2022-03-19 [Anjali] Spike again at 21.5k steps and monitoring\nTensorboard\n\n2022-03-19 [Anjali] Spike of activation norm at 20k steps but trending down\nTensorboard\nAnother thing I observed is that post the clip-norm change the wps has dropped from 126k to 123k. Still within\nacceptable range but we should follow up:\nTensorboard\n2022-03-18 [Anjali] Activ norm spiking, restarting job with clip-norm=0.3\nSlight spike in the activation norm. PPL seems ok so monitoring for now since there have been spikes\npreviously\n\nTensorboard\nLooks like activ norm is getting unstable:\nTensorboard\nRestarting job with clip-norm=0.3\nRUN_ID=66B_run03.20\nRESTORE_BLOB_PREFIX=\"<redacted>/66B_run03.19\"\nBLOB_PREFIX=\"<redacted>/66B_run03.20\"\nBLOB_AUTH=\"<redacted>\"\nRESTORE_FILE=\"${RESTORE_BLOB_PREFIX}/checkpoint_6_16500.pt?${BLOB_AUTH}\"\npython -m <redacted baselines script>     -n 64 -g 8 -t 1     -p $RUN_ID     --checkpoints-dir\n/shared/home/anj/checkpoints/66B/     --local-checkpoints-dir /mnt/scratch/anj/checkpoints/$(date\n+%Y-%m-%d).$RUN_ID     --full-azure-upload-path \"${BLOB_PREFIX}/?${BLOB_AUTH}\"     --model-size 66b\n--restore-file $RESTORE_FILE\n2022-03-18 [Anjali] Job failed, restarting again at 16k (or where the job died)\nCheckpoint from 14k step.\nBranch: anj/66b_gcmf_1\nSame LR: 8.0e-5\nRUN_ID=66B_run03.18\nRESTORE_BLOB_PREFIX=\"<redacted>/66B_run03.01\"\n\nBLOB_PREFIX=\"<redacted>/66B_run03.18\"\nBLOB_AUTH=\"<redacted>\"\nRESTORE_FILE=\"${RESTORE_BLOB_PREFIX}/checkpoint_last.pt?${BLOB_AUTH}\"\npython -m <redacted baselines script>     -n 64 -g 8 -t 1     -p $RUN_ID     --checkpoints-dir\n/shared/home/anj/checkpoints/66B/     --local-checkpoints-dir /mnt/scratch/anj/checkpoints/$(date\n+%Y-%m-%d).$RUN_ID     --full-azure-upload-path \"${BLOB_PREFIX}/?${BLOB_AUTH}\"     --model-size 66b\n--restore-file $RESTORE_FILE\nLooks like I started a little too behind. I am going to fast forward to 16k steps and restart the run.\nRUN_ID=66B_run03.19\nRESTORE_BLOB_PREFIX=\"<redacted>/66B_run03.01\"\nBLOB_PREFIX=\"<redacted>/66B_run03.19\"\nBLOB_AUTH=\"<redacted>\"\nRESTORE_FILE=\"${RESTORE_BLOB_PREFIX}/checkpoint_6_16000.pt?${BLOB_AUTH}\"\npython -m <redacted baselines script>     -n 64 -g 8 -t 1     -p $RUN_ID     --checkpoints-dir\n/shared/home/anj/checkpoints/66B/     --local-checkpoints-dir /mnt/scratch/anj/checkpoints/$(date\n+%Y-%m-%d).$RUN_ID     --full-azure-upload-path \"${BLOB_PREFIX}/?${BLOB_AUTH}\"     --model-size 66b\n--restore-file $RESTORE_FILE\nLogs look to be close to the last starting point.\nLooks ok at 9:18AM PST\nTensorboard\n2022-03-17 [Anjali] Update at ~14.5k steps\nGraphs trending as below: activ norm is slowly increasing but lower than previous runs. Wps is about the\nsame. Loss and PPL are following previous trends.\n\nTensorboard\nTensorboard\n2022-03-17 [Anjali] Update at 13.5k steps with new 8.0e-5 LR\nTensorboard\nWPS is lower than before restarts:\n\nTensorboard\nHowever the TFLOPs are still 133 so what we had initially gotten. It looks like it increased steadily after 2k\nsteps so I'm going to see if we see the same behavior. Unsure why the speed was higher before.\n2022-03-17 [Anjali]: Restart 66B run from checkpoint at 12k steps\nRun failed at 2022-03-17 13:41:15 - Loss exploded\nCheckpoint from 12k step. Branch: anj/66b_gcmf_1.\nRUN_ID=66B_run03.17\nBLOB_PREFIX=\"<redacted>/66B_run03.01\"\nBLOB_AUTH=\"<redacted>\"\nRESTORE_FILE=\"${BLOB_PREFIX}/checkpoint_5_12500.pt?${BLOB_AUTH}\"\npython -m <redacted baselines script>     -n 64 -g 8 -t 1     -p $RUN_ID     --checkpoints-dir\n/shared/home/anj/checkpoints/66B/     --local-checkpoints-dir /mnt/scratch/anj/checkpoints/$(date\n+%Y-%m-%d).$RUN_ID     --full-azure-upload-path \"${BLOB_PREFIX}/?${BLOB_AUTH}\"     --model-size 66b\n--restore-file $RESTORE_FILE\nOutput logs from restarted job show wps is back to ~124k\nWe started with ~124k throughput in the beginning of the run before it got higher. Trying to understand why we\nend up increasing the throughput over time.\n\nTensorboard\nRestarted from checkpoint where there was high WPS and stability:\nTensorboard\n\n2021-03-15 [Susan]: 66B diverges, wps change upon restarts?\nTensorboard\nZooming in:\nTensorboard\n●\nTensorboard is pointing to /shared/home/anj/checkpoints/66B/tensorboard/run03/tb, so added symlinks\nto the above symlinks there as well.\n●\nNext steps:\n\n○\nRestart from ???\n2022-03-11 [Anjali]: 66B Run starts\nRUN_ID=66B_run03.16\nMP=8  Batch size=2M\n'66b':  Size(64, 9216, 72, 128, int(2.0 * M), 1.0e-4, 8),  # 66b\n2021-03-08 [Susan]: 30B run completes\nTensorboard\nTensorboard\n\n2021-03-01 [Susan]: 30B_run10, restart from 29.5k after CUDA error\n●\n30B_run09 failed with:  RuntimeError: CUDA error: unknown error\n●\nRelaunched from 29500:\nBLOB_PREFIX=\"<redacted>/30B_run04\"\nBLOB_AUTH=???\nRESTORE_FILE=\"${BLOB_PREFIX}/checkpoint_18_29500.pt?${BLOB_AUTH}\"\nRUN_ID=30B_run10\n./<redacted> \\\n-n 112 -g 8 -t 1 \\\n-p $RUN_ID \\\n--model-size 30b \\\n--checkpoints-dir /shared/home/susanz/checkpoints/30B/ \\\n--local-checkpoints-dir /mnt/scratch/$USER/checkpoints/$(date +%Y-%m-%d).$RUN_ID \\\n--restore-file $RESTORE_FILE \\\n--full-azure-upload-path \"${BLOB_PREFIX}/?${BLOB_AUTH}\"\n●\nFailed with division by zero?  Relaunching from 29000 instead.\nBLOB_PREFIX=\"<redacted>/30B_run04\"\nBLOB_AUTH=???\nRESTORE_FILE=\"${BLOB_PREFIX}/checkpoint_18_29000.pt?${BLOB_AUTH}\"\nRUN_ID=30B_run11\n./<redacted> \\\n-n 112 -g 8 -t 1 \\\n-p $RUN_ID \\\n--model-size 30b \\\n--checkpoints-dir /shared/home/susanz/checkpoints/30B/ \\\n--local-checkpoints-dir /mnt/scratch/$USER/checkpoints/$(date +%Y-%m-%d).$RUN_ID \\\n--restore-file $RESTORE_FILE \\\n--full-azure-upload-path \"${BLOB_PREFIX}/?${BLOB_AUTH}\"\n2021-02-28 [Susan]: 30B_run07, restart from 25k after grad overflow\n●\nGradient overflow issues - see loss diverging, activation norm blowing up\n\nTensorboard\n●\nRestarting from 25k.\nBLOB_PREFIX=\"<redacted>/30B_run04\"\nBLOB_AUTH=???\nRESTORE_FILE=\"${BLOB_PREFIX}/checkpoint_16_25000.pt?${BLOB_AUTH}\"\nRUN_ID=30B_run07\n./<redacted> \\\n-n 112 -g 8 -t 1 \\\n-p $RUN_ID \\\n--model-size 30b \\\n--checkpoints-dir /shared/home/susanz/checkpoints/30B/ \\\n--local-checkpoints-dir /mnt/scratch/$USER/checkpoints/$(date +%Y-%m-%d).$RUN_ID \\\n--restore-file $RESTORE_FILE \\\n--full-azure-upload-path \"${BLOB_PREFIX}/?${BLOB_AUTH}\"\n●\nIf this blows up in the same spot again, will likely have to lower LR as the next step.\n●\nBlew up in the same place again (~26k steps). Increasing checkpointing to be every 500 steps (instead\nof 1k), and lowering LR to 8e-5 (from 1e-4).\nBLOB_PREFIX=\"<redacted>/30B_run04\"\nBLOB_AUTH=???\nRESTORE_FILE=\"${BLOB_PREFIX}/checkpoint_16_25000.pt?${BLOB_AUTH}\"\nRUN_ID=30B_run08\n./<redacted> \\\n-n 112 -g 8 -t 1 \\\n-p $RUN_ID \\\n--model-size 30b \\\n--checkpoints-dir /shared/home/susanz/checkpoints/30B/ \\\n--local-checkpoints-dir /mnt/scratch/$USER/checkpoints/$(date +%Y-%m-%d).$RUN_ID \\\n--restore-file $RESTORE_FILE \\\n--full-azure-upload-path \"${BLOB_PREFIX}/?${BLOB_AUTH}\"\n\n●\nLowering LR got us past 26k!\nTensorboard\n2021-02-26 [Susan]: 30B Baseline Restart, 30B_run05, 06\n●\nRun failed due to NCCL errors.\n●\nFixmyazure catches 75 hosts with 0 IB bandwidth\n●\nTurns out IB isn’t detected (UFM not up?).\n●\nAzure folks alerted, fixed with rebooting VMs. 17 busy hosts will need to be rebooted later too, and\nmore diagnostics needed on the 7 hosts in drain (+1 from running fixmyazure afterwards).\n●\nRelaunched:\nBLOB_PREFIX=\"<redacted>/30B_run04\"\nBLOB_AUTH=???\nRESTORE_FILE=\"${BLOB_PREFIX}/checkpoint_11_18000.pt?${BLOB_AUTH}\"\nRUN_ID=30B_run05\n./<redacted> \\\n-n 112 -g 8 -t 1 \\\n-p $RUN_ID \\\n--model-size 30b \\\n--checkpoints-dir /shared/home/susanz/checkpoints/30B/ \\\n--local-checkpoints-dir /mnt/scratch/$USER/checkpoints/$(date +%Y-%m-%d).$RUN_ID \\\n--restore-file $RESTORE_FILE \\\n--full-azure-upload-path \"${BLOB_PREFIX}/?${BLOB_AUTH}\"\n\n2021-02-24 [Susan]: 30B, 66B Baseline\nhttps://github.com/fairinternal/fairseq-py/pull/3131/files\n30B [Azure]: 2x MP, 4M batch size, 112 hosts\n●\nLaunching from https://github.com/fairinternal/fairseq-py/tree/susan/30b\n●\nLogs in /shared/home/susanz/checkpoints/30B/ , 2x\n○\nGot 272k WPS with 2M batch size (run03), 112 hosts\n○\nGot 418k WPS with 4M batch size (run04), 112 hosts\n○\nKeeping 4M batch size run: http://52.190.63.124:6020/ (Tensorboard)\n■\nTFLOP calculator shows only ~117 TFLOPs / GPU utilization - will need to improve:\nSusan TFLOPs Copy\n66B [RSC]: 4x MP, 4M batch size, 128 hosts\n●\n66B baseline is waiting in queue in the RSC for benchmarking.\n2021-01-16 & 2021-01-17 [Stephen]\n●\nAlso launched a 350M\n●\nWeirdly, found it was having a really terrible time converging\n●\nLowered the LR a little bit (to 1e-3) and things went fine\n2021-01-07 [Stephen]\n●\nNaman’s run crashed, so relaunched from scratch\n○\nDue to the larger batchsize, I had to remove some of the validation sets\n○\nSo the “combined” ppl is not directly comparable\n●\nLaunched 6.7B with the other 512 GPUs. It’s more than fast enough to finish before OSHA run.\n2021-01-06 [Stephen]\n●\nLaunched 13B on azure with 512 GPUs. Observed it was very slow (25 days to finish)\n○\nNaman tweaked it and doubled the batchsize to 4B, will be done in 6 days or so\n2021-12-26 [Stephen]\n●\nDecided to pause the 6.7B, which was running too slow and isn’t very important\n●\nLaunched the 2.7B in its place.\n2021-12-24 [Stephen]\n●\nManaged to resolve all cluster issues by going back to CUDA 11.3\n●\nWe’ll have to downgrade all the clusters to a new AMI\n●\nLaunched 13B and 6.7B. 6.7B was running very slow\n2021-12-18 [Stephen]\n●\nFigured out what was up with the tokenizers segfaults\n○\nSomething about using custom compiled NCCL\n\n○\nSwitched to using the 2.11.4 version released by Nvidia + the CUDA/EFA libraries existing on\nthe cluster.\n○\nTODO: Update the AWS instructions\n○\nAlso noted that I needed to source a bunch of crap from /etc/profile.d\n●\nNote: Got all my ssh connections suddenly disconnected at one point.\n●\nLaunched the 13b baseline\n○\nDoesn’t bode well.\n○\nA node went down pretty fast before we even finished initializing: Large-25\n■\nWith the one down, not sure 248 gpus is going to be enough to launch the 6.7b baseline.\n■\nPut it in drain but it wasn’t magically replaced like before :(\n■\nLooks like it came back within an hour or so\n○\nTried again and couldn’t get past initialization.\n■\nRuntimeError: CUDA error: CUBLAS_STATUS_ALLOC_FAILED when calling\n`cublasCreate(handle)\n○\nIt failed a 3rd time so I dropped back to NCCL 2.7.8(?). That’s what’s printed in the logs, but the\nDLAMI is supposed to be 2.11.4\n■\nThis is easy to do by dropping LD_PRELOAD\n■\nThat seemed to do it – past initialization\n■\nBut I was getting 0.5x speed compared to num_workers 0.\n■\nTried again with setting NCCL_NET_SHARED_BUFFERS=0, no beuno\n○\nTrying now with the nccl 2.11.4+cuda11.5 from DLAMI\n■\nThis required me to set LD_PRELOAD to their libnccl.so\n■\nAlso refused to start\n○\nLOL I TRIED COMPILING MYSELF AND WE ARE BACK TO SEGFAULTS\n○\nBack to DLAMI versions, but with my own compiled aws-ofi-nccl:\n■\nOff the shelf, segfaults\n■\nexport FI_EFA_USE_DEVICE_RDMA=0 works.\n2021-12-10 [Stephen] Set up baselines\n●\nSet up PR for gpt-z baselines: https://github.com/fairinternal/fairseq-py/pull/2781\n●\nLaunched 125M for fun\n",
      "fetch_method": "direct-pdf"
    }
  ]
}