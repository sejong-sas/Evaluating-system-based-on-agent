{
  "4-1 (Pre-training Data)": "The provided information indicates that the OPT family—including the 125 M-parameter variant—was trained on exactly the same corpus that was first assembled for the 175 B model. This corpus is described as a union of five large-scale sources: (i) three component datasets that were originally employed in RoBERTa, (ii) a subset of the Pile, and (iii) the Pushshift.io Reddit dump (as processed in earlier work by Roller et al.). Meta’s logbook for the OPT baselines explicitly states that every baseline model, regardless of size, \"uses the same data as the 175B model,\" so the 125 M checkpoint inherits the full mixture without modification. The documentation further emphasizes that the creators applied \"the latest best practices in data collection and efficient training,\" and explicitly confirms that \"this dataset was used to pre-train the OPT models.\" In short, OPT-125 M was pre-trained on a five-part English-language web-scale corpus combining RoBERTa’s sources, a Pile subset, and processed Reddit content, with the overall goal of matching GPT-3-class performance levels while adhering to contemporary data-collection standards.",
  "4-2 (Fine-tuning Data)": "",
  "4-3 (Reinforcement Learning Data)": "",
  "4-4 (Data Filtering)": "",
  "4-1 (Pre-training Data)__evidence": [
    {
      "source": "[pdf_text]",
      "quote": "We train the OPT models to roughly match the performance and sizes of the GPT-3 class of models, while also applying the latest best practices in data collection and efficient training."
    },
    {
      "source": "[sections/C.1 Motivation]",
      "quote": "The pre-training data for training the OPT-175B model was created by a union of five datasets, including three datasets used by RoBERTa (Liu et al., 2019b), a subset of the Pile (Gao et al., 2021a), along with the Pushshift.io Reddit dataset that was developed in (Baumgartner et al., 2020) and processed in (Roller et al., 2021)."
    },
    {
      "source": "[sections/C.5 Uses]",
      "quote": "Yes, this dataset was used to pre-train the OPT models."
    },
    {
      "source": "[pdf_text]",
      "quote": "The pre-training data for training the OPT-175B model was created by a union of five datasets, including three datasets used by RoBERTa (Liu et al., 2019b), a subset of the Pile (Gao et al., 2021a), along with the Pushshift.io Reddit dataset that was developed in (Baumgartner et al., 2020) and processed in (Roller et al., 2021)."
    },
    {
      "source": "[sections/https://raw.githubusercontent.com/facebookresearch/metaseq/main/projects/OPT/chronicles/OPT_Baselines_Logbook.pdf]",
      "quote": "OPT Baselines Logbook ● All use the same data as the 175B model"
    }
  ],
  "4-2 (Fine-tuning Data)__evidence": [],
  "4-3 (Reinforcement Learning Data)__evidence": [],
  "4-4 (Data Filtering)__evidence": []
}