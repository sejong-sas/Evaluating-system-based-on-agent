{
  "repo": "facebookresearch/metaseq",
  "full_texts": [
    {
      "arxiv_id": "https://github.com/huggingface/transformers/releases/tag/v4.19.0",
      "full_text": " Release v4.19.0: OPT, FLAVA, YOLOS, RegNet, TAPEX, Data2Vec vision, FSDP integration · huggingface/transformers · GitHub Skip to content Navigation Menu Toggle navigation Sign in Appearance settings Platform GitHub Copilot Write better code with AI GitHub Spark New Build and deploy intelligent apps GitHub Models New Manage and compare prompts GitHub Advanced Security Find and fix vulnerabilities Actions Automate any workflow Codespaces Instant dev environments Issues Plan and track work Code Review Manage code changes Discussions Collaborate outside of code Code Search Find more, search less Explore Why GitHub Documentation GitHub Skills Blog Integrations GitHub Marketplace View all features Solutions By company size Enterprises Small and medium teams Startups Nonprofits By use case DevSecOps DevOps CI/CD View all use cases By industry Healthcare Financial services Manufacturing Government View all industries View all solutions Resources Topics AI DevOps Security Software Development View all Explore Learning Pathways Events &amp; Webinars Ebooks &amp; Whitepapers Customer Stories Partners Executive Insights Open Source GitHub Sponsors Fund open source developers The ReadME Project GitHub community articles Repositories Topics Trending Collections Enterprise Enterprise platform AI-powered developer platform Available add-ons GitHub Advanced Security Enterprise-grade security features Copilot for business Enterprise-grade AI features Premium Support Enterprise-grade 24/7 support Pricing Search or jump to... Search code, repositories, users, issues, pull requests... --> Search Clear Search syntax tips Provide feedback --> We read every piece of feedback, and take your input very seriously. Include my email address so I can be contacted Cancel Submit feedback Saved searches Use saved searches to filter your results more quickly --> Name Query To see all available qualifiers, see our documentation . Cancel Create saved search Sign in Sign up Appearance settings Resetting focus You signed in with another tab or window. Reload to refresh your session. You signed out in another tab or window. Reload to refresh your session. You switched accounts on another tab or window. Reload to refresh your session. Dismiss alert {{ message }} huggingface / transformers Public Notifications You must be signed in to change notification settings Fork 30.4k Star 150k Code Issues 1.1k Pull requests 919 Actions Projects 1 Security Uh oh! There was an error while loading. Please reload this page . Insights Additional navigation options Code Issues Pull requests Actions Projects Security Insights Releases v4.19.0 v4.19.0: OPT, FLAVA, YOLOS, RegNet, TAPEX, Data2Vec vision, FSDP integration Compare Choose a tag to compare Could not load tags Nothing to show {{ refName }} default Loading View all tags LysandreJik released this 12 May 14:57 &middot; 10662 commits to main since this release v4.19.0 a22db88 Disclaimer : this release is the first release with no Python 3.6 support. OPT The OPT model was proposed in Open Pre-trained Transformer Language Models by Meta AI. OPT is a series of open-sourced large causal language models which perform similar in performance to GPT3. Add OPT by @younesbelkada in #17088 FLAVA The FLAVA model was proposed in FLAVA: A Foundational Language And Vision Alignment Model by Amanpreet Singh, Ronghang Hu, Vedanuj Goswami, Guillaume Couairon, Wojciech Galuba, Marcus Rohrbach, and Douwe Kiela and is accepted at CVPR 2022. The paper aims at creating a single unified foundation model which can work across vision, language as well as vision-and-language multimodal tasks. [feat] Add FLAVA model by @apsdehal in #16654 YOLOS The YOLOS model was proposed in You Only Look at One Sequence: Rethinking Transformer in Vision through Object Detection by Yuxin Fang, Bencheng Liao, Xinggang Wang, Jiemin Fang, Jiyang Qi, Rui Wu, Jianwei Niu, Wenyu Liu. YOLOS proposes to just leverage the plain Vision Transformer (ViT) for object detection, inspired by DETR. It turns out that a base-sized encoder-only Transformer can also achieve 42 AP on COCO, similar to DETR and much more complex frameworks such as Faster R-CNN. Add YOLOS by @NielsRogge in #16848 RegNet The RegNet model was proposed in Designing Network Design Spaces by Ilija Radosavovic, Raj Prateek Kosaraju, Ross Girshick, Kaiming He, Piotr Dollár. The authors design search spaces to perform Neural Architecture Search (NAS). They first start from a high dimensional search space and iteratively reduce the search space by empirically applying constraints based on the best-performing models sampled by the current search space. RegNet by @FrancescoSaverioZuppichini in #16188 TAPEX The TAPEX model was proposed in TAPEX: Table Pre-training via Learning a Neural SQL Executor by Qian Liu, Bei Chen, Jiaqi Guo, Morteza Ziyadi, Zeqi Lin, Weizhu Chen, Jian-Guang Lou. TAPEX pre-trains a BART model to solve synthetic SQL queries, after which it can be fine-tuned to answer natural language questions related to tabular data, as well as performing table fact checking. Add TAPEX by @NielsRogge in #16473 Data2Vec: vision The Data2Vec model was proposed in data2vec: A General Framework for Self-supervised Learning in Speech, Vision and Language by Alexei Baevski, Wei-Ning Hsu, Qiantong Xu, Arun Babu, Jiatao Gu and Michael Auli. Data2Vec proposes a unified framework for self-supervised learning across different data modalities - text, audio and images. Importantly, predicted targets for pre-training are contextualized latent representations of the inputs, rather than modality-specific, context-independent targets. The vision model is added in v4.19.0. [Data2Vec] Add data2vec vision by @patrickvonplaten in #16760 Add Data2Vec for Vision in TF by @sayakpaul in #17008 FSDP integration in Trainer PyTorch recently upstreamed the Fairscale FSDP into PyTorch Distributed with additional optimizations. This PR is aimed at integrating it into Trainer API. It enables Distributed Training at Scale. It's a wrapper for sharding Module parameters across data parallel workers. This is inspired by Xu et al. as well as the ZeRO Stage 3 from DeepSpeed. PyTorch FSDP will focus more on production readiness and long-term support. This includes better integration with ecosystems and improvements on performance, usability, reliability, debuggability and composability. PyTorch FSDP integration in Trainer by @pacman100 in #17136 Training scripts New example scripts were added for image classification and semantic segmentation. Both now have versions that leverage the Trainer API and Accelerate. Add image classification script, no trainer by @NielsRogge in #16727 Add semantic script no trainer, v2 by @NielsRogge in #16788 Add semantic script, trainer by @NielsRogge in #16834 Documentation in Spanish To continue democratizing good machine learning, we're making the Transformers documentation more accessible to non-English speakers; starting with Spanish (572M speakers worldwide). Added es version of language_modeling.mdx doc by @jQuinRivero in #17021 Spanish translation of the file philosophy.mdx by @jkmg in #16922 Documentation: Spanish translation of fast_tokenizers.mdx by @jloayza10 in #16882 Translate index.mdx (to ES) and add Spanish models to quicktour.mdx examples by @omarespejel in #16685 Spanish translation of the file multilingual.mdx by @SimplyJuanjo in #16329 Added spanish translation of autoclass_tutorial. by @duedme in #17069 Fix style error in Spanish docs by @osanseviero in #17197 Improvements and bugfixes [modeling_utils] rearrange text by @stas00 in #16632 Added Annotations for PyTorch models by @anmolsjoshi in #16619 Allow the same config in the auto mapping by @sgugger in #16631 Update no_trainer scripts with new Accelerate functionalities by @muellerzr in #16617 Fix doc example by @NielsRogge in #16448 Add inputs vector to calculate metric method by @lmvasque in #16461 [megatron-bert-uncased-345m] fix conversion by @stas00 in #16639 Remove parent/child tests in auto model tests by @sgugger in #16653 Updated _load_pretrained_model_low_mem to check if keys are in the state_dict by @FrancescoSaverioZuppichini in #16643 Update Support image on README.md by @BritneyMuller in #16615 bert: properly mention deprecation of TF2 conversion script by @stefan-it in #16171 add vit tf doctest with @add_code_sample_docstrings by @johko in #16636 Fix error in doc of DataCollatorWithPadding by @secsilm in #16662 Fix QA sample by @ydshieh in #16648 TF generate refactor - Beam Search by @gante in #16374 Add tests for no_trainer and fix existing examples by @muellerzr in #16656 only load state dict when the checkpoint is not None by @laurahanu in #16673 [Trainer] tf32 arg doc by @stas00 in #16674 Update audio examples with MInDS-14 by @stevhliu in #16633 add a warning in SpmConverter for sentencepiece's model using the byte fallback feature by @SaulLu in #16629 Fix some doc examples in task summary by @ydshieh in #16666 Jia multi gpu eval by @liyongsea in #16428 Generate: min length can't be larger than max length by @gante in #16668 fixed crash when deleting older checkpoint and a file f\"{checkpoint_prefix}-*\" exist by @sadransh in #16686 [Doctests] Correct task summary by @patrickvonplaten in #16644 Add Doc Test for BERT by @vumichien in #16523 Fix t5 shard on TPU Pods by @agemagician in #16527 update decoder_vocab_size when resizing embeds by @patil-suraj in #16700 Fix TF_MASKED_LM_SAMPLE by @ydshieh in #16698 Rename the method test_torchscript by @ydshieh in #16693 Reduce memory leak in _create_and_check_torchscript by @ydshieh in #16691 Enable more test_torchscript by @ydshieh in #16679 Don't push checkpoints to hub in no_trainer scripts by @muellerzr in #16703 Private repo TrainingArgument by @nbroad1881 in #16707 Handle image_embeds in ViltModel by @ydshieh in #16696 Improve PT/TF equivalence test by @ydshieh in #16557 Fix example logs repeating themselves by @muellerzr in #16669 [Bart] correct doc test by @patrickvonplaten in #16722 Add Doc Test GPT-2 by @ArEnSc in #16439 Only call get_output_embeddings when tie_word_embeddings is set by @smelm in #16667 Update run_translation_no_trainer.py by @raki-1203 in #16652 Qdqbert example add benchmark script with ORT-TRT by @shangz-ai in #16592 Replace assertion with exception by @anmolsjoshi in #16720 Change the chunk_iter function to handle by @Narsil in #16730 Remove duplicate header by @sgugger in #16732 Moved functions to pytorch_utils.py by @anmolsjoshi in #16625 TF: remove set_tensor_by_indices_to_value by @gante in #16729 Add Doc Tests for Reformer PyTorch by @hiromu166 in #16565 [FlaxSpeechEncoderDecoder] Fix input shape bug in weights init by @sanchit-gandhi in #16728 [FlaxWav2Vec2Model] Fix bug in attention mask by @sanchit-gandhi in #16725 add Bigbird ONNX config by @vumichien in #16427 TF generate: handle case without cache in beam search by @gante in #16704 Fix decoding score comparison when using logits processors or warpers by @bryant1410 in #10638 [Doctests] Fix all T5 doc tests by @patrickvonplaten in #16646 Fix #16660 (tokenizers setters of ids of special tokens) by @davidleonfdez in #16661 [from_pretrained] refactor find_mismatched_keys by @stas00 in #16706 Add Doc Test for GPT-J by @ArEnSc in #16507 Fix and improve CTRL doctests by @jeremyadamsfisher in #16573 [modeling_utils] better explanation of ignore keys by @stas00 in #16741 CI: setup-dependent pip cache by @gante in #16751 Reduce Funnel PT/TF diff by @ydshieh in #16744 Add defensive check for config num_labels and id2label by @sgugger in #16709 Add self training code for text classification by @tuvuumass in #16738 [self-scheduled ci] explain where dependencies are by @stas00 in #16757 Fixup no_trainer examples scripts and add more tests by @muellerzr in #16765 [Doctest] added doctest changes for electra by @bhadreshpsavani in #16675 Enabling Tapex in table question answering pipeline. by @Narsil in #16663 [Flax .from_pretrained ] Raise a warning if model weights are not in float32 by @sanchit-gandhi in #16762 Fix batch size in evaluation loop by @sgugger in #16763 Make nightly install dev accelerate by @muellerzr in #16783 [deepspeed / m2m_100] make deepspeed zero-3 work with layerdrop by @stas00 in #16717 Kill async pushes when calling push_to_hub with blocking=True by @sgugger in #16755 Improve image classification example by @NielsRogge in #16585 [SpeechEncoderDecoderModel] Fix bug in reshaping labels by @sanchit-gandhi in #16748 Fix issue avoid-missing-comma found at https://codereview.doctor by @code-review-doctor in #16768 [trainer / deepspeed] fix hyperparameter_search by @stas00 in #16740 [modeling utils] revamp from_pretrained(..., low_cpu_mem_usage=True) + tests by @stas00 in #16657 Fix PT TF ViTMAE by @ydshieh in #16766 Update README.md by @NielsRogge in #16797 Pin Jax to last working release by @sgugger in #16808 CI: non-remote GH Actions now use a python venv by @gante in #16789 TF generate refactor - XLA sample by @gante in #16713 Raise error and suggestion when using custom optimizer with Fairscale or Deepspeed by @allanj in #16786 Create empty venv on cache miss by @gante in #16816 [ViT, BEiT, DeiT, DPT] Improve code by @NielsRogge in #16799 [Quicktour Audio] Improve &amp;&amp; remove ffmpeg dependency by @patrickvonplaten in #16723 fix megatron bert convert state dict naming by @Codle in #15820 use base_version to check torch version in torch_less_than_1_11 by @nbroad1881 in #16806 Allow passing encoder_ouputs as tuple to EncoderDecoder Models by @jsnfly in #16814 Refactor issues with yaml by @LysandreJik in #16772 fix _setup_devices in case where there is no torch.distributed package in build by @dlwh in #16821 Clean up semantic segmentation tests by @NielsRogge in #16801 Fix LayoutLMv2 tokenization docstrings by @qqaatw in #16187 Wav2 vec2 phoneme ctc tokenizer optimisation by @ArthurZucker in #16817 [Flax] improve large model init and loading by @patil-suraj in #16148 Some tests misusing assertTrue for comparisons fix by @code-review-doctor in #16771 Type hints added for TFMobileBert by @Dahlbomii in #16505 fix rum_clm.py seeking text column name twice by @dandelin in #16624 Add onnx export of models with a multiple choice classification head by @echarlaix in #16758 [ASR Pipeline] Correct init docs by @patrickvonplaten in #16833 Add doc about attention_mask on gpt2 by @wiio12 in #16829 TF: Add sigmoid activation function by @gante in #16819 Correct Logging of Eval metric to Tensorboard by @Jeevesh8 in #16825 replace Speech2TextTokenizer by Speech2TextFeatureExtractor in some docstrings by @SaulLu in #16835 Type hints added to Speech to Text by @Dahlbomii in #16506 Improve test_pt_tf_model_equivalence on PT side by @ydshieh in #16731 Add support for bitsandbytes by @manuelciosici in #15622 [Typo] Fix typo in modeling utils by @patrickvonplaten in #16840 add DebertaV2 fast tokenizer by @mingboiz in #15529 Fixing return type tensor with num_return_sequences&gt;1 . by @Narsil in #16828 [modeling_utils] use less cpu memory with sharded checkpoint loading by @stas00 in #16844 [docs] fix url by @stas00 in #16860 Fix custom init sorting script by @sgugger in #16864 Fix multiproc metrics in no_trainer examples by @muellerzr in #16865 Long QuestionAnsweringPipeline fix. by @Narsil in #16778 t5: add conversion script for T5X to FLAX by @stefan-it in #16853 tiny tweak to allow BatchEncoding.token_to_char when token doesn't correspond to chars by @ghlai9665 in #15901 Adding support for array key in raw dictionnaries in ASR pipeline. by @Narsil in #16827 Return input_ids in ImageGPT feature extractor by @sgugger in #16872 Use ACT2FN to fetch ReLU activation by @eldarkurtic in #16874 Fix GPT-J onnx conversion by @chainyo in #16780 Fix doctest list by @ydshieh in #16878 New features for CodeParrot training script by @loubnabnl in #16851 Add missing entries in mappings by @ydshieh in #16857 TF: rework XLA generate tests by @gante in #16866 Minor fixes/improvements in convert_file_size_to_int by @mariosasko in #16891 Add doc tests for Albert and Bigbird by @vumichien in #16774 Add OnnxConfig for ConvBERT by @chainyo in #16859 TF: XLA repetition penalty by @gante in #16879 Changes in create_optimizer to support tensor parallelism with SMP by @cavdard in #16880 [DocTests] Fix some doc tests by @patrickvonplaten in #16889 add bigbird typo fixes by @chainyo in #16897 Fix doc test quicktour dataset by @patrickvonplaten in #16929 Add missing ckpt in config docs by @ydshieh in #16900 Fix PyTorch RAG tests GPU OOM by @ydshieh in #16881 Fix RemBertTokenizerFast by @ydshieh in #16933 TF: XLA logits processors - minimum length, forced eos, and forced bos by @gante in #16912 TF: XLA Logits Warpers by @gante in #16899 added deit onnx config by @rushic24 in #16887 TF: XLA stable softmax by @gante in #16892 Replace deprecated logger.warn with warning by @sanchit-gandhi in #16876 Fix issue probably-meant-fstring found at https://codereview.doctor by @code-review-doctor in #16913 Limit the use of PreTrainedModel.device by @sgugger in #16935 apply torch int div to layoutlmv2 by @ManuelFay in #15457 FIx Iterations for decoder by @agemagician in #16934 Add onnx config for RoFormer by @skrsna in #16861 documentation: some minor clean up by @mingboiz in #16850 Fix RuntimeError message format by @ftnext in #16906 use original loaded keys to find mismatched keys by @tricktreat in #16920 [Research] Speed up evaluation for XTREME-S by @anton-l in #16785 Fix HubertRobustTest PT/TF equivalence test on GPU by @ydshieh in #16943 Misc. fixes for Pytorch QA examples: by @searchivarius in #16958 [HF Argparser] Fix parsing of optional boolean arguments by @NielsRogge in #16946 Fix distributed_concat with scalar tensor by @Yard1 in #16963 Update custom_models.mdx by @mishig25 in #16964 Fix add-new-model-like when model doesn't support all frameworks by @sgugger in #16966 Fix multiple deletions of the same files in save_pretrained by @sgugger in #16947 Fixup no_trainer save logic by @muellerzr in #16968 Fix doc notebooks links by @sgugger in #16969 Fix check_all_models_are_tested by @ydshieh in #16970 Add -e flag to some GH workflow yml files by @ydshieh in #16959 Update tokenization_bertweet.py by @datquocnguyen in #16941 Update check_models_are_tested to deal with Windows path by @ydshieh in #16973 Add parameter --config_overrides for run_mlm_wwm.py by @conan1024hao in #16961 Rename a class to reflect framework pattern AutoModelXxx -&gt; TFAutoModelXxx by @amyeroberts in #16993 set eos_token_id to None to generate until max length by @ydshieh in #16989 Fix savedir for by epoch by @muellerzr in #16996 Update README to latest release by @sgugger in #16997 use scale=1.0 in floats_tensor called in speech model testers by @ydshieh in #17007 Update all require decorators to use skipUnless when possible by @muellerzr in #16999 TF: XLA bad words logits processor and list of processors by @gante in #16974 Make create_extended_attention_mask_for_decoder static method by @pbelevich in #16893 Update README_zh-hans.md by @tarzanwill in #16977 Updating variable names. by @Narsil in #16445 Revert \"Updating variable names. by @Narsil in #16445 )\" Replace dict/BatchEncoding instance checks by Mapping by @sgugger in #17014 Result of new doc style with fixes by @sgugger in #17015 Add a check on config classes docstring checkpoints by @ydshieh in #17012 Add translating guide by @omarespejel in #17004 update docs of length_penalty by @manandey in #17022 [FlaxGenerate] Fix bug in decoder_start_token_id by @sanchit-gandhi in #17035 Fx with meta by @michaelbenayoun in #16836 [Flax(Speech)EncoderDecoder] Fix bug in decoder_module by @sanchit-gandhi in #17036 Fix typo in RetriBERT docstring by @mpoemsl in #17018 add torch.no_grad when in eval mode by @JunnYu in #17020 Disable Flax GPU tests on push by @sgugger in #17042 Clean up vision tests by @NielsRogge in #17024 [Trainer] Move logic for checkpoint loading into separate methods for easy overriding by @calpt in #17043 Update no_trainer examples to use new logger by @muellerzr in #17044 Fix no_trainer examples to properly calculate the number of samples by @muellerzr in #17046 Allow all imports from transformers by @LysandreJik in #17050 Make the sacremoses dependency optional by @LysandreJik in #17049 Clean up setup.py by @sgugger in #17045 [T5 Tokenizer] Model has no fixed position ids - there is no hardcode… by @patrickvonplaten in #16990 [FlaxBert] Add ForCausalLM by @sanchit-gandhi in #16995 Move test model folders by @ydshieh in #17034 Make Trainer compatible with sharded checkpoints by @sgugger in #17053 Remove Python and use v2 action by @sgugger in #17059 Fix RNG reload in resume training from epoch checkpoint by @sgugger in #17055 Remove device parameter from create_extended_attention_mask_for_decoder by @pbelevich in #16894 Fix hashing for deduplication by @thomasw21 in #17048 Skip RoFormer ONNX test if rjieba not installed by @lewtun in #16981 Remove masked image modeling from BEIT ONNX export by @lewtun in #16980 Make sure telemetry arguments are not returned as unused kwargs by @sgugger in #17063 Type hint complete Albert model file. by @karthikrangasai in #16682 Deprecate model templates by @sgugger in #17062 Update to build via git for accelerate by @muellerzr in #17084 Allow saved_model export of TFCLIPModel in save_pretrained by @seanmor5 in #16886 Fix DeBERTa token_type_ids by @deutschmn in #17082 📝 open fresh PR for pipeline doctests by @stevhliu in #17073 minor change on TF Data2Vec test by @ydshieh in #17085 type hints for pytorch models by @robotjellyzone in #17064 Add type hints for BERTGeneration by @robsmith155 in #17047 Fix MLflowCallback and add support for MLFLOW_EXPERIMENT_NAME by @orieg in #17091 Remove torchhub test by @sgugger in #17097 fix missing \"models\" in pipeline test module by @ydshieh in #17090 Fix link to example scripts by @stevhliu in #17103 Fix self-push CI report path in cat by @ydshieh in #17111 Added BigBirdPegasus onnx config by @nandwalritik in #17104 split single_gpu and multi_gpu by @ydshieh in #17083 LayoutLMv2Processor: ensure 1-to-1 mapping between images and samples in case of overflowing tokens by @ghlai9665 in #17092 Add type hints for BigBirdPegasus and Data2VecText PyTorch models by @robsmith155 in #17123 add mobilebert onnx configs by @manandey in #17029 [WIP] Fix Pyright static type checking by replacing if-else imports with try-except by @d-miketa in #16578 Add the auto_find_batch_size capability from Accelerate into Trainer by @muellerzr in #17068 Fix MLflowCallback end_run() and add support for tags and nested runs by @orieg in #17130 Fix all docs for accelerate install directions by @muellerzr in #17145 LogSumExp trick question_answering pipeline. by @Narsil in #17143 train args defaulting None marked as Optional by @d-miketa in #17156 [trainer] sharded _load_best_model by @stas00 in #17150 [Deepspeed] add many more models to the model zoo test by @stas00 in #12695 Fixing the output of code examples in the preprocessing chapter by @HallerPatrick in #17162 missing file by @stas00 in #17164 Add MLFLOW_FLATTEN_PARAMS support in MLflowCallback by @orieg in #17148 Fix template init by @sgugger in #17163 MobileBERT tokenizer tests by @leondz in #16896 [M2M100 doc] remove duplicate example by @patil-suraj in #17175 Extend Transformers Trainer Class to Enable PyTorch SGD/Adagrad Optimizers for Training by @jianan-gu in #17154 propagate \"attention_mask\" dtype for \"use_past\" in OnnxConfig.generate_dummy_inputs by @arampacha in #17105 Convert image to rgb for clip model by @hengkuanwee in #17101 Add missing RetriBERT tokenizer tests by @mpoemsl in #17017 [WIP] Enable reproducibility for distributed trainings by @hasansalimkanmaz in #16907 Remove unnecessary columns for all dataset types in Trainer by @Yard1 in #17166 Fix LED documentation by @manuelciosici in #17181 Ensure tensors are at least 1d for pad and concat by @Yard1 in #17179 add shift_tokens_right in FlaxMT5 by @patil-suraj in #17188 Remove columns before passing to data collator by @Yard1 in #17187 Remove duplicated os.path.join by @shijie-wu in #17192 Fix contents in index.mdx to match docs' sidebar by @omarespejel in #17198 ViT and Swin symbolic tracing with torch.fx by @michaelbenayoun in #17182 migrate azure blob for beit checkpoints by @donglixp in #16902 Update data2vec.mdx to include a Colab Notebook link (that shows fine-tuning) by @sayakpaul in #17194 Significant community contributions The following contributors have made significant changes to the library over the last release: @anmolsjoshi Added Annotations for PyTorch models ( #16619 ) Replace assertion with exception ( #16720 ) Moved functions to pytorch_utils.py ( #16625 ) @vumichien Add Doc Test for BERT ( #16523 ) add Bigbird ONNX config ( #16427 ) Add doc tests for Albert and Bigbird ( #16774 ) @tuvuumass Add self training code for text classification ( #16738 ) @sayakpaul Add Data2Vec for Vision in TF ( #17008 ) @robotjellyzone type hints for pytorch models ( #17064 ) @d-miketa [WIP] Fix Pyright static type checking by replacing if-else imports with try-except ( #16578 ) train args defaulting None marked as Optional ( #17156 ) Contributors dlwh, manuelciosici, and 102 other contributors Assets 2 Loading Uh oh! There was an error while loading. Please reload this page . --> 👍 22 liu-yihong, Codle, nikolaydubina, abdouaziz, themrinalsinha, ma-xu, lmlima, oborchers, aaossa, davidycliao, and 12 more reacted with thumbs up emoji 😄 1 ChrisTaylor17 reacted with laugh emoji 🎉 16 abdouaziz, stephenroller, shpotes, felixdittrich92, oborchers, aaossa, ngoquanghuy99, nateraw, samuelrince, mozharovsky, and 6 more reacted with hooray emoji 🚀 2 johnnv1 and GuillaumeTech reacted with rocket emoji All reactions 👍 22 reactions 😄 1 reaction 🎉 16 reactions 🚀 2 reactions 33 people reacted Footer &copy; 2025 GitHub,&nbsp;Inc. Footer navigation Terms Privacy Security Status Docs Contact Manage cookies Do not share my personal information You can’t perform that action at this time. "
    },
    {
      "arxiv_id": "https://raw.githubusercontent.com/facebookresearch/metaseq/main/projects/OPT-IML/optiml_paper_v1.pdf",
      "full_text": "[February 1, 2023] Please check out our paper on ArXiv:\nhttps://arxiv.org/abs/2212.12017\n1\n"
    },
    {
      "arxiv_id": "https://raw.githubusercontent.com/facebookresearch/metaseq/main/projects/OPT/chronicles/OPT175B_Logbook.pdf",
      "full_text": "OPT-175 Logbook\nGoal: Get a 175B dense model up and running by any means necessary.\nPurpose of this document:\nTo provide a source of truth of what we did, when, and why, and any context that was important to those\ndecisions. To provide each other with a clear place to find information about what is happening without having\nto ping.\nInstructions\n-\nAdd a dated entry for each log, in reverse chronological order.\n-\nEntries do not have to correspond to launches, but may include notes.\n-\nHIGHLIGHT IN RED ANYTHING THE NEXT ONCALL SHOULD ABSOLUTELY NOTICE\n-\nFor all launches, include:\n-\nDate\n-\nRemember to update the pointers at the top of this document\n-\nContext of why changes were necessary (Analysis of previous run)\n-\nInclude tensorboard screenshots of spikes or divergences if applicable\n-\nLaunch steps:\n-\nCheckpoint/log folder\n-\nRelevant commits\n-\nPR of a change to sweep script if relevant\n-\nOncall responsibilities are at the bottom of this doc\nSpare Node Tracker\nNOTE: TRY TO KEEP KNOWN GREEN NODES IN IDLE AND DRAIN ALL BAD NODES.\nNodeList\n# nodes\nState\nNotes\n\n175B Log\nNOTE: You don’t need to specify the log file when launching the monitor script, it’ll find it automatically\nbased on the job id.\n-------\n2022-01-06 15:47 ET [Everyone]\n2022-01-06\n15:46:44\n| INFO | fairseq_cli.train\n|\ndone training in\n89691.5 seconds\n2022-01-05 14:10 [Stephen + Mikel]\nTimeline\n1.\nStephen’s devfair was hosting an ssh session running Tensorboard.\na.\nThe devfair was taken offline for yearly maintenance, causing ssh\n2.\nAn unknown person was running monitor.py, and that was also taken offline for an unknown reason.\n3.\nCurrent oncall took action to relaunch monitor.py, which immediately detected a file-not-modified issue\nand attempted to restart the job.\na.\nDue to user error, the wrong train.log file was specified, causing a false positive on the file not\nmodified detector.\nb.\nJob was put on hold\n\nc.\nfixmycloud was run\nd.\nWe reached the part about NCCL tests and silently failed on fixmycloud, and crashing\nmonitor.py. It did attempt to send an email (into the void\nMitigations taken:\n1.\nNoticed 10 nodes were in drain state, mostly from a “kill task failed”\na.\nWe don’t always know whether this indicates a bad node or not.\n2.\nManually undrained and launched fixmycloud.\na.\nThis initially hung because pg0-8 was unresponsive but scheduled a nccl test anyway.\nb.\nManually drained pg0-8 and relaunched fixmycloud\nc.\nTwo nodes were drained for failing tests.\n3.\nManually ran touch train.log on the latest run to ensure time modified would be new\n4.\nManually removed the nodelist via sudo scontrol update job=6848 'NodeList='\n5.\nManually ran sudo scontrol release 6848 to resume the job\n6.\nCurrent oncall ran monitor.py with updated train.log argument and verified stability\nFuture actions required:\n1.\nWe need to add the nccl binaries directly to the repo, and remove the get_nccl_scripts.sh file/check.\n2.\nmonitor.py should log who is running it.\n3.\nmonitor.py should use scontrol show jobid to identify the logfile automatically, rather than require\nmanual specification\n4.\nEmails need to be fixed or another alerting system needs to be found.\n5.\nmonitor.py should probably run via some sort of nohup or as a daemon\n6.\ntensorboard serve should probably run via some sort of nohup\n2022-01-06 10:30ish [Susan + Mikel]\nTimestamps in the logs were in ET time because the last oncall had the TZ environment variable set when\nlaunching the jobs. This made it look like the job was stuck for 5 hours at first glance.\nMitigations taken: patch to ensure logs are always in UTC.\n2022-01-04 15:22 ET [Sam]\nAuto-recovery failed with no email. Restarting from 2021-12-30 09:30 ET [Stephen] - Job recovery failed.\n2022-01-03 05:10 ET [Daniel]\nUpdate 12:47 ET [Stephen]: Trend is continuing. Agree with holding steady.\nTrain and validation ppl still show opposite trends (in the unusual way). Very confused, but based on a previous\ncomment from Stephen in the chat I’ll leave the training untouched and wait for someone to help interpret this\nin the morning.\n\nActivation norm seems to repeat an upward step pattern:\n2022-01-02 17:26 ET [Stephen]\nNoting that we have seen a spike in activation norms and train PPL is trending up\nValidation not really affected:\n\n2021-12-31 02:53 ET [Punit]\nStarted the training monitor script\n./scripts/cloud/monitor_train_log.py --mailto <scrubbed>@fb.com /shared/home/namangoyal/checkpoints/175B/175B_run12.57*/train.log\n--modified-threshold 3600 --slurm-jobid 6214\n2021-12-31 12:00 ET [Moya]\n●\nMonitor script detects train.log not getting updated; tries to autorecover (6 am ET ish)\n●\nAutorecover is successful.\n●\n…however I had two monitor scripts running going to two separate emails (<scrubbed>)\n○\nWhich meant recovery took twice as long. (Up at 9 am ish rather than sooner)\n○\n…and the emails didn’t even send anyway :(\n●\nnode-[5,13] seem to be new drained nodes; undrain them and run ./fixmycloud on them to see what’s\nup… Seems like ssh issue\n○\npdsh@ip-0A1E0404: node-5: ssh exited with exit code 15\n○\npdsh@ip-0A1E0404: node-13: ssh exited with exit code 15\n●\nSSH into the two nodes run `nvidia-smi`\n○\nGet `Unable to determine the device handle for GPU 000B:00:00.0: GPU is lost.  Reboot the\nsystem to recover this GPU` for both\n●\nPut node-[5,13] back into drain mode\n●\n…and while I happened to be updating the log for this, 150 enters into a “connection timed out mode”,\nsame as drained* node.\nhpc*         up   infinite      1 drain* node-82\nhpc*         up   infinite      1  idle* node-150\nhpc*         up   infinite      3  drain node-[5,13,148]\nhpc*         up   infinite      1    mix node-11\nhpc*         up   infinite    124  alloc node-[1-4,6-10,12,14-32,34-81,83-92,94,96-98,100-106,108-116,118-134]\nhpc*         up   infinite     11   idle node-[137-146,149]\n2021-12-30 17:00 ET [Moya] - nvidia_smi.py bug fix; machine check\n●\nSome spares were caught under `fixmycloud` when they shouldn’t have been.\n\n●\nUndrained the relevant nodes (node-[5,18,47,118,120-121,149]) then reran fixmycloud after fixing the\nrelevant bug in nvidia_smi\n●\nNoticed infoROM corruption in one of the nodes and we’ve got enough spares to not need yellows, so\nmarking as drain\n○\n2021-12-30 22:58:27 WARNING  nvidia_smi | node-148: infoROM is corrupted at gpu\n0001:00:00.0\n○\n2021-12-30 22:58:27 WARNING  nvidia_smi | node-148: infoROM is corrupted at gpu\n000E:00:00.0\nCluster stats after the above:\nhpc*         up   infinite      1 drain* node-82           – [2021-12-30 8PM EST] CSP sync - Has wrong IP address.\nCSP looking into, UI issue\nhpc*         up   infinite      1  drain node-148\nhpc*         up   infinite      1    mix node-11\nhpc*         up   infinite    124  alloc\nnode-[1-4,6-10,12-17,19-32,34-46,48-81,83-92,94,96-98,100-106,108-116,119,122-134,137-140]\nhpc*         up   infinite     14   idle node-[5,18,47,118,120-121,141-146,149-150]\n2021-12-30 09:30 ET [Stephen] - Job recovery failed\n●\nWe crashed with some sort of hardware failure\n●\nJob actually got dequeued from slurm before the auto-recovery could hold it\n●\nAs a result, the monitor script crashed and auto-recovery didn’t execute\n●\nNo emails got sent.\n●\nManually resuming. Choosing to bump run ID bc our tensorboards are getting crowded.\nAlso note that I noticed the monitor script fails to auto-lock due to the original run directory being\n0775. I’ve manually changed run directories to be 0777 for now.\nCluster status after fixmycloud:\nhpc*         up   infinite      3 drain* node-[33,47,95]\nhpc*         up   infinite      5  drain node-[82,118,120-121,149]\nhpc*         up   infinite      1    mix node-11\nhpc*         up   infinite    132   idle\nnode-[1-4,6-10,12-17,19-32,34-46,48-81,83-92,94,96-98,100-106,108-116,119,122-134,137-146,148,150]\nSeveral of those drains might be false positives (ECC errors now are over aggressive, catching aggregate\nones rather than recent ones)\n# LAUNCH OF 12.57\n# use updated fairscale\ncd ~/fairscale\ngit fetch origin prefetch_fsdp_params_simple\ngit checkout prefetch_fsdp_params_simple\ncd ~/Megatron-LM\ngit checkout tags/v2.6\ncd ~/fairseq-py\ngit fetch origin gshard_combine_megatron_fsdp\ngit checkout gshard_combine_megatron_fsdp\n\n# use previous blob URL\nBLOB_PREFIX=\"<<<SCRUBBED FOR RELEASE>>>\"\nBLOB_AUTH=\"<<<SCRUBBED FOR RELEASE>>>\"\nRUN_ID=175B_run12.57\n# Use hosts verified good from fixmycloud from above, but don’t manually specify\npython -m fb_sweep.opt.sweep_opt_en_lm_175b \\\n-n 124 -g 8 -t 1 \\\n-p $RUN_ID \\\n--checkpoints-dir /shared/home/namangoyal/checkpoints/175B/ \\\n--local-checkpoints-dir /mnt/scratch/$USER/checkpoints/$(date +%Y-%m-%d).$RUN_ID \\\n--full-cloud-upload-path \"${BLOB_PREFIX}/?${BLOB_AUTH}\"\n./scripts/cloud/monitor_train_log.py --mailto <scrubbed> /shared/home/namangoyal/checkpoints/175B/175B_run12.57*/train.log\n--modified-threshold 3600 --slurm-jobid 6214\n2021-12-29 13:40 ET [Stephen] - Cluster maintenance and relaunch\n●\nObserved WPS drop\n●\nManually paused to run fixmycloud. Found node-95 had terrible NCCL\n●\nDrained and reported\n●\nJob requeued\nLater:\n●\nNoticed a typo in nvidia_smi.py\n●\nFound node-149 had high uncorrectable ECCs after fixing typo\nAlso checked active nodes:\n2021-12-29 18:40:12 CRITICAL nvidia_smi | node-118: ecc high uncorrectables: DRAM Uncorrectable: 1335\ns\n2021-12-29 18:40:12 CRITICAL nvidia_smi | node-121: ecc high uncorrectables: DRAM Uncorrectable: 8\nTook no action. The job is running fine…\n2021-12-28 16:00 PT [Susan] - cluster maintenance\n●\nJob restarted and auto-recovered\n●\n5 nodes in drain:\n(base) susanz@ip-0A1E0404:/shared/home/namangoyal/checkpoints/175B/tensorboard$ sinfo\nPARTITION AVAIL  TIMELIMIT  NODES  STATE NODELIST\nhpc*         up   infinite      1 drain* node-33\nhpc*         up   infinite      5  drain node-[11,47,111,132-133]\nhpc*         up   infinite    124  alloc\nnode-[1-4,6-10,12-17,19-32,34-46,48-92,94-98,100-106,108-110,112-116,118-131,134,137-138]\nhpc*         up   infinite     11   idle node-[139-146,148-150]\n(base) susanz@ip-0A1E0465:~/fairseq-py$ python scripts/cloud/slurm.py summary\nnode-11: Kill task failed [root@2021-12-29T03:08:14]\nnode-47: Kill task failed [root@2021-12-29T03:08:13]\nnode-111: Kill task failed [root@2021-12-29T03:08:13]\nnode-132: Kill task failed [root@2021-12-29T03:08:14]\n\nnode-133: Kill task failed [root@2021-12-29T03:08:13]\n●\nSsh-ing to each:\n○\n11 infoROM corrupted\n○\n47 lost GPU\n○\n111 infoROM corrupted\n○\n132-133 not sure what’s wrong here, undraining and running fixmycloud\n(base) susanz@ip-0A1E0465:~/fairseq-py$ python scripts/cloud/slurm.py undrain node-132\n2021-12-29 04:10:09 WARNING  slurm      | Undraining node-132 because \"No reason given\"\n(base) susanz@ip-0A1E0465:~/fairseq-py$ python scripts/cloud/slurm.py undrain node-133\n2021-12-29 04:10:12 WARNING  slurm      | Undraining node-133 because \"No reason given\"\n●\nFixmycloud shows 132 and 133 as fine. Leaving as idle.\n●\nReported 47 to CSP.\n2021-12-28 9:10 ET [Myle/Stephen] - manually recovered job\n●\nAutorecovery failed due to a permission error on the lock file :/\n○\nFixed in PR #2845\n●\nNOT relaunched monitoring script, will let someone else do it\nStephen:\n●\nI’m the captain now\n●\nRunning fixmycloud & relaunching the monitor script\n●\nReported 136, 117, 19, and 18 to CSP\n●\nLater reported 5 too\nThey gave us back ​node-[19,55,56,91,92].\n2021-12-27 9:25 ET [Myle] - postmortem on autorecovery issue\n●\nIn the past, validation takes ~14 minutes for all subsets\n○\nValidation prints several lines to train.log, so it shouldn’t have triggered the 15 minute timeout\n●\nBased on train.log and monitor.log, it seems like the job genuinely hung in validation, but hanging 3\ntimes in a row during validation seems very suspicious.\nHypotheses:\n1.\nSomething is buffering writes to train.log for the whole validation step, which tripped the 15 minute\nmodified threshold.\na.\nMoving the threshold to 1 hour (--modified-threshold=3600) seems like a good solution in this\ncase.\n2.\nAnecdotally we seem to crash during validation a lot. It'd be good to quantify, but perhaps something in\nour code or dataloader is causing hangs during validation?\na.\nWe should quantify frequency of TERM during validation vs. training\n3.\nIt's possible we did have three bad nodes in a short time period, but what are the odds that they all\nfailed in validation? Even if we did have bad nodes, it's possible there's something in validation that\nmakes validation more likely to break nodes.\n\nSeparately it'd be great to add timestamps to train.stderr somehow, to make it easier to cross-reference\ntrain.log and train.stderr\n2021-12-27 13:19 CET [Mikel] - restarting autorecovery script\n●\nThe autorecovery script has kicked in 3 times in the last few hours, all 3 times during validation\n●\nI suspect it could be because train.log is less verbose during validation and the previous\n--modified-threshold could be too agressive\n●\nKilled the previous autorecovery job and relaunched it with --modified-threshold 3600:\n./scripts/cloud/monitor_train_log.py --mailto <scrubbed>@fb.com\n/shared/home/namangoyal/checkpoints/175B/175B_run12.56*/train.log --modified-threshold 3600\n--slurm-jobid 4136\n2021-12-25  04:18 ET [Myle] - starting improved autorecovery script\n●\nAutorecovery has been made more robust in #2842\n●\nRelaunched with (note that --modified-threshold was originally 900 but has been adjusted in the\ncommand below to 3600): ./scripts/cloud/monitor_train_log.py --mailto <scrubbed>@fb.com\n/shared/home/namangoyal/checkpoints/175B/175B_run12.56*/train.log --modified-threshold 3600\n--slurm-jobid 4136\n●\nOncall can tail with: tail -f /data/users/common/monitor.log\n2021-12-25 09:48 ET [Myle] - RCA on autorecovery failure\nTimeline (all times UTC):\n●\n2021-12-25 09:56: job hangs\n○\nRoot cause is node-19\n■\nStderr message: mlx5: ip-0A1E0444: got completion with error\n■\nscripts/cloud/find_host.py maps ip-0A1E0444 to node-19\n●\n2021-12-25 10:15: monitoring script detects hung job and starts autorecovery\n●\n2021-12-25 10:17: fixmycloud requeues job with new nodelist\n●\n2021-12-25 10:18: requeued job begins\n●\n2021-12-25 10:22: monitoring script thinks auto-recovery was successful, sends email\n●\n2021-12-25 10:23: done with model init\n●\n2021-12-25 10:27: done with blob download, begin fast forwarding dataloader\n●\n2021-12-25 10:42: monitoring script attempts auto-recovery once again\n○\nNote: there was no log message about why it’s launching autorecovery, but it’s because the\ntrain.log had not been modified in the previous 15 min\n●\n2021-12-25 10:45: job is resumed and the cycle repeats\n2021-12-25 08:49 ET [Stephen]\n$ python scripts/cloud/slurm.py summary\nnode-11: infoROM_corrupted [susanz@2021-12-25T11:52:58]\nnode-19: Kill task failed [root@2021-12-25T10:16:13]\nnode-111: infoROM_corrupted [susanz@2021-12-25T11:53:04]\nnode-148: infoROM_corrupted [susanz@2021-12-25T11:53:09]\n\n$ ssh node-19\n$ nvidia-smi\nUnable to determine the device handle for GPU 0001:00:00.0: GPU is lost.  Reboot the system to recover this\nGPU\n$ sudo reboot\nContext: CSP had asked us if we had tried rebooting to fix this error recently. Giving it a shot.\nThis node seems to NOT BE COMING BACK.\nPlease HOLD ON TO IT FOR CSP.\nUpdated node list.\n2021-12-25 06:25 ET: [Daniel/Susan]\nAuto-recovery script in action. monitor_train_log.py managed to send warning email about progress but then\nmy instance crashed with :\nPermissionError: [Errno 13] Permission denied:\n'/shared/home/namangoyal/checkpoints/175B/175B_run12.56.me_fp16.minscale0.25.fsdp.gpf32.0.relu.transfor\nmer_lm_megatron.nlay96.emb12288.lrnpos.0emb_scale.bm_none.tps2048.gpt2.adam.b2_0.95.eps1e-08.cl0.3\n.lr3e-05.endlr6e-06.wu2000.dr0.1.atdr0.1.0emb_dr.wd0.1.ms8.uf1.mu143052.s1.ngpu992/train.log.autorecover\n_lock'\nMonitoring the restart: the training seems to have gotten terminated by something. My first guess is that the\nmonitor script is misbehaving, I’m killing all instances\nps aux | grep monitor\nsudo kill 43600\n[Susan butting in]\n●\nPut job in requeue / held state\n●\nRan fixmycloud idle\n●\nPut the infoROM corrupted nodes in drain: 11, 111, 148\n●\nUpdated nodelist:\nsudo scontrol update job=4136\nNodeList=node-[1-10,12-18,20-32,34-54,56-90,94-98,100-106,108-110,112-131,136-138]\n●\nReleased the job\n2021-12-24 12:40 ET: Auto-recovery script\n●\nAfter merging #2837 it is now possible to auto-recover from node failures or hung jobs\n○\nIn the case of auto-recovery, the oncall will get a sequence of emails:\n○\nEmail #1: File not modified in 900 seconds\n○\nEmail #2: Detected hang, auto-recovery in progress\n○\nEmail #3: Auto-recovery was apparently successful\n■\nThis last email should contain the last few train.log lines\n●\nSingle pass of updating oncall docs with auto-recovery information.\n\n●\nNote: it is fine for multiple people to run this command, since there is a locking mechanism to prevent\nmultiple scripts from auto-recovering the same job\n●\nInvocation:\n./scripts/cloud/monitor_train_log.py --mailto <scrubbed>@fb.com\n/shared/home/namangoyal/checkpoints/175B/175B_run12.56*/train.log --modified-threshold 900\n--slurm-jobid 4136\n2021-12-24 10:00 ET: Kurt\n●\nChecking status of idle nodes via fixmycloud\n○\nStill getting infoROM warnings on 148\n○\nEverything else passes\n●\nUpdated the node tracker table above.\n2021-12-23 6:00 PM ET: Kurt\n●\nChecking status of idle nodes via fixmycloud:\n○\n2021-12-23 23:06:23 WARNING  nvidia_smi | node-148: infoROM is corrupted at gpu 0001:00:00.0\n○\n2021-12-23 23:06:23 WARNING  nvidia_smi | node-148: infoROM is corrupted at gpu 000E:00:00.0\n○\nEverything else seems OK\n2021-12-23 8:30 ET: Myle\n●\nFollowups for CSP:\n○\nnode-107 died in the night\n○\nWhy was node-55 added to the cluster in an unhealthy state (slow NCCL)?\n●\nJob died ~30 min ago:\n○\nsrun: error: Node failure on node-107\n○\nJob actually died, so will need to manually relaunch\n●\nSeeing 133 idle hosts, running fixmycloud.py to confirm they are healthy\n○\nFound 3 hosts with bad NCCL:\n■\nnode-55: max bandwidth 147.82 below threshold 180\n●\nThis was “Off” last night, so CSP must have added it to the cluster overnight (and\nit’s bad)\n■\nnode-91: max bandwidth 49.03 below threshold 180\n●\nFrom notes, this was bad last night too\n■\nnode-92: max bandwidth 134.91 below threshold 180\n●\nFrom notes, this was bad last night too\n○\nDown to 130 healthy nodes\n■\nnode-[33,145-150] are idle/healthy after relaunching job\n# LAUNCH OF 12.56\n# use updated fairscale\ncd ~/fairscale\ngit fetch origin prefetch_fsdp_params_simple\ngit checkout prefetch_fsdp_params_simple\ncd ~/Megatron-LM\ngit checkout tags/v2.6\ncd ~/fairseq-py\ngit fetch origin gshard_combine_megatron_fsdp\n\ngit checkout gshard_combine_megatron_fsdp\n# use previous blob URL\nBLOB_PREFIX=\"<<<SCRUBBED FOR RELEASE>>>\"\nBLOB_AUTH=\"<<<SCRUBBED FOR RELEASE>>>\"\n# Use `checkpoint_33_98000` which was the last successful checkpoint\nRESTORE_FILE=\"${BLOB_PREFIX}/checkpoint_33_98000.pt?${BLOB_AUTH}\"\nRUN_ID=175B_run12.56\n# Use hosts verified good from fixmycloud from above\nINCLUDED_HOSTS=node-[1-6,8-32,34-54,56-68,70-73,76,78-79,81-84,88-90,96,98,100-106,108-114,116-144] \\\npython -m fb_sweep.opt.sweep_opt_en_lm_175b \\\n-n 124 -g 8 -t 1 \\\n-p $RUN_ID \\\n--checkpoints-dir /shared/home/namangoyal/checkpoints/175B/ \\\n--local-checkpoints-dir /mnt/scratch/$USER/checkpoints/$(date +%Y-%m-%d).$RUN_ID \\\n--restore-file $RESTORE_FILE \\\n--full-cloud-upload-path \"${BLOB_PREFIX}/?${BLOB_AUTH}\"\n./scripts/cloud/monitor_train_log.py --mailto <scrubbed> /shared/home/namangoyal/checkpoints/175B/175B_run12.56*/train.log\n--modified-threshold 900\n2021-12-23 7:00 ET: Stephen\nManually drained/undrained nodes to mark the correspondence to what CSP reports. Ran update_hosts. We\nstill see some issues with slurm not forgetting the IP address of nodes that went down.\n2021-12-22 3:30 pm ET: [Myle] new oncall\n●\nCurrent status:\n○\nJob seems healthy\n○\n3 healthy spare nodes (node-[33,96,101])\n○\n6 nodes in drain\n■\nnode-[55,124]\n●\n“Off” according to cloud UI\n■\nnode-[91,92,135,147]\n●\n“Ready” according to cloud UI\n●\ndrained with reason Bad_infiniband\n○\nTarget:\n■\nIncrease to 7 nodes tonight\n■\nTomorrow morning (coordinate on chat)\n●\nRelease 5 nodes back to CSP\n●\nThey will take a few hours to make this healthy\n■\nBy end of day tomorrow 12 healthy nodes\n○\nRCA\n■\nNot sure about root cause\n■\nCould be “PCIe training”\n■\nRebooting will cycle some systems, but not everything; can try it, but unlikely for it to\nwork\n■\nCSP pre-flight tests didn’t cover multi-node tests previously\n●\nCSP has now recently added these to their preflight tests\n●\n+ CSP has been able to repro the poor NCCL test results\n●\nNote from Stephen (shared with CSP)\n\n○\nBeing very explicit to sync both sides.\nOkay I confirmed we have the new 4 nodes, and all 4 still fail our NCCL\ntests, but are on standby as emergency replacements (at a 20% slowdown,\nbut that’s better than 100%)\n33 and 101 are yellow for very high correctable ECC failures, as discussed\nearlier in chat. They are our current main backups.\n124 and 55 and still showing some slurm weirdness and are absolute no gos.\nThose nodes are down and I don’t know why slurm is confused.\nThanks everyone.\n2021-12-21 4:30 pm ET: [Moya] Kick off train 12.55\nRan fixmycloud on\nnode-[1-6,8-32,34-54,56-68,70-73,78-79,81-84,88-90,100,102-114,116-123,125-134,136-146,148-150]\nSince per thread those were the ones Stephen last used\n●\nTodo - make error message on NCCL print and not just say “exit code 1”\n●\n`please run /shared/home/mpchen/fairseq-py/scripts/cloud/nccl_tests/get_nccl_tests.sh first` was the\nerror it ate\nInfoROM + ECC are yellows; have to run with anyway (cause this is 126 hosts, per conversing offline with\nStephen)\n# LAUNCH OF 12.55\n# use updated fairscale\ncd ~/fairscale\ngit fetch origin prefetch_fsdp_params_simple\ngit checkout prefetch_fsdp_params_simple\ncd ~/Megatron-LM\ngit checkout tags/v2.6\ncd ~/fairseq-py\ngit fetch origin gshard_combine_megatron_fsdp\ngit checkout gshard_combine_megatron_fsdp\n# use new blob URL\nOLD_BLOB_PREFIX=\"<<<SCRUBBED FOR RELEASE>>>\"\nNEW_BLOB_PREFIX=\"<<<SCRUBBED FOR RELEASE>>>\"\"\nBLOB_AUTH=\"<<<SCRUBBED FOR RELEASE>>>\"\n# Use `checkpoint_31_92000` per comment in thread of that being how far things got to\n\nRESTORE_FILE=\"${OLD_BLOB_PREFIX}/checkpoint_31_92000.pt?${BLOB_AUTH}\"\nRUN_ID=175B_run12.55\n# Use hosts verified good from fixmycloud from above\nINCLUDED_HOSTS=node-[1-6,8-32,34-54,56-68,70-73,78-79,81-84,88-90,100,102-114,116-123,125-134,136-146,148-150] \\\npython -m fb_sweep.opt.sweep_opt_en_lm_175b \\\n-n 124 -g 8 -t 1 \\\n-p $RUN_ID \\\n--checkpoints-dir /shared/home/namangoyal/checkpoints/175B/ \\\n--local-checkpoints-dir /mnt/scratch/$USER/checkpoints/$(date +%Y-%m-%d).$RUN_ID \\\n--restore-file $RESTORE_FILE \\\n--full-cloud-upload-path \"${NEW_BLOB_PREFIX}/?${BLOB_AUTH}\"\n2021-12-21 (morning until 3 pm ish) [Stephen] Omicron Sev\nCSP fat fingered and deleted our entire cluster when trying to replenish our buffer nodes.\nAt 14:25 ET they asked us to initiate our preflight checks\nUpon reallocation we had:\n2021-12-21 19:00:04 ERROR updatehost | Nodes node-42 and node-91 have same IP (10.30.4.95)\n2021-12-21 19:00:04 ERROR updatehost | Nodes node-90 and node-92 have same IP (10.30.4.248)\n2021-12-21 19:00:04 ERROR updatehost | Nodes node-9 and node-115 have same IP (10.30.4.23)\n2021-12-21 19:00:04 ERROR updatehost | Nodes node-96 and node-135 have same IP (10.30.4.45)\n2021-12-21 19:00:04 ERROR updatehost | Nodes node-99 and node-147 have same IP (10.30.4.60)\nThis was corrected by CSP. According to CSP, this is lag until slurm notices, but I repeated this procedure\nseveral times over at least 10 minutes.\n92 and 115 also seemed unreachable but slurm wasn’t having them fail their heartbeat. Manually marked as\ndrained\nRan healthchecks:\n2021-12-21 19:36:00 WARNING nvidia_smi | node-33: ecc high correctables: DRAM Correctable: 170435\n2021-12-21 19:36:00 WARNING nvidia_smi | node-65: ecc high correctables: DRAM Correctable: 37642\n2021-12-21 19:36:00 WARNING nvidia_smi | node-101: ecc high correctables: DRAM Correctable:\n1700812021-12-21 19:36:21 WARNING nvidia_smi | node-11: infoROM is corrupted at gpu 000B:00:00.0\n2021-12-21 19:36:21 WARNING nvidia_smi | node-93: infoROM is corrupted at gpu 0004:00:00.0\n2021-12-21 19:36:21 WARNING nvidia_smi | node-93: infoROM is corrupted at gpu 000E:00:00.0\n2021-12-21 19:36:21 WARNING nvidia_smi | node-107: infoROM is corrupted at gpu 0001:00:00.0\n2021-12-21 19:36:21 WARNING nvidia_smi | node-111: infoROM is corrupted at gpu 0002:00:00.0\n2021-12-21 19:36:21 WARNING nvidia_smi | node-148: infoROM is corrupted at gpu 0001:00:00.0\n2021-12-21 19:36:21 WARNING nvidia_smi | node-148: infoROM is corrupted at gpu 000E:00:00.0\n2021-12-21 19:39:23 ERROR nccl | node-7: max bandwidth 150.11 below threshold 180\n2021-12-21 19:39:23 ERROR nccl | node-69: max bandwidth 143.8 below threshold 180\n2021-12-21 19:39:23 ERROR nccl | node-74: max bandwidth 144.8 below threshold 180\n2021-12-21 19:39:23 ERROR nccl | node-75: max bandwidth 145.35 below threshold 180\n2021-12-21 19:39:23 ERROR nccl | node-76: max bandwidth 145.35 below threshold 180\n\n2021-12-21 19:39:23 ERROR nccl | node-77: max bandwidth 145.15 below threshold 180\n2021-12-21 19:39:23 ERROR nccl | node-80: max bandwidth 145.67 below threshold 180\n2021-12-21 19:39:23 ERROR nccl | node-85: max bandwidth 149.82 below threshold 180\n2021-12-21 19:39:23 ERROR nccl | node-86: max bandwidth 149.54 below threshold 180\n2021-12-21 19:39:23 ERROR nccl | node-87: max bandwidth 144.02 below threshold 180\n2021-12-21 19:39:23 ERROR nccl | node-93: max bandwidth 144.57 below threshold 180\n2021-12-21 19:39:23 ERROR nccl | node-94: max bandwidth 140.92 below threshold 180\n2021-12-21 19:39:23 ERROR nccl | node-95: max bandwidth 146.5 below threshold 180\n2021-12-21 19:39:23 ERROR nccl | node-97: max bandwidth 146.5 below threshold 180\n2021-12-21 19:39:23 ERROR nccl | node-98: max bandwidth 143.02 below threshold 180\n2021-12-21 19:39:23 ERROR nccl | node-124: max bandwidth 148.34 below threshold 180\nReplicated the list of bad nccl nodes with multiple tries.\nManually drained pg0-33 and pg0-105 as they had highest correctables.\nThen I requeued susan’s job (which had automatically been put on hold when nodes went down) with\nsudo scontrol hold job=3129\nsudo scontrol update job=3129\n'NodeList=node-[1-6,8-32,34-54,56-68,70-73,78-79,81-84,88-90,100,102-114,116-123,125-134,136-146,148-150]'\nsudo scontrol release 3129\n2021-12-21 5:30am ET: [Susan] Node down, restart from 91,250 with lower LR - Run\n12.53, 12.54\n●\nRunning healthcheck: python scripts/cloud/fixmycloud.py idle\n○\nChecked node-81:\n■\nUnable to determine the device handle for GPU 0001:00:00.0: GPU is lost.  Reboot the\nsystem to recover this GPU\n●\nPR to lower LR: #2833\n○\n12.53 crashed with tokenization error, reverted the cache tokenization change to resume and\ndebug later\n○\nLuckily 91250 is early in a shard-epoch. Took only ~5 min for data loading to finish.\n# LAUNCH OF 12.54\n# use updated fairscale\ncd ~/src/fairscale\ngit fetch origin prefetch_fsdp_params_simple\ngit checkout prefetch_fsdp_params_simple\ncd ~/src/Megatron-LM\ngit checkout tags/v2.6\ncd ~/src/fairseq-py\ngit fetch origin susan/run12.53\ngit checkout susan/run12.53\n# use new blob URL\nBLOB_PREFIX=\"<<<SCRUBBED FOR RELEASE>>>\"\n\nBLOB_AUTH=\"<<<SCRUBBED FOR RELEASE>>>\"\n# confirm epoch of checkpoint from previous train.log - we checkpoint every 250 steps\nRESTORE_FILE=\"${BLOB_PREFIX}/checkpoint_31_91250.pt?${BLOB_AUTH}\"\nRUN_ID=175B_run12.54\nINCLUDED_HOSTS=node-[1-2,4-22,24-25,27-52,54-67,69-80,82-84,86-87,89-96,98-104,106-107,109-112,115-117,119-\n135,147-148,150] \\\npython -m fb_sweep.opt.sweep_opt_en_lm_175b \\\n-n 124 -g 8 -t 1 \\\n-p $RUN_ID \\\n--checkpoints-dir /shared/home/namangoyal/checkpoints/175B/ \\\n--local-checkpoints-dir /mnt/scratch/$USER/checkpoints/$(date +%Y-%m-%d).$RUN_ID \\\n--restore-file $RESTORE_FILE \\\n--full-cloud-upload-path \"${BLOB_PREFIX}/?${BLOB_AUTH}\"\n2021-12-19 12pm ET: Crossing the epoch boundary\n●\nSurprisingly uneventful, no large drop in training ppl\n●\nRan this command to make a backup of the epoch 1 checkpoint:\ncp --recursive --include-pattern \"checkpoint_last*.pt\" <<<SCRUBBED FOR RELEASE>>>\n●\nUploaded a version of the epoch 1 checkpoint without optimizer state here:\n/opt/backups/175B/checkpoint1_eval/\n○\nIt’s also located here on Cloud: /data/175B_checkpoints/checkpoint1_eval\n2021-12-20 12:12 AM PT: [Punit] Node down - requeue for 12.52a\n(fairseq-20210913-py38) punitkoura@ip-0A1E0404:~/src/fairseq-py$ tail\n/shared/home/namangoyal/checkpoints/175B/175B_run12.52.me_fp16.minscale0.25.fsdp.gpf32.0.relu.tran\nsformer_lm_megatron.nlay96.emb12288.lrnpos.0emb_scale.bm_none.tps2048.gpt2.adam.b2_0.95.eps1e-08.c\nl0.3.lr4.5e-05.endlr6e-06.wu2000.dr0.1.atdr0.1.0emb_dr.wd0.1.ms8.uf1.mu143052.s1.ngpu992/train.log\nbuo1u00001Z:46592:47116 [0] NCCL INFO include/net.h:28 -> 2\nbuo1u00001Z:46592:47116 [0] NCCL INFO transport/net.cc:491 -> 2\nbuo1u00001Z:46592:47116 [0] NCCL INFO proxy.cc:351 -> 2\nbuo1u00001Z:46592:47116 [0] NCCL INFO proxy.cc:452 -> 2 [Proxy Thread]\nbuo1u00001Z:46597:47109 [0] ib_plugin.c:670 NCCL WARN NET/IB : Got completion with error 11,\nopcode 32722, len 0, vendor err 137\nbuo1u00001Z:46597:47109 [0] NCCL INFO include/net.h:28 -> 2\nbuo1u00001Z:46597:47109 [0] NCCL INFO transport/net.cc:491 -> 2\nbuo1u00001Z:46597:47109 [0] NCCL INFO proxy.cc:351 -> 2\nbuo1u00001Z:46597:47109 [0] NCCL INFO proxy.cc:452 -> 2 [Proxy Thread]\n(fairseq-20210913-py38) punitkoura@ip-0A1E0404:~/src/fairseq-py$\nNode failure for buo1u00001Z\n\n(fairseq-20210913-py38) punitkoura@ip-0A1E0404:~/src/fairseq-py$ python scripts/cloud/find_host.py\nbuo1u00001Z\nnode-68\n(fairseq-20210913-py38) punitkoura@ip-0A1E0404:~/src/fairseq-py$\nnode-68 is the problematic node.\n(fairseq-20210913-py38) punitkoura@ip-0A1E0404:~/src/fairseq-py$ squeue\n<scrubbed>\n(fairseq-20210913-py38) punitkoura@ip-0A1E0404:~/src/fairseq-py$\nThe current job (2606) has\nnode-[1-2,4-25,27-36,38-52,54-72,74-87,89-96,98-104,106-107,109-113,115-131,147-148,150]\nAs the node list.\nWe need to swap out 68.\nChecking current idle nodes\n(fairseq-20210913-py38) punitkoura@ip-0A1E0404:~/src/fairseq-py$ sinfo\nPARTITION AVAIL  TIMELIMIT  NODES  STATE NODELIST\nhpc*         up   infinite      1 drain~ node-146\nhpc*         up   infinite      7 drain* node-[26,53,88,97,105,114,149]\nhpc*         up   infinite      1  down* node-108\nhpc*         up   infinite      2    mix node-[3,37]\nhpc*         up   infinite    132  alloc\nnode-[1-2,4-25,27-36,38-52,54-72,74-87,89-96,98-104,106-107,109-113,115-135,139-142,147-148,150]\nhpc*         up   infinite      7   idle node-[73,136-138,143-145]\n(fairseq-20210913-py38) punitkoura@ip-0A1E0404:~/src/fairseq-py$\nPotential new host list (adding 143)\nnode-[1-2,4-25,27-36,38-52,54-67,69-72,74-87,89-96,98-104,106-107,109-113,115-131,143,147-148,150]\nRequeue + hold to pause the ongoing training job.\nsudo scontrol requeue job=2606\nsudo scontrol hold job=2606\nNote that the job must be paused before fixmycloud, since NCCL tests require that. See Performing health\nchecks section.\nRunning fixmycloud to check if the proposed new node list is healthy\n(fairseq-20210913-py38) punitkoura@ip-0A1E0404:~/src/fairseq-py$ python scripts/cloud/fixmycloud.py --hosts\nnode-[1-2,4-25,27-36,38-52,54-67,69-72,74-87,89-96,98-104,106-107,109-113,115-131,143,147-148,150]\nLooks like ECC, GPU burn tests etc passed. NCCL tests didn’t complete properly.\nNCCL failures investigation\nRe-ran the failing NCCL command, turns out there was another command which is to be run before the tests\ncan go through.\n\nmissing libnccl and all_reduce_perf; please run\n/shared/home/punitkoura/src/fairseq-py/scripts/cloud/nccl_tests/get_nccl_tests.sh\nfirst!\nAfter running the above command, fixmycloud worked fine. No errors found in the host list.\nResuming job with new host list\nWith confirmation from fixmycloud, the next step is to resume the job with a new host list.\nsudo scontrol update job=2606\nNodelist=node-[1-2,4-25,27-36,38-52,54-67,69-72,74-87,89-96,98-104,106-107,109-113,115-131,143,147-148,150]\nsudo scontrol release job=2606\nTrain.log seems to be working again.\nEnable tensorboard\n​cd /shared/home/namangoyal/checkpoints/175B/tensorboard\nsudo ln -s\n/shared/home/namangoyal/checkpoints/175B/175B_run12.52.me_fp16.minscale0.25.fsdp.gpf32.0.relu.transformer_l\nm_megatron.nlay96.emb12288.lrnpos.0emb_scale.bm_none.tps2048.gpt2.adam.b2_0.95.eps1e-08.cl0.3.lr4.5e-05.end\nlr6e-06.wu2000.dr0.1.atdr0.1.0emb_dr.wd0.1.ms8.uf1.mu143052.s1.ngpu992/tbB run12.52b\nVerified that 12.52b shows up on the tensorboard.\n2021-12-17 15:34 ET: [Daniel] Node down - requeue for 12.52a\nmlx5: buo1u00002X: got completion with error:\n00000000 00000000 00000000 00000000\n00000000 00000000 00000000 00000000\n00000010 00000000 00000000 00000000\n00000000 00008914 10001d2a f7d20ad3\npython scripts/cloud/find_host.py buo1u00002X → node-105\nsudo scontrol requeue job=2606\nsudo scontrol hold job=2606\n# Try swapping in 25:\nsudo scontrol update job=2606 NodeList=node-[1-2,4-25,27-36,38-52,54-72,74-104,106, 107,109-113,115-131,147]\nsudo scontrol release job=2606\n# Turns out 88 and 97 are drained\nCheck that the proposed nodelist is healthy:\npython scripts/cloud/fixmycloud.py --hosts\nnode-[1-2,4-25,27-36,38-52,54-72,74-87,89-96,98-104,106,107,109-113,115-131,147-148,150]\n\n# these two commands together pause the job\nsudo scontrol requeue job=2606\nsudo scontrol hold job=2606\nsudo scontrol update job=2606\nNodeList=node-[1-2,4-25,27-36,38-52,54-72,74-87,89-96,98-104,106,107,109-113,115-131,147-148,150]\nsudo scontrol release job=2606\nDiagnostics on original bad node:\nssh node-105\n(base) danielsimig@buo1u00002X:~/fairseq-py$ python scripts/cloud/gather_diagnostics.py\nDiagnostics uploaded to: <<<SCRUBBED FOR RELEASE>>>\nRe-enable tensorboard\ncd /shared/home/namangoyal/checkpoints/175B/tensorboard\nsudo ln -s\n/shared/home/namangoyal/checkpoints/175B/175B_run12.52.me_fp16.minscale0.25.fsdp.gpf32.0.relu.transfor\nmer_lm_megatron.nlay96.emb12288.lrnpos.0emb_scale.bm_none.tps2048.gpt2.adam.b2_0.95.eps1e-08.cl0.3\n.lr4.5e-05.endlr6e-06.wu2000.dr0.1.atdr0.1.0emb_dr.wd0.1.ms8.uf1.mu143052.s1.ngpu992/tbA\n175B_run12.52a\n2021-12-16 12:15 ET: increased job time limit from 3 days to unlimited:\nsudo scontrol update job=2606 TimeLimit=UNLIMITED\n2021-12-14 18:30 ET: [Kurt] Drain a few nodes\nRan `python scripts/cloud/fixmycloud.py idle`, and found that two nodes broke on NCCL errors (53, 108); see\nentry for (2021-12-09 16:00 PT) where they were marked to be drained\nDrained the nodes:\npython scripts/cloud/slurm.py drain node-53 --reason=\"failing nccl tests\"\n2021-12-14 23:34:15 WARNING  slurm      | Draining node-53 because \"failing nccl tests\"\npython scripts/cloud/slurm.py drain node-108 --reason=\"failing nccl tests\"\n2021-12-14 23:34:26 WARNING  slurm      | Draining node-108 because \"failing nccl tests\"\n2021-12-14 13:30 ET: [Moya] Scancel + Resubmit\nAs decided in conversation, we restart at 72750 steps in order to lower the LR such that we’re far away from a\nnew shard. (Though, maybe could’ve done it an hour earlier as per where `loading train data` showed up in the\nlogs.)\nWhile running ran into a\nbuo1u000088:16674:16674 [1] init.cc:988 NCCL WARN Cuda failure 'uncorrectable\nNVLink error detected during the execution'\nIn the stdlog with \"Please install the megatron submodule\" in the stderr (despite having megatron installed)\n\nWhich was fixed by\ngit submodule update --init fairseq/model_parallel/megatron\nAlso took way too long to realize that it’s completely kosher just outright copy/pasting BLOB_PREFIX and\nBLOB_AUTH from the previous runs… but I blame the peanut gallery. :P\nsqueue # Get id of the 175b run; also for the \"INCLUDED_HOSTS\" command below, to use recently determined cleaned hosts\nscancel 2589 # The id of the 175 b run\n## BEFORE: Change sweep_opt_en_lm_175b to 4.5 (+ commit diff as such)\n#######################\n# LAUNCH OF 12.52\n# use updated fairscale\ncd ~/fairscale\ngit fetch origin prefetch_fsdp_params_simple\ngit checkout prefetch_fsdp_params_simple\ncd ~/Megatron-LM\ngit fetch --tags && git checkout v2.6 # more verbose in case you haven't already fetched the tags\ncd ~/fairseq-py\ngit fetch origin gshard_combine_megatron_fsdp\ngit checkout gshard_combine_megatron_fsdp\nBLOB_PREFIX=\"<<<SCRUBBED FOR RELEASE>>>\"\nBLOB_AUTH=\"<<<SCRUBBED FOR RELEASE>>>\"\nRUN_ID=175B_run12.52 # New id to run\n# Uses included hosts from currently existing run (ie, copy/pasted from squeue prior to cancelling before)\nINCLUDED_HOSTS=node-[1-24,27-36,38-52,54-72,74-87,89-107,109-113,115-131,147] \\\npython -m fb_sweep.opt.sweep_opt_en_lm_175b \\\n-n 124 -g 8 -t 1 \\\n-p $RUN_ID \\\n--checkpoints-dir /shared/home/namangoyal/checkpoints/175B/ \\\n--local-checkpoints-dir /mnt/scratch/$USER/checkpoints/$(date +%Y-%m-%d).$RUN_ID \\\n--full-cloud-upload-path \"${BLOB_PREFIX}/?${BLOB_AUTH}\"\nAlso don’t forget to run the command to update the tensorboard after all of this! (As an aside, I think someone\nelse might’ve run this after I tried to since I forgot to do `sudo` when I ran it, but it’s something a la)\ncd /shared/home/namangoyal/checkpoints/175B/tensorboard\nsudo ln -s <dir of new run in /shared/home/namangoyal/checkpoints/175B/$RUNID...> $RUNID\n2021-12-14 03:30 ET: [Susan] Requeue\n●\nPrevious run crashed with CUDA launch failure again:\n○\nRuntimeError: CUDA error: unspecified launch failure\n●\nRan ecc error check:\n○\nWCOLL=~myleott/hosts PDSH_RCMD_TYPE=ssh pdsh nvidia-smi -q -d \"ECC\" > ecc_error.log\nSaw:\npdsh@ip-0A1E0404: node-88: ssh exited with exit code 15\nSSH’ed over to node-88:\n(base) susanz@buo1u00004D:~$ nvidia-smi\n\nUnable to determine the device handle for GPU 000C:00:00.0: GPU is lost.  Reboot the system to recover this\nGPU\n●\nSwap in 3 in for 88\nsudo scontrol requeue job=2589\nsudo scontrol hold job=2589\nsudo scontrol update job=2589 NodeList=node-[1-24,27-36,38-52,54-72,74-87,89-107,109-113,115-131,147]\nsudo scontrol release job=2589\nNOTE: “2021-12-13: Preemptive Plan” for changing LR still has not occurred yet\nInitiated replace_node on node-88. Sent to Cormac.\n2021-12-13 14:00 ET: Reverse Shadow and Oncall onboardings\nReverse Shadows\nMonday 2pm: Myle\nTuesday 2pm: Stephen\nWednesday 2pm: Susan\nThursday 2pm: Sam\nFriday 2pm: Naman\nMain oncalls\nMonday 2pm: Moya\nTuesday 2pm: Kurt\nWednesday 2pm: Punit\nThursday 2pm: Mikel\nFriday 2pm: Daniel\n2021-12-13: Preemptive Plan\nOn the next crash, we plan to lower the LR from 6.0 -> 4.5. We will observe that for a bit and lower it again to\n3.0 subject to signal.\nTHIS SHOULD CAUSE THE RUN_ID AND THE BLOB FOLDER TO BOTH BE BUMPED.\n2021-12-13 11:49 ET: [Stephen] Bumping timelimit\nAfter we joked about the world record of hitting >2D, realized needed to update the timelimit of the job.\nCommand executed:\nsudo scontrol update job=2589 TimeLimit=UNLIMITED\n2021-12-11 07:53 ET: [Stephen] Cluster Maintenance\n5 down nodes. Time for some reprovisioning.\nnode-25: Kill task failed [root@2021-12-11T10:57:40]\n\nnode-26: Bad_infiniband [roller@2021-12-10T11:30:21]\nnode-53: No_reason_given [susanz@2021-12-10T00:12:45]\nnode-108: No_reason_given [susanz@2021-12-10T00:12:33]\nnode-114: Failed_GPU_burn [roller@2021-12-10T11:15:37]\nInitiated replace_node on 3 bad nodes: 25, 26, 53\n2021-12-11 02:52 PT: [Susan] Noticed IB issues/lost GPU.\nFuture readers: this is now simplified with #2785\nHotswap 25 <> 145\nPrevious run hung with:\nmlx5: buo1u00000S: got completion with error:\n00000000 00000000 00000000 00000000\n00000000 00000000 00000000 00000000\n0000000d 00000000 00000000 00000000\n00000000 01005104 08001c17 10a7b3d3\n# ssh’ing onto node shows GPU lost:\n(base) susanz@buo1u00000S:~$ nvidia-smi\nUnable to determine the device handle for GPU 000C:00:00.0: GPU is lost.  Reboot the system to recover this\nGPU\n# Finding IP address for buo1u00000S\n(base) susanz@buo1u00000S:~$ ifconfig eth0\neth0: flags=4163<UP,BROADCAST,RUNNING,MULTICAST>  mtu 1500\ninet 10.30.4.29  netmask 255.255.252.0  broadcast 10.30.7.255\ninet6 fe80::222:48ff:fe25:4f2d  prefixlen 64  scopeid 0x20<link>\nether 00:22:48:25:4f:2d  txqueuelen 1000  (Ethernet)\nRX packets 1223684089  bytes 1350989781677 (1.3 TB)\nRX errors 0  dropped 0  overruns 0  frame 0\nTX packets 163407144  bytes 3319218659584 (3.3 TB)\nTX errors 0  dropped 0 overruns 0  carrier 0  collisions 0\n# Get hostname from IP address\n(base) susanz@buo1u00000S:~$ grep 10.30.4.29 /etc/hosts\n# Swap in 147 in for 25\nsudo scontrol requeue job=2589\nsudo scontrol hold job=2589\nsudo scontrol update job=2589 NodeList=node-[1-2,4-24,27-36,38-52,54-72,74-107,109-113,115-131,147]\nsudo scontrol release job=2589\nNotes:\n●\nNode 25 went into drain after job was updated with new hostlist\n●\nWe need to replace all the nodes in drain (?)\n\nPARTITION AVAIL  TIMELIMIT  NODES  STATE NODELIST\nhpc*         up   infinite      1 drain~ node-146\nhpc*         up   infinite      5  drain node-[25-26,53,108,114]\nhpc*         up   infinite      2    mix node-[3,37]\nhpc*         up   infinite    136  alloc node-[1-2,4-24,27-36,38-52,54-107,109-113,115-142,147]\nhpc*         up   infinite      3   idle node-[143-145]\n2021-12-10 23:14: [Stephen] 12.51 Resuming\n●\nRemembered I forgot to raise the learning rate\n●\nAlso took at the logs and saw it was fastforwarding the dataloader a lot. We should be right at an epoch\nboundary though! I think I didn’t wait long enough for checkpoints to upload? -- looks like we only have\n976/992 shards.\n○\nObserved via ls \"${BLOB_PREFIX}/?${BLOB_AUTH}\" | grep 61000 | wc -l\nWrote a fresh upload script and launched on nodes with:\npdsh -R ssh -w 'node-[1-2,4-25,27-36,38-52,54-72,74-107,109-113,115-135,139-145,147]' bash ~/restore_61000.sh\nGave a lot of logspam bc of nodes that didn’t participate in the previous job but that’s okay. It was clearly\ncopying on the ones left. Confirmed all 992 were uploaded at the end. Created a PR to increase the learning\nrate back to 6e-5 (GPT-3’s value) and relaunched.\n# LAUNCH OF 12.51\n# use updated fairscale\ncd ~/src/fairscale\ngit fetch origin prefetch_fsdp_params_simple\ngit checkout prefetch_fsdp_params_simple\ncd ~/src/Megatron-LM\ngit checkout tags/v2.6\ncd ~/src/fairseq-py\ngit fetch origin gshard_combine_megatron_fsdp_1251\ngit checkout gshard_combine_megatron_fsdp_1251\n# use new blob URL\nBLOB_PREFIX=\"<<<SCRUBBED FOR RELEASE>>>\"\nBLOB_AUTH=\"<<<SCRUBBED FOR RELEASE>>>\"\nRUN_ID=175B_run12.51 # big money no whammies\n# Doing something radical. All idle nodes should now be safe nodes.\npython -m fb_sweep.opt.sweep_opt_en_lm_175b \\\n-n 124 -g 8 -t 1 \\\n-p $RUN_ID \\\n--checkpoints-dir /shared/home/namangoyal/checkpoints/175B/ \\\n--local-checkpoints-dir /mnt/scratch/$USER/checkpoints/$(date +%Y-%m-%d).$RUN_ID \\\n--full-cloud-upload-path \"${BLOB_PREFIX}/?${BLOB_AUTH}\"\n# For the record, this ended up launching with\n# node-[1-2,4-25,27-36,38-52,54-72,74-107,109-113,115-131]\n\nConfirmed number of fast forward batches looks low (236). As of 00:23 ET looks healthy and signing off. Also\nleft two baselines running for fun.\nJust another last note: we seem to have improved massively in utilization. Here’s PPL with true wall clock:\nGaps are much better since the week of hell.\n2021-12-10 22:42: [Stephen] 12.50 Resuming\n●\nAblation is done. Time to resume.\n●\nMoved Megatron LM back to 2.6.\n# use updated fairscale\ncd ~/src/fairscale\ngit fetch origin prefetch_fsdp_params_simple\ngit checkout prefetch_fsdp_params_simple\ncd ~/src/Megatron-LM\ngit checkout tags/v2.6\ncd ~/src/fairseq-py\ngit fetch origin gshard_combine_megatron_fsdp\ngit checkout gshard_combine_megatron_fsdp\n# use new blob URL\nBLOB_PREFIX=\"<<<SCRUBBED FOR RELEASE>>>\"\nBLOB_AUTH=\"<<<SCRUBBED FOR RELEASE>>>\"\nRUN_ID=175B_run12.50 # big money no whammies\n# Doing something radical. All idle nodes should now be safe nodes.\npython -m fb_sweep.opt.sweep_opt_en_lm_175b \\\n\n-n 124 -g 8 -t 1 \\\n-p $RUN_ID \\\n--checkpoints-dir /shared/home/namangoyal/checkpoints/175B/ \\\n--local-checkpoints-dir /mnt/scratch/$USER/checkpoints/$(date +%Y-%m-%d).$RUN_ID \\\n--full-cloud-upload-path \"${BLOB_PREFIX}/?${BLOB_AUTH}\"\n2021-12-10 20:58 ET: [Stephen] The ablation\nUpdate: Bad news y’all. That ain’t the commit. New launch is clearly on the track as Naman’s. Messing up my\nown environment can't explain it, as this run was when I first upgraded to 2.6.\nBefore launch:\n●\nLame. Hit shard boundary at 60765 updates. Waiting until 61k since I know we have had issues\nrestoring from epoch boundaries before. Beginning ablation at 21:51.\n●\nReverting the bad commit in Megatron-LM with “git revert  -m 1 0be405”\n○\nConfirmed files looked like the right ones touched.\n# use updated fairscale\ncd ~/src/fairscale\ngit fetch origin prefetch_fsdp_params_simple\ngit checkout prefetch_fsdp_params_simple\ncd ~/src/fairseq-py\ngit fetch origin gshard_combine_megatron_fsdp\ngit checkout gshard_combine_megatron_fsdp\n\n# use new blob URL\nOLD_BLOB_PREFIX=\"<<<SCRUBBED FOR RELEASE>>>\"\nBLOB_AUTH=\"<<<SCRUBBED FOR RELEASE>>>\"\nRESTORE_FILE=\"${OLD_BLOB_PREFIX}/checkpoint_18_51750.pt?${BLOB_AUTH}\"\n# change blob prefix so that we don’t clobber Naman’s checkpoints\nBLOB_PREFIX=\"<<<SCRUBBED FOR RELEASE>>>”\nRUN_ID=175B_run12.47.byebad\n# Doing something radical. All idle nodes should now be safe nodes.\npython -m fb_sweep.opt.sweep_opt_en_lm_175b \\\n-n 124 -g 8 -t 1 \\\n-p $RUN_ID \\\n--checkpoints-dir /shared/home/namangoyal/checkpoints/175B/ \\\n--local-checkpoints-dir /mnt/scratch/$USER/checkpoints/$(date +%Y-%m-%d).$RUN_ID \\\n--restore-file $RESTORE_FILE \\\n--full-cloud-upload-path \"${BLOB_PREFIX}/?${BLOB_AUTH}\"\n2021-12-10 05:52 ET: [Stephen] Cluster maintenance\n●\nJust generally checking health of all our fresh idle nodes, making sure fixmycloud is up to the task.\n●\nFound update hosts to not quite be working\n●\nAfter running update hosts, observed that pg0-114 and pg0-3 have the same IP address.\n○\nIn “scontrol show nodes” pg0-114 has  NodeHostName of 10.30.4.11\n○\nIn “scontrol show nodes” pg0-3 has  NodeHostName of 10.30.5.6\n○\nSo why isn’t our script working?\n●\nAlso found hundreds of IP addresses that seem to only be listed once, don’t have slurm names. Ex\n10.30.7.95. I assume those are nodes we used to have\n●\nAlso noticed pg0-146 seems unreachable\n○\nIts slurm NodeAddr is given as a name to itself, not an ip address like the others.\n○\nThis exception was already carved out into the host updating script, so I guess it’s intentional\n●\nNoticed bugs in update_hosts:\n○\nClean known hosts was called before the hosts were updated lol\n○\nIt could easily skip unreachable nodes like -146\n○\nAfter fixing these, the incongruities in the hosts file disappeared. I think we just hadn’t been able\nto run update hosts successfully.\n●\nRan fixmycloud with lots of fixes and improvements\n○\n#2776\n○\nDrained one node bc of bad infiniband, one node bc of gpu burn\n○\nStarted replacing pg0-146\n●\nCurrent status: 124 active; 18 good spares; 4 drained + 1 being replaced\n2021-12-09 16:00 PT: [Susan] Run 12.49, restart due to NCCL errors\n●\nExactly the same garbage as “2021-12-05 12:15pm ET: Requeueing 12.44 and 12.45” run.\n●\nExcluding 108 and 53 and putting in drain.\n●\nLeaves us with:\n(fairseq-20210913) susanz@ip-0A1E0404:~/fairseq-py$ sinfo\n\nPARTITION AVAIL  TIMELIMIT  NODES  STATE NODELIST\nhpc*         up   infinite      4 drain* node-[3,37,73,124]\nhpc*         up   infinite      4  drain node-[26,53,108,114]\nhpc*         up   infinite    124   idle node-[1-2,4-25,27-36,38-52,54-72,74-107,109-113,115-123,125-132]\n●\nRestarting on the only 124 nodes we have that are functional:\n# use updated fairscale\ncd ~/src/fairscale\ngit fetch origin prefetch_fsdp_params_simple\ngit checkout prefetch_fsdp_params_simple\ncd ~/src/fairseq-py\ngit fetch origin gshard_combine_megatron_fsdp\ngit checkout gshard_combine_megatron_fsdp\n# use new blob URL\nBLOB_PREFIX=\"<<<SCRUBBED FOR RELEASE>>>\"\nBLOB_AUTH=\"<<<SCRUBBED FOR RELEASE>>>\"\n# confirm epoch of checkpoint from previous train.log - we checkpoint every 250 steps\nRESTORE_FILE=\"${BLOB_PREFIX}/checkpoint_19_57000.pt?${BLOB_AUTH}\"\nRUN_ID=175B_run12.49\nINCLUDED_HOSTS=node-[1-2,4-25,27-36,38-52,54-72,74-107,109-113,115-123,125-132] \\\npython -m fb_sweep.opt.sweep_opt_en_lm_175b \\\n-n 124 -g 8 -t 1 \\\n-p $RUN_ID \\\n--checkpoints-dir /shared/home/namangoyal/checkpoints/175B/ \\\n--local-checkpoints-dir /mnt/scratch/$USER/checkpoints/$(date +%Y-%m-%d).$RUN_ID \\\n--restore-file $RESTORE_FILE \\\n--full-cloud-upload-path \"${BLOB_PREFIX}/?${BLOB_AUTH}\"\n# monitor train.log - launch in a tmux session\n./scripts/cloud/monitor_train_log.py --mailto <scrubbed>@fb.com\n/shared/home/namangoyal/checkpoints/175B/175B_run12.49.me_fp16.minscale0.25.fsdp.gpf32.0.relu.transformer_l\nm_megatron.nlay96.emb12288.lrnpos.0emb_scale.bm_none.tps2048.gpt2.adam.b2_0.95.eps1e-08.cl0.3.lr4.5e-05.end\nlr6e-06.wu2000.dr0.1.atdr0.1.0emb_dr.wd0.1.ms8.uf1.mu143052.s1.ngpu992/train.log --modified-threshold 900\n# Add new entry for TB - TB dir takes a while to come up since training takes a while to restart (data\nloader has to fast forward, goes through ~75 batches / min)\ncd /shared/home/namangoyal/checkpoints/175B/tensorboard\nsudo ln -s\n../175B_run12.49.me_fp16.minscale0.25.fsdp.gpf32.0.relu.transformer_lm_megatron.nlay96.emb12288.lrnpos.0emb\n_scale.bm_none.tps2048.gpt2.adam.b2_0.95.eps1e-08.cl0.3.lr4.5e-05.endlr6e-06.wu2000.dr0.1.atdr0.1.0emb_dr.w\nd0.1.ms8.uf1.mu143052.s1.ngpu992/tb/ run12.49\n●\n[16:22 PT] Launched\n●\n[16:47 PT] Still waiting for data loader to fast forward\n○\n2021-12-10 00:32:58 | WARNING | fairseq.data.iterators | Fast-forwarding dataloader by 2334\nbatches...\n●\n[16:57 PT] Omg still waiting for data loader to fast forward ;_;\n○\nOur fast forwarding speed, based on notes from run 12.48, seems to be 75 batches / min.\n●\n[17:04 PT] Oh hallelujah\n\n○\n2021-12-10 01:04:14 | WARNING | fairseq.data.iterators | done fast-forwarding dataloader in\n1939.7 seconds\n2021-12-09 10:45am PT: Provisioning error in Cloud for node 124\n●\nReprovisioning, failed again.\n●\nYelled at CSP to give us more machines - will be getting 15 more at some point.\n●\nAlso need them to bump up ingress/egress limits on blob store\n2021-12-09: Megatron v2.6 Debrief + CSP Sync\nDiscussion about what happened\n●\nDiscussion about how it was discovered, reviewing Susan’s observations.\n●\nReviewed how we started going through to find what differed between Naman and Myle’s env.\n●\nReview that we have two versions megatron\n○\nSubmodule, which lets us do the model parallel MHA\n○\nImported, which lets us get fused_softmax\nFuture mitigations & Lessons learned:\n●\nUse single environment for all launches\n○\nContainers?\n○\nAdd assert to sweep.py to check version numbers?\n●\nWe should test upgrading dependencies periodically, in case there are bug fixes\n●\nWhy aren’t relaunches deterministic?\n○\nWe assumed it was just loss scale history, but it seems things are different even when I\nrelaunch the same thing multiple times\n●\nWhat was the bug that Nvidia fixed?\n○\nMegatron v2.6 vs v2.4\n■\nIncludes https://github.com/NVIDIA/Megatron-LM/pull/133\n●\nWhat is the impact of this bug on the first 37% of training?\n●\nDo we want to make any changes after fixing the bug?\n○\nIncrease learning rate?\n○\nIncrease clipping?\n2021-12-08 8:55pm ET: run12.48: relaunch checkpoint_18_54250\n●\nRun with Megatron v2.6 based on previous entries\n●\nResume from 54250, but has to fast-forward through 2630 batches, which takes 35 minutes with CPUs\nall at 100% :(\n\n# use updated fairscale\ncd ~/src/fairscale\ngit fetch origin prefetch_fsdp_params_simple\ngit checkout prefetch_fsdp_params_simple\ncd ~/src/fairseq-py\ngit fetch origin gshard_combine_megatron_fsdp\ngit checkout gshard_combine_megatron_fsdp\n# use new blob URL\nBLOB_PREFIX=\"<<<SCRUBBED FOR RELEASE>>>\"\nBLOB_AUTH=\"<<<SCRUBBED FOR RELEASE>>>\"\nRESTORE_FILE=\"${BLOB_PREFIX}/checkpoint_18_54250.pt?${BLOB_AUTH}\"\nRUN_ID=175B_run12.48\nINCLUDED_HOSTS=node-[1-2,4-25,27-36,38-72,74-112,115-117,119-123,125-132] \\\npython -m fb_sweep.opt.sweep_opt_en_lm_175b \\\n-n 124 -g 8 -t 1 \\\n-p $RUN_ID \\\n--checkpoints-dir /shared/home/namangoyal/checkpoints/175B/ \\\n--local-checkpoints-dir /mnt/scratch/$USER/checkpoints/$(date +%Y-%m-%d).$RUN_ID \\\n--restore-file $RESTORE_FILE \\\n--full-cloud-upload-path \"${BLOB_PREFIX}/?${BLOB_AUTH}\"\n2021-12-08 TBD: Analysis from 12.47.myle\n●\nThe results look different between my run and Naman’s original 12.47 run\n●\n●\nCurrent hypothesis is something in `pip list` is different between the two\n○\nMake myself into Naman: sudo -i namangoyal\n○\nNaman and Myle seem to have differences in our pip-installed version of megatron-lm!\n■\n~myleott/src/Megatron-LM\n■\n~/namangoyal/src/Megatron-LM\n■\nMyle is on v2.4 (42c1cf4279acea5a554500dcb552211f44cbec45)\n■\nNaman is on v2.6 (3860e995269df61d234ed910d4756e104e1ab844)\n●\nGoing to relaunch a few more times:\n○\nrun12.47.myle: my initial relaunch of 12.47 with my env\n○\nrun12.47.myle2: a second relaunch of above to test determinism\n■\nLooks like it’s not deterministic?!\n\n○\nrun12.47.myle3: a third relaunch with Megatron v2.6\n■\nLooks like Megatron v2.6 was making things better:\n■\n2021-12-08 05:05pm ET: GPU failure; launch debug run with Myle’s env\n●\nThere was a GPU failure, which caused 12.47 to hang\n○\nLatest checkpoint is checkpoint_18_54250\n●\nBased on the discussion below, I will relaunch from checkpoint_18_51750 with my env, to see if it\nmatches Naman’s results. This run should be identical to Naman’s original 12.47 run, except launched\nfrom my environment\n○\nIf it matches, then it seems we just got lucky!\n○\nIf it doesn’t match, then we need to dig more and understand if there’s something unique about\nNaman’s environment, or if resuming from checkpoints is nondeterministic somehow\n○\nIn either case, I will cancel and resume from checkpoint_18_54250 thereafter\n# use updated fairscale\ncd ~/src/fairscale\ngit fetch origin prefetch_fsdp_params_simple\ngit checkout prefetch_fsdp_params_simple\ncd ~/src/fairseq-py\ngit fetch origin gshard_combine_megatron_fsdp\ngit checkout gshard_combine_megatron_fsdp\n# use new blob URL\nBLOB_PREFIX=\"<<<SCRUBBED FOR RELEASE>>>\"\nBLOB_AUTH=\"<<<SCRUBBED FOR RELEASE>>>\"\nRESTORE_FILE=\"${BLOB_PREFIX}/checkpoint_18_51750.pt?${BLOB_AUTH}\"\n# change blob prefix so that we don’t clobber Naman’s checkpoints\nBLOB_PREFIX=\"/myleott/2021-12-08/175B_run12.47.myle\"\nRUN_ID=175B_run12.47.myle2\nINCLUDED_HOSTS=node-[1-2,4-25,27-36,38-72,74-113,115-123,125-130] \\\npython -m fb_sweep.opt.sweep_opt_en_lm_175b \\\n-n 124 -g 8 -t 1 \\\n-p $RUN_ID \\\n--checkpoints-dir /shared/home/namangoyal/checkpoints/175B/ \\\n--local-checkpoints-dir /mnt/scratch/$USER/checkpoints/$(date +%Y-%m-%d).$RUN_ID \\\n\n--restore-file $RESTORE_FILE \\\n--full-cloud-upload-path \"${BLOB_PREFIX}/?${BLOB_AUTH}\"\n2021-12-08 04:00am PT: Checking in\n[From Susan: How did Naman’s run just get a free 2% bump in wps? Truly the golden touch. Also seems to be\na significant drop in actv_norm as well, which generally helps us avoid overflow issues. Getting lucky here?]\n●\nComparison of config namespaces shows no significant differences:\npython scripts/compare_namespaces.py \\\n175B_run12.47*/train.log \\\n175B_run12.46*/train.log\n●\nPerhaps some difference in environment? Cc\nto confirm:\nNaman Goyal\n○\nPyTorch version: '1.9.0+cu111'\n■\nConfirmed this matches Myle’s env for 12.46\n○\nFairscale version:\n■\nBranch: prefetch_fsdp_params_simple\n■\nCommit: 8820049331331c773077c257667aa81baf4cc9f9\n●\nConfirmed this matches Myle’s env for 12.46\n○\nMegatron submodule version:\n■\nBranch: fairseq_v2 (16623c2dce9068f3f9574348b5b3c35c0c5a85c6)\n●\nConfirmed this matches Myle’s env for 12.46 and Susan’s env\n○\nFairseq commit:\n■\nCommit: fc24ce0ae48626a6d18dbb45b486600c3732c14f\n■\nBranch: gshard_combine_megatron_fsdp\n■\nMyle’s env for 12.46 was 6c973d4f92d0c9813f439a4920b23c7f93429511. There’s only\ntwo commits separating this from Naman’s and they are unrelated to training\n○\nCode snapshots, for reference:\n■\n12.47 (Naman):\n/shared/home/namangoyal/src/fairseq_gshard/fairseq-py/slurm_snapshot_code/2021-12\n-08T05_07_39.022577\n■\n12.46 (Myle):\n/shared/home/myleott/src/fairseq2/slurm_snapshot_code/2021-12-06T13_33_13.859846\n■\nConfirmed no meaningful differences with:\ndiff -bur --exclude __pycache__\n/shared/home/namangoyal/src/fairseq_gshard/fairseq-py/slurm_snapshot_code/2021-12-08T05\n_07_39.022577\n/shared/home/myleott/src/fairseq2/slurm_snapshot_code/2021-12-06T13_33_13.859846\n●\nFor future reference, here are the machines used for each run:\n○\nrun 12.46.2: node-[1-2,4-25,27-50,52-64,66-128]\n○\nrun 12.47: node-[1-2,4-25,27-72,74-96,98-112,114-117,119-123,125-131]\n○\nDiff:\n■\nRemove node-[73,97,113,118,124]\n■\nAdd node-[51,65,129,130,131]\n\n2021-12-07 10:59pm ET: RuntimeError: CUDA error:\nCUBLAS_STATUS_EXECUTION_FAILED when calling\nNOTES FOR FUTURE:\n1) 113 and 118 has info ram issue, look into if its a real issue or not, if it is then recycle\n●\nError stack trace:\ndata.storage().resize_(size.numel())\nRuntimeError: CUDA error: unspecified launch failure\nCUDA kernel errors might be asynchronously reported at some other API call,so the stacktrace below might be\nincorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1.\n●\nRan ECC error check:\n○\nWCOLL=~/hosts PDSH_RCMD_TYPE=ssh pdsh nvidia-smi -q -d \"ECC\" > ecc_error.log\n○\nhcp-pg0-97 and node-124 hang and are completely unreachable\n○\nnode-132 has “Could not resolve hostname node-132”\n●\nRan nvidia-smi check on all but above 3 nodes:\npython scripts/cloud/nvidia_smi.py node-[1-2,4-25,27-72,74-96,98-123,125-131]\n2021-12-08 04:33:21 WARNING  __main__   | node-23: ecc high correctables: DRAM Correctable: 170081\n2021-12-08 04:33:21 WARNING  __main__   | node-88: ecc high correctables: DRAM Correctable: 506807\n2021-12-08 04:33:21 WARNING  __main__   | node-85: ecc high correctables: DRAM Correctable: 16758\n2021-12-08 04:33:21 WARNING  __main__   | node-85: ecc high correctables: DRAM Correctable: 16758\n2021-12-08 04:33:21 INFO     __main__   | All nodes pass ECC checks.\n2021-12-08 04:33:21 INFO     __main__   | Running MIG checks on node-[1-2,4-25,27-72,74-96,98-123,125-131]\n2021-12-08 04:33:24 INFO     __main__   | All nodes pass MIG tests\n2021-12-08 04:33:24 INFO     __main__   | Running InfoROM checks on\nnode-[1-2,4-25,27-72,74-96,98-123,125-131]\n\n2021-12-08 04:33:38 WARNING  __main__   | node-113: infoROM is corrupted at gpu 0002:00:00.0\n2021-12-08 04:33:38 WARNING  __main__   | node-118: infoROM is corrupted at gpu 000B:00:00.0\n●\nGiven above have to choose whether to exclude nodes with high correctable ecc error or inforam\nissues.\n○\nTaking a call to exclude “inforom is corrupted” hosts and choosing to go with following hosts:\n# use updated fairscale\ncd ~/src/fairscale\ngit fetch origin prefetch_fsdp_params_simple\ngit checkout prefetch_fsdp_params_simple\ncd ~/src/fairseq-py\ngit fetch origin gshard_combine_megatron_fsdp\ngit checkout gshard_combine_megatron_fsdp\n# use new blob URL\nBLOB_PREFIX=\"<<<SCRUBBED FOR RELEASE>>>\"\nBLOB_AUTH=\"<<<SCRUBBED FOR RELEASE>>>\"\nRESTORE_FILE=\"${BLOB_PREFIX}/checkpoint_18_51750.pt?${BLOB_AUTH}\"\nRUN_ID=175B_run12.47\nINCLUDED_HOSTS=node-[1-2,4-25,27-72,74-96,98-112,114-117,119-123,125-131] \\\npython -m fb_sweep.opt.sweep_opt_en_lm_175b \\\n-n 124 -g 8 -t 1 \\\n-p $RUN_ID \\\n--checkpoints-dir /shared/home/namangoyal/checkpoints/175B/ \\\n--local-checkpoints-dir /mnt/scratch/$USER/checkpoints/$(date +%Y-%m-%d).$RUN_ID \\\n--restore-file $RESTORE_FILE \\\n--full-cloud-upload-path \"${BLOB_PREFIX}/?${BLOB_AUTH}\"\n# monitor train.log\n./scripts/cloud/monitor_train_log.py --mailto <scrubbed>\n/shared/home/namangoyal/checkpoints/175B/175B_run12.46*/train.log --modified-threshold 900\n# Add new entry for TB\ncd /shared/home/namangoyal/checkpoints/175B/tensorboard\nsudo ln -s\n../175B_run12.47.me_fp16.minscale0.25.fsdp.gpf32.0.relu.transformer_lm_megatron.nlay96.emb12288.lrnpos.0emb\n_scale.bm_none.tps2048.gpt2.adam.b2_0.95.eps1e-08.cl0.3.lr4.5e-05.endlr6e-06.wu2000.dr0.1.atdr0.1.0emb_dr.w\nd0.1.ms8.uf1.mu143052.s1.ngpu992/tb/ run12.47\n2021-12-07 10:48am ET: ECC error, requeueing 12.46\n●\nBlame to node-26\n○\nBased on first line in stderr: “srun: error: node-26: task 196: Aborted (core dumped)”\n○\nssh to node and nvidia-smi:\n■\nUnable to determine the device handle for GPU 000B:00:00.0: GPU is lost.\n●\nUndraining node-108, which had “port error” previously\n○\nsudo scontrol update node=node-108 state=resume\n\n●\nRequeue with:\n○\nsudo scontrol requeue job=2487\n○\nsudo scontrol hold job=2487\n○\nsudo scontrol update job=2487 NodeList=node-[1-2,4-25,27-50,52-64,66-128]\n○\nsudo scontrol release job=2487\n2021-12-06 8:30am ET: Lowering LR and launching 12.46\n●\nLowering LR to 4e-5\n# use updated fairscale\ncd ~/src/fairscale\ngit fetch origin prefetch_fsdp_params_simple\ngit checkout prefetch_fsdp_params_simple\ncd ~/src/fairseq-py\ngit fetch origin gshard_combine_megatron_fsdp\ngit checkout gshard_combine_megatron_fsdp\n# use new blob URL\nBLOB_PREFIX=\"<<<SCRUBBED FOR RELEASE>>>\"\nBLOB_AUTH=\"<<<SCRUBBED FOR RELEASE>>>\"\nRESTORE_FILE=\"${BLOB_PREFIX}/checkpoint_16_46250.pt?${BLOB_AUTH}\"\nRUN_ID=175B_run12.46\nINCLUDED_HOSTS=node-[1-2,4-50,52-64,66-107,109-128] \\\npython -m fb_sweep.opt.sweep_opt_en_lm_175b \\\n-n 124 -g 8 -t 1 \\\n-p $RUN_ID \\\n--checkpoints-dir /shared/home/namangoyal/checkpoints/175B/ \\\n--local-checkpoints-dir /mnt/scratch/$USER/checkpoints/$(date +%Y-%m-%d).$RUN_ID \\\n--restore-file $RESTORE_FILE \\\n--full-cloud-upload-path \"${BLOB_PREFIX}/?${BLOB_AUTH}\"\n# monitor train.log\n./scripts/cloud/monitor_train_log.py --mailto <scrubbed>\n/shared/home/namangoyal/checkpoints/175B/175B_run12.46*/train.log --modified-threshold 900\n2021-12-06 05:13 PT: Job Hanging - 3 Machines Down\n●\n5 nodes are partly or fully compromised.\n○\n2x: Unable to determine the device handle for GPU 0002:00:00.0: GPU is lost.\n■\nnode-51\n■\nnode-3\n○\n2x: NCCL error (Got async event : port error)\n■\n[note: unclear if this means the nodes can’t be used; we’ve been using one of them\nsuccessfully for the last 12 hours]\n■\nnode-108\n■\nnode-53\n○\n1x: nvidia-smi hangs\n■\nnode-65\n●\nNext steps:\n\n○\nThe \"port error\" nodes seem to be fine -- we were actually using one of them last night without a\nproblem.\n○\nWill reprovision node-51 and node-3 now, since they will not get reallocated to us, since\nCloud’s built-in health checks will reject them\n2021-12-06 05:00 PT: Grad norm spiking, ppl trending up\n[From Susan: reading more tea leaves here, but seems like we’ve had a couple of grad norm spikes and our\nppl is now slowly diverging. Recommending restarting with half the LR]\n2021-12-05 9pm ET: Requeue after GPU error\n●\nRuntimeError: CUDA error: CUBLAS_STATUS_EXECUTION_FAILED when calling `cublasGemmEx(\nhandle, opa, opb, m, n, k, &falpha, a, CUDA_R_16F, lda, b, CUDA_R_16F, ldb, &fbeta, c,\nCUDA_R_16F, ldc, CUDA_R_32F, CUBLAS_GEMM_DFALT_TENSOR_OP)`\n●\nBlame is node-51\n○\nRan nvidia-smi and node-51 has “Unable to determine the device handle for GPU\n0002:00:00.0: GPU is lost.  Reboot the system to recover this GPU”\n●\nRequeue while replacing node-51 with node-88\n○\nsudo scontrol requeue job=2486\n○\nsudo scontrol hold job=2486\n○\nsudo scontrol update job=2486 NodeList=node-[1,3-50,52-64,66-107,109-128]\n■\nThis replaces node-51 with node-88\n○\nsudo scontrol release job=2486\n●\nNote: It took 50 minutes to resume training from checkpoint_15_45000!\n○\n~30 minutes just fast-forwarding the dataloader!\n○\nAlso added more logging: PR #2748\n\n2021-12-05 18:30 ET: Poking through dmesg to see if we can find where we hung\n●\nNoticed “nvidia-nvswitch: Version Mismatch”. Happens on both pg0-1 and pg0-53\n●\nDmesg actually reports when the job is officially hung: “task python:35662 blocked for more than 120\nseconds” (Though I think this might be the dataloader workers, not the main proc?)\n●\nWhen controlling for looking at timestamps, i don’t see a lot in dmesg :(\n2021-12-05 12:15pm ET: Requeueing 12.44 and 12.45\n●\nThere was a NCCL error (Got async event : port error) on node-108 and node-53\n○\nIt’s not clear which node is bad…\n○\nHow to find the bad nodes:\n■\nThere are two hosts that have port errors in train.log: buo1u000030 and buo1u00001K\n■\nTo translate these into node-XX hostnames:\n●\nssh to each host (e.g., buo1u000030)\n●\nget IP address (e.g., `ifconfig eth0` yields 10.30.4.109)\n●\nuse /etc/hosts to map IP address (e.g., `grep 10.30.4.109 /etc/hosts` yields\nnode-108)\n●\nAction item: update monitor_train_log.py script to alert to port errors\n○\n“NCCL WARN NET/IB : Got async event :”\n●\nGoing to try requeueing and swapping the node out via scontrol\n○\nsudo scontrol requeue job=2483\n○\nsudo scontrol hold job=2483\n○\nsudo scontrol update job=2483 NodeList=node-[1,3-64,66-87,89-107,109-128]\n■\nThis replaces node-108 with node-113\n○\nsudo scontrol release job=2483\n○\nDidn’t work, seems to be trying to download into a blob URL :/\n●\nFall back to manual launch:\n# use updated fairscale\ncd ~/src/fairscale\ngit fetch origin prefetch_fsdp_params_simple\ngit checkout prefetch_fsdp_params_simple\ncd ~/src/fairseq-py\ngit fetch origin gshard_combine_megatron_fsdp\ngit checkout gshard_combine_megatron_fsdp\n# use new blob URL\nBLOB_PREFIX=\"<<<SCRUBBED FOR RELEASE>>>\"\nBLOB_AUTH=\"<<<SCRUBBED FOR RELEASE>>>\"\nRESTORE_FILE=\"${BLOB_PREFIX}/checkpoint_15_44000.pt?${BLOB_AUTH}\"\nRUN_ID=175B_run12.45\nINCLUDED_HOSTS=node-[1,3-64,66-87,89-107,109-128] \\\npython -m fb_sweep.opt.sweep_opt_en_lm_175b \\\n-n 124 -g 8 -t 1 \\\n-p $RUN_ID \\\n--checkpoints-dir /shared/home/namangoyal/checkpoints/175B/ \\\n--local-checkpoints-dir /mnt/scratch/$USER/checkpoints/$(date +%Y-%m-%d).$RUN_ID \\\n--restore-file $RESTORE_FILE \\\n--full-cloud-upload-path \"${BLOB_PREFIX}/?${BLOB_AUTH}\"\n\n# monitor train.log\n./scripts/cloud/monitor_train_log.py --mailto <scrubbed>\n/shared/home/namangoyal/checkpoints/175B/175B_run12.44*/train.log\n2021-12-05 05:35 PT: Checking on 12.44\n[From Susan: looks like lowering LR helps keep us on track wrt ppl. Loss scale that stays below 1 for too long\ncould be a leading indicator of instability going forward.  Set smoothing to ~0.95 to see these trends.]\n2021-12-05 02:24 ET: Side experiment\nNote that I (Stephen) launched a 768M equivalent model to train for a bit to help unblock Anjali. If you need\nspare nodes, please just kill the job, as it is less important.\n02:56 ET both runs look stable. Signing off.\n2021-12-05- 00:00 ET: Loss scale exploding 2 - 12.44\nNote that if we decide to relaunch this, there’s an epoch boundary VERY SHORTLY after 42500 that might be\npreferable.\n●\nSusan pointed out how gnorms seem unusually high. Individual updates are fine, but the frequency of\nspikes has definitely increased (from 12.42+12.43, 58/73 of the spikes >0.2 have been since 42500\nupdates)\n●\nProposed mitigation: Roll back to 42000 and resubmit.\n●\nAlternative mitigation: Lower learning rate further\n○\nOpted to lower it by a factor of 0.9\n○\nAnd restore from 42500\n\n○\nAnd bumped the blob folder\n# use updated fairscale\ncd ~/src/fairscale\ngit fetch origin prefetch_fsdp_params_simple\ngit checkout prefetch_fsdp_params_simple\ncd ~/src/fairseq-py\ngit fetch origin gshard_combine_megatron_fsdp\ngit checkout gshard_combine_megatron_fsdp\n# use new blob URL\nBLOB_PREFIX=\"<<<SCRUBBED FOR RELEASE>>>\"\nBLOB_AUTH=\"<<<SCRUBBED FOR RELEASE>>>\"\nRESTORE_FILE=\"${OLD_BLOB_PREFIX}/checkpoint_14_42500.pt?${BLOB_AUTH}\"\nRUN_ID=175B_run12.44\nINCLUDED_HOSTS=node-[1,3-64,66-87,89-112,114-128] \\\npython -m fb_sweep.opt.sweep_opt_en_lm_175b \\\n-n 124 -g 8 -t 1 \\\n-p $RUN_ID \\\n--checkpoints-dir /shared/home/namangoyal/checkpoints/175B/ \\\n--local-checkpoints-dir /mnt/scratch/$USER/checkpoints/$(date +%Y-%m-%d).$RUN_ID \\\n--restore-file $RESTORE_FILE \\\n--full-cloud-upload-path \"${BLOB_PREFIX}/?${BLOB_AUTH}\"\n# monitor train.log\n./scripts/cloud/monitor_train_log.py --mailto <scrubbed>\n/shared/home/namangoyal/checkpoints/175B/175B_run12.44*/train.log\n2021-12-04 20:24 ET: Loss scale exploding\n●\nObserved loss scale exploding via alerting.\n○\nWaiting some short time to requeue\n○\nIt's teetering. Checking back in 30.\n\n●\nNote: we did manage to checkpoint during this (u 42750). We probably want to roll back to 42500 just\nbecause initializing from a bad loss scale is bad. Loss scale was 8! At that moment in time\n●\nStill confirmed no uploads. Looks like to restore from checkpoint i will need to copy from all the nodes to\na safe directory.\n●\nForced uploading of all local checkpoints via:\n○\npdsh -R ssh -w “$(squeue | tail -n 1 | awk ‘{print $8}’)” ~roller/upload_checkpoints.sh | tee\nscary_upload_log\n●\nBy the time the upload had finished, we had hit 43000 updates and out of alert territory. However,\nloss scale was still only 0.25.\n●\nDecided to again, let it live on. NO ACTUAL ACTION WAS TAKEN WRT THE JOB\n2021-12-04 5:35am ET: Launch of 12.43: Fix blob upload\n●\nAnalysis of 12.42:\n○\nThere was some CUDA error around 1am that caused training to hang\n■\nRuntimeError: CUDA error: unspecified launch failure\n■\nCUDA kernel errors might be asynchronously reported at some other API call,so\nthe stacktrace below might be incorrect.\n○\nLooks like node-65 is to blame (nvidia-smi is slow, has hung process), but I suggest leaving this\nnode in drain for a couple days to report to CSP\n●\nLaunch of 12.43\n○\nUsing this as an opportunity to replace the blob URL with a new one that works\n■\nUsing a fresh blob URL that points to Susan’s blob container:\n/susanz/2021-12-04/175B_run12.42\n■\nSince checkpoints were sitting on local disk on each node, I manually uploaded all of\nthem to the new path with this hacky script.\n○\nReplaced node-65 with node-71\n# use updated fairscale\ncd ~/src/fairscale\ngit fetch origin prefetch_fsdp_params_simple\ngit checkout prefetch_fsdp_params_simple\ncd ~/src/fairseq-py\ngit fetch origin gshard_combine_megatron_fsdp\ngit checkout gshard_combine_megatron_fsdp\n# use new blob URL\nBLOB_PREFIX=\"<<<SCRUBBED FOR RELEASE>>>\"\nBLOB_AUTH=\"<<<SCRUBBED FOR RELEASE>>>\"\nRESTORE_FILE=\"${BLOB_PREFIX}/checkpoint_14_40750.pt?${BLOB_AUTH}\"\nRUN_ID=175B_run12.43\nINCLUDED_HOSTS=node-[1,3-64,66-87,89-112,114-128] \\\npython -m fb_sweep.opt.sweep_opt_en_lm_175b \\\n-n 124 -g 8 -t 1 \\\n-p $RUN_ID \\\n--checkpoints-dir /shared/home/namangoyal/checkpoints/175B/ \\\n--local-checkpoints-dir /mnt/scratch/$USER/checkpoints/$(date +%Y-%m-%d).$RUN_ID \\\n--restore-file $RESTORE_FILE \\\n--full-cloud-upload-path \"${BLOB_PREFIX}/?${BLOB_AUTH}\"\n\n# monitor train.log\n./scripts/cloud/monitor_train_log.py --mailto <scrubbed>\n/shared/home/namangoyal/checkpoints/175B/175B_run12.43*/train.log\n2021-12-03, 10:35pm ET: DO NOT REBOOT OR REPROVISION ANY NODES UNTIL\nFURTHER NOTICE\nIt seems the upload-to-blob is broken due to a limit on the Cloud side, so the latest checkpoints are sitting on\nthe local disks on each node and are not being uploaded to blob. I manually copied 40500 to /data for now, but\nplease do not reprovision any nodes until this is resolved, since we will lose the latest checkpoint data in that\ncase.\nThe error seems to be “409 The uncommitted block count cannot exceed the maximum limit of 100,000\nblocks.”\n●\nSome Google’ing suggests this is due to having too many incomplete/failed uploads to the given path.\n●\nManually uploading any file to the blob path consistently fails with the same error.\n●\nI tried deleting some of the stored checkpoints to see if that resolved anything, but no luck.\n○\nNote: I’ve now deleted most of the historical checkpoints at *250 and *750 steps, so between\n~25k and ~40k steps we now only have checkpoints at *500 and *000 steps\n●\nThis CSP help article suggests the “Wait 7 days for the uncommitted block list to garbage collect.”\n●\nSeems the easiest fix for now is to switch to a new blob container.\n●\nMitigation: Switch to a new blob container:\nsusanz/2021-12-04/175B_run12.42\n2021-12-03\n●\nMyle to finish the SGD code -- done\n●\nStephen to launch a 4 node tiny model for Anjali\n2021-12-03 7:20am ET: Launch of 12.42: Switch back to Adam\n●\n12.41 seemed to make no progress in terms of pnorms or loss\n●\nWe also implemented SGD instead of SGDW (i.e., weight decay was wrong)\n●\nProposal: roll back to 12.39 with AdamW and further lower learning rate from 9e-5 to 6e-5 to match\nGPT-3\n○\n#2736\n# use updated fairscale\ncd ~/src/fairscale\ngit fetch origin prefetch_fsdp_params_simple\ngit checkout prefetch_fsdp_params_simple\ncd ~/src/fairseq-py\ngit fetch origin gshard_combine_megatron_fsdp\ngit checkout gshard_combine_megatron_fsdp\nBLOB_PREFIX=\"<<<SCRUBBED FOR RELEASE>>>\"\nBLOB_AUTH=\"<<<SCRUBBED FOR RELEASE>>>\"\n\nRESTORE_FILE=\"${BLOB_PREFIX}/checkpoint_13_38500.pt?${BLOB_AUTH}\"\nRUN_ID=175B_run12.42\nINCLUDED_HOSTS=node-[1,3-70,72-87,89-112,114-128] \\\npython -m fb_sweep.opt.sweep_opt_en_lm_175b \\\n-n 124 -g 8 -t 1 \\\n-p $RUN_ID \\\n--checkpoints-dir /shared/home/namangoyal/checkpoints/175B/ \\\n--local-checkpoints-dir /mnt/scratch/$USER/checkpoints/$(date +%Y-%m-%d).$RUN_ID \\\n--restore-file $RESTORE_FILE \\\n--full-cloud-upload-path \"${BLOB_PREFIX}?${BLOB_AUTH}\"\n2021-12-02 17:16 ET: Fake SGD debacle: Debrief discussion\nSummary of events and mitigations\nCurrent hypothesis is that all our beta1=0 runs didn’t work.\n●\nReason for this bug was because adam state dicts load the betas\n●\n12.35 (cloud checkpoints caused to never launch) and 12.36 (beta1=0) and 12.37 (simple requeue of\n36)\n●\nNote 12.38 and 12.39 were always meant to be true adam\nDecision made to hard switch to true vanilla SGD (with launch of 12.41). Note, this required implementing\nfp16-friendly SGD.\nNOTE FOR FUTURE: BECAUSE OF THE BETA1 BUG [WARNING: See 2021-12-02 17:16 ET: Debrief]\ntherefore any ablations with 12.36/12.37 are no longer valid.\nNext paths\n●\nPrediction: 12.41 [true sgd] is probably going to drop rapidly due to rapidly annealing learning rate.\n●\nSome debate\n●\nAction Item: We should review the megatron code and see if they have anything about the switching.\nWe don’t believe they did this trick.\n2021-12-02 16:08 ET: Launch of 12.41: Switching to true Vanilla SGD\nDO NOT REQUEUE THIS RUN, IT CONTAINS RESTORE FILE LOGIC THAT WILL RESET BACK TO 37K\nEVERY TIME!\nDO NOT REQUEUE THIS RUN, IT CONTAINS --RESET-OPTIMIZER LOGIC!\n\n# use updated fairscale\ncd ~/src/fairscale\ngit fetch origin prefetch_fsdp_params_simple\ngit checkout prefetch_fsdp_params_simple\ncd ~/src/fairseq-py\ngit fetch origin gcmf-1241\ngit checkout gcmf-1241\nBLOB_URL=\"<<<SCRUBBED FOR RELEASE>>>”\nCKPT_DIR=/data/users/susanz/checkpoints\n# these two lines skipped because susan already downloaded them, but leaving them as future reference\n#### cd $CKPT_DIR\n#### cp --recursive --include-pattern \"checkpoint_13_37000*.pt\" \"$BLOB_URL\" checkpoint_13_37000\nRESTORE_FILE=$CKPT_DIR/checkpoint_13_37000/175B_run12.14.me_fp16.minscale0.25.fsdp.gpf32.0.relu.transformer_lm_me\ngatron.nlay96.emb12288.lrnpos.0emb_scale.bm_none.tps2048.gpt2.adam.b2_0.95.eps1e-08.cl1.0.lr0.00012.endlr6e-06.wu\n2000.dr0.1.atdr0.1.0emb_dr.wd0.1.ms8.uf1.mu143052.s1.ngpu992/checkpoint_13_37000.pt\nRUN_ID=175B_run12.41\nINCLUDED_HOSTS=node-[1,3-70,72-87,89-112,114-128] \\\npython -m fb_sweep.opt.sweep_opt_en_lm_175b \\\n-n 124 -g 8 -t 1 \\\n-p $RUN_ID \\\n--checkpoints-dir /shared/home/namangoyal/checkpoints/175B/ \\\n--local-checkpoints-dir /mnt/scratch/$USER/checkpoints/2021-12-02.$RUN_ID \\\n--restore-file $RESTORE_FILE \\\n--full-cloud-upload-path $BLOB_URL\n2021-12-02 10:25am ET: Launch of 12.40: Intended to be fake SGD with lower learning\nrate [WARNING: See 2021-12-02 17:16 ET: Debrief on why that may not be]\nDebate ensued. Decided to launch Fake SGD with:\n●\nLower learning rate\n●\nLower clip\nReasoning: The ablations give signal on which direction SGD learning should go. And both changing\nhyperparameters effectively lower LR so they won’t conflict. Signal is directionally sane.\nNOTE FOR FUTURE: BECAUSE OF THE BETA1 BUG [WARNING: See 2021-12-02 17:16 ET: Debrief]\ntherefore any ablations with 12.36/12.37 are no longer valid.\n\n# use updated fairscale\ncd ~/src/fairscale\ngit fetch origin prefetch_fsdp_params_simple\ngit checkout prefetch_fsdp_params_simple\ncd ~/src/fairseq-py\ngit fetch origin gcmf-1240\ngit checkout gcmf-1240\nBLOB_URL=\"<<<SCRUBBED FOR RELEASE>>>\"\nCKPT_DIR=/data/users/susanz/checkpoints\n# these two lines skipped because susan already downloaded them, but leaving them as future reference\n#### cd $CKPT_DIR\n#### cp --recursive --include-pattern \"checkpoint_13_37000*.pt\" \"$BLOB_URL\" checkpoint_13_37000\nRESTORE_FILE=$CKPT_DIR/checkpoint_13_37000/175B_run12.14.me_fp16.minscale0.25.fsdp.gpf32.0.relu.transformer_lm_me\ngatron.nlay96.emb12288.lrnpos.0emb_scale.bm_none.tps2048.gpt2.adam.b2_0.95.eps1e-08.cl1.0.lr0.00012.endlr6e-06.wu\n2000.dr0.1.atdr0.1.0emb_dr.wd0.1.ms8.uf1.mu143052.s1.ngpu992/checkpoint_13_37000.pt\nRUN_ID=175B_run12.40\nINCLUDED_HOSTS=node-[1,3-70,72-87,89-112,114-128] \\\npython -m fb_sweep.opt.sweep_opt_en_lm_175b \\\n-n 124 -g 8 -t 1 \\\n-p $RUN_ID \\\n--checkpoints-dir /shared/home/namangoyal/checkpoints/175B/ \\\n--local-checkpoints-dir /mnt/scratch/$USER/checkpoints/2021-12-02.$RUN_ID \\\n--restore-file $RESTORE_FILE \\\n--full-cloud-upload-path $BLOB_URL\n2021-12-01 1:30pm ET: Launch of 12.39\nAnalysis of 12.38\nExploded 10 steps early compared to 12.33! Woot!\nSpent a surprising amount of time riding the line of 0.0625 loss pretty happily\nLaunch of 12.39\nDO NOT REQUEUE THIS RUN, IT CONTAINS RESTORE FILE LOGIC THAT WILL RESET BACK TO 37K\nEVERY TIME! Additionally, instead of requeueing, we should initiate the launch of 12.40!\n\n# use updated fairscale\ncd ~/src/fairscale\ngit fetch origin prefetch_fsdp_params_simple\ngit checkout prefetch_fsdp_params_simple\ncd ~/src/fairseq-py\ngit fetch origin back2adam12.39_gshard_combine_megatron_fsdp\ngit checkout back2adam12.39_gshard_combine_megatron_fsdp\nBLOB_URL=\"<<<SCRUBBED FOR RELEASE>>>\"\nCKPT_DIR=/data/users/susanz/checkpoints\n# these two lines skipped because susan already downloaded them, but leaving them as future reference\n#### cd $CKPT_DIR\n#### cp --recursive --include-pattern \"checkpoint_13_37000*.pt\" \"$BLOB_URL\" checkpoint_13_37000\nRESTORE_FILE=$CKPT_DIR/checkpoint_13_37000/175B_run12.14.me_fp16.minscale0.25.fsdp.gpf32.0.relu.transformer_lm_me\ngatron.nlay96.emb12288.lrnpos.0emb_scale.bm_none.tps2048.gpt2.adam.b2_0.95.eps1e-08.cl1.0.lr0.00012.endlr6e-06.wu\n2000.dr0.1.atdr0.1.0emb_dr.wd0.1.ms8.uf1.mu143052.s1.ngpu992/checkpoint_13_37000.pt\nRUN_ID=175B_run12.39\nINCLUDED_HOSTS=node-[1,3-70,72-87,89-112,114-128] \\\npython -m fb_sweep.opt.sweep_opt_en_lm_175b \\\n-n 124 -g 8 -t 1 \\\n-p $RUN_ID \\\n--checkpoints-dir /shared/home/namangoyal/checkpoints/175B/ \\\n--local-checkpoints-dir /mnt/scratch/$USER/checkpoints/2021-12-01.$RUN_ID \\\n--restore-file $RESTORE_FILE \\\n--full-cloud-upload-path $BLOB_URL\nDiscussion\nDecision: Why not both?\n●\nAblate and optimize for knowledge\n●\nBoth runs beginning at 37k\n●\nBoth runs will keep the loss scale changes. We handled lower rates for a healthy amount of time, and\nthe end result was the same.\n●\nLaunch 12.39:\n○\nADAM with max LR adjusted to 9e-5 (bringing us down to 6.8e-5 at current step)\n○\nKeep clip at 0.3\n○\nLaunched first bc it’s more likely to die quickly, therefore make a happier oncall.\n●\nLaunch 12.40:\n○\nFake SGD with max LR left the same (1.2e-4) and clip lowered to 0.3\nAnalysis\nObservations:\n●\nFake SGD made it further than ADAM\n○\nIt made it a bit further from a requeue (1400 updates from CP, vs 400)\n○\nWithout requeue it slightly later (725 updates from CP vs 400)\n○\nRegardless of ADAM/SGD decision, we probably should change something else.\n●\nThe loss scale changes didn’t change help ADAM:\n\n○\nIt dies in the exact same places as before (within 10 updates)\n●\nGnorm is spiking in the same places for both (data!) but magn differ by method greatly.\n●\nNo signal on validation performance.\nKey:\n●\nPurple = Old ADAM run (12.33)\n●\nLight green = Fake SGD Run (12.36)\n●\nDark green = Manual requeue of Light green after it hit 0.25 bottom out (12.37)\n●\nBlue = New ADAM run (12.38)\nMax distance each got:\nFirst just Fake SGD runs:\n\nNext just the adam runs (Too difficult to distinguish on the same graph)\n\nAnd here is both Fake SGD (with the requeue) and ADAM:\n2021-12-01 8:39am ET: 12.38 True Adam with Lower LR\nDO NOT REQUEUE THIS RUN, IT CONTAINS RESTORE FILE LOGIC THAT WILL RESET BACK TO 37K\nEVERY TIME!\nAnd we broke at the exact same spot. (38414 updates)\nSummary of discussions for next steps:\n●\nProposals:\n○\nSwitch to true SGD\n○\nFutz with loss scale windows / logic\n○\nChange clipping back to 0.3\n●\nDecision:\n○\nRevert to change 37k (last pure adam ckpt), back to pure adam / clip 0.3\n○\nChange the loss scale logic to halve the raise window 132 => 64\n\n○\nChange the loss scale logic to never load from checkpoint\n○\nChanges in #2714\n●\nRationale\n○\nWe get a nice post-hoc comparison of fake-sgd vs adam\n○\nWe’re seeing this loss scale issue come up for both fake-sgd and adam\n○\nRequeues are regularly buying us second lives, which are essentially only messing with loss\nscale windows\nLaunching 12.38:\n# use updated fairscale\ncd ~/src/fairscale\ngit fetch origin prefetch_fsdp_params_simple\ngit checkout prefetch_fsdp_params_simple\ncd ~/src/fairseq-py\ngit fetch origin back2adam12.38_gshard_combine_megatron_fsdp\ngit checkout back2adam12.38_gshard_combine_megatron_fsdp\nBLOB_URL=\"<<<SCRUBBED FOR RELEASE>>>\"\nCKPT_DIR=/data/users/susanz/checkpoints\n# these two lines skipped because susan already downloaded them, but leaving them as future reference\n#### cd $CKPT_DIR\n#### cp --recursive --include-pattern \"checkpoint_13_37000*.pt\" \"$BLOB_URL\" checkpoint_13_37000\nRESTORE_FILE=$CKPT_DIR/checkpoint_13_37000/175B_run12.14.me_fp16.minscale0.25.fsdp.gpf32.0.relu.transformer_lm_me\ngatron.nlay96.emb12288.lrnpos.0emb_scale.bm_none.tps2048.gpt2.adam.b2_0.95.eps1e-08.cl1.0.lr0.00012.endlr6e-06.wu\n2000.dr0.1.atdr0.1.0emb_dr.wd0.1.ms8.uf1.mu143052.s1.ngpu992/checkpoint_13_37000.pt\nRUN_ID=175B_run12.38\nINCLUDED_HOSTS=node-[1,3-70,72-87,89-112,114-128] \\\npython -m fb_sweep.opt.sweep_opt_en_lm_175b \\\n-n 124 -g 8 -t 1 \\\n-p $RUN_ID \\\n--checkpoints-dir /shared/home/namangoyal/checkpoints/175B/ \\\n--local-checkpoints-dir /mnt/scratch/$USER/checkpoints/2021-12-30.$RUN_ID \\\n--restore-file $RESTORE_FILE \\\n--full-cloud-upload-path $BLOB_URL\n2021-12-01 2:21am ET: [Stephen oncall] Run 12.37 [WARNING: See 2021-12-02\n17:16 ET: Debrief on why it may not be SGD]\nWe got much further this time (38414 updates) but did start hitting 0.25’s. Requeued.\n2021-11-30 7:24pm ET: [Stephen oncall] Run 12.37 Manual requeue of 12.36.\n[WARNING: See 2021-12-02 17:16 ET: Debrief on why it may not be SGD]\nHit 37725 updates and started getting 0.25’s. Couldn’t requeue bc previous run ignored downloading\ncheckpoints. Reverted that diff and relaunched unchanged.\n\n# use updated fairscale\ncd ~/src/fairscale\ngit fetch origin prefetch_fsdp_params_simple\ngit checkout prefetch_fsdp_params_simple\ncd ~/src/fairseq-py\n# changes for restoring without downloading, and also changes to loss scale logic\ngit fetch origin sgd_withdownload_gshard_combine_megatron_fsdp\ngit checkout sgd_withdownload_gshard_combine_megatron_fsdp\nBLOB_URL=\"<<<SCRUBBED FOR RELEASE>>>\"\nRUN_ID=175B_run12.37\nINCLUDED_HOSTS=node-[1,3-70,72-87,89-112,114-128] \\\npython -m fb_sweep.opt.sweep_opt_en_lm_175b \\\n-n 124 -g 8 -t 1 \\\n-p $RUN_ID \\\n--checkpoints-dir /shared/home/namangoyal/checkpoints/175B/ \\\n--local-checkpoints-dir /mnt/scratch/$USER/checkpoints/2021-11-29.$RUN_ID \\\n--full-cloud-upload-path $BLOB_URL\n2021-11-20 7:24pm ET: [Stephen oncall]\nSam called out the loss scalar hitting minimum. Took action to requeue the job\nA few minutes later, Susan pointed out that wouldn’t work because we reverted the download-from-cloud\nchange.\nSee next entry for mitigation.\n2021-11-30 10:10am PT: 12.36 restart from 37k, SGD mimicking [WARNING: See\n2021-12-02 17:16 ET: Debrief on why it may not be SGD]\n# use updated fairscale\ncd ~/src/fairscale\ngit fetch origin prefetch_fsdp_params_simple\ngit checkout prefetch_fsdp_params_simple\ncd ~/src/fairseq-py\ngit fetch origin sgd_gshard_combine_megatron_fsdp\ngit checkout sgd_gshard_combine_megatron_fsdp\nCKPT_DIR=/data/users/susanz/checkpoints\nBLOB_URL=\"<<<SCRUBBED FOR RELEASE>>>\"\ncd $CKPT_DIR\ncp --recursive --include-pattern \"checkpoint_13_37000*.pt\" \"$BLOB_URL\" checkpoint_13_37000\nRESTORE_FILE=$CKPT_DIR/checkpoint_13_37000/175B_run12.14.me_fp16.minscale0.25.fsdp.gpf32.0.relu.transformer_lm_me\ngatron.nlay96.emb12288.lrnpos.0emb_scale.bm_none.tps2048.gpt2.adam.b2_0.95.eps1e-08.cl1.0.lr0.00012.endlr6e-06.wu\n2000.dr0.1.atdr0.1.0emb_dr.wd0.1.ms8.uf1.mu143052.s1.ngpu992/checkpoint_13_37000.pt\nRUN_ID=175B_run12.36\n\nINCLUDED_HOSTS=node-[1,3-70,72-87,89-112,114-128] \\\npython -m fb_sweep.opt.sweep_opt_en_lm_175b \\\n-n 124 -g 8 -t 1 \\\n-p $RUN_ID \\\n--checkpoints-dir /shared/home/namangoyal/checkpoints/175B/ \\\n--restore-file $RESTORE_FILE \\\n--full-cloud-upload-path $BLOB_URL\n●\nWas just about to launch and saw all nodes to launch are were in drain:\n(fairseq-20210913) susanz@ip-0A1E0404:~/fairseq-py$ sinfo\nPARTITION AVAIL  TIMELIMIT  NODES  STATE NODELIST\nhpc*         up   infinite      1 drain~ node-129\nhpc*         up   infinite      2  down~ node-[130-131]\nhpc*         up   infinite      1  idle~ node-132\nhpc*         up   infinite    124  drain node-[1,3-70,72-87,89-112,114-128]\nhpc*         up   infinite      4   idle node-[2,71,88,113]\n●\nsudo scontrol update node=node-[1,3-70,72-87,89-112,114-128] state=idle\n2021-11-30 9:00am PT: 12.35 restart from 37k, SGD mimicking [WARNING: See\n2021-12-02 17:16 ET: Debrief on why it may not be SGD]\n●\nDownloaded checkpoints for 37k.\n●\nCheckpoints got borked / clobbered by “always download cloud checkpoints” logic. Redownloading and\nreverting that change.\n●\nResized /data up to 85TB (was almost full at 70TB).\nNote: This never actually ran\n2021-11-30 9:00am ET: 12.34 requeue\n[Stephen] Looks like it’s got an enormous number of 0.25 loss scales, basically all night long. Requeueing.\n2021-11-29 7:43pm PT [Susan]: 12.34 restart\n●\nRestarting with 6e4124680c960a8bc584f2fb0d4404a232745e8b pulled in\n\n# use updated fairscale\ncd ~/src/fairscale\ngit fetch origin prefetch_fsdp_params_simple\ngit checkout prefetch_fsdp_params_simple\n# we use a single Cloud blob path for all checkpoints now\nPERMANENT_CLOUD_UPLOAD_PATH=\"<<<SCRUBBED FOR RELEASE>>>\"\n# increment the run ID so we don’t clobber tensorboard\nRUN_ID=175B_run12.34\n# relaunch training and restore checkpoint from Cloud blob storage\nINCLUDED_HOSTS=node-[1,3-70,72-87,89-112,114-128] \\\npython -m fb_sweep.opt.sweep_opt_en_lm_175b \\\n-n 124 -g 8 -t 1 \\\n--checkpoints-dir /shared/home/namangoyal/checkpoints/175B/ \\\n--local-checkpoints-dir /mnt/scratch/$USER/checkpoints/2021-11-29.$RUN_ID \\\n--full-cloud-upload-path $PERMANENT_CLOUD_UPLOAD_PATH \\\n-p $RUN_ID\n2021-11-30 7:43pm PT [Susan]: 12.33 requeue\n●\nGot stuck for 510 iterations with gradient overflow / loss scale at 0.25\n●\nsudo scontrol requeue job=2229\n○\nTakes about 5 minutes to get nodelist out of (BeginTime) state\n●\nThis doesn’t look right:\n\n●\nSeems like 438 checkpoints downloaded while 554 didn’t. Unclear if this is intended behavior. Since the\njob still hasn’t started after over an hour, scanceling and starting from a new checkpoint dir with a new\nrun id.\n2021-11-28 6:34pm ET [Stephen]: 12.33\npg0-2 threw an illegal memory exception. Swapped for 90 and began to replace node node-2.\nSuccessfully 15 updates in at 7:04pm.\n# use updated fairscale\ncd ~/src/fairscale\ngit fetch origin prefetch_fsdp_params_simple\ngit checkout prefetch_fsdp_params_simple\n# we use a single Cloud blob path for all checkpoints now\nPERMANENT_CLOUD_UPLOAD_PATH=\"<<<SCRUBBED FOR RELEASE>>>\"\n# increment the run ID so we don’t clobber tensorboard\nRUN_ID=175B_run12.32\n# relaunch training and restore checkpoint from Cloud blob storage\nINCLUDED_HOSTS=node-[1,3-70,72-87,89-112,114-128] \\\npython -m fb_sweep.opt.sweep_opt_en_lm_175b \\\n-n 124 -g 8 -t 1 \\\n--checkpoints-dir /shared/home/namangoyal/checkpoints/175B/ \\\n--local-checkpoints-dir /mnt/scratch/$USER/checkpoints/2021-11-28.$RUN_ID \\\n--full-cloud-upload-path $PERMANENT_CLOUD_UPLOAD_PATH \\\n-p $RUN_ID\n\n2021-11-28 5:52pm ET [Stephen]: 12.32\n●\nLooks like we’ve hit the zero-message hang. It’s been almost 20 minutes without updates. We’re at\n33658, considerably further than we got before.\n●\nRan pdsh -w 'node-[1,3-87,89,91-112,114-128]' -R ssh\nnvidia-smi | vim -\n○\nObserved that pg0-71 gave an exit code 15 and pg0-118 gave an exit code 14.\n■\nFrom https://developer.download.nvidia.com/compute/DCGM/docs/nvidia-smi-367.38.pdf\n■\nReturn code 14 - infoROM is corrupted\n■\nReturn code 15 - The GPU has fallen off the bus or has otherwise become inaccessible\n○\nAll other hosts returned the expected 100% utilization from being blocked. We knew about\npg0-118 and infoROM. Investigating pg0-71\n●\nLspci shows the right number of devices\n●\nInvestigating pg0-71\n○\n[putting off running nvidia-smi in case it hangs]\n○\nHtop: Shows 24 threads pegged. (By comparison, pg0-70 shows 24 threads pegged)\n○\nstraced 2 child threads pegged and found them in sched_yield. Straced a parent process was\npegged in futex_wait.\n○\nGdb backtrace of a parent thread actually showed a rich backtrace (compared to the simple\n“stuck in cudart.so” I saw previously)\n■\nFirst non-OS Bt call I see:\n■\n0x00007f2efbed1d6b in torch::autograd::ReadyQueue::pop() () from\n/shared/home/roller/miniconda3/envs/fairseq-20210913/lib/python3.8/site-packages/torc\nh/lib/libtorch_cpu.so\n■\nPg0-70 shows itself stuck in the same place\n○\nRerunning pdsh nvidia-smi repeats pg0-71 as the problem node\n○\nFinally running nvidia-smi on pg0-71 gives “Unable to determine the device handle for GPU\n000E:00:00.0: GPU is lost.  Reboot the system to recover this GPU”\n●\nMitigations taken:\n○\nLaunching 12.32, replacing pg0-71 with pg0-2\n■\nThought about doing a scontrol hotswap, but because i have the --restore-file in my\narguments I decided against it, falling back to cloud checkpoints this time.\n○\n[in progress] Rebooting and then maybe replacing pg0-71\n# use updated fairscale\ncd ~/src/fairscale\ngit fetch origin prefetch_fsdp_params_simple\ngit checkout prefetch_fsdp_params_simple\n# we use a single Cloud blob path for all checkpoints now\nPERMANENT_CLOUD_UPLOAD_PATH=\"<<<SCRUBBED FOR RELEASE>>>\"\n# increment the run ID so we don’t clobber tensorboard\nRUN_ID=175B_run12.32\n# relaunch training and restore checkpoint from Cloud blob storage\nINCLUDED_HOSTS=node-[1-70,72-87,89,91-112,114-128] \\\npython -m fb_sweep.opt.sweep_opt_en_lm_175b \\\n-n 124 -g 8 -t 1 \\\n--checkpoints-dir /shared/home/namangoyal/checkpoints/175B/ \\\n--local-checkpoints-dir /mnt/scratch/$USER/checkpoints/2021-11-28.$RUN_ID \\\n--full-cloud-upload-path $PERMANENT_CLOUD_UPLOAD_PATH \\\n-p $RUN_ID\n\n2021-11-28 12:28pm ET [Stephen]: 12.31\nSee previous entry (below run block) for context.\n●\n12:42pm ET - looks like we’re tokenizing. ETA  based on 12.30 is 1:17pm.\n●\n1:15pm ET - Pg0-90 doesn’t look like ssh is recovering from a reboot. Successfully replaced and\nmarked as approved for usage\n●\n1:21pm ET - job is making updates\n●\n1:38pm ET - at 50 updates!!! I’m going for lunch.\n●\n14:52 ET - got past 250 updates. New checkpoint saved. Life is good.\n# use updated fairscale\ncd ~/src/fairscale\ngit fetch origin prefetch_fsdp_params_simple\ngit checkout prefetch_fsdp_params_simple\n# we use a single Cloud blob path for all checkpoints now\nPERMANENT_CLOUD_UPLOAD_PATH=\"<<<SCRUBBED FOR RELEASE>>>\"\nCKPT_DIR=/data/users/roller/175B_run12.27_restore/checkpoint_11_33416/175B_run12.14.me_fp16.minscale0.25.fsdp.gpf\n32.0.relu.transformer_lm_megatron.nlay96.emb12288.lrnpos.0emb_scale.bm_none.tps2048.gpt2.adam.b2_0.95.eps1e-08.cl\n1.0.lr0.00012.endlr6e-06.wu2000.dr0.1.atdr0.1.0emb_dr.wd0.1.ms8.uf1.mu143052.s1.ngpu992\nRESTORE_FILE=$CKPT_DIR/checkpoint_11_33000.pt\n# increment the run ID so we don’t clobber tensorboard\nRUN_ID=175B_run12.31\n# relaunch training and restore checkpoint from Cloud blob storage\nINCLUDED_HOSTS=node-[1,3-87,89,91-112,114-128] \\\npython -m fb_sweep.opt.sweep_opt_en_lm_175b \\\n-n 124 -g 8 -t 1 \\\n--checkpoints-dir /shared/home/namangoyal/checkpoints/175B/ \\\n--local-checkpoints-dir /mnt/scratch/$USER/checkpoints/2021-11-28.$RUN_ID \\\n--full-cloud-upload-path $PERMANENT_CLOUD_UPLOAD_PATH \\\n--restore-file $RESTORE_FILE \\\n-p $RUN_ID\n2021-11-28 10:09am ET [Stephen]: 12.30\n●\n12.29 failed with the same `filename storages not found`\n○\nSince the exception said pg0-55, i ssh’d into it and tried manually loading its checkpoints. All 6\nparts got the same storages exception!\n○\nI could replicate this with the storages I had manually downloaded\n○\nConclusion: 33250 checkpoints are corrupt. Maybe from R12.26 and R12.25 aggressively\noverwriting the checkpoints.\n●\nRemedies taken (status: monitoring for success)\n○\nAdd a quick patch to avoid the constant-rewrite bug witnessed. Testing in prod.\n■\nNote I now have local backups of 33000 and 33416.x\n○\nSince I don’t like that 33250 is corrupted (I would rather us not have any corrupt checkpoints), I\nam rolling back to 33000.\n○\nThis is frustrating since we have 33416 which is the epoch boundary, but let’s just bite the bullet\n\n# use updated fairscale\ncd ~/src/fairscale\ngit fetch origin prefetch_fsdp_params_simple\ngit checkout prefetch_fsdp_params_simple\n# we use a single Cloud blob path for all checkpoints now\nPERMANENT_CLOUD_UPLOAD_PATH=\"<<<SCRUBBED FOR RELEASE>>>\"\nCKPT_DIR=/data/users/roller/175B_run12.27_restore/checkpoint_11_33416/175B_run12.14.me_fp16.minscale0.25.fsdp.gpf\n32.0.relu.transformer_lm_megatron.nlay96.emb12288.lrnpos.0emb_scale.bm_none.tps2048.gpt2.adam.b2_0.95.eps1e-08.cl\n1.0.lr0.00012.endlr6e-06.wu2000.dr0.1.atdr0.1.0emb_dr.wd0.1.ms8.uf1.mu143052.s1.ngpu992\nRESTORE_FILE=$CKPT_DIR/checkpoint_11_33000.pt\n# increment the run ID so we don’t clobber tensorboard\nRUN_ID=175B_run12.30\n# relaunch training and restore checkpoint from Cloud blob storage\nINCLUDED_HOSTS=node-[1,3-87,89-112,114-117,119-128] \\\npython -m fb_sweep.opt.sweep_opt_en_lm_175b \\\n-n 124 -g 8 -t 1 \\\n--checkpoints-dir /shared/home/namangoyal/checkpoints/175B/ \\\n--local-checkpoints-dir /mnt/scratch/$USER/checkpoints/2021-11-28.$RUN_ID \\\n--full-cloud-upload-path $PERMANENT_CLOUD_UPLOAD_PATH \\\n--restore-file $RESTORE_FILE \\\n-p $RUN_ID\nLaunched at 11:00am ET\n●\n11:23am - nodes just finished loading checkpoints\n●\n11:57am - still tokenizing, I think. We need to add a log line for after a checkpoint was successfully\nloaded and when the iterator is successfully fast forwarded\n●\n12:03am - looks like gpus are finally burning electricity, and we have our first step\n○\nLooks like the hot patch for checkpointing didn’t get triggered here: loss scale didn’t need to be\nlowered on the first step\n○\nWPS looks healthy for now. Monitoring for hangs\n●\n12:08pm Started replacing pg0-2\n●\n12:18pm Looks like we’ve hung again. No error message. Made it 26 updates.\nWhile it was still running, tried pdsh nvidia-smi. Found nvidia-smi was hanging in node-90 when I eventually\nctrl-c’d nvidia-smi. SSH’ing in and running nvidia-smi seemed to also hang.\nSwapping for pg0-118, which has the inforam message but that was only a guess. Also attempting to reboot\npg0-90\n2021-11-28 9:41am ET [Stephen]: 12.29\n●\nWow this time we didn’t even really get past init. On pg0-2: RuntimeError: CUDA error: an illegal\nmemory access was encountered\n●\nNote this is the same machine that gave me issues before\n●\nNvidia-smi shows no issue\nSince 40 managed to successfully re-init overnight, swapping 2 for 40.\n\n# use updated fairscale\ncd ~/src/fairscale\ngit fetch origin prefetch_fsdp_params_simple\ngit checkout prefetch_fsdp_params_simple\n# we use a single Cloud blob path for all checkpoints now\nPERMANENT_CLOUD_UPLOAD_PATH=\"<<<SCRUBBED FOR RELEASE>>>\"\n# increment the run ID so we don’t clobber tensorboard\n# note we have the SECOND tensorboard clobbering fix, so we can requeue after this\nRUN_ID=175B_run12.29\n# relaunch training and restore checkpoint from Cloud blob storage\nINCLUDED_HOSTS=node-[1,3-87,89-112,114-117,119-128] \\\npython -m fb_sweep.opt.sweep_opt_en_lm_175b \\\n-n 124 -g 8 -t 1 \\\n--checkpoints-dir /shared/home/namangoyal/checkpoints/175B/ \\\n--local-checkpoints-dir /mnt/scratch/$USER/checkpoints/2021-11-28.0 \\\n--full-cloud-upload-path $PERMANENT_CLOUD_UPLOAD_PATH \\\n-p $RUN_ID\n2021-11-28 3:20am ET [Stephen]: 12.28\nLast ditch resort. Doing the exact same as 27 except bumping the storage directory version.\nCheckpoint_last is still downloading. See 12.27 entry for more info\n# use updated fairscale\ncd ~/src/fairscale\ngit fetch origin prefetch_fsdp_params_simple\ngit checkout prefetch_fsdp_params_simple\n# we use a single Cloud blob path for all checkpoints now\nPERMANENT_CLOUD_UPLOAD_PATH=\"<<<SCRUBBED FOR RELEASE>>>\"\n# increment the run ID so we don’t clobber tensorboard\n# note we have the SECOND tensorboard clobbering fix, so we can requeue after this\nRUN_ID=175B_run12.28\n# relaunch training and restore checkpoint from Cloud blob storage\nINCLUDED_HOSTS=node-[1-39,41-87,89-112,114-117,119-128] \\\npython -m fb_sweep.opt.sweep_opt_en_lm_175b \\\n-n 124 -g 8 -t 1 \\\n--checkpoints-dir /shared/home/namangoyal/checkpoints/175B/ \\\n--local-checkpoints-dir /mnt/scratch/$USER/checkpoints/2021-11-27.3 \\\n--full-cloud-upload-path $PERMANENT_CLOUD_UPLOAD_PATH \\\n-p $RUN_ID\n2021-11-28 1:50am ET [Stephen]: 12.27\nLooks like 26 tried to immediately upload a checkpoint and failed its cp commands! Then it took another step,\nlowered its scalar, and tried uploading again! And again! The humanity! We’re already at loss scale 0.25.\nAfter it did this 3 times, it looks like it’s hanging again. Some CPUs look pegged but using strace on the\nprocesses displays only sched_yield, so it’s probably just an idle loop.\n\nGiven this is our third hang in a row, somewhere must have something wrong. Leaving the job up to check on\nit.\nSsh’d into pg0-39 and ran “gdb -p <pid>” on one of the fairseq processes. Confirmed we were hanging in\n0x00007ff8d76eecb1 in ?? () from /usr/lib/x86_64-linux-gnu/libcuda.so.1\nEither we have a bug in our code causing workers to be out of sync, or there’s a bad node somewhere.\nInitiated a cluster wide nccl test, it came back a bit slow(161 but using all nodes) but it finished.\nGlobal run of nvidia-smi didn’t find any uncorrectable errors, but pg-88 did show 506k correctable ones; and\npg-23 had 170k correctable errors. Other nodes showed at most a couple thousand. Decided to launch again\nreplacing 88 with 7.\n# use updated fairscale\ncd ~/src/fairscale\ngit fetch origin prefetch_fsdp_params_simple\ngit checkout prefetch_fsdp_params_simple\n# we use a single Cloud blob path for all checkpoints now\nPERMANENT_CLOUD_UPLOAD_PATH=\"<<<SCRUBBED FOR RELEASE>>>\"\n# increment the run ID so we don’t clobber tensorboard\n# note we have the SECOND tensorboard clobbering fix, so we can requeue after this\nRUN_ID=175B_run12.27\n# relaunch training and restore checkpoint from Cloud blob storage\nINCLUDED_HOSTS=node-[1-39,41-87,89-112,114-117,119-128] \\\npython -m fb_sweep.opt.sweep_opt_en_lm_175b \\\n-n 124 -g 8 -t 1 \\\n--checkpoints-dir /shared/home/namangoyal/checkpoints/175B/ \\\n--local-checkpoints-dir /mnt/scratch/$USER/checkpoints/2021-11-27.2 \\\n--full-cloud-upload-path $PERMANENT_CLOUD_UPLOAD_PATH \\\n-p $RUN_ID\nThis one failed pretty fast with “KeyError: \"filename 'storages' not found\"” inside tarfile.py, suggesting that\nsomething got corrupted.\n-\nHypothesis: some node is hanging on uploading checkpoints?\n-\nThat doesn’t make sense with Susan’s hang some 8 steps in.\nObserved 2 nodes (7 and 17) had checkpoint downloads that were the wrong file size.\nIn parallel, downloaded all our latest checkpoints (including checkpoint_last) and putting them in\n/data/users/roller/175B_run12.27_restore/checkpoint_11_33416/175B_run12.14.me_fp16.minscale0.25.fsdp.gpf32.0.relu.transformer_lm_megatron.nl\nay96.emb12288.lrnpos.0emb_scale.bm_none.tps2048.gpt2.adam.b2_0.95.eps1e-08.cl1.0.lr0.00012.endlr6e-06.wu2000.dr0.1.atdr0.1.0emb_dr.wd0.1.ms8\n.uf1.mu143052.s1.ngpu992\nA good next move might be to roll back to 33000 (which is downloaded) or to checkpoint_last (33461).\n2021-11-27 11:39 ET [Stephen]: Run 12.26\n●\nLooks like things are hanging, debugging\n●\nStephen: fixing the TB clobber overgenerate thing\n●\nnvidia-smi on:\n○\nnode-40: WARNING: infoROM is corrupted at gpu 000B:00:00.0\n\n■\n9:07pm PT: rebooting; 9:11pm alive again\n■\ninforam message still appears!\n■\nCurrently replacing\n○\nnode-113: WARNING: infoROM is corrupted at gpu 0002:00:00.0\n■\n9:08pm PT: rebooting; 9:13pm alive again\n■\ninforam message still appears!\n○\nhttps://forums.developer.nvidia.com/t/inforom-is-corrupted-at-gpu/74277\n■\n“There is no publicly available utility to fix this. The card is damaged. Unless it is under\nwarranty, there isn’t anything you can do to repair it.”\n●\nRe: possible replacement nodes\n○\nnode-118 seems to have MIG enabled; Myle has noticed this in the past and we should add\nchecks for this to our automation scripts. The fix is to do `sudo nvidia-smi -mig 0` and then\nrestart.\n■\n9pm PT: ran `sudo nvidia-smi -mig 0` and rebooted node-118\n■\n9:07pm PT: node-118 came back without MIG\n■\n9:10pm PT: ran NCCL tests on node-[3,118], but got only 144GB/s instead of 189GB/s!\n■\nConclusion: node-118 has bad IB\n●\nCurrently replacing\n○\nnode-65 seems to be in a weird state. NCCL tests on node-[12,65] showed 65 with an error:\n`Test CUDA failure common.cu:762 'all CUDA-capable devices are busy or unavailable'`\n■\n9:06pm PT: rebooted node-65 (and also node-12 just because)\n■\n9:15pm PT: NCCL tests on node-[3,65] came back good (189GB/s)\n■\nConclusion: node-65 is healthy\n○\nnode-12: rebooted as part of debug process above\n■\n9:13pm PT: NCCL tests on node-[3,12] came back good (189GB/s)\n■\nConclusion: node-12 is healthy\n●\nWhoa, while fixing the tboard bug, ran test case on node-2 and got:\n○\nRuntimeError: CUDA error: misaligned address\n○\nO_o -- resolution: pray this is nothing\nOther weird thing noticed in the logs: It seems like we immediately dumped a checkpoint before moving any\niterations… 12.24 didn’t do this, neither did 12.23!\nNote: 12.26 seems to be doing this too, we may wish to roll back to 33000. Also note the last epoch was\nat 33416! We’re already rolling back!\n2021-11-28 02:35:19 | WARNING | fairseq.cloud_utils | [rank 830] done with cloud download in 788.0 Seconds\n2021-11-28 03:00:18 | INFO | fairseq.trainer | begin training epoch 11\n2021-11-28 03:00:18 | INFO | fairseq_cli.train | Start iterating over samples\n2021-11-28 03:12:26 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 2.0\n2021-11-28 03:12:26 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 11 @ 33250 updates\n2021-11-28 03:12:26 | INFO | fairseq.trainer | Saving checkpoint to\n/mnt/scratch/susanz/checkpoints/2021-11-27/175B_run12.25.me_fp16.minscale0.25.fsdp.gpf32.0.relu.transformer_lm_megatron.nlay96.emb12288.l\nrnpos.0emb_scal\n2021-11-28 03:12:27 | INFO | fairseq.trainer | Finished saving checkpoint to\n/mnt/scratch/susanz/checkpoints/2021-11-27/175B_run12.25.me_fp16.minscale0.25.fsdp.gpf32.0.relu.transformer_lm_megatron.nlay96.emb12288.l\nrnpos.\n2021-11-28 03:12:27 | INFO | fairseq.checkpoint_utils | Saved checkpoint\n/mnt/scratch/susanz/checkpoints/2021-11-27/175B_run12.25.me_fp16.minscale0.25.fsdp.gpf32.0.relu.transformer_lm_megatron.nlay96.emb12288.l\nrnpos.0emb\n2021-11-28 03:12:31 | INFO | fairseq_cli.train | preparing to copy\n/mnt/scratch/susanz/checkpoints/2021-11-27/175B_run12.25.me_fp16.minscale0.25.fsdp.gpf32.0.relu.transformer_lm_megatron.nlay96.emb12288.l\nrnpos.0emb_sca\n2021-11-28 03:12:50 | INFO | train_inner | {\"epoch\": 11, \"gnorm__fsdp_wrapped…[truncated]\n\nNew Launch\n# use updated fairscale\ncd ~/src/fairscale\ngit fetch origin prefetch_fsdp_params_simple\ngit checkout prefetch_fsdp_params_simple\n# we use a single Cloud blob path for all checkpoints now\nPERMANENT_CLOUD_UPLOAD_PATH=\"<<<SCRUBBED FOR RELEASE>>>\"\n# increment the run ID so we don’t clobber tensorboard\n# note we have the SECOND tensorboard clobbering fix, so we can requeue after this\nRUN_ID=175B_run12.26\n# relaunch training and restore checkpoint from Cloud blob storage\nINCLUDED_HOSTS=node-[1-6,8-39,41-112,114-117,119-128] \\\npython -m fb_sweep.opt.sweep_opt_en_lm_175b \\\n-n 124 -g 8 -t 1 \\\n--checkpoints-dir /shared/home/namangoyal/checkpoints/175B/ \\\n--local-checkpoints-dir /mnt/scratch/$USER/checkpoints/2021-11-27 \\\n--full-cloud-upload-path $PERMANENT_CLOUD_UPLOAD_PATH \\\n-p $RUN_ID\n2021-11-27 6:10pm PT: Run 12.25 [Susan restart]\n●\nNoticed job stuck for ~4 hours.\n●\nTrying sudo scontrol requeue job=2194 to see if things can restart smoothly.\n○\nscanceling and relaunching with new id instead. We’re out of tb directories.\n# use updated fairscale\ncd ~/src/fairscale\ngit fetch origin prefetch_fsdp_params_simple\ngit checkout prefetch_fsdp_params_simple\n# we use a single Cloud blob path for all checkpoints now\nPERMANENT_CLOUD_UPLOAD_PATH=\"<<<SCRUBBED FOR RELEASE>>>\"\n# increment the run ID so we don’t clobber tensorboard\nRUN_ID=175B_run12.25\n# relaunch training and restore checkpoint from Cloud blob storage\nINCLUDED_HOSTS=node-[1-6,8-11,13-64,66-117,119-128] \\\npython -m fb_sweep.opt.sweep_opt_en_lm_175b \\\n-n 124 -g 8 -t 1 \\\n--checkpoints-dir /shared/home/namangoyal/checkpoints/175B/ \\\n--local-checkpoints-dir /mnt/scratch/$USER/checkpoints/2021-11-27 \\\n--full-cloud-upload-path $PERMANENT_CLOUD_UPLOAD_PATH \\\n-p $RUN_ID\n●\nStuck again - after 8 updates.\n2021-11-27 10:59am PT: Run 12.24 [Myle rerunning job, but AFK rest of day]\n●\n...tbZ already exists. Ran out of possible suffixes.\n●\nRelaunching with new RUN_ID\n\n# use updated fairscale\ncd ~/src/fairscale\ngit fetch origin prefetch_fsdp_params_simple\ngit checkout prefetch_fsdp_params_simple\n# we use a single Cloud blob path for all checkpoints now\nPERMANENT_CLOUD_UPLOAD_PATH=\"<<<SCRUBBED FOR RELEASE>>>\"\n# increment the run ID so we don’t clobber tensorboard\nRUN_ID=175B_run12.24\n# relaunch training and restore checkpoint from Cloud blob storage\nINCLUDED_HOSTS=node-[1-6,8-11,13-64,66-117,119-128] \\\npython -m fb_sweep.opt.sweep_opt_en_lm_175b \\\n-n 124 -g 8 -t 1 \\\n--checkpoints-dir /shared/home/namangoyal/checkpoints/175B/ \\\n--local-checkpoints-dir /mnt/scratch/$USER/checkpoints/2021-11-2 \\\n--full-cloud-upload-path $PERMANENT_CLOUD_UPLOAD_PATH \\\n-p $RUN_ID\n2021-11-26 9:47am ET [Stephen managing cluster]\n●\nManaged to get new nodes for pg0-7, pg0-85\n○\nBoth are showing clean on the ECC uncorrectables\n○\nBoth have been marked idle and safe to use\n○\nI didn’t touch pg0-88 but it also looks safe to use (maybe Susan did this one)\n●\nSuccessfully updated hosts mapping and initialized the scratch directories\n○\nNOTE: Added these two lines to my ~/.ssh/config\n○\nHost node-*\n○\nStrictHostKeyChecking no\n●\npg0-118 is looking a bit funny.\n○\nSome of its ECC’s logs say\n■\nSRAM Uncorrectable            : N/A\n■\nDRAM Uncorrectable            : N/A\n○\nOthers only show 0’s, not N/A’s\n○\nRemedy: Draining it, rebooting it\n○\nAfter several minutes, host did not seem to recover from reboot\n■\nStarted to re-alloc\n■\nAnd then it came right back! Omg. while it was in the process of terminating\n■\nMaybe I should’ve been slightly more patient :(. Maybe I needed to set it to “drain”\ninstead of “drain*”\n■\nFortunately the release and reclaim was fast\n●\nCurrently running reallocs:\n○\nPg0-129 (Stuck on acquiring)\n○\nPg0-118 (Current “creating vm” 10:18am)\n●\nNow getting on a plane. Will pray for success.\nSome observations:\n●\nIt seems like there might be a near 1:1 mapping between our hosts and what csp gives us. For\nexample, if we already have a node and we terminate it, then restart it, then we seem to get an\n\nallocation immediately. However, nodes like 129--132 are simply never allocated (even though 118 was\nmade available while that was queuing!)\n2021-11-25 8:53am ET [Susan]: Run 12.23\nRun 22.22 got stuck after IB issues cropped up and everyone came to rescue it at the exact same time:\nmlx5: buo1u00003A: got completion with error:\n00000000 00000000 00000000 00000000\n00000000 00000000 00000000 00000000\n0000000d 00000000 00000000 00000000\n00000000 02005104 08001219 38dcf5d2\nLost node-118 GPU (automatically put on drain after job was scanceled):\nUnable to determine the device handle for GPU 0001:00:00.0: GPU is lost.  Reboot the system to\nrecover this GPU\nDRAM Uncorrectable on node-7:\nGPU 0000000D:00:00.0\nEcc Mode\nCurrent                           : Enabled\nPending                           : Enabled\nECC Errors\nVolatile\nSRAM Correctable              : 0\nSRAM Uncorrectable            : 0\nDRAM Correctable              : 0\nDRAM Uncorrectable            : 0\nAggregate\nSRAM Correctable              : 0\nSRAM Uncorrectable            : 0\nDRAM Correctable              : 10\nDRAM Uncorrectable            : 101\n●\nsudo scontrol update node=node-7 state=drain reason=dram\nCurrent list of node states:\n$ sinfo\nPARTITION AVAIL  TIMELIMIT  NODES  STATE NODELIST\nhpc*         up   infinite      2  down~ node-[129-130]\nhpc*         up   infinite      2  idle~ node-[131-132]\nhpc*         up   infinite      3  drain node-[7,85,118]\nhpc*         up   infinite    125   idle node-[1-6,8-84,86-117,119-128]\nHigh amount of DRAM correctable errors on node-88 (506807), excluding to be safe.\n\n# use updated fairscale\ncd ~/src/fairscale\ngit fetch origin prefetch_fsdp_params_simple\ngit checkout prefetch_fsdp_params_simple\n# we use a single Cloud blob path for all checkpoints now\nPERMANENT_CLOUD_UPLOAD_PATH=\"<<<SCRUBBED FOR RELEASE>>>\"\n# increment the run ID so we don’t clobber tensorboard\nRUN_ID=175B_run12.23\n# relaunch training and restore checkpoint from Cloud blob storage\nINCLUDED_HOSTS=node-[1-6,8-84,86-87,89-117,119-128] \\\npython -m fb_sweep.opt.sweep_opt_en_lm_175b \\\n-n 124 -g 8 -t 1 \\\n--checkpoints-dir /shared/home/namangoyal/checkpoints/175B/ \\\n--local-checkpoints-dir /mnt/scratch/$USER/checkpoints/2021-11-25 \\\n--full-cloud-upload-path $PERMANENT_CLOUD_UPLOAD_PATH \\\n-p $RUN_ID\n●\nNew log dir:\n/shared/home/namangoyal/checkpoints/175B/175B_run12.23.me_fp16.minscale0.25.fsdp.gpf32.0.relu.t\nransformer_lm_megatron.nlay96.emb12288.lrnpos.0emb_scale.bm_none.tps2048.gpt2.adam.b2_0.95.\neps1e-08.cl0.3.lr0.00012.endlr6e-06.wu2000.dr0.1.atdr0.1.0emb_dr.wd0.1.ms8.uf1.mu143052.s1.ngpu\n992\n2021-11-25 11:35am ET [Myle]: Run 12.22\n●\nNvm, realized this won’t resume properly due to cache, and will clobber tensorboard; will increment run\nID instead\n●\nAdd 3rd party timeout to handle blob retries\n○\n#2686\n○\ntimeout-decorator is a new dependency\n■\npip install timeout-decorator\n●\nRe: spare nodes:\n○\nRan ./scripts/nccl_tests/cloud/run_nccl_allreduce.sh to validate NCCL perf\n○\nConfirmed node-[1,7,47] are good\n○\nDrained ​node-85, which had bad NCCL perf (only 140GB/s instead of 180)\n# use updated fairscale\ncd ~/src/fairscale\ngit fetch origin prefetch_fsdp_params_simple\ngit checkout prefetch_fsdp_params_simple\n# we use a single Cloud blob path for all checkpoints now\nPERMANENT_CLOUD_UPLOAD_PATH=\"<<<SCRUBBED FOR RELEASE>>>\"\n# increment the run ID so we don’t clobber tensorboard\nRUN_ID=175B_run12.22\n# relaunch training and restore checkpoint from Cloud blob storage\nINCLUDED_HOSTS=node-[1,3-42,44-57,59-84,86-128] \\\npython -m fb_sweep.opt.sweep_opt_en_lm_175b \\\n-n 124 -g 8 -t 1 \\\n\n--checkpoints-dir /shared/home/namangoyal/checkpoints/175B/ \\\n--local-checkpoints-dir /mnt/scratch/$USER/checkpoints/2021-11-25 \\\n--full-cloud-upload-path $PERMANENT_CLOUD_UPLOAD_PATH \\\n-p $RUN_ID\n2021-11-25 11:20am ET: Run 12.21 (requeue)\n●\nTraining got stuck in loss scale 0.25 loop, after step 28628\n●\nDid scontrol requeue job=2167 to see if it magically fixes after restarting from checkpoint\n2021-11-24 11:18pm ET [Susan]: Run 12.21\n●\nTesting out #2681\n●\nRemove node 58, replacing with 7.\n○\nNode 58 came up with GPUs! But, there’s this:\nDRAM Correctable              : 18446744073709551615\n○\nsudo scontrol update node=node-58 state=drain reason=dramtoast\n# use updated fairscale\ncd ~/src/fairscale\ngit fetch origin prefetch_fsdp_params_simple\ngit checkout prefetch_fsdp_params_simple\n# we use a single Cloud blob path for all checkpoints now\nPERMANENT_CLOUD_UPLOAD_PATH=\"<<<SCRUBBED FOR RELEASE>>>\"\n# increment the run ID so we don’t clobber tensorboard\nRUN_ID=175B_run12.21\n# relaunch training and restore checkpoint from Cloud blob storage\nINCLUDED_HOSTS=node-[1,3-42,44-57,59-84,86-128] \\\npython -m fb_sweep.opt.sweep_opt_en_lm_175b \\\n-n 124 -g 8 -t 1 \\\n--checkpoints-dir /shared/home/namangoyal/checkpoints/175B/ \\\n--local-checkpoints-dir /mnt/scratch/susanz/checkpoints/2021-11-24.3 \\\n--full-cloud-upload-path $PERMANENT_CLOUD_UPLOAD_PATH \\\n-p $RUN_ID\n2021-11-24 10:40pm ET [Susan]: Run 12.20\n●\nTesting out #2681\n\n# use updated fairscale\ncd ~/src/fairscale\ngit fetch origin prefetch_fsdp_params_simple\ngit checkout prefetch_fsdp_params_simple\n# we use a single Cloud blob path for all checkpoints now\nPERMANENT_CLOUD_UPLOAD_PATH=\"<<<SCRUBBED FOR RELEASE>>>\"\n# increment the run ID so we don’t clobber tensorboard\nRUN_ID=175B_run12.20\n# relaunch training and restore checkpoint from Cloud blob storage\nINCLUDED_HOSTS=node-[1,3-6,8-42,44-84,86-128] \\\npython -m fb_sweep.opt.sweep_opt_en_lm_175b \\\n-n 124 -g 8 -t 1 \\\n--checkpoints-dir /shared/home/namangoyal/checkpoints/175B/ \\\n--local-checkpoints-dir /mnt/scratch/susanz/checkpoints/2021-11-24.3 \\\n--full-cloud-upload-path $PERMANENT_CLOUD_UPLOAD_PATH \\\n-p $RUN_ID\n●\nLost GPU on node-58:\n(base) susanz@buo1u00001P:~$ nvidia-smi\nUnable to determine the device handle for GPU 0001:00:00.0: GPU is lost.  Reboot the system to\nrecover this GPU\n●\nsudo scontrol update node=node-58 state=drain reason=lostgpu\n2021-11-24 3:30pm ET [Susan]: Run 12.19\n●\nLaunching with node 85. Bringing in 47 instead.\n# use updated fairscale\ncd ~/src/fairscale\ngit fetch origin prefetch_fsdp_params_simple\ngit checkout prefetch_fsdp_params_simple\n# we use a single Cloud blob path for all checkpoints now\nPERMANENT_CLOUD_UPLOAD_PATH=\"<<<SCRUBBED FOR RELEASE>>>\"\n# increment the run ID so we don’t clobber tensorboard\nRUN_ID=175B_run12.19\n# relaunch training and restore checkpoint from Cloud blob storage\nINCLUDED_HOSTS=node-[1,3-6,8-42,44-84,86-128] \\\npython -m fb_sweep.opt.sweep_opt_en_lm_175b \\\n-n 124 -g 8 -t 1 \\\n--checkpoints-dir /shared/home/namangoyal/checkpoints/175B/ \\\n--local-checkpoints-dir /mnt/scratch/susanz/checkpoints/2021-11-24.2 \\\n--full-cloud-upload-path $PERMANENT_CLOUD_UPLOAD_PATH \\\n-p $RUN_ID\n●\nHung for 5 hours - missing 4 shards.\n\n2021-11-24 2:10pm ET [Susan]: Run 12.18\n●\nRelaunched exactly the same as 12.17 to see if the hanging was just data loading fast forwarding.\n●\nIt’s actually something else?\n○\nSomething is taking half an hour before “begin training epoch 9”\n○\nWe don’t seem to print all ranks for cloud download.\n# use updated fairscale\ncd ~/src/fairscale\ngit fetch origin prefetch_fsdp_params_simple\ngit checkout prefetch_fsdp_params_simple\n# we use a single Cloud blob path for all checkpoints now\nPERMANENT_CLOUD_UPLOAD_PATH=\"<<<SCRUBBED FOR RELEASE>>>\"\n# increment the run ID so we don’t clobber tensorboard\nRUN_ID=175B_run12.18\n# relaunch training and restore checkpoint from Cloud blob storage\nINCLUDED_HOSTS=node-[1,3-6,8-42,44-46,48-128] \\\npython -m fb_sweep.opt.sweep_opt_en_lm_175b \\\n-n 124 -g 8 -t 1 \\\n--checkpoints-dir /shared/home/namangoyal/checkpoints/175B/ \\\n--local-checkpoints-dir /mnt/scratch/susanz/checkpoints/2021-11-24.1 \\\n--full-cloud-upload-path $PERMANENT_CLOUD_UPLOAD_PATH \\\n-p $RUN_ID\n2021-11-24 1:00pm ET [Susan]: Run 12.17\n●\nRun 12.16 got stuck at step 27247 after hitting ECC error.\n●\nWCOLL=~myleott/hosts PDSH_RCMD_TYPE=ssh pdsh nvidia-smi -q -d \"ECC\" > ecc_error.log\n●\nShows 2 and 43 with ECC errors. Restarting in cloud UI.\n○\nTaking a long time to provision for some reason.\n○\n2 failed to provision with:\n“Error: ProvisioningState/failed/OSProvisioningTimedOut OS Provisioning for VM\nnv6ormrlczgkx_245' did not finish in the allotted time. The VM may still finish provisioning\nsuccessfully. Please check provisioning state later.”\nShutting down and restarting got:\n“Reimaging virtual machine due to error on creation”\n●\nAlso restarting 85 since VM failed to start for some reason\n\n●\n85 came back. Keep 7,47 removed in case they’re the bad IB nodes. Taking a gamble on 1.\n●\n43 came back as well.\n# use updated fairscale\ncd ~/src/fairscale\ngit fetch origin prefetch_fsdp_params_simple\ngit checkout prefetch_fsdp_params_simple\n# we use a single Cloud blob path for all checkpoints now\nPERMANENT_CLOUD_UPLOAD_PATH=\"<<<SCRUBBED FOR RELEASE>>>\"\n# increment the run ID so we don’t clobber tensorboard\nRUN_ID=175B_run12.17\n# relaunch training and restore checkpoint from Cloud blob storage\nINCLUDED_HOSTS=node-[1,3-6,8-42,44-46,48-128] \\\npython -m fb_sweep.opt.sweep_opt_en_lm_175b \\\n-n 124 -g 8 -t 1 \\\n--checkpoints-dir /shared/home/namangoyal/checkpoints/175B/ \\\n--local-checkpoints-dir /mnt/scratch/susanz/checkpoints/2021-11-24.1 \\\n--full-cloud-upload-path $PERMANENT_CLOUD_UPLOAD_PATH \\\n-p $RUN_ID\n●\nNotes:\n○\nUpdate local-checkpoints-dir! Permissions issue if using any other username there but your\nown.\n○\nConfirmed right checkpoint loaded (should see this by ~6 minutes in):\n2021-11-24 18:19:14 | INFO | fairseq.trainer | Loaded checkpoint\n/mnt/scratch/susanz/checkpoints/2021-11-24.1/175B_run12.17.me_fp16.minscale0.25.fsdp.gpf3\n2.0.relu.transformer_lm_megatron.nlay96.emb12288.lrnpos.0emb_scale.bm_none.tps2048.gpt2\n.adam.b2_0.95.eps1e-08.cl0.3.lr0.00012.endlr6e-06.wu2000.dr0.1.atdr0.1.0emb_dr.wd0.1.ms8.\nuf1.mu143052.s1.ngpu992/checkpoint_last-model_part-0-shard0.pt (epoch 9 @ 27000 updates)\n○\nGot stuck at 988 log lines of cloud downloads, even though 992 shards exist on machines.\n○\nWCOLL=~myleott/hosts PDSH_RCMD_TYPE=ssh pdsh ls -la\n/mnt/scratch/susanz/checkpoints/2021-11-24.1/175B_run12.17.me_fp16.minscale0.25.fsdp.g\npf32.0.relu.transformer_lm_megatron.nlay96.emb12288.lrnpos.0emb_scale.bm_none.tps2048.\ngpt2.adam.b2_0.95.eps1e-08.cl0.3.lr0.00012.endlr6e-06.wu2000.dr0.1.atdr0.1.0emb_dr.wd0\n.1.ms8.uf1.mu143052.s1.ngpu992 > copied_files.log\n○\n(fairseq-20210913) susanz@ip-0A1E0404:~/fairseq-py$ grep shard copied_files.log | wc\n-l\n992\n\n2021-11-23 10:50am [Myle]: Run 12.16\n●\nDecided to reduce clipping to 0.3 and relaunch from 24.5K checkpoint\n○\nIf this fails, we will try resetting adam stats and do a fresh warmup.\n●\nUncovered a hang when restoring from Cloud blob; fix in PR #2673\n●\nNew train log:\n/shared/home/namangoyal/checkpoints/175B/175B_run12.16.me_fp16.minscale0.25.fsdp.gpf32.0.relu.t\nransformer_lm_megatron.nlay96.emb12288.lrnpos.0emb_scale.bm_none.tps2048.gpt2.adam.b2_0.95.\neps1e-08.cl0.3.lr0.00012.endlr6e-06.wu2000.dr0.1.atdr0.1.0emb_dr.wd0.1.ms8.uf1.mu143052.s1.ngpu\n992/train.log\n# use updated fairscale\ncd ~/src/fairscale\ngit fetch origin prefetch_fsdp_params_simple\ngit checkout prefetch_fsdp_params_simple\n# we use a single Cloud blob path for all checkpoints now\nPERMANENT_CLOUD_UPLOAD_PATH=\"<<<SCRUBBED FOR RELEASE>>>\"\n# increment the run ID so we don’t clobber tensorboard\nRUN_ID=175B_run12.16\n# relaunch training and restore checkpoint from Cloud blob storage\nINCLUDED_HOSTS=node-[2-6,8-46,48-84,86-128] \\\npython -m fb_sweep.opt.sweep_opt_en_lm_175b \\\n-n 124 -g 8 -t 1 \\\n--checkpoints-dir /shared/home/namangoyal/checkpoints/175B/ \\\n--local-checkpoints-dir /mnt/scratch/myleott/checkpoints/2021-11-23.9 \\\n--full-cloud-upload-path $PERMANENT_CLOUD_UPLOAD_PATH \\\n-p $RUN_ID\n2021-11-22 7:45am [Myle]: Run 12.15\n●\n12.14 failed with CUDA error:\n\n○\nTHCudaCheck FAIL file=/pytorch/aten/src/THC/THCCachingHostAllocator.cpp line=278\nerror=999 : unknown error\n○\nRuntimeError: Caught RuntimeError in pin memory thread for device 4.\n●\nTo find bad node:\n○\nRun nvidia-smi on all hosts and save output in nvidia_smi_logs directory\n■\nmkdir nvidia_smi_logs\n■\nscontrol show hostnames node-[2-6,8-46,48-94,96-128] | PDSH_RCMD_TYPE=ssh pdsh -w -\nnvidia-smi -f nvidia_smi_logs/%h\n○\nConclusion: node-85: Unable to determine the device handle for GPU 000B:00:00.0: GPU is\nlost.  Reboot the system to recover this GPU\n●\nReboot the bad node\n○\nssh node-85\n○\nsudo reboot\n●\nBad node never came back!\n○\nTry manually restarting from UI\n●\nCurrent status:\n○\nwe technically have quota up to 132 nodes\n○\nwe currently have 125 nodes running, but only 123 are good\n○\ntwo nodes have IB issues that make them slow when part of any distributed jobs\n○\nwe are not able to grow beyond 125 nodes – this makes me think the cluster does not have any\nspare nodes at this point\n# use updated fairscale\ncd ~/src/fairscale\ngit fetch origin prefetch_fsdp_params_simple\ngit checkout prefetch_fsdp_params_simple\n# we use a single Cloud blob path for all checkpoints now\nPERMANENT_CLOUD_UPLOAD_PATH=\"<<<SCRUBBED FOR RELEASE>>>\"\n# increment the run ID so we don’t clobber tensorboard\nRUN_ID=175B_run12.15\n# relaunch training and restore checkpoint from Cloud blob storage\nINCLUDED_HOSTS=node-[2-6,8-46,48-72,74-128] \\\npython -m fb_sweep.opt.sweep_opt_en_lm_175b \\\n-n 124 -g 8 -t 1 \\\n--checkpoints-dir /shared/home/namangoyal/checkpoints/175B/ \\\n--local-checkpoints-dir /mnt/scratch/myleott/checkpoints/2021-11-22.3 \\\n--full-cloud-upload-path $PERMANENT_CLOUD_UPLOAD_PATH \\\n-p $RUN_ID\nAfter launch:\nsudo scontrol update job=2028 TimeLimit=UNLIMITED\nsudo scontrol update job=2028 MailUser=<scrubbed> MailType=ALL\n./scripts/poll_file.py /shared/home/namangoyal/checkpoints/175B/175B_run12.15*/train.log --mailto <scrubbed>\n2021-11-21 4:15pm [Myle]: Run 12.14\n●\nRun 12.13\n○\nSuccessfully tested the automatic-resume-from-latest-blob-checkpoint functionality\n\n○\nBut loss exploded even faster than 12.12, since the loss scale state is not properly reloaded\nfrom checkpoint, causing the loss scale to stay low since there isn’t enough history built up to\nincrease it\n●\nNext step: roll back to a slightly older checkpoint (22,500 => 22,250) and set\n--threshold-loss-scale=0.25\n●\nNew train log:\n/shared/home/namangoyal/checkpoints/175B/175B_run12.14.me_fp16.minscale0.25.fsdp.gpf32.0.relu.t\nransformer_lm_megatron.nlay96.emb12288.lrnpos.0emb_scale.bm_none.tps2048.gpt2.adam.b2_0.95.\neps1e-08.cl1.0.lr0.00012.endlr6e-06.wu2000.dr0.1.atdr0.1.0emb_dr.wd0.1.ms8.uf1.mu143052.s1.ngpu\n992/train.log\n# use updated fairscale\ncd ~/src/fairscale\ngit fetch origin prefetch_fsdp_params_simple\ngit checkout prefetch_fsdp_params_simple\nCKPT_DIR=/data/users/myleott/175B_run12.12.me_fp16.fsdp.gpf32.0.relu.transformer_lm_megatron.nlay96.emb12288.lrnp\nos.0emb_scale.bm_none.tps2048.gpt2.adam.b2_0.95.eps1e-08.cl1.0.lr0.00012.endlr6e-06.wu2000.dr0.1.atdr0.1.0emb_dr.\nwd0.1.ms8.uf1.mu143052.s1.ngpu992\nBLOB_URL=\"<<<SCRUBBED FOR RELEASE>>>\"\ncd $CKPT_DIR\ncp --recursive --include-pattern \"checkpoint_8_22000*.pt\" \"$BLOB_URL\" checkpoint_8_22000\ncp --recursive --include-pattern \"checkpoint_8_22250*.pt\" \"$BLOB_URL\" checkpoint_8_22250\nRESTORE_FILE=$CKPT_DIR/checkpoint_8_22250/checkpoint_8_22250.pt\nRUN_ID=175B_run12.14\nINCLUDED_HOSTS=node-[2-6,8-46,48-94,96-128] \\\npython -m fb_sweep.opt.sweep_opt_en_lm_175b \\\n-n 124 -g 8 -t 1 \\\n-p $RUN_ID \\\n--checkpoints-dir /shared/home/namangoyal/checkpoints/175B/ \\\n--restore-file $RESTORE_FILE\nAfter launch:\nsudo scontrol update job=2021 TimeLimit=UNLIMITED\nsudo scontrol update job=2021 MailUser=<scrubbed> MailType=ALL\n2021-11-21 3pm [Myle]: Run 12.13\n●\nManually killed and relaunched with blob requeueing fixes from #2666\n●\nNew train log:\n/shared/home/namangoyal/checkpoints/175B/175B_run12.13.me_fp16.fsdp.gpf32.0.relu.transformer_l\nm_megatron.nlay96.emb12288.lrnpos.0emb_scale.bm_none.tps2048.gpt2.adam.b2_0.95.eps1e-08.cl1\n.0.lr0.00012.endlr6e-06.wu2000.dr0.1.atdr0.1.0emb_dr.wd0.1.ms8.uf1.mu143052.s1.ngpu992/train.log\n\n# use updated fairscale\ncd ~/src/fairscale\ngit fetch origin prefetch_fsdp_params_simple\ngit checkout prefetch_fsdp_params_simple\n# we use a single Cloud blob path for all checkpoints now\nPERMANENT_CLOUD_UPLOAD_PATH=\"<<<SCRUBBED FOR RELEASE>>>\"\n# increment the run ID so we don’t clobber tensorboard\nRUN_ID=175B_run12.13\n# relaunch training and restore checkpoint from Cloud blob storage\nINCLUDED_HOSTS=node-[2-6,8-46,48-94,96-128] \\\npython -m fb_sweep.opt.sweep_opt_en_lm_175b \\\n-n 124 -g 8 -t 1 \\\n--checkpoints-dir /shared/home/namangoyal/checkpoints/175B/ \\\n--local-checkpoints-dir /mnt/scratch/myleott/checkpoints/2021-11-21 \\\n--full-cloud-upload-path $PERMANENT_CLOUD_UPLOAD_PATH \\\n-p $RUN_ID\nAfter launch:\nsudo scontrol update job=2019 TimeLimit=UNLIMITED\nsudo scontrol update job=2019 MailUser=<scrubbed> MailType=ALL\n2021-11-21 Analysis of 12.X series\nEta of completion:\n●\nAt observed rate (including downtime, total average WPS, etc)\n○\nGoal: 144k updates; Currently at: 22k\n○\nStarted Nov 11 22:00; Currently Nov 21, 13:00; duration 231 hours.\n■\n= 38 seconds per update with downtime\n■\n= 53 more days at total average pace\n■\n= Jan 3\n●\nAt optimal rate (including WPS improvements from Naman, no downtime):\n○\n122k updates to go\n○\n19.3 seconds per update with no downtime\n○\n= 27.2523148 days\n○\n= Dec 18\nRelated idle thoughts [Stephen] about a potential v2:\n●\nTuning WD or LR might be most beneficial\n●\nI still think fresh BPE might be nice\n●\nI’d really like to add in multilingual/non-English data as the next big chunk of data. (Would definitely\nnecessitate a new BPE)\n2021-11-20 9:30pm [Myle]: Run 12.12\n●\nHad previously tried requeuing job, but now realized it loaded the previous checkpoint at 17750 (likely\ndue to the presence of --restore-file)\n●\nWasn’t able to get Sam’s blob reload logic to work, reverted to manual download\n●\nNew log:\n/shared/home/namangoyal/checkpoints/175B/175B_run12.12.me_fp16.fsdp.gpf32.0.relu.transformer_l\n\nm_megatron.nlay96.emb12288.lrnpos.0emb_scale.bm_none.tps2048.gpt2.adam.b2_0.95.eps1e-08.cl1\n.0.lr0.00012.endlr6e-06.wu2000.dr0.1.atdr0.1.0emb_dr.wd0.1.ms8.uf1.mu143052.s1.ngpu992/train.log\n# use updated fairscale\ncd ~/src/fairscale\ngit fetch origin prefetch_fsdp_params_simple\ngit checkout prefetch_fsdp_params_simple\nCKPT_DIR=/data/users/myleott/175B_run12.11.me_fp16.fsdp.gpf32.0.relu.transformer_lm_megatron.nlay96.emb12288.lrnp\nos.0emb_scale.bm_none.tps2048.gpt2.adam.b2_0.95.eps1e-08.cl1.0.lr0.00012.endlr6e-06.wu2000.dr0.1.atdr0.1.0emb_dr.\nwd0.1.ms8.uf1.mu143052.s1.ngpu992\nBLOB_URL=\"<<<SCRUBBED FOR RELEASE>>>\"\ncd $CKPT_DIR\ncp --recursive --include-pattern \"checkpoint_7_20250*.pt\" \"$BLOB_URL\" checkpoint_7_20250\nRESTORE_FILE=$CKPT_DIR/checkpoint_7_20250/checkpoint_7_20250.pt\nRUN_ID=175B_run12.12\nINCLUDED_HOSTS=node-[2-6,8-46,48-94,96-128] \\\npython -m fb_sweep.opt.sweep_opt_en_lm_175b \\\n-n 124 -g 8 -t 1 \\\n-p $RUN_ID \\\n--checkpoints-dir /shared/home/namangoyal/checkpoints/175B/ \\\n--restore-file $RESTORE_FILE\nAfter launch:\nsudo scontrol update job=2001 TimeLimit=UNLIMITED\nsudo scontrol update job=2001 MailUser=<scrubbed> MailType=ALL\n2021-11-19 9:30am [Myle]: Run 12.11\n●\ntraced down IB issues with node-[7,43,90,95].\nSam Shleifer\n●\nfound ECC errors on node-1\nSusan Zhang\n●\nUpdated fairscale with Naman’s FSDP speedup:\nfairscale@8820049331331c773077c257667aa81baf4cc9f9\nCKPT_DIR=/data/users/myleott/175B_run12.10.me_fp16.fsdp.gpf32.0.relu.transformer_lm_megatron.nlay96.emb12288.lrnp\nos.0emb_scale.bm_none.tps2048.gpt2.adam.b2_0.95.eps1e-08.cl1.0.lr0.00012.endlr6e-06.wu2000.dr0.1.atdr0.1.0emb_dr.\nwd0.1.ms8.uf1.mu143052.s1.ngpu992\nBLOB_URL=\"<<<SCRUBBED FOR RELEASE>>>\"\ncd $CKPT_DIR\ncp --recursive --include-pattern \"checkpoint_6_17750*.pt\" \"$BLOB_URL\" checkpoint_6_17750\nexport RESTORE_FILE=$CKPT_DIR/checkpoint_6_17750/checkpoint_6_17750.pt\nexport RUN_ID=175B_run12.11\n# use updated fairscale\ncd ~/src/fairscale\ngit fetch origin prefetch_fsdp_params_simple\ngit checkout prefetch_fsdp_params_simple\n# initial test run to validate nodes\n\nINCLUDED_HOSTS=node-[2-6,8-89,91-94,96-128] \\\npython -m fb_sweep.opt.sweep_opt_en_lm_175b \\\n-n 124 -g 8 -t 1 \\\n-p test4_${RUN_ID} \\\n--checkpoints-dir /shared/home/namangoyal/checkpoints/175B/\n# resume from previous checkpoint\nINCLUDED_HOSTS=node-[2-6,8-89,91-94,96-128] \\\npython -m fb_sweep.opt.sweep_opt_en_lm_175b \\\n-n 124 -g 8 -t 1 \\\n-p $RUN_ID \\\n--checkpoints-dir /shared/home/namangoyal/checkpoints/175B/ \\\n--restore-file $RESTORE_FILE\nAfter launch:\nsudo scontrol update job=1984 TimeLimit=UNLIMITED\nsudo scontrol update job=1984 MailUser=<scrubbed> MailType=ALL\n2021-11-18 List of issues for Cloud Complaints\nUnable to train continuously for more than 1-2 days on a cluster of 128 nodes. Many failures require manual\ndetection and remediation, wasting compute resources and researcher time:\n●\nGPU reliability issues (e.g., ECC errors) leading to frequent job restarts and manual reprovisioning of\nproblematic nodes/GPUs\n●\nIB issues lead to degraded training speed (-20% throughput), requiring manual bisection of problematic\nnodes and manual reprovisioning\n●\nUnexpected job hangs, likely due to IB/NCCL issues, requiring manual detection\nConcrete failures seen:\n●\nGPUs randomly disconnecting\n○\n“Unable to determine the device handle for GPU 000B:00:00.0: GPU is lost.  Reboot the system\nto recover this GPU”\n●\nNodes with bad IB\n○\n“p2p_plugin.c:141 NCCL WARN NET/IB : Got async event : port error”\n●\nFrequently see GPUs with high rates of uncorrectable ECC errors\n○\nOccurs every 1-2 days on a cluster of 128 nodes\n○\nReprovisioning nodes often returns the same node with ECC errors\nAsks:\n●\nOn CSP side:\n○\nGPUs should be validated to be sufficiently burned-in and ECC checked.\n○\nCSP to check for IB issues before allocating to us.\n○\nNeed mechanism to retire problematic nodes so that they are not reassigned to us before they\nhave been fixed/validated.\n●\nOn our side:\n○\nAutomated health checks to be run at the start and end of each job.\n○\nAutomatic detection and requeuing of jobs that hang due to IB/NCCL issues.\n2021-11-18 5:30pm [Stephen]: Notes from 12.10\n●\nObserved slow down is persistent\n\n●\nObserve that the model will reach the next epoch boundary (and minimize wasted tokenization) in\nabout 12-15 hours.\n●\nDecision:\n○\nWe will let it continue to run at lower WPS overnight\n○\nIn the morning we will kill it, bisect and find the node with bad IB\n○\nWe will simultaneously launch with Naman’s improvements to WPS.\n○\nOnce we identify the bad node, we need to document these lists of things to complain to CSP\nand get our money back\n●\nReplaced node-[1,39] in cloud UI\n○\nnode-1 came back up with ECC errors\n2021-11-18 3pm [Stephen]: Run 12.10\nRelaunching for observed slowness.\nCKPT_DIR=/data/users/roller/175B_run12.09.me_fp16.fsdp.gpf32.0.relu.transformer_lm_megatron.nlay96.emb12288.lrnpo\ns.0emb_scale.bm_none.tps2048.gpt2.adam.b2_0.95.eps1e-08.cl1.0.lr0.00012.endlr6e-06.wu2000.dr0.1.atdr0.1.0emb_dr.w\nd0.1.ms8.uf1.mu143052.s1.ngpu992\nCHKPT=checkpoint_6_16750\nBLOB_URL=\"<<<SCRUBBED FOR RELEASE>>>\"\nmkdir $CKPT_DIR\ncd $CKPT_DIR\ncp --recursive --include-pattern \"checkpoint_6_16750*.pt\" \"$BLOB_URL\" checkpoint_6_16750\nexport RESTORE_FILE=$CKPT_DIR/checkpoint_6_16750/checkpoint_6_16750.pt\nexport RUN_ID=175B_run12.10\ncd ~/working/fairseq\nINCLUDED_HOSTS=node-[2-38,40-89,91-119,121-128] \\\npython -m fb_sweep.opt.sweep_opt_en_lm_175b \\\n-n 124 -g 8 -t 1 \\\n-p $RUN_ID \\\n--checkpoints-dir /shared/home/namangoyal/checkpoints/175B/ \\\n--restore-file $RESTORE_FILE\n# After launch:\nsudo scontrol update job=1477 TimeLimit=UNLIMITED\nsudo scontrol update job=1477 MailUser=<scrubbed> MailType=ALL\n# update tensorboard\n●\nObserved exceptions during workflow:\n\n○\nHad to change my SAS url to a new value (was given the new value in the exception)\n○\nHad to `pip install cloud-storage-blob`\n○\nSome nodes were left stuck in a drain state (1, 39)\n○\nSsh’d into 43 after receiving guidance it was probably okay, and ran `nvidia-smi -q -d \"ECC\"` to\ncheck for ECC errors. All reported 0 so switched 1 to that node.\n○\nNeeded to cd back to my fairseq directory before launch\n○\nHad to relaunch due to a typo in the checkpoint filenames (updated instructions)\n●\nLaunched at 16:04 ET\n●\nNew Log file:\n○\n/shared/home/namangoyal/checkpoints/175B/175B_run12.10.me_fp16.fsdp.gpf32.0.relu.transfo\nrmer_lm_megatron.nlay96.emb12288.lrnpos.0emb_scale.bm_none.tps2048.gpt2.adam.b2_0.95\n.eps1e-08.cl1.0.lr0.00012.endlr6e-06.wu2000.dr0.1.atdr0.1.0emb_dr.wd0.1.ms8.uf1.mu143052.\ns1.ngpu992/train.log\n●\nPotential problematic machines (nov 18):\n○\nDiagnosis: ssh’ing in and running nvidia-smi reports “Unable to determine the device handle for\nGPU 000B:00:00.0: GPU is lost.  Reboot the system to recover this GPU”\n○\nnode-1\n○\nnode-39\n○\nSuggested remedy is both of them need to be rebooted manually by ssh + sudo reboot\n●\nPotential Problematic Machines [updated Nov 17]:\n○\nnode-90 (N/A uncorrectable errors instead of 0)\n■\nSRAM Correctable              : N/A\n■\nSRAM Uncorrectable            : N/A\n■\nDRAM Correctable              : N/A\n■\nDRAM Uncorrectable            : N/A\n2021-11-17 11pm [Myle]: Run 12.09\n●\nPrevious run failed with mysterious error: “RuntimeError: CUDA error: unknown error”\n○\nHappened as validation was starting on run 12.08:\n2021-11-18 02:24:01 | INFO | fairseq_cli.train | Begin looping over validation\n\"valid/Gutenberg_PG-19\" subset with length \"0\"\n●\nNew log file:\n/shared/home/namangoyal/checkpoints/175B/175B_run12.09.me_fp16.fsdp.gpf32.0.relu.transformer_l\nm_megatron.nlay96.emb12288.lrnpos.0emb_scale.bm_none.tps2048.gpt2.adam.b2_0.95.eps1e-08.cl1\n.0.lr0.00012.endlr6e-06.wu2000.dr0.1.atdr0.1.0emb_dr.wd0.1.ms8.uf1.mu143052.s1.ngpu992/train.log\nCKPT_DIR=/data/users/myleott/175B_run12.08.me_fp16.fsdp.gpf32.0.relu.transformer_lm_megatron.nlay96.emb12288.lrnp\nos.0emb_scale.bm_none.tps2048.gpt2.adam.b2_0.95.eps1e-08.cl1.0.lr0.00012.endlr6e-06.wu2000.dr0.1.atdr0.1.0emb_dr.\nwd0.1.ms8.uf1.mu143052.s1.ngpu992\nBLOB_URL=\"<<<SCRUBBED FOR RELEASE>>>\"\ncd $CKPT_DIR\ncp --recursive --include-pattern \"checkpoint_6_15750*.pt\" \"$BLOB_URL\" checkpoint_6_15750\nexport RESTORE_FILE=$CKPT_DIR/checkpoint_6_15750/checkpoint_6_15750.pt\nexport RUN_ID=175B_run12.09\nINCLUDED_HOSTS=node-[1-38,40-42,44-89,91-119,121-128] \\\npython -m fb_sweep.opt.sweep_opt_en_lm_175b \\\n\n-n 124 -g 8 -t 1 \\\n-p $RUN_ID \\\n--checkpoints-dir /shared/home/namangoyal/checkpoints/175B/ \\\n--restore-file $RESTORE_FILE\nAfter launch:\nsudo scontrol update job=1403 TimeLimit=UNLIMITED\nsudo scontrol update job=1403 MailUser=<scrubbed> MailType=ALL\n2021-11-16 11pm [Myle]: Run 12.08\n●\nPrevious run failed with mysterious error: “p2p_plugin.c:141 NCCL WARN NET/IB : Got async event :\nport error”\n●\nNew log file:\n/shared/home/namangoyal/checkpoints/175B/175B_run12.08.me_fp16.fsdp.gpf32.0.relu.transformer_l\nm_megatron.nlay96.emb12288.lrnpos.0emb_scale.bm_none.tps2048.gpt2.adam.b2_0.95.eps1e-08.cl1\n.0.lr0.00012.endlr6e-06.wu2000.dr0.1.atdr0.1.0emb_dr.wd0.1.ms8.uf1.mu143052.s1.ngpu992/train.log\nCKPT_DIR=/data/users/myleott/175B_run12.07.me_fp16.fsdp.gpf32.0.relu.transformer_lm_megatron.nlay96.emb12288.lrnp\nos.0emb_scale.bm_none.tps2048.gpt2.adam.b2_0.95.eps1e-08.cl1.0.lr0.00012.endlr6e-06.wu2000.dr0.1.atdr0.1.0emb_dr.\nwd0.1.ms8.uf1.mu143052.s1.ngpu992\nBLOB_URL=\"<<<SCRUBBED FOR RELEASE>>>\"\ncd $CKPT_DIR\ncp --recursive --include-pattern \"checkpoint_5_13250*.pt\" \"$BLOB_URL\" checkpoint_5_13250\nexport RESTORE_FILE=$CKPT_DIR/checkpoint_5_13250/checkpoint_5_13250.pt\nexport RUN_ID=175B_run12.08\nINCLUDED_HOSTS=node-[1-38,40-89,91-94,96-119,121-128] \\\npython -m fb_sweep.opt.sweep_opt_en_lm_175b \\\n-n 124 -g 8 -t 1 \\\n-p $RUN_ID \\\n--checkpoints-dir /shared/home/namangoyal/checkpoints/175B/ \\\n--restore-file $RESTORE_FILE\nAfter launch:\nsudo scontrol update job=1394 TimeLimit=UNLIMITED\nsudo scontrol update job=1394 MailUser=<scrubbed> MailType=ALL\n2021-11-14 9:45pm [Myle]: Run 12.07\n●\nPrevious run silently hung\n●\nNew log file:\n/shared/home/namangoyal/checkpoints/175B/175B_run12.07.me_fp16.fsdp.gpf32.0.relu.transformer_l\nm_megatron.nlay96.emb12288.lrnpos.0emb_scale.bm_none.tps2048.gpt2.adam.b2_0.95.eps1e-08.cl1\n.0.lr0.00012.endlr6e-06.wu2000.dr0.1.atdr0.1.0emb_dr.wd0.1.ms8.uf1.mu143052.s1.ngpu992/train.log\n\nCKPT_DIR=/data/users/myleott/175B_run12.06.me_fp16.fsdp.gpf32.0.relu.transformer_lm_megatron.nlay96.emb12288.lrnp\nos.0emb_scale.bm_none.tps2048.gpt2.adam.b2_0.95.eps1e-08.cl1.0.lr0.00012.endlr6e-06.wu2000.dr0.1.atdr0.1.0emb_dr.\nwd0.1.ms8.uf1.mu143052.s1.ngpu992\nBLOB_URL=\"<<<SCRUBBED FOR RELEASE>>>\"\ncd $CKPT_DIR\ncp --recursive --include-pattern \"checkpoint_3_7500*.pt\" \"$BLOB_URL/*\" checkpoint_3_7500\nexport RESTORE_FILE=$CKPT_DIR/checkpoint_3_7500/checkpoint_3_7500.pt\nexport RUN_ID=175B_run12.07\nINCLUDED_HOSTS=node-[1-38,40-89,91-94,96-119,121-128] \\\npython -m fb_sweep.opt.sweep_opt_en_lm_175b \\\n-n 124 -g 8 -t 1 \\\n-p $RUN_ID \\\n--checkpoints-dir /shared/home/namangoyal/checkpoints/175B/ \\\n--restore-file $RESTORE_FILE\nAfter launch:\nsudo scontrol update job=1390 TimeLimit=UNLIMITED\nsudo scontrol update job=1390 MailUser=<scrubbed> MailType=ALL\n2021-11-12 10:30pm [Myle]: Run 12.06\n●\nPrevious restore failed because checkpoint_1_2000 is missing shards in Cloud blob!\n○\nRun 12.02 must have been interrupted before all the checkpoints were uploaded.\n○\nFortunately we can get the shards from local storage on the nodes\n# copy missing checkpoints from local storage on each node\nMNT_DIR=/mnt/scratch/susanz/checkpoints/2021-11-12/175B_run12.02.me_fp16.fsdp.gpf32.0.relu.transformer_lm_megatro\nn.nlay96.emb12288.lrnpos.0emb_scale.bm_none.tps2048.gpt2.adam.b2_0.95.eps1e-08.cl1.0.lr0.00012.endlr6e-06.wu2000.\ndr0.1.atdr0.1.0emb_dr.wd0.1.ms8.uf1.mu143052.s1.ngpu992\nWCOLL=~/hosts PDSH_RCMD_TYPE=ssh pdsh ls $MNT_DIR | grep \"checkpoint_1_2000.*pt$\"\n(...)\ncd $RESTORE_DIR\nscp node-30:$MNT_DIR/checkpoint_1_2000-model_part-5-shard29.pt .\nscp node-19:$MNT_DIR/checkpoint_1_2000-model_part-4-shard18.pt .\nscp node-37:$MNT_DIR/checkpoint_1_2000-model_part-0-shard36.pt .\nscp node-56:$MNT_DIR/checkpoint_1_2000-model_part-0-shard54.pt .\nscp node-56:$MNT_DIR/checkpoint_1_2000-model_part-1-shard54.pt .\nscp node-56:$MNT_DIR/checkpoint_1_2000-model_part-2-shard54.pt .\nscp node-56:$MNT_DIR/checkpoint_1_2000-model_part-3-shard54.pt .\nscp node-56:$MNT_DIR/checkpoint_1_2000-model_part-4-shard54.pt .\nscp node-56:$MNT_DIR/checkpoint_1_2000-model_part-5-shard54.pt .\nscp node-56:$MNT_DIR/checkpoint_1_2000-model_part-6-shard54.pt .\nscp node-56:$MNT_DIR/checkpoint_1_2000-model_part-7-shard54.pt .\nscp node-68:$MNT_DIR/checkpoint_1_2000-model_part-0-shard66.pt .\nscp node-103:$MNT_DIR/checkpoint_1_2000-model_part-0-shard99.pt .\nexport\nRESTORE_FILE=/data/users/susanz/175B_run12.02.me_fp16.fsdp.gpf32.0.relu.transformer_lm_megatron.nlay96.emb12288.l\nrnpos.0emb_scale.bm_none.tps2048.gpt2.adam.b2_0.95.eps1e-08.cl1.0.lr0.00012.endlr6e-06.wu2000.dr0.1.atdr0.1.0emb_\ndr.wd0.1.ms8.uf1.mu143052.s1.ngpu992/checkpoint_1_2000/checkpoint_1_2000.pt\nexport RUN_ID=175B_run12.06\n\nINCLUDED_HOSTS=node-[1-38,41-94,96-119,121-128] \\\npython -m fb_sweep.opt.sweep_opt_en_lm_175b \\\n-n 124 -g 8 -t 1 \\\n-p $RUN_ID \\\n--checkpoints-dir /shared/home/namangoyal/checkpoints/175B/ \\\n--restore-file $RESTORE_FILE\nAfter launch:\nsudo scontrol update job=1387 TimeLimit=UNLIMITED\nsudo scontrol update job=1387 MailUser=<scrubbed> MailType=ALL\n●\nMisc\n○\nRe-enable tensor init on GPU\n○\nAlso run `sudo reboot` on node-[39,40]\ngit checkout 08cb44d9dc3dcbe90605dd03b4f2156996ea2bac\nexport\nRESTORE_FILE=/data/users/susanz/175B_run12.02.me_fp16.fsdp.gpf32.0.relu.transformer_lm_megatron.nlay96.emb12288.l\nrnpos.0emb_scale.bm_none.tps2048.gpt2.adam.b2_0.95.eps1e-08.cl1.0.lr0.00012.endlr6e-06.wu2000.dr0.1.atdr0.1.0emb_\ndr.wd0.1.ms8.uf1.mu143052.s1.ngpu992/checkpoint_1_2000/checkpoint_1_2000.pt\nexport RUN_ID=175B_run12.06\nINCLUDED_HOSTS=node-[1-38,41-94,96-119,121-128] \\\npython -m fb_sweep.opt.sweep_opt_en_lm_175b \\\n-n 124 -g 8 -t 1 \\\n-p $RUN_ID \\\n--checkpoints-dir /shared/home/namangoyal/checkpoints/175B/ \\\n--restore-file $RESTORE_FILE\nAfter launch:\nsudo scontrol update job=1386 TimeLimit=UNLIMITED\nsudo scontrol update job=1386 MailUser=<scrubbed> MailType=ALL\n2021-11-12 7pm [Susan]: Run 12.05\n●\nRemoved tensor init on gpu.\ngit checkout ddbb690ed49d49653a1c12374386de1a2102d3a2\nexport\nRESTORE_FILE=/data/users/susanz/175B_run12.02.me_fp16.fsdp.gpf32.0.relu.transformer_lm_megatron.nlay96.emb12288.l\nrnpos.0emb_scale.bm_none.tps2048.gpt2.adam.b2_0.95.eps1e-08.cl1.0.lr0.00012.endlr6e-06.wu2000.dr0.1.atdr0.1.0emb_\ndr.wd0.1.ms8.uf1.mu143052.s1.ngpu992/checkpoint_1_2000/checkpoint_1_2000.pt\nexport RUN_ID=175B_run12.05\nINCLUDED_HOSTS=node-[1-38,40-94,96,98-119,121-128] \\\npython -m fb_sweep.opt.sweep_opt_en_lm_175b \\\n-n 124 -g 8 -t 1 \\\n-p $RUN_ID \\\n--checkpoints-dir /shared/home/namangoyal/checkpoints/175B/ \\\n--restore-file $RESTORE_FILE\nAfter launch:\nsudo scontrol update job=1385 TimeLimit=UNLIMITED\nsudo scontrol update job=1385 MailUser=<scrubbed> MailType=ALL\n\n2021-11-12 6pm [Susan]: Run 12.04\n●\nHost 95 and 120 are both full of ECC errors. Put both in drain. Replacing 95 seems to give the same\nmachine, so will wait to restart later.\ngit checkout 38dab5485ef7d5c1e29187185680b6e4f314e7b9\nexport\nRESTORE_FILE=/data/users/susanz/175B_run12.02.me_fp16.fsdp.gpf32.0.relu.transformer_lm_megatron.nlay96.emb12288.l\nrnpos.0emb_scale.bm_none.tps2048.gpt2.adam.b2_0.95.eps1e-08.cl1.0.lr0.00012.endlr6e-06.wu2000.dr0.1.atdr0.1.0emb_\ndr.wd0.1.ms8.uf1.mu143052.s1.ngpu992/checkpoint_1_2000/checkpoint_1_2000.pt\nexport RUN_ID=175B_run12.04\nINCLUDED_HOSTS=node-[1-38,40-94,96,98-119,121-128] \\\npython -m fb_sweep.opt.sweep_opt_en_lm_175b \\\n-n 124 -g 8 -t 1 \\\n-p $RUN_ID \\\n--checkpoints-dir /shared/home/namangoyal/checkpoints/175B/ \\\n--restore-file $RESTORE_FILE\nAfter launch:\nsudo scontrol update job=1384 TimeLimit=UNLIMITED\nsudo scontrol update job=1384 MailUser=<scrubbed> MailType=ALL\n2021-11-12 3pm [Susan]: Run 12.03\n●\nRestarting requires following the same steps as 11.2 to download checkpoints (takes ~25 minutes).\n●\nDownloading into\n/data/users/susanz/175B_run12.02.me_fp16.fsdp.gpf32.0.relu.transformer_lm_megatron.nlay96.emb12\n288.lrnpos.0emb_scale.bm_none.tps2048.gpt2.adam.b2_0.95.eps1e-08.cl1.0.lr0.00012.endlr6e-06.wu\n2000.dr0.1.atdr0.1.0emb_dr.wd0.1.ms8.uf1.mu143052.s1.ngpu992\n●\nLoading checkpoint when run comes up takes ~15 minutes.\n●\nLoading data when run comes up takes ~30 minutes (to fast forward to data point within epoch).\n# Find cloud path of where the checkpoints went - grab last one\n(fairseq-20210913)\nsusanz@ip-0A1E0404:/shared/home/namangoyal/checkpoints/175B/175B_run12.02.me_fp16.fsdp.gpf32.0.relu.transformer_l\nm_megatron.nlay96.emb12288.lrnpos.0emb_scale.bm_none.tps2048.gpt2.adam.b2_0.95.eps1e-08.cl1.0.lr0.00012.endlr6e-0\n6.wu2000.dr0.1.atdr0.1.0emb_dr.wd0.1.ms8.uf1.mu143052.s1.ngpu992$ grep \"<<< SCRUBBED FOR RELEASE >>>\" train.log |\ntail -n 1\n2021-11-12 19:34:30 | INFO | fairseq_cli.train | preparing to copy\n/mnt/scratch/susanz/checkpoints/2021-11-12/175B_run12.02.me_fp16.fsdp.gpf32.0.relu.transformer_lm_megatron.nlay96\n.emb12288.lrnpos.0emb_scale.bm_none.tps2048.gpt2.adam.b2_0.95.eps1e-08.cl1.0.lr0.00012.endlr6e-06.wu2000.dr0.1.at\ndr0.1.0emb_dr.wd0.1.ms8.uf1.mu143052.s1.ngpu992/checkpoint_1_2000-model_part-0-shard0.pt to <<<SCRUBBED FOR\nRELEASE>>>\nexport\nRUN_ID=\"175B_run12.02.me_fp16.fsdp.gpf32.0.relu.transformer_lm_megatron.nlay96.emb12288.lrnpos.0emb_scale.bm_none\n.tps2048.gpt2.adam.b2_0.95.eps1e-08.cl1.0.lr0.00012.endlr6e-06.wu2000.dr0.1.atdr0.1.0emb_dr.wd0.1.ms8.uf1.mu14305\n2.s1.ngpu992\"\n\nexport CHECKPOINT=\"checkpoint_1_2000\"\nmkdir /data/users/susanz/$RUN_ID\ncd /data/users/susanz/$RUN_ID\nmkdir $CHECKPOINT\nBLOB_URL=\"<<<SCRUBBED FOR RELEASE>>>\"\ncp --recursive --include-pattern \"$CHECKPOINT-*.pt\" \"$BLOB_URL/*\" $CHECKPOINT/\nLaunch commands:\ngit checkout 38dab5485ef7d5c1e29187185680b6e4f314e7b9\nexport\nRESTORE_FILE=/data/users/susanz/175B_run12.02.me_fp16.fsdp.gpf32.0.relu.transformer_lm_megatron.nlay96.emb12288.l\nrnpos.0emb_scale.bm_none.tps2048.gpt2.adam.b2_0.95.eps1e-08.cl1.0.lr0.00012.endlr6e-06.wu2000.dr0.1.atdr0.1.0emb_\ndr.wd0.1.ms8.uf1.mu143052.s1.ngpu992/checkpoint_1_2000/checkpoint_1_2000.pt\nexport RUN_ID=175B_run12.03\n# DO DRY RUN!! Node configuration may have been changed !!!\nINCLUDED_HOSTS=node-[1-38,40-87,89-96,98-119,121-128] \\\npython -m fb_sweep.opt.sweep_opt_en_lm_175b \\\n-n 124 -g 8 -t 1 \\\n-p $RUN_ID \\\n--checkpoints-dir /shared/home/namangoyal/checkpoints/175B/ \\\n--restore-file $RESTORE_FILE \\\n--dry-run\nINCLUDED_HOSTS=node-[1-38,40-87,89-94,96,98-128] \\\npython -m fb_sweep.opt.sweep_opt_en_lm_175b \\\n-n 124 -g 8 -t 1 \\\n-p $RUN_ID \\\n--checkpoints-dir /shared/home/namangoyal/checkpoints/175B/ \\\n--restore-file $RESTORE_FILE\nAfter launch:\nsudo scontrol update job=1382 TimeLimit=UNLIMITED\nsudo scontrol update job=1382 MailUser=<scrubbed> MailType=ALL\n\nAnalysis of Run 12.02 [Susan]\n●\nLoss of ppl starting to oscillate more heavily, potentially indicating that LR is too high.\n●\nCUDA error crashed the run after 2008 updates.\n2021-11-11 11pm [Susan]: Run 12.02\n●\nRelaunched with node 7 put back, same nodelist as 11.10.\n●\nThis worked! Expect roughly 2 minutes between “Start iterating over samples” and the first log line:\n\nAnything longer than that may be an issue…\n●\nNew train dir:\n/shared/home/namangoyal/checkpoints/175B/175B_run12.02.me_fp16.fsdp.gpf32.0.relu.transformer_l\nm_megatron.nlay96.emb12288.lrnpos.0emb_scale.bm_none.tps2048.gpt2.adam.b2_0.95.eps1e-08.cl1\n.0.lr0.00012.endlr6e-06.wu2000.dr0.1.atdr0.1.0emb_dr.wd0.1.ms8.uf1.mu143052.s1.ngpu992\n●\nCommand ran:\ngit checkout 7b7ccd38f30a9db9c32df360922d803620268ce6\nINCLUDED_HOSTS=node-[1-38,40-87,89-96,98-119,121-128] \\\npython -m fb_sweep.opt.sweep_opt_en_lm_175b \\\n-n 124 -g 8 -t 1 \\\n-p 175B_run12.01 \\\n--checkpoints-dir /shared/home/namangoyal/checkpoints/175B/\nAfter launch:\nsudo scontrol update job=1378 TimeLimit=UNLIMITED\nsudo scontrol update job=1378 MailUser=<scrubbed> MailType=ALL\n2021-11-11 5:40pm [Susan]: Run 12.01\n●\nSame as 12.00 but with node 7 removed, and 97 swapped in.\n●\nStill got stuck. Relaunching with node 7 put back.\n●\nNew train dir:\n/shared/home/namangoyal/checkpoints/175B/175B_run12.01.me_fp16.fsdp.gpf32.0.relu.transformer_l\nm_megatron.nlay96.emb12288.lrnpos.0emb_scale.bm_none.tps2048.gpt2.adam.b2_0.95.eps1e-08.cl1\n.0.lr0.00012.endlr6e-06.wu2000.dr0.1.atdr0.1.0emb_dr.wd0.1.ms8.uf1.mu143052.s1.ngpu992\n●\nCommand ran:\ngit checkout 7b7ccd38f30a9db9c32df360922d803620268ce6\nINCLUDED_HOSTS=node-[1-6,8-38,40-87,89-119,121-128] \\\npython -m fb_sweep.opt.sweep_opt_en_lm_175b \\\n-n 124 -g 8 -t 1 \\\n-p 175B_run12.01 \\\n--checkpoints-dir /shared/home/namangoyal/checkpoints/175B/\nAfter launch:\nsudo scontrol update job=1377 TimeLimit=UNLIMITED\nsudo scontrol update job=1377 MailUser=<scrubbed> MailType=ALL\n●\nRebooting node-7:\n○\nDrain first: sudo scontrol update node=node-7 state=drain reason=lostgpu\n2021-11-11 5:30pm [Susan]: Run 12.00 - Lost a GPU on node-7, restarting.\n●\nDiscussion on gradient predivide:\nQuick primer on gradient predivide\n\n●\nSummary of weight init decision:\n●\nWeight Init from different codebases\n○\nMegatron-LM uses sigma / math.sqrt(2.0 * num_layers)\n■\nNOTE: The layer wise scaling is only being applied to the output layer i.e fc2 of ffn and\nout_proj of attn and not all the weight matrix. (this is mostly already implemented in\ngshard_combine_megatron_fsdp but with small differences.)\n■\nsigma = 0.006 for 175B\n■\nWord Embedding\n●\nnormal(0, 0.006)\n■\nPosition Embedding\n●\nnormal(0, 0.006)\n■\nMHA\n●\nQKV input projection\n○\nnormal(0, 0.006)\n●\nOutput projection\n○\nsigma / math.sqrt(2.0 * num_layers)\n○\nStddev: 0.00043\n●\nAll biases: zero\n■\nFFN\n●\nFC1\n○\nnormal(0, 0.006)\n●\nFC2\n○\nsigma / math.sqrt(2.0 * num_layers)\n■\nStddev: 0.00043\n●\nAll biases: zero\n■\nLayer norms\n●\nGamma: 1.0\n●\nBeta: 0.0\n○\nFairseq gshard_combine_megatron_fsdp does:\n■\nWord Embedding\n●\nnormal(0, 0.009)\n■\nPosition Embedding:\n●\nnormal(0, 0.009)\n■\nMHA\n●\nQKV input projection\n○\nStddev: 0.006\n■\nModel parallel fairseq is approximately normal(0, 0.006)\n\n■\nNote: this matches non-model parallel fairseq’s xaviar_uniform(...,\ngain=1 / math.sqrt(2)), which is similar to normal(0, 0.006)\n●\nOutput projection\n○\nStddev: 0.009\n■\nModel parallel fairseq is approximately normal(0, 0.009)\n■\nNote: this matches non-model parallel fairseq’s xavier_uniform,\nwhich is similar to normal(0, 0.009)\n■\nFFN\n●\nFC1\n○\nStddev: 0.005\n■\nUses kaiming_uniform(..., a=math.sqrt(5))\n●\nFC2\n○\nStddev: 0.005\n■\nUses kaiming_uniform(..., a=math.sqrt(5))\n■\nLayer norms\n●\nGamma: 1.0\n●\nBeta: 0.0\n○\nDeepSpeed uses sigma / math.sqrt(2.0 * num_layers)\n○\nMesh tensorflow doesn’t seem to scale init based on num layers\n○\nLingvo doesn’t seem to scale init at all\n○\nPaddle Paddle uses pytorch defaults lmao\n○\nGPT NeoX uses same as deepspeed\n●\nNew train dir:\n/shared/home/namangoyal/checkpoints/175B/175B_run12.00.me_fp16.fsdp.gpf32.0.relu.transformer_l\nm_megatron.nlay96.emb12288.lrnpos.0emb_scale.bm_none.tps2048.gpt2.adam.b2_0.95.eps1e-08.cl1\n.0.lr0.00012.endlr6e-06.wu2000.dr0.1.atdr0.1.0emb_dr.wd0.1.ms8.uf1.mu143052.s1.ngpu992\n●\nCommand ran:\ngit checkout 7b7ccd38f30a9db9c32df360922d803620268ce6\nINCLUDED_HOSTS=node-[1-38,40-87,89-96,98-119,121-128] \\\npython -m fb_sweep.opt.sweep_opt_en_lm_175b \\\n-n 124 -g 8 -t 1 \\\n-p 175B_run12.00 \\\n--checkpoints-dir /shared/home/namangoyal/checkpoints/175B/\nAfter launch:\nsudo scontrol update job=1374 TimeLimit=UNLIMITED\nsudo scontrol update job=1374 MailUser=<scrubbed> MailType=ALL\n●\nLost a GPU:\n\nRestarting and rotating in 97 (rotating out 7).\n2021-11-10 5pm: Run 11.10\nNOTE: To resume Run 11, we must revert 7eacba2\nAnalysis of 11.9\nLoss scale started dropping and hit min at 5280 updates\nDecision\n-\nLaunch 11.10 where we:\n-\nSwitch to RELU\n-\nSwitch to Stable MHA\n-\nWe assume this will die\n-\nPresumably Thursday AM we will launch 12.00 with:\n-\nKill normformer\n-\nLower LR\n-\nBatch Skipping\n\n-\nActual LPE, no scale emb\n-\nAnd possibly some changes to init (layerwise init possibly)\n-\nIn parallel, we will use the RSC to ablate different layerwise init options:\n-\nDefault (as is now)\n-\nLayerwise init scaled by a global constant (as a function of total layers)\n-\nLayerwise init scaled by per-layer-index\nLaunch steps for Run 11.10\n●\nNew train log:\n/shared/home/namangoyal/checkpoints/175B/175B_run11.10.me_fp16.fsdp.gpf32.0.relu.transformer_l\nm_megatron.nlay96.emb12288.lrnsin.nffc.nfatt.nfhd.bm_none.tps2048.gpt2.adam.b2_0.95.eps1e-08.cl\n1.0.lr6e-05.endlr6e-06.wu1000.dr0.1.atdr0.1.0emb_dr.wd0.1.ms8.uf1.mu143052.s1.ngpu992/train.log\ngit checkout 075f54e1b8da88ab90d6a0717d6e73eba33b98a0\nexport INCLUDED_HOSTS=node-[1-38,40-87,89-96,98-119,121-128]\nRESTORE_FILE=/data/users/namangoyal/175B_run11.3.me_fp16.fsdp.gpf32.0.gelu.transformer_lm_megatron.nlay96.emb1228\n8.lrnsin.nffc.nfatt.nfhd.bm_none.tps2048.gpt2.adam.b2_0.98.eps1e-08.cl1.5.lr7.5e-05.endlr7.5e-06.wu1000.dr0.1.atd\nr0.1.0emb_dr.wd0.05.ms8.uf1.mu143052.s1.ngpu992/checkpoint_2_4750/checkpoint_2_4750.pt\nRUN_ID=175B_run11.10\n# note that this command has --reset-dataloader, which is not desired in general\npython -m fb_sweep.opt.sweep_opt_en_lm_175b \\\n-n 124 -g 8 -t 1 \\\n-p $RUN_ID \\\n--checkpoints-dir /shared/home/namangoyal/checkpoints/175B/ \\\n--restore-file $RESTORE_FILE \\\n--reset-dataloader\nAfter launch:\nsudo scontrol update job=1365 TimeLimit=UNLIMITED\nsudo scontrol update job=1365 MailUser=<scrubbed> MailType=ALL\n\n[2021-11-10]: Run 11.9: Lowered LR to 6e-5, match exp 11.6 otherwise\nAnalysis of 11.8\n●\nTraining stalled at 5160, made it 41 steps further than 11.6 (when weight decay was also 0.05)\n○\n11.6 got to num_updates 5119 before naning out\n●\nMassive loss scale drop after 5159:\n\n●\nStill layer 92 that infs\n●\nLast log shows lr of 7.30233e-05\nDecisions for 11.9\n●\nTurn clipping back on (removing the “throw batch out” logic)\n●\nLower LR to 6e-5\n●\nOtherwise match run 11.7:\n○\nIncrease weight decay to 0.1\n○\nLower beta2 to 0.95\n○\nRoll back to checkpoint @ 4750 steps\n○\nReset dataloader\n●\nMisc:\n○\nAdd pnorm logging\nLaunch steps for 11.9\n●\nNew train log:\n/shared/home/namangoyal/checkpoints/175B/175B_run11.9.me_fp16.fsdp.gpf32.0.gelu.transformer_lm\n_megatron.nlay96.emb12288.lrnsin.nffc.nfatt.nfhd.bm_none.tps2048.gpt2.adam.b2_0.95.eps1e-08.cl1.\n0.lr6e-05.endlr6e-06.wu1000.dr0.1.atdr0.1.0emb_dr.wd0.1.ms8.uf1.mu143052.s1.ngpu992/train.log\ngit checkout a69e5d30b4199f40c4651deac918c58ab594f018\nexport INCLUDED_HOSTS=node-[1-38,40-87,89-96,98-119,121-128]\nRESTORE_FILE=/data/users/namangoyal/175B_run11.3.me_fp16.fsdp.gpf32.0.gelu.transformer_lm_megatron.nlay96.emb1228\n8.lrnsin.nffc.nfatt.nfhd.bm_none.tps2048.gpt2.adam.b2_0.98.eps1e-08.cl1.5.lr7.5e-05.endlr7.5e-06.wu1000.dr0.1.atd\nr0.1.0emb_dr.wd0.05.ms8.uf1.mu143052.s1.ngpu992/checkpoint_2_4750/checkpoint_2_4750.pt\nRUN_ID=175B_run11.9\n# note that this command has --reset-dataloader, which is not desired in general\npython -m fb_sweep.opt.sweep_opt_en_lm_175b \\\n-n 124 -g 8 -t 1 \\\n-p $RUN_ID \\\n--checkpoints-dir /shared/home/namangoyal/checkpoints/175B/ \\\n--restore-file $RESTORE_FILE \\\n--reset-dataloader\nAfter launch:\nsudo scontrol update job=1336 TimeLimit=UNLIMITED\nsudo scontrol update job=1336 MailUser=<scrubbed> MailType=ALL\n[2021-11-09]: Run 11.8: Rolling back weight decay, start from 4750\nAnalysis of 11.7\n●\nTraining stalled at 4849\n\n●\nDecisions for 11.8\n●\nBisect recent changes to see what fixes instability\n●\nStart with weight decay 0.1 => 0.05\nLaunch steps for 11.8\n●\nNew train.log:\n/shared/home/namangoyal/checkpoints/175B/175B_run11.8.me_fp16.fsdp.gpf32.0.gelu.transformer_lm\n_megatron.nlay96.emb12288.lrnsin.nffc.nfatt.nfhd.bm_none.tps2048.gpt2.adam.b2_0.95.eps1e-08.cl1.\n0.lr7.5e-05.endlr7.5e-06.wu1000.dr0.1.atdr0.1.0emb_dr.wd0.05.ms8.uf1.mu143052.s1.ngpu992/train.lo\ng\ngit checkout b4842e181dababaaad4f0170ad6c90b782877b1c\nexport INCLUDED_HOSTS=node-[1-63,65-87,89-96,98-119,121-128]\nRESTORE_FILE=/data/users/namangoyal/175B_run11.3.me_fp16.fsdp.gpf32.0.gelu.transformer_lm_megatron.nlay96.emb1228\n8.lrnsin.nffc.nfatt.nfhd.bm_none.tps2048.gpt2.adam.b2_0.98.eps1e-08.cl1.5.lr7.5e-05.endlr7.5e-06.wu1000.dr0.1.atd\nr0.1.0emb_dr.wd0.05.ms8.uf1.mu143052.s1.ngpu992/checkpoint_2_4750/checkpoint_2_4750.pt\nRUN_ID=175B_run11.8\n# note that this command has --reset-dataloader, which is not desired in general\npython -m fb_sweep.opt.sweep_opt_en_lm_175b \\\n-n 124 -g 8 -t 1 \\\n-p $RUN_ID \\\n--checkpoints-dir /shared/home/namangoyal/checkpoints/175B/ \\\n--restore-file $RESTORE_FILE \\\n--reset-dataloader\nAfter launch:\nsudo scontrol update job=1336 TimeLimit=UNLIMITED\nsudo scontrol update job=1336 MailUser=<scrubbed> MailType=ALL\n\n[2021-11-09]: Run 11.7: Changing tons of stuff, start from 4750\nAnalysis\nPotential concerns:\n-\n[Preemptive] Data composition\nLaunch steps for 11.7\n●\nAnalysis to come\n●\nFairseq commit: d9c903ea0a2894071fb5bee96b0c9612f1e5a402\n●\nnew data: /data/opt/corpus_dedup_10_10_1_0.05_run11.7/\n●\nBeta2: 0.95 , weight_decay: 0.1, reset dataloader true\n●\nLog:\n/shared/home/namangoyal/checkpoints/175B/175B_run11.7.me_fp16.fsdp.gpf32.0.gelu.transformer_lm\n_megatron.nlay96.emb12288.lrnsin.nffc.nfatt.nfhd.bm_none.tps2048.gpt2.adam.b2_0.95.eps1e-08.cl1.\n0.lr7.5e-05.endlr7.5e-06.wu1000.dr0.1.atdr0.1.0emb_dr.wd0.1.ms8.uf1.mu143052.s1.ngpu992/train.log\n[2021-11-09]: Run 11.6: Starting to skip batches\nEmergency meeting notes\n-\nStarted by cleaning up tech debt of these notes\n-\nPerformed analysis of 11.6 (see below)\n-\nBrainstormed intervention paths\nDecided actions (becomes 11.7)\n-\nReset dataloader (see description in brainstorm)\n-\nResume from 4750\n-\nIncrease WD to 0.05 -> 0.1\n-\nLower beta2 to 0.98 -> 0.95\n-\n(Note we need a new runid)\n-\n[Myle] swap the shards\n-\n[Naman] Launch the run\nBackup option if things die in the middle of the night:\n-\n[Sam] Check at 11pm\n-\nLower the LR to 6e-5\nBrainstorm of interventions (With initializing from some checkpoint):\n-\nReset dataloader for faster startup\n-\n--reset-dataloader\n-\nSwap shard1 with shard29\n-\nSwap shard2 with shard30\n-\nReasoning: we’re late in shard 2 and so we end up doing 45 minutes of tokenization before we\ncan see updates\n-\nTo be done together:\n-\nLower beta2 to 0.95\n-\nIncrease Weight Decay to 0.1\n\n-\nHow much would we need to roll back to address this?\n-\nLower LR to 6e-5\n-\nHotswap GELU => RELU\n-\nUnsure if the code path works due to fused -- need to test on 100M params\n-\nClamp activations\n-\nShuffle the data\n-\nAdding the max to the layer norm\n-\nManually shrink gradient to embeddings\n-\nIncrease layernorm epsilon\n-\n[bottom pri] Adam epsilon 1e-8 => 1e-6\nBrainstorm of interventions that require full restart [Unanimously unpopular:\n-\nCold swap GELU => RELU (with restart)\n-\nRemove normformer\n-\nDifferent data\n-\nLower LR with cold restart\nAnalysis of 11.6\nHypothesis:\n-\nInstead of clipping, start throwing stuff away\n-\nResume from 5000 steps (140 steps rewound)\nNote 11.6 is NOT equivalent to 11.5 even with restart\n-\nWe reset the loss scale state (accidentally - this isn’t checkpointed: num updates for which overflow\ndidn’t happen)\n-\nLosses match to 2nd decimal, gnorms start diverging\n-\nLoss scales start diverging slightly after gnorm\n-\nInteresting: slightly higher loss scale made it survive slightly more updates\nCute observations:\n-\nWe are seeing a very the same thing: layer 92 attn goes in the range of [-inf, almost inf]\n-\nAND this is happening in the forward pass\n-\nWe only threw away one batch before the loss scale went crazy\n[2021-11-08] Run 11.5: clip 1.5 -> 1.0\nAnalysis of 11.5\n●\nReason: 11.5 exploded at 5139 updates\n●\nDiagnosis: gnorm spike at end: (we had survived bigger spikes but this one exploded the run)\n●\nNote: loss scale is very low (.000122…)\n●\nLayer 92 attention layer norm inputs seemed to be where the input to attn layer norm had -inf (and a\nmaximum value pretty damn close to +inf)\n○\nWe were already dropping loss scale really far back.\n○\nLarge gnorm at step 5136 there was a large, clipped grad norm\n■\nUnsolved mystery: which layer really the problem?\n○\nWe do a handful more updates and we -inf out\n\n●\nAction:\n○\nWe decided to skip gradient update when gnorm is higher than clip norm threshold: #2602\n○\nOther theories: attn is numerically unstable\n●\nRestart logs:\n○\n/shared/home/namangoyal/checkpoints/175B/175B_run11.6.me_fp16.fsdp.gpf32.0.gelu.transfor\nmer_lm_megatron.nlay96.emb12288.lrnsin.nffc.nfatt.nfhd.bm_none.tps2048.gpt2.adam.b2_0.98\n.eps1e-08.cl1.0.lr7.5e-05.endlr7.5e-06.wu1000.dr0.1.atdr0.1.0emb_dr.wd0.05.ms8.uf1.mu1430\n52.s1.ngpu992/train.log\nAnalysis of 11.4\n●\n11.4 exploded at around ~3.94k.\n●\nThe gnorm looks like below\n●\nReduced clipping to 1.0 and restarted\n●\nLogs:\n●\n/shared/home/namangoyal/checkpoints/175B/175B_run11.5.me_fp16.fsdp.gpf32.0.gelu.transformer_lm\n_megatron.nlay96.emb12288.lrnsin.nffc.nfatt.nfhd.bm_none.tps2048.gpt2.adam.b2_0.98.eps1e-08.cl1.\n0.lr7.5e-05.endlr7.5e-06.wu1000.dr0.1.atdr0.1.0emb_dr.wd0.05.ms8.uf1.mu143052.s1.ngpu992/train.lo\ng\n\n[Undated] Run 11.4: Changed validation freq\nJust trying to spend less time in validation/saving.\nNote: validation sets changed, so valid_ppl is not comparable to previous runs\n●\nRestarted with following changes (1959e415390560c7b2d680317ca3c07a5f24f8cc):\n○\nReducing validation frequency to 1000 instead of 250\n○\nModel Initialization on gpu\n○\nReduced validation set sizes\n●\nLogs:\n○\n/shared/home/namangoyal/checkpoints/175B/175B_run11.4.me_fp16.fsdp.gpf32.0.gelu.transfor\nmer_lm_megatron.nlay96.emb12288.lrnsin.nffc.nfatt.nfhd.bm_none.tps2048.gpt2.adam.b2_0.98\n.eps1e-08.cl1.5.lr7.5e-05.endlr7.5e-06.wu1000.dr0.1.atdr0.1.0emb_dr.wd0.05.ms8.uf1.mu1430\n52.s1.ngpu992/train.log\n●\nAdded to tensorboard:\n○\ncd /shared/home/namangoyal/checkpoints/175B/tensorboard\n○\nsudo ln -s\n../175B_run11.4.me_fp16.fsdp.gpf32.0.gelu.transformer_lm_megatron.nlay96.emb12288.lrnsin.\nnffc.nfatt.nfhd.bm_none.tps2048.gpt2.adam.b2_0.98.eps1e-08.cl1.5.lr7.5e-05.endlr7.5e-06.wu1\n000.dr0.1.atdr0.1.0emb_dr.wd0.05.ms8.uf1.mu143052.s1.ngpu992/tb/ run11.4\n\n[2021-11-07] Run 11.3: ECC Failure\nNo configuration changes.\n●\nECC error for node node-64 at 2285 updates\n○\nAction: Node put on drain\n○\nDownloaded 2250 checkpoint to:\n■\n/data/users/namangoyal/175B_run11.3.me_fp16.fsdp.gpf32.0.gelu.transformer_lm_meg\natron.nlay96.emb12288.lrnsin.nffc.nfatt.nfhd.bm_none.tps2048.gpt2.adam.b2_0.98.eps1\ne-08.cl1.5.lr7.5e-05.endlr7.5e-06.wu1000.dr0.1.atdr0.1.0emb_dr.wd0.05.ms8.uf1.mu143\n052.s1.ngpu992/checkpoint_1_2250\n●\nNew train.log path:\n/shared/home/namangoyal/checkpoints/175B/175B_run11.3.me_fp16.fsdp.gpf32.0.gelu.transformer_lm\n_megatron.nlay96.emb12288.lrnsin.nffc.nfatt.nfhd.bm_none.tps2048.gpt2.adam.b2_0.98.eps1e-08.cl1.\n5.lr7.5e-05.endlr7.5e-06.wu1000.dr0.1.atdr0.1.0emb_dr.wd0.05.ms8.uf1.mu143052.s1.ngpu992/train.lo\ng\ngit checkout 9b0645d7779a247c2861742b46112a131bf0a67b\nexport INCLUDED_HOSTS=node-[1-63,65-87,89-96,98-119,121-128]\nRESTORE_FILE=/data/users/namangoyal/175B_run11.3.me_fp16.fsdp.gpf32.0.gelu.transformer_lm_megatron.nlay96.emb1228\n8.lrnsin.nffc.nfatt.nfhd.bm_none.tps2048.gpt2.adam.b2_0.98.eps1e-08.cl1.5.lr7.5e-05.endlr7.5e-06.wu1000.dr0.1.atd\nr0.1.0emb_dr.wd0.05.ms8.uf1.mu143052.s1.ngpu992/checkpoint_1_2250/checkpoint_1_2250.pt\nRUN_ID=175B_run11.3\npython -m fb_sweep.opt.sweep_opt_en_lm_175b \\\n-n 124 -g 8 -t 1 \\\n-p $RUN_ID \\\n--checkpoints-dir /shared/home/namangoyal/checkpoints/175B/ \\\n--restore-file $RESTORE_FILE\n[2021-11-06] Run 11.2: clip 2.5 -> 1.5\n●\nResumed from 1K steps with peak LR of 7.5e-5\n●\nNode failure at ~1950 steps\n○\nThe good news is that slurm did try to requeue the job, so once we build the automatic\ndownload-latest-checkpoint-from-blob-and-resume functionality, this kind of problem will recover\nautomatically\n○\nWe also had some ECC errors from yesterday, so all of our 4 buffer nodes are bad\n(node-[61,88,97,120]). Manually recycled them this morning following the instructions at the top\nof the Cloud cluster admin doc.\n●\nRelaunching run on remaining 124 nodes\n○\nThere was a big ppl jump ~1430 steps, and the model was just recovering between 1430-1950\nsteps (after which the node failure stopped the job). Looking at gnorms, decided to roll back to\n1250 steps and lower clipping from 2.5 => 1.5.\n○\nMyle is AFK for the rest of the day. Will defer to Naman, Sam, Stephen and Susan to react to\njob failures if needed\n●\nKnown problems / TODOs\n\n○\nBlob storage URL seems to change across requeues, so we’ll need to address this (probably in\nfb_sweep/sweep/slurm.py) before we can automatically download checkpoints from blob and\nresume training\n○\nTensorboard seems to be bad at resuming training from a checkpoint. If we reuse the same\ntensorboard dir, then the graphs have a weird overlap. If we switch to a new save_dir\n(tensorboard dir), then we lose the previous training history :/\n■\nSolution for now is new tensorboard dir. It’s easy enough to copy the tfevent files if we\nlater decide to stitch things back together.\n●\nNew fairseq commit: 9b0645d7779a247c2861742b46112a131bf0a67b\n●\nNew train.log path:\n/shared/home/namangoyal/checkpoints/175B/175B_run11.2.me_fp16.fsdp.gpf32.0.gelu.transformer_lm_megatr\non.nlay96.emb12288.lrnsin.nffc.nfatt.nfhd.bm_none.tps2048.gpt2.adam.b2_0.98.eps1e-08.cl1.5.lr7.5e-05.\nendlr7.5e-06.wu1000.dr0.1.atdr0.1.0emb_dr.wd0.05.ms8.uf1.mu143052.s1.ngpu992/train.log\ncd\n/data/users/myleott/175B_run11.me_fp16.fsdp.gpf32.0.gelu.transformer_lm_megatron.nlay96.emb12288.lrnsin.nffc.nfat\nt.nfhd.bm_none.tps2048.gpt2.adam.b2_0.98.eps1e-08.cl2.5.lr0.0003.endlr\n1e-05.wu4000.dr0.1.atdr0.1.0emb_dr.wd0.05.ms8.uf1.mu143052.s1.ngpu992\n# Note that the blob storage URL seems to change across requeues,\n# need to get the latest one from train.log\nBLOB_URL=\"<<<SCRUBBED FOR RELEASE>>>\"\ncp --recursive --include-pattern \"checkpoint_1_1250-*.pt\" \"$BLOB_URL/*\" checkpoint_1_1250/\ncd /path/to/fairseq-py\nRESTORE_FILE=/data/users/myleott/175B_run11.me_fp16.fsdp.gpf32.0.gelu.transformer_lm_megatron.nlay96.emb12288.lrn\nsin.nffc.nfatt.nfhd.bm_none.tps2048.gpt2.adam.b2_0.98.eps1e-08.cl2.5.lr0.0003.endlr1e-05.wu4000.dr0.1.atdr0.1.0em\nb_dr.wd0.05.ms8.uf1.mu143052.s1.ngpu992/checkpoint_1_1250/checkpoint_1_1250.pt\n# increment the minor version to get a new save dir / tensorboard dir\nRUN_ID=175B_run11.2\nINCLUDED_HOSTS=node-[1-60,62-87,89-96,98-119,121-128] python -m fb_sweep.opt.sweep_opt_en_lm_175b \\\n-n 124 -g 8 -t 1 \\\n-p $RUN_ID \\\n--checkpoints-dir /shared/home/namangoyal/checkpoints/175B/ \\\n--restore-file $RESTORE_FILE \\\n--dry-run\nAfter launch:\nsudo scontrol update job=1286 TimeLimit=UNLIMITED\nsudo scontrol update job=1286 MailUser=<scrubbed> MailType=ALL\n[2021-11-05] Run 11.1: peak LR down to 7.5e-5\n●\nNote: This was actually run in the same train.log file as 11.0\n●\nHere we capped the warmup at 1000 steps, using a peak LR of 7.5e-5\n[2021-11-05] Run 11.0: LETS GO\n●\n2M bsz\n●\nFP32 Adam\n\n●\nTensor parallel (8x MP)\n●\nNew data - from experiment 29\n●\nLearned positional embeddings with sinusoidal  init\n●\nWeight decay of 0.05\n●\nLR of 3e-4, end LR of 1e-5\n●\nNo dropout on embeddings\n●\nNormformer (impact on grad norm is making earlier layers be more similar with later layers)\n●\nGradient pre-divide factor: 32 (Naman has been running with this)\n●\nClip (l2 norm): 2.5\n●\nFairseq commit: 52ac2df400bd3f42301438217151826b0853c43c\nLog path:\n/shared/home/namangoyal/checkpoints/175B/175B_run11.me_fp16.fsdp.gpf32.0.gelu.transformer_lm_megatron.nlay96.\nemb12288.lrnsin.nffc.nfatt.nfhd.bm_none.tps2048.gpt2.adam.b2_0.98.eps1e-08.cl2.5.lr7.5e-05.endlr7.5e-06.wu100\n0.dr0.1.atdr0.1.0emb_dr.wd0.05.ms8.uf1.mu143052.s1.ngpu992/train.log\nINCLUDED_HOSTS=node-[1-5,7-34,36-87,89-127] python -m fb_sweep.opt.sweep_opt_en_lm_175b \\\n-n 124 -g 8 -t 1 \\\n-p 175B_run11 \\\n--checkpoints-dir /shared/home/namangoyal/checkpoints/175B/\nAfter launch (slurm job id 1264):\nsudo scontrol update job=1264 TimeLimit=UNLIMITED\nsudo scontrol update job=1264 MailUser=<scrubbed> MailType=ALL\nLoss exploded between 1K and 1.25K steps. Decided to roll back to 1K steps and set peak LR\nNov 6, 2021\nto 7.5e-5: 76ae8349c0180daf90174c05d88ba7b6075cde51\ncd\n/data/users/myleott/175B_run11.me_fp16.fsdp.gpf32.0.gelu.transformer_lm_megatron.nlay96.emb12288.lrnsin.nffc.nfat\nt.nfhd.bm_none.tps2048.gpt2.adam.b2_0.98.eps1e-08.cl2.5.lr0.0003.endlr\n1e-05.wu4000.dr0.1.atdr0.1.0emb_dr.wd0.05.ms8.uf1.mu143052.s1.ngpu992\ncp --recursive --include-pattern \"checkpoint_1_1000-*.pt\" <<<SCRUBBED FOR RELEASE>>>\nRESTORE_FILE=/data/users/myleott/175B_run11.me_fp16.fsdp.gpf32.0.gelu.transformer_lm_megatron.nlay96.emb12288.lrn\nsin.nffc.nfatt.nfhd.bm_none.tps2048.gpt2.adam.b2_0.98.eps1e-08.cl2.5.lr0.0003.endlr1e-05.wu4000.dr0.1.atdr0.1.0em\nb_dr.wd0.05.ms8.uf1.mu143052.s1.ngpu992/checkpoint_1_1000/checkpoint_1_1000.pt\n# Note that the new save_dir is different, because we adjusted the peak LR.\n# I manually copied the contents of the old checkpoint dir (tb, train.log)\n# to the new save_dir for continuity\nINCLUDED_HOSTS=node-[1-60,62-87,89-119,121-127] python -m fb_sweep.opt.sweep_opt_en_lm_175b \\\n-n 124 -g 8 -t 1 \\\n-p 175B_run11 \\\n--checkpoints-dir /shared/home/namangoyal/checkpoints/175B/ \\\n--restore-file $RESTORE_FILE \\\n--resume-failed --dry-run\nAfter launch:\nsudo scontrol update job=1284 TimeLimit=UNLIMITED\nsudo scontrol update job=1284 MailUser=<scrubbed> MailType=ALL\n\nRun 10\n●\nTry to make run 9 identical to run 8, but with tensor parallelism\n●\n4M bsz\n●\nFP16 Adam\n●\nTensor parallel (8x MP)\n●\n--gradient-predivide-factor 11.1\n●\nFairseq commit: gshard-175b-run10\nLog path:\n/shared/home/namangoyal/checkpoints/175B/175B_run10.me_fp16.fsdp.gelu.transformer_lm_megatron.nlay9\n6.emb12288.bm_none.tps2048.adam.fp16adam.b2_0.98.eps1e-08.cl1.0.lr6e-05.wu290.dr0.1.atdr0.1.ms16.uf1\n.0.mu73832.s1.wd0.1.ngpu992/train.log\nINCLUDED_HOSTS=node-[1-7,9-88,90-108,110-127] python -m fb_sweep.opt.sweep_opt_en_lm \\\n-n 124 -g 8 -t 1 \\\n--weight-decay 0.1 --gradient-predivide-factor 11.1 \\\n--model-parallel-size 8 --distribute-checkpointed-activations --batch-size 16 \\\n-p 175B_run10 --model-size 175B_opt_h2_2021 \\\n--checkpoints-dir /shared/home/namangoyal/checkpoints/175B/\nOutcome: OOM during validation after 300 steps\nRun 9\n●\nSimilar to run 8, except:\n●\nTensor parallel (8x MP)\n●\n2M bsz\n●\nFP32 Adam\n●\nFairseq commit: gshard-175b-run9\nLog path:\n/shared/home/namangoyal/checkpoints/175B/175B_run9.me_fp16.fsdp.gelu.transformer_lm_megatron.nlay96.\nemb12288.bm_none.tps2048.adam.b2_0.98.eps1e-08.cl1.0.lr6e-05.wu580.dr0.1.atdr0.1.ms8.uf1.0.mu147665.\ns1.wd0.1.ngpu992/train.log\nINCLUDED_HOSTS=node-[1-7,9-74,76-78,80-127] python -m fb_sweep.opt.sweep_opt_en_lm \\\n-n 124 -g 8 -t 1 \\\n--weight-decay 0.1 --fp32-adam \\\n--model-parallel-size 8 --distribute-checkpointed-activations --batch-size 8 \\\n-p 175B_run9 --model-size 175B_opt_h2_2021 \\\n--checkpoints-dir /shared/home/namangoyal/checkpoints/175B/\nOutcome: loss exploded\nRun 8\n●\nRevert to Run 5 config, but with use-sharded-state\n●\nFairseq commit: gshard-175b-run8\n\nLog path:\n/shared/home/namangoyal/checkpoints/175B/175B_run8.fsdp.me_fp16.transformer_lm_gpt.nlay96.emb12288.\nbm_none.tps2048.adam.fp16adam.b2_0.98.eps1e-08.cl1.0.lr6e-05.wu290.dr0.1.atdr0.1.wd0.1.ms2.uf1.mu738\n32.s1.ngpu992/train.log\nINCLUDED_HOSTS=node-[1-7,9-74,76-78,80-127] python -m fb_sweep.opt.sweep_opt_en_lm \\\n-n 124 -g 8 -t 1 \\\n-p 175B_run8 --model-size 175B_opt_h2_2021 \\\n--checkpoints-dir /shared/home/namangoyal/checkpoints/175B/\nOutcome: Good! See workplace post.\nRun 7\n●\nRevert to Run 5 config, but with tensor parallelism\n○\nOne difference: run 5 had a 4M bsz, but here we are using 2M bsz\n●\nClipping 1.0\n●\nWeight decay 0.1\n●\nLonger warmup (290 steps)\n●\nThis also hardcodes skip_remainder_batch=True for Trainer.get_valid_iterator\n●\nFairseq commit: gshard-175b-run7\n●\nFairscale commit: 3584965cd4356c3c522e7d97aa13994cfa95ea5b\nLog path:\n/shared/home/namangoyal/checkpoints/175B/run7.me_fp16.fsdp.gelu.transformer_lm_megatron.nlay96.emb1\n2288.bm_none.tps2048.adam.b2_0.98.eps1e-08.cl1.0.lr6e-05.wu290.dr0.1.atdr0.1.ms8.uf1.mu73832.s1.ngpu\n992/train.log\nINCLUDED_HOSTS=node-[1-7,9-74,76-78,80-127] python -m fb_sweep.opt.sweep_opt_en_lm \\\n-n 124 -g 8 -t 1 \\\n-p run7 --model-size 175B_opt_h2_2021 --update-freq 1 \\\n--batch-size-per-gpu 8 --model-parallel-size 8   --dropout 0.1 \\\n--checkpoints-dir /shared/home/namangoyal/checkpoints/175B/ \\\n--distribute-checkpointed-activations --fp32-adam \\\n--save-interval 300 --validate-interval 300 --gradient-predivide-factor 11.1 --dry-run\nOutcome: seems to get stuck at 10.7 loss again, seems tensor parallel isn’t fixed\n2021-10-22: Run 6\n3ad9e48cc started Oct 22 1:01 AM\n●\nTensor parallelism\n●\nClipping 1.0\n●\nWeight decay 0.01\n●\n1x warmup\n●\nAdam Beta2 0.95\n●\nAdam Eps 1e-6\nscancel 385\nLog Path:\n\n/shared/home/namangoyal/checkpoints/175B/run6.me_fp16.fsdp.gelu.transformer_lm_megatron.nlay96.emb1\n2288.bm_none.tps2048.adam.b2_0.95.eps1e-06.cl1.0.lr6e-05.wu93.dr0.1.atdr0.1.ms8.uf1.mu147666.s1.wd0.\n01.ngpu992//train.log\nJob 378\n-\nBack to tensor parallel with cpu weight init (which seems to improve things, at least in my env).\n-\nweight-decay .01 because all the evidence I have points against increasing.\n-\nA little more save-interval, validate-interval to get more signal overnight.\nINCLUDED_HOSTS=node-[1-7,9-74,76-78,80-127] python -m fb_sweep.opt.sweep_opt_en_lm  -n 124 -g 8 -t 1 \\\n-p run6 --model-size 175B_opt_h2_2021 --update-freq 1 \\\n--batch-size-per-gpu 8   --model-parallel-size 8   --dropout 0.1 \\\n--max-update 147666 --checkpoints-dir /shared/home/namangoyal/checkpoints/175B/ \\\n--distribute-checkpointed-activations --fp32-adam --save-interval 500  --validate-interval 500\n--gradient-predivide-factor 11.1 \\\n--weight-decay .01\nOutcome: seems to get stuck at 10.7 loss again!\nRun 5\n●\nSimilar to Run4, but with some extra safety knobs\n●\nClipping 1.0\n●\nWeight decay 0.1\n●\n3x longer warmup\n●\nFairseq commit: d70abfba80ded958cde92af62fc505bbb99ea170\n●\nNew log path:\n/shared/home/namangoyal/checkpoints/175B/175B_run5.fsdp.me_fp16.transformer_lm_gpt.nlay96.em\nb12288.bm_none.tps2048.adam.fp16adam.b2_0.98.eps1e-08.cl1.0.lr6e-05.wu290.dr0.1.atdr0.1.wd0.1.\nms2.uf1.mu73832.s1.ngpu992/train.log\n●\nRelaunched with: INCLUDED_HOSTS=node-[1-7,9-74,76-78,80-127] python -m\nfb_sweep.opt.sweep_opt_en_lm -n 124 -g 8 -t 1 -p 175B_run5 --model-size 175B_opt_h2_2021\n--checkpoints-dir /shared/home/namangoyal/checkpoints/175B/\nOutcome:\nLoss went down for 300 steps, validation ran.\nHtop & nvidia-smi looked fine on a random node 2 hours after the last log.\nStarted stalling Oct 21, 2021 20:40 -> Oct 21, 2021 23:34 (killed by Sam)\nUpdate by Myle, 10/22 @ 8am: it seems I forgot to add use-sharded-state, so it was trying to consolidate state\non a rank 0! PR with fix pushed here: #2490\nRun 4\n●\nRevert to Run1 config on gshard stable with zero3\n●\nFairseq commit: 53d993880508f1ea3272b406e8ecc99298305c7b\n●\nFairscale commit: 8acbec718f3c70a6b9785470bb9e05cd84fc3f8e\n●\nNew log path:\n/shared/home/namangoyal/checkpoints/175B/175B_run4.fsdp.me_fp16.transformer_lm_gpt.nlay96.em\n\nb12288.bm_none.tps2048.adam.fp16adam.b2_0.98.eps1e-08.cl0.0.lr6e-05.wu96.dr0.1.atdr0.1.wd0.01.\nms2.uf1.mu73832.s1.ngpu992/train.log\n●\nRelaunched with: INCLUDED_HOSTS=node-[1-7,9-74,76-78,80-127] python -m\nfb_sweep.opt.sweep_opt_en_lm -n 124 -g 8 -t 1 -p 175B_run4 --model-size 175B_opt_h2_2021\n--checkpoints-dir /shared/home/namangoyal/checkpoints/175B/\nOutcome: Loss exploding again\nRun 3\n●\nClipping 1.0\n●\nAdam beta2 0.95\n●\nAdam eps 1e-6\n●\n--fp32-reduce-scatter\n●\nNew fairseq commit hash: 6b629aec74a917f4fbaf3b208f261aa4fc375c84\n●\nThis was buggy (don’t use): Also changed fairscale a08a523f6b2fb401f1e12522af9160673fe41e32\n●\nNew log path:\n/shared/home/namangoyal/checkpoints/175B/175B_third_attempt.2.me_fp16.fsdp.gelu.transformer_lm\n_megatron.nlay96.emb12288.fp32reduce.bm_none.tps2048.adam.b2_0.95.eps1e-06.cl1.0.lr6e-05.wu1\n93.dr0.1.atdr0.1.wd0.1.ms8.uf1.mu147666.s1.ngpu992/train.log\n●\nRelaunched with: INCLUDED_HOSTS=node-[1-7,9-74,76-78,80-127] python -m\nfb_sweep.opt.sweep_opt_en_lm     -n 124 -g 8 -t 1     -p 175B_third_attempt --model-size\n175B_opt_h2_2021     --update-freq 1     --batch-size-per-gpu 8   --model-parallel-size 8   --dropout 0.1\n--max-update 147666       --checkpoints-dir /shared/home/namangoyal/checkpoints/175B/\n--distribute-checkpointed-activations --fp32-adam  --tensor-parallel-init-model-on-gpu  --save-interval\n300  --validate-interval 300 --gradient-predivide-factor 11.1 --warmup-updates 193\nOutcome: Sam discovered that model parallel branch doesn’t converge even for 125M model\n[Undated] Run 2\n●\nIncrease weight decay to 0.1\n●\nLog path:\n/shared/home/namangoyal/checkpoints/175B/175B_first_attempt.me_fp16.fsdp.gelu.transformer_lm_m\negatron.nlay96.emb12288.bm_none.tps2048.adam.b2_0.98.eps1e-08.cl0.0.lr6e-05.wu193.dr0.1.atdr0.\n1.wd0.1.ms8.uf1.mu147666.s1.ngpu992/train.log\nOutcome: loss plateaus, fails to go below 10.7\n2021-10-20:  Run 1\n20th October\nFairseq: 468a050d4996a4f18c99519c0cfc2d9512f5ab6f\nFairscale: 3584965cd4356c3c522e7d97aa13994cfa95ea5b\nMegatron submodule: b6a6ed16ae0a6ff5a57089f66f13a617e1390d1f\nConda env on cloud:\n\nCmd: python -m fb_sweep.opt.sweep_opt_en_lm     -n 124 -g 8 -t 1     -p 175B_first_attempt --model-size\n175B_opt_h2_2021     --update-freq 1     --batch-size-per-gpu 8   --model-parallel-size 8   --dropout 0.1\n--max-update 147666       --checkpoints-dir /shared/home/namangoyal/checkpoints/175B/\n--distribute-checkpointed-activations --fp32-adam  --salloc  --tensor-parallel-init-model-on-gpu  --save-interval\n300  --validate-interval 300 --gradient-predivide-factor 11.1 --warmup-updates 193\nNodes: node-[1-7,9-74,76-78,80-127]\nlog:\n/shared/home/namangoyal/checkpoints/175B/175B_first_attempt.me_fp16.fsdp.gelu.transformer_lm_megatron\n.nlay96.emb12288.bm_none.tps2048.adam.b2_0.98.eps1e-08.cl0.0.lr6e-05.wu193.dr0.1.atdr0.1.wd0.01.ms8.u\nf1.mu147666.s1.ngpu992/train.log\nOutcome: Loss kinda exploded. Trying weight decay next\nKitchen sink: Analysis of Exp 21--29\nThis keeps record of some of our kitchen sink findings.\nDescription of experiments:\n●\n21: New data after fixing encoding issues and collapsing newlines\n●\n22: Old data + pushift.io (control)\n●\n23: Subset of new data closest to the “classic” roberta data. (BookCorpus + CC + OWT2 + wiki +\nccnews2 + pushift.io + stories)\n●\n24: Ablate out BookCorpus from 23. So new versions of (CC + OWT2 + wiki + ccnews2 + pushift.io\n+ stories)\n●\n25: Safer version of all new data (cc + pg19 + hn + OST + OWT2 + USPTO + wiki + ccnews +\npushift.io + stories)\n●\n26: Add in the FAIR version of BookCorpus from 23. So new versions of (BookCorpusFair + CC +\nOWT2 + wiki + ccnews2 + pushift.io + stories)\n●\n27: Safer versions of all new data (BookCorpusFair + cc + pg19 + hn + OST + OWT2 + USPTO +\nwiki + ccnews + pushift.io + stories)\n●\n28: learned positional embeddings vs sinusoidal (with caveats)\n●\n29: Manually cleaned version of corpora via handcrafted regexes\nExp 21 -- 23\nWanted to see what was the “dangerous” corpora in 21. Performed this experiment and found BookCorpus,\nEuroParl, and Pg19, Enron, and EuroParl all look iffy for gnorms\nDM and SX look kinda sketchy for PPL reasons\n\nExp 23, 24 and 25 (Drop out BookCorpus)\nOur predominant goal here is to consider whether the “safe” version of BookCorpus is really safer.\nTo that end, our main comparison is 23 vs 24.\n\nIn our judgment, 24 was a little bit less spiky than 23, indicating that we feel comfortable that the main problem\nwith our “new data” is the BookCorpus coming from the Pile.\nWe can also compare 24 vs 25 to see if we managed to exclude the dangerous corpora:\nIn general, 24 looks fairly smooth (after you consider this is log interval 1), and 25 still has some issues.\nAt this point, we killed them early and grabbed a copy of BookCorpus from the old cluster.\n\nExp 26 and 27: Adding in Book Corpus\nOn stability via only the new books corpus (24 vs 26)\nOverall they seem about the same if one were to average 26 at a smaller log interval.\nExp 29: Manually cleaned up corpora\nLearned Embeddings\nExp 27 vs 28\nThese two runs use the same dataset, and predominantly differ choices for positional embeddings. In\nparticular, 27 uses sinusoidal embeddings, while 28 uses LPE + some tricks around initialization and scaling.\nSee the PR for 28 for an exact description.\nThey are neck to neck, but 27 does come out on top after 45k updates.\n\nWe also observe that the gnorms of 28 are a bit spikier:\nOncall Debugging\nAKA: Help! I’m oncall, it’s 3am, and everything is on fire!\nPhilosophy\nWhen you are on call, you are ON CALL. You are fully expected to fulfill the responsibilities within a reasonable\ntime frame.\nBut when you are not on call, you are NOT on call. Try to leave monitoring and resolution to the oncall as much\nas you can. It both prevents burnout, and the oncall from growing lazy.\nIf you have conflicts or cannot fulfill your shift, that’s okay! Just make sure to arrange a trade with someone.\n\nRemember to document your actions. This helps people understand what you did without relying on pinging\nyou.\nResponsibilities\n1.\nRegularly monitor jobs (by checking tensorboard and tailed logs) for aberrant behavior.\na.\nRun `./scripts/cloud/monitor_train_log.py --mailto <scrubbed>@fb.com --slurm-jobid JOBID\n--modified-threshold 900` to get emails about aberrations (the email listed will send to the\noncall) and enable auto-recovery.\nb.\nSee links at the top of the document for the latest tensorboard\n2.\nPerform cluster maintenance in the case of hardware failures\na.\nMost of this is done via fixmycloud and the auto-recovery monitor now.\nb.\nIdentify the broken node\nc.\nRelaunch the slurm job with it replaced using one of the spares\nd.\nDrain the bad node, note it in the Spare Node Tracker table.\ne.\nPotentially replace the node.\n3.\nKeep nodes from being idle and ensure that something productive is always running.\na.\nPreferably the main job.\nb.\nIf it looks unrecoverable, execute any pre-documented Plan B choices\nc.\nBias for action. Make discretionary decisions if other members are not available.\nOnboarding & Gotchas\nThese are the instructions for getting onboarded into the cluster to fulfill your oncall duties.\n●\nGetting onto Cloud and setting up environment:\n○\n< link to cloud cluster instructions doc >\n○\nBegin following this including the “getting on” and “setup environment”\n○\nYou can have a tmux session on cloud, but Stephen usually just keeps tmux on his devfair and\nssh’s in from the devfair.\n○\nOnce you’re onboarded, make sure you have the ability to run “sudo”. You will need it, a lot.\n○\n`git clone git@github.com:facebookresearch/fairscale` and do install instructions\n■\nMake sure to be on the right checkpoints for `fairscale` and `fairseq-py` (Look at the\nmost recent copy/paste of run+install instructions in the oncall log; scripts won’t exist\notherwise)\n○\nAlso make sure to once run “python3 scripts/cloud/ssh.py” Which will ensure your ssh is\nconfigured to disregard host keys, as our nodes are re-imaged and change keys frequently\n●\nGotcha: Avoid doing anything compute-heavy on the login node. It is a very light machine and a shared\nresource. Heavy operations on it can slow down EVERYONE.\n●\nGotcha: most of our oncall tooling expects to be run from the login node.\n○\nHowever, sometimes it’s useful to directly log into nodes when trying to debug a failure. Just\n“ssh node-X”\nQuestions\n●\nIs it safe to assume the checkpoint last used in the log is the right one?\n○\nWe are constantly writing checkpoints to blob storage. Checkpoints are not written to any sort of\nshared disk (like /shared/home)\n\n○\n(We actually write them to local SSD on every worker, and then initiate a separate upload on\nevery worker, which makes it very fast).\n○\nWhen launched without `--restore-file`, our code will automatically fetch the latest checkpoint\nfrom blob and resume from that.\n○\nBut in the case of things like loss exploding, or more radical changes, you may need to\nmanually provide a `--restore-file` in order to resume from that checkpoint.\n○\nSee entry “2021-12-06 8:30am ET: Lowering LR and launching 12.46” for the latest example of\ndoing that.\n●\nWhere will I find log files that I need to tail?\n○\n/shared/home/namangoyal/checkpoints/175B/ contains many folders. Each folder has its “12.46”\nor “12.37” folder or what not.\n●\nWhat is this BLOB_PREFIX and BLOB_AUTH stuff?\n○\nBLOB_AUTH contains authentication variables so that Cloud’s blob storage knows what\naccount we are using. It generally will never change.\n○\nBLOB_PREFIX is roughly the bucket/folder that we are uploading to. Due to limitations on the\nCloud side, sometimes we need to switch to new buckets.\n■\nThe BLOB_PREFIX also contains a “run id” in it generally speaking. We only bump this\nwhen we do something that could potentially clobber checkpoints. Therefore you will\noften find yourself using BLOB PREFIXES with RunIDs that are not 1:1 with the\nrun you are launching.\nRun is stuck in loop of “lower loss scale”\nUh oh. The loss exploded. We don’t know why.\nRemember to document your actions in the logbook.\nActions:\n1.\nDon’t panic.\n2.\nPing the group chat to discuss potential options. If no one responds within 10 minutes, you will have to\nmake a decision by yourself. Letting nodes idle costs $2500/hour so it is strongly discouraged.\na.\nBias for action, but also double check your commands to avoid catastrophic losses (clobbering\ncheckpoints, etc). As expensive as it is, an extra idle hour is still cheaper than having to redo\nmultiple days.\ni.\nIt’s generally a good idea to bump your blob RUNID variables. Search this document for\nOLD_BLOB_PREFIX for the most recent examples of that.\n3.\nGo read the logs and check the tensorboard.\na.\nWhat happened to Gnorm in the updates preceding the explosion? Did we see a large gnorm\nspike? (~0.7 or above)?\nb.\nWhat about actv norm, pnorms, loss, etc? Did those spike too?\nc.\nWhat was the loss scale doing during the moments before? Did it drop very rapidly (20-50\nupdates)? Or was it a slow gradual fall out?\n4.\nGo read the log book. Was there a plan B put in place already? If so, execute it. If not, you will have to\nmake some decisions yourself.\n5.\nActions you might take:\na.\nLet it keep running another hour or so. Sometimes we recover. This is unlikely if you are seeing\n40+ loss scale flatlines.\n\nb.\nRequeue. This is a generally safe option, unless the logbook tells you otherwise. This will restart\nthe job from the last checkpoint and just pray for the best. Frequently this just kicks the can\ndown the road 2-6 more hours, but that can buy you some time to sleep and discuss.\nc.\nMore extreme measures are then suitable. Preferably these should be group discussion, but\nthat’s not always possible.\nd. Taking extreme measures should always cause you to bump the run ID and the\nBLOB_URL.\ne.\nPrior actions we have taken as examples:\ni.\nLowering the LR, usually by some factor (0.9x or 0.75x).\n1.\nThis is our current default mode of action.\n2.\nAs of Dec 7, this seems to buy us a couple days.\nii.\nImplementing clamping of activations (Not actually tried yet, but a plan B)\niii.\nLowering the clip 1.0 -> 0.3.\niv.\nHot swap the optimizer from Adam to SGD. This has gone poorly and is not currently\nrecommended.\nWPS has dropped a lot\nAs of 2021-12-02, WPS is consistently around ~100k and does not fluctuate more than 1%, except during\nvalidation.\nIf you observe WPS dropping below ~90k or more for a sustained period (>20 minutes), we probably have an\ninfiniband problem.\nActions:\n1.\nRun health checks only on idle nodes. Identify a suitable replacement.\na.\nIf you cannot, then you should leave the job as is. Better to run slow than not at all.\n2.\nPause the job\n3.\nRun global health checks. You are particularly looking out for NCCL issues this time.\n4.\nReplace the node and resume the job.\nJob is Hanging\nRemember to document your actions in the logbook.\nPossible Actions:\n1.\nDo nothing. Auto recovery in monitor.py may handle this for you. You should receive an email\nregarding success/failure.\n2.\nRead the train.log. Check the timestamp. Remember it’s in UTC time (run “date” to see what time the\ncomputer thinks it is now)\n3.\nCheck the stderr. Is there a mention of a CUDA or NCCL Failure?\na.\nA node has probably kicked the bucket. Hopefully the auto-recovery script has kicked in. Check\nits logs (which should be coming in via email)\nb.\nIf not, pause the job (see below) and follow the Health Checks guidelines.\n4.\nIs there really nothing in the stderr? Let’s check on the CPUs, the GPUs, and the Disk to see if there\nare active bottlenecks we just need to be patient for.\na.\nCPU: Nodes may be stuck doing some tokenization. ssh into one and run htop.\ni.\nIf many CPUs are used or spiking, then just be patient. We may be tokenizing.\n\nii.\nIf you see ~16 processes pegged at 100% but the other 80 unused, you should continue\ndown this checklist.\nb.\nGPU: run “watch -n0.5 nvidia-smi”. Watch for about a minute. Are all the GPUs at 100%? Is the\ntemperature 40C or 72C? Is the power usage ~70W or or ~385W?\ni.\nHotter and higher power mean the GPU is active. Let it be.\nii.\nCooler and low power means we are hanging on communication, and probably have a\nbad node.\nc.\nDisk: Run “sudo iotop” and watch.\ni.\nIs the top process sitting at just 0mb/s read/write? We are not disk bottlenecked.\nii.\nIs it very high? We might be doing I/O or writing a checkpoint.\n5.\nFull health checks are called for.\na.\nPause or cancel the job\nb.\nRun fixmycloud all\nc.\nReplace the bad node(s) with good nodes in the INCLUDED_HOSTS environmental variable.\nd.\nRelaunch the job\nPerforming health checks\nUse fixmycloud to check quality across the entire cluster\n# check only idle, unused nodes\npython scripts/cloud/fixmycloud.py idle\n# check all nodes. Should only be done when a job is not running\npython scripts/cloud/fixmycloud.py all\nIt will output information about potential warnings and possibly suggested mitigations.\nYou may wish to run partial checks, which can be done by calling individual scripts rather than the fixmycloud\nscript. The health checks include:\n●\nnvidia-smi checks. These do not interfere with running nodes, and are safe to run on active nodes.\n●\ngpu-burn checks. These DO interfere with running nodes, and should not be used\n●\nnccl checks. These require the node be undrained and idle, and require you to check 3+ nodes at\nonce.\n●\nblocklist checks. These check if a node is a known bad host, and are safe to run on active nodes.\nI need to reprovision/replace a node\nWe no longer replace nodes ourselves, and instead CSP does it for us. As of 2022-01-13, the fixmycloud tool\nshould upload a diagnostics dump to a public Blob bucket and CSP is responsible for monitoring this bucket\nactively.\nBefore you reprovision a node it’s a good idea to get the metadata and hardware ID for it because you need\nto report it to CSP as a bad node. To get the metadata:\n1.\nssh to the node you plan to reprovision. Some notes about hostnames:\n○\nEach node has multiple hostnames, any of which can be used with ssh:\n■\nnode-XXX: this is the alias used by Slurm\n■\nip-XXXXXX: this is the alias used by Cloud\n■\n10.30.4.XXX: this is the internal IP address of the node\n\n■\nbuoXXXX: this is another alias used by Cloud\n○\nYou can resolve all of these to node-XXX format using the “scripts/cloud/find_host.py” script.\n2.\nRun the scripts/cloud/gather_diagnostics.py script and save the blob URL that is printed and send to\nCSP.\nUsing the command line\nWarning: You probably don’t want this unless you really know what you’re doing. CSP is supposed to\nbe handling this for us.\n# reprovision node node-2\n# warning: you probably don't want to do this on idle or utilized nodes\npython scripts/cloud/replace_node.py node-2\nMake sure you update the “Spare Node Tracker” to indicate it is being replaced and update the timestamp.\nI need to mark a node as unusable\nDraining nodes is useful when you want to mark them as unusable by SLURM. You can then take other\nmitigation actions (like rebooting a node, or reprovisioning it). As of 2021-12-02, nodes with serious errors\n(marked as error/fatal/critical in fixmycloud) should be drained and reported to CSP via chat using the\ndiagnostics script.\n# drain node node-2, marking it unusable\npython scripts/cloud/slurm.py drain node-2 --reason=”Why I am draining”\nYou may also undrain a node similarly with “slurm.py undrain [node]”\nRebooting a node\n$ ssh node-X\n$ sudo reboot\n# [ you will be logged out ]\n$ ping -O node-X\n# [ wait until you start getting \"no answer yet\"]\n# [ start monitoring on cloud UI]\n# [ wait until pings return ]\n# [ then wait an extra minute ]\n$ ssh node-X # assert you can get back in. do a health check\nIf it takes longer than say, 20 minutes, you should probably switch to replacing the node (see above).\nI want to hot swap a node in a running job without explicitly re-launching\nNote: Monitor script should now do this automatically for you.\nThis is useful if you want to correct for a hardware failure without changing any hyperparameters, and just\nresume training.\n\nFigure out the job ID using squeue. The main job is going to be the one with 124 nodes. Let’s assume the job\nID is 2589.\n# these two commands together pause the job\nsudo scontrol requeue job=2589\nsudo scontrol hold job=2589\n# find whatever node(s) caused the issue and drain it\npython scripts/cloud/slurm.py drain node-25\n# OPTIONAL: you can explicitly tell SLURM which nodes to use\n# However, if you are only interested in using any idle node, you can skip\nsudo scontrol update job=2589 NodeList=node-[1-2,4-24,27-36,38-52,54-72,74-107,109-113,115-131,147]\n# allow it to resume\nsudo scontrol release job=2589\n# note the job will be in a \"BeginTime\" held state for 1 minute. This is done intentionally to allow for cleanups on\nshutdown.\nTable of Contents/Index\nInstructions\nSpare Node Tracker\n175B Log\n2022-01-06 15:47 ET [Everyone]\n2022-01-05 14:10 [Stephen + Mikel]\n2022-01-06 10:30ish [Susan + Mikel]\n2022-01-04 15:22 ET [Sam]\n2022-01-03 05:10 ET [Daniel]\n2022-01-02 17:26 ET [Stephen]\n2021-12-31 02:53 ET [Punit]\n2021-12-31 12:00 ET [Moya]\n2021-12-30 17:00 ET [Moya] - nvidia_smi.py bug fix; machine check\n2021-12-30 09:30 ET [Stephen] - Job recovery failed\n2021-12-29 13:40 ET [Stephen] - Cluster maintenance and relaunch\n2021-12-28 16:00 PT [Susan] - cluster maintenance\n2021-12-28 9:10 ET [Myle/Stephen] - manually recovered job\n2021-12-27 9:25 ET [Myle] - postmortem on autorecovery issue\n2021-12-27 13:19 CET [Mikel] - restarting autorecovery script\n2021-12-25  04:18 ET [Myle] - starting improved autorecovery script\n2021-12-25 09:48 ET [Myle] - RCA on autorecovery failure\n2021-12-25 08:49 ET [Stephen]\n2021-12-25 06:25 ET: [Daniel/Susan]\n2021-12-24 12:40 ET: Auto-recovery script\n2021-12-24 10:00 ET: Kurt\n2021-12-23 6:00 PM ET: Kurt\n2021-12-23 8:30 ET: Myle\n2021-12-23 7:00 ET: Stephen\n\n2021-12-22 3:30 pm ET: [Myle] new oncall\n2021-12-21 4:30 pm ET: [Moya] Kick off train 12.55\n2021-12-21 (morning until 3 pm ish) [Stephen] Omicron Sev\n2021-12-21 5:30am ET: [Susan] Node down, restart from 91,250 with lower LR - Run 12.53, 12.54\n2021-12-19 12pm ET: Crossing the epoch boundary\n2021-12-20 12:12 AM PT: [Punit] Node down - requeue for 12.52a\nNCCL failures investigation\nResuming job with new host list\nEnable tensorboard\n2021-12-17 15:34 ET: [Daniel] Node down - requeue for 12.52a\n2021-12-16 12:15 ET: increased job time limit from 3 days to unlimited:\nsudo scontrol update job=2606 TimeLimit=UNLIMITED\n2021-12-14 18:30 ET: [Kurt] Drain a few nodes\n2021-12-14 13:30 ET: [Moya] Scancel + Resubmit\n2021-12-14 03:30 ET: [Susan] Requeue\n2021-12-13 14:00 ET: Reverse Shadow and Oncall onboardings\n2021-12-13: Preemptive Plan\n2021-12-13 11:49 ET: [Stephen] Bumping timelimit\n2021-12-11 07:53 ET: [Stephen] Cluster Maintenance\n2021-12-11 02:52 PT: [Susan] Noticed IB issues/lost GPU.\n2021-12-10 23:14: [Stephen] 12.51 Resuming\n2021-12-10 22:42: [Stephen] 12.50 Resuming\n2021-12-10 20:58 ET: [Stephen] The ablation\n2021-12-10 05:52 ET: [Stephen] Cluster maintenance\n2021-12-09 16:00 PT: [Susan] Run 12.49, restart due to NCCL errors\n2021-12-09 10:45am PT: Provisioning error in Cloud for node 124\n2021-12-09: Megatron v2.6 Debrief + CSP Sync\n2021-12-08 8:55pm ET: run12.48: relaunch checkpoint_18_54250\n2021-12-08 TBD: Analysis from 12.47.myle\n2021-12-08 05:05pm ET: GPU failure; launch debug run with Myle’s env\n2021-12-08 04:00am PT: Checking in\n2021-12-07 10:59pm ET: RuntimeError: CUDA error: CUBLAS_STATUS_EXECUTION_FAILED when\ncalling\n2021-12-07 10:48am ET: ECC error, requeueing 12.46\n2021-12-06 8:30am ET: Lowering LR and launching 12.46\n2021-12-06 05:13 PT: Job Hanging - 3 Machines Down\n2021-12-06 05:00 PT: Grad norm spiking, ppl trending up\n2021-12-05 9pm ET: Requeue after GPU error\n2021-12-05 18:30 ET: Poking through dmesg to see if we can find where we hung\n2021-12-05 12:15pm ET: Requeueing 12.44 and 12.45\n2021-12-05 05:35 PT: Checking on 12.44\n2021-12-05 02:24 ET: Side experiment\n2021-12-05- 00:00 ET: Loss scale exploding 2 - 12.44\n2021-12-04 20:24 ET: Loss scale exploding\n2021-12-04 5:35am ET: Launch of 12.43: Fix blob upload\n\n2021-12-03, 10:35pm ET: DO NOT REBOOT OR REPROVISION ANY NODES UNTIL FURTHER NOTICE\n2021-12-03\n2021-12-03 7:20am ET: Launch of 12.42: Switch back to Adam\n2021-12-02 17:16 ET: Fake SGD debacle: Debrief discussion\nSummary of events and mitigations\nNext paths\n2021-12-02 16:08 ET: Launch of 12.41: Switching to true Vanilla SGD\n2021-12-02 10:25am ET: Launch of 12.40: Intended to be fake SGD with lower learning rate [WARNING:\nSee 2021-12-02 17:16 ET: Debrief on why that may not be]\n2021-12-01 1:30pm ET: Launch of 12.39\nAnalysis of 12.38\nLaunch of 12.39\nDiscussion\nAnalysis\n2021-12-01 8:39am ET: 12.38 True Adam with Lower LR\n2021-12-01 2:21am ET: [Stephen oncall] Run 12.37 [WARNING: See 2021-12-02 17:16 ET: Debrief on why\nit may not be SGD]\n2021-11-30 7:24pm ET: [Stephen oncall] Run 12.37 Manual requeue of 12.36. [WARNING: See 2021-12-02\n17:16 ET: Debrief on why it may not be SGD]\n2021-11-20 7:24pm ET: [Stephen oncall]\n2021-11-30 10:10am PT: 12.36 restart from 37k, SGD mimicking  [WARNING: See 2021-12-02 17:16 ET:\nDebrief on why it may not be SGD]\n2021-11-30 9:00am PT: 12.35 restart from 37k, SGD mimicking  [WARNING: See 2021-12-02 17:16 ET:\nDebrief on why it may not be SGD]\n2021-11-30 9:00am ET: 12.34 requeue\n2021-11-29 7:43pm PT [Susan]: 12.34 restart\n2021-11-30 7:43pm PT [Susan]: 12.33 requeue\n2021-11-28 6:34pm ET [Stephen]: 12.33\n2021-11-28 5:52pm ET [Stephen]: 12.32\n2021-11-28 12:28pm ET [Stephen]: 12.31\n2021-11-28 10:09am ET [Stephen]: 12.30\n2021-11-28 9:41am ET [Stephen]: 12.29\n2021-11-28 3:20am ET [Stephen]: 12.28\n2021-11-28 1:50am ET [Stephen]: 12.27\n2021-11-27 11:39 ET [Stephen]: Run 12.26\n2021-11-27 6:10pm PT: Run 12.25 [Susan restart]\n2021-11-27 10:59am PT: Run 12.24 [Myle rerunning job, but AFK rest of day]\n2021-11-26 9:47am ET [Stephen managing cluster]\n2021-11-25 8:53am ET [Susan]: Run 12.23\n2021-11-25 11:35am ET [Myle]: Run 12.22\n2021-11-25 11:20am ET: Run 12.21 (requeue)\n2021-11-24 11:18pm ET [Susan]: Run 12.21\n2021-11-24 10:40pm ET [Susan]: Run 12.20\n2021-11-24 3:30pm ET [Susan]: Run 12.19\n2021-11-24 2:10pm ET [Susan]: Run 12.18\n2021-11-24 1:00pm ET [Susan]: Run 12.17\n\n2021-11-23 10:50am [Myle]: Run 12.16\n2021-11-22 7:45am [Myle]: Run 12.15\n2021-11-21 4:15pm [Myle]: Run 12.14\n2021-11-21 3pm [Myle]: Run 12.13\n2021-11-21 Analysis of 12.X series\n2021-11-20 9:30pm [Myle]: Run 12.12\n2021-11-19 9:30am [Myle]: Run 12.11\n2021-11-18 List of issues for Cloud Complaints\n2021-11-18 5:30pm [Stephen]: Notes from 12.10\n2021-11-18 3pm [Stephen]: Run 12.10\n2021-11-17 11pm [Myle]: Run 12.09\n2021-11-16 11pm [Myle]: Run 12.08\n2021-11-14 9:45pm [Myle]: Run 12.07\n2021-11-12 10:30pm [Myle]: Run 12.06\n2021-11-12 7pm [Susan]: Run 12.05\n2021-11-12 6pm [Susan]: Run 12.04\n2021-11-12 3pm [Susan]: Run 12.03\nAnalysis of Run 12.02 [Susan]\n2021-11-11 11pm [Susan]: Run 12.02\n2021-11-11 5:40pm [Susan]: Run 12.01\n2021-11-11 5:30pm [Susan]: Run 12.00 - Lost a GPU on node-7, restarting.\n2021-11-10 5pm: Run 11.10\nAnalysis of 11.9\nDecision\nLaunch steps for Run 11.10\n[2021-11-10]: Run 11.9: Lowered LR to 6e-5, match exp 11.6 otherwise\nAnalysis of 11.8\nDecisions for 11.9\nLaunch steps for 11.9\n[2021-11-09]: Run 11.8: Rolling back weight decay, start from 4750\nAnalysis of 11.7\nDecisions for 11.8\nLaunch steps for 11.8\n[2021-11-09]: Run 11.7: Changing tons of stuff, start from 4750\nAnalysis\nLaunch steps for 11.7\n[2021-11-09]: Run 11.6: Starting to skip batches\nEmergency meeting notes\nAnalysis of 11.6\n[2021-11-08] Run 11.5: clip 1.5 -> 1.0\nAnalysis of 11.5\nAnalysis of 11.4\n[Undated] Run 11.4: Changed validation freq\n[2021-11-07] Run 11.3: ECC Failure\n[2021-11-06] Run 11.2: clip 2.5 -> 1.5\n\n[2021-11-05] Run 11.1: peak LR down to 7.5e-5\n[2021-11-05] Run 11.0: LETS GO\nRun 10\nRun 9\nRun 8\nRun 7\n2021-10-22: Run 6\nRun 5\nOutcome:\nRun 4\nRun 3\n[Undated] Run 2\n2021-10-20:  Run 1\nKitchen sink: Analysis of Exp 21--29\nDescription of experiments:\nExp 21 -- 23\nExp 23, 24 and 25 (Drop out BookCorpus)\nExp 26 and 27: Adding in Book Corpus\nExp 29: Manually cleaned up corpora\nLearned Embeddings\nExp 27 vs 28\nOncall Debugging\nResponsibilities\nOnboarding & Gotchas\nRun is stuck in loop of “lower loss scale”\nWPS has dropped a lot\nJob is Hanging\nPerforming health checks\nI need to reprovision/replace a node\nI need to mark a node as unusable\nRebooting a node\nI want to hot swap a node in a running job without explicitly re-launching\nTable of Contents/Index\n"
    },
    {
      "arxiv_id": "https://raw.githubusercontent.com/facebookresearch/metaseq/main/projects/OPT/chronicles/OPT_Baselines_Logbook.pdf",
      "full_text": "OPT Baselines Logbook\n●\nAll baselines created using <redacted>.\n●\nOccasionally hyperparameters were set differently for one model.\n●\nAll use the same data as the 175B model\nTraining Log\n2022-05-17 [Susan] WE ARE DONE!!!\n●\nOne more pdsh run to push on the checkpoints to blob. Hooray!\nCopying blobs over to final blob path:\nazcopy copy \"<redacted>\" <redacted>\" --include-pattern \"checkpoint_49_143000*.pt\" --recursive\nFinal view of run16-43 (missing run36 from home dir issue and no backups):\nTensorboard\n●\nTakeaways:\n○\n66B seemed more difficult to train with 2M batch size than 175B with 2M batch size\n■\nBF16 likely would work better here (but doesn’t explain the 66B vs 175B instability)\n○\nLR may have been too low for too long, despite validation on wikipedia_en continuing to\n“improve”\n\n○\nTensorboard with wikipedia_en validation ppl\n○\nAdjusting dynamic loss scaling window (to be a function of loss scalar value) seemed to have\nhelped with stability, but won’t be needed at all when switching to bf16\n2022-05-15 [Susan] Recover failed uploads, restart with LR at 6e-6\nPDSH_RCMD_TYPE=ssh pdsh -w hpc-pg0-[9-12,14-31,34-43,45-48,50-69,71-78]\n'/shared/home/susanz/bin/azcopy copy \"<redacted>/*.pt\" \"<redacted>\"'\n●\nKicked off run 43\nBLOB_PREFIX1=\"<redacted>/66B_run42\"\nBLOB_PREFIX2=<redacted>/66B_run43\"\nRESTORE_FILE=\"${BLOB_PREFIX1}/checkpoint_48_137750.pt?${BLOB_AUTH}\"\nRUN_ID=66B_run43\n./<redacted> \\\n-n 64 -g 8 -t 1 \\\n-p $RUN_ID \\\n--azure \\\n--model-size 66b \\\n--checkpoints-dir /shared/home/susanz/checkpoints/66B/ \\\n--local-checkpoints-dir /mnt/scratch/$USER/checkpoints/$(date +%Y-%m-%d).$RUN_ID \\\n--full-azure-upload-path \"${BLOB_PREFIX2}/?${BLOB_AUTH}\" \\\n--restore-file $RESTORE_FILE\n●\nLogging hostlists given continued azcopy failures:\n30279       hpc 66B_run4   susanz  R    8:42:52     64 hpc-pg0-[9-12,14-31,34-40,42-43,45-48,50-69,71-78,84]\n2022-05-15 [Susan] Nan grads, lowered LR to 9e-6\n●\nEnd LR is kept at 0.5*LR = 4.5e-6\nBLOB_PREFIX1=\"<redacted>/66B_run40\"\nBLOB_PREFIX2=\"<redacted>/66B_run41\"\nRESTORE_FILE=\"${BLOB_PREFIX1}/checkpoint_47_135250.pt?${BLOB_AUTH}\"\nRUN_ID=66B_run41\n./<redacted> \\\n-n 64 -g 8 -t 1 \\\n-p $RUN_ID \\\n--azure \\\n\n--model-size 66b \\\n--checkpoints-dir /shared/home/susanz/checkpoints/66B/ \\\n--local-checkpoints-dir /mnt/scratch/$USER/checkpoints/$(date +%Y-%m-%d).$RUN_ID \\\n--full-azure-upload-path \"${BLOB_PREFIX2}/?${BLOB_AUTH}\" \\\n--restore-file $RESTORE_FILE\n●\nNode failures on host 13, did not auto-recover. Manually drained, and resuming again from the same\npoint given intermittent azcopy failures too.\n○\nAdded “args.requeue_on_fail = True” to sweep (wasn’t there before).\n●\nRelaunched exactly the same as run 41, just incremented to run 42.\n●\nAzcopy failures persist, tracking hostlist to recover from:\n30152       hpc 66B_run4   susanz  R    6:49:09     64 hpc-pg0-[9-12,14-31,34-43,45-48,50-69,71-78]\n2022-05-13 [Susan] 65 hosts in drain for IB issues, lowered LR to 1.2e-5\n●\nEnd LR is kept at 0.5*LR = 6e-6\nBLOB_PREFIX1=\"<redacted>66B_run39\"\nBLOB_PREFIX2=<redacted>/66B_run40\"\nRESTORE_FILE=\"${BLOB_PREFIX1}/checkpoint_45_130500.pt?${BLOB_AUTH}\"\nRUN_ID=66B_run40\nEXCLUDED_HOSTS=<redacted> \\\n./<redacted> \\\n-n 64 -g 8 -t 1 \\\n-p $RUN_ID \\\n--azure \\\n--model-size 66b \\\n--checkpoints-dir /shared/home/susanz/checkpoints/66B/ \\\n--local-checkpoints-dir /mnt/scratch/$USER/checkpoints/$(date +%Y-%m-%d).$RUN_ID \\\n--full-azure-upload-path \"${BLOB_PREFIX2}/?${BLOB_AUTH}\" \\\n--restore-file $RESTORE_FILE\n2022-05-11 [Susan] Nans in grad, lost GPU on node 32\n2022-05-11 11:22:12 | INFO | fairseq.trainer | NOTE: floating point error detected, ignoring gradient, Fatal error:\ngradients are inconsistent between workers. Try --ddp-backend=legacy_ddp. Or are you mixing up different\ngeneration of GPUs in training?\n●\nfixmyazure drained node 32 for lost GPU error.\n●\nOverall things look pretty healthy for now. Missing chunk of logs in the middle from when the clusters’\nhome directory got swamped / wiped by another team’s usage.  Logs are now getting backed up\nperiodically to cloud.\n\nTensorboard\n●\nValidation ppl on wikipedia looks good too:\nTensorboard\n\n2022-05-07 [Susan] Model is finally chugging along\n●\nNo more learning rate changes since last entry.\n●\nA few restarts from hardware failures.\n●\nWe are about ~73% through.\n●\nETA for completion: ~9 days\n●\nSpot-checked validation ppls: seems to still be improving, though more slowly now (LR is at 1e-5).\n2022-04-27 [Susan] Lowering LR to 1.6e-5, restart @ 63,750\n●\nAlso increase end LR to be 0.5 * start LR\nBLOB_PREFIX1=\"<redacted>/66B_run35\"\nBLOB_PREFIX2=\"<redacted>/66B_run36\"\nRESTORE_FILE=\"${BLOB_PREFIX1}/checkpoint_22_63750.pt?${BLOB_AUTH}\"\nRUN_ID=66B_run36\n./<redacted> \\\n-n 64 -g 8 -t 1 \\\n-p $RUN_ID \\\n--azure \\\n--model-size 66b \\\n--checkpoints-dir /shared/home/susanz/checkpoints/66B/ \\\n--local-checkpoints-dir /mnt/scratch/$USER/checkpoints/$(date +%Y-%m-%d).$RUN_ID \\\n--full-azure-upload-path \"${BLOB_PREFIX2}/?${BLOB_AUTH}\" \\\n--restore-file $RESTORE_FILE\n2022-04-25 [Susan] Nan/Inf grads @ 56029, skipping batches from\nswallowing FloatingPointError\n●\nNext restart should increase end LR - maybe keep at original end LR (6e-6), or leave flat / try\nincreasing end LR to warm up to a point where things crash\n●\nValidation ppl on Wikipedia is still dropping, though now only 0.01 every 1k steps.\n●\nConsistent crashing at 56029 step.\n●\nTried:\n○\n1.6e-5 start lr, 8e-6 end lr, no good.\n○\n1.2e-6 start lr, 6e-6 end lr, no good.\n●\nTrying start lr of 1e-6 with end lr at 1e-5 (higher end LR than start, to try and “warm up LR” again).\n○\nDidn’t work.\n●\nTried swapping shard 20 and 29 (experiment 32), didn’t work.\n○\nUsed: <redacted> as new data dir\n●\nDrastically cutting LR (since the shard swapping indicates a 0 LR would also not be useful): trying now\nwith starting LR of 2e-6 (end LR of 1e-6, for experiment 33).\n○\nDidn’t work.\n○\nGrad norm ends up being nan. Need clipping?\n●\nRestarting with LR back to 2e-5, but increase clip to 0.25 (experiment 34).\n○\nDidn’t work (didn’t clip anything).\n○\nTrying with clip down to 0.22 (still experiment 34).\n○\nStill doesn’t work, stuck at same place (56029).\n○\nTrying with commenting out throwing FloatingPointError (and skip the batch?) - experiment 35\n○\nOk this worked. -____-\n\nBLOB_PREFIX1=\"<redacted>/66B_run30\"\nBLOB_PREFIX2=\"<redacted>/66B_run35\"\nRESTORE_FILE=\"${BLOB_PREFIX1}/checkpoint_20_56000.pt?${BLOB_AUTH}\"\nRUN_ID=66B_run35\n./<redacted> \\\n-n 64 -g 8 -t 1 \\\n-p $RUN_ID \\\n--azure \\\n--model-size 66b \\\n--checkpoints-dir /shared/home/susanz/checkpoints/66B/ \\\n--local-checkpoints-dir /mnt/scratch/$USER/checkpoints/$(date +%Y-%m-%d).$RUN_ID \\\n--full-azure-upload-path \"${BLOB_PREFIX2}/?${BLOB_AUTH}\" \\\n--restore-file $RESTORE_FILE\n2022-04-22 [Susan] Keep lower LR of 2e-5, resume from 48k\n●\nThings keep crashing\n●\nTime to lower LR from an earlier checkpoint\n●\n(wiki) validation looks “ok” though after lower LR of 2e-5 (continues to drop), so lowering it earlier\nshouldn’t hurt.\n●\nLoss scales started looking rough after 48k, hence resuming from 48k with a lower LR.\nTensorboard\nBLOB_PREFIX1=\"<redacted>/66B_run28\"\nBLOB_PREFIX2=\"<redacted>/66B_run30\"\nRESTORE_FILE=\"${BLOB_PREFIX1}/checkpoint_17_48000.pt?${BLOB_AUTH}\"\nRUN_ID=66B_run30\n./<redacted> \\\n-n 64 -g 8 -t 1 \\\n-p $RUN_ID \\\n\n--azure \\\n--model-size 66b \\\n--checkpoints-dir /shared/home/susanz/checkpoints/66B/ \\\n--local-checkpoints-dir /mnt/scratch/$USER/checkpoints/$(date +%Y-%m-%d).$RUN_ID \\\n--full-azure-upload-path \"${BLOB_PREFIX2}/?${BLOB_AUTH}\" \\\n--restore-file $RESTORE_FILE\n2022-04-22 [Susan] Lower LR to 2.4e-5 2e-5, resume from 50k\nBLOB_PREFIX1=\"<redacted>/66B_run28\"\nBLOB_PREFIX2=\"<redacted>/66B_run29\"\nRESTORE_FILE=\"${BLOB_PREFIX1}/checkpoint_17_50000.pt?${BLOB_AUTH}\"\nRUN_ID=66B_run29\n./<redacted> \\\n-n 64 -g 8 -t 1 \\\n-p $RUN_ID \\\n--azure \\\n--model-size 66b \\\n--checkpoints-dir /shared/home/susanz/checkpoints/66B/ \\\n--local-checkpoints-dir /mnt/scratch/$USER/checkpoints/$(date +%Y-%m-%d).$RUN_ID \\\n--full-azure-upload-path \"${BLOB_PREFIX2}/?${BLOB_AUTH}\" \\\n--restore-file $RESTORE_FILE\n2022-04-19 [Susan]\nBLOB_PREFIX1=\"<redacted>/66B_run25\"\nBLOB_PREFIX2=\"<redacted>/66B_run26\"\nRESTORE_FILE=\"${BLOB_PREFIX1}/checkpoint_14_40000.pt?${BLOB_AUTH}\"\nRUN_ID=66B_run26\n./<redacted> \\\n-n 64 -g 8 -t 1 \\\n-p $RUN_ID \\\n--azure \\\n--model-size 66b \\\n--checkpoints-dir /shared/home/susanz/checkpoints/66B/ \\\n--local-checkpoints-dir /mnt/scratch/$USER/checkpoints/$(date +%Y-%m-%d).$RUN_ID \\\n--full-azure-upload-path \"${BLOB_PREFIX2}/?${BLOB_AUTH}\" \\\n--restore-file $RESTORE_FILE\n●\nHung after crashing log scale, didn’t recover.\n●\nIncreasing LR to 8e-5, restarting from 41500.\n○\nNope, bad idea. Should stick with the convention of lowering LR. Down to 3e-5.  Still calling this\nrun27 since the path will be different with different LR.\nBLOB_PREFIX1=\"<redacted>/66B_run26\"\nBLOB_PREFIX2=\"<redacted>/66B_run27\"\nRESTORE_FILE=\"${BLOB_PREFIX1}/checkpoint_15_41500.pt?${BLOB_AUTH}\"\nRUN_ID=66B_run27\n./<redacted> \\\n-n 64 -g 8 -t 1 \\\n-p $RUN_ID \\\n\n--azure \\\n--model-size 66b \\\n--checkpoints-dir /shared/home/susanz/checkpoints/66B/ \\\n--local-checkpoints-dir /mnt/scratch/$USER/checkpoints/$(date +%Y-%m-%d).$RUN_ID \\\n--full-azure-upload-path \"${BLOB_PREFIX2}/?${BLOB_AUTH}\" \\\n--restore-file $RESTORE_FILE\n●\nRewinding some more to 40250:\nBLOB_PREFIX1=\"<redacted>/66B_run25\"\nBLOB_PREFIX2=\"<redacted>/66B_run28\"\nRESTORE_FILE=\"${BLOB_PREFIX1}/checkpoint_14_40250.pt?${BLOB_AUTH}\"\nRUN_ID=66B_run28\n./<redacted> \\\n-n 64 -g 8 -t 1 \\\n-p $RUN_ID \\\n--azure \\\n--model-size 66b \\\n--checkpoints-dir /shared/home/susanz/checkpoints/66B/ \\\n--local-checkpoints-dir /mnt/scratch/$USER/checkpoints/$(date +%Y-%m-%d).$RUN_ID \\\n--full-azure-upload-path \"${BLOB_PREFIX2}/?${BLOB_AUTH}\" \\\n--restore-file $RESTORE_FILE\n2022-04-17 [Susan]: Gradient crashed and monitor did not kick in (given no\nlag in logging), restarting with no changes (outside of reducing logging)\nBLOB_PREFIX1=\"<redacted>/66B_run24\"\nBLOB_PREFIX2=\"<redacted>/66B_run25\"\nRESTORE_FILE=\"${BLOB_PREFIX1}/checkpoint_12_35000.pt?${BLOB_AUTH}\"\nRUN_ID=66B_run25\n./<redacted> \\\n-n 64 -g 8 -t 1 \\\n-p $RUN_ID \\\n--azure \\\n--model-size 66b \\\n--checkpoints-dir /shared/home/susanz/checkpoints/66B/ \\\n--local-checkpoints-dir /mnt/scratch/$USER/checkpoints/$(date +%Y-%m-%d).$RUN_ID \\\n--full-azure-upload-path \"${BLOB_PREFIX2}/?${BLOB_AUTH}\" \\\n--restore-file $RESTORE_FILE\n2022-04-15 [Susan]: Loss scale window logic change, LR 4e-5\n●\nChange loss scale window to also scale down with loss scale, lower bounded loss scale to 0.03125 and\ncommented out raising loss scale min threshold error (result is skipping those batches).\n●\nSeems stable for the day, with activation norm slowly trending down:\n\nTensorboard\nBLOB_PREFIX1=\"<redacted>/66B_run23\"\nBLOB_PREFIX2=\"<redacted>/66B_run24\"\nRESTORE_FILE=\"${BLOB_PREFIX1}/checkpoint_9_24750.pt?${BLOB_AUTH}\"\nRUN_ID=66B_run24\n./<redacted> \\\n-n 64 -g 8 -t 1 \\\n-p $RUN_ID \\\n--azure \\\n--model-size 66b \\\n--checkpoints-dir /shared/home/susanz/checkpoints/66B/ \\\n--local-checkpoints-dir /mnt/scratch/$USER/checkpoints/$(date +%Y-%m-%d).$RUN_ID \\\n--full-azure-upload-path \"${BLOB_PREFIX2}/?${BLOB_AUTH}\" \\\n--restore-file $RESTORE_FILE\n2022-04-14 [Susan]: Things looked good but nope.\n●\nActivation norm curve looks more sane, loss scales look better too (orange vs pale orange before)\n\nTensorboard\n●\nUpdate @ 4PM CDT: Spoke too soon - need to restart from 22.5k\n○\nLowered LR to 5e-5\nBLOB_PREFIX1=\"<redacted>/66B_run22\"\nBLOB_PREFIX2=\"<redacted>/66B_run23\"\nBLOB_AUTH=\"<redacted>\"\nRESTORE_FILE=\"${BLOB_PREFIX1}/checkpoint_8_22500?${BLOB_AUTH}\"\nRUN_ID=66B_run23\n./<redacted> \\\n-n 64 -g 8 -t 1 \\\n-p $RUN_ID \\\n--azure \\\n--model-size 66b \\\n--checkpoints-dir /shared/home/susanz/checkpoints/66B/ \\\n--local-checkpoints-dir /mnt/scratch/$USER/checkpoints/$(date +%Y-%m-%d).$RUN_ID \\\n--full-azure-upload-path \"${BLOB_PREFIX2}/?${BLOB_AUTH}\" \\\n--restore-file $RESTORE_FILE\n\n2022-04-13 [Susan]: Restart from 27.5k with lr of 4e-5, ppl diverged at\n~28.7k\nTensorboard\n●\nLR was lowered around here previously as well\n○\nBad data batch?\nBLOB_PREFIX1=\"<redacted>/66B_run18\"\nBLOB_PREFIX2=\"<redacted>/66B_run20\"\nBLOB_AUTH=\"<redacted>\"\nRESTORE_FILE=\"${BLOB_PREFIX1}/checkpoint_10_27500.pt?${BLOB_AUTH}\"\nRUN_ID=66B_run20\n./<redacted> \\\n-n 64 -g 8 -t 1 \\\n-p $RUN_ID \\\n\n--azure \\\n--model-size 66b \\\n--checkpoints-dir /shared/home/susanz/checkpoints/66B/ \\\n--local-checkpoints-dir /mnt/scratch/$USER/checkpoints/$(date +%Y-%m-%d).$RUN_ID \\\n--full-azure-upload-path \"${BLOB_PREFIX2}/?${BLOB_AUTH}\" \\\n--restore-file $RESTORE_FILE\n●\nLowered LR again to 3e-5 and launched 66B_run21\n●\nStill no bueno\nTensorboard\n●\nSeems like we should be restarting around 24k (or even ~20k) instead, before activation norm curve\nshot up and became convex.\n●\nGoing to restart run18 (starting from 19k) with a lower LR and see if it’s more stable.\nBLOB_PREFIX1=\"<redacted>/66B_run17\"\nBLOB_PREFIX2=\"<redacted>/66B_run22\"\nBLOB_AUTH=\"<redacted>\"\nRESTORE_FILE=\"${BLOB_PREFIX1}/checkpoint_7_19000.pt?${BLOB_AUTH}\"\nRUN_ID=66B_run22\n./<redacted> \\\n-n 64 -g 8 -t 1 \\\n-p $RUN_ID \\\n--azure \\\n--model-size 66b \\\n--checkpoints-dir /shared/home/susanz/checkpoints/66B/ \\\n--local-checkpoints-dir /mnt/scratch/$USER/checkpoints/$(date +%Y-%m-%d).$RUN_ID \\\n--full-azure-upload-path \"${BLOB_PREFIX2}/?${BLOB_AUTH}\" \\\n\n--restore-file $RESTORE_FILE\n2022-04-12 [Susan]: Restart from 27.5k with lr of 6e-5, min loss scale\nreached at 28292\n●\nReducing LR from 8e-5 to 6e-5\nBLOB_PREFIX1=\"<redacted>/66B_run18\"\nBLOB_PREFIX2=\"<redacted>/66B_run19\"\nBLOB_AUTH=\"<redacted>\"\nRESTORE_FILE=\"${BLOB_PREFIX1}/checkpoint_10_27500.pt?${BLOB_AUTH}\"\nRUN_ID=66B_run19\n./<redacted> \\\n-n 64 -g 8 -t 1 \\\n-p $RUN_ID \\\n--azure \\\n--model-size 66b \\\n--checkpoints-dir /shared/home/susanz/checkpoints/66B/ \\\n--local-checkpoints-dir /mnt/scratch/$USER/checkpoints/$(date +%Y-%m-%d).$RUN_ID \\\n--full-azure-upload-path \"${BLOB_PREFIX2}/?${BLOB_AUTH}\" \\\n--restore-file $RESTORE_FILE\n2022-04-10 [Susan]: Restart from 19k, clip 1.0 -> 0.3\n●\nActivation norm + ppl diverged.\nBLOB_PREFIX1=\"<redacted>/66B_run17\"\nBLOB_PREFIX2=\"<redacted>/66B_run18\"\nBLOB_AUTH=\"<redacted>\"\nRESTORE_FILE=\"${BLOB_PREFIX1}/checkpoint_7_19000.pt?${BLOB_AUTH}\"\nRUN_ID=66B_run18\n./<redacted> \\\n-n 64 -g 8 -t 1 \\\n-p $RUN_ID \\\n--azure \\\n--model-size 66b \\\n--checkpoints-dir /shared/home/susanz/checkpoints/66B/ \\\n--local-checkpoints-dir /mnt/scratch/$USER/checkpoints/$(date +%Y-%m-%d).$RUN_ID \\\n--full-azure-upload-path \"${BLOB_PREFIX2}/?${BLOB_AUTH}\" \\\n--restore-file $RESTORE_FILE\n2022-04-07 [Susan]: Restart from 4750, no changes.\n●\nHad to fix the checkpointing path bug anyway, took the chance to upload remaining checkpoints from\n/mnt/scratch up to Azure blob.\n●\nRestarted from 4750 (last checkpoint before failure).\n●\nRun looks much more stable already. Seems like apex version is likely the culprit for instability.\nBLOB_PREFIX1=\"<redacted>/66B_run16\"\nBLOB_PREFIX2=\"<redacted>/66B_run17\"\nBLOB_AUTH=\"<redacted>\"\n\nRESTORE_FILE=\"${BLOB_PREFIX1}/checkpoint_2_4750.pt?${BLOB_AUTH}\"\nRUN_ID=66B_run17\n./<redacted> \\\n-n 64 -g 8 -t 1 \\\n-p $RUN_ID \\\n--azure \\\n--model-size 66b \\\n--checkpoints-dir /shared/home/susanz/checkpoints/66B/ \\\n--local-checkpoints-dir /mnt/scratch/$USER/checkpoints/$(date +%Y-%m-%d).$RUN_ID \\\n--full-azure-upload-path \"${BLOB_PREFIX2}/?${BLOB_AUTH}\" \\\n--restore-file $RESTORE_FILE\n2022-04-06 [Susan]: Restart 66B from scratch. LR @ 8e-5, clip @ 1.0.\nRemove no-reshard-after-forward, remove padding (previous validation fix).\nBranch susan/66b_apr6_restart from fairseq-big-internal.\nConda env: fairseq-20210913-old-apex\nBLOB_PREFIX1=\"<redacted>/66B_run16\"\nBLOB_AUTH=\"<redacted>\"\nRUN_ID=66B_run16\n./<redacted> \\\n-n 64 -g 8 -t 1 \\\n-p $RUN_ID \\\n--azure \\\n--model-size 66b \\\n--checkpoints-dir /shared/home/susanz/checkpoints/66B/ \\\n--local-checkpoints-dir /mnt/scratch/$USER/checkpoints/$(date +%Y-%m-%d).$RUN_ID \\\n--full-azure-upload-path \"${BLOB_PREFIX2}/?${BLOB_AUTH}\"\n●\nTested batch size of 3M, slowed down WPS dramatically.\n●\nCheckpoints not being uploaded, need to pull from scratch space on hosts:\n20938       hpc 66B_run1   susanz  R    7:03:28     64\nhpc-pg0-[2-3,40-70,76-80,83-97,99-100,121,128-135]\nPDSH_RCMD_TYPE=ssh pdsh -w hpc-pg0-[2-3,40-70,76-80,83-97,99-100,121,128-135]\n'/shared/home/susanz/bin/azcopy copy \"<redacted>/*.pt\" \"<redacted>/66B_run16?<redacted>\"'\n2022-04-05 [Susan]: revert apex version, keep LR @ 4e-6, move clip back\nup to 0.3, restart from 38.5k\n(Susan’s conda env: fairseq-20210913-old-apex ->\n/shared/home/susanz/miniconda3/envs/fairseq-20210913-old-apex)\nBLOB_PREFIX1=\"<redacted>/66B_run12\"\nBLOB_PREFIX2=\"<redacted>/66B_run13\"\nBLOB_AUTH=\"<redacted>\"\nRESTORE_FILE=\"${BLOB_PREFIX1}/checkpoint_14_38500.pt?${BLOB_AUTH}\"\nRUN_ID=66B_run13\n\n./<redacted> \\\n-n 64 -g 8 -t 1 \\\n-p $RUN_ID \\\n--azure \\\n--model-size 66b \\\n--checkpoints-dir /shared/home/susanz/checkpoints/66B/ \\\n--local-checkpoints-dir /mnt/scratch/$USER/checkpoints/$(date +%Y-%m-%d).$RUN_ID \\\n--full-azure-upload-path \"${BLOB_PREFIX2}/?${BLOB_AUTH}\" \\\n--restore-file $RESTORE_FILE\n●\nStill looks to be problematic. Reducing clip down to 0.25 (job was stalled from being held by someone\non the cluster anyway).\nBLOB_PREFIX1=\"<redacted>/66B_run12\"\nBLOB_PREFIX2=\"<redacted>/66B_run14\"\nBLOB_AUTH=\"<redacted>\"\nRESTORE_FILE=\"${BLOB_PREFIX1}/checkpoint_14_38500.pt?${BLOB_AUTH}\"\nRUN_ID=66B_run14\n./<redacted> \\\n-n 64 -g 8 -t 1 \\\n-p $RUN_ID \\\n--azure \\\n--model-size 66b \\\n--checkpoints-dir /shared/home/susanz/checkpoints/66B/ \\\n--local-checkpoints-dir /mnt/scratch/$USER/checkpoints/$(date +%Y-%m-%d).$RUN_ID \\\n--full-azure-upload-path \"${BLOB_PREFIX2}/?${BLOB_AUTH}\" \\\n--restore-file $RESTORE_FILE\nTensorboard\n●\nEffect of changing clipping from 0.3 (pink) -> 0.25 (green)\n\n●\nLR is now at 2e-6 already\n2022-04-04 [Susan]: actv_norm exploding again, lowering LR to 4e-6, clip to\n0.2, restart from 36,250\nBLOB_PREFIX1=\"<redacted>/66B_run11\"\nBLOB_PREFIX2=\"<redacted>/66B_run12\"\nBLOB_AUTH=\"<redacted>\"\nRESTORE_FILE=\"${BLOB_PREFIX1}/checkpoint_13_36250.pt?${BLOB_AUTH}\"\nRUN_ID=66B_run12\n./<redacted> \\\n-n 64 -g 8 -t 1 \\\n-p $RUN_ID \\\n--azure \\\n--model-size 66b \\\n--checkpoints-dir /shared/home/susanz/checkpoints/66B/ \\\n--local-checkpoints-dir /mnt/scratch/$USER/checkpoints/$(date +%Y-%m-%d).$RUN_ID \\\n--full-azure-upload-path \"${BLOB_PREFIX2}/?${BLOB_AUTH}\" \\\n--restore-file $RESTORE_FILE\n2022-04-03 [Susan]: actv_norm exploding again, lowering LR to 6e-6,\nrestart from 34k again\nTensorboard\nBLOB_PREFIX1=\"<redacted>/66B_run09\"\nBLOB_PREFIX2=\"<redacted>/66B_run11\"\nBLOB_AUTH=\"<redacted>\"\n\nRESTORE_FILE=\"${BLOB_PREFIX1}/checkpoint_12_34000.pt?${BLOB_AUTH}\"\nRUN_ID=66B_run11\n./<redacted> \\\n-n 64 -g 8 -t 1 \\\n-p $RUN_ID \\\n--azure \\\n--model-size 66b \\\n--checkpoints-dir /shared/home/susanz/checkpoints/66B/ \\\n--local-checkpoints-dir /mnt/scratch/$USER/checkpoints/$(date +%Y-%m-%d).$RUN_ID \\\n--full-azure-upload-path \"${BLOB_PREFIX2}/?${BLOB_AUTH}\" \\\n--restore-file $RESTORE_FILE\n2022-04-01 [Susan]: actv_norm exploding again, lowering LR to 8e-6,\nrestart from 34k\nTensorboard\nBLOB_PREFIX1=\"<redacted>/66B_run09\"\nBLOB_PREFIX2=\"<redacted>/66B_run10\"\nBLOB_AUTH=\"<redacted>\"\nRESTORE_FILE=\"${BLOB_PREFIX1}/checkpoint_12_34000.pt?${BLOB_AUTH}\"\nRUN_ID=66B_run10\n./<redacted> \\\n-n 64 -g 8 -t 1 \\\n-p $RUN_ID \\\n--azure \\\n--model-size 66b \\\n--checkpoints-dir /shared/home/susanz/checkpoints/66B/ \\\n--local-checkpoints-dir /mnt/scratch/$USER/checkpoints/$(date +%Y-%m-%d).$RUN_ID \\\n--full-azure-upload-path \"${BLOB_PREFIX2}/?${BLOB_AUTH}\" \\\n--restore-file $RESTORE_FILE\n\n2022-04-01 [Susan]: actv_norm exploding again, lowering LR to 1e-5,\nrestart from 31k\nTensorboard\nBLOB_PREFIX1=\"<redacted>/66B_run08\"\nBLOB_PREFIX2=\"<redacted>/66B_run09\"\nBLOB_AUTH=\"<redacted>\"\nRESTORE_FILE=\"${BLOB_PREFIX1}/checkpoint_11_31000.pt?${BLOB_AUTH}\"\nRUN_ID=66B_run09\n./<redacted> \\\n-n 64 -g 8 -t 1 \\\n-p $RUN_ID \\\n--azure \\\n--model-size 66b \\\n--checkpoints-dir /shared/home/susanz/checkpoints/66B/ \\\n--local-checkpoints-dir /mnt/scratch/$USER/checkpoints/$(date +%Y-%m-%d).$RUN_ID \\\n--full-azure-upload-path \"${BLOB_PREFIX2}/?${BLOB_AUTH}\" \\\n--restore-file $RESTORE_FILE\n\n2022-04-01 [Susan]: actv_norm exploding again, lowering LR to 2e-5,\nrestart from 27.75k\nTensorboard\nBLOB_PREFIX1=\"<redacted>/66B_run07\"\nBLOB_PREFIX2=\"<redacted>/66B_run08\"\nBLOB_AUTH=\"<redacted>\"\nRESTORE_FILE=\"${BLOB_PREFIX1}/checkpoint_10_27750.pt?${BLOB_AUTH}\"\nRUN_ID=66B_run08\n/<redacted> \\\n-n 64 -g 8 -t 1 \\\n-p $RUN_ID \\\n--azure \\\n--model-size 66b \\\n--checkpoints-dir /shared/home/susanz/checkpoints/66B/ \\\n--local-checkpoints-dir /mnt/scratch/$USER/checkpoints/$(date +%Y-%m-%d).$RUN_ID \\\n--full-azure-upload-path \"${BLOB_PREFIX2}/?${BLOB_AUTH}\" \\\n--restore-file $RESTORE_FILE\n\n2022-03-31 [Susan]: actv_norm exploding again, lowering LR to 3e-5,\nrestart from 25.75k\nTensorboard\n●\nReverting to 25.75k when loss scale was still “healthy” at around 0.5, lowering LR again from 4e-5 to\n3e-5\nBLOB_PREFIX1=\"<redacted>/66B_run06\"\nBLOB_PREFIX2=\"<redacted>/66B_run07\"\nBLOB_AUTH=\"<redacted>\"\nRESTORE_FILE=\"${BLOB_PREFIX1}/checkpoint_9_25750.pt?${BLOB_AUTH}\"\nRUN_ID=66B_run07\n./<redacted> \\\n-n 64 -g 8 -t 1 \\\n-p $RUN_ID \\\n--azure \\\n--model-size 66b \\\n--checkpoints-dir /shared/home/susanz/checkpoints/66B/ \\\n--local-checkpoints-dir /mnt/scratch/$USER/checkpoints/$(date +%Y-%m-%d).$RUN_ID \\\n--full-azure-upload-path \"${BLOB_PREFIX2}/?${BLOB_AUTH}\" \\\n--restore-file $RESTORE_FILE\n2022-03-29 [Susan]: actv_norm exploding again, lowering LR to 4e-5,\nrestart from 16.75k\n\nTensorboard\nTensorboard\n\nTensorboard\nTensorboard\n●\nAccidentally deleted the run05 logs from the cluster, took above screenshots before tensorboard\nrefreshed.\n●\nLowered LR to 4e-5\n●\nRelaunched with:\nBLOB_PREFIX1=\"<redacted>/66B_run05\"\nRESTORE_FILE=\"${BLOB_PREFIX1}/checkpoint_6_16750.pt?${BLOB_AUTH}\"\nBLOB_PREFIX2=\"<redacted>/66B_run06\"\nBLOB_AUTH=\"<redacted>\"\nRUN_ID=66B_run06\n./<redacted> \\\n-n 64 -g 8 -t 1 \\\n-p $RUN_ID \\\n--azure \\\n--model-size 66b \\\n\n--checkpoints-dir /shared/home/susanz/checkpoints/66B/ \\\n--local-checkpoints-dir /mnt/scratch/$USER/checkpoints/$(date +%Y-%m-%d).$RUN_ID \\\n--full-azure-upload-path \"${BLOB_PREFIX2}/?${BLOB_AUTH}\" \\\n--restore-file $RESTORE_FILE\n●\nDoing some more comparisons before data is refreshed in tb\nTensorboard\nTensorboard\n\n2022-03-28 [Susan]: Checking in - turning on clip 0.3 earlier seems to have\nbeen necessary\nTensorboard\n●\nFrom clip == 0.3, we start clipping much earlier (blue vs red, where clip == 1.0).\n●\nLoss scales look healthier after clipping too (doesn’t crash to < 0.25).\n●\nActivation norm is also lowered.\n2022-03-27 [Susan]: Restarting from 13,750, with clip == 0.3\n●\nActivation norm at infinity\n\nTensorboard\n●\nSeems like lowering clip to 0.3 and restarting from 17,750 wasn’t drastic enough. Trying a rollback to\neven further back.\n●\nLR is already set at the same value as what we had for the 175B. Batch size is also the same (2M).\n○\nWe did lower LR to 3e-5 after ~91k in the 175B run. Worst case we do this earlier here too.\nBLOB_PREFIX=\"<redacted>/66B\"\nBLOB_AUTH=\"<redacted>\"\nRESTORE_FILE=\"${BLOB_PREFIX}/checkpoint_5_13750.pt?${BLOB_AUTH}\"\nRUN_ID=66B_run04\n./<redacted> \\\n-n 64 -g 8 -t 1 \\\n-p $RUN_ID \\\n--azure \\\n--model-size 66b \\\n--checkpoints-dir /shared/home/susanz/checkpoints/66B/ \\\n--local-checkpoints-dir /mnt/scratch/$USER/checkpoints/$(date +%Y-%m-%d).$RUN_ID \\\n--full-azure-upload-path \"${BLOB_PREFIX}/?${BLOB_AUTH}\" \\\n--restore-file $RESTORE_FILE\n●\nAuto restart ended up resuming from the end of run03, given the same blob prefix. Needed to restart\nagain with separating the restore file path from the upload path:\nBLOB_PREFIX1=\"<redacted>/66B\"\nRESTORE_FILE=\"${BLOB_PREFIX1}/checkpoint_5_13750.pt?${BLOB_AUTH}\"\nBLOB_PREFIX2=\"<redacted>/66B_run05\"\nBLOB_AUTH=\"<redacted>\"\nRUN_ID=66B_run05\n./<redacted> \\\n-n 64 -g 8 -t 1 \\\n\n-p $RUN_ID \\\n--azure \\\n--model-size 66b \\\n--checkpoints-dir /shared/home/susanz/checkpoints/66B/ \\\n--local-checkpoints-dir /mnt/scratch/$USER/checkpoints/$(date +%Y-%m-%d).$RUN_ID \\\n--full-azure-upload-path \"${BLOB_PREFIX2}/?${BLOB_AUTH}\" \\\n--restore-file $RESTORE_FILE\n2022-03-26 [Susan]: Restarting from 17,750, lowering clip to 0.3\nTensorboard\n●\nDiverged and looks like it was unable to recover\n●\nRestarting from 17,750, before we started clipping heavily when clip was 1.0\nBLOB_PREFIX=\"<redacted>/66B\"\nBLOB_AUTH=\"<redacted>\"\nRESTORE_FILE=\"${BLOB_PREFIX}/checkpoint_7_17750.pt?${BLOB_AUTH}\"\nRUN_ID=66B_run03\nEXCLUDED_HOSTS=hpc-pg0-[9,11] \\\n./<redacted> \\\n-n 64 -g 8 -t 1 \\\n-p $RUN_ID \\\n--azure \\\n--model-size 66b \\\n--checkpoints-dir /shared/home/susanz/checkpoints/66B/ \\\n--local-checkpoints-dir /mnt/scratch/$USER/checkpoints/$(date +%Y-%m-%d).$RUN_ID \\\n--full-azure-upload-path \"${BLOB_PREFIX}/?${BLOB_AUTH}\" \\\n--restore-file $RESTORE_FILE\n●\nManual host exclusion due to\nhttps://github.com/fairinternal/cluster-health/pull/4#discussion_r835766331\n2022-03-21 [Susan]: Restarting from 3.75k, validation on\n●\nRebased susan/66b_mar20_restart on top of main, after validation fix went in\nLOB_PREFIX=\"<redacted>/66B\"\nBLOB_AUTH=\"<redacted>\"\nRESTORE_FILE=\"${BLOB_PREFIX}/checkpoint_2_3750.pt?${BLOB_AUTH}\"\n\nRUN_ID=66B_run02\n./<redacted> \\\n-n 64 -g 8 -t 1 \\\n-p $RUN_ID \\\n--azure \\\n--model-size 66b \\\n--checkpoints-dir /shared/home/susanz/checkpoints/66B/ \\\n--local-checkpoints-dir /mnt/scratch/$USER/checkpoints/$(date +%Y-%m-%d).$RUN_ID \\\n--full-azure-upload-path \"${BLOB_PREFIX}/?${BLOB_AUTH}\" \\\n--restore-file $RESTORE_FILE\nCluster maintenance\n●\n9 hosts in drain\n●\nhpc-pg0-6 seems to have ECC uncorrectable errors\n●\nhpc-pg0-114 seem to be diagnosed with bad IB as well\n●\nhpc-pg0-[46,55,68] all seem to have lost GPU\n●\nRest were identified and reported as having bad IB already\n●\nPip installed new cluster-health module: https://github.com/fairinternal/cluster-health\n2022-03-20 [Susan]: Relaunching 66B from scratch, from latest code, 6e-5\nLR from start\n●\nBring us up to the point of latest fairseq-big-internal\n●\nBranched off for 66B run: https://github.com/fairinternal/fairseq-big-internal/pull/128\n●\nDiscussions w/ Stephen & Anj on restarting clean, mainly to catch new code changes / potential env\ndifferences (wps diffs ended up being IB issues)\nBLOB_PREFIX=\"<redacted>/66B\"\nBLOB_AUTH=\"<redacted>\"\nRUN_ID=66B_run01\n./<redacted> \\\n-n 64 -g 8 -t 1 \\\n-p $RUN_ID \\\n--azure \\\n--model-size 66b \\\n--checkpoints-dir /shared/home/susanz/checkpoints/66B/ \\\n--local-checkpoints-dir /mnt/scratch/$USER/checkpoints/$(date +%Y-%m-%d).$RUN_ID \\\n--full-azure-upload-path \"${BLOB_PREFIX}/?${BLOB_AUTH}\"\n●\nActivation norm drops ~200 steps in with 6e-5 LR\n\nTensorboard\n2022-03-20 [Anjali] Made it past the last instability point, activ_norm is\ntrending up\nTensorboard\nPPL went below lowest point but then increased again\n\nTensorboard\nWe will need to restart the job with 6e-5 way before than we did i.e at 21.25k. Maybe at 10k or even earlier.\n2022-03-19 [Anjali]  Activ norm is blowing up. Restarting job with lower\nLR=6.0e-5\nTensorboard\nTensorboard\n\nRestarting job with a lower LR of 6e-5.\nRUN_ID=66B_run03.21\nRESTORE_BLOB_PREFIX=\"<redacted>/66B_run03.20\"\nBLOB_PREFIX=\"<redacted>/66B_run03.21\"\nBLOB_AUTH=\"<redacted>\"\nRESTORE_FILE=\"${RESTORE_BLOB_PREFIX}/checkpoint_8_21250.pt?${BLOB_AUTH}\"\nRUN_ID=66B_run03.22\nRESTORE_BLOB_PREFIX=\"<redacted>/66B_run03.20\"\nBLOB_PREFIX=\"<redacted>/66B_run03.22\"\nBLOB_AUTH=\"<redacted>\"\nRESTORE_FILE=\"${RESTORE_BLOB_PREFIX}/checkpoint_8_21250.pt?${BLOB_AUTH}\"\npython -m <redacted baselines script>     -n 64 -g 8 -t 1     -p $RUN_ID     --checkpoints-dir\n/shared/home/anj/checkpoints/66B/     --local-checkpoints-dir /mnt/scratch/anj/checkpoints/$(date\n+%Y-%m-%d).$RUN_ID     --full-azure-upload-path \"${BLOB_PREFIX}/?${BLOB_AUTH}\"     --model-size 66b\n--restore-file $RESTORE_FILE\nRestarted at the right point:\nTensorboard\n2022-03-19 [Anjali] Spike again at 21.5k steps and monitoring\nTensorboard\n\n2022-03-19 [Anjali] Spike of activation norm at 20k steps but trending down\nTensorboard\nAnother thing I observed is that post the clip-norm change the wps has dropped from 126k to 123k. Still within\nacceptable range but we should follow up:\nTensorboard\n2022-03-18 [Anjali] Activ norm spiking, restarting job with clip-norm=0.3\nSlight spike in the activation norm. PPL seems ok so monitoring for now since there have been spikes\npreviously\n\nTensorboard\nLooks like activ norm is getting unstable:\nTensorboard\nRestarting job with clip-norm=0.3\nRUN_ID=66B_run03.20\nRESTORE_BLOB_PREFIX=\"<redacted>/66B_run03.19\"\nBLOB_PREFIX=\"<redacted>/66B_run03.20\"\nBLOB_AUTH=\"<redacted>\"\nRESTORE_FILE=\"${RESTORE_BLOB_PREFIX}/checkpoint_6_16500.pt?${BLOB_AUTH}\"\npython -m <redacted baselines script>     -n 64 -g 8 -t 1     -p $RUN_ID     --checkpoints-dir\n/shared/home/anj/checkpoints/66B/     --local-checkpoints-dir /mnt/scratch/anj/checkpoints/$(date\n+%Y-%m-%d).$RUN_ID     --full-azure-upload-path \"${BLOB_PREFIX}/?${BLOB_AUTH}\"     --model-size 66b\n--restore-file $RESTORE_FILE\n2022-03-18 [Anjali] Job failed, restarting again at 16k (or where the job died)\nCheckpoint from 14k step.\nBranch: anj/66b_gcmf_1\nSame LR: 8.0e-5\nRUN_ID=66B_run03.18\nRESTORE_BLOB_PREFIX=\"<redacted>/66B_run03.01\"\n\nBLOB_PREFIX=\"<redacted>/66B_run03.18\"\nBLOB_AUTH=\"<redacted>\"\nRESTORE_FILE=\"${RESTORE_BLOB_PREFIX}/checkpoint_last.pt?${BLOB_AUTH}\"\npython -m <redacted baselines script>     -n 64 -g 8 -t 1     -p $RUN_ID     --checkpoints-dir\n/shared/home/anj/checkpoints/66B/     --local-checkpoints-dir /mnt/scratch/anj/checkpoints/$(date\n+%Y-%m-%d).$RUN_ID     --full-azure-upload-path \"${BLOB_PREFIX}/?${BLOB_AUTH}\"     --model-size 66b\n--restore-file $RESTORE_FILE\nLooks like I started a little too behind. I am going to fast forward to 16k steps and restart the run.\nRUN_ID=66B_run03.19\nRESTORE_BLOB_PREFIX=\"<redacted>/66B_run03.01\"\nBLOB_PREFIX=\"<redacted>/66B_run03.19\"\nBLOB_AUTH=\"<redacted>\"\nRESTORE_FILE=\"${RESTORE_BLOB_PREFIX}/checkpoint_6_16000.pt?${BLOB_AUTH}\"\npython -m <redacted baselines script>     -n 64 -g 8 -t 1     -p $RUN_ID     --checkpoints-dir\n/shared/home/anj/checkpoints/66B/     --local-checkpoints-dir /mnt/scratch/anj/checkpoints/$(date\n+%Y-%m-%d).$RUN_ID     --full-azure-upload-path \"${BLOB_PREFIX}/?${BLOB_AUTH}\"     --model-size 66b\n--restore-file $RESTORE_FILE\nLogs look to be close to the last starting point.\nLooks ok at 9:18AM PST\nTensorboard\n2022-03-17 [Anjali] Update at ~14.5k steps\nGraphs trending as below: activ norm is slowly increasing but lower than previous runs. Wps is about the\nsame. Loss and PPL are following previous trends.\n\nTensorboard\nTensorboard\n2022-03-17 [Anjali] Update at 13.5k steps with new 8.0e-5 LR\nTensorboard\nWPS is lower than before restarts:\n\nTensorboard\nHowever the TFLOPs are still 133 so what we had initially gotten. It looks like it increased steadily after 2k\nsteps so I'm going to see if we see the same behavior. Unsure why the speed was higher before.\n2022-03-17 [Anjali]: Restart 66B run from checkpoint at 12k steps\nRun failed at 2022-03-17 13:41:15 - Loss exploded\nCheckpoint from 12k step. Branch: anj/66b_gcmf_1.\nRUN_ID=66B_run03.17\nBLOB_PREFIX=\"<redacted>/66B_run03.01\"\nBLOB_AUTH=\"<redacted>\"\nRESTORE_FILE=\"${BLOB_PREFIX}/checkpoint_5_12500.pt?${BLOB_AUTH}\"\npython -m <redacted baselines script>     -n 64 -g 8 -t 1     -p $RUN_ID     --checkpoints-dir\n/shared/home/anj/checkpoints/66B/     --local-checkpoints-dir /mnt/scratch/anj/checkpoints/$(date\n+%Y-%m-%d).$RUN_ID     --full-azure-upload-path \"${BLOB_PREFIX}/?${BLOB_AUTH}\"     --model-size 66b\n--restore-file $RESTORE_FILE\nOutput logs from restarted job show wps is back to ~124k\nWe started with ~124k throughput in the beginning of the run before it got higher. Trying to understand why we\nend up increasing the throughput over time.\n\nTensorboard\nRestarted from checkpoint where there was high WPS and stability:\nTensorboard\n\n2021-03-15 [Susan]: 66B diverges, wps change upon restarts?\nTensorboard\nZooming in:\nTensorboard\n●\nTensorboard is pointing to /shared/home/anj/checkpoints/66B/tensorboard/run03/tb, so added symlinks\nto the above symlinks there as well.\n●\nNext steps:\n\n○\nRestart from ???\n2022-03-11 [Anjali]: 66B Run starts\nRUN_ID=66B_run03.16\nMP=8  Batch size=2M\n'66b':  Size(64, 9216, 72, 128, int(2.0 * M), 1.0e-4, 8),  # 66b\n2021-03-08 [Susan]: 30B run completes\nTensorboard\nTensorboard\n\n2021-03-01 [Susan]: 30B_run10, restart from 29.5k after CUDA error\n●\n30B_run09 failed with:  RuntimeError: CUDA error: unknown error\n●\nRelaunched from 29500:\nBLOB_PREFIX=\"<redacted>/30B_run04\"\nBLOB_AUTH=???\nRESTORE_FILE=\"${BLOB_PREFIX}/checkpoint_18_29500.pt?${BLOB_AUTH}\"\nRUN_ID=30B_run10\n./<redacted> \\\n-n 112 -g 8 -t 1 \\\n-p $RUN_ID \\\n--model-size 30b \\\n--checkpoints-dir /shared/home/susanz/checkpoints/30B/ \\\n--local-checkpoints-dir /mnt/scratch/$USER/checkpoints/$(date +%Y-%m-%d).$RUN_ID \\\n--restore-file $RESTORE_FILE \\\n--full-azure-upload-path \"${BLOB_PREFIX}/?${BLOB_AUTH}\"\n●\nFailed with division by zero?  Relaunching from 29000 instead.\nBLOB_PREFIX=\"<redacted>/30B_run04\"\nBLOB_AUTH=???\nRESTORE_FILE=\"${BLOB_PREFIX}/checkpoint_18_29000.pt?${BLOB_AUTH}\"\nRUN_ID=30B_run11\n./<redacted> \\\n-n 112 -g 8 -t 1 \\\n-p $RUN_ID \\\n--model-size 30b \\\n--checkpoints-dir /shared/home/susanz/checkpoints/30B/ \\\n--local-checkpoints-dir /mnt/scratch/$USER/checkpoints/$(date +%Y-%m-%d).$RUN_ID \\\n--restore-file $RESTORE_FILE \\\n--full-azure-upload-path \"${BLOB_PREFIX}/?${BLOB_AUTH}\"\n2021-02-28 [Susan]: 30B_run07, restart from 25k after grad overflow\n●\nGradient overflow issues - see loss diverging, activation norm blowing up\n\nTensorboard\n●\nRestarting from 25k.\nBLOB_PREFIX=\"<redacted>/30B_run04\"\nBLOB_AUTH=???\nRESTORE_FILE=\"${BLOB_PREFIX}/checkpoint_16_25000.pt?${BLOB_AUTH}\"\nRUN_ID=30B_run07\n./<redacted> \\\n-n 112 -g 8 -t 1 \\\n-p $RUN_ID \\\n--model-size 30b \\\n--checkpoints-dir /shared/home/susanz/checkpoints/30B/ \\\n--local-checkpoints-dir /mnt/scratch/$USER/checkpoints/$(date +%Y-%m-%d).$RUN_ID \\\n--restore-file $RESTORE_FILE \\\n--full-azure-upload-path \"${BLOB_PREFIX}/?${BLOB_AUTH}\"\n●\nIf this blows up in the same spot again, will likely have to lower LR as the next step.\n●\nBlew up in the same place again (~26k steps). Increasing checkpointing to be every 500 steps (instead\nof 1k), and lowering LR to 8e-5 (from 1e-4).\nBLOB_PREFIX=\"<redacted>/30B_run04\"\nBLOB_AUTH=???\nRESTORE_FILE=\"${BLOB_PREFIX}/checkpoint_16_25000.pt?${BLOB_AUTH}\"\nRUN_ID=30B_run08\n./<redacted> \\\n-n 112 -g 8 -t 1 \\\n-p $RUN_ID \\\n--model-size 30b \\\n--checkpoints-dir /shared/home/susanz/checkpoints/30B/ \\\n--local-checkpoints-dir /mnt/scratch/$USER/checkpoints/$(date +%Y-%m-%d).$RUN_ID \\\n--restore-file $RESTORE_FILE \\\n--full-azure-upload-path \"${BLOB_PREFIX}/?${BLOB_AUTH}\"\n\n●\nLowering LR got us past 26k!\nTensorboard\n2021-02-26 [Susan]: 30B Baseline Restart, 30B_run05, 06\n●\nRun failed due to NCCL errors.\n●\nFixmyazure catches 75 hosts with 0 IB bandwidth\n●\nTurns out IB isn’t detected (UFM not up?).\n●\nAzure folks alerted, fixed with rebooting VMs. 17 busy hosts will need to be rebooted later too, and\nmore diagnostics needed on the 7 hosts in drain (+1 from running fixmyazure afterwards).\n●\nRelaunched:\nBLOB_PREFIX=\"<redacted>/30B_run04\"\nBLOB_AUTH=???\nRESTORE_FILE=\"${BLOB_PREFIX}/checkpoint_11_18000.pt?${BLOB_AUTH}\"\nRUN_ID=30B_run05\n./<redacted> \\\n-n 112 -g 8 -t 1 \\\n-p $RUN_ID \\\n--model-size 30b \\\n--checkpoints-dir /shared/home/susanz/checkpoints/30B/ \\\n--local-checkpoints-dir /mnt/scratch/$USER/checkpoints/$(date +%Y-%m-%d).$RUN_ID \\\n--restore-file $RESTORE_FILE \\\n--full-azure-upload-path \"${BLOB_PREFIX}/?${BLOB_AUTH}\"\n\n2021-02-24 [Susan]: 30B, 66B Baseline\nhttps://github.com/fairinternal/fairseq-py/pull/3131/files\n30B [Azure]: 2x MP, 4M batch size, 112 hosts\n●\nLaunching from https://github.com/fairinternal/fairseq-py/tree/susan/30b\n●\nLogs in /shared/home/susanz/checkpoints/30B/ , 2x\n○\nGot 272k WPS with 2M batch size (run03), 112 hosts\n○\nGot 418k WPS with 4M batch size (run04), 112 hosts\n○\nKeeping 4M batch size run: http://52.190.63.124:6020/ (Tensorboard)\n■\nTFLOP calculator shows only ~117 TFLOPs / GPU utilization - will need to improve:\nSusan TFLOPs Copy\n66B [RSC]: 4x MP, 4M batch size, 128 hosts\n●\n66B baseline is waiting in queue in the RSC for benchmarking.\n2021-01-16 & 2021-01-17 [Stephen]\n●\nAlso launched a 350M\n●\nWeirdly, found it was having a really terrible time converging\n●\nLowered the LR a little bit (to 1e-3) and things went fine\n2021-01-07 [Stephen]\n●\nNaman’s run crashed, so relaunched from scratch\n○\nDue to the larger batchsize, I had to remove some of the validation sets\n○\nSo the “combined” ppl is not directly comparable\n●\nLaunched 6.7B with the other 512 GPUs. It’s more than fast enough to finish before OSHA run.\n2021-01-06 [Stephen]\n●\nLaunched 13B on azure with 512 GPUs. Observed it was very slow (25 days to finish)\n○\nNaman tweaked it and doubled the batchsize to 4B, will be done in 6 days or so\n2021-12-26 [Stephen]\n●\nDecided to pause the 6.7B, which was running too slow and isn’t very important\n●\nLaunched the 2.7B in its place.\n2021-12-24 [Stephen]\n●\nManaged to resolve all cluster issues by going back to CUDA 11.3\n●\nWe’ll have to downgrade all the clusters to a new AMI\n●\nLaunched 13B and 6.7B. 6.7B was running very slow\n2021-12-18 [Stephen]\n●\nFigured out what was up with the tokenizers segfaults\n○\nSomething about using custom compiled NCCL\n\n○\nSwitched to using the 2.11.4 version released by Nvidia + the CUDA/EFA libraries existing on\nthe cluster.\n○\nTODO: Update the AWS instructions\n○\nAlso noted that I needed to source a bunch of crap from /etc/profile.d\n●\nNote: Got all my ssh connections suddenly disconnected at one point.\n●\nLaunched the 13b baseline\n○\nDoesn’t bode well.\n○\nA node went down pretty fast before we even finished initializing: Large-25\n■\nWith the one down, not sure 248 gpus is going to be enough to launch the 6.7b baseline.\n■\nPut it in drain but it wasn’t magically replaced like before :(\n■\nLooks like it came back within an hour or so\n○\nTried again and couldn’t get past initialization.\n■\nRuntimeError: CUDA error: CUBLAS_STATUS_ALLOC_FAILED when calling\n`cublasCreate(handle)\n○\nIt failed a 3rd time so I dropped back to NCCL 2.7.8(?). That’s what’s printed in the logs, but the\nDLAMI is supposed to be 2.11.4\n■\nThis is easy to do by dropping LD_PRELOAD\n■\nThat seemed to do it – past initialization\n■\nBut I was getting 0.5x speed compared to num_workers 0.\n■\nTried again with setting NCCL_NET_SHARED_BUFFERS=0, no beuno\n○\nTrying now with the nccl 2.11.4+cuda11.5 from DLAMI\n■\nThis required me to set LD_PRELOAD to their libnccl.so\n■\nAlso refused to start\n○\nLOL I TRIED COMPILING MYSELF AND WE ARE BACK TO SEGFAULTS\n○\nBack to DLAMI versions, but with my own compiled aws-ofi-nccl:\n■\nOff the shelf, segfaults\n■\nexport FI_EFA_USE_DEVICE_RDMA=0 works.\n2021-12-10 [Stephen] Set up baselines\n●\nSet up PR for gpt-z baselines: https://github.com/fairinternal/fairseq-py/pull/2781\n●\nLaunched 125M for fun\n"
    }
  ]
}