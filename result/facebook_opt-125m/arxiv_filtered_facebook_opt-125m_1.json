{
  "1-1 (Weights)": "The release statement explicitly says, “We are releasing all of our models between 125M and 66B parameters,” which means the 125 M-parameter checkpoint for facebook/opt-125m is publicly available. The same sentence notes that only the separate 175 B variant requires a special request, so the 125 M model is part of the openly downloadable set. More generally, the team “present[s] Open Pre-trained Transformers (OPT), a suite of decoder-only pre-trained transformers ranging from 125M to 175B parameters, which we aim to fully and responsibly share with interested researchers,” confirming that 125 M sits at the bottom of that shared suite. The project commits to giving researchers access to weights: they are “providing researchers access to model weights … along with a suite of smaller baselines mirroring the setup for OPT-175B,” and the 125 M model is explicitly one of those “smaller baseline models.” All of these baselines, including 125 M, are distributed under the same terms as the larger models, as reiterated in the remark, “License: OPT-175B and the smaller baseline models are made available through a non-commercial use license agreement provided in our model license.” Therefore, the 125 M weights can be freely downloaded under that licence without any additional gated‐access process, and they come accompanied by the full training logbook information.",
  "1-2 (Code)": "The authors emphasise that they are releasing training code, not only inference snippets. They write, “We are also releasing both the logbook of our model creation as well as our code-base, metaseq, which enabled training OPT-175B,” and because the same repository was used to produce “a suite of smaller baselines,” the 125 M model shares that exact training pipeline. The open-source repository, metaseq, therefore contains the configuration files, schedules, and scripts necessary to reproduce pre-training for the 125 M checkpoint. A second quote reinforces this, noting that “More details are also available in metaseq, our open-source repository,” and a third sentence adds that they are “disclosing all of the details involved in training OPT-175B through our logbook, our code, and providing researchers access … along with a suite of smaller baselines mirroring the setup for OPT-175B.” Taken together, these lines confirm that complete training assets—data-prep instructions, distributed training scripts, and performance tuning notes—are publicly posted, and that they cover the 125 M baseline every bit as much as the flagship model.",
  "1-3 (License)": "Every licence statement explicitly describes a non-commercial research licence. One quote states, “License: OPT-175B and the smaller baseline models are made available through a non-commercial use license agreement provided in our model license.” The 125 M model is one of those “smaller baseline models,” so the same terms apply. Another passage clarifies intent: “By limiting access to OPT-175B to the research community with a non-commercial license, we aim to focus development efforts on quantifying the limitations of the LLMs first.” Although that sentence references the 175 B model, the companion line repeats that the exact same non-commercial licence governs the baselines, which necessarily includes 125 M. A separate reflection notes that the non-commercial framing is intended “to increase communication, transparency, and study … especially in areas which may not be aligned with commercial interests.” In practical terms the licence grants researchers the right to download and use the 125 M weights and code for research, analysis, and publication, but it forbids commercial redistribution, commercial services, or derivative products offered for profit.",
  "1-4 (Paper)": "The canonical reference is the technical report introduced with the sentence, “In this technical report, we present Open Pre-trained Transformers (OPT), a suite of decoder-only pre-trained transformers ranging from 125M to 175B parameters.” The paper’s short title is also given explicitly: “OPT: Open Pre-trained Transformer Language Models.” A citation note adds, “Paper or other resource for more information: See the rest of this paper for more details on OPT-175B as well as the corresponding post on the Meta AI Research Blog.” Because the report covers the entire suite, it provides experimental and architectural details for the 125 M baseline alongside larger checkpoints. Thus, the official documentation for facebook/opt-125m consists of this technical report plus the companion Meta AI Research Blog post, both of which elaborate training setup, evaluation results, and release rationale for the 125 M model.",
  "1-1 (Weights)__evidence": [
    {
      "source": "[pdf_text]",
      "quote": "We are releasing all of our models between 125M and 66B parameters, and will provide full research access to OPT-175B upon request. Access will be granted to academic researchers; those affiliated with organizations in government, civil society, and academia; and those in industry research laboratories."
    },
    {
      "source": "[sections/Model Card]",
      "quote": "License: OPT-175B and the smaller baseline models are made available through a non-commercial use license agreement provided in our model license."
    },
    {
      "source": "[abstract]",
      "quote": "We present Open Pre-trained Transformers (OPT), a suite of decoder-only pre-trained transformers ranging from 125M to 175B parameters, which we aim to fully and responsibly share with interested researchers."
    },
    {
      "source": "[sections/Introduction]",
      "quote": "We are releasing all of our models between 125M and 66B parameters, and will provide full research access to OPT-175B upon request."
    },
    {
      "source": "[sections/Considerations for Release]",
      "quote": "Following the recommendations for individual re­searchers generated by the Partnership for AI,7 along with the governance guidance outlined by NIST,8 we are disclosing all of the details involved in training OPT-175B through our logbook,9 our code, and providing researchers access to model weights for OPT-175B, along with a suite of smaller baselines mirroring the setup for OPT-175B."
    },
    {
      "source": "[sections/Considerations for Release]",
      "quote": "To enable experimentation at 175B scale, we are providing researchers with direct access to the parameters of OPT-175B."
    },
    {
      "source": "[sections/Model Card D.1]",
      "quote": "License: OPT-175B and the smaller baseline models are made available through a non-commercial use license agreement provided in our model license."
    }
  ],
  "1-2 (Code)__evidence": [
    {
      "source": "[pdf_text]",
      "quote": "We are also releasing both the logbook of our model creation as well as our code-base, metaseq, which enabled training OPT-175B on 992 80GB A100 GPUs, reaching 147 TFLOP/s utilization per GPU."
    },
    {
      "source": "[sections/Model Card]",
      "quote": "Paper or other resource for more information: See the rest of this paper for more details on OPT-175B as well as the corresponding post on the Meta AI Research Blog. More details are also available in metaseq, our open-source repository."
    },
    {
      "source": "[sections/Introduction]",
      "quote": "We are also releasing both the logbook of our model creation as well as our codebase, metaseq,3 which enabled training OPT-175B on 992 80GB A100 GPUs, reaching 147 TFLOP/s utilization per GPU."
    },
    {
      "source": "[sections/Considerations for Release]",
      "quote": "Following the recommendations for individual re­searchers generated by the Partnership for AI,7 along with the governance guidance outlined by NIST,8 we are disclosing all of the details involved in training OPT-175B through our logbook,9 our code, and providing researchers access to model weights for OPT-175B, along with a suite of smaller baselines mirroring the setup for OPT-175B."
    },
    {
      "source": "[sections/Model Card D.1]",
      "quote": "Paper or other resource for more information: See the rest of this paper for more details on OPT-175B as well as the corresponding post on the Meta AI Research Blog. More details are also available in metaseq, our open-source repository."
    }
  ],
  "1-3 (License)__evidence": [
    {
      "source": "[pdf_text]",
      "quote": "By limiting access to OPT-175B to the research community with a non-commercial license, we aim to focus development efforts on quantifying the limitations of the LLMs first, before broader commercial deployment occurs."
    },
    {
      "source": "[sections/Model Card]",
      "quote": "License: OPT-175B and the smaller baseline models are made available through a non-commercial use license agreement provided in our model license."
    },
    {
      "source": "[sections/Model Card]",
      "quote": "In general, OPT-175B is not immune from the plethora of issues that plague modern large language models. By releasing with a non-commercial license, we also hope to increase communication, transparency, and study of the problems of large language models, especially in areas which may not be aligned with commercial interests."
    },
    {
      "source": "[sections/Considerations for Release]",
      "quote": "By limiting access to OPT-175B to the research community with a non-commercial license, we aim to focus development efforts on quantifying the limitations of the LLMs ﬁrst, before broader commercial deployment occurs."
    },
    {
      "source": "[sections/Model Card D.1]",
      "quote": "License: OPT-175B and the smaller baseline models are made available through a non-commercial use license agreement provided in our model license."
    }
  ],
  "1-4 (Paper)__evidence": [
    {
      "source": "[pdf_text]",
      "quote": "In this technical report, we present Open Pre-trained Transformers (OPT), a suite of decoder-only pre-trained transformers ranging from 125M to 175B parameters, which we aim to fully and responsibly share with interested researchers."
    },
    {
      "source": "[pdf_text]",
      "quote": "OPT: Open Pre-trained Transformer Language Models"
    },
    {
      "source": "[sections/Model Card]",
      "quote": "Paper or other resource for more information: See the rest of this paper for more details on OPT-175B as well as the corresponding post on the Meta AI Research Blog."
    },
    {
      "source": "[sections/Introduction]",
      "quote": "In this technical report, we present Open Pre-trained Transformers (OPT), a suite of decoder-only pre-trained transformers ranging from 125M to 175B parameters, which we aim to fully and responsibly share with interested researchers."
    },
    {
      "source": "[sections/Conclusion]",
      "quote": "In this technical report, we introduced OPT, a collection of auto-regressive language models ranging in size from 125M to 175B parameters."
    },
    {
      "source": "[sections/Model Card D.1]",
      "quote": "Paper or other resource for more information: See the rest of this paper for more details on OPT-175B as well as the corresponding post on the Meta AI Research Blog."
    }
  ]
}