{
  "1-1 (Weights)": "The OPT release explicitly says it is making the actual weight files available. The authors state that OPT is “a suite of decoder-only pre-trained transformers ranging from 125M to 175B parameters, which we aim to fully and responsibly share with interested researchers.”  For every model at or below 66 B parameters – and that includes facebook/opt-125m – they write: “We are releasing all of our models between 125M and 66B parameters…”.  This means the 125 M-parameter checkpoint can be downloaded directly by anyone (no application process is mentioned for these smaller baselines).  By contrast, the 175 B model is gated (“will provide full research access to OPT-175B upon request” and “providing researchers with direct access to the parameters of OPT-175B”).  Multiple passages repeat that the public bundle contains “a suite of smaller baselines mirroring the setup for OPT-175B.”  All released weights, including OPT-125m, are covered by the same distribution terms described in the license section (“OPT-175B and the smaller baseline models are made available through a non-commercial use license agreement”).  No further hosting details (URL, storage service, or hash) are given in the supplied text, but the quotes make clear that the weights are downloadable and that the 125 M checkpoint is part of the open, non-gated set.",
  "1-2 (Code)": "Training-time code for OPT is also being open-sourced.  The paper announces that, together with the weight release, Meta is “releasing our logbook detailing the infrastructure challenges we faced, along with code for experimenting with all of the released models.”  They identify the public repository by name: “our codebase, metaseq, which enabled training OPT-175B on 992 80 GB A100 GPUs.”  Another line clarifies that this repository is meant for end-users: “Information on how to use the model can be found at metaseq, our open-source repository.”  The authors emphasise that the disclosure covers training internals, not just inference: “we are disclosing all of the details involved in training OPT-175B through our logbook, our code.”  Thus, for OPT-125m, the complete training pipeline (configuration files, schedules, distributed training scripts, etc.) is available in the open-source metaseq repo, alongside example notebooks or scripts for fine-tuning and evaluation.",
  "1-3 (License)": "Every quote that mentions legal terms points to a single, unified licence: a non-commercial, research-only agreement.  The messaging is consistent: “License: OPT-175B and the smaller baseline models are made available through a non-commercial use license agreement provided in our model license.”  The intention is spelled out twice: “By limiting access to OPT-175B to the research community with a non-commercial license, we aim to focus development efforts on quantifying the limitations of the LLMs first, before broader commercial deployment occurs.”  Although the exact licence text is not reproduced, the repeated phrase “non-commercial use license” implies that the grant of rights (a) allows research use, (b) is silent or restrictive on modification and redistribution, and (c) explicitly forbids commercial exploitation.  No quote mentions derivative-works rules, sublicensing, or patent clauses, so those points remain unspecified here.",
  "1-4 (Paper)": "There is an official technical report and accompanying blog post.  The title is given as “OPT: Open Pre-trained Transformer Language Models.”  The authors refer to it several times: “In this technical report, we present Open Pre-trained Transformers (OPT), a suite of decoder-only pre-trained transformers ranging from 125M to 175B parameters…” and later, “In this technical report, we introduced OPT, a collection of auto-regressive language models ranging in size from 125M to 175B parameters.”  A pointer for further reading is offered: “See the rest of this paper for more details on OPT-175B as well as the corresponding post on the Meta AI Research Blog. More details are also available in metaseq, our open-source repository.”  Therefore, the canonical reference for facebook/opt-125m is the OPT technical report plus the linked Meta AI Research Blog article; no separate arXiv number or journal citation is included in the provided quotes.",
  "1-1 (Weights)__evidence": [
    {
      "source": "[abstract]",
      "quote": "We present Open Pre-trained Transformers (OPT), a suite of decoder-only pre-trained transformers ranging from 125M to 175B parameters, which we aim to fully and responsibly share with interested researchers."
    },
    {
      "source": "[pdf_text]",
      "quote": "We are releasing all of our models between 125M and 66B parameters, and will provide full research access to OPT-175B upon request."
    },
    {
      "source": "[sections/Considerations for Release]",
      "quote": "To enable experimentation at 175B scale, we are providing researchers with direct access to the parameters of OPT-175B."
    },
    {
      "source": "[sections/Model Card]",
      "quote": "License: OPT-175B and the smaller baseline models are made available through a non-commercial use license agreement provided in our model license."
    },
    {
      "source": "[sections/2205.01068]",
      "quote": "In this technical report, we present Open Pre-trained Transformers (OPT), a suite of decoder-only pre-trained transformers ranging from 125M to 175B parameters, which we aim to fully and responsibly share with interested researchers."
    },
    {
      "source": "[sections/2205.01068]",
      "quote": "We are releasing all of our models between 125M and 66B parameters, and will provide full research access to OPT-175B upon request."
    },
    {
      "source": "[sections/Considerations for Release]",
      "quote": "Following the recommendations for individual re- searchers generated by the Partnership for AI,7 along with the governance guidance outlined by NIST,8 we are disclosing all of the details in- volved in training OPT-175B through our log- book,9 our code, and providing researchers access to model weights for OPT-175B, along with a suite of smaller baselines mirroring the setup for OPT-175B."
    },
    {
      "source": "[sections/Model Card D.1]",
      "quote": "License: OPT-175B and the smaller baseline models are made available through a non-commercial use license agreement provided in our model license."
    },
    {
      "source": "[sections/Model Card D.2]",
      "quote": "We release OPT-175B for research into Language Models, especially as it pertains to Responsible AI."
    }
  ],
  "1-2 (Code)__evidence": [
    {
      "source": "[abstract]",
      "quote": "We present Open Pre-trained Transformers (OPT), a suite of decoder-only pre-trained transformers ranging from 125M to 175B parameters, which we aim to fully and responsibly share with interested researchers. We are also releasing our logbook detailing the infrastructure challenges we faced, along with code for experimenting with all of the released models."
    },
    {
      "source": "[pdf_text]",
      "quote": "We are also releasing both the logbook of our model creation as well as our codebase, metaseq,3 which enabled training OPT-175B on 992 80GB A100 GPUs."
    },
    {
      "source": "[sections/Model Card]",
      "quote": "Primary intended uses: We release OPT-175B for research into Language Models, especially as it pertains to Responsible AI. See Section 6 for more detailed Considerations for Release. Information on how to use the model can be found at metaseq, our open-source repository."
    },
    {
      "source": "[sections/2205.01068]",
      "quote": "We are releasing all of our models between 125M and 66B parameters, and will provide full research access to OPT-175B upon request. We are also releasing both the logbook of our model creation as well as our codebase, metaseq, which enabled training OPT-175B on 992 80GB A100 GPUs, reaching 147 TFLOP/s utilization per GPU."
    },
    {
      "source": "[abstract]",
      "quote": "We present Open Pre-trained Transformers (OPT), a suite of decoder-only pre-trained transformers ranging from 125M to 175B parameters, which we aim to fully and responsibly share with interested researchers. ... We are also releasing our logbook detailing the infrastructure challenges we faced, along with code for experimenting with all of the released models."
    },
    {
      "source": "[sections/Considerations for Release]",
      "quote": "Following the recommendations for individual re- searchers generated by the Partnership for AI,7 along with the governance guidance outlined by NIST,8 we are disclosing all of the details in- volved in training OPT-175B through our log- book,9 our code, and providing researchers access to model weights for OPT-175B, along with a suite of smaller baselines mirroring the setup for OPT-175B."
    },
    {
      "source": "[sections/Model Card D.1]",
      "quote": "Paper or other resource for more information: See the rest of this paper for more details on OPT-175B as well as the corresponding post on the Meta AI Research Blog. More details are also available in metaseq, our open-source repository."
    }
  ],
  "1-3 (License)__evidence": [
    {
      "source": "[sections/Considerations for Release]",
      "quote": "By limiting access to OPT-175B to the research community with a non-commercial license, we aim to focus development efforts on quantifying the limitations of the LLMs first, before broader commercial deployment occurs."
    },
    {
      "source": "[sections/Model Card]",
      "quote": "License: OPT-175B and the smaller baseline models are made available through a non-commercial use license agreement provided in our model license."
    },
    {
      "source": "[sections/Considerations for Release]",
      "quote": "By limiting access to OPT-175B to the research community with a non-commercial license, we aim to focus development efforts on quantifying the limitations of the LLMs ﬁrst, be- fore broader commercial deployment occurs."
    },
    {
      "source": "[sections/Model Card D.1]",
      "quote": "License: OPT-175B and the smaller baseline models are made available through a non-commercial use license agreement provided in our model license."
    },
    {
      "source": "[sections/Model Card D.3]",
      "quote": "In general, OPT-175B is not immune from the plethora of issues that plague modern large language models. By releasing with a non-commercial license, we also hope to increase communication, transparency, and study of the problems of large language models, especially in areas which may not be aligned with commercial interests."
    }
  ],
  "1-4 (Paper)__evidence": [
    {
      "source": "[sections/Introduction]",
      "quote": "In this technical report, we present Open Pre-trained Transformers (OPT), a suite of decoder-only pre-trained transformers ranging from 125M to 175B parameters, which we aim to fully and responsibly share with interested researchers."
    },
    {
      "source": "[sections/Model Card]",
      "quote": "Paper or other resource for more information: See the rest of this paper for more details on OPT-175B as well as the corresponding post on the Meta AI Research Blog. More details are also available in metaseq, our open-source repository.12"
    },
    {
      "source": "[title]",
      "quote": "OPT: Open Pre-trained Transformer Language Models"
    },
    {
      "source": "[sections/2205.01068]",
      "quote": "In this technical report, we present Open Pre-trained Transformers (OPT), a suite of decoder-only pre-trained transformers ranging from 125M to 175B parameters..."
    },
    {
      "source": "[sections/Conclusion]",
      "quote": "In this technical report, we introduced OPT, a col- lection of auto-regressive language models ranging in size from 125M to 175B parameters."
    },
    {
      "source": "[sections/Model Card D.1]",
      "quote": "Paper or other resource for more information: See the rest of this paper for more details on OPT-175B as well as the corresponding post on the Meta AI Research Blog."
    }
  ]
}