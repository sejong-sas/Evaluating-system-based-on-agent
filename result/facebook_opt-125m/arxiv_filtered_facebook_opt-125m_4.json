{
  "4-1 (Pre-training Data)": "The quotations describe a single, unified pre-training mixture that was constructed specifically for the OPT-175B model and, by extension, the broader family of OPT models. The corpus is said to be “created by a union of five datasets,” consisting of: (1) three separate datasets that had previously been used for RoBERTa, (2) a subset taken from the Pile, and (3) the Pushshift.io Reddit dataset that had earlier been processed in Roller et al., 2021. The designers state that the training data for OPT-175B was “selected based on a combination of breadth and availability,” indicating that wide topical coverage and public accessibility guided inclusion. A CommonCrawl snapshot forms the temporal boundary of the crawl-based component; newer dialogue resources such as MultiSessionChat (MSC) and Wizard of Internet (WoI) were released after that snapshot, so the authors note “minimal risk of leakage” from those materials into the training set. Evaluation on a subset of MSC yields a perplexity of 9.7 and a UF1 of 0.177, demonstrating that the pre-training mixture generalizes to held-out conversational data. The Pushshift.io Reddit portion is identified as “a primary data source for OPT-175B” and is flagged by prior work (Nangia et al., 2020) as containing higher rates of stereotypes and discriminatory language than Wikipedia; consequently, the authors warn that the model may have internalized such biases, affecting downstream behaviour on benchmarks like CrowS-Pairs. Finally, multiple statements explicitly confirm that “this dataset was used to pre-train the OPT models,” underscoring that the same multi-corpus mixture underlies the entire OPT series.",
  "4-2 (Fine-tuning Data)": "The supplied quotations indicate that no supervised fine-tuning data has yet been applied to the model: “OPT-175B, in a fully unsupervised setting, performs competitively against fully supervised models.” At the same time, the authors recommend that “future experimentation of OPT-175B for dialogue should contain explicit fine-tuning on curated datasets in order to improve the safety profile.” Thus, the present release is trained solely on the pre-training mixture described above, and fine-tuning remains a planned, not executed, step aimed at enhancing safety and dialogue quality.",
  "4-3 (Reinforcement Learning Data)": "",
  "4-4 (Data Filtering)": "",
  "4-1 (Pre-training Data)__evidence": [
    {
      "source": "[sections/C.1 Motivation]",
      "quote": "The pre-training data for training the OPT-175B model was created by a union of five datasets, including three datasets used by RoBERTa (Liu et al., 2019b), a subset of the Pile (Gao et al., 2021a), along with the Pushshift.io Reddit dataset that was developed in (Baumgartner et al., 2020) and processed in (Roller et al., 2021)."
    },
    {
      "source": "[sections/C.5 Uses]",
      "quote": "Yes, this dataset was used to pre-train the OPT models."
    },
    {
      "source": "[sections/D.3 Data, Limitations, and Recommendations]",
      "quote": "Training data for OPT-175B was selected based on a combination of breadth and availability."
    },
    {
      "source": "[pdf_text]",
      "quote": "Furthermore, we evaluated OPT-175B on a subset of the ConvAI2-like MultiSessionChat (MSC) dataset (Xu et al., 2021b) and obtained a perplexity of 9.7 and UF1 of .177, indicating the model is generalizing well across multiple PersonaChat-like datasets. Since both MSC and WoI datasets were released after the CommonCrawl snapshot used in pre-training corpus, there is minimal risk of leakage."
    },
    {
      "source": "[pdf_text]",
      "quote": "Nangia et al. (2020) showed that Pushshift.io Reddit corpus has a higher incidence rate for stereotypes and discriminatory text than other corpora (e.g. Wikipedia). Given this is a primary data source for OPT-175B, the model may have learned more discriminatory associations, which directly impacts its performance on CrowS-Pairs."
    },
    {
      "source": "[sections/C.5 Uses]",
      "quote": "• Has the dataset been used for any tasks already? If so, please provide a description. Yes, this dataset was used to pre-train the OPT models."
    }
  ],
  "4-2 (Fine-tuning Data)__evidence": [
    {
      "source": "[pdf_text]",
      "quote": "OPT-175B, in a fully unsupervised setting, performs competitively against fully supervised models."
    },
    {
      "source": "[pdf_text]",
      "quote": "We conclude that future experimentation of OPT-175B for dialogue should contain explicit fine-tuning on curated datasets in order to improve the safety profile."
    }
  ],
  "4-3 (Reinforcement Learning Data)__evidence": [],
  "4-4 (Data Filtering)__evidence": []
}