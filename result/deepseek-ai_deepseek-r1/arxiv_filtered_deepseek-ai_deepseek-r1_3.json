{
  "2-3 (API)": "The only direct information about an API comes from an explicit statement that “The open source DeepSeek-R1, as well as its API, will benefit the research community to distill better smaller models in the future.”  From this we can say that an official, publicly referenced API exists for DeepSeek-R1, that the model itself is open-sourced, and that the authors view the API as a vehicle for community experimentation—specifically for exploring knowledge-distillation and reasoning-pattern transfer from large to small models. No further technical details (end-points, rate limits, authentication, hosting, etc.) are given, but the language makes it clear that (1) an API is already available, (2) it is tied directly to the released DeepSeek-R1 weights/code, and (3) the intended users are external researchers who can access this interface to run or integrate the model in their own workflows.",
  "3-1 (Pre-training)": "Pre-training details are sparse, but one sentence establishes the lineage and a key technique: “Specifically, we use DeepSeek-V3-Base as the base model and employ GRPO (Shao et al., 2024) as the RL framework to improve model performance in reasoning.”  This indicates that the foundation (initial checkpoint) for subsequent work is the DeepSeek-V3-Base model.  Although the quote frames GRPO as an RL framework, its inclusion in the same sentence with the base model suggests that an RL-centric regimen (GRPO) is applied early—at the stage the authors label as ‘base’—to strengthen reasoning ability before any later supervised fine-tuning or alignment.  Thus, the take-away for the pre-training phase is: (a) the project inherits weights from the DeepSeek-V3-Base family, and (b) GRPO is already integrated at this stage to steer the emerging representation toward stronger reasoning competence.",
  "3-2 (Fine-tuning)": "The fine-tuning pipeline is described in three complementary sentences.  First, the team “collect[s] thousands of cold-start data to fine-tune the DeepSeek-V3-Base as the starting point for RL,” establishing that an initial supervised fine-tuning step (SFT) is performed before any reinforcement learning.  Second, they quantify this phase: “We fine-tune DeepSeek-V3-Base for two epochs using the above curated dataset of about 800 k samples.”  Hence the SFT dataset ultimately grows from the initial ‘thousands’ to roughly 800 000 examples, and the model is trained for two full passes over that corpus.  Finally, the process outcome is made explicit: after these SFT steps “we obtained a checkpoint referred to as DeepSeek-R1.”  In short, fine-tuning proceeds as (1) cold-start data collection, (2) two-epoch SFT on ~0.8 M examples, producing the DeepSeek-R1 checkpoint that will subsequently serve as the starting point for further RL and alignment stages.",
  "3-3 (Reinforcement Learning)": "Reinforcement learning is carried out in multiple structured stages.  For the variant named DeepSeek-R1-Zero, the authors “adopt a rule-based reward system that mainly consists of two types of rewards,” indicating a hand-crafted reward design rather than purely learned or preference-model–based feedback.  After the earlier fine-tuning described above, they “apply the same large-scale reinforcement learning training process as employed in DeepSeek-R1-Zero,” implying that the rule-based reward schedule is reused for the main DeepSeek-R1 line, scaled up to large compute.  The full development pipeline for DeepSeek-R1 is summarized as containing “two RL stages aimed at discovering improved reasoning patterns and aligning with human preferences, as well as two SFT stages that serve as the seed for the model’s reasoning and non-reasoning capabilities.”  Therefore, RL is interleaved with supervised stages: (1) an RL stage that searches for better reasoning strategies, (2) a subsequent RL or alignment stage focused on human preference alignment, both of which are wrapped around the SFT checkpoints to progressively refine the model.",
  "2-3 (API)__evidence": [
    {
      "source": "[pdf_text]",
      "quote": "• We demonstrate that the reasoning patterns of larger models can be distilled into smaller models, resulting in better performance compared to the reasoning patterns discovered through RL on small models. The open source DeepSeek-R1, as well as its API, will benefit the research community to distill better smaller models in the future."
    },
    {
      "source": "[sections/Contributions]",
      "quote": "• We demonstrate that the reasoning patterns of larger models can be distilled into smaller models, resulting in better performance compared to the reasoning patterns discovered through RL on small models. The open source DeepSeek-R1, as well as its API, will benefit the research community to distill better smaller models in the future."
    }
  ],
  "3-1 (Pre-training)__evidence": [
    {
      "source": "[sections/Approach]",
      "quote": "Specifically, we use DeepSeek-V3-Base as the base model and employ GRPO (Shao et al., 2024) as the RL framework to improve model performance in reasoning."
    }
  ],
  "3-2 (Fine-tuning)__evidence": [
    {
      "source": "[sections/2.3.1 Cold Start]",
      "quote": "In this work, we collect thousands of cold-start data to fine-tune the DeepSeek-V3-Base as the starting point for RL."
    },
    {
      "source": "[sections/2.3.3 Rejection Sampling and Supervised Fine-Tuning]",
      "quote": "We fine-tune DeepSeek-V3-Base for two epochs using the above curated dataset of about 800k samples."
    },
    {
      "source": "[sections/Introduction]",
      "quote": "Specifically, we begin by collecting thousands of cold-start data to fine-tune the DeepSeek-V3-Base model. ... After these steps, we obtained a checkpoint referred to as DeepSeek-R1."
    }
  ],
  "3-3 (Reinforcement Learning)__evidence": [
    {
      "source": "[sections/2.2.2 Reward Modeling]",
      "quote": "To train DeepSeek-R1-Zero, we adopt a rule-based reward system that mainly consists of two types of rewards:"
    },
    {
      "source": "[sections/2.3.2 Reasoning-oriented Reinforcement Learning]",
      "quote": "After fine-tuning DeepSeek-V3-Base on the cold start data, we apply the same large-scale reinforcement learning training process as employed in DeepSeek-R1-Zero."
    },
    {
      "source": "[sections/Contributions]",
      "quote": "We introduce our pipeline to develop DeepSeek-R1. The pipeline incorporates two RL stages aimed at discovering improved reasoning patterns and aligning with human preferences, as well as two SFT stages that serve as the seed for the model’s reasoning and non-reasoning capabilities."
    }
  ]
}