{
  "2-3 (API)": "The quotes indicate that DeepSeek-R1 is accompanied by an officially provided, publicly accessible API. The API is explicitly mentioned as being part of the same open-source release as the model (e.g., “The open source DeepSeek-R1, as well as its API…”). The stated purpose of making the API available is to “benefit the research community,” specifically by enabling researchers to perform knowledge-distillation experiments in which the reasoning patterns of larger systems are transferred into smaller models. In short, DeepSeek-R1 is not merely downloadable weights; it also offers an online interface (API) that outside users can call, with the declared goal of facilitating further research and model distillation work.",
  "3-1 (Pre-training)": "",
  "3-2 (Fine-tuning)": "All fine-tuning information revolves around a multi-stage pipeline designed for DeepSeek-R1. First, the developers perform two SFT (supervised fine-tuning) stages that are explicitly described as “the seed for the model’s reasoning and non-reasoning capabilities.” To avoid the “unstable cold start phase” of beginning RL from a raw base model, they “construct and collect a small amount of long Chain-of-Thought (CoT) data” and use it to fine-tune the model, thereby producing an initial RL actor. After an RL phase (see 3-3), they reuse the RL checkpoint to generate new SFT data via rejection sampling; this fresh data is merged with additional supervised data that already exists from DeepSeek-V3 in multiple domains (“writing, factual QA, and self-cognition”). The combined dataset is then employed to “retrain the DeepSeek-V3-Base model.” Altogether, DeepSeek-R1’s pipeline alternates between SFT and RL: SFT(1) → RL(1) → SFT(2) → RL(2), with each SFT stage either seeding capabilities or consolidating knowledge extracted from previous RL checkpoints.",
  "3-3 (Reinforcement Learning)": "DeepSeek-R1’s reinforcement-learning strategy is detailed through comparisons with its variant DeepSeek-R1-Zero. DeepSeek-R1-Zero is said to be “trained via large-scale reinforcement learning (RL) without supervised fine-tuning,” and this purely-RL approach already yields “remarkable reasoning capabilities” and “numerous powerful and intriguing reasoning behaviors.” Building on that, DeepSeek-R1 itself “incorporates multi-stage training and cold-start data before RL” to further enhance reasoning. Concretely, the quoted pipeline description states that there are “two RL stages” in DeepSeek-R1: one stage aims at “discovering improved reasoning patterns,” and a later stage focuses on “aligning with human preferences.” The project team also cites related reinforcement-learning work (e.g., “Deepseek-prover-v1.5” that uses proof-assistant feedback combined with Monte-Carlo Tree Search), underlining their broader experience with RL techniques. In summary, DeepSeek-R1 relies on a multi-phase RLHF-style procedure: (1) an initial RL block that surfaces better reasoning strategies, followed by (2) a second RL block that performs preference alignment, with both blocks embedded inside a larger SFT↔RL iterative pipeline.",
  "2-3 (API)__evidence": [
    {
      "source": "[pdf_text]",
      "quote": "The open source DeepSeek-R1, as well as its API, will benefit the research community to distill better smaller models in the future."
    },
    {
      "source": "[pdf_text]",
      "quote": "• We demonstrate that the reasoning patterns of larger models can be distilled into smaller models, resulting in better performance compared to the reasoning patterns discovered through RL on small models. The open source DeepSeek-R1, as well as its API, will benefit the research community to distill better smaller models in the future."
    }
  ],
  "3-1 (Pre-training)__evidence": [],
  "3-2 (Fine-tuning)__evidence": [
    {
      "source": "[pdf_text]",
      "quote": "We introduce our pipeline to develop DeepSeek-R1. The pipeline incorporates two RL stages aimed at discovering improved reasoning patterns and aligning with human preferences, as well as two SFT stages that serve as the seed for the model’s reasoning and non-reasoning capabilities."
    },
    {
      "source": "[pdf_text]",
      "quote": "Unlike DeepSeek-R1-Zero, to prevent the early unstable cold start phase of RL training from the base model, for DeepSeek-R1 we construct and collect a small amount of long CoT data to fine-tune the model as the initial RL actor."
    },
    {
      "source": "[pdf_text]",
      "quote": "• We introduce our pipeline to develop DeepSeek-R1. The pipeline incorporates two RL stages aimed at discovering improved reasoning patterns and aligning with human preferences, as well as two SFT stages that serve as the seed for the model’s reasoning and non-reasoning capabilities."
    },
    {
      "source": "[pdf_text]",
      "quote": "Following this, we perform reasoning-oriented RL like DeepSeek-R1-Zero. Upon nearing convergence in the RL process, we create new SFT data through rejection sampling on the RL checkpoint, combined with supervised data from DeepSeek-V3 in domains such as writing, factual QA, and self-cognition, and then retrain the DeepSeek-V3-Base model."
    }
  ],
  "3-3 (Reinforcement Learning)__evidence": [
    {
      "source": "[pdf_text]",
      "quote": "DeepSeek-R1-Zero, a model trained via large-scale reinforcement learning (RL) without supervised fine-tuning (SFT) as a preliminary step, demonstrates remarkable reasoning capabilities."
    },
    {
      "source": "[pdf_text]",
      "quote": "Through RL, DeepSeek-R1-Zero naturally emerges with numerous powerful and intriguing reasoning behaviors."
    },
    {
      "source": "[pdf_text]",
      "quote": "To address these issues and further enhance reasoning performance, we introduce DeepSeek-R1, which incorporates multi-stage training and cold-start data before RL."
    },
    {
      "source": "[pdf_text]",
      "quote": "• We introduce our pipeline to develop DeepSeek-R1. The pipeline incorporates two RL stages aimed at discovering improved reasoning patterns and aligning with human preferences, as well as two SFT stages that serve as the seed for the model’s reasoning and non-reasoning capabilities."
    },
    {
      "source": "[pdf_text]",
      "quote": "H. Xin, Z. Z. Ren, J. Song, Z. Shao, W. Zhao, H. Wang, B. Liu, L. Zhang, X. Lu, Q. Du, W. Gao, Q. Zhu, D. Yang, Z. Gou, Z. F. Wu, F. Luo, and C. Ruan. Deepseek-prover-v1.5: Harnessing proof assistant feedback for reinforcement learning and monte-carlo tree search, 2024. URL https://arxiv.org/abs/2408.08152."
    }
  ]
}