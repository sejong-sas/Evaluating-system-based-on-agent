{
  "1-5 (Architecture)": "For the target model series deepseek-r1, the available configuration lines state that \"DeepSeek-R1-Zero & DeepSeek-R1 are trained based on DeepSeek-V3-Base.\"  The same block also lists the exact architecture string as \"DeepseekV3ForCausalLM\" and identifies the overall \"model_type\" as \"deepseek_v3\".  Taken together, these quotes show that every deepseek-r1 variant inherits its core network design directly from the earlier DeepSeek-V3-Base backbone and is instantiated through the DeepseekV3ForCausalLM class, which is recorded inside the model configuration under the deepseek_v3 type label.",
  "1-5 (Architecture)__evidence": [
    {
      "source": "[readme]",
      "quote": "DeepSeek-R1-Zero & DeepSeek-R1 are trained based on DeepSeek-V3-Base."
    },
    {
      "source": "[config]",
      "quote": "\"architectures\": [\n    \"DeepseekV3ForCausalLM\"\n  ],"
    },
    {
      "source": "[config]",
      "quote": "\"model_type\": \"deepseek_v3\","
    }
  ],
  "1-6 (Tokenizer)": "The tokenizer used with the deepseek-r1 family is slightly modified from the open-source baselines employed for DeepSeek-R1-Distill, as the authors note: \"We slightly change their configs and tokenizers. Please use our setting to run these models.\"  Explicit configuration fields give a \"vocab_size\" of 129 280, a \"bos_token_id\" of 0, and an \"eos_token_id\" of 1, indicating that inference or fine-tuning should rely on these specific special-token IDs and the enlarged vocabulary when using deepseek-r1 checkpoints.",
  "1-6 (Tokenizer)__evidence": [
    {
      "source": "[readme]",
      "quote": "DeepSeek-R1-Distill models are fine-tuned based on open-source models, using samples generated by DeepSeek-R1.\nWe slightly change their configs and tokenizers. Please use our setting to run these models."
    },
    {
      "source": "[config]",
      "quote": "\"vocab_size\": 129280,"
    },
    {
      "source": "[config]",
      "quote": "\"bos_token_id\": 0,"
    },
    {
      "source": "[config]",
      "quote": "\"eos_token_id\": 1,"
    }
  ],
  "2-1 (Hardware)": "No hardware details (GPU/TPU model counts or compute totals) are provided for deepseek-r1 in the supplied quotes.",
  "2-1 (Hardware)__evidence": [],
  "2-2 (Software)": "The training stack for deepseek-r1 is documented only by the presence of the Transformers library: the configuration shows \"library_name: transformers\" with recorded versions \"4.46.3\" and \"4.39.3\".  These lines confirm that Hugging Face Transformers was the primary framework used, and that at least two close version tags (4.39.3 and the newer 4.46.3) were associated with the project.",
  "2-2 (Software)__evidence": [
    {
      "source": "[readme]",
      "quote": "library_name: transformers"
    },
    {
      "source": "[config]",
      "quote": "\"transformers_version\": \"4.46.3\","
    },
    {
      "source": "[generation_config]",
      "quote": "\"transformers_version\": \"4.39.3\""
    }
  ]
}