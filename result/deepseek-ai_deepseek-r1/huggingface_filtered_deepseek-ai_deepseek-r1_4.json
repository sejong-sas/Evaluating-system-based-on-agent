{
  "4-1 (Pre-training Data)": "The project states only two concrete facts about pre-training for the target model family. First, “DeepSeek-R1-Zero & DeepSeek-R1 are trained based on DeepSeek-V3-Base,” establishing that the R1 line is not built entirely from scratch but is further trained from the earlier DeepSeek-V3-Base checkpoint. Second, the authors note that “we introduce DeepSeek-R1, which incorporates cold-start data before RL.” Hence the pre-training recipe entails (i) inheritance of the DeepSeek-V3-Base weights and (ii) an additional cold-start data segment that is injected prior to any reinforcement-learning phase. No other details—such as data sources, licenses, or token counts—are disclosed in the supplied material.",
  "4-1 (Pre-training Data)__evidence": [
    {
      "source": "[readme]",
      "quote": "DeepSeek-R1-Zero & DeepSeek-R1 are trained based on DeepSeek-V3-Base."
    },
    {
      "source": "[readme]",
      "quote": "we introduce DeepSeek-R1, which incorporates cold-start data before RL."
    }
  ],
  "4-2 (Fine-tuning Data)": "Fine-tuning for the DeepSeek-R1 ecosystem is described as a two-stage supervised pipeline followed by, or interleaved with, RL. One quotation explains that “The pipeline incorporates two RL stages … as well as two SFT stages that serve as the seed for the model’s reasoning and non-reasoning capabilities,” confirming the existence of two dedicated supervised fine-tuning (SFT) phases whose data initialise those capabilities. A second quotation specifies how distillation variants obtain their training signals: “DeepSeek-R1-Distill models are fine-tuned based on open-source models, using samples generated by DeepSeek-R1.” Thus the SFT data for the distill line consist entirely of DeepSeek-R1-generated examples rather than human-written corpora. The final quotation adds quantity and licensing information for one subset: “DeepSeek-R1-Distill-Qwen-… are derived from Qwen-2.5 series … and now finetuned with 800k samples curated with DeepSeek-R1.” In summary, fine-tuning relies on two SFT stages seeded with 800 000 R1-generated examples (for the Qwen-based distill models) and is executed on top of multiple open-source model backbones that remain under their original Apache-2.0 license.",
  "4-2 (Fine-tuning Data)__evidence": [
    {
      "source": "[readme]",
      "quote": "The pipeline incorporates two RL stages aimed at discovering improved reasoning patterns and aligning with human preferences, as well as two SFT stages that serve as the seed for the model's reasoning and non-reasoning capabilities."
    },
    {
      "source": "[readme]",
      "quote": "DeepSeek-R1-Distill models are fine-tuned based on open-source models, using samples generated by DeepSeek-R1."
    },
    {
      "source": "[readme]",
      "quote": "DeepSeek-R1-Distill-Qwen-1.5B, DeepSeek-R1-Distill-Qwen-7B, DeepSeek-R1-Distill-Qwen-14B and DeepSeek-R1-Distill-Qwen-32B are derived from Qwen-2.5 series, which are originally licensed under Apache 2.0 License, and now finetuned with 800k samples curated with DeepSeek-R1."
    }
  ],
  "4-3 (Reinforcement Learning Data)": "The reinforcement-learning component is defined by three explicit statements. First, “We directly apply reinforcement learning (RL) to the base model without relying on supervised fine-tuning (SFT) as a preliminary step,” indicating that at least one training track begins RL from the base weights alone. Second, “DeepSeek-R1-Zero, a model trained via large-scale reinforcement learning (RL) without supervised fine-tuning (SFT) as a preliminary step, demonstrated remarkable performance on reasoning,” giving a concrete example—DeepSeek-R1-Zero—where this RL-first strategy was used and shown effective. Third, “The pipeline incorporates two RL stages aimed at discovering improved reasoning patterns and aligning with human preferences,” showing that the overall training schedule reserves two distinct RL phases: one to enhance reasoning and one to align with preference data. Beyond these structural notes (direct RL, two stages, reasoning/human-preference focus), no specifics about the feedback format, task selection, or reward source are disclosed in the excerpts provided.",
  "4-3 (Reinforcement Learning Data)__evidence": [
    {
      "source": "[readme]",
      "quote": "We directly apply reinforcement learning (RL) to the base model without relying on supervised fine-tuning (SFT) as a preliminary step."
    },
    {
      "source": "[readme]",
      "quote": "DeepSeek-R1-Zero, a model trained via large-scale reinforcement learning (RL) without supervised fine-tuning (SFT) as a preliminary step, demonstrated remarkable performance on reasoning."
    },
    {
      "source": "[readme]",
      "quote": "The pipeline incorporates two RL stages aimed at discovering improved reasoning patterns and aligning with human preferences"
    }
  ],
  "4-4 (Data Filtering)": "The supplied quotations do not contain any statements about data-cleaning tools, numerical thresholds, duplicate removal, toxicity filters, or other filtering criteria for DeepSeek-R1. Consequently, no information on data-filtering procedures or their quantitative impact is available from the provided material.",
  "4-4 (Data Filtering)__evidence": []
}