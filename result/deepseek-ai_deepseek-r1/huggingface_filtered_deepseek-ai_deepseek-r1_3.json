{
  "2-3 (API)": "For DeepSeek-R1, the public can interact with the model in two main ways. First, real-time conversational access is offered through the official DeepSeek chat site at “chat.deepseek.com,” where users can enable the special “DeepThink” toggle to engage the model. Second, the provider exposes an OpenAI-compatible, programmatic interface: the DeepSeek Platform, reachable at “platform.deepseek.com.” This API is explicitly described as compatible with the standard OpenAI pattern, implying developers can integrate DeepSeek-R1 into their applications via familiar endpoint conventions while leveraging the model’s capabilities.",
  "2-3 (API)__evidence": [
    {
      "source": "[readme]",
      "quote": "You can chat with DeepSeek-R1 on DeepSeek's official website: [chat.deepseek.com](https://chat.deepseek.com), and switch on the button \"DeepThink\""
    },
    {
      "source": "[readme]",
      "quote": "We also provide OpenAI-Compatible API at DeepSeek Platform: [platform.deepseek.com](https://platform.deepseek.com/)"
    }
  ],
  "3-1 (Pre-training)": "The authors state that DeepSeek-R1-Zero and DeepSeek-R1 were both trained on top of the backbone called DeepSeek-V3-Base. Concrete architectural hyper-parameters are listed: a vocabulary size of 129,280 tokens, a hidden (model) dimension of 7,168, an intermediate (MLP) dimension of 18,432, a depth of 61 transformer layers, 128 attention heads, and support for sequences up to 163,840 positions. These values define the capacity of DeepSeek-R1’s pre-training configuration and show that the model inherits its fundamental architecture directly from the DeepSeek-V3-Base initialization before any subsequent stages such as RL or SFT.",
  "3-1 (Pre-training)__evidence": [
    {
      "source": "[readme]",
      "quote": "DeepSeek-R1-Zero & DeepSeek-R1 are trained based on DeepSeek-V3-Base."
    },
    {
      "source": "[config]",
      "quote": "\"vocab_size\": 129280,"
    },
    {
      "source": "[config]",
      "quote": "\"hidden_size\": 7168,"
    },
    {
      "source": "[config]",
      "quote": "\"intermediate_size\": 18432,"
    },
    {
      "source": "[config]",
      "quote": "\"num_hidden_layers\": 61,"
    },
    {
      "source": "[config]",
      "quote": "\"num_attention_heads\": 128,"
    },
    {
      "source": "[config]",
      "quote": "\"max_position_embeddings\": 163840,"
    }
  ],
  "3-2 (Fine-tuning)": "DeepSeek-R1’s own generated reasoning data is reused to improve other networks: the team fine-tunes several widely adopted dense models using those R1-produced samples. In addition, a set of “DeepSeek-R1-Distill” variants is created by fine-tuning open-source base models with the same R1-generated examples. Thus, the fine-tuning strategy positions DeepSeek-R1 outputs as high-quality supervision data for both research-community baselines and for distilled descendants that inherit R1’s reasoning strengths.",
  "3-2 (Fine-tuning)__evidence": [
    {
      "source": "[readme]",
      "quote": "Using the reasoning data generated by DeepSeek-R1, we fine-tuned several dense models that are widely used in the research community."
    },
    {
      "source": "[readme]",
      "quote": "DeepSeek-R1-Distill models are fine-tuned based on open-source models, using samples generated by DeepSeek-R1."
    }
  ],
  "3-3 (Reinforcement Learning)": "The reinforcement-learning strategy for DeepSeek-R1 is multifaceted. A special branch, DeepSeek-R1-Zero, is trained purely with large-scale RL, explicitly skipping any supervised fine-tuning (SFT) warm-up, yet still achieves strong reasoning ability. In the general R1 development pipeline, the team “directly” applies RL to the base model but also designs a four-stage process: two RL phases (one to uncover better reasoning patterns, another to align with human preferences) are interleaved with two SFT phases that seed the model’s general reasoning and non-reasoning skills. This indicates that RL is central to performance and alignment, while the SFT steps provide initial behavioral scaffolding.",
  "3-3 (Reinforcement Learning)__evidence": [
    {
      "source": "[readme]",
      "quote": "DeepSeek-R1-Zero, a model trained via large-scale reinforcement learning (RL) without supervised fine-tuning (SFT) as a preliminary step, demonstrated remarkable performance on reasoning."
    },
    {
      "source": "[readme]",
      "quote": "We directly apply reinforcement learning (RL) to the base model without relying on supervised fine-tuning (SFT) as a preliminary step."
    },
    {
      "source": "[readme]",
      "quote": "We introduce our pipeline to develop DeepSeek-R1. The pipeline incorporates two RL stages aimed at discovering improved reasoning patterns and aligning with human preferences, as well as two SFT stages that serve as the seed for the model's reasoning and non-reasoning capabilities."
    }
  ]
}