{
  "1-1 (Weights)": "The quotes explicitly state that the model family’s weights are openly released. One sentence notes: “To support the research community, we open-source DeepSeek-R1-Zero, DeepSeek-R1, and six dense models (1.5B, 7B, 8B, 14B, 32B, 70B) distilled from DeepSeek-R1 based on Qwen and Llama.”  Two further sentences reinforce that status: “The open source DeepSeek-R1, as well as its API, will benefit the research community to distill better smaller models in the future,” and “The open source DeepSeek-R1, as well as its API, will benefit the research community to distill better smaller models in the future.”  Taken together, these quotes confirm (1) public availability of DeepSeek-R1 and DeepSeek-R1-Zero weight files, (2) accompanying access through an API, and (3) release of six additional dense distilled checkpoints ranging in parameter count from 1.5 B to 70 B.  No access restrictions or gated-download language appears in the provided material; the repeated phrase “open-source” implies that anyone can obtain the checkpoints.",
  "1-2 (Code)": "No sentences in the supplied quotes mention the publication status of DeepSeek-R1 training code, data-processing scripts, configuration files, or any other part of the end-to-end training pipeline.  Therefore, no information about code availability can be extracted from the provided material.",
  "1-3 (License)": "The quote set contains no sentences that describe the license name, version, or any specific terms regarding use, modification, redistribution, or commercial rights.  Consequently, no licensing information is available from the given excerpts.",
  "1-4 (Paper)": "Three separate quotes provide evidence of an official technical report or paper: “DeepSeek-R1: Incentivizing Reasoning Capability in LLMs via Reinforcement Learning,” along with contextual references: “We introduce our first-generation reasoning models, DeepSeek-R1-Zero and DeepSeek-R1,” and “Figure 1 | Benchmark performance of DeepSeek-R1.”  Together, these lines show that (a) a formal document bearing that title exists, (b) it introduces both DeepSeek-R1-Zero and DeepSeek-R1 as the first generation of the series, and (c) it contains at least one benchmark-summary figure (Figure 1) highlighting performance metrics.  No further bibliographic details (venue, date, authorship, URL) appear in the provided quotes.",
  "1-1 (Weights)__evidence": [
    {
      "source": "[abstract]",
      "quote": "To support the research community, we open-source DeepSeek-R1-Zero, DeepSeek-R1, and six dense models (1.5B, 7B, 8B, 14B, 32B, 70B) distilled from DeepSeek-R1 based on Qwen and Llama."
    },
    {
      "source": "[sections/Introduction 1.1 Contributions]",
      "quote": "The open source DeepSeek-R1, as well as its API, will benefit the research community to distill better smaller models in the future."
    },
    {
      "source": "[sections/Contributions]",
      "quote": "• We demonstrate that the reasoning patterns of larger models can be distilled into smaller models, resulting in better performance compared to the reasoning patterns discovered through RL on small models. The open source DeepSeek-R1, as well as its API, will benefit the research community to distill better smaller models in the future."
    }
  ],
  "1-2 (Code)__evidence": [],
  "1-3 (License)__evidence": [],
  "1-4 (Paper)__evidence": [
    {
      "source": "[title]",
      "quote": "DeepSeek-R1: Incentivizing Reasoning Capability in LLMs via Reinforcement Learning"
    },
    {
      "source": "[abstract]",
      "quote": "We introduce our first-generation reasoning models, DeepSeek-R1-Zero and DeepSeek-R1."
    },
    {
      "source": "[pdf_text]",
      "quote": "Figure 1 | Benchmark performance of DeepSeek-R1."
    }
  ],
  "1-5 (Architecture)": "The authors report that they \"open-source DeepSeek-R1-Zero, DeepSeek-R1, and six dense models (1.5B, 7B, 8B, 14B, 32B, 70B) distilled from DeepSeek-R1 based on Qwen and Llama,\" establishing that the main release family centres on DeepSeek-R1 and that, besides the full model, a set of size-reduced, fully-dense variants are derived from it. In the same release material they note that \"DeepSeek-R1 outputs are capped at a maximum of 32,768 tokens for each benchmark,\" which fixes the maximum sequence length the architecture can generate in evaluation scenarios. A comparative architecture table lists \"DeepSeek R1\" as an \"MoE\" model with \"# Activated Params … 37 B\" and \"# Total Params … 671 B,\" making clear that DeepSeek-R1 is implemented as a Mixture-of-Experts system in which 37 billion parameters are active per token while the complete parameter pool sums to roughly 671 billion.",
  "1-6 (Tokenizer)": "",
  "2-1 (Hardware)": "",
  "2-2 (Software)": "",
  "1-5 (Architecture)__evidence": [
    {
      "source": "[pdf_text]",
      "quote": "To support the research community, we open-source DeepSeek-R1-Zero, DeepSeek-R1, and six dense models (1.5B, 7B, 8B, 14B, 32B, 70B) distilled from DeepSeek-R1 based on Qwen and Llama."
    },
    {
      "source": "[pdf_text]",
      "quote": "DeepSeek-R1 outputs are capped at a maximum of 32,768 tokens for each benchmark."
    },
    {
      "source": "[sections/Experiment]",
      "quote": "Claude-3.5- GPT-4o DeepSeek V3 OpenAI o1-mini OpenAI o1-1217 DeepSeek R1\nArchitecture                     -      -      MoE      -      -      MoE\n# Activated Params               -      -       37B     -      -      37B\n# Total Params                   -      -      671B     -      -     671B"
    }
  ],
  "1-6 (Tokenizer)__evidence": [],
  "2-1 (Hardware)__evidence": [],
  "2-2 (Software)__evidence": [],
  "2-3 (API)": "The only direct information about an API comes from an explicit statement that “The open source DeepSeek-R1, as well as its API, will benefit the research community to distill better smaller models in the future.”  From this we can say that an official, publicly referenced API exists for DeepSeek-R1, that the model itself is open-sourced, and that the authors view the API as a vehicle for community experimentation—specifically for exploring knowledge-distillation and reasoning-pattern transfer from large to small models. No further technical details (end-points, rate limits, authentication, hosting, etc.) are given, but the language makes it clear that (1) an API is already available, (2) it is tied directly to the released DeepSeek-R1 weights/code, and (3) the intended users are external researchers who can access this interface to run or integrate the model in their own workflows.",
  "3-1 (Pre-training)": "Pre-training details are sparse, but one sentence establishes the lineage and a key technique: “Specifically, we use DeepSeek-V3-Base as the base model and employ GRPO (Shao et al., 2024) as the RL framework to improve model performance in reasoning.”  This indicates that the foundation (initial checkpoint) for subsequent work is the DeepSeek-V3-Base model.  Although the quote frames GRPO as an RL framework, its inclusion in the same sentence with the base model suggests that an RL-centric regimen (GRPO) is applied early—at the stage the authors label as ‘base’—to strengthen reasoning ability before any later supervised fine-tuning or alignment.  Thus, the take-away for the pre-training phase is: (a) the project inherits weights from the DeepSeek-V3-Base family, and (b) GRPO is already integrated at this stage to steer the emerging representation toward stronger reasoning competence.",
  "3-2 (Fine-tuning)": "The fine-tuning pipeline is described in three complementary sentences.  First, the team “collect[s] thousands of cold-start data to fine-tune the DeepSeek-V3-Base as the starting point for RL,” establishing that an initial supervised fine-tuning step (SFT) is performed before any reinforcement learning.  Second, they quantify this phase: “We fine-tune DeepSeek-V3-Base for two epochs using the above curated dataset of about 800 k samples.”  Hence the SFT dataset ultimately grows from the initial ‘thousands’ to roughly 800 000 examples, and the model is trained for two full passes over that corpus.  Finally, the process outcome is made explicit: after these SFT steps “we obtained a checkpoint referred to as DeepSeek-R1.”  In short, fine-tuning proceeds as (1) cold-start data collection, (2) two-epoch SFT on ~0.8 M examples, producing the DeepSeek-R1 checkpoint that will subsequently serve as the starting point for further RL and alignment stages.",
  "3-3 (Reinforcement Learning)": "Reinforcement learning is carried out in multiple structured stages.  For the variant named DeepSeek-R1-Zero, the authors “adopt a rule-based reward system that mainly consists of two types of rewards,” indicating a hand-crafted reward design rather than purely learned or preference-model–based feedback.  After the earlier fine-tuning described above, they “apply the same large-scale reinforcement learning training process as employed in DeepSeek-R1-Zero,” implying that the rule-based reward schedule is reused for the main DeepSeek-R1 line, scaled up to large compute.  The full development pipeline for DeepSeek-R1 is summarized as containing “two RL stages aimed at discovering improved reasoning patterns and aligning with human preferences, as well as two SFT stages that serve as the seed for the model’s reasoning and non-reasoning capabilities.”  Therefore, RL is interleaved with supervised stages: (1) an RL stage that searches for better reasoning strategies, (2) a subsequent RL or alignment stage focused on human preference alignment, both of which are wrapped around the SFT checkpoints to progressively refine the model.",
  "2-3 (API)__evidence": [
    {
      "source": "[pdf_text]",
      "quote": "• We demonstrate that the reasoning patterns of larger models can be distilled into smaller models, resulting in better performance compared to the reasoning patterns discovered through RL on small models. The open source DeepSeek-R1, as well as its API, will benefit the research community to distill better smaller models in the future."
    },
    {
      "source": "[sections/Contributions]",
      "quote": "• We demonstrate that the reasoning patterns of larger models can be distilled into smaller models, resulting in better performance compared to the reasoning patterns discovered through RL on small models. The open source DeepSeek-R1, as well as its API, will benefit the research community to distill better smaller models in the future."
    }
  ],
  "3-1 (Pre-training)__evidence": [
    {
      "source": "[sections/Approach]",
      "quote": "Specifically, we use DeepSeek-V3-Base as the base model and employ GRPO (Shao et al., 2024) as the RL framework to improve model performance in reasoning."
    }
  ],
  "3-2 (Fine-tuning)__evidence": [
    {
      "source": "[sections/2.3.1 Cold Start]",
      "quote": "In this work, we collect thousands of cold-start data to fine-tune the DeepSeek-V3-Base as the starting point for RL."
    },
    {
      "source": "[sections/2.3.3 Rejection Sampling and Supervised Fine-Tuning]",
      "quote": "We fine-tune DeepSeek-V3-Base for two epochs using the above curated dataset of about 800k samples."
    },
    {
      "source": "[sections/Introduction]",
      "quote": "Specifically, we begin by collecting thousands of cold-start data to fine-tune the DeepSeek-V3-Base model. ... After these steps, we obtained a checkpoint referred to as DeepSeek-R1."
    }
  ],
  "3-3 (Reinforcement Learning)__evidence": [
    {
      "source": "[sections/2.2.2 Reward Modeling]",
      "quote": "To train DeepSeek-R1-Zero, we adopt a rule-based reward system that mainly consists of two types of rewards:"
    },
    {
      "source": "[sections/2.3.2 Reasoning-oriented Reinforcement Learning]",
      "quote": "After fine-tuning DeepSeek-V3-Base on the cold start data, we apply the same large-scale reinforcement learning training process as employed in DeepSeek-R1-Zero."
    },
    {
      "source": "[sections/Contributions]",
      "quote": "We introduce our pipeline to develop DeepSeek-R1. The pipeline incorporates two RL stages aimed at discovering improved reasoning patterns and aligning with human preferences, as well as two SFT stages that serve as the seed for the model’s reasoning and non-reasoning capabilities."
    }
  ],
  "4-1 (Pre-training Data)": "",
  "4-2 (Fine-tuning Data)": "The fine-tuning stage for the DeepSeek series begins with \"collect[ing] thousands of cold-start data to fine-tune the DeepSeek-V3-Base as the starting point for RL.\"  From this effort the authors obtain \"the above curated dataset of about 800k samples,\" and they \"fine-tune DeepSeek-V3-Base for two epochs\" on that corpus.  The overall methodology that ultimately yields DeepSeek-R1 is outlined as follows: \"We introduce our pipeline to develop DeepSeek-R1.  The pipeline incorporates two RL stages aimed at discovering improved reasoning patterns and aligning with human preferences, as well as two SFT stages that serve as the seed for the model’s reasoning and non-reasoning capabilities.\"  After DeepSeek-R1 itself has generated additional reasoning-focused material, the team extends the impact of that data: \"Using the reasoning data generated by DeepSeek-R1, we fine-tuned several dense models that are widely used in the research community.\"  In combination, these statements describe a fine-tuning regimen that (1) starts from a modest set of manually gathered cold-start examples, (2) scales to an 800 k-sample curated dataset, (3) applies two full epochs of supervised updates to DeepSeek-V3-Base, (4) interleaves those supervised runs with two dedicated RL stages inside a larger pipeline, and (5) eventually re-uses the resulting DeepSeek-R1 reasoning outputs to fine-tune other popular dense architectures.",
  "4-3 (Reinforcement Learning Data)": "The reinforcement-learning phase is repeatedly described as the same \"large-scale reinforcement learning training process as employed in DeepSeek-R1-Zero.\"  It is launched only \"after fine-tuning DeepSeek-V3-Base on the cold start data,\" and its explicit goal is to \"enhanc[e] the model’s reasoning capabilities, particularly in reasoning-intensive tasks such as coding, mathematics, science, and logic reasoning, which involve well-defined problems with clear solutions.\"  The reward design is rule-based and centers on two primary signals: \"Accuracy rewards: The accuracy reward model evaluates whether the response is correct,\" and \"Format rewards: … we employ a format reward model that enforces the model to put its thinking process between ‘<think>’ and ‘</think>’ tags.\"  A supplementary signal combats multilingual drift: \"To mitigate the issue of language mixing, we introduce a language consistency reward during RL training, which is calculated as the proportion of target language words in the CoT.\"  Thus, the RL dataset is implicitly composed of task instances from coding, math, science, and logic; its evaluations depend on correctness, adherence to a designated thinking-tag format, and the measured fraction of target-language tokens, all of which guide policy updates within the DeepSeek-R1 pipeline.",
  "4-4 (Data Filtering)": "Data cleaning in the DeepSeek-R1 development flow is centered on readability and language consistency.  During supervised data construction, the team states: \"When creating cold-start data for DeepSeek-R1, we design a readable pattern that includes a summary at the end of each response and filters out responses that are not reader-friendly.\"  They reiterate the motivation by contrasting with a prior variant: \"Readability: A key limitation of DeepSeek-R1-Zero is that its content is often not suitable for reading.  In contrast, when creating cold-start data for DeepSeek-R1, we design a readable pattern that includes a summary at the end of each response and filters out responses that are not reader-friendly.\"  Filtering therefore explicitly discards any items whose answers do not conform to the readable template.  During the RL stage, an additional automatic criterion is applied: \"To mitigate the issue of language mixing, we introduce a language consistency reward during RL training, which is calculated as the proportion of target language words in the CoT.\"  Although expressed as a reward, this proportion acts as a continuous filter that penalizes—and effectively suppresses—examples containing excessive words from non-target languages.  Together, the read-time response pattern enforcement and the language-proportion reward constitute the documented filtering mechanisms used when building and refining datasets for DeepSeek-R1.",
  "4-1 (Pre-training Data)__evidence": [],
  "4-2 (Fine-tuning Data)__evidence": [
    {
      "source": "[sections/Approach §2.3.1 Cold Start]",
      "quote": "In this work, we collect thousands of cold-start data to fine-tune the DeepSeek-V3-Base as the starting point for RL."
    },
    {
      "source": "[sections/Approach §2.3.3 Rejection Sampling and Supervised Fine-Tuning]",
      "quote": "We fine-tune DeepSeek-V3-Base for two epochs using the above curated dataset of about 800k samples."
    },
    {
      "source": "[sections/1.1 Contributions]",
      "quote": "We introduce our pipeline to develop DeepSeek-R1. The pipeline incorporates two RL stages aimed at discovering improved reasoning patterns and aligning with human preferences, as well as two SFT stages that serve as the seed for the model’s reasoning and non-reasoning capabilities."
    },
    {
      "source": "[sections/1.1 Contributions]",
      "quote": "Using the reasoning data generated by DeepSeek-R1, we fine-tuned several dense models that are widely used in the research community."
    }
  ],
  "4-3 (Reinforcement Learning Data)__evidence": [
    {
      "source": "[sections/Approach §2.3.2 Reasoning-oriented Reinforcement Learning]",
      "quote": "After fine-tuning DeepSeek-V3-Base on the cold start data, we apply the same large-scale reinforcement learning training process as employed in DeepSeek-R1-Zero. This phase focuses on enhancing the model’s reasoning capabilities, particularly in reasoning-intensive tasks such as coding, mathematics, science, and logic reasoning, which involve well-defined problems with clear solutions."
    },
    {
      "source": "[sections/Approach §2.2.2 Reward Modeling]",
      "quote": "To train DeepSeek-R1-Zero, we adopt a rule-based reward system that mainly consists of two types of rewards: • Accuracy rewards: The accuracy reward model evaluates whether the response is correct. • Format rewards: In addition to the accuracy reward model, we employ a format reward model that enforces the model to put its thinking process between ‘<think>’ and ‘</think>’ tags."
    },
    {
      "source": "[sections/2.2.2 Reward Modeling]",
      "quote": "To train DeepSeek-R1-Zero, we adopt a rule-based reward system that mainly consists of two types of rewards:"
    },
    {
      "source": "[sections/2.3.2 Reasoning-oriented Reinforcement Learning]",
      "quote": "After fine-tuning DeepSeek-V3-Base on the cold start data, we apply the same large-scale reinforcement learning training process as employed in DeepSeek-R1-Zero. To mitigate the issue of language mixing, we introduce a language consistency reward during RL training, which is calculated as the proportion of target language words in the CoT."
    }
  ],
  "4-4 (Data Filtering)__evidence": [
    {
      "source": "[sections/Approach §2.3.1 Cold Start]",
      "quote": "When creating cold-start data for DeepSeek-R1, we design a readable pattern that includes a summary at the end of each response and filters out responses that are not reader-friendly."
    },
    {
      "source": "[sections/2.3.1 Cold Start]",
      "quote": "Readability: A key limitation of DeepSeek-R1-Zero is that its content is often not suitable for reading. In contrast, when creating cold-start data for DeepSeek-R1, we design a readable pattern that includes a summary at the end of each response and filters out responses that are not reader-friendly."
    },
    {
      "source": "[sections/2.3.2 Reasoning-oriented Reinforcement Learning]",
      "quote": "After fine-tuning DeepSeek-V3-Base on the cold start data, we apply the same large-scale reinforcement learning training process as employed in DeepSeek-R1-Zero. To mitigate the issue of language mixing, we introduce a language consistency reward during RL training, which is calculated as the proportion of target language words in the CoT."
    }
  ],
  "__usage": {
    "fine_tuning": "used",
    "rl": "used"
  }
}