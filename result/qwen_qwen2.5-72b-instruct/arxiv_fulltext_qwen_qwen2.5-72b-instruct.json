{
  "model_id": "qwen/qwen2.5-72b-instruct",
  "full_texts": [
    {
      "arxiv_id": "2309.00071",
      "full_text": "YaRN: Efficient Context Window Extension of Large\nLanguage Models\nBowen Peng1∗\nJeffrey Quesnelle1†\nHonglu Fan23\nEnrico Shippole‡\n1Nous Research\n2EleutherAI\n3University of Geneva\nAbstract\nRotary Position Embeddings (RoPE) have been shown to effectively encode posi-\ntional information in transformer-based language models. However, these models\nfail to generalize past the sequence length they were trained on. We present YaRN\n(Yet another RoPE extensioN method), a compute-efficient method to extend the\ncontext window of such models, requiring 10x less tokens and 2.5x less training\nsteps than previous methods. Using YaRN, we show that LLaMA models can\neffectively utilize and extrapolate to context lengths much longer than their original\npre-training would allow, while also surpassing previous the state-of-the-art at\ncontext window extension. In addition, we demonstrate that YaRN exhibits the\ncapability to extrapolate beyond the limited context of a fine-tuning dataset. The\nmodels fine-tuned using YaRN has been made available and reproduced online up\nto 128k context length at https://github.com/jquesnelle/yarn.\n0\n20000\n40000\n60000\n80000\n100000\n120000\nContext Window\n2.2\n2.4\n2.6\n2.8\n3.0\n3.2\n3.4\n3.6\n3.8\nPerplexity (lower is better)\nCodeLlama-13b-hf\nYarn-Llama-2-13b-64k\nYarn-Llama-2-13b-128k\ntogethercomputer/LLaMA-2-7B-32K\nCodeLlama-7b-hf\nYarn-Llama-2-7b-64k\nYarn-Llama-2-7b-128k\nFigure 1: Sliding window perplexity (S = 256) of ten 128k Proof-pile documents truncated to evaluation\ncontext window size\n∗Reddit: /u/bloc97 GitHub: bloc97\n†Reddit: /u/emozilla X: @theemozilla GitHub: jquesnelle\n‡X: @EnricoShippole GitHub: conceptofmind\nPreprint. Under review.\narXiv:2309.00071v2  [cs.CL]  1 Nov 2023\n\n1\nIntroduction\nTransformer-based Large Language Models[40] (LLMs) have become the near-ubiquitous choice for\nmany natural language processing (NLP) tasks where long-range abilities such as in-context learning\n(ICL) has been crucial. In performing the NLP tasks, the maximal length of the sequences (the\ncontext window) determined by its training processes has been one of the major limits of a pretrained\nLLM. Being able to dynamically extend the context window via a small amount of fine-tuning (or\nwithout fine-tuning) has become more and more desirable. To this end, the position encodings of\ntransformers are the center of the discussions.\nThe original Transformer architecture used an absolute sinusoidal position encoding, which was later\nimproved to a learnable absolute position encoding [15]. Since then, relative positional encoding\nschemes [32] have further increased the performance of Transformers. Currently, the most popular\nrelative positional encodings are T5 Relative Bias [30], RoPE [34], XPos [35], and ALiBi [27].\nOne reoccurring limitation with positional encodings is the inability to generalize past the context\nwindow seen during training. While some methods such as ALiBi are able to do limited generalization,\nnone are able to generalize to sequences significantly longer than their pre-trained length [22].\nSome works have been done to overcome such limitation. [9] and concurrently [21] proposed to\nextend the context length by slightly modifying RoPE via Position Interpolation (PI) and fine-tuning\non a small amount of data. As an alternative, [6] proposed the \"NTK-aware\" interpolation by\ntaking the loss of high frequency into account. Since then, two improvements of the \"NTK-aware\"\ninterpolation have been proposed, with different emphasis:\n• the \"Dynamic NTK\" interpolation method [14] for pre-trained models without fine-tuning.\n• the \"NTK-by-parts\" interpolation method [7] which performs the best when fine-tuned on a\nsmall amount of longer-context data.\nThe \"NTK-aware\" interpolation and the \"Dynamic NTK\" interpolation have already seen their\npresence in the open-source models such as Code Llama [31] (using \"NTK-aware\" interpolation) and\nQwen 7B [2] (using \"Dynamic NTK\").\nIn this paper, in addition to making a complete account of the previous unpublished works on the\n\"NTK-aware\", the \"Dynamic NTK\" and the \"NTK-by-part\" interpolations, we present YaRN (Yet\nanother RoPE extensioN method), an improved method to efficiently extend the context window\nof models trained with Rotary Position Embeddings (RoPE) including the LLaMA [38], the GPT-\nNeoX [5], and the PaLM [10] families of models.\nYaRN reaches state-of-the-art performances in context window extensions after fine-tuning on less\nthan ∼0.1% of the original pre-training data. In the meantime, by combining with the inference-time\ntechnique called Dynamic Scaling, the Dynamic-YaRN allows for more than 2x context window\nextension without any fine-tuning.\n2\nBackground and Related Work\n2.1\nRotary Position Embeddings\nThe basis of our work is the Rotary Position Embedding (RoPE) introduced in [34]. We work on\na hidden layer where the set of hidden neurons are denoted by D. Given a sequence of vectors\nx1, · · · , xL ∈R|D|, following the notation of [34], the attention layer first converts the vectors into\nthe query vectors and the key vectors:\nqm = fq(xm, m) ∈R|D|, kn = fk(xn, n) ∈R|D|.\n(1)\nNext, the attention weights are calculated as\nsoftmax(qT\nmkn\np\n|D|\n),\n(2)\nwhere qm, kn are considered as column vectors so that qT\nmkn is simply the Euclidean inner product.\nIn RoPE, we first assume that |D| is even and identify the embedding space and the hidden states as\n2\n\ncomplex vector spaces:\nR|D| ∼= C|D|/2\nwhere the inner product qT k becomes the real part of the standard Hermitian inner product Re(q∗k).\nMore specifically, the isomorphisms interleave the real part and the complex part\n\u0000(xm)1, · · · , (xm)|D|\n\u0001\n7→\n\u0000(xm)1 + i(xm)2, · · · , ((xm)|D|−1 + i(xm)|D|)\n\u0001\n,\n(3)\n\u0000(qm)1, · · · , (qm)|D|\n\u0001\n7→\n\u0000(qm)1 + i(qm)2, · · · , ((qm)|D|−1 + i(qm)|D|)\n\u0001\n.\n(4)\nTo convert embeddings xm, xn into query and key vectors, we are first given R-linear operators\nWq, Wk : R|D| →R|D|.\nIn complex coordinates, the functions fq, fk are given by\nfq(xm, m) = eimθWqxm, fk(xn, n) = einθWkxn,\n(5)\nwhere θ = diag(θ1, · · · , θ|D|/2) is the diagonal matrix with θd = b−2d/|D| and b = 10000. This way,\nRoPE associates each (complex-valued) hidden neuron with a separate frequency θd. The benefit\nof doing so is that the dot product between the query vector and the key vector only depends on the\nrelative distance m −n as follows\n⟨fq(xm, m), fk(xn, n)⟩R\n(6)\n=Re(⟨fq(xm, m), fk(xn, n)⟩C)\n(7)\n=Re(x∗\nmW∗\nqWkxneiθ(m−n))\n(8)\n=g(xm, xn, m −n).\n(9)\nIn real coordinates, the RoPE can be written using the following function\nfW(xm, m, θd) =\n\n\n\n\n\n\n\ncosmθ1\n−sinmθ1\n0\n0\n· · ·\n0\n0\nsinmθ1\ncosmθ1\n0\n0\n· · ·\n0\n0\n0\n0\ncosmθ2\n−sinmθ2\n· · ·\n0\n0\n0\n0\nsinmθ2\ncosmθ2\n· · ·\n0\n0\n0\n0\n0\n0\n· · ·\ncosmθl\n−sinmθl\n0\n0\n0\n0\n· · ·\nsinmθl\ncosmθl\n\n\n\n\n\n\n\nWxm,\nso that\nfq = fWq, fk = fWk.\n2.2\nPosition Interpolation\nAs language models are usually pre-trained with a fixed context length, it is natural to ask how to\nextend the context length by fine-tuning on relatively less amount of data. For language models using\nRoPE as the position embedding, Chen et al. [9], and concurrently kaiokendev [21] proposed the\nPosition Interpolation (PI) to extend the context length beyond the pre-trained limit. While a direct\nextrapolation does not perform well on sequences w1, · · · , wL with L larger than the pre-trained\nlimit, they discovered that interpolating the position indicies within the pre-trained limit works well\nwith the help of a small amount of fine-tuning. Specifically, given a pre-trained language model with\nRoPE, they modify the RoPE by\nf ′\nW (xm, m, θd) = fW\n\u0012\nxm, mL\nL′ , θd\n\u0013\n,\n(10)\nwhere L′ > L is a new context window beyond the pre-trained limit. With the original pre-trained\nmodel plus the modified RoPE formula, they fine-tuned the language model further on several orders\nof magnitude fewer tokens (a few billion in Chen et al. [9]) and successfully acheived context window\nextension.\n3\n\n2.3\nAdditional Notation\nThe ratio between the extended context length and the original context length has been of special\nimportance, and we introduce the notation s defined by\ns = L′\nL ,\n(11)\nand we call s the scale factor.\nWe also rewrite and simplify Eq. 10 into the following general form:\nf ′\nW(xm, m, θd) = fW(xm, g(m), h(θd)),\n(12)\nwhere g(m), h(θd) are method-dependent functions. For PI, we have g(m) = m/s, h(θd) = θd. In\nthe subsequent sections, when we introduce a new interpolation method, we sometimes only specify\nthe functions g(m) and h(θd).\nAdditionally, we define λd as the wavelength of the RoPE embedding at d-th hidden dimension:\nλd = 2π\nθd\n= 2πb\n2d\n|D| .\n(13)\nThe wavelength describes the length of tokens needed in order for the RoPE embedding at dimension\nd to perform a full rotation (2π).\nGiven that some interpolation methods (eg. PI) do not care about the wavelength of the dimensions,\nwe will refer to those methods as \"blind\" interpolation methods, while others do (eg. YaRN), which\nwe will classify as \"targeted\" interpolation methods.\n2.4\nRelated work\nReRoPE [33] also aims to extend the context size of existing models pre-trained with RoPE, and claims\n\"infinite\" context length without needing any fine-tuning. This claim is backed by a monotonically\ndecreasing loss with increasing context length up to 16k on the Llama 2 13B model. It achieves\ncontext extension by modifying the attention mechanism and thus is not purely an embedding\ninterpolation method. Since it is currently not compatible with Flash Attention 2 [13] and requires\ntwo attention passes during inference, we do not consider it for comparison.\nConcurrently with our work, LM-Infinite [16] proposes similar ideas to YaRN, but focuses on \"on-the-\nfly\" length generalization for non-fine-tuned models. Since they also modify the attention mechanism\nof the models, it is not an embedding interpolation method and is not immediately compatible with\nFlash Attention 2.\n3\nMethodology\nWhereas PI stretches all RoPE dimensions equally, we find that the theoretical interpolation bound\ndescribed by PI [9] is insufficient at predicting the complex dynamics between RoPE and the LLM’s\ninternal embeddings. In the following subsections, we describe the main issues with PI we have\nindividually identified and solved, so as to give the readers the context, origin and justifications of\neach method which we use in concert to obtain the full YaRN method.\n3.1\nLoss of High Frequency information - \"NTK-aware\" interpolation\nIf we look at RoPE only from an information encoding perspective, it was shown in [36], using\nNeural Tangent Kernel (NTK) theory, that deep neural networks have trouble learning high frequency\ninformation if the input dimension is low and the corresponding embeddings lack high frequency\ncomponents. Here we can see the similarities: a token’s positional information is one-dimensional,\nand RoPE expands it to an n-dimensional complex vector embedding.\n4\n\nRoPE closely resembles Fourier Features [36] in many aspects, as it is possible to define RoPE as a\nspecial 1D case of a Fourier Feature. Stretching the RoPE embeddings indiscriminately results in the\nloss of important high frequency details which the network needs in order to resolve tokens that are\nboth very similar and very close together (the rotation describing the smallest distance needs to not\nbe too small for the network to be able to detect it).\nWe hypothesise that the slight increase of perplexity for short context sizes after fine-tuning on larger\ncontext sizes seen in PI [9] might be related to this problem. Under ideal circumstances, there is no\nreason that fine-tuning on larger context sizes should degrade the performance of smaller context\nsizes.\nIn order to resolve the problem of losing high frequency information when interpolating the RoPE\nembeddings, the \"NTK-aware\" interpolation was developed in [6]. Instead of scaling every dimension\nof RoPE equally by a factor s, we spread out the interpolation pressure across multiple dimensions\nby scaling high frequencies less and low frequencies more. One can obtain such a transformation in\nmany ways, but the simplest would be to perform a base change on the value of θ.\nMore precisely, following the notations set out in Section 2.3, we define the \"NTK-aware\" interpola-\ntion scheme as follows (see the Appendix A.1 for the details of the deduction).\nDefinition 1 The \"NTK-aware\" interpolation is a modification of RoPE by using Eq. 12 with the\nfollowing functions.\ng(m) = m\n(14)\nh(θd) = b′−2d/|D|,\n(15)\nwhere\nb′ = b · s\n|D|\n|D|−2 .\n(16)\nGiven the results from [6], this method performs much better at extending the context size of non-fine-\ntuned models compared to PI [9]. However, one major disadvantage of this method is that given it is\nnot just an interpolation scheme, some dimensions are slightly extrapolated to \"out-of-bound\" values,\nthus fine-tuning with \"NTK-aware\" interpolation [6] yields inferior results to PI [9]. Furthermore,\ndue to the \"out-of-bound\" values, the theoretical scale factor s does not accurately describe the true\ncontext extension scale. In practice, the scale value s has to be set higher than the expected scale for\na given context length extension.\nWe note that shortly before the release of this article, Code Llama [31] was released and uses\n\"NTK-aware\" scaling by manually scaling the base b to 1M.\n3.2\nLoss of Relative Local Distances - \"NTK-by-parts\" interpolation\nIn the case of blind interpolation methods like PI and \"NTK-aware\" interpolation, we treat all the\nRoPE hidden dimensions equally (as in they have the same effect on the network). However, there\nare strong clues that point us towards the need for targeted interpolation methods.\nIn this section, we think heavily in terms of the wavelengths λd defined in Eq. 13 in the formula of\nRoPE. For simplicity, we omit the subscript d in λd and the reader is encouraged to think about λ as\nthe wavelength of an arbitrary periodic function.\nOne interesting observation of RoPE embeddings is that given a context size L, there are some\ndimensions d where the wavelength is longer than the maximum context length seen during pretraining\n(λ > L), this suggests that some dimensions’ embeddings might not be distributed evenly in the\nrotational domain. In such cases, we presume having all unique position pairs implies that the\nabsolute positional information remains intact. On the contrary, when the wavelength is short, only\nrelative positional information is accessible to the network.\nMoreover, when we stretch all the RoPE dimensions either by a scale s or using a base change b′,\nall tokens become closer to each other, as the dot product of two vectors rotated by a lesser amount\nis bigger. This scaling severely impairs a LLM’s ability to understand small and local relationships\nbetween its internal embeddings. We hypothesize that such compression leads to the model being\nconfused on the positional order of close-by tokens, and consequently harming the model’s abilities.\n5\n\nIn order to remedy this issue, given the two previous observations that we have found, we choose not\nto interpolate the higher frequency dimensions at all while always interpolating the lower frequency\ndimensions. In particular,\n• if the wavelength λ is much smaller than the context size L, we do not interpolate;\n• if the wavelength λ is equal to or bigger than the context size L, we want to only interpolate\nand avoid any extrapolation (unlike the previous \"NTK-aware\" method);\n• dimensions in-between can have a bit of both, similar to the \"NTK-aware\" interpolation.\nAs a result, it is more convenient to introduce the ratio r = L\nλ between the original context size L and\nthe wavelength λ. In the d-th hidden state, the ratio r depends on d in the following way:\nr(d) = L\nλd\n=\nL\n2πb′ 2d\n|D| .\n(17)\nIn order to define the boundary of the different interpolation strategies as above, we introduce\ntwo extra parameters α, β. All hidden dimensions d where r(d) < α are those where we linearly\ninterpolate by a scale s (exactly like PI, avoiding any extrapolation), and the d where r(d) > β are\nthose where we do not interpolate at all. Define the ramp function γ to be\nγ(r) =\n\n\n\n\n\n\n\n0,\nif r < α\n1,\nif r > β\nr −α\nβ −α,\notherwise.\n(18)\nWith the help of the ramp function, the \"NTK-by-parts\" method can be described as follows.\nDefinition 2 The \"NTK-by-parts\" interpolation is a modification of RoPE by using Eq. 12 with the\nfollowing functions4.\ng(m) = m\n(19)\nh(θd) =\n\u0010\n1 −γ\n\u0000r(d)\n\u0001\u0011θd\ns + γ\n\u0000r(d)\n\u0001\nθd.\n(20)\nThe values of α and β should be tuned on a case-by-case basis. For example, we have found\nexperimentally that for the Llama family of models, good values for α and β are α = 1 and β = 32.\nUsing the techniques described in this section, a variant of the resulting method was released under\nthe name \"NTK-by-parts\" interpolation [7]. This improved method performs better than the previous\nPI [9] and \"NTK-aware\" 3.1 interpolation methods, both with non-fine-tuned models and with\nfine-tuned models, as shown in [7].\n3.3\nDynamic Scaling - \"Dynamic NTK\" interpolation\nIn a lot of use cases, multiple forward-passes are performed with varying sequence lengths from 1 to\nthe maximal context size. A typical example is the autoregressive generation where the sequence\nlengths increment by 1 after each step. There are two ways of applying an interpolation method that\nuses a scale factor s (including PI, \"NTK-aware\" and \"NTK-by-parts\"):\n1. Throughout the whole inference cycle, the embedding layer is fixed including the scale\nfactor s = L′/L where L′ is the fixed number of extended context size.\n2. In each forward-pass, the position embedding updates the scale factor s = max(1, l′/L)\nwhere l′ is the sequence length of the current sequence.\nThe problem of (1) is that the model may experience a performance discount at a length less than\nL and an abrupt degradation when the sequence length is longer than L′. But by doing Dynamic\n4The interpolation by linear ramp on h may have alternatives, such as a harmonic mean over θd/s and\nθd converted from a linear interpolation on wavelengths. The choice of h here was for the simplicity of\nimplementation, but both would work.\n6\n\nScaling as (2), it allows the model to gracefully degrade instead of immediately breaking when hitting\nthe trained context limit L′. We call this inference-time method the Dynamic Scaling method. When\nit is combined with \"NTK-awared\" interpolation, we call it \"Dynamic NTK\" interpolation. It first\nappeared in public as a reddit post in [14].\nOne notable fact is that the \"Dynamic NTK\" interpolation works exceptionally well on models pre-\ntrained on L without any finetuning (L′ = L). This is supported by the experiment in Appendix B.3.\nOften in the repeated forward-passes, the kv-caching [8] is applied so that we can reuse the previous\nkey-value vectors and improve the overall efficiency. We point out that in some implementations when\nthe RoPE embeddings are cached, some care has to be taken in order to modify it for Dynamic Scaling\nwith kv-caching. The correct implementation should cache the kv-embeddings before applying RoPE,\nas the RoPE embedding of every token changes when s changes.\n3.4\nYaRN\nIn addition to the previous interpolation techniques, we also observe that introducing a temperature t\non the logits before the attention softmax has a uniform impact on perplexity regardless of the data\nsample and the token position over the extended context window (See Appendix A.2). More precisely,\ninstead of Eq. 2, we modify the computation of attention weights into\nsoftmax\n \nqT\nmkn\nt\np\n|D|\n!\n.\n(21)\nThe reparametrization of RoPE as a set of 2D matrices has a clear benefit on the implementation of\nthis attention scaling: we can instead use a \"length scaling\" trick which scales both qm and kn by a\nconstant factor\np\n1/t by simply scaling the complex RoPE embeddings by the same amount. With\nthis, YaRN can effectively alter the attention mechanism without modifying its code. Furthermore, it\nhas zero overhead during both inference and training, as RoPE embeddings are generated in advance\nand are reused for all forward passes. Combining it with the \"NTK-by-parts\" interpolation, we have\nthe YaRN method.\nDefinition 3 By the \"YaRN method\", we refer to a combination of the attention scaling in Eq. 21 and\nthe \"NTK-by-parts\" interpolation introduced in Section 3.2.\nFor LLaMA and Llama 2 models, we recommend the following values:\nr\n1\nt = 0.1 ln(s) + 1.\n(22)\nThe equation above is found by fitting\np\n1/t at the lowest perplexity against the scale extension\nby various factors s using the \"NTK-by-parts\" method (Section 3.2) on LLaMA 7b, 13b, 33b and\n65b models without fine-tuning. We note that the same values of t also apply fairly well to Llama\n2 models (7b, 13b and 70b). It suggests that the property of increased entropy and the temperature\nconstant t may have certain degree of \"universality\" and may be generalizable across some models\nand training data.\nThe YaRN method combines all our findings and surpasses all previous methods in both fine-tuned\nand non-fine-tuned scenarios. Thanks to its low footprint, YaRN allows for direct compatibility with\nlibraries that modify the attention mechanism such as Flash Attention 2 [13].\n4\nExperiments\nWe show that YaRN successfully achieves context window extension of language models using RoPE\nas its position embedding. Moreover, this result is achieved with only 400 training steps, representing\napproximately 0.1% of the model’s original pre-training corpus, a 10x reduction from Rozière et al.\n[31] and 2.5x reduction in training steps from Chen et al. [9], making it highly compute-efficient for\ntraining with no additional inference costs. We calculate the perplexity of long documents and score\n7\n\non established benchmarks to evaluate the resulting models, finding that they surpass all other context\nwindow extension methods.\nWe broadly followed the training and evaluation procedures as outlined in [9].\n4.1\nTraining\nFor training, we extended the Llama 2 [39] 7B and 13B parameter models. No changes were made to\nthe LLaMA model architecture other than the calculation of the embedding frequencies as described\nin 3.4 with s = 16 and s = 32.\nWe used a learning rate of 2 × 10−5 with no weight decay and a linear warmup of 20 steps along with\nAdamW [24] β1 = 0.9 and β2 = 0.95. For s = 16 we fine-tuned for 400 steps with global batch\nsize 64 using PyTorch [26] Fully Sharded Data Parallelism [42] and Flash Attention 2 [13] on the\nPG19 dataset [29] chunked into 64k segments bookended with the BOS and EOS token. For s = 32\nwe followed the same procedure, but started from the finished s = 16 checkpoint and trained for an\nadditional 200 steps.\n4.2\nExtrapolation and Transfer Learning\nIn Code Llama [31], a dataset with 16k context was used with a scale factor set to s ≈88.6, which\ncorresponds to a context size of 355k. They show that the network extrapolates up to 100k context\nwithout ever seeing those context sizes during training. Similar to 3.1 and Rozière et al. [31], YaRN\nalso supports training with a higher scale factor s than the length of the dataset. Due to compute\nconstraints, we test only s = 32 by further fine-tuning the s = 16 model for 200 steps using the same\ndataset with 64k context.\nWe show in 4.3.1 that the s = 32 model successfully extrapolates up to 128k context using only 64k\ncontext during training. Unlike previous \"blind\" interpolation methods, YaRN is much more efficient\nat transfer learning when increasing the scale s. This demonstrates successful transfer learning from\ns = 16 to s = 32 without the network needing to relearn the interpolated embeddings, as the s = 32\nmodel is equivalent to the s = 16 model across the entire context size, despite only being trained on\ns = 32 for 200 steps.\n4.3\nEvaluation\nThe evaluations focus on three aspects:\n1. the perplexity scores of fine-tuned models with extended context window,\n2. the passkey retrieval task on fine-tuned models,\n3. the common LLM benchmark results of fine-tuned models,\n4.3.1\nLong Sequence Language Modeling\nTo evaluate the long sequence language modeling performances, we use the GovReport [18] and\nProof-pile [4] datasets both of which contain many long sequence samples. For all evaluations, the\ntest splits of both datasets were used exclusively. All perplexity evaluations were calculated using the\nsliding window method from Press et al. [27] with S = 256.\nFirstly, we evaluated how the model performed as the context window increased. We selected 10\nrandom samples from Proof-pile with at least 128k tokens each and evaluated the perplexity of each\nof these samples when truncated at 2k steps from a sequence length of 2k tokens through 128k tokens.\nTable 1 shows a side-by-side comparison of Llama-2 model extended from 4096 to 8192 context\nlength via PI (LLongMA-2 7b5), \"NTK-aware\" and YaRN. Note that PI and \"NTK-aware\" models\nwere trained using the methodology in Chen et al. [9], while YaRN used the same methodology but\n2.5x less training steps and data, as described in 4.\n5LLongMA-2 7b [28] is fine-tuned from Llama-2 7b, trained at 8k context length with PI using the RedPajama\ndataset [12].\n8\n\nExtension\nTrained\nContext\nEvaluation Context Window Size\nMethod\nTokens\nWindow\n2048\n4096\n6144\n8192\n10240\nPI (s = 2)\n1B\n8k\n3.92\n3.51\n3.51\n3.34\n8.07\nNTK (θ = 20k)\n1B\n8k\n4.20\n3.75\n3.74\n3.59\n6.24\nYaRN (s = 2)\n400M\n8k\n3.91\n3.50\n3.51\n3.35\n6.04\nTable 1: Sliding window perplexity (S = 256) of ten 128k Proof-pile documents over Llama-2 extended via PI,\nNTK and YaRN\nWe further evaluated YaRN at the scale factor s = 16, 32 and compared them against a few open-\nsource models fine-tuned from Llama-2 and extended to more than 32k context window such as\nTogether.ai [37] and \"NTK-aware\" Code Llama [31]. The results are summarized in Table 2 (with a\nmore detailed plot in Figure 1).\nModel\nModel\nContext\nExtension\nEvaluation Context Window Size\nSize\nName\nWindow\nMethod\n8192\n32768\n65536\n98304\n131072\n7B\nTogether\n32k\nPI\n3.50\n2.64\n> 102\n> 103\n> 104\n7B\nCode Llama\n100k\nNTK\n3.71\n2.74\n2.55\n2.54\n2.71\n7B\nYaRN (s = 16)\n64k\nYaRN\n3.51\n2.65\n2.42\n> 101\n> 101\n7B\nYaRN (s = 32)\n128k\nYaRN\n3.56\n2.70\n2.45\n2.36\n2.37\n13B\nCode Llama\n100k\nNTK\n3.54\n2.63\n2.41\n2.37\n2.54\n13B\nYaRN (s = 16)\n64k\nYaRN\n3.25\n2.50\n2.29\n> 101\n> 101\n13B\nYaRN (s = 32)\n128k\nYaRN\n3.29\n2.53\n2.31\n2.23\n2.24\nTable 2: Sliding window perplexity (S = 256) of ten 128k Proof-pile documents truncated to evaluation context\nwindow size\nWe observe that the model exhibits strong performance across the entire targeted context size, with\nYaRN interpolation being the first method to successfully extend the effective context size of Llama 2\nto 128k. Of particular note are the YaRN (s = 32) models, which show continued declining perplexity\nthrough 128k, despite the fine-tuning data being limited to 64k tokens in length, demonstrating that\nthe model is able to generalize to unseen context lengths.\nFurthermore, in Appendix B.1, we show the results of the average perplexity on 50 untruncated\nGovReport documents with at least 16k tokens per sample evaluated on the setting of 32k maximal\ncontext window without Dynamic Scaling in Table 4. Similar to the Proof-pile results, the GovReport\nresults show that fine-tuning with YaRN achieves good performance on long sequences.\n4.3.2\nPasskey Retrieval\nThe passkey retrieval task as defined in [25] measures a model’s ability to retrieve a simple passkey\n(i.e., a five-digit number) from amongst a large amount of otherwise meaningless text. For our\nevaluation of the models, we performed 10 iterations of the passkey retrieval task with the passkey\nplaced at a random location uniformly distributed across the evaluation context window on different\ncontext window sizes ranging from 8k to 128k. Both 7b and 13b models fine-tuned using YaRN at\n128k context size passes the passkey retrieval task with very high accuracy (> 99%) within the entire\ncontext window size. We show detailed results in Appendix B.2.\n4.3.3\nStandardized Benchmarks\nThe Hugging Face Open LLM Leaderboard [19] compares a multitude of LLMs across a standard-\nized set of four public benchmarks. Specifically, we use 25-shot ARC-Challenge [11], 10-shot\nHellaSwag [41], 5-shot MMLU [17], and 0-shot TruthfulQA [23].\nTo test the degradation of model performance under context extension, we evaluated our models\nusing this suite and compared it to established scores for the Llama 2 baselines as well as publicly\navailable PI and \"NTK-aware\" models. The results are summarized in Table 3.\n9\n\nModel\nModel\nContext\nExtension\nARC-c\nHellaswag\nMMLU\nTruthfulQA\nSize\nName\nWindow\nMethod\n7B\nLlama 2\n4k\nNone\n53.1\n77.8\n43.8\n39.0\n7B\nTogether\n32k\nPI\n47.6\n76.1\n43.3\n39.2\n7B\nCode Llama\n100k\nNTK\n39.9\n60.8\n31.1\n37.8\n7B\nYaRN (s = 16)\n64k\nYaRN\n52.3\n78.8\n42.5\n38.2\n7B\nYaRN (s = 32)\n128k\nYaRN\n52.1\n78.4\n41.7\n37.3\n13B\nLlama 2\n4k\nNone\n59.4\n82.1\n55.8\n37.4\n13B\nCode Llama\n100k\nNTK\n40.9\n63.4\n32.8\n43.8\n13B\nYaRN (s = 16)\n64k\nYaRN\n58.1\n82.3\n52.8\n37.8\n13B\nYaRN (s = 32)\n128k\nYaRN\n58.0\n82.2\n51.9\n37.3\nTable 3: Performance of context window extensions methods on the Hugging Face Open LLM benchmark suite\ncompared with original Llama 2 baselines\nWe observe that there is minimal performance degradation between the YaRN models and their\nrespective Llama 2 baselines. We also observe that there was on average a 0.49% drop in scores\nbetween the YaRN s = 16 and s = 32 models. From this we conclude that the the iterative extension\nfrom 64k to 128k results in negligible performance loss.\n5\nConclusion\nIn conclusion, we have shown that YaRN improves upon all existing RoPE interpolation methods\nand can act as a drop-in replacement to PI, with no downsides and minimal implementation effort.\nThe fine-tuned models preserve their original abilities on multiple benchmarks while being able\nto attend to a very large context size. Furthermore, YaRN allows efficient extrapolation with fine-\ntuning on shorter datasets and can take advantage of transfer learning for faster convergence, both of\nwhich are crucial under compute-constrained scenarios. Finally, we have shown the effectiveness of\nextrapolation with YaRN where it is able to \"train short, and test long\".\n10\n\n6\nReproducibility\nTo aid in reproducibility, we provide, as supplementary material, the entirety of of the code used to\ntrain the YaRN models in Table 2, as well as the evaluation code that produced Figure 1 and Tables\n1, 2, 3, 4, and 5. The code also contains implementations of various extension methods referenced\nthroughout the paper. For training YaRN, we used the publicly available PG19 dataset [29] tokenized\nto 64k tokens.\nReferences\n[1] Mistrallite. URL https://huggingface.co/amazon/MistralLite.\n[2] Introducing Qwen-7B: Open foundation and human-aligned models (of the state-of-the-arts).\nURL https://github.com/QwenLM/Qwen-7B/blob/main/tech_memo.md.\n[3] Long-data collections. URL https://huggingface.co/datasets/togethercomputer/\nLong-Data-Collections.\n[4] Z. Azerbayev, E. Ayers, , and B. Piotrowski. Proof-pile, 2022. URL https://github.com/\nzhangir-azerbayev/proof-pile.\n[5] S. Black, S. Biderman, E. Hallahan, Q. Anthony, L. Gao, L. Golding, H. He, C. Leahy,\nK. McDonell, J. Phang, M. Pieler, U. S. Prashanth, S. Purohit, L. Reynolds, J. Tow, B. Wang,\nand S. Weinbach. GPT-NeoX-20B: An open-source autoregressive language model, 2022.\narXiv: 2204.06745.\n[6] bloc97.\nNTK-Aware\nScaled\nRoPE\nallows\nLLaMA\nmodels\nto\nhave\nextended\n(8k+)\ncontext\nsize\nwithout\nany\nfine-tuning\nand\nminimal\nperplexity\ndegradation.,\n2023. URL https://www.reddit.com/r/LocalLLaMA/comments/14lz7j5/ntkaware_\nscaled_rope_allows_llama_models_to_have/.\n[7] bloc97. Add NTK-Aware interpolation \"by parts\" correction, 2023. URL https://github.\ncom/jquesnelle/scaled-rope/pull/1.\n[8] C. Chen.\nTransformer Inference Arithmetic, 2022.\nURL https://kipp.ly/blog/\ntransformer-inference-arithmetic/.\n[9] S. Chen, S. Wong, L. Chen, and Y. Tian. Extending context window of large language models\nvia positional interpolation, 2023. arXiv: 2306.15595.\n[10] A. Chowdhery, S. Narang, J. Devlin, M. Bosma, G. Mishra, A. Roberts, P. Barham, H. W.\nChung, C. Sutton, S. Gehrmann, P. Schuh, K. Shi, S. Tsvyashchenko, J. Maynez, A. Rao,\nP. Barnes, Y. Tay, N. Shazeer, V. Prabhakaran, E. Reif, N. Du, B. Hutchinson, R. Pope,\nJ. Bradbury, J. Austin, M. Isard, G. Gur-Ari, P. Yin, T. Duke, A. Levskaya, S. Ghemawat,\nS. Dev, H. Michalewski, X. Garcia, V. Misra, K. Robinson, L. Fedus, D. Zhou, D. Ippolito,\nD. Luan, H. Lim, B. Zoph, A. Spiridonov, R. Sepassi, D. Dohan, S. Agrawal, M. Omernick,\nA. M. Dai, T. S. Pillai, M. Pellat, A. Lewkowycz, E. Moreira, R. Child, O. Polozov, K. Lee,\nZ. Zhou, X. Wang, B. Saeta, M. Diaz, O. Firat, M. Catasta, J. Wei, K. Meier-Hellstern, D. Eck,\nJ. Dean, S. Petrov, and N. Fiedel. PaLM: Scaling language modeling with pathways, 2022.\narXiv: 2204.02311.\n[11] P. Clark, I. Cowhey, O. Etzioni, T. Khot, A. Sabharwal, C. Schoenick, and O. Tafjord. Think\nyou have solved question answering? try ARC, the AI2 Reasoning Challenge, 2018. arXiv:\n1803.05457.\n[12] T. Computer. Redpajama: An open source recipe to reproduce llama training dataset, 2023.\nURL https://github.com/togethercomputer/RedPajama-Data.\n[13] T. Dao. Flashattention-2: Faster attention with better parallelism and work partitioning, 2023.\narXiv: 2307.08691.\n11\n\n[14] emozilla. Dynamically Scaled RoPE further increases performance of long context LLaMA\nwith zero fine-tuning, 2023. URL https://www.reddit.com/r/LocalLLaMA/comments/\n14mrgpr/dynamically_scaled_rope_further_increases/.\n[15] J. Gehring, M. Auli, D. Grangier, D. Yarats, and Y. N. Dauphin. Convolutional sequence to\nsequence learning, 2017. arXiv: 1705.03122.\n[16] C. Han, Q. Wang, W. Xiong, Y. Chen, H. Ji, and S. Wang. LM-Infinite: Simple on-the-fly length\ngeneralization for large language models, 2023. arXiv: 2308.16137.\n[17] D. Hendrycks, C. Burns, S. Basart, A. Zou, M. Mazeika, D. Song, and J. Steinhardt. Measuring\nmassive multitask language understanding. Proceedings of the International Conference on\nLearning Representations (ICLR), 2021.\n[18] L. Huang, S. Cao, N. Parulian, H. Ji, and L. Wang. Efficient attentions for long document\nsummarization. In Proceedings of the 2021 Conference of the North American Chapter of the\nAssociation for Computational Linguistics: Human Language Technologies, pages 1419–1436.\nAssociation for Computational Linguistics, June 2021.\n[19] Hugging Face. Open LLM Leaderboard, 2023. URL https://huggingface.co/spaces/\nHuggingFaceH4/open_llm_leaderboard.\n[20] A. Q. Jiang, A. Sablayrolles, A. Mensch, C. Bamford, D. S. Chaplot, D. de las Casas, F. Bressand,\nG. Lengyel, G. Lample, L. Saulnier, L. R. Lavaud, M.-A. Lachaux, P. Stock, T. L. Scao, T. Lavril,\nT. Wang, T. Lacroix, and W. E. Sayed. Mistral 7b, 2023.\n[21] kaiokendev. Things I’m learning while training superhot., 2023. URL https://kaiokendev.\ngithub.io/til#extending-context-to-8k.\n[22] A. Kazemnejad, I. Padhi, K. N. Ramamurthy, P. Das, and S. Reddy. The impact of positional\nencoding on length generalization in transformers, 2023. arXiv: 2305.19466.\n[23] S. Lin, J. Hilton, and O. Evans. TruthfulQA: Measuring how models mimic human falsehoods.\nIn Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics\n(Volume 1: Long Papers), pages 3214–3252, May 2022.\n[24] I. Loshchilov and F. Hutter. Decoupled weight decay regularization. In International Conference\non Learning Representations, 2019.\n[25] A. Mohtashami and M. Jaggi. Landmark attention: Random-access infinite context length for\ntransformers, 2023. arXiv: 2305.16300.\n[26] A. Paszke, S. Gross, F. Massa, A. Lerer, J. Bradbury, G. Chanan, T. Killeen, Z. Lin,\nN. Gimelshein, L. Antiga, A. Desmaison, A. Köpf, E. Yang, Z. DeVito, M. Raison, A. Tejani,\nS. Chilamkurthy, B. Steiner, L. Fang, J. Bai, and S. Chintala. PyTorch: An imperative style,\nhigh-performance deep learning library. In NeurIPS, pages 8024–8035, 2019.\n[27] O. Press, N. Smith, and M. Lewis. Train Short, Test Long: Attention with linear biases enables\ninput length extrapolation. In International Conference on Learning Representations, 2022.\n[28] J. Quesnelle, E. Shippole, and \"Kaiokendev\". Llongma: Scaling rotary embeddings through lin-\near positional interpolation. https://huggingface.co/conceptofmind/LLongMA-2-7b/,\n2023.\n[29] J. W. Rae, A. Potapenko, S. M. Jayakumar, C. Hillier, and T. P. Lillicrap. Compressive\ntransformers for long-range sequence modelling. In International Conference on Learning\nRepresentations, 2020.\n[30] A. Roberts, C. Raffel, K. Lee, M. Matena, N. Shazeer, P. J. Liu, S. Narang, W. Li, and Y. Zhou.\nExploring the limits of transfer learning with a unified text-to-text transformer. Technical report,\nGoogle, 2019.\n12\n\n[31] B. Rozière, J. Gehring, F. Gloeckle, S. Sootla, I. Gat, X. E. Tan, Y. Adi, J. Liu, T. Remez,\nJ. Rapin, A. Kozhevnikov, I. Evtimov, J. Bitton, M. Bhatt, C. C. Ferrer, A. Grattafiori, W. Xiong,\nA. Défossez, J. Copet, F. Azhar, H. Touvron, L. Martin, N. Usunier, T. Scialom, and G. Synnaeve.\nCode Llama: Open foundation models for code, 2023. arXiv: 2308.12950.\n[32] P. Shaw, J. Uszkoreit, and A. Vaswani. Self-attention with relative position representations.\nIn Proceedings of the 2018 Conference of the North American Chapter of the Association for\nComputational Linguistics: Human Language Technologies, Volume 2 (Short Papers), pages\n464–468, New Orleans, Louisiana, June 2018. Association for Computational Linguistics.\n[33] J. Su. Rectified rotary position embeddings. https://github.com/bojone/rerope, 2023.\n[34] J. Su, Y. Lu, S. Pan, A. Murtadha, B. Wen, and Y. Liu. RoFormer: Enhanced transformer with\nrotary position embedding, 2022. arXiv: 2104.09864.\n[35] Y. Sun, L. Dong, B. Patra, S. Ma, S. Huang, A. Benhaim, V. Chaudhary, X. Song, and F. Wei. A\nlength-extrapolatable transformer, 2022. arXiv: 2212.10554.\n[36] M. Tancik, P. P. Srinivasan, B. Mildenhall, S. Fridovich-Keil, N. Raghavan, U. Singhal, R. Ra-\nmamoorthi, J. T. Barron, and R. Ng. Fourier features let networks learn high frequency functions\nin low dimensional domains. In Proceedings of the 34th International Conference on Neural\nInformation Processing Systems, NIPS’20, Red Hook, NY, USA, 2020. Curran Associates Inc.\nISBN 9781713829546.\n[37] Together.ai.\nLLaMA-2-7B-32K,\n2023.\nURL\nhttps://huggingface.co/\ntogethercomputer/LLaMA-2-7B-32K.\n[38] H. Touvron, T. Lavril, G. Izacard, X. Martinet, M.-A. Lachaux, T. Lacroix, B. Rozière, N. Goyal,\nE. Hambro, F. Azhar, A. Rodriguez, A. Joulin, E. Grave, and G. Lample. LLaMA: Open and\nefficient foundation language models, 2023. arXiv: 2302.13971.\n[39] H. Touvron, L. Martin, K. Stone, P. Albert, A. Almahairi, Y. Babaei, N. Bashlykov, S. Batra,\nP. Bhargava, S. Bhosale, D. Bikel, L. Blecher, C. C. Ferrer, M. Chen, G. Cucurull, D. Esiobu,\nJ. Fernandes, J. Fu, W. Fu, B. Fuller, C. Gao, V. Goswami, N. Goyal, A. Hartshorn, S. Hosseini,\nR. Hou, H. Inan, M. Kardas, V. Kerkez, M. Khabsa, I. Kloumann, A. Korenev, P. S. Koura, M.-A.\nLachaux, T. Lavril, J. Lee, D. Liskovich, Y. Lu, Y. Mao, X. Martinet, T. Mihaylov, P. Mishra,\nI. Molybog, Y. Nie, A. Poulton, J. Reizenstein, R. Rungta, K. Saladi, A. Schelten, R. Silva, E. M.\nSmith, R. Subramanian, X. E. Tan, B. Tang, R. Taylor, A. Williams, J. X. Kuan, P. Xu, Z. Yan,\nI. Zarov, Y. Zhang, A. Fan, M. Kambadur, S. Narang, A. Rodriguez, R. Stojnic, S. Edunov, and\nT. Scialom. Llama 2: Open foundation and fine-tuned chat models, 2023.\n[40] A. Vaswani, N. Shazeer, N. Parmar, J. Uszkoreit, L. Jones, A. N. Gomez, L. Kaiser, and\nI. Polosukhin. Attention is all you need. In Advances in Neural Information Processing Systems,\nvolume 30. Curran Associates, Inc., 2017.\n[41] R. Zellers, A. Holtzman, Y. Bisk, A. Farhadi, and Y. Choi. HellaSwag: Can a machine really\nfinish your sentence?\nIn Proceedings of the 57th Annual Meeting of the Association for\nComputational Linguistics, 2019.\n[42] Y. Zhao, A. Gu, R. Varma, L. Luo, C.-C. Huang, M. Xu, L. Wright, H. Shojanazeri, M. Ott,\nS. Shleifer, A. Desmaison, C. Balioglu, B. Nguyen, G. Chauhan, Y. Hao, and S. Li. PyTorch\nFSDP: Experiences on scaling fully sharded data parallel, 2023. arXiv: 2304.11277.\n13\n\nA\nAdditional details on interpolation methods\nA.1\nShort notes on the deduction of \"NTK-aware\" interpolation\nIn Section 3.1, we introduce a change of basis from b to b′ in the definition of \"NTK-aware\"\ninterpolation method. Here is a short note on its mathematical deduction.\nRecall that our goal is to spread out the interpolation pressure across the hidden dimensions using a\nbase-change instead of scaling the frequencies by a fixed factor s. The property we want to guarantee\nis that: The lowest frequency needs to be scaled as much as linear positional scaling and the highest\nfrequency to stay constant.\nWe introduce a new base b′ such that the last dimension matches the wavelength of linear interpolation\nwith a scale factor s. Since the original RoPE method skips odd dimensions in order to concatenate\nboth cos( 2πx\nλ ) and sin( 2πx\nλ ) components into a single embedding, the last dimension d ∈D is\n|D| −2.\nThe new base b′ can be chosen so that\nb′\n|D|−2\n|D|\n= s · b\n|D|−2\n|D| .\n(23)\nSolving for b′ yields\nb′ = b · s\n|D|\n|D|−2 .\n(24)\nA.2\nThe impact of pre-softmax scaling of YaRN on perplexity\nIn Section 3.4, we mention the impact of the factor t inside the softmax computation of attention\nweights. Here we fix 896 16k-token documents from RedPajama [12]6, and calculate their perplexity\nscores with different scaling 1/\n√\nt. The result is in Figure 2. For comparison, recall that our\nrecommended factor in this case (s = 8) is given by the following.\nr\n1\nt = 0.1 ln(s) + 1 ≈1.208.\n(25)\n6We choose RedPajama because it is the open-source dataset closest to the training dataset of LLaMA as far\nas we are aware of.\n14\n\nTo show the impact of the factor 1/\n√\nt on different token positions, we cut each 16k-token document\ninto chunks of 2048 tokens, and further plot the mean perplexity change comparing to t = 1 in\npercentages\nppl(t) −ppl(t = 1)\nppl(t = 1)\n(26)\nof each chunk. The plot is shown in Figure 3.\nTo further demonstrate the best values of t across all samples over different token positions, we plot\nthe sample counts with minimal perplexity at a given 1/\n√\nt for each of the 8 position segments over\nthe 16k-token range in Figure 4.\nWe observe that:\n• for a suitable t, a sample may obtain better perplexity scores across the extended context\nwindow;\n• the best value of t is mostly consistent across different samples and different positions.\nWe remark that this finding is consistent for different values of s and the best value of t follows our\nrecommended formula (Eq. 22) closely.\nB\nAdditional tables and charts\nB.1\nGovReport evaluations\nIn Section 4.3.1, we mention the evaluation on GovReport documents. The evaluation results are\ndetailed in Table 4 below.\nB.2\nPasskey Retrieval\nHere we can observe that the lowest perplexity point alone does not provide a comprehensive depiction\non the \"effective context size\" that an LLM can attend to. While the Code Llama 13b model exhibits\nincreasing perplexity above 100k context lengths, it was still able to accurately retrieve the passkey at\na context length of 128k. This suggest that while the output of Code Llama might start to degrade in\nquality above 100k context size, it is still able to maintain strong retrieval capabilities.\nIn addition, as YaRN with s = 32 was trained for 200 more steps than YaRN with s = 16 while\nhaving a higher passkey accuracy with similar perplexity, we hypothesize that perplexity may not be a\ngreat indicator of whether an LLM is able to attend to all tokens and does not exhaustively determine\nlong context performance. This also suggests that the YaRN models with s = 16 might be relatively\nundertrained for the passkey retrieval task.\nB.3\nDynamic scaling on models without any fine-tuning\nWe first recall from Section 3.3 that the Dynamic Scaling technique is an inference-time technique\nthat dynamically update the factor s in interpolation methods such as PI, \"NTK-by-parts\" and YaRN.\nWe choose the original Llama 2, fix a sample in GovReport and calculate its perplexity on a sliding\nwindow of 256 tokens using RoPE, Dynamic-PI and Dynamic-YaRN. Since the original maximal\ncontext length of Llama 2 is 4096, we observe that Dynamic Scaling effectively extend the inference\nlength and Dynamic-YaRN achieves better performance than Dynamic-PI. The resulting chart is in\nFigure 5.\nWe see that\n• Dynamic Scaling effectively prevents the blow-up of perplexity score beyond pretrained\ncontext window;\n• Dynamic-YaRN outperforms Dynamic-PI in terms of long-range perplexity on pretrained\nLlama-2 without any finetuning.\n15\n\nFigure 2: Fix s = 8, compare the LLaMA 7b perplexity on 896 16k-token documents over different scaling\n1/\n√\nt. The shaded area represents 1 standard deviation (68%).\nFigure 3: Fix s = 8, compare the mean of perplexity change percentages ppl(t) −ppl(t = 1)\nppl(t = 1)\nat different\nsegments of token positions on 896 16k-token documents over different scaling 1/\n√\nt.\nModel\nModel\nContext\nExtension\nPerplexity\nSize\nName\nWindow\nMethod\n7B\nTogether\n32k\nPI\n3.67\n7B\nCode Llama\n100k\nNTK\n4.44\n7B\nYaRN (s = 16)\n64k\nYaRN\n3.59\n7B\nYaRN (s = 32)\n128k\nYaRN\n3.64\n13B\nCode Llama\n100k\nNTK\n4.22\n13B\nYaRN (s = 16)\n64k\nYaRN\n3.35\n13B\nYaRN (s = 32)\n128k\nYaRN\n3.39\nTable 4: Sliding window perplexity (S = 256) of 50 long GovReport documents with a fixed context window\nsize of 32k\n16\n\nFigure 4: The sample counts (out of the 896 samples) with minimal perplexity at a given 1/\n√\nt for a given\nsegment of token positions over the 16k-token range.\nModel\nModel\nScaling\nContext\nTraining\nExtension\nPasskey\nPasskey\nSize\nName\nFactor (s)\nWindow\nData Context\nMethod\nContext\nAccuracy\n7B\nTogether\n4\n32k\n32k\nPI\n32k\n100%\n7B\nCode Llama\n88.6\n100k\n16k\nNTK\n112k\n94.3%\n7B\nYaRN\n16\n64k\n64k\nYaRN\n64k\n96.3%\n7B\nYaRN\n32\n128k\n64k\nYaRN\n128k\n99.4%\n13B\nCode Llama\n88.6\n100k\n16k\nNTK\n128k\n99.4%\n13B\nYaRN\n16\n64k\n64k\nYaRN\n64k\n97.5%\n13B\nYaRN\n32\n128k\n64k\nYaRN\n128k\n99.4%\nTable 5: Passkey retrieval performance of various models. The passkey context denotes the maximum tested\ncontext window size where the accuracy of passkey retrieval was >= 80%, and the passkey accuracy is the\naverage accuracy of passkey retrieval on all context sizes tested that were smaller or equal than the passkey\ncontext size.\nFigure 5: The comparison between RoPE, Dynamic-PI and Dynamic-YaRN using Llama 2 on a long\nGovReport sample. This model has not been finetuned for long context.\n17\n\n0\n20000\n40000\n60000\n80000\n100000\n120000\nContext Window\n2.2\n2.4\n2.6\n2.8\n3.0\n3.2\n3.4\nPerplexity (lower is better)\nYarn-Mistral-7b-64k\nYarn-Mistral-7b-128k\namazon/MistralLite\nmistralai/Mistral-7B-v0.1\nFigure 6: Sliding window perplexity (S = 256) of ten 128k Proof-pile documents truncated to evaluation\ncontext window size\nB.4\nMistral\nWe additionally extended the Mistral 7B v0.1 model [20], which broadly follows the Llama architec-\nture. For Mistral we trained a 64k context window model (s = 8) for 1000 steps using 16k sequence\nlengths with a constant learning rate of 1 × 10−6. The model’s sliding window attention size was set\nto the context window size, effectively disabling sliding window attention. We then trained for an\nadditional 500 steps at s = 16 to arrive at a 128k context window model. The training data was a mix\nof the pre-train and fine-tune splits of Together Computer’s Long-Data Collections [3].\nWe evaluated the models following the same procedure as described in 4.3.1, comparing against the\nbase v0.1 model and MistralLite [1], an NTK-aware (θ = 1M) version of v0.1. The results (Figure 6\nand Table 6) were consistent with those of the Llama family of models.\nModel\nModel\nContext\nExtension\nEvaluation Context Window Size\nSize\nName\nWindow\nMethod\n4096\n8192\n16384\n65536\n131072\n7B\nMistral v0.1\n8k\n-\n3.09\n2.96\n36.8\n> 103\n> 103\n7B\nMistralLite\n16k\nNTK\n3.26\n3.13\n47.3\n> 103\n> 103\n7B\nYaRN (s = 8)\n64k\nYaRN\n3.18\n3.04\n2.65\n2.20\n57.4\n7B\nYaRN (s = 16)\n128k\nYaRN\n3.21\n3.08\n2.68\n2.24\n2.19\nTable 6: Sliding window perplexity (S = 256) of ten 128k Proof-pile documents truncated to evaluation context\nwindow size\n18\n"
    },
    {
      "arxiv_id": "2407.10671",
      "full_text": "QWEN2 TECHNICAL REPORT\nAn Yang, Baosong Yang, Binyuan Hui, Bo Zheng, Bowen Yu, Chang Zhou, Chengpeng Li,\nChengyuan Li, Dayiheng Liu, Fei Huang, Guanting Dong, Haoran Wei, Huan Lin, Jialong Tang,\nJialin Wang, Jian Yang, Jianhong Tu, Jianwei Zhang, Jianxin Ma, Jianxin Yang, Jin Xu, Jingren\nZhou, Jinze Bai, Jinzheng He, Junyang Lin, Kai Dang, Keming Lu, Keqin Chen, Kexin Yang, Mei\nLi, Mingfeng Xue, Na Ni, Pei Zhang, Peng Wang, Ru Peng, Rui Men, Ruize Gao, Runji Lin, Shijie\nWang, Shuai Bai, Sinan Tan, Tianhang Zhu, Tianhao Li, Tianyu Liu, Wenbin Ge, Xiaodong Deng,\nXiaohuan Zhou, Xingzhang Ren, Xinyu Zhang, Xipin Wei, Xuancheng Ren, Xuejing Liu, Yang Fan,\nYang Yao, Yichang Zhang, Yu Wan, Yunfei Chu, Yuqiong Liu, Zeyu Cui, Zhenru Zhang, Zhifang\nGuo, and Zhihao Fan\nQwen Team, Alibaba Group∗\nABSTRACT\nThis report introduces the Qwen2 series, the latest addition to our large lan-\nguage models and large multimodal models. We release a comprehensive suite of\nfoundational and instruction-tuned language models, encompassing a parameter\nrange from 0.5 to 72 billion, featuring dense models and a Mixture-of-Experts\nmodel. Qwen2 surpasses most prior open-weight models, including its predecessor\nQwen1.5, and exhibits competitive performance relative to proprietary models\nacross diverse benchmarks on language understanding, generation, multilingual\nproficiency, coding, mathematics, and reasoning.\nThe flagship model, Qwen2-72B, showcases remarkable performance: 84.2 on\nMMLU, 37.9 on GPQA, 64.6 on HumanEval, 89.5 on GSM8K, and 82.4 on BBH as\na base language model. The instruction-tuned variant, Qwen2-72B-Instruct, attains\n9.1 on MT-Bench, 48.1 on Arena-Hard, and 35.7 on LiveCodeBench. Moreover,\nQwen2 demonstrates robust multilingual capabilities, proficient in approximately\n30 languages, spanning English, Chinese, Spanish, French, German, Arabic, Rus-\nsian, Korean, Japanese, Thai, Vietnamese, and more, underscoring its versatility\nand global reach.\nTo foster community innovation and accessibility, we have made the Qwen2 model\nweights openly available on Hugging Face1 and ModelScope2, and the supplemen-\ntary materials including example code on GitHub3. These platforms also include\nresources for quantization, fine-tuning, and deployment, facilitating a wide range\nof applications and research endeavors.\n∗Authors are ordered alphabetically by the first name.\n1https://huggingface.co/Qwen\n2https://modelscope.cn/organization/qwen\n3https://github.com/QwenLM/Qwen2\n1\narXiv:2407.10671v4  [cs.CL]  10 Sep 2024\n\nCONTENTS\n1\nIntroduction\n3\n2\nTokenizer & Model\n3\n2.1\nTokenizer . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n3\n2.2\nModel Architecture . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n4\n2.2.1\nQwen2 Dense Model . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n4\n2.2.2\nQwen2 Mixture-of-experts Model . . . . . . . . . . . . . . . . . . . . . .\n4\n2.2.3\nModel Configuration . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n5\n3\nPre-training\n5\n3.1\nPre-training Data . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n5\n3.2\nLong-context Training\n. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n6\n4\nPost-training\n6\n4.1\nPost-training Data . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n6\n4.1.1\nCollaborative Data Annotation . . . . . . . . . . . . . . . . . . . . . . . .\n7\n4.1.2\nAutomated Data Synthesis . . . . . . . . . . . . . . . . . . . . . . . . . .\n7\n4.2\nSupervised Fine-tuning . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n8\n4.3\nReinforcement Learning from Human Feedback . . . . . . . . . . . . . . . . . . .\n8\n5\nEvaluation\n8\n5.1\nBase Language Models . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n8\n5.1.1\nCore Capabilities . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n8\n5.2\nInstruction-tuned Model\n. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n12\n5.2.1\nOpen Benchmark Evaluation . . . . . . . . . . . . . . . . . . . . . . . . .\n12\n5.2.2\nIn-house Automatic Evaluation\n. . . . . . . . . . . . . . . . . . . . . . .\n14\n5.2.3\nLong Context Capabilities . . . . . . . . . . . . . . . . . . . . . . . . . .\n15\n5.2.4\nMultilingual Evaluation\n. . . . . . . . . . . . . . . . . . . . . . . . . . .\n18\n5.2.5\nSafety & Responsibility\n. . . . . . . . . . . . . . . . . . . . . . . . . . .\n18\n5.2.6\nContamination Analysis\n. . . . . . . . . . . . . . . . . . . . . . . . . . .\n19\n6\nConclusion\n20\n2\n\n1\nINTRODUCTION\nFollowing the emergence of ChatGPT (OpenAI, 2022), enthusiasm for large language models\n(LLMs) has escalated globally. The release of the Llama series (Touvron et al., 2023) has further\nignited interests within the open-source community, particularly regarding GPT-level local LLMs.\nRecently, Claude-3 Opus (Anthropic, 2024) and GPT-4o (omni) (OpenAI, 2024), the updated model\nfor ChatGPT, have ascended to the pinnacle of the Chatbot Arena (Chiang et al., 2024) in quick\nsuccession. This platform is well-regarded for its human evaluations of LLMs. Moreover, Llama-\n3 (AI@Meta, 2024) has emerged as the state-of-the-art open-weight model series, narrowing the\nperformance gap with leading proprietary models and widely acknowledged as GPT-4–level. An\nincreasing number of competitive LLMs are now pursuing advancements similar to those made by the\nGPT series from OpenAI. Many of these models, including Qwen (Bai et al., 2023a), Mistral (Jiang\net al., 2023a), Gemma (Mesnard et al., 2024), etc., have been released in an open-weight manner.\nOver recent months, we have successively introduced the Qwen series (Bai et al., 2023a) and\nprogressed to Qwen1.5 (Qwen Team, 2024a). In the meantime, we have unveiled the vision-language\nmodel Qwen-VL (Bai et al., 2023b), and launched the audio-language model Qwen-Audio (Chu\net al., 2023). In this work, we introduce the newest addition to the Qwen family of large language\nmodels and large multimodal modles: Qwen2. Qwen2 is a series of LLMs, grounded in the\nTransformer architecture (Vaswani et al., 2017), trained using next-token prediction. The model\nseries encompasses foundational, i.e., base language models, pre-trained but unaligned to human\npreferences, and instruction-tuned models, fine-tuned with single-turn and multi-turn instruction-\nfollowing datasets suitable for chat and agent purposes. Our release comprises four dense models\nwith parameter counts of 0.5 billion, 1.5 billion, 7 billion, and 72 billion, plus a Mixture-of-Experts\n(MoE) model with 57 billion parameters, of which 14 billion are activated for each token. The smaller\nmodels, specifically Qwen2-0.5B and Qwen2-1.5B, are designed for easy deployment on portable\ndevices such as smartphones, earphones, and smart glasses. Conversely, the larger models cater to\ndeployment across GPUs of varying scales.\nAll models were pre-trained on a high-quality, large-scale dataset comprising over 7 trillion tokens,\ncovering a wide range of domains and languages. Compared to previous editions of Qwen, Qwen2\nincludes a broader spectrum of linguistic data, enhancing the quantity and quality of code and mathe-\nmatics content. This enrichment is hypothesized to improve reasoning abilities of LLMs. Regarding\npost-training, all models underwent supervised fine-tuning and direct preference optimization (DPO,\nRafailov et al., 2023), aligning them with human preferences through learning from human feedback.\nThis process endows the models with the capability to follow instructions effectively.\nWe have conducted a thorough evaluation of Qwen2, alongside a selection of baseline models includ-\ning both open-weight and proprietary models accessible via API. Qwen2 outperforms competing\nmodels in evaluations of both fundamental language capabilities and instruction-tuned functionalities\nSpecifically, Qwen2-72B-Instruct, our instruction-tuned variant, scores 9.1 on MT-Bench (Zheng\net al., 2023), 48.1 on Arena-Hard (Chiang et al., 2024), and 35.7 on LiveCodeBench (Jain et al.,\n2024). Meanwhile, Qwen2-72B, the base language model, achieves 84.2 on MMLU (Hendrycks\net al., 2021a), 37.9 on GPQA (Rein et al., 2023), 64.6 on HumanEval (Chen et al., 2021), 89.5 on\nGSM8K (Cobbe et al., 2021), and 82.4 on BBH (Suzgun et al., 2023).\n2\nTOKENIZER & MODEL\nThis section introduces the tokenizer and model design of Qwen2. We detail the model architecture\nand configurations for different model sizes.\n2.1\nTOKENIZER\nFollowing Qwen (Bai et al., 2023a), we employ the identical tokenizer based on byte-level byte-\npair encoding. Notably, this tokenizer exhibits high encoding efficiency, as evidenced by its better\ncompression rate relative to alternatives, facilitating the multilingual capabilities of Qwen2.\nModels of all sizes employ a common vocabulary consisting of 151,643 regular tokens and 3 control\ntokens. For more information, please refer to Bai et al. (2023a). It should be noted that, owing to\nconsiderations in distributed training, the effective size for the embeddings is larger.\n3\n\n2.2\nMODEL ARCHITECTURE\nThe Qwen2 series fundamentally constitute large language models based on the Transformer ar-\nchitecture, featuring self-attention with causal masks (Vaswani et al., 2017). Specifically, this\nseries encompasses dense language models of 4 scales and a Mixture-of-Experts (MoE) model. We\nintroduce the specifics of the dense models before delving into the MoE model’s distinctive attributes.\n2.2.1\nQWEN2 DENSE MODEL\nThe architecture of the Qwen2 dense models comprises multiple Transformer layers, each equipped\nwith causal attention mechanisms and feed-forward neural networks (FFNs). Key differences from\nQwen are described below:\nGrouped Query Attention\nWe adopt Grouped Query Attention (GQA, Ainslie et al., 2023) instead\nof conventional multi-head attention (MHA). GQA optimizes KV cache usage during inference,\nsignificantly enhancing throughput. Detailed KV head configurations for various model sizes are\nreported in Section 2.2.3.\nDual Chunk Attention with YARN\nTo expand the context window of Qwen2, we implement Dual\nChunk Attention (DCA, An et al., 2024), which segments long sequences into chunks of manageable\nlengths. If the input can be handled in a chunk, DCA produces the same result as the original\nattention. Otherwise, DCA facilitates effective capture of relative positional information between\ntokens within and across chunks, thereby improving long context performance. Moreover, we also\nemploy YARN (Peng et al., 2023) to rescale the attention weights for better length extrapolation.\nMoreover, we follow Qwen with the usage of SwiGLU (Dauphin et al., 2017) for activation, Rotary\nPositional Embeddings (RoPE, Su et al., 2024) for positional embedding, QKV bias (Su, 2023) for\nattention, RMSNorm (Jiang et al., 2023b) and pre-normalization for training stability.\n2.2.2\nQWEN2 MIXTURE-OF-EXPERTS MODEL\nThe architecture of Qwen2 MoE models closely mirrors that of Qwen1.5-MoE-A2.7B (Qwen Team,\n2024c). As a substitute for the original FFN, the MoE FFN consists of n individual FFNs, each serving\nas an expert. Each token is directed to a specific expert Ei for computation based on probabilities\nassigned by a gated network G:\np = softmax (G (x)) ,\n(1)\ny =\nX\ni∈topk(p) piEi(x).\n(2)\nIn the following, we present critical design considerations of Qwen2 MoE.\nExpert Granularity\nThe key structural difference between MoE models and dense models is\nthat MoE layers incorporate multiple FFNs, each serving as an individual expert. Consequently,\none straightforward strategy to transition from a dense architecture to an MoE architecture is to set\nthe parameters of each expert equal to those of a single FFN from the original dense model. For\nexample, transitioning from Mistral-7B (Jiang et al., 2023a) to Mixtral 8x7B (Jiang et al., 2024),\ninvolves activating two of the eight experts at a time. Differently, our model employs fine-grained\nexperts (Dai et al., 2024), creating smaller-scale experts while activating a greater number of experts\nsimultaneously. Given an equal total number of expert parameters and activated parameters, fine-\ngrained experts offer a richer set of expert combinations. By leveraging these fine-grained experts,\nQwen2 MoE facilitates more diverse and dynamic expert utilization, thereby enhancing overall\nperformance and adaptability.\nExpert Routing\nThe design of expert routing mechanisms is crucial for enhancing the performance\nof MoE models. Recently, there has been a notable trend towards integrating both shared and\nrouting-specific experts within MoE layers (Rajbhandari et al., 2022; Dai et al., 2024). We adopt this\napproach, as it facilitates the application of shared experts across various tasks while reserving others\nfor selective use in specific routing scenarios. The introduction of shared and specialized experts\noffers a more adaptable and efficient method for developing MoE routing mechanisms.\n4\n\nTable 1: Architecture of Qwen2 dense and MoE models. For MoE models, 57B-A14B denotes that\nthe model has 57B parameters in total and for each token 14B parameters are active, the Intermediate\nsize denotes that of each expert, and # Activated Experts excludes the shared experts.\nConfiguration\n0.5B\n1.5B\n7B\n72B\n57B-A14B\nHidden Size\n896\n1,536\n3,584\n8,192\n3,584\n# Layers\n24\n28\n28\n80\n28\n# Query Heads\n14\n12\n28\n64\n28\n# KV Heads\n2\n2\n4\n8\n4\nHead Size\n64\n128\n128\n128\n128\nIntermediate Size\n4,864\n8,960\n18,944\n29,568\n2,560\n# Routed Experts\n-\n-\n-\n-\n64\n# Activated Experts\n-\n-\n-\n-\n8\n# Shared Experts\n-\n-\n-\n-\n8\nEmbedding Tying\nTrue\nTrue\nFalse\nFalse\nFalse\nVocabulary Size\n151,646\n151,646\n151,646\n151,646\n151,646\n# Trained Tokens\n12T\n7T\n7T\n7T\n4.5T\nExpert Initialization\nWe initialize the experts in a similar way to upcycling (Komatsuzaki et al.,\n2023), leveraging the weights of a dense model. In contrast, our approach emphasizes diversification\namong fine-grained experts to enhance the model’s representational breadth. Given the designated\nexpert intermediate size hE, the number of experts n, and the original FFN intermediate size hFFN, the\nFFN is replicated ⌈n×hE/hFFN⌉times. This replication ensures compatibility with the specified number\nof experts while accommodating any arbitrary expert intermediate size. To promote diversity within\neach FFN copy, parameters are shuffled along the intermediate dimension. This guarantees that each\nfine-grained expert exhibits unique characteristics, even across different FFN copies. Subsequently,\nthese experts are extracted from the FFN copies, and the remaining dimensions are discarded. For\neach fine-grained expert, 50% of its parameters are randomly reinitialized. This process introduces\nadditional stochasticity into expert initialization, potentially enhancing the model’s capacity for\nexploration during training.\n2.2.3\nMODEL CONFIGURATION\nIn the following, we provide the key configuration and information for the Qwen2 series.\nThe Qwen2 series consists of models of 5 sizes, which are Qwen2-0.5B, Qwen2-1.5B, Qwen2-7B,\nQwen2-57B-A14B, and Qwen2-72B. Table 1 lists the hyper-parameters and important information,\ne.g., the number of pre-trained tokens. Particularly, Qwen2-57B-A14B is upscaled from Qwen2-7B.\nNotably, Qwen2 models demonstrate a substantially lower Key-Value (KV) size per token relative\nto Qwen1.5 models. This characteristic translates into a reduced memory footprint, particularly\nadvantageous in long-context inference tasks.\n3\nPRE-TRAINING\nIn the pre-training of Qwen2, our efforts were focused on refining the dataset and investigating\nmethods to handle extended context lengths effectively.\n3.1\nPRE-TRAINING DATA\nThe pre-training of the Qwen2 models involves the development of a new, large-scale, high-quality\nmultilingual dataset. This dataset represents an improvement over the corpora used in previous\nQwen and Qwen1.5 models (Bai et al., 2023a; Qwen Team, 2024a), enhancing the scale, quality, and\ndiversity of the pre-training data in several key areas:\nQuality Enhancement\nThe filtering algorithm has been refined with additional heuristic and model-\nbased methods, including the use of the Qwen models to filter out low-quality data. Moreover, these\nmodels are utilized to synthesize high-quality pre-training data.\n5\n\nData Expansion\nCompared to Qwen1.5 (Qwen Team, 2024a), we have collected a significantly\nlarger volume of high-quality code, mathematics, and multilingual data, enhancing the model’s capa-\nbilities in respective areas. This new dataset supports approximately 30 languages, such as English,\nChinese, Spanish, French, German, Arabic, Russian, Korean, Japanese, Thai, and Vietnamese.\nDistribution Improvement\nTo ensure the model learns the distribution akin to human-like learning,\nwe conduct experiments on scaled-down models to optimize the mixing of data from various sources\nand domains.\nBased on these enhancements, the pre-training data was expanded from 3 trillion tokens in\nQwen1.5 (Qwen Team, 2024a) to 7 trillion tokens. An attempt to further relax the quality threshold\nresulted in a 12 trillion token dataset. However, the model trained on this dataset did not show a\nsignificant performance improvement over the 7 trillion token model. It is suspected that increasing\nthe volume of data does not necessarily benefit model pre-training. Considering training costs, we\nopted to use the higher-quality 7 trillion token dataset for training larger models, leaving further\nexploration for future model iterations.\nAll Qwen2 dense models, excluding Qwen2-0.5B, were pre-trained on this large-scale dataset of\nover 7 trillion tokens. Qwen2-0.5B were pre-trained using the 12 trillion token dataset. The MoE\nmodel received an additional 4.5 trillion tokens of pre-training, in line with the principle of upcycling.\nSimilar to previous Qwen models, high-quality multi-task instruction data is integrated into the\nQwen2 pre-training process to enhance in-context learning and instruction-following abilities.\n3.2\nLONG-CONTEXT TRAINING\nTo enhance the long-context capability of Qwen2, we augmented the context length from 4,096 tokens\nto 32,768 tokens during the concluding phase of pre-training. This expansion was complemented by\nthe introduction of a significantly increased volume of high-quality, lengthy data. In conjunction with\nthese enhancements, we modified the base frequency of RoPE from 10,000 to 1,000,000 to optimize\nperformance in long-context scenarios (Xiong et al., 2023).\nTo fully leverage the model’s length extrapolation potential, we adopted the YARN mechanism (Peng\net al., 2023) and the Dual Chunk Attention mechanism (An et al., 2024). These strategies enable\nthe model to process sequences of up to 131,072 tokens while maintaining high performance, as\nevidenced by minimal perplexity degradation in preliminary experiments.\n4\nPOST-TRAINING\nFollowing extensive large-scale pre-training, we engage in a post-training phase for Qwen2. This\nprocess is pivotal in enhancing its proficiency across a broad spectrum of domains, including coding,\nmathematics, logical reasoning, instruction following, and multilingual comprehension. Moreover,\nit ensures that the generation from the models is in harmony with human values, making it helpful,\nhonest, and harmless. Unlike traditional methods that heavily rely on extensive human supervision,\nour approach focuses on scalable alignment with minimal human annotation (Cao et al., 2024).\nSpecifically, we investigate methods to acquire high-quality demonstration and preference data for\nSupervised Fine-Tuning (SFT) and Reinforcement Learning from Human Feedback (RLHF), aiming\nto minimize the need for human labeling while maximizing the quality and reliability of the data.\n4.1\nPOST-TRAINING DATA\nThe post-training data primarily consists of two components: demonstration data D = {(xi, yi)} and\npreference data P = {(xi, y+\ni , y−\ni )}, where xi represents the instruction, yi represents a satisfactory\nresponse, and y+\ni and y−\ni are two responses to xi, with y+\ni being the preferred choice over y−\ni . The\nset D is utilized in SFT, whereas P is employed in RLHF.\nThe construction of training data entails a two-step process: collaborative data annotation and\nautomated data synthesis. First, we extract the data ontology from large-scale instruction corpora,\nleading to a broad and diverse set of high-quality instructions. These instructions are systematically\nenhanced to incorporate greater complexity. Through human annotation, we obtain the target response\nyi and their positive and negative counterparts (y+\ni , y−\ni ). Subsequently, a variety of automated\n6\n\nalignment strategies are employed to synthesize a substantial volume of artificially annotated data\nacross the domains of code, mathematics, instruction-following, creation, role-playing, and safety.\n4.1.1\nCOLLABORATIVE DATA ANNOTATION\nAutomatic Ontology Extraction\nThe process initiates with the application of InsTag (Lu et al.,\n2024c), an open-set fine-grained tagger, to extract the underlying ontology from a large-scale\ninstruction dataset. Subsequent manual refinement ensures the accuracy of the extracted ontology.\nInstruction Selection\nEach instruction, with tags annotated, is evaluated for tag diversity, semantic\nrichness, complexity, and intent completeness. Based on these criteria, we select a set of representative\ninstructions (Dong et al., 2023).\nInstruction Evolution\nTo enrich the instruction dataset, a self-evolution strategy (Zhao et al., 2024)\nis employed, prompting the Qwen models to add constraints or requirements to existing instructions,\nthereby increasing their complexity and ensuring a diverse range of difficulty levels within the dataset.\nHuman Annotation\nMultiple responses to an instruction are obtained using diverse generation\nstrategies and Qwen models of different scales. Annotators rank these responses based on their\npreferences, ensuring the best response meets established criteria, yielding both demonstration and\npreference data.\n4.1.2\nAUTOMATED DATA SYNTHESIS\nMaintaining the quality of annotations for responses to instructions presents significant challenges on\na large scale, particularly those that require expertise, experience, carefulness, or patience. To address\nthese challenges, we devised various automated alignment strategies to synthesize data at scale.\nRejection Sampling\nFor mathematical or similar tasks with definitive final answers, rejection\nsampling (Yuan et al., 2023) is applied to improve the quality of solutions. Large language models\n(LLMs) are tasked to generate multiple responses, namely the reasoning paths, for each instruction.\nPaths that result in accurate conclusions and are considered reasonable by the model are preserved,\nserving as demonstration data. Preference data is generated by contrasting correct and incorrect paths.\nExecution Feedback\nFor coding tasks, LLMs are employed to generate solutions and associated\ntest cases. The efficacy of these solutions is evaluated by compiling and executing them against the\ntest cases, thereby creating demonstration and preference data. This methodology is also applicable\nto assessing instruction following (Dong et al., 2024). For each instruction with constraints, e.g.,\nlength limit, the LLM is tasked to generate a Python verification function to ensure the response\naligns with the instruction requirements.\nData Repurposing\nCreating skilled responses in literary writing tasks is challenging for annotators\nwithout specialized training. To tackle this problem, we aggregate high-quality literary works\nfrom the public domain and employ LLMs to develop instructions with varying levels of detail.\nThese instructions, paired with the original works, serve as demonstration data. For example, to\ncompile roleplay data with vivid and engaging responses, we source detailed character profiles from\nknowledge repositories such as Wikipedia and instruct LLMs to generate corresponding instructions\nand responses (Lu et al., 2024b). This process, similar to a reading comprehension task, ensures that\nthe integrity of the character’s profile is maintained.\nConstitutional Feedback\nConstitutional AI refers to the process of guiding LLMs to generate\nresponses based on predefined sets of principles (Bai et al., 2022). To ensure adherence to guidelines\nsuch as safety and values, a constitution dataset was compiled. This dataset delineates principles to\nbe followed and those to be avoided. It was used to instruct LLMs to produce responses that either\nare aligned with or deviated from these guidelines, serving as a reference for demonstration and\npreference data.\n7\n\n4.2\nSUPERVISED FINE-TUNING\nWe have assembled an extensive instruction dataset featuring more than 500,000 examples that\ncover skills such as instruction following, coding, mathematics, logical reasoning, role-playing,\nmultilingualism, and safety. Our model was fine-tuned for two epochs with a sequence length of\n32,768 tokens. To optimize learning, the learning rate was gradually decreased from 7 × 10−6 to\n7 × 10−7. To address overfitting, we applied a weight decay of 0.1 and gradients were clipped at a\nmaximum value of 1.0.\n4.3\nREINFORCEMENT LEARNING FROM HUMAN FEEDBACK\nOur training regime for RLHF comprises two sequential stages: offline and online training. In the\noffline training stage, we use a pre-compiled preference dataset P to maximize the difference in\nlikelihood between y+\ni and y−\ni with Direct Preference Optimization (DPO, Rafailov et al., 2023). In\nthe online training stage, the model iteratively refines its performance in real-time, leveraging reward\nmodels for immediate feedback. Specifically, we sample multiple responses from the current policy\nmodel, and the reward model selects the most and the least preferred responses, forming preference\npairs that are used for DPO in each episode. Moreover, we employ Online Merging Optimizer (Lu\net al., 2024a) to mitigate the alignment tax, i.e., the performance degradation associated with aligning\nmodel generation with human preferences.\n5\nEVALUATION\nTo thoroughly assess the Qwen2 models, consisting of both base and instruction-tuned models,\nwe implement a comprehensive evaluation protocol. This protocol examines a range of compe-\ntencies, including general knowledge understanding, language comprehension, generation, coding,\nmathematics, reasoning, and additional areas of expertise. Specifically, base models are assessed\nusing established benchmark datasets for large language models (LLMs), with responses elicited\nthrough few-shot prompting, unless specified otherwise. For instruction-tuned models, in addition to\nbenchmark evaluations, we prioritize human preference assessments.\n5.1\nBASE LANGUAGE MODELS\nIn this section, we illustrate the evaluation of the base language models of the Qwen2 series. Specifi-\ncally, we evaluate the models on benchmark datasets for knowledge and basic capabilities and apply\nmultilingual benchmark datasets to evaluate their support of languages. As there are multiple model\nsizes, we compare them with the state-of-the-art (SOTA) models of similar or larger sizes.\n5.1.1\nCORE CAPABILITIES\nBenchmarks and Evaluation Protocol\nThe common practice of evaluating the core capabilities\nof base language models is the implementation of benchmark dataset evaluation with few-shot or\nzero-shot prompting. The evaluation mainly focuses on the model performance of natural language\nunderstanding, general question answering, coding, mathematics, scientific knowledge, reasoning, etc.\nThe datasets for evaluation include MMLU (Hendrycks et al., 2021a) (5-shot), MMLU-Pro (Wang\net al., 2024) (5-shot), GPQA (Rein et al., 2023) (5shot), Theorem QA (Chen et al., 2023a) (5-shot),\nBBH (Suzgun et al., 2023) (3-shot), HellaSwag (Zellers et al., 2019) (10-shot), Winogrande (Sak-\naguchi et al., 2021) (5-shot), TruthfulQA (Lin et al., 2022a) (0-shot), ARC-C (Clark et al., 2018)\n(25-shot), HumanEval (Chen et al., 2021) (0-shot), MBPP (Austin et al., 2021) (0-shot), EvalPlus(Liu\net al., 2023a) (0-shot), MultiPL-E (Cassano et al., 2023) (0-shot on Python, C++, Java, PHP, Type-\nScript, C#, Bash, and JavaScript), GSM8K (Cobbe et al., 2021) (5-shot), MATH (Hendrycks et al.,\n2021b) (4-shot), C-Eval (Huang et al., 2023) (5-shot), and CMMLU (Li et al., 2023) (5-shot). Multi-\nlingual datasets can be grouped into four categories: (a) Exam: M3Exam (5-shot, we only choose\nexamples that require no image), IndoMMLU (Koto et al., 2023) (3-shot), ruMMLU (Fenogenova\net al., 2024) (5-shot), and translated MMLU (Chen et al., 2023b) (5-shot on Arabic, Spanish, French,\nPortuguese, German, Italian, Japanese, and Korean); (b) Understanding: BELEBELE (Bandarkar\net al., 2023) (5-shot), XCOPA (Ponti et al., 2020) (5-shot), XWinograd (Muennighoff et al., 2023)\n(5-shot), XStoryCloze (Lin et al., 2022b) (0-shot) and PAWS-X (Yang et al., 2019) (5-shot); (c)\n8\n\nTable 2: Performance of the 70B+ models. We compare Qwen2-72B with the baselines, including\nMixtral-8x22B, Llama-3-70B, Qwen1.5-110B, and Qwen1.5-72B. For most datasets, Qwen2-72B\ndemonstrates advantages over the baselines.\nDatasets\nMixtral-8x22B\nLlama-3-70B\nQwen1.5-72B\nQwen1.5-110B\nQwen2-72B\nEnglish\nMMLU\n77.8\n79.5\n77.5\n80.4\n84.2\nMMLU-Pro\n49.5\n52.8\n45.8\n49.4\n55.6\nGPQA\n34.3\n36.3\n36.3\n35.9\n37.9\nTheorem QA\n35.9\n32.3\n29.3\n34.9\n43.1\nBBH\n78.9\n81.0\n65.5\n74.8\n82.4\nHellaSwag\n88.7\n88.0\n86.0\n87.5\n87.6\nWinogrande\n85.0\n85.3\n83.0\n83.5\n85.1\nARC-C\n70.7\n68.8\n65.9\n69.6\n68.9\nTruthfulQA\n51.0\n45.6\n59.6\n49.6\n54.8\nCoding\nHumanEval\n46.3\n48.2\n46.3\n54.3\n64.6\nMBPP\n71.7\n70.4\n66.9\n70.9\n76.9\nEvalPlus\n54.1\n54.8\n52.9\n57.7\n65.4\nMultiPL-E\n46.7\n46.3\n41.8\n52.7\n59.6\nMathematics\nGSM8K\n83.7\n83.0\n79.5\n85.4\n89.5\nMATH\n41.7\n42.5\n34.1\n49.6\n51.1\nChinese\nC-Eval\n54.6\n65.2\n84.1\n89.1\n91.0\nCMMLU\n53.4\n67.2\n83.5\n88.3\n90.1\nMultilingual\nExam\n63.5\n70.0\n66.4\n75.6\n76.6\nUnderstanding\n77.7\n79.9\n78.2\n78.2\n80.7\nMathematics\n62.9\n67.1\n61.7\n64.4\n76.0\nTranslation\n23.3\n38.0\n35.6\n36.2\n37.8\nMathematics: MGSM (Goyal et al., 2022) (8-shot CoT); and (d) Translation: Flores-101 (Goyal et al.,\n2022) (5-shot).\nQwen2-72B\nIn terms of the largest model of Qwen2, we compare Qwen2-72B with competitive\nbaseline open-weight models, including Mixtral-8x22B (Jiang et al., 2024), Llama-3-70B (AI@Meta,\n2024), as well as Qwen1.5-72B (Qwen Team, 2024a) and Qwen1.5-110B (Qwen Team, 2024b).\nThe results are reported in Table 2. Qwen2-72B outperforms Llama-3-70B in general knowledge\nunderstanding on both MMLU and MMLU-Pro, achieving accuracy improvements of 4.7 and 2.8,\nrespectively. In scientific assessments, Qwen2-72B demonstrates superiority over Llama-3-70B with\nenhancements of 1.6 and 9.8 on GPQA and Theorem QA. Upon enrichment of coding data, Qwen2-\n72B exhibits a significant 18.3 and 10.0 percentage point advantage over Qwen1.5-72B in HumanEval\nand MBPP evaluations. Enhanced mathematics-related data allows Qwen2-72B to outperform\nQwen1.5-72B by 10.0 and 17.0 percentage points in the GSM8K and MATH benchmarks. Qwen2-\n72B displays reasoning capabilities equivalent to Llama-3-70B, considering BBH, Winogrande,\nand ARC-C, attributable to its improved coding and mathematical data. In assessing language\nunderstanding in Chinese, Qwen2-72B significantly outperforms Mixtral-8x22B and Llama-3-70B,\nand also outperforms Qwen1.5-72B.\nQwen2-57B-A14B\nFor the evaluation of the MoE model, Qwen2-57B-A14B is compared against\nbaselines of similar sizes. These baselines include other MoE models, such as Mixtral-8x7B (Jiang\net al., 2024) and Jamba (Lieber et al., 2024), and dense models, such as Yi-1.5-34B (Young et al., 2024)\n9\n\nTable 3: Performance of the 30B+ dense models and 40B+ MoE models. Qwen2-57B-A14B, an\nMoE model with a total of 57 billion parameters and 14 billion activated parameters, is designed\nto match the performance of 30 billion parameter dense models. This comparison includes dense\nmodel baselines: Yi-1.5-34B and Qwen1.5-32B, as well as MoE baselines: Mixtral-8x7B and Jamba.\nResults demonstrate that Qwen2-57B-A14B achieves competitive performance overall, with a notable\nsuperiority in coding and mathematics tasks.\nDatasets\nJamba\nMixtral-8x7B\nYi-1.5-34B\nQwen1.5-32B\nQwen2-57B-A14B\nArchitecture\nMoE\nMoE\nDense\nDense\nMoE\n# Act Params\n12B\n12B\n32B\n34B\n14B\n# Params\n52B\n47B\n32B\n34B\n57B\nEnglish\nMMLU\n67.4\n71.8\n77.1\n74.3\n76.5\nMMLU-Pro\n-\n41.0\n48.3\n44.0\n43.0\nGPQA\n-\n29.2\n-\n30.8\n34.3\nTheorem QA\n-\n23.2\n-\n28.8\n33.5\nBBH\n45.4\n50.3\n76.4\n66.8\n67.0\nHellaSwag\n87.1\n86.5\n85.9\n85.0\n85.2\nWinogrande\n82.5\n81.9\n84.9\n81.5\n79.5\nARC-C\n64.4\n66.0\n65.6\n63.6\n64.1\nTruthfulQA\n46.4\n51.1\n53.9\n57.4\n57.7\nCoding\nHumanEval\n29.3\n37.2\n46.3\n43.3\n53.0\nMBPP\n-\n63.9\n65.5\n64.2\n71.9\nEvalPlus\n-\n46.4\n51.9\n50.4\n57.2\nMultiPL-E\n-\n39.0\n39.5\n38.5\n49.8\nMathematics\nGSM8K\n59.9\n62.5\n82.7\n76.8\n80.7\nMATH\n-\n30.8\n41.7\n36.1\n43.0\nChinese\nC-Eval\n-\n-\n-\n83.5\n87.7\nCMMLU\n-\n-\n84.8\n82.3\n88.5\nMultilingual\nExam\n-\n56.1\n58.3\n61.6\n65.5\nUnderstanding\n-\n70.7\n73.9\n76.5\n77.0\nMathematics\n-\n45.0\n49.3\n56.1\n62.3\nTranslation\n-\n29.8\n30.0\n33.5\n34.5\nand Qwen1.5-32B (Qwen Team, 2024a), both of which have approximately 30 billion parameters.\nThe results are shown in Table 3. We anticipate that Qwen2-57B-A14B, which activates 14 billion\nparameters, will match the performance of a 30 billion parameter dense equivalent Qwen2 model. Our\nevaluation reveals that Qwen2-57B-A14B performs comparably to Yi-1.5-34B in natural language\nunderstanding tasks. Moreover, it outperforms the baseline models in coding and mathematics tasks.\nAdditionally, Qwen2-57B-A14B demonstrates robust Chinese language understanding capabilities,\nrivaling the larger Qwen2-72B model. In essence, Qwen2-57B-A14B is an efficient model that, while\nactivating only 14 billion parameters per forward pass, maintains the performance level of a 30 billion\nparameter dense model.\nQwen2-7B\nThe 7B model is widely utilized, as it enables the execution in 16-bit floating points on\naccelerators equipped with 16GB memory. Our focus is on comparing this model with other leading\n7B models, including Llama-3-8B, which has recently demonstrated exceptional performance in the\nChatbot Arena (Chiang et al., 2024). This comparison also includes Mistral-7B-v0.2 (Jiang et al.,\n2023a), Gemma-7B (Mesnard et al., 2024), and our predecessor, Qwen1.5-7B (Qwen Team, 2024a).\n10\n\nTable 4: Performance of the 7B+ models. We compare Qwen2-7B with previously released state-of-\nthe-art 7B+ models including Mixtral-7B, Gemma-7B, Llama-3-8B, and our previous Qwen1.5-7B.\nQwen2-7B demonstrates significant advantages over the baselines in most of the evaluation datasets.\nDatasets\nMistral-7B\nGemma-7B\nLlama-3-8B\nQwen1.5-7B\nQwen2-7B\nEnglish\nMMLU\n64.2\n64.6\n66.6\n61.0\n70.3\nMMLU-Pro\n30.9\n33.7\n35.4\n29.9\n40.0\nGPQA\n24.7\n25.7\n25.8\n26.7\n31.8\nTheorem QA\n19.2\n21.5\n22.1\n14.2\n31.1\nBBH\n56.1\n55.1\n57.7\n40.2\n62.6\nHellaSwag\n83.2\n82.2\n82.1\n78.5\n80.7\nWinogrande\n78.4\n79.0\n77.4\n71.3\n77.0\nARC-C\n60.0\n61.1\n59.3\n54.2\n60.6\nTruthfulQA\n42.2\n44.8\n44.0\n51.1\n54.2\nCoding\nHumanEval\n29.3\n37.2\n33.5\n36.0\n51.2\nMBPP\n51.1\n50.6\n53.9\n51.6\n65.9\nEvalplus\n36.4\n39.6\n40.3\n40.0\n54.2\nMultiPL-E\n29.4\n29.7\n22.6\n28.1\n46.3\nMathematics\nGSM8K\n52.2\n46.4\n56.0\n62.5\n79.9\nMATH\n13.1\n24.3\n20.5\n20.3\n44.2\nChinese\nC-Eval\n47.4\n43.6\n49.5\n74.1\n83.2\nCMMLU\n-\n-\n50.8\n73.1\n83.9\nMultilingual\nExam\n47.1\n42.7\n52.3\n47.7\n59.2\nUnderstanding\n63.3\n58.3\n68.6\n67.6\n72.0\nMathematics\n26.3\n39.1\n36.3\n37.3\n57.5\nTranslation\n23.3\n31.2\n31.9\n28.4\n31.5\nThe results can be found in Table 4. Qwen2-7B demonstrates superior performance across most\ndatasets compared to other models, particularly excelling in coding tasks, mathematics, and Chinese\nlanguage tasks. It also shows strong performance in multilingual understanding and exams. This\nindicates that Qwen2-7B has been optimized for a wide range of language and logic-based tasks,\nshowcasing its versatility and advanced capabilities.\nQwen2-1.5B & Qwen2-0.5B\nTo evaluate the performance of our smaller models, specifically\nQwen2-1.5B and Qwen2-0.5B, we compare them against established baselines: Phi-2 (Abdin et al.,\n2024), Gemma-2B (Mesnard et al., 2024), and Qwen1.5-1.8B (Qwen Team, 2024a). The results\nare given in Table 5. In language understanding, Qwen2-1.5B outperforms Phi-2, a model trained\non textbook-like data. For coding tasks, Qwen2-0.5B matches the performance of Gemma-2B and\nQwen1.5-1.8B, while Qwen2-1.5B surpasses these baselines, except for Phi-2. Both Qwen2 models\nexhibit superior performance in mathematics compared to their competitors. In terms of general\nreasoning, we find that Phi-2 generally outperforms all others, which to some extent reflects the\nsignificance of textbook data for reasoning capabilities. In TruthfulQA, Qwen2-1.5B performs the\nbest, demonstrating that smaller models does not necessarily suffer from hallucination. In Chinese\nlanguage understanding, both Qwen2 models outperform all the others, a trend consistent with larger\nmodels in their respective comparisons.\nIn general, the Qwen2 series demonstrates superior performance against the baselines across different\nmodel sizes. Notably, Qwen2-72B exhibits the highest performance among all Qwen2 models,\nunderscoring the efficacy of model size scaling.\n11\n\nTable 5: Performance of the smaller models. We compare our Qwen2-0.5B and Qwen2-1.5B\nwith the previous SOTA small models including Phi-2, Gemma-2B and Qwen1.5-1.8B. Qwen2-0.5B\nwith a much smaller model size achieves competitive performance, and Qwen2-1.5B significantly\noutperforms Qwen2-0.5B.\nDatasets\nPhi-2\nGemma-2B\nQwen1.5-1.8B\nQwen2-0.5B\nQwen2-1.5B\n# Non-Emb Params\n2.5B\n2.0B\n1.2B\n0.3B\n1.2B\nMMLU\n52.7\n42.3\n46.8\n45.4\n56.5\nMMLU-Pro\n-\n15.9\n-\n14.7\n21.8\nTheorem QA\n-\n-\n-\n8.9\n15.0\nBBH\n43.4\n35.2\n24.2\n28.4\n37.2\nHellaSwag\n73.1\n71.4\n61.4\n49.3\n66.6\nWinogrande\n74.4\n66.8\n60.3\n56.8\n66.2\nARC-C\n61.1\n48.5\n37.9\n31.5\n43.9\nTruthfulQA\n44.5\n33.1\n39.4\n39.7\n45.9\nHumanEval\n47.6\n22.0\n20.1\n22.0\n31.1\nMBPP\n55.0\n29.2\n18.0\n22.0\n37.4\nGSM8K\n57.2\n17.7\n38.4\n36.5\n58.5\nMATH\n3.5\n11.8\n10.1\n10.7\n21.7\nC-Eval\n23.4\n28.0\n59.7\n58.2\n70.6\nCMMLU\n24.2\n-\n57.8\n55.1\n70.3\n5.2\nINSTRUCTION-TUNED MODEL\nTo critically evaluate instruction-tuned models, we implement a multifaceted approach. Assessments\nof foundational skills and human preferences are conducted using open datasets and benchmarks. Our\ndetailed in-house examinations further probe model competencies in key areas. A particular focus is\nplaced on assessing long context capability. Safety measures include multilingual safety assessments\nand red teaming exercises. The following sections detail the evaluation methods and their outcomes.\n5.2.1\nOPEN BENCHMARK EVALUATION\nTo comprehensively evaluate the quality of instruction-tuned models, we compile automatic and\nhuman evaluation to assess the capabilities and human preference. For the evaluation of basic\ncapabilities, we apply similar datasets in the pre-trained model evaluation, which target on natural\nlanguage understanding, coding, mathematics, and reasoning. Specifically, we evaluate on MMLU,\nMMLU-Pro, GPQA, and Theorem QA for language understanding and knowledge, HumanEval,\nMBPP, MultiPL-E, and LiveCodeBench v1 (Jain et al., 2024) for coding, GSM8K and MATH for\nmathematics. Additionally, we assess the performance of human preference alignment and instruction\nfollowing by evaluating on benchmarks including MT-Bench (Zheng et al., 2023), Arena-Hard (Li\net al., 2024), AlignBench (Liu et al., 2023b), MixEval (Ni et al., 2024) whose results approximate\nthose of Chatbot Arena, and IFEval (Zhou et al., 2023)4 for instruction following.\nQwen2-72B-Instruct\nWe compare Qwen2-72B-Instruct against the instruction-tuned models in-\ncluding Mixtral-8x22B-Instruct, Llama-3-70B-Instruct, as well as Qwen1.5-72B-Chat. The results are\npresented in Table 6. It can be found that a strong base language model can help boost the downstream\nperformance of the instruction-tuned model. Specifically, Qwen2-72B-Instruct outshines its peers in\nareas such as language understanding, coding, and mathematics, with the exception of GPQA and\nMBPP. Regarding human preference alignment and instruction following, Qwen2-72B has significant\nadvantages over the baselines. We assume this achievement is attributed to both the high-quality\npre-trained model and improvements in both data and training techniques for post-training.\n4For simplicity, we report the results of the subset strict-prompt.\n12\n\nTable 6: Performance of 70B+ instruction-tuned models. We compare Qwen2-72B-Instruct with\nMixtral-8x22B-Instruct, Llama-3-70B-Instruct, Qwen1.5-72B-Chat, and Qwen1.5-110B-Chat. “-\nInstruct” or “-Chat” is omitted in the table. Qwen2-72B-Instruct demonstrates advantages in core\ncapabilities, and superior performance in human preference alignment.\nDatasets\nMixtral-8x22B Llama-3-70B Qwen1.5-72B Qwen1.5-110B Qwen2-72B\nEnglish\nMMLU\n74.0\n82.0\n75.6\n76.5\n82.3\nMMLU-Pro\n56.1\n56.2\n51.7\n50.5\n64.4\nGPQA\n49.7\n41.9\n39.4\n32.8\n42.4\nTheorem QA\n40.8\n42.5\n28.8\n18.8\n44.4\nCoding\nHumanEval\n73.8\n81.7\n71.3\n74.4\n86.0\nMBPP\n75.9\n82.3\n71.9\n76.4\n80.2\nMultiPL-E\n61.1\n63.4\n48.1\n55.4\n69.2\nLiveCodeBench v1\n21.8\n29.3\n17.9\n25.3\n35.7\nMathematics\nGSM8K\n89.1\n93.0\n82.7\n84.5\n93.2\nMATH\n47.4\n50.4\n42.5\n42.0\n69.0\nAlignment\nMT-Bench\n8.66\n8.95\n8.61\n8.88\n9.12\nMixEval\n82.3\n84.0\n84.1\n85.7\n86.7\nArena-Hard\n36.4\n41.1\n36.1\n39.8\n48.1\nIFEval strict-prompt\n67.1\n77.3\n55.8\n57.5\n77.6\nAlignBench\n-\n7.42\n7.28\n7.87\n8.27\nQwen2-57B-A14B-Instruct\nFor medium-size models, we compare Qwen2-57B-A14B-Instruct\nwith Mixtral-8x7B-Instruct, another MoE baseline, as well as the dense SOTA models with over 30\nbillion parameters, e.g., Yi-1.5-34B-Chat and Qwen1.5-32B-Chat. The results are provided in Table 7.\nCompared with Qwen1.5-32B-Chat, Qwen2-57B-A14B-Instruct reaches superior performance in\nalmost all benchmarks, and compared with the 30B SOTA model Yi-1.5-34B-Chat, Qwen2-57B-\nA14B-Instruct has gained advantages in most evaluations except for those for mathematics. In terms\nof the evaluation for alignment, the advantages of Qwen2-57B-A14B-Instruct are notably evident.\nQwen2-7B-Instruct\nWithin the spectrum of 7B to 9B models, we compare Qwen2-7B-Instruct\nwith Llama-3-8B-Instruct, Yi-1.5-9B-Chat, GLM-4-9B-Chat, and Qwen1.5-7B-Chat. The results\ncan be found in Table 8. Qwen2-7B-Instruct demonstrates substantial advancements compared\nto its predecessor, Qwen1.5-7B-Chat, across comprehensive evaluations, notably achieving higher\nscores in coding and mathematics-related tasks. Compared with the recent SOTA model, Llama-3-\n8B-Instruct, Qwen2-7B-Instruct demonstrates competitive performance and specifically it achieves\nsuperior performance in coding. Nonetheless, in terms of instruction following, Qwen2-7B-Instruct\ngreatly falls behind the competitor. To address this limitation, we plan to augment the 7B model’s\ninstruction-following ability by enhancing the quality of post-training data, ensuring a more robust\nunderstanding and execution of complex commands.\nQwen2-1.5B-Instruct & Qwen2-0.5B-Instruct\nIn the context of smaller models, we compare\nQwen2-0.5B-Instruct with Qwen1.5-0.5B-Chat, and Qwen2-1.5B-Instruct with Qwen1.5-1.8B-Chat.\nNotably, the complexity of certain datasets designed for larger models exceeds the capabilities of\nthese smaller models; thus, our analysis focuses on a selected subset. As detailed in Table 9, the\nQwen2 models demonstrate a marked advantage over their predecessors in both core capabilities and\ninstruction-following tasks. The achievement mainly attributes to the scaling of pre-training data.\nConsequently, our results affirm that data scaling remains an effective strategy for enhancing model\nperformance, even in the domain of sub-billion parameter models.\n13\n\nTable 7: Performance of 30B+ dense and 40B+ MoE instruction-tuned models. We compare\nQwen2-57B-A14B-Instruct with the similar-size MoE model Mixtral-8x7B-Instruct, 30B dense\nmodels such as Yi-1.5-34B-Chat and Qwen1.5-32B-Chat. “-Instruct” or “-Chat” is omitted in the\ntable. Qwen2-57B-A14B-Instruct is competitive with the recent SOTA 30B dense models, and\nsignificantly outcompetes the MoE baseline.\nDatasets\nMixtral-8x7B\nYi-1.5-34B\nQwen1.5-32B\nQwen2-57B-A14B\nArchitecture\nMoE\nDense\nDense\nMoE\n# Act Params\n12B\n32B\n34B\n14B\n# Params\n47B\n32B\n34B\n57B\nEnglish\nMMLU\n71.4\n76.8\n74.8\n75.4\nMMLU-Pro\n43.3\n52.3\n46.4\n52.8\nGPQA\n-\n-\n30.8\n34.3\nTheorem QA\n-\n-\n30.9\n33.1\nCoding\nHumanEval\n45.1\n75.2\n68.3\n79.9\nMBPP\n59.5\n74.6\n67.9\n70.9\nMultiPL-E\n-\n-\n50.7\n66.4\nLiveCodeBench v1\n12.3\n-\n15.2\n25.5\nMathematics\nGSM8K\n65.7\n90.2\n83.6\n85.3\nMATH\n30.7\n50.1\n42.4\n49.1\nAlignment\nMT-Bench\n8.30\n8.50\n8.30\n8.55\nMixEval\n70.0\n81.7\n81.0\n82.3\nIFEval strict-prompt\n-\n-\n50.3\n59.9\nAlignBench\n5.70\n7.20\n7.19\n7.36\n5.2.2\nIN-HOUSE AUTOMATIC EVALUATION\nDespite a number of open benchmark datasets for the evaluation, we believe that it is far from\nsufficient to fully comprehend the capabilities of LLMs. Specifically, we have made a series of\nin-house datasets that assess different capabilities of the models, e.g., knowledge understanding,\ntext generation, coding, etc. The evaluation is in Chinese and English. The results are gathered in\nTable 10 and Table 11, respectively.\nChinese Evaluation\nFor the evaluations in Chinese, we focus on comparing the performance of\nQwen2 models with the Qwen1.5 counterparts. For the small models, Qwen2-1.5B-Instruct generally\noutperforms Qwen1.5-1.8B-Chat in almost all the evaluations even with fewer parameters. In terms of\nthe comparison of 7B models, the advantages of Qwen2 are more significant. Noteworthy is Qwen2-\n72B’s superior performance to Qwen1.5-110B-Chat, despite the latter’s greatly more parameters.\nThe MoE model displays superior performance across most domains relative to Qwen1.5-32B-Chat,\nexcluding knowledge understanding. This discrepancy may be attributed to a short of pre-training\ntokens. In the near future, we are about to continue the pre-training of the MoE model to discover its\nscaling behaviors.\nEnglish Evaluation\nFor English, we compare Qwen2 with both Qwen1.5 and Llama-3. Similarly,\nthe small models of Qwen2 significantly outcompete the Qwen1.5 counterparts. However, in com-\nparison with Llama-3-70B, Qwen2-72B-Instruct is falling behind by small margins especially in\ncomprehension and coding. We assume both the amount of English tokens for pre-training and the\nquantity and diversity of data for post-training lead to the performance gap in English.\n14\n\nTable 8: Performance of 7B+ instruction-tuned models. We compare Qwen2-7B-Instruct with the\nrecent SOTA models with 7-9 billion parameters, including Llama-3-8B-Instruct, Yi-1.5-9B-Chat,\nGLM-4-9B-Chat, and Qwen1.5-7B-Chat. “-Instruct” or “-Chat” is omitted in the table. Qwen2-7B-\nInstruct demonstrates competitive performance against Llama-3-8B-Instruct.\nDatasets\nLlama-3-8B\nYi-1.5-9B\nGLM-4-9B\nQwen1.5-7B\nQwen2-7B\nEnglish\nMMLU\n68.4\n69.5\n72.4\n59.5\n70.5\nMMLU-Pro\n41.0\n-\n-\n29.1\n44.1\nGPQA\n34.2\n-\n-\n27.8\n34.3\nTheorem QA\n23.0\n-\n-\n14.1\n25.3\nCoding\nHumanEval\n62.2\n66.5\n71.8\n46.3\n79.9\nMBPP\n67.9\n-\n-\n48.9\n67.2\nMultiPL-E\n48.5\n-\n-\n27.2\n59.1\nLiveCodeBench v1\n17.3\n-\n-\n6.0\n26.6\nMathematics\nGSM8K\n79.6\n84.8\n79.6\n60.3\n85.7\nMATH\n30.0\n47.7\n50.6\n23.2\n52.9\nAlignment\nMT-Bench\n8.05\n8.20\n8.35\n7.60\n8.41\nMixEval\n75.0\n74.2\n-\n71.4\n76.5\nIFEval strict-prompt\n72.1\n-\n69.0\n38.3\n54.7\nAlignBench\n6.20\n6.90\n7.01\n6.20\n7.21\nTable 9: Performance of smaller instruction-tuned models. We compare both Qwen2-0.5B-Instruct\nand Qwen2-1.5B-Instruct with Qwen1.5-0.5B-Chat and Qwen2-1.8B-Chat. “-Instruct” or “-Chat”\nis omitted in the table. Compared with the similar-size baselines, Qwen2 significant surpasses the\nperformance of Qwen1.5.\nDatasets\nQwen1.5-0.5B\nQwen2-0.5B\nQwen1.5-1.8B\nQwen2-1.5B\nMMLU\n35.0\n37.9\n43.7\n52.4\nHumanEval\n10.4\n29.9\n27.4\n47.0\nMBPP\n14.5\n37.8\n28.6\n51.9\nGSM8K\n11.3\n40.1\n35.3\n61.6\nIFEval strict-prompt\n14.6\n20.0\n16.8\n29.0\n5.2.3\nLONG CONTEXT CAPABILITIES\nThree methods to evaluate long context capabilities are employed: the Needle in a Haystack (NIAH,\nKamradt, 2023), NeedleBench (OpenCompass Contributors, 2023), and LV-Eval (Yuan et al., 2024).\nNeedle in a Haystack\nThis experiment assesses a model’s proficiency in pinpointing facts within\nvoluminous texts. Texts with 8K, 16K, ..., 128K tokens in length were crafted, with facts strategically\npositioned at varying depths. Each depth interval, e.g., from 0% to 10%, encompassed two instances.\nFor contexts over 32K, YARN (Peng et al., 2023) was applied in this evaluation. As illustrated in\nFigure 1, Qwen2-72B-Instruct exhibits exceptional accuracy in retrieving information from the entire\n128K context. Coupled with its inherent strength, this model emerges as the optimal choice for\nprocessing extensive texts, assuming sufficient resources are accessible. Additionally, models within\nthe same series showcases remarkable performance across different context lengths. Precisely, Qwen2-\n7B-Instruct achieves a high level of accuracy in handling contexts up to 128K tokens. Meanwhile,\nQwen2-57B-A14B-Instruct manages contexts up to 64K tokens proficiently, and the two smaller\nmodels in the Qwen2 series could support contexts of 32K tokens.\n15\n\nTable 10: Performances of Qwen2-Instruct models on our in-house Chinese automatic evaluation\nbenchmark. Scores of Qwen2 models surpassing their comparable-sized Qwen1.5 counterparts are\nin bold. Qwen2-57B-A14B-Instruct is compared with Qwen1.5-32B-Chat.\nModels\nKnowledge\nExam\nComprehension\nCoding\nMath\nReasoning\nAvg.\nProprietary LLMs\nGPT-4o-2024-05-13\n66.68\n69.04\n76.85\n59.58\n71.16\n69.94\n68.87\nQwen-Max-0428\n76.65\n74.80\n73.66\n49.48\n66.01\n70.84\n68.57\nQwen1.5 Series\nQwen1.5-0.5B-Chat\n28.55\n36.99\n29.70\n3.82\n13.10\n25.47\n22.94\nQwen1.5-1.8B-Chat\n30.31\n44.98\n44.81\n6.86\n29.85\n34.61\n31.90\nQwen1.5-4B-Chat\n33.67\n47.17\n50.44\n14.05\n36.20\n39.98\n36.92\nQwen1.5-MoE-A2.7B-Chat\n52.76\n60.49\n52.84\n19.34\n38.45\n43.07\n44.49\nQwen1.5-7B-Chat\n56.77\n59.36\n55.50\n18.85\n46.41\n48.77\n47.61\nQwen1.5-14B-Chat\n63.35\n66.13\n60.06\n28.19\n54.80\n50.20\n53.79\nQwen1.5-32B-Chat\n68.63\n67.59\n64.67\n35.28\n60.62\n62.87\n59.94\nQwen1.5-72B-Chat\n71.52\n70.04\n66.70\n38.22\n63.09\n61.30\n61.81\nQwen1.5-110B-Chat\n76.26\n74.00\n71.25\n44.25\n64.92\n64.47\n65.86\nQwen2 Series\nQwen2-0.5B-Instruct\n28.18\n38.09\n35.90\n9.40\n21.20\n25.61\n26.40\nQwen2-1.5B-Instruct\n35.46\n51.93\n44.70\n14.05\n34.58\n35.94\n36.11\nQwen2-7B-Instruct\n61.54\n66.66\n59.63\n34.74\n60.99\n58.22\n56.96\nQwen2-57B-A14B-Instruct\n64.15\n73.67\n67.52\n40.66\n63.90\n59.89\n61.63\nQwen2-72B-Instruct\n76.19\n75.65\n74.72\n49.53\n70.80\n70.59\n69.58\nTable 11: Performances of Qwen2-Instruct models on our in-house English automatic evaluation\nbenchmark. Scores of Qwen2 models surpassing their comparable-sized Qwen1.5 and Llama-3\ncounterparts are in bold. Qwen2-57B-A14B-Instruct is compared with Qwen1.5-32B-Chat.\nModels\nKnowledge\nComprehension\nCoding\nMath\nAvg.\nProprietary LLMs\nGPT-4o-2024-05-13\n87.29\n76.30\n55.87\n84.99\n76.11\nQwen-Max-0428\n80.73\n71.63\n48.76\n79.12\n70.06\nQwen1.5 Series\nQwen1.5-0.5B-Chat\n30.12\n25.44\n1.78\n15.48\n18.21\nQwen1.5-1.8B-Chat\n40.37\n41.87\n4.99\n29.71\n29.23\nQwen1.5-4B-Chat\n51.44\n50.16\n15.45\n44.83\n40.47\nQwen1.5-MoE-A2.7B-Chat\n61.64\n54.79\n21.28\n50.46\n47.04\nQwen1.5-7B-Chat\n64.86\n58.61\n20.79\n54.24\n49.62\nQwen1.5-14B-Chat\n74.41\n59.80\n28.18\n66.91\n57.32\nQwen1.5-32B-Chat\n76.38\n64.70\n37.39\n73.04\n62.88\nQwen1.5-72B-Chat\n77.59\n67.58\n37.30\n73.76\n64.06\nQwen1.5-110B-Chat\n78.29\n70.17\n44.12\n78.87\n67.86\nLlama-3 Series\nLlama-3-8B-Instruct\n71.01\n64.71\n42.56\n65.82\n61.03\nLlama-3-70B-Instruct\n83.06\n76.31\n57.18\n79.70\n74.06\nQwen2 Series\nQwen2-0.5B-Instruct\n43.19\n29.57\n6.95\n31.52\n27.81\nQwen2-1.5B-Instruct\n56.03\n45.08\n17.61\n50.44\n42.29\nQwen2-7B-Instruct\n73.75\n63.09\n36.41\n75.67\n62.23\nQwen2-57B-A14B-Instruct\n76.80\n67.92\n42.37\n77.04\n66.03\nQwen2-72B-Instruct\n83.00\n73.58\n53.03\n82.15\n72.94\n16\n\nContext Length (# Tokens)\nPlaced Fact \nDocument \nDepth\nTop of\nDocument\nBottom of\nDocument\nTesting Qwen2-Instruct via “Needle in A HayStack”\nRetrieve Facts from Given Documents across Context Lengths and Document Depth\nQwen2-72B-Instruct\nContext Length (# Tokens)\nPlaced Fact \nDocument \nDepth\nTop of\nDocument\nBottom of\nDocument\nQwen2-7B-Instruct\nQwen2-7B-Instruct\nContext Length (# Tokens)\nPlaced Fact \nDocument \nDepth\nTop of\nDocument\nBottom of\nDocument\n100%\nAccuracy of \nRetrieval\n50%\nAccuracy of \nRetrieval\n0%\nAccuracy of \nRetrieval\nQwen2-57B-A14B-Instruct\nQwen2-1.5B-Instruct\nQwen2-0.5B-Instruct\nFigure 1: Performance of Qwen2 instruction-tuned models on Needle in A Haystack Test. All\nmodels that supports context lengths above 32k tokens integrates the YARN mechanism.\nTable 12: Performance of Qwen2-72B-Instruct and Qwen2-7B-Instruct on NeedleBench and\nLV-Eval. +YARN+DCA does not change the model behavior within 32k tokens.\nDatasets\nNeedleBench\nLV-Eval\n8k\n32k\n128k\n256k\n16k\n32k\n64k\n128k\n256k\nChatGLM4-9B-1M\n56.61\n49.15\n44.30\n45.29\n46.40\n43.23\n42.92\n40.41\n36.95\nQwen2-7B-Instruct\n87.07\n73.64\n38.77\n2.92\n49.77\n46.93\n28.03\n11.01\n0.55\n+ YARN + DCA\n66.32\n60.71\n42.14\n36.64\n34.72\nQwen2-72B-Instruct\n91.90\n92.01\n73.05\n17.13\n58.82\n56.70\n42.92\n31.79\n2.88\n+ YARN + DCA\n90.27\n85.21\n53.03\n48.83\n42.35\n17\n\nTable 13: Performance of Qwen2-72B-Instruct and proprietary LLMs in multilingual human\nevaluation. We compare Qwen2-72B-Instruct with GPT-3.5-Turbo-1106, GPT-4-Turbo-0409, GPT-\n4o-0513, Claude-3-Opus-0229. Scores range from 1 to 5. Overall, Qwen2-72B-Instruct performs\nsubstantially better than GPT-3.5-Turbo but there is progress to be made to be competitive with the\nproprietary models released in the last 6 months.\nLanguage\nGPT-3.5-Turbo GPT-4-Turbo\nGPT-4o\nClaude-3-Opus\nQwen2-72B-Instruct\nArabic\n2.52\n3.44\n3.55\n4.15\n3.86\nFrench\n3.47\n4.19\n4.16\n4.23\n4.01\nIndonesian\n3.56\n4.09\n4.39\n4.40\n3.83\nJapanese\n2.75\n3.68\n3.72\n3.85\n3.63\nKorean\n2.37\n4.24\n4.40\n4.23\n4.14\nPortuguese\n3.37\n3.86\n3.89\n4.09\n3.97\nRussian\n3.24\n4.27\n4.32\n4.25\n4.15\nSpanish\n4.07\n4.08\n4.26\n4.31\n4.10\nThai\n3.38\n4.11\n4.09\n4.01\n3.75\nVietnamese\n3.90\n3.84\n4.14\n3.98\n3.91\nAverage\n3.16\n3.98\n4.09\n4.15\n3.93\nNeedleBench\nNeedleBench ups the challenge on NIAH by including multiple facts (two to five) in\npassages, necessitating simultaneous identification and multi-hop reasoning. Table 12 reveals that\nthe integration of YARN and DCA (An et al., 2024) notably improves Qwen2 models’ long-context\nabilities. Qwen2-7B-Instruct surpasses ChatGLM4-9B-1M (Zeng et al., 2024), which claims a 1M\ncontext length. Moreover, Qwen2-72B-Instruct demonstrates strong performance, with an accuracy\nreduction of just 6 points, compared to ChatGLM4-9B-1M, which shows a more pronounced decline\nof 11 points, particularly given its lower initial accuracy.\nLV-Eval\nLV-Eval comprises 11 diverse QA datasets that demand comprehension of multiple pieces\nof evidence at once. To rectify the shortcomings of its original metric, which was excessively stringent\nand led to a high rate of false negatives, we adopt the keyword recall as the reported score. As shown\nin Table 12, integrating YARN and DCA substantially bolsters the long-context competencies of\nQwen2 models on LV-Eval. Qwen2-7B-Instruct achieves parity with ChatGLM4-9B-1M, albeit with\na more noticeable decline at extended contexts. Moreover, Qwen2-72B-Instruct demonstrates strong\nperformance across all lengths, confirming its proficiency in handling long-context tasks.\n5.2.4\nMULTILINGUAL EVALUATION\nFor the multilingual evaluation, we implement a comprehensive human evaluation for the assessment\nof multilingual capabilities. Specifically, we design diverse test cases assessing different capabilities\nof large language models, and we have test cases that are in a number of languages. For the annotators,\nwe invite one professional annotator for each language who majors in the language for the evaluation.\nFor each test case, the annotator grades the response from model with a score from 1 to 5.\nWe report the results of our model and the baselines in the evaluation of different languages. From\nTable 13, it can be found that on average Qwen2-72B-Instruct significantly outperforms GPT-3.5-\nTurbo and it is competitive with GPT-4-Turbo and slightly falls behind Claude-3-Opus. This shows\nthat our multilingual pre-training and instruction tuning data contribute to the multilingual capabilities\nof Qwen2-72B-Instruct and it is competitive with most state-of-the-art proprietary LLMs.\n5.2.5\nSAFETY & RESPONSIBILITY\nLLMs with openly accessible weights effectively accelerate the development of the research as well\nas their applications. Moreover, we believe that it is crucial to build safe and responsible LLMs so\nthat the effect of the misuse of AI technologies could be significantly alleviated.\nWe implement a multilingual safety evaluation that tests the LLMs in different languages. Specif-\nically, we assess the safety performance of the models in the topics about illegal behaviors, fraud,\n18\n\nTable 14: Performance of models in safety evaluation. We compare Qwen2-72B-Instruct with\nGPT-4 and Mixtral-8x22B-Instruct. The lower, the better. Qwen2-72B-Instruct rejected more prompts\nwith risks than the competitors.\nRisk Category\nGPT-4\nMixtral-8x22B\nQwen2-72B-Instruct\nIllegal\n0.00\n6.87\n0.00\nFraud\n3.40\n8.49\n2.41\nPornography\n23.63\n33.82\n22.91\nPrivacy\n3.37\n15.03\n2.47\nTable 15: Contamination Analysis. The contaminated samples in this table are identified using a\nstrict criterion: any test sample with a 13-gram overlap with the pre-training or post-training data is\nconsidered contaminated. We report the percentage of contaminated samples as well as the model\nperformance on both the original and non-contaminated test sets.\nTest set\nPercent of\nQwen2-72B-Instruct\nQwen2-7B-Instruct\nContamination Original\nNon-Contam.\n∆\nOriginal\nNon-Contam.\n∆\nMMLU\n11.2%\n82.3\n83.2\n0.9\n70.5\n71.3\n0.8\nMMLU-Pro\n11.6%\n64.4\n65.6\n1.2\n44.1\n46.5\n2.4\nGPQA\n1.0%\n42.4\n41.8\n0.6\n34.3\n34.1\n-0.2\nHumanEval\n75.0%\n86.0\n87.0\n1.0\n79.9\n87.8\n7.9\nMBPP\n29.6%\n80.2\n79.7\n0.5\n67.2\n69.0\n1.8\nMultiPL-E\n37.7%\n69.2\n69.2\n0.0\n59.1\n58.9\n-0.2\nGSM8k\n0.7%\n93.2\n92.8\n-0.4\n85.7\n85.6\n-0.1\nMath\n31.7%\n69.0\n74.6\n5.6\n52.9\n57.6\n4.7\nIFEval\n0.9%\n77.6\n77.4\n-0.2\n54.7\n53.7\n-1.0\npornography, and privacy. We have collected prompts prone to jail-breaking and use them to test\nwhether the models can provide safe responses by rejection.\nThe results are presented in Table 14, where the proportion of harmful responses generated by the\nmodels are shown and the lower, the better. It can be observed that Qwen2-72B-Instruct performs\nbetter than the proprietary model, GPT-4, and significantly outperforms the open-weight model,\nMixtral-8x22B-Instruct. However, we believe that there is still much room for our model to improve to\nbe a safer and more responsible model, especially in terms of pornography, which is a conventionally\ndifficult category to differentiate even for humans.\n5.2.6\nCONTAMINATION ANALYSIS\nFor large language models, what counts as contamination and how to run contamination analysis\nremain an active area of research (Ravaut et al., 2024; Golchin & Surdeanu, 2024; Sainz et al., 2023).\nIn the following, we first introduce how we try to decontaminate the training corpora against the\nevaluation datasets, and then estimate the extent to which benchmark scores are influenced by the\nremaining contamination.\nDuring the construction of the pre-training and post-training datasets, we exclude potentially contam-\ninated data using n-gram matching. However, we found that this approach may lead to a high false\nnegative rate, because there could be commonly used expressions, especially in mathematical and\ncoding data. Therefore, we also applied another constraint based on the longest common subsequence\n(LCS). Specifically, we first remove all symbols and punctuation from both the test and training\nsequences and perform tokenization. For a training sequence st, we remove it if there is a test\nsequence se such that |LCS(st, se)| ≥13 and |LCS(st, se)| ≥0.6 × min(|st|, |se|).\nTo assess the potential effects of leaking data on the test performance, we follow OpenAI (2023) to\nconstruct a strict non-contaminated test set to check if there is a significant performance degradation\nafter strict decontamination. Specifically, we construct the non-contaminated test set by excluding any\n19\n\nsample which has 13-gram overlap with the pre-training or the post-training data (without constraint\non LCS), and then compute the corresponding metric on the test set.\nThe results are presented in Table 15. Although some datasets exhibit a high percentage of contam-\nination under the strict criterion, we noticed that most of the identified contaminated samples are\nfalse positives, primarily stemming from the mathematics and coding datasets. It is likely that certain\ncode snippets and mathematical equations are so common that they do not provide any meaningful\nadvantage in solving the test data. Furthermore, our analysis shows that the performance of the\nQwen2 models remains consistent between the original and non-contaminated test data, suggesting\nthat the potential issue of data contamination does not significantly impact the model’s performance.\n6\nCONCLUSION\nThis technical report has presented the Qwen2 series, a versatile suite of foundational and instruction-\ntuned language models, ranging from 0.5 to 72 billion parameters, including models of dense and\nMixture-of-Experts architecture. Qwen2 outperforms previous open-weight models, notably its\npredecessor Qwen1.5, and displays competitive performance against proprietary models across a\nbroad spectrum of benchmarks in language understanding, generation, multilingual capabilities,\ncoding, mathematics, and reasoning. In this update, we have extra focus on long-context, multi-\nlingual, coding, mathematics capabilities and safety and responsibility. In a commitment to fostering\ninnovation and accessibility within the community, we have made the Qwen2 model weights openly\naccessible, which enables researchers and developers to harness the full potential of Qwen2 in a\nvariety of applications and research projects. Through these efforts, we aim to contribute to the\nadvancement of AI technologies and their positive impact on society.\n20\n\nREFERENCES\nMarah Abdin, Jyoti Aneja, Sebastien Bubeck, Caio C´esar Teodoro Mendes, Weizhu Chen, Allie Del\nGiorno, Ronen Eldan, Sivakanth Gopi, Suriya Gunasekar, Mojan Javaheripi, Piero Kauffmann,\nYin Tat Lee, Yuanzhi Li, Anh Nguyen, Gustavo de Rosa, Olli Saarikivi, Adil Salim, Shital Shah,\nMichael Santacroce, Harkirat Singh Behl, Adam Taumann Kalai, Xin Wang, Rachel Ward, Philipp\nWitte, Cyril Zhang, and Yi Zhang. Phi-2: The surprising power of small language models,\n2024. URL https://www.microsoft.com/en-us/research/blog/phi-2-the-\nsurprising-power-of-small-language-models/.\nAI@Meta. Llama 3 model card, 2024. URL https://github.com/meta-llama/llama3/\nblob/main/MODEL_CARD.md.\nJoshua Ainslie, James Lee-Thorp, Michiel de Jong, Yury Zemlyanskiy, Federico Lebr´on, and Sumit\nSanghai. GQA: Training generalized multi-query Transformer models from multi-head checkpoints.\nIn EMNLP, pp. 4895–4901. Association for Computational Linguistics, 2023.\nChenxin An, Fei Huang, Jun Zhang, Shansan Gong, Xipeng Qiu, Chang Zhou, and Lingpeng Kong.\nTraining-free long-context scaling of large language models. CoRR, abs/2402.17463, 2024.\nAnthropic.\nThe\nClaude\n3\nmodel\nfamily:\nOpus,\nSonnet,\nHaiku.\nTechnical\nre-\nport,\nAnthropic,\nAI,\n2024.\nURL\nhttps://www-cdn.anthropic.com/\nde8ba9b01c9ab7cbabf5c33b80b7bbc618857627/Model_Card_Claude_3.pdf.\nJacob Austin, Augustus Odena, Maxwell I. Nye, Maarten Bosma, Henryk Michalewski, David Dohan,\nEllen Jiang, Carrie J. Cai, Michael Terry, Quoc V. Le, and Charles Sutton. Program synthesis with\nlarge language models. CoRR, abs/2108.07732, 2021.\nJinze Bai, Shuai Bai, Yunfei Chu, Zeyu Cui, Kai Dang, Xiaodong Deng, Yang Fan, Wenbin Ge,\nYu Han, Fei Huang, Binyuan Hui, Luo Ji, Mei Li, Junyang Lin, Runji Lin, Dayiheng Liu, Gao Liu,\nChengqiang Lu, Keming Lu, Jianxin Ma, Rui Men, Xingzhang Ren, Xuancheng Ren, Chuanqi Tan,\nSinan Tan, Jianhong Tu, Peng Wang, Shijie Wang, Wei Wang, Shengguang Wu, Benfeng Xu, Jin\nXu, An Yang, Hao Yang, Jian Yang, Shusheng Yang, Yang Yao, Bowen Yu, Hongyi Yuan, Zheng\nYuan, Jianwei Zhang, Xingxuan Zhang, Yichang Zhang, Zhenru Zhang, Chang Zhou, Jingren\nZhou, Xiaohuan Zhou, and Tianhang Zhu. Qwen technical report. CoRR, abs/2309.16609, 2023a.\nJinze Bai, Shuai Bai, Shusheng Yang, Shijie Wang, Sinan Tan, Peng Wang, Junyang Lin, Chang\nZhou, and Jingren Zhou. Qwen-VL: A frontier large vision-language model with versatile abilities.\nCoRR, abs/2308.12966, 2023b.\nYuntao Bai, Saurav Kadavath, Sandipan Kundu, Amanda Askell, Jackson Kernion, Andy Jones,\nAnna Chen, Anna Goldie, Azalia Mirhoseini, Cameron McKinnon, Carol Chen, Catherine Olsson,\nChristopher Olah, Danny Hernandez, Dawn Drain, Deep Ganguli, Dustin Li, Eli Tran-Johnson,\nEthan Perez, Jamie Kerr, Jared Mueller, Jeffrey Ladish, Joshua Landau, Kamal Ndousse, Kamile\nLukosiute, Liane Lovitt, Michael Sellitto, Nelson Elhage, Nicholas Schiefer, Noem´ı Mercado, Nova\nDasSarma, Robert Lasenby, Robin Larson, Sam Ringer, Scott Johnston, Shauna Kravec, Sheer El\nShowk, Stanislav Fort, Tamera Lanham, Timothy Telleen-Lawton, Tom Conerly, Tom Henighan,\nTristan Hume, Samuel R. Bowman, Zac Hatfield-Dodds, Ben Mann, Dario Amodei, Nicholas\nJoseph, Sam McCandlish, Tom Brown, and Jared Kaplan. Constitutional AI: Harmlessness from\nAI feedback. CoRR, abs/2212.08073, 2022.\nLucas Bandarkar, Davis Liang, Benjamin Muller, Mikel Artetxe, Satya Narayan Shukla, Donald Husa,\nNaman Goyal, Abhinandan Krishnan, Luke Zettlemoyer, and Madian Khabsa. The Belebele bench-\nmark: A parallel reading comprehension dataset in 122 language variants. CoRR, abs/2308.16884,\n2023.\nBoxi Cao, Keming Lu, Xinyu Lu, Jiawei Chen, Mengjie Ren, Hao Xiang, Peilin Liu, Yaojie Lu, Ben\nHe, Xianpei Han, Le Sun, Hongyu Lin, and Bowen Yu. Towards scalable automated alignment of\nLLMs: A survey. CoRR, abs/2406.01252, 2024.\nFederico Cassano, John Gouwar, Daniel Nguyen, Sydney Nguyen, Luna Phipps-Costin, Donald\nPinckney, Ming-Ho Yee, Yangtian Zi, Carolyn Jane Anderson, Molly Q. Feldman, Arjun Guha,\nMichael Greenberg, and Abhinav Jangda. MultiPL-E: A scalable and polyglot approach to\nbenchmarking neural code generation. IEEE Trans. Software Eng., 49(7):3675–3691, 2023.\n21\n\nMark Chen, Jerry Tworek, Heewoo Jun, Qiming Yuan, Henrique Pond´e de Oliveira Pinto, Jared\nKaplan, Harrison Edwards, Yuri Burda, Nicholas Joseph, Greg Brockman, Alex Ray, Raul Puri,\nGretchen Krueger, Michael Petrov, Heidy Khlaaf, Girish Sastry, Pamela Mishkin, Brooke Chan,\nScott Gray, Nick Ryder, Mikhail Pavlov, Alethea Power, Lukasz Kaiser, Mohammad Bavarian,\nClemens Winter, Philippe Tillet, Felipe Petroski Such, Dave Cummings, Matthias Plappert, Fotios\nChantzis, Elizabeth Barnes, Ariel Herbert-Voss, William Hebgen Guss, Alex Nichol, Alex Paino,\nNikolas Tezak, Jie Tang, Igor Babuschkin, Suchir Balaji, Shantanu Jain, William Saunders,\nChristopher Hesse, Andrew N. Carr, Jan Leike, Joshua Achiam, Vedant Misra, Evan Morikawa,\nAlec Radford, Matthew Knight, Miles Brundage, Mira Murati, Katie Mayer, Peter Welinder, Bob\nMcGrew, Dario Amodei, Sam McCandlish, Ilya Sutskever, and Wojciech Zaremba. Evaluating\nlarge language models trained on code. CoRR, abs/2107.03374, 2021.\nWenhu Chen, Ming Yin, Max Ku, Pan Lu, Yixin Wan, Xueguang Ma, Jianyu Xu, Xinyi Wang, and\nTony Xia. TheoremQA: A theorem-driven question answering dataset. In EMNLP, pp. 7889–7901.\nAssociation for Computational Linguistics, 2023a.\nZhihong Chen, Shuo Yan, Juhao Liang, Feng Jiang, Xiangbo Wu, Fei Yu, Guiming Hardy Chen,\nJunying Chen, Hongbo Zhang, Li Jianquan, Wan Xiang, and Benyou Wang.\nMultilingual-\nSIFT: Multilingual supervised instruction fine-tuning, 2023b. URL https://github.com/\nFreedomIntelligence/MultilingualSIFT.\nWei-Lin Chiang, Lianmin Zheng, Ying Sheng, Anastasios Nikolas Angelopoulos, Tianle Li, Dacheng\nLi, Hao Zhang, Banghua Zhu, Michael I. Jordan, Joseph E. Gonzalez, and Ion Stoica. Chatbot\narena: An open platform for evaluating LLMs by human preference. CoRR, abs/2403.04132, 2024.\nYunfei Chu, Jin Xu, Xiaohuan Zhou, Qian Yang, Shiliang Zhang, Zhijie Yan, Chang Zhou, and\nJingren Zhou. Qwen-Audio: Advancing universal audio understanding via unified large-scale\naudio-language models. CoRR, abs/2311.07919, 2023.\nPeter Clark, Isaac Cowhey, Oren Etzioni, Tushar Khot, Ashish Sabharwal, Carissa Schoenick, and\nOyvind Tafjord. Think you have solved question answering? Try ARC, the AI2 reasoning challenge.\nCoRR, abs/1803.05457, 2018.\nKarl Cobbe, Vineet Kosaraju, Mohammad Bavarian, Mark Chen, Heewoo Jun, Lukasz Kaiser,\nMatthias Plappert, Jerry Tworek, Jacob Hilton, Reiichiro Nakano, Christopher Hesse, and John\nSchulman. Training verifiers to solve math word problems. CoRR, abs/2110.14168, 2021.\nDamai Dai, Chengqi Deng, Chenggang Zhao, R. X. Xu, Huazuo Gao, Deli Chen, Jiashi Li, Wangding\nZeng, Xingkai Yu, Y. Wu, Zhenda Xie, Y. K. Li, Panpan Huang, Fuli Luo, Chong Ruan, Zhifang Sui,\nand Wenfeng Liang. DeepSeekMoE: Towards ultimate expert specialization in mixture-of-experts\nlanguage models. CoRR, abs/2401.06066, 2024.\nYann N. Dauphin, Angela Fan, Michael Auli, and David Grangier. Language modeling with gated\nconvolutional networks. In ICML, volume 70 of Proceedings of Machine Learning Research, pp.\n933–941. PMLR, 2017.\nGuanting Dong, Hongyi Yuan, Keming Lu, Chengpeng Li, Mingfeng Xue, Dayiheng Liu, Wei Wang,\nZheng Yuan, Chang Zhou, and Jingren Zhou. How abilities in large language models are affected\nby supervised fine-tuning data composition. CoRR, abs/2310.05492, 2023.\nGuanting Dong, Keming Lu, Chengpeng Li, Tingyu Xia, Bowen Yu, Chang Zhou, and Jingren Zhou.\nSelf-play with execution feedback: Improving instruction-following capabilities of large language\nmodels. CoRR, abs/2406.13542, 2024.\nAlena Fenogenova, Artem Chervyakov, Nikita Martynov, Anastasia Kozlova, Maria Tikhonova, Al-\nbina Akhmetgareeva, Anton A. Emelyanov, Denis Shevelev, Pavel Lebedev, Leonid Sinev, Ulyana\nIsaeva, Katerina Kolomeytseva, Daniil Moskovskiy, Elizaveta Goncharova, Nikita Savushkin,\nPolina Mikhailova, Denis Dimitrov, Alexander Panchenko, and Sergey Markov. MERA: A com-\nprehensive LLM evaluation in russian. CoRR, abs/2401.04531, 2024.\nShahriar Golchin and Mihai Surdeanu. Time travel in llms: Tracing data contamination in large\nlanguage models. In ICLR. OpenReview.net, 2024.\n22\n\nNaman Goyal, Cynthia Gao, Vishrav Chaudhary, Peng-Jen Chen, Guillaume Wenzek, Da Ju, Sanjana\nKrishnan, Marc’Aurelio Ranzato, Francisco Guzm´an, and Angela Fan. The Flores-101 evalua-\ntion benchmark for low-resource and multilingual machine translation. Trans. Assoc. Comput.\nLinguistics, 10:522–538, 2022.\nDan Hendrycks, Collin Burns, Steven Basart, Andy Zou, Mantas Mazeika, Dawn Song, and Jacob\nSteinhardt. Measuring massive multitask language understanding. In ICLR. OpenReview.net,\n2021a.\nDan Hendrycks, Collin Burns, Saurav Kadavath, Akul Arora, Steven Basart, Eric Tang, Dawn Song,\nand Jacob Steinhardt. Measuring mathematical problem solving with the MATH dataset. In\nNeurIPS Datasets and Benchmarks, 2021b.\nYuzhen Huang, Yuzhuo Bai, Zhihao Zhu, Junlei Zhang, Jinghan Zhang, Tangjun Su, Junteng Liu,\nChuancheng Lv, Yikai Zhang, Jiayi Lei, Yao Fu, Maosong Sun, and Junxian He. C-Eval: A\nmulti-level multi-discipline chinese evaluation suite for foundation models. In NeurIPS, 2023.\nNaman Jain, King Han, Alex Gu, Wen-Ding Li, Fanjia Yan, Tianjun Zhang, Sida Wang, Armando\nSolar-Lezama, Koushik Sen, and Ion Stoica. LiveCodeBench: Holistic and contamination free\nevaluation of large language models for code. CoRR, abs/2403.07974, 2024.\nAlbert Q. Jiang, Alexandre Sablayrolles, Arthur Mensch, Chris Bamford, Devendra Singh Chaplot,\nDiego de Las Casas, Florian Bressand, Gianna Lengyel, Guillaume Lample, Lucile Saulnier,\nL´elio Renard Lavaud, Marie-Anne Lachaux, Pierre Stock, Teven Le Scao, Thibaut Lavril, Thomas\nWang, Timoth´ee Lacroix, and William El Sayed. Mistral 7B. CoRR, abs/2310.06825, 2023a.\nAlbert Q. Jiang, Alexandre Sablayrolles, Antoine Roux, Arthur Mensch, Blanche Savary, Chris\nBamford, Devendra Singh Chaplot, Diego de Las Casas, Emma Bou Hanna, Florian Bressand,\nGianna Lengyel, Guillaume Bour, Guillaume Lample, L´elio Renard Lavaud, Lucile Saulnier, Marie-\nAnne Lachaux, Pierre Stock, Sandeep Subramanian, Sophia Yang, Szymon Antoniak, Teven Le\nScao, Th´eophile Gervet, Thibaut Lavril, Thomas Wang, Timoth´ee Lacroix, and William El Sayed.\nMixtral of experts. CoRR, abs/2401.04088, 2024.\nZixuan Jiang, Jiaqi Gu, Hanqing Zhu, and David Z. Pan. Pre-RMSNorm and Pre-CRMSNorm\nTransformers: Equivalent and efficient pre-LN Transformers. CoRR, abs/2305.14858, 2023b.\nGregory Kamradt. Needle in a haystack - pressure testing LLMs, 2023. URL https://github.\ncom/gkamradt/LLMTest_NeedleInAHaystack.\nAran Komatsuzaki, Joan Puigcerver, James Lee-Thorp, Carlos Riquelme Ruiz, Basil Mustafa, Joshua\nAinslie, Yi Tay, Mostafa Dehghani, and Neil Houlsby. Sparse upcycling: Training mixture-of-\nexperts from dense checkpoints. In ICLR. OpenReview.net, 2023.\nFajri Koto, Nurul Aisyah, Haonan Li, and Timothy Baldwin. Large language models only pass\nprimary school exams in Indonesia: A comprehensive test on IndoMMLU. In EMNLP, pp.\n12359–12374. Association for Computational Linguistics, 2023.\nHaonan Li, Yixuan Zhang, Fajri Koto, Yifei Yang, Hai Zhao, Yeyun Gong, Nan Duan, and Timothy\nBaldwin. CMMLU: Measuring massive multitask language understanding in Chinese. CoRR,\nabs/2306.09212, 2023.\nTianle Li, Wei-Lin Chiang, Evan Frick, Lisa Dunlap, Tianhao Wu, Banghua Zhu, Joseph E. Gon-\nzalez, and Ion Stoica. From crowdsourced data to high-quality benchmarks: Arena-Hard and\nBenchBuilder pipeline. CoRR, abs/2406.11939, 2024.\nOpher Lieber, Barak Lenz, Hofit Bata, Gal Cohen, Jhonathan Osin, Itay Dalmedigos, Erez Safahi,\nShaked Meirom, Yonatan Belinkov, Shai Shalev-Shwartz, Omri Abend, Raz Alon, Tomer Asida,\nAmir Bergman, Roman Glozman, Michael Gokhman, Avashalom Manevich, Nir Ratner, Noam\nRozen, Erez Shwartz, Mor Zusman, and Yoav Shoham. Jamba: A hybrid Transformer-Mamba\nlanguage model. CoRR, abs/2403.19887, 2024.\nStephanie Lin, Jacob Hilton, and Owain Evans. TruthfulQA: Measuring how models mimic human\nfalsehoods. In ACL (1), pp. 3214–3252. Association for Computational Linguistics, 2022a.\n23\n\nXi Victoria Lin, Todor Mihaylov, Mikel Artetxe, Tianlu Wang, Shuohui Chen, Daniel Simig, Myle Ott,\nNaman Goyal, Shruti Bhosale, Jingfei Du, Ramakanth Pasunuru, Sam Shleifer, Punit Singh Koura,\nVishrav Chaudhary, Brian O’Horo, Jeff Wang, Luke Zettlemoyer, Zornitsa Kozareva, Mona T.\nDiab, Veselin Stoyanov, and Xian Li. Few-shot learning with multilingual generative language\nmodels. In EMNLP, pp. 9019–9052. Association for Computational Linguistics, 2022b.\nJiawei Liu, Chunqiu Steven Xia, Yuyao Wang, and Lingming Zhang. Is your code generated by\nChatGPT really correct? Rigorous evaluation of large language models for code generation. In\nNeurIPS, 2023a.\nXiao Liu, Xuanyu Lei, Shengyuan Wang, Yue Huang, Zhuoer Feng, Bosi Wen, Jiale Cheng, Pei Ke,\nYifan Xu, Weng Lam Tam, Xiaohan Zhang, Lichao Sun, Hongning Wang, Jing Zhang, Minlie\nHuang, Yuxiao Dong, and Jie Tang. AlignBench: Benchmarking Chinese alignment of large\nlanguage models. CoRR, abs/2311.18743, 2023b.\nKeming Lu, Bowen Yu, Fei Huang, Yang Fan, Runji Lin, and Chang Zhou. Online merging optimizers\nfor boosting rewards and mitigating tax in alignment. CoRR, abs/2405.17931, 2024a.\nKeming Lu, Bowen Yu, Chang Zhou, and Jingren Zhou. Large language models are superpositions\nof all characters: Attaining arbitrary role-play via self-alignment. CoRR, abs/2401.12474, 2024b.\nKeming Lu, Hongyi Yuan, Zheng Yuan, Runji Lin, Junyang Lin, Chuanqi Tan, Chang Zhou, and\nJingren Zhou. #InsTag: Instruction tagging for analyzing supervised fine-tuning of large language\nmodels. In ICLR. OpenReview.net, 2024c.\nThomas Mesnard, Cassidy Hardin, Robert Dadashi, Surya Bhupatiraju, Shreya Pathak, Laurent Sifre,\nMorgane Rivi`ere, Mihir Sanjay Kale, Juliette Love, Pouya Tafti, L´eonard Hussenot, Pier Giuseppe\nSessa, Aakanksha Chowdhery, Adam Roberts, Aditya Barua, Alex Botev, Alex Castro-Ros,\nAmbrose Slone, Am´elie H´eliou, Andrea Tacchetti, Anna Bulanova, Antonia Paterson, Beth Tsai,\nBobak Shahriari, Charline Le Lan, Christopher A. Choquette-Choo, Cl´ement Crepy, Daniel Cer,\nDaphne Ippolito, David Reid, Elena Buchatskaya, Eric Ni, Eric Noland, Geng Yan, George\nTucker, George-Christian Muraru, Grigory Rozhdestvenskiy, Henryk Michalewski, Ian Tenney,\nIvan Grishchenko, Jacob Austin, James Keeling, Jane Labanowski, Jean-Baptiste Lespiau, Jeff\nStanway, Jenny Brennan, Jeremy Chen, Johan Ferret, Justin Chiu, Justin Mao-Jones, Katherine\nLee, Kathy Yu, Katie Millican, Lars Lowe Sjoesund, Lisa Lee, Lucas Dixon, Machel Reid, Maciej\nMikuła, Mateo Wirth, Michael Sharman, Nikolai Chinaev, Nithum Thain, Olivier Bachem, Oscar\nChang, Oscar Wahltinez, Paige Bailey, Paul Michel, Petko Yotov, Rahma Chaabouni, Ramona\nComanescu, Reena Jana, Rohan Anil, Ross McIlroy, Ruibo Liu, Ryan Mullins, Samuel L Smith,\nSebastian Borgeaud, Sertan Girgin, Sholto Douglas, Shree Pandya, Siamak Shakeri, Soham De,\nTed Klimenko, Tom Hennigan, Vlad Feinberg, Wojciech Stokowiec, Yu hui Chen, Zafarali Ahmed,\nZhitao Gong, Tris Warkentin, Ludovic Peran, Minh Giang, Cl´ement Farabet, Oriol Vinyals, Jeff\nDean, Koray Kavukcuoglu, Demis Hassabis, Zoubin Ghahramani, Douglas Eck, Joelle Barral,\nFernando Pereira, Eli Collins, Armand Joulin, Noah Fiedel, Evan Senter, Alek Andreev, and\nKathleen Kenealy. Gemma: Open models based on Gemini research and technology. CoRR,\nabs/2403.08295, 2024.\nNiklas Muennighoff, Thomas Wang, Lintang Sutawika, Adam Roberts, Stella Biderman, Teven Le\nScao, M. Saiful Bari, Sheng Shen, Zheng Xin Yong, Hailey Schoelkopf, Xiangru Tang, Dragomir\nRadev, Alham Fikri Aji, Khalid Almubarak, Samuel Albanie, Zaid Alyafeai, Albert Webson,\nEdward Raff, and Colin Raffel. Crosslingual generalization through multitask finetuning. In ACL\n(1), pp. 15991–16111. Association for Computational Linguistics, 2023.\nJinjie Ni, Fuzhao Xue, Xiang Yue, Yuntian Deng, Mahir Shah, Kabir Jain, Graham Neubig, and\nYang You. MixEval: Deriving wisdom of the crowd from LLM benchmark mixtures. CoRR,\nabs/2406.06565, 2024.\nOpenAI. Introducing ChatGPT, 2022. URL https://openai.com/index/chatgpt/.\nOpenAI. GPT4 technical report. arXiv preprint arXiv:2303.08774, 2023.\nOpenAI. Hello GPT-4o, 2024. URL https://openai.com/index/hello-gpt-4o/.\n24\n\nOpenCompass Contributors. OpenCompass: A universal evaluation platform for foundation models,\n2023. URL https://github.com/open-compass/opencompass.\nBowen Peng, Jeffrey Quesnelle, Honglu Fan, and Enrico Shippole. YaRN: Efficient context window\nextension of large language models. CoRR, abs/2309.00071, 2023.\nEdoardo Maria Ponti, Goran Glavas, Olga Majewska, Qianchu Liu, Ivan Vulic, and Anna Korhonen.\nXCOPA: A multilingual dataset for causal commonsense reasoning. In EMNLP (1), pp. 2362–2376.\nAssociation for Computational Linguistics, 2020.\nQwen Team.\nIntroducing Qwen1.5, 2024a.\nURL https://qwenlm.github.io/blog/\nqwen1.5/.\nQwen Team. Qwen1.5-110B: The first 100B+ model of the Qwen1.5 series, 2024b. URL https:\n//qwenlm.github.io/blog/qwen1.5-110b/.\nQwen Team. Qwen1.5-MoE: Matching 7B model performance with 1/3 activated parameters, 2024c.\nURL https://qwenlm.github.io/blog/qwen-moe/.\nRafael Rafailov, Archit Sharma, Eric Mitchell, Christopher D. Manning, Stefano Ermon, and Chelsea\nFinn. Direct preference optimization: Your language model is secretly a reward model. In NeurIPS,\n2023.\nSamyam Rajbhandari, Conglong Li, Zhewei Yao, Minjia Zhang, Reza Yazdani Aminabadi, Am-\nmar Ahmad Awan, Jeff Rasley, and Yuxiong He. DeepSpeed-MoE: Advancing mixture-of-experts\ninference and training to power next-generation AI scale. In ICML, volume 162 of Proceedings of\nMachine Learning Research, pp. 18332–18346. PMLR, 2022.\nMathieu Ravaut, Bosheng Ding, Fangkai Jiao, Hailin Chen, Xingxuan Li, Ruochen Zhao, Chengwei\nQin, Caiming Xiong, and Shafiq Joty. How much are LLMs contaminated? A comprehensive\nsurvey and the llmsanitize library. CoRR, abs/2404.00699, 2024.\nDavid Rein, Betty Li Hou, Asa Cooper Stickland, Jackson Petty, Richard Yuanzhe Pang, Julien\nDirani, Julian Michael, and Samuel R. Bowman. GPQA: A graduate-level Google-proof Q&A\nbenchmark. CoRR, abs/2311.12022, 2023.\nOscar Sainz, Jon Ander Campos, Iker Garc´ıa-Ferrero, Julen Etxaniz, Oier Lopez de Lacalle, and\nEneko Agirre. NLP evaluation in trouble: On the need to measure LLM data contamination\nfor each benchmark. In EMNLP (Findings), pp. 10776–10787. Association for Computational\nLinguistics, 2023.\nKeisuke Sakaguchi, Ronan Le Bras, Chandra Bhagavatula, and Yejin Choi. WinoGrande: An\nadversarial winograd schema challenge at scale. Commun. ACM, 64(9):99–106, 2021.\nJianlin Su. The magical effect of the Bias term: RoPE + Bias = better length extrapolation, 2023.\nURL https://spaces.ac.cn/archives/9577.\nJianlin Su, Murtadha H. M. Ahmed, Yu Lu, Shengfeng Pan, Wen Bo, and Yunfeng Liu. Roformer:\nEnhanced Transformer with rotary position embedding. Neurocomputing, 568:127063, 2024.\nMirac Suzgun, Nathan Scales, Nathanael Sch¨arli, Sebastian Gehrmann, Yi Tay, Hyung Won Chung,\nAakanksha Chowdhery, Quoc V. Le, Ed H. Chi, Denny Zhou, and Jason Wei. Challenging BIG-\nBench tasks and whether chain-of-thought can solve them. In ACL (Findings), pp. 13003–13051.\nAssociation for Computational Linguistics, 2023.\nHugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier Martinet, Marie-Anne Lachaux, Timoth´ee\nLacroix, Baptiste Rozi`ere, Naman Goyal, Eric Hambro, Faisal Azhar, Aur´elien Rodriguez, Armand\nJoulin, Edouard Grave, and Guillaume Lample. LLaMA: Open and efficient foundation language\nmodels. CoRR, abs/2302.13971, 2023.\nAshish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N. Gomez, Lukasz\nKaiser, and Illia Polosukhin. Attention is all you need. In NIPS, pp. 5998–6008, 2017.\n25\n\nYubo Wang, Xueguang Ma, Ge Zhang, Yuansheng Ni, Abhranil Chandra, Shiguang Guo, Weiming\nRen, Aaran Arulraj, Xuan He, Ziyan Jiang, Tianle Li, Max Ku, Kai Wang, Alex Zhuang, Rongqi\nFan, Xiang Yue, and Wenhu Chen. MMLU-Pro: A more robust and challenging multi-task\nlanguage understanding benchmark. CoRR, abs/2406.01574, 2024.\nWenhan Xiong, Jingyu Liu, Igor Molybog, Hejia Zhang, Prajjwal Bhargava, Rui Hou, Louis Martin,\nRashi Rungta, Karthik Abinav Sankararaman, Barlas Oguz, Madian Khabsa, Han Fang, Yashar\nMehdad, Sharan Narang, Kshitiz Malik, Angela Fan, Shruti Bhosale, Sergey Edunov, Mike\nLewis, Sinong Wang, and Hao Ma. Effective long-context scaling of foundation models. CoRR,\nabs/2309.16039, 2023.\nYinfei Yang, Yuan Zhang, Chris Tar, and Jason Baldridge. PAWS-X: A cross-lingual adversarial\ndataset for paraphrase identification. In EMNLP/IJCNLP (1), pp. 3685–3690. Association for\nComputational Linguistics, 2019.\nAlex Young, Bei Chen, Chao Li, Chengen Huang, Ge Zhang, Guanwei Zhang, Heng Li, Jiangcheng\nZhu, Jianqun Chen, Jing Chang, Kaidong Yu, Peng Liu, Qiang Liu, Shawn Yue, Senbin Yang,\nShiming Yang, Tao Yu, Wen Xie, Wenhao Huang, Xiaohui Hu, Xiaoyi Ren, Xinyao Niu, Pengcheng\nNie, Yuchi Xu, Yudong Liu, Yue Wang, Yuxuan Cai, Zhenyu Gu, Zhiyuan Liu, and Zonghong Dai.\nYi: Open foundation models by 01.AI. CoRR, abs/2403.04652, 2024.\nTao Yuan, Xuefei Ning, Dong Zhou, Zhijie Yang, Shiyao Li, Minghui Zhuang, Zheyue Tan, Zhuyu\nYao, Dahua Lin, Boxun Li, Guohao Dai, Shengen Yan, and Yu Wang. LV-Eval: A balanced\nlong-context benchmark with 5 length levels up to 256K. CoRR, abs/2402.05136, 2024.\nZheng Yuan, Hongyi Yuan, Chengpeng Li, Guanting Dong, Chuanqi Tan, and Chang Zhou. Scaling re-\nlationship on learning mathematical reasoning with large language models. CoRR, abs/2308.01825,\n2023.\nRowan Zellers, Ari Holtzman, Yonatan Bisk, Ali Farhadi, and Yejin Choi. Hellaswag: Can a machine\nreally finish your sentence? In ACL (1), pp. 4791–4800. Association for Computational Linguistics,\n2019.\nAohan Zeng, Bin Xu, Bowen Wang, Chenhui Zhang, Da Yin, Diego Rojas, Guanyu Feng, Hanlin\nZhao, Hanyu Lai, Hao Yu, Hongning Wang, Jiadai Sun, Jiajie Zhang, Jiale Cheng, Jiayi Gui, Jie\nTang, Jing Zhang, Juanzi Li, Lei Zhao, Lindong Wu, Lucen Zhong, Mingdao Liu, Minlie Huang,\nPeng Zhang, Qinkai Zheng, Rui Lu, Shuaiqi Duan, Shudan Zhang, Shulin Cao, Shuxun Yang,\nWeng Lam Tam, Wenyi Zhao, Xiao Liu, Xiao Xia, Xiaohan Zhang, Xiaotao Gu, Xin Lv, Xinghan\nLiu, Xinyi Liu, Xinyue Yang, Xixuan Song, Xunkai Zhang, Yifan An, Yifan Xu, Yilin Niu, Yuantao\nYang, Yueyan Li, Yushi Bai, Yuxiao Dong, Zehan Qi, Zhaoyu Wang, Zhen Yang, Zhengxiao Du,\nZhenyu Hou, and Zihan Wang. ChatGLM: A family of large language models from GLM-130B to\nGLM-4 all tools. CoRR, abs/2406.12793, 2024.\nYingxiu Zhao, Bowen Yu, Binyuan Hui, Haiyang Yu, Minghao Li, Fei Huang, Nevin L. Zhang, and\nYongbin Li. Tree-Instruct: A preliminary study of the intrinsic relationship between complexity\nand alignment. In LREC/COLING, pp. 16776–16789. ELRA and ICCL, 2024.\nLianmin Zheng, Wei-Lin Chiang, Ying Sheng, Siyuan Zhuang, Zhanghao Wu, Yonghao Zhuang,\nZi Lin, Zhuohan Li, Dacheng Li, Eric P. Xing, Hao Zhang, Joseph E. Gonzalez, and Ion Stoica.\nJudging LLM-as-a-judge with MT-Bench and Chatbot Arena. In NeurIPS, 2023.\nJeffrey Zhou, Tianjian Lu, Swaroop Mishra, Siddhartha Brahma, Sujoy Basu, Yi Luan, Denny Zhou,\nand Le Hou. Instruction-following evaluation for large language models. CoRR, abs/2311.07911,\n2023.\n26\n"
    },
    {
      "arxiv_id": "2412.15115",
      "full_text": "2025-01-06\nQwen2.5 Technical Report\nQwen Team\nhttps://huggingface.co/Qwen\nhttps://modelscope.cn/organization/qwen\nhttps://github.com/QwenLM/Qwen2.5\nAbstract\nIn this report, we introduce Qwen2.5, a comprehensive series of large language models\n(LLMs) designed to meet diverse needs. Compared to previous iterations, Qwen 2.5 has\nbeen significantly improved during both the pre-training and post-training stages. In\nterms of pre-training, we have scaled the high-quality pre-training datasets from the\nprevious 7 trillion tokens to 18 trillion tokens. This provides a strong foundation for\ncommon sense, expert knowledge, and reasoning capabilities. In terms of post-training,\nwe implement intricate supervised finetuning with over 1 million samples, as well as\nmultistage reinforcement learning, including offline learning DPO and online learning\nGRPO. Post-training techniques significantly enhance human preference, and notably\nimprove long text generation, structural data analysis, and instruction following.\nTo handle diverse and varied use cases effectively, we present Qwen2.5 LLM series in rich\nconfigurations. The open-weight offerings include base models and instruction-tuned\nmodels in sizes of 0.5B, 1.5B, 3B, 7B, 14B, 32B, and 72B parameters. Quantized versions\nof the instruction-tuned models are also provided. Over 100 models can be accessed\nfrom Hugging Face Hub, ModelScope, and Kaggle. In addition, for hosted solutions, the\nproprietary models currently include two mixture-of-experts (MoE) variants: Qwen2.5-\nTurbo and Qwen2.5-Plus, both available from Alibaba Cloud Model Studio.\nQwen2.5 has demonstrated top-tier performance on a wide range of benchmarks eval-\nuating language understanding, reasoning, mathematics, coding, human preference\nalignment, etc. Specifically, the open-weight flagship Qwen2.5-72B-Instruct outperforms\na number of open and proprietary models and demonstrates competitive performance to\nthe state-of-the-art open-weight model, Llama-3-405B-Instruct, which is around 5 times\nlarger. Qwen2.5-Turbo and Qwen2.5-Plus offer superior cost-effectiveness while perform-\ning competitively against GPT-4o-mini and GPT-4o respectively. Additionally, as the\nfoundation, Qwen2.5 models have been instrumental in training specialized models such\nas Qwen2.5-Math (Yang et al., 2024b), Qwen2.5-Coder (Hui et al., 2024), QwQ (Qwen\nTeam, 2024d), and multimodal models.\n3T\n7T\n18T\ntokens\nMath\nMBPP\nBBH\nMMLU\nQwen1.5-72B\nQwen2-72B\nQwen2.5-72B\nFigure 1: In the iterative development of the Qwen series, data scaling has played a crucial role. Qwen 2.5,\nwhich leverages 18 trillion tokens for pre-training, has demonstrated the most advanced capabilities\nwithin the Qwen series, especially in terms of domain expertise, underscoring the importance of scale\ntogether with mixture in enhancing the model’s capabilities.\n1\narXiv:2412.15115v2  [cs.CL]  3 Jan 2025\n\n1\nIntroduction\nThe sparks of artificial general intelligence (AGI) are increasingly visible through the fast development\nof large foundation models, notably large language models (LLMs) (Brown et al., 2020; OpenAI, 2023;\n2024a; Gemini Team, 2024; Anthropic, 2023a;b; 2024; Bai et al., 2023; Yang et al., 2024a; Touvron et al.,\n2023a;b; Dubey et al., 2024). The continuous advancement in model and data scaling, combined with\nthe paradigm of large-scale pre-training followed by high-quality supervised fine-tuning (SFT) and\nreinforcement learning from human feedback (RLHF) (Ouyang et al., 2022), has enabled large language\nmodels (LLMs) to develop emergent capabilities in language understanding, generation, and reasoning.\nBuilding on this foundation, recent breakthroughs in inference time scaling, particularly demonstrated\nby o1 (OpenAI, 2024b), have enhanced LLMs’ capacity for deep thinking through step-by-step reasoning\nand reflection. These developments have elevated the potential of language models, suggesting they may\nachieve significant breakthroughs in scientific exploration as they continue to demonstrate emergent\ncapabilities indicative of more general artificial intelligence.\nBesides the fast development of model capabilities, the recent two years have witnessed a burst of open\n(open-weight) large language models in the LLM community, for example, the Llama series (Touvron\net al., 2023a;b; Dubey et al., 2024), Mistral series (Jiang et al., 2023a; 2024a), and our Qwen series (Bai\net al., 2023; Yang et al., 2024a; Qwen Team, 2024a; Hui et al., 2024; Qwen Team, 2024c; Yang et al.,\n2024b). The open-weight models have democratized the access of large language models to common\nusers and developers, enabling broader research participation, fostering innovation through community\ncollaboration, and accelerating the development of AI applications across diverse domains.\nRecently, we release the details of our latest version of the Qwen series, Qwen2.5. In terms of the open-\nweight part, we release pre-trained and instruction-tuned models of 7 sizes, including 0.5B, 1.5B, 3B, 7B,\n14B, 32B, and 72B, and we provide not only the original models in bfloat16 precision but also the quantized\nmodels in different precisions. Specifically, the flagship model Qwen2.5-72B-Instruct demonstrates\ncompetitive performance against the state-of-the-art open-weight model, Llama-3-405B-Instruct, which is\naround 5 times larger. Additionally, we also release the proprietary models of Mixture-of-Experts (MoE,\nLepikhin et al., 2020; Fedus et al., 2022; Zoph et al., 2022), namely Qwen2.5-Turbo and Qwen2.5-Plus1,\nwhich performs competitively against GPT-4o-mini and GPT-4o respectively.\nIn this technical report, we introduce Qwen2.5, the result of our continuous endeavor to create better\nLLMs. Below, we show the key features of the latest version of Qwen:\n• Better in Size: Compared with Qwen2, in addition to 0.5B, 1.5B, 7B, and 72B models, Qwen2.5\nbrings back the 3B, 14B, and 32B models, which are more cost-effective for resource-limited\nscenarios and are under-represented in the current field of open foundation models. Qwen2.5-\nTurbo and Qwen2.5-Plus offer a great balance among accuracy, latency, and cost.\n• Better in Data: The pre-training and post-training data have been improved significantly. The\npre-training data increased from 7 trillion tokens to 18 trillion tokens, with focus on knowledge,\ncoding, and mathematics. The pre-training is staged to allow transitions among different mixtures.\nThe post-training data amounts to 1 million examples, across the stage of supervised finetuning\n(SFT, Ouyang et al., 2022), direct preference optimization (DPO, Rafailov et al., 2023), and group\nrelative policy optimization (GRPO, Shao et al., 2024).\n• Better in Use: Several key limitations of Qwen2 in use have been eliminated, including larger\ngeneration length (from 2K tokens to 8K tokens), better support for structured input and output,\n(e.g., tables and JSON), and easier tool use. In addition, Qwen2.5-Turbo supports a context length\nof up to 1 million tokens.\n2\nArchitecture & Tokenizer\nBasically, the Qwen2.5 series include dense models for opensource, namely Qwen2.5-0.5B / 1.5B / 3B\n/ 7B / 14B / 32B / 72B, and MoE models for API service, namely Qwen2.5-Turbo and Qwen2.5-Plus.\nBelow, we provide details about the architecture of models.\nFor dense models, we maintain the Transformer-based decoder architecture (Vaswani et al., 2017; Radford\net al., 2018) as Qwen2 (Yang et al., 2024a). The architecture incorporates several key components:\nGrouped Query Attention (GQA, Ainslie et al., 2023) for efficient KV cache utilization, SwiGLU activation\nfunction (Dauphin et al., 2017) for non-linear activation, Rotary Positional Embeddings (RoPE, Su\n1Qwen2.5-Turbo is identified as qwen-turbo-2024-11-01 and Qwen2.5-Plus is identified as qwen-plus-2024-xx-xx\n(to be released) in the API.\n2\n\nTable 1: Model architecture and license of Qwen2.5 open-weight models.\nModels\nLayers\nHeads (Q / KV)\nTie Embedding\nContext / Generation Length\nLicense\n0.5B\n24\n14 / 2\nYes\n32K / 8K\nApache 2.0\n1.5B\n28\n12 / 2\nYes\n32K / 8K\nApache 2.0\n3B\n36\n16 / 2\nYes\n32K / 8K\nQwen Research\n7B\n28\n28 / 4\nNo\n128K / 8K\nApache 2.0\n14B\n48\n40 / 8\nNo\n128K / 8K\nApache 2.0\n32B\n64\n40 / 8\nNo\n128K / 8K\nApache 2.0\n72B\n80\n64 / 8\nNo\n128K / 8K\nQwen\net al., 2024) for encoding position information, QKV bias (Su, 2023) in the attention mechanism and\nRMSNorm (Jiang et al., 2023b) with pre-normalization to ensure stable training.\nBuilding upon the dense model architectures, we extend it to MoE model architectures. This is achieved\nby replacing standard feed-forward network (FFN) layers with specialized MoE layers, where each layer\ncomprises multiple FFN experts and a routing mechanism that dispatches tokens to the top-K experts.\nFollowing the approaches demonstrated in Qwen1.5-MoE (Yang et al., 2024a), we implement fine-grained\nexpert segmentation (Dai et al., 2024) and shared experts routing (Rajbhandari et al., 2022; Dai et al., 2024).\nThese architectural innovations have yielded substantial improvements in model performance across\ndownstream tasks.\nFor tokenization, we utilize Qwen’s tokenizer (Bai et al., 2023), which implements byte-level byte-pair\nencoding (BBPE, Brown et al., 2020; Wang et al., 2020; Sennrich et al., 2016) with a vocabulary of 151,643\nregular tokens. We have expanded the set of control tokens from 3 to 22 compared to previous Qwen\nversions, adding two new tokens for tool functionality and allocating the remainder for other model\ncapabilities. This expansion establishes a unified vocabulary across all Qwen2.5 models, enhancing\nconsistency and reducing potential compatibility issues.\n3\nPre-training\nOur language model pre-training process consists of several key components. First, we carefully curate\nhigh-quality training data through sophisticated filtering and scoring mechanisms, combined with\nstrategic data mixture. Second, we conduct extensive research on hyperparameter optimization to\neffectively train models at various scales. Finally, we incorporate specialized long-context pre-training\nto enhance the model’s ability to process and understand extended sequences. Below, we detail our\napproaches to data preparation, hyperparameter selection, and long-context training.\n3.1\nPre-training Data\nQwen2.5 demonstrates significant enhancements in pre-training data quality compared to its predecessor\nQwen2. These improvements stem from several key aspects:\n(1) Better data filtering. High-quality pre-training data is crucial for model performance, mak-\ning data quality assessment and filtering a critical component of our pipeline. We leverage\nQwen2-Instruct models as data quality filters that perform comprehensive, multi-dimensional\nanalysis to evaluate and score training samples. The filtering method represents a significant\nadvancement over our previous approach used for Qwen2, as it benefits from Qwen2’s expanded\npre-training on a larger multilingual corpus. The enhanced capabilities enable more nuanced\nquality assessment, resulting in both improved retention of high-quality training data and more\neffective filtering of low-quality samples across multiple languages.\n(2) Better math and code data. During the pre-training phase of Qwen2.5, we incorporate training\ndata from Qwen2.5-Math (Yang et al., 2024b) and Qwen2.5-Coder (Hui et al., 2024). This data\nintegration strategy proves highly effective, as these specialized datasets are instrumental in\nachieving state-of-the-art performance on mathematical and coding tasks. By leveraging these\nhigh-quality domain-specific datasets during pre-training, Qwen2.5 inherits strong capabilities\nin both mathematical reasoning and code generation.\n(3) Better synthetic data. To generate high-quality synthetic data, particularly in mathematics, code,\nand knowledge domains, we leverage both Qwen2-72B-Instruct (Yang et al., 2024a) and Qwen2-\nMath-72B-Instruct (Qwen Team, 2024c). The quality of this synthesized data is further enhanced\nthrough rigorous filtering using our proprietary general reward model and the specialized\nQwen2-Math-RM-72B (Qwen Team, 2024c) model.\n3\n\n(4) Better data mixture. To optimize the pre-training data distribution, we employ Qwen2-Instruct\nmodels to classify and balance content across different domains. Our analysis revealed that\ndomains like e-commerce, social media, and entertainment are significantly overrepresented\nin web-scale data, often containing repetitive, template-based, or machine-generated content.\nConversely, domains such as technology, science, and academic research, while containing higher-\nquality information, are traditionally underrepresented. Through strategic down-sampling of\noverrepresented domains and up-sampling of high-value domains, we ensure a more balanced\nand information-rich training dataset that better serves our model’s learning objectives.\nBuilding on these techniques, we have developed a larger and higher-quality pre-training dataset,\nexpanding from the 7 trillion tokens used in Qwen2 (Yang et al., 2024a) to 18 trillion tokens.\n3.2\nScaling Law for Hyper-parameters\nWe develop scaling laws for hyper-parameter based on the pre-training data of Qwen2.5 (Hoffmann et al.,\n2022; Kaplan et al., 2020). While previous studies (Dubey et al., 2024; Almazrouei et al., 2023; Hoffmann\net al., 2022) primarily used scaling laws to determine optimal model sizes given compute budgets, we\nleverage them to identify optimal hyperparameters across model architectures. Specifically, our scaling\nlaws help determine key training parameters like batch size B and learning rate µ for both dense models\nand MoE models of varying sizes.\nThrough extensive experimentation, we systematically study the relationship between model architecture\nand optimal training hyper-parameters. Specifically, we analyze how the optimal learning rate µopt\nand batch size Bopt vary with model size N and pre-training data size D. Our experiments cover a\ncomprehensive range of architectures, including dense models with 44M to 14B parameters and MoE\nmodels with 44M to 1B activated parameters, trained on datasets ranging from 0.8B to 600B tokens.\nUsing these optimal hyper-parameter predictions, we then model the final loss as a function of model\narchitecture and training data scale.\nAdditionally, we leverage scaling laws to predict and compare the performance of MoE models with\nvarying parameter counts against their dense counterparts. This analysis guides our hyper-parameter\nconfiguration for MoE models, enabling us to achieve performance parity with specific dense model\nvariants (such as Qwen2.5-72B and Qwen2.5-14B) through careful tuning of both activated and total\nparameters.\n3.3\nLong-context Pre-training\nFor optimal training efficiency, Qwen2.5 employs a two-phase pre-training approach: an initial phase\nwith a 4,096-token context length, followed by an extension phase for longer sequences. Following\nthe strategy used in Qwen2, we extend the context length from 4,096 to 32,768 tokens during the final\npre-training stage for all model variants except Qwen2.5-Turbo. Concurrently, we increase the base\nfrequency of RoPEfrom 10,000 to 1,000,000 using the ABF technique (Xiong et al., 2023).\nFor Qwen2.5-Turbo, we implement a progressive context length expansion strategy during training,\nadvancing through four stages: 32,768 tokens, 65,536 tokens, 131,072 tokens, and ultimately 262,144\ntokens, with a RoPE base frequency of 10,000,000. At each stage, we carefully curate the training data\nto include 40% sequences at the current maximum length and 60% shorter sequences. This progressive\ntraining methodology enables smooth adaptation to increasing context lengths while maintaining the\nmodel’s ability to effectively process and generalize across sequences of varying lengths.\nTo enhance our models’ ability to process longer sequences during inference, we implement two key\nstrategies: YARN (Peng et al., 2023) and Dual Chunk Attention (DCA, An et al., 2024). Through these\ninnovations, we achieve a four-fold increase in sequence length capacity, enabling Qwen2.5-Turbo to\nhandle up to 1 million tokens and other models to process up to 131,072 tokens. Notably, these approaches\nnot only improve the modeling of long sequences by reducing perplexity but also maintain the models’\nstrong performance on shorter sequences, ensuring consistent quality across varying input lengths.\n4\nPost-training\nQwen 2.5 introduces two significant advancements in its post-training design compared to Qwen 2:\n(1) Expanded Supervised Fine-tuning Data Coverage: The supervised fine-tuning process leverages\na massive dataset comprising millions of high-quality examples. This expansion specifically\naddresses key areas where the previous model showed limitations, such as long-sequence\n4\n\ngeneration, mathematical problem-solving, coding, instruction-following, structured data under-\nstanding, logical reasoning, cross-lingual transfer, and robust system instruction.\n(2) Two-stage Reinforcement Learning: The reinforcement learning (RL) process in Qwen 2.5 is\ndivided into two distinct stages: Offline RL and Online RL.\n• Offline RL: This stage focuses on developing capabilities that are challenging for the reward\nmodel to evaluate, such as reasoning, factuality, and instruction-following. Through meticu-\nlous construction and validation of training data, we ensure that the Offline RL signals are\nboth learnable and reliable (Xiang et al., 2024), enabling the model to acquire those complex\nskills effectively.\n• Online RL: The Online RL phase leverages the reward model’s ability to detect nuances in\noutput quality, including truthfulness, helpfulness, conciseness, relevance, harmlessness\nand debiasing. It enables the model to generate responses that are precise, coherent, and\nwell-structured while maintaining safety and readability. As a result, the model’s outputs\nconsistently meet human quality standards and expectations.\n4.1\nSupervised Fine-tuning\nIn this section, we detail the key enhancements made during the SFT phase of Qwen2.5, focusing on\nseveral critical areas:\n(1) Long-sequence Generation: Qwen2.5 is capable of generating high-quality content with an\noutput context length of up to 8,192 tokens, a significant advancement over the typical post-\ntraining response length, which often remains under 2,000 tokens. To address this gap, we\ndevelop long-response datasets (Quan et al., 2024). We employ back-translation techniques to\ngenerate queries for long-text data from pre-training corpora, impose output length constraints,\nand use Qwen2 to filter out low-quality paired data.\n(2) Mathematics: We introduce the chain-of-thought data of Qwen2.5-Math (Yang et al., 2024b),\nwhich encompasses a diverse range of query sources, including public datasets, K-12 problem\ncollections, and synthetic problems. To ensure high-quality reasoning, we employ rejection\nsampling (Yuan et al., 2023) along with reward modeling and annotated answers for guidance,\nproducing step-by-step reasoning process.\n(3) Coding: To enhance coding capabilities, we incorporate the instruction tuning data of Qwen2.5-\nCoder (Hui et al., 2024). We use multiple language-specific agents into a collaborative framework,\ngenerating diverse and high-quality instruction pairs across nearly 40 programming languages.\nWe expand our instruction dataset by synthesizing new examples from code-related Q&A\nwebsites and gathering algorithmic code snippets from GitHub. A comprehensive multilingual\nsandbox is used to perform static code checking and validate code snippets through automated\nunit testing, ensuring code quality and correctness (Dou et al., 2024; Yang et al., 2024c).\n(4) Instruction-following:\nTo ensure high-quality instruction-following data, we implement a\nrigorous code-based validation framework. In this approach, LLMs generate both instructions\nand corresponding verification code, along with comprehensive unit tests for cross-validation.\nThrough execution feedback-based rejection sampling, we carefully curate the training data used\nfor Supervised Fine-Tuning, thereby guaranteeing the model’s faithful adherence to intended\ninstructions (Dong et al., 2024).\n(5) Structured Data Understanding: We develop a comprehensive structured understanding dataset\nthat encompasses both traditional tasks, such as tabular question-answering, fact verification,\nerror correction, and structural understanding, as well as complex tasks involving structured\nand semi-structured data. By incorporating reasoning chains into the model’s responses, we\nsignificantly enhance its ability to infer information from structured data, thereby improving its\nperformance across these diverse tasks. This approach not only broadens the scope of the dataset\nbut also deepens the model’s capacity to reason and derive meaningful insights from complex\ndata structures.\n(6) Logical Reasoning: To enhance the model’s logical reasoning capabilities, we introduce a diverse\nset of 70,000 new queries spanning various domains. These queries encompass multiple-choice\nquestions, true / false questions, and open-ended questions. The model is trained to approach\nproblems systematically, employing a range of reasoning methods such as deductive reason-\ning, inductive generalization, analogical reasoning, causal reasoning, and statistical reasoning.\nThrough iterative refinement, we systematically filter out data containing incorrect answers or\nflawed reasoning processes. This process progressively strengthens the model’s ability to reason\nlogically and accurately, ensuring robust performance across different types of reasoning tasks.\n5\n\n(7) Cross-Lingual Transfer: To facilitate the transfer of the model’s general capabilities across lan-\nguages, we employ a translation model to convert instructions from high-resource languages\ninto various low-resource languages, thereby generating corresponding response candidates. To\nensure the accuracy and consistency of these responses, we evaluate the semantic alignment be-\ntween each multilingual response and its original counterpart. This process preserves the logical\nstructure and stylistic nuances of the original responses, thereby maintaining their integrity and\ncoherence across different languages.\n(8) Robust System Instruction: We construct hundreds of general system prompts to improve the\ndiversity of system prompts in post-training, ensuring consistency between system prompts and\nconversations. Evaluations with different system prompts show that the model maintains good\nperformance (Lu et al., 2024b) and reduced variance, indicating improved robustness.\n(9) Response Filtering: To evaluate the quality of responses, we employ multiple automatic an-\nnotation methods, including a dedicated critic model and a multi-agent collaborative scoring\nsystem. Responses are subjected to rigorous assessment, and only those deem flawless by all\nscoring systems are retained. This comprehensive approach ensures that our outputs maintain\nthe highest quality standards.\nUltimately, we construct a dataset of over 1 million SFT examples. The model is fine-tuned for two epochs\nwith a sequence length of 32,768 tokens. To optimize learning, the learning rate is gradually decreased\nfrom 7 × 10−6 to 7 × 10−7. To address overfitting, we apply a weight decay of 0.1, and gradient norms\nare clipped at a maximum value of 1.0.\n4.2\nOffline Reinforcement Learning\nCompared to Online Reinforcement Learning (RL), Offline RL enables the pre-preparation of training\nsignals, which is particularly advantageous for tasks where standard answers exist but are challenging to\nevaluate using reward models. In this study, we focus on objective query domains such as mathematics,\ncoding, instruction following, and logical reasoning, where obtaining accurate evaluations can be complex.\nIn the previous phase, we extensively employ strategies like execution feedback and answer matching to\nensure the quality of responses. For the current phase, we reuse that pipeline, employing the SFT model\nto resample responses for a new set of queries. Responses that pass our quality checks are used as positive\nexamples, while those that fail are treated as negative examples for Direct Preference Optimization (DPO)\ntraining (Rafailov et al., 2023). To further enhance the reliability and accuracy of the training signals, we\nmake use of both human and automated review processes (Cao et al., 2024). This dual approach ensures\nthat the training data is not only learnable but also aligned with human expectations. Ultimately, we\nconstruct a dataset consisting of approximately 150,000 training pairs. The model is then trained for one\nepoch using the Online Merging Optimizer (Lu et al., 2024a), with a learning rate of 7 × 10−7.\n4.3\nOnline Reinforcement Learning\nTo develop a robust reward model for online RL, we adhere to a set of carefully defined labeling criteria.\nThose criteria ensure that the responses generated by the model are not only high-quality but also aligned\nwith ethical and user-centric standards (Wang et al., 2024a). The specific guidelines for data labeling are\nas follows:\n• Truthfulness: Responses must be grounded in factual accuracy, faithfully reflecting the pro-\nvided context and instructions. The model should avoid generating information that is false or\nunsupported by the given data.\n• Helpfulness: The model’s output should be genuinely useful, addressing the user’s query\neffectively while providing content that is positive, engaging, educational, and relevant. It\nshould follow the given instructions precisely and offer value to the user.\n• Conciseness: Responses should be succinct and to the point, avoiding unnecessary verbosity.\nThe goal is to convey information clearly and efficiently without overwhelming the user with\nexcessive detail.\n• Relevance: All parts of the response should be directly related to the user’s query, dialogue\nhistory, and the assistant’s context. The model should tailor its output to ensure it is perfectly\naligned with the user’s needs and expectations.\n• Harmlessness: The model must prioritize user safety by avoiding any content that could lead\nto illegal, immoral, or harmful behavior. It should promote ethical conduct and responsible\ncommunication at all times.\n6\n\n• Debiasing: The model should produce responses that are free from bias, including but not\nlimited to gender, race, nationality, and politics. It should treat all topics equally and fairly,\nadhering to widely accepted moral and ethical standards.\nThe queries utilized to train the reward model are drawn from two distinct datasets: publicly available\nopen-source data and a proprietary query set characterized by higher complexity. Responses are gener-\nated from checkpoints of the Qwen models, which have been fine-tuned using different methods—SFT,\nDPO, and RL—at various stages of training. To introduce diversity, those responses are sampled at\ndifferent temperature settings. Preference pairs are created through both human and automated labeling\nprocesses, and the training data for DPO is also integrated into this dataset.\nIn our online reinforcement learning (RL) framework, we employ Group Relative Policy Optimization\n(GRPO, Shao et al., 2024). The query set utilized for training the reward model is identical to the one used\nin the RL training phase. The sequence in which queries are processed during training is determined by\nthe variance of their response scores, as evaluated by the reward model. Specifically, queries with higher\nvariance in response scores are prioritized to ensure more effective learning. We sample 8 responses\nfor each query. All models are trained with a 2048 global batch size and 2048 samples in each episode,\nconsidering a pair of queries and responses as a sample.\n4.4\nLong Context Fine-tuning\nTo further extend the context length of Qwen2.5-Turbo, we introduce longer SFT examples during\npost-training, enabling it to better align with human preference in long queries.\nIn the SFT phase, we employ a two-stage approach. In the first stage, the model is fine-tuned exclusively\nusing short instructions, each containing up to 32,768 tokens. This stage uses the same data and training\nsteps as those employed for the other Qwen2.5 models, ensuring strong performance on short tasks.\nIn the second stage, the fine-tuning process combines both short instructions (up to 32,768 tokens)\nand long instructions (up to 262,144 tokens). This hybrid approach effectively enhances the model’s\ninstruction-following ability in long context tasks while maintaining its performance on short tasks.\nDuring the RL stage, we use a training strategy similar to that used for the other Qwen2.5 models,\nfocusing solely on short instructions. This design choice is driven by two primary considerations: first,\nRL training is computationally expensive for long context tasks; second, there is currently a scarcity of\nreward models that provide suitable reward signals for long context tasks. Additionally, we find that\nadopting RL on short instructions alone can still significantly enhance the model’s alignment with human\npreferences in long context tasks.\n5\nEvaluation\nThe base models produced by pre-training and the instruction-tuned models produced by post-training\nare evaluated accordingly with a comprehensive evaluation suite, including both commonly-used open\nbenchmarks and skill-oriented in-house datasets. The evaluation suite is designed to be primarily\nautomatic with minimal human interaction.\nTo prevent test data leakage, we exclude potentially contaminated data using n-gram matching when\nconstructing the pre-training and post-training datasets. Following the criteria used in Qwen2, a training\nsequence st is removed from the training data if there exists a test sequence se such that the length of the\nlongest common subsequence (LCS) between tokenized st and se satisfies both |LCS(st, se)| ≥13 and\n|LCS(st, se)| ≥0.6 × min(|st|, |se|).\n5.1\nBase Models\nWe conduct comprehensive evaluations of the base language models of the Qwen2.5 series. The evaluation\nof base models primarily emphasizes their performance in natural language understanding, general\nquestion answering, coding, mathematics, scientific knowledge, reasoning, and multilingual capabilities.\nThe evaluation datasets include:\nGeneral Tasks\nMMLU (Hendrycks et al., 2021a) (5-shot), MMLU-Pro (Wang et al., 2024b) (5-shot),\nMMLU-redux (Gema et al., 2024) (5-shot), BBH (Suzgun et al., 2023) (3-shot), ARC-C (Clark et al.,\n2018) (25-shot), TruthfulQA (Lin et al., 2022a) (0-shot), Winogrande (Sakaguchi et al., 2021) (5-shot),\nHellaSwag (Zellers et al., 2019) (10-shot).\n7\n\nTable 2: Performance of the 70B+ base models and Qwen2.5-Plus.\nDatasets\nLlama-3-70B Mixtral-8x22B Llama-3-405B Qwen2-72B Qwen2.5-72B Qwen2.5-Plus\nGeneral Tasks\nMMLU\n79.5\n77.8\n85.2\n84.2\n86.1\n85.4\nMMLU-Pro\n52.8\n51.6\n61.6\n55.7\n58.1\n64.0\nMMLU-redux\n75.0\n72.9\n-\n80.5\n83.9\n82.8\nBBH\n81.0\n78.9\n85.9\n82.4\n86.3\n85.8\nARC-C\n68.8\n70.7\n-\n68.9\n72.4\n70.9\nTruthfulQA\n45.6\n51.0\n-\n54.8\n60.4\n55.3\nWindoGrande\n85.3\n85.0\n86.7\n85.1\n83.9\n85.5\nHellaSwag\n88.0\n88.7\n-\n87.3\n87.6\n89.2\nMathematics & Science Tasks\nGPQA\n36.3\n34.3\n-\n37.4\n45.9\n43.9\nTheoremQA\n32.3\n35.9\n-\n42.8\n42.4\n48.5\nMATH\n42.5\n41.7\n53.8\n50.9\n62.1\n64.4\nMMLU-stem\n73.7\n71.7\n-\n79.6\n82.7\n81.2\nGSM8K\n77.6\n83.7\n89.0\n89.0\n91.5\n93.0\nCoding Tasks\nHumanEval\n48.2\n46.3\n61.0\n64.6\n59.1\n59.1\nHumanEval+\n42.1\n40.2\n-\n56.1\n51.2\n52.4\nMBPP\n70.4\n71.7\n73.0\n76.9\n84.7\n79.7\nMBPP+\n58.4\n58.1\n-\n63.9\n69.2\n66.9\nMultiPL-E\n46.3\n46.7\n-\n59.6\n60.5\n61.0\nMultilingual Tasks\nMulti-Exam\n70.0\n63.5\n-\n76.6\n78.7\n78.5\nMulti-Understanding\n79.9\n77.7\n-\n80.7\n89.6\n89.2\nMulti-Mathematics\n67.1\n62.9\n-\n76.0\n76.7\n82.4\nMulti-Translation\n38.0\n23.3\n-\n37.8\n39.0\n40.4\nMathematics & Science Tasks\nGPQA (Rein et al., 2023) (5-shot), Theorem QA (Chen et al., 2023a)\n(5-shot), GSM8K (Cobbe et al., 2021) (4-shot), MATH (Hendrycks et al., 2021b) (4-shot).\nCoding Tasks\nHumanEval (Chen et al., 2021) (0-shot), HumanEval+ (Liu et al., 2023)(0-shot), MBPP (Austin\net al., 2021) (0-shot), MBPP+ (Liu et al., 2023) (0-shot), MultiPL-E (Cassano et al., 2023) (0-shot) (Python,\nC++, JAVA, PHP, TypeScript, C#, Bash, JavaScript).\nMultilingual Tasks\nWe group them into four categories: (a) Exam: M3Exam (5-shot, we only choose\nexamples that require no image), IndoMMLU (Koto et al., 2023) (3-shot), ruMMLU (Fenogenova et al.,\n2024) (5-shot), and translated MMLU (Chen et al., 2023b) (5-shot on Arabic, Spanish, French, Portuguese,\nGerman, Italian, Japanese, and Korean); (b) Understanding: BELEBELE (Bandarkar et al., 2023) (5-shot),\nXCOPA (Ponti et al., 2020) (5-shot), XWinograd (Muennighoff et al., 2023) (5-shot), XStoryCloze (Lin\net al., 2022b) (0-shot) and PAWS-X (Yang et al., 2019) (5-shot); (c) Mathematics: MGSM (Goyal et al., 2022)\n(8-shot CoT); and (d) Translation: Flores-101 (Goyal et al., 2022) (5-shot).\nFor base models, we compare Qwen2.5 models with Qwen2 models and other leading open-weight\nmodels in terms of scales of parameters.\nQwen2.5-72B & Qwen2.5-Plus\nWe compare the base models of Qwen2.5-72B and Qwen2.5-Plus to other\nleading open-weight base models: Llama3-70B (Dubey et al., 2024), Llama3-405B (Dubey et al., 2024),\nMixtrail-8x22B (Jiang et al., 2024a), and our previous 72B version, the Qwen2-72B (Yang et al., 2024a).\nThe Qwen2.5-72B base model significantly outperforms its peers in the same category across a wide\nrange of tasks. It achieves results comparable to Llama-3-405B while utilizing only one-fifth of the pa-\nrameters. Furthermore, when compared to its predecessor, Qwen2-72B, the Qwen2.5-72B shows marked\nimprovements in nearly all benchmark evaluations, particularly excelling in general tasks, mathematics,\nand coding challenges. With significantly lower training and inference costs, Qwen2.5-Plus achieves\nvery competitive performance results compared to Qwen2.5-72B and Llama3-405B, outperforming other\nbaseline models on the Hellaswag, TheoremQA, MATH, GSM8K, MultiPL-E, Multi-Mathematics, and\nMulti-Translation. Moreover, Qwen2.5-Plus achieves 64.0 on MMLU-Pro, which is 5.9 points higher than\nQwen2.5-72B.\nQwen2.5-14B/32B & Qwen2.5-Turbo\nThe evaluation of the Qwen2.5-Turbo, Qwen2.5-14B, and 32B\nmodels is compared against baselines of similar sizes. These baselines include Yi-1.5-34B (Young et al.,\n8\n\nTable 3: Performance of the 14B-30B+ base models and Qwen2.5-Turbo.\nDatasets\nQwen1.5-32B Gemma2-27B Yi-1.5-34B Qwen2.5-Turbo Qwen2.5-14B Qwen2.5-32B\nGeneral Tasks\nMMLU\n74.3\n75.2\n77.2\n79.5\n79.7\n83.3\nMMLU-pro\n44.1\n49.1\n48.3\n55.6\n51.2\n55.1\nMMLU-redux\n69.0\n-\n74.1\n77.1\n76.6\n82.0\nBBH\n66.8\n74.9\n76.4\n76.1\n78.2\n84.5\nARC-C\n63.6\n71.4\n65.6\n67.8\n67.3\n70.4\nTruthfulQA\n57.4\n40.1\n53.9\n56.3\n58.4\n57.8\nWinogrande\n81.5\n59.7\n84.9\n81.1\n81.0\n82.0\nHellaswag\n85.0\n86.4\n85.9\n85.0\n84.3\n85.2\nMathematics & Science Tasks\nGPQA\n30.8\n34.9\n37.4\n41.4\n32.8\n48.0\nTheoremqa\n28.8\n35.8\n40.0\n42.1\n43.0\n44.1\nMATH\n36.1\n42.7\n41.7\n55.6\n55.6\n57.7\nMMLU-stem\n66.5\n71.0\n72.6\n77.0\n76.4\n80.9\nGSM8K\n78.5\n81.1\n81.7\n88.3\n90.2\n92.9\nCoding Tasks\nHumanEval\n43.3\n54.9\n46.3\n57.3\n56.7\n58.5\nHumanEval+\n40.2\n46.3\n40.2\n51.2\n51.2\n52.4\nMBPP\n64.2\n75.7\n65.5\n76.2\n76.7\n84.5\nMBPP+\n53.9\n60.2\n55.4\n63.0\n63.2\n67.2\nMultiPL-E\n38.5\n48.0\n39.5\n53.9\n53.5\n59.4\nMultilingual Tasks\nMulti-Exam\n61.6\n65.8\n58.3\n70.3\n70.6\n75.4\nMulti-Understanding\n76.5\n82.2\n73.9\n85.3\n85.9\n88.4\nMulti-Mathematics\n56.1\n61.6\n49.3\n71.3\n68.5\n73.7\nMulti-Translation\n33.5\n38.7\n30.0\n36.8\n36.2\n37.3\nTable 4: Performance of the 7B+ base models.\nDatasets\nMistral-7B\nLlama3-8B\nGemma2-9B\nQwen2-7B\nQwen2.5-7B\nGeneral Tasks\nMMLU\n64.2\n66.6\n71.3\n70.3\n74.2\nMMLU-pro\n30.9\n35.4\n44.7\n40.1\n45.0\nMMLU-redux\n58.1\n61.6\n67.9\n68.1\n71.1\nBBH\n56.1\n57.7\n68.2\n62.3\n70.4\nARC-C\n60.0\n59.3\n68.2\n60.6\n63.7\nTruthfulQA\n42.2\n44.0\n45.3\n54.2\n56.4\nWinogrande\n78.4\n77.4\n79.5\n77.0\n75.9\nHellaSwag\n83.3\n82.1\n81.9\n80.7\n80.2\nMathematics & Science Tasks\nGPQA\n24.7\n25.8\n32.8\n30.8\n36.4\nTheoremQA\n19.2\n22.1\n28.9\n29.6\n36.0\nMATH\n10.2\n20.5\n37.7\n43.5\n49.8\nMMLU-stem\n50.1\n55.3\n65.1\n64.2\n72.3\nGSM8K\n36.2\n55.3\n70.7\n80.2\n85.4\nCoding Tasks\nHumanEval\n29.3\n33.5\n37.8\n51.2\n57.9\nHumanEval+\n24.4\n29.3\n30.5\n43.3\n50.6\nMBPP\n51.1\n53.9\n62.2\n64.2\n74.9\nMBPP+\n40.9\n44.4\n50.6\n51.9\n62.9\nMultiPL-E\n29.4\n22.6\n34.9\n41.0\n50.3\nMultilingual Tasks\nMulti-Exam\n47.1\n52.3\n61.2\n59.2\n59.4\nMulti-Understanding\n63.3\n68.6\n78.3\n72.0\n79.3\nMulti-Mathematics\n26.3\n36.3\n53.0\n57.5\n57.8\nMulti-Translation\n23.3\n31.9\n36.5\n31.5\n32.4\n9\n\nTable 5: Performance of the smaller base models.\nDatasets\nQwen2-0.5B Qwen2.5-0.5B Qwen2-1.5B Qwen2.5-1.5B Gemma2-2.6B Qwen2.5-3B\nGeneral Tasks\nMMLU\n44.3\n47.5\n55.9\n60.9\n52.2\n65.6\nMMLU-pro\n14.7\n15.7\n21.6\n28.5\n23.0\n34.6\nMMLU-redux\n40.7\n45.1\n51.8\n58.5\n50.9\n63.7\nBBH\n18.2\n20.3\n36.5\n45.1\n41.9\n56.3\nARC-C\n31.0\n35.6\n43.7\n54.7\n55.7\n56.5\nTruthfulQA\n39.7\n40.2\n45.9\n46.6\n36.2\n48.9\nWinogrande\n56.9\n56.3\n65.0\n65.0\n71.5\n71.1\nHellaswag\n49.1\n52.1\n67.0\n67.9\n74.6\n74.6\nMathematics & Science Tasks\nGPQA\n29.8\n24.8\n20.7\n24.2\n25.3\n26.3\nTheoremQA\n9.6\n16.0\n14.8\n22.1\n15.9\n27.4\nMATH\n11.2\n19.5\n21.6\n35.0\n18.3\n42.6\nMMLU-STEM\n27.5\n39.8\n42.7\n54.8\n45.8\n62.5\nGSM8K\n36.4\n41.6\n46.9\n68.5\n30.3\n79.1\nCoding Tasks\nHumanEval\n22.6\n30.5\n34.8\n37.2\n19.5\n42.1\nHumanEval+\n18.9\n26.8\n29.9\n32.9\n15.9\n36.0\nMBPP\n33.1\n39.3\n46.9\n60.2\n42.1\n57.1\nMBPP+\n27.6\n33.8\n37.6\n49.6\n33.6\n49.4\nMultiPL-E\n16.3\n18.9\n27.9\n33.1\n17.6\n41.2\nMultilingual Tasks\nMulti-Exam\n29.4\n30.8\n43.1\n47.9\n38.1\n54.6\nMulti-Understanding\n40.4\n41.0\n50.7\n65.1\n46.8\n76.6\nMulti-Mathematics\n7.8\n13.5\n21.3\n37.5\n18.2\n48.9\nMulti-Translation\n14.1\n15.3\n23.8\n25.0\n26.9\n29.3\n2024), Gemma2-27B (Gemma Team et al., 2024), and Qwen1.5-32B (Qwen Team, 2024b). The results\nare shown in Table 3. The Qwen2.5-14B model demonstrates a solid performance across various tasks,\nparticularly excelling in general tasks like MMLU and BBH, where it achieves scores of 79.7 and 78.2,\noutcompeting competitors of larger sizes. Meanwhile, Qwen2.5-32B, in particular, showcases exceptional\ncapabilities, often surpassing larger models of similar model sizes. Notably, it outperforms its predecessor\nQwen1.5-32B significantly, especially in challenging areas such as mathematics and coding, with notable\nscores of 57.7 in MATH and 84.5 in MBPP. For Qwen2.5-Turbo, although its training cost and inference cost\nare significantly smaller than those of Qwen2.5-14B, it achieves comparable results, where its MMLU-Pro\nscore is even better than that of Qwen2.5-32B.\nQwen2.5-7B\nFor 7B-level models, we focus on comparing Qwen2.5-7B with other leading 7B+ models,\nincluding Mistral-7B (Jiang et al., 2023a), Llama3-8B (Dubey et al., 2024), Gemma2-9B (Gemma Team et al.,\n2024), and our predecessor, Qwen2-7B (Yang et al., 2024a). The results can be found in Table 4. Note that\nthe non-embedding parameters of Qwen2-7B and Qwen2.5-7B are only 6.5B, while that of Gemma2-9B\nis 8.2B. The Qwen2.5-7B model surpasses its predecessors and counterparts in numerous benchmarks,\ndespite having fewer non-embedding parameters. It demonstrates significant improvements across\nvarious tasks, achieving 74.2 on general benchmarks like MMLU (Hendrycks et al., 2021a), 49.8 on math\nchallenges such as MATH (Hendrycks et al., 2021b), and 57.9 on coding tasks like HumanEval (Chen\net al., 2021).\nQwen2.5-0.5B/1.5B/3B\nFor edge-side models, we compare Qwen2.5-0.5B, 1.5B, and 3B against estab-\nlished baselines: Qwen2-0.5B/1.5B (Yang et al., 2024a) and Gemma2-2.6B (Gemma Team et al., 2024). The\nresults are given in Table 5. Qwen2.5-0.5B, 1.5B, and 3B continue to maintain strong performance across\nnearly all benchmarks. Notably, the Qwen2.5-0.5B model outperforms the Gemma2-2.6B on various math\nand coding tasks.\n5.2\nInstruction-tuned Model\nTo critically evaluate instruction-tuned models, we adopt a multifaceted approach. Foundational skills\nand human preferences are assessed using open datasets and benchmarks. Additionally, our detailed\nin-house evaluations delve deeper into the models’competencies in key areas and multilingualism. A\nparticular focus is placed on assessing long-context capability. The subsequent sections outline the\nevaluation methods and present the results.\n10\n\nTable 6: Performance of the 70B+ Instruct models and Qwen2.5-Plus.\nDatasets\nLlama-3.1-70B Llama-3.1-405B Qwen2-72B Qwen2.5-72B Qwen2.5-Plus\nGeneral Tasks\nMMLU-Pro\n66.4\n73.3\n64.4\n71.1\n72.5\nMMLU-redux\n83.0\n86.2\n81.6\n86.8\n86.3\nLiveBench 0831\n46.6\n53.2\n41.5\n52.3\n54.6\nMathematics & Science Tasks\nGPQA\n46.7\n51.1\n42.4\n49.0\n49.7\nMATH\n68.0\n73.8\n69.0\n83.1\n84.7\nGSM8K\n95.1\n96.8\n93.2\n95.8\n96.0\nCoding Tasks\nHumanEval\n80.5\n89.0\n86.0\n86.6\n87.8\nMBPP\n84.2\n84.5\n80.2\n88.2\n85.5\nMultiPL-E\n68.2\n73.5\n69.2\n75.1\n77.0\nLiveCodeBench\n32.1\n41.6\n32.2\n55.5\n51.4\nAlignment Tasks\nIFEval\n83.6\n86.0\n77.6\n84.1\n86.3\nArena-Hard\n55.7\n69.3\n48.1\n81.2\n81.4\nMTbench\n8.79\n9.08\n9.12\n9.35\n9.30\nTable 7: Performance of the 14B-30B+ instruction-tuned models and Qwen2.5-Turbo.\nDatasets\nQwen2-57BA14B Gemma2-27B GPT4o-mini Qwen2.5-Turbo Qwen2.5-14B Qwen2.5-32B\nGeneral Tasks\nMMLU-Pro\n52.8\n55.5\n63.1\n64.5\n63.7\n69.0\nMMLU-redux\n72.6\n75.7\n81.5\n81.7\n80.0\n83.9\nLiveBench 0831\n31.1\n39.6\n43.3\n42.3\n44.4\n50.7\nMathematics & Science Tasks\nGPQA\n34.3\n38.4\n40.2\n42.3\n45.5\n49.5\nMATH\n49.1\n54.4\n70.2\n81.1\n80.0\n83.1\nGSM8K\n85.3\n90.4\n93.2\n93.8\n94.8\n95.9\nCoding Tasks\nHumanEval\n79.9\n78.7\n88.4\n86.6\n83.5\n88.4\nMBPP\n70.9\n81.0\n85.7\n82.8\n82.0\n84.0\nMultiPL-E\n66.4\n67.4\n75.0\n73.7\n72.8\n75.4\nLiveCodeBench\n22.5\n-\n40.7\n37.8\n42.6\n51.2\nAlignment Tasks\nIFEval\n59.9\n77.1\n80.4\n76.3\n81.0\n79.5\nArena-Hard\n17.8\n57.5\n74.9\n67.1\n68.3\n74.5\nMTbench\n8.55\n9.10\n-\n8.81\n8.88\n9.20\n5.2.1\nOpen Benchmark Evaluation\nTo comprehensively evaluate the quality of instruction-tuned models, we compile automatic and human\nevaluation to assess the capabilities and human preference. For the evaluation of basic capabilities, we\napply similar datasets in the pre-trained model evaluation, which target on natural language understand-\ning, coding, mathematics, and reasoning. Specifically, we evaluate on MMLU-Pro, MMLU-redux and\nLiveBench 0831 (White et al., 2024) for general evaluation, GPQA, GSM8K and MATH for science and\nmathematics, HumanEval, MBPP, MultiPL-E and LiveCodeBench 2305-2409 (Jain et al., 2024) for coding,\nIFEval (Zhou et al., 2023)2 for instruction following. Additionally, we assess the performance of human\npreference alignment and instruction following by evaluating on benchmarks including MT-Bench (Zheng\net al., 2023) and Arena-Hard (Li et al., 2024).\nQwen2.5-72B-Instruct & Qwen2.5-Plus\nAs shown in Table 6, we compare Qwen2.5-72B-Instruct and\nQwen2.5-Plus to other leading open-weight instrution-tuned models: Llama3.1-70B-Instruct (Dubey\n2For simplicity, we report the results of the subset strict-prompt.\n11\n\nTable 8: Performance of the 7B+ instruction-tuned models.\nDatasets\nGemma2-9B Llama3.1-8B Qwen2-7B Qwen2.5-7B\nGeneral Tasks\nMMLU-Pro\n52.1\n48.3\n44.1\n56.3\nMMLU-redux\n72.8\n67.2\n67.3\n75.4\nLiveBench 0831\n30.6\n26.7\n29.2\n35.9\nMathematics & Science Tasks\nGPQA\n32.8\n32.8\n34.3\n36.4\nMATH\n44.3\n51.9\n52.9\n75.5\nGSM8K\n76.7\n84.5\n85.7\n91.6\nCoding Tasks\nHumanEval\n68.9\n72.6\n79.9\n84.8\nMBPP\n74.9\n69.6\n67.2\n79.2\nMultiPL-E\n53.4\n50.7\n59.1\n70.4\nLiveCodeBench\n18.9\n8.3\n23.9\n28.7\nAlignment Tasks\nIFEval\n70.1\n75.9\n54.7\n71.2\nArena-Hard\n41.6\n27.8\n25.0\n52.0\nMTbench\n8.49\n8.23\n8.26\n8.75\nTable 9: Performance comparison of 2B-4B instruction-tuned models.\nDatasets\nGemma2-2B Phi3.5-Mini MiniCPM3-4B Qwen2.5-3B\nNon-Emb Params\n2.0B\n3.6B\n4.0B\n2.8B\nGeneral Tasks\nMMLU-Pro\n26.7\n47.5\n43.0\n43.7\nMMLU-redux\n51.9\n67.7\n59.9\n64.4\nLiveBench 0831\n20.1\n27.4\n27.6\n26.8\nMathematics & Science Tasks\nGPQA\n29.3\n27.2\n31.3\n30.3\nMATH\n26.6\n48.5\n46.6\n65.9\nGSM8K\n63.2\n86.2\n81.1\n86.7\nCoding Tasks\nHumanEval\n68.9\n72.6\n74.4\n74.4\nMBPP\n74.9\n63.2\n72.5\n72.7\nMultiPL-E\n30.5\n47.2\n49.1\n60.2\nLiveCodeBench\n5.8\n15.8\n23.8\n19.9\nAlignment Tasks\nIFEval\n51.0\n52.1\n68.4\n58.2\net al., 2024), Llama3.1-405B-Instruct (Dubey et al., 2024), and our previous 72B version, Qwen2-72B-\nInstruct (Yang et al., 2024a). The Qwen2.5-72B-Instruct model delivers exceptional performance, even\nsurpassing the larger Llama-3.1-405B-Instruct in several critical benchmarks including MMLU-redux,\nMATH, MBPP, MultiPL-E, LiveCodeBench, Arena-Hard and MTBench. Moreover, Qwen2.5-Plus outper-\nforms Qwen2.5-72B-Instruct on 9 out of 13 benchmarks.\nQwen2.5-14B/32B-Instruct & Qwen2.5-Turbo\nThe performance of the Qwen2.5-Turbo, Qwen2.5-14B-\nInstruct, and Qwen2.5-32B-Instruct models is evaluated and compared against baselines of similar sizes.\nThe baselines include GPT4o-mini, Gemma2-27B-IT (Gemma Team et al., 2024), and Qwen2-57BA14B-\nInstruct (Yang et al., 2024a). The results are summarized in Table 7. The Qwen2.5-32B-Instruct model\nexhibits superior performance across most tasks when compared to other models of similar size. Notably,\nour open-weight Qwen2.5-14B-Instruct model delivers competitive results across all benchmarks, rivaling\nthose of GPT-4o-mini. Despite its significantly lower training and inference costs, the Qwen2.5-Turbo\nmodel outperforms Qwen2.5-14B-Instruct on eight out of ten benchmarks. This demonstrates that\nQwen2.5-Turbo achieves remarkable efficiency and effectiveness, making it a compelling choice for\nresource-constrained environments.\n12\n\nTable 10: Performance comparison of 0.5B-1.5B instruction-tuned models.\nDatasets\nQwen2-0.5B Qwen2.5-0.5B Qwen2-1.5B Qwen2.5-1.5B\nGeneral Tasks\nMMLU-Pro\n14.4\n15.0\n22.9\n32.4\nMMLU-redux\n12.9\n24.1\n41.2\n50.7\nLiveBench\n7.4\n12.6\n12.4\n18.8\nMathematics & Science Tasks\nGPQA\n23.7\n29.8\n21.2\n29.8\nMATH\n13.9\n34.4\n25.3\n55.2\nGSM8K\n40.1\n49.6\n61.6\n73.2\nCoding Tasks\nHumanEval\n31.1\n35.4\n42.1\n61.6\nMBPP\n39.7\n49.6\n44.2\n63.2\nMultiPL-E\n20.8\n28.5\n38.5\n50.4\nLiveCodeBench\n1.6\n5.1\n4.5\n14.8\nAlignment Tasks\nIFEval\n14.6\n27.9\n29.0\n42.5\nTable 11: Performance Comparison on our in-house English automatic evaluation benchmark.\nModels\nIF\nKnowledge\nComprehension\nCoding\nMath\nReasoning\nProprietary LLMs\nGPT-4o-2024-08-06\n83.28\n68.08\n76.51\n58.05\n52.36\n66.45\nGPT-4o-2024-11-20\n80.06\n65.25\n79.07\n60.19\n49.74\n67.07\nClaude3.5-sonnet-2024-10-22\n84.22\n74.61\n79.02\n67.17\n48.67\n70.20\nQwen2 Series\nQwen2-0.5B-Instruct\n18.33\n18.59\n30.64\n5.42\n13.16\n32.03\nQwen2-1.5B-Instruct\n29.42\n29.23\n45.81\n17.02\n20.34\n38.86\nQwen2-7B-Instruct\n50.47\n44.79\n58.04\n43.04\n38.31\n50.25\nQwen2-72B-Instruct\n76.08\n59.49\n72.19\n48.95\n48.07\n60.33\nLlama-3.1 Series\nLlama-3.1-70B-Instruct\n81.33\n63.42\n69.29\n55.96\n48.00\n63.18\nLlama-3.1-405B-Instruct\n83.33\n67.10\n75.55\n58.14\n47.09\n64.74\nQwen2.5 Series\nQwen2.5-0.5B-Instruct\n33.35\n30.29\n29.78\n15.41\n26.29\n36.13\nQwen2.5-1.5B-Instruct\n40.25\n41.19\n47.69\n26.19\n40.99\n42.23\nQwen2.5-3B-Instruct\n60.60\n46.11\n57.98\n41.43\n49.38\n49.80\nQwen2.5-7B-Instruct\n70.01\n52.74\n62.69\n48.41\n56.93\n54.69\nQwen2.5-14B-Instruct\n74.17\n59.78\n69.11\n52.68\n59.68\n62.51\nQwen2.5-Turbo\n72.76\n58.56\n68.70\n54.48\n57.77\n61.06\nQwen2.5-32B-Instruct\n76.79\n64.08\n71.28\n58.90\n60.97\n65.49\nQwen2.5-72B-Instruct\n82.65\n66.09\n74.43\n60.41\n59.73\n65.90\nQwen2.5-Plus\n83.18\n68.41\n79.35\n59.58\n62.52\n66.92\nOther Instruction-tuned Models\nAs illustrated in Table 8, the Qwen2.5-7B-Instruct model significantly\noutperforms its competitors, Gemma2-9B-IT and Llama3.1-8B-Instruct, across all tasks except IFEval.\nNotably, Qwen2.5-7B-Instruct exhibits clear advantages in mathematics (MATH: 75.5) and coding (Hu-\nmanEval: 84.8). For the edge-side instruction models, the Qwen2.5-3B-Instruct model, despite having\nfewer parameters than both the Phi3.5-mini-instruct (Abdin et al., 2024) and MiniCPM3-4B-Instruct (Hu\net al., 2024) models, surpasses them in mathematics and coding tasks, as shown in Table 9. Additionally,\nit delivers competitive results in language understanding. The Qwen2.5-1.5B-Instruct and Qwen2.5-0.5B-\nInstruct models have also seen substantial performance improvements over their previous versions, as\ndetailed in Table 10. These enhancements make them particularly well-suited for edge-side applications\nin highly resource-constrained environments.\n13\n\nTable 12: Performance Comparison on our in-house Chinese automatic evaluation benchmark.\nModels\nIF\nKnowledge\nComprehension\nCoding\nMath\nReasoning\nProprietary LLMs\nGPT-4o-2024-08-06\n42.50\n68.55\n80.11\n61.53\n61.74\n56.88\nGPT-4o-2024-11-20\n42.71\n71.29\n83.04\n62.39\n66.04\n62.04\nClaude3.5-sonnet-2024-10-22\n49.25\n72.09\n82.16\n66.00\n63.71\n66.60\nQwen2 Series\nQwen2-0.5B-Instruct\n4.69\n40.43\n39.13\n9.85\n14.07\n32.73\nQwen2-1.5B-Instruct\n6.81\n51.54\n46.89\n14.14\n24.57\n35.19\nQwen2-7B-Instruct\n16.83\n65.95\n60.30\n37.05\n50.52\n44.96\nQwen2-72B-Instruct\n31.98\n74.96\n75.49\n41.57\n65.55\n58.19\nLlama-3.1 Series\nLlama-3.1-70B-Instruct\n28.96\n57.41\n67.24\n54.82\n41.18\n52.42\nLlama-3.1-405B-Instruct\n30.39\n63.79\n72.27\n60.73\n46.05\n55.88\nQwen2.5 Series\nQwen2.5-0.5B-Instruct\n6.12\n39.13\n42.97\n9.60\n24.03\n33.72\nQwen2.5-1.5B-Instruct\n7.38\n48.68\n49.69\n22.96\n37.30\n39.17\nQwen2.5-3B-Instruct\n16.50\n57.18\n62.55\n29.88\n51.64\n39.57\nQwen2.5-7B-Instruct\n26.64\n65.77\n67.55\n39.56\n61.06\n49.70\nQwen2.5-14B-Instruct\n26.87\n70.28\n76.96\n49.78\n67.01\n56.41\nQwen2.5-Turbo\n32.94\n72.93\n74.37\n51.92\n66.08\n53.30\nQwen2.5-32B-Instruct\n32.64\n74.70\n79.46\n54.45\n67.86\n60.19\nQwen2.5-72B-Instruct\n37.22\n75.86\n78.85\n56.71\n68.39\n63.02\nQwen2.5-Plus\n46.15\n72.07\n82.64\n58.48\n69.96\n62.98\n5.2.2\nIn-house Automatic Evaluation\nDespite the availability of several open benchmark datasets for evaluation, we believe that these are\ninsufficient to fully capture the capabilities of LLMs. To address this, we have developed a series\nof in-house datasets designed to assess various aspects of model performance, including knowledge\nunderstanding, text generation, coding, and more. These evaluations are conducted in both Chinese\nand English. In addition, we have specifically evaluated the multilingual performance of instruction-\ntuned models. The results are summarized in Table 11 for English, Table 12 for Chinese, Table 13 for\nmultilingualism of 70B+ Instruct models, and Table 14 for 7B-14B models, respectively.\nEnglish & Chinese Evaluation\nWe compare the performance of Qwen2.5-Instruct models against\nseveral leading language models, including GPT-4, Claude3.5-sonnet, Qwen2, and Llama-3.1, across both\nEnglish and Chinese languages. Our analysis focuses on model size and its impact on performance, as\nwell as how our latest Qwen2.5 series compares to previous iterations and competing models. For smaller\nmodels, we observe that the Qwen2.5-0.5B model achieves performance that is on par with or even\nsurpasses the Qwen2-1.5B model. This indicates that the Qwen2.5 series has optimized parameter usage,\nenabling mid-sized models to achieve similar performance levels to larger models from the previous\ngeneration. The Qwen2.5-3B model demonstrates performance that is comparable to the Qwen2-7B\nmodel. Notably, the Qwen2.5-32B model exhibits a remarkable improvement over the Qwen2-72B model.\nOur flagship model, Qwen2.5-72B, further narrows the gap between Qwen and state-of-the-art models\nlike GPT-4 and Claude3.5-sonnet. In particular, Qwen2.5-72B matches or exceeds the performance\nof Llama-3.1-405B in all metrics except for instruction following. This achievement underscores the\ncompetitiveness of Qwen2.5-72B in a wide range of language processing tasks, while also identifying\nareas for future improvement. Qwen2.5-Plus addresses the previous shortcomings in Chinese instruction\nfollowing and further enhances its advantages in other areas.\nMultilingual Evaluation\nTo comprehensively evaluate the multilingual capabilities of instruction-tuned\nmodels, we followed P-MMEval (Zhang et al., 2024) and extended several benchmarks as follows: (1)\nIFEval (Multilingual): We expanded the IFEval benchmark, originally in English, to include multilingual\nexamples. To ensure language neutrality, we removed instances that contained language-specific content\n(e.g., ”start with letter A”). (2) Knowledge Utilization: to assess the knowledge utilization abilities\nof the Qwen2.5 series models across multiple languages, we employed five MMLU-like benchmarks\n(multiple-choice format). These benchmarks include: AMMLU (Arabic), JMMLU (Japanese), KMMLU\n(Korean), IndoMMLU (Indonesian), and TurkishMMLU (Turkish). Additionally, we evaluated the models’\nperformance on the translated version of the MMLU benchmark (okapi MMLU), which has been adapted\n14\n\nTable 13: Performance of the 70B+ Instruct models on Multilingual Tasks.\nDatasets\nQwen2-72B Llama3.1-70B Qwen2.5-32B Mistral-Large GPT4o-mini Qwen2.5-72B\nInstruction Following\nIFEval (multilingual)\n79.69\n80.47\n82.68\n82.69\n85.03\n86.98\nKnowledge\nAMMLU (Arabic)\n68.85\n70.08\n70.44\n69.24\n69.73\n72.44\nJMMLU (Japanese)\n77.37\n73.89\n76.55\n75.77\n73.74\n80.56\nKMMLU (Korean)\n57.04\n53.23\n60.75\n56.42\n56.77\n61.96\nIndoMMLU (Indonesian)\n66.31\n67.50\n66.42\n63.21\n67.75\n69.25\nTurkishMMLU (Turkish)\n69.22\n66.89\n72.41\n64.78\n71.19\n76.12\nokapi MMLU (translated)\n77.84\n76.49\n77.16\n78.37\n73.44\n79.97\nMath Reasoning\nMGSM8K (extended)\n82.72\n73.31\n87.15\n89.01\n87.36\n88.16\nCultural Nuances\nBLEnD\n25.90\n30.49\n27.88\n33.47\n35.91\n32.48\nTable 14: Performance of the 7B-14B Instruct models on Multilingual Tasks.\nDatasets\nQwen2-7B Llama3.1-8B Qwen2.5-7B Gemma2-9B Qwen2.5-14B\nInstruction Following\nIFEval (multilingual)\n51.43\n60.68\n74.87\n77.47\n77.08\nKnowledge\nAMMLU (Arabic)\n54.87\n54.28\n59.78\n60.26\n66.81\nJMMLU (Japanese)\n57.71\n53.26\n61.88\n64.59\n72.78\nKMMLU (Korean)\n43.96\n42.28\n46.59\n46.24\n59.71\nIndoMMLU (Indonesian)\n54.05\n53.92\n56.42\n61.73\n65.09\nTurkishMMLU (Turkish)\n49.27\n45.61\n54.28\n55.44\n66.85\nokapi MMLU (translated)\n60.47\n55.18\n66.98\n46.72\n72.12\nMath Reasoning\nMGSM8K (extended)\n56.13\n66.05\n66.11\n78.37\n82.27\nCultural Nuances\nBLEnD\n22.49\n19.47\n23.66\n28.31\n26.99\ninto multiple languages from its original English form. (3) MGSM8K (Extended): Building upon the\noriginal MGSM8K benchmark, we extended the language support to include Arabic (ar), Korean (ko),\nPortuguese (pt), and Vietnamese (vi). (4) Cultural Nuances: To evaluate the models’ ability to capture\ncultural nuances, we utilized the BLEnD benchmark (Myung et al., 2024). This benchmark is specifically\ndesigned to test LLMs on their understanding of cultural subtleties.\nQwen2.5 exhibits competitive performance in instruction following, multilingual knowledge, and mathe-\nmatical reasoning, aligning well with models of comparable size. Although it shows notable improve-\nments in capturing cultural nuances relative to its predecessor, Qwen2, there remains potential for further\nrefinement in this domain.\n5.2.3\nReward Model\nThe reward model serves as the cornerstone for guiding RL processes, and thus we conduct a separate\nevaluation of the reward model used in the Qwen2.5 series. Our assessment benchmarks encompass\nReward Bench (Lambert et al., 2024), RMB (Zhou et al., 2024), PPE (Frick et al., 2024b), and an internally\ncollected out-of-domain Chinese human preference benchmark (Human-Preference-Chinese) to provide\na comprehensive analysis. For comparison, we included baseline models such as Nemotron-4-340B-\nReward (Adler et al., 2024), Llama-3.1-Nemotron-70B-Reward (Wang et al., 2024c), and Athene-RM-\n70B (Frick et al., 2024a). The results are shown in Table 15. Overall, our findings indicate that Llama-3.1-\nNemotron-70B-Reward excels on the Reward Bench, while Athene-RM-70B performs best on the RMB\nbenchmark. The Qwen2.5-RM-72B, leads in both the PPE and Human-Preference-Chinese evaluations,\nranking second only to Athene-RM-70B on the RMB and achieving a performance level comparable to\n15\n\nTable 15: Performance comparison across multiple RM benchmarks.\nMetric\nNemotron-4-340B-\nReward\nLlama-3.1-Nemotron-\n70B-Reward\nAthene-RM\n-70B\nQwen2.5-RM\n-72B\nReward Bench\nChat\n95.80\n97.50\n98.32\n97.21\nChat Hard\n87.10\n85.70\n70.61\n78.73\nSafety\n91.50\n95.10\n92.10\n92.71\nReasoning\n93.60\n98.10\n92.19\n97.65\nScore\n92.00\n94.10\n88.32\n91.59\nRMB\nHelpfulness (BoN)\n48.85\n61.02\n67.24\n65.72\nHelpfulness (Pairwise)\n68.70\n75.28\n80.82\n78.83\nHarmlessness (BoN)\n50.92\n52.00\n67.02\n56.35\nHarmlessness (Pairwise)\n70.84\n69.96\n80.83\n73.94\nOverall\n59.83\n64.57\n73.98\n68.71\nPPE\nHuman Preference\n59.28\n64.32\n66.48\n64.80\nIFEval\n62.66\n63.40\n62.15\n67.97\nGPQA\n56.56\n59.14\n59.26\n59.80\nMATH\n65.12\n69.73\n79.14\n81.48\nMBPP-Plus\n49.15\n55.62\n67.97\n64.34\nMMLU-Pro\n69.69\n70.20\n76.95\n75.66\nObjective-Avg\n60.64\n63.62\n69.09\n69.85\nHuman-Preference-Chinese\nAccuracy\n50.46\n59.95\n61.11\n61.27\nNemotron-4-340B-Reward on the Reward Bench, albeit slightly behind Llama-3.1-Nemotron-70B-Reward.\nDue to the lack of evaluation methods for reward models, current reward models are typically evaluated\nusing Reward Bench. However, our evaluation results from multiple RM benchmarks suggest that over-\noptimization on a specific benchmark may trigger Goodhart’s law (Hoskin, 1996), resulting in degraded\nperformance on other benchmarks and potentially impacting downstream alignment performance. This\nhighlights the need for comprehensive evaluation of reward models across diverse benchmarks rather\nthan relying solely on a single benchmark.\nMore importantly, through iterative experimentation, we have also come to recognize a critical limitation:\ncurrent reward model evaluation benchmarks do not accurately predict the performance of the RL models\ntrained under their guidance. In other words, a higher score on RM benchmarks does not necessarily\ncorrelate with superior performance of the resulting RL model. This insight underscores the need for\nfurther research into more predictive evaluation methods for reward models.\n5.2.4\nLong Context Capabilities\nWe utilize three benchmarks to evaluate long context capabilities of Qwen2.5 models: RULER (Hsieh\net al., 2024), LV-Eval (Yuan et al., 2024), and Longbench-Chat (Bai et al., 2024). In LV-Eval, we adopt\nkeyword recall as the reported score to mitigate the high rate of false negatives present in the original\nmetrics.\nThe results are shown in Table 16 and Table 17. We can observe that the Qwen2.5 models, after equipping\nlength extrapolation techniques (i.e., DCA + YARN), have demonstrated strong long context processing\ncapabilities on the three datasets. Among them, Qwen2.5-72B-Instruct has shown the strongest perfor-\nmance across all context lengths, significantly outperforming existing open-weight long-context models\nas well as the proprietary models like GPT-4o-mini and GPT-4.\nFurthermore, as shown in Figure 2, Qwen2.5-Turbo achieves 100% accuracy in the 1M-token passkey\nretrieval task, demonstrating its exceptional ability to capture detailed information from ultra-long\ncontexts. We develop a sparse attention mechanism based on Minference (Jiang et al., 2024b) to signifi-\ncantly enhance inference speed, which is critical for user experience when processing long contexts. For\nsequences of 1M tokens, this approach reduces the computational load of the attention mechanism by\n12.5 times. Figure 3 illustrates the time to first token (TTFT) of Qwen2.5-Turbo across various hardware\nconfigurations, where our method achieves a 3.2 to 4.3 times speedup.\n16\n\nTable 16: Performance of Qwen2.5 Models on RULER. YARN+DCA does not change the model behavior\nwithin 32K tokens.\nModel\nClaimed\nLength\nRULER\nAvg.\n4K\n8K\n16K\n32K\n64K\n128K\nGLM4-9b-Chat-1M\n1M\n89.9\n94.7\n92.8\n92.1\n89.9\n86.7\n83.1\nLlama-3-8B-Instruct-Gradient-1048k\n1M\n88.3\n95.5\n93.8\n91.6\n87.4\n84.7\n77.0\nLlama-3.1-70B-Instruct\n128K\n89.6\n96.5\n95.8\n95.4\n94.8\n88.4\n66.6\nGPT-4o-mini\n128K\n87.3\n95.0\n92.9\n92.7\n90.2\n87.6\n65.8\nGPT-4\n128K\n91.6\n96.6\n96.3\n95.2\n93.2\n87.0\n81.2\nQwen2.5-7B-Instruct\n128K\n85.4\n96.7\n95.1\n93.7\n89.4\n82.3\n55.1\nw/o DCA + YARN\n80.1\n96.7\n95.1\n93.7\n89.4\n74.5\n31.4\nQwen2.5-14B-Instruct\n128K\n91.4\n97.7\n96.8\n95.9\n93.4\n86.7\n78.1\nw/o DCA + YARN\n86.5\n97.7\n96.8\n95.9\n93.4\n82.3\n53.0\nQwen2.5-32B-Instruct\n128K\n92.9\n96.9\n97.1\n95.5\n95.5\n90.3\n82.0\nw/o DCA + YARN\n88.0\n96.9\n97.1\n95.5\n95.5\n85.3\n57.7\nQwen2.5-72B-Instruct\n128K\n95.1\n97.7\n97.2\n97.7\n96.5\n93.0\n88.4\nw/o DCA + YARN\n90.8\n97.7\n97.2\n97.7\n96.5\n88.5\n67.0\nQwen2.5-Turbo\n1M\n93.1\n97.5\n95.7\n95.5\n94.8\n90.8\n84.5\nTable 17: Performance of Qwen2.5 Models on LV-Eval and LongBench-Chat. YARN+DCA does not\nchange the model behavior within 32k tokens.\nModel\nClaimed\nLength\nLV-Eval\nLongBench-\nChat\n16k\n32k\n64k\n128k\n256k\nGLM4-9B-Chat-1M\n1M\n46.4\n43.2\n42.9\n40.4\n37.0\n7.82\nLlama-3-8B-Instruct-Gradient-1048k\n1M\n31.7\n31.8\n28.8\n26.3\n21.1\n6.20\nLlama-3.1-70B-Instruct\n128k\n48.6\n47.4\n42.9\n26.2\nN/A\n6.80\nGPT-4o-mini\n128k\n52.9\n48.1\n46.0\n40.7\nN/A\n8.48\nQwen2.5-7B-Instruct\n128k\n55.9\n49.7\n48.0\n41.1\n36.9\n7.42\nw/o DCA + YARN\n55.9\n49.7\n33.1\n13.6\n0.5\n-\nQwen2.5-14B-Instruct\n128k\n53.0\n50.8\n46.8\n43.6\n39.4\n8.04\nw/o DCA + YARN\n53.0\n50.8\n37.0\n18.4\n0.8\n-\nQwen2.5-32B-Instruct\n128k\n56.0\n53.6\n48.8\n45.3\n41.0\n8.70\nw/o DCA + YARN\n56.0\n53.6\n40.1\n20.5\n0.7\n-\nQwen2.5-72B-Instruct\n128k\n60.4\n57.5\n53.9\n50.9\n45.2\n8.72\nw/o DCA + YARN\n60.4\n57.5\n47.4\n27.0\n2.4\n-\nQwen2.5-Turbo\n1M\n53.4\n50.0\n45.4\n43.9\n38.0\n8.34\nContext Length (# Tokens)\nDocument \nDepth\nTop of\nDocument\nBottom of\nDocument\nTesting Qwen2.5-Turbo via “Passkey Retrieval”\nRetrieve Hidden Number from Irrelevant Sentences across Context Lengths and Document Depth\n100%\nAccuracy of \nRetrieval\n50%\nAccuracy of \nRetrieval\n0%\nAccuracy of \nRetrieval\nFigure 2: Performance of Qwen2.5-Turbo on Passkey Retrieval Task with 1M Token Lengths.\n17\n\n4.3x\n3.1x\n1.7x\n3.2x\n2.4x\n1.4x\n5.6x\n4.1x\n2.3x\n4x\n2.9x\n1.7x\nFigure 3: TTFT (Time To First Token) of Qwen2.5-Turbo and Qwen2.5-7B with Full Attention and Our\nMethod.\n6\nConclusion\nQwen2.5 represents a significant advancement in large language models (LLMs), with enhanced pre-\ntraining on 18 trillion tokens and sophisticated post-training techniques, including supervised fine-tuning\nand multi-stage reinforcement learning. These improvements boost human preference alignment, long\ntext generation, and structural data analysis, making Qwen2.5 highly effective for instruction-following\ntasks. Available in various configurations, Qwen2.5 offers both open-weight from 0.5B to 72B parameters\nand proprietary models including cost-effective MoE variants like Qwen2.5-Turbo and Qwen2.5-Plus.\nEmpirical evaluations show that Qwen2.5-72B-Instruct matches the performance of the state-of-the-art\nLlama-3-405B-Instruct, despite being six times smaller. Qwen2.5 also serves as a foundation for specialized\nmodels, demonstrating its versatility for domain-specific applications. We believe that Qwen2.5’s robust\nperformance, flexible architecture, and broad availability make it a valuable resource for both academic\nresearch and industrial applications, positioning it as a key player of future innovations.\nIn the future, we will focus on advancing robust foundational models. First, we will iteratively refine both\nbase and instruction-tuned large language models (LLMs) by incorporating broader, more diverse, higher-\nquality data. Second, we will also continue to develop multimodal models. Our goal is to integrate\nvarious modalities into a unified framework. This will facilitate seamless, end-to-end information\nprocessing across textual, visual, and auditory domains. Third, we are committed to enhancing the\nreasoning capabilities of our models. This will be achieved through strategic scaling of inference compute\nresources. These efforts aim to push the boundaries of current technological limitations and contribute to\nthe broader field of artificial intelligence.\n7\nAuthors\nCore Contributors: An Yang, Baosong Yang, Beichen Zhang, Binyuan Hui, Bo Zheng, Bowen Yu,\nChengyuan Li, Dayiheng Liu, Fei Huang, Haoran Wei, Huan Lin, Jian Yang, Jianhong Tu, Jianwei Zhang,\nJianxin Yang, Jiaxi Yang, Jingren Zhou, Junyang Lin, Kai Dang, Keming Lu, Keqin Bao, Kexin Yang, Le\nYu, Mei Li, Mingfeng Xue, Pei Zhang, Qin Zhu, Rui Men, Runji Lin, Tianhao Li, Tianyi Tang, Tingyu Xia,\n18\n\nXingzhang Ren, Xuancheng Ren, Yang Fan, Yang Su, Yichang Zhang, Yu Wan, Yuqiong Liu, Zeyu Cui,\nZhenru Zhang, Zihan Qiu\nContributors: Biao Sun, Bin Luo, Bin Zhang, Binghai Wang, Chaojie Yang, Chang Si, Cheng Chen,\nChengpeng Li, Chujie Zheng, Fan Hong, Guanting Dong, Guobin Zhao, Hangrui Hu, Hanyu Zhao, Hao\nLin, Hao Xiang, Haoyan Huang, Humen Zhong, Jialin Wang, Jialong Tang, Jiandong Jiang, Jianqiang\nWan, Jianxin Ma, Jianyuan Zeng, Jie Zhang, Jin Xu, Jinkai Wang, Jinzheng He, Jun Tang, Ke Yi, Keqin\nChen, Langshi Chen, Le Jiang, Lei Zhang, Liang Chen, Man Yuan, Mingkun Yang, Minmin Sun, Na Ni,\nNuo Chen, Peng Wang, Peng Zhu, Pengcheng Zhang, Pengfei Wang, Qiaoyu Tang, Qing Fu, Rong Zhang,\nRu Peng, Ruize Gao, Shanghaoran Quan, Shen Huang, Shuai Bai, Shuang Luo, Sibo Song, Song Chen,\nTao He, Ting He, Wei Ding, Wei Liao, Weijia Xu, Wenbin Ge, Wenbiao Yin, Wenyuan Yu, Xianyan Jia,\nXianzhong Shi, Xiaodong Deng, Xiaoming Huang, Ximing Zhou, Xinyu Wang, Xipin Wei, Xuejing Liu,\nYang Liu, Yang Yao, Yang Zhang, Yibo Miao, Yidan Zhang, Yikai Zhu, Yinger Zhang, Yong Jiang, Yong Li,\nYongan Yue, Yuanzhi Zhu, Yunfei Chu, Zekun Wang, Zhaohai Li, Zheren Fu, Zhi Li, Zhibo Yang, Zhifang\nGuo, Zhipeng Zhang, Zhiying Xu, Zile Qiao, Ziye Meng\nReferences\nMarah I Abdin, Sam Ade Jacobs, Ammar Ahmad Awan, Jyoti Aneja, Ahmed Awadallah, Hany Awadalla,\nNguyen Bach, Amit Bahree, Arash Bakhtiari, Harkirat S. Behl, Alon Benhaim, Misha Bilenko, Johan\nBjorck, S´ebastien Bubeck, Martin Cai, Caio C´esar Teodoro Mendes, Weizhu Chen, Vishrav Chaudhary,\nParul Chopra, Allie Del Giorno, Gustavo de Rosa, Matthew Dixon, Ronen Eldan, Dan Iter, Amit\nGarg, Abhishek Goswami, Suriya Gunasekar, Emman Haider, Junheng Hao, Russell J. Hewett, Jamie\nHuynh, Mojan Javaheripi, Xin Jin, Piero Kauffmann, Nikos Karampatziakis, Dongwoo Kim, Mahoud\nKhademi, Lev Kurilenko, James R. Lee, Yin Tat Lee, Yuanzhi Li, Chen Liang, Weishung Liu, Eric\nLin, Zeqi Lin, Piyush Madan, Arindam Mitra, Hardik Modi, Anh Nguyen, Brandon Norick, Barun\nPatra, Daniel Perez-Becker, Thomas Portet, Reid Pryzant, Heyang Qin, Marko Radmilac, Corby Rosset,\nSambudha Roy, Olatunji Ruwase, Olli Saarikivi, Amin Saied, Adil Salim, Michael Santacroce, Shital\nShah, Ning Shang, Hiteshi Sharma, Xia Song, Masahiro Tanaka, Xin Wang, Rachel Ward, Guanhua\nWang, Philipp Witte, Michael Wyatt, Can Xu, Jiahang Xu, Sonali Yadav, Fan Yang, Ziyi Yang, Donghan\nYu, Chengruidong Zhang, Cyril Zhang, Jianwen Zhang, Li Lyna Zhang, Yi Zhang, Yue Zhang, Yunan\nZhang, and Xiren Zhou. Phi-3 technical report: A highly capable language model locally on your\nphone. CoRR, abs/2404.14219, 2024.\nBo Adler, Niket Agarwal, Ashwath Aithal, Dong H. Anh, Pallab Bhattacharya, Annika Brundyn, Jared\nCasper, Bryan Catanzaro, Sharon Clay, Jonathan Cohen, Sirshak Das, Ayush Dattagupta, Olivier\nDelalleau, Leon Derczynski, Yi Dong, Daniel Egert, Ellie Evans, Aleksander Ficek, Denys Fridman,\nShaona Ghosh, Boris Ginsburg, Igor Gitman, Tomasz Grzegorzek, Robert Hero, Jining Huang, Vibhu\nJawa, Joseph Jennings, Aastha Jhunjhunwala, John Kamalu, Sadaf Khan, Oleksii Kuchaiev, Patrick\nLeGresley, Hui Li, Jiwei Liu, Zihan Liu, Eileen Peters Long, Ameya Mahabaleshwarkar, Somshubra\nMajumdar, James Maki, Miguel Martinez, Maer Rodrigues de Melo, Ivan Moshkov, Deepak Narayanan,\nSean Narenthiran, Jesus Navarro, Phong Nguyen, Osvald Nitski, Vahid Noroozi, Guruprasad Nutheti,\nChristopher Parisien, Jupinder Parmar, Mostofa Patwary, Krzysztof Pawelec, Wei Ping, Shrimai\nPrabhumoye, Rajarshi Roy, Trisha Saar, Vasanth Rao Naik Sabavat, Sanjeev Satheesh, Jane Polak\nScowcroft, Jason D. Sewall, Pavel Shamis, Gerald Shen, Mohammad Shoeybi, Dave Sizer, Misha\nSmelyanskiy, Felipe Soares, Makesh Narsimhan Sreedhar, Dan Su, Sandeep Subramanian, Shengyang\nSun, Shubham Toshniwal, Hao Wang, Zhilin Wang, Jiaxuan You, Jiaqi Zeng, Jimmy Zhang, Jing Zhang,\nVivienne Zhang, Yian Zhang, and Chen Zhu. Nemotron-4 340B technical report. CoRR, abs/2406.11704,\n2024.\nJoshua Ainslie, James Lee-Thorp, Michiel de Jong, Yury Zemlyanskiy, Federico Lebr´on, and Sumit\nSanghai. GQA: Training generalized multi-query Transformer models from multi-head checkpoints. In\nEMNLP, pp. 4895–4901. Association for Computational Linguistics, 2023.\nEbtesam Almazrouei, Hamza Alobeidli, Abdulaziz Alshamsi, Alessandro Cappelli, Ruxandra Cojocaru,\nM´erouane Debbah, ´Etienne Goffinet, Daniel Hesslow, Julien Launay, Quentin Malartic, Daniele Maz-\nzotta, Badreddine Noune, Baptiste Pannier, and Guilherme Penedo. The Falcon series of open language\nmodels. CoRR, abs/2311.16867, 2023.\nChenxin An, Fei Huang, Jun Zhang, Shansan Gong, Xipeng Qiu, Chang Zhou, and Lingpeng Kong.\nTraining-free long-context scaling of large language models. CoRR, abs/2402.17463, 2024.\nAnthropic. Introducing Claude, 2023a. URL https://www.anthropic.com/index/introducing-claude.\nAnthropic. Claude 2. Technical report, Anthropic, 2023b. URL https://www-files.anthropic.com/pro\nduction/images/Model-Card-Claude-2.pdf.\n19\n\nAnthropic. The Claude 3 model family: Opus, Sonnet, Haiku. Technical report, Anthropic, AI, 2024. URL\nhttps://www-cdn.anthropic.com/de8ba9b01c9ab7cbabf5c33b80b7bbc618857627/Model Card Claud\ne 3.pdf.\nJacob Austin, Augustus Odena, Maxwell I. Nye, Maarten Bosma, Henryk Michalewski, David Dohan,\nEllen Jiang, Carrie J. Cai, Michael Terry, Quoc V. Le, and Charles Sutton. Program synthesis with large\nlanguage models. CoRR, abs/2108.07732, 2021.\nJinze Bai, Shuai Bai, Yunfei Chu, Zeyu Cui, Kai Dang, Xiaodong Deng, Yang Fan, Wenbin Ge, Yu Han, Fei\nHuang, Binyuan Hui, Luo Ji, Mei Li, Junyang Lin, Runji Lin, Dayiheng Liu, Gao Liu, Chengqiang Lu,\nKeming Lu, Jianxin Ma, Rui Men, Xingzhang Ren, Xuancheng Ren, Chuanqi Tan, Sinan Tan, Jianhong\nTu, Peng Wang, Shijie Wang, Wei Wang, Shengguang Wu, Benfeng Xu, Jin Xu, An Yang, Hao Yang,\nJian Yang, Shusheng Yang, Yang Yao, Bowen Yu, Hongyi Yuan, Zheng Yuan, Jianwei Zhang, Xingxuan\nZhang, Yichang Zhang, Zhenru Zhang, Chang Zhou, Jingren Zhou, Xiaohuan Zhou, and Tianhang\nZhu. Qwen technical report. CoRR, abs/2309.16609, 2023.\nYushi Bai, Xin Lv, Jiajie Zhang, Yuze He, Ji Qi, Lei Hou, Jie Tang, Yuxiao Dong, and Juanzi Li. LongAlign:\nA recipe for long context alignment of large language models. In EMNLP (Findings), pp. 1376–1395.\nAssociation for Computational Linguistics, 2024.\nLucas Bandarkar, Davis Liang, Benjamin Muller, Mikel Artetxe, Satya Narayan Shukla, Donald Husa,\nNaman Goyal, Abhinandan Krishnan, Luke Zettlemoyer, and Madian Khabsa. The Belebele benchmark:\nA parallel reading comprehension dataset in 122 language variants. CoRR, abs/2308.16884, 2023.\nTom B. Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared Kaplan, Prafulla Dhariwal, Arvind\nNeelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, Sandhini Agarwal, Ariel Herbert-Voss,\nGretchen Krueger, Tom Henighan, Rewon Child, Aditya Ramesh, Daniel M. Ziegler, Jeffrey Wu,\nClemens Winter, Christopher Hesse, Mark Chen, Eric Sigler, Mateusz Litwin, Scott Gray, Benjamin\nChess, Jack Clark, Christopher Berner, Sam McCandlish, Alec Radford, Ilya Sutskever, and Dario\nAmodei. Language models are few-shot learners. In NeurIPS, 2020.\nBoxi Cao, Keming Lu, Xinyu Lu, Jiawei Chen, Mengjie Ren, Hao Xiang, Peilin Liu, Yaojie Lu, Ben He,\nXianpei Han, Le Sun, Hongyu Lin, and Bowen Yu. Towards scalable automated alignment of LLMs: A\nsurvey. CoRR, abs/2406.01252, 2024.\nFederico Cassano, John Gouwar, Daniel Nguyen, Sydney Nguyen, Luna Phipps-Costin, Donald Pinckney,\nMing-Ho Yee, Yangtian Zi, Carolyn Jane Anderson, Molly Q. Feldman, Arjun Guha, Michael Greenberg,\nand Abhinav Jangda. MultiPL-E: A scalable and polyglot approach to benchmarking neural code\ngeneration. IEEE Trans. Software Eng., 49(7):3675–3691, 2023.\nMark Chen, Jerry Tworek, Heewoo Jun, Qiming Yuan, Henrique Pond´e de Oliveira Pinto, Jared Kaplan,\nHarrison Edwards, Yuri Burda, Nicholas Joseph, Greg Brockman, Alex Ray, Raul Puri, Gretchen\nKrueger, Michael Petrov, Heidy Khlaaf, Girish Sastry, Pamela Mishkin, Brooke Chan, Scott Gray, Nick\nRyder, Mikhail Pavlov, Alethea Power, Lukasz Kaiser, Mohammad Bavarian, Clemens Winter, Philippe\nTillet, Felipe Petroski Such, Dave Cummings, Matthias Plappert, Fotios Chantzis, Elizabeth Barnes,\nAriel Herbert-Voss, William Hebgen Guss, Alex Nichol, Alex Paino, Nikolas Tezak, Jie Tang, Igor\nBabuschkin, Suchir Balaji, Shantanu Jain, William Saunders, Christopher Hesse, Andrew N. Carr,\nJan Leike, Joshua Achiam, Vedant Misra, Evan Morikawa, Alec Radford, Matthew Knight, Miles\nBrundage, Mira Murati, Katie Mayer, Peter Welinder, Bob McGrew, Dario Amodei, Sam McCandlish,\nIlya Sutskever, and Wojciech Zaremba. Evaluating large language models trained on code. CoRR,\nabs/2107.03374, 2021.\nWenhu Chen, Ming Yin, Max Ku, Pan Lu, Yixin Wan, Xueguang Ma, Jianyu Xu, Xinyi Wang, and Tony Xia.\nTheoremQA: A theorem-driven question answering dataset. In EMNLP, pp. 7889–7901. Association for\nComputational Linguistics, 2023a.\nZhihong Chen, Shuo Yan, Juhao Liang, Feng Jiang, Xiangbo Wu, Fei Yu, Guiming Hardy Chen, Junying\nChen, Hongbo Zhang, Li Jianquan, Wan Xiang, and Benyou Wang. MultilingualSIFT: Multilingual\nsupervised instruction fine-tuning, 2023b. URL https://github.com/FreedomIntelligence/Multili\nngualSIFT.\nPeter Clark, Isaac Cowhey, Oren Etzioni, Tushar Khot, Ashish Sabharwal, Carissa Schoenick, and Oyvind\nTafjord. Think you have solved question answering? Try ARC, the AI2 reasoning challenge. CoRR,\nabs/1803.05457, 2018.\nKarl Cobbe, Vineet Kosaraju, Mohammad Bavarian, Mark Chen, Heewoo Jun, Lukasz Kaiser, Matthias\nPlappert, Jerry Tworek, Jacob Hilton, Reiichiro Nakano, Christopher Hesse, and John Schulman.\nTraining verifiers to solve math word problems. CoRR, abs/2110.14168, 2021.\n20\n\nDamai Dai, Chengqi Deng, Chenggang Zhao, R. X. Xu, Huazuo Gao, Deli Chen, Jiashi Li, Wangding\nZeng, Xingkai Yu, Y. Wu, Zhenda Xie, Y. K. Li, Panpan Huang, Fuli Luo, Chong Ruan, Zhifang Sui, and\nWenfeng Liang. DeepSeekMoE: Towards ultimate expert specialization in mixture-of-experts language\nmodels. CoRR, abs/2401.06066, 2024.\nYann N. Dauphin, Angela Fan, Michael Auli, and David Grangier. Language modeling with gated\nconvolutional networks. In ICML, volume 70 of Proceedings of Machine Learning Research, pp. 933–941.\nPMLR, 2017.\nGuanting Dong, Keming Lu, Chengpeng Li, Tingyu Xia, Bowen Yu, Chang Zhou, and Jingren Zhou.\nSelf-play with execution feedback: Improving instruction-following capabilities of large language\nmodels. CoRR, abs/2406.13542, 2024.\nShihan Dou, Jiazheng Zhang, Jianxiang Zang, Yunbo Tao, Haoxiang Jia, Shichun Liu, Yuming Yang,\nShenxi Wu, Shaoqing Zhang, Muling Wu, et al. Multi-programming language sandbox for llms. CoRR,\nabs/2410.23074, 2024.\nAbhimanyu Dubey, Abhinav Jauhri, Abhinav Pandey, Abhishek Kadian, Ahmad Al-Dahle, Aiesha\nLetman, Akhil Mathur, Alan Schelten, Amy Yang, Angela Fan, Anirudh Goyal, Anthony Hartshorn,\nAobo Yang, Archi Mitra, Archie Sravankumar, Artem Korenev, Arthur Hinsvark, Arun Rao, Aston\nZhang, Aur´elien Rodriguez, Austen Gregerson, Ava Spataru, Baptiste Rozi`ere, Bethany Biron, Binh\nTang, Bobbie Chern, Charlotte Caucheteux, Chaya Nayak, Chloe Bi, Chris Marra, Chris McConnell,\nChristian Keller, Christophe Touret, Chunyang Wu, Corinne Wong, Cristian Canton Ferrer, Cyrus\nNikolaidis, Damien Allonsius, Daniel Song, Danielle Pintz, Danny Livshits, David Esiobu, Dhruv\nChoudhary, Dhruv Mahajan, Diego Garcia-Olano, Diego Perino, Dieuwke Hupkes, Egor Lakomkin,\nEhab AlBadawy, Elina Lobanova, Emily Dinan, Eric Michael Smith, Filip Radenovic, Frank Zhang,\nGabriel Synnaeve, Gabrielle Lee, Georgia Lewis Anderson, Graeme Nail, Gr´egoire Mialon, Guan\nPang, Guillem Cucurell, Hailey Nguyen, Hannah Korevaar, Hu Xu, Hugo Touvron, Iliyan Zarov,\nImanol Arrieta Ibarra, Isabel M. Kloumann, Ishan Misra, Ivan Evtimov, Jade Copet, Jaewon Lee, Jan\nGeffert, Jana Vranes, Jason Park, Jay Mahadeokar, Jeet Shah, Jelmer van der Linde, Jennifer Billock,\nJenny Hong, Jenya Lee, Jeremy Fu, Jianfeng Chi, Jianyu Huang, Jiawen Liu, Jie Wang, Jiecao Yu,\nJoanna Bitton, Joe Spisak, Jongsoo Park, Joseph Rocca, Joshua Johnstun, Joshua Saxe, Junteng Jia,\nKalyan Vasuden Alwala, Kartikeya Upasani, Kate Plawiak, Ke Li, Kenneth Heafield, Kevin Stone, and\net al. The Llama 3 herd of models. CoRR, abs/2407.21783, 2024.\nWilliam Fedus, Barret Zoph, and Noam Shazeer. Switch transformers: Scaling to trillion parameter\nmodels with simple and efficient sparsity. J. Mach. Learn. Res., 23:120:1–120:39, 2022.\nAlena Fenogenova, Artem Chervyakov, Nikita Martynov, Anastasia Kozlova, Maria Tikhonova, Albina\nAkhmetgareeva, Anton A. Emelyanov, Denis Shevelev, Pavel Lebedev, Leonid Sinev, Ulyana Isaeva, Ka-\nterina Kolomeytseva, Daniil Moskovskiy, Elizaveta Goncharova, Nikita Savushkin, Polina Mikhailova,\nDenis Dimitrov, Alexander Panchenko, and Sergey Markov. MERA: A comprehensive LLM evaluation\nin russian. CoRR, abs/2401.04531, 2024.\nEvan Frick, Peter Jin, Tianle Li, Karthik Ganesan, Jian Zhang, Jiantao Jiao, and Banghua Zhu. Athene-70b:\nRedefining the boundaries of post-training for open models, July 2024a. URL https://nexusflow.ai/b\nlogs/athene.\nEvan Frick, Tianle Li, Connor Chen, Wei-Lin Chiang, Anastasios Nikolas Angelopoulos, Jiantao Jiao,\nBanghua Zhu, Joseph E. Gonzalez, and Ion Stoica. How to evaluate reward models for RLHF. CoRR,\nabs/2410.14872, 2024b.\nAryo Pradipta Gema, Joshua Ong Jun Leang, Giwon Hong, Alessio Devoto, Alberto Carlo Maria Mancino,\nRohit Saxena, Xuanli He, Yu Zhao, Xiaotang Du, Mohammad Reza Ghasemi Madani, et al. Are we\ndone with mmlu? CoRR, abs/2406.04127, 2024.\nGemini Team. Gemini 1.5: Unlocking multimodal understanding across millions of tokens of context.\nTechnical report, Google, 2024. URL https://storage.googleapis.com/deepmind-media/gemini/gemi\nni v1 5 report.pdf.\nGemma Team, Morgane Riviere, Shreya Pathak, Pier Giuseppe Sessa, Cassidy Hardin, Surya Bhupatiraju,\nL´eonard Hussenot, Thomas Mesnard, Bobak Shahriari, Alexandre Ram´e, et al. Gemma 2: Improving\nopen language models at a practical size. CoRR, abs/2408.00118, 2024.\nNaman Goyal, Cynthia Gao, Vishrav Chaudhary, Peng-Jen Chen, Guillaume Wenzek, Da Ju, Sanjana\nKrishnan, Marc’Aurelio Ranzato, Francisco Guzm´an, and Angela Fan. The Flores-101 evaluation\nbenchmark for low-resource and multilingual machine translation. Trans. Assoc. Comput. Linguistics, 10:\n522–538, 2022.\n21\n\nDan Hendrycks, Collin Burns, Steven Basart, Andy Zou, Mantas Mazeika, Dawn Song, and Jacob\nSteinhardt. Measuring massive multitask language understanding. In ICLR. OpenReview.net, 2021a.\nDan Hendrycks, Collin Burns, Saurav Kadavath, Akul Arora, Steven Basart, Eric Tang, Dawn Song,\nand Jacob Steinhardt. Measuring mathematical problem solving with the MATH dataset. In NeurIPS\nDatasets and Benchmarks, 2021b.\nJordan Hoffmann, Sebastian Borgeaud, Arthur Mensch, Elena Buchatskaya, Trevor Cai, Eliza Rutherford,\nDiego de Las Casas, Lisa Anne Hendricks, Johannes Welbl, Aidan Clark, et al. Training compute-\noptimal large language models. CoRR, abs/2203.15556, 2022.\nKeith Hoskin. The “awful idea of accountability”: Inscribing people into the measurement of objects.\nAccountability: Power, ethos and the technologies of managing, 1996.\nCheng-Ping Hsieh, Simeng Sun, Samuel Kriman, Shantanu Acharya, Dima Rekesh, Fei Jia, Yang Zhang,\nand Boris Ginsburg. RULER: What’s the real context size of your long-context language models? CoRR,\nabs/2404.06654, 2024.\nShengding Hu, Yuge Tu, Xu Han, Chaoqun He, Ganqu Cui, Xiang Long, Zhi Zheng, Yewei Fang, Yuxiang\nHuang, Weilin Zhao, Xinrong Zhang, Zhen Leng Thai, Kai Zhang, Chongyi Wang, Yuan Yao, Chenyang\nZhao, Jie Zhou, Jie Cai, Zhongwu Zhai, Ning Ding, Chao Jia, Guoyang Zeng, Dahai Li, Zhiyuan Liu,\nand Maosong Sun. MiniCPM: Unveiling the potential of small language models with scalable training\nstrategies. CoRR, abs/2404.06395, 2024.\nBinyuan Hui, Jian Yang, Zeyu Cui, Jiaxi Yang, Dayiheng Liu, Lei Zhang, Tianyu Liu, Jiajun Zhang, Bowen\nYu, Keming Lu, et al. Qwen2.5-Coder technical report. CoRR, abs/2409.12186, 2024.\nNaman Jain, King Han, Alex Gu, Wen-Ding Li, Fanjia Yan, Tianjun Zhang, Sida Wang, Armando Solar-\nLezama, Koushik Sen, and Ion Stoica. LiveCodeBench: Holistic and contamination free evaluation of\nlarge language models for code. CoRR, abs/2403.07974, 2024.\nAlbert Q. Jiang, Alexandre Sablayrolles, Arthur Mensch, Chris Bamford, Devendra Singh Chaplot, Diego\nde Las Casas, Florian Bressand, Gianna Lengyel, Guillaume Lample, Lucile Saulnier, L´elio Renard\nLavaud, Marie-Anne Lachaux, Pierre Stock, Teven Le Scao, Thibaut Lavril, Thomas Wang, Timoth´ee\nLacroix, and William El Sayed. Mistral 7B. CoRR, abs/2310.06825, 2023a.\nAlbert Q. Jiang, Alexandre Sablayrolles, Antoine Roux, Arthur Mensch, Blanche Savary, Chris Bamford,\nDevendra Singh Chaplot, Diego de Las Casas, Emma Bou Hanna, Florian Bressand, Gianna Lengyel,\nGuillaume Bour, Guillaume Lample, L´elio Renard Lavaud, Lucile Saulnier, Marie-Anne Lachaux,\nPierre Stock, Sandeep Subramanian, Sophia Yang, Szymon Antoniak, Teven Le Scao, Th´eophile Gervet,\nThibaut Lavril, Thomas Wang, Timoth´ee Lacroix, and William El Sayed. Mixtral of experts. CoRR,\nabs/2401.04088, 2024a.\nHuiqiang Jiang, Yucheng Li, Chengruidong Zhang, Qianhui Wu, Xufang Luo, Surin Ahn, Zhenhua Han,\nAmir H Abdi, Dongsheng Li, Chin-Yew Lin, Yuqing Yang, and Lili Qiu. Minference 1.0: Accelerating\npre-filling for long-context llms via dynamic sparse attention. arXiv preprint arXiv:2407.02490, 2024b.\nZixuan Jiang, Jiaqi Gu, Hanqing Zhu, and David Z. Pan. Pre-RMSNorm and Pre-CRMSNorm Transform-\ners: Equivalent and efficient pre-LN Transformers. CoRR, abs/2305.14858, 2023b.\nJared Kaplan, Sam McCandlish, Tom Henighan, Tom B Brown, Benjamin Chess, Rewon Child, Scott\nGray, Alec Radford, Jeffrey Wu, and Dario Amodei. Scaling laws for neural language models. CoRR,\nabs/2001.08361, 2020.\nFajri Koto, Nurul Aisyah, Haonan Li, and Timothy Baldwin. Large language models only pass primary\nschool exams in Indonesia: A comprehensive test on IndoMMLU. In EMNLP, pp. 12359–12374.\nAssociation for Computational Linguistics, 2023.\nNathan Lambert, Valentina Pyatkin, Jacob Daniel Morrison, Lester James Validad Miranda, Bill Yuchen\nLin, Khyathi Raghavi Chandu, Nouha Dziri, Sachin Kumar, Tom Zick, Yejin Choi, Noah A. Smith,\nand Hanna Hajishirzi. RewardBench: Evaluating reward models for language modeling. CoRR,\nabs/2403.13787, 2024.\nDmitry Lepikhin, HyoukJoong Lee, Yuanzhong Xu, Dehao Chen, Orhan Firat, Yanping Huang, Maxim\nKrikun, Noam Shazeer, and Zhifeng Chen. GShard: Scaling giant models with conditional computation\nand automatic sharding. CoRR, abs/2006.16668, 2020.\n22\n\nTianle Li, Wei-Lin Chiang, Evan Frick, Lisa Dunlap, Tianhao Wu, Banghua Zhu, Joseph E. Gonzalez,\nand Ion Stoica. From crowdsourced data to high-quality benchmarks: Arena-Hard and BenchBuilder\npipeline. CoRR, abs/2406.11939, 2024.\nStephanie Lin, Jacob Hilton, and Owain Evans. TruthfulQA: Measuring how models mimic human\nfalsehoods. In ACL (1), pp. 3214–3252. Association for Computational Linguistics, 2022a.\nXi Victoria Lin, Todor Mihaylov, Mikel Artetxe, Tianlu Wang, Shuohui Chen, Daniel Simig, Myle Ott,\nNaman Goyal, Shruti Bhosale, Jingfei Du, Ramakanth Pasunuru, Sam Shleifer, Punit Singh Koura,\nVishrav Chaudhary, Brian O’Horo, Jeff Wang, Luke Zettlemoyer, Zornitsa Kozareva, Mona T. Diab,\nVeselin Stoyanov, and Xian Li. Few-shot learning with multilingual generative language models. In\nEMNLP, pp. 9019–9052. Association for Computational Linguistics, 2022b.\nJiawei Liu, Chunqiu Steven Xia, Yuyao Wang, and Lingming Zhang. Is your code generated by ChatGPT\nreally correct? Rigorous evaluation of large language models for code generation. In NeurIPS, 2023.\nKeming Lu, Bowen Yu, Fei Huang, Yang Fan, Runji Lin, and Chang Zhou. Online merging optimizers for\nboosting rewards and mitigating tax in alignment. CoRR, abs/2405.17931, 2024a.\nKeming Lu, Bowen Yu, Chang Zhou, and Jingren Zhou. Large language models are superpositions of all\ncharacters: Attaining arbitrary role-play via self-alignment. CoRR, abs/2401.12474, 2024b.\nNiklas Muennighoff, Thomas Wang, Lintang Sutawika, Adam Roberts, Stella Biderman, Teven Le Scao,\nM. Saiful Bari, Sheng Shen, Zheng Xin Yong, Hailey Schoelkopf, Xiangru Tang, Dragomir Radev,\nAlham Fikri Aji, Khalid Almubarak, Samuel Albanie, Zaid Alyafeai, Albert Webson, Edward Raff, and\nColin Raffel. Crosslingual generalization through multitask finetuning. In ACL (1), pp. 15991–16111.\nAssociation for Computational Linguistics, 2023.\nJunho Myung, Nayeon Lee, Yi Zhou, Jiho Jin, Rifki Afina Putri, Dimosthenis Antypas, Hsuvas Borkakoty,\nEunsu Kim, Carla P´erez-Almendros, Abinew Ali Ayele, V´ıctor Guti´errez-Basulto, Yazm´ın Ib´a˜nez-\nGarc´ıa, Hwaran Lee, Shamsuddeen Hassan Muhammad, Ki-Woong Park, Anar Sabuhi Rzayev, Nina\nWhite, Seid Muhie Yimam, Mohammad Taher Pilehvar, Nedjma Ousidhoum, Jos´e Camacho-Collados,\nand Alice Oh. Blend: A benchmark for llms on everyday knowledge in diverse cultures and languages.\nCoRR, abs/2406.09948, 2024.\nOpenAI. GPT4 technical report. CoRR, abs/2303.08774, 2023.\nOpenAI. Hello GPT-4o, 2024a. URL https://openai.com/index/hello-gpt-4o/.\nOpenAI. Learning to reason with LLMs, 2024b. URL https://openai.com/index/learning-to-reaso\nn-with-llms/.\nLong Ouyang, Jeffrey Wu, Xu Jiang, Diogo Almeida, Carroll L. Wainwright, Pamela Mishkin, Chong\nZhang, Sandhini Agarwal, Katarina Slama, Alex Ray, John Schulman, Jacob Hilton, Fraser Kelton, Luke\nMiller, Maddie Simens, Amanda Askell, Peter Welinder, Paul F. Christiano, Jan Leike, and Ryan Lowe.\nTraining language models to follow instructions with human feedback. In NeurIPS, 2022.\nBowen Peng, Jeffrey Quesnelle, Honglu Fan, and Enrico Shippole. YaRN: Efficient context window\nextension of large language models. CoRR, abs/2309.00071, 2023.\nEdoardo Maria Ponti, Goran Glavas, Olga Majewska, Qianchu Liu, Ivan Vulic, and Anna Korhonen.\nXCOPA: A multilingual dataset for causal commonsense reasoning. In EMNLP (1), pp. 2362–2376.\nAssociation for Computational Linguistics, 2020.\nShanghaoran Quan, Tianyi Tang, Bowen Yu, An Yang, Dayiheng Liu, Bofei Gao, Jianhong Tu, Yichang\nZhang, Jingren Zhou, and Junyang Lin. Language models can self-lengthen to generate long texts.\nCoRR, abs/2410.23933, 2024.\nQwen Team. Code with CodeQwen1.5, 2024a. URL https://qwenlm.github.io/blog/codeqwen1.5/.\nQwen Team. Introducing Qwen1.5, 2024b. URL https://qwenlm.github.io/blog/qwen1.5/.\nQwen Team. Introducing Qwen2-Math, 2024c. URL https://qwenlm.github.io/blog/qwen2-math/.\nQwen Team. QwQ: Reflect deeply on the boundaries of the unknown, 2024d. URL https://qwenlm.git\nhub.io/blog/qwq-32b-preview/.\nAlec Radford, Karthik Narasimhan, Tim Salimans, Ilya Sutskever, et al. Improving language understand-\ning by generative pre-training. Technical report, OpenAI, 2018.\n23\n\nRafael Rafailov, Archit Sharma, Eric Mitchell, Christopher D. Manning, Stefano Ermon, and Chelsea Finn.\nDirect preference optimization: Your language model is secretly a reward model. In NeurIPS, 2023.\nSamyam Rajbhandari, Conglong Li, Zhewei Yao, Minjia Zhang, Reza Yazdani Aminabadi, Ammar Ahmad\nAwan, Jeff Rasley, and Yuxiong He. DeepSpeed-MoE: Advancing mixture-of-experts inference and\ntraining to power next-generation AI scale. In ICML, volume 162 of Proceedings of Machine Learning\nResearch, pp. 18332–18346. PMLR, 2022.\nDavid Rein, Betty Li Hou, Asa Cooper Stickland, Jackson Petty, Richard Yuanzhe Pang, Julien Dirani,\nJulian Michael, and Samuel R. Bowman. GPQA: A graduate-level Google-proof Q&A benchmark.\nCoRR, abs/2311.12022, 2023.\nKeisuke Sakaguchi, Ronan Le Bras, Chandra Bhagavatula, and Yejin Choi. WinoGrande: An adversarial\nwinograd schema challenge at scale. Commun. ACM, 64(9):99–106, 2021.\nRico Sennrich, Barry Haddow, and Alexandra Birch. Neural machine translation of rare words with\nsubword units. In ACL (1). The Association for Computer Linguistics, 2016.\nZhihong Shao, Peiyi Wang, Qihao Zhu, Runxin Xu, Junxiao Song, Mingchuan Zhang, Y. K. Li, Y. Wu, and\nDaya Guo. Deepseekmath: Pushing the limits of mathematical reasoning in open language models.\nCoRR, abs/2402.03300, 2024.\nJianlin Su. The magical effect of the Bias term: RoPE + Bias = better length extrapolation, 2023. URL\nhttps://spaces.ac.cn/archives/9577.\nJianlin Su, Murtadha H. M. Ahmed, Yu Lu, Shengfeng Pan, Wen Bo, and Yunfeng Liu. Roformer:\nEnhanced Transformer with rotary position embedding. Neurocomputing, 568:127063, 2024.\nMirac Suzgun, Nathan Scales, Nathanael Sch¨arli, Sebastian Gehrmann, Yi Tay, Hyung Won Chung,\nAakanksha Chowdhery, Quoc V. Le, Ed H. Chi, Denny Zhou, and Jason Wei. Challenging BIG-Bench\ntasks and whether chain-of-thought can solve them. In ACL (Findings), pp. 13003–13051. Association\nfor Computational Linguistics, 2023.\nHugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier Martinet, Marie-Anne Lachaux, Timoth´ee Lacroix,\nBaptiste Rozi`ere, Naman Goyal, Eric Hambro, Faisal Azhar, Aur´elien Rodriguez, Armand Joulin,\nEdouard Grave, and Guillaume Lample. LLaMA: Open and efficient foundation language models.\nCoRR, abs/2302.13971, 2023a.\nHugo Touvron, Louis Martin, Kevin Stone, Peter Albert, Amjad Almahairi, Yasmine Babaei, Nikolay\nBashlykov, Soumya Batra, Prajjwal Bhargava, Shruti Bhosale, Dan Bikel, Lukas Blecher, Cristian Canton-\nFerrer, Moya Chen, Guillem Cucurull, David Esiobu, Jude Fernandes, Jeremy Fu, Wenyin Fu, Brian\nFuller, Cynthia Gao, Vedanuj Goswami, Naman Goyal, Anthony Hartshorn, Saghar Hosseini, Rui\nHou, Hakan Inan, Marcin Kardas, Viktor Kerkez, Madian Khabsa, Isabel Kloumann, Artem Korenev,\nPunit Singh Koura, Marie-Anne Lachaux, Thibaut Lavril, Jenya Lee, Diana Liskovich, Yinghai Lu,\nYuning Mao, Xavier Martinet, Todor Mihaylov, Pushkar Mishra, Igor Molybog, Yixin Nie, Andrew\nPoulton, Jeremy Reizenstein, Rashi Rungta, Kalyan Saladi, Alan Schelten, Ruan Silva, Eric Michael\nSmith, Ranjan Subramanian, Xiaoqing Ellen Tan, Binh Tang, Ross Taylor, Adina Williams, Jian Xiang\nKuan, Puxin Xu, Zheng Yan, Iliyan Zarov, Yuchen Zhang, Angela Fan, Melanie Kambadur, Sharan\nNarang, Aur´elien Rodriguez, Robert Stojnic, Sergey Edunov, and Thomas Scialom. Llama 2: Open\nfoundation and fine-tuned chat models. CoRR, abs/2307.09288, 2023b.\nAshish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N. Gomez, Lukasz\nKaiser, and Illia Polosukhin. Attention is all you need. In NIPS, pp. 5998–6008, 2017.\nBinghai Wang, Rui Zheng, Lu Chen, Yan Liu, Shihan Dou, Caishuang Huang, Wei Shen, Senjie Jin, Enyu\nZhou, Chenyu Shi, et al. Secrets of RLHF in large language models part II: Reward modeling. CoRR,\nabs/2401.06080, 2024a.\nChanghan Wang, Kyunghyun Cho, and Jiatao Gu. Neural machine translation with byte-level subwords.\nIn AAAI, pp. 9154–9160. AAAI Press, 2020.\nYubo Wang, Xueguang Ma, Ge Zhang, Yuansheng Ni, Abhranil Chandra, Shiguang Guo, Weiming Ren,\nAaran Arulraj, Xuan He, Ziyan Jiang, Tianle Li, Max Ku, Kai Wang, Alex Zhuang, Rongqi Fan, Xiang\nYue, and Wenhu Chen. MMLU-Pro: A more robust and challenging multi-task language understanding\nbenchmark. CoRR, abs/2406.01574, 2024b.\nZhilin Wang, Alexander Bukharin, Olivier Delalleau, Daniel Egert, Gerald Shen, Jiaqi Zeng, Oleksii\nKuchaiev, and Yi Dong. HelpSteer2-Preference: Complementing ratings with preferences. CoRR,\nabs/2410.01257, 2024c.\n24\n\nColin White, Samuel Dooley, Manley Roberts, Arka Pal, Benjamin Feuer, Siddhartha Jain, Ravid Shwartz-\nZiv, Neel Jain, Khalid Saifullah, Siddartha Naidu, Chinmay Hegde, Yann LeCun, Tom Goldstein, Willie\nNeiswanger, and Micah Goldblum. LiveBench: A challenging, contamination-free LLM benchmark.\nCoRR, abs/2406.19314, 2024.\nHao Xiang, Bowen Yu, Hongyu Lin, Keming Lu, Yaojie Lu, Xianpei Han, Le Sun, Jingren Zhou, and\nJunyang Lin. Aligning large language models via self-steering optimization. CoRR, abs/2410.17131,\n2024.\nWenhan Xiong, Jingyu Liu, Igor Molybog, Hejia Zhang, Prajjwal Bhargava, Rui Hou, Louis Martin, Rashi\nRungta, Karthik Abinav Sankararaman, Barlas Oguz, Madian Khabsa, Han Fang, Yashar Mehdad,\nSharan Narang, Kshitiz Malik, Angela Fan, Shruti Bhosale, Sergey Edunov, Mike Lewis, Sinong Wang,\nand Hao Ma. Effective long-context scaling of foundation models. CoRR, abs/2309.16039, 2023.\nAn Yang, Baosong Yang, Binyuan Hui, Bo Zheng, Bowen Yu, Chang Zhou, Chengpeng Li, Chengyuan Li,\nDayiheng Liu, Fei Huang, Guanting Dong, Haoran Wei, Huan Lin, Jialong Tang, Jialin Wang, Jian Yang,\nJianhong Tu, Jianwei Zhang, Jianxin Ma, Jianxin Yang, Jin Xu, Jingren Zhou, Jinze Bai, Jinzheng He,\nJunyang Lin, Kai Dang, Keming Lu, Keqin Chen, Kexin Yang, Mei Li, Mingfeng Xue, Na Ni, Pei Zhang,\nPeng Wang, Ru Peng, Rui Men, Ruize Gao, Runji Lin, Shijie Wang, Shuai Bai, Sinan Tan, Tianhang Zhu,\nTianhao Li, Tianyu Liu, Wenbin Ge, Xiaodong Deng, Xiaohuan Zhou, Xingzhang Ren, Xinyu Zhang,\nXipin Wei, Xuancheng Ren, Xuejing Liu, Yang Fan, Yang Yao, Yichang Zhang, Yu Wan, Yunfei Chu,\nYuqiong Liu, Zeyu Cui, Zhenru Zhang, Zhifang Guo, and Zhihao Fan. Qwen2 technical report. CoRR,\nabs/2407.10671, 2024a.\nAn Yang, Beichen Zhang, Binyuan Hui, Bofei Gao, Bowen Yu, Chengpeng Li, Dayiheng Liu, Jianhong Tu,\nJingren Zhou, Junyang Lin, et al. Qwen2.5-Math technical report: Toward mathematical expert model\nvia self-improvement. CoRR, abs/2409.12122, 2024b.\nJian Yang, Jiaxi Yang, Ke Jin, Yibo Miao, Lei Zhang, Liqun Yang, Zeyu Cui, Yichang Zhang, Binyuan Hui,\nand Junyang Lin. Evaluating and aligning codellms on human preference. CoRR, abs/2412.05210,\n2024c.\nYinfei Yang, Yuan Zhang, Chris Tar, and Jason Baldridge. PAWS-X: A cross-lingual adversarial dataset\nfor paraphrase identification. In EMNLP/IJCNLP (1), pp. 3685–3690. Association for Computational\nLinguistics, 2019.\nAlex Young, Bei Chen, Chao Li, Chengen Huang, Ge Zhang, Guanwei Zhang, Heng Li, Jiangcheng\nZhu, Jianqun Chen, Jing Chang, Kaidong Yu, Peng Liu, Qiang Liu, Shawn Yue, Senbin Yang, Shiming\nYang, Tao Yu, Wen Xie, Wenhao Huang, Xiaohui Hu, Xiaoyi Ren, Xinyao Niu, Pengcheng Nie, Yuchi\nXu, Yudong Liu, Yue Wang, Yuxuan Cai, Zhenyu Gu, Zhiyuan Liu, and Zonghong Dai. Yi: Open\nfoundation models by 01.AI. CoRR, abs/2403.04652, 2024.\nTao Yuan, Xuefei Ning, Dong Zhou, Zhijie Yang, Shiyao Li, Minghui Zhuang, Zheyue Tan, Zhuyu Yao,\nDahua Lin, Boxun Li, Guohao Dai, Shengen Yan, and Yu Wang. LV-Eval: A balanced long-context\nbenchmark with 5 length levels up to 256K. CoRR, abs/2402.05136, 2024.\nZheng Yuan, Hongyi Yuan, Chengpeng Li, Guanting Dong, Chuanqi Tan, and Chang Zhou. Scaling\nrelationship on learning mathematical reasoning with large language models. CoRR, abs/2308.01825,\n2023.\nRowan Zellers, Ari Holtzman, Yonatan Bisk, Ali Farhadi, and Yejin Choi. HellaSwag: Can a machine\nreally finish your sentence? In ACL (1), pp. 4791–4800. Association for Computational Linguistics,\n2019.\nYidan Zhang, Boyi Deng, Yu Wan, Baosong Yang, Haoran Wei, Fei Huang, Bowen Yu, Junyang Lin, and\nJingren Zhou. P-MMEval: A parallel multilingual multitask benchmark for consistent evaluation of\nLLMs. CoRR, abs/2411.09116, 2024.\nLianmin Zheng, Wei-Lin Chiang, Ying Sheng, Siyuan Zhuang, Zhanghao Wu, Yonghao Zhuang, Zi Lin,\nZhuohan Li, Dacheng Li, Eric P. Xing, Hao Zhang, Joseph E. Gonzalez, and Ion Stoica. Judging\nLLM-as-a-judge with MT-Bench and Chatbot Arena. In NeurIPS, 2023.\nEnyu Zhou, Guodong Zheng, Bing Wang, Zhiheng Xi, Shihan Dou, Rong Bao, Wei Shen, Limao Xiong,\nJessica Fan, Yurong Mou, Rui Zheng, Tao Gui, Qi Zhang, and Xuanjing Huang. RMB: Comprehensively\nbenchmarking reward models in LLM alignment. CoRR, abs/2410.09893, 2024.\nJeffrey Zhou, Tianjian Lu, Swaroop Mishra, Siddhartha Brahma, Sujoy Basu, Yi Luan, Denny Zhou, and\nLe Hou. Instruction-following evaluation for large language models. CoRR, abs/2311.07911, 2023.\n25\n\nBarret Zoph, Irwan Bello, Sameer Kumar, Nan Du, Yanping Huang, Jeff Dean, Noam Shazeer, and William\nFedus. ST-MoE: Designing stable and transferable sparse expert models. CoRR, abs/2202.08906, 2022.\n26\n"
    }
  ]
}