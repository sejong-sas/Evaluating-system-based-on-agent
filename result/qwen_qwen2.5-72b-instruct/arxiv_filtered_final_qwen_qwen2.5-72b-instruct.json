{
  "1-1 (Weights)": "The quotes state that the weights for the Qwen2.5 series – including the 72 B-parameter instruction-tuned checkpoint that corresponds to “qwen/qwen2.5-72b-instruct” – are openly released.  Availability is explicitly said to be on Hugging Face and ModelScope, and the material is also mirrored on Kaggle.  The open-weight catalogue covers seven discrete model sizes (0.5 B, 1.5 B, 3 B, 7 B, 14 B, 32 B and 72 B) and is offered both as base pre-trained models and as instruction-tuned variants.  The team further notes that “over 100 models” are reachable through the hubs, because each size is distributed in multiple formats: original bfloat16 precision as well as several quantised versions.  These public releases are positioned as a way to “foster community innovation and accessibility,” and they sit alongside “supplementary materials including example code on GitHub.”  One quote also clarifies that, in addition to the fully open models, there exist proprietary MoE versions such as “Qwen2.5-Turbo” and “Qwen2.5-Plus,” but the open part explicitly spans the full 0.5 B-to-72 B range.",
  "1-2 (Code)": "All code information comes from two distinct statements.  First, the authors say that, together with the model uploads, they provide “supplementary materials including example code on GitHub,” and that these GitHub resources cover “quantization, fine-tuning, and deployment,” thereby making it clear that users can obtain scripts for additional training and for serving.  Second, the January 6 2025 entry (“Qwen2.5 Technical Report … https://github.com/QwenLM/Qwen2.5”) supplies the concrete repository location, indicating that the public GitHub project named QwenLM/Qwen2.5 is the canonical place to fetch those materials.  The quotes do not explicitly separate pre-training code from fine-tuning or inference code, but they do confirm that at least the latter two categories (quantisation/fine-tuning + deployment) are included, and that everything is published under the same Qwen2.5 GitHub namespace.",
  "1-3 (License)": "The only licensing lines provided come from “Table 1: Model architecture and license of Qwen2.5 open-weight models. …  Apache 2.0.”  This indicates that every open-weight release in the Qwen2.5 family, including the 72 B instruction-tuned checkpoint, is distributed under the Apache License 2.0.  No other restrictions, special clauses or alternative licences are mentioned in the quoted material.",
  "1-4 (Paper)": "Multiple citations point to official documentation.  A primary reference is the “Qwen2.5 Technical Report,” which appears on arXiv as “arXiv:2412.15115v2 [cs.CL] 3 Jan 2025.”  The quotes further mention companion technical reports that specialise the family: “Qwen2.5-Coder technical report. CoRR, abs/2409.12186, 2024” and “Qwen2.5-Math technical report: Toward mathematical expert model via self-improvement. CoRR, abs/2409.12122, 2024b.”  Earlier background is supplied by generic Qwen2 or QWEN2 technical reports that “introduce the Qwen2 series,” and one sentence explicitly states, “In this report, we introduce Qwen2.5, a comprehensive series of large language models (LLMs) designed to meet diverse needs.”  Collectively, these references form the official written record for the architecture and training details of the Qwen2.5 models, including the 72 B instruction-tuned variant.",
  "1-1 (Weights)__evidence": [
    {
      "source": "[pdf_text]",
      "quote": "To foster community innovation and accessibility, we have made the Qwen2 model weights openly available on Hugging Face1 and ModelScope2, and the supplementary materials including example code on GitHub3."
    },
    {
      "source": "[abstract]",
      "quote": "To handle diverse and varied use cases effectively, we present Qwen2.5 LLM series in rich configurations. The open-weight offerings include base models and instruction-tuned models in sizes of 0.5B, 1.5B, 3B, 7B, 14B, 32B, and 72B parameters. Quantized versions of the instruction-tuned models are also provided. Over 100 models can be accessed from Hugging Face Hub, ModelScope, and Kaggle."
    },
    {
      "source": "[sections/Introduction]",
      "quote": "Recently, we release the details of our latest version of the Qwen series, Qwen2.5. In terms of the open-weight part, we release pre-trained and instruction-tuned models of 7 sizes, including 0.5B, 1.5B, 3B, 7B, 14B, 32B, and 72B, and we provide not only the original models in bfloat16 precision but also the quantized models in different precisions."
    },
    {
      "source": "[sections/Conclusion]",
      "quote": "Available in various configurations, Qwen2.5 offers both open-weight from 0.5B to 72B parameters and proprietary models including cost-effective MoE variants like Qwen2.5-Turbo and Qwen2.5-Plus."
    },
    {
      "source": "[sections/2407.10671]",
      "quote": "To foster community innovation and accessibility, we have made the Qwen2 model weights openly available on Hugging Face1 and ModelScope2, and the supplementary materials including example code on GitHub3."
    },
    {
      "source": "[abstract]",
      "quote": "To handle diverse and varied use cases effectively, we present Qwen2.5 LLM series in rich configurations. The open-weight offerings include base models and instruction-tuned models in sizes of 0.5B, 1.5B, 3B, 7B, 14B, 32B, and 72B parameters."
    },
    {
      "source": "[sections/2412.15115]",
      "quote": "Recently, we release the details of our latest version of the Qwen series, Qwen2.5. In terms of the open-weight part, we release pre-trained and instruction-tuned models of 7 sizes, including 0.5B, 1.5B, 3B, 7B, 14B, 32B, and 72B, and we provide not only the original models in bfloat16 precision but also the quantized models in different precisions."
    }
  ],
  "1-2 (Code)__evidence": [
    {
      "source": "[pdf_text]",
      "quote": "To foster community innovation and accessibility, we have made the Qwen2 model weights openly available on Hugging Face1 and ModelScope2, and the supplementary materials including example code on GitHub3. These platforms also include resources for quantization, fine-tuning, and deployment, facilitating a wide range of applications and research endeavors."
    },
    {
      "source": "[pdf_text]",
      "quote": "2025-01-06 Qwen2.5 Technical Report Qwen Team https://huggingface.co/Qwen https://modelscope.cn/organization/qwen https://github.com/QwenLM/Qwen2.5"
    },
    {
      "source": "[sections/2407.10671]",
      "quote": "To foster community innovation and accessibility, we have made the Qwen2 model weights openly available on Hugging Face1 and ModelScope2, and the supplementary materials including example code on GitHub3. These platforms also include resources for quantization, fine-tuning, and deployment, facilitating a wide range of applications and research endeavors."
    }
  ],
  "1-3 (License)__evidence": [
    {
      "source": "[pdf_text]",
      "quote": "Table 1: Model architecture and license of Qwen2.5 open-weight models. 0.5B ... Apache 2.0"
    },
    {
      "source": "[sections/2412.15115]",
      "quote": "Table 1: Model architecture and license of Qwen2.5 open-weight models."
    }
  ],
  "1-4 (Paper)__evidence": [
    {
      "source": "[pdf_text]",
      "quote": "This report introduces the Qwen2 series, the latest addition to our large language models and large multimodal models."
    },
    {
      "source": "[title]",
      "quote": "Qwen2.5 Technical Report"
    },
    {
      "source": "[pdf_text]",
      "quote": "arXiv:2412.15115v2  [cs.CL]  3 Jan 2025"
    },
    {
      "source": "[sections/References]",
      "quote": "Binyuan Hui, Jian Yang, Zeyu Cui, Jiaxi Yang, Dayiheng Liu, Lei Zhang, Tianyu Liu, Jiajun Zhang, Bowen Yu, Keming Lu, et al. Qwen2.5-Coder technical report. CoRR, abs/2409.12186, 2024."
    },
    {
      "source": "[sections/References]",
      "quote": "An Yang, Beichen Zhang, Binyuan Hui, Bofei Gao, Bowen Yu, Chengpeng Li, Dayiheng Liu, Jianhong Tu, Jingren Zhou, Junyang Lin, et al. Qwen2.5-Math technical report: Toward mathematical expert model via self-improvement. CoRR, abs/2409.12122, 2024b."
    },
    {
      "source": "[sections/2407.10671]",
      "quote": "QWEN2 TECHNICAL REPORT"
    },
    {
      "source": "[sections/2407.10671]",
      "quote": "This report introduces the Qwen2 series, the latest addition to our large language models and large multimodal models."
    },
    {
      "source": "[sections/2412.15115]",
      "quote": "In this report, we introduce Qwen2.5, a comprehensive series of large language models (LLMs) designed to meet diverse needs."
    }
  ],
  "1-5 (Architecture)": "The public material that explicitly mentions Qwen, Qwen2 or Qwen2.5 paints the following picture of the architecture that also applies to the 72-billion-parameter instruction-tuned checkpoint (qwen/qwen2.5-72b-instruct).\n• Family and size.  Qwen2.5 is released as a set of open-weight dense models ranging from 0.5 B to 72 B parameters; the 72 B variant is the largest open model.  A table entry that reads “72B 80 64 / 8 No 128K / 8K” indicates that the 72 B model is built from roughly 80 stacked blocks (Transformer layers).  Because the series keeps a single-token-per-step dense design, all 72 B parameters are simultaneously active during inference.\n• Core backbone.  As with earlier Qwen generations, Qwen2.5 maintains a decoder-only, Transformer-based architecture that follows Vaswani et al. (2017) and the GPT lineage (Radford et al., 2018).  Every layer contains causal self-attention followed by a feed-forward network (FFN).\n• Attention implementation.  Several optimisations are added on top of the vanilla Transformer:  – Grouped Query Attention (GQA) is adopted to lower the size of the per-token key–value cache and improve memory efficiency; – Dual Chunk Attention (DCA) segments long sequences into manageable chunks so that the same model can be trained and served with greatly extended context windows; – The project also introduces a sparse attention kernel based on Minference to accelerate inference when very long contexts are supplied.  In addition, conventional Rotary Position Embeddings (RoPE) are kept, and the attention projections retain an explicit QKV bias.\n• Feed-forward block and normalisation.  The FFN uses a SwiGLU activation, and the entire stack is pre-normalised with RMSNorm, a lighter alternative to LayerNorm that keeps training stable at scale.\n• Context length and extrapolation.  Long-context ability is a major engineering focus.  Training context was gradually increased from 4 096 tokens to 32 768 tokens in the final stage of pre-training, and the combination of DCA and YARN length-extrapolation techniques lets the released checkpoints cope with contexts as large as 128 K tokens while retaining an 8 K sliding-window of fully attended content.\n• Memory footprint.  Owing to GQA and other refinements, Qwen2.* models—including the 72 B checkpoint—significantly reduce key–value (KV) cache size per token versus the older Qwen1.5 generation.\nCollectively, these design choices give qwen/qwen2.5-72b-instruct a modern, highly optimised Transformer layout that emphasises efficient long-context processing without sacrificing standard autoregressive performance.",
  "1-6 (Tokenizer)": "All publicly available sentences that mention Qwen make it clear that the entire Qwen2.5 range—including the 72 B instruction model—shares exactly one tokenizer.  The tokenizer is inherited from the original Qwen release and is based on byte-level Byte-Pair Encoding (BBPE).  It ships with a fixed vocabulary of 151 643 regular tokens.  Earlier generations exposed only three special/control tokens, but Qwen2.5 enlarges this set to 22 control tokens so that new functionality (for tools or future capabilities) can be uniformly expressed across every Qwen2.5 model.  Because every size from 0.5 B up to 72 B uses the same vocabulary, downstream users can swap between checkpoints without needing to retokenise data, which reduces incompatibility risk and simplifies deployment.",
  "2-1 (Hardware)": "",
  "2-2 (Software)": "",
  "1-5 (Architecture)__evidence": [
    {
      "source": "[sections/Model Architecture]",
      "quote": "The architecture of the Qwen2 dense models comprises multiple Transformer layers, each equipped with causal attention mechanisms and feed-forward neural networks (FFNs)."
    },
    {
      "source": "[sections/Model Architecture]",
      "quote": "To expand the context window of Qwen2, we implement Dual Chunk Attention (DCA, An et al., 2024), which segments long sequences into chunks of manageable lengths."
    },
    {
      "source": "[sections/Model Architecture]",
      "quote": "The architecture of Qwen2 MoE models closely mirrors that of Qwen1.5-MoE-A2.7B (Qwen Team, 2024c)."
    },
    {
      "source": "[sections/2.2.2 QWEN2 MIXTURE-OF-EXPERTS MODEL]",
      "quote": "The architecture of Qwen2 MoE models closely mirrors that of Qwen1.5-MoE-A2.7B (Qwen Team, 2024c)."
    },
    {
      "source": "[sections/2.2.3 MODEL CONFIGURATION]",
      "quote": "In the following, we provide the key configuration and information for the Qwen2 series. The Qwen2 series consists of models of 5 sizes, which are Qwen2-0.5B, Qwen2-1.5B, Qwen2-7B, Qwen2-57B-A14B, and Qwen2-72B."
    },
    {
      "source": "[table]",
      "quote": "Table 1: Architecture of Qwen2 dense and MoE models. For MoE models, 57B-A14B denotes that the model has 57B parameters in total and for each token 14B parameters are active, the Intermediate size denotes that of each expert, and # Activated Experts excludes the shared experts."
    },
    {
      "source": "[sections/2.2.3 MODEL CONFIGURATION]",
      "quote": "Notably, Qwen2 models demonstrate a substantially lower Key-Value (KV) size per token relative to Qwen1.5 models."
    },
    {
      "source": "[sections/Architecture & Tokenizer]",
      "quote": "Basically, the Qwen2.5 series include dense models for opensource, namely Qwen2.5-0.5B / 1.5B / 3B / 7B / 14B / 32B / 72B, and MoE models for API service, namely Qwen2.5-Turbo and Qwen2.5-Plus. Below, we provide details about the architecture of models. For dense models, we maintain the Transformer-based decoder architecture (Vaswani et al., 2017; Radford et al., 2018) as Qwen2 (Yang et al., 2024a). The architecture incorporates several key components: Grouped Query Attention (GQA, Ainslie et al., 2023) for efficient KV cache utilization, SwiGLU activation function (Dauphin et al., 2017) for non-linear activation, Rotary Positional Embeddings (RoPE, Su et al., 2024) for encoding position information, QKV bias (Su, 2023) in the attention mechanism and RMSNorm (Jiang et al., 2023b) with pre-normalization to ensure stable training."
    },
    {
      "source": "[sections/Architecture & Tokenizer]",
      "quote": "Table 1: Model architecture and license of Qwen2.5 open-weight models."
    },
    {
      "source": "[sections/Conclusion]",
      "quote": "Available in various configurations, Qwen2.5 offers both open-weight from 0.5B to 72B parameters and proprietary models including cost-effective MoE variants like Qwen2.5-Turbo and Qwen2.5-Plus."
    },
    {
      "source": "[sections/Long Context Capabilities]",
      "quote": "We can observe that the Qwen2.5 models, after equipping length extrapolation techniques (i.e., DCA + YARN), have demonstrated strong long context processing capabilities on the three datasets."
    },
    {
      "source": "[sections/Long Context Capabilities]",
      "quote": "Furthermore, as shown in Figure 2, Qwen2.5-Turbo achieves 100% accuracy in the 1M-token passkey retrieval task, demonstrating its exceptional ability to capture detailed information from ultra-long contexts. We develop a sparse attention mechanism based on Minference (Jiang et al., 2024b) to significantly enhance inference speed, which is critical for user experience when processing long contexts."
    },
    {
      "source": "[sections/Tokenizer & Model]",
      "quote": "The architecture of the Qwen2 dense models comprises multiple Transformer layers, each equipped with causal attention mechanisms and feed-forward neural networks (FFNs)."
    },
    {
      "source": "[sections/Tokenizer & Model]",
      "quote": "The architecture of Qwen2 MoE models closely mirrors that of Qwen1.5-MoE-A2.7B (Qwen Team, 2024c)."
    },
    {
      "source": "[sections/Tokenizer & Model]",
      "quote": "The Qwen2 series consists of models of 5 sizes, which are Qwen2-0.5B, Qwen2-1.5B, Qwen2-7B, Qwen2-57B-A14B, and Qwen2-72B."
    },
    {
      "source": "[sections/Tokenizer & Model]",
      "quote": "Notably, Qwen2 models demonstrate a substantially lower Key-Value (KV) size per token relative to Qwen1.5 models."
    },
    {
      "source": "[sections/MODEL CONFIGURATION]",
      "quote": "The Qwen2 series consists of models of 5 sizes, which are Qwen2-0.5B, Qwen2-1.5B, Qwen2-7B, Qwen2-57B-A14B, and Qwen2-72B. Table 1 lists the hyper-parameters and important information, e.g., the number of pre-trained tokens."
    },
    {
      "source": "[sections/MODEL CONFIGURATION]",
      "quote": "Notably, Qwen2 models demonstrate a substantially lower Key-Value (KV) size per token relative to Qwen1.5 models."
    },
    {
      "source": "[sections/LONG-CONTEXT TRAINING]",
      "quote": "To enhance the long-context capability of Qwen2, we augmented the context length from 4,096 tokens to 32,768 tokens during the concluding phase of pre-training."
    },
    {
      "source": "[sections/2412.15115]",
      "quote": "For dense models, we maintain the Transformer-based decoder architecture (Vaswani et al., 2017; Radford et al., 2018) as Qwen2 (Yang et al., 2024a). The architecture incorporates several key components: Grouped Query Attention (GQA, Ainslie et al., 2023) for efficient KV cache utilization, SwiGLU activation function (Dauphin et al., 2017) for non-linear activation, Rotary Positional Embeddings (RoPE, Su et al., 2024) for encoding position information, QKV bias (Su, 2023) in the attention mechanism and RMSNorm (Jiang et al., 2023b) with pre-normalization to ensure stable training."
    },
    {
      "source": "[sections/2412.15115]",
      "quote": "Table 1: Model architecture and license of Qwen2.5 open-weight models. … 72B 80 64 / 8 No 128K / 8K Qwen"
    }
  ],
  "1-6 (Tokenizer)__evidence": [
    {
      "source": "[sections/Tokenizer]",
      "quote": "Following Qwen (Bai et al., 2023a), we employ the identical tokenizer based on byte-level byte-pair encoding. Models of all sizes employ a common vocabulary consisting of 151,643 regular tokens and 3 control tokens."
    },
    {
      "source": "[sections/Architecture & Tokenizer]",
      "quote": "For tokenization, we utilize Qwen’s tokenizer (Bai et al., 2023), which implements byte-level byte-pair encoding (BBPE, Brown et al., 2020; Wang et al., 2020; Sennrich et al., 2016) with a vocabulary of 151,643 regular tokens. We have expanded the set of control tokens from 3 to 22 compared to previous Qwen versions, adding two new tokens for tool functionality and allocating the remainder for other model capabilities. This expansion establishes a unified vocabulary across all Qwen2.5 models, enhancing consistency and reducing potential compatibility issues."
    },
    {
      "source": "[sections/Tokenizer & Model]",
      "quote": "Following Qwen (Bai et al., 2023a), we employ the identical tokenizer based on byte-level byte-pair encoding."
    },
    {
      "source": "[sections/2412.15115]",
      "quote": "For tokenization, we utilize Qwen’s tokenizer (Bai et al., 2023), which implements byte-level byte-pair encoding (BBPE, Brown et al., 2020; Wang et al., 2020; Sennrich et al., 2016) with a vocabulary of 151,643 regular tokens. We have expanded the set of control tokens from 3 to 22 compared to previous Qwen versions, adding two new tokens for tool functionality and allocating the remainder for other model capabilities."
    }
  ],
  "2-1 (Hardware)__evidence": [],
  "2-2 (Software)__evidence": [],
  "2-3 (API)": "The quotes explicitly state that Qwen2.5-72B belongs to the open-source “dense” branch of the Qwen2.5 series, whose weights are released on both Hugging Face and ModelScope, with supplementary example code hosted on GitHub. Within the same paragraph the authors note that, alongside these open-source checkpoints (0.5 B → 72 B), the team also exposes separate MoE variants for API use: Qwen2.5-Turbo (surfaced in the API as the model-name “qwen-turbo-2024-11-01”) and a forthcoming Qwen2.5-Plus (placeholder “qwen-plus-2024-xx-xx”). Thus, an external caller can either download qwen2.5-72B directly, or invoke the hosted Turbo / Plus endpoints through the official API. No rate limits, pricing, or endpoint URLs are given in these quotes, but the public availability of downloadable checkpoints and documented API identifiers is explicitly confirmed.",
  "3-1 (Pre-training)": "Qwen2.5-72B is part of the Qwen2.5 family that advances upon Qwen2 in three major ways. (1) Data scale and quality: the high-quality pre-training corpus is expanded from ≈7 T tokens (Qwen2) to 18 T tokens for Qwen2.5, and additional filtering/refinement improves overall quality. (2) Two-phase curriculum: training starts with a 4,096-token context window and then enters an extension phase where the maximum context length is raised to 32,768 tokens; a large volume of long-form data is injected during the latter phase. (3) Long-context adaptation tricks: the RoPE base frequency is changed from 10,000 to 1,000,000 to stabilise >32 k-token inference. The model family continues to use next-token prediction on a Transformer backbone and still mixes in high-quality multi-task instruction data during pre-training. Scaling-law–based hyper-parameter schedules (after Hoffmann/Kaplan) are derived on this 18 T-token dataset to choose learning-rate, batch-size, and related knobs. All dense checkpoints except the smallest 0.5 B variant—therefore including the 72 B model—are trained on this full regimen.",
  "3-2 (Fine-tuning)": "The instruction-tuned variant qwen2.5-72B-instruct is produced in a dedicated post-training stage. Supervised fine-tuning (SFT) leverages a curated corpus of >1 M high-quality single- and multi-turn examples that cover coding, maths, reasoning, multilingual tasks, etc. The 72 B model is run for two SFT epochs with a sequence length of 32,768 tokens, enabling long-form generation; the resulting chat model can emit responses up to 8,192 tokens—several times longer than typical 2 k-token chat limits. Extra coding skill is injected via the specialised Qwen2.5-Coder instruction-tuning subset. Overall, the post-training pipeline converts the base language model into an “aligned” chat/agent model by combining massive SFT data, long-sequence training, and domain-specific augmentation.",
  "3-3 (Reinforcement Learning)": "After SFT, Qwen2.5-72B-instruct undergoes a two-stage reinforcement-learning refinement. Stage 1 (Offline RL) and Stage 2 (Online RL) form a sequential curriculum. The process is driven by a dedicated reward model, Qwen2.5-RM-72B, which tops the PPE and Human-Preference-Chinese benchmarks and ranks second on RMB, performing on par with larger systems like Nemotron-4-340B-Reward. Direct Preference Optimization (DPO) is explicitly cited as part of the post-training stack for all Qwen models, meaning that the 72 B instruct checkpoint is aligned to human preferences via DPO in addition to the multi-stage RL. Consequently, the model benefits from supervised learning plus offline RL fine-grained reward shaping, followed by online RL to polish real-time behaviour, resulting in a chat system tuned to human feedback.",
  "2-3 (API)__evidence": [
    {
      "source": "[pdf_text]",
      "quote": "To foster community innovation and accessibility, we have made the Qwen2 model weights openly available on Hugging Face1 and ModelScope2, and the supplementary materials including example code on GitHub3."
    },
    {
      "source": "[sections/Architecture & Tokenizer]",
      "quote": "Basically, the Qwen2.5 series include dense models for opensource, namely Qwen2.5-0.5B / 1.5B / 3B / 7B / 14B / 32B / 72B, and MoE models for API service, namely Qwen2.5-Turbo and Qwen2.5-Plus."
    },
    {
      "source": "[sections/Architecture & Tokenizer]",
      "quote": "Qwen2.5-Turbo is identified as qwen-turbo-2024-11-01 and Qwen2.5-Plus is identified as qwen-plus-2024-xx-xx (to be released) in the API."
    },
    {
      "source": "[sections/2412.15115]",
      "quote": "Basically, the Qwen2.5 series include dense models for opensource, namely Qwen2.5-0.5B / 1.5B / 3B / 7B / 14B / 32B / 72B, and MoE models for API service, namely Qwen2.5-Turbo and Qwen2.5-Plus."
    },
    {
      "source": "[sections/2412.15115]",
      "quote": "1Qwen2.5-Turbo is identified as qwen-turbo-2024-11-01 and Qwen2.5-Plus is identified as qwen-plus-2024-xx-xx (to be released) in the API."
    }
  ],
  "3-1 (Pre-training)__evidence": [
    {
      "source": "[pdf_text]",
      "quote": "Qwen2 is a series of LLMs, grounded in the Transformer architecture (Vaswani et al., 2017), trained using next-token prediction."
    },
    {
      "source": "[sections/3 PRE-TRAINING]",
      "quote": "In the pre-training of Qwen2, our efforts were focused on refining the dataset and investigating methods to handle extended context lengths effectively."
    },
    {
      "source": "[sections/3 PRE-TRAINING]",
      "quote": "All Qwen2 dense models, excluding Qwen2-0.5B, were pre-trained on this large-scale dataset of over 7 trillion tokens."
    },
    {
      "source": "[sections/3 PRE-TRAINING]",
      "quote": "To enhance the long-context capability of Qwen2, we augmented the context length from 4,096 tokens to 32,768 tokens during the concluding phase of pre-training. This expansion was complemented by the introduction of a significantly increased volume of high-quality, lengthy data. In conjunction with these enhancements, we modified the base frequency of RoPE from 10,000 to 1,000,000 to optimize performance in long-context scenarios (Xiong et al., 2023)."
    },
    {
      "source": "[sections/3 PRE-TRAINING]",
      "quote": "Similar to previous Qwen models, high-quality multi-task instruction data is integrated into the Qwen2 pre-training process to enhance in-context learning and instruction-following abilities."
    },
    {
      "source": "[abstract]",
      "quote": "Compared to previous iterations, Qwen 2.5 has been significantly improved during both the pre-training and post-training stages. In terms of pre-training, we have scaled the high-quality pre-training datasets from the previous 7 trillion tokens to 18 trillion tokens."
    },
    {
      "source": "[sections/Pre-training]",
      "quote": "Qwen2.5 demonstrates significant enhancements in pre-training data quality compared to its predecessor Qwen2."
    },
    {
      "source": "[sections/Pre-training]",
      "quote": "For optimal training efficiency, Qwen2.5 employs a two-phase pre-training approach: an initial phase with a 4,096-token context length, followed by an extension phase for longer sequences."
    },
    {
      "source": "[sections/Pre-training]",
      "quote": "We develop scaling laws for hyper-parameter based on the pre-training data of Qwen2.5 (Hoffmann et al., 2022; Kaplan et al., 2020)."
    },
    {
      "source": "[sections/Conclusion]",
      "quote": "Qwen2.5 represents a significant advancement in large language models (LLMs), with enhanced pre-training on 18 trillion tokens and sophisticated post-training techniques, including supervised fine-tuning and multi-stage reinforcement learning."
    },
    {
      "source": "[sections/2407.10671]",
      "quote": "In this work, we introduce the newest addition to the Qwen family of large language models and large multimodal modles: Qwen2. All models were pre-trained on a high-quality, large-scale dataset comprising over 7 trillion tokens, covering a wide range of domains and languages."
    },
    {
      "source": "[sections/PRE-TRAINING]",
      "quote": "In the pre-training of Qwen2, our efforts were focused on refining the dataset and investigating methods to handle extended context lengths effectively."
    },
    {
      "source": "[sections/PRE-TRAINING]",
      "quote": "All Qwen2 dense models, excluding Qwen2-0.5B, were pre-trained on this large-scale dataset of over 7 trillion tokens."
    },
    {
      "source": "[sections/PRE-TRAINING]",
      "quote": "Based on these enhancements, the pre-training data was expanded from 3 trillion tokens in Qwen1.5 (Qwen Team, 2024a) to 7 trillion tokens."
    },
    {
      "source": "[sections/PRE-TRAINING]",
      "quote": "To enhance the long-context capability of Qwen2, we augmented the context length from 4,096 tokens to 32,768 tokens during the concluding phase of pre-training."
    },
    {
      "source": "[sections/PRE-TRAINING]",
      "quote": "Similar to previous Qwen models, high-quality multi-task instruction data is integrated into the Qwen2 pre-training process to enhance in-context learning and instruction-following abilities."
    },
    {
      "source": "[sections/2412.15115]",
      "quote": "Qwen2.5 demonstrates significant enhancements in pre-training data quality compared to its predecessor Qwen2. Building on these techniques, we have developed a larger and higher-quality pre-training dataset, expanding from the 7 trillion tokens used in Qwen2 (Yang et al., 2024a) to 18 trillion tokens."
    },
    {
      "source": "[sections/2412.15115]",
      "quote": "We develop scaling laws for hyper-parameter based on the pre-training data of Qwen2.5 (Hoffmann et al., 2022; Kaplan et al., 2020)."
    },
    {
      "source": "[sections/2412.15115]",
      "quote": "For optimal training efficiency, Qwen2.5 employs a two-phase pre-training approach: an initial phase with a 4,096-token context length, followed by an extension phase for longer sequences."
    }
  ],
  "3-2 (Fine-tuning)__evidence": [
    {
      "source": "[pdf_text]",
      "quote": "Qwen2 is a series of LLMs, grounded in the Transformer architecture (Vaswani et al., 2017), trained using next-token prediction. The model series encompasses foundational, i.e., base language models, pre-trained but unaligned to human preferences, and instruction-tuned models, fine-tuned with single-turn and multi-turn instruction-following datasets suitable for chat and agent purposes."
    },
    {
      "source": "[sections/Post-training]",
      "quote": "Qwen 2.5 introduces two significant advancements in its post-training design compared to Qwen 2: (1) Expanded Supervised Fine-tuning Data Coverage: The supervised fine-tuning process leverages a massive dataset comprising millions of high-quality examples."
    },
    {
      "source": "[sections/Post-training]",
      "quote": "In this section, we detail the key enhancements made during the SFT phase of Qwen2.5, focusing on several critical areas: Ultimately, we construct a dataset of over 1 million SFT examples. The model is fine-tuned for two epochs with a sequence length of 32,768 tokens."
    },
    {
      "source": "[sections/Conclusion]",
      "quote": "Qwen2.5 represents a significant advancement in large language models (LLMs), with enhanced pre-training on 18 trillion tokens and sophisticated post-training techniques, including supervised fine-tuning and multi-stage reinforcement learning."
    },
    {
      "source": "[sections/2407.10671]",
      "quote": "In this work, we introduce the newest addition to the Qwen family of large language models and large multimodal modles: Qwen2. Regarding post-training, all models underwent supervised fine-tuning and direct preference optimization (DPO, Rafailov et al., 2023), aligning them with human preferences through learning from human feedback."
    },
    {
      "source": "[sections/POST-TRAINING]",
      "quote": "Following extensive large-scale pre-training, we engage in a post-training phase for Qwen2. This process is pivotal in enhancing its proficiency across a broad spectrum of domains, including coding, mathematics, logical reasoning, instruction following, and multilingual comprehension."
    },
    {
      "source": "[sections/2412.15115]",
      "quote": "Qwen 2.5 introduces two significant advancements in its post-training design compared to Qwen 2: (1) Expanded Supervised Fine-tuning Data Coverage: The supervised fine-tuning process leverages a massive dataset comprising millions of high-quality examples."
    },
    {
      "source": "[sections/2412.15115]",
      "quote": "Long-sequence Generation: Qwen2.5 is capable of generating high-quality content with an output context length of up to 8,192 tokens, a significant advancement over the typical post-training response length, which often remains under 2,000 tokens."
    },
    {
      "source": "[sections/2412.15115]",
      "quote": "To enhance coding capabilities, we incorporate the instruction tuning data of Qwen2.5-Coder (Hui et al., 2024)."
    }
  ],
  "3-3 (Reinforcement Learning)__evidence": [
    {
      "source": "[sections/Post-training]",
      "quote": "The reinforcement learning (RL) process in Qwen 2.5 is divided into two distinct stages: Offline RL and Online RL."
    },
    {
      "source": "[sections/Conclusion]",
      "quote": "Qwen2.5 represents a significant advancement in large language models (LLMs), with enhanced pre-training on 18 trillion tokens and sophisticated post-training techniques, including supervised fine-tuning and multi-stage reinforcement learning."
    },
    {
      "source": "[sections/Reward Model]",
      "quote": "The reward model serves as the cornerstone for guiding RL processes, and thus we conduct a separate evaluation of the reward model used in the Qwen2.5 series."
    },
    {
      "source": "[sections/Reward Model]",
      "quote": "The Qwen2.5-RM-72B, leads in both the PPE and Human-Preference-Chinese evaluations, ranking second only to Athene-RM-70B on the RMB and achieving a performance level comparable to Nemotron-4-340B-Reward on the Reward Bench, albeit slightly behind Llama-3.1-Nemotron-70B-Reward."
    },
    {
      "source": "[sections/2407.10671]",
      "quote": "In this work, we introduce the newest addition to the Qwen family of large language models and large multimodal modles: Qwen2. Regarding post-training, all models underwent supervised fine-tuning and direct preference optimization (DPO, Rafailov et al., 2023), aligning them with human preferences through learning from human feedback."
    },
    {
      "source": "[sections/2412.15115]",
      "quote": "(2) Two-stage Reinforcement Learning: The reinforcement learning (RL) process in Qwen 2.5 is divided into two distinct stages: Offline RL and Online RL."
    },
    {
      "source": "[sections/5.2.3 Reward Model]",
      "quote": "The reward model serves as the cornerstone for guiding RL processes, and thus we conduct a separate evaluation of the reward model used in the Qwen2.5 series."
    }
  ],
  "4-1 (Pre-training Data)": "The pre-training pipeline for qwen2.5-72B-instruct is built on successive expansions of the core Qwen corpora.  Earlier Qwen1.5 models used 3 T tokens, Qwen2 models moved to more than 7 T tokens, and the new Qwen 2.5 generation scales this to 18 T tokens.  Across every stage the data are described as “high-quality, large-scale” and “multilingual”, with explicit emphasis on raising the proportion and quality of code and mathematics material.  All dense Qwen2 models—except the 0.5 B variant—were trained on the 7 T token mix; Qwen2-0.5 B was the only model trained on an even larger 12 T token set.  For qwen2.5-72B-instruct the 18 T-token corpus is further enriched by incorporating specialized slices from Qwen2.5-Math and Qwen2.5-Coder.  Synthetic augmentation is also mentioned: mathematics, code and knowledge examples are generated with Qwen2-72B-Instruct and Qwen2-Math-72B-Instruct before being inserted back into the pool.  Collectively, these steps produce what the team calls “a new, large-scale, high-quality multilingual dataset” whose scale and domain balance are repeatedly highlighted as the key driver of qwen2.5-72B-instruct’s improved capabilities.",
  "4-2 (Fine-tuning Data)": "For post-training, qwen2.5-72B-instruct relies on a multi-part supervised fine-tuning (SFT) stack.  All quoted material stresses that every Qwen2 and Qwen 2.5 model is first aligned with human preferences through SFT plus Direct Preference Optimization (DPO).  Instruction data are expanded using a “self-evolution” procedure: existing instructions are fed back through Qwen models, which automatically add new constraints or requirements, thereby boosting difficulty and diversity.  Data quality is reinforced by a human-in-the-loop stage in which multiple answers—generated with Qwen models of different sizes and decoding setups—are ranked by annotators to create both demonstration data and preference pairs.  Qwen 2.5 specifically “introduces two significant advancements”: (1) the SFT coverage now spans “millions of high-quality examples”, and (2) domain-focused additions are folded in.  Those domain additions include chain-of-thought corpora from Qwen2.5-Math (sourced from public math sets, K-12 collections and synthetic problems) and specialized instruction-tuning data from Qwen2.5-Coder to sharpen coding skills.  Altogether, the fine-tuning mixture is portrayed as broad (many domains), deep (millions of samples) and meticulously preference-ranked to keep qwen2.5-72B-instruct helpful, honest and harmless.",
  "4-3 (Reinforcement Learning Data)": "After SFT, reinforcement learning from human feedback is applied.  The quoted material states that the team “investigate methods to acquire high-quality demonstration and preference data for … RLHF, aiming to minimize the need for human labeling while maximizing quality”.  For qwen2.5-72B-instruct the RL phase is explicitly “two-stage”: an Offline RL pass followed by an Online RL pass.  Training focuses “solely on short instructions”, mirroring the strategy used for the rest of the Qwen2.5 family.  Reward modeling is critical: the reward model used in the Qwen2.5 series is benchmarked on Reward Bench, RMB, PPE and an internal Chinese preference set, establishing an empirical signal for policy optimization.  During training, responses are continuously sampled from checkpoints that have undergone SFT, DPO and earlier RL rounds; these responses are scored by the reward model to produce preference data for subsequent updates.  These mechanisms collectively supply the demonstration, preference and reward feedback that drive qwen2.5-72B-instruct toward better alignment with human values across coding, math, logical reasoning, instruction following and multilingual tasks.",
  "4-4 (Data Filtering)": "Data quality control is treated as a dedicated, multi-layer pipeline.  First, the developers describe “Better data filtering” in which Qwen2-Instruct models act as automatic quality filters, performing “comprehensive, multi-dimensional analysis” to score every training sample.  The same models are also used to “classify and balance content across different domains”, ensuring a controlled distribution in the final pre-training mix.  Concrete de-duplication rules are given: a candidate training sequence sₜ is dropped if there exists a test sequence sₑ such that |LCS(sₜ, sₑ)| ≥ 13 tokens AND the common subsequence covers at least 60 % of the shorter sequence (|LCS| ≥ 0.6 × min(|sₜ|,|sₑ|)).  Beyond generic filtering, domain-specific stages exist: synthesized math data pass through “rigorous filtering” by a proprietary reward model and the specialized Qwen2-Math-RM-72B; long-response datasets are built via back-translation and then pruned using Qwen2 scoring.  Overall, the workflow combines heuristic rules, explicit numeric thresholds, back-translation checks and multiple Qwen-based classifiers to strip low-quality, duplicated or off-distribution content before any text reaches the model, reinforcing the final performance of qwen2.5-72B-instruct.",
  "4-1 (Pre-training Data)__evidence": [
    {
      "source": "[pdf_text]",
      "quote": "All models were pre-trained on a high-quality, large-scale dataset comprising over 7 trillion tokens, covering a wide range of domains and languages. Compared to previous editions of Qwen, Qwen2 includes a broader spectrum of linguistic data, enhancing the quantity and quality of code and mathematics content."
    },
    {
      "source": "[sections/Pre-training Data]",
      "quote": "The pre-training of the Qwen2 models involves the development of a new, large-scale, high-quality multilingual dataset. This dataset represents an improvement over the corpora used in previous Qwen and Qwen1.5 models (Bai et al., 2023a; Qwen Team, 2024a), enhancing the scale, quality, and diversity of the pre-training data in several key areas:"
    },
    {
      "source": "[sections/Pre-training Data]",
      "quote": "Based on these enhancements, the pre-training data was expanded from 3 trillion tokens in Qwen1.5 (Qwen Team, 2024a) to 7 trillion tokens."
    },
    {
      "source": "[sections/Pre-training Data]",
      "quote": "All Qwen2 dense models, excluding Qwen2-0.5B, were pre-trained on this large-scale dataset of over 7 trillion tokens. Qwen2-0.5B were pre-trained using the 12 trillion token dataset."
    },
    {
      "source": "[abstract]",
      "quote": "Compared to previous iterations, Qwen 2.5 has been significantly improved during both the pre-training and post-training stages. In terms of pre-training, we have scaled the high-quality pre-training datasets from the previous 7 trillion tokens to 18 trillion tokens."
    },
    {
      "source": "[sections/3.1 Pre-training Data]",
      "quote": "During the pre-training phase of Qwen2.5, we incorporate training data from Qwen2.5-Math (Yang et al., 2024b) and Qwen2.5-Coder (Hui et al., 2024)."
    },
    {
      "source": "[sections/Conclusion]",
      "quote": "Qwen2.5 represents a significant advancement in large language models (LLMs), with enhanced pre-training on 18 trillion tokens and sophisticated post-training techniques, including supervised fine-tuning and multi-stage reinforcement learning."
    },
    {
      "source": "[sections/Introduction]",
      "quote": "Compared to previous editions of Qwen, Qwen2 includes a broader spectrum of linguistic data, enhancing the quantity and quality of code and mathematics content."
    },
    {
      "source": "[sections/Pre-training]",
      "quote": "The pre-training of the Qwen2 models involves the development of a new, large-scale, high-quality multilingual dataset."
    },
    {
      "source": "[sections/3.1 PRE-TRAINING DATA]",
      "quote": "The pre-training of the Qwen2 models involves the development of a new, large-scale, high-quality multilingual dataset."
    },
    {
      "source": "[sections/3.1 PRE-TRAINING DATA]",
      "quote": "Based on these enhancements, the pre-training data was expanded from 3 trillion tokens in Qwen1.5 (Qwen Team, 2024a) to 7 trillion tokens."
    },
    {
      "source": "[sections/3.1 PRE-TRAINING DATA]",
      "quote": "All Qwen2 dense models, excluding Qwen2-0.5B, were pre-trained on this large-scale dataset of over 7 trillion tokens."
    },
    {
      "source": "[sections/2412.15115]",
      "quote": "Qwen2.5 demonstrates significant enhancements in pre-training data quality compared to its predecessor Qwen2."
    },
    {
      "source": "[sections/2412.15115]",
      "quote": "Qwen2.5, which leverages 18 trillion tokens for pre-training, has demonstrated the most advanced capabilities within the Qwen series, especially in terms of domain expertise, underscoring the importance of scale together with mixture in enhancing the model’s capabilities."
    },
    {
      "source": "[sections/2412.15115]",
      "quote": "Building on these techniques, we have developed a larger and higher-quality pre-training dataset, expanding from the 7 trillion tokens used in Qwen2 (Yang et al., 2024a) to 18 trillion tokens."
    },
    {
      "source": "[sections/2412.15115]",
      "quote": "During the pre-training phase of Qwen2.5, we incorporate training data from Qwen2.5-Math (Yang et al., 2024b) and Qwen2.5-Coder (Hui et al., 2024)."
    },
    {
      "source": "[sections/2412.15115]",
      "quote": "To generate high-quality synthetic data, particularly in mathematics, code, and knowledge domains, we leverage both Qwen2-72B-Instruct (Yang et al., 2024a) and Qwen2-Math-72B-Instruct (Qwen Team, 2024c)."
    }
  ],
  "4-2 (Fine-tuning Data)__evidence": [
    {
      "source": "[pdf_text]",
      "quote": "Compared to previous editions of Qwen, Qwen2 includes a broader spectrum of linguistic data, enhancing the quantity and quality of code and mathematics content. Regarding post-training, all models underwent supervised fine-tuning and direct preference optimization (DPO, Rafailov et al., 2023), aligning them with human preferences through learning from human feedback."
    },
    {
      "source": "[sections/Collaborative Data Annotation]",
      "quote": "Instruction Evolution To enrich the instruction dataset, a self-evolution strategy (Zhao et al., 2024) is employed, prompting the Qwen models to add constraints or requirements to existing instructions, thereby increasing their complexity and ensuring a diverse range of difficulty levels within the dataset."
    },
    {
      "source": "[sections/Collaborative Data Annotation]",
      "quote": "Multiple responses to an instruction are obtained using diverse generation strategies and Qwen models of different scales. Annotators rank these responses based on their preferences, ensuring the best response meets established criteria, yielding both demonstration and preference data."
    },
    {
      "source": "[sections/4 Post-training]",
      "quote": "Qwen 2.5 introduces two significant advancements in its post-training design compared to Qwen 2: (1) Expanded Supervised Fine-tuning Data Coverage: The supervised fine-tuning process leverages a massive dataset comprising millions of high-quality examples."
    },
    {
      "source": "[sections/4.1 Supervised Fine-tuning]",
      "quote": "We introduce the chain-of-thought data of Qwen2.5-Math (Yang et al., 2024b), which encompasses a diverse range of query sources, including public datasets, K-12 problem collections, and synthetic problems."
    },
    {
      "source": "[sections/4.1 Supervised Fine-tuning]",
      "quote": "To enhance coding capabilities, we incorporate the instruction tuning data of Qwen2.5-Coder (Hui et al., 2024)."
    },
    {
      "source": "[sections/Conclusion]",
      "quote": "Qwen2.5 represents a significant advancement in large language models (LLMs), with enhanced pre-training on 18 trillion tokens and sophisticated post-training techniques, including supervised fine-tuning and multi-stage reinforcement learning."
    },
    {
      "source": "[sections/4.1.1 COLLABORATIVE DATA ANNOTATION]",
      "quote": "To enrich the instruction dataset, a self-evolution strategy (Zhao et al., 2024) is employed, prompting the Qwen models to add constraints or requirements to existing instructions, thereby increasing their complexity and ensuring a diverse range of difficulty levels within the dataset."
    },
    {
      "source": "[sections/4.1.1 COLLABORATIVE DATA ANNOTATION]",
      "quote": "Human Annotation Multiple responses to an instruction are obtained using diverse generation strategies and Qwen models of different scales. Annotators rank these responses based on their preferences, ensuring the best response meets established criteria, yielding both demonstration and preference data."
    },
    {
      "source": "[sections/2412.15115]",
      "quote": "Qwen 2.5 introduces two significant advancements in its post-training design compared to Qwen 2: (1) Expanded Supervised Fine-tuning Data Coverage: The supervised fine-tuning process leverages a massive dataset comprising millions of high-quality examples."
    },
    {
      "source": "[sections/2412.15115]",
      "quote": "We introduce the chain-of-thought data of Qwen2.5-Math (Yang et al., 2024b), which encompasses a diverse range of query sources, including public datasets, K-12 problem collections, and synthetic problems."
    },
    {
      "source": "[sections/2412.15115]",
      "quote": "To enhance coding capabilities, we incorporate the instruction tuning data of Qwen2.5-Coder (Hui et al., 2024)."
    }
  ],
  "4-3 (Reinforcement Learning Data)__evidence": [
    {
      "source": "[pdf_text]",
      "quote": "Compared to previous editions of Qwen, Qwen2 includes a broader spectrum of linguistic data, enhancing the quantity and quality of code and mathematics content. Regarding post-training, all models underwent supervised fine-tuning and direct preference optimization (DPO, Rafailov et al., 2023), aligning them with human preferences through learning from human feedback."
    },
    {
      "source": "[sections/Post-Training]",
      "quote": "Following extensive large-scale pre-training, we engage in a post-training phase for Qwen2. Specifically, we investigate methods to acquire high-quality demonstration and preference data for Supervised Fine-Tuning (SFT) and Reinforcement Learning from Human Feedback (RLHF), aiming to minimize the need for human labeling while maximizing the quality and reliability of the data."
    },
    {
      "source": "[sections/4 Post-training]",
      "quote": "Two-stage Reinforcement Learning: The reinforcement learning (RL) process in Qwen 2.5 is divided into two distinct stages: Offline RL and Online RL."
    },
    {
      "source": "[sections/4.4 Long Context Fine-tuning]",
      "quote": "During the RL stage, we use a training strategy similar to that used for the other Qwen2.5 models, focusing solely on short instructions."
    },
    {
      "source": "[sections/5.2.3 Reward Model]",
      "quote": "The reward model serves as the cornerstone for guiding RL processes, and thus we conduct a separate evaluation of the reward model used in the Qwen2.5 series. Our assessment benchmarks encompass Reward Bench (Lambert et al., 2024), RMB (Zhou et al., 2024), PPE (Frick et al., 2024b), and an internally collected out-of-domain Chinese human preference benchmark (Human-Preference-Chinese) to provide a comprehensive analysis."
    },
    {
      "source": "[sections/4 POST-TRAINING]",
      "quote": "Following extensive large-scale pre-training, we engage in a post-training phase for Qwen2. This process is pivotal in enhancing its proficiency across a broad spectrum of domains, including coding, mathematics, logical reasoning, instruction following, and multilingual comprehension. Moreover, it ensures that the generation from the models is in harmony with human values, making it helpful, honest, and harmless. Specifically, we investigate methods to acquire high-quality demonstration and preference data for Supervised Fine-Tuning (SFT) and Reinforcement Learning from Human Feedback (RLHF), aiming to minimize the need for human labeling while maximizing the quality and reliability of the data."
    },
    {
      "source": "[sections/2412.15115]",
      "quote": "The reinforcement learning (RL) process in Qwen 2.5 is divided into two distinct stages: Offline RL and Online RL."
    },
    {
      "source": "[sections/2412.15115]",
      "quote": "Responses are generated from checkpoints of the Qwen models, which have been fine-tuned using different methods—SFT, DPO, and RL—at various stages of training."
    }
  ],
  "4-4 (Data Filtering)__evidence": [
    {
      "source": "[sections/Pre-training Data]",
      "quote": "Quality Enhancement The filtering algorithm has been refined with additional heuristic and model-based methods, including the use of the Qwen models to filter out low-quality data."
    },
    {
      "source": "[sections/3.1 Pre-training Data]",
      "quote": "Better data filtering. High-quality pre-training data is crucial for model performance, making data quality assessment and filtering a critical component of our pipeline. We leverage Qwen2-Instruct models as data quality filters that perform comprehensive, multi-dimensional analysis to evaluate and score training samples."
    },
    {
      "source": "[sections/3.1 Pre-training Data]",
      "quote": "To optimize the pre-training data distribution, we employ Qwen2-Instruct models to classify and balance content across different domains."
    },
    {
      "source": "[sections/5 Evaluation]",
      "quote": "Following the criteria used in Qwen2, a training sequence sₜ is removed from the training data if there exists a test sequence sₑ such that the length of the longest common subsequence (LCS) between tokenized sₜ and sₑ satisfies both |LCS(sₜ, sₑ)| ≥ 13 and |LCS(sₜ, sₑ)| ≥ 0.6 × min(|sₜ|, |sₑ|)."
    },
    {
      "source": "[sections/3.1 PRE-TRAINING DATA]",
      "quote": "Quality Enhancement The filtering algorithm has been refined with additional heuristic and model-based methods, including the use of the Qwen models to filter out low-quality data."
    },
    {
      "source": "[sections/2412.15115]",
      "quote": "We leverage Qwen2-Instruct models as data quality filters that perform comprehensive, multi-dimensional analysis to evaluate and score training samples."
    },
    {
      "source": "[sections/2412.15115]",
      "quote": "The quality of this synthesized data is further enhanced through rigorous filtering using our proprietary general reward model and the specialized Qwen2-Math-RM-72B (Qwen Team, 2024c) model."
    },
    {
      "source": "[sections/2412.15115]",
      "quote": "we develop long-response datasets (Quan et al., 2024). We employ back-translation techniques to generate queries for long-text data from pre-training corpora, impose output length constraints, and use Qwen2 to filter out low-quality paired data."
    }
  ],
  "__usage": {
    "fine_tuning": "used",
    "rl": "used"
  }
}