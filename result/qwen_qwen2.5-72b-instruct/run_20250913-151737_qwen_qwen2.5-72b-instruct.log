
======== 1/2 ▶ Qwen/Qwen2.5-72B-Instruct ========
📁 Directory to create/use: qwen_qwen2.5-72b-instruct
📁 Output path: qwen_qwen2.5-72b-instruct
1️⃣ HF: True, GH: False
🔎 Candidate rejected: inferless/Qwen2-72B-Instruct (score=-10, detail={'org_affinity': -6, 'name_hits': 5, 'readme_hits': 5, 'path_hits': 0, 'bad_keywords': 8, 'from_hf_link': 0, 'version_conflict': -8})
🔎 Candidate rejected: vllm-project/vllm (score=-10, detail={'org_affinity': -6, 'name_hits': 0, 'readme_hits': 0, 'path_hits': 3, 'bad_keywords': 14, 'from_hf_link': 0, 'version_conflict': 0})
✅ Resolved GH repo: QwenLM/Qwen2.5 (score=22, detail={'org_affinity': 9, 'name_hits': 2, 'readme_hits': 3, 'path_hits': 3, 'bad_keywords': 20, 'from_hf_link': 1, 'version_conflict': 0})
✅ HF model: qwen/qwen2.5-72b-instruct (found at priority: 1)
📄 Reports saved/merged (HF): qwen_qwen2.5-72b-instruct\reports_fulltext_huggingface_qwen_qwen2.5-72b-instruct.json
✅ JSON file saved: qwen_qwen2.5-72b-instruct\huggingface_qwen_qwen2.5-72b-instruct.json
📄 Reports merged to: qwen_qwen2.5-72b-instruct\reports_fulltext_qwen_qwen2.5-72b-instruct.json (HF sources)
✅ Saved group 1 result: qwen_qwen2.5-72b-instruct\huggingface_filtered_qwen_qwen2.5-72b-instruct_1.json
✅ Saved group 2 result: qwen_qwen2.5-72b-instruct\huggingface_filtered_qwen_qwen2.5-72b-instruct_2.json
✅ Saved group 3 result: qwen_qwen2.5-72b-instruct\huggingface_filtered_qwen_qwen2.5-72b-instruct_3.json
✅ Saved group 4 result: qwen_qwen2.5-72b-instruct\huggingface_filtered_qwen_qwen2.5-72b-instruct_4.json
✅ Saved final merged result: qwen_qwen2.5-72b-instruct\huggingface_filtered_final_qwen_qwen2.5-72b-instruct.json
✅ GH repo: QwenLM/Qwen2.5
📄 Reports saved/merged (GH): qwen_qwen2.5-72b-instruct\reports_fulltext_github_qwenlm_qwen2.5.json
ℹ️ Merge skipped: could not determine a single target HF model id.
✅ GitHub JSON file saved: qwen_qwen2.5-72b-instruct\github_QwenLM_Qwen2.5.json
evidence counts before/after model-guard: {'raw': {'1-1 (Weights)': 7, '1-2 (Code)': 1, '1-3 (License)': 4, '1-4 (Paper)': 3}, 'kept': {'1-1 (Weights)': 7, '1-2 (Code)': 1, '1-3 (License)': 4, '1-4 (Paper)': 2}}
✅ Saved group 1 result: qwen_qwen2.5-72b-instruct\github_filtered_QwenLM_Qwen2.5_1.json
evidence counts before/after model-guard: {'raw': {'1-5 (Architecture)': 2, '1-6 (Tokenizer)': 3, '2-1 (Hardware)': 1, '2-2 (Software)': 2}, 'kept': {'1-5 (Architecture)': 2, '1-6 (Tokenizer)': 1, '2-1 (Hardware)': 0, '2-2 (Software)': 2}}
✅ Saved group 2 result: qwen_qwen2.5-72b-instruct\github_filtered_QwenLM_Qwen2.5_2.json
evidence counts before/after model-guard: {'raw': {'2-3 (API)': 2, '3-1 (Pre-training)': 0, '3-2 (Fine-tuning)': 0, '3-3 (Reinforcement Learning)': 0}, 'kept': {'2-3 (API)': 1, '3-1 (Pre-training)': 0, '3-2 (Fine-tuning)': 0, '3-3 (Reinforcement Learning)': 0}}
✅ Saved group 3 result: qwen_qwen2.5-72b-instruct\github_filtered_QwenLM_Qwen2.5_3.json
evidence counts before/after model-guard: {'raw': {'4-1 (Pre-training Data)': 0, '4-2 (Fine-tuning Data)': 0, '4-3 (Reinforcement Learning Data)': 0, '4-4 (Data Filtering)': 0}, 'kept': {'4-1 (Pre-training Data)': 0, '4-2 (Fine-tuning Data)': 0, '4-3 (Reinforcement Learning Data)': 0, '4-4 (Data Filtering)': 0}}
✅ Saved group 4 result: qwen_qwen2.5-72b-instruct\github_filtered_QwenLM_Qwen2.5_4.json
✅ Saved final merged result: qwen_qwen2.5-72b-instruct\github_filtered_final_QwenLM_Qwen2.5.json
🔎 HF tags found arXiv IDs: ['2309.00071', '2407.10671']
🔄 Simplified query: 'qwen2.5'
🔎 Tavily search: qwen2.5 paper
  → arXiv link found: https://arxiv.org/abs/2502.13923
🔎 Tavily search: qwen2.5 technical report
  → arXiv link found: https://arxiv.org/abs/2412.15115
🛰️ Tavily candidates: ['2502.13923', '2412.15115']
🔬 Verifying 2 Tavily candidate(s) with GPT…
  • Candidate: 2502.13923
    - GPT verdict: ❌ no match (The candidate paper is the technical report for Qwen2.5-VL, a vision-language variant. The target model is Qwen2.5-72B instruct, which is a different variant. Even though the major version (2.5) match)
  • Candidate: 2412.15115
    - GPT verdict: ✅ match (The candidate paper is explicitly the technical report for Qwen2.5, presenting detailed information on the Qwen2.5 series including the flagship Qwen2.5-72B-Instruct model. The version (2.5) exactly m)
✅ GPT-verified IDs: ['2412.15115']
📦 Final merged arXiv IDs: ['2309.00071', '2407.10671', '2412.15115']
📄 PDF saved: qwen_qwen2.5-72b-instruct\arxiv_2309.00071.pdf
📄 PDF saved: qwen_qwen2.5-72b-instruct\arxiv_2407.10671.pdf
📄 PDF saved: qwen_qwen2.5-72b-instruct\arxiv_2412.15115.pdf
✅ Full paper text saved: qwen_qwen2.5-72b-instruct\arxiv_fulltext_qwen_qwen2.5-72b-instruct.json
📄 Reports merged to: qwen_qwen2.5-72b-instruct\reports_fulltext_qwen_qwen2.5-72b-instruct.json
🔎 recall keys: ['1-1 (Weights)', '1-2 (Code)', '1-3 (License)', '1-4 (Paper)']
🔎 recall keys: ['1-1 (Weights)', '1-2 (Code)', '1-3 (License)', '1-4 (Paper)']
🔎 recall keys: ['1-1 (Weights)', '1-2 (Code)', '1-3 (License)', '1-4 (Paper)']
🔎 recall keys: ['1-1 (Weights)', '1-2 (Code)', '1-3 (License)', '1-4 (Paper)']
🔎 recall keys: ['1-1 (Weights)', '1-2 (Code)', '1-3 (License)', '1-4 (Paper)']
🔎 recall keys: ['1-1 (Weights)', '1-2 (Code)', '1-3 (License)', '1-4 (Paper)']
🔎 recall keys: ['1-1 (Weights)', '1-2 (Code)', '1-3 (License)', '1-4 (Paper)']
🔎 recall keys: ['1-1 (Weights)', '1-2 (Code)', '1-3 (License)', '1-4 (Paper)']
evidence counts before/after model-guard: {'raw': {'1-1 (Weights)': 10, '1-2 (Code)': 3, '1-3 (License)': 2, '1-4 (Paper)': 12}, 'kept': {'1-1 (Weights)': 7, '1-2 (Code)': 3, '1-3 (License)': 2, '1-4 (Paper)': 8}}
✅ Saved group 1 : qwen_qwen2.5-72b-instruct\arxiv_filtered_qwen_qwen2.5-72b-instruct_1.json
🔎 recall keys: ['1-5 (Architecture)', '1-6 (Tokenizer)', '2-1 (Hardware)', '2-2 (Software)']
🔎 recall keys: ['1-5 (Architecture)', '1-6 (Tokenizer)', '2-1 (Hardware)', '2-2 (Software)']
🔎 recall keys: ['1-5 (Architecture)', '1-6 (Tokenizer)', '2-1 (Hardware)', '2-2 (Software)']
🔎 recall keys: ['1-5 (Architecture)', '1-6 (Tokenizer)', '2-1 (Hardware)', '2-2 (Software)']
🔎 recall keys: ['1-5 (Architecture)', '1-6 (Tokenizer)', '2-1 (Hardware)', '2-2 (Software)']
🔎 recall keys: ['1-5 (Architecture)', '1-6 (Tokenizer)', '2-1 (Hardware)', '2-2 (Software)']
🔎 recall keys: ['1-5 (Architecture)', '1-6 (Tokenizer)', '2-1 (Hardware)', '2-2 (Software)']
🔎 recall keys: ['1-5 (Architecture)', '1-6 (Tokenizer)', '2-1 (Hardware)', '2-2 (Software)']
evidence counts before/after model-guard: {'raw': {'1-5 (Architecture)': 23, '1-6 (Tokenizer)': 4, '2-1 (Hardware)': 0, '2-2 (Software)': 0}, 'kept': {'1-5 (Architecture)': 21, '1-6 (Tokenizer)': 4, '2-1 (Hardware)': 0, '2-2 (Software)': 0}}
✅ Saved group 2 : qwen_qwen2.5-72b-instruct\arxiv_filtered_qwen_qwen2.5-72b-instruct_2.json
🔎 recall keys: ['2-3 (API)', '3-1 (Pre-training)', '3-2 (Fine-tuning)', '3-3 (Reinforcement Learning)']
🔎 recall keys: ['2-3 (API)', '3-1 (Pre-training)', '3-2 (Fine-tuning)', '3-3 (Reinforcement Learning)']
🔎 recall keys: ['2-3 (API)', '3-1 (Pre-training)', '3-2 (Fine-tuning)', '3-3 (Reinforcement Learning)']
🔎 recall keys: ['2-3 (API)', '3-1 (Pre-training)', '3-2 (Fine-tuning)', '3-3 (Reinforcement Learning)']
🔎 recall keys: ['2-3 (API)', '3-1 (Pre-training)', '3-2 (Fine-tuning)', '3-3 (Reinforcement Learning)']
🔎 recall keys: ['2-3 (API)', '3-1 (Pre-training)', '3-2 (Fine-tuning)', '3-3 (Reinforcement Learning)']
🔎 recall keys: ['2-3 (API)', '3-1 (Pre-training)', '3-2 (Fine-tuning)', '3-3 (Reinforcement Learning)']
🔎 recall keys: ['2-3 (API)', '3-1 (Pre-training)', '3-2 (Fine-tuning)', '3-3 (Reinforcement Learning)']
evidence counts before/after model-guard: {'raw': {'2-3 (API)': 5, '3-1 (Pre-training)': 20, '3-2 (Fine-tuning)': 10, '3-3 (Reinforcement Learning)': 9}, 'kept': {'2-3 (API)': 5, '3-1 (Pre-training)': 19, '3-2 (Fine-tuning)': 9, '3-3 (Reinforcement Learning)': 7}}
✅ Saved group 3 : qwen_qwen2.5-72b-instruct\arxiv_filtered_qwen_qwen2.5-72b-instruct_3.json
🔎 recall keys: ['4-1 (Pre-training Data)', '4-2 (Fine-tuning Data)', '4-3 (Reinforcement Learning Data)', '4-4 (Data Filtering)']
🔎 recall keys: ['4-1 (Pre-training Data)', '4-2 (Fine-tuning Data)', '4-3 (Reinforcement Learning Data)', '4-4 (Data Filtering)']
🔎 recall keys: ['4-1 (Pre-training Data)', '4-2 (Fine-tuning Data)', '4-3 (Reinforcement Learning Data)', '4-4 (Data Filtering)']
🔎 recall keys: ['4-1 (Pre-training Data)', '4-2 (Fine-tuning Data)', '4-3 (Reinforcement Learning Data)', '4-4 (Data Filtering)']
🔎 recall keys: ['4-1 (Pre-training Data)', '4-2 (Fine-tuning Data)', '4-3 (Reinforcement Learning Data)', '4-4 (Data Filtering)']
🔎 recall keys: ['4-1 (Pre-training Data)', '4-2 (Fine-tuning Data)', '4-3 (Reinforcement Learning Data)', '4-4 (Data Filtering)']
🔎 recall keys: ['4-1 (Pre-training Data)', '4-2 (Fine-tuning Data)', '4-3 (Reinforcement Learning Data)', '4-4 (Data Filtering)']
🔎 recall keys: ['4-1 (Pre-training Data)', '4-2 (Fine-tuning Data)', '4-3 (Reinforcement Learning Data)', '4-4 (Data Filtering)']
evidence counts before/after model-guard: {'raw': {'4-1 (Pre-training Data)': 19, '4-2 (Fine-tuning Data)': 13, '4-3 (Reinforcement Learning Data)': 9, '4-4 (Data Filtering)': 9}, 'kept': {'4-1 (Pre-training Data)': 17, '4-2 (Fine-tuning Data)': 12, '4-3 (Reinforcement Learning Data)': 8, '4-4 (Data Filtering)': 8}}
✅ Saved group 4 : qwen_qwen2.5-72b-instruct\arxiv_filtered_qwen_qwen2.5-72b-instruct_4.json
✅ Saved final merged: qwen_qwen2.5-72b-instruct\arxiv_filtered_final_qwen_qwen2.5-72b-instruct.json
evidence counts before/after model-guard: {'raw': {'1-1 (Weights)': 7, '1-2 (Code)': 1, '1-3 (License)': 2, '1-4 (Paper)': 17}, 'kept': {'1-1 (Weights)': 7, '1-2 (Code)': 1, '1-3 (License)': 2, '1-4 (Paper)': 13}}
✅ Saved group 1 : qwen_qwen2.5-72b-instruct\reports_filtered_qwen_qwen2.5-72b-instruct_1.json
evidence counts before/after model-guard: {'raw': {'1-5 (Architecture)': 12, '1-6 (Tokenizer)': 1, '2-1 (Hardware)': 0, '2-2 (Software)': 0}, 'kept': {'1-5 (Architecture)': 12, '1-6 (Tokenizer)': 1, '2-1 (Hardware)': 0, '2-2 (Software)': 0}}
✅ Saved group 2 : qwen_qwen2.5-72b-instruct\reports_filtered_qwen_qwen2.5-72b-instruct_2.json
evidence counts before/after model-guard: {'raw': {'2-3 (API)': 9, '3-1 (Pre-training)': 10, '3-2 (Fine-tuning)': 8, '3-3 (Reinforcement Learning)': 3}, 'kept': {'2-3 (API)': 9, '3-1 (Pre-training)': 10, '3-2 (Fine-tuning)': 8, '3-3 (Reinforcement Learning)': 3}}
✅ Saved group 3 : qwen_qwen2.5-72b-instruct\reports_filtered_qwen_qwen2.5-72b-instruct_3.json
evidence counts before/after model-guard: {'raw': {'4-1 (Pre-training Data)': 11, '4-2 (Fine-tuning Data)': 0, '4-3 (Reinforcement Learning Data)': 1, '4-4 (Data Filtering)': 2}, 'kept': {'4-1 (Pre-training Data)': 11, '4-2 (Fine-tuning Data)': 0, '4-3 (Reinforcement Learning Data)': 1, '4-4 (Data Filtering)': 2}}
✅ Saved group 4 : qwen_qwen2.5-72b-instruct\reports_filtered_qwen_qwen2.5-72b-instruct_4.json
✅ Saved final merged: qwen_qwen2.5-72b-instruct\reports_filtered_final_qwen_qwen2.5-72b-instruct.json
🧱 Pretrained (base) model found by heuristic: Qwen/Qwen2.5-72B
📄 Reports saved/merged (HF): qwen_qwen2.5-72b-instruct\reports_fulltext_huggingface_qwen_qwen2.5-72b.json
✅ JSON file saved: qwen_qwen2.5-72b-instruct\huggingface_Qwen_Qwen2.5-72B.json
✅ Saved qwen_qwen2.5-72b-instruct\pretrain_hf_qwen_qwen2.5-72b.json
🔎 Candidate rejected: vllm-project/vllm (score=-10, detail={'org_affinity': -6, 'name_hits': 0, 'readme_hits': 0, 'path_hits': 2, 'bad_keywords': 15, 'from_hf_link': 0, 'version_conflict': 0})
✅ Resolved GH repo: QwenLM/Qwen2.5 (score=22, detail={'org_affinity': 9, 'name_hits': 2, 'readme_hits': 2, 'path_hits': 2, 'bad_keywords': 20, 'from_hf_link': 1, 'version_conflict': 0})
📄 Reports saved/merged (GH): qwen_qwen2.5-72b-instruct\reports_fulltext_github_qwenlm_qwen2.5.json
ℹ️ Merge skipped: could not determine a single target HF model id.
✅ GitHub JSON file saved: qwen_qwen2.5-72b-instruct\github_QwenLM_Qwen2.5.json
⚠️ GH fetch/dispatch failed: cannot import name 'filter_pretrain_gh' from 'pretrain_github_Dispatcher' (c:\Users\user\Desktop\lab git\lab git\Evaluating-system-based-on-agent\code\pretrain_github_Dispatcher.py)
🔎 HF tags found arXiv IDs: ['2407.10671']
🔄 Simplified query: 'Qwen2.5'
🔎 Tavily search: Qwen2.5 paper
  → arXiv link found: https://arxiv.org/abs/2502.13923
🔎 Tavily search: Qwen2.5 technical report
  → arXiv link found: https://arxiv.org/abs/2412.15115
🛰️ Tavily candidates: ['2502.13923', '2412.15115']
🔬 Verifying 2 Tavily candidate(s) with GPT…
  • Candidate: 2502.13923
    - GPT verdict: ❌ no match (The candidate paper is the technical report for Qwen2.5-VL (the vision-language variant), while the target model is identified as Qwen2.5-72B without the VL designation. Although the major version (2.)
  • Candidate: 2412.15115
    - GPT verdict: ✅ match (The report is explicitly titled as the Qwen2.5 Technical Report and details multiple Qwen2.5 models including the 72B variant. Since the target is Qwen2.5-72B and the version (2.5) matches exactly, it)
✅ GPT-verified IDs: ['2412.15115']
📦 Final merged arXiv IDs: ['2407.10671', '2412.15115']
📄 PDF saved: qwen_qwen2.5-72b-instruct\arxiv_2407.10671.pdf
📄 PDF saved: qwen_qwen2.5-72b-instruct\arxiv_2412.15115.pdf
✅ Full paper text saved: qwen_qwen2.5-72b-instruct\arxiv_fulltext_qwen_qwen2.5-72b.json
✅ Saved: qwen_qwen2.5-72b-instruct\pretrain_arxiv_qwen_qwen2.5-72b.json
✅ Saved pretrain reports: qwen_qwen2.5-72b-instruct\pretrain_reports_qwen_qwen2.5-72b.json
📝 Starting openness evaluation...
📝 Saved evaluation result: qwen_qwen2.5-72b-instruct\openness_score_qwen_qwen2.5-72b-instruct.json
✅ Openness evaluation complete. Result file: qwen_qwen2.5-72b-instruct\openness_score_qwen_qwen2.5-72b-instruct.json
✅ Saved model ID: qwen_qwen2.5-72b-instruct\identified_model.txt
⏳ **Time taken for this model: 4435.76 seconds**
🧾 Log saved to: qwen_qwen2.5-72b-instruct\run_20250913-151737_qwen_qwen2.5-72b-instruct.log
