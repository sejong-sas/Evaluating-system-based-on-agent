{
  "model_id": "qwen/qwen2.5-72b-instruct",
  "full_texts": [
    {
      "arxiv_id": "https://qwenlm.github.io/blog/qwen2.5/",
      "full_text": " Qwen2.5: A Party of Foundation Models! | Qwen Blog Publication About Try Qwen Chat &nbsp; Qwen2.5: A Party of Foundation Models! September 19, 2024 &nbsp;·&nbsp;9 min&nbsp;·&nbsp;1738 words&nbsp;·&nbsp;Qwen Team&nbsp;|&nbsp;Translations: 简体中文 GITHUB HUGGING FACE MODELSCOPE DEMO DISCORD Introduction # In the past three months since Qwen2&rsquo;s release, numerous developers have built new models on the Qwen2 language models, providing us with valuable feedback. During this period, we have focused on creating smarter and more knowledgeable language models. Today, we are excited to introduce the latest addition to the Qwen family: Qwen2.5 . We are announcing what might be the largest opensource release in history! Let&rsquo;s get the party started! Our latest release features the LLMs Qwen2.5 , along with specialized models for coding, Qwen2.5-Coder , and mathematics, Qwen2.5-Math . All open-weight models are dense, decoder-only language models, available in various sizes, including: Qwen2.5: 0.5B, 1.5B, 3B, 7B, 14B, 32B, and 72B Qwen2.5-Coder: 1.5B, 7B, and 32B on the way Qwen2.5-Math: 1.5B, 7B, and 72B. All our open-source models, except for the 3B and 72B variants, are licensed under Apache 2.0. You can find the license files in the respective Hugging Face repositories. In addition to these models, we offer APIs for our flagship language models: Qwen-Plus and Qwen-Turbo through Model Studio, and we encourage you to explore them! Furthermore, we have also open-sourced the Qwen2-VL-72B , which features performance enhancements compared to last month&rsquo;s release. For more details about Qwen2.5, Qwen2.5-Coder, and Qwen2.5-Math, feel free to visit the following links: Qwen2.5 LLM Qwen2.5-Coder Qwen2.5-Math Get ready to unlock a world of possibilities with our extensive lineup of models! We&rsquo;re excited to share these cutting-edge models with you, and we can&rsquo;t wait to see the incredible things you&rsquo;ll achieve with them! Takeaways # In terms of Qwen2.5 , the language models, all models are pretrained on our latest large-scale dataset, encompassing up to 18 trillion tokens. Compared to Qwen2, Qwen2.5 has acquired significantly more knowledge (MMLU: 85+) and has greatly improved capabilities in coding (HumanEval 85+) and mathematics (MATH 80+). Additionally, the new models achieve significant improvements in instruction following, generating long texts (over 8K tokens), understanding structured data (e.g, tables), and generating structured outputs especially JSON. Qwen2.5 models are generally more resilient to the diversity of system prompts, enhancing role-play implementation and condition-setting for chatbots. Like Qwen2, the Qwen2.5 language models support up to 128K tokens and can generate up to 8K tokens. They also maintain multilingual support for over 29 languages, including Chinese, English, French, Spanish, Portuguese, German, Italian, Russian, Japanese, Korean, Vietnamese, Thai, Arabic, and more. Below, we provide basic information about the models and details of the supported languages. The specialized expert language models, namely Qwen2.5-Coder for coding and Qwen2.5-Math for mathematics, have undergone substantial enhancements compared to their predecessors, CodeQwen1.5 and Qwen2-Math. Specifically, Qwen2.5-Coder has been trained on 5.5 trillion tokens of code-related data, enabling even smaller coding-specific models to deliver competitive performance against larger language models on coding evaluation benchmarks. Meanwhile, Qwen2.5-Math supports both Chinese and English and incorporates various reasoning methods, including Chain-of-Thought (CoT), Program-of-Thought (PoT), and Tool-Integrated Reasoning (TIR). Performance # Qwen2.5 # To showcase Qwen2.5&rsquo;s capabilities, we benchmark our largest open-source model, Qwen2.5-72B - a 72B-parameter dense decoder-only language model - against leading open-source models like Llama-3.1-70B and Mistral-Large-V2. We present comprehensive results from instruction-tuned versions across various benchmarks, evaluating both model capabilities and human preferences. Besides the instruction-tuned language models, we figure out that the base language model of our flagship opensource model Qwen2.5-72B reaches top-tier performance even against larger models like Llama-3-405B. Furthermore, we benchmark the latest version of our API-based model, Qwen-Plus , against leading proprietary and open-source models, including GPT4-o, Claude-3.5-Sonnet, Llama-3.1-405B, and DeepSeek-V2.5. This comparison showcases Qwen-Plus&rsquo;s competitive standing in the current landscape of large language models. We show that Qwen-Plus significantly outcompetes DeepSeek-V2.5 and demonstrates competitive performance against Llama-3.1-405B, while still underperforming compared to GPT4-o and Claude-3.5-Sonnet in some aspects. This benchmarking not only highlights Qwen-Plus&rsquo;s strengths but also identifies areas for future improvement, reinforcing our commitment to continuous enhancement and innovation in the field of large language models. A significant update in Qwen2.5 is the reintroduction of our 14B and 32B models, Qwen2.5-14B and Qwen2.5-32B . These models outperform baseline models of comparable or larger sizes, such as Phi-3.5-MoE-Instruct and Gemma2-27B-IT, across diverse tasks. They achieve an optimal balance between model size and capability, delivering performance that matches or exceeds some larger models. Additionally, our API-based model, Qwen-Turbo , offers highly competitive performance compared to the two open-source models, while providing a cost-effective and rapid service. In recent times, there has been a notable shift towards small language models (SLMs). Although SLMs have historically trailed behind their larger counterparts (LLMs), the performance gap is rapidly diminishing. Remarkably, even models with just 3 billion parameters are now delivering highly competitive results. The accompanying figure illustrates a significant trend: newer models achieving scores above 65 in MMLU are increasingly smaller, underscoring the accelerated growth in knowledge density among language models. Notably, our Qwen2.5-3B stands out as a prime example, achieving impressive performance with only around 3 billion parameters, showcasing its efficiency and capability compared to its predecessors. In addition to the notable enhancements in benchmark evaluations, we have refined our post-training methodologies. Our four key updates include support for long text generation of up to 8K tokens, significantly improved comprehension of structured data, more reliable generation of structured outputs, particularly in JSON format, and enhanced performance across diverse system prompts, which facilitates effective role-playing. Check the LLM blog for details about how to leverage these capabilities. Qwen2.5-Coder # Since the launch of CodeQwen1.5, we have attracted numerous users who rely on this model for various coding tasks, such as debugging, answering coding-related questions, and providing code suggestions. Our latest iteration, Qwen2.5-Coder, is specifically designed for coding applications. In this section, we present the performance results of Qwen2.5-Coder-7B-Instruct, benchmarked against leading open-source models, including those with significantly larger parameter sizes. We believe that Qwen2.5-Coder is an excellent choice as your personal coding assistant. Despite its smaller size, it outperforms many larger language models across a range of programming languages and tasks, demonstrating its exceptional coding capabilities. Qwen2.5-Math # In terms of the math specific language models, we released the first models, Qwen2-Math, last month, and this time, compared to Qwen2-Math, Qwen2.5-Math has been pretrained larger-scale of math related data, including the synthetic data generated by Qwen2-Math. Additionally we extend the support of Chinese this time and we also strengthen its reasoning capabilities by endowing it with the abilities to perform CoT, PoT, and TIR. The general performance of Qwen2.5-Math-72B-Instruct surpasses both Qwen2-Math-72B-Instruct and GPT4-o, and even very small expert model like Qwen2.5-Math-1.5B-Instruct can achieve highly competitive performance against large language models. Develop with Qwen2.5 # The simplest way to use is through Hugging Face Transfomer as demonstrated in the model card : from transformers import AutoModelForCausalLM , AutoTokenizer model_name = &#34;Qwen/Qwen2.5-7B-Instruct&#34; model = AutoModelForCausalLM . from_pretrained ( model_name , torch_dtype = &#34;auto&#34; , device_map = &#34;auto&#34; ) tokenizer = AutoTokenizer . from_pretrained ( model_name ) prompt = &#34;Give me a short introduction to large language model.&#34; messages = [ { &#34;role&#34; : &#34;user&#34; , &#34;content&#34; : prompt } ] text = tokenizer . apply_chat_template ( messages , tokenize = False , add_generation_prompt = True ) model_inputs = tokenizer ([ text ], return_tensors = &#34;pt&#34; ) . to ( model . device ) generated_ids = model . generate ( ** model_inputs , max_new_tokens = 512 ) generated_ids = [ output_ids [ len ( input_ids ):] for input_ids , output_ids in zip ( model_inputs . input_ids , generated_ids ) ] response = tokenizer . batch_decode ( generated_ids , skip_special_tokens = True )[ 0 ] To use Qwen2.5 with vLLM, running the following command can deploy an OpenAI API compatible service: python - m vllm . entrypoints . openai . api_server \\ -- model Qwen / Qwen2 .5 - 7 B - Instruct or use vllm serve if you use vllm>=0.5.3 . Then you can communicate with Qwen2.5 via curl : curl http://localhost:8000/v1/chat/completions -H &#34;Content-Type: application/json&#34; -d &#39;{ &#34;model&#34;: &#34;Qwen/Qwen2.5-7B-Instruct&#34;, &#34;messages&#34;: [ {&#34;role&#34;: &#34;user&#34;, &#34;content&#34;: &#34;Tell me something about large language models.&#34;} ], &#34;temperature&#34;: 0.7, &#34;top_p&#34;: 0.8, &#34;repetition_penalty&#34;: 1.05, &#34;max_tokens&#34;: 512 }&#39; Furthermore, Qwen2.5 supports vllm&rsquo;s built-in tool calling. This functionality requires vllm>=0.6 . If you want to enable this functionality, please start vllm&rsquo;s OpenAI-compatible service with: vllm serve Qwen/Qwen2.5-7B-Instruct --enable-auto-tool-choice --tool-call-parser hermes You can then use it in the same way you use GPT&rsquo;s tool calling . Qwen2.5 also supports Ollama&rsquo;s tool calling . You can use it by starting Ollama&rsquo;s OpenAI-compatible service and using it in the same way you use GPT&rsquo;s tool calling. Qwen2.5&rsquo;s chat template also includes a tool calling template, meaning that you can use Hugging Face transformers&rsquo; tool calling support . The vllm / Ollama / transformers tool calling support uses a tool calling template inspired by Nous&rsquo; Hermes . Historically, Qwen-Agent provided tool calling support using Qwen2&rsquo;s own tool calling template (which is harder to be integrated with vllm and Ollama), and Qwen2.5 maintains compatibility with Qwen2&rsquo;s template and Qwen-Agent as well. Friends of Qwen # 💗 Qwen is nothing without its friends! So many thanks to the support of these old buddies and new friends : Hugging Face Transformers Finetuning: Peft , ChatLearn , Llama-Factory , Axolotl , Firefly , Swift , XTuner , Unsloth , Liger Kernel Quantization: AutoGPTQ , AutoAWQ , Neural Compressor Deployment: vLLM , SGL , SkyPilot , TensorRT-LLM , OpenVino , TGI , Xinference API Platforms: Together , Fireworks , OpenRouter , Sillicon Flow Local Run: MLX , Llama.cpp , Ollama , LM Studio , Jan Agent and RAG Frameworks: Dify , LlamaIndex , CrewAI Evaluation: LMSys , OpenCompass , Open LLM Leaderboard Model Training: Arcee AI , Sailor , Dolphin , Openbuddy We would like to extend our heartfelt gratitude to the numerous teams and individuals who have contributed to Qwen, even if they haven&rsquo;t been specifically mentioned. Your support is invaluable, and we warmly invite more friends to join us in this exciting journey. Together, we can enhance collaboration and drive forward the research and development of the open-source AI community, making it stronger and more innovative than ever before. What&rsquo;s Next? # While we are thrilled to launch numerous high-quality models simultaneously, we recognize that significant challenges remain. Our recent releases demonstrate our commitment to developing robust foundation models across language, vision-language, and audio-language domains. However, it is crucial to integrate these different modalities into a single model to enable seamless end-to-end processing of information across all three. Additionally, although we have made strides in enhancing reasoning capabilities through data scaling, we are inspired by the recent advancements in reinforcement learning (e.g., o1) and are dedicated to further improving our models&rsquo; reasoning abilities by scaling inference compute. We look forward to introducing you to the next generation of models soon! Stay tuned for more exciting developments! Citation # We are going to release the technical report for Qwen2.5 very soon. Before the release, feel free to cite our Qwen2 paper as well as this blog @misc{qwen2.5, title = {Qwen2.5: A Party of Foundation Models}, url = {https://qwenlm.github.io/blog/qwen2.5/}, author = {Qwen Team}, month = {September}, year = {2024} } @article{qwen2, title={Qwen2 technical report}, author={Yang, An and Yang, Baosong and Hui, Binyuan and Zheng, Bo and Yu, Bowen and Zhou, Chang and Li, Chengpeng and Li, Chengyuan and Liu, Dayiheng and Huang, Fei and others}, journal={arXiv preprint arXiv:2407.10671}, year={2024} } &copy; 2025 Qwen Powered by Hugo ",
      "fetch_method": "direct-html"
    },
    {
      "arxiv_id": "https://qwen.readthedocs.io/en/latest/",
      "full_text": " Qwen Contents Menu Expand Light mode Dark mode Auto light/dark, in light mode Auto light/dark, in dark mode Hide navigation sidebar Hide table of contents sidebar Skip to content Toggle site navigation sidebar Qwen Toggle Light / Dark / Auto color theme Toggle table of contents sidebar Qwen Getting Started Quickstart Key Concepts Speed Benchmark Performance of Quantized Models Inference Transformers Run Locally llama.cpp Ollama MLX LM Deployment SGLang vLLM TGI dstack SkyPilot OpenLLM Quantization AWQ GPTQ llama.cpp Training Axolotl LLaMA-Factory MS-SWIFT Unsloth verl Framework Qwen-Agent Function Calling LlamaIndex Langchain Back to top View this page Toggle Light / Dark / Auto color theme Toggle table of contents sidebar Welcome to Qwen! ¶ Qwen is the large language model and large multimodal model series of the Qwen Team, Alibaba Group. Both language models and multimodal models are pretrained on large-scale multilingual and multimodal data and post-trained on quality data for aligning to human preferences. Qwen is capable of natural language understanding, text generation, vision understanding, audio understanding, tool use, role play, playing as AI agent, etc. Qwen3-2507 ¶ With input from the community and insights from further research, Instruct-only and Thinking-only models are coming back! The results are Qwen3-2507: Qwen3-Instruct-2507 has the following features: Significant improvements in general capabilities, including instruction following, logical reasoning, text comprehension, mathematics, science, coding and tool usage . Substantial gains in long-tail knowledge coverage across multiple languages . Markedly better alignment with user preferences in subjective and open-ended tasks , enabling more helpful responses and higher-quality text generation. Enhanced capabilities in 256K long-context understanding , extensible to 1M. Qwen3-Thinking-2507 has the following features: Significantly improved performance on reasoning tasks, including logical reasoning, mathematics, science, coding, and academic benchmarks that typically require human expertise — achieving state-of-the-art results among open-source thinking models . Markedly better general capabilities , such as instruction following, tool usage, text generation, and alignment with human preferences. Enhanced 256K long-context understanding capabilities, extensible to 1M. Qwen3 ¶ Qwen3, aka Qwen3-2504, has the following features: Dense and Mixture-of-Experts (MoE) models , available in 0.6B, 1.7B, 4B, 8B, 14B, 32B and 30B-A3B, 235B-A22B. Seamless switching between thinking mode (for complex logical reasoning, math, and coding) and non-thinking mode (for efficient, general-purpose chat) within a single model , ensuring optimal performance across various scenarios. Significantly enhancement in reasoning capabilities , surpassing previous QwQ (in thinking mode) and Qwen2.5 instruct models (in non-thinking mode) on mathematics, code generation, and commonsense logical reasoning. Superior human preference alignment , excelling in creative writing, role-playing, multi-turn dialogues, and instruction following, to deliver a more natural, engaging, and immersive conversational experience. Expertise in agent capabilities , enabling precise integration with external tools in both thinking and unthinking modes and achieving leading performance among open-source models in complex agent-based tasks. Support of 100+ languages and dialects with strong capabilities for multilingual instruction following and translation . Resource &amp; Links ¶ For more information, please visit our: Qwen Home Page Chat with Qwen (with Deep Research and Web Dev) Blog GitHub Hugging Face ModelScope Qwen3 Collection Join our community by joining our Discord and WeChat group. We are looking forward to seeing you there! Next Quickstart Copyright &#169; 2024, Qwen Team Made with Sphinx and @pradyunsg 's Furo On this page Welcome to Qwen! Qwen3-2507 Qwen3 Resource &amp; Links ",
      "fetch_method": "direct-html"
    },
    {
      "arxiv_id": "https://arxiv.org/abs/2309.00071",
      "full_text": " [2309.00071] YaRN: Efficient Context Window Extension of Large Language Models Skip to main content We gratefully acknowledge support from the Simons Foundation, member institutions , and all contributors. Donate &gt; cs &gt; arXiv:2309.00071 Help | Advanced Search All fields Title Author Abstract Comments Journal reference ACM classification MSC classification Report number arXiv identifier DOI ORCID arXiv author ID Help pages Full text Search open search GO open navigation menu quick links Login Help Pages About --> Computer Science > Computation and Language arXiv:2309.00071 (cs) [Submitted on 31 Aug 2023 ( v1 ), last revised 1 Nov 2023 (this version, v2)] Title: YaRN: Efficient Context Window Extension of Large Language Models Authors: Bowen Peng , Jeffrey Quesnelle , Honglu Fan , Enrico Shippole View a PDF of the paper titled YaRN: Efficient Context Window Extension of Large Language Models, by Bowen Peng and 3 other authors View PDF Abstract: Rotary Position Embeddings (RoPE) have been shown to effectively encode positional information in transformer-based language models. However, these models fail to generalize past the sequence length they were trained on. We present YaRN (Yet another RoPE extensioN method), a compute-efficient method to extend the context window of such models, requiring 10x less tokens and 2.5x less training steps than previous methods. Using YaRN, we show that LLaMA models can effectively utilize and extrapolate to context lengths much longer than their original pre-training would allow, while also surpassing previous the state-of-the-art at context window extension. In addition, we demonstrate that YaRN exhibits the capability to extrapolate beyond the limited context of a fine-tuning dataset. The models fine-tuned using YaRN has been made available and reproduced online up to 128k context length at this https URL Subjects: Computation and Language (cs.CL) ; Artificial Intelligence (cs.AI); Machine Learning (cs.LG) Cite as: arXiv:2309.00071 [cs.CL] &nbsp; (or arXiv:2309.00071v2 [cs.CL] for this version) &nbsp; https://doi.org/10.48550/arXiv.2309.00071 Focus to learn more arXiv-issued DOI via DataCite Submission history From: Jeffrey Quesnelle [ view email ] [v1] Thu, 31 Aug 2023 18:18:07 UTC (42 KB) [v2] Wed, 1 Nov 2023 17:28:26 UTC (354 KB) Full-text links: Access Paper: View a PDF of the paper titled YaRN: Efficient Context Window Extension of Large Language Models, by Bowen Peng and 3 other authors View PDF TeX Source Other Formats view license Current browse context: cs.CL &lt;&nbsp;prev &nbsp; | &nbsp; next&nbsp;&gt; new | recent | 2023-09 Change to browse by: cs cs.AI cs.LG References &amp; Citations NASA ADS Google Scholar Semantic Scholar 3 blog links ( what is this? ) a export BibTeX citation Loading... BibTeX formatted citation &times; loading... Data provided by: Bookmark Bibliographic Tools Bibliographic and Citation Tools Bibliographic Explorer Toggle Bibliographic Explorer ( What is the Explorer? ) Connected Papers Toggle Connected Papers ( What is Connected Papers? ) Litmaps Toggle Litmaps ( What is Litmaps? ) scite.ai Toggle scite Smart Citations ( What are Smart Citations? ) Code, Data, Media Code, Data and Media Associated with this Article alphaXiv Toggle alphaXiv ( What is alphaXiv? ) Links to Code Toggle CatalyzeX Code Finder for Papers ( What is CatalyzeX? ) DagsHub Toggle DagsHub ( What is DagsHub? ) GotitPub Toggle Gotit.pub ( What is GotitPub? ) Huggingface Toggle Hugging Face ( What is Huggingface? ) Links to Code Toggle Papers with Code ( What is Papers with Code? ) ScienceCast Toggle ScienceCast ( What is ScienceCast? ) Demos Demos Replicate Toggle Replicate ( What is Replicate? ) Spaces Toggle Hugging Face Spaces ( What is Spaces? ) Spaces Toggle TXYZ.AI ( What is TXYZ.AI? ) Related Papers Recommenders and Search Tools Link to Influence Flower Influence Flower ( What are Influence Flowers? ) Core recommender toggle CORE Recommender ( What is CORE? ) Author Venue Institution Topic About arXivLabs arXivLabs: experimental projects with community collaborators arXivLabs is a framework that allows collaborators to develop and share new arXiv features directly on our website. Both individuals and organizations that work with arXivLabs have embraced and accepted our values of openness, community, excellence, and user data privacy. arXiv is committed to these values and only works with partners that adhere to them. Have an idea for a project that will add value for arXiv's community? Learn more about arXivLabs . Which authors of this paper are endorsers? | Disable MathJax ( What is MathJax? ) About Help contact arXiv Click here to contact arXiv Contact subscribe to arXiv mailings Click here to subscribe Subscribe Copyright Privacy Policy Web Accessibility Assistance arXiv Operational Status Get status notifications via email or slack ",
      "fetch_method": "direct-html"
    },
    {
      "arxiv_id": "https://qwen.readthedocs.io/en/latest/deployment/vllm.html",
      "full_text": " vLLM - Qwen Contents Menu Expand Light mode Dark mode Auto light/dark, in light mode Auto light/dark, in dark mode Hide navigation sidebar Hide table of contents sidebar Skip to content Toggle site navigation sidebar Qwen Toggle Light / Dark / Auto color theme Toggle table of contents sidebar Qwen Getting Started Quickstart Key Concepts Speed Benchmark Performance of Quantized Models Inference Transformers Run Locally llama.cpp Ollama MLX LM Deployment SGLang vLLM TGI dstack SkyPilot OpenLLM Quantization AWQ GPTQ llama.cpp Training Axolotl LLaMA-Factory MS-SWIFT Unsloth verl Framework Qwen-Agent Function Calling LlamaIndex Langchain Back to top View this page Toggle Light / Dark / Auto color theme Toggle table of contents sidebar vLLM ¶ We recommend you trying vLLM for your deployment of Qwen. It is simple to use, and it is fast with state-of-the-art serving throughput, efficient management of attention key value memory with PagedAttention, continuous batching of input requests, optimized CUDA kernels, etc. To learn more about vLLM, please refer to the paper and documentation . Environment Setup ¶ By default, you can install vllm with pip in a clean environment: pip install &quot;vllm&gt;=0.8.5&quot; Please note that the prebuilt vllm has strict dependencies on torch and its CUDA versions. Check the note in the official document for installation ( link ) for more help. API Service ¶ It is easy to build an OpenAI-compatible API service with vLLM, which can be deployed as a server that implements OpenAI API protocol. By default, it starts the server at http://localhost:8000 . You can specify the address with --host and --port arguments. Run the command as shown below: vllm serve Qwen/Qwen3-8B By default, if the model does not point to a valid local directory, it will download the model files from the Hugging Face Hub. To download model from ModelScope, set the following before running the above command: export VLLM_USE_MODELSCOPE = true For distributed inference with tensor parallelism, it is as simple as vllm serve Qwen/Qwen3-8B --tensor-parallel-size 4 The above command will use tensor parallelism on 4 GPUs. You should change the number of GPUs according to your demand. Basic Usage ¶ Then, you can use the create chat interface to communicate with Qwen: curl curl http://localhost:8000/v1/chat/completions -H &quot;Content-Type: application/json&quot; -d &#39;{ &quot;model&quot;: &quot;Qwen/Qwen3-8B&quot;, &quot;messages&quot;: [ {&quot;role&quot;: &quot;user&quot;, &quot;content&quot;: &quot;Give me a short introduction to large language models.&quot;} ], &quot;temperature&quot;: 0.6, &quot;top_p&quot;: 0.95, &quot;top_k&quot;: 20, &quot;max_tokens&quot;: 32768 }&#39; Python You can use the API client with the openai Python SDK as shown below: from openai import OpenAI # Set OpenAI&#39;s API key and API base to use vLLM&#39;s API server. openai_api_key = &quot;EMPTY&quot; openai_api_base = &quot;http://localhost:8000/v1&quot; client = OpenAI ( api_key = openai_api_key , base_url = openai_api_base , ) chat_response = client . chat . completions . create ( model = &quot;Qwen/Qwen3-8B&quot; , messages = [ { &quot;role&quot; : &quot;user&quot; , &quot;content&quot; : &quot;Give me a short introduction to large language models.&quot; }, ], max_tokens = 32768 , temperature = 0.6 , top_p = 0.95 , extra_body = { &quot;top_k&quot; : 20 , }, ) print ( &quot;Chat response:&quot; , chat_response ) Tip vllm will use the sampling parameters from the generation_config.json in the model files. While the default sampling parameters would work most of the time for thinking mode, it is recommended to adjust the sampling parameters according to your application, and always pass the sampling parameters to the API. Thinking &amp; Non-Thinking Modes ¶ Qwen3 models will think before respond. This behavior could be controlled by either the hard switch, which could disable thinking completely, or the soft switch, where the model follows the instruction of the user on whether it should think. The hard switch is available in vLLM through the following configuration to the API call. To disable thinking, use curl curl http://localhost:8000/v1/chat/completions -H &quot;Content-Type: application/json&quot; -d &#39;{ &quot;model&quot;: &quot;Qwen/Qwen3-8B&quot;, &quot;messages&quot;: [ {&quot;role&quot;: &quot;user&quot;, &quot;content&quot;: &quot;Give me a short introduction to large language models.&quot;} ], &quot;temperature&quot;: 0.7, &quot;top_p&quot;: 0.8, &quot;top_k&quot;: 20, &quot;max_tokens&quot;: 8192, &quot;presence_penalty&quot;: 1.5, &quot;chat_template_kwargs&quot;: {&quot;enable_thinking&quot;: false} }&#39; Python You can use the API client with the openai Python SDK as shown below: from openai import OpenAI # Set OpenAI&#39;s API key and API base to use vLLM&#39;s API server. openai_api_key = &quot;EMPTY&quot; openai_api_base = &quot;http://localhost:8000/v1&quot; client = OpenAI ( api_key = openai_api_key , base_url = openai_api_base , ) chat_response = client . chat . completions . create ( model = &quot;Qwen/Qwen3-8B&quot; , messages = [ { &quot;role&quot; : &quot;user&quot; , &quot;content&quot; : &quot;Give me a short introduction to large language models.&quot; }, ], max_tokens = 8192 , temperature = 0.7 , top_p = 0.8 , presence_penalty = 1.5 , extra_body = { &quot;top_k&quot; : 20 , &quot;chat_template_kwargs&quot; : { &quot;enable_thinking&quot; : False }, }, ) print ( &quot;Chat response:&quot; , chat_response ) Note Please note that passing enable_thinking is not OpenAI API compatible. The exact method may differ among frameworks. Tip To completely disable thinking, you could use a custom chat template when starting the model: vllm serve Qwen/Qwen3-8B --chat-template ./qwen3_nonthinking.jinja The chat template prevents the model from generating thinking content, even if the user instructs the model to do so with /think . Tip It is recommended to set sampling parameters differently for thinking and non-thinking modes. Parsing Thinking Content ¶ vLLM supports parsing the thinking content from the model generation into structured messages: vllm serve Qwen/Qwen3-8B --enable-reasoning --reasoning-parser deepseek_r1 Since vLLM 0.9.0, one can also use vllm serve Qwen/Qwen3-8B --reasoning-parser qwen3 The response message will have a field named reasoning_content in addition to content , containing the thinking content generated by the model. Note Please note that this feature is not OpenAI API compatible. Important As of vLLM 0.8.5, enable_thinking=False is not compatible with this feature. If you need to pass enable_thinking=False to the API, you should disable parsing thinking content. This is resolved in vLLM 0.9.0 with the qwen3 reasoning parser. Parsing Tool Calls ¶ vLLM supports parsing the tool calling content from the model generation into structured messages: vllm serve Qwen/Qwen3-8B --enable-auto-tool-choice --tool-call-parser hermes For more information, please refer to our guide on Function Calling . Structured/JSON Output ¶ vLLM supports structured/JSON output. Please refer to vLLM’s documentation for the guided_json parameters. Besides, it is also recommended to instruct the model to generate the specific format in the system message or in your prompt. Serving Quantized models ¶ Qwen3 comes with two types of pre-quantized models, FP8 and AWQ. The command serving those models are the same as the original models except for the name change: # For FP8 quantized model vllm serve Qwen/Qwen3-8B-FP8 # For AWQ quantized model vllm serve Qwen/Qwen3-8B-AWQ Note The FP8 models of Qwen3 are block-wise quant, which is supported on NVIDIA GPUs with compute capability &gt; 8.9, that is, Ada Lovelace, Hopper, and later GPUs and runs as w8a8. Since vLLM v0.9.0, FP8 Marlin has supported block-wise quants (running as w8a16) and you can also run Qwen3 FP8 models on Ampere cards. Note If you encountered the following error when deploying the FP8 models, it indicates that the tensor parallel size does not agree with the model weights: File &quot;.../vllm/vllm/model_executor/layers/quantization/fp8.py&quot; , line 477 , in create_weights raise ValueError ( ValueError : The output_size of gate &#39;s and up&#39; s weight = 192 is not divisible by weight quantization block_n = 128. We recommend lowering the degree of tensor parallel, e.g., --tensor-parallel-size 4 or enabling expert parallel, e.g., --tensor-parallel-size 8 --enable-expert-parallel . Context Length ¶ The context length for Qwen3 models in pretraining is up to 32,768 tokens. To handle context length substantially exceeding 32,768 tokens, RoPE scaling techniques should be applied. We have validated the performance of YaRN , a technique for enhancing model length extrapolation, ensuring optimal performance on lengthy texts. vLLM supports YaRN, which can be configured as vllm serve Qwen/Qwen3-8B --rope-scaling &#39;{&quot;rope_type&quot;:&quot;yarn&quot;,&quot;factor&quot;:4.0,&quot;original_max_position_embeddings&quot;:32768}&#39; --max-model-len 131072 Note vLLM implements static YaRN, which means the scaling factor remains constant regardless of input length, potentially impacting performance on shorter texts. We advise adding the rope_scaling configuration only when processing long contexts is required. It is also recommended to modify the factor as needed. For example, if the typical context length for your application is 65,536 tokens, it would be better to set factor as 2.0. Note The default max_position_embeddings in config.json is set to 40,960, which used by vLLM, if --max-model-len is not specified. This allocation includes reserving 32,768 tokens for outputs and 8,192 tokens for typical prompts, which is sufficient for most scenarios involving short text processing and leave adequate room for model thinking. If the average context length does not exceed 32,768 tokens, we do not recommend enabling YaRN in this scenario, as it may potentially degrade model performance. Python Library ¶ vLLM can also be directly used as a Python library, which is convenient for offline batch inference but lack some API-only features, such as parsing model generation to structure messages. The following shows the basic usage of vLLM as a library: from transformers import AutoTokenizer from vllm import LLM , SamplingParams # Initialize the tokenizer tokenizer = AutoTokenizer . from_pretrained ( &quot;Qwen/Qwen3-8B&quot; ) # Configurae the sampling parameters (for thinking mode) sampling_params = SamplingParams ( temperature = 0.6 , top_p = 0.95 , top_k = 20 , max_tokens = 32768 ) # Initialize the vLLM engine llm = LLM ( model = &quot;Qwen/Qwen3-8B&quot; ) # Prepare the input to the model prompt = &quot;Give me a short introduction to large language models.&quot; messages = [ { &quot;role&quot; : &quot;user&quot; , &quot;content&quot; : prompt } ] text = tokenizer . apply_chat_template ( messages , tokenize = False , add_generation_prompt = True , enable_thinking = True , # Set to False to strictly disable thinking ) # Generate outputs outputs = llm . generate ([ text ], sampling_params ) # Print the outputs. for output in outputs : prompt = output . prompt generated_text = output . outputs [ 0 ] . text print ( f &quot;Prompt: { prompt !r} , Generated text: { generated_text !r} &quot; ) Since vLLM v0.9.0, you can also use the LLM.chat interface which includes support for chat_template_kwargs : from vllm import LLM , SamplingParams # Configurae the sampling parameters (for thinking mode) sampling_params = SamplingParams ( temperature = 0.6 , top_p = 0.95 , top_k = 20 , max_tokens = 32768 ) # Initialize the vLLM engine llm = LLM ( model = &quot;Qwen/Qwen3-8B&quot; ) # Prepare the input to the model prompt = &quot;Give me a short introduction to large language models.&quot; messages = [ { &quot;role&quot; : &quot;user&quot; , &quot;content&quot; : prompt } ] # Generate outputs outputs = llm . chat ( [ messages ], sampling_params , chat_template_kwargs = { &quot;enable_thinking&quot; : True }, # Set to False to strictly disable thinking ) # Print the outputs. for output in outputs : prompt = output . prompt generated_text = output . outputs [ 0 ] . text print ( f &quot;Prompt: { prompt !r} , Generated text: { generated_text !r} &quot; ) FAQ ¶ You may encounter OOM issues that are pretty annoying. We recommend two arguments for you to make some fix. The first one is --max-model-len . Our provided default max_position_embedding is 40960 and thus the maximum length for the serving is also this value, leading to higher requirements of memory. Reducing it to a proper length for yourself often helps with the OOM issue. Another argument you can pay attention to is --gpu-memory-utilization . vLLM will pre-allocate this much GPU memory. By default, it is 0.9 . This is also why you find a vLLM service always takes so much memory. If you are in eager mode (by default it is not), you can level it up to tackle the OOM problem. Otherwise, CUDA Graphs are used, which will use GPU memory not controlled by vLLM, and you should try lowering it. If it doesn’t work, you should try --enforce-eager , which may slow down inference, or reduce the --max-model-len . For more usage guide with vLLM, please see vLLM’s Qwen3 Usage Guide . Next TGI Previous SGLang Copyright &#169; 2024, Qwen Team Made with Sphinx and @pradyunsg 's Furo On this page vLLM Environment Setup API Service Basic Usage Thinking &amp; Non-Thinking Modes Parsing Thinking Content Parsing Tool Calls Structured/JSON Output Serving Quantized models Context Length Python Library FAQ ",
      "fetch_method": "direct-html"
    },
    {
      "arxiv_id": "https://r.jina.ai/https://qwen.readthedocs.io/en/latest/benchmark/speed_benchmark.html",
      "full_text": "Title: Documentation page not found - Read the Docs Community\n\nURL Source: https://qwen.readthedocs.io/en/latest/benchmark/speed_benchmark.html\n\nWarning: Target URL returned error 404: Not Found\nWarning: This page contains shadow DOM that are currently hidden, consider enabling shadow DOM processing.\n\nMarkdown Content:\n404  Documentation page not found\n\n[qwen.readthedocs.io](https://qwen.readthedocs.io/)\n\nThe documentation page you requested does not exist or may have been removed.\n",
      "fetch_method": "jina-reader"
    },
    {
      "arxiv_id": "https://r.jina.ai/https://qwenlm.github.io/blog/qwen2.5/},",
      "full_text": "Title: 404 Page not found\n\nURL Source: https://qwenlm.github.io/blog/qwen2.5/%7D,\n\nWarning: Target URL returned error 404: Not Found\n\nMarkdown Content:\n404 Page not found | Qwen\n\n===============\n\n[![Image 1](https://qwenlm.github.io/img/logo.png)](https://qwenlm.github.io/ \"Qwen (Alt + H)\")\n\n*   [Blog](https://qwenlm.github.io/blog/ \"Blog\")\n*   [Publication](https://qwenlm.github.io/publication \"Publication\")\n*   [About](https://qwenlm.github.io/about \"About\")\n*   [Try Qwen Chat](https://chat.qwen.ai/ \"Try Qwen Chat\")\n\n404\n===\n\nYou will be redirected to home page shortly. If not, please click [here](https://qwenlm.github.io/).\n\n© 2025 [Qwen](https://qwenlm.github.io/)Powered by [Hugo](https://gohugo.io/)[](https://qwenlm.github.io/blog/qwen2.5/%7D,#top \"Go to Top (Alt + G)\")\n",
      "fetch_method": "jina-reader"
    },
    {
      "arxiv_id": "https://arxiv.org/abs/2505.09388",
      "full_text": " [2505.09388] Qwen3 Technical Report Skip to main content We gratefully acknowledge support from the Simons Foundation, member institutions , and all contributors. Donate &gt; cs &gt; arXiv:2505.09388 Help | Advanced Search All fields Title Author Abstract Comments Journal reference ACM classification MSC classification Report number arXiv identifier DOI ORCID arXiv author ID Help pages Full text Search open search GO open navigation menu quick links Login Help Pages About --> Computer Science > Computation and Language arXiv:2505.09388 (cs) [Submitted on 14 May 2025] Title: Qwen3 Technical Report Authors: An Yang , Anfeng Li , Baosong Yang , Beichen Zhang , Binyuan Hui , Bo Zheng , Bowen Yu , Chang Gao , Chengen Huang , Chenxu Lv , Chujie Zheng , Dayiheng Liu , Fan Zhou , Fei Huang , Feng Hu , Hao Ge , Haoran Wei , Huan Lin , Jialong Tang , Jian Yang , Jianhong Tu , Jianwei Zhang , Jianxin Yang , Jiaxi Yang , Jing Zhou , Jingren Zhou , Junyang Lin , Kai Dang , Keqin Bao , Kexin Yang , Le Yu , Lianghao Deng , Mei Li , Mingfeng Xue , Mingze Li , Pei Zhang , Peng Wang , Qin Zhu , Rui Men , Ruize Gao , Shixuan Liu , Shuang Luo , Tianhao Li , Tianyi Tang , Wenbiao Yin , Xingzhang Ren , Xinyu Wang , Xinyu Zhang , Xuancheng Ren , Yang Fan , Yang Su , Yichang Zhang , Yinger Zhang , Yu Wan , Yuqiong Liu , Zekun Wang , Zeyu Cui , Zhenru Zhang , Zhipeng Zhou , Zihan Qiu View a PDF of the paper titled Qwen3 Technical Report, by An Yang and 59 other authors View PDF Abstract: In this work, we present Qwen3, the latest version of the Qwen model family. Qwen3 comprises a series of large language models (LLMs) designed to advance performance, efficiency, and multilingual capabilities. The Qwen3 series includes models of both dense and Mixture-of-Expert (MoE) architectures, with parameter scales ranging from 0.6 to 235 billion. A key innovation in Qwen3 is the integration of thinking mode (for complex, multi-step reasoning) and non-thinking mode (for rapid, context-driven responses) into a unified framework. This eliminates the need to switch between different models--such as chat-optimized models (e.g., GPT-4o) and dedicated reasoning models (e.g., QwQ-32B)--and enables dynamic mode switching based on user queries or chat templates. Meanwhile, Qwen3 introduces a thinking budget mechanism, allowing users to allocate computational resources adaptively during inference, thereby balancing latency and performance based on task complexity. Moreover, by leveraging the knowledge from the flagship models, we significantly reduce the computational resources required to build smaller-scale models, while ensuring their highly competitive performance. Empirical evaluations demonstrate that Qwen3 achieves state-of-the-art results across diverse benchmarks, including tasks in code generation, mathematical reasoning, agent tasks, etc., competitive against larger MoE models and proprietary models. Compared to its predecessor Qwen2.5, Qwen3 expands multilingual support from 29 to 119 languages and dialects, enhancing global accessibility through improved cross-lingual understanding and generation capabilities. To facilitate reproducibility and community-driven research and development, all Qwen3 models are publicly accessible under Apache 2.0. Subjects: Computation and Language (cs.CL) Cite as: arXiv:2505.09388 [cs.CL] &nbsp; (or arXiv:2505.09388v1 [cs.CL] for this version) &nbsp; https://doi.org/10.48550/arXiv.2505.09388 Focus to learn more arXiv-issued DOI via DataCite Submission history From: Binyuan Hui [ view email ] [v1] Wed, 14 May 2025 13:41:34 UTC (2,293 KB) Full-text links: Access Paper: View a PDF of the paper titled Qwen3 Technical Report, by An Yang and 59 other authors View PDF TeX Source Other Formats view license Current browse context: cs.CL &lt;&nbsp;prev &nbsp; | &nbsp; next&nbsp;&gt; new | recent | 2025-05 Change to browse by: cs References &amp; Citations NASA ADS Google Scholar Semantic Scholar a export BibTeX citation Loading... BibTeX formatted citation &times; loading... Data provided by: Bookmark Bibliographic Tools Bibliographic and Citation Tools Bibliographic Explorer Toggle Bibliographic Explorer ( What is the Explorer? ) Connected Papers Toggle Connected Papers ( What is Connected Papers? ) Litmaps Toggle Litmaps ( What is Litmaps? ) scite.ai Toggle scite Smart Citations ( What are Smart Citations? ) Code, Data, Media Code, Data and Media Associated with this Article alphaXiv Toggle alphaXiv ( What is alphaXiv? ) Links to Code Toggle CatalyzeX Code Finder for Papers ( What is CatalyzeX? ) DagsHub Toggle DagsHub ( What is DagsHub? ) GotitPub Toggle Gotit.pub ( What is GotitPub? ) Huggingface Toggle Hugging Face ( What is Huggingface? ) Links to Code Toggle Papers with Code ( What is Papers with Code? ) ScienceCast Toggle ScienceCast ( What is ScienceCast? ) Demos Demos Replicate Toggle Replicate ( What is Replicate? ) Spaces Toggle Hugging Face Spaces ( What is Spaces? ) Spaces Toggle TXYZ.AI ( What is TXYZ.AI? ) Related Papers Recommenders and Search Tools Link to Influence Flower Influence Flower ( What are Influence Flowers? ) Core recommender toggle CORE Recommender ( What is CORE? ) Author Venue Institution Topic About arXivLabs arXivLabs: experimental projects with community collaborators arXivLabs is a framework that allows collaborators to develop and share new arXiv features directly on our website. Both individuals and organizations that work with arXivLabs have embraced and accepted our values of openness, community, excellence, and user data privacy. arXiv is committed to these values and only works with partners that adhere to them. Have an idea for a project that will add value for arXiv's community? Learn more about arXivLabs . Which authors of this paper are endorsers? | Disable MathJax ( What is MathJax? ) About Help contact arXiv Click here to contact arXiv Contact subscribe to arXiv mailings Click here to subscribe Subscribe Copyright Privacy Policy Web Accessibility Assistance arXiv Operational Status Get status notifications via email or slack ",
      "fetch_method": "direct-html"
    },
    {
      "arxiv_id": "https://qwenlm.github.io/blog/qwen3/",
      "full_text": " Qwen3: Think Deeper, Act Faster | Qwen Blog Publication About Try Qwen Chat &nbsp; Qwen3: Think Deeper, Act Faster April 29, 2025 &nbsp;·&nbsp;10 min&nbsp;·&nbsp;2036 words&nbsp;·&nbsp;Qwen Team&nbsp;|&nbsp;Translations: 简体中文 QWEN CHAT GitHub Hugging Face ModelScope Kaggle DEMO DISCORD Introduction # Today, we are excited to announce the release of Qwen3 , the latest addition to the Qwen family of large language models. Our flagship model, Qwen3-235B-A22B , achieves competitive results in benchmark evaluations of coding, math, general capabilities, etc., when compared to other top-tier models such as DeepSeek-R1, o1, o3-mini, Grok-3, and Gemini-2.5-Pro. Additionally, the small MoE model, Qwen3-30B-A3B , outcompetes QwQ-32B with 10 times of activated parameters, and even a tiny model like Qwen3-4B can rival the performance of Qwen2.5-72B-Instruct. We are open-weighting two MoE models: Qwen3-235B-A22B , a large model with 235 billion total parameters and 22 billion activated parameters, and Qwen3-30B-A3B , a smaller MoE model with 30 billion total parameters and 3 billion activated parameters. Additionally, six dense models are also open-weighted, including Qwen3-32B , Qwen3-14B , Qwen3-8B , Qwen3-4B , Qwen3-1.7B , and Qwen3-0.6B , under Apache 2.0 license. Models Layers Heads (Q / KV) Tie Embedding Context Length Qwen3-0.6B 28 16 / 8 Yes 32K Qwen3-1.7B 28 16 / 8 Yes 32K Qwen3-4B 36 32 / 8 Yes 32K Qwen3-8B 36 32 / 8 No 128K Qwen3-14B 40 40 / 8 No 128K Qwen3-32B 64 64 / 8 No 128K Models Layers Heads (Q / KV) # Experts (Total / Activated) Context Length Qwen3-30B-A3B 48 32 / 4 128 / 8 128K Qwen3-235B-A22B 94 64 / 4 128 / 8 128K The post-trained models, such as Qwen3-30B-A3B , along with their pre-trained counterparts (e.g., Qwen3-30B-A3B-Base ), are now available on platforms like Hugging Face , ModelScope , and Kaggle . For deployment, we recommend using frameworks like SGLang and vLLM . For local usage, tools such as Ollama , LMStudio , MLX , llama.cpp , and KTransformers are highly recommended. These options ensure that users can easily integrate Qwen3 into their workflows, whether in research, development, or production environments. We believe that the release and open-sourcing of Qwen3 will significantly advance the research and development of large foundation models. Our goal is to empower researchers, developers, and organizations around the world to build innovative solutions using these cutting-edge models. Feel free to try Qwen3 out in Qwen Chat Web ( chat.qwen.ai ) and mobile APP! Key Features # Hybrid Thinking Modes Qwen3 models introduce a hybrid approach to problem-solving. They support two modes: Thinking Mode: In this mode, the model takes time to reason step by step before delivering the final answer. This is ideal for complex problems that require deeper thought. Non-Thinking Mode: Here, the model provides quick, near-instant responses, suitable for simpler questions where speed is more important than depth. This flexibility allows users to control how much &ldquo;thinking&rdquo; the model performs based on the task at hand. For example, harder problems can be tackled with extended reasoning, while easier ones can be answered directly without delay. Crucially, the integration of these two modes greatly enhances the model&rsquo;s ability to implement stable and efficient thinking budget control. As demonstrated above, Qwen3 exhibits scalable and smooth performance improvements that are directly correlated with the computational reasoning budget allocated. This design enables users to configure task-specific budgets with greater ease, achieving a more optimal balance between cost efficiency and inference quality. Multilingual Support Qwen3 models are supporting 119 languages and dialects . This extensive multilingual capability opens up new possibilities for international applications, enabling users worldwide to benefit from the power of these models. Language Family Languages & Dialects Indo-European English, French, Portuguese, German, Romanian, Swedish, Danish, Bulgarian, Russian, Czech, Greek, Ukrainian, Spanish, Dutch, Slovak, Croatian, Polish, Lithuanian, Norwegian Bokmål, Norwegian Nynorsk, Persian, Slovenian, Gujarati, Latvian, Italian, Occitan, Nepali, Marathi, Belarusian, Serbian, Luxembourgish, Venetian, Assamese, Welsh, Silesian, Asturian, Chhattisgarhi, Awadhi, Maithili, Bhojpuri, Sindhi, Irish, Faroese, Hindi, Punjabi, Bengali, Oriya, Tajik, Eastern Yiddish, Lombard, Ligurian, Sicilian, Friulian, Sardinian, Galician, Catalan, Icelandic, Tosk Albanian, Limburgish, Dari, Afrikaans, Macedonian, Sinhala, Urdu, Magahi, Bosnian, Armenian Sino-Tibetan Chinese (Simplified Chinese, Traditional Chinese, Cantonese), Burmese Afro-Asiatic Arabic (Standard, Najdi, Levantine, Egyptian, Moroccan, Mesopotamian, Ta&rsquo;izzi-Adeni, Tunisian), Hebrew, Maltese Austronesian Indonesian, Malay, Tagalog, Cebuano, Javanese, Sundanese, Minangkabau, Balinese, Banjar, Pangasinan, Iloko, Waray (Philippines) Dravidian Tamil, Telugu, Kannada, Malayalam Turkic Turkish, North Azerbaijani, Northern Uzbek, Kazakh, Bashkir, Tatar Tai-Kadai Thai, Lao Uralic Finnish, Estonian, Hungarian Austroasiatic Vietnamese, Khmer Other Japanese, Korean, Georgian, Basque, Haitian, Papiamento, Kabuverdianu, Tok Pisin, Swahili Improved Agentic Capabilities We have optimized the Qwen3 models for coding and agentic capabilities, and also we have strengthened the support of MCP as well. Below we provide examples to show how Qwen3 thinks and interacts with the environment. Pre-training # In terms of pretraining, the dataset for Qwen3 has been significantly expanded compared to Qwen2.5. While Qwen2.5 was pre-trained on 18 trillion tokens, Qwen3 uses nearly twice that amount, with approximately 36 trillion tokens covering 119 languages and dialects. To build this large dataset, we collected data not only from the web but also from PDF-like documents. We used Qwen2.5-VL to extract text from these documents and Qwen2.5 to improve the quality of the extracted content. To increase the amount of math and code data, we used Qwen2.5-Math and Qwen2.5-Coder to generate synthetic data. This includes textbooks, question-answer pairs, and code snippets. The pre-training process consists of three stages. In the first stage (S1), the model was pretrained on over 30 trillion tokens with a context length of 4K tokens. This stage provided the model with basic language skills and general knowledge. In the second stage (S2), we improved the dataset by increasing the proportion of knowledge-intensive data, such as STEM, coding, and reasoning tasks. The model was then pretrained on an additional 5 trillion tokens. In the final stage, we used high-quality long-context data to extend the context length to 32K tokens. This ensures the model can handle longer inputs effectively. Due to advancements in model architecture, increase in training data, and more effective training methods, the overall performance of Qwen3 dense base models matches that of Qwen2.5 base models with more parameters. For instance, Qwen3-1.7B/4B/8B/14B/32B-Base performs as well as Qwen2.5-3B/7B/14B/32B/72B-Base, respectively. Notably, in areas like STEM, coding, and reasoning, Qwen3 dense base models even outperform larger Qwen2.5 models. For Qwen3-MoE base models, they achieve similar performance to Qwen2.5 dense base models while using only 10% of the active parameters. This results in significant savings in both training and inference costs. Post-training # To develop the hybrid model capable of both step-by-step reasoning and rapid responses, we implemented a four-stage training pipeline. This pipeline includes: (1) long chain-of-thought (CoT) cold start, (2) reasoning-based reinforcement learning (RL), (3) thinking mode fusion, and (4) general RL. In the first stage, we fine-tuned the models using diverse long CoT data, covering various tasks and domains such as mathematics, coding, logical reasoning, and STEM problems. This process aimed to equip the model with fundamental reasoning abilities. The second stage focused on scaling up computational resources for RL, utilizing rule-based rewards to enhance the model&rsquo;s exploration and exploitation capabilities. In the third stage, we integrated non-thinking capabilities into the thinking model by fine-tuning it on a combination of long CoT data and commonly used instruction-tuning data. This data was generated by the enhanced thinking model from the second stage, ensuring a seamless blend of reasoning and quick response capabilities. Finally, in the fourth stage, we applied RL across more than 20 general-domain tasks to further strengthen the model’s general capabilities and correct undesired behaviors. These tasks included instruction following, format following, and agent capabilities, etc. Develop with Qwen3 # Below is a simple guide for you to use Qwen3 on different frameworks. First of all, we provide an standard example of using Qwen3-30B-A3B in Hugging Face transformers: from modelscope import AutoModelForCausalLM , AutoTokenizer model_name = &#34;Qwen/Qwen3-30B-A3B&#34; # load the tokenizer and the model tokenizer = AutoTokenizer . from_pretrained ( model_name ) model = AutoModelForCausalLM . from_pretrained ( model_name , torch_dtype = &#34;auto&#34; , device_map = &#34;auto&#34; ) # prepare the model input prompt = &#34;Give me a short introduction to large language model.&#34; messages = [ { &#34;role&#34; : &#34;user&#34; , &#34;content&#34; : prompt } ] text = tokenizer . apply_chat_template ( messages , tokenize = False , add_generation_prompt = True , enable_thinking = True # Switch between thinking and non-thinking modes. Default is True. ) model_inputs = tokenizer ([ text ], return_tensors = &#34;pt&#34; ) . to ( model . device ) # conduct text completion generated_ids = model . generate ( ** model_inputs , max_new_tokens = 32768 ) output_ids = generated_ids [ 0 ][ len ( model_inputs . input_ids [ 0 ]):] . tolist () # parsing thinking content try : # rindex finding 151668 (&lt;/think&gt;) index = len ( output_ids ) - output_ids [:: - 1 ] . index ( 151668 ) except ValueError : index = 0 thinking_content = tokenizer . decode ( output_ids [: index ], skip_special_tokens = True ) . strip ( &#34; \\n &#34; ) content = tokenizer . decode ( output_ids [ index :], skip_special_tokens = True ) . strip ( &#34; \\n &#34; ) print ( &#34;thinking content:&#34; , thinking_content ) print ( &#34;content:&#34; , content ) To disable thinking, you just need to make changes to the argument enable_thinking like the following: text = tokenizer . apply_chat_template ( messages , tokenize = False , add_generation_prompt = True , enable_thinking = False # True is the default value for enable_thinking. ) For deployment, you can use sglang>=0.4.6.post1 or vllm>=0.8.4 to create an OpenAI-compatible API endpoint: SGLang: python -m sglang.launch_server --model-path Qwen/Qwen3-30B-A3B --reasoning-parser qwen3 vLLM: vllm serve Qwen/Qwen3-30B-A3B --enable-reasoning --reasoning-parser deepseek_r1 If you use it for local development, you can use ollama by running a simple command ollama run qwen3:30b-a3b to play with the model, or you can use LMStudio or llama.cpp and ktransformers to build locally. Advanced Usages # We provide a soft switch mechanism that allows users to dynamically control the model&rsquo;s behavior when enable_thinking=True. Specifically, you can add /think and /no_think to user prompts or system messages to switch the model&rsquo;s thinking mode from turn to turn. The model will follow the most recent instruction in multi-turn conversations. Here is an example of a multi-turn conversation: from transformers import AutoModelForCausalLM , AutoTokenizer class QwenChatbot : def __init__ ( self , model_name = &#34;Qwen/Qwen3-30B-A3B&#34; ): self . tokenizer = AutoTokenizer . from_pretrained ( model_name ) self . model = AutoModelForCausalLM . from_pretrained ( model_name ) self . history = [] def generate_response ( self , user_input ): messages = self . history + [{ &#34;role&#34; : &#34;user&#34; , &#34;content&#34; : user_input }] text = self . tokenizer . apply_chat_template ( messages , tokenize = False , add_generation_prompt = True ) inputs = self . tokenizer ( text , return_tensors = &#34;pt&#34; ) response_ids = self . model . generate ( ** inputs , max_new_tokens = 32768 )[ 0 ][ len ( inputs . input_ids [ 0 ]):] . tolist () response = self . tokenizer . decode ( response_ids , skip_special_tokens = True ) # Update history self . history . append ({ &#34;role&#34; : &#34;user&#34; , &#34;content&#34; : user_input }) self . history . append ({ &#34;role&#34; : &#34;assistant&#34; , &#34;content&#34; : response }) return response # Example Usage if __name__ == &#34;__main__&#34; : chatbot = QwenChatbot () # First input (without /think or /no_think tags, thinking mode is enabled by default) user_input_1 = &#34;How many r&#39;s in strawberries?&#34; print ( f &#34;User: { user_input_1 } &#34; ) response_1 = chatbot . generate_response ( user_input_1 ) print ( f &#34;Bot: { response_1 } &#34; ) print ( &#34;----------------------&#34; ) # Second input with /no_think user_input_2 = &#34;Then, how many r&#39;s in blueberries? /no_think&#34; print ( f &#34;User: { user_input_2 } &#34; ) response_2 = chatbot . generate_response ( user_input_2 ) print ( f &#34;Bot: { response_2 } &#34; ) print ( &#34;----------------------&#34; ) # Third input with /think user_input_3 = &#34;Really? /think&#34; print ( f &#34;User: { user_input_3 } &#34; ) response_3 = chatbot . generate_response ( user_input_3 ) print ( f &#34;Bot: { response_3 } &#34; ) Agentic Usages # Qwen3 excels in tool calling capabilities. We recommend using Qwen-Agent to make the best use of agentic ability of Qwen3. Qwen-Agent encapsulates tool-calling templates and tool-calling parsers internally, greatly reducing coding complexity. To define the available tools, you can use the MCP configuration file, use the integrated tool of Qwen-Agent, or integrate other tools by yourself. from qwen_agent.agents import Assistant # Define LLM llm_cfg = { &#39;model&#39; : &#39;Qwen3-30B-A3B&#39; , # Use the endpoint provided by Alibaba Model Studio: # &#39;model_type&#39;: &#39;qwen_dashscope&#39;, # &#39;api_key&#39;: os.getenv(&#39;DASHSCOPE_API_KEY&#39;), # Use a custom endpoint compatible with OpenAI API: &#39;model_server&#39; : &#39;http://localhost:8000/v1&#39; , # api_base &#39;api_key&#39; : &#39;EMPTY&#39; , # Other parameters: # &#39;generate_cfg&#39;: { # # Add: When the response content is `&lt;think&gt;this is the thought&lt;/think&gt;this is the answer; # # Do not add: When the response has been separated by reasoning_content and content. # &#39;thought_in_content&#39;: True, # }, } # Define Tools tools = [ { &#39;mcpServers&#39; : { # You can specify the MCP configuration file &#39;time&#39; : { &#39;command&#39; : &#39;uvx&#39; , &#39;args&#39; : [ &#39;mcp-server-time&#39; , &#39;--local-timezone=Asia/Shanghai&#39; ] }, &#34;fetch&#34; : { &#34;command&#34; : &#34;uvx&#34; , &#34;args&#34; : [ &#34;mcp-server-fetch&#34; ] } } }, &#39;code_interpreter&#39; , # Built-in tools ] # Define Agent bot = Assistant ( llm = llm_cfg , function_list = tools ) # Streaming generation messages = [{ &#39;role&#39; : &#39;user&#39; , &#39;content&#39; : &#39;https://qwenlm.github.io/blog/ Introduce the latest developments of Qwen&#39; }] for responses in bot . run ( messages = messages ): pass print ( responses ) Friends of Qwen # Thanks to the support of so many friends. Qwen is nothing without its friends! We welcome more people or organizations to join our community and help us become better! Future Work # Qwen3 represents a significant milestone in our journey toward Artificial General Intelligence (AGI) and Artificial Superintelligence (ASI). By scaling up both pretraining and reinforcement learning (RL), we have achieved higher levels of intelligence. We have seamlessly integrated thinking and non-thinking modes, offering users the flexibility to control the thinking budget. Additionally, we have expanded support for a wide range of languages, enhancing global accessibility. Looking ahead, we aim to enhance our models across multiple dimensions. This includes refining model architectures and training methodologies to achieve several key objectives: scaling data, increasing model size, extending context length, broadening modalities, and advancing RL with environmental feedback for long-horizon reasoning. We believe we are transitioning from an era focused on training models to one centered on training agents. Our next iteration promises to bring meaningful advancements to everyone&rsquo;s work and life. &copy; 2025 Qwen Powered by Hugo ",
      "fetch_method": "direct-html"
    },
    {
      "arxiv_id": "https://qwen.readthedocs.io/",
      "full_text": " Qwen Contents Menu Expand Light mode Dark mode Auto light/dark, in light mode Auto light/dark, in dark mode Hide navigation sidebar Hide table of contents sidebar Skip to content Toggle site navigation sidebar Qwen Toggle Light / Dark / Auto color theme Toggle table of contents sidebar Qwen Getting Started Quickstart Key Concepts Speed Benchmark Performance of Quantized Models Inference Transformers Run Locally llama.cpp Ollama MLX LM Deployment SGLang vLLM TGI dstack SkyPilot OpenLLM Quantization AWQ GPTQ llama.cpp Training Axolotl LLaMA-Factory MS-SWIFT Unsloth verl Framework Qwen-Agent Function Calling LlamaIndex Langchain Back to top View this page Toggle Light / Dark / Auto color theme Toggle table of contents sidebar Welcome to Qwen! ¶ Qwen is the large language model and large multimodal model series of the Qwen Team, Alibaba Group. Both language models and multimodal models are pretrained on large-scale multilingual and multimodal data and post-trained on quality data for aligning to human preferences. Qwen is capable of natural language understanding, text generation, vision understanding, audio understanding, tool use, role play, playing as AI agent, etc. Qwen3-2507 ¶ With input from the community and insights from further research, Instruct-only and Thinking-only models are coming back! The results are Qwen3-2507: Qwen3-Instruct-2507 has the following features: Significant improvements in general capabilities, including instruction following, logical reasoning, text comprehension, mathematics, science, coding and tool usage . Substantial gains in long-tail knowledge coverage across multiple languages . Markedly better alignment with user preferences in subjective and open-ended tasks , enabling more helpful responses and higher-quality text generation. Enhanced capabilities in 256K long-context understanding , extensible to 1M. Qwen3-Thinking-2507 has the following features: Significantly improved performance on reasoning tasks, including logical reasoning, mathematics, science, coding, and academic benchmarks that typically require human expertise — achieving state-of-the-art results among open-source thinking models . Markedly better general capabilities , such as instruction following, tool usage, text generation, and alignment with human preferences. Enhanced 256K long-context understanding capabilities, extensible to 1M. Qwen3 ¶ Qwen3, aka Qwen3-2504, has the following features: Dense and Mixture-of-Experts (MoE) models , available in 0.6B, 1.7B, 4B, 8B, 14B, 32B and 30B-A3B, 235B-A22B. Seamless switching between thinking mode (for complex logical reasoning, math, and coding) and non-thinking mode (for efficient, general-purpose chat) within a single model , ensuring optimal performance across various scenarios. Significantly enhancement in reasoning capabilities , surpassing previous QwQ (in thinking mode) and Qwen2.5 instruct models (in non-thinking mode) on mathematics, code generation, and commonsense logical reasoning. Superior human preference alignment , excelling in creative writing, role-playing, multi-turn dialogues, and instruction following, to deliver a more natural, engaging, and immersive conversational experience. Expertise in agent capabilities , enabling precise integration with external tools in both thinking and unthinking modes and achieving leading performance among open-source models in complex agent-based tasks. Support of 100+ languages and dialects with strong capabilities for multilingual instruction following and translation . Resource &amp; Links ¶ For more information, please visit our: Qwen Home Page Chat with Qwen (with Deep Research and Web Dev) Blog GitHub Hugging Face ModelScope Qwen3 Collection Join our community by joining our Discord and WeChat group. We are looking forward to seeing you there! Next Quickstart Copyright &#169; 2024, Qwen Team Made with Sphinx and @pradyunsg 's Furo On this page Welcome to Qwen! Qwen3-2507 Qwen3 Resource &amp; Links ",
      "fetch_method": "direct-html"
    },
    {
      "arxiv_id": "https://qwen.readthedocs.io/en/latest/",
      "full_text": " Qwen Contents Menu Expand Light mode Dark mode Auto light/dark, in light mode Auto light/dark, in dark mode Hide navigation sidebar Hide table of contents sidebar Skip to content Toggle site navigation sidebar Qwen Toggle Light / Dark / Auto color theme Toggle table of contents sidebar Qwen Getting Started Quickstart Key Concepts Speed Benchmark Performance of Quantized Models Inference Transformers Run Locally llama.cpp Ollama MLX LM Deployment SGLang vLLM TGI dstack SkyPilot OpenLLM Quantization AWQ GPTQ llama.cpp Training Axolotl LLaMA-Factory MS-SWIFT Unsloth verl Framework Qwen-Agent Function Calling LlamaIndex Langchain Back to top View this page Toggle Light / Dark / Auto color theme Toggle table of contents sidebar Welcome to Qwen! ¶ Qwen is the large language model and large multimodal model series of the Qwen Team, Alibaba Group. Both language models and multimodal models are pretrained on large-scale multilingual and multimodal data and post-trained on quality data for aligning to human preferences. Qwen is capable of natural language understanding, text generation, vision understanding, audio understanding, tool use, role play, playing as AI agent, etc. Qwen3-2507 ¶ With input from the community and insights from further research, Instruct-only and Thinking-only models are coming back! The results are Qwen3-2507: Qwen3-Instruct-2507 has the following features: Significant improvements in general capabilities, including instruction following, logical reasoning, text comprehension, mathematics, science, coding and tool usage . Substantial gains in long-tail knowledge coverage across multiple languages . Markedly better alignment with user preferences in subjective and open-ended tasks , enabling more helpful responses and higher-quality text generation. Enhanced capabilities in 256K long-context understanding , extensible to 1M. Qwen3-Thinking-2507 has the following features: Significantly improved performance on reasoning tasks, including logical reasoning, mathematics, science, coding, and academic benchmarks that typically require human expertise — achieving state-of-the-art results among open-source thinking models . Markedly better general capabilities , such as instruction following, tool usage, text generation, and alignment with human preferences. Enhanced 256K long-context understanding capabilities, extensible to 1M. Qwen3 ¶ Qwen3, aka Qwen3-2504, has the following features: Dense and Mixture-of-Experts (MoE) models , available in 0.6B, 1.7B, 4B, 8B, 14B, 32B and 30B-A3B, 235B-A22B. Seamless switching between thinking mode (for complex logical reasoning, math, and coding) and non-thinking mode (for efficient, general-purpose chat) within a single model , ensuring optimal performance across various scenarios. Significantly enhancement in reasoning capabilities , surpassing previous QwQ (in thinking mode) and Qwen2.5 instruct models (in non-thinking mode) on mathematics, code generation, and commonsense logical reasoning. Superior human preference alignment , excelling in creative writing, role-playing, multi-turn dialogues, and instruction following, to deliver a more natural, engaging, and immersive conversational experience. Expertise in agent capabilities , enabling precise integration with external tools in both thinking and unthinking modes and achieving leading performance among open-source models in complex agent-based tasks. Support of 100+ languages and dialects with strong capabilities for multilingual instruction following and translation . Resource &amp; Links ¶ For more information, please visit our: Qwen Home Page Chat with Qwen (with Deep Research and Web Dev) Blog GitHub Hugging Face ModelScope Qwen3 Collection Join our community by joining our Discord and WeChat group. We are looking forward to seeing you there! Next Quickstart Copyright &#169; 2024, Qwen Team Made with Sphinx and @pradyunsg 's Furo On this page Welcome to Qwen! Qwen3-2507 Qwen3 Resource &amp; Links ",
      "fetch_method": "direct-html"
    },
    {
      "arxiv_id": "https://qwen.readthedocs.io/zh-cn/latest/",
      "full_text": " Qwen Contents Menu Expand Light mode Dark mode Auto light/dark, in light mode Auto light/dark, in dark mode Hide navigation sidebar Hide table of contents sidebar Skip to content Toggle site navigation sidebar Qwen Toggle Light / Dark / Auto color theme Toggle table of contents sidebar Qwen 快速开始 快速开始 核心概念 效率评估 量化模型效果评估 推理 Transformers 本地运行 llama.cpp Ollama MLX LM 部署 SGLang vLLM TGI dstack SkyPilot OpenLLM 量化 AWQ GPTQ llama.cpp 训练 Axolotl LLaMA-Factory MS-SWIFT Unsloth verl 框架 Qwen-Agent 函数调用 LlamaIndex Langchain Back to top View this page Toggle Light / Dark / Auto color theme Toggle table of contents sidebar 欢迎来到Qwen ¶ Qwen是阿里巴巴集团Qwen团队研发的大语言模型和大型多模态模型系列。无论是语言模型还是多模态模型，均在大规模多语言和多模态数据上进行预训练，并通过高质量数据进行后期微调以贴近人类偏好。Qwen具备自然语言理解、文本生成、视觉理解、音频理解、工具使用、角色扮演、作为AI Agent进行互动等多种能力。 Qwen3-2507 ¶ With input from the community and insights from further research, Instruct-only and Thinking-only models are coming back! The results are Qwen3-2507: Qwen3-Instruct-2507 has the following features: Significant improvements in general capabilities, including instruction following, logical reasoning, text comprehension, mathematics, science, coding and tool usage . Substantial gains in long-tail knowledge coverage across multiple languages . Markedly better alignment with user preferences in subjective and open-ended tasks , enabling more helpful responses and higher-quality text generation. Enhanced capabilities in 256K long-context understanding , extensible to 1M. Qwen3-Thinking-2507 has the following features: Significantly improved performance on reasoning tasks, including logical reasoning, mathematics, science, coding, and academic benchmarks that typically require human expertise — achieving state-of-the-art results among open-source thinking models . Markedly better general capabilities , such as instruction following, tool usage, text generation, and alignment with human preferences. Enhanced 256K long-context understanding capabilities, extensible to 1M. Qwen3 ¶ Qwen3, aka Qwen3-2504, has the following features: 全尺寸稠密与混合专家模型 ：0.6B, 1.7B, 4B, 8B, 14B, 32B and 30B-A3B, 235B-A22B 支持在**思考模式**（用于复杂逻辑推理、数学和编码）和 非思考模式 （用于高效通用对话）之间**无缝切换**，确保在各种场景下的最佳性能。 显著增强的推理能力 ，在数学、代码生成和常识逻辑推理方面超越了之前的 QwQ（在思考模式下）和 Qwen2.5 指令模型（在非思考模式下）。 卓越的人类偏好对齐 ，在创意写作、角色扮演、多轮对话和指令跟随方面表现出色，提供更自然、更吸引人和更具沉浸感的对话体验。 擅长智能体能力 ，可以在思考和非思考模式下精确集成外部工具，在复杂的基于代理的任务中在开源模型中表现领先。 支持 100 多种语言和方言 ，具有强大的多语言理解、推理、指令跟随和生成能力。 Resource &amp; Links ¶ 想了解更多信息，欢迎访问： Qwen Home Page Chat with Qwen (with Deep Research and Web Dev) 博客 GitHub Hugging Face ModelScope Qwen3 Collection 加入社区，加入 Discord 和 微信群 。很期待见到你们！ Next 快速开始 Copyright &#169; 2024, Qwen Team Made with Sphinx and @pradyunsg 's Furo On this page 欢迎来到Qwen Qwen3-2507 Qwen3 Resource &amp; Links ",
      "fetch_method": "direct-html"
    },
    {
      "arxiv_id": "https://qwenlm.github.io/blog/qwen3",
      "full_text": " Qwen3: Think Deeper, Act Faster | Qwen Blog Publication About Try Qwen Chat &nbsp; Qwen3: Think Deeper, Act Faster April 29, 2025 &nbsp;·&nbsp;10 min&nbsp;·&nbsp;2036 words&nbsp;·&nbsp;Qwen Team&nbsp;|&nbsp;Translations: 简体中文 QWEN CHAT GitHub Hugging Face ModelScope Kaggle DEMO DISCORD Introduction # Today, we are excited to announce the release of Qwen3 , the latest addition to the Qwen family of large language models. Our flagship model, Qwen3-235B-A22B , achieves competitive results in benchmark evaluations of coding, math, general capabilities, etc., when compared to other top-tier models such as DeepSeek-R1, o1, o3-mini, Grok-3, and Gemini-2.5-Pro. Additionally, the small MoE model, Qwen3-30B-A3B , outcompetes QwQ-32B with 10 times of activated parameters, and even a tiny model like Qwen3-4B can rival the performance of Qwen2.5-72B-Instruct. We are open-weighting two MoE models: Qwen3-235B-A22B , a large model with 235 billion total parameters and 22 billion activated parameters, and Qwen3-30B-A3B , a smaller MoE model with 30 billion total parameters and 3 billion activated parameters. Additionally, six dense models are also open-weighted, including Qwen3-32B , Qwen3-14B , Qwen3-8B , Qwen3-4B , Qwen3-1.7B , and Qwen3-0.6B , under Apache 2.0 license. Models Layers Heads (Q / KV) Tie Embedding Context Length Qwen3-0.6B 28 16 / 8 Yes 32K Qwen3-1.7B 28 16 / 8 Yes 32K Qwen3-4B 36 32 / 8 Yes 32K Qwen3-8B 36 32 / 8 No 128K Qwen3-14B 40 40 / 8 No 128K Qwen3-32B 64 64 / 8 No 128K Models Layers Heads (Q / KV) # Experts (Total / Activated) Context Length Qwen3-30B-A3B 48 32 / 4 128 / 8 128K Qwen3-235B-A22B 94 64 / 4 128 / 8 128K The post-trained models, such as Qwen3-30B-A3B , along with their pre-trained counterparts (e.g., Qwen3-30B-A3B-Base ), are now available on platforms like Hugging Face , ModelScope , and Kaggle . For deployment, we recommend using frameworks like SGLang and vLLM . For local usage, tools such as Ollama , LMStudio , MLX , llama.cpp , and KTransformers are highly recommended. These options ensure that users can easily integrate Qwen3 into their workflows, whether in research, development, or production environments. We believe that the release and open-sourcing of Qwen3 will significantly advance the research and development of large foundation models. Our goal is to empower researchers, developers, and organizations around the world to build innovative solutions using these cutting-edge models. Feel free to try Qwen3 out in Qwen Chat Web ( chat.qwen.ai ) and mobile APP! Key Features # Hybrid Thinking Modes Qwen3 models introduce a hybrid approach to problem-solving. They support two modes: Thinking Mode: In this mode, the model takes time to reason step by step before delivering the final answer. This is ideal for complex problems that require deeper thought. Non-Thinking Mode: Here, the model provides quick, near-instant responses, suitable for simpler questions where speed is more important than depth. This flexibility allows users to control how much &ldquo;thinking&rdquo; the model performs based on the task at hand. For example, harder problems can be tackled with extended reasoning, while easier ones can be answered directly without delay. Crucially, the integration of these two modes greatly enhances the model&rsquo;s ability to implement stable and efficient thinking budget control. As demonstrated above, Qwen3 exhibits scalable and smooth performance improvements that are directly correlated with the computational reasoning budget allocated. This design enables users to configure task-specific budgets with greater ease, achieving a more optimal balance between cost efficiency and inference quality. Multilingual Support Qwen3 models are supporting 119 languages and dialects . This extensive multilingual capability opens up new possibilities for international applications, enabling users worldwide to benefit from the power of these models. Language Family Languages & Dialects Indo-European English, French, Portuguese, German, Romanian, Swedish, Danish, Bulgarian, Russian, Czech, Greek, Ukrainian, Spanish, Dutch, Slovak, Croatian, Polish, Lithuanian, Norwegian Bokmål, Norwegian Nynorsk, Persian, Slovenian, Gujarati, Latvian, Italian, Occitan, Nepali, Marathi, Belarusian, Serbian, Luxembourgish, Venetian, Assamese, Welsh, Silesian, Asturian, Chhattisgarhi, Awadhi, Maithili, Bhojpuri, Sindhi, Irish, Faroese, Hindi, Punjabi, Bengali, Oriya, Tajik, Eastern Yiddish, Lombard, Ligurian, Sicilian, Friulian, Sardinian, Galician, Catalan, Icelandic, Tosk Albanian, Limburgish, Dari, Afrikaans, Macedonian, Sinhala, Urdu, Magahi, Bosnian, Armenian Sino-Tibetan Chinese (Simplified Chinese, Traditional Chinese, Cantonese), Burmese Afro-Asiatic Arabic (Standard, Najdi, Levantine, Egyptian, Moroccan, Mesopotamian, Ta&rsquo;izzi-Adeni, Tunisian), Hebrew, Maltese Austronesian Indonesian, Malay, Tagalog, Cebuano, Javanese, Sundanese, Minangkabau, Balinese, Banjar, Pangasinan, Iloko, Waray (Philippines) Dravidian Tamil, Telugu, Kannada, Malayalam Turkic Turkish, North Azerbaijani, Northern Uzbek, Kazakh, Bashkir, Tatar Tai-Kadai Thai, Lao Uralic Finnish, Estonian, Hungarian Austroasiatic Vietnamese, Khmer Other Japanese, Korean, Georgian, Basque, Haitian, Papiamento, Kabuverdianu, Tok Pisin, Swahili Improved Agentic Capabilities We have optimized the Qwen3 models for coding and agentic capabilities, and also we have strengthened the support of MCP as well. Below we provide examples to show how Qwen3 thinks and interacts with the environment. Pre-training # In terms of pretraining, the dataset for Qwen3 has been significantly expanded compared to Qwen2.5. While Qwen2.5 was pre-trained on 18 trillion tokens, Qwen3 uses nearly twice that amount, with approximately 36 trillion tokens covering 119 languages and dialects. To build this large dataset, we collected data not only from the web but also from PDF-like documents. We used Qwen2.5-VL to extract text from these documents and Qwen2.5 to improve the quality of the extracted content. To increase the amount of math and code data, we used Qwen2.5-Math and Qwen2.5-Coder to generate synthetic data. This includes textbooks, question-answer pairs, and code snippets. The pre-training process consists of three stages. In the first stage (S1), the model was pretrained on over 30 trillion tokens with a context length of 4K tokens. This stage provided the model with basic language skills and general knowledge. In the second stage (S2), we improved the dataset by increasing the proportion of knowledge-intensive data, such as STEM, coding, and reasoning tasks. The model was then pretrained on an additional 5 trillion tokens. In the final stage, we used high-quality long-context data to extend the context length to 32K tokens. This ensures the model can handle longer inputs effectively. Due to advancements in model architecture, increase in training data, and more effective training methods, the overall performance of Qwen3 dense base models matches that of Qwen2.5 base models with more parameters. For instance, Qwen3-1.7B/4B/8B/14B/32B-Base performs as well as Qwen2.5-3B/7B/14B/32B/72B-Base, respectively. Notably, in areas like STEM, coding, and reasoning, Qwen3 dense base models even outperform larger Qwen2.5 models. For Qwen3-MoE base models, they achieve similar performance to Qwen2.5 dense base models while using only 10% of the active parameters. This results in significant savings in both training and inference costs. Post-training # To develop the hybrid model capable of both step-by-step reasoning and rapid responses, we implemented a four-stage training pipeline. This pipeline includes: (1) long chain-of-thought (CoT) cold start, (2) reasoning-based reinforcement learning (RL), (3) thinking mode fusion, and (4) general RL. In the first stage, we fine-tuned the models using diverse long CoT data, covering various tasks and domains such as mathematics, coding, logical reasoning, and STEM problems. This process aimed to equip the model with fundamental reasoning abilities. The second stage focused on scaling up computational resources for RL, utilizing rule-based rewards to enhance the model&rsquo;s exploration and exploitation capabilities. In the third stage, we integrated non-thinking capabilities into the thinking model by fine-tuning it on a combination of long CoT data and commonly used instruction-tuning data. This data was generated by the enhanced thinking model from the second stage, ensuring a seamless blend of reasoning and quick response capabilities. Finally, in the fourth stage, we applied RL across more than 20 general-domain tasks to further strengthen the model’s general capabilities and correct undesired behaviors. These tasks included instruction following, format following, and agent capabilities, etc. Develop with Qwen3 # Below is a simple guide for you to use Qwen3 on different frameworks. First of all, we provide an standard example of using Qwen3-30B-A3B in Hugging Face transformers: from modelscope import AutoModelForCausalLM , AutoTokenizer model_name = &#34;Qwen/Qwen3-30B-A3B&#34; # load the tokenizer and the model tokenizer = AutoTokenizer . from_pretrained ( model_name ) model = AutoModelForCausalLM . from_pretrained ( model_name , torch_dtype = &#34;auto&#34; , device_map = &#34;auto&#34; ) # prepare the model input prompt = &#34;Give me a short introduction to large language model.&#34; messages = [ { &#34;role&#34; : &#34;user&#34; , &#34;content&#34; : prompt } ] text = tokenizer . apply_chat_template ( messages , tokenize = False , add_generation_prompt = True , enable_thinking = True # Switch between thinking and non-thinking modes. Default is True. ) model_inputs = tokenizer ([ text ], return_tensors = &#34;pt&#34; ) . to ( model . device ) # conduct text completion generated_ids = model . generate ( ** model_inputs , max_new_tokens = 32768 ) output_ids = generated_ids [ 0 ][ len ( model_inputs . input_ids [ 0 ]):] . tolist () # parsing thinking content try : # rindex finding 151668 (&lt;/think&gt;) index = len ( output_ids ) - output_ids [:: - 1 ] . index ( 151668 ) except ValueError : index = 0 thinking_content = tokenizer . decode ( output_ids [: index ], skip_special_tokens = True ) . strip ( &#34; \\n &#34; ) content = tokenizer . decode ( output_ids [ index :], skip_special_tokens = True ) . strip ( &#34; \\n &#34; ) print ( &#34;thinking content:&#34; , thinking_content ) print ( &#34;content:&#34; , content ) To disable thinking, you just need to make changes to the argument enable_thinking like the following: text = tokenizer . apply_chat_template ( messages , tokenize = False , add_generation_prompt = True , enable_thinking = False # True is the default value for enable_thinking. ) For deployment, you can use sglang>=0.4.6.post1 or vllm>=0.8.4 to create an OpenAI-compatible API endpoint: SGLang: python -m sglang.launch_server --model-path Qwen/Qwen3-30B-A3B --reasoning-parser qwen3 vLLM: vllm serve Qwen/Qwen3-30B-A3B --enable-reasoning --reasoning-parser deepseek_r1 If you use it for local development, you can use ollama by running a simple command ollama run qwen3:30b-a3b to play with the model, or you can use LMStudio or llama.cpp and ktransformers to build locally. Advanced Usages # We provide a soft switch mechanism that allows users to dynamically control the model&rsquo;s behavior when enable_thinking=True. Specifically, you can add /think and /no_think to user prompts or system messages to switch the model&rsquo;s thinking mode from turn to turn. The model will follow the most recent instruction in multi-turn conversations. Here is an example of a multi-turn conversation: from transformers import AutoModelForCausalLM , AutoTokenizer class QwenChatbot : def __init__ ( self , model_name = &#34;Qwen/Qwen3-30B-A3B&#34; ): self . tokenizer = AutoTokenizer . from_pretrained ( model_name ) self . model = AutoModelForCausalLM . from_pretrained ( model_name ) self . history = [] def generate_response ( self , user_input ): messages = self . history + [{ &#34;role&#34; : &#34;user&#34; , &#34;content&#34; : user_input }] text = self . tokenizer . apply_chat_template ( messages , tokenize = False , add_generation_prompt = True ) inputs = self . tokenizer ( text , return_tensors = &#34;pt&#34; ) response_ids = self . model . generate ( ** inputs , max_new_tokens = 32768 )[ 0 ][ len ( inputs . input_ids [ 0 ]):] . tolist () response = self . tokenizer . decode ( response_ids , skip_special_tokens = True ) # Update history self . history . append ({ &#34;role&#34; : &#34;user&#34; , &#34;content&#34; : user_input }) self . history . append ({ &#34;role&#34; : &#34;assistant&#34; , &#34;content&#34; : response }) return response # Example Usage if __name__ == &#34;__main__&#34; : chatbot = QwenChatbot () # First input (without /think or /no_think tags, thinking mode is enabled by default) user_input_1 = &#34;How many r&#39;s in strawberries?&#34; print ( f &#34;User: { user_input_1 } &#34; ) response_1 = chatbot . generate_response ( user_input_1 ) print ( f &#34;Bot: { response_1 } &#34; ) print ( &#34;----------------------&#34; ) # Second input with /no_think user_input_2 = &#34;Then, how many r&#39;s in blueberries? /no_think&#34; print ( f &#34;User: { user_input_2 } &#34; ) response_2 = chatbot . generate_response ( user_input_2 ) print ( f &#34;Bot: { response_2 } &#34; ) print ( &#34;----------------------&#34; ) # Third input with /think user_input_3 = &#34;Really? /think&#34; print ( f &#34;User: { user_input_3 } &#34; ) response_3 = chatbot . generate_response ( user_input_3 ) print ( f &#34;Bot: { response_3 } &#34; ) Agentic Usages # Qwen3 excels in tool calling capabilities. We recommend using Qwen-Agent to make the best use of agentic ability of Qwen3. Qwen-Agent encapsulates tool-calling templates and tool-calling parsers internally, greatly reducing coding complexity. To define the available tools, you can use the MCP configuration file, use the integrated tool of Qwen-Agent, or integrate other tools by yourself. from qwen_agent.agents import Assistant # Define LLM llm_cfg = { &#39;model&#39; : &#39;Qwen3-30B-A3B&#39; , # Use the endpoint provided by Alibaba Model Studio: # &#39;model_type&#39;: &#39;qwen_dashscope&#39;, # &#39;api_key&#39;: os.getenv(&#39;DASHSCOPE_API_KEY&#39;), # Use a custom endpoint compatible with OpenAI API: &#39;model_server&#39; : &#39;http://localhost:8000/v1&#39; , # api_base &#39;api_key&#39; : &#39;EMPTY&#39; , # Other parameters: # &#39;generate_cfg&#39;: { # # Add: When the response content is `&lt;think&gt;this is the thought&lt;/think&gt;this is the answer; # # Do not add: When the response has been separated by reasoning_content and content. # &#39;thought_in_content&#39;: True, # }, } # Define Tools tools = [ { &#39;mcpServers&#39; : { # You can specify the MCP configuration file &#39;time&#39; : { &#39;command&#39; : &#39;uvx&#39; , &#39;args&#39; : [ &#39;mcp-server-time&#39; , &#39;--local-timezone=Asia/Shanghai&#39; ] }, &#34;fetch&#34; : { &#34;command&#34; : &#34;uvx&#34; , &#34;args&#34; : [ &#34;mcp-server-fetch&#34; ] } } }, &#39;code_interpreter&#39; , # Built-in tools ] # Define Agent bot = Assistant ( llm = llm_cfg , function_list = tools ) # Streaming generation messages = [{ &#39;role&#39; : &#39;user&#39; , &#39;content&#39; : &#39;https://qwenlm.github.io/blog/ Introduce the latest developments of Qwen&#39; }] for responses in bot . run ( messages = messages ): pass print ( responses ) Friends of Qwen # Thanks to the support of so many friends. Qwen is nothing without its friends! We welcome more people or organizations to join our community and help us become better! Future Work # Qwen3 represents a significant milestone in our journey toward Artificial General Intelligence (AGI) and Artificial Superintelligence (ASI). By scaling up both pretraining and reinforcement learning (RL), we have achieved higher levels of intelligence. We have seamlessly integrated thinking and non-thinking modes, offering users the flexibility to control the thinking budget. Additionally, we have expanded support for a wide range of languages, enhancing global accessibility. Looking ahead, we aim to enhance our models across multiple dimensions. This includes refining model architectures and training methodologies to achieve several key objectives: scaling data, increasing model size, extending context length, broadening modalities, and advancing RL with environmental feedback for long-horizon reasoning. We believe we are transitioning from an era focused on training models to one centered on training agents. Our next iteration promises to bring meaningful advancements to everyone&rsquo;s work and life. &copy; 2025 Qwen Powered by Hugo ",
      "fetch_method": "direct-html"
    },
    {
      "arxiv_id": "https://qwenlm.github.io/blog/qwen2.5",
      "full_text": " Qwen2.5: A Party of Foundation Models! | Qwen Blog Publication About Try Qwen Chat &nbsp; Qwen2.5: A Party of Foundation Models! September 19, 2024 &nbsp;·&nbsp;9 min&nbsp;·&nbsp;1738 words&nbsp;·&nbsp;Qwen Team&nbsp;|&nbsp;Translations: 简体中文 GITHUB HUGGING FACE MODELSCOPE DEMO DISCORD Introduction # In the past three months since Qwen2&rsquo;s release, numerous developers have built new models on the Qwen2 language models, providing us with valuable feedback. During this period, we have focused on creating smarter and more knowledgeable language models. Today, we are excited to introduce the latest addition to the Qwen family: Qwen2.5 . We are announcing what might be the largest opensource release in history! Let&rsquo;s get the party started! Our latest release features the LLMs Qwen2.5 , along with specialized models for coding, Qwen2.5-Coder , and mathematics, Qwen2.5-Math . All open-weight models are dense, decoder-only language models, available in various sizes, including: Qwen2.5: 0.5B, 1.5B, 3B, 7B, 14B, 32B, and 72B Qwen2.5-Coder: 1.5B, 7B, and 32B on the way Qwen2.5-Math: 1.5B, 7B, and 72B. All our open-source models, except for the 3B and 72B variants, are licensed under Apache 2.0. You can find the license files in the respective Hugging Face repositories. In addition to these models, we offer APIs for our flagship language models: Qwen-Plus and Qwen-Turbo through Model Studio, and we encourage you to explore them! Furthermore, we have also open-sourced the Qwen2-VL-72B , which features performance enhancements compared to last month&rsquo;s release. For more details about Qwen2.5, Qwen2.5-Coder, and Qwen2.5-Math, feel free to visit the following links: Qwen2.5 LLM Qwen2.5-Coder Qwen2.5-Math Get ready to unlock a world of possibilities with our extensive lineup of models! We&rsquo;re excited to share these cutting-edge models with you, and we can&rsquo;t wait to see the incredible things you&rsquo;ll achieve with them! Takeaways # In terms of Qwen2.5 , the language models, all models are pretrained on our latest large-scale dataset, encompassing up to 18 trillion tokens. Compared to Qwen2, Qwen2.5 has acquired significantly more knowledge (MMLU: 85+) and has greatly improved capabilities in coding (HumanEval 85+) and mathematics (MATH 80+). Additionally, the new models achieve significant improvements in instruction following, generating long texts (over 8K tokens), understanding structured data (e.g, tables), and generating structured outputs especially JSON. Qwen2.5 models are generally more resilient to the diversity of system prompts, enhancing role-play implementation and condition-setting for chatbots. Like Qwen2, the Qwen2.5 language models support up to 128K tokens and can generate up to 8K tokens. They also maintain multilingual support for over 29 languages, including Chinese, English, French, Spanish, Portuguese, German, Italian, Russian, Japanese, Korean, Vietnamese, Thai, Arabic, and more. Below, we provide basic information about the models and details of the supported languages. The specialized expert language models, namely Qwen2.5-Coder for coding and Qwen2.5-Math for mathematics, have undergone substantial enhancements compared to their predecessors, CodeQwen1.5 and Qwen2-Math. Specifically, Qwen2.5-Coder has been trained on 5.5 trillion tokens of code-related data, enabling even smaller coding-specific models to deliver competitive performance against larger language models on coding evaluation benchmarks. Meanwhile, Qwen2.5-Math supports both Chinese and English and incorporates various reasoning methods, including Chain-of-Thought (CoT), Program-of-Thought (PoT), and Tool-Integrated Reasoning (TIR). Performance # Qwen2.5 # To showcase Qwen2.5&rsquo;s capabilities, we benchmark our largest open-source model, Qwen2.5-72B - a 72B-parameter dense decoder-only language model - against leading open-source models like Llama-3.1-70B and Mistral-Large-V2. We present comprehensive results from instruction-tuned versions across various benchmarks, evaluating both model capabilities and human preferences. Besides the instruction-tuned language models, we figure out that the base language model of our flagship opensource model Qwen2.5-72B reaches top-tier performance even against larger models like Llama-3-405B. Furthermore, we benchmark the latest version of our API-based model, Qwen-Plus , against leading proprietary and open-source models, including GPT4-o, Claude-3.5-Sonnet, Llama-3.1-405B, and DeepSeek-V2.5. This comparison showcases Qwen-Plus&rsquo;s competitive standing in the current landscape of large language models. We show that Qwen-Plus significantly outcompetes DeepSeek-V2.5 and demonstrates competitive performance against Llama-3.1-405B, while still underperforming compared to GPT4-o and Claude-3.5-Sonnet in some aspects. This benchmarking not only highlights Qwen-Plus&rsquo;s strengths but also identifies areas for future improvement, reinforcing our commitment to continuous enhancement and innovation in the field of large language models. A significant update in Qwen2.5 is the reintroduction of our 14B and 32B models, Qwen2.5-14B and Qwen2.5-32B . These models outperform baseline models of comparable or larger sizes, such as Phi-3.5-MoE-Instruct and Gemma2-27B-IT, across diverse tasks. They achieve an optimal balance between model size and capability, delivering performance that matches or exceeds some larger models. Additionally, our API-based model, Qwen-Turbo , offers highly competitive performance compared to the two open-source models, while providing a cost-effective and rapid service. In recent times, there has been a notable shift towards small language models (SLMs). Although SLMs have historically trailed behind their larger counterparts (LLMs), the performance gap is rapidly diminishing. Remarkably, even models with just 3 billion parameters are now delivering highly competitive results. The accompanying figure illustrates a significant trend: newer models achieving scores above 65 in MMLU are increasingly smaller, underscoring the accelerated growth in knowledge density among language models. Notably, our Qwen2.5-3B stands out as a prime example, achieving impressive performance with only around 3 billion parameters, showcasing its efficiency and capability compared to its predecessors. In addition to the notable enhancements in benchmark evaluations, we have refined our post-training methodologies. Our four key updates include support for long text generation of up to 8K tokens, significantly improved comprehension of structured data, more reliable generation of structured outputs, particularly in JSON format, and enhanced performance across diverse system prompts, which facilitates effective role-playing. Check the LLM blog for details about how to leverage these capabilities. Qwen2.5-Coder # Since the launch of CodeQwen1.5, we have attracted numerous users who rely on this model for various coding tasks, such as debugging, answering coding-related questions, and providing code suggestions. Our latest iteration, Qwen2.5-Coder, is specifically designed for coding applications. In this section, we present the performance results of Qwen2.5-Coder-7B-Instruct, benchmarked against leading open-source models, including those with significantly larger parameter sizes. We believe that Qwen2.5-Coder is an excellent choice as your personal coding assistant. Despite its smaller size, it outperforms many larger language models across a range of programming languages and tasks, demonstrating its exceptional coding capabilities. Qwen2.5-Math # In terms of the math specific language models, we released the first models, Qwen2-Math, last month, and this time, compared to Qwen2-Math, Qwen2.5-Math has been pretrained larger-scale of math related data, including the synthetic data generated by Qwen2-Math. Additionally we extend the support of Chinese this time and we also strengthen its reasoning capabilities by endowing it with the abilities to perform CoT, PoT, and TIR. The general performance of Qwen2.5-Math-72B-Instruct surpasses both Qwen2-Math-72B-Instruct and GPT4-o, and even very small expert model like Qwen2.5-Math-1.5B-Instruct can achieve highly competitive performance against large language models. Develop with Qwen2.5 # The simplest way to use is through Hugging Face Transfomer as demonstrated in the model card : from transformers import AutoModelForCausalLM , AutoTokenizer model_name = &#34;Qwen/Qwen2.5-7B-Instruct&#34; model = AutoModelForCausalLM . from_pretrained ( model_name , torch_dtype = &#34;auto&#34; , device_map = &#34;auto&#34; ) tokenizer = AutoTokenizer . from_pretrained ( model_name ) prompt = &#34;Give me a short introduction to large language model.&#34; messages = [ { &#34;role&#34; : &#34;user&#34; , &#34;content&#34; : prompt } ] text = tokenizer . apply_chat_template ( messages , tokenize = False , add_generation_prompt = True ) model_inputs = tokenizer ([ text ], return_tensors = &#34;pt&#34; ) . to ( model . device ) generated_ids = model . generate ( ** model_inputs , max_new_tokens = 512 ) generated_ids = [ output_ids [ len ( input_ids ):] for input_ids , output_ids in zip ( model_inputs . input_ids , generated_ids ) ] response = tokenizer . batch_decode ( generated_ids , skip_special_tokens = True )[ 0 ] To use Qwen2.5 with vLLM, running the following command can deploy an OpenAI API compatible service: python - m vllm . entrypoints . openai . api_server \\ -- model Qwen / Qwen2 .5 - 7 B - Instruct or use vllm serve if you use vllm>=0.5.3 . Then you can communicate with Qwen2.5 via curl : curl http://localhost:8000/v1/chat/completions -H &#34;Content-Type: application/json&#34; -d &#39;{ &#34;model&#34;: &#34;Qwen/Qwen2.5-7B-Instruct&#34;, &#34;messages&#34;: [ {&#34;role&#34;: &#34;user&#34;, &#34;content&#34;: &#34;Tell me something about large language models.&#34;} ], &#34;temperature&#34;: 0.7, &#34;top_p&#34;: 0.8, &#34;repetition_penalty&#34;: 1.05, &#34;max_tokens&#34;: 512 }&#39; Furthermore, Qwen2.5 supports vllm&rsquo;s built-in tool calling. This functionality requires vllm>=0.6 . If you want to enable this functionality, please start vllm&rsquo;s OpenAI-compatible service with: vllm serve Qwen/Qwen2.5-7B-Instruct --enable-auto-tool-choice --tool-call-parser hermes You can then use it in the same way you use GPT&rsquo;s tool calling . Qwen2.5 also supports Ollama&rsquo;s tool calling . You can use it by starting Ollama&rsquo;s OpenAI-compatible service and using it in the same way you use GPT&rsquo;s tool calling. Qwen2.5&rsquo;s chat template also includes a tool calling template, meaning that you can use Hugging Face transformers&rsquo; tool calling support . The vllm / Ollama / transformers tool calling support uses a tool calling template inspired by Nous&rsquo; Hermes . Historically, Qwen-Agent provided tool calling support using Qwen2&rsquo;s own tool calling template (which is harder to be integrated with vllm and Ollama), and Qwen2.5 maintains compatibility with Qwen2&rsquo;s template and Qwen-Agent as well. Friends of Qwen # 💗 Qwen is nothing without its friends! So many thanks to the support of these old buddies and new friends : Hugging Face Transformers Finetuning: Peft , ChatLearn , Llama-Factory , Axolotl , Firefly , Swift , XTuner , Unsloth , Liger Kernel Quantization: AutoGPTQ , AutoAWQ , Neural Compressor Deployment: vLLM , SGL , SkyPilot , TensorRT-LLM , OpenVino , TGI , Xinference API Platforms: Together , Fireworks , OpenRouter , Sillicon Flow Local Run: MLX , Llama.cpp , Ollama , LM Studio , Jan Agent and RAG Frameworks: Dify , LlamaIndex , CrewAI Evaluation: LMSys , OpenCompass , Open LLM Leaderboard Model Training: Arcee AI , Sailor , Dolphin , Openbuddy We would like to extend our heartfelt gratitude to the numerous teams and individuals who have contributed to Qwen, even if they haven&rsquo;t been specifically mentioned. Your support is invaluable, and we warmly invite more friends to join us in this exciting journey. Together, we can enhance collaboration and drive forward the research and development of the open-source AI community, making it stronger and more innovative than ever before. What&rsquo;s Next? # While we are thrilled to launch numerous high-quality models simultaneously, we recognize that significant challenges remain. Our recent releases demonstrate our commitment to developing robust foundation models across language, vision-language, and audio-language domains. However, it is crucial to integrate these different modalities into a single model to enable seamless end-to-end processing of information across all three. Additionally, although we have made strides in enhancing reasoning capabilities through data scaling, we are inspired by the recent advancements in reinforcement learning (e.g., o1) and are dedicated to further improving our models&rsquo; reasoning abilities by scaling inference compute. We look forward to introducing you to the next generation of models soon! Stay tuned for more exciting developments! Citation # We are going to release the technical report for Qwen2.5 very soon. Before the release, feel free to cite our Qwen2 paper as well as this blog @misc{qwen2.5, title = {Qwen2.5: A Party of Foundation Models}, url = {https://qwenlm.github.io/blog/qwen2.5/}, author = {Qwen Team}, month = {September}, year = {2024} } @article{qwen2, title={Qwen2 technical report}, author={Yang, An and Yang, Baosong and Hui, Binyuan and Zheng, Bo and Yu, Bowen and Zhou, Chang and Li, Chengpeng and Li, Chengyuan and Liu, Dayiheng and Huang, Fei and others}, journal={arXiv preprint arXiv:2407.10671}, year={2024} } &copy; 2025 Qwen Powered by Hugo ",
      "fetch_method": "direct-html"
    },
    {
      "arxiv_id": "https://qwenlm.github.io/blog/qwen2/",
      "full_text": " Hello Qwen2 | Qwen Blog Publication About Try Qwen Chat &nbsp; Hello Qwen2 June 7, 2024 &nbsp;·&nbsp;15 min&nbsp;·&nbsp;3119 words&nbsp;·&nbsp;Qwen Team&nbsp;|&nbsp;Translations: 简体中文 GITHUB HUGGING FACE MODELSCOPE DEMO DISCORD Introduction # After months of efforts, we are pleased to announce the evolution from Qwen1.5 to Qwen2. This time, we bring to you: Pretrained and instruction-tuned models of 5 sizes, including Qwen2-0.5B, Qwen2-1.5B, Qwen2-7B, Qwen2-57B-A14B, and Qwen2-72B ; Having been trained on data in 27 additional languages besides English and Chinese; State-of-the-art performance in a large number of benchmark evaluations; Significantly improved performance in coding and mathematics; Extended context length support up to 128K tokens with Qwen2-7B-Instruct and Qwen2-72B-Instruct. We have opensourced the models in Hugging Face and ModelScope to you and we are looking forward to hearing from you! Model Information # The Qwen2 series include base and instruction-tuned models of 5 sizes, including Qwen2-0.5B, Qwen2-1.5B, Qwen2-7B, Qwen2-57B-A14B, Qwen2-72B. We illustrate the key information of the models in the following table: Models Qwen2-0.5B Qwen2-1.5B Qwen2-7B Qwen2-57B-A14B Qwen2-72B # Params 0.49B 1.54B 7.07B 57.41B 72.71B # Non-Emb Params 0.35B 1.31B 5.98B 56.32B 70.21B GQA True True True True True Tie Embedding True True False False False Context Length 32K 32K 128K 64K 128K Specifically, previously in Qwen1.5, only Qwen1.5-32B and Qwen1.5-110B have adopted Group Query Attention (GQA). This time, for all model sizes, we apply GQA so that they can enjoy the benefits of faster speed and less memory usage in model inference. For small models, we prefer the application of tying embedding as the large sparse embeddings take up a large proportion of the total model parameters. In terms of the context length, all base language models have been pretrained on data of the context length of 32K tokens, and we observe satisfactory extrapolation capabilities up to 128K in PPL evaluation. However, for instruction-tuned models, we are not satisfied with merely PPL evaluation; we need the models to be capable of correctly understanding long context and completing tasks. In the table, we list the context length capabilities of instruction-tuned models, as assessed through the evaluation of the Needle in a Haystack task. Notably, when augmented with YARN, both Qwen2-7B-Instruct and Qwen2-72B-Instruct models demonstrate an impressive capacity to handle context lengths extending up to 128K tokens. Significant efforts were directed towards augmenting both the volume and quality of pretraining and instruction-tuning datasets across a diverse linguistic spectrum, beyond English and Chinese, to bolster its multilingual competencies. Although large language models possess an inherent capacity to generalize to other languages, we explicitly highlight the inclusion of 27 additional languages in our training: Regions Languages Western Europe German, French, Spanish, Portuguese, Italian, Dutch Eastern & Central Europe Russian, Czech, Polish Middle East Arabic, Persian, Hebrew, Turkish Eastern Asia Japanese, Korean South-Eastern Asia Vietnamese, Thai, Indonesian, Malay, Lao, Burmese, Cebuano, Khmer, Tagalog Southern Asia Hindi, Bengali, Urdu Additionally, we have devoted significant effort to addressing code-switching, a frequent occurrence in multilingual evaluation. Consequently, our models&rsquo; proficiency in handling this phenomenon have notably enhanced. Evaluations using prompts that typically induce code-switching across languages confirm a substantial reduction in associated issues. Performance # Comparative assessments reveal substantial enhancements in performance for large-scale models (70B+ parameters) relative to Qwen1.5. Here our evaluation centers on the large-size model Qwen2-72B. In terms of base language models, Qwen2-72B and state-of-the-art open models are evaluated for different capbilities including natural language understanding, knowledge acquisition, coding proficiency, mathematical skills, and multilingual abilities. Benefiting from meticulously curated datasets and optimized training methods, Qwen2-72B exhibits superior performance compared to leading models such as Llama-3-70B. Notably, it surpasses the performance of its predecessor, Qwen1.5-110B, despite having fewer parameters. After extensive large-scale pre-training, we conduct post-training to further enhance Qwen&rsquo;s intelligence, bringing it closer to human. This process further improves the model&rsquo;s capabilities in areas such as coding, mathematics, reasoning, instruction following, multilingual understanding, and more. Additionally, it aligns the model&rsquo;s output with human values, ensuring that it is helpful, honest, and harmless. Our post-training phase is designed with the principle of scalable training with minimal human annotation. Specifically, we investigate how to obtain high-quality, reliable, diverse and creative demonstration data and preference data with various automated alignment strategies, such as rejection sampling for math, execution feedback for coding and instruction-following, back-translation for creative writing, scalable oversight for role-play, etc. As for training, we apply a combination of supervised fine-tuning, reward model training and online DPO training. We also employ a novel Online Merging Optimizer to minimize the alignment tax. These collective efforts have significantly boosted the capabilities and intelligence of our models, as illustrated in the following table. We comprehensively evaluate Qwen2-72B-Instruct on 16 benchmarks across various domains. Qwen2-72B-Instruct strikes a balance between obtaining better capabilities and aligning well with human values. Specifically, Qwen2-72B-Instruct significantly surpasses Qwen1.5-72B-Chat across all benchmarks, and also reaches competitive performance compared with Llama-3-70B-Instruct. 1 In terms of smaller models, our Qwen2 models also outcompete the SOTA models of similar or even larger sizes. In comparison with the very recently released SOTA models, Qwen2-7B-Instruct can still demonstrate advantages across benchmarks, showing specifically outstanding performance on coding and Chinese-related metrics. 1 Highlights # Coding & Mathematics # We have persistently dedicated our efforts to enhance the advanced capabilities of Qwen, particularly in coding and mathematics. In coding, we have successfully integrated the code training experience and data from CodeQwen1.5 , resulting in significant improvements in Qwen2-72B-Instruct across various programming languages. Regarding mathematics, by exploiting the extensive and high-quality datasets, Qwen2-72B-Instruct has reflects stronger capabilities in solving mathematic problems. Long Context Understanding # In Qwen2, all instruction-tuned models have been trained on 32k length contexts, and extrapolated to longer context lengths using techniques like YARN or Dual Chunk Attention . The figure below shows our test results on the Needle in a Haystack . Notably, Qwen2-72B-Instruct is capable of flawlessly handling information extraction tasks within a 128k context. Coupled with its inherent strong performance, it becomes the preferred choice for handling long text tasks when resources are sufficient. Additionally, it&rsquo;s worth noting the impressive capabilities of other models in the series: Qwen2-7B-Instruct nearly flawlessly handles contexts up to 128k in length, Qwen2-57B-A14B-Instruct manages contexts up to 64k, and the two smaller models in the lineup support contexts of 32k. Alongside the long-context models, we have also open-sourced an agent solution for efficiently processing documents containing up to 1 million tokens. For more details, see our dedicated blog post on this topic . Safety and Responsibility # The table below presents the proportion of harmful responses generated by large models for four categories of multilingual unsafe querys(Illegal Activity, Fraud, Pornography, Privacy Violence). The test data was derived from Jailbreak and translated into multiple languages for evaluation. We find that Llama-3 does not effectively handle multilingual prompts, and therefore, it is not included in the comparison. Through significance testing (P_value), we found that the Qwen2-72B-Instruct model performs comparably to GPT-4 in terms of safety, and significantly outperforms the Mistral-8x22B model. Language Illegal Activity Fraud Pornography Privacy Violence GPT-4 Mistral-8x22B Qwen2-72B-Instruct GPT-4 Mistral-8x22B Qwen2-72B-Instruct GPT-4 Mistral-8x22B Qwen2-72B-Instruct GPT-4 Mistral-8x22B Qwen2-72B-Instruct zh 0% 13% 0% 0% 17% 0% 43% 47% 53% 0% 10% 0% en 0% 7% 0% 0% 23% 0% 37% 67% 63% 0% 27% 3% ar 0% 13% 0% 0% 7% 0% 15% 26% 15% 3% 13% 0% es 0% 7% 0% 3% 0% 0% 48% 64% 50% 3% 7% 3% fr 0% 3% 0% 3% 3% 7% 3% 19% 7% 0% 27% 0% ko 0% 4% 0% 3% 8% 4% 17% 29% 10% 0% 26% 4% pt 0% 7% 0% 3% 7% 3% 47% 57% 47% 4% 26% 4% th 0% 10% 0% 7% 23% 3% 13% 17% 10% 13% 7% 7% vi 0% 4% 0% 4% 11% 0% 22% 26% 22% 0% 0% 0% Average 0% 8% 0% 3% 11% 2% 27% 39% 31% 3% 16% 2% Developing with Qwen2 # Now all models have been released in Hugging Face and ModelScope. Feel free to visit the model cards for detailed usages, and learn more information about each model, including its features, performance, etc. For a long time, a lot of friends have been supporting the development of Qwen, including finetuning ( Axolotl , Llama-Factory , Firefly , Swift , XTuner ), quantization ( AutoGPTQ , AutoAWQ , Neural Compressor ), deployment ( vLLM , SGL , SkyPilot , TensorRT-LLM , OpenVino , TGI ), API platforms ( Together , Fireworks , OpenRouter ), local run ( MLX , Llama.cpp , Ollama , LM Studio ), Agent and RAG Frameworks ( LlamaIndex , CrewAI , OpenDevin ) , Evaluation ( LMSys , OpenCompass , Open LLM Leaderboard ), model training ( Dolphin , Openbuddy ) etc. For how to use Qwen2 with the third-party frameworks, please refer to the respective documentation as well as our official documentation . Still there are a number of teams and people not mentioned that have made contributions to Qwen. We sincerely thank them for the support, and we hope that our collaboration can boost the research and development of the opensource AI community. License # This time, we change the licenses of our models to different ones. While Qwen2-72B as well as its instruction-tuned models still uses the original Qianwen License, all other models, including Qwen2-0.5B, Qwen2-1.5B, Qwen2-7B, and Qwen2-57B-A14B, turn to adopt Apache 2.0 ! We believe that the enhanced openness of our models to the community can accelerate the applications and commercial usages of Qwen2 all around the world. What&rsquo;s Next for Qwen2? # We are training larger Qwen2 models to further explore model scaling along with our recent data scaling. Additionally, we extend the Qwen2 language models to multimodal, capable of understanding both vision and audio information. In the near future, we will continue opensource new models to accelerate opensource AI. Stay tuned! Citation # If you find our work helpful, feel free to give us a cite! @article{qwen2, title={Qwen2 Technical Report}, author={An Yang and Baosong Yang and Binyuan Hui and Bo Zheng and Bowen Yu and Chang Zhou and Chengpeng Li and Chengyuan Li and Dayiheng Liu and Fei Huang and Guanting Dong and Haoran Wei and Huan Lin and Jialong Tang and Jialin Wang and Jian Yang and Jianhong Tu and Jianwei Zhang and Jianxin Ma and Jin Xu and Jingren Zhou and Jinze Bai and Jinzheng He and Junyang Lin and Kai Dang and Keming Lu and Keqin Chen and Kexin Yang and Mei Li and Mingfeng Xue and Na Ni and Pei Zhang and Peng Wang and Ru Peng and Rui Men and Ruize Gao and Runji Lin and Shijie Wang and Shuai Bai and Sinan Tan and Tianhang Zhu and Tianhao Li and Tianyu Liu and Wenbin Ge and Xiaodong Deng and Xiaohuan Zhou and Xingzhang Ren and Xinyu Zhang and Xipin Wei and Xuancheng Ren and Yang Fan and Yang Yao and Yichang Zhang and Yu Wan and Yunfei Chu and Yuqiong Liu and Zeyu Cui and Zhenru Zhang and Zhihao Fan}, journal={arXiv preprint arXiv:2407.10671}, year={2024} } Appendix # Base Language Model Evaluation # The evaluation of base models mainly focuses on the model performance of natural language understanding, general question answering, coding, mathematics, scientific knowledge, reasoning, multilingual capability, etc. The datasets for evaluation include: English Tasks : MMLU (5-shot), MMLU-Pro (5-shot), GPQA (5shot), Theorem QA (5-shot), BBH (3-shot), HellaSwag (10-shot), Winogrande (5-shot), TruthfulQA (0-shot), ARC-C (25-shot) Coding Tasks : EvalPlus (0-shot) (HumanEval, MBPP, HumanEval+, MBPP+), MultiPL-E (0-shot) (Python, C++, JAVA, PHP, TypeScript, C#, Bash, JavaScript) Math Tasks : GSM8K (4-shot), MATH (4-shot) Chinese Tasks : C-Eval(5-shot), CMMLU (5-shot) Multilingual Tasks : Multi-Exam (M3Exam 5-shot, IndoMMLU 3-shot, ruMMLU 5-shot, mMMLU 5-shot), Multi-Understanding (BELEBELE 5-shot, XCOPA 5-shot, XWinograd 5-shot, XStoryCloze 0-shot, PAWS-X 5-shot), Multi-Mathematics (MGSM 8-shot), Multi-Translation (Flores-101 5-shot) Qwen2-72B performance # Datasets DeepSeek-V2 Mixtral-8x22B Llama-3-70B Qwen1.5-72B Qwen1.5-110B Qwen2-72B Architecture MoE MoE Dense Dense Dense Dense #Activated Params 21B 39B 70B 72B 110B 72B #Params 236B 140B 70B 72B 110B 72B English MMLU 78.5 77.8 79.5 77.5 80.4 84.2 MMLU-Pro - 49.5 52.8 45.8 49.4 55.6 GPQA - 34.3 36.3 36.3 35.9 37.9 Theorem QA - 35.9 32.3 29.3 34.9 43.1 BBH 78.9 78.9 81.0 65.5 74.8 82.4 HellaSwag 87.8 88.7 88.0 86.0 87.5 87.6 WindoGrande 84.8 85.0 85.3 83.0 83.5 85.1 ARC-C 70.0 70.7 68.8 65.9 69.6 68.9 TruthfulQA 42.2 51.0 45.6 59.6 49.6 54.8 Coding HumanEval 45.7 46.3 48.2 46.3 54.3 64.6 MBPP 73.9 71.7 70.4 66.9 70.9 76.9 EvalPlus 55.0 54.1 54.8 52.9 57.7 65.4 MultiPL-E 44.4 46.7 46.3 41.8 52.7 59.6 Mathematics GSM8K 79.2 83.7 83.0 79.5 85.4 89.5 MATH 43.6 41.7 42.5 34.1 49.6 51.1 Chinese C-Eval 81.7 54.6 65.2 84.1 89.1 91.0 CMMLU 84.0 53.4 67.2 83.5 88.3 90.1 Multilingual Mulit-Exam 67.5 63.5 70.0 66.4 75.6 76.6 Multi-Understanding 77.0 77.7 79.9 78.2 78.2 80.7 Multi-Mathematics 58.8 62.9 67.1 61.7 64.4 76.0 Multi-Translation 36.0 23.3 38.0 35.6 36.2 37.8 Qwen2-57B-A14B # Datasets Jamba Mixtral-8x7B Yi-1.5-34B Qwen1.5-32B Qwen2-57B-A14B Architecture MoE MoE Dense Dense MoE #Activated Params 12B 12B 34B 32B 14B #Params 52B 47B 34B 32B 57B English MMLU 67.4 71.8 77.1 74.3 76.5 MMLU-Pro - 41.0 48.3 44.0 43.0 GPQA - 29.2 - 30.8 34.3 Theorem QA - 23.2 - 28.8 33.5 BBH 45.4 50.3 76.4 66.8 67.0 HellaSwag 87.1 86.5 85.9 85.0 85.2 Winogrande 82.5 81.9 84.9 81.5 79.5 ARC-C 64.4 66.0 65.6 63.6 64.1 TruthfulQA 46.4 51.1 53.9 57.4 57.7 Coding HumanEval 29.3 37.2 46.3 43.3 53.0 MBPP - 63.9 65.5 64.2 71.9 EvalPlus - 46.4 51.9 50.4 57.2 MultiPL-E - 39.0 39.5 38.5 49.8 Mathematics GSM8K 59.9 62.5 82.7 76.8 80.7 MATH - 30.8 41.7 36.1 43.0 Chinese C-Eval - - - 83.5 87.7 CMMLU - - 84.8 82.3 88.5 Multilingual Multi-Exam - 56.1 58.3 61.6 65.5 Multi-Understanding - 70.7 73.9 76.5 77.0 Multi-Mathematics - 45.0 49.3 56.1 62.3 Multi-Translation - 29.8 30.0 33.5 34.5 Qwen2-7B # Datasets Mistral-7B Gemma-7B Llama-3-8B Qwen1.5-7B Qwen2-7B # Params 7.2B 8.5B 8.0B 7.7B 7.6B # Non-emb Params 7.0B 7.8B 7.0B 6.5B 6.5B English MMLU 64.2 64.6 66.6 61.0 70.3 MMLU-Pro 30.9 33.7 35.4 29.9 40.0 GPQA 24.7 25.7 25.8 26.7 31.8 Theorem QA 19.2 21.5 22.1 14.2 31.1 BBH 56.1 55.1 57.7 40.2 62.6 HellaSwag 83.2 82.2 82.1 78.5 80.7 Winogrande 78.4 79.0 77.4 71.3 77.0 ARC-C 60.0 61.1 59.3 54.2 60.6 TruthfulQA 42.2 44.8 44.0 51.1 54.2 Coding HumanEval 29.3 37.2 33.5 36.0 51.2 MBPP 51.1 50.6 53.9 51.6 65.9 EvalPlus 36.4 39.6 40.3 40.0 54.2 MultiPL-E 29.4 29.7 22.6 28.1 46.3 Mathematics GSM8K 52.2 46.4 56.0 62.5 79.9 MATH 13.1 24.3 20.5 20.3 44.2 Chinese C-Eval 47.4 43.6 49.5 74.1 83.2 CMMLU - - 50.8 73.1 83.9 Multilingual Multi-Exam 47.1 42.7 52.3 47.7 59.2 Multi-Understanding 63.3 58.3 68.6 67.6 72.0 Multi-Mathematics 26.3 39.1 36.3 37.3 57.5 Multi-Translation 23.3 31.2 31.9 28.4 31.5 Qwen2-0.5B & Qwen2-1.5B # Datasets Phi-2 Gemma-2B MiniCPM Qwen1.5-1.8B Qwen2-0.5B Qwen2-1.5B #Non-Emb Params 2.5B 2.0B 2.4B 1.3B 0.35B 1.3B MMLU 52.7 42.3 53.5 46.8 45.4 56.5 MMLU-Pro - 15.9 - - 14.7 21.8 Theorem QA - - - - 8.9 15.0 HumanEval 47.6 22.0 50.0 20.1 22.0 31.1 MBPP 55.0 29.2 47.3 18.0 22.0 37.4 GSM8K 57.2 17.7 53.8 38.4 36.5 58.5 MATH 3.5 11.8 10.2 10.1 10.7 21.7 BBH 43.4 35.2 36.9 24.2 28.4 37.2 HellaSwag 73.1 71.4 68.3 61.4 49.3 66.6 Winogrande 74.4 66.8 - 60.3 56.8 66.2 ARC-C 61.1 48.5 - 37.9 31.5 43.9 TruthfulQA 44.5 33.1 - 39.4 39.7 45.9 C-Eval 23.4 28.0 51.1 59.7 58.2 70.6 CMMLU 24.2 - 51.1 57.8 55.1 70.3 Instruction-tuned Model Evaluation 1 # Qwen2-72B-Instruct # Datasets Llama-3-70B-Instruct Qwen1.5-72B-Chat Qwen2-72B-Instruct English MMLU 82.0 75.6 82.3 MMLU-Pro 56.2 51.7 64.4 GPQA 41.9 39.4 42.4 TheroemQA 42.5 28.8 44.4 MT-Bench 8.95 8.61 9.12 Arena-Hard 41.1 36.1 48.1 IFEval (Prompt Strict-Acc.) 77.3 55.8 77.6 Coding HumanEval 81.7 71.3 86.0 MBPP 82.3 71.9 80.2 MultiPL-E 63.4 48.1 69.2 EvalPlus 75.2 66.9 79.0 LiveCodeBench 29.3 17.9 35.7 Mathematics GSM8K 93.0 82.7 91.1 MATH 50.4 42.5 59.7 Chinese C-Eval 61.6 76.1 83.8 AlignBench 7.42 7.28 8.27 Qwen2-57B-A14B-Instruct # Datasets Mixtral-8x7B-Instruct-v0.1 Yi-1.5-34B-Chat Qwen1.5-32B-Chat Qwen2-57B-A14B-Instruct Architecture MoE Dense Dense MoE #Activated Params 12B 34B 32B 14B #Params 47B 34B 32B 57B English MMLU 71.4 76.8 74.8 75.4 MMLU-Pro 43.3 52.3 46.4 52.8 GPQA - - 30.8 34.3 TheroemQA - - 30.9 33.1 MT-Bench 8.30 8.50 8.30 8.55 Coding HumanEval 45.1 75.2 68.3 79.9 MBPP 59.5 74.6 67.9 70.9 MultiPL-E - - 50.7 66.4 EvalPlus 48.5 - 63.6 71.6 LiveCodeBench 12.3 - 15.2 25.5 Mathematics GSM8K 65.7 90.2 83.6 79.6 MATH 30.7 50.1 42.4 49.1 Chinese C-Eval - - 76.7 80.5 AlignBench 5.70 7.20 7.19 7.36 Qwen2-7B-Instruct # Datasets Llama-3-8B-Instruct Yi-1.5-9B-Chat GLM-4-9B-Chat Qwen1.5-7B-Chat Qwen2-7B-Instruct English MMLU 68.4 69.5 72.4 59.5 70.5 MMLU-Pro 41.0 - - 29.1 44.1 GPQA 34.2 - - 27.8 25.3 TheroemQA 23.0 - - 14.1 25.3 MT-Bench 8.05 8.20 8.35 7.60 8.41 Coding Humaneval 62.2 66.5 71.8 46.3 79.9 MBPP 67.9 - - 48.9 67.2 MultiPL-E 48.5 - - 27.2 59.1 Evalplus 60.9 - - 44.8 70.3 LiveCodeBench 17.3 - - 6.0 26.6 Mathematics GSM8K 79.6 84.8 79.6 60.3 82.3 MATH 30.0 47.7 50.6 23.2 49.6 Chinese C-Eval 45.9 - 75.6 67.3 77.2 AlignBench 6.20 6.90 7.01 6.20 7.21 Qwen2-0.5B-Instruct & Qwen2-1.5B-Instruct # Datasets Qwen1.5-0.5B-Chat Qwen2-0.5B-Instruct Qwen1.5-1.8B-Chat Qwen2-1.5B-Instruct MMLU 35.0 37.9 43.7 52.4 HumanEval 9.1 17.1 25.0 37.8 GSM8K 11.3 40.1 35.3 61.6 C-Eval 37.2 45.2 55.3 63.8 IFEval (Prompt Strict-Acc.) 14.6 20.0 16.8 29.0 Multilingual capability of instruction-tuned models # We compare Qwen2 instruction-tuned models with other recent LLMs on several cross-lingual open benchmarks as well as by human evaluation. For benchmarks, we show the results on 2 evaluation datasets: M-MMLU from Okapi: multilingual commonsense evaluation (we evaluate with a subset on ar, de, es, fr, it, nl, ru, uk, vi, zh) MGSM : math evaluation on languages including de, en, es, fr, ja, ru, th, zh and bn The results are averaged over languages for each benchmark and shown as follows: Models M-MMLU (5-shot) MGSM (0-shot, CoT) Proprietary LLMs GPT-4-0613 78.0 87.0 GPT-4-Turbo-0409 79.3 90.5 GPT-4o-0513 83.2 89.6 Claude-3-Opus-20240229 80.1 91.0 Claude-3-Sonnet-20240229 71.0 85.6 Open-source LLMs command-r-plus-110b 65.5 63.5 Qwen1.5-7B-Chat 50.0 37.0 Qwen1.5-32B-Chat 65.0 65.0 Qwen1.5-72B-Chat 68.4 71.7 Qwen2-7B-Instruct 60.0 57.0 Qwen2-57B-A14B-Instruct 68.0 74.0 Qwen2-72B-Instruct 78.0 86.6 For human evaluation, we compare Qwen2-72B-Instruct with GPT3.5, GPT4 and Claude-3-Opus using in-house evaluation set, which includes 10 languages ar, es, fr, ko, th, vi, pt, id, ja and ru (the scores range from 1~5): Models ar es fr ko th vi pt id ja ru Average Claude-3-Opus-20240229 4.15 4.31 4.23 4.23 4.01 3.98 4.09 4.40 3.85 4.25 4.15 GPT-4o-0513 3.55 4.26 4.16 4.40 4.09 4.14 3.89 4.39 3.72 4.32 4.09 GPT-4-Turbo-0409 3.44 4.08 4.19 4.24 4.11 3.84 3.86 4.09 3.68 4.27 3.98 Qwen2-72B-Instruct 3.86 4.10 4.01 4.14 3.75 3.91 3.97 3.83 3.63 4.15 3.93 GPT-4-0613 3.55 3.92 3.94 3.87 3.83 3.95 3.55 3.77 3.06 3.63 3.71 GPT-3.5-Turbo-1106 2.52 4.07 3.47 2.37 3.38 2.90 3.37 3.56 2.75 3.24 3.16 Grouped by task types, the results are shown as follows: Models Knowledge Understanding Creation Math Claude-3-Opus-20240229 3.64 4.45 4.42 3.81 GPT-4o-0513 3.76 4.35 4.45 3.53 GPT-4-Turbo-0409 3.42 4.29 4.35 3.58 Qwen2-72B-Instruct 3.41 4.07 4.36 3.61 GPT-4-0613 3.42 4.09 4.10 3.32 GPT-3.5-Turbo-1106 3.37 3.67 3.89 2.97 These results demonstrate the strong multilingual capabilities of Qwen2 instruction-tuned models. Update on 2024-07-16: The results of instruction-tuned models may differ from those presented in the technical report; in case of any discrepancy, the results documented in the technical report should take precedence.&#160; &#8617;&#xfe0e; &#160; &#8617;&#xfe0e; &#160; &#8617;&#xfe0e; &copy; 2025 Qwen Powered by Hugo ",
      "fetch_method": "direct-html"
    },
    {
      "arxiv_id": "https://qwenlm.github.io/blog/qwen-moe/",
      "full_text": " Qwen1.5-MoE: Matching 7B Model Performance with 1/3 Activated Parameters | Qwen Blog Publication About Try Qwen Chat &nbsp; Qwen1.5-MoE: Matching 7B Model Performance with 1/3 Activated Parameters March 28, 2024 &nbsp;·&nbsp;7 min&nbsp;·&nbsp;1411 words&nbsp;·&nbsp;Qwen Team&nbsp;|&nbsp;Translations: 简体中文 GITHUB HUGGING FACE MODELSCOPE DEMO DISCORD Introduction # Since the surge in interest sparked by Mixtral, research on mixture-of-expert (MoE) models has gained significant momentum. Both researchers and practitioners are keenly interested in understanding how to effectively train such models and assessing their efficiency and effectiveness. Today, we introduce Qwen1.5-MoE-A2.7B, a small MoE model with only 2.7 billion activated parameters yet matching the performance of state-of-the-art 7B models like Mistral 7B and Qwen1.5-7B. Compared to Qwen1.5-7B, which contains 6.5 billion non-embedding parameters, Qwen1.5-MoE-A2.7B contains only 2.0 billion non-embedding parameters, approximately one-third of Qwen1.5-7B&rsquo;s size. Notably, it achieves a 75% decrease in training expenses and accelerates inference speed by a factor of 1.74, offering substantial improvements in resource utilization without compromising performance. Architecture # We build the Qwen1.5-MoE models with a specially designed MoE architecture. Typically, as seen in methods like Mixtral, MoE layers within each transformer block employ eight experts and utilize a top-2 gating strategy for routing purposes. This configuration, while straightforward and efficacious, presents ample scope for enhancement. Consequently, through an extensive series of experiments, we have introduced several modifications to this architecture: Finegrained experts Initialization, which we call it &ldquo;upcycling&rdquo; Routing mechanism, with shared and routing experts Previous research projects such as DeepSeek-MoE and DBRX have demonstrated the effectiveness of using fine-grained experts. Conventionally, when transitioning from a standard FFN layer to a Mixture-of-Experts (MoE) layer, one merely replicates the FFN multiple times to create multiple experts. However, in the context of fine-grained experts, the goal is to generate a larger number of experts without increasing the parameter count. To accomplish this, we partition a single FFN into several segments, each serving as an individual expert. This is a more nuanced approach to constructing experts. We have identified an optimal configuration with a total of 64 experts, representing an 8-time increase compared to the conventional MoE setup of 8 experts. The initialization stage of the model is critical. Our initial experiments suggest that training a MoE model from scratch may prove inefficient and challenging to elevate it to the anticipated peak performance. Instead, we start by repurposing our existing Qwen-1.8B, transforming it into Qwen1.5-MoE-A2.7B. A noteworthy finding is that introducing randomness during initialization significantly expedites convergence and results in superior overall performance throughout the pre-training process. An essential aspect deserving attention is the routing methodology employed. Presently, there is a growing trend towards using shared and routing-specific experts within the MoE layer. To view it from a broader perspective, this is a generalized MoE routing approach, as having zero shared experts effectively reduces to the conventional MoE routing setup. In the case of Qwen1.5-MoE-A2.7B model, we have incorporated 4 shared experts to be always activated alongside 60 routing experts with 4 to be activated. This configuration offers a more adaptable method for constructing the MoE routing mechanism, providing greater flexibility and efficiency. Performance # In order to thoroughly assess and showcase the capabilities and superiority of our newly developed model, we have conducted extensive evaluations across various benchmark datasets for both the base and chat models. For the base model, we evaluated its performance on 3 benchmarks: MMLU, GSM8K, and HumanEval for evaluating language understanding, mathematics, and coding. Additionally, to gauge its multilingual proficiency, we followed the evaluation protocol of Qwen1.5 and tested it on several benchmarks that spanned diverse domains such as exams, understanding, math, and translation, presenting an aggregate score in the &ldquo;Multilingual&rdquo; column. For the chat model, rather than employing traditional benchmarks, we subjected it to testing using MT-Bench. In this comparative analysis, we juxtaposed Qwen1.5-MoE-A2.7B against top-performing 7B base models like Mistral-7B (v0.1 base and v0.2 instruct), Gemma-7B, and Qwen1.5-7B. Furthermore, we included a comparison with other MoE models of comparable parameter counts, notably DeepSeekMoE 16B. The results are summarized in the table below: Model MMLU GSM8K HumanEval Multilingual MT-Bench Mistral-7B 64.1 47.5 27.4 40.0 7.60 Gemma-7B 64.6 50.9 32.3 - - Qwen1.5-7B 61.0 62.5 36.0 45.2 7.60 DeepSeekMoE 16B 45.0 18.8 26.8 - 6.93 Qwen1.5-MoE-A2.7B 62.5 61.5 34.2 40.8 7.17 The Qwen1.5-MoE-A2.7B model has demonstrated competitive performance akin to the top 7B models in various evaluations. Despite this parity, our analysis reveals untapped potential for enhancement in the domain of chat models specifically. As such, we are committed to furthering our research efforts towards refining the effective finetuning strategies for MoE models. Costs and Efficiency # The training costs of MoE models deviates significantly from that of their dense counterparts. Despite a larger parameter count, MoE models&rsquo; training expenses can be notably reduced due to sparsity. To better understand this, let&rsquo;s first delve into three key components: total number of parameters, the count of active parameters, and non-embedding parameters and make a comparison between models: Model #Parameters #(Activated) Parameters #(Activated) Non-embedding parameters Mistral-7B 7.2 7.2 7.0 Qwen1.5-7B 7.7 7.7 6.4 Gemma-7B 8.5 7.8 7.8 DeepSeekMoE 16B 16.4 2.8 2.4 Qwen1.5-MoE-A2.7B 14.3 2.7 2.0 It is obvious that the count of non-embedding parameters of our MoE model is much smaller than those of 7B models. In our practical implementation, we have observed a remarkable reduction of 75% in training costs when using Qwen1.5-MoE-A2.7B in comparison to Qwen1.5-7B. Of particular significance is the fact that, through upcycling, the necessity for training an equivalent volume of tokens as in the original model has been eliminated. This constitutes a substantial enhancement in terms of economizing on training expenses. We have deployed both Qwen1.5-7B and Qwen1.5-MoE-A2.7B models with vLLM and conducted performance tests using a single NVIDIA A100-80G GPU. Under the experimental setup where the input token count was set at 1000 and the output tokens at 1000, we measured the performance in terms of throughput (requests processed per second) and tokens per second (TPS): Model Throughput TPS Qwen1.5-7B-Chat 1.15 2298.89 Qwen1.5-MoE-A2.7B-Chat 2.01 4010.27 The Qwen1.5-MoE-A2.7B model exhibits an impressive improvement in speed, being approximately 1.74 times faster compared to the Qwen1.5-7B model. This acceleration is primarily attributed to the fact that the MoE architecture activates a notably smaller portion of its total parameters, thereby reducing computational demands. Moreover, the integration of shared experts contributes substantially to enhancing the model&rsquo;s inference efficiency. Consequently, despite the increased memory requirements associated with MoE models, they demonstrate clear advantages in terms of both throughput and inference speed. Develop with Qwen1.5-MoE # To utilize the Qwen1.5-MoE model with the qwen2_moe implementation in Hugging Face&rsquo;s transformers, since the latest release does not include this feature yet, you will have to install transformers from source instead of installing it via pip or conda: git clone https://github.com/huggingface/transformers cd transformers pip install -e . The following step is indeed straightforward and akin to using models such as Qwen1.5, Mistral, or Llama. We demonstrate an example of the usage of Qwen1.5-MoE-A2.7B-Chat. To use the quantized model instead, you can just substitute the model name Qwen1.5-MoE-A2.7B-Chat with Qwen1.5-MoE-A2.7B-Chat-GPTQ-Int4 (temporarily AWQ is not supported). from transformers import AutoModelForCausalLM , AutoTokenizer model = AutoModelForCausalLM . from_pretrained ( &#34;Qwen/Qwen1.5-MoE-A2.7B-Chat-GPTQ-Int4&#34; , device_map = &#34;auto&#34; ) tokenizer = AutoTokenizer . from_pretrained ( &#34;Qwen/Qwen1.5-MoE-A2.7B-Chat&#34; ) prompt = &#34;Give me a short introduction to large language model.&#34; messages = [ { &#34;role&#34; : &#34;system&#34; , &#34;content&#34; : &#34;You are a helpful assistant.&#34; }, { &#34;role&#34; : &#34;user&#34; , &#34;content&#34; : prompt } ] text = tokenizer . apply_chat_template ( messages , tokenize = False , add_generation_prompt = True ) model_inputs = tokenizer ([ text ], return_tensors = &#34;pt&#34; ) . to ( model . device ) generated_ids = model . generate ( model_inputs . input_ids , max_new_tokens = 512 ) generated_ids = [ output_ids [ len ( input_ids ):] for input_ids , output_ids in zip ( model_inputs . input_ids , generated_ids ) ] response = tokenizer . batch_decode ( generated_ids , skip_special_tokens = True )[ 0 ] To use the model with vLLM, follow our fork first and then install vLLM from source as well: git clone https://github.com/vllm-project/vllm.git cd vllm pip install -e . Here we demonstrate an example to show how to use vLLM to build an OpenAI-API compatible interface for our model: python -m vllm.entrypoints.openai.api_server --model Qwen/Qwen1.5-MoE-A2.7B-Chat curl http://localhost:8000/v1/chat/completions \\ -H &#34;Content-Type: application/json&#34; \\ -d &#39;{ &#34;model&#34;: &#34;Qwen/Qwen1.5-MoE-A2.7B-Chat&#34;, &#34;messages&#34;: [ {&#34;role&#34;: &#34;system&#34;, &#34;content&#34;: &#34;You are a helpful assistant.&#34;}, {&#34;role&#34;: &#34;user&#34;, &#34;content&#34;: &#34;Tell me something about large language models.&#34;} ] }&#39; There remains an extensive array of tasks on our agenda, including the support of llama.cpp for GGUF files, MLX support, etc. We will continue updating the support of third-party frameworks. Conclusion # We are thrilled to introduce our pioneering MoE model, Qwen1.5-MoE-A2.7B, which achieves parity with contemporary 7B parameter models. Furthermore, we have shown substantial reductions in both training costs and inference time when compared to conventional 7B models. Our model developments underscore the vast potential of MoE models. In light of these encouraging outcomes, we remain steadfast in our commitment to advancing this technology further. Citation # @misc{qwen_moe, title = {Qwen1.5-MoE: Matching 7B Model Performance with 1/3 Activated Parameters&#34;}, url = {https://qwenlm.github.io/blog/qwen-moe/}, author = {Qwen Team}, month = {February}, year = {2024} } &copy; 2025 Qwen Powered by Hugo ",
      "fetch_method": "direct-html"
    },
    {
      "arxiv_id": "https://qwen.readthedocs.io/en/latest/getting_started/speed_benchmark.html",
      "full_text": " Speed Benchmark - Qwen Contents Menu Expand Light mode Dark mode Auto light/dark, in light mode Auto light/dark, in dark mode Hide navigation sidebar Hide table of contents sidebar Skip to content Toggle site navigation sidebar Qwen Toggle Light / Dark / Auto color theme Toggle table of contents sidebar Qwen Getting Started Quickstart Key Concepts Speed Benchmark Performance of Quantized Models Inference Transformers Run Locally llama.cpp Ollama MLX LM Deployment SGLang vLLM TGI dstack SkyPilot OpenLLM Quantization AWQ GPTQ llama.cpp Training Axolotl LLaMA-Factory MS-SWIFT Unsloth verl Framework Qwen-Agent Function Calling LlamaIndex Langchain Back to top View this page Toggle Light / Dark / Auto color theme Toggle table of contents sidebar Speed Benchmark ¶ We report the speed performance of bfloat16 models and quantized models (including FP8, GPTQ, AWQ) of the Qwen3 series. Specifically, we report the inference speed (tokens/s) as well as memory footprint (GB) under different context lengths. Environments ¶ Hugging Face Transformers ¶ Hardware : NVIDIA H20 96GB Software for Non-AutoAWQ : PyTorch 2.6.0 Flash Attention 2.7.4 Transformers 4.51.3 GPTQModel 2.2.0+cu128torch2.6 Software for AutoAWQ : PyTorch 2.6.0+cu124 Transformers 4.51.3 AutoAWQ 0.2.9 AutoAWQ_kernels 0.0.9 SGLang ¶ Hardware : NVIDIA H20 96GB Software : PyTorch 2.6.0+cu124 Transformers 4.51.3 SGLang 0.4.6.post1 SGL-kernel 0.1.0 vLLM 0.7.2 (Required by SGLang for AWQ quantization) Notes ¶ Inference Speed (tokens/s) is calculated as: \\[\\text{Speed} = \\frac{\\text{tokens}_{\\text{prompt}} + \\text{tokens}_{\\text{generation}}}{\\text{time}}\\] We use a batch size of 1 and the minimum number of GPUs possible for evaluation. We test the speed and memory usage when generating 2048 tokens , with input lengths of 1 , 6144 , 14336 , 30720 , 63488 , and 129024 tokens. For SGLang : Memory usage is not reported because SGLang pre-allocates all GPU memory. By default, we set mem_fraction_static=0.85 . We configure context_length=140000 and enable enable_mixed_chunk=True . For AWQ quantization , we use the awq_marlin backend. We set skip_tokenizer_init=True and perform generation using input_ids instead of raw text prompts. FP8 Performance in Transformers : The inference speed of Transformers in FP8 mode is currently not optimal and requires further optimization. GPTQ-INT4 Performance in SGLang : The performance of GPTQ-INT4 in SGLang also needs improvement, and we are actively working with the team to enhance it. Results ¶ Qwen3-0.6B (SGLang) ¶ Model Input Length Quantization GPU Num Speed (tokens/s) Note Qwen3-0.6B 1 BF16 1 414.17 FP8 1 458.03 GPTQ-Int8 1 344.92 6144 BF16 1 1426.46 FP8 1 1572.95 GPTQ-Int8 1 1234.29 14336 BF16 1 2478.02 FP8 1 2689.08 GPTQ-Int8 1 2198.82 30720 BF16 1 3577.42 FP8 1 3819.86 GPTQ-Int8 1 3342.06 Qwen3-0.6B (Transformers) ¶ Model Input Length Quantization GPU Num Speed (tokens/s) GPU Memory(MB) Qwen3-0.6B 1 BF16 1 58.57 1394 FP8 1 24.60 1217 GPTQ-Int8 1 26.56 986 6144 BF16 1 154.82 2066 FP8 1 73.96 1943 GPTQ-Int8 1 93.84 1658 14336 BF16 1 168.48 2963 FP8 1 104.99 2839 GPTQ-Int8 1 219.61 2554 30720 BF16 1 175.93 4755 FP8 1 132.78 4632 GPTQ-Int8 1 345.71 4347 Qwen3-1.7B (SGLang) ¶ Model Input Length Quantization GPU Num Speed (tokens/s) Note Qwen3-1.7B 1 BF16 1 227.80 FP8 1 333.90 GPTQ-Int8 1 257.40 6144 BF16 1 838.28 FP8 1 1198.20 GPTQ-Int8 1 945.91 14336 BF16 1 1525.71 FP8 1 2095.61 GPTQ-Int8 1 1707.63 30720 BF16 1 2439.03 FP8 1 3165.32 GPTQ-Int8 1 2706.16 Qwen3-1.7B (Transformers) ¶ Model Input Length Quantization GPU Num Speed (tokens/s) GPU Memory(MB) Qwen3-1.7B 1 BF16 1 59.83 3412 FP8 1 23.83 2726 GPTQ-Int8 1 28.06 2229 6144 BF16 1 238.53 4213 FP8 1 90.87 3462 GPTQ-Int8 1 110.82 2901 14336 BF16 1 352.59 5109 FP8 1 153.37 4359 GPTQ-Int8 1 222.78 3798 30720 BF16 1 418.13 6902 FP8 1 235.61 6151 GPTQ-Int8 1 386.85 5590 Qwen3-4B (SGLang) ¶ Model Input Length Quantization GPU Num Speed (tokens/s) Note Qwen3-4B 1 BF16 1 133.13 FP8 1 200.61 AWQ-INT4 1 199.71 6144 BF16 1 466.19 FP8 1 662.26 AWQ-INT4 1 640.07 14336 BF16 1 789.25 FP8 1 1066.23 AWQ-INT4 1 1006.23 30720 BF16 1 1165.75 FP8 1 1467.71 AWQ-INT4 1 1358.84 63488 BF16 1 1423.98 FP8 1 1660.67 AWQ-INT4 1 1513.97 129042 BF16 1 1371.04 FP8 1 1497.27 AWQ-INT4 1 1375.71 Qwen3-4B (Transformers) ¶ Model Input Length Quantization GPU Num Speed (tokens/s) GPU Memory(MB) Qwen3-4B 1 BF16 1 45.94 7973 FP8 1 17.33 5281 AWQ-INT4 1 51.57 2915 6144 BF16 1 159.95 8860 FP8 1 60.55 6144 AWQ-INT4 1 183.04 3881 14336 BF16 1 195.31 10012 FP8 1 96.81 7297 AWQ-INT4 1 265.22 5151 30720 BF16 1 217.97 12317 FP8 1 138.84 9611 AWQ-INT4 1 481.69 7742 Qwen3-8B (SGLang) ¶ Model Input Length Quantization GPU Num Speed (tokens/s) Note Qwen3-8B 1 BF16 1 81.73 FP8 1 150.25 AWQ-INT4 1 144.11 6144 BF16 1 296.25 FP8 1 516.64 AWQ-INT4 1 477.89 14336 BF16 1 524.70 FP8 1 859.92 AWQ-INT4 1 770.44 30720 BF16 1 832.67 FP8 1 1242.24 AWQ-INT4 1 1075.91 63488 BF16 1 1112.78 FP8 1 1476.46 AWQ-INT4 1 1254.91 129042 BF16 1 1173.32 FP8 1 1393.21 AWQ-INT4 1 1198.06 Qwen3-8B (Transformers) ¶ Model Input Length Quantization GPU Num Speed (tokens/s) GPU Memory(MB) Qwen3-8B 1 BF16 1 45.32 15947 FP8 1 15.46 9323 AWQ-INT4 1 51.33 6177 6144 BF16 1 146.12 16811 FP8 1 55.07 10187 AWQ-INT4 1 163.23 7113 14336 BF16 1 183.29 17963 FP8 1 89.64 11340 AWQ-INT4 1 242.97 8409 30720 BF16 1 208.98 20267 FP8 1 130.93 13644 AWQ-INT4 1 438.62 11001 Qwen3-14B (SGLang) ¶ Model Input Length Quantization GPU Num Speed (tokens/s) Note Qwen3-14B 1 BF16 1 47.10 FP8 1 97.11 AWQ-INT4 1 96.49 6144 BF16 1 174.85 FP8 1 342.95 AWQ-INT4 1 321.62 14336 BF16 1 317.56 FP8 1 587.33 AWQ-INT4 1 525.74 30720 BF16 1 525.80 FP8 1 880.72 AWQ-INT4 1 744.74 63488 BF16 1 742.36 FP8 1 1089.04 AWQ-INT4 1 884.06 129042 BF16 1 826.15 FP8 1 1049.64 AWQ-INT4 1 857.56 Qwen3-14B (Transformers) ¶ Model Input Length Quantization GPU Num Speed (tokens/s) GPU Memory (MB) Qwen3-14B 1 BF16 1 40.66 28402 FP8 1 13.02 16012 AWQ-INT4 1 44.67 9962 6144 BF16 1 108.52 29495 FP8 1 44.86 16972 AWQ-INT4 1 128.08 11020 14336 BF16 1 136.36 30775 FP8 1 71.96 18253 AWQ-INT4 1 220.62 12438 30720 BF16 1 155.38 33336 FP8 1 102.63 20813 AWQ-INT4 1 363.25 15323 Qwen3-32B (SGLang) ¶ Model Input Length Quantization GPU Num Speed (tokens/s) Note Qwen3-32B 1 BF16 1 20.72 FP8 1 46.17 AWQ-INT4 1 47.67 6144 BF16 1 77.82 FP8 1 165.71 AWQ-INT4 1 159.99 14336 BF16 1 143.08 FP8 1 287.60 AWQ-INT4 1 260.44 30720 BF16 1 240.75 FP8 1 436.59 AWQ-INT4 1 366.84 63488 BF16 1 342.96 FP8 1 532.18 AWQ-INT4 1 425.23 129042 BF16 2 711.40 TP=2 FP8 1 491.45 AWQ-INT4 1 395.96 Qwen3-32B (Transformers) ¶ Model Input Length Quantization GPU Num Speed (tokens/s) GPU Memory (MB) Qwen3-32B 1 BF16 1 26.24 62751 FP8 1 7.37 33379 AWQ-INT4 1 41.8 19109 6144 BF16 1 51.41 64583 FP8 1 23.57 34915 AWQ-INT4 1 68.71 20795 14336 BF16 1 62.41 66632 FP8 1 36.30 36963 AWQ-INT4 1 107.02 23105 30720 BF16 1 69.16 70728 FP8 1 49.44 41060 AWQ-INT4 1 188.11 27718 Qwen3-30B-A3B (SGLang) ¶ Model Input Length Quantization GPU Num Speed (tokens/s) Note Qwen3-30B-A3B 1 BF16 1 137.18 FP8 1 155.55 GPTQ-INT4 1 31.29 GPTQ-Marlin 6144 BF16 1 490.10 FP8 1 551.34 GPTQ-INT4 1 120.13 GPTQ-Marlin 14336 BF16 1 849.62 FP8 1 945.13 GPTQ-INT4 1 227.27 GPTQ-Marlin 30720 BF16 1 1283.94 FP8 1 1405.91 GPTQ-INT4 1 404.45 GPTQ-Marlin 63488 BF16 1 1538.79 FP8 1 1647.89 GPTQ-INT4 1 617.09 GPTQ-Marlin 129042 BF16 1 1385.65 FP8 1 1442.14 GPTQ-INT4 1 704.82 GPTQ-Marlin Qwen3-30B-A3B (Transformers) ¶ Model Input length Quantization GPU Num Speed (tokens/s) GPU Memory (MB) Notes Qwen3-30B-A3B 1 BF16 1 1.89 58462 FP8 1 0.44 30296 GPTQ-INT4 - - - MoE Kernel Unsupported 6144 BF16 1 7.45 59037 FP8 1 1.77 30872 GPTQ-INT4 - - - MoE Kernel Unsupported 14336 BF16 1 14.47 59806 FP8 1 3.5 31641 GPTQ-INT4 - - - MoE Kernel Unsupported 30720 BF16 1 27.03 61342 FP8 1 6.86 33177 GPTQ-INT4 - - - MoE Kernel Unsupported Qwen3-235B-A22B (SGLang) ¶ Model Input Length Quantization GPU Num Speed (tokens/s) Note Qwen3-235B-A22B 1 BF16 8 74.50 TP=8 FP8 4 71.65 TP=4 GPTQ-INT4 4 14.69 TP=4 GPTQ-Marlin 6144 BF16 8 289.03 TP=8 FP8 4 275.16 TP=4 GPTQ-INT4 4 56.97 TP=4 GPTQ-Marlin 14336 BF16 8 546.73 TP=8 FP8 4 514.23 TP=4 GPTQ-INT4 4 109.13 TP=4 GPTQ-Marlin 30720 BF16 8 979.41 TP=8 FP8 4 887.90 TP=4 GPTQ-INT4 4 198.99 TP=4 GPTQ-Marlin 63488 BF16 8 1493.91 TP=8 FP8 4 1269.34 TP=4 GPTQ-INT4 4 422.77 TP=4 GPTQ-Marlin 129042 BF16 8 1639.54 TP=8 FP8 4 1319.66 TP=4 GPTQ-INT4 4 552.28 TP=4 GPTQ-Marlin Next Performance of Quantized Models Previous Key Concepts Copyright &#169; 2024, Qwen Team Made with Sphinx and @pradyunsg 's Furo On this page Speed Benchmark Environments Hugging Face Transformers SGLang Notes Results Qwen3-0.6B (SGLang) Qwen3-0.6B (Transformers) Qwen3-1.7B (SGLang) Qwen3-1.7B (Transformers) Qwen3-4B (SGLang) Qwen3-4B (Transformers) Qwen3-8B (SGLang) Qwen3-8B (Transformers) Qwen3-14B (SGLang) Qwen3-14B (Transformers) Qwen3-32B (SGLang) Qwen3-32B (Transformers) Qwen3-30B-A3B (SGLang) Qwen3-30B-A3B (Transformers) Qwen3-235B-A22B (SGLang) ",
      "fetch_method": "direct-html"
    },
    {
      "arxiv_id": "https://qwen.readthedocs.io/en/latest/run_locally/llama.cpp.html",
      "full_text": " llama.cpp - Qwen Contents Menu Expand Light mode Dark mode Auto light/dark, in light mode Auto light/dark, in dark mode Hide navigation sidebar Hide table of contents sidebar Skip to content Toggle site navigation sidebar Qwen Toggle Light / Dark / Auto color theme Toggle table of contents sidebar Qwen Getting Started Quickstart Key Concepts Speed Benchmark Performance of Quantized Models Inference Transformers Run Locally llama.cpp Ollama MLX LM Deployment SGLang vLLM TGI dstack SkyPilot OpenLLM Quantization AWQ GPTQ llama.cpp Training Axolotl LLaMA-Factory MS-SWIFT Unsloth verl Framework Qwen-Agent Function Calling LlamaIndex Langchain Back to top View this page Toggle Light / Dark / Auto color theme Toggle table of contents sidebar llama.cpp ¶ llama.cpp as a C++ library Before starting, let’s first discuss what is llama.cpp and what you should expect, and why we say “use” llama.cpp, with “use” in quotes. llama.cpp is essentially a different ecosystem with a different design philosophy that targets light-weight footprint, minimal external dependency, multi-platform, and extensive, flexible hardware support: Plain C/C++ implementation without external dependencies Support a wide variety of hardware: AVX, AVX2 and AVX512 support for x86_64 CPU Apple Silicon via Metal and Accelerate (CPU and GPU) NVIDIA GPU (via CUDA), AMD GPU (via hipBLAS), Intel GPU (via SYCL), Ascend NPU (via CANN), and Moore Threads GPU (via MUSA) Vulkan backend for GPU Various quantization schemes for faster inference and reduced memory footprint CPU+GPU hybrid inference to partially accelerate models larger than the total VRAM capacity It’s like the Python frameworks torch + transformers or torch + vllm but in C++. However, this difference is crucial: Python is an interpreted language: The code you write is executed line-by-line on-the-fly by an interpreter. You can run the example code snippet or script with an interpreter or a natively interactive interpreter shell. In addition, Python is learner friendly, and even if you don’t know much before, you can tweak the source code here and there. C++ is a compiled language: The source code you write needs to be compiled beforehand, and it is translated to machine code and an executable program by a compiler. The overhead from the language side is minimal. You do have source code for example programs showcasing how to use the library. But it is not very easy to modify the source code if you are not verse in C++ or C. To use llama.cpp means that you use the llama.cpp library in your own program, like writing the source code of Ollama , LM Studio , GPT4ALL , llamafile etc. But that’s not what this guide is intended or could do. Instead, here we introduce how to use the llama-cli example program, in the hope that you know that llama.cpp does support Qwen2.5 models and how the ecosystem of llama.cpp generally works. In this guide, we will show how to “use” llama.cpp to run models on your local machine, in particular, the llama-cli and the llama-server example program, which comes with the library. The main steps are: Get the programs Get the Qwen3 models in GGUF [ 1 ] format Run the program with the model Note llama.cpp supports Qwen3 and Qwen3MoE from version b5092 . Getting the Program ¶ You can get the programs in various ways. For optimal efficiency, we recommend compiling the programs locally, so you get the CPU optimizations for free. However, if you don’t have C++ compilers locally, you can also install using package managers or downloading pre-built binaries. They could be less efficient but for non-production example use, they are fine. Compile Locally Here, we show the basic command to compile llama-cli locally on macOS or Linux . For Windows or GPU users, please refer to the guide from llama.cpp . Installing Build Tools To build locally, a C++ compiler and a build system tool are required. To see if they have been installed already, type cc --version or cmake --version in a terminal window. If installed, the build configuration of the tool will be printed to the terminal, and you are good to go! If errors are raised, you need to first install the related tools: On macOS, install with the command xcode-select --install On Ubuntu, install with the command sudo apt install build-essential . For other Linux distributions, the command may vary; the essential packages needed for this guide are gcc and cmake . Compiling the Program For the first step, clone the repo and enter the directory: git clone https://github.com/ggml-org/llama.cpp cd llama.cpp Then, build llama.cpp using CMake: cmake -B build cmake --build build --config Release The first command will check the local environment and determine which backends and features should be included. The second command will actually build the programs. To shorten the time, you can also enable parallel compiling based on the CPU cores you have, for example: cmake --build build --config Release -j 8 This will build the programs with 8 parallel compiling jobs. The built programs will be in ./build/bin/ . Package Managers For macOS and Linux users, llama-cli and llama-server can be installed with package managers including Homebrew, Nix, and Flox. Here, we show how to install llama-cli and llama-server with Homebrew. For other package managers, please check the instructions here . Installing with Homebrew is very simple: Ensure that Homebrew is available on your operating system. If you don’t have Homebrew, you can install it as in its website . Second, you can install the pre-built binaries, llama-cli and llama-server included, with a single command: brew install llama.cpp Note that the installed binaries might not be built with the optimal compile options for your hardware, which can lead to poor performance. They also don’t support GPU on Linux systems. Binary Release You can also download pre-built binaries from GitHub Releases . Please note that those pre-built binaries files are architecture-, backend-, and os-specific. If you are not sure what those mean, you probably don’t want to use them and running with incompatible versions will most likely fail or lead to poor performance. The file name is like llama-&lt;version&gt;-bin-&lt;os&gt;-&lt;feature&gt;-&lt;arch&gt;.zip . There are three simple parts: &lt;version&gt; : the version of llama.cpp. The latest is preferred, but as llama.cpp is updated and released frequently, the latest may contain bugs. If the latest version does not work, try the previous release until it works. &lt;os&gt; : the operating system. win for Windows; macos for macOS; linux for Linux. &lt;arch&gt; : the system architecture. x64 for x86_64 , e.g., most Intel and AMD systems, including Intel Mac; arm64 for arm64 , e.g., Apple Silicon or Snapdragon-based systems. The &lt;feature&gt; part is somewhat complicated for Windows: Running on CPU x86_64 CPUs: We suggest try the avx2 one first. noavx : No hardware acceleration at all. avx2 , avx , avx512 : SIMD-based acceleration. Most modern desktop CPUs should support avx2, and some CPUs support avx512 . openblas : Relying on OpenBLAS for acceleration for prompt processing but not generation. arm64 CPUs: We suggest try the llvm one first. llvm and msvc are different compilers Running on GPU: We suggest try the cu&lt;cuda_verison&gt; one for NVIDIA GPUs, kompute for AMD GPUs, and sycl for Intel GPUs first. Ensure that you have related drivers installed. vulcan : support certain NVIDIA and AMD GPUs kompute : support certain NVIDIA and AMD GPUs sycl : Intel GPUs, oneAPI runtime is included cu&lt;cuda_verison&gt; : NVIDIA GPUs, CUDA runtime is not included. You can download the cudart-llama-bin-win-cu&lt;cuda_version&gt;-x64.zip and unzip it to the same directory if you don’t have the corresponding CUDA toolkit installed. You don’t have much choice for macOS or Linux. Linux: only one prebuilt binary, llama-&lt;version&gt;-bin-linux-x64.zip , supporting CPU. macOS: llama-&lt;version&gt;-bin-macos-x64.zip for Intel Mac with no GPU support; llama-&lt;version&gt;-bin-macos-arm64.zip for Apple Silicon with GPU support. After downloading the .zip file, unzip them into a directory and open a terminal at that directory. Getting the GGUF ¶ GGUF [ 1 ] is a file format for storing information needed to run a model, including but not limited to model weights, model hyperparameters, default generation configuration, and tokenizer. You can use the official Qwen GGUFs from our Hugging Face Hub or prepare your own GGUF file. Using the Official Qwen3 GGUFs ¶ We provide a series of GGUF models in our Hugging Face organization, and to search for what you need you can search the repo names with -GGUF . Download the GGUF model that you want with huggingface-cli (you need to install it first with pip install huggingface_hub ): huggingface-cli download &lt;model_repo&gt; &lt;gguf_file&gt; --local-dir &lt;local_dir&gt; For example: huggingface-cli download Qwen/Qwen3-8B-GGUF qwen3-8b-q4_k_m.gguf --local-dir . This will download the Qwen3-8B model in GGUF format quantized with the scheme Q4_K_M. Preparing Your Own GGUF ¶ Model files from Hugging Face Hub can be converted to GGUF, using the convert-hf-to-gguf.py Python script. It does require you to have a working Python environment with at least transformers installed. Obtain the source file if you haven’t already: git clone https://github.com/ggml-org/llama.cpp cd llama.cpp Suppose you would like to use Qwen3-8B you can make a GGUF file for the fp16 model as shown below: python convert-hf-to-gguf.py Qwen/Qwen3-8B --outfile qwen3-8b-f16.gguf The first argument to the script refers to the path to the HF model directory or the HF model name, and the second argument refers to the path of your output GGUF file. Remember to create the output directory before you run the command. The fp16 model could be a bit heavy for running locally, and you can quantize the model as needed. We introduce the method of creating and quantizing GGUF files in this guide . You can refer to that document for more information. Run Qwen with llama.cpp ¶ Note Regarding switching between thinking and non-thinking modes, while the soft switch is always available, the hard switch implemented in the chat template is not exposed in llama.cpp. The quick workaround is to pass a custom chat template equivalent to always enable_thinking=False via --chat-template-file . llama-cli ¶ llama-cli is a console program which can be used to chat with LLMs. Simple run the following command where you place the llama.cpp programs: ./llama-cli -hf Qwen/Qwen3-8B-GGUF:Q8_0 --jinja --color -ngl 99 -fa -sm row --temp 0 .6 --top-k 20 --top-p 0 .95 --min-p 0 -c 40960 -n 32768 --no-context-shift Here are some explanations to the above command: Model : llama-cli supports using model files from local path, remote URL, or Hugging Face hub. -hf Qwen/Qwen3-8B-GGUF:Q8_0 in the above indicates we are using the model file from Hugging Face hub To use a local path, pass -m qwen3-8b-q8_0.gguf instead To use a remote URL, pass -mu https://hf.co/Qwen/Qwen3-8B-GGUF/resolve/main/qwen3-8b-Q8_0.gguf?download=true instead Speed Optimization : CPU: llama-cli by default will use CPU and you can change -t to specify how many threads you would like it to use, e.g., -t 8 means using 8 threads. GPU: If the programs are built with GPU support, you can use -ngl , which allows offloading some layers to the GPU for computation. If there are multiple GPUs, it will offload to all the GPUs. You can use -dev to control the devices used and -sm to control which kinds of parallelism is used. For example, -ngl 99 -dev cuda0,cuda1 -sm row means offload all layers to GPU 0 and GPU1 using the split mode row. Adding -fa may also speed up the generation. Sampling Parameters : llama.cpp supports a variety of sampling methods and has default configuration for many of them. It is recommended to adjust those parameters according to the actual case and the recommended parameters from Qwen3 modelcard could be used as a reference. If you encounter repetition and endless generation, it is recommended to pass in addition --presence-penalty up to 2.0 . Context Management : llama.cpp adopts the “rotating” context management by default. The -c controls the maximum context length (default 4096, 0 means loaded from model), and -n controls the maximum generation length each time (default -1 means infinite until ending, -2 means until context full). When the context is full but the generation doesn’t end, the first --keep tokens (default 0, -1 means all) from the initial prompt is kept, and the first half of the rest is discarded. Then, the model continues to generate based on the new context tokens. You can set --no-context-shift to prevent this rotating behavior and the generation will stop once -c is reached. llama.cpp supports YaRN, which can be enabled by -c 131072 --rope-scaling yarn --rope-scale 4 --yarn-orig-ctx 32768 . Chat : --jinja indicates using the chat template embedded in the GGUF which is preferred and --color indicates coloring the texts so that user input and model output can be better differentiated. If there is a chat template, like in Qwen3 models, llama-cli will enter chat mode automatically. To stop generation or exit press “Ctrl+C”. You can use -sys to add a system prompt. llama-server ¶ llama-server is a simple HTTP server, including a set of LLM REST APIs and a simple web front end to interact with LLMs using llama.cpp. The core command is similar to that of llama-cli. In addition, it supports thinking content parsing and tool call parsing. ./llama-server -hf Qwen/Qwen3-8B-GGUF:Q8_0 --jinja --reasoning-format deepseek -ngl 99 -fa -sm row --temp 0 .6 --top-k 20 --top-p 0 .95 --min-p 0 -c 40960 -n 32768 --no-context-shift By default, the server will listen at http://localhost:8080 which can be changed by passing --host and --port . The web front end can be assessed from a browser at http://localhost:8080/ . The OpenAI compatible API is at http://localhost:8080/v1/ . What’s More ¶ If you still find it difficult to use llama.cpp, don’t worry, just check out other llama.cpp-based applications. For example, Qwen3 has already been officially part of Ollama and LM Studio, which are platforms for your to search and run local LLMs. Have fun! [ 1 ] ( 1 , 2 ) GPT-Generated Unified Format Next Ollama Previous Transformers Copyright &#169; 2024, Qwen Team Made with Sphinx and @pradyunsg 's Furo On this page llama.cpp Getting the Program Getting the GGUF Using the Official Qwen3 GGUFs Preparing Your Own GGUF Run Qwen with llama.cpp llama-cli llama-server What’s More ",
      "fetch_method": "direct-html"
    },
    {
      "arxiv_id": "https://qwen.readthedocs.io/en/latest/run_locally/llama.cpp.html#llama-cli",
      "full_text": " llama.cpp - Qwen Contents Menu Expand Light mode Dark mode Auto light/dark, in light mode Auto light/dark, in dark mode Hide navigation sidebar Hide table of contents sidebar Skip to content Toggle site navigation sidebar Qwen Toggle Light / Dark / Auto color theme Toggle table of contents sidebar Qwen Getting Started Quickstart Key Concepts Speed Benchmark Performance of Quantized Models Inference Transformers Run Locally llama.cpp Ollama MLX LM Deployment SGLang vLLM TGI dstack SkyPilot OpenLLM Quantization AWQ GPTQ llama.cpp Training Axolotl LLaMA-Factory MS-SWIFT Unsloth verl Framework Qwen-Agent Function Calling LlamaIndex Langchain Back to top View this page Toggle Light / Dark / Auto color theme Toggle table of contents sidebar llama.cpp ¶ llama.cpp as a C++ library Before starting, let’s first discuss what is llama.cpp and what you should expect, and why we say “use” llama.cpp, with “use” in quotes. llama.cpp is essentially a different ecosystem with a different design philosophy that targets light-weight footprint, minimal external dependency, multi-platform, and extensive, flexible hardware support: Plain C/C++ implementation without external dependencies Support a wide variety of hardware: AVX, AVX2 and AVX512 support for x86_64 CPU Apple Silicon via Metal and Accelerate (CPU and GPU) NVIDIA GPU (via CUDA), AMD GPU (via hipBLAS), Intel GPU (via SYCL), Ascend NPU (via CANN), and Moore Threads GPU (via MUSA) Vulkan backend for GPU Various quantization schemes for faster inference and reduced memory footprint CPU+GPU hybrid inference to partially accelerate models larger than the total VRAM capacity It’s like the Python frameworks torch + transformers or torch + vllm but in C++. However, this difference is crucial: Python is an interpreted language: The code you write is executed line-by-line on-the-fly by an interpreter. You can run the example code snippet or script with an interpreter or a natively interactive interpreter shell. In addition, Python is learner friendly, and even if you don’t know much before, you can tweak the source code here and there. C++ is a compiled language: The source code you write needs to be compiled beforehand, and it is translated to machine code and an executable program by a compiler. The overhead from the language side is minimal. You do have source code for example programs showcasing how to use the library. But it is not very easy to modify the source code if you are not verse in C++ or C. To use llama.cpp means that you use the llama.cpp library in your own program, like writing the source code of Ollama , LM Studio , GPT4ALL , llamafile etc. But that’s not what this guide is intended or could do. Instead, here we introduce how to use the llama-cli example program, in the hope that you know that llama.cpp does support Qwen2.5 models and how the ecosystem of llama.cpp generally works. In this guide, we will show how to “use” llama.cpp to run models on your local machine, in particular, the llama-cli and the llama-server example program, which comes with the library. The main steps are: Get the programs Get the Qwen3 models in GGUF [ 1 ] format Run the program with the model Note llama.cpp supports Qwen3 and Qwen3MoE from version b5092 . Getting the Program ¶ You can get the programs in various ways. For optimal efficiency, we recommend compiling the programs locally, so you get the CPU optimizations for free. However, if you don’t have C++ compilers locally, you can also install using package managers or downloading pre-built binaries. They could be less efficient but for non-production example use, they are fine. Compile Locally Here, we show the basic command to compile llama-cli locally on macOS or Linux . For Windows or GPU users, please refer to the guide from llama.cpp . Installing Build Tools To build locally, a C++ compiler and a build system tool are required. To see if they have been installed already, type cc --version or cmake --version in a terminal window. If installed, the build configuration of the tool will be printed to the terminal, and you are good to go! If errors are raised, you need to first install the related tools: On macOS, install with the command xcode-select --install On Ubuntu, install with the command sudo apt install build-essential . For other Linux distributions, the command may vary; the essential packages needed for this guide are gcc and cmake . Compiling the Program For the first step, clone the repo and enter the directory: git clone https://github.com/ggml-org/llama.cpp cd llama.cpp Then, build llama.cpp using CMake: cmake -B build cmake --build build --config Release The first command will check the local environment and determine which backends and features should be included. The second command will actually build the programs. To shorten the time, you can also enable parallel compiling based on the CPU cores you have, for example: cmake --build build --config Release -j 8 This will build the programs with 8 parallel compiling jobs. The built programs will be in ./build/bin/ . Package Managers For macOS and Linux users, llama-cli and llama-server can be installed with package managers including Homebrew, Nix, and Flox. Here, we show how to install llama-cli and llama-server with Homebrew. For other package managers, please check the instructions here . Installing with Homebrew is very simple: Ensure that Homebrew is available on your operating system. If you don’t have Homebrew, you can install it as in its website . Second, you can install the pre-built binaries, llama-cli and llama-server included, with a single command: brew install llama.cpp Note that the installed binaries might not be built with the optimal compile options for your hardware, which can lead to poor performance. They also don’t support GPU on Linux systems. Binary Release You can also download pre-built binaries from GitHub Releases . Please note that those pre-built binaries files are architecture-, backend-, and os-specific. If you are not sure what those mean, you probably don’t want to use them and running with incompatible versions will most likely fail or lead to poor performance. The file name is like llama-&lt;version&gt;-bin-&lt;os&gt;-&lt;feature&gt;-&lt;arch&gt;.zip . There are three simple parts: &lt;version&gt; : the version of llama.cpp. The latest is preferred, but as llama.cpp is updated and released frequently, the latest may contain bugs. If the latest version does not work, try the previous release until it works. &lt;os&gt; : the operating system. win for Windows; macos for macOS; linux for Linux. &lt;arch&gt; : the system architecture. x64 for x86_64 , e.g., most Intel and AMD systems, including Intel Mac; arm64 for arm64 , e.g., Apple Silicon or Snapdragon-based systems. The &lt;feature&gt; part is somewhat complicated for Windows: Running on CPU x86_64 CPUs: We suggest try the avx2 one first. noavx : No hardware acceleration at all. avx2 , avx , avx512 : SIMD-based acceleration. Most modern desktop CPUs should support avx2, and some CPUs support avx512 . openblas : Relying on OpenBLAS for acceleration for prompt processing but not generation. arm64 CPUs: We suggest try the llvm one first. llvm and msvc are different compilers Running on GPU: We suggest try the cu&lt;cuda_verison&gt; one for NVIDIA GPUs, kompute for AMD GPUs, and sycl for Intel GPUs first. Ensure that you have related drivers installed. vulcan : support certain NVIDIA and AMD GPUs kompute : support certain NVIDIA and AMD GPUs sycl : Intel GPUs, oneAPI runtime is included cu&lt;cuda_verison&gt; : NVIDIA GPUs, CUDA runtime is not included. You can download the cudart-llama-bin-win-cu&lt;cuda_version&gt;-x64.zip and unzip it to the same directory if you don’t have the corresponding CUDA toolkit installed. You don’t have much choice for macOS or Linux. Linux: only one prebuilt binary, llama-&lt;version&gt;-bin-linux-x64.zip , supporting CPU. macOS: llama-&lt;version&gt;-bin-macos-x64.zip for Intel Mac with no GPU support; llama-&lt;version&gt;-bin-macos-arm64.zip for Apple Silicon with GPU support. After downloading the .zip file, unzip them into a directory and open a terminal at that directory. Getting the GGUF ¶ GGUF [ 1 ] is a file format for storing information needed to run a model, including but not limited to model weights, model hyperparameters, default generation configuration, and tokenizer. You can use the official Qwen GGUFs from our Hugging Face Hub or prepare your own GGUF file. Using the Official Qwen3 GGUFs ¶ We provide a series of GGUF models in our Hugging Face organization, and to search for what you need you can search the repo names with -GGUF . Download the GGUF model that you want with huggingface-cli (you need to install it first with pip install huggingface_hub ): huggingface-cli download &lt;model_repo&gt; &lt;gguf_file&gt; --local-dir &lt;local_dir&gt; For example: huggingface-cli download Qwen/Qwen3-8B-GGUF qwen3-8b-q4_k_m.gguf --local-dir . This will download the Qwen3-8B model in GGUF format quantized with the scheme Q4_K_M. Preparing Your Own GGUF ¶ Model files from Hugging Face Hub can be converted to GGUF, using the convert-hf-to-gguf.py Python script. It does require you to have a working Python environment with at least transformers installed. Obtain the source file if you haven’t already: git clone https://github.com/ggml-org/llama.cpp cd llama.cpp Suppose you would like to use Qwen3-8B you can make a GGUF file for the fp16 model as shown below: python convert-hf-to-gguf.py Qwen/Qwen3-8B --outfile qwen3-8b-f16.gguf The first argument to the script refers to the path to the HF model directory or the HF model name, and the second argument refers to the path of your output GGUF file. Remember to create the output directory before you run the command. The fp16 model could be a bit heavy for running locally, and you can quantize the model as needed. We introduce the method of creating and quantizing GGUF files in this guide . You can refer to that document for more information. Run Qwen with llama.cpp ¶ Note Regarding switching between thinking and non-thinking modes, while the soft switch is always available, the hard switch implemented in the chat template is not exposed in llama.cpp. The quick workaround is to pass a custom chat template equivalent to always enable_thinking=False via --chat-template-file . llama-cli ¶ llama-cli is a console program which can be used to chat with LLMs. Simple run the following command where you place the llama.cpp programs: ./llama-cli -hf Qwen/Qwen3-8B-GGUF:Q8_0 --jinja --color -ngl 99 -fa -sm row --temp 0 .6 --top-k 20 --top-p 0 .95 --min-p 0 -c 40960 -n 32768 --no-context-shift Here are some explanations to the above command: Model : llama-cli supports using model files from local path, remote URL, or Hugging Face hub. -hf Qwen/Qwen3-8B-GGUF:Q8_0 in the above indicates we are using the model file from Hugging Face hub To use a local path, pass -m qwen3-8b-q8_0.gguf instead To use a remote URL, pass -mu https://hf.co/Qwen/Qwen3-8B-GGUF/resolve/main/qwen3-8b-Q8_0.gguf?download=true instead Speed Optimization : CPU: llama-cli by default will use CPU and you can change -t to specify how many threads you would like it to use, e.g., -t 8 means using 8 threads. GPU: If the programs are built with GPU support, you can use -ngl , which allows offloading some layers to the GPU for computation. If there are multiple GPUs, it will offload to all the GPUs. You can use -dev to control the devices used and -sm to control which kinds of parallelism is used. For example, -ngl 99 -dev cuda0,cuda1 -sm row means offload all layers to GPU 0 and GPU1 using the split mode row. Adding -fa may also speed up the generation. Sampling Parameters : llama.cpp supports a variety of sampling methods and has default configuration for many of them. It is recommended to adjust those parameters according to the actual case and the recommended parameters from Qwen3 modelcard could be used as a reference. If you encounter repetition and endless generation, it is recommended to pass in addition --presence-penalty up to 2.0 . Context Management : llama.cpp adopts the “rotating” context management by default. The -c controls the maximum context length (default 4096, 0 means loaded from model), and -n controls the maximum generation length each time (default -1 means infinite until ending, -2 means until context full). When the context is full but the generation doesn’t end, the first --keep tokens (default 0, -1 means all) from the initial prompt is kept, and the first half of the rest is discarded. Then, the model continues to generate based on the new context tokens. You can set --no-context-shift to prevent this rotating behavior and the generation will stop once -c is reached. llama.cpp supports YaRN, which can be enabled by -c 131072 --rope-scaling yarn --rope-scale 4 --yarn-orig-ctx 32768 . Chat : --jinja indicates using the chat template embedded in the GGUF which is preferred and --color indicates coloring the texts so that user input and model output can be better differentiated. If there is a chat template, like in Qwen3 models, llama-cli will enter chat mode automatically. To stop generation or exit press “Ctrl+C”. You can use -sys to add a system prompt. llama-server ¶ llama-server is a simple HTTP server, including a set of LLM REST APIs and a simple web front end to interact with LLMs using llama.cpp. The core command is similar to that of llama-cli. In addition, it supports thinking content parsing and tool call parsing. ./llama-server -hf Qwen/Qwen3-8B-GGUF:Q8_0 --jinja --reasoning-format deepseek -ngl 99 -fa -sm row --temp 0 .6 --top-k 20 --top-p 0 .95 --min-p 0 -c 40960 -n 32768 --no-context-shift By default, the server will listen at http://localhost:8080 which can be changed by passing --host and --port . The web front end can be assessed from a browser at http://localhost:8080/ . The OpenAI compatible API is at http://localhost:8080/v1/ . What’s More ¶ If you still find it difficult to use llama.cpp, don’t worry, just check out other llama.cpp-based applications. For example, Qwen3 has already been officially part of Ollama and LM Studio, which are platforms for your to search and run local LLMs. Have fun! [ 1 ] ( 1 , 2 ) GPT-Generated Unified Format Next Ollama Previous Transformers Copyright &#169; 2024, Qwen Team Made with Sphinx and @pradyunsg 's Furo On this page llama.cpp Getting the Program Getting the GGUF Using the Official Qwen3 GGUFs Preparing Your Own GGUF Run Qwen with llama.cpp llama-cli llama-server What’s More ",
      "fetch_method": "direct-html"
    },
    {
      "arxiv_id": "https://qwen.readthedocs.io/en/latest/deployment/openllm.html",
      "full_text": " OpenLLM - Qwen Contents Menu Expand Light mode Dark mode Auto light/dark, in light mode Auto light/dark, in dark mode Hide navigation sidebar Hide table of contents sidebar Skip to content Toggle site navigation sidebar Qwen Toggle Light / Dark / Auto color theme Toggle table of contents sidebar Qwen Getting Started Quickstart Key Concepts Speed Benchmark Performance of Quantized Models Inference Transformers Run Locally llama.cpp Ollama MLX LM Deployment SGLang vLLM TGI dstack SkyPilot OpenLLM Quantization AWQ GPTQ llama.cpp Training Axolotl LLaMA-Factory MS-SWIFT Unsloth verl Framework Qwen-Agent Function Calling LlamaIndex Langchain Back to top View this page Toggle Light / Dark / Auto color theme Toggle table of contents sidebar OpenLLM ¶ Attention To be updated for Qwen3. OpenLLM allows developers to run Qwen2.5 models of different sizes as OpenAI-compatible APIs with a single command. It features a built-in chat UI, state-of-the-art inference backends, and a simplified workflow for creating enterprise-grade cloud deployment with Qwen2.5. Visit the OpenLLM repository to learn more. Installation ¶ Install OpenLLM using pip . pip install openllm Verify the installation and display the help information: openllm --help Quickstart ¶ Before you run any Qwen2.5 model, ensure your model repository is up to date by syncing it with OpenLLM’s latest official repository. openllm repo update List the supported Qwen2.5 models: openllm model list --tag qwen2.5 The results also display the required GPU resources and supported platforms: model version repo required GPU RAM platforms ------- --------------------- ------- ------------------ ----------- qwen2.5 qwen2.5:0.5b default 12G linux qwen2.5:1.5b default 12G linux qwen2.5:3b default 12G linux qwen2.5:7b default 24G linux qwen2.5:14b default 80G linux qwen2.5:14b-ggml-q4 default macos qwen2.5:14b-ggml-q8 default macos qwen2.5:32b default 80G linux qwen2.5:32b-ggml-fp16 default macos qwen2.5:72b default 80Gx2 linux qwen2.5:72b-ggml-q4 default macos To start a server with one of the models, use openllm serve like this: openllm serve qwen2.5:7b By default, the server starts at http://localhost:3000/ . Interact with the model server ¶ With the model server up and running, you can call its APIs in the following ways: CURL Send an HTTP request to its /generate endpoint via CURL: curl -X &#39;POST&#39; \\ &#39;http://localhost:3000/api/generate&#39; \\ -H &#39;accept: text/event-stream&#39; \\ -H &#39;Content-Type: application/json&#39; \\ -d &#39;{ &quot;prompt&quot;: &quot;Tell me something about large language models.&quot;, &quot;model&quot;: &quot;Qwen/Qwen2.5-7B-Instruct&quot;, &quot;max_tokens&quot;: 2048, &quot;stop&quot;: null }&#39; Python client Call the OpenAI-compatible endpoints with frameworks and tools that support the OpenAI API protocol. Here is an example: from openai import OpenAI client = OpenAI ( base_url = &#39;http://localhost:3000/v1&#39; , api_key = &#39;na&#39; ) # Use the following func to get the available models # model_list = client.models.list() # print(model_list) chat_completion = client . chat . completions . create ( model = &quot;Qwen/Qwen2.5-7B-Instruct&quot; , messages = [ { &quot;role&quot; : &quot;user&quot; , &quot;content&quot; : &quot;Tell me something about large language models.&quot; } ], stream = True , ) for chunk in chat_completion : print ( chunk . choices [ 0 ] . delta . content or &quot;&quot; , end = &quot;&quot; ) Chat UI OpenLLM provides a chat UI at the /chat endpoint for the LLM server at http://localhost:3000/chat . Model repository ¶ A model repository in OpenLLM represents a catalog of available LLMs. You can add your own repository to OpenLLM with custom Qwen2.5 variants for your specific needs. See our documentation to learn details . Next AWQ Previous SkyPilot Copyright &#169; 2024, Qwen Team Made with Sphinx and @pradyunsg 's Furo On this page OpenLLM Installation Quickstart Interact with the model server Model repository ",
      "fetch_method": "direct-html"
    },
    {
      "arxiv_id": "https://raw.githubusercontent.com/QwenLM/Qwen2.5/main/Qwen3_Technical_Report.pdf",
      "full_text": "2025-05-14\nQwen3 Technical Report\nQwen Team\nhttps://huggingface.co/Qwen\nhttps://modelscope.cn/organization/qwen\nhttps://github.com/QwenLM/Qwen3\nAbstract\nIn this work, we present Qwen3, the latest version of the Qwen model family. Qwen3\ncomprises a series of large language models (LLMs) designed to advance performance,\nefficiency, and multilingual capabilities. The Qwen3 series includes models of both dense\nand Mixture-of-Expert (MoE) architectures, with parameter scales ranging from 0.6 to\n235 billion. A key innovation in Qwen3 is the integration of thinking mode (for complex,\nmulti-step reasoning) and non-thinking mode (for rapid, context-driven responses) into a\nunified framework. This eliminates the need to switch between different models—–such\nas chat-optimized models (e.g., GPT-4o) and dedicated reasoning models (e.g., QwQ-\n32B)—–and enables dynamic mode switching based on user queries or chat templates.\nMeanwhile, Qwen3 introduces a thinking budget mechanism, allowing users to allocate\ncomputational resources adaptively during inference, thereby balancing latency and\nperformance based on task complexity. Moreover, by leveraging the knowledge from the\nflagship models, we significantly reduce the computational resources required to build\nsmaller-scale models, while ensuring their highly competitive performance. Empirical\nevaluations demonstrate that Qwen3 achieves state-of-the-art results across diverse\nbenchmarks, including tasks in code generation, mathematical reasoning, agent tasks,\netc., competitive against larger MoE models and proprietary models. Compared to its\npredecessor Qwen2.5, Qwen3 expands multilingual support from 29 to 119 languages\nand dialects, enhancing global accessibility through improved cross-lingual understand-\ning and generation capabilities. To facilitate reproducibility and community-driven\nresearch and development, all Qwen3 models are publicly accessible under Apache 2.0.\n1\n\n1\nIntroduction\nThe pursuit of artificial general intelligence (AGI) or artificial super intelligence (ASI) has long been a goal\nfor humanity. Recent advancements in large foundation models, e.g., GPT-4o (OpenAI, 2024), Claude\n3.7 (Anthropic, 2025), Gemini 2.5 (DeepMind, 2025), DeepSeek-V3 (Liu et al., 2024a), Llama-4 (Meta-AI,\n2025), and Qwen2.5 (Yang et al., 2024b), have demonstrated significant progress toward this objective.\nThese models are trained on vast datasets spanning trillions of tokens across diverse domains and tasks,\neffectively distilling human knowledge and capabilities into their parameters. Furthermore, recent\ndevelopments in reasoning models, optimized through reinforcement learning, highlight the potential\nfor foundation models to enhance inference-time scaling and achieve higher levels of intelligence, e.g.,\no3 (OpenAI, 2025), DeepSeek-R1 (Guo et al., 2025). While most state-of-the-art models remain proprietary,\nthe rapid growth of open-source communities has substantially reduced the performance gap between\nopen-weight and closed-source models. Notably, an increasing number of top-tier models (Meta-AI, 2025;\nLiu et al., 2024a; Guo et al., 2025; Yang et al., 2024b) are now being released as open-source, fostering\nbroader research and innovation in artificial intelligence.\nIn this work, we introduce Qwen3, the latest series in our foundation model family, Qwen. Qwen3 is\na collection of open-weight large language models (LLMs) that achieve state-of-the-art performance\nacross a wide variety of tasks and domains. We release both dense and Mixture-of-Experts (MoE) models,\nwith the number of parameters ranging from 0.6 billion to 235 billion, to meet the needs of different\ndownstream applications. Notably, the flagship model, Qwen3-235B-A22B, is an MoE model with a\ntotal of 235 billion parameters and 22 billion activated ones per token. This design ensures both high\nperformance and efficient inference.\nQwen3 introduces several key advancements to enhance its functionality and usability. First, it integrates\ntwo distinct operating modes, thinking mode and non-thinking mode, into a single model. This allows\nusers to switch between these modes without alternating between different models, e.g., switching from\nQwen2.5 to QwQ (Qwen Team, 2024). This flexibility ensures that developers and users can adapt the\nmodel’s behavior to suit specific tasks efficiently. Additionally, Qwen3 incorporates thinking budgets, pro-\nviding users with fine-grained control over the level of reasoning effort applied by the model during task\nexecution. This capability is crucial to the optimization of computational resources and performance, tai-\nloring the model’s thinking behavior to meet varying complexity in real-world applications. Furthermore,\nQwen3 has been pre-trained on 36 trillion tokens covering up to 119 languages and dialects, effectively\nenhancing its multilingual capabilities. This broadened language support amplifies its potential for\ndeployment in global use cases and international applications. These advancements together establish\nQwen3 as a cutting-edge open-source large language model family, capable of effectively addressing\ncomplex tasks across various domains and languages.\nThe pre-training process for Qwen3 utilizes a large-scale dataset consisting of approximately 36 trillion\ntokens, curated to ensure linguistic and domain diversity. To efficiently expand the training data, we\nemploy a multi-modal approach: Qwen2.5-VL (Bai et al., 2025) is finetuned to extract text from extensive\nPDF documents. We also generate synthetic data using domain-specific models: Qwen2.5-Math (Yang\net al., 2024c) for mathematical content and Qwen2.5-Coder (Hui et al., 2024) for code-related data. The\npre-training process follows a three-stage strategy. In the first stage, the model is trained on about 30\ntrillion tokens to build a strong foundation of general knowledge. In the second stage, it is further trained\non knowledge-intensive data to enhance reasoning abilities in areas like science, technology, engineering,\nand mathematics (STEM) and coding. Finally, in the third stage, the model is trained on long-context\ndata to increase its maximum context length from 4,096 to 32,768 tokens.\nTo better align foundation models with human preferences and downstream applications, we employ a\nmulti-stage post-training approach that empowers both thinking (reasoning) and non-thinking modes. In\nthe first two stages, we focus on developing strong reasoning abilities through long chain-of-thought\n(CoT) cold-start finetuning and reinforcement learning focusing on mathematics and coding tasks. In the\nfinal two stages, we combine data with and without reasoning paths into a unified dataset for further\nfine-tuning, enabling the model to handle both types of input effectively, and we then apply general-\ndomain reinforcement learning to improve performance across a wide range of downstream tasks. For\nsmaller models, we use strong-to-weak distillation, leveraging both off-policy and on-policy knowledge\ntransfer from larger models to enhance their capabilities. Distillation from advanced teacher models\nsignificantly outperforms reinforcement learning in performance and training efficiency.\nWe evaluate both pre-trained and post-trained versions of our models across a comprehensive set of\nbenchmarks spanning multiple tasks and domains. Experimental results show that our base pre-trained\nmodels achieve state-of-the-art performance. The post-trained models, whether in thinking or non-\nthinking mode, perform competitively against leading proprietary models and large mixture-of-experts\n(MoE) models such as o1, o3-mini, and DeepSeek-V3. Notably, our models excel in coding, mathematics,\nand agent-related tasks. For example, the flagship model Qwen3-235B-A22B achieves 85.7 on AIME’24\n2\n\nand 81.5 on AIME’25 (AIME, 2025), 70.7 on LiveCodeBench v5 (Jain et al., 2024), 2,056 on CodeForces,\nand 70.8 on BFCL v3 (Yan et al., 2024). In addition, other models in the Qwen3 series also show strong\nperformance relative to their size. Furthermore, we observe that increasing the thinking budget for\nthinking tokens leads to a consistent improvement in the model’s performance across various tasks.\nIn the following sections, we describe the design of the model architecture, provide details on its training\nprocedures, present the experimental results of pre-trained and post-trained models, and finally, conclude\nthis technical report by summarizing the key findings and outlining potential directions for future\nresearch.\n2\nArchitecture\nThe Qwen3 series includes 6 dense models, namely Qwen3-0.6B, Qwen3-1.7B, Qwen3-4B, Qwen3-8B,\nQwen3-14B, and Qwen3-32B, and 2 MoE models, Qwen3-30B-A3B and Qwen3-235B-A22B. The flagship\nmodel, Qwen3-235B-A22B, has a total of 235B parameters with 22B activated ones. Below, we elaborate\non the architecture of the Qwen3 models.\nThe architecture of the Qwen3 dense models is similar to Qwen2.5 (Yang et al., 2024b), including using\nGrouped Query Attention (GQA, Ainslie et al., 2023), SwiGLU (Dauphin et al., 2017), Rotary Positional\nEmbeddings (RoPE, Su et al., 2024), and RMSNorm (Jiang et al., 2023) with pre-normalization. Besides,\nwe remove QKV-bias used in Qwen2 (Yang et al., 2024a) and introduce QK-Norm (Dehghani et al., 2023)\nto the attention mechanism to ensure stable training for Qwen3. Key information on model architecture\nis provided in Table 1.\nThe Qwen3 MoE models share the same fundamental architecture as the Qwen3 dense models. Key\ninformation on model architecture is provided in Table 2. We follow Qwen2.5-MoE (Yang et al., 2024b)\nand implement fine-grained expert segmentation (Dai et al., 2024). The Qwen3 MoE models have 128 total\nexperts with 8 activated experts per token. Unlike Qwen2.5-MoE, the Qwen3-MoE design excludes shared\nexperts. Furthermore, we adopt the global-batch load balancing loss (Qiu et al., 2025) to encourage expert\nspecialization. These architectural and training innovations have yielded substantial improvements in\nmodel performance across downstream tasks.\nQwen3 models utilize Qwen’s tokenizer (Bai et al., 2023), which implements byte-level byte-pair encoding\n(BBPE, Brown et al., 2020; Wang et al., 2020; Sennrich et al., 2016) with a vocabulary size of 151,669.\nTable 1: Model architecture of Qwen3 dense models.\nModels\nLayers\nHeads (Q / KV)\nTie Embedding\nContext Length\nQwen3-0.6B\n28\n16 / 8\nYes\n32K\nQwen3-1.7B\n28\n16 / 8\nYes\n32K\nQwen3-4B\n36\n32 / 8\nYes\n128K\nQwen3-8B\n36\n32 / 8\nNo\n128K\nQwen3-14B\n40\n40 / 8\nNo\n128K\nQwen3-32B\n64\n64 / 8\nNo\n128K\nTable 2: Model architecture of Qwen3 MoE models.\nModels\nLayers\nHeads (Q / KV)\n# Experts (Total / Activated)\nContext Length\nQwen3-30B-A3B\n48\n32 / 4\n128 / 8\n128K\nQwen3-235B-A22B\n94\n64 / 4\n128 / 8\n128K\n3\nPre-training\nIn this section, we describe the construction of our pretraining data, the details of our pretraining\napproach, and present experimental results from evaluating the base models on standard benchmarks.\n3.1\nPre-training Data\nCompared with Qwen2.5 (Yang et al., 2024b), we have significantly expanded the scale and diversity of\nour training data. Specifically, we collected twice as many pre-training tokens—covering three times\nmore languages. All Qwen3 models are trained on a large and diverse dataset consisting of 119 languages\nand dialects, with a total of 36 trillion tokens. This dataset includes high-quality content in various\n3\n\ndomains such as coding, STEM (Science, Technology, Engineering, and Mathematics), reasoning tasks,\nbooks, multilingual texts, and synthetic data.\nTo further expand the pre-training data corpus, we first employ the Qwen2.5-VL model (Bai et al., 2025)\nto perform text recognition on a large volume of PDF-like documents. The recognized text is then refined\nusing the Qwen2.5 model (Yang et al., 2024b), which helps improve its quality. Through this two-step\nprocess, we are able to obtain an additional set of high-quality text tokens, amounting to trillions in total.\nBesides, we employ Qwen2.5 (Yang et al., 2024b), Qwen2.5-Math (Yang et al., 2024c), and Qwen2.5-Coder\n(Hui et al., 2024) models to synthesize trillions of text tokens in different formats, including textbooks,\nquestion-answering, instructions, and code snippets, covering dozens of domains. Finally, we further\nexpand the pre-training corpus by incorporating additional multilingual data and introducing more\nlanguages. Compared to the pre-training data used in Qwen2.5, the number of supported languages has\nbeen significantly increased from 29 to 119, enhancing the model’s linguistic coverage and cross-lingual\ncapabilities.\nWe have developed a multilingual data annotation system designed to enhance both the quality and\ndiversity of training data. This system has been applied to our large-scale pre-training datasets, annotating\nover 30 trillion tokens across multiple dimensions such as educational value, fields, domains, and safety.\nThese detailed annotations support more effective data filtering and combination. Unlike previous\nstudies (Xie et al., 2023; Fan et al., 2023; Liu et al., 2024b) that optimize the data mixture at the data source\nor domain level, our method optimizes the data mixture at the instance-level through extensive ablation\nexperiments on small proxy models with the fine-grained data labels.\n3.2\nPre-training Stage\nThe Qwen3 models are pre-trained through a three-stage process:\n(1) General Stage (S1): At the first pre-training stage, all Qwen3 models are trained on over 30\ntrillion tokens using a sequence length of 4,096 tokens. At this stage, the models have been fully\npre-trained on language proficiency and general world knowledge, with training data covering\n119 languages and dialects.\n(2) Reasoning Stage (S2): To further improve the reasoning ability, we optimize the pre-training\ncorpus of this stage by increasing the proportion of STEM, coding, reasoning, and synthetic data.\nThe models are further pre-trained with about 5T higher-quality tokens at a sequence length of\n4,096 tokens. We also accelerate the learning rate decay during this stage.\n(3) Long Context Stage: In the final pre-training stage, we collect high-quality long context corpora\nto extend the context length of Qwen3 models. All models are pre-trained on hundreds of billions\nof tokens with a sequence length of 32,768 tokens. The long context corpus includes 75% of text\nbetween 16,384 to 32,768 tokens in length, and 25% of text between 4,096 to 16,384 in length.\nFollowing Qwen2.5 (Yang et al., 2024b), we increase the base frequency of RoPE from 10,000 to\n1,000,000 using the ABF technique (Xiong et al., 2023). Meanwhile, we introduce YARN (Peng\net al., 2023) and Dual Chunk Attention (DCA, An et al., 2024) to achieve a four-fold increase in\nsequence length capacity during inference.\nSimilar to Qwen2.5 (Yang et al., 2024b), we develop scaling laws for optimal hyper-parameters (e.g.,\nlearning rate scheduler, and batch size) predictions based on three pre-training stages mentioned above.\nThrough extensive experiments, we systematically study the relationship between model architecture,\ntraining data, training stage, and optimal training hyper-parameters. Finally, we set the predicted optimal\nlearning rate and batch size strategy for each dense or MoE model.\n3.3\nPre-training Evaluation\nWe conduct comprehensive evaluations of the base language models of the Qwen3 series. The evaluation\nof base models mainly focuses on their performance in general knowledge, reasoning, mathematics,\nscientific knowledge, coding, and multilingual capabilities. The evaluation datasets for pre-trained base\nmodels include 15 benchmarks:\n• General Tasks: MMLU (Hendrycks et al., 2021a) (5-shot), MMLU-Pro (Wang et al., 2024) (5-\nshot, CoT), MMLU-redux (Gema et al., 2024) (5-shot), BBH (Suzgun et al., 2023) (3-shot, CoT),\nSuperGPQA (Du et al., 2025)(5-shot, CoT).\n• Math & STEM Tasks: GPQA (Rein et al., 2023) (5-shot, CoT), GSM8K (Cobbe et al., 2021) (4-shot,\nCoT), MATH (Hendrycks et al., 2021b) (4-shot, CoT).\n4\n\n• Coding Tasks: EvalPlus (Liu et al., 2023a) (0-shot) (Average of HumanEval (Chen et al., 2021),\nMBPP (Austin et al., 2021), Humaneval+, MBPP+) (Liu et al., 2023a), MultiPL-E (Cassano et al.,\n2023) (0-shot) (Python, C++, JAVA, PHP, TypeScript, C#, Bash, JavaScript), MBPP-3shot (Austin\net al., 2021), CRUX-O of CRUXEval (1-shot) (Gu et al., 2024).\n• Multilingual Tasks: MGSM (Shi et al., 2023) (8-shot, CoT), MMMLU (OpenAI, 2024) (5-shot),\nINCLUDE (Romanou et al., 2024) (5-shot).\nFor the base model baselines, we compare the Qwen3 series base models with the Qwen2.5 base models\n(Yang et al., 2024b) and other leading open-source base models, including DeepSeek-V3 Base (Liu et al.,\n2024a), Gemma-3 (Team et al., 2025), Llama-3 (Dubey et al., 2024), and Llama-4 (Meta-AI, 2025) series\nbase models, in terms of scale of parameters. All models are evaluated using the same evaluation pipeline\nand the widely-used evaluation settings to ensure fair comparison.\nSummary of Evaluation Results\nBased on the overall evaluation results, we highlight some key\nconclusions of Qwen3 base models.\n(1) Compared with the previously open-source SOTA dense and MoE base models (such as DeepSeek-\nV3 Base, Llama-4-Maverick Base, and Qwen2.5-72B-Base), Qwen3-235B-A22B-Base outperforms\nthese models in most tasks with significantly fewer total parameters or activated parameters.\n(2) For the Qwen3 MoE base models, our experimental results indicate that: (a) Using the same\npre-training data, Qwen3 MoE base models can achieve similar performance to Qwen3 dense\nbase models with only 1/5 activated parameters. (b) Due to the improvements of the Qwen3\nMoE architecture, the scale-up of the training tokens, and more advanced training strategies,\nthe Qwen3 MoE base models can outperform the Qwen2.5 MoE base models with less than 1/2\nactivated parameters and fewer total parameters. (c) Even with 1/10 of the activated parameters of\nthe Qwen2.5 dense base model, the Qwen3 MoE base model can achieve comparable performance,\nwhich brings us significant advantages in inference and training costs.\n(3) The overall performance of the Qwen3 dense base models is comparable to the Qwen2.5 base\nmodels at higher parameter scales. For example, Qwen3-1.7B/4B/8B/14B/32B-Base achieve\ncomparable performance to Qwen2.5-3B/7B/14B/32B/72B-Base, respectively. Especially in\nSTEM, coding, and reasoning benchmarks, the performance of Qwen3 dense base models even\nsurpasses Qwen2.5 base models at higher parameter scales.\nThe detailed results are as follows.\nQwen3-235B-A22B-Base\nWe compare Qwen3-235B-A22B-Base to our previous similar-sized MoE\nQwen2.5-Plus-Base (Yang et al., 2024b) and other leading open-source base models: Llama-4-Maverick\n(Meta-AI, 2025), Qwen2.5-72B-Base (Yang et al., 2024b), DeepSeek-V3 Base (Liu et al., 2024a). From\nthe results in Table 3, the Qwen3-235B-A22B-Base model attains the highest performance scores across\nmost of the evaluated benchmarks. We further compare Qwen3-235B-A22B-Base with other baselines\nseparately for the detailed analysis.\n(1) Compared with the recently open-source model Llama-4-Maverick-Base, which has about twice\nthe number of parameters, Qwen3-235B-A22B-Base still performs better on most benchmarks.\n(2) Compared with the previously state-of-the-art open-source model DeepSeek-V3-Base, Qwen3-\n235B-A22B-Base outperforms DeepSeek-V3-Base on 14 out of 15 evaluation benchmarks with\nonly about 1/3 the total number of parameters and 2/3 activated parameters, demonstrating the\npowerful and cost-effectiveness of our models.\n(3) Compared with our previous MoE Qwen2.5-Plus of similar size, Qwen3-235B-A22B-Base sig-\nnificantly outperforms it with fewer parameters and activated parameters, which shows the\nremarkable advantages of Qwen3 in pre-training data, training strategy, and model architecture.\n(4) Compared with our previous flagship open-source dense model Qwen2.5-72B-Base, Qwen3-\n235B-A22B-Base surpasses the latter in all benchmarks and uses fewer than 1/3 of the activated\nparameters. Meanwhile, due to the advantage of the model architecture, the inference costs and\ntraining costs on each trillion tokens of Qwen3-235B-A22B-Base are much cheaper than those of\nQwen2.5-72B-Base.\nQwen3-32B-Base\nQwen3-32B-Base is our largest dense model among the Qwen3 series. We compare\nit to the baselines of similar sizes, including Gemma-3-27B (Team et al., 2025) and Qwen2.5-32B (Yang\net al., 2024b). In addition, we introduce two strong baselines: the recently open-source MoE model Llama-\n4-Scout, which has three times the parameters of Qwen3-32B-Base but half the activated parameters;\n5\n\nTable 3: Comparison among Qwen3-235B-A22B-Base and other representative strong open-source\nbaselines. The highest, the second-best scores are shown in bold and underlined, respectively.\nQwen2.5-72B Qwen2.5-Plus Llama-4-Maverick DeepSeek-V3 Qwen3-235B-A22B\nBase\nBase\nBase\nBase\nBase\nArchitecture\nDense\nMoE\nMoE\nMoE\nMoE\n# Total Params\n72B\n271B\n402B\n671B\n235B\n# Activated Params\n72B\n37B\n17B\n37B\n22B\nGeneral Tasks\nMMLU\n86.06\n85.02\n85.16\n87.19\n87.81\nMMLU-Redux\n83.91\n82.69\n84.05\n86.14\n87.40\nMMLU-Pro\n58.07\n63.52\n63.91\n59.84\n68.18\nSuperGPQA\n36.20\n37.18\n40.85\n41.53\n44.06\nBBH\n86.30\n85.60\n83.62\n86.22\n88.87\nMath & STEM Tasks\nGPQA\n45.88\n41.92\n43.94\n41.92\n47.47\nGSM8K\n91.50\n91.89\n87.72\n87.57\n94.39\nMATH\n62.12\n62.78\n63.32\n62.62\n71.84\nCoding Tasks\nEvalPlus\n65.93\n61.43\n68.38\n63.75\n77.60\nMultiPL-E\n58.70\n62.16\n57.28\n62.26\n65.94\nMBPP\n76.00\n74.60\n75.40\n74.20\n81.40\nCRUX-O\n66.20\n68.50\n77.00\n76.60\n79.00\nMultilingual Tasks\nMGSM\n82.40\n82.21\n79.69\n82.68\n83.53\nMMMLU\n84.40\n83.49\n83.09\n85.88\n86.70\nINCLUDE\n69.05\n66.97\n73.47\n75.17\n73.46\nTable 4: Comparison among Qwen3-32B-Base and other strong open-source baselines. The highest\nand second-best scores are shown in bold and underlined, respectively.\nQwen2.5-32B\nQwen2.5-72B\nGemma-3-27B\nLlama-4-Scout\nQwen3-32B\nBase\nBase\nBase\nBase\nBase\nArchitecture\nDense\nDense\nDense\nMoE\nDense\n# Total Params\n32B\n72B\n27B\n109B\n32B\n# Activated Params\n32B\n72B\n27B\n17B\n32B\nGeneral Tasks\nMMLU\n83.32\n86.06\n78.69\n78.27\n83.61\nMMLU-Redux\n81.97\n83.91\n76.53\n71.09\n83.41\nMMLU-Pro\n55.10\n58.07\n52.88\n56.13\n65.54\nSuperGPQA\n33.55\n36.20\n29.87\n26.51\n39.78\nBBH\n84.48\n86.30\n79.95\n82.40\n87.38\nMath & STEM Tasks\nGPQA\n47.97\n45.88\n26.26\n40.40\n49.49\nGSM8K\n92.87\n91.50\n81.20\n85.37\n93.40\nMATH\n57.70\n62.12\n51.78\n51.66\n61.62\nCoding Tasks\nEvalPlus\n66.25\n65.93\n55.78\n59.90\n72.05\nMultiPL-E\n58.30\n58.70\n45.03\n47.38\n67.06\nMBPP\n73.60\n76.00\n68.40\n68.60\n78.20\nCRUX-O\n67.80\n66.20\n60.00\n61.90\n72.50\nMultilingual Tasks\nMGSM\n78.12\n82.40\n73.74\n79.93\n83.06\nMMMLU\n82.40\n84.40\n77.62\n74.83\n83.83\nINCLUDE\n64.35\n69.05\n68.94\n68.09\n67.87\n6\n\nTable 5: Comparison among Qwen3-14B-Base, Qwen3-30B-A3B-Base, and other strong open-source\nbaselines. The highest and second-best scores are shown in bold and underlined, respectively.\nGemma-3-12B Qwen2.5-14B Qwen2.5-32B Qwen2.5-Turbo Qwen3-14B Qwen3-30B-A3B\nBase\nBase\nBase\nBase\nBase\nBase\nArchitecture\nDense\nDense\nDense\nMoE\nDense\nMoE\n# Total Params\n12B\n14B\n32B\n42B\n14B\n30B\n# Activated Params\n12B\n14B\n32B\n6B\n14B\n3B\nGeneral Tasks\nMMLU\n73.87\n79.66\n83.32\n79.50\n81.05\n81.38\nMMLU-Redux\n70.70\n76.64\n81.97\n77.11\n79.88\n81.17\nMMLU-Pro\n44.91\n51.16\n55.10\n55.60\n61.03\n61.49\nSuperGPQA\n24.61\n30.68\n33.55\n31.19\n34.27\n35.72\nBBH\n74.28\n78.18\n84.48\n76.10\n81.07\n81.54\nMath & STEM Tasks\nGPQA\n31.31\n32.83\n47.97\n41.41\n39.90\n43.94\nGSM8K\n78.01\n90.22\n92.87\n88.32\n92.49\n91.81\nMATH\n44.43\n55.64\n57.70\n55.60\n62.02\n59.04\nCoding Tasks\nEvalPlus\n52.65\n60.70\n66.25\n61.23\n72.23\n71.45\nMultiPL-E\n43.03\n54.79\n58.30\n53.24\n61.69\n66.53\nMBPP\n60.60\n69.00\n73.60\n67.60\n73.40\n74.40\nCRUX-O\n52.00\n61.10\n67.80\n60.20\n68.60\n67.20\nMultilingual Tasks\nMGSM\n64.35\n74.68\n78.12\n70.45\n79.20\n79.11\nMMMLU\n72.50\n78.34\n82.40\n79.76\n79.69\n81.46\nINCLUDE\n63.34\n60.26\n64.35\n59.25\n64.55\n67.00\nTable 6: Comparison among Qwen8B-Base and other strong open-source baselines. The highest and\nsecond-best scores are shown in bold and underlined, respectively.\nLlama-3-8B\nQwen2.5-7B\nQwen2.5-14B\nQwen3-8B\nBase\nBase\nBase\nBase\nArchitecture\nDense\nDense\nDense\nDense\n# Total Params\n8B\n7B\n14B\n8B\n# Activated Params\n8B\n7B\n14B\n8B\nGeneral Tasks\nMMLU\n66.60\n74.16\n79.66\n76.89\nMMLU-Redux\n61.59\n71.06\n76.64\n76.17\nMMLU-Pro\n35.36\n45.00\n51.16\n56.73\nSuperGPQA\n20.54\n26.34\n30.68\n31.64\nBBH\n57.70\n70.40\n78.18\n78.40\nMath & STEM Tasks\nGPQA\n25.80\n36.36\n32.83\n44.44\nGSM8K\n55.30\n85.36\n90.22\n89.84\nMATH\n20.50\n49.80\n55.64\n60.80\nCoding Tasks\nEvalPlus\n44.13\n62.18\n60.70\n67.65\nMultiPL-E\n31.45\n50.73\n54.79\n58.75\nMBPP\n48.40\n63.40\n69.00\n69.80\nCRUX-O\n36.80\n48.50\n61.10\n62.00\nMultilingual Tasks\nMGSM\n38.92\n63.60\n74.68\n76.02\nMMMLU\n59.65\n71.34\n78.34\n75.72\nIINCLUDE\n44.94\n53.98\n60.26\n59.40\n7\n\nTable 7: Comparison among Qwen3-4B-Base and other strong open-source baselines. The highest and\nsecond-best scores are shown in bold and underlined, respectively.\nGemma-3-4B\nQwen2.5-3B\nQwen2.5-7B\nQwen3-4B\nBase\nBase\nBase\nBase\nArchitecture\nDense\nDense\nDense\nDense\n# Total Params\n4B\n3B\n7B\n4B\n# Activated Params\n4B\n3B\n7B\n4B\nGeneral Tasks\nMMLU\n59.51\n65.62\n74.16\n72.99\nMMLU-Redux\n56.91\n63.68\n71.06\n72.79\nMMLU-Pro\n29.23\n34.61\n45.00\n50.58\nSuperGPQA\n17.68\n20.31\n26.34\n28.43\nBBH\n51.70\n56.30\n70.40\n72.59\nMath & STEM Tasks\nGPQA\n24.24\n26.26\n36.36\n36.87\nGSM8K\n43.97\n79.08\n85.36\n87.79\nMATH\n26.10\n42.64\n49.80\n54.10\nCoding Tasks\nEvalPlus\n43.23\n46.28\n62.18\n63.53\nMultiPL-E\n28.06\n39.65\n50.73\n53.13\nMBPP\n46.40\n54.60\n63.40\n67.00\nCRUX-O\n34.00\n36.50\n48.50\n55.00\nMultilingual Tasks\nMGSM\n33.11\n47.53\n63.60\n67.74\nMMMLU\n59.62\n65.55\n71.34\n71.42\nINCLUDE\n49.06\n45.90\n53.98\n56.29\nTable 8: Comparison among Qwen3-1.7B-Base, Qwen3-0.6B-Base, and other strong open-source base-\nlines. The highest and second-best scores are shown in bold and underlined, respectively.\nQwen2.5-0.5B\nQwen3-0.6B\nGemma-3-1B\nQwen2.5-1.5B\nQwen3-1.7B\nBase\nBase\nBase\nBase\nBase\nArchitecture\nDense\nDense\nDense\nDense\nDense\n# Total Params\n0.5B\n0.6B\n1B\n1.5B\n1.7B\n# Activated Params\n0.5B\n0.6B\n1B\n1.5B\n1.7B\nGeneral Tasks\nMMLU\n47.50\n52.81\n26.26\n60.90\n62.63\nMMLU-Redux\n45.10\n51.26\n25.99\n58.46\n61.66\nMMLU-Pro\n15.69\n24.74\n9.72\n28.53\n36.76\nSuperGPQA\n11.30\n15.03\n7.19\n17.64\n20.92\nBBH\n20.30\n41.47\n28.13\n45.10\n54.47\nMath & STEM Tasks\nGPQA\n24.75\n26.77\n24.75\n24.24\n28.28\nGSM8K\n41.62\n59.59\n2.20\n68.54\n75.44\nMATH\n19.48\n32.44\n3.66\n35.00\n43.50\nCoding Tasks\nEvalPlus\n31.85\n36.23\n8.98\n44.80\n52.70\nMultiPL-E\n18.70\n24.58\n5.15\n33.10\n42.71\nMBPP\n29.80\n36.60\n9.20\n43.60\n55.40\nCRUX-O\n12.10\n27.00\n3.80\n29.60\n36.40\nMultilingual Tasks\nMGSM\n12.07\n30.99\n1.74\n32.82\n50.71\nMMMLU\n31.53\n50.16\n26.57\n60.27\n63.27\nINCLUDE\n24.74\n34.26\n25.62\n39.55\n45.57\n8\n\nand our previous flagship open-source dense model Qwen2.5-72B-Base, which has more than twice the\nnumber of parameters compared to Qwen3-32B-Base. The results are shown in Table 4, which support\nthree key conclusions:\n(1) Compared with the similar-sized models, Qwen3-32B-Base outperforms Qwen2.5-32B-Base and\nGemma-3-27B Base on most benchmarks. Notably, Qwen3-32B-Base achieves 65.54 on MMLU-\nPro and 39.78 on SuperGPQA, significantly outperforming its predecessor Qwen2.5-32B-Base.\nIn addition, Qwen3-32B-Base achieves significantly higher encoding benchmark scores than all\nbaseline models.\n(2) Surprisingly, we find that Qwen3-32B-Base achieves competitive results compared to Qwen2.5-\n72B-Base. Although Qwen3-32B-Base has less than half the number of parameters of Qwen2.5-\n72B-Base, it outperforms Qwen2.5-72B-Base in 10 of the 15 evaluation benchmarks. On coding,\nmathematics, and reasoning benchmarks, Qwen3-32B-Base has remarkable advantages.\n(3) Compared to Llama-4-Scout-Base, Qwen3-32B-Base significantly outperforms it on all 15 bench-\nmarks, with only one-third of the number of parameters of Llama-4-Scout-Base, but twice the\nnumber of activated parameters.\nQwen3-14B-Base & Qwen3-30B-A3B-Base\nThe evaluation of the Qwen3-14B-Base and Qwen3-30B-\nA3B-Base is compared against baselines of similar sizes, including Gemma-3-12B Base, Qwen2.5-14B\nBase. Similarly, we also introduce two strong baselines: (1) Qwen2.5-Turbo (Yang et al., 2024b), which\nhas 42B parameters and 6B activated parameters. Note that its activated parameters are twice those of\nQwen3-30B-A3B-Base. (2) Qwen2.5-32B-Base, which has 11 times the activated parameters of Qwen3-\n30B-A3B and more than twice that of Qwen3-14B. The results are shown in Table 5, where we can draw\nthe following conclusions.\n(1) Compared with the similar-sized models, Qwen3-14B-Base significantly performs better than\nQwen2.5-14B-Base and Gemma-3-12B-Base on all 15 benchmarks.\n(2) Similarly, Qwen3-14B-Base also achieves very competitive results compared to Qwen2.5-32B-Base\nwith less than half of the parameters.\n(3) With only 1/5 activated non-embedding parameters, Qwen3-30B-A3B significantly outperforms\nQwen2.5-14B-Base on all tasks, and achieves comparable performance to Qwen3-14B-Base and\nQwen2.5-32B-Base, which brings us significant advantages in inference and training costs.\nQwen3-8B / 4B / 1.7B / 0.6B-Base\nFor edge-side models, we take similar-sized Qwen2.5, Llama-3, and\nGemma-3 base models as the baselines. The results can be seen in Table 6, Table 7, and Table 8. All Qwen3\n8B / 4B / 1.7B / 0.6B-Base models continue to maintain strong performance across nearly all benchmarks.\nNotably, Qwen3-8B / 4B / 1.7B-Base models even outperform larger size Qwen2.5-14B / 7B / 3B Base\nmodels on over half of the benchmarks, especially on STEM-related and coding benchmarks, reflecting\nthe significant improvement of the Qwen3 models.\n4\nPost-training\nStage 1\nLong-CoT Cold Start\nStage 2\nReasoning RL\nStage 3\nThinking Mode Fusion\nStage 4\nGeneral RL\nStrong-to-Weak Distillation\nQwen3-235B-A22B\nQwen3-32B\nQwen3-30B-A3B\n14B/8B/4B/1.7B/0.6B\nBase Models\nBase Models\nLightweight Models\nFlagship Models\nFigure 1: Post-training pipeline of the Qwen3 series models.\n9\n\nThe post-training pipeline of Qwen3 is strategically designed with two core objectives:\n(1) Thinking Control: This involves the integration of two distinct modes, namely the “non-thinking”\nand “thinking” modes, providing users with the flexibility to choose whether the model should\nengage in reasoning or not, and to control the depth of thinking by specifying a token budget for\nthe thinking process.\n(2) Strong-to-Weak Distillation: This aims to streamline and optimize the post-training process\nfor lightweight models. By leveraging the knowledge from large-scale models, we substantially\nreduce both the computational costs and the development efforts required for building smaller-\nscale models.\nAs illustrated in Figure 1, the flagship models in the Qwen3 series follow a sophisticated four-stage\ntraining process. The first two stages focus on developing the models’ “thinking” abilities. The next two\nstages aim to integrate strong “non-thinking” functionalities into the models.\nPreliminary experiments suggest that directly distilling the output logits from teacher models into\nlightweight student models can effectively enhance their performance while maintaining fine-grained\ncontrol over their reasoning processes. This approach eliminates the necessity of performing an exhaustive\nfour-stage training process individually for every small-scale model. It leads to better immediate\nperformance, as indicated by higher Pass@1 scores, and also improves the model’s ability of exploration,\nas reflected in improved Pass@64 results. In addition, it achieves these gains with much greater training\nefficiency, requiring only 1/10 of the GPU hours compared to the four-stage training method.\nIn the following sections, we present the four-stage training process and provide a detailed explanation\nof the Strong-to-Weak Distillation approach.\n4.1\nLong-CoT Cold Start\nWe begin by curating a comprehensive dataset that spans a wide range of categories, including math,\ncode, logical reasoning, and general STEM problems. Each problem in the dataset is paired with verified\nreference answers or code-based test cases. This dataset serves as the foundation for the “cold start”\nphase of long Chain-of-Thought (long-CoT) training.\nThe dataset construction involves a rigorous two-phase filtering process: query filtering and response\nfiltering. In the query filtering phase, we use Qwen2.5-72B-Instruct to identify and remove queries that\nare not easily verifiable. This includes queries containing multiple sub-questions or those asking for\ngeneral text generation. Furthermore, we exclude queries that Qwen2.5-72B-Instruct can answer correctly\nwithout using CoT reasoning. This helps prevent the model from relying on superficial guessing and\nensures that only complex problems requiring deeper reasoning are included. Additionally, we annotate\neach query’s domain using Qwen2.5-72B-Instruct to maintain balanced domain representation across the\ndataset.\nAfter reserving a validation query set, we generate N candidate responses for each remaining query\nusing QwQ-32B (Qwen Team, 2025). When QwQ-32B consistently fails to generate correct solutions,\nhuman annotators manually assess the accuracy of the responses. For queries with positive Pass@N,\nfurther stringent filtering criteria are applied to remove responses that (1) yield incorrect final answers,\n(2) contain substantial repetition, (3) clearly indicate guesswork without adequate reasoning, (4) exhibit\ninconsistencies between the thinking and summary contents, (5) involve inappropriate language mixing or\nstylistic shifts, or (6) are suspected of being overly similar to potential validation set items. Subsequently,\na carefully selected subset of the refined dataset is used for the initial cold-start training of the reasoning\npatterns. The objective at this stage is to instill foundational reasoning patterns in the model without\noverly emphasizing immediate reasoning performance. This approach ensures that the model’s potential\nis not limited, allowing for greater flexibility and improvement during the subsequent reinforcement\nlearning (RL) phase. To achieve this objective effectively, it is preferable to minimize both the number of\ntraining samples and the training steps during this preparatory phase.\n4.2\nReasoning RL\nThe query-verifier pairs used in the Reasoning RL stage must satisfy the following four criteria: (1) They\nwere not used during the cold-start phase. (2) They are learnable for the cold-start model. (3) They are\nas challenging as possible. (4) They cover a broad range of sub-domains. We ultimately collect a total\nof 3,995 query-verifier pairs, and employed GRPO (Shao et al., 2024) to update the model parameters.\nWe observe that using a large batch size and a high number of rollouts per query, along with off-policy\ntraining to improve sample efficiency, is beneficial to the training process. We have also addressed how\nto balance exploration and exploitation by controlling the model’s entropy to increase steadily or remain\n10\n\nTable 9: Examples of SFT data for thinking and non-thinking modes during the thinking mode fusion\nstage. For the thinking mode, the /think flag can be omitted since it represents the default behavior. This\nfeature has been implemented in the chat template1 supported by the Hugging Face’s tokenizer, where\nthe thinking mode can be disabled using an additional parameter enable thinking=False.\nThinking Mode\nNon-Thinking Mode\n<|im start|>user\n{query} /think<|im end|>\n<|im start|>assistant\n<think>\n{thinking content}\n</think>\n{response}<|im end|>\n<|im start|>user\n{query} /no think<|im end|>\n<|im start|>assistant\n<think>\n</think>\n{response}<|im end|>\nstable, which is crucial for maintaining stable training. As a result, we achieve consistent improvements\nin both training reward and validation performance over the course of a single RL run, without any\nmanual intervention on hyperparameters. For instance, the AIME’24 score of the Qwen3-235B-A22B\nmodel increases from 70.1 to 85.1 over a total of 170 RL training steps.\n4.3\nThinking Mode Fusion\nThe goal of the Thinking Mode Fusion stage is to integrate the “non-thinking” capabilities into the\npreviously developed “thinking” model. This approach allows developers to manage and control\nreasoning behaviors, while also reducing the cost and complexity of deploying separate models for\nthinking and non-thinking tasks. To achieve this, we conduct continual supervised fine-tuning (SFT)\non the Reasoning RL model and design a chat template to fuse the two modes. Moreover, we find that\nmodels capable of handling both modes proficiently perform consistently well under different thinking\nbudgets.\nConstruction of SFT data.\nThe SFT dataset combines both the “thinking” and “non-thinking” data.\nTo ensure that the performance of the Stage 2 model is not compromised by the additional SFT, the\n“thinking” data is generated via rejection sampling on Stage 1 queries using the Stage 2 model itself. The\n“non-thinking” data, on the other hand, is carefully curated to cover a diverse range of tasks, including\ncoding, mathematics, instruction-following, multilingual tasks, creative writing, question answering,\nand role-playing. Additionally, we employ automatically generated checklists for assessing the response\nquality of “non-thinking” data. To enhance the performance on tasks with low-resource languages, we\nparticularly increase the proportion of translation tasks.\nChat Template Design.\nTo better integrate the two modes and enable users to dynamically switch the\nmodel’s thinking process, we design chat templates for Qwen3, as shown in Table 9. Specifically, for\nsamples in thinking mode and non-thinking mode, we introduce /think and /no think flags in the user\nquery or system message, respectively. This allows the model to follow the user’s input and select the\nappropriate thinking mode accordingly. For non-thinking mode samples, we retain an empty thinking\nblock in the assistant’s response. This design ensures internal format consistency within the model and\nallows developers to prevent the model from engaging in thinking behavior by concatenating an empty\nthink block in the chat template. By default, the model operates in thinking mode; therefore, we add\nsome thinking mode training samples where the user queries do not include /think flags. For more\ncomplex multi-turn dialogs, we randomly insert multiple /think and /no think flags into users’ queries,\nwith the model response adhering to the last flag encountered.\nThinking Budget.\nAn additional advantage of Thinking Mode Fusion is that, once the model learns to\nrespond in both non-thinking and thinking modes, it naturally develops the ability to handle intermediate\ncases—generating responses based on incomplete thinking. This capability lays the foundation for\nimplementing budget control over the model’s thinking process. Specifically, when the length of the\nmodel’s thinking reaches a user-defined threshold, we manually halt the thinking process and insert\nthe stop-thinking instruction: “Considering the limited time by the user, I have to give the\nsolution based on the thinking directly now.\\n</think>.\\n\\n”. After this instruction is inserted,\nthe model proceeds to generate a final response based on its accumulated reasoning up to that point. It\nis worth noting that this ability is not explicitly trained but emerges naturally as a result of applying\nThinking Mode Fusion.\n11\n\n4.4\nGeneral RL\nThe General RL stage aims to broadly enhance the models’ capabilities and stability across diverse\nscenarios. To facilitate this, we have established a sophisticated reward system covering over 20 distinct\ntasks, each with customized scoring criteria. These tasks specifically target enhancements in the following\ncore capabilities:\n• Instruction Following: This capability ensures that models accurately interpret and follow user\ninstructions, including requirements related to content, format, length, and the use of structured\noutput, delivering responses that align with user expectations.\n• Format Following: In addition to explicit instructions, we expect the model to adhere to specific\nformatting conventions. For instance, it should respond appropriately to the /think and /\nno think flags by switching between thinking and non-thinking modes, and consistently use\ndesignated tokens (e.g., <think> and </think>) to separate the thinking and response parts in\nthe final output.\n• Preference Alignment: For open-ended queries, preference alignment focuses on improving the\nmodel’s helpfulness, engagement, and style, ultimately delivering a more natural and satisfying\nuser experience.\n• Agent Ability: This involves training the model to correctly invoke tools via designated interfaces.\nDuring the RL rollout, the model is allowed to perform complete multi-turn interaction cycles\nwith real environment execution feedback, thereby improving its performance and stability in\nlong-horizon decision-making tasks.\n• Abilities for Specialized Scenarios: In more specialized scenarios, we design tasks tailored to the\nspecific context. For example, in Retrieval-Augmented Generation (RAG) tasks, we incorporate\nreward signals to guide the model toward generating accurate and contextually appropriate\nresponses, thereby minimizing the risk of hallucination.\nTo provide feedback for the aforementioned tasks, we utilized three distinct types of rewards:\n(1) Rule-based Reward: The rule-based reward has been widely used in the reasoning RL stage,\nand is also useful for general tasks such as instruction following (Lambert et al., 2024) and format\nadherence. Well-designed rule-based rewards can assess the correctness of model outputs with\nhigh precision, preventing issues like reward hacking.\n(2) Model-based Reward with Reference Answer: In this approach, we provide a reference answer\nfor each query and prompt Qwen2.5-72B-Instruct to score the model’s response based on this\nreference. This method allows for more flexible handling of diverse tasks without requiring strict\nformatting, avoiding false negatives that can occur with purely rule-based rewards.\n(3) Model-based Reward without Reference Answer: Leveraging human preference data, we train\na reward model to assign scalar scores to model responses. This approach, which does not\ndepend on a reference answer, can handle a broader range of queries while effectively enhancing\nthe model’s engagement and helpfulness.\n4.5\nStrong-to-Weak Distillation\nThe Strong-to-Weak Distillation pipeline is specifically designed to optimize lightweight models, encom-\npassing 5 dense models (Qwen3-0.6B, 1.7B, 4B, 8B, and 14B) and one MoE model (Qwen3-30B-A3B). This\napproach enhances model performance while effectively imparting robust mode-switching capabilities.\nThe distillation process is divided into two primary phases:\n(1) Off-policy Distillation: At this initial phase, we combine the outputs of teacher models generated\nwith both /think and /no think modes for response distillation. This helps lightweight student\nmodels develop basic reasoning skills and the ability to switch between different modes of\nthinking, laying a solid foundation for the next on-policy training phase.\n(2) On-policy Distillation: In this phase, the student model generates on-policy sequences for\nfine-tuning. Specifically, prompts are sampled, and the student model produces responses in\neither /think or /no think mode. The student model is then fine-tuned by aligning its logits\nwith those of a teacher model (Qwen3-32B or Qwen3-235B-A22B) to minimize the KL divergence.\n4.6\nPost-training Evaluation\nTo comprehensively evaluate the quality of instruction-tuned models, we adopted automatic benchmarks\nto assess model performance under both thinking and non-thinking modes. These benchmarks are\n12\n\nTable 10: Multilingual benchmarks and the included languages. The languages are identified in IETF\nlanguage tags.\nBenchmark\n# Langs\nLanguages\nMulti-IF\n8\nen, es, fr, hi, it, pt, ru, zh\nINCLUDE\n44\nar, az, be, bg, bn, de, el, es, et, eu, fa, fi, fr, he, hi, hr, hu, hy, id, it, ja, ka,\nkk, ko, lt, mk, ml, ms, ne, nl, pl, pt, ru, sq, sr, ta, te, tl, tr, uk, ur, uz, vi, zh\nMMMLU\n14\nar, bn, de, en, es, fr, hi, id, it, ja, ko, pt, sw, zh\nMT-AIME2024\n55\naf, ar, bg, bn, ca, cs, cy, da, de, el, en, es, et, fa, fi, fr, gu, he, hi, hr, hu, id,\nit, ja, kn, ko, lt, lv, mk, ml, mr, ne, nl, no, pa, pl, pt, ro, ru, sk, sl, so, sq, sv,\nsw, ta, te, th, tl, tr, uk, ur, vi, zh-Hans, zh-Hant\nPolyMath\n18\nar, bn, de, en, es, fr, id, it, ja, ko, ms, pt, ru, sw, te, th, vi, zh\nMLogiQA\n10\nar, en, es, fr, ja, ko, pt, th, vi, zh\ncategorized into several dimensions:\n• General Tasks: We utilize benchmarks including MMLU-Redux (Gema et al., 2024), GPQA-\nDiamond (Rein et al., 2023), C-Eval (Huang et al., 2023), and LiveBench (2024-11-25) (White et al.,\n2024). For GPQA-Diamond, we sample 10 times for each query and report the averaged accuracy.\n• Alignment Tasks: To evaluate how well the model aligns with human preferences, we employ\na suite of specialized benchmarks. For instruction-following performance, we report the strict-\nprompt accuracy of IFEval (Zhou et al., 2023). To assess alignment with human preferences on\ngeneral topics, we utilize Arena-Hard (Li et al., 2024) and AlignBench v1.1 (Liu et al., 2023b). For\nwriting tasks, we rely on Creative Writing V3 (Paech, 2024) and WritingBench (Wu et al., 2025) to\nevaluate the model’s proficiency and creativity.\n• Math & Text Reasoning: For evaluating mathematical and logical reasoning skills, we employ\nhigh-level math benchmarks including MATH-500 (Lightman et al., 2023), AIME’24 and AIME’25\n(AIME, 2025), and text reasoning tasks including ZebraLogic (Lin et al., 2025) and AutoLogi\n(Zhu et al., 2025). For AIME problems, each year’s questions include Part I and Part II, totaling\n30 questions. For each question, we sample 64 times and take the average accuracy as the final\nscore.\n• Agent & Coding: To test the model’s proficiency in coding and agent-based tasks, we use BFCL\nv3 (Yan et al., 2024), LiveCodeBench (v5, 2024.10-2025.02) (Jain et al., 2024), and Codeforces\nRatings from CodeElo (Quan et al., 2025). For BFCL, all Qwen3 models are evaluated using the\nFC format, and yarn was used to deploy the models to a context length of 64k for Multi-Turn\nevaluation. Some baselines are derived from the BFCL leaderboard, taking the higher scores\nbetween FC and Prompt formats. For models not reported on the leaderboard, the Prompt\nformats are evaluated. For LiveCodeBench, for the non-thinking mode, we use the officially\nrecommended prompt, while for the thinking mode, we adjust the prompt template to allow\nthe model to think more freely, by removing the restriction You will not return anything\nexcept for the program. To evaluate the performance gap between models and competitive\nprogramming experts, we use CodeForces to calculate Elo ratings. In our benchmark, each\nproblem is solved by generating up to eight independent reasoning attempts.\n• Multilingual Tasks: For multilingual capabilities, we evaluate four kinds of tasks: instruction\nfollowing, knowledge, mathematics, and logical reasoning. Instruction following is assessed\nusing Multi-IF (He et al., 2024), which focuses on 8 key languages. Knowledge assessment\nconsisted of two types: regional knowledge evaluated through INCLUDE (Romanou et al.,\n2024), covering 44 languages, and general knowledge assessed with MMMLU (OpenAI, 2024)\nacross 14 languages, excluding the unoptimized Yoruba language; for these two benchmarks,\nwe sample only 10% of the original data to improve evaluation efficiency. The mathematics task\nemploy MT-AIME2024 (Son et al., 2025), encompassing 55 languages, and PolyMath (Wang et al.,\n2025), which includes 18 languages. Logical reasoning is evaluated using MlogiQA, covering 10\nlanguages, sourced from Zhang et al. (2024).\nFor all Qwen3 models in the thinking mode, we utilize a sampling temperature of 0.6, a top-p value\nof 0.95, and a top-k value of 20. Additionally, for Creative Writing v3 and WritingBench, we apply a\npresence penalty of 1.5 to encourage the generation of more diverse content. For Qwen3 models in the\nnon-thinking mode, we configure the sampling hyperparameters with temperature = 0.7, top-p = 0.8,\ntop-k = 20, and presence penalty = 1.5. For both the thinking and non-thinking modes, we set the max\noutput length to 32,768 tokens, except AIME’24 and AIME’25 where we extend this length to 38,912\ntokens to provide sufficient thinking space.\n13\n\nTable 11: Comparison among Qwen3-235B-A22B (Thinking) and other reasoning baselines. The\nhighest and second-best scores are shown in bold and underlined, respectively.\nOpenAI-o1\nDeepSeek-R1 Grok-3-Beta\n(Think)\nGemini2.5-Pro Qwen3-235B-A22B\nArchitecture\n-\nMoE\n-\n-\nMoE\n# Activated Params\n-\n37B\n-\n-\n22B\n# Total Params\n-\n671B\n-\n-\n235B\nGeneral\nTasks\nMMLU-Redux\n92.8\n92.9\n-\n93.7\n92.7\nGPQA-Diamond\n78.0\n71.5\n80.2\n84.0\n71.1\nC-Eval\n85.5\n91.8\n-\n82.9\n89.6\nLiveBench 2024-11-25\n75.7\n71.6\n-\n82.4\n77.1\nAlignment\nTasks\nIFEval strict prompt\n92.6\n83.3\n-\n89.5\n83.4\nArena-Hard\n92.1\n92.3\n-\n96.4\n95.6\nAlignBench v1.1\n8.86\n8.76\n-\n9.03\n8.94\nCreative Writing v3\n81.7\n85.5\n-\n86.0\n84.6\nWritingBench\n7.69\n7.71\n-\n8.09\n8.03\nMath & Text\nReasoning\nMATH-500\n96.4\n97.3\n98.8\n98.0\nAIME’24\n74.3\n79.8\n83.9\n92.0\n85.7\nAIME’25\n79.2\n70.0\n77.3\n86.7\n81.5\nZebraLogic\n81.0\n78.7\n-\n87.4\n80.3\nAutoLogi\n79.8\n86.1\n-\n85.4\n89.0\nAgent &\nCoding\nBFCL v3\n67.8\n56.9\n-\n62.9\n70.8\nLiveCodeBench v5\n63.9\n64.3\n70.6\n70.4\n70.7\nCodeForces (Rating / Percentile) 1891 / 96.7%\n2029 / 98.1%\n-\n2001 / 97.9%\n2056 / 98.2%\nMultilingual\nTasks\nMulti-IF\n48.8\n67.7\n-\n77.8\n71.9\nINCLUDE\n84.6\n82.7\n-\n85.1\n78.7\nMMMLU 14 languages\n88.4\n86.4\n-\n86.9\n84.3\nMT-AIME2024\n67.4\n73.5\n-\n76.9\n80.8\nPolyMath\n38.9\n47.1\n-\n52.2\n54.7\nMLogiQA\n75.5\n73.8\n-\n75.6\n77.1\nTable 12: Comparison among Qwen3-235B-A22B (Non-thinking) and other non-reasoning baselines.\nThe highest and second-best scores are shown in bold and underlined, respectively.\nGPT-4o\n-2024-11-20 DeepSeek-V3 Qwen2.5-72B\n-Instruct\nLLaMA-4\n-Maverick\nQwen3-235B-A22B\nArchitecture\n-\nMoE\nDense\nMoE\nMoE\n# Activated Params\n-\n37B\n72B\n17B\n22B\n# Total Params\n-\n671B\n72B\n402B\n235B\nGeneral\nTasks\nMMLU-Redux\n87.0\n89.1\n86.8\n91.8\n89.2\nGPQA-Diamond\n46.0\n59.1\n49.0\n69.8\n62.9\nC-Eval\n75.5\n86.5\n84.7\n83.5\n86.1\nLiveBench 2024-11-25\n52.2\n60.5\n51.4\n59.5\n62.5\nAlignment\nTasks\nIFEval strict prompt\n86.5\n86.1\n84.1\n86.7\n83.2\nArena-Hard\n85.3\n85.5\n81.2\n82.7\n96.1\nAlignBench v1.1\n8.42\n8.64\n7.89\n7.97\n8.91\nCreative Writing v3\n81.1\n74.0\n61.8\n61.3\n80.4\nWritingBench\n7.11\n6.49\n7.06\n5.46\n7.70\nMath & Text\nReasoning\nMATH-500\n77.2\n90.2\n83.6\n90.6\n91.2\nAIME’24\n11.1\n39.2\n18.9\n38.5\n40.1\nAIME’25\n7.6\n28.8\n15.0\n15.9\n24.7\nZebraLogic\n27.4\n42.1\n26.6\n40.0\n37.7\nAutoLogi\n65.9\n76.1\n66.1\n75.2\n83.3\nAgent &\nCoding\nBFCL v3\n72.5\n57.6\n63.4\n52.9\n68.0\nLiveCodeBench v5\n32.7\n33.1\n30.7\n37.2\n35.3\nCodeForces (Rating / Percentile) 864 / 35.4%\n1134 / 54.1%\n859 / 35.0%\n712 / 24.3%\n1387 / 75.7%\nMultilingual\nTasks\nMulti-IF\n65.6\n55.6\n65.3\n75.5\n70.2\nINCLUDE\n78.8\n76.7\n69.6\n80.9\n75.6\nMMMLU 14 languages\n80.3\n81.1\n76.9\n82.5\n79.8\nMT-AIME2024\n9.2\n20.9\n12.7\n27.0\n32.4\nPolyMath\n13.7\n20.4\n16.9\n26.1\n27.0\nMLogiQA\n57.4\n58.9\n59.3\n59.9\n67.6\n14\n\nSummary of Evaluation Results\nFrom the evaluation results, we summarize several key conclusions of\nthe finalized Qwen3 models as follows:\n(1) Our flagship model, Qwen3-235B-A22B, demonstrates the state-of-the-art overall performance\namong open-source models in both the thinking and non-thinking modes, surpassing strong\nbaselines such as DeepSeek-R1 and DeepSeek-V3. Qwen3-235B-A22B is also highly competitive\nto closed-source leading models, such as OpenAI-o1, Gemini2.5-Pro, and GPT-4o, showcasing its\nprofound reasoning capabilities and comprehensive general abilities.\n(2) Our flagship dense model, Qwen3-32B, outperforms our previous strongest reasoning model,\nQwQ-32B, in most of the benchmarks, and performs comparably to the closed-source OpenAI-o3-\nmini, indicating its compelling reasoning capabilities. Qwen3-32B is also remarkably performant\nin the non-thinking mode and surpasses our previous flagship non-reasoning dense model,\nQwen2.5-72B-Instruct.\n(3) Our lightweight models, including Qwen3-30B-A3B, Qwen3-14B, and other smaller dense ones,\npossess consistently superior performance to the open-source models with a close or larger\namount of parameters, proving the success of our Strong-to-Weak Distillation approach.\nThe detailed results are as follows.\nQwen3-235B-A22B\nFor our flagship model Qwen3-235B-A22B, we compare it with the leading reason-\ning and non-reasoning models. For the thinking mode, we take OpenAI-o1 (OpenAI, 2024), DeepSeek-R1\n(Guo et al., 2025), Grok-3-Beta (Think) (xAI, 2025), and Gemini2.5-Pro (DeepMind, 2025) as the reasoning\nbaselines. For the non-thinking mode, we take GPT-4o-2024-11-20 (OpenAI, 2024), DeepSeek-V3 (Liu\net al., 2024a), Qwen2.5-72B-Instruct (Yang et al., 2024b), and LLaMA-4-Maverick (Meta-AI, 2025) as the\nnon-reasoning baselines. We present the evaluation results in Table 11 and 12.\n(1) From Table 11, with only 60% activated and 35% total parameters, Qwen3-235B-A22B (Thinking)\noutperforms DeepSeek-R1 on 17/23 the benchmarks, particularly on the reasoning-demanded\ntasks (e.g., mathematics, agent, and coding), demonstrating the state-of-the-art reasoning capabil-\nities of Qwen3-235B-A22B among open-source models. Moreover, Qwen3-235B-A22B (Thinking)\nis also highly competitive to the closed-source OpenAI-o1, Grok-3-Beta (Think), and Gemini2.5-\nPro, substantially narrowing the gap in the reasoning capabilities between open-source and\nclose-source models.\n(2) From Table 12, Qwen3-235B-A22B (Non-thinking) exceeds the other leading open-source models,\nincluding DeepSeek-V3, LLaMA-4-Maverick, and our previous flagship model Qwen2.5-72B-\nInstruct, and also surpasses the closed-source GPT-4o-2024-11-20 in 18/23 the benchmarks,\nindicating its inherent strong capabilities even when not enhanced with the deliberate thinking\nprocess.\nQwen3-32B\nFor our flagship dense model, Qwen3-32B, we take DeepSeek-R1-Distill-Llama-70B, OpenAI-\no3-mini (medium), and our previous strongest reasoning model, QwQ-32B (Qwen Team, 2025), as the\nbaselines in the thinking mode. We also take GPT-4o-mini-2024-07-18, LLaMA-4-Scout, and our previ-\nous flagship model, Qwen2.5-72B-Instruct, as the baselines in the non-thinking mode. We present the\nevaluation results in Table 13 and 14.\n(1) From Table 13, Qwen3-32B (Thinking) outperforms QwQ-32B on 17/23 the benchmarks, making\nit the new state-of-the-art reasoning model at the sweet size of 32B. Moreover, Qwen3-32B (Think-\ning) also competes with the closed-source OpenAI-o3-mini (medium) with better alignment and\nmultilingual performance.\n(2) From Table 14, Qwen3-32B (Non-thinking) exhibits superior performance to all the baselines\non almost all the benchmarks. Particularly, Qwen3-32B (Non-thinking) performs on par with\nQwen2.5-72B-Instruct on the general tasks with significant advantages on the alignment, multi-\nlingual, and reasoning-related tasks, again proving the fundamental improvements of Qwen3\nover our previous Qwen2.5 series models.\nQwen3-30B-A3B & Qwen3-14B\nFor Qwen3-30B-A3B and Qwen3-14B, we compare them with DeepSeek-\nR1-Distill-Qwen-32B and QwQ-32B in the thinking mode, and Phi-4 (Abdin et al., 2024), Gemma-3-27B-IT\n(Team et al., 2025), and Qwen2.5-32B-Instruct in the non-thinking mode, respectively. We present the\nevaluation results in Table 15 and 16.\n(1) From Table 15, Qwen3-30B-A3B and Qwen3-14B (Thinking) are both highly competitive to\nQwQ-32B, especially on the reasoning-related benchmarks. It is noteworthy that Qwen3-30B-\nA3B achieves comparable performance to QwQ-32B with a smaller model size and less than\n15\n\nTable 13: Comparison among Qwen3-32B (Thinking) and other reasoning baselines. The highest and\nsecond-best scores are shown in bold and underlined, respectively.\nDeepSeek-R1\n-Distill-Llama-70B\nQwQ-32B\nOpenAI-o3-mini\n(medium)\nQwen3-32B\nArchitecture\nDense\nDense\n-\nDense\n# Activated Params\n70B\n32B\n-\n32B\n# Total Params\n70B\n32B\n-\n32B\nGeneral\nTasks\nMMLU-Redux\n89.3\n90.0\n90.0\n90.9\nGPQA-Diamond\n65.2\n65.6\n76.8\n68.4\nC-Eval\n71.8\n88.4\n75.1\n87.3\nLiveBench 2024-11-25\n54.5\n72.0\n70.0\n74.9\nAlignment\nTasks\nIFEval strict prompt\n79.3\n83.9\n91.5\n85.0\nArena-Hard\n60.6\n89.5\n89.0\n93.8\nAlignBench v1.1\n6.74\n8.70\n8.38\n8.72\nCreative Writing v3\n62.1\n82.4\n74.8\n81.0\nWritingBench\n6.08\n7.86\n7.52\n7.90\nMath & Text\nReasoning\nMATH-500\n94.5\n98.0\n98.0\n97.2\nAIME’24\n70.0\n79.5\n79.6\n81.4\nAIME’25\n56.3\n69.5\n74.8\n72.9\nZebraLogic\n71.3\n76.8\n88.9\n88.8\nAutoLogi\n83.5\n88.1\n86.3\n87.3\nAgent &\nCoding\nBFCL v3\n49.3\n66.4\n64.6\n70.3\nLiveCodeBench v5\n54.5\n62.7\n66.3\n65.7\nCodeForces (Rating / Percentile)\n1633 / 91.4%\n1982 / 97.7%\n2036 / 98.1%\n1977 / 97.7%\nMultilingual\nTasks\nMulti-IF\n57.6\n68.3\n48.4\n73.0\nINCLUDE\n62.1\n69.7\n73.1\n73.7\nMMMLU 14 languages\n69.6\n80.9\n79.3\n80.6\nMT-AIME2024\n29.3\n68.0\n73.9\n75.0\nPolyMath\n29.4\n45.9\n38.6\n47.4\nMLogiQA\n60.3\n75.5\n71.1\n76.3\nTable 14: Comparison among Qwen3-32B (Non-thinking) and other non-reasoning baselines. The\nhighest and second-best scores are shown in bold and underlined, respectively.\nGPT-4o-mini\n-2024-07-18\nLLaMA-4\n-Scout\nQwen2.5-72B\n-Instruct\nQwen3-32B\nArchitecture\n-\nMoE\nDense\nDense\n# Activated Params\n-\n17B\n72B\n32B\n# Total Params\n-\n109B\n72B\n32B\nGeneral\nTasks\nMMLU-Redux\n81.5\n86.3\n86.8\n85.7\nGPQA-Diamond\n40.2\n57.2\n49.0\n54.6\nC-Eval\n66.3\n78.2\n84.7\n83.3\nLiveBench 2024-11-25\n41.3\n47.6\n51.4\n59.8\nAlignment\nTasks\nIFEval strict prompt\n80.4\n84.7\n84.1\n83.2\nArena-Hard\n74.9\n70.5\n81.2\n92.8\nAlignBench v1.1\n7.81\n7.49\n7.89\n8.58\nCreative Writing v3\n70.3\n55.0\n61.8\n78.3\nWritingBench\n5.98\n5.49\n7.06\n7.54\nMath & Text\nReasoning\nMATH-500\n78.2\n82.6\n83.6\n88.6\nAIME’24\n8.1\n28.6\n18.9\n31.0\nAIME’25\n8.8\n10.0\n15.0\n20.2\nZebraLogic\n20.1\n24.2\n26.6\n29.2\nAutoLogi\n52.6\n56.8\n66.1\n78.5\nAgent &\nCoding\nBFCL v3\n64.0\n45.4\n63.4\n63.0\nLiveCodeBench v5\n27.9\n29.8\n30.7\n31.3\nCodeForces (Rating / Percentile)\n1113 / 52.6%\n981 / 43.7%\n859 / 35.0%\n1353 / 71.0%\nMultilingual\nTasks\nMulti-IF\n62.4\n64.2\n65.3\n70.7\nINCLUDE\n66.0\n74.1\n69.6\n70.9\nMMMLU 14 languages\n72.1\n77.5\n76.9\n76.5\nMT-AIME2024\n6.0\n19.1\n12.7\n24.1\nPolyMath\n12.0\n20.9\n16.9\n22.5\nMLogiQA\n42.6\n53.9\n59.3\n62.9\n16\n\nTable 15: Comparison among Qwen3-30B-A3B / Qwen3-14B (Thinking) and other reasoning baselines.\nThe highest and second-best scores are shown in bold and underlined, respectively.\nDeepSeek-R1\n-Distill-Qwen-32B\nQwQ-32B\nQwen3-14B\nQwen3-30B-A3B\nArchitecture\nDense\nDense\nDense\nMoE\n# Activated Params\n32B\n32B\n14B\n3B\n# Total Params\n32B\n32B\n14B\n30B\nGeneral\nTasks\nMMLU-Redux\n88.2\n90.0\n88.6\n89.5\nGPQA-Diamond\n62.1\n65.6\n64.0\n65.8\nC-Eval\n82.2\n88.4\n86.2\n86.6\nLiveBench 2024-11-25\n45.6\n72.0\n71.3\n74.3\nAlignment\nTasks\nIFEval strict prompt\n72.5\n83.9\n85.4\n86.5\nArena-Hard\n60.8\n89.5\n91.7\n91.0\nAlignBench v1.1\n7.25\n8.70\n8.56\n8.70\nCreative Writing v3\n55.0\n82.4\n80.3\n79.1\nWritingBench\n6.13\n7.86\n7.80\n7.70\nMath & Text\nReasoning\nMATH-500\n94.3\n98.0\n96.8\n98.0\nAIME’24\n72.6\n79.5\n79.3\n80.4\nAIME’25\n49.6\n69.5\n70.4\n70.9\nZebraLogic\n69.6\n76.8\n88.5\n89.5\nAutoLogi\n74.6\n88.1\n89.2\n88.7\nAgent &\nCoding\nBFCL v3\n53.5\n66.4\n70.4\n69.1\nLiveCodeBench v5\n54.5\n62.7\n63.5\n62.6\nCodeForces (Rating / Percentile)\n1691 / 93.4%\n1982 / 97.7%\n1766 / 95.3%\n1974 / 97.7%\nMultilingual\nTasks\nMulti-IF\n31.3\n68.3\n74.8\n72.2\nINCLUDE\n68.0\n69.7\n71.7\n71.9\nMMMLU 14 languages\n78.6\n80.9\n77.9\n78.4\nMT-AIME2024\n44.6\n68.0\n73.3\n73.9\nPolyMath\n35.1\n45.9\n45.8\n46.1\nMLogiQA\n63.3\n75.5\n71.1\n70.1\nTable 16: Comparison among Qwen3-30B-A3B / Qwen3-14B (Non-thinking) and other non-reasoning\nbaselines. The highest and second-best scores are shown in bold and underlined, respectively.\nPhi-4\nGemma-3\n-27B-IT\nQwen2.5-32B\n-Instruct\nQwen3-14B\nQwen3-30B-A3B\nArchitecture\nDense\nDense\nDense\nDense\nMoE\n# Activated Params\n14B\n27B\n32B\n14B\n3B\n# Total Params\n14B\n27B\n32B\n14B\n30B\nGeneral\nTasks\nMMLU-Redux\n85.3\n82.6\n83.9\n82.0\n84.1\nGPQA-Diamond\n56.1\n42.4\n49.5\n54.8\n54.8\nC-Eval\n66.9\n66.6\n80.6\n81.0\n82.9\nLiveBench 2024-11-25\n41.6\n49.2\n50.0\n59.6\n59.4\nAlignment\nTasks\nIFEval strict prompt\n62.1\n80.6\n79.5\n84.8\n83.7\nArena-Hard\n75.4\n86.8\n74.5\n86.3\n88.0\nAlignBench v1.1\n7.61\n7.80\n7.71\n8.52\n8.55\nCreative Writing v3\n51.2\n82.0\n54.6\n73.1\n68.1\nWritingBench\n5.73\n7.22\n5.90\n7.24\n7.22\nMath & Text\nReasoning\nMATH-500\n80.8\n90.0\n84.6\n90.0\n89.8\nAIME’24\n22.9\n32.6\n18.8\n31.7\n32.8\nAIME’25\n17.3\n24.0\n12.8\n23.3\n21.6\nZebraLogic\n32.3\n24.6\n26.1\n33.0\n33.2\nAutoLogi\n66.2\n64.2\n65.5\n82.0\n81.5\nAgent &\nCoding\nBFCL v3\n47.0\n59.1\n62.8\n61.5\n58.6\nLiveCodeBench v5\n25.2\n26.9\n26.4\n29.0\n29.8\nCodeForces (Rating / Percentile) 1280 / 65.3% 1063 / 49.3%\n903 / 38.2%\n1200 / 58.6%\n1267 / 64.1%\nMultilingual\nTasks\nMulti-IF\n49.5\n69.8\n63.2\n72.9\n70.8\nINCLUDE\n65.3\n71.4\n67.5\n67.8\n67.8\nMMMLU 14 languages\n74.7\n76.1\n74.2\n72.6\n73.8\nMT-AIME2024\n13.1\n23.0\n15.3\n23.2\n24.6\nPolyMath\n17.4\n20.3\n18.3\n22.0\n23.3\nMLogiQA\n53.1\n58.5\n58.0\n58.9\n53.3\n17\n\nTable 17: Comparison among Qwen3-8B / Qwen3-4B (Thinking) and other reasoning baselines. The\nhighest and second-best scores are shown in bold and underlined, respectively.\nDeepSeek-R1\n-Distill-Qwen-14B\nDeepSeek-R1\n-Distill-Qwen-32B\nQwen3-4B\nQwen3-8B\nArchitecture\nDense\nDense\nDense\nDense\n# Activated Params\n14B\n32B\n4B\n8B\n# Total Params\n14B\n32B\n4B\n8B\nGeneral\nTasks\nMMLU-Redux\n84.1\n88.2\n83.7\n87.5\nGPQA-Diamond\n59.1\n62.1\n55.9\n62.0\nC-Eval\n78.1\n82.2\n77.5\n83.4\nLiveBench 2024-11-25\n52.3\n45.6\n63.6\n67.1\nAlignment\nTasks\nIFEval strict prompt\n72.6\n72.5\n81.9\n85.0\nArena-Hard\n48.0\n60.8\n76.6\n85.8\nAlignBench v1.1\n7.43\n7.25\n8.30\n8.46\nCreative Writing v3\n54.2\n55.0\n61.1\n75.0\nWritingBench\n6.03\n6.13\n7.35\n7.59\nMath & Text\nReasoning\nMATH-500\n93.9\n94.3\n97.0\n97.4\nAIME’24\n69.7\n72.6\n73.8\n76.0\nAIME’25\n44.5\n49.6\n65.6\n67.3\nZebraLogic\n59.1\n69.6\n81.0\n84.8\nAutoLogi\n78.6\n74.6\n87.9\n89.1\nAgent &\nCoding\nBFCL v3\n49.5\n53.5\n65.9\n68.1\nLiveCodeBench v5\n45.5\n54.5\n54.2\n57.5\nCodeForces (Rating / Percentile)\n1574 / 89.1%\n1691 / 93.4%\n1671 / 92.8%\n1785 / 95.6%\nMultilingual\nTasks\nMulti-IF\n29.8\n31.3\n66.3\n71.2\nINCLUDE\n59.7\n68.0\n61.8\n67.8\nMMMLU 14 languages\n73.8\n78.6\n69.8\n74.4\nMT-AIME2024\n33.7\n44.6\n60.7\n65.4\nPolyMath\n28.6\n35.1\n40.0\n42.7\nMLogiQA\n53.6\n63.3\n65.9\n69.0\nTable 18: Comparison among Qwen3-8B / Qwen3-4B (Non-thinking) and other non-reasoning baselines.\nThe highest and second-best scores are shown in bold and underlined, respectively.\nLLaMA-3.1-8B\n-Instruct\nGemma-3\n-12B-IT\nQwen2.5-7B\n-Instruct\nQwen2.5-14B\n-Instruct\nQwen3-4B\nQwen3-8B\nArchitecture\nDense\nDense\nDense\nDense\nDense\nDense\n# Activated Params\n8B\n12B\n7B\n14B\n4B\n8B\n# Total Params\n8B\n12B\n7B\n14B\n4B\n8B\nGeneral\nTasks\nMMLU-Redux\n61.7\n77.8\n75.4\n80.0\n77.3\n79.5\nGPQA-Diamond\n32.8\n40.9\n36.4\n45.5\n41.7\n39.3\nC-Eval\n52.0\n61.1\n76.2\n78.0\n72.2\n77.9\nLiveBench 2024-11-25\n26.0\n43.7\n34.9\n42.2\n48.4\n53.5\nAlignment\nTasks\nIFEval strict prompt\n75.0\n80.2\n71.2\n81.0\n81.2\n83.0\nArena-Hard\n30.1\n82.6\n52.0\n68.3\n66.2\n79.6\nAlignBench v1.1\n6.01\n7.77\n7.27\n7.67\n8.10\n8.38\nCreative Writing v3\n52.8\n79.9\n49.8\n55.8\n53.6\n64.5\nWritingBench\n4.57\n7.05\n5.82\n5.93\n6.85\n7.15\nMath & Text\nReasoning\nMATH-500\n54.8\n85.6\n77.6\n83.4\n84.8\n87.4\nAIME’24\n6.3\n22.4\n9.1\n15.2\n25.0\n29.1\nAIME’25\n2.7\n18.8\n12.1\n13.6\n19.1\n20.9\nZebraLogic\n12.8\n17.8\n12.0\n19.7\n35.2\n26.7\nAutoLogi\n30.9\n58.9\n42.9\n57.4\n76.3\n76.5\nAgent &\nCoding\nBFCL v3\n49.6\n50.6\n55.8\n58.7\n57.6\n60.2\nLiveCodeBench v5\n10.8\n25.7\n14.4\n21.9\n21.3\n22.8\nCodeForces (Rating / Percentile)\n473 / 14.9%\n462 / 14.7%\n191 / 0.0%\n904 / 38.3%\n842 / 33.7% 1110 / 52.4%\nMultilingual\nTasks\nMulti-IF\n52.1\n65.6\n47.7\n55.5\n61.3\n69.2\nINCLUDE\n34.0\n65.3\n53.6\n63.5\n53.8\n62.5\nMMMLU 14 languages\n44.4\n70.0\n61.4\n70.3\n61.7\n66.9\nMT-AIME2024\n0.4\n16.7\n5.5\n8.5\n13.9\n16.6\nPolyMath\n5.8\n17.6\n11.9\n15.0\n16.6\n18.8\nMLogiQA\n41.9\n54.5\n49.5\n51.3\n49.9\n51.4\n18\n\nTable 19: Comparison among Qwen3-1.7B / Qwen3-0.6B (Thinking) and other reasoning baselines.\nThe highest and second-best scores are shown in bold and underlined, respectively.\nDeepSeek-R1\n-Distill-Qwen-1.5B\nDeepSeek-R1\n-Distill-Llama-8B\nQwen3-0.6B\nQwen3-1.7B\nArchitecture\nDense\nDense\nDense\nDense\n# Activated Params\n1.5B\n8B\n0.6B\n1.7B\n# Total Params\n1.5B\n8B\n0.6B\n1.7B\nGeneral\nTasks\nMMLU-Redux\n45.4\n66.4\n55.6\n73.9\nGPQA-Diamond\n33.8\n49.0\n27.9\n40.1\nC-Eval\n27.1\n50.4\n50.4\n68.1\nLiveBench 2024-11-25\n24.9\n40.6\n30.3\n51.1\nAlignment\nTasks\nIFEval strict prompt\n39.9\n59.0\n59.2\n72.5\nArena-Hard\n4.5\n17.6\n8.5\n43.1\nAlignBench v1.1\n5.00\n6.24\n6.10\n7.60\nCreative Writing v3\n16.4\n51.1\n30.6\n48.0\nWritingBench\n4.03\n5.42\n5.61\n7.02\nMath & Text\nReasoning\nMATH-500\n83.9\n89.1\n77.6\n93.4\nAIME’24\n28.9\n50.4\n10.7\n48.3\nAIME’25\n22.8\n27.8\n15.1\n36.8\nZebraLogic\n4.9\n37.1\n30.3\n63.2\nAutoLogi\n19.1\n63.4\n61.6\n83.2\nAgent &\nCoding\nBFCL v3\n14.0\n21.5\n46.4\n56.6\nLiveCodeBench v5\n13.2\n42.5\n12.3\n33.2\nMultilingual\nTasks\nMulti-IF\n13.3\n27.0\n36.1\n51.2\nINCLUDE\n21.9\n34.5\n35.9\n51.8\nMMMLU 14 languages\n27.3\n40.1\n43.1\n59.1\nMT-AIME2024\n12.4\n13.2\n7.8\n36.1\nPolyMath\n14.5\n10.8\n11.4\n25.2\nMLogiQA\n29.0\n32.8\n40.9\n56.0\nTable 20: Comparison among Qwen3-1.7B / Qwen3-0.6B (Non-thinking) and other non-reasoning\nbaselines. The highest and second-best scores are shown in bold and underlined, respectively.\nGemma-3\n-1B-IT\nPhi-4-mini Qwen2.5-1.5B\n-Instruct\nQwen2.5-3B\n-Instruct\nQwen3-0.6B Qwen3-1.7B\nArchitecture\nDense\nDense\nDense\nDense\nDense\nDense\n# Activated Params\n1.0B\n3.8B\n1.5B\n3.1B\n0.6B\n1.7B\n# Total Params\n1.0B\n3.8B\n1.5B\n3.1B\n0.6B\n1.7B\nGeneral\nTasks\nMMLU-Redux\n33.3\n67.9\n50.7\n64.4\n44.6\n64.4\nGPQA-Diamond\n19.2\n25.2\n29.8\n30.3\n22.9\n28.6\nC-Eval\n28.5\n40.0\n53.3\n68.2\n42.6\n61.0\nLiveBench 2024-11-25\n14.4\n25.3\n18.0\n23.8\n21.8\n35.6\nAlignment\nTasks\nIFEval strict prompt\n54.5\n68.6\n42.5\n58.2\n54.5\n68.2\nArena-Hard\n17.8\n32.8\n9.0\n23.7\n6.5\n36.9\nAlignBench v1.1\n5.3\n6.00\n5.60\n6.49\n5.60\n7.20\nCreative Writing v3\n52.8\n10.3\n31.5\n42.8\n28.4\n43.6\nWritingBench\n5.18\n4.05\n4.67\n5.55\n5.13\n6.54\nMath & Text\nReasoning\nMATH-500\n46.4\n67.6\n55.0\n67.2\n55.2\n73.0\nAIME’24\n0.9\n8.1\n0.9\n6.7\n3.4\n13.4\nAIME’25\n0.8\n5.3\n0.4\n4.2\n2.6\n9.8\nZebraLogic\n1.9\n2.7\n3.4\n4.8\n4.2\n12.8\nAutoLogi\n16.4\n28.8\n22.5\n29.9\n37.4\n59.8\nAgent &\nCoding\nBFCL v3\n16.3\n31.3\n47.8\n50.4\n44.1\n52.2\nLiveCodeBench v5\n1.8\n10.4\n5.3\n9.2\n3.6\n11.6\nMultilingual\nTasks\nMulti-IF\n32.8\n40.5\n20.2\n32.3\n33.3\n44.7\nINCLUDE\n32.7\n43.8\n33.1\n43.8\n34.4\n42.6\nMMMLU 14 languages\n32.5\n51.4\n40.4\n51.8\n37.1\n48.3\nMT-AIME2024\n0.2\n0.9\n0.7\n1.6\n1.5\n4.9\nPolyMath\n3.5\n6.7\n5.0\n7.3\n4.6\n10.3\nMLogiQA\n31.8\n39.5\n40.9\n39.5\n37.3\n41.1\n19\n\n1/10 activated parameters, demonstrating the effectiveness of our Strong-to-Weak Distillation\napproach in endowing lightweight models with profound reasoning capabilities.\n(2) From Table 16, Qwen3-30B-A3B and Qwen3-14B (Non-thinking) surpass the non-reasoning\nbaselines in most of the benchmarks. They exceed our previous Qwen2.5-32B-Instruct model\nwith significantly fewer activated and total parameters, allowing for more efficient and cost-\neffective performance.\nQwen3-8B / 4B / 1.7B / 0.6B\nFor Qwen3-8B and Qwen3-4B, we compare them with DeepSeek-R1-Distill-\nQwen-14B and DeepSeek-R1-Distill-Qwen-32B in the thinking mode, and LLaMA-3.1-8B-Instruct (Dubey\net al., 2024), Gemma-3-12B-IT (Team et al., 2025), Qwen2.5-7B-Instruct, and Qwen2.5-14B-Instruct in the\nnon-thinking mode, respectively. For Qwen3-1.7B and Qwen3-0.6B, we compare them with DeepSeek-\nR1-Distill-Qwen-1.5B and DeepSeek-R1-Distill-Llama-8B in the thinking mode, and Gemma-3-1B-IT,\nPhi-4-mini, Qwen2.5-1.5B-Instruct, and Qwen2.5-3B-Instruct in the non-thinking mode, respectively. We\npresent the evaluation results of Qwen3-8B and Qwen3-4B in Table 17 and 18 and those of Qwen3-1.7B\nand Qwen3-0.6B in Table 19 and 20, respectively. Overall, these edge-side models exhibit impressive\nperformance and outperform baselines even with more parameters, including our previous Qwen2.5\nmodels, in either the thinking or the non-thinking mode. These results, once again, demonstrate the\nefficacy of our Strong-to-Weak Distillation approach, making it possible for us to build the lightweight\nQwen3 models with remarkably reduced costs and efforts.\n4.7\nDiscussion\nThe Effectiveness of Thinking Budget\nTo verify that Qwen3 can enhance its intelligence level by\nleveraging an increased thinking budget, we adjust the allocated thinking budget on four benchmarks\nacross Mathematics, Coding, and STEM domains. The resulting scaling curves are presented in Figure 2,\nQwen3 demonstrates scalable and smooth performance improvements correlated to the allocated thinking\nbudget. Moreover, we observe that if we further extend the output length beyond 32K, the model’s\nperformance is expected to improve further in the future. We leave this exploration as future work.\nFigure 2: Performance of Qwen3-235B-A22B with respect to the thinking budget.\nThe Effectiveness and Efficiency of On-Policy Distillation\nWe evaluate the effectiveness and efficiency\nof on-policy distillation by comparing the performance and computational cost—measured in GPU\nhours—after undergoing distillation versus direct reinforcement learning, both starting from the same\noff-policy distilled 8B checkpoint. For simplicity, we focus solely on math and code-related queries in\n20\n\nthis comparison. The results, summarized in Table 21, show that distillation achieves significantly better\nperformance than reinforcement learning while requiring approximately only 1/10 of the GPU hours.\nFurthermore, distillation from teacher logits enables the student model to expand its exploration space\nand enhance its reasoning potential, as evidenced by the improved pass@64 scores on the AIME’24\nand AIME’25 benchmarks after distillation, compared to the initial checkpoint. In contrast, reinforce-\nment learning does not lead to any improvement in pass@64 scores. These observations highlight the\nadvantages of leveraging a stronger teacher model in guiding student model learning.\nTable 21: Comparison of reinforcement learning and on-policy distillation on Qwen3-8B. Numbers in\nparentheses indicate pass@64 scores.\nMethod\nAIME’24\nAIME’25\nMATH500 LiveCodeBench\nv5\nMMLU\n-Redux\nGPQA\n-Diamond\nGPU\nHours\nOff-policy Distillation\n55.0 (90.0) 42.8 (83.3)\n92.4\n42.0\n86.4\n55.6\n-\n+ Reinforcement Learning 67.6 (90.0) 55.5 (83.3)\n94.8\n52.9\n86.9\n61.3\n17,920\n+ On-policy Distillation\n74.4 (93.3) 65.5 (86.7)\n97.0\n60.3\n88.3\n63.3\n1,800\nThe Effects of Thinking Mode Fusion and General RL\nTo evaluate the effectiveness of Thinking Mode\nFusion and General Reinforcement Learning (RL) during the post-training, we conduct evaluations on\nvarious stages of the Qwen-32B model. In addition to the datasets mentioned earlier, we introduce several\nin-house benchmarks to monitor other capabilities. These benchmarks include:\n• CounterFactQA: Contains counterfactual questions where the model needs to identify that the\nquestions are not factual and avoid generating hallucinatory answers.\n• LengthCtrl: Includes creative writing tasks with length requirements; the final score is based on\nthe difference between the generated content length and the target length.\n• ThinkFollow: Involves multi-turn dialogues with randomly inserted /think and /no think\nflags to test whether the model can correctly switch thinking modes based on user queries.\n• ToolUse: Evaluates the stability of the model in single-turn, multi-turn, and multi-step tool calling\nprocesses. The score includes accuracy in intent recognition, format accuracy, and parameter\naccuracy during the tool calling process.\nTable 22: Performance of Qwen3-32B after Reasoning RL (Stage 2), Thinking Mode Fusion (Stage 3), and\nGeneral RL (Stage 4). Benchmarks with * are in-house datasets.\nStage 2\nReasoning RL\nStage 3\nThinking Mode Fusion\nStage 4\nGeneral RL\nBenchmark\nThinking\nThinking\nNon-Thinking\nThinking\nNon-Thinking\nGeneral\nTasks\nLiveBench 2024-11-25\n68.6\n70.9+2.3\n57.1\n74.9+4.0\n59.8+2.8\nArena-Hard\n86.8\n89.4+2.6\n88.5\n93.8+4.4\n92.8+4.3\nCounterFactQA*\n50.4\n61.3+10.9\n64.3\n68.1+6.8\n66.4+2.1\nInstruction\n& Format\nFollowing\nIFEval strict prompt\n73.0\n78.4+5.4\n78.4\n85.0+6.6\n83.2+4.8\nMulti-IF\n61.4\n64.6+3.2\n65.2\n73.0+8.4\n70.7+5.5\nLengthCtrl*\n62.6\n70.6+8.0\n84.9\n73.5+2.9\n87.3+2.4\nThinkFollow*\n-\n88.7\n98.9+10.2\nAgent\nBFCL v3\n69.0\n68.4-0.6\n61.5\n70.3+1.9\n63.0+1.5\nToolUse*\n63.3\n70.4+7.1\n73.2\n85.5+15.1\n86.5+13.3\nKnowledge &\nSTEM\nMMLU-Redux\n91.4\n91.0-0.4\n86.7\n90.9-0.1\n85.7-1.0\nGPQA-Diamond\n68.8\n69.0+0.2\n50.4\n68.4-0.6\n54.6+4.3\nMath &\nCoding\nAIME’24\n83.8\n81.9-1.9\n28.5\n81.4-0.5\n31.0+2.5\nLiveCodeBench v5\n68.4\n67.2-1.2\n31.1\n65.7-1.5\n31.3+0.2\nThe results are shown in Table 22, where we can draw the following conclusions:\n(1) Stage 3 integrates the non-thinking mode into the model, which already possesses thinking\ncapabilities after the first two stages of training. The ThinkFollow benchmark score of 88.7\nindicates that the model has developed an initial ability to switch between modes, though it still\noccasionally makes errors. Stage 3 also enhances the model’s general and instruction-following\ncapabilities in thinking mode, with CounterFactQA improving by 10.9 points and LengthCtrl by\n8.0 points.\n21\n\n(2) Stage 4 further strengthens the model’s general, instruction-following, and agent capabilities\nin both thinking and non-thinking modes. Notably, the ThinkFollow score improves to 98.9,\nensuring accurate mode switching.\n(3) For Knowledge, STEM, Math, and Coding tasks, Thinking Mode Fusion and General RL do\nnot bring significant improvements. In contrast, for challenging tasks like AIME’24 and Live-\nCodeBench, the performance in thinking mode actually decreases after these two training stages.\nWe conjecture this degradation is due to the model being trained on a broader range of general\ntasks, which may compromise its specialized capabilities in handling complex problems. During\nthe development of Qwen3, we choose to accept this performance trade-off to enhance the\nmodel’s overall versatility.\n5\nConclusion\nIn this technical report, we introduce Qwen3, the latest version of the Qwen series. Qwen3 features\nboth thinking mode and non-thinking mode, allowing users to dynamically manage the number of\ntokens used for complex thinking tasks. The model was pre-trained on an extensive dataset containing\n36 trillion tokens, enabling it to understand and generate text in 119 languages and dialects. Through a\nseries of comprehensive evaluations, Qwen3 has shown strong performance across a range of standard\nbenchmarks for both pre-trained and post-trained models, including tasks related to code generation,\nmathematics, reasoning, and agents.\nIn the near future, our research will focus on several key areas. We will continue to scale up pretraining by\nusing data that is both higher in quality and more diverse in content. At the same time, we will work on\nimproving model architecture and training methods for the purposes of effective compression, scaling to\nextremely long contexts, etc. In addition, we plan to increase computational resources for reinforcement\nlearning, with a particular emphasis on agent-based RL systems that learn from environmental feedback.\nThis will allow us to build agents capable of tackling complex tasks that require inference time scaling.\n6\nAuthors\nCore Contributors: An Yang, Anfeng Li, Baosong Yang, Beichen Zhang, Binyuan Hui, Bo Zheng, Bowen\nYu, Chang Gao, Chengen Huang, Chenxu Lv, Chujie Zheng, Dayiheng Liu, Fan Zhou, Fei Huang, Feng\nHu, Hao Ge, Haoran Wei, Huan Lin, Jialong Tang, Jian Yang, Jianhong Tu, Jianwei Zhang, Jianxin Yang,\nJiaxi Yang, Jing Zhou, Jingren Zhou, Junyang Lin, Kai Dang, Keqin Bao, Kexin Yang, Le Yu, Lianghao\nDeng, Mei Li, Mingfeng Xue, Mingze Li, Pei Zhang, Peng Wang, Qin Zhu, Rui Men, Ruize Gao, Shixuan\nLiu, Shuang Luo, Tianhao Li, Tianyi Tang, Wenbiao Yin, Xingzhang Ren, Xinyu Wang, Xinyu Zhang,\nXuancheng Ren, Yang Fan, Yang Su, Yichang Zhang, Yinger Zhang, Yu Wan, Yuqiong Liu, Zekun Wang,\nZeyu Cui, Zhenru Zhang, Zhipeng Zhou, Zihan Qiu\nContributors: Bei Chen, Biao Sun, Bin Luo, Bin Zhang, Binghai Wang, Bowen Ping, Boyi Deng, Chang\nSi, Chaojie Yang, Chen Cheng, Chenfei Wu, Chengpeng Li, Chengyuan Li, Fan Hong, Guobin Zhao,\nHang Zhang, Hangrui Hu, Hanyu Zhao, Hao Lin, Hao Xiang, Haoyan Huang, Hongkun Hao, Humen\nZhong, Jialin Wang, Jiandong Jiang, Jianqiang Wan, Jianyuan Zeng, Jiawei Chen, Jie Zhang, Jin Xu, Jinkai\nWang, Jinyang Zhang, Jinzheng He, Jun Tang, Kai Zhang, Ke Yi, Keming Lu, Keqin Chen, Langshi Chen,\nLe Jiang, Lei Zhang, Linjuan Wu, Man Yuan, Mingkun Yang, Minmin Sun, Mouxiang Chen, Na Ni,\nNuo Chen, Peng Liu, Peng Wang, Peng Zhu, Pengcheng Zhang, Pengfei Wang, Qiaoyu Tang, Qing Fu,\nQiuyue Wang, Rong Zhang, Rui Hu, Runji Lin, Shen Huang, Shuai Bai, Shutong Jiang, Sibo Song, Siqi\nZhang, Song Chen, Tao He, Ting He, Tingfeng Hui, Wei Ding, Wei Liao, Wei Lin, Wei Zhang, Weijia\nXu, Wenbin Ge, Wenmeng Zhou, Wenyuan Yu, Xianyan Jia, Xianzhong Shi, Xiaodong Deng, Xiaoming\nHuang, Xiaoyuan Li, Ximing Zhou, Xinyao Niu, Xipin Wei, Xuejing Liu, Yang Liu, Yang Yao, Yang Zhang,\nYanpeng Li, Yantao Liu, Yidan Zhang, Yikai Zhu, Yiming Wang, Yiwen Hu, Yong Jiang, Yong Li, Yongan\nYue, Yu Guan, Yuanzhi Zhu, Yunfei Chu, Yunlong Feng, Yuxin Zhou, Yuxuan Cai, Zeyao Ma, Zhaohai Li,\nZheng Li, Zhengyang Tang, Zheren Fu, Zhi Li, Zhibo Yang, Zhifang Guo, Zhipeng Zhang, Zhiying Xu,\nZhiyu Yin, Zhongshen Zeng, Zile Qiao, Ziye Meng, Zongmeng Zhang\n22\n\nA\nAppendix\nA.1\nAdditional Evaluation Results\nA.1.1\nLong-Context Ability\nTable 23: Performance of Qwen3 Models on the RULER benchmark.\nModel\nRULER\nAvg.\n4K\n8K\n16K\n32K\n64K\n128K\nQwen2.5-7B-Instruct\n85.4\n96.7\n95.1\n93.7\n89.4\n82.3\n55.1\nQwen2.5-14B-Instruct\n91.4\n97.7\n96.8\n95.9\n93.4\n86.7\n78.1\nQwen2.5-32B-Instruct\n92.9\n96.9\n97.1\n95.5\n95.5\n90.3\n82.0\nQwen2.5-72B-Instruct\n95.1\n97.7\n97.2\n97.7\n96.5\n93.0\n88.4\nNon-thinking\nMode\nQwen3-4B\n85.2\n95.1\n93.6\n91.0\n87.8\n77.8\n66.0\nQwen3-8B\n89.1\n96.3\n96.0\n91.8\n91.2\n82.1\n77.4\nQwen3-14B\n94.6\n98.0\n97.8\n96.4\n96.1\n94.0\n85.1\nQwen3-32B\n93.7\n98.4\n96.0\n96.2\n94.4\n91.8\n85.6\nQwen3-30B-A3B\n91.6\n96.5\n97.0\n95.3\n92.4\n89.1\n79.2\nQwen3-235B-A22B\n95.0\n97.7\n97.2\n96.4\n95.1\n93.3\n90.6\nThinking\nMode\nQwen3-4B\n83.5\n92.7\n88.7\n86.5\n83.2\n83.0\n67.2\nQwen3-8B\n84.4\n94.7\n94.4\n86.1\n80.8\n78.3\n72.0\nQwen3-14B\n90.1\n95.4\n93.6\n89.8\n91.9\n90.6\n79.0\nQwen3-32B\n91.0\n94.7\n93.7\n91.6\n92.5\n90.0\n83.5\nQwen3-30B-A3B\n86.6\n94.1\n92.7\n89.0\n86.6\n82.1\n75.0\nQwen3-235B-A22B\n92.2\n95.1\n94.8\n93.0\n92.3\n92.0\n86.0\nFor evaluating long-context processing capabilities, we report the results on the RULER benchmark (Hsieh\net al., 2024) in Table 23. To enable length extrapolation, we utilize YARN (Peng et al., 2023) with a\nscaling factor=4. In thinking mode, we set the thinking budget to 8192 tokens to mitigate overly\nverbose reasoning on the extremely long inputs.\nThe results show that:\n1. In non-thinking mode, Qwen3 outperforms Qwen2.5 models of a similar size in long-context\nprocessing tasks.\n2. In thinking mode, the model’s performance slightly degrades. We hypothesize that the thinking\ncontent does not provide significant benefits for these retrieval tasks, which do not rely on\nreasoning and may instead interfere with the retrieval process. We are committed to enhancing\nthe long-context capability in the thinking mode in future versions.\nA.1.2\nMultilingual Ability\nTable 24-35 presents the detailed benchmark scores across various languages, including Spanish, French,\nPortuguese, Italian, Arabic, Japanese, Korean, Indonesian, Russian, Vietnamese, German, and Thai. The\nresults of these tables demonstrate that the Qwen3 series models achieve competitive performance across\nall evaluated benchmarks, showcasing their strong multilingual capabilities.\nTo evaluate the performance of Qwen3 across a broader range of languages, we utilize Belebele (Bandarkar\net al., 2023), a benchmark for natural language understanding. We conduct evaluations on 80 supported\nlanguages from the benchmark, excluding 42 unoptimized languages, as shown in Table 36 (organized\nby language family). The performance comparison between Qwen3 and other baseline models on\nthe Belebele benchmark is presented in Table 37. The results show that Qwen3 achieves comparable\nperformance to similarly-sized Gemma models while outperforming Qwen2.5 significantly.\n23\n\nTable 24: Benchmark scores for language: Spanish (es). The highest and second-best scores are shown\nin bold and underlined, respectively.\nModel\nMulti-IF MLogiQA INCLUDE MMMLU MT-AIME24 PolyMath Average\nThinking\nMode\nGemini2.5-Pro\n80.1\n70.0\n96.4\n88.7\n90.0\n54.4\n79.9\nQwQ-32B\n70.0\n75.0\n81.8\n84.5\n76.7\n52.2\n73.4\nQwen3-235B-A22B\n74.2\n76.2\n89.1\n86.7\n86.7\n57.3\n78.4\nQwen3-32B\n74.7\n68.8\n90.9\n82.8\n76.7\n51.8\n74.3\nQwen3-30B-A3B\n74.9\n71.2\n80.0\n81.9\n76.7\n48.5\n72.2\nQwen3-14B\n76.2\n67.5\n83.6\n81.1\n73.3\n50.3\n72.0\nQwen3-8B\n74.1\n70.0\n78.2\n79.2\n70.0\n43.7\n69.2\nQwen3-4B\n69.1\n68.8\n72.7\n75.7\n66.7\n42.3\n65.9\nQwen3-1.7B\n56.0\n55.0\n72.7\n64.5\n46.7\n30.2\n54.2\nQwen3-0.6B\n39.2\n42.5\n54.5\n48.8\n13.3\n14.3\n35.4\nNon-thinking\nMode\nGPT-4o-2024-1120\n67.5\n52.5\n89.1\n80.6\n10.0\n15.5\n52.5\nGemma-3-27b-IT\n73.5\n57.5\n89.1\n77.7\n30.0\n22.4\n58.4\nQwen2.5-72B-Instruct\n66.7\n61.3\n80.0\n80.1\n20.0\n18.8\n54.5\nQwen3-235B-A22B\n71.7\n66.2\n83.6\n83.7\n33.3\n29.5\n61.3\nQwen3-32B\n72.1\n65.0\n83.6\n80.4\n26.7\n24.7\n58.8\nQwen3-30B-A3B\n72.1\n53.8\n85.5\n78.3\n33.3\n25.0\n58.0\nQwen3-14B\n76.2\n63.7\n78.2\n77.4\n40.0\n25.0\n60.1\nQwen3-8B\n73.1\n50.0\n80.0\n73.7\n16.7\n21.3\n52.5\nQwen3-4B\n65.8\n50.0\n60.0\n68.3\n13.3\n17.3\n45.8\nQwen3-1.7B\n47.9\n43.8\n50.9\n54.3\n10.0\n11.6\n36.4\nQwen3-0.6B\n35.5\n37.5\n43.6\n39.5\n3.3\n5.8\n27.5\nTable 25: Benchmark scores for language: French (fr). The highest and second-best scores are shown in\nbold and underlined, respectively.\nModel\nMulti-IF MLogiQA INCLUDE MMMLU MT-AIME24 PolyMath Average\nThinking\nMode\nGemini2.5-Pro\n80.5\n73.8\n85.7\n88.3\n80.0\n52.8\n76.8\nQwQ-32B\n72.4\n78.8\n76.2\n84.0\n80.0\n49.4\n73.5\nQwen3-235B-A22B\n77.3\n78.8\n85.7\n86.6\n86.7\n57.4\n78.8\nQwen3-32B\n76.7\n81.2\n76.2\n82.1\n83.3\n47.1\n74.4\nQwen3-30B-A3B\n75.2\n67.5\n83.3\n81.0\n76.7\n46.9\n71.8\nQwen3-14B\n77.6\n71.2\n73.8\n80.4\n73.3\n44.2\n70.1\nQwen3-8B\n73.8\n66.2\n85.7\n77.9\n70.0\n45.3\n69.8\nQwen3-4B\n71.3\n63.7\n71.4\n74.5\n66.7\n40.2\n64.6\nQwen3-1.7B\n52.6\n56.2\n54.8\n64.8\n60.0\n28.7\n52.8\nQwen3-0.6B\n36.1\n48.8\n47.6\n48.4\n6.7\n14.0\n33.6\nNon-thinking\nMode\nGPT-4o-2024-1120\n67.8\n56.2\n85.7\n81.8\n10.0\n15.3\n52.8\nGemma-3-27b-IT\n73.9\n57.5\n73.8\n78.3\n23.3\n21.5\n54.7\nQwen2.5-72B-Instruct\n72.1\n55.0\n81.0\n80.2\n26.7\n15.7\n55.1\nQwen3-235B-A22B\n73.2\n65.0\n88.1\n81.1\n36.7\n28.1\n62.0\nQwen3-32B\n75.8\n60.0\n73.8\n79.5\n30.0\n23.0\n57.0\nQwen3-30B-A3B\n75.6\n52.5\n69.0\n77.9\n26.7\n27.3\n54.8\nQwen3-14B\n78.4\n63.7\n73.8\n75.1\n33.3\n24.4\n58.1\nQwen3-8B\n71.9\n52.5\n71.4\n71.7\n20.0\n21.4\n51.5\nQwen3-4B\n64.2\n47.5\n61.9\n67.6\n20.0\n19.2\n46.7\nQwen3-1.7B\n46.1\n43.8\n64.3\n53.2\n3.3\n11.6\n37.0\nQwen3-0.6B\n32.8\n35.0\n38.1\n39.4\n6.7\n4.6\n26.1\n24\n\nTable 26: Benchmark scores for language: Portuguese (pt). The highest and second-best scores are\nshown in bold and underlined, respectively.\nModel\nMulti-IF MLogiQA INCLUDE MMMLU MT-AIME24 PolyMath Average\nThinking\nMode\nGemini2.5-Pro\n80.5\n73.8\n83.9\n88.9\n73.3\n52.2\n75.4\nQwQ-32B\n70.5\n70.0\n80.4\n84.0\n80.0\n48.7\n72.3\nQwen3-235B-A22B\n73.6\n78.8\n78.6\n86.2\n86.7\n58.3\n77.0\nQwen3-32B\n74.1\n76.2\n76.8\n82.6\n80.0\n52.4\n73.7\nQwen3-30B-A3B\n76.1\n71.2\n71.4\n81.0\n76.7\n49.3\n71.0\nQwen3-14B\n77.3\n68.8\n75.0\n81.6\n83.3\n46.7\n72.1\nQwen3-8B\n73.9\n67.5\n75.0\n78.6\n56.7\n44.8\n66.1\nQwen3-4B\n70.6\n62.5\n71.4\n75.1\n73.3\n44.2\n66.2\nQwen3-1.7B\n55.6\n60.0\n53.6\n64.6\n46.7\n28.2\n51.4\nQwen3-0.6B\n38.7\n33.8\n42.9\n47.5\n10.0\n12.7\n30.9\nNon-thinking\nMode\nGPT-4o-2024-1120\n66.8\n57.5\n78.6\n80.7\n10.0\n15.0\n51.4\nGemma-3-27b-IT\n72.9\n55.0\n75.0\n77.1\n33.3\n20.9\n55.7\nQwen2.5-72B-Instruct\n68.8\n55.0\n71.4\n82.2\n23.3\n11.3\n52.0\nQwen3-235B-A22B\n72.5\n67.5\n82.1\n83.5\n33.3\n28.3\n61.2\nQwen3-32B\n71.1\n61.3\n73.2\n80.6\n30.0\n23.9\n56.7\nQwen3-30B-A3B\n72.3\n47.5\n67.9\n77.8\n26.7\n24.0\n52.7\nQwen3-14B\n75.5\n58.8\n75.0\n76.5\n26.7\n25.8\n56.4\nQwen3-8B\n71.9\n56.2\n71.4\n72.9\n20.0\n19.7\n52.0\nQwen3-4B\n66.1\n50.0\n73.2\n66.7\n10.0\n18.1\n47.4\nQwen3-1.7B\n49.5\n33.8\n39.3\n52.9\n6.7\n12.8\n32.5\nQwen3-0.6B\n36.6\n37.5\n42.9\n37.5\n3.3\n5.7\n27.2\nTable 27: Benchmark scores for language: Italian (it). The highest and second-best scores are shown in\nbold and underlined, respectively.\nModel\nMulti-IF INCLUDE MMMLU MT-AIME24 PolyMath Average\nThinking\nMode\nGemini2.5-Pro\n80.9\n100.0\n87.2\n90.0\n54.1\n82.4\nQwQ-32B\n71.2\n96.4\n84.9\n76.7\n49.3\n75.7\nQwen3-235B-A22B\n73.7\n96.4\n85.7\n80.0\n57.4\n78.6\nQwen3-32B\n76.6\n90.9\n81.6\n80.0\n49.7\n75.8\nQwen3-30B-A3B\n75.9\n94.5\n81.9\n80.0\n48.1\n76.1\nQwen3-14B\n79.0\n94.5\n80.2\n70.0\n47.0\n74.1\nQwen3-8B\n74.6\n89.1\n77.5\n76.7\n46.1\n72.8\nQwen3-4B\n69.8\n83.6\n74.4\n76.7\n44.5\n69.8\nQwen3-1.7B\n54.6\n74.5\n64.2\n53.3\n29.6\n55.2\nQwen3-0.6B\n37.8\n45.5\n45.9\n6.7\n13.3\n29.8\nNon-thinking\nMode\nGPT-4o-2024-1120\n67.6\n98.2\n80.7\n13.3\n15.2\n55.0\nGemma-3-27b-IT\n74.6\n90.9\n78.4\n23.3\n20.5\n57.5\nQwen2.5-72B-Instruct\n67.2\n94.5\n80.7\n16.7\n16.7\n55.2\nQwen3-235B-A22B\n72.9\n92.7\n82.6\n33.3\n28.6\n62.0\nQwen3-32B\n71.4\n92.7\n79.5\n30.0\n23.0\n59.3\nQwen3-30B-A3B\n73.9\n87.3\n77.7\n33.3\n24.8\n59.4\nQwen3-14B\n75.8\n89.1\n75.7\n26.7\n27.6\n59.0\nQwen3-8B\n72.1\n85.5\n72.9\n13.3\n23.8\n53.5\nQwen3-4B\n63.0\n78.2\n67.8\n23.3\n19.3\n50.3\nQwen3-1.7B\n46.1\n70.9\n53.4\n6.7\n11.9\n37.8\nQwen3-0.6B\n35.1\n43.6\n39.0\n0.0\n4.5\n24.4\n25\n\nTable 28: Benchmark scores for language: Arabic (ar). The highest and second-best scores are shown in\nbold and underlined, respectively.\nModel\nMLogiQA INCLUDE MMMLU MT-AIME24 PolyMath Average\nThinking\nMode\nGemini2.5-Pro\n75.0\n89.3\n87.8\n76.7\n52.6\n76.3\nQwQ-32B\n75.0\n67.9\n81.8\n80.0\n41.3\n69.2\nQwen3-235B-A22B\n80.0\n71.4\n83.6\n76.7\n53.7\n73.1\nQwen3-32B\n66.2\n73.2\n80.1\n86.7\n47.0\n70.6\nQwen3-30B-A3B\n66.2\n66.1\n77.2\n83.3\n47.3\n68.0\nQwen3-14B\n71.2\n67.9\n77.4\n83.3\n46.6\n69.3\nQwen3-8B\n65.0\n67.9\n74.4\n76.7\n44.9\n65.8\nQwen3-4B\n62.5\n55.4\n67.7\n66.7\n41.2\n58.7\nQwen3-1.7B\n55.0\n44.6\n53.2\n36.7\n25.8\n43.1\nQwen3-0.6B\n40.0\n41.1\n38.9\n10.0\n11.7\n28.3\nNon-thinking\nMode\nGPT-4o-2024-1120\n51.2\n78.6\n80.9\n13.3\n12.9\n47.4\nGemma-3-27b-IT\n56.2\n62.5\n74.4\n26.7\n22.8\n48.5\nQwen2.5-72B-Instruct\n56.2\n66.1\n77.2\n6.7\n14.7\n44.2\nQwen3-235B-A22B\n66.2\n67.9\n79.5\n40.0\n28.2\n56.4\nQwen3-32B\n55.0\n69.6\n75.7\n23.3\n25.4\n49.8\nQwen3-30B-A3B\n48.8\n64.3\n71.6\n30.0\n22.6\n47.5\nQwen3-14B\n52.5\n60.7\n69.5\n23.3\n23.5\n45.9\nQwen3-8B\n45.0\n58.9\n64.6\n13.3\n16.4\n39.6\nQwen3-4B\n52.5\n42.9\n56.7\n13.3\n15.3\n36.1\nQwen3-1.7B\n31.2\n37.5\n43.6\n3.3\n9.4\n25.0\nQwen3-0.6B\n40.0\n39.3\n35.4\n0.0\n3.8\n23.7\nTable 29: Benchmark scores for language: Japanese (ja). The highest and second-best scores are shown\nin bold and underlined, respectively.\nModel\nMLogiQA INCLUDE MMMLU MT-AIME24 PolyMath Average\nThinking\nMode\nGemini2.5-Pro\n72.5\n74.5\n83.8\n83.3\n55.4\n73.9\nQwQ-32B\n73.8\n86.3\n82.3\n53.3\n39.9\n67.1\nQwen3-235B-A22B\n75.0\n94.1\n84.8\n73.3\n52.7\n76.0\nQwen3-32B\n70.0\n90.2\n80.2\n76.7\n47.7\n73.0\nQwen3-30B-A3B\n66.2\n88.2\n79.9\n73.3\n47.4\n71.0\nQwen3-14B\n68.8\n88.2\n79.4\n66.7\n45.7\n69.8\nQwen3-8B\n71.2\n86.3\n74.9\n73.3\n44.7\n70.1\nQwen3-4B\n63.7\n80.4\n72.5\n53.3\n40.7\n62.1\nQwen3-1.7B\n53.8\n74.5\n61.8\n36.7\n28.5\n51.1\nQwen3-0.6B\n47.5\n47.1\n45.1\n13.3\n14.5\n33.5\nNon-thinking\nMode\nGPT-4o-2024-1120\n60.0\n92.2\n81.9\n10.0\n12.5\n51.3\nGemma-3-27b-IT\n66.2\n86.3\n76.5\n20.0\n17.3\n53.3\nQwen2.5-72B-Instruct\n55.0\n94.1\n77.7\n16.7\n17.7\n52.2\nQwen3-235B-A22B\n67.5\n92.2\n80.9\n26.7\n26.9\n58.8\nQwen3-32B\n58.8\n92.2\n78.0\n20.0\n20.5\n53.9\nQwen3-30B-A3B\n51.2\n82.4\n74.9\n30.0\n20.6\n51.8\nQwen3-14B\n55.0\n84.3\n73.8\n33.3\n19.8\n53.2\nQwen3-8B\n47.5\n82.4\n69.9\n20.0\n18.5\n47.7\nQwen3-4B\n46.2\n76.5\n64.8\n13.3\n15.1\n43.2\nQwen3-1.7B\n40.0\n68.6\n46.3\n3.3\n11.6\n34.0\nQwen3-0.6B\n37.5\n37.3\n37.9\n3.3\n3.7\n23.9\n26\n\nTable 30: Benchmark scores for language: Korean (ko). The highest and second-best scores are shown in\nbold and underlined, respectively.\nModel\nMLogiQA INCLUDE MMMLU MT-AIME24 PolyMath Average\nThinking\nMode\nGemini2.5-Pro\n75.0\n88.0\n85.9\n76.7\n50.0\n75.1\nQwQ-32B\n76.2\n72.0\n81.8\n60.0\n40.0\n66.0\nQwen3-235B-A22B\n71.2\n80.0\n84.7\n80.0\n55.7\n74.3\nQwen3-32B\n71.2\n74.0\n79.2\n80.0\n48.5\n70.6\nQwen3-30B-A3B\n68.8\n72.0\n78.6\n76.7\n46.6\n68.5\nQwen3-14B\n67.5\n74.0\n79.6\n76.7\n46.0\n68.8\nQwen3-8B\n60.0\n80.0\n74.7\n76.7\n42.3\n66.7\nQwen3-4B\n66.2\n74.0\n68.8\n70.0\n40.6\n63.9\nQwen3-1.7B\n53.8\n66.0\n57.8\n43.3\n25.2\n49.2\nQwen3-0.6B\n33.8\n52.0\n41.5\n13.3\n11.8\n30.5\nNon-thinking\nMode\nGPT-4o-2024-1120\n63.7\n80.0\n80.5\n13.3\n12.9\n50.1\nGemma-3-27b-IT\n58.8\n76.0\n75.9\n20.0\n18.3\n49.8\nQwen2.5-72B-Instruct\n58.8\n68.0\n76.7\n6.7\n17.7\n45.6\nQwen3-235B-A22B\n63.7\n76.0\n79.8\n33.3\n27.9\n56.1\nQwen3-32B\n60.0\n74.0\n77.2\n26.7\n21.2\n51.8\nQwen3-30B-A3B\n52.5\n72.0\n72.5\n16.7\n20.7\n46.9\nQwen3-14B\n52.5\n68.0\n73.3\n20.0\n18.7\n46.5\nQwen3-8B\n52.5\n76.0\n66.5\n23.3\n16.3\n46.9\nQwen3-4B\n46.2\n74.0\n59.9\n13.3\n16.6\n42.0\nQwen3-1.7B\n48.8\n58.0\n46.0\n6.7\n9.0\n33.7\nQwen3-0.6B\n40.0\n52.0\n36.9\n0.0\n5.5\n26.9\nTable 31: Benchmark scores for language: Indonesian (id). The highest and second-best scores are\nshown in bold and underlined, respectively.\nModel\nINCLUDE MMMLU MT-AIME24 PolyMath Average\nThinking\nMode\nGemini2.5-Pro\n80.0\n86.3\n83.3\n51.3\n75.2\nQwQ-32B\n76.4\n83.7\n73.3\n47.3\n70.2\nQwen3-235B-A22B\n80.0\n87.2\n80.0\n53.5\n75.2\nQwen3-32B\n80.0\n82.0\n76.7\n45.6\n71.1\nQwen3-30B-A3B\n81.8\n80.4\n80.0\n44.9\n71.8\nQwen3-14B\n78.2\n79.6\n70.0\n45.3\n68.3\nQwen3-8B\n72.7\n77.7\n70.0\n43.8\n66.0\nQwen3-4B\n70.9\n72.3\n66.7\n41.2\n62.8\nQwen3-1.7B\n63.6\n61.2\n36.7\n26.8\n47.1\nQwen3-0.6B\n36.4\n46.6\n10.0\n12.6\n26.4\nNon-thinking\nMode\nGPT-4o-2024-1120\n80.0\n81.1\n10.0\n14.7\n46.4\nGemma-3-27b-IT\n76.4\n75.9\n13.3\n22.6\n47.0\nQwen2.5-72B-Instruct\n74.5\n78.8\n10.0\n16.6\n45.0\nQwen3-235B-A22B\n81.8\n81.9\n33.3\n27.5\n56.1\nQwen3-32B\n81.8\n77.2\n23.3\n24.3\n51.6\nQwen3-30B-A3B\n70.9\n76.4\n30.0\n25.9\n50.8\nQwen3-14B\n70.9\n74.1\n26.7\n24.6\n49.1\nQwen3-8B\n78.2\n69.6\n20.0\n21.6\n47.4\nQwen3-4B\n67.3\n66.5\n13.3\n19.0\n41.5\nQwen3-1.7B\n52.7\n49.0\n3.3\n10.8\n29.0\nQwen3-0.6B\n52.7\n40.0\n3.3\n5.1\n25.3\n27\n\nTable 32: Benchmark scores for language: Russian (ru). The highest and second-best scores are shown\nin bold and underlined, respectively.\nModel\nMulti-IF INCLUDE MT-AIME24 PolyMath Average\nThinking\nMode\nGemini2.5-Pro\n68.1\n80.4\n70.0\n52.3\n67.7\nQwQ-32B\n61.2\n73.2\n76.7\n43.6\n63.7\nQwen3-235B-A22B\n62.2\n80.4\n80.0\n53.1\n68.9\nQwen3-32B\n62.5\n73.2\n63.3\n46.5\n61.4\nQwen3-30B-A3B\n60.7\n76.8\n73.3\n45.4\n64.0\nQwen3-14B\n63.6\n80.4\n66.7\n46.4\n64.3\nQwen3-8B\n62.9\n69.6\n63.3\n37.7\n58.4\nQwen3-4B\n52.8\n69.6\n56.7\n36.6\n53.9\nQwen3-1.7B\n37.8\n46.4\n20.0\n22.8\n31.8\nQwen3-0.6B\n26.4\n46.4\n3.3\n7.0\n20.8\nNon-thinking\nMode\nGPT-4o-2024-1120\n52.0\n80.4\n20.0\n13.7\n41.5\nGemma-3-27b-IT\n57.3\n71.4\n23.3\n21.6\n43.4\nQwen2.5-72B-Instruct\n54.1\n67.9\n20.0\n13.3\n38.8\nQwen3-235B-A22B\n56.7\n75.0\n40.0\n26.1\n49.4\nQwen3-32B\n58.6\n71.4\n30.0\n23.3\n45.8\nQwen3-30B-A3B\n58.0\n73.2\n30.0\n21.1\n45.6\nQwen3-14B\n60.3\n71.4\n26.7\n24.2\n45.6\nQwen3-8B\n59.3\n58.9\n20.0\n22.8\n40.2\nQwen3-4B\n46.1\n58.9\n13.3\n17.8\n34.0\nQwen3-1.7B\n34.8\n41.1\n3.3\n13.2\n23.1\nQwen3-0.6B\n25.5\n46.4\n0.0\n5.8\n19.4\nTable 33: Benchmark scores for language: Vietnamese (vi). The highest and second-best scores are\nshown in bold and underlined, respectively.\nModel\nMLogiQA INCLUDE MT-AIME24 PolyMath Average\nThinking\nMode\nGemini2.5-Pro\n72.5\n89.1\n70.0\n52.1\n70.9\nQwQ-32B\n71.2\n69.1\n70.0\n49.2\n64.9\nQwen3-235B-A22B\n75.0\n87.3\n83.3\n55.1\n75.2\nQwen3-32B\n67.5\n81.8\n83.3\n44.0\n69.2\nQwen3-30B-A3B\n68.8\n78.2\n76.7\n46.1\n67.4\nQwen3-14B\n72.5\n72.7\n73.3\n45.8\n66.1\nQwen3-8B\n65.0\n72.7\n73.3\n42.9\n63.5\nQwen3-4B\n68.8\n63.6\n60.0\n42.2\n58.6\nQwen3-1.7B\n52.5\n61.8\n30.0\n26.9\n42.8\nQwen3-0.6B\n33.8\n38.2\n6.7\n9.8\n22.1\nNon-thinking\nMode\nGPT-4o-2024-1120\n57.5\n81.8\n10.0\n13.0\n40.6\nGemma-3-27b-IT\n52.5\n74.5\n33.3\n20.6\n45.2\nQwen2.5-72B-Instruct\n61.3\n72.7\n26.7\n18.6\n44.8\nQwen3-235B-A22B\n70.0\n83.6\n36.7\n27.1\n54.4\nQwen3-32B\n60.0\n81.8\n23.3\n21.8\n46.7\nQwen3-30B-A3B\n52.5\n81.8\n20.0\n24.7\n44.8\nQwen3-14B\n63.7\n67.3\n20.0\n21.6\n43.2\nQwen3-8B\n48.8\n65.5\n20.0\n19.1\n38.4\nQwen3-4B\n48.8\n65.5\n20.0\n19.0\n38.3\nQwen3-1.7B\n36.2\n60.0\n3.3\n10.9\n27.6\nQwen3-0.6B\n30.0\n36.4\n3.3\n3.9\n18.4\n28\n\nTable 34: Benchmark scores for language: German (de). The highest and second-best scores are shown\nin bold and underlined, respectively.\nModel\nINCLUDE MMMLU MT-AIME24 PolyMath Average\nThinking\nMode\nGemini2.5-Pro\n50.0\n85.6\n86.7\n53.8\n69.0\nQwQ-32B\n57.1\n83.8\n76.7\n51.0\n67.2\nQwen3-235B-A22B\n71.4\n86.0\n83.3\n55.4\n74.0\nQwen3-32B\n64.3\n81.9\n86.7\n48.1\n70.2\nQwen3-30B-A3B\n64.3\n81.9\n80.0\n46.6\n68.2\nQwen3-14B\n57.1\n80.9\n70.0\n48.1\n64.0\nQwen3-8B\n64.3\n78.1\n66.7\n43.6\n63.2\nQwen3-4B\n57.1\n74.0\n73.3\n43.1\n61.9\nQwen3-1.7B\n64.3\n63.4\n36.7\n26.8\n47.8\nQwen3-0.6B\n57.1\n47.6\n10.0\n13.7\n32.1\nNon-thinking\nMode\nGPT-4o-2024-1120\n57.1\n80.4\n10.0\n13.5\n40.2\nGemma-3-27b-IT\n57.1\n76.1\n26.7\n20.2\n45.0\nQwen2.5-72B-Instruct\n64.3\n79.9\n16.7\n19.3\n45.0\nQwen3-235B-A22B\n71.4\n81.7\n40.0\n25.9\n54.8\nQwen3-32B\n57.1\n77.2\n30.0\n21.9\n46.6\nQwen3-30B-A3B\n57.1\n77.7\n23.3\n25.2\n45.8\nQwen3-14B\n57.1\n76.0\n30.0\n24.5\n46.9\nQwen3-8B\n64.3\n70.8\n20.0\n19.9\n43.8\nQwen3-4B\n64.3\n66.0\n26.7\n16.4\n43.4\nQwen3-1.7B\n42.9\n53.2\n10.0\n10.6\n29.2\nQwen3-0.6B\n42.9\n37.8\n3.3\n5.7\n22.4\nTable 35: Benchmark scores for language: Thai (th). The highest and second-best scores are shown in\nbold and underlined, respectively.\nModel\nMLogiQA MT-AIME24 PolyMath Average\nThinking\nMode\nGemini2.5-Pro\n73.8\n80.0\n50.7\n68.2\nQwQ-32B\n75.0\n60.0\n41.3\n58.8\nQwen3-235B-A22B\n73.8\n86.7\n53.6\n71.4\nQwen3-32B\n73.8\n76.7\n46.9\n65.8\nQwen3-30B-A3B\n63.7\n80.0\n45.2\n63.0\nQwen3-14B\n65.0\n76.7\n44.4\n62.0\nQwen3-8B\n68.8\n70.0\n41.3\n60.0\nQwen3-4B\n60.0\n60.0\n39.4\n53.1\nQwen3-1.7B\n48.8\n33.3\n23.7\n35.3\nQwen3-0.6B\n33.8\n13.3\n11.4\n19.5\nNon-thinking\nMode\nGPT-4o-2024-1120\n52.5\n10.0\n11.9\n24.8\nGemma-3-27b-IT\n50.0\n16.7\n19.0\n28.6\nQwen2.5-72B-Instruct\n58.8\n6.7\n17.4\n27.6\nQwen3-235B-A22B\n61.3\n23.3\n27.6\n37.4\nQwen3-32B\n61.3\n13.3\n22.2\n32.3\nQwen3-30B-A3B\n50.0\n30.0\n22.3\n34.1\nQwen3-14B\n47.5\n23.3\n22.1\n31.0\nQwen3-8B\n42.5\n10.0\n17.2\n23.2\nQwen3-4B\n43.8\n13.3\n16.1\n24.4\nQwen3-1.7B\n42.5\n6.7\n9.5\n19.6\nQwen3-0.6B\n37.5\n0.0\n3.6\n13.7\n29\n\nTable 36: Language families and language codes supported by Qwen3 in Belebele Benchmark\nLanguage family # Langs Language code (ISO 639-3 ISO 15924)\nIndo-European\n40\npor Latn, deu Latn, tgk Cyrl, ces Latn, nob Latn, dan Latn, snd Arab, spa Latn,\nisl Latn, slv Latn, eng Latn, ory Orya, hrv Latn, ell Grek, ukr Cyrl, pan Guru,\nsrp Cyrl, npi Deva, mkd Cyrl, guj Gujr, nld Latn, swe Latn, hin Deva, rus Cyrl,\nasm Beng, cat Latn, als Latn, sin Sinh, urd Arab, mar Deva, lit Latn, slk Latn,\nita Latn, pol Latn, bul Cyrl, afr Latn, ron Latn, fra Latn, ben Beng, hye Armn\nSino-Tibetan\n3\nzho Hans, mya Mymr, zho Hant\nAfro-Asiatic\n8\nheb Hebr, apc Arab, acm Arab, ary Arab, ars Arab, arb Arab, mlt Latn, erz Arab\nAustronesian\n7\nilo Latn, ceb Latn, tgl Latn, sun Latn, jav Latn, war Latn, ind Latn\nDravidian\n4\nmal Mlym, kan Knda, tel Telu, tam Taml\nTurkic\n4\nkaz Cyrl, azj Latn, tur Latn, uzn Latn\nTai-Kadai\n2\ntha Thai, lao Laoo\nUralic\n3\nfin Latn, hun Latn, est Latn\nAustroasiatic\n2\nvie Latn, khm Khmr\nOther\n7\neus Latn, kor Hang, hat Latn, swh Latn, kea Latn, jpn Jpan, kat Geor\nTable 37: Comparison of Belebele Benchmark performance between Qwen3 and other baseline models.\nScores are highlighted with the highest in bold and the second-best underlined.\nModel\nIndo-\nEuropean\nSino-\nTibetan\nAfro-\nAsiatic Austronesian Dravidian Turkic\nTai-\nKadai Uralic Austroasiatic Other\nGemma-3-27B-IT\n89.2\n86.3\n85.9\n84.1\n83.5\n86.8\n81.0\n91.0\n86.5\n87.0\nQwen2.5-32B-Instruct\n85.5\n82.3\n80.4\n70.6\n67.8\n80.8\n74.5\n87.0\n79.0\n72.6\nQwQ-32B\n86.1\n83.7\n81.9\n71.3\n69.3\n80.3\n77.0\n88.0\n83.0\n74.0\nQwen3-32B (Thinking)\n90.7\n89.7\n84.8\n86.7\n84.5\n89.3\n83.5\n91.3\n88.0\n83.1\nQwen3-32B (Non-thinking)\n89.1\n88.0\n82.3\n83.7\n84.0\n85.0\n85.0\n88.7\n88.0\n81.3\nGemma-3-12B-IT\n85.8\n83.3\n83.4\n79.3\n79.0\n82.8\n77.5\n89.0\n83.0\n81.6\nQwen2.5-14B-Instruct\n82.7\n78.9\n80.4\n69.1\n66.2\n74.2\n72.2\n83.9\n77.9\n70.4\nQwen3-14B (Thinking)\n88.6\n87.3\n82.4\n82.4\n81.0\n83.8\n83.5\n91.0\n82.5\n81.7\nQwen3-14B (Non-thinking)\n87.4\n82.7\n80.1\n80.7\n78.0\n81.8\n80.5\n87.7\n81.5\n77.0\nGemma-3-4B-IT\n71.8\n72.0\n63.5\n61.7\n64.8\n64.0\n61.5\n70.7\n71.0\n62.6\nQwen2.5-3B-Instruct\n58.0\n62.3\n57.2\n47.9\n36.9\n45.1\n49.8\n50.6\n56.8\n48.4\nQwen3-4B (Thinking)\n82.2\n77.7\n74.1\n73.0\n74.3\n76.3\n68.5\n83.0\n74.5\n67.9\nQwen3-4B (Non-thinking)\n76.0\n77.0\n65.6\n65.6\n65.5\n64.0\n60.5\n74.0\n74.0\n61.0\nGemma-3-1B-IT\n36.5\n36.0\n30.0\n29.1\n28.8\n27.3\n28.0\n32.7\n33.0\n30.9\nQwen2.5-1.5B-Instruct\n41.5\n43.0\n39.6\n34.8\n28.6\n29.7\n39.4\n33.8\n42.0\n36.0\nQwen3-1.7B (Thinking)\n69.7\n66.0\n59.4\n58.6\n52.8\n57.8\n53.5\n70.3\n63.5\n53.4\nQwen3-1.7B (Non-thinking)\n58.8\n62.7\n50.8\n53.0\n43.3\n48.0\n46.0\n54.3\n54.0\n43.9\n30\n\nReferences\nMarah Abdin, Jyoti Aneja, Harkirat Behl, S´ebastien Bubeck, Ronen Eldan, Suriya Gunasekar, Michael\nHarrison, Russell J Hewett, Mojan Javaheripi, Piero Kauffmann, et al. Phi-4 technical report. arXiv\npreprint arXiv:2412.08905, 2024.\nAIME. AIME problems and solutions, 2025. URL https://artofproblemsolving.com/wiki/index.p\nhp/AIME Problems and Solutions.\nJoshua Ainslie, James Lee-Thorp, Michiel de Jong, Yury Zemlyanskiy, Federico Lebr´on, and Sumit\nSanghai. GQA: Training generalized multi-query Transformer models from multi-head checkpoints. In\nEMNLP, pp. 4895–4901. Association for Computational Linguistics, 2023.\nChenxin An, Fei Huang, Jun Zhang, Shansan Gong, Xipeng Qiu, Chang Zhou, and Lingpeng Kong.\nTraining-free long-context scaling of large language models. CoRR, abs/2402.17463, 2024.\nAnthropic. Claude 3.7 Sonnet, 2025. URL https://www.anthropic.com/news/claude-3-7-sonnet.\nJacob Austin, Augustus Odena, Maxwell I. Nye, Maarten Bosma, Henryk Michalewski, David Dohan,\nEllen Jiang, Carrie J. Cai, Michael Terry, Quoc V. Le, and Charles Sutton. Program synthesis with large\nlanguage models. CoRR, abs/2108.07732, 2021.\nJinze Bai, Shuai Bai, Yunfei Chu, Zeyu Cui, Kai Dang, Xiaodong Deng, Yang Fan, Wenbin Ge, Yu Han, Fei\nHuang, Binyuan Hui, Luo Ji, Mei Li, Junyang Lin, Runji Lin, Dayiheng Liu, Gao Liu, Chengqiang Lu,\nKeming Lu, Jianxin Ma, Rui Men, Xingzhang Ren, Xuancheng Ren, Chuanqi Tan, Sinan Tan, Jianhong\nTu, Peng Wang, Shijie Wang, Wei Wang, Shengguang Wu, Benfeng Xu, Jin Xu, An Yang, Hao Yang,\nJian Yang, Shusheng Yang, Yang Yao, Bowen Yu, Hongyi Yuan, Zheng Yuan, Jianwei Zhang, Xingxuan\nZhang, Yichang Zhang, Zhenru Zhang, Chang Zhou, Jingren Zhou, Xiaohuan Zhou, and Tianhang\nZhu. Qwen technical report. CoRR, abs/2309.16609, 2023.\nShuai Bai, Keqin Chen, Xuejing Liu, Jialin Wang, Wenbin Ge, Sibo Song, Kai Dang, Peng Wang, Shijie\nWang, Jun Tang, et al. Qwen2.5-VL technical report. arXiv preprint arXiv:2502.13923, 2025.\nLucas Bandarkar, Davis Liang, Benjamin Muller, Mikel Artetxe, Satya Narayan Shukla, Donald Husa,\nNaman Goyal, Abhinandan Krishnan, Luke Zettlemoyer, and Madian Khabsa. The Belebele benchmark:\nA parallel reading comprehension dataset in 122 language variants. CoRR, abs/2308.16884, 2023.\nTom B. Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared Kaplan, Prafulla Dhariwal, Arvind\nNeelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, Sandhini Agarwal, Ariel Herbert-Voss,\nGretchen Krueger, Tom Henighan, Rewon Child, Aditya Ramesh, Daniel M. Ziegler, Jeffrey Wu,\nClemens Winter, Christopher Hesse, Mark Chen, Eric Sigler, Mateusz Litwin, Scott Gray, Benjamin\nChess, Jack Clark, Christopher Berner, Sam McCandlish, Alec Radford, Ilya Sutskever, and Dario\nAmodei. Language models are few-shot learners. In NeurIPS, 2020.\nFederico Cassano, John Gouwar, Daniel Nguyen, Sydney Nguyen, Luna Phipps-Costin, Donald Pinckney,\nMing-Ho Yee, Yangtian Zi, Carolyn Jane Anderson, Molly Q. Feldman, Arjun Guha, Michael Greenberg,\nand Abhinav Jangda. MultiPL-E: A scalable and polyglot approach to benchmarking neural code\ngeneration. IEEE Trans. Software Eng., 49(7):3675–3691, 2023.\nMark Chen, Jerry Tworek, Heewoo Jun, Qiming Yuan, Henrique Pond´e de Oliveira Pinto, Jared Kaplan,\nHarrison Edwards, Yuri Burda, Nicholas Joseph, Greg Brockman, Alex Ray, Raul Puri, Gretchen\nKrueger, Michael Petrov, Heidy Khlaaf, Girish Sastry, Pamela Mishkin, Brooke Chan, Scott Gray, Nick\nRyder, Mikhail Pavlov, Alethea Power, Lukasz Kaiser, Mohammad Bavarian, Clemens Winter, Philippe\nTillet, Felipe Petroski Such, Dave Cummings, Matthias Plappert, Fotios Chantzis, Elizabeth Barnes,\nAriel Herbert-Voss, William Hebgen Guss, Alex Nichol, Alex Paino, Nikolas Tezak, Jie Tang, Igor\nBabuschkin, Suchir Balaji, Shantanu Jain, William Saunders, Christopher Hesse, Andrew N. Carr,\nJan Leike, Joshua Achiam, Vedant Misra, Evan Morikawa, Alec Radford, Matthew Knight, Miles\nBrundage, Mira Murati, Katie Mayer, Peter Welinder, Bob McGrew, Dario Amodei, Sam McCandlish,\nIlya Sutskever, and Wojciech Zaremba. Evaluating large language models trained on code. CoRR,\nabs/2107.03374, 2021.\nKarl Cobbe, Vineet Kosaraju, Mohammad Bavarian, Mark Chen, Heewoo Jun, Lukasz Kaiser, Matthias\nPlappert, Jerry Tworek, Jacob Hilton, Reiichiro Nakano, Christopher Hesse, and John Schulman.\nTraining verifiers to solve math word problems. CoRR, abs/2110.14168, 2021.\nDamai Dai, Chengqi Deng, Chenggang Zhao, R. X. Xu, Huazuo Gao, Deli Chen, Jiashi Li, Wangding\nZeng, Xingkai Yu, Y. Wu, Zhenda Xie, Y. K. Li, Panpan Huang, Fuli Luo, Chong Ruan, Zhifang Sui, and\nWenfeng Liang. DeepSeekMoE: Towards ultimate expert specialization in mixture-of-experts language\nmodels. CoRR, abs/2401.06066, 2024.\n31\n\nYann N. Dauphin, Angela Fan, Michael Auli, and David Grangier. Language modeling with gated\nconvolutional networks. In ICML, volume 70 of Proceedings of Machine Learning Research, pp. 933–941.\nPMLR, 2017.\nGoogle DeepMind. Gemini 2.5, 2025. URL https://blog.google/technology/google-deepmind/gemi\nni-model-thinking-updates-march-2025/.\nMostafa Dehghani, Josip Djolonga, Basil Mustafa, Piotr Padlewski, Jonathan Heek, Justin Gilmer, An-\ndreas Peter Steiner, Mathilde Caron, Robert Geirhos, Ibrahim Alabdulmohsin, Rodolphe Jenatton,\nLucas Beyer, Michael Tschannen, Anurag Arnab, Xiao Wang, Carlos Riquelme Ruiz, Matthias Minderer,\nJoan Puigcerver, Utku Evci, Manoj Kumar, Sjoerd van Steenkiste, Gamaleldin Fathy Elsayed, Aravindh\nMahendran, Fisher Yu, Avital Oliver, Fantine Huot, Jasmijn Bastings, Mark Collier, Alexey A. Gritsenko,\nVighnesh Birodkar, Cristina Nader Vasconcelos, Yi Tay, Thomas Mensink, Alexander Kolesnikov, Filip\nPavetic, Dustin Tran, Thomas Kipf, Mario Lucic, Xiaohua Zhai, Daniel Keysers, Jeremiah J. Harmsen,\nand Neil Houlsby. Scaling vision transformers to 22 billion parameters. In ICML, volume 202 of\nProceedings of Machine Learning Research, pp. 7480–7512. PMLR, 2023.\nXinrun Du, Yifan Yao, Kaijing Ma, Bingli Wang, Tianyu Zheng, King Zhu, Minghao Liu, Yiming Liang,\nXiaolong Jin, Zhenlin Wei, et al. SuperGPQA: Scaling LLM evaluation across 285 graduate disciplines.\narXiv preprint arXiv:2502.14739, 2025.\nAbhimanyu Dubey, Abhinav Jauhri, Abhinav Pandey, Abhishek Kadian, Ahmad Al-Dahle, Aiesha\nLetman, Akhil Mathur, Alan Schelten, Amy Yang, Angela Fan, Anirudh Goyal, Anthony Hartshorn,\nAobo Yang, Archi Mitra, Archie Sravankumar, Artem Korenev, Arthur Hinsvark, Arun Rao, Aston\nZhang, Aur´elien Rodriguez, Austen Gregerson, Ava Spataru, Baptiste Rozi`ere, Bethany Biron, Binh\nTang, Bobbie Chern, Charlotte Caucheteux, Chaya Nayak, Chloe Bi, Chris Marra, Chris McConnell,\nChristian Keller, Christophe Touret, Chunyang Wu, Corinne Wong, Cristian Canton Ferrer, Cyrus\nNikolaidis, Damien Allonsius, Daniel Song, Danielle Pintz, Danny Livshits, David Esiobu, Dhruv\nChoudhary, Dhruv Mahajan, Diego Garcia-Olano, Diego Perino, Dieuwke Hupkes, Egor Lakomkin,\nEhab AlBadawy, Elina Lobanova, Emily Dinan, Eric Michael Smith, Filip Radenovic, Frank Zhang,\nGabriel Synnaeve, Gabrielle Lee, Georgia Lewis Anderson, Graeme Nail, Gr´egoire Mialon, Guan\nPang, Guillem Cucurell, Hailey Nguyen, Hannah Korevaar, Hu Xu, Hugo Touvron, Iliyan Zarov,\nImanol Arrieta Ibarra, Isabel M. Kloumann, Ishan Misra, Ivan Evtimov, Jade Copet, Jaewon Lee, Jan\nGeffert, Jana Vranes, Jason Park, Jay Mahadeokar, Jeet Shah, Jelmer van der Linde, Jennifer Billock,\nJenny Hong, Jenya Lee, Jeremy Fu, Jianfeng Chi, Jianyu Huang, Jiawen Liu, Jie Wang, Jiecao Yu,\nJoanna Bitton, Joe Spisak, Jongsoo Park, Joseph Rocca, Joshua Johnstun, Joshua Saxe, Junteng Jia,\nKalyan Vasuden Alwala, Kartikeya Upasani, Kate Plawiak, Ke Li, Kenneth Heafield, Kevin Stone, and\net al. The Llama 3 herd of models. CoRR, abs/2407.21783, 2024.\nSimin Fan, Matteo Pagliardini, and Martin Jaggi. DoGE: Domain reweighting with generalization\nestimation. arXiv preprint arXiv:2310.15393, 2023.\nAryo Pradipta Gema, Joshua Ong Jun Leang, Giwon Hong, Alessio Devoto, Alberto Carlo Maria Mancino,\nRohit Saxena, Xuanli He, Yu Zhao, Xiaotang Du, Mohammad Reza Ghasemi Madani, et al. Are we\ndone with MMLU? CoRR, abs/2406.04127, 2024.\nAlex Gu, Baptiste Rozi`ere, Hugh Leather, Armando Solar-Lezama, Gabriel Synnaeve, and Sida I.\nWang. CRUXEval: A benchmark for code reasoning, understanding and execution. arXiv preprint\narXiv:2401.03065, 2024.\nDaya Guo, Dejian Yang, Haowei Zhang, Junxiao Song, Ruoyu Zhang, Runxin Xu, Qihao Zhu, Shirong Ma,\nPeiyi Wang, Xiao Bi, et al. DeepSeek-R1: Incentivizing reasoning capability in LLMs via reinforcement\nlearning. arXiv preprint arXiv:2501.12948, 2025.\nYun He, Di Jin, Chaoqi Wang, Chloe Bi, Karishma Mandyam, Hejia Zhang, Chen Zhu, Ning Li, Tengyu\nXu, Hongjiang Lv, et al. Multi-IF: Benchmarking LLMs on multi-turn and multilingual instructions\nfollowing. arXiv preprint arXiv:2410.15553, 2024.\nDan Hendrycks, Collin Burns, Steven Basart, Andy Zou, Mantas Mazeika, Dawn Song, and Jacob\nSteinhardt. Measuring massive multitask language understanding. In ICLR. OpenReview.net, 2021a.\nDan Hendrycks, Collin Burns, Saurav Kadavath, Akul Arora, Steven Basart, Eric Tang, Dawn Song,\nand Jacob Steinhardt. Measuring mathematical problem solving with the MATH dataset. In NeurIPS\nDatasets and Benchmarks, 2021b.\nCheng-Ping Hsieh, Simeng Sun, Samuel Kriman, Shantanu Acharya, Dima Rekesh, Fei Jia, Yang Zhang,\nand Boris Ginsburg. RULER: What’s the real context size of your long-context language models? CoRR,\nabs/2404.06654, 2024.\n32\n\nYuzhen Huang, Yuzhuo Bai, Zhihao Zhu, Junlei Zhang, Jinghan Zhang, Tangjun Su, Junteng Liu,\nChuancheng Lv, Yikai Zhang, Jiayi Lei, Yao Fu, Maosong Sun, and Junxian He. C-Eval: A multi-\nlevel multi-discipline chinese evaluation suite for foundation models. In NeurIPS, 2023.\nBinyuan Hui, Jian Yang, Zeyu Cui, Jiaxi Yang, Dayiheng Liu, Lei Zhang, Tianyu Liu, Jiajun Zhang, Bowen\nYu, Keming Lu, et al. Qwen2.5-Coder technical report. CoRR, abs/2409.12186, 2024.\nNaman Jain, King Han, Alex Gu, Wen-Ding Li, Fanjia Yan, Tianjun Zhang, Sida Wang, Armando Solar-\nLezama, Koushik Sen, and Ion Stoica. LiveCodeBench: Holistic and contamination free evaluation of\nlarge language models for code. CoRR, abs/2403.07974, 2024.\nZixuan Jiang, Jiaqi Gu, Hanqing Zhu, and David Z. Pan. Pre-RMSNorm and Pre-CRMSNorm Transform-\ners: Equivalent and efficient pre-LN Transformers. CoRR, abs/2305.14858, 2023.\nNathan Lambert, Jacob Morrison, Valentina Pyatkin, Shengyi Huang, Hamish Ivison, Faeze Brahman,\nLester James V. Miranda, Alisa Liu, Nouha Dziri, Shane Lyu, Yuling Gu, Saumya Malik, Victoria Graf,\nJena D. Hwang, Jiangjiang Yang, Ronan Le Bras, Oyvind Tafjord, Chris Wilhelm, Luca Soldaini, Noah A.\nSmith, Yizhong Wang, Pradeep Dasigi, and Hannaneh Hajishirzi. T¨ulu 3: Pushing frontiers in open\nlanguage model post-training. CoRR, abs/2411.15124, 2024.\nTianle Li, Wei-Lin Chiang, Evan Frick, Lisa Dunlap, Tianhao Wu, Banghua Zhu, Joseph E. Gonzalez,\nand Ion Stoica. From crowdsourced data to high-quality benchmarks: Arena-Hard and BenchBuilder\npipeline. CoRR, abs/2406.11939, 2024.\nHunter Lightman, Vineet Kosaraju, Yura Burda, Harri Edwards, Bowen Baker, Teddy Lee, Jan Leike, John\nSchulman, Ilya Sutskever, and Karl Cobbe. Let’s verify step by step. CoRR, abs/2305.20050, 2023.\nBill Yuchen Lin, Ronan Le Bras, Kyle Richardson, Ashish Sabharwal, Radha Poovendran, Peter Clark,\nand Yejin Choi. ZebraLogic: On the scaling limits of LLMs for logical reasoning. CoRR, abs/2502.01100,\n2025.\nAixin Liu, Bei Feng, Bing Xue, Bingxuan Wang, Bochao Wu, Chengda Lu, Chenggang Zhao, Chengqi\nDeng, Chenyu Zhang, Chong Ruan, et al. DeepSeek-V3 technical report. arXiv preprint arXiv:2412.19437,\n2024a.\nJiawei Liu, Chunqiu Steven Xia, Yuyao Wang, and Lingming Zhang. Is your code generated by ChatGPT\nreally correct? Rigorous evaluation of large language models for code generation. In NeurIPS, 2023a.\nQian Liu, Xiaosen Zheng, Niklas Muennighoff, Guangtao Zeng, Longxu Dou, Tianyu Pang, Jing Jiang,\nand Min Lin. RegMix: Data mixture as regression for language model pre-training. arXiv preprint\narXiv:2407.01492, 2024b.\nXiao Liu, Xuanyu Lei, Shengyuan Wang, Yue Huang, Zhuoer Feng, Bosi Wen, Jiale Cheng, Pei Ke, Yifan\nXu, Weng Lam Tam, Xiaohan Zhang, Lichao Sun, Hongning Wang, Jing Zhang, Minlie Huang, Yuxiao\nDong, and Jie Tang. AlignBench: Benchmarking Chinese alignment of large language models. CoRR,\nabs/2311.18743, 2023b.\nMeta-AI. The Llama 4 herd: The beginning of a new era of natively multimodal AI innovation, 2025.\nURL https://ai.meta.com/blog/llama-4-multimodal-intelligence/.\nOpenAI. Hello GPT-4o, 2024. URL https://openai.com/index/hello-gpt-4o/.\nOpenAI. Multilingual massive multitask language understanding, 2024. URL https://huggingface.co\n/datasets/openai/MMMLU.\nOpenAI. Learning to reason with LLMs, 2024. URL https://openai.com/index/learning-to-reaso\nn-with-llms/.\nOpenAI. Introducing openai o3 and o4-mini, 2025. URL https://openai.com/index/introducing-o\n3-and-o4-mini/.\nSamuel J. Paech. Creative writing v3, 2024. URL https://eqbench.com/creative writing.html.\nBowen Peng, Jeffrey Quesnelle, Honglu Fan, and Enrico Shippole. YaRN: Efficient context window\nextension of large language models. CoRR, abs/2309.00071, 2023.\nZihan Qiu, Zeyu Huang, Bo Zheng, Kaiyue Wen, Zekun Wang, Rui Men, Ivan Titov, Dayiheng Liu,\nJingren Zhou, and Junyang Lin. Demons in the detail: On implementing load balancing loss for\ntraining specialized mixture-of-expert models. CoRR, abs/2501.11873, 2025.\n33\n\nShanghaoran Quan, Jiaxi Yang, Bowen Yu, Bo Zheng, Dayiheng Liu, An Yang, Xuancheng Ren, Bofei\nGao, Yibo Miao, Yunlong Feng, Zekun Wang, Jian Yang, Zeyu Cui, Yang Fan, Yichang Zhang, Binyuan\nHui, and Junyang Lin. CodeElo: Benchmarking competition-level code generation of LLMs with\nhuman-comparable Elo ratings. CoRR, abs/2501.01257, 2025.\nQwen Team. QwQ: Reflect deeply on the boundaries of the unknown, November 2024. URL https:\n//qwenlm.github.io/blog/qwq-32b-preview/.\nQwen Team. QwQ-32B: Embracing the power of reinforcement learning, March 2025. URL https:\n//qwenlm.github.io/blog/qwq-32b/.\nDavid Rein, Betty Li Hou, Asa Cooper Stickland, Jackson Petty, Richard Yuanzhe Pang, Julien Dirani,\nJulian Michael, and Samuel R. Bowman. GPQA: A graduate-level Google-proof Q&A benchmark.\nCoRR, abs/2311.12022, 2023.\nAngelika Romanou, Negar Foroutan, Anna Sotnikova, Zeming Chen, Sree Harsha Nelaturu, Shivalika\nSingh, Rishabh Maheshwary, Micol Altomare, Mohamed A. Haggag, Snegha A, Alfonso Amayuelas,\nAzril Hafizi Amirudin, Viraat Aryabumi, Danylo Boiko, Michael Chang, Jenny Chim, Gal Cohen,\nAditya Kumar Dalmia, Abraham Diress, Sharad Duwal, Daniil Dzenhaliou, Daniel Fernando Erazo Flo-\nrez, Fabian Farestam, Joseph Marvin Imperial, Shayekh Bin Islam, Perttu Isotalo, Maral Jabbarishiviari,\nB¨orje F. Karlsson, Eldar Khalilov, Christopher Klamm, Fajri Koto, Dominik Krzeminski, Gabriel Adri-\nano de Melo, Syrielle Montariol, Yiyang Nan, Joel Niklaus, Jekaterina Novikova, Johan Samir Obando\nCeron, Debjit Paul, Esther Ploeger, Jebish Purbey, Swati Rajwal, Selvan Sunitha Ravi, Sara Rydell,\nRoshan Santhosh, Drishti Sharma, Marjana Prifti Skenduli, Arshia Soltani Moakhar, Bardia Soltani\nMoakhar, Ran Tamir, Ayush Kumar Tarun, Azmine Toushik Wasi, Thenuka Ovin Weerasinghe, Serhan\nYilmaz, Mike Zhang, Imanol Schlag, Marzieh Fadaee, Sara Hooker, and Antoine Bosselut. INCLUDE:\nevaluating multilingual language understanding with regional knowledge. CoRR, abs/2411.19799,\n2024.\nRico Sennrich, Barry Haddow, and Alexandra Birch. Neural machine translation of rare words with\nsubword units. In ACL (1). The Association for Computer Linguistics, 2016.\nZhihong Shao, Peiyi Wang, Qihao Zhu, Runxin Xu, Junxiao Song, Mingchuan Zhang, Y. K. Li, Y. Wu, and\nDaya Guo. DeepSeekMath: Pushing the limits of mathematical reasoning in open language models.\nCoRR, abs/2402.03300, 2024.\nFreda Shi, Mirac Suzgun, Markus Freitag, Xuezhi Wang, Suraj Srivats, Soroush Vosoughi, Hyung Won\nChung, Yi Tay, Sebastian Ruder, Denny Zhou, Dipanjan Das, and Jason Wei. Language models are\nmultilingual chain-of-thought reasoners. In ICLR. OpenReview.net, 2023.\nGuijin Son, Jiwoo Hong, Hyunwoo Ko, and James Thorne. Linguistic generalizability of test-time scaling\nin mathematical reasoning. CoRR, abs/2502.17407, 2025.\nJianlin Su, Murtadha H. M. Ahmed, Yu Lu, Shengfeng Pan, Wen Bo, and Yunfeng Liu. Roformer:\nEnhanced Transformer with rotary position embedding. Neurocomputing, 568:127063, 2024.\nMirac Suzgun, Nathan Scales, Nathanael Sch¨arli, Sebastian Gehrmann, Yi Tay, Hyung Won Chung,\nAakanksha Chowdhery, Quoc V. Le, Ed H. Chi, Denny Zhou, and Jason Wei. Challenging BIG-Bench\ntasks and whether chain-of-thought can solve them. In ACL (Findings), pp. 13003–13051. Association\nfor Computational Linguistics, 2023.\nGemma Team, Aishwarya Kamath, Johan Ferret, Shreya Pathak, Nino Vieillard, Ramona Merhej, Sarah\nPerrin, Tatiana Matejovicova, Alexandre Ram´e, Morgane Rivi`ere, et al. Gemma 3 technical report.\narXiv preprint arXiv:2503.19786, 2025.\nChanghan Wang, Kyunghyun Cho, and Jiatao Gu. Neural machine translation with byte-level subwords.\nIn AAAI, pp. 9154–9160. AAAI Press, 2020.\nYiming Wang, Pei Zhang, Jialong Tang, Haoran Wei, Baosong Yang, Rui Wang, Chenshu Sun, Feitong\nSun, Jiran Zhang, Junxuan Wu, Qiqian Cang, Yichang Zhang, Fei Huang, Junyang Lin, Fei Huang, and\nJingren Zhou. PolyMath: Evaluating mathematical reasoning in multilingual contexts, 2025.\nYubo Wang, Xueguang Ma, Ge Zhang, Yuansheng Ni, Abhranil Chandra, Shiguang Guo, Weiming Ren,\nAaran Arulraj, Xuan He, Ziyan Jiang, Tianle Li, Max Ku, Kai Wang, Alex Zhuang, Rongqi Fan, Xiang\nYue, and Wenhu Chen. MMLU-Pro: A more robust and challenging multi-task language understanding\nbenchmark. CoRR, abs/2406.01574, 2024.\n34\n\nColin White, Samuel Dooley, Manley Roberts, Arka Pal, Benjamin Feuer, Siddhartha Jain, Ravid Shwartz-\nZiv, Neel Jain, Khalid Saifullah, Siddartha Naidu, Chinmay Hegde, Yann LeCun, Tom Goldstein, Willie\nNeiswanger, and Micah Goldblum. LiveBench: A challenging, contamination-free LLM benchmark.\nCoRR, abs/2406.19314, 2024.\nYuning Wu, Jiahao Mei, Ming Yan, Chenliang Li, Shaopeng Lai, Yuran Ren, Zijia Wang, Ji Zhang, Mengyue\nWu, Qin Jin, and Fei Huang. WritingBench: A comprehensive benchmark for generative writing. CoRR,\nabs/2503.05244, 2025.\nxAI. Grok 3 beta — the age of reasoning agents, 2025. URL https://x.ai/news/grok-3.\nSang Michael Xie, Hieu Pham, Xuanyi Dong, Nan Du, Hanxiao Liu, Yifeng Lu, Percy S Liang, Quoc V\nLe, Tengyu Ma, and Adams Wei Yu. Doremi: Optimizing data mixtures speeds up language model\npretraining. Advances in Neural Information Processing Systems, 36:69798–69818, 2023.\nWenhan Xiong, Jingyu Liu, Igor Molybog, Hejia Zhang, Prajjwal Bhargava, Rui Hou, Louis Martin, Rashi\nRungta, Karthik Abinav Sankararaman, Barlas Oguz, Madian Khabsa, Han Fang, Yashar Mehdad,\nSharan Narang, Kshitiz Malik, Angela Fan, Shruti Bhosale, Sergey Edunov, Mike Lewis, Sinong Wang,\nand Hao Ma. Effective long-context scaling of foundation models. CoRR, abs/2309.16039, 2023.\nFanjia Yan, Huanzhi Mao, Charlie Cheng-Jie Ji, Tianjun Zhang, Shishir G. Patil, Ion Stoica, and Joseph E.\nGonzalez. Berkeley function calling leaderboard. https://gorilla.cs.berkeley.edu/blogs/8 ber\nkeley function calling leaderboard.html, 2024.\nAn Yang, Baosong Yang, Binyuan Hui, Bo Zheng, Bowen Yu, Chang Zhou, Chengpeng Li, Chengyuan Li,\nDayiheng Liu, Fei Huang, Guanting Dong, Haoran Wei, Huan Lin, Jialong Tang, Jialin Wang, Jian Yang,\nJianhong Tu, Jianwei Zhang, Jianxin Ma, Jianxin Yang, Jin Xu, Jingren Zhou, Jinze Bai, Jinzheng He,\nJunyang Lin, Kai Dang, Keming Lu, Keqin Chen, Kexin Yang, Mei Li, Mingfeng Xue, Na Ni, Pei Zhang,\nPeng Wang, Ru Peng, Rui Men, Ruize Gao, Runji Lin, Shijie Wang, Shuai Bai, Sinan Tan, Tianhang Zhu,\nTianhao Li, Tianyu Liu, Wenbin Ge, Xiaodong Deng, Xiaohuan Zhou, Xingzhang Ren, Xinyu Zhang,\nXipin Wei, Xuancheng Ren, Xuejing Liu, Yang Fan, Yang Yao, Yichang Zhang, Yu Wan, Yunfei Chu,\nYuqiong Liu, Zeyu Cui, Zhenru Zhang, Zhifang Guo, and Zhihao Fan. Qwen2 technical report. CoRR,\nabs/2407.10671, 2024a.\nAn Yang, Baosong Yang, Beichen Zhang, Binyuan Hui, Bo Zheng, Bowen Yu, Chengyuan Li, Dayiheng\nLiu, Fei Huang, Haoran Wei, et al. Qwen2.5 technical report. arXiv preprint arXiv:2412.15115, 2024b.\nAn Yang, Beichen Zhang, Binyuan Hui, Bofei Gao, Bowen Yu, Chengpeng Li, Dayiheng Liu, Jianhong Tu,\nJingren Zhou, Junyang Lin, et al. Qwen2.5-Math technical report: Toward mathematical expert model\nvia self-improvement. CoRR, abs/2409.12122, 2024c.\nYidan Zhang, Boyi Deng, Yu Wan, Baosong Yang, Haoran Wei, Fei Huang, Bowen Yu, Junyang Lin, and\nJingren Zhou. P-MMEval: A parallel multilingual multitask benchmark for consistent evaluation of\nLLMs. CoRR, abs/2411.09116, 2024.\nJeffrey Zhou, Tianjian Lu, Swaroop Mishra, Siddhartha Brahma, Sujoy Basu, Yi Luan, Denny Zhou, and\nLe Hou. Instruction-following evaluation for large language models. CoRR, abs/2311.07911, 2023.\nQin Zhu, Fei Huang, Runyu Peng, Keming Lu, Bowen Yu, Qinyuan Cheng, Xipeng Qiu, Xuanjing Huang,\nand Junyang Lin. AutoLogi: Automated generation of logic puzzles for evaluating reasoning abilities\nof large language models. CoRR, abs/2502.16906, 2025.\n35\n",
      "fetch_method": "direct-pdf"
    }
  ]
}