{
  "model": "baidu/ERNIE-4.5-21B-A3B-Thinking",
  "scores": {
    "1-1 Weights": {
      "score": 1.0,
      "reason": "Hugging Face presence detected; forced Open per FORCE_HF_CORE_OPEN."
    },
    "1-2 Code": {
      "score": 0.0,
      "reason": "No training pipeline files; README mentions are ignored."
    },
    "1-3 License": {
      "score": 1.0,
      "reason": "Model is distributed under Apache-2.0, which allows use, modification, redistribution and commercial use."
    },
    "1-4 Paper": {
      "score": 1.0,
      "reason": "The README cites an official “ERNIE 4.5 Technical Report,” i.e., a provider-authored technical paper about this model."
    },
    "1-5 Architecture": {
      "score": 1.0,
      "reason": "Hugging Face presence detected; forced Open per FORCE_HF_CORE_OPEN."
    },
    "1-6 Tokenizer": {
      "score": 1.0,
      "reason": "Hugging Face presence detected; forced Open per FORCE_HF_CORE_OPEN."
    },
    "2-1 Hardware": {
      "score": 1.0,
      "reason": "Quote gives both type and quantity: “on 2016 NVIDIA H800 GPUs.”"
    },
    "2-2 Software": {
      "score": 0.5,
      "reason": "Training stack partially described: FP8 mixed precision, ZeRO-1, expert/pipeline/tensor parallelism, heterogeneous hybrid strategy, but not a full reproducible list with versions."
    },
    "2-3 API": {
      "score": 1.0,
      "reason": "Only generic or non-official mentions; no qualifying official API docs.  apifind: API_OPEN via https://yiyan.baidu.com/, https://cloud.baidu.com/product/ernie, https://github.com/PaddlePaddle/ERNIE. Baidu's ERNIE Bot 4.5 series, including the 21B-A3B-Thinking model, is accessible via the official ERNIE Bot website and Baidu AI Cloud's Qianfan platform, offering API access for developers. Major version matched."
    },
    "3-1 Pre-training": {
      "score": 0.5,
      "reason": "Multi-stage MoE pre-training strategy, parallelism scheme and FP8 usage are described, but not enough for complete reproducibility."
    },
    "3-2 Fine-tuning": {
      "score": 0.5,
      "reason": "SFT process, topical-domain taxonomy and FP8-QAT fine-tuning are sketched, but full recipe/configs are absent."
    },
    "3-3 Reinforcement Learning": {
      "score": 0.5,
      "reason": "States that PPO-based RL (UPO/GRPO) with a three-stage schedule is used, but lacks full hyper-parameter detail."
    },
    "4-1 Pre-training Data": {
      "score": 0.5,
      "reason": "High-level source list (web pages, papers, images, videos, synthetic) but no released corpora or exact manifest."
    },
    "4-2 Fine-tuning Data": {
      "score": 0.5,
      "reason": "Lists ten topical domains for SFT, yet no concrete datasets or downloads are given."
    },
    "4-3 Reinforcement Learning Data": {
      "score": 0.5,
      "reason": "Mentions logic corpora and unified rewarding system, but provides no dataset release."
    },
    "4-4 Data Filtering": {
      "score": 0.5,
      "reason": "References “comprehensive data-filtering pipelines” and techniques like dynamic sampling and over-long filtering, but omits thresholds or step-by-step specifics necessary for full reproduction."
    }
  },
  "included_scores": {
    "1-1 Weights": {
      "score": 1.0,
      "reason": "Hugging Face presence detected; forced Open per FORCE_HF_CORE_OPEN."
    },
    "1-2 Code": {
      "score": 0.0,
      "reason": "No training pipeline files; README mentions are ignored."
    },
    "1-3 License": {
      "score": 1.0,
      "reason": "Model is distributed under Apache-2.0, which allows use, modification, redistribution and commercial use."
    },
    "1-4 Paper": {
      "score": 1.0,
      "reason": "The README cites an official “ERNIE 4.5 Technical Report,” i.e., a provider-authored technical paper about this model."
    },
    "1-5 Architecture": {
      "score": 1.0,
      "reason": "Hugging Face presence detected; forced Open per FORCE_HF_CORE_OPEN."
    },
    "1-6 Tokenizer": {
      "score": 1.0,
      "reason": "Hugging Face presence detected; forced Open per FORCE_HF_CORE_OPEN."
    },
    "2-1 Hardware": {
      "score": 1.0,
      "reason": "Quote gives both type and quantity: “on 2016 NVIDIA H800 GPUs.”"
    },
    "2-2 Software": {
      "score": 0.5,
      "reason": "Training stack partially described: FP8 mixed precision, ZeRO-1, expert/pipeline/tensor parallelism, heterogeneous hybrid strategy, but not a full reproducible list with versions."
    },
    "2-3 API": {
      "score": 1.0,
      "reason": "Only generic or non-official mentions; no qualifying official API docs.  apifind: API_OPEN via https://yiyan.baidu.com/, https://cloud.baidu.com/product/ernie, https://github.com/PaddlePaddle/ERNIE. Baidu's ERNIE Bot 4.5 series, including the 21B-A3B-Thinking model, is accessible via the official ERNIE Bot website and Baidu AI Cloud's Qianfan platform, offering API access for developers. Major version matched."
    },
    "3-1 Pre-training": {
      "score": 0.5,
      "reason": "Multi-stage MoE pre-training strategy, parallelism scheme and FP8 usage are described, but not enough for complete reproducibility."
    },
    "3-2 Fine-tuning": {
      "score": 0.5,
      "reason": "SFT process, topical-domain taxonomy and FP8-QAT fine-tuning are sketched, but full recipe/configs are absent."
    },
    "3-3 Reinforcement Learning": {
      "score": 0.5,
      "reason": "States that PPO-based RL (UPO/GRPO) with a three-stage schedule is used, but lacks full hyper-parameter detail."
    },
    "4-1 Pre-training Data": {
      "score": 0.5,
      "reason": "High-level source list (web pages, papers, images, videos, synthetic) but no released corpora or exact manifest."
    },
    "4-2 Fine-tuning Data": {
      "score": 0.5,
      "reason": "Lists ten topical domains for SFT, yet no concrete datasets or downloads are given."
    },
    "4-3 Reinforcement Learning Data": {
      "score": 0.5,
      "reason": "Mentions logic corpora and unified rewarding system, but provides no dataset release."
    },
    "4-4 Data Filtering": {
      "score": 0.5,
      "reason": "References “comprehensive data-filtering pipelines” and techniques like dynamic sampling and over-long filtering, but omits thresholds or step-by-step specifics necessary for full reproduction."
    }
  },
  "final_score_10pt": 6.875,
  "meta": {
    "usage_from_dispatch": {
      "fine_tuning": "used",
      "rl": "used"
    },
    "excluded": [],
    "denominator": 16,
    "raw_sum": 11.0,
    "scale": "10/16",
    "code_detection_reason": "No training pipeline files; README mentions are ignored.",
    "pretrain_sources_used": false
  }
}