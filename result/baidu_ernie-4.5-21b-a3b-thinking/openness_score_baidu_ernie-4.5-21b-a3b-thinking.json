{
  "model": "baidu/ERNIE-4.5-21B-A3B-Thinking",
  "scores": {
    "1-1 Weights": {
      "score": 1.0,
      "reason": "Hugging Face presence detected; forced Open per FORCE_HF_CORE_OPEN."
    },
    "1-2 Code": {
      "score": 0.0,
      "reason": "No training pipeline files; README mentions are ignored."
    },
    "1-3 License": {
      "score": 1.0,
      "reason": "Distributed under Apache-2.0 which permits use, modification, redistribution and commercial use."
    },
    "1-4 Paper": {
      "score": 1.0,
      "reason": "An official provider-authored “ERNIE 4.5 Technical Report” is referenced for citation."
    },
    "1-5 Architecture": {
      "score": 1.0,
      "reason": "Hugging Face presence detected; forced Open per FORCE_HF_CORE_OPEN."
    },
    "1-6 Tokenizer": {
      "score": 1.0,
      "reason": "Hugging Face presence detected; forced Open per FORCE_HF_CORE_OPEN."
    },
    "2-1 Hardware": {
      "score": 1.0,
      "reason": "Quote states training used “2016 NVIDIA H800 GPUs,” giving both type and quantity."
    },
    "2-2 Software": {
      "score": 0.5,
      "reason": "Quotes list parallelism scheme (expert, pipeline, ZeRO-1), FP8, recomputation, but do not enumerate the full software stack with versions ⇒ partial disclosure."
    },
    "2-3 API": {
      "score": 0.0,
      "reason": "Only generic or non-official mentions; no qualifying official API docs.  No qualifying official API docs via web search."
    },
    "3-1 Pre-training": {
      "score": 0.5,
      "reason": "Multi-stage strategy, parallelism and FP8 optimisation are explained, but objectives, schedules and full pipeline are not fully reproducible."
    },
    "3-2 Fine-tuning": {
      "score": 0.5,
      "reason": "SFT process, topical taxonomy and FlashMask use are outlined, yet complete hyper-parameters/schedules are missing."
    },
    "3-3 Reinforcement Learning": {
      "score": 0.5,
      "reason": "PPO/UPO recipe and reward-model origin are given, but not enough detail for full reproduction."
    },
    "4-1 Pre-training Data": {
      "score": 0.5,
      "reason": "Sources (web pages, papers, images, video, synthetic) are listed, but no sizes or proportions."
    },
    "4-2 Fine-tuning Data": {
      "score": 0.5,
      "reason": "Ten topical domains are described; datasets and sizes are not."
    },
    "4-3 Reinforcement Learning Data": {
      "score": 0.5,
      "reason": "Mentions ‘logic corpora’ and unified rewarding system, but no concrete corpus breakdown."
    },
    "4-4 Data Filtering": {
      "score": 0.5,
      "reason": "States that ‘comprehensive data-filtering pipelines’ and over-long filtering are used, yet gives no algorithmic detail or thresholds."
    }
  },
  "included_scores": {
    "1-1 Weights": {
      "score": 1.0,
      "reason": "Hugging Face presence detected; forced Open per FORCE_HF_CORE_OPEN."
    },
    "1-2 Code": {
      "score": 0.0,
      "reason": "No training pipeline files; README mentions are ignored."
    },
    "1-3 License": {
      "score": 1.0,
      "reason": "Distributed under Apache-2.0 which permits use, modification, redistribution and commercial use."
    },
    "1-4 Paper": {
      "score": 1.0,
      "reason": "An official provider-authored “ERNIE 4.5 Technical Report” is referenced for citation."
    },
    "1-5 Architecture": {
      "score": 1.0,
      "reason": "Hugging Face presence detected; forced Open per FORCE_HF_CORE_OPEN."
    },
    "1-6 Tokenizer": {
      "score": 1.0,
      "reason": "Hugging Face presence detected; forced Open per FORCE_HF_CORE_OPEN."
    },
    "2-1 Hardware": {
      "score": 1.0,
      "reason": "Quote states training used “2016 NVIDIA H800 GPUs,” giving both type and quantity."
    },
    "2-2 Software": {
      "score": 0.5,
      "reason": "Quotes list parallelism scheme (expert, pipeline, ZeRO-1), FP8, recomputation, but do not enumerate the full software stack with versions ⇒ partial disclosure."
    },
    "2-3 API": {
      "score": 0.0,
      "reason": "Only generic or non-official mentions; no qualifying official API docs.  No qualifying official API docs via web search."
    },
    "3-1 Pre-training": {
      "score": 0.5,
      "reason": "Multi-stage strategy, parallelism and FP8 optimisation are explained, but objectives, schedules and full pipeline are not fully reproducible."
    },
    "3-2 Fine-tuning": {
      "score": 0.5,
      "reason": "SFT process, topical taxonomy and FlashMask use are outlined, yet complete hyper-parameters/schedules are missing."
    },
    "3-3 Reinforcement Learning": {
      "score": 0.5,
      "reason": "PPO/UPO recipe and reward-model origin are given, but not enough detail for full reproduction."
    },
    "4-1 Pre-training Data": {
      "score": 0.5,
      "reason": "Sources (web pages, papers, images, video, synthetic) are listed, but no sizes or proportions."
    },
    "4-2 Fine-tuning Data": {
      "score": 0.5,
      "reason": "Ten topical domains are described; datasets and sizes are not."
    },
    "4-3 Reinforcement Learning Data": {
      "score": 0.5,
      "reason": "Mentions ‘logic corpora’ and unified rewarding system, but no concrete corpus breakdown."
    },
    "4-4 Data Filtering": {
      "score": 0.5,
      "reason": "States that ‘comprehensive data-filtering pipelines’ and over-long filtering are used, yet gives no algorithmic detail or thresholds."
    }
  },
  "final_score_10pt": 6.25,
  "meta": {
    "usage_from_dispatch": {
      "fine_tuning": "used",
      "rl": "used"
    },
    "excluded": [],
    "denominator": 16,
    "raw_sum": 10.0,
    "scale": "10/16",
    "code_detection_reason": "No training pipeline files; README mentions are ignored.",
    "pretrain_sources_used": false
  }
}