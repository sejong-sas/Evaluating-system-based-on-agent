{
  "model_id": "baidu/ernie-4.5-21b-a3b-thinking",
  "full_texts": [
    {
      "arxiv_id": "https://ernie.baidu.com/blog/ernie4.5",
      "full_text": " ERNIE Blog Publication About ERNIE Bot &nbsp; 中文 ERNIE Blog Welcome to ERNIE Blog! ERNIE 4.5 Gets a Major Inference Speed Boost How the new PLAS sparse attention update delivers performance gains for long-context inference on ERNIE 4.5 models. September 12, 2025 &nbsp;·&nbsp;3 min FastDeploy 2.0: A Large-Scale Model Inference and Deployment Toolkit with Native Support for ERNIE 4.5 As large models such as the ERNIE 4.5 family continue to be open-sourced, interest in their inference performance and deployment efficiency has multiplied across both research and industry. FastDeploy 2.0, built on the PaddlePaddle framework, addresses this demand by offering an end-to-end toolkit for efficient deployment and high-performance inference of large models. August 14, 2025 &nbsp;·&nbsp;9 min Announcing the Open Source Release of the ERNIE 4.5 Model Family We introduce ERNIE 4.5, a new family of large-scale multimodal models comprising 10 distinct variants. The model family consist of Mixture-of-Experts (MoE) models with 47B and 3B active parameters, with the largest model having 424B total parameters, as well as a 0.3B dense model. June 30, 2025 &nbsp;·&nbsp;6 min Copyright © 2025 Baidu, Inc. All Rights Reserved. ",
      "fetch_method": "direct-html"
    },
    {
      "arxiv_id": "https://ernie.baidu.com/blog/publication/ERNIE_Technical_Report.pdf",
      "full_text": "ERNIE 4.5 Technical Report\nERNIE Team, Baidu\nernie@baidu.com\nAbstract\nIn this report, we introduce ERNIE 4.5, a new family of large-scale foundation models\ncomprising 10 distinct variants. The model family consists of Mixture-of-Experts\n(MoE) models with 47B and 3B active parameters (the largest containing 424B total\nparameters), as well as a dense model with 0.3B parameters. For the MoE architecture,\nwe propose a novel multimodal heterogeneous structure that supports parameter\nsharing across modalities while also allowing dedicated parameters for each modality.\nThis MoE architecture has the advantage of enhancing multimodal understanding\nwhile also improving performance on text-related tasks. All of our models are trained\nwith optimized efficiency using the PaddlePaddle deep learning framework, which\nenables high-performance inference and streamlined deployment. We achieve 47%\nModel FLOPs Utilization (MFU) during the pre-training of our largest ERNIE 4.5\nlanguage model. Experimental results show that our models achieve state-of-the-\nart performance across multiple text and multimodal benchmarks, particularly in\ninstruction following, world knowledge memorization, visual understanding, and\nmultimodal reasoning. All models are publicly available under the Apache 2.0 license\nto support future research and development in the field. Additionally, we open-source\nthe development toolkits for ERNIE 4.5, which feature industrial-grade capabilities,\nresource-efficient training and inference workflows, and multi-hardware compatibility.\nGithub: https://github.com/PaddlePaddle/ERNIE\nHuggingface: https://huggingface.co/baidu\nJune 29, 2025\n1\n\nContents\n1\nIntroduction\n4\n2\nArchitecture\n5\n2.1\nHeterogeneous MoE\n. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n5\n2.2\nVision Encoder . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n7\n2.3\nAdapter . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n8\n2.4\nMultimodal Position Embedding . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n8\n3\nPre-Training\n8\n3.1\nPre-Training Data . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n8\n3.2\nREEAO: Bitwise-Deterministic Pre-Training Data Manager . . . . . . . . . . . . . . . . . .\n10\n3.3\nPre-Training Recipe . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n10\n3.3.1\nStage I: Text-Only Training\n. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n10\n3.3.2\nStage II: Vision-Only Training . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n11\n3.3.3\nStage III: Joint Multimodal Training . . . . . . . . . . . . . . . . . . . . . . . . . . .\n12\n3.4\nModel Optimization\n. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n12\n3.4.1\nRouter Orthogonalization Loss . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n12\n3.4.2\nToken-Balanced Loss . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n12\n3.5\nExponential Moving Average . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n13\n4\nPost-Training\n14\n4.1\nPost-Training of LLMs . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n14\n4.1.1\nSupervised Fine-Tuning . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n14\n4.1.2\nUnified Rewarding System\n. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n15\n4.1.3\nReinforcement Learning . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n16\n4.2\nPost-Training of VLMs . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n17\n4.2.1\nSupervised Fine-tuning\n. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n17\n4.2.2\nReinforcement Learning with Verifiable Rewards\n. . . . . . . . . . . . . . . . . . .\n18\n5\nTraining Framework\n19\n5.1\nHeterogeneous Parallelism for Multimodal Model Training . . . . . . . . . . . . . . . . . .\n20\n5.1.1\nHeterogeneous Parallelism Architecture . . . . . . . . . . . . . . . . . . . . . . . . .\n20\n5.1.2\nHierarchical Load Balance Strategy\n. . . . . . . . . . . . . . . . . . . . . . . . . . .\n21\n5.2\nHybrid Parallelism for MoE Backbone . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n22\n5.2.1\nIntra-Node Expert Parallelism\n. . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n23\n5.2.2\nMemory-Efficient Pipeline Scheduling . . . . . . . . . . . . . . . . . . . . . . . . . .\n23\n5.3\nFP8 Mixed Precision Training . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n24\n5.4\nComputational Optimizations . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n25\n5.4.1\nRecomputation with Best Computation-Memory Tradeoffs . . . . . . . . . . . . . .\n25\n5.4.2\nFlashMask for Flexible Attention Mask and Long Context Training . . . . . . . . .\n26\n5.5\nFramework-Native Fault Tolerance System . . . . . . . . . . . . . . . . . . . . . . . . . . .\n26\n6\nInference and Deployment\n28\n6.1\nQuantization . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n28\n6.1.1\nW4A8 Quantization\n. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n28\n6.1.2\n2-Bit Quantization\n. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n30\n6.1.3\nAttention and KV Cache Quantization . . . . . . . . . . . . . . . . . . . . . . . . . .\n30\n6.2\nInference Acceleration\n. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n32\n6.2.1\nW4A8 Kernel Acceleration . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n32\n6.2.2\nEfficient Attention Kernel . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n33\n6.2.3\nSpeculative Decoding\n. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n34\n6.3\nDeployment . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n34\n7\nOpen-Source Development Tools\n35\n7.1\nERNIEKit . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n35\n7.2\nFastDeploy . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n35\n8\nEvaluation and Results\n36\n8.1\nEvaluation of Language Models . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n36\n8.1.1\nResults of Pre-Trained Language Models . . . . . . . . . . . . . . . . . . . . . . . .\n36\n8.1.2\nResults of Post-Trained Language Models . . . . . . . . . . . . . . . . . . . . . . . .\n37\n8.2\nEvaluation of Multimodal Models\n. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n38\n2\n\n9\nConclusion\n41\nA Appendix\n42\nA.1 Ablation Study for Router Orthogonalization Loss . . . . . . . . . . . . . . . . . . . . . . .\n42\nA.2 EMA in Terms of Update Deltas . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n42\nA.3 Effective Decay Window of EMA . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n42\nB\nQualitative examples\n43\nB.1\nOCR Parsing and Document Understanding I . . . . . . . . . . . . . . . . . . . . . . . . . .\n44\nB.2\nOCR Parsing and Document Understanding II . . . . . . . . . . . . . . . . . . . . . . . . .\n45\nB.3\nMultilingual OCR Parsing . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n46\nB.4\nVideo Temporal Grounding . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n47\nB.5\nOCR Ancient Chinese Character Recognition . . . . . . . . . . . . . . . . . . . . . . . . . .\n48\nB.6\nReasoning Cases: Chemistry . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n49\nB.7\nReasoning Cases: Math . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n50\nB.8\nReasoning Cases: Deep Semantic Image Understanding . . . . . . . . . . . . . . . . . . . .\n51\nB.9\nVisual Reasoning: Visual Pattern Recognition . . . . . . . . . . . . . . . . . . . . . . . . . .\n52\nB.10 Visual Reasoning: Emoji Quiz . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n53\nB.11 Visual Reasoning: Depth Sorting . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n54\nB.12 Visual Reasoning: Counting . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n54\nB.13 Common Sense Reasoning . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n55\nB.14 Code Synthesis\n. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n56\nB.15 Image Conditioned Creative Writing . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n57\n3\n\nERNIE 4.5 Technical Report\nJune 29, 2025\n1\nIntroduction\nIn recent years, the field of artificial intelligence has witnessed remarkable progress, largely driven by\nthe development of large-scale foundation models. These models, powered by massive datasets and\nadvanced training techniques, have demonstrated unprecedented capabilities across a wide range of\ndomains. In the field of text understanding and reasoning, models such as GPT-4.1 (OpenAI, 2025a),\nGPT-4.5 (OpenAI, 2025b), o3 (OpenAI, 2025d), Qwen-3 (Yang et al., 2025a), DeepSeek-V3 (DeepSeek-AI\net al., 2024b), DeepSeek-R1 (DeepSeek-AI et al., 2025), Claude 4 (Anthropic, 2025), Gemini 2.5 (DeepMind,\n2025), and Llama-4 (Meta-AI, 2025) have set new state-of-the-art records, demonstrating impressive\ncapabilities in comprehension, reasoning, and problem-solving. For multimodal understanding, models\nlike GPT-4 series (OpenAI, 2024; 2025a;b), Gemini 2.5 (DeepMind, 2025), Gemma 3 (Kamath et al., 2025),\nand Qwen2.5-VL (Bai et al., 2025) have extended these abilities to visual data, enabling robust visual\nreasoning and interpretation. These models have not only set new benchmarks in natural language\nprocessing, image comprehension, and video understanding, but have also facilitated the emergence of\npowerful applications in reasoning, conversation, and creative generation.\nIn this paper, we present a new family of foundation models: ERNIE 4.5. The ERNIE 4.5 models listed\nin Table 1 include several Mixture-of-Experts (MoE) models and two dense models. Specifically, except\nfor the 0.3B dense language models, all other models in this family are MoE-based. Unlike traditional\nunimodal MoE models, ERNIE 4.5 adopts a novel multimodal heterogeneous structure. This structure\nsupports parameter sharing across modalities, including self-attention parameter sharing and expert\nparameter sharing, while also allowing for separate parameters for each modality. To further enhance\nperformance, we propose a modality-isolated routing technique and employ multimodal joint pre-\ntraining. These innovations enable ERNIE 4.5 to efficiently learn visual information through dedicated\nvision experts and improve the reasoning capabilities of language models. Based on this architecture,\nour MoE models can be categorized into Large Language Models (LLMs) and Vision-Language Models\n(VLMs). Since parameters are separated for each modality, LLMs have fewer total parameters than VLMs.\nModel\nMultimodal\nMoE\nPost-Trained\nThinking / Non-Thinking Mode\nERNIE-4.5-300B-A47B-Base\n✗\n✓\n✗\n-\nERNIE-4.5-300B-A47B\n✗\n✓\n✓\nnon-thinking\nERNIE-4.5-21B-A3B-Base\n✗\n✓\n✗\n-\nERNIE-4.5-21B-A3B\n✗\n✓\n✓\nnon-thinking\nERNIE-4.5-0.3B-Base\n✗\n✗\n✗\n-\nERNIE-4.5-0.3B\n✗\n✗\n✓\nnon-thinking\nERNIE-4.5-VL-424B-A47B-Base\n✓\n✓\n✗\n-\nERNIE-4.5-VL-424B-A47B\n✓\n✓\n✓\nboth\nERNIE-4.5-VL-28B-A3B-Base\n✓\n✓\n✗\n-\nERNIE-4.5-VL-28B-A3B\n✓\n✓\n✓\nboth\nTable 1: Overview of the ERNIE 4.5 family.\nAll of our models are trained using the PaddlePaddle framework. We efficiently pre-train ERNIE 4.5\nusing a heterogeneous hybrid parallelism approach and a hierarchical load balancing solution tailored\nto multimodal large models. Through our extreme optimizations, including efficient intra-node expert\nparallelism, FP8 mixed-precision training, and fine-grained recomputation methods, we achieve 47%\nModel FLOPs Utilization (MFU) when pre-training our largest ERNIE 4.5 language model on 2016\nNVIDIA H800 GPUs. Our training approach enables optimal training performance with limited compute\nresources, such as 96 GPUs.\nThe ERNIE 4.5 series consists of MoE and dense models with different parameter sizes, suitable for\nvarious deployment scenarios. All models are capable of performing inference directly using BF16\nand FP8 precision. To improve inference efficiency, we propose a near-lossless, inference-friendly low-\nbit quantization solution with hardware-optimized operators to reduce memory usage and accelerate\ncomputation. The overall size of parameters and the compression techniques allow our largest ERNIE 4.5\nmodel to be conveniently deployed with minimal computational resources (4x 80GB GPUs for 4-bit, 1x\n141GB GPU for 2-bit). By further adopting Prefill-Decode (PD) disaggregation with expert parallelism,\nthe largest ERNIE 4.5 language model achieves an inference throughput of 56k input TPS (Tokens Per\nSecond) and 18k output TPS per H800 node.\nOur model family is characterized by three key innovations:\n1. Multimodal Heterogeneous MoE Pre-Training: Our models are jointly trained on both textual\nand visual modalities to learn from multimodal information and improve performance on tasks\n4\n\nERNIE 4.5 Technical Report\nJune 29, 2025\ninvolving text understanding and generation, image understanding, and cross-modal reasoning.\nTo achieve this without interference between modalities, we design a heterogeneous MoE structure,\nincorporate modality-isolated routing, and employ both router orthogonalization loss and multimodal\ntoken-balanced loss. This architecture ensures that both modalities are effectively represented,\nenabling mutual reinforcement during training.\n2. Scaling-Efficient Infrastructure: We propose a novel heterogeneous hybrid parallelism and\nhierarchical load balancing strategy for efficient training of ERNIE 4.5 models. Through intra-\nnode expert parallelism, memory-efficient pipeline scheduling, FP8 mixed-precision training,\nand fine-grained recomputation methods, we achieve remarkable pre-training throughput. For\ninference, we propose a multi-expert parallel collaboration method and a convolutional code quanti-\nzation algorithm to achieve 4-bit/2-bit near-lossless quantization. Furthermore, we introduce\nPD disaggregation with dynamic role switching for effective resource utilization to enhance\ninference performance for ERNIE 4.5 MoE models. Built on PaddlePaddle, ERNIE 4.5 delivers\nhigh-performance inference across a wide range of hardware platforms.\n3. Modality-Specific Post-Training: To meet the diverse requirements of real-world applications,\nwe fine-tune variants of the pre-trained model for specific modalities. Our LLMs are optimized for\ngeneral-purpose language understanding and generation. The VLMs focus on vision-language\nunderstanding and support both thinking and non-thinking modes. Each model employs a\ncombination of Supervised Fine-tuning (SFT), Direct Preference Optimization (DPO) or a modified\nreinforcement learning method named Unified Preference Optimization (UPO) for post-training.\nWe conduct extensive evaluations of our models across a wide range of benchmarks, covering language\nunderstanding and generation, reasoning, and multimodal tasks. Our models achieve strong perfor-\nmance, especially in instruction following, world knowledge memorization, visual understanding, and\nmultimodal reasoning.\nAll models in this release, including model weights and development toolkits, are fully open source\nto encourage broad adoption and collaborative research. In the following sections of this report, we\nwill provide detailed descriptions of our model architecture, training procedures, and comprehensive\nevaluation results. We hope that our efforts contribute meaningfully to the research community and help\naccelerate progress in the field of large-scale foundation models.\n2\nArchitecture\nFigure 1 illustrates the Transformer architecture adopted by ERNIE 4.5. It supports image, video, and text\nmodalities as input and generates text as output. For visual modalities (images and videos), a variable-\nresolution ViT encoder is employed, followed by an adapter that projects representations into a shared\nembedding space with text. Subsequently, ERNIE 4.5 employs a fine-grained Mixture-of-Experts (MoE)\narchitecture with multimodal positional embeddings to model unified hidden states across modalities.\nThe key components of the architecture include:\n• Heterogeneous MoE: Text and vision features are routed to separate sets of experts, while both\nmodalities also pass through a group of shared experts, as well as all self-attention parameters.\nThe visual experts contain one-third of the parameters of the textual experts.\n• Vision Encoder: The vision encoder is equipped with an adaptive-resolution transformation and\n2D rotary position embedding (RoPE).\n• Adapter: The adapter aligns the representations from the visual and textual modalities, and\nincorporates both spatial and temporal compression.\n• Multimodal Positional Embedding: We utilize 3D RoPE within the multimodal model, encoding\ntemporal, width, and height positions independently.\n2.1\nHeterogeneous MoE\nERNIE 4.5 is built upon a fine-grained MoE backbone. Text and vision inputs are routed to distinct sets of\nexperts tailored to their respective characteristics, which mitigates cross-modal interference. A subset of\nshared experts, along with all self-attention layers, is maintained for all tokens to facilitate cross-modal\nknowledge integration. Additionally, we introduce a modality-aware expert allocation strategy where\nvisual experts contain only one-third of the parameters of textual experts, thereby improving the efficiency\nof visual information processing.\nIn multimodal modeling, the MoE router is prone to instability, especially during sudden shifts in data\ndistribution. For instance, extending a text-only MoE model to handle multimodal inputs may cause the\n5\n\nERNIE 4.5 Technical Report\nJune 29, 2025\nVision Experts\nMultimodal Self-Attention\nText Experts\nxN\nERNIE 4.5\nShared Experts\nVision Router\nText Router\n[512px, 480px] video1: ( 5 minutes ) \nVision Encoder\nAdaptive Resolution Transform\nTemporal Compression\n[1080px,112px] image1: Large Aspect Ratio\nSpatial Compression \nAdapter\n  video1  \n  image1  \n  image2  \n[3000px, 2000px] image2: \nHigh Resolution\n......\nText Tokenizer\nDescribe this image in detail\nDescribe\nthis\nin\nimage\ndetail\nFigure 1: Architecture of ERNIE 4.5 supporting image, video, and text inputs with text outputs. The sys-\ntem comprises three core components: (1) Heterogeneous MoE routing text features (red) to text-specific\nexperts and visual features (blue) to vision-specific experts, while shared experts and self-attention\nparameters (purple) process unified cross-modal hidden states; (2) Adapter projecting visual represen-\ntations into shared embedding space via dual-compression layers; (3) Vision encoder implementing\nadaptive-resolution ViT for images/videos, featuring dynamic frame-resolution sampling and times-\ntamps rendered at top-left corners.\nrouter to collapse, leading to a degradation of textual capabilities (Liang et al., 2025). To mitigate this\nissue, we propose a modality-isolated routing strategy. Figure 2 presents heatmaps of expert activation\nratios across different layers on a hold-out dataset. It illustrates that textual experts exhibit concentrated\nactivations while visual experts display more dispersed activation patterns. These findings substantiate\nthe necessity of modality-separated MoE designs to ensure effective multimodal joint training.\nSpecifically, the FFN experts in ERNIE 4.5 are categorized into three types: text experts, vision experts,\nand shared experts. Both text and vision tokens are processed by the shared experts in a non-routed\nmanner, while each is independently routed to its corresponding modality-specific experts. In other\nwords, text experts exclusively process text tokens, vision experts exclusively process vision tokens, and\nshared experts operate on both modalities. The final output of the MoE layer is obtained by summing the\noutputs from both the shared and modality-specific experts.\nSeveral engineering decisions further enhance the scalability and efficiency of our approach. Given the\nredundant nature of visual tokens, experts in the vision group use an intermediate dimension that is\none-third the size of that used by text experts. Since the FLOPs of an FFN layer scale with the product of\nthe input and intermediate dimensions, this adjustment achieves a roughly 66% reduction in per-token\n6\n\nERNIE 4.5 Technical Report\nJune 29, 2025\nFigure 2: Expert usage across Transformer layers. Left: text modality expert group; right: vision modality\nexpert group.\nFFN computation for vision tokens. We further exclude visual experts from the final Transformer layer,\nas their weights do not contribute to the cross-entropy loss. To address the device-level load imbalance\nproblem caused by modality-isolated experts, we design a custom load-aware expert parallelism strategy,\ndetailed in Section 5.2.1.\nThe heterogeneous MoE design brings several key advantages:\n• Unified Multimodal Modeling: It enables the construction of a unified multimodal model in\nwhich all parameters—textual and visual—are optimized jointly. Compared to partial-tuning\napproaches, this design is more data-efficient and scalable, supporting model growth up to\nhundreds of billions of parameters.\n• Routing Stability: Vision experts can be introduced in the later stages of training, thereby\navoiding routing collapse. This staged training reduces overall computation while preserving\nperformance, since visual understanding largely builds on prior textual knowledge.\n• Computational Efficiency: Text and vision experts can be deployed separately. In text-only infer-\nence scenarios, vision experts can be skipped to reduce memory overhead. For multimodal infer-\nence, we support modality-aware partitioning of the inference pipeline. Specifically, allocating\ndifferent inference budgets for each modality and deploying the Prefill-Text, Prefill-Vision,\nand Decode-Text modules independently can significantly reduce cross-device communication.\nCompared to previous approaches, our modality-isolated fine-grained MoE strategy introduces several\nimportant innovations. Unlike Wang et al. (2024b) and Wang et al. (2022), which use dense FFNs, we\nadopt a fine-grained MoE backbone that offers improved scalability. Unlike Liang et al. (2025), we retain\ndense attention layers to preserve cross-modal interactions while restricting MoE routing to the FFN\nlayers. In contrast to Lin et al. (2024d), which relies on expert choice gating mechanisms, we employ\nfine-grained top-k routing (DeepSeek-AI et al., 2024a;b). This approach maintains compatibility with\nautoregressive decoding and enables scaling to long context training.\n2.2\nVision Encoder\nImage Encoding.\nVision Transformers (ViTs) are widely employed as vision encoders in vision-language\nmodels (Radford et al., 2021b; Zhai et al., 2023; Fang et al., 2024; Sun et al., 2023). However, existing ViTs\nare typically pre-trained on fixed-resolution inputs, which requires resizing images to a square shape\nbefore processing. In this work, we employ an adaptive-resolution vision encoder (Dehghani et al.,\n2023b; Wang et al., 2024a; Guo et al., 2025). Instead of enforcing a square input, we independently resize\nthe height and width of each input image to the nearest multiples of the ViT patch size. This approach\napproximately preserves the original aspect ratio, avoiding the distortions caused by fixed-size resizing.\nThe adaptively resized image is then divided into patches, resulting in a variable-length 1D sequence\nof tokens. To encode the 2D spatial origin of each patch, we employ 2D Rotary Position Embedding\n(RoPE) (Su et al., 2024b; Wang et al., 2024a; Guo et al., 2025), which separately encodes spatial information\nalong the height and width dimensions. Additionally, we adopt the image packing technique (Dehghani\net al., 2023b), which efficiently packs multiple images into a single batch while maintaining positional\nconsistency among patches. This enables a more effective utilization of computational resources without\nrequiring modifications to the model architecture.\nVideo Encoding.\nVideos are processed as sequences of sampled frames, but this quickly exhausts the\nmodel’s limited sequence length budget and makes comprehensive temporal coverage challenging. We\n7\n\nERNIE 4.5 Technical Report\nJune 29, 2025\npropose an adaptive frame-resolution sampling strategy that dynamically adjusts both the number of\nframes and their spatial resolution based on each video’s duration and the available sequence length.\nSpecifically, frames are sampled at a predefined frame rate; if this exceeds the upper or lower frame count\nlimits, uniform sampling is applied at the respective bound. If the total number of visual tokens exceeds\nthe limit, we reduce the resolution until it reaches a lower bound, and if necessary, further decrease the\nnumber of frames. For multi-video inputs, the frame allocation is proportional to each video’s length.\nThis approach maximizes the use of sequence length, offering higher detail for short videos and adequate\nframe coverage for long videos.\nTo further enhance temporal modeling, we introduce a timestamp rendering technique that overlays\nabsolute timestamps onto each frame. Unlike position embeddings (Bai et al., 2025) or textual token-based\ntime encoding (Guo et al., 2025; Hong et al., 2024), our method is flexible to any frame rate, consumes no\nextra tokens, and provides the model with explicit temporal cues directly in the visual stream. This direct\nsupervision reduces learning difficulty and enables more accurate temporal understanding.\n2.3\nAdapter\nTo align visual and textual representations in a unified embedding space, we design an Adapter that\nserves as a modality bridging module between the visual encoder and the language model. The Adapter\nincorporates spatial and temporal token compression to perform feature fusion and reduce sequence\nlength. Specifically, the spatial compression operates on non-overlapping 2×2 patches, yielding a 4×\ndecrease in token count along spatial dimensions (Chen et al., 2024b; Wang et al., 2024a), while the\ntemporal compression reduces the sequence length by a factor of 2. Both compression operations leverage\npixel shuffle (Shi et al., 2016), which rearranges spatially or temporally adjacent token features into a more\ncompact form. The rearranged features are then processed through MLP layers. To unify the processing\nof images and videos, each static image is treated as a synthetic two-frame video by duplicating the\nimage feature, enabling consistent temporal modeling across modalities. Overall, the Adapter not only\nperforms efficient token compaction through spatio-temporal fusion but, more importantly, aligns the\nmultimodal feature space to the textual embedding space, facilitating deeper cross-modal interaction in\nthe subsequent transformer layers.\n2.4\nMultimodal Position Embedding\nTo effectively handle multimodal sequences of text, images, and videos, we employ a unified 3D RoPE\nposition embedding scheme for the input layer of the vision-language transformer. 3D RoPE encodes\ntemporal and spatial positions separately by assigning distinct frequency bands to each axis for visual\ninputs, and defaults to standard 1D RoPE for text tokens (Wang et al., 2024a). Specifically, lower\nfrequencies are allocated to the temporal axis (which varies most slowly), and the remaining frequencies\nare interleaved between the spatial axes (height and width), enabling both symmetric spatial modeling\nand strong long-term temporal modeling (Wei et al., 2025). Unlike conventional 2D RoPE used for images\nand 1D RoPE used for text, our method accommodates the extra temporal dimension in videos, enabling\nflexible and consistent position encoding across all modalities within a single embedding space.\n3\nPre-Training\nIn this section, we first describe the construction of our large-scale text and multimodal datasets (Section\n3.1). Next, we introduce a data manager REEAO (Record Everything Everywhere All at Once) that\nsupports bitwise-deterministic data processing, enabling reproducible and non-redundant data access\nacross training runs (Section 3.2). Subsequently, we present the training recipes for both textual and\nmultimodal pre-training stages (Section 3.3). We also detail our innovations in the training objective\n(Section 3.4), and conclude by sharing our findings regarding the EMA process (Section 3.5).\n3.1\nPre-Training Data\nERNIE 4.5 models are trained on data curated from web pages, academic papers, documents, images,\nvideos, and synthetic data. Given the diversity and noise inherent in the raw datasets (Awadalla et al.,\n2024; Kim et al., 2022), we implement comprehensive data filtering pipelines for text, images, videos, and\naudio. These pipelines primarily involve deduplication and the removal of noisy or irrelevant content.\nSubsequently, we perform data labeling and clustering for further data analysis. This process enables\nknowledge-based analysis, more effective data management, and improved training data composition.\nFinally, we evaluate the performance of models trained on the data to identify weaknesses for further data\noptimization, establishing a human-model-in-the-loop iterative cycle for data refinement. The following\npresents the key steps of this process.\n8\n\nERNIE 4.5 Technical Report\nJune 29, 2025\n• Data Denoising and Synthesis: Our data filtering pipeline involves heuristic rules and model-\nbased approaches. We use heuristic rules to perform deduplication and low-quality data removal,\nand employ model-based filtering method for quality assessment, which automatically filters\nlow-quality samples to ensure data quality. While noise filtering is essential for ensuring data\nquality, it inevitably reduces the overall data volume, resulting in a trade-off between quality and\nquantity. To address this challenge, we propose data synthesis strategies to augment high-value\ndata. Specifically, we leverage techniques such as self-distillation and multimodal transformation\nto diversify and enrich data sources, effectively mitigating the issue of data scarcity.\n• Data Analysis: To better understand and manage data, we meticulously construct a pre-training\ndata map to support data mining and data analysis. This data map organizes the data based\non multiple aspects, including language, knowledge, domain, application scenarios, and data\nquality. Through comprehensive multi-dimensional data analysis, we refine training data mixing\nstrategies, enable staged data selection, and improve model performance monitoring throughout\nthe training process.\n• Human-Model-in-the-Loop Data Refinement: We design a human-model-in-the-loop pipeline\nto continuously improve data quality. The pipeline consists of core dataset selection, candidate\nmodel identification for data evaluation, and manual analysis of the results. Through this data\niteration loop, we can ensure the effectiveness of each filtering and data mining strategy, thereby\nimproving the overall quality of text, image, and video data.\nIn the following section, we describe the construction of various data sources, including knowledge-\ncentric data, multimodal aligned data, and domain-specific data.\nKnowledge-Based Data.\nThrough large-scale data analysis, we observe that the distribution of knowl-\nedge contained in natural language corpora is inherently uneven and can be systematically categorized\ninto multiple levels. Inspired by the DIKW framework (Wikipedia, 2025), we define five distinct tiers of\nknowledge and develop a knowledge-level classification model to automatically annotate pre-training\ndata according to these levels. This classification framework enables a deeper analysis of the value\ndistribution within pre-training datasets.\nBased on this framework, our analysis reveals that high-value data is scarce and constitutes only a small\nfraction of available corpora. To address this scarcity, we employ data synthesis to augment the limited\nhigh-value training data. Specifically, we propose a key-point-based data synthesis method capable of\ngenerating diverse and high-quality samples across domains such as mathematics (Wei et al., 2024b; Yang\net al., 2025b; Li et al., 2024b; Yu et al., 2024), factual knowledge (Gunasekar et al., 2023; Wettig et al., 2024),\nand programming code (Chang et al., 2024; Wei et al., 2024b). To ensure broad knowledge coverage,\nwe use textbooks and educational websites as seeds to extract structured key points that guide the data\ngeneration process.\nFurthermore, to enhance the model’s performance on reasoning tasks, we perform multiple processing\nsteps on reasoning-related corpora. These steps involve selecting representative samples, organizing them\nby reasoning type and difficulty level, and filtering low-quality items to retain the valuable instances (Sun\net al., 2021; Xie et al., 2023; Su et al., 2024a; Yang et al., 2025b). Empirical results demonstrate that these\nprocessing steps substantially improve the model’s reasoning capabilities.\nInterleaved Text-Image Data.\nInterleaved text-image data plays a crucial role in enhancing multimodal\nlearning capabilities (Alayrac et al., 2022; Lin et al., 2024c). However, existing datasets suffer from limited\nscale and weak alignment between visual and textual components. To address these challenges, we\ndevelop a comprehensive data curation strategy. First, we systematically collect extensive web data\nby identifying high-quality web pages and documents with well-integrated visual elements. Next,\nwe augment this dataset by extracting interleaved content from video sources, leveraging the rich\nexplanatory knowledge embedded in online videos through keyframe extraction and Automatic Speech\nRecognition (ASR) (Miech et al., 2019; Xu et al., 2023; Zellers et al., 2021). Finally, we implement\nrigorous quality enhancement procedures, including intra-page image-text deduplication, filtering of\nlow-resolution images and irrelevant content, removal of garbled text and advertisements, and detection\nof disordered content. This multi-faceted strategy greatly enriches our interleaved dataset, resulting in\nnotable improvements in the model’s knowledge capacity and multimodal understanding.\nImage-Text Pairs.\nImage-text pairs are essential for learning transferable representations in vision-\nlanguage models (Radford et al., 2021a). Although such data is abundant on the internet (Schuhmann\net al., 2022), it suffers from noise such as irrelevant descriptions, trivial or generic captions, redundant\nimages, and poor data quality. To address these challenges, we employ filtering techniques, including\nimage-text similarity scoring (e.g. CLIP-score thresholding) to remove low-relevance pairs, followed\n9\n\nERNIE 4.5 Technical Report\nJune 29, 2025\nby deduplication of both images and text (Zauner, 2010; Abbas et al., 2023). We then perform image\nclassification and tagging, categorizing images into natural scenes, tables, screenshots, charts, documents,\nand other types. Finally, we sample a subset of the data for recaptioning (Betker et al.) to improve the\nquality of image-text alignment. To further enhance grid-style image understanding, we synthesize\ntraining samples by stitching multiple images into a grid layout and concatenating the corresponding\ncaptions into a matching structure. This strategy helps the model better localize and interpret information\nin composite visual inputs.\nDomain-Specific Data.\nTo strengthen the model’s capability in domain-specific tasks, we construct\nlarge-scale datasets covering domains such as industry, finance, healthcare, and consumer entertainment.\nGiven the scarcity and specialized nature of high-quality domain data, we develop a diversified data\nsourcing strategy to address these limitations. Our approach encompasses two primary data sources:\n• Progressive Mining and Conditional Training: Inspired by Shao et al. (2024), we employ pro-\ngressive mining methods to systematically extract large amounts of domain-specific data. During\npre-training, we find that conditional pre-training schemes (Korbak et al., 2023) significantly\nimprove learning efficiency for this domain data, particularly in creative writing tasks.\n• Audio Transcription and Enhancement: We use Automatic Speech Recognition (ASR) models to\ntranscribe valuable domain-specific content from audio sources such as video soundtracks and\npodcasts. To refine the transcriptions and enrich our dataset with colloquial and conversational\nlanguage, we develop a robust rewriting and filtering pipeline.\n3.2\nREEAO: Bitwise-Deterministic Pre-Training Data Manager\nTraining large language models at scale involves processing trillions of tokens across heterogeneous\ndatasets and leveraging dynamically changing computational infrastructures. Large-scale training often\nencounters frequent interruptions and changes, such as resuming from checkpoints, recovering from\nnode failures, adjusting computational resources, handling variable sequence lengths, and updating\ndatasets. These variations usually disrupt the underlying data pipeline, potentially resulting in significant\nissues such as inadvertent data duplication or omission.\nTo address these challenges, we introduce REEAO (Record Everything Everywhere All at Once), a\ndata flow manager built on five core principles: Reproducibility, Efficiency, Elasticity, Adaptivity, and\nObservability. REEAO chunks multimodal data sources into fixed-length records and fundamentally\nguarantees that the training process produces a bitwise-deterministic token sequence which is fully\ndetermined immediately after the pre-training data is configured and before actual training begins.\nIn addition, REEAO maintains a distributed-independent record of data source consumption to ensure\nthat no data is duplicated. This guarantee holds even under complex scenarios, such as changes in the\nnumber of training nodes, distribution strategies, global batch size, or context length, and remains robust\nwhen resources are scaled dynamically or training data is updated on the fly.\n3.3\nPre-Training Recipe\nWe develop a series of Transformer-based models with varying scales, attention configurations, and\noptional MoE modules, adopting architectural choices tailored to each model’s parameter budget. Table 2\nsummarizes the architectural hyperparameters and scale-aware training settings for the ERNIE 4.5 family.\nSpecifically, we present key architectural decisions alongside training hyperparameters such as batch\nsize, learning rate, and optimization strategies. All models use the Warmup-Stable-Decay learning rate\nschedule (Hu et al., 2024) during pre-training, with configurations specific to each model.\nTo ensure stability in multimodal joint training, we design a multi-stage pre-training strategy for ERNIE\n4.5, as shown in Table 3.\n3.3.1\nStage I: Text-Only Training\nThis stage focuses on establishing a robust language backbone, ensuring that the model possesses strong\nlinguistic understanding and can efficiently capture long-range dependencies. In particular, this stage\ncomprises both short-context and long-context training phases:\n• Short-Context: We begin with large-scale pre-training on trillions of text-only tokens sourced\nfrom diverse domains. This sub-stage focuses on developing fundamental language under-\nstanding, factual knowledge acquisition, and effective text generation, all under a standard\nshort-context configuration with an 8k context length.\n10\n\nERNIE 4.5 Technical Report\nJune 29, 2025\nModels\nERNIE-4.5-A47B-Base ERNIE-4.5-A3B-Base ERNIE-4.5-0.3B-Base\nText Params\n300B\n21B\n0.36B\nTotal Params\n424B\n28B\n–\nLayers\n54\n28\n18\nHeads (Q/KV)\n64/8\n20/4\n16/2\n# Text Experts (Total/Activated)\n64/8\n64/6\n–\n# Vision Experts (Total/Activated)\n64/8\n64/6\n–\n# Shared Experts\n–\n2\n–\nContext Length\n131,072\n131,072\n131,072\nLearning Rate\n2.2e-4\n3.14e-4\n4.4e-4\nBatch Size\n65M\n50M\n8M\nWeight Decay\n0.1\n0.1\n0.1\nGradNorm Clip\n1.0\n1.0\n1.0\nViT Layer-Wise LR Decay\n0.9\n0.9\n-\nTable 2: Model and training hyperparameters of three ERNIE-4.5-Base models.\nStages\nTrainable Parameters\nSequence Length\nStage I: Text-Only Training\nShort-Context\nLLM\n4,096\nLong-Context\nLLM\n32,768 →131,072\nStage II: Vision-Only Training\nVision Encoder\nViT\n8,192\nVision Pre-Alignment\nAdapter + Vision Experts\n8,192\nVision Integration\nViT + Adapter + Vision Experts\n8,192\nStage III: Joint Multimodal Training\nShort-Context Multimodal\nFull Model\n8,192\nLong-Context Multimodal\nFull Model\n131,072\nTable 3: The training stages of our ERNIE-4.5-VL-424B-A47B-Base and ERNIE-4.5-VL-28B-A3B-Base\nmodels.\n• Long-Context: To extend the model’s context length to 128k tokens, we adopt a progressive\ntraining strategy. First, we increase the maximum context length to 32k and raise the Rotary\nPosition Embedding (RoPE) (Su et al., 2024b) frequency base θ from 10k to 160k. Next, we\nfurther extend the context length to 128k tokens and increase θ from 160k to 500k. Throughout\nthis process, we upsample documents exceeding 16k tokens to ensure sufficient exposure to\nlong-range dependencies.\n3.3.2\nStage II: Vision-Only Training\nThis stage focuses on learning visual understanding capabilities. By carefully designing alignment\nstrategies, we ensure that visual knowledge is efficiently incorporated without compromising the model’s\nexisting language capabilities. This stage consists of the following three phrases:\n• Vision Encoder: We first pre-train the vision encoder alongside a smaller language model,\nutilizing a large-scale dataset of image-text pairs. This process encourages the vision encoder to\ncapture rich and comprehensive visual knowledge.\n• Vision Pre-Alignment: We then integrate all components into the complete multimodal architec-\nture. All LLM and ViT parameters are frozen, while the vision adapter, vision experts, and vision\nrouter are trained. The adapter is initialized from scratch, and the vision experts are derived\nfrom text experts via structural pruning. This stage ensures the seamless integration of visual\nmodules with the LLM backbone while preserving its original performance.\n• Vision Integration: At this stage, we unfreeze the vision encoder and upsample high-quality\nimage-text pairs such as captions and alt text, to enhance alignment between visual and language\nrepresentations.\n11\n\nERNIE 4.5 Technical Report\nJune 29, 2025\n3.3.3\nStage III: Joint Multimodal Training\nIn this final stage, we train all model parameters on both short and long context lengths. Specifically, this\nstage consists of the following two phases:\n• Short-Context: We unfreeze the entire model and jointly train it on a mixture of text, image,\nand video data with a context length of 8k. This stage serves as a fusion phase, consolidating\nmodality-specific alignments.\n• Long-Context: The context length is further extended to 128k, enabling the model to generalize\neffectively to long-context multimodal tasks.\n3.4\nModel Optimization\nTraining multimodal MoE models faces challenges such as expert load imbalance and gradient instability\ndue to input length variability. To address these issues, besides standard loss functions such as the\nauxiliary loss and the z-loss (Lepikhin et al., 2021; Zoph et al., 2022), we introduce two novel loss\nfunctions: router orthogonalization loss and token-balanced loss. It is expected that these proposed\nlosses can promote balanced expert utilization and stabilize gradients, thereby enabling more robust\noptimization and more effective multimodal pre-training.\n3.4.1\nRouter Orthogonalization Loss\nMixture-of-Experts (MoE) models often suffer from the expert homogenization problem, where different\nexperts learn highly overlapping or redundant representations (DeepSeek-AI et al., 2024b). To address\nthis issue, we propose the router orthogonalization loss, which encourages orthogonality among the\nrouter’s expert weights, leading to more balanced routing and better expert specialization.\nThe orthogonalization loss is defined as:\nLorth =\nk\n∑\ni=1\nk\n∑\nj=1\n\u0000ˆw⊤\ni ˆwj −δij\n\u00012,\nwhere\nˆwi =\nwi\n∥wi∥2\n.\n(1)\nHere, wi is the weight vector for expert i, and δi,j is the Kronecker delta. By encouraging orthogonality\namong these column vectors, the router produces a more uniform expert selection distribution, which\nfacilitates specialization among experts and improves generalization on out-of-distribution (OOD) tasks.\nLike weight decay, this orthogonalization loss depends solely on the router weights. Directly incorporat-\ning this term into the total loss disrupts Adam optimizer (Kingma & Ba, 2015) gradient estimates and\nleads to suboptimal training dynamics. To address this, we modify the Adam optimizer in a manner\nanalogous to AdamW (Loshchilov & Hutter, 2019), thus enabling the orthogonalization loss to update\nthe router weights directly without interfering with Adam’s gradient estimates. The coefficient for the\northogonalization loss in ERNIE 4.5 is set to 1 × 10−3, and unlike weight decay, it is not scaled by the\nlearning rate.\nIn our ablation experiments, incorporating router orthogonalization loss yields a notable improvement\non text benchmarks, detailed in Appendix A.1.\n3.4.2\nToken-Balanced Loss\nIn standard implementations, cross-entropy loss is calculated by taking the arithmetic mean of per-token\nlosses across all valid positions of a sample sequence. However, in multimodal training, only textual\ntokens contribute to the loss, and their proportion varies significantly between samples. This discrepancy\ncan induce substantial gradient variance, thus undermining the stability and efficiency of optimization.\nTo address this issue, we propose the token-balanced loss, which normalizes the loss with the sequence\nlength, thereby reducing gradient variance and promoting more stable and consistent optimization in\nmultimodal settings.\nLet Mi and Ui denote the loss mask and its complement (unmasked region), respectively, for sample i.\nThe conventional loss for sample i is:\nL(i) = −1\n|Ui| ∑\nj∈Ui\nlog P(y(i)\nj\n| y(i)\n<j(i); θ).\n(2)\nThis loss function indicates that image tokens and prompt positions are masked out and excluded from\nthe cross-entropy loss computation during pre-training. The current loss aggregation strategy introduces\n12\n\nERNIE 4.5 Technical Report\nJune 29, 2025\n0\n25\n50\n75\n100\n125\n150\n175\n200\nModel merging step i\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\nEffective learning rate ( )\ni\nEMA Mimics Learning Rate Decay\n = 0.9\n = 0.95\n = 0.97\nFigure 3: Learning rate decay shape of EMA at different coefficient α.\na gradient scaling issue: samples containing fewer unmasked tokens yield amplified gradients, leading\nto an optimization bias problem. To mitigate this gradient imbalance, we introduce the token-balanced\nloss function:\nL(i)\nbalanced =\n1\n|Ui| + |Mi| ∑\nj∈Ui\nℓ(i)\nj ,\n(3)\nwhere ℓ(i)\nj\n= −log P(y(i)\nj\n| y(i)\n<j(i); θ) represents the individual token loss contribution. The normalization\nfactor (|Ui| + |Mi|)−1 ensures that each sample’s loss contribution is weighted by the inverse of its total\nsequence length, independent of the specific masking configuration.\n3.5\nExponential Moving Average\nIn addition to loss functions, another crucial factor influencing training stability and final performance\nis the use of parameter smoothing techniques. Among these techniques, Exponential Moving Average\n(EMA) is widely adopted in large-scale pre-training to stabilize training dynamics and improve general-\nization. Despite its empirical success, the choice of the EMA decay coefficient α is often heuristic, lacking\ntheoretical guidance, especially in the context of large-scale pre-training.\nTo better understand the role of EMA, we conduct a theoretical analysis and demonstrate that EMA\ncan be viewed as analogous to learning rate decay. This perspective offers a principled explanation\nfor a commonly observed phenomenon: EMA models often match the final model checkpoints trained\nunder explicit learning rate decay schedules (DeepSeek-AI et al., 2024b; Li et al., 2025). Building on this\ninsight, we explore the relationship between the decay coefficient α and the effective decay window\nof EMA, which governs how much influence recent parameter updates have on the EMA-averaged\nmodel. Specifically, we introduce a framework for controlling the decay window size to optimize model\nperformance.\nAnalyzing EMA through Effective Learning Rate Decay.\nWe show that EMA applies an exponential\nweighting to parameter updates, in a manner similar to learning rate decay, resulting in a monotonically\ndecreasing “effective learning rate” over the course of training. Specifically, let δt = θt+1 −θt be the\nupdate at step t, the EMA parameters after n steps can be written as:\nθEMA\nn\n= θ0 +\nn−1\n∑\ni=0\n\u0010\nη(α)\ni\n\u0011\nδi,\nη(α)\ni\n= 1 −αn−i,\n(4)\nwhere η(α)\ni\nrepresents the effective learning rate assigned to the i-th update. See Appendix A.2 for\nderivation. This formalism reveals that, unlike vanilla parameter updates (which assign unit weight to\nall updates), EMA progressively downweights recent updates. Figure 3 further visualizes how the decay\nshape of η(α)\ni\nmirrors explicit learning rate schedules, such as cosine or warmup-stable-decay schedules,\nand shows that larger α yields a smoother and longer decay window. Crucially, this perspective provides\na principled way to select α by directly linking it to a desired effective window size, rather than relying\non rule-of-thumb choices.\n13\n\nERNIE 4.5 Technical Report\nJune 29, 2025\nWhile both EMA and learning rate decay introduce temporal smoothing effects, it is important to\nnote that EMA achieves this implicitly via historical parameter averaging, whereas learning rate decay\nexplicitly modulates the optimization step size through predefined scheduling rules. However, through\nour empirical study, we find that applying continuous EMA during pre-training achieves comparable\nperformance to explicit learning rate decay. Based on this observation, we propose a ”decay no more”\napproach: instead of using repeated learning rate decay to capture the model’s early-stage performance,\nwe propose a simpler method to use EMA.\nControlling the Effective Decay Window of EMA.\nIn addition to shaping the decay behavior, the EMA\ndecay coefficient α also leads to an effective decay window, which means the range of recent updates is\nsignificantly affected by EMA smoothing. To precisely control the effective decay window size through α,\nwe introduce a small threshold ϵ ∈(0, 1) (e.g., ϵ = 0.001). An update δi is considered to be outside the\neffective decay window if its effective learning rate satisfies η(α)\ni\n≥1 −ϵ ≈1. In this case, the update is\nbarely affected by the EMA smoothing, behaving as in a non-EMA model. Conversely, if η(α)\ni\n< 1 −ϵ,\nthe update δi is considered to be within the effective decay window, indicating that it is still substantially\ninfluenced by the EMA process.\nThe relationship between the effective window size and the EMA decay coefficient is shown in Equa-\ntion 5. This formulation enables precise control over the desired decay window size ˆW by selecting an\nappropriate EMA decay coefficient ˆα for a specified threshold ϵ (see Appendix A.3 for more details):\nˆα = exp\n\u0012 1\nˆW log ϵ\n\u0013\n.\n(5)\nIn practice, EMA is typically updated every s training steps, which we refer to as the EMA interval.\nOver the course of training, the EMA decay window spans T = ˆW · s training steps. Inspired by the\ndecay behavior in explicit learning rate schedules (Hu et al., 2024), we set T to one-tenth of the total\ntraining steps. Moreover, our preliminary experiments suggest that higher merging frequencies lead to\nimproved performance. Therefore, during the pre-training stage, we set s = 4 and determine α according\nto Equation 5.\nTo enable high-frequency model merging without sacrificing training efficiency, we propose an asyn-\nchronous online EMA mechanism, which enables extremely high-frequency EMA by offloading GPU\nparameters directly into host memory without interrupting the training loop. An independent CPU-based\nworker asynchronously performs EMA accumulation and periodically writes the resulting checkpoint to\ndisk. Related code has been open-sourced in PaddleNLP to facilitate further research.\n4\nPost-Training\nOur model is designed to enable a clean separation between the text-only and vision-related components\nafter multimodal pre-training. Specifically, by removing the multimodal experts, vision encoder, and\nadapter layers, the model reduces to a pure language model that can be used more efficiently in text-only\nscenarios. Using this modular design, we post-train the text-specific parameters to obtain ERNIE-\n4.5 optimized for text-only tasks. The complete set of parameters, including both textual and visual\ncomponents, is further tuned to obtain the multimodal model ERNIE-4.5-VL.\n4.1\nPost-Training of LLMs\nAs shown in Figure 4, the entire post-training pipeline consists of two stages: supervised fine-tuning\nand reinforcement learning (RL). In the RL stage, we introduce two key techniques: progressive RL\nand Unified Preference Optimization (UPO), which enhance multi-task compatibility and stabilize the\noverall RL training procedure. In Section 4.1.1, we introduce supervised fine-tuning. In Section 4.1.2\nand Section 4.1.3, we respectively expound on our approaches regarding unified reward system and\nreinforcement learning.\n4.1.1\nSupervised Fine-Tuning\nThis section elaborates on the supervised fine-tuning (SFT) process implemented for ERNIE-4.5. To\nmaximize the efficacy of the model, we implement a systematic taxonomy to categorize supervised\nfine-tuning (SFT) data into distinct topical domains. Specifically, we develop a comprehensive suite of\nten distinct topical domains, encompassing areas including science & math, coding, logic, information\nprocessing, creative writing, multilingual, knowledge QA, multi-turn & role play, and safety.\n14\n\nERNIE 4.5 Technical Report\nJune 29, 2025\nUnified Rewarding System\nERNIE𝑻\"𝟏\n𝒚𝒘\n𝒚𝒍\nPreference\nModeling\nIterative update\nSampling\nData Construction\nPreference-format\nQuery\nERNIE𝑻\n𝒚#$𝟏\n𝒚𝒏\n𝒚𝟏\n𝒚𝟐\n...\nReinforcement\nLearning\n𝒚(\nReward-format\nDomain Normalization\nInformative Prompt Filter\nModel Training\n<\n,\n>\nr\n<\n,\n>\nRule-based\nRLLM\nSandbox\nRDRM\nChecklist-aware Verifier\nGRM\nDRM\nReasoning Tasks\nNon-Reasoning Tasks\nERNIE-4.5-\nBase\nERNIE-4.5\nSFT\nStage 1:\nLogic RL\nStage 2:\nReasoning RL\nStage 3:\nGeneral RL\nProgressive RL\nUnified Preference Optimization\nFigure 4: Illustration of LLM Post-Training Pipeline for ERNIE-4.5.\nBeyond that, the SFT data are further systematically categorized into reasoning and non-reasoning tasks.\nThe reasoning data comprises complex tasks that necessitate extended chains of thought (CoT) to ensure\nthat the complexity and diversity of these tasks are fully captured. In contrast, the non-reasoning data\ncomprises tasks requiring no in-depth reasoning, but ensuring the accuracy and conciseness of such data\nis vital for boosting the model’s overall performance and versatility.\nFurthermore, the emphasis on the quality and diversity of SFT data constitutes a foundational element\nfor the subsequent reinforcement learning (RL) phase. To further enrich the diversity of the supervised\nfine-tuning (SFT) data, we provide multiple responses with distinct reasoning chains for certain queries\nwithin the reasoning tasks. This focus is instrumental in improving the foundational capabilities of the\nmodel and equipping it with the resilience necessary to engage in exploration during the RL training\nprocess. Based on the approaches defined in the preceding paragraph, we construct an SFT dataset\ncontaining 2.3 million samples. We train the model on this dataset for an average of two epochs to\noptimize the model’s performance.\n4.1.2\nUnified Rewarding System\nThis section demonstrates the unified rewarding system for subsequent reinforcement learning. This\nsystem is carefully crafted to accommodate both reasoning and non-reasoning tasks through employing\ndistinct reward combinations. It offers precise and comprehensive feedback signals that facilitate prefer-\nence learning, which provides a basis for subsequent preference optimization illustrated in Section 4.1.3.\nFor reasoning tasks that require precision and strict adherence to predefined criteria, we prioritize the\ndeployment of rule-based verifiers. However, rule-based verifiers inherently exhibit limited generaliza-\ntion capabilities. To mitigate this limitation and enhance the accuracy of feedback in reasoning tasks, we\nincorporate additional complementary mechanisms.\n• Reference-Guided LLM-as-a-Judge (RLLM): The reference-guided LLM-as-a-judge component\nexploits the advanced capabilities of a LLM as an impartial evaluator, rigorously benchmarking\nmodel-generated outputs against a well-defined corpus of reference answers.\n• Sandbox: The sandbox is a secure and isolated testing environment engineered to support the\nexecution and systematic evaluation of tasks related to programming. Within a controlled and\nisolated setup, the model’s responses are rigorously tested to directly assess their functionality,\naccuracy, reliability, and compliance with predefined requirements.\n• Reference-Guided Discriminative Reward Model (RDRM): Motivated by the success of reference-\nguided LLM-as-a-judge, we introduce a novel reference-guided discriminative reward model\n(RDRM). Unlike traditional models that work in isolation, our RDRM is explicitly guided by\nreference answers during the scoring process. Instead of a closed-book exam that requires the\nmodel to rely solely on its internal knowledge, our RDRM has access to reference answers,\neffectively transforming the evaluation process into an open-book test. RDRM ensures that the\nmodel’s outputs closely approximate the content and structural characteristics of the reference\n15\n\nERNIE 4.5 Technical Report\nJune 29, 2025\nanswers, thereby guaranteeing comprehensive coverage.\nIn the context of non-reasoning tasks, which are inherently open-ended and depend on individual\ninterpretative judgments, we implement a methodological paradigm tailored to effectively accommodate\nthese specific epistemic characteristics:\n• Checklist-Aware Verifiers: We introduce a novel method called checklist-aware verifiers, which\ndraws inspiration from the RLVR (Lambert et al., 2024; DeepSeek-AI et al., 2025). Our method\nbegins by meticulously defining a set of explicit criteria. These criteria are carefully crafted to be\nboth clearly definable and objectively assessable, ensuring that there is no ambiguity in what the\nmodel’s outputs should achieve. By instituting this rigorously defined, yet adaptable evaluative\nframework, our checklist-aware verifiers ensure that the generated responses consistently meet\nestablished normative standards.\n• Generative Reward Models (GRM): By further advancing the evaluation process by incorporat-\ning multi-dimensional evaluation criteria and dynamic feedback mechanisms, GRM conducts\na tailored evaluation for each query, thereby implementing a more systematic and nuanced\nassessment and enhancing both the accuracy and robustness of the evaluative outcomes.\n• Discriminative Reward Models (DRM): DRM constitute a fundamental aspect of classical\nreinforcement learning frameworks, wherein reward functions are learned via discriminative\ntasks to effectively steer the model towards producing outputs that more accurately align with\nthe intended objectives.\nBy systematically tailoring our reward system to the distinct requirements of both reasoning and non-\nreasoning tasks, we enable ERNIE-4.5 to demonstrate enhanced proficiency across a diverse spectrum of\napplications. The unified reward system not only substantively improves the model’s overall performance\nbut also facilitates a more nuanced elucidation of the latent preferences and evaluative criteria intrinsic\nto human judgment, thereby facilitating the progression toward more nuanced, sophisticated, and\nhuman-aligned interactions.\n4.1.3\nReinforcement Learning\nWe conduct the RL training process of the ERNIE-4.5 within the framework of Proximal Policy Optimiza-\ntion (PPO) (Schulman et al., 2017; Ouyang et al., 2022). To enhance training stability and optimize the\nmodel’s ultimate performance, we introduce the key techniques of our RL training recipe:\n• Progressive Reinforcement Learning (PRL): PRL implements a three-stage Reinforcement Learn-\ning (RL) algorithm as shown in Figure 4, which employs a staged progression: (1) In the initial\nstage, the model is trained exclusively on logic corpora, which systematically build up the robust\nfoundational ability for logical analysis and abstract reasoning, serving as the cornerstone for all\nsubsequent stages of learning within the PRL framework. (2) In the second stage, the training\ncorpus mainly includes mathematics and programming code. This incorporation facilitates the\ntransfer of abstract reasoning skills to tasks that are characterized by stronger requirements for\nstructural expressiveness and executable precision. (3) In the third phase, the model undergoes\ntraining on a general dataset that encompasses both non-reasoning and reasoning tasks, which\nenhances generalizability across a broad spectrum of tasks by systematically leveraging the\nknowledge acquired in earlier stages.\n• Unified Preference Optimization (UPO): In conventional reinforcement learning algorithms\nsuch as PPO, training is designed to maximize the expected reward associated with a single\nresponse generated for each given query, and there is a lack of explicit pairwise comparative\nsignals to guide the learning process. We introduce a novel UPO strategy. Specifically, UPO\nintegrates the pairwise preference modeling loss, i.e., Direct Preference Optimization (DPO) loss,\ninto the PPO framework. Based on the different ways of constructing pairwise preference data,\nthe UPO algorithm can be categorized into online and offline versions. The online-UPO constructs\npreference pairs by employing a rejection sampling strategy on the multiple responses generated\nfor each query at each reinforcement training iteration, while all preference data for each query\nis pre-generated before the RL training process in offline-UPO. By integrating learning from\npreference pairs that capture substantive behavioral distinctions, rather than relying exclusively\non potentially unreliable reward signals, the UPO algorithm not only enhances the stability of\nreinforcement learning training, but also effectively mitigates the risk of reward hacking.\nIn contrast to traditional reinforcement learning, we systematically develop dedicated data for utilizing\nboth the verifiers and the reward models listed in Section 4.1.2 across comprehensive topical domains\nintroduced in Section 4.1.1, which leads to quite different reward scoring ranges and distributions. To\n16\n\nERNIE 4.5 Technical Report\nJune 29, 2025\nFigure 5: Illustration of VLM Post-Training for ERNIE-4.5-VL.\nfurther enhance training stability and optimize the model’s ultimate performance, we implement a series\nof improvements. Specifically, we exclude prompts associated with 1 or 0 accuracy from the dataset\ntailored to support verifiers, whose reward signals exhibit explicit verifiability. For the remaining prompts,\nwe filter prompts predicated on the intra-group variance of reward signals within each sample cohort. In\nother words, the prompts corresponding to groups characterized by insubstantial variance, indicating a\nlack of discriminative information, are excluded from the training process. Besides, within each training\niteration, the rewards derived from verifiers and reward models are firstly disentangled, and further\nstratified according to specific topical domains, yielding multiple topical-domain-specific subsets. A\nreward normalization is independently applied to each subset. Empirical evaluations demonstrate that\nthese improvements effectively reduce the heterogeneity of reward signals across different sources and\ndomains, thereby enhancing stability and convergence during reinforcement learning.\n4.2\nPost-Training of VLMs\nThe entire VLM post-training procedure is shown in Figure 5, which consists of three SFT stages and\none reasoning RL stage. Notably, the third SFT stage is designed to incorporate a mixture of thinking\nand non-thinking data, with the goal of promoting the capabilities of general vision understanding and\ncomplex vision reasoning.\n4.2.1\nSupervised Fine-tuning\nWe design a supervised fine-tuning framework to strengthen two key aspects of multimodal models:\nimage understanding and reasoning capabilities. Accordingly, we focus on enhancing visual perception\nand unifying thinking and non-thinking behaviors, through targeted data construction and progressive\ntraining strategies, respectively.\nData.\nOur empirical study indicates that vision-language models (VLMs) often perform poorly on\ncertain challenging tasks, such as puzzle tests, geometry problems, and chart interpretation. These tasks\nrequire foundational perceptual understanding and strong reasoning capabilities. However, VLMs often\nstruggle with foundational perceptual understanding (Rahmanzadehgervi et al., 2024; Chen et al., 2024a).\nAlthough enhancing this perceptual capacity is critical, a major obstacle lies in the scarcity of dense\nimage-caption pairs in natural corpora.\nTo overcome this, we synthesize a large volume of perceptual data, including programmatically generated\npuzzles (Ghosal et al., 2024), geometric figures, and mathematical functions. These synthetic datasets\nafford fine-grained control over spatial layout and structural properties, enabling the generation of high-\nquality visual-text pairs with minimal ambiguity. Despite their cleanliness and scalability, synthetic data\nfall short in visual diversity and the contextual richness present in real-world images. As a result, their\ngeneralizability is limited, motivating a shift toward recaptioning natural STEM (Science, Technology,\nEngineering, and Mathematics) imagery.\nAccordingly, we perform fine-grained caption synthesis on large-scale collections of real STEM images.\nUnlike synthetic data, natural images demand captions that are both informative and hallucination-\nresistant. To this end, we frame captioning as a constrained optimization problem: producing image\ndescriptions that enable a text-only reasoning model to answer the associated question without visual\ninput. Our pipeline begins with extracting problem-answer pairs from curated datasets. Captions are\ngenerated by a VLM and validated through repeated inference using a text-only model. Only samples\nyielding consistent correct answers are retained. Furthermore, we filter out samples solvable via visible\ntext (e.g., OCR), ensuring that visual understanding is required.\nBy integrating these synthesized perceptual data during post-training, we achieve significant improve-\nments in the model’s ability to comprehend and reason in visually complex STEM tasks, including puzzle\ntests, mathematical problem solving, and related domains. It also establishes a solid foundation for image\nunderstanding capabilities.\n17\n\nERNIE 4.5 Technical Report\nJune 29, 2025\nThinking and Non-Thinking Joint Training.\nObtaining high-quality multimodal reasoning data for\ncold-start training presents significant challenges. Although manually annotated samples can ensure a\nhigh degree of accuracy, they often fall short in diversity and coverage. To overcome these challenges and\nreduce the cost of extensive manual annotation, we propose a three-stage progressive training framework\nthat leverages cross-modal transfer capabilities and expert merging techniques.\n• Step 1: Text-only Reasoning Cold Start. We collect a diverse corpus of text-only reasoning data\ncovering mathematics, science, code generation, instruction following, and dialogue. To ensure\nhigh-quality reasoning supervision, we apply a combination of agent-based and rule-based\nfilters to exclude samples with flawed logic, including circular reasoning, contradictions, and\nconceptual mistakes. Remarkably, despite being trained exclusively on curated textual data and\nnever exposed to visual inputs, the model exhibits emergent multimodal reasoning behaviors,\nfor example, producing reflective cues such as “let me take another look at the image.”\n• Step 2: Reject Sampling for Multimodal Enhancement. Building upon the Step 1 model,\nwe employ reject sampling to generate reasoning data for vision-related capabilities across\nSTEM, comprehension tasks, chart and document analysis, and creative writing. This process\nsystematically expands the coverage of reasoning capabilities while ensuring data quality through\nthe verifiable reward mechanisms detailed in Section 4.2.2. Furthermore, during the RL training\nphase, we continuously track higher-quality response trajectories via the verification system.\nThese superior trajectories are persistently recorded and progressively incorporated to update\nand enrich our supervised fine-tuning (SFT) multimodal reasoning dataset.\n• Step 3: Thinking and Non-Thinking Fusion. After strengthening multimodal reasoning through\ntargeted data generation in Step 2, we proceed to unify reasoning and non-reasoning capabilities\ninto a single model through two approaches:\n1. Mixed Training with Reasoning and Non-Reasoning Data: We conduct joint training using\nboth reasoning (generated in step 2) and non-reasoning datasets. For all non-reasoning data,\nwe prepend empty thinking tags <think>\\n\\n</think> to the response, with these tags\nmasked and excluded from gradient updates. This approach enables the model to maintain\nnon-reasoning capabilities while preserving reasoning competencies.\n2. Experts Merging: Following DeepSeek-R1T-Chimera (GmbH, 2025), we merge experts from\nthe thinking and non-thinking models by transferring multimodal experts from the non-\nreasoning model to the reasoning model. This fusion strategy creates a unified model with\nboth reasoning and non-reasoning capabilities, where non-reasoning performance surpasses\nthe original baseline. This approach enables us to effectively combine models with distinct\nstrengths in reasoning and visual perception.\nThis progressive training methodology successfully addresses the cold-start challenge for multimodal\nreasoning models while achieving superior performance across both reasoning and non-reasoning tasks .\n4.2.2\nReinforcement Learning with Verifiable Rewards\nReinforcement Learning with Verifiable Rewards (RLVR) (DeepSeek-AI et al., 2025; Lambert et al., 2024)\nhas emerged as a crucial paradigm for improving the performance of multimodal language models in\ndomains where ground-truth verification is feasible. In this section, we introduce several types of tasks\nemployed in our multimodal RL training that leverage verifier-based reward mechanisms. These tasks\ninclude visual STEM, visual puzzles and UI2Code (Chen et al., 2018).\nVisual STEM.\nVisual STEM problems consist of image-based questions that are accompanied by\nground-truth answers, making them particularly suitable for use in RLVR. We curate a diverse collection\nof visual STEM questions from both open-sourced resources and proprietary K-12 educational resources.\nSimilar to the method in Guo et al. (2025), we reformulate the multiple-choice questions into open-ended\nformats to discourage models from random guessing. Additionally, we filter out examples that models\nconsistently answer correctly or incorrectly, as such questions contribute little to learning progress and\nreduce training efficiency. This curation pipeline ensures a high-quality, challenging dataset conducive to\neffective policy learning.\nVisual Puzzles.\nVisual puzzles are image-based reasoning tasks that require visual perception and\ncognitive reasoning to obtain a correct answer. We synthesize a dataset of over 10k visual puzzles\nalong with their verified solutions for RLVR training. The preprocessing of visual puzzle data follows a\nsimilar approach to that used for visual STEM tasks. In contrast to conventional verification approaches,\nwhich prompt models to output their final answers in \\boxed{} format and evaluate correctness via\nstring matching (DeepSeek-AI et al., 2025), we employ two large language models (LLMs) to assess the\n18\n\nERNIE 4.5 Technical Report\nJune 29, 2025\ncorrectness of the policy model’s responses. One LLM is used to evaluate whether a response contains\nany internally inconsistent or conflicting answers, while the other verifies the correctness of the final\nanswer. A response is considered correct only if both LLMs return positive evaluations. This evaluation\nstrategy does not impose constraints on the response format of the policy model, thereby enabling more\nflexible outputs.\nUI2Code.\nTo enhance the model’s capability in practical multimodal applications, we collect the\nUI2Code (Chen et al., 2018) and Image2Struct (Roberts et al., 2024) datasets, which focus on generating\nHTML code from UI design images. We deploy a UI2Code verifier environment that evaluates the visual\nfidelity between the user-provided reference image (typically a UI design mockup) and the UI rendered\nfrom the HTML code generated by the VLM. This ensures that the VLM learns to produce syntactically\ncorrect and visually faithful HTML representations.\nHybrid Reinforcement Learning.\nTo enable the model to perform well in both reasoning and gen-\neral capabilities, we design a unified reinforcement learning framework that integrates RLVR and\nRLHF (Ouyang et al., 2022). Accordingly, we develop a multimodal reward model trained using the\nBradley-Terry reward modeling objective. The reward model is initialized from ERNIE-4.5-Base to\neffectively handle queries containing visual inputs. We adopt GRPO (Shao et al., 2024) as our reinforce-\nment learning algorithm, incorporating improvements inspired by DAPO (Yu et al., 2025), including\ndynamic sampling and overlong filtering. These strategies collectively ensure stable training dynamics\nand enhance exploratory capability.\n5\nTraining Framework\nThe training of ERNIE 4.5 is supported by PaddlePaddle (Ma et al., 2019). The inherent heterogeneity of\nmultimodal model coupled with the large-scale MoE architecture presents significant systemic challenges\nfor distributed training at scale. We introduce an optimized training framework with the following key\ninnovations:\n1. Heterogeneous Parallelism for Multimodal Model Training: ERNIE 4.5 combines a ViT encoder\nwith a multimodal MoE backbone. The fundamental divergences in parameter scale, computa-\ntional complexity, and memory requirements between these components create challenges for\nhomogeneous parallelism strategy. To mitigate this, we introduce a heterogeneous parallelism\nstrategy for efficient joint training. Moreover, we propose a hierarchical load balance method to\nenhance scaling efficiency for variable-resolution training.\n2. Hybrid Parallelism for MoE Backbone: Through meticulous co-design with ERNIE 4.5 ar-\nchitecture, we implement intra-node expert parallelism to eliminate the overhead associated\nwith cross-node all-to-all communication. In addition, we propose a memory-efficient pipeline\nscheduling method to reduce activation memory during large-scale training.\nIn addition, we make other notable optimizations to further improve training performance and stability.\n1. FP8 Mixed Precision Training: We introduce an FP8 mixed-precision training framework with\nfine-grained memory optimization, well-designed operator fusion and communication optimiza-\ntion. The precision for each operator and communication is well designed to simultaneously\nmaximize training throughput while maintaining convergence.\n2. Computational Optimizations: To minimize recomputation overhead, we propose an optimized\nrecomputation strategy that achieves superior computation-memory tradeoffs. We also integrate\nFlashMask (Wang et al., 2025) to accelerate attention operators.\n3. Framework-Native Fault Tolerance System: We introduce a fault tolerance system, which\ndeeply integrates with our training framework, to overcome the challenges of frequent failures\nin large-scale training. Especially, we propose a Zero Cost Checkpoint technique, a superior\ncheckpointing approach, to minimize the interruption cost.\nOur largest ERNIE 4.5 language model employs an 8-way expert parallelism (EP) (Lepikhin et al., 2021),\n12-way pipeline parallelism (PP) (Huang et al., 2019), and ZeRO-1 data parallelism (DP) (Rajbhandari\net al., 2020) configuration. Through these comprehensive optimizations above, we achieve 47% Model\nFLOPs Utilization (MFU) in our largest ERNIE 4.5 language model, on 2016 NVIDIA H800 GPUs and\nRoCE interconnection with 4096 sequence length and 15120 global batch size.\n19\n\nERNIE 4.5 Technical Report\nJune 29, 2025\nMoE Decoder Layer\nPP stage 0\nMoE Decoder Layer\nPP Stage N-1\nPatchify (DP)\nViT\nEncoder\nRMSNorm\nAttention\nRMSNorm\nTP\n(Optional)\nRouted Expert FFN\nGating\nShared Expert FFN\nEP\nAdapter (PSP)\nPP stage 0\nMoE Decoder Layer\nPP stage 1\nAttention (Head Sharded)\nFFN (PSP)\nx L\nMoE\nBackbone\nGather to PP stage 0\nFigure 6: Overview of ERNIE 4.5’s multimodal heterogeneous parallelism strategy.\nViT\nMoE Decoder Layer\nPP 0\nPP N-1\nViT\nvision feature\nForward\nvision feature\nvision feature\nMoE Decoder Layer\nForward\nViT\nPP 0\nPP N-1\nViT\ngradient\ngradient\ngradient\nvision feature\nvision feature\nAllReduce\nM * [B, S, H]\nForward\nBackward\nForward\nBackward\nno grad\nno grad\nrecompute\nrecompute\nGather\nScatter\nM * [B, S, H]\nForward\nBackward\nbackward\npipeline stage\nmodels\nactivations\nforward\nrecompute\ngradients\ndistributed ops\nM: # of micro batch size\nB: micro batch size\nS: sequence length\nH: hidden size\nM × Forward-Backward\nFigure 7: Joint training of the ViT encoder and the MoE backbone.\n5.1\nHeterogeneous Parallelism for Multimodal Model Training\n5.1.1\nHeterogeneous Parallelism Architecture\nAs illustrated in Figure 1, ERNIE 4.5 supports unified training with mixed text, image, and video\nmodalities. For ERNIE-4.5-VL-424B-A47B-Base, the vision inputs (images and videos) are processed by a\nunified ViT encoder comprising 630 million parameters. This encoder is jointly trained with a backbone\nnetwork that employs a large-scale MoE architecture, totaling 424 billion parameters with 47 billion\nparameters activated during computation.\nThe large-scale MoE backbone necessitates hybrid parallelism training, combining expert parallelism\n(EP), pipeline parallelism (PP), and optional tensor parallelism (TP) (Shoeybi et al., 2019), integrated\nwith ZeRO-1 data parallelism (DP). While a straightforward approach would place the ViT encoder\n20\n\nERNIE 4.5 Technical Report\nJune 29, 2025\nEP0\nEP0\nEP1\nEP1\nEP0\nEP0\nEP1\nEP1\nPP0\nPP1\nPP2\nPP3\nDP0\nDP4-PSP0\nDP6-PSP0\nDP5-PSP1\nDP7-PSP1\nDP0-PSP0\nDP2-PSP0\nDP1-PSP1\nDP3-PSP1\nFine-grained Load\nBalance Group 0\nDP12-PSP0\nDP14-PSP0\nDP13-PSP1\nDP15-PSP1\nDP8-PSP0\nDP10-PSP0\nDP9-PSP1\nDP11-PSP1\nParallelism topology of the MoE backbone\nParallelism topology of the ViT encoder\nmerge & sort\nround-robin partitioning\nDP0\nDP1\nDP14\nDP15\nall-to-all\neven\npartition\nPSP0\nPSP1\n[BS,HD]\nFFN\n[BS/P,HD]\n[BS/P,H,D]\n[BS,H/P,D]\nAttention\n[BS/P,H,D]\nconcat\nB: batch size\nS: sequence length\nH: attention head number\nD: head dimension\nP: packed sequence parallel degree\n[BS,H/P,D]\nall-to-all\n(a) Shared parallelism layout between the MoE backbone and the ViT encoder \n(b) Coarse-grained Load Balancing\n(c) Fine-grained Dynamic Balanced Partitioning\ninside attention\noutside attention\noutside attention\nFine-grained Load\nBalance Group 1\nFine-grained Load\nBalance Group 2\nFine-grained Load\nBalance Group 3\nFine-grained Load\nBalance Group 4\nFine-grained Load\nBalance Group 5\nFine-grained Load\nBalance Group 6\nFine-grained Load\nBalance Group 7\nEP0\nEP0\nEP1\nEP1\nEP0\nEP0\nEP1\nEP1\nPP0\nPP1\nPP2\nPP3\nDP1\nDP0\nDP1\nDP14\nDP15\nFigure 8: Illustration of hierarchical load balance strategy for variable resolution multimodal training.\nonly in the first pipeline stage of the MoE backbone, this induces severe workload imbalance across\npipeline stages, significantly degrading training efficiency. Given its substantially smaller parameter\ncount, the ViT encoder is optimally suited for data parallelism. To enable efficient joint training of the ViT\nencoder and MoE backbone, we propose the heterogeneous parallelism strategy illustrated in Figure 6.\nThe ViT encoder parameters are replicated across all devices, and the data parallelism dimension of\nthe ViT encoder is nested within the hybrid parallelism topology of the MoE backbone. The internal\nparallelism architecture of the ViT encoder and the adapter in Figure 6 would be comprehensively\ndetailed in Section 5.1.2.\nIn the forward pass, the ViT encoder on each device computes vision features independently. These\nfeatures are then gathered to the first pipeline stage of the MoE backbone. Subsequent forward propagation\noccurs through the MoE backbone’s pipeline parallelism stages. However, during the backward pass,\nthe gradients of the ViT encoder parameters cannot be computed directly. This is because the automatic\ndifferentiation backpropagation of the pipeline parallelism naturally ends in the first module of the MoE\nbackbone but not the ViT encoder.\nTo address this challenge, we implement a customized backpropagation mechanism in Figure 7 to train\nthe ViT encoder. Upon completing the MoE backbone’s backward pass, all vision feature gradients\nbecome available in the first pipeline stage. These gradients are then scattered to each pipeline stage of\nthe MoE backbone, allowing each ViT encoder with different data parallel ranks to receive gradients\nspecific to its local vision features. In this way, we can perform standard automatic differentiation\nbackpropagation through the ViT encoder. Finally, since ViT operates in data parallel mode, its parameter\ngradients should be synchronized across all devices via an all-reduce communication to ensure consistent\nparameter updates. Notably, the recomputation of the ViT encoder in Figure 7 is optional and serves to\nreduce the activation memory of the ViT encoder.\n5.1.2\nHierarchical Load Balance Strategy\nERNIE 4.5 demonstrates enhanced adaptability by supporting input images and videos with arbitrary and\ncontinuously variable resolutions. To enable ViT encoder training with variable resolutions, we organize\ntokens from patchified images or video frames into packed sequences (Dehghani et al., 2023a). However,\nmultimodal data exhibits significant imbalance challenges: it is not only that the image and video data\ndiffer in spatial resolutions, but also video data presents more severe imbalance due to temporal length\nvariations. Therefore, both the number of packed sequences per training sample and the token count\n21\n\nERNIE 4.5 Technical Report\nJune 29, 2025\nUp-Gate Linear\nSwiGLU\nDown Linear\nAll-to-all\nprobability0\nAll-to-all\nx2\nx4\nx3\nx1\nx6\nx7\nPermutation\nx0\nUp-Gate Linear\nSwiGLU\nDown Linear\nAll-to-all\nprobability0\nAll-to-all\nAll-to-all\nx2\nx4\nx3\nx1\ny1\ny2\ny3\nprobability2\nUnpermutation\nPermutation\nx0\noutput\nPermutation\nprobability1 \noutput\nUnpermutation\nExpert\nExpert\n(a) Conventional MoE implementation\n(b) ERNIE 4.5’s MoE implementation\nFigure 9: Comparsion of different MoE implementions.\nper packed sequence exhibit significant variation. This variability leads to extreme computational and\nmemory imbalances between different data parallel ranks of the ViT encoder.\nTo address these challenges, we propose a hierarchical multimodal load balance strategy, as illustrated in\nFigure 8. Our load balance method is summarized as follows:\nLevel 1: Coarse-grained Load Balance.\nFirst, we collect and sort all packed sequences in ascending\norder by their token counts across the data parallel group of the ViT encoder. Then, using a round-robin\npartitioning algorithm, we distribute the packed sequences to each device to ensure approximately bal-\nanced total token counts. This redistribution of packed sequences achieves coarse-grained computational\nand memory load balance, as shown in Figure 8(b).\nLevel 2: Fine-grained Dynamic Balanced Partitioning.\nDue to the natural token count divergences in\neach packed sequence, the total token counts on each device may differ a lot after coarse-grained load\nbalance. Specifically, we perform dynamic partitioning methods inside and outside attention operators to\nachieve load balance further, as illustrated in Figure 8(c):\n• Outside Attention Operations: We propose the packed sequence parallelism (PSP) strategy\nfor the operators outside the attention. Unlike the sequence parallelism proposed in Megatron-\nLM (Korthikanti et al., 2023) which builds upon tensor parallelism and partitions the model\nparameters, our proposed packed sequence parallelism method concatenates the packed se-\nquences and evenly partitions along the sequence length dimension. The packed sequence\nparallelism is also applied in the adapter module in Figure 6.\n• Inside Attention Operations: Since attention operators require the full sequence length di-\nmension for each packed sequence, we perform an all-to-all communication before attention\ncomputation to exchange the sequence length and attention head dimensions. After attention\ncomputation, these dimensions are exchanged back to resume packed sequence parallelism.\nThrough our hierarchical load balance strategy, the resource utilization efficiency of computation, memory\nand communication is significantly improved. Experimental results demonstrate that ERNIE-4.5-VL-\n424B-A47B-Base achieves up to 32% overall performance improvement in end-to-end multimodal training\ncompared to baseline approaches without load balance.\n5.2\nHybrid Parallelism for MoE Backbone\nTo effectively scale the training of our largest ERNIE 4.5 language model, we employ three-dimensional\nparallelism on the MoE backbone, specifically, expert parallelism, pipeline parallelism, and ZeRO-1\ndata parallelism, for text pre-training. As for its multimodal model pre-training, we incorporate tensor\nparallelism to accommodate the increased sequence length and extra vision expert parameters.\n22\n\nERNIE 4.5 Technical Report\nJune 29, 2025\n6\n5\n4\n3\n2\n1\n6\n5\n4\n3\n2\n1\n6\n5\n4\n3\n2\n1\n6\n5\n4\n3\n2\n1\nDevice 1\n6\n5\n4\n3\n2\n1\n6\n5\n4\n3\n2\n1\n6\n5\n4\n3\n2\n1\n6\n5\n4\n3\n2\n1\nDevice 2\n6\n5\n4\n3\n2\n1\n6\n5\n4\n3\n2\n1\n6\n5\n4\n3\n2\n1\n6\n5\n4\n3\n2\n1\nDevice 3\n6\n5\n4\n3\n2\n1\n6\n5\n4\n3\n2\n1\n6\n5\n4\n3\n2\n1\n6\n5\n4\n3\n2\n1\nDevice 4\n6\n5\n4\n3\n2\n1\n6\n5\n4\n3\n6\n2\n5\n1\n4\n3\n2\n1\n6\n5\n4\n3\n2\n1\nDevice 1\n6\n5\n4\n3\n2\n1\n6\n5\n4\n6\n3\n5\n2\n4\n1\n3\n2\n1\n6\n5\n4\n3\n2\n1\nDevice 2\n6\n5\n4\n3\n2\n1\n6\n5\n6\n4\n5\n3\n4\n2\n3\n1\n2\n1\n6\n5\n4\n3\n2\n1\nDevice 3\n6\n5\n4\n3\n2\n1\n6\n6\n5\n5\n4\n4\n3\n3\n2\n2\n1\n1\n6\n5\n4\n3\n2\n1\nDevice 4\nConventional VPP Scheduling (F-then-B)\nMemory Efficient VPP Scheduling (F-then-B)\nTime\nForward Pass\nBackward Pass\nFigure 10: Memory-efficient F-then-B virtual pipeline scheduling.\n11\n10\n9\n8\n7\n11\n10\n9\n8\n7\n6\n5\n4\n11\n3\n10\n2\n9\n1\n8\n6\n7\n5\n11\n4\n10\n3\n9\n2\n8\n1\n7\n6\n5\n4\n3\n2\n1\n6\n5\n4\n3\n2\n1\nDevice 1\n11\n10\n9\n8\n7\n11\n10\n9\n8\n7\n6\n11\n5\n10\n4\n9\n3\n8\n2\n7\n1\n11\n6\n10\n5\n9\n4\n8\n3\n7\n2\n6\n1\n5\n4\n3\n2\n1\n6\n5\n4\n3\n2\n1\nDevice 2\n11\n10\n9\n8\n7\n11\n10\n9\n8\n11\n7\n10\n6\n9\n5\n8\n4\n7\n3\n11\n2\n10\n1\n9\n6\n8\n5\n7\n4\n6\n3\n5\n2\n4\n1\n3\n2\n1\n6\n5\n4\n3\n2\n1\nDevice 3\n11\n10\n9\n8\n7\n11\n10\n11\n9\n10\n8\n9\n7\n8\n6\n7\n5\n11\n4\n10\n3\n9\n2\n8\n1\n7\n6\n6\n5\n5\n4\n4\n3\n3\n2\n2\n1\n1\n6\n5\n4\n3\n2\n1\nDevice 4\n11\n10\n9\n8\n7\n6\n11\n10\n9\n8\n7\n5\n4\n11\n3\n10\n2\n9\n1\n8\n6\n7\n5\n11\n4\n10\n3\n9\n2\n8\n1\n7\n6\n5\n4\n3\n2\n1\n6\n5\n4\n3\n2\n1\nDevice 1\n11\n10\n9\n8\n7\n6\n11\n10\n9\n8\n7\n11\n5\n10\n4\n9\n3\n8\n2\n7\n1\n11\n6\n10\n5\n9\n4\n8\n3\n7\n2\n6\n1\n5\n4\n3\n2\n1\n6\n5\n4\n3\n2\n1\nDevice 2\n11\n10\n9\n8\n7\n6\n11\n10\n9\n11\n8\n10\n7\n9\n5\n8\n4\n7\n3\n11\n2\n10\n1\n9\n6\n8\n5\n7\n4\n6\n3\n5\n2\n4\n1\n3\n2\n1\n6\n5\n4\n3\n2\n1\nDevice 3\n11\n10\n9\n8\n7\n6\n11\n11\n10\n10\n9\n9\n8\n8\n7\n7\n5\n11\n4\n10\n3\n9\n2\n8\n1\n7\n6\n6\n5\n5\n4\n4\n3\n3\n2\n2\n1\n1\n6\n5\n4\n3\n2\n1\nDevice 4\nTime\nForward Pass\nBackward Pass\nMemory Efficient VPP Scheduling (1F1B)\nConventional VPP Scheduling (1F1B)\nFigure 11: Memory-efficient 1F1B virtual pipeline scheduling.\nFurthermore, we introduce several techniques to optimize the memory footprint and reduce communica-\ntion overhead, ultimately enhancing the training efficiency. These innovations ensure that our approach\nprovides superior scalability and performance in the training of large-scale MoE models.\n5.2.1\nIntra-Node Expert Parallelism\nWe design the model architecture and configuration to avoid costly inter-node expert parallelism com-\nmunication. By confining expert parallelism communication to intra-node, we implement MoE all-to-all\ncommunication based on NCCL-compatible collective primitives. This approach achieves end-to-end\nthroughput comparable to DeepEP-based MoE implementations (Zhao et al., 2025) on ERNIE 4.5, and\ncan easily be deployed on AI clusters without NVIDIA GPUs and InfiniBand (IB) networks.\nAs shown in Figure 9(a), conventional MoE implementations apply the gating probability multiplication\noperator after the second all-to-all communication. This method necessitates retaining the output tensor\nof the second all-to-all communication for backpropagation, which creates significant memory pressure.\nOur solution in Figure 9(b) repositions the gating probability multiplication operator within the expert\ncomputation block. This architectural modification enables immediate release of the second all-to-all\ncommunication output tensor after consumption. While introducing minor overhead through probability\npermutation and an additional lightweight all-to-all operation, this optimization significantly reduces\npeak memory usage and eliminates numerous recomputations during backpropagation.\n5.2.2\nMemory-Efficient Pipeline Scheduling\nWhen scaling to larger cluster training, maintaining a fixed global batch size necessitates reducing the\ngradient accumulation steps, which in turn increases the pipeline bubble time fraction, significantly\ndegrading training throughput. Virtual Pipeline Parallelism (VPP) (Narayanan et al., 2021) is usually\nadopted to reduce the pipeline bubble time fraction.\nGenerally, the first pipeline stage of VPP is expected to consume the most activation memory. However,\nthe last pipeline stage involves loss function computations, which can become a memory bottleneck\n23\n\nERNIE 4.5 Technical Report\nJune 29, 2025\nUp-Gate Linear\nSwiGLU\nDown Linear\nAll-to-all\nprobability0 (FP32)\nAll-to-all\nAll-to-all\nFP8\nBF16\nBF16\nFP8\nBF16\nBF16\nBF16\nprobability2 (FP32)\nUnpermutation\nPermutation\nBF16\nBF16\nPermutation\nprobability1 (FP32)\nExpert\nForward\nFP8 Quantization\nBF16\nFP8 Quantization\nFP8\nPermutation\nBF16\nAll-to-all\nBF16\nDown Linear \nActivation Backward\nFP8 Quantization\nDown Linear\nWeight Backward\nMultiply Backward\nBF16\nFP8\nFP8\nBF16\nFP8\nDequantization & \nQuantization\nUp-Gate Linear\nActivation Backward\nFP8\nUp-Gate Linear\nWeight Backward\nFP8 Quantization\nBF16\nFP8\nFP8\nAll-to-all\nUnpermutation\nBF16\nBF16\nBF16\nMoE Forward\nMoE Backward\nSwiGLU Backward\nBF16\nExpert\nBackward\nUp-Gate Linear\nSwiGLU\nFP8 Quantization\nFP8\nBF16\nBF16\nBF16\nOptional Recomputation\nFP32\nΣ\nOverlap\nΣ\nFP32\nFP32\nWeight\nGradient\nAccumulation\nWeight\nGradient\nAccumulation\nFigure 12: FP8 mixed precision training strategy.\ninstead. To address this, we propose a memory-efficient virtual pipeline scheduling strategy. Once\nthe last pipeline stage completes the forward computation of the loss function, it immediately begins\nits backward computation and releases the activation memory of the loss function. In this way, the\nlast pipeline stage should at most retain the activation memory of a single VPP chunk. Figures 10\nand 11 illustrate our memory-efficient pipeline scheduling under forward-then-backward (F-then-B) and\none-forward-one-backward (1F1B).\nWhen the gradient accumulation steps are fewer than twice of the PP degree, we can only use the F-then-B\nscheduling method. In these scenarios, we propose a parameter gradient release technique to reduce\nthe memory usage. At the end of each training step, we release the memory allocated for parameter\ngradients. The parameter gradient release method significantly reduces peak memory usage in F-then-B\nscheduling, especially when maintaining FP32 gradients during BF16 or FP8 mixed-precision training.\n5.3\nFP8 Mixed Precision Training\nFP8 format (Micikevicius et al., 2022) reduces the bit-width by half compared to BF16, offering significant\nadvantages in large model training including improved computational throughput, reduced memory\nconsumption, and decreased communication overhead (NVIDIA, 2024; Peng et al., 2023; torchao, 2024).\nERNIE 4.5 adopts a similar quantization strategy to DeepSeek-V3 (DeepSeek-AI et al., 2024b) in the MoE\nFFN modules, utilizing E4M3 FP8 numerical format with an online quantization strategy that employs\nblock-wise quantization for weights and tile-wise quantization for activations. The FP8 mixed precision\ntraining strategy for ERNIE 4.5 is illustrated in Figure 12. We highlight our engineering insights from\nefficient end-to-end FP8 training to benefit the community.\nFine-Grained Memory Optimization on FP8 Training.\nThe primary benefit of FP8 mixed precision\ntraining stems from memory savings, enabling us to reduce the most expensive recomputations and\nimprove the throughput. In the MoE FFN module, the major activation memory comes from the input\nactivations of the up-gate linear, down linear, SwiGLU, and gate probability multiplication.\n1. For the up-gate linear, we retain its FP8 input activations XFP8 for the backward pass rather\nthan the BF16 tensors XBF16. In the backward pass, the FP8 quantization version of transposed\nXBF16 is required to compute the weight gradient. Therefore, we need to apply a dequantize-\ntranspose-quantize operation to XFP8 during weight gradient computation. In this way, we can\nreduce the memory usage of the up-gate linear, and the first all-to-all communication can be\nperformed in FP8 precision to save the communication cost. It is a tradeoff between the memory,\ncommunication and precision, and we found that this method can keep the same convergence\nrates with the baseline implementation.\n24\n\nERNIE 4.5 Technical Report\nJune 29, 2025\nAlgorithm 1 Pseudocode of the conventional recomputation method\ndef\nmy_module(x):\ny1 = op1(x)\ny2 = op2(y1)\ny3 = op3(y2)\nreturn y3\ndef\nmy_module_with_conventional_recomputation (x):\ny1 = recompute(op1 , x)\ny2 = op2(y1)\ny3 = recompute(op3 , y2)\nreturn y3\nAlgorithm 2 Pseudocode of our operator-level recomputation method\nclass\nOperatorLevelRecomputation :\noutput_tensor: paddle.Tensor\ndef\nforward(self , op , x):\nif not\nis_grad_enabled (): # the first\nforward\nself.output_tensor = op(x)\nreturn\nself.output_tensor\nelse: # the second\nforward\nreturn\nself.output_tensor\ndef\nbackward(self , op_grad , y_grad ):\nx_grad = op_grad(y_grad)\nreturn\nx_grad\ndef\nmy_module_with_operator_level_recomputation (x):\ny1 = op1(x)\ny2 = OperatorLevelRecomputation ()(op2 , y1)\ny3 = op3(y2)\nreturn y3\ny3 = recompute( my_module_with_operator_level_recomputation , x)\n2. For the down linear, there are two options to save the memory: (1) retain the BF16 output tensor\nof the up-gate linear; (2) recompute up-gate linear using the XFP8 tensor above to generate\nthe BF16 output tensor of the up-gate linear. Both of these two methods require a lightweight\nrecomputation of the SwiGLU and gate probability multiplication operators, so that we can save\nthe memory of the input tensors of these two operators.\nFP8 Quantization Operator Fusion Optimization.\nWe reduce data movement overhead and improve\ncomputational intensity through operator fusion, specifically: (1) fusion of permutation and FP8 quantiza-\ntion in the forward pass, and (2) fusion of SwiGLU, gate probability multiplication, and FP8 quantization\nin both forward and backward passes.\nFP8 Communication Optimization and Overlap.\nIn the forward pass, the first all-to-all communication\nis performed in FP8 precision to reduce the communication costs compared to BF16. In the backward pass,\nthe second all-to-all communication is overlapped with the computation of the up-gate linear weight\ngradient.\n5.4\nComputational Optimizations\n5.4.1\nRecomputation with Best Computation-Memory Tradeoffs\nThe conventional recomputation strategy operates at the module level to minimize computational\noverhead by focusing on cost-effective modules. In contrast, ERNIE 4.5 adopts an operator-level recom-\nputation strategy, offering a finer-grained balance between memory and computation, thereby further\n25\n\nERNIE 4.5 Technical Report\nJune 29, 2025\nRDMA Communication\nZero Cost Checkpoint\nCompute Node\nPaddlePaddle Fault Tolerance Enhancements\nModel\nExcecute\nTraceHang\nOnline\nProfiler\nOnline SDC\nScanner\nParallelized\nWarmup\nPaddlePaddle Core\nHardware\nMonitor\n2\n1\n2\n3\nData\nCompute Cluster\nFault Tolerance Core System\n3\nRapid Recovery\n2\nComprehensive Detection\n1\nEnd-to-end monitor\n3\nMulti-source Metrics\n1\nModel\nFramework\nDriver\nCentralized Risk Analyzer\n2\nComputation\nCommunication\nStorage\nMulti-level Schedule\n3\nprocess-level\nnode-level\njob-level\nFaulty Node\nAvailable Node\n2\nIsolate\nOps System\nRisk Identification\nHealthy Node\nRepair\n2\n3\nCompute Node\nPaddlePaddle Fault Tolerance Enhancements\nModel\nExcecute\nTraceHang\nOnline\nProfiler\nOnline SDC\nScanner\nParallelized\nWarmup\nPaddlePaddle Core\nHardware\nMonitor\n2\n1\n2\n3\nData\nPre-Initialization\n3\nFigure 13: Framework-native fault tolerance system.\noptimizing training performance.\nConsider three operators in Algorithm 1, op1, op2, and op3, each needing to retain input tensors for the\nbackward pass. If we opt not to recompute op2, the conventional recomputation method would retain x,\ny1, and y2 for backward computation. However, since y1 will be recomputed during the backward pass\nof recompute(op1, x), retaining y1 during the forward pass is inefficient and unnecessary. This illustrates\nthat the conventional recomputation method is suboptimal at the operator level.\nWe propose an operator-level recomputation method in Algorithm 2 for training ERNIE 4.5, where we\nretain the output tensor of op2 rather than its input tensor. This optimization allows us to retain only x\nand y2 for backward computation while excluding y1, thereby reducing memory overhead.\nTo develop an optimal recomputation strategy, we conducted a detailed analysis of each operator in the\nmodel, assessing its memory usage against computational time. By selectively applying operator-level\nrecomputation to the most cost-effective operators—those offering significant memory savings with\nminimal runtime penalties—we devised an optimal checkpointing scheme that maximizes training\nefficiency.\n5.4.2\nFlashMask for Flexible Attention Mask and Long Context Training\nWe propose FlashMask (Wang et al., 2025) to accommodate the diverse attention masks required in ERNIE\n4.5 multimodal pre-training. FlashMask introduces a memory-efficient representation for attention masks\nacross various tasks in both textual and multimodal training, reducing memory complexity from O(N2)\nto O(N). We also apply FlashMask in Supervised Fine-Tuning (SFT), Direct Policy Optimization (DPO),\nand Reinforcement Learning training, particularly for long-context training, to reduce memory usage\nand improve the throughput.\n5.5\nFramework-Native Fault Tolerance System\nThe use of large-scale GPU clusters introduces elevated failure interruptions, posing significant challenges\nto training efficiency. To mitigate the interruption frequency and minimize recovery costs during ERNIE\n4.5 training, we propose a novel framework-native fault tolerance system that spans the entire model,\nframework, and underlying hardware, as illustrated in Figure 13. Unlike traditional fault tolerance\nsystems, our system leverages detailed information on the computation and communication workloads\nof the training process, enabling robust fault detection and rapid recovery from interruptions. To illustrate\nthe benefits of deep integration of the training framework, we present TraceHang, Online SDC Scanner for\ncomprehensive detection and Parallelized Warmup, Zero Cost Checkpoint (ZCC) for rapid recovery.\nTraceHang.\nHangs without explicit failures can be particularly troubling when training on large-scale\nclusters. To address this issue, we propose the TraceHang technique that leverages parallelism information\nand communication records to automatically diagnose the initiator of such hangs. By systematically\n26\n\nERNIE 4.5 Technical Report\nJune 29, 2025\nZCC DtoH Worker\nTraining Process\nF&B\nC\nF&B\nF&B\nD\nD\nP\nD\nparameters and optimizer states remain unchanged\nU\nU\nPersistent Checkpoint\nRDMA P2P transmission\nwith adaptive bandwidth\nZCC on Cluster\nAlternative\ntransmisson\nF&B\nC\nD\n\\\nFaulty Node\nMemory\nCheckpoint\nHealthy Node\nMemory\nCheckpoint\nZCC on Training Framework\nTraining\nProcess\nZCC DtoH\nworker\nU\nOptimizer and\nparameter update\nF&B Forward & Backward pass (w/o\ninternode communication)\nC\nInternode\ncommunication\nD\nDtoH checkpoint shards\nP\nPersist to SSD/remote\nrequire conflict physical or\nlogical resources\nFigure 14: Zero Cost Checkpoint.\nanalyzing these data, TraceHang can identify the root cause of the hang, facilitating quicker resolution\nand minimizing downtime.\nOnline SDC Scanner. Silent Data Corruption (SDC) poses a significant threat to model convergence due\nto its elusive nature (Dixit et al., 2021), which traditionally requires costly offline diagnostic procedures\nfor detection. We observe that pipeline parallelism naturally introduces idle periods on devices, known\nas pipeline bubble times. During these bubble times at each stage of pipeline parallelism, we perform\ncomputations and communications with fixed inputs, subsequently verifying the results against a ground\ntruth. This online method has successfully identified several SDC nodes without adversely affecting\ntraining throughput.\nParallelized Warmup.\nIt is well known that the initial step of the training process often exhibits\ntemporary performance degradation due to the initialization of resources such as the cuBLAS handle\nand NCCL communication handle. However, when training models with a large pipeline degree, this\nperformance degradation is exacerbated by a factor of pipeline degree p due to data dependencies present\nin the pipeline warmup stages. To address this, we conducted a mock pipeline chunk operation across all\npipeline stages simultaneously, effectively warming up all devices at the onset of training. This approach\nreduces the latency of the first step to 1/p of the baseline.\nZero Cost Checkpoint (ZCC).\nWhen a node failure occurs, it typically necessitates the removal of\nthe faulty node, replacement with a new one, and subsequent resumption of training from the most\nrecent checkpoint. The frequency of checkpointing is a pivotal factor influencing recovery time after\ninterruptions; however, excessive checkpointing can severely impact training efficiency. To overcome\nthis challenge, we introduce Zero Cost Checkpoint (ZCC) technique. This innovative approach enables\ncheckpoint saving at every training step without any overhead for training throughput, thereby ensuring\nno training progress is lost during interruptions. As shown in Figure 14, ZCC operates on two sides: the\ntraining framework side and the cluster side.\nOn the training framework side, we introduce a zero-cost saving technique based on a key observation:\nparameters and optimizer states remain unchanged except during optimizer execution. This insight\nprovides ample opportunity to overlap checkpoint routines (e.g., DtoH copies and flushing to persistent\nstorage) with ongoing training routines (typical computation and communication tasks). However,\nDtoH copies during checkpointing and inter-node communication during training contend for the same\nphysical resources on the PCIe bus. To ensure that asynchronous checkpointing does not degrade\ntraining throughput, we divide training routines into two types: conflict routines (involving inter-\nnode communication, such as pipeline parallelism send/receive and ViT vision feature gather/scatter)\nand non-conflict routines (such as attention operators, dense feed forward networks and intra-node\ncommunication). DtoH copies are finely decomposed and allocated to these non-conflict routines to\navoid impacting the training process.\nOn the cluster side, we propose a zero-cost transmission technique. Upon detecting a faulty node, we\nleverage all available healthy Network Interface Cards (NICs) to adaptively maximize the bandwidth,\nsubsequently performing fast RDMA Peer-to-Peer transmission of the latest in-memory checkpoint to a\nnew healthy node. Moreover, this transmission process is fully overlapped with the subsequent recovery\noperations, such as the initialization of the new node’s execution environment. In cases where the fault\nnode’s memory is not accessible, recovery from a checkpoint on persistent storage serves as an alternative\nmethod.\nOverall, we present a framework-native fault tolerance system, which is seamlessly integrated with\n27\n\nERNIE 4.5 Technical Report\nJune 29, 2025\nPaddlePaddle. This integration enables it to fully comprehend the intricate parallelism inherent in the\ntraining processes, thereby offering a superior solution for ensuring stability in large-scale training\nclusters. Specifically, we have reduced the end-to-end automatic recovery time, which is measured from\nthe timestamp of the latest step before interruption to the timestamp of resuming exactly the same step,\nto less than 8 minutes for ERNIE 4.5. Furthermore, given the assumption that interruption frequency\nscales linearly with cluster size, training models on a 10,000-GPU cluster leveraging our fault-tolerant\nframework can sustain over 98% effective training time.\n6\nInference and Deployment\nERNIE 4.5 series consists of MoE and dense models with varying parameter sizes, suitable for various\ndeployment scenarios. Owing to the compact parameter size of ERNIE 4.5, even its largest model can\nbe efficiently deployed. To accommodate diverse application scenarios and hardware platforms, we\noffer multiple quantization schemes including FP8, INT8, INT4, and even 2-bit weight-only quantization.\nSpecifically, ERNIE-4.5-A47B model can run on 8 GPUs with 8-bit parameters or 4 GPUs with 4-bit param-\neters. Additionally, we provide Prefill-Decode (PD) disaggregation deployment with large-scale expert\nparallelism. First, we will delve into our mixed-precision quantization solution, which is designed to re-\nduce GPU memory usage and accelerate matrix computation. Next, we will introduce high-performance\noptimizations in our inference engine, such as hardware-level acceleration and speculative decoding.\nFinally, we present system-level optimization strategies for production deployment.\n6.1\nQuantization\nTo further enhance inference efficiency and support a wider range of hardware, we provide not only BF16\nand FP8 inference capabilities but also a variety of lower-precision inference options. We develop multiple\nquantization strategies to ensure that low-precision models achieve performance comparable to BF16\nmodels. For low-cost scenarios, we adopt W4A8 precision to improve inference throughput. For resource-\nconstrained scenarios, we employ 2-bit weight-only quantization to reduce the deployment barrier. For\nlong-context scenarios, we optimize memory and computation by applying KV cache quantization and\nattention quantization, respectively.\n6.1.1\nW4A8 Quantization\nIn the ERNIE-4.5-300B-A47B model, expert weights constitute over 90% of the total parameters, con-\nsuming about 40% of the inference time during the prefilling stage and about 60% during the decode\nphase of inference. To enable fast and cost-effective inference, we employ W4A8 (INT4 weights and INT8\nactivations) quantization on the General Matrix Multiplications (GEMMs) associated with the expert\ncomponents. Through the innovative design of the inference-friendly W4A8 quantization algorithms, we\nachieve speedup with no accuracy drop and minimal inference-time overhead.\nTo strike an optimal balance between accuracy and inference cost, we employ channel-wise static INT4\nquantization for expert weights and tensor-wise static INT8 quantization for activations.\nDuring the MoE compression process of ERNIE 4.5, several challenges arise:\n• Slow GPTQ on MoE: GPTQ is a widely-used approach for weight quantization (Frantar et al.,\n2022). However, for the GPTQ of MoE modules, a large dataset is required not only for activating\nall the experts but also for optimizing thousands of linear layers. This process would cause\nsignificant calibration costs, especially when there are 64 experts in ERNIE-4.5-300B-A47B.\n• Inter-Node Outliers: When deployed using Tensor Parallel (TP), weights, activations, and their\noutliers are split into all the GPU nodes, causing globally distributed quantization errors.\n• Intra-Node Outliers: For each GEMM operation within the MoE module, outliers present in\nboth the weights and activations. The effectiveness of methods such as SmoothQuant (Xiao et al.,\n2023) and Adaptive Weight Quantization (AWQ) (Lin et al., 2024b) is limited, as they tend to shift\noutliers between activations and weights, rather than fundamentally mitigating the underlying\nissue.\nWe design a quantization framework to enhance and ensure near-lossless quantization accuracy with\nminimal inference-time overhead, as illustrated in Figure 15. In detail, we introduce the Multi-Expert\nParallel Collaboration (MEPC) algorithm to streamline the calibration process. Additionally, we devise a\nframework incorporating Outlier Transfer and Layer-Adaptive Permutation Rotation techniques to mitigate\noutliers, which significantly reduces quantization error. We evaluate our quantized model against the\nW8A16 baseline across six core task categories. Results demonstrate that our method effectively preserves\n28\n\nERNIE 4.5 Technical Report\nJune 29, 2025\nExpert\nExpert\nExpert\nExpert\nExpert\nExpert\n...\n1. Multi-Expert Parallel Collaboration\nCombination and Calibration\n2. Outlier Transfer Algorithm Across GPU Nodes \n3. Layer-Adaptive Permutation Rotation\nBlock-\nRotation\nPermutation\nQ\nQuantization\nLinear\nActivations\nPR\nTransformation\nUp-Gate Linear\nDown Linear\nD\nC\nB\nA\nReorder\nA\nB\nC\nD\nGPU1\nGPU2\nGPU3\nGPU4\nAll \nReduce\noutliers\nQ\nPR\nQ\nPR\nQ\nPR\nQ\nPR\nExpert\nFigure 15: Quantization scheme in the MoE module. Three key methods: 1) Multi-Expert Parallel\nCollaboration, combines all experts into one tensor to calibrate efficiently and collaboratively; 2) Outlier\nTransfer Algorithm Across GPU Nodes, aggregates all outliers onto GPU4 by reordering weights of linear;\n3) Layer-Adaptive Permutation Rotation, mitigates the impact of intra-node outliers by permutation and\nrotation.\nmodel performance: performance on general tasks (-0.49%), instruction following (-0.06%), reasoning\ntasks (+0.11%), math tasks (-0.02%), and code tasks (-1.10%) remains comparable to baseline levels,\nconfirming the quantization’s effectiveness.\nMEPC: Multi-Expert Parallel Collaboration Quantization.\nTo address the aforementioned bottlenecks\nof MoE-LLMs activations calibration and GPTQ, we adopt a prefill-only approach during the calibration\nprocess to ensure that a sufficient number of experts are activated. For experts remaining unactivated,\nwe employ a shared quantization parameter strategy. Specifically, the quantization parameters for\nunactivated experts are set as the average of those from all activated experts within the same layer. This\nmethod is motivated by our empirical observation that experts in the same layer tend to have highly\nsimilar quantization parameters during the quantization process.\nInspired by QMoE (Frantar & Alistarh, 2024), we process all experts in parallel when applying GPTQ by\nconcatenating their weights, thereby fully utilizing the parallel computation capabilities of GPUs and\nenabling highly efficient GPTQ quantization. Furthermore, we enhance this process with a hotspot-expert\nparallel GPTQ update strategy. Here, hotspot experts—those deemed more important based on their\ntoken activation frequency—are prioritized for optimization during quantization. Specifically, since\nparallel GPTQ quantization can impose a substantial computational burden due to the large Hessian\nmatrix in the Up-Gate Linear, we employ a targeted GPTQ update strategy for hotspot experts within\nthis layer. In contrast, the Down Linear contains a much smaller Hessian matrix, which allows us to\nefficiently perform full GPTQ updates for all experts in this layer without excessive computational cost.\nOutlier Transfer Algorithm Across GPU Nodes.\nAs previously discussed, activation outliers dis-\ntributed across GPU nodes can result in quantization loss for all nodes. In the context of TP, Down Linear\nis partitioned across N GPU nodes, with each node maintaining its own quantization scale. Inspired by\nRPTQ (Yuan et al., 2023), we implement an outlier transfer algorithm that aggregates all outliers onto a\nsingle node, while distributing the regular (non-outlier) activations to the remaining nodes. Specifically,\nwe first collect the channel-wise absolute maximum activation values from all N GPUs. Using these statis-\ntics, we globally rank the columns of Up-Gate Linear and the rows of Down Linear, co-locating weight\nchannels with similar activation ranges onto the same GPU. This reordering adjusts the local weight and\nactivation layouts without affecting the final model output, as subsequent AllReduce operations restore\nconsistency through element-wise summation. Notably, our approach performs this reordering offline,\nresulting in a quantization-friendly model without compromising inference efficiency.\n29\n\nERNIE 4.5 Technical Report\nJune 29, 2025\nLayer-Adaptive Permutation Rotation Quantization.\nINT8 static quantization is widely adopted for\nits superior inference efficiency and broad hardware compatibility. However, it suffers from significant\nperformance degradation in the presence of intra-node outliers. Alternative approaches such as dynamic\nquantization or FP8, while effective at handling outliers, are often limited by computational overhead\nor hardware support. To address this challenge, we propose a layer-adaptive permutation rotation\nquantization framework that efficiently mitigates the impact of outliers while maintaining hardware\nfriendliness.\nInspired by Quarot (Ashkboos et al., 2024), we first employ a block-rotation strategy to smooth outliers\nin either weights or activations. To further enhance the robustness of this approach, we integrate a\npermutation mechanism (Lin et al., 2024a), enabling a more flexible redistribution of outliers within\neach block. Our method begins by pre-analyzing the outlier distribution in each layer’s activations to\ndetermine whether outlier smoothing should prioritize weights or activations. For layers exhibiting\nclustered outliers, we apply the permutation-rotation procedure to redistribute these outliers within the\nrotated blocks, thereby reducing quantization noise and preserving model accuracy. This design achieves\na balance between quantization granularity, computational efficiency, and hardware compatibility.\n6.1.2\n2-Bit Quantization\nTo further lower the entry barrier for ERNIE 4.5, we implement a nearly lossless 2-bit quantization\nalgorithm, thereby reducing its model size by 80% from the BF16 baseline. The 2-bit ERNIE-4.5-300B-\nA47B can be deployed on one 141GB H20 GPU.\nFor scalar-based methods, compressing a model to 2 bits would result in a significant collapse of the\nmodel’s effectiveness. Currently, the state-of-the-art weight-only quantization algorithms primarily focus\non employing vector quantization (Gray, 1984; Liu et al., 2024a) and incoherent processing (Tseng et al.,\n2024a) to optimize model performance under extremely low-bit quantization. However, both of these\napproaches introduce computational overhead during inference.\n• Exponential Cost in Dequantization: Vector quantization necessitates an additional codebook\nto store the centroid vectors, and the dequantization process involves indexing operations within\na vast codebook space to reconstruct the weights, which is a time-consuming task.\n• Computational Overhead from Incoherent Processing: Meanwhile, although incoherent pro-\ncessing in weight-only quantization can mitigate quantization errors, it requires extra real-time\nprocessing of weights during the inference phase.\nTo address the aforementioned issues, we propose Convolutional Code Quantization (CCQ), a scalar\nquantization algorithm based on the convolutional code. The approach not only retains the high-precision\ndata quantization capability of vector quantization but also preserves the low computational complexity\nof scalar quantization. Combined with scale quantization and optimization, we achieve the highest\npossible compression ratio while simultaneously minimizing inference overhead.\nConvolutional Codebook.\nInspired by QTIP (Tseng et al., 2024b), we innovatively integrate convolu-\ntional code with scalar quantization through a series of meticulously designed coding structures. Based\non convolutional codes, we construct a lookup-free codebook that achieves a linear mapping between the\ncodebook and weight vectors, thereby optimizing inference performance. Meanwhile, by drawing on the\nconcept of data mapping from vector quantization, we minimize the performance degradation of the\nmodel under extremely low-bit conditions.\nHybrid Encoding.\nWe employ convolutional codes with varying coding configurations to accommo-\ndate the storage of encoded values in INT8 and INT16 formats. As a result, we successfully compress\n4-bit scalar quantization to an equivalent of 2.75 bits and 3-bit scalar quantization to 2.5 bits.\nCode Cluster.\nFurthermore, by analyzing the distribution of encoded values across each channel,\nwe observe that they conform to a normal distribution, enabling deeper compression along the coding\ndimension. Through clusters of the convolutional codes, we can compress any coding configuration to an\nequivalent of 2 bits, thereby further enhancing the model compression rate.\n6.1.3\nAttention and KV Cache Quantization\nSupporting larger batch sizes and longer context lengths is essential for reducing the cost per request and\naccommodating diverse application scenarios. The GPU memory overhead of the Key-Value (KV) cache\nand the computational cost of attention mechanisms escalate linearly or even quadratically, posing new\nbottlenecks for inference computation.\n30\n\nERNIE 4.5 Technical Report\nJune 29, 2025\nWq\nWk\nWv\nRoPE\nRoPE\nBMM Softmax\nWo\nR2\nR2\n-1\nBMM\nV\nK\nQ\nKV Quantization\nAttention Quantization\nV\nK\nCache KV\nWq\nWk\nWv\nRoPE\nRoPE\nI/F\nSoftmax\nWo\nR2\nR2\n-1\nV\nK\nQ\nCache KV\nI/F\nBMM\nI/F\nBMM\nQ\nQ\nQ\nQ\nQ\nQ\nDQ\nDQ\nDQ\nQ\nquantization\nquantized \nactivation\nmerged \noperation\ndequantization\nBF16 activation\nWq\nquantized\n weight\nBMM\nBF16 \noperation\nI/F\nBMM\nINT8/FP8 \noperation\nDQ\nFigure 16: Quantization scheme in the attention module: KV Cache is quantized as 4 bits or 8 bits for\nmemory reduction. Batched Matrix Multiplication (BMM) and Softmax are computed in 8-bit precision\nfor latency reduction. Both quantization recipes involve RHT to eliminate outliers in the activations.\nMainstream approaches like KIVI (Zirui Liu et al., 2023) and KVQuant (Hooper et al., 2024) maintain\naccuracy during KV cache quantization by employing dynamic token-wise quantization and outlier\nisolation at inference time. However, this dynamic processing incurs inference latency. To alleviate\nthis issue, we propose a static quantization solution for both the KV cache and attention mechanisms\n(illustrated in Figure 16). In pursuit of the optimal trade-off between model accuracy and inference\nperformance, we introduce several key strategies.\n• Flexible Quantization Configurations: Our proposed method supports two distinct quantiza-\ntion granularities, head-wise and channel-wise. We provide a variety of precision levels, such as\nFP8, INT8, and INT4 to cater to diverse inference requirements and hardware constraints.\n• Static Quantization: We utilize static quantization, employing a dedicated sampled dataset\nderived from the SFT phase, which circumvents the need for scale collections during the inference\nprocess.\n• Light-Weight RHT: We leverage blocked Random Hadamard Transform (RHT) as proposed\nin Liu et al. (2024d) and Lin et al. (2024a). This technique is employed to mitigate the impact\nof outliers in the activations. Notably, the RHT operation can be seamlessly integrated into the\nmatrices Wv and Wo, thereby incurring no additional computational overhead during inference.\nTo further enhance the precision of FP8 quantization, we adopt a strategy of reducing the representable\nrange of FP8 values, which yields substantial improvements in quantization performance. Experimental\nresults demonstrate that this approach achieves approximately 2% accuracy improvement on reasoning\ntasks. We attribute this improvement to the characteristics of the E4M3 FP8 format, which exhibits\nsignificant sparsity and large quantization errors near its extreme values (±448). To address this issue, we\nproactively clip or exclude certain outliers during the calibration process. Specifically, we set a customized\nvalue for fp8 max (distinct from the default 448), as shown in Equation 6:\nxquantized = float8 e4m3\n\u0012\nclip\n\u0012\u0016 xbf16 × fp8 max\nscale\n\u0019\n, −448.0, 448.0\n\u0013\u0013\n.\n(6)\n31\n\nERNIE 4.5 Technical Report\nJune 29, 2025\n0.96\n1.93\n3.78\n7.51\n15.15\n30.24\n2.09\n4.18\n8.21\n16.27\n32.52\n64.94\n0.00\n10.00\n20.00\n30.00\n40.00\n50.00\n60.00\n70.00\n256\n512\n1024\n2048\n4096\n8192\nKernel Latency/(ms)\nTokens Per Expert\nPrefill\nW4A8\nW4A16\n(a)\n0.31\n0.31\n0.32\n0.33\n0.35\n0.36\n0.37\n0.37\n0.37\n0.38\n0.60\n0.60\n0.00\n0.10\n0.20\n0.30\n0.40\n0.50\n0.60\n0.70\n8\n16\n24\n32\n48\n64\nKernel Latency/(ms)\nTokens Per Expert\nDecode\nW4A8\nW4A16\n(b)\nFigure 17: Kernel execution time on A800, N=1792, K=8192. (a) Comparison of prefill kernel execution\ntime, (b) Comparison of decode kernel execution time.\n6.2\nInference Acceleration\nHaving fully considered the collaboration with quantization algorithms and hardware architectures, we\ndevelop a series of efficiently optimized inference kernels. For the MoE W4A8 quantization, an efficient\nCUTLASS kernel, with fast bit-shift dequantization, achieves significant improvements in memory\nthroughput and inference speed. For attention and KV quantization, we design different polynomial\napproximations of the exponential function for INT8-formatted and FP8-formatted softmax calculation\non different GPU architectures, delivering 1.68x speedup over FlashAttention-2 and 1.48x speedup over\nFlashAttention-3.\n6.2.1\nW4A8 Kernel Acceleration\nIn LLM inference, 4-bit weight-only and INT8/FP8 quantization are commonly employed acceleration\ntechniques, but W4A8-based GEMM on the MoE module delivers two key advantages compared with\nthem: (1) a 50% memory reduction compared to FP8/INT8 quantization through 4-bit weight representa-\ntion, and (2) 2 times improvement in theoretical computational throughput over FP16 or BF16 baseline by\nleveraging INT8 Tensor Core.\nTo optimize computational efficiency, we streamline the fast conversion process from INT4 to INT8\nweights which is co-designed with quantization algorithms. The technical details are as follows:\n• Range Mapping: The representable range of INT4 is [-8, 7]. We constrain the mapping range to\n[-7, 7].\n• Bit-Shift Conversion: The INT4 weights are converted to INT8 via a 4-bit left shift operation,\nresulting in an effective mapped range of [-112, 112] for the INT8 weights.\n• Weight Layout Optimization: To minimize operations required for achieving the Tensor Core-\ncompatible INT8 weight layout, we employ a weight prepacking strategy that arranges weights\nin an interleaved format. This design enables the conversion of every eight INT8 data elements\nusing only 3 instructions (via efficient utilization of LOP3 and bit-shift operations).\nWe leverage Tensor Core units for multiply-accumulate operations and implement a high-efficiency kernel\nvia CUTLASS, with dequantization in the epilogue phase to maximize pipeline throughput. A problem\nvisitor module dynamically balances expert workloads, while systematic hyperparameter pre-tuning\nenables real-time adaptive configuration selection during inference.\nThe aforementioned optimizations yield the following performance gains for W4A8 MoE GEMM kernel:\n• Achieves 70% to 80% of memory bandwidth for decoder-shaped workloads.\n• Delivers over 100% speedup on encoder-shaped workloads compared to W4A16 and over 20%\nspeedup on decoder-shaped workloads.\nTo mitigate precision loss during intermediate computations, we maintain INT32 accumulation precision\nthroughout the MAC phase. For activations, we support flexible quantization granularity (per-token or\nper-expert), while for weights, we adopt per-channel quantization to achieve optimal accuracy.\n32\n\nERNIE 4.5 Technical Report\nJune 29, 2025\n6.2.2\nEfficient Attention Kernel\nTo balance accuracy with hardware capabilities in attention quantization, we leverage FP8 computation on\nHopper architecture GPU and INT8 on Ampere architecture GPU. When batched GEMM computations\nleverage Tensor Core-based INT8 or FP8, their execution time is significantly reduced. Consequently,\nsoftmax and dequantization operations computed on CUDA Cores emerge as the performance bottleneck.\nTo address this, we employ a novel implementation for exponential function and dequantization to\nminimize computation overhead on the CUDA Cores.\nINT8-Formatted Attention.\nWe implement a fast exponentiation computation, building upon the\nfindings presented in Schraudolph (1999). The exponential function for floating-point data x can be\nexpressed as:\nex ≈Ff loat\n\u0010\n223 × ⌊scale × x + bias⌋\n\u0011\n,\n(7)\nwhere scale and bias are predefined constants, and ⌊⌋is calculated by integer casting, Ff loat represents\nreinterpret cast in programing. After the INT8 quantization of Q ∗KT operation, the dequantization step\n(with a coefficient denoted as Sqk ) and the subsequent exponential operation can be merged into a single\nFused Multiply-Add (FMA) instruction, as described by the following formula:\nex×Sqk ≈Ff loat\n\u0010\n⌊223 × scale × Sqk⌋× x + ⌊223 × bias⌋\n\u0011\n.\n(8)\nExperimental results demonstrate that this approach achieves a 68% performance improvement over\nFlashAttention-2 in the prefilling stage with 128K as the input sequence length on A800 GPUs.\nFP8-Formatted Attention.\nFor the attention module using C4 (asymmetrical channel-wise 4-bit quanti-\nzation for KV cache), we quantize the two GEMM operations into the FP8E4M3 format and implement\nthe following optimizations.\n• Fast Conversion: By placing 4-bit KV data in the lower 4 bits of FP8E4M3 data, UINT4-to-\nFP8E4M3 conversion is established:\nY = 2−9 × X\nY in FP8E4M3\nX in UINT4.\n(9)\nSimilarly, UINT8-to-BF16 conversion can also adopt this method. For CacheKV storage, the\noriginal data sequence [0,1,2,3,4,5,6,7] is interleaved into [0,4,1,5,2,6,3,7], enabling the conversion\nof eight 4-bit values with a single bit operation and LOP3 instruction. a single SHIFT instruction\nand a single LOP3 instruction. This reduces the cost to 0.25 instructions per value conversion.\n• Fast Dequantization: For the dequantization of K, the following equivalence holds:\nP = softmax(\nQ × (KT −Zk) × Sqk\n√dk\n)\n= softmax((Q × Sqk) × KT\n√dk\n),\n(10)\nwhere Zk is the dequantization zero point of K, and Sqk is the product of dequantization scales of\nQ and K. The fast UINT4-to-FP8E4M3 conversion can be applied to K, which only requires shift\nand bitwise AND operations, significantly reducing FMA instructions and data type conversion\noverhead. For V dequantization, the computation follows:\nO = P × V ∗Sv −P × Zv × Sv,\n(11)\nwhere Zv and Sv are the quantization zero point and coefficient of V. Since the matrix multipli-\ncation of P × V accumulates along the sequence length dimension while Sv is a channel-wise\nparameter, the dequantization of V can be deferred until after P × V. Additionally, the term\nP × Zv × Sv can be efficiently computed using pre-stored softmax row summations.\n• Matrix Transposition: The hopper architecture GPUs require matrix B in A*B to be loaded from\nshared memory for GEMM. Therefore, we restructure the computation flow for 4-bit quantized\nK and V, transformed into following formula to avoid moving quantized K and V from registers\nto shared memory.\nP = (K × QT)T\nand\nO = (VT × PT)T.\n(12)\nThese optimizations generally make the input 128k sequence length of C4 decode attention reach more\nthan 80% of the theoretical bandwidth utilization. In the FP8 attention, the exponential function is\napproximated using its limit. Ultimately, compared to FlashAttention-3’s FP8 implementation, this\napproach delivers an average 50% speedup in the prefilling stage with 128K as the input sequence length.\n33\n\nERNIE 4.5 Technical Report\nJune 29, 2025\n6.2.3\nSpeculative Decoding\nERNIE 4.5 is equipped with a Multi-Token Prediction (MTP) module, so speculative decoding is used\nduring the inference phase. We implement a unified speculative decoding framework, which features\na concurrent proposer architecture, enabling seamless integration with MTP schemes. By leveraging\nparallel sampling and verification, as well as deeply customized attention kernel, MTP achieves a\n60% increase in output throughput while maintaining TPOT (Time per Output Token) comparable to\nautoregressive decoding.\n6.3\nDeployment\nFor the ERNIE-4.5-300B-A47B model, we optimize system throughput and latency through a PD disag-\ngregation deployment combined with expert parallelism. Specifically, the prefilling stage employs 8-way\nexpert parallelism (EP8) without tensor parallelism for the attention module. During the decoding stage,\nthe system supports flexible parallelization ranging from EP8 to EP64 configurations. Given the distinct\ncomputational characteristics of prefilling and decoding stages, we adopt different quantization schemes\ntailored to each stage’s requirements. The prefilling phase utilizes block-wise FP8 quantization, while the\ndecoding stage employs W4A8 quantization to optimize memory bandwidth and reduce latency.\nThe efficacy of the PD disaggregated system critically depends on three key factors: (1) inter-node\ntransmission efficiency of KV Cache, (2) all-to-all communication efficiency for expert routing, and (3)\nload balancing across distributed compute resources. These components collectively determine the\noverall system performance and scalability of the disaggregated architecture.\nKV Cache Transfer.\nWe develop a cross-node KV cache transfer module based on Remote Direct Mem-\nory Access (RDMA) with minimal dependencies. It only requires a basic RDMA runtime environment\nsuch as OFED, making it lightweight and easy to deploy. Moreover, to achieve better performance, our\nimplementation has some detailed differences with existent solutions, including reducing the Completion\nQueue Entry (CQE) number and enabling PCIe relaxed ordering. Furthermore, building on an unified\nKV Cache transfer design, our system intelligently routes KV cache traffic through the optimal channel,\nutilizing NVLink intra-node and RDMA inter-node.\nAll-to-All Communication.\nFor all-to-all communication, we utilize NVLink P2P copy for intra-node\ndata transfer in low-latency scenarios. This enhancement improves decoding throughput in smaller-scale\ndecoding instances, with EP8 deployment efficiency increased by 70% compared to EP16.\nMulti-level Load Balancing.\nLoad balancing is extremely important in a large-scale PD disaggrega-\ntion system that incorporates data parallelism and expert parallelism. We implement multi-level load\nbalancing in our system.\n• Load Balancing of Data Parallelism: A global load-aware scheduler dispatches queries based on\nthe KV cache hit rate and token count, ensuring that the number of tokens processed each DP\nrank is relatively balanced in the prefilling and decoding stage.\n• Load Balancing of Expert Parallelism: Our system incorporates a dynamic expert redundancy\nstrategy (DeepSeek-AI et al., 2024b) and enforces global expert rescheduling to optimize load\nbalancing in MoE computations. To reduce the impact of expert rearrangement on online services,\nwe employ a scheme of weight prefetching and asynchronous loading to achieve extremely low\nlatency service stagnation. A gray-level rearrangement strategy is used to avoid the occurrence\nof the thundering herd problem and to ensure the stability of the overall service.\n• Load Balancing of PD Disaggregation: In the PD disaggregation system, a mismatched PD\nratio can make it challenging to meet Service-Level Objective (SLO) and may result in low GPU\nutilization. In online service, dynamically varying input and output lengths result in an imbalance\nbetween the prefilling and decoding stages. To mitigate this, we continuously monitor workload\nacross all instances and dynamically adjust the ratio between prefill and decode to achieve\nrelative balance. However, we observe that system throughput remains frequently bottlenecked\nby resource constraints in the prefilling stage. To address this limitation, our system supports\ndynamic role switching for instances. This capability enables decoding instances to intelligently\nhandle prefilling for short-input requests based on real-time load conditions. Critically, this\napproach incurs virtually no impact on TPOT while simultaneously boosting resource utilization\nwithin the Decode stage.\nLeveraging the aforementioned optimization strategies, ERNIE-4.5-300B-A47B achieves an inference\nperformance of 56k input TPS and 18k output TPS per H800 node without prompt caching for an input\n34\n\nERNIE 4.5 Technical Report\nJune 29, 2025\nlength of 2K and an output length of 400, under the constraint of a 50ms Time Per Output Token (TPOT)\nlatency.\nIt is worth noting that, under our PD disaggregation deployment solution for the largest ERNIE 4.5 lan-\nguage model, both prefilling and decoding demand a minimum deployment unit of one node with 8 GPUs\nwhich significantly enhances operational efficiency and reduces the complexity of cluster management in\nproduction environments.\nWhile the PD disaggregation on large-scale GPU clusters delivers outstanding performance, streamlined\nsingle-node deployment provides advantages for rapid application in diverse scenarios. Even the largest\nmodel, ERNIE-4.5-A47B, can be deployed on a single node, such as 4x80GB A800 or H800 with 4-bit\nprecision and 1x141G H20 with 2-bit precision as mentioned above. ERNIE 4.5 can not only be efficiently\ndeployed on NVIDIA GPUs but also on a wide range of hardware platforms based on PaddlePaddle,\nincluding Kunlunxin XPU, Hygon DCU, Ascend NPU, and beyond.\n7\nOpen-Source Development Tools\nWe open-source ERNIEKit1 and FastDeploy2 based on PaddlePaddle framework to streamline model\ntraining and deployment for ERNIE 4.5. These toolkits feature industrial-grade capabilities, resource-\nefficient training and inference workflows, and multi-hardware compatibility.\n7.1\nERNIEKit\nERNIEKit is an industrial-grade development toolkit for ERNIE 4.5. It provides model training and\ncompression capabilities, including pre-training, Supervised Fine-Tuning (SFT), Low-Rank Adaptation\n(LoRA), Direct Preference Optimization (DPO), Quantization-Aware Training (QAT) and Post-Training\nQuantization (PTQ) techniques. To enable developers to fully experience the capabilities of ERNIE 4.5,\nwe have introduced the following technical innovations:\n• Industrial-Grade High-Performance Pre-Training: The toolkit has provided a high-performance\nimplementation of our largest ERNIE 4.5 language model pre-training, including the hybrid\nparallelism training strategy and FP8 mixed precision optimization.\n• Low-Bit Quantization-Aware Fine-tuning: To significantly reduce fine-tuning and deployment\nresources for ERNIE 4.5, we introduce a novel FP8-QAT solution integrating low-precision\ntraining with optimizer offloading. The resulting models match the quality of BF16 fine-tuned\n(SFT) models while reducing the minimum GPU requirement of our largest ERNIE 4.5 language\nmodel from 96 GPUs to 16 GPUs. Crucially, unlike pre-training FP8 schemes requiring dynamic\n(block/tile-wise) quantization, models from our FP8-QAT support efficient offline tensor-wise\nFP8 quantization for inference, eliminating runtime quantization overhead.\n• Visual Training & Inference Interface: The integrated Gradio-based WebUI supports zero-code\nfine-tuning, alignment, and inference operations on ERNIE 4.5 with out-of-the-box functionality.\n7.2\nFastDeploy\nFastDeploy is an inference and deployment toolkit for large language models and vision language\nmodels. It is designed for ease of use, offering out-of-the-box compatibility with vLLM interfaces. To\naddress the requirements of both enterprise users and individual developers, we have introduced the\nfollowing technical features:\n• PD Disaggregation with Multi-level Load Balancing: We open-source an industrial-grade\nPrefill-Decode Disaggregation Deployment with context caching. It offers optimal distributed\ninference configurations for NVIDIA GPU based on ERNIE 4.5’s architecture characteristics.\nBenefiting from the unified KV Cache transfer design, our system automatically selects the most\nefficient communication link between NVLink and RDMA. During multi-machine deployment,\ninstances dynamically switch between prefill and decode roles according to load conditions to\nimprove throughput.\n• Comprehensive Low-Bit Quantized Inference Support: FastDeploy supports a variety of\nquantization precisions, including W8A8, W8A16, W4A8, W4A16, and W2A16, with data types\nsuch as INT4, INT8, FP8, and BF16. Notably, we provide a built-in 2-bit weight-only quantized\nmodel to lower the deployment resource requirements for ERNIE 4.5. This model delivers nearly\n1https://github.com/PaddlePaddle/ERNIE\n2https://github.com/PaddlePaddle/FastDeploy\n35\n\nERNIE 4.5 Technical Report\nJune 29, 2025\nlossless performance compared to FP8 precision across multiple benchmarks, while enabling\nsingle-card inference on NVIDIA H20 GPU with 141GB of memory.\n• Multi-hardware Support: Benefiting from PaddlePaddle’s multi-hardware adaptation capabil-\nities, in addition to NVIDIA GPU, ERNIE 4.5 also supports inference deployment on various\nchips including Kunlunxin XPU, Hygon DCU and Ascend NPU.\n8\nEvaluation and Results\nTo comprehensively demonstrate the capabilities of ERNIE 4.5, we conduct extensive evaluations on a\nwide variety of text and vision benchmarks. The comparative results of ERNIE 4.5’s language models\nand multimodal models against other leading models are presented in Sections 8.1 and 8.2, respectively.\n8.1\nEvaluation of Language Models\n8.1.1\nResults of Pre-Trained Language Models\nWe present a comprehensive evaluation of ERNIE-4.5-Base against state-of-the-art models, including\nDeepSeek-V3-Base (DeepSeek-AI et al., 2024b) and Qwen3-30B-A3B-Base (Yang et al., 2025a). We conduct\nsystematic evaluations across a diverse set of benchmarks encompassing five fundamental capabilities for\nthese pre-trained models. The evaluation benchmarks and corresponding settings are detailed as follows:\n• General Tasks: C-Eval (5-shot) (Huang et al., 2023), CMMLU (5-shot) (Li et al., 2024a), MMCU\n(5-shot) (Zeng, 2023), AGIEval (5-shot) (Zhong et al., 2024), MMLU (5-shot) (Hendrycks et al.,\n2021a), MMLU-Redux (5-shot) (Gema et al., 2025), and MMLU-Pro (5-shot) (Wang et al., 2024c),\nassessing comprehensive knowledge and understanding across various domains.\n• Factual Knowledge: SimpleQA (10-shot) (Wei et al., 2024a) and ChineseSimpleQA (10-shot) (He\net al., 2024a), testing the grasp of world knowledge across different languages.\n• Reasoning: BBH (3-shot) (Suzgun et al., 2023), DROP (3-shot) (Dua et al., 2019), ARC-Easy &\nChallenge (5-shot) (Clark et al., 2018), HellaSwag (5-shot) (Zellers et al., 2019), PIQA (5-shot) (Bisk\net al., 2020), WinoGrande (5-shot) (Sakaguchi et al., 2020), and CLUEWSC (5-shot) (Xu et al., 2020),\nevaluating logical reasoning, reading comprehension, and commonsense inference capabilities.\n• Math: GSM8K (5-shot, we removed the calculator tag to ensure stable model performance) (Cobbe\net al., 2021), MATH (8-shot, chain-of-thought and direct-answer) (Hendrycks et al., 2021b),\nCM17K (5-shot) (Qin et al., 2021), MGSM (8-shot) (Shi et al., 2023), ASDIV (5-shot) (Miao et al.,\n2020), MATHQA (5-shot) (Amini et al., 2019), CMATH (5-shot) (Wei et al., 2023), and SVAMP (5-\nshot) (Patel et al., 2021), evaluating mathematical problem solving at different levels of difficulty.\n• Coding: Evalplus (0-shot, including HumanEval+ and MBPP+) (Liu et al., 2023), and MultiPL-\nE (0-shot) (Cassano et al., 2023), assessing programming abilities and code understanding in\nmultiple programming languages.\nThe results are summarized in Table 4. Based on the overall evaluation, we highlight the following key\nfindings regarding the performance of ERNIE-4.5-Base models.\nERNIE-4.5-300B-A47B-Base.\nERNIE-4.5-300B-A47B-Base surpasses DeepSeek-V3-671B-A37B-Base on\n22 out of 28 benchmarks, demonstrating leading performance across various tasks. These results highlight\nthe significant performance gains achieved by scaling up the ERNIE-4.5-Base models, particularly in\ngeneral tasks, reasoning tasks, and knowledge-intensive tasks, when compared to other state-of-the-art\nlarge models. The model demonstrates particularly strong performance on general and knowledge-\nintensive tasks in Chinese, including CMMLU and ChineseSimpleQA. This superior performance in\nChinese can be attributed to the iterative refinement of pre-training data. In addition, the model’s\noutstanding QA performance benefits from the optimization of commonsense data formats and the\nincorporation of high-quality synthetic data, further strengthening its ability to handle factual question\nanswering and complex Chinese linguistic contexts.\nERNIE-4.5-21B-A3B-Base.\nWith a total parameter size of 21B (approximately 70% that of Qwen3-30B),\nERNIE-4.5-21B-A3B-Base outperforms Qwen3-30B-A3B-Base on several math and reasoning benchmarks,\nincluding BBH and CMATH. ERNIE-4.5-21B-A3B-Base remains highly competitive given its significantly\nsmaller model size, demonstrating notable parameter efficiency and favorable performance trade-offs.\n36\n\nERNIE 4.5 Technical Report\nJune 29, 2025\nCapability\nBenchmark\nERNIE-4.5-\n0.3B-\nBase\nQwen3-\n30B-A3B-\nBase\nERNIE-4.5-\n21B-A3B-\nBase\nDeepSeek-V3\n671B-A37B-\nBase\nERNIE-4.5-\n300B-A47B-\nBase\nGeneral\nC-Eval\n40.7\n87.2\n88.0\n90.2\n91.5\nCMMLU\n39.8\n86.0\n86.2\n88.2\n91.2\nMMCU\n37.2\n88.8\n94.0\n94.0\n95.9\nAGIEVAL\n28.5\n72.8\n68.4\n75.8\n78.4\nMMLU-Pro\n16.0\n56.7\n51.2\n66.7\n69.5\nMMLU\n41.9\n81.0\n78.9\n87.9\n87.4\nMMLU-Redux\n43.2\n84.6\n80.7\n89.4\n89.2\nKnowledge SimpleQA\n1.8\n7.1\n30.4\n24.9\n38.4\nChineseSimpleQA\n7.4\n52.0\n54.8\n64.8\n72.2\nReasoning\nBBH\n30.4\n72.7\n77.5\n87.5\n89.4\nARC-Easy\n60.7\n98.6\n96.9\n98.7\n99.2\nARC-Challenge\n40.6\n93.7\n90.7\n95.1\n96.3\nHellaSwag\n33.0\n87.7\n92.1\n91.3\n96.6\nPIQA\n55.2\n91.0\n80.6\n94.1\n94.6\nWinoGrande\n51.3\n76.3\n70.6\n88.2\n88.3\nDROP\n28.6\n39.6\n70.8\n84.6\n82.8\nCLUEWSC\n48.6\n76.8\n76.2\n81.4\n79.9\nMath\nGSM8K\n25.2\n70.8\n81.0\n90.6\n91.8\nMATH\n12.4\n61.0\n50.8\n63.0\n69.1\nCM17K\n4.6\n69.4\n64.9\n79.3\n86.4\nMGSM\n2.7\n71.2\n69.2\n79.8\n88.6\nASDIV\n29.8\n82.8\n83.5\n89.7\n90.2\nMATHQA\n21.6\n39.4\n56.1\n76.9\n83.0\nCMATH\n52.2\n88.9\n93.7\n90.7\n96.2\nSVAMP\n24.0\n86.3\n79.7\n92.7\n90.0\nCoding\nHumanEval+\n25.0\n83.5\n86.0\n76.0\n84.8\nMultiPL-E\n14.2\n66.1\n65.4\n63.0\n68.6\nMBPP+\n40.2\n76.2\n75.1\n76.7\n74.9\nTable 4: Performance of ERNIE-4.5-Base pre-trained models. Here, we report our own evaluation results\nfor Qwen3-30B-A3B-Base and DeepSeek-V3-671B-A37B-Base models instead of scores reported in their\noriginal papers. Bold values indicate the best scores within each group of comparable parameter sizes.\n8.1.2\nResults of Post-Trained Language Models\nWe present a comprehensive evaluation of ERNIE-4.5 against state-of-the-art models, including DeepSeek-\nV3-0324 (DeepSeek-AI et al., 2024b), GPT-4.1 (OpenAI, 2025a), and Qwen3-235B-A22B (Yang et al., 2025a)\nunder non-thinking mode. We conduct systematic evaluations across a diverse set of benchmarks encom-\npassing six fundamental capabilities as shown in Table 5. The evaluation benchmarks and corresponding\nmethodologies are detailed as follows:\n• General Tasks: MMLU (Hendrycks et al., 2021a), MMLU-Pro (Wang et al., 2024c), C-Eval (Huang\net al., 2023), and CMMLU (Li et al., 2024a), LiveBench (2024-11-25) (White et al., 2025), assessing\nbroad knowledge and understanding across multi-domain question answering.\n• Factual Knowledge: ChineseSimpleQA (He et al., 2024b) and SimpleQA (Wei et al., 2024a),\ntesting the grasp of world knowledge across different languages\n• Reasoning: BBH (Suzgun et al., 2023), MUSR (Sprague et al., 2024), DROP (Dua et al., 2019),\nZebralogic (Lin et al., 2025), evaluating logical reasoning, commonsense inference, and real-time\nproblem solving.\n• Math: MATH-500 (Hendrycks et al., 2021c), GSM8K (Cobbe et al., 2021), CMATH (Wei et al.,\n2023), OlympicArena (Huang et al., 2024), AIME’24 and AIME’25 (AIME, 2025) , evaluating\ncomplex mathematical reasoning and problem solving.\n• Coding: HumanEval+ (Liu et al., 2023), MBPP+ (Liu et al., 2023), LiveCodeBench (v6, 20240801-\n20250501) (Jain et al., 2025), FullStackBench (Cheng et al., 2024), measuring code generation\ncorrectness and code comprehension across multiple programming languages.\n37\n\nERNIE 4.5 Technical Report\nJune 29, 2025\nCapability\nBenchmark\nQwen3-235B-\nA22B\nDeepSeek-\nV3-0324\nGPT-4.1\nERNIE-4.5-\n300B-A47B\nGeneral\nC-Eval\n86.1\n87.3\n77.9\n90.6\nCMMLU\n87.5\n88.2\n78.1\n90.2\nMMLU\n85.5\n86.5\n90.2\n86.5\nMMLU-Pro\n72.9\n81.2\n80.7\n78.4\nLiveBench\n62.5\n66.3\n67.7\n68.9\nKnowledge\nSimpleQA\n12.4\n27.3\n40.2\n45.9\nChineseSimpleQA\n63.0\n72.0\n64.0\n77.1\nReasoning\nBBH\n84.5\n91.0\n84.1\n94.3\nMUSR\n66.8\n65.9\n62.6\n69.9\nDROP\n88.7\n89.7\n89.2\n91.1\nZebralogic\n37.7\n81.8\n56.3\n58.1\nMath\nMATH-500\n91.2\n94.0\n92.8\n96.4\nGSM8K\n96.4\n96.3\n95.9\n96.6\nCMath\n95.7\n94.8\n92.8\n96.7\nOlympicArena\n63.7\n76.2\n69.5\n75.3\nAIME’24\n40.1\n59.4\n48.1\n54.8\nAIME’25\n24.7\n47.7\n36.7\n35.1\nCoding\nLiveCodeBench\n36.1\n45.4\n40.5\n38.8\nHumanEval+\n89.6\n89.6\n92.1\n92.1\nMBPP+\n77.0\n77.8\n79.9\n81.0\nFullStackBench\n57.1\n64.7\n68.5\n67.1\nInstruction Following\nIFEval\n83.2\n82.3\n87.4\n88.0\nMulti-IF\n70.2\n66.9\n70.8\n76.6\nSysbench\n51.3\n71.3\n74.1\n69.8\nTable 5: Performance comparison among post-trained language model ERNIE-4.5-300B-A47B and other\nbaselines on text benchmarks. Bold values indicate the best scores.\n• Instruction Following: IFEval (Zhou et al., 2023), Multi-IF (He et al., 2024c), Sysbench (Qin et al.,\n2024) , evaluating complex instruction following abilities.\nERNIE-4.5-300B-A47B.\nTable 5 reveals the comparative performance of ERNIE-4.5-300B-A47B against\nother models across various benchmarks. Notably, it demonstrates significant strengths in instruction\nfollowing and knowledge tasks, as evidenced by the state-of-the-art scores on benchmarks such as\nIFEval, Multi-IF and ChineseSimpleQA. The model’s strong capabilities in instruction following and\nknowledge utilization in single-turn, multi-turn, and multilingual scenarios may be attributed to our\nunified rewarding system, which incorporates carefully designed reward mechanisms to guide the model\nin better interpreting and adhering to diverse user instructions and internal knowledge.\nIn mathematics and coding tasks, ERNIE-4.5-300B-A47B significantly outperforms Qwen3-235B-A22B\non most benchmarks, and it also achieves comparable results with GPT-4.1 and DeepSeek-V3-0324. The\nresults demonstrate the model’s advanced proficiency in mathematical reasoning, accurate computation,\nand code generation, making it particularly well-suited for applications in mathematical computing and\nprogramming-related tasks. However, in some challenging math and coding tasks, there remains room\nfor further improvement.\nLightweight Language Models.\nThe benchmark results for lightweight language models are sum-\nmarized in Table 6. The experimental results indicate that the lightweight model ERNIE-4.5-21B-A3B\nachieves competitive performance compared to Qwen3-30B-A3B, despite having approximately 30%\nfewer total parameters. Furthermore, the ultra-lightweight ERNIE-4.5-0.3B demonstrates reasonable\nperformance given its extremely compact size, enabling efficient inference even on standard laptops.\n8.2\nEvaluation of Multimodal Models\nIn this section, we present a comprehensive multimodal evaluation of ERNIE-4.5-VL against leading\nbaseline models, including Qwen2.5-VL (Bai et al., 2025) and OpenAI-o1 (OpenAI, 2025c). We conduct\na comprehensive evaluation across multimodal benchmarks designed to assess five fundamental ca-\npabilities: general visual knowledge comprehension, document and chart understanding, multimodal\n38\n\nERNIE 4.5 Technical Report\nJune 29, 2025\nCapability\nBenchmark\nERNIE-4.5-\n0.3B\nQwen3-\n30B-A3B\nERNIE-4.5-\n21B-A3B\nGeneral\nC-Eval\n32.9\n85.0\n85.4\nCMMLU\n34.5\n82.9\n84.8\nMMLU\n34.3\n77.2\n76.0\nMMLU-Pro\n18.9\n69.7\n66.0\nLiveBench\n16.7\n46.6\n51.3\nKnowledge\nSimpleQA\n0.7\n4.7\n24.2\nChineseSimpleQA\n3.5\n45.6\n48.9\nReasoning\nBBH\n30.0\n85.8\n83.0\nMUSR\n42.6\n58.4\n67.7\nDROP\n36.5\n88.2\n84.4\nZebralogic\n2.5\n29.2\n34.1\nMath\nMATH-500\n19.4\n87.4\n87.2\nGSM8K\n35.1\n94.8\n93.3\nCMath\n49.0\n94.8\n94.8\nOlympicArena\n8.8\n54.9\n58.8\nAIME’24\n0.4\n32.9\n27.9\nAIME’25\n0.2\n23.2\n19.7\nCoding\nLiveCodeBench\n2.6\n31.7\n26.4\nHumanEval+\n36.6\n90.2\n89.6\nMBPP+\n40.2\n75.4\n77.5\nFullStackBench\n11.5\n51.8\n48.2\nInstruction Following\nIFEval\n43.8\n82.3\n80.0\nMulti-IF\n23.0\n67.0\n61.0\nSysbench\n2.9\n35.5\n48.0\nTable 6: Performance comparison between ERNIE-4.5-21B-A3B and Qwen3-30B-A3B on text benchmarks,\nwith ERNIE-4.5-0.3B provided for reference. Bold values indicate the best scores.\nreasoning, visual perception, and video understanding. All models are evaluated using a standard\nzero-shot protocol across all multimodal tasks. The evaluation benchmarks are detailed below:\n• Visual Knowledge: CCBench (Liu et al., 2024b), SimpleVQA (Cheng et al., 2025), and MMBench-\nv1.1 (Liu et al., 2024b), assessing the general grasp of visual knowledge.\n• Document & Chart Understanding: OCRBench (Liu et al., 2024c), TableVQA (Kim et al., 2024),\nChartQA (Masry et al., 2022), AI2D (Kembhavi et al., 2016), and DocVQA (validation set) (Mathew\net al., 2021), evaluating text recognition, document and chart interpretation on structured infor-\nmation.\n• Multimodal Reasoning: VisualPuzzle (Song et al., 2025), Zerobench (Roberts et al., 2025),\nMMMU (Yue et al., 2024a), MMMU-Pro (Yue et al., 2024b), and MathVista (Lu et al., 2023),\ntesting logical reasoning and visual-related problem-solving capabilities through complex visual\nscenarios.\n• Visual Perception: CV-Bench (Tong et al., 2024), CountBench (Paiss et al., 2023), and Real-\nWorldQA (xAI, 2024), evaluating spatial reasoning, counting, and other basic visual perception\ncapabilities.\n• Video Understanding: MVBench (Li et al., 2024c), VideoMME with/without subtitles (Fu\net al., 2024), and LongVideoBench (Wu et al., 2024), evaluating temporal understanding, video\ncomprehension, and long-form video analysis capabilities.\nAll image benchmark results are re-evaluated using our own evaluation infrastructure. The results for\nQwen2.5-VL are based on evaluations using the open-source model weights. The results for OpenAI-o1\nwere collected using its official API in June 2025. For OCRBench, RealWorldQA and DocVQA (Mathew\net al., 2021), we use only the evaluation metrics provided by their official implementations. For all\nother benchmarks, we follow the official implementation guidelines and additionally incorporate an\nLLM-as-judge pipeline to ensure the accuracy and reliability of our evaluations. In particular, we use\nGPT-4.1 (OpenAI, 2025a) and a distilled smaller model that inherits its judging capability as the evaluator.\nFor the CCBench (Liu et al., 2024b) benchmark, we employ a “Caption Then Answer” prompting strategy\n39\n\nERNIE 4.5 Technical Report\nJune 29, 2025\nCapability\nBenchmark\nQwen2.5-\nQwen2.5-\nERNIE-4.5-VL-\nQwen2.5-\nERNIE-4.5-VL-\nVL-7B\nVL-32B\n28B-A3B\nVL-72B\n424B-A47B\nVisual\nKnowledge\nCCBench\n70.4\n73.0\n81.0\n76.5\n84.4\nSimpleVQA\n51.7\n53.6\n46.0\n60.1\n58.3\nMMBench-cn\n85.7\n89.7\n85.8\n91.0\n89.8\nMMBench-en\n87.0\n89.9\n86.9\n91.1\n90.8\nDocument\nand Chart\nOCRBench\n868\n815\n885\n872\n883\nTableVQA\n70.6\n69.7\n70.0\n78.1\n79.6\nChartQA\n80.3\n82.9\n82.2\n84.0\n86.4\nAI2D\n93.3\n93.6\n95.4\n95.4\n96.0\nDocVQA\n93.3\n92.2\n94.1\n92.4\n94.3\nMultimodal\nReasoning\nVisualPuzzle\n31.8\n41.1\n34.6\n45.0\n41.0\nZeroBench(sub)\n14.1\n14.4\n16.2\n16.2\n20.1\nVisulogic\n26.1\n26.9\n27.3\n25.6\n28.7\nMMMU\n51.8\n64.4\n58.9\n67.1\n67.3\nMMMU-Pro\n38.8\n50.4\n53.0\n52.8\n52.6\nMathVista\n68.5\n75.5\n74.9\n77.6\n78.8\nVisual\nPerception\nCV-Bench\n79.7\n82.1\n76.1\n82.6\n85.5\nCountBench\n86.8\n83.1\n87.6\n93.1\n93.3\nRealWorldQA\n70.6\n69.7\n69.2\n72.6\n75.2\nVideo\nUnderstanding\nMVBench\n69.6\n-\n72.0\n70.4\n70.6\nVideoMME w/subs\n71.6\n77.9\n74.4\n79.1\n79.7\nVideoMME wo/subs\n65.1\n70.5\n66.8\n73.3\n70.0\nLongVideoBench\n56.0\n-\n62.1\n60.7\n66.2\nTable 7: Performance comparison of VLMs in the non-thinking mode on multimodal benchmarks. Bold\nvalues indicate the best scores.\nCapability\nBenchmark\nOpenAI-o1\nERNIE-4.5-VL-\nERNIE-4.5-VL-\n28B-A3B\n424B-A47B\nVisual\nKnowledge\nCCBench\n79.7\n81.5\n83.9\nSimpleVQA\n64.1\n50.8\n59.8\nMMBench-cn\n86.8\n87.5\n90.9\nMMBench-en\n85.9\n88.8\n92.0\nDocument\nand Chart\nOCRBench\n761\n805\n872\nTableVQA\n84.5\n60.2\n86.7\nChartQA\n80.4\n79.2\n86.8\nAI2D\n94.2\n93.5\n95.3\nDocVQA\n81.0\n89.1\n93.1\nMultimodal\nReasoning\nVisualPuzzle\n48.5\n36.9\n46.3\nZeroBench(sub)\n20.2\n15.3\n22.5\nVisulogic\n29.0\n26.0\n27.3\nMMMU\n77.7\n61.8\n70.0\nMMMU-Pro\n65.8\n50.8\n58.8\nMathVista\n71.4\n72.6\n78.9\nVisual\nPerception\nCV-Bench\n81.3\n76.7\n84.8\nCountBench\n87.2\n85.3\n89.0\nRealWorldQA\n72.5\n71.8\n73.2\nTable 8: Performance comparison of VLMs in the thinking mode on multimodal benchmarks3. Bold\nvalues indicate the best scores.\nto improve model performance. For AI2D, we adopt the transparent setting and carefully align our\nimplementation with Molmo (Deitke et al., 2024).\n3QVQ-72B-Preview was excluded from this comparison because it is a preview version, not optimized well on the\nvisual understanding benchmarks we evaluated.\n40\n\nERNIE 4.5 Technical Report\nJune 29, 2025\nFor the video benchmarks, the results for Qwen2.5-VL are taken from its original paper or Hugging\nFace post. To ensure a fair comparison, we adopt a sequence length of 32,768, similar to Qwen2.5-VL.\nThe hyperparameters for dynamic frame-resolution sampling are set as follows: a frame rate of 2 FPS, a\nmaximum of 480 frames, a minimum of 16 frames, and a minimum resolution of 360p (or the original size\nif it is less than 360p). The actual frame resolution and the number of frames are determined dynamically\naccording to this method, ensuring that the combined token count of the video and text input fits within\nthe sequence length constraint. The answers are automatically graded by letter matching.\nWe report the results for ERNIE-4.5-VL in both non-thinking and thinking modes in Table 7 and Table 8.\nThe detailed performance across different benchmarks is discussed below.\nNon-Thinking Mode: Robust Visual Perception and Knowledge Factuality.\nERNIE-4.5-VL in non-\nthinking mode demonstrates robust capabilities for perceiving image details and recalling relevant\nknowledge. First, it exhibits notable perceptual capabilities when handling various types of image and\nvideo input. For example, ERNIE-4.5-VL-424B-A47B attains high scores on perception benchmarks\nsuch as CountBench, CV-Bench, and RealWorldQA. It also shows robust results on document and\nchart understanding tasks, with examples highlighted in Appendices B.1, B.2 and B.3. In addition, it\ndisplays solid capabilities on video comprehension benchmarks spanning both short and long videos.\nAppendix B.4 presents an example illustrating its temporal grounding ability, which benefits from the\ntimestamp rendering method.\nBeyond perceptual ability, ERNIE-4.5-VL also demonstrates a deep understanding of Chinese visual\nknowledge, as evidenced by its sound performance on CCBench. This capability can be attributed to the\nextensive pre-training of the vision encoder on visual concepts, further improved by multimodal joint\ntraining and the incorporation of high-quality Chinese textual and visual data. Appendix B.5 provides an\nillustrative case of the recognition of ancient Chinese characters.\nThinking Mode: Enhanced Multimodal Reasoning Capabilities.\nCompared to its non-thinking mode,\nERNIE-4.5-VL-424B-A47B in the thinking mode exhibits a clear advantage in reasoning-focused tasks,\nnarrowing the performance gap, and in some cases surpassing OpenAI-o1 on challenging benchmarks\nsuch as MathVista, MMMU, and VisualPuzzle. This performance results from its improved understanding\nof STEM-related images acquired during the SFT and RL stages. Appendices B.6 and B.7 provide\nreasoning examples in thinking mode.\nWhile the thinking mode excels in reasoning tasks, the joint training of both thinking and non-thinking\nmodes benefits the model as a whole. The thinking mode retains reliable perceptual performance, while\nthe non-thinking mode achieves notable gains in problem-solving and multimodal reasoning. Together,\nthese improvements ensure that the model performs competitively across a wide range of benchmarks in\nboth modes.\nLightweight VLM.\nThe experimental results show that the lightweight vision–language model ERNIE-\n4.5-28B-A3B achieves competitive, or even superior, across most benchmarks compared with Qwen2.5-VL-\n7B and Qwen2.5-VL-32B, despite having significantly fewer activation parameters. Notably, it supports\nboth thinking and non-thinking modes, offering functionalities consistent with ERNIE-4.5-VL-424B-A47B.\n9\nConclusion\nIn this work, we present ERNIE 4.5, a family of large-scale foundation models. Our models employ a\nnovel heterogeneous MoE structure, which supports parameter sharing and modality-specific expert\nspecialization, allowing for more flexible and effective multimodal joint training and knowledge fusion.\nExtensive evaluations demonstrate that our models achieve state-of-the-art performance across multiple\ntext and multimodal benchmarks, especially in textual and visual knowledge factuality, instruction\nfollowing, visual understanding, and multimodal reasoning.\nThe development of ERNIE 4.5 has benefited greatly from the collective wisdom of the research com-\nmunity, drawing on best practices and recent advances in large-scale model training. To facilitate future\nresearch and practical deployment, we have released our technical innovations in distributed training\nand quantization techniques, and open-sourced our development toolkits ERNIEKit and FastDeploy. By\nsharing these insights and tools, we aim to make a meaningful contribution to encourage innovation in\nefficient large-model training and deployment. In the future, we look forward to collaborating with the\nresearch and development community to advance progress in this rapidly evolving field.\n41\n\nERNIE 4.5 Technical Report\nJune 29, 2025\nA\nAppendix\nA.1\nAblation Study for Router Orthogonalization Loss\nTo verify the efficiency of router orthogonalization loss, we conduct ablation experiments on a MoE\nmodel with 21B total parameters and 3B activated parameters. Both baseline and experimental models\nare trained on the the same amount of tokens using identical data, hyperparameters, and exponential\nmoving average (EMA) parameter accumulation. The coefficient for the orthogonalization loss is set to\n1 × 10−2. Table 9 presents the results of a few-shot evaluation on downstream benchmarks. Although\nthe training loss remains similar, MoE training with the router orthogonalization loss outperforms the\nbaseline, indicating that constraining the router weights promotes better out-of-distribution (OOD)\nperformance.\nModel\nMean\nMMLU\nCEval\nCMMLU\nMMCU\nHumanEval\nAGIEval\nBBH\nMBPP\nGSM8K\nMath\nBaseline\n50.2\n46.1\n55.4\n51.7\n66.8\n50.6\n47.1\n37.7\n65.6\n63.3\n21.5\n+ Router Orthogonal Loss\n53.1\n51.6\n63.5\n63.4\n68.4\n58.5\n47.3\n38.9\n66.1\n60.7\n21.3\nTable 9: Performance comparison between the baseline and the model trained with Router Orthogonal-\nization Loss across multiple benchmarks.\nA.2\nEMA in Terms of Update Deltas\nLet θt ∈Rd denote the model parameters at training step t and let δt = θt+1 −θt be the parameter update\ndelta applied at step t. The EMA model parameters, denoted by θEMA\nt\n, is defined recursively as:\nθEMA\nt\n= αθEMA\nt−1 + (1 −α)θt,\nwith θEMA\n0\n= θ0,\n(13)\nwhere α ∈(0, 1) is the EMA decay coefficient.\nUnfolding the recursion in Equation 13 yields:\nθEMA\n1\n= αθ0 + (1 −α)θ1\n(14)\n= θ0 + (1 −α)δ0,\n(15)\nθEMA\n2\n= αθEMA\n1\n+ (1 −α)θ2\n= θ0 + (1 −α2)δ0 + (1 −α)δ1,\n(16)\nθEMA\nn\n= θ0 +\nn−1\n∑\ni=0\n(1 −αn−i)δi.\n(17)\nA.3\nEffective Decay Window of EMA\nSince effective learning rate η(α)\ni\n= 1 −αn−i is monotonically decreasing with respect to i, we formally\ndefine the effective decay window size as:\nW := n −j,\n(18)\nwhere j ∈{0, . . . , n} is the smallest index such that:\nη(α)\nj\n< 1 −ϵ\nand\nη(α)\nj−1 ≥1 −ϵ.\n(19)\nAll updates δi with i > j satisfy η(α)\ni\n< 1 −ϵ and are therefore considered to lie within the effective decay\nwindow.\nIn practice, we may wish to select α to induce a desired effective decay window size, denoted by ˆW. We\nenforce the boundary condition at position j = n −ˆW to construct the following equation:\nη(ˆα)\nj\n= 1 −ˆα ˆW = 1 −ϵ.\n(20)\nSolving for ˆα yields:\nˆα = exp\n\u0012 1\nˆW log ϵ\n\u0013\n.\n(21)\n42\n\nERNIE 4.5 Technical Report\nJune 29, 2025\nB\nQualitative examples\nThis section presents a selection of qualitative examples that illustrate the capabilities of the ERNIE-\n4.5-VL model through responses generated from various prompts. Document understanding and OCR\ncapabilities are highlighted in Figures 18, 19, and 20. Extending beyond static images, Figure 21 presents\nan example of generating video subtitles and locating specific events based on user instructions. Figure 22\nhighlights the model’s ability to recognize ancient Chinese characters. Figures 23, 24 and 25 demonstrate\nthe reasoning abilities of the model in different disciplines. The model’s proficiency in solving visual\nreasoning problems is presented in Figures 26, 27, 28, and 29. Examples demonstrating the model’s\ncommon-sense reasoning are provided in Figure 30. Figure 31 illustrates the model’s ability in code\nsynthesis. Figure 32 showcases an instance of image-conditioned creative writing generated by our\nmodel.\n43\n\nERNIE 4.5 Technical Report\nJune 29, 2025\nB.1\nOCR Parsing and Document Understanding I\nPrompts: Using the Roman history depicted in the image, create a mind map about the development of\nRome. Requirements: in English, with conciseness and logical structure.\nModel Responses: <think>...</think> Based on the provided text, here is a concise and logically\nstructured mind map outlining the development of Ancient Rome:\nCentral Theme: Development of Ancient Rome\n1. Geography & Foundation:\n• Located in Italy (south of Europe, surrounded by seas).\n• Occupies Apennine Peninsula with a fairly regular coastline.\n• Rich volcanic soil, good weather, and rainwater benefited agriculture.\n• Early inhabitants: Potentially Ligurians; later influxes: Europeans (Apennines 1000 BC), Illyri-\nans/Etruscans (1200 BC West/North), Greeks ( 800 BC South/Sicily), Gauls (5th Century BC\nNorth).\n• Became a multi-national land due to invasions, colonization, and migration.\n2. Historical Timeline:\n• Early Settlements (c. 1000 BC): Originated from small villages in central Italy near the Tiber\nRiver.\n• Etruscan Influence: Ruled by seven kings (last three possibly Etruscan), including Tarquinius\nSuperbus.\n• Roman Republic (c. 509 BC): Founded after expelling the last king. Governed by consuls\n(archons) and a senate.\n• Expansion: Grew from a city-state obscured by provinces to dominate the Mediterranean.\n• Transition to Empire (End of Republic): Augustus/Octavian founded dictatorship, replacing\nthe Republic (over 500 years after its founding).\n3. Social Conflicts (Aristocracy vs. Common People):\n• Stage 1 (Political Rights): Common people united in withdrawal, refusing to fight. Forced\naristocrats to yield, gaining rights (e.g., appointment as consuls).\n• Stage 2 (Debt & Land): Common people in debt allowed to repay with property, abolishing\ndebt slavery. Result: Relieved class distinction, expanded social foundations, secured common\npeople’s interest/power.\n4. Major Conflict: Punic Wars with Carthage:\n• Cause: Demonstrated Roman ambition; conflict lasted 100 years, marked by violence and\nruthlessness.\n• Carthage: Founded by Phoenician migrants in North Africa; coexisted peacefully for 3 centuries\nbefore conflict.\n• First Punic War: Roman forces crossed Straits of Messina. No decisive victory; Carthage\nsuffered heavy losses in the Second Punic War.\n• Final Battle (146 BC): Rome razed Carthage, enslaved inhabitants, ploughed the site. Symbol-\nized Rome’s destructive end for Carthage.\nThis mind map captures the key geographical, chronological, social, and conflict-driven elements of Rome’s\ndevelopment as described in the text.\nFigure 18: This case demonstrates the document comprehension ability of ERNIE-4.5-VL.\n44\n\nERNIE 4.5 Technical Report\nJune 29, 2025\nB.2\nOCR Parsing and Document Understanding II\nPrompts: What is the purpose of this medication? How should I take it?\nModel Responses: <think>...</think> Based on the supplement facts label, here’s the information you need:\n1. Purpose: This supplement is formulated for joint health and support.\n• Key Ingredients & Their Roles:\n– Glucosamine Hydrochloride & Chondroitin Sulfate Sodium: These are the primary ingredients.\nThey are building blocks of cartilage and are commonly used to support joint comfort, flexibility, and\nmobility, often targeted at osteoarthritis (especially knee osteoarthritis).\n– Hyaluronic Acid: Helps lubricate joints and may support joint cushioning.\n– Calcium Fructoborate: A form of boron that may support bone health and joint function.\n• Overall Goal: The combination aims to support the structure and function of joints, potentially reducing\ndiscomfort and improving mobility.\n2. How to Take It (Directions):\n• Adults take two (2) tablets once a day with a meal OR one (1) tablet twice daily with meals.\n• Key Points:\n– Consistency: Take it regularly as directed.\n– With Food: Always take it with a meal to help with absorption and minimize potential stomach upset.\n– Dosage: Follow the label instructions (2 tabs/day total, either all at once or split into two doses). Do\nnot exceed the recommended dosage unless directed by a healthcare professional.\n– Not for Children: The label specifies ”Adults,” so it’s not intended for children.\nImportant Considerations:\n• Not FDA-Approved: Remember, this is a dietary supplement, not a drug. It is not intended to diagnose,\ntreat, cure, or prevent any disease. Its effectiveness can vary between individuals.\n• Consult Your Doctor:\n– Before Starting: Always talk to your doctor or a qualified healthcare provider before starting any new\nsupplement, especially if you have underlying health conditions (like diabetes, shellfish allergies - see\nbelow), are pregnant or breastfeeding, or are taking other medications.\n– Shellfish Allergy: This product contains shellfish (shrimp, crab, lobster, crayfish). Do not take it if you\nhave a shellfish allergy.\n– Interactions: Supplements can interact with medications (e.g., blood thinners like warfarin). Your\ndoctor can advise on potential interactions.\n– Underlying Conditions: If you have severe joint pain or a diagnosed condition, your doctor can\ndetermine the best treatment plan, which may or may not include this supplement.\n• Sodium: While low (<1% DV), if you are on a strict sodium-restricted diet, be aware of the sodium content\n(2g per serving).\n• Other Ingredients: The label lists other components like microcrystalline cellulose, coatings, and fillers. If\nyou have sensitivities to any of these, check the ”Other Ingredients” list carefully.\nIn summary: This supplement is designed to support joint health. Take 2 tablets daily, either all at once with a meal\nor split into two doses (1 tablet twice daily with meals). Crucially, consult your doctor before starting it, particularly\nif you have health conditions, take medications, or have a shellfish allergy.\nFigure 19: Demonstration of ERNIE-4.5-VL’s ability to accurately extract text from an image, understand\nthe user’s query, and retrieve the answer from the document. Source: Photo taken by an author from the\nBaidu ERNIE team.\n45\n\nERNIE 4.5 Technical Report\nJune 29, 2025\nB.3\nMultilingual OCR Parsing\nPrompts: Extract the medical test guidelines and create an Excel sheet (including the items and reference\nvalues with complete condition ranges and units).\nModel Responses: <think>...</think> Here is the extracted medical testing data structured into an\nExcel-style table:\nItem\nTest Result\nReference Range\nUnit\nLipids\nTotal Cholesterol\n3.8\n1.9–3.9\nmmol/L\nTriglyceride\n2.47\n—\nmmol/L\nHDL Cholesterol\n3.15\n—\nmmol/L\nLDL Cholesterol\n-0.48\n—\nmmol/L\nNon-HDL Cholesterol\n0.65\n—\nmmol/L\nTotal Cholesterol/HDL ratio\n1.2\n—\n(Ratio)\nElectrolytes\nSodium\n145\n147–156\nmmol/L\nPotassium\n3.9\n3.8–4.6\nmmol/L\nChloride\n107\n115–123\nmmol/L\nRenal Function\nUrea\n4.0\n5.4–10.7\nmmol/L\nCreatinine\n69\n70–160\nµmol/L\nUric Acid\n<0.01\n<0.07\nmmol/L\nCalcium\n2.14\n1.75–2.50\nmmol/L\nCorrected Calcium\n2.30\n1.75–2.50\nmmol/L\nPhosphate\n1.28\n1.29–2.26\nmmol/L\nLiver Function\nTotal Protein\n71\n56–80\ng/L\nAlbumin\n34\n22–35\ng/L\nGlobulin\n37\n28–48\ng/L\nAlbumin/Globulin ratio\n0.9\n1.0–2.5\n(Ratio)\nAlkaline Phosphatase\n9\n5–80\nU/L\nTotal Bilirubin\n27\n2–10\nµmol/L\nGGT\n<3\n1–10\nU/L\nAST\n56\n10–60\nU/L\nALT\n74\n5–80\nU/L\nNotes:\n• ”—” indicates no reference range provided in the report.\n• Units are preserved as listed (e.g., µmol/L, mmol/L, g/L).\n• Inequalities (e.g., ¡0.01) are retained from the original report.\n• Ratios (e.g., Total Cholesterol/HDL) lack units but are labeled as ”Ratio” for clarity.\nFigure 20: This example demonstrates the OCR parsing and instruction-following capabilities of ERNIE-\n4.5-VL.\n46\n\nERNIE 4.5 Technical Report\nJune 29, 2025\nB.4\nVideo Temporal Grounding\nInput video\nPrompts: Based on the video, could you help me create some engaging subtitles to put below it?\nModel Responses:\n• Exploring the vibrant streets of Chongqing at dusk, where tradition meets modernity.\n• A glimpse of the adorable pandas in their natural habitat, a true symbol of China.\n• The sleek monorail glides above the river, offering stunning views of the cityscape.\n• ”I am in Chongqing” - A bold declaration on a city building, embracing the local spirit.\n• The ancient art of shadow play comes to life, a captivating cultural experience.\n• Savoring the elegance of traditional Chinese tea sets, a moment of tranquility.\nPrompts: At which second does ”CHONGQING” appear in the video?\nModel Responses: At 5 seconds, the word ’CHONGQING’ appears in red on a large white screen.\nFigure 21: The first query tests the model’s ability to break down a video into its important events and\ndescribe them concisely, similar to creating detailed captions for different parts of the video. The second\nquery assesses the model’s capability to pinpoint and identify specific events within a video, such as\nlocating a particular scene. Source: Video taken by an author from the Baidu ERNIE team.\n47\n\nERNIE 4.5 Technical Report\nJune 29, 2025\nB.5\nOCR Ancient Chinese Character Recognition\nPrompts: 请问这张图片中的文字是什么？出自哪里？\nModel Responses: <think>...</think> 图片中的文字为篆书字体，内容是\n诸葛亮《前出师表》中的经典段落：\n「（先帝）三顾臣于草庐之中，咨臣以当世之事，由是感激，遂许先帝以驱\n驰。」\n解析：\n1. 文字识别：篆书逐字转写为简体字后，内容为：「（臣）三顾臣于草庐之\n中，咨臣以当世之事，由是感激，遂许先帝」（图片文字略有省略，但核心句\n意完整）。\n2. 出处：出自三国时期诸葛亮《前出师表》。此段描述刘备（先帝）三次到诸\n葛亮隐居的草庐拜访，询问天下大事，诸葛亮感其诚意，决心出山辅佐。\n3. 书法特点：篆书以线条圆转、结构对称为特征，此作体现了古朴典雅的篆书\n风格，符合汉代以后篆书用于铭刻、庄重文本的传统。\n背景意义：\n《出师表》是诸葛亮北伐前写给后主刘禅的奏章，既表达忠君报国之志，又蕴\n含对先帝刘备的深切追念。此段文字因体现君臣知遇之恩，成为千古名句。\nTranslated prompts: What is the text in this picture? Where did it come from?\nTranslated model responses: <think>...</think> The characters in the im-\nage are written in Seal Script, featuring a classic passage from Zhuge Liang’s\n”Former Memorial to the Emperor on Sending Troops”: ”The Late Emperor\nthrice honored me with his visit to my humble thatched cottage, consulting\nme on matters governing our tumultuous age. Moved by such magnanimous\ncourtesy and profound trust, I thus pledged my unwavering service to His\nMajesty”s cause.”\nAnalysis:\n1. Character Recognition: After transcribing the Seal Script characters into\nSimplified Chinese, the content reads:\n”The Late Emperor thrice honored me with his visit to my humble thatched cot-\ntage, consulting me on matters governing our tumultuous age. Moved by such\nmagnanimous courtesy and profound trust, I thus pledged my unwavering\nservice to His Majesty”s cause.” (Some wording is omitted in the image, but\nthe core meaning remains intact.)\n2. Source: This passage is from Zhuge Liang’s ”Former Memorial to the Em-\nperor on Sending Troops” (”Qian Chu Shi Biao”) of the Three Kingdoms period.\nIt describes how Liu Bei (the late emperor) visited Zhuge Liang three times\nin his humble abode to seek his counsel on state affairs. Touched by Liu Bei’s\nsincerity, Zhuge Liang resolved to leave seclusion and assist him.\n3. Calligraphic Features: Seal Script is characterized by its rounded strokes and\nsymmetrical structure. This work exemplifies the ancient and elegant style of\nSeal Script, in keeping with the tradition of using this script for inscriptions\nand formal texts after the Han Dynasty.\nBackground Significance:\n”Former Memorial to the Emperor on Sending Troops” is a memorial written by\nZhuge Liang to Emperor Liu Shan before his northern expedition. It not only\nexpresses Zhuge Liang’s loyalty and dedication to his country but also conveys\nhis deep respect and remembrance for the late Emperor Liu Bei. This passage,\nin particular, has become a timeless quote for its portrayal of the profound\nbond between sovereign and minister.\nFigure 22: This example highlights the model’s ability to recognize ancient Chinese characters. Our\nmodel can accurately recognize Small Seal Script and understand the historical allusions behind the\ncharacters. We omit thinking tokens for brevity.\n48\n\nERNIE 4.5 Technical Report\nJune 29, 2025\nB.6\nReasoning Cases: Chemistry\nPrompts: The diagram above represents the idealized titration curve for the reaction of sodium oxalate\n(Na2C2O4) with a strong acid like hydrochloric acid (HCl). E and F represent the pH at the endpoints. G and\nH will depend on the composition of the sample with the possibility that one may not be present. In addition\nto water, what are the predominant species in solution at E?\nOptions:\n(A) Na2C2O4 and HCl\n(B) Na+, Cl−, and HC2O−\n4\n(C) C2O2−\n4\nand H+\n(D) Na+, H+, and C2O2−\n4\nModel Responses:\n<think>...</think> At point E on the titration curve, the first equivalence\npoint is reached. Sodium oxalate (Na2C2O4) dissociates completely into 2Na+ and C2O2−\n4 . During titration\nwith HCl, H+ ions react with C2O2−\n4\nto form HC2O−\n4 . At the first equivalence point, all C2O2−\n4\nis converted\nto HC2O−\n4 . The solution also contains Na+ (from the original salt) and Cl−(from HCl).\nAnalysis of options:\n• (A) Incorrect. Na2C2O4 and HCl are not present at equivalence; the reaction is complete, and HCl is\nneutralized.\n• (B) Correct. Na+ (spectator ion), Cl−(from HCl), and HC2O−\n4 (product of the reaction) are the\npredominant species.\n• (C) Incorrect. C2O2−\n4\nis fully converted to HC2O−\n4 , and H+ is not a dominant species at this stage.\n• (D) Incorrect. While Na+ is present, H+ and C2O2−\n4\nare not predominant; HC2O−\n4 dominates\ninstead.\nAnswer: (B) (Na+, Cl−, and HC2O−\n4 )\nFigure 23: An example of ERNIE-4.5-VL solving a chemistry problem highlights the model’s logical\nreasoning ability. We omit thinking tokens for brevity.\n49\n\nERNIE 4.5 Technical Report\nJune 29, 2025\nB.7\nReasoning Cases: Math\nPrompts: In the figure above if the area of triangle CAF is equal to the area of rectangle CDEF, what is the\nlength of segment AD?\nChoices:\nA.\n7/2\nB.\n5\nC.\n7\nD.\n15/2\nE.\n15\nModel Responses:\n<think>...</think> The rectangle CDEF has CD = 7.\nThe area of rectangle\nCDEF is given by the product of its adjacent sides, CD and CF (where CF is the same as FC, a side of the\nrectangle). Let the length of CF be l. Thus, the area of rectangle CDEF is\n7 × l = 7l.\nTriangle CAF is a right triangle with the right angle at C. The legs are CA and CF. Since D is on CA and CD =\n7, and assuming A is beyond D so CA = CD + AD, let AD = d. Then CA = 7 + d. The area of triangle CAF is\n1\n2 × CA × CF = 1\n2 × (7 + d) × l.\nGiven that the area of triangle CAF equals the area of rectangle CDEF:\n1\n2(7 + d)l = 7l.\nAssuming l ̸= 0 (since CF is a side of the rectangle and must have positive length), divide both sides by l:\n1\n2(7 + d) = 7.\nMultiply both sides by 2:\n7 + d = 14.\nSolve for d:\nd = 7.\nThus, the length of segment AD is 7.\nThis result is independent of the length of CF, as l cancels out in the equation.\nAnswer: C\nFigure 24: This example demonstrates the ability to solve a mathematical analytical geometry problem,\nshowcasing logical reasoning and mathematical computation skills in ERNIE-4.5-VL. We omit thinking\ntokens for brevity.\n50\n\nERNIE 4.5 Technical Report\nJune 29, 2025\nB.8\nReasoning Cases: Deep Semantic Image Understanding\nPrompts: What is this figure trying to convey?\nModel Responses: <think>...</think> This image is a satirical, metaphorical illustration of machine\nlearning concepts, using grotesque and humorous visuals to convey key ideas:\n• Unsupervised Learning (large creature):\n– Represented as a chaotic, multi-eyed monster with tentacles.\n– Symbolizes the unstructured, exploratory nature of unsupervised learning—finding patterns in\nunlabeled data without predefined rules.\n– The creature’s complexity reflects how unsupervised methods (e.g., clustering, generative\nmodels) can be messy but powerful.\n• Supervised Fine-tuning (pink humanoid):\n– Depicted as a smaller, vulnerable figure being manipulated by the monster.\n– Represents the process of refining a pre-trained model (e.g., a large language model) using\nlabeled data for specific tasks.\n– The humanoid’s subservience highlights how fine-tuning builds upon unsupervised founda-\ntions but is more controlled.\n• RLHF (Reinforcement Learning from Human Feedback) (tennis ball):\n– Labeled as the “cherry on top” with a smiley face.\n– Symbolizes RLHF as a “reward” or refinement step where human feedback aligns model\noutputs with human preferences (e.g., making responses helpful/harmless).\n– The tennis ball (a treat) suggests RLHF adds a layer of polish or user-friendliness.\nOverall Message:\n• The image humorously critiques the machine learning pipeline:\n– Unsupervised learning is the chaotic “base” that generates raw capability.\n– Supervised fine-tuning refines this base for practical use.\n– RLHF is the final touch that makes outputs more aligned with human values.\n• The grotesque style satirizes the complexity and potential “monstrosity” of AI systems, while the\nsmiley face on the tennis ball adds irony—RLHF is portrayed as a superficial “fix” for deeper issues.\nThis metaphor emphasizes how modern AI combines unstructured exploration (unsupervised), task-specific\nrefinement (supervised), and human-centric adjustments (RLHF) to create functional systems.\nFigure 25: This showcases the advanced image understanding capabilities of the ERNIE-4.5-VL. We omit\nthinking tokens for brevity.\n51\n\nERNIE 4.5 Technical Report\nJune 29, 2025\nB.9\nVisual Reasoning: Visual Pattern Recognition\nPrompts: Choose the most appropriate option from the four given\nchoices to fill in the question mark, so that the figures follow a\npattern.\nModel Responses: <think>...</think> The figures can be cate-\ngorized based on their structural composition:\n• Category 1 (Figures 1, 3, 6): These figures consist of dis-\ntinct elements connected or overlaid (e.g., separate cir-\ncles linked by a line, a triangle with a superimposed circle,\nor a hexagon with intersecting internal lines).\n• Category 2 (Figures 2, 4, 5): These figures feature inte-\ngrated structures (e.g., a triangle subdivided by lines, a\ndiamond with attached smaller diamonds, or a triangle\npartitioned into smaller triangles).\nAnswer: D\nPrompts: Choose the most appropriate option from the four given\nchoices to fill in the question mark, so that the figures follow a\npattern.\nModel Responses: The sequence of figures follows a pattern\nbased on symmetry, complexity, and the introduction of new\nelements. Here’s the analysis:\n• Symmetry Progression: The figures exhibit varying sym-\nmetries—vertical (triangle), rotational (circle with spiral),\nvertical/horizontal (cross), diagonal (overlapping paral-\nlelograms), and radial (flower). The next figure should\nalign with this progression. Option C (star in a parallelo-\ngram) introduces diagonal symmetry, complementing the\nearlier parallelograms, while D (circle with a dot) main-\ntains radial symmetry. However, the sequence does not\nstrictly repeat symmetry types, favoring structural evolu-\ntion instead.\n• Complexity and Layering: The figures grow more com-\nplex, from simple shapes (triangle, circle) to combined\nforms (cross with circles, overlapping parallelograms)\nand organic forms (flower). Option C adds a star (a new\nelement) within a parallelogram, reflecting this layering\ntrend.\n• Element Introduction: Each figure introduces a new\ncomponent (e.g., spiral, cross arms, overlapping shapes,\npetals). Option C continues this by combining a star (new)\nwith a parallelogram (previously seen).\nAnswer: C\nFigure 26: Two examples of ERNIE-4.5-VL using its visual inductive reasoning ability to correctly identify\npattern and predict the missing elements.\n52\n\nERNIE 4.5 Technical Report\nJune 29, 2025\nB.10\nVisual Reasoning: Emoji Quiz\nPrompts: 你能根据图中的emoji猜出著名电影吗？\nModel Responses: <think>...</think> 根据图中的emoji，对应的著名电影\n是《飞屋环游记》（Up）。以下是解析过程：\n1. 飞机：象征“飞”，对应电影中的飞行元素。\n2. 房子：代表“屋”，即电影中飞行的房子。\n3. 红色圆圈：象征“环”（如“环游”中的“环”）。\n4. 游泳的人：可能隐含“游”（环游的“游”）。\n5. 公鸡：谐音“记”（鸡→记），呼应电影名的结尾字。\n组合后即为《飞屋环游记》（英文原名Up），完美契合emoji的隐喻与谐音逻\n辑。\nTranslated prompts: Can you guess the famous movie based on the emojis in\nthe picture?\nTranslated model responses: <think>...</think> The famous movie corre-\nsponding to the emojis in the picture is ’Up’. Here is the analysis process:\n1. Airplane: symbolizes ’fly’, corresponding to the flying element in the movie.\n2. House: represents ’house’, referring to the flying house in the movie.\n3. Red Circle: symbolizes ’circle’ (as in the ’circle’ in ’circumnavigate’).\n4. Swimming Person: might imply ’travel’ (as in the ’travel’ in ’circumnavi-\ngate’).\n5. Rooster: phonetically similar to ’remember’ (chicken →remember), echoing\nthe last character of the movie title.\nWhen combined, it forms “Up” (the original English title), perfectly fitting the\nmetaphor and phonetic logic of the emoji.\nPrompts: 图中是一个什么梗？\nModel Responses: <think>...</think> 图中是一个结合视觉与谐音的梗。上\n半部分正常标注“这是马”，而下半部分将马周围布满数字“2”，利用以下逻辑\n制造笑点：\n1. 视觉混淆：数字“2”密集排列形成类似“马赛克”或“二维码”的视觉效果。\n2. 谐音梗：中文“马”与“码”同音，数字“2”在中文网络语境中常代指“二”。\n当“马”被大量“2”包围时，可解读为“码（马）的二维码”（即“二+马”的重复组\n合）。\n3.\n最终答案：填空处应为“二维码”，既呼应数字排列的视觉效果，又通\n过“码=马”的谐音完成双关。\n该梗通过简单的元素重组与语言游戏，制造出“意料之外又情理之中”的幽默效\n果。\nTranslated prompts: What kind of joke is it in the picture?\nTranslated model responses: <think>...</think> The image is a joke that\ncombines visual and phonetic elements. The upper part is normally annotated\nas ”this is a horse”, while the lower part fills the area around the horse with\nthe number ”2”, creating humor through the following logic:\n1. Visual confusion: The densely arranged number ”2” forms a visual effect\nsimilar to a ”mosaic” or a ”QR code”.\n2. Phonetic joke: In Chinese, ”horse” sounds the same as ”code”, and the\nnumber ”2” often represents ”two” in the Chinese internet context. When the\n”horse” is surrounded by a large number of ”2”, it can be interpreted as ”the\nQR code of the horse” (i.e., a repeated combination of ”two” and ”horse”).\n3. Final answer: The blank should be filled with ”QR code”, which not only\nechoes the visual effect of the number arrangement, but also completes the pun\nthrough the homophonic ”code=horse”.\nThis joke, through simple element recombination and language play, creates a\nhumorous effect that is ’unexpected yet makes perfect sense’.\nFigure 27: These two examples successfully demonstrate how ERNIE-4.5-VL solves problems based on\nthe given emoji inputs. These cases highlight the model’s ability to recognize emoji icons, understand\nmovie plot content, and analyze images of internet memes.\n53\n\nERNIE 4.5 Technical Report\nJune 29, 2025\nB.11\nVisual Reasoning: Depth Sorting\nPrompts: 请列出图中除了小熊以外的所有物品，并且以与小熊的距离为依据\n来升序排序。\nModel Responses: <think>...</think> 根据与小熊的距离由近到远排序如\n下：\n1. 黄色小鸭玩具（LINLEE）：紧邻小熊右下方，距离最近。\n2. 海苔圈零食（蓝色包装）：位于小熊正前方偏左，稍远于小鸭。\n3. 维达抽纸（蓝色包装）：在零食左侧，距离小熊略远于零食本身。\n4. 雀巢茶萃（绿色饮料盒）与太魔性酸梅汤（紫色饮料盒）：叠放于小熊左后\n方，为最远物品（两者距离相近，并列最后）。\n理由：小鸭与小熊直接接触，最近；零食和纸巾位于前方但分散左右；饮料盒\n因处于后方且叠放，距离最远。\nTranslated prompts: Please list all the items in the picture except the little bear\nand sort them in ascending order based on their distance from the little bear.\nTranslated model responses: <think>...</think> The order from the closest\nto the farthest distance from the little bear is as follows: 1. Yellow duck toy\n(LINLEE)：It is right next to the lower right of the little bear, the closest in\ndistance.\n2. Seaweed ring snack (blue packaging): It is located just in front and slightly\nto the left of the little bear, a bit farther than the duck.\n3. Vinda tissue paper (blue packaging): It is on the left side of the snack, slightly\nfarther from the bear than the snack itself.\n4. Nestle tea extract (green beverage box) and Taimo Plum Soup (purple\nbeverage box): They are stacked at the back left of the bear and are the farthest\nitems (both are at similar distances, ranked last).\nReason: The duck is in direct contact with the bear and is the closest; the snack\nand tissue are in front but scattered left and right; the beverage boxes are the\nfarthest because they are at the back and stacked.\nFigure 28: This illustration demonstrates the model’s capability to deduce the spatial arrangement of\nobjects in terms of depth from a single photograph. Source: Photo taken by an author from the Baidu\nERNIE team.\nB.12\nVisual Reasoning: Counting\nPrompts: Can you tell me how many different colors of pens are in the picture?\nModel Responses:\n<think>...</think> There are 5 different colors\nof pens in the picture:\n1. Red (two pens, but same color)\n2. Blue\n3. Green\n4. Pink\n5. Brown\nThese are all visible in the black mesh pen holder.\nFigure 29: An example of ERNIE-4.5-VL recognizing queried objects and counting their occurrences.\nSource: Photo taken by an author from the Baidu ERNIE team.\n54\n\nERNIE 4.5 Technical Report\nJune 29, 2025\nB.13\nCommon Sense Reasoning\nPrompts: Where were the cultural relics in the picture unearthed?\nModel Responses: The cultural relic shown in the picture is a bronze\nmask unearthed from Sanxingdui. The Sanxingdui site is located in\nGuanghan City, Sichuan Province, and is one of the important archaeo-\nlogical discoveries in China in the 20th century. It provides important\nphysical materials for studying the ancient history, culture, and society\nof the Shu (Sichuan) region.\nPrompts: What is his occupation?\nModel Responses: The character in the image is from the anime ”De-\ntective Conan” (Case Closed). He is Kogoro Mori, who is a private\ndetective. Despite his sometimes comical and lazy demeanor, he runs\na detective agency and often finds himself involved in solving cases,\nsometimes with the help of Conan Edogawa.\nPrompts: What’s in the picture?\nModel Responses: The picture shows a small, yellow LEGO or building\nblock figure resembling Pikachu, a popular Pok´emon character. The\nfigure has black-tipped ears, black eyes, a black nose, and red cheeks. It\nis positioned on a white surface next to a white electronic device with\nblack buttons and a black cable.\nFigure 30: Three illustrative instances of ERNIE-4.5-VL successfully identify famous cultural relics and\ncelebrities based on their corresponding inputs. These examples highlight the model’s common-sense\nabilities.\n55\n\nERNIE 4.5 Technical Report\nJune 29, 2025\nB.14\nCode Synthesis\nPrompts: In python, using the process as shown in the figure, how many steps are needed to calculate\n10,000,000 before it becomes 1?\nModel Responses:\nTo determine how many steps are needed to reduce 10,000,000 to 1 using the\nprocess described in the flowchart (Collatz conjecture), we can write a Python function to simulate the\nprocess. Here’s how you can implement it:\ndef\ncollatz_steps(n):\nsteps = 0\nwhile n != 1:\nif n %\nn = n // 2\nelse:\nn = 3 * n + 1\nsteps += 1\nreturn\nsteps\n# Calculate\nthe number of steps for 10 ,000 ,000\nn = 10000000\nsteps_needed = collatz_steps (n)\nprint(f\"Number of steps to reduce {n} to 1: {steps_needed }\")\nRunning this code will give you the number of steps required to reduce 10,000,000 to 1. The Collatz conjecture\nis the basis for this process, and while it’s proven for many numbers, it hasn’t been proven for all integers.\nHowever, for 10,000,000, the process will eventually reach 1.\nWhen you run the code, it will output the number of steps needed.\nThe exact number might be\nlarge, as the process can take many iterations for large numbers.\nFigure 31: This example validates the capability of ERNIE-4.5-VL to convert flowcharts into Python code\nbased on user commands, showcasing its strengths in both adherence to instructions and code generation.\n56\n\nERNIE 4.5 Technical Report\nJune 29, 2025\nB.15\nImage Conditioned Creative Writing\nPrompts: Besides the ingredients in the fridge, there are also flour, rice and other materials at home. Please\nhelp me design a lunch for two people based on the ingredients. It requires three dishes and one soup. Please\nlist it in the form of a table.\nModel Responses: I see a well-stocked fridge with various ingredients.\nLet me design a balanced\nlunch menu for two people with three dishes and one soup, utilizing what’s available. Here’s a table with the\nmenu:\nCategory\nDish Name\nMain Ingredients Used\nDescription\nMain Dish 1\nGrilled Mackerel\nMackerel fillets\nSimple\nseasoned\ngrilled\nmackerel - a healthy protein\nsource\nMain Dish 2\nStir-fried\nChicken\nwith Vegetables\nChicken\nslices,\ncabbage,\ngreen peppers, other avail-\nable veggies\nQuick stir-fry with soy sauce\nseasoning\nSide Dish\nSteamed Egg Custard\nEggs, with optional dashi or\nsoy milk\nSilky smooth egg custard,\ncan add some green onions\non top\nSoup\nMiso Soup with Tofu\nand Wakame\nTofu, miso paste, wakame\nseaweed (if available)\nClassic Japanese soup to com-\nplete the meal\nFor the stir-fry, I’d use the chicken slices along with whatever vegetables look freshest - probably cabbage,\ngreen peppers, and maybe some broccoli if available. The egg custard is simple but comforting, and miso\nsoup ties everything together nicely.\nThis menu provides a good balance of protein (fish, chicken, tofu, eggs), vegetables, and carbohy-\ndrates (from the rice that’s mentioned as being available at home). The flavors are complementary and it’s all\nquite easy to prepare.\nFigure 32: The above example illustrates image-conditional creative text generation. The model processes\nan input image and generates a menu that is semantically related to the visual content.\n57\n\nERNIE 4.5 Technical Report\nJune 29, 2025\nReferences\nAmro Abbas, Kushal Tirumala, Daniel Simig, Surya Ganguli, and Ari S. Morcos. Semdedup: Data-\nefficient learning at web-scale through semantic deduplication. CoRR, abs/2303.09540, 2023. doi:\n10.48550/ARXIV.2303.09540. URL https://doi.org/10.48550/arXiv.2303.09540.\nAIME. Aime problems and solutions, 2025. URL https://artofproblemsolving.com/wiki/index.php\n/AIME Problems and Solutions.\nJean-Baptiste Alayrac, Jeff Donahue, Pauline Luc, Antoine Miech, Iain Barr, Yana Hasson, Karel Lenc,\nArthur Mensch, Katherine Millican, Malcolm Reynolds, Roman Ring, Eliza Rutherford, Serkan Cabi,\nTengda Han, Zhitao Gong, Sina Samangooei, Marianne Monteiro, Jacob L. Menick, Sebastian Borgeaud,\nAndy Brock, Aida Nematzadeh, Sahand Sharifzadeh, Mikolaj Binkowski, Ricardo Barreira, Oriol\nVinyals, Andrew Zisserman, and Kar´en Simonyan. Flamingo: a visual language model for few-\nshot learning. In Sanmi Koyejo, S. Mohamed, A. Agarwal, Danielle Belgrave, K. Cho, and A. Oh\n(eds.), Advances in Neural Information Processing Systems 35: Annual Conference on Neural Information\nProcessing Systems 2022, NeurIPS 2022, New Orleans, LA, USA, November 28 - December 9, 2022, 2022.\nURL http://papers.nips.cc/paper files/paper/2022/hash/960a172bc7fbf0177ccccbb411a7d\n800-Abstract-Conference.html.\nAida Amini, Saadia Gabriel, Shanchuan Lin, Rik Koncel-Kedziorski, Yejin Choi, and Hannaneh Hajishirzi.\nMathqa: Towards interpretable math word problem solving with operation-based formalisms. In\nJill Burstein, Christy Doran, and Thamar Solorio (eds.), Proceedings of the 2019 Conference of the North\nAmerican Chapter of the Association for Computational Linguistics: Human Language Technologies, NAACL-\nHLT 2019, Minneapolis, MN, USA, June 2-7, 2019, Volume 1 (Long and Short Papers), pp. 2357–2367.\nAssociation for Computational Linguistics, 2019. doi: 10.18653/V1/N19-1245. URL https://doi.or\ng/10.18653/v1/n19-1245.\nAnthropic. Introducing claude 4, 2025. URL https://www.anthropic.com/news/claude-4.\nSaleh Ashkboos, Amirkeivan Mohtashami, Maximilian L. Croci, Bo Li, Pashmina Cameron, Martin Jaggi,\nDan Alistarh, Torsten Hoefler, and James Hensman. Quarot: Outlier-free 4-bit inference in rotated llms.\nIn Amir Globersons, Lester Mackey, Danielle Belgrave, Angela Fan, Ulrich Paquet, Jakub M. Tomczak,\nand Cheng Zhang (eds.), Advances in Neural Information Processing Systems 38: Annual Conference on\nNeural Information Processing Systems 2024, NeurIPS 2024, Vancouver, BC, Canada, December 10 - 15, 2024,\n2024. URL http://papers.nips.cc/paper files/paper/2024/hash/b5b939436789f76f08b9d0da5\ne81af7c-Abstract-Conference.html.\nAnas Awadalla, Le Xue, Oscar Lo, Manli Shu, Hannah Lee, Etash Guha, Sheng Shen, Mohamed Awadalla,\nSilvio Savarese, Caiming Xiong, Ran Xu, Yejin Choi, and Ludwig Schmidt. MINT-1T: scaling open-\nsource multimodal data by 10x: A multimodal dataset with one trillion tokens. In Amir Globersons,\nLester Mackey, Danielle Belgrave, Angela Fan, Ulrich Paquet, Jakub M. Tomczak, and Cheng Zhang\n(eds.), Advances in Neural Information Processing Systems 38: Annual Conference on Neural Information\nProcessing Systems 2024, NeurIPS 2024, Vancouver, BC, Canada, December 10 - 15, 2024, 2024. URL\nhttp://papers.nips.cc/paper files/paper/2024/hash/40b9196c25fe1d64d87ca3a80a91d0ce-A\nbstract-Datasets and Benchmarks Track.html.\nShuai Bai, Keqin Chen, Xuejing Liu, Jialin Wang, Wenbin Ge, Sibo Song, Kai Dang, Peng Wang, Shijie\nWang, Jun Tang, Humen Zhong, Yuanzhi Zhu, Ming-Hsuan Yang, Zhaohai Li, Jianqiang Wan, Pengfei\nWang, Wei Ding, Zheren Fu, Yiheng Xu, Jiabo Ye, Xi Zhang, Tianbao Xie, Zesen Cheng, Hang Zhang,\nZhibo Yang, Haiyang Xu, and Junyang Lin. Qwen2.5-vl technical report. CoRR, abs/2502.13923, 2025.\ndoi: 10.48550/ARXIV.2502.13923. URL https://doi.org/10.48550/arXiv.2502.13923.\nJames Betker, Gabriel Goh, Li Jing, TimBrooks, Jianfeng Wang, Linjie Li, LongOuyang, JuntangZhuang,\nJoyceLee, YufeiGuo, WesamManassra, PrafullaDhariwal, CaseyChu, YunxinJiao, and Aditya Ramesh.\nImproving image generation with better captions. URL https://api.semanticscholar.org/Corpus\nID:264403242.\nYonatan Bisk, Rowan Zellers, Ronan Le Bras, Jianfeng Gao, and Yejin Choi. PIQA: reasoning about\nphysical commonsense in natural language. In The Thirty-Fourth AAAI Conference on Artificial Intelligence,\nAAAI 2020, The Thirty-Second Innovative Applications of Artificial Intelligence Conference, IAAI 2020, The\nTenth AAAI Symposium on Educational Advances in Artificial Intelligence, EAAI 2020, New York, NY,\nUSA, February 7-12, 2020, pp. 7432–7439. AAAI Press, 2020. doi: 10.1609/AAAI.V34I05.6239. URL\nhttps://doi.org/10.1609/aaai.v34i05.6239.\nFederico Cassano, John Gouwar, Daniel Nguyen, Sydney Nguyen, Luna Phipps-Costin, Donald Pinckney,\nMing-Ho Yee, Yangtian Zi, Carolyn Jane Anderson, Molly Q. Feldman, Arjun Guha, Michael Greenberg,\n58\n\nERNIE 4.5 Technical Report\nJune 29, 2025\nand Abhinav Jangda. Multipl-e: A scalable and polyglot approach to benchmarking neural code\ngeneration. IEEE Trans. Software Eng., 49(7):3675–3691, 2023. doi: 10.1109/TSE.2023.3267446. URL\nhttps://doi.org/10.1109/TSE.2023.3267446.\nYaoyao Chang, Lei Cui, Li Dong, Shaohan Huang, Yangyu Huang, Yupan Huang, Scarlett Li, Tengchao\nLv, Shuming Ma, Qinzheng Sun, Wenhui Wang, Furu Wei, Ying Xin, Mao Yang, Qiufeng Yin, and\nXingxing Zhang. Redstone: Curating general, code, math, and QA data for large language models.\nCoRR, abs/2412.03398, 2024. doi: 10.48550/ARXIV.2412.03398. URL https://doi.org/10.48550/arX\niv.2412.03398.\nChunyang Chen, Ting Su, Guozhu Meng, Zhenchang Xing, and Yang Liu. From UI design image to GUI\nskeleton: a neural machine translator to bootstrap mobile GUI implementation. In Michel Chaudron,\nIvica Crnkovic, Marsha Chechik, and Mark Harman (eds.), Proceedings of the 40th International Conference\non Software Engineering, ICSE 2018, Gothenburg, Sweden, May 27 - June 03, 2018, pp. 665–676. ACM, 2018.\ndoi: 10.1145/3180155.3180240. URL https://doi.org/10.1145/3180155.3180240.\nLin Chen, Jinsong Li, Xiaoyi Dong, Pan Zhang, Yuhang Zang, Zehui Chen, Haodong Duan, Jiaqi Wang,\nYu Qiao, Dahua Lin, and Feng Zhao. Are we on the right way for evaluating large vision-language\nmodels? In Amir Globersons, Lester Mackey, Danielle Belgrave, Angela Fan, Ulrich Paquet, Jakub M.\nTomczak, and Cheng Zhang (eds.), Advances in Neural Information Processing Systems 38: Annual\nConference on Neural Information Processing Systems 2024, NeurIPS 2024, Vancouver, BC, Canada, December\n10 - 15, 2024, 2024a. URL http://papers.nips.cc/paper files/paper/2024/hash/2f8ee6a3d766b\n426d2618e555b5aeb39-Abstract-Conference.html.\nZhe Chen, Weiyun Wang, Hao Tian, Shenglong Ye, Zhangwei Gao, Erfei Cui, Wenwen Tong, Kongzhi\nHu, Jiapeng Luo, Zheng Ma, Ji Ma, Jiaqi Wang, Xiaoyi Dong, Hang Yan, Hewei Guo, Conghui He,\nBotian Shi, Zhenjiang Jin, Chao Xu, Bin Wang, Xingjian Wei, Wei Li, Wenjian Zhang, Bo Zhang, Pinlong\nCai, Licheng Wen, Xiangchao Yan, Min Dou, Lewei Lu, Xizhou Zhu, Tong Lu, Dahua Lin, Yu Qiao,\nJifeng Dai, and Wenhai Wang. How far are we to gpt-4v? closing the gap to commercial multimodal\nmodels with open-source suites. Sci. China Inf. Sci., 67(12), 2024b. doi: 10.1007/S11432-024-4231-5.\nURL https://doi.org/10.1007/s11432-024-4231-5.\nXianfu Cheng, Wei Zhang, Shiwei Zhang, Jian Yang, Xiangyuan Guan, Xianjie Wu, Xiang Li, Ge Zhang,\nJiaheng Liu, Yuying Mai, Yutao Zeng, Zhoufutu Wen, Ke Jin, Baorui Wang, Weixiao Zhou, Yunhong\nLu, Tongliang Li, Wenhao Huang, and Zhoujun Li. Simplevqa: Multimodal factuality evaluation for\nmultimodal large language models. CoRR, abs/2502.13059, 2025. doi: 10.48550/ARXIV.2502.13059.\nURL https://doi.org/10.48550/arXiv.2502.13059.\nYao Cheng, Jianfeng Chen, Jie Chen, Li Chen, Liyu Chen, Wentao Chen, Zhengyu Chen, Shijie Geng,\nAoyan Li, Bo Li, Bowen Li, Linyi Li, Boyi Liu, Jerry Liu, Kaibo Liu, Qi Liu, Shukai Liu, Siyao Liu,\nTianyi Liu, Tingkai Liu, Yongfei Liu, Rui Long, Jing Mai, Guanghan Ning, Z. Y. Peng, Kai Shen,\nJiahao Su, Jing Su, Tao Sun, Yifan Sun, Yunzhe Tao, Guoyin Wang, Siwei Wang, Xuwu Wang, Yite\nWang, Zihan Wang, Jinxiang Xia, Liang Xiang, Xia Xiao, Yongsheng Xiao, Chenguang Xi, Shulin\nXin, Jingjing Xu, Shikun Xu, Hongxia Yang, Jack Yang, Yingxiang Yang, Jianbo Yuan, Jun Zhang,\nYufeng Zhang, Yuyu Zhang, Shen Zheng, He Zhu, and Ming Zhu. Fullstack bench: Evaluating\nllms as full stack coders. CoRR, abs/2412.00535, 2024. doi: 10.48550/ARXIV.2412.00535. URL\nhttps://doi.org/10.48550/arXiv.2412.00535.\nPeter Clark, Isaac Cowhey, Oren Etzioni, Tushar Khot, Ashish Sabharwal, Carissa Schoenick, and Oyvind\nTafjord. Think you have solved question answering? try arc, the AI2 reasoning challenge. CoRR,\nabs/1803.05457, 2018. URL http://arxiv.org/abs/1803.05457.\nKarl Cobbe, Vineet Kosaraju, Mohammad Bavarian, Mark Chen, Heewoo Jun, Lukasz Kaiser, Matthias\nPlappert, Jerry Tworek, Jacob Hilton, Reiichiro Nakano, Christopher Hesse, and John Schulman.\nTraining verifiers to solve math word problems. CoRR, abs/2110.14168, 2021. URL https://arxiv.or\ng/abs/2110.14168.\nGoogle DeepMind. Gemini 2.5, 2025. URL https://blog.google/technology/google-deepmind/gemi\nni-model-thinking-updates-march-2025/.\nDeepSeek-AI, Aixin Liu, Bei Feng, Bin Wang, Bingxuan Wang, Bo Liu, Chenggang Zhao, Chengqi Deng,\nChong Ruan, Damai Dai, Daya Guo, Dejian Yang, Deli Chen, Dongjie Ji, Erhang Li, Fangyun Lin, Fuli\nLuo, Guangbo Hao, Guanting Chen, Guowei Li, Hao Zhang, Hanwei Xu, Hao Yang, Haowei Zhang,\nHonghui Ding, Huajian Xin, Huazuo Gao, Hui Li, Hui Qu, J. L. Cai, Jian Liang, Jianzhong Guo, Jiaqi Ni,\nJiashi Li, Jin Chen, Jingyang Yuan, Junjie Qiu, Junxiao Song, Kai Dong, Kaige Gao, Kang Guan, Lean\nWang, Lecong Zhang, Lei Xu, Leyi Xia, Liang Zhao, Liyue Zhang, Meng Li, Miaojun Wang, Mingchuan\nZhang, Minghua Zhang, Minghui Tang, Mingming Li, Ning Tian, Panpan Huang, Peiyi Wang, Peng\n59\n\nERNIE 4.5 Technical Report\nJune 29, 2025\nZhang, Qihao Zhu, Qinyu Chen, Qiushi Du, R. J. Chen, R. L. Jin, Ruiqi Ge, Ruizhe Pan, Runxin Xu,\nRuyi Chen, S. S. Li, Shanghao Lu, Shangyan Zhou, Shanhuang Chen, Shaoqing Wu, Shengfeng Ye,\nShirong Ma, Shiyu Wang, Shuang Zhou, Shuiping Yu, Shunfeng Zhou, Size Zheng, Tao Wang, Tian Pei,\nTian Yuan, Tianyu Sun, W. L. Xiao, Wangding Zeng, Wei An, Wen Liu, Wenfeng Liang, Wenjun Gao,\nWentao Zhang, X. Q. Li, Xiangyue Jin, Xianzu Wang, Xiao Bi, Xiaodong Liu, Xiaohan Wang, Xiaojin\nShen, Xiaokang Chen, Xiaosha Chen, Xiaotao Nie, Xiaowen Sun, Zihan Wang, and et al. Deepseek-v2:\nA strong, economical, and efficient mixture-of-experts language model. CoRR, abs/2405.04434, 2024a.\ndoi: 10.48550/ARXIV.2405.04434. URL https://doi.org/10.48550/arXiv.2405.04434.\nDeepSeek-AI, Aixin Liu, Bei Feng, Bing Xue, Bingxuan Wang, Bochao Wu, Chengda Lu, Chenggang\nZhao, Chengqi Deng, Chenyu Zhang, Chong Ruan, Damai Dai, Daya Guo, Dejian Yang, Deli Chen,\nDongjie Ji, Erhang Li, Fangyun Lin, Fucong Dai, Fuli Luo, Guangbo Hao, Guanting Chen, Guowei\nLi, H. Zhang, Han Bao, Hanwei Xu, Haocheng Wang, Haowei Zhang, Honghui Ding, Huajian Xin,\nHuazuo Gao, Hui Li, Hui Qu, J. L. Cai, Jian Liang, Jianzhong Guo, Jiaqi Ni, Jiashi Li, Jiawei Wang,\nJin Chen, Jingchang Chen, Jingyang Yuan, Junjie Qiu, Junlong Li, Junxiao Song, Kai Dong, Kai Hu,\nKaige Gao, Kang Guan, Kexin Huang, Kuai Yu, Lean Wang, Lecong Zhang, Lei Xu, Leyi Xia, Liang\nZhao, Litong Wang, Liyue Zhang, Meng Li, Miaojun Wang, Mingchuan Zhang, Minghua Zhang,\nMinghui Tang, Mingming Li, Ning Tian, Panpan Huang, Peiyi Wang, Peng Zhang, Qiancheng Wang,\nQihao Zhu, Qinyu Chen, Qiushi Du, R. J. Chen, R. L. Jin, Ruiqi Ge, Ruisong Zhang, Ruizhe Pan, Runji\nWang, Runxin Xu, Ruoyu Zhang, Ruyi Chen, S. S. Li, Shanghao Lu, Shangyan Zhou, Shanhuang Chen,\nShaoqing Wu, Shengfeng Ye, Shengfeng Ye, Shirong Ma, Shiyu Wang, Shuang Zhou, Shuiping Yu,\nShunfeng Zhou, Shuting Pan, T. Wang, Tao Yun, Tian Pei, Tianyu Sun, W. L. Xiao, and Wangding Zeng.\nDeepseek-v3 technical report. CoRR, abs/2412.19437, 2024b. doi: 10.48550/ARXIV.2412.19437. URL\nhttps://doi.org/10.48550/arXiv.2412.19437.\nDeepSeek-AI, Daya Guo, Dejian Yang, Haowei Zhang, Junxiao Song, Ruoyu Zhang, Runxin Xu, Qihao\nZhu, Shirong Ma, Peiyi Wang, Xiao Bi, Xiaokang Zhang, Xingkai Yu, Yu Wu, Z. F. Wu, Zhibin Gou,\nZhihong Shao, Zhuoshu Li, Ziyi Gao, Aixin Liu, Bing Xue, Bingxuan Wang, Bochao Wu, Bei Feng,\nChengda Lu, Chenggang Zhao, Chengqi Deng, Chenyu Zhang, Chong Ruan, Damai Dai, Deli Chen,\nDongjie Ji, Erhang Li, Fangyun Lin, Fucong Dai, Fuli Luo, Guangbo Hao, Guanting Chen, Guowei Li,\nH. Zhang, Han Bao, Hanwei Xu, Haocheng Wang, Honghui Ding, Huajian Xin, Huazuo Gao, Hui Qu,\nHui Li, Jianzhong Guo, Jiashi Li, Jiawei Wang, Jingchang Chen, Jingyang Yuan, Junjie Qiu, Junlong Li,\nJ. L. Cai, Jiaqi Ni, Jian Liang, Jin Chen, Kai Dong, Kai Hu, Kaige Gao, Kang Guan, Kexin Huang, Kuai\nYu, Lean Wang, Lecong Zhang, Liang Zhao, Litong Wang, Liyue Zhang, Lei Xu, Leyi Xia, Mingchuan\nZhang, Minghua Zhang, Minghui Tang, Meng Li, Miaojun Wang, Mingming Li, Ning Tian, Panpan\nHuang, Peng Zhang, Qiancheng Wang, Qinyu Chen, Qiushi Du, Ruiqi Ge, Ruisong Zhang, Ruizhe\nPan, Runji Wang, R. J. Chen, R. L. Jin, Ruyi Chen, Shanghao Lu, Shangyan Zhou, Shanhuang Chen,\nShengfeng Ye, Shiyu Wang, Shuiping Yu, Shunfeng Zhou, Shuting Pan, and S. S. Li. Deepseek-r1:\nIncentivizing reasoning capability in llms via reinforcement learning. CoRR, abs/2501.12948, 2025. doi:\n10.48550/ARXIV.2501.12948. URL https://doi.org/10.48550/arXiv.2501.12948.\nMostafa Dehghani, Basil Mustafa, Josip Djolonga, Jonathan Heek, Matthias Minderer, Mathilde Caron,\nAndreas Steiner, Joan Puigcerver, Robert Geirhos, Ibrahim M. Alabdulmohsin, Avital Oliver, Piotr\nPadlewski, Alexey A. Gritsenko, Mario Lucic, and Neil Houlsby. Patch n’ pack: Navit, a vision\ntransformer for any aspect ratio and resolution. 2023a. URL http://papers.nips.cc/paper files/p\naper/2023/hash/06ea400b9b7cfce6428ec27a371632eb-Abstract-Conference.html.\nMostafa Dehghani, Basil Mustafa, Josip Djolonga, Jonathan Heek, Matthias Minderer, Mathilde Caron,\nAndreas Steiner, Joan Puigcerver, Robert Geirhos, Ibrahim M. Alabdulmohsin, Avital Oliver, Piotr\nPadlewski, Alexey A. Gritsenko, Mario Lucic, and Neil Houlsby. Patch n’ pack: Navit, a vision\ntransformer for any aspect ratio and resolution. In Alice Oh, Tristan Naumann, Amir Globerson, Kate\nSaenko, Moritz Hardt, and Sergey Levine (eds.), Advances in Neural Information Processing Systems 36:\nAnnual Conference on Neural Information Processing Systems 2023, NeurIPS 2023, New Orleans, LA, USA,\nDecember 10 - 16, 2023, 2023b. URL http://papers.nips.cc/paper files/paper/2023/hash/06ea4\n00b9b7cfce6428ec27a371632eb-Abstract-Conference.html.\nMatt Deitke, Christopher Clark, Sangho Lee, Rohun Tripathi, Yue Yang, Jae Sung Park, Mohammadreza\nSalehi, Niklas Muennighoff, Kyle Lo, Luca Soldaini, Jiasen Lu, Taira Anderson, Erin Bransom, Kiana\nEhsani, Huong Ngo, Yen-Sung Chen, Ajay Patel, Mark Yatskar, Chris Callison-Burch, Andrew Head,\nRose Hendrix, Favyen Bastani, Eli VanderBilt, Nathan Lambert, Yvonne Chou, Arnavi Chheda, Jenna\nSparks, Sam Skjonsberg, Michael Schmitz, Aaron Sarnat, Byron Bischoff, Pete Walsh, Chris Newell,\nPiper Wolters, Tanmay Gupta, Kuo-Hao Zeng, Jon Borchardt, Dirk Groeneveld, Jen Dumas, Crystal\nNam, Sophie Lebrecht, Caitlin Wittlif, Carissa Schoenick, Oscar Michel, Ranjay Krishna, Luca Weihs,\nNoah A. Smith, Hannaneh Hajishirzi, Ross B. Girshick, Ali Farhadi, and Aniruddha Kembhavi. Molmo\nand pixmo: Open weights and open data for state-of-the-art multimodal models. CoRR, abs/2409.17146,\n2024. doi: 10.48550/ARXIV.2409.17146. URL https://doi.org/10.48550/arXiv.2409.17146.\n60\n\nERNIE 4.5 Technical Report\nJune 29, 2025\nHarish Dattatraya Dixit, Sneha Pendharkar, Matt Beadon, Chris Mason, Tejasvi Chakravarthy, Bharath\nMuthiah, and Sriram Sankar. Silent data corruptions at scale. CoRR, abs/2102.11245, 2021. URL\nhttps://arxiv.org/abs/2102.11245.\nDheeru Dua, Yizhong Wang, Pradeep Dasigi, Gabriel Stanovsky, Sameer Singh, and Matt Gardner. DROP:\nA reading comprehension benchmark requiring discrete reasoning over paragraphs. In Jill Burstein,\nChristy Doran, and Thamar Solorio (eds.), Proceedings of the 2019 Conference of the North American\nChapter of the Association for Computational Linguistics: Human Language Technologies, NAACL-HLT 2019,\nMinneapolis, MN, USA, June 2-7, 2019, Volume 1 (Long and Short Papers), pp. 2368–2378. Association for\nComputational Linguistics, 2019. doi: 10.18653/V1/N19-1246. URL https://doi.org/10.18653/v1/\nn19-1246.\nAlex Fang, Albin Madappally Jose, Amit Jain, Ludwig Schmidt, Alexander T. Toshev, and Vaishaal\nShankar. Data filtering networks. In The Twelfth International Conference on Learning Representations,\nICLR 2024, Vienna, Austria, May 7-11, 2024. OpenReview.net, 2024. URL https://openreview.net/f\norum?id=KAk6ngZ09F.\nElias Frantar and Dan Alistarh. Qmoe: Sub-1-bit compression of trillion parameter models. Proceedings of\nMachine Learning and Systems, 6:439–451, 2024. URL https://proceedings.mlsys.org/paper files\n/paper/2024/file/c74b624843218d9b6713fcf299d6d5e4-Paper-Conference.pdf.\nElias Frantar, Saleh Ashkboos, Torsten Hoefler, and Dan Alistarh. GPTQ: accurate post-training quantiza-\ntion for generative pre-trained transformers. CoRR, abs/2210.17323, 2022. doi: 10.48550/ARXIV.2210.\n17323. URL https://doi.org/10.48550/arXiv.2210.17323.\nChaoyou Fu, Yuhan Dai, Yondong Luo, Lei Li, Shuhuai Ren, Renrui Zhang, Zihan Wang, Chenyu Zhou,\nYunhang Shen, Mengdan Zhang, Peixian Chen, Yanwei Li, Shaohui Lin, Sirui Zhao, Ke Li, Tong Xu,\nXiawu Zheng, Enhong Chen, Rongrong Ji, and Xing Sun. Video-mme: The first-ever comprehensive\nevaluation benchmark of multi-modal llms in video analysis. CoRR, abs/2405.21075, 2024. doi:\n10.48550/ARXIV.2405.21075. URL https://doi.org/10.48550/arXiv.2405.21075.\nAryo Pradipta Gema, Joshua Ong Jun Leang, Giwon Hong, Alessio Devoto, Alberto Carlo Maria Mancino,\nRohit Saxena, Xuanli He, Yu Zhao, Xiaotang Du, Mohammad Reza Ghasemi Madani, Claire Barale,\nRobert McHardy, Joshua Harris, Jean Kaddour, Emile van Krieken, and Pasquale Minervini. Are we\ndone with mmlu? In Luis Chiruzzo, Alan Ritter, and Lu Wang (eds.), Proceedings of the 2025 Conference\nof the Nations of the Americas Chapter of the Association for Computational Linguistics: Human Language\nTechnologies, NAACL 2025 - Volume 1: Long Papers, Albuquerque, New Mexico, USA, April 29 - May 4, 2025,\npp. 5069–5096. Association for Computational Linguistics, 2025. doi: 10.18653/V1/2025.NAACL-LON\nG.262. URL https://doi.org/10.18653/v1/2025.naacl-long.262.\nDeepanway Ghosal, Vernon Toh Yan Han, Yew Ken Chia, and Soujanya Poria. Are language models\npuzzle prodigies? algorithmic puzzles unveil serious challenges in multimodal reasoning. CoRR,\nabs/2403.03864, 2024. doi: 10.48550/ARXIV.2403.03864. URL https://doi.org/10.48550/arXiv.2\n403.03864.\nTNG Technology Consulting GmbH. Deepseek-r1t-chimera, April 2025. URL https://huggingface.co\n/tngtech/DeepSeek-R1T-Chimera.\nR. Gray. Vector quantization. IEEE ASSP Magazine, 1(2):4–29, 1984. doi: 10.1109/MASSP.1984.1162229.\nURL https://ieeexplore.ieee.org/document/1162229.\nSuriya Gunasekar, Yi Zhang, Jyoti Aneja, Caio C´esar Teodoro Mendes, Allie Del Giorno, Sivakanth\nGopi, Mojan Javaheripi, Piero Kauffmann, Gustavo de Rosa, Olli Saarikivi, Adil Salim, Shital Shah,\nHarkirat Singh Behl, Xin Wang, S´ebastien Bubeck, Ronen Eldan, Adam Tauman Kalai, Yin Tat Lee, and\nYuanzhi Li. Textbooks are all you need. CoRR, abs/2306.11644, 2023. doi: 10.48550/ARXIV.2306.11644.\nURL https://doi.org/10.48550/arXiv.2306.11644.\nDong Guo, Faming Wu, Feida Zhu, Fuxing Leng, Guang Shi, Haobin Chen, Haoqi Fan, Jian Wang,\nJianyu Jiang, Jiawei Wang, Jingji Chen, Jingjia Huang, Kang Lei, Liping Yuan, Lishu Luo, Pengfei\nLiu, Qinghao Ye, Rui Qian, Shen Yan, Shixiong Zhao, Shuai Peng, Shuangye Li, Sihang Yuan, Sijin\nWu, Tianheng Cheng, Weiwei Liu, Wenqian Wang, Xianhan Zeng, Xiao Liu, Xiaobo Qin, Xiaohan\nDing, Xiaojun Xiao, Xiaoying Zhang, Xuanwei Zhang, Xuehan Xiong, Yanghua Peng, Yangrui Chen,\nYanwei Li, Yanxu Hu, Yi Lin, Yiyuan Hu, Yiyuan Zhang, Youbin Wu, Yu Li, Yudong Liu, Yue Ling,\nYujia Qin, Zanbo Wang, Zhiwu He, Aoxue Zhang, Bairen Yi, Bencheng Liao, Can Huang, Can Zhang,\nChaorui Deng, Chaoyi Deng, Cheng Lin, Cheng Yuan, Chenggang Li, Chenhui Gou, Chenwei Lou,\nChengzhi Wei, Chundian Liu, Chunyuan Li, Deyao Zhu, Donghong Zhong, Feng Li, Feng Zhang,\nGang Wu, Guodong Li, Guohong Xiao, Haibin Lin, Haihua Yang, Haoming Wang, Heng Ji, Hongxiang\n61\n\nERNIE 4.5 Technical Report\nJune 29, 2025\nHao, Hui Shen, Huixia Li, Jiahao Li, Jialong Wu, Jianhua Zhu, Jianpeng Jiao, Jiashi Feng, Jiaze Chen,\nJianhui Duan, Jihao Liu, Jin Zeng, Jingqun Tang, Jingyu Sun, Joya Chen, Jun Long, Junda Feng,\nJunfeng Zhan, Junjie Fang, Junting Lu, Kai Hua, Kai Liu, Kai Shen, Kaiyuan Zhang, and Ke Shen.\nSeed1.5-vl technical report. CoRR, abs/2505.07062, 2025. doi: 10.48550/ARXIV.2505.07062. URL\nhttps://doi.org/10.48550/arXiv.2505.07062.\nYancheng He, Shilong Li, Jiaheng Liu, Yingshui Tan, Weixun Wang, Hui Huang, Xingyuan Bu, Hangyu\nGuo, Chengwei Hu, Boren Zheng, Zhuoran Lin, Xuepeng Liu, Dekai Sun, Shirong Lin, Zhicheng\nZheng, Xiaoyong Zhu, Wenbo Su, and Bo Zheng. Chinese simpleqa: A chinese factuality evaluation\nfor large language models. CoRR, abs/2411.07140, 2024a. doi: 10.48550/ARXIV.2411.07140. URL\nhttps://doi.org/10.48550/arXiv.2411.07140.\nYancheng He, Shilong Li, Jiaheng Liu, Yingshui Tan, Weixun Wang, Hui Huang, Xingyuan Bu, Hangyu\nGuo, Chengwei Hu, Boren Zheng, Zhuoran Lin, Xuepeng Liu, Dekai Sun, Shirong Lin, Zhicheng\nZheng, Xiaoyong Zhu, Wenbo Su, and Bo Zheng. Chinese simpleqa: A chinese factuality evaluation\nfor large language models. volume abs/2411.07140, 2024b. doi: 10.48550/ARXIV.2411.07140. URL\nhttps://doi.org/10.48550/arXiv.2411.07140.\nYun He, Di Jin, Chaoqi Wang, Chloe Bi, Karishma Mandyam, Hejia Zhang, Chen Zhu, Ning Li, Tengyu\nXu, Hongjiang Lv, Shruti Bhosale, Chenguang Zhu, Karthik Abinav Sankararaman, Eryk Helenowski,\nMelanie Kambadur, Aditya Tayade, Hao Ma, Han Fang, and Sinong Wang. Multi-if: Benchmarking\nllms on multi-turn and multilingual instructions following. CoRR, abs/2410.15553, 2024c. doi: 10.485\n50/ARXIV.2410.15553. URL https://doi.org/10.48550/arXiv.2410.15553.\nDan Hendrycks, Collin Burns, Steven Basart, Andy Zou, Mantas Mazeika, Dawn Song, and Jacob\nSteinhardt. Measuring massive multitask language understanding. In 9th International Conference on\nLearning Representations, ICLR 2021, Virtual Event, Austria, May 3-7, 2021. OpenReview.net, 2021a. URL\nhttps://openreview.net/forum?id=d7KBjmI3GmQ.\nDan Hendrycks, Collin Burns, Saurav Kadavath, Akul Arora, Steven Basart, Eric Tang, Dawn Song,\nand Jacob Steinhardt. Measuring mathematical problem solving with the MATH dataset. In Joaquin\nVanschoren and Sai-Kit Yeung (eds.), Proceedings of the Neural Information Processing Systems Track on\nDatasets and Benchmarks 1, NeurIPS Datasets and Benchmarks 2021, December 2021, virtual, 2021b. URL\nhttps://datasets-benchmarks-proceedings.neurips.cc/paper/2021/hash/be83ab3ecd0db773\neb2dc1b0a17836a1-Abstract-round2.html.\nDan Hendrycks, Collin Burns, Saurav Kadavath, Akul Arora, Steven Basart, Eric Tang, Dawn Song,\nand Jacob Steinhardt. Measuring mathematical problem solving with the MATH dataset. In Joaquin\nVanschoren and Sai-Kit Yeung (eds.), Proceedings of the Neural Information Processing Systems Track on\nDatasets and Benchmarks 1, NeurIPS Datasets and Benchmarks 2021, December 2021, virtual, 2021c. URL\nhttps://datasets-benchmarks-proceedings.neurips.cc/paper/2021/hash/be83ab3ecd0db773\neb2dc1b0a17836a1-Abstract-round2.html.\nWenyi Hong, Weihan Wang, Ming Ding, Wenmeng Yu, Qingsong Lv, Yan Wang, Yean Cheng, Shiyu\nHuang, Junhui Ji, Zhao Xue, Lei Zhao, Zhuoyi Yang, Xiaotao Gu, Xiaohan Zhang, Guanyu Feng, Da Yin,\nZihan Wang, Ji Qi, Xixuan Song, Peng Zhang, Debing Liu, Bin Xu, Juanzi Li, Yuxiao Dong, and Jie Tang.\nCogvlm2: Visual language models for image and video understanding. CoRR, abs/2408.16500, 2024.\ndoi: 10.48550/ARXIV.2408.16500. URL https://doi.org/10.48550/arXiv.2408.16500.\nColeman Hooper, Sehoon Kim, Hiva Mohammadzadeh, Michael W. Mahoney, Yakun Sophia Shao, Kurt\nKeutzer, and Amir Gholami. Kvquant: Towards 10 million context length LLM inference with KV\ncache quantization. In Amir Globersons, Lester Mackey, Danielle Belgrave, Angela Fan, Ulrich Paquet,\nJakub M. Tomczak, and Cheng Zhang (eds.), Advances in Neural Information Processing Systems 38:\nAnnual Conference on Neural Information Processing Systems 2024, NeurIPS 2024, Vancouver, BC, Canada,\nDecember 10 - 15, 2024, 2024. URL http://papers.nips.cc/paper files/paper/2024/hash/028fcbc\nf85435d39a40c4d61b42c99a4-Abstract-Conference.html.\nShengding Hu, Yuge Tu, Xu Han, Chaoqun He, Ganqu Cui, Xiang Long, Zhi Zheng, Yewei Fang,\nYuxiang Huang, Weilin Zhao, Xinrong Zhang, Zhen Leng Thai, Kai Zhang, Chongyi Wang, Yuan Yao,\nChenyang Zhao, Jie Zhou, Jie Cai, Zhongwu Zhai, Ning Ding, Chao Jia, Guoyang Zeng, Dahai Li,\nZhiyuan Liu, and Maosong Sun. Minicpm: Unveiling the potential of small language models with\nscalable training strategies. CoRR, abs/2404.06395, 2024. doi: 10.48550/ARXIV.2404.06395. URL\nhttps://doi.org/10.48550/arXiv.2404.06395.\nYanping Huang, Youlong Cheng, Ankur Bapna, Orhan Firat, Dehao Chen, Mia Xu Chen, HyoukJoong\nLee, Jiquan Ngiam, Quoc V. Le, Yonghui Wu, and Zhifeng Chen. Gpipe: Efficient training of giant\nneural networks using pipeline parallelism. In Hanna M. Wallach, Hugo Larochelle, Alina Beygelzimer,\n62\n\nERNIE 4.5 Technical Report\nJune 29, 2025\nFlorence d’Alch´e-Buc, Emily B. Fox, and Roman Garnett (eds.), Advances in Neural Information Processing\nSystems 32: Annual Conference on Neural Information Processing Systems 2019, NeurIPS 2019, December\n8-14, 2019, Vancouver, BC, Canada, pp. 103–112, 2019. URL https://proceedings.neurips.cc/paper\n/2019/hash/093f65e080a295f8076b1c5722a46aa2-Abstract.html.\nYuzhen Huang, Yuzhuo Bai, Zhihao Zhu, Junlei Zhang, Jinghan Zhang, Tangjun Su, Junteng Liu,\nChuancheng Lv, Yikai Zhang, Jiayi Lei, Yao Fu, Maosong Sun, and Junxian He. C-eval: A multi-\nlevel multi-discipline chinese evaluation suite for foundation models. In Alice Oh, Tristan Naumann,\nAmir Globerson, Kate Saenko, Moritz Hardt, and Sergey Levine (eds.), Advances in Neural Information\nProcessing Systems 36: Annual Conference on Neural Information Processing Systems 2023, NeurIPS 2023,\nNew Orleans, LA, USA, December 10 - 16, 2023, 2023. URL http://papers.nips.cc/paper files/pap\ner/2023/hash/c6ec1844bec96d6d32ae95ae694e23d8-Abstract-Datasets and Benchmarks.html.\nZhen Huang, Zengzhi Wang, Shijie Xia, Xuefeng Li, Haoyang Zou, Ruijie Xu, Run-Ze Fan, Lyumanshan\nYe, Ethan Chern, Yixin Ye, Yikai Zhang, Yuqing Yang, Ting Wu, Binjie Wang, Shichao Sun, Yang Xiao,\nYiyuan Li, Fan Zhou, Steffi Chern, Yiwei Qin, Yan Ma, Jiadi Su, Yixiu Liu, Yuxiang Zheng, Shaoting\nZhang, Dahua Lin, Yu Qiao, and Pengfei Liu. Olympicarena: Benchmarking multi-discipline cognitive\nreasoning for superintelligent AI. In Amir Globersons, Lester Mackey, Danielle Belgrave, Angela Fan,\nUlrich Paquet, Jakub M. Tomczak, and Cheng Zhang (eds.), Advances in Neural Information Processing\nSystems 38: Annual Conference on Neural Information Processing Systems 2024, NeurIPS 2024, Vancouver,\nBC, Canada, December 10 - 15, 2024, 2024. URL http://papers.nips.cc/paper files/paper/2024/ha\nsh/222d2eaf24cf8259a35d6c7130d31425-Abstract-Datasets and Benchmarks Track.html.\nNaman Jain, King Han, Alex Gu, Wen-Ding Li, Fanjia Yan, Tianjun Zhang, Sida Wang, Armando Solar-\nLezama, Koushik Sen, and Ion Stoica. Livecodebench: Holistic and contamination free evaluation of\nlarge language models for code. In The Thirteenth International Conference on Learning Representations,\nICLR 2025, Singapore, April 24-28, 2025. OpenReview.net, 2025. URL https://openreview.net/forum\n?id=chfJJYC3iL.\nAishwarya Kamath, Johan Ferret, Shreya Pathak, Nino Vieillard, Ramona Merhej, Sarah Perrin, Tatiana\nMatejovicova, Alexandre Ram´e, Morgane Rivi`ere, Louis Rouillard, Thomas Mesnard, Geoffrey Cideron,\nJean-Bastien Grill, Sabela Ramos, Edouard Yvinec, Michelle Casbon, Etienne Pot, Ivo Penchev, Ga¨el Liu,\nFrancesco Visin, Kathleen Kenealy, Lucas Beyer, Xiaohai Zhai, Anton Tsitsulin, R´obert Busa-Fekete,\nAlex Feng, Noveen Sachdeva, Benjamin Coleman, Yi Gao, Basil Mustafa, Iain Barr, Emilio Parisotto,\nDavid Tian, Matan Eyal, Colin Cherry, Jan-Thorsten Peter, Danila Sinopalnikov, Surya Bhupatiraju,\nRishabh Agarwal, Mehran Kazemi, Dan Malkin, Ravin Kumar, David Vilar, Idan Brusilovsky, Jiaming\nLuo, Andreas Steiner, Abe Friesen, Abhanshu Sharma, Abheesht Sharma, Adi Mayrav Gilady, Adrian\nGoedeckemeyer, Alaa Saade, Alexander Kolesnikov, Alexei Bendebury, Alvin Abdagic, Amit Vadi,\nAndr´as Gy¨orgy, Andr´e Susano Pinto, Anil Das, Ankur Bapna, Antoine Miech, Antoine Yang, Antonia\nPaterson, Ashish Shenoy, Ayan Chakrabarti, Bilal Piot, Bo Wu, Bobak Shahriari, Bryce Petrini, Charlie\nChen, Charline Le Lan, Christopher A. Choquette-Choo, CJ Carey, Cormac Brick, Daniel Deutsch,\nDanielle Eisenbud, Dee Cattle, Derek Cheng, Dimitris Paparas, Divyashree Shivakumar Sreepathihalli,\nDoug Reid, Dustin Tran, Dustin Zelle, Eric Noland, Erwin Huizenga, Eugene Kharitonov, Frederick\nLiu, Gagik Amirkhanyan, Glenn Cameron, Hadi Hashemi, Hanna Klimczak-Plucinska, Harman Singh,\nHarsh Mehta, Harshal Tushar Lehri, Hussein Hazimeh, Ian Ballantyne, Idan Szpektor, and Ivan Nardini.\nGemma 3 technical report. CoRR, abs/2503.19786, 2025. doi: 10.48550/ARXIV.2503.19786. URL\nhttps://doi.org/10.48550/arXiv.2503.19786.\nAniruddha Kembhavi, Mike Salvato, Eric Kolve, Min Joon Seo, Hannaneh Hajishirzi, and Ali Farhadi.\nA diagram is worth a dozen images. In Bastian Leibe, Jiri Matas, Nicu Sebe, and Max Welling (eds.),\nComputer Vision - ECCV 2016 - 14th European Conference, Amsterdam, The Netherlands, October 11-14, 2016,\nProceedings, Part IV, volume 9908 of Lecture Notes in Computer Science, pp. 235–251. Springer, 2016. doi:\n10.1007/978-3-319-46493-0\\ 15. URL https://doi.org/10.1007/978-3-319-46493-0 15.\nGeewook Kim, Teakgyu Hong, Moonbin Yim, JeongYeon Nam, Jinyoung Park, Jinyeong Yim, Wonseok\nHwang, Sangdoo Yun, Dongyoon Han, and Seunghyun Park. Ocr-free document understanding\ntransformer. In Shai Avidan, Gabriel J. Brostow, Moustapha Ciss´e, Giovanni Maria Farinella, and Tal\nHassner (eds.), Computer Vision - ECCV 2022 - 17th European Conference, Tel Aviv, Israel, October 23-27,\n2022, Proceedings, Part XXVIII, volume 13688 of Lecture Notes in Computer Science, pp. 498–517. Springer,\n2022. doi: 10.1007/978-3-031-19815-1\\ 29. URL https://doi.org/10.1007/978-3-031-19815-1 29.\nYoonsik Kim, Moonbin Yim, and Ka Yeon Song. Tablevqa-bench: A visual question answering benchmark\non multiple table domains. CoRR, abs/2404.19205, 2024. doi: 10.48550/ARXIV.2404.19205. URL\nhttps://doi.org/10.48550/arXiv.2404.19205.\nDiederik P. Kingma and Jimmy Ba. Adam: A method for stochastic optimization. 2015. URL http:\n//arxiv.org/abs/1412.6980.\n63\n\nERNIE 4.5 Technical Report\nJune 29, 2025\nTomasz Korbak, Kejian Shi, Angelica Chen, Rasika Vinayak Bhalerao, Christopher L. Buckley, Jason\nPhang, Samuel R. Bowman, and Ethan Perez. Pretraining language models with human preferences. In\nAndreas Krause, Emma Brunskill, Kyunghyun Cho, Barbara Engelhardt, Sivan Sabato, and Jonathan\nScarlett (eds.), International Conference on Machine Learning, ICML 2023, 23-29 July 2023, Honolulu, Hawaii,\nUSA, volume 202 of Proceedings of Machine Learning Research, pp. 17506–17533. PMLR, 2023. URL\nhttps://proceedings.mlr.press/v202/korbak23a.html.\nVijay Anand Korthikanti, Jared Casper, Sangkug Lym, Lawrence McAfee, Michael Andersch, Mohammad\nShoeybi, and Bryan Catanzaro. Reducing activation recomputation in large transformer models. 2023.\nURL https://proceedings.mlsys.org/paper files/paper/2023/hash/80083951326cf5b35e510\n0260d64ed81-Abstract-mlsys2023.html.\nNathan Lambert, Jacob Morrison, Valentina Pyatkin, Shengyi Huang, Hamish Ivison, Faeze Brahman,\nLester James V. Miranda, Alisa Liu, Nouha Dziri, Shane Lyu, Yuling Gu, Saumya Malik, Victoria Graf,\nJena D. Hwang, Jiangjiang Yang, Ronan Le Bras, Oyvind Tafjord, Chris Wilhelm, Luca Soldaini, Noah A.\nSmith, Yizhong Wang, Pradeep Dasigi, and Hannaneh Hajishirzi. T¨ulu 3: Pushing frontiers in open\nlanguage model post-training. CoRR, abs/2411.15124, 2024. doi: 10.48550/ARXIV.2411.15124. URL\nhttps://doi.org/10.48550/arXiv.2411.15124.\nDmitry Lepikhin, HyoukJoong Lee, Yuanzhong Xu, Dehao Chen, Orhan Firat, Yanping Huang, Maxim\nKrikun, Noam Shazeer, and Zhifeng Chen. Gshard: Scaling giant models with conditional computation\nand automatic sharding. In 9th International Conference on Learning Representations, ICLR 2021, Virtual\nEvent, Austria, May 3-7, 2021. OpenReview.net, 2021. URL https://openreview.net/forum?id=qrwe\n7XHTmYb.\nHaonan Li, Yixuan Zhang, Fajri Koto, Yifei Yang, Hai Zhao, Yeyun Gong, Nan Duan, and Timothy\nBaldwin. CMMLU: measuring massive multitask language understanding in chinese. In Lun-Wei\nKu, Andre Martins, and Vivek Srikumar (eds.), Findings of the Association for Computational Linguistics,\nACL 2024, Bangkok, Thailand and virtual meeting, August 11-16, 2024, pp. 11260–11285. Association\nfor Computational Linguistics, 2024a. doi: 10.18653/V1/2024.FINDINGS-ACL.671. URL https:\n//doi.org/10.18653/v1/2024.findings-acl.671.\nHaoran Li, Qingxiu Dong, Zhengyang Tang, Chaojun Wang, Xingxing Zhang, Haoyang Huang, Shaohan\nHuang, Xiaolong Huang, Zeqiang Huang, Dongdong Zhang, Yuxian Gu, Xin Cheng, Xun Wang,\nSi-Qing Chen, Li Dong, Wei Lu, Zhifang Sui, Benyou Wang, Wai Lam, and Furu Wei. Synthetic data\n(almost) from scratch: Generalized instruction tuning for language models. CoRR, abs/2402.13064,\n2024b. doi: 10.48550/ARXIV.2402.13064. URL https://doi.org/10.48550/arXiv.2402.13064.\nKunchang Li, Yali Wang, Yinan He, Yizhuo Li, Yi Wang, Yi Liu, Zun Wang, Jilan Xu, Guo Chen, Ping\nLou, Limin Wang, and Yu Qiao. Mvbench: A comprehensive multi-modal video understanding\nbenchmark. In IEEE/CVF Conference on Computer Vision and Pattern Recognition, CVPR 2024, Seattle,\nWA, USA, June 16-22, 2024, pp. 22195–22206. IEEE, 2024c. doi: 10.1109/CVPR52733.2024.02095. URL\nhttps://doi.org/10.1109/CVPR52733.2024.02095.\nYunshui Li, Yiyuan Ma, Shen Yan, Chaoyi Zhang, Jing Liu, Jianqiao Lu, Ziwen Xu, Mengzhao Chen,\nMinrui Wang, Shiyi Zhan, Jin Ma, Xunhao Lai, Deyi Liu, Yao Luo, Xingyan Bin, Hongbin Ren, Mingji\nHan, Wenhao Hao, Bairen Yi, LingJun Liu, Bole Ma, Xiaoying Jia, Xun Zhou, Siyuan Qiao, Liang Xiang,\nand Yonghui Wu. Model merging in pre-training of large language models. CoRR, abs/2505.12082,\n2025. doi: 10.48550/ARXIV.2505.12082. URL https://doi.org/10.48550/arXiv.2505.12082.\nWeixin Liang, Lili Yu, Liang Luo, Srini Iyer, Ning Dong, Chunting Zhou, Gargi Ghosh, Mike Lewis,\nWen-tau Yih, Luke Zettlemoyer, and Xi Victoria Lin. Mixture-of-transformers: A sparse and scalable\narchitecture for multi-modal foundation models. Trans. Mach. Learn. Res., 2025, 2025. URL https:\n//openreview.net/forum?id=Nu6N69i8SB.\nBill Yuchen Lin, Ronan Le Bras, Kyle Richardson, Ashish Sabharwal, Radha Poovendran, Peter Clark,\nand Yejin Choi. Zebralogic: On the scaling limits of llms for logical reasoning. volume abs/2502.01100,\n2025. doi: 10.48550/ARXIV.2502.01100. URL https://doi.org/10.48550/arXiv.2502.01100.\nHaokun Lin, Haobo Xu, Yichen Wu, Jingzhi Cui, Yingtao Zhang, Linzhan Mou, Linqi Song, Zhenan Sun,\nand Ying Wei. Duquant: Distributing outliers via dual transformation makes stronger quantized llms.\nIn Amir Globersons, Lester Mackey, Danielle Belgrave, Angela Fan, Ulrich Paquet, Jakub M. Tomczak,\nand Cheng Zhang (eds.), Advances in Neural Information Processing Systems 38: Annual Conference on\nNeural Information Processing Systems 2024, NeurIPS 2024, Vancouver, BC, Canada, December 10 - 15, 2024,\n2024a. URL http://papers.nips.cc/paper files/paper/2024/hash/9febda1c8344cc5f2d5171396\n4864e93-Abstract-Conference.html.\n64\n\nERNIE 4.5 Technical Report\nJune 29, 2025\nJi Lin, Jiaming Tang, Haotian Tang, Shang Yang, Wei-Ming Chen, Wei-Chen Wang, Guangxuan Xiao,\nXingyu Dang, Chuang Gan, and Song Han. AWQ: activation-aware weight quantization for on-device\nLLM compression and acceleration. In Phillip B. Gibbons, Gennady Pekhimenko, and Christopher De\nSa (eds.), Proceedings of the Seventh Annual Conference on Machine Learning and Systems, MLSys 2024,\nSanta Clara, CA, USA, May 13-16, 2024. mlsys.org, 2024b. URL https://proceedings.mlsys.org/pa\nper files/paper/2024/hash/42a452cbafa9dd64e9ba4aa95cc1ef21-Abstract-Conference.html.\nJi Lin, Hongxu Yin, Wei Ping, Pavlo Molchanov, Mohammad Shoeybi, and Song Han. VILA: on pre-\ntraining for visual language models. In IEEE/CVF Conference on Computer Vision and Pattern Recognition,\nCVPR 2024, Seattle, WA, USA, June 16-22, 2024, pp. 26679–26689. IEEE, 2024c. doi: 10.1109/CVPR52733.\n2024.02520. URL https://doi.org/10.1109/CVPR52733.2024.02520.\nXi Victoria Lin, Akshat Shrivastava, Liang Luo, Srinivasan Iyer, Mike Lewis, Gargi Ghosh, Luke\nZettlemoyer, and Armen Aghajanyan. Moma: Efficient early-fusion pre-training with mixture of\nmodality-aware experts. CoRR, abs/2407.21770, 2024d. doi: 10.48550/ARXIV.2407.21770. URL\nhttps://doi.org/10.48550/arXiv.2407.21770.\nJiawei Liu, Chunqiu Steven Xia, Yuyao Wang, and Lingming Zhang. Is your code generated by chatgpt\nreally correct? rigorous evaluation of large language models for code generation. In Alice Oh, Tristan\nNaumann, Amir Globerson, Kate Saenko, Moritz Hardt, and Sergey Levine (eds.), Advances in Neural\nInformation Processing Systems 36: Annual Conference on Neural Information Processing Systems 2023,\nNeurIPS 2023, New Orleans, LA, USA, December 10 - 16, 2023, 2023. URL http://papers.nips.cc/pap\ner files/paper/2023/hash/43e9d647ccd3e4b7b5baab53f0368686-Abstract-Conference.html.\nYifei Liu, Jicheng Wen, Yang Wang, Shengyu Ye, Li Lyna Zhang, Ting Cao, Cheng Li, and Mao Yang.\nVPTQ: extreme low-bit vector post-training quantization for large language models. In Yaser Al-\nOnaizan, Mohit Bansal, and Yun-Nung Chen (eds.), Proceedings of the 2024 Conference on Empirical\nMethods in Natural Language Processing, EMNLP 2024, Miami, FL, USA, November 12-16, 2024, pp. 8181–\n8196. Association for Computational Linguistics, 2024a. doi: 10.18653/V1/2024.EMNLP-MAIN.467.\nURL https://doi.org/10.18653/v1/2024.emnlp-main.467.\nYuan Liu, Haodong Duan, Yuanhan Zhang, Bo Li, Songyang Zhang, Wangbo Zhao, Yike Yuan, Jiaqi Wang,\nConghui He, Ziwei Liu, Kai Chen, and Dahua Lin. Mmbench: Is your multi-modal model an all-around\nplayer? In Ales Leonardis, Elisa Ricci, Stefan Roth, Olga Russakovsky, Torsten Sattler, and G¨ul Varol\n(eds.), Computer Vision - ECCV 2024 - 18th European Conference, Milan, Italy, September 29-October 4, 2024,\nProceedings, Part VI, volume 15064 of Lecture Notes in Computer Science, pp. 216–233. Springer, 2024b.\ndoi: 10.1007/978-3-031-72658-3\\ 13. URL https://doi.org/10.1007/978-3-031-72658-3 13.\nYuliang Liu, Zhang Li, Mingxin Huang, Biao Yang, Wenwen Yu, Chunyuan Li, Xu-Cheng Yin, Cheng-Lin\nLiu, Lianwen Jin, and Xiang Bai. Ocrbench: on the hidden mystery of OCR in large multimodal models.\nSci. China Inf. Sci., 67(12), 2024c. doi: 10.1007/S11432-024-4235-6. URL https://doi.org/10.1007/s1\n1432-024-4235-6.\nZechun Liu, Changsheng Zhao, Igor Fedorov, Bilge Soran, Dhruv Choudhary, Raghuraman Krish-\nnamoorthi, Vikas Chandra, Yuandong Tian, and Tijmen Blankevoort. Spinquant: LLM quantization\nwith learned rotations. CoRR, abs/2405.16406, 2024d. doi: 10.48550/ARXIV.2405.16406. URL\nhttps://doi.org/10.48550/arXiv.2405.16406.\nIlya Loshchilov and Frank Hutter. Decoupled weight decay regularization. In 7th International Conference\non Learning Representations, ICLR 2019, New Orleans, LA, USA, May 6-9, 2019. OpenReview.net, 2019.\nURL https://openreview.net/forum?id=Bkg6RiCqY7.\nPan Lu, Hritik Bansal, Tony Xia, Jiacheng Liu, Chunyuan Li, Hannaneh Hajishirzi, Hao Cheng, Kai-Wei\nChang, Michel Galley, and Jianfeng Gao. Mathvista: Evaluating math reasoning in visual contexts with\ngpt-4v, bard, and other large multimodal models. CoRR, abs/2310.02255, 2023. doi: 10.48550/ARXIV.2\n310.02255. URL https://doi.org/10.48550/arXiv.2310.02255.\nYanjun Ma, Dianhai Yu, Tian Wu, and Haifeng Wang. Paddlepaddle: An open-source deep learning\nplatform from industrial practice. Frontiers of Data and Domputing, 1(1):105–115, 2019. URL http:\n//www.jfdc.cnic.cn/EN/10.11871/jfdc.issn.2096.742X.2019.01.011.\nAhmed Masry, Do Xuan Long, Jia Qing Tan, Shafiq R. Joty, and Enamul Hoque. Chartqa: A benchmark\nfor question answering about charts with visual and logical reasoning. In Smaranda Muresan, Preslav\nNakov, and Aline Villavicencio (eds.), Findings of the Association for Computational Linguistics: ACL 2022,\nDublin, Ireland, May 22-27, 2022, pp. 2263–2279. Association for Computational Linguistics, 2022. doi:\n10.18653/V1/2022.FINDINGS-ACL.177. URL https://doi.org/10.18653/v1/2022.findings-acl\n.177.\n65\n\nERNIE 4.5 Technical Report\nJune 29, 2025\nMinesh Mathew, Dimosthenis Karatzas, and C. V. Jawahar. Docvqa: A dataset for VQA on document\nimages. In IEEE Winter Conference on Applications of Computer Vision, WACV 2021, Waikoloa, HI, USA,\nJanuary 3-8, 2021, pp. 2199–2208. IEEE, 2021. doi: 10.1109/WACV48630.2021.00225. URL https:\n//doi.org/10.1109/WACV48630.2021.00225.\nMeta-AI. The Llama 4 herd: The beginning of a new era of natively multimodal AI innovation, 2025.\nURL https://ai.meta.com/blog/llama-4-multimodal-intelligence/.\nShen-Yun Miao, Chao-Chun Liang, and Keh-Yih Su. A diverse corpus for evaluating and developing\nenglish math word problem solvers. In Dan Jurafsky, Joyce Chai, Natalie Schluter, and Joel R. Tetreault\n(eds.), Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, ACL 2020,\nOnline, July 5-10, 2020, pp. 975–984. Association for Computational Linguistics, 2020. doi: 10.18653/V\n1/2020.ACL-MAIN.92. URL https://doi.org/10.18653/v1/2020.acl-main.92.\nPaulius Micikevicius, Dusan Stosic, Neil Burgess, Marius Cornea, Pradeep Dubey, Richard Grisenth-\nwaite, Sangwon Ha, Alexander Heinecke, Patrick Judd, John Kamalu, Naveen Mellempudi, Stuart F.\nOberman, Mohammad Shoeybi, Michael Y. Siu, and Hao Wu. FP8 formats for deep learning. CoRR,\nabs/2209.05433, 2022. doi: 10.48550/ARXIV.2209.05433. URL https://doi.org/10.48550/arXiv.2\n209.05433.\nAntoine Miech, Dimitri Zhukov, Jean-Baptiste Alayrac, Makarand Tapaswi, Ivan Laptev, and Josef\nSivic. Howto100m: Learning a text-video embedding by watching hundred million narrated video\nclips. In 2019 IEEE/CVF International Conference on Computer Vision, ICCV 2019, Seoul, Korea (South),\nOctober 27 - November 2, 2019, pp. 2630–2640. IEEE, 2019. doi: 10.1109/ICCV.2019.00272. URL\nhttps://doi.org/10.1109/ICCV.2019.00272.\nDeepak Narayanan, Mohammad Shoeybi, Jared Casper, Patrick LeGresley, Mostofa Patwary, Vijay\nKorthikanti, Dmitri Vainbrand, Prethvi Kashinkunti, Julie Bernauer, Bryan Catanzaro, Amar Phan-\nishayee, and Matei Zaharia. Efficient large-scale language model training on GPU clusters using\nmegatron-lm. In Bronis R. de Supinski, Mary W. Hall, and Todd Gamblin (eds.), International Con-\nference for High Performance Computing, Networking, Storage and Analysis, SC 2021, St. Louis, Mis-\nsouri, USA, November 14-19, 2021, pp.\n58. ACM, 2021.\ndoi: 10.1145/3458817.3476209.\nURL\nhttps://doi.org/10.1145/3458817.3476209.\nNVIDIA. Transformerengine, 2024. URL https://github.com/NVIDIA/TransformerEngine. Accessed:\n2024-11-19.\nOpenAI. Hello GPT-4o, 2024. URL https://openai.com/index/hello-gpt-4o/.\nOpenAI. Introducing gpt-4.1 in the api, 2025a. URL https://openai.com/index/gpt-4-1/.\nOpenAI. Introducing gpt-4.5, 2025b. URL https://openai.com/index/introducing-gpt-4-5/.\nOpenAI. Introducing openai o1, 2025c. URL https://openai.com/index/openai-o1-system-card/.\nOpenAI. Introducing openai o3 and o4-mini, 2025d. URL https://openai.com/index/introducing-o\n3-and-o4-mini/.\nLong Ouyang, Jeffrey Wu, Xu Jiang, Diogo Almeida, Carroll L. Wainwright, Pamela Mishkin, Chong\nZhang, Sandhini Agarwal, Katarina Slama, Alex Ray, John Schulman, Jacob Hilton, Fraser Kelton,\nLuke Miller, Maddie Simens, Amanda Askell, Peter Welinder, Paul F. Christiano, Jan Leike, and\nRyan Lowe. Training language models to follow instructions with human feedback. 2022. URL\nhttp://papers.nips.cc/paper files/paper/2022/hash/b1efde53be364a73914f58805a001731-A\nbstract-Conference.html.\nRoni Paiss, Ariel Ephrat, Omer Tov, Shiran Zada, Inbar Mosseri, Michal Irani, and Tali Dekel. Teaching\nCLIP to count to ten. In IEEE/CVF International Conference on Computer Vision, ICCV 2023, Paris,\nFrance, October 1-6, 2023, pp. 3147–3157. IEEE, 2023. doi: 10.1109/ICCV51070.2023.00294. URL\nhttps://doi.org/10.1109/ICCV51070.2023.00294.\nArkil Patel, Satwik Bhattamishra, and Navin Goyal. Are NLP models really able to solve simple math\nword problems? In Kristina Toutanova, Anna Rumshisky, Luke Zettlemoyer, Dilek Hakkani-T¨ur,\nIz Beltagy, Steven Bethard, Ryan Cotterell, Tanmoy Chakraborty, and Yichao Zhou (eds.), Proceedings\nof the 2021 Conference of the North American Chapter of the Association for Computational Linguistics:\nHuman Language Technologies, NAACL-HLT 2021, Online, June 6-11, 2021, pp. 2080–2094. Association\nfor Computational Linguistics, 2021. doi: 10.18653/V1/2021.NAACL-MAIN.168. URL https:\n//doi.org/10.18653/v1/2021.naacl-main.168.\n66\n\nERNIE 4.5 Technical Report\nJune 29, 2025\nHouwen Peng, Kan Wu, Yixuan Wei, Guoshuai Zhao, Yuxiang Yang, Ze Liu, Yifan Xiong, Ziyue Yang,\nBolin Ni, Jingcheng Hu, Ruihang Li, Miaosen Zhang, Chen Li, Jia Ning, Ruizhe Wang, Zheng Zhang,\nShuguang Liu, Joe Chau, Han Hu, and Peng Cheng. FP8-LM: training FP8 large language models.\nCoRR, abs/2310.18313, 2023. doi: 10.48550/ARXIV.2310.18313. URL https://doi.org/10.48550/arX\niv.2310.18313.\nJinghui Qin, Xiaodan Liang, Yining Hong, Jianheng Tang, and Liang Lin. Neural-symbolic solver for\nmath word problems with auxiliary tasks. In Chengqing Zong, Fei Xia, Wenjie Li, and Roberto Navigli\n(eds.), Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th\nInternational Joint Conference on Natural Language Processing, ACL/IJCNLP 2021, (Volume 1: Long Papers),\nVirtual Event, August 1-6, 2021, pp. 5870–5881. Association for Computational Linguistics, 2021. doi:\n10.18653/V1/2021.ACL-LONG.456. URL https://doi.org/10.18653/v1/2021.acl-long.456.\nYanzhao Qin, Tao Zhang, Tao Zhang, Yanjun Shen, Wenjing Luo, Haoze Sun, Yan Zhang, Yujing Qiao,\nWeipeng Chen, Zenan Zhou, Wentao Zhang, and Bin Cui. Sysbench: Can large language models\nfollow system messages? volume abs/2408.10943, 2024. doi: 10.48550/ARXIV.2408.10943. URL\nhttps://doi.org/10.48550/arXiv.2408.10943.\nAlec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh, Sandhini Agarwal, Girish\nSastry, Amanda Askell, Pamela Mishkin, Jack Clark, Gretchen Krueger, and Ilya Sutskever. Learning\ntransferable visual models from natural language supervision. In Marina Meila and Tong Zhang\n(eds.), Proceedings of the 38th International Conference on Machine Learning, ICML 2021, 18-24 July 2021,\nVirtual Event, volume 139 of Proceedings of Machine Learning Research, pp. 8748–8763. PMLR, 2021a. URL\nhttp://proceedings.mlr.press/v139/radford21a.html.\nAlec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh, Sandhini Agarwal, Girish\nSastry, Amanda Askell, Pamela Mishkin, Jack Clark, Gretchen Krueger, and Ilya Sutskever. Learning\ntransferable visual models from natural language supervision. In Marina Meila and Tong Zhang\n(eds.), Proceedings of the 38th International Conference on Machine Learning, ICML 2021, 18-24 July 2021,\nVirtual Event, volume 139 of Proceedings of Machine Learning Research, pp. 8748–8763. PMLR, 2021b. URL\nhttp://proceedings.mlr.press/v139/radford21a.html.\nPooyan Rahmanzadehgervi, Logan Bolton, Mohammad Reza Taesiri, and Anh Totti Nguyen. Vision\nlanguage models are blind. volume abs/2407.06581, 2024. doi: 10.48550/ARXIV.2407.06581. URL\nhttps://doi.org/10.48550/arXiv.2407.06581.\nSamyam Rajbhandari, Jeff Rasley, Olatunji Ruwase, and Yuxiong He. Zero: Memory optimizations\ntoward training trillion parameter models. pp. 1–16, 2020. doi: 10.1109/SC41405.2020.00024. URL\nhttps://arxiv.org/abs/1910.02054.\nJonathan Roberts, Mohammad Reza Taesiri, Ansh Sharma, Akash Gupta, Samuel Roberts, Ioana Croitoru,\nSimion-Vlad Bogolin, Jialu Tang, Florian Langer, Vyas Raina, Vatsal Raina, Hanyi Xiong, Vishaal\nUdandarao, Jingyi Lu, Shiyang Chen, Sam Purkis, Tianshuo Yan, Wenye Lin, Gyungin Shin, Qiaochu\nYang, Anh Totti Nguyen, Kai Han, and Samuel Albanie. Zerobench: An impossible visual benchmark\nfor contemporary large multimodal models. CoRR, abs/2502.09696, 2025. doi: 10.48550/ARXIV.2502.\n09696. URL https://doi.org/10.48550/arXiv.2502.09696.\nJosselin Somerville Roberts, Tony Lee, Chi Heem Wong, Michihiro Yasunaga, Yifan Mai, and Percy Liang.\nImage2struct: Benchmarking structure extraction for vision-language models. In Amir Globersons,\nLester Mackey, Danielle Belgrave, Angela Fan, Ulrich Paquet, Jakub M. Tomczak, and Cheng Zhang\n(eds.), Advances in Neural Information Processing Systems 38: Annual Conference on Neural Information\nProcessing Systems 2024, NeurIPS 2024, Vancouver, BC, Canada, December 10 - 15, 2024, 2024. URL\nhttp://papers.nips.cc/paper files/paper/2024/hash/d0718553fd6b227a353c6432cf893285-A\nbstract-Datasets and Benchmarks Track.html.\nKeisuke Sakaguchi, Ronan Le Bras, Chandra Bhagavatula, and Yejin Choi. Winogrande: An adversarial\nwinograd schema challenge at scale. In The Thirty-Fourth AAAI Conference on Artificial Intelligence,\nAAAI 2020, The Thirty-Second Innovative Applications of Artificial Intelligence Conference, IAAI 2020, The\nTenth AAAI Symposium on Educational Advances in Artificial Intelligence, EAAI 2020, New York, NY,\nUSA, February 7-12, 2020, pp. 8732–8740. AAAI Press, 2020. doi: 10.1609/AAAI.V34I05.6399. URL\nhttps://doi.org/10.1609/aaai.v34i05.6399.\nNicol N. Schraudolph. A fast, compact approximation of the exponential function. Neural Comput., 11(4):\n853–862, 1999. doi: 10.1162/089976699300016467. URL https://doi.org/10.1162/08997669930001\n6467.\n67\n\nERNIE 4.5 Technical Report\nJune 29, 2025\nChristoph Schuhmann, Romain Beaumont, Richard Vencu, Cade Gordon, Ross Wightman, Mehdi Cherti,\nTheo Coombes, Aarush Katta, Clayton Mullis, Mitchell Wortsman, Patrick Schramowski, Srivatsa\nKundurthy, Katherine Crowson, Ludwig Schmidt, Robert Kaczmarczyk, and Jenia Jitsev. LAION-5B: an\nopen large-scale dataset for training next generation image-text models. In Sanmi Koyejo, S. Mohamed,\nA. Agarwal, Danielle Belgrave, K. Cho, and A. Oh (eds.), Advances in Neural Information Processing\nSystems 35: Annual Conference on Neural Information Processing Systems 2022, NeurIPS 2022, New Orleans,\nLA, USA, November 28 - December 9, 2022, 2022. URL http://papers.nips.cc/paper files/paper/2\n022/hash/a1859debfb3b59d094f3504d5ebb6c25-Abstract-Datasets and Benchmarks.html.\nJohn Schulman, Filip Wolski, Prafulla Dhariwal, Alec Radford, and Oleg Klimov. Proximal policy\noptimization algorithms. CoRR, abs/1707.06347, 2017. URL http://arxiv.org/abs/1707.06347.\nZhihong Shao, Peiyi Wang, Qihao Zhu, Runxin Xu, Junxiao Song, Mingchuan Zhang, Y. K. Li, Y. Wu, and\nDaya Guo. Deepseekmath: Pushing the limits of mathematical reasoning in open language models.\nCoRR, abs/2402.03300, 2024. doi: 10.48550/ARXIV.2402.03300. URL https://doi.org/10.48550/arX\niv.2402.03300.\nFreda Shi, Mirac Suzgun, Markus Freitag, Xuezhi Wang, Suraj Srivats, Soroush Vosoughi, Hyung Won\nChung, Yi Tay, Sebastian Ruder, Denny Zhou, Dipanjan Das, and Jason Wei. Language models\nare multilingual chain-of-thought reasoners.\nIn The Eleventh International Conference on Learning\nRepresentations, ICLR 2023, Kigali, Rwanda, May 1-5, 2023. OpenReview.net, 2023.\nURL https:\n//openreview.net/forum?id=fR3wGCk-IXp.\nWenzhe Shi, Jose Caballero, Ferenc Huszar, Johannes Totz, Andrew P. Aitken, Rob Bishop, Daniel\nRueckert, and Zehan Wang. Real-time single image and video super-resolution using an efficient sub-\npixel convolutional neural network. In 2016 IEEE Conference on Computer Vision and Pattern Recognition,\nCVPR 2016, Las Vegas, NV, USA, June 27-30, 2016, pp. 1874–1883. IEEE Computer Society, 2016. doi:\n10.1109/CVPR.2016.207. URL https://doi.org/10.1109/CVPR.2016.207.\nMohammad Shoeybi, Mostofa Patwary, Raul Puri, Patrick LeGresley, Jared Casper, and Bryan Catanzaro.\nMegatron-lm: Training multi-billion parameter language models using model parallelism. CoRR,\nabs/1909.08053, 2019. URL http://arxiv.org/abs/1909.08053.\nYueqi Song, Tianyue Ou, Yibo Kong, Zecheng Li, Graham Neubig, and Xiang Yue. Visualpuzzles:\nDecoupling multimodal reasoning evaluation from domain knowledge. CoRR, abs/2504.10342, 2025.\ndoi: 10.48550/ARXIV.2504.10342. URL https://doi.org/10.48550/arXiv.2504.10342.\nZayne Sprague, Xi Ye, Kaj Bostrom, Swarat Chaudhuri, and Greg Durrett. Musr: Testing the limits\nof chain-of-thought with multistep soft reasoning. In The Twelfth International Conference on Learning\nRepresentations, ICLR 2024, Vienna, Austria, May 7-11, 2024. OpenReview.net, 2024.\nURL https:\n//openreview.net/forum?id=jenyYQzue1.\nDan Su, Kezhi Kong, Ying Lin, Joseph Jennings, Brandon Norick, Markus Kliegl, Mostofa Patwary,\nMohammad Shoeybi, and Bryan Catanzaro. Nemotron-cc: Transforming common crawl into a refined\nlong-horizon pretraining dataset. CoRR, abs/2412.02595, 2024a. doi: 10.48550/ARXIV.2412.02595.\nURL https://doi.org/10.48550/arXiv.2412.02595.\nJianlin Su, Murtadha H. M. Ahmed, Yu Lu, Shengfeng Pan, Wen Bo, and Yunfeng Liu. Roformer:\nEnhanced transformer with rotary position embedding. Neurocomputing, 568:127063, 2024b. doi:\n10.1016/J.NEUCOM.2023.127063. URL https://doi.org/10.1016/j.neucom.2023.127063.\nQuan Sun, Yuxin Fang, Ledell Wu, Xinlong Wang, and Yue Cao. EVA-CLIP: improved training techniques\nfor CLIP at scale. CoRR, abs/2303.15389, 2023. doi: 10.48550/ARXIV.2303.15389. URL https:\n//doi.org/10.48550/arXiv.2303.15389.\nYu Sun, Shuohuan Wang, Shikun Feng, Siyu Ding, Chao Pang, Junyuan Shang, Jiaxiang Liu, Xuyi Chen,\nYanbin Zhao, Yuxiang Lu, Weixin Liu, Zhihua Wu, Weibao Gong, Jianzhong Liang, Zhizhou Shang,\nPeng Sun, Wei Liu, Xuan Ouyang, Dianhai Yu, Hao Tian, Hua Wu, and Haifeng Wang. ERNIE 3.0:\nLarge-scale knowledge enhanced pre-training for language understanding and generation. CoRR,\nabs/2107.02137, 2021. URL https://arxiv.org/abs/2107.02137.\nMirac Suzgun, Nathan Scales, Nathanael Sch¨arli, Sebastian Gehrmann, Yi Tay, Hyung Won Chung,\nAakanksha Chowdhery, Quoc V. Le, Ed H. Chi, Denny Zhou, and Jason Wei. Challenging big-bench\ntasks and whether chain-of-thought can solve them. In Anna Rogers, Jordan L. Boyd-Graber, and\nNaoaki Okazaki (eds.), Findings of the Association for Computational Linguistics: ACL 2023, Toronto, Canada,\nJuly 9-14, 2023, pp. 13003–13051. Association for Computational Linguistics, 2023. doi: 10.18653/V1/20\n23.FINDINGS-ACL.824. URL https://doi.org/10.18653/v1/2023.findings-acl.824.\n68\n\nERNIE 4.5 Technical Report\nJune 29, 2025\nPeter Tong, Ellis Brown, Penghao Wu, Sanghyun Woo, Adithya Iyer, Sai Charitha Akula, Shusheng\nYang, Jihan Yang, Manoj Middepogu, Ziteng Wang, Xichen Pan, Rob Fergus, Yann LeCun, and Saining\nXie. Cambrian-1: A fully open, vision-centric exploration of multimodal llms. In Amir Globersons,\nLester Mackey, Danielle Belgrave, Angela Fan, Ulrich Paquet, Jakub M. Tomczak, and Cheng Zhang\n(eds.), Advances in Neural Information Processing Systems 38: Annual Conference on Neural Information\nProcessing Systems 2024, NeurIPS 2024, Vancouver, BC, Canada, December 10 - 15, 2024, 2024. URL\nhttp://papers.nips.cc/paper files/paper/2024/hash/9ee3a664ccfeabc0da16ac6f1f1cfe59-A\nbstract-Conference.html.\ntorchao. Torchao: Pytorch-native training-to-serving model optimization, oct 2024. URL https://gith\nub.com/pytorch/torchao.\nAlbert Tseng, Jerry Chee, Qingyao Sun, Volodymyr Kuleshov, and Christopher De Sa. Quip#: Even\nbetter LLM quantization with hadamard incoherence and lattice codebooks. In Forty-first International\nConference on Machine Learning, ICML 2024, Vienna, Austria, July 21-27, 2024. OpenReview.net, 2024a.\nURL https://openreview.net/forum?id=9BrydUVcoe.\nAlbert Tseng, Qingyao Sun, David Hou, and Christopher De Sa. QTIP: quantization with trellises and\nincoherence processing. In Amir Globersons, Lester Mackey, Danielle Belgrave, Angela Fan, Ulrich\nPaquet, Jakub M. Tomczak, and Cheng Zhang (eds.), Advances in Neural Information Processing Systems\n38: Annual Conference on Neural Information Processing Systems 2024, NeurIPS 2024, Vancouver, BC, Canada,\nDecember 10 - 15, 2024, 2024b. URL http://papers.nips.cc/paper files/paper/2024/hash/6de2e\n84b8da47bb2eb5e2ac96c63d2b0-Abstract-Conference.html.\nGuoxia Wang, Jinle Zeng, Xiyuan Xiao, Siming Wu, Jiabin Yang, Lujing Zheng, Zeyu Chen, Jiang Bian,\nDianhai Yu, and Haifeng Wang. Flashmask: Efficient and rich mask extension of flashattention. In The\nThirteenth International Conference on Learning Representations, ICLR 2025, Singapore, April 24-28, 2025.\nOpenReview.net, 2025. URL https://openreview.net/forum?id=wUtXB43Chi.\nPeng Wang, Shuai Bai, Sinan Tan, Shijie Wang, Zhihao Fan, Jinze Bai, Keqin Chen, Xuejing Liu, Jialin\nWang, Wenbin Ge, Yang Fan, Kai Dang, Mengfei Du, Xuancheng Ren, Rui Men, Dayiheng Liu, Chang\nZhou, Jingren Zhou, and Junyang Lin. Qwen2-vl: Enhancing vision-language model’s perception of\nthe world at any resolution. CoRR, abs/2409.12191, 2024a. doi: 10.48550/ARXIV.2409.12191. URL\nhttps://doi.org/10.48550/arXiv.2409.12191.\nWeihan Wang, Qingsong Lv, Wenmeng Yu, Wenyi Hong, Ji Qi, Yan Wang, Junhui Ji, Zhuoyi Yang, Lei\nZhao, Xixuan Song, Jiazheng Xu, Keqin Chen, Bin Xu, Juanzi Li, Yuxiao Dong, Ming Ding, and Jie\nTang. Cogvlm: Visual expert for pretrained language models. In Amir Globersons, Lester Mackey,\nDanielle Belgrave, Angela Fan, Ulrich Paquet, Jakub M. Tomczak, and Cheng Zhang (eds.), Advances in\nNeural Information Processing Systems 38: Annual Conference on Neural Information Processing Systems 2024,\nNeurIPS 2024, Vancouver, BC, Canada, December 10 - 15, 2024, 2024b. URL http://papers.nips.cc/pap\ner files/paper/2024/hash/dc06d4d2792265fb5454a6092bfd5c6a-Abstract-Conference.html.\nWenhui Wang, Hangbo Bao, Li Dong, Johan Bjorck, Zhiliang Peng, Qiang Liu, Kriti Aggarwal,\nOwais Khan Mohammed, Saksham Singhal, Subhojit Som, and Furu Wei. Image as a foreign lan-\nguage: Beit pretraining for all vision and vision-language tasks. CoRR, abs/2208.10442, 2022. doi:\n10.48550/ARXIV.2208.10442. URL https://doi.org/10.48550/arXiv.2208.10442.\nYubo Wang, Xueguang Ma, Ge Zhang, Yuansheng Ni, Abhranil Chandra, Shiguang Guo, Weiming\nRen, Aaran Arulraj, Xuan He, Ziyan Jiang, Tianle Li, Max Ku, Kai Wang, Alex Zhuang, Rongqi\nFan, Xiang Yue, and Wenhu Chen. Mmlu-pro: A more robust and challenging multi-task language\nunderstanding benchmark. In Amir Globersons, Lester Mackey, Danielle Belgrave, Angela Fan, Ulrich\nPaquet, Jakub M. Tomczak, and Cheng Zhang (eds.), Advances in Neural Information Processing Systems\n38: Annual Conference on Neural Information Processing Systems 2024, NeurIPS 2024, Vancouver, BC, Canada,\nDecember 10 - 15, 2024, 2024c. URL http://papers.nips.cc/paper files/paper/2024/hash/ad236\nedc564f3e3156e1b2feafb99a24-Abstract-Datasets and Benchmarks Track.html.\nJason Wei, Nguyen Karina, Hyung Won Chung, Yunxin Joy Jiao, Spencer Papay, Amelia Glaese, John\nSchulman, and William Fedus. Measuring short-form factuality in large language models. CoRR,\nabs/2411.04368, 2024a. doi: 10.48550/ARXIV.2411.04368. URL https://doi.org/10.48550/arXiv.2\n411.04368.\nTianwen Wei, Jian Luan, Wei Liu, Shuang Dong, and Bin Wang. CMATH: can your language model pass\nchinese elementary school math test? CoRR, abs/2306.16636, 2023. doi: 10.48550/ARXIV.2306.16636.\nURL https://doi.org/10.48550/arXiv.2306.16636.\n69\n\nERNIE 4.5 Technical Report\nJune 29, 2025\nXilin Wei, Xiaoran Liu, Yuhang Zang, Xiaoyi Dong, Pan Zhang, Yuhang Cao, Jian Tong, Haodong\nDuan, Qipeng Guo, Jiaqi Wang, Xipeng Qiu, and Dahua Lin. Videorope: What makes for good video\nrotary position embedding? CoRR, abs/2502.05173, 2025. doi: 10.48550/ARXIV.2502.05173. URL\nhttps://doi.org/10.48550/arXiv.2502.05173.\nYuxiang Wei, Federico Cassano, Jiawei Liu, Yifeng Ding, Naman Jain, Zachary Mueller, Harm de Vries,\nLeandro von Werra, Arjun Guha, and Lingming Zhang. Selfcodealign: Self-alignment for code\ngeneration. In Amir Globersons, Lester Mackey, Danielle Belgrave, Angela Fan, Ulrich Paquet, Jakub M.\nTomczak, and Cheng Zhang (eds.), Advances in Neural Information Processing Systems 38: Annual\nConference on Neural Information Processing Systems 2024, NeurIPS 2024, Vancouver, BC, Canada, December\n10 - 15, 2024, 2024b. URL http://papers.nips.cc/paper files/paper/2024/hash/72da102da91a8\n042a0b2aa968429a9f9-Abstract-Conference.html.\nAlexander Wettig, Aatmik Gupta, Saumya Malik, and Danqi Chen. Qurating: Selecting high-quality data\nfor training language models. In Forty-first International Conference on Machine Learning, ICML 2024,\nVienna, Austria, July 21-27, 2024. OpenReview.net, 2024. URL https://openreview.net/forum?id=GL\nGYYqPwjy.\nColin White, Samuel Dooley, Manley Roberts, Arka Pal, Benjamin Feuer, Siddhartha Jain, Ravid Shwartz-\nZiv, Neel Jain, Khalid Saifullah, Sreemanti Dey, Shubh-Agrawal, Sandeep Singh Sandha, Siddartha V.\nNaidu, Chinmay Hegde, Yann LeCun, Tom Goldstein, Willie Neiswanger, and Micah Goldblum.\nLivebench: A challenging, contamination-limited LLM benchmark. In The Thirteenth International\nConference on Learning Representations, ICLR 2025, Singapore, April 24-28, 2025. OpenReview.net, 2025.\nURL https://openreview.net/forum?id=sKYHBTAxVa.\nWikipedia. Dikw pyramid, 2025. URL https://en.wikipedia.org/wiki/DIKW pyramid.\nHaoning Wu, Dongxu Li, Bei Chen, and Junnan Li. Longvideobench: A benchmark for long-context\ninterleaved video-language understanding. In Amir Globersons, Lester Mackey, Danielle Belgrave,\nAngela Fan, Ulrich Paquet, Jakub M. Tomczak, and Cheng Zhang (eds.), Advances in Neural Information\nProcessing Systems 38: Annual Conference on Neural Information Processing Systems 2024, NeurIPS 2024,\nVancouver, BC, Canada, December 10 - 15, 2024, 2024. URL http://papers.nips.cc/paper files/pap\ner/2024/hash/329ad516cf7a6ac306f29882e9c77558-Abstract-Datasets and Benchmarks Track\n.html.\nxAI. Realworldqa: A benchmark for real-world spatial understanding, 2024. URL https://huggingfac\ne.co/datasets/xai-org/RealworldQA. Accessed: 2025-04-26.\nGuangxuan Xiao, Ji Lin, Micka¨el Seznec, Hao Wu, Julien Demouth, and Song Han. Smoothquant:\nAccurate and efficient post-training quantization for large language models. In Andreas Krause, Emma\nBrunskill, Kyunghyun Cho, Barbara Engelhardt, Sivan Sabato, and Jonathan Scarlett (eds.), International\nConference on Machine Learning, ICML 2023, 23-29 July 2023, Honolulu, Hawaii, USA, volume 202 of\nProceedings of Machine Learning Research, pp. 38087–38099. PMLR, 2023. URL https://proceedings.\nmlr.press/v202/xiao23c.html.\nSang Michael Xie, Shibani Santurkar, Tengyu Ma, and Percy Liang. Data selection for language models\nvia importance resampling. In Alice Oh, Tristan Naumann, Amir Globerson, Kate Saenko, Moritz\nHardt, and Sergey Levine (eds.), Advances in Neural Information Processing Systems 36: Annual Conference\non Neural Information Processing Systems 2023, NeurIPS 2023, New Orleans, LA, USA, December 10 - 16,\n2023, 2023. URL http://papers.nips.cc/paper files/paper/2023/hash/6b9aa8f418bde2840d5f4\nab7a02f663b-Abstract-Conference.html.\nHaiyang Xu, Qinghao Ye, Xuan Wu, Ming Yan, Yuan Miao, Jiabo Ye, Guohai Xu, Anwen Hu, Yaya Shi,\nGuangwei Xu, Chenliang Li, Qi Qian, Maofei Que, Ji Zhang, Xiao Zeng, and Fei Huang. Youku-mplug:\nA 10 million large-scale chinese video-language dataset for pre-training and benchmarks. CoRR,\nabs/2306.04362, 2023. doi: 10.48550/ARXIV.2306.04362. URL https://doi.org/10.48550/arXiv.2\n306.04362.\nLiang Xu, Hai Hu, Xuanwei Zhang, Lu Li, Chenjie Cao, Yudong Li, Yechen Xu, Kai Sun, Dian Yu,\nCong Yu, Yin Tian, Qianqian Dong, Weitang Liu, Bo Shi, Yiming Cui, Junyi Li, Jun Zeng, Rongzhao\nWang, Weijian Xie, Yanting Li, Yina Patterson, Zuoyu Tian, Yiwen Zhang, He Zhou, Shaoweihua\nLiu, Zhe Zhao, Qipeng Zhao, Cong Yue, Xinrui Zhang, Zhengliang Yang, Kyle Richardson, and\nZhenzhong Lan. CLUE: A chinese language understanding evaluation benchmark. In Donia Scott,\nN´uria Bel, and Chengqing Zong (eds.), Proceedings of the 28th International Conference on Computational\nLinguistics, COLING 2020, Barcelona, Spain (Online), December 8-13, 2020, pp. 4762–4772. International\nCommittee on Computational Linguistics, 2020. doi: 10.18653/V1/2020.COLING-MAIN.419. URL\nhttps://doi.org/10.18653/v1/2020.coling-main.419.\n70\n\nERNIE 4.5 Technical Report\nJune 29, 2025\nAn Yang, Anfeng Li, Baosong Yang, Beichen Zhang, Binyuan Hui, Bo Zheng, Bowen Yu, Chang Gao,\nChengen Huang, Chenxu Lv, Chujie Zheng, Dayiheng Liu, Fan Zhou, Fei Huang, Feng Hu, Hao Ge,\nHaoran Wei, Huan Lin, Jialong Tang, Jian Yang, Jianhong Tu, Jianwei Zhang, Jian Yang, Jiaxi Yang,\nJingren Zhou, Jingren Zhou, Junyang Lin, Kai Dang, Keqin Bao, Kexin Yang, Le Yu, Lianghao Deng,\nMei Li, Mingfeng Xue, Mingze Li, Pei Zhang, Peng Wang, Qin Zhu, Rui Men, Ruize Gao, Shixuan\nLiu, Shuang Luo, Tianhao Li, Tianyi Tang, Wenbiao Yin, Xingzhang Ren, Xinyu Wang, Xinyu Zhang,\nXuancheng Ren, Yang Fan, Yang Su, Yichang Zhang, Yinger Zhang, Yu Wan, Yuqiong Liu, Zekun\nWang, Zeyu Cui, Zhenru Zhang, Zhipeng Zhou, and Zihan Qiu. Qwen3 technical report, 2025a. URL\nhttps://doi.org/10.48550/arXiv.2505.09388.\nZitong Yang, Neil Band, Shuangping Li, Emmanuel J. Cand`es, and Tatsunori Hashimoto. Synthetic\ncontinued pretraining. In The Thirteenth International Conference on Learning Representations, ICLR 2025,\nSingapore, April 24-28, 2025. OpenReview.net, 2025b. URL https://openreview.net/forum?id=07yv\nxWDSla.\nLonghui Yu, Weisen Jiang, Han Shi, Jincheng Yu, Zhengying Liu, Yu Zhang, James T. Kwok, Zhenguo Li,\nAdrian Weller, and Weiyang Liu. Metamath: Bootstrap your own mathematical questions for large\nlanguage models. In The Twelfth International Conference on Learning Representations, ICLR 2024, Vienna,\nAustria, May 7-11, 2024. OpenReview.net, 2024. URL https://openreview.net/forum?id=N8N0hgND\nRt.\nQiying Yu, Zheng Zhang, Ruofei Zhu, Yufeng Yuan, Xiaochen Zuo, Yu Yue, Tiantian Fan, Gaohong Liu,\nLingjun Liu, Xin Liu, Haibin Lin, Zhiqi Lin, Bole Ma, Guangming Sheng, Yuxuan Tong, Chi Zhang,\nMofan Zhang, Wang Zhang, Hang Zhu, Jinhua Zhu, Jiaze Chen, Jiangjie Chen, Chengyi Wang, Hongli\nYu, Weinan Dai, Yuxuan Song, Xiangpeng Wei, Hao Zhou, Jingjing Liu, Wei-Ying Ma, Ya-Qin Zhang,\nLin Yan, Mu Qiao, Yonghui Wu, and Mingxuan Wang. DAPO: an open-source LLM reinforcement\nlearning system at scale. CoRR, abs/2503.14476, 2025. doi: 10.48550/ARXIV.2503.14476. URL\nhttps://doi.org/10.48550/arXiv.2503.14476.\nZhihang Yuan, Lin Niu, Jiawei Liu, Wenyu Liu, Xinggang Wang, Yuzhang Shang, Guangyu Sun, Qiang\nWu, Jiaxiang Wu, and Bingzhe Wu. RPTQ: reorder-based post-training quantization for large language\nmodels. CoRR, abs/2304.01089, 2023. doi: 10.48550/ARXIV.2304.01089. URL https://doi.org/10.4\n8550/arXiv.2304.01089.\nXiang Yue, Yuansheng Ni, Tianyu Zheng, Kai Zhang, Ruoqi Liu, Ge Zhang, Samuel Stevens, Dongfu\nJiang, Weiming Ren, Yuxuan Sun, Cong Wei, Botao Yu, Ruibin Yuan, Renliang Sun, Ming Yin, Boyuan\nZheng, Zhenzhu Yang, Yibo Liu, Wenhao Huang, Huan Sun, Yu Su, and Wenhu Chen. MMMU:\nA massive multi-discipline multimodal understanding and reasoning benchmark for expert AGI.\nIn IEEE/CVF Conference on Computer Vision and Pattern Recognition, CVPR 2024, Seattle, WA, USA,\nJune 16-22, 2024, pp. 9556–9567. IEEE, 2024a. doi: 10.1109/CVPR52733.2024.00913. URL https:\n//doi.org/10.1109/CVPR52733.2024.00913.\nXiang Yue, Tianyu Zheng, Yuansheng Ni, Yubo Wang, Kai Zhang, Shengbang Tong, Yuxuan Sun, Botao\nYu, Ge Zhang, Huan Sun, Yu Su, Wenhu Chen, and Graham Neubig. Mmmu-pro: A more robust\nmulti-discipline multimodal understanding benchmark. CoRR, abs/2409.02813, 2024b. doi: 10.48550\n/ARXIV.2409.02813. URL https://doi.org/10.48550/arXiv.2409.02813.\nChristoph Zauner. Implementation and benchmarking of perceptual image hash functions. 2010. URL\nhttps://api.semanticscholar.org/CorpusID:17075066.\nRowan Zellers, Ari Holtzman, Yonatan Bisk, Ali Farhadi, and Yejin Choi. Hellaswag: Can a machine\nreally finish your sentence? In Anna Korhonen, David R. Traum, and Llu´ıs M`arquez (eds.), Proceedings\nof the 57th Conference of the Association for Computational Linguistics, ACL 2019, Florence, Italy, July 28-\nAugust 2, 2019, Volume 1: Long Papers, pp. 4791–4800. Association for Computational Linguistics, 2019.\ndoi: 10.18653/V1/P19-1472. URL https://doi.org/10.18653/v1/p19-1472.\nRowan Zellers, Ximing Lu, Jack Hessel, Youngjae Yu, Jae Sung Park, Jize Cao, Ali Farhadi, and Yejin Choi.\nMERLOT: multimodal neural script knowledge models. In Marc’Aurelio Ranzato, Alina Beygelzimer,\nYann N. Dauphin, Percy Liang, and Jennifer Wortman Vaughan (eds.), Advances in Neural Information\nProcessing Systems 34: Annual Conference on Neural Information Processing Systems 2021, NeurIPS 2021,\nDecember 6-14, 2021, virtual, pp. 23634–23651, 2021. URL https://proceedings.neurips.cc/paper/2\n021/hash/c6d4eb15f1e84a36eff58eca3627c82e-Abstract.html.\nHui Zeng. Measuring massive multitask chinese understanding. CoRR, abs/2304.12986, 2023. doi:\n10.48550/ARXIV.2304.12986. URL https://doi.org/10.48550/arXiv.2304.12986.\n71\n\nERNIE 4.5 Technical Report\nJune 29, 2025\nXiaohua Zhai, Basil Mustafa, Alexander Kolesnikov, and Lucas Beyer.\nSigmoid loss for language\nimage pre-training. In IEEE/CVF International Conference on Computer Vision, ICCV 2023, Paris, France,\nOctober 1-6, 2023, pp. 11941–11952. IEEE, 2023. doi: 10.1109/ICCV51070.2023.01100. URL https:\n//doi.org/10.1109/ICCV51070.2023.01100.\nChenggang Zhao, Shangyan Zhou, Liyue Zhang, Chengqi Deng, Zhean Xu, Yuxuan Liu, Kuai Yu, Jiashi\nLi, and Liang Zhao. Deepep: an efficient expert-parallel communication library. https://github.com\n/deepseek-ai/DeepEP, 2025.\nWanjun Zhong, Ruixiang Cui, Yiduo Guo, Yaobo Liang, Shuai Lu, Yanlin Wang, Amin Saied, Weizhu\nChen, and Nan Duan. Agieval: A human-centric benchmark for evaluating foundation models. In\nKevin Duh, Helena G´omez-Adorno, and Steven Bethard (eds.), Findings of the Association for Compu-\ntational Linguistics: NAACL 2024, Mexico City, Mexico, June 16-21, 2024, pp. 2299–2314. Association\nfor Computational Linguistics, 2024.\ndoi: 10.18653/V1/2024.FINDINGS-NAACL.149.\nURL\nhttps://doi.org/10.18653/v1/2024.findings-naacl.149.\nJeffrey Zhou, Tianjian Lu, Swaroop Mishra, Siddhartha Brahma, Sujoy Basu, Yi Luan, Denny Zhou, and\nLe Hou. Instruction-following evaluation for large language models. CoRR, abs/2311.07911, 2023. doi:\n10.48550/ARXIV.2311.07911. URL https://doi.org/10.48550/arXiv.2311.07911.\nZirui Liu, Jiayi Yuan, Hongye Jin, Shaochen Zhong, Zhaozhuo Xu, Vladimir Braverman, Beidi Chen, and\nXia Hu. Kivi : Plug-and-play 2bit kv cache quantization with streaming asymmetric quantization. 2023.\ndoi: 10.13140/RG.2.2.28167.37282. URL https://rgdoi.net/10.13140/RG.2.2.28167.37282.\nBarret Zoph, Irwan Bello, Sameer Kumar, Nan Du, Yanping Huang, Jeff Dean, Noam Shazeer, and William\nFedus. St-moe: Designing stable and transferable sparse expert models. arXiv preprint arXiv:2202.08906,\n2022. URL https://arxiv.org/abs/2202.08906.\n72\n",
      "fetch_method": "direct-pdf"
    }
  ]
}