{
  "1-5 (Architecture)": "DeepSeek-V3 is a Transformer-based Mixture-of-Experts (MoE) language model that totals 671 billion parameters, but only 37 billion parameters are activated for any given token. Its design inherits and extends components proven in DeepSeek-V2: it integrates Multi-head Latent Attention (MLA) together with the custom DeepSeekMoE routing scheme, adds extra RMSNorm layers after compressed latent vectors, and applies additional scaling factors at width bottlenecks. The network is configured with 61 Transformer layers and a hidden dimension of 7 168. To improve routing efficiency it introduces an auxiliary-loss-free load-balancing strategy, and to boost predictive power it replaces standard next-token prediction with a multi-token objective that simultaneously forecasts the next two tokens (MTP). The model was trained on 14.8 trillion tokens. Altogether, these choices deliver both high performance and significantly lower training/inference cost relative to dense models of comparable capability.",
  "1-6 (Tokenizer)": "DeepSeek-V3 uses a Byte-level BPE tokenizer (Shibata et al., 1999) that has been enlarged to a 128 000-token vocabulary. No other tokenizer variants are reported for this model.",
  "2-1 (Hardware)": "Training was carried out on a cluster containing 2 048 NVIDIA H800 GPUs. With this hardware, processing one trillion tokens takes about 180 000 H800 GPU-hours—equivalent to roughly 3.7 days on the full cluster—and end-to-end pre-training of the 14.8 T-token corpus consumed 2.788 million H800 GPU-hours in total.",
  "2-2 (Software)": "Model training leverages the in-house HAI-LLM framework. Parallelism is arranged as 16-way Pipeline Parallelism, 64-way Expert Parallelism across 8 nodes, and ZeRO-1 Data Parallelism. The team enabled cost efficiency through FP8 training and other engineering optimizations. After core pre-training, YaRN is applied in two 1 000-step phases to expand the context window from 4 K → 32 K → 128 K tokens. Optimisation techniques include Group Relative Policy Optimization (GRPO) for instruction tuning, and a cosine-decay learning-rate schedule (5 × 10⁻⁶ down to 1 × 10⁻⁶) during two-epoch supervised fine-tuning of the DeepSeek-V3-Base variant.",
  "1-5 (Architecture)__evidence": [
    {
      "source": "[abstract]",
      "quote": "We present DeepSeek-V3, a strong Mixture-of-Experts (MoE) language model with 671B total parameters with 37B activated for each token."
    },
    {
      "source": "[abstract]",
      "quote": "To achieve efficient inference and cost-effective training, DeepSeek-V3 adopts Multi-head Latent Attention (MLA) and DeepSeekMoE architectures, which were thoroughly validated in DeepSeek-V2."
    },
    {
      "source": "[sections/Introduction]",
      "quote": "To further push the boundaries of open-source model capabilities, we scale up our models and introduce DeepSeek-V3, a large Mixture-of-Experts (MoE) model with 671B parameters, of which 37B are activated for each token."
    },
    {
      "source": "[sections/Architecture]",
      "quote": "The basic architecture of DeepSeek-V3 is still within the Transformer framework. For efficient inference and economical training, DeepSeek-V3 also adopts MLA and DeepSeekMoE, which have been thoroughly validated by DeepSeek-V2."
    },
    {
      "source": "[sections/4.2 Hyper-Parameters]",
      "quote": "As DeepSeek-V2, DeepSeek-V3 also employs additional RMSNorm layers after the compressed latent vectors, and multiplies additional scaling factors at the width bottlenecks. Under this configuration, DeepSeek-V3 comprises 671B total parameters, of which 37B are activated for each token."
    },
    {
      "source": "[sections/Conclusion]",
      "quote": "In this paper, we introduce DeepSeek-V3, a large MoE language model with 671B total parameters and 37B activated parameters, trained on 14.8T tokens."
    },
    {
      "source": "[abstract]",
      "quote": "Furthermore, DeepSeek-V3 pioneers an auxiliary-loss-free strategy for load balancing and sets a multi-token prediction training objective for stronger performance."
    },
    {
      "source": "[sections/Architecture]",
      "quote": "Compared with DeepSeek-V2, an exception is that we additionally introduce an auxiliary-loss-free load balancing strategy for DeepSeekMoE to mitigate the performance degradation induced by the effort to ensure load balance."
    },
    {
      "source": "[sections/Hyper-Parameters]",
      "quote": "We set the number of Transformer layers to 61 and the hidden dimension to 7168. Under this configuration, DeepSeek-V3 comprises 671B total parameters, of which 37B are activated for each token."
    },
    {
      "source": "[sections/Hyper-Parameters]",
      "quote": "As DeepSeek-V2, DeepSeek-V3 also employs additional RMSNorm layers after the compressed latent vectors, and multiplies additional scaling factors at the width bottlenecks."
    },
    {
      "source": "[sections/Conclusion]",
      "quote": "In this paper, we introduce DeepSeek-V3, a large MoE language model with 671B total parameters and 37B activated parameters, trained on 14.8T tokens. In addition to the MLA and DeepSeekMoE architectures, it also pioneers an auxiliary-loss-free strategy for load balancing and sets a multi-token prediction training objective for stronger performance."
    },
    {
      "source": "[sections/5.4.3 Multi-Token Prediction Evaluation]",
      "quote": "Instead of predicting just the next single token, DeepSeek-V3 predicts the next 2 tokens through the MTP technique."
    }
  ],
  "1-6 (Tokenizer)__evidence": [
    {
      "source": "[sections/4.1 Data Construction]",
      "quote": "The tokenizer for DeepSeek-V3 employs Byte-level BPE (Shibata et al., 1999) with an extended vocabulary of 128K tokens."
    },
    {
      "source": "[sections/Data Construction]",
      "quote": "The tokenizer for DeepSeek-V3 employs Byte-level BPE (Shibata et al., 1999) with an extended vocabulary of 128K tokens."
    }
  ],
  "2-1 (Hardware)__evidence": [
    {
      "source": "[sections/Compute Clusters]",
      "quote": "DeepSeek-V3 is trained on a cluster equipped with 2048 NVIDIA H800 GPUs."
    },
    {
      "source": "[sections/Introduction]",
      "quote": "During the pre-training stage, training DeepSeek-V3 on each trillion tokens requires only 180K H800 GPU hours, i.e., 3.7 days on our cluster with 2048 H800 GPUs."
    },
    {
      "source": "[abstract]",
      "quote": "Despite its excellent performance, DeepSeek-V3 requires only 2.788M H800 GPU hours for its full training."
    },
    {
      "source": "[sections/4.4.2 Evaluation Results]",
      "quote": "Due to our efficient architectures and comprehensive engineering optimizations, DeepSeek-V3 achieves extremely high training efficiency. Under our training framework and infrastructures, training DeepSeek-V3 on each trillion tokens requires only 180K H800 GPU hours, which is much cheaper than training 72B or 405B dense models."
    }
  ],
  "2-2 (Software)__evidence": [
    {
      "source": "[sections/Training Framework]",
      "quote": "The training of DeepSeek-V3 is supported by the HAI-LLM framework, an efficient and lightweight training framework crafted by our engineers from the ground up."
    },
    {
      "source": "[sections/Training Framework]",
      "quote": "On the whole, DeepSeek-V3 applies 16-way Pipeline Parallelism (PP), 64-way Expert Parallelism (EP) spanning 8 nodes, and ZeRO-1 Data Parallelism (DP)."
    },
    {
      "source": "[sections/4.3 Long Context Extension]",
      "quote": "We adopt a similar approach to DeepSeek-V2 (DeepSeek-AI, 2024c) to enable long context capabilities in DeepSeek-V3. After the pre-training stage, we apply YaRN (Peng et al., 2023a) for context extension and perform two additional training phases, each comprising 1000 steps, to progressively expand the context window from 4K to 32K and then to 128K."
    },
    {
      "source": "[sections/Training Framework]",
      "quote": "The training of DeepSeek-V3 is supported by the HAI-LLM framework, an efficient and lightweight training framework crafted by our engineers from the ground up. On the whole, DeepSeek-V3 applies 16-way Pipeline Parallelism (PP) (Qi et al., 2023a), 64-way Expert Parallelism (EP) (Lepikhin et al., 2021) spanning 8 nodes, and ZeRO-1 Data Parallelism (DP) (Rajbhandari et al., 2020)."
    },
    {
      "source": "[sections/Conclusion]",
      "quote": "The training of DeepSeek-V3 is cost-effective due to the support of FP8 training and meticulous engineering optimizations."
    },
    {
      "source": "[sections/5.2.2 Group Relative Policy Optimization]",
      "quote": "Similar to DeepSeek-V2 (DeepSeek-AI, 2024c), we adopt Group Relative Policy Optimization (GRPO) (Shao et al., 2024), which foregoes the critic model that is typically with the same size as the policy model, and estimates the baseline from group scores instead."
    },
    {
      "source": "[sections/5.1 SFT Settings]",
      "quote": "We fine-tune DeepSeek-V3-Base for two epochs using the SFT dataset, using the cosine decay learning rate scheduling that starts at 5 × 10−6 and gradually decreases to 1 × 10−6."
    }
  ]
}