{
  "1-5 (Architecture)": "DeepSeek-V3 is described as a very large Mixture-of-Experts (MoE) Transformer-family language model that contains 671 billion total parameters, while only 37 billion parameters are actually activated for every input token. The model keeps the overall Transformer backbone but replaces standard sub-components with two in-house designs: Multi-head Latent Attention (MLA) for the attention stack and DeepSeekMoE for the feed-forward (FFN) parts. MLA reduces inference cost by using a single KV head, and DeepSeekMoE lowers training cost through expert routing. Compared with the earlier V2 generation, V3 keeps MLA and DeepSeekMoE but adds (1) an auxiliary-loss-free load-balancing strategy to avoid the quality drop normally induced by MoE load balancing and (2) a multi-token-prediction (MTP) objective that predicts the next two tokens instead of only one, improving benchmark performance. Additional RMSNorm layers are placed after compressed latent vectors, and width-bottleneck scaling factors are applied just as in DeepSeek-V2. Internally, the gating function for expert selection now uses a sigmoid to compute affinity scores, followed by normalization over the selected scores, slightly diverging from V2’s formulation. Overall, the design choices—Transformer backbone, MLA attention, DeepSeekMoE FFNs, single-KV-head attention, auxiliary-loss-free balancing, and MTP objective—collectively target efficient inference, cheaper MoE training, and stronger downstream accuracy while keeping the active parameter count to 37 B per token.",
  "1-6 (Tokenizer)": "The model is tokenised with a Byte-level BPE scheme and uses an enlarged 128 K-token vocabulary specifically built for DeepSeek-V3.",
  "2-1 (Hardware)": "Training is carried out exclusively on NVIDIA H800 GPUs. The authors report a cluster of 2 048 H800 cards, organised as 256 nodes with 8 GPUs each. Within a node the eight H800s are connected through NVLink and NVSwitch, and cross-node traffic is fully interconnected by InfiniBand (IB). Efficiency numbers are provided in GPU-hours: pre-training consumes 180 K H800 GPU-hours per trillion tokens, which on the 2 048-GPU cluster translates to roughly 3.7 days for each trillion tokens. Adding 119 K GPU-hours for context-length extension and 5 K GPU-hours for post-training, the complete training run costs 2.788 million H800 GPU-hours—highlighted by the authors as being cheaper than training 72 B or 405 B dense counterparts.",
  "2-2 (Software)": "DeepSeek-V3 is trained with an in-house, lightweight framework called HAI-LLM. The distributed strategy combines several parallelism modes: 16-way Pipeline Parallelism, 64-way Expert Parallelism spread across 8 nodes, and ZeRO-1 Data Parallelism. To accelerate the pipeline stage scheduling the engineers created a DualPipe algorithm. FP8 arithmetic support is integrated to cut memory use and FLOP cost, contributing to the claimed cost-effectiveness. Hyper-parameter scheduling during supervised fine-tuning (SFT) is documented: two epochs on the SFT set with a cosine-decay learning rate that starts at 5×10⁻⁶ and anneals to 1×10⁻⁶. In summary, the software stack couples the custom HAI-LLM framework, FP8 precision, DualPipe-optimised 16-PP / 64-EP / ZeRO-1-DP topology, and standard LR scheduling to realise scalable, economical MoE training.",
  "1-5 (Architecture)__evidence": [
    {
      "source": "[pdf_text]",
      "quote": "We present DeepSeek-V3, a strong Mixture-of-Experts (MoE) language model with 671B total parameters with 37B activated for each token."
    },
    {
      "source": "[pdf_text]",
      "quote": "To achieve efficient inference and cost-effective training, DeepSeek-V3 adopts Multi-head Latent Attention (MLA) and DeepSeekMoE architectures, which were thoroughly validated in DeepSeek-V2."
    },
    {
      "source": "[pdf_text]",
      "quote": "Therefore, in terms of architecture, DeepSeek-V3 still adopts Multi-head Latent Attention (MLA) (DeepSeek-AI, 2024c) for efficient inference and DeepSeekMoE (Dai et al., 2024) for cost-effective training."
    },
    {
      "source": "[pdf_text]",
      "quote": "Secondly, DeepSeek-V3 employs a multi-token prediction training objective, which we have observed to enhance the overall performance on evaluation benchmarks."
    },
    {
      "source": "[pdf_text]",
      "quote": "The basic architecture of DeepSeek-V3 is still within the Transformer (Vaswani et al., 2017) framework."
    },
    {
      "source": "[pdf_text]",
      "quote": "Compared with DeepSeek-V2, an exception is that we additionally introduce an auxiliary-loss-free load balancing strategy (Wang et al., 2024a) for DeepSeekMoE to mitigate the performance degradation induced by the effort to ensure load balance."
    },
    {
      "source": "[pdf_text]",
      "quote": "For attention, DeepSeek-V3 adopts the MLA architecture."
    },
    {
      "source": "[pdf_text]",
      "quote": "For Feed-Forward Networks (FFNs), DeepSeek-V3 employs the DeepSeekMoE architecture (Dai et al., 2024)."
    },
    {
      "source": "[pdf_text]",
      "quote": "Slightly different from DeepSeek-V2, DeepSeek-V3 uses the sigmoid function to compute the affinity scores, and applies a normalization among all selected affinity scores to produce the gating values."
    },
    {
      "source": "[pdf_text]",
      "quote": "Under this configuration, DeepSeek-V3 comprises 671B total parameters, of which 37B are activated for each token."
    },
    {
      "source": "[pdf_text]",
      "quote": "In this paper, we introduce DeepSeek-V3, a large MoE language model with 671B total pa-\nrameters and 37B activated parameters, trained on 14.8T tokens."
    },
    {
      "source": "[pdf_text]",
      "quote": "In addition to the MLA and\nDeepSeekMoE architectures, it also pioneers an auxiliary-loss-free strategy for load balancing\nand sets a multi-token prediction training objective for stronger performance."
    },
    {
      "source": "[pdf_text]",
      "quote": "Instead of predicting just the next single token, DeepSeek-V3 predicts the next 2 tokens through\nthe MTP technique."
    },
    {
      "source": "[title:SGLang v0.3 Release: 7x Faster DeepSeek MLA, 1.5x Faster torch.compile, Multi-Image/Video LLaVA-OneVision | LMSYS Org]",
      "quote": "Multi-head Latent Attention (MLA) is a new attention variant introduced by the DeepSeek team to improve inference efficiency."
    },
    {
      "source": "[title:SGLang v0.4: Zero-Overhead Batch Scheduler, Cache-Aware Load Balancer, Faster Structured Outputs | LMSYS Org]",
      "quote": "For example, DeepSeek models use MLA and only have one KV head."
    },
    {
      "source": "[sections/https://arxiv.org/pdf/2412.19437]",
      "quote": "We present DeepSeek-V3, a strong Mixture-of-Experts (MoE) language model with 671B total parameters with 37B activated for each token."
    },
    {
      "source": "[sections/https://arxiv.org/pdf/2412.19437]",
      "quote": "To achieve efficient inference and cost-effective training, DeepSeek-V3 adopts Multi-head Latent Attention (MLA) and DeepSeekMoE architectures, which were thoroughly validated in DeepSeek-V2."
    },
    {
      "source": "[sections/https://arxiv.org/pdf/2412.19437]",
      "quote": "Therefore, in terms of architecture, DeepSeek-V3 still adopts Multi-head Latent Attention (MLA) (DeepSeek-AI, 2024c) for efficient inference and DeepSeekMoE (Dai et al., 2024) for cost-effective training."
    },
    {
      "source": "[sections/https://arxiv.org/pdf/2412.19437]",
      "quote": "The basic architecture of DeepSeek-V3 is still within the Transformer (Vaswani et al., 2017) framework."
    },
    {
      "source": "[sections/https://arxiv.org/pdf/2412.19437]",
      "quote": "For attention, DeepSeek-V3 adopts the MLA architecture."
    },
    {
      "source": "[sections/https://arxiv.org/pdf/2412.19437]",
      "quote": "For Feed-Forward Networks (FFNs), DeepSeek-V3 employs the DeepSeekMoE architecture (Dai et al., 2024)."
    },
    {
      "source": "[pdf_text]",
      "quote": "As DeepSeek-V2, DeepSeek-V3 also employs additional RMSNorm layers after the compressed latent vectors, and multiplies additional scaling factors at the width bottlenecks."
    },
    {
      "source": "[pdf_text]",
      "quote": "In this paper, we introduce DeepSeek-V3, a large MoE language model with 671B total parameters and 37B activated parameters, trained on 14.8T tokens."
    },
    {
      "source": "[pdf_text]",
      "quote": "In addition to the MLA and DeepSeekMoE architectures, it also pioneers an auxiliary-loss-free strategy for load balancing and sets a multi-token prediction training objective for stronger performance."
    },
    {
      "source": "[pdf_text]",
      "quote": "Instead of predicting just the next single token, DeepSeek-V3 predicts the next 2 tokens through the MTP technique."
    },
    {
      "source": "[sections/https://lmsys.org/blog/2024-12-04-sglang-v0-4/#data-parallelism-attention-for-deepseek-models]",
      "quote": "For example, DeepSeek models use MLA and only have one KV head."
    }
  ],
  "1-6 (Tokenizer)__evidence": [
    {
      "source": "[pdf_text]",
      "quote": "The tokenizer for DeepSeek-V3 employs Byte-level BPE (Shibata et al., 1999) with an extended vocabulary of 128K tokens."
    }
  ],
  "2-1 (Hardware)__evidence": [
    {
      "source": "[pdf_text]",
      "quote": "Despite its excellent performance, DeepSeek-V3 requires only 2.788M H800 GPU hours for its full training."
    },
    {
      "source": "[pdf_text]",
      "quote": "During the pre-training stage, training DeepSeek-V3 on each trillion tokens requires only 180K H800 GPU hours, i.e., 3.7 days on our cluster with 2048 H800 GPUs."
    },
    {
      "source": "[pdf_text]",
      "quote": "Combined with 119K GPU hours for the context length extension and 5K GPU hours for post-training, DeepSeek-V3 costs only 2.788M GPU hours for its full training."
    },
    {
      "source": "[pdf_text]",
      "quote": "DeepSeek-V3 is trained on a cluster equipped with 2048 NVIDIA H800 GPUs. Each node in the H800 cluster contains 8 GPUs connected by NVLink and NVSwitch within nodes."
    },
    {
      "source": "[pdf_text]",
      "quote": "Under our training framework and infrastruc-\ntures, training DeepSeek-V3 on each trillion tokens requires only 180K H800 GPU hours, which\nis much cheaper than training 72B or 405B dense models."
    },
    {
      "source": "[sections/https://arxiv.org/pdf/2412.19437]",
      "quote": "DeepSeek-V3 is trained on a cluster equipped with 2048 NVIDIA H800 GPUs. Each node in the H800 cluster contains 8 GPUs connected by NVLink and NVSwitch within nodes."
    },
    {
      "source": "[sections/https://arxiv.org/pdf/2412.19437]",
      "quote": "During the pre-training stage, training DeepSeek-V3 on each trillion tokens requires only 180K H800 GPU hours, i.e., 3.7 days on our cluster with 2048 H800 GPUs."
    },
    {
      "source": "[pdf_text]",
      "quote": "We deploy DeepSeek-V3 on the H800 cluster, where GPUs within each node are interconnected using NVLink, and all GPUs across the cluster are fully interconnected via IB."
    },
    {
      "source": "[pdf_text]",
      "quote": "Under our training framework and infrastructures, training DeepSeek-V3 on each trillion tokens requires only 180K H800 GPU hours, which is much cheaper than training 72B or 405B dense models."
    }
  ],
  "2-2 (Software)__evidence": [
    {
      "source": "[pdf_text]",
      "quote": "In the remainder of this paper, we first present a detailed exposition of our DeepSeek-V3 model architecture (Section 2). Subsequently, we introduce our infrastructures, encompassing our compute clusters, the training framework, the support for FP8 training, the inference deployment strategy, and our suggestions on future hardware design."
    },
    {
      "source": "[pdf_text]",
      "quote": "The training of DeepSeek-V3 is supported by the HAI-LLM framework, an efficient and lightweight training framework crafted by our engineers from the ground up."
    },
    {
      "source": "[pdf_text]",
      "quote": "On the whole, DeepSeek-V3 applies 16-way Pipeline Parallelism (PP) (Qi et al., 2023a), 64-way Expert Parallelism (EP) (Lepikhin et al., 2021) spanning 8 nodes, and ZeRO-1 Data Parallelism (DP) (Rajbhandari et al., 2020)."
    },
    {
      "source": "[pdf_text]",
      "quote": "We fine-tune DeepSeek-V3-Base for two epochs using the SFT dataset, using the cosine decay learning rate scheduling that starts at 5 × 10−6 and gradually decreases to 1 × 10−6."
    },
    {
      "source": "[sections/https://arxiv.org/pdf/2412.19437]",
      "quote": "The training of DeepSeek-V3 is supported by the HAI-LLM framework, an efficient and lightweight training framework crafted by our engineers from the ground up."
    },
    {
      "source": "[sections/https://arxiv.org/pdf/2412.19437]",
      "quote": "On the whole, DeepSeek-V3 applies 16-way Pipeline Parallelism (PP) (Qi et al., 2023a), 64-way Expert Parallelism (EP) (Lepikhin et al., 2021) spanning 8 nodes, and ZeRO-1 Data Parallelism (DP) (Rajbhandari et al., 2020)."
    },
    {
      "source": "[sections/https://arxiv.org/pdf/2412.19437]",
      "quote": "In order to facilitate efficient training of DeepSeek-V3, we implement meticulous engineering optimizations. Firstly, we design the DualPipe algorithm for efficient pipeline parallelism."
    },
    {
      "source": "[pdf_text]",
      "quote": "The training of DeepSeek-V3 is cost-effective due to the support of FP8 training and meticulous engineering optimizations."
    }
  ]
}