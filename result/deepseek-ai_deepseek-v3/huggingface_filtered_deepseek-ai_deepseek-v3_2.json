{
  "1-5 (Architecture)": "DeepSeek-V3 is explicitly described as “a strong Mixture-of-Experts (MoE) language model with 671B total parameters with 37B activated for each token.”  The authors explain that, to keep both inference and training economical, “DeepSeek-V3 adopts Multi-head Latent Attention (MLA) and DeepSeekMoE architectures, which were thoroughly validated in DeepSeek-V2.”  The open-source code base exposes “the configuration class to store the configuration of a [DeepseekV3Model] … defining the model architecture,” and they add that “instantiating a configuration with the defaults will yield a similar configuration to that of the DeepSeek-V3.”  A file header notes it is a “PyTorch DeepSeek model,” confirming the implementation framework.  Several custom building blocks are called out: “DeepseekV3RMSNorm is equivalent to T5LayerNorm,” and there are two enhanced positional-encoding modules—“DeepseekV3RotaryEmbedding extended with linear scaling” and “DeepseekV3RotaryEmbedding extended with Dynamic NTK scaling.”  Taken together, the quotes portray DeepSeek-V3 as a 671-billion-parameter MoE transformer that activates 37 B parameters per token, employs MLA attention and the DeepSeekMoE routing strategy, relies on an RMSNorm variant, uses advanced rotary embeddings with linear and NTK scaling, and is fully configurable via an officially provided PyTorch configuration class.",
  "1-5 (Architecture)__evidence": [
    {
      "source": "[readme]",
      "quote": "We present DeepSeek-V3, a strong Mixture-of-Experts (MoE) language model with 671B total parameters with 37B activated for each token."
    },
    {
      "source": "[readme]",
      "quote": "To achieve efficient inference and cost-effective training, DeepSeek-V3 adopts Multi-head Latent Attention (MLA) and DeepSeekMoE architectures, which were thoroughly validated in DeepSeek-V2."
    },
    {
      "source": "[py_files/configuration_deepseek.py]",
      "quote": "This is the configuration class to store the configuration of a [`DeepseekV3Model`]. It is used to instantiate an DeepSeek model according to the specified arguments, defining the model architecture."
    },
    {
      "source": "[py_files/configuration_deepseek.py]",
      "quote": "Instantiating a configuration with the defaults will yield a similar configuration to that of the DeepSeek-V3."
    },
    {
      "source": "[py_files/modeling_deepseek.py]",
      "quote": "\"\"\" PyTorch DeepSeek model.\"\"\""
    },
    {
      "source": "[py_files/modeling_deepseek.py]",
      "quote": "\"\"\"\n        DeepseekV3RMSNorm is equivalent to T5LayerNorm\n        \"\"\""
    },
    {
      "source": "[py_files/modeling_deepseek.py]",
      "quote": "\"\"\"DeepseekV3RotaryEmbedding extended with linear scaling. Credits to the Reddit user /u/kaiokendev\"\"\""
    },
    {
      "source": "[py_files/modeling_deepseek.py]",
      "quote": "\"\"\"DeepseekV3RotaryEmbedding extended with Dynamic NTK scaling. Credits to the Reddit users /u/bloc97 and /u/emozilla\"\"\""
    }
  ],
  "1-6 (Tokenizer)": "",
  "1-6 (Tokenizer)__evidence": [],
  "2-1 (Hardware)": "Two concrete statements quantify the compute budget: “DeepSeek-V3 requires only 2.788M H800 GPU hours for its full training,” and “at an economical cost of only 2.664M H800 GPU hours, we complete the pre-training of DeepSeek-V3 on 14.8T tokens.”  These figures show that the model was trained on H800 GPUs and that roughly 2.7–2.8 million GPU-hours were consumed in total, with 2.664 million of those hours devoted specifically to pre-training on a 14.8-trillion-token corpus.",
  "2-1 (Hardware)__evidence": [
    {
      "source": "[readme]",
      "quote": "Despite its excellent performance, DeepSeek-V3 requires only 2.788M H800 GPU hours for its full training."
    },
    {
      "source": "[readme]",
      "quote": "- At an economical cost of only 2.664M H800 GPU hours, we complete the pre-training of DeepSeek-V3 on 14.8T tokens, producing the currently strongest open-source base model."
    }
  ],
  "2-2 (Software)": "",
  "2-2 (Software)__evidence": []
}