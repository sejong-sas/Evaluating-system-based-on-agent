{
  "2-3 (API)": "The project states that \"The model checkpoints are available at https://github.com/deepseek-ai/DeepSeek-V3,\" confirming that the DeepSeek-V3 weights can be freely downloaded.  Several command-line examples illustrate how DeepSeek models are exposed through an OpenAI-compatible HTTP endpoint.  One line starts an sglang service:  `python3 -m sglang.launch_server --model deepseek-ai/DeepSeek-Coder-V2-Lite-Instruct --enable-mla --disable-radix --trust-remote-code`, while others rely on vLLM’s `openai.api_server`, e.g.  `python3 -m vllm.entrypoints.openai.api_server --model deepseek-ai/DeepSeek-Coder-V2-Instruct --disable-log-requests --tensor-parallel-size 8 --trust-remote-code --max-model-len 4096` and a similar command that adds `--quantization fp8`.  Although the options are shown for earlier DeepSeek-Coder-V2 variants, they demonstrate the typical pattern for spinning up an OpenAI-style REST service around a DeepSeek checkpoint, including flags for tensor parallelism, FP8 quantization, request logging, maximum context length, and trusting remote code.  Taken together, the quotes indicate that DeepSeek-V3 is publicly downloadable and can be served through standard OpenAI-compatible API servers or the sglang launcher with only a single shell command.",
  "3-1 (Pre-training)": "DeepSeek-V3 is introduced as \"a large MoE language model with 671 B total parameters, 37 B of which are activated per token,\" trained from scratch on \"14.8 trillion diverse and high-quality tokens.\"  The tokenizer is a Byte-level BPE with a 128 K vocabulary.  Compared with DeepSeek-V2, the corpus increases the proportion of mathematics and programming data and broadens multilingual coverage beyond English and Chinese; the Fill-in-the-Middle (FIM) strategy is also retained.  The pre-training pipeline is highly cost–efficient: each trillion tokens consumes \"only 180 K H800 GPU hours (≈3.7 days on 2 048 H800 GPUs),\" and the entire base model pre-train finishes for \"2.664 M H800 GPU hours.\"  FP8 training and extensive engineering optimizations drive the low cost.  After the base run, a two-stage context-length extension is performed—first to 32 K tokens and then to 128 K—via two additional training phases, enabling 128 K-token inputs while preserving quality.  Under the same framework, pre-training plus context-length extension and post-training sum to \"2.788 M H800 GPU hours.\"  Architectural novelties include the MLA and DeepSeekMoE designs, an auxiliary-loss-free load-balancing strategy, and a multi-token-prediction objective.  Overall, evaluations show DeepSeek-V3 as \"the strongest open-source base model\" with performance approaching GPT-4o and Claude-3.5-Sonnet, yet trained at markedly lower hardware cost.",
  "3-2 (Fine-tuning)": "After pre-training, the model undergoes \"post-training, including Supervised Fine-Tuning (SFT) and Reinforcement Learning (RL)\" to align it with human preferences.  The authors \"fine-tune DeepSeek-V3-Base for two epochs\" on an SFT corpus using a cosine-decay schedule that starts at 5 × 10⁻⁶ and anneals to 1 × 10⁻⁶.  Long-context capability is added by first running YaRN to stretch the window from 4 K to 32 K tokens and then to 128 K, with \"two additional training phases, each comprising 1 000 steps.\"  Figure-8 (referenced) shows that, \"following supervised fine-tuning, [the model] achieves notable performance on the ‘Needle In A HayStack’ test\" and remains robust across the 128 K context.  A separate \"innovative methodology\" distills the reasoning skills of a long Chain-of-Thought DeepSeek-R1 model into DeepSeek-V3, and the post-training successfully \"distill[s] the reasoning capability\" while balancing accuracy and compute.  All of these procedures leverage the same FP8/engineering stack that kept pre-training inexpensive.",
  "3-3 (Reinforcement Learning)": "Reinforcement Learning is the final alignment stage: \"We pre-train DeepSeek-V3 … followed by Supervised Fine-Tuning and Reinforcement Learning\" and then \"conduct post-training … RL on the base model … to align it with human preferences.\"  The reward model itself \"is trained from the DeepSeek-V3 SFT checkpoints.\"  For policy optimization the team \"adopt[s] Group Relative Policy Optimization (GRPO),\" which discards a full-size critic and instead uses baseline estimates from group scores, mirroring the method used in DeepSeek-V2.  To produce richer feedback, the project employs \"the constitutional AI approach,\" letting DeepSeek-V3’s own voting evaluations act as an additional reward signal, particularly for long-context tasks.  Collectively, these steps finish the human-preference alignment process without the computational overhead of a same-sized critic model.",
  "2-3 (API)__evidence": [
    {
      "source": "[title:SGLang v0.3 Release: 7x Faster DeepSeek MLA, 1.5x Faster torch.compile, Multi-Image/Video LLaVA-OneVision | LMSYS Org]",
      "quote": "python3 - m sglang . launch_server --model deepseek-ai / DeepSeek-Coder-V2-Lite-Instruct --enable-mla --disable-radix --trust-remote-code"
    },
    {
      "source": "[title:SGLang v0.3 Release: 7x Faster DeepSeek MLA, 1.5x Faster torch.compile, Multi-Image/Video LLaVA-OneVision | LMSYS Org]",
      "quote": "python3 - m vllm . entrypoints . openai . api_server --model deepseek-ai / DeepSeek-Coder-V2-Lite-Instruct --disable-log-requests --trust-remote-code --max-model-len 4096"
    },
    {
      "source": "[pdf_text]",
      "quote": "The model checkpoints are available at https://github.com/deepseek-ai/DeepSeek-V3."
    },
    {
      "source": "[pdf_text]",
      "quote": "python3 - m vllm . entrypoints . openai . api_server --model deepseek-ai / DeepSeek-Coder-V2-Instruct --disable-log-requests --tensor-parallel-size 8 --trust-remote-code --max-model-len 4096"
    },
    {
      "source": "[pdf_text]",
      "quote": "python3 - m vllm . entrypoints . openai . api_server --model neuralmagic / DeepSeek-Coder-V2-Instruct-FP8 --quantization fp8 --disable-log-requests --tensor-parallel-size 8 --trust-remote-code --max-model-len 4096"
    }
  ],
  "3-1 (Pre-training)__evidence": [
    {
      "source": "[pdf_text]",
      "quote": "We pre-train DeepSeek-V3 on 14.8 trillion diverse and high-quality tokens, followed by Supervised Fine-Tuning and Reinforcement Learning stages to fully harness its capabilities."
    },
    {
      "source": "[pdf_text]",
      "quote": "During pre-training, we train DeepSeek-V3 on 14.8T high-quality and diverse tokens."
    },
    {
      "source": "[pdf_text]",
      "quote": "At an economical cost of only 2.664M H800 GPU hours, we complete the pre-training of DeepSeek-V3 on 14.8T tokens, producing the currently strongest open-source base model."
    },
    {
      "source": "[pdf_text]",
      "quote": "During the pre-training stage, training DeepSeek-V3 on each trillion tokens requires only 180K H800 GPU hours, i.e., 3.7 days on our cluster with 2048 H800 GPUs."
    },
    {
      "source": "[pdf_text]",
      "quote": "Compared with DeepSeek-V2, we optimize the pre-training corpus by enhancing the ratio of mathematical and programming samples, while expanding multilingual coverage beyond English and Chinese."
    },
    {
      "source": "[pdf_text]",
      "quote": "Finally, the training corpus for DeepSeek-V3 consists of 14.8T high-quality and diverse tokens in our tokenizer."
    },
    {
      "source": "[pdf_text]",
      "quote": "In alignment with DeepSeekCoder-V2, we also incorporate the FIM strategy in the pre-training of DeepSeek-V3."
    },
    {
      "source": "[pdf_text]",
      "quote": "Under this configuration, DeepSeek-V3 comprises 671B total parameters, of which 37B are activated for each token."
    },
    {
      "source": "[pdf_text]",
      "quote": "Through this two-phase extension training, DeepSeek-V3 is capable of handling inputs up to 128K in length while maintaining strong performance."
    },
    {
      "source": "[pdf_text]",
      "quote": "In this paper, we introduce DeepSeek-V3, a large MoE language model with 671B total parameters and 37B activated parameters, trained on 14.8T tokens."
    },
    {
      "source": "[pdf_text]",
      "quote": "Under our training framework and infrastructures, training DeepSeek-V3 on each trillion tokens requires only 180K H800 GPU hours, which is much cheaper than training 72B or 405B dense models."
    },
    {
      "source": "[pdf_text]",
      "quote": "Comprehensive evaluations demonstrate that DeepSeek-V3 has emerged as the strongest open-source model currently available, and achieves performance comparable to leading closed-source models like GPT-4o and Claude-3.5-Sonnet. Despite its strong performance, it also maintains economical training costs. It requires only 2.788M H800 GPU hours for its full training, including pre-training, context length extension, and post-training."
    },
    {
      "source": "[pdf_text]",
      "quote": "Next, we conduct a two-stage context length extension for DeepSeek-V3. In the first stage, the maximum context length is extended to 32K, and in the second stage, it is further extended to 128K."
    },
    {
      "source": "[pdf_text]",
      "quote": "• At an economical cost of only 2.664M H800 GPU hours, we complete the pre-training of DeepSeek-V3 on 14.8T tokens, producing the currently strongest open-source base model."
    },
    {
      "source": "[pdf_text]",
      "quote": "The tokenizer for DeepSeek-V3 employs Byte-level BPE (Shibata et al., 1999) with an extended vocabulary of 128K tokens."
    },
    {
      "source": "[pdf_text]",
      "quote": "The training of DeepSeek-V3 is cost-effective due to the support of FP8 training and meticulous engineering optimizations."
    },
    {
      "source": "[pdf_text]",
      "quote": "In this paper, we introduce DeepSeek-V3, a large MoE language model with 671B total parameters and 37B activated parameters, trained on 14.8T tokens. In addition to the MLA and DeepSeekMoE architectures, it also pioneers an auxiliary-loss-free strategy for load balancing and sets a multi-token prediction training objective for stronger performance."
    },
    {
      "source": "[pdf_text]",
      "quote": "Comprehensive evaluations demonstrate that DeepSeek-V3 has emerged as the strongest open-source model currently available, and achieves performance comparable to leading closed-source models like GPT-4o and Claude-3.5-Sonnet. It requires only 2.788M H800 GPU hours for its full training, including pre-training, context length extension, and post-training."
    }
  ],
  "3-2 (Fine-tuning)__evidence": [
    {
      "source": "[pdf_text]",
      "quote": "Following this, we conduct post-training, including Supervised Fine-Tuning (SFT) and Reinforcement Learning (RL) on the base model of DeepSeek-V3, to align it with human preferences and further unlock its potential."
    },
    {
      "source": "[pdf_text]",
      "quote": "• We introduce an innovative methodology to distill reasoning capabilities from the long-Chain-of-Thought (CoT) model, specifically from one of the DeepSeek R1 series models, into standard LLMs, particularly DeepSeek-V3."
    },
    {
      "source": "[pdf_text]",
      "quote": "Through this two-phase extension training, DeepSeek-V3 is capable of handling inputs up to 128K in length while maintaining strong performance. Figure 8 illustrates that DeepSeek-V3, following supervised fine-tuning, achieves notable performance on the \"Needle In A HayStack\" (NIAH) test, demonstrating consistent robustness across context window lengths up to 128K."
    },
    {
      "source": "[pdf_text]",
      "quote": "We fine-tune DeepSeek-V3-Base for two epochs using the SFT dataset, using the cosine decay learning rate scheduling that starts at 5 × 10−6 and gradually decreases to 1 × 10−6."
    },
    {
      "source": "[pdf_text]",
      "quote": "We adopt a similar approach to DeepSeek-V2 (DeepSeek-AI, 2024c) to enable long context capabilities in DeepSeek-V3. After the pre-training stage, we apply YaRN (Peng et al., 2023a) for context extension and perform two additional training phases, each comprising 1000 steps, to progressively expand the context window from 4K to 32K and then to 128K."
    },
    {
      "source": "[pdf_text]",
      "quote": "Through this two-phase extension training, DeepSeek-V3 is capable of handling inputs up to 128K in length while maintaining strong performance."
    },
    {
      "source": "[pdf_text]",
      "quote": "The training of DeepSeek-V3 is cost-effective due to the support of FP8 training and meticulous engineering optimizations. The post-training also makes a success in distilling the reasoning capability from the DeepSeek-R1 series of models."
    },
    {
      "source": "[pdf_text]",
      "quote": "To maintain a balance between model accuracy and computational efficiency, we carefully selected optimal settings for DeepSeek-V3 in distillation."
    }
  ],
  "3-3 (Reinforcement Learning)__evidence": [
    {
      "source": "[pdf_text]",
      "quote": "We pre-train DeepSeek-V3 on 14.8 trillion diverse and high-quality tokens, followed by Supervised Fine-Tuning and Reinforcement Learning stages to fully harness its capabilities."
    },
    {
      "source": "[pdf_text]",
      "quote": "Following this, we conduct post-training, including Supervised Fine-Tuning (SFT) and Reinforcement Learning (RL) on the base model of DeepSeek-V3, to align it with human preferences and further unlock its potential."
    },
    {
      "source": "[pdf_text]",
      "quote": "The reward model is trained from the DeepSeek-V3 SFT checkpoints."
    },
    {
      "source": "[pdf_text]",
      "quote": "Similar to DeepSeek-V2 (DeepSeek-AI, 2024c), we adopt Group Relative Policy Optimization (GRPO) (Shao et al., 2024), which foregoes the critic model that is typically with the same size as the policy model, and estimates the baseline from group scores instead."
    },
    {
      "source": "[pdf_text]",
      "quote": "During the development of DeepSeek-V3, for these broader contexts, we employ the constitutional AI approach (Bai et al., 2022), leveraging the voting evaluation results of DeepSeek-V3 itself as a feedback source."
    },
    {
      "source": "[pdf_text]",
      "quote": "Rewards play a pivotal role in RL, steering the optimization process. During the development of DeepSeek-V3, for these broader contexts, we employ the constitutional AI approach (Bai et al., 2022), leveraging the voting evaluation results of DeepSeek-V3 itself as a feedback source."
    }
  ]
}