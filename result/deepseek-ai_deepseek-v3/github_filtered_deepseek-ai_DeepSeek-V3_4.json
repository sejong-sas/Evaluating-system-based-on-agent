{
  "4-1 (Pre-training Data)": "The publicly released information for DeepSeek-V3’s pre-training stage is extremely concise but still conveys several concrete numerical facts. The authors repeatedly emphasize that the model is trained on “14.8 trillion diverse and high-quality tokens,” explicitly attributing this token count to DeepSeek-V3 itself. They also disclose the computational budget: “only 2.664 million H800 GPU hours” were consumed to complete the pre-training run. These quotations make no mention of the exact data sources, domain breakdowns, licensing status, or geographic/linguistic origins of the corpus; likewise, they supply no list of dataset names or URLs. However, they do frame the corpus qualitatively as “diverse” and “high-quality,” suggesting a deliberate curation strategy even though the details of that strategy remain unstated. Finally, the authors claim that this large-scale training run results in “the currently strongest open-source base model,” implying that the 14.8 T token mixture is central to attaining the model’s reported performance. No additional specifics on data composition, filtering, or provenance are provided in the quoted material.",
  "4-1 (Pre-training Data)__evidence": [
    {
      "source": "[readme]",
      "quote": "We pre-train DeepSeek-V3 on 14.8 trillion diverse and high-quality tokens, followed by Supervised Fine-Tuning and Reinforcement Learning stages to fully harness its capabilities."
    },
    {
      "source": "[readme]",
      "quote": "At an economical cost of only 2.664M H800 GPU hours, we complete the pre-training of DeepSeek-V3 on 14.8T tokens, producing the currently strongest open-source base model."
    }
  ],
  "4-2 (Fine-tuning Data)": "Two separate statements outline the fine-tuning work that follows DeepSeek-V3’s pre-training. First, the team confirms that after the 14.8 T-token pre-training phase the model undergoes “Supervised Fine-Tuning” (SFT) as part of a larger pipeline that also includes a reinforcement-learning stage. Second, they describe “an innovative methodology to distill reasoning capabilities from the long-Chain-of-Thought (CoT) model, specifically from one of the DeepSeek R1 series models, into standard LLMs, particularly DeepSeek-V3.” This indicates that part of the fine-tuning data consists of distilled outputs—or at least supervisory signals—generated by an earlier DeepSeek model capable of long CoT reasoning. Although the quotes do not enumerate dataset sizes, domains, or release status, they do make clear that the SFT corpus is explicitly engineered to transfer stronger reasoning traces from DeepSeek R1 into DeepSeek-V3. No further details—such as exact prompt/response pairs, human-authored instructions, or licensing terms—are provided in the excerpts.",
  "4-2 (Fine-tuning Data)__evidence": [
    {
      "source": "[readme]",
      "quote": "We pre-train DeepSeek-V3 on 14.8 trillion diverse and high-quality tokens, followed by Supervised Fine-Tuning and Reinforcement Learning stages to fully harness its capabilities."
    },
    {
      "source": "[readme]",
      "quote": "We introduce an innovative methodology to distill reasoning capabilities from the long-Chain-of-Thought (CoT) model, specifically from one of the DeepSeek R1 series models, into standard LLMs, particularly DeepSeek-V3."
    }
  ],
  "4-3 (Reinforcement Learning Data)": "The only direct information about DeepSeek-V3’s reinforcement-learning data is its placement in the training pipeline: the model is \"followed by ... Reinforcement Learning stages\" after pre-training and SFT. The quotes do not describe the reward model’s construction, the nature of preference or comparison data, sampling strategies, or the size and provenance of any RLHF/RLAIF datasets. They simply confirm that an RL stage exists and that it is used \"to fully harness [the model’s] capabilities.\" Consequently, all specific attributes—such as the number of comparisons, whether human or synthetic feedback was used, or any public release plans—remain undisclosed in the provided material.",
  "4-3 (Reinforcement Learning Data)__evidence": [
    {
      "source": "[readme]",
      "quote": "We pre-train DeepSeek-V3 on 14.8 trillion diverse and high-quality tokens, followed by Supervised Fine-Tuning and Reinforcement Learning stages to fully harness its capabilities."
    }
  ],
  "4-4 (Data Filtering)": "The supplied quotations contain no explicit statements about data-filtering or cleaning for DeepSeek-V3. There are no references to automated classifiers, heuristic rules, deduplication thresholds, language or domain filters, toxicity or copyright screens, or any numeric acceptance ratios. Accordingly, no summary of the project’s filtering methodology can be drawn from the given text.",
  "4-4 (Data Filtering)__evidence": []
}