{
  "1-1 (Weights)": "The DeepSeek-V3 weights are openly hosted on HuggingFace under the repository ‚Äúdeepseek-ai/DeepSeek-V3‚Äù ( ‚Äú| DeepSeek-V3   | ‚Ä¶ | [ü§ó HuggingFace](https://huggingface.co/deepseek-ai/DeepSeek-V3) |‚Äù ).  The public snapshot contains 685 billion parameters in total: 671 billion for the main model plus 14 billion for the Multi-Token-Prediction (MTP) module ( ‚ÄúNOTE: The total size of DeepSeek-V3 models on HuggingFace is 685B, which includes 671B of the Main Model weights and 14B of the Multi-Token Prediction (MTP) Module weights.‚Äù ).  Users are instructed to ‚ÄúDownload the model weights from HuggingFace, and put them into `/path/to/DeepSeek-V3` folder.‚Äù  Only FP8 format checkpoints are provided because ‚ÄúFP8 training is natively adopted in our framework, we only provide FP8 weights.‚Äù  The repository ships the usual weight-management files such as ‚ÄúREADME_WEIGHTS.md‚Äù and sharded tensors like ‚Äúmodel-00001-of-000163.safetensors,‚Äù showing that anyone who can fetch from HuggingFace can obtain the complete set of shards locally.",
  "1-1 (Weights)__evidence": [
    {
      "source": "[readme]",
      "quote": "| DeepSeek-V3   | 671B | 37B |  128K   | [ü§ó HuggingFace](https://huggingface.co/deepseek-ai/DeepSeek-V3)   |"
    },
    {
      "source": "[readme]",
      "quote": "NOTE: The total size of DeepSeek-V3 models on HuggingFace is 685B, which includes 671B of the Main Model weights and 14B of the Multi-Token Prediction (MTP) Module weights."
    },
    {
      "source": "[readme]",
      "quote": "Download the model weights from HuggingFace, and put them into `/path/to/DeepSeek-V3` folder."
    },
    {
      "source": "[readme]",
      "quote": "Since FP8 training is natively adopted in our framework, we only provide FP8 weights."
    },
    {
      "source": "[files]",
      "quote": "README_WEIGHTS.md"
    },
    {
      "source": "[files]",
      "quote": "model-00001-of-000163.safetensors"
    }
  ],
  "1-2 (Code)": "The public code that accompanies DeepSeek-V3 is, at present, inference-oriented.  The maintainers supply a ‚Äú**DeepSeek-Infer Demo**‚Äù described as ‚Äúa simple and lightweight demo for FP8 and BF16 inference.‚Äù  Prospective users start by cloning ‚Äúour DeepSeek-V3 GitHub repository,‚Äù after which they can run the demo.  The repository‚Äôs copyright notice‚Äî‚ÄúCopyright 2023 DeepSeek-AI and The HuggingFace Inc. team. All rights reserved.‚Äù‚Äîclarifies stewardship, and the README notes that the implementation is ‚Äúbased on EleutherAI‚Äôs GPT-NeoX library and the GPT-NeoX and OPT implementations in this library.‚Äù  No quote references public release of pre-training, supervised fine-tuning, or RLHF training scripts; therefore only inference/serving code is confirmed to be available.",
  "1-2 (Code)__evidence": [
    {
      "source": "[readme]",
      "quote": "**DeepSeek-Infer Demo**: We provide a simple and lightweight demo for FP8 and BF16 inference."
    },
    {
      "source": "[readme]",
      "quote": "First, clone our DeepSeek-V3 GitHub repository:"
    },
    {
      "source": "[py_files/modeling_deepseek.py]",
      "quote": "Copyright 2023 DeepSeek-AI and The HuggingFace Inc. team. All rights reserved."
    },
    {
      "source": "[py_files/modeling_deepseek.py]",
      "quote": "This code is based on EleutherAI's GPT-NeoX library and the GPT-NeoX and OPT implementations in this library."
    }
  ],
  "1-3 (License)": "Licensing is split between code and model weights.  According to the repo: ‚ÄúThis code repository is licensed under [the MIT License](LICENSE-CODE). The use of DeepSeek-V3 Base/Chat models is subject to [the Model License](LICENSE-MODEL). DeepSeek-V3 series (including Base and Chat) supports commercial use.‚Äù  Because the statement immediately precedes the MIT text, the license grant itself is reproduced: ‚ÄúPermission is hereby granted, free of charge, to any person obtaining a copy of this software‚Ä¶ to deal in the Software without restriction, including ‚Ä¶ the rights to use, copy, modify, merge, publish, distribute, sublicense, and/or sell copies of the Software‚Ä¶.‚Äù  A second section in the docs reiterates: ‚ÄúThis code repository is licensed under [the MIT License](LICENSE-CODE). The use of DeepSeek-V3 Base/Chat models is subject to [the Model License](LICENSE-MODEL).‚Äù  Thus, (1) all code is MIT, giving broad rights including commercial redistribution; (2) the weights/models have a distinct Model License but are explicitly stated to ‚Äúsupport commercial use.‚Äù  No additional restrictive clauses are quoted for the models.",
  "1-3 (License)__evidence": [
    {
      "source": "[readme]",
      "quote": "This code repository is licensed under [the MIT License](LICENSE-CODE). The use of DeepSeek-V3 Base/Chat models is subject to [the Model License](LICENSE-MODEL). DeepSeek-V3 series (including Base and Chat) supports commercial use."
    },
    {
      "source": "[license_file]",
      "quote": "Permission is hereby granted, free of charge, to any person obtaining a copy of this software and associated documentation files (the \"Software\"), to deal in the Software without restriction, including without limitation the rights to use, copy, modify, merge, publish, distribute, sublicense, and/or sell copies of the Software, and to permit persons to whom the Software is furnished to do so, subject to the following conditions:"
    },
    {
      "source": "[py_files/modeling_deepseek.py]",
      "quote": "Licensed under the Apache License, Version 2.0 (the \"License\"); you may not use this file except in compliance with the License."
    },
    {
      "source": "[license_file]",
      "quote": "p-by-step guidance on Ascend NPUs, please follow the [instructions here](https://modelers.cn/models/MindIE/deepseekv3).\n\n\n## 7. License\nThis code repository is licensed under [the MIT License](LICENSE-CODE). The use of DeepSeek-V3 Base/Chat models is subject to [the Model License](LICENSE-MODEL). DeepSeek-V3 series (including Base and Chat)"
    }
  ],
  "1-4 (Paper)": "DeepSeek-AI provides a dedicated technical report.  The README links directly to the PDF‚Äî‚Äú<a href=\"https://github.com/deepseek-ai/DeepSeek-V3/blob/main/DeepSeek_V3.pdf\"><b>Paper Link</b>üëÅÔ∏è</a>.‚Äù  A full citation is also embedded: ‚Äú@misc{deepseekai2024deepseekv3technicalreport, title={DeepSeek-V3 Technical Report}, author={DeepSeek-AI}, year={2024}, eprint={2412.19437}, archivePrefix={arXiv}, primaryClass={cs.CL}, url={https://arxiv.org/abs/2412.19437} }.‚Äù  Therefore an official, citable document is openly released on arXiv (e-print 2412.19437, 2024) and mirrored in the GitHub repository.",
  "1-4 (Paper)__evidence": [
    {
      "source": "[readme]",
      "quote": "<p align=\"center\">  <a href=\"https://github.com/deepseek-ai/DeepSeek-V3/blob/main/DeepSeek_V3.pdf\"><b>Paper Link</b>üëÅÔ∏è</a> </p>"
    },
    {
      "source": "[readme]",
      "quote": "@misc{deepseekai2024deepseekv3technicalreport,\n      title={DeepSeek-V3 Technical Report}, \n      author={DeepSeek-AI},\n      year={2024},\n      eprint={2412.19437},\n      archivePrefix={arXiv},\n      primaryClass={cs.CL},\n      url={https://arxiv.org/abs/2412.19437}, \n}"
    }
  ],
  "1-5 (Architecture)": "DeepSeek-V3 is explicitly described as ‚Äúa strong Mixture-of-Experts (MoE) language model with 671B total parameters with 37B activated for each token.‚Äù  The authors explain that, to keep both inference and training economical, ‚ÄúDeepSeek-V3 adopts Multi-head Latent Attention (MLA) and DeepSeekMoE architectures, which were thoroughly validated in DeepSeek-V2.‚Äù  The open-source code base exposes ‚Äúthe configuration class to store the configuration of a [DeepseekV3Model] ‚Ä¶ defining the model architecture,‚Äù and they add that ‚Äúinstantiating a configuration with the defaults will yield a similar configuration to that of the DeepSeek-V3.‚Äù  A file header notes it is a ‚ÄúPyTorch DeepSeek model,‚Äù confirming the implementation framework.  Several custom building blocks are called out: ‚ÄúDeepseekV3RMSNorm is equivalent to T5LayerNorm,‚Äù and there are two enhanced positional-encoding modules‚Äî‚ÄúDeepseekV3RotaryEmbedding extended with linear scaling‚Äù and ‚ÄúDeepseekV3RotaryEmbedding extended with Dynamic NTK scaling.‚Äù  Taken together, the quotes portray DeepSeek-V3 as a 671-billion-parameter MoE transformer that activates 37 B parameters per token, employs MLA attention and the DeepSeekMoE routing strategy, relies on an RMSNorm variant, uses advanced rotary embeddings with linear and NTK scaling, and is fully configurable via an officially provided PyTorch configuration class.",
  "1-5 (Architecture)__evidence": [
    {
      "source": "[readme]",
      "quote": "We present DeepSeek-V3, a strong Mixture-of-Experts (MoE) language model with 671B total parameters with 37B activated for each token."
    },
    {
      "source": "[readme]",
      "quote": "To achieve efficient inference and cost-effective training, DeepSeek-V3 adopts Multi-head Latent Attention (MLA) and DeepSeekMoE architectures, which were thoroughly validated in DeepSeek-V2."
    },
    {
      "source": "[py_files/configuration_deepseek.py]",
      "quote": "This is the configuration class to store the configuration of a [`DeepseekV3Model`]. It is used to instantiate an DeepSeek model according to the specified arguments, defining the model architecture."
    },
    {
      "source": "[py_files/configuration_deepseek.py]",
      "quote": "Instantiating a configuration with the defaults will yield a similar configuration to that of the DeepSeek-V3."
    },
    {
      "source": "[py_files/modeling_deepseek.py]",
      "quote": "\"\"\" PyTorch DeepSeek model.\"\"\""
    },
    {
      "source": "[py_files/modeling_deepseek.py]",
      "quote": "\"\"\"\n        DeepseekV3RMSNorm is equivalent to T5LayerNorm\n        \"\"\""
    },
    {
      "source": "[py_files/modeling_deepseek.py]",
      "quote": "\"\"\"DeepseekV3RotaryEmbedding extended with linear scaling. Credits to the Reddit user /u/kaiokendev\"\"\""
    },
    {
      "source": "[py_files/modeling_deepseek.py]",
      "quote": "\"\"\"DeepseekV3RotaryEmbedding extended with Dynamic NTK scaling. Credits to the Reddit users /u/bloc97 and /u/emozilla\"\"\""
    }
  ],
  "1-6 (Tokenizer)": "",
  "1-6 (Tokenizer)__evidence": [],
  "2-1 (Hardware)": "Two concrete statements quantify the compute budget: ‚ÄúDeepSeek-V3 requires only 2.788M H800 GPU hours for its full training,‚Äù and ‚Äúat an economical cost of only 2.664M H800 GPU hours, we complete the pre-training of DeepSeek-V3 on 14.8T tokens.‚Äù  These figures show that the model was trained on H800 GPUs and that roughly 2.7‚Äì2.8 million GPU-hours were consumed in total, with 2.664 million of those hours devoted specifically to pre-training on a 14.8-trillion-token corpus.",
  "2-1 (Hardware)__evidence": [
    {
      "source": "[readme]",
      "quote": "Despite its excellent performance, DeepSeek-V3 requires only 2.788M H800 GPU hours for its full training."
    },
    {
      "source": "[readme]",
      "quote": "- At an economical cost of only 2.664M H800 GPU hours, we complete the pre-training of DeepSeek-V3 on 14.8T tokens, producing the currently strongest open-source base model."
    }
  ],
  "2-2 (Software)": "",
  "2-2 (Software)__evidence": [],
  "2-3 (API)": "The quotes indicate that DeepSeek-V3 can be accessed in two official ways. First, users who simply want an interactive chat experience can go to DeepSeek‚Äôs public web interface at \"chat.deepseek.com,\" where DeepSeek-V3 is available for direct conversation. Second, developers who need programmatic access are served by an \"OpenAI-Compatible\" interface hosted at \"platform.deepseek.com.\" This explicitly means that DeepSeek-V3 exposes an API that is designed to follow the same request/response format and tooling expectations as OpenAI‚Äôs own services. Because the quote states, ‚ÄúWe also provide OpenAI-Compatible API at DeepSeek Platform,‚Äù it is clear that the endpoints are publicly reachable (i.e., not private research endpoints) and that accompanying documentation/examples exist on that platform site. In summary, DeepSeek-V3 is publicly available both as a browser-based chat product and as a fully featured, OpenAI-style REST API for automated integration.",
  "2-3 (API)__evidence": [
    {
      "source": "[readme]",
      "quote": "You can chat with DeepSeek-V3 on DeepSeek's official website: [chat.deepseek.com](https://chat.deepseek.com/sign_in)\n\nWe also provide OpenAI-Compatible API at DeepSeek Platform: [platform.deepseek.com](https://platform.deepseek.com/)"
    }
  ],
  "3-1 (Pre-training)": "According to the quotes, the DeepSeek-V3 base model is produced through a very large-scale pre-training stage. Specifically, DeepSeek says: ‚ÄúWe pre-train DeepSeek-V3 on 14.8 trillion diverse and high-quality tokens.‚Äù The phrase ‚Äúdiverse and high-quality‚Äù emphasizes that the token mixture is broad and carefully filtered rather than a narrow or noisy corpus. The work was carried out with notable compute efficiency: ‚ÄúAt an economical cost of only 2.664 M H800 GPU hours, we complete the pre-training of DeepSeek-V3 on 14.8T tokens.‚Äù Using 2.664 million hours on NVIDIA H800 GPUs highlights the scale of resources devoted while simultaneously stressing that, relative to the total data volume, the cost is described as economical. The outcome of this effort is described as ‚Äúthe currently strongest open-source base model,‚Äù underscoring that DeepSeek-V3‚Äôs pre-training, by itself, yields a highly capable foundation before any supervised or reinforcement learning steps. Pre-training is therefore the foundational stage in the DeepSeek-V3 pipeline, after which additional fine-tuning stages are applied.",
  "3-1 (Pre-training)__evidence": [
    {
      "source": "[readme]",
      "quote": "We pre-train DeepSeek-V3 on 14.8 trillion diverse and high-quality tokens, followed by Supervised Fine-Tuning and Reinforcement Learning stages to fully harness its capabilities."
    },
    {
      "source": "[readme]",
      "quote": "- At an economical cost of only 2.664M H800 GPU hours, we complete the pre-training of DeepSeek-V3 on 14.8T tokens, producing the currently strongest open-source base model."
    }
  ],
  "3-2 (Fine-tuning)": "The DeepSeek-V3 training pipeline explicitly states a distinct ‚ÄúSupervised Fine-Tuning‚Äù (SFT) phase that follows the massive 14.8-trillion-token pre-training run. The quote reads: ‚ÄúWe pre-train DeepSeek-V3 on 14.8 trillion diverse and high-quality tokens, followed by Supervised Fine-Tuning and Reinforcement Learning stages to fully harness its capabilities.‚Äù This shows that SFT is positioned immediately after pre-training and before any RL methods, serving to align the raw model with curated, human-labeled instruction data or high-quality demonstration data (though the exact dataset is not detailed in the provided text). The phrase ‚Äúto fully harness its capabilities‚Äù indicates that the supervised step is essential for unlocking performance that is not reachable through pre-training alone. No explicit hyper-parameters, dataset sizes, or objectives are given in the quotes, but the ordering and necessity of this stage are made clear.",
  "3-2 (Fine-tuning)__evidence": [
    {
      "source": "[readme]",
      "quote": "We pre-train DeepSeek-V3 on 14.8 trillion diverse and high-quality tokens, followed by Supervised Fine-Tuning and Reinforcement Learning stages to fully harness its capabilities."
    }
  ],
  "3-3 (Reinforcement Learning)": "Reinforcement learning appears as the final optimization layer in the DeepSeek-V3 pipeline. The same sentence that confirms SFT also states: ‚ÄúWe pre-train DeepSeek-V3 on 14.8 trillion diverse and high-quality tokens, followed by Supervised Fine-Tuning and Reinforcement Learning stages to fully harness its capabilities.‚Äù This indicates that, after supervised alignment, DeepSeek applies an RL-based approach‚Äîlikely RLHF or a closely related method‚Äîto further polish model behavior. While the quote does not specify the exact RL algorithm (e.g., PPO, DPO, or specialized reward modeling), its placement in the sequence (after SFT) implies a standard modern alignment stack where RL is used to optimize for human preferences or policy quality above and beyond the supervised signal. Therefore, DeepSeek-V3 uses a three-step training recipe: large-scale pre-training ‚Üí supervised instruction tuning ‚Üí reinforcement learning, with the RL stage acting as the final refinement step.",
  "3-3 (Reinforcement Learning)__evidence": [
    {
      "source": "[readme]",
      "quote": "We pre-train DeepSeek-V3 on 14.8 trillion diverse and high-quality tokens, followed by Supervised Fine-Tuning and Reinforcement Learning stages to fully harness its capabilities."
    }
  ],
  "4-1 (Pre-training Data)": "The available statements that explicitly reference deepseek or deepseek-v3 indicate that DeepSeek-V3‚Äôs pre-training relied on a very large, diverse corpus totaling 14.8 trillion high-quality tokens.  The project team emphasizes both scale and data variety by describing the tokens as ‚Äúdiverse and high-quality.‚Äù  They also note the practical aspect of the effort, reporting that pre-training consumed 2.664 million H800 GPU-hours, which they portray as economical relative to the outcome.  The authors claim that this large-scale run produced ‚Äúthe currently strongest open-source base model,‚Äù underscoring their view of the pre-training data volume and composition as a decisive factor in model quality.",
  "4-1 (Pre-training Data)__evidence": [
    {
      "source": "[readme]",
      "quote": "We pre-train DeepSeek-V3 on 14.8 trillion diverse and high-quality tokens, followed by Supervised Fine-Tuning and Reinforcement Learning stages to fully harness its capabilities."
    },
    {
      "source": "[readme]",
      "quote": "- At an economical cost of only 2.664M H800 GPU hours, we complete the pre-training of DeepSeek-V3 on 14.8T tokens, producing the currently strongest open-source base model."
    }
  ],
  "4-2 (Fine-tuning Data)": "The fine-tuning phase for DeepSeek-V3 is described as a two-part effort: (1) a generic Supervised Fine-Tuning (SFT) stage that follows pre-training, and (2) an innovative procedure that transfers reasoning abilities from a specialized long-Chain-of-Thought DeepSeek R1 series model into DeepSeek-V3.  The quoted material stresses that the reasoning knowledge is ‚Äòdistilled‚Äô from a related DeepSeek model family, implying that curated exemplars or distilled outputs from the R1 model serve as the core fine-tuning data for V3.  The statements do not give explicit dataset sizes, licenses, or public-release information, but they make clear that the fine-tuning content is purpose-built to strengthen reasoning and that it is sourced internally from DeepSeek‚Äôs own lineage of models rather than from unrelated external corpora.",
  "4-2 (Fine-tuning Data)__evidence": [
    {
      "source": "[readme]",
      "quote": "We pre-train DeepSeek-V3 on 14.8 trillion diverse and high-quality tokens, followed by Supervised Fine-Tuning and Reinforcement Learning stages to fully harness its capabilities."
    },
    {
      "source": "[readme]",
      "quote": "-   We introduce an innovative methodology to distill reasoning capabilities from the long-Chain-of-Thought (CoT) model, specifically from one of the DeepSeek R1 series models, into standard LLMs, particularly DeepSeek-V3."
    }
  ],
  "4-3 (Reinforcement Learning Data)": "After supervised fine-tuning, the DeepSeek-V3 training pipeline proceeds to a Reinforcement Learning (RL) stage.  The quotes reveal that RL is an integral part of the training recipe‚Äî‚Äòfollowed by‚Ä¶Reinforcement Learning stages to fully harness its capabilities‚Äô‚Äîbut do not disclose specific datasets, reward models, or collection methods.  The only concrete information provided is that RL follows SFT in the chronology and serves as an additional optimization pass to unlock more of DeepSeek-V3‚Äôs potential.",
  "4-3 (Reinforcement Learning Data)__evidence": [
    {
      "source": "[readme]",
      "quote": "We pre-train DeepSeek-V3 on 14.8 trillion diverse and high-quality tokens, followed by Supervised Fine-Tuning and Reinforcement Learning stages to fully harness its capabilities."
    }
  ],
  "4-4 (Data Filtering)": "",
  "4-4 (Data Filtering)__evidence": [],
  "__usage": {
    "fine_tuning": "used",
    "rl": "used"
  }
}