{
  "4-1 (Pre-training Data)": "DeepSeek-V3 is pre-trained from scratch on a very large, curated corpus that the authors repeatedly characterize as “14.8 trillion high-quality and diverse tokens.”  The same 14.8 T figure is mentioned in several places, confirming that all pre-training runs—completed with 2.664 million H800 GPU-hours—use exactly this token budget.  The corpus composition was deliberately adjusted relative to DeepSeek-V2: the share of mathematical and programming samples is boosted and multilingual coverage is extended beyond English and Chinese, while a refined data-processing pipeline is applied to reduce redundancy yet preserve diversity.  Tokenization relies on a Byte-level BPE vocabulary expanded to 128 K symbols, and the Fill-in-Middle (FIM) strategy previously used for DeepSeekCoder-V2 is explicitly incorporated in DeepSeek-V3’s pre-training phase.  In total the model is a 671 B-parameter MoE system with 37 B parameters active per token during pre-training, and that entire parameter set is exposed to the 14.8 T-token mixture before any supervised or reinforcement post-training begins.",
  "4-2 (Fine-tuning Data)": "After the base model is produced, the team performs a post-training stage made up of Supervised Fine-Tuning (SFT) and Reinforcement Learning (RL).  The SFT data are split into two broad categories.  (1) “Reasoning Data” covering mathematics, code-competition problems, and logic puzzles are generated in-house with an internal DeepSeek-R1 model.  (2) “Non-Reasoning Data,” such as creative writing, role-play, and simpler question answering, are generated with DeepSeek-V2.5; human annotators then verify the correctness and accuracy of every example.  During distillation and other SFT procedures, hyper-parameters are chosen so that DeepSeek-V3 achieves an optimal trade-off between final model quality and computational cost.  All of these curated SFT datasets collectively align the base model with human-preferred behavior before it enters the reinforcement stage.",
  "4-3 (Reinforcement Learning Data)": "The subsequent RL-based alignment also starts from the DeepSeek-V3 checkpoints produced by SFT.  A dedicated reward model is trained on preference data that not only labels final answers but also includes the entire chain-of-thought that leads up to each judgment, increasing reward reliability.  For policy optimization, the authors adopt Group Relative Policy Optimization (GRPO), the same technique previously used for DeepSeek-V2.  GRPO eliminates a separate critic model by estimating baselines from group scores.  Prompts for RL cover a diverse set of domains—coding, mathematics, creative writing, role-play, and general question answering.  In addition, a constitutional-AI style procedure is layered on top: DeepSeek-V3 itself performs voting evaluations and supplies feedback signals on open-ended questions, allowing self-feedback to further strengthen alignment.",
  "4-4 (Data Filtering)": "Although the paper offers relatively few concrete numeric thresholds, it does describe several filtering and cleaning principles.  First, an effective load-balancing strategy is employed during training, with the explicit remark that DeepSeek-V3 therefore “does not drop any tokens,” indicating that upstream filtering has already produced a corpus judged clean enough to keep in its entirety.  Second, when constructing the response-generation datasets (e.g., those derived from DeepSeek-R1), an additional filtering pass ensures that the final data “retain the strengths of DeepSeek-R1 while producing responses that are concise and effective.”  Third, in the pre-training pipeline the corpus is “refined to minimize redundancy while maintaining corpus diversity,” and the team increases the proportion of mathematical and programming examples while broadening multilingual content.  Together, these steps constitute the stated data-filtering regime for DeepSeek-V3: redundancy reduction, targeted domain-ratio adjustments, multilingual expansion, and a final validation stage that obviates token dropping during training.",
  "4-1 (Pre-training Data)__evidence": [
    {
      "source": "[pdf_text]",
      "quote": "We pre-train DeepSeek-V3 on 14.8 trillion diverse and high-quality tokens, followed by Supervised Fine-Tuning and Reinforcement Learning stages to fully harness its capabilities."
    },
    {
      "source": "[pdf_text]",
      "quote": "During pre-training, we train DeepSeek-V3 on 14.8T high-quality and diverse tokens."
    },
    {
      "source": "[pdf_text]",
      "quote": "Finally, the training corpus for DeepSeek-V3 consists of 14.8T high-quality and diverse tokens in our tokenizer."
    },
    {
      "source": "[pdf_text]",
      "quote": "In alignment with DeepSeekCoder-V2, we also incorporate the FIM strategy in the pre-training of DeepSeek-V3."
    },
    {
      "source": "[pdf_text]",
      "quote": "The tokenizer for DeepSeek-V3 employs Byte-level BPE (Shibata et al., 1999) with an extended vocabulary of 128K tokens."
    },
    {
      "source": "[pdf_text]",
      "quote": "In this paper, we introduce DeepSeek-V3, a large MoE language model with 671B total parameters and 37B activated parameters, trained on 14.8T tokens."
    },
    {
      "source": "[sections/https://arxiv.org/pdf/2412.19437]",
      "quote": "We pre-train DeepSeek-V3 on 14.8 trillion diverse and high-quality tokens, followed by Supervised Fine-Tuning and Reinforcement Learning stages to fully harness its capabilities."
    },
    {
      "source": "[sections/https://arxiv.org/pdf/2412.19437]",
      "quote": "During pre-training, we train DeepSeek-V3 on 14.8T high-quality and diverse tokens."
    },
    {
      "source": "[sections/https://arxiv.org/pdf/2412.19437]",
      "quote": "At an economical cost of only 2.664M H800 GPU hours, we complete the pre-training of DeepSeek-V3 on 14.8T tokens, producing the currently strongest open-source base model."
    },
    {
      "source": "[pdf_text]",
      "quote": "Compared with DeepSeek-V2, we optimize the pre-training corpus by enhancing the ratio of mathematical and programming samples, while expanding multilingual coverage beyond English and Chinese. Also, our data processing pipeline is refined to minimize redundancy while maintaining corpus diversity."
    }
  ],
  "4-2 (Fine-tuning Data)__evidence": [
    {
      "source": "[pdf_text]",
      "quote": "Next, we conduct post-training, including Supervised Fine-Tuning (SFT) and Reinforcement Learning (RL) on the base model of DeepSeek-V3, to align it with human preferences and further unlock its potential."
    },
    {
      "source": "[pdf_text]",
      "quote": "Reasoning Data. For reasoning-related datasets, including those focused on mathematics, code competition problems, and logic puzzles, we generate the data by leveraging an internal DeepSeek-R1 model."
    },
    {
      "source": "[pdf_text]",
      "quote": "Non-Reasoning Data. For non-reasoning data, such as creative writing, role-play, and simple question answering, we utilize DeepSeek-V2.5 to generate responses and enlist human annotators to verify the accuracy and correctness of the data."
    },
    {
      "source": "[sections/https://arxiv.org/pdf/2412.19437]",
      "quote": "Following this, we conduct post-training, including Supervised Fine-Tuning (SFT) and Reinforcement Learning (RL) on the base model of DeepSeek-V3, to align it with human preferences and further unlock its potential."
    },
    {
      "source": "[pdf_text]",
      "quote": "For non-reasoning data, such as creative writing, role-play, and simple question answering, we utilize DeepSeek-V2.5 to generate responses and enlist human annotators to verify the accuracy and correctness of the data."
    },
    {
      "source": "[pdf_text]",
      "quote": "To maintain a balance between model accuracy and computational efficiency, we carefully selected optimal settings for DeepSeek-V3 in distillation."
    }
  ],
  "4-3 (Reinforcement Learning Data)__evidence": [
    {
      "source": "[pdf_text]",
      "quote": "Next, we conduct post-training, including Supervised Fine-Tuning (SFT) and Reinforcement Learning (RL) on the base model of DeepSeek-V3, to align it with human preferences and further unlock its potential."
    },
    {
      "source": "[pdf_text]",
      "quote": "The reward model is trained from the DeepSeek-V3 SFT checkpoints. To enhance its reliability, we construct preference data that not only provides the final reward but also includes the chain-of-thought leading to the reward."
    },
    {
      "source": "[sections/https://arxiv.org/pdf/2412.19437]",
      "quote": "Following this, we conduct post-training, including Supervised Fine-Tuning (SFT) and Reinforcement Learning (RL) on the base model of DeepSeek-V3, to align it with human preferences and further unlock its potential."
    },
    {
      "source": "[pdf_text]",
      "quote": "Similar to DeepSeek-V2 (DeepSeek-AI, 2024c), we adopt Group Relative Policy Optimization (GRPO) (Shao et al., 2024), which foregoes the critic model that is typically with the same size as the policy model, and estimates the baseline from group scores instead. We incorporate prompts from diverse domains, such as coding, math, writing, role-playing, and question answering, during the RL process."
    },
    {
      "source": "[pdf_text]",
      "quote": "The reward model is trained from the DeepSeek-V3 SFT checkpoints."
    },
    {
      "source": "[pdf_text]",
      "quote": "During the development of DeepSeek-V3, for these broader contexts, we employ the constitutional AI approach (Bai et al., 2022), leveraging the voting evaluation results of DeepSeek-V3 itself as a feedback source."
    },
    {
      "source": "[pdf_text]",
      "quote": "Therefore, we employ DeepSeek-V3 along with voting to offer self-feedback on open-ended questions, thereby improving the effectiveness and robustness of the alignment process."
    }
  ],
  "4-4 (Data Filtering)__evidence": [
    {
      "source": "[pdf_text]",
      "quote": "Due to the effective load balancing strategy, DeepSeek-V3 keeps a good load balance during its full training. Therefore, DeepSeek-V3 does not drop any tokens during training."
    },
    {
      "source": "[pdf_text]",
      "quote": "This method ensures that the final training data retains the strengths of DeepSeek-R1 while producing responses that are concise and effective."
    },
    {
      "source": "[pdf_text]",
      "quote": "Compared with DeepSeek-V2, we optimize the pre-training corpus by enhancing the ratio of mathematical and programming samples, while expanding multilingual coverage beyond English and Chinese. Also, our data processing pipeline is refined to minimize redundancy while maintaining corpus diversity."
    }
  ]
}