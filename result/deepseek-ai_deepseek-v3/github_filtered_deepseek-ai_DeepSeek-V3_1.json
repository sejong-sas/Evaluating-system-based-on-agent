{
  "1-1 (Weights)": "The provided material explicitly states that the DeepSeek-V3 weight files are publicly hosted on Hugging Face. A precise breakdown is given: “The total size of DeepSeek-V3 models on Hugging Face is 685B, which includes 671B of the Main Model weights and 14B of the Multi-Token Prediction (MTP) Module weights.”  A concrete download instruction is also supplied: “Download the model weights from Hugging Face, and put them into `/path/to/DeepSeek-V3` folder.”  From these two quotes we can conclude that (a) the weights are available for direct download without mention of any gating or approval process, (b) the storage location is Hugging Face, and (c) users are expected to place the retrieved files in a local directory named “DeepSeek-V3” for subsequent use.  No quote mentions alternative mirrors, checkpoints, or private-access requirements, so the only officially documented distribution channel is Hugging Face.  The quoted size figures (671 B main + 14 B MTP = 685 B) give an exact total footprint, indicating that both the core model and its auxiliary MTP module are shipped together as separate but related weight sets.",
  "1-1 (Weights)__evidence": [
    {
      "source": "[readme]",
      "quote": "The total size of DeepSeek-V3 models on Hugging Face is 685B, which includes 671B of the Main Model weights and 14B of the Multi-Token Prediction (MTP) Module weights."
    },
    {
      "source": "[readme]",
      "quote": "Download the model weights from Hugging Face, and put them into `/path/to/DeepSeek-V3` folder."
    }
  ],
  "1-2 (Code)": "Two sentences reference public code resources, both focused on inference rather than training.  First: “DeepSeek-V3 can be deployed locally using the following hardware and open-source community software:” (the hardware/software list is implied but not quoted).  Second: “1. **DeepSeek-Infer Demo**: We provide a simple and lightweight demo for FP8 and BF16 inference.”  From these, we can infer that (i) deployment code is released, (ii) that code includes an official ‘DeepSeek-Infer Demo’ supporting FP8 and BF16 precision, and (iii) it is intended for local execution.  No quote describes training scripts, data-preparation utilities, or hyper-parameter schedules, so there is no evidence that full training or fine-tuning code is public.  Consequently, only inference/serving code is confirmed open-sourced, while the end-to-end training pipeline remains undisclosed in the quoted material.",
  "1-2 (Code)__evidence": [
    {
      "source": "[readme]",
      "quote": "DeepSeek-V3 can be deployed locally using the following hardware and open-source community software:"
    },
    {
      "source": "[readme]",
      "quote": "1. **DeepSeek-Infer Demo**: We provide a simple and lightweight demo for FP8 and BF16 inference."
    }
  ],
  "1-3 (License)": "Licensing is split between code and model artifacts.  For code, the repository states: “This code repository is licensed under [the MIT License](LICENSE-CODE).”  The MIT excerpt is directly quoted: “Permission is hereby granted, free of charge, to any person obtaining a copy of this software and associated documentation files ... to deal in the Software without restriction, including without limitation the rights to use, copy, modify, merge, publish, distribute, sublicense, and/or sell copies of the Software,” indicating full rights for use (a), modification (b), redistribution (c), and commercial exploitation (d) under MIT.  For the weights and complementary material, a distinct model license applies: “The use of DeepSeek-V3 Base/Chat models is subject to [the Model License](LICENSE-MODEL). DeepSeek-V3 series (including Base and Chat) supports commercial use.”  The model license further clarifies: “Subject to the terms and conditions of this License, DeepSeek hereby grants to You a perpetual, worldwide, non-exclusive, no-charge, royalty-free, irrevocable copyright license to reproduce, prepare, publicly display, publicly perform, sublicense, and distribute the Complementary Material, the Model, and Derivatives of the Model.”  Taken together, these clauses confer broad rights for both code and model, explicitly permitting commercial activities and redistribution, with no ‘research-only’ or ‘non-commercial’ restrictions present in the quoted text.",
  "1-3 (License)__evidence": [
    {
      "source": "[readme]",
      "quote": "This code repository is licensed under [the MIT License](LICENSE-CODE). The use of DeepSeek-V3 Base/Chat models is subject to [the Model License](LICENSE-MODEL). DeepSeek-V3 series (including Base and Chat) supports commercial use."
    },
    {
      "source": "[license_files]",
      "quote": "Permission is hereby granted, free of charge, to any person obtaining a copy of this software and associated documentation files (the \"Software\"), to deal in the Software without restriction, including without limitation the rights to use, copy, modify, merge, publish, distribute, sublicense, and/or sell copies of the Software,"
    },
    {
      "source": "[license_files]",
      "quote": "Subject to the terms and conditions of this License, DeepSeek hereby grants to You a perpetual, worldwide, non-exclusive, no-charge, royalty-free, irrevocable copyright license to reproduce, prepare, publicly display, publicly perform, sublicense, and distribute the Complementary Material, the Model, and Derivatives of the Model."
    }
  ],
  "1-4 (Paper)": "The only direct bibliographic reference reads: “title={DeepSeek-V3 Technical Report},”.  This confirms the existence of an official technical report dedicated to DeepSeek-V3.  It signals that at least one formal document describing the architecture, training, or evaluation details has been authored, titled exactly “DeepSeek-V3 Technical Report.”  No DOI, arXiv link, or publication venue is supplied in the quote, so the precise outlet remains unspecified, but the presence of a technical report implies publicly accessible documentation beyond marketing materials.",
  "1-4 (Paper)__evidence": [
    {
      "source": "[readme]",
      "quote": "title={DeepSeek-V3 Technical Report},"
    }
  ]
}