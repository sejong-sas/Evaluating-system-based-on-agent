{
  "2-3 (API)": "The only information disclosed about public-facing access mechanisms for DeepSeek-V3 is operational rather than documentation-oriented. The authors state that “We deploy DeepSeek-V3 on the H800 cluster, where GPUs within each node are interconnected using NVLink, and all GPUs across the cluster are fully interconnected via IB. To simultaneously ensure both the Service-Level Objective (SLO) for online services and high throughput, we employ the following deployment strategy that separates the prefilling and decoding stages.” From this we can infer that an online-serving endpoint exists (because SLOs and a specialised inference pipeline are discussed) and that the serving stack explicitly splits the high-bandwidth ‘prefill’ phase from the latency-critical ‘decode’ phase to meet real-time requirements. No further details about REST/gRPC endpoints, authentication, rate limits, or public documentation are provided in the available quotes.",
  "3-1 (Pre-training)": "DeepSeek-V3 is introduced as “a large MoE language model with 671 B total parameters and 37 B activated parameters, trained on 14.8 T tokens.” The model is pre-trained exclusively on “14.8 trillion diverse and high-quality tokens.” Compared with its predecessor DeepSeek-V2, the corpus composition is adjusted: there is “enhancing the ratio of mathematical and programming samples, while expanding multilingual coverage beyond English and Chinese.” The tokenizer is a “Byte-level BPE … with an extended vocabulary of 128 K tokens.” Engineering choices mirror prior DeepSeek work: “In alignment with DeepSeekCoder-V2, we also incorporate the FIM strategy,” and, as with DeepSeek-V2, the network “employs additional RMSNorm layers after the compressed latent vectors, and multiplies additional scaling factors at the width bottlenecks.”\n\nThe training run is notable for scale and efficiency. Training “on each trillion tokens requires only 180 K H800 GPU hours, i.e., 3.7 days on our cluster with 2048 H800 GPUs,” and the complete life-cycle—“pre-training, context length extension, and post-training”—totals “2.788 M H800 GPU hours.” Stable FP8 training plus “meticulous engineering optimizations” are cited for cost savings. Hardware is specified: “DeepSeek-V3 is trained on a cluster equipped with 2048 NVIDIA H800 GPUs.” Under the MoE setup, only 37 B parameters are active per token, reducing per-step memory and compute while preserving a 671 B total capacity. Finally, the authors note that “besides the exact next token, each token will predict one additional token,” signalling a dual-target objective that augments the standard language-model loss.",
  "3-2 (Fine-tuning)": "After base pre-training, the team performs a structured post-training pipeline. First, “We fine-tune DeepSeek-V3-Base for two epochs using the SFT dataset, using the cosine decay learning rate scheduling that starts at 5 × 10⁻⁶ and gradually decreases to 1 × 10⁻⁶.” This Supervised Fine-Tuning (SFT) stage is explicitly framed as the first component of “post-training, including Supervised Fine-Tuning (SFT) and Reinforcement Learning (RL) on the base model of DeepSeek-V3, to align it with human preferences.”\n\nLong-context capability is introduced after pre-training via a two-phase curriculum: “After the pre-training stage, we apply YaRN … for context extension and perform two additional training phases, each comprising 1000 steps, to progressively expand the context window from 4 K to 32 K and then to 128 K.” The effectiveness of this extended window is evidenced by post-SFT evaluation, where “DeepSeek-V3, following supervised fine-tuning, achieves notable performance on the ‘Needle In A Haystack’ (NIAH) test,” and remains “consistent … across context window lengths up to 128 K.”\n\nCollectively, these quotes reveal (1) a clear distinction between base model and aligned model, (2) explicit hyperparameters for SFT (two epochs, cosine decay from 5e-6 to 1e-6), and (3) a reproducible two-stage YaRN procedure that upgrades the context length while continuing to train on the same hardware stack.",
  "3-3 (Reinforcement Learning)": "Reinforcement learning is the second half of the post-training pipeline: “We pre-train DeepSeek-V3 … followed by Supervised Fine-Tuning and Reinforcement Learning stages to fully harness its capabilities.” The RL phase begins with reward-model construction: “The reward model is trained from the DeepSeek-V3 SFT checkpoints,” meaning it leverages the SFT-aligned model as its backbone. For policy optimisation, the team adopts “Group Relative Policy Optimization (GRPO) … which foregoes the critic model … and estimates the baseline from group scores instead.” This choice reduces memory/compute overhead because no full-size critic is required.\n\nDuring broader-context training, “we employ the constitutional AI approach … leveraging the voting evaluation results of DeepSeek-V3 itself as a feedback source.” Thus the RL phase combines GRPO’s efficiency with constitutional-style self-evaluation to refine alignment with human-consistent principles. No explicit learning-rate, step-count, or batch-size figures are reported for the RL stage in the available quotes, but the pipeline ordering (pre-train → SFT → reward model → GRPO policy tuning) and the use of self-generated constitutional feedback are clearly documented.",
  "2-3 (API)__evidence": [
    {
      "source": "[sections/Inference and Deployment]",
      "quote": "We deploy DeepSeek-V3 on the H800 cluster, where GPUs within each node are interconnected using NVLink, and all GPUs across the cluster are fully interconnected via IB. To simultaneously ensure both the Service-Level Objective (SLO) for online services and high throughput, we employ the following deployment strategy that separates the prefilling and decoding stages."
    }
  ],
  "3-1 (Pre-training)__evidence": [
    {
      "source": "[abstract]",
      "quote": "We pre-train DeepSeek-V3 on 14.8 trillion diverse and high-quality tokens, followed by Supervised Fine-Tuning and Reinforcement Learning stages to fully harness its capabilities."
    },
    {
      "source": "[sections/Introduction]",
      "quote": "During pre-training, we train DeepSeek-V3 on 14.8T high-quality and diverse tokens."
    },
    {
      "source": "[sections/Introduction]",
      "quote": "During the pre-training stage, training DeepSeek-V3 on each trillion tokens requires only 180K H800 GPU hours, i.e., 3.7 days on our cluster with 2048 H800 GPUs."
    },
    {
      "source": "[sections/Pre-Training]",
      "quote": "Compared with DeepSeek-V2, we optimize the pre-training corpus by enhancing the ratio of mathematical and programming samples, while expanding multilingual coverage beyond English and Chinese."
    },
    {
      "source": "[sections/Pre-Training]",
      "quote": "Finally, the training corpus for DeepSeek-V3 consists of 14.8T high-quality and diverse tokens in our tokenizer."
    },
    {
      "source": "[sections/Pre-Training]",
      "quote": "In alignment with DeepSeekCoder-V2, we also incorporate the FIM strategy in the pre-training of DeepSeek-V3."
    },
    {
      "source": "[sections/Pre-Training]",
      "quote": "The tokenizer for DeepSeek-V3 employs Byte-level BPE (Shibata et al., 1999) with an extended vocabulary of 128K tokens."
    },
    {
      "source": "[sections/Pre-Training]",
      "quote": "Under this configuration, DeepSeek-V3 comprises 671B total parameters, of which 37B are activated for each token."
    },
    {
      "source": "[sections/Pre-Training]",
      "quote": "Besides the exact next token, each token will predict one additional token. As DeepSeek-V2, DeepSeek-V3 also employs additional RMSNorm layers after the compressed latent vectors, and multiplies additional scaling factors at the width bottlenecks."
    },
    {
      "source": "[abstract]",
      "quote": "We pre-train DeepSeek-V3 on 14.8 trillion diverse and\nhigh-quality tokens, followed by Supervised Fine-Tuning and Reinforcement Learning stages to\nfully harness its capabilities."
    },
    {
      "source": "[sections/Introduction]",
      "quote": "During pre-training, we train DeepSeek-V3 on 14.8T high-quality and diverse tokens. The\npre-training process is remarkably stable."
    },
    {
      "source": "[sections/Compute Clusters]",
      "quote": "DeepSeek-V3 is trained on a cluster equipped with 2048 NVIDIA H800 GPUs."
    },
    {
      "source": "[sections/Pre-Training/Data Construction]",
      "quote": "Finally, the training corpus for DeepSeek-V3 consists of 14.8T high-quality and diverse tokens in our tokenizer."
    },
    {
      "source": "[sections/Pre-Training/Data Construction]",
      "quote": "The tokenizer for DeepSeek-V3 employs Byte-level BPE (Shibata et al., 1999) with an extended vocabulary of 128K tokens."
    },
    {
      "source": "[sections/Pre-Training/Hyper-Parameters]",
      "quote": "Under this configuration, DeepSeek-V3 comprises 671B total parameters, of which 37B are activated for each token."
    },
    {
      "source": "[sections/Pre-Training/Data Construction]",
      "quote": "In alignment with DeepSeekCoder-V2, we also incorporate the FIM strategy in the pre-training of DeepSeek-V3."
    },
    {
      "source": "[sections/Conclusion]",
      "quote": "In this paper, we introduce DeepSeek-V3, a large MoE language model with 671B total parameters and 37B activated parameters, trained on 14.8T tokens. It requires only 2.788M H800 GPU hours for its full training, including pre-training, context length extension, and post-training."
    },
    {
      "source": "[sections/Conclusion]",
      "quote": "The training of DeepSeek-V3 is cost-effective due to the support of FP8 training and meticulous engineering optimizations."
    }
  ],
  "3-2 (Fine-tuning)__evidence": [
    {
      "source": "[abstract]",
      "quote": "We pre-train DeepSeek-V3 on 14.8 trillion diverse and high-quality tokens, followed by Supervised Fine-Tuning and Reinforcement Learning stages to fully harness its capabilities."
    },
    {
      "source": "[sections/Introduction]",
      "quote": "Following this, we conduct post-training, including Supervised Fine-Tuning (SFT) and Reinforcement Learning (RL) on the base model of DeepSeek-V3, to align it with human preferences and further unlock its potential."
    },
    {
      "source": "[sections/Supervised Fine-Tuning]",
      "quote": "We fine-tune DeepSeek-V3-Base for two epochs using the SFT dataset, using the cosine decay learning rate scheduling that starts at 5 × 10−6 and gradually decreases to 1 × 10−6."
    },
    {
      "source": "[sections/Introduction]",
      "quote": "Following this, we conduct post-training, including Supervised Fine-Tuning (SFT) and\nReinforcement Learning (RL) on the base model of DeepSeek-V3, to align it with human preferences\nand further unlock its potential."
    },
    {
      "source": "[sections/Long Context Extension]",
      "quote": "We adopt a similar approach to DeepSeek-V2 (DeepSeek-AI, 2024c) to enable long context capabilities in DeepSeek-V3. After the pre-training stage, we apply YaRN (Peng et al., 2023a) for context extension and perform two additional training phases, each comprising 1000 steps, to progressively expand the context window from 4K to 32K and then to 128K."
    },
    {
      "source": "[sections/Long Context Extension]",
      "quote": "Figure 8 illustrates that DeepSeek-V3, following supervised fine-tuning, achieves notable performance on the \"Needle In A Haystack\" (NIAH) test, demonstrating consistent robustness across context window lengths up to 128K."
    }
  ],
  "3-3 (Reinforcement Learning)__evidence": [
    {
      "source": "[abstract]",
      "quote": "We pre-train DeepSeek-V3 on 14.8 trillion diverse and high-quality tokens, followed by Supervised Fine-Tuning and Reinforcement Learning stages to fully harness its capabilities."
    },
    {
      "source": "[sections/Introduction]",
      "quote": "Following this, we conduct post-training, including Supervised Fine-Tuning (SFT) and Reinforcement Learning (RL) on the base model of DeepSeek-V3, to align it with human preferences and further unlock its potential."
    },
    {
      "source": "[sections/Reinforcement Learning]",
      "quote": "The reward model is trained from the DeepSeek-V3 SFT checkpoints."
    },
    {
      "source": "[sections/Reinforcement Learning]",
      "quote": "Similar to DeepSeek-V2 (DeepSeek-AI, 2024c), we adopt Group Relative Policy Optimization (GRPO) (Shao et al., 2024), which foregoes the critic model that is typically with the same size as the policy model, and estimates the baseline from group scores instead."
    },
    {
      "source": "[abstract]",
      "quote": "We pre-train DeepSeek-V3 on 14.8 trillion diverse and\nhigh-quality tokens, followed by Supervised Fine-Tuning and Reinforcement Learning stages to\nfully harness its capabilities."
    },
    {
      "source": "[sections/Introduction]",
      "quote": "Following this, we conduct post-training, including Supervised Fine-Tuning (SFT) and\nReinforcement Learning (RL) on the base model of DeepSeek-V3, to align it with human preferences\nand further unlock its potential."
    },
    {
      "source": "[sections/Self-Rewarding]",
      "quote": "During the development of DeepSeek-V3, for these broader contexts, we employ the constitutional AI approach (Bai et al., 2022), leveraging the voting evaluation results of DeepSeek-V3 itself as a feedback source."
    }
  ]
}