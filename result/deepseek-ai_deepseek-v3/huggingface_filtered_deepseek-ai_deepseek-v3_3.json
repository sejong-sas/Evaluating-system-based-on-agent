{
  "2-3 (API)": "The quotes indicate that DeepSeek-V3 can be accessed in two official ways. First, users who simply want an interactive chat experience can go to DeepSeek’s public web interface at \"chat.deepseek.com,\" where DeepSeek-V3 is available for direct conversation. Second, developers who need programmatic access are served by an \"OpenAI-Compatible\" interface hosted at \"platform.deepseek.com.\" This explicitly means that DeepSeek-V3 exposes an API that is designed to follow the same request/response format and tooling expectations as OpenAI’s own services. Because the quote states, “We also provide OpenAI-Compatible API at DeepSeek Platform,” it is clear that the endpoints are publicly reachable (i.e., not private research endpoints) and that accompanying documentation/examples exist on that platform site. In summary, DeepSeek-V3 is publicly available both as a browser-based chat product and as a fully featured, OpenAI-style REST API for automated integration.",
  "2-3 (API)__evidence": [
    {
      "source": "[readme]",
      "quote": "You can chat with DeepSeek-V3 on DeepSeek's official website: [chat.deepseek.com](https://chat.deepseek.com/sign_in)\n\nWe also provide OpenAI-Compatible API at DeepSeek Platform: [platform.deepseek.com](https://platform.deepseek.com/)"
    }
  ],
  "3-1 (Pre-training)": "According to the quotes, the DeepSeek-V3 base model is produced through a very large-scale pre-training stage. Specifically, DeepSeek says: “We pre-train DeepSeek-V3 on 14.8 trillion diverse and high-quality tokens.” The phrase “diverse and high-quality” emphasizes that the token mixture is broad and carefully filtered rather than a narrow or noisy corpus. The work was carried out with notable compute efficiency: “At an economical cost of only 2.664 M H800 GPU hours, we complete the pre-training of DeepSeek-V3 on 14.8T tokens.” Using 2.664 million hours on NVIDIA H800 GPUs highlights the scale of resources devoted while simultaneously stressing that, relative to the total data volume, the cost is described as economical. The outcome of this effort is described as “the currently strongest open-source base model,” underscoring that DeepSeek-V3’s pre-training, by itself, yields a highly capable foundation before any supervised or reinforcement learning steps. Pre-training is therefore the foundational stage in the DeepSeek-V3 pipeline, after which additional fine-tuning stages are applied.",
  "3-1 (Pre-training)__evidence": [
    {
      "source": "[readme]",
      "quote": "We pre-train DeepSeek-V3 on 14.8 trillion diverse and high-quality tokens, followed by Supervised Fine-Tuning and Reinforcement Learning stages to fully harness its capabilities."
    },
    {
      "source": "[readme]",
      "quote": "- At an economical cost of only 2.664M H800 GPU hours, we complete the pre-training of DeepSeek-V3 on 14.8T tokens, producing the currently strongest open-source base model."
    }
  ],
  "3-2 (Fine-tuning)": "The DeepSeek-V3 training pipeline explicitly states a distinct “Supervised Fine-Tuning” (SFT) phase that follows the massive 14.8-trillion-token pre-training run. The quote reads: “We pre-train DeepSeek-V3 on 14.8 trillion diverse and high-quality tokens, followed by Supervised Fine-Tuning and Reinforcement Learning stages to fully harness its capabilities.” This shows that SFT is positioned immediately after pre-training and before any RL methods, serving to align the raw model with curated, human-labeled instruction data or high-quality demonstration data (though the exact dataset is not detailed in the provided text). The phrase “to fully harness its capabilities” indicates that the supervised step is essential for unlocking performance that is not reachable through pre-training alone. No explicit hyper-parameters, dataset sizes, or objectives are given in the quotes, but the ordering and necessity of this stage are made clear.",
  "3-2 (Fine-tuning)__evidence": [
    {
      "source": "[readme]",
      "quote": "We pre-train DeepSeek-V3 on 14.8 trillion diverse and high-quality tokens, followed by Supervised Fine-Tuning and Reinforcement Learning stages to fully harness its capabilities."
    }
  ],
  "3-3 (Reinforcement Learning)": "Reinforcement learning appears as the final optimization layer in the DeepSeek-V3 pipeline. The same sentence that confirms SFT also states: “We pre-train DeepSeek-V3 on 14.8 trillion diverse and high-quality tokens, followed by Supervised Fine-Tuning and Reinforcement Learning stages to fully harness its capabilities.” This indicates that, after supervised alignment, DeepSeek applies an RL-based approach—likely RLHF or a closely related method—to further polish model behavior. While the quote does not specify the exact RL algorithm (e.g., PPO, DPO, or specialized reward modeling), its placement in the sequence (after SFT) implies a standard modern alignment stack where RL is used to optimize for human preferences or policy quality above and beyond the supervised signal. Therefore, DeepSeek-V3 uses a three-step training recipe: large-scale pre-training → supervised instruction tuning → reinforcement learning, with the RL stage acting as the final refinement step.",
  "3-3 (Reinforcement Learning)__evidence": [
    {
      "source": "[readme]",
      "quote": "We pre-train DeepSeek-V3 on 14.8 trillion diverse and high-quality tokens, followed by Supervised Fine-Tuning and Reinforcement Learning stages to fully harness its capabilities."
    }
  ]
}