{
  "1-1 (Weights)": "The available quotes provide clear evidence that the model weights for deepseek-ai/deepseek-v3 are publicly released. Two identical sentences explicitly state: “The model checkpoints are available at https://github.com/deepseek-ai/DeepSeek-V3.”  In addition, several sentences emphasize that DeepSeek-V3 is positioned as an “open-source model,” including: “DeepSeek-V3 stands as the best-performing open-source model,” and “Overall, DeepSeek-V3-Base comprehensively outperforms DeepSeek-V2-Base and Qwen2.5 72B Base, and surpasses LLaMA-3.1 405B Base in the majority of benchmarks, essentially becoming the strongest open-source model.”  Because the checkpoints are explicitly hosted on GitHub and repeatedly described as open-source, the quotes together confirm that anyone can obtain the weights by downloading the checkpoints from that GitHub repository.",
  "1-2 (Code)": "",
  "1-3 (License)": "",
  "1-4 (Paper)": "The supplied excerpts indicate the existence of an official technical document for deepseek-ai/deepseek-v3.  One quote explicitly references “DeepSeek-V3 Technical Report,” signaling that a formal report has been published.  Multiple sentences confirm it is indeed a paper: “In the remainder of this paper, we first present a detailed exposition of our DeepSeek-V3 model architecture (Section 2),” and “In this paper, we introduce DeepSeek-V3, a large MoE language model with 671 B total parameters and 37 B activated parameters, trained on 14.8 T tokens.”  Collectively, these quotes establish that an official paper or technical report exists, provides architectural details in Section 2, and describes DeepSeek-V3 as a 671-billion-parameter Mixture-of-Experts model with 37 B active parameters per token trained on 14.8 trillion tokens.",
  "1-1 (Weights)__evidence": [
    {
      "source": "[pdf_text]",
      "quote": "The model checkpoints are available at https://github.com/deepseek-ai/DeepSeek-V3."
    },
    {
      "source": "[pdf_text]",
      "quote": "DeepSeek-V3-Base comprehensively outperforms DeepSeek-V2-Base and Qwen2.5 72B Base, and surpasses LLaMA-3.1 405B Base in the majority of benchmarks, essentially becoming the strongest open-source model."
    },
    {
      "source": "[pdf_text]",
      "quote": "Table 6 presents the evaluation results, showcasing that DeepSeek-V3 stands as the best-performing open-source model."
    },
    {
      "source": "[sections/2412.19437]",
      "quote": "The model checkpoints are available at https://github.com/deepseek-ai/DeepSeek-V3."
    },
    {
      "source": "[sections/Evaluation]",
      "quote": "Overall, DeepSeek-V3-Base comprehensively outperforms DeepSeek-V2-Base and Qwen2.5 72B Base, and surpasses LLaMA-3.1 405B Base in the majority of benchmarks, essentially becoming the strongest open-source model."
    },
    {
      "source": "[pdf_text]",
      "quote": "DeepSeek-V3 stands as the best-performing open-source model, and also exhibits competitive performance against frontier closed-source models."
    }
  ],
  "1-2 (Code)__evidence": [],
  "1-3 (License)__evidence": [],
  "1-4 (Paper)__evidence": [
    {
      "source": "[pdf_text]",
      "quote": "In the remainder of this paper, we first present a detailed exposition of our DeepSeek-V3 model architecture (Section 2)."
    },
    {
      "source": "[pdf_text]",
      "quote": "We present DeepSeek-V3, a strong Mixture-of-Experts (MoE) language model with 671B total parameters with 37B activated for each token."
    },
    {
      "source": "[sections/Conclusion]",
      "quote": "In this paper, we introduce DeepSeek-V3, a large MoE language model with 671B total parameters and 37B activated parameters, trained on 14.8T tokens."
    },
    {
      "source": "[sections/2412.19437]",
      "quote": "DeepSeek-V3 Technical Report"
    }
  ],
  "1-5 (Architecture)": "DeepSeek-V3 is a Transformer-based Mixture-of-Experts (MoE) language model that totals 671 billion parameters, but only 37 billion parameters are activated for any given token. Its design inherits and extends components proven in DeepSeek-V2: it integrates Multi-head Latent Attention (MLA) together with the custom DeepSeekMoE routing scheme, adds extra RMSNorm layers after compressed latent vectors, and applies additional scaling factors at width bottlenecks. The network is configured with 61 Transformer layers and a hidden dimension of 7 168. To improve routing efficiency it introduces an auxiliary-loss-free load-balancing strategy, and to boost predictive power it replaces standard next-token prediction with a multi-token objective that simultaneously forecasts the next two tokens (MTP). The model was trained on 14.8 trillion tokens. Altogether, these choices deliver both high performance and significantly lower training/inference cost relative to dense models of comparable capability.",
  "1-6 (Tokenizer)": "DeepSeek-V3 uses a Byte-level BPE tokenizer (Shibata et al., 1999) that has been enlarged to a 128 000-token vocabulary. No other tokenizer variants are reported for this model.",
  "2-1 (Hardware)": "Training was carried out on a cluster containing 2 048 NVIDIA H800 GPUs. With this hardware, processing one trillion tokens takes about 180 000 H800 GPU-hours—equivalent to roughly 3.7 days on the full cluster—and end-to-end pre-training of the 14.8 T-token corpus consumed 2.788 million H800 GPU-hours in total.",
  "2-2 (Software)": "Model training leverages the in-house HAI-LLM framework. Parallelism is arranged as 16-way Pipeline Parallelism, 64-way Expert Parallelism across 8 nodes, and ZeRO-1 Data Parallelism. The team enabled cost efficiency through FP8 training and other engineering optimizations. After core pre-training, YaRN is applied in two 1 000-step phases to expand the context window from 4 K → 32 K → 128 K tokens. Optimisation techniques include Group Relative Policy Optimization (GRPO) for instruction tuning, and a cosine-decay learning-rate schedule (5 × 10⁻⁶ down to 1 × 10⁻⁶) during two-epoch supervised fine-tuning of the DeepSeek-V3-Base variant.",
  "1-5 (Architecture)__evidence": [
    {
      "source": "[abstract]",
      "quote": "We present DeepSeek-V3, a strong Mixture-of-Experts (MoE) language model with 671B total parameters with 37B activated for each token."
    },
    {
      "source": "[abstract]",
      "quote": "To achieve efficient inference and cost-effective training, DeepSeek-V3 adopts Multi-head Latent Attention (MLA) and DeepSeekMoE architectures, which were thoroughly validated in DeepSeek-V2."
    },
    {
      "source": "[sections/Introduction]",
      "quote": "To further push the boundaries of open-source model capabilities, we scale up our models and introduce DeepSeek-V3, a large Mixture-of-Experts (MoE) model with 671B parameters, of which 37B are activated for each token."
    },
    {
      "source": "[sections/Architecture]",
      "quote": "The basic architecture of DeepSeek-V3 is still within the Transformer framework. For efficient inference and economical training, DeepSeek-V3 also adopts MLA and DeepSeekMoE, which have been thoroughly validated by DeepSeek-V2."
    },
    {
      "source": "[sections/4.2 Hyper-Parameters]",
      "quote": "As DeepSeek-V2, DeepSeek-V3 also employs additional RMSNorm layers after the compressed latent vectors, and multiplies additional scaling factors at the width bottlenecks. Under this configuration, DeepSeek-V3 comprises 671B total parameters, of which 37B are activated for each token."
    },
    {
      "source": "[sections/Conclusion]",
      "quote": "In this paper, we introduce DeepSeek-V3, a large MoE language model with 671B total parameters and 37B activated parameters, trained on 14.8T tokens."
    },
    {
      "source": "[abstract]",
      "quote": "Furthermore, DeepSeek-V3 pioneers an auxiliary-loss-free strategy for load balancing and sets a multi-token prediction training objective for stronger performance."
    },
    {
      "source": "[sections/Architecture]",
      "quote": "Compared with DeepSeek-V2, an exception is that we additionally introduce an auxiliary-loss-free load balancing strategy for DeepSeekMoE to mitigate the performance degradation induced by the effort to ensure load balance."
    },
    {
      "source": "[sections/Hyper-Parameters]",
      "quote": "We set the number of Transformer layers to 61 and the hidden dimension to 7168. Under this configuration, DeepSeek-V3 comprises 671B total parameters, of which 37B are activated for each token."
    },
    {
      "source": "[sections/Hyper-Parameters]",
      "quote": "As DeepSeek-V2, DeepSeek-V3 also employs additional RMSNorm layers after the compressed latent vectors, and multiplies additional scaling factors at the width bottlenecks."
    },
    {
      "source": "[sections/Conclusion]",
      "quote": "In this paper, we introduce DeepSeek-V3, a large MoE language model with 671B total parameters and 37B activated parameters, trained on 14.8T tokens. In addition to the MLA and DeepSeekMoE architectures, it also pioneers an auxiliary-loss-free strategy for load balancing and sets a multi-token prediction training objective for stronger performance."
    },
    {
      "source": "[sections/5.4.3 Multi-Token Prediction Evaluation]",
      "quote": "Instead of predicting just the next single token, DeepSeek-V3 predicts the next 2 tokens through the MTP technique."
    }
  ],
  "1-6 (Tokenizer)__evidence": [
    {
      "source": "[sections/4.1 Data Construction]",
      "quote": "The tokenizer for DeepSeek-V3 employs Byte-level BPE (Shibata et al., 1999) with an extended vocabulary of 128K tokens."
    },
    {
      "source": "[sections/Data Construction]",
      "quote": "The tokenizer for DeepSeek-V3 employs Byte-level BPE (Shibata et al., 1999) with an extended vocabulary of 128K tokens."
    }
  ],
  "2-1 (Hardware)__evidence": [
    {
      "source": "[sections/Compute Clusters]",
      "quote": "DeepSeek-V3 is trained on a cluster equipped with 2048 NVIDIA H800 GPUs."
    },
    {
      "source": "[sections/Introduction]",
      "quote": "During the pre-training stage, training DeepSeek-V3 on each trillion tokens requires only 180K H800 GPU hours, i.e., 3.7 days on our cluster with 2048 H800 GPUs."
    },
    {
      "source": "[abstract]",
      "quote": "Despite its excellent performance, DeepSeek-V3 requires only 2.788M H800 GPU hours for its full training."
    },
    {
      "source": "[sections/4.4.2 Evaluation Results]",
      "quote": "Due to our efficient architectures and comprehensive engineering optimizations, DeepSeek-V3 achieves extremely high training efficiency. Under our training framework and infrastructures, training DeepSeek-V3 on each trillion tokens requires only 180K H800 GPU hours, which is much cheaper than training 72B or 405B dense models."
    }
  ],
  "2-2 (Software)__evidence": [
    {
      "source": "[sections/Training Framework]",
      "quote": "The training of DeepSeek-V3 is supported by the HAI-LLM framework, an efficient and lightweight training framework crafted by our engineers from the ground up."
    },
    {
      "source": "[sections/Training Framework]",
      "quote": "On the whole, DeepSeek-V3 applies 16-way Pipeline Parallelism (PP), 64-way Expert Parallelism (EP) spanning 8 nodes, and ZeRO-1 Data Parallelism (DP)."
    },
    {
      "source": "[sections/4.3 Long Context Extension]",
      "quote": "We adopt a similar approach to DeepSeek-V2 (DeepSeek-AI, 2024c) to enable long context capabilities in DeepSeek-V3. After the pre-training stage, we apply YaRN (Peng et al., 2023a) for context extension and perform two additional training phases, each comprising 1000 steps, to progressively expand the context window from 4K to 32K and then to 128K."
    },
    {
      "source": "[sections/Training Framework]",
      "quote": "The training of DeepSeek-V3 is supported by the HAI-LLM framework, an efficient and lightweight training framework crafted by our engineers from the ground up. On the whole, DeepSeek-V3 applies 16-way Pipeline Parallelism (PP) (Qi et al., 2023a), 64-way Expert Parallelism (EP) (Lepikhin et al., 2021) spanning 8 nodes, and ZeRO-1 Data Parallelism (DP) (Rajbhandari et al., 2020)."
    },
    {
      "source": "[sections/Conclusion]",
      "quote": "The training of DeepSeek-V3 is cost-effective due to the support of FP8 training and meticulous engineering optimizations."
    },
    {
      "source": "[sections/5.2.2 Group Relative Policy Optimization]",
      "quote": "Similar to DeepSeek-V2 (DeepSeek-AI, 2024c), we adopt Group Relative Policy Optimization (GRPO) (Shao et al., 2024), which foregoes the critic model that is typically with the same size as the policy model, and estimates the baseline from group scores instead."
    },
    {
      "source": "[sections/5.1 SFT Settings]",
      "quote": "We fine-tune DeepSeek-V3-Base for two epochs using the SFT dataset, using the cosine decay learning rate scheduling that starts at 5 × 10−6 and gradually decreases to 1 × 10−6."
    }
  ],
  "2-3 (API)": "The only information disclosed about public-facing access mechanisms for DeepSeek-V3 is operational rather than documentation-oriented. The authors state that “We deploy DeepSeek-V3 on the H800 cluster, where GPUs within each node are interconnected using NVLink, and all GPUs across the cluster are fully interconnected via IB. To simultaneously ensure both the Service-Level Objective (SLO) for online services and high throughput, we employ the following deployment strategy that separates the prefilling and decoding stages.” From this we can infer that an online-serving endpoint exists (because SLOs and a specialised inference pipeline are discussed) and that the serving stack explicitly splits the high-bandwidth ‘prefill’ phase from the latency-critical ‘decode’ phase to meet real-time requirements. No further details about REST/gRPC endpoints, authentication, rate limits, or public documentation are provided in the available quotes.",
  "3-1 (Pre-training)": "DeepSeek-V3 is introduced as “a large MoE language model with 671 B total parameters and 37 B activated parameters, trained on 14.8 T tokens.” The model is pre-trained exclusively on “14.8 trillion diverse and high-quality tokens.” Compared with its predecessor DeepSeek-V2, the corpus composition is adjusted: there is “enhancing the ratio of mathematical and programming samples, while expanding multilingual coverage beyond English and Chinese.” The tokenizer is a “Byte-level BPE … with an extended vocabulary of 128 K tokens.” Engineering choices mirror prior DeepSeek work: “In alignment with DeepSeekCoder-V2, we also incorporate the FIM strategy,” and, as with DeepSeek-V2, the network “employs additional RMSNorm layers after the compressed latent vectors, and multiplies additional scaling factors at the width bottlenecks.”\n\nThe training run is notable for scale and efficiency. Training “on each trillion tokens requires only 180 K H800 GPU hours, i.e., 3.7 days on our cluster with 2048 H800 GPUs,” and the complete life-cycle—“pre-training, context length extension, and post-training”—totals “2.788 M H800 GPU hours.” Stable FP8 training plus “meticulous engineering optimizations” are cited for cost savings. Hardware is specified: “DeepSeek-V3 is trained on a cluster equipped with 2048 NVIDIA H800 GPUs.” Under the MoE setup, only 37 B parameters are active per token, reducing per-step memory and compute while preserving a 671 B total capacity. Finally, the authors note that “besides the exact next token, each token will predict one additional token,” signalling a dual-target objective that augments the standard language-model loss.",
  "3-2 (Fine-tuning)": "After base pre-training, the team performs a structured post-training pipeline. First, “We fine-tune DeepSeek-V3-Base for two epochs using the SFT dataset, using the cosine decay learning rate scheduling that starts at 5 × 10⁻⁶ and gradually decreases to 1 × 10⁻⁶.” This Supervised Fine-Tuning (SFT) stage is explicitly framed as the first component of “post-training, including Supervised Fine-Tuning (SFT) and Reinforcement Learning (RL) on the base model of DeepSeek-V3, to align it with human preferences.”\n\nLong-context capability is introduced after pre-training via a two-phase curriculum: “After the pre-training stage, we apply YaRN … for context extension and perform two additional training phases, each comprising 1000 steps, to progressively expand the context window from 4 K to 32 K and then to 128 K.” The effectiveness of this extended window is evidenced by post-SFT evaluation, where “DeepSeek-V3, following supervised fine-tuning, achieves notable performance on the ‘Needle In A Haystack’ (NIAH) test,” and remains “consistent … across context window lengths up to 128 K.”\n\nCollectively, these quotes reveal (1) a clear distinction between base model and aligned model, (2) explicit hyperparameters for SFT (two epochs, cosine decay from 5e-6 to 1e-6), and (3) a reproducible two-stage YaRN procedure that upgrades the context length while continuing to train on the same hardware stack.",
  "3-3 (Reinforcement Learning)": "Reinforcement learning is the second half of the post-training pipeline: “We pre-train DeepSeek-V3 … followed by Supervised Fine-Tuning and Reinforcement Learning stages to fully harness its capabilities.” The RL phase begins with reward-model construction: “The reward model is trained from the DeepSeek-V3 SFT checkpoints,” meaning it leverages the SFT-aligned model as its backbone. For policy optimisation, the team adopts “Group Relative Policy Optimization (GRPO) … which foregoes the critic model … and estimates the baseline from group scores instead.” This choice reduces memory/compute overhead because no full-size critic is required.\n\nDuring broader-context training, “we employ the constitutional AI approach … leveraging the voting evaluation results of DeepSeek-V3 itself as a feedback source.” Thus the RL phase combines GRPO’s efficiency with constitutional-style self-evaluation to refine alignment with human-consistent principles. No explicit learning-rate, step-count, or batch-size figures are reported for the RL stage in the available quotes, but the pipeline ordering (pre-train → SFT → reward model → GRPO policy tuning) and the use of self-generated constitutional feedback are clearly documented.",
  "2-3 (API)__evidence": [
    {
      "source": "[sections/Inference and Deployment]",
      "quote": "We deploy DeepSeek-V3 on the H800 cluster, where GPUs within each node are interconnected using NVLink, and all GPUs across the cluster are fully interconnected via IB. To simultaneously ensure both the Service-Level Objective (SLO) for online services and high throughput, we employ the following deployment strategy that separates the prefilling and decoding stages."
    }
  ],
  "3-1 (Pre-training)__evidence": [
    {
      "source": "[abstract]",
      "quote": "We pre-train DeepSeek-V3 on 14.8 trillion diverse and high-quality tokens, followed by Supervised Fine-Tuning and Reinforcement Learning stages to fully harness its capabilities."
    },
    {
      "source": "[sections/Introduction]",
      "quote": "During pre-training, we train DeepSeek-V3 on 14.8T high-quality and diverse tokens."
    },
    {
      "source": "[sections/Introduction]",
      "quote": "During the pre-training stage, training DeepSeek-V3 on each trillion tokens requires only 180K H800 GPU hours, i.e., 3.7 days on our cluster with 2048 H800 GPUs."
    },
    {
      "source": "[sections/Pre-Training]",
      "quote": "Compared with DeepSeek-V2, we optimize the pre-training corpus by enhancing the ratio of mathematical and programming samples, while expanding multilingual coverage beyond English and Chinese."
    },
    {
      "source": "[sections/Pre-Training]",
      "quote": "Finally, the training corpus for DeepSeek-V3 consists of 14.8T high-quality and diverse tokens in our tokenizer."
    },
    {
      "source": "[sections/Pre-Training]",
      "quote": "In alignment with DeepSeekCoder-V2, we also incorporate the FIM strategy in the pre-training of DeepSeek-V3."
    },
    {
      "source": "[sections/Pre-Training]",
      "quote": "The tokenizer for DeepSeek-V3 employs Byte-level BPE (Shibata et al., 1999) with an extended vocabulary of 128K tokens."
    },
    {
      "source": "[sections/Pre-Training]",
      "quote": "Under this configuration, DeepSeek-V3 comprises 671B total parameters, of which 37B are activated for each token."
    },
    {
      "source": "[sections/Pre-Training]",
      "quote": "Besides the exact next token, each token will predict one additional token. As DeepSeek-V2, DeepSeek-V3 also employs additional RMSNorm layers after the compressed latent vectors, and multiplies additional scaling factors at the width bottlenecks."
    },
    {
      "source": "[abstract]",
      "quote": "We pre-train DeepSeek-V3 on 14.8 trillion diverse and\nhigh-quality tokens, followed by Supervised Fine-Tuning and Reinforcement Learning stages to\nfully harness its capabilities."
    },
    {
      "source": "[sections/Introduction]",
      "quote": "During pre-training, we train DeepSeek-V3 on 14.8T high-quality and diverse tokens. The\npre-training process is remarkably stable."
    },
    {
      "source": "[sections/Compute Clusters]",
      "quote": "DeepSeek-V3 is trained on a cluster equipped with 2048 NVIDIA H800 GPUs."
    },
    {
      "source": "[sections/Pre-Training/Data Construction]",
      "quote": "Finally, the training corpus for DeepSeek-V3 consists of 14.8T high-quality and diverse tokens in our tokenizer."
    },
    {
      "source": "[sections/Pre-Training/Data Construction]",
      "quote": "The tokenizer for DeepSeek-V3 employs Byte-level BPE (Shibata et al., 1999) with an extended vocabulary of 128K tokens."
    },
    {
      "source": "[sections/Pre-Training/Hyper-Parameters]",
      "quote": "Under this configuration, DeepSeek-V3 comprises 671B total parameters, of which 37B are activated for each token."
    },
    {
      "source": "[sections/Pre-Training/Data Construction]",
      "quote": "In alignment with DeepSeekCoder-V2, we also incorporate the FIM strategy in the pre-training of DeepSeek-V3."
    },
    {
      "source": "[sections/Conclusion]",
      "quote": "In this paper, we introduce DeepSeek-V3, a large MoE language model with 671B total parameters and 37B activated parameters, trained on 14.8T tokens. It requires only 2.788M H800 GPU hours for its full training, including pre-training, context length extension, and post-training."
    },
    {
      "source": "[sections/Conclusion]",
      "quote": "The training of DeepSeek-V3 is cost-effective due to the support of FP8 training and meticulous engineering optimizations."
    }
  ],
  "3-2 (Fine-tuning)__evidence": [
    {
      "source": "[abstract]",
      "quote": "We pre-train DeepSeek-V3 on 14.8 trillion diverse and high-quality tokens, followed by Supervised Fine-Tuning and Reinforcement Learning stages to fully harness its capabilities."
    },
    {
      "source": "[sections/Introduction]",
      "quote": "Following this, we conduct post-training, including Supervised Fine-Tuning (SFT) and Reinforcement Learning (RL) on the base model of DeepSeek-V3, to align it with human preferences and further unlock its potential."
    },
    {
      "source": "[sections/Supervised Fine-Tuning]",
      "quote": "We fine-tune DeepSeek-V3-Base for two epochs using the SFT dataset, using the cosine decay learning rate scheduling that starts at 5 × 10−6 and gradually decreases to 1 × 10−6."
    },
    {
      "source": "[sections/Introduction]",
      "quote": "Following this, we conduct post-training, including Supervised Fine-Tuning (SFT) and\nReinforcement Learning (RL) on the base model of DeepSeek-V3, to align it with human preferences\nand further unlock its potential."
    },
    {
      "source": "[sections/Long Context Extension]",
      "quote": "We adopt a similar approach to DeepSeek-V2 (DeepSeek-AI, 2024c) to enable long context capabilities in DeepSeek-V3. After the pre-training stage, we apply YaRN (Peng et al., 2023a) for context extension and perform two additional training phases, each comprising 1000 steps, to progressively expand the context window from 4K to 32K and then to 128K."
    },
    {
      "source": "[sections/Long Context Extension]",
      "quote": "Figure 8 illustrates that DeepSeek-V3, following supervised fine-tuning, achieves notable performance on the \"Needle In A Haystack\" (NIAH) test, demonstrating consistent robustness across context window lengths up to 128K."
    }
  ],
  "3-3 (Reinforcement Learning)__evidence": [
    {
      "source": "[abstract]",
      "quote": "We pre-train DeepSeek-V3 on 14.8 trillion diverse and high-quality tokens, followed by Supervised Fine-Tuning and Reinforcement Learning stages to fully harness its capabilities."
    },
    {
      "source": "[sections/Introduction]",
      "quote": "Following this, we conduct post-training, including Supervised Fine-Tuning (SFT) and Reinforcement Learning (RL) on the base model of DeepSeek-V3, to align it with human preferences and further unlock its potential."
    },
    {
      "source": "[sections/Reinforcement Learning]",
      "quote": "The reward model is trained from the DeepSeek-V3 SFT checkpoints."
    },
    {
      "source": "[sections/Reinforcement Learning]",
      "quote": "Similar to DeepSeek-V2 (DeepSeek-AI, 2024c), we adopt Group Relative Policy Optimization (GRPO) (Shao et al., 2024), which foregoes the critic model that is typically with the same size as the policy model, and estimates the baseline from group scores instead."
    },
    {
      "source": "[abstract]",
      "quote": "We pre-train DeepSeek-V3 on 14.8 trillion diverse and\nhigh-quality tokens, followed by Supervised Fine-Tuning and Reinforcement Learning stages to\nfully harness its capabilities."
    },
    {
      "source": "[sections/Introduction]",
      "quote": "Following this, we conduct post-training, including Supervised Fine-Tuning (SFT) and\nReinforcement Learning (RL) on the base model of DeepSeek-V3, to align it with human preferences\nand further unlock its potential."
    },
    {
      "source": "[sections/Self-Rewarding]",
      "quote": "During the development of DeepSeek-V3, for these broader contexts, we employ the constitutional AI approach (Bai et al., 2022), leveraging the voting evaluation results of DeepSeek-V3 itself as a feedback source."
    }
  ],
  "4-1 (Pre-training Data)": "All available statements consistently describe a single, very large corpus built specifically for DeepSeek-V3. Across the quotes it is repeatedly emphasized that “DeepSeek-V3” (or “the training corpus for DeepSeek-V3”) is trained on “14.8 T / 14.8 trillion high-quality and diverse tokens.” No other quantity is ever mentioned, and the 14.8 T figure is contrasted with both the earlier DeepSeek-V2 corpus and with external baselines (e.g., Qwen2.5’s 18 T token corpus). The authors explain that, relative to DeepSeek-V2, the V3 corpus was “optimized by enhancing the ratio of mathematical and programming samples” and “expanding multilingual coverage beyond English and Chinese,” suggesting deliberate class-balance adjustments and an intentional broadening of language coverage. The material is always described as “high-quality,” “diverse,” and “tokenized with our tokenizer,” but no specific public data source list, license information, or precise language breakdown is provided in the quoted text. The same 14.8 T figure is referenced when introducing the model (“a large MoE language model with 671 B total parameters and 37 B activated parameters, trained on 14.8 T tokens”) and when outlining the overall training pipeline (“pre-train … on 14.8 trillion diverse and high-quality tokens, followed by Supervised Fine-Tuning and Reinforcement Learning”). Thus, the pre-training data for DeepSeek-V3 can be summarized as a single 14.8-trillion-token, internally processed corpus that deliberately boosts math/code coverage and multilingual diversity compared with the predecessor release while maintaining high quality and low redundancy.",
  "4-2 (Fine-tuning Data)": "Fine-tuning is split into a Supervised Fine-Tuning (SFT) stage lasting two epochs and a subsequent RL stage. The SFT data are explicitly divided into two broad categories:\n• Reasoning data – These cover “mathematics, code competition problems, and logic puzzles.” Every answer in this subset is machine-generated by an internal model called “DeepSeek-R1.”\n• Non-reasoning data – These cover open-ended or creative tasks such as “creative writing, role-play, and simple question answering.” Responses for this partition are produced by “DeepSeek-V2.5” and then “human annotators verify the accuracy and correctness.”\nTraining hyper-parameters are partially disclosed: DeepSeek-V3-Base is “fine-tuned for two epochs … using the cosine decay learning-rate schedule that starts at 5×10⁻⁶ and gradually decreases to 1×10⁻⁶.” The authors reiterate that these SFT and RL steps are applied “on the base model of DeepSeek-V3” to “align it with human preferences and further unlock its potential.” No explicit token counts or public release information for the SFT data are given in the selected quotes, but the creation process is clearly a mixture of synthetic data generation from earlier DeepSeek models followed by human validation for quality control.",
  "4-3 (Reinforcement Learning Data)": "Reinforcement Learning (RL) data for DeepSeek-V3 are constructed around a preference-learning pipeline. The quotes specify that the reward model itself is “trained from the DeepSeek-V3 SFT checkpoints,” meaning it inherits the SFT data distribution as its base. To improve reliability, the team “construct[s] preference data that not only provides the final reward but also includes the chain-of-thought leading to the reward,” indicating that each preference sample stores both the chosen answer and its step-by-step rationale. During policy optimization the authors “adopt Group Relative Policy Optimization (GRPO)”—the same strategy used in DeepSeek-V2—so there is no separate value-function network; instead, the baseline is estimated “from group scores.” Prompts span “coding, math, writing, role-playing, and question answering,” explicitly stressing domain diversity. All these details show that RL data stem from a mixture of SFT-derived preference pairs augmented with chain-of-thought traces, and that training covers a wide set of task types while using the GRPO algorithm to avoid a heavyweight critic.",
  "4-4 (Data Filtering)": "The statements reveal three main filtering / cleaning mechanisms in the DeepSeek-V3 pipeline. (1) Corpus-level de-duplication and balancing: compared with DeepSeek-V2, the team “optimize[s] the pre-training corpus by enhancing the ratio of mathematical and programming samples … while expanding multilingual coverage,” and they further note that “our data processing pipeline is refined to minimize redundancy while maintaining corpus diversity.” Although no numeric thresholds are provided, this indicates an explicit redundancy-reduction pass plus deliberate topical re-weighting. (2) Tokenizer-aware preprocessing: a “new pretokenizer introduces tokens that combine punctuations and line breaks.” The authors caution that this choice “may introduce the token boundary bias … when the model processes multi-line prompts,” implying an awareness of potential evaluation artefacts caused by the new segmentation scheme. (3) Post-RL rejection sampling: “Upon completing the RL training phase, we implement rejection sampling to curate high-quality SFT data for the final model, where the expert models are used as data generation sources.” The goal is to “ensure that the final training data retains the strengths of DeepSeek-R1 while producing responses that are concise and effective.” Together, these points describe a multi-stage filtering strategy: deduplication and topical balancing at corpus build time, tokenizer-level cleaning that merges certain character sequences, and a final rejection-sampling pass that filters newly generated answers before they become part of the model’s last-stage training data.",
  "4-1 (Pre-training Data)__evidence": [
    {
      "source": "[pdf_text]",
      "quote": "During pre-training, we train DeepSeek-V3 on 14.8T high-quality and diverse tokens."
    },
    {
      "source": "[sections/4.1 Data Construction]",
      "quote": "Compared with DeepSeek-V2, we optimize the pre-training corpus by enhancing the ratio of mathematical and programming samples, while expanding multilingual coverage beyond English and Chinese."
    },
    {
      "source": "[sections/4.1 Data Construction]",
      "quote": "Finally, the training corpus for DeepSeek-V3 consists of 14.8T high-quality and diverse tokens in our tokenizer."
    },
    {
      "source": "[abstract]",
      "quote": "We pre-train DeepSeek-V3 on 14.8 trillion diverse and high-quality tokens, followed by Supervised Fine-Tuning and Reinforcement Learning stages to fully harness its capabilities."
    },
    {
      "source": "[sections/Pre-Training]",
      "quote": "During pre-training, we train DeepSeek-V3 on 14.8T high-quality and diverse tokens."
    },
    {
      "source": "[sections/Conclusion]",
      "quote": "In this paper, we introduce DeepSeek-V3, a large MoE language model with 671B total parameters and 37B activated parameters, trained on 14.8T tokens."
    },
    {
      "source": "[sections/Chinese Benchmarks]",
      "quote": "DeepSeek-V3 surpasses Qwen2.5-72B by 16.4 points, despite Qwen2.5 being trained on a larger corpus compromising 18T tokens, which are 20% more than the 14.8T tokens that DeepSeek-V3 is pre-trained on."
    }
  ],
  "4-2 (Fine-tuning Data)__evidence": [
    {
      "source": "[sections/5.1 Supervised Fine-Tuning]",
      "quote": "Reasoning Data. For reasoning-related datasets, including those focused on mathematics, code competition problems, and logic puzzles, we generate the data by leveraging an internal DeepSeek-R1 model."
    },
    {
      "source": "[sections/5.1 Supervised Fine-Tuning]",
      "quote": "Non-Reasoning Data. For non-reasoning data, such as creative writing, role-play, and simple question answering, we utilize DeepSeek-V2.5 to generate responses and enlist human annotators to verify the accuracy and correctness of the data."
    },
    {
      "source": "[sections/Introduction]",
      "quote": "Following this, we conduct post-training, including Supervised Fine-Tuning (SFT) and Reinforcement Learning (RL) on the base model of DeepSeek-V3, to align it with human preferences and further unlock its potential."
    },
    {
      "source": "[sections/Supervised Fine-Tuning]",
      "quote": "We fine-tune DeepSeek-V3-Base for two epochs using the SFT dataset, using the cosine decay learning rate scheduling that starts at 5 × 10−6 and gradually decreases to 1 × 10−6."
    },
    {
      "source": "[sections/Supervised Fine-Tuning]",
      "quote": "Reasoning Data. For reasoning-related datasets, including those focused on mathematics, code competition problems, and logic puzzles, we generate the data by leveraging an internal DeepSeek-R1 model."
    },
    {
      "source": "[sections/Supervised Fine-Tuning]",
      "quote": "For non-reasoning data, such as creative writing, role-play, and simple question answering, we utilize DeepSeek-V2.5 to generate responses and enlist human annotators to verify the accuracy and correctness of the data."
    }
  ],
  "4-3 (Reinforcement Learning Data)__evidence": [
    {
      "source": "[sections/5.2 Reinforcement Learning]",
      "quote": "Similar to DeepSeek-V2 (DeepSeek-AI, 2024c), we adopt Group Relative Policy Optimization (GRPO) (Shao et al., 2024), which foregoes the critic model that is typically with the same size as the policy model, and estimates the baseline from group scores instead. We incorporate prompts from diverse domains, such as coding, math, writing, role-playing, and question answering, during the RL process."
    },
    {
      "source": "[sections/5.2 Reinforcement Learning]",
      "quote": "The reward model is trained from the DeepSeek-V3 SFT checkpoints. To enhance its reliability, we construct preference data that not only provides the final reward but also includes the chain-of-thought leading to the reward."
    },
    {
      "source": "[abstract]",
      "quote": "We pre-train DeepSeek-V3 on 14.8 trillion diverse and high-quality tokens, followed by Supervised Fine-Tuning and Reinforcement Learning stages to fully harness its capabilities."
    },
    {
      "source": "[sections/Introduction]",
      "quote": "Following this, we conduct post-training, including Supervised Fine-Tuning (SFT) and Reinforcement Learning (RL) on the base model of DeepSeek-V3, to align it with human preferences and further unlock its potential."
    },
    {
      "source": "[sections/Reinforcement Learning/Reward Model]",
      "quote": "The reward model is trained from the DeepSeek-V3 SFT checkpoints."
    },
    {
      "source": "[sections/Reinforcement Learning/GRPO]",
      "quote": "Similar to DeepSeek-V2 (DeepSeek-AI, 2024c), we adopt Group Relative Policy Optimization (GRPO) (Shao et al., 2024), which foregoes the critic model that is typically with the same size as the policy model, and estimates the baseline from group scores instead. We incorporate prompts from diverse domains, such as coding, math, writing, role-playing, and question answering, during the RL process."
    }
  ],
  "4-4 (Data Filtering)__evidence": [
    {
      "source": "[sections/4.1 Data Construction]",
      "quote": "Compared with DeepSeek-V2, we optimize the pre-training corpus by enhancing the ratio of mathematical and programming samples, while expanding multilingual coverage beyond English and Chinese. Also, our data processing pipeline is refined to minimize redundancy while maintaining corpus diversity."
    },
    {
      "source": "[sections/4.1 Data Construction]",
      "quote": "In addition, compared with DeepSeek-V2, the new pretokenizer introduces tokens that combine punctuations and line breaks. However, this trick may introduce the token boundary bias (Lundberg, 2023) when the model processes multi-line prompts without terminal line breaks, particularly for few-shot evaluation prompts."
    },
    {
      "source": "[sections/Supervised Fine-Tuning]",
      "quote": "Upon completing the RL training phase, we implement rejection sampling to curate high-quality SFT data for the final model, where the expert models are used as data generation sources. This method ensures that the final training data retains the strengths of DeepSeek-R1 while producing responses that are concise and effective."
    }
  ],
  "__usage": {
    "fine_tuning": "used",
    "rl": "used"
  }
}